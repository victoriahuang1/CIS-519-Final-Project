<doc id="345" url="https://de.wikipedia.org/wiki?curid=345" title="Arabische Sprache">
Arabische Sprache

Die arabische Sprache ("Arabisch", Eigenbezeichnung , kurz , ) ist die verbreitetste Sprache des semitischen Zweigs der afroasiatischen Sprachfamilie und in ihrer Hochsprachform eine der sechs Amtssprachen der Vereinten Nationen. Schätzungsweise wird Arabisch von 206 Millionen Menschen als Muttersprache und von weiteren 246 Millionen als Zweit- oder Fremdsprache gesprochen. Auch durch seine Rolle als Sakralsprache entwickelte sich das Arabische zur Weltsprache. Die moderne arabische Standardsprache beruht auf dem klassischen Arabischen, der Sprache des Korans und der Dichtung, und unterscheidet sich stark von den gesprochenen Varianten des Arabischen.

Die einzelnen arabischen Dialekte in den verschiedenen Ländern unterscheiden sich teilweise sehr stark voneinander, wenn auch meist nur in der Aussprache, und sind bei vorliegender geographischer Distanz gegenseitig nicht oder nur schwer verständlich. So werden beispielsweise algerische Filme, die im dortigen Dialekt gedreht worden sind, zum Teil hocharabisch untertitelt, wenn sie in den Golfstaaten ausgestrahlt werden.

Die arabische Sprache umfasst eine Vielzahl verschiedener Sprachformen, die in den letzten anderthalb Jahrtausenden gesprochen wurden und werden. Das Maltesische ist mit den maghrebinisch-arabischen Dialekten stark verwandt, wurde jedoch im Gegensatz zu den anderen gesprochenen Formen des Arabischen zu einer eigenständigen Standardsprache ausgebaut.

Aus dem klassischen Arabisch hat sich eine Vielzahl von Dialekten entwickelt. Für alle Sprecher dieser Sprache, außer den Sprechern des Maltesischen, ist Hocharabisch Schrift- und Dachsprache.

Ob Hocharabisch als moderne Standardsprache zu betrachten ist, ist umstritten (siehe auch Ausbausprache). Es fehlt oft an einem einheitlichen Wortschatz für viele Begriffe der modernen Welt sowie am Fachwortschatz in vielen Bereichen moderner Wissenschaften. Darüber hinaus ist Hocharabisch innerhalb der einzelnen arabischen Länder relativ selten ein Mittel zur mündlichen Kommunikation.

Gute Kenntnisse des klassischen Arabisch sind unerlässlich für das Verständnis des Korans; die bloße Kenntnis eines Dialekts ist nicht ausreichend. Daher können Koranausgaben Erläuterungen in modernem Hocharabisch enthalten, um das Verständnis zu erleichtern.

Varianten des Arabischen werden von etwa 370 Millionen Menschen gesprochen und damit weltweit am sechsthäufigsten verwendet. Es ist Amtssprache in folgenden Ländern: Ägypten, Algerien, Bahrain, Dschibuti, Irak, Israel, Jemen, Jordanien, Katar, Komoren, Kuwait, Libanon, Libyen, Mali, Marokko, Mauretanien, Niger, Oman, Palästinensische Autonomiegebiete, Saudi-Arabien, Somalia, Sudan, Syrien, Tschad, Tunesien, Vereinigte Arabische Emirate und Westsahara. Verkehrssprache ist es in Eritrea, Sansibar (Tansania), Südsudan, wird von muslimischen Bevölkerungsteilen in Äthiopien gesprochen und gewinnt auf den Malediven an Bedeutung. Darüber hinaus ist es eine der sechs offiziellen Sprachen der Vereinten Nationen.

In allerneuester Zeit gewinnt das gesprochene Hocharabische wieder an Zuspruch. An dieser Entwicklung maßgeblich beteiligt sind die panarabischen Satellitensender, z. B. "al-Dschazira" in Katar. Allerdings ist Hocharabisch (fuṣḥā) auf allgemeiner Kommunikationsebene nicht vorherrschend, vielmehr bewegen sich die Sprachformen in den Registern der sog. "ʾal-luġa ʾal-wusṭā", also als eine „mittlere Sprache“ zwischen Hocharabisch und Dialekt.

Durch die im arabischen Verbreitungsgebiet dominierende ägyptische Film- und Fernsehproduktion (u. a. bedingt durch die Bevölkerungszahl) gilt der gesprochene Kairoer Dialekt in den jeweiligen Gesellschaften gemeinhin als verständlich, ist sozusagen „gemeinsprachlich“ etabliert. Gewöhnliche Filme auf Hocharabisch zu drehen, ist eher unüblich, da diese Sprachnorm generell ernsteren Themen vorbehalten ist, wie sie z. B. in Fernseh- und Rundfunknachrichten, religiösen Sendungen oder Gottesdiensten vorkommen.

Das klassische Hocharabisch unterscheidet sich nur geringfügig von der altarabischen Sprache. Versucht man durch Vergleich aller semitischen Sprachen die Herkunft eines Wortes zu ermitteln, stößt man häufig auf dieselbe klassisch-arabische Form. Dadurch kommt dem klassischen Hocharabisch eine zentrale Stellung innerhalb der semitischen Sprachen zu.

Lange betrachteten viele Semitisten das klassische Arabisch als die ursprünglichste semitische Sprache überhaupt. Erst allmählich stellt sich durch Vergleiche mit anderen afro-asiatischen Sprachen heraus, dass vieles nicht derart ursprünglich ist, wie angenommen wurde. Klassisches Hocharabisch stellt dem Wissensstand nach eine neuere Schicht semitischer Sprachen dar, die viele Möglichkeiten konsequent ausgebaut hat, die in der Grammatik der semitischen Sprachen bereits angelegt waren. So hat es einen umfangreichen semitischen Wortschatz bewahrt und ihn darüber hinaus erweitert. Die heutigen Dialekte waren vielen Veränderungen unterworfen, wie sie andere semitische Sprachen schon sehr viel früher (vor etwa 2000 bis 3000 Jahren) erfahren hatten.

Schon in vorislamischer Zeit existierte auf der arabischen Halbinsel eine reichhaltige Dichtersprache, die in Gedichtsammlungen wie der Mu'allaqat auch schriftlich überliefert ist. Auf dieser Dichtersprache fußt zum Teil das Arabische des Korans, das aber wohl schon modernere Züge aufwies, wie man am Konsonantentext sehen kann. Wohl erst nachträglich hat man durch Zusatzzeichen das Koran-Arabisch für neue nichtarabische Muslime einfacher gemacht. In frühislamischer Zeit wurden viele Gedichte dieser Sprache schriftlich festgehalten. Bis heute ist das Auswendiglernen von Texten ein wichtiger Bestandteil der islamischen Kultur. So werden bis heute Menschen sehr geachtet, die den gesamten Koran auswendig vortragen können ("Hafiz"/"Ḥāfiẓ"). Dies ist ein Grund, warum Koranschulen in der muslimischen Welt (insbesondere Pakistan) weiter einen regen Zustrom erfahren.

Das klassische Hocharabisch ist insbesondere die Sprache des Korans, die sich aus dem Zentrum der arabischen Halbinsel, dem Hedschas, im Zuge der islamischen Eroberungen über den ganzen Vorderen Orient verbreitete. Der Kalif Abd al-Malik erhob in den 90er Jahren des 7. Jahrhunderts diese Form des Arabischen zur offiziellen Verwaltungssprache des islamischen Reiches.

Im Laufe der Jahrhunderte änderte sich die Sprache zunehmend, was jedoch an der Schrift zum Teil nicht zu erkennen ist, da die kurzen Vokale außerhalb des Korans im Allgemeinen nicht notiert wurden, und da die Orthographie von späteren Formen der Sprache, wenn überhaupt niedergeschrieben, sich an der Schreibung des klassischen Arabisch orientierte.

Das ursprüngliche Hocharabisch wird heute als Muttersprache kaum mehr gesprochen. Doch wird es, lediglich mit Wortschatzänderungen, schriftlich mittels Bücher und Zeitungen noch benutzt (außer in Tunesien, Marokko und in etwas geringerem Maße in Algerien, wo sich das Arabische diese Rolle mit dem Französischen teilt). Im wissenschaftlich-technischen Bereich wird in den anderen arabischen Ländern aus Mangel an spezifischem Fachwortschatz neben Französisch oft auch Englisch gebraucht.

Bei offiziellen Anlässen wird die in der Regel nur geschriebene Sprache auch verbal verwendet. Diese Sprache wird deshalb häufig als modernes Hocharabisch bezeichnet. Sie unterscheidet sich vom klassischen Hocharabisch vor allem in Wortschatz und je nach Bildungsgrad des Sprechers teilweise auch in Grammatik und Aussprache.

"Siehe auch:" Arabische Literatur.

Das Hocharabische Lautsystem ist wenig ausgeglichen. Es gibt nur die drei mit den Lippen gebildeten Laute , und ; und fehlen. Dagegen gibt es sehr viele an den Zähnen gebildete Laute. Charakteristisch sind die sogenannten emphatischen (pharyngalisierten) Konsonanten [], [], [] und [] (angegeben ist die IPA-Lautschrift). Der kehlige, rauhe Lauteindruck des Arabischen entsteht durch die zahlreichen Gaumen- und Kehllaute wie dem tief in der Kehle gesprochenen oder dem Kehlkopf-Presslaut („Ain“) und dessen stimmloser Variante („Ḥa“). Der Knacklaut („Hamza“) ist ein vollwertiges Phonem.

Im Hocharabischen existieren nur die drei Vokale "a", "i" und "u", die jeweils kurz oder lang sein können, sowie die zwei Diphthonge "ai" und "au". Die Aussprache der Vokale wird von den umgebenden Konsonanten beeinflusst und variiert stark. Beispielsweise sind , und mögliche Allophone des Phonems "/a/".

Das Hocharabische verfügt über 28 Konsonantenphoneme. Die Halbvokale und werden in der Grammatiktradition der westlichen Arabistik als „konsonantische Vokale“ gezählt. Alle Konsonanten können geminiert (verdoppelt) vorkommen.

Im klassischen Arabischen gibt es offene bzw. kurze Silben der Form KV und geschlossene bzw. lange Silben der Form KV̅ oder KVK (K steht für einen Konsonanten, V für einen Kurzvokal, V̅ für einen Langvokal). Nach dem Langvokal "ā" und nach "ay" kann auch ein verdoppelter Konsonant stehen und eine überlange Silbe KV̅K verursachen (z. B. "dābba" „Tier“).

Im modernen Hocharabischen ändert sich die Silbenstruktur, weil die klassischen Endungen meist weggelassen werden. Dadurch sind am Wortende neben den langen auch überlange Silben der Form KV̅K und KVKK möglich (z. B. "bāb", aus "bābun" „Tür“ oder "šams", aus "šamsun" „Sonne“).

Da eine Silbe nur mit einem einzelnen Konsonanten beginnt, können am Wortanfang keine Konsonantenverbindungen stehen. Bei älteren Lehnwörtern werden anlautende Konsonantenverbindungen durch einen vorangesetzten Hilfsvokal beseitigt (z. B. "usṭūl" „Flotte“, aus griechisch "στόλος", "stólos"). Bei neueren Lehnwörtern wird ein Vokal zwischen die anlautenden Konsonanten geschoben (z. B. "faransā" „Frankreich“), während frühere Entlehnungen von „Franken“ als "ʾifranǧ" wiedergegeben wurden.

Da die arabische Schrift die Betonung nicht notiert und die mittelalterlichen Grammatiker sich zur Betonung an keiner Stelle geäußert haben, kann man strenggenommen keine sicheren Aussagen über die Betonung des historischen klassischen Arabisch machen. Diesbezügliche Empfehlungen in Lehrbüchern beruhen auf der Betonung, die von modernen Sprechern auf das klassische Arabisch angewandt wird, wobei man sich in Europa gewöhnlich an den Aussprachegewohnheiten im Raum Libanon/Syrien orientiert. In Gebieten wie z. B. Marokko oder Ägypten werden klassisch-arabische Texte mit durchaus anderer Betonung gelesen.

Nach der üblichen Auffassung ist die Wortbetonung im Arabischen nicht bedeutungsunterscheidend und auch zum Teil nicht genau festgelegt. Generell ziehen lange Silben den Ton auf sich. Für das klassische Arabisch gilt, dass die Betonung auf der vor- oder drittletzten Silbe liegen kann. Die vorletzte Silbe wird betont, wenn sie geschlossen bzw. lang ist (z. B. "faʿáltu" „ich tat“); ansonsten wird die drittletzte Silbe betont (z. B. "fáʿala" „er tat“).

Im modernen Hocharabischen kann durch den Ausfall der klassischen Endungen auch die letzte Silbe betont werden (z. B. "kitā́b", aus "kitā́bun" „Buch“). Teilweise verschiebt sich die Betonung weiter nach vorne (z. B. "mádrasa" statt "madrásatun" „Schule“; die in Ägypten übliche Aussprache dieses Wortes ist aber z. B. "madrása", in Marokko hört man "madrasá"). Das marokkanische Arabisch ist im Gegensatz zum klassischen Arabisch und zu den anderen modernen Dialekten eine Tonsprache.

Die Phonologie der neuarabischen Dialekte unterscheidet sich stark von der des klassischen Arabischen und des modernen Hocharabischen. Die "i" und "u" werden teils als und gesprochen. Die meisten Dialekte monophthongisieren "ay" und "aw" zu [] und [], wodurch die Dialekte über fünf statt drei Vokalphoneme verfügen. Kurze Vokale werden oft zum Schwa reduziert oder fallen völlig aus. Dadurch sind in manchen Dialekten auch Konsonantenhäufungen am Wortanfang möglich. Beispiel: für baḥr: bḥar (Meer); für laḥm: lḥam (Fleisch) im tunesischen Dialekt, wobei die geöffnete bzw. geschlossene Silbe ausgetauscht wird.

Die Dialekte haben zum Teil Konsonanten des Hocharabischen verloren, zum Teil haben sie auch neue Phoneme entwickelt. Die Laute [] und [] fallen in nahezu sämtlichen Dialekten zu einem Phonem zusammen, dessen Aussprache regional variiert. Ebenfalls hat der Laut seinen Phonemstatus verloren. In einigen Dialekten sind und zu und geworden; bei Wörtern aus dem Hocharabischen werden sie aber als und ausgesprochen. Das hocharabische wird auf unterschiedliche Arten realisiert, unter anderem in Ägypten als und in Teilen Nordafrikas und der Levante als . Das hocharabische wird in Teilen Ägyptens und der Levante als gesprochen, in einigen anderen Dialekten hat es sich zu entwickelt. Oft wird jedoch die Aussprache bei Wörtern aus dem Hocharabischen beibehalten, so dass die Phoneme und parallel existieren. Einige Dialekte haben durch Lehnwörter aus anderen Sprachen fremde Phoneme übernommen, z. B. die Maghreb-Dialekte den Laut aus dem Französischen oder der irakische Dialekt den Laut aus dem Persischen.

Geschrieben wird das Arabische von rechts nach links mit dem arabischen Alphabet, das nur Konsonanten und Langvokale kennt. Es gibt allerdings als Lern- und Lesehilfe ein nachträglich hinzugefügtes System mit Kennzeichen (Taschkil) für die Kurzvokale A, I und U, und das in der klassischen Grammatik wichtige End-N, Konsonantenverdopplungen und Konsonanten ohne nachfolgenden Vokal. Der Koran wird immer mit allen Zusatzzeichen geschrieben und gedruckt. Grundsätzlich wäre das vokalisierte und mit Zusatzzeichen versehene Schriftarabisch gleichzeitig eine präzise Lautschrift, diese wird jedoch fast nur für den Koran genutzt. Bei allen anderen Texten muss man die kurzen Vokale auswendig wissen, was nur möglich ist, wenn man die grammatische Struktur vollständig analysieren kann, so dass man die richtigen Endungen einfügen kann.

Die arabische Schrift ist eine Kurrentschrift, die sich im Laufe der Geschichte verschliffen hat. Da die Buchstaben in einem Wort verbunden werden, gibt es bis zu vier verschiedene Formen eines Buchstabens: allein stehend, nach rechts verbunden, nach links verbunden und beidseitig verbunden. Als immer mehr Buchstaben in der Gestalt zusammenfielen, entwickelte man ein System, diese durch Punkte über und unter den Konsonanten zu unterscheiden. Alte Formen der arabischen Schrift, wie "Kufi" (), benutzen noch keine Punkte. Im Laufe der Zeit wurde Kufi mehr und mehr durch die Kursive "Naschī" () ersetzt.

In vielen islamischen Ländern gibt es Bestrebungen, sich bei der Aussprache der modernen Hochsprache einem Standard zu nähern, der dem nahekommen soll, was als Aussprachestandard für das klassische Hocharabisch gilt. Grundlage dabei ist meistens der Aussprachestandard der Rezitation (ar. "tilāwa" ) des Korans, der weitgehend kodifiziert ist und in modernen Korandrucken auch durch Diakritika wiedergegeben wird. Diese Ausspracheform genießt ein hohes Prestige, wird allerdings in der Regel nur im religiösen Kontext verwendet.

Die frühere Aussprache des Hocharabischen ist nicht mit Sicherheit in allen Einzelheiten bekannt. Ein typischer Fall, in dem bis heute keine völlige Klarheit über die Aussprachenormen des klassischen Hocharabisch besteht, ist die so genannte Nunation, also die Frage, ob die Kasusendungen bei den meisten unbestimmten Nomina auf "n" auslauten oder nicht ("kitābun" oder "kitāb"). Für beide Varianten lassen sich Argumente finden, und da in alten Handschriften das Vokalzeichen der Endung nicht geschrieben wurde, kann man nicht mit Bestimmtheit sagen, wie diese Formen ausgesprochen wurden.

Das Arabische kennt indeterminierte (unbestimmte) und determinierte (bestimmte) Nomina, die sich in der Hochsprache (nicht mehr im Dialekt) durch ihre Endungen unterscheiden. Indeterminierte Nomina erhalten, sofern sie nicht diptotisch flektiert werden (siehe unter Kasus), die Nunation. Determiniert wird ein Nomen vor allem durch den vorangestellten Artikel "al-" (, dialektal oft "el-" oder "il-"), welcher in seiner Form zwar unveränderlich ist, aber nach einem Vokal im Satzinneren ohne Stimmabsatz (Hamza) gesprochen wird (siehe Wasla). Außerdem kommt es (beim Sprechen) zu einer Assimilation des im Artikel enthaltenen "l" an den nachfolgenden Laut, wenn es sich bei diesem um einen sogenannten Sonnenbuchstaben handelt (Bsp.: "asch-schams" – „die Sonne“ – statt "al-schams"). Bei Mondbuchstaben bleibt der Artikel "al-" und der nachfolgende Laut wird nicht verdoppelt. Determiniert ist ein Wort auch im Status constructus durch einen nachfolgenden (determinierten) Genitiv oder ein angehängtes Personalsuffix; ferner sind auch viele Eigennamen (z. B. , "Lubnan" – Libanon) ohne Artikel determiniert.

Ein Beispiel: , "al-qamar(u)" – „der Mond“ im Gegensatz zu , "qamar(un)" – „ein Mond“

Im Arabischen gibt es zwei Genera (Geschlechter): das Femininum (weiblich) und das Maskulinum (männlich). Die meisten weiblichen Wörter enden auf "a", das – so es sich um ein Ta marbuta handelt – im Status constructus zu "at" wird. Weibliche Personen (Mutter, Schwester etc.), die meisten Eigennamen von Ländern und Städten sowie die Namen doppelt vorhandener Körperteile (Fuß - qadam; Hand - yad; Auge -ʿayn) sind auch ohne weibliche Endung weiblich. Das Gleiche gilt für einige weitere Substantive wie z. B. die Wörter für „Wind“ "(rīḥ)", „Feuer“ "(nār)", „Erde“ "(arḍ)" oder „Markt“ "(sūq)".

Beispiele:

Es gibt drei Numeri: Singular (Einzahl), Dual (Zweizahl) und Plural (Mehrzahl). In den Dialekten hat sich die Kategorie des Numerus jedoch teilweise auf bemerkenswerte Weise verändert. So ist im ägyptischen Dialekt bei den meisten Substantiven der Dual nicht mehr im Gebrauch und daher das Inventar auf zwei Numeri reduziert. Auf der anderen Seite haben einige Substantive für Zeiteinheiten nicht nur den Dual bewahrt, sondern als vierten Numerus noch einen gesonderten Zählplural ausgebildet, z. B. „Tag“: Singular "yōm", Dual "yōmēn", Plural "ayyām", Plural nach Zahlwörtern "tiyyām".

Das Arabische kennt auch ein Kollektivum, das u. a. bei Obst- und Gemüsesorten vorkommt. Ein Beispiel hierfür ist ; um den Singular eines Kollektivums zu bilden, wird ein Ta marbuta angehängt: .

Man unterscheidet drei Fälle: Nominativ (al-marfūʿ; auf -u endend), Genitiv (al-maǧrūr; auf -i endend) und Akkusativ (al-manṣūb; auf -a endend), die meist durch die kurzen Vokale der Wortendungen (im Schriftbild durch orthographische Hilfszeichen) markiert werden. Die meisten Nomina werden triptotisch flektiert, d. h., sie weisen den drei Kasus entsprechend drei unterschiedliche Endungen auf (determiniert: "-u, -i, -a"; indeterminiert: "-un, -in, -an"). Daneben gibt es Diptota – Nomina, bei denen die Genitivendung im Status indeterminatus gleich der Akkusativendung -a lautet (die beiden Kasus werden formal nicht unterschieden) und die keine Nunation haben ("-u, -a, -a"). Diptotisch flektiert werden vor allem Adjektive der Grundform "afʿal" (darunter Farbadjektive wie "aḥmar-u, aḥmar-a – rot") und bestimmte Pluralstrukturen (wie "fawāʿil", Bsp.: "rasāʾil-u, rasāʾil-a – Briefe").

Der Genitiv folgt beispielsweise immer nach Präpositionen (z. B. "fi ’l-kitābi – in dem Buch") und in einer Genitivverbindung auf das Nomen regens (Bsp.: "baitu ’r-raǧuli – das Haus des Mannes").

Die arabische Sprache unterscheidet nicht wie das Deutsche zwischen einem direkten (Akkusativ-) Objekt und einem indirekten (Dativ-) Objekt. Stattdessen kann die Konstruktion aus Präposition und Genitiv im Deutschen häufig mit dem Dativ wiedergegeben werden.

Beispiel: fi ’l- baiti - in dem Haus

Die wirkliche Komplexität der arabischen Sprache liegt in der Vielfalt ihrer Verbalformen und der daraus abgeleiteten Verbalsubstantive, Adjektive, Adverbien und Partizipien. Jedes arabische Verb verfügt mit dem Perfekt und dem Imperfekt zunächst über zwei Grundformen, von denen erstere eine vollendete Handlung in der Vergangenheit ausdrückt (Beispiel: "kataba – er schrieb/hat geschrieben"), letztere hingegen eine unvollendete im Präsens oder Futur ("yaktubu – er schreibt/wird schreiben"). Das Futur (I) kann aber auch durch Anhängen des Präfixes "sa-" oder durch die Partikel "saufa" vor dem Imperfekt gebildet werden ("sayaktubu/saufa yaktubu – er wird schreiben"). Zudem kennt das Arabische gleichfalls eine Art Verlaufsform der Vergangenheit ("kāna yaktubu – er pflegte zu schreiben") und die beiden Zeitstufen Futur II ("yakūnu qad kataba – er wird geschrieben haben") und Plusquamperfekt ("kāna qad kataba – er hatte geschrieben"), die allerdings in erster Linie in geschriebenen Texten vorkommen. Das Imperfekt gliedert sich in die Modi Indikativ "(yaktubu)", Konjunktiv "(yaktuba)", Apokopat "(yaktub)" und Energikus ("yaktubanna" oder "yaktuban"). Der Konjunktiv kommt u. a. nach Modalverben (z. B. arāda - wollen) im Zusammenhang mit ʾan (dass) oder als negierte Form des Futurs mit der Partikel lan (lan yaktuba - er wird nicht schreiben) vor. Der Apokopat wird zumeist als Verneinung der Vergangenheit zusammen mit der Partikel lam verwendet (lam yaktub - er schrieb nicht). Der Energikus kann häufig mit der Konstruktion fa+l(i) gebildet werden ((fal-)yaktubanna- er soll/ muss schreiben). Eine weitere wichtige Form ist das Verbalsubstantiv ("kitābatun – das Schreiben"). Die Bildung der Verbalsubstantive erfolgt bis auf den Grundstamm nach einem festen Schema, d. h., die Verbalsubstantive der Stämme II - X lassen sich bis auf wenige Ausnahmen nach bestimmten Stammbildungsmorphemen ableiten (Bsp.: tafʿīl für den II. Stamm, mufāʿala/ fiʿāl für den III. Stamm usw.).

Bsp.: nāqaša (III) - diskutieren → munāqaša/niqāš - Dialog; Diskussion
Viele Verben existieren in mehreren von insgesamt 15, durch Umbildung der Wurzel abgeleiteten Stämmen, die jeweils bestimmte Bedeutungsaspekte (z. B. intensivierend, kausativ, denominativ, aktiv oder passiv, transitiv oder intransitiv, reflexiv oder reziprok) haben können. Von diesen 15 Stämmen werden in der heutigen arabischen Schriftsprache allerdings nur neun regelmäßig verwendet, die Stämme IX und XI–XV kommen nur selten vor. Der 9. Stamm wird hauptsächlich verwendet, um die Verben für Farben bzw. körperliche Eigenschaften zu bezeichnen:

iḥmarra (von aḥmar) - erröten, rot werden

iḥwalla (von aḥwal) - schielen

Die Übersetzung der Verben der Stämme II - X kann teilweise durch bestimmte Regeln erfolgen. Bei der Ableitung eines Verbs vom Grundstamm kann z. B. der 3. Stamm eine Tätigkeit bezeichnen, die mit oder durch eine Person geschieht, während der 7. Stamm oft ein Passiv ausdrückt:

kātaba (III) - korrespondieren mit jmdm.

Jeder Stamm besitzt bestimmte Eigenschaften, z. B. ein Präfix, Verlängerung, Änderung oder Wegfall eines Vokals oder auch Verdopplung (Gemination) des mittleren Radikals (d. h. Wurzelkonsonanten). Die Art und Reihenfolge dieser Konsonanten, mit Ausnahme sogenannter schwacher Radikale, ändern sich hingegen innerhalb einer Wortfamilie nie. Die meisten Verbformen lassen sich schematisch ableiten.

Eine Eigenheit der arabischen Grammatik erleichtert die mündliche Wiedergabe des Hocharabischen sehr: Am Ende eines Satzes fällt im Hocharabischen die Vokalendung meist weg. Man nennt diese Form „Pausalform“. Nun werden aber die drei Fälle und auch zum Teil die Modi gerade durch diese Endungen ausgedrückt, die bei einer Sprechpause wegfallen. Deshalb benutzen viele Sprecher, wenn sie modernes Hocharabisch sprechen, sehr häufig diese „Pausalform“ und ersparen sich so einen Teil der manchmal komplizierten Grammatik. Das komplizierte System der Verbformen ist in vielen Dialekten noch weitestgehend erhalten, sodass die Dialektsprecher damit weniger Schwierigkeiten haben. Obwohl wie unten beschrieben die Bedeutung eines Wortes meist an den Konsonanten hängt, sind es gerade die kurzen Vokale, die einen großen Teil der komplizierten Grammatik ausmachen.

Das Arabische ist eine Sprache, in der die Verben „sein“ und „haben“ viel unvollständiger als im Deutschen ausgebildet sind. Positive Nominalsätze mit präsentischem Zeitwert werden in der Regel ohne Kopula gebildet ("ʾanā kabīr" – „ich [bin] groß“); nur zur Verstärkung oder wenn die Syntax es formal notwendig macht (z. B. nach der Konjunktion أن "ʾan" – „dass“) wird – wie in der Zeitstufe der Vergangenheit – das temporale Hilfsverb "kāna" für „sein“ gebraucht. Zur Verneinung des Nominalsatzes (ohne Kopula) dient im Präsens die flektierbare Negation "laisa" („nicht sein“). Das Verb „haben“ existiert gar nicht, es wird stattdessen durch die Präpositionen "li-" („für“), "fī" („in“), "maʿa" („mit“) und besonders "ʿinda" („bei“) + Personalsuffix ausgedrückt, wobei es sich wieder um einen Nominalsatz handelt ("ʿindī..." – „bei mir [ist]...“ = „ich habe...“; verneint: "laisa ʿindī..." – „bei mir [ist] nicht...“ = „ich habe nicht...“).

Da ferner das Arabische relativ wenige eigenständige Adverbien (im Deutschen wären das z. B. „noch“, „fast“, „nicht mehr“ etc.) besitzt, enthalten manche Verben neben ihrer ursprünglichen Bedeutung auch noch eine adverbiale Bedeutung. Diese Verben können im Satz alleine oder in Verbindung mit einem anderen Verb im Imperfekt stehen, z. B. "mā zāla" (wörtlich: „nicht aufgehört haben“) – ((immer) noch (sein)) oder "kāda" (fast/beinahe (sein)). 
In manchen Dialekten werden diese Adverbien anders ausgedrückt. So heißt "noch" in Ägypten "lissa" oder "bardu".

Eine weitere Verbkategorie sind die Zustandsverben (z. B. "kabura" – „groß sein“, "ṣaġura" – „klein sein“), welche ein Adjektiv verbalisieren und anstelle eines Nominalsatzes verwendet werden können. Das Wortmuster dieser Verben ist häufig "faʿila" oder "faʿula". Diese Kategorie enthält einen großen Wortschatz, wird aber im Vergleich zu den Verben, welche eine Aktion ausdrücken (z. B. "ʾakala" – „essen“), seltener benutzt.

Arabische Wörterbücher sind häufig so angelegt, dass die einzelnen Wörter nach ihren Wurzeln, also quasi ihren „Wortfamilien“, geordnet sind. Daher ist es beim Erlernen des Arabischen wichtig, die Wurzelkonsonanten eines Wortes identifizieren zu können. Der überwiegende Teil der Wörter hat drei Wurzelkonsonanten, einige auch vier. Durch das Abtrennen bestimmter Vor-, Zwischen- und Endsilben erhält man die Wurzel eines Wortes. Gerade Anfänger sollten solche nach Wurzeln geordnete Wörterbücher benutzen, da der Gebrauch „mechanisch-alphabetisch“ geordneter Lexika bei geringen Grammatikkenntnissen oft dazu führt, dass eine Form nicht erkannt und falsch übersetzt wird.

Im Arabischen gibt es streng genommen nur drei Wortarten: Nomen (اِسْم), Verb (فِعْل) und Präposition (حَرْف). Präpositionen, die wir aus dem Deutschen oder Englischen kennen, sind im Arabischen Adverbien. Es gibt so genannte "echte Präpositionen", Wörter, die im Arabischen مَبْنِيّ (undeklinierbar) genannt werden, weil sie ihre Form - egal in welchem Fall, egal an welcher Position des Satzes sie stehen - nicht verändern. Ein Beispiel ist das Wort فِي.

Zu den echten Präpositionen zählen: 
Auf diese Frage gibt es keine eindeutige Antwort. Die meisten Grammatiker jedoch sehen مع als "Nomen" (اِسْم) weil das Wort مع Nunation (تَنْوِين) erhalten kann. Zum Beispiel: Sie kamen gemeinsam - جاؤوا مَعًا

Eine Präposition (حَرْف) ist per Definition مَبْنِيّ, kann also keinesfalls Nunation bekommen. Deshalb ist das Wort مع ein Adverb der Zeit oder des Orts (ظَرْف مَكان; ظَرْف زَمان), Grammatiker sagen auch:  اِسْم لِمَكان الاِصْطِحاب أَو وَقْتَهُ

Die meisten arabischen Wörter bestehen aus drei Wurzelkonsonanten (Radikalen). Daraus werden dann verschiedene Wörter gebildet, beispielsweise kann man unter anderem aus den drei Radikalen K-T-B folgende Wörter und Formen bilden:


Im klassischen Hocharabisch treten noch die meist nicht geschriebenen Endungen -a, -i, -u, -an, -in, -un, -ta, -ti, -tu, -tan, -tin, -tun oder auch keine Endung auf. Für das T in den Endungen siehe Ta marbuta; für das N in diesen Endungen siehe Nunation.

Der Wortschatz ist zwar extrem reich, aber oft nicht klar normiert und mit Bedeutungen aus der Vergangenheit überfrachtet. So gibt es zum Beispiel kein Wort, das dem europäischen Wort „Nation“ relativ genau entspricht. Das dafür gebrauchte Wort (, "Umma") bedeutete ursprünglich und im religiösen Kontext bis heute „Gemeinschaft der Gläubigen (Muslime)“; oder z. B. „Nationalität“ (, "ǧinsiyya") eigentlich „Geschlechtszugehörigkeit“ im Sinne von „Sippenzugehörigkeit“ – „Geschlechtsleben“ z. B. heißt (, "al-ḥayāt al-ǧinsiyya"), wobei "al-ḥayāt" „das Leben“ heißt. Das Wort für „Nationalismus“ (, "qaumiyya") bezieht sich ursprünglich auf die Rivalität von „(Nomaden-)Stämmen“ und kommt von "qaum", was ursprünglich und bis heute oft noch „Stamm“ im Sinne von „Nomadenstamm“ bedeutet. So überlagern sich oft in einem Wort sehr alte und sehr moderne Konzepte, ohne dass das eine über das andere obsiegen würde. „Umma“ z. B. gewinnt wieder mehr seine alte religiöse Bedeutung zurück. Es gibt durch Kontakt mit klassischen Kulturen zahlreiche alte Lehnwörter aus dem Aramäischen und Griechischen und seit dem 19. Jahrhundert viele neuere aus dem Englischen und Französischen.

Wie in anderen Sprachen sind auch im Arabischen die Strukturwörter am häufigsten. Je nach Zählmethode und Textkorpus erhält man unterschiedliche Ergebnisse.

Eine Studie der Universität Riad kommt zu folgendem Ergebnis:


Die vorstehende Liste enthält weder monomorphematische Wörter noch Personalsuffixe. In einer anderen Wortliste sind diese berücksichtigt:


Beide Zählungen lassen den bestimmten Artikel "al-" (der, die, das) außer Acht.

Das häufigste Substantiv, das im Deutschen eine substantivische Entsprechung hat, ist laut der Riader Studie يوم "yaum" („Tag“), das häufigste Adjektiv كبير "kabīr" („groß“).

Allgemeine Erklärung der Menschenrechte:

Übertragungen ins Arabische erfolgen meist aus dem Englischen und Französischen, oft aus dem Spanischen sowie zur Zeit der Sowjetunion aus dem Russischen. Selten sind Übertragungen aus anderen europäischen Sprachen wie auch aus dem Japanischen, Chinesischen, Persisch, Türkisch und Hebräisch. So liegen zum Beispiel Werke von Jürgen Habermas lediglich in einer in Syrien erschienenen Übertragung aus dem Französischen vor. Einige Werke von Friedrich Nietzsche, ebenfalls aus dem Französischen, wurden in Marokko verlegt. In Syrien erschien "Der Antichrist" von Nietzsche in einer Übersetzung aus dem Italienischen. Die Buchmesse Kairo, zweitgrößte der Welt für den arabischen bzw. nordafrikanischen Raum, ist staatlich.

Zahlreiche deutschsprachige Universitäten und gemeinnützige Weiterbildungseinrichtungen bieten Kurse für Arabisch als Fremdsprache an, z. B. als Teil der Orientalistik, Theologie, oder eben der Arabistik, der Wissenschaft der arabischen Sprache und Literatur. Das Interesse für Arabisch als Fremdsprache beruht unter anderem darauf, dass es die Sprache des Koran ist und alle islamischen Begriffe in ihrem Ursprung arabisch sind. In muslimischen Schulen weltweit gehört Arabisch zum Pflichtprogramm. Es gibt eine Vielzahl von Arabisch-Sprachschulen, wobei sich die meisten im arabischsprachigen Raum oder auch in nichtarabischen muslimischen Regionen befinden.

Für westliche Lerner des Arabischen ist das erste große Hindernis die arabische Schrift, im deutschsprachigen Raum wird vor allem auf das Erlernen des Modernen Standard-Arabischen (MSA) gezielt, welches im Unterschied zu den arabischen Dialekten auch geschrieben wird. Seine Mutterform, "Fusha", gilt als Sakralsprache und beachtet die sog. Nunation, worauf beim MSA größtenteils verzichtet wird.
Da die arabische Schrift eine Konsonantenschrift ist und mit Ausnahme von Lehrbüchern und Korantexten ohne Vokalisierung geschrieben wird, nimmt das Erlernen des geschriebenen Wortschatzes unverhältnismäßig viel Zeit in Anspruch, verglichen mit den Alphabetschriften anderer Sprachen. Auch in arabischsprachigen Ländern wird in den ersten zwei Schuljahren ausnahmslos alles mit Vokalisation geschrieben.

Was die Grammatik des modernen Standard-Arabischen betrifft, so wirkt sich der spätere Wegfall der Vokalisierungen bremsend auf die Lerngeschwindigkeit aus. Sogar für Muttersprachler wird in der Schule ein Großteil des Arabischunterrichts für die korrekte Konjugation verwendet.


Allgemeine Beschreibungen

Grammatiken

Lehrbücher

Wörterbücher

Fachliteratur zu spezifischen Themen







</doc>
<doc id="347" url="https://de.wikipedia.org/wiki?curid=347" title="Aorta">
Aorta

Die Aorta (, von ), auch Hauptschlagader oder große Körperschlagader, ist ein großes Blutgefäß, das aus der linken Seite des Herzens entspringt. Sie leitet das Blut aus der linken Herzkammer (linker Ventrikel) in die Gefäße des großen Blutkreislaufs.

Gemeint war damit bei Hippokrates (460–um 370 v. Chr.) noch die Luftröhre mit den zwei Hauptbronchien, an denen die „Lungen hingen“; erst Aristoteles (384–322 v. Chr.) übertrug die Bezeichnung auf die große Körperschlagader

Die Aorta ist die größte Schlagader (Arterie) des Körpers. Beim erwachsenen Menschen hat sie in der Regel einen Durchmesser von etwa 2,5–3,5 cm und eine Länge von 30–40 cm. Sie hat die Form eines aufrechten Spazierstocks mit einem bogenförmigen Anfang und einem geraden Verlauf nach unten bis in den Beckenbereich.

In der Anatomie, Chirurgie und bei bildgebenden Verfahren unterscheidet man zur besseren Orientierung folgende Aortenabschnitte:


Aufgrund der großen Elastizität ihrer Gefäßwand erfüllt die Aorta eine so genannte Windkesselfunktion, die durch Druckausgleich aus dem schubweise aus dem Herz ausgestoßenen Blut einen kontinuierlichen Blutstrom macht. Der Druck des Blutes wird dabei ständig durch Drucksensoren (so genannte Barorezeptoren) gemessen.


Man unterscheidet folgende Typen der Bauchaortenstenose:



</doc>
<doc id="348" url="https://de.wikipedia.org/wiki?curid=348" title="AutoCAD">
AutoCAD

AutoCAD [] ist Teil der CAD-Produktpalette von Autodesk.

AutoCAD wurde als grafischer Zeichnungseditor von der Firma AutoDesk entwickelt. Hauptsächlich wurde AutoCAD als einfaches CAD-Programm mit Programmierschnittstellen zum Erstellen von technischen Zeichnungen verwendet. Heute umfasst die Produktpalette eine umfangreiche 3D-Funktion zum Modellieren von Objekten sowie spezieller Erweiterungen insbesondere für Ingenieure, Maschinenbauingenieure, Architekten, Innenarchitekten und Designfachleute sowie Geoinformatiker, Gebäudetechniker und allgemeine Bauingenieure.

AutoCAD ist grundsätzlich ein vektororientiertes Zeichenprogramm, das auf einfachen Objekten wie Linien, Polylinien, Kreisen, Bögen und Texten aufgebaut ist, die wiederum die Grundlage für kompliziertere 3D-Objekte darstellen.

Die zu AutoCAD entwickelten Dateiformate .dwg sowie .dxf bilden einen Industriestandard zum Austausch von CAD-Daten. Laut Autodesk wurden seit der Erfindung des DWG-Formates rund drei Milliarden Dateien erstellt, davon wurden im Jahr 2006 eine Milliarde aktiv bearbeitet.

AutoCAD lief unter MS-DOS und wurde auch auf Unix und Apple Macintosh portiert. Seit Release 14 in den 1990er Jahren wurde nur noch Microsoft Windows als Betriebssystem unterstützt. Seit dem 15. Oktober 2010 ist AutoCAD für macOS erhältlich (ab Version 10.5.8).

Mit "AutoCAD 360" (vormals "AutoCAD WS"), stehen auch vereinfachte, kostenfreie Versionen als Webapp und native Mobile App für Smartphones und Tablet-PCs zur Verfügung (Android und iOS).

Betriebssystemkompatibilität für Microsoft Windows:

Die aktuelle Version ist AutoCAD 2018, erschienen im März 2017.

AutoCAD Ersterscheinung
Die ARX (AutoCad Runtime Extension)-Version wird durch die interne Versions-Variable ACADVER angezeigt. ARX-Anwendungen sind nur innerhalb des ganzzahligen Versions-Anteils (z. B. 17) kompatibel ausführbar.

AutoCAD wird in verschiedenen Varianten mit unterschiedlichem Funktionsumfang angeboten.

"AutoCAD" ist eine Software zur Bearbeitung von technischen Zeichnungen als Vektorgrafiken in 2D- und 3D. Die Software ist programmiert in C++ und besitzt mehrere Programmierschnittstellen wie zum Beispiel AutoLISP. AutoCAD wird häufig mit zusätzlicher Software eingesetzt, die mit vorgegebenen Symbolen, Makros und Berechnungsfunktionen zur schnellen Erstellung von technischen Zeichnungen dient. Im Zuge der Weiterentwicklung wurden diese Funktionen direkt in die auf AutoCAD basierenden Produkte integriert.

"AutoCAD LT" ist eine vereinfachte AutoCAD-Variante, mit der meist 2D-Zeichnungen erstellt werden und die weniger Programmierschnittstellen besitzt. Auch hier gibt es zusätzliche Software, die durch die vorgegebenen Symbole, Makros und Software mit Berechnungsfunktionen zur schnellen Erstellung von technischen Zeichnungen dient. Aufgrund der geringeren Funktionalität ist "AutoCAD LT" kostengünstiger als die 3D-Variante AutoCAD.

"AutoCAD Mechanical" ist eine Erweiterung von AutoCAD für den Maschinenbau-Bereich (CAD/CAM), die aus dem ehemaligen deutschen Softwarehaus "GENIUS CAD software GmbH" im bayerischen Amberg durch Übernahme seitens "Autodesk" entstanden ist. Es ist eine sehr leistungsfähige 2D-Applikation mit deutlich erweitertem Befehlsumfang, Normteilen, Berechnungs- und Stücklistenfunktionen.

Die früher vertriebene Erweiterung "Mechanical Desktop" für die mechanische 3D-Konstruktion wird nicht mehr weiterentwickelt. Stattdessen gibt es das wesentlich leistungsfähigere und modernere parametrische 3D-Programm für die Konstruktion in Mechanik und Maschinenbau "Autodesk Inventor". "AutoCAD", "AutoCAD Mechanical" und "Autodesk Inventor" werden mit weiteren Produkten als Paket mit dem Namen „Product Design Suite“ vermarktet. Eine Erweiterung stellt „Product Design Suite Ultimate“ mit dem „Inventor Professional“ dar, das die Funktionalität um FEM-Berechnung, dynamische Simulation, Rohrleitungs- und Kabelbaumkonstruktion erweitert. Die Verwaltung der Konstruktionsdaten kann mit "Autodesk Vault" erfolgen.

"AutoCAD Architecture" ist eine erweiterte AutoCAD-Variante für den Bau- und Architekturmarkt (CAAD), die über eine vordefinierte 3D-Bibliothek für Bauteile, die zum Konstruieren von Gebäuden benötigt werden (Wände, Fenster, Treppen, Dächer, etc.) verfügt. AutoCAD Architecture ersetzt den bis zur Einführung von Autodesk entwickelten Architectural Desktop (ADT). Wie bei anderen Software-Lösungen auf Basis von AutoCAD (Civil3d, Inventor, …) handelt es sich um ein sogenanntes "vertikales Produkt". Die Zeichnung wird wahlweise in 2D oder 3D angefertigt und Grundrisse, Ansichten und Schnitte, die für den Bau notwendig sind, werden automatisch erstellt. Da AutoCAD Architecture objektorientiert arbeitet und das IFC-Format beherrscht, kann es zu den BIM-CAD Systemen gezählt werden.

"AutoCAD MEP" () ist eine erweiterte AutoCAD-Architecture-Variante für die Gebäudetechnik (HVAC/MEP), die über eine vordefinierte 3D-Bibliothek für Bauteile, die zum Konstruieren von gebäudetechnischen Anlagen benötigt werden (Heizkessel, Heizkörper, Rohrleitungen, Rohrleitungsarmaturen, Klimakomponenten, Elektrotrassen, Schalter und Dosen, etc.) verfügt. Die Zeichnung wird vollständig 3D angefertigt und Grundrisse, Ansichten und Schnitte, die für die Gebäudetechnik notwendig sind, werden wie bei "AutoCAD Architecture" automatisch erstellt. Die Kompatibilität zu "AutoCAD Architecture" ist damit gewährleistet.

AutoCAD ReCap ist eine AutoCAD-Erweiterung, die zusätzlich zu den 3D-Modellen das Verarbeiten von Punktwolken, wie sie zum Beispiel Laserscanner liefern, in AutoCAD ermöglicht.

AutoCAD Map 3D basiert auf AutoCAD und ergänzt dieses um umfangreiche Funktionen für den Bereich Kartografie. Mit dem Programm erstellt und bearbeitet man technische Karten. Es lassen sich durch diverse Schnittstellen Daten aus zahlreichen Quellen integrieren und in gewissem Umfang auch Geodaten-Analysen durchführen. In der aktuellen Version sind die 3D-Funktionen erweitert worden, so lassen sich unter anderem auch Höhenlinienpläne generieren.

1998 wurde die Software "Topobase" von der Schweizer Firma "C-Plan AG" in Gümligen als AutoCAD-Erweiterung veröffentlicht. 2005 wurde angekündigt, dass die Firma von Autodesk übernommen werde, was 2006 geschah. Die Erweiterung macht "AutoCAD Map 3D" zu einem Geoinformationssystem und basiert auf einer nach Standards des Open Geospatial Consortium schematisierten Datenbank von Oracle mit Spatial-Erweiterung. Ab der Version 2012 ist sie in Map 3D integriert und für alle Fachschalen einsetzbar.

Auch "TB-Web GIS" wurde von der Firma "C-Plan" neben Topobase entwickelt. Nach Übernahme durch Autodesk wurde die Software als "Autodesk Topobase Web" angeboten.

Autodesk entwickelte das PHP-basierte Web-GIS-Framework "Autodesk MapGuide Enterprise", das von der Open Source Geospatial Foundation OSGeo quelloffen als MapGuide Open Source erhältlich ist.

Die Produkte "Autodesk MapGuide Enterprise" und "Autodesk Topobase Web" wurden zusammengelegt zur Mapserver-Software mit Web-GIS-Framework namens Autodesk Infrastructure Map Server.

AutoCAD Civil 3D basiert auf AutoCAD und ist für die Bearbeitung von Tiefbauprojekten, insbesondere Verkehrswege-, Landschaftsplanung, Geländemodellierung und Wasserbau , geeignet. Um die Bearbeitung von Projekten zu ermöglichen, die sich über weite und komplexe Geländeformen ziehen, ist die volle Funktionalität von AutoCAD Map 3D in AutoCAD Civil 3D integriert.

AutoCAD ecscad basiert auf AutoCAD und ist für die Planung elektrotechnischer Steuerungssysteme, sogenannter Stromlaufpläne geeignet.

Das ebenfalls von "Autodesk" stammende Programm "Autosketch" ist kein richtiges CAD-Programm, sondern ein einfaches Vektor-Zeichenprogramm. Aktuelle Version ist Autosketch 2011. Das Programm wird von Autodesk nicht mehr unterstützt oder weiterentwickelt. Es wurde abgelöst von dem Programm "Autodesk SketchBook", welches auch gratis in einer abgespeckten Expressversion für Windows, MacOs, IOS und Android erhältlich ist.

AutoCAD bietet eine Vielzahl an Programmierschnittstellen (APIs) für Customizing und Automatisierung. Als interne Programmierschnittstellen stehen heute zur Verfügung

sowie weitere Schnittstellen zu:

Durch den Einsatz von Vorgabezeichnungen, Blöcken, Symbolen, Linientypen und externen Spezialprogrammen, zum Beispiel für die Ausgabe von Berechnungsergebnissen können relativ einfach fast alle geometrischen und technischen Darstellungen erzeugt oder modifiziert werden.

AutoCAD verwendet überwiegend eigene Dateiformate.

Nach außen ist dieses Dateiformat durch den Dateinamenanhang ".dwg", für ‚normale‘ Zeichnungsdateien gekennzeichnet. Das Kürzel steht für (engl. für „Zeichnung“). Die Dokumentation der Dateistruktur ist nicht frei erhältlich, jedoch findet man im Internet eine Dokumentation der Open Design Alliance.

Das DWG-Dateiformat wurde kontinuierlich an die Anforderungen der jeweiligen AutoCAD-Versionen angepasst und erweitert. So wurde das Format mit Einführung der Versionen AutoCAD 2000, 2004, 2007, 2010, 2013 und 2018 geändert. Die als "DWG 2000", "DWG 2004", "DWG 2007", "DWG 2010", "DWG 2013" und "DWG 2018" bezeichneten Formate können nicht in ältere AutoCAD-Versionen eingelesen werden. Die eingeschränkte Kompatibilität des DWG-Dateiformats zu älteren AutoCAD-Versionen kann durch Abspeichern in älteren Formatversionen (kann im Programm generell festgelegt werden) sowie durch die Verwendung des DXF-Dateiformats und den Einsatz von externen Konverterprogrammen teilweise umgangen werden. Bei Nutzung des DXF-Formats ist dabei mit dem Zerfall von nicht unterstützen Objekten in einfachere Basisobjekte zu rechnen. Die ersten 6 Bytes einer dwg-Datei sind mit einem gewöhnlichen Texteditor lesbar. Sie geben die Version der DWG-Datei an. Aktuelle Dateien der Version AutoCAD 2013 bis AutoCAD 2015 beginnen mit dem Header AC1027.

Die DXF-Schnittstelle ist eine quelloffene Schnittstelle des Herstellers Autodesk und unterliegt keinem neutralen Normungsausschuss, die Dokumentation für DXF ist aber frei verfügbar. Sie ist ein in ASCII-Zeichen lesbares Abbild der binär abgespeicherten DWG. AutoCAD unterstützt DXF (engl. , „Zeichnungsaustauschformat“) für den Datenaustausch mit anderen CAD-Programmen in der aktuellen Version und jeweils noch meist 3–4 älteren Stände.

Das DXF-Dateiformat unterstützt direkt 2D- und 3D-Koordinaten sowie zum Beispiel Linien, Bögen und einfache Flächen und weitere komplexe Geometrieelemente wie zum Beispiel Blöcke, ARX-Objekte und Bemaßungen. Es ist mit einfachen Mitteln zum Beispiel mit Texteditoren und fast allen Programmiersprachen, einschließlich mit dem VBA von Excel möglich, DXF-Dateien zu erzeugen, auszuwerten oder zu manipulieren. Diese Möglichkeiten bieten sich besonders für geometrische und auf geometriebasierende Berechnungen von CAD-Modellen zum Beispiel zur Optimierung von Flächen an. Der Aufbau ist sehr klar, einfach und strukturiert.

Diese Schnittstelle hat sich im CAD-Markt als ein Quasi-Datenaustauschstandard etabliert, obwohl sie nicht von Autodesk mit diesem Ziel entwickelt wurde. Das DXF-Format wurde von Autodesk dazu geschaffen, um geometrische Informationen von AutoCAD an eine interne oder externe Applikation zur weiteren Verwendung zu übergeben. Genauso sollte das Ergebnis zum Beispiel einer Berechnung wieder aus der Applikation zurück an AutoCAD übergeben werden. Dazu wurde eine Liste von geometrischen Objekten von den Entwicklern erstellt und sauber dokumentiert. Diese offene Dokumentation wurde dann von anderen CAD-, CNC- und CAM-Herstellern wegen ihrer einfachen Struktur und Übersicht als CAD-Schnittstelle übernommen. Sie ist der oft kleinste gemeinsame Nenner vieler Vektorgrafikprogramme und wird von fast allen unterstützt. Allerdings werden meist nicht alle Funktionen von den anderen Herstellern voll unterstützt und es gehen daher manchmal entscheidende Details beim Austausch via DXF verloren.

Auch das DXF-Dateiformat wurde, wie das DWG-Dateiformat, kontinuierlich an die Anforderungen der jeweiligen AutoCAD-Versionen angepasst und erweitert.

Das DXB-Dateiformat (engl. ) ist eine binäre Form des DXF-Dateiformates. Es ist extrem kompakt, kann im Verhältnis zu DXF schnell gelesen und geschrieben werden, ist aber für den Programmierer wesentlich aufwendiger als die ASCII-Variante. DXB wird nur in wenigen, hauptsächlich zeitkritischen Anwendungsfällen verwendet.

Ein weiteres Format ist das Dateiformat "DWF" (engl. ) als hochkomprimiertes Vektorformat zur Präsentation im Internet und zur Ansicht. Das Format ist dokumentiert. Ein DWF-Toolkit mit C++-API zum Lesen und Schreiben ist mit Quelltext kostenlos bei Autodesk erhältlich. DWFx ist eine Weiterentwicklung von DWF, die auf dem XPS-Format von Microsoft basiert.

Ein weiteres Format ist das Dateiformat "DGN", das von MicroStation definiert wird und auch in den aktuellen AutoCAD Versionen unterstützt wird. Das Kürzel DGN steht für (engl. für „Entwurf“).

Ein weiteres Format ist das Dateiformat "SHP" (engl. Shapefile; nicht zu verwechseln mit dem ESRI-Shapefile), eine Symboldefinition. Dieses Dateiformat wird zur Codierung von Zeichnungselementen auf unterster Ebene eingesetzt und wird vor der Verwendung zu "SHX" kompiliert. Anwendungsgebiete sind benutzerdefinierte Schraffuren, Linien, Bemaßungen oder Schriftarten. Es können nur die elementarsten Objekte definiert werden wie Linien und Bögen.

Die Dateiendung für AutoCad-Schriftarten (Fonts) und Linientypen. Eine Schrift-shx-Datei ist jedoch in einem Binärformat codiert, eine Linientyp-shx-Datei in Reintext.
Schriften im shx-Format werden z. T. graphisch anders behandelt, als z. B. Schriften, die vom Betriebssystem zur Verfügung gestellt werden (TrueType, Postscript-Fonts), da sie keine Füllungsflächen oder Rundungen unterstützen.

Für AutoCAD gibt es zu vielen Bereichen Spezial-Anwendungen. Beispielsweise für das Bauwesen, den Maschinenbau (siehe oben), den Landschaftsbau, die Versorgungs- und Elektrotechnik. Diese sind in der Regel in C++ geschrieben. Autodesk bietet hier mit ObjectARX (C++-API) die entsprechenden Grundlagen. Die Entwicklung geht auch hier zu .NET. Einfache Programmwerkzeuge (Tools) sind bisweilen in Visual Basic oder VBA geschrieben worden. Hinzu kommen eine Vielzahl von AutoLISP-Routinen, die oft in freien Foren ausgetauscht werden. Eine Auflistung kommerzieller Anwendungen findet sich im Autodesk-Katalog.

Die Stiftung Warentest hat im Februar 2015 AutoCAD-Kurse für Einsteiger getestet. Sieben Kurse wurden getestet, vier davon bekamen eine gute Qualität bescheinigt. Unter den Anbietern waren Handwerkskammern, Industrie- und Handelskammern und kommerzielle Bildungsanbieter. Die Kosten für die drei- bis fünftägigen Kurse variierten zwischen 143 und 2090 Euro, wobei sowohl der günstigste als auch der teuerste Kurs nur mittelmäßig abschnitten.




</doc>
<doc id="349" url="https://de.wikipedia.org/wiki?curid=349" title="Alessandro Volta">
Alessandro Volta

Alessandro Giuseppe Antonio Anastasio Volta, ab 1810 "Graf von Volta" (* 18. Februar 1745 in Como; † 5. März 1827 ebenda) war ein italienischer Physiker. Er gilt als Erfinder der elektrischen Batterie und als einer der Begründer der Elektrizitätslehre.

Volta wurde als Sohn einer wohlhabenden Familie in Como im damals habsburgischen Norditalien als eines von neun Kindern geboren, von denen fünf – wie auch einige Onkel – Priester wurden. Der Vater selbst war lange Jesuitennovize. Voltas Eltern, Filippo Volta und Maria Maddalena dei Conti Inzaghi, hatten aber eine andere Laufbahn für Volta vorgesehen und schickten ihn in Vorbereitung einer Juristenlaufbahn von 1758 bis 1760 auf eine Jesuitenschule. Im Selbststudium beschäftigte er sich mit Büchern über Elektrizität (Pieter van Musschenbroek, Jean-Antoine Nollet, Giambatista Beccaria) und korrespondierte mit führenden Gelehrten. Der Turiner Physik-Professor Beccaria riet ihm, sich auf experimentelle Arbeit zu konzentrieren. 1769 veröffentlichte er seine erste physikalische Arbeit, die schon Kritik an den Autoritäten laut werden ließ. 1775 wuchs seine Bekanntheit durch die Erfindung des bald in ganz Europa benutzten Elektrophors, mit dem durch Influenz erzeugte statische Elektrizität erzeugt und transportiert werden konnte. 1774 wurde er zum Superintendenten und Direktor der staatlichen Schulen in Como ernannt. Schon 1775 wurde er dann zum Professor für Experimentalphysik an der Schule in Como berufen. 1776 entdeckte er in aus den Sümpfen am Lago Maggiore aufsteigenden Gasblasen als Erster das Methan und begann mit dem brennbaren Gas zu experimentieren (Volta-Pistole, in der ein elektrischer Funke in einer Flasche die Verbrennung auslöst, also eine Art Gasfeuerzeug). Er konstruierte damit stetig brennende Lampen und benutzte seine Volta-Pistole als Messgerät für den Sauerstoffgehalt von Gasen (Eudiometer). 

All diese Entdeckungen führten dazu, dass er 1778 (nach einer Reise in die Schweiz 1777, wo er u. a. Voltaire traf) zum Professor für Physik und bis 1819 Lehrstuhlinhaber für Experimentalphysik an die Universität Pavia berufen wurde. 

Dort erfand er ein („Strohhalm“-) Elektroskop zur Messung kleinster Elektrizitätsmengen (1783), quantifizierte die Messungen unter Einführung eigener Spannungseinheiten (das Wort „Spannung“ stammt von ihm) und formulierte die Proportionalität von aufgebrachter Ladung und Spannung im Kondensator. 

1792 erfuhr er von den Frosch-Experimenten des angesehenen Anatomen Luigi Galvani, die dieser auf animalische Elektrizität zurückführte. Volta erkannte aber die Ursache der Muskelzuckungen in äußeren Spannungen (etwa Kontaktelektrizität, falls mit mehreren Metallen experimentiert wurde), und es entsprang ein Streit um den Galvanismus, der die Wissenschaftler in ganz Europa in Lager teilte. Für Galvani lag die Erklärung darin, dass der Frosch eine Art Leidener Flasche (also ein Kondensator) war, für Volta war er nur eine Art Detektor. Heute ist immer noch wichtig, dass sich daraus Voltas langjährige Untersuchungen zur Kontaktelektrizität und schließlich seine bahnbrechende Erfindung der Batterie ergab.

Volta soll in seinen Schriften auch die Idee des Telegraphen und das Gay-Lussac-Gesetz (Volumenausdehnung von Gasen proportional zur Temperatur) vorweggenommen haben.

Seine größte und erfolgreichste Erfindung war jedoch die um 1800 konstruierte Voltasche Säule, die erste funktionierende Batterie (nachdem er schon in den 1790er Jahren elektrische Spannungsreihen verschiedener Metalle untersucht hatte). Sie bestand aus übereinander geschichteten Elementen aus je einer Kupfer- und einer Zinkplatte, die von Textilien, die mit Säure (zunächst Wasser bzw. Salzlake) getränkt waren, voneinander getrennt waren. Er schildert die Erfindung in einem berühmten Brief an Sir Joseph Banks von der Royal Society. Erst diese Erfindung der Batterie ermöglichte die weitere Erforschung der magnetischen Eigenschaften elektrischer Ströme und die Anwendung der Elektrizität in der Chemie im folgenden Jahrhundert.

1791 ernannte ihn die Londoner Royal Society zum Mitglied und verlieh ihm 1794 ihre Copley-Medaille. 1792 ging er auf seine zweite Auslandsreise, bei der er u. a. Pierre Simon Laplace, Antoine Laurent de Lavoisier in Paris sowie Georg Christoph Lichtenberg in Göttingen besuchte. 1801 reiste er nach Paris, wo er am 7. November Napoleon Bonaparte seine Batterie vorführte. 

1802 erhielt er vom Institut de France die Ehrenmedaille in Gold und von Napoleon eine Pension. 1805 wurde er zum auswärtigen Mitglied der Göttinger Akademie der Wissenschaften gewählt. Seit 1808 war er auswärtiges Mitglied der Bayerischen Akademie der Wissenschaften. Nachdem Napoleon Italien erobert hatte, ernannte er Volta, der sich schon damals eigentlich zur Ruhe setzen wollte, 1809 zum Senator und erhob ihn 1810 in den Grafenstand. Nach der Erfindung der Batterie gab er die Forschung und Lehre zunehmend auf, wurde aber durch die Ernennung zum Dekan der philosophischen Fakultät 1813 noch zum Bleiben bewogen bis zu seiner endgültigen Emeritierung 1819. Seine Karriere hatte die wechselnden Herrschaftsverhältnisse unbeschadet überstanden – er war sowohl bei den Habsburgern als auch bei Napoleon in Gunst. Im Ruhestand zog er sich auf sein Landhaus in Camnago nahe Como zurück.

Volta heiratete, nachdem er vorher lange Jahre mit der Sängerin Marianna Paris gelebt hatte, 1794 die wohlhabende Teresa Peregrini, mit der er zwei gemeinsame Söhne aufzog. Er liegt in Camnago, einem Ortsteil von Como begraben, der seit 1863 "Camnago Volta" heißt. Dort kann man auch seine Instrumente im Museum Tempio Voltiano sehen.

Im 19. Jahrhundert wurde Volta mit der höchsten Auszeichnung, die einem Physiker vermutlich zuteilwerden kann, geehrt: Zu seinen Ehren wurde die Maßeinheit für die elektrische Spannung international mit der Bezeichnung Volt betitelt. Vorgeschlagen wurde dies zuerst 1861 von einem Komitee der British Association for the Advancement of Science. 

1964 wurde der Mondkrater "Volta" nach ihm benannt, ebenso das Fernheizkraftwerk Volta in Basel (1980).





</doc>
<doc id="350" url="https://de.wikipedia.org/wiki?curid=350" title="Anwendungssoftware">
Anwendungssoftware

Als Anwendungssoftware (auch Anwendungsprogramm, kurz Anwendung oder Applikation; , kurz App) werden Computerprogramme bezeichnet, die genutzt werden, um eine nützliche oder gewünschte nicht systemtechnische Funktionalität zu bearbeiten oder zu unterstützen. Sie dienen der „Lösung von Benutzerproblemen“. Beispiele für Anwendungsgebiete sind: Bildbearbeitung, E-Mail-Programme, Webbrowser, Textverarbeitung, Tabellenkalkulation oder Computerspiele.

Aus dem englischen Begriff "application" hat sich in der Alltagssprache auch die Bezeichnung "Applikation", kurz "App", eingebürgert. Im deutschen Sprachraum wird die Abkürzung "App" seit dem Erscheinen des iOS App Store (2008) fast ausschließlich mit "mobiler App" gleichgesetzt, also Anwendungssoftware für Mobilgeräte wie Smartphones und Tabletcomputer. Allerdings wird inzwischen auch teilweise Desktop-Anwendungssoftware "App" genannt, zum Beispiel bei Microsofts Betriebssystem Windows seit Windows 8 (Windows-Apps), das sowohl auf Desktop-PCs als auch Tablets eingesetzt wird, oder bei Apples Betriebssystem OS X mit dem Mac App Store.

Anwendungssoftware wird in erheblichem Umfang zur Unterstützung der Verwaltung in Behörden und Unternehmen eingesetzt. Anwendungssoftware ist zum Teil Standardsoftware, zu großen Teilen werden auf den jeweiligen Anwendungsfall zugeschnittene Branchenlösungen als Individualsoftware eingesetzt. Im Bereich der strategischen und wirtschaftlichen Anwendungssoftware innerhalb eines Unternehmens (wie Enterprise-Resource-Planning-Systeme oder Portal-Software) spricht man auch von "Business-Anwendungen" oder "Business Software".

Mobile Apps können über einen in das mobile Betriebssystem integrierten App Store bezogen und direkt auf dem Gerät installiert werden. Mobile Web-Apps werden über den Webbrowser des Mobilgeräts abgerufen und müssen nicht installiert werden.

Eine besondere Form von Anwendungssoftware sind Webanwendungen. Auf diese wird vom Arbeitsplatzrechner oder Mobilgerät über einen Webbrowser zugegriffen und sie laufen im Browser ab. Webanwendungen erfordern im Gegensatz zu Desktop-Anwendungen kein spezielles Betriebssystem, teilweise jedoch spezielle Laufzeitumgebungen.

Anwendungssoftware steht (nach ISO/IEC 2382) im Gegensatz zu Systemsoftware und Dienstprogrammen. Dazu , die aber keinen Endbenutzer-bezogenen 'Nutzen' bringen. Beispiele sind das Betriebssystem, Compiler für verschiedene Programmiersprachen oder Datenbanksysteme.

Anwendungssoftware kann sowohl lokal auf einem Desktop-Computer (Desktop-Anwendung) bzw. auf einem Mobilgerät installiert sein oder auf einem Server laufen, auf den vom Desktop-Computer bzw. Mobilgerät zugegriffen wird (Client-Server- bzw. Webanwendung). Sie kann, abhängig von der technischen Implementierung, im Modus Stapelverarbeitung oder im Dialogmodus (mit direkter Benutzer-Interaktion) ausgeführt werden. Diese beiden Unterscheidungen gelten aber für alle Computerprogramme, grundsätzlich auch für Systemsoftware.



</doc>
<doc id="352" url="https://de.wikipedia.org/wiki?curid=352" title="Antoni van Leeuwenhoek">
Antoni van Leeuwenhoek

Antoni van Leeuwenhoek [] () (auch "Antony", "Anthonie" oder "Antonie"; * 24. Oktober 1632 in Delft; 4. November 1632 getauft als "Thonis Philipszoon"; † 26. August 1723 ebenda) war ein niederländischer Naturforscher, Erbauer und Nutzer von Lichtmikroskopen.

Leeuwenhoek war Sohn eines Korbmachers, der 1638 früh verstarb. Er nannte sich später „van Leeuwenhoek“, da sein Geburtshaus in Delft am "Leeuwenpoort", dem „Löwentor“, lag. Die Mutter, Tochter eines Bierbrauers, schickte ihren Sohn auf ein Gymnasium in der Nähe von Leiden. Der Onkel führte ihn in die Grundlagen der Mathematik und Philosophie ein. 1648 ging er auf Geheiß seiner Mutter nach Amsterdam, denn er sollte dort Buchhalter werden. Er hingegen nahm eine Stelle bei einem schottischen Tuchhändler an. 1654 kehrte er nach Delft zurück, wo er den Rest seines Lebens verbrachte. Er kaufte sich ein Haus, eröffnete einen Tuchladen und wurde Kammerherr des städtischen Gerichtshofs. Als zuverlässige und kluge Person wurde er 1679 zum Eichmeister für alkoholische Getränke ernannt, (schon 1669 war er als Landvermesser zugelassen (vgl. Meyer 1998: 15-24)). Er war mit dem Maler Jan Vermeer befreundet und, nach dessen Tod 1675, sein Nachlassverwalter. Da die Gelehrtenbilder von Vermeer "Der Astronom" und "Der Geograph" eine große Ähnlichkeit mit van Leeuwenhoek aufweisen, besteht die Möglichkeit, dass der Wissenschaftler Modell für die beiden Gemälde gestanden hat.

Leeuwenhoek konnte es sich leisten, seinem Hobby nachzugehen, der Mikroskopie. Er erlernte die Kunst des Linsenschleifens und baute seine eigenen Mikroskope. Das Lichtmikroskop aus zusammengesetzten Linsen, wie wir es heute kennen, war zwar schon vor Leeuwenhoeks Geburt in Gebrauch, vor allem die Linsen wiesen jedoch Mängel auf. Sie waren unzureichend geschliffen und besaßen Einschlüsse, sodass die Mikroskope vor allem im höheren Auflösungsbereich schlechte Ergebnisse lieferten; er aber baute solche, die aus jeweils nur einer winzigen Linse bestanden, die dafür von perfekter Qualität war. Mit diesen erreichte er Vergrößerungen bis zum 270-fachen, was die Leistung der ersten mehrlinsigen Mikroskope bei weitem übertraf.

Seine winzigen, bikonvexen Linsen montierte er zwischen Messingplatten und hielt sie nahe an das Auge. Damit konnte er Objekte, die er an Nadelspitzen befestigt hatte, betrachten. Im Jahr 1668 bestätigte er die Entdeckung des Kapillarsystems (siehe Blutkreislauf) durch den italienischen Anatomen Marcello Malpighi und zeigte, wie rote Blutkörperchen durch die Kapillaren eines Kaninchenohres und eines Froschbeines zirkulierten. 1674 lieferte er die erste genaue Beschreibung von roten Blutkörperchen. Diese waren 1658 von seinem Kollegen und Konkurrenten in der mikroskopischen Forschung Jan Swammerdam entdeckt worden.

1675 beobachtete er Protozoen und Bakterien – beide nannte er Animalcula (Tierchen) – im Teichwasser, Regenwasser und im menschlichen Speichel. Diese Beobachtung wurde jedoch zunächst von der Royal Society mit außergewöhnlichem Spott kommentiert. Die Überprüfung seiner Angaben bestätigte diese jedoch, sodass er 1680 zum Mitglied ernannt wurde; er nahm aber nie an einem Treffen teil. 1683 entdeckte er Bakterien im eigenen Zahnbelag und dem von Kontrollpersonen.

1677 beschrieb er auch als Erster (nach seinem Schüler Johan Ham) Spermatozoen (Samenzellen) von Insekten und Menschen, die er ebenfalls als Animalcula bezeichnete, und widersprach der vorherrschenden Theorie von der Spontanzeugung der kleinsten Lebewesen; er postulierte stattdessen ebenso wie Swammerdam die zu der Zeit neu aufkommende Präformationslehre, nach der die "animalcules" bereits vollausgebildete Menschen im Spermienkopf darstellen würden. Entsprechend zählte er auch zu den Animalculisten, die jegliche Beteiligung der Eizelle an der Entwicklung des Menschen, die über eine reine Ernährungsfunktion hinausginge, ablehnten; Swammerdam hingegen vertrat als Ovist das Gegenteil. Leeuwenhoek wies weiter zur Widerlegung der Spontanzeugung nach, dass sich Kornkäfer, Flöhe und Muscheln aus Eiern entwickeln und nicht, wie man damals glaubte, spontan aus Schmutz oder Sand entstehen.

Er beschrieb weiter die Querstreifung der Muskulatur und das Netzwerk, das die Zellen des Herzmuskels bilden.

Leeuwenhoek fertigte mehr als 500 Mikroskope an, einige innerhalb kürzester Zeit. Das Schleifen einer Linse ist jedoch ein sehr langwieriger Prozess. Seine Methode hat er nie veröffentlicht, es wird vermutet, dass die Linsen erschmolzen wurden und nicht ausschließlich durch Schleifen entstanden. Er könnte einen Stab aus Kalk-Natron-Glas in der Mitte erhitzt und durch Auseinanderziehen der Stabenden das Glas zu einem dünnen Haar gedehnt haben. Wird ein solches Haarende erneut erhitzt, entsteht eine winzige Glaskugel, die Linse des Mikroskops.
Zudem wird vermutet, dass er neben den mit der Hand gehaltenen einlinsigen Modellen auch mehrlinsige Mikroskope mit Stativ verwendet hat. Nach Leeuwenhoeks Tod sollte es fast 250 Jahre dauern, bis wieder Mikroskope mit vergleichbar hoher Auflösung konstruiert werden konnten.

Er beschrieb drei Bakterienformen: Bazillen, Kokken und Spirillen. Er hütete die Kunst des Linsenherstellens jedoch als Geheimnis.

Leeuwenhoek hatte nicht studiert, sondern den Beruf des Tuchhändlers erlernt. Er konnte daher kein Latein, in dem damals alle wissenschaftlichen Arbeiten veröffentlicht wurden. Im April 1673 berichtete Reinier De Graaf, ein in Delft geborenes Mitglied der Royal Society of London, er habe von der hervorragenden Qualität der Mikroskope van Leeuwenhoeks gehört. Von diesem Zeitpunkt an wurde ihm erlaubt, seine Arbeiten an die Royal Society zu senden. Er empfing Besuche von bedeutenden Persönlichkeiten wie z. B. von der britischen Königin Anne, dem Zaren von Russland Peter dem Großen und Leibniz.

Während das Teleskop sofort zu Beobachtung des Weltalls und zum Entdecken von bis dahin unsichtbaren Details und Himmelskörpern eingesetzt wurde, kam vor Leeuwenhoek kaum jemand auf die Idee, mit Mikroskopen nach Strukturen oder Objekten zu suchen, die so klein sind, dass sie mit bloßem Auge nicht sichtbar sind. Bis dahin beschränkte man sich darauf, kleine, aber sichtbare Objekte wie z. B. Insekten mit Linsen zu untersuchen. Leeuwenhoek dagegen entdeckte das Reich des mikroskopisch Kleinen ("animalcula"). Möglicherweise entwickelte er auch eine Camera obscura, was die Detailpräzision der Werke seines Freundes Jan Vermeer erklären könnte. Nach seinem Tod überließ er 26 seiner Mikroskope der Royal Society.

Der Asteroid (2766) Leeuwenhoek trägt seinen Namen.




</doc>
<doc id="353" url="https://de.wikipedia.org/wiki?curid=353" title="Ambra">
Ambra

Die Ambra oder der Amber ist eine graue, wachsartige Substanz aus dem Verdauungstrakt von Pottwalen. Sie wurde früher bei der Parfümherstellung verwendet. Heute ist sie von synthetischen Substanzen weitgehend verdrängt und wird nur noch in wenigen teuren Parfüms verwendet.

Etymologen leiten die Wortformen "Ambra" und "Amber" vom arabischen Wort "anbar" ab, das häufig wie "ambar" ausgesprochen wird. "Ambra" ist auch die mittellateinische Bezeichnung, die über das Italienische ins Deutsche gelangte, während die Form "Amber" über französisch "ambre" vermittelt wurde.

Das arabische Wort "anbar" wurde von den Kreuzfahrern nach Europa gebracht. Schon im späten 13. Jahrhundert wurde das Wort in Europa auch zur Bezeichnung von Bernstein verwendet, möglicherweise weil Bernstein wie Ambra an Stränden angespült wird. Es ist aber nicht auszuschließen, dass trotz der gleichlautenden Bezeichnungen für Bernstein und Ambra keine etymologische Verwandtschaft besteht. Als die Verwendung von Ambra stark zurückging, bekam beispielsweise das Wort "ambre" im Französischen beziehungsweise "amber" im Englischen die Hauptbedeutung „Bernstein“. Zur sprachlichen Unterscheidung diente nun die Farbe. Bernstein wurde im Englischen früher "white amber" oder "yellow amber" genannt; Ambra wird bis heute im Französischen als "ambre gris" („grauer Amber“) bezeichnet und als "ambergris" im Englischen.

Der Zusammenhang der Bezeichnungen für Ambra und Bernstein zeigt sich auch in weiteren Sprachen:

Neben dem Bernstein wurde in der alten Literatur auch Walrat, weißer Liquidambar und echter flüssiger Storax als "weiße Ambra" bezeichnet. al-Kindi gab zudem unter dem Stichwort "Amber" drei Duftstoffrezepturen an, die nichts mit der Ambra zu tun hatten.

Ambra entsteht bei der Nahrungsaufnahme von Pottwalen. Die unverdaulichen Teile wie Schnäbel oder Hornkiefer von Tintenfischen und Kraken werden in Ambra eingebettet. Im Darm einzelner Pottwale können bis zu 400 Kilogramm Ambra enthalten sein. Solche Mengen führen jedoch gehäuft zu Darmverschluss und schließlich zum Tod dieser Tiere. Über die genaue Ursache der Entstehung besteht Unklarheit. Möglicherweise liegt eine Stoffwechselkrankheit des Pottwals vor, wenn er Ambra bildet. Einer anderen Theorie zufolge dient der Stoff dem antibiotischen Wundverschluss bei Verletzungen der Darmwand. Ins Meer gelangt die Substanz durch Erbrechen, als Kotsteine oder durch den natürlichen Tod der Tiere.

Ambra wird auf dem Meer treibend in Klumpen von meist bis zu 10 Kilogramm gefunden, in Einzelfällen aber auch über 100 Kilogramm. Diese Ambra-Klumpen können über Jahre bis Jahrzehnte über die Meere treiben. Selten finden sich Ambra-Brocken als Strandgut an der Küste.

Über die Entstehung der Ambra wurde schon im 10. Jahrhundert spekuliert. Der arabische Reisende Al-Masudi gab Berichte von Händlern und Seeleuten wieder, die behaupteten, Ambra wachse wie Pilze auf dem Meeresboden. Sie werde bei Stürmen aufgewirbelt und so an die Küsten gespült. Ambra komme in zwei unterschiedlichen Formen, einer weißen und einer schwarzen, vor. Al-Masudi berichtete auch davon, dass an einer Stelle der arabischen Küste am Indischen Ozean die Bewohner ihre Kamele auf die Suche nach Ambra abgerichtet hätten.

Aus Arabien stammt auch die Vorstellung, dass Ambra aus Quellen floss, die sich nahe der Meeresküste befanden. In der Märchenerzählung "Tausendundeine Nacht" strandete Sindbad, nachdem er Schiffbruch erlitten hatte, auf einer Wüsteninsel, auf der er eine Quelle mit stinkendem, rohem Ambra entdeckte. Die Substanz floss wie Wachs in das Meer, wo sie von riesigen Fischen erst verschluckt und dann wieder in Gestalt wohlriechender Klumpen erbrochen wurde, die an den Strand trieben.

In China bezeichnete man Ambra bis etwa 1000 n. Chr. als "lung sien hiang" (Lóngxiánxiāng 龍涎香), als das „Speichelparfüm der Drachen“, da man glaubte, dass die Substanz aus dem Speichel von Drachen stamme, die auf Felsen am Rande des Meeres schliefen. Im Orient ist Ambra noch heute unter diesem Namen bekannt.

Im antiken Griechenland, wo Ambra wegen seiner angeblich die Alkoholwirkung verstärkenden Eigenschaft Wein beigemischt wurde, nahm man eine Quelle in Meeresnähe als Ursprungsort der Ambra an.

In weiten Teilen des antiken und frühmittelalterlichen Europa nahm man an, dass echter Bernstein und Ambra gleichen oder zumindest ähnlichen Ursprungs seien. Vermutlich geht diese Vorstellung auf die Übereinstimmungen dieser beiden Substanzen im Wohlgeruch, der Seltenheit und des Wertes sowie im äußeren Erscheinungsbild und des Vorkommens (an Meeresküsten) zurück. Allerdings wird schon in frühen Chroniken ein Unterschied zwischen Ambra und Bernstein erwähnt. Ambra wurde danach entweder als Sperma von Fischen oder Walen angesehen, als Kot unbekannter Seevögel (vermutlich aus einer fehlerhaften Deutung der in der Ambra enthaltenen Tintenfischschnäbel) oder als große Bienenstöcke aus Küstengebieten.

Marco Polo kannte bereits die Herkunft von Ambra aus dem Magen von Walen. Er berichtete, dass die Bewohner der Insel Sokotra, die nahe dem Horn von Afrika liegt, mit großen Mengen Ambra handelten. Laut seinem Bericht zogen sie die Kadaver von verendeten Walen an Land, um Ambra aus dem Magen zu holen und „Öl“ aus dem Kopf zu gewinnen.

Johannes Hartlieb gab in seinem "Kräuterbuch" (Entstehung zwischen 1435 und 1450) an, dass Ambra auf dem Meeresboden wachse und dort durch Wasserturbulenzen von Walen losgelöst werde. Dies entsprach Al-Masudis Theorie über die Herkunft der Ambra, die im 16. Jahrhundert auch von Adam Lonitzer vertreten wurde.

1574 folgerte der flämische Botaniker Carolus Clusius als erster aus Einschlüssen von Tintenfischschnäbeln im Ambra, dass dieses aus dem Verdauungstrakt von Walen stammt. Dies blieb aber lange Zeit wenig beachtet. Erst später, als man bei der Schlachtung von Pottwalen frische Ambra im Darm einzelner Tiere entdeckte, wurde Clusius’ Aussage bestätigt.

Der Schiffsarzt Exquemelin deutete Ambra noch im 17. Jahrhundert als Wachs von Wildbienen: 
„In diesen Landschaften gibt es ja auch viele Bienen, die an den Waldbäumen ihren Honig machen, und so passiert es denn nicht selten, dass durch heftige Stürme das Wachs zusamt dem an den Bäumen hängenden Honig dem Meere zugetrieben wird. […] Was wohl recht glaubhaft ist, denn dieses Ambra ist, wenn man es findet, noch weich und riecht wie Wachs.“

Das 1721 in Leipzig erschienene "Allgemeine Lexicon der Künste und Wissenschafften" beschreibt als wahrscheinlichste Erklärung für Ambra dies als ein „Erdpech“, das durch die Flut angeschwemmt und durch Luft und Meerwasser gehärtet werde.

Im Jahre 1783 legte der Botaniker Joseph Banks der Royal Society eine Arbeit des in London lebenden deutschen Arztes Franz Xaver Schwediauer vor, in der dieser die in Westeuropa vorherrschenden Irrtümer über Ambra und den Ursprung dieser Substanz beschrieb. Er identifizierte Ambra als ein Erzeugnis des oft unnatürlich aufgeblähten Darms kranker Pottwale und brachte die Entstehung von Ambra mit den Schnäbeln von Tintenfischen, der Hauptnahrung der Pottwale, in Verbindung.

Ambra ist eine graue bis schwarze mit hellgelben bis grauen Streifen oder Punkten durchsetzte, undurchsichtige, wachsartige, zähe Masse. Die Dichte beträgt etwa 0,8–0,9 g/cm, sie ist in Wasser unlöslich, in Alkohol und Ether schwach löslich, der Schmelzpunkt liegt bei ca. 60 °C, der Siedepunkt bei etwa 100 °C. 

Frische Ambra ist weiß, weich und riecht abstoßend. Erst durch den über Jahre oder Jahrzehnte währenden Kontakt mit Luft, Licht und Salzwasser erhält sie ihre feste Konsistenz und ihren angenehmen Duft.
Sie besteht zu etwa 95 % aus geruchslosen Sterinen (Epicoprosterol, Coprosterol, Coprostanone, Cholesterol) und dem ebenfalls geruchslosen Triterpen­alkohol Ambrein, sowie Pristan und Ketonen. Die geruchsbestimmenden Inhaltsstoffe ca. 0,5 % werden durch Luft und Licht aus Ambrein gebildet - u. a. Ambrox und Ambrinol. Die Duftnote wird als holzig, trocken, balsamisch, etwas tabakartig mit aphrodisierendem Einschlag beschrieben. Ambra, bzw. ihre synthetische Form, wird typischerweise als Basisnote in Duftkompositionen eingesetzt.

Die beiden französischen Chemiker Joseph Bienaimé Caventou und Pierre Joseph Pelletier waren die ersten, die Ambrein isolierten, charakterisierten und so benannten.

Der grauen und schwarzen Ambra kam bei der Herstellung von Parfüm erhebliche Bedeutung zu. In Asien ist Ambra ein beliebter Räucherstoff, der schon viele Jahrhunderte vor Christus bei verschiedenen Ritualen und Zeremonien eingesetzt wurde. Im Orient wird Ambra auch als Gewürz für Nahrungsmittel und Weine und als Aphrodisiakum verwendet. Ambra wurde früher auch zur Zubereitung besonders exklusiver Speisen verwendet.

Im Mittelalter wurde Ambra als Arznei im Rahmen der Humoralpathologie verwendet. Johannes Hartlieb erläuterte in seinem "Kräuterbuch", die Substanz wirke im zweiten Grade trocken und heiß. Dadurch helfe Ambra hervorragend bei allen Herzerkrankungen, es gilt als "die hochst erznei zu dem herzen". Ferner wirke Ambra gegen Ohnmachten, Epilepsie und Gebärmutterhochstand.

Jan Huygen van Linschoten schrieb in seinen Reiseberichten über die Ambra:
Adam Lonitzer gab in seinem "Kreüterbuch" mit folgenden Worten eine Ersatzrezeptur für echte Ambra an:
Bereits im 15. Jahrhundert wurde Ambra in Europa gehandelt und mit Gold aufgewogen, wenngleich diese Funde nur in seltenen Fällen den höchsten Qualitätsansprüchen genügten. Leo Africanus schrieb im 16. Jahrhundert, dass in Fès der Preis für ein Pfund Ambra bei 60 Dukaten liege (im Vergleich dazu kostete ein Sklave 20, ein Eunuch 40 und ein Kamel 50 Dukaten). Damit war es eine sehr kostbare Substanz.

Aufgrund der Synthetisierung dieser Substanz und des Handelsverbots von Pottwalprodukten gemäß dem Washingtoner Artenschutz-Übereinkommen wird Ambra heutzutage kein Wert mehr beigemessen. Für angespülte Fundstücke werden jedoch nach wie vor hohe Summen gezahlt, die je nach Qualität pro Kilogramm auch im fünfstelligen Eurobereich liegen können.

Ein im Dezember 2012 vor der niederländischen Insel Texel angespülter Pottwalkadaver enthielt einen Ambrabrocken mit einem Gewicht von 83 Kilogramm im Wert von etwa 500.000 Euro. Im Januar 2013 wurde nahe Blackpool in England ein 3 Kilogramm schwerer Brocken gefunden, dessen Wert auf etwa 100.000 Pfund geschätzt wurde. Nach eingehender Analyse stellte ein Experte jedoch fest, dass es sich nicht um Ambra handelte.

In der Liebeslyrik wurde häufig die Ambra genannt.

In Herman Melvilles "Moby-Dick" heißt es:




</doc>
<doc id="354" url="https://de.wikipedia.org/wiki?curid=354" title="Albedo">
Albedo

Die Albedo (lateinisch "albedo" „Weiße“; v. lat. "albus" „weiß“) ist ein Maß für das Rückstrahlvermögen (Reflexionsstrahlung) von diffus reflektierenden, also nicht selbst leuchtenden Oberflächen. Sie wird als dimensionslose Zahl angegeben und entspricht dem Verhältnis von rückgestrahltem zu einfallendem Licht (eine Albedo von 0,9 entspricht 90 % Rückstrahlung). Die Albedo hängt bei einer gegebenen Oberfläche von der Wellenlänge des einstrahlenden Lichtes ab und kann für Wellenlängenbereiche – z. B. das Sonnenspektrum oder das sichtbare Licht – angegeben werden. Vor allem in der Meteorologie ist sie von Bedeutung, da sie Aussagen darüber ermöglicht, wie stark sich eine Oberfläche erwärmt – und damit auch die Luft in Kontakt mit der Oberfläche.

In der Klimatologie ist die so genannte Eis-Albedo-Rückkopplung ein wesentlicher, den Strahlungsantrieb und damit die Strahlungsbilanz der Erde beeinflussender Faktor, der relevant für den Erhalt des Weltklimas ist. 
In der 3D-Computergrafik findet die Albedo ebenfalls Verwendung; dort dient sie als Maß für die diffuse Streukraft verschiedener Materialien für Simulationen der Volumenstreuung.

In der Astronomie spielt die Albedo eine wichtige Rolle, da sie mit grundlegenden Parametern von Himmelskörpern (z. B. Durchmesser, scheinbare/absolute Helligkeit) zusammenhängt.

Es werden verschiedene Arten der Albedo unterschieden:

Das Verhältnis zwischen sphärischer Albedo und geometrischer Albedo ist das sogenannte Phasenintegral (siehe Phase), das die winkelabhängige Reflektivität jedes Flächenelements berücksichtigt.

Die Messung der Albedo erfolgt über Albedometer und wird in Prozent angegeben. In der Astronomie können aufgrund der großen Entfernungen keine Albedometer eingesetzt werden. Die geometrische Albedo kann hier aber aus der scheinbaren Helligkeit und dem Radius des Himmelskörpers und den Entfernungen zwischen Erde, Objekt und Sonne berechnet werden. Um die sphärische Albedo zu bestimmen, muss auch das Phasenintegral (und somit die Phasenfunktion) bekannt sein. Diese ist allerdings nur für diejenigen Himmelskörper vollständig bekannt, die sich innerhalb der Erdbahn bewegen (Merkur, Venus). Für die oberen Planeten kann die Phasenfunktion nur teilweise bestimmt werden, wodurch auch die Werte für ihre sphärische Albedo nicht exakt bekannt sind.

Satelliten der US-Raumfahrtbehörde NASA messen seit ca. 2004 die Albedo der Erde. Diese ist insgesamt, abgesehen von kurzfristigen Schwankungen, in den letzten zwei Jahrzehnten konstant geblieben; regional dagegen gab es Veränderungen von mehr als 8 %. In der Arktis z. B. ist die Rückstrahlung geringer, in Australien höher geworden.

Das Deep Space Climate Observatory misst seit 2015 die Erd-Albedo in einem Abstand von 1,5 Millionen Kilometern zur Erde vom Lagrange-Punkt L1 aus. An diesem Punkt hat die Sonde einen dauerhaften Blick auf die sonnenbeschienene Seite der Erde.

Die Oberflächenbeschaffenheit eines Himmelskörpers bestimmt seine Albedo. Der Vergleich mit den Albedowerten irdischer Substanzen ermöglicht es also, Rückschlüsse auf die Beschaffenheit anderer planetarer Oberflächen zu ziehen. Gemäß der Definition der sphärischen Albedo ist die Voraussetzung von parallel einfallendem Licht wegen der großen Entfernungen der reflektierenden Himmelskörper von der Sonne als Lichtquelle sehr gut gegeben. Die stets geschlossene Wolkendecke der Venus strahlt viel mehr Licht zurück als die basaltartigen Oberflächenteile des Mondes. Die Venus besitzt daher mit einer mittleren sphärischen Albedo von 0,76 ein sehr hohes, der Mond mit durchschnittlich 0,12 ein sehr geringes Rückstrahlvermögen. Die Erde hat eine mittlere sphärische Albedo von 0,3. Die höchsten bisher gemessenen Werte fallen auf die Saturnmonde Telesto (0,994) und Enceladus (0,99). Der niedrigste Mittelwert wurde mit nur 0,03 am Kometen Borrelly festgestellt.

Glatte Oberflächen wie Wasser, Sand oder Schnee haben einen relativ hohen Anteil spiegelnder Reflexion, ihre Albedo ist deshalb stark abhängig vom Einfallswinkel der Sonnenstrahlung (siehe Tabelle).

Die Albedo ist außerdem abhängig von der Wellenlänge des Lichts, das untersucht wird, weswegen bei der Angabe der Albedowerte immer der entsprechende Wellenlängenbereich angegeben werden sollte.




</doc>
<doc id="355" url="https://de.wikipedia.org/wiki?curid=355" title="Algebraische Zahl">
Algebraische Zahl

In der Mathematik ist eine algebraische Zahl "x" eine reelle oder komplexe Zahl, die Nullstelle eines Polynoms vom Grad größer als Null
mit rationalen Koeffizienten formula_2, also Lösung der Gleichung formula_3, ist.

Die so definierten algebraischen Zahlen bilden eine echte Teilmenge formula_4 der komplexen Zahlen formula_5.
Offenbar ist jede rationale Zahl formula_6 algebraisch, da sie die Gleichung formula_7 löst. Es gilt also formula_8. 

Ist eine reelle (oder allgemeiner komplexe) Zahl nicht algebraisch, so heißt sie transzendent.

Die ebenfalls gebräuchliche Definition der algebraischen Zahlen als Nullstellen von Polynomen mit ganzzahligen Koeffizienten ist äquivalent zur oben angegebenen. Jedes Polynom mit rationalen Koeffizienten kann durch Multiplikation mit dem Hauptnenner der Koeffizienten in eines mit ganzzahligen Koeffizienten umgewandelt werden. Das entstehende Polynom hat exakt die gleichen Nullstellen wie das Ausgangspolynom.

Polynome mit rationalen Koeffizienten kann man "normieren", indem man alle Koeffizienten durch den Koeffizienten formula_9 dividiert. Nullstellen von normierten Polynomen, deren Koeffizienten ganzzahlig sind, nennt man "ganzalgebraische Zahlen" oder auch "ganze algebraische Zahlen". Die ganzalgebraischen Zahlen bilden einen Unterring der algebraischen Zahlen. Zum allgemeinen Begriff der Ganzheit siehe Ganzheit (kommutative Algebra).

Man kann den Begriff der algebraischen Zahl zu dem des algebraischen Elements erweitern, indem man die Koeffizienten des Polynoms statt aus formula_10, aus einem beliebigen Körper entnimmt.

Für viele Untersuchungen algebraischer Zahlen sind der im Folgenden definierte Grad und das Minimalpolynom einer algebraischen Zahl wichtig.

Ist "x" eine algebraische Zahl, die eine algebraische Gleichung
mit formula_12, formula_13 erfüllt, aber keine derartige Gleichung geringeren Grades, dann nennt man "n" den Grad von "x". Damit sind alle rationalen Zahlen vom Grad 1. Alle irrationalen Quadratwurzeln sind vom Grad 2.

Die Zahl "n" ist gleichzeitig der Grad des Polynoms "f", des so genannten "Minimalpolynoms" von "x".

Beispielsweise ist formula_14 eine algebraische Zahl, denn sie ist eine Lösung der Gleichung formula_15. Ebenso ist die imaginäre Einheit formula_16 als Lösung von formula_17 algebraisch.

Gegen Ende des 19. Jahrhunderts wurde bewiesen, dass die Kreiszahl formula_18 und die Eulersche Zahl formula_19 nicht algebraisch sind. Von anderen Zahlen, wie zum Beispiel formula_20, weiß man bis heute nicht, ob sie algebraisch oder transzendent sind. Siehe dazu den Artikel Transzendente Zahl.

Die Menge der algebraischen Zahlen ist abzählbar und bildet einen Körper.

Der Körper der algebraischen Zahlen ist algebraisch abgeschlossen, d. h. jedes Polynom mit algebraischen Koeffizienten besitzt nur algebraische Nullstellen. Dieser Körper ist ein minimaler algebraisch abgeschlossener Oberkörper von formula_21 und ist damit ein algebraischer Abschluss von formula_10. Man schreibt ihn oft als formula_23 (für "algebraischer Abschluss von Q"; verwechselbar mit anderen Abschlussbegriffen) oder als formula_4 (für "Algebraische Zahlen").

Oberhalb des Körpers der rationalen Zahlen und unterhalb des Körpers der algebraischen Zahlen befinden sich unendlich viele Zwischenkörper; etwa die Menge aller Zahlen der Form formula_25, wobei formula_26 und formula_27 rationale Zahlen sind, und formula_6 die Quadratwurzel einer rationalen Zahl formula_29 ist. Auch der Körper der mit Zirkel und Lineal aus formula_30 konstruierbaren Punkte der komplexen Zahlenebene ist ein solcher algebraischer Zwischenkörper. → Siehe dazu euklidischer Körper.

Im Rahmen der Galoistheorie werden diese Zwischenkörper untersucht, um so tiefe Einblicke über die Lösbarkeit oder Nicht-Lösbarkeit von Gleichungen zu erhalten. Ein Resultat der Galoistheorie ist, dass zwar jede komplexe Zahl, die man aus rationalen Zahlen durch Verwendung der Grundrechenarten (Addition, Subtraktion, Multiplikation und Division) sowie durch Ziehen "n"-ter Wurzeln ("n" eine natürliche Zahl) erhalten kann (man nennt solche Zahlen "durch Radikale darstellbar"), algebraisch ist, umgekehrt aber algebraische Zahlen existieren, die man nicht in dieser Weise darstellen kann; alle diese Zahlen sind Nullstellen von Polynomen des Grades ≥ 5.


</doc>
<doc id="356" url="https://de.wikipedia.org/wiki?curid=356" title="André Gide">
André Gide

André Paul Guillaume Gide [] (* 22. November 1869 in Paris; † 19. Februar 1951 ebenda) war ein französischer Schriftsteller. 1947 erhielt er den Literaturnobelpreis.

Gide war das einzige Kind einer wohlhabenden calvinistischen Familie. Der Vater, Paul Gide (1832–1880), war Professor der Rechtswissenschaft und stammte aus der mittleren Bourgeoisie der südfranzösischen Kleinstadt Uzès, die Mutter, Juliette Rondeaux (1835–1895), aus der Großbourgeoisie von Rouen. Die Familie lebte in Paris, verbrachte die Tage um Neujahr regelmäßig in Rouen, die Osterzeit in Uzès und die Sommermonate auf den beiden Landsitzen der Rondeaux' in der Normandie, La Roque-Baignard im Pays d’Auge und Cuverville im Pays de Caux.

Mit knapp elf Jahren verlor Gide seinen Vater. Zwar trat dadurch keine materielle Notlage ein, doch war er nun ganz der puritanischen Erziehung seiner Mutter unterworfen. In seiner Autobiographie wird Gide die eigene Kindheit und Jugend, speziell das Wirken der strengen, freud- und lieblosen Mutter in dunklen Farben malen und für seine Probleme als Heranwachsender verantwortlich machen: „In dem unschuldigen Alter, in dem man in der Seele gerne nichts als Lauterkeit, Zartheit und Reinheit sieht, entdecke ich in mir nur Finsternis, Häßlichkeit und Heimtücke.“ Gide hatte seit 1874 Unterricht bei Privatlehrern, besuchte phasenweise auch reguläre Schulen, immer wieder unterbrochen durch Nervenleiden, die ärztliche Behandlung und Kuraufenthalte erforderlich machten. Im Oktober 1887 trat der fast 18-jährige Gide in die Unterprima der reformpädagogischen "École Alsacienne" ein, wo er sich mit Pierre Louÿs anfreundete. Im Jahr darauf besuchte er die Oberprima des Traditionsgymnasiums Henri IV, an dem er im Oktober 1889 das Baccalauréat ablegte. In dieser Zeit begann seine Freundschaft mit Léon Blum.

Bei einem Besuch in Rouen im Dezember 1882 verliebte sich Gide in seine Kusine Madeleine Rondeaux (1867–1938), die Tochter von Juliette Gides Bruder Émile Rondeaux. Der 13-jährige André erlebte damals, wie sehr seine Kusine unter der ehelichen Untreue ihrer Mutter litt und sah in ihr fortan den Inbegriff von Reinheit und Tugend im Gegensatz zu seiner eigenen, so empfundenen Unreinheit. Diese Jugendliebe überdauerte die folgenden Jahre und 1891 machte Gide Madeleine erstmals einen Heiratsantrag, den diese aber ablehnte. Erst nach dem Tod Juliette Gides im Mai 1895 verlobte sich das Paar und heiratete noch im Oktober 1895, dies zu einem Zeitpunkt, da Gide sich seiner Homosexualität bereits bewusst geworden war. Die Spannung, die sich daraus ergab, wird Gides literarisches Werk – zumindest bis 1914 – maßgeblich prägen und seine Ehe schwer belasten. Das Verhältnis, das er ab 1917 mit Marc Allégret einging, konnte ihm Madeleine dann nicht mehr verzeihen, weshalb sie 1918 sämtliche Briefe verbrannte, die er ihr je geschrieben hatte. Zwar blieb das Paar verheiratet, doch lebten beide nun meist getrennt. Nach Madeleines Tod im Jahr 1938 reflektierte Gide ihre Beziehung – „die verborgene Tragödie“ seines Lebens – in der Schrift "Et nunc manet in te". Der befreundete Schriftsteller Jean Schlumberger widmete dieser Ehe das Buch "Madeleine und André Gide", in dem Madeleine eine gerechtere Darstellung findet als in Gides autobiographischen Schriften.

Gide entschied sich nach dem Baccalauréat gegen ein Studium und war auch nicht gezwungen, einer Erwerbsarbeit nachzugehen. Sein Ziel war, Schriftsteller zu werden. Erste Versuche hatte er schon während der Schulzeit unternommen, als er mit Freunden, darunter Marcel Drouin und Pierre Louÿs, im Januar 1889 die literarische Zeitschrift "Potache-Revue" gründete und dort seine ersten Verse veröffentlichte. Im Sommer 1890 begab er sich alleine nach Menthon-Saint-Bernard am Lac d’Annecy, um sein erstes Buch zu schreiben: "Les Cahiers d’André Walter" („Die Tagebücher des André Walter“), die er auf eigene Kosten drucken ließ (wie alle Werke bis 1909!) und die 1891 erschienen. Gides autobiographisch geprägter Erstling hat die Form eines posthum aufgefundenen Tagebuchs des jungen André Walter, der sich, nachdem er seine Hoffnung auf die geliebte Emmanuèle hat aufgeben müssen, in die Einsamkeit zurückgezogen hat, um den Roman "Allain" zu schreiben; das Tagebuch dokumentiert seinen Weg in den Wahnsinn. Während der "André Walter" zum Druck vorbereitet wurde, besuchte Gide im Dezember 1890 seinen Onkel Charles Gide in Montpellier, wo er – vermittelt durch Pierre Louÿs – Paul Valéry kennenlernte, dem er später (1894) seine ersten Schritte in Paris erleichterte und dem er bis zu dessen Tod 1945 freundschaftlich verbunden bleiben sollte.

Zwar brachte der "André Walter" Gide keinen kommerziellen Erfolg („Ja, der Erfolg war gleich Null.“), doch ermöglichte er ihm den Zugang zu wichtigen Kreisen der Symbolisten in Paris. Wiederum vermittelt durch Pierre Louÿs, wurde er 1891 in die Zirkel von José-Maria de Heredia und von Stéphane Mallarmé aufgenommen. Dort verkehrte er mit berühmten Literaten seiner Zeit, darunter Henri de Régnier, Maurice Barrès, Maurice Maeterlinck, Bernard Lazare und Oscar Wilde, mit dem er 1891/92 regelmäßig in Kontakt stand. Gide selbst lieferte 1891 mit der kleinen Abhandlung "Traité du Narcisse. Théorie du symbole" („Traktat vom Narziß. Theorie des Symbols“) eine symbolistische Programmschrift, die für das Verständnis seiner Poetik – auch jenseits seiner symbolistischen Anfänge – grundlegende Bedeutung hat. Im Narziss-Mythos entwirft Gide sein eigenes Bild als Schriftsteller, der sich selbst bespiegelt, in permanentem Dialog mit sich selbst steht, für sich selbst schreibt und sich dadurch als Person erst erschafft. Die dieser Haltung angemessene Gattung ist das Tagebuch, das Gide seit 1889 konsequent führt, ergänzt durch weitere autobiographische Texte; aber auch in seinen erzählenden Werken ist das Tagebuch als Darstellungsmittel allgegenwärtig.

Im Jahr 1892 veröffentlichte Gide das Gedichtbändchen "Poésies d’André Walter" („Die Gedichte des André Walter“), in dem eine Auswahl aus jenen Versen geboten wurde, die der Schüler bereits in "Potache-Revue" publiziert hatte. Im Februar 1893 lernte er in den literarischen Zirkeln von Paris Eugène Rouart (1872–1936) kennen, der ihm die Bekanntschaft mit Francis Jammes vermittelte. Die Freundschaft zu Rouart, die bis zu dessen Tod bestand, hatte für Gide große Bedeutung, weil er in dem ebenfalls homosexuell veranlagten Freund einen Kommunikationspartner fand, den die gleiche Identitätssuche umtrieb. Der Briefwechsel beider, vor allem in den Jahren 1893 bis 1895, zeigt den vorsichtig tastenden Dialog, der beispielsweise über das 1893 auf Französisch erschienene Werk "Die conträre Sexualempfindung" von Albert Moll geführt wurde. Im Jahr 1893 veröffentlichte Gide die kurze Erzählung "La Tentative amoureuse" („Der Liebesversuch“), deren Haupthandlung aus einer unverklemmten Liebesgeschichte besteht, deren leicht ironischer Nachspann dagegen eine „Madame“ anspricht, die sichtlich diffiziler ist als die Geliebte der Haupthandlung. Im selben Jahr verfasste er die lyrische lange Erzählung "Le Voyage d’Urien" („Die Reise Urians“), wo er in Form eines phantastischen Reiseberichts wieder einmal die schwierige Suche eines müßigen, materiell sorgenfreien jungen Intellektuellen nach dem „wahren Leben“ thematisiert.

Im Jahr 1893 ergab sich für Gide die Möglichkeit, den befreundeten Maler Paul-Albert Laurens (1870–1934) nach Nordafrika zu begleiten. Die Freunde setzten im Oktober von Marseille nach Tunis über, reisten weiter nach Sousse, verbrachten die Wintermonate im algerischen Biskra und kehrten über Italien zurück. Gide war im November 1892 wegen einer leichten Tuberkulose vom Militärdienst befreit worden; während der Reise nach Sousse brach die Krankheit aus. Die Monate in Biskra dienten der Regeneration und waren mit ersten heterosexuellen Kontakten zu jungen Prostituierten verbunden. Für Gide noch bedeutsamer war die erste homosexuelle Erfahrung mit einem Jugendlichen, die sich bereits in Sousse zugetragen hatte. Die langsame Genesung und die erwachte Sinnlichkeit in der nordafrikanischen Landschaft erlebte Gide als Wendepunkt in seinem Leben: „mir schien, als hätte ich zuvor gar nicht gelebt, als träte ich aus dem Tal der Schatten und des Todes ins Licht des wahren Lebens, in ein neues Dasein, in dem alles Erwartung, alles Hingabe wäre.“

Von der Reise zurückgekehrt empfand Gide ein „Gefühl der Entfremdung“ seinen bisherigen Lebensumständen gegenüber, was er in dem Werk" Paludes" („Sümpfe“) verarbeitete, das während eines Aufenthaltes in La Brévine in der Schweiz entstand (Oktober bis Dezember 1894). In "Palude" karikiert er nicht ohne Melancholie den Leerlauf in den Literatenzirkeln der Hauptstadt, aber auch seine eigene Rolle darin. Schon im Januar 1895 reiste Gide erneut nach Nordafrika, diesmal alleine. Seine Aufenthaltsorte waren Algier, Blida und Biskra. In Blida traf er zufällig auf Oscar Wilde und dessen Geliebten Alfred Douglas. Wilde, der zu diesem Zeitpunkt kurz vor seiner Rückkehr nach England stand, die ihn gesellschaftlich vernichten sollte, organisierte während eines gemeinsamen Aufenthaltes in Algier eine sexuelle Begegnung Gides mit einem Jugendlichen (Mohammed, ca. 14-jährig), dem Gide in seiner Autobiographie zentrale Bedeutung beimessen wird: „(...) erst jetzt fand ich endlich zu meiner eigenen Norm.“

Gide hat in seinem Leben eine Vielzahl weiterer Afrikareisen unternommen, schon die Hochzeitsreise führte ihn 1896 nach Nordafrika zurück. Klaus Mann verglich die lebensgeschichtliche Bedeutung der ersten beiden Afrikareisen für Gide, „die Wonne echter Neugeburt“, mit dem Italienerlebnis Goethes. Gide verarbeitete dieses Erlebnis in zentralen Texten seines literarischen Werkes: in lyrischer Prosa in "Les Nourritures terrestres" („Uns nährt die Erde“, 1897), als problemorientierte Erzählung in "L’Immoraliste" („Der Immoralist“, 1902), schließlich autobiographisch in "Si le grain ne meurt" („Stirb und werde“, 1926). Speziell die Autobiographie sorgte zeitgenössisch wegen ihres offenen Bekenntnisses zur Homosexualität für kontroverse Debatten; später galt sie als Meilenstein in der Entwicklung schwulen Selbstbewusstseins in westlichen Gesellschaften. Unterdessen hat sich die Perspektive verändert: die von Gide in "Stirb und werde" aufgeworfenen Fragen: „Im Namen welchen Gottes, welchen Ideals verbietet ihr mir, nach meiner Natur zu leben? Und wohin würde diese Natur mich führen, wenn ich ihr einfach folgte?“ müssen heute mit dem Hinweis auf Missbrauch und Sextourismus beantwortet werden.

Nach der Rückkehr aus Afrika verbrachte Gide zwei Wochen in Gesellschaft seiner Mutter in Paris, bevor diese auf das Gut La Roque abreiste, wo sie am 31. Mai 1895 verstarb. Die spannungsreiche Beziehung zwischen Mutter und Sohn hatte sich zuletzt versöhnlicher gestaltet und Juliette Gide hatte ihren Widerstand gegen eine Eheschließung von Madeleine und André aufgegeben. Nach ihrem Tod verlobte sich das Paar am 17. Juni und heiratete am 7. Oktober 1895 in Cuverville. Gide berichtete später, dass er noch vor der Verlobung einen Arzt aufgesucht habe, um über seine homosexuellen Neigungen zu sprechen. Der Mediziner empfahl die Ehe als Heilmittel („Sie kommen mir vor wie ein Ausgehungerter, der bis heute versucht hat, sich von Essiggurken zu nähren.“). Die Ehe wurde wohl nie vollzogen, Gide trennte radikal zwischen Liebe und sexuellem Verlangen. Die Hochzeitsreise führte in die Schweiz, nach Italien und Nordafrika (Tunis, Biskra). Seine Reisenotizen "Feuilles de route" („Blätter von unterwegs“) sparen die Eheproblematik aus und konzentrieren sich ganz auf die sinnliche Präsenz von Natur und Kultur. Nach ihrer Rückkehr erfuhr Gide im Mai 1896, dass er zum Bürgermeister des Dorfes La Roque-Baignard gewählt worden war. Er übte dieses Amt aus, bis das Gut La Roque im Jahr 1900 verkauft wurde. Die Gides behielten nur Madeleines Erbe Cuverville.

Im Jahr 1897 erschien "Les Nourritures terrestres". Seit der ersten Übersetzung ins Deutsche durch Hans Prinzhorn im Jahr 1930 war dieses Werk unter dem Titel "Uns nährt die Erde" bekannt, die neuere Übertragung durch Hans Hinterhäuser übersetzt "Die Früchte der Erde". Gide hatte seit der ersten Afrikareise an dem Text gearbeitet, dessen erste Fragmente 1896 in der Zeitschrift L’Ermitage publiziert wurden. In einer Mischung aus Lyrik und hymnischer Prosa gibt Gide seinem Befreiungserlebnis Ausdruck: in der Ablehnung des Gegensatzes von Gut und Böse, im Eintreten für Sinnlichkeit und Sexualität in jeder Form, in der Feier des rauschhaften Genusses gegen Reflexion und Rationalität. Mit diesem Werk wandte sich Gide vom Symbolismus und von der Salonkultur des Pariser Fin de Siècle ab. Es verschaffte ihm unter jüngeren Literaten Bewunderung und Gefolgschaft, war zunächst aber kein kommerzieller Erfolg; bis zum Ersten Weltkrieg verkauften sich nur einige Hundert Exemplare. Nach 1918 aber entwickelte sich "Les Nourritures terrestres" „für mehrere Generationen zur Bibel“.

Gide trat in seinen frühen Jahren kaum als politischer Autor hervor. Sein Antinaturalismus erstrebte Kunstwerke, „die außerhalb der Zeit und aller »Kontingenzen« stünden“. In der Dreyfus-Affäre jedoch, die Frankreichs Öffentlichkeit seit 1897/98 spaltete, positionierte er sich klar auf Seiten Emile Zolas und unterzeichnete im Januar 1898 die Petition der Intellektuellen zugunsten eines Revisionsverfahrens für Alfred Dreyfus. Im selben Jahr publizierte er in der Zeitschrift "L'Ermitage" den Artikel "A propos der «Déracinés»", in dem er sich anlässlich der Veröffentlichung des Romans "Les Déracinés" („Die Entwurzelten“) von Maurice Barrès, gegen dessen nationalistische Entwurzelungstheorie wandte. Wieder ganz dem absoluten Kunstideal entsprach dann die 1899 veröffentlichte Erzählung "Le Prométhée mal enchaîné" („Der schlechtgefesselte Prometheus“), die um das Motiv des acte gratuit kreist, einer völlig freien, willkürlichen Handlung.

Gide zeigte früh Interesse am deutschsprachigen Kulturraum. Schon als Schüler begeisterte er sich für Heinrich Heine, dessen Buch der Lieder er im Original las. Im Jahr 1892 führte ihn eine erste Deutschland-Reise nach München. Bald ergaben sich persönliche Kontakte zu deutschsprachigen Autoren, etwa zu dem Symbolisten und Lyriker Karl Gustav Vollmoeller, den er 1898 kennenlernte und noch im selben Jahr in dessen Sommerresidenz in Sorrent besuchte. Durch Vollmöller kam Gide 1904 in Kontakt mit Felix Paul Greve. In diese Zeit fällt auch seine Bekanntschaft mit Franz Blei. Greve und Blei traten als frühe Übersetzer Gides ins Deutsche hervor. Auf Einladung Harry Graf Kesslers besuchte Gide 1903 Weimar und hielt vor der Fürstin Amalie am Weimarer Hof den Vortrag "Über die Wichtigkeit des Publikums". Gide sollte sich zeitlebens, insbesondere nach 1918, für die französisch-deutschen Beziehungen einsetzen. An geistigen Einflüssen sind Goethe und Nietzsche hervorzuheben; mit letzterem setzte sich Gide seit 1898 intensiv auseinander.

Um die Jahrhundertwende wandte sich Gide verstärkt dramatischen Arbeiten zu. Seine szenischen Werke knüpfen an Stoffe der antiken Überlieferung oder biblische Geschichten an. Im Mittelpunkt seiner Ideendramen stehen Figuren, auf die Gide seine eigenen Erfahrungen und Ideen projiziert. In dem für seine dramaturgischen Überlegungen wichtigen Vortrag "De l'évolution du théâtre" („Über die Entwicklung des Theaters“), gehalten 1904 in Brüssel, zitiert Gide Goethe zustimmend: „Für den Dichter ist keine Person historisch, es beliebt ihm, eine sittliche Welt darzustellen und er erweist zu diesem Zweck gewissen Personen aus der Geschichte die Ehre, ihren Namen seinen Geschöpfen zu leihen.“ Der 1898 in der Zeitschrift La Revue blanche publizierte Text "Philoctète ou Le Traité des trois morales" („Philoktet oder der Traktat von den drei Arten der Tugend“) lehnte sich an Sophokles an und hatte den Charakter eines Traktats in dramaturgischer Form; eine Aufführung war weder geplant, noch wurde sie tatsächlich realisiert. Das erste Drama Gides, das die Bühne erreichte, war 1901 "Le roi Candaule" („König Kandaules“), dessen Stoff Herodots Historien und Platons Politeia entnommen war. Das Werk wurde am 9. Mai 1901 in der Regie Aurélien Lugné-Poes in Paris uraufgeführt. In der Übersetzung Franz Bleis kam es schon 1906 zu einer Aufführung im Deutschen Volkstheater in Wien. Im Jahr 1903 publizierte Gide das Drama "Saül" („Saul“), dessen Grundlage das Buch Samuel ist. Das Stück war schon 1898 vollendet, von Gide aber erst veröffentlicht worden, nachdem alle Versuche es zur Aufführung zu bringen, gescheitert waren. Tatsächlich wurde "Saul" erst 1922 von Jacques Copeau im "Théâtre du Vieux-Colombier" in Paris uraufgeführt. Wenngleich Gide nach dieser intensiven Theaterarbeit um 1900 erst 1930 mit "Œdipe" („Oedipus“) wieder ein großes Drama vorlegte, blieb er dem Theater doch immer verbunden. Dies zeigen etwa seine Übersetzungen von William Shakespeares "Hamlet" und "Antonius und Cleopatra" oder von Rabindranath Tagores "Das Postamt"; auch das von Gide verfasste Opernlibretto für "Perséphone", das Igor Strawinsky vertonte, steht für sein theatralisches Interesse. Im Jahr 1913 gehörte Gide selbst zu den Gründern des "Théâtre du Vieux-Colombier".

Gide arbeitete meist an mehreren Werken gleichzeitig, die über Jahre reiften und in einem dialektischen Verhältnis zueinander standen. Dies gilt insbesondere für zwei seiner wichtigsten Erzählungen vor 1914: "L’Immoraliste" („Der Immoralist“, 1902) und "La Porte étroite" („Die enge Pforte“, 1909), die Gide selbst als „Zwilling(e)“ bezeichnete, die „im Wettstreit miteinander in meinem Geist wuchsen“. Die ersten Überlegungen zu beiden Texten, die das Problem der Tugend aus unterschiedlichen Perspektiven behandeln sollten, gehen auf die Zeit nach der Rückkehr von der ersten Afrikareise zurück. Beide Erzählungen sind in den Örtlichkeiten wie in den personellen Konstellationen stark autobiographisch geprägt, weshalb es nahe lag, in Marceline und Michel (im "Immoralist") und in Alissa und Jérôme (in "Die enge Pforte") Madeleine und André Gide zu erkennen. Gide hat diese Gleichsetzung immer zurückgewiesen und das Bild verschiedener Knospen gebraucht, die er als Anlagen in sich trage: Aus einer dieser Knospen konnte die Figur des Michel erwachsen, der sich ganz der absoluten, selbstbezogenen Vitalität hingibt und dafür seine Frau opfert; aus einer anderen Knospe ging die Figur der Alissa hervor, die ihre Liebe der absoluten und blinden Nachfolge Christi opfert. Beide Figuren scheitern in ihrer entgegengesetzten Radikalität, beide verkörpern Anlagen und Versuchungen Gides, die er gerade nicht ins Extrem treibt, sondern – auch hier dem Vorbild Goethes folgend – in sich zu vereinen und auszugleichen sucht.

Im Jahr 1906 erwarb Gide die "Villa Montmorency" in Auteuil, wo er bis 1925 lebte, soweit er sich nicht in Cuverville aufhielt oder auf Reisen war. In dieser Zeit arbeitete Gide hauptsächlich an "Die enge Pforte". Diese Arbeit unterbrach er im Februar und März 1907 und stellte in kurzer Zeit die Erzählung "Le Retour de l’enfant prodigue" („Die Rückkehr des verlorenen Sohnes“) fertig, die noch im Frühjahr 1907 erschien. Die Erzählung greift das biblische Motiv von der Heimkehr des verlorenen Sohns auf, der bei Gide jedoch dem jüngeren Bruder rät, das elterliche Haus ebenfalls zu verlassen und nicht zurückzukommen, d. h. sich definitiv zu emanzipieren. Die Erzählung erschien bereits 1914 in der Übersetzung Rainer Maria Rilkes auf Deutsch und entwickelte sich – gerade in Deutschland – „zu einem Bekenntnisbuch der Jugendbewegung“. Nach Raimund Theis ist "Die Rückkehr des verlorenen Sohnes" „eine der formal geschlossensten, vollendetsten Dichtungen André Gides“. Bald nach der Fertigstellung des "Verlorenen Sohnes", im Juli 1907, besuchte Gide seinen Freund Eugène Rouart auf dessen Gut in Südfrankreich. Dort kam es zu einer Liebesnacht zwischen Gide und dem 17-jährigen Landarbeitersohn Ferdinand Pouzac (1890–1910), die Gide in der kleinen Erzählung "Le Ramier" („Die Ringeltaube“) verarbeitete, die erst 2002 aus seinem Nachlass veröffentlicht wurde.

Um Gide bildete sich seit "Les Nourritures terrestres" ein Kreis jüngerer Literaten, bestehend aus Marcel Drouin, André Ruyters, Henri Ghéon, Jean Schlumberger und Jacques Copeau. Die Gruppe plante, eine literarische Zeitschrift nach ihren Vorstellungen zu gründen, nachdem "L’Ermitage", in der auch Gide seit 1896 publiziert hatte, 1906 eingegangen war. Gemeinsam mit Eugène Montfort bereitete die Gruppe um Gide für November 1908 das erste Heft des neuen Periodikums vor, das den Titel "La Nouvelle Revue française" ("NRF") tragen sollte. Nach Differenzen mit Montfort trennte man sich und die sechs Freunde begründeten die Zeitschrift unter gleichem Namen mit einer neuen ersten Nummer, die im Februar 1909 erschien. Die Zeitschrift konnte schnell namhafte Autoren der Zeit gewinnen, darunter Charles-Louis Philippe, Jean Giraudoux, Paul Claudel, Francis Jammes, Paul Valéry und Jacques Rivière. Der "NRF" wurde 1911 ein eigenes Verlagshaus angegliedert ("Éditions de la NRF"), dessen Leitung der bald einflussreiche Verleger Gaston Gallimard übernahm. Gides Erzählung "Isabelle" erschien 1911 als drittes Buch des neuen Verlages. Über die Zeitschrift und den NRF-Verlag wurde Gide nach 1918 einer der tonangebenden französischen Literaten seiner Epoche, der mit fast allen zeitgenössischen europäischen Autoren von Rang Kontakte pflegte. In die Literaturgeschichte ging die im Jahr 1912 erfolgte Ablehnung des ersten Bandes von Marcel Prousts Roman "Auf der Suche nach der verlorenen Zeit" durch den Verlag ein: Gide trug als Lektor die Hauptverantwortung und begründete sein Votum damit, dass Proust „ein Snob und literarischer Amateur“ sei. Seinen Fehler räumte Gide später Proust gegenüber ein: „Die Ablehnung des Buchs wird der größte Fehler der NRF bleiben und (denn ich schäme mich, weitgehend dafür verantwortlich zu sein) einer der stechendsten Schmerzen und Gewissensbisse meines Lebens.“

Gide teilte seine erzählenden Werke in zwei Gattungen ein: die "Soties" und die "Récits". Zu den "Soties" zählte er "Die Reise Urians", "Paludes" und "Der schlecht gefesselte Prometheus", zu den "Récits" "Der Immoralist", "Die enge Pforte", "Isabelle" und die 1919 erschienene "Pastoralsymphonie". Während die "Sotie" mit satirischen und parodistischen Stilmitteln arbeitet, ist der "Récit" als Erzählung zu charakterisieren, in der eine Figur idealtypisch, aus der Fülle des Lebens herausgelöst, als Fall dargestellt wird. Beide Gattungen grenzte Gide klar von der Totalität des Romans ab, was bedeutete, dass er selbst noch keinen Roman vorgelegt hatte. Auch das 1914 publizierte Werk "Les Caves du Vatican" („Die Verliese des Vatikans“) ließ er trotz der komplexen Handlungsführung nicht als Roman gelten, sondern rechnete es zu seinen "Soties".
Vor dem Hintergrund realer Gerüchte in den 1890er Jahren, Freimaurer hätten Leo XIII. eingesperrt und durch einen falschen Papst ersetzt, entwirft Gide ein Gesellschaftsbild voller Falschheit und Heuchelei, in dem Religion, Wissenschaftsgläubigkeit und bürgerliche Werte durch seine Figuren, allesamt Narren, ad absurdum geführt werden. Nur die schillernde Figur des schönen jungen Kosmopoliten Lafcadio Wluiki, ganz frei und bindungslos, scheint positiv besetzt: – und er begeht einen Mord als „acte gratuit“. "Die Verliese des Vatikans" stießen bei Erscheinen teils auf positive Resonanz, wofür Prousts Begeisterung stehen mag, teils auf heftige Kritik, wofür die Ablehnung Paul Claudels charakteristisch ist. Der mit Gide befreundete Claudel hatte ihn in den Jahren vor 1914 zum Katholizismus bekehren wollen und Gide schien der Konversion phasenweise nahe. Das neue Werk konnte Claudel nun nur als Absage verstehen, zumal sich im Buch auch eine satirisch dargestellte Konversion findet. Claudels noch 1914 in einem Brief an Gide geäußerte Kritik setzte aber bei einer homoerotischen Passage an: „Um Himmels willen, Gide, wie konnten Sie den Absatz schreiben (...). Muss man also in der Tat annehmen – ich habe mich immer geweigert es zu tun –, dass Sie selbst diese entsetzlichen Sitten praktizieren? Antworten Sie mir. Sie müssen antworten.“ Und Gide antwortete in einer Weise, die zeigt, dass er des Doppellebens, das er führte, überdrüssig war (auch wenn das öffentliche Bekenntnis erst nach 1918 folgte): „Niemals habe ich bei einer Frau ein Verlangen gespürt; und die traurigste Erfahrung, die ich in meinem Leben gemacht habe, ist, dass die beständigste, längste, lebendigste Liebe nicht von dem begleitet war, was ihr im Allgemeinen vorausgeht. Im Gegenteil: Liebe schien mir das Verlangen zu verhindern. (...) Ich habe nicht gewählt, so zu sein.“

Nach dem Ausbruch des Ersten Weltkrieges engagierte sich Gide, der nicht zum Militärdienst eingezogen wurde, bis Frühjahr 1916 in der privaten Hilfsorganisation "Foyer Franco-Belge", die sich um Flüchtlinge aus den von deutschen Truppen besetzten Gebieten kümmerte. Dort arbeitete er mit Charles Du Bos und Maria van Rysselberghe, der Frau des belgischen Malers Théo van Rysselberghe, zusammen. Maria (genannt "La Petite Dame") wurde zu einer engen Vertrauten Gides. Seit 1918 erstellte sie ohne sein Wissen Aufzeichnungen über Begegnungen und Gespräche mit ihm, die zwischen 1973 und 1977 unter dem Titel "Les Cahiers de la Petite Dame" publiziert wurden. Mit der Tochter der Rysselberghes, Elisabeth, zeugte Gide 1922 eine Tochter: Catherine Gide (1923–2013), die er vor Madeleine verheimlichte und erst nach deren Tod 1938 offiziell als Tochter anerkannte. Nach dem Verkauf der Villa in Auteuil lebte Gide seit 1926 meist in Paris in einem Haus in der rue Vaneau mit Maria, Elisabeth und Catherine, während sich Madeleine ganz nach Cuverville zurückzog.

Die Jahre 1915/16 wurden durch eine tiefe moralische und religiöse Krise Gides überschattet. Er vermochte nicht, seinen Lebenswandel mit den tief in ihm verankerten religiösen Prägungen in Einklang zu bringen. Zudem erlebte er seit Jahren die Anziehungskraft, die der Katholizismus auf Personen seines engsten Umfeldes ausübte. Dies galt nicht nur für Claudel, sondern auch für Francis Jammes, der schon 1905 konvertiert war. 1915 folgte Henri Gheon, dann Jacques Copeau, Charles Du Bos und Paul-Albert Laurens. In einer Tagebuchnotiz des Jahres 1929 gestand Gide ein: „Ich möchte nicht behaupten, daß ich nicht zu einer bestimmten Zeit meines Lebens ziemlich nahe daran gewesen wäre, zu konvertieren.“ Als Zeugnis seiner Glaubenskämpfe publizierte Gide im Jahr 1922 in kleiner Auflage (70 Exemplare) anonym die Schrift "Numquid et tu...?"; 1926 ließ er eine größere Auflage (2650 Exemplare) unter eigenem Namen folgen.

Gide überwand die Krise durch die Liebe zu Marc Allégret. Er kannte Marc, Jahrgang 1900, von Geburt an, da er ein Freund der Familie war. Marcs Vater, der Pastor Élie Allégret (1865–1940), hatte Gide als Kind unterrichtet und ihn seither wie einen jüngeren Bruder betrachtet. Als er 1916 in die Mission nach Kamerun geschickt wurde, vertraute er Gide die Sorge um seine sechs Kinder an. Dieser verliebte sich in den viertältesten Sohn Marc und ging seit 1917 ein intimes Verhältnis zu ihm ein; erstmals erlebte er die Übereinstimmung von Liebe und sexueller Begierde. Gide und Allégret unternahmen Reisen, 1917 in die Schweiz und 1918 nach England. Die Kehrseite dieses Glücks war die schwere Krise in der Beziehung zu Madeleine. Diese hatte 1916 durch Zufall vom Doppelleben ihres Mannes erfahren und war durch dessen monatelangen Aufenthalt in England zusammen mit Marc so tief verletzt, dass sie alle Briefe, die ihr Gide über 30 Jahre hinweg geschrieben hatte, noch einmal las und dann verbrannte. Als Gide im November 1918 davon erfuhr, war er zutiefst verzweifelt: „Damit verschwindet mein Bestes (...).“ Vor dem Hintergrund dieser emotionalen Wechselfälle entstand 1918 die Erzählung "La Symphonie pastorale" („Die Pastoral-Symphonie“), die 1919 veröffentlicht wurde. Dabei handelte es sich um die Geschichte eines Pastors, der ein blindes Waisenmädchen in seine Familie aufnimmt, sie erzieht, sich in sie verliebt, sie aber an seinen Sohn verliert. Die "Symphonie" war der größte Bucherfolg Gides zu seinen Lebzeiten, mit mehr als einer Million Exemplaren und rund 50 Übersetzungen. Im selben Jahr erschien auch die "Nouvelle Revue française", die seit 1914 eingestellt war, erstmals nach dem Krieg. In dieser Zeit vollzog sich Gides Aufstieg zur zentralen Figur im literarischen Leben Frankreichs: "le contemporain capital", der "bedeutendste Zeitgenosse", wie der Kritiker André Rouveyre schrieb.

In die Jahre nach dem Ersten Weltkrieg fiel Gides zunehmende Rezeption in Deutschland, wofür der Romanist Ernst Robert Curtius von großer Bedeutung war. In seinem 1919 publizierten Werk "Die literarischen Wegbereiter des neuen Frankreich" würdigte er Gide als einen der bedeutendsten französischen Gegenwartsautoren. Zwischen Curtius und Gide entspann sich seit 1920 ein Briefwechsel, der bis zu Gides Tod andauerte. Schon 1921 begegneten sich beide erstmals persönlich im luxemburgischen Colpach. Im örtlichen Schloss veranstaltete das Industriellenehepaar Aline und Emil Mayrisch Treffen französischer und deutscher Intellektueller. Gide kannte Aline Mayrisch, die in der "NRF" über deutsche Literatur schrieb, seit Beginn des Jahrhunderts. Sie und Gide sorgten auch für Curtius' Einladung zum zweiten, regelmäßig stattfindenden Treffen von Intellektuellen, die sich der europäische Verständigung verschrieben hatten: den Dekaden von Pontigny, die Paul Desjardins seit 1910 veranstaltete. Die während des Krieges unterbrochenen Dekaden fanden 1922 erstmals wieder statt und Gide, der schon vor 1914 zugegen war, gehörte zu den regelmäßigen Teilnehmern. Die auf gegenseitigem Respekt beruhende Freundschaft zwischen Gide und Curtius gründete auch auf der gemeinsamen Ablehnung von Nationalismus und Internationalismus; beide befürworteten ein "Europa der Kulturen". Ernst Robert Curtius hat zudem Werke Gides ins Deutsche übersetzt.

Im Jahr 1923 veröffentlichte Gide ein Buch über Dostojewski: "Dostoïevsky. Articles et causeries" („Dostojewski. Aufsätze und Vorträge“). Er hatte sich mit dem russischen Romancier seit 1890 auseinandergesetzt, als er erstmals das Buch "Le Roman russe" von Eugène-Melchior de Vogüé gelesen hatte. Das 1886 publizierte Werk Vogüés hatte die russische Literatur in Frankreich popularisiert und speziell Dostojewski als Repräsentanten einer „Religion des Leidens“ vorgestellt. Gide wandte sich seit der ersten Lektüre gegen diese Interpretation und betonte die psychologischen Qualitäten des russischen Erzählers, der in seinen Figuren die extremsten Möglichkeiten der menschlichen Existenz ausleuchte. Er publizierte einige Artikel über Dostojewski und plante vor 1914 eine Lebensbeschreibung, die er aber nie vorlegte. Aus Anlass des 100. Geburtstages hielt er 1921 einen Vortrag im Theater "Vieux-Colombier", woraus sich eine Vortragsreihe in den Jahren 1921/22 entwickelte. Das 1923 veröffentlichte Werk sammelte ältere Aufsätze und die Vorträge im "Vieux-Colombier", die nicht weiter bearbeitet worden waren. Dass sich sein Blick auf Dostojewski nicht gewandelt hatte, verdeutlichte Gide mit Nietzsches Ausspruch, Dostojewski sei „der einzige Psychologe, von dem ich etwas zu lernen hatte“, den er dem Buch als Motto voranstellte. Die Publikation über Dostojewski ist Gides umfassendste Auseinandersetzung mit einem anderen Schriftsteller, die gerade in die Jahre fiel, in denen er selbst an einer Theorie des Romans arbeitete. Gides Interesse an der russischen Literatur zeigt auch seine 1928 vorgelegte Übertragung der Novellen Puschkins.

Mitte der 1920er Jahre kulminierte Gides Werk in der Veröffentlichung dreier Bücher, an denen er über Jahre gearbeitet hatte und die ihn als öffentliche Person wie auch als Künstler endgültig definierten: 1924 erschien "Corydon. Quatre dialogues socratiques" („Corydon. Vier sokratische Dialoge“), 1926 "Si le grain ne meurt" („Stirb und werde“) und im selben Jahr "Les Faux-Monnayeurs" („Die Falschmünzer“). Mit "Corydon" und "Stirb und werde" outete sich Gide als Homosexueller in der breiten Öffentlichkeit. Für ihn war dies ein Befreiungsschlag, da er sein Privatleben und sein öffentliches Bild zunehmend als falsch und heuchlerisch empfand. Im Entwurf eines Vorworts zu "Stirb und werde" benannte er seine Motivation: „Ich meine, es ist besser, für das, was man ist, gehaßt, als für das, was man nicht ist, geliebt zu werden. Unter der Lüge, so glaube ich, habe ich am meisten in meinem Leben gelitten.“

"Corydon" hatte eine lange Entstehungsgeschichte. Seit 1908 arbeitete Gide an sokratischen Dialogen zum Thema Homosexualität. Im Jahr 1911 ließ er unter dem Titel "C. R. D. N." eine Ausgabe in 22 Exemplaren anonym erscheinen. Zu diesem Zeitpunkt waren die ersten beiden und ein Teil des dritten Dialogs fertiggestellt. Noch vor dem Krieg setzte Gide die Arbeit fort, unterbrach sie in den ersten Kriegsjahren und nahm sie ab 1917 wieder auf. Den entstandenen Text ließ er, wieder anonym, in einer Auflage von nur 21 Exemplaren drucken, die bereits den Titel "Corydon. Vier sokratische Dialoge" trugen. Nach weiteren Bearbeitungen seit 1922 erschien das Buch im Mai 1924 unter Gides Namen, 1932 erstmals auf Deutsch in der Übersetzung von Joachim Moras. "Corydon" kombiniert die fiktionale literarische Form des Dialogs mit nichtfiktionalen Inhalten, über die sich die beiden Gesprächspartner austauschen (Geschichte, Medizin, Literatur usw.). Gides Anliegen war es, die Homosexualität als naturgegeben zu verteidigen, wobei er die Spielart der Päderastie präferierte.

Auch die Entstehung der Autobiografie "Stirb und werde", die Gides Leben bis zur Heirat 1895 darstellt, geht auf die Jahre vor dem Ersten Weltkrieg zurück. Im Jahr 1920 ließ Gide einen Privatdruck (12 Exemplare) des ersten Teils der Memoiren anfertigen, der nur für seine Freunde bestimmt war; 1921 folgten 13 Exemplare des zweiten Teils zum selben Zweck. Die für den Buchhandel bestimmte Ausgabe lag 1925 gedruckt vor, wurde auf Gides Wunsch aber erst im Oktober 1926 ausgeliefert. "Stirb und werde" ist eine Lebensbeschreibung, die nicht dem Muster des organischen Wachstums oder der stufenweisen Entfaltung der Persönlichkeit folgt, sondern einen radikalen Umbruch darstellt: die Zeit vor den Afrikareisen (Teil I) erscheint als dunkle Epoche der Selbstentfremdung, die Erfahrungen in Afrika (Teil II) als Befreiung und Erlösung des eigenen Selbst. Gide war sich bewusst, dass eine derart stilisierte Sinngebung in der eigenen Lebensbeschreibung die Realität vereinfachte und die Widersprüche in seinem Wesen harmonisierte. In einer in die Autobiographie aufgenommenen Selbstreflexion bemerkte er daher: „So sehr man sich auch um Wahrheit bemüht, die Beschreibung des eigenen Lebens bleibt immer nur halb aufrichtig: In Wirklichkeit ist alles viel verwickelter, als es dargestellt wird. Vielleicht kommt man im Roman der Wahrheit sogar näher.“ Mit den "Falschmünzern" löste er diesen Anspruch ein.

Gide bezeichnete "Die Falschmünzer" in der Widmung des Werkes für Roger Martin du Gard als seinen „ersten Roman“ – und nach seiner eigenen Definition des Romans blieb es auch sein einziger. Er hatte die Arbeit an dem Buch im Jahr 1919 aufgenommen und im Juni 1925 abgeschlossen. Anfang 1926 erschien es mit dem auf 1925 datierten Copyright. Parallel zur Entstehung des Romans führte Gide ein Tagebuch, in dem er seine Reflexionen über das sich entwickelnde Werk festhielt. Dieser Text erschien im Oktober 1926 unter dem Titel "Journal des Faux-Monnayeurs" („Tagebuch der Falschmünzer“). "Die Falschmünzer" ist ein sehr kunstvoll angelegter Roman um die Entstehung eines Romans. Die Handlung, die damit beginnt, dass einer der Protagonisten seine außereheliche Zeugung entdeckt, wirkt etwas verwirrend, steht aber auf der Höhe der zeitgenössischen theoretischen und erzähltechnischen Errungenschaften der Gattung Roman, die sich selbst inzwischen zum Problem geworden war. Die "Faux-Monnayeurs" gelten heute als ein richtungweisendes Werk der modernen europäischen Literatur.

Im Jahr 1925 verkaufte Gide seine Villa in Auteuil und ging mit Allégret auf eine fast einjährige Reise durch die damaligen französischen Kolonien Congo (Brazzaville) und Tschad. Die seines Erachtens unhaltbaren ausbeuterischen Zustände dort schilderte er anschließend in Vorträgen und Artikeln sowie in den Büchern "Voyage au Congo" („Kongoreise“) (1927) und "Retour du Tchad" („Rückkehr aus dem Tschad“) (1928), womit er heftige Diskussionen entfachte und viele Angriffe nationalistischer Franzosen auf sich zog.
1929 erschien "L’École des femmes" („Die Schule der Frauen“), die tagebuchartige Geschichte einer Frau, die ihren Mann als starren und seelenlosen Vertreter der bürgerlichen Normen demaskiert und ihn verlässt, um im Krieg Verwundete zu pflegen.

1931 beteiligte sich Gide an der von Jean Cocteau ausgelösten Welle antikisierender Dramen mit dem Stück: "Œdipe" („Ödipus“).

Ab 1932, im Rahmen der wachsenden politischen Polarisierung zwischen links und rechts in Frankreich und ganz Europa, engagierte Gide sich zunehmend auf Seiten der französischen kommunistischen Partei (PCF) und antifaschistischer Organisationen. So reiste er z. B. 1934 nach Berlin, um dort die Freilassung kommunistischer Regimegegner zu verlangen. 1935 gehörte er zur Leitung eines Kongresses antifaschistischer Schriftsteller in Paris, der teilweise verdeckt mit Geldern aus Moskau finanziert wurde. Er verteidigte dabei das Sowjetregime gegen Angriffe von trotzkistischen Delegierten, die die sofortige Freilassung des in der Sowjetunion internierten Schriftstellers Victor Serge verlangten. Auch mäßigte er – zumindest theoretisch – seinen bis dahin vertretenen kompromisslosen Individualismus zugunsten einer Position, die die Rechte des Ganzen und der Anderen vor die des Einzelnen setzt.

Im Juni 1936 reiste er auf Einladung des sowjetischen Schriftstellerverbandes mehrere Wochen durch die UdSSR. Ihn betreute der Vorsitzende der Auslandskommission des Verbandes, der Journalist Michail Kolzow. Am Tag nach der Ankunft Gides starb der Vorsitzende des Schriftstellerverbandes Maxim Gorki. Gide hielt auf dem Lenin-Mausoleum, auf dem auch das Politbüro mit Stalin an der Spitze Aufstellung genommen hatte, eine der Trauerreden. Doch zu der von ihm erhofften Audienz bei Stalin im Kreml kam es nicht. Den Forschungen von Literaturhistorikern zufolge war Stalin über Gides Absichten gut unterrichtet. Dieser hatte vor seiner Abreise dem in Paris als Korrespondent sowjetischer Zeitungen arbeitenden Schriftsteller Ilja Ehrenburg anvertraut: „Ich habe mich entschlossen, die Frage nach seiner Haltung zu meinen Gesinnungsgenossen aufzuwerfen.“ Er wolle Stalin nach der „rechtlichen Lage der Päderasten fragen“, hielt Ehrenburg fest.

Gides Enttäuschung beim Blick hinter die Kulissen der kommunistischen Diktatur war jedoch groß. Seine Eindrücke von dieser Reise, die ihn auch nach Georgien führte, schilderte er in dem kritischen Bericht "Retour de l’U.R.S.S."(„Zurück aus der Sowjetunion“), in dem er sich indes bemühte, Emotionen und Polemik zu vermeiden. Er beschrieb das Sowjetregime als „Diktatur eines Mannes“, die die Ursprungsideen von der „Befreiung des Proletariats“ pervertiert habe. Die sowjetische Presse reagierte mit heftigen Attacken auf ihn, seine Bücher wurden aus allen Bibliotheken des Landes entfernt, eine bereits begonnene mehrbändige Werkausgabe wurde nicht fortgesetzt. Als viele westliche Kommunisten ihn attackierten und ihm vorwarfen, er unterstütze mit seiner Kritik indirekt Hitler, ging Gide vollends auf Distanz zur Partei.

Nach dem Ausbruch des Zweiten Weltkriegs 1939 zog er sich zu Freunden nach Südfrankreich zurück und ging 1942 nach Nordafrika, nachdem er sich von einem passiven Sympathisanten des Regierungschefs des Kollaborationsregimes von Marschall Philippe Pétain zu einem aktiven Helfer der Londoner Exilregierung unter Charles de Gaulle entwickelt hatte. Diese versuchte er z. B. 1944 mit einer Propagandareise durch die westafrikanischen Kolonien zu unterstützen, deren Gouverneure lange zwischen Pétain und de Gaulle schwankten.

1946 publizierte Gide sein letztes größeres Werk, "Thésée" („Theseus“), eine fiktive Autobiografie des antiken Sagenhelden Theseus, in den er sich hineinprojiziert.

In seinen letzten Jahren konnte er noch seinen Ruhm genießen mit Einladungen zu Vorträgen, Ehrendoktorwürden, der Verleihung des Nobelpreises 1947, Interviews, Filmen zu seiner Person u. ä. m. Die Begründung für den Nobelpreis lautet: „für seine weit umfassende und künstlerisch bedeutungsvolle Verfasserschaft, in der Fragen und Verhältnisse der Menschheit mit unerschrockener Wahrheitsliebe und psychologischem Scharfsinn dargestellt werden“.

1939, 1946 und 1950 erschienen seine Tagebücher unter dem Titel "Journal".

1949 erhielt Gide die Goetheplakette der Stadt Frankfurt am Main.

Eine indirekte Anerkennung seiner Bedeutung war, dass 1952 sein Gesamtwerk auf den Index Romanus der katholischen Kirche gesetzt wurde.

Von seiner Tochter im Nachlass entdeckt und herausgegeben, erschien 2002 postum die 1907 entstandene homoerotische Novelle "Le Ramier" (dt. "Die Ringeltaube").







</doc>
<doc id="357" url="https://de.wikipedia.org/wiki?curid=357" title="Aggregatzustand">
Aggregatzustand

Als Aggregatzustände werden die unterschiedlichen Zustände eines Stoffes bezeichnet, die sich durch bloße Änderungen von Temperatur oder Druck ineinander umwandeln können. Es gibt die drei klassischen Aggregatzustände "fest", "flüssig" und "gasförmig" sowie in der Physik weitere nicht klassische Zustände wie z. B. das Plasma.

Die in der Thermodynamik verwendete Phase ist enger gefasst, sie unterteilt insbesondere den festen Zustand nach seiner inneren Struktur.
Welcher Aggregatzustand bzw. welche Phase abhängig von Druck und Temperatur stabil ist, wird in einem Phasendiagramm dargestellt.

Es gibt drei klassische Aggregatzustände:

Für feste Stoffe und flüssige Stoffe gibt es den zusammenfassenden Begriff kondensierte Materie.

Bei Feststoffen unterscheidet man auch nach anderen Merkmalen:

Die klassischen Aggregatzustände lassen sich mit einem Teilchenmodell erklären, das die kleinsten Teilchen eines Stoffes (Atome, Moleküle, Ionen) auf kleine runde Kugeln reduziert.

Die mittlere kinetische Energie aller Teilchen ist in allen Zuständen ein Maß für die Temperatur. Die Art der Bewegung ist in den drei Aggregatzuständen jedoch völlig unterschiedlich. Im Gas bewegen sich die Teilchen geradlinig wie Billardkugeln, bis sie mit einem anderen oder mit der Gefäßwand zusammenstoßen. In der Flüssigkeit müssen sich die Teilchen durch Lücken zwischen ihren Nachbarn hindurchzwängen (Diffusion, Brownsche Molekularbewegung). Im Festkörper schwingen die Teilchen nur um ihre Ruhelage.

Die kleinsten Teilchen sind bei einem Feststoff nur wenig in Bewegung. Sie schwingen um eine feste Position, ihren Gitterplatz, und rotieren meist um ihre Achsen. Je höher die Temperatur wird, desto heftiger schwingen bzw. rotieren sie, und der Abstand zwischen den Teilchen nimmt (meist) zu. Ausnahme: Dichteanomalie.

Hinweis: Betrachtet man die Teilchen mit quantenmechanischen Grundsätzen, so dürfen aufgrund der Heisenbergschen Unschärferelation eigentlich Teilchen nie ruhig stehen. Sie haben kleine Schwingungen, die man auch als Nullpunktsfluktuationen bezeichnet. Das entspricht dem Grundzustand des harmonischen Oszillators.

Zwischen den kleinsten Teilchen wirken verschiedene Kräfte, nämlich die Van-der-Waals-Kräfte, die elektrostatische Kraft zwischen Ionen, Wasserstoffbrückenbindungen oder kovalente Bindungen. Die Art der Kraft ist durch den atomaren Aufbau der Teilchen (Ionen, Moleküle, Dipole …) bestimmt. Bei Stoffen, die auch bei hohen Temperaturen fest sind, ist die Anziehung besonders stark.

Durch die schwache Bewegung und den festen Zusammenhalt sind die Teilchen regelmäßig angeordnet.

Durch die starke Anziehung sind die Teilchen eng beieinander (hohe Packungsdichte)

Die Teilchen sind nicht wie beim Feststoff ortsfest, sondern können sich gegenseitig verschieben. Bei Erhöhung der Temperatur werden die Teilchenbewegungen immer schneller.

Durch die Erwärmung ist die Bewegung der Teilchen so stark, dass die Wechselwirkungskräfte nicht mehr ausreichend sind, um die Teilchen an ihrem Platz zu halten. Die Teilchen können sich nun frei bewegen.

Obwohl der Abstand der Teilchen durch die schnellere Bewegung ein wenig größer wird (die meisten festen Stoffe nehmen beim Schmelzen einen größeren Raum ein), hängen die Teilchen weiter aneinander. Für die Verringerung des Volumens einer Flüssigkeit durch Kompression gilt ähnliches wie bei einem Festkörper, wobei der entsprechende Kompressionsmodul der Flüssigkeit zum Tragen kommt. Bei einer Temperaturverringerung wird das Volumen ebenfalls kleiner, bei Wasser jedoch nur bis zu einer Temperatur von 4 °C (Anomalie des Wassers), während darunter bis 0 °C das Volumen wieder ansteigt.

Obwohl die Teilchen sich ständig neu anordnen und Zitter-/Rotationsbewegungen durchführen, kann eine Anordnung festgestellt werden. Diese Nahordnung ist ähnlich wie im amorphen Festkörper, die Viskosität ist jedoch sehr viel niedriger, d. h. die Teilchen sind beweglicher.

Bei Stoffen im gasförmigen Zustand sind die Teilchen schnell in Bewegung. Ein Gas oder gasförmiger Stoff verteilt sich schnell in einem Raum. In einem geschlossenen Raum führt das Stoßen der kleinsten Teilchen gegen die Wände zum Druck des Gases.

Beim gasförmigen Zustand ist die Bewegungsenergie der kleinsten Teilchen so hoch, dass sie nicht mehr zusammenhalten. Die kleinsten Teilchen des gasförmigen Stoffes verteilen sich gleichmäßig im gesamten zur Verfügung stehenden Raum.

Durch die schnelle Bewegung der Teilchen in einem Gas sind sie weit voneinander entfernt. Sie stoßen nur hin und wieder einander an, bleiben aber im Vergleich zur flüssigen Phase auf großer Distanz. Ein gasförmiger Stoff lässt sich komprimieren, d. h. das Volumen lässt sich verringern.

Wegen der Bewegung sind die Teilchen ungeordnet.

In der physikalischen Chemie unterscheidet man zwischen Dampf und Gas. Beide sind physikalisch gesehen nichts anderes als der gasförmige Aggregatzustand; die Begriffe haben auch nicht direkt mit realem Gas und idealem Gas zu tun. Was "umgangssprachlich" als „Dampf“ bezeichnet wird, ist physikalisch gesehen eine Mischung aus flüssigen und gasförmigen Bestandteilen, welche man im Falle des Wassers als Nassdampf bezeichnet.

Bei einem Dampf im engeren Sinn handelt es sich um einen Gleichgewichtszustand zwischen flüssiger und gasförmiger Phase. Er kann, ohne Arbeit verrichten zu müssen, verflüssigt werden, das heißt beim Verflüssigen erfolgt kein Druckanstieg. Ein solcher Dampf wird in der Technik als Nassdampf bezeichnet im Gegensatz zum sogenannten Heißdampf oder überhitzten Dampf, der im eigentlichen Sinn ein reales Gas aus Wassermolekülen darstellt und dessen Temperatur oberhalb der Kondensationstemperatur der flüssigen Phase beim jeweiligen Druck liegt.

Reinstoffe werden entsprechend ihrem Aggregatzustand bei einer Temperatur von 20 °C (siehe Raumtemperatur) und einem Druck von 1013,25 hPa (Normaldruck) als Feststoff, Flüssigkeit oder Gas bezeichnet. Beispiel: Brom ist bei Raumtemperatur und Normaldruck flüssig (siehe Tabelle), also gilt Brom als Flüssigkeit.

Diese Bezeichnungen (Feststoff, Flüssigkeit, Gas) werden zwar auch gebraucht, wenn Stoffe unter veränderten Bedingungen einen anderen Aggregatzustand annehmen. Im engeren Sinne bezieht sich die Einteilung jedoch auf die oben genannten Standardbedingungen; jeder Stoff gehört dann zu einer der Kategorien.

Bei der Vermischung von Stoffen ergeben sich abhängig vom Aggregatzustand der Bestandteile und ihrem mengenmäßigen Anteil charakteristische Gemische, zum Beispiel Nebel oder Schaum.

Die Übergänge zwischen den verschiedenen Aggregatzuständen haben spezielle Namen (eoc, omc, eon) und spezielle Übergangsbedingungen, die bei Reinstoffen aus Druck und Temperatur bestehen. Diese Übergangsbedingungen entsprechen dabei Punkten auf den Phasengrenzlinien von Phasendiagrammen. Hierbei ist für jeden Phasenübergang eine bestimmte Wärmemenge notwendig bzw. wird dabei freigesetzt.

Die Sublimation und das Verdampfen kommen auch unterhalb der Sublimations- beziehungsweise Siedepunktes vor. Man spricht hier von einer Verdunstung.

Alle Übergänge können am Beispiel Wasser im Alltag beobachtet werden (siehe Abbildung):

Schnee oder Eis fängt im Frühjahr an flüssig zu werden, sobald Temperaturen oberhalb der Schmelztemperatur herrschen.

Kühlt das Wasser in Seen unter den Gefrierpunkt ab, bilden sich Eiskristalle, die mit der Zeit immer größer werden, bis die Oberfläche mit einer Eisschicht überzogen ist.

Wird Wasser im Kochtopf über seine Siedetemperatur erhitzt, so wird das Wasser gasförmig. Das „Blubbern“ im Kochtopf kommt zustande, weil das Wasser am heißen Topfboden zuerst die Siedetemperatur erreicht - Die aufsteigenden Blasen sind der Wasserdampf, der (wie die meisten gasförmigen Stoffe) unsichtbar ist. "Verdunstung", der Übergang von flüssig in gasförmig ohne Erreichen der Siedetemperatur, ist bei Schweiß auf der Haut gut zu beobachten.

Der deutlich sichtbare Nebel oberhalb kochenden Wassers, der meist umgangssprachlich als „Dampf“ bezeichnet wird, ist zu winzigen Wassertröpfchen kondensierter Wasserdampf. Tau und Wolken entstehen ebenfalls durch kondensierenden Wasserdampf.

Gefrorene Pfützen können im Winter, auch bei Temperaturen weit unterhalb des Gefrierpunktes, durch Sublimation nach und nach „austrocknen“, bis das Eis vollständig sublimiert und die Pfütze verschwunden ist.

Raureif oder Eisblumen, die sich im Winter bilden, entstehen durch den aus der Umgebungsluft resublimierenden Wasserdampf.

Durch Erhöhen der Temperatur (Zufuhr von thermischer Energie) bewegen sich die kleinsten Teilchen immer heftiger, und ihr Abstand voneinander wird (normalerweise) immer größer. Die Van-der-Waals-Kräfte halten sie aber noch in ihrer Position, ihrem Gitterplatz. Erst ab der sogenannten Schmelztemperatur wird die Schwingungsamplitude der Teilchen so groß, dass die Gitterstruktur teilweise zusammenbricht. Es entstehen Gruppen von Teilchen, die sich frei bewegen können. In ihnen herrscht eine Nahordnung, im Gegensatz zur Fernordnung von Teilchen innerhalb des Kristallgitters fester Stoffe.

Mit Sinken der Temperatur nimmt die Bewegung der Teilchen ab, und ihr Abstand zueinander wird immer geringer. Auch die Rotationsenergie nimmt ab. Bei der sogenannten Erstarrungstemperatur wird der Abstand so klein, dass sich die Teilchen gegenseitig blockieren und miteinander verstärkt anziehend wechselwirken – sie nehmen eine feste Position in einem dreidimensionalen Gitter ein.

Es gibt Flüssigkeiten, die sich bei sinkender Temperatur ausdehnen, beispielsweise Wasser. Dieses Verhalten wird als Dichteanomalie bezeichnet.

Die Geschwindigkeit der kleinsten Teilchen ist nicht gleich. Ein Teil ist schneller, ein Teil ist langsamer als der Durchschnitt. Dabei ändern die Teilchen durch Kollisionen ständig ihre aktuelle Geschwindigkeit.

An der Grenze eines Festkörpers oder einer Flüssigkeit, dem Übergang einer Phase in eine gasförmige, kann es mitunter vorkommen, dass ein Teilchen von seinen Nachbarn zufällig einen so starken Impuls bekommt, dass es aus dem Einflussbereich der Kohäsionskraft entweicht. Dieses Teilchen tritt dann in den gasförmigen Zustand über und nimmt etwas Wärmeenergie in Form der Bewegungsenergie mit, das heißt die feste oder flüssige Phase kühlt ein wenig ab.

Wird thermische Energie einem System zugeführt und erreicht die Temperatur die Sublimations- oder Siedetemperatur, geschieht dieser Vorgang kontinuierlich, bis alle kleinsten Teilchen in die gasförmige Phase übergetreten sind. In diesem Fall bleibt die Temperatur in der verdampfenden Phase in der Regel unverändert, bis alle Teilchen mit einer höheren Temperatur aus dem System verschwunden sind. Die Wärmezufuhr wird somit in eine Erhöhung der Entropie umgesetzt.

Wenn die Kohäsionskräfte sehr stark sind, beziehungsweise es sich eigentlich um eine viel stärkere Metall- oder Ionenbindung handelt, dann kommt es nicht zur Verdampfung.

Die durch Verdampfen starke Volumenzunahme eines Stoffes kann, wenn sehr viel Hitze schlagartig zugeführt wird, zu einer Physikalischen Explosion führen.

Der umgekehrte Vorgang ist die Kondensation beziehungsweise Resublimation. Ein kleinstes Teilchen trifft zufällig auf einen festen oder flüssigen Stoff, überträgt seinen Impuls und wird von den Kohäsionskräften festgehalten. Dadurch erwärmt sich der Körper um die Energie, die das kleinste Teilchen mehr trug als der Durchschnitt der kleinsten Teilchen in der festen beziehungsweise flüssigen Phase.

Stammt das Teilchen allerdings von einem Stoff, der bei dieser Temperatur gasförmig ist, sind die Kohäsionskräfte zu schwach, es festzuhalten. Selbst wenn es zufällig so viel Energie verloren hat, dass es gebunden wird, schleudert es die nächste Kollision mit benachbarten kleinsten Teilchen wieder in die Gasphase. Durch Absenken der Temperatur kann man den kleinsten Teilchen ihre Energie entziehen. Dadurch ballen sie sich beim Unterschreiten der Sublimations- oder Erstarrungstemperatur durch die Wechselwirkungskräfte mit anderen Teilchen zusammen und bilden wieder einen Feststoff oder eine Flüssigkeit.

Das p-T-Phasendiagramm eines Stoffes beschreibt in Abhängigkeit von Druck und Temperatur, in wie vielen Phasen ein Stoff vorliegt und in welchem Aggregatzustand sich diese befinden. Anhand der Linien kann man also erkennen, bei welchem Druck und welcher Temperatur die Stoffe ihren Aggregatzustand verändern. Gewissermaßen findet auf den Linien der Übergang zwischen den Aggregatzuständen statt, weshalb man diese auch als Phasengrenzlinien bezeichnet. Auf ihnen selbst liegen die jeweiligen Aggregatzustände in Form eines dynamischen Gleichgewichts nebeneinander in verschiedenen Phasen vor.






Neben den drei klassischen Aggregatzuständen gibt es weitere Materiezustände, die zum Teil nur unter extremen Bedingungen auftreten (nach Temperatur, tendenziell von hoher zu niedriger, sortiert).





</doc>
<doc id="358" url="https://de.wikipedia.org/wiki?curid=358" title="Arbeitsmarkt">
Arbeitsmarkt

Arbeitsmarkt ist ein Markt, an dem die Nachfrage nach Arbeitskräften mit dem Angebot von Arbeitskräften zusammentrifft. 

Er setzt eine Klasse von Menschen voraus, die ihren Lebensunterhalt nicht über eigene Produktionsmittel (Boden und Kapital) sichern können und deswegen für andere an deren Produktionsmitteln arbeiten müssen (Lohnarbeiter). Eine Masse solcher Menschen — das sogenannte Industrieproletariat — entstand in der europäischen Neuzeit im Zuge der Bevölkerungsexplosion, der Bauernbefreiungen und der industriellen Revolution. Das damit entstandene Problem der Arbeitslosigkeit (Einkommenslosigkeit und Armut mangels eigener Produktionsmittel und mangels einer Person, die den eigentums- und damit arbeitslosen Menschen für sich arbeiten lassen will) bildete einen der wichtigsten Aspekte der "sozialen Frage" (Pauperismus) und stellt eines der wichtigsten Strukturmerkmale der europäischen ("westlichen") Neuzeit dar. 

Während nach neoklassischer Sicht der Arbeitsmarkt wie ein Gütermarkt funktioniert, unterscheidet er sich nach institutionalistischer und arbeitsökonomischer Sicht in charakteristischer Weise vom Gütermarkt. Für Robert M. Solow ist „Arbeit als Ware etwas Besonderes […] und daher auch der Arbeitsmarkt“. Auch die keynesianische Kritik an der Neoklassik sieht dies so (siehe Arbeitsmarktpolitik).

Auf dem Arbeitsmarkt wird Arbeitskraft in Zeiteinheiten und Qualifikationen angeboten und nachgefragt. Menschen, die über ihre Arbeitskraft persönlich frei verfügen können, verkaufen (korrekter: vermieten) gegen Arbeitsentgelt ihre Arbeitskraft zur Verrichtung produktiver Tätigkeiten an Arbeitgeber, unter deren Anleitung sie Güter herstellen oder Dienstleistungen ausführen, in Kombination mit (meist) von den Arbeitgebern zur Verfügung gestellten Rohstoffen und Produktionsmitteln.

Der Arbeitsmarkt ist kein Markt für Arbeits"leistungen"; Arbeitsergebnisse sind Gegenstand von Werkverträgen. Ähnlich wie Ärzte werden auch Arbeitnehmer für ihre „Bemühungen“ bezahlt und nicht für deren Erfolg. Der Arbeitsvertrag ist ein Vertrag sui generis. 

Die Besonderheit der „Ware Arbeitskraft“ besteht darin, dass sie unauflöslich an Menschen als Träger dieser Ware gebunden ist. Insofern ist eine Verfügung über diese Ware immer auch eine Verfügung über ihren Träger, dessen Menschenwürde beachtet werden muss. Das für Sachen charakteristische „ius utendi et abutendi“, das Recht, eine Sache zu gebrauchen, aber auch zu missbrauchen, ist auf Tiere und Menschen nur sehr begrenzt anwendbar. So haben Arbeitnehmer insbesondere ein Recht auf Freizeit, über deren Gestaltung der Arbeitgeber nur sehr bedingt Mitspracherechte hat, und auf Freizügigkeit. 

Die "Arbeitsnachfrage" lässt sich im Zusammenhang mit dem Grenzprodukt der Arbeit (1. Ableitung der Produktionsfunktion) errechnen (siehe hier). Der Marktpreis für die Arbeitskraft eines bestimmten Arbeitnehmers kann unter seinem Existenzminimum liegen. In diesem Fall besteht eine Pflicht eines Staates, der sich als Sozialstaat versteht, darin zu verhindern, dass die betreffende Person ein Einkommen (einschließlich Transferleistungen) unterhalb ihres Existenzminimums erzielt.

Der Zusammenschluss der Arbeitnehmer zu Gewerkschaften und das Arbeitsrecht als Schutzrecht für die Arbeitnehmer sind als Konsequenzen einer unterstellten „Macht-Asymmetrie“ (Claus Offe) auf den Arbeitsmärkten und des Charakters des Arbeitsverhältnisses als „Herrschaftsverhältnis“ (Max Weber) zu verstehen. Diese Theorie beruht auf der Prämisse, dass Arbeitsmärkte in der Regel Käufermärkte seien, d. h. dass eine hohe Zahl an Arbeitswilligen mit einer beschränkten Zahl an Arbeitsplätzen konfrontiert werde, was ohne Regulierungen wie Tarifentgelte oder einen gesetzlichen Mindestlohn zwangsläufig zu niedrigen Arbeitsentgelten führen würde.

Es wird unterschieden zwischen dem

Der Arbeitsmarkt entwickelte sich im Zuge der fortschreitenden Arbeitsteilung.

Wichtige Kennzahlen des Arbeitsmarktes sind die Erwerbsquote sowie die Arbeitslosenquote. Sie wird oft regional oder nach Wirtschaftssektoren getrennt dargestellt.

Man kann den Arbeitsmarkt für Analysezwecke unterschiedlich strukturieren:

Die volkswirtschaftliche Statistik der Bundesrepublik unterscheidet zwischen so genannten

Im Standardmodell der neoklassischen Theorie lässt sich der Arbeitsmarkt wie auf einem Gütermarkt durch steigende Angebotskurven und fallende Nachfragekurven charakterisieren: Je höher der Lohn, desto höher ist das Arbeitskraftangebot und desto geringer die Arbeitskraftnachfrage. Hierbei wird ein repräsentativer Akteur unterstellt, was auf sehr einfache Weise die Übertragung einzelwirtschaftlicher Beobachtungen auf die gesamtwirtschaftliche Analyse ermöglicht. Die dem Modell zugrunde liegende Annahme vollkommener Markttransparenz sowie die Unterstellung des Faktors Arbeit als homogen schränken seine Anwendbarkeit aus Sicht moderner Theorien des Arbeitsmarktes allerdings ein.

Die klassische Lehre nimmt Löhne als flexibel an und erklärt dadurch eine Markträumung. Arbeitslosigkeit existiert in dieser Betrachtungsweise nicht. In der Realität sind Löhne allerdings nicht flexibel, denn sie werden in der Regel tariflich auf einen Zeitraum bestimmt. Tatsächlich sind sie nach unten sogar meist starr.

Weitere Arbeitsmarkttheorien:

Zu internen Arbeitsmärkten:

Es ist in der deutschen Sprache üblich, denjenigen, der die Arbeit gibt (verrichtet), den Arbeitnehmer zu nennen, während der, der die Arbeit nimmt (Arbeitsleistung entgegennimmt), Arbeitgeber genannt wird.

Die Dienstleistungen, die auf dem Arbeitsmarkt gehandelt werden, unterscheiden sich von anderen Dienstleistungen (z. B. einem Haarschnitt beim Friseur) vor allem in diesen Punkten:

Seit 2005 werden auf dem Arbeitsmarkt drei Arbeitsverhältnisse unterschieden:
Dazu abgestuft werden entsprechend Sozialversicherungsbeiträge und Steuern eingezogen. Die Neuregelung beruht auf dem Hartz-Konzept und soll die Zahl der Arbeitsverhältnisse erhöhen.

Arbeitsmarkt- und Berufsforschung befasst sich mit der theoretischen und empirischen Untersuchung von Arbeitsmarkt, Berufsgruppen- und Branchenentwicklung etc. in wirtschaftlichen und sozialen Zusammenhängen. Für diese Disziplin wurde 1968 an der damaligen Bundesagentur für Arbeit das Institut für Arbeitsmarkt- und Berufsforschung gegründet. Hier wird das Forschungsfeld interdisziplinär von Soziologen, Ökonomen und Ökonometrikern untersucht.

Die Forschung unterscheidet zwischen Ländern mit liberalem (Bsp. USA), konservativem (Bsp. Bundesrepublik Deutschland) und sozialdemokratischem (Bsp. Schweden) Wohlfahrtsstaatsmodell und deren spezifischen Auswirkungen auf den Arbeitsmarkt. Analysiert man diese Modelle z. B. anhand ihrer Auswirkungen auf das Geschlechterverhältnis im Arbeitsmarkt, ergibt sich folgendes Bild: Im liberalen Modell findet eine allgemein positive Entwicklung der Geschlechtergleichheit auf dem Arbeitsmarkt weitgehend zu Lasten gering verdienender Frauen statt. Im konservativen Modell ist v. a. eine hohe vertikale Segregation – d. h. geringe Aufstiegschancen von Frauen – zu beobachten. Das sozialdemokratische Modell produziert im Gegenzug eine starke horizontale Segregation, also eine Teilung des Arbeitsmarktes in spezifische Frauen- und Männerberufe.





</doc>
<doc id="360" url="https://de.wikipedia.org/wiki?curid=360" title="Assemblersprache">
Assemblersprache

Eine Assemblersprache (von ; kurz auch „Assembler“) ist eine hardwarenahe Programmiersprache. Für verschiedene Computertypen gibt es jeweils eine spezielle, auf den Befehlssatz des Prozessors, Mikrocontrollers, digitalen Signalprozessors oder anderweitig programmierbaren Geräts zugeschnittene Assemblersprache. Von den Maschinensprachen unterscheiden sie sich dadurch, dass anstelle eines für den Menschen schwer verständlichen Binärcodes die Befehle und deren Operanden durch leichter verständliche mnemonische Symbole in Textform (z. B. „MOVE“), Operanden (u. a. auch) als symbolische Adressen (z. B. „PLZ“), notiert und dargestellt werden.

Assemblersprachen bezeichnet man – als Nachfolger der direkten Programmierung mit Zahlencodes – als Programmiersprachen der zweiten Generation. Ihre Befehle werden mit einer Übersetzungssoftware (Assembler) direkt und in der Regel 1:1 in Maschinenbefehle übersetzt, in höheren Programmiersprachen (Hochsprache, dritte Generation) hingegen übersetzt ein Compiler komplexere Programmanweisungen in mehrere Maschinenbefehle.

Im Sprachgebrauch werden die Ausdrücke „Maschinensprache“ und „Assembler(sprache)“ häufig synonym verwendet.

Ein Quelltext in Assemblersprache wird auch als "Assemblercode" oder auch einfach als "Assembler" (üblich bis in die 1990er) bezeichnet. Er wird durch einen sogenannten Assembler in direkt ausführbare Maschinensprache (auch "Maschinencode") umgewandelt.

Programme in Assemblersprachen zeichnen sich dadurch aus, dass alle Verarbeitungsmöglichkeiten des Mikroprozessors genutzt und die Hardwarekomponenten direkt angesteuert werden können.
Die Nutzung von Assemblersprachen ist heutzutage selten erforderlich. Sie finden im Allgemeinen nur noch dann Anwendung, wenn Programme bzw. einzelne Teile davon sehr zeitkritisch (z. B. beim Hochleistungsrechnen oder bei Echtzeitsystemen) oder hardwarenah (z. B. Gerätetreiber) sind. Ihre Nutzung kann auch dann sinnvoll sein, wenn für die Programme nur sehr wenig Speicherplatz zur Verfügung steht (z. B. in eingebetteten Systemen). Außerdem können sie verwendet werden, wenn noch keine Hochsprachbibliotheken existieren, z. B. bei neu herausgebrachter Technik.

Unter dem Aspekt der Geschwindigkeitsoptimierung kann der Einsatz von Assemblercode auch bei verfügbaren hochoptimierenden Compilern noch seine Berechtigung haben, muss aber differenziert betrachtet und für die spezifische Anwendung abgewogen werden.
Bei komplexer Technik wie Intel Itanium und verschiedenen digitalen Signalprozessoren kann ein Compiler u. U. durchaus besseren Code erzeugen als ein durchschnittlicher Assemblerprogrammierer, da das Ablaufverhalten solcher Architekturen mit komplexen mehrstufigen intelligenten Optimierungen (z. B. Out-of-order execution, Pipeline-Stalls, …) hochgradig nichtlinear ist.
Die Geschwindigkeitsoptimierung wird immer komplexer, da zahlreiche Nebenbedingungen eingehalten werden müssen. Dies ist ein gleichermaßen wachsendes Problem für die immer besser werdenden Compiler der Hochsprachen als auch für Programmierer der Assemblersprache. Für einen optimalen Code wird immer mehr Kontextwissen benötigt (z. B. Cachenutzung, räumliche und zeitliche Lokalität der Speicherzugriffe), welches der Assemblerprogrammierer teilweise (im Gegensatz zum Compiler) durch Laufzeitprofiling des ausgeführten Codes in seinem angestrebten Anwendungsfeld gewinnen kann. Ein Beispiel hierfür ist der SSE-Befehl MOVNTQ, welcher wegen des fehlenden Kontextwissens von Compilern kaum optimal eingesetzt werden kann.

Die Rückwandlung von Maschinencode in menschenlesbare Assemblersprache wird Disassemblierung genannt.
Wenn die ausführbare Datei keine Debug-Informationen enthält, lassen sich zusätzliche Informationen wie ursprüngliche Bezeichner oder Kommentare nicht wiederherstellen, da diese beim Assemblieren nicht in den Maschinencode übernommen wurden.

Programmbefehle in Maschinensprache bilden sich aus dem Opcode und meist weiteren, je nach Befehl individuell festgelegten Angaben wie Adressen, im Befehl eingebettete Literale, Längenangaben etc. Da die Zahlenwerte der Opcodes schwierig zu merken sind, verwenden Assemblersprachen leichter merkbare Kürzel, sogenannte "mnemonische Symbole" (kurz "Mnemonics").

Beispiel:

Der folgende Befehl in der Maschinensprache von x86-Prozessoren

entspricht dem Assemblerbefehl
movb $0x61, %al ; AT&T-Syntax (alles nach „;“ ist Kommentar)

bzw.

mov al, 61h ; Intel-Syntax; das ‚mov‘ als mnemotechnischem Kürzel erkennt
und bedeutet, dass der hexadezimale Wert „61“ (dezimal 97) in den niederwertigen Teil des Registers „ax“ geladen wird; „ax“ bezeichnet das ganze Register, „al“ (für low) den niederwertigen Teil des Registers. Der hochwertige Teil des Registers kann mit „ah“ angesprochen werden (für „high“).

Am Beispiel ist zu erkennen, dass – obwohl in denselben Maschinencode übersetzt wird – die beiden Assembler-Dialekte deutlich verschieden formulieren.

Mit Computerhilfe kann man das eine in das andere weitgehend eins zu eins übersetzen. Jedoch werden Adressumformungen vorgenommen, so dass man symbolische Adressen benutzen kann. Im Allgemeinen haben die Assembler neben den eigentlichen Codes auch Steueranweisungen, die die Programmierung bequemer machen, zum Beispiel zur Definition eines Basisregisters.

Häufig werden komplexere Assemblersprachen (Makroassembler) verwendet, um die Programmierarbeit zu erleichtern. Makros sind dabei im Quelltext enthaltene Aufrufe, die vor dem eigentlichen Assemblieren automatisch durch (meist kurze) Folgen von Assemblerbefehlen ersetzt werden. Dabei können einfache, durch Parameter steuerbare Ersetzungen vorgenommen werden. Die Disassemblierung von derart generiertem Code ergibt allerdings den reinen Assemblercode ohne die beim Übersetzen expandierten Makros.

Ein sehr einfaches Programm, das zu Demonstrationszwecken häufig benutzte "Hallo-Welt"-Beispielprogramm, kann zum Beispiel in der Assemblersprache MASM für MS-DOS aus folgendem Assemblercode bestehen:

DATA SEGMENT ;- Beginn des Datensegments
Meldung db "Hallo Welt" ;- Die Zeichenkette „Hallo Welt“
DATA ENDS ;- Ende des Datensegments
CODE SEGMENT ;- Beginn des Codesegments
ASSUME CS:CODE,DS:DATA ;- dem Assembler die vorgesehenen Segmente und Segmentregister mitteilen
Anfang: ;- Einsprung-Label fuer den Anfang des Programms
CODE ENDS ;- Ende des Codesegments
END Anfang ;- dem Assembler- und Linkprogramm den Programm-Einsprunglabel mitteilen
Vergleichende Gegenüberstellungen für das "Hallo-Welt"-Programm in unterschiedlichen Assemblerdialekten enthält diese Liste.

In einem Pascal-Quelltext (eine Hochsprache) kann der Programmcode für codice_1 dagegen deutlich kürzer sein:
program Hallo;
begin
end.

Jede Computerarchitektur hat ihre eigene Maschinensprache und damit Assemblersprache. Mitunter existieren auch mehrere Assemblersprachen-Dialekte („verschiedene Assemblersprachen“, sowie zugehörige Assembler) für die gleiche Prozessorarchitektur. Die Sprachen verschiedener Architekturen unterscheiden sich in Anzahl und Typ der Operationen.

Jedoch haben alle Architekturen die folgenden grundlegenden Operationen:

Bestimmte Rechnerarchitekturen haben oft auch komplexere Befehle (CISC) wie z. B.:

Der erste Assembler wurde zwischen 1948 und 1950 von Nathaniel Rochester für eine IBM 701 geschrieben.

Früher wurden Betriebssysteme in einer Assemblersprache geschrieben. Heute werden jedoch Hochsprachen bevorzugt, in den meisten Fällen C. Allerdings müssen häufig kleine Assemblerroutinen hardwarenahe Aufgaben in Betriebssystemen übernehmen. Dazu gehört zum Beispiel das Zwischenspeichern von Registern bei Prozesswechsel (siehe Scheduler), oder bei der x86-Architektur der Teil des Boot-Loaders, der innerhalb des 512 Byte großen Master Boot Records untergebracht sein muss. Auch Teile von Gerätetreibern werden in Assemblersprache geschrieben, falls aus den Hochsprachen kein effizienter Hardware-Zugriff möglich ist. Manche Hochsprachencompiler erlauben es, direkt im eigentlichen Quellcode Assemblercode, sogenannte Inline-Assembler, einzubetten.

Bis ca. 1990 wurden die meisten Computerspiele in Assemblersprachen programmiert, da nur so auf Heimcomputern und den damaligen Spielkonsolen eine akzeptable Spielgeschwindigkeit und eine den kleinen Speicher dieser Systeme nicht sprengende Programmgröße zu erzielen war. Noch heute gehören Computerspiele zu den Programmen, bei denen am ehesten kleinere assemblersprachliche Programmteile zum Einsatz kommen, um so Prozessorerweiterungen wie SSE zu nutzen.

Bei vielen Anwendungen für Geräte, die von Mikrocontrollern gesteuert sind, war früher oft eine Programmierung in Assembler notwendig, um die knappen Ressourcen dieser Mikrocontroller optimal auszunutzen. Um Assemblercode für solche Mikrocontroller zu Maschinencode zu übersetzen, werden Cross-Assembler bei der Entwicklung eingesetzt.
Heute sind Mikrocontroller so günstig und leistungsfähig, dass moderne C-Compiler auch in diesem Bereich die Assembler weitgehend abgelöst haben. Nicht zuletzt aufgrund größerer Programmspeicher bei geringen Aufpreisen für die Chips fallen die Vorteile von Hochsprachen gegenüber den teils geringen Vorteilen der Assemblersprache immer mehr ins Gewicht.

Assemblerprogramme sind sehr "hardwarenah" geschrieben, da sie direkt die unterschiedlichen Spezifikationen und Befehlssätze der einzelnen Computerarchitekturen abbilden. Daher kann ein Assemblerprogramm i. A. nicht auf ein anderes Computersystem übertragen werden, ohne dass der Quelltext angepasst wird. Das erfordert, abhängig von den Unterschieden der Assemblerssprachen, hohen Umstellungsaufwand, unter Umständen ist ein komplettes Neuschreiben des Programmtextes erforderlich. Im Gegensatz dazu muss bei Hochsprachen oft nur ein Compiler für die neue Zielplattform verwendet werden.

Quelltexte in Assemblersprache sind fast immer "deutlich länger" als in einer Hochsprache, da die Instruktionen weniger komplex sind und deshalb gewisse Funktionen/Operationen mehrere Assemblerbefehle erfordern; z. B. müssen beim logischen Vergleich von Daten (= > < …) ungleiche Datenformate oder -Längen zunächst angeglichen werden. Die dadurch "größere Befehlsanzahl" erhöht das Risiko, unübersichtlichen, schlecht strukturierten und schlecht wartbaren Programmcode herzustellen.

Nach wie vor dient Assembler zur Mikro-Optimierung von Berechnungen, für die der Hochsprachencompiler nicht ausreichend effizienten Code generiert. In solchen Fällen können Berechnungen effizienter direkt in Assembler programmiert werden. Beispielsweise sind im Bereich des wissenschaftlichen Rechnens die schnellsten Varianten mathematischer Bibliotheken wie BLAS oder bei architekturabhängigen Funktionen wie der C-Standardfunktion codice_3 weiterhin die mit Assembler-Code. Auch lassen sich gewisse, sehr systemnahe Operationen unter Umgehung des Betriebssystems (z. B. direktes Schreiben in den Bildschirmspeicher) nicht in allen Hochsprachen ausführen.

Der Nutzen von Assembler liegt auch im Verständnis der Arbeits- und Funktionsweise eines Systems, das durch Konstrukte in Hochsprachen versteckt wird. Auch heute noch wird an vielen Hochschulen Assembler gelehrt, um ein Verständnis für die Rechnerarchitektur und seine Arbeitsweise zu bekommen.

Die meisten Hochsprachencompiler übersetzen zuerst in Assemblercode oder können diesen optional ausgeben, so dass man Details genauer betrachten und gewisse Stellen von Hand optimieren kann.





</doc>
<doc id="368" url="https://de.wikipedia.org/wiki?curid=368" title="Analog-Digital-Umsetzer">
Analog-Digital-Umsetzer

Ein Analog-Digital-Umsetzer ist ein elektronisches Gerät, Bauelement oder Teil eines Bauelements zur Umsetzung analoger Eingangssignale in einen digitalen Datenstrom, der dann weiterverarbeitet oder gespeichert werden kann. Weitere Namen und Abkürzungen sind ADU, Analog-Digital-Wandler oder A/D-Wandler, ("analog-to-digital converter") oder kurz A/D.

Eine Vielzahl von Umsetz-Verfahren ist in Gebrauch. Das Gegenstück ist der Digital-Analog-Umsetzer (DAU).

Analog-Digital-Umsetzer sind elementare Bestandteile fast aller Geräte der modernen Kommunikations- und Unterhaltungselektronik wie z. B. Mobiltelefonen, Digitalkameras, oder Camcordern. Zudem werden sie zur Messwerterfassung in Forschungs- und industriellen Produktionsanlagen, in Maschinen und technischen Alltagsgegenständen wie Kraftfahrzeugen oder Haushaltsgeräten eingesetzt.

Ein ADU diskretisiert ein zeit-kontinuierliches Eingangssignal (entweder durch sein Funktionsprinzip oder durch eine vorgeschaltete bzw. integrierte Sample-and-Hold-Stufe) in einzelne diskrete Abtastwerte. Diese Abtastwerte werden anschließend in Digitalwerte umgesetzt. Auf Grund einer endlichen Anzahl von möglichen Ausgangswerten erfolgt dabei immer eine Quantisierung. Das Ergebnis einer AD-Umsetzung kann man sich in einem Signal-Zeit-Diagramm in einer Punktfolge mit gestuften horizontalen und vertikalen Abständen vorstellen. Die Hauptparameter eines ADUs sind seine Bittiefe und seine maximale Abtastrate. Die Umsetzzeit ist meist wesentlich kleiner als das Reziproke der Abtastrate.

Schon die Bittiefe eines AD-Umsetzers begrenzt die maximal mögliche Genauigkeit, mit der das Eingangssignal umgesetzt werden kann. Die nutzbare Genauigkeit ist durch weitere Fehlerquellen des ADUs geringer. Neben möglichst schnellen Verfahren gibt es auch langsame (integrierende) Verfahren zur Unterdrückung von Störeinkopplungen.

Die minimal notwendige Abtastfrequenz für eine verlustfreie Diskretisierung ergibt sich aus der Bandbreite des Eingangssignals.
Um das Signal später vollständig rekonstruieren zu können, muss die Abtastfrequenz größer als das Doppelte der maximal möglichen Frequenz im Eingangssignal sein. Anderenfalls kommt es zu einer Unterabtastung, die im rekonstruierten Signal im Eingangssignal nicht vorhandene Frequenzen enthält. Daher muss das Eingangssignal bandbegrenzt sein. Entweder ist es dies von sich aus oder es wird durch Tiefpassfilterung zu solch einem Signal gemacht.

Manchmal ist das abzutastende Signal allerdings so hochfrequent, dass man diese Bedingung technisch nicht realisieren kann. Wenn das Eingangssignal jedoch periodisch ist, kann man durch Mehrfachabtastung mit zeitlichem Versatz dennoch eine Rekonstruktion ermöglichen, ohne dabei das Abtasttheorem zu verletzen, da bei mehrfachem Durchlauf des Signals Zwischenpunkte ermittelt werden und so eine größere Zahl von Stützstellen entsteht, was im Endeffekt einer Erhöhung der Abtastrate entspricht.

Während der Signalumsetzung darf sich bei vielen Umsetzverfahren das Eingangssignal nicht ändern. Dann schaltet man dem eigentlichen AD-Umsetzer eine Abtast-Halte-Schaltung (Sample-and-Hold-Schaltung) vor, die den Signalwert () analog so zwischenspeichert, dass er während der Quantisierung konstant bleibt. Dies trifft besonders auf die stufen- und bitweisen Umsetzer zu, die längere Umsetzzeiten benötigen. Wenn ein Umsetzer diese Abtast-Halte-Schaltung erfordert, so ist sie bei Realisierung als integrierter Schaltkreis heute meist enthalten.

In vielen Anwendungen soll das Eingangssignal in immer exakt gleichen Zeitabständen abgetastet werden. Durch zufällige Variationen der Abstände tritt jedoch ein Effekt auf, den man als Jitter bezeichnet. Er verfälscht das ursprüngliche Signal bei der späteren Rekonstruktion, da diese wieder äquidistant – also mit gleichen Zeitabständen – erfolgt.

Nicht verwechselt werden darf die Umsetzdauer mit der Latenzzeit eines Umsetzers, d. h. die Zeit die nach der Erfassung vergeht, bis ein AD-Umsetzer das Datum weitergegeben hat. Diese Zeit kann weitaus größer als die Umsetzdauer sein, was insbesondere in der Regelungstechnik störend sein kann. Sie wird verursacht durch Pipelining des Umsetzers, Nachbearbeitung der Daten und die serielle Datenübertragung.

Die Quantisierung des vorher zeitdiskretisierten Signals stellt den eigentlichen Übergang von einem analogen Signal zu einem digitalen Signal dar. Auf Grund der endlichen Bittiefe des Umsetzers gibt es nur eine gewisse Anzahl an Codeworten und deren dazugehörige Eingangsspannung. Das Signal wird quantisiert. Die Abweichung zwischen der wahren Eingangsspannung und der quantisierten Eingangsspannung nennt man Quantisierungsabweichung. Je mehr Bits bzw. Codeworte zur Verfügung stehen, umso kleiner ist diese unvermeidbare Abweichung. Bei einem idealen AD-Umsetzer verringert jedes zusätzliche Bit dieses Rauschen um 6,02 dB. Bei realen AD-Umsetzern kann man über die ENOB abschätzen, was ein weiteres Bit bei dem betrachteten Umsetzer bringen würde (so würde ein weiteres Bit bei einem 12-bit-Umsetzer mit einem ENOB von 11 bit ca. 0,15 bit bzw. 0,9 dB bringen).

Das Verhältnis aus maximal möglicher unverzerrter Eingangsspannung und dem Rauschen bei signallosem Eingang nennt man Dynamikumfang. Umsetzer, die bei fehlendem Eingangssignal ein konstantes Codewort liefern, haben einen unendlich hohen Dynamikumfang. Sinnvoller ist die Angabe des Signal-Rausch-Verhältnisses (bzw. des SINAD, "signal to noise and distortion ratio," Verhältnis des Signals zur Summe aus Rauschen und Verzerrungen), d. h. des Verhältnisses des maximal möglichen Signals zum dabei auftretenden Rauschen. Bei unbewertetem Rauschen und idealen Umsetzern mit "n" Bits gilt die Gleichung:
oder
Bei realen Umsetzern reduzieren sich die Werte durch zusätzliche Abweichungen des Umsetzers. Ein weiterer Aspekt ist, dass in der Praxis das Rauschen häufig bewertet (z. B. DIN-A oder CCIR-468) oder bandbegrenzt (z. B. 0 Hz … 20 kHz) wird.

Da das dem ADU zugeführte Analogsignal in einen größenlosen Digitalwert umgesetzt wird, muss es mit einem vorgegebenen Wert oder Signal bewertet werden (Eingangssignalbereich bzw. Messbereich). Im Allgemeinen wird ein feststehender Bezugswert formula_1 (z. B. eine intern erzeugte Referenzspannung) verwendet. Das analoge Eingangssignal wird digital abgebildet, die Referenz legt den zulässigen Scheitelwert des Eingangssignals fest.

Bei Analog-Digital-Umsetzern besteht zwischen Eingangs- und Ausgangsgröße immer ein nichtlinearer Zusammenhang. Ändert sich allerdings bei steigender Eingangsspannung der Digitalwert in konstanten Abständen oder nähert sich bei extrem feiner Stufung die Kennlinie einer Geraden, spricht man dennoch von einem linearen Analog-Digital-Umsetzer. Es gibt

wobei daneben auch andere Kodierungen, beispielsweise Zweierkomplement, BCD-Code verwendbar sind.


Zusätzlich zu dem unvermeidbaren Quantisierungsfehler haben reale AD-Umsetzer folgende Fehler:

Als Abweichungen der Kennlinien zwischen realem und idealem Umsetzer sind folgende Fehler definiert (siehe Bild):
Der Verstärkungsfehler wird oft als Bruchteil des aktuellen Wertes angegeben, der Nullpunktfehler zusammen mit dem Quantisierungsfehler und der Nichtlinearitätsfehler als Bruchteile des Endwertes oder als Vielfache eines LSB.

Einzelne Stufen können unterschiedlich breit ausfallen.

Bei kontinuierlich steigender Eingangsgröße kann es je nach Realisierungsverfahren vorkommen, dass ein Wert der Ausgangsgröße übersprungen wird, insbesondere dann, wenn es einen Übertrag über mehrere Binärstellen gibt, beispielsweise von 0111 1111 nach 1000 0000. Man spricht hierzu von „missing codes“.

Bei Wandlung jedes nichtkonstanten Eingangssignals entsteht durch zeitliche Schwankungen des Umsetzer-Taktes Δ"t" (clock jitter) ein der zeitlichen Änderung des Eingangssignals proportionaler Fehler.
Bei einem Sinussignal der Frequenz f und der Amplitude A beträgt es formula_2. Jeglicher Jitter erzeugt weiteres Rauschen – es gibt keinen Schwellwert, unterhalb dem es zu keiner Verschlechterung des Signal-Rausch-Verhältnisses kommt. Viele aktuelle Wandler (insbesondere Delta-Sigma-Umsetzer) haben eine interne Taktaufbereitung. Der Hintergrund ist der, dass viele Wandler einen höheren internen Takt benötigen bzw. bei Delta-Sigma-Umsetzern, dass dort Jitter direkt (d. h. auch bei konstantem Eingangssignal) Wandlungsfehler verursacht.

Es gibt eine große Anzahl von Verfahren, die zur Umsetzung von analogen in digitale Signale benutzt werden können. Im Folgenden sind die wichtigsten Prinzipien aufgeführt. Als Eingangsgröße wird in allen Beispielen die elektrische Spannung zugrunde gelegt.

Den inneren Ablauf einer Umsetzung steuern die Bausteine selbst. Für die Zusammenarbeit mit einem Rechner kann ein ADU mit einem Start-Eingang versehen sein für die Anforderung zu einer neuen Umsetzung, mit einem „busy“-Ausgang für die Meldung der noch andauernden Umsetzung und mit bus-kompatiblen Datenausgängen für das Auslesen des entstandenen Digitalwertes.

Bei diesen Verfahren finden zwei Vorgänge statt:
Beim Nachlauf-Umsetzer wird ebenfalls gezählt. Dieser wird ohne Kondensator als rückgekoppelter Umsetzer betrieben und weiter unten erklärt.

Beim Sägezahnverfahren wird die Ausgangsspannung formula_3 eines Sägezahngenerators über zwei Komparatoren K und K mit dem Massepotenzial (0 V) und mit der ADU-Eingangsspannung formula_4 verglichen. Während des Zeitraums, in dem die Sägezahnspannung den Bereich zwischen 0 V und der Spannung formula_4 durchläuft, werden die Impulse eines Quarzoszillators gezählt. Aufgrund der konstanten Steigung der Sägezahnspannung ist die verstrichene Zeit und somit der Zählerstand bei Erreichen von formula_6 proportional zur Höhe der ADU-Eingangsspannung. Zum Ende des Zählvorgangs wird das Zählergebnis in ein Register übertragen und steht als digitales Signal zur Verfügung. Anschließend wird der Zähler zurückgesetzt, und ein neuer Umsetzungsvorgang beginnt.

Die Umsetzungszeit bei diesem ADU ist abhängig von der Eingangsspannung. Schnell veränderliche Signale können mit diesem Umsetzertyp nicht erfasst werden. Umsetzer nach dem Sägezahnverfahren sind ungenau, da der Sägezahngenerator mit Hilfe eines temperatur- und alterungsabhängigen Integrationskondensators arbeitet. Sie werden wegen ihres relativ geringen Schaltungsaufwands für einfache Aufgaben eingesetzt, beispielsweise in Spielkonsolen, um die Stellung eines Potentiometers, das durch einen Joystick oder ein Lenkrad bewegt wird, zu digitalisieren.

Dual- und Quadslope-Umsetzer bestehen im Wesentlichen aus einem Integrator und mehreren Zählern und elektronischen Schaltern. Der Integrator arbeitet mit einem externen, hochwertigen Kondensator, der in zwei oder mehr Zyklen geladen und entladen wird. Beim Zweirampenverfahren wird zunächst der Integratoreingang mit der unbekannten ADU-Eingangsspannung verbunden, und es erfolgt die Ladung über ein fest vorgegebenes Zeitintervall. Für die anschließende Entladung wird der Integrator mit einer bekannten Referenzspannung entgegengesetzter Polarität verbunden. Die benötigte Entladezeit bis zum Erreichen der Spannung null am Integratorausgang wird durch einen Zähler ermittelt; der Zählerstand steht bei geeigneter Dimensionierung unmittelbar für die Eingangsspannung. Die Größe der Kapazität kürzt sich bei diesem Verfahren aus dem Ergebnis heraus. Zur Korrektur des Nullpunktfehlers des ADU wird beim Vierrampenverfahren noch ein weiterer Lade-/Entladezyklus bei kurzgeschlossenem Integratoreingang durchgeführt. Die Referenzspannung ist die bestimmende Größe für die Genauigkeit; das heißt beispielsweise, dass thermisch bedingte Schwankungen vermieden werden müssen.

Derartige Umsetzer nach dem Mehrrampenverfahren sind langsam, benötigen keine Abtast-Halte-Schaltung und bieten eine hohe Auflösung sowie gute differentielle Linearität und gute Unterdrückung von Störsignalen wie Rauschen oder Netzeinkopplung. Das typische Einsatzgebiet sind anzeigende Messgeräte (Digitalmultimeter), die kaum eine Umsetzzeit unter 500 ms benötigen und bei geeigneter Integrationsdauer überlagerte 50-Hz-Störungen der Netzfrequenz eliminieren können.

Beim Ladungsbilanzverfahren (Charge-Balancing-Verfahren) wird der Kondensator eines Integrators durch einen zur Eingangsgröße proportionalen elektrischen Strom geladen und durch kurze Stromstöße in entgegengesetzter Richtung entladen, so dass sich im Mittel keine Ladung aufbaut. Je größer der Ladestrom ist, desto häufiger wird entladen. Die Häufigkeit ist proportional zur Eingangsgröße; die Anzahl der Entladungen in einer festen Zeit wird gezählt und liefert den Digitalwert. In seinem Verhalten ist das Verfahren dem Dual-Slope-Verfahren ähnlich. Auch andere analoge Eingangsstufen, die einen Spannungs-Frequenz-Umformer mit genügend hochwertiger Genauigkeit enthalten, führen über eine Frequenzzählung auf einen Digitalwert.

Diese arbeiten mit einem DAU, der einen Vergleichswert formula_7 liefert. Dieser wird nach einer geeigneten Strategie an das analoge Eingangssignal formula_4 angenähert. Der zum Schluss am DAU eingestellte Digitalwert ist das Ergebnis des ADU.

Hier wird ein Zähler als Datenspeicher eingesetzt. Je nach Vorzeichen von formula_9 wird um einen Schritt aufwärts oder abwärts gezählt und neu verglichen – gezählt und neu verglichen, bis die Differenz kleiner ist als der kleinste einstellbare Schritt. Diese Umsetzer „fahren“ dem Signal einfach nach, wobei die Umsetzungszeit vom Abstand des aktuellen Eingangssignals zum Signal bei der letzten Umsetzung abhängt.
Diese arbeiten mit einem DAU, der einen Vergleichswert formula_7 jedes Mal neu aufbaut. Das Eingangssignal wird mittels Intervallschachtelung eingegrenzt. Einfache sukzessive Approximation setzt dabei pro Schritt ein Bit um. Ein um Größenordnungen genaueres und schnelleres Umsetzen kann dadurch erreicht werden, dass die Umsetzung redundant erfolgt, indem mit kleinerer Schrittweite umgesetzt wird, als einem Bit entspricht.

Ein mögliches Approximationsverfahren ist das Wägeverfahren. Dabei werden in einem Datenspeicher ("successive approximation register", SAR) zunächst alle Bits auf null gesetzt. Beginnend beim höchstwertigen Bit (Most Significant Bit, MSB) werden abwärts bis zum niederwertigsten Bit (Least Significant Bit, LSB) nacheinander alle Bits des Digitalwerts ermittelt.

Vom Steuerwerk wird jeweils das in Arbeit befindliche Bit probeweise auf eins gesetzt; der Digital-Analog-Umsetzer erzeugt die dem aktuellen Digitalwert entsprechende Vergleichsspannung. Der Komparator vergleicht diese mit der Eingangsspannung formula_4 und veranlasst das Steuerwerk, das in Arbeit befindliche Bit wieder auf null zurückzusetzen, wenn die Vergleichsspannung höher ist als die Eingangsspannung. Sonst ist das Bit mindestens notwendig und bleibt gesetzt. Nach der Einstellung des niederwertigsten Bits ist formula_9 kleiner als der kleinste einstellbare Schritt.

Während der Umsetzung darf sich das Eingangssignal formula_4 nicht ändern, da sonst die niederwertigen Bits auf Grundlage der festgestellten, aber nicht mehr gültigen höherwertigen Bits gewonnen würden. Deshalb ist dem Eingang eine Abtast-Halte-Schaltung (S/H) vorgeschaltet. Für jedes Bit an Genauigkeit benötigt der ADU jeweils einen Taktzyklus Umsetzungszeit. Derartige Umsetzer erreichen Auflösungen von 16 Bit bei einer Umsetzungsrate von 1 MHz.

Dem Wägeverfahren ähnliche redundante Analog-Digital-Umsetzer gehen davon aus, dass keine exakte Halbierung des noch offenen Intervalls um den Zielwert herum erfolgt, sondern dieses Intervall nur um einen Anteil davon eingeschränkt wird. Dazu haben sie einen Digital-Analog-Umsetzer, dessen Elemente nicht nach dem Dualsystem gestaffelt sind, also immer um den Faktor 2, sondern um einem kleineren Faktor. Sie nehmen damit einerseits in Kauf, dass mehr Elemente benötigt werden, um den gleichen Wertebereich abzudecken, ermöglichen aber andererseits, dass der Umsetzer um eine Größenordnung schneller arbeiten und eine um mehrere Größenordnungen höhere Genauigkeit erzielen kann:
Die schnellere Funktion kommt dadurch, dass der Komparator in jedem Schritt nicht abwarten muss, bis sich seine Verstärker bis zu einem Mehrfachen der Zielgenauigkeit eingeschwungen haben (immer etwas größenordnungsmäßig so viele Einschwing-Zeitkonstanten, wie der Umsetzer Bits umsetzen soll), sondern eine Entscheidung schon nach der kurzen 50-Prozent-Einschwingzeit abgeben kann, die dann in einem recht großen Bereich innerhalb des Restintervalls fehlerhaft ist. Das wird allerdings mehr als abgefangen durch die redundant ausgelegten Umsetzerelemente. Die Gesamtumsetzdauer eines solchen Umsetzers liegt größenordnungsmäßig eine Zehnerpotenz unter der seines einfachen Vorbilds. Durch den redundanten Umsetzungsprozess hat ein solcher Umsetzer ein viel geringeres Eigenrauschen als sein rein dualer Gegenpart.

Zusätzlich kann sich ein solcher ADU selbst einmessen, und zwar bis zu einer Genauigkeit, die nur durch das Rauschen begrenzt ist. Indem man das Selbsteinmessen wesentlich langsamer ablaufen lässt als die Umsetzung in der Nutzanwendung, kann der Rauscheinfluss in diesem Prozess um eine Größenordnung gedrückt werden. Die resultierende Kennlinie eines solchen Umsetzers ist bis auf eine rauschartige Abweichung um wenige Vielfache des kleinsten beim Selbsteinmessen verwendeten Elements absolut linear. Indem zwei derartige Umsetzer nebeneinander auf denselben Chip platziert werden und einer immer im Einmess-Modus ist, können solche Umsetzer nahezu resistent gegen Herstellungstoleranzen, Temperatur- und Betriebsspannungsänderungen gemacht werden. Die erreichbare Auflösung ist ausschließlich rauschbegrenzt.

Das Delta-Sigma-Verfahren, auch als 1-Bit-Umsetzer bezeichnet, basiert auf der Delta-Sigma-Modulation. In der einfachsten Form (Modulator erster Ordnung) kommt das Eingangssignal über einen analogen Subtrahierer zum Integrator und verursacht an dessen Ausgang ein Signal, das von einem Komparator mit eins oder null bewertet wird. Ein Flipflop erzeugt daraus ein zeitdiskretes binäres Signal, mit dem ein 1-Bit-Digital-Analog-Umsetzer in eine positive oder negative elektrische Spannung liefert, die über den Subtrahierer den Integrator wieder auf null zurückzieht (Regelkreis). Ein nachgeschalteter Digitalfilter setzt den seriellen und hochfrequenten Bit-Strom in Daten niedriger Erneuerungsrate, aber großer Bitbreite (16 oder 24 Bit) und hohem Signal-Rausch-Verhältnis (94 bis 115 dB) um. In der Praxis werden Delta-Sigma-Umsetzer als Systeme dritter oder vierter Ordnung aufgebaut, das heißt durch mehrere seriell angeordnete Differenz- und Integratorstufen. Dies erlaubt eine bessere Rauschformung und damit einen höheren Gewinn an Auflösung bei gleicher Überabtastung.

Ein Vorteil des Delta-Sigma-Umsetzers ist, dass die Dynamik in gewissen Grenzen durch die Bandbreite wechselseitig ausgetauscht werden kann. Durch die kontinuierliche Abtastung am Eingang wird auch keine Halteschaltung (engl. sample and hold) benötigt. Außerdem werden geringe Anforderungen an das analoge Anti-Aliasing-Filter gestellt.

Die Vorteile werden durch den Nachteil der vergleichsweise hohen Latenzzeit erkauft, welche vor allem durch die digitalen Filterstufen bedingt ist. Delta-Sigma-Umsetzer werden daher dort eingesetzt, wo kontinuierliche Signalverläufe und nur moderate Bandbreiten benötigt werden, wie beispielsweise im Audiobereich. Praktisch alle Audiogeräte im Bereich der Unterhaltungselektronik wie zum Beispiel DAT-Rekorder setzen diese Umsetzer ein.

Auch bei Datenumsetzern in der Kommunikationstechnik und der Messtechnik wird es aufgrund der fallenden Preise zunehmend eingesetzt. Durch die dabei notwendige hohe Überabtastung sind dem Verfahren bei höheren Frequenzen allerdings Grenzen gesetzt. Die Grenze liegt bei etwa 2,5 MHz.

Während die sukzessive Approximation mehrere Vergleiche mit nur einem Komparator ausführt, kommt die "direkte Methode" oder auch "Flash-Umsetzung" mit nur einem Vergleich aus. Dazu ist bei Flash-Umsetzern aber für jeden möglichen Ausgangswert (bis auf den größten) ein separat implementierter Komparator erforderlich. Beispielsweise ein 8-Bit-Flash-Umsetzer benötigt somit 2−1 = 255 Komparatoren.

Das analoge Eingangssignal wird im Flash-Umsetzer gleichzeitig von allen Komparatoren mit den (über einen linearen Spannungsteiler erzeugten) Vergleichsgrößen verglichen. Anschließend erfolgt durch eine Kodeumsetzung der 2−1 Komparatorsignale in einen "n" bit breiten Binärkode (mit "n": Auflösung in Bit). Das Resultat steht damit nach den Durchlaufverzögerungen (Schaltzeit der Komparatoren sowie Verzögerung in der Dekodierlogik) sofort zur Verfügung. Im Ergebnis sind die Flash-Umsetzer also sehr schnell, bringen aber im Allgemeinen auch hohe Verlustleistungen und Anschaffungskosten mit sich (insbesondere bei den hohen Auflösungen).

Die Codeumsetzung erfordert unabhängig von der Auflösung nur eine Spalte mit Und-Gattern und eine Spalte mit Oder-Gattern (siehe Bild). Sie rechnet das Ergebnis der Komparatoren um in eine Binärzahl. Sie arbeitet mit einer sehr kurzen und für alle Binärziffern gleich langen Durchlaufverzögerung. Für die vier möglichen Werte eines Zwei-Bit-Umsetzers sind drei Komparatoren erforderlich. Der vierte hat nur die Funktion, eine Messbereichsüberschreitung zu signalisieren und die Codeumsetzung zu unterstützen.

Pipeline-Umsetzer sind mehrstufige Analog-Digital-Umsetzer mit mehreren selbständigen Stufen, die in Pipeline-Architektur aufgebaut sind. Ihre Stufen bestehen in der Regel aus Flash-Umsetzern über wenige Bits.

In jeder Pipelinestufe wird eine grobe Quantisierung vorgenommen, dieser Wert wieder mit einem DAU in ein analoges Signal umgesetzt und vom zwischengespeicherten Eingangssignal abgezogen. Der Restwert wird verstärkt der nächsten Stufe zugeführt. Der Vorteil liegt in der stark verminderten Anzahl an Komparatoren, z. B. 30 für einen zweistufigen Acht-Bit-Umsetzer. Ferner kann eine höhere Auflösung erreicht werden. Die Mehrstufigkeit erhöht die Latenzzeit, aber vermindert die Abtastrate nicht wesentlich. Die Pipeline-Umsetzer haben die echten Parallelumsetzer außer bei extrem zeitkritischen Anwendungen ersetzt. Diese mehrstufigen Umsetzer erreichen Datenraten von 250 MSPS (Mega-Samples Per Second) bei einer Auflösung von 12 Bit (MAX1215, AD9480) oder eine Auflösung von 16 Bit bei 200 MSPS (ADS5485).

Die Werte der Quantisierungsstufen werden unter Berücksichtigung ihrer Gewichtung addiert. Meistens enthält ein Korrektur-ROM noch Kalibrierungsdaten, die dazu dienen, Fehler zu korrigieren, die in den einzelnen Digitalisierungsstufen entstehen. Bei manchen Ausführungen werden diese Korrekturdaten auch auf ein externes Signal hin generiert und in einem RAM abgelegt.

Pipeline-Umsetzer kommen normalerweise in allen Digitaloszilloskopen und bei der Digitalisierung von Videosignalen zur Anwendung. Als Beispiel ermöglicht der MAX109 bei einer Auflösung von 8 bit eine Abtastrate von 2,2 GHz. Mittlerweile gibt es aber noch schnellere (4 GSPS) und genauere Umsetzer (16 bit @1 GSPS). Bei heutigen Digitaloszilloskopen mit möglichen Abtastraten von 240 GSPS werden zusätzlich noch Demultiplexer vorgeschaltet.

Ein Hybrid-Umsetzer ist kein eigenständiger Umsetzer, sondern er kombiniert zwei oder mehr Umsetzungsverfahren, zum Beispiel auf Basis einer SAR-Struktur, wobei der ursprüngliche Komparator durch einen Flash-Umsetzer ersetzt wird. Dadurch können in jedem Approximationsschritt mehrere Bits gleichzeitig ermittelt werden.

Am Markt kommen im Wesentlichen vier Verfahren vor:


Mit diesen Verfahren kann man fast alle praktischen Anforderungen abdecken und bei gemäßigten Anforderungen (z. B. 12 bit, 125 KSPS, 4 Kanäle) sind diese Wandler kostengünstig (ca. 1 €) zu bekommen.






</doc>
<doc id="373" url="https://de.wikipedia.org/wiki?curid=373" title="Anglikanische Gemeinschaft">
Anglikanische Gemeinschaft

Die Anglikanische Gemeinschaft (auch "Anglikanische Kommunion", ; von ‚englisch‘), umgangssprachlich die anglikanische Kirche, ist eine weltweite christliche Kirchengemeinschaft, die in ihrer Tradition evangelische und katholische Glaubenselemente vereinigt, wobei die katholische Tradition in der Liturgie und im Sakramentsverständnis (insbesondere dem Amtsverständnis) vorherrscht und die evangelische in der Theologie.

Die anglikanischen Landeskirchen sehen sich als Teile der einen, heiligen, katholischen und apostolischen Kirche, die sich der Tradition und Theologie der englischen (und zum Teil schottischen) Reformation verpflichtet haben. Jedoch versteht die anglikanische Kirche ihre „Reformation“ nicht als einen Bruch mit der vorreformatorischen Kirche, sondern als notwendige Reform der katholischen Kirche der britischen Inseln. Damit ist die anglikanische Kirche sowohl katholische Kirche als auch reformatorische Kirche, die allerdings seit der Reformation eine bewusst eigenständige christlich-anglikanische Tradition und Theologie entwickelt hat. Sie hat ca. 80 Millionen Mitglieder. Weltweit besteht die Kirchengemeinschaft aus rund 385 Diözesen.

Die Kirchengemeinschaft besteht aus 38 selbständigen Landeskirchen bzw. Provinzen. Neben der Church of England sind diese beispielsweise die Anglikanische Kirche von Kanada, die Episkopalkirche der Vereinigten Staaten und die Church of South India. Für eine vollständige Liste aller Mitglieder der Gemeinschaft siehe Liste der Mitgliedskirchen der Anglikanischen Gemeinschaft. Die Provinzen bzw. Landeskirchen, denen jeweils ein Primas "(primate)" vorsteht, bestehen aus mehreren Bistümern.

Weltweit gibt es etwa 80 Millionen Christen in der Anglikanischen Kommunion, davon etwa 42 Millionen im Vereinigten Königreich (größtenteils in England "(Church of England)", da die selbständigen anglikanischen Landeskirchen in Schottland und Wales weder Staatskirchen sind noch die Mehrheit der Bevölkerung zu ihren Mitgliedern zählt). Die "Kirche von England" – oft vereinfachend „Anglikanische Kirche“ genannt – ist nach der römisch-katholischen Kirche (1,2 Mrd. Gläubige) und der orthodoxen Kirchengemeinschaft (283 Mio. Gläubige) die drittgrößte Kirche weltweit; zählt man sie zu den protestantischen Kirchen, kommen sie mit 543 Mio. Gläubigen gar an zweiter Stelle.

Die anglikanische Kirche ist in englischsprachigen Gebieten sowie in den Ländern des Commonwealth (insbesondere den Commonwealth Realms) führend vertreten. Vereinzelte Gemeinden finden sich auch in den meisten übrigen Staaten.

Darüber hinaus gibt es vereinzelt Menschen, die eine in der anglikanischen Tradition stehende Kirche besuchen, die jedoch "nicht" zur Anglikanischen Gemeinschaft gehören (u. a. die so genannten "Anglican continuing churches"), da sie die Kommunion mit der Gemeinschaft abgebrochen haben. Solche Kirchen sollten "nicht" (und wollen auch nicht) zu den Kirchen der Anglikanischen Gemeinschaft gezählt werden.

Die Gemeinschaft kennt keine zentralisierten Strukturen der Autorität, sondern vertritt seit der englischen Reformation das Prinzip, dass kein Bischof (ob von Rom, Canterbury oder Konstantinopel) für die Geschäfte eines anderen Bistums weisungsbefugt ist. Dennoch gibt es in der Gemeinschaft vier sogenannte "Instruments of Unity" („Instrumente der Einheit“). In der Reihenfolge ihres Alters sind diese: Der Erzbischof von Canterbury, die Lambeth Conference, das Anglican Consultative Council und das Treffen der Primasse (die ranghöchsten Bischöfe der einzelnen Landeskirchen).


Die Mitgliedskirchen werden Landeskirche oder Provinz genannt. Einige dieser Kirchenprovinzen decken sich mit den Grenzen politischer Staaten, einige schließen mehrere Staaten ein, während andere nur Teilbereiche einer Nation einschließen.
Geleitet wird jede Kirche von einer Synode, die sich aus den Bischöfen, Klerusvertretern und Laienvertretern zusammensetzt. Die Versammlung kann aus einem Haus (Synode) oder aus zwei Kammern (z. B. Haus der Bischöfe, Haus der Deputierten) zusammengesetzt sein und tagt stets zu bestimmten Zeiten. In einigen Fällen wird sie jedoch ständig durch das Provinzbüro, in dem alle laufenden Angelegenheiten der Kirche bearbeitet werden, aufrechterhalten. In anderen Provinzen besteht an der Stelle des Provinzbüros ein "Executive Committee", das sich ebenfalls mit Fragen der Finanzen oder Klerikerausbildung beschäftigt. Oft wird das Büro bzw. Komitee von einem Generalsekretär geleitet.

Jeder anglikanischen Kirche steht ein Primas vor. Dieser kann der Bischof eines bestimmten Sitzes sein, so wie für England in Canterbury, ist aber für gewöhnlich nicht daran gebunden, sondern wird auf der Provinzsynode gewählt. In den meisten Fällen trägt er den Titel eines Erzbischofs, und oft ist seine Amtszeit beschränkt. In der Episkopalkirche der USA heißt der Primas "Presiding Bishop".

Jeder Diözese steht ein "Bischof" vor, dem – wenn die Diözese von entsprechender Größe ist – Suffraganbischöfe beigeordnet sind. Diese haben eine ähnliche Rolle wie die Weihbischöfe der römisch-katholischen Kirche. Es kann zwar vorkommen, dass sie einen eigenen Bezirk innerhalb der Diözese zugeordnet bekommen, sie bleiben aber in jedem Fall dem Diözesanbischof untergeordnet. Manchmal wird die Regionalaufsicht stattdessen durch einen sogenannten Archidiakon oder einen Dekan ausgeführt (siehe unten).

Die Kirchengemeinschaft sieht sich als in der Apostolischen Sukzession stehend an. Dieser Anspruch wird sowohl von der orthodoxen als auch von der altkatholischen Kirche anerkannt, nicht jedoch von der römisch-katholischen Kirche (siehe Apostolicae curae).

Der Diözesanbischof besitzt meistens eine Kathedrale, an welcher es den "Kathedralklerus" gibt. Dieser untersteht dem "Dean" (Dekan) oder "Provost" (Propst) und kann sich ggf. in Residenzkanoniker, Honorarkanoniker (Praebendare) und Niedere Kanoniker unterteilen.

Der "Diözesanklerus" besteht aus Priestern, die entsprechend der Gemeinde, in der sie tätig sind, vicars oder rectors (Pfarrer), curates (Kapläne/Vikare) oder assistant curates/priests sind, sowie aus Diakonen. Mehrere Pfarrkirchen sind in Dekanaten zusammengefasst. Besonders große Diözesen sind in "Archdeaneries" (Erzdekanate) unterteilt, denen "Archdeacons" vorstehen (manchmal mit den Suffraganbischöfen identisch). Diese Erzdekanate ähneln den Regionen in großen römisch-katholischen Diözesen.

Um in die Reihen des Klerus der anglikanischen Kirche aufgenommen zu werden, bewirbt man sich beim Bischof einer Diözese. Dieser wird diese Bewerbung an ein Gremium "(Selection Conference)" weiterleiten, welches den Bewerber auf seine Eignung im Geistlichen, Geistigen, Gesundheitlichen und Familiären prüft. Ist dies geschehen, so sendet es ein Gutachten an den jeweiligen Bischof, in welchem es den Bewerber empfiehlt, bedingt empfiehlt oder ablehnt. Zugleich macht es auch Vorschläge zur Ausbildung, welche von einem normalen Hochschulstudium bis zu regionalen Kursen reichen. Hat der Bewerber die Zustimmung des Bischofs erhalten, so untersteht er fortan einem "Diocesan Director for Ordinands", der den Bewerber in allen Bereichen seines Lebens begleitet und zum Ende der Ausbildung ein Zeugnis zu dessen Eignung abgibt. Ist dieses positiv ausgefallen, so steht der Weihe des Bewerbers oder der Bewerberin nichts mehr im Wege.

Im 19. Jahrhundert gründeten sowohl die Church of England als auch die US-Episkopalkirche in Europa Pfarreien. Die Zielgruppe waren Glaubensangehörige im Ausland sowie Reisende. Zuständig für diese Pfarreien war ursprünglich der Bischof von London (im Fall der Church of England) bzw. der Presiding Bishop (im Fall der US-Episkopalkirche). Später übertrug der Bischof von London die Zuständigkeit an den neu eingesetzten Bischof von Gibraltar.

1980 wurde durch eine Union der Diözese von Gibraltar und der vom Bischof von Fulham geleiteten Jurisdiktion of North and Central Europe die Diözese in Europa "(Diocese [of Gibraltar] in Europe)" gegründet, die seither für die Gemeinden in Europa außerhalb Großbritanniens und Irlands zuständig ist. Parallel stärkte die US-Kirche die Autonomie der europäischen Pfarreien durch die Struktur der Convocation of Episcopal Churches in Europe, eine bistumsähnliche Organisation mit einem Suffraganbischof, der ursprünglich vom "Presiding Bishop" ernannt wurde, und mit der Wahl des jetzigen Amtsinhabers, Pierre Whalon, erstmals durch Klerus und Volk der "Convocation" gewählt wurde.

In Deutschland gibt es rund 40 anglikanische bzw. episkopale Gemeinden, von denen sich die englischsprachigen 1997 zum Council of Anglican and Episcopal Churches in Germany (CAECG) zusammengeschlossen haben. Viele von ihnen wurden im 19. Jahrhundert gegründet und erfuhren in der Besatzungszeit nach dem Zweiten Weltkrieg neuen Aufschwung, manche – wie die Gemeinde in Hamburg – gehen auf Handelsbeziehungen zwischen England und Kontinentaleuropa im 17. Jahrhundert zurück. Eine Reihe von deutschsprachigen Gemeinden entstanden in den letzten Jahren und zählen sich zur Church of England, der Anglican Independent Communion oder der Anglican Church in North America.

Gemeinden der Church of England existieren u. a. in Berlin, Bonn/Köln, Düsseldorf, Freiburg im Breisgau, Hamburg, Heidelberg, Leipzig und Stuttgart. Sie gehören zur Erzdiakonie von Nordeuropa und Deutschland, die von Archdeakon Ven. Colin Williams geleitet wird, der auch Pfarrer der anglikanischen Gemeinde von Kopenhagen ist. Gemeinden der Episcopal Church in the USA (ECUSA) existieren u. a. in Frankfurt am Main, München, und Wiesbaden; dazu kommen Missionen in Augsburg, Karlsruhe, Nürnberg und Ulm.

Die anglikanischen Gemeinden in der Schweiz gehören der Church of England an und bilden in deren Diözese in Europa eine eigene Erzdiakonie, welcher der Ven. Arthur Siddall vorsteht. Sie hat acht Standortpfarreien in größeren Städten und einige weitere Gemeinden, die von dort betreut werden. Die US-Episkopalkirche hat eine Gemeinde in Genf.

In Österreich gibt es eine anglikanische Gemeinde in Wien, die Christ Church Vienna; von dort aus werden so genannte Satellitengemeinden z. B. in Klagenfurt betreut. Sie wird geleitet von Ven. Patrick Curran, Bischofsvikar des östlichen Europas.

Die anglikanische Kirche ist eine christliche Kirche, die sich „zum dreieinigen Gott bekennt, wie er sich in Jesus Christus offenbart hat“. Grundlage der Lehre sind die 39 Artikel, das "Book of Common Prayer" und die "Ordnung zur Ernennung von Bischöfen, Priestern und Diakonen". Letztere drei entstanden im 16. Jahrhundert und sind größtenteils das Werk von Thomas Cranmer, Erzbischof von Canterbury.

In der anglikanischen Lehre gibt es ein weites Spektrum zwischen der "High Church" (Anglo-Katholizismus), die in Liturgie und Lehre den anderen katholischen Kirchen nahesteht, und der "Low Church", die dem Protestantismus, insbesondere dem Calvinismus, nahesteht.

Theologisch sind innerhalb der anglikanischen Kirche sehr liberale wie auch streng evangelikale und konservative anglokatholische Richtungen vertreten.

Dieses sehr weite Meinungsspektrum wird teilweise als Stärke der Anglikaner betrachtet, die somit ein weites Spektrum des heutigen Christentums ohne Kirchenspaltung in einer Kirche umfassen können. Teilweise wird aber auch kritisiert, dass die Kirche so für nichts mehr stehe und der Beliebigkeit zu viel Raum gebe.

Der Gottesdienst der anglikanischen Kirche ist liturgisch ähnlich aufgebaut wie der der katholischen Kirchen. Er ist im "Book of Common Prayer" niedergelegt, von dem die einzelnen Gliedkirchen eigene Ausgaben haben.

Die Liturgie der anglikanischen Kirche hat die Eucharistie als zentrales Element und auch anderweitig zeugt sie vom anglikanischen Selbstverständnis als Teil der universalen katholischen Kirche. Die gottesdienstlichen Handlungen im Anglikanismus wurden allerdings, anders als in der römisch-katholischen Kirche, schon seit der Reformationszeit in der jeweiligen Landessprache abgehalten.

Das Stundengebet, in der anglikanischen Kirche als "Daily Office" bezeichnet, ist eine wichtige Tradition der anglikanischen Kirche. Insbesondere das Morgen- und Abendgebet sind weit verbreitet und werden in einigen Kirchen täglich gebetet. Die anglikanische Kirche kennt auch die Rosenkranztradition, wobei der Aufbau des Rosenkranzes und die Gebete sich von denen der katholischen Kirche unterscheiden und weniger auf Maria, dafür stärker auf Jesus Christus ausgerichtet sind. Diese Tradition wird jedoch nur in den eher zum Anglokatholizismus neigenden Gemeinden praktiziert; viele Anglikaner aus anderen Gemeinden sind mit dem Ave Maria-Gebet, das nicht im "Book of Common Prayer" vorkommt, gar nicht vertraut.

Wie die katholische Kirche, so kennt auch die anglikanische Kirche Männer- und Frauenorden. Die meisten zur Reformationszeit bereits bestehenden katholischen Orden haben anglikanische Entsprechungen. Diese leben nach den Evangelischen Räten und in Gemeinschaft, sind entweder kontemplativ oder aktiv in der Erziehung bzw. in der Pflege tätig.

Die Kirche von England datiert ihre Geschichte zurück bis in die römische Zeit, als das Christentum sich im gesamten römischen Reich ausbreitete. Nach dem Abzug der Römer und Einfall der Angeln, Sachsen und Jüten, die ihre heidnische Religion mit sich brachten, blieben nur noch die keltischen Stämme Britanniens im äußersten Norden und Westen der Inseln als Christen zurück. (Siehe dazu: Keltische Kirche) Mit der Mission von Augustinus von Canterbury an den Hof von Kent im Jahr 597 kam die christliche Religion erneut aus Rom nach England. Diese Missionstätigkeit ergänzte sich mit einer Mission durch irische Mönche (siehe Columban).

Die Bistümer in England waren in die beiden Kirchenprovinzen Canterbury und York aufgeteilt. Als 1529 unter Heinrich VIII. Streitigkeiten zwischen dem englischen Thron und dem Papst in Rom über die Rechtmäßigkeit der königlichen Ehen aufkamen, erklärten die Bischöfe Englands, dass sie in Heinrich und nicht im Papst das Oberhaupt der englischen Kirche sahen, womit sich die englische Kirche von Rom lossagte. So ergab es sich, dass in den Kirchenprovinzen von York und Canterbury im 16. Jahrhundert Ideen der Reformation in die Praxis umgesetzt werden konnten.

Unter Heinrich VIII. änderte sich für die englische Kirche zunächst sehr wenig. Der Gebrauch der lateinischen Sprache wurde zugunsten der englischen aufgegeben. Mit Eduard VI. wurde jedoch das erste "Book of Common Prayer" am Pfingstfest 1549 eingeführt, unter maßgeblicher Arbeit von Thomas Cranmer, Erzbischof von Canterbury. Damit wurde auch eine Tradition eingeleitet, nach der sich der Anglikanismus vorrangig durch eine episkopale Kirchenordnung und eine einheitliche gottesdienstliche Praxis definiert.

Nach einer kurzen Zwischenepisode unter der römisch-katholischen Monarchin Maria I. setzte sich die Reformbewegung der englischen Kirche unter Elisabeth I. und den ersten beiden Stuart-Königen fort. Es entwickelte sich ein sich vertiefender Streit zwischen Puritanern, die eine stärker reformierte Theologie verfolgten, und Theologen wie Lancelot Andrewes, die eine katholischere Position einnahmen. Dieser Streit verursachte unter anderem den Englischen Bürgerkrieg. Nach der Restauration der Stuarts unter Karl II. wurden diese Streitigkeiten mit einem neuen "Book of Common Prayer" (1661) beigelegt. Dies ist auch heute noch die offizielle Version, die in England neben dem Book of Alternative Services Anwendung findet.

Weitgehend unbekannt ist die Abspaltung einer kleinen Glaubensgemeinschaft im Gefolge der Glorious Revolution 1688/89. Die Gruppe der Non-Juror spaltete sich aufgrund ihrer strengen Auslegung der anglikanischen Staatsvorstellungen ab und griff liturgisch auf das erste Book of Common Prayer zurück. Auf die Gesamtentwicklung der Church of England hatte diese kurzlebige Glaubensgemeinschaft zwar keinen Einfluss; in Schottland jedoch führte die Weigerung der Bischöfe, den Treueid zu leisten, zur Etablierung der presbyterianischen Kirche und folglich zur Gründung einer eigenen anglikanischen Kirche, der Scottish Episcopal Church.

Nach der amerikanischen Unabhängigkeit bildete sich auch in den USA eine weitere anglikanische Kirche, die nunmehr auch neben der nichtetablierten episkopalen Kirche Schottlands existierte. Es folgten auch in anderen Ländern weitere Kirchen, die aber alle zusammen im sog. Anglikanischen Kommunion eine Kirchengemeinschaft pflegen.

Im 19. Jahrhundert, unter dem Einfluss der romantischen Bewegung, fand eine neue Hinwendung zu liturgischer Tradition statt. Diese Bewegung fand ihren Ausgang in Oxford und wurde deshalb Oxford-Bewegung genannt. Zu ihren Unterstützern gehörte John Henry Newman. Siehe auch Anglo-Katholizismus (High Church), Low Church.

In der zweiten Hälfte des 20. Jahrhunderts fand in vielen anglikanischen Kirchen eine liturgische Reformbewegung statt, die zu neuen "Books of Common Prayer" führten. Diese waren enger an altkirchliche Liturgien angelehnt, enthielten jedoch eine modernere Sprache und Theologie. Parallel dazu wurde die Priesterschaft auch für Frauen geöffnet. Seit den späten 1980er Jahren wurden auch die ersten Bischöfinnen in apostolischer Nachfolge geweiht.

Die Anglikanische Kirchengemeinschaft ist Mitglied im Ökumenischen Rat der Kirchen. Sie steht darüber hinaus in Kirchengemeinschaft mit der Utrechter Union der Altkatholischen Kirchen, der Unabhängigen Philippinischen Kirche und der indischen Mar-Thoma-Kirche. 2003 unterzeichneten methodistische und anglikanische Vertreter ein Abkommen, das die Zusammenarbeit zwischen den beiden Denominationen – u. a. gemeinsame Gotteshäuser und -dienste – stärken und die Beziehungen vertiefen soll.

Uneinigkeit über die Zulassung von Frauen zu den Weihen führt bis in die Gegenwart zu Kontroversen innerhalb der anglikanischen Kirchen. Für die Mitgliedskirchen der Anglikanischen Gemeinschaft sind nur die Prinzipien des Lambeth-Quadrilateral verbindlich. Die Frage der Frauenordination wird darin nicht behandelt, daher gibt es keine einheitliche für alle Gliedkirchen bindende Regelung. Die einzelnen Kirchenprovinzen vertreten daher unterschiedliche Haltungen, manche lehnen die Frauenordination grundsätzlich ab, manche erlauben die Weihe zum Diakon, andere auch zum Priestertum oder Bischofsamt.

Als erste Frau empfing Florence Li Tim-Oi 1944 in Hong Kong durch Ronald Owen Hall, den Bischof von Victoria, Hongkong und Macau, die anglikanische Priesterweihe. Dies gilt allerdings als kriegsbedingter Einzelfall, nach Kriegsende ließ sie ihr Priesteramt zur Vermeidung eines Disputs mit dem Erzbischof von Canterbury bereits ab 1945 ruhen. Erst in den 1970er Jahren wurde die Frauenordination in den Kirchen der USA, Kanadas und Neuseelands etabliert. Die erste anglikanische Bischöfin war Barbara Clementine Harris, die 1989 zur Suffraganbischöfin der Episcopal Diocese of Massachusetts geweiht wurde. Die erste Diözesanbischöfin war von 1990 bis 2004 Penny Jamieson in der Diocese of Dunedin in Neuseeland. Am 18. Juni 2006 wurde Katharine Jefferts Schori als "Presiding Bishop" an die Spitze der Episkopalkirche in den USA gewählt und somit als erste Frau Primas einer anglikanischen Kirche. Die Generalsynode der Church of England hat die Zulassung von Frauen zum Bischofsamt mehrheitlich als „theologisch gerechtfertigt“ bezeichnet. Während der Antrag bei den Bischöfen und dem Klerus die notwendige Zweidrittelmehrheit erhielt, verfehlte er sie bei den Laienvertretern um fünf Stimmen. Eine Arbeitsgruppe sollte eine entsprechende Kirchengesetzgebung auf den Weg bringen. Auf der Generalsynode 2008 wurde das Thema erneut diskutiert. Am 7. Juli 2008 beschloss die Synode die Einrichtung einer Arbeitsgruppe zur Regelung der Ordination von Frauen zum Bischofsamt. Dabei sollten die Wünsche der Mitglieder, die aus theologischen Gründen die Ordination von Bischöfinnen ablehnen, berücksichtigt werden. Es gab den Vorschlag, in jeder Kirchenprovinz nötigenfalls einen männlichen Suffragansitz einzurichten, dem sich Gemeinden anschließen können, die die Ordination von Bischöfinnen ablehnen. Er wurde bis 2010 entworfen und bis zum Februar von 42 von 44 Diözesen angenommen. Bei der Generalsynode im November 2012 erreichte dieses Kirchengesetz jedoch nicht die nötige Mehrheit bei den Laienmitgliedern. Erst 2014 wurden die nötigen Mehrheiten auf der Generalsynode erreicht. Am 25. Januar 2015 wurde mit Libby Lane die erste Bischöfin in der Church of England ordiniert. Zur ersten Diözesanbischöfin – von Gloucester – wurde am 26. März 2015 Rachel Treweek ernannt. Zur ersten anglikanischen Bischöfin der Church of Ireland wurde im September 2013 Pat Storey geweiht. Die erste anglikanische Bischöfin in der Church in Wales, Joanna Penberthy, wurde am 30. November 2016 in ihr Amt geweiht.

Die 38 Provinzen vertreten unterschiedliche Positionen zur Frauenordination:


Die weltweite Anglikanische Kirchengemeinschaft befindet sich seit der Bischofsweihe von Gene Robinson am 2. November 2003 in New Hampshire (USA) in einem Diskussionsprozess, bei dem die Einheit der Kirchengemeinschaft bedroht wird. Nicht nur der Vollzug der Bischofsweihe hat zu Kontroversen geführt, sondern auch die parallel verlaufende Entscheidung der kanadischen Diözese von New Westminster, Riten für die Segnung gleichgeschlechtlicher Paare zu entwickeln. Da Robinson seine Beziehungen zu seinem gleichgeschlechtlichen Partner nicht verheimlicht, wurde mit seiner Weihe erneut ersichtlich, dass es zum Thema Homosexualität innerhalb der Anglikanischen Kommunion Meinungsverschiedenheiten gibt, obwohl dies bereits auf den Lambeth-Konferenzen der Jahre 1988 und 1998 deutlich war. Konservative Geistliche und Mitglieder (vor allem aus Asien und Afrika, aber auch aus der American Anglican Council) sehen diesen Schritt als unvereinbar mit der bislang vertretenen Lehre der Kirche und als Bruch der Kommunion. Zwar hatten die Konferenzen mehrheitlich festgestellt, dass homosexuelle Praxis mit der Heiligen Schrift unvereinbar sei; allerdings hat eine Lambeth-Konferenz im Anglikanismus nicht den Status einer gesetzgebenden Körperschaft oder einer Lehrinstanz, sondern kann nur Konsens feststellen, solange und soweit dieser vorhanden ist, denn der Anglikanismus ist nicht hierarchisch, sondern dezentral gegliedert.

Innerhalb der Anglikanischen Gemeinschaft hat die Scottish Episcopal Church und in der nicht-anglikanischen Ökumene auch die Metropolitan Community Church die Entscheidungen der nordamerikanischen Kirchen begrüßt. Die Befürworter der Weihe (vor allem aus Nordamerika, Europa, Neuseeland und Südafrika) betonen einerseits die anglikanische Tradition, dass die Ortskirche ihre Angelegenheiten vor Ort regeln darf, andererseits vertreten sie die Auffassung, dass homosexuelle und heterosexuelle Christen gleichwürdig sind, Leitungsämter der Kirche auszufüllen.

Am 19. Februar 2007 haben die anglikanischen Kirchenoberhäupter in Daressalam eine Verlautbarung veröffentlicht, in welcher sie die US-amerikanische Episkopalkirche zu einer Umkehr von ihrer Haltung zur Homosexualität und zur Weihe homosexueller Bischöfe auffordern Innerhalb der US-Kirche bestehen erhebliche Bedenken, dass die Forderungen der anderen Kirchen mit dem Kirchenrecht der US-Kirche selber vereinbar sind, und man hält die Vorschläge darüber hinaus für „eine beispiellose Machtverschiebung zugunsten der Primasse“. Die US-Kirche kritisierte auch die fehlende Bereitschaft der anderen Kirchenoberhäupter, stärkere Schritte gegen die Kriminalisierung von Homosexualität in anderen Ländern zu unternehmen, und die Verweigerung vieler anglikanischer Landeskirchen, in den von mehreren Lambeth-Konferenzen (1998, 2008) und vom Windsor Report verlangten "Listening Process" (Dialog) mit homosexuellen Anglikanern zu treten. Im März 2010 wurde die offen lesbische Mary Douglas Glasspool zur Bischöfin der Episkopalkirche im Bistum Los Angeles gewählt. Die Bischöfe der anglikanischen Kirche im südlichen Afrika wandten sich im Mai 2010 gegen jede Form der Kriminalisierung von Homosexuellen.

Im September 2013 hat die Church of England erlaubt, dass homosexuelle anglikanische Geistliche eine Lebenspartnerschaft eingehen dürfen, solange sie sexuell enthaltsam leben. Im April 2014 ließ sich ein homosexueller anglikanischer Priester standesamtlich trauen und wurde in der Folge entlassen. Im November 2013 erlaubte die Church of England die Segnung gleichgeschlechtlicher Paare in einem Gottesdienst. 2017 erlaubte die Anglikanische Kirche in Schottland die Trauung gleichgeschlechtlicher Paare, so wie diese bereits in der Anglikanischen Kirche in den Vereinigten Staaten und in der Anglikanischen Kirche in Kanada kirchenrechtlich ermöglicht wurde.



</doc>
<doc id="378" url="https://de.wikipedia.org/wiki?curid=378" title="Artemisia (Pflanze)">
Artemisia (Pflanze)

Artemisia ist eine Pflanzengattung in der Familie der Korbblütler (Asteraceae). Einzelne Arten werden Beifuß, Wermut, Stabwurz oder Edelraute genannt. Zu dieser artenreichen Gattung gehören 250 bis 500 Arten, die hauptsächlich in den gemäßigten Zonen vorkommen. Fast alle Arten haben ihre Verbreitungsgebiete auf der Nordhalbkugel in Nordamerika und Eurasien. Nur wenige Arten findet man in Südamerika und Afrika.

"Artemisia" wurde bereits bei Dioskurides und Plinius erwähnt, die damit "Artemisia vulgaris" und ähnliche Arten beschrieben. Angeblich stammt der Name der Gattung vom Namen der griechischen Göttin Artemis Ilithyia (bzw. Artemis Eileithyia), der Schwester des Heilgottes Apollon sowie Geburtshelferin und Beschützerin der Gebärenden, wegen der Wirksamkeit bei Frauenleiden, oder von der Königin Artemisia, Gattin des Mausolos von Halikarnassos, die die Heilkräfte bekannt gemacht haben soll.

"Artemisia"-Arten sind ein- bis zweijährige oder meist ausdauernde krautige Pflanzen, Halbsträucher und seltener Sträucher. Sie erreichen je nach Art Wuchshöhen von 3 bis 350 Zentimeter. Die Pflanzenteile sind meistens kahl und mehr oder weniger aromatisch. Die wechselständig angeordneten Laubblätter sind gestielt oder ungestielt. Die Blattspreiten sind einfach bis mehrfach fiederteilig.

In traubigen oder rispigen Blütenständen sind meistens zahlreich, kleine, oft nickende körbchenförmige Teilblütenstände angeordnet. Die Hülle ist glockig, zylindrisch, eiförmig bis kugelig und besteht aus zahlreichen, dachziegelartig angeordneten, angedrückten und am Rand meist trockenhäutigen Hüllblättern. Der Körbchenboden ist flach, kahl oder mehr oder weniger behaart und ohne Spreublätter.

Die Blüten sind alle röhrig, entweder homogam, zwittrig oder heterogam. Die in der Mitte stehenden Blüten sind zwittrig und die randständigen weiblich. Die Antheren haben meistens lanzettliche Anhängsel an der Spitze. Die Schenkel der Griffel ragen bei den weiblichen Randblüten oft weit heraus.

Die Achänen sind zylindrisch oder zusammengedrückt und haben keine starken Rippen, oft mehr oder weniger verschleimend. Ein Pappus fehlt meist.

Die Erstveröffentlichung der Gattung "Artemisia" erfolgte 1753 durch Carl von Linné in "Species Plantarum", Band 2, Seite 845. Synonyme für "Artemisia" sind "Absinthium" , "Chamartemisia" , "Elachanthemum" & , "Oligosporus" und "Seriphidium" () 

Die Gattung "Artemisia" umfasst etwa 250 bis 500 Arten (Auswahl):


Schon in der Antike waren "Artemisia"-Arten als Heil- und Gewürzpflanzen bekannt.

Fast alle "Artemisia"-Arten enthalten viel Bitterstoffe und ätherische Öle. Sie werden vor allem wegen ihrer dekorativen, oft duftenden und bisweilen insektenabwehrenden Laubblätter kultiviert.

Der Einjährige Beifuß wird in der Traditionellen Chinesischen Medizin als Malaria-Mittel genutzt. Auf Extrakten aus dem Einjährigen Beifuß beruht die von der WHO empfohlene Therapie gegen Malaria.



</doc>
<doc id="379" url="https://de.wikipedia.org/wiki?curid=379" title="Adam und Eva">
Adam und Eva

Adam und Eva waren nach der biblischen Erzählung im Buch Genesis (Kapitel 2 bis 5) das erste Menschenpaar und Stammeltern aller Menschen. Auch der Koran erwähnt Adam und Eva.
Adam wurde demnach von Gott aus Staub erschaffen, danach wurde ihm der Lebensatem eingehaucht. Adam gab den Tieren Namen, fand aber kein partnerschaftliches Gegenüber. Darauf ließ Gott Adam in einen tiefen Schlaf fallen, entnahm ihm eine Rippe bzw. Seite und schuf aus dieser sein Gegenüber "Eva". Wurde in der Erzählung bis dahin immer von „dem Menschen“ (Adam) gesprochen, erkennt Adam in der Begegnung mit dem neuen Wesen in sich den Mann und in seinem Gegenüber die Frau.

Das Wort Adam (, ), welches in der Schöpfungserzählung als Eigenname gebraucht wird, bedeutet „Mensch“ (im Gegensatz zu anderen Lebewesen, insbesondere den Tieren). Auf das ähnlich klingende Wort Adamah ( „Erde / Erdboden“) wird durch den Schöpfungsakt Bezug genommen.

Der Name Eva (, oder ()) wird mit dem Verb חיה "chajah" „leben / am Leben bleiben“ in Verbindung gebracht und bedeutet daher „die Belebte“. Dieser Name wird im Alten Testament nur an zwei Stellen genannt, nämlich in und 4,1 . Zuvor wird sie stets als „Frau“ Adams bezeichnet. Das hebräischen Worte für Frau ( ) und Mann ( ) sind einander sehr ähnlich, obwohl sie nicht miteinander verwandt sind. Es handelt sich um ein Wortspiel: So wie der Mensch ("ādām") aus der Erde ("ădāmāh") hervorgeht, so geht die Frau ("iššāh") aus dem Mann ("īš") hervor.

In der Septuaginta wird der Name Adam als Eigenname αδαμ "adam" wiedergegeben, während der Name Eva mit Ζωἠ "Zoë" „Leben“ übersetzt wird. Im Neuen Testament, wo Eva nur an zwei Stellen erwähnt wird ( und ), wird der Name Eva hingegen griechisch Εὕα "Heúa" transkribiert. In der Vulgata lautet der Name schließlich "Hava", "Heva" oder "Eva".

Der erste biblische Schöpfungsbericht besagt – siehe : "Und Gott schuf den Menschen zu seinem Bilde, zum Bilde Gottes schuf er ihn, und er schuf sie als Mann und Weib." Adam und Eva leben zunächst im Garten Eden. Dort überredet sie die Schlange entgegen dem Verbot Gottes vom Baum der Erkenntnis von Gut und Böse zu essen. (Diese Schlange wird in der christlichen Tradition oft auf den Teufel bezogen. Diese Gleichsetzung findet sich schon im Neuen Testament in .) Da sich Adam und Eva nach dem Genuss der Früchte mit Feigenblättern bekleiden, könnte mit der verbotenen Frucht eine Feige gemeint sein, die in der biblischen Systematik der Früchte die 4. Frucht ist (vgl. Dtn 8,8) und auf die Zahl Vier verweist, die symbolisch für die materielle Welt steht. Das gängige Bild vom Apfel als verbotener Paradiesfrucht beruht nicht etwa auf einem Übersetzungsfehler der lateinischen Bibel, der Vulgata, sondern darauf, dass in der lateinischen Sprache „malus“ „Apfelbaum“ bedeuten kann, aber auch „schlimm, böse“, ebenso wie „malum“ „Apfel“ bedeuten kann oder „das Übel, das Schlechte, das Böse“. Daraus ergab sich ein ziemlich naheliegendes Wortspiel, zumal die Vulgata den „Baum der Erkenntnis von Gut und Böse“ aus dem hebräischen Urtext übersetzte mit „lignum scientiae boni et mali“. Die in dem Essen der verbotenen Frucht zum Ausdruck kommende Abkehr von Gottes Geboten gilt sowohl in der jüdischen als auch in der christlichen Religion als Ungehorsam gegenüber Gott, wie auch die Rebellion des Teufels gegen diesen. Hier als Übertrag vom Teufel auf den Menschen, welcher den Menschen in seiner seelischen und körperlichen Beschaffenheit veränderte. Das Christentum spricht vom Sündenfall.

Als Folge der Rebellion beschreibt die Bibel, dass Adam und Eva ihre Nacktheit erkennen, woraufhin sie sich Kleidung aus Feigenblättern anfertigen. Vor Gott versuchen sie sich zu verstecken. Zum ersten Mal ist etwas im Paradies vorhanden, was vorher nicht bekannt war: die Verletzung des Schamgefühls. Gott stellt sie zur Rede, woraufhin Adam die Schuld Eva zuschreibt und Eva der Schlange. Beide werden aus dem Garten Eden vertrieben. Eva muss fortan Kinder unter Schmerzen gebären, Adam wird der harte und mühselige Ackerbau auferlegt. Die klassischen Worte aus :
weisen zunächst auf den Ritus der Erdbestattung hin und bringen zudem nach christlicher Interpretation zum Ausdruck, dass nun der Tod in die Welt getreten ist. Zwischen Eva, der Schlange und ihren jeweiligen Nachkommen wird Feindschaft herrschen.

In der biblischen Erzählung zeugt Adam nach der Vertreibung aus dem Paradies mit Eva Kain, Abel und Set. Das biblische Buch Genesis 5,4 erwähnt außerdem nicht namentlich genannte Töchter und weitere Söhne, die Adam nach der Geburt Sets zeugte. Adams gesamtes Lebensalter wird mit 930 Jahren angegeben.

Die niederländischen Wissenschaftler Marjo C.A. Korpel und Johannes de Moor von der Protestantisch-Theologischen Universität in Amsterdam publizierten 2014 die Ergebnisse ihrer Untersuchung von Tontafeln aus Ugarit aus dem 13. Jahrhundert v. Chr., die eine frühe Version des Mythos von Adam und Eva enthalten. Rund 800 Jahre älter als die Fassung im 1. Buch Mose, erzählt dieser in ugaritischer Sprache in Keilschrift verfasste Text von einem Kampf zwischen dem Schöpfergott El, dem höchsten der Götter, und einem Widersacher namens Horon, der El stürzen möchte: Die Götter leben in einem paradiesischen Garten, in dem auch der Unsterblichkeit verleihende Baum des Lebens wächst. Horon wird von dort verbannt, woraufhin er die Gestalt einer großen Schlange annimmt, den Baum des Lebens vergiftet und in einen Baum des Todes verwandelt, der alles Leben auf der Erde bedroht. Die Götter wählen einen aus ihrer Mitte aus, um den Abtrünnigen zu bekämpfen. Doch der Auserwählte, Adam, scheitert, als Horon in Form der Schlange ihn beißt und ihn so seiner Unsterblichkeit beraubt. Den verbliebenen Göttern gelingt es, Horon zu zwingen, den vergifteten Baum zu entwurzeln. So bleibt die Unsterblichkeit zwar verloren, aber das Leben kann weitergehen. Die Sonnengöttin erschafft als Partnerin für den nun sterblichen Adam eine „gute Frau“. Sie und Adam erlangen, indem sie Nachkommen zeugen, eine neue Form der Unsterblichkeit.

Vorstellungen zu einem Stammelternpaar gibt es auch in anderen Schöpfungsberichten. So findet sich in der germanischen Mythologie die Geschichte von Ask und Embla.

Aus dem Vers : "Es ist nicht gut, dass der Mensch allein sei" wird nach rabbinischer Deutung die Verpflichtung des Menschen zur Eheschließung abgeleitet.

In der Sicht des hellenistisch-jüdischen Philosophen Philo von Alexandria hat die Rebellion die folgende Bedeutung: Es existieren zwei Schöpfungen, die des himmlischen und die des irdischen, aus Staub geschaffenen und der Vergänglichkeit unterworfenen Menschen. Adam steht für die Vernunft, Eva für die Sinnlichkeit, die Schlange für die Lust. Der Aufstand gegen Gott entsteht durch eine Störung der betrachtenden Vernunft, wobei die Schlange als Vehikel der Versuchung dient.

Die lateinische Kirche entwickelt aus der biblischen Erzählung den Begriff der Erbsünde, sie begreift Adam als Typ und Haupt-Figur der Menschheit. Als solcher kann er, wie der Apostel Paulus im Römerbrief schreibt, ursächlich für den Tod aller Menschen sein. Diesem „alten (Menschentypus) Adam“ wird Jesus Christus als der eine „neue Adam“ gegenübergestellt, dessen Kreuzestod im Gehorsam gegenüber dem Willen des Vaters () und dessen Auferstehung ein Leben im Sieg über die Mächte des Todes hinaus ermöglichen (vgl. , siehe auch Sündenfall). Diese Interpretation wird aber nicht von der Ostkirche akzeptiert, wo die Erbsünde unbekannt ist; es heißt nur, dass der Tod durch Adam und Eva in die Welt gebracht wurde und in der Auferstehung Jesu das Paradies wieder erschlossen ist (vgl. die Anastasis-Ikonen, wo Adam und Eva an der Hand des Auferstandenen aus dem Todesgrab herausgeführt werden, siehe unten).

Der Gegensatz von „Geist“ und „Fleisch“, der für Paulus grundlegend ist und der bei ihm auch hinter dem Gegensatz zwischen dem „neuen Adam“ Jesus und dem „alten Adam“ steht (vgl. , , ), ist schon in den ersten Kapiteln der Genesis zu finden. „Alle, die zu Jesus Christus gehören, haben das Fleisch und damit ihre Leidenschaften und Begierden gekreuzigt. Wenn wir aus dem Geist leben, dann wollen wir dem Geist auch folgen“ (). Das „Kreuzigen“ bedeutet nicht töten, sondern der Bestimmung durch den Geist unterwerfen im Sinn der inneren Beschneidung des Herzens durch den Geist (vgl. ; ; vgl. ). Das so beschnittene Herz hat wieder Zugang zur Gnade und zur Sehkraft der „Hoffnung auf die Herrlichkeit Gottes“ () und die Unsterblichkeit (), die den "Adam paradisus" im Gnadenstand des Paradieses auszeichnet (vgl. , , ). Auch die beiden Bäume in der einen Mitte des Paradieses lassen sich auf diesen Gegensatz von Geist (Lebensbaum) und Fleisch (Erkenntnisbaum) zurückführen.

In der Theologie der Mormonen ist Adam die Verkörperung des Erzengels Michael.

Die Sicht Philos von Alexandrien wurde auch von Paulus und den Kirchenvätern aufgegriffen und weiterentwickelt. Danach verkörpern Adam und Eva oder das Männliche und Weibliche die zwei Seiten der menschlichen Wirklichkeit: das Innere und Erinnernde des Geistes (hebr. "sachar" bedeutet „männlich“ und „erinnern“) sowie das Äußere, Erscheinende oder Umhüllende des Fleisches, welches dann im Bund der Beschneidung zurückgedrängt wird.

Die geöffnete „Seite“ Adams, aus der heraus Gott die Frau bildet, wird mit „Fleisch“ geschlossen (). Die „Rippe“ symbolisiert hier die Mondsichel. Hebr. "zela" übersetzt Othmar Schilling mit „das Gekrümmte“; zu verweisen ist auch auf "zelem" („Bild“) und "zel" („Schatten“). Luna galt in den alten Kulturen als „Urgrund aller Geburt“ (Johannes Lydos) oder „Mutter des irdischen Lebens“ (vgl. ), deren monatlicher Zyklus die Menstruation der Frau bestimmt.

Auf den Mond verweist auch der Zahlenwert 19 von Eva, hebr. "Chewa(h)" (wie "Chaja" für „Tier“), in Zahlen 8-6-5, in der Summe 19. Das Mondjahr kann wegen der Differenz zum Sonnenjahr von knapp elf Tagen nicht einfach in zwölf gleich große Teile eingeteilt werden, sondern muss durch das Einschalten eines 13. Monats immer wieder an das Sonnenjahr angepasst werden. Dabei beträgt die Differenz in 19 Jahren genau sieben Mond-Monate. Diese 19 Jahre nennt man „ein ‚mechasor‘, eine Wiederholung, eine Zurückkehr, und somit auch Kreis oder Zyklus.“ Im Bildtypus der Maria Immaculata erscheint die Mutter Jesu als ‚neue Eva‘ auf der Mondsichel stehend und der Schlange (des nur zeitlich-irdischen Werdens und Vergehens) den Kopf zertretend (nach ; ).

Der frühchristliche Bischof Theophilus von Antiochien sagt in seiner Auslegung der Erschaffung von Sonne und Mond am 4. Schöpfungstag: „Die Sonne ist das Bild Gottes, der Mond das des Menschen“; im monatlich ‚sterbenden‘ und dann wieder erscheinenden Mond sieht er also „ein Sinnbild des Menschen“ und zugleich „ein Vorbild unserer künftigen Auferstehung“ (An Autolykus II, 15). Auch deshalb wird Ostern am ersten Sonntag nach dem Frühlings-Vollmond gefeiert. Im gleichen Sinn deutet dann Bonaventura (13. Jh.) Sonne und Mond als Sinnbild für das In-eins-Sein von Gottheit und Menschheit in Jesus Christus: „Das Licht des Lammes gibt ihm [Jerusalem] Schönheit und Glanz, seine Gottheit leuchtet an Stelle der Sonne, seine Menschheit an Stelle des Mondes […]“

Auf Darstellungen von der Erschaffung der Frau aus dem Mann werden beide häufig von Sonne und Mond flankiert (vgl. zum Beispiel den Schalldeckel der Kanzel der Klosterkirche der ehemaligen Zisterzienserabtei Bebenhausen bei Tübingen).

Auch der Koran kennt die Geschichte von Adam und Eva. Hier spielt Iblis eine wichtige Rolle. Aus Überheblichkeit widersetzt er sich als einziger dem Befehl Gottes, sich vor Adam niederzuwerfen. Daraufhin wird er von Gott aus dem Paradies verwiesen, erbittet sich aber Aufschub bis zum Tag des jüngsten Gerichts, um nun zu versuchen, die Menschen ebenfalls abbitten zu lassen – was ihm auch gelingen soll. Dies gilt im Islam als irdische Prüfung (Koran: Sure 15, Vers 34–40). Gott warnt die Menschen vor diesem Versucher, sie lassen sich aber betören und verführen (Sure 7, Vers 22). Im Gegensatz zur christlichen Überlieferung teilen sich laut islamischer Lehre Adam und Eva die Schuld am Verzehr der verbotenen Frucht. (Sure 7, Vers 22)

Nach dem Koran ist Adams Sünde ein Fehltritt (Sure 2, Vers 36), nicht aber Abfall von Gott und Zerstörung der Beziehung zu ihm. Deshalb ist die Folge auch nicht so schwerwiegend wie im biblischen Bericht: Statt der Ankündigung: „… sonst werdet ihr sterben“ warnt Gott den Menschen vor Satan: „Dass er euch nur nicht aus dem Paradies vertreibt und dich unglücklich macht!“ (Sure 20, Vers 117) Durch die Sünde schadet der Mensch nur sich selber: „Unser Herr, wir haben uns selbst Unrecht getan.“ (Sure 7, Vers 23)

„Hierauf erwählte ihn sein Herr und er wandte sich ihm wieder zu und leitete ihn recht.“ (Sure 20, Vers 122) Adam und Eva werden zwar aus dem Paradies vertrieben, aber ihnen wird gesagt: „Wenn dann von mir eine Rechtleitung zu euch kommt, dann haben diejenigen, die meiner Rechtleitung folgen, nichts zu befürchten und sie werden nicht traurig sein.“ (Sure 2, Vers 38f)

Adam und Eva wird von Gott explizit im Koran verziehen: Sure 2 (al-Baqara), Vers 37 am Ende der Erzählung der Adamgeschichte: „Da empfing Adam von seinem Herren Worte (Bittgebete). Und er wandte sich zu ihm zu. Er ist ja der Vergebende, sich wieder Zuwendende und der Barmherzige“. Diese Stelle steht im Gegensatz zu einem Glauben an eine „Erbsünde“. Jeder Mensch wird mit einem „weißen Blatt“ geboren, heißt es in Sprüchen des Propheten als Bestätigung. Somit wird nach islamischer Lehre jeder Mensch sündenfrei geboren.


Die künstlerischen Darstellungen des Mythos von Adam und Eva sind außerordentlich zahlreich und über Jahrhunderte immer wieder neu variiert und verändert worden. Dabei bewegen sich die Darstellungen zwischen verschiedenen Polen der theologischen Deutung des Geschehens:

In einigen Werken erscheinen Adam und Eva in ihrer paradiesischen Gottesnähe. Kurt Flasch verweist etwa auf ein Deckengemälde aus der Zeit um 1200 in der Klosterkirche St. Michael in Hildesheim, das „Eva und Adam als Herrscherpaar in paradiesischer Herrlichkeit“ zeige. Eva erscheint vielen Künstlern als mächtige Urmutter der Menschheit, als Geschenk Gottes an Adam, erst aus ihrer Tat erwachsen Zeit und die menschliche Geschichte.

In entsprechenden Darstellungen der Schöpfungsszene, in der Gott Eva aus der Seite Adams erstehen lässt, erscheint die starke Eva als Bindeglied zwischen Adam und Gott, so im abgebildeten Relief des Doms zu Orvieto oder einem dem frühen, Donatello zugeschriebenen Terracottarelief im Dommuseum in Florenz.

Auf der anderen Seite ist die Zuweisung der Hauptschuld am Sündenfall an Eva ein Thema der Kunst. Der Sündenfall wird zum Ausgangspunkt der Herrschaft des Mannes über die Frau, Eva zur Gegenfigur der jungfräulichen Maria und zum Ursprung allen Elends der Menschheitsgeschichte.

Ebenfalls um 1200 gestaltet ein unbekannter Künstler die Vertreibung aus dem Paradies an einem Kapitell der Kathedrale von Clermont-Ferrand.

„Der Cherubim verschließt das Paradiestor; er zerrt Adam an den Haaren heraus. Eva und Adam sind beide Bestrafte, Verjagte, Hinausgeworfene, aber wie verschieden ist ihre Lage: Adam steht, Eva kniet oder liegt am Boden; sie ist gestürzt; ihre Position ist jetzt unter ihm, und er demonstriert dies: Er zieht sie am Haarschopf, wie der Engel ihn gepackt hat. Er setzt die Strafe fort; er gibt seiner Frau einen Fußtritt.“

Aber trotz der theologischen Legitimation dieser Negativsicht Evas über Jahrhunderte eröffnet sich in der Kunst eine Vielfalt der Akzentuierungen und Motiven zwischen den Polen Sünde/Strafe und der positiven Sicht der ersten Menschen.

Eine „positive Vollendung“ der Geschichte von Adam und Eva kennt die Ikonentradition. Die Auferstehungsikone (Anastasis), ein häufiges Motiv, zeigt nicht (wie die westliche Kunst) die Abbildung der Auferstehung Jesu selbst oder des leeren Grabes, sondern die Illustration eines Satzes aus dem Apostolischen Glaubensbekenntnis: "… hinabgestiegen in das Reich des Todes."
Der auferstandene Christus tritt die Türen des (oft personifizierten) Hades ein und zieht Adam und Eva als erste der Menschen aus dem Reich des Todes.

Ein ganz anderer Motivbereich der künstlerischen Gestaltung von Adam und Eva ist die Darstellung der Arbeit. Mit der Vertreibung aus dem Paradies beginnt der Zwang zur Arbeit, der Künstlern Darstellungsmöglichkeiten alltäglicher menschlicher Aktivitäten bietet. Dabei werden sowohl traditionelle weibliche und männliche Tätigkeitsfelder zum Gegenstand, als auch neue Arbeitsgebiete der jeweiligen Zeit.

In der Frührenaissance bot die Darstellung von Adam und Eva zudem den Künstlern eine erste Möglichkeit, Aktmalerei in einer Zeit zu betreiben, in der die Darstellung menschlicher Nacktheit noch weitgehend verpönt war.

Siehe auch:






</doc>
<doc id="383" url="https://de.wikipedia.org/wiki?curid=383" title="Anaximander">
Anaximander

Anaximander ( "Anaxímandros"; * um 610 v. Chr. in Milet; † nach 547 v. Chr. in Milet) war ein vorsokratischer griechischer Philosoph. Er gehört neben Thales und Anaximenes zu den wichtigsten Vertretern jenes philosophischen Aufbruchs, der mit Sammelbegriffen wie ionische Aufklärung und jonische Naturphilosophie bezeichnet wird.

Apollodor von Athen zufolge lebte Anaximander um 610 – 546 v. Chr. in Milet. Es ist wahrscheinlich, dass er Thales gekannt und mit ihm in enger Gedankengemeinschaft gelebt hat. Jedenfalls gilt er als Nachfolger und Schüler des Thales.

Ihn beschäftigte dasselbe Grundproblem wie Thales, nämlich die Frage nach dem Ursprung allen Seins, nach der Arché (). Dafür hielt er jedoch nicht das Wasser, sondern das stofflich unbestimmte Ápeiron (): das hinsichtlich seiner Größe „Unbegrenzte“ bzw. „Unermessliche“. Von Anaximanders Philosophie ist in eigenen Worten nur ein einziges Fragment überliefert. Dabei handelt es sich um den ersten erhaltenen griechischen Text in Prosaform. Der Großteil der philosophischen Anschauungen Anaximanders ist der zwei Jahrhunderte späteren Überlieferung des Aristoteles zu entnehmen und mit einigen Unsicherheiten behaftet.

Als bedeutender Astronom und Astrophysiker entwarf er als erster eine rein physikalische Kosmogonie. Er gründete seine Überlegungen zur Entstehung des Weltganzen ausschließlich auf Beobachtung und rationales Denken. Von ihm ging die Bezeichnung der Welt als "Kosmos" (κόσμος) und ihre Erfassung als ein planvoll geordnetes Ganzes aus. Anaximander zeichnete ebenfalls als erster nicht nur eine geographische Karte mit der damals bekannten Verteilung von Land und Meer, sondern er konstruierte auch eine "Sphäre", einen Himmelsglobus. Die Karte ist heute verschollen, wurde aber später durch Hekataios ausgewertet, aus dessen Werk eine halbwegs konkrete Darstellung der damaligen Weltsicht überliefert ist.

Nach ihm ist der Mondkrater Anaximander benannt.

Die Grundsubstanz alles Gewordenen nach Anaximander, das Apeiron, wird unterschiedlich gedeutet: als räumlich und zeitlich unbegrenzten Urstoff, als unendlich hinsichtlich Masse oder Teilbarkeit, als unbestimmt oder grenzenlos u. a.m. Der Begriff des Unermesslichen spiegelt wohl am ehesten die Offenheit der Deutungsmöglichkeiten des Apeiron. Nach Aristoteles hat Anaximander es als ein den Göttern der Volksreligion vergleichbares unsterbliches und unzerstörbares Wesen betrachtet.

Mit dem einzigen erhaltenen Anaximander-Fragment liegt der erste schriftlich gefasste und überlieferte Satz der griechischen Philosophie überhaupt vor. Allerdings ist die diesbezügliche Forschung uneins, in welchem Umfang das Überlieferungsgut tatsächlich authentisch auf Anaximander zurückgeht. Die Kernaussage lautet:

Die gleichsam gesetzmäßige wechselseitige Ablösung gegenstrebiger Wirkkräfte oder Substanzen in einem kontinuierlichen und ausgeglichenen Prozess dürfte für die beständige Ordnung des Kosmos stehen: ein dem Wechsel und der Veränderung ausgesetztes und doch in sich stabiles System. Uneinig ist die Forschung darüber, ob auch das Apeiron an diesem Geschehen beteiligt ist oder ob es sich um einen rein innerweltlichen Ausgleichsprozess handelt, sodass die Wirkung des Apeiron sich allein auf die Phase der Weltentstehung beschränkte. Im anderen Fall kämen auch Vorstellungen von einer Mehrzahl neben- oder nacheinander existierender Welten in Betracht. Robinson erwägt, dass Anaximander sich das Universum als einen ewigen Prozess gedacht haben könnte, „in dem eine unendliche Anzahl galaktischer Systeme aus dem Apeiron geboren und wieder in es aufgenommen wird. Damit hätte er auf brillante Weise die Weltsicht der Atomisten Demokrit und Leukipp vorweggenommen, die für gewöhnlich als deren eigene Leistung betrachtet wird.“

Anaximander meinte, bei der Entstehung des heutigen, geordneten Universums habe sich aus dem Ewigen ein Wärme- und Kältezeugendes abgesondert, und daraus sei eine Feuerkugel um die die Erde umgebende Luft gewachsen, wie um einen Baum die Rinde.

Die Gestirne entstehen laut Anaximander durch die geplatzte Feuerkugel, indem das abgespaltene Feuer von Luft eingeschlossen wird. An ihnen befänden sich gewisse röhrenartige Durchgänge als Ausblasestellen; sie seien dort als "Gestirne" sichtbar. In gleicher Weise entstünden auch die Finsternisse, nämlich durch Verriegelung der Ausblasestellen.

Das Meer sei ein Überrest des ursprünglich Feuchten. „Ursprünglich war die ganze Oberfläche der Erde feucht gewesen. Wie sie aber dann von der Sonne ausgetrocknet wurde, verdunstete allmählich der eine Teil. Es entstanden dadurch die Winde und die Wenden von Sonne und Mond, aus dem übrigen Teil hingegen das Meer. Daher würde es durch Austrocknung immer weniger Wasser haben, und schließlich würde es allmählich ganz trocken werden“ (Aristoteles über Anaximander). Aus einem Teil dieses Feuchten, das durch die Sonne verdampfe, entstünden die Winde, indem die feinsten Ausdünstungen der Luft sich ausschieden und, wenn sie sich sammelten, in Bewegung gerieten. Auch die Sonnen- und Mondwenden geschähen, weil diese eben, jener Dämpfe und Ausdünstungen wegen, ihre Wenden vollführten, indem sie sich solchen Orten zuwendeten, wo ihnen die Zufuhr dieser Ausdünstung gewährleistet sei.

Die Erde sei das, was vom ursprünglich Feuchten an den hohlen Stellen der Erde übrig geblieben sei. Anaximander meinte, die Erde sei schwebend, von nichts überwältigt und in Beharrung ruhend infolge ihres gleichen Abstandes von allen "Himmelskreisen". Ihre Gestalt sei rund, gewölbt und ähnele in der Art eines steinernen Säulensegments einem Zylinder. Wir stünden auf der einen ihrer Grundflächen; die andere sei dieser entgegengesetzt. Regengüsse bildeten sich aus der Ausdünstung, welche infolge der Sonnenstrahlung aus der Erde hervorgerufen werde. Blitze entstünden, indem der Wind sich in die Wolken hineinstürze und sie auseinanderschlage.

Die Entstehung der Menschheit führte Anaximander auf andere Lebewesen zurück. Ihm war aufgefallen, dass der Mensch im Vergleich zu anderen Arten im Frühstadium seiner Entwicklung sehr lange Zeit benötigt, bis er für die Selbstversorgung und das Überleben aus eigenen Kräften sorgen kann. Deshalb nahm er an, dass die ersten Menschen aus Tieren hervorgegangen waren, und zwar aus Fischen oder fischähnlichen Lebewesen. Denn den Ursprung des Lebendigen suchte er im Wasser; das Leben war für ihn eine Spontanentstehung aus dem feuchten Milieu: „Anaximander sagt, die ersten Lebewesen seien im Feuchten entstanden und von stachligen Rinden umgeben gewesen. Im weiteren Verlauf ihrer Lebenszeit seien sie auf das trockene Land gegangen und hätten, nachdem die sie umgebende Rinde aufgeplatzt sei, ihr Leben noch für kurze Zeit auf andere Weise verbracht.“

Die Seele hielt Anaximander für "luftartig". Der Vorstellung von der Seele als "Aër" mag die Verbindung mit dem Leben bzw. dem Ein- und Ausatmen zugrunde gelegen haben. Unklar ist, ob er zwischen der Atemseele des Menschen und der anderer Lebewesen unterschied.

Wie sich Anaximanders Auffassung von Apeiron und Kosmos zu seiner Vorstellung von der Seele verhielt, ob es zwischen ihnen überhaupt eine Beziehung gab, ist ungewiss. Da Anaximander die Seele für luftartig hielt, vermuten manche, dass er der Seele Unsterblichkeit zusprach. Ob er an eine Beseelung des Kosmos, ferner an eine Allbeseelung, ähnlich wie sie sich Thales vermutlich vorgestellt hatte, und darüber hinaus an die Unsterblichkeit individueller Seelen dachte, bleibt dahingestellt.


Übersichtsdarstellungen in Handbüchern
Einführungen und Untersuchungen
Rezeption

Textausgaben
Literatur


</doc>
<doc id="385" url="https://de.wikipedia.org/wiki?curid=385" title="Anita">
Anita

Anita ist ein weiblicher Vorname.

Anita ist eine spanische, portugiesische und kroatische Verkleinerungsform von Anna (hebräisch für "die Begnadete") oder Kurzform von Juanita (siehe Johanna).

Die Vergabe des Namens "Anita" setzt im deutschsprachigen Raum gegen Ende des 19. Jahrhunderts ein. Wahrscheinlich erfolgte die Entlehnung nicht direkt aus dem Spanischen, sondern aus dem italienischen Raum. In Italien geht die Verbreitung dieses Namens auf die Gattin des Freiheitskämpfers Giuseppe Garibaldi, Anita (Anna Maria) Ribeira zurück.

Namenstag ist für die im Ursprung hebräische Form der 26. Juli.




</doc>
<doc id="386" url="https://de.wikipedia.org/wiki?curid=386" title="Andreas">
Andreas

Andreas ist ein männlicher Vorname, der auch als Familienname vorkommt.

Der Name Andreas stammt aus Griechenland und findet dort erstmals in hellenistischer Zeit um 250 v. Chr. Erwähnung. Andere Varianten des Namens sind jedoch früher belegt, so taucht beispielsweise in der Olympialiste um 688 v. Chr. der Name "Androlos" auf.

Anders als vielfach behauptet, leitet sich der Name Andreas nicht direkt von griech. "andrós" ab, dem Genitiv des Substantivs "anêr" („Mann“). Andreas stammt vom altgriechischen Wort "andreia" für „Tapferkeit, Tüchtigkeit, Mannhaftigkeit“ beziehungsweise vom Adjektiv "andreios" für „mannhaft, tapfer, tüchtig“ ab. Diese sind jedoch Ableitungen von „anêr“.

Die Verbreitung des Namens geht auf den Apostel Andreas zurück. Durch das lateinische Christentum gelangte er erstmals nach Westeuropa. Aber auch in anderen Gebieten wie Palästina verbreitete sich der Name schnell. Nach England kam der Name „Andreas“ beziehungsweise „Andreus“ durch den Einfall der Normannen 1066. Dort ist der Name seit 1086 belegt. Das romanische "Andreus" wandelte sich mit der Zeit zu "Andreu" und schließlich zum englischen "Andrew". Seit dem Mittelalter tritt der Name in ganz Europa häufig auf, besonders in England (13. Jahrhundert), Schottland und Skandinavien. Einen weiteren „Aufschwung“ gab es nach der Reformation.

Wenngleich "-as" eine sehr verbreitete Namensendung in Griechenland ist, findet sich der Nachname trotz der Beliebtheit des Vornamens selten ohne zusätzliche Endungen.

Der häufigste schwedische Nachname ist "Andersson" („Sohn des Anders = Andreas“), was daran liegt, dass "Anders" um 1900 der häufigste Vorname in Schweden war. Zu dieser Zeit wurden aus den Patronymen Nachnamen.

Der Name Andreas war bereits im ausgehenden 19. Jahrhundert ein mäßig populärer Jungenvorname in Deutschland. Seine Popularität sank zunächst, um dann Mitte der 1940er Jahre stark anzusteigen. Von der Mitte der 1950er bis Anfang der 1980er Jahre war der Name unter den zehn beliebtesten Jungenvornamen des jeweiligen Jahres. Obwohl der Name es niemals auf Platz eins der Häufigkeitsstatistik eines Jahres schaffte, gehört er doch zu den vier insgesamt häufigsten Vornamen seit 1890 (nach Peter, Michael und Thomas). Ab den 1990er Jahren nahm seine Beliebtheit dann stark ab.

Der Namenstag von Andreas ist der 30. November (Andreastag), der Gedenktag des Apostels Andreas.

Weitere Gedenktage der römisch-katholischen Kirche sind:

Der Gedenktag des Anderl von Rinn war der 12. Juli. Anderl von Rinn wurde jedoch 1994 aus der Liste der Seligen gestrichen.



Weibliche Versionen sind Andrea, Andrina und Andriane.



</doc>
<doc id="387" url="https://de.wikipedia.org/wiki?curid=387" title="Anna">
Anna

Anna ist ein weiblicher Vorname.

Der Name kommt von dem hebräischen Vornamen Hannah ("channa", "channah") und bedeutet „Huld“, „Gnade“.

Er kann allerdings auch als weibliche Form des alten deutschen männlichen Vornamens Anno gedeutet werden oder aber mit dem keltischen Namen der Göttin Anu in Verbindung stehen.

Anna ist in der römischen Mythologie die Schwester der Königin Dido (Vergil Aeneis 4, 9).
Bei Ovid wird sie mit der römischen Göttin Anna Perenna identifiziert (Ovid Heroides 7, 191; Fasti 3,545-656).

Nach katholischer und orthodoxer Überlieferung ist „Anna“ (hebräisch Hannah) der Name der Großmutter Jesu Christi, der Mutter Marias und Gattin Joachims (hebräisch Jojakim). Siehe dazu heilige Anna und Anna selbdritt.


Namenstag ist für die im Ursprung hebräische Form der 26. Juli, der Gedenktag der heiligen Anna. Für die schwedische Variante Annika ist es heute der 21. April; 1986 bis 1992 war es der 7. August.

In dem Jahrzehnt vor 1900 bis etwa 1905 war Anna einer der häufigsten vergebenen weiblichen Vornamen und befand sich oft auf Platz eins der Häufigkeitsstatistik. Die Popularität des Namens sank in der Folgezeit auf ein mittleres Niveau. Seit 1970 ist wieder ein deutlicher Aufwärtstrend zu beobachten. In den neunziger Jahren und im neuen Jahrtausend war der Name bereits einige Mal wieder auf der Spitzenposition der populärsten Namen.

In Island ist der Name sehr populär, 2012 war er der zweithäufigste Name.

Es existierten auch männliche Träger dieses Namens:



Es gibt eine große Zahl von Persönlichkeiten mit Vornamen Anna. Siehe Wikipedia Personensuche.


Das Wort „Anna“ ist ein Palindrom.


</doc>
<doc id="410" url="https://de.wikipedia.org/wiki?curid=410" title="Auschwitz-Erlass">
Auschwitz-Erlass

Als Auschwitz-Erlass wird der Erlass des Reichsführers SS Heinrich Himmler vom 16. Dezember 1942 bezeichnet, mit dem die Deportation der innerhalb des Deutschen Reichs lebenden Sinti und Roma angeordnet wurde, um sie als Minderheiten – anders als bei vorausgegangenen individuellen oder kollektiven Deportationen – komplett zu vernichten. Dem fielen im Porajmos bis 1945 500.000 ermordete Personen zum Opfer (maximale Schätzung).

Der Erlass selbst ist nicht überliefert. Er wird jedoch in den ihm folgenden Ausführungsbestimmungen („Schnellbrief“) des Reichskriminalpolizeiamts (RKPA) vom 29. Januar 1943 als Bezug zitiert:

Der Schnellbrief trug den Titel „Einweisung von Zigeunermischlingen, Rom-Zigeunern und balkanischen Zigeunern in ein Konzentrationslager.“

Gleichartige Deportationsanordnungen ergingen am 26. und 28. Januar 1943 für die „Donau- und Alpenreichsgaue“ sowie am 29. März 1943 für den Bezirk Bialystok, das Elsass, Lothringen, Belgien, Luxemburg und die Niederlande. Gegenüber den Burgenlandroma und den ostpreußischen Roma verwies das RKPA auf ähnliche Anweisungen vom 26. Mai bzw. 1. Oktober 1941 sowie vom 6. Juli 1942.
Eine entscheidende Vorstufe des Erlasses war das "Himmler-Thierack-Abkommen" vom 17. September 1942. Es betrifft die Aufgabenteilung zwischen den NS-Behörden und wurde zwischen Reichsjustizministerium (Thierack) und dem oberstem Polizeichef (Himmler) vereinbart. Es lautete:

Darin werden die Justizbehörden (Gefängnisse, Untersuchungshaftanstalten etc) angewiesen, Gefangene direkt und ohne Verfahren an die SS zu überstellen. Die Tötungsabsicht gegenüber der Minderheit durch die Zwangsarbeit ist in kaum einem anderen offiziellen Papier so offen dargestellt worden. 

Die Deportation nach den Vorgaben des Erlasses setzte die Kategorisierung und reichsweite Erfassung der zu Deportierenden voraus. Zu der Frage, wer „Zigeuner“ sei, gab es im NS-Zigeunerdiskurs im Wesentlichen drei Meinungen:

Gemeinsam war diesen Zuschreibungsvarianten die sowohl ethnische als auch soziale Interpretation der rassenideologischen Grundposition. Demnach verlief die rassische bzw. völkische Demarkationslinie zwischen „Vollzigeunern“ und „Zigeunermischlingen“, die zusammen die „fremdrassige“ und kollektiv „asoziale“ Gruppe der „Zigeuner“ ausmachten, auf der einen und einer Vielzahl von vor allem subproletarischen Sozialgruppen „deutschblütiger Asozialer“ auf der anderen Seite. In diesem Sinn waren bereits im Gefolge der Nürnberger Gesetze seit 1936 wie bei den Ehevorschriften gegen Juden Heiraten zwischen „Deutschblütigen“ und „Vollzigeunern“ bzw. „Zigeunermischlingen“ genehmigungspflichtig.

Am 8. Dezember 1938 hatte Himmler in einem Runderlass eine „Regelung der Zigeunerfrage aus dem Wesen dieser Rasse“ angekündigt. Bestimmend für dessen Umsetzung in operative reichszentrale Vorschriften wurden die Vorstellungen von RHF und RKPA. 1937 nahm die RHF ihre Erfassungstätigkeit auf. 1940 ging deren Leiter Robert Ritter von 32.230 „Zigeunern“ im Deutschen Reich aus (einschließlich Österreich und Sudetenland, aber ausschließlich Elsass-Lothringen). Bis zum November 1942, d. h. bis kurz vor dem Auschwitz-Erlass entstanden in der RHF nach Angabe ihres Leiters 18.922 Gutachten. 2.652 davon ergaben „Nichtzigeuner“, wie sie für ein gesondertes „Landfahrersippenarchiv“ erfasst wurden. Dessen Bezugsraum beschränkte sich im Wesentlichen auf bestimmte Teilregionen im Süden des Reichs. Die Arbeiten daran wurden 1944 eingestellt, ohne dass es bis zu diesem Zeitpunkt zu Deportationen wie nach dem Auschwitz-Erlass gekommen wäre.

Eine Teilgruppe der „Nichtzigeuner“ bildeten „nach Zigeunerart lebende“ Jenische. Es gelang der RHF nicht, die Verantwortlichen für die Normierung der nationalsozialistischen Rasse- und Asozialenpolitik „davon zu überzeugen, dass die Jenischen eine relevante rassenhygienische Gruppe und Bedrohung darstellen“. Das erklärt, dass sie als Fallgruppe im Auschwitz-Erlass bzw. in dessen Ausführungsbestimmungen vom 29. Januar 1943 und demzufolge, soweit erkennbar, im „Hauptbuch“ des „Zigeunerlagers“ in Birkenau nicht vorkommen.

Der RHF und dem RKPA galten „Zigeuner“ insgesamt als eine in einem langen Zeitraum entstandene „Mischrasse“. Die Unterscheidung zwischen „stammechten Zigeunern“ und „Mischlingszigeunern“ wurde pseudowissenschaftlich mit sich aus der Abstammung ergebenden „gemischten Blutsanteilen“ begründet, wodurch die Bindung der „Mischlinge“ an traditionelle „Stammes“normierungen reduziert oder aufgegeben worden sei. Die Teilgruppe der „Mischlinge“ galt der RHF nicht zuletzt aufgrund einer angeblich ungewöhnlichen sexuellen „Hemmungslosigkeit“ als besonders gefährlich. Ihre Angehörigen würden danach streben, in den deutschen Volkskörper einzudringen.

Ähnlich sah es die Führung der SS, wenngleich sie von „rassereinen“ statt von „stammechten Zigeunern“ sprach, die sie als noch ursprüngliche „Arier“ und Forschungsobjekte in einem Reservat unterzubringen beabsichtigte, in dem ihnen zugestanden werden sollte, ein ihnen unterstelltes archaisches „Nomadentum“ auszuleben.

Der Erlass zur „Auswertung der rassenbiologischen Gutachten über zigeunerische Personen“ vom 7. August 1941 differenzierte stärker als bislang im Sinne des ethnischen Rassismus und ließ den alten Begriff des „nach Zigeunerart umherziehenden Landfahrers“ fallen. Er unterschied zwischen „Vollzigeunern bzw. stammechten Zigeunern“, „Zigeuner-Mischlingen mit vorwiegend zigeunerischem Blutsanteil“ (1. Grades, 2. Grades), „Zigeuner-Mischlingen mit vorwiegend deutschem Blutsanteil“ und „Nicht-Zigeunern“: „NZ bedeutet Nicht-Zigeuner, d. h. die Person ist oder gilt als deutschblütig“. Diese Aufgliederung lag den Gutachten und den Auflistungen der RHF zugrunde, nach denen ab Frühjahr 1943 von regionalen und lokalen Instanzen die Selektionsentscheidungen getroffen wurden. Den ganz überwiegenden Teil der „Zigeuner“ stufte die RHF als „Mischlinge“ ein. Insoweit „Zigeuner-Mischlinge mit vorwiegend deutschem Blutsanteil“ als „Nicht-Zigeuner“ geltend eingestuft werden konnten, legte eine gemeinsame Besprechung von RHF, RKPA und RSHA Mitte Januar 1943 fest, dass sie zwar „polizeilich wie Deutschblütige“ anzusehen, im übrigen aber zu sterilisieren seien.

Steht auch der Auschwitz-Erlass im allgemeinen Zusammenhang nationalsozialistischer Rassenpolitik und -hygiene, so verweist doch der Zeitpunkt auf einen weiteren Kontext: den des verstärkten Arbeitseinsatzes von KZ-Häftlingen in der Industrie, weshalb die Zahl der Inhaftierten gesteigert werden sollte.

Der Schnellbrief vom 29. Januar 1943 sah die Herausnahme einiger Gruppen aus der Deportation vor. Alle anderen über „Zigeuner“ verhängten Verfolgungsmaßnahmen blieben auch für sie in Kraft.

So wie einerseits „Nicht-Zigeuner“ bereits vom Auschwitz-Erlass selbst ausgenommen waren, sollten andererseits nach dem Schnellbrief vom 29. Januar 1943 die „reinrassigen“ oder als „im zigeunerischen Sinne gute Mischlinge“ kategorisierten Angehörigen von zwei Teilethnien der Roma – Sinti und Lalleri, von der Umsetzung des Erlasses ausgenommen sein. Die Zahl der von „Zigeunerhäuptlingen“, die das RKPA eingesetzt hatte, auf diesem Weg von der Auschwitz-Deportation Ausgenommenen war „verschwindend gering“. Sie betrug „weniger als ein Prozent“ der rund 30.000 bei Kriegsbeginn im Deutschen Reich Lebenden.

Als weitere Ausnahmegruppen nannte der Schnellbrief mit „Deutschblütigen“ Verheiratete, Wehrmachtssoldaten, Kriegsversehrte, mit Auszeichnung aus der Wehrmacht Entlassene, „sozial angepaßte Zigeunermischlinge“ und solche, die von den Arbeitsämtern oder den Rüstungsinspektionen als wehrwirtschaftlich unverzichtbare Arbeitskräfte bezeichnet wurden. Die Ausnahmebestimmungen eröffneten den unteren staatlichen Instanzen, der Wirtschaft und der Wehrmacht erhebliche Handlungsspielräume, die auf sehr unterschiedliche Weise genutzt wurden.

Ziel der Deportation war das Vernichtungslager Auschwitz II in Birkenau. Dort entstand im Lagerabschnitt B II e als abgetrennter Bereich das „Zigeunerlager“. Ein erster Transport traf dort am 26. Februar 1943 ein. Bis Ende Juli 1944 waren es etwa 23.000 Menschen, die entsprechend dem Schnellbrief vom 29. Januar 1943 als Familien „möglichst geschlossen“ in das „Familienlager“ verbracht worden waren.

Über die Zusammensetzung der Transportlisten entschieden vor allem die lokalen und regionalen Behörden. Dabei bildeten die Gutachten der RHF – soweit solche vorlagen – die Leitlinie. Lokalstudien, aber auch Aussagen von Rudolf Höß und anderen Verantwortlichen belegen, dass die Vorschriften über Ausnahmefallgruppen nur begrenzt Beachtung fanden. Demnach habe der Mischlingsgrad bei der Einweisung nach Auschwitz keine Bedeutung gehabt. Hunderte Soldaten, darunter Kriegsversehrte und Ausgezeichnete, seien eingewiesen worden. Aus der Wittgensteiner Kleinstadt Berleburg wurden 134 Personen deportiert, die als „sozial angepasst“ zu gelten hatten und sich nach 200 Jahren Sesshaftigkeit so gut wie ausnahmslos nicht als „Zigeuner“ sahen. Da die Selbsteinschätzung der Betroffenen kein Auswahlkriterium war, wurde mutmaßlich auch eine nicht bestimmbare, jedenfalls aber geringe Zahl von Nicht-Roma, die aufgrund verwandtschaftlicher Beziehungen zu Roma als „Zigeunermischlinge“ eingestuft waren, deportiert.

„Insgesamt wurden an die 15.000 Menschen aus Deutschland zwischen 1938 und 1945 als ‚Zigeuner‘ oder ‚Zigeunermischlinge‘ umgebracht“, davon etwa 10.500 in Auschwitz-Birkenau.

Zum Gedenken an den Erlass hat der Künstler Gunter Demnig in Kooperation mit dem Verein Rom e. V. am 16. Dezember 1992, dem 50. Jahrestag des Erlasses, einen Stolperstein vor dem historischen Kölner Rathaus in das Pflaster eingelassen. Auf dem Stein zu lesen sind die ersten Zeilen des den Erlass zitierenden Schnellbriefs. Demnig mischte sich mit diesem Stein in die Diskussion um das Bleiberecht von aus Jugoslawien geflohenen Roma ein.





</doc>
<doc id="412" url="https://de.wikipedia.org/wiki?curid=412" title="Asteroid">
Asteroid

Als Asteroiden (von ), Kleinplaneten oder Planetoiden werden astronomische Kleinkörper bezeichnet, die sich auf keplerschen Umlaufbahnen um die Sonne bewegen, größer als Meteoroiden (Millimeter bis Meter), aber kleiner als Zwergplaneten (tausend Kilometer) sind.

Der Begriff „Asteroid“ wird oft als synonym mit „Kleinplanet“ verwendet, bezieht sich aber hauptsächlich auf Objekte innerhalb der Neptun­bahn und ist kein von der IAU definierter Begriff. Jenseits der Neptunbahn werden solche Körper auch transneptunische Objekte (TNO) genannt. Nach neuerer Definition fasst der Begriff Kleinplanet die „klassischen“ Asteroiden und die TNO zusammen.

Bislang sind 755.017 Asteroiden im Sonnensystem bekannt (Stand: 2. Februar 2018), wobei die tatsächliche Anzahl wohl in die Millionen gehen dürfte. Asteroiden haben im Gegensatz zu den Zwergplaneten eine zu geringe Masse, um eine annähernd runde Form anzunehmen, und sind daher generell unregelmäßig geformte Körper. Nur die wenigsten haben mehr als einige 100 Kilometer Durchmesser.

Große Asteroiden im Asteroidengürtel sind die Objekte (2) Pallas, (3) Juno, (4) Vesta, (5) Astraea, (6) Hebe, (7) Iris, (10) Hygiea und (15) Eunomia.

Die Bezeichnung "Asteroid" bezieht sich auf die Größe der Objekte. Fast alle sind so klein, dass sie im Teleskop wie der Lichtpunkt eines Sterns erscheinen.

Die Bezeichnung "Kleinplanet" oder "Planetoid" rührt daher, dass sich die Objekte am Firmament wie Planeten relativ zu den Sternen bewegen. Asteroiden sind keine Planeten und gelten auch nicht als Zwergplaneten, denn aufgrund ihrer geringen Größe ist die Gravitation zu schwach, um sie annähernd zu einer Kugel zu formen. Gemeinsam mit Kometen und Meteoroiden gehören Asteroiden zur Klasse der Kleinkörper. Meteoroiden sind kleiner als Asteroiden, aber zwischen ihnen und Asteroiden gibt es weder von der Größe noch von der Zusammensetzung her eine eindeutige Grenze.

Seit der 26. Generalversammlung der Internationalen Astronomischen Union (IAU) und ihrer Definition vom 24. August 2006 zählen die großen runden Objekte, deren Gestalt sich im hydrostatischen Gleichgewicht befindet, strenggenommen nicht mehr zu den Asteroiden, sondern zu den Zwergplaneten.

(1) Ceres (975 km Durchmesser) ist das größte Objekt im Asteroidengürtel und wird heute zu den Zwergplaneten gezählt. (2) Pallas ist mit um die 560 km Durchmesser ein weiteres großes Objekt im Asteroidengürtel, das womöglich auch zu den Zwergplaneten zählt. (4) Vesta befindet sich nicht im hydrostatischen Gleichgewicht und ist nicht annähernd rund, ist somit definitiv kein Zwergplanet.

Im Kuipergürtel gibt es neben dem – früher als Planet und heute als Zwergplanet eingestuften – Pluto (2390 km Durchmesser) weitere Zwergplaneten: (136199) Eris (2326 km), (136472) Makemake (1430 × 1502 km), (136108) Haumea (elliptisch, etwa 1920 × 1540 × 990 km), (50000) Quaoar (1110 km) und (90482) Orcus (917 km).

Das Ende 2003 jenseits des Kuipergürtels entdeckte etwa 995 km große Objekt (90377) Sedna dürfte ebenfalls als Zwergplanet einzustufen sein.

Bereits im Jahr 1760 entwickelte der deutsche Gelehrte Johann Daniel Titius eine einfache mathematische Formel (Titius-Bode-Reihe), nach der die Abstände der Planeten zueinander ins Verhältnis gesetzt werden. Die Reihe enthält jedoch eine Lücke, da zwischen Mars und Jupiter, im Abstand von 2,8 AE (Astronomische Einheit), ein Planet fehlt. Ende des 18. Jahrhunderts setzte eine regelrechte Jagd auf den unentdeckten Planeten ein. Für eine koordinierte Suche wurde 1800, als erstes internationales Forschungsvorhaben, die "Himmelspolizey" gegründet. Organisator war Baron Franz Xaver von Zach, der seinerzeit an der Sternwarte Gotha tätig war. Der Sternhimmel wurde in 24 Sektoren eingeteilt, die von Astronomen in ganz Europa systematisch abgesucht wurden. Für den Planeten hatte man bereits den Namen „Phaeton“ reservieren lassen.

Die Suche blieb insofern erfolglos, als der erste Kleinplanet (Ceres) zu Jahresbeginn 1801 durch Zufall entdeckt wurde. Allerdings bewährte sich die "Himmelspolizey" bald in mehrfacher Hinsicht: mit der Wiederauffindung des verlorenen Kleinplaneten, mit verbesserter Kommunikation über Himmelsentdeckungen und mit der erfolgreichen Suche nach weiteren Kleinplaneten zwischen 1802 und 1807.

In der Neujahrsnacht des Jahres 1801 entdeckte der Astronom und Theologe Giuseppe Piazzi im Teleskop der Sternwarte von Palermo (Sizilien) bei der Durchmusterung des Sternbildes Stier einen schwachen Stern, der in keiner Sternkarte verzeichnet war. Piazzi hatte von Zachs Forschungsvorhaben gehört und beobachtete den Stern in den folgenden Nächten, da er vermutete, den gesuchten Planeten gefunden zu haben. Er sandte seine Beobachtungsergebnisse an Zach, wobei er das Objekt zunächst als neuen Kometen bezeichnete. Piazzi erkrankte jedoch und konnte seine Beobachtungen nicht fortsetzen. Bis zur Veröffentlichung seiner Beobachtungen verging viel Zeit. Der Himmelskörper war inzwischen weiter in Richtung Sonne gewandert und konnte zunächst nicht wiedergefunden werden.

Der Mathematiker Gauß hatte allerdings ein numerisches Verfahren entwickelt, das es erlaubte, unter Anwendung der Methode der kleinsten Quadrate die Bahnen von Planeten oder Kometen anhand nur weniger Positionen zu bestimmen. Nachdem Gauß die Veröffentlichungen Piazzis gelesen hatte, berechnete er die Bahn des Himmelskörpers und sandte das Ergebnis nach Gotha. Heinrich Wilhelm Olbers entdeckte das Objekt daraufhin am 31. Dezember 1801 wieder, das schließlich den Namen Ceres erhielt. Im Jahr 1802 entdeckte Olbers einen weiteren Himmelskörper, den er Pallas nannte. 1803 wurde Juno, 1807 Vesta entdeckt. 

Bis zur Entdeckung des fünften Asteroiden, Astraea im Jahr 1845, vergingen allerdings 38 Jahre. Die bis dahin entdeckten Asteroiden wurden damals noch nicht als solche bezeichnet – sie galten damals als vollwertige Planeten. So kam es, dass der Planet Neptun bei seiner Entdeckung im Jahr 1846 nicht als achter, sondern als dreizehnter Planet gezählt wurde. Ab dem Jahr 1847 folgten allerdings so rasch weitere Entdeckungen, dass bald beschlossen wurde, für die zahlreichen, aber allesamt doch recht kleinen Himmelskörper, die die Sonne zwischen Mars und Jupiter umkreisen, eine neue Objektklasse von Himmelskörpern einzuführen: die "Asteroiden", die sogenannten "kleinen Planeten". Die Zahl der "großen Planeten" sank somit auf acht. Bis zum Jahr 1890 wurden insgesamt über 300 Asteroiden entdeckt.

Nach 1890 brachte die Anwendung der Fotografie in der Astronomie wesentliche Fortschritte. Die Asteroiden, die bis dahin mühsam durch den Vergleich von Teleskopbeobachtungen mit Himmelskarten gefunden wurden, verrieten sich nun durch Lichtspuren auf den fotografischen Platten. Durch die im Vergleich zum menschlichen Auge höhere Lichtempfindlichkeit der fotografischen Emulsionen konnten, in Kombination mit langen Belichtungszeiten bei Nachführung des Teleskops quasi im Zeitraffer, äußerst lichtschwache Objekte nachgewiesen werden. Durch den Einsatz der neuen Technik stieg die Zahl der entdeckten Asteroiden rasch an.

Ein Jahrhundert später, um 1990, löste die digitale Fotografie in Gestalt der CCD-Kameratechnik einen weiteren Entwicklungssprung aus, der durch die Möglichkeiten der computerunterstützten Auswertung der elektronischen Aufnahmen noch potenziert wird. Seither hat sich die Zahl jährlich aufgefundener Asteroiden nochmals vervielfacht.

Ist die Bahn eines Asteroiden bestimmt worden, kann die Größe des Himmelskörpers aus der Untersuchung seiner Helligkeit und des Rückstrahlvermögens, der Albedo, ermittelt werden. Dazu werden Messungen mit sichtbaren Lichtfrequenzen sowie im Infrarotbereich durchgeführt. Diese Methode ist allerdings mit Unsicherheiten verbunden, da die Oberflächen der Asteroiden chemisch unterschiedlich aufgebaut sind und das Licht unterschiedlich stark reflektieren.

Genauere Ergebnisse können mittels Radarbeobachtungen erzielt werden. Dazu können Radioteleskope verwendet werden, die, als Sender umfunktioniert, starke Radiowellen in Richtung der Asteroiden aussenden. Durch die Messung der Laufzeit der von den Asteroiden reflektierten Wellen kann deren exakte Entfernung bestimmt werden. Die weitere Auswertung der Radiowellen liefert Daten zu Form und Größe. Regelrechte „Radarbilder“ lieferte beispielsweise die Beobachtung der Asteroiden (4769) Castalia und (4179) Toutatis.

Neue und weiterentwickelte Technologien sowie fortgesetzte Leistungssteigerung von Detektoren und elektronischer Datenverarbeitung ermöglichten seit den 1990er Jahren eine Reihe von automatisierten Suchprogrammen mit verschiedenen Zielsetzungen. Diese Durchmusterungen haben einen erheblichen Anteil an der Neuentdeckung von Asteroiden.

Eine Reihe von Suchprogrammen konzentriert sich auf Erdnahe Asteroiden z. B. LONEOS, LINEAR, NEAT, NeoWise, Spacewatch, Catalina Sky Survey und Pan-STARRS. Diese haben erheblichen Anteil daran, dass quasi täglich neue Asteroiden gefunden werden, deren Anzahl inzwischen eine knappe Dreiviertelmillion erreicht hat.

In naher Zukunft wird sich die Zahl der bekannten Asteroiden nochmals dramatisch erhöhen, da für die nächsten Jahre Durchmusterungen mit erhöhter Empfindlichkeit geplant sind, zum Beispiel Gaia und LSST. Allein die Raumsonde Gaia soll nach Modellrechnungen bis zu eine Million bisher unbekannter Asteroiden entdecken.

Eine Reihe von Asteroiden konnte mittels Raumsonden näher untersucht werden:


Weitere Missionen sind geplant, unter anderem: 

Die Namen der Asteroiden setzen sich aus einer vorangestellten Nummer und einem Namen zusammen. Die Nummer gab früher die Reihenfolge der Entdeckung des Himmelskörpers an. Heute ist sie eine rein numerische Zählform, da sie erst vergeben wird, wenn die Bahn des Asteroiden gesichert und das Objekt jederzeit wiederauffindbar ist; das kann durchaus erst Jahre nach der Erstbeobachtung erfolgen. Von den bisher bekannten 755.017 Asteroiden haben 508.765 eine Nummer (Stand: 2. Februar 2018).

Der Entdecker hat innerhalb von zehn Jahren nach der Nummerierung das Vorschlagsrecht für die Vergabe eines Namens. Dieser muss jedoch durch eine Kommission der IAU bestätigt werden, da es Richtlinien für die Namen astronomischer Objekte gibt. Dementsprechend existieren zahlreiche Asteroiden zwar mit Nummer, aber ohne Namen, vor allem in den oberen Zehntausendern.

Neuentdeckungen, für die noch keine Bahn mit ausreichender Genauigkeit berechnet werden konnte, werden mit dem Entdeckungsjahr und einer Buchstabenkombination, beispielsweise 2003 UB, gekennzeichnet. Die Buchstabenkombination setzt sich aus dem ersten Buchstaben für die Monatshälfte (beginnend mit A und fortlaufend bis Y ohne I) und einem fortlaufenden Buchstaben (A bis Z ohne I) zusammen. Wenn mehr als 25 Kleinplaneten in einer Monatshälfte entdeckt werden – was heute die Regel ist – beginnt die Buchstabenkombination von vorne, gefolgt von jeweils einer je Lauf um eins erhöhten laufenden Nummer.

Der erste Asteroid wurde 1801 von Giuseppe Piazzi an der Sternwarte Palermo auf Sizilien entdeckt. Piazzi taufte den Himmelskörper auf den Namen „Ceres Ferdinandea“. Die römische Göttin Ceres ist Schutzpatronin der Insel Sizilien. Mit dem zweiten Namen wollte Piazzi König Ferdinand IV., den Herrscher über Italien und Sizilien ehren. Dies missfiel der internationalen Forschergemeinschaft und der zweite Name wurde fallengelassen. Die offizielle Bezeichnung des Asteroiden lautet demnach (1) Ceres.

Bei den weiteren Entdeckungen wurde die Nomenklatur beibehalten und die Asteroiden wurden nach römischen und griechischen Göttinnen benannt; dies waren (2) Pallas, (3) Juno, (4) Vesta, (5) Astraea, (6) Hebe, und so weiter.

Als immer mehr Asteroiden entdeckt wurden, gingen den Astronomen die antiken Gottheiten aus. So wurden Asteroiden unter anderem nach den Ehefrauen der Entdecker, zu Ehren historischer Persönlichkeiten oder Persönlichkeiten des öffentlichen Lebens, Städten und Märchenfiguren benannt. Beispiele hierfür sind die Asteroiden (21) Lutetia, (216) Kleopatra, (719) Albert, (1773) Rumpelstilz, (5535) Annefrank, (17744) Jodiefoster.

Neben Namen aus der griechisch-römischen Mythologie kommen auch Namen von Gottheiten aus anderen Kulturkreisen zur Anwendung, insbesondere für neu entdeckte, größere Objekte, wie (20000) Varuna, (50000) Quaoar und (90377) Sedna.

Monde von Asteroiden erhalten zu ihrem Namen keine permanente Nummer und gelten nicht als Asteroiden oder Kleinkörper, da sie nicht selbstständig die Sonne umlaufen.

Zunächst gingen die Astronomen davon aus, dass die Asteroiden das Ergebnis einer kosmischen Katastrophe seien, bei der ein Planet zwischen Mars und Jupiter auseinanderbrach und Bruchstücke auf seiner Bahn hinterließ. Es zeigte sich jedoch, dass die Gesamtmasse der im Hauptgürtel vorhandenen Asteroiden sehr viel geringer ist als die des Erdmondes. Schätzungen der Gesamtmasse der Kleinplaneten schwanken zwischen 0,1 und 0,01 Prozent der Erdmasse (Der Mond hat etwa 1,23 Prozent der Erdmasse). Daher wird heute angenommen, dass die Asteroiden eine Restpopulation von Planetesimalen aus der Entstehungsphase des Sonnensystems darstellen. Die Gravitation von Jupiter, dessen Masse am schnellsten zunahm, verhinderte die Bildung eines größeren Planeten aus dem Asteroidenmaterial. Die Planetesimale wurden auf ihren Bahnen gestört, kollidierten immer wieder heftig miteinander und zerbrachen. Ein Teil wurde auf Bahnen abgelenkt, die sie auf Kollisionskurs mit den Planeten brachten. Hiervon zeugen noch die Einschlagkrater auf den Planetenmonden und den inneren Planeten. Die größten Asteroiden wurden nach ihrer Entstehung stark erwärmt (hauptsächlich durch den radioaktiven Zerfall des Aluminium-Isotops Al und möglicherweise auch des Eisenisotops Fe) und im Innern aufgeschmolzen. Schwere Elemente, wie Nickel und Eisen, setzten sich infolge der Schwerkraftwirkung im Inneren ab, die leichteren Verbindungen, wie die Silikate, verblieben in den Außenbereichen. Dies führte zur Bildung von differenzierten Körpern mit metallischem Kern und silikatischem Mantel. Ein Teil der differenzierten Asteroiden zerbrach bei weiteren Kollisionen, wobei Bruchstücke, die in den Anziehungsbereich der Erde geraten, als Meteoriten niedergehen.

Die spektroskopische Untersuchung der Asteroiden zeigte, dass deren Oberflächen chemisch unterschiedlich zusammengesetzt sind. Analog erfolgte eine Einteilung in verschiedene spektrale beziehungsweise taxonomische Klassen, die 1984 von David J. Tholen erstellt wurde:


In der Vergangenheit gingen Wissenschaftler davon aus, dass die Asteroiden monolithische Felsbrocken, also kompakte Gebilde sind. Die geringen Dichten etlicher Asteroiden sowie das Vorhandensein von riesigen Einschlagkratern deuten jedoch darauf hin, dass viele Asteroiden locker aufgebaut sind und eher als "rubble piles" anzusehen sind, als lose „Schutthaufen“, die nur durch die Gravitation zusammengehalten werden. Locker aufgebaute Körper können die bei Kollisionen auftretenden Kräfte absorbieren ohne zerstört zu werden. Kompakte Körper werden dagegen bei größeren Einschlagereignissen durch die Stoßwellen auseinandergerissen. Darüber hinaus weisen die großen Asteroiden nur geringe Rotationsgeschwindigkeiten auf. Eine schnelle Rotation um die eigene Achse würde sonst dazu führen, dass die auftretenden Fliehkräfte die Körper auseinanderreißen ("siehe auch: YORP-Effekt"). Man geht heute davon aus, dass der überwiegende Teil der über 200 Meter großen Asteroiden derartige kosmische Schutthaufen sind.

Anders als die Planeten besitzen viele Asteroiden keine annähernd kreisrunden Umlaufbahnen. Sie haben, abgesehen von den meisten Hauptgürtelasteroiden und den Cubewanos im Kuipergürtel, meist sehr exzentrische Orbits, deren Ebenen in vielen Fällen stark gegen die Ekliptik geneigt sind. Ihre relativ hohen Exzentrizitäten machen sie zu Bahnkreuzern; das sind Objekte, die während ihres Umlaufs die Bahnen eines oder mehrerer Planeten passieren. Die Schwerkraft des Jupiter sorgt allerdings dafür, dass sich Asteroiden, bis auf wenige Ausnahmen, nur jeweils innerhalb oder außerhalb seiner Umlaufbahn bewegen.

Innerhalb der Marsbahn bewegen sich einige unterschiedliche Asteroidengruppen, die alle bis auf wenige Ausnahmen aus Objekten von unter fünf Kilometer Größe (überwiegend jedoch deutlich kleiner) bestehen. Einige dieser Objekte sind Merkur- und Venusbahnkreuzer, von denen sich mehrere nur innerhalb der Erdbahn bewegen, manche können sie auch kreuzen. Wiederum andere bewegen sich hingegen nur außerhalb der Erdbahn.

Die Existenz der als Vulkanoiden bezeichneten Gruppe von Asteroiden konnte bislang nicht nachgewiesen werden. Diese Asteroiden sollen sich auf sonnennahen Bahnen innerhalb der von Merkur bewegen.

Asteroiden, deren Bahnen dem Orbit der Erde nahe kommen, werden als "erdnahe Asteroiden" bezeichnet. Üblicherweise wird als Abgrenzungskriterium ein Perihel kleiner als 1,3 AE verwendet. Wegen einer theoretischen Kollisionsgefahr mit der Erde wird seit einigen Jahren systematisch nach ihnen gesucht. Bekannte Suchprogramme sind zum Beispiel Lincoln Near Earth Asteroid Research (LINEAR), der Catalina Sky Survey, Pan-STARRS, NEAT und LONEOS.


Etwa 90 Prozent der bekannten Asteroiden bewegen sich zwischen den Umlaufbahnen von Mars und Jupiter. Sie füllen damit die Lücke in der Titius-Bode-Reihe. Die größten Objekte sind hier (1) Ceres, (2) Pallas, (4) Vesta und (10) Hygiea.

Die überwiegende Mehrzahl der Objekte, deren Bahnhalbachsen zwischen der Mars- und Jupiterbahn liegen, sind Teil des Asteroiden-Hauptgürtels. Ihnen gemeinsam ist eine Bahnneigung unter 20° und Exzentrizitäten unter 0,25. Die meisten entstanden durch Kollisionen größerer Asteroiden in dieser Zone und bilden daher Gruppen mit ähnlicher chemischer Zusammensetzung. Ihre Umlaufbahnen werden durch die sogenannten Kirkwoodlücken, die durch Bahnresonanzen zu Jupiter gebildet werden, begrenzt. Dadurch lässt sich der Hauptgürtel in drei Zonen einteilen:

Außerhalb des Asteroidengürtels liegen vereinzelt kleinere Asteroidengruppen, deren Umlaufbahnen meist in Resonanz zur Jupiterbahn stehen und dadurch stabilisiert werden. Außerdem existieren weitere Gruppen, die ähnliche Längen der Bahnhalbachsen aufweisen wie die Hauptgürtelasteroiden, jedoch deutlich stärker geneigte Bahnen (teilweise über 25°) oder andere ungewöhnliche Bahnelemente aufweisen:



Im äußeren Sonnensystem, jenseits der Neptunbahn, bewegen sich die transneptunischen Objekte, von denen die meisten als Teil des Kuipergürtels betrachtet werden (Kuiper belt objects; KBO). Dort wurden die bislang größten Asteroiden oder Planetoiden entdeckt. Die Objekte dieser Zone lassen sich anhand ihrer Bahneigenschaften in drei Gruppen einteilen:

Asteroiden, die sich in den Lagrange-Punkten der Planeten befinden, werden „Trojaner“ genannt. Zuerst wurden diese Begleiter bei Jupiter entdeckt. Sie bewegen sich auf der Jupiterbahn vor beziehungsweise hinter dem Planeten. Jupitertrojaner sind beispielsweise (588) Achilles und (1172) Äneas. 1990 wurde der erste Marstrojaner entdeckt und (5261) Eureka genannt. In der Folgezeit wurden weitere Marstrojaner entdeckt. Auch Neptun besitzt Trojaner und 2011 wurde mit 2011 QF der erste Uranustrojaner entdeckt.

Manche Asteroiden bewegen sich auf einer Hufeisenumlaufbahn auf einer Planetenbahn, wie zum Beispiel der Asteroid 2002 AA in der Nähe der Erde.
Im Oktober 2017 wurde mit 1I/ʻOumuamua der erste interstellar reisende Asteroid entdeckt. Er ist länglich geformt, rund 400 Meter lang und näherte sich etwa im rechten Winkel der Bahnebene der Planeten. Nachdem seine Bahn durch die Gravitation der Sonne um etwa 90° abgelenkt wurde, flog er auf seinem neuen Kurs in Richtung des Sternbildes Pegasus in ca. 24 Millionen Kilometern Entfernung am 14. Oktober 2017 an der Erde vorbei.

Im Sonnensystem bewegen sich einige Asteroiden, die Charakteristika aufweisen, die sie mit keinem anderen Objekt teilen. Dazu zählen unter anderem (944) Hidalgo, der sich auf einer stark exzentrischen, kometenähnlichen Umlaufbahn zwischen Saturn und dem Hauptgürtel bewegt, und (279) Thule, der sich als einziger Vertreter einer potenziellen Gruppe von Asteroiden in 4:3-Resonanz zu Jupiter bei 4,3 AE um die Sonne bewegt. Ein weiteres Objekt ist (90377) Sedna, ein relativ großer Asteroid, der weit außerhalb des Kuipergürtels eine exzentrische Umlaufbahn besitzt, die ihn bis zu 900 AE von der Sonne entfernt. Inzwischen wurden allerdings mindestens fünf weitere Objekte mit ähnlichen Bahncharakteristika wie Sedna entdeckt, diese bilden die neue Gruppe der Sednoiden.

Einige Charakteristika wie ihre Form lassen sich aus ihrer Lichtkurve berechnen.

Asteroiden, die mit wesentlich größeren Himmelskörpern wie Planeten kollidieren, erzeugen Einschlagkrater. Die Größe des Einschlagkraters und die damit verbundene Energiefreisetzung (Explosion) wird maßgeblich durch die Geschwindigkeit, Größe, Masse und Zusammensetzung des Asteroiden bestimmt.

Die Flugbahnen der Asteroiden im Sonnensystem sind nicht genau genug bekannt, um auf längere Zeit berechnen zu können, ob und wann genau ein Asteroid auf der Erde (oder auf einem anderen Planeten) einschlagen wird. Durch Annäherung an andere Himmelskörper unterliegen die Bahnen der Asteroiden ständig kleineren Veränderungen. Deswegen wird auf Basis der bekannten Bahndaten und -unsicherheiten lediglich das "Risiko" von Einschlägen errechnet. Es verändert sich bei neuen, genaueren Beobachtungen fortlaufend.

Mit der "Turiner Skala" und der "Palermo-Skala" gibt es zwei gebräuchliche Methoden zur Bewertung des Einschlagrisikos von Asteroiden auf der Erde und der damit verbundenen Energiefreisetzung und Zerstörungskraft:


Eine Auflistung irdischer Krater findet sich in der Liste der Einschlagkrater der Erde sowie als Auswahl unter Große und bekannte Einschlagkrater.

Die Wissenschaft benennt mehrere mögliche Kollisionen zwischen Asteroiden untereinander:

2001 etablierte das "Committee on the Peaceful Uses of Outer Space" (COPUOS) der UNO das "Action Team on Near-Earth Objects" (Action Team 14). Empfohlen wurde 2013 die Errichtung eines "international asteroid warning network" (IAWN) und einer "space mission planning advisory group" (SMPAG). Das Action Team 14 hat sein Mandat erfüllt und wurde 2015 aufgelöst. Am 30. Juni 2015 wurde der erste Asteroid Day ausgerufen.






</doc>
<doc id="414" url="https://de.wikipedia.org/wiki?curid=414" title="Asen (Mythologie)">
Asen (Mythologie)

Die Asen (altnordisch Nominativ Singular "áss", Plural "æsir") sind nach Snorri Sturluson in der Prosa-Edda ein Göttergeschlecht der nordischen Mythologie. Dieses Geschlecht ist nach der Zahl der ihm zugehörigen Gottheiten größer als das zweite nordische Göttergeschlecht der Wanen.

Die Asen werden von ihrer Mentalität als kriegerische und herrschende Götter geschildert, wohingegen die Wanen als Fruchtbarkeitsgottheiten stilisiert werden. Bei Snorri findet jedoch eine stringente Trennung der Geschlechter nicht statt. Zudem wird der Begriff „Ase“ in Quellen als ein genereller Begriff für „Gott“ gebraucht (siehe auch: Abschnitt Etymologie im germanischen Sprachraum bei "Gott").

Der Begriff „Ase“ ist inschriftlich zuerst fassbar belegt in einer Runeninschrift aus dem 2. Jahrhundert aus Vimose in Dänemark: asau wija „ich weihe dem Asen/Gott“. Ein weiterer Beleg ist die Form "Ansis" bei Jordanes (Getica 13,78), hier werden diese als mythische Vorfahren der Goten als "semideos", lateinisch für „Halbgötter“, bezeichnet.

Das altisländische, beziehungsweise altnordische "áss" weist durch den runischen Beleg einen u-Stamm auf, wodurch auf ein germanisches "*ansu-z" zu schließen ist. Durch den Beleg bei Jordanes wird in der Forschung diskutiert, ob durch die Form "ansis", neben der altnordischen Form mit dem u-Stamm, berechtigt ein i-Stamm anzunehmen ist und in der Folge auf ein germanisches "*ansi-z" rückzuführen ist.

Nach der Jüngeren Edda wohnen zwölf Asen in Asgard (Sitz der Götter). Sie herrschen über die Welt und die Menschen. Ihnen werden Eigenschaften wie Stärke, Macht und Kraft zugeschrieben. Sie sind weitgehend vermenschlicht, haben also einen irdischen Alltag. Wie die Menschen sind sie sterblich. Nur durch die Äpfel der Idun halten sie sich jung, bis fast alle von ihnen zur Ragnarök getötet werden.

Die folgende Übersicht zeigt die verwandtschaftlichen Beziehungen zwischen den bekanntesten nordischen Gottheiten aus den Geschlechtern der Asen und Wanen:




</doc>
<doc id="416" url="https://de.wikipedia.org/wiki?curid=416" title="Altes Reich">
Altes Reich

Das Alte Reich dauerte ungefähr von 2700 bis 2200 v. Chr. und ist die älteste der drei klassischen Perioden des Alten Ägyptens. Sie folgte auf die Thinitenzeit, die zur Bildung des ägyptischen Staates führte, und umfasste die 3. bis 6. Dynastie. Die darauffolgende Erste Zwischenzeit führte zu instabilen politischen Verhältnissen im Land.

Die Alten Ägypter selbst sahen die Epoche als Goldenes Zeitalter und Höhepunkt ihrer Kultur an. De facto war die Zeit von äußerst lang anhaltender politischer Stabilität geprägt und die innere Ordnung des Landes durch keinerlei äußere Bedrohungen gefährdet. Die bereits zur Thinitenzeit begonnene Zentralisierung des Staates und der langsam einsetzende Wohlstand in der Bevölkerung führten zu beachtlichen architektonischen und künstlerischen Leistungen. Es kam zur Entstehung erster wichtiger Gattungen der klassischen ägyptischen Literatur, zur Kanonisierung von Malerei und Bildhauerei, aber auch zu zahlreichen Neueinführungen in der Verwaltung, die fast drei Jahrtausende überdauerten.

Das Alte Reich ist das Zeitalter der großen Pyramiden, die in der Gegend der damaligen Hauptstadt Memphis entstanden: zunächst die Stufenpyramide von Pharao Djoser in Sakkara, später dann die drei monumentalen Pyramiden auf dem Plateau von Gizeh (von Cheops, Chephren und Mykerinos), die zu den sieben Weltwundern der Antike zählen. Die Pyramiden spiegeln eindrucksvoll die zentrale Stellung der Herrscher in der Gesellschaft wider und ihre Macht, die danach in der altägyptischen Geschichte unerreicht blieb. Zugleich machen sie die Weiterentwicklung des Verwaltungsapparates deutlich und seine Fähigkeit, materielle und menschliche Ressourcen zu mobilisieren. Sie zeigen aber auch die erheblichen Fortschritte in Architektur und Kunst auf und lassen erkennen, welche zentrale Rolle Totenkult und Jenseitsglaube zu dieser Zeit gespielt haben.

Die Hauptquelle für das Alte Reich sind die Pyramiden und ihre Tempelanlagen, die letzteren sind jedoch häufig stark zerstört. Vor allem in den Pyramidengrabkammern der 6. Dynastie fanden sich Texte, die eine umfangreiche Quelle zum Totenglauben der Epoche darstellen.

Neben den Pyramiden finden sich umfangreiche Friedhöfe der höchsten Beamten, deren Gräber oftmals reich mit Reliefs und Inschriften dekoriert sind. Nekropolen in den Provinzen fanden in der Forschung bisher wenig Beachtung. Immerhin stammen aus dem späten Alten Reich einige bedeutende dekorierte Grabanlagen lokaler Würdenträger.

Siedlungen des Alten Reiches sind bisher kaum ausgegraben. Eine Ausnahme stellt neuerdings die Pyramidenstadt von Gizeh dar. Nur wenige Tempel dieser Zeit sind bisher untersucht worden. Im Gegensatz zu späteren Epochen scheinen sie eher klein und undekoriert gewesen zu sein. Eine Ausnahme stellen zwei Heiligtümer zum Sonnen- und Königskult dar. Aus dem Alten Reich gibt es nur wenige Papyri, die wichtigsten fanden sich in Abusir und stellen Verwaltungsurkunden eines Pyramidentempels dar.

Die 3. Dynastie (2700 bis 2620 v. Chr.) folgte auf die Thinitenzeit, die zur Reichseinigung und zur Bildung des ersten ägyptischen Staates geführt hat. Die Dynastie wird gemeinhin dem Alten Reich zugerechnet, da sie den Beginn des für diese Epoche ausschlaggebenden Pyramidenbaus markiert. Einige Forscher ziehen es vor, sie in direkte Kontinuität mit der 1. und 2. Dynastie zu setzen, die zahlreiche Gemeinsamkeiten hinsichtlich politischer Organisation und kultureller Aspekte aufweisen. Der Übergang von der 2. zur 3. Dynastie stellt keinen echten Dynastiewechsel dar, da die ersten Herrscher der 3. Dynastie in direkter Linie vom Königshaus der vorhergehenden Dynastie abstammen.

Über die Anzahl und Abfolge der Könige der 3. Dynastie herrscht einige Unklarheit. Probleme bereitet die Tatsache, dass die fast ausschließlich in späteren Listen überlieferten Geburtsnamen nur selten in Einklang mit den zeitgenössisch auftretenden Horusnamen gebracht werden können. Von den aus dieser Zeit überlieferten fünf oder sechs Horusnamen kann lediglich Netjeri-chet eindeutig König Djoser zugewiesen werden. Die Reihenfolge der nächsten drei Herrscher Sechemchet, Chaba und Sanacht dagegen ist ungewiss. Der vermutlich letzte Herrscher Huni wurde versuchsweise mit dem Horusnamen Qahedjet identifiziert, jedoch ist diese Zuweisung nicht sicher. Die Angaben für die Dauer der 3. Dynastie schwanken zwischen 50 und 75 Jahren.

König Djoser wird als bedeutendster Herrscher der 3. Dynastie angesehen, da er mit seinem Pyramidenkomplex in Sakkara die erste Stufenpyramide und den ersten monumentalen Steinbau in der Geschichte der Menschheit schaffen ließ. Für den Bau der Pyramide, die einen Höhepunkt des frühen ägyptischen Steinbaus darstellt, waren enorme Fortschritte in der logistischen Organisation nötig, die nur durch eine gut funktionierende, straff organisierte Verwaltung erzielt werden konnten. Als einer der berühmtesten Beamten dieser Zeit ist Imhotep bekannt, der als Berater und Baumeister von Djoser wirkte und bis in die Spätzeit hinein als legendärer Erfinder verehrt und sogar vergöttlicht wurde. Sichtbare Fortschritte im Bereich Bildender Kunst lassen sich am besten an Werken aus Djosers Regierungszeit ablesen, die sich in ihrer Ausführung deutlich von der vorhergehenden Epoche abheben.

Die 3. Dynastie war von der Stärkung des Zentralstaates geprägt, der von der neuen Hauptstadt Memphis aus an der Grenze zwischen Ober- und Unterägypten regiert wurde. Die hohen Würdenträger ließen sich nicht mehr wie in der Frühzeit auf Friedhöfen in der Provinz bestatten, sondern erhielten nun einen Platz in der königlichen Nekropole. Die gegen Ende der Dynastie an vielen Stellen im Reich errichteten Kleinpyramiden unterstreichen dagegen den wachsenden Einfluss der Zentralregierung auf die Provinzen. Zur Beschaffung von Baumaterial und anderen wichtigen Rohstoffen wurden Expeditionen in weit entfernte Gegenden entsandt, etwa ins Wadi Maghara im Westen des Sinai.

Die 4. Dynastie (2620 bis 2500 v. Chr.) wurde von Snofru gegründet, der möglicherweise ein Sohn von Huni war. In einer Zeitspanne von etwa 100 Jahren wechselten sich sieben Könige ab, von denen die vier Könige Snofru, Cheops, Chephren und Mykerinos eine relativ lange Regierungszeit zwischen 18 und 30 Jahren ausübten.

Die Dynastie war eine Blütezeit Ägyptens und ist der Nachwelt durch die größten jemals in Ägypten errichteten Pyramiden in Erinnerung geblieben. Ihr Gründer Snofru, der als Idealbild des gerechten Königs galt, unternahm weit entfernte Expeditionen nach Nubien und Libyen und ließ eine Grenzfestung zum Schutze des Landes bauen; weiterhin ließ er in Meidum und Dahschur nacheinander drei große Pyramidenanlagen errichten, die den Übergang von der Stufenpyramide zur echten Pyramide einleiteten. Sein Nachfolger Cheops wählte das Plateau von Gizeh als Bauplatz für seine Pyramide, die mit ursprünglich 146,59 Metern Höhe die höchste der Welt ist. Ihre enorme Größe veranlasste antike Autoren dazu, ihn im Gegensatz zu seinem Vater als größenwahnsinnigen und tyrannischen Herrscher darzustellen, ihm wurde aber in Wirklichkeit bis in die Spätzeit hinein noch eine eigenständige kultische Verehrung zuteil. Zwei seiner Söhne, Djedefre und Chephren, folgten ihm auf den Thron. Djedefre nannte sich als erster König „Sohn des Re“, Chephren ließ sich eine weitere große Pyramide neben der seines Vaters erbauen. Von dem nachfolgenden König Bicheris ist nur wenig bekannt. Erst sein Nachfolger Mykerinos ließ eine dritte Pyramide in Gizeh bauen. Der letzte König der Dynastie hieß Schepseskaf. Im Turiner Königspapyrus ist ein gewisser Thamphthis überliefert, der jedoch nicht durch zeitgenössische Belege bestätigt werden kann.

Die Umsetzung der großen Bauvorhaben dieser Dynastie erforderte eine weitaus komplexere und effizientere Verwaltung als in der Dynastie zuvor. Die Verwaltung wurde immer weiter ausgebaut und umfasste schließlich ein „Amt für königliche Arbeiten“, das sich speziell um die Errichtung von Denkmälern kümmerte. Zu den Aufgaben gehörte die Rekrutierung von Arbeitskräften, die Errichtung von ausgedehnten Arbeitersiedlungen für ihre Versorgung und Unterkunft unweit der Bauplätze, die Umsetzung der großen und technisch anspruchsvollen Bauvorhaben, die Erschließung von Steinbrüchen, die sich außerhalb des zuständigen Verwaltungsgebietes befinden konnten (Hatnub, Fayyum, Wadi Hammamat, Sinai) sowie die Lieferung von Rohmaterial in die Region von Memphis. Alle staatliche Macht konzentrierte sich in der Person des Pharaos, dessen göttlicher Charakter sich mehr als jemals zuvor in seinen Bauprojekten manifestierte. Die höchsten Amtsposten in der Verwaltung wurden meist von Prinzen besetzt. Von einigen hohen Hofbeamten ist bekannt, dass sie gelegentlich als Verwalter in den Provinzen eingesetzt werden konnten.

Die Herrscher der 5. Dynastie (2504 bis 2347 v. Chr.) sind sowohl durch archäologische Funde als auch durch überlieferte Texte genauer bekannt als die der vorangegangenen Dynastien. Ihre Zeit ist durch kleinere Pyramiden, oft bei Abusir gelegen, und Tempel des Sonnengottes Re gekennzeichnet.

Die Könige (Pharaonen) mussten ihre absolute Macht mit dem aufstrebenden Adel und einer wachsenden Bürokratie teilen. Letzterer verdanken wir viele der erhaltenen Texte.

In der Pyramide des Unas fanden sich erstmals die so genannten Pyramidentexte, die somit die ältesten überlieferten religiösen Texte der Menschheitsgeschichte sind. Die Tradition der Anbringung von Jenseitsliteratur in Königsgräbern wurde im Mittleren Reich mit den Sargtexten und im Neuen Reich mit den verschiedenen Unterweltsbüchern (Amduat, Höhlenbuch, Pfortenbuch etc.) fortgeführt. In nicht-königlichen Gräbern des Neuen Reiches fand man Papyri mit dem Totenbuch.

Die 6. Dynastie (2347 bis 2216 v. Chr.) setzte die 5. Dynastie kulturell fort.
Eine Dezentralisierung der Verwaltungsstrukturen mit über das Land verteilten Verwaltern stellte regionale Zentren her, die mit nachgebendem Einfluss der Herrscher an Bedeutung gewannen. Die Zentralregierung verlor nach Kriegszügen gegen Libyen, Nubien und Palästina immer mehr an Einfluss.

Neueste Forschungen deuten darauf hin, dass Klimaveränderungen (siehe 4,2-Kilojahr-Ereignis), mit ausbleibenden Nilhochwassern, zum Niedergang des Alten Reiches beigetragen haben könnten. Auch die zeitnahen Umbrüche in Sumer und der Indus-Kultur sprechen dafür.

Mit dem Zerfall der Zentralregierung beginnt die Erste Zwischenzeit.




</doc>
<doc id="417" url="https://de.wikipedia.org/wiki?curid=417" title="Aussetzung des Handels">
Aussetzung des Handels

Aussetzung des Handels () ist eine Maßnahme der Geschäftsführung einer Börse, die den Börsenhandel mit Handelsobjekten allgemein oder einem bestimmten Handelsobjekt für einen unbestimmten Zeitraum untersagt.

Börsenkurse sind dadurch gekennzeichnet, dass sie mehr oder weniger starken Schwankungen unterliegen. Die Kursbildung beruht auf den unterschiedlichen Vorstellungen der Marktteilnehmer und stellt für diese ein Kursrisiko dar. Wenn die Kursschwankungen und damit Kursrisiken jedoch so groß sind, dass ein ordnungsgemäßer Börsenhandel nicht mehr stattfinden kann, darf die Geschäftsführung in den Handel eingreifen und Kursnotierungen (vorübergehend) untersagen. Auch Direktgeschäfte zwischen Börsenteilnehmern sind dann verboten.

Das Börsengesetz (BörsG) kennt zwei Arten der Kursaussetzung. Die Geschäftsführung einer Börse ist nach Abs. 1 BörsG befugt, den Handel von Handelsobjekten auszusetzen oder einzustellen:

Bei Aussetzung oder Einstellung geht das Gesetz davon aus, dass ein "ordnungsgemäßer Börsenhandel" nicht mehr stattfinden kann. Ist der Börsengeschäftsführung aufgrund der Mitteilung des Emittenten erkennbar, dass sich die von den Handelsteilnehmern vereinbarten Preise infolge ihrer Informationsdefizite nicht fair und transparent bilden, so ist die Ordnungsmäßigkeit des Börsenhandels nicht mehr gewährleistet. Temporäre Gefährdungen führen zur Aussetzung, länger anhaltende zur länger andauernden Einstellung des Handels.

Beide Maßnahmen führen zur Einstellung der Kursnotiz und sind der Börsenaufsicht und der BaFin zu melden. Die Vorschrift des § 25 Abs. 1 BörsG übernimmt in § 57 die Börsenordnung für die Frankfurter Wertpapierbörse. Danach kann die Geschäftsführung den Handel im regulierten Markt aussetzen, „wenn ein ordnungsgemäßer Börsenhandel zeitweilig gefährdet oder wenn dies zum Schutz des Publikums geboten erscheint.“ Wenn dieser ordnungsgemäße Börsenhandel nicht mehr möglich ist, kann der Handel sogar ganz eingestellt werden.

Nach Abs. 1 Nr. 5 KWG kann die BaFin die Schließung eines Kreditinstituts für den Verkehr mit der Kundschaft anordnen, wodurch es den Bankkunden nicht mehr möglich ist, Wertpapierorders für die Börse abzugeben.

Eine so genannte Volatilitätsunterbrechung beim Computerhandelssystem Xetra findet statt, wenn ein möglicher Kurs „außerhalb des dynamischen Preiskorridors“ liegt. Dieser Korridor wird von den Handelssystemen der Börse errechnet, eine Aussetzung des Handels wird den Anlegern unmittelbar mitgeteilt. Volatilitätsunterbrechungen dauern bei Aktien aus dem DAX und Werten der STOXX Europe 50-Indizes drei Minuten, bei allen anderen Wertpapieren fünf Minuten. Der Umfang des Preiskorridors wird nicht veröffentlicht, damit sich die Marktteilnehmer nicht hierauf einstellen können.

Schwanken Kurse über längere Zeit sehr stark, kann die Börsenleitung mit dem Status des „Fast Market“ die Kursnotierungen aussetzen. Dies geschah zuletzt nach der Insolvenz des Bankhauses Lehman Brothers im September 2008.

Auslöser für Eingriffe in den Börsenhandel können sowohl Ereignisse bei einem einzelnen Wertpapier (angekündigte Ad-hoc-Publizität durch Unternehmen), beim Publikum (Panikkäufe und -verkäufe) oder Extremereignisse (Terroranschläge am 11. September 2001) sein. In WpHG ist vorgeschrieben, dass kursbeeinflussende Tatsachen unmittelbar vor ihrer Veröffentlichung den Börsen und der BaFin mitzuteilen sind. Dadurch erhalten die Börsen etwa 20 Minuten vor der Öffentlichkeit diese Informationen und haben ausreichend Zeit, über eine Kursaussetzung zu entscheiden. Ad-hoc-Mitteilungen sind geeignet, einen erheblichen Kursausschlag herbeizuführen.

Die Unterbrechung des Börsenhandels ist eine Ermessensentscheidung der Börsengeschäftsführung. Ist die Entscheidung der Börsengeschäftsführung zur Kursaussetzung gefallen, erhält der letzte Kurs des betroffenen Wertpapiers den Kurszusatz

Der Anleger kann aus einer Kursaussetzung schließen, dass Ereignisse eingetreten sind, die für die Bewertung seines Handelsobjekts maßgeblich sein können. Kursaussetzungen führen dann – anders als bei der Handelsunterbrechung („trading halt“) – zum Erlöschen aller betroffenen Orders (§ 6 Abs. 4 Börsengeschäftsbedingungen). Denn die Marktteilnehmer haben angesichts der veröffentlichten kursbeeinflussenden Tatsachen „kein Interesse mehr an der Ausführung ihrer in Unkenntnis dieser Tatsachen erteilten Kauf- und Verkaufsaufträge“. Wegen der schwerwiegenden Folgen einer Kursaussetzung durch die Börsenorgane ist ein solcher Vorgang die Ausnahme. Nach Kursaussetzung wird mit dem zuletzt festgestellten Kurs begonnen bzw. bei Kursaussetzung vor Börsenbeginn mit dem Schlusskurs des Vortages.

Aussetzungen oder Einstellungen führen zu keinen Umsätzen, so dass die Kauf- und Verkaufsabsichten nicht realisiert werden können. Diese Maßnahmen zielen darauf ab, durch Unterbrechung des Handels die Marktteilnehmer zu beruhigen und ihnen Zeit für die Beschaffung aktueller Informationen zu geben, um das Informationsgleichgewicht wiederherzustellen. Matthias Ecke wies 2005 nach, dass Ereignisse mit Kursaussetzungen durch eine starke Reaktion bei Wiederaufnahme des Handels gekennzeichnet sind. Im Vergleich mit – nicht kursausgesetzten – Ad-hoc-Meldungen zeigen Kursaussetzungen stärkere Ereigniseffekte. Ecke gelangt zu dem Ergebnis, dass Emittenten und Anleger darauf vertrauen können, dass Kursaussetzungen in der Regel nur dann vorgenommen werden, wenn sie mit sehr sensitiven Informationen in Verbindung stehen.

Da starke Kursschwankungen im Rahmen eines Spillovers von der Finanzwirtschaft auch auf die Realwirtschaft übergreifen können, kann eine Kursaussetzung auch Finanz- und Wirtschaftskrisen abschwächen oder gar verhindern.

In den USA ist bei „Trading halts“ oder „Suspensions“ zu unterscheiden, ob die US-Finanzaufsicht "Security Exchange Commission" (SEC) oder die Börse eine Aussetzung initiiert hat. In den meisten Fällen werden Aussetzungen von den Börsen selbst initiiert. Die Gründe dafür können Verstöße gegen Listing-Bedingungen, gegen handelssegmentspezifische Anforderungen oder Nichtnachkommen von Publizitätspflichten sein. Der am meisten zu beobachtende Grund sind anstehende Unternehmensnachrichten. Die SEC hat die Möglichkeit, Aktien bis zu 10 Handelstage vom Börsenhandel auszuschließen. Die Handelsaussetzung bleibt bis 23.59 Uhr des zehnten Handelstages nach der Ankündigung der Aktion bestehen.

Handelsaussetzungen beschränken sich nicht auf Aktien, sondern können sich auf alle Handelsobjekte erstrecken, so auch auf Staatsanleihen, Leerverkäufe oder Geld. So wurden im Jahr 2002 sowohl in Argentinien als auch Uruguay so genannte Bankfeiertage ausgerufen. Effektiv wurde der Handel mit Geld, Abhebungen und Überweisungen ausgesetzt. Auch in diesen Fällen war die Stabilisierung der jeweiligen Währung das Ziel. Die längerfristige Aussetzung des Handels mit dem einzig allgemein akzeptierten Tauschmittel eines Landes hat fatale Folgen für die arbeitsteilige Wirtschaft: Ohne Tauschmittel kommt die Arbeitsteilung zum Erliegen. In der Regel folgt solchen Umständen eine allgemeine Verelendung.

Befürworter sehen in der Aussetzung des Handels mit Wertpapieren einzelner Unternehmen einen Anlegerschutz. So wird der Handel mit Aktien eines Unternehmens ausgesetzt, wenn dieses Unternehmen Pflichtmitteilungen zu tätigen hat, die einen erheblichen Einfluss auf den Kurs haben können wie z. B. Insolvenzanträge, Fusionen eines Unternehmens oder Unternehmensübernahmen. Hierzu bekommen die Kontrollorgane die Pflichtmitteilung vor der Veröffentlichung und können den Handel aussetzen, bevor die Nachricht bekannt wird.

Kritiker sehen in Aussetzungen des Handels einen Widerspruch zur Idee des wirtschaftlichen Liberalismus, da dort Märkte frei von Beeinflussung sein sollen. Häufig wird der Handel eines bestimmten Wertpapiers oder gleich aller Wertpapiere an einer Wertpapierbörse ausgesetzt. Die Begründung dafür ist, dass bei steigenden Kursen Wertpapiere gekauft werden, womit der Kurs noch weiter steigt, und dass bei fallenden Kursen Wertpapiere verkauft werden, womit der Kurs noch weiter fällt. Die Nichtaussetzung von Kursen hat möglicherweise trendverschärfende Effekte.



</doc>
<doc id="419" url="https://de.wikipedia.org/wiki?curid=419" title="Aššur-nâṣir-apli II.">
Aššur-nâṣir-apli II.

Aššur-nâṣir-apli II. (auch "Aschschur-nasir-apli", "Assur-nasirpal") regierte als assyrischer König von 883 bis 859 v. Chr. Er war Sohn des Tukulti-Ninurta II.

Das Neuassyrische Reich gewann unter ihm wieder die Ausmaße der Zeit von Tukulti-apil-Ešarra I. Feldzüge führte Aššur-nâṣir-apli u. a. gegen Babylon und Urartu. In einem Feldzug gegen die Aramäer wurden 300 Talente Eisen als Kriegsbeute erwähnt, die den wirtschaftlichen Aufschwung der Aramäer deutlich machen.

Die Berichte seiner Eroberungen sind in übertreibenden Phrasen geschrieben. So wird die flache Gebirgsgruppe des Dschabal Bischri südwestlich des Euphrat als "hohe Berge, deren Spitzen die Vögel nicht erreichen" beschrieben.

Die Tribute und die Beute wurden unter anderem benötigt, um Bauvorhaben in Aššur, Ninive und Kalḫu zu finanzieren. Aššur-nâṣir-apli verlegte die Hauptstadt von Assur nach Kalḫu. Die Einweihung von Kalḫu wurde zehn Tage lang gefeiert. Anwesend waren rund 70.000 Gäste (Assyrer, Fremde aus mindestens zwölf Ländern, darunter Sidon, Tyros, Muṣaṣir, Kumme, Gurgum, Gilzanu und Melid), wie auf der Bankett-Stele beschrieben. Der Palast besaß auch einen prächtigen Garten. In Imgur-Enlil errichtete er einen Tempel des Traumgottes Mamu, dessen Tore er mit Bronzereliefs verzieren ließ.

Aššur-nâṣir-apli führte Verwaltungsreformen durch. Ein eng gespanntes Botennetz erlaubte eine schnelle Nachricht von Aufständen, die so im Keim erstickt werden konnten. Unter Aššur-nâṣir-apli II. wurden die Deportationen in großem Stil ausgeweitet. Sein Nachfolger war sein Sohn Salmānu-ašarēd III. Seine Gemahlin war Mullissu-mukannišat-Ninua, die auch Mutter des Nachfolgers war.



</doc>
<doc id="420" url="https://de.wikipedia.org/wiki?curid=420" title="Antioxidans">
Antioxidans

Ein Antioxidans oder Antioxidationsmittel (Mehrzahl Antioxidantien, auch Antioxidanzien) ist eine chemische Verbindung, die eine Oxidation anderer Substanzen verlangsamt oder gänzlich verhindert.

Antioxidantien haben eine große physiologische Bedeutung durch ihre Wirkung als Radikalfänger. Sie inaktivieren im Organismus reaktive Sauerstoffspezies (ROS), deren "übermäßiges Vorkommen" zu oxidativem Stress führt, der in Zusammenhang mit dem Altern und der Entstehung einer Reihe von Krankheiten gebracht wird. "Geringe, d. h. physiologische Mengen" an ROS dagegen sind als Signalmoleküle, die die Stressabwehrkapazität, Gesundheit und Lebenserwartung von Modellorganismen und des Menschen steigern, durchaus erforderlich. Eine nahrungsergänzende Zufuhr (Supplementierung) von Antioxidantien kann daher bestimmten Studien zufolge zu einer gesteigerten Krebshäufigkeit und zu einem erhöhten Sterberisiko des Menschen führen. 

Antioxidationsmittel sind außerdem von großer, insbesondere technologischer Bedeutung als Zusatzstoffe für verschiedenste Produkte (Lebensmittel, Arzneimittel, Bedarfsgegenstände, Kosmetik, Gebrauchsmaterialien), um darin einen – besonders durch Luftsauerstoff bewirkten – oxidativen Abbau empfindlicher Moleküle zu verhindern. Der oxidative Abbau bestimmter Inhaltsstoffe oder Bestandteile wirkt sich wertmindernd aus, weil sich Geschmack oder Geruch unangenehm verändern (Lebensmittel, Kosmetika), die Wirkung nachlässt (bei Arzneimitteln), schädliche Abbauprodukte entstehen oder physikalische Gebrauchseigenschaften nachlassen (z. B. bei Kunststoffen). Insofern ist insbesondere die Lebensmittelindustrie mit dem Problem konfrontiert, trotz inzwischen bekannter negativer Auswirkungen auf die menschliche Gesundheit (siehe oben) weiterhin zwingend auf die Verwendung von Antioxidantien angewiesen zu sein. 

Nach Art des chemischen Wirkmechanismus werden Antioxidantien in Radikalfänger und Reduktionsmittel unterschieden. Im weiteren Sinne werden auch Antioxidations-Synergisten zu den Antioxidantien gerechnet.

Bei Oxidationsreaktionen zwischen organischen Verbindungen treten vielfach kettenartige Radikalübertragungen auf. Hier werden Stoffe mit sterisch behinderten Phenolgruppen wirksam, die im Ablauf dieser Übertragungen reaktionsträge stabile Radikale bilden, die nicht weiter reagieren, wodurch es zum Abbruch der Reaktionskaskade kommt (Radikalfänger). Zu ihnen zählen natürliche Stoffe wie die Tocopherole und synthetische wie Butylhydroxyanisol (BHA), Butylhydroxytoluol (BHT) und die Gallate. Sie sind wirksam in lipophiler Umgebung.

Reduktionsmittel haben ein sehr niedriges Redox-Potential - ihre Schutzwirkung kommt dadurch zustande, dass sie eher oxidiert werden als die zu schützende Substanz. Vertreter sind etwa Ascorbinsäure (−0,04 V bei pH 7 und 25 °C), Salze der Schwefligen Säure (+0,12 V bei pH 7 und 25 °C) und bestimmte organische schwefelhaltige Verbindungen (z. B. Glutathion, Cystein, Thiomilchsäure), die vorwiegend in hydrophilen Matrices wirksam sind.

Synergisten unterstützen die Wirkung von Antioxidantien, beispielsweise, indem sie verbrauchte Antioxidantien wieder regenerieren. Durch Komplexierung von Metallspuren (Natrium-EDTA) oder Schaffung eines oxidationshemmenden pH-Wertes können Synergisten die antioxidative Wirkung eines Radikalfängers oder Reduktionsmittels verstärken.

Viele Antioxidantien sind natürlich und endogen vorkommende Stoffe. Im Säugetierorganismus stellt das Glutathion ein sehr wichtiges Antioxidans dar, auch eine antioxidative Aktivität von Harnsäure und Melatonin ist bekannt. Ferner sind Proteine wie Transferrin, Albumin, Coeruloplasmin, Hämopexin und Haptoglobin antioxidativ wirksam. Antioxidative Enzyme, unter denen die wichtigsten die Superoxiddismutase (SOD), die Glutathionperoxidase (GPX) und die Katalase darstellen, sind zur Entgiftung freier Radikale in den Körperzellen ebenfalls von entscheidender Bedeutung. Für ihre enzymatische Aktivität sind Spurenelemente wie Selen, Kupfer, Mangan und Zink wichtig. Als antioxidativ wirksames Coenzym ist Ubichinon-10 zu nennen. Für den menschlichen Organismus essentiell notwendige und antioxidativ wirksame Stoffe wie Ascorbinsäure (Vitamin C), Tocopherol (Vitamin E) und Betacarotin (Provitamin A) können nicht bedarfsdeckend synthetisiert werden und müssen mit der Nahrung zugeführt werden (exogene Antioxidantien). Eine Reihe von Antioxidantien werden als Bestandteil der Muttermilch an den Säugling weitergegeben, um dort ihre Wirkung zu entfalten.

Als sekundäre Pflanzenstoffe kommen Antioxidantien wie Carotinoide und verschiedenste polyphenolische Verbindungen (Flavonoide, Anthocyane, Phytoöstrogene, Nordihydroguajaretsäure und andere) in zahlreichen Gemüse- und Obstarten, Kräutern, Früchten, Samen etc. sowie daraus hergestellten Lebensmitteln vor. 

Zu den künstlichen Antioxidationsmitteln zählen die Gallate, Butylhydroxyanisol (BHA) und Butylhydroxytoluol (BHT). Durch eine synthetische Veresterung der Vitamine Ascorbinsäure und Tocopherol wird deren Löslichkeit verändert, um das Einsatzgebiet zu erweitern und verarbeitungstechnische Eigenschaften zu verbessern (Ascorbylpalmitat, Ascorbylstearat, Tocopherolacetat).

Freie Radikale sind hochreaktive Sauerstoffverbindungen, die im Körper gebildet werden und in verstärktem Maß durch UV-Strahlung und Schadstoffe aus der Umwelt entstehen. Ihr Vorkommen "im Übermaß" (oxidativer Stress) erzeugt Zellschäden und gilt nicht nur als mitverantwortlich für das Altern, sondern wird auch in Zusammenhang mit der Entstehung einer Reihe von Krankheiten gebracht. "Geringe, d.h. physiologische Mengen" an ROS dagegen sind als Signalmoleküle, die die Stressabwehrkapazität, Gesundheit und Lebenserwartung von Modellorganismen und des Menschen steigern, erforderlich. Einen Schutz vor den schädlichen Folgen zu hoher Mengen an freien Radikalen stellt das körpereigene Abwehrsystem dar, welches durch geringe Mengen an ROS – einer Impfung ähnlich – immer wieder aktiviert wird (siehe auch Mitohormesis). 

Außer endogen gebildeten Antioxidantien wirken im Abwehrsystem auch solche, die mit der Nahrung zugeführt werden. Eine gesunde Ernährung unter Einbeziehung von mit an antioxidativ wirksamen Stoffen reichen Lebensmitteln gilt als effektive Vorbeugung vor Herz-Kreislauferkrankungen, eine Schutzwirkung vor bestimmten Krebsarten wird als möglich erachtet. Beides jedoch wird inzwischen nicht mehr als durch aussagekräftige Studien gesichert betrachtet. 
Neuere Studien einer schwedischen Forschergruppe erbrachten vielmehr bei Versuchen an Mäusen inzwischen Indizien dafür, dass Antioxidantien bei Hautkrebs diesen schneller Tochtergeschwulste bilden lässt – „die Ergebnisse müssen allerdings noch am Menschen bestätigt werden“. 

Die neueren wissenschaftlichen Erkenntnisse veranlassten die Zeitschrift Nature, die Behauptung, dass freie Radikale eine schnellere Alterung bewirkten und diese Wirkung durch Antioxidantien verhindert werden könne, als Mythos zu bezeichnen. "„Die Vorstellung von Oxidation und Altern wird von Leuten am Leben gehalten, die damit Geld verdienen.“" Als Vitamine oder Vorstufen von Vitaminen haben die Antioxidantien beta-Carotin, Vitamin A, Vitamin C und Vitamin E demzufolge, von der Vorbeugung heutzutage extrem seltener Mangelerscheinungen abgesehen, keinerlei erwiesene Rolle für die menschliche Gesundheit. Vielmehr bewirkt die nahrungsergänzende Zufuhr von Vitamin E sowie Vitamin A und dessen Carotinoid-Vorstufen beim Menschen eine gesteigerte Entstehung von Krebs sowie eine Verringerung der Lebenserwartung , während Vitamin C als Supplement bestenfalls wirkungslos ist. 

Die Beurteilung polyphenolischer Pflanzeninhaltsstoffe dagegen ist in diesem Zusammenhang deutlich besser gesichert, und die wissenschaftliche Beweislage für die gesundheitsfördernde Wirkung bestimmter Polyphenole, besonders der im Tee, Kakao, Beeren und Rotwein vorkommenden Flavanole, hat sich in den letzten Jahren verstärkt. Dies scheint aber nicht damit in Verbindung zu stehen, dass diese Substanzen antioxidative Eigenschaften "in vitro" besitzen. Ein Expertengutachten geht davon aus, dass die antioxidative Kapazität, welche die Polyphenole und Flavonoide "in vitro" zeigen, kein Messwert für deren Wirkung im menschlichen Körper ist. Die Europäische Behörde für Lebensmittelsicherheit (EFSA) schloss sich dieser Einschätzung weitgehend an.

Nach einer US-amerikanischen Untersuchung aus dem Jahr 2005 stammt der mit Abstand größte Teil der mit der täglichen Nahrung zugeführten physiologischen Antioxidantien in den USA aus dem Genussmittel Kaffee, was allerdings weniger daran liege, dass Kaffee außergewöhnlich große Mengen an Antioxidantien enthalte, als vielmehr an der Tatsache, dass die US-Amerikaner zu wenig Obst und Gemüse zu sich nähmen, dafür aber umso mehr Kaffee konsumierten.

Die antioxidative Kapazität eines Lebensmittels und somit die Fähigkeit zum Abfangen von Sauerstoffradikalen wird mit dessen ORAC-Wert angegeben.

Antioxidativ wirksame Substanzen werden in einer Reihe von Nahrungsergänzungsmitteln als „Anti-Aging“-Präparate und zur Krankheitsprävention (z. B. vor Krebs) auf dem Markt angeboten. Die enthaltenen antioxidativen Substanzen kommen auch natürlicherweise in der Nahrung vor, außerdem werden sie vielen Lebensmitteln zugesetzt, sodass in der Regel kein Mangel besteht. Es fehlen belastbare wissenschaftliche Nachweise, dass die Einnahme von Nahrungsergänzungsmitteln – in denen antioxidativ wirksame Substanzen meist isoliert und nicht im Verbund mit natürlichen Begleitstoffen enthalten sind – gesundheitlich vorteilhaft ist.
Im Gegenteil bewirkt die Supplementierung der Antioxidantien beta-Carotin, Vitamin A, Vitamin C sowie Vitamin E beim gesunden Menschen eine gesteigerte Entstehung von Krebs und eine Verringerung der Lebenserwartung , während Vitamin C als Supplement bzgl. Krebs und Lebenserwartung wirkungslos ist. Bei gesunden Sportlern wurde in einer 2009 veröffentlichten Studie ein kontraproduktiver Einfluss von Vitamin C und E auf den Trainingseffekt und die Vorbeugung von Diabetes gemessen, da diese Antioxidantien den Anstieg von Radikalen im Körper unterdrücken, so dass er sich weniger gut an die Belastung anpasste.

Auch bei bestimmten pathologischen Zuständen soll sich eine antioxidative Nahrungsergänzung nachteilig auswirken: bei Krebspatienten wurden Wechselwirkungen mit antineoplastischen Behandlungsmethoden (Chemotherapie, Strahlentherapie) oder andere schädliche Auswirkungen beschrieben. 

Die Bestimmung der totalen antioxidativen Kapazität ("total antioxidant capacity", TAC) in Körperflüssigkeiten liefert einen pauschalen Eindruck über die relative antioxidative Aktivität einer biologischen Probe. Es stehen verschiedene Möglichkeiten für die Bestimmung der antioxidativen Kapazität in Körperflüssigkeiten zur Verfügung. Das Grundprinzip all dieser Methoden ist gleich. Die in der biologischen Probe enthaltenen Antioxidantien schützen ein Substrat vor dem durch ein Radikal induzierten oxidativen Angriff. Die Zeitspanne und das Ausmaß, mit der die Probe diese Oxidation verhindert, kann bestimmt werden und wird meist mit Trolox (wasserlösliches Vitamin-E-Derivat) oder Vitamin C als Standard verglichen. Je länger es dauert, ein Substrat zu oxidieren, desto höher ist die antioxidative Kapazität. Durch verschiedene Extraktionen kann man die antioxidative Kapazität lipidlöslicher und wasserlöslicher Substanzen untersuchen. Oft angewandte Tests sind TRAP, ORAC, TEAC, FRAP und PCL.

Im Jahre 2010 wurde in den USA die totale antioxidative Kapazität durch Ernährung und Nahrungsergänzungsmittel bei Erwachsenen untersucht. Dabei wurden Datenbanken des US-Department für Landwirtschaft, Daten zu Nahrungsergänzungsmitteln und zum Lebensmittelverzehr von 4391 US-Erwachsenen im Alter ab 19 Jahren ausgewertet. Um die Daten zur Aufnahme von einzelnen antioxidativen Verbindungen zu TAC-Werten zu konvertieren, wurde die Messung des Vitamin-C-Äquivalent (VCE) von 43 antioxidativen Nährstoffen zuvor angewendet. Die tägliche TAC lag durchschnittlich bei 503,3 mg VCE/Tag, davon ca. 75 % aufgenommen durch die Nahrung und 25 % durch Nahrungsergänzungsmittel.

Die Hochleistungsflüssigkeitschromatographie gilt als Goldstandard zur Bestimmung von Antioxidantien. Die Analyse erfordert entweder Blutproben oder die Entnahme von Hautbiopsien. Zur Analyse kurzfristiger Änderungen eignet sie sich deshalb nur bedingt. Aus diesem Grund wurden Verfahren entwickelt, mit denen Antioxidantien nichtinvasiv durch direkte Messung an spezifischen Hautarealen bestimmt werden. Gemessen werden Carotinoide als bester biologischer Marker für den Konsum von Obst und Gemüse.

Ein in diesem Zusammenhang wichtiges Verfahren ist die Resonanz-Raman-Spektroskopie. Prinzipiell erfordert sie schmalbandige Lichtquellen (meist Laser) zur Beleuchtung. Das aus der Haut zurückgestreute Licht wird über ein dispersives Element (meist ein Optisches Gitter) in seine spektralen Anteile zerlegt. Während die Messung in der Vergangenheit mit Laborgeräten erfolgte, sind inzwischen auch Tischgeräte verfügbar.

Ein weiteres Verfahren, mit dem Antioxidantien erfolgreich am Menschen gemessen wurden, ist die Reflexionsspektroskopie. Im Unterschied zur Resonanz-Raman-Spektroskopie kommen zur Beleuchtung der Haut breitbandige Lichtquellen oder LEDs zum Einsatz. Der apparative Aufwand ist insgesamt geringer, so dass Antioxidantien-Scanner auch als Handgeräte realisiert werden können, die in ihrem Messergebnis jedoch sehr gut mit den Ergebnissen der Resonanz-Raman-Spektroskopie korrelieren.

In der Industrie werden Antioxidantien als Zusatzstoffe (Additive) benötigt, um die oxidative Degradation von Kunststoffen, Elastomeren und Klebstoffen zu verhindern. Sie dienen außerdem als Stabilisatoren in Treib- und Schmierstoffen. In Kosmetika auf Fettbasis, etwa Lippenstiften und Feuchtigkeitscremes, verhindern sie Ranzigkeit. In Lebensmitteln wirken sie Farb- und Geschmacksverlusten entgegen und verhindern ebenfalls das Ranzigwerden von Fetten.

Obwohl diese Additive nur in sehr geringen Dosen benötigt werden, typischerweise weniger als 0,5 %, beeinflussen ihr Typ, die Menge und Reinheit drastisch die physikalischen Parameter, Verarbeitung, Lebensdauer und oft auch Wirtschaftlichkeit der Endprodukte. Ohne Zugabe von Antioxidantien würden viele Kunststoffe nur kurz überleben. Die meisten würden sogar überhaupt nicht existieren, da viele Plastikartikel nicht ohne irreversible Schäden fabriziert werden könnten. Das Gleiche gilt auch für viele andere organische Materialien.

Es kommen hauptsächlich sterisch gehinderte Amine ("hindered amine stabilisers", HAS) aus der Gruppe der Arylamine zum Einsatz und sterisch gehinderte Phenolabkömmlinge, die sich strukturell oft vom Butylhydroxytoluol ableiten (Handelsnamen "Irganox", "Ethanox", "Isonox" und andere).

Zulässige Antioxidantien sind in Deutschland in der Zusatzstoff-Zulassungsverordnung und der Kosmetik-Verordnung geregelt. Es kommen sowohl natürliche als auch synthetische Antioxidantien zum Einsatz.

Beispiele für antioxidative Lebensmittelzusatzstoffe sind in der Tabelle angegeben.

Als Lebensmittelzusatz aufgrund lebertoxischer Wirkungen seit 1968 nicht mehr erlaubt ist die Nordihydroguajaretsäure, ein höchst wirksames Antioxidans zur Haltbarmachung von Fetten und Ölen, das aber weiterhin in kosmetischen Präparaten zulässig ist.

Lebensmitteltechnisch und pharmazeutisch gebräuchliche Antioxidationssynergisten sind unter anderem Citronensäure und ihre Salze (E330–E333), Weinsäure und ihre Salze (E334–E337), Phosphorsäure und ihre Salze (E338–E343) und Ethylendiamintetraessigsäure (EDTA) und ihre Salze (Calciumdinatrium-EDTA, E385).



</doc>
<doc id="422" url="https://de.wikipedia.org/wiki?curid=422" title="Aszites">
Aszites

Der Aszites (gr. "askítēs"; Synonyme Hydraskos, Bauchwassersucht) ist eine pathologische (krankhafte) Ansammlung meist klarer seröser Flüssigkeit im Peritonealraum.

Bei Gesunden enthält der Peritonealraum zirka 50 bis 70 Milliliter Flüssigkeit, bei vielen Krankheiten lässt sich dagegen erheblich mehr Flüssigkeit nachweisen.

Kleinere Aszitesmengen sind meist symptomlos. Erst größere Volumina machen sich als Schwellung des Bauches bemerkbar, die meist schmerzlos ist.

Allen Ursachen gemeinsam ist der Übertritt von Flüssigkeit aus den Blutgefäßen in den Peritonealraum. Häufigste Ursache mit rund 75 % ist die Leberzirrhose, die durch eine Erhöhung des Druckes im Pfortaderkreislauf zu einem Flüssigkeitsaustritt aus den Blutgefäßen führt. Aszites kann auch bei einem akuten Leberschaden auftreten. Ebenso kann ein Verschluss der venösen Sinus der Leber z. B. beim Budd-Chiari-Syndrom zur Bildung von Aszites führen. Auch bei einer Herzinsuffizienz oder einer Pericarditis constrictiva kann es zu Aszites kommen. Tumore der Leber oder Metastasen in den Bauchraum können ebenso eine Flüssigkeitsansammlung bedingen. Bei Perforationen von Hohlorganen führt eine sekundäre Peritonitis auch zur Bildung von freier Flüssigkeit im Peritonealraum. Eine Tuberkulose kann solchen Aszites chronisch bedingen. Ebenso kann eine Pankreatitis oder Fisteln des Gallen- oder Pankreassystems zur Bildung von Aszites führen. Bei schwerer Unterernährung kommt es durch Albuminmangel oft zu Aszites.


Um zu klären, warum sich Aszites gebildet hat, ist eine Punktion des Peritonealraums und Untersuchung der Flüssigkeit obligat.

Der Aszites ist meist eine klare Flüssigkeit. Milchiger Aszites weist auf eine Verletzung oder Störung des Lymphabflusses (zum Beispiel durch ein Trauma) hin, kann aber auch bei anderen Erkrankungen vorkommen. Dunkelbrauner Aszites erhält seine Farbe oft durch einen hohen Anteil an Bilirubin und ist hinweisend für ein Galleleck. Schwarzer Aszites kann auf Nekrosen des Pankreas oder ein metastasiertes Melanom hinweisen.

Der Albumingehalt des Aszites in Relation zum Albumingehalts des Bluts kann einen Hinweis für die Genese erbringen. Beträgt dieser Serum-Aszites-Albumin-Gradient (SAAG) mehr als 1,1 g/dl, ist von einer Bildung des Aszites durch einen Bluthochdruck in der Pfortader auszugehen. Dabei spricht ein Gradient von 1,1 g/dl bis 2,5 g/dl für das Vorliegen einer Leberzirrhose, der Spätform eines Budd-Chiari-Syndroms oder einer massiven Metastasierung der Leber. Ein über 2,5 g/dl erhöhter Gradient spricht für Aszitesbildung im Rahmen einer Herzinsuffizienz oder eines frühen Budd-Chiari-Syndroms. Ein SAAG von kleiner 1,1 g/dl spricht gegen eine Genese des Aszites im Rahmen eines Pfortaderhochdrucks. Dies kann im Rahmen einer Pankreatitis, Peritonealkarzinose, eines Gallelecks, Tuberkulose oder eines nephrotischen Syndroms auftreten. Eine laborchemische Untersuchung auf Zellzahl kann eine spontan bakterielle Peritonitis nachweisen. Mikrobiologische und zytologische Untersuchungen sind ebenso sinnvoll.


Eine gefährliche Komplikation des Aszites ist die spontane bakterielle Peritonitis (SBP): Bei etwa 15 % der Patienten mit portalem Aszites (also Aszites aufgrund einer Druckerhöhung in der Pfortader wie bei Leberzirrhose) kommt es zu einer Auswanderung von Darmbakterien aus dem Darm mit anschließender Peritonitis. Die häufigsten Erreger sind hierbei "Escherichia coli" (50 %), grampositive Kokken (30 %) und Klebsiellen (10 %). Die Patienten haben meist weder Fieber noch Abdominalschmerzen, diagnostisch hilft die Aszitespunktion, bei der sich über 250 Granulozyten/µl finden. Der Keimnachweis gelingt oft nicht. Dennoch ist die SBP mit einer hohen Letalität von bis zu 50 % verbunden. Therapie: Cephalosporine der dritten Generation, anschließend Rezidivprophylaxe mit oralem Fluorchinolon.

Eine neuere mikrobiologische Studie widerlegt allerdings die Vermutung, dass die für SBP verantwortlichen Bakterien ausschließlich Mitglieder der Darmflora sind. Außerdem waren Bakterien bereits vor dem Erscheinen der SBP-Symptome nachweisbar. Die genauen Mechanismen, die zu einer Besiedlung des Peritoneums führen, sind daher weitgehend unklar.<ref name="DOI10.1371/journal.pone.0074884">Geraint B. Rogers, Christopher J. van der Gast u. a.: "Ascitic Microbiota Composition Is Correlated with Clinical Severity in Cirrhosis with Portal Hypertension." In: "PLoS ONE." 8, 2013, S. e74884, .</ref>

Leichte Fälle des Aszites können mit Natriumrestriktion behandelt werden. Etabliert hat sich beim portalen Aszites auch die Gabe von Spironolacton, einem Aldosteronantagonisten. Elektrolyte und Gewicht müssen regelmäßig kontrolliert werden, ebenso sollte man eine Flüssigkeitsbilanz ziehen.

Mittelschwere Fälle werden mit der zusätzlichen Gabe eines Schleifendiuretikums, z. B. Furosemid, behandelt. Die Ausschwemmung sollte schonend erfolgen, d. h. nicht mehr als 500 g Gewichtsabnahme pro Tag, um der Entstehung eines hepatorenalen Syndroms vorzubeugen.

Schwere, therapierefraktäre Verläufe können zusätzlich mit Parazentese, also der Abpunktion der Flüssigkeit, mit gleichzeitiger Albumingabe und anschließender Rezidivprophylaxe mit Diuretika (Medikamente zur Steigerung der Nierenausscheidung), behandelt werden. Bei dieser Methode wird der Erguss durch die Bauchdecke punktiert und abgelassen. Da sich der Aszites meist schnell wieder bildet, muss diese Methode zwangsläufig wiederholt werden. Dies kann vom Arzt zwar ambulant durchgeführt werden. Nachteil dabei ist, dass bei jeder Wiederholung das Risiko der Blutung, Bakterien-Infektion des Bauchraums und Verletzungen vorhanden ist. Es hat sich gezeigt, dass Patienten aufgrund der Schmerzen und Unannehmlichkeiten oft die Punktionen hinauszögern, bis die Symptome unerträglich sind. Als Alternative zu den fortlaufenden Punktionen hat sich in letzter Zeit das Legen eines dünnen Ablaufschlauches (PleurX Aszites) in die Bauchhöhle bewährt. Der im Bauchraum liegende Teil des Silikonschlauches hat mehrere Löcher, über die der Erguss in den Katheter eintreten kann. Außerhalb der Bauchhöhle verläuft der Schlauch im Unterhautfettgewebe, um bakterielle Entzündungen zu verhindern. Am Ende des Schlauches befindet sich ein Ventil, das Eintreten von Luft und das Auslaufen von Flüssigkeit verhindert, wenn keine aktive Entlastung durch den Patienten oder den Pflegedienst stattfindet. Die Entlastung des Ergusses erfolgt mit einer Vakuumflasche, die über ein Spezialventil mit dem Katheter verbunden wird. Wenn die Klemmen an der Drainageflasche geöffnet werden, kann Flüssigkeit aus dem Bauchraum aktiv und schnell abgelassen werden. Die meisten Patienten können dies nach einer Einweisung selbst bewerkstelligen. Der Katheter kann ambulant mit örtlicher Betäubung gelegt werden, das Ablassen des Aszites selbst ist schmerzfrei. Ebenso kann ein TIPS (Transjugulärer intrahepatischer portosystemischer (Stent-)Shunt), also eine Verbindung zwischen der Pfortader und der unteren Hohlader, angelegt werden. Hierbei kommt es anschließend allerdings zu einem beinahe ungehinderten Anstrom der normal von der Leber abgebauten Stoffe in den Körperkreislauf. 

Eine weitere Therapieoption stellt bei Leberzirrhose die Transplantation dar.

Maligner Aszites wird häufig mit wiederholten Parazentesen behandelt. Zudem kommen Shunts und Chemotherapien zum Einsatz, die teilweise direkt in den Peritonealraum (intraperitoneal) verabreicht werden, ebenso wie der speziell für die Therapie des Malignen Aszites zugelassene Antikörper Catumaxomab.
Zwei wissenschaftliche Studien aus den Jahren 2011 und 2013 zeigen als Alternative zur Parazentese bei malignem und nicht malignem Aszites die Anlage eines im Unterhautfettgewebe getunnelten Katheters. Der Patient, seine Angehörigen oder der Pflegedienst kann im häuslichen Umfeld den Aszites drainieren.



</doc>
<doc id="423" url="https://de.wikipedia.org/wiki?curid=423" title="Aggregat 4">
Aggregat 4

Aggregat 4 (A4) war die Typenbezeichnung der im Jahr 1942 weltweit ersten funktionsfähigen Großrakete mit Flüssigkeitstriebwerk. Sie war als ballistische Artillerie-Rakete großer Reichweite konzipiert und das erste von Menschen konstruierte Objekt, das die Grenze zum Weltraum – nach Definition der Fédération Aéronautique Internationale mehr als 100 Kilometer Höhe – am 20. Juni 1944 durchstieß. Die A4 bildete ab Mitte 1945 den Grundstock der Raumfahrtentwicklungen der USA.

Die ballistische Boden-Boden-Rakete A4 wurde im Deutschen Reich in der Heeresversuchsanstalt Peenemünde (HVA) auf Usedom ab 1939 unter der Leitung von Wernher von Braun entwickelt und kam im Zweiten Weltkrieg ab 1944 in großer Zahl zum Einsatz. Neben der flugzeugähnlichen Fieseler Fi 103, genannt V1, bezeichnete die NS-Propaganda auch die Rakete A4 als kriegsentscheidende „Wunderwaffe“. Im Oktober 1944 wurde sie von Propagandaminister Joseph Goebbels zur "Vergeltungswaffe 2", kurz V2 erklärt. Die Starteinheiten von Wehrmacht und SS nannten sie schlicht „Das Gerät“. 

Für die Raketenentwicklung in der Heeresversuchsanstalt (HVA) bestand seit März 1936 folgendes Anforderungsprofil:
Eine Tonne Sprengstoff sollte über 250 Kilometer befördert werden.

Neben dem Technischen Direktor Wernher von Braun war eine große Zahl von Wissenschaftlern und Ingenieuren in der HVA tätig, unter ihnen Walter Thiel, Helmut Hölzer, Klaus Riedel, Helmut Gröttrup, Kurt Debus und Arthur Rudolph. Leiter der HVA bzw. deren Kommandant war Major Walter Dornberger, Chef der Raketenabteilung im Heereswaffenamt.

Die Vorgängermodelle des Aggregats 4 waren nur teilweise erfolgreich: Aggregat 1 explodierte beim Brennversuch in der Heeresversuchsanstalt Kummersdorf, Aggregat 2 absolvierte 1934 zwei erfolgreiche Starts auf Borkum und im Dezember 1937 hatte Aggregat 3 vier Fehlstarts. Erst der direkte Nachfolger Aggregat 5 war 1938 erfolgreich. Das Aggregat 4 wurde ab 1939 entwickelt und erstmals im März 1942 getestet. Am 3. Oktober 1942 gelang ein erfolgreicher Start, bei dem es mit einer Spitzengeschwindigkeit von fast Mach 5 (4824 km/h) eine Gipfelhöhe von 84,5 km erreichte und damit erstmals in den Grenzbereich zum Weltraum vordrang. Am 20. Juni 1944 wurde bei einem Senkrechtstart eine Höhe von 174,6 km erzielt.

Nach den Luftangriffen der Royal Air Force auf Peenemünde (s. Operation Hydra 17. August 1943) wurde beschlossen, die Ausbildung der Raketentruppen und die Erprobung der A4-Raketen nicht in Peenemünde, sondern in Südostpolen außerhalb der Reichweite der alliierten Bomber durchzuführen: anfangs für die westalliierten Bomber unerreichbar im Karpatenvorland auf dem SS-Truppenübungsplatz Heidelager bei Blizna im Generalgouvernement, wurden die Übungen wegen der anrückenden Roten Armee später auf den SS-Truppenübungsplatz Westpreußen in die Tucheler Heide nördlich von Bromberg verlegt.

Die Bevölkerung um Blizna war dabei rücksichtslos den A4- und V1-Einschlägen ausgeliefert. Auf Flugblättern warnte man vor Ort lediglich vor gefährlichen Kraftstoffbehältern, die aber keine Bomben seien.

Am 20. Mai 1944 stellten Mitglieder der polnischen Heimatarmee Teile einer abgestürzten A4 sicher. Die wichtigsten Teile wurden zusammen mit den in Polen vorgenommenen Auswertungen in der Nacht vom 25. zum 26. Juli 1944 mit einer DC-3 der RAF, die in der Nähe von Żabno gelandet war, nach Brindisi ausgeflogen (Operation Most III). Von dort aus kamen die Teile nach London.

Von der HVA Peenemünde und der Greifswalder Oie aus erfolgten noch bis einschließlich 20. Februar 1945 Versuchsstarts von A4-Raketen.

Die A4-Rakete war 14 Meter hoch und hatte eine Masse von 13,5 Tonnen. Die einstufige Rakete bestand aus etwa 20.000 Einzelteilen. Der Rumpf bestand aus Spanten und Stringern, die mit dünnem Stahlblech beplankt waren. Die Technik bestand aus vier Baugruppen:

Die etwa 738 kg Sprengstoff einer Amatol-Mischung waren in der Raketenspitze untergebracht. Da sich diese während des Flugs durch Kompressionswärme aufheizte, konnten nur Sprengstoffmischungen verwendet werden, deren Zündtemperatur über 200 °C lag.

Für die Stabilisierung und Steuerung sorgte das Leitwerk mit den Luftrudern, welche aber erst bei höherer Geschwindigkeit wirkten. Kurz nach dem Start waren die direkt im Gasstrom liegenden vier Strahlruder aus Graphit für die Stabilisierung zuständig. Alle Ruder wurden von Servomotoren bewegt.

Als einer der ersten Flugkörper war die A4 mit einem für die damalige Zeit sehr fortschrittlichen Trägheitsnavigationssystem ausgestattet, das mit zwei Kreiselinstrumenten (Gyroskopen) selbsttätig den eingestellten Kurs hielt. Die elektrische Energie für Kurssteuerung und Ruderanlage wurde den beiden Bordbatterien entnommen, die aus dem Werk Hagen der Accumulatoren Fabrik AG (AFA) stammten. Die Batterien waren unterhalb des Sprengkopfes im Geräteraum eingebaut, wo sich auch das sogenannte „Mischgerät“ befand, ein elektronischer Analogrechner, der die von den Gyroskopen registrierten Abweichungen von Quer- und Seitenachse auswertete und zur Kurskorrektur die Servomotoren der Strahl- und Luftruder ansteuerte. Um eine bessere Zielgenauigkeit zu erreichen, wurde in mehreren Versuchsraketen auch eine Funksteuerung erprobt, die aber im späteren Einsatz wegen möglicher Störungen von Seiten des Feindes nicht verwendet wurde.

Die beim Start eingestellte Zeitschaltuhr sorgte dafür, dass der Neigungswinkel der Kreiselplattform nach drei Sekunden Brennzeit so verändert wurde, dass die Rakete aus der Senkrechten in eine geneigte Flugbahn überging. Der Neigungswinkel war so eingestellt, dass sich je nach zu erzielender Entfernung die gewünschte ballistische Flugbahn ergab. Vor dem Start musste die Rakete auf ihrem Starttisch exakt senkrecht gestellt und so gedreht werden, dass eine besonders markierte Flosse in Zielrichtung zeigte.

Das Aggregat 4 war eine Flüssigkeitsrakete und wurde mit einem Gemisch aus 75-prozentigem Ethanol und Flüssigsauerstoff angetrieben. Unter der Leitung des Ingenieurs Walter Thiel wurden das beste Mischungsverhältnis der Treibstoffe, die Einspritzdüsenanordnung sowie die Formgebung des Raketenofens ermittelt. Eine Pumpenbaugruppe war nötig, welche die großen Mengen an Alkohol und flüssigem Sauerstoff in die Brennkammer fördern konnte, um die erforderliche Schubkraft des Triebwerks zu erzeugen. Zum Antrieb dieser Doppelpumpe diente eine integrierte Dampfturbine von 500 PS Leistung. In einem Dampferzeuger wurde durch die katalytische Zersetzung von Wasserstoffperoxid mittels Kaliumpermanganat Dampf erzeugt. Zur Förderung des Wasserstoffperoxids war auf 200 Bar komprimierter Stickstoff in mehreren Druckbehältern an Bord; dieser diente auch zur Betätigung diverser Ventile. Die Kreiselsteuerung und das präzise und daher sehr aufwendig zu fertigende Pumpenaggregat waren die beiden teuersten Bauteile des A4.

Die Rakete erreichte nach einer Brenndauer von etwa 60 Sekunden ihre Höchstgeschwindigkeit von etwa 5500 km/h, etwa Mach 5. Die Verbrennungsgase verließen den Brennofen (Raketenmotor) mit etwa 2000 m/s. Da der gesamte Flug bei einer Reichweite von 250 bis 300 km nur 5 Minuten dauerte, gab es damals keine Abwehrmöglichkeit gegen diese Waffe.

Die Fertigungsstätten für Teile der A4 waren über ganz Deutschland und Österreich verstreut: Unter dem Tarnnamen „Rebstock“ bei Ahrweiler an der Ahr wurden in unfertigen Eisenbahntunneln Bodenanlagen und Fahrzeuge für die Rakete unter Tage produziert. Zwischen 1942 und September 1944 wurde unter starker Geheimhaltung auch in Oberraderach gefertigt. Das Gelände wurde im Januar 1945 beim Herannahen französischer Truppen geräumt. Weitere Lieferanten waren die Firmen "Gustav Schmale" in Lüdenscheid, in der Teile der Brennkammer gefertigt wurden, und die "Accumulatoren Fabrik AG" (AFA) in Hagen-Wehringhausen, welche die speziellen Akkumulatoren herstellte. Anfang 1944 wurde im KZ-Nebenlager Redl-Zipf auf dem Gemeindegebiet von Neukirchen an der Vöckla der Betrieb eines Triebwerksprüfstandes aufgenommen.

1943 lief in insgesamt vier Orten die Serienfertigung der A4, welche, so Dornberger in einem Protokoll zu einer Besprechung mit Gerhard Degenkolb und Kunze, „grundsätzlich mit Sträflingen durchgeführt werde“. Dafür zog man Häftlinge aus folgenden Konzentrationslagern heran: KZ Buchenwald (HVA-Peenemünde ab Juni), KZ Dachau (Luftschiffbau Zeppelin „Friedrichshafener Zeppelinwerke“ ab Juni/Juli), KZ Mauthausen (Rax-Werke in Wiener Neustadt ab Juni/Juli) und KZ Sachsenhausen (DEMAG-Panzer in Falkensee bei Berlin ab März). Einzelne wissenschaftliche Mitarbeiter wählte Wernher von Braun persönlich unter den Häftlingen im KZ-Buchenwald aus.

Insgesamt wurden während des Zweiten Weltkrieges 5975 Raketen von Zwangsarbeitern, KZ-Häftlingen und deutschen Zivilbeschäftigten aus tausenden Einzelteilen zusammengebaut.

Am 29. Oktober 1944 wurde Dornberger nach dem Einsatz der V2 an der Westfront mit dem Ritterkreuz des Kriegsverdienstkreuzes mit Schwertern ausgezeichnet.

Ab 1944 fand die Montage der A4 im unterirdischen Komplex der Mittelwerk GmbH im Kohnstein nahe Nordhausen durch Häftlinge des KZ Mittelbau-Dora statt. Im Schnitt waren etwa 5000 Häftlinge des KZ Mittelbau unter Aufsicht von ungefähr 3000 Zivilangestellten mit dem Zusammenbau beschäftigt. Für das hochtechnologische Projekt wurden auch spezialisierte inhaftierte Facharbeiter und Ingenieure aus dem gesamten Reichsgebiet und den besetzten Staaten gezielt herangezogen. Obwohl viele von ihnen erst nach einer handwerklichen Prüfung in den Kohnstein verschleppt wurden, erwarteten sie dort keine besseren Arbeits- und Haftbedingungen als in anderen Konzentrationslagern. Vielmehr befürchteten sie, dass man sie wegen ihrer Einblicke in dieses Staatsgeheimnis nicht mehr freilassen würde. Wie unmenschlich die Behandlung auch durch zivile Ingenieure zeitweise war, zeigt etwa eine schriftliche Anweisung, die Häftlinge bei Verfehlungen nicht mehr mit spitzen Gegenständen zu stechen. Dennoch kam es immer wieder zu Sabotageakten, die allerdings die Fertigung der Rakete nie ernstlich behinderten. Zwar erwies sich bei der Endabnahme jede zweite Rakete als nicht voll funktionstüchtig und musste nachgebessert werden, dies lag jedoch in erster Linie daran, dass die Ingenieure aus Peenemünde fast täglich bauliche Änderungen vorgaben, die den laufenden Produktionsprozess erheblich beeinträchtigten.

16.000 bis 20.000 KZ-Häftlinge und Zwangsarbeiter, die meisten zwanzig- bis vierzigjährig, starben nach zurückhaltenden Schätzungen zwischen September 1943 und April 1945 im Lagerkomplex Mittelbau-Dora, auf Liquidations- oder sogenannten Evakuierungstransporten. Etwa 8.000 Menschen verloren ihr Leben durch den Einsatz der Waffe, die meisten im Raum London und Antwerpen (s. u. Einsatz).

Laut Jens-Christian Wagner, Leiter der Gedenkstätte KZ Mittelbau-Dora, sind somit Einziger Ingenieur der V2-Produktion, der je vor Gericht gestellt wurde, war der DEMAG-Geschäftsführer und Generaldirektor der Mittelwerk GmbH Georg Rickhey. 1947 im „Dachauer Dora-Prozess“ angeklagt, wurde er freigesprochen, obwohl im Prozess der Mitangeklagte Funktionshäftling Josef Kilian aussagte, dass Rickhey bei einer besonders brutal inszenierten Massenstrangulation von 30 Häftlingen am 21. März 1945 in Mittelbau-Dora anwesend war.

1943 gelang es der österreichischen Widerstandsgruppe rund um Kaplan Heinrich Maier durch die Verbindungen zum Wiener Stadtkommandanten Heinrich Stümpfl, der wahrscheinlich dem Widerstand zugerechnet werden muss, die exakten Zeichnungen der V-2-Rakete dem amerikanischen Office of Strategic Services zukommen zu lassen. Auch wurden Lageskizzen von V-Raketen-Fabrikationsanlagen in Peenemünde an alliierte Generalstäbe übermittelt, um damit alliierten Bombern Luftschläge zu ermöglichen. Die Gruppe wurde nach und nach von der Gestapo erkannt und die meisten Mitglieder wurden hingerichtet.

Anmerkung:
Für den Zeitraum zwischen Juli 1943 und Februar 1945 liegen keine vollständigen Startlisten vor. Bei einem Versuchsstart am 13. Juni 1944 zur Erprobung von Komponenten der Flugabwehrrakete Wasserfall stürzte eine von Peenemünde aus gestartete A4-Rakete in Südschweden ab.

Schon 1943 hatte die NS-Propaganda zur Erwiderung alliierter Luftangriffe auf deutsche Städte die Bombardierung Englands mit „Vergeltungswaffen“ angekündigt, um den Durchhaltewillen der deutschen Bevölkerung und den Kampfgeist der an der Front kämpfenden Soldaten aufrechtzuerhalten. Mit ständigen Beschwörungen von der Wirksamkeit der neuen „Wunderwaffen“ propagierte das NS-Regime den Glauben, die Wehrmacht habe mit neuen überlegenen Waffensystemen ein technisches Mittel in Händen, um die Wende im Krieg doch noch herbeiführen zu können. Allerdings schlug die nach dem Kriegseinsatz der „Vergeltungswaffen“ kurzzeitig entstandene euphorische Stimmung der Bevölkerung schon im Sommer 1944 wieder in Skepsis um, als die V-Raketen nicht die erwarteten spürbaren Erfolge erzielen konnten. Auch vor dem Hintergrund der sich abzeichnenden Niederlage versprach am 30. Januar 1945 Adolf Hitler in seiner letzten Rundfunkrede immer noch den „Endsieg“ durch einen verstärkten Einsatz sogenannter „Wunderwaffen“, zu denen auch die V2 gehörte.

Wie wenig die propagierte Bezeichnung „Vergeltungswaffe“ für die A4 zutraf, zeigen die Äußerungen von Walter Dornberger Ende März 1942, der Raketeneinsatz sei derart geplant, dass „bei Tag und Nacht in unregelmäßigen Abständen, unabhängig von der Wetterlage, sich lohnende Ziele wie London, Industriegebiete, Hafenstädte, pp. unter Feuer genommen werden“.
Zuvor hatte er schon, als er Juli 1941 für das neue Waffensystem warb, auf die „nicht mehr vorhandene Luftüberlegenheit“ hingewiesen. Damit nahm er ganz klar auf die verlorene Luftschlacht um England Bezug. Bereits ab Ende 1939 ging es schon dem Entwurf nach in der Sache um eine Kriegsrakete. Auch Hitler drohte Großbritannien deutlich im September 1940: „Wenn sie erklären, sie werden unsere Städte in großem Maße angreifen – wir werden ihre Städte ausradieren!“

Als am 8. September 1944 die erste A4 nur den Londoner Vorort Chiswick und nicht die Großstadt selbst traf, räumte Dornberger ein, dass es sich bei der A4 um eine „unzureichende“ Waffe handele. Trotzdem taufte Propagandaminister Goebbels die A4 sofort in V2 um und propagierte diese als „Vergeltungswaffe“.
Mit Sprengköpfen bestückt und von mobilen Startrampen aus wurden mit ihr vor allem London und später Antwerpen bombardiert; London nach offizieller Verlautbarung als Vergeltung für britische Bombenangriffe. Zwar war die Treffergenauigkeit gering, aber die plötzlichen Einschläge ohne Vorwarnung übten eine psychologische Wirkung (Demoralisierung) auf die Beschossenen aus, wenngleich diese wohl niedriger war als bei der V1. Während man bei Angriffen der V1 noch Fliegeralarm auslösen konnte, war dies durch die hohe Geschwindigkeit der A4 kaum noch möglich, da der Überschallknall erst nach der plötzlichen Explosion zu hören war.

Insgesamt wurden etwa 3200 Raketen abgefeuert:


Von Den Haag aus wurden 1039 Raketen gestartet, die vor allem auf London gerichtet waren. Bei einem alliierten Luftangriff auf die Startrampen am 3. März 1945 kamen 510 Menschen ums Leben.

In Frankreich waren mehrere große Bunker zum Start der A4 geplant oder im Bau, welche aber durch Bombardierungen oder wegen des Vormarschs der Alliierten nach der Invasion nicht mehr fertiggestellt wurden und nicht zum Einsatz kamen. Die bekanntesten sind das Blockhaus von Éperlecques, der Kuppelbau von Helfaut-Wizernes und die Anlagen im Raum Cherbourg.

Die Raketenstarts gegen die diversen Städte sind als reine Terrormaßnahmen gegen Zivilisten zu werten. Ausnahmen waren zum einen die elf erfolglosen Starts gegen die Ludendorff-Brücke bei Remagen und Erpel, nachdem die Rhein-Brücke von den Alliierten eingenommen worden war, zum anderen die 1610 Einsätze gegen den Seehafen von Antwerpen. Die V2-Treffer behinderten hier zumindest den Truppentransport der Alliierten für Wochen ganz erheblich. Am meisten hatte aber auch hier die Zivilbevölkerung zu leiden. Die letzte Rakete im Kampfeinsatz wurde am 27. März 1945 von deutscher Seite gegen Antwerpen gestartet.

Danach wurden nach und nach nahezu alle A4-Batterien aufgelöst. Trotzdem wurden noch Vorbereitungen für das VIII. Sonderschießen getroffen. Dazu war die ehemalige „Lehr- und Versuchsbatterie 444“, jetzt umbenannt in „Lehr- und Versuchsabteilung z. V.“, bereits am 28. Januar 1945 aus dem Einsatz in Holland zurückgezogen und zur Ruhe und Auffrischung nach Buddenhagen (Wolgast) befohlen worden. Von hier aus verlegte man diese Abteilung zusammen mit der „Gruppe Erprobung“ bzw. dem „Entwicklungskommando Rethem“ über Rethem (Aller) in den Raum Kirchlinteln (Kreis Verden (Aller)). Ziel des Sonderschießens war die „Schwerpunkterhöhung der Treffgenauigkeit und Einschlagprozente“. Die Zielpunkte lagen im Wattenmeer östlich der Insel Sylt und zwischen den dänischen Inseln Römö und Fanö.
Im Zeitraum von Mitte März 1945 bis zum 6. April 1945 wurden aus zwei Startstellungen etwa zehn Versuchsraketen abgefeuert. Dabei kam auch die Steuerung mit Hilfe der Leitstrahltechnik zum Einsatz. Nach dem Abzugsbefehl vom 6. April 1945 durch General Hans Kammler, der am 9. Mai Suizid beging, verlegte man die „Lehr- und Versuchsabteilung z. V.“ aus dem „Stellungsraum Neddenaverbergen“ (heute Gemeinde Kirchlinteln, Kreis Verden/Aller) über den Kreis Herzogtum Lauenburg nach Welmbüttel im Kreis Dithmarschen in Schleswig-Holstein, etwa 10 km östlich von Heide gelegen. Hier wurden die mitgebrachten Fahrzeuge und Sondergerätschaften und vermutlich auch einige Raketen, die durch eine nicht weiter bekannte Nachschubeinheit angeliefert worden waren, in einem Moor versenkt bzw. gesprengt. Am 1. Mai 1945 wurden noch 20 bis 30 Soldaten zu einem Flakregiment in den Raum Bargteheide/Trittau abgestellt. Ab dem 3. Mai 1945 wurde die letzte noch existierende und voll ausgerüstete A4-Abteilung aufgelöst, indem die noch verbleibenden Soldaten durch die Vorgesetzten offiziell entlassen wurden.

Der Einsatz der A4 als Terrorinstrument führte in London zu Diskussionen, diesen mit chemischen Waffen zu vergelten.

Insgesamt forderte der Einsatz der A4-Raketen mehr als 8000 Menschenleben, hauptsächlich Zivilisten. Die größte Zahl an Opfern auf einen Schlag war am 16. Dezember 1944 in Antwerpen zu beklagen, als eine A4 das vollbesetzte Kino „Rex“ traf und 567 Menschen tötete.

Am 24. Januar 1945 wurde in Peenemünde eine geflügelte Version der A4-Rakete, die A4b, erstmals erfolgreich gestartet. Sie sollte die doppelte Reichweite der A4 erreichen, stürzte allerdings wegen eines Flügelbruchs vorzeitig ab. Zu weiteren Starts dieses Flugkörpers kam es aufgrund der Kriegslage nicht mehr.

Von 1943 bis zum Kriegsende 1945 entwickelte man eine Interkontinentalrakete. Diese war als zweistufige Fernrakete ausgelegt und trug die Bezeichnung A 9/10. Sie übertraf die A4 in Umfang und Höhe um das Doppelte. Die A 9/10 bestand aus zwei unabhängigen Raketen, der A10 und der A9, die bis zum Abtrennen der ausgebrannten Startrakete A10 unter einer gemeinsamen Hülle miteinander verbunden blieben. Nach dem Ausbrennen der A10 sollte der Weiterflug von der A9 übernommen werden, die in etwa den Plänen der späteren A4b entsprach. Die projektierte Reichweite dieser sogenannten „Amerikarakete“, deren erklärtes Ziel es war, New York anzugreifen, betrug 5500 km. Über das Planungsstadium kam dieses Projekt nicht hinaus. Der Prüfstand VII der HVA-Peenemünde war allerdings schon beim Bau 1938 für die A9-/A10-Rakete dimensioniert.

Den Amerikanern waren am 29. März 1945 auf einem Militärzug am Bahnhof Bromskirchen in Hessen zehn komplette A4-Raketen des "Artillerieregimentes z.b.V. 901 (mot)" mit den mobilen Startrampen, Treibstoff und Bedienungsanleitung in die Hände gefallen. Dies wurde in den alliierten Wochenschauen ausführlich thematisiert. Der Zug sollte die Raketen vom Westerwald kommend am 22. März über die Aar-Salzböde-Bahn in neue Stellungen im Raum Schelderwald bzw. in die Nähe von Marburg bringen. Diese zehn A4 wurden drei Tage später von den Amerikanern vom Hafen Antwerpen aus in die USA verschifft, wo sie die Grundlage der neuen amerikanischen Raketentechnik bildeten.

Am 2. Mai 1945 stellte sich Wernher von Braun den Streitkräften der Vereinigten Staaten und wurde zusammen mit anderen Wissenschaftlern aus seinem Mitarbeiterstab ebenfalls in die USA gebracht (Operation Paperclip).

Die Briten ließen im Oktober 1945 mehrere A4-Raketen durch Kriegsgefangene aus ehemaligen deutschen Starteinheiten in der Nähe von Cuxhaven starten, um Vertretern der alliierten Besatzungsmächte die „Wunderwaffe V2“ beim Start zu demonstrieren (Operation Backfire). Hierbei entstand auch ein zunächst geheimer Dokumentarfilm, der heute im Museum Peenemünde zu sehen ist.

Etwa 100 erbeutete A4 und Teile davon wurden im Mittelwerk Nordhausen noch vor dem Einmarsch der Roten Armee von US-Truppen verladen und in die USA verfrachtet. Sie bildeten den Grundstock der Raumfahrtentwicklungen der USA. Eines dieser Exemplare steht im National Air and Space Museum in Washington (D.C.), ein weiteres kam anlässlich von Filmarbeiten Ende der 1950er-Jahre wieder nach Deutschland zurück und befindet sich heute im Deutschen Museum in München. Die Übersiedlung der führenden Raketentechniker ab Sommer 1945 in die USA lief im Rahmen der geheimen Operation Overcast.

Teststarts mit erbeuteten A4-Raketen in den USA erfolgten beispielsweise im März 1948 von der White Sands Missile Range in New Mexico. Die Modifizierung der A4 mit einer Corporal-Rakete als zweite Stufe nannte man Bumper. Die ersten Raketenstarts von Cape Canaveral in Florida wurden 1950 mit Bumper-Raketen durchgeführt. Auf US-Seite wurden unter anderem Fruchtfliegen im Juli 1946 mit einer A4 transportiert und als erste Organismen im All bezeichnet.

In Huntsville (Alabama) wurde mit dem Redstone Arsenal ein erstes Zentrum für die Raketenentwicklung gegründet, wo zusammen mit den deutschen Wissenschaftlern insgesamt 67 A4-Raketen gestartet wurden. Sie bildeten den Grundstock für die späteren Redstone-Raketen und für diverse Weiterentwicklungen ähnlicher Kriegswaffen, letztlich aber auch für die Saturn-V-Raketen.

Ebenso wurde von der Sowjetunion zunächst eine große Anzahl von deutschen Wissenschaftlern in der Sowjetischen Besatzungszone schon im Sommer 1945 verhaftet und dann 1946 mit ihren Familien sowie Resten der Raketentechnik und der Fertigungsanlagen in die Sowjetunion auf die Insel Gorodomlija gebracht, um dort ebenfalls die Basis für spätere Entwicklungen zu bilden. So war die sowjetische R-1-Rakete ein direkter Nachbau der A4. Sie wurde erstmals 1947 vom Testgelände Kapustin Jar gestartet. Die A4 bildete somit eine der Grundlagen der sowjetischen Raumfahrttechnologie und Raketenwaffen.

Im Rahmen der Operation Sandy gelang am 6. September 1947 mit dem Start einer V2 vom Flugdeck des amerikanischen Flugzeugträgers "Midway" erstmals der Start einer Langstreckenrakete von einem Schiff aus.

Die gegenseitige Bedrohung mit Raketen von Land wie auch von Unterseebooten aus stellte ein wesentliches Moment des Kalten Krieges dar.

Im Juni 1949 wurde mit einer V-2 erstmals ein Säugetier, der Rhesusaffe Albert II., in den Weltraum transportiert. Bei der Rückkehr öffnete sich der Fallschirm nicht und der Rhesusaffe verstarb.

Die Firma Canadian Arrow baute im Rahmen des Ansari X-Prize eine um zwei Meter verlängerte A4-Rakete nach, die Touristen ins All bringen sollte.

Der ehemalige Reichsminister für Rüstung und Kriegsproduktion Albert Speer schrieb später zur Bewertung des V2-Projektes: „Unser aufwendigstes Projekt war zugleich unser sinnlosestes. Unser Stolz und zeitweilig mein favorisiertes Rüstungsziel erwies sich als einzige Fehlinvestition.“

Ein offizieller Festakt der deutschen Luft- und Raumfahrtindustrie unter der Schirmherrschaft der damaligen Bundesregierung zum 50. Jahrestag des Erstfluges der V2 wurde erst wegen internationaler Proteste kurzfristig abgesagt. Die A4-Großrakete wurde im Ausland stark mit dem KZ Mittelbau-Dora in Bezug gebracht, in dem auch KZ-Insassen die Rakete in Serienfertigung bauten.

Im Militärhistorischen Museum der Bundeswehr in Dresden ist eine vollständig erhaltene V2-Rakete in der Dauerausstellung aufgestellt. Auch in der Luft- und Raumfahrtabteilung des Deutschen Museums in München befindet sich eine komplette A4-Rakete. Das Heeresgeschichtliche Museum in Wien besitzt in der Dauerausstellung „Republik und Diktatur“ (Saal VII) ein Triebwerk einer V2, das kurz nach dem Kriegsende aus dem Toplitzsee, wo zwischen 1943 und 1945 zahlreiche waffentechnische Versuche durchgeführt worden waren, geborgen wurde. Im Deutschen Museum Flugwerft Schleißheim und im Deutschen Technikmuseum Berlin ist ebenfalls je ein A4-Triebwerk ausgestellt.
Zusammenhänge und Hintergründe sind in der ständigen Ausstellung der KZ-Gedenkstätte Mittelbau-Dora (Nordhausen) dokumentiert; Besichtigungen der Untertageanlage sind möglich.





</doc>
<doc id="424" url="https://de.wikipedia.org/wiki?curid=424" title="Apfel">
Apfel

Apfel bezeichnet eine Frucht bzw. einen Baum:

Anderes:
Apfel ist der Familienname folgender Personen:

Siehe auch:



</doc>
<doc id="425" url="https://de.wikipedia.org/wiki?curid=425" title="Ares">
Ares

Ares (; , gespr. "Áris") ist in der griechischen Mythologie der Gott des schrecklichen Krieges, des Blutbades und Massakers. Er gehört zu den zwölf olympischen Gottheiten, den Olympioi. Obwohl die Bedeutung nicht ganz gleich ist, wurde er später von den Römern dem eigenen Kriegsgott Mars gleichgestellt. Doch stand Mars bei den Römern höher im Ansehen als Ares bei den Griechen.

Die etymologischen Wurzeln des Namens sind unklar, vermutlich bedeutet "Ares" der „Verderber“, der „Rächer“. Wahrscheinlich stammt die Gestalt des mordenden Ares aus dem bronzezeitlichen Thrakien, sie wird auch mythologisch als seine Heimat genannt. Aber bereits in mykenischer Zeit (1600–1050 v. Chr.) ist er auf dem griechischen Festland nachweisbar und verbreitet. Möglich auch, dass er in vorgriechischer Zeit ein Fruchtbarkeitsgott und seiner italischen Entsprechung ähnlicher war.

Als ehelicher Sohn des Zeus und der Hera gehörte er zu den zentralen Gestalten in der griechischen Götterwelt. Ares wird als roher, wilder, nicht zu bändigender Kriegsgott beschrieben, der Gefallen an Gewalt findet und mit den wilden Tieren zog, um sich an deren Blut zu laben. Während Athene, die Göttin der Weisheit und der Kriegslist, für den heroischen Part des Krieges steht, ist Ares eher ein finsterer Gott, ein „göttlicher Raufbold“. Im Kampf um Troja kämpft Ares auf Seiten der Trojaner, Athene auf Seiten der Griechen. Es kommt sogar zum Kampf beider Gottheiten gegeneinander, als Athene Diomedes hilft, so dass dieser Ares verwunden kann. Auf dem Olymp wird er später von Asklepios (Gott der Heilkunst) behandelt und lässt sich von seiner Schwester Hebe ausgiebig baden. Ares ist aggressiv, grausam, unbarmherzig und blutrünstig, mischt sich auch des Öfteren aktiv in die Gefechte der Sterblichen ein und stachelt deren Kampfgeist weiter auf. Streit, Plünderungen, Blutbäder, das Geräusch klirrender Waffen und das Geräusch brechender Knochen bereiten ihm großes Vergnügen. Mit den schönen Künsten der anderen Götter konnte Ares nur wenig anfangen. Mit seinen Eigenschaften war er auch bei den anderen olympischen Göttern unbeliebt, ja verhasst.

Obwohl Ares als Kriegsgott bei Göttern und Menschen verhasst war – selbst sein Vater Zeus verachtete ihn – galt er doch auch, über die Verkörperung des Männern vorbehaltenen Kriegshandwerkes, als Sinnbild männlicher Kraft und Schönheit.

Mythologisch wird dies in seiner Liebesbeziehung zur Liebesgöttin Aphrodite versinnbildlicht: Diese, obwohl mit dem rechtschaffenen, aber missgestalteten Gott der Schmiede Hephaistos verheiratet, fühlt sich von ihm angezogen und lässt sich auf eine leidenschaftliche und andauernde Affäre mit Ares ein. Der vom Sonnengott Helios davon unterrichtete eifersüchtige Ehemann bringt all seine Handwerkskunst auf und schmiedet ein unsichtbares unzerreißbares Netz, mit welchem er beide in flagranti erwischt. Die so Übertölpelten werden den herbeigerufenen Göttern vorgeführt, die sich aber nicht entrüsten, sondern auf Hephaistos' Kosten in ein unstillbares Gelächter ausbrechen.

Eros, der mit Pfeil und Bogen bewaffnete Liebesgott, wird als gemeinsamer Sohn von Ares und Aphrodite angesehen. Des Weiteren werden dieser Verbindung Anteros (Gott der Gegenliebe) und Harmonia (Göttin der Eintracht) zugesprochen sowie seine wichtigsten Begleiter, die Götter Deimos (Gott des Grauens), Phobos (Gott der Furcht) und Enyalios (Gott des Kampfes).

Wie anderen Göttern auch, werden ihm zahlreiche Liebschaften innerhalb und auch außerhalb der Welt der Unsterblichen nachgesagt. So werden unter anderem die ihn ständig begleitende Göttin des Neides und der Zwietracht Eris, die Göttin der Morgenröte Eos und etliche Sterbliche erwähnt.
Zu seinen Verbündeten zählen Hades, der Beherrscher der Unterwelt, Ker, die Göttin des gewaltsamen Todes, und Ate, die Göttin der Verblendung.

Die kriegerischen und ebenfalls nahe Thrakien angesiedelten Amazonen wurden mit Ares als Stammvater in Verbindung gebracht.

Anders als seine römische Entsprechung wurde Ares kaum kultisch verehrt, galt doch bereits jeder Krieg – den die Griechen oft auch miteinander führten – als eine Huldigung für ihn. Als seltene und berühmteste Ausnahme kultischer Orte ist hier der Areopag zu nennen, dessen Namenspate er ist, wobei dies ebenfalls mythologisch begründet wird. Andere Kultstätten sind in Ätolien, Thessalien oder Athen zu finden; im peloponnesischen Hermione stand eine Kultstatue von ihm.

Als unbeliebter und ungern gehuldigter Gott war er selten Gegenstand in der Kunst, anders seine römische Entsprechung. Ares’ Symbole sind: brennende Fackel, Hund und Geier sowie für einen Kriegsgott typisch Schwert, Helm und Schild.




</doc>
<doc id="426" url="https://de.wikipedia.org/wiki?curid=426" title="Angola">
Angola

Angola ( [], []; auf Kimbundu, Umbundu und Kikongo Ngola genannt) ist ein Staat in Südwest-Afrika. Nationalfeiertag ist der 11. November, Tag der Unabhängigkeit (1975). Angola grenzt an Namibia, Sambia, die Republik Kongo, die Demokratische Republik Kongo und den Atlantischen Ozean – die zu Angola gehörige Exklave Cabinda liegt im Norden zwischen der Demokratischen Republik Kongo und der Republik Kongo am Atlantik.

Der Name "Angola" leitet sich von dem Titel "Ngola" der Könige von Ndongo, einem östlich von Luanda gelegenen Vasallenstaat des historischen Kongoreiches, ab. Die Region um Luanda erhielt diesen Namen im 16. Jahrhundert durch die ersten portugiesischen Seefahrer, die an der dortigen Küste anlandeten und ein Padrão, ein steinernes Kreuz, errichteten, als Zeichen der Inbesitznahme für den portugiesischen König. Die Bezeichnung wurde Ende des 17. Jahrhunderts auf die Region um Benguela ausgedehnt, im 19. Jahrhundert dann auf das (damals noch nicht umgrenzte) Territorium, dessen koloniale Besetzung sich Portugal vornahm.

Die Republik Angola liegt zwischen 4° 22′ und 18° 02′ südlicher Breite sowie 11° 41′ und 24° 05′ östlicher Länge. Das Land gliedert sich grob in eine schmale Niederung entlang der Atlantikküste, die in Richtung Osten, zum Landesinneren hin, zum Hochland von Bié ansteigt: Es macht den größten Teil Angolas aus, ist im Süden flach und in der Landesmitte bergig. Der höchste Berg ist der in diesem Hochland liegende Môco mit 2619 m über dem Meeresspiegel.
Der Osten Angolas wird vom Sambesi durchflossen.

Angola ist in drei Klimazonen eingeteilt:

An der Küste und im Norden des Landes ist es tropisch, das heißt, es gibt das ganze Jahr hohe Tagestemperaturen zwischen 25 und 30 °C, nachts ist es nur unwesentlich kühler. Von November bis April ist Regenzeit. Das Klima wird stark durch den kühlen Benguelastrom (17–26 °C) beeinflusst, so dass Nebel häufig ist. Die durchschnittliche Niederschlagsmenge liegt bei 500 mm, im Süden kaum bei 100 mm jährlich.

Das Hochland im Zentrum und Süden des Landes ist gemäßigt-tropisch, es gibt vor allem im Winter deutliche Temperaturunterschiede zwischen Tag und Nacht. So liegen etwa in Huambo die Temperaturen im Juli zwischen 25 °C tagsüber und 7–8 °C nachts, dazu kommt noch eine enorme Trockenheit. Ähnlich wie an der Küste ist die Regenzeit von Oktober bis April. Es fallen im Schnitt rund 1000 mm Regen pro Jahr.

Im Südosten des Landes ist es überwiegend heiß und trocken mit kühlen Nächten im Winter und Hitze und gelegentlichen Niederschlägen im Sommer. Die Jahresniederschläge schwanken um 250 mm.

Die Vegetation reicht klimabedingt von tropischem Regenwald im Norden und in Cabinda über Baumsavannen im Zentrum bis zur trockenen Grassavanne, die durchsetzt ist mit Euphorbien (Wolfsmilchgewächsen), Akazien und Affenbrotbäumen. Von Namibia ausgehend, zieht sich entlang der Südwestküste ein Wüstenstreifen. Die Fauna Angolas ist reich an Wildtieren, es finden sich Elefanten, Flusspferde, Geparden, Gnus, Krokodile, Strauße, Nashörner und Zebras. Die Ausweitung der Landwirtschaft, aber auch die Zerstörungen durch die Bürgerkriege und der Handel mit Elfenbein gefährden das Überleben vieler Arten.

Zur Bevölkerung Angolas gibt es seit der Volkszählung „Recenseamento Geral da População e Habitação“ im Jahr 2014 erstmals gesicherte Daten. Demnach betrug die Bevölkerung 24,4 Millionen. 52 % sind weiblichen Geschlechtes.
Die Vereinten Nationen dagegen schätzen für das Jahr 2014 die Bevölkerung auf 26,9 Millionen. 
Im Jahr 2017 wird die Bevölkerung von den Vereinten Nationen auf 29,8 Millionen geschätzt. 
Andere Quellen schätzen die Bevölkerung für das Jahr 2017 auf 29,3 Millionen oder 24,3 Millionen. 

Die Bevölkerung Angolas ist eine der am schnellsten wachsenden der Welt. Im Jahr 2017 beträgt das Bevölkerungswachstum 3,5 % und die Fertilität pro Frau beträgt 6,2 Kinder. Das Durchschnittsalter der Bevölkerung liegt im selben Jahr bei 18,2 Jahren. 

Ein akutes demografisches Problem, mit unabsehbaren wirtschaftlichen, sozialen und politischen Folgen, hat sich in Angola aus dem Kriegszustand ergeben, der sich über vier Jahrzehnte hingezogen hat. Um 2000 war ein erheblicher Teil der Landbevölkerung in die Städte, in unwegsame Gebiete (Berge, Wald, Sumpfland) oder ins benachbarte Ausland (Namibia, Botswana, Sambia, Kongo-Kinshasa, Kongo-Brazzaville) geflohen. Entgegen allen Erwartungen ist es nach dem Friedensschluss nicht zu einem massiven Rückfluss gekommen. Zwar ist ein Teil der Bevölkerung in ihre Ursprungsorte zurückgekehrt, aber – wie die Erhebungen der letzten Jahre zeigen – per Saldo hat das Binnenland sogar weiter an Bevölkerung verloren. Dies hängt nicht zuletzt damit zusammen, dass sich die Wirtschaft – mit Ausnahme der Landwirtschaft und der Förderung von Diamanten – ganz überwiegend auf den Küstenstreifen konzentriert. Die Volkszählung von 2014 hat allerdings aufgedeckt, dass der Rückgang der ländlichen Bevölkerung trotz generell schlechter Lebensbedingungen weniger drastisch war, als befürchtet: sie macht knapp über 60 % der Gesamtbevölkerung aus.

Die meisten Angolaner sind Bantu und gehören drei Ethnien an: mehr als ein Drittel sind Ovimbundu, ansässig auf dem Zentralhochland, dem angrenzenden Küstenstreifen und nunmehr ebenfalls stark präsent in allen größeren Städten auch außerhalb dieses Gebietes; ein knappes Viertel sind Ambundu (Sprache: Kimbundu), die in einem breiten Landstrich von Luanda bis Malanje überwiegen; schließlich gehören 10 bis 15 % den Bakongo an, einem Volk, das im Westen von Kongo-Brazzaville und der Demokratischen Republik Kongo sowie im Nordwesten Angolas angesiedelt ist und nunmehr auch in Luanda eine starke Minderheit darstellt.

Zahlenmäßig kleinere Volksgruppen sind die Ganguela, eigentlich ein Konglomerat aus kleineren Gruppen Ostangolas, dann Nyaneka-Nkhumbi im Südwesten, die zumeist Hirtenbauern sind, die Ovambo (Ambo) und Herero Südangolas (mit Verwandten in Namibia) sowie die Tshokwe (einschließlich der Lunda) aus dem Nordosten Angolas (und Süden der DR Kongo sowie Nordwesten Sambias), die im Verlaufe des letzten Jahrhunderts in kleinen Gruppen südwärts gewandert sind. Einige kleine Gruppen im äußersten Südwesten werden als Xindonga bezeichnet. Schließlich gibt es noch residuale Gruppen der Khoisan (San), die verstreut in Südangola leben und nicht zu den Bantu gehören.

Etwa 2 % der Bevölkerung sind "mestiços", also Mischlinge von Afrikanern und Europäern. Die Portugiesen waren mit 320.000 bis 350.000 Menschen am Ende der Kolonialzeit die größte europäischstämmige Volksgruppe im Land. Über die Hälfte von ihnen war im Lande geboren, nicht selten in der zweiten oder dritten Generation, und fühlte sich mehr Angola zugehörig als Portugal. Die anderen waren in der spätkolonialen Phase zugewandert oder als Angestellte/Beamte staatlicher Einrichtungen (einschließlich des Militärs) nach dort versetzt worden. Die meisten Portugiesen flohen kurz vor oder nach der Unabhängigkeitserklärung Angolas von Ende 1975 nach Portugal, Brasilien oder Südafrika, doch ist ihre Zahl inzwischen wieder auf mehr als 100.000 angewachsen, zu denen eine möglicherweise ähnlich große Zahl anderer Europäer sowie Latein- und Nordamerikaner kommt.
Zu den Europäern kommt inzwischen eine große, auf etwa 300.000 Menschen geschätzte Gruppe von Chinesen, die im Zuge einer Immigrationswelle nach Afrika kamen und kommen.

Bis 1974/75 lebten auch etwa 130 deutsche Familien (Angola-Deutsche) als Farmer oder Unternehmer im Land, vor allem in den Regionen um Huambo und Benguela; in der Stadt Benguela gab es seinerzeit sogar eine deutsche Schule. Fast alle haben seither aber das Land verlassen.

Die ethnischen Unterschiede haben, im Gegensatz zu anderen (afrikanischen und nichtafrikanischen) Ländern, in Angola nur in Maßen für gesellschaftlichen Zündstoff gesorgt. Als sich Bakongo, die in den 1970er Jahren in den Kongo-Kinshasa geflohen waren, bei ihrer Rückkehr in großer Zahl in Luanda niederließen, hat das zwar zu gegenseitigem „Fremdeln“ zwischen ihnen und den ansässigen Ambundu geführt, nicht aber zu massiven oder gar gewalttätigen Konflikten. Als sich im Bürgerkrieg Ambundu und Ovimbundu gegenüberstanden, bekam der Konflikt auf seinem Höhepunkt auch ethnische Untertöne; seit Frieden herrscht, sind diese deutlich abgeklungen. Bei Konflikten aller Art können solche Abgrenzungen aber wieder ins Spiel kommen. Außerdem ist das Problem der Rassenbeziehungen zwischen Schwarzen, Mischlingen und Weißen noch in keiner Weise ausgestanden, zumal es von der Politik her manipuliert wird und seinerseits die Politik bedingt.

Fast alle der in Angola gesprochenen Sprachen gehören zur Bantu-Sprachfamilie. Portugiesisch ist Amtssprache in Angola und wird heute von etwa 30 % der Angolaner – vor allem in der Hauptstadt Luanda – auch als Muttersprache gesprochen. Von den übrigen Angolanern sprechen sehr viele, in den Städten sogar eine ganz überwiegende Mehrheit, ebenfalls Portugiesisch, häufig im täglichen Umgang mit afrikanischen Wörtern durchsetzt. Vermutlich ist Angola das afrikanische Land, das sich alles in allem die Sprache der ehemaligen Kolonialmacht am stärksten zu eigen gemacht hat.

Unter den afrikanischen Sprachen Angolas am weitesten verbreitet sind das Umbundu, von der ethnischen Gruppe der Ovimbundu gesprochen, das Kimbundu der Ambundu und das Kikongo der Bakongo sowie dessen Kreolvariante Kituba (siehe oben). Andere Sprachen sind Ngangela, Oshivambo (Kwanyama, Ndonga), Mwila, Nkhumbi, Otjiherero und Chokwe sowie das im 20. Jahrhundert von Rückwanderern aus dem Zaire eingeführte Lingala. In Angola werden insgesamt (je nach Einteilungskriterien) rund 40 verschiedene Sprachen/Dialekte gesprochen.

In Angola gibt es knapp 1000 Religionsgemeinschaften. Zur Anzahl der Mitglieder der Religionsgemeinschaften sind nur grobe Schätzungen möglich. Etwas mehr als die Hälfte der Bevölkerung dürften Anhänger der römisch-katholischen Kirche sein. Rund ein Viertel gehören den während der Kolonialzeit gegründeten protestantischen Kirchen und Freikirchen an.

Methodisten sind besonders im Gebiet von Luanda bis Malanje vertreten, Baptisten im Nordwesten und Luanda. In Zentralangola und den angrenzenden Küstenstädten ist vor allem die "Igreja Evangélica Congregacional de Angola" "(Evangelisch-kongregationale Kirche Angolas)" vertreten. Aus der Kolonialzeit stammen auch verschiedene kleinere Gemeinschaften, so Lutheraner (z. B. in Südangola) und Reformierte (vor allem in Luanda). Dazu kommen Adventisten, neuapostolische Christen sowie (nicht zuletzt durch Einflüsse aus Brasilien) seit der Unabhängigkeit eine Vielfalt pfingstlich-charismatischer Freikirchen und die Zeugen Jehovas. Die neuen Gemeinschaften sind besonders in den größeren Städten entstanden und haben teilweise erheblichen Zulauf; dies gilt vor allem für die „Igreja Unida do Reino de Deus“ (IURD, Vereinigte Kirche des Reichs Gottes), die in Brasilien entstand und sich von dort aus in die anderen portugiesischsprachigen Länder ausbreitete.

Aufgrund von Einflüssen aus Südafrika und Namibia hat sich in den 2000er Jahren ein kleiner Ableger der anglikanischen Kirche des südlichen Afrika gebildet. Schließlich gibt es zwei christlich-synkretistische Gemeinschaften, die in der DR Kongo verwurzelten Kimbangisten und die im kolonialen Angola entstandenen Tokoisten.

Nur noch ein verschwindend geringer Teil der Bevölkerung hängt ausschließlich traditionellen Religionen an, aber unter den Christen finden sich nicht selten Bruchstücke von Vorstellungen, die aus diesen Religionen stammen. Der Anteil der Muslime (fast alle sunnitisch) beträgt schätzungsweise ein bis zwei Prozent. Er setzt sich aus Einwanderern aus verschiedenen, meist afrikanischen Ländern zusammen, die aufgrund ihrer Verschiedenartigkeit keine Gemeinschaft bilden. Saudi-Arabien bemüht sich in letzter Zeit um eine Ausbreitung des Islams in Angola. So hat es 2010 angekündigt, dass es in Luanda die Errichtung einer islamischen Universität finanzieren werde.
Im November 2013 wurde dem Islam und zahlreichen anderen Organisationen die Anerkennung als Religionsgemeinschaft verweigert. Zudem wurden Gebäude, die ohne Baugenehmigung errichtet wurden, zum Abriss vorgesehen. Berichten zufolge stünde u. a. die Schließung von mehr als 60 Moscheen im Land bevor.

Die katholische Kirche, die traditionellen protestantischen Kirchen und die eine oder andere Freikirche unterhalten soziale Einrichtungen, die dazu bestimmt sind, Mängel in der gesellschaftlichen oder staatlichen Versorgung auszugleichen. Die katholische Kirche und die traditionellen protestantischen Kirchen äußern sich gelegentlich zu politischen Fragen und finden dabei unterschiedliches Gehör.

Die Ernährungs- und Gesundheitssituation der angolanischen Bevölkerung ist – aus europäischer Perspektive – größtenteils katastrophal. Nur rund 30 % der Bevölkerung haben Zugang zu grundlegender medizinischer Versorgung und nur 40 % haben Zugang zu ausreichend reinem Trinkwasser. Jährlich sterben tausende Menschen an Krankheiten wie Durchfallerkrankungen oder Atemwegsentzündungen. Daneben sind Malaria, Meningitis, Tuberkulose und Erkrankungen durch Wurmbefall verbreitet. Die Infektionsrate mit HIV liegt nach Schätzungen von UNAIDS bei 2 % und damit für die Region sehr niedrig. Als Grund hierfür wird die Abschottung des Landes während des Bürgerkrieges genannt.

Etwa ein Drittel der Bevölkerung ist teilweise oder vollständig von ausländischen Nahrungsmittelhilfen abhängig. 2015 waren 14,0 % der Bevölkerung unterernährt. Im Jahr 2000 waren es noch 50,0 % der Bevölkerung. 
Die Sterblichkeitsrate von Kindern unter fünf Jahren ist die zweithöchste der Welt, statistisch stirbt alle drei Minuten ein Kind in Angola. Aufgrund der mangelnden medizinischen Versorgung ist auch die Zahl der Frauen, die während der Geburt sterben, extrem hoch. Die durchschnittliche Lebenserwartung bei der Geburt wird mit 60,2 Jahren angegeben (Frauen: 63,0 Jahre, Männer: 57,4 Jahre). Lepra bleibt in Angola eine große Sorge der Gesundheitsbehörden im Land. Im Jahr 2010 wurden insgesamt 1048 Fälle dieser chronisch infektiösen Krankheit festgestellt.

Quelle: UN

Während der Kolonialzeit wurde das Bildungswesen bis auf das letzte Jahrzehnt vernachlässigt und war stets ein Instrument der Kolonialpolitik. Nach der Unabhängigkeit setzte ein systematischer Neubeginn an, bei dem die Zusammenarbeit mit Kuba eine wichtige Rolle spielte. Der Bürgerkrieg behinderte diese Anstrengungen sehr; er führte zur Zerstörung vieler Schulen und zum Tode oder zur Flucht zahlreicher Lehrer, vor allem auf dem Lande. Der Aufbau eines neuen Bildungswesens wurde insgesamt jedoch fortgesetzt, besonders in den Städten, in denen sich nach und nach die Hälfte der Bevölkerung konzentrierte. Seit dem Frieden 2002 wurden und werden große Anstrengungen unternommen, um die Situation zu verbessern und die enormen Defizite auszuräumen. In der gleichen Zeit begann in Angola eine Schulreform mit der Absicht, die Inhalte der Schule für die Kinder relevanter zu machen und bessere Ergebnisse zu erzielen.

In Angola gehen weniger als zwei Drittel der Kinder im schulpflichtigen Alter zur Schule. In den Grundschulen wiederholen 54 % der Kinder eine oder mehrere Klassen. Wenn die Kinder die fünfte Klasse erreichen, gehen nur noch 6 % der Kinder ihrer Altersgruppe in die Schule. Dies hat auch mit dem Umstand zu tun, dass für die Versetzung in höhere Klassen ein gültiger Personalausweis vorzulegen ist, den viele nicht haben. Diese hohe Schulabbrecherquote entspricht dem Mangel an Schulen mit fünfter und sechster Klasse. Die Alphabetisierungsrate der erwachsenen Bevölkerung betrug 2015 71,1 % (Frauen: 60,2 %, Männer: 82,0 %)

In Zusammenarbeit mit dem angolanischen Bildungsministerium betreibt die Hilfsorganisation "Ajuda de desenvolvimento de Povo para Povo em Angola" sieben Lehrerausbildungsstätten in Huambo, Caxito, Cabinda, Benguela, Luanda, Zaire und Bié, die so genannten "Escolas dos Professores do Futuro", an denen bis Ende 2006 mehr als 1000 Lehrer für den Einsatz in den ländlichen Gebieten ausgebildet wurden. Bis 2015 sollen acht weitere dieser Lehrerausbildungsstätten eingerichtet und 8000 Lehrer ausgebildet werden.

Das Hochschulwesen bestand bis in die späten 1990er Jahre aus der staatlichen Universidade Agostinho Neto, deren etwa 40 Fakultäten über das ganze Land verteilt waren und sich insgesamt in einem schlechten Zustand befanden. Daneben gab es lediglich noch die Universidade Católica de Angola (UCAN) in Luanda. Inzwischen gibt es (vor allem in Luanda) eine wachsende Anzahl privater Universitäten. Zu nennen sind einmal die Universidade Lusíada de Angola, die Universidade Lusófona de Angola und die Universidade Jean Piaget de Angola, sämtlich mit engen Verbindungen zu den gleichnamigen Universitäten in Portugal. Mit Unterstützung einer Lissaboner Universität ist auch die Angola Business School entstanden. Rein angolanische Initiativen sind die Universidade Privada de Angola, seit kurzem auch die Universidade Metodista de Angola, die Universidade Metropolitana de Angola, die Universidade Independente de Angola, die Universidade Técnica de Angola, die Universidade Gregório Semedo, die Universidade Óscar Ribas, die Universidade de Belas, und das Instituto Superior de Ciências Sociais e Relações Internacionais.

Alle diese Universitäten sind in Luanda angesiedelt, obwohl einige auch „pólos“ genannte Außenstellen in anderen Städten haben, so die Universidade Privada de Angola in Lubango, die Universidade Lusófona de Angola in Huambo und die Universidade Jean Piaget in Benguela. Im Sinne einer Dezentralisierung des Hochschulwesens war es jedoch entscheidend, dass 2008/2009 aus der Universidade Agostinho Neto sechs Regionaluniversitäten mit je eigenem Namen ausgegliedert wurden, die die bestehenden Fakultäten übernahmen und meist weitere gründeten, und die innerhalb ihres jeweiligen Zuständigkeitsgebiets in anderen Städten „pólos“ einrichteten. In Benguela entstand so die Universidade Katyavala Bwila, in Cabinda die Universidade 11 de Novembro, in Huambo die Universidade José Eduardo dos Santos mit „pólo“ in Bié, in Lubango die Universidade Mandume ya Ndemufayo mit „pólo“ in Ondjiva, in Malanje mit Saurimo und Luena die Universidade Lueij A’Nkonde und in Uíge die Universidade Kimpa Vita.

In den meisten Fällen waren die Namensgeber afrikanische Führungsfiguren aus vorkolonialer Zeit oder aus der Zeit des Primärwiderstands gegen die koloniale Eroberung. Sämtliche Universitäten haben mit Aufbauschwierigkeiten zu kämpfen. Der Zuständigkeitsbereich der Universidade Agostinho Neto wurde auf die Provinzen Luanda und Bengo beschränkt. Die qualitativen Unzulänglichkeiten des Hochschulwesens sind durch diese Entwicklung jedoch bislang nur teilweise überwunden worden. In Luanda haben aufgrund der Vielfalt der Universitäten einige von ihnen mit einer abnehmenden Nachfrage zu kämpfen.

Die ersten Bewohner des heutigen Angola waren Khoisan, die später weitgehend von Bantu-Volksgruppen verdrängt wurden. 1483 begann die Errichtung von portugiesischen Handelsposten an der Küste, vor allem in Luanda und Hinterland, ein Jahrhundert später auch in Benguela. Erst Anfang des 19. Jahrhunderts begann die systematische Eroberung und Besetzung des heutigen Territoriums, die erst Mitte der 1920er Jahre abgeschlossen war.

Von der Mitte der 1920er Jahre bis Anfang der 1960er Jahre war Angola einem „klassischen“ Kolonialsystem unterworfen. Die Kolonialmacht Portugal wurde von 1926 bis zur Nelkenrevolution 1974 von einer Militärdiktatur regiert (bis 1932 Carmona, bis 1968 Salazar, bis 1974 Caetano).

Die wichtigste ökonomische Grundlage Angolas war bis zum Ende der Kolonialzeit die Landwirtschaft und Viehzucht, die sowohl in Großbetrieben von europäischen Siedlern stattfand als auch in den Familienbetrieben der Afrikaner. Die Förderung von Diamanten war für den Kolonialstaat von zentraler Bedeutung. Eine weitere wichtige Komponente war der Handel. Zu einer bescheidenen Industrialisierung und Entwicklung des Dienstleistungssektors kam es erst in der spätkolonialen Phase, also in den 1960er und 1970er Jahren. In den 1950er Jahren wurden auf dem Festland Erdölvorkommen geortet, in den 1960er Jahren auch im Meer vor Cabinda, doch kam es erst ganz am Ende der Kolonialzeit zu einer Förderung größeren Ausmaßes.

In den 1950er Jahren begann sich ein nationalistischer Widerstand zu formieren, der 1961 in einen bewaffneten Befreiungskampf mündete (1960 – im „Afrika-Jahr“ – hatten 18 Kolonien in Afrika (14 französische, zwei britische, je eine belgische und italienische) die Unabhängigkeit von ihren Kolonialmächten erlangt; siehe auch Dekolonisation Afrikas).

Ab 1962 führte Portugal deswegen einschneidende Reformen durch und leitete eine spätkoloniale Phase ein, die in Angola eine qualitativ neue Situation schuf, die jedoch den Unabhängigkeitskrieg nicht zum Einhalten brachte. Der Unabhängigkeitskrieg kam abrupt zu einem Ende, als am 25. April 1974 ein Militärputsch in Portugal die Nelkenrevolution auslöste und die dortige Diktatur zum Einsturz brachte und das neue demokratische Regime sofort mit der Entkolonisierung begann.

Der Umsturz in Portugal löste in Angola bewaffnete Auseinandersetzungen zwischen den Befreiungsbewegungen FNLA, MPLA und UNITA aus, deren ethnische Verwurzelung im Lande durchaus unterschiedlich war. In diese Auseinandersetzungen griffen die USA, Zaire (seit 1997 „Demokratische Republik Kongo“) und Südafrika (noch unter dem Apartheid-Regime) auf Seiten von FNLA und UNITA ein, die Sowjetunion und Kuba auf Seiten der MPLA. Letztere behielt die Oberhand und rief 1975 in Luanda die Unabhängigkeit aus, gleichzeitig FNLA und UNITA in Huambo.

Die „Gegenregierung“ von FNLA und UNITA löste sich zwar rasch auf, aber sofort nach der Unabhängigkeitserklärung setzte ein Bürgerkrieg zwischen den drei Bewegungen ein, aus dem die FNLA nach kurzer Zeit ausschied, während ihn die UNITA bis zum Tode ihres Anführers Jonas Savimbi im Jahre 2002 weiterführte. Gleichzeitig errichtete die MPLA ein politisch-ökonomisches Regime, das dem der damals sozialistischen Länder nachempfunden war. Bemerkenswert war die zivile Entwicklungshilfe Kubas während dieser Zeit.

Dieses Regime wurde 1990/91 während einer Unterbrechung des Bürgerkriegs zugunsten eines Mehrparteiensystems aufgegeben. 1992 fanden Wahlen statt, an denen auch die UNITA teilnahm. Die MPLA erreichte dabei im Parlament die absolute Mehrheit, während ihr Präsidentschaftskandidat, José Eduardo dos Santos, nur die relative Mehrheit erhielt und nach der Verfassung ein zweiter Wahlgang (gegen Jonas Savimbi) notwendig gewesen wäre.

Es ergab sich daraus eine bizarre Situation, die bis 2002 anhielt. Einerseits nahmen Vertreter der UNITA und der FNLA am Parlament und sogar der Regierung teil, andererseits nahm der militärische Arm der UNITA den bewaffneten Kampf sofort nach der Wahl wieder auf. Das politische System entwickelte sich zu einer autoritären Präsidialdemokratie, während im Lande Zerstörungen z. T. erheblichen Ausmaßes vor sich gingen.

Nachdem Jonas Savimbi 2002 im Osten des Landes von der Armee entdeckt und erschossen worden war, stellte die UNITA den Kampf sofort ein. Sie löste ihren militärischen Arm auf, der zu einem Teil in die angolanische Armee übernommen wurde. Unter einem neuen Vorsitzenden, Isaias Samakuva, hat sie die Rolle einer normalen Oppositionspartei übernommen. 2008 kam es zu erneuten Parlamentswahlen, bei denen die MPLA knapp über 80 % der Stimmen erhielt und UNITA sowie FNLA kaum noch zahlenmäßiges Gewicht erhielten.

Unterdessen ist der Wiederaufbau der zerstörten Städte, Dörfer und Infrastrukturen im Gange; es kommt vielerorts zu einem Aufbau, der deutlich über den Zustand am Ende der Kolonialzeit hinausgeht. Dank der Erdölförderung und des hohen Ölpreises gibt es dafür genug Devisen.

Eine im Januar 2010 verabschiedete neue Verfassung stärkt inzwischen die Stellung der MPLA und besonders des Staatspräsidenten in einem Maße, das es rechtfertigt, von einem stark autoritären Präsidialsystem zu sprechen.

Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „nicht frei“ bewertet. In der Kategorie „politische Rechte“ erhält Angola die Note 6, bei der Wahrung der Bürgerrechte erhält das Land ebenfalls die Note 6 (die Note 1 ist die beste und die 7 die schlechteste). Angola zählt nicht zu den Wahldemokratien.

Zurzeit ist die politische Macht auf die Präsidentschaft konzentriert. Die Exekutive besteht aus dem Präsidenten, José Eduardo dos Santos, der zugleich Oberkommandierender der Streitkräfte und Regierungschef ist, und dem Ministerrat. Der Ministerrat, bestehend aus allen Regierungsministern und Vizeministern, trifft sich regelmäßig, um über politische Themen zu diskutieren. Die Gouverneure der 18 Provinzen werden vom Präsidenten ernannt und handeln nach seinen Vorstellungen. Das Verfassungsrecht von 1992 begründet die wesentlichen Merkmale der Regierungsstruktur und nennt die Rechte und Pflichten der Bürger. Das Rechtssystem, das auf dem portugiesischen Recht und dem Gewohnheitsrecht basiert, ist schwach und bruchstückhaft. Gerichte sind nur in zwölf von mehr als 140 Stadtverwaltungen tätig. Das oberste Gericht dient als Rechtsmittelinstanz. Ein Verfassungsgericht – mit der Fähigkeit einer unparteiischen Bewertung – wurde bis 2010 nicht ernannt, obwohl es das Gesetz vorsieht.

Die 2010 vom Parlament angenommene Verfassung hat die autoritären Züge des politischen Systems nochmals verschärft. Hervorzuheben ist, dass die Präsidentschaftswahl abgeschafft wurde und in Zukunft der Vorsitzende und der stellvertretende Vorsitzende derjenigen Partei, die bei den Parlamentswahlen die meisten Stimmen erhält, automatisch Staatspräsident bzw. Vizepräsident sind. Der Staatspräsident kontrolliert über verschiedene Mechanismen sämtliche Staatsorgane, einschließlich des nunmehr geschaffenen Verfassungsgerichts; von einer Gewaltenteilung kann man infolgedessen nicht sprechen. Es handelt sich also nicht mehr um ein Präsidialsystem, wie es das etwa in den USA oder Frankreich gibt, sondern um ein System, das verfassungsrechtlich in dieselbe Kategorie fällt, wie die cäsaristische Monarchie Napoleon Bonapartes, das korporative System António de Oliveira Salazars nach der portugiesischen Verfassung von 1933, die brasilianische Militärregierung nach der Verfassung von 1967/1969 sowie verschiedene autoritäre Regime im gegenwärtigen Afrika.

Der 27 Jahre andauernde Bürgerkrieg in Angola hat die politischen und gesellschaftlichen Einrichtungen des Landes stark beschädigt. Die UN vermutet, dass es in Angola 1,8 Millionen Flüchtlinge gab. Mehrere Millionen Menschen waren direkt von Kriegshandlungen betroffen. Täglich spiegelten die Lebensbedingungen im ganzen Land, besonders in Luanda (durch immense Landflucht ist die Hauptstadt auf über fünf Millionen Einwohner angewachsen), den Zusammenbruch der Verwaltungsinfrastruktur und der vielen gesellschaftlichen Einrichtungen wider. Krankenhäuser hatten oft weder Medikamente noch eine Grundausstattung, Schulen hatten keine Bücher, und Angestellte im öffentlichen Dienst besaßen keine Ausstattung, um ihrer täglichen Arbeit nachzugehen. Seit dem Ende des Bürgerkriegs im Jahre 2002 sind massive Bemühungen um Wiederaufbau unternommen worden, doch finden sich dessen Spuren bis heute überall im Lande. Die vielfältigen Probleme und Möglichkeiten des Wiederaufbaus werden in großer Ausführlichkeit beschrieben vom Angolaportugiesen José Manuel Zenha Rela.

Die zwei einflussreichsten Gewerkschaften sind:


Am 5. und 6. September 2008 wählten die Angolaner erstmals seit Ende des Bürgerkrieges eine neue Nationalversammlung. Nach Auffassung von Wahlbeobachtern der SADC und der Afrikanischen Union (AU) verlief die Wahl „allgemein frei und fair“. Beobachter der EU wiesen zwar auf die sehr gute technische und logistische Vorbereitung der Wahlen, die hohe Wahlbeteiligung sowie den friedlichen Prozess der Stimmabgabe hin. Kritisiert wurde allerdings die chaotische Abhaltung der Wahlen vor allem in der Hauptstadt Luanda. Nach Auffassung internationaler Beobachter bestanden in der Zeit vor den Wahlen keine freien und für alle Parteien gleichen Voraussetzungen für faire Wahlen. Es wird von fast allen Beobachtern übereinstimmend hervorgehoben, dass die staatlichen Medienanstalten massiv zugunsten der MPLA missbraucht wurden, freier Zugang zu den elektronischen Medien für die Oppositionsparteien außerhalb Luandas nicht gegeben war. Die angolanische Zivilgesellschaft spricht von staatlich finanzierten Wahlgeschenken der MPLA und Einschüchterungen durch deren Sympathisanten.
Die MPLA gewann die Wahl mit knapp 82 % der abgegebenen Stimmen, während die UNITA etwas mehr als 10 % der Stimmen auf sich vereinigen konnte. Die größte Oppositionspartei legte zunächst Beschwerde gegen die Wahl ein, gestand nach deren Ablehnung jedoch ihre Niederlage ein.

Folgende Parteien verfügten nach dieser Wahl über Sitze im Parlament:


Das Regime bestätigte 2011/2012 emphatisch seine Absicht, 2012 erneut Parlamentswahlen abzuhalten und so zum ersten Mal die verfassungsmäßige Bestimmung zu achten, nach der Wahlen alle vier Jahre stattfinden müssen. Außer den gegenwärtig im Parlament vertretenen Parteien waren weitere 67 Parteien berechtigt, bei diesen Wahlen anzutreten. José Eduardo dos Santos tat wiederholt seine Absicht kund, bei diesen Wahlen nicht erneut zu kandidieren, sodass sich die Frage stellte, wer sein Nachfolger als Staatspräsident sein würde.

Die Wahlen fanden dann am 31. August 2012 statt. Im Gegensatz zu seinen vorherigen Erklärungen war José Eduardo dos Santos erneut Spitzenkandidat des MPLA. Nach den vorläufigen Ergebnissen erhielt das MPLA etwas mehr als 70 % der Stimmen – also weniger als 2008, aber immer noch eine sehr komfortable Mehrheit, die nicht zuletzt José Eduardo dos Santos das Verbleiben im Amt garantierte. Die UNITA erhielt um die 18 % und die Neugründung CASA (Convergência Ampla de Salvação de Angola) rund 6 %. Weitere Parteien werden nicht ins Parlament einziehen, da keine auch nur 2 % der Stimmen erreichte. Bemerkenswert sind die starken Unterschiede zwischen den Regionen, besonders in Hinsicht auf die Resultate der Opposition: so erhielt diese rund 40 % in den Provinzen Luanda und Cabinda, in denen das Niveau der Politisierung besonders hoch ist.

Am 23. August 2017 fanden erneut Wahlen statt, bei der die MPLA rund 65 % der Stimmen erhielt und damit weiter den Präsidenten stellt. Nach dem 2016 angekündigten Rücktritt dos Santos’ wird dies João Lourenço sein. Die UNITA kam auf rund 27 %.

2008 kam es laut Amnesty International wiederholt zu willkürlichen Festnahmen von Personen, die ihr Recht auf freie Meinungsäußerung bzw. auf Versammlungs- und Vereinigungsfreiheit wahrgenommen hatten. Ein staatliches soziales Sicherungssystem gibt es nicht. Alleinstehende Frauen stehen vor allem in den ländlichen Gebieten vor zusätzlichen Schwierigkeiten. In einigen Gemeinden ist es Frauen traditionell untersagt, eigenes Land zu besitzen und dieses zu kultivieren. Homosexualität gilt in Angola als „Verstoß gegen die öffentliche Moral“, ist illegal und kann mit bis zu drei Jahren Haft oder Arbeitslager bestraft werden.

In einem offenen Brief forderten mehrere Menschenrechtsgruppen und Persönlichkeiten des Landes die US-amerikanische Außenministerin Hillary Clinton auf, den Zustand der Demokratie in Angola auf ihrer Afrikareise 2009 anzusprechen. „Weltweit ist die Vorstellung in Umlauf, dass Angola große demokratische Fortschritte macht. In Wirklichkeit werden die Menschen mit anderen Ideen (als jene der Regierung) verfolgt und festgenommen. Das Kundgebungsrecht existiert nicht“, klagte David Mendes von der Organisation „Associação Mãos Livres“ (Vereinigung der Freien Hände). China bekomme immer mehr Einfluss in Angola. „Und jeder weiß, dass China die Menschenrechte nicht respektiert“, sagte Mendes. Amnesty International rief bereits 2007 in einem offenen Brief an die EU auf, die schwierige Situation der Menschenrechte in Angola anzusprechen und auf ihre Agenda zu setzen.

Offenbar unter dem Einfluss der Volksaufstände in arabischen Ländern, gab es Versuche am 7. März 2011 und dann wieder zu einem späteren Zeitpunkt, in Luanda eine Großdemonstration gegen das politische Regime in Angola zu organisieren. Es handelte sich um Versuche, Protest unabhängig von den Oppositionsparteien zu artikulieren. Die MPLA hat am 5. März in Luanda eine „präventive Gegendemonstration“ mit vorgeblich 1 Millionen Anhängern veranstaltet. Während der folgenden Monate fanden Proteste im Internet und bei Rapveranstaltungen statt. Am 3. September 2011 wurde dann erneut die Erlaubnis zu einer regimekritischen, vor allem gegen die Person des Staatspräsidenten gerichteten Demonstration erteilt, die dann jedoch unter Einsatz von Schlagstöcken und Schusswaffen gewaltsam aufgelöst wurde, als sie den ihr zugestandenen Bereich zu überschreiten begann. Etwa 50 Personen wurden verhaftet und sahen einer summarischen Verurteilung entgegen.

Angola ist seit 1976 Mitglied der Vereinten Nationen, seit 1996 Mitglied der WTO und seit 2007 bei der OPEC sowie Gründungsmitglied der Südafrikanischen Entwicklungsgemeinschaft SADC, als auch bei der AU (Afrikanische Union) und der CPLP, der Gemeinschaft der Staaten portugiesischer Sprache.

Am 15. Oktober 2013 wurde die strategische Partnerschaft mit Portugal von Angola aufgekündigt. Präsident dos Santos erklärte die Beziehungen zwischen den beiden Ländern wären nicht gut. Die Ursache ist der Umstand, dass die portugiesische Justiz einige politisch gewichtige Angolaner, die zum engeren Umkreis des Staatspräsidenten gehören, aufgrund von in Portugal begangenen Delikten (vor allem massiver Geldwäsche) unter Anklage gestellt hat.

"Siehe auch: Liste der angolanischen Botschafter beim Heiligen Stuhl, Liste der angolanischen Botschafter in Brasilien, Liste der angolanischen Botschafter in Frankreich, Liste der angolanischen Botschafter in São Tomé und Príncipe"

Angola gliedert sich in 18 Provinzen (portugiesisch: "províncias", Singular – "província"):
Diese 18 Provinzen untergliedern sich weiter in 157 Municípios und 618 Kommunen.

Wie bereits erwähnt, gibt es in Angola für die nachkoloniale Zeit keine zuverlässigen Bevölkerungsstatistiken; das gilt nicht zuletzt für die Einwohnerzahlen der Städte. Von der Veröffentlichung der Erhebung des Instituto Nacional de Estatística aus dem Jahr 2008, die nach 2011 zur Verfügung stand, wurde ein qualitativer Fortschritt erwartet. Gesicherte Daten zur Bevölkerung in Angola wird erst die Volkszählung 2014 liefern. Bis dahin ist man auf geschätzte Größenordnungen angewiesen, die vor allem die vom Bürgerkrieg verursachte oder verschärfte Landflucht in Rechnung stellen. Dabei ergibt sich folgendes Bild:


Die Streitkräfte Angolas unterhalten ein etwa 110.000 Mann starkes Militär, die Forças Armadas Angolanas (FAA). Die jährlichen Ausgaben betrugen 2015 etwa 3,5 % des BIP. Es gibt drei Teilstreitkräfte: "Heer", "Marine" sowie "Luftwaffe und Luftabwehrkräfte", wovon das Heer zahlenmäßig die größte darstellt. Militärisches Gerät stammt hauptsächlich aus der ehemaligen Sowjetunion. Kleine Kontingente sind in der Republik Kongo und der Demokratischen Republik Kongo stationiert. Generalstabschef ist seit Oktober 2010 ein ehemaliger General der UNITA, Geraldo Sachipengo Nunda.

Mit einem Bruttoinlandsprodukt von 95,8 Milliarden US-Dollar (2016) ist Angola nach Südafrika und Nigeria die drittgrößte Volkswirtschaft Subsahara-Afrikas. Gleichzeitig lebt ein großer Teil der Bevölkerung in Armut.

Das Bruttoinlandsprodukt pro Kopf betrug im selben Jahr 3.502 US-Dollar (6.844 USD kaufkraftbereinigt). Angola stand damit weltweit an 120. Stelle (von ca. 200 Ländern insgesamt)

Angolas Wirtschaft leidet unter den Folgen des jahrzehntelangen Bürgerkriegs. Dank seiner Bodenschätze – vorrangig der Ölvorkommen und Diamantenabbau – gelang dem Land jedoch während der letzten Jahre ein großer wirtschaftlicher Aufschwung. Das Wirtschaftswachstum Angolas ist momentan das größte in Afrika. Allerdings kommen die Einkünfte aus den Rohstoffvorkommen nicht bei dem Großteil der Bevölkerung an, sondern bei korrupten Nutznießern innerhalb der politisch und ökonomisch Herrschenden des Landes sowie einer sich langsam bildenden Mittelschicht. Nicht zu Unrecht preisen daher in- und ausländische Unternehmer Angola als eine Art Paradies. Ein großer Teil der Bürger ist arbeitslos und etwa die Hälfte leben unterhalb der Armutsgrenze, wobei es drastische Unterschiede zwischen Stadt und Land gibt. Eine Erhebung des Instituto Nacional de Estatística von 2008 kommt zu dem Ergebnis, dass auf dem Lande rund 58 % als arm zu betrachten waren, in den Städten jedoch nur 19 %, insgesamt 37 %. Die Gesamtbevölkerung wird auf 16 bis 18 Millionen geschätzt. In den Städten, in denen sich inzwischen mehr als 50 % der Angolaner zusammenballen, ist die Mehrheit der Familien auf Überlebensstrategien angewiesen. Dort wird auch die soziale Ungleichheit am deutlichsten greifbar, insbesondere in Luanda. Im Index der menschlichen Entwicklung der UNO nimmt Angola stets einen Platz unter den letzten ein. 2008 wies Angola auf dem Gini-Index, der die Einkommensunterschiede in einem Land misst, den sehr hohen Wert von 0,62 auf.

Die wichtigsten Handelspartner für den Export von Gütern und Rohstoffen sind die USA, China, Frankreich, Belgien und Spanien. Importpartner sind überwiegend Portugal, Südafrika, USA, Frankreich und Brasilien. 2009 entwickelte sich Angola für Portugal zum größten Exportmarkt außerhalb Europas, und rund 24.000 Portugiesen übersiedelten in den letzten Jahren nach Angola, suchten dort Beschäftigung oder gründeten Unternehmen. Erheblich wichtiger ist jedoch die Präsenz Chinas in Form einer ganzen Reihe großer Unternehmen.

Von grundlegender Bedeutung für die Bevölkerung Angolas ist die Schattenwirtschaft, die sich schon während der „sozialistischen“ Phase entwickelte und in der Phase der Liberalisierung exponentiell angewachsen ist und die zurückzudrängen sich die Regierung gegenwärtig bemüht.

Lange Zeit war Angola abhängig von seinen Erdölexporten. Der Verfall des Ölpreises drückte empfindlich auf den Staatshaushalt des südwestafrikanischen Landes. Seit einigen Jahren bemüht es sich, seine Wirtschaft zu diversifizieren – weg allein vom Erdöl. Dafür ist der Ausbau der Infrastruktur nötig, die Modernisierung der Energieversorgung und bessere Bedingungen für private Investoren.

In ihre Fischfabrik von Solmar investierte Elizabete Dias Dos Santos 25 Millionen US-Dollar. Die Verarbeitungungsanlage eröffnete im Herbst 2016. Diese Art von Fließbandproduktion ist in dem Sektor einzigartig in Angola. 120 Menschen arbeiten in der Fabrik. Daneben profitieren die Zulieferer, denn mehr als 50.000 Menschen leben vom traditionellen Fischfang in Angola. 40 % der Ankäufe erfolgt bei den Kleinfischern. Um private Investoren zu gewinnen, hatte die angolanische Regierung die Bedingungen für einheimische und ausländische Unternehmen verbessert durch unter anderem Steuervergünstigungen, Hilfe bei der Finanzierung und vereinfachten Verfahren zur Firmengründung.

Bei Aceria de Angola, nördlich der Hauptstadt Luanda ging 2015 ein Stahlwerk mit einer Kapazität von 500.000 Tonnen pro Jahr in Betrieb. 350 Millionen Dollar wurden investiert. Das Werk hat mehr als 500 Arbeitsplätze und bietet vielen Menschen eine Ausbildung. In dem Werk wird vornehmlich Schrott recycelt und daraus Baustahl für Betonbauten hergestellt. Ziel des libanesisch-senegalesischen Betreibers Georges Fayez Choucair ist es, zu Exportieren. Daher ist die Kapazität des Werkes doppelt so hoch wie der angolanische Bedarf.

Mit dem Werk wurde auch die Region elektrifiziert und die Wasserversorgung erschlossen. Es musste eigens eine Hochspannungsleitung hierher gelegt werden. Die Arbeitslosigkeit in der Region sank von circa siebzig auf etwa zwanzig Prozent. Fayez Choucair ist überzeugt: „Man kann nicht in einem neuen Land investieren, in einer völlig neuen Bevölkerung und ankommen und sich einnisten nach dem Motto ‚ich bin reich‘ – nein! Man muss heute die Bevölkerung für sich gewinnen, das ist kein Projekt eines Einzelnen, sondern ein Gemeinschaftsprojekt!“

Im Jahre 2011 lag Angola bezüglich der jährlichen Erzeugung mit 5,512 Mrd. kWh an Stelle 119 und bzgl. der installierten Leistung mit 1.657 MW an Stelle 114 in der Welt. 2014 betrug die installierte Leistung 1.848 MW, davon 888 MW in Wärmekraftwerken und 960 MW in Wasserkraftwerken.

Da zurzeit (Stand 2014) nur 30 bis 40 % der Bevölkerung an das Stromnetz angeschlossen sind, plant die Regierung erhebliche Investitionen (bis 2017 23,4 Mrd. US-Dollar) im Bereich der Stromversorgung. Dies beinhaltet den Bau neuer Kraftwerke, Investitionen in die Übertragungsnetze sowie die ländliche Elektrifizierung. Es sollen eine Reihe von Wasserkraftwerken an Cuanza und Kunene errichtet werden, um das Wasserkraftpotenzial (geschätzt 18.000 MW) auszuschöpfen. Das Wasserkraftpotenzial des Kunene war schon in der Vergangenheit eine Basis für Projekte und Teilinvestitionen umfangreicher und nie komplett verwirklichter Planungen, die im Rahmen des ehemaligen Cunene-Projektes zwischen Südafrika und Angola bzw. Portugal entstanden.Die Talsperre Laúca mit einer geplanten Leistung von 2.070 MW wird zurzeit errichtet. Sie soll voraussichtlich im Juli 2017 in Betrieb gehen.

Gegenwärtig (Stand April 2015) gibt es in Angola kein nationales Verbundnetz, sondern es existieren drei voneinander unabhängige regionale Netze für den Norden, das Zentrum und den Süden des Landes sowie weitere isolierte Insellösungen. Dadurch können die Überschüsse aus dem nördlichen Netz nicht in die übrigen Netze eingespeist werden. Das bei weitem wichtigste Netz ist das nördliche, das auch die Hauptstadt Luanda umfasst. Nach Fertigstellung der Talsperre Laúca sollen auch die drei Stromnetze miteinander verbunden werden.

Die Stromversorgung ist im ganzen Land unzuverlässig und verbunden mit regelmäßigen Stromausfällen, die durch den Betrieb teurer Generatoren kompensiert werden müssen. Der Preis je kWh liegt bei 3 AOA (ca. 2,5 €-cent), wird jedoch erheblich subventioniert und ist nicht kostendeckend.

Ein strukturelles Problem der angolanischen Wirtschaft sind die extremen Unterschiede zwischen den verschiedenen Regionen, die zum Teil auf den langanhaltenden Bürgerkrieg zurückzuführen sind. Rund ein Drittel der wirtschaftlichen Tätigkeit konzentriert sich auf Luanda und die angrenzende Provinz Bengo, die immer stärker zum Expansionsraum der Hauptstadt wird. Auf der anderen Seite herrscht in verschiedenen Regionen des Binnenlandes Stillstand oder gar Rückschritt. Mindestens ebenso gravierend wie die soziale Ungleichheit sind die deutlichen wirtschaftlichen Unterschiede zwischen den Regionen. 2007 konzentrierten sich in Luanda 75,1 % aller geschäftlichen Transaktionen und 64,3 % der Arbeitsplätze in (öffentlichen oder privaten) Wirtschaftsunternehmen. 2010 waren 77 % aller Unternehmen in Luanda, Benguela, Cabinda, der Provinz Kwanza Sul und Namibe angesiedelt. Das BIP pro Kopf war 2007 in Luanda samt angrenzender Provinz Bengo auf rund 8.000 US-Dollar angewachsen, während es im westlichen Mittelangola dank Benguela und Lobito etwas unter 2.000 US-Dollar lag, im übrigen Land jedoch deutlich unter 1.000 US-Dollar. Die Tendenz zur Ballung der Wirtschaft im Küstenstreifen, insbesondere im „Wasserkopf“ Luanda/Bengo, hat seit dem Ende des Bürgerkriegs nicht etwa abgenommen, sondern sich fortgesetzt und bringt eine „Entleerung“ eines großen Teils des Binnenlandes mit sich. Die globalen Wachstumszahlen täuschen also darüber hinweg, dass die Wirtschaft Angolas unter extremen Ungleichgewichten leidet.

Eines der am stärksten ausgeprägten Merkmale des heutigen Angola ist eine allgegenwärtige Korruption. In den Erhebungen von Transparency International erscheint das Land regelmäßig unter den weltweit korruptesten, in Afrika in einer Kategorie mit Somalia und Äquatorialguinea. In den ersten fünf Jahren des 21. Jahrhunderts wurde geschätzt, dass Öleinnahmen im Wert von vier Milliarden US-Dollar oder 10 % des damaligen Bruttoinlandsprodukts durch Korruption versickerten.

Seit Jahren steht der Kampf gegen die Korruption im Regierungsprogramm, doch nur ganz selten ist nachzuweisen, dass diese Absichtserklärung in die Tat umgesetzt wird. Eine aufsehenerregende Ausnahme war Ende 2010 die Entlassung von zehn Abteilungsleitern und fast 100 Beamten der Fremden- und Grenzpolizei SME (Serviço de Migrações e Estrangeiros), die nicht nur für die Grenzkontrolle, sondern auch für die Erteilung von Einreise-, Aufenthalts- und Ausreisegenehmigungen zuständig ist.


Das Bruttoinlandsprodukt und der Außenhandel Angolas sind in den letzten Jahren aufgrund steigender Einkünfte durch die Erdölausfuhr massiv gewachsen. Mit dem Sinken des Ölpreises ab 2014 kam es zu einem Einbruch.

Die wichtigen Wirtschaftskennzahlen Bruttoinlandsprodukt, Inflation, Haushaltssaldo und Außenhandel entwickelten sich folgendermaßen:

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 33,50 Milliarden US-Dollar, dem standen Einnahmen von umgerechnet 27,27 Milliarden US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 6,5 % des BIP.

Angolas Schulden beliefen sich im Dezember 2011 auf insgesamt 31,4 Milliarden Dollar. Nahezu die Hälfte davon, ca. 17,8 Milliarden, waren nach Aussage von Finanzminister Carlos Alberto Lopes Auslandsschulden. Hauptgläubiger der angolanischen Regierung waren China mit 5,6 Milliarden, Brasilien mit 1,8 Milliarden, Portugal mit 1,4 Milliarden und Spanien mit 1,2 Milliarden. Die Inlandsschulden in Höhe von 13,6 Milliarden Dollar resultieren hauptsächlich aus Anleihen und Schatzanweisungen zur Unterstützung der laufenden staatlichen Investitions-Programme.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Seit Ende des Bürgerkriegs steigen die privaten Investitionen von Angolanern im Ausland ständig an. Dies hängt damit zusammen, dass sich im Lande die Akkumulation auf eine kleine gesellschaftliche Gruppe konzentriert und dieser daran gelegen ist, ihren Besitz aus Gründen der Sicherheit und der Profitmaximierung zu streuen. Bevorzugtes Anlageziel ist Portugal, wo angolanische Anleger (einschließlich der Familie des Staatspräsidenten) in Banken und Energieunternehmen, in der Telekommunikation und in der Presse präsent sind, aber auch z. B. Weingüter und Tourismusobjekte aufkaufen.

Der Schienenverkehr in Angola ist auf die Häfen ausgerichtet. Er wird auf drei Netzen betrieben, die nicht verbunden sind. Eine weitere, nicht mit den drei Netzen verbundene Strecke wurde inzwischen eingestellt. Es finden sowohl Güter- als auch Personenverkehr statt. Die gesamte Streckenlänge beträgt 2764 Kilometer, davon 2641 Kilometer in der im südlichen Afrika üblichen Kapspur und 123 Kilometer in 600-Millimeter-Spur (Stand 2010). Alleiniger Betreiber ist die staatliche Gesellschaft Caminhos de Ferro de Angola (CFA).

Einige bekannte angolanische Schriftsteller sind Mário Pinto de Andrade, Luandino Vieira, Arlindo Barbeitos, Alda Lara, Agostinho Neto, Pepetela, Ondjaki und José Eduardo Agualusa.

Unter dem Eintrag "Arquivos dos Dembos / Ndembu Archives" wurden 1160 Manuskripte aus Angola vom 17. bis frühen 20. Jahrhundert von der UNESCO in die Liste des Weltdokumentenerbes aufgenommen.

Zu den bekanntesten Pop-Musikern zählen Waldemar Bastos, Paulo Flores, Bonga, Vum Vum Kamusasadi, Maria de Lourdes Pereira dos Santos Van-Dúnem, Ana Maria Mascarenhas, Mario Gama, Pérola, Yola Semedo und Anselmo Ralph. Zu den international bekanntesten Musikstilen Angolas zählt der Ende der 1980er Jahre entstandene Kuduro.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Angola Platz 125 von 180 Ländern. Die Situation der Pressefreiheit im Land wird von Reporter ohne Grenzen als „schwierig“ eingestuft.

Fernsehen

Televisão Pública de Angola (angolanisch, staatlich), TV Zimbo (angolanisch, privat), AngoTV (angolanisch, privat), Rádio Televisão Portuguesa (portugiesisch, öffentlich-rechtlich), Rádio Televisão Portuguesa Internacional (portugiesisch, öffentlich-rechtlich), Televisão Comercial de Angola (angolanisch, staatlich), ZON Multimédia (privat), TV Record (brasilianisch, privat) TV Globo (brasilianisch, privat), Televisão de Moçambique (TVM) (mosambikanisch, staatlich)

Radio

RNA (Rádio Nacional de Angola) (staatlich), Rádio LAC (Luanda Antena Comercial), Rádio Ecclesia (katholischer Radiosender), Rádio Cinco (Sportradio), Rádio Despertar (der UNITA nahestehend), Rádio Mais (privat), TSF (portugiesisches Radio), Rádio Holanda (auf Portugiesisch)

2016 nutzten 23,0 % der Bevölkerung das Internet.

Zeitungen

Jornal de Angola (staatlich)

"Wochenzeitungen" (alle privat): Semanário Angolense, O País, A Capital, Folha 8, Agora, Angolense, Actual, Independente, Cara, Novo Jornal, O Apostolado (kirchlich), Gazeta de Luanda

"Wirtschaftswochenzeitungen:" Jornal de Economia & Finanças (staatlich), Semanário Económico (privat), Expansão (privat)

Zeitschriften

Rumo (Wirtschaftszeitschrift, privat)

Nachrichtenagenturen

Agência Angola Press (ANGOP; staatlich)

Am 8. Oktober 2005 gelang es der angolanischen Fußballnationalmannschaft, sich unerwartet für die WM 2006 in Deutschland zu qualifizieren. Ein knappes 1:0 beim Gruppenletzten in Ruanda reichte aus, um das Ticket zu lösen und Nigeria, das seit 1994 an jeder WM-Endrunde teilnahm, aus dem Wettbewerb zu werfen. Das angolanische Team nahm damit zum ersten Mal an einer WM-Endrunde teil, wo es nach einem gegen Portugal, einem gegen Mexiko und einem gegen den Iran als Gruppendritter in der Vorrunde ausschied. Weiterhin nahm die Mannschaft an den Afrikameisterschaften 1996, 1998, 2006, 2008, 2010 (als Ausrichter), 2012 und 2013 teil.

Die angolanische Basketballnationalmannschaft der Herren hat elf der letzten dreizehn Austragungen der Afrikameisterschaft gewonnen, womit sie die erfolgreichste Mannschaft der Wettbewerbsgeschichte ist. Daher nimmt sie regelmäßig an der Weltmeisterschaft und den Olympischen Spielen teil. Bei den Spielen 1992 war Angola der erste Gegner des US-amerikanischen Dream Teams. Größter sportlicher Erfolg war bislang das Überstehen der Vorrunde bei den Weltmeisterschaften 2002, 2006 und 2010.

Die Frauen-Handballnationalmannschaft hat bereits elfmal den Afrikameistertitel geholt und ist zudem als erste afrikanische Mannschaft bei einer WM in die Endrunde gelangt.




</doc>
<doc id="427" url="https://de.wikipedia.org/wiki?curid=427" title="Asta Nielsen">
Asta Nielsen

Asta Nielsen (* 11. September 1881 in Kopenhagen; † 25. Mai 1972 in Frederiksberg; vollständiger Name "Asta Sofie Amalie Nielsen") war eine dänische Schauspielerin.

Asta Nielsen wuchs in Schweden und Dänemark auf. Ihr Vater war ein häufig arbeitsloser Schmied, der starb, als Asta 14 Jahre alt war, die Mutter eine Wäscherin. Bereits als Kind kam sie mit dem Theater in Berührung. Ihre uneheliche Tochter Jesta wurde 1901 geboren. Ab 1902 war sie in Kopenhagen fest angestellt.

Ihr erster Film, "Afgrunden" (1910), brachte ihr und dem Regisseur Urban Gad gleich einen Vertrag zur Produktion von mehreren Filmen in Deutschland, der aufgrund des einsetzenden Erfolges bis 1915 verlängert wurde. Sie drehte anfangs ausschließlich unter der Regie von Urban Gad, den sie 1912 heiratete. 1915 endete die berufliche Zusammenarbeit, 1918 erfolgte die Scheidung.

Meist waren ihre Rollen konfliktbeladene Frauen, deren Verhalten nicht den gesellschaftlichen Konventionen entsprach, so in "Der fremde Vogel" (1911) und "Die arme Jenny" (1912). Nielsen hatte aber auch Talent für komische Rollen und war beim Publikum damit vor allem in "Engelein" (1914) so erfolgreich, dass eine Fortsetzung gedreht wurde.

1916 ging sie wieder nach Dänemark und kehrte erst nach Ende des Ersten Weltkrieges wieder nach Deutschland zurück, wo sie fortan vorwiegend in Literaturverfilmungen und Dramen auftrat. Zwischen 1920 und 1922 produzierte sie drei Filme selbst. Darunter eine Verfilmung von Shakespeares "Hamlet", in der sie den Dänenprinzen spielt. Nach der im Film vertretenen Theorie war Hamlet eine als männlicher Thronfolger erzogene Prinzessin, was seine/deren abweisende Haltung gegenüber Ophelia erklären soll, in Wahrheit aber wohl eher Asta Nielsen zu einer interessanten Rolle verhelfen sollte. Herausragend ist auch ihre Darstellung von Frauen am untersten Rand der Gesellschaft in "Die freudlose Gasse" (1925) von Georg Wilhelm Pabst und "Dirnentragödie" (1927) von Bruno Rahn.

Asta Nielsen war der große Star des Stummfilms, im Prinzip sogar der erste weibliche Filmstar überhaupt in der Geschichte des Kinos, in der sie als eines ihrer ersten Sexsymbole gelten kann. Asta Nielsen ließ sich nie auf ein Rollenfach festlegen: Sie spielte sowohl gebrochene, leidende Frauen als auch Prostituierte; Tänzerinnen ebenso wie einfache Arbeiterinnen. Ihre Körpersprache war immer dezent, dabei aber ausdrucksstark.

Ihre Filmkarriere endete mit dem Tonfilm, sie trat nur in einem einzigen, "Unmögliche Liebe", auf. Obwohl sie eine angenehme Stimme hatte, ging ihr gekonntes Mienenspiel in diesem neuen Medium unter. Filmangebote lehnte sie kontinuierlich ab. Sie widmete sich fortan dem Theater und veröffentlichte 1946 ihre Autobiographie "Die schweigende Muse". 1963 wurde sie mit dem Filmband in Gold für ihr langjähriges und hervorragendes Wirken im deutschen Film ausgezeichnet. 1968 erschien ein von ihr produzierter, autobiografischer Dokumentarfilm.

Asta Nielsen starb 1972 und wurde auf dem Vestre Kirkegård (Westfriedhof) in Kopenhagen in einem anonymen Gemeinschaftsgrab beigesetzt.

Im September 2010 wurde sie mit einem Stern auf dem Boulevard der Stars in Berlin geehrt.

Asta Nielsen besaß ab 1929 auf der Ostseeinsel Hiddensee ein Haus, das sie „Karusel“ nannte (dänische Bezeichnung für Karussell). Der Name leitet sich von der runden Form des Gebäudes ab. Sie verbrachte dort mit ihrer Tochter und ihrem Mann oft mehrere Monate im Sommer. Zu den Freunden und Bekannten, die sie dort besuchten, zählen auch Joachim Ringelnatz mit Frau, Heinrich George und Gerhart Hauptmann. Nach 1935 oder 1936 nutzte sie das Haus nicht mehr.

Das 1923 erbaute Haus des Bauhaus-Architekten Max Taut wurde 1975 unter Denkmalschutz gestellt. Ihre Erben verkauften es 1989 an die Gemeinde. 2015 wurde es als „Asta-Nielsen-Haus“ eröffnet und enthält auch eine Ausstellung zu Asta Nielsen.



2000 gründete sich in Frankfurt am Main durch eine Initiative von Filmemacherinnen, Kuratorinnen, Kritikerinnen, Studierenden, Historikerinnen und Filmliebhaberinnen die Kinothek Asta Nielsen. Die Kinothek ist ein Verein, der es sich zur Aufgabe gemacht hat, die Filmarbeit von Frauen zu dokumentieren und wieder in die Kinos zu bringen und schließt mit ihrer Arbeit an die feministische Filmarbeit der 1970er und 1980er-Jahre an. Seit 2006 ist Karola Gramann die künstlerische Leiterin der Kinothek Asta Nielsen.

Die Kinothek widmete ihrer Namensgeberin mehrere Retrospektiven.





</doc>
<doc id="428" url="https://de.wikipedia.org/wiki?curid=428" title="Autismus">
Autismus

Autismus (von „selbst“) ist eine tiefgreifende Entwicklungsstörung, die als Autismus-Spektrum-Störung diagnostiziert wird. Diese tritt in der Regel vor dem dritten Lebensjahr auf und zeigt sich in drei Bereichen:

Aufgrund ihrer Einschränkungen benötigen die meisten Autisten eine lebenslange Hilfe und Unterstützung. Autismus ist unabhängig von der Intelligenzentwicklung, jedoch gehört Intelligenzminderung zu den häufigen zusätzlichen Einschränkungen. Trotz umfangreicher Forschungsanstrengungen gibt es derzeit keine allgemein anerkannte Erklärung der Ursachen autistischer Störungen.

Das aktuelle ICD-10 unterscheidet noch zwischen verschiedenen Formen von Autismus (z. B. "frühkindlichem Autismus", dem "Asperger-Syndrom" und dem "atypischen Autismus"). Das DSM-5 und das ICD-11 (für 2018 erwartet) hingegen unterscheiden keine Subtypen mehr und sprechen nur noch von einer allgemeinen "Autismus-Spektrum-Störung" (ASS; , kurz "ASD"). Grund für diese Änderung war die zunehmende Erkenntnis in der Wissenschaft, dass eine klare Abgrenzung von Subtypen (noch) nicht möglich ist – und man stattdessen von einem fließenden Übergang zwischen milden und stärkeren Autismusformen ausgehen sollte.

Das ICD-10 definiert folgendes:

Der Schweizer Psychiater Eugen Bleuler prägte den Begriff "Autismus" um 1911 im Rahmen seiner Forschungen zur Schizophrenie. Er bezog ihn ursprünglich zunächst nur auf diese Erkrankung und wollte damit eines ihrer Grundsymptome beschreiben – die Zurückgezogenheit in eine innere Gedankenwelt. Bleuler verstand unter "Autismus" „die Loslösung von der Wirklichkeit zusammen mit dem relativen oder absoluten Überwiegen des Binnenlebens.“ (Originalzitat)

Sigmund Freud übernahm die Begriffe „Autismus“ und „autistisch“ von Bleuler und setzte sie annähernd mit „Narzissmus“ bzw. „narzisstisch“ gleich – als Gegensatz zu „sozial“. Die Begriffsbedeutung wandelte sich mit der Zeit von „dem Leben in einer eigenen Gedanken- und Vorstellungswelt“ hin zu „Selbstbezogenheit“ in einem allgemeinen Sinne.

Hans Asperger und Leo Kanner nahmen den Autismus-Begriff dann auf (siehe historische Literatur), möglicherweise unabhängig voneinander. Sie sahen in ihm aber nicht mehr nur ein einzelnes Symptom (wie Bleuler), sondern versuchten damit gleich ein ganzes Störungsbild eigener Art zu erfassen. Sie unterschieden dabei Menschen mit Schizophrenie, die sich aktiv in ihr Inneres zurückziehen, von jenen, die von Geburt an in einem Zustand der inneren Zurückgezogenheit leben. Letzteres definierte nunmehr den Begriff „Autismus“.

Kanner fasste den Begriff „Autismus“ eng, was im Wesentlichen dem heute sogenannten frühkindlichen Autismus entsprach (daher: "Kanner-Syndrom"). Seine Sichtweise erreichte internationale Anerkennung und wurde zur Grundlage der weiteren Autismusforschung. Die Veröffentlichungen Aspergers hingegen beschrieben „Autismus“ etwas anders und wurden zunächst international kaum wahrgenommen. Dies lag zum einen an der zeitlichen Überlagerung mit dem Zweiten Weltkrieg und zum anderen daran, dass Asperger auf Deutsch publizierte und man seine Texte jahrzehntelang nicht ins Englische übersetzte. Hans Asperger selbst nannte das von ihm beschriebene Syndrom „Autistische Psychopathie“. Die englische Psychiaterin Lorna Wing (siehe historische Literatur) führte seine Arbeit in den 1980er Jahren fort und benutzte erstmals die Bezeichnung "Asperger-Syndrom". Internationale Bekanntheit in Fachkreisen erreichten die Forschungen Aspergers aber erst ab 1990.

Im deutschsprachigen Raum sind drei Diagnosearten des Autismus gebräuchlich:


Im DSM-5 wurden 2013 alle Einzelkategorien unter die "Autismus-Spektrum-Störung (ASS)" (autism spectrum disorders) zusammengefasst. Die Begründung hierfür lautete, die Forscher gingen heute davon aus, dass es sich weniger um qualitativ unterschiedliche Erkrankungen handele als um ein Kontinuum von sehr milden bis schweren Verlaufsformen einer Entwicklungsstörung, die bereits in der frühen Kindheit beginne. Bei den Symptomen wird unterschieden zwischen Defiziten in zwei Kategorien: Gestört ist erstens die soziale Interaktion und Kommunikation (zum Beispiel Blickkontakte, Fähigkeit zur Konversation oder Aufbau von Beziehungen sind schwach ausgeprägt). Zweitens sind repetitive Verhaltensweisen und fixierte Interessen und Verhaltensweisen Merkmale autistischer Störungen.

Diagnosekriterien: Überblick

Im DSM-IV wird der frühkindliche Autismus den tiefgreifenden Entwicklungsstörungen zugeordnet und durch folgende diagnostische Kriterien beschrieben:

A. Es müssen insgesamt aus 1., 2. und 3. mindestens sechs Kriterien zutreffen, wobei mindestens zwei Punkte aus 1. und je ein Punkt aus 2. und 3. stammen müssen:

B. Verzögerungen oder abnorme Funktionsfähigkeit in mindestens einem der folgenden Bereiche mit Beginn vor dem dritten Lebensjahr:


C. Die Störung kann nicht besser durch das Rett- oder Heller-Syndrom erklärt werden.

Darüber hinaus nennt ICD-10 noch unspezifische Probleme wie Befürchtungen, Phobien, Schlafstörungen, Essstörungen, Wutausbrüche, Aggressionen und selbstverletzendes Verhalten (Automutilation).

Die drei wichtigsten Bereiche sind:

Soziale Interaktion

Eine qualitative Beeinträchtigung der sozialen Interaktion zeigt sich manchmal schon in den ersten Lebensmonaten durch fehlende Kontaktaufnahme zu den Eltern, insbesondere zur Mutter. Viele Kinder mit frühkindlichem Autismus strecken der Mutter nicht die Arme entgegen, um hochgehoben zu werden. Sie lächeln nicht zurück, wenn sie angelächelt werden, und nehmen zu den Eltern keinen angemessenen Blickkontakt auf. Nichtsdestoweniger sind autistische Kinder genauso stark emotional mit ihrer Mutter verbunden wie nicht-autistische Kinder und haben genauso viel Mitgefühl wie nicht-autistische Kinder.

In neuerer Forschung finden sich Hinweise darauf, dass sowohl die kognitive als auch die emotionale Empathie bei Menschen mit Autismus eingeschränkt sind. Kinder mit frühkindlichem Autismus zeigen zudem eine starke Objektbezogenheit, die häufig auf eine bestimmte Art von Gegenständen beschränkt ist. Ihre Aufmerksamkeit ist auf wenige Dinge, wie Wasserhähne, Türklinken, Fugen zwischen Steinplatten oder kariertes Papier gerichtet, die sie sehr stark anziehen, so dass alles andere sekundär wird und nicht oder kaum beachtet wird. Oft finden sie in Gegenständen eine normalerweise ungewöhnliche Systematik (sortieren beispielsweise die Einzelteile einer Spielzeugeisenbahn nach Größe und Farbe) oder Anwendung (beispielsweise ist ihr einziges Interesse an einem Spielzeugauto, die Räder unablässig zu drehen).

Kommunikation

Etwa jedes zweite Kind mit frühkindlichem Autismus entwickelt keine Lautsprache. Bei den anderen verzögert sich die Sprachentwicklung. Die Entwicklung der Lautsprache erfolgt oft über eine lange Phase der Echolalie, manche der betroffenen Personen kommen über diese Phase nicht hinaus. Im Kindesalter werden oft die Pronomina vertauscht (pronominale Umkehr). Sie reden von Anderen als „ich“ und von sich selbst als „du“ oder in der dritten Person. Diese Eigenart bessert sich üblicherweise im Laufe der Entwicklung. Zudem gibt es oft Probleme mit Ja/Nein-Antworten, Gesagtes wird stattdessen durch Wiederholung bestätigt. Probleme gibt es auch mit der Semantik: Wortneuschöpfungen (Neologismen) treten häufig auf. Manche Menschen mit frühkindlichem Autismus haften auch an bestimmten Formulierungen (Perseveration). Am ausgeprägtesten ist die Beeinträchtigung der Pragmatik: In der Kommunikation mit anderen Menschen haben autistische Menschen Schwierigkeiten, Gesagtes über die genaue Wortbedeutung hinaus zu verstehen, zwischen den Zeilen zu lesen. Ihre Stimme klingt oft eintönig (fehlende Prosodie).

Die Probleme in der Kommunikation äußern sich in schwieriger Kontaktaufnahme zur Außenwelt und zu anderen Menschen. Manche Autisten scheinen die Außenwelt kaum wahrzunehmen und teilen sich ihrer Umwelt auf ihre ganz individuelle Art mit. Deshalb wurden autistische Kinder früher auch "Muschelkinder" oder "Igelkinder" genannt. Die visuellen und auditiven Wahrnehmungen sind oft ungewöhnlich intensiv. Daher besteht manchmal die Annahme, Abschaltfunktionen im Gehirn würden als Selbstschutz mögliche Reizüberflutungen vermindern. Autisten haben ein individuell unterschiedlich ausgeprägtes Bedürfnis nach Körperkontakt. Einerseits nehmen manche mit fremden Menschen direkten und teils sozial unangemessenen Kontakt auf, andererseits kann auch jede Berührung für sie aufgrund der Überempfindlichkeit ihres Tastsinns unangenehm sein.

Vor diesem Hintergrund ist verstehende Kommunikation mit einem Autisten schwer. Emotionen werden oft falsch gedeutet oder gar nicht erst verstanden. Diese möglichen Probleme müssen bei der Kontaktaufnahme berücksichtigt werden und verlangen ein großes Einfühlungs- und Vorstellungsvermögen.

Repetitive und stereotype Verhaltensmuster

Veränderungen ihrer Umwelt, wie zum Beispiel umgestellte Möbel oder ein anderer Schulweg, beunruhigen und verunsichern manche autistische Menschen. Manchmal geraten Betroffene auch in Panik, wenn sich Gegenstände nicht mehr an ihrem gewöhnlichen Platz oder in einer bestimmten Anordnung befinden, oder es bringt sie ein unangekündigter Besuch oder spontaner Ortswechsel völlig aus der Fassung. Handlungen laufen meist ritualisiert ab, und Abweichungen von diesen Ritualen führen zu Chaos im Kopf, denn autistische Menschen haben bei unerwarteten Veränderungen von Situationen oder Abläufen in der Regel keine alternativen Strategien.

Unter stark autistischen Menschen anzutreffende, repetitive (sich wiederholende) Stereotypien können sein: Jaktationen (Schaukeln mit Kopf oder Oberkörper), im Kreis umhergehen, Finger verdrehen, Oberflächen betasten und vereinzelt auch selbstverletzendes Verhalten, wie etwa Finger blutig knibbeln, Nägel bis über das Nagelbett hinaus abkauen, den Kopf anschlagen, mit der Hand an den Kopf schlagen, sich selbst kratzen, beißen oder anderes. Dieses selbstverletzende Verhalten hinterlässt mehr oder weniger sichtbare Spuren wie Bissspuren, Narben und verschorfte Wunden auf der Haut. Diese selbstverletzenden Verhaltensweisen sind jedoch nicht zu verwechseln mit dem bewusst selbstverletzenden Verhalten, das typischerweise zum Spannungsabbau eingesetzt wird (etwa durch Verbrennungen oder Ritzen am Unterarm) oder – seltener – aus suizidalen Tendenzen heraus entsteht und dann ein anderes (suizidales) Verletzungsmuster aufweist.

Sich wiederholende Verhaltensweisen wirken auf alle Menschen beruhigend (wie Puppe oder Teddybär bei kleinen Kindern, die überallhin mitgenommen werden) und sind möglicherweise mehr ein Kennzeichen für starken Stress als für Autismus selbst, was die Frage aufwirft, warum Autisten oft zu viel Stress ausgesetzt sind. Positive Effekte sich wiederholender Verhaltensweisen werden zum Beispiel im Yoga benutzt, und es gibt auch auf Autismus angepassten Yogaunterricht.

Treten alle Symptome des frühkindlichen Autismus zusammen mit normaler Intelligenz (einem IQ von mehr als 70) auf, so spricht man vom hochfunktionalen Autismus (high-functioning autism, HFA). Diagnostisch wichtig ist hier insbesondere die verzögerte Sprachentwicklung. Gegenüber dem Asperger-Syndrom sind die motorischen Fähigkeiten meist deutlich besser.

Oftmals wird, durch die Verzögerung der Sprachentwicklung, zunächst der niedrigfunktionale frühkindliche Autismus (LFA) diagnostiziert. Es kann dann aber später eine normale Sprachentwicklung erfolgen, bei der durchaus ein mit dem Asperger-Syndrom vergleichbares Funktionsniveau erreicht wird. Viele HFA-Autisten sind deshalb als Erwachsene nicht von Asperger-Autisten zu unterscheiden, meistens bleiben die autistischen Symptome aber wesentlich deutlicher ausgeprägt als beim Asperger-Syndrom. Die Sprache muss sich dabei nicht zwangsläufig entwickeln, viele nicht sprechende HFA-Autisten können trotzdem eigenständig leben und lernen, sich schriftlich zu äußern. Internetbasierte Kommunikationsformen helfen gerade diesen Menschen, ihre Lebensqualität deutlich zu steigern.

Atypischer Autismus unterscheidet sich vom frühkindlichen Autismus dadurch, dass Kinder nach dem dritten Lebensjahr autistisches Verhalten zeigen (atypisches Erkrankungsalter) oder nicht alle Symptome aufweisen (atypische Symptomatik).

Autistische Kinder mit atypischem Erkrankungsalter zeigen bei den Symptomen das Vollbild des frühkindlichen Autismus, der sich bei ihnen aber erst nach dem dritten Lebensjahr manifestiert.

Autistische Kinder mit atypischer Symptomatik legen Auffälligkeiten an den Tag, die für den frühkindlichen Autismus typisch sind, jedoch die Diagnosekriterien des frühkindlichen Autismus nicht vollständig erfüllen. Dabei können sich die Symptome sowohl vor als auch nach dem dritten Lebensjahr manifestieren.

Im, vor allem in den USA gebräuchlichen, psychiatrischen Diagnosehandbuch (DSM-IV) gibt es keine Diagnose „atypischer Autismus“, dort wird stattdessen „tiefgreifende Entwicklungsstörung – nicht anders bezeichnet“ (PDD-NOS) als Diagnose verwendet. Umgangssprachlich wird PDD-NOS dort oft auch falsch nur als „tiefgreifende Entwicklungsstörung (PDD)“ bezeichnet, was nur die diagnostische Kategorie bezeichnet, aber selbst keine Diagnose ist.

Wenn atypischer Autismus zusammen mit erheblicher Intelligenzminderung auftritt, wird manchmal auch von „Intelligenzminderung mit autistischen Zügen“ gesprochen. Neuere Forschungen deuten jedoch darauf hin, dass die Annahme einer Intelligenzminderung bei Autisten mit dem Wechsler-IQ-Test verfälscht wird, und Autisten beim Ravens-Matrizentest um bis zu 30 Punkte besser abschneiden, was nicht auf weniger, sondern auf eine andere Intelligenz hindeutet.

Das nach dem österreichischen Mediziner Hans Asperger benannte Asperger-Syndrom (AS) gilt als leichte Form des Autismus und manifestiert sich etwa vom vierten Lebensjahr an. Obwohl viele Verhaltensweisen das soziale Netz der Betroffenen, insbesondere das der nächsten Bekannten und der Familie, stark in Anspruch nehmen, gibt es nicht nur negative Aspekte des Asperger-Syndroms. Es gibt zahlreiche Berichte über das gleichzeitige Auftreten von überdurchschnittlicher Intelligenz oder auch von Inselbegabungen. Leichtere Fälle von Asperger-Syndrom werden im Englischen umgangssprachlich auch als „Little Professor Syndrome“, „Geek Syndrome“ oder „Nerd Syndrome“ bezeichnet.

Soziale Interaktion

Eines der schwerwiegendsten Probleme für Menschen mit Asperger-Syndrom ist die Beeinträchtigung von sozialem Interaktionsverhalten, besonders in zwei Bereichen: zum einen in einer eingeschränkten Fähigkeit, zwanglose Beziehungen zu anderen Menschen herzustellen, und zum anderen Einschränkungen in Bezug auf nonverbale Kommunikation.

Bei Kindern und Jugendlichen mit Asperger-Syndrom fehlt oft der Wunsch, Beziehungen zu Gleichaltrigen herzustellen. Dieser Wunsch entsteht bei ihnen normalerweise erst in der Adoleszenz, meist fehlt dann aber die Fähigkeit dazu.

Die Beeinträchtigungen im Bereich der nonverbalen Kommunikation betreffen sowohl das Verstehen nonverbaler Botschaften anderer Menschen als auch das Aussenden eigener nonverbaler Signale. Dazu zählt in einigen Fällen etwa auch die Anpassung der Tonhöhe und Lautstärke der eigenen Sprache.

Als besonders problematisch erweist sich die soziale Interaktion, da Menschen mit Asperger-Syndrom nach außen hin keine offensichtlichen Anzeichen einer Behinderung haben. So kann es geschehen, dass die Schwierigkeiten von Menschen mit Asperger-Syndrom als bewusste Provokation empfunden werden, obwohl dies nicht der Fall ist. Wenn etwa eine betroffene Person auf eine an sie gerichtete Frage nur mit Schweigen reagiert, wird dies oft als Sturheit und Unhöflichkeit gedeutet.

Im Alltag macht sich die schwierige soziale Interaktion vielfältig bemerkbar. Menschen mit Asperger-Syndrom können schlecht Blickkontakt mit anderen Menschen aufnehmen oder halten. Sie vermeiden Körperkontakt wie etwa Händeschütteln. Sie sind unsicher, wenn es darum geht, Gespräche mit anderen zu führen, besonders wenn es sich um eher belanglosen Smalltalk handelt. Soziale Regeln, die andere intuitiv beherrschen, verstehen Menschen mit Asperger-Syndrom nicht intuitiv, sondern müssen sie sich erst aneignen. Daher haben Menschen mit Asperger-Syndrom oft keine oder weniger Freunde. In der Schule etwa sind sie in den Pausen lieber für sich, weil sie mit dem üblichen Umgang anderer Schüler untereinander nur wenig anfangen können. Im Unterricht sind sie in der Regel wesentlich besser im schriftlichen als im mündlichen Bereich. In der Ausbildung und im Beruf macht ihnen der fachliche Bereich meist keine Schwierigkeiten, nur der Smalltalk mit Kollegen oder der Kontakt mit Kunden. Auch das Telefonieren kann Probleme bereiten. Im Studium können mündliche Prüfungen oder Vorträge große Hürden darstellen. Da auf dem Arbeitsmarkt wohl in allen Bereichen Kontakt- und Teamfähigkeit genauso viel zählen wie fachliche Eignung, haben Menschen mit Asperger-Syndrom Probleme, überhaupt eine geeignete Stelle zu finden. Viele sind selbständig, jedoch können sie sich bei Problemen mit Kunden kaum durchsetzen. In einer Werkstatt für behinderte Menschen indes wären sie völlig unterfordert.

Die meisten Menschen mit Asperger-Syndrom können durch hohe Schauspielkunst nach außen hin eine Fassade aufrechterhalten, so dass ihre Probleme auf den ersten Blick nicht gleich sichtbar sind, jedoch bei persönlichem Kontakt durchscheinen, etwa in einem Vorstellungsgespräch. Menschen mit Asperger-Syndrom gelten nach außen hin zwar als extrem schüchtern, jedoch ist das nicht das eigentliche Problem. Schüchterne Menschen verstehen die sozialen Regeln, trauen sich aber nicht, sie anzuwenden. Menschen mit Asperger-Syndrom würden sich schon trauen, sie anzuwenden, verstehen sie aber nicht und haben deshalb Probleme, damit umzugehen. Die Fähigkeit zur kognitiven Empathie (Einfühlungsvermögen) ist gar nicht oder nur schwach ausgeprägt, jedoch ist die affektive Empathie (Mitgefühl) gegenüber anderen durchaus genauso oder sogar stärker ausgeprägt als bei nicht-autistischen Menschen. Menschen mit Asperger-Syndrom können sich schlecht in andere Menschen hineinversetzen und deren Stimmungen oder Gefühle an äußeren Anzeichen ablesen. Überhaupt können sie nur schwer zwischen den Zeilen lesen und nicht-wörtliche Bedeutungen von Ausdrücken oder Redewendungen verstehen. Sie ecken an, weil sie die für andere Menschen offensichtlichen nonverbalen Signale nicht verstehen. Da es ihnen meist schwerfällt, Gefühle zu benennen und auszudrücken, passiert es oft, dass ihre Mitmenschen dies als mangelndes persönliches Interesse missdeuten. Auch können sie in gefährliche Situationen geraten, da sie äußere Anzeichen, die auf eine bevorstehende Gefahr – etwa durch Betrüger oder Gewalttäter – hinweisen, oft nicht richtig deuten können.

Stereotype Verhaltensmuster und Sonderinteressen

Menschen mit Asperger-Syndrom zeigen in ihrer Lebensgestaltung und in ihren Interessen repetitive und stereotype Verhaltensmuster. Das Leben von Menschen mit Asperger-Syndrom ist durch ausgeprägte Routinen bestimmt. Werden sie in diesen gestört, können sie erheblich beeinträchtigt werden. In ihren Interessen sind Menschen mit Asperger-Syndrom teilweise auf ein Gebiet begrenzt, auf dem sie meist ein enormes Fachwissen haben. Ungewöhnlich ist das Ausmaß, mit dem sie sich ihrem Interessensgebiet widmen; für andere Gebiete als das eigene sind sie meist nur schwer zu begeistern. Da Menschen mit Asperger-Syndrom meist gut logisch denken können, liegen ihre Interessensgebiete oft im mathematisch-naturwissenschaftlichen Bereich; die gesamte geisteswissenschaftliche Palette, aber auch andere Gebiete sind möglich.

Ritualisiertes und stereotypes Denken und Wahrnehmen

Zu den ritualisierten Handlungen können neben motorischen Schematismen, Stereotypien und repetitiven Sprechhandlungen auch repetitive und stereotype Formen des Denkens und der Wahrnehmung gezählt werden. Diese bestehen in der Konzentration auf einige wenige, jedoch mit großer Intensität verfolgte Spezialinteressen. Die intensive Herausbildung von Spezialinteressen führt zur Entwicklung von Inselbegabungen, die mehr oder weniger stark ausgeprägt sein können. Die sogenannten Inselbegabungen sind also keine Fähigkeit, die unabhängig von den Handlungen der jeweiligen Person einfach vorhanden ist, sondern sie sind erst das Ergebnis einer langen und intensiven Beschäftigung mit einem bestimmten Gegenstandsbereich. Hier bilden sich neuronale Felder und Netze von hoher lokaler Konnektivität heraus, die jedoch nur schwach durch globale Konnektivität im Gehirn mit anderen Arealen verbunden sind.

Die Interessen von Autisten sind häufig auf bestimmte Gebiete konzentriert. Jedoch besitzen manche auf einem solchen Gebiet außergewöhnliche Fähigkeiten, zum Beispiel im Kopfrechnen, Zeichnen, in der Musik oder in der Merkfähigkeit. Man spricht dann von einer „Inselbegabung“. Diejenigen, die sie haben, nennt man auch "Savants" (Wissende). Etwa 50 Prozent der Inselbegabten sind Autisten. Andersherum ist nur ein kleiner Teil der Autisten inselbegabt.

Nach den Daten von 178 erst im Erwachsenenalter diagnostizierten Patienten (Zeitraum 2005–2009) der auf Autismus spezialisierten Kölner Spezialsprechstunde erfolgte die erstmalige Diagnose im Durchschnitt im Alter von 34 Jahren.

Bei der Diagnostik ist wichtig zu beachten, dass nicht die einzelnen Symptome autismusspezifisch sind, da ähnliche Merkmale auch bei anderen Störungen auftreten. Spezifisch für Autismus ist vielmehr erst die Kombination von mehreren dieser Symptome, d. h. die Symptomkonstellation.

Autismus wird im fünften Kapitel der ICD-10 als tiefgreifende Entwicklungsstörung aufgeführt. Sie wird unter dem Schlüssel F84 wie folgt unterteilt:


Manche der oben genannten alternativen Bezeichnungen sind zwar veraltet, jedoch noch heute in der ICD-10 zu finden. 2018 wird die überarbeitete Version des ICD-11 erwartet.

Das DSM-5 (die US-amerikanische Klassifikation psychischer Störungen) fasst alle Formen des Autismus in der Diagnose "Autismus-Spektrum-Störung (ASS)" zusammen. Die Diagnosekriterien der ASS sind in fünf Gebiete aufgeteilt (A-E):
Basierend auf den Beeinträchtigungen der sozialen Kommunikation und eingeschränkten, repetitiven Verhaltensmustern wird der aktuelle Schweregrad bestimmt (Schweregrad 3 - „Sehr Umfangreiche Unterstützung Erforderlich“, Schweregrad 2 - „Umfangreiche Unterstützung Erforderlich“, Schweregrad 1 - „Unterstützung Erforderlich“). DSM 5 weist ausdrücklich darauf hin, dass Personen mit einer gesicherten DSM-IV-Diagnose einer autistischen Störung, Asperger-Syndrom oder nicht näher bezeichneten tiefgreifenden Entwicklungsstörung eine ASS-Diagnose bekommen sollen. Bei deutlichen sozialen Kommunikationdefiziten, die sonst aber nicht die Kriterien der Autismus-Spektrum-Störung erfüllen, soll die Diagnose "Soziale Kommunikationsstörung" erwogen werden.

Autistische Verhaltensweisen können auch bei anderen Syndromen und psychischen Erkrankungen auftreten. Von diesen muss Autismus daher abgegrenzt werden:


Zusammen mit Autismus können verschiedene begleitende (komorbide) körperliche und psychische Erkrankungen auftreten:


Eine Analyse von 11.091 Interviews von 2014 durch das "National Center for Health Statistics" der USA ergab eine Häufigkeit (Lebenszeitprävalenz) des ASS von 2,24 % in der Altersgruppe 3–17 Jahre, 3,29 % bei Jungen und 1,15 % bei Mädchen.

Eine Übersicht von 2015 zeigte, dass die Zahlen zur Geschlechterverteilung wegen methodischer Schwierigkeiten stark variierten. Das Verhältnis männlich-weiblich betrage jedoch mindestens 2:1 bis 3:1, was auf einen biologischen Faktor in dieser Frage hindeute.
Die Zahl der Autismus-Fälle scheint in den vergangenen Jahrzehnten stark gestiegen zu sein. Die Centers for Disease Control (CDC) in den USA geben einen Anstieg der Autismusfälle um 57 % an (zwischen 2002 und 2006). 2006 war 1 von 110 Kindern im Alter von 8 Jahren von Autismus betroffen. Obwohl bessere und frühere Diagnostik eine Rolle spielt, kann laut CDC nicht ausgeschlossen werden, dass ein Teil des Anstiegs auf eine tatsächliche Erhöhung der Fälle zurückzuführen ist.

Früher gab es den Verdacht, dass Umweltgifte oder Impfstoff-Zusätze die Entstehung von Autismus begünstigen könnten. Nach dem Stand von 2017 gilt letzteres als widerlegt und ersteres als nicht ausreichend erforscht.

Folgende Faktoren spielen bei der Zunahme der Fallzahlen in jüngerer Zeit eine Rolle:


Autismus kann die Entwicklung der Persönlichkeit, die Berufschancen und die Sozialkontakte erheblich beeinträchtigen. Der Langzeitverlauf einer Störung aus dem Autismusspektrum hängt von der individuellen Ausprägung beim einzelnen Patienten ab. Die Ursache des Autismus kann nicht behandelt werden, da sie nicht bekannt ist. Möglich ist lediglich eine unterstützende Behandlung in einzelnen Symptombereichen.

Andererseits sind viele Schwierigkeiten, über die autistische Menschen berichten, durch Anpassungen der Umwelt vermeidbar oder verminderbar. Beispielsweise berichten manche von einem Schmerzempfinden für bestimmte Tonfrequenzen. Solchen Menschen geht es in einem reizarmen Umfeld deutlich besser. Eine autismusgerechte Umwelt zu finden bzw. herzustellen ist deshalb ein wesentliches Ziel.

Kommunikationstraining für Autisten sowie für deren Freunde und Angehörige kann für alle Beteiligten hilfreich sein und wird beispielsweise in Großbritannien von der "National Autistic Society" angeboten und wissenschaftlich weiterentwickelt. Eine zunehmende Zahl von Schulen, Colleges und Arbeitgebern speziell für autistische Menschen demonstriert den Erfolg, Autisten in autismusgerechten Umfeldern leben zu lassen.

Die autistischen Syndrome gehören nach dem (deutschen) Schwerbehindertenrecht zur Gruppe der psychischen Behinderungen. Nach den Grundsätzen der Versorgungsmedizin-Verordnung beträgt der Grad der Behinderung je nach Ausmaß der sozialen Anpassungsschwierigkeiten 10 bis 100.

Beim frühkindlichen und atypischen Autismus bleibt eine Besserung des Symptombilds meist in engen Grenzen. Etwa 10–15 % der Menschen mit frühkindlichem Autismus erreichen im Erwachsenenalter eine eigenständige Lebensführung. Der Rest benötigt in der Regel eine intensive, lebenslange Betreuung und eine geschützte Unterbringung.

Über den Langzeitverlauf beim Asperger-Syndrom gibt es noch keine Studien. Hans Asperger selbst nahm einen positiven Langzeitverlauf an. In der Regel lernen Menschen mit Asperger-Syndrom im Laufe ihrer Entwicklung, ihre Probleme – abhängig vom Grad ihrer intellektuellen Fähigkeiten – mehr oder weniger gut zu kompensieren. Der britische Autismusexperte Tony Attwood vergleicht den Entwicklungsprozess von Menschen mit Asperger-Syndrom mit der Erstellung eines Puzzles: Mit der Zeit bekommen sie die einzelnen Teile des Puzzles zusammen und erkennen das ganze Bild. So können sie das Puzzle (oder Rätsel) des Sozialverhaltens lösen. Schließlich können Menschen mit Asperger-Syndrom einen Status erreichen, in dem ihre Störung im Alltag nicht mehr auffällt.

Es existiert eine Reihe von Büchern über autistische Menschen. Der Neurologe Oliver Sacks und Psychologe Torey L. Hayden haben Bücher über ihre Patienten mit Autismus und deren Lebensweg veröffentlicht. An Büchern, die von Autisten selbst geschrieben wurden, sind insbesondere die Werke der US-amerikanischen Tierwissenschaftlerin Temple Grandin, der australischen Schriftstellerin und Künstlerin Donna Williams, der US-amerikanischen Erziehungswissenschaftlerin Liane Holliday Willey und des deutschen Schriftstellers und Filmemachers Axel Brauns bekannt.

Welche Schule für Menschen mit Autismus geeignet ist, hängt von Intelligenz, Sprachentwicklung und Ausprägung des Autismus beim Einzelnen ab. Sind Intelligenz und Sprachentwicklung normal ausgeprägt, können Kinder mit Autismus eine Regelschule besuchen. Andernfalls kann der Besuch einer Förderschule in Betracht gezogen werden.

Hinsichtlich Ausbildung und Beruf muss ebenfalls der individuelle Entwicklungsstand des Einzelnen berücksichtigt werden. Sind Intelligenz und Sprachentwicklung normal ausgeprägt, können ein reguläres Studium oder eine reguläre Berufsausbildung absolviert werden. Andernfalls kann etwa eine Tätigkeit in einer Werkstatt für behinderte Menschen in Frage kommen. In jedem Fall ist es für die Integration und das Selbstwertgefühl autistischer Menschen wichtig, einer Tätigkeit nachgehen zu können, die ihren individuellen Fähigkeiten und Interessen entspricht.

Einerseits kann der Einstieg ins reguläre Berufsleben problematisch werden, da viele Autisten die hohen sozialen Anforderungen der heutigen Arbeitswelt nicht erfüllen können. Verständnisvolle Vorgesetzte und Kollegen sind für Menschen mit Autismus unerlässlich. Wichtig sind außerdem geregelte Arbeitsabläufe und überschaubare Sozialkontakte.

Auf der anderen Seite sind Menschen mit Autismussyndrom und den damit unter Umständen verbundenen Teilleistungsstärken („Inselbegabung“) teilweise gerade besonders gut für bestimmte Berufe bzw. Tätigkeiten geeignet, z. B. in der Informatik usw. Teilweise existieren besondere Vermittlungsagenturen: In einem ökologischen Weltbild geht es darum, dass sehr unterschiedliche Menschen, die in einem „Ökosystem“ zusammenleben (im Falle des Menschen einem sozio-ökonomischen Ökosystem) passende Nischen finden, in denen sie gut zurechtkommen. Eine autismusgerechte Umwelt zu finden bzw. herzustellen, beispielsweise spezialisierte Schulen, ist daher ein wesentliches Ziel. In den USA gibt es Zentren, die erwachsenen Autisten Arbeitsplätze vermitteln, gegebenenfalls in Kombination mit betreutem Wohnen. Die dänische Zeitarbeitsfirma Specialisterne demonstriert den Erfolg, Autisten in autismusgerechte Umfelder zu vermitteln. Der richtige Arbeitsplatz für Autisten, der besondere Eigenarten der Autisten berücksichtigt, kann schwieriger zu finden, aber oft auch sehr erfüllend sein. Spezialisierte Berufsberatungen für das Autismusspektrum gibt es kaum, da für Autisten in Deutschland die Integrationsämter zuständig sind. Die dänische Firma plant, auch in anderen Ländern etwa in Datenbankführung oder Programmierung zu vermitteln – Berufsfelder u. a., in denen oft speziell begabte Menschen mit Autismus sogar besser als andere sein können. Derart lässt sich etwa ein phänomenales Zahlengedächtnis einsetzen – stets ohne geräuschvolles Großraumbüro und mit mäßiger Arbeitszeit usw. Ende 2011 wurde in Berlin die Firma Auticon gegründet. Sie hat sich darauf spezialisiert, die oft enormen Begabungen von Menschen mit Asperger-Autismus zum Beispiel in der Qualitätskontrolle von Software zu nutzen. Die israelischen Streitkräfte setzen Autisten bevorzugt bei der Auswertung von Luftfotos ein.

Der britische Psychologe Attwood schreibt über die Diagnose von leicht autistischen Erwachsenen, dass diese teilweise gut zurechtkommen, wenn sie etwa einen passenden Arbeitsplatz gefunden haben, aber im Fall von Krisen – etwa durch Erwerbslosigkeit – von ihrem Wissen über das Asperger-Syndrom zur Bewältigung von Krisen profitieren können.

Ausgehend von der individuellen Entwicklung wird ein Plan aufgestellt, in dem die Art der Behandlung einzelner Symptome festgelegt und aufeinander abgestimmt wird. Dem Übereinkommen über die Rechte von Menschen mit Behinderungen (UN-Behindertenrechtskonvention) entsprechend sollte eine passende Umgebung geschaffen werden, in der alle Beteiligten lernen, wie sie die Eigenarten des Kindes am besten berücksichtigen können. Bei Kindern wird das gesamte Umfeld (Eltern, Familien, Kindergarten, Schule) in den Behandlungsplan einbezogen. Angebote für Erwachsene sind vielerorts erst im Aufbau begriffen.

Einen Überblick über Anwendungen, Therapien und Interventionen hat die englische "National Autistic Society" veröffentlicht. Eine Auswahl von Behandlungsmethoden soll im Folgenden kurz vorgestellt werden.

Zur Behandlung bei Erwachsenen liegt eine umfassende Übersicht von 2013 durch die Freiburger Autismus-Studiengruppe vor. Die systematische Auswertung von Behandlungsversuchen bei älteren Jugendlichen und Erwachsenen ist (Stand 2017) – im Gegensatz zur Situation bei Kindern – noch unbefriedigend, was auf die historisch spätere Aufmerksamkeit bei der Erfassung dieser Altersgruppen zurückgeführt wurde.

Die Verhaltenstherapie ist in der Autismustherapie die am besten wissenschaftlich abgesicherte Therapieform. Zu den Wirkfaktoren der Verhaltenstherapie bei Autimus liegt eine umfassende Studie von 2014 vor. Ziel ist es, einerseits störende und unangemessene Verhaltensweisen wie übermäßige Stereotypien oder (auto)aggressives Verhalten abzubauen und andererseits soziale und kommunikative Fertigkeiten aufzubauen. Im Prinzip wird dabei so vorgegangen, dass erwünschtes Verhalten durchgängig und erkennbar belohnt wird (positive Verstärkung). Verhaltenstherapien können entweder ganzheitlich oder auf einzelne Symptome ausgerichtet sein.

Die "Angewandte Verhaltensanalyse" (Applied Behavior Analysis, ABA) ist eine ganzheitlich ausgerichtete Therapieform, die in den 1960er Jahren von Ivar Lovaas entwickelt wurde. Diese Therapieform ist auf die Frühförderung ausgerichtet. Zunächst wird anhand einer Systematik festgestellt, welche Fähigkeiten und Funktionen das Kind bereits besitzt und welche nicht. Hierauf aufbauend werden spezielle Programme erstellt, die das Kind befähigen sollen, die fehlenden Funktionen zu erlernen. Die Eltern werden in die Therapie einbezogen. Die Verfahrensweisen von ABA basieren im Wesentlichen auf Methoden des operanten Konditionierens. Hauptbestandteile sind Belohnung bei richtigem Verhalten und Löschung bei falschem Verhalten. Lernversuche und -erfolge sowie erwünschtes Verhalten werden möglichst direkt verstärkt, wobei primäre (angeborene) Verstärker (wie Nahrungsmittel) und sekundäre (erlernte) Verstärker (wie Spielzeug) eingesetzt werden, um erwünschtes Verhalten zu belohnen. In den 1980er Jahren wurde ABA durch Jack Michael, Mark Sundberg und James Partington weiterentwickelt, indem auch die Vermittlung sprachlicher Fähigkeiten einbezogen wurde. Es gibt zurzeit in der Bundesrepublik Deutschland nur zwei Institute, die diese Therapie anbieten.

Ein weiteres ganzheitlich orientiertes pädagogisches Förderkonzept ist "TEACCH" (Treatment and Education of Autistic and related Communication-handicapped Children), das sich sowohl an Kinder als auch an Erwachsene mit Autismus richtet. TEACCH ist darauf ausgerichtet, die Lebensqualität von Menschen mit Autismus zu verbessern und sie anzuleiten, sich im Alltag zurechtzufinden. Zentrale Annahmen des Konzeptes sind, dass Lernprozesse durch Strukturierung und Visualisierung bei Menschen mit autistischen Merkmalen eingeleitet werden können.

Eltern autistischer Kinder erleben nachweislich signifikant mehr Stress als Eltern von Kindern mit anderen Abweichungen oder Behinderungen. Eine Reduzierung des Stresses der Eltern zeigt deutliche Besserungen im Verhalten ihrer autistischen Kinder. Es gibt starke Hinweise für einen Zusammenhang zwischen der Stressbelastung der Eltern und den Verhaltensproblemen ihrer Kinder, unabhängig von der Schwere des Autismus. Verhaltensprobleme der Kinder zeigen sich nicht vor, sondern auch während erhöhter Stressbelastung der Eltern. Die National Autistic Society hat das „NAS EarlyBird“ Programm entwickelt, ein dreimonatiges Trainingsprogramm für Eltern, um sie auf das Thema Autismus effektiv vorzubereiten.

Die medikamentöse Behandlung von Begleitsymptomen, wie etwa Angst, Depressionen, Aggressivität oder Zwänge mit Antidepressiva (etwa SSRI), atypischen Neuroleptika oder Benzodiazepinen kann eine Komponente im Gesamtbehandlungsplan sein. Sie bedarf jedoch besonderer Vorsicht und aufmerksamer Beobachtung, denn nicht selten verschlimmern sie bei falscher Anwendung die Symptome, statt sie zu mildern.

Mit besonderer Vorsicht ist bei der Gabe von Stimulanzien, wie sie bei Hyperaktivität (ADHS) verschrieben werden, vorzugehen, da sie bei vorhandenem Autismus und der hier häufig vorkommenden Überempfindlichkeit auf Reize der Sinnesorgane letztere noch verstärken können. Die Wirksamkeit von Methylphenidat (Ritalin) ist bei Autisten reduziert (ca. 50 statt 75 Prozent der Patienten), 10 mal häufiger seien unerwünschte Nebenwirkungen wie z. B. Reizbarkeit oder Schlafstörungen. Zu beachten ist ferner, dass Reizüberempfindlichkeiten unabhängig von Autismus auftreten können. Ein konkreter Bezug zu Autismus wird bei einzelnen Stimulanzien nicht im Beipackzettel genannt.

Mögliche ergänzende Methoden sind etwa Musik-, Kunst-, Massagetherapie, ebenso wie Reit- und Delfintherapie oder der Einsatz von Therapierobotern (Keepon) oder Echolokationslauten (Dolphin Space). Sie können die Lebensqualität steigern, indem sie positiv auf Stimmung, Ausgeglichenheit und Kontaktfähigkeit einwirken.
Das zeigt 2008 etwa ein umfassender wissenschaftsjournalistischer Bericht über zwei eigene autistische Kinder – mit Hund.

Weitere bekannte Maßnahmen sind Festhaltetherapie, Gestützte Kommunikation und Daily-Life-Therapie. Diese Maßnahmen „sind im Kontext der Behandlung des Autismus entweder äußerst umstritten und unglaubwürdig oder deren Annahmen und Versprechungen wurden durch wissenschaftliche Untersuchungen im Wesentlichen widerlegt“.

Des Weiteren gibt es verschiedene „biologisch begründete“ Therapiemethoden – etwa die Behandlung mit dem Darmhormon Sekretin –, unter Verwendung hoher Dosen von Vitaminen und Mineralien oder mit besonderen Diäten. Auch hier fehlen bisher Wirksamkeitsnachweise, so dass von diesen Maßnahmen abgeraten wird.

Seit dem Inkrafttreten des Übereinkommens über die Rechte von Menschen mit Behinderungen (UN-Behindertenrechtskonvention) wird zumindest in Deutschland als zusätzliches Angebot für Kinder die Möglichkeit geschaffen, eine passende Umgebung zu gestalten, so dass Behinderung durch Regulierbarkeit der Barrieren vermindert wird. Defizite in der Entwicklung können bei einem förderlichen Umgang mit den Kindern sowie durch eine Umgebung, die Vertrautheit, Ruhe, Überschaubarkeit und Vorhersagbarkeit bietet, teilweise ausgeglichen werden. Ob hierbei zusätzlich Medikamente verordnet werden sollten, wird im Rahmen der Debatte um Neurodiversität kritisch diskutiert und verschieden gehandhabt.

Mögliche Ursachen oder Auslöser von Autismus werden heute auf unterschiedlichen Gebieten erforscht. Die noch bis in die 1960er Jahre vertretene These, Autismus entstehe aufgrund der emotionalen Kälte der Mutter (ehemaliger Terminus der sogenannten „Kühlschrankmutter“), durch lieblose Erziehung, mangelnde Zuwendung oder psychische Traumata, gilt heute als widerlegt.

Genetische Faktoren

Die genetischen Ursachen des Gesamtbereichs des Autismusspektrums haben sich als äußerst vielfältig und hochkomplex erwiesen. In einer Übersicht von 2011 wurden bereits 103 Gene und 44 Genorte (Genloci) als Kandidaten für eine Beteiligung identifiziert, und es wurde vermutet, dass die Zahlen weiter stark steigen würden. Es wird allgemein davon ausgegangen, dass die immensen Kombinationsmöglichkeiten vieler genetischer Abweichungen die Ursache für die große Vielfalt und Breite des Autismusspektrums sind.

Spiegelneuronen

Bis 2013 gab es widersprüchliche Hinweise zu der Hypothese, dass Systeme von Spiegelneuronen bei Menschen mit Autismus möglicherweise nicht hinreichend funktionstüchtig seien. In einer Metaanalyse von 2013 wurde dann festgestellt, dass es wenig gebe, das die Hypothese stütze und dass das Datenmaterial eher mit der Annahme vereinbar sei, dass die absteigende (Top-down-)Modulierung sozialer Reaktionen bei Autismus atypisch sei.

Abweichungen im Verdauungstrakt

Obwohl Verdauungsstörungen im Zusammenhang mit ASS oft beschrieben wurden, gibt es bis heute (Stand November 2015) keine zuverlässigen Daten zu einer möglichen Korrelation oder gar einem möglichen ursächlichen Zusammenhang – weder in die eine noch die andere Richtung.

Vermännlichung des Gehirns

Die Theorie, dass eine Vermännlichung des Gehirns (Extreme Male Brain Theory) durch einen hohen Testosteronspiegel im Mutterleib ein Risikofaktor für ASS sein könnte, wurde in neueren Studien gezielt untersucht, konnte jedoch nicht bestätigt werden.

Atypische Konnektivität

2004 entdeckte eine Forschergruppe um Marcel Just und Nancy Minshew an der Carnegie Mellon University in Pittsburgh (USA) die Erscheinung der veränderten Konnektivität (großräumiger Informationsfluss, engl. "connectivity") im Gehirn bei den Gruppendaten von 17 Probanden aus dem Asperger-Bereich des Autismusspektrums. Funktionelle Gehirnscans (fMRI) zeigten im Vergleich zur Kontrollgruppe sowohl Bereiche erhöhter als auch Bereiche verminderter Aktivität, sowie eine verminderte Synchronisation der Aktivitätsmuster verschiedener Gehirnbereiche. Auf der Grundlage dieser Ergebnisse entwickelten die Autoren erstmals die Theorie der "Unterkonnektivität" "(underconnectivity)" für die Erklärung des Autismusspektrums.

Die Ergebnisse wurden relativ schnell in weiteren Studien bestätigt, ausgebaut und präzisiert, und das Konzept der "Unterkonnektivität" wurde entsprechend fortentwickelt. Bezüglich anderer Theorien wurde es nicht als Gegenmodell, sondern als übergreifendes Generalmodel präsentiert. In den folgenden Jahren nahm die Anzahl der Studien zur "Konnektivität" bei Autismusspektrum explosionsartig zu.

Dabei wurde neben eher globaler "Unterkonnektivität" häufig auch eher lokale "Überkonnektivität" gefunden. Letztere wird allerdings – gestützt auf Kenntnisse der frühkindlichen Gehirnentwicklung bei Autismus – eher als Überspezialisierung und nicht als Steigerung der Effektivität verstanden. Um beide Erscheinungen zu berücksichtigen, wird das Konzept nun "atypische Konnektivität" genannt. Es zeichnet sich ab (Stand: Juli 2015), dass es sich als Konsensmodell in der Forschung etabliert. Dies gilt auch, wenn der Asperger-Bereich des Autismusspektrums für sich betrachtet wird. Die beim Autismusspektrum vorliegende "atypische Konnektivität" wird verstanden als Ursache des hier beobachteten besonderen Verhaltens, wie etwa bei der Erfassung von Zusammenhängen zwischen Dingen, Personen, Gefühlen und Situationen.

Umwelt- und mögliche kombinierte Faktoren

Während die Hypothese, dass ein Zusatz von Thiomersal in Impfstoffen das Risiko von ASS erhöhen könnte, als vielfach widerlegt gilt (siehe nachfolgender Abschnitt), ist der mögliche Einfluss anderweitiger – umweltbedingter – erhöhter Aufnahme von Quecksilber auf das ASS-Risiko aufgrund widersprüchlicher Untersuchungsergebnisse noch umstritten. Eine Studie der Swinburne University im "Journal of Toxicology and Environmental Health" aus dem Jahr 2011, die auf einer Umfrage unter den Enkelkindern der Überlebenden der „Rosa-Krankheit“ basiert (Infantile Akrodynie, eine wahrscheinlich durch Quecksilberintoxikation verursachte Stammhirnenzephalopathie mit Haut- und multiplen Organsymptomen bei Kleinkindern), legt nahe, dass tatsächlich eine Kombination aus genetischen und umweltbedingten Faktoren bei der Entstehung autistischer Symptome eine Rolle spielen kann, allerdings nur bei Kindern mit einer (angeborenen) Präposition für Quecksilber-Überempfindlichkeit. Die Studie verweist allerdings darauf, dass sich die Autismusdiagnosen in dieser Studie nicht bestätigt hätten. Für die weitere Erforschung eines möglichen Zusammenhangs zwischen Autismus und Quecksilbervergiftung in vergleichbar hohen Mengen sei zunächst die weitere Erforschung der „Rosa-Krankheit“ erforderlich. In den 50er Jahren wurde Quecksilber in wesentlichen Mengen gegen Kinderkrankheiten verabreicht, diese Form der Akrodynie ist seit damals praktisch verschwunden. In der Studie wurden offenbar weder die Betroffenen selbst untersucht noch eine Übertragbarkeit zwischen der Akrodynie und anderen Quecksilberbelastungen aufgezeigt.

Psychodynamische Theoriebildung geht davon aus, dass an der Entstehung autistischer Störungen Projektive Identifikation als Abwehrmechanismus beteiligt ist. Es wird in dem Modell angenommen, dass in der frühkindlichen Phase der Entwicklung die Hoffnung auf ein containendes Objekt aufgegeben wurde, was verschiedenste Ursachen der Lebensgeschichte haben kann. Durch psychische Einkapselung werden Selbst- und Objektbeziehungen in Folge vermieden. Zum Schutz zieht ein Kind seine Aufmerksamkeit von der Welt ab zugunsten selbsterzeugter Empfindungen, die gut vorhersagbar sind und eine Überforderung vermeiden, die die Gefahr einer Auflösung der Persönlichkeit beinhalten würde. 

Schäden durch falsche Impfung/Impfstoffe

Es taucht immer wieder das Gerücht auf, Autismus könne durch Impfungen etwa gegen Mumps, Masern oder Röteln (MMR) verursacht werden, wobei eine im Impfstoff enthaltene organische Quecksilberverbindung, das Konservierungsmittel Thiomersal (englisch: Thimerosal), als auslösende Substanz verdächtigt wird. Derlei Berichte entbehren jedoch "„jeglicher wissenschaftlicher Grundlage, so unterscheidet sich die Häufigkeit von Autismus nicht bei geimpften und ungeimpften Kindern.“" Durch verschiedene Studien ist der Zusammenhang zwischen Thiomersal enthaltenden Impfstoffen und Autismus mittlerweile widerlegt. Ungeachtet dessen ist heute in der Regel in Impfstoffen kein Thiomersal mehr enthalten. Eine Abnahme der Anzahl der Neuerkrankungen war erwartungsgemäß in Folge nicht zu beobachten – eine weitere Schwächung der „Autismus-durch-Impfung“-Hypothese.

Die Annahmen, dass Autismus eine Folge von Impfschäden sein soll, ging auf eine Veröffentlichung von Andrew Wakefield in der Fachzeitschrift "The Lancet" 1998 zurück. 2004 wurde bekannt, dass Wakefield vor der Veröffentlichung von Anwälten, die Eltern Autismus-betroffener Kinder vertraten, 55.000 £ an Drittmitteln erhalten hatte. Diese suchten Verbindungen zwischen Autismus und der Impfung, um Hersteller des Impfstoffes zu verklagen. Die Gelder waren weder den Mitautoren noch der Zeitschrift bekannt gewesen. Daraufhin traten zehn der dreizehn Autoren des Artikels von diesem zurück. Im Januar 2010 entschied die britische Ärztekammer (General Medical Council), dass Wakefield „unethische Forschungsmethoden“ angewandt hatte und seine Ergebnisse in „unehrlicher“ und „unverantwortlicher“ Weise präsentiert wurden. "The Lancet" zog daraufhin Wakefields Veröffentlichung vollständig zurück. In der Folge wurde im Mai 2010 auch ein Berufsverbot in Großbritannien gegen ihn ausgesprochen.

Die amerikanische "Food and Drug Administration" hat im September 2006 einen Zusammenhang zwischen Autismus und Impfstoffen als unbegründet abgewiesen, zahlreiche wissenschaftliche und medizinische Einrichtungen folgten dieser Einschätzung.

Die Ausprägungen von Autismus umfassen ein breites Spektrum. Viele Menschen mit Autismus wünschen sich keine „Heilung“, da sie Autismus nicht per se als Krankheit, sondern als normalen Teil ihres Selbst betrachten. Viele Erwachsene mit leichterem Autismus haben gelernt, mit ihrer Umwelt zurechtzukommen. Sie wünschen sich statt Pathologisierung oft nur die Toleranz und Akzeptanz durch ihre Mitmenschen. Auch sehen sie Autismus nicht als etwas von ihnen Getrenntes, sondern als integralen Bestandteil ihrer Persönlichkeit.

Die australische Künstlerin und Kanner-Autistin Donna Williams hat in diesem Zusammenhang den Begriff "Auties" eingeführt, der sich entweder speziell auf Menschen mit Kanner-Autismus oder allgemein auf alle Menschen im Autismus-Spektrum bezieht. Von der US-amerikanischen Erziehungswissenschaftlerin und Asperger-Autistin Liane Holliday Willey stammt die Bezeichnung "Aspies" für Menschen mit Asperger-Syndrom. Die Psychologen Tony Attwood und Carol Gray richten in ihrem Essay "Die Entdeckung von „Aspie“" den Blick auf positive Eigenschaften von Menschen mit Asperger-Syndrom. Die Begriffe Auties und Aspies wurden von vielen Selbsthilfeorganisationen von Menschen im Autismusspektrum übernommen.

Um dem Wunsch vieler Autisten nach Akzeptanz durch ihre Mitmenschen Ausdruck zu verleihen, feiern einige seit 2005 jährlich am 18. Juni den "Autistic Pride Day." Das Schlagwort der Autismusrechtsbewegung – „Neurodiversität“ ("neurodiversity") – bringt die Idee zum Ausdruck, dass eine untypische neurologische Entwicklung einem normalen menschlichen Unterschied entspreche, der ebenso Akzeptanz verdiene wie jede andere – physiologische oder sonstige – menschliche Variante.

In der Grundlagenforschung wurde bei der visuellen Wahrnehmung von Autisten ein überscharfer Aufmerksamkeitswinkel festgestellt, der in seiner Schärfe ("sharpness") stark mit der Schwere der autistischen Symptome korrelierte, sowie eine erhöhte Empfindlichkeit für visuelle Feinkontraste.

Klinische Beobachtungen von Uta Frith (2003) verdeutlichten, dass Menschen mit Autismus häufig erhebliche Schwierigkeiten haben, sprachliche Äußerungen in der gegebenen sprachlichen Situation (Kontext) angemessen zu verstehen. Ergebnisse von Melanie Eberhardt und Christoph Michael Müller deuteten darauf hin, dass ein Autismus-Konzept einer am Detail orientierte Verarbeitung von Sprache viele Besonderheiten des Sprachverstehens autistischer Menschen erklären kann.

Aktuelle Ergebnisse der internationalen Autismusforschung werden auf der seit 2007 jährlich stattfindenden "Wissenschaftlichen Tagung Autismus-Spektrum" (WTAS) vorgestellt. Diese Tagung ist mit Gründung der "Wissenschaftlichen Gesellschaft Autismus-Spektrum" (WGAS) 2008 auch deren wesentliches Organ.

Ein besonderes Forschungszentrum im deutschsprachigen Raum ist das Universitäre Zentrum Autismus Spektrum (UZAS) in Freiburg.

Barrierefreiheit

Eine UN-Studie erkennt die kulturelle Eigenart von Autisten, barrierefrei online Gemeinschaften zu bilden, als im Rahmen der Menschenrechte gleichwertig an: „Here, the concept of community should not be necessarily limited to a geographic and physical location: some persons with autism have found that support provided online may be more effective, in certain cases, than support received in person.“

Autisten haben in Deutschland das Recht auf barrierefreie fernschriftliche Kommunikation. Das kann beispielsweise einer Entscheidung des Bundessozialgerichts vom 14. November 2013 entnommen werden, die von der "Enthinderungsselbsthilfe von Autisten für Autisten" erstritten wurde.

Schwerbehinderung

Autisten galten in Deutschland bislang nach den früheren Anhaltspunkten für die ärztliche Gutachtertätigkeit (AHG) im sozialen Entschädigungsrecht und nach dem Schwerbehindertenrecht Teil 2 SGB IX automatisch als Schwerbehinderte mit einem Grad der Behinderung (GdB) zwischen 50 und 100. Außerdem wurde bei autistischen Kindern bis zum 16. Lebensjahr Hilflosigkeit angenommen, ggf. auch darüber hinaus.

Mit Geltung ab 23. Dezember 2010 hat sich dieser Sachverhalt jedoch geändert. Seit dem 5. November 2011 gilt Folgendes:

1) Grad der Behinderung: 

2) Hilflosigkeit: 

Dokumentationen

Kinofilme
Im Folgenden eine Liste von Filmen, die Autismus als zentrales Thema behandeln:

Fernsehserien


Aktuelle Leitlinien
Werke von historischer Bedeutung


Neurobiologie des Autismusspektrums

Einführungs- und Ratgeberliteratur



</doc>
<doc id="429" url="https://de.wikipedia.org/wiki?curid=429" title="Austria">
Austria

Austria ist der latinisierte Landesname Österreichs. Er bezeichnete ursprünglich nur das heutige Niederösterreich, später die gesamte Habsburgermonarchie und in der spanischen Form "Casa de Austria" deren Herrscherdynastie. Er dient in verschiedenen Sprachen als Übersetzung für „Österreich“ und wird auch als Markenbegriff verwendet, um einen Österreichbezug herzustellen.

Die Allegorie der Austria, ein Symbol des österreichischen Staats, wird als Frauengestalt mit einem Mauerkranz im Haar und einem Speer in der Hand dargestellt, die sich auf einen Wappenschild stützt. 

Erstmals erwähnt wird der Name Austria in einer auf Latein verfassten Urkunde König Konrads III. vom 25. Februar 1147, die heute im Stift Klosterneuburg der Augustiner-Chorherren aufbewahrt wird. Darin ist die Rede von Gütern, die von den "Austrie marchionibus", den "Markgrafen Österreichs" ("Marchiones Austriae") verschenkt wurden. 

Die Bezeichnung geht jedoch nicht auf die lateinische, sondern auf die urgermanische Sprache zurück. Das althochdeutsche "*austar-" bedeutet soviel wie „östlich“ oder „im Osten“, und die altisländische Edda nennt den mythischen Zwerg des Ostens "Austri". Eng mit dem Wort "Austria" verwandt sind auch die Namen "Austrasien" und "Austrien" für das Ostfrankenreich bzw. "Ostreich". Auch in der älteren Bezeichnung "Ostarrîchi" ist die Wurzel "ôstar-" erkennbar. Die Ähnlichkeit mit dem lateinischen "Auster" für „Südwind“ und "terra australis" für das „Südland“ Australien ist zufällig.

Seit dem Mittelalter bezeichnete man das Erzherzogtum Österreich als "Austria" und den (Erz-)Herzog von Österreich als "(Archi-)Dux Austriae". Seit dem 15. Jahrhundert wird der erstmals 1326 nachgewiesene Begriff "domus Austriae" für das gesamte Haus Österreich verwendet, dessen spanische Übersetzung "Casa de Austria" im engeren Sinne aber nur für die spanische Linie der Habsburger. Seit dem 18. Jahrhundert ist die "Austria" als Nationalallegorie Österreichs in der bildenden Kunst bekannt.

In Österreich zugelassene Kraftfahrzeuge werden mit dem Buchstaben codice_1 als Länderkennzeichen gekennzeichnet. Nach dem früher üblichen Aufkleber, der bei Auslandsreisen am Wagenheck zu führen war, befindet sich der Buchstabe heute auf dem Euro-Kennzeichen.

In der ISO-3166-Kodierliste findet sich Österreich mit den Kürzeln codice_2 und codice_3; die Top-Level-Domain ist codice_4.

Die Führung des Wortes "Austria" in Unternehmensnamen (Firma) oder anderen Institutionen ist nur mit bundesbehördlicher Genehmigung zulässig.
 Z2 Unternehmensgesetzbuch besagt Dieser Grundsatz wurde in Bezug auf die Namenszusätze "Austria", "austro" – aber auch "Österreich", "österreichisch" und die Namen anderer Gebietskörperschaften, wie "steirisch", "Vienna" – dahingehend ausgelegt, dass sie Analoges gilt für Vereine und andere Verbände.

Staatliche Organisationen und Verbände:

Universitäre Organisationen und nahe Vereine

Unternehmen:

Österreichische Sportvereine:

Produkte aus Österreich erhalten das Merkmal "Made in Austria" („hergestellt in Österreich“), das keinen expliziten rechtlichen Schutz genießt, aber durch gerichtliche Entscheide gesichert ist. Es gibt Bestrebungen, den Begriff durch Made in EU zu ersetzen.

Bei Einhaltung bestimmter Bedingungen wird seit 1946 das österreichische Gütesiegel Austria Gütezeichen vergeben, das vom Wirtschaftsministerium überwacht wird.





</doc>
<doc id="430" url="https://de.wikipedia.org/wiki?curid=430" title="Anatexis">
Anatexis

Als Anatexis (griechisch ανατηξις „Schmelzen“), auch Migmatisierung, bezeichnet man das partielle Aufschmelzen von Gesteinen der Erdkruste infolge von Temperaturerhöhung, Druckentlastung und/oder Fluidzufuhr (z. B. von HO, CO). Anatexis findet in der tieferen Erdkruste statt, zumeist im Laufe von Gebirgsbildungsprozessen, z. B. im Himalaya und den Alpen, aber auch im Thüringer Wald. Die resultierenden Gesteine werden als Migmatite bezeichnet bzw. als migmatisierte Gneise, Schiefer etc.

Das Anfangsstadium der Aufschmelzung wird als "Metatexis" bezeichnet. Die Mobilisation findet hierbei nur an den Korngrenzen statt und betrifft nur einen Teil des Mineralbestands (partielle Schmelze). Im höheren Stadium, der "Diatexis", werden zunehmend auch die dunklen (mafischen) Mineralbestandteile aufgeschmolzen, bis es schließlich zur Bildung von Magma und magmatischen Gesteinen kommt. Die mehr oder weniger aufgeschmolzenen Gesteine ("Metatexite" und "Diatexite") werden als Migmatite (oder "Anatexite") zusammengefasst.

Bei granitischen Gesteinen findet die Anatexis unter fluidgesättigten Bedingungen bei Temperaturen oberhalb von 650 °C statt. Typische Drücke liegen bei 0,5 bis 1 GPa (entspricht dem Druck in 15 bis 35 km Tiefe). Hierbei werden hauptsächlich der Quarz und die Feldspäte aufgeschmolzen. Bei steigenden Temperaturen bilden sich sukzessive granitische, granodioritische und quarzdioritische Schmelzen. Basische Gesteine schmelzen erst bei deutlich höheren Temperaturen. Da Quarzite, Amphibolite und Kalksilikatfelse in der Natur praktisch nie als Anatexite angetroffen werden und selbst in ausgedehnten Migmatitgebieten als sogenannte "Resisters" unverändert erhalten sind, schätzt man die maximale Temperatur, die bei einer regionalen Aufschmelzung erreicht wird, auf etwa 800 °C.



</doc>
<doc id="431" url="https://de.wikipedia.org/wiki?curid=431" title="Axiomensystem">
Axiomensystem

Ein Axiomensystem (auch: Axiomatisches System) ist ein System von "grundlegenden Aussagen", Axiomen, die ohne Beweis angenommen und aus denen alle Sätze (Theoreme) des Systems logisch abgeleitet werden. Die Ableitung erfolgt dabei durch die Regeln eines formalen logischen Kalküls. Eine Theorie besteht aus einem Axiomensystem und all seinen daraus abgeleiteten Theoremen. Mathematische Theorien werden in der Regel als Elementare Sprache (auch: Sprache erster Stufe mit Symbolmenge) im Rahmen der Prädikatenlogik erster Stufe axiomatisiert. 

Ein Axiomensystem als Produkt der Axiomatisierung eines Wissensgebietes dient der präzisen, ökonomischen und übersichtlichen "Darstellung der in ihm geltenden Sätze und der zwischen ihnen bestehenden Folgerungszusammenhänge." Die Axiomatisierung zwingt zugleich zu einer eindeutigen Begrifflichkeit. Elemente eines axiomatischen Systems sind:

Die Gruppentheorie formuliert man als elementare Sprache im Rahmen der Prädikatenlogik 1. Stufe.


Auf dieses Axiomensystem lässt sich die gesamte Gruppentheorie aufbauen; d. h., alle Theoreme der Gruppentheorie lassen sich hiermit ableiten.

Wir bezeichnen im Folgenden wie üblich die Ableitbarkeitsrelation des zugrundegelegten logischen Kalküls (Sequenzenkalkül, Kalkül des natürlichen Schließens) mit formula_9; sei formula_10 die zugehörige Inferenzoperation, die also jeder Menge M von Axiomen die zugehörige "Theorie" formula_11 zuordnet.

Die Inferenzoperation ist ein Hüllenoperator, d. h., es gilt insbesondere formula_12 (Idempotenz des Hüllenoperators).

Deshalb sind Theorien deduktiv abgeschlossen, man kann also nichts Weiteres aus T herleiten, was nicht schon aus M beweisbar wäre. M nennt man auch eine Axiomatisierung von T.

Eine Menge M von Axiomen (und auch die dazugehörende Theorie T) wird "konsistent" (oder "widerspruchsfrei") genannt, falls man aus diesen Axiomen keine Widersprüche ableiten kann. Das bedeutet: Es ist nicht möglich, sowohl einen Satz A als auch seine Negation ¬ A mit den Regeln des Axiomensystems aus M (bzw. T) herzuleiten.

In Worten von Tarski: 

Ein Ausdruck A wird unabhängig von einer Menge M von Axiomen genannt, wenn A nicht aus den Axiomen in M hergeleitet werden kann. Entsprechend ist eine Menge M von Axiomen unabhängig, wenn jedes einzelne der Axiome in M von den restlichen Axiomen unabhängig ist:

formula_13 für alle formula_14.
Prägnant zusammengefasst: „Unabhängig sind die Axiome, wenn keines von ihnen aus den anderen ableitbar ist“.

Eine Menge M von Axiomen wird vollständig (auch negationstreu) genannt, wenn für jeden Satz A der Sprache gilt, dass der Satz A selbst oder seine Negation ¬ A aus den Axiomen in M hergeleitet werden kann. Dazu gleichbedeutend ist, dass jede Erweiterung von M durch einen bisher nicht beweisbaren Satz widersprüchlich wird. Analoges gilt für eine Theorie. Vollständige Theorien zeichnen sich also dadurch aus, dass sie keine widerspruchsfreien Erweiterungen haben.

Vorsicht: die Vollständigkeit einer Theorie oder einer Axiomenmenge ist eine rein syntaktische Eigenschaft und darf nicht mit der semantischen Vollständigkeit verwechselt werden.

Für das Folgende nehmen wir an, dass der zugrundeliegende Kalkül "korrekt" ist; d. h., dass jede syntaktischen Ableitung auch die semantische Folgerung impliziert (dies ist eine Minimalforderung an ein axiomatisches System, die z. B. für den Sequenzenkalkül der Prädikatenlogik 1. Stufe gilt).

Wenn es zu einem Axiomensystem ein Modell besitzt, dann ist M widerspruchsfrei. Denn angenommen, es gäbe einen Ausdruck A mit formula_15 und formula_16. Jedes Modell von M wäre dann sowohl Modell von formula_17 als auch von formula_18 - was nicht sein kann.

Die "Widerspruchsfreiheit" eines Axiomensystems lässt sich also durch Angabe eines einzigen Modells zeigen. So folgt z. B. die Widerspruchsfreiheit der obigen Axiome der Gruppentheorie durch die Angabe der konkreten Menge formula_19 mit formula_20 und der Definition von formula_4 durch die Addition modulo 2 (formula_22). 

Modelle kann man auch verwenden, um die "Unabhängigkeit" der Axiome eines Systems zu zeigen: Man konstruiert zwei Modelle für das Teilsystem, aus dem ein spezielles Axiom A entfernt wurde - ein Modell, in dem A gilt und ein anderes, in dem A nicht gilt.

Zwei Modelle heißen isomorph, wenn es eine eineindeutige Korrespondenz zwischen ihren Elementen gibt, die sowohl Relationen als auch Funktionen erhält. Ein Axiomensystem, für das alle Modelle zueinander isomorph sind, heißt "kategorisch". Ein kategorisches Axiomensystem ist "vollständig". Denn sei das Axiomensystem nicht vollständig; d. h., es gebe einen Ausdruck A, für den weder A noch formula_18 aus dem System herleitbar ist. Dann gibt es sowohl ein Modell für formula_24 als auch eines für formula_25. Diese beiden Modelle, die natürlich auch Modelle für formula_26 sind, sind aber nicht isomorph.

Für die elementare Aussagenlogik, die Prädikatenlogik erster Stufe und verschiedene Modallogiken gibt es axiomatische Systeme, die die genannten Anforderungen erfüllen.

Für die Prädikatenlogiken höherer Stufen lassen sich nur widerspruchsfreie, aber nicht vollständige axiomatische Systeme entwickeln. Das Entscheidungsproblem ist in ihnen nicht lösbar.

Für die Arithmetik gilt der Gödelsche Unvollständigkeitssatz. Dies wird weiter unten diskutiert.

David Hilbert gelang es 1899, die euklidische Geometrie zu axiomatisieren.


Günther Ludwig legte in den 1980er Jahren eine Axiomatisierung der Quantenmechanik vor

Karl Bühler versuchte 1933, eine Axiomatik der Sprachwissenschaft zu entwickeln.

Arnis Vilks schlug 1991 ein Axiomensystem für die neoklassische Wirtschaftstheorie vor.

Die Gödelschen Unvollständigkeitssätze von 1931 sprechen über höchstens rekursiv aufzählbar axiomatisierte Theorien, die in der Logik erster Stufe formuliert sind. Es wird ein vollständiger und korrekter Beweiskalkül vorausgesetzt. Der erste Satz sagt aus: Falls die Axiome der Arithmetik widerspruchsfrei sind, dann ist die Arithmetik unvollständig. Es gibt also mindestens einen Satz formula_27, so dass weder formula_27 noch seine Negation ¬formula_27 in der Arithmetik beweis­bar sind. Des Weiteren lässt sich zeigen, dass jede Erweiterung der Axiome, die rekursiv aufzählbar bleibt, ebenfalls unvollständig ist. Damit ist die Unvollständigkeit der Arithmetik ein systematisches Phänomen und lässt sich nicht durch eine einfache Erweiterung der Axiome beheben. Der zweite Unvollständigkeitssatz sagt aus, dass sich insbesondere die Widerspruchsfreiheit der Arithmetik nicht im axiomatischen System der Arithmetik beweisen lässt.



</doc>
<doc id="432" url="https://de.wikipedia.org/wiki?curid=432" title="Axiom">
Axiom

Ein Axiom (von griechisch ἀξίωμα: „Wertschätzung, Urteil, als wahr angenommener Grundsatz“) ist ein Grundsatz einer Theorie, einer Wissenschaft oder eines axiomatischen Systems, der innerhalb dieses Systems nicht begründet oder deduktiv abgeleitet wird.

Innerhalb einer formalisierbaren Theorie ist eine These ein Satz, der bewiesen werden soll. Ein Axiom ist ein Satz, der nicht in der Theorie bewiesen werden soll, sondern beweislos vorausgesetzt wird. Wenn die gewählten Axiome der Theorie "logisch unabhängig" sind, so kann keines von ihnen aus den anderen hergeleitet werden. Im Rahmen eines formalen Kalküls sind die Axiome dieses Kalküls immer ableitbar. Dabei handelt es sich im formalen oder syntaktischen Sinne um einen Beweis; semantisch betrachtet handelt es sich um einen Zirkelschluss. Ansonsten gilt: „Geht eine Ableitung von den Axiomen eines Kalküls bzw. von wahren Aussagen aus, so spricht man von einem Beweis.“

Axiom wird als Gegenbegriff zu Theorem (im engeren Sinn) verwendet. Theoreme wie Axiome sind Sätze eines formalisierten Kalküls, die durch Ableitungsbeziehungen verbunden sind. Theoreme sind also Sätze, die durch formale Beweisgänge von Axiomen abgeleitet werden. Mitunter werden die Ausdrücke These und Theorem jedoch im weiteren Sinn für alle gültigen Sätze eines formalen Systems verwendet, d. h. als Oberbegriff, der sowohl Axiome als auch Theoreme im ursprünglichen Sinn umfasst.

Axiome können somit als Bedingungen der vollständigen Theorie verstanden werden, insofern diese in einem formalisierten Kalkül ausdrückbar sind. Innerhalb einer interpretierten formalen Sprache können verschiedene Theorien durch die Auswahl der Axiome unterschieden werden. Bei nicht-interpretierten Kalkülen der formalen Logik spricht man statt von Theorien allerdings von "logischen Systemen," die durch Axiome und Schlussregeln vollständig bestimmt sind. Dies relativiert den Begriff der Ableitbarkeit oder Beweisbarkeit: Sie besteht immer nur in Bezug auf ein gegebenes System. Die Axiome und die abgeleiteten Aussagen gehören zur Objektsprache, die Regeln zur Metasprache.

Ein Kalkül ist jedoch nicht notwendigerweise ein "Axiomatischer Kalkül," der also „aus einer Menge von Axiomen und einer möglichst kleinen Menge von Schlussregeln“ besteht. Daneben gibt es auch Beweis-Kalküle und Tableau-Kalküle.

Immanuel Kant bezeichnet Axiome als „synthetische Grundsätze a priori, sofern sie unmittelbar gewiß sind“ und schließt sie durch diese Definition aus dem Bereich der Philosophie aus. Diese nämlich gründe sich auf Begriffe, die als abstrakte Vorstellungsbilder niemals als Gegenstand unmittelbarer Anschauung Evidenz besitzen. Daher grenzt er die diskursiven Grundsätze der Philosophie von den intuitiven der Mathematik ab: Erstere müssten sich „bequemen, ihre Befugniß wegen derselben durch gründliche Deduction zu rechtfertigen“ und erfüllen daher nicht die Kriterien eines a priori.

Der Ausdruck "Axiom" wird in drei Grundbedeutungen verwendet. Er bezeichnet

Der klassische Axiombegriff wird auf die "Elemente" der Geometrie des Euklid und die Analytica posteriora des Aristoteles zurückgeführt. "Axiom" bezeichnet in dieser Auffassung ein unmittelbar einleuchtendes Prinzip bzw. eine Bezugnahme auf ein solches. Ein Axiom in diesem essentialistischen Sinne bedarf aufgrund seiner empirischen Evidenz keines Beweises. Axiome wurden dabei angesehen als unbedingt wahre Sätze über existierende Gegenstände, die diesen Sätzen als objektive Realitäten gegenüberstehen. Diese Bedeutung war bis in das 19. Jahrhundert hinein vorherrschend.

Am Ende des 19. Jahrhunderts erfolgte eine „Abnabelung der Geometrie von der Wirklichkeit“. Die systematische Untersuchung unterschiedlicher Axiomensysteme für unterschiedliche Geometrien (euklidische, hyperbolische, sphärische Geometrie usw.), die unmöglich allesamt die aktuale Welt beschreiben konnten, musste zur Folge haben, dass der Axiombegriff formalistischer verstanden wurde und Axiome insgesamt im Sinne von Definitionen einen konventionellen Charakter erhielten. Als wegweisend erwiesen sich die Schriften David Hilberts zur Axiomatik, der das aus den empirischen Wissenschaften stammende Evidenzpostulat durch die formalen Kriterien von Vollständigkeit und Widerspruchsfreiheit ersetzte. Eine alternative Auffassungsweise bezieht daher ein Axiomensystem nicht einfach hin auf die aktuale Welt, sondern folgt dem Schema: "Wenn" irgendeine Struktur die Axiome erfüllt, "dann" erfüllt sie auch die Ableitungen aus den Axiomen (sog. "Theoreme"). Derartige Auffassungen lassen sich im Implikationismus, Deduktivismus oder eliminativen Strukturalismus verorten.

In axiomatisierten Kalkülen im Sinne der modernen formalen Logik können die klassischen epistemologischen (Evidenz, Gewissheit), ontologischen (Referenz auf ontologisch Grundlegenderes) oder konventionellen (Akzeptanz in einem bestimmten Kontext) Kriterien für die Auszeichnung von Axiomen entfallen. Axiome unterscheiden sich von Theoremen dann nur formal dadurch, dass sie die Grundlage logischer Ableitungen in einem gegebenen Kalkül sind. Als „grundsätzliches“ und „unabhängiges“ Prinzip sind sie innerhalb des Axiomensystems nicht aus anderen Ausgangssätzen ableitbar und somit keinem Beweis zugänglich.

In den empirischen Wissenschaften bezeichnet man als Axiome auch grundlegende Gesetze, die vielfach empirisch bestätigt worden sind. Als Beispiel werden die Newtonschen Axiome der Mechanik genannt.

Auch wissenschaftliche Theorien, insbesondere die Physik, beruhen auf Axiomen. Aus diesen werden Theorien geschlussfolgert, deren Theoreme und Korollare Vorhersagen über den Ausgang von Experimenten treffen. Stehen Aussagen der Theorie im Widerspruch zur experimentellen Beobachtung, werden die Axiome angepasst. Beispielsweise liefern die Newtonschen Axiome nur für „langsame“ und „große“ Systeme gute Vorhersagen und sind durch die Axiome der speziellen Relativitätstheorie und der Quantenmechanik abgelöst bzw. ergänzt worden. Trotzdem verwendet man die Newtonschen Axiome weiter für solche Systeme, da die Folgerungen einfacher sind und für die meisten Anwendungen die Ergebnisse hinreichend genau sind.

Durch Hilbert (1899) wurde ein formaler Axiombegriff herrschend: Ein Axiom ist jede unabgeleitete Aussage. Dies ist eine rein formale Eigenschaft. Die Evidenz oder der ontologische Status eines Axioms spielt keine Rolle und bleibt einer gesondert zu betrachtenden Interpretation überlassen.

Ein "Axiom" ist dann eine grundlegende Aussage, die

Teilweise wird behauptet, in diesem Verständnis seien Axiome völlig willkürlich: Ein Axiom sei „ein unbewiesener und daher unverstandener Satz“, denn ob ein Axiom auf Einsicht beruht und daher „verstehbar“ ist, spielt zunächst keine Rolle. Richtig daran ist, dass ein Axiom – bezogen auf eine Theorie – unbewiesen ist. Das heißt aber nicht, dass ein Axiom unbeweisbar sein muss. Die Eigenschaft, ein Axiom zu sein, ist relativ zu einem formalen System. Was in einer Wissenschaft ein Axiom ist, kann in einer anderen ein Theorem sein.

Ein Axiom ist "unverstanden" nur insofern, als seine Wahrheit formal nicht bewiesen, sondern vorausgesetzt ist. Der moderne Axiombegriff dient dazu, die Axiomeigenschaft von der Evidenzproblematik abzukoppeln, was aber nicht notwendigerweise bedeutet, dass es keine Evidenz gibt. Es ist allerdings ein bestimmendes Merkmal der axiomatischen Methode, dass bei der Deduktion der Theoreme nur auf der Basis formaler Regeln geschlossen wird und nicht von der Deutung der axiomatischen Zeichen Gebrauch gemacht wird.


Die ursprüngliche Formulierung stammt aus der naiven Mengenlehre Georg Cantors und schien lediglich den Zusammenhang zwischen Extension und Intension eines Begriffs klar auszusprechen. Es bedeutete einen großen Schock, als sich herausstellte, dass es in der Axiomatisierung durch Gottlob Frege nicht "widerspruchsfrei" zu den anderen Axiomen hinzugefügt werden konnte, sondern die Russellsche Antinomie hervorrief.

Generell werden in der Mathematik Begriffe wie Gruppe, Ring, Körper, Hilbertraum, Topologischer Raum etc. durch ein System von Axiomen definiert. Manchmal werden dabei Axiome auch Gesetze genannt (z. B. das Assoziativgesetz).

Auch Theorien der empirischen Wissenschaften lassen sich „axiomatisiert“ rekonstruieren. In der Wissenschaftstheorie existieren allerdings unterschiedliche Auffassungen darüber, was es überhaupt heißt, eine „Axiomatisierung einer Theorie“ vorzunehmen. Für unterschiedliche physikalische Theorien wurden Axiomatisierungen vorgeschlagen. Hans Reichenbach widmete sich u. a. in drei Monographien seinem Vorschlag einer Axiomatik der Relativitätstheorie, wobei er insbesondere stark von Hilbert beeinflusst war. Auch Alfred Robb und Constantin Carathéodory legten Axiomatisierungsvorschläge zur speziellen Relativitätstheorie vor. Sowohl für die spezielle wie für die allgemeine Relativitätstheorie existiert inzwischen eine Vielzahl von in der Wissenschaftstheorie und in der Philosophie der Physik diskutierten Axiomatisierungsversuchen. Patrick Suppes und andere haben etwa für die klassische Partikelmechanik in ihrer Newtonschen Formulierung eine vieldiskutierte axiomatische Rekonstruktion im modernen Sinne vorgeschlagen, ebenso legten bereits Georg Hamel, ein Schüler Hilberts, sowie Hans Hermes Axiomatisierungen der klassischen Mechanik vor. Zu den meistbeachteten Vorschlägen einer Axiomatisierung der Quantenmechanik zählt nach wie vor das Unternehmen von Günther Ludwig. Für die Axiomatische Quantenfeldtheorie war v. a. die Formulierung von Arthur Wightman aus den 1950er Jahren wichtig. Im Bereich der Kosmologie war für Ansätze einer Axiomatisierung u. a. Edward Arthur Milne besonders einflussreich. Für die klassische Thermodynamik existieren Axiomatisierungsvorschläge u. a. von Giles, Boyling, Jauch, Lieb und Yngvason. Für alle physikalischen Theorien, die mit Wahrscheinlichkeiten operieren, insbes. die Statistische Mechanik, wurde die Axiomatisierung der Wahrscheinlichkeitsrechnung durch Kolmogorow wichtig.

Die Axiome einer physikalischen Theorie sind weder formal beweisbar noch, so die inzwischen übliche Sichtweise, direkt und insgesamt durch Beobachtungen verifizierbar oder falsifizierbar. Einer insbesondere im wissenschaftstheoretischen Strukturalismus verbreiteten Sichtweise von Theorien und ihrem Verhältnis zu Experimenten und resultierenden Redeweise zufolge betreffen Prüfungen einer bestimmten Theorie an der Realität vielmehr üblicherweise Aussagen der Form „dieses System ist eine klassische Partikelmechanik“. Gelingt ein entsprechender Theorietest, wurden z. B. korrekte Prognosen von Messwerten angegeben, kann diese Überprüfung ggf. als "Bestätigung" dafür gelten, dass ein entsprechendes System zutreffenderweise unter die intendierten Anwendungen der entsprechenden Theorie gezählt wurde, bei wiederholten Fehlschlägen kann und sollte die Menge der intendierten Anwendungen um entsprechende Typen von Systemen reduziert werden.




</doc>
<doc id="435" url="https://de.wikipedia.org/wiki?curid=435" title="Akrostichon">
Akrostichon

Ein Akrostichon (von und στίχος "stíchos" ‚Vers‘, ‚Zeile‘) ist eine Form (meist Versform), bei der die Anfänge von Wort- oder Versfolgen (Buchstaben bei Wortfolgen oder Wörter bei Versfolgen) hintereinander gelesen einen Sinn, beispielsweise einen Namen oder einen Satz, ergeben. Die deutsche Bezeichnung für diese Versform ist Leistenvers oder Leistengedicht.

Akrosticha gehören sowohl zur Kategorie der Steganographie als auch zu den rhetorischen Figuren. Sie sind abzugrenzen gegen reine Abkürzungen beziehungsweise Aneinanderreihungen von Wörtern, also beispielsweise Akronyme wie INRI.

In der jüdischen Literatur sind Akrosticha weit verbreitet, angefangen mit der hebräischen Bibel. In einigen Psalmen folgen die jeweils ersten Buchstaben von 22 Versen der Reihe der 22 Buchstaben des hebräischen Alphabetes (Psalm 9, 10, 25, 34, 37, 111, 112, 119 und 145). Die ersten vier Wörter des Psalms 96, 11 () enthalten ein Akrostichon des Namens Gottes, JHWH. In späterer rabbinischer Literatur deuten die Anfangsbuchstaben von Werken oder Liedstrophen jeweils auf den Verfasser hin. Dies ist zum Beispiel der Fall bei der Sabbathymne Lecha Dodi, bei der die Anfangsbuchstaben der ersten acht Strophen den Namen "Schlomo ha-Levi" ergeben und auf den Autor Schlomo Alkabez hinweisen.

Das Akrostichon war in antiker, mittelalterlicher und barocker Dichtung beliebt, so zum Beispiel bei Otfrid von Weißenburg (um 800–870) oder Martin Opitz (1597–1639). Paul Gerhardts Lied "Befiehl du deine Wege" ist ein Akrostichon aus Psalm 37,5. In dem Werk "diu crône" von Heinrich von dem Türlin findet sich der Name des Dichters als Akrostichon. Ein Beispiel aus moderner Zeit ist „Lust = Leben unter Strom“ von Elfriede Hablé (* 1934). Joachim Ringelnatz (1883–1934) beteiligte sich unter dem Namen "Erwin Christian Stolze" mit einem Akrostichon an der Ausschreibung zu einer olympischen Hymne. Die Anfangsbuchstaben ergaben seinen vollständigen Namen.

Akrosticha begegnet man auch als Eselsbrücken für wissenschaftliche oder alltägliche Zusammenhänge. Ein Sonderfall ist der Abecedarius, bei dem die Anfangsbuchstaben das Alphabet bilden.

Ein Gedicht, bei dem die Endbuchstaben ein Wort oder einen Satz ergeben, ist ein Telestichon, trifft das für die mittleren Buchstaben zu, handelt es sich um ein Mesostichon. Ein Akroteleuton ist ein mehrfaches Akrostichon oder eine Kombination aus Akrostichon und Telestichon.




</doc>
<doc id="437" url="https://de.wikipedia.org/wiki?curid=437" title="Amtliches Verzeichnis der Ortsnetzkennzahlen">
Amtliches Verzeichnis der Ortsnetzkennzahlen

Amtliches Verzeichnis der Ortsnetzkennzahlen (Abkürzung: "„AVON“") war das Verzeichnis der Telefonvorwahlnummern in Deutschland. Herausgegeben und bearbeitet wurde es vom Fernmeldetechnischen Zentralamt in Darmstadt im Auftrage der Deutschen Bundespost und später der Deutschen Telekom.

Das AVON erschien nur nach Bedarf als Beilage zum Telefonbuch. Es enthielt neben allen deutschen Vorwahlen eine Auswahl an internationalen Vorwahlnummern. Für einige dieser Länder gab es ausführliche kostenlose Vorwahlnummern-Verzeichnisse, welche beim Fernmeldeamt Gießen bestellt werden konnten.

Die von jedem Ortsnetz im Selbstwählferndienst (SWFD) zugelassenen Ortsnetze waren Ende der 1960er Jahre in einem „Amtlichen Verzeichnis der Ortsnetzkennzahlen für den Selbstwählferndienst“ kurz: "AVON" enthalten, das dem amtlichen Fernsprechbuch bei der Ausgabe beigegeben oder den Fernsprechteilnehmern besonders übersandt wurde. Aus diesem Verzeichnis konnten die zum SWFD zugelassenen Ortsnetze der Bundesrepublik und eine Auswahl der zum SWFD zugelassenen Orte des Auslands mit den dazugehörigen Ortsnetzkennzahlen sowie Länderkennzahlen und die jeweilige Sprechdauer für eine Gebühreneinheit entnommen werden. Anfang der 1970er Jahre war auf der Vorderseite des Umschlags die Namen der Ortsnetze angegeben, für die das AVON gültig war, in der Regel handelte es sich dabei jeweils um die Ortsnetze des Bereiches einer Knotenvermittlungsstelle. Später enthielt das AVON alle Ortsnetze der Bundesrepublik.

Bis in die 1990er Jahre enthielt das AVON auf der letzten Umschlagseite "„Wichtige Hinweise zur Vorsorge und Eigenhilfe des Bürgers – zum Selbstschutz“" des Bundesverband für den Selbstschutz. Das erste Gesamtdeutsche AVON nach der Wiedervereinigung erschien anlässlich der Neunummerierung der neuen Bundesländer zum 1. Juni 1992.

Das Verzeichnis hieß zuletzt einfach "„Das Vorwahlverzeichnis“" und wurde letztmals 2001 in gedruckter Form aufgelegt. Seither erfüllen Internetsuchseiten den Zweck dieses Verzeichnisses.



</doc>
<doc id="438" url="https://de.wikipedia.org/wiki?curid=438" title="Autobahn">
Autobahn

Eine Autobahn ist eine Fernverkehrsstraße, die ausschließlich dem Schnellverkehr und dem Güterfernverkehr mit Kraftfahrzeugen dient.

Autobahnen bestehen im Normalfall aus zwei Richtungsfahrbahnen mit jeweils mehreren Fahrstreifen. Für die Benutzung von Autobahnen wird in vielen Ländern eine Maut (Straßennutzungsgebühr) erhoben. Deutsche Autobahnen sind seit Längerem mit mindestens zwei Fahrstreifen pro Richtungsfahrbahn versehen. In der Regel ist auch ein zusätzlicher Seitenstreifen (auch "Standstreifen" oder "Pannenstreifen" genannt) vorhanden. Bei modernen Autobahnen sind die Fahrbahnen voneinander durch einen Mittelstreifen getrennt, in dem passive Schutzeinrichtungen wie Stahlschutzplanken oder Betonschutzwände errichtet sind. Die Fahrbahnbefestigung erfolgt heutzutage durch Beton- oder Asphaltbelag.

Im Gegensatz zu anderen Straßenkategorien besitzen Autobahnen stets höhenfreie Knotenpunkte. So erfolgt der Übergang von einer Autobahn auf eine andere durch Brücken und Unterführungen (Autobahnkreuz beziehungsweise in Österreich „Knoten“) oder Abzweigungen (Autobahndreieck, in der Schweiz „Verzweigung“); Übergänge ins untergeordnete Straßennetz werden (Autobahn-) Anschlussstellen genannt. Je nach Verlauf der Trasse spricht man in manchen Fällen von Ringautobahnen oder Stadtautobahnen. Tunnel und Brücken im Verlauf der Autobahnen sowie die meisten Straßenbrücken über die Autobahnen sind Teile der Autobahnen.

An den meisten Autobahnen befinden sich Autobahnraststätten und Autobahnparkplätze, um den Ver- und Entsorgungsbedürfnissen der Autobahnnutzer nachzukommen und diesen eine Möglichkeit zu geben, sich zu erholen. Oft befinden sich dort auch Attraktionen und Spielgeräte für Kinder. Diese Anlagen sind Teile der Autobahnen. Auf Fahrstreifen der Autobahnen ist das Halten und Parken sowie das Wenden nicht erlaubt.

Des Weiteren ist auf dem Seitenstreifen beziehungsweise Pannenstreifen und in den Nothaltebuchten das Halten nur in besonderen Fällen (etwa eines technischen Defektes oder auf Anweisung eines ausführenden Staatsorgans) erlaubt, um den nachfolgenden Verkehr nicht zu behindern und sich nicht in Lebensgefahr zu begeben. Die Nutzung der Autobahn ist für Fußgänger und Fahrradfahrer in vielen Ländern verboten.

Die Vielzahl von einzelnen Autobahnen bildet zusammen ein Straßennetz, das sich über Ländergrenzen hinweg erstreckt. Ein besonders dichtes Netz bilden Autobahnen in Europa, Nordamerika und in einigen asiatischen Ländern. Auf Kontinenten wie Afrika, Australien und Südamerika sind nur im Einzugsbereich von Großstädten Autobahnabschnitte zu finden. Eine flächenmäßige Erschließung ist nicht vorhanden. Gründe dafür sind der Mangel an finanziellen Mitteln, das geringe motorisierte Verkehrsaufkommen in ländlichen Gegenden und/oder die geringere Besiedlungsdichte.

In den meisten europäischen Ländern bilden Autobahnen eine eigene Straßenart, in einigen Ländern (zum Beispiel Schweden) werden sie zu den anderen Fernstraßen (beispielsweise Europastraßen) gezählt. Autobahnen sind in allen europäischen Staaten außer Estland, Island, Lettland, Malta, der Moldau, Montenegro und den Zwergstaaten vorhanden.

In Europa werden ständig neue Autobahnen gebaut oder bestehende Autobahnabschnitte erweitert. In den Jahren 1994 bis 2004 wuchs das Autobahnnetz in den neuen EU-Mitgliedstaaten um 1000 km, in den alten Mitgliedstaaten sogar um 12.000 km.<ref name="EEA Briefing 3/2004">Europäische Umweltagentur (Hrsg.): . Kopenhagen 2004</ref>

Mit 38,6 km Autobahn auf 100.000 Einwohner hat Zypern die höchste Autobahndichte in Europa.

Die USA sind von einem für die Größe des Landes sehr dichten Autobahnnetz (sogenannte "Interstate Highways") überzogen, die zum Teil deutlich großzügiger angelegt sind als in Europa. Während Überlandautobahnen weniger ausgelastet sind, leiden Autobahnen in Ballungsräumen unter einem starken Verkehrsaufkommen. Der Straßenquerschnitt wird aus diesem Grund sehr breit ausgebildet (bis zu neun Fahrstreifen je Richtungsfahrbahn) und mit großzügigen Anschlussstellen und Knotenpunkten angelegt.

Das Interstate-Highway-Netz ist weitgehend systematisch angelegt. In Nord-Süd-Richtung verlaufen die Highways mit der Nummerierung 5 (also von der Westküste nach Osten 5, 15, 25 usw. bis 95 an der Ostküste), in Ost-West-Richtung mit der Nummerierung 0 (also vom Süden nach Norden 10, 20, 30 usw. bis 90 im Norden an der kanadischen Grenze). Das Highway-System begann im November 1921 mit dem Bundesgesetz "The Federal Aid Highway Act", das im Juni 1956 erneuert wurde und danach zu einem intensiven Ausbau des Systems führte. Der erste größere Interstate-Highway war "The National Road" ab Maryland in Richtung Westen, durch Gesetz vom März 1806 ins Leben gerufen. Die Bauarbeiten begannen im Juli 1811 in Cumberland (Maryland) und erreichten 1839 Vandalia (Illinois). Das National Highway System (NHS) umfasst heute rund 257.000 km, das sind 1,1 % aller US-Straßen. Zum NHS gehören auch die "Freeways" und "Expressways", die beide meist in "Metropolitan Areas" zu finden sind.

Der Osten Kanadas, darunter vor allem Ontario und Québec, verfügt über ein ähnlich gut ausgebautes Autobahnnetz wie die Vereinigten Staaten. Bis auf den bundesstaatlich verwalteten Trans-Canada-Highway obliegt der übrige Straßenbau den einzelnen Provinzen. Der Trans-Canada-Highway ist außerhalb von Ballungsräumen und dichter besiedelten Gebieten noch nicht durchgängig mehrspurig ausgebaut.

In Mexiko sind die wichtigsten Städte des Landes mit Autobahnen verbunden, wobei der große Teil der Autobahnen mautpflichtig ist. Die kostenpflichtigen Straßen sind in einem ähnlichen Zustand wie in anderen nordamerikanischen Ländern. In Ballungszentren sind die "carreteras" durchgehend mehrspurig, in weniger dicht besiedelten Regionen manchmal zweispurig mit Gegenverkehr.

Japan (japanisch: "kōsokudōro") und Südkorea (koreanisch: "gosokdoro") verfügen über Autobahnnetze mit hohem Ausbaustandard. Die Maximalgeschwindigkeit beträgt in Japan offiziell 100 km/h. Mautabgaben ("ETC – electronic toll control") sind auch für kurze Strecken und Privatbenutzung üblich.
Viele aufstrebende Staaten Ost- und Südostasiens bauen ihre Autobahnnetze mit großer Geschwindigkeit aus.

China () hat durch ein groß angelegtes Ausbauprogramm das deutsche Autobahnnetz im Hinblick auf die Länge überholt.
In Thailand fasste die Regierung aufgrund steigender Zulassungen von Kraftfahrzeugen und einem Bedarf an Hochgeschwindigkeitsstraßen mit begrenztem Zugang im Jahr 1997 einen Kabinettsbeschluss, in dem ein Masterplan für den Bau von Autobahnen festgelegt wurde. Darin wurden einige Abschnitte von Schnellstraßen als Autobahn "Motorway" bezeichnet. Derzeit verfügt das Land über zwei Autobahnen und mehrere Hochstraßen ("Expressways"). Den ersten "Expressway" (dort die Stadtautobahn) gibt es in Bangkok seit 1981. Die Höchstgeschwindigkeit liegt bei 120 km/h.

Malaysia verfügt über etwa 1200 Kilometer Autobahn. So verbindet der North-South Expressway etwa den Nordzipfel von Malaysia an der Grenze zu Thailand mit Johor Bahru an der Grenze zu Singapur im Süden. Die Autobahnen sind größtenteils mautpflichtig, die Gebühr wird dabei direkt an Ort und Stelle in Mautstationen kassiert.

Ähnlich dem Europastraßennetz werden auch im asiatischen Raum einzelstaatliche Autobahnen und autobahnähnlichen Straßen zu einem Gesamtnetz verbunden. Das Vorhaben läuft unter dem Namen Asiatisches Fernstraßen-Projekt.

→ "Hauptartikel: Autobahn (Deutschland) und Reichsautobahn"

Die erste autobahnähnliche Strecke der Welt war die AVUS in Berlin. Sie wurde privat finanziert und 1921 eröffnet. Ihre Benutzung war gebührenpflichtig. Die AVUS diente daher zunächst hauptsächlich als Renn- und Teststrecke und nicht dem öffentlichen Verkehr.

Der Begriff „Autobahn“ wurde erstmals von Robert Otzen verwendet, der 1929 vorschlug, statt des bislang gebräuchlichen und „unhandlichen“ Begriffs „Nur-Autostraße“ in Analogie zur Eisenbahn „Autobahn“ zu verwenden.

Als erste öffentliche Autobahn Deutschlands – damals noch offiziell als „kreuzungsfreie Kraftfahr-Straße“ bezeichnet – gilt die heutige A 555 zwischen Köln und Bonn, die nach dreijähriger Bauzeit am 6. August 1932 durch den damaligen Kölner Oberbürgermeister Konrad Adenauer eröffnet wurde. Die Strecke war bereits höhenfrei und je Fahrtrichtung zweispurig, was heute als Mindestmaßstab für die Bezeichnung „Autobahn“ gilt. Baulich getrennte Richtungsfahrbahnen, ein weiteres typisches Charakteristikum von Autobahnen, besaß sie jedoch noch nicht. Daher bekam der Abschnitt erst 1958 nach weiterem Ausbau den offiziellen Status einer Autobahn.

Auch der Bau der Autobahn Köln–Düsseldorf (heute Teil der A 3) wurde bereits 1929 durch den Provinzialverband der preußischen Rheinprovinz rechtlich fixiert. Ein 2,5 km langer Abschnitt bei Opladen wurde 1931 begonnen und 1933 fertiggestellt. Der erste Plan zum Bau einer wirklich großen und bedeutenden Autobahn war der HaFraBa-Plan (= Hansestädte–Frankfurt–Basel), der in etwa dem Verlauf der heutigen Autobahn A 5 und dem nördlichen Teil der A 7 entsprach. Dieser wurde noch vor der Machtergreifung der Nationalsozialisten erstellt. Tatsächlich gebaut wurde dann nach den HaFraBa-Plänen ab 1933 die Strecke Frankfurt–Darmstadt–Mannheim–Heidelberg. Mit der HaFraBa verbunden ist die Entstehung des Wortes „Autobahn“, das 1932 in der gleichnamigen Fachzeitschrift geprägt wurde. Die Bezeichnung „Nur-Autostraße“ habe man verworfen, schrieb die HaFraBa, um dem Umstand Ausdruck zu verleihen, man wolle eine „Autobahn“ in Analogie zur Eisenbahn errichten. Der Mythos, der Begriff „Autobahn“ und die grundsätzliche Idee hierfür gehe direkt auf Hitler zurück, ist eine Geschichtsfälschung. Unterlagen aus dem Bundesarchiv belegen, wie der damalige "Generalinspektor für das Deutsche Straßenwesen", Fritz Todt, 1934 eine derartige geistige Urheberschaft in das Jahr 1923 zurückdatiert und entsprechend schlussfolgerte: „Die Reichsautobahnen, wie wir sie jetzt bauen, haben nicht als von der ‚HAFRABA‘ vorbereitet zu gelten, sondern einzig und allein als ‚Die Straßen Adolf Hitlers‘.“

Der Autobahnbau begann in Österreich mit dem Anschluss an das Deutsche Reich 1938. Noch im selben Jahr wurden erste Teilstücke der West Autobahn (A 1) bei Salzburg fertiggestellt. Im Zuge des Zweiten Weltkrieges kam der Ausbau des Autobahnnetzes bei einer Gesamtlänge von 16,8 km allerdings zum Erliegen und wurde erst nach dem Wiedererlangen der Souveränität Österreichs mit dem Österreichischen Staatsvertrag von 1955 wieder aufgenommen.

Die erste Autobahn der Schweiz war die "Ausfallstraße Luzern-Süd", welche den Verkehr von Luzern an Horw vorbeileitete. Sie wurde am 11. Juni 1955 eröffnet und ist heute Teil der A2. Am 8. März 1960 trat das "Bundesgesetz über die Nationalstrassen" in Kraft, welches die Kompetenzen zur Planung und zum Bau von Strassen mit nationaler Bedeutung dem Bund übertrug und im Artikel 2 die Autobahn als Nationalstrasse erster Klasse beschreibt. Am 21. Juni 1960 folgte der "Bundesbeschluss über das Nationalstrassennetz", in welchem das anzustrebende Autobahnnetz festgelegt ist. Die erste vom Bund finanzierte Strasse war die am 10. Mai 1962 eröffnete acht Kilometer lange Grauholzautobahn, welche der Umfahrung von Zollikofen dient. 1963 wurde das erste längere Teilstück einer Autobahn eröffnet, die A1 zwischen Genf und Lausanne. Im September 1980 wurde der Gotthard-Strassentunnel dem Betrieb übergeben, 1985 das letzte Teilstück der A2 im Tessin zwischen Biasca und Bellinzona.

Der erste Spatenstich zum Bau des Autobahnringes in Bulgarien erfolgte am 4. Oktober 1974. Es sollten drei Autobahnen mit einer Gesamtlänge von knapp 900 km gebaut werden, die Sofia, Warna und Burgas verbinden sollten. Bis zur Wende 1990 wurden etwa 270 km fertiggestellt, in den folgenden 20 Jahren lediglich weitere 116 km. Seit dem EU-Beitritt Bulgariens im Jahr 2007 wächst das Autobahnnetz um etwa 60 km jährlich, sodass Mitte 2014 über 50 % der geplanten Gesamtlänge von 1182 km bereits in Betrieb ist.

Die erste Autobahn Frankreichs, die "Autoroute de Normandie", sollte 1940 fertiggestellt werden. Durch den Zweiten Weltkrieg verzögerte sich allerdings der Bau und die Strecke wurde erst 1946 eröffnet. Die offizielle Bezeichnung "„Autoroute“ (Autostraße)" wurde 1955 eingeführt. Inzwischen hat Frankreich mit derzeit etwa 11.650 Kilometern das fünftlängste Autobahnnetz weltweit und das drittlängste in Europa.

Als erste Autobahn Italiens und gleichzeitig erste für alle zugängliche Autostraße der Welt gilt die Autostrada dei Laghi. Das erste Teilstück zwischen Mailand und Varese wurde 1924 privat von Piero Puricelli errichtet und war gebührenpflichtig. Die Autostraße besaß je Fahrtrichtung einen Fahrstreifen und war noch nicht höhenfrei ausgebildet. Spezielle Ein- und Ausfahrten waren angelegt.

Seit der Gründung der Republik Kroatien erfuhr der Ausbau des Autobahnnetzes in Kroatien zunehmende Bedeutung. Die derzeitige Gesamtlänge aller Autobahnen (kroatisch: Autoceste; singular Autocesta) beträgt 1270,2 km. Zusätzlich befinden sich 185,6 Autobahnkilometer im Bau. Das geplante Autobahnnetz soll 1670,5 km lang werden.

Die Niederlande haben mit 57,5 Kilometern den größten Anteil Autobahnkilometer pro 1000 km² in der Europäischen Union. Insgesamt besitzen die Niederlande ein Autobahnnetz von 2360 km. Die älteste niederländische Autobahn ist die A12 zwischen Den Haag und Utrecht, deren erster Abschnitt 1937 in Betrieb genommen wurde.

In den 1960er Jahren begann Norwegen mit der Planung eines umfassenden Autobahnnetzes, doch der Bau wurde nach nur 45 Streckenkilometern aufgrund der hohen Baukosten gestoppt. Stattdessen beschränkte man sich auf den Bau von Schnellstraßen. Erst zur Jahrtausendwende kam der Autobahnbau wieder in Schwung – vor allem durch die Erweiterung von vorhandenen Schnellstraßen. 2004 umfasste das norwegische Autobahnnetz im Vergleich immer noch geringe 193 km.

Eine Besonderheit ist, dass der Ausbau meist über eine Maut auf dem entsprechenden Abschnitt finanziert wird, nicht jedoch die Instandhaltung („Öffentlich-private Partnerschaft“). Sobald die Investitionskosten abbezahlt sind, entfallen die Mautgebühren.

Nach der Westverschiebung 1945 verfügte Polen lediglich über einige noch unter nationalsozialistischer Planung errichtete Autobahnfragmente in Pommern und Schlesien. Unter der kommunistischen Führung wurden anschließend vorwiegend in den Ballungszentren Polens Schnellstraßen, jedoch keine weiteren Autobahnstrecken errichtet. Erst in den 1980ern begann der ernsthafte Bau eines umfassenden Autobahnnetzes, das allerdings vor allem im Nordosten und Osten des Landes immer noch erhebliche Lücken aufweist. Am 1. Dezember 2012 wies das polnische Autobahnnetz eine Gesamtstreckenlänge von 1342 km auf.

Die schwedischen Autobahnen, in Schweden "Motorvägar" genannt, entstanden erst nach dem Zweiten Weltkrieg. Der erste "Motorväg" mit Autobahnstandard wurde, nach deutschem Vorbild, mit einer Fahrbahn aus Beton zwischen Malmö und Lund angelegt und 1953 von Prinz Bertil von Schweden eingeweiht. Das Teilstück war 17 km lang. 2012 umfasste das schwedische Autobahnnetz ungefähr 1920 km.

Bereits nach dem Zweiten Weltkrieg wurde eine Autostraße durch Jugoslawien gebaut, welche vier der sechs Teilrepubliken (darunter Serbien) passierte und den Namen „Straße der Brüderlichkeit und Einheit“ ("Autoput Bratstvo i jedinstvo") erhielt. Mit dem steigenden Verkehrsaufkommen auf den Straßen der Teilrepubliken wurde die Autostraße zum Autoput mit zwei Richtungsfahrbahnen umgebaut. Seit der Unabhängigkeit Serbiens 2006 befinden sich viele Autobahnen und Schnellstraßen in Planung und Bau. Bereits heute zählt das serbische Autobahnnetz zu einem der am schnellsten expandierenden in Europa.

Die erste Autobahn in Slowenien war die „Avtocesta Vrhnika – Postojna“. Sie wurde am 29. Dezember 1972 eröffnet und ist heute Teil der A1.

Die erste Planung für Autobahnen in Ungarn stammt aus dem Jahr 1942. Im Jahr 1964 wurde die erste Autobahn M7 von Budapest zum Plattensee fertiggestellt. Später begann der Bau nach Tatabánya, nach Győr und in Richtung Wien.
Ungarn hat ein radiales Autobahnnetz mit dem Zentrum Budapest: M1; M2; M3; M4; M5; M6; M7 und ein äußerer Ring: M0.

Der Bau von Autobahnen im Vereinigten Königreich begann 1949 mit dem "Special Roads Act". Darin definierte das britische Parlament „spezielle Straßen“ (später offiziell als „Motorway“ bezeichnet), für die nur bestimmte Fahrzeugtypen zugelassen waren. Die erste dieser „speziellen Straßen“, die heutige M6 bei Preston, wurde am 5. Dezember 1958 eröffnet. Ein Jahr später, am 2. November 1959, gab man die heutige M1 bei London für den Verkehr frei. Da die M6 damals recht kurz war und lange nicht erweitert wurde, wird häufig die M1 als erste Autobahn Großbritanniens bezeichnet.

Der Long Island Motor Parkway (LIMP), auch bekannt als Vanderbilt Parkway, war eine ab 1908 errichtete private Straßenverbindung im Staate New York, die als Mautstraße dem Automobilverkehr vorbehalten war und auch als Rennstrecke diente. Ihre kreuzungsfreie Bauweise und die Verwendung geteilter Richtungsfahrbahnen machen sie zu einem Vorläufer der Autobahnen. Sie wurde 1938 vom Staat New York übernommen und stillgelegt.

Um dem Verkehrsteilnehmer die Orientierung zu erleichtern, erhalten Fernstraßen eine charakteristische Beschilderung, welche durch Farbe und Symbolik die Bedeutung der Straße hervorhebt. Die Farben Blau und Grün sind laut dem Wiener Übereinkommen über Straßenverkehrszeichen von 1968 zugelassen. Das weithin bekannte „Autobahnsymbol“ wird von vielen Ländern verwendet.

Um eine einheitliche Farbgebung anzustreben und weder grüner noch blauer Beschilderung den Vorzug zu geben, kam in den 1990er-Jahren die Idee auf, eine andere Farbgebung zu nutzen; man einigte sich aber auf die rote Farbe. Da die Farbänderung sämtlicher Beschilderungen jedoch exorbitante Kosten verursacht hätte, sind zum einen nur wenige Länder der neuen Farbgebung gefolgt (F/CH/PL), zum anderen wurde ausschließlich die Nummerierungsbeschilderung auf rote Farbe umgestellt, die Hinweis- und Verkehrsschilder jedoch in der bisherigen Farbe belassen. So sind z. B. auf gewissen Strecken in der Schweiz nach wie vor grüne Nummerierungsbeschilderungen zu sehen.

Fast alle Länder haben eine Geschwindigkeitsbegrenzung zwischen 100 und 130 km/h für Autobahnen eingeführt. In Europa ist Deutschland das einzige Land, in dem die Geschwindigkeit auf Autobahnen nicht generell begrenzt ist; es gibt eine Autobahn-Richtgeschwindigkeits-Verordnung, die aber nur empfehlenden Charakter hat. Allgemein ist die Geschwindigkeit den Straßen-, Verkehrs-, Sicht- und Wetterverhältnissen (beispielsweise Schnee oder Nebel) sowie den persönlichen Fahrfähigkeiten und den Eigenschaften von Fahrzeug und Ladung anzupassen.

In den meisten Ländern sind Fahrzeuge, die bauartbedingt eine bestimmte Mindestgeschwindigkeit nicht erreichen können (45 bis 80 km/h, länderabhängig), von der Autobahnbenutzung ausgeschlossen, da sie den Verkehrsfluss unverhältnismäßig behindern und so die Unfallgefahr erhöhen würden.

Das Halten, Wenden, Rückwärtsfahren sowie das Auf- und Abfahren an nicht gekennzeichneten Stellen auf der Autobahn ist generell verboten. Ausnahmen zum Haltverbot bilden die Autobahnparkplätze und Autobahnraststätten. Verkehrsteilnehmer, deren Fahrzeug eine Panne hat, warten am äußersten Fahrbahnrand (beziehungsweise auf dem Standstreifen, falls vorhanden).
Bei Verkehrssituationen, die zu einem Rückstau führen, ist in Deutschland, Österreich und anderen Ländern eine Rettungsgasse zu bilden. Dabei haben die Verkehrsteilnehmer der linken Spur ihre Fahrzeuge ganz an den linken Fahrbahnrand zu lenken. Verkehrsteilnehmer in der rechten Fahrspur haben ihre Fahrzeuge ganz an den rechten Fahrbahnrand zu lenken. Damit bildet sich zwischen den beiden Fahrzeugkolonnen eine für Einsatzfahrzeuge reservierte Fahrspur. Bei mehreren Fahrspuren befindet sich die Rettungsgasse immer rechts von der am weitesten links befindlichen Fahrspur.
In Deutschland gilt die Standspur gemäß Straßenverkehrs-Ordnung (StVO) nicht als Bestandteil der Fahrbahn und darf deshalb nicht regulär befahren werden. Der Seitenstreifen darf hingegen benutzt werden, wenn er durch das Verkehrszeichen 223.1 zum regulären Befahren freigegeben ist oder die Fahrzeuge durch die Polizei dorthin geleitet werden. Ein Fahrzeug mit Wegerecht (Blaulicht, Sirene) kann auch unmittelbar anordnen, freie Bahn zu schaffen.

Autobahnen sind baulich dafür ausgelegt, ein hohes Verkehrsaufkommen auszuhalten. So eignet sich beispielsweise eine vierstreifige Autobahn mit Standstreifen für bis zu 70.000 Fahrzeuge pro Tag. Je nach Anteil des Güterverkehrs kann dieser Wert unterschiedlich ausfallen.
Der höchste Verkehrsfluss mit bis zu 2600 Autos pro Stunde und Fahrstreifen stellt sich – wegen des verringerten Sicherheitsabstandes – bei etwa 85 km/h ein, wenn sich die Geschwindigkeiten der einzelnen Fahrzeuge einander anpassen. Eine noch höhere Belastung führt dann zum Stau. Bei subjektiv „freier Strecke“ können hingegen nur etwa 1800 Autos pro Stunde und Fahrstreifen passieren. Das theoretische Modell für den Zusammenhang zwischen Geschwindigkeit und Durchlassfähigkeit einer Straße liefert das Fundamentaldiagramm des Verkehrsflusses.

Autobahnen haben einen geringen Anteil an den Gesamtunfallzahlen. 2014 fanden 31 % der Fahrleistung in Deutschland auf Autobahnen statt, der Anteil an den Verkehrstoten lag aber nur bei 11 %. Die Getötetenrate lag auf Autobahn mit 1,6 Todesfällen pro Milliarden-Fahrzeugkilometer im Vergleich deutlich niedriger als der Wert von 4,6 im gesamten Verkehr beziehungsweise 6,5 auf außerstädtischen Bundesstraßen.<ref name="http://www.bast.de/ 2012"></ref> Neben dem Risiko, mit einem anderen Fahrzeug zu kollidieren, besteht auch die Gefahr eines Wildunfalls. Die zum Teil schweren Unfälle treten besonders in Waldgebieten auf. Die Straßenverkehrsbehörden errichten daher in gefährdeten Gebieten Wildschutzzäune entlang der Straße, um dem Wild den Zutritt zur Fahrbahn zu verwehren.

Im Regelfall werden in Deutschland und Österreich Autobahnen bei Nacht nur im innerstädtischen Bereich (Stadtautobahn) sowie bei unfallträchtigen Ein- und Ausfahrten beleuchtet. Autobahntunnel werden – wie andere Straßentunnel – immer beleuchtet. In vielen anderen Ländern (beispielsweise Frankreich oder Großbritannien) sind die meisten Autobahnen in Ballungsräumen und alle Autobahnknotenpunkte beleuchtet. Es gibt einige Länder wie Belgien, Luxemburg oder die Vereinigten Arabischen Emirate, wo sogar fast alle Autobahnen nachts beleuchtet werden. Im Zuge der Energieeinsparungen und der aufwändigen Wartung sowie der sogenannten Lichtverschmutzung wird aber auch in einigen dieser Länder die Beleuchtung nach und nach eingestellt.

In vielen Ländern muss man für das Befahren der Autobahn eine Benutzungsgebühr (sogenannte Maut) bezahlen. Dieses kann zeitabhängig in Form einer Vignette geschehen oder streckenabhängig durch Bezahlen an Mautposten, wie in zahlreichen weiteren europäischen Staaten üblich.

In Österreich gilt für LKW seit dem 1. Januar 2004 eine Mautpflicht, die über mitzuführende GO-Boxen und ein entlang der Autobahn installiertes Dezimeterwellen-System abgerechnet wird. Um ein Ausweichen auf Landstraßen zu verhindern, werden häufig Fahrverbote für LKW auf parallel verlaufenden Straßen verhängt. In der Schweiz gilt die LSVA auf allen Straßen, somit existiert das Ausweichen des Schwerverkehrs auf Landstraßen nicht. In Deutschland gilt seit dem 1. Januar 2005 eine LKW-Maut, die entweder an Automaten über dort anzugebende Fahrtrouten im Voraus oder über eingebaute Maut-Geräte satellitengestützt automatisiert erhoben wird. Die Erhebung erfolgt über das Unternehmen Toll Collect.

Die Autobahnen haben auch zu kultureller Auseinandersetzung angeregt. Ein Beispiel ist die Errichtung von Autobahnkirchen.
Beispiele im Bereich der Musik sind das Album "Autobahn" der Musikgruppe Kraftwerk und das Lied "Deutschland Autobahn" des US-amerikanischen Country-Sängers Dave Dudley, dessen Großeltern aus Deutschland stammten. In dem Film The Big Lebowski (1998) taucht ein Trio deutscher Nihilisten auf, das als Gruppe "Autobahn" (ein Kraftwerk-Verschnitt) Ende der 1970er seine erste Techno-LP mit dem Titel "Nagelbett" herausgebracht haben soll. Auch tragen einige Diskotheken, die an oder in der Nähe einer Autobahn liegen, den Namen dieser Autobahn, so zum Beispiel eine Kasseler Diskothek namens "A7" an der A 7. Die deutsche Filmkomödie "Superstau" spielt fast komplett auf einer Autobahn.

Die Baukosten von Autobahnen sind stark von den Gegebenheiten der jeweiligen Strecke abhängig. Einfache Streckenführungen verursachen Kosten von etwa vier bis sechs Millionen Euro pro Autobahnkilometer, bei komplexen Strecken, etwa mit Brücken und Tunneln, liegen die Kosten um ein Vielfaches höher. Zusätzlich zu den reinen Baukosten kommen Kosten für den Planungsprozess, für Gutachten und Beratungsleistungen externer Ingenieure, Genehmigungsverfahren und für begleitende Investitionen, also etwa für Lärmschutzwände, Straßenbegleitgrün und Wechselverkehrszeichen hinzu.

Entsprechend einer Beispielrechnung ergeben sich in Deutschland durchschnittliche Kosten von 26,8 Millionen Euro pro Autobahnkilometer, wovon ein Viertel reine Baukosten und drei Viertel zusätzliche Kosten sind.

Beispielsweise kostet der Neubau der A20 von 1992 bis 2005 etwa 1,9 Milliarden Euro bei einer Länge von 322 km, das entspricht 6 Mio. Euro pro Autobahnkilometer.

Der Neu- oder Ausbau von Autobahnen wird immer wieder durch Natur- und Umweltschützer kritisiert. Umweltschützer sehen den Autobahnbau vor allem als falsches verkehrspolitisches Signal, da der motorisierte Individualverkehr und somit der zusätzliche Ausstoß von klimaschädlichen Abgasen gefördert wird. Außerdem warnen sie vor zusätzlicher Lärmbelastung für die Umgebung der Autobahn.

Naturschützer hingegen bemängeln vor allem den Flächenverbrauch sowie die zunehmende Landschaftszerschneidung, welche durch neue Autobahnen gefördert wird. Zudem wird bemängelt, dass der Arten- wie der Biotopschutz bei der Verkehrsplanung oft eine untergeordnete Rolle spielt.
Aus diesen Gründen versuchen Umweltschutzorganisation und Naturschutzverbände durch Aufklärungsarbeit, aber auch mithilfe des Verbandsklagerechts, den Bau weiterer Autobahnen zu verhindern oder umweltverträglichere Streckenführungen beim Neubau durchzusetzen.

Des Weiteren fördern Autobahnen aktiv die Zersiedelung. Durch die verkürzte Wegstrecke und die Verkürzung der Fahrzeiten wird es z. B. Arbeitnehmern ermöglicht, sich weiter weg von ihren Arbeitsplätzen anzusiedeln. Dies führt wiederum zu mehr Verkehr auf den Autobahnen, was wiederum zu einem Bedarf für neue Straßen oder für den Straßenausbau führt. So sind z. B. in Los Angeles selbst achtstreifige Autobahnen regelmäßig von Staus betroffen.
Ein weiteres Beispiel sind Gewerbegebiete an Autobahnen außerhalb von Städten. Diese sind in der Regel nicht mit anderen Verkehrsmitteln zu erreichen, ein weiterer Anstieg des Individualverkehrs ist die Folge.

Eine Möglichkeit, die negativen Auswirkungen von Autobahnen auf die Umwelt zu verringern, ist das Verfahren der Trassenbündelung. So wird neben einem bestehenden Verkehrsweg (etwa einer Bahnstrecke) eine Autobahn errichtet. Durch dieses Prinzip werden die Flächenzerschneidung und die Neuverlärmung reduziert. Ein weiteres Mittel, mit dem versucht wird die Auswirkungen der Landschaftszerschneidung zu verringern, ist der Bau von Grünbrücken.






</doc>
<doc id="440" url="https://de.wikipedia.org/wiki?curid=440" title="Attac">
Attac

Attac (ursprünglich association pour une taxation des transactions financières pour l'aide aux citoyens"; seit 2009: association pour la taxation des transactions financières et pour l'action citoyenne"; ‚Vereinigung zur Besteuerung von Finanztransaktionen im Interesse der BürgerInnen‘) ist eine globalisierungskritische Nichtregierungsorganisation.

Attac hat weltweit ca. 90.000 Mitglieder und agiert in fünfzig Ländern, hauptsächlich jedoch in Europa.

Attac wurde am 3. Juni 1998 in Frankreich gegründet. Den Anstoß zur Gründung gab ein Leitartikel von Ignacio Ramonet, der im Dezember 1997 in der Zeitung Le Monde diplomatique veröffentlicht wurde und die Gründung einer "Association pour une taxe Tobin pour l'aide aux citoyens" (deutsch: ‚Vereinigung für eine Tobin-Steuer zum Nutzen der Bürger‘) vorschlug.

Seine Idee war, auf weltweiter Ebene eine Nichtregierungsorganisation (NGO) ins Leben zu rufen, die Druck auf Regierungen machen sollte, um eine internationale „Solidaritätssteuer“ zur Kontrolle der Finanzmärkte, genannt Tobin-Steuer, einzuführen. Gemeint war damit die durch den US-amerikanischen Ökonomen James Tobin Ende der 1970er Jahre vorgeschlagene Steuer in Höhe von 0,1 % auf spekulative internationale Devisengeschäfte. Der von Ramonet gleichzeitig vorgeschlagene Name dieser Organisation „attac“ sollte, aufgrund seiner sprachlichen Nähe zum französischen Wort "attaque", zugleich den Übergang zur „Gegenattacke“ signalisieren, nach Jahren der Anpassung an die Globalisierung.

Die Aktivitäten von Attac weiteten sich schnell über den Bereich der Tobinsteuer und die „demokratische Kontrolle der Finanzmärkte“ hinaus aus. Mittlerweile umfasst der Tätigkeitsbereich von Attac auch die Handelspolitik der WTO, die Verschuldung der Dritten Welt und die Privatisierung der staatlichen Sozialversicherungen und öffentlichen Dienste. Die Organisation ist inzwischen in einer Reihe von afrikanischen, europäischen und lateinamerikanischen Ländern präsent.

Im deutschsprachigen Raum hatten 1999 die NGOs Weltwirtschaft, Ökologie und Entwicklung (WEED) und Kairos Europa die Initiative zur Gründung von Attac ergriffen.

Attac Deutschland ist ein Projekt des eingetragenen Vereins "Attac Trägerverein e.V." Der Verein ist beim Amtsgericht Frankfurt am Main unter der Vereinsregisternummer VR 12648 registriert.

In Frankfurt/Main beschlossen am 22. Januar 2000 ca. 100 Teilnehmer der Gründungsversammlung ein „Netzwerk zur demokratischen Kontrolle der internationalen Finanzmärkte“ zu gründen. Dieses soll eng mit der im Jahr 1998 gegründeten französischen Organisation Attac zusammenarbeiten.

Attac Deutschland besteht aus Mitgliedsorganisationen und Einzelmitgliedern (über 29.000, Stand: Dezember 2015) aber auch vielen mitarbeitenden Nicht-Mitgliedern. Attac versteht sich als „Bildungsbewegung“ mit Aktionscharakter und Expertise. Über Vorträge, Publikationen, Podiumsdiskussionen und Pressearbeit sollen die Zusammenhänge der Globalisierungsthematik einer breiten Öffentlichkeit vermittelt und Alternativen zum „neoliberalen Dogma“ aufgezeigt werden. Seit mehreren Jahren begleitet ein wissenschaftlicher Beirat die Arbeit von Attac. Mit Aktionen soll Druck auf Politik und Wirtschaft zur Umsetzung der Alternativen erzeugt werden.

Mit dem Jugendnetzwerk Noya sollen insbesondere junge Menschen für globalisierungskritische Themen angesprochen werden. Daneben existieren etliche Campus-Gruppen, die speziell auf Studenten und Bildungsthemen ausgerichtet sind.

Attac versteht sich als Netzwerk, in dem sowohl Einzelpersonen als auch Organisationen aktiv sein können. In Deutschland gehören circa 200 Organisationen Attac an, darunter ver.di, BUND, Pax Christi, Evangelische StudentInnengemeinde in Deutschland (Bundes-ESG) Deutsche Friedensgesellschaft – Vereinigte KriegsdienstgegnerInnen (DFG-VK), Medico international und viele entwicklungspolitische und kapitalismuskritische Gruppen. Momentan sind von den über 29.000 Mitgliedern viele in den etwa 170 Regionalgruppen oder den bundesweiten Arbeitsgruppen aktiv.

Attac Deutschland ist Mitglied im "Tax Justice Network".

Im Oktober 2014 hat das Finanzamt Frankfurt am Main der Organisation den Status der Gemeinnützigkeit aberkannt und begründete diesen Schritt mit „den allgemeinpolitischen Zielen“. Attac hat Widerspruch gegen die Entscheidung des Finanzamts erhoben. Im November 2016 wurde dem Widerspruch durch das Hessische Finanzgericht stattgegeben und damit die Gemeinnützigkeit gerichtlich festgestellt. Gegen das Urteil ist allerdings das Finanzamt auf Weisung des Bundesfinanzministeriums mit einer Nichtzulassungsbeschwerde vorgegangen, die der Bundesfinanzhof am 13. Dezember 2017 angenommen hat. Damit ist das Urteil bis zu einer Entscheidung im Revisionsverfahren nicht rechtskräftig und Attac ohne anerkannte Gemeinnützigkeit.

Ursprünglich setzte sich Attac vor allem für die Einführung der Tobin-Steuer auf Finanztransaktionen und eine demokratische Kontrolle der internationalen Finanzmärkte ein. Inzwischen hat sich Attac auch anderer Themen der globalisierungskritischen Bewegung angenommen, als deren Teil es sich sieht. Seine Mitglieder nehmen häufig an Aktionen und Demonstrationen teil, die tendenziell dem linken politischen Spektrum zugeordnet werden. Attac kritisiert dabei die neoliberale Globalisierung und versucht u. a. mit Demonstrationen und Bildungs- und Aufklärungsarbeit gegen Armut und Ausbeutung zu kämpfen.

Attac befasst sich in Deutschland vor allem mit folgenden Themen, zu denen es zum Großteil auch gesonderte bundesweite Arbeitszusammenhänge gibt:


Attacs Hauptkritik an den „Kräften der neoliberalen Globalisierung“ (im Sprachverständnis von Attac zu unterscheiden von kultureller, ökologischer, politischer Globalisierung) ist, dass diese das Versprechen eines „Wohlstands für alle“ nicht haben einlösen können. Im Gegenteil: Die Kluft zwischen Arm und Reich werde immer größer, sowohl innerhalb der Gesellschaften als auch zwischen Nord und Süd. Motor dieser Art von Globalisierung seien die internationalen Finanzmärkte. Banker und Finanzmanager setzten täglich Milliardenbeträge auf diesen Finanzmärkten um und nähmen über ihre Anlageentscheidungen immer mehr Einfluss auf die gesellschaftliche Entwicklung. Damit würden die Finanzmärkte letztendlich die Demokratie untergraben. Deshalb plädiert Attac, neben anderen Maßnahmen, für die besagte Besteuerung von Finanztransaktionen, die so genannte Tobin-Steuer. Attac behauptet, neoliberale Entwicklungen seien politisch gewollt, d. h. die Politik sei nicht Opfer, sondern Hauptakteur dieses Prozesses.

Attac tritt für eine „demokratische Kontrolle“ und Regulierung der internationalen Märkte für Kapital, Güter und Dienstleistungen ein. Politik müsse sich an den Leitlinien von Gerechtigkeit, Demokratie und ökologisch verantwortbarer Entwicklung ausrichten. Nur so könne die durch die kapitalistische Wirtschaftsweise entstehende gesellschaftliche Ungleichheit ausgeglichen werden.

Attac möchte nach eigenen Angaben ein breites gesellschaftliches Bündnis als Gegenmacht zu den internationalen Märkten bilden. Die Behauptung, Globalisierung in ihrer jetzt herrschenden Form sei ein alternativloser Sachzwang, wird von Attac als reine Ideologie zurückgewiesen. Stattdessen wird unter Stichworten wie Alternative Weltwirtschaftsordnung, Global Governance, Deglobalisierung, Re-Regionalisierung und Solidarische Ökonomie über Alternativen diskutiert.

Der Begriff „Ökonomische Alphabetisierung“ bezeichnet die Strategie von Attac, eine Vermittlung von ökonomischen Grundkenntnissen an weite Teile der Bevölkerung vorzunehmen. Da immer mehr Bereiche des öffentlichen Lebens den marktwirtschaftlichen Prinzipien unterworfen würden, seien immer öfter ökonomische Grundkenntnisse für eine Partizipation im demokratischen Prozess und für die Meinungsbildung erforderlich.

Attac sagt über sich selbst, Grundsatz sei ein ideologischer Pluralismus. Inhaltlich bestehe allerdings auch ein unüberbrückbarer Gegensatz zum wirtschaftlichen Liberalismus. Attac lehnt Gewalt als Mittel der politischen Auseinandersetzung ab.

Entscheidungen werden bei Attac nicht nach dem Mehrheits-, sondern nach dem Konsensprinzip getroffen. Das heißt, dass Entscheidungen zunächst diskutiert und – falls niemand widerspricht – von allen mitgetragen werden. So können Entscheidungen auch auf vorläufiger Basis getroffen und später erneut diskutiert werden, falls eine Seite dazu anrät. Auf diese Weise kann das Meinungsspektrum der Mitglieder und Mitgliedsorganisationen besser integriert werden und kann sich keine Kultur von Mehrheitsabstimmungen entwickeln, die zum Übergehen von Minderheiten führen würde. Da Attac keine politische Partei ist, die zu jedem Thema einen abrufbaren und einheitlichen Standpunkt bereithalten muss, fallen die Nachteile des Konsensprinzips aus der Sicht der Aktivisten kaum ins Gewicht. Die Mitwirkung bei Attac findet vorwiegend in "Arbeitskreisen (AKs)" oder "Arbeitsgemeinschaften (AGs)" statt, die es sowohl auf regionaler als auch auf nationaler Ebene zu den verschiedenen Themengebieten gibt, sowie in zahlreichen Regionalgruppen.

Meinungen von Attac zu wirtschaftspolitischen Themen werden zum Teil auch gesellschaftlich wahrgenommen, wie die vermehrten Auftritte von Attac-Mitgliedern in den Medien (DeutschlandRadio, Phönix) und bei Politik-Talkshows (z. B. Sven Giegold bei "Sabine Christiansen", "Maybrit Illner" oder Jutta Sundermann als Gast von Bettina Böttinger im "Kölner Treff") zeigten.

Der Attac-Ratschlag ist bei Attac Deutschland das höchste Entscheidungsgremium. Er trifft sich zweimal jährlich, und zwar einmal als "Attac-Basistreffen" mit dem Schwerpunkt auf Erfahrungsaustausch und ein weiteres Mal als Entscheidungsgremium unter anderem mit den jährlichen Wahlen zum Attac-Rat und zum Koordinierungskreis. Beide Treffen sind öffentliche Vollversammlungen.

Der Attac-Ratschlag ist ein bundesweites, öffentliches Treffen aller interessierten Menschen aus den Mitgliedsorganisationen, Ortsgruppen sowie den bundesweiten Arbeitszusammenhängen und aktiver Nichtmitglieder. Entscheidungen werden im Wesentlichen im Konsensverfahren getroffen, Abstimmungen sollen die Ausnahme sein. Für den Fall von Abstimmungen und Wahlen werden von den Mitgliedsorganisationen und Ortsgruppen Delegierte bestimmt.

Auf dem Ratschlag haben alle Anwesenden, egal ob Attac-Mitglieder oder nicht, Rede- und Stimmrecht zu inhaltlichen Fragen. Die Verabschiedung des Haushaltes und die Wahlen der Gremien sind jedoch den Delegierten vorbehalten. Diese Delegierten werden von Attac-Gruppen, Mitgliedsorganisationen und bundesweiten Arbeitszusammenhängen bestimmt, jeweils nach ihren eigenen Verfahren, die nicht zentral geregelt sind.

Jede Attac-Ortsgruppe bestimmt zwei Delegierte. Attac-Gruppen mit mehr als 100 Attac-Mitgliedern bestimmen vier Delegierte. Gruppen mit mehr als 200 Attac-Mitgliedern bestimmen sechs Delegierte. Die bundesweit tätigen Mitgliedsorganisationen bestimmen jeweils zwei Delegierte.
Bundesweite Arbeitsgruppen, Kampagnen, "feminist attac" (früher: Frauennetzwerk), wissenschaftlicher Beirat usw. bestimmen auch jeweils zwei Delegierte. (Beschluss Ratschläge Frankfurt 2002 und Aachen 2003)

Für die Delegationen zum Ratschlag gilt eine Frauenquote. Die Delegierten der Attac-Gruppen sollen so gewählt werden, dass mindestens die Hälfte der Delegierten Frauen sein können, aber maximal die Hälfte Männer. D. h.: bleiben Frauenplätze unbesetzt, sind diese nicht durch Männer auffüllbar, jedoch können leere Männerplätze durch Frauen besetzt werden.



Im 2001 gegründeten wissenschaftlichen Beirat von Attac Deutschland arbeiten ca. 120 Professoren, Wissenschaftler und Experten mit. Sie vertreten ein breites Spektrum unterschiedlicher Fachrichtungen. Engagiert sind Ökonomen, Soziologen, Politologen, Juristen, Psychologen und Fachleute anderer Professionen. Ihnen gemeinsam ist die Absicht, ihre Expertise in den Dienst des globalisierungskritischen Netzwerks Attac Deutschland zu stellen.

Attac Deutschland versteht sich als Netzwerk, dem neben ca. 29.000 Einzelmitgliedern (2015) etwa 200 Organisationen angehören, darunter:

Auswahl aus der 

Attac Österreich ist ein in Wien unter der Nummer ZVR 969464512 eingetragener bundesweiter Verein und wurde am 6. November 2000 gegründet. Die Gründung war von 50 Personen aus allen Gesellschaftsbereichen vorbereitet worden. Zur Auftaktveranstaltung in Wien kamen mehr als 300 Interessierte. Neben den Proponenten saßen die Politikwissenschaftlerin Susan George von Attac Frankreich, Stephan Schulmeister vom Österreichischen Institut für Wirtschaftsforschung (WIFO) und Brigitte Unger, Professorin für Ökonomie an der Wirtschaftsuniversität Wien, auf dem Podium. Der Verein hat über 5400 Einzelmitglieder und mehr als 70 Mitgliedsorganisationen. Prominente Unterstützer sind unter anderem die Schriftsteller Franzobel und Robert Menasse. Zu den wichtigsten regelmäßigen Veranstaltungen zählen eine jährliche Sommerakademie an wechselnden Orten Österreichs sowie seit 2009 eine gemeinsam mit Greenpeace und anderen NGOs veranstaltete Aktionsakademie. Attac Österreich zeichnet auch für die Initiative Wege aus der Krise verantwortlich. Zu den Unterstützern dieser Initiative zählen die Armutskonferenz, die Gewerkschaft der Gemeindebediensteten, Kunst, Medien, Sport und freie Berufe (GdG-KMSfB), Global 2000 Österreich, die Gewerkschaft der Privatangestellten (GPA), Greenpeace Austria, die Katholische ArbeitnehmerInnen Bewegung Österreich (KAB), die Österreichische Hochschülerinnen- und Hochschülerschaft, die Gewerkschaft PRO-GE, SOS Mitmensch und die Gewerkschaft Vida sowie eine Reihe weiterer Organisationen.

Attac Österreich ist ein unabhängiger, in Wien eingetragener bundesweiter Verein. Attac Österreich steht nach eigenen Angaben keiner Partei nahe. Die koordinierende Stelle ist der auf der jährlichen Generalversammlung gewählte Vorstand. Bei der konstituierenden Generalversammlung am 20. Mai 2001 wurde das Prinzip des Gender Mainstreaming in den Statuten Attacs verankert. Der Vorstand besteht laut diesen Statuten zu mindestens 50 Prozent aus Frauen. Der Großteil der Arbeit basiert auf dem Engagement ehrenamtlicher Aktivisten in knapp 30 Regional- und zahlreichen Inhaltsgruppen.

Arbeitnehmerorganisationen

Kirche

Entwicklungspolitik

Arbeitsmarkt, Soziales, Pensionen, Sozialismus

Gemeinden

In der Schweiz wurde Attac bereits 1999 gegründet und besteht aus etwa einem Dutzend Lokalgruppen. Im Gegensatz zur Schreibweise in Deutschland und anderen Ländern schreibt sich ATTAC Schweiz mit Großbuchstaben.

Eine Innenansicht mit Kritik am Organisierungsmodell von Attac und den prägenden Inhalten ist im 2004 veröffentlichten Buch "Mythos Attac" von Jörg Bergstedt zu finden. Parallel zum Buch sind Internetseiten mit gesammelten Kritiken und Zitaten aus der Organisation entstanden.

James Tobin, der „Erfinder“ der Tobin-Steuer, distanzierte sich in einem Interview mit dem deutschen Magazin "Der Spiegel" im Jahr 2001 von Attac und anderen Globalisierungskritikern: „Ich kenne wirklich die Details der Attac-Vorschläge nicht genau. Die jüngsten Proteste sind ziemlich widersprüchlich und uneinheitlich, ich weiß nicht einmal, ob all das Attac widerspiegelt. Im großen Ganzen sind deren Positionen gut gemeint und schlecht durchdacht. Ich will meinen Namen nicht damit assoziiert wissen.“

Attac wurde von verschiedenen Seiten eine Nähe zum Antisemitismus vorgeworfen. In Deutschland dementierte der Attac-Koordinierungskreis im Dezember 2002 diese Vorwürfe in Form eines Diskussionspapieres. Darin heißt es, dass Attac sich als pluralistisches und offenes Bündnis verstehe. Pluralismus würde jedoch nicht als prinzipienlose Beliebigkeit definiert, sondern fände dort seine Grenzen, wo Rassismus, Antisemitismus und Nationalismus ins Spiel kämen. Auch nach dieser Darstellung gab es weitere Kritik in diesem Bereich, zum Beispiel im Hinblick auf ein Plakat, das auf dem Attac-Ratschlag 2003 neben der Bühne stand und das nach Ansicht von Kritikern die Zinsknechtschaft anprangerte oder ein Aufruf der Attac-AG "Globalisierung und Krieg" zum Boykott von Waren aus jüdischen Siedlungen in Palästinensergebieten. Toralf Staud schreibt in der Wochenzeitung Die Zeit, dass, wenn über „das Finanzkapital“ oder „die Wall Street“ geraunt würde, dies das alte Vorurteil vom geldgierigen Juden wachrufe. Etliche Globalisierungskritiker erlägen der Versuchung, für unübersichtliche Entwicklungen Sündenböcke verantwortlich zu machen. Die komplexen Zusammenhänge der Globalisierung reduzierten sie auf ein „Komplott dunkler Mächte“.

In Österreich veranstaltete Attac vom 18. bis 20. Juni 2004 den Kongress "Blinde Flecken der Globalisierungskritik" gegen antisemitische Tendenzen und rechtsextreme Vereinnahmung, unterstützt vom Dokumentationsarchiv des österreichischen Widerstandes. Der Kongress ist in einem Reader dokumentiert.






</doc>
<doc id="441" url="https://de.wikipedia.org/wiki?curid=441" title="Almandin">
Almandin

Almandin, auch als Eisentongranat oder Eisen-Tonerdegranat bezeichnet, ist ein Mineral aus der Gruppe der Granate innerhalb der Mineralklasse der „Silikate und Germanate“. Er kristallisiert im kubischen Kristallsystem mit der idealisierten Zusammensetzung FeAl[SiO], ist also chemisch gesehen ein Eisen-Aluminium-Silikat, das strukturell zu den Inselsilikaten gehört.

Almandin ist das Eisen-Analogon zu Spessartin (MnAl[SiO]) und Pyrop (MgAl[SiO]) und bildet mit diesen eine Mischkristallreihe, die sogenannte „Pyralspit-Reihe“. Da Almandin zudem mit Grossular (CaAl[SiO]) Mischkristalle bildet, weist natürlicher Almandin ein entsprechend weites Spektrum der Zusammensetzung mit je nach Bildungsbedingungen mehr oder weniger großen Anteilen von Mangan, Magnesium und Calcium auf. Zusätzlich können noch Spuren von Natrium, Kalium, Chrom und Vanadium, seltener auch Scandium, Yttrium, Europium, Ytterbium, Hafnium, Thorium und Uran vorhanden sein.

Das Mineral ist durchsichtig bis durchscheinend und entwickelt typischerweise Rhombendodekaeder oder Ikositetraeder sowie Kombinationen dieser Kristallformen, die fast kugelig wirken. Ebenfalls oft zu finden sind körnige bis massige Mineral-Aggregate. Im Allgemeinen können Almandinkristalle eine Größe von mehreren Zentimetern Durchmesser erreichen. Es wurden jedoch auch Riesenkristalle von bis zu einem Meter Durchmesser bekannt. Die Farbe von Almandin variiert meist zwischen dunkelrot und rotviolett, kann aber auch bräunlichrot bis fast schwarz sein.

Almandin ist die weltweit am häufigsten auftretende Granatart und kommt oft in schleifwürdigen Qualitäten mit starkem, glasähnlichem Glanz vor, was ihn zu einem begehrten Schmuckstein macht.

Almandin war bereits Plinius dem Älteren (ca. 23–79 n. Chr.) unter dem Namen "alabandicus" bekannt und gehörte allgemein zu den „Karfunkelsteinen“ ("carbunculus"), das heißt roten Edelsteinen. Benannt wurde er nach der antiken Stadt Alabanda in Karien (Kleinasien, heute in der türkischen Provinz Aydın), wo der Stein bearbeitet worden sein soll. Alabanda gilt daher auch als Typlokalität für Almandin.

Im Mittelalter waren verschiedene Abwandlungen des Namens im Umlauf wie unter anderem "alabandina", "alabandra" und "alabanda". Albertus Magnus (um 1200–1280) führte die Bezeichnung "alamandina" ein, die fast der heutigen Form entsprach.

Um 1800 wurde die Bezeichnung Almandin schließlich endgültig von Dietrich Ludwig Gustav Karsten (1768–1810) auf den Eisentongranat festgelegt.

Kurioserweise wurde das 1784 erstmals beschriebene und namentlich ähnliche Mangansulfid Alabandin ebenfalls nach dem türkischen Ort Alabanda benannt, obwohl es dort bisher nicht nachgewiesen werden konnte.

In der mittlerweile veralteten, aber noch gebräuchlichen 8. Auflage der Mineralsystematik nach Strunz gehörte der Almandin zur Abteilung der „Inselsilikate (Nesosilikate)“, wo er zusammen mit Andradit, Calderit, Goldmanit, Grossular, Henritermierit, Hibschit, Holtstamit, Hydrougrandit, Katoit, Kimzeyit, Knorringit, Majorit, Morimotoit, Pyrop, Schorlomit, Spessartin, Uwarowit, Wadalit und "Yamatoit" (diskreditiert, da identisch mit Momoiit) die „Granatgruppe“ mit der System-Nr. "VIII/A.08" bildete.

Die seit 2001 gültige und von der International Mineralogical Association (IMA) verwendete 9. Auflage der Strunz’schen Mineralsystematik ordnet den Almandin ebenfalls in die Abteilung der „Inselsilikate (Nesosilikate)“ ein. Diese ist allerdings weiter unterteilt nach der möglichen Anwesenheit weiterer Anionen und der Koordination der beteiligten Kationen, so dass das Mineral entsprechend seiner Zusammensetzung in der Unterabteilung „Inselsilikate ohne weitere Anionen; Kationen in oktahedraler [6] und gewöhnlich größerer Koordination“ zu finden ist, wo es zusammen mit Andradit, Blythit, Calderit, Goldmanit, Grossular, Henritermierit, Hibschit, Holtstamit, Hydroandradit, Katoit, Kimzeyit, Knorringit, Majorit, Momoiit, Morimotoit, Pyrop, Schorlomit, Skiagit, Spessartin, Uwarowit und Wadalit die „Granatgruppe“ mit der System-Nr. "9.AD.25" bildet.

Auch die vorwiegend im englischen Sprachraum gebräuchliche Systematik der Minerale nach Dana ordnet den Almandin in die Abteilung der „Inselsilikatminerale“ ein. Hier ist er zusammen mit Pyrop, Spessartin, Knorringit, Majorit und Calderit in der „Granatgruppe (Pyralspit-Reihe)“ mit der System-Nr. "51.04.03a" innerhalb der Unterabteilung „“ zu finden.

Almandin kristallisiert kubisch in der mit dem Gitterparameter "a" = 11,53 Å sowie 8 Formeleinheiten pro Elementarzelle.

"Rhodolithe", allgemein auch als "orientalische Granate" bekannt, sind rosa- bis rotviolette Almandin-Varietäten, die genau genommen Almandin-Pyrop-Mischkristalle mit einem Mischungsverhältnis von Magnesium : Eisen ≈ 2 : 1 und einer Dichte von ≈ 3,84 g/cm³ sind. Bekannte Vorkommen für "Rhodolith" sind unter anderem Brasilien, Indien, Kenia, Madagaskar, Mexiko, Sambia und Tansania.

Auch der "Malaya-Granat" ist ein Almandin-Pyrop-Mischkristall mit den gleichen Fundgebieten wie "Rhodolith", allerdings von eher rötlich oranger Farbe. Benannt wurde er nach dem Suaheli-Wort "malaya" für „außerhalb der Familie stehend“.

Almandin ist ein charakteristisches Mineral metamorpher Gesteine wie unter anderem Glimmerschiefer, Amphibolit, Granulit und Gneis. Almandinreiche Granate können sich aber auch in magmatischen Gesteinen wie Granit und Granit-Pegmatit bilden. Die Kristalle sind normalerweise im Mutter-Gestein eingebettet (Blasten) und von anderen Almandin-Kristallen getrennt. Granate mit den bisher höchsten bekannten Almandingehalten von 86,7 % (Stand: 1995) fand man bei Kayove in Ruanda, aber auch in Deutschland traten schon almandinreiche Kristalle von rund 76 % auf, so unter anderem bei Bodenmais.

Als häufige Mineralbildung ist Almandin an vielen Fundorten anzutreffen, wobei bisher (Stand: 2014) rund 2200 Fundorten als bekannt gelten. Begleitet wird Almandin unter anderem von verschiedenen Amphibolen, Chloriten, Plagioklasen und Pyroxenen sowie von Andalusit, Biotit, Cordierit, Hämatit, Kyanit, Sillimanit und Staurolith.

Neben seiner Typlokalität Alabanda trat das Mineral in der Türkei bisher nur noch in den Granat-Amphiboliten nahe Çamlıca auf der asiatischen Seite Istanbuls auf.

In Deutschland konnte Almandin an mehreren Orten im Schwarzwald (Freiburg im Breisgau, Grube Clara in Oberwolfach) in Baden-Württemberg, an vielen Orten in Bayern (Bayerischer Wald, Oberpfälzer Wald, Spessart), bei Ruhlsdorf/Eberswalde-Finow in Brandenburg, an einigen Orten im Odenwald (Erlenbach, Lindenfels), bei Bad Harzburg in Niedersachsen, bei Bad Doberan in Mecklenburg-Vorpommern, bei Perlenhardt und am Drachenfels (Königswinter) in Nordrhein-Westfalen, an vielen Orten in der Eifel in Rheinland-Pfalz, in der Grube „Gottesbelohnung“ bei Schmelz im Saarland, im Steinbruch Diethensdorf und bei Penig sowie an vielen Orten im Erzgebirge in Sachsen und an einigen Orten in Schleswig-Holstein (Barmstedt, Kiel, Schleswig, Travemünde) gefunden werden.

In Österreich fand sich das Mineral bisher vor allem in Kärnten in den Gurktaler Alpen und der Saualpe, in der Koralpe von Kärnten bis zur Steiermark und in den Niederen Tauern, aber auch an mehreren Orten in Niederösterreich (Wachau, Waldviertel), Salzburg (Hohe Tauern), im Tiroler Gurgler Tal und Zillertal sowie an einigen Fundpunkten in Oberösterreich und Vorarlberg.

In der Schweiz sind Almandinfunde bisher nur von einigen Orten in den Kantonen Tessin (Gotthardmassiv) und Wallis (Binntal) bekannt geworden.

Bekannt aufgrund außergewöhnlicher Almandinfunde sind unter anderem die Ishikawa-Pegmatite in der Präfektur Fukushima auf der japanischen Insel Honshū und Shengus am Haramosh in Pakistan, wo gut ausgebildete Almandinkristalle von bis zu 15 Zentimeter Durchmesser entdeckt wurden. Bis zu 5 Zentimeter große Kristalle fand man unter anderem in den Glimmerschiefern und Gneisen bei Fort Wrangell in Alaska und bei Bodø in Norwegen. Auch in Italien, bzw. Südtirol wurden Almandine von beträchtlicher Größe am Granatkogel im Seebertal gefunden.

Weitere Fundorte liegen unter anderem in Afghanistan, Ägypten, Äthiopien, Algerien, Angola, der Antarktis, Argentinien, Australien, Belgien, Bolivien, Brasilien, Bulgarien, Burkina Faso, Chile, China, der Demokratischen Republik Kongo, Finnland, Frankreich und Französisch-Guayana, Griechenland, Grönland, Guatemala, Indien, Ireland, Israel, Kanada, Kolumbien, Korea, Madagaskar, Malawi, Mexiko, der Mongolei, Myanmar, Namibia, Nepal, Neukaledonien, Neuseeland, Norwegen, Polen, Portugal, Rumänien, Russland, Saudi-Arabien, Schweden, Simbabwe, der Slowakei, Slowenien, Spanien, Sri Lanka, Südafrika, Taiwan, Tadschikistan, Thailand, Tschechien, der Ukraine, Ungarn, Usbekistan, im Vereinigten Königreich (UK) und den Vereinigten Staaten von Amerika (USA).

Auch in Gesteinsproben vom Mond konnte Almandin nachgewiesen werden.

Almandin wird wie die meisten anderen Minerale der Granatfamilie vor allem als Schmuckstein verwendet, die je nach Reinheit und Klarheit in Facettenform oder zu Cabochonen geschliffen werden. Weniger edle, das heißt zu dunkle und undurchsichtige Varietäten werden auch als Schleifmittel genutzt.

Verwechslungsgefahr besteht vor allem mit den verschiedenen Granatvarietäten aufgrund der überwiegenden Mischkristallbildung zwischen den einzelnen Endgliedern. Daneben kann Almandin aber auch mit Rubin, Spinell und roten Turmalinen verwechselt werden. Aufgrund der schwierigen Unterscheidung werden die verschiedenen Granatnamen im Edelsteinhandel inzwischen häufig als Farbbezeichnung genutzt, wobei Almandin und Rhodolith die rosa bis violetten Granate vertreten.

Der bisher größte bekannte und geschliffene Almandin-Edelstein ist ein Cabochon von 175 ct, der in der Smithsonian Institution in Washington, D.C aufbewahrt wird.





</doc>
<doc id="443" url="https://de.wikipedia.org/wiki?curid=443" title="Aktiva">
Aktiva

Unter Aktiva (Plural von "Aktivum") versteht man die Summe des einem Unternehmen zur Verfügung stehenden Vermögens, das auf der linken Seite einer Bilanz zu finden ist. Gegensatz sind die Passiva. 

Aktiva (lat. agere, „tätig sein“, „handeln“) stehen auf der Aktivseite der Bilanz und stellen die einem Unternehmen zur Verfügung stehenden Vermögensgegenstände dar. Die Aktivseite lässt die Verwendung des auf der Passivseite angegebenen Kapitals erkennen. Die Aktivseite der Bilanz zeigt mithin die Verwendung der finanziellen Mittel bzw. den Besitz des Wirtschaftssubjektes, während die rechte Seite der Bilanz (Passivseite) die Mittelherkunft anzeigt.<ref name="Wöhe/Kußmaul S.5">Günter Wöhe / Heinz Kußmaul, "Grundzüge der Buchführung und Bilanztechnik", 8. Auflage, München 2012, S. 5</ref> Von Aktivierung spricht man, wenn ein Bilanzposten auf der Aktivseite verbucht wird. Dabei ist zu unterscheiden, ob die Aktiva einer Aktivierungspflicht, einem Aktivierungswahlrecht oder einem Aktivierungsverbot unterliegen. 

Die Buchhaltung führt die Endbestände der Aktiv- und Passivkonten zusammen, die Gegenüberstellung der Aktiva mit den Passiva zu einer kontenmäßigen Einheit heißt Bilanz. Hierin sind die Summen der Aktiva und der Passiva (Bilanzsumme) formal identisch, dies ist ein wesentliches Merkmal der Bilanz. Der so gefasste Bilanzbegriff unterscheidet sich vom Kontobegriff nur darin, dass man beim Konto von Soll und Haben spricht. Dass Aktiva und Passiva summenmäßig identisch sind, liegt am systematischen Wertausgleich durch den Saldoposten des Erfolges. Daraus ergibt sich die Gleichung

Die drei Bilanzprinzipien der Bilanzwahrheit, Bilanzklarheit und Bilanzkontinuität gelten sowohl für Aktiv- als auch Passivseite. Aus Gründen des Vorsichtsprinzips und des damit einhergehenden Gläubigerschutzes können bestimmte Teile der Aktiva (insbesondere Anlagevermögen, Roh-, Hilfs- und Betriebsstoffe und Forderungen) im Rahmen des Niederstwertprinzips unterbewertet werden, Passiva können hingegen überbewertet werden. Unterbewertung bedeutet, dass den Vermögensgegenständen im Rahmen des Niederstwertprinzips und der vernünftigen kaufmännischen Beurteilung ein niedrigerer Bilanzwert beigemessen werden darf als es dem tatsächlichen Zeitwert entspricht. Hiermit soll den Vermögensrisiken angemessen Rechnung getragen werden, die in der Gefahr eines ganzen oder teilweisen Wertverlustes einzelner Vermögensgegenstände bestehen. 

Der Begriff Aktivseite ist ein bestimmter Rechtsbegriff, der im Gliederungsschema des Abs. 2 HGB erwähnt wird. Danach besteht die Aktivseite auf der ersten Gliederungsebene abschließend aus Anlagevermögen, Umlaufvermögen, aktiven Rechnungsabgrenzungsposten, aktiven latenten Steuern und dem aktiven Unterschiedsbetrag aus der Vermögensverrechnung. Bei den Vermögensgegenständen unterscheidet man zwischen materiellem und immateriellem Anlagevermögen. Die Verbindlichkeit dieser Gliederungspunkte und der festgelegten weiteren Unterteilung letzterer richtet sich gemäß § 266 Abs. 1 und 2, und HGB nach bestimmten Kriterien, wie z.B. der Rechtsform oder der Größenklasse des bilanzierenden Unternehmens.

Im Anlagevermögen sind gemäß Abs. 2 HGB nur die Gegenstände auszuweisen, die bestimmt sind, dauernd dem Geschäftsbetrieb zu dienen. Das Anlagevermögen beinhaltet somit die mittel- und langfristig gebundenen Mittel des Unternehmens. Ebenfalls zum Anlagevermögen gerechnet werden Finanzanlagen mit dauerhaftem Charakter, beispielsweise langfristige Anleihen und Beteiligungen, Ausleihungen oder Anteile an anderen Unternehmen.<ref name="Bieg/Kußmaul/Waschbusch S.117">Hartmut Bieg/Heinz Kußmaul/Gerd Waschbusch, "Externes Rechnungswesen", 6. Auflage, München 2012, S. 117</ref> 

Weiterhin umfasst das Anlagevermögen auch immaterielle Vermögensgegenstände. Es handelt sich um solche Vermögensgegenstände, die nicht körperlich fassbar sind. Hierzu zählen entgeltlich erworbene immaterielle Vermögensgegenstände, wie Lizenzen, gewerbliche Schutzrechte und Konzessionen. So zählt in der Medienindustrie das immaterielle Vermögen zu den wichtigsten Elementen der Bilanz, werden hier doch die zukünftig zu erwartenden Erträge aus Film- oder Musikrechten kapitalisiert und aufgeführt. Auch immaterielle Vermögensgegenstände, die nicht entgeltlich erworben wurden, sind dem Anlagevermögen zuzuordnen. Allerdings besteht handelsrechtlich unter den Voraussetzungen des Abs. 2 HGB lediglich ein Aktivierungswahlrecht für solche selbst geschaffenen immateriellen Vermögensgegenstände; steuerlich besteht gemäß Abs. 2 EStG sogar ein Aktivierungsverbot. 

Das Umlaufvermögen umfasst diejenigen Vermögensgegenstände, die das Unternehmen zur kurzfristigen Verwendung besitzt. Dazu zählen beispielsweise die Kassenbestände, Bankguthaben sowie kurzfristig verfügbare Finanzanlagen. Daneben bilden zum Beispiel auch für die Produktion notwendige Rohstoffe und Vorprodukte sowie kurzfristig verkaufbare Lagerbestände an Fertigprodukten Teile des Umlaufvermögens.

Rechnungsabgrenzungsposten dienen dazu, Aufwendungen und Erträge der Periode zuzuordnen, welcher sie wirtschaftlich zugerechnet werden müssen. Als Rechnungsabgrenzungsposten auf der Aktivseite der Bilanz - auch aktive Rechnungsabgrenzungsposten genannt - sind nach Abs. 1 HGB Ausgaben auszuweisen, die vor dem Bilanzstichtag getätigt wurden, aber erst später einen Aufwand darstellen. Hierzu zählen z. B. im Voraus bezahlte Mieten, die im laufenden Geschäftsjahr gezahlt wurden aber erst im nächsten Geschäftsjahr fällig wären und auch erst dann einen Aufwand darstellen.

Differenzen zwischen handelsrechtlichen und steuerrechtlichen Wertansätzen, die zu einer Steuerentlastung führen und sich in späteren Perioden voraussichtlich auflösen, können gemäß Abs. 1 Satz 2 HGB als aktive latente Steuern aktiviert werden.

Als aktiver Unterschiedsbetrag aus der Vermögensverrechnung ist der beizulegende Zeitwert von Vermögensgegenständen abzüglich der entsprechenden Schulden anzusetzen, sofern eine Verrechnung von Vermögensgegenständen und Schulden im Sinne des Abs. 2 Satz 2 HGB erfolgt und das Ergebnis hieraus positiv ist.

Unter bestimmten Umständen kann oder muss die Aktivseite um weitere Posten ergänzt werden. Gemäß Abs. 5 HGB können weitere Posten hinzugefügt werden, wenn deren Inhalt nicht von einem anderen, vorgeschriebenen Posten gedeckt ist.<ref name="Bieg/Kußmaul/Waschbusch S.123f.">Hartmut Bieg, Heinz Kußmaul, Gerd Waschbusch: "Externes Rechnungswesen", 6. Auflage, München 2012, S. 123 f.</ref> Die Gliederung und Bezeichnung der Posten müssen nach Abs. 6 HGB geändert werden, wenn dies wegen Besonderheiten des Unternehmens zur Aufstellung eines klaren und übersichtlichen Jahresabschlusses notwendig ist. Des Weiteren besteht die Möglichkeit, Bilanzposten unter den Voraussetzungen des Abs. 7 und 8 HGB zusammenzufassen oder ganz wegzulassen.

Aufwendungen für die Ingangsetzung und Erweiterung des Geschäftsbetriebs, können nach § 67 Abs. 5 Satz 1 EGHGB auch nach dem Bilanzrechtsmodernisierungsgesetz (BilMoG) als Bilanzierungshilfe auf der Aktivseite vor dem Anlagevermögen ausgewiesen werden, wenn diese Bilanzierungshilfe für ein Geschäftsjahr, das vor 2010 begonnen hat, gebildet wurde.

Im Rahmen der Vermögensanalyse interessiert sich die Bilanzanalyse für die Zusammensetzung der Aktiva, deren Verhältnis zu anderen Bilanzpositionen und ermittelt betriebswirtschaftliche Kennzahlen, die sich mit der vertikalen Vermögensstruktur der Aktivseite, dem Verhältnis einzelner Aktivpositionen zu den entsprechenden Passivpositionen sowie dem Verhältnis zu den Erträgen befassen.<ref name="Coenenberg/Haller/Mattner/Schultze S.582">Adolf G. Coenenberg, Axel Haller, Gerhard Mattner, Wolfgang Schultze: "Einführung in das Rechnungswesen", 4. Auflage, Stuttgart 2012, S. 582.</ref> Hierzu gehört insbesondere die Anlagenintensität, die Teile des Gesamtvermögens mit dem Gesamtvermögen in Beziehung setzt. Die horizontale Kapitalstruktur befasst sich mit dem Verhältnis von Aktiv- zu Passivseite einer Bilanz im Rahmen der Anlagendeckung. Die Sachanlagen- und Forderungsbindung ermöglicht Aussagen über das Verhältnis der Sachanlagen bzw. des Forderungsbestands zu den Umsatzerlösen. 

Die Aktiva alleine geben wenig Aufschluss über die Liquidität und Ertragsfähigkeit, da nur der Zusammenhang zwischen Kapitalverwendung und Kapitalaufbringung Rückschlüsse auf die Entwicklungen und Zukunftsaussichten des Unternehmens zulässt.<ref name="Coenenberg/Haller/Mattner/Schultze S.572ff.">Adolf G. Coenenberg, Axel Haller, Gerhard Mattner, Wolfgang Schultze: "Einführung in das Rechnungswesen", 4. Auflage, Stuttgart 2012, S. 572 ff.</ref> Insbesondere gilt als goldene Regel zur Beurteilung der Finanzierung des Anlagevermögens, dass langfristige Investitionen nicht mit kurzfristigem Fremdkapital finanziert werden dürfen.<ref name="Coenenberg/Haller/Mattner/Schultze S.586">Adolf G. Coenenberg, Axel Haller, Gerhard Mattner, Wolfgang Schultze: "Einführung in das Rechnungswesen", 4. Auflage, Stuttgart 2012, S. 586.</ref> Damit soll vermieden werden, dass die Pflicht zur Rückzahlung des Fremdkapitals bereits vor einer erfolgreichen Nutzung der erworbenen Vermögensgegenstände besteht.

Insbesondere die immateriellen Vermögensgegenstände (IV) sollten detailliert betrachtet werden. Der nicht direkt messbare Wert dieses Vermögens lässt sich nur mit eindeutigen Regeln bilanzieren.<ref name="Bieg/Kußmaul/Waschbusch S.107">Hartmut Bieg, Heinz Kußmaul, Gerd Waschbusch: "Externes Rechnungswesen", 6. Auflage, München 2012, S. 107.</ref> Die Regeln sollen klar formuliert, nachvollziehbar sein sowie die Umsetzung des Niederstwertprinzips im Sinne von Abs. 3 und 4 HGB bezwecken.

Die volkswirtschaftlichen Gesamtrechnungen „vermitteln ein umfassendes quantitatives Gesamtbild des wirtschaftlichen Geschehens“. Hierbei werden innerhalb der Vermögensrechnung sogenannte Vermögensbilanzen erstellt, deren Aktivseite aus Sach- und Geldvermögen besteht.




</doc>
<doc id="444" url="https://de.wikipedia.org/wiki?curid=444" title="Alexander Graham Bell">
Alexander Graham Bell

Alexander Graham Bell (* 3. März 1847 in Edinburgh, Schottland; † 2. August 1922 in Baddeck, Kanada) war ein britischer und später US-amerikanischer Sprechtherapeut, Erfinder und Großunternehmer. Er gilt als der erste Mensch, der aus der Erfindung des Telefons Kapital geschlagen hat, indem er Ideen seiner Vorgänger zur Marktreife weiterentwickelte. Zu seinen Ehren wurde die dimensionslose Maßeinheit (Pseudomaß) für logarithmische Verhältniswerte, mit dem auch Schallpegel gemessen werden, mit Bel benannt.

Bell, der wie seine beiden Brüder zunächst von der Mutter unterrichtet wurde, besuchte ab dem 10. Lebensjahr eine Privatschule in Edinburgh und ab dem 14. Lebensjahr eine Schule in London. Er studierte in Edinburgh Latein und Griechisch. Bereits der Großvater Alexander und der Vater Alexander Melville Bell beschäftigten sich mit Sprechtechnik, wobei Letzterer als Professor der Rede- und Vortragskunst das erste universale phonetische Schriftsystem bzw. eine Lautschrift oder phonetisches Alphabet entwickelte, das er "Visible Speech" nannte, weil damit die Laute abgebildet würden.

Sohn Alexander, der in Bewunderung für einen Freund der Familie noch als Kind den Zunamen "Graham" annahm, wurde mit 17 Jahren Lehrer an der Weston House Academy für Sprechtechnik und Musik in Elgin, Schottland. Während dieser Zeit begannen seine ersten selbstständigen Forschungsarbeiten auf dem Gebiet der Akustik. Dabei lernte er auch den deutschen Physiker und Physiologen Hermann von Helmholtz kennen, der mit seinem 1863 erschienenen Werk „Lehre von den Tonempfindungen als physiologische Grundlage für die Theorie der Musik“ den jungen Bell wesentlich beeinflusste.

Schließlich folgte er seinem Vater nach London, wo dieser am University College als Lehrer für Sprechtechnik tätig war und seinen Sohn als Assistenten einstellte. Bell studierte bis 1870 Anatomie und Physiologie der menschlichen Stimme.

Nachdem Alexanders Brüder Edward (1868) und Melville (1870) beide an Tuberkulose gestorben waren, siedelten Alexander und seine Eltern 1870 nach Kanada über, wo der Vater ein besseres Klima erhoffte und eine Lehrtätigkeit aufnahm.

Die historisch nachhaltigste Wirkung hatte Bell 1876 mit der Entwicklung und Einführung des Telefons zu einem gebrauchsfähigen System. In der Folge entstand die Bell Telephone Company, die sich später zum weltweit größten Telekommunikationskonzern AT&T entwickelte.

1876 heiratete er die gehörlose Tochter Mabel seines Geschäftspartners Hubbard, die er als Gehörlosenlehrer (damals „Taubstummenlehrer“) an der Clarke-Schule kennenlernte. Mit ihr hatte er zwei Töchter, Elsie May und Marian (Daisy) Bell, sowie die Söhne Edward und Robert, die beide im Kindesalter starben.

1882 erhielt Bell die Staatsbürgerschaft der USA. Bis zu seinem Tode 1922 beschäftigte sich Bell vor allem mit weiteren Entwicklungen und Erfindungen auf zahlreichen technischen Gebieten sowie auch mit Untersuchungen zur Eugenik der Taubheit.

1890 half er bei der Gründung der "American Association to Promote the Teaching of Speech to the Deaf (AAPTSD)" (heute Alexander Graham Bell Association for the Deaf and Hard of Hearing), deren erster Präsident er wurde.

1897 wurde Bell nach dem Tod von Gardiner Greene Hubbard zum zweiten Präsidenten der National Geographic Society gewählt.

Bells Mutter Eliza Symonds Bell war stark schwerhörig, Bell konnte sich jedoch mit ihr mit besonders tiefer Stimme unterhalten. Außerdem konnte sie die Schwingungen seiner Klaviermusik spüren. Das sowie die familiär vorgeprägte berufliche Laufbahn veranlassten Bell offensichtlich, einer der engagiertesten Befürworter des lautsprachlich orientierten Erziehungsprinzips für Gehörlose im Gegensatz zu gebärdensprachlich orientierten Methoden zu werden.

1868 gab Bell an Susanna Hulls Schule in London Sprechunterricht für gehörlose Kinder. 1871 ging Bell als Gehörlosenlehrer an die in Northampton eingerichtete spätere „Clarke School“ in Northampton (Massachusetts). Ein Luftballon, den sich jedes dieser Kinder ans Ohr hielt, konnte die Schwingungen in der Stimme aufnehmen. Bell bleibt danach für den Rest seines Lebens Mitglied des Aufsichtsrats der Schule und wird in den letzten fünf Lebensjahren auch dessen Vorsitzender. An dieser Schule lernt er auch Mabel, seine spätere Frau kennen. In der gleichen Zeit unterrichtete er auch neben Edward Miner Gallaudet am American Asylum for the Deaf in Hartford (Connecticut).

Von 1873 bis 1877 war er Professor für Sprechtechnik und Physiologie der Stimme an der Universität Boston.

Bell soll sich in erster Linie als Gehörlosenlehrer und weniger als Erfinder gesehen haben. Eine Ironie der Geschichte ist, dass Bell, der stets beabsichtigte, taube Menschen zu fördern, mit dem Telefon ein System verbreitete, das zum Standard-Instrument in Beruf, Geschäftsleben und Alltag wurde, aber für Gehörlose fast ein Jahrhundert lang noch nicht benutzbar war.

Bereits in den 1830 machte der Italienische Wissenschaftler (Studium der Chemie und Mechanik), Erfinder und Gründer Antonio Meucci die Entdeckung, dass Schall durch elektrische Schwingungen in Kupferdraht übertragen werden kann. Nachdem er 1850 in die USA umsiedelte, entwickelte er ein Telefon, mit dem er das Krankenzimmer seiner Ehefrau mit seiner Werkstatt verband. In den nächsten 10 Jahren vervollkommnete er seine Anordnung, präsentierte sie ab 1860 öffentlich und berichtete in der italienischsprachigen Presse. Jedoch fand er in angelsächsischen Medien keine Erwähnung. 1871 stellte er für sein "Telettrofono" schließlich einen Patentantrag, der über zwei Jahre lang nicht erteilt wurde und deshalb erlosch. Später wurde verbreitet, er hätte nicht die nötigen Mittel für die Erteilung gehabt. Diese Darstellung wird allerdings von Kritikern angezweifelt, da er in derselben Zeit (1872–1876) vier andere Patente erteilt bekam.

Meucci reichte seine Unterlagen und Geräte bei Edward B. Grant ein, dem Vizepräsidenten der American District Telegraph Co., um seine Erfindung an deren Telegraphenkabel testen zu lassen, wurde aber über zwei Jahre lang hingehalten. Inzwischen nutzte Bell, der jetzt in den ehemaligen Werkstätten von Meucci bei der American District Telegraph Co. arbeitete, Meuccis Materialien und Unterlagen für die Patentierung "seines" Telefons.
Als Meucci 1874 diese Gerätschaften und Unterlagen von Grant zurückforderte, wurde ihm mitgeteilt, man habe diese verloren. Meucci war des Englischen nicht mächtig und beauftragte einen Anwalt, gegen Bells Vorgehen zu protestieren, was allerdings nie geschah. Trotz jahrzehntelanger Streitigkeiten gelang es Antonio Meucci nicht, das Patent oder wenigstens finanzielle Entschädigungen von Bell zu erhalten. Er starb als verarmter Mann. Am 11. Juni 2002 würdigte das Repräsentantenhaus des amerikanischen Kongresses der Vereinigten Staaten in einer Resolution Antonio Meuccis Erfindung und seine Arbeit bei der Einführung des Telefons.

Von 1858 bis 1863 hatte Johann Philipp Reis auch ein funktionierendes Gerät zur Übertragung von Tönen über elektrische Leitungen entwickelt und seiner Erfindung den Namen „Telephon“ gegeben. Am 26. Oktober 1861 führte er den Fernsprecher zahlreichen Mitgliedern des Physikalischen Vereins in Frankfurt vor. Reis nahm den Morse-Telegraphen als Vorbild, der mit Unterbrechung des Stromkreislaufs arbeitet. Damit konnte er Musiknoten an einen Empfänger schicken, für Sprache war das Gerät (noch) nicht geeignet. Danach verbesserte Reis den Apparat bis 1863 wesentlich und verkaufte ihn in größeren Mengen als wissenschaftliches Demonstrationsobjekt. So kamen auch Exemplare ins Ausland.

Bell lernte ein frühes Modell des Reis’schen Telefonapparates bereits 1862 in Edinburgh kennen. Sein Vater versprach ihm und seinen Brüdern einen Preis, wenn sie diese "Sprechmaschine" weiterentwickeln würden. 1865 konnte der britisch-amerikanische Erfinder David Edward Hughes in England gute Resultate mit dem deutschen „Telephon“ erzielen. Ab 1868 wurde in den USA mit der deutschen Erfindung gearbeitet. Als Bell im März 1875 an der amerikanischen Forschungs- und Bildungseinrichtung Smithsonian Institution mit dem Reis’schen Telefonapparat experimentierte, war die Erfindung in Fachkreisen bereits gut bekannt und Reis seit über einem Jahr verstorben. Bell konnte aber von der für ihn wichtigen Grundlagenforschung des Deutschen profitieren.

Um 1873 versuchte Bell, einen „harmonischen Telegraphen“ zu entwickeln, der durch Benutzung mehrerer isolierter musikalischer Tonlagen mehrere Nachrichten gleichzeitig senden können sollte, betrieb das jedoch mit wenig Engagement. 1874 führt Bell akustische Experimente zur Aufzeichnung von Schallwellen durch. Er konstruierte damit den „Phonautographen“, ein Gerät, das die Vibrationen des Schalls auf einem berußten Zylinder aufzeichnete.

Der prominente Bostoner Rechtsanwalt und gleichzeitige Direktor der „Clarke School for the Deaf“ Gardiner Greene Hubbard und der wohlhabende Geschäftsmann Thomas Sanders aus Salem erfuhren von Bells Experimenten und bewogen ihn, die Entwicklung am Harmonischen Telegraphen voranzutreiben. Die drei unterzeichneten eine Vereinbarung, nach der Bell finanzielle Unterstützung erhielt im Gegenzug für spätere Beteiligung von Hubbard und Sanders an den Erträgen. Hubbards gehörlose Tochter Mabel wurde als Druckmittel eingesetzt. Bell durfte sie erst 1877 heiraten, nachdem er seine Erfindung fertiggestellt hatte.
Obwohl Bell bei seinen Versuchen zufällig entdeckt haben soll, dass statt der erwarteten Telegraphenimpulse auch Tonfolgen übertragen werden konnten, gelang es ihm nicht, diese Entdeckung zu wiederholen. Gleichwohl meinte er, das Prinzip für die Übertragung von Tönen für einen Patentantrag beschreiben zu können. Zugute kam ihm dabei, dass das Patentamt einige Jahre zuvor die Anforderung hatte fallen lassen, mit dem Patentantrag ein funktionierendes Modell einzureichen. Am 14. Februar 1876 reichte Bells Anwalt, Gardiner Greene Hubbard, den Patentantrag ein, nur zwei Stunden bevor der Lehrer, Erfinder und Unternehmer, Elisha Gray Gleiches tun konnte. Der wesentliche Unterschied zwischen beiden Fernsprechern war, dass Bells Erfindung im Gegensatz zu der von Gray nicht funktionierte. Während Bell bei seinem Antrag auch nur sehr vage blieb, beschrieb Gray sein Telefon in einer ins einzelne gehenden Schrift. Bells Eile war nicht unbegründet, wusste er doch von mehreren Erfindern, die auch an Telefonen arbeiteten.

Der von Hubbard eingereichte Antrag löste den größten Patentstreit der Geschichte aus. Bell verwendete bei der späteren praktischen Ausführung seines Telefons u. a. einen regelbaren Widerstand. Dieser war als Draht ausgeführt, der in einer Schwefellösung getaucht war. Bell soll diesen Widerstand nie zuvor ausprobiert haben. Zudem war dieser Widerstand in seiner Patentschrift nicht aufgeführt. Elisha Grays Antrag hingegen enthielt einen solchen Widerstand. Besonders nachdem Bells Patent am 7. März 1876 erteilt worden war, wurden die Stimmen lauter, die eine illegale Verbindung zwischen Bell und dem Patentamt sahen. Ein Beamter beschuldigte sich selbst der Bestechung, doch wurde seine wankelmütige Aussage in der internationalen Fachpresse bezweifelt.

Das von Bells sachkundigem Mechaniker Thomas A. Watson gebaute erste funktionierende Telefon sah den Berichten zufolge merkwürdig aus. Die im Patentstreit umstrittene säuregefüllte Metalldose war mit einer Scheibe bedeckt, die einen Draht hielt, der in die Säure getaucht war. Außen an der Metalldose befand sich ein anderer Draht, der zum Empfängertelefon führt. Das Hineinbrüllen in einen senkrecht darüber angeordneten Trichter brachte Scheibe und Draht zum Schwingen. Durch diese Schwingungen veränderten sich der Abstand und damit auch der Stromfluss durch Draht und Säure zum Empfängertelefon. Dort wurden die Schwankungen des Stromes wieder in gleichartige Membranvibrationen umgesetzt, die dann Töne produzierten. Am 10. März 1876 soll der erste deutlich übertragene Satz übertragen worden sein: „Watson, come here. I need you.“ (Watson komm’ her, ich brauche dich). Bell soll sich aus Versehen Säure über die Kleidung geschüttet und nach Watson gerufen haben.

Dieses Telefon war nicht sonderlich gebrauchstauglich, doch Bell verbesserte es, da er im Gegensatz zu der Reis’schen Schallübertragungsmethode, die auf der Schwingung einer Membran beruhte, nun die elektromagnetische Induktion benutzte, welche der englische Physiker und Chemiker Michael Faraday (1791–1867) entdeckt hatte. Bell verwendete jetzt sowohl für den Lautsprecher als auch das Mikrofon elektromagnetische Spulen, Dauermagnete und den bereits erwähnten Widerstand. 1877 wurde dann ein neuartiger Schallwandler verbaut, der den druckabhängigen Übergangswiderstand zwischen Membran und einem Stück Kohle zur Signalgewinnung nutzte. Als Erfinder dieses Kohlemikrofons, das auf dem von Philipp Reis erfundenen Kontaktmikrofon aufbaut, gelten sowohl der britisch-amerikanische Konstrukteur und Erfinder David Edward Hughes, der 1865 mit einem importierten Telefon des Deutschen experimentiert hatte, als auch der deutsch-amerikanische Erfinder Emil Berliner 1877 während seiner Tätigkeit bei den Bell Labs. Dennoch dauerte es noch bis 1881, bis das Bell-Telefon praktisch einsatzfähig war.

Im Juli 1877 gründete Bell zusammen mit Thomas Sanders und Gardiner Greene Hubbard unter Einschluss seines Assistenten Thomas Watson die Bell Telephone Company. Zwei Tage später heiratete er die taube Tochter Mabel seines Geschäftspartners Hubbard, die er zuvor schon im Lippenlesen und Sprechen geschult hatte.

Nicht ganz überraschend war der Bedarf an Telefonapparaten zunächst gering und Bell und seine Partner hatten anfangs Absatzschwierigkeiten. Es kam dabei so weit, dass sie ihre Patente der mächtigen Western Union Telegrafengesellschaft – Elisha Grays Arbeitgebern – für 100.000 $ zum Kauf anboten. Die Western Union lehnte ab, was sich bald als große Fehlentscheidung herausstellen sollte.

Dennoch sahen Amerikas Telegraphengesellschaften voraus, dass Bells Telefon eine Bedrohung für ihr Geschäft darstellte, und versuchten, dem gegenzusteuern. Die Western Union Company ließ Thomas Alva Edison ein eigenes Telefon mit anderer Technik entwickeln. Bell verklagte daraufhin Western Union wegen der Verletzung seiner Patentrechte. Diese versuchte zu argumentieren, dass eigentlich Elisha Gray das Telefon erfunden habe, verlor jedoch diesen und zahlreiche weitere Prozesse.
Auch Emil Berliner hatte Ärger mit dem Patentamt und Thomas Edison, für dessen Phonographen er ganz neue Vorstellungen eingereicht hatte. Berliner hatte auch ein Mikrofon entwickelt, das er 1877 für 50.000 Dollar an die „Bell Telephone Company“ verkaufte. Er zog nach Boston und arbeitete für Bell Telephone bis 1883.

Im März 1879 fusionierte die Bell Telephone Company mit der New England Telephone Company zur National Bell Telephone Company, deren Präsident William H. Forbes, Schwiegersohn von Ralph Waldo Emerson, wurde. Im April 1880 erfolgte eine weitere Fusion mit der American Speaking Telephone Company zur American Bell Telephone Company.

1885 wurde die American Telephone and Telegraph Company (AT&T) in New York als Tochterunternehmen von Graham Bell gegründet, um die Fernverbindungslinien quer durch die USA für das Bell’sche System zu erobern. Theodore Vail wurde der erste Präsident der Gesellschaft.

1889 wurden sämtliche Geschäftsaktivitäten der American Bell Telephone Company zur American Telephone and Telegraph Company transferiert, da Gesetze in Massachusetts das aggressive Wachstum verhindert hätten. Diese markiert den Anfang der heutigen Gesellschaft AT&T.

1925 wurden die Bell Telephone Laboratories aufgebaut, um die Forschungslaboratorien der AT&T und der Western Electric Company zusammenzufassen.

Für seine Erfindung verlieh Frankreich Bell 1880 den Volta-Preis im Wert von 50.000 Franc. Mit diesem Geld gründete er das Volta Laboratory in Washington D.C., wo er im gleichen Jahr mit seinem Assistenten, Charles Sumner Tainter und seinem Cousin ersten Grades Chichester Alexander Bell das Photophon entwickelte, welches Licht als Mittel der Projektion der Informationen verwendete, während das Telefon auf Strom angewiesen war. Im Jahr 1881 konnten sie erfolgreich eine Nachricht über das Photophon 200 Meter von einem Gebäude zum anderen versenden. Bell sah das Photophon als „die größte Erfindung, die ich je gemacht habe“. 

Im Jahr 1886 erfanden Bell und seine Mitarbeiter im Volta Labor die erste Phonographenwalze, die Schallwellen auf Wachs aufzeichnen konnte. Diese Neuerung ging einher mit der Weiterentwicklung des von Thomas Alva Edison erfundenen Phonographen, hin zum Graphophon. Im gleichen Zeitraum experimentierten die drei Mitglieder der Volta Laboratory Association mit einer flachen Wachsscheibe in senkrechter Position und nahmen somit die Idee einer Schallplatte vorweg. Die dabei verwendete Funktionsweise ähnelte der später vom Emil Berliner bei seinem Grammophon zum Tragen kommende horizontale Anordnung der verwendeten Tonträger. Nach Gründung der Volta Graphophone Company, deren Aufgabe darin bestand die Patente der Mitglieder der Volta Laboratory Association zu vermarkten, erfolgte die Veräußerung dieser an die American Gramophone Company mittels Verschmelzung beider Unternehmen.

Neben der Kommunikation ging Bell einer großen Vielfalt von wissenschaftlichen Experimenten nach, bei denen Drachen, Flugzeuge, tetraedrische Strukturen, Mehrlingsgeburten in der Schafzucht, künstliche Beatmung, sowie Entsalzung und Destillation von Meerwasser eine Rolle spielten. Das Attentat auf Präsident Garfield 1881 brachte Bell auf Idee zur Induktionswaage (Lokalisierung von Metallgegenständen im menschlichen Körper). 1881 starb Bells neugeborener Sohn, Edward, an Erkrankung der Atemwege. Bell reagierte auf diese Tragödie durch die Erfindung einer Metall-Vakuum-Jacke, die das Atmen erleichtern sollte. Dieser Apparat war ein Vorläufer der eisernen Lunge, die in den 1950er Jahren Anwendung fand. Immer wieder beschäftigte er sich mit der Taubheit und entwickelte das Audiometer zum Messen der Gehörleistung

1907 – vier Jahre nach dem ersten Flug der Brüder Wright in Kitty Hawk – gründete Bell mit Glenn Curtiss, William „Casey“ Baldwin, Thomas Selfridge, und J.A.D. McCurdy die Aerial Experiment Association. Diese jungen Ingenieure hatten das Ziel, ebenfalls eine Flugmaschine zu bauen. Bis 1909 hatten sie vier Tragflügel-Flugzeuge gebaut, von denen dem besten, „Silver Dart“ der erste erfolgreiche Flug am 23. Februar in Kanada gelang. Er wurde von einem zugefrorenen See in Baddeck, Nova Scotia, gestartet, wo Bell ein Haus besaß.

A. G. Bell erforschte zwischen 1882 und 1892 die Häufung von Gehörlosigkeit auf der Insel Martha’s Vineyard nahe Boston, vermutete dahinter richtigerweise erbbedingte Anlagen. Die Zusammenhänge konnte er jedoch nicht beweisen, da ihn irritierte, dass nicht jedes Kind von anscheinend erblich veranlagten Eltern taub wurde. Ihm fehlten dazu die Kenntnisse, die Gregor Mendel zwar schon 1865 formulierte, die aber bis zum Jahr 1900 der Öffentlichkeit weitgehend unbekannt blieben. Dennoch empfahl er in der Monographie "Memoir upon the Formation of a Deaf Variety of the Human Race" ein Eheverbot unter Taubstummen, warnte vor Internaten an den „Taubstummen“-Schulen als möglichen Brutstätten einer tauben Menschenrasse und empfahl die eugenische Kontrolle von USA-Immigranten. Spätere Arbeiten von Rassehygienikern stützten sich bis weit in das 20. Jahrhundert ungeprüft auf Bells Angaben. Als Folge wurden zahlreiche Taube ohne ihr Wissen und ohne ihr Einverständnis sterilisiert. Dabei soll Bell durchaus die methodischen Schwächen seiner Untersuchungen gekannt haben.

1921 war Bell Honorarpräsident des zweiten internationalen Eugenikkongresses unter der Schirmherrschaft des "American Museum of Natural History" in New York. Er arbeitete mit den Organisationen zusammen mit dem Ziel, Gesetze zur Verhinderung der Ausweitung von „defekten Rassen“ einzuführen.

George Veditz, Präsident der National Association of the Deaf, nannte Bell 1907 „den Feind, den die tauben Amerikaner am meisten zu fürchten haben“. Alexander Graham Bell haftet damit der Ruf an, die Entwicklung der Gemeinschaft der gehörlosen Menschen und der Gebärdensprache massiv gestört zu haben mit Auswirkungen, die noch heute in vielen Ländern spürbar sind.







</doc>
<doc id="445" url="https://de.wikipedia.org/wiki?curid=445" title="Abu Nidal">
Abu Nidal

Abu Nidal (), eigentlicher Name Sabri Chalil al-Banna (; * Mai 1937 in Jaffa; † 16. August 2002 in Bagdad), war palästinensischer Terrorist und der Gründer der Abu-Nidal-Organisation, einer Abspaltung der PLO im Jahr 1974. Die Terrororganisation Abu Nidals führte über 100 Anschläge in mehr als 20 Ländern aus.

Abu Nidal wurde im Mai 1937 am Hafen von Jaffa an der Küste Palästinas geboren, das damals unter britischer Herrschaft stand. Sein Vater Chalil war ein reicher Geschäftsmann, der sein Geld mit Orangenplantagen machte und seine elf Kinder in einem fünfstöckigen Haus am Strand großzog (heute in Verwendung als israelisches Militärgericht). Chalil verliebte sich in eines seiner Stubenmädchen, eine erst 16-jährige Alawitin und nahm sie entgegen dem Wunsch seiner Familie zu seiner Frau. Sie bekam sein zwölftes Kind, Sabri Chalil al-Banna. Verschiedenen Quellen zufolge war sie seine zweite oder achte Frau.

Verachtet von seinen älteren Halbgeschwistern verlief seine Kindheit sehr unglücklich. Der Vater starb 1945, als Abu Nidal sieben war, woraufhin seine Familie seine ungeliebte Mutter aus dem Haus warf. Obwohl er gemeinsam mit seinen Geschwistern leben durfte, wurde seine Erziehung vernachlässigt. Er entwickelte sich zum Frauenverachter, zwang später seine eigene Frau in die Isolation und verbot auch den Frauen seiner Anhänger jegliche Freundschaften untereinander.

Als der Palästinakrieg zwischen Arabern und Juden ausbrach, verlor seine Familie die Orangenplantagen, da Jaffa im Kriegsgebiet lag. Sie flohen ins Flüchtlingslager Al-Burj nach Gaza, wo sie ein Jahr in Zelten lebten. Danach gingen sie nach Nablus und dann nach Jordanien.

Abu Nidal verbrachte seine Teenagerjahre in Nablus in diversen Jobs. Mit 18 trat er der Baath-Partei bei, die jedoch 1957 verboten wurde. Als Drahtzieher eines fehlgeschlagenen Attentats auf König Hussein floh er nach Saudi-Arabien, wo er sich als Elektriker und Anstreicher niederließ und auch zeitweise im Labor für Aramco arbeitete.

In Riad sammelte er eine kleine Gruppe junger Palästinenser, die sich „Palästina-Geheimorganisation“ nannte, und lernte seine Frau Hiyam Al-Bitar kennen, mit der er den Sohn Nidal und zwei Töchter, Badia und Bissam bekam. Als Israel 1967 den Sechstagekrieg gewann, wurde er angesichts der neuen diplomatischen Lage von den Saudis eingesperrt und gefoltert und schließlich aus dem Land vertrieben. Er zog nach Amman, Jordanien, um und gründete die Handelsfirma Impex, und trat Fatah, Jassir Arafats Partei innerhalb der PLO bei. Impex wurde bald die Plattform für Tätigkeiten der Fatah und Abu Nidal wurde ein wohlhabender Mann, unter anderem als Geflügelexporteur nach Polen. Gleichzeitig diente die Firma als Deckmantel für seine politische Gewalttätigkeit und seine Multimillionen-Dollar-Waffengeschäfte, Söldnertätigkeiten und Schutzschläger.

Impex diente als Treffpunkt für Fatahmitglieder und als deren Finanzkraft. Abu Nidal war in dieser Zeit als sauberer Geschäftsmann bekannt. Während des Schwarzen Septembers in Jordanien zwischen Fedajin und Truppen König Husseins blieb er zuhause, verließ nie sein Büro. Sein Talent für Organisation erkennend, ernannte Abu Ijad ihn 1968 zum Fatah-Repräsentanten in Khartum, Sudan, dann in Bagdad 1970, nur zwei Monate vor dem zweiten Attentat auf König Hussein.

Nidal war inoffizieller Mitarbeiter der DDR-Staatssicherheit.

Kurz vor der PLO-Vertreibung aus Jordanien und während der drei folgenden Jahre begannen sich einige radikale palästinensische und andere arabische Parteien, wie George Habaschs PFLP, DFLP, Arabische Befreiungsfront, Sa'iqa und die Palästinensische Befreiungsfront, von der PLO abzuspalten und starteten ihre eigenen Terrorangriffe gegen israelische militärische und zivile Ziele.

Kurz nach der Vertreibung aus Jordanien fing Abu Nidal an, Kritik an der PLO über„ Stimme von Palästina“, die PLO-eigene Radiostation im Irak zu übertragen und beschuldigte sie der Feigheit, wegen der Zustimmung zu einer Waffenruhe mit König Hussein. Während des dritten Fatah-Kongresses in Damaskus 1971 trat Abu Nidal als Führer eines linksgerichteten Bündnisses gegen Arafat auf. Zusammen mit Abu Daud (einer von Fatahs unbarmherzigsten Kommandanten, der später für die Planung der Geiselnahme von München verantwortlich war, bei dem 11 israelische Athleten im olympischen Dorf in München als Geiseln genommen und getötet wurden) und dem palästinensischen Intellektuellen Nadschi Allusch, erklärte er Arafat zum Feind des palästinensischen Volkes und verlangte mehr Demokratie innerhalb der Fatah sowie Rache gegen König Hussein. Es war Abu Nidals letzter Kongress.

1985 zog Abu Nidal nach Tripolis, wo er mit Gaddafi Freundschaft schloss, der bald sein Partner wurde und auch Gebrauch davon machte. Am 15. April 1986 griffen US-Truppen in der Operation El Dorado Canyon von britischen Stützpunkten aus Tripolis und Bengasi an. Dutzende wurden getötet. Diese Aktion war die Vergeltung für ein Bombenattentat zehn Tage davor auf einen Berliner Nachtclub, der häufig von US-Soldaten besucht wurde. Nach Abu Nidals Tod berichtete Atef Abu Bakr, ein ehemaliges Mitglied der Fatah RC gegenüber Journalisten, dass Gaddafi Abu Nidal gebeten hatte, gemeinsam mit seinem Geheimdienstchef, Abdullah al-Senussi, eine Serie von Racheattentaten gegen britische und US-Ziele zu planen.

Abu Nidal arrangierte die Entführung von zwei Briten und einem Amerikaner, die später ermordet aufgefunden wurden. Danach schlug er Senussi vor, ein Flugzeug zu entführen oder zu sprengen. Am 5. September 1986 entführte ein Fatah RC-Team den Pan Am Flug 73 in Karatschi, 22 Passagiere wurden getötet, etliche verwundet. Im August 1987 ließ er eine Bombe auf einen Flug von Belgrad einer unbekannten Fluglinie schmuggeln, die allerdings nicht explodierte. Verärgert über dessen Versagen befahl Senussi Abu Nidal, eine Bombe zu bauen, die von einem libyschen Agenten an Bord des PanAm Fluges 103 nach New York gebracht wurde. Am 21. Dezember 1988 explodierte das Flugzeug über Lockerbie in Schottland. 259 Menschen an Bord und 11 am Boden wurden getötet.

2000 verurteilte ein schottisches Gericht Abdel Basset Ali al-Megrahi, den ehemaligen Sicherheitschef der Libyan Arab Airlines, für seine Rolle beim Attentat. Er hatte den Koffer in Malta so beschriftet, dass er an Bord der Maschine gelangen würde. Die Beteiligung Abu Nidals wurde niemals bestätigt.

Nach dem Bruch mit Gaddafi, der bemüht war, den Kontakt mit den Vereinigten Staaten und dem Vereinigten Königreich wiederherzustellen und sich vom Terror zu distanzieren, ließ sich Abu Nidal 1999 im Irak nieder. Die irakische Regierung meinte zwar, er sei mit einem gefälschten Pass eingereist, aber ab 2001 lebte er dort offiziell, obwohl er in Abwesenheit in Jordanien zum Tode verurteilt worden war, für seine Rolle bei der Ermordung eines jordanischen Diplomaten 1994 in Beirut.

Am 19. August 2002 berichtete das palästinensische Amtsblatt "Al-Ayyam", dass Abu Nidal mehrfachen Schusswunden erlegen sei. Am 21. August gab der irakische Geheimdienst eine Pressekonferenz, bei der Fotos des blutigen Körpers Abu Nidals und ein Obduktionsbericht präsentiert wurden, die beweisen sollten, dass er an einer einzigen Schussverletzung, einem Kopfschuss durch den Mund, gestorben sei. Laut der Darstellung des Geheimdienstes drangen Agenten in sein Haus ein, um ihn für ein geplantes Attentat an Saddam Hussein zu verhaften. Mit der Bitte, sich etwas anziehen zu dürfen, habe er sich in sein Schlafzimmer zurückgezogen und sich in den Mund geschossen. Nach acht Stunden Intensivbehandlung sei er gestorben. Wie schon längere Zeit bekannt war, litt er an Leukämie.

Anderen Quellen zufolge (s. o.) soll ihn der irakische Geheimdienst schon längere Zeit observiert haben, sie fanden Dokumente über einen US-Angriff auf den Irak. Agenten drangen bereits am 14. in sein Haus ein, wo ein Kampf mit seinen Anhängern ausgebrochen sei. Er floh in sein Schlafzimmer, wo er starb. Unklar ist, ob er sich selbst tötete. Sein Körper wies mehrere Schusswunden auf, so die Quelle.




</doc>
<doc id="446" url="https://de.wikipedia.org/wiki?curid=446" title="Alternativweltgeschichte">
Alternativweltgeschichte

Alternativweltgeschichten sind eine Ausformung des Science-Fiction-Genres und unter den Bezeichnungen "Allohistoria", "Parahistorie", "Virtuelle Geschichte", "Imaginäre Geschichte", "Ungeschehene Geschichte", "Potentielle Geschichte", "Eventualgeschichte", "Alternate History", "Alternative History" oder "Uchronie" bekannt.

In der Geschichtswissenschaft werden derartige Gedankenspiele, die allerdings Bezug auf die historischen Quellen nehmen, als kontrafaktische Geschichte bezeichnet.

Die Geschichten dieser Werke spielen in einer Welt, in der der Lauf der Weltgeschichte irgendwann (am so genannten "Divergenzpunkt") von dem uns bekannten abgewichen ist. Während Science-Fiction mit dem Potentialis operiert, operiert die kontrafaktische Geschichte mit dem Irrealis, stellt also die Frage: „Was hätte sein können, wenn …?“ Das Genre wurde insbesondere in der englischsprachigen Literatur der Nachkriegszeit entwickelt.

Alternativweltgeschichten werden gerne als Mittel der Satire benutzt.

Als Hintergrund für Alternativweltgeschichten dienen historische Ereignisse – zumeist markante wie Schlachten oder politische Entscheidungen –, welche als Ausgangspunkt für einen alternativen Geschichtsverlauf hergenommen werden, indem bestimmte Faktoren dieser Ereignisse verändert oder entfernt werden. Bekannte und oft genutzte Ereignisse sind u. a.:


Darüber hinaus gibt es auch Parallelwelterzählungen, in denen sich beide Möglichkeiten der Geschichte unabhängig voneinander weiterentwickeln und später, meist in der Gegenwart, miteinander in Verbindung treten.











</doc>
<doc id="449" url="https://de.wikipedia.org/wiki?curid=449" title="Ada (Programmiersprache)">
Ada (Programmiersprache)

Ada ist eine strukturierte Programmiersprache mit statischer Typenbindung. Sie wurde von Jean Ichbiah von dem Unternehmen Honeywell Bull in den 1970er Jahren entworfen. Es war die erste standardisierte Hochsprache. Ada ist vom Erscheinungsbild ähnlich zur Programmiersprache Pascal und ist genauso wie Modula-2 als Wirthsche Sprache zu betrachten. Ebenso wie Modula ist Ada aber auch strenger in der Programmierung als Pascal. Benannt wurde die Sprache nach Lady Ada Lovelace (1815–1852), der Tochter von Lord Byron und Mitarbeiterin von Charles Babbage, die auch als erste Programmiererin bezeichnet wird. Die richtige Schreibweise ist daher "Ada" und nicht, wie gelegentlich verwendet, "ADA".

Ada wurde anfänglich stark vom US-Verteidigungsministerium gefördert und unterstützt. Die drei gängigen Versionen sind Ada 83 (das erste standardisierte Ada, das zunächst einfach nur Ada hieß, aber später zur Abgrenzung vom Nachfolger Ada 83 genannt wurde), Ada 95, das um zahlreiche neue Sprachmittel erweitert wurde und Ada 2005 (auch als Ada 05 bekannt), dessen Standardisierungsprozess 2007 abgeschlossen wurde. Ada-Compiler können sich einem standardisierten Test (Validierung) unterziehen, der praktisch Grundvoraussetzung für den professionellen Einsatz ist. Aufgrund der hohen Anforderung, die validierte Compiler erfüllen müssen, hat Ada sich vor allem in sicherheitskritischen Bereichen durchgesetzt, zum Beispiel in der Flugsicherung, in Sicherheits-Einrichtungen der Eisenbahn, in Waffensystemen, der Raumfahrt, der Medizin oder der Steuerung von Kernkraftwerken.

Ada zielte ursprünglich auf eingebettete "(embedded)" und Echtzeit-Systeme "(real-time systems)" und wird auch heute noch oft für diese Zwecke verwendet. Ada 95 – von Tucker Taft von Intermetrics zwischen 1992 und 1995 entworfen – verbesserte die Möglichkeiten zum Entwurf numerischer, systemnaher und bankwirtschaftlicher Programme. Herausragende Merkmale von Ada sind etwa das strenge Typsystem "(starke Typisierung)", zahlreiche Prüfungen zur Programmlaufzeit, Nebenläufigkeit, Ausnahmebehandlung und generische Systeme. Ada 95 führte sogenannte "tagged types" (erweiterbare Typen) ein, die das Ada zugrundeliegende Konzept des Programmieren durch Erweiterung weiter ausbauen und dynamische Polymorphie ermöglichen.

Implementierungen von Ada benutzen üblicherweise keine automatische Speicherbereinigung "(garbage collection)" zur Speicherverwaltung, der Standard "erlaubt" dies jedoch. Ada unterstützt Laufzeittests, um Speicherüberläufe, Zugriff auf nicht zugewiesenen Speicher, "off-by-one-Fehler" und andere, ähnlich geartete Fehler frühzeitig zu erkennen und zu vermeiden. Für eine höhere Effizienz können diese Tests abgeschaltet werden. Auch zur Programmverifikation stehen verschiedene Spracheigenschaften zur Verfügung. Mit Ada ist es auch zum ersten Mal gelungen, Programme automatisch auf Korrektheit zu überprüfen. Dazu wird die Ada-Variante SPARK verwendet. Dies ist eine Untermenge von Ada mit speziellen Annotationen. Die Korrektheit eines SPARK-Programms wird mit einem Verifikationsprogramm "(SPARK Examiner)" durch statische Analyse der Annotationen überprüft.

Für Ada gibt es den quelloffenen Compiler GNAT unter GP-Lizenz. Die Firma AdaCore entwickelt die integrierte Entwicklungsumgebung (IDE) GNAT Programming Studio, die in einer freien und einer kommerziellen Version angeboten wird. Darüber hinaus bieten verschiedene namhafte Hersteller Compiler mit IDEs für Ada an.
Ein Plugin für Eclipse, die GNATbench, wurde zunächst in der kommerziellen Version GNATpro vertrieben, ist mittlerweile aber auch unter einer GPL verfügbar. Daneben gibt es einige kleinere IDEs, die Ada unterstützen und sich vor allem für die Lehre eignen, zum Beispiel jGRASP, oder unter Windows das bekannte AdaGIDE. Außerdem existiert ein emacs-Mode für Ada.

In den 1970ern zeigte sich das Verteidigungsministerium der Vereinigten Staaten besorgt über die wachsende Anzahl von Programmiersprachen, die in seinen Projekten verwendet wurden. Wartung, Ausbildung, Modularität und Wiederverwendung waren dadurch schwer beeinträchtigt. Viele der Programmiersprachen waren zudem proprietär (man war also vom Anbieter abhängig) oder schlicht veraltet. 1975 sollte eine Arbeitsgruppe diesen Dschungel lichten und eine Sprache finden oder erfinden, welche die Bedingungen des Ministeriums erfüllt. Eine Reihe von Anforderungskatalogen, bezeichnet als "Strawmann", "Woodenman", "Tinman", "Ironman", "Sandman" (nicht veröffentlicht) und "Steelman" (später auch noch "Pebbleman" und "Stoneman" für eine integrierte Entwicklungsumgebung), wurden erstellt und viele existierende Sprachen wurden überprüft, doch 1977 kam man zum Ergebnis, dass keine der vorhandenen Sprachen geeignet war. Nach einer Ausschreibung kamen vier Kandidaten in die nähere Auswahl ("Red", "Green", "Blue" und "Yellow" genannt), und im Mai 1979 entschied man sich für "Green" von Jean Ichbiah, welches dann auf den Namen Ada getauft wurde. Die ursprüngliche Beschreibung wurde am 10. Dezember 1980 gebilligt, dem Geburtstag von Lady Ada Lovelace. Der Standard erhielt die Bezeichnung MIL-STD 1815, da 1815 ihr Geburtsjahr war.

Nach der Einführung Adas 1983 fiel die Anzahl der verwendeten Programmiersprachen im Verantwortungsbereich des US-amerikanischen Verteidigungsministeriums bis 1996 von über 450 auf 37. Das US-Verteidigungsministerium schrieb vor, dass jedes Softwareprojekt mit einem Anteil von mehr als 30 % neuem Code in Ada geschrieben werden musste. Diese Vorschrift wurde 1997 aufgehoben, zudem wurden häufig Ausnahmen genehmigt. In vielen anderen Staaten der NATO wurden ähnliche Vorschriften erlassen. 1983 wurde die Sprache zu einer ANSI-Norm (ANSI/MIL-STD 1815), die ISO übernahm die Norm 1987 als ISO-8652:1987. Diese Version wird heute als Ada 83 bezeichnet, nach dem Jahr der ANSI-Normung. Ada 95, die gemeinsame ISO/ANSI-Norm ISO-8652:1995, wurde im Februar 1995 angenommen. Damit wurde Ada 95 zur ersten objektorientierten Programmiersprache mit einer ISO-Norm. Im September 2000 wurde die erste technische Korrektur gemäß den Statuten der ISO als ISO/IEC 8652:1995(E)/Cor.1:2000 angenommen. Zurzeit ist die ISO/ANSI-Norm ISO-8652:1995/AMD 1:2007, informell Ada 2005, der aktuelle Standard.

Um die Verbreitung des Standards und der Sprache allgemein zu unterstützen, finanzierte die US Air Force die Entwicklung des kostenfreien GNAT-Compilers. Die auf Sicherheit ausgelegten Spracheigenschaften von Ada alleine können Fehler nicht verhindern: Ariane V88, die erste Ariane 5, ging im Juni 1996 samt Nutzlast verloren, unter anderem weil ein arithmetischer Überlauf auftrat und für die vom Compiler generierte Ausnahme keine angemessene Ausnahmebehandlung implementiert worden war. Entgegen Bertrand Meyers Behauptung hätte auch die von ihm entwickelte Programmiersprache Eiffel den Verlust der Rakete nicht verhindert. Im April 2008 kam Ada 2005 wieder in die Schlagzeilen, nachdem Lockheed Martin ein Update zum Flugsicherungssystem der Federal Aviation Administration vor der vereinbarten Lieferzeit und unter dem erwarteten Budget abgeliefert hatte.

Das Hallo-Welt-Programm in Ada:

with Ada.Text_IO;

procedure Hallo is
begin
end Hallo;





Tutorial

Referenz und Glossar


</doc>
<doc id="455" url="https://de.wikipedia.org/wiki?curid=455" title="Absinth">
Absinth

Absinth, auch "Absinthe" oder Wermutspirituose genannt, ist ein alkoholisches Getränk, das traditionell aus Wermut, Anis, Fenchel sowie einer je nach Rezeptur unterschiedlichen Reihe weiterer Kräuter hergestellt wird. Bei einer sehr großen Anzahl von Absinthmarken ist die Spirituose von grüner Farbe. Deswegen wird Absinth auch „Die grüne Fee“ () genannt. Der Alkoholgehalt liegt üblicherweise zwischen 45 und 85 Volumenprozent und ist demnach dem oberen Bereich der Spirituosen zuzuordnen. Aufgrund der Verwendung bitter schmeckender Kräuter, insbesondere von Wermut, gilt Absinth als Bitterspirituose, obwohl er nicht unbedingt bitter schmeckt.

Absinth wurde ursprünglich im 18. Jahrhundert im Val de Travers im heutigen Schweizer Kanton Neuenburg ("Canton de Neuchâtel") als Heilelixier hergestellt. Große Popularität fand diese Spirituose, die traditionell mit Wasser vermengt getrunken wird, in der zweiten Hälfte des 19. und dem frühen 20. Jahrhundert in Frankreich. Zu den berühmten Absinth-Trinkern zählen unter anderem Charles Baudelaire, Paul Gauguin, Vincent van Gogh, Ernest Hemingway, Edgar Allan Poe, Arthur Rimbaud, Aleister Crowley, Henri de Toulouse-Lautrec und Oscar Wilde.

Auf dem Höhepunkt seiner Popularität stand das Getränk in dem Ruf, aufgrund seines Thujon-Gehalts abhängig zu machen und schwerwiegende gesundheitliche Schäden hervorzurufen. 1915 war das Getränk in einer Reihe europäischer Staaten und den USA verboten. Moderne Studien haben eine Schädigung durch Absinthkonsum über die Wirkung von Alkohol hinaus nicht nachweisen können; die damals festgestellten gesundheitlichen Schäden werden heute auf die schlechte Qualität des Alkohols und die hohen konsumierten Alkoholmengen zurückgeführt. Seit 1998 ist Absinth in den meisten europäischen Staaten wieder erhältlich. Auch in der Schweiz sind seit 2005 die Herstellung und der Verkauf von Absinth wieder erlaubt.

Außer Wermut ("Artemisia absinthium") enthält in Frankreich und der Schweiz hergestellter Absinth Anis, teilweise ersetzt durch den preisgünstigeren Sternanis, Fenchel, Ysop, Zitronenmelisse und pontischen Wermut. Varianten verwenden auch Angelika, Kalmus, "Origanum dictamnus", Koriander, Veronica, Wacholder, Muskat und verschiedene weitere Kräuter. Wermut, Anis und Fenchel machen den typischen Geschmack des Absinths aus. Die übrigen Gewürze dienen der geschmacklichen Abrundung. Die grüne Farbe, die viele Absinthsorten aufweisen, stammt vom Chlorophyll in pontischem Wermut, Ysop, Melisse und Minze.

Thujon ist ein Bestandteil des ätherischen Öls des Wermuts, das für die Absinthherstellung verwendet wird. Die schädlichen Auswirkungen, die während des Höhepunkts der Absinth-Popularität im 19. Jahrhundert in Frankreich zu beobachten waren und zu denen unter anderem Schwindel, Halluzinationen, Wahnvorstellungen, Depressionen, Krämpfe, Blindheit sowie geistiger und körperlicher Verfall gehörten, wurden auf diese Substanz zurückgeführt. Thujon ist ein Nervengift, das in höherer Dosierung Verwirrtheit und epileptische Krämpfe (Konvulsionen) hervorrufen kann. Aus diesem Grund wurde in der Europäischen Union der Thujongehalt in alkoholischen Getränken begrenzt (5 mg/kg in alkoholischen Getränken mit einem Alkoholgehalt von bis zu 25 % vol. und bis zu 10 mg/kg in alkoholischen Getränken mit einem Alkoholgehalt von mehr als 25 % vol. sowie bis zu 35 mg/kg in Bitter-Spirituosen.)

Die insbesondere in Tierexperimenten des 19. Jahrhunderts beobachtete konvulsive Wirkung des Absinths wird heute auf eine Blockierung von GABA-Rezeptoren und eine Desensibilisierung von Serotonin-5-HT-Rezeptoren durch Thujon zurückgeführt. Es ist jedoch inzwischen widerlegt, dass die im Absinth enthaltene Thujonmenge ausreicht, um in diesem Maße toxisch zu wirken. Als Ursache oder wesentlicher Faktor eher wahrscheinlich ist der im Absinth enthaltene Alkohol. Auch ein möglicher gemeinsamer Wirkmechanismus mit dem Cannabis-Wirkstoff Tetrahydrocannabinol über eine Aktivierung von Cannabinoid-Rezeptoren konnte nicht bestätigt werden. Eine in der Clubszene und in den Medien proklamierte euphorisierende und aphrodisierende Wirkung heutiger Absinthe kann anhand dieser experimentellen Daten nicht auf die in diesen Getränken enthaltene Thujondosis zurückgeführt werden.

Der Absinth des 19. Jahrhunderts hatte entgegen früheren Berichten, die von bis zu 350 Milligramm je Liter sprachen, im Wesentlichen keinen höheren Thujongehalt als die heutigen reglementierten Absinthe. In einer Untersuchung von Absinthen auf Basis historischer Rezepte und Prozesse und von 1930 hergestelltem Absinth konnten nur Thujonmengen von unter 10 mg/kg nachgewiesen werden. Der Thujongehalt kann jedoch höher liegen, wenn Wermutauszüge oder Wermutöle zugesetzt werden. Die Absinthe werden auf diese Weise jedoch sehr bitter.

Der Alkoholgehalt historischer Absinthe lag zwischen 45 und 78 %. In diesem Bereich befinden sich mit wenigen Ausnahmen auch die heute erhältlichen Absinthsorten. Absinth ist aber auch mit einem Alkoholgehalt von bis zu 90 % erhältlich. Wegen des hohen Alkoholgehalts wird Absinth in der Regel verdünnt getrunken.

Historisch belegt sind fünf Qualitätsgrade: "Absinthe des essences" (dt. Absinth-Auszüge), "Absinthe ordinaire" (dt. gewöhnlicher Absinth), "Absinthe demi-fine" (dt. Absinth halb-fein), "Absinthe fine" (dt. Absinth fein) und "Absinthe Suisse" (dt. Schweizer Absinth), wobei "Absinthe des essences" den geringsten Alkoholgehalt und die niedrigste Qualität repräsentiert. "Absinthe Suisse" verweist nicht auf das Herstellungsland, sondern auf einen besonders hohen Alkoholgehalt und hohe Qualität.

Rückblickend wird heute nicht mehr Thujon, sondern der Alkoholgehalt des Absinths als die vorrangige Ursache des im 19. und Anfang des 20. Jahrhunderts verbreiteten "Absinthismus" angesehen. 1914 lag die von erwachsenen Franzosen pro Kopf konsumierte reine Alkoholmenge bei jährlich 30 Litern. Im Vergleich dazu führt heute (2013, laut WHO) Moldawien mit 18,22 Litern reinem Alkohol pro Erwachsenem weltweit die Statistiken des Alkoholkonsums an. Die Symptome des Absinthismus unterscheiden sich nicht von denen eines chronischen Alkoholmissbrauchs (Alkoholismus).

Ein zusätzliches Problem des Absinths des 19. Jahrhunderts war, dass der verwendete Alkohol oft minderwertig war und viel Amylalkohol und andere Fuselöle enthielt. Auch Methanol, das Schwindel, Kopfschmerzen und Übelkeit bewirkt und als Spätfolge Erblindung, Schüttellähmung oder bei einer Überdosis den Tod nach sich zieht, war im damaligen Absinth enthalten. Um dem Absinth seine charakteristische Farbe zu verleihen, wurden bisweilen Zusatzstoffe wie Anilingrün, Kupfersulfat, Kupferacetat und Indigo zugesetzt. Ebenso wurde Antimontrichlorid hinzugefügt, um den Louche-Effekt (die milchige Trübung des sonst klaren Getränks, wenn es mit Wasser verdünnt oder sehr stark gekühlt wird) hervorzurufen. Jedoch lagen die in historischen Proben gefundenen Konzentrationen potenzieller Schadstoffe wie Pinocamphon, Fenchon, Alkoholverunreinigungen, Kupfer- und Antimon-Ionen in einem für den Rückschluss auf Absinthismus unverdächtigen Bereich.

Bei der Herstellung werden Wermut, Anis und Fenchel in Neutralalkohol oder Weinalkohol mazeriert (eingeweicht) und anschließend destilliert. Die Destillation trägt dazu bei, die starken Bitterstoffe des Wermuts abzutrennen. Diese sind weniger flüchtig als die Aromastoffe und bleiben bei der Destillation zurück. Andernfalls wäre das Ergebnis unangenehm bis ungenießbar bitter. Eine unverhältnismäßige Bitterkeit bei Absinth kann ein Indiz dafür sein, dass bei der Produktion auf die Destillation ganz oder teilweise verzichtet wurde, bei der Herstellung Wermutextrakte verwendet wurden und es sich um minderwertigen beziehungsweise unechten Absinth handeln könnte.

Das Destillat kann mit Kräutern wie Pontischem Wermut, Melisse und Ysop eingefärbt werden. Die Färbung durch Kräuter trägt zum geschmacklichen Gesamtbild des Endprodukts bei. Sie stellt hohe Ansprüche an die Fertigkeiten des Herstellers bei der Auswahl der Färbekräuter, ihrem quantitativen Verhältnis und der Dauer der Färbung. Bei altem Absinth kann sich die Färbung des Getränks von einem ursprünglich leuchtenden Grün in ein gelbliches Grün oder Braun wandeln, da sich das Chlorophyll zersetzt. Sehr alte Absinthe sind gelegentlich bernsteinfarben. Klarer Absinth, auch „Blanche“ oder „La Bleue“ genannt, ist typisch für den in illegalen Destillerien in der Schweiz hergestellten Absinth. Der Verzicht auf die normalerweise für Absinth typische Färbung erleichterte den heimlichen Verkauf in Zeiten, in denen Absinth in der Schweiz verboten war.

Heute wird Absinth häufig hergestellt, indem Absinthessenz in hochprozentigen Alkohol gegeben wird. Erst ab der gehobenen Mittelklasse werden klassische Produktionsmethoden wie die Mazeration angewandt. Einige der heute hergestellten Absinthe werden mit Lebensmittelfarbe gefärbt. Es handelt sich auch dabei meist um minderwertige Absinthe mit einem vereinfachten Produktionsprozess, der den Absinth wichtiger geschmacklicher Nuancen beraubt. Neben der „Grünen Fee“ gibt es auch rot, schwarz oder blau gefärbten Absinth. Diese für Absinth ungewöhnliche Färbung geschieht vor allem aus Marketinggründen.

„Absinth“ ist die Eindeutschung des französischen "absinthe", das ursprünglich „Wermut“ bedeutete. Es geht zurück auf die lateinische und die griechische Bezeichnung für Wermut, "absinthium" und "ἀψίνθιον (apsinthion)".

Wermut gehört zur Gattung "Artemisia" (Beifuß), deren Vertreter in den gemäßigten Klimazonen der nördlichen Hemisphäre wachsen. Viele Arten dieser duftenden und häufig insektenabwehrenden Pflanzen haben eine lange Tradition als Heilpflanze. Hinweise auf die Verwendung von Beifuß-Arten zu Heilzwecken finden sich bereits im Papyrus Ebers, der Texte aus der Zeit von 3550 bis 1550 vor Christus enthält. Auch das Alte Testament nimmt an mehreren Stellen Bezug auf die Bitterkeit der "Artemisia"-Kräuter. In deutschen Ausgaben werden die Pflanzen meist mit „Wermut“ übersetzt, obwohl es sich um andere Arten der Gattung "Artemisia" handelt. 2007 haben deutsche Forscher in einer Doppelblind-Studie herausgefunden, dass Wermut eine „signifikante Verbesserung“ bei Patienten mit Morbus Crohn bringe.

Für die Entstehung des Absinths ist die Verwendung von "Artemisia"-Kräutern in Tinkturen und Extrakten von Bedeutung. Sie wird schon von Theophrast und Hippokrates erwähnt. Wermutabkochungen in Wein wurden von Hildegard von Bingen als Entwurmungsmittel empfohlen. Wermutweine, bei denen Wermutblätter gemeinsam mit Trauben vergoren werden, sind für das 16. Jahrhundert belegt. Sie standen in dem Ruf, besonders wirksame Magenmittel zu sein.

Das Rezept für Absinth ist in der zweiten Hälfte des 18. Jahrhunderts im Val-de-Travers des heutigen schweizerischen Kantons Neuenburg (Neuchâtel) entstanden. Für diese Gegend ist der Konsum von Wein, der mit Wermut versetzt wurde, ab 1737 belegt. Während der ursprüngliche Herstellungsort gesichert ist, werden je nach Quelle unterschiedliche Personen als Urheber der ursprünglichen Rezeptur genannt. Der aus politischen Gründen in das preußische Fürstentum geflohene französische Arzt Dr. Pierre Ordinaire, der in Couvet als Landarzt praktizierte, soll einen selbst hergestellten „élixir d’absinthe“ bei seinen Patienten verwendet haben. Nach seinem Tod gelangte das Rezept an die gleichfalls in Couvet ansässige Familie Henriod, die es als Heilmittel deklarierte und über Apotheken verkaufte. Nach anderen Quellen wurde ein Absinthelixir in der Familie Henriod bereits länger hergestellt – ein Wermutelixier sei schon um die Mitte des 18. Jahrhunderts von einer Henriette Henriod destilliert worden. Auch die mit dem Gastwirt Henry-Francois Henriod verheiratete heilkundige Suzanne-Marguerite Motta, „Mutter Henriod“ genannt, wird als Urheberin der Originalrezeptur genannt. Helmut Werner hat in seiner Geschichte des Absinths die These aufgestellt, dass Pierre Ordinaire auf Basis seiner medizinischen Erfahrung lediglich den Herstellungsprozess eines Familienrezeptes der Henriod-Familie optimierte und auf größere Mengen auslegte.

Gesichert ist, dass 1797 ein Major Dubied die Rezeptur von einem Mitglied der Familie Henriod erwarb und mit seinem Sohn Marcellin und seinem Schwiegersohn Henri Louis Pernod eine Absinth-Brennerei gründete. Anfänglich wurden täglich nur 16 Liter produziert, und der größte Teil der Produktion ging ins nahe gelegene Frankreich. Um die umständlichen Zollformalitäten zu umgehen, verlegte Henri Louis Pernod im Jahre 1805 die Destillerie ins französische Pontarlier und produzierte dort anfangs täglich 400 Liter. Sein Erfolg zog das Entstehen einer Reihe weiterer Absinthbrennereien sowohl in Frankreich als auch im Fürstentum Neuenburg nach sich.

1830 besetzte Frankreich Algerien. Die unzureichenden sanitären Einrichtungen führten regelmäßig zu Epidemien unter den französischen Soldaten, die von Militärärzten unter anderem mit einer Mischung aus Wein, Wasser und Absinth bekämpft wurden. Bereits die ersten Schiffe, die nach Algerien übersetzten, hatten Fässer mit Absinth an Bord. Die Soldaten erhielten tägliche Absinthrationen, weil man hoffte, auf diese Weise sowohl die Auswirkungen von schlechtem Trinkwasser als auch die Malaria bekämpfen zu können. Auf die Absinthproduktion zeigte dies deutliche Auswirkungen. Die Firma Pernod steigerte ihre Produktion auf täglich 20.000 Liter, und ihr Konkurrent Berger gründete eine Absinthbrennerei in der Nähe von Marseille, um die Transportwege nach Algerien zu verkürzen.

Aus Algerien zurückkehrende Soldaten machten Absinth in ganz Frankreich bekannt. Populär wurde das Getränk insbesondere in Paris, wo die Kriegsheimkehrer Absinth regelmäßig in den späten Nachmittagsstunden in den Cafés genossen.

Bereits um 1860 war die sogenannte „grüne Stunde“, die „heure verte“ im Alltagsleben französischer Metropolen etabliert. Absinthtrinken zwischen 17 und 19 Uhr galt als chic. Zu seinem Ruf als zeitgemäßes Getränk der späten Nachmittagsstunden trugen auch die zahlreichen Trinkrituale bei. Auf den Tischen der Bars und Cafés der Pariser Boulevards standen häufig hohe Wasserbehälter mit mehreren Hähnen. Ein Absinthtrinker platzierte einen der spatelförmigen und gelochten oder geschlitzten Absinthlöffel auf sein Glas und legte darauf ein Stück Zucker. Dann drehte er einen der Hähne des Wasserbehälters auf, wodurch mit etwa einem Tropfen pro Sekunde Wasser auf den Löffel herabtropfte. Jeder Tropfen gezuckerten Wassers, der in das darunter stehende Absinthglas fiel, hinterließ im Absinth eine milchige Spur, bis schließlich ein Mischungsverhältnis erreicht war, das dem Getränk insgesamt eine milchig-grünliche Färbung verlieh.

Marie-Claude Delahaye gilt in Frankreich als die Historikerin, die sich am intensivsten mit der Geschichte des Absinths auseinandergesetzt hat. In einem Interview mit Taras Grescoe wies sie darauf hin, dass Absinth in mehrfacher Hinsicht eine Neuerung in den französischen Trinkgewohnheiten darstellte. Erstmals tranken Franzosen ein alkoholhaltiges Getränk in größeren Mengen, dessen Geschmack wesentlich von Kräutern bestimmt war und das mit Wasser verdünnt wurde. Absinth war gleichzeitig die erste hochprozentige Spirituose, die Frauen, die nicht zur Halbwelt gehörten, in der Öffentlichkeit konsumieren konnten. Bereits um die Mitte des 19. Jahrhunderts hatte Absinth sich als Getränk der Bohème etabliert.

Absinth war ein verhältnismäßig preisgünstiges Getränk, das selbst nach der Besteuerung durch die französische Regierung preisgünstiger blieb als Wein. Es ließ sich außerdem mit billigem Alkohol aus Zuckerrüben oder Getreide produzieren, und ein einzelnes Glas Absinth konnte wegen seiner Verdünnung mit Wasser über Stunden die Berechtigung erkaufen, in einer der Bars auszuharren. Mit etwa drei Sous pro Glas konnten sich nicht nur Künstler dieses Getränk erlauben, sondern auch Arbeiter. Für viele von ihnen wurde es zur Gewohnheit, nach Beendigung ihrer Arbeit in eine der Bars einzukehren und Absinth zu trinken. Angesichts beengter Wohnverhältnisse und eines sehr geringen Freizeitangebots war die Einkehr in eine Bar eine der wenigen Unterhaltungsmöglichkeiten. Daraus erklärt sich, dass es in Paris zur Wende des 20. Jahrhunderts 11,5 Kneipen je 1000 Einwohner gab. 1912 betrug der Jahreskonsum von Absinth in Frankreich 221,9 Mio. Liter.

Absinthgenuss wird bis heute mit der französischen Kunstszene dieser Zeit verbunden. So schreiben Hannes Bertschi und Marcus Reckewitz: „Es scheint, als sei die gesamte europäische Elite der Literatur und der bildenden Künste im Absinthrausch durch das ausgehende 19. und beginnende 20. Jahrhundert getorkelt.“ Vereinsamte, heruntergekommene Absinthtrinker waren immer wieder Motive der damaligen Malerei und der Literatur. Édouard Manets Gemälde "Der Absinthtrinker", das um 1859 entstand, erregte mit dem Sujet eines verwahrlosten Alkoholikers großen Anstoß und wurde vom Auswahlkomitee des Pariser Salons abgelehnt. Die literarische Vorlage zu dem Gemälde war ein Gedicht von Charles Baudelaire, der selbst Absinth in großen Mengen konsumierte und so versuchte, durch Syphilis verursachte Schmerzen und Schwindelgefühle zu bekämpfen. Weitere frühe Darstellungen sind die Karikaturen "Le premier verre, le sixième verre" von Honoré Daumier und "L’Èclipse" von André Gill. Edgar Degas’ Gemälde "Der Absinth" von 1876 zeigt ein sich nicht mehr wahrnehmendes, apathisch nebeneinander sitzendes Paar in einer der französischen Bars.

Neben Camille Pissarro und Alfred Sisley gehörte auch Henri Toulouse-Lautrec zu den bekannten Absinthtrinkern, der seinen Malerkollegen Vincent van Gogh 1887 in einem Café mit einem Glas Absinth porträtierte. Im selben Jahr entstand dessen "Stillleben mit Absinth". Ein Beleg für die Verbreitung des Getränkes außerhalb von Paris ist sein in Arles entstandenes Gemälde "Nachtcafé an der Place Lamartine", das ebenso wie Paul Gauguins "Dans un café à Arles" Barbesucher beim Absinthkonsum zeigt. Zu Beginn des 20. Jahrhunderts wählte Pablo Picasso wiederholt Absinthtrinker als Motiv. Neben verschiedenen Bildern seiner "Blauen Periode", wie zum Beispiel "Buveuse assoupie" aus dem Jahr 1902, entstand 1911 das kubistische Gemälde "Das Glas Absinth" und 1914 eine Skulptur mit gleichem Titel. Ebenfalls aus dieser Zeit stammen die Bilder "Der Absinthtrinker" des tschechischen Malers Viktor Oliva sowie "The Absinthe Drinker" des irischen Künstlers William Orpen.

Beispiele für Absinth in der Literatur sind "À Rebours" von Joris-Karl Huysmans, "Lendemain" von Charles Cros und "Comédie de la Soif" von Arthur Rimbaud. Dieser wurde am 10. Juli 1873 von seinem betrunkenen Liebhaber Paul Verlaine angeschossen, was möglicherweise auf übermäßigen Absinthkonsum zurückzuführen ist. Oscar Wilde beschrieb Absinth mit den poetischen Worten: „Absinthe has a wonderful color, green. A glass of absinthe is as poetical as anything in the world.“ Zugleich gab er zu bedenken:
„Nach dem ersten Glas sieht man die Dinge so, wie man sie gern sehen möchte. […] Am Ende sieht man die Dinge so, wie sie sind, und das ist das Entsetzlichste, das geschehen kann.“

Auch wenn die Darstellungen in Kunst und Literatur häufig explizit Absinth nennen, spiegeln sich in den Gemälden und Romanen die alkoholbedingten Probleme einer Gesellschaft, in der traditionell Wein konsumiert wurde und hochprozentige Alkoholika bis zur Einführung von Absinth verhältnismäßig selten genossen wurden. Rund 100 Jahre zuvor hatte ein sprunghaft angestiegener Branntweinkonsum in Großbritannien vergleichbare soziale Probleme geschaffen und war nur durch gesetzliche Maßnahmen wieder auf verträglichere Maße rückführbar gewesen.

Bereits um das Jahr 1850 wurden Sorgen über die Folgen des Langzeit-Absinth-Konsums laut. Dieser führe zu "Absinthismus". Als Symptome galten Abhängigkeit, Übererregbarkeit und Halluzinationen. Nachdem Émile Zolas 1877 veröffentlichter Roman "L’assommoir" (dt. "Der Totschläger") auf die gravierenden sozialen Folgen des Alkoholismus aufmerksam gemacht hatte, hatten eine Reihe von Antialkoholikervereinigungen versucht, Absinth verbieten zu lassen – verschiedentlich gemeinsam mit den Weinproduzenten. 1907 gingen 4000 Demonstranten in Paris unter dem Slogan „Tous pour le vin, contre l’absinthe“ (Alle für den Wein und gegen den Absinth) auf die Straße. Wein galt im Frankreich jener Zeit als gesundes Getränk und Grundnahrungsmittel. „Absinth macht kriminell, führt zu Wahnsinn, Epilepsie und Tuberkulose und ist verantwortlich für den Tod tausender Franzosen. Aus dem Mann macht Absinth ein wildes Biest, aus Frauen Märtyrerinnen und aus Kindern Debile, er ruiniert und zerstört Familien und bedroht die Zukunft dieses Landes“, zitiert Barnaby Conrad in seiner Geschichte des Absinth die damaligen Kritiker. Auch Zola beschrieb in seinem einflussreichen Roman Schnaps als ein menschenverderbendes Getränk, Wein dagegen als das Recht des Arbeiters. Unterstützung fand diese Sichtweise auch bei Medizinern. Alkoholismus war in Frankreich erstmals in den 1850er Jahren wissenschaftlich beschrieben worden. Französische Mediziner hatten um 1900 bei den billigen Absinthmarken, die im kalten Auszugsverfahren hergestellt wurden, besonders viele Schadstoffe festgestellt. Auch aus ihrer Sicht war Absinth das erste Getränk, das verboten werden sollte.

Ein spektakulärer Mordfall im August des Jahres 1905 in der Waadtländer Gemeinde Commugny, der europaweit ausführlich in den Medien dargestellt wurde, war der letzte Anstoß, Herstellung und Verkauf von thujonhaltigen Getränken in den meisten europäischen Ländern und den USA gesetzlich zu verbieten.

Der Weinbergarbeiter Jean Lanfray war starker Alkoholiker, der bis zu fünf Liter Wein pro Tag trank. An dem Tag, an dem er seine schwangere Frau, seine zweijährige Tochter Blanche und seine vierjährige Tochter Rose in einem Wutanfall ermordete, hatte er neben Wein auch Branntwein sowie zwei Gläser Absinth zu sich genommen. In der Verbotsdebatte, an der sich auch Weinproduzenten lebhaft beteiligten, konzentrierte man sich auf den Absinthgenuss, der dem Mord vorausgegangen war. In Belgien nahm man den Vorfall zum Anlass, noch im selben Jahr Absinth zu verbieten. In der Schweiz wurde das Absinth-Verbot im Jahre 1910 aufgrund einer Volksinitiative, bei der sich am 5. Juli 1908 63,5 Prozent der abstimmenden männlichen Bevölkerung dafür aussprachen, in die Verfassung aufgenommen. Das Verbot trat am 7. Oktober 1910 in Kraft. In Frankreich ließ man sich mit dem Verbot bis 1914 Zeit. Ob sich das Verbot von Absinth positiv auf die französische Volksgesundheit auswirkte, lässt sich nicht mehr feststellen. Mangelnde Gesundheitsstatistiken und die Zäsur des Ersten Weltkriegs verhindern entsprechende Analysen.

Zu den heutigen EU-Ländern, die sich dem Absinth-Verbot zu Beginn des 20. Jahrhunderts nicht anschlossen, zählten lediglich Spanien und Portugal. Auch in Großbritannien, wo Absinth im 19. Jahrhundert nur ein Nischendasein fristete, blieb zumindest der Verkauf erlaubt. Das Verbot des Absinth führte in Frankreich zu einer wachsenden Popularität des Absinthsubstituts Pastis, für dessen Herstellung kein Wermut verwendet wird, das aber ebenfalls mit Wasser verdünnt in den Nachmittagsstunden genossen wird.

Nach dem französischen Verbot verlegte die Firma Pernod, einer der größten französischen Absinth-Hersteller, ihre Absinth-Produktion zunächst nach Spanien, konzentrierte sich aber dann auf die Herstellung von Anis-Schnäpsen. Das Val-de-Travers dagegen, das ursprüngliche Herstellungsgebiet, litt stärker unter dem Verbot. Über ein Jahrhundert hatte man in dem eher ärmlichen Tal vom Wermutanbau, dem Verkauf des getrockneten Krauts sowie der Absinth-Destillation gelebt. Nach dem Verbot ließ die Schweizer Regierung die Wermut-Felder unterpflügen. Die Destillation wurde jedoch heimlich weitergeführt – die Einwohner des Tales legen Wert darauf, auf eine 250-jährige ununterbrochene Geschichte der Absinth-Produktion verweisen zu können.

Die Zahl der Destillerien, die im Val-de-Travers illegal Absinth herstellten, schätzen Interviewpartner des Autors Taras Grescoe zu Beginn des 21. Jahrhunderts auf etwa sechzig bis achtzig. Ähnlich wie den illegal gebrannten Moonshine der USA, den norwegischen Hjemmebrent oder den irischen Poteen, umgibt auch den illegalen Schweizer Absinth eine reiche Folklore. Berthe Zurbuchen, eine Berühmtheit des Val-de-Travers, die achtzig Jahre lang illegal Absinth brannte und in einem Schauprozess in den 1960er Jahren zu 3000 Franken Strafzahlung verurteilt wurde, soll ihren Richter nach dem Urteilsspruch gefragt haben, ob sie sofort zahlen solle oder erst, wenn er das nächste Mal vorbeikäme, um sich seine wöchentliche Flasche abzuholen. Nach der Verurteilung strich sie ihr Haus demonstrativ absinthgrün. Im Jahr 1983 servierte man anlässlich eines Staatsbesuchs dem französischen Präsidenten François Mitterrand ein mit Absinth glasiertes Soufflé. Für den Restaurantbesitzer führte die demonstrative Verwendung des illegalen Absinths zu einer Hausdurchsuchung und einer viertägigen Gefängnisstrafe auf Bewährung. Der Vertreter des Kantons, Pierre-André Delachaux, der das absinthglasierte Soufflé angeregt hatte, entging nur knapp einem erzwungenen Rücktritt von seinem Amt und dem Ende seiner politischen Karriere.

Ab 2001 wurde im Val-de-Travers neben illegalem Absinth auch eine legale Absinth-Variante produziert. Dies war möglich, weil der Alkohol- und Thujongehalt soweit reduziert wurde, dass dieses Produkt laut Gesetz kein Absinth mehr war. Noch vor der Absinth-Legalisierung in der Schweiz am 1. März 2005 bemühte man sich, Absinth als intellektuelles Gut des Val-de-Travers unter dem IGP, dem „indication géographique protégée“ schützen zu lassen. Man ist dabei in direkter Konkurrenz zu den Nachbarn auf der französischen Seite der Grenze, die gleichermaßen versuchen, eine „appellation d’origine réglementée“ zu erhalten. Unabhängig davon, wer in diesem Wettstreit erfolgreich sein wird, könnten nach einer Verleihung nur noch solche Produkte die Bezeichnung Absinth tragen, die aus dieser Region des Jura stammen und bei deren Herstellung bestimmte Qualitätsstandards eingehalten wurden.

So schreibt Grescoe in seinem Essay "Absinthe Suisse – One glass and You are Dead". Auch die im Internet verfügbaren Rezepte für die Heimherstellung von Absinth können als Indiz dafür gewertet werden, dass das Verbot zum Mythos dieses Getränks beigetragen hat. Für andere, einst populäre Getränke wie etwa Veilchen- oder Vanillelikör lässt sich keine auch nur annähernd vergleichbare Fülle an Rezepturen finden.

Eine breite öffentliche Wahrnehmung der Spirituose Absinth setzte ein, als ein auf alkoholische Getränke spezialisierter Importeur in den 1990er Jahren bemerkte, dass es in Großbritannien keine spezifische Gesetzgebung gab, die den Verkauf von Absinth untersagte. Hill’s Liquere, eine tschechische Brennerei, begann für den britischen Markt Hill’s Absinth herzustellen – ein Getränk, von dem Taras Grescoe behauptet, es wäre nichts anderes als ein hochprozentiger Wodka, den man mit Lebensmittelfarbe eingefärbt habe. Der beginnende Wiederausschank von Absinth wurde von einer Reihe von Artikeln in Lifestyle-Magazinen begleitet, die sich über seine gerne kolportierte halluzinogene und erotisierende Wirkung, das in vielen Ländern geltende Absinth-Verbot, van Goghs angeblich absinthinduzierte Selbstverstümmelung und die elaborierten Trinkrituale ausließen. Diese breite Medienabdeckung lässt sich auch in allen anderen europäischen Ländern beobachten, die in den Folgejahren den Ausschank von Absinth wieder erlaubten. Selbst Filme griffen Absinth als epochentypisches Ausstattungsmerkmal auf, so 1992 in Bram Stoker’s Dracula. 2001 berauscht sich Johnny Depp im Film From Hell auf seiner Jagd nach Jack the Ripper an Opium und smaragdgrünem Absinth. Beides gemeinsam schuf eine neue Nachfrage nach diesem Getränk, die Importeure und Brennereien länderspezifische Gesetzgebungen überprüfen ließ. In den Niederlanden ist der Verkauf von Absinth beispielsweise seit Juli 2004 wieder erlaubt, nachdem der Amsterdamer Weinhändler Menno Boorsma erfolgreich gegen das Verbot geklagt hatte.

Klagen gegen ein Absinth-Verbot hatten in der EU generell große Aussicht auf Erfolg, da sowohl Spanien als auch Portugal die Absinth-Herstellung und den Verkauf erlaubten. Einige Länder wie etwa Belgien hoben ihr Absinthverbot mit dem Hinweis auf eine ausreichende EU-Gesetzgebung auf, ohne durch Klagen dazu gezwungen worden zu sein. In Deutschland, wo seit 1923 nicht nur die Herstellung von Absinth, sondern sogar die Verbreitung von Rezepten zur Herstellung verboten waren, war bereits 1981 das Verbot wegen bestehender EWG-Regelungen soweit gelockert worden, dass ein Thujongehalt von bis zu 10 mg/l erlaubt war. Am 29. Januar 1998 wurden die gesetzlichen Bestimmungen an das EU-Recht angeglichen. Seitdem ist in Deutschland wie den meisten EU-Ländern Absinth mit einem Thujonanteil erlaubt, der je nach Alkoholgehalt bei bis zu 35 mg/kg liegen kann. 1999 strich die Schweiz das 1910 in die Verfassung aufgenommene Absinth-Verbot. In den USA fiel das Verbot 2007 mit diversen Auflagen. Frankreich hob 2011 das seit 1915 geltende Absinth-Verbot auf.

Ähnlich den Anis-Spirituosen Pastis, Rakı oder Ouzo wird Absinth grundsätzlich nicht pur getrunken, sondern mit Wasser verdünnt. Die klare Flüssigkeit opalisiert dabei, das heißt, sie trübt sich milchig ein. Dieses Phänomen wird Louche-Effekt genannt. Ursache des Effekts ist die schlechte Wasserlöslichkeit des im Absinth enthaltenen ätherischen Öls Anethol. Die verschiedenen Trinkrituale, die sich rund um den Absinth entwickelt haben, werden "Französisches Trinkritual", "Schweizer Trinkweise" und "Tschechisches" oder "Feuerritual" genannt. Ihnen allen ist eigen, dass der Absinth im Verhältnis zwischen 1:1 bis 1:5 mit Eiswasser vermischt wird. Die meisten Absinthtrinker wählen ein Mischungsverhältnis von mindestens 1:3.

Die Schweizer Trinkweise ist dabei die am wenigsten etablierte. Bei ihr werden lediglich zwei bis vier Zentiliter Absinth mit kaltem Wasser vermischt. Auf Zucker wird verzichtet, da die in der Schweiz getrunkenen Absinthe grundsätzlich weniger bitter waren als die französischen.

Das Feuerritual, auch tschechische Trinkweise genannt, ist historisch nicht mit dem Absinthkonsum verbunden. Es wurde in den 1990er Jahren von tschechischen Absinthproduzenten entwickelt, um den Genuss des Getränks attraktiver zu machen. Dazu werden ein bis zwei mit Absinth getränkte Würfelzucker auf einen Absinthlöffel gelegt und angezündet. Sobald der Zucker karamellisiert und Blasen wirft, werden die Flammen gelöscht und der Zucker erst dann in den Absinth gegeben. Geraten noch brennende Zuckerstücke in das Glas, besteht die Gefahr, dass sich der darin befindliche Absinth entzündet. Auch hier wird der Absinth in einem Verhältnis von 1:3 bis 1:5 mit Eiswasser vermischt.

Das französische Trinkritual besitzt dagegen eine historisch belegbare Tradition. Absinth wurde im 19. Jahrhundert bis hin zum Verbot zu Beginn des 20. Jahrhunderts in Frankreich auf diese Weise genossen. Ähnlich wie beim Feuerritual wird der Absinth mit Zucker getrunken. Dazu werden ein oder zwei Stück Würfelzucker auf einem Absinthlöffel platziert und sehr langsam kaltes Wasser über den Zucker gegossen oder geträufelt. Das Mischungsverhältnis liegt bei 1:3 bis 1:5.

Für das Hinzugeben des Wassers kann auf eine Absinthfontäne oder einen speziellen Glasaufsatz – das Brouille – zurückgegriffen werden. Bei einer Fontäne wird der Zucker auf dem Absinthlöffel durch einen dünnen Strahl aus den Fontänenhähnen aufgelöst. Das Brouille wird hingegen direkt auf das Absinthglas gesetzt, so dass der Absinth ohne Absinthlöffel zubereitet wird. Durch ein kleines Loch im Glasaufsatz strömt das Wasser in das darunter befindliche Absinthglas.

Deutsch 

Englisch

Französisch



</doc>
<doc id="458" url="https://de.wikipedia.org/wiki?curid=458" title="Artikel">
Artikel

Artikel (aus lateinischen "", wörtlich für „Gelenk“ oder „Glied“ und übertragen „Abschnitt“ oder „Teil [beispielsweise eines Textes]“) bezeichnet:

Siehe auch:



</doc>
<doc id="460" url="https://de.wikipedia.org/wiki?curid=460" title="Brandenburg (Begriffsklärung)">
Brandenburg (Begriffsklärung)

Brandenburg steht für:

Staatswesen (in zeitlicher Reihenfolge):

Orte:

Berge:

Asteroid:

Bauwerke:

Adelsgeschlechter:

Handelsschiffe:

Kriegsschiffe:

Militaria:
Siehe auch:



</doc>
<doc id="461" url="https://de.wikipedia.org/wiki?curid=461" title="Bundesrepublik">
Bundesrepublik

Bundesrepublik steht für:



</doc>
<doc id="462" url="https://de.wikipedia.org/wiki?curid=462" title="Beruf">
Beruf

Ein Beruf ist die im Rahmen einer arbeitsteiligen Wirtschaftsordnung aufgrund besonderer Eignung und Neigung systematisch erlernte, spezialisierte, meistens mit einem Qualifikationsnachweis versehene, dauerhaft und gegen Entgelt ausgeübte Betätigung eines Menschen. Der Begriff ist abzugrenzen vom umgangssprachlichen Ausdruck Job, der eine Erwerbstätigkeit bezeichnet, die nur vorübergehend ausgeübt wird oder nicht an eine besondere Eignung oder Ausbildung gebunden ist.
Beruf geht auf „berufen“ (mhdt. "beruofen") zurück, einer Präfixbildung des Verbs „rufen“.

Die Ständelehre des Mittelalters kannte die „vocatio interna“ und die „vocatio externa“. Im Mittelalter betrachteten insbesondere Theologen den Beruf unter zwei Teilaspekten, dem „inneren Beruf“ () und dem „äußeren Beruf“ (). Martin Luther übersetzte das lateinische "vocatio" als die Berufung durch Gott. „Jeder bleibe in dem Beruf, in dem ihn Gottes Ruf traf“ oder „Jeder bleibe in der Berufung, in der er berufen wurde“ (). Er verwendete das Wort Beruf auch für den Stand, das Amt und die Arbeit des Menschen in der Welt. Luther hatte beide Aspekte zusammengefasst, weil für ihn Christen bei jeder Tätigkeit einer inneren und äußeren Berufung folgten. Diese innere Tätigkeit mache jede Tätigkeit, auch die in der Familie, zum Beruf. "Vocatio interna" ist die von Gott ausgehende innere Berufung einer Person zum heiligen Amt (Priester oder Mönch), die durch Gisbert Voetius in seiner „Politica ecclesiastica“ (1663–1676) neues Gewicht erhielt. Die innere Berufung ist das eingenommene geistliche Amt, die äußere Berufung betraf weltliche Berufsstände.

Im Rahmen der späteren Säkularisierung verschwanden die religiösen Bestandteile, während die soziale Verpflichtung im Rahmen der Arbeitsteilung erhalten blieb. Über Beruf und Berufsausbildung wurden in den Zünften die handwerklichen Aktivitäten gesteuert und die ständische Gesellschaftsordnung repräsentiert. Erst seit dem Übergang in das 19. Jahrhundert erhält der Begriff Beruf jenen Inhalt einer eine fachliche Qualifikation voraussetzenden, in der Regel mit einem Erwerbseinkommen verbundenen Tätigkeit. Beruf ist „der Kreis von Tätigkeiten mit zugehörigen Pflichten und Rechten, den der Mensch im Rahmen der Sozialordnung als dauernde Aufgabe ausfüllt und der ihm zumeist zum Erwerb des Lebensunterhaltes dient“. Der Soziologe Max Weber sieht 1925 im industriellen Beruf die „Spezifizierung, Spezialisierung und Kombination von Leistungen“, die für Personen die „Grundlage einer kontinuierlichen Versorgungs- und Erwerbschance“ bildeten. Seit Webers Definition werden Berufe amtlich erfragt und in Statistiken veröffentlicht. Die amtliche deutsche Statistik versteht unter Beruf „die auf Erwerb gerichteten, besondere Kenntnisse und Fertigkeiten sowie Erfahrung erfordernden und in einer typischen Kombination zusammenfließenden Arbeitsverrichtungen … und die in der Regel auch die Lebensgrundlage für ihn und seine nicht berufstätigen Angehörigen bilden.“

Berufsinhalt sind neben der Einkommenserzielung und dem Erwerb von Rentenansprüchen auch der persönliche Lebensinhalt, Interessen, Wertvorstellungen und Ziele, die spezifische gesellschaftliche Wertschätzung und das soziale Ansehen. Berufe und Berufsinhalte unterliegen heute einem mehr oder weniger starken Wandel insbesondere hinsichtlich der Arbeitsbedingungen. Die Berufsausbildung war ursprünglich so gestaltet, dass der Mensch den einmal erlernten Beruf sein gesamtes Berufsleben ausüben sollte. Technischer Fortschritt, ökonomischer Wandel und zunehmende Arbeitsteilung haben jedoch weltweit dazu geführt, dass ganze Berufsgruppen überflüssig wurden und der Beruf als „Lebensaufgabe“ nicht mehr Begriffsinhalt darstellt. Das hängt zusammen mit dem Wandel von der Berufsorientierung hin zur Prozessorientierung, der durch die Veränderung der Berufsbilder und -anforderungen zum Berufswechsel und Umschulung zwingen kann.

Die zur Ausübung eines Berufs erforderlichen Fähigkeiten und Kenntnisse werden durch Ausbildung, Praxis oder Selbststudium erworben. Die Aufnahme in einen Berufsstand kann aber auch erfolgen durch Zuschreibung ("adscription"), etwa bei Erbfolge (z. B. als Bauer, zünftiger Handwerker), durch Gelöbnisse (Soldaten), Diensteide (Beamte) oder durch Ordination (Geistlicher).

Die meisten Berufe sind das Ergebnis fortschreitender Differenzierung der Arbeit. Sie haben häufig eine jahrhundertelange Tradition, da viele von der Gesellschaft benötigte oder gewünschte Leistungen im Wesentlichen konstant geblieben sind. Daher rührt auch die auffällige soziale Erscheinung der "Berufsvererbung".

Zu den ältesten, frühgeschichtlichen Berufen gehören Schmied, Zimmermann, Heiler, Priester, Wandererzähler und -sänger und Wächter. Seit dem Mittelalter fanden sich viele Berufsgruppen in Zünften und Gilden zusammen, die auch die Ausbildung des beruflichen Nachwuchses übernahmen. Auch „unehrliche Berufe“ bildeten eigene Organisationen. Die Ständeliteratur verzeichnet entsprechend der Ständeordnung eine sich innerhalb der Frühen Neuzeit etablierte Vielfalt der Berufe mit ganz unterschiedlichen Qualifizierungs- und Tätigkeitsmerkmalen sowie Rahmenbedingungen. Die Komplexität der Berufskonzepte steigert sich entsprechend dem wissenschaftlich-technischen Fortschritt.

In einigen Berufen wird auf die sogenannte "Berufung" des/der Einzelnen besonderer Wert gelegt (zum Beispiel Pfarrer, Priester, aber auch Arzt, Lehrer, Apotheker, Richter). Ein Wissenschaftler erhält einen sogenannten "Ruf" auf eine Professorenstelle, wenn die Hochschule ihn gern in ihrem Kollegenkreis hätte. Bemerkenswert in diesem Zusammenhang ist die Pflicht zur Arbeit im Rahmen des Ordo Socialis.

Das Grundgesetz garantiert das Grundrecht der freien Berufswahl, denn alle Deutschen haben das Recht, Beruf, Arbeitsplatz und Ausbildungsstätte frei zu wählen ( Abs. 1 GG). Unter Beruf versteht man verfassungsrechtlich jede auf Dauer angelegte, der Einkommenserzielung dienende menschliche Betätigung. Dem verfassungsrechtlichen Berufsbegriff sind zwei Elemente immanent, nämlich "Lebensaufgabe" und "Lebensgrundlage". Für den Beruf als Lebensaufgabe ist wesentlich, dass jemand eine innere Beziehung zu seinem Beruf hat, für den er sich verpflichtet und verantwortlich fühlt. Lebensgrundlage setzt wiederum voraus, dass ein Beruf für eine gewisse Dauer gegen Entgelt ausgeübt wird. Gerhard Pfennig erklärt den Berufsstatus am Beispiel der Soldaten und verweist darauf, dass wehrpflichtige Soldaten lediglich eine öffentliche Dienstleistung erfüllten. Der soldatische Dienst als Beruf komme lediglich für Berufssoldaten in Frage, deren Wehrdienst durch die soldatische Laufbahn zu einem Beruf geworden sei. Der Begriff des Berufs ist dabei nicht auf bestimmte traditionelle oder rechtlich fixierte Berufsbilder beschränkt, sondern umfasst jede frei gewählte Form der (erlaubten) Erwerbstätigkeit und ist daher für die Entwicklung neuer Berufsbilder offen.

Einem Beruf ist also eine nicht nur kurzfristige Tätigkeit immanent; ebenso muss er auf Einkommenserwerb abzielen (Erwerbstätigkeit). Der Begriff Einkommen ist weit auszulegen, hierunter können neben dem typischen Geldeinkommen auch Naturalleistungen (Deputatlohn wie freie Wohnmöglichkeit, Speisen und Getränke) verstanden werden.

Man unterscheidet den "erlernten" und den "ausgeübten" Beruf. Der erlernte Beruf beruht auf einer abgeschlossenen Berufsausbildung und urkundlich bestätigtem Qualifikationsnachweis. Ausgeübter Beruf ist die von einem Arbeitnehmer tatsächlich verrichtete Tätigkeit, für welche keine abgeschlossene Berufsausbildung nachgewiesen werden kann. Wer den Beruf ausübt, für den er eine Berufsausbildung abgeschlossen hat, ist im erlernten Beruf tätig. Wer hingegen in einem Beruf tätig ist, den er ursprünglich nicht erlernt hat, arbeitet in einem ausgeübten Beruf. Freie Berufe sind überwiegend selbständige Tätigkeiten, die nicht der Gewerbeordnung unterliegen. Auch diese sind auf eine gewisse Dauer angelegt und beruhen auf fachlicher Vorbildung, unterscheiden sich von den anderen Berufen jedoch durch wirtschaftliche Selbständigkeit.

Ein Berufstätiger kann sich sowohl mit seinem Arbeitgeber als auch mit seinem Beruf identifizieren. Wird eine Tätigkeit als wichtig für den Selbstwert einer Person erachtet, spricht man von der Identifikation mit dem Beruf („“). Eine hohe Berufsidentifikation kann zu höheren Zielen bei der Arbeitsleistung beitragen. Während die Arbeit an die technisch-wirtschaftliche Dimension des Leistungsvollzugs anknüpft, kennzeichnet der Berufsbegriff deren qualitative Voraussetzungen sowie deren soziale Integration und die daraus resultierende Identitätsfindung.

Der Beruf ist auch ein bedeutender Mechanismus für den sozialen Status einer Person. Dabei gilt der berufliche Status in modernen Gesellschaften als der beste Einzelindikator für den sozialen Status einer Person. Das Prestige von Berufen hängt von der Qualifikation und dem erzielten Einkommen ab. Der formale Status ergibt sich aus der Einteilung in Arbeiter, Angestellte, Beamte und Selbständige, während der materielle Status meist auf die Einkommenshöhe reduziert wird.

Die Angabe des Berufs ist in allen Statistiken und Erhebungen zum Arbeitsmarkt oder zur sozioökonomischen Lage weltweit unverzichtbar. Der Beruf ist nach wie vor ein dominierender Aspekt in der Beschreibung von Arbeitsmarktentwicklungen. Auch in der Vermittlungsarbeit der Arbeitsämter hat die Angabe des Berufs eine zentrale Bedeutung. Eine Berufsklassifikation schafft für die Vermittlung die Möglichkeit, über sinnvolle und praxisgerechte Zusammenfassungen von ähnlichen beruflichen Tätigkeiten zu verfügen und anzuwenden.

International verwendet die Internationale Standardklassifikation der Berufe (ISCO) seit 1957 ein Schema für die Eingruppierung von Berufen. In Deutschland nutzt die Bundesagentur für Arbeit seit Januar 2011 eine mit diesem Schema weitgehend kompatible Neusystematisierung der Berufe, die auch vom Statistischen Bundesamt als Klassifizierung der Berufe übernommen wurde.

Heute wird die Berufsausbildung (Inhalte, Dauer) in den meisten europäischen Ländern staatlich festgelegt. Die staatliche Reglementierung der Berufswahl findet aber in Deutschland und den meisten anderen Ländern ihre Grenzen im Grundrecht der Berufsfreiheit.

Wer welchen Beruf ausüben darf, wurde und wird in verschiedenen Kulturkreisen unterschiedlich gehandhabt. In Europa gilt prinzipiell das Recht der freien Berufsausübung, das jedoch einigen Einschränkungen unterliegt. Bei so genannten reglementierten Berufen ist für die Ausübung eine entsprechende Ausbildung und Qualifikation erforderlich: Als Arzt oder Rechtsanwalt darf beispielsweise nur tätig sein, wer ein medizinisches bzw. juristisches Hochschulstudium erfolgreich abgeschlossen, entsprechende Praxiserfahrung (Referendariat) nachweisen kann und die Zulassung einer Ärztekammer oder Rechtsanwaltskammer besitzt. Ebenfalls unterliegt die Ausübung handwerklicher Berufe bestimmten Einschränkungen: So ist beispielsweise zur selbständigen Ausübung eines Handwerks in Deutschland ein Fachschulabschluss zum Staatliche geprüfter Techniker, Abschluss zum Handwerksmeister (Meisterbrief) oder Hoch- bzw. Universitätsabschluss erforderlich. (Novellierung der Handwerksordnung § 7.2)

Erfolgreich sozial herausgebildete Berufe entwickeln eine mehr oder minder ausgeprägte Berufsethik.

Die Abgrenzung zum Job wird meist durch den Aspekt der Dauerhaftigkeit vorgenommen. Job ist eine temporäre, kurzfristige Tätigkeit ohne innere Beziehung und Verantwortung zur Tätigkeit, eine Gelegenheitsarbeit. Das kommt beim Wort „jobben“ zum Ausdruck, mit dem eine vorübergehende Tätigkeit zwecks Einkommenserzielung umschrieben wird. Diese Abgrenzung findet auch in angelsächsischen Ländern statt, wo bei Beruf von „profession“ (lat. "professio") oder „occupation“ die Rede ist und „job“ eher als Nebentätigkeit klassifiziert wird.

Der Berufsbegriff wird auch benutzt, um Einkommenserwerb und fachliche Qualifikation zu betonen. Berufsmusiker (Berufssportler, Berufssoldaten, Berufsrichter) sind fachlich ausgebildet und werden für ihre Arbeitsleistung bezahlt, Amateurmusiker oder Laienrichter hingegen mehr oder weniger nicht.

Nach Angaben des Statistischen Bundesamtes werden die Berufe in 369 so genannte Berufsordnungen unterschieden, in der alle existierenden Berufe eingruppiert sind, wodurch eine Klassifizierung der Berufe entsteht. Vgl. Frauenanteile in der Berufswelt.





Deutschland:

Österreich:


</doc>
<doc id="464" url="https://de.wikipedia.org/wiki?curid=464" title="Biologie">
Biologie

Biologie (von und "λόγος", lógos, hier: ‚Lehre‘) ist die Wissenschaft von den Lebewesen. Sie ist ein Teilgebiet der Naturwissenschaften und befasst sich sowohl mit den allgemeinen Gesetzmäßigkeiten des Lebendigen als auch mit den Besonderheiten der einzelnen Lebewesen: zum Beispiel mit ihrer Entwicklung, ihrem Bauplan und den physikalischen und biochemischen Vorgängen in deren Innerem. Im Fach "Biologie" wird in zahlreichen Teilgebieten geforscht. Zu den ganz allgemein auf das Verständnis des Lebendigen ausgerichteten Teilgebieten gehören insbesondere Biophysik, Genetik, Molekularbiologie, Ökologie, Physiologie, Theoretische Biologie und Zellbiologie. Mit großen Gruppen der Lebewesen befassen sich die Botanik (Pflanzen), die Zoologie (Tiere) und die Mikrobiologie (Kleinstlebewesen und Viren).

Die Betrachtungsobjekte der Biologie umfassen u. a. Moleküle, Organellen, Zellen und Zellverbände, Gewebe und Organe, aber auch das Verhalten einzelner Organismen sowie deren Zusammenspiel mit anderen Organismen in ihrer Umwelt. Diese Vielfalt an Betrachtungsobjekten hat zur Folge, dass im Fach Biologie eine Vielfalt an Methoden, Theorien und Modellen angewandt und gelehrt wird.

Die Ausbildung von Biologen erfolgt an Universitäten im Rahmen eines Biologiestudiums.

In neuerer Zeit haben sich infolge der fließenden Übergänge in andere Wissenschaftsbereiche (z. B. Medizin, Psychologie und Ernährungswissenschaften) sowie wegen des interdisziplinären Charakters der Forschung neben der Bezeichnung "Biologie" weitere Forschungsrichtungen und Ausbildungsgänge etabliert wie zum Beispiel die Biowissenschaften, Life Sciences und Lebenswissenschaften.

Überlegungen zum Leben gab es bereits um 600 v. Chr. bei dem griechischen Naturphilosophen Thales von Milet, der das Wasser als den Anfang – den Urgrund – aller Dinge bezeichnet haben soll. Von der Antike bis ins Mittelalter beruhte die Biologie allerdings hauptsächlich auf der "Beobachtung" der Natur, also nicht auf Experimenten. In die Interpretation der Beobachtungen flossen zudem häufig Theorien wie die Vier-Elemente-Lehre oder verschiedene spirituelle Haltungen ein, so auch der Schöpfungsmythos der biblischen Genesis, dem zufolge „Gott der HERR den Menschen aus Staub von der Erde“ formte (Adam) und ihm „den Odem des Lebens in seine Nase“ blies – „und so ward der Mensch ein lebendiges Wesen.“

Erst mit Beginn der wissenschaftlichen Revolution in der frühen Neuzeit begannen Naturforscher sich vom Übernatürlichen zu lösen. Im 16. und 17. Jahrhundert erweiterte sich zum Beispiel das Wissen über die Anatomie durch die Wiederaufnahme von Sektionen, und Erfindungen wie das Mikroskop ermöglichen ganz neue Einblicke in eine bis dahin nahezu unsichtbare Welt. Die Entwicklung der Chemie brachte auch in der Biologie Fortschritte. Experimente, die zur Entdeckung von molekularen Lebensvorgängen wie der Fermentation und der Fotosynthese führten, wurden möglich. Im 19. Jahrhundert wurden die Grundsteine für zwei große neue Wissenschaftszweige der Naturforschung gelegt: Gregor Mendels Arbeiten an Pflanzenkreuzungen begründeten die Vererbungslehre und die spätere Genetik, und Werke von Jean-Baptiste de Lamarck, Charles Darwin und Alfred Russel Wallace begründeten die Evolutionstheorien.

Die Bezeichnung Biologie, im modernen Sinne verwendet, scheint mehrfach unabhängig voneinander eingeführt worden zu sein. Gottfried Reinhold Treviranus ("Biologie oder Philosophie der lebenden Natur", 1802) und Jean-Baptiste Lamarck ("Hydrogéologie", 1802) verwendeten und definierten ihn erstmals. Das Wort selbst wurde schon 1797 von Theodor Gustav August Roose (1771–1803) im Vorwort seiner Schrift "Grundzüge der Lehre von der Lebenskraft" verwendet und taucht im Titel des dritten Bands von Michael Christoph Hanows "Philosophiae naturalis sive physicae dogmaticae: Geologia, biologia, phytologia generalis et dendrologia" von 1766 auf. Zu den Ersten, die „Biologie“ in einem umfassenden Sinn prägten, gehörte der deutsche Anatom und Physiologe Karl Friedrich Burdach.

Mit der Weiterentwicklung der Untersuchungsmethoden drang die Biologie in immer kleinere Dimensionen vor. Im 20. Jahrhundert kamen die Teilgebiete Physiologie und Molekularbiologie zur Entfaltung. Grundlegende Strukturen wie die DNA, Enzyme, Membransysteme und die gesamte Maschinerie der Zelle können seitdem auf atomarer Ebene sichtbar gemacht und in ihrer Funktion genauer untersucht werden. Zugleich gewann die Bewertung von Datenerhebungen mit Hilfe statistischer Methoden immer größere Bedeutung und verdrängte die zunehmend als bloß anekdotisch empfundene Beschreibung von Einzelphänomenen. Als Zweig der Theoretischen Biologie begann sich seit den 1920er Jahren zudem, eine "mathematische Biologie" zu etablieren.

Ebenfalls seit dem Ende des 20. Jahrhunderts entwickeln sich aus der Biologie neue angewandte Disziplinen: Beispielsweise ergänzt die Gentechnik unter anderem die klassischen Methoden der Tier- und Pflanzenzucht und eröffnet zusätzliche Möglichkeiten, die Umwelt den menschlichen Bedürfnissen anzupassen.

Die praktische Biologie und Medizin gehörten zu den Disziplinen, in denen im Deutschen Reich noch Ende des 19. Jahrhunderts im Vergleich mit anderen Disziplinen am vehementesten Gegenwehr gegen die Zulassung von Frauen geübt wurde. So versuchten unter anderem E. Huschke, C. Vogt, P. J. Möbius und T. a.L. a.W. von Bischoff die geistige Inferiorität von Frauen nachzuweisen, um deren Zulassung zum Studium zu verhindern. Hingegen waren die beschreibenden biologischen Naturwissenschaften (aber auch andere beschreibende Naturwissenschaften wie Physik und Mathematik) weiter. Hier zeigten sich die noch ausschließlich männlichen Lehrenden in einer Studie A. Kirchhoffs (1897) zumeist offen für die Zulassung von Frauen zum Studium.


Die Biologie als Wissenschaft lässt sich durch die Vielzahl von Lebewesen, Untersuchungstechniken und Fragestellungen nach verschiedenen Kriterien in Teilbereiche untergliedern: Zum einen kann die Fachrichtung nach den jeweils betrachteten Organismengruppen (Pflanzen in der Botanik, Bakterien in der Mikrobiologie) eingeteilt werden. Andererseits kann sie auch anhand der bearbeiteten mikro- und makroskopischen Hierarchie-Ebenen (Molekülstrukturen in der Molekularbiologie, Zellen in der Zellbiologie) geordnet werden.

Die verschiedenen Systeme überschneiden sich jedoch, da beispielsweise die Genetik viele Organismengruppen betrachtet und in der Zoologie sowohl die molekulare Ebene der Tiere als auch ihr Verhalten untereinander erforscht wird. Die Abbildung zeigt in kompakter Form eine Ordnung, die beide Systeme miteinander verbindet.

Im Folgenden wird ein Überblick über die verschiedenen Hierarchie-Ebenen und die zugehörigen Gegenstände der Biologie gegeben. In seiner Einteilung orientiert er sich an der Abbildung. Beispielhaft sind Fachgebiete aufgeführt, die vornehmlich die jeweilige Ebene betrachten.

Sie ist die Wissenschaft und Lehre von den Mikroorganismen, also von den Lebewesen, die als Individuen nicht mit bloßem Auge erkannt werden können: Bakterien und andere Einzeller, bestimmte Pilze, ein- und wenigzellige Algen („Mikroalgen“) und Viren.

Die Botanik ging aus der Heilpflanzenkunde hervor und beschäftigt sich vor allem mit dem Bau, der Stammesgeschichte, der Verbreitung und dem Stoffwechsel der Pflanzen.

Die Zoologie beschäftigt sich vor allem mit dem Bau, der Stammesgeschichte, der Verbreitung und den Lebensäußerungen der Tiere.

Die Humanbiologie ist eine Disziplin, die sich im engeren Sinn mit der Biologie des Menschen sowie den biologischen Grundlagen der Humanmedizin und im weiteren Sinn mit den für den Menschen relevanten Teilbereichen der Biologie befasst. Die Humanbiologie entstand als eigenständige Wissenschaftsdisziplin erst in der zweiten Hälfte des 20. Jahrhunderts.

Ihr verwandt ist die biologische Anthropologie, welche jedoch zur Anthropologie gezählt wird. Ziel der biologischen Anthropologie mit ihren Teilgebieten Primatologie, Evolutionstheorie, Sportanthropologie, Paläoanthropologie, Bevölkerungsbiologie, Industrieanthropologie, Genetik, Wachstum (Auxologie), Konstitution und Forensik ist die Beschreibung, Ursachenanalyse und evolutionsbiologische Interpretation der Verschiedenheit biologischer Merkmale der Hominiden. Ihre Methoden sind sowohl beschreibend als auch analytisch.

Die grundlegende Stufe der Hierarchie bildet die Molekularbiologie. Sie ist jene biologische Teildisziplin, die sich mit Molekülen in lebenden Systemen beschäftigt. Zu den biologisch wichtigen Molekülklassen gehören Nukleinsäuren, Proteine, Kohlenhydrate und Lipide.

Die Nukleinsäuren DNA und RNA sind als Speicher der Erbinformation ein wichtiges Objekt der Forschung. Es werden die verschiedenen Gene und ihre Regulation entschlüsselt sowie die darin codierten Proteine untersucht.
Eine weitere große Bedeutung kommt den Proteinen zu. Sie sind zum Beispiel in Form von Enzymen als biologische Katalysatoren für beinahe alle stoffumsetzenden Reaktionen in Lebewesen verantwortlich. Neben den aufgeführten Gruppen gibt es noch viele weitere, wie Alkaloide, Terpene und Steroide. Allen gemeinsam ist ein Grundgerüst aus Kohlenstoff, Wasserstoff und oft auch Sauerstoff, Stickstoff und Schwefel. Auch Metalle spielen in sehr geringen Mengen in manchen Biomolekülen (z. B. Chlorophyll oder Hämoglobin) eine Rolle.

Biologische Disziplinen, die sich auf dieser Ebene beschäftigen, sind:

Zellen sind grundlegende strukturelle und funktionelle Einheiten von Lebewesen. Man unterscheidet zwischen prokaryotischen Zellen, die keinen Zellkern besitzen und wenig untergliedert sind, und eukaryotischen Zellen, deren Erbinformation sich in einem Zellkern befindet und die verschiedene Zellorganellen enthalten. Zellorganellen sind durch einfache oder doppelte Membranen abgegrenzte Reaktionsräume innerhalb einer Zelle. Sie ermöglichen den gleichzeitigen Ablauf verschiedener, auch entgegengesetzter chemischer Reaktionen. Einen großen Teil der belebten Welt stellen Organismen, die nur aus einer Zelle bestehen, die Einzeller. Sie können dabei aus einer prokaryotischen Zelle bestehen (die Bakterien), oder aus einer eukaryotischen (wie manche Pilze).

In mehrzelligen Organismen schließen sich viele Zellen gleicher Bauart und mit gleicher Funktion zu Geweben zusammen. Mehrere Gewebe mit Funktionen, die ineinandergreifen, bilden ein Organ.

"Biologische Disziplinen, vornehmlich auf dieser Ebene (Beispiele)":

Jedes Lebewesen ist Resultat einer Entwicklung. Nach Ernst Haeckel lässt sich diese Entwicklung auf zwei zeitlich unterschiedlichen Ebenen betrachten:

Die Physiologie befasst sich mit den physikalischen, biochemischen und informationsverarbeitenden Funktionen der Lebewesen. Physiologisch geforscht und ausgebildet wird sowohl in den akademischen Fachrichtungen Biologie und Medizin als auch in der Psychologie.

Als Begründer der Genetik gilt Gregor Mendel. So entdeckte er die später nach ihm benannten Mendelschen Regeln, die in der Wissenschaft allerdings erst im Jahr 1900 rezipiert und bestätigt wurden. Der heute weitaus wichtigste Teilbereich der Genetik ist die Molekulargenetik, die in den 1940er Jahren begründet wurde.

Die Verhaltensbiologie erforscht das Verhalten der Tiere und des Menschen. Sie beschreibt das Verhalten, stellt Vergleiche zwischen Individuen und Arten an und versucht, das Entstehen bestimmter Verhaltensweisen im Verlauf der Stammesgeschichte zu erklären, also den „Nutzen“ für das Individuum.

Das Fachgebiet Ökologie setzt sich mit den Wechselwirkungen zwischen den Organismen und den abiotischen und biotischen Faktoren ihres Lebensraumes auf verschiedenen Organisationsebenen auseinander.

Eine Population ist eine Fortpflanzungsgemeinschaft innerhalb einer Art in einem zeitlich und räumlich begrenzten Gebiet. Die Populationsökologie betrachtet vor allem die Dynamik der Populationen eines Lebensraumes auf Grund der Veränderungen der Geburten- und Sterberate, durch Veränderungen im Nahrungsangebot oder abiotischer Umweltfaktoren. Diese Ebene wird auch von der Verhaltensbiologie und der Soziobiologie untersucht.

Im Zusammenhang mit der Beschreibung und Untersuchung sozialer Verbände wie Herden oder Rudel können auch die auf den Menschen angewandten Gesellschaftswissenschaften gesehen werden.


Die Lebewesen können sich positiv (z. B. Symbiose), negativ (z. B. Fressfeinde, Parasitismus) oder einfach gar nicht beeinflussen.

Lebensgemeinschaft (Biozönose) und Lebensraum (Biotop) bilden zusammen ein Ökosystem.

Biologische Disziplinen, die sich mit Ökosystemen beschäftigen (Beispiele):

Da die Evolution der Organismen zu einer Anpassung an eine bestimmte Umwelt führen kann, besteht ein intensiver Austausch zwischen beiden Fachdisziplinen, was insbesondere in der Disziplin der Evolutionsökologie zum Ausdruck kommt.

Die "Phylogenese" beschreibt die Entwicklung einer Art im Verlauf von Generationen. Hier betrachtet die Evolutionsbiologie die langfristige Anpassung an Umweltbedingungen und die Aufspaltung in neue Arten.

Auf der Grundlage der phylogenetischen Entwicklung ordnet die biologische Taxonomie alle Lebewesen in ein Schema ein. Die Gesamtheit aller Organismen wird in drei Gruppen, die Domänen, unterteilt, welche wiederum weiter untergliedert werden:

Mit der Klassifizierung der Tiere in diesem System beschäftigt sich die Spezielle Zoologie, mit der Einteilung der Pflanzen die Spezielle Botanik, mit der Einteilung der Archaeen, Bakterien und Pilze die Mikrobiologie.

Als häufige Darstellung wird ein "phylogenetischer Baum" gezeichnet. Die Verbindungslinien zwischen den einzelnen Gruppen stellen dabei die evolutionäre Verwandtschaft dar. Je kürzer der Weg zwischen zwei Arten in einem solchen Baum, desto enger sind sie miteinander verwandt. Als Maß für die Verwandtschaft wird häufig die Sequenz eines weitverbreiteten Gens herangezogen.

Als in gewissem Sinne eine Synthese von Ökologie, Evolutionsbiologie und Systematik hat sich seit Ende der 1980er Jahre die Biodiversitäts­forschung etabliert, die auch den Brückenschlag zu Schutzbestrebungen für die biologische Vielfalt und zu politischen Abkommen über Schutz und Nachhaltigkeit bildet.

In diesem Fachgebiet versuchen Bio-Ingenieure, künstliche lebensfähige Systeme herzustellen, die wie naturgegebene Organismen von einem Genom gesteuert werden.

Die Theoretische Biologie befasst sich mit mathematisch formulierbaren Grundprinzipien biologischer Systeme auf allen Organisationsstufen.

Die Biologie nutzt viele allgemein gebräuchliche wissenschaftliche Methoden, wie strukturiertes Beobachten, Dokumentation (Notizen, Fotos, Filme), Hypothesen­bildung, mathematische Modellierung, Abstraktion und Experimente. Bei der Formulierung von allgemeinen Prinzipien in der Biologie und der Knüpfung von Zusammenhängen stützt man sich sowohl auf empirische Daten als auch auf mathematische Sätze. Je mehr Versuche mit verschiedenen Ansatzpunkten auf das gleiche Ergebnis hinweisen, desto eher wird es als gültig anerkannt. Diese pragmatische Sicht ist allerdings umstritten; insbesondere Karl Popper hat sich gegen sie gestellt. Aus seiner Sicht können Theorien durch Experimente oder Beobachtungen und selbst durch erfolglose Versuche, eine Theorie zu widerlegen, nicht untermauert, sondern nur untergraben werden (siehe Unterdeterminierung von Theorien durch Evidenz).

Einsichten in die wichtigsten Strukturen und Funktionen der Lebewesen sind mit Hilfe von Nachbarwissenschaften möglich. Die Physik beispielsweise liefert eine Vielzahl Untersuchungsmethoden. Einfache optische Geräte wie das Lichtmikroskop ermöglichen das Beobachten von kleineren Strukturen wie Zellen und Zellorganellen. Das brachte neues Verständnis über den Aufbau von Organismen und mit der Zellbiologie eröffnete sich ein neues Forschungsfeld. Mittlerweile gehört eine Palette hochauflösender bildgebender Verfahren, wie Fluoreszenzmikroskopie oder Elektronenmikroskopie, zum Standard.

Als eigenständiges Fach zwischen den Wissenschaften Biologie und Chemie hat sich die Biochemie herausgebildet. Sie verbindet das Wissen um die chemischen und physikalischen Eigenschaften von den Bausteinen des Lebens mit der Wirkung auf das biologische Gesamtgefüge. Mit chemischen Methoden ist es möglich, bei biologischer Versuchsführung zum Beispiel Biomoleküle mit einem Farbstoff oder einem radioaktiven Isotop versehen. Das ermöglicht ihre Verfolgung durch verschiedene Zellorganellen, den Organismus oder durch eine ganze Nahrungskette.

Die Bioinformatik ist eine sehr junge Disziplin zwischen der Biologie und der Informatik. Die Bioinformatik versucht, mit Methoden der Informatik biologische Fragestellungen zu lösen. Im Gegensatz zur theoretischen Biologie, welche häufig nicht mit empirischen Daten arbeitet, um konkrete Fragen zu lösen, benutzt die Bioinformatik biologische Daten. So war eines der Großforschungsprojekte der Biologie, die Genomsequenzierung, nur mit Hilfe der Bioinformatik möglich. Die Bioinformatik wird aber auch in der Strukturbiologie eingesetzt, hier existieren enge Wechselwirkungen mit der Biophysik und Biochemie. Eine der fundamentalen Fragestellungen der Biologie, die Frage nach dem Ursprung der Lebewesen (auch als phylogenetischer Baum des Lebens bezeichnet, siehe Abbildung oben), wird heute mit bioinformatischen Methoden bearbeitet.

Die Mathematik dient als Hauptinstrument der theoretischen Biologie der Beschreibung und Analyse allgemeinerer Zusammenhänge der Biologie. Beispielsweise erweist sich die Modellierung durch Systeme gewöhnlicher Differenzialgleichungen in vielen Bereichen der Biologie (etwa der Evolutionstheorie, Ökologie, Neurobiologie und Entwicklungsbiologie) als grundlegend. Fragen der Phylogenetik werden mit Methoden der diskreten Mathematik und algebraischen Geometrie bearbeitet. 

Zu Zwecken der Versuchsplanung und Analyse finden Methoden der Statistik Anwendung. 

Die unterschiedlichen biologischen Teildisziplinen nutzen verschiedene systematische Ansätze:

Die Biologie ist eine naturwissenschaftliche Disziplin, die sehr viele Anwendungsbereiche hat. Durch biologische Forschung werden Erkenntnisse über den Aufbau des Körpers und die funktionellen Zusammenhänge gewonnen. Sie bilden eine zentrale Grundlage, auf der die Medizin und Veterinärmedizin Ursachen und Auswirkungen von Krankheiten bei Mensch und Tier untersucht. Auf dem Gebiet der Pharmazie werden Medikamente, wie beispielsweise Insulin oder zahlreiche Antibiotika, aus genetisch veränderten Mikroorganismen statt aus ihrer natürlichen biologischen Quelle gewonnen, weil diese Verfahren preisgünstiger und um ein Vielfaches produktiver sind. Für die Landwirtschaft werden Nutzpflanzen mittels Molekulargenetik mit Resistenzen gegen Schädlinge versehen und unempfindlicher gegen Trockenheit und Nährstoffmangel gemacht. In der Genussmittel- und Nahrungsmittelindustrie sorgt die Biologie für eine breite Palette länger haltbarer und biologisch hochwertigerer Nahrungsmittel. Einzelne Lebensmittelbestandteile stammen auch hier von genetisch veränderten Mikroorganismen. So wird das Lab zur Herstellung von Käse heute nicht mehr aus Kälbermagen extrahiert, sondern mikrobiell erzeugt.

Weitere angrenzende Fachgebiete, die ihre eigenen Anwendungsfelder haben, sind Ethnobiologie, Bionik, Bioökonomie, Bioinformatik und Biotechnologie.




</doc>
<doc id="467" url="https://de.wikipedia.org/wiki?curid=467" title="Bruce Willis">
Bruce Willis

Walter Bruce Willis (* 19. März 1955 in Idar-Oberstein, Deutschland) ist ein US-amerikanischer Schauspieler, Filmproduzent, Musiker/Komponist und Drehbuchautor. Seinen Durchbruch als Schauspieler hatte er 1988 mit dem Kinoerfolg "Stirb langsam".

Bruce Willis wurde 1955 in Idar-Oberstein als Sohn des US-amerikanischen Soldaten David Willis und dessen deutscher Frau Marlene aus Kaufungen bei Kassel geboren. Er verbrachte die ersten zwei Jahre mit den Eltern in Deutschland, ehe die Familie 1957 in die Vereinigten Staaten übersiedelte. Zusammen mit den drei jüngeren Geschwistern Florence, dem Produzenten David und Robert, der 2001 starb, wuchs er in New Jersey auf. Als Therapie gegen sein Stottern kam er bereits während der Schulzeit zur Schauspielerei. Nach Abschluss der Highschool nahm er Schauspielunterricht am Montclair State College. Um diesen finanzieren zu können, arbeitete er nebenher in einer Chemiefabrik.

Erste Erfahrungen als Schauspieler sammelte er an New Yorker Theatern und als Darsteller in Werbespots. Ab 1985 spielte er die Hauptrolle in der Detektivserie "Das Model und der Schnüffler" "(Moonlighting)", die unter anderem mit einem Emmy und einem Golden Globe ausgezeichnet wurde. Eine Gastrolle hatte er in "Miami Vice". 

Anschließend arbeitete er zweimal mit Regisseur Blake Edwards zusammen, ehe ihm 1988 mit dem Kinoerfolg "Stirb langsam" der Durchbruch gelang. Die Rolle des Polizisten John McClane, der nur die eigenen Regeln befolgt und immer einen lockeren Spruch auf den Lippen hat, war Willis wie auf den Leib geschneidert und machte ihn zum Star des Actionfilms. Abgesehen von Actionfilmen wie "Stirb langsam 2" und "Last Boy Scout – Das Ziel ist Überleben" hatte Willis bis Mitte der 1990er Jahre kaum kommerziellen Erfolg, was unter anderem auch daran lag, dass er durch Rollen in Komödien wie "Der Tod steht ihr gut" versuchte, sein Action-Image loszuwerden. 1994 wurde die Rolle des Boxers Butch in "Pulp Fiction" von Quentin Tarantino mit Willis besetzt, dessen Darstellung von der Kritik gelobt wurde. Es folgten weitere Kassenschlager wie "12 Monkeys", "Das fünfte Element" oder "Armageddon – Das jüngste Gericht". Mit Filmen wie "The Sixth Sense" oder "Unbreakable – Unzerbrechlich" widmete er sich anschließend verstärkt Dramen.

Im Oktober 2006 wurde Bruce Willis mit einem Stern auf dem Hollywood Walk of Fame geehrt. Willis zählt laut amerikanischem Forbes Magazine zu den bestbezahlten Schauspielern Hollywoods. Zwischen Juni 2007 und Juni 2008 erhielt er Gagen in Höhe von insgesamt 41 Millionen US-Dollar und rangierte damit hinter Will Smith, Johnny Depp, Eddie Murphy, Mike Myers und Leonardo DiCaprio auf Platz sechs.

Willis ist auch als Theaterschauspieler und Produzent tätig. Darüber hinaus veröffentlichte er zwei sehr erfolgreiche LPs/CDs: "The Return of Bruno", die mit Platin ausgezeichnet wurde, und "If It Don’t Kill You, It Just Makes You Stronger". Chartplatzierungen erreichte er mit den Singleauskopplungen "Respect Yourself", dem Cover eines Songs der Staple Singers, und "Under the Boardwalk". Sporadisch tritt er auch heute noch mit seiner Band auf. 2010 hatte Willis einen Gastauftritt im Musikvideo zu "Stylo", der 2010 erschienenen Single der Gorillaz.

Er ist Mitgründer der Restaurantkette Planet Hollywood. 2007 ernannte die Stadt Idar-Oberstein den hier geborenen Willis zum Sonderbotschafter.

Mit der Schauspielerin Demi Moore, mit der er von 1987 bis 2000 verheiratet war, hat Bruce Willis drei Töchter: Rumer (* 1988), Scout LaRue (* 1991) und Tallulah Belle (* 1994). Alle Töchter waren seit 1995 schon vor der Kamera, jedoch nur in Filmen, in denen ein Elternteil mitspielte. Rumer war an der Seite ihres Vaters in "Hostage – Entführt", Scout LaRue und Tallulah Belle erschienen erstmals in dem Film "Der scharlachrote Buchstabe", danach 2001 in "Banditen!". 

Beim Sundance Film Festival in Utah präsentierte er sich am im Januar 2008 mit seiner neuen Lebensgefährtin, dem Fotomodell Emma Heming. Ihre Hochzeit fand am 21. März 2009 auf den Caicos-Inseln statt. Am 1. April 2012 wurde ihre erste gemeinsame Tochter geboren. Am 5. Mai 2014 bekam das Paar eine weitere Tochter.

In einem Interview Anfang 2006 dementierte Willis das Gerücht, er sei Anhänger der Regierung von George W. Bush. Er sagte, er sei kein Republikaner und hasse Regierungen: „Wir können unsere Politiker nicht beeinflussen. Die interessieren sich einen Dreck für uns.“ Außerdem rechtfertigte er sich, den Krieg im Irak zu befürworten: „Ich bin kein gewalttätiger Mensch. Aber wir leben in einer gewalttätigen Welt. Dieses Land wurde auf Gewalt aufgebaut.“ Anlässlich des Films "Stirb langsam 4.0" meinte er in einem Interview mit der "Süddeutschen Zeitung", dass er ein politisch schwer einzuordnender Mensch sei, der differenziert denke.


Ronald Nitschke lieh ihm in "Das Model und der Schnüffler", "Der Tod steht ihr gut", "Blind Date" und "Four Rooms" seine Stimme. In fast allen anderen Filmen wird er von Manfred Lehmann synchronisiert. In "" bekam er ausnahmsweise die Stimme von Thomas Danneberg, da Lehmann zu diesem Zeitpunkt mit Dreharbeiten beschäftigt war.




</doc>
<doc id="468" url="https://de.wikipedia.org/wiki?curid=468" title="Benchmarking in der Betriebswirtschaft">
Benchmarking in der Betriebswirtschaft

Benchmarking (engl. "Benchmark" = „Maßstab“, steht für eine Bezugs- oder Richtgröße) bezeichnet in der Betriebswirtschaft eine Managementmethode, mit der sich durch zielgerichtete Vergleiche unter mehreren Unternehmen das jeweils beste als Referenz zur Leistungsoptimierung herausfinden lässt. Dazu ist es notwendig, durch Vergleich bessere Methoden und Praktiken "(Best Practices)" zu identifizieren, zu verstehen, auf die eigene Situation anzupassen und zu integrieren. Benchmarking ist eine Weiterentwicklung des Betriebsvergleichs.

Die Durchführung eines Benchmarkings beruht auf der Orientierung an den Besten einer vergleichbaren Gruppe. Diese Vorgehensweise bezeichnet man als „Best Practice“ (wörtlich: "bestes Verfahren" oder „die beste Lösung“). Best Practice existiert auf verschiedenen Betrachtungsebenen und tritt in folgenden Formen auf:

Auf der obersten Betrachtungsebene werden Konzepte in Frage gestellt. Es geht darum, die „richtigen Dinge zu tun“. Auf der untersten Betrachtungsebene der Detailprozesse wird der Prozess in Frage gestellt. Hier geht es darum, die „Dinge richtig zu tun“ – also die Prozesseffizienz zu verbessern.

Ein häufiger Fehler beim Benchmarking besteht darin, die Betrachtungsebenen im Vorfeld nicht klar zu definieren. Erfahrungen zeigen, dass ein Benchmarking auf Konzeptebene sich kaum mit einem auf Detailprozessebene kombinieren lässt. Die für ein Benchmarking benötigten Ansprechpartner für die beiden Ebenen sind allzu verschieden und die Methoden zur Identifikation geeigneter Benchmarking-Partner unterscheiden sich je nach Betrachtungsebene.

Benchmarking ist einer der effektivsten Wege, externes Wissen rasch in das eigene Unternehmen einzubringen. Das in einem Benchmarking-Projekt erarbeitete Wissen ist in höchstem Maße praxisorientiert – denn es stammt aus der Praxis und hat sich im Alltag bewährt.

In der Europäischen Union wird das Benchmarking seit Ende 1996 als eine Methode angewandt, um die Leistungskraft der einzelnen Arbeitsmärkte der EU-Länder zu vergleichen. Dabei sollen Schwächen einzelner Mitgliedstaaten offengelegt und die jeweiligen Regierungen in die Lage versetzt werden, dringend benötigte Reformen durchzuführen. Entsprechende Vergleichsmethoden sind auf den nationalen Ebenen der Politik bisher eher unüblich, der Drang nationaler oder regionaler Politik zu mehr Transparenz ist steigerungsfähig.

In der Bundesrepublik Deutschland gibt es im Bereich der öffentlichen Verwaltungen und Organisationen zum einen gesetzliche Vorschriften für das Benchmarking (z. B. Krankenhäuser, Rentenversicherung), zum anderen auch freiwillige Aktivitäten in sogenannten Benchmarking-Clubs. So sind beispielsweise die gesetzlichen Unfallversicherungsträger mit wissenschaftlicher Begleitung dabei ein Prozessbenchmarking durchzuführen und weiter zu entwickeln, das sowohl qualitative, als auch quantitative Ziele und Wirkungen berücksichtigt.

Es gibt bisher keine Beispiele international genormter (ISO) Benchmarks, industrielle Standards werden beispielsweise durch SPEC gesetzt.

Benchmarking-Methoden unterscheiden sich meist nur im Detaillierungsgrad und in den Bezeichnungen. Das Grundschema bleibt in etwa gleich: Vergleiche zeigen Unterschiede zwischen der eigenen Organisation und den Benchmarking-Partnern. Daraus lassen sich Folgerungen für die eigene Organisation ableiten, ein Lernprozess beginnt.

Aus praktischer Sicht hat sich eine Einteilung von Benchmarking in vier Grundtypen bewährt. Die Klassifizierung erfolgt auf Grund der Eigenschaften der Benchmarking-Partner. Diese können in der eigenen oder in einer fremden Branche gefunden werden und sie gehören entweder zur eigenen oder zu einer fremden Organisation.

Internes Benchmarking findet innerhalb der eigenen Organisation und Branche statt. Ein Beispiel ist ein internationaler Zementhersteller, der den Produktionsprozess aller seiner Werke periodisch miteinander vergleicht.

Der große Vorteil dieses Benchmarking-Typs ist es, dass erstens die Daten gut erhältlich und zweitens Vergleiche auf Kennzahlenebene möglich sind. Ein möglicher Nachteil kann mangelnde Akzeptanz sein, die auf das Phänomen zurückgeht, dass „der Prophet im eigenen Land“ oft nichts gilt.

Eine besondere Form des internen Benchmarkings ist das Konzern-Benchmarking. Dabei zählen Benchmarking-Anstrengungen innerhalb von Konzernen, Holdings oder Konglomeraten. Das interne Benchmarking beschränkt sich also nicht mehr auf die gleiche Branche, sondern wird auf das ganze Unternehmen ausgedehnt. Der Vorteil beim Konzern-Benchmarking ist die gute Verfügbarkeit von Informationen. Abhängig von der Betrachtungsebene ist kennzahlenorientiertes Benchmarking allerdings nur noch beschränkt möglich, da die beteiligten Unternehmenseinheiten anderen Branchen angehören. Spielt sich das Benchmarking auf der Konzeptebene ab, sind Successful Practices oft nicht mehr 1:1 übertragbar.

Die Benchmarking-Partner sind Unternehmen aus derselben Branche. Die Benchmarking-Partner können denselben Markt oder unterschiedliche Märkte bearbeiten. In beiden Fällen gilt das Hauptinteresse oft dem Vergleich von Kennzahlen, der allerdings aus Wettbewerbsgründen nur eingeschränkt möglich ist.

Wettbewerbs-Benchmarking bedingt eine besonders gute Vorbereitung und eine sehr offene Kommunikation. Jeder Teilnehmer muss die Sicherheit erhalten, dass die abgegebenen und erhaltenen Informationen in einem ausgewogenen Verhältnis stehen. Dies lässt sich kaum mehr realisieren, sobald einer der Wettbewerber die Federführung übernimmt und für „sein“ Projekt Partner sucht.

Der Vorteil dieses Benchmarking-Typs ist die klare Positionierung des eigenen Unternehmens im Wettbewerb. Dem stehen zwei Nachteile gegenüber: Erstens ist es schwierig, Kennzahlen oder sogar Prozesse mit der direkten Konkurrenz zu benchmarken. Zweitens zeigt die Erfahrung des "Transferzentrums für Technologiemanagement (TECTEM) an der Universität St. Gallen", dass die Erkenntnisse aus Wettbewerbs-Benchmarking einen geringen Neuigkeitsgrad haben.

Hier erfolgt der Vergleich mit Unternehmen der gleichen Branche in getrennten Märkten. Man vergleicht den Organisationsbereich bezüglich seiner Funktion, wie beispielsweise die interne Logistikabteilung mit einem Versandunternehmen. Es ist die universelle Art des Benchmarkings. Es bietet ein sehr hohes Lernpotenzial, da es am meisten neue Ideen und Anregungen bietet. Dieses Potenzial kann insbesondere bei Benchmarking auf Konzeptebene voll ausgeschöpft werden.
Das generische Benchmarking ist die reinste Form des Benchmarkings. Hier findet ein branchen- und funktionsübergreifender Vergleich von Prozessen und Methoden statt. Es wird hier ein Vergleich zwischen unverwandten Unternehmen angestrebt.

Ein immer wieder zitiertes Beispiel für das generische Benchmarking ist der Vergleich der Zeiten für Be- und Entladung, Betankung, Reinigung und Sicherheitschecks von Flugzeugen mit den Boxenstopps beim Indy-500-Autorennen, den die Southwest Air Lines durchgeführt hat. Durch das Benchmarking soll eine Zeitreduzierung um 50 % möglich gewesen sein.

Der grundlegende Ablauf eines Benchmarking-Projekts, den zuerst Robert Camp beschrieben hat, bildet die Grundlage der meisten Veröffentlichungen zum Thema Benchmarking. Er kann grob in die folgenden Phasen eingeteilt werden





Diese Schritte können wiederum zum Ausgangspunkt für ein neuerliches Benchmarking werden, das erneut in die Vergleichsphase eintritt.

Zur "Etymologie" des Wortes „Benchmarking“ siehe Benchmark.

Der betriebswirtschaftliche Begriff „Benchmarking“ geht zurück auf die US-amerikanische Firma Xerox Corporation. Obwohl die beim Benchmarking angewandten Methoden schon sehr viel früher in der Praxis der modernen industriellen Produktion entstanden sind – zum Beispiel lässt sich die Übertragung des Fließbandprinzips aus einer Großschlachterei in die Automobilproduktion durch Henry Ford als sehr erfolgreiches Benchmarking bezeichnen –, hat sich der Begriff des Benchmarkings erst in den 1980er Jahren mit den Veröffentlichungen von Robert C. Camp zu Benchmarking-Projekten bei Xerox durchgesetzt. Diese Schriften haben der Benchmarking-Idee zum Durchbruch verholfen.

Der Kopiererhersteller Xerox befand sich Ende der 1970er Jahre bedingt durch Qualitäts- und Kostenprobleme in einer schwierigen Wettbewerbsposition. Der japanische Konkurrent Canon brachte einen Kopierer zu einem Verkaufspreis auf den Markt, der wesentlich unter den Herstellkosten für vergleichbare Geräte bei Xerox lag. Die Marktanteile von Xerox fielen auf dem Kopierermarkt steil ab. Aus diesem Grund wandte Xerox 1979 zum ersten Mal ein Benchmarking an: Es wurde ein Kopierer der Konkurrenz gekauft, zerlegt und die einzelnen Komponenten mit jenen der eigenen Kopierer verglichen. So konnten die niedrigeren Herstellungskosten von Canon zu einem großen Teil erklärt werden.

In einem nächsten Schritt wurden die Aktivitäten der einzelnen Wertschöpfungsketten im Unternehmen analysiert, mit dem Ergebnis, dass erhebliche Probleme in den Logistik- und Vertriebsprozessen aufgedeckt werden konnten. Unter anderem wurden die logistischen Prozesse der Materialentnahme bei dem Kopiergerätehersteller mit dem des Outdoor-Artikel-Versenders L.L.Bean verglichen. Dies veranlasste Xerox zu Folgeprojekten, wobei unter anderem Pharmagroßhändler und Haushaltsgerätehersteller als Benchmark dienten. Aufgrund dieser ersten Erfolge wurde Benchmarking bald zu einer Hauptsäule der Xerox-Strategie.

1992 erfolgte die erste Gründung eines IBC ("international benchmarking clearing house"), einer Einrichtung, die als Dienstleistung anbietet, Benchmarkingpartner zu vermitteln. 1994 gründete das Fraunhofer-Institut für Produktionsanlagen und Konstruktionstechnik (IPK) das Informationszentrum Benchmarking (IZB). Das Global Benchmarking Network (GBN) wurde 1994 aus der Taufe gehoben.

Für Camp stand die Identifikation und Implementation von „Best Practices“ im Mittelpunkt. Einem rein quantitativen Benchmarking, also dem bloßen Vergleich von Kennzahlen, sprach er nur in Verbindung mit einer vertieften Analyse der Praktiken einen Nutzen zu. Seither hat sich das Benchmarking über mehrere Generationen weiterentwickelt und zu unterschiedlichen Erscheinungsformen geführt.

Der im Benchmarking häufig verwendete Begriff der Best Practices ist nur dann korrekt, wenn ein Benchmarking eine vollständige, weltweite Analyse beinhaltet. Dies ist nur in den wenigsten Fällen praktikabel. Ein Beispiel wäre der Herstellungsprozess von Mikrochips für hoch integrierte Prozessoren. Hier sind alle „Global Players“ bekannt und könnten in ein Benchmarking einbezogen werden.

In der Regel ist eine globale Analyse aus Zeit- und Kostengründen allerdings kaum möglich oder sinnvoll. Der Anspruch des Benchmarking besteht in den meisten Fällen nicht darin, die besten Praktiken zu finden, sondern erfolgreiche (wobei die Kriterien für den Erfolgsnachweis im Vorfeld definiert werden müssen). Die Literatur spricht hier von Successful Practices.

Auch in der Volkswirtschaftslehre wird Benchmarking betrieben. Hier geht es um die Wettbewerbsfähigkeit von:

Benchmarking-Ergebnisse werden u. a. im Global Competitiveness Report des World Economic Forum veröffentlicht.






</doc>
<doc id="469" url="https://de.wikipedia.org/wiki?curid=469" title="Beobachter">
Beobachter

Beobachter steht für:


Zeitschriften:
Siehe auch:


</doc>
<doc id="470" url="https://de.wikipedia.org/wiki?curid=470" title="Binär">
Binär

binär bzw. Binärsystem (von lat. ', für „je zwei“ oder ', für „doppelt“ oder „paarweise“) bezieht sich auf:

Mathematik:

Informationstechnik:

Biologie:

Chemie:

Astronomie:

Siehe auch:


</doc>
<doc id="472" url="https://de.wikipedia.org/wiki?curid=472" title="Black Box">
Black Box

Black Box oder Blackbox (engl. blæk bɒks ‚schwarzer Kasten‘) steht für:


Umgangssprachlich für Datenschreiber verschiedener Verkehrsmittel:

Werke:


</doc>
<doc id="473" url="https://de.wikipedia.org/wiki?curid=473" title="Brainstorming">
Brainstorming

Brainstorming ist eine von Alex F. Osborn 1939 entwickelte und von Charles Hutchison Clark modifizierte Methode zur Ideenfindung, die die Erzeugung von neuen, ungewöhnlichen Ideen in einer Gruppe von Menschen fördern soll. Er benannte sie nach der Idee dieser Methode, nämlich "using the brain to storm a problem" (wörtlich: „Das Gehirn verwenden zum Sturm auf ein Problem“). Hilbert Meyer verwendet in "UnterrichtsMethoden" als Übersetzungsangebot den Begriff „Kopfsalat“, der VDS schlägt „Denkrunde“ und „Ideensammlung“ vor.

Der Name "Brainstorming" hat sich schnell verbreitet, wird heute aber auch für andere Techniken als die von Osborn beschriebene verwendet.

Anwendung findet dieses Verfahren bevorzugt im gesamten Bereich der Werbung. Es wird aber mit mehr oder weniger Erfolg auch bei sämtlichen Problemen eingesetzt, zum Beispiel bei der Produktentwicklung oder beim Konstruieren neuer technischer Geräte. Die Ergebnisse eines Brainstormings können in weiteren Arbeitsschritten verwendet werden, es kann aber auch das (ergebnislose) Brainstorming allein als kreative Lockerungsübung eingesetzt werden. Das ursprüngliche Verfahren sieht zwei Schritte vor:

Es wird eine Gruppe aus beliebig vielen Personen zusammengestellt. Je nach Problemstellung kann sie aus Experten/Mitarbeitern, Laien oder Experten anderer Fachgebiete bestehen. Die Gruppenleitung bereitet Anschauungsmaterial vor und führt die Gruppe in das Problem ein, das dabei analysiert und präzisiert wird. Dabei sollte die Frage- bzw. Aufgabenstellung weder zu breit und allgemein gehalten sein („Wie können wir die Welt retten?“) noch zu kleinteilig bzw. spezifisch („Welches Klebeverfahren um Bauteil A an B zu befestigen?“). Den Gruppenmitgliedern wird im Vorfeld der Ablauf des Brainstormings mitgeteilt, ob es sich um ein moderiertes oder nichtmoderiertes Brainstorming handelt. Ein Protokollant kann ernannt werden. Vier grundsätzliche Regeln gelten beim Brainstorming:

Nun nennen die Teilnehmer spontan Ideen zur Lösungsfindung, wobei sie sich im optimalen Fall gegenseitig inspirieren und untereinander Gesichtspunkte in neue Lösungsansätze und Ideen einfließen lassen. Die Ideen werden protokolliert. Alle Teilnehmenden sollen ohne jede Einschränkung Ideen produzieren und mit anderen Ideen kombinieren. Die Gruppe sollte in eine möglichst produktive und erfindungsreiche Stimmung versetzt werden. In dieser Phase gelten folgende Grundregeln:

Nach einer Pause werden nun sämtliche Ideen (von der Gruppenleitung) vorgelesen und von den Teilnehmern bewertet und sortiert. Hierbei geht es zunächst nur um bloße thematische Zugehörigkeit und das Aussortieren von problemfernen Ideen. Die Bewertung und Auswertung kann in derselben Diskussion durch dieselben Teilnehmer erfolgen oder von anderen Fachleuten getrennt vorgenommen werden.

Nach einer Studie aus dem Jahr 2002 von Henk Wilke und Arjaan Wit spielt die Gruppendynamik beim Brainstorming eine große Rolle. Als bekannteste und weit verbreitete Kreativitätstechnik ist es für einen effektiven und effizienten Einsatz von Brainstorming sinnvoll, gruppendynamische Prozesse und Problemfelder zu kennen und ihnen gegebenenfalls entgegenzuwirken. Es geht hierbei um Auswirkungen der Gruppenstruktur, aber auch um potentielle Prozess- sowie Motivationsverluste, die Einfluss auf die Ergebnisse des Brainstormings nehmen können. Dabei sind Aspekte der Gruppenstruktur, der Rollendifferenzierung, der Statusdifferenzierung und der Kommunikationsmuster zu beachten, ansonsten können Prozessverluste und Motivationsverluste entstehen.

Untersuchungen behaupten, dass schon die Äußerung einer Idee die Ideenfindung der anderen Teilnehmenden beeinflusst. Daher sei es sinnvoll, alle Teilnehmenden vor dem eigentlichen Brainstorming ihre Ideen aufschreiben zu lassen, um danach zunächst gänzlich unbeeinflusst davon berichten zu können.

Laut einem Bericht in „Bild der Wissenschaft“ 1/2005 nützt die traditionelle Brainstorming-Methode jedoch nachweislich nichts: 50 Studien zeigten ein vernichtendes Ergebnis, die Kandidaten konnten es in Gruppen nicht besser, weil sie sich gegenseitig blockierten. Meist mussten sie warten, bis ein anderer ausgeredet hatte, was ihre Kreativität hemmte. Dieses Phänomen wird "Produktionsblockierung" genannt. Einzelkämpfer hingegen hatten nicht nur mehr, sondern auch bessere Eingebungen als die Gruppe. Kreativität hinge somit eher vom Bewusstseinsstand der Einzelnen ab.

Anders verhält es sich mit elektronischem Brainstorming, das mit Hilfe elektronischer Meetingsysteme online durchgeführt wird. Diese Systeme setzen wesentliche Grundregeln des Brainstormings auf technischer Ebene durch und hebeln schädliche Einflüsse der Gruppenarbeit durch Anonymisierung und Parallelisierung der Eingaben aus. Die positiven Effekte elektronischen Brainstormings verstärken sich mit wachsender Gruppengröße.

Um weniger ausdrucksstarke, aber gleichwertig qualifizierte Mitarbeiter einzubeziehen, kann auch auf Brainwriting oder die Collective-Notebook-Methode ausgewichen werden. Auch hier gilt, dass jede Variation in Umgebung und Art der Durchführung neue Impulse liefert. Als hilfreich erweist sich bei Brainstormings auch, sogenannte „Outsider“ in das Brainstorming einzubeziehen. Mitglieder innerhalb einer Organisation blockieren zumeist bei der Ideenfindung, weil sie zu sehr in bestimmten Strukturen denken und darin gefangen sind. Leute von außerhalb können die Denkprozesse beschleunigen und positiv beeinflussen.

Andererseits sind wiederum geübt kreative Menschen in der Lage, sich innerhalb einer Brainstorming-Sitzung gegenseitig anzuregen und zu beflügeln. Die Brauchbarkeit der Ideen hängt wesentlich von der Vertrautheit der Teilnehmenden mit dem Problemgebiet ab, vielfältige Interessen und breite Allgemeinbildung sind ebenfalls vorteilhaft.

Brainstorming und verwandte Methoden werden manchmal nur deshalb angewendet, um möglichst viele Personen an der Problemlösung zu beteiligen, also aus (betriebs-)politischen Gründen. In solchen Fällen spielt die Effektivität keine große Rolle. Wird Brainstorming streng ergebnisorientiert eingesetzt und auch nur von für diese Methode geeigneten Personen ausgeübt, kann es sehr schnell zu guten Teilergebnissen führen, die wiederum weitere Arbeitsschritte befruchten.

Ein Sozialpsychologe der Universität Utrecht machte bezüglich Brainstorming ein Experiment, in dem 20 allein nachdenkende Menschen bis zu 50 % mehr und originellere Einfälle hatten als „Teams“, die klassisches Brainstorming betrieben.







</doc>
<doc id="474" url="https://de.wikipedia.org/wiki?curid=474" title="Richard Buckminster Fuller">
Richard Buckminster Fuller

Richard Buckminster Fuller (oft abgekürzt zu "R. Buckminster Fuller," auch "Bucky Fuller" genannt; * 12. Juli 1895 in Milton, Massachusetts; † 1. Juli 1983 in Los Angeles) war ein US-amerikanischer Architekt, Konstrukteur, Visionär, Designer, Philosoph und Schriftsteller.

Fuller stammte aus einer wohlhabenden Familie. Er war eines von vier Kindern des Lederwarenhändlers Richard Buckminster Fuller und dessen Frau Caroline Wolcott Andrews. Sein Cousin war der Journalist und Schriftsteller John Phillips Marquand, der 1938 den Pulitzer-Preis für seinen Roman "Der selige Mister Apley" erhielt.

1914 lernte er Anne Hewlett (1896–1983), Tochter des prominenten New Yorker Architekten James Monroe Hewlett, kennen. Die beiden heirateten in New York am 12. Juli 1917, seinem 22. Geburtstag. Sie hatten zwei Töchter, Alexandra (1918–1923) und Allegra (* 1927). Alexandra starb mit vier Jahren an Kinderlähmung und spinaler Meningitis. Allegra Fuller Snyder gründete das Board of Directors des Buckminster Fuller Institute und wurde dessen Vorsitzende.
Fuller ist der Großneffe von Margaret Fuller, einer prominenten Frauenrechtlerin und Vertreterin des Transzendentalismus.

Er begann 1912 in Harvard zu studieren, brach das Studium jedoch ab und wurde Marinesoldat. 1927, im Alter von 32 Jahren, war er bankrott und ohne Anstellung, und nach dem Tode seines ersten Kindes nahe daran, Selbstmord zu begehen. Er beschloss, sein weiteres Leben als Experiment zu verstehen: Er wollte feststellen, was eine einzelne Person dazu beitragen kann, die Welt zum Nutzen der Menschheit zu verändern. Er begann sein Leben peinlich genau in einem Tagebuch zu dokumentieren, das er dann ein halbes Jahrhundert lang führte.

Nach mehreren Tätigkeiten in der Industrie, unter anderem in der Exportabteilung eines fleischverarbeitenden Unternehmens, begann er als Architekt zu arbeiten. In den späten zwanziger Jahren hatte er einigen Erfolg mit neuen Gebäudekonzepten, die er unter dem Namen Dymaxion (Dymaxion-Globus) der Öffentlichkeit vorstellte. Weitere Entwürfe energie- und/oder materialeffizienter Konstruktionen (zum Beispiel ein stromlinienförmiges Auto, ein Badezimmer, Tensegrity) folgten, wurden patentiert und teilweise ebenfalls unter dem Warenzeichen "Dymaxion" vermarktet. 1970 wurde Fuller zum Mitglied ("NA") der National Academy of Design gewählt. 

Bekannt wurde Fuller durch seine "Domes" oder geodätischen Kuppeln, auch Fuller-Kuppeln genannt, die man meist auf Ausstellungen, in Science-Fiction-Filmen oder als Teil von Militäranlagen (Radarkuppeln) sehen kann. Die bekannteste ist die Biosphère, der Ausstellungspavillon der Vereinigten Staaten an der Expo 67 in Montreal. Auch Achterbahnen fanden in diesen Kugeln Platz (zum Beispiel der Eurosat im Europa-Park bei Rust). Sie basieren auf einer Weiterentwicklung von einfachsten geometrischen Grundkörpern (Tetraeder als 3-Simplex, Oktaeder und dichteste Kugelpackungen) und sind extrem stabil und mit geringstem Materialaufwand realisierbar. Das Konstruktionsprinzip wurde 1954 patentiert.

Fuller hat als einer der Ersten das Wirken der Natur als durchgängiges systemisches Wirken unter wirtschaftlichen Prinzipien (Material- und Energieeffizienz und die damit verbundene Tendenz zur Ephemerisierung) gesehen. Ein anderer wichtiger Aspekt war für ihn das Entdecken von nutzbaren Synergien, ein Begriff, den er mitprägte.

Er wirkte als Designer, Wissenschaftler, Forscher, Entwickler und Schriftsteller und propagierte dabei schon frühzeitig globale und kosmische Sichtweisen, wie in „Bedienungsanleitung für das Raumschiff Erde“. 
Den Sinn des Lebens in der Moderne untersuchte er mit der Frage nach der „Integralfunktion des Menschen im Universum“. Seine in späteren Jahren entwickelten Methoden und technischen Konstruktionen versuchen Minimalprinzipien in den Bereichen der Technik fortzuentwickeln, um damit zur Vermeidung des „kosmischen Bankrottes“ der Menschheit Mittel zur nachhaltigen Fortentwicklung unserer Zivilisation bereitzustellen. Diesem Zweck diente auch die Propagierung des World Game, das – im Gegensatz zu Konfliktsimulationen – praktisch aufzeigen sollte, wie spontane Kooperation das Leben aller Menschen verbessern kann.

R. Buckminster Fuller starb 1983 in Los Angeles, keine 36 Stunden vor seiner Frau Anne. Das Grab der beiden befindet sich auf dem Friedhof Mount Auburn in Cambridge, Massachusetts.

1968 wurde er Professor an der Southern Illinois University Carbondale, später auch an der University of Pennsylvania. Fuller erhielt zwischen 1954 und 1981 47 Ehrendoktorate und über 100 Auszeichnungen und Preise.

Nach ihm wurde die dritte bekannte elementare Modifikation des Kohlenstoffs benannt, die Fullerene, deren chemische Struktur an seine Kuppelbauten erinnert. Das bisher am besten erforschte Fulleren C wird auch als "Buckminster-Fulleren" beziehungsweise "Bucky Ball" bezeichnet.

Fuller war Mitglied und später auch Vorsitzender des Vereins für Hochbegabte Mensa International. Außerdem wurde er 1963 in die American Academy of Arts and Letters und 1968 in die American Academy of Arts and Sciences aufgenommen.

2004 ehrte ihn der United States Postal Service mit einer Briefmarke zum 50. Jahrestag der Patentierung der geodätischen Kuppeln.


Der amerikanische Künstler Matthew Day Jackson ehrte Fuller durch das im Jahre 2007 entstandene Porträt "Bucky" aus Holz mit einem aus Perlmutt eingelegten Auge. Das Objekt wurde im Jahre 2010 bei Christie’s in New York an den Juwelier Laurence Graff verkauft.

Konzepte und Gebäude:







</doc>
<doc id="475" url="https://de.wikipedia.org/wiki?curid=475" title="Bayern">
Bayern

Der Freistaat Bayern () ist eines der 16 Länder in Deutschland und liegt in dessen Südosten. Mit mehr als 70.500 Quadratkilometern ist es das flächenmäßig größte und mit rund 12,9 Millionen Einwohnern nach Nordrhein-Westfalen das zweitbevölkerungsreichste deutsche Land.

Der Freistaat hat im Süden Anteil am Hochgebirge der Ostalpen und dem bis zur Donau reichenden flachen Alpenvorland. Nördlich der Donau bestimmen Mittelgebirge wie etwa der Bayerische Wald oder das Fichtelgebirge das Landschaftsbild. Größte Stadt Bayerns ist die Landeshauptstadt und Millionenmetropole München, gefolgt von Nürnberg und Augsburg. Weitere Großstädte sind Regensburg, Ingolstadt, Würzburg, Fürth und Erlangen.

Bayern ist eine parlamentarische Republik mit dem Bayerischen Landtag als Legislative und der Bayerischen Staatsregierung, an deren Spitze der Ministerpräsident als Regierungschef steht, als Exekutive. Grundlage der Politik ist die Verfassung des Freistaates Bayern, gemäß der Bayern ein Volks-, Rechts-, Kultur- und Sozialstaat ist. Die Bezeichnung "Freier Volksstaat" bzw. "Freistaat" als monarchiefreie Republiken trugen in Bayern erstmals 1918 zwei Staaten, mit der Ausrufung durch den revolutionären Ministerpräsidenten Kurt Eisner am 8. November 1918 der "Freie Volksstaat Bayern" und der "Freistaat Coburg", der sich 1920 mit Bayern vereinigte.

Bereits im Jahre 555 n. Chr. soll ein bayerisches Stammesherzogtum existiert haben, das unter den Merowingern Teil des fränkischen Herrschaftsbereichs wurde. Zwischen 1180 und 1805 wurde Bayern als Territorialherzogtum sowie im Zeitraum 1806 und 1808 als absolute Monarchie von den Wittelsbachern regiert. Durch die Gewährung der Verfassungen von 1808, 1818 und 1848 wurde Bayern konstitutionelle Monarchie. Napoleon Bonaparte erhob 1805 Bayern zum Königreich, die Proklamation des ersten Königs erfolgte am 1. Januar 1806. Damit war die vollkommene Souveränität verbunden. Bayern konnte auf dem Wiener Kongress 1814 als eine der Siegermächte einen großen Teil der Gebietsgewinne behalten; unter anderem kamen das heutige Nordbayern, Teile Schwabens und die neugeschaffene linksrheinische Pfalz zu Bayern. 1918 brach die Wittelsbachermonarchie in der Novemberrevolution zusammen. Nach der Besetzung durch US-amerikanische Truppen wurde Bayern 1949 Teil der neu gegründeten Bundesrepublik. Die Pfalz wurde von Bayern abgetrennt und ist heute Teil von Rheinland-Pfalz. Es begann ein wirtschaftlicher Aufschwung und eine Entwicklung vom Agrarstaat zum modernen Industriestaat.

Bayern liegt fast vollständig im oberdeutschen Sprachraum. Traditionell gliedert es sich in die drei Landesteile Franken (heute die Regierungsbezirke Ober-, Mittel- und Unterfranken), Schwaben (gleichnamiger Regierungsbezirk) und Altbayern (Regierungsbezirke Oberpfalz, Ober- sowie Niederbayern).

Die allein verwendete Schreibweise des Landesnamens mit „y“ geht auf eine Anordnung von König Ludwig I., König von Bayern, vom 20. Oktober 1825 zurück, mit der die vorher geltende Schreibweise „Baiern“ abgelöst wurde. Diese Anordnung des Königs und seine Vorschrift des „griechischen“ Ypsilons steht im Zusammenhang mit dem Philhellenismus. Bis dahin wurde der Landesname oft mit „i“ geschrieben, wenngleich es viele ältere Belege mit einem „y“ gibt. Während der Münchner Räterepublik kehrte man vorübergehend zur Schreibweise „Baiern“ zurück.

Der Stamm der einheimischen Bevölkerung Altbayerns (also ohne Franken und bayerisches Schwaben) wird aber weiterhin als "die Baiern" (in anderen Abschnitten auch Altbayern genannt) bezeichnet. Vgl. auch: Bairisch bzw. Bajuwaren.

Der Begriff Bayern bzw. Baiern ist auf das Volk der Bajuwaren zurückzuführen. Der volle Name der Bajuwaren wird hergeleitet aus einem mutmaßlichen germanischen Kompositum *"Bajowarjōz" (Plural). Überliefert ist dieser Name als althochdeutsch "Beiara, Peigira," latinisiert "Baiovarii". Es wird angenommen, dass es sich dabei um ein Endonym handelt. Hinter dem Erstglied "Baio" steckt das Ethnikon des zuvorbewohnenden keltischen Stammes der Boier, der im althochdeutschen Landschaftsnamen "Bēheima" ‚Böhmen‘ (germanisch *"Bajohaimaz" ‚Heim der Boier‘, spätlateinisch dann "Boiohaemum") und im onomastischen Anknüpfungspunkt ("Baias, Bainaib" usw.) erhalten ist. Das Zweitglied "-ware" bzw. "-varii" der Bewohnerbezeichnung "Bajuwaren" stammt aus *"warjaz" ‚Bewohner‘ (vgl. altnordisch "Rómverjar" ‚Römer‘, altenglisch "burhware" ‚Stadtbewohner‘), die noch aus indogermanischer Zeit stammt (vgl. walisisch "gwerin" ‚Menschenmenge‘). Der Baiernname wird deshalb als ‚Männer aus Böhmen‘ gedeutet. Die Namensdeutung ist allerdings weiterhin umstritten.

Als Binnenland grenzt Bayern an folgende Staaten: im Osten an Tschechien, im Südosten und Süden an Österreich, im Südwesten über den Bodensee indirekt an die Schweiz. Hinzu kommen die deutschen Bundesländer Baden-Württemberg (im Westen), Hessen (im Nordwesten), Thüringen (im Norden) und Sachsen (im Nordosten). Die Landesgrenze Bayerns ist insgesamt 2705 Kilometer lang. Bayern grenzt, im Westen beginnend, im Uhrzeigersinn an Baden-Württemberg (829 Kilometer Grenzlänge), Hessen (262 Kilometer), Thüringen (381 Kilometer), Sachsen (41 Kilometer), an die tschechischen Regionen Karlsbad, Pilsen und Südböhmen (357 Kilometer), an die österreichischen Länder Oberösterreich, Salzburg, Tirol und Vorarlberg (816 Kilometer) sowie an den Schweizer Kanton St. Gallen (19 Kilometer), wobei der Grenzverlauf im Bodensee nicht festgelegt ist.

Bis 1990 bildete die Grenze zu Thüringen, Sachsen und der damaligen Tschechoslowakei einen Abschnitt des Eisernen Vorhangs. Sie stellte durch die Grenzsicherungsanlagen eine physisch nahezu unüberwindbare Trennung zwischen der NATO und dem Warschauer Pakt dar. Bei Prex gab es ein Dreiländereck. Nicht zum bayerischen Staatsgebiet und daher nicht zum deutschen Bundesgebiet gehören die in Österreich gelegenen Saalforsten, die im privatrechtlichen Eigentum des Freistaates Bayern stehen. Andererseits gehört der Egerer Stadtwald, der historisch zur böhmischen Stadt Eger (tschechisch. Cheb) gehört, zum bayerischen Staatsgebiet und wird von einer Stiftung verwaltet.

Bayern liegt in Süddeutschland und umfasst:


Die Bayerischen Alpen im äußersten Süden Bayerns gehören den Nördlichen Kalkalpen an. Bayern ist das einzige deutsche Bundesland, das Anteil an den Alpen hat. Meist werden unter den Bayerischen Alpen nur die zwischen den Flüssen Lech und Saalach gelegenen Gebirgsteile verstanden. In diesem engeren Sinn zählen daher die Allgäuer Alpen, auf die sich das bayerische Staatsgebiet erst seit jüngerer Zeit erstreckt, und die Berchtesgadener Alpen nicht zu den Bayerischen Alpen. Er ist nicht mit dem Begriff der Bayerischen Voralpen, die nördlich an das Gebirge angrenzen, zu verwechseln. Letztere umfassen nur den bayerischen Anteil der Voralpen zwischen der Loisach im Westen und dem Inn im Osten. Während die Voralpen nur vereinzelt ausgeprägte Kalkfelswände zeigen, sind die Alpen durch die im Jungpleistozän entstandenen Kare, Seen und die typischen U-Täler durch Gletscher geprägt. Ablagerungen der eiszeitlichen Flüsse sowie vor allem die Gletscher ließen insbesondere im Alpenvorland eine hügelige Landschaft mit Seen und Mooren entstehen. Hier liegen etwa der Chiemgau, das Fünfseenland und das Allgäu.

Während zwischen den Alpen und südlich der Donau das Gelände flach bis hügelig ist, liegen nördlich davon mehrere Gebirge, die eine Höhe von über Tausend Metern erreichen, darunter beispielsweise der Bayerische Wald mit dem Großen Arber als höchsten Berg Bayens außerhalb der Alpen und das Fichtelgebirge mit dem Schneeberg als höchsten Berg Frankens. Die Fränkische Alb als geologische Fortsetzung des Schweizer Juras und der Schwäbischen Alb zieht sich um einen Bogen durch den Norden Bayerns und schirmt Teile Frankens von Altbayern ab. Nördlich davon liegen zahlreiche Zeugenberge wie etwa der Hesselberg. Der äußerste Südwesten der Alb grenzt ans Nördlinger Ries, Rest eines während des Ries-Ereignisses vor etwa 14,6 Millionen Jahren entstandenen Einschlagkraters. Das Fränkische Keuper-Lias-Land, in dem etwa Aischgrund, Steigerwald und Frankenhöhe liegen, geht in die Mainfränkische Platten über. Südwestlich davon liegen die Mittelgebirge Odenwald, Spessart und Rhön. Die östliche Hälfte Bayerns wird hingegen von Mittelgebirgen wie dem Bayerischen Wald oder dem Frankenwald geprägt. Hier befindet sich das größte nicht zerschnittene Waldgebiet Mitteleuropas. Teile des Vogtlands liegen in Bayern. Der westliche Teil Unterfrankens als Bestandteil der Tiefebene des Rheins gehört zum Bayerischen Untermain.

Die niedrigste Stelle Bayerns ist mit der Wasserspiegel des Mains in Kahl am Main (Unterfranken), die höchste auf dem Gipfel der Zugspitze (), des höchsten deutschen Berges im Wettersteingebirge. Alle 30 höchsten Berggipfel Deutschlands liegen in den Bayerischen Alpen, konzentriert in den Berchtesgadener Alpen, den Allgäuer Alpen und dem Wettersteingebirge.

Der geographische Mittelpunkt Bayerns liegt etwa 500 Meter östlich des Marktes Kipfenberg im Landkreis Eichstätt . Historisch betrachteten sich mehrere Orte in Bayern als Mittelpunkt Europas. Seit dem EU-Beitritt Kroatiens am 1. Juli 2013 lag der geographische Mittelpunkt der Europäischen Union im Landkreis Aschaffenburg, im Ortsteil Oberwestern der bayerischen Gemeinde Westerngrund (). Durch den Beitritt des französischen Übersee-Départements Mayotte, einer Inselgruppe in Nordwesten Madagaskars, hat er sich nochmals um 500 Meter verschoben ().

Der bedeutendste Fluss des Bundeslandes ist die Donau, diese fließt in der Südhälfte des Landes von West nach Ost, gelangt bei Ulm auf das Landesgebiet und tritt bei Passau nach Österreich über. Ihre größten Nebenflüsse sind (stromabwärts):


Die vier südlichen Nebenflüsse entspringen in den Alpen und sind wasserreicher. Inn und Lech führen (wegen des langen Oberlaufs) bei ihrer Mündung meist etwas mehr Wasser als die Donau.

Im Heimatkundeunterricht wird zur Donau vielerorts folgender Merkspruch aufgesagt: „Brigach und Breg bringen die Donau zu weg. Iller, Lech, Isar, Inn, fließen rechts zur Donau hin. Wörnitz, Altmühl, Naab und Regen fließen links dagegen.“

Der größte Teil Frankens wird durch den Main von Ost nach West in den Rhein entwässert. In seinem markant geschwungenen Lauf bildet er das sog. Maindreieck und Mainviereck. Seine größten Nebenflüsse sind die Regnitz und Tauber von links und die Fränkische Saale von rechts. Im Nordosten Oberfrankens entspringen die linken Nebenflüsse der Elbe, die Sächsische Saale und die Eger.

In den Endmoränenlandschaften im südlichen Teil des nördlichen Alpenvorlandes gibt es viele Seen, die teilweise ins Gebirge hineinragen, etwa der Tegernsee, der Starnberger See und der Schliersee. Bayern hat Anteil am Bodensee, dem größten See des westlichen Mitteleuropa. Der größte See innerhalb Bayerns ist der Chiemsee. Nördlich der Fränkischen Alb wurden die Stauseen des Fränkischen Seenlands gebildet. Sie dienen zur Wasserregulierung Nordbayerns, insbesondere der Wasserversorgung des Main-Donau-Kanals, einer wichtigen Wasserstraße in Nordbayern. In den Alpen wurde 1924 das Walchenseekraftwerk in Betrieb genommen, das das natürliche Gefälle zwischen dem als „Oberbecken“ fungierenden Walchensee und dem „Unterbecken“ Kochelsee zur Stromerzeugung nutzt.

Durch Teile Bayerns führt die Europäische Hauptwasserscheide. Sie trennt die Flusssysteme von Rhein im nördlichen und Donau im südlichen Teil Bayerns.

Entlang dem Nordrand der Alb gibt es etliche Steinerne Rinnen. Die etwa 150 Meter lange und 0,2 Meter hohe Käsrinne bei Heidenheim und die Steinerne Rinne bei Wolfsbronn mit 128 Metern Länge und 1,5 Metern Höhe sind die längsten dieser „wachsenden Bäche“ in Bayern.

In Bayern liegen der Nationalpark Bayerischer Wald und der Nationalpark Berchtesgaden. Von der UNESCO anerkannte Biosphärenreservate sind das Biosphärenreservat Berchtesgadener Land und das Biosphärenreservat Rhön. Es gibt 18 Naturparke in Bayern, das älteste ist das 1969 gegründete Naturpark Altmühltal.

In Bayern sind 603 Naturschutzgebiete, 702 Landschaftsschutzgebiete, 674 FFH-Gebiete, 84 Europäischen Vogelschutzgebiete, 160 Naturwaldreservate und über 3.400 Geotope vom Bayerischen Landesamt für Umwelt ausgewiesen (Stand: März 2017). Einhundert besonders sehenswerte Geotope sind als Bayerns schönste Geotope ausgewiesen. Größtes Naturschutzgebiet sind die Allgäuer Hochalpen, kleinstes ist der Drabafelsen.

Siehe auch:
Mit 70.550,19 Quadratkilometern ist Bayern das flächenmäßig größte deutsche Bundesland und hat damit knapp 22.000 Quadratkilometer mehr Fläche als Niedersachsen. Der Freistaat entspricht etwa 19 Prozent der deutschen Staatsfläche. Bayern ist größer als die meisten Staaten Europas, etwa die Niederlande oder Irland.

Das Staatsgebiet Bayerns erstreckt sich von 47° 16′ bis zu 50° 34′ nördlicher Breite und von 8° 58′ bis 13° 50′ östlicher Länge. Bayern erstreckt sich in west-östlicher Richtung über maximal 384, in nord-südlicher über 362 Kilometer. Der südlichst gelegene Ort in Bayern ist Einödsbach, der westlichste Großwelzheim, der nördlichste Weimarschmieden und der östlichste Breitenberg. Die südlichste Stelle Bayerns und ganz Deutschlands ist der Haldenwanger Eck. Die Mitteleuropäische Zeit (MEZ) ist der Ortszeit an der äußersten Westgrenze des Landes um 24 Minuten und 8 Sekunden, an der äußersten Ostgrenze um 4 Minuten und 40 Sekunden voraus.

Etwa fünf Sechstel (84,2 Prozent) der Fläche werden land- und forstwirtschaftlich genutzt. 11,5 Prozent sind Siedlungs- und Verkehrsflächen.

Das Klima geht vom Nordwesten (relativ ausgeglichen) nach Osten vom Seeklima ("Cfb") ins Kontinentalklima ("Dfb") über. An etwa 100 Tagen sind die Temperaturen unter null Grad Celsius, die Westwinde bringen durchschnittlich 700 mm Niederschlag, im Nordstau der Alpen lokal bis 1800 mm pro Jahr. Die mittlere jährliche Sonnenscheindauer beträgt 1600 bis 1900 Stunden. Der wärmste Monat ist meist der Juli, kältester der Januar. Der Föhn beeinflusst das Wetter im gesamten Alpenvorland und kann stellenweise bis an die Fränkische Alb reichen. Der Norden Bayerns ist trockener und wärmer als der Süden; die Region um Würzburg weist die meisten Sonnentage Süddeutschlands auf. Der Klimawandel zeigt sich auch verstärkt in Bayern: Gletscher schmelzen und extreme Wetterlagen, wie etwa das Hochwasser in Mitteleuropa 2013, nehmen zu.

Bayern wäre von Natur aus hauptsächlich von Wäldern bedeckt. Im Flach- und Hügelland würden buchendominierte Mischwälder vorherrschen, die in den Gebirgen in Bergmischwälder übergehen würden. In den höheren Gebirgslagen würden Fichtenwälder vorkommen und die Flüsse würden von ausgedehnten Auwäldern begleitet. Nur die Gewässer und die Gebirgslagen oberhalb der Waldgrenze sowie Sonderstandorte wie Hochmoore wären natürlicherweise nicht bewaldet. Durch umfangreiche Rodungen für landwirtschaftliche Flächen und Siedlungen hat der Mensch bereits aber im Mittelalter die Waldfläche in Bayern zurückgedrängt. Aktuell sind mit 2,6 Millionen Hektar 36,9 Prozent der bayerischen Landesfläche bewaldet. Damit befindet sich rund ein Viertel der deutschen Wälder in Bayern. Die Baumartenzusammensetzung der Wälder in Bayern ist stark von der forstwirtschaftlichen Nutzung geprägt. Die häufigste Baumart in Bayerns Wälder ist die Gemeine Fichte mit 41,8 Prozent Flächenanteil, gefolgt von der Waldkiefer mit 17,9 Prozent, der Rotbuche mit 13,9 Prozent und den Eichen mit 6,8 Prozent Anteilen. Besonders große Waldgebiete finden sich noch in den Mittelgebirgen in Nord- und Ostbayern, wie zum Beispiel im Spessart, im Fichtelgebirge, im Steigerwald und im Bayerischen Wald, sowie in den Bayerischen Alpen.

Dagegen sind insbesondere die Gegenden mit fruchtbaren Böden im Voralpenland, im Hügelland und in den Flussniederungen von überwiegend landwirtschaftlich genutzten Offenlandschaften mit Wiesen, Äckern und nur wenigen Einzelbäumen und kleineren Wäldern geprägt. Franken weist gebietsweise für Süddeutschland einzigartige Sandlebensräume auf, die als Sandachse Franken geschützt sind. In den Flusstälern entlang von Main und Tauber wurde die Landschaft für den Weinanbau umgestaltet. Weit verbreitet sind Magerrasen, ein extensiv genutztes Grünland an besonders nährstoffarmen, „mageren“ Standorten. Besonders die Südliche Frankenalb mit dem Altmühltal ist gekennzeichnet von solchen Magerrasen. Viele dieser Gebiete sind als Schutzgebiet ausgewiesen.

In den Wäldern Bayerns leben, wie in anderen Teilen Deutschlands, nur noch relativ wenige Großtierarten. Es gibt hier unter anderem verschiedene Marderarten, Dam- und Rothirsche, Rehe sowie Wildschweine und Füchse. In naturnahen Gebieten wie dem Fichtelgebirge leben Luchse und Auerhähne, aber auch Biber und Otter verbreiten sich wieder. Vereinzelt gibt es Sichtungen von seit längerem in Mitteleuropa ausgerotteten Tieren in Bayern, beispielsweise vom Wolf. In hochalpinen Regionen leben der wieder eingebürgerte Alpensteinbock und das Alpenmurmeltier. Seltener ist die Gämse in einigen Mittelgebirgen, wie der Fränkischen Alb, beheimatet. Der Steinadler kommt in den Bayerischen Alpen vor.

Zur Zeit des Kaisers Augustus wurde das keltisch besiedelte Gebiet Altbayerns südlich der Donau Teil des Römischen Reiches. Nach Zusammenbruch der römischen Herrschaft bildete sich das Volk der Bajuwaren. Vermutlich haben sich die Bajuwaren aus verschiedenen Volksgruppen gebildet:

Es wird von einer Stammesbildung der Bajuwaren im eigenen Land, also dem Land zwischen Donau und Alpen, ausgegangen.

Wie bereits oben erwähnt ist für das Jahr 555 n. Chr. die Existenz eines bairischen Stammesherzogtumes mit Sitz in Freising und Regensburg unter den Agilolfingern belegt, das unter den Merowingern Teil des fränkischen Herrschaftsbereichs "Austrasien" wurde. Der Sieg Karls des Großen über den Bayernherzog Tassilo III. 788 markiert das Ende des sogenannten Älteren Stammesherzogtums. Seit 788 bis zum Beginn des 10. Jahrhunderts gab es keinen baierischen Herzog. Die Karolinger regierten als bayerische Könige oder Unterkönige und setzten zur Herrschaftsausübung bisweilen Statthalter (Präfekten) ein.

Der Niedergang der Karolinger ermöglichte ein Wiederaufleben der Eigenständigkeit der bayerischen Herzöge im sogenannten Jüngeren Stammesherzogtum. Nach Ende der Herrschaftsperiode der Karolinger kam es erneut dazu, dass die Eigenständigkeit der einzelnen Gebiete allmählich erstarkte. Unterstützt wurde dies durch die Bedrohung von außen durch die Ungarneinfälle ab etwa 862. Markgraf Luitpold von Bayern fiel 907 in der Schlacht von Pressburg in einer Niederlage gegen die Ungarn, jedoch wird das Datum durch den Antritt seines Sohns Arnulf I. als Herzog von Bayern gleichzeitig als Beginn des jüngeren baierischen Stammesherzogtums gesehen. Nach dem Sieg in der Schlacht auf dem Lechfeld erfolgte eine zweite Welle baierischer Ostsiedlung mit Gewinn von Gebieten im heutigen Niederösterreich, Istrien und der Krain. Der Streit mit den Ottonen führte wieder zu einer starken Abhängigkeit vom deutschen Königtum. 976 wurde der Südosten Bayerns als Teil eines neu geschaffenen Herzogtums Kärnten abgetrennt. Zusätzlich regierte das Geschlecht der Babenberger in der "Marcha Orientalis" (Ostarrichi) zunehmend unabhängiger vom bayerischen Herzog.

Ab 1070 kam es unter den Welfen zu einem Wiedererstarken der Macht der bayerischen Herzöge. 1180 stürzte Friedrich I. Barbarossa auf Betreiben der Fürsten Herzog Heinrich den Löwen, den Herzog von Bayern und Sachsen. Das Herzogtum Bayern wurde durch die Abtrennung der Steiermark und der andechsischen Markgrafschaft Istrien weiter verkleinert.

Von 1180 bis 1918 wurde Bayern als Territorialherzogtum von den Wittelsbachern regiert. Es erlebte von 1255 bis 1503 eine Periode zahlreicher Teilungen in Einzelherzogtümer. In einer kurzen Zeit der Wiedervereinigung erlangte Ludwig IV. 1328 als erster Wittelsbacher die Kaiserwürde, was für Bayern einen neuen Höhepunkt der Macht bedeutete. Im Hausvertrag von Pavia von 1329 teilte er den Besitz in eine pfälzische Linie mit der Rheinpfalz und der später so genannten Oberpfalz und in eine altbayerische Linie auf. Die von ihm neu hinzugewonnenen Gebiete Brandenburg, Tirol, die niederländischen Provinzen Holland, Seeland und Friesland sowie der Hennegau gingen unter seinen Nachfolgern sehr bald wieder verloren. Tirol fiel 1363 an die Habsburger, Brandenburg 1373 an die Luxemburger. Mit der Goldenen Bulle 1356 ging die Kurfürstenwürde für die altbayerische Linie an die Pfalz verloren.

1429 wurde nach dem Aussterben der Linie Straubing-Holland das Herzogtum Bayern-Straubing unter den Linien München, Ingolstadt und Landshut aufgeteilt. 1447 fiel Bayern-Ingolstadt an Bayern-Landshut, das seinerseits 1503 im Landshuter Erbfolgekrieg von Bayern-München gewonnen wurde. Durch das Primogeniturgesetz von Herzog Albrecht IV. von 1506 fanden die Landesteilungen ein Ende.

In der Gegenreformation nahm Bayern eine führende Stellung ein und ging aus dem Dreißigjährigen Krieg mit Gebietsgewinnen und dem Aufstieg zum Kurfürstentum hervor: 1620 besiegten die Truppen der Katholischen Liga unter Führung des bayerischen Feldherrn Tilly in der Schlacht am Weißen Berge bei Prag die Protestanten. Anschließend ließ Tilly die Pfalz besetzen. Zum Dank erhielt Maximilian I. 1623 die Kurfürstenwürde und 1628 die von ihm besetzte Oberpfalz als Kriegsentschädigung. Nach dem Krieg widmete sich Kurfürst Ferdinand Maria dem Wiederaufbau des verwüsteten Landes und verfolgte eine vorsichtige Neutralitätspolitik.

Während des Spanischen und Österreichischen Erbfolgekrieges und im Zuge der Großmachtpolitik Maximilians II. Emanuels und seines Sohnes Karl Albrecht wurde das absolutistische Bayern vorübergehend von Österreich besetzt. 1705 erhob sich das bayerische Volk gegen die kaiserliche Besatzung. Die bayerische Volkserhebung umfasste weite Gebiete Niederbayerns, das Innviertel und das östliche Oberbayern. Ein Landesdefensionskongress tagte im Dezember 1705 im damals noch bayerischen Braunau am Inn. Erst die Schlacht von Aidenbach am 8. Januar 1706 endete mit der völligen Niederlage der Volkserhebung. Maximilian III. Joseph beendete 1745 endgültig die Großmachtpolitik seiner Vorgänger und widmete sich inneren Reformen. Nach dem Aussterben der altbayerischen Linie der Wittelsbacher entstand 1777 das Doppel-Kurfürstentum Kurpfalz-Bayern unter der Regentschaft des Kurfürsten Karl Theodor aus der pfälzischen Linie der Wittelsbacher.

Zur Zeit Napoleons stand Bayern anfangs auf der Seite Frankreichs und konnte durch Säkularisation und Mediatisierung große Gebietsgewinne verzeichnen. So fielen Salzburg, Tirol, Vorarlberg sowie das Innviertel vorübergehend an Bayern. Im Frieden von Pressburg, der am 26. Dezember 1805 zwischen Frankreich und dem deutschen Kaiser Franz II. abgeschlossen wurde, wurde das mit Napoleon verbündete Bayern zum Königreich proklamiert. König Maximilians Minister Maximilian Graf von Montgelas gilt dabei als Schöpfer des modernen bayerischen Staates. Das neue Königreich beseitigte alle Relikte der Leibeigenschaft, die das alte Reich hinterlassen hatte. Durch das Religionsedikt von 1803 wurden alle drei christlichen Bekenntnisse gleichberechtigt – Katholiken, Reformierte und Lutheraner. 1807 wurden die ständischen Steuerprivilegien abgeschafft. 1805 wurden alle erblichen und käuflichen Ämter durch die große Dienstespragmatik abgeschafft. Das Münchner Regulativ von 1805 und das Juden-Edikt von 1813 gewährte den Israeliten im neuen Bayern erste Freiheiten. Am 27. August 1807 führte Bayern als erstes Land der Welt eine Pockenimpfung ein. 1812 wurde die bayerische Gendarmerie gegründet. Durch ein neues Strafgesetzbuch, das Anselm von Feuerbach entworfen hatte, wurde 1813 die Folter abgeschafft. Das Fürstentum Ansbach fiel 1806 durch einen von Napoleon erzwungenen Gebietstausch an das Königreich Bayern, das protestantische Fürstentum Bayreuth wurde 1810 von Napoleon an Bayern verkauft. Durch den rechtzeitigen Wechsel auf die Seite der Gegner Napoleons im Vertrag von Ried konnte Bayern auf dem Wiener Kongress 1814 als Siegermacht einen Teil der Gebietsgewinne behalten. Für den Verlust Tirols und der rechtsrheinischen Pfalz wurde es durch wirtschaftlich weiter entwickelte Gebiete um Würzburg und Aschaffenburg entschädigt. Der 1816 neugeschaffene linksrheinische Rheinkreis kam zu Bayern und war ab 1837 die Pfalz (Bayern).

König Ludwig I., der seit 1825 regierte, baute die bayerische Hauptstadt München zur Kunst- und Universitätsstadt aus. Er führte die Zensur wieder ein und beseitigte die Pressefreiheit. Das Hambacher Fest 1832 in der Pfalz auf dem Hambacher Schloss bei Neustadt an der Weinstraße hatte seine Wurzeln in der Unzufriedenheit der pfälzischen Bevölkerung mit der bayerischen Verwaltung. Wegen einer Affäre mit der Tänzerin Lola Montez musste er 1848 im Zuge der Märzunruhen abdanken. Unter seinem Sohn und Nachfolger Maximilian II. kam es zu einer schrittweisen Liberalisierung aber auch zum Pfälzischer Aufstand. Der König war ein Anhänger der Trias-Politik, die vorsah die deutschen Mittelstaaten unter Führung Bayerns zur dritten Kraft neben den beiden Großmächten Preußen und Österreich zu entwickeln. 1864 wurde Maximilians Sohn Ludwig II. zum König von Bayern proklamiert. Er ging wegen des Baues von Neuschwanstein und anderer Schlösser als "Märchenkönig" in die Geschichte ein. Im Deutschen Krieg 1866 erlitt Bayern an der Seite Österreichs eine Niederlage gegen Preußen. 1871 wurde es Teil des neu gegründeten Deutschen Reiches, erhielt dabei sogenannte Reservatrechte (eigenes Post-, Eisenbahn- und Heereswesen). 1886 übernahm Prinzregent Luitpold die Regentschaft. Die „Prinzregentenzeit“, wie die Epoche Prinz Luitpolds häufig bezeichnet wird, gilt aufgrund der politischen Passivität Luitpolds als Ära der allmählichen Rückstellung bayerischer Interessen hinter die des Reichs.

Im Rahmen der Novemberrevolution wurde die Wittelsbacher Monarchie abgesetzt. In Folge rief am 8. November 1918 Kurt Eisner, Schriftsteller und Journalist, Gründungsmitglied der USPD, Bayern einen Volksstaat bzw. Freistaat aus, den "Freien Volksstaat Bayern". 1919 konnten sozialistische Gruppen für kurze Zeit eine Räterepublik installieren.

Durch eine Volksabstimmung kam 1920 der Freistaat Coburg zu Bayern (→ Landkreis Coburg).

Durch die Bestimmungen des Versailler Vertrages verlor Bayern 1920 den westlichsten Teil der bayerischen Pfalz, der dem neugegründeten Saargebiet zugeschlagen wurde.

Nach der Münchner Räterepublik blieb Bayern eine Hochburg konservativer und nationalistischer Kräfte, sie wurde als „Ordnungszelle des Reiches“ bezeichnet. Am 8. und 9. November 1923, zur Zeit der Weimarer Republik, wurde Bayern Schauplatz des Hitlerputsches.

Als Verwaltungseinheit war Bayern während der Zeit des Nationalsozialismus (1933 bis 1945) weitgehend bedeutungslos. Die Stadt München, in der Adolf Hitler seit 1919 lange gelebt und seinen politischen Aufstieg begonnen hatte, wurde von den Nationalsozialisten allerdings propagandistisch zur Hauptstadt der Bewegung stilisiert. Mit dem KZ Dachau errichtete das NS-Regime nur wenige Wochen nach der sogenannten Machtergreifung das erste durchgehend betriebene Konzentrationslager. Im fränkischen Nürnberg hielt die NSDAP von 1933 bis 1938 auf dem Reichsparteitagsgelände ihre Reichsparteitage und andere Propagandaveranstaltungen ab. Auf dem Obersalzberg nahe Berchtesgaden ließ Hitler den Berghof errichten, der ihm als zweiter Regierungssitz diente und sich zu einem zentralen Ort der Macht im nationalsozialistischen Deutschen Reich entwickelte.

Im Zweiten Weltkrieg erlitten bayerische Städte wie München, Nürnberg, Würzburg, Aschaffenburg
und Augsburg starke Zerstörungen (siehe Luftangriffe auf München, Nürnberg, Würzburg, Aschaffenburg und Augsburg).

Die Besatzungsmächte leiteten Vertriebene aus Schlesien und dem Sudetenland gezielt in das dünn besiedelte Bayern. Dadurch wuchs die Bevölkerung bis 1949 um ein Viertel. Es entstanden mehrere Vertriebenenstädte.

General Eisenhower stellte mit der Proklamation Nummer 2 vom 28. September 1945 Bayern offiziell als Staat wieder her; die Exekutive lag zwischen 1945 und 1952 in den Händen von US-amerikanischen Militärgouverneuren. Nach der Besetzung durch US-Truppen wurde Bayern Bestandteil der amerikanischen Besatzungszone, während im Jahr 1946 die in der französischen Besatzungszone gelegene stark industrialisierte Rheinpfalz dem neugebildeten Land Rheinland-Pfalz eingegliedert wurde.

Da die US-Militärregierung entschieden gegen die Wiederherstellung einer Monarchie eingestellt war, verbot sie 1946 die wiedergegründete Bayerische Heimat- und Königspartei. Diese wurde jedoch 1949 – nach dem Ende der Militärregierung – neu instituiert. Der noch immer populäre frühere Kronprinz Rupprecht von Bayern (1869–1955), den die Nationalsozialisten in den Untergrund getrieben hatten, vertrat die Auffassung, wenn es schon im Deutschen Kaiserreich auf Länderebene republikanische Landesverfassungen gegeben habe (so in den Hansestädten Hamburg und Bremen), dann könne sich doch auch Bayern als Bundesland innerhalb der Bundesrepublik Deutschland eine monarchische Verfassung geben, bei der die politischen Kompetenzen des Ministerpräsidenten von diesem gewählten Politiker, die Repräsentationsaufgaben des Landesvaters jedoch von einem erblichen Monarchen wahrgenommen würden. Ab dem 30. Juni 1946 tagte in München eine "Verfassungsgebende Versammlung". Eine neue, republikanische Verfassung des Freistaates Bayern wurde 1946 mit großer Mehrheit durch das Volk angenommen (Näheres im Artikel Bayerische Verfassungsgeschichte).

1949 wurde Bayern als Land Teil der Bundesrepublik Deutschland, obwohl das Grundgesetz vom bayerischen Landtag abgelehnt worden war. Ein langanhaltender Wirtschaftsaufschwung („Wirtschaftswunder“) trug dazu bei, dass Bayern nicht nur Agrarland blieb, sondern Heimat vieler Industriebetriebe wurde. Von 1962 bis 2008 sowie ab 2013 hatte die CSU die absolute Mehrheit in Bayern inne.

Rund 16 Prozent der deutschen Bevölkerung lebt in Bayern. Mit rund 12,6 Millionen Menschen leben im Freistaat 5 Millionen weniger Menschen als im bevölkerungsreicheren Nordrhein-Westfalen und rund 2 Millionen Menschen mehr als im bevölkerungsärmeren Baden-Württemberg. Nach England und Nordrhein-Westfalen ist Bayern die drittbevölkerungsreichste subnationale Entität der Europäischen Union und einer der bevölkerungsreicheren Gliedstaaten in der Westlichen Welt. Bayern hat mehr Einwohner als die meisten mittelgroßen Staaten: Wäre Bayern ein eigenes Land, wäre es bevölkerungsmäßig der zehntbevölkerungsreichste Staat Europas.

Bayerns Bevölkerung wächst kontinuierlich. Seit 1840 hat sich die Einwohnerzahl Bayerns mehr als verdreifacht. Zur Volkszählung 1970 wurden erstmals mehr als 10 Millionen Einwohner gezählt. Rund 50,92 Prozent der Bevölkerung ist weiblich. Von der Gesamtbevölkerung war etwas unter einem Fünftel zwischen 0 und 18 Jahre alt und etwas unter einem Fünftel älter als 65. 2012 gab es im Gebiet des Freistaates Bayern 107.039 Geburten, 125.448 Todesfälle, 845.228 Zuzüge und 753.642 Wegzüge; dies entspricht einer Bevölkerungszunahme von 73.177 Menschen. Ende 2012 entsprach der Ausländeranteil 9,6 Prozent.

Wie in ganz (West-)Deutschland kam es ab den 1950er Jahren vor dem Hintergrund des deutschen Wirtschaftswunders zu Zuwanderung insbesondere aus der Türkei, dem damaligen Jugoslawien und Italien und verstärkt um 1990 nach dem Ende des Kalten Krieges aus ostmitteleuropäischen und osteuropäischen Ländern.

Am 9. Mai 2011 lebten in Bayern 12.397.614 Menschen. Der Ausländeranteil betrug 8,2 Prozent. Es existierten 5.679.508 Privathaushalte, davon waren rund 40 Prozent Einpersonenhaushalte. Etwa ein Sechstel der Bevölkerung ist unter 18 Jahre alt, rund ein Fünftel ist über 65. 2013 gab es im Freistaat Bayern 109.562 Geburten, 126.903 Gestorbene, 888.596 Zugezogene sowie 790.949 Fortgezogene.

Anfang 1778 wurden das Kurfürstentum Bayern und die Kurpfalz durch Erbfolge der Wittelsbacher wiedervereinigt. Während in napoleonischer Zeit die rechtsrheinische Pfalz an Baden kam, fielen große Teile der linksrheinischen Pfalz 1816 als Rheinkreis an Bayern zurück. Gleichzeitig kamen durch Napoleon weitere schwäbische Gebiete und große Teile Frankens an das Königreich Bayern. Bis zum Zweiten Weltkrieg verstand man unter den vier Stämmen Bayerns die Altbayern, Franken, Schwaben und Pfälzer. Zahlreiche Monumente aus dieser Zeit betonen diesen Sachverhalt.

Nach 1945 gelangten über zwei Millionen Flüchtlinge und Heimatvertriebene, vor allem die als Sudetendeutsche bezeichneten Deutschböhmen, Deutschmährer und Sudetenschlesier, nach Bayern. Die Sudetendeutschen wurden von Franz Josef Strauß als „vierter Stamm“ bezeichnet, wobei die deutschböhmische, -mährische und sudetenschlesische Bevölkerung aus unterschiedlichen Dialekt- und Kulturräumen des deutschen Sprachraumes innerhalb der damaligen Tschechoslowakei stammte und somit heterogen ist. Sinti und Jenische haben ebenfalls eine staatlich bisher nicht anerkannte lange Tradition in Bayern.

Seit dem Zweiten Weltkrieg und dem endgültigen Verlust der Rheinpfalz 1946 wird nun die ursprünglich ansässige und die unmittelbar nach dem Zweiten Weltkrieg zugewanderte deutschböhmische Bevölkerung Bayerns in „Vier Stämme“ (siehe Deutsche Stämme für die Wortherkunft) untergliedert und setzt sich zusammen aus den drei traditionell ansässigen Stämmen, den Franken, den Schwaben und den für den Freistaat namensgebenden Altbayern sowie als viertem Stamm den Sudetendeutschen, für die Bayern die "Schirmherrschaft" übernahm.

Unterscheidungsmerkmale der drei ursprünglich ansässigen Stämme sind etwa eigene Dialekte (bairisch, ostfränkisch, alemannisch), eigene Küchen (bayerisch, fränkisch, schwäbisch), eigene Traditionen, Trachten und teilweise Architekturstile. Es werden gelegentlich den einzelnen Stämmen sogar unterschiedliche Mentalitäten zugeschrieben, so etwa sind laut Bayerischer Staatsregierung die Altbayern weltoffen, beharrlich und musisch begabt, Franken hingegen zeichnen ein ausgeprägter „Gemeinschaftssinn, Organisationstalent, Heiterkeit und ein schnelles Auffassungsvermögen“ aus, während man Schwaben als sparsame Menschen sieht, die einen „Hang zur Untertreibung“ haben. Bei Kabinettsbildungen werden aus Proporzgründen die Ämter nach den Stämmen verteilt. Auch die drei CSU-Ministerposten des Bundeskabinetts Merkel III sind auf die drei Stämme Franken, Baiern und Schwaben verteilt.

Bekannt ist vor allem der lange Streit zwischen den Franken und den Altbayern um die angebliche Ungleichbehandlung der Franken durch die Bayern. Franken sei gemäß einer Organisationen in den Parteistrukturen unterrepräsentiert und bekomme weniger Steuermittel. Auch werde sich generell weniger um Belange und Probleme der fränkischen Regierungsbezirke gekümmert. Gelegentlich wird auch die Rückgabe von sogenannter "Beutekunst", Kunstwerke aus fränkischen Städten und Schatzkammern, die vom Königreich Bayern im 19. Jahrhundert konfisziert und nach München gebracht wurden, gefordert. Viele derartige Beschwerden gelten allerdings als gegenstandslos. So sind beispielsweise fränkische Politiker in den Strukturen der großen Parteien nicht unterrepräsentiert. Auch ist die Forderung der Rückgabe von Kunstschätzen problematisch, da etwa der "Hofer Altar" in München und einige Dürer-Gemälde in der Alten Pinakothek nicht geraubt, sondern freiwillig abgegeben wurden.

Bayern ist mit 50,5 % (Stand: 2016) nach dem Saarland das Bundesland mit dem höchsten römisch-katholischen Bevölkerungsanteil in Deutschland. Weiter sind 18,8 % der Bevölkerung evangelisch-lutherisch. Diese beiden Konfessionen verteilen sich ungleich über die Bezirke. So sind Altbayern und Unterfranken überwiegend katholisch, Mittelfranken und Teile Oberfrankens protestantisch geprägt. Sowohl die Markgraftümer Ansbach und Bayreuth als auch die Mehrzahl der freien Reichsstädte (wie etwa Nürnberg oder Rothenburg ob der Tauber) sind lutherisches Kernland und waren Hochburgen der Reformation. Die Zahl der Katholiken und Protestanten hat sich seit 1970 deutlich verringert.

Der bayerische Staat zahlt der römisch-katholischen Kirche jährlich 65 Millionen Euro und der evangelischen Kirche 21 Millionen Euro Staatsdotationen aus dem allgemeinen Haushalt. Die katholischen Pfarreien gehören dem Bistum Würzburg, dem Bistum Eichstätt und dem Erzbistum Bamberg in der Kirchenprovinz Bamberg sowie dem Bistum Regensburg, dem Bistum Passau, dem Bistum Augsburg, und dem Erzbistum München und Freising in der Kirchenprovinz München und Freising an. Weitere katholische Gemeinden unterstehen der Jurisdiktion katholischer Ostkirchen. 

Die Evangelisch-Lutherische Kirche in Bayern ist eine Gliedkirche der Evangelischen Kirche in Deutschland (EKD) und gliedert sich in sechs Kirchenkreise Ansbach-Würzburg, Augsburg, Bayreuth, München, Nürnberg und Regensburg. Zur EKD gehören ebenfalls die zehn bayerischen Gemeinden der Evangelisch-reformierten Kirche. 

Evangelische Freikirchen finden sich vor allem in den Ballungsräumen. Die altkatholische Kirche verfügt über ein bayerisches Dekanat.

Jüdische Gemeinden gab es bis zum 19. Jahrhundert vor allem in ländlichen Gebieten Frankens und Schwabens sowie den freien Reichsstädten wie etwa in Nürnberg (Ansiedlungsverbot 1499 bis 1850) und Regensburg. Im wittelsbachischen Altbayern gab es so gut wie keine Juden, seit der Judenemanzipation aber zunehmend in bayerischen Städten. Von fast 200 jüdischen Gemeinden vor dem Holocaust existieren in Bayern noch oder wieder 13 Gemeinden.

Von wachsender Bedeutung ist insbesondere in Großstädten der Islam. Viele Moscheegemeinden versuchen, ihre bisherigen Hinterhofmoscheen durch repräsentative Neubauten zu ersetzen.
Eine Studie des Bundesministeriums des Innern beziffert den Anteil der Muslime an der bayerischen Gesamtbevölkerung im Jahr 2008 auf ungefähr 4 Prozent (etwa 13 Prozent der in Deutschland wohnhaften Muslime).

Wie in vielen Gegenden in Deutschland wächst der Anteil konfessionell ungebundener Einwohner in Bayern. So gibt es einen Humanistischen Verband Bayern, Landesverband im Humanistischen Verbands Deutschlands. Dieser versteht sich als Weltanschauungsgemeinschaft und Interessenvertretung nichtreligiöser Menschen. Der Verband, der im März 2007 eine Zahl von 1.800 Mitgliedern ausgewiesen hatte, ist in Bayern unter anderem Träger des Humanistischen Lebenskundeunterrichts, er betreibt seit 2008 eine Grundschule in freier Trägerschaft sowie über ein Dutzend Kindertagesstätten. Er unterhält ferner ein eigenes Sozialwerk und den Turm der Sinne in Nürnberg.

Besonders in Ballungszentren gibt es kleinere buddhistische, alevitische, bahaiistische und hinduistische Gemeinden, Königreichssäle der Zeugen Jehovas und Kirchen kleinerer christlicher Kirchen wie den Siebenten-Tags-Adventisten. Zuflucht gefunden etwa in München hat die fast ausgestorbene Religionsgemeinschaft der mandäischen Gemeinde. Aufgrund des Konflikts mit dem deutschen Staat wegen der Schulpflicht gerieten die Zwölf Stämme bei Nördlingen negativ in die Schlagzeilen.

Größte Stadt sowie einzige Millionenstadt und Weltstadt Bayerns ist die Landeshauptstadt München mit rund 1,45 Millionen Einwohnern. Sie ist deutschlandweit die drittgrößte Stadt und die zwölftgrößte Stadt der Europäischen Union. Sie ist die größte Stadt Deutschlands, die kein Stadtstaat ist. Zudem ist München mit rund 4670 Einwohnern je Quadratkilometer (Dezember 2015) die am dichtesten bevölkerte Gemeinde Deutschlands sowie mit dessen höchstgelegene Großstadt. Nürnberg ist mit seinen rund 510.000 Einwohnern die bayernweit zweitgrößte Stadt und belegt deutschlandweit den 14. Platz. Augsburg belegt den deutschlandweit 23. Platz. In Bayern gibt es insgesamt acht Großstädte. Jüngste Großstadt (mindestens 100.000 Einwohner) Bayerns ist Fürth (seit 1990), das zuvor bereits von 1951 bis 1956 und von 1972 bis 1976 eine Großstadt war. Nürnberg bildet zusammen mit Erlangen, Fürth und Schwabach ein ausgeprägtes Städteband. Metropolregionen in Bayern stellen die Metropolregion München, die Metropolregion Nürnberg und das Rhein-Main-Gebiet dar.

In Bayern gibt es über 400 Krankenhäuser, davon fünf Universitätskliniken. Sie verfügen insgesamt über rund 73.000 Betten und rund 4.000 teilstationäre. Etwa 60 Prozent der Krankenhäuser befinden sich in öffentlicher Trägerschaft. In Bayern gibt es zudem über 3200 Apotheken und rund 60.000 Ärzte. Die Arztdichte beträgt 208 Einwohner je Arzt, damit hat Bayern nach dem Saarland einen Spitzenwert unter den Flächenländern. Dennoch wird vor den Folgen eines Ärztemangels in Bayern, vor allem auf dem Land und unter Fachärzten, gewarnt. Die meisten Bewohner Bayerns sind bei der AOK Bayern krankenversichert.

Amts- und Verkehrssprache ist Deutsch. Zahlreiche weitere Sprachen werden von jenen gesprochen, die aus anderen Sprachregionen kommen bzw. den entsprechenden Migrationshintergrund haben.

In Bayern dominieren Dialekte der oberdeutschen Dialektfamilie. Daneben werden räumlich eng begrenzt mitteldeutsche Mundarten gesprochen.

Die Dialekte in Bayern lassen sich folgenden Dialektgruppen zuordnen (von Nord nach Süd):

"Mitteldeutsche Dialekte:"

"Oberdeutsche Dialekte:"

Zwischen diesen Mundarträumen bestehen nicht zu unterschätzende Übergangsgebiete, die sich nicht ohne Bruch einem dieser Gebiete zuordnen lassen. Es existieren bairisch-fränkische (etwa Nürnberg und Umgebung), bairisch-schwäbische (unter anderem Lechrain) und schwäbisch-fränkische (Gebiet um Dinkelsbühl und Hesselberggebiet) Übergangsgebiete, in manchen Orten sogar bairisch-schwäbisch-fränkische Mischdialekte (zum Beispiel Treuchtlingen, Eichstätt).

Die Dialekte sind bei den Einheimischen, besonders außerhalb der großen Städte, sehr verbreitet, wobei in Ballungsgebieten wie München ein Aussterben der Dialekte zu beobachten ist.

Lexikographisch erfasst werden die bairischen Dialekte durch das Bayerische Wörterbuch, die schwäbischen durch das Schwäbische Wörterbuch und die ostfränkischen durch das Fränkische Wörterbuch, wozu noch zahlreiche Wörterbücher über Orts- und Regionalmundarten kommen. Areallinguistisch werden die Dialekte Bayerns vom Sprachatlas von Bayerisch-Schwaben, dem Sprachatlas von Mittelfranken, dem Sprachatlas von Niederbayern, dem Sprachatlas von Nordostbayern, dem Sprachatlas von Oberbayern und dem Sprachatlas von Unterfranken aufgearbeitet.

Zahlreiche weitere deutsche und nichtdeutsche Dialekte werden von jenen gesprochen, die aus anderen Dialekt- oder Sprachregionen kommen.

Grundlage der Politik in Bayern ist die durch Volksabstimmung am 1. Dezember 1946 angenommene Verfassung des Freistaates Bayern. Die Verfassung trat am 2. Dezember 1946 in Kraft. Bayern ist demnach Freistaat (Republik) und Volksstaat (Demokratie). Seit dem 1. Januar 2000 existiert nach der Abschaffung des Senats ein parlamentarisches Einkammersystem.

Die gesetzgebende Gewalt liegt beim Bayerischen Landtag, dessen Abgeordnete alle fünf Jahre (bis 1998: alle vier Jahre) gewählt werden. Bis Ende 1999 existierte mit dem Senat eine zweite Kammer, mit der Vertreter sozialer und wirtschaftlicher Interessenverbände ein politisches Gegengewicht zum Landtag schaffen sollten. In einem Volksentscheid wurde am 8. Februar 1998 die Abschaffung dieser Kammer beschlossen. Bis dahin war Bayern das einzige deutsche Land mit einer zweiten Kammer. Sie hatte jedoch nur bedingten Einfluss, weil sie keine Gesetze einbringen durfte, sondern nur das Recht der Mitwirkung besaß.

Die Staatsregierung wird vom Bayerischen Ministerpräsidenten geführt. Er leitet die Geschäfte, bestimmt die Richtlinien der Politik, vertritt Bayern nach außen und ernennt die Staatsminister und -sekretäre.

Außer vom Landtag können in Bayern Gesetze und Verfassungsänderungen durch Volksbegehren und Volksentscheid beschlossen werden. Zwingend notwendig ist ein Volksentscheid bei jeder Änderung der Bayerischen Verfassung.

Der Bayerische Landtag ist das Landesparlament des Freistaates Bayern. Sie sitzt im Maximilianeum in München. Eine Legislaturperiode dauert fünf Jahre. Im Bayerischen Landtag sind vier Parteien vertreten. Nach der Wahl vom 15. September 2013 ergaben sich folgende Sitzverteilung und Stimmenanteile (insgesamt 180 Sitze):

Die Staatsregierung ist die oberste Behörde des Freistaates Bayern. Der vom Landtag gewählte Ministerpräsident beruft und entlässt die Staatsminister und die Staatssekretäre mit Zustimmung des Landtags. Er weist den Staatsministern einen Geschäftsbereich oder eine Sonderaufgabe zu, den diese nach dem Ressortprinzip und den vom Ministerpräsidenten bestimmten Richtlinien der Politik (Richtlinienkompetenz) eigenverantwortlich verwalten. Die Staatssekretäre sind gegenüber ihren Staatsministern weisungsgebunden. Die Bayerische Staatskanzlei unterstützt den Ministerpräsidenten und die Staatsregierung in ihren verfassungsmäßigen Aufgaben. Die verfassungsrechtliche Höchstgrenze der 18 Mitglieder im Kabinett der Staatsregierung wird meist voll ausgeschöpft.

Das oberste bayerische Gericht ist der Bayerische Verfassungsgerichtshof. Des Weiteren gibt es diverse obere Landesgerichte, darunter drei Oberlandesgerichte in München, Nürnberg und Bamberg, den Bayerischen Verwaltungsgerichtshof, zwei Landesarbeitsgerichte (München und Nürnberg), das Bayerische Landessozialgericht sowie die restliche Judikative. Das Bayerische Oberste Landesgericht als oberstes bayerisches Gericht der ordentlichen Gerichtsbarkeit wurde mit Wirkung zum 1. Juli 2006 aufgelöst.

Im Vergleich zu Wahlen auf Bundesebene weist das bayerische Wahlrecht mehrere Besonderheiten auf: Direktkandidaten, die in ihrem Wahlbezirk (Stimmkreis) die Wahl gewonnen haben, können nur in den Landtag einziehen, wenn auch ihre Partei die Hürde von fünf Prozent erreicht hat.

Darüber hinaus ergibt sich die Sitzverteilung im Landtag aus der Summe der Erst- und Zweitstimmen. In anderen Bundesländern und bei Bundestagswahlen entscheidet die Erststimme über die Wahl des Direktkandidaten im Wahlbezirk und allein die Zweitstimme bestimmt die Zahl der Sitze im Parlament, was üblicherweise dazu führt, dass Erststimmen häufiger den großen Parteien mit aussichtsreichen Direktkandidaten gegeben werden. Wer bei einer bayerischen Landtagswahl eine kleinere Partei mit beiden Stimmen wählt, verschenkt seine Erststimme also nicht, da beide Stimmen dieser Partei zugutekommen, selbst wenn der entsprechende Stimmkreiskandidat den Einzug in den Landtag nicht schaffen sollte. Zudem besteht bei der Zweitstimme die Möglichkeit diese einem bestimmten Kandidaten einer Partei zu geben, sodass sich die Reihung der Bewerber gegenüber den von den Parteien aufgestellten Listen ändern kann.

Eine weitere Besonderheit findet sich im Kommunalwahlrecht. Es besteht die Möglichkeit des Kumulierens („Häufeln“, bis zu drei Stimmen können für einen Kandidaten abgegeben werden) und des Panaschierens (Stimmen können auf Kandidaten verschiedener Listen verteilt werden).

In Bayern gibt es zahlreiche direktdemokratische Elemente. Neben Volksbegehren und Volksentscheid auf Landesebene wurde am 1. Oktober 1995 durch einen Volksentscheid die direkte Demokratie auf Kommunalebene eingeführt. Das bayerische Verfassungsgericht hat die Regelungen 1997 zwar verschärft (unter anderem durch Einführung eines Abstimmungsquorums), dennoch initiieren die Bayern jährlich rund 100 Bürgerentscheide.

Bei den Kommunalwahlen sind die drei größten Kräfte meist CSU, SPD und Freie Wähler. Von den 71 Landräten werden 50 von der CSU, 6 von der SPD, 13 von den Freien Wählern und zwei von den Grünen gestellt; fünf Landräte sind weiblich. Von den Oberbürgermeistern der 25 kreisfreien Städte werden elf von der CSU und elf von der SPD, darunter in Nürnberg (Ulrich Maly) und München (Dieter Reiter) gestellt. Ferner sind je ein CDU-Mitglied, eine Parteilose und ein Kandidat der Wählervereinigung Bayreuther Gemeinschaft ins Amt gewählt worden. Die letzte Bayerische Kommunalwahl fand am 16. März 2014 statt.

Die Parteienlandschaft im Freistaat Bayern weist Merkmale auf, die in der übrigen Bundesrepublik nicht vorhanden sind: die CDU als Volkspartei ist mit keinem eigenen Landesverband vertreten und nimmt nicht an Wahlen teil. Stattdessen überlässt sie der CSU als Schwesterpartei den Vortritt. Ungewöhnlich für ein deutsches Flächenland ist auch, dass die CSU seit 1957 ununterbrochen den Ministerpräsidenten stellt; des Öfteren in Verbindung mit einer absoluten Mehrheit im Landtag.

Die SPD hat in Bayern den geringsten Zulauf aller SPD-Landesverbände. Sie bildet die größte Oppositionspartei mit einem Fünftel bis zu einem Drittel der Stimmen bei Landtagswahlen. Die bayerischen Grünen ziehen mit etwas unter zehn Prozent der Stimmen ins Parlament ein. Im Gegensatz zu anderen Bundesländern und zur Bundesebene ziehen in Bayern die Freien Wähler regelmäßig ins Landesparlament ein, während Die Linke, die Piratenpartei und die FDP politisch kaum eine Rolle spielen und nur selten bzw. noch nie in den Landtag gewählt wurden.

In Bayern sind mehrere Regionalparteien aktiv. Die Bayernpartei z. B. setzt sich für die Möglichkeit einer Volksabstimmung über den Austritt Bayerns aus dem deutschen Staatsverband ein. Während sie in den 1950er und 1960er Jahren in den Landtag und Bundestag einzog und sich von 1962 bis 1966 an der Staatsregierung beteiligte, verlor diese Partei nach ihrem Ausscheiden aus dem Landtag vollkommen an Bedeutung. 2009 gründete sich in Bamberg die Partei für Franken, eine regionale Kleinpartei, die sich unter anderem für eine bessere wirtschaftliche Gleichstellung Frankens innerhalb des Freistaates einsetzt. Außer bei den Kommunalwahlen 2014 konnte diese Partei bisher keine besonderen Wahlerfolge vorweisen.

Am 5. Juni 1954 übernahmen der Freistaat Bayern und die Bayerische Staatsregierung unter Hans Ehard die Patenschaft für die sudetendeutsche Volksgruppe, die nach der Vertreibung der Deutschen aus der Tschechoslowakei in die Bundesrepublik zogen.

Mit folgenden Partnerregionen pflegt der Freistaat Bayern Kontakte:


Die Staatsverschuldung des Freistaates Bayern entwickelte sich wie auf Bundesebene und in anderen Bundesländern langfristig nach oben. Seit 2011 werden jährlich Schulden getilgt. 1992 lagen die Schulden noch bei rund 15 Milliarden, 2010 wurde ein Höchststand von 29 Milliarden erreicht. 2016 waren es 19 Milliarden Euro Schulden. Die Staatsregierung möchte mittelfristig bis 2030 alle Schulden tilgen.

Die bayerische Polizei ist der größte Polizeiverband der Bundesrepublik. Im Jahr 2007 wurden in Bayern 666.807 Straftaten statistisch erfasst. 428.766 Fälle (64,3 Prozent) konnten aufgeklärt, 305.711 Tatverdächtige ermittelt werden. Dies stellt die höchste Aufklärungsquote im Bundesgebiet dar. Die bayerische Polizei unterhält mit der Polizeihubschrauberstaffel Bayern außerdem die größte Polizeihubschrauberstaffel einer Landespolizei.

Eine Bayerische Armee existierte als stehendes Heer von 1682 bis zu seiner Auflösung im Jahre 1919. Über eine gewisse Eigenständigkeit verfügte bis 1924 die Bayerische Reichswehr. In der Gegenwart hat die Bundeswehr rund 60 Standorte in Bayern mit insgesamt 50.700 Dienstposten, im Zuge der Neuausrichtung der Bundeswehr schließen in Bayern drei Standorte und die Anzahl an Dienstposten sinkt auf 31.000. Größte Kaserne Bayerns ist die Hochstaufen-Kaserne in Bad Reichenhall.

In München und Würzburg befinden sich Bundeswehrfachschulen. Die aus der Division Süd entstandene 10. Panzerdivision der Bundeswehr sitzt in Veitshöchheim. In München haben die Sanitätsakademie der Bundeswehr, das Truppendienstgericht Süd und die Universität der Bundeswehr, in Erding das Wehrwissenschaftliches Institut für Werk- und Betriebsstoffe ihren Sitz. Oberste Kommandobehörde Bayerns ist das Landeskommando Bayern. Im Freistaat gibt es über 150.000 verbandlich organisierte Reservisten.

In Bayern gibt es zahlreiche ausländische Militärbasen. Drittgrößter Truppenübungsplatz Europas ist der Truppenübungsplatz Grafenwöhr.

Mit der Verbindung zur Europäischen Politik und Verwaltung ist die Bayerische Staatsministerin für Europaangelegenheiten und regionale Beziehungen Beate Merk betraut. Die Bayerische Staatskanzlei besitzt eine Vertretung bei der Europäischen Union in Brüssel.

Bayern hat viele Beziehungen und Kooperationen zu den europäischen Nachbarstaaten, unter anderem zur Koordinierung der Alpen- und Donaupolitik. Der Freistaat Bayern wird durch Politiker im Ausschuss der Regionen vertreten. Dem Europäischen Parlament gehören zwölf Abgeordnete aus Bayern an: fünf von der CSU, drei von der SPD jeweils einer von Bündnis 90/Die Grünen, Freie Wähler, ÖDP und Linkspartei.

Der Freistaat Bayern erhält zahlreiche Förderungen von der EU, etwa 495 Millionen Euro aus dem Europäischen Fonds für Regionale Entwicklung (EFRE) und knapp 300 Millionen Euro aus dem Europäischen Sozialfonds (ESF).

Im Bundesrat hat Bayern ebenso wie Niedersachsen, Baden-Württemberg und Nordrhein-Westfalen die höchstmögliche Anzahl von sechs Stimmen. Vertreten wird Bayern durch den Ministerpräsidenten Markus Söder sowie die Minister Marcel Huber, Ilse Aigner, Markus Söder, Joachim Herrmann und Emilia Müller. Die Arbeit im Bundesrat wird von der Vertretung des Freistaates Bayern beim Bund koordiniert. Für Bundesangelegenheiten zuständig ist im bayerischen Kabinett Marcel Huber.

Die wirtschaftliche Bedeutung Bayerns für den Außenhandel der Bundesrepublik Deutschland sowie die Bedeutung als wichtiges Zentrum von Industrie, Handel und Logistik, als Sitz von ausländischen Unternehmen und mit einer Bevölkerung, in der zahlreiche verschiedene Staatsangehörigkeiten vertreten sind, hat dazu geführt, dass auf dem Gebiet des Freistaats Bayern 115 Staaten mit ihren Konsulaten (Juni 2015) vertreten sind. Darunter sind 42 berufs- sowie 73 honorarkonsularische Vertretungen. Die meisten Vertretungen befinden sich in München und seinem Umland. Ferner sind ausländische Vertretungen in Nürnberg, Fürth und Regensburg vorhanden. Doyen des bayerischen Corps Consulaire ist der italienische Generalkonsul Filippo Scammacca Del Murgo.

Die höchste Auszeichnung, die der Freistaat Bayern verleiht, ist der 1853 gestiftete Bayerische Maximiliansorden für Wissenschaft und Kunst. Zur Anerkennung für hervorragende Verdienste werden etwa der Bayerische Verdienstorden, die Bayerische Verfassungsmedaille und zahlreiche weitere Orden, Medaillen und Ehrenzeichen verliehen. Auf Vorschlag von Jurys verleiht der Freistaat zahlreiche Auszeichnungen wie etwa der Bayerische Filmpreis oder der Bayerische Printmedienpreis.

Das Bayerische Staatswappen besteht aus sechs heraldischen Komponenten: Der goldene Löwe, ursprünglich mit der wittelsbachischen Pfalz am Rhein verbunden, steht für die Oberpfalz, der „fränkische Rechen“ für die drei fränkischen Bezirke, der blaue Panther für die Altbayern und die drei schwarzen Löwen für Schwaben. Der weiß-blaue Herzschild deutet den Gesamtstaat Bayern an, die Volkskrone bezeichnet nach dem Wegfall der Königskrone die Volkssouveränität. Das Kleine Staatswappen zeigt die bayerischen Rauten und die Volkskrone. Als Wappenzeichen fungieren die bayerischen Rauten und der fränkische Rechen.

Der Freistaat Bayern besitzt zwei gleichgestellte Staatsflaggen, die weiß-blau gerautete Flagge und die Flagge mit horizontalen Streifen in den Farben Weiß und Blau. Die Rautenflagge hat immer vom Betrachter aus gesehen links oben (heraldisch rechte, obere Ecke) eine angeschnittene, weiße Raute (auch im Wappen) und mindestens 21 (angeschnittene) Rauten. Die gleichen weiß-blauen Rauten sind in vielen Städte- und Kreiswappen in den Gebieten der historischen Kurpfalz zu finden, da die Wittelsbacher auch in der Pfalz begütet waren. Ein Flaggenstreit besteht um die "Frankenfahne".

Die Landeshymne ist das "Bayernlied". Der Ursprungstext umfasst drei Strophen und wurde von Michael Öchsner verfasst, die Melodie stammt von Konrad Max Kunz. Dieser historisch-korrekte Text ist auch heute noch die offizielle Version (ersten beiden Strophen), wird in Schulen gelehrt sowie zu offiziellen und staatlichen Veranstaltungen gespielt. Das Bayernlied (Originalfassung) hat seit 1966 Hymnen-Status der Bundesrepublik Deutschland.

Die Bavaria (der latinisierte Ausdruck für Bayern) ist die weibliche Symbolgestalt Bayerns und tritt als personifizierte Allegorie für das Staatsgebilde Bayern in verschiedenen Formen und Ausprägungen auf. Sie stellt damit das säkulare Gegenstück zu Maria als religiöser Patrona Bavariae dar. Der Bayerische Löwe ist Symbolfigur des Freistaats Bayern, unter anderem auf Denkmälern und Auszeichnungen. Die Landesfarben sind weiß-blau.

Das Staatsgebiet des Freistaates Bayern ist für den Bereich der allgemeinen und inneren Verwaltung in Verwaltungssprengel eingeteilt, die Regierungsbezirke (laut Verfassung "Kreise") genannt werden. Die Regierungsbezirke werden durch die Regierungen geleitet, denen je ein Regierungspräsident vorsteht, der vom Innenminister ernannt wird. Die Regierungen sind die Mittelbehörden der allgemeinen und inneren Verwaltung und unterstehen dem Staatsministerium des Inneren. Nachstehend sind die Regierungsbezirke sortiert nach dem Amtlichen Gemeindeschlüssel (AGS) und mit den Abkürzungen des Bayerischen Staatsministeriums des Innern.

Geographisch deckungsgleich mit den Regierungsbezirken sind in Bayern die "Bezirke" gleichen Namens. Anders als die Regierungsbezirke, welche die örtliche Zuständigkeit der Regierungen festlegen, sind die Bezirke kommunale Gebietskörperschaften des öffentlichen Rechts. Der Bezirk ist in Bayern die dritte kommunale Ebene über den Gemeinden (erste Ebene) und Landkreisen (zweite Ebene). Sie sind Selbstverwaltungskörperschaften und haben daher demokratisch gewählte Verwaltungsorgane, den Bezirkstag, der alle fünf Jahre von den Wahlberechtigten des Bezirks direkt gewählt wird und einen Bezirkstagspräsidenten, der aus der Mitte des Bezirkstags gewählt wird. Sie können anders als die Regierungsbezirke Wappen und Flaggen wie eine Gemeinde oder ein Landkreis führen.

In Bayern gibt es 18 Planungsregionen. Sie sind regionale Planungsräume, in denen nach dem Bayerischen Landesentwicklungsprogramm ausgewogene Lebens- und Wirtschaftsbeziehungen erhalten oder entwickelt werden sollen. Hierzu wird für jede Region ein Regionalplan aufgestellt. Die Region Donau-Iller (15) ist die erste länderübergreifende Planungsregion Deutschlands mit einem östlichen Teil in Bayern und einem westlichen Teil in Baden-Württemberg.

Die sieben Regierungsbezirke unterteilen sich in 71 Landkreise und 25 kreisfreie Städte. Die Landkreise und die kreisfreien Städte sind kommunale Gebietskörperschaften mit Selbstverwaltungsrecht. Die Landkreise haben als Verwaltungsorgane den Kreistag und den Landrat. Die kreisfreie Stadt handelt durch den Stadtrat und den Oberbürgermeister. Sowohl der Landrat beziehungsweise der Oberbürgermeister als auch der Kreistag beziehungsweise der Stadtrat werden von den Wahlberechtigten auf die Dauer von sechs Jahren gewählt (süddeutsche Ratsverfassung).

Die Landkreise bilden gleichzeitig Sprengel, welche die örtliche Zuständigkeit der Unterbehörden der allgemeinen und inneren Verwaltung festlegen. Anders als auf der Ebene der Regierungsbezirke hat der Staat hier jedoch keine eigenen inneren Behörden errichtet, sondern bedient sich durch Organleihe des Landrates zur Erfüllung der Aufgaben der staatlichen Verwaltung; der Landrat ist insoweit Kreisverwaltungsbehörde. Bei den kreisfreien Städten ist im Gegensatz dazu eine Vollkommunalisierung gegeben, da ihnen die Aufgaben der unteren staatlichen Verwaltungsbehörde zur selbstständigen Erledigung übertragen werden.

Flächengrößter Landkreis Bayerns ist der Landkreis Ansbach (Mittelfranken). Der Landkreis München (Oberbayern) ist mit rund 330.000 Einwohnern der einwohnerstärkste Landkreis des Freistaates. Schwabach (Mittelfranken) ist Bayerns kleinste kreisfreie Stadt.

Der Freistaat Bayern besteht aus 2056 politisch selbständigen Gemeinden sowie 192 gemeindefreien Gebieten, meist Seen und Forste (Stand: 13. Februar 2013). Von den 2031 kreisangehörigen Gemeinden sind 985 Mitgliedsgemeinden in einer der 312 Verwaltungsgemeinschaften (Stand: 12. Februar 2013).

Die Gemeinden verteilen sich (Stand: 12. Februar 2013) auf 25 kreisfreie Städte und 29 Große Kreisstädte. Zum 1. Januar 2009 gab es 262 sonstige Städte, 386 Märkte und 1355 sonstige Gemeinden. Zusätzlich sind 13 Orte zu leistungsfähigen kreisangehörigen Gemeinden ernannt worden, die bestimmte Aufgaben der Bauaufsicht übernehmen. Während kreisfreie Städte die gleichen Aufgaben wie Landkreise übernehmen, haben Große Kreisstädte nur einige Aufgaben, die über die einer „normalen“ kreisangehörigen Gemeinde hinausgehen. Obwohl das Marktrecht heute keine rechtliche Bedeutung hat, können größere kreisangehörige Gemeinden auf deren Antrag auch heute noch von der Bayerischen Staatsregierung offiziell zum „Markt“ erklärt werden. Der Begriff „Marktgemeinde“ ist in Bayern keine offizielle Bezeichnung für eine Kommune. Es kommt dort aber vor, dass der Begriff „Markt“ offizieller Bestandteil des Gemeindenamens ist, zum Beispiel Markt Berolzheim oder Markt Einersheim.

Flächengrößte und einwohnerstärkste Gemeinde Bayerns ist München, flächenkleinste Gemeinde ist Buckenhof im Landkreis Erlangen-Höchstadt, einwohnerärmste Gemeinde ist Chiemsee im Landkreis Rosenheim. Unterhaching ist die einwohnerstärkste Gemeinde Bayerns ohne eigenes Markt- oder Stadtrecht.

Die einzige Kreisreform in Bayern fand mit Wirkung vom 1. Juli 1972 statt. Alle weiteren Änderungen der Kreisgrenzen sind auf kleinere, nicht grundlegende Reformen zurückzuführen, bei denen es sich hauptsächlich um Gemeindegebietsreformen handelte. Manchmal wurden Gemeinden in bestehende Gemeinden eingemeindet, manchmal wurde auch eine neue Gemeinde gebildet. Von 1971 bis 1980 wurden in Folge von Gemeindefusionen die Effizienz der Gemeinden stark vergrößert. Die Zahl der Gemeinden wurde von etwa 7000 auf rund 2050 reduziert, die Zahl der Landkreise sank von 143 auf 71, die Zahl der kreisfreien Städte sank von 48 auf 25. Zugleich wurde die Verwaltungseinheit der Großen Kreisstadt eingeführt.

Bayern gilt als sehr wirtschaftsstarker und reicher Staat, er hat sich in den letzten Jahrzehnten vom Agrar- zum Technologiestandort entwickelt. Das Bruttoinlandsprodukt betrug 2014 522 Milliarden Euro; das Bruttoinlandsprodukt pro Kopf betrug 2014 39.691 Euro, der Anteil Bayerns an der deutschen Wirtschaftsleistung betrug 18,0 Prozent. Die wirtschaftlich stärkste Region ist der Großraum München mit Automobilindustrie (BMW, Audi, MAN, Knorr-Bremse), IT-Sektor (Siemens, Nokia Networks, Infineon, Microsoft), Medien und Verlagen (ProSiebenSat.1 Media, Vodafone Kabel Deutschland, Hubert Burda Media), Rüstungsindustrie (Airbus, Krauss-Maffei), Touristik (Museen, Oktoberfest, Kongressen, Messen). Weitere bedeutende Wirtschaftsstandorte in Südbayern sind Augsburg (Airbus, Fujitsu Technology Solutions, MAN, KUKA, UPM-Kymmene, Verlagsgruppe Weltbild), Ingolstadt (Audi, MediaMarktSaturn Retail Group) und das Bayerische Chemiedreieck zwischen Chiemsee, Inn und Salzach.

In Nordbayern ist die Metropolregion Nürnberg–Fürth–Erlangen (Siemens, Adidas, Schaeffler-Gruppe, Uvex, Ergo Direkt, Playmobil, Diehl Stiftung) ein wichtiger Standort. Daneben kann der Raum zwischen Aschaffenburg und Würzburg/Schweinfurt sehr gute Wirtschaftsdaten aufweisen, etwa eine Arbeitslosigkeit von durchschnittlich unter sechs Prozent und eine florierende Wirtschaft. Gleiches gilt für Regensburg (Continental Automotive, Maschinenfabrik Reinhausen, BMW, Siemens, Infineon, Osram Opto Semiconductors), das seit Jahren an Wirtschaftskraft zunimmt.

Manche Grenzregionen sind durch Wettbewerbsvorteile in den Nachbarstaaten einesteils und mangelnde Infrastruktur andernteils von Subventionen abhängig. Speziell der Bayerische Wald hatte zu Zeiten des Kalten Krieges durch seine abseitige Lage im Zonenrandgebiet wenig Standortattraktivität besessen. Zwar fiel nach 1990 dort der Eiserne Vorhang zur CSFR, gleichzeitig wurde im wiedervereinigten Deutschland die Zonenrandförderung aufgehoben, und zugleich bot das angrenzende Tschechien – ab 2004 EU-Mitglied – oft bessere Investitionsanreize.

Bayern konnte im Jahr 2006 ein Wirtschaftswachstum von 4,0 Prozent verbuchen. Dieser Wert entsprach 2006 etwa dem Bundesdurchschnitt. Nach zwischenzeitlichem Rückgang des Wachstums 2008 (+0,8 Prozent) und dem Fall in die Rezession im Jahr 2009 (−2,4 Prozent) infolge der weltweiten Finanzkrise erholte sich die Bayerische Wirtschaft wieder und konnte bereits 2010 wieder ein Wirtschaftswachstum von 5,5 Prozent verzeichnen. Im Jahr 2011 erreichte es 6,6 Prozent, 2012 2,6 Prozent, 2013 2,7 Prozent und 2014 stieg das Wachstum auf 3,5 Prozent.

Im Vergleich mit dem Bruttoinlandsprodukt der Europäischen Union, ausgedrückt in Kaufkraftstandards, erreichte Bayern 2014 einen Index von 151 (Oberbayern: 186; EU-28: 100; Deutschland: 131; Hamburg: 215). Die Arbeitslosenquote betrug ; damit hat Bayern die niedrigste Arbeitslosenquote in Deutschland. Die höchste Arbeitslosenquote Bayerns hat Nürnberg.

Bayern musste immer wieder Werkschließungen und die Verlagerung von Arbeitsplätzen hinnehmen. Mitte der 1980er Jahre begann der Niedergang des Büromaschinenherstellers Triumph-Adler; 2003 löste sich die Grundig AG auf. Der ehemals weltgrößte Versandhaus-Konzern Quelle GmbH ging im Juni 2009 in Insolvenz und wurde ab Oktober des Jahres aufgelöst. Wichtige Messen befinden sich in Augsburg, München und Nürnberg.

Der Tourismus gilt aufgrund seines hohen Beitrags zur bayerischen Wirtschaft als „Leitökonomie“. So betrug der Bruttoumsatz der Tourismuswirtschaft 2016 fast 24 Milliarden Euro, die Tagesreisen stellten mit 63 Prozent den größten Anteil davon. Die Beherbergungsindustrie spielt in Bayern mit 13.400 Beherbergungsbetrieben mit mindestens neun Betten und 548.000 Gästebetten eine große Rolle. Das bedeutet, dass sich etwa jeder vierte deutsche Beherbergungsbetrieb in Bayern befindet. Nach Mecklenburg-Vorpommern war Bayern im Jahre 2014 das zweitbeliebteste innerdeutsche Urlaubsziel (gemessen an Reisen ab fünf Tagen Dauer). 2015 hat der Tourismus in Bayern mit 34,2 Mio. Ankünften und 88,1 Mio. Übernachtungen den vierten Ankunfts- und Übernachtungsrekord in Folge erzielt. Bayern war 2016 das beliebteste Reiseziel ausländischer Gäste in Deutschland.

Touristisch sind neben München besonders die Regionen um die bayerischen Seen und in den Alpen, die kulturhistorisch bedeutende Stadt Augsburg mit der Fuggerei, Stadtmauer, Renaissancebauten, sowie Regensburg mit der historischen Altstadt als UNESCO-Welterbe seit 2007 stark. Oberbayern nimmt mit 38,0 Mio. Übernachtungen einen Spitzenplatz unter den Regierungsbezirken ein, die zweitstärkste Destination ist Bayerisch Schwaben mit 15,5 Mio. Die offizielle Marketinggesellschaft der bayerischen Tourismus- und Freizeitwirtschaft ist seit Jahresbeginn 2000 die BAYERN TOURISMUS Marketing GmbH (München). Der Claim lautet: „Bayern – traditionell anders“. 

Der Primärenergieverbrauch im Land ist recht konstant und lag im Jahr 2010 bei 578,2 Mrd. kWh, nach 556,8  Mrd. kWh im Jahr 2009 und 566,6 Mrd. kWh im Jahr 2008. Dies kann einer steigenden Energieproduktivität zugeschrieben werden, also einer verbesserten wirtschaftlichen Produktivität im Verhältnis zur eingesetzten Energie. Diese ist seit 1995, das als Basisjahr angelegt wird, fast durchgehend gestiegen. Die Jahre 2008 und 2009 fallen hinter den Bestwert im Jahr 2007 zurück.

Die größten Energieverbraucher im Jahr 2010 waren die Privathaushalte, die 29 Prozent des Endenergieverbrauches ausmachten. Die Bereiche Industrie und Verkehr benötigten gleichermaßen 28 Prozent der Endenergie und damit nur geringfügig weniger als die Haushalte. Etwas abgeschlagen war der Bereich Gewerbe, Handel, Dienstleistungen mit insgesamt 15 Prozent am Endenergieverbrauch.

In Bayern sind im Jahr 2013 ungefähr 420 Energieversorger angesiedelt, die in einem oder mehreren Bereichen tätig sind: Circa 350 dieser Versorger engagieren sich in der Stromversorgung, ungefähr 100 wirtschaften im Bereich der Wärme- und Kälteversorgung, etwa 140 befassen sich mit der Erdgas-Sparte.

Die Kernenergie bildet mit 48,7 Prozent den größten Anteil an der Nettostromerzeugung. Auf dem zweiten Platz folgen erneuerbare Energien mit 29,2 Prozent. Beide Anteile sind damit im Vergleich zum bundesweiten Durchschnitt (18 bzw. 21 Prozent) überdurchschnittlich groß. Konventionelle Gase tragen mit 15,5 Prozent zur Nettostromerzeugung bei – dieser Anteil liegt nahezu im Bundesdurchschnitt (14 Prozent). Die Stromerzeugung aus Kohlekraftwerken ist relativ unbedeutend, ihr Anteil beträgt 4,1 Prozent (Bundesweiter Durchschnitt von Braun- und Steinkohle insgesamt 42 Prozent). Einen noch geringeren Anteil besitzen mineralische Öle mit 2,6 Prozent, die bundesweit im Schnitt seltener genutzt werden (Heizöl, Pumpspeicher und andere hier insgesamt 5 Prozent) (Stand: jeweils 2011).

An zwei Standorten in Bayern befinden sich Kernkraftwerke (KKW Isar und KKW Gundremmingen), außerdem wird in Garching bei München ein Forschungsreaktor betrieben (FRM II). Eine Studie des Deutschen Instituts für Wirtschaftsforschung (DIW) stellte 2015 fest, dass der Atomausstieg die Versorgungssicherheit in Bayern und Deutschland nicht gefährdet.

Erneuerbare Energien trugen 2014 mit 36,2 % zur Bruttostromerzeugung bei. Der hohe Anteil erneuerbarer Energien an der Nettostromerzeugung fußt vor allem auf dem bedeutenden Anteil der bereits seit Jahrzehnten genutzten Wasserkraft: Ihr Anteil an der Stromerzeugung aus erneuerbaren Energien liegt bei 35,3 Prozent. Zweitwichtigster regenerativer Energielieferant ist mittlerweile die Photovoltaik mit 35,2 Prozent am Gesamtanteil der erneuerbaren Energien, die mit im Rahmen der Energiewende stark ausgebaut wurde. Die Stromerzeugung aus Biomasse hatte einen Anteil von 25,4 Prozent. Weiterhin eher unbedeutend ist die Nutzung der Windenergie – der Beitrag beläuft sich auf 5,6 Prozent der erneuerbaren Stromerzeugung (Stand: jeweils 2014). Im Bundesländervergleich „Erneuerbare Energien“ belegte Bayern im Jahr 2012 den zweiten Platz nach Brandenburg. Bis 2021 sollen erneuerbare Energien dem Bayerischen Energiekonzept (2011) zufolge einen Anteil von 20 Prozent am Endenergieverbrauch und 50 Prozent am Stromverbrauch erreichen.

Nach diesem Energiekonzept sollte die Windenergie bis 2025 ca. 6 bis 10 Prozent des Strombedarfes decken, was etwa dem Neubau von 1.000 bis 1.500 zusätzlichen Windkraftanlagen entspricht. Im Widerspruch zu diesem Ziel wurden 2014 deutlich verschärfte restriktive Abstandsregelungen in Form der 10-H-Regelung eingeführt und ein Jahr später das Windenergieausbauziel auf 5 bis 6 Prozent des Strombedarfes reduziert. Daraufhin brachen die Zahl der Baugenehmigungen von 336 im Jahr 2014 auf 25 im Jahr 2015 ein. Bei einer Gesamthöhe von 200 m stehen für die Windenergienutzung theoretisch nur 0,05 % der Landesfläche zur Verfügung; unter Berücksichtigung, dass nur manche Standorte auch genügend Wind aufweisen nur 0,01 %. Bis Mitte 2016 waren 1.002 Windkraftanlagen mit einer Gesamtleistung von 2.067 MW installiert.

Bayern ist Sitz mehrerer bedeutender Medienunternehmen, insbesondere in der Landeshauptstadt München. Dort, beziehungsweise im Umland von München, befinden sich etwa öffentlich-rechtliche Medien wie der Bayerische Rundfunk und die Programmdirektion des ARD-Gemeinschaftsprogramms Das Erste und des ZDF-Landesstudios Bayern sowie private Fernseh- und Hörfunkanbieter wie ProSiebenSat.1 Media, Sport1 oder Sky Deutschland. Des Weiteren sind in München etwa 250 ansässige Verlage und große Zeitungen wie etwa die Süddeutsche Zeitung (SZ) angesiedelt. Nürnberg bildet einen der größten Verlagsstandorte Deutschland; dort werden beispielsweise das bundesweit erscheinende Sportmagazin Kicker des Nürnberger Olympia-Verlags und die Nürnberger Nachrichten, eine der größten deutschen Regionalzeitungen mit einer Auflage von rund 300.000 Exemplaren, herausgegeben.

Im Jahr 2012 bewarb sich Bayern im Rahmen des Bewerbungsverfahrens für neue generische Top Level Domains (gTLD) um die eigene Top Level Domain ".bayern". Verantwortlich dafür ist die "Bayern Connect GmbH". Am 29. September 2014 erhielt das Unternehmen die gTLD ".bayern" zugesprochen.

Eine Domain darf zwischen 3 und 63 Zeichen beinhalten (ohne Endung). Umlaute sind möglich.

Im internationalen Straßen- und Bahnverkehr sind die Verbindungen von Deutschland nach Österreich und darüber hinaus nach Italien und Südosteuropa von überragender Bedeutung. Beispielhaft sind hier die Verbindungen von Nürnberg über Regensburg und Passau nach Linz, die Verbindungen von Würzburg bzw. Nürnberg und München über Rosenheim nach Salzburg beziehungsweise Innsbruck sowie die Verbindung von München über Lindau nach Bregenz und Zürich zu erwähnen. Hingegen sind die Verkehrsverbindungen ins benachbarte Tschechien bei weitem nicht von vergleichbarer Relevanz, lediglich die Bundesautobahn 6 wurde nach der politischen Wende in der Tschechischen Republik verwirklicht. Insbesondere die Bahnverbindungen in die Tschechische Republik sind bis heute nicht sehr leistungsfähig. Eine Elektrifizierung wurde bisher auf keiner nach Tschechien führenden Verbindung umgesetzt. Vor dem Zweiten Weltkrieg befand sich in der tschechoslowakischen Stadt Cheb "(Eger)" ein Bahnknotenpunkt, der mittels Korridorverbindungen im deutschen Binnenverkehr genutzt wurde. Seit seinem Bestehen unterstand er bis zum Ende des Zweiten Weltkrieges bayerischen beziehungsweise deutschen Bahnverwaltungen. Zu Zeiten des Kalten Krieges besaß die Verbindung von Nürnberg über Cheb nach Prag eine vergleichsweise wichtige Bedeutung. Heute besteht von Nürnberg aus nur noch Umsteigeverbindungen mit Regionalzügen über Cheb und Furth im Wald, während von München aus mit dem Alex eine Direktverbindung über Regensburg (Richtungswechsel), Schwandorf (erneuter Richtungswechsel) und Furth im Wald nach Prag besteht. Für die Zukunft wird im Rahmen des Projektes Donau-Moldau-Bahn eine Elektrifizierung der Verbindung geplant. Für den Fernverkehr von Nürnberg nach Prag bietet die Deutsche Bahn Fernbusse an.

Bayern ist straßenverkehrsmäßig gut erschlossen. Dennoch stammen mit 344 Projekten zu Bundesautobahnen und Bundesstraßen 18,5 Prozent der Anmeldungen zum Bundesverkehrswegeplan 2030 aus Bayern.

Durch Bayern führen unter anderem die Autobahnen A 3, 6, 7, 8, 9 und 70 sowie die seit dem Herbst 2005 fertiggestellte A 71 und die im August 2008 fertiggestellte A 73, die beide Bayern mit Thüringen verbinden. Anbindung an das Bundesland Hessen besteht über die A 3, die A 7 sowie ein kleines Teilstück der A 45. Über die A 72 erhält man Anschluss an den Freistaat Sachsen. Sternförmig von München aus führen die A 95 nach Garmisch-Partenkirchen, die A 96 über Memmingen nach Lindau, die A 93 über Regensburg nach Hof, die A 92 über Landshut nach Deggendorf und die A 94 in Abschnitten nach Passau. Von der A 95 zweigt die A 952 als Verbindung zum Starnberger See ab. Südlich verbindet ein Stück der A 93 die A 8 mit der Brenner Autobahn. Seit den 1970er Jahren geplant, bisher wegen Streitigkeiten um die Trassenführung nur in Abschnitten fertiggestellt ist die A 94 von München über Altötting nach Passau. Daneben führt auch eine große Anzahl an Bundesstraßen durch Bayern. Den Münchner Ring bilden die Autobahnen A 99 mit der Eschenrieder Spange (auch A 99a genannt) und die A 995. Ergänzt werden diese durch Staats-, Kreis- und Gemeindestraßen.

Im Süden wird Bayern überdies gerne als Abkürzung im "innerösterreichischen" Verkehr benutzt, da aufgrund der geographischen Gegebenheiten der Weg durch die Alpen bei weitem länger ist als von Innsbruck über die A 8 oder von Lofer über die B 21 oder B 305 nach Salzburg („Großes“ bzw. „Kleines Deutsches Eck“).

Bayern verfügt über ein dichtes Streckennetz im Eisenbahnverkehr mit zahlreichen Bahnhöfen. Der Münchener Hauptbahnhof – einer der größten in Deutschland – stellt dabei einen wichtigen Knotenpunkt im transeuropäischen Verkehr dar. Die Städte München und Nürnberg verfügen über U- und S-Bahnen mit einem weiten Einzugsgebiet.

Eines der größten europäischen Drehkreuze für den Flugverkehr ist der Flughafen München „Franz Josef Strauß“. Zwei weitere internationale Verkehrsflughäfen befinden sich in Nürnberg und Memmingen, überdies gibt es zahlreiche Verkehrslandeplätze.

Die meisten Binnenschifffahrten finden auf der Donau, dem Main sowie auf dem Main-Donau-Kanal statt. Hierfür gibt es zahlreiche Güterhäfen.

Bayern verfügt über vier Staatstheater. Die Bayerische Staatsoper im Nationaltheater München gilt als das national und international renommierteste Haus in Bayern. Des Weiteren spielen in München das Bayerische Staatsschauspiel, das Residenztheater München, das Bayerische Staatsballett und das Staatstheater am Gärtnerplatz. Als letzte große Neugründung eines Staatstheaters ging 2005 aus den Städtischen Bühnen Nürnberg das Staatstheater Nürnberg hervor.

17 Theater werden von bayerischen Kommunen getragen. Mehrspartenhäuser sind hierbei die Theater in Augsburg, Regensburg, Würzburg und Coburg. Vorwiegend oder ausschließlich Sprechtheater bieten das E.T.A.-Hoffmann-Theater in Bamberg, Das Theater Erlangen, das Stadttheater Fürth und das Stadttheater Ingolstadt sowie in der bayerischen Landeshauptstadt die Münchner Kammerspiele und das Münchner Volkstheater. Die vier Landesbühnen sind in Memmingen, Landshut/Passau/Straubing, Coburg und Dinkelsbühl beheimatet.

Theater für Kinder und Jugendliche bieten neben den Kinder- und Jugendsparten der kommunal geförderten Häuser unter anderem die Schauburg in München oder das Theater Pfütze und das Theater Mummpitz in Nürnberg an. Internationale Bekanntheit erlangte die Augsburger Puppenkiste durch ihre Fernsehproduktionen, das Figurentheater bietet jedoch auch ein Programm für Erwachsene an.

Des Weiteren gibt es eine Vielzahl an freien und privaten Theatern sowie Volks- und Bauerntheatergruppen.

Die von Richard Wagner gegründeten Bayreuther Festspiele sind ein international bedeutendes Festival. Jährlich finden in wechselnden Städten die Bayerischen Theatertage statt.

Bayern wird traditionell als die Heimat der Volksmusik, der Jodler und Schuhplattler angesehen. Aus Bayern stammen bekannte Komponisten wie Max Reger, Carl Orff, Wilfried Hiller, Richard Strauss und Christoph Willibald Gluck. Die Regensburger Domspatzen, die Augsburger Domsingknaben, der Tölzer Knabenchor und der Windsbacher Knabenchor sowie der Münchener Bach-Chor sind weltweit bekannte Chöre. Zu den bekanntesten in Bayern beheimateten klassischen Orchestern zählen die Münchner Philharmoniker, das Symphonieorchester des Bayerischen Rundfunks, die Bamberger Symphoniker, das Bayerische Staatsorchester am Nationaltheater, das Münchener Bach-Orchester, die Bayerische Kammerphilharmonie in Augsburg, die Münchner Symphoniker, das Münchener Kammerorchester, das Georgische Kammerorchester in Ingolstadt, die Nürnberger Philharmoniker am Staatstheater Nürnberg, die Nürnberger Symphoniker und die Hofer Symphoniker.

Unter den Musikfestspielen herausragend sind die Bayreuther Festspiele und die Münchner Opernfestspiele.
Ein weiteres Highlight in der Musikszene sind die Thurn-und-Taxis-Schlossfestspiele, die in Regensburg seit mehreren Jahren unter dem Protektorat von Gloria von Thurn und Taxis gegeben werden. In den letzten Jahren stieg die Zahl der Besucher stetig an. Ebenfalls beachtenswert ist der Münchner Kaiserball sowie der Nürnberger Opernball.

Auch erfreuten sich in den letzten Jahren lokale Bands und Solokünstler größerer Beliebtheit. Die Gruppe LaBrassBanda feiert seit 2008 große Erfolge nicht nur in Bayern, sondern auch auf Bundesebene und in benachbarten deutschsprachigen Ländern.

Bayern ist zusammen mit seiner Landeshauptstadt München, dem Bayerischen Rundfunk sowie der Spitzenorganisation der Filmwirtschaft Gesellschafter der "Internationalen Münchner Filmwochen GmbH", die sowohl das alljährliche Filmfest München als auch das Internationale Festival der Filmhochschulen München organisieren.

Durch das Nebeneinander der zwei bayerischen Stämme Altbayern und Franken, dazu kulturelle Teile von Schwaben, ist die bayerische Küche sehr vielfältig:


Volksfeste und Kirchweihen sind in Bayern weit verbreitet. Anfänglich gedachte man damit der Kirchenweihe. Vielerorts gibt es viele Kirchweih-Traditionen, wie etwa das Aufstellen eines Kirchweihbaumes. In größeren Städten wird meist anstatt einer Kirchweih ein Volksfest begangen. Das größte Volksfest der Welt ist das Münchner Oktoberfest "(Wiesn)" mit 6,3 Millionen Besuchern im Jahr 2014. Weitere große Volksfeste in Bayern sind das Gäubodenvolksfest in Straubing, das Nürnberger Volksfest, die Regensburger Dult, die Fürther Michaeliskirchweih, Fürth und die Erlanger Bergkirchweih.

Bayern hat offiziell kein Landesfest, jedoch hat der Tag der Franken, der seit 2006 in Franken begangen wird, den Charakter eines Landesfesttags. Der Nürnberger Christkindlesmarkt ist ein Weihnachtsmarkt auf dem Hauptmarkt und gehört mit über zwei Millionen Besuchern jährlich zu den größten Weihnachtsmärkten Deutschlands und den bekanntesten weltweit.

Aushängeschild unter den traditionellen Dorfkirchweihfesten ist die "Limmersdorfer Lindenkirchweih" im oberfränkischen Thurnau (Landkreis Kulmbach), in deren Mittelpunkt der Tanz in der Lindenkrone zählt. Sie wird seit mindestens 1729 als fränkische "Plankirchweih" ununterbrochen durchgeführt. 2014 wurde sie deshalb als Sinnbild dörflicher fränkischer Festkultur als einer von 27 Bräuchen in Deutschland in die Nationale Liste des Immateriellen Kulturerbes aufgenommen.

"Siehe:" Schafkopf, Watten

"Siehe:" Bayerische Tracht

Mit der Maxhütte (Sulzbach-Rosenberg) verfügt Bayern über eines der bedeutendsten Industriedenkmale Europas. Die technik- und architekturhistorisch einmalige Anlage wird trotz bestehenden Denkmalschutzes teilweise demontiert. Aktuell wird versucht, das Industriedenkmal vor einem endgültigen Abriss zu bewahren.

Bayern kann auf eine über 1000 Jahre alte Kultur- und Geistesgeschichte zurückblicken.
Laut Artikel 3 der Verfassung des Freistaates Bayern ist Bayern ein Kulturstaat. Der Freistaat Bayern fördert in seinem Haushalt 2003 Kunst und Kultur mit jährlich über 500 Millionen Euro, zusätzlich kommen erhebliche Leistungen der bayerischen Kommunen und privater Träger hinzu.

Die ersten Steinbauten in Bayern entstanden in der Römerzeit. Beispielsweise wurde in Weißenburg eine römische Therme ausgegraben.
Zeugnisse aus dem Frühmittelalter gibt es nur wenige. Ein Beispiel ist jedoch die Krypta des Bamberger Doms aus der Zeit Heinrichs II.
Im Hochmittelalter wurden Würzburg und Regensburg zu wohlhabenden Handelsstädten. In Regensburg entstanden wie in Italien mächtige Geschlechtertürme. Der Regensburger Dom ist ein Hauptwerk der gotischen Architektur in Süddeutschland. Da die Steinerne Brücke lange Zeit die einzige Brücke zwischen Ulm und Wien an der Donau war, führte sie den Handel hierher. Zu den historischen Stadtkernen gehören neben den großen Städten auch Rothenburg ob der Tauber, Dinkelsbühl, Straubing und Nördlingen. Die Stadt Landshut ist für die Martinskirche, den größten Backsteinturm der Welt sowie für die Stadtresidenz mit ihren Renaissancemalereien bekannt. In Augsburg errichteten die Fugger die Fuggerei, die älteste Sozialsiedlung der Welt. Das Rathaus der Stadt gilt als das Paradestück dieser Zeit. Zu den Sehenswürdigkeiten in München gehören das Siegestor und die Antikensammlungen. Unter Ludwig II. entstanden Schloss Neuschwanstein, Schloss Linderhof und Herrenchiemsee. Das Reichsparteitagsgelände aus der Zeit der Nationalsozialisten kann noch heute besichtigt werden.

Zum Welterbe der UNESCO innerhalb Bayerns gehört unter anderem seit 1981 die Würzburger Residenz, ein Schlossbau des süddeutschen Barocks, samt Hofgarten und angrenzendem Residenzplatz. Die bemerkenswert prächtig ausgestattete Wieskirche bei Steingaden wurde 1983 zum Welterbe erklärt. 1993 wurde die Altstadt von Bamberg zum Weltkulturerbe erklärt. Seit dem 15. Juli 2005 gehört der Obergermanisch-Raetische Limes, mit insgesamt 550 Kilometern Länge das längste Bodendenkmal Europas, zum Welterbe. Teil des Welterbes sind mehrere zum Limes gehörende Bauten wie die Thermen in Weißenburg. 2006 wurde die Altstadt von Regensburg mit Altem Rathaus, Dom und Steinerner Brücke aufgenommen, 2012 folgte das Markgräfliche Opernhaus in Bayreuth. Zu den Prähistorische Pfahlbauten um die Alpen (Welterbe seit 2011) gehören mehrere in Bayern gelegene urgeschichtlichen Siedlungen.

Zum Weltdokumentenerbe der UNESCO gehören seit 2003 die Bamberger Apokalypse, einer Handschrift des Klosters Reichenau, sowie seit 2013 das Lorscher Arzneibuch. Beide Schriften werden in der Staatsbibliothek Bamberg aufbewahrt. In der Bayerischen Staatsbibliothek in München liegen das 2003 aufgenommene Perikopenbuch Heinrichs II., das 2009 aufgenommene Hohenems-Münchener Handschrift A, das 2003 aufgenommene Evangeliar aus dem Bamberger Dom, das 2003 aufgenommene Evangeliar Ottos III. und das 2005 aufgenommene Bibliotheca Corviniana. Eine Goldene Bulle (aufgenommen 2013) lagert im Bayerischen Hauptstaatsarchiv, eine weitere im Staatsarchiv Nürnberg.

Bayern ist mit rund 1350 Museen das museumsreichste Bundesland Deutschlands und eine der museumsreichsten Regionen des Kontinents. Zur vielfältigen Museumslandschaft zählen Sammlungen, Schlösser, Gärten und private Sammlungen. Zu den größten und bekanntesten gehören das Germanische Nationalmuseum in Nürnberg und das Bayerische Nationalmuseum in München. Größtes naturwissenschaftlich-technisches Museum der Welt ist das Deutsche Museum. Zu den größten und bedeutendsten ihrer Art gehören das seit 1976 bestehende Fränkische Freilandmuseum in Bad Windsheim und das seit 1990 bestehende Fränkische Freilandmuseum in Fladungen. Einen wesentlichen Teil des Gemälde- und Kunstbesitzes des Freistaates Bayern betreuen die Bayerischen Staatsgemäldesammlungen.

Größte Bibliothek des Freistaates ist die Bayerische Staatsbibliothek in München. Sie ist die zentrale Landesbibliothek Bayerns und eine der bedeutendsten europäischen Forschungs- und Universalbibliotheken mit internationalem Rang. Mit 10,22 Millionen Medieneinheiten ist sie die drittgrößte Bibliothek Deutschlands und besitzt eine der größten Sammlungen im deutschsprachigen Raum. Größte Universitätsbibliothek ist die Münchner Universitätsbibliothek der Ludwig-Maximilians-Universität. Größte öffentliche Bibliothek ist die Münchner Stadtbibliothek. Größte Bibliothek Bayerns außerhalb Münchens ist die Bibliothek der Friedrich-Alexander-Universität Erlangen-Nürnberg. Um die Versorgung mit wissenschaftlicher Literatur in allen Regionen zu gewährleisten, existieren zehn staatliche Regionalbibliotheken, die größte ist die Staatsbibliothek Bamberg.

Das Bayerische Hauptstaatsarchiv in München ist das größte bayerische Staatsarchiv und aufgrund der langen staatlichen Existenz Bayerns auch eines der bedeutendsten Archive in Europa. Daneben gibt es noch zahlreiche andere Staatsarchive.

In Bayern fand schon zweimal die Bundesgartenschau statt: Die Bundesgartenschau 1983, die mit der Internationalen Gartenbauausstellung zusammenfiel, und die Bundesgartenschau 2005, die beide jeweils in München stattfanden.

Bayern konzipierte als eines der ersten Bundesländer eine eigene Landesgartenschau, die erstmals 1980 zusammen mit dem Land Baden-Württemberg in Neu-Ulm/Ulm stattfand.

Seit 2006 erscheinen jährlich 2-Euro-Gedenkmünzen mit einem Motiv des Landes, das den Präsidenten des Bundesrates stellt. 2012 war dies Bayern, somit wurden mit dem Ausgabedatum 3. Februar 2012 rund 30 Millionen 2-Euro-Münzen mit dem Schloss Neuschwanstein als Motiv geprägt, die als offizielles Zahlungsmittel im Umlauf sind und beim Münzen sammeln beliebt sind.

Neben den bundesweit gültigen Feiertagen Neujahr (1. Januar), Karfreitag, Ostersonntag und Ostermontag, Tag der Arbeit (1. Mai), Christi Himmelfahrt, Pfingstsonntag und Pfingstmontag, Tag der Deutschen Einheit (3. Oktober) sowie der 1. und 2. Weihnachtsfeiertag (25./26. Dezember) gibt es im Freistaat Bayern gemäß dem "Gesetz über den Schutz der Sonn- und Feiertage" noch weitere Feiertage. In ganz Bayern als Feiertag gültig sind Heilige Drei Könige (6. Januar), Fronleichnam und Allerheiligen (1. November). Aus Anlass des 500. Reformationsjubiläums im Jahr 2017 ist der Reformationstag am 31. Oktober 2017 ein einmaliger gesetzlicher Feiertag. Mariä Himmelfahrt (15. August) ist nur in Gemeinden mit überwiegend katholischer Bevölkerung ein gesetzlicher Feiertag. Das Augsburger Friedensfest (8. August) ist nur in der Stadt Augsburg ein gesetzlicher Feiertag. Buß- und Bettag war bis 1994 Feiertag. Seitdem ist der Tag zwar Werktag, jedoch haben Schüler unterrichtsfrei.

Mit zwölf landesweit gültigen Feiertagen ist Bayern das Bundesland mit den meisten Feiertagen, inklusive Mariä Himmelfahrt sind es in katholischen Gebieten 13 Feiertage, in Augsburg sind es 14.

Neben den Feiertagen sind stille Tage, an denen besondere Einschränkungen zu beachten sind, festgelegt. Es sind öffentliche Unterhaltungsveranstaltungen verboten, die nicht dem ernsten Charakter dieser Tage entsprechen, beispielsweise herrscht an Karfreitag Tanzverbot. Die stillen Tage in Bayern sind Aschermittwoch, Gründonnerstag, Karfreitag, Karsamstag, Allerheiligen, Volkstrauertag, Totensonntag, Buß- und Bettag sowie der Heilige Abend (24. Dezember).

Es sind etwa 5500 Schulen im Freistaat Bayern vorhanden, die nach dem Bayerischen Gesetz über das Erziehungs- und Unterrichtswesen arbeiten. Es folgt nach der vierjährigen Grundschule das dreigliedrige Schulsystem mit Mittelschule, Realschule und Gymnasium mit Abitur nach der zwölften Klasse. Ab der siebten Klasse gibt es die Möglichkeit, die Wirtschaftsschule, und ab der zehnten, die Berufliche Oberschule (Fachoberschule und Berufsoberschule) mit Erlangung des bayerischen Abiturs nach der 13. Klasse zu besuchen. Hinzu treten sonderpädagogische Förderschulen sowie Schulen für Kranke. Das Schulsystem ist generell durchlässig, und jedem Schüler steht mit jedem erreichten Abschluss der Weg zum nächsthöheren schulischen Abschluss offen.

Als Schulen besonderer Art sind in Bayern fünf Gesamtschulen vorhanden. Ferner gibt es in Bayern zahlreiche Internate, Privatschulen sowie Einrichtungen des zweiten Bildungswegs. Eine Besonderheit im bayerischen Bildungswesen sind Schulvorbereitenden Einrichtungen für Kinder mit sonderpädagogischem Förderbedarf, die es so in der Form in keinem anderen Bundesland gibt. Weitere Besonderheiten des bayerischen Schulsystems sind Jahrgangsstufentests, Absentenheftführer sowie das Elitenetzwerk Bayern zur akademischen Spitzenausbildung.

In den von der OECD durchgeführten PISA-Studien erreichen die Schüler Bayerns regelmäßig Spitzenplätze.

In Bayern existieren neun staatliche Universitäten des Freistaates, sowie die Universität der Bundeswehr München. Bis 1962 existierten lediglich vier Universitäten in München (LMU, TU), Würzburg und Erlangen (ab 1966 Erlangen-Nürnberg). Zwischen 1962 und 1975 wurden in Regensburg, Augsburg, Bamberg, Bayreuth und Passau fünf weitere durch den Freistaat gegründet. Hinzu kam 1973 noch die neu gegründete Bundeswehruniversität.

Daneben gibt es 18 staatliche Fachhochschulen in Bayern, die zwischen 1971 und 1996 gegründet wurden. Darüber hinaus existiert mit der 1980 gegründeten Katholischen Universität Eichstätt-Ingolstadt eine kirchliche Universität, sowie vier weitere private bzw. kirchliche Hochschulen und 10 Kunsthochschulen.

Zu den beliebtesten Sportarten gehört der Fußball. Der Bayerische Fußball-Verband zählt rund 1,5 Millionen Mitglieder und ist damit der mitgliederstärkste Verband des Deutschen Fußball-Bunds. International bekannt ist der FC Bayern München, er ist Rekordmeister der Fußball-Bundesliga, der er seit 1965 angehört, und darüber hinaus mehrfacher Gewinner von internationalen Fußballwettbewerben wie der UEFA Champions League. Ebenfalls in der höchsten deutschen Spielklasse spielt seit 2011 der FC Augsburg. In der zweithöchsten Spielklasse, der 2. Fußball-Bundesliga, spielen vier bayerische Vereine: Der FC Ingolstadt 04, der SSV Jahn Regensburg, der 1. FC Nürnberg und die SpVgg Greuther Fürth. In der Saison 2017/18 spielen zudem die Würzburger Kickers in der 3. Fußball-Liga. Ein weiterer Verein mit langjähriger Erstligazugehörigkeit ist der TSV 1860 München, der jedoch 2017 in die Regionalliga Bayern abstieg.

Im Volleyball sind der mehrmalige deutsche Meister des Frauen-Vereins Roten Raben Vilsbiburg und die Herren-Abteilung von Generali Haching spielen in der obersten Volleyball-Bundesliga aktiv. Die Basketballmannschaften der S.oliver Würzburg, medi Bayreuth und die von München spielen in der höchsten deutschen Spielklasse. Die Brose Baskets aus Bamberg ist die beste bayerische Herrenmannschaft. Neben mehreren Vizemeistertiteln wurden sie 2005, 2007, 2010, 2011, 2012, 2013 und 2015 Deutscher Meister und Pokalsieger 2010, 2011 und 2012. Im Damen-Basketball konnte die in der 1. Bundesliga spielende Mannschaft des TSV Wasserburg in den Jahren 2004 bis 2007 den Deutschen Meistertitel erringen und wurde zudem 2005–2007 Deutscher Pokalsieger.

Derzeit sind der HSC 2000 Coburg und der HC Erlangen die höchstklassig spielenden Handballvereine im Freistaat Bayern. Sie spielen in der Saison 2015/16 in der 2. Handball-Bundesliga.
Die Handballabteilung des TV Großwallstadt aus dem Landkreis Miltenberg spielte lange Jahre in der 1. Handball-Bundesliga. Bekannt, wenn auch ebenso nicht mehr erstklassig, sind die Münchener Vereine TSV Milbertshofen und MTSV Schwabing. Im Damenhandball ist der 1. FC Nürnberg das erfolgreichste bayerische Team. In der jüngeren Vergangenheit wurden sie 2005, 2007 und 2008 Deutscher Meister und stießen 2007/2008 bis in die Hauptrunde der EHF Champions League vor.

Jedes Jahr findet in München ein großes Tennisturnier statt, das von BMW gesponsert wird: Die BMW Open. Sie gelten als Eingangstor für spätere Karrieren. Namhafte Tennisspieler der ganzen Welt reisen dorthin, um wichtige ATP-Punkte mitzunehmen. Weibliche Tennisspieler nehmen an den WTA-Wettkämpfen teil. Bekannte Repräsentanten des bayrischen Tennis sind Philipp Kohlschreiber aus Augsburg und David Prinosil aus Amberg. Bayern steht mit 163 Golfplätzen deutschlandweit an der Spitze, Oberbayern hat gemeinsam mit der Metropolregion Hamburg die größte Golfplatzdichte. Es sind 110.000 aktive Golfer registriert.

Im Baseball haben sich in den vergangenen Jahren immer mehr Mannschaften hervorgebracht, die erfolgreich in der 1. und 2. Baseball-Bundesliga spielen. Dazu zählt unter anderem der Deutsche Meister der Saison 2008, 2010 und 2011 aus Regensburg, die Regensburg Legionäre. Zu den weiteren erfolgreichen bayerischen Teams gehören die Gauting Indians, die Ingolstadt Schanzer und die Haar Disciples. Auf Landesverbandsebene gehören unter anderem die Augsburg Gators, die Erlangen White Sox, die Fürth Pirates, die Deggendorf Dragons und die Garching Atomics zu den bayerischen Vereinen. Mit rund 60 angemeldeten Vereinen ist der BBSV, der Bayerische Baseball und Softball Verband, einer der größten in Deutschland.

Auch im American Football ist Bayern mit einigen Mannschaften in den höchsten Spielklassen vertreten. In der Bundesliga, genannt GFL (German Football League), spielen 2018 die Munich Cowboys, die Ingolstadt Dukes, die Allgäu Comets (aus Kempten) und der Aufsteiger Kirchdorf Wildcats aus Kirchdorf am Inn. In der zweithöchsten Spielklasse, GFL2, sind zwei bayerische Mannschaften vertreten, die Nürnberg Rams und der Aufsteiger Straubing Spiders. Im Damenfootball sind die Munich Cowboys Ladies und die München Rangers Ladies in der Bundesliga vertreten, sowie die Allgäu Comets Ladies, die Nürnberg Rams Ladies, die Erlangen Sharks Ladies und Regensburg Phoenix Ladies in der zweiten Liga.

Korbball wird vor allem in Franken, aber auch im Allgäu gespielt. Schweinfurt gilt seit 1937 als Zentrum des Korbballs. In der Region nehmen mehr als 80 Vereine am Spielbetrieb teil.

Speziell im alpinen Raum hat der Wintersport eine traditionell große Bedeutung. Günstige Bedingungen für Ski-Rennlauf finden sich in den Bayerischen Alpen. Die herausragenden Vertreter dieser Sportart sind Mirl Buchner, Heidi Biebl, Rosi Mittermaier, Marina Kiehl, Christa Kinshofer, Martina Ertl-Renz, Hilde Gerg, Maria Höfl-Riesch, Franz Pfnür und Markus Wasmeier. Aus dem Biathlon-Bundesleistungszentrum in Ruhpolding gingen zahlreiche Gewinner internationaler Wettbewerbe hervor, die bekanntesten unter ihnen sind Fritz Fischer, Michael Greis, Uschi Disl, Martina Glagow und Magdalena Neuner. Im Langlauf erreichten Tobias Angerer und Evi Sachenbacher-Stehle bedeutende Resultate.

In Bayern gibt es fünf Eishockey-Vereine in der Deutschen Eishockey Liga; die Augsburger Panther, der ERC Ingolstadt, die Nürnberg Ice Tigers, der EHC Red Bull München und die Straubing Tigers. In der DEL2 spielen die Starbulls Rosenheim, EHC Bayreuth sowie der ESV Kaufbeuren und der SC Riessersee. Weitere, besonders durch ihre Nachwuchsarbeit bekannte Vereine sind die Tölzer Löwen, der TEV Miesbach sowie der EV Füssen.

Vor allem im Gebiet des Oberallgäuer Orts Oberstdorf und in Garmisch-Partenkirchen finden zahlreiche Sportveranstaltungen statt, etwa der Auftakt der Vierschanzentournee oder der Zugspitz-Extremberglauf. In Garmisch fanden die Olympischen Winterspiele 1936 statt. Auch zahlreiche Welt- und Europameisterschaften, etwa im Bereich Rennrodeln, Eiskunstlauf, Curling oder Skiflug, fanden dort statt.

Nicht zuletzt durch den Dokumentarfilm "Am Limit" wurden im Bereich des Bergklettern die Sportkletterer Thomas und Alexander Huber einem größeren Publikum ein Begriff. Auch früher waren Bayern unter den weltbesten Gipfelstürmern, unter anderen Johann Grill, Josef Enzensperger, Otto Herzog, Anderl Heckmair oder Toni Schmid.
In Bayern haben sich einige Brauchtumssportarten wie Fingerhakeln und Eisstockschießen erhalten, die in organisierten Ligen betrieben werden.
Das Sautrogrennen gehört zu den bayerischen Brauchtumssportarten. Besondere Beliebtheit erfreut sich diese Sportart im Süden Bayerns an den Flüssen Donau, Iller, Isar und Lech. In Franken wird anlässlich von Volksfesten in den meist noch vorhandenen örtlichen Dorf- bzw. Löschwasserteichen diesem Sport gehuldigt. Unter größter Belustigung der Zuschauer werden ernste regionale und überregionale Meisterschaften bestritten. Seit 2010 in Schwarzenbach an der Saale sogar echte Weltmeisterschaften.

Im Bereich des Motorsports gibt es die alljährliche Tourenwagenrennen zur DTM auf dem Norisring in Nürnberg-Dutzendteich. Speedwayrennen gibt es in Landshut, Pocking, Abensberg und Olching, sowie Sandbahnrennen in Mühldorf am Inn, Pfarrkirchen, Vilshofen, Dingolfing und Plattling. In Inzell gibt es internationale Eisspeedwayrennen. Sportschießen wird in den Disziplinen Gewehr, Pistole, Bogen, Wurfscheibe, Laufende Scheibe und Armbrust im gesamten Land ausgeübt. Die Sportschützen stellen mit dem Bayerischen Sportschützenbund (BSSB) den viertgrößten Sportfachverband im Land. Viele bayerische Teilnehmer bei Olympischen Spielen konnten bereits Erfolge erzielen.

Im Tanzsport spielen die Latein- und die Standardformation des RGC Nürnberg tanzen in der zweiten Bundesliga. Im Einzeltanzbereich gibt es sehr viele erfolgreiche Tänzer. Viele Turnvereine haben in Bayern eine lange Tradition. Das Landesleistungszentrum war in Nürnberg, wurde aber mit dem Bundesleistungszentrum in Frankfurt am Main zusammengelegt. Zentren sind Augsburg, Würzburg, Schweinfurt, Nürnberg, Landshut, Passau und Rosenheim.
Bayerische Turnerinnen belegten in den 1920er Jahren oftmals weltweit Spitzenplätze.

Seit 2000 gibt es für Kinder eine „Bayerische Kinderturnolympiade“. Austragungsort war fünf Mal Neumarkt in der Oberpfalz, im Jahre 2000, 2004, 2008, 2012 und 2016. 2012 fand in Neumarkt zudem die 31. Turnerjugend-Festspiele im Rahmen der Kinderturnolympiade statt. Im Jahre 2020 wird die nächste Kinderturnolympiade ausgetragen.




</doc>
<doc id="476" url="https://de.wikipedia.org/wiki?curid=476" title="Bliss">
Bliss

Bliss (engl. für „Glückseligkeit“) steht für:

Musikkultur:

Personen:

Orte in den Vereinigten Staaten:

Gewässer in den Vereinigten Staaten:

im NRHP gelistete Objekte:
BLISS ist die Abkürzung für:
Siehe auch:


</doc>
<doc id="477" url="https://de.wikipedia.org/wiki?curid=477" title="Baden-Württemberg">
Baden-Württemberg

Baden-Württemberg ( "BW") ist eine parlamentarische Republik und ein teilsouveräner Gliedstaat (Land) im Südwesten der Bundesrepublik Deutschland. Es wurde 1952 durch Zusammenschluss der Länder Württemberg-Baden, Baden und Württemberg-Hohenzollern gegründet. Sowohl nach Einwohnerzahl als auch bezüglich der Fläche steht Baden-Württemberg an dritter Stelle der deutschen Länder. Größte Stadt Baden-Württembergs ist die Landeshauptstadt Stuttgart, gefolgt von Karlsruhe und Mannheim. Weitere Großstädte sind Freiburg im Breisgau, Heidelberg, Heilbronn, Pforzheim, Reutlingen und Ulm.

Im Süden grenzt Baden-Württemberg mit dem Klettgau und dem Hotzenwald an den Hochrhein, im Hegau und Linzgau an den Bodensee und im Westen mit dem Breisgau und dem Markgräflerland an den Oberrhein. Im Norden zieht sich die Landesgrenze über Odenwald und Tauberland, im Osten über Frankenhöhe und Ries, entlang von Donau und Iller sowie durch das westliche Allgäu.

Benachbarte deutsche Länder sind im Osten und Nordosten Bayern, im Norden Hessen und im Nordwesten Rheinland-Pfalz. Im Westen grenzt Baden-Württemberg an die französische Region Grand Est. Die Schweizer Grenze im Süden wird von den Kantonen Basel-Stadt, Basel-Landschaft, Aargau, Zürich, Schaffhausen und Thurgau gebildet. Der Kanton St. Gallen ist nur über den Bodensee verbunden. Über den Bodensee ist Baden-Württemberg außerdem mit dem österreichischen Bundesland Vorarlberg verbunden. Mit diesem teilt es – weil dort ebenfalls alemannischer Dialekt gesprochen wird – den manchmal umgangssprachlich verwendeten Beinamen „Ländle“ respektive Alemannisch „Ländli“.

Der geographische Mittelpunkt Baden-Württembergs bei wird von einem Denkmal in einem Waldstück auf der Gemarkung von Tübingen markiert. Es handelt sich dabei um den Schwerpunkt der Landesfläche.
Im Gegensatz dazu wurde die Mitte von Baden-Württemberg aus den Extremwerten (nördlichster, südlichster, östlichster und westlichster Landpunkt) ermittelt. Das Mittel aus der geographischen Breite des nördlichsten und südlichsten Punktes und das Mittel aus der geographischen Länge des östlichsten und westlichsten Punktes im Bezugssystem WGS84 errechnet sich zu . Diese vier Extremkoordinaten Baden-Württembergs sind: im Norden in der Stadt Wertheim, im Süden in der Gemeinde Grenzach-Wyhlen, im Westen in der Gemeinde Efringen-Kirchen und im Osten in der Gemeinde Dischingen. Die Mitte von Baden-Württemberg befindet sich 14,3 km nördlich vom Tübinger Schwerpunkt in Böblingen in einem kleinen Waldstück, dem Hörnleswald, an der Tübinger Straße von Böblingen nach Holzgerlingen und ist mit einem Steinpfeiler markiert.

Die höchste Erhebung des Landes ist der Feldberg im Schwarzwald mit . Der tiefste Punkt liegt im Mannheimer Naturschutzgebiet Ballauf-Wilhelmswörth am Rheinufer und an der Grenze zu Hessen auf .

Innerhalb Baden-Württembergs werden nach geologischen und geomorphologischen Kriterien fünf Großräume unterschieden:


Baden-Württemberg liegt in einem Übergangsgebiet zwischen Seeklima im Westen und Kontinentalklima im Osten. Das bewirkt, dass abwechselnd ozeanische und kontinentale Klimaeinflüsse wirksam werden. Aufgrund der vorherrschenden Westwinde überwiegen die ozeanischen Klimaeinflüsse, wobei diese in den östlichen Landesteilen abnehmen. Die Vielgestaltigkeit der Oberflächenformen, also das Nebeneinander hoher Bergländer und abgeschirmter Beckenräume, führt zu deutlichen klimatischen Unterschieden schon auf kurzen Entfernungen.

Durch die südliche Lage ist Baden-Württemberg gegenüber anderen Ländern hinsichtlich der Temperaturen begünstigt. Das Oberrheinische Tiefland weist Jahresmitteltemperaturen von 10 °C auf und gehört damit zu den wärmsten Gebieten Deutschlands. Klimatisch begünstigt sind auch der Kraichgau, das Neckartal nördlich von Stuttgart, das Bodenseegebiet, das Hochrheingebiet und das Taubertal. Mit der Höhe sinkt die Durchschnittstemperatur, und der Südschwarzwald ist mit durchschnittlich 4 °C eines der kältesten Gebiete Deutschlands. Eine Ausnahme von dieser Regel ist die im Winter vorkommende Inversionswetterlage, bei der höhere Lagen wärmer sind als tiefer gelegene, weil bei windstillem Hochdruckwetter die von den Höhen abfließende Kaltluft sich in Beckenräumen sammelt. Extreme Kältewerte lassen sich deshalb in der Baar beobachten. Hier kann es im Winter zu Temperaturen von unter −30 °C kommen.

Die mit dem Westwind herantransportierten Luftmassen stauen sich vor allem an Schwarzwald und Odenwald, daneben auch an der Schwäbischen Alb, den höheren Lagen der Keuperwaldberge und den Voralpen. Deshalb fällt auf der Luvseite reichlich Niederschlag (über 1000 mm pro Jahr, im Südschwarzwald stellenweise über 2000 mm). Auf der Leeseite im Regenschatten fällt wesentlich weniger Niederschlag. Hier gibt es ausgeprägte Trockengebiete: Im nördlichen Oberrheinischen Tiefland, der Freiburger Bucht (Leeseite der Vogesen) und dem Taubergrund fallen etwa 600 mm, im mittleren Neckarraum und der Donauniederung bei Ulm etwa 700 mm pro Jahr.

Im Auftrag der baden-württembergischen Landesregierung wurden seit Ende der 1990er Jahre mehrere Studien zu den regionalen Folgen der globalen Erwärmung durchgeführt. Laut einer Zusammenfassung dieser Ergebnisse aus dem Jahr 2012 stieg die Jahresdurchschnittstemperatur in Baden-Württemberg im Zeitraum 1906–2005 um 1,0 °C an (weltweit 0,7 °C), von durchschnittlich 8 °C auf 9 °C. Der größte Anstieg erfolgte dabei in den letzten 30 Jahren. Die Anzahl der Höchstniederschläge im Winter und die Zahl der Hochwasserereignisse haben in diesem Zeitraum um 35 % zugenommen, die Anzahl der Tage mit Schneedecke in tiefer gelegenen Regionen haben um 30–40 % abgenommen. Von 1953 bis 2009 nahm die Anzahl der Eistage (Höchsttemperatur unter 0 °C) in Stuttgart von 25 auf 15 ab, die Anzahl der Sommertage (Höchsttemperatur mindestens 25 °C) dagegen erhöhte sich von 25 auf 45 (vgl. auch Hitzewelle 2003). Die Wahrscheinlichkeit einer ausgeprägt trockenen Vegetationsperiode im Sommer hat sich seit 1985 versechsfacht. Klimamodelle prognostizieren eine Weiterführung dieser Trends. In Folge wurde im Juli 2013 ein Klimaschutzgesetz für Baden-Württemberg verabschiedet.

Aufgrund der bergigen Topographie spielten und spielen die Flüsse und ihre Täler eine erhebliche Rolle für Besiedlung, Verkehrswesen und Geschichte des Landes. Die Europäische Hauptwasserscheide zwischen Rhein und Donau hat im Hochschwarzwald ihre westlichste Ausbuchtung und verläuft über die Baar im Norden entlang der Schwäbischen Alb, im Süden durch das Alpenvorland. Das Einzugsgebiet des Rhein-Zuflusses Neckar nimmt mit etwa 14.000 km² fast zwei Fünftel der Landesfläche ein.

Der Rhein ist der wasserreichste Fluss des Landes. Mit ihm ist Baden-Württemberg an eine der bedeutendsten Wasserstraßen der Welt angeschlossen. Sein Einzugsgebiet (ohne Neckar) im Land ist etwa 11.000 km² groß. Im 19. Jahrhundert wurde der Oberrhein ausgehend von den Plänen des badischen Ingenieurs Tulla begradigt. Er bildet mit wenigen Ausnahmen die westliche Landesgrenze. Hochrhein, Seerhein und Bodensee bilden den größten Teil der südlichen Landesgrenze.

Der Neckar entspringt am Ostrand des Schwarzwalds bei Villingen-Schwenningen und durchfließt das Zentrum des Landes, bis er im Nordwesten in Mannheim in den Rhein mündet. Er wird durch zahlreiche Schleusen reguliert und dient als Verkehrsweg für die industriereiche Landesmitte.

Die Donau entsteht bei Donaueschingen aus den vom Schwarzwald kommenden Quellflüssen Brigach und Breg und fließt etwa ostnordöstlich, wobei sie die Schwäbische Alb nach Süden und Oberschwaben nach Norden begrenzt und hinter Ulm nach Bayern fließt. Sie entwässert etwa 9.400 km² und damit mehr als ein Viertel des Landes.

Während der Rhein das Land bei Mannheim auf einer Höhe von etwa verlässt, liegt die Donau an der bayerischen Grenze bei Ulm noch über hoch. Die zum Rhein entwässernden Flüsse haben daher eine größere Erosionskraft und vergrößern ihr Einzugsgebiet langfristig auf Kosten der Donau.

Unter den übrigen Flüssen sind die längsten die Zwillingsflüsse Kocher und Jagst, die den Nordosten des Landes durchfließen und in den Neckar münden. Ganz im Nordosten fließt die Tauber. Hier grenzt das Landesgebiet an den Main.

Mit dem Bodensee hat das Land Anteil am zweitgrößten Alpenrandsee. Über die Bodensee-Wasserversorgung erhalten mehrere Millionen Einwohner vor allem im mittleren Neckarraum ihr Trinkwasser.

Der 2014 gegründete Nationalpark Schwarzwald ist der erste Nationalpark in Baden-Württemberg. Die größten der mehr als 1000 Naturschutzgebiete des Landes sind die eiszeitlich überprägten Gebiete Feldberg und Gletscherkessel Präg im Hochschwarzwald, das Hochmoorgebiet Wurzacher Ried im ebenfalls glazial geprägten Alpenvorland und das Auengebiet Taubergießen am Oberrhein. Etwa 22,8 Prozent der Landesfläche sind als Landschaftsschutzgebiete ausgewiesen. Sieben Naturparke nehmen zusammen ein Drittel der Fläche Baden-Württembergs ein. Die Biosphärengebiete Schwäbische Alb und Schwarzwald sind als Biosphärenreservate der UNESCO anerkannt.

Nach Daten des Statistischen Landesamtes, Stand 2015.

Baden-Württemberg liegt innerhalb des als Blaue Banane bezeichneten, von London nach Norditalien verlaufenden europäischen Agglomerationsbandes. Der gültige Landesentwicklungsplan aus dem Jahr 2002 unterscheidet zwischen den Raumkategorien „Verdichtungsräume“, „Randzonen der Verdichtungsräume“ und „Ländlicher Raum“, wobei letzterer eigene Verdichtungsbereiche enthält. Außer dem größten und zentralen Raum Stuttgart liegen die sieben Verdichtungsräume in grenzüberschreitenden Gunsträumen entlang der Peripherie des Landes. Die meisten sind als Teil Europäischer Metropolregionen ausgewiesen:


Der Oberrheinraum von Karlsruhe über Offenburg und Freiburg bis Lörrach/Weil am Rhein ist Teil der 2010 mit den angrenzenden südpfälzischen, französischen und Schweizer Regionen gebildeten Trinationalen Metropolregion Oberrhein.

Die fünf Verdichtungsbereiche im Ländlichen Raum sind:

Neun Städte im Land haben mehr als 100.000 Einwohner.
Das Gebiet des heutigen Baden-Württemberg war nachweislich bereits vor mindestens einer halben Million Jahren von Vertretern der Gattung "Homo" besiedelt. Der bei Mauer gefundene Unterkiefer von Mauer und der bei Steinheim an der Murr entdeckte "Homo steinheimensis", die heute beide zur Hominini-Art "Homo heidelbergensis" eingeordnet werden, zählen mit einem Alter von rund 500.000 beziehungsweise 250.000 Jahren zu den ältesten Funden der Gattung "Homo" in Europa überhaupt.

Bedeutende paläolithische Nachweise kulturellen Lebens in Baden-Württemberg reichen circa 35.000 bis 40.000 Jahre zurück. So alt sind die Funde der ältesten bekannten Musikinstrumente der Menschheit (eine Elfenbeinflöte, ausgegraben 1979 im Geißenklösterle) und Kunstwerke (Löwenmensch), die in Höhlen der Schwäbischen Alb entdeckt wurden, vor allem in denen des Lonetals. Die wichtigsten dieser Höhlen sind die sogenannten Höhlen der ältesten Eiszeitkunst.

Vor allem aus dem Neolithikum finden sich zahlreiche Belege von Siedlungen und Bestattungen von der frühesten Zeit an, die ab der Bandkeramik auf die unterschiedlichsten Kulturkomplexe zurückgehen und eine ununterbrochene Linie bis zum Beginn der Bronzezeit und bis zur Eisenzeit repräsentieren. Bei Kleinkems in Südbaden befindet sich das älteste deutsche Jaspisbergwerk aus der Jungsteinzeit.

In der Hallstattzeit besiedelten die Kelten große Teile des Landes. Dies ist durch die zahlreichen Hügelgräber belegt, deren bekanntestes das Grab des Keltenfürsten von Hochdorf ist, und durch hallstattzeitliche Siedlungen wie der Heuneburg oder dem Münsterhügel von Breisach.

Seit Caesars Gallischem Krieg 55 v. Chr. bildete der Rhein im Norden die Ostgrenze des römischen Reiches. Um 15 v. Chr. überschritten die Römer unter Tiberius die Alpen. Die neu gegründete Provinz Raetia erstreckte sich bis an die Donau und umfasste damit auch das heutige Oberschwaben.

Der Landweg zwischen Mainz und Augsburg war strategisch sehr wichtig. Um diesen zu verkürzen, bauten die Römer um 73/74 n. Chr. eine Straße durch das Kinzigtal im mittleren Schwarzwald; zum Schutz dieser Straße gründeten sie Rottweil. Weitere Gründungen dieser Zeit sind Ladenburg, Bad Wimpfen, Rottenburg am Neckar, Heidelberg und Baden-Baden; eine Siedlungskontinuität ist jedoch nur für Baden-Baden, Ladenburg und Rottweil wahrscheinlich. Die später gebaute Straße über Bad Cannstatt verkürzte den Weg zwischen Mainz und Augsburg noch weiter. Die Landnahme in Südwestdeutschland sicherten die Römer durch Feldzüge im heutigen Hessen ab. Um 85 n. Chr. gründete Kaiser Domitian die Provinz Germania superior (Obergermanien).

Die Grenze des römischen Reiches verlief von ungefähr 98–159 n. Chr. entlang des Neckar-Odenwald-Limes, später entlang des Obergermanisch-Rätischen Limes. Den vom Limes umschlossenen Teil des Gebietes rechts des Rheines und links der Donau bezeichneten die Römer als Dekumatland. Der nordöstliche Teil des heutigen Baden-Württemberg war nie Teil des römischen Reiches.

Um 233 n. Chr. plünderten Alamannen das Dekumatland; in der Zeit der Reichskrise des 3. Jahrhunderts gaben die Römer um 260 n. Chr. nach erneuten Überfällen die bisherige Grenze auf und zogen sich hinter Rhein, Donau und Iller dem Donau-Iller-Rhein-Limes zurück. Sie hielten die Rheingrenze noch bis zum Rheinübergang von 406.

Im 5. Jahrhundert kam das Gebiet des Herzogtums Alemannien zum Fränkischen Reich. Die Nordgrenze Alemanniens wurde nach Süden verschoben und deckte sich grob mit dem Verlauf der heutigen alemannisch-fränkischen Dialektgrenze. Das nördliche Drittel Baden-Württembergs lag somit im direkten fränkischen Einflussbereich (Bistümer Mainz, Speyer, Worms, Würzburg), die südlichen zwei Drittel verblieben im alemannischen Einflussbereich (Bistümer Konstanz, Augsburg, Straßburg). Im 8. Jahrhundert wurden Grafschaften (Gaue) als Verwaltungseinheiten installiert. Mit der Neubildung der Stammesherzogtümer gehörten die südlichen Gebiete des heutigen Bundeslandes bis zum Ausgang des Hochmittelalters zum Herzogtum Schwaben, die nördlichen Gebiete befanden sich beim Herzogtum Franken.

Im Hochmittelalter gehörte das Gebiet zu den zentralen Landschaften des Heiligen Römischen Reiches Deutscher Nation. Es ist Heimat zahlreicher aufstrebender Adelsdynastien und lag im Schnittpunkt einiger wichtiger Fernhandelsrouten. Der Hochadel und die Klöster lenkten einen intensiven Landesausbau, in dessen Verlauf die Mittelgebirge erschlossen und zahlreiche Städte gegründet wurden, und erweiterten so ihre Machtbasis. Wichtige Familien waren neben den Herzogshäusern vor allem die fränkischen Salier und die schwäbischen Staufer, die sich zu ihrer Zeit den Kaiserthron erkämpften. Weitere wichtige Adelshäuser waren die – ursprünglich aus Oberschwaben stammenden – Welfen, die Zähringer und die Habsburger und auch die unterschwäbischen Hohenzollern.

Nach dem Ende der Stauferdynastie im 13. Jahrhundert kam es zu einer bleibenden Dezentralisierung des Reiches. Die ohnehin traditionell schwache Zentralmacht von Kaisern und Königen verlor zunehmend Rechte und Befugnisse an aufstrebende Regionalmächte. Dieser langfristige Trend wurde auch und gerade in Südwestdeutschland spürbar. Es kam zur territorialen Zersplitterung in Hunderte von kleinen Grafschaften, Reichsstädten, geistlichen Gebieten oder gar einzelnen ritterschaftlichen Dörfern.
Die sich auf dem Gebiet der alten Stammesherzogtümer Franken und Schwaben im Hoch- und Spätmittelalter entwickelnden Territorien erwiesen sich zumeist als beständig und dominierten bis zu den Umbruchsjahren 1803/1806. Zu den bedeutendsten unter ihnen zählen:


Zur horizontalen Diversifizierung trat die vertikale Aufteilung von Rechten an einem Ort in verschiedene Rechteinhaber. So konnten die zahlreichen finanziellen, wirtschaftlichen, militärischen und jurisdiktionalen Rechte innerhalb eines Dorfes in den Händen mehrerer Staaten, Herren oder Familien liegen.

Die frühe Neuzeit war geprägt von der Reformation und den Expansionsbestrebungen der entstehenden Flächenstaaten Österreich, Preußen, Frankreich und Schweden. Aus diesen resultierten Konflikte wie der Bauernkrieg, der Dreißigjährige Krieg und der Pfälzische Erbfolgekrieg. Im heutigen Baden-Württemberg, das territorial außerordentlich stark zersplittert blieb, lag dabei einer der Schwerpunkte der Kampfhandlungen mit entsprechenden Folgen für Bevölkerung und Wirtschaft.

Das spätere Baden war Schauplatz der Bundschuh-Verschwörungen. Der aus Untergrombach stammende Joß Fritz führte von 1501 bis 1517 im Fürstbistum Speyer und in Vorderösterreich insgesamt drei Verschwörungen an.

Bereits 1518 lernten junge südwestdeutsche Gelehrte bei der Heidelberger Disputation Martin Luther und seine Lehren kennen. Der Brettener Philipp Melanchthon folgte Luther nach Wittenberg und wurde zu einem der führenden Köpfe der lutherischen Reformation. Johannes Brenz ging von Heidelberg nach Schwäbisch Hall, führte dort die Reformation ein und unterstützte später Herzog Christoph von Württemberg beim Aufbau der evangelischen Landeskirche.

Der Deutsche Bauernkrieg hatte einen seiner Schwerpunkte im deutschen Südwesten. Bereits 1524 versammelten sich in Stühlingen, Furtwangen und Biberach mehrere tausend Bauern.

Am Ostersonntag 1525 stürmten und besetzten schwäbische Bauern die Burg Weinsberg und töteten den Grafen von Helfenstein, der ein Schwiegersohn Kaiser Maximilians I. war. Diese Weinsberger Bluttat kostete die Bauern viele Sympathien. In der Folge zogen sie unter anderem in Stuttgart ein und zerstörten zahlreiche Burgen und Klöster, darunter die Burg Hohenstaufen, das Kloster Lorch und das Kloster Murrhardt. Am 24. April 1525 übertrugen die Aufständischen dem Hauptmann Götz von Berlichingen die militärische Führung. Am 23. Mai 1525 nahmen südbadische Bauern Freiburg ein.

Der Bauernaufstand wurde durch ein Söldnerheer, das im Auftrag des Schwäbischen Bundes unter der Führung von Georg Truchsess von Waldburg-Zeil kämpfte, noch im Sommer 1525 brutal niedergeschlagen. Man schätzt, dass dabei ca. 100.000 Aufständische zu Tode kamen.

Besonders in den südwestdeutschen Reichsstädten verbreitete sich die Reformation schnell. Der Protestation zu Speyer gehörten 1529 fünf Reichsstädte aus dem heutigen Baden-Württemberg an. Als Markgraf Philipp von Baden 1533 kinderlos starb, wurde die Markgrafschaft unter seinen Brüdern Ernst und Bernhard III. in das protestantische Baden-Durlach und das katholische Baden-Baden aufgeteilt. Herzog Ulrich von Württemberg führte die Reformation ein, als er 1534 durch die siegreiche Schlacht bei Lauffen nach fünfzehnjähriger Habsburgischer Zwangsverwaltung wieder auf den Stuttgarter Thron zurückkehrte.

In der Kurpfalz führte Kurfürst Ottheinrich 1557 die Reformation lutherischer Prägung ein. Unter seinem Nachfolger Friedrich III., der 1563 den Heidelberger Katechismus ausarbeiten ließ, wurde die Kurpfalz calvinistisch.

Hauptschauplätze des Dreißigjährigen Kriegs im deutschen Südwesten waren die Kurpfalz und Vorderösterreich, aber auch die übrigen Gebiete wurden durch Plünderungen und Mundraub der durchziehenden und lagernden Heere schwer getroffen.

Nach der Schlacht am Weißen Berg verlagerte sich der Böhmisch-Pfälzische Krieg in die Kurpfalz. Die vereinigten Heere der Grafen Peter von Mansfeld und Georg Friedrich von Baden-Durlach besiegten Tilly 1622 bei Mingolsheim. Wenig später unterlag der von Mansfeld getrennte Markgraf von Baden Tilly in der Schlacht bei Wimpfen.

Während sich die Kriegsereignisse danach nach Norden verlagerten, blieb die Kurpfalz von Spaniern besetzt. 1632 wurden diese durch die Schweden unter König Gustav Adolf vertrieben. 1634 eroberten die Schweden die Festung Philippsburg und zogen noch im selben Jahr bis an den Hochrhein. 1635 eroberte Johann von Werth Philippsburg und Heidelberg zurück.

1638 feierten die protestantisch-schwedischen Verbände unter Bernhard von Sachsen-Weimar in Vorderösterreich bei der Schlacht bei Rheinfelden, in Breisach und in Freiburg Erfolge. 1643/44 schlug das Schlachtenglück in Schlachten bei Tuttlingen und Freiburg zugunsten der kaiserlich-katholischen Truppen um. Die Kämpfe im Südwesten dauerten noch bis Kriegsende an.

Im Jahre 1647 unterzeichneten Bayern, Schweden und Frankreich in Ulm ein Waffenstillstandsabkommen, in dessen Folge sich die in Bayern eingefallenen schwedischen und französischen Truppen nach Oberschwaben und Württemberg zurückzogen. Im Westfälischen Frieden 1648 erhielt Karl I. Ludwig die Pfalz sowie die 1623 im Regensburger Reichstag verlorene Kurwürde zurück und Breisach wurde französisch.

Als Folge des Dreißigjährigen Kriegs war die Bevölkerung um mehr als die Hälfte, regional um zwei Drittel, zurückgegangen, der Viehbestand war fast völlig vernichtet, ein Drittel des Nutzlandes lag brach. Die Region brauchte lange, um sich davon zu erholen.

→ Hauptartikel für die Zeit von 1693–1733 in Württemberg: Eberhard Ludwig
Nach dem Ende des Holländischen Kriegs 1679 annektierte Frankreich Freiburg im Breisgau. Die vorderösterreichische Regierung verlegte während der französischen Herrschaft über Freiburg ihren Sitz nach Waldshut. 

Im Pfälzischen Erbfolgekrieg verwüsteten französische Truppen unter der Führung von General Melac den nordwestlichen Teil des heutigen Baden-Württembergs. Zwischen 1689 und 1693 ließ Melac flächendeckend nahezu alle Dörfer und Städte niederbrennen, darunter die Residenzstädte Heidelberg mitsamt dem Schloss, Durlach und Baden sowie Mannheim, Bretten, Pforzheim und Marbach. Nach Kriegsende musste Frankreich Freiburg und Breisach am Rhein an Österreich zurückgeben.

In der Folge zogen mehrere der Landes- und Kirchenfürsten aus den alten Residenzstädten aus und errichteten neue Barockresidenzen nach dem Vorbild von Versailles. So entstanden barocke Planstädte mit großen Schlössern in Karlsruhe, Ludwigsburg und Rastatt, die kurpfälzische Residenz Schloss Mannheim und Sommerresidenz Schloss Schwetzingen sowie Schloss Bruchsal als Sitz des Fürstbistums Speyer.

Von 1703 bis 1713 war die Oberrheinebene zwischen Freiburg und Heidelberg im Spanischen Erbfolgekrieg Aufmarschgebiet der kaiserlichen Truppen und mehrfach Schauplatz von Kämpfen zwischen diesen und denen Frankreichs.

Im österreichischen Erbfolgekrieg belagerten und eroberten französische Truppen unter dem persönlichen Kommando Ludwigs XV. 1744 Freiburg.

1782 wurde in den vorderösterreichischen Gebieten, d. h. in großen Teilen des südlichen heutigen Landesteils, die Leibeigenschaft im Zuge der Reformen Kaiser Josephs II. abgeschafft.

Hatten zu Beginn des 19. Jahrhunderts noch etwa 300 Staaten im Gebiet des heutigen Baden-Württembergs territoriale Rechte inne, so reduzierte sich deren Zahl nach der Auflösung des Alten Reiches auf vier. Vor allem das Königreich Württemberg und das Großherzogtum Baden gehörten zu den Gewinnern der Koalitionskriege. Die beiden Fürstentümer Hohenzollern-Sigmaringen und Hohenzollern-Hechingen überlebten aufgrund besonderer Beziehungen zu Napoléon die Mediatisierung. Zudem war die Stadt Wimpfen eine hessische Exklave.

1849 wurde die Badische Revolution durch preußische Interventionstruppen niedergeschlagen, die Badische Armee aufgelöst und unter preußischer Führung neu aufgebaut. 1850 wurden die beiden hohenzollerischen Staaten zur preußischen Provinz "Hohenzollernsche Lande". Im Deutschen Krieg 1866 standen Baden und Württemberg auf der Seite Österreichs und mussten nach Kriegsende eine Entschädigung an das siegreiche Preußen zahlen und militärische Geheimverträge mit dem Norddeutschen Bund schließen. Dies führte 1870 zum Eintritt dieser Staaten in den Deutsch-Französischen Krieg. Infolge des Kriegs schlossen sich Baden und Württemberg dem neu gegründeten und von Preußen angeführten Deutschen Kaiserreich an.

1919 gaben sich die Republik Baden und der Volksstaat Württemberg demokratische Verfassungen.

1933 wurden die selbständigen Landesregierungen im Zuge der Gleichschaltung zu Gunsten nationalsozialistischer Gauleiter und Reichsstatthalter entmachtet. Die Machtergreifung wurde von Terror gegen die politischen Gegner begleitet und unterstützt.

In Baden ernannte sich Gauleiter Robert Wagner am 11. März 1933 selbst zum Staatspräsidenten. Diese Selbsternennung legalisierte Reichspräsident Hindenburg am 5. Mai 1933 nachträglich durch Wagners Ernennung zum Reichsstatthalter. Das Amt des badischen Ministerpräsidenten übernahm Walter Köhler. Der württembergische Landtag wählte am 15. März 1933 Wilhelm Murr mit den Stimmen der NSDAP, DNVP und des Bauernbundes zum Staatspräsidenten. Am 6. Mai 1933 wurde er zum Reichsstatthalter ernannt, während das Amt des Ministerpräsidenten auf Christian Mergenthaler überging. Diese Dualität in der Machtausübung blieb bis Kriegsende erhalten.

Die Regimegegner, vor allem Kommunisten und Sozialdemokraten, wurden ab März 1933 in einer Verhaftungswelle der Gestapo in „Schutzhaft“ genommen und in den Lagern Kislau (bei Bad Schönborn), Ankenbuck (bei Villingen) und Heuberg (bei Stetten am kalten Markt) interniert. Regimekritische Frauen wurden im Frauengefängnis Gotteszell festgehalten. Die badische SPD-Führung wurde am 16. Mai 1933 von Karlsruhe nach Kislau verschleppt, wobei der Abtransport öffentlich inszeniert wurde.

Nach der Umbildung der Landtage gemäß dem Ergebnis der Reichstagswahl vom 5. März 1933 beschlossen die Landtage am 8. Juni 1933 in Württemberg bzw. am 9. Juni 1933 in Baden Landesermächtigungsgesetze. An den Abstimmungen durften sich die Abgeordneten der inzwischen verbotenen KPD nicht mehr beteiligen. Die SPD-Abgeordneten enthielten sich in Württemberg der Stimme, während die fünf verbliebenen in Baden offen mit „Nein“ stimmten. Alle anderen Abgeordneten – in Württemberg waren dies Zentrum, DNVP, Bauernbund, CSVD und NSDAP – stimmten der Selbstentmachtung zu.

Das Lager Heuberg wurde Ende 1933 wegen Überfüllung geschlossen. Die Insassen wurden auf das Fort Oberer Kuhberg in Ulm verlegt. Mitglieder von Gestapo, SS und SA ermordeten den führenden badischen Sozialdemokraten Ludwig Marum am 29. März 1934 in Kislau. 1936 meldete die Gestapo, sie habe die „illegalen“ Strukturen von SPD und KPD zerschlagen.

Dem Massenmord der Nationalsozialisten an der deutschen Zivilbevölkerung fielen in Baden-Württemberg ca. 12.000 Juden, eine große Zahl von Angehörigen der Roma-Minderheit, 10.000 Kranke sowie eine unbekannte Anzahl von Regimegegnern zum Opfer.

Von 1933 bis 1939 waren zwei Drittel der ca. 35.000 Juden, die 1933 in Baden-Württemberg gelebt hatten, ausgewandert. Am 22. Oktober 1940 ließ Robert Wagner ca. 6000 badische Juden in das Lager Gurs verschleppen. Von dort aus wurden die meisten von ihnen in deutsche Vernichtungslager in Osteuropa gebracht und dort ermordet. Die württembergischen Juden wurden ab November 1941 in mehreren Direktzügen zu je ca. 1000 Personen nach Riga, Izbica, Auschwitz und Theresienstadt transportiert, wo sie umgebracht wurden.

In Grafeneck bei Münsingen ermordeten die Machthaber im Rahmen der Aktion T4 mehr als 10.000 Patienten psychiatrischer Kliniken in einer Gaskammer. Roma, und unter ihnen viele Sinti, wurden z. T. in lokalen „Zigeunerlagern“ interniert, zum Beispiel in Ravensburg, und 1940 nach Polen und 1943 in das Vernichtungslager Auschwitz-Birkenau verschleppt. Zahlreiche Insassen baden-württembergischer Konzentrationslager starben bei der Zwangsarbeit. Beispielsweise kostete in Bisingen bei Hechingen der Versuch, Schieferöl zu gewinnen, 1000 Menschen das Leben. Andere Häftlinge kamen auf den sogenannten "Todesmärschen", mit denen die Machthaber kurz vor Kriegsende die Konzentrationslager angesichts der anrückenden amerikanischen Truppen räumen wollten, ums Leben.

Mit dem in Stuttgart aufgewachsenen Graf von Stauffenberg, den Geschwistern Scholl, die ihre Kindheit in Forchtenberg, Ludwigsburg und Ulm verbracht haben, sowie dem Hitler-Attentäter Georg Elser, der auf der Ostalb und in Konstanz lebte, haben vier der bekanntesten deutschen Widerstandskämpfer ihre Wurzeln im Südwesten.

Weitere Beispiele sind die Freiburgerin Gertrud Luckner, die Juden bei der Ausreise unterstützte, 1943 verhaftet wurde und das KZ Ravensbrück überlebte, der Mannheimer Georg Lechleiter, der eine Untergrundorganisation der KPD anführte und 1942 in Stuttgart hingerichtet wurde sowie der Karlsruher Reinhold Frank und die Stuttgarter Fritz Elsas und Eugen Bolz, die als Mitglieder der Verschwörung vom 20. Juli 1944 im Jahre 1945 hingerichtet wurden.

Ebenfalls zum Widerstand rechnet man die Wirtschaftswissenschaftler des Freiburger Kreises um Walter Eucken, den Rottenburger Bischof Joannes Sproll, der 1938 seiner Diözese verwiesen wurde, nachdem er sich an der Volksabstimmung um den „Anschluss“ Österreichs nicht beteiligt hatte, und Robert Bosch, der Juden und andere Verfolgte in seinem Unternehmen unterbrachte.

Im Oktober 1944 wurde die Regierung des Vichy-Regimes unter Marschall Pétain auf Befehl Hitlers von Vichy nach Sigmaringen verlegt. Das Schloss Sigmaringen blieb bis Kriegsende Sitz der aus Sicht der Nationalsozialisten "offiziellen" französischen Regierung.

Die alliierten Luftangriffe im Zweiten Weltkrieg trafen die Städte in Südwestdeutschland nicht alle in gleichem Maße. Beim Luftangriff auf Pforzheim am 23. Februar 1945 starben innerhalb von wenigen Minuten 17.600 Menschen. Sehr schwer getroffen wurden auch Stuttgart, Mannheim, Heilbronn, Friedrichshafen, Freiburg und Ulm. Schwere Schäden trugen Karlsruhe, Reutlingen, Böblingen, Sindelfingen, Offenburg und Göppingen davon. Andere Städte, z. B. Rottweil, Heidelberg, Baden-Baden, Esslingen, Ludwigsburg, Tübingen, Villingen, Konstanz, Aalen oder Schwäbisch Gmünd blieben nahezu unversehrt und haben deshalb noch heute intakte Altstädte.

Im Frühjahr 1945 besiegten amerikanische und französische Bodentruppen auch auf dem Gebiet Baden-Württembergs diejenigen der Wehrmacht. Die Amerikaner besetzten Mannheim am 29. März 1945. Stuttgart eroberten die französischen Truppen am 22. April 1945. Teilweise schwere Kämpfe führten dazu, dass in den letzten Kriegswochen noch Crailsheim, Waldenburg und Freudenstadt zerstört wurden.

Nach dem Zweiten Weltkrieg kamen die nördlichen Teile von Baden und Württemberg zur US-amerikanischen Besatzungszone, die südlichen Teile sowie Hohenzollern zur französischen. Die Aufteilung erfolgte entlang der Kreisgrenzen, wobei zur US-amerikanischen Zone bewusst alle die Kreise geschlagen wurden, durch die die Autobahn Karlsruhe-München (heutige A 8) verlief. Die Militärregierungen der Besatzungszonen gründeten 1945/46 die Länder Württemberg-Baden in der amerikanischen sowie Württemberg-Hohenzollern und Baden in der französischen Zone. Diese Länder wurden am 23. Mai 1949 Teil der Bundesrepublik Deutschland.

Das Grundgesetz für die Bundesrepublik Deutschland traf in Regelungen zu einer Neugliederung des Bundesgebiets mithilfe von Volksabstimmungen. Dieser Artikel trat jedoch wegen Vorbehalten der Besatzungsmächte zunächst nicht in Kraft. Abweichend davon wurden in Artikel 118 die drei Länder im Südwesten dazu angehalten, eine Neugliederung durch gegenseitige Vereinbarung zu regeln. Dieser Artikel beruhte auf der noch vor Beginn der Beratungen über das Grundgesetz getroffenen Entscheidung vom 31. August 1948 bei der Konferenz der Ministerpräsidenten auf Jagdschloss Niederwald zur Schaffung eines Südweststaats. Für den Fall, dass eine solche Regelung nicht zustande käme, wurde eine Regelung durch ein Bundesgesetz vorgeschrieben. Als Alternativen kamen entweder eine Vereinigung zu einem "Südweststaat" oder die separate Wiederherstellung Badens und Württembergs (einschließlich Hohenzollerns) in Frage, wobei die Regierungen Württemberg-Badens und Württemberg-Hohenzollerns für Ersteres, diejenige Badens für Letzteres eintraten. Eine Übereinkunft der Regierungen über eine Volksabstimmung scheiterte an der Frage des Abstimmungsmodus. Das daraufhin am 4. Mai 1951 verabschiedete Bundesgesetz sah eine Einteilung des Abstimmungsgebiets in vier Zonen vor (Nordwürttemberg, Nordbaden, Südwürttemberg-Hohenzollern, Südbaden). Die Vereinigung der Länder sollte als akzeptiert gelten, wenn sich eine Mehrheit im gesamten Abstimmungsgebiet sowie in drei der vier Zonen ergab. Da eine Mehrheit in den beiden württembergischen Zonen sowie in Nordbaden bereits abzusehen war (hierfür wurden Probeabstimmungen durchgeführt), favorisierte diese Regelung die Vereinigungsbefürworter. Die (süd-)badische Regierung strengte eine Verfassungsklage gegen das Gesetz an, die jedoch erfolglos blieb.

Vor der Volksabstimmung, die am 9. Dezember 1951 stattfand, bekämpften sich Befürworter und Gegner des geplanten "Südweststaates". Die führenden Vertreter der Pro-Seite waren der Ministerpräsident Württemberg-Badens Reinhold Maier und der Staatspräsident Württemberg-Hohenzollerns Gebhard Müller, Anführer der Südweststaat-Gegner war der Staatspräsident Badens Leo Wohleb. Bei der Abstimmung votierten die Wähler in beiden Teilen Württembergs mit 93 % für die Fusion, in Nordbaden mit 57 %, während in Südbaden nur 38 % dafür waren. In drei von vier Abstimmungsbezirken gab es eine Mehrheit für die Bildung des Südweststaates, so dass die Bildung eines Südweststaates beschlossen war. Hätte das Ergebnis in Gesamtbaden gezählt, so hätte sich eine Mehrheit von 52 % für eine Wiederherstellung des (separaten) Landes Baden ergeben.


Am 9. März 1952 wurde die "Verfassunggebende Landesversammlung" gewählt. Auf einer Sitzung am 25. April 1952 wurde der erste Ministerpräsident gewählt. Damit war das Land Baden-Württemberg gegründet.
"Reinhold Maier" (FDP/DVP) bildete als erster Ministerpräsident eine Koalition aus SPD, FDP/DVP und BHE. Nach Inkrafttreten der Verfassung wirkte die "Verfassunggebende Landesversammlung" bis 1956 als erster Landtag von Baden-Württemberg.

Der Name des Landes war Gegenstand eines längeren Streites. Der im "Überleitungsgesetz" vom 15. Mai 1952 genannte Name "Baden-Württemberg" war zunächst nur übergangsweise vorgesehen, setzte sich jedoch letztlich durch, da kein anderer Name von allen Seiten akzeptiert wurde. Die am 19. November 1953 in Kraft getretene Landesverfassung wurde lediglich von der Verfassunggebenden Landesversammlung beschlossen, anschließend aber nicht durch eine Volksabstimmung bestätigt.

Reinhold Maier hatte mit seiner schnellen Regierungsbildung 1952 die CDU als stärkste Fraktion ausgeschlossen. Das erzeugte Unmut, sowohl bei den zwei südlichen Landesteilen Südbaden und Südwürttemberg-Hohenzollern, die sich in der neuen Regierung nicht oder nur unzureichend vertreten fühlten, als auch bei "Gebhard Müller", dem neuen CDU-Fraktionsvorsitzenden, der die Nichtbeteiligung der CDU als persönlichen Affront empfand. Bei der Bundestagswahl vom 6. September 1953, die von Reinhold Maier zugleich zum Plebiszit über seine Politik erklärt worden war, errang die CDU in Baden-Württemberg die absolute Mehrheit der Stimmen. Reinhold Maier zog die Konsequenzen und trat als Ministerpräsident zurück. Sein Nachfolger wurde Gebhard Müller, der eine Allparteienregierung aus CDU, SPD, FDP/DVP und BHE bildete, die bis 1958 Bestand hatte. Nachfolger Müllers wurde Kurt Georg Kiesinger als dritter Ministerpräsident des Landes.

Die badischen Vereinigungsgegner gaben den Kampf gegen den Südweststaat auch nach 1952 nicht auf. Im "Heimatbund Badnerland" organisiert, erstrebten sie weiterhin die Wiederherstellung Badens. Abs. 2 GG sah vor, dass in Gebieten, deren Landeszugehörigkeit nach Ende des Zweiten Weltkriegs ohne Volksabstimmung geändert worden war, ein Volksbegehren über die Neugliederung möglich sei. Nachdem dieser Passus infolge des Deutschlandvertrags 1955 in Kraft trat, stellte der Heimatbund einen Antrag auf ein Volksbegehren zur Wiederherstellung des Landes Baden in den Grenzen von 1945. Das Bundesinnenministerium lehnte diese Forderung unter anderem mit der Begründung ab, das neue Bundesland sei bereits durch eine Volksabstimmung zustande gekommen. In der darauf folgenden Klage vor dem Bundesverfassungsgericht bekam der Heimatbund 1956 Recht. Das Gericht argumentierte, dass die Abstimmung von 1951 keine Abstimmung in Sinne von GG gewesen sei, da hierbei die zahlenmäßig stärkere Bevölkerung Württembergs und Hohenzollerns die zahlenmäßig schwächere Badens habe überstimmen können. Der Wille der badischen Bevölkerung sei durch die Besonderheit der politisch-geschichtlichen Entwicklung überspielt worden, weshalb ein Volksbegehren nach GG zulässig sei.

Das Bundesverfassungsgericht setzte in seinem Urteil keine Frist für die Abstimmung, weshalb sie immer wieder verschleppt wurde. Es bedurfte einer weiteren Entscheidung des Bundesverfassungsgerichtes im Jahre 1969, in der es die Abstimmung bis spätestens zum 30. Juni 1970 anordnete. Diese wurde am 7. Juni 1970 durchgeführt und ergab mit 81,9 % eine große Zustimmung zum Verbleib von Baden im gemeinsamen Land Baden-Württemberg. Die Wahlbeteiligung lag bei 62,5 %.

Die Ablehnung des Volksbegehrens machte den Weg frei zu einer administrativen Neugliederung des Landes. 1971 wurde eine Reform der Landkreise und der Regierungsbezirke eingeleitet, die 1973 in Kraft trat. Seitdem sind die ehemaligen Landesgrenzen kaum noch im Kartenbild zu erkennen.

Die Bevölkerungsentwicklung in Baden-Württemberg war zwischen 1950 und 2008 im Allgemeinen von einem stetigen Anstieg geprägt. In den Fünfzigerjahren stieg die Bevölkerung Baden-Württembergs um knapp 1,3 Millionen Menschen an. Auch in den Sechzigerjahren stieg die Bevölkerung nochmals um knapp 1,2 Millionen Menschen an. 1971 überstieg die Bevölkerungszahl erstmals die Neun-Millionen-Marke. Die Siebzigerjahre waren dagegen bevölkerungsmäßig weitgehend von Stagnation geprägt.

Vor allem in den zehn Jahren von 1977 bis 1987 trat die Bevölkerungsentwicklung weitgehend auf der Stelle. Ein Rückgang Anfang der Achtzigerjahre wurde zwar ausgeglichen, in den zehn Jahren nach 1977 nahm die Bevölkerung jedoch nur um rund 165.000 Menschen auf knapp 9,3 Millionen zu. Mit dem Ende des Kalten Krieges und dem Zustrom von Menschen aus Zentral- und Osteuropa änderte sich dies jedoch sehr deutlich.

Die zwanzig Jahre von 1988 bis 2008 waren von einem kontinuierlichen Bevölkerungsanstieg geprägt. Die Bevölkerung nahm in dieser Zeit um fast 1,5 Millionen Menschen zu. Am 31. März 2011 wurde der bisher höchste Stand von offiziell 10.754.995 Menschen registriert. In den Jahren 1990 und 1991 wuchs die Bevölkerung jeweils um fast 200.000 Personen.

Insgesamt ist in den 50 Jahren zwischen 1952 und 2002 die Bevölkerung Baden-Württembergs um knapp vier Millionen von 6,7 auf 10,7 Millionen Menschen gewachsen, das ist eine Zunahme um knapp 60 Prozent. In den Jahren 2008 und 2009 gab es einen kleinen Bevölkerungsrückgang im ansonsten von Wachstum geprägten Baden-Württemberg. Auch bisher ist die Bevölkerung stets höchstens drei Jahre in Folge geschrumpft, um dann wieder und weiter anzuwachsen. Dennoch prognostiziert das Statistische Landesamt einen Rückgang der Bevölkerung bis zum Jahr 2030 um 3,5 Prozent auf rund 10,3 Millionen Menschen.

Die Studie „Wegweiser Kommune“ der Bertelsmann-Stiftung geht in einer Prognose aus dem Jahr 2011 von einem Bevölkerungsrückgang von 0,4 Prozent für Baden-Württemberg bis 2030 (gegenüber 2009) aus, womit Baden-Württemberg nach Bayern das Flächenland mit der stabilsten Bevölkerungsgröße ist.

Das Wappen zeigt drei schreitende Löwen auf goldenem Grund. Dies ist das Wappen der Staufer und Herzöge von Schwaben. Über dem großen Landeswappen befinden sich die sechs Wappen der historischen Landschaften, aus denen oder aus deren Teilen Baden-Württemberg gebildet worden ist. Es sind dies: Vorderösterreich (rot-weiß-rot geteilter Schild), Kurpfalz (steigender Löwe), Württemberg (drei Hirschstangen), Baden (roter Schrägbalken), Hohenzollern (weiß-schwarz geviert) und Franken (drei silberne Spitzen auf rotem Grund). Dabei sind die Wappen Badens und Württembergs etwas größer dargestellt. Schildhalter sind der badische Greif und der württembergische Hirsch. Auf dem kleinen Landeswappen ruht stattdessen eine Blattkrone.

Die Benutzung des Landeswappens ist genehmigungspflichtig und grundsätzlich nur den Behörden gestattet.

Die Landesflagge ist schwarz-gold; die Landesdienstflagge trägt zusätzlich das kleine Landeswappen.

Baden-Württemberg ist seit dem 1. Januar 1973 in vier Regierungsbezirke, zwölf Regionen (mit je einem Regionalverband) sowie 35 Landkreise und neun Stadtkreise eingeteilt.


Die Region Donau-Iller umfasst auch angrenzende Gebiete in Bayern. Die Region Rhein-Neckar umfasst auch angrenzende Gebiete in Hessen und Rheinland-Pfalz.

Im Land bestehen die folgenden neun Stadtkreise (in Klammern die jeweiligen Kfz-Kennzeichen):

Die 35 Landkreise sind:
Zum Landkreis Konstanz gehört die Exklave Büsingen am Hochrhein, die in der Nähe von Schaffhausen liegt und völlig von Schweizer Gebiet umschlossen ist.

Die Landkreise haben sich 1956 zum Landkreistag Baden-Württemberg zusammengeschlossen.

"Siehe auch: Liste der Städte und Gemeinden in Baden-Württemberg, Liste der größten Städte Baden-Württembergs (alle Gemeinden mit mehr als 20.000 Einwohnern) sowie Gemeindeordnungen in Deutschland"

Das Land Baden-Württemberg gliedert sich in insgesamt 1101 Gemeinden (Stand: 1. Januar 2009) sowie die zwei unbewohnten gemeindefreien Gebiete Gutsbezirk Münsingen und Gemeindefreier Grundbesitz Rheinau.

Die Rechte und Pflichten der Gemeinden werden vor allem in der baden-württembergischen Landesverfassung (§§ 69–76) und in der baden-württembergischen Gemeindeordnung (GemO) festgelegt. In § 1 GemO sind die Gemeinden als „Grundlage und Glied des demokratischen Staates“ beschrieben, und die „Teilnahme an der […] Verwaltung der Gemeinde“ als „Recht und Pflicht“ der Gemeindebewohner.

Als ein Gemeindegebiet wird in § 7 GemO die Gesamtheit der zur Gemeinde gehörenden "Grundstücke" definiert. Diese Grundstückseinheit ist als Gemarkung im Grundbuch dokumentiert. Ferner ist festgelegt, dass alle Grundstücke Baden-Württembergs zu einer Gemeinde gehören sollen – „besondere Gründe“ rechtfertigen aber den Verbleib von Grundstücken außerhalb eines gemeindlichen Markungsverbandes. Solche „gemeindefreien Grundstücke“ existieren in Baden-Württemberg in zwei unbewohnten gemeindefreien Gebieten – Gutsbezirk Münsingen und Gemeindefreier Grundbesitz Rheinau.

Hinter den neun Großstädten des Landes sind die größten Mittelstädte Ludwigsburg, Esslingen, Tübingen, Villingen-Schwenningen und Konstanz.

In § 3 GemO sind als besondere Gemeindetypen Stadtkreise (außerhalb Baden-Württembergs "Kreisfreie Stadt" genannt) und Große Kreisstädte erwähnt. Sie unterscheiden sich von den verbleibenden Gemeinden durch die ganze oder teilweise Übernahme von Kreisaufgaben. In Baden-Württemberg sind neun Gemeinden zu Stadtkreisen und 91 Gemeinden zu Großen Kreisstädten erklärt worden.

Von den in § 8 GemO genannten Gemeindegebietsänderungen haben "Eingliederung" (Eingemeindung) und "Neubildung" (Gemeindefusion/Zusammenlegung) das Ende der politischen Selbständigkeit einer Gemeinde zur Folge. Umfangreiche derartige Gebietsänderungen wurden unter dem Stichwort "Gebietsreform" in den 1970er Jahren verfügt. Die Eingliederung von Tennenbronn nach Schramberg am 1. Mai 2006 war die erste Aufgabe der Selbstständigkeit einer Gemeinde seit 1977.

Die alle fünf Jahre stattfindenden Kommunalwahlen wurden zuletzt am 25. Mai 2014 durchgeführt. Bei den vorherigen Wahlen im Jahr 2009 waren 18.233 Gemeinderäte und 1.960 Kreisräte zu wählen.

Der Ministerpräsident ist Vorsitzender der Landesregierung von Baden-Württemberg, die aus Ministern und Staatssekretären und ehrenamtlichen Staatsräten besteht. Die Ministerpräsidenten seit 1952:

Baden-Württemberg ist politisch bürgerlich-konservativ geprägt, die CDU und die FDP/DVP sind in Baden-Württemberg verhältnismäßig stark und haben die meisten Regierungen des Landes gestellt. Aus diesem Grund hatte die SPD dort stets einen schweren Stand; ihre Ergebnisse lagen bislang immer unter dem Bundesdurchschnitt. Die CDU ging bis 2011 bei jeder Wahl als stärkste Partei hervor, während das Bundesland für die FDP das bislang einzige darstellt, bei dem sie bei Landtagswahlen noch nie an der Fünf-Prozent-Hürde scheiterte. Seit den 1980er Jahren ist Baden-Württemberg auch eine Hochburg der in Karlsruhe gegründeten Grünen, deren Wahlergebnisse im Land stets über dem Bundesdurchschnitt lagen; ihr erstmaliger Einzug in den Landtag im Jahr 1980 war gleichzeitig der erste in einem Flächenland; seit dem Erfolg bei den Landtagswahlen 2011 stellen die Grünen hier ihren ersten Ministerpräsidenten überhaupt. Während der Ministerpräsident von 1953 bis 2011 immer von der CDU gestellt wurde, waren an der Regierung teilweise die FDP/DVP beziehungsweise die SPD (Große Koalition) beteiligt. Während der 1990er Jahre waren die Republikaner im Landtag vertreten (10,9 % 1992 und 9,1 % 1996), die in diesem Bundesland ihren größten Zulauf hatten. Zuvor saß zwischen 1968 und 1972 ebenso die NPD mit 9,8 % der Wählerstimmen im Landtag. 2016 zog die AfD mit 15,1 % in den Landtag ein. In keinem anderen der alten (westdeutschen) Länder erreichten Parteien rechts von CDU und CSU derart hohe Wahlergebnisse.

Die CDU erreichte bei allen Wahlen zwischen 1972 und 1988 die absolute Mehrheit im Landtag. Aufgrund des Austrittes des Landtagsabgeordneten Ulrich Maurer aus der SPD am 27. Juni 2005 und seinem Eintritt in die WASG am 1. Juli war diese im Landtag vertreten. Stefan Mappus wurde am 10. Februar 2010 zum Ministerpräsidenten gewählt, verlor allerdings seine schwarz-gelbe Regierungsmehrheit nach der Landtagswahl 2011. Die CDU selbst fuhr mit 39,0 % das zweitschlechteste Wahlergebnis in der Geschichte der Landespartei ein, die FDP schaffte nur knapp den Sprung in den Landtag (5,3 %). Die Grünen dagegen erreichten mit 24,2 % das zu dem Zeitpunkt beste Ergebnis der Partei auf Landesebene. Die SPD erreichte mit 23,1 % ihr in Baden-Württemberg bis dahin schlechtestes Wahlergebnis und trat als Juniorpartner in eine grün-rote Koalition ein. Bei der Landtagswahl 2016 setzte sich der Trend fort: Sowohl CDU als auch SPD verschlechterten sich nochmals auf ihre bis dahin jeweils schlechtesten Ergebnisse im Land, wohingegen die Grünen weiter zugewinnen konnten. Die Neuangetretene AfD konnte 15,1 % der Stimmen erzielen. In der Folge bildeten die Grünen zusammen mit der CDU eine Koalition unter Ministerpräsident Kretschmann.
Das Land unterhält zwei Landesvertretungen außerhalb von Baden-Württemberg. Seit 1954 existiert die Vertretung des Landes Baden-Württemberg beim Bund, welche ihren Sitz bis zum Umzug der Bundesregierung in der Bundesstadt Bonn hatte und heute in der Bundeshauptstadt Berlin sitzt. Im Jahre 1987 kam die Vertretung des Landes Baden-Württemberg bei der Europäischen Union dazu, welche als Bindeglied zwischen dem Bundesland Baden-Württemberg und der Europäischen Union fungiert.

Baden-Württemberg und die japanische Präfektur Kanagawa pflegen seit 1989 eine bilaterale Partnerschaft. Innerhalb Europas bildet Baden-Württemberg zusammen mit den Regionen Katalonien, Lombardei und Rhône-Alpes die multilaterale Arbeitsgemeinschaft "Vier Motoren für Europa".

Mit service-bw steht den Bürgern eine E-Government-Plattform zur Verfügung.
Im landeseigenen Umweltinformationssystem Baden-Württemberg sind aktuelle Messergebnisse zur Luftqualität, zum Bodensee, Unwetterwarnungen, Geoinformationen, und ein Informationssystem für Wasser, Immissionsschutz, Boden, Abfall und Arbeitsschutz abrufbar.

Baden-Württemberg zählt zu den wirtschaftsstärksten und wettbewerbsfähigsten Regionen Europas. Insbesondere im Bereich der industriellen Hochtechnologie sowie Forschung und Entwicklung gilt Baden-Württemberg als die innovativste Region der Europäischen Union. Die Forschungsstärke spiegelt sich in den Ausgaben für Forschung und Entwicklung wider, welche 2005 bei 4,2 % des Bruttoinlandsprodukt liegen, der höchste Wert unter den EU-Regionen (NUTS 1).

Gemessen am Bruttoinlandsprodukt, das 2016 rund 476,76 Milliarden Euro betrug, gehört Baden-Württemberg zu den wohlhabenderen Regionen der EU mit einem Index von 144 (EU-28: 100, Deutschland: 126) (2014). Nach Hamburg und Bayern belegt Baden-Württemberg den dritten Platz im Kaufkraftvergleich 2016 mit 23.368 Euro pro Einwohner. Die Arbeitslosenquote betrug . Sie ist dabei in den eher ländlich geprägten Regionen traditionell niedriger als in den Städten. So betrug die Quote im Juli 2014 im Landkreis Biberach lediglich 2,5 %, im Bodenseekreis 2,6 % sowie im Alb-Donau-Kreis 2,7 %, während sie in den Stadtkreisen Mannheim mit 6,1 %, Heilbronn mit 6,2 % und insbesondere Pforzheim mit 7,6 % deutlich höher lag. Ungefähr 50.000 Baden-Württemberger gehen als Grenzgänger einer Arbeit in der Schweiz nach.
Seit 1999 wirbt die Landesregierung mit dem Motto „Wir können alles. Außer Hochdeutsch.“ für Baden-Württemberg als Wirtschaftsstandort und Lebensumfeld. Ziel der von der Landesregierung als äußerst erfolgreich eingeschätzten Kampagne ist es, die wirtschaftliche Leistungsfähigkeit des Landes bekannter zu machen und sie mit den kulturellen, landschaftlichen und gastronomischen Vorzügen zu assoziieren. Das Motto wurde von der Werbeagentur Scholz & Friends erfunden und zunächst dem Freistaat Sachsen angeboten, der seine Nutzung jedoch ablehnte.

Das Land weist für die Landwirtschaft höchst unterschiedliche natürliche Bedingungen auf (vgl. Abschnitt Geographie). In der Bilanz sind die tiefer gelegenen Tal- und Beckenräume des Landes wie Oberrhein-Tiefland und Neckartal oder auch das Bodenseegebiet ausgesprochene Gunsträume für die Landwirtschaft. Hier finden sich neben Ackerbau auch Intensivkulturen wie z. B. Obst- und Weinbau mit den Weinbaugebieten Baden und Württemberg. Der überwiegende Teil des Landes weist mittlere Höhenlagen auf, die für den Getreidebau günstig sind, der in unterschiedlichen Kombinationen mit Grünlandwirtschaft und Futterbau auftritt. Ungünstige Wuchsklimate finden sich in den Höhengebieten des Schwarzwalds und der Schwäbischen Alb sowie in der Baar, hier herrschen Futterbau und Viehhaltung auf Grünland oder Forstwirtschaft vor.

Der allgemeine Strukturwandel der Landwirtschaft, ihre betriebliche Konzentration und die Intensivierung der Produktion, vollzieht sich in Baden-Württemberg aufgrund seiner kleinteiligeren Landwirtschaft mit einiger Verzögerung letztlich in gleicher Geschwindigkeit. Indikatoren sind z. B.

Industrie und Gewerbe beschäftigten 2005 in 8.600 Betrieben gut 1,2 Millionen Menschen, was 38,3 % der sozialversicherungspflichtig Beschäftigten darstellt. Damit ist Baden-Württemberg das deutsche Bundesland mit dem höchsten Anteil der Industriebeschäftigten und dem höchsten Industrieanteil am Bruttoinlandsprodukt. Die international hohe Wettbewerbsfähigkeit der Industriebranchen des Landes wird maßgeblich durch hohe Forschungsleistungen der Unternehmen begünstigt (Wirtschaftsanteil an Forschung und Entwicklung: 3,4 % vom Bruttoinlandsprodukt).

Die drei nach Beschäftigtenzahlen wichtigsten Branchen sind

Im Schwarzwald war früher die Feinmechanik sehr bedeutend, insbesondere die Uhrenindustrie sowie später die Unterhaltungselektronik (Junghans, Kienzle, SABA, Dual).

Auf der Schwäbischen Alb war und ist hauptsächlich die Textilindustrie (mit Hugo Boss, Trigema und Steiff) von Bedeutung.

Die Mineralölraffinerie Oberrhein in Karlsruhe ist die zweitgrößte Mineralölraffinerie in Deutschland.

In Walldorf hat das größte europäische Software-Unternehmen SAP seinen Sitz. Aus Baden-Württemberg stammen die bekannten Programme VirtualBox, TeamSpeak und TeamViewer. Mit Lexware ist ein weiterer Softwareentwickler in Baden-Württemberg beheimatet und vor allem durch kaufmännische Softwarelösungen bekannt.

In Baden-Württemberg gibt es zwei Kernkraftwerke, das Kernkraftwerk Philippsburg und das Kernkraftwerk Neckarwestheim. Ein drittes Kernkraftwerk in Obrigheim wurde 2005 stillgelegt. 2011 wurden die jeweils ältesten Blöcke der Kernkraftwerke Neckarwestheim und Philippsburg stillgelegt. Seitdem sind nur noch zwei Blöcke mit einer Gesamtbruttoleistung von 2868 MW in Betrieb.

Die Flüsse des Landes weisen zahlreiche Laufwasserkraftwerke auf. Mitte der 1970er Jahre wurde das Rheinkraftwerk Iffezheim gebaut. Es wurde 2013 erweitert und ist seitdem mit 148 MW das größte dieser Art in Deutschland.

Mit Stand Ende 2015 waren in Baden-Württemberg 515 Windkraftanlagen mit einer Leistung von insgesamt 880 MW installiert, von denen 186 MW im ersten Halbjahr 2016 errichtet wurden. Ab 2015, als sich die von der grün-roten Regierung beschlossene Windenergieerlass auszuwirken begann, nahm der Ausbau deutlich zu. Allerdings hat Baden-Württemberg weiterhin die niedrigste installierte Windenergieleistung aller deutschen Flächenländer mit Ausnahme des Saarlandes. Mit Stand November 2015 war der aus 14 Windkraftanlagen bestehende Windpark Harthäuser Wald der größte Windpark des Landes.

In Baden-Württemberg produzieren fast 50 Zeitungsverlage täglich mehr als 220 unterschiedliche Tageszeitungen mit einer Auflage von mehr als zwei Millionen Exemplaren. Im Zeitungsbereich gibt es 17 Regionalzeitungen. Die auflagenstärksten (mind. 80.000 Exemplare) sind die "Südwest Presse", die "Stuttgarter Nachrichten", die "Schwäbische Zeitung", der "Mannheimer Morgen", die "Badische Zeitung", die "Badischen Neuesten Nachrichten", die "Rhein-Neckar-Zeitung", die "Heilbronner Stimme" und die "Stuttgarter Zeitung". Die meisten Lokalzeitungen beziehen den Mantel von einer Regionalzeitung.

Über 500 Verlage in Baden-Württemberg produzieren jährlich über 10.000 Neuerscheinungen. Viele traditionsreiche Unternehmen wie beispielsweise der Ernst Klett Verlag, die Verlagsgruppe Georg von Holtzbrinck oder die Verlagsgruppe Hüthig Jehle Rehm haben ihren Stammsitz im Land. Weiterhin befindet sich in Offenburg der Sitz der Hubert Burda Media, einer der größten Verlags- und Medienkonzerne Deutschlands, der auch auf dem internationalen Markt von Bedeutung ist.

Die wichtigsten wissenschaftlichen Bibliotheken Baden-Württembergs sind die Württembergische Landesbibliothek und die Badische Landesbibliothek. In den 800 öffentlichen Bibliotheken des Landes in kommunaler Trägerschaft werden etwa 16 Millionen Medien verfügbar gehalten. Hinzu kommen mehrere hundert Bibliotheken in kirchlicher Trägerschaft.
Der öffentlich-rechtliche Rundfunk wird vom Südwestrundfunk betrieben, der auch Klangkörper unterhält, die zu den führenden in Europa gehören: das SWR Symphonieorchester, das SWR Vokalensemble Stuttgart sowie die SWR Big Band Stuttgart.

Im privaten Hörfunk gibt es neben 13 Lokalsendern drei regionale Bereichssender (Radio Regenbogen, Antenne 1, Radio 7) und einen überregionalen Sender vorwiegend für junge Menschen (bigFM). Zwölf nichtkommerzielle private Hörfunkveranstalter, wie beispielsweise Bermudafunk, Querfunk oder radioaktiv, und fünf Lernradios ergänzen das Angebot.

Die Sender BWeins, HD-Campus-TV und Sport in Baden TV bieten ein privates TV-Landesprogramm. Darüber hinaus gibt es 14 regionale TV-Sender, wie das Rhein-Neckar Fernsehen, Regio TV Schwaben oder RTF.1 Neckar-Alb. Acht private bundesweite Veranstalter senden aus Baden-Württemberg.

Pro Jahr werden im Fremdenverkehrsgewerbe Baden-Württemberg rund 49 Millionen Übernachtungen gezählt. Das mittelständisch geprägte Tourismusgewerbe trägt rund fünf Prozent zum Bruttoinlandsprodukt bei. Der Tourismus bietet etwa 200.000 Arbeitsplätze sowie 8.000 Ausbildungsplätze. Da die Arbeitsplätze standortgebunden sind, gelten sie als relativ sicher.

Der Schwarzwald ist die wichtigste Erholungsregion in Baden-Württemberg und das meistbesuchte Urlaubsziel unter den deutschen Mittelgebirgen. Er ist insbesondere für seine romantischen Täler, Schluchten, Mühlen und die typischen Bauernhöfe sowie als Herkunftsort der Kuckucksuhr bekannt. Er ist auch wegen seines guten Wegenetzes mit Fernwanderwegen wie dem Westweg ein beliebtes Wandergebiet. Rund um den Feldberg (1493 m), dem höchsten Berg im Schwarzwald, sowie in vielen anderen Orten des Schwarzwalds hat der Wintersport eine lange Tradition.

Der Bodensee mit der Alpenkette im Hintergrund ist ebenfalls ein gut besuchtes Reiseziel und auch Naherholungsziel für die Städter; hier finden sich mit den Pfahlbauten Unteruhldingen und der zum UNESCO-Welterbe zählenden Klosterinsel Reichenau Zeugnisse unterschiedlichster Epochen. Am See haben die Blumeninsel Mainau und die alten Städte Konstanz und Meersburg die höchsten Besucherzahlen. Nicht weit von der Region um den Bodensee liegen das Donautal sowie Oberschwaben mit den alten reichsstädtisch geprägten Kleinstädten Biberach an der Riß und Ravensburg. Die Oberschwäbische Barockstraße führt durch dieses Zentrum des Barocks nördlich der Alpen.

Das württembergische Allgäu lockt mit seiner Landschaft und vielen Wandermöglichkeiten, ebenso wie weiter nördlich der Naturpark Schwäbisch-Fränkischer Wald.
Die Schwäbische Alb ist für ihre kleinen romantischen Städte (z. B. Bad Urach), die Heidelandschaften, die ausgedehnten Wälder, die Höhlen, Burgen und Schlösser bekannt (Burg Hohenzollern, Schloss Lichtenstein, Schloss Sigmaringen). Baden-Württemberg hat rund 60 Heilbäder und Kurorte, insbesondere im Schwarzwald und in Oberschwaben.

Anziehungspunkte für Städtereisende sind auch die Kurstadt Baden-Baden mit ihrer berühmten Spielbank, die von ihrer akademischen Bevölkerung geprägten alten Universitätsstädte Heidelberg (Heidelberger Schloss und Altstadt), Freiburg im Breisgau (Münster und „Bächle“ in der Altstadt) und Tübingen (am Rande des idyllischen Waldes Schönbuch gelegen, auch bekannt für seine Stocherkähne auf dem Neckar), die alten Reichsstädte Esslingen am Neckar, Reutlingen und Ulm (dort steht der höchste Kirchturm der Welt) und die zentral gelegene Landeshauptstadt Stuttgart mit dem zoologisch-botanischen Garten Wilhelma, der Staatsgalerie und den Automobilmuseen (Mercedes, Porsche).

Der Europa-Park im südbadischen Rust, Deutschlands größter Freizeitpark, zieht auch internationale Gäste an.

Beliebt sind auch die badische und die schwäbische Gastronomie sowie die badischen und württembergischen Weine. Im Schwarzwaldort Baiersbronn befinden sich mit der Schwarzwaldstube und dem Restaurant Bareiss gleich zwei Restaurants, die vom Guide Michelin mit drei Sternen ausgezeichnet sind. Insgesamt befinden sich 74 Sternelokale in Baden-Württemberg.

Die wichtigsten Autobahnen sind in Süd-Nord-Richtung die A 5 (von Basel über Karlsruhe bis Weinheim und weiter Richtung Frankfurt am Main) und die A 81 (von Singen am Hohentwiel über Stuttgart nach Würzburg). Weiter östlich stellt die A 7, die allerdings nur auf einem kurzen Abschnitt zwischen Ulm und Ellwangen durch baden-württembergisches Gebiet verläuft, eine weitere Süd-Nord-Verbindung dar.

In West-Ost-Richtung haben die A 6 (von Saarbrücken kommend über Mannheim und Heilbronn nach Crailsheim und weiter Richtung Nürnberg) und die A 8 (von Karlsruhe über Stuttgart nach Ulm und weiter Richtung München) die größte Bedeutung. Besondere straßenbauliche Herausforderung war und ist der Albaufstieg, der auf 16 km Länge rund 380 m Höhendifferenz vom Albvorland bis zur Albhochfläche überwindet.

Beide West-Ost-Autobahnen liegen weitgehend in der nördlichen Hälfte des Landes, in der bergigen Südhälfte fehlt eine durchgehende West-Ost-Autobahn. Der Verkehr in diesen Richtungen wird hier durch Bundesstraßen aufgenommen, wie z. B. durch die B 31, welche durch den Südschwarzwald sowie am nördlichen Bodenseeufer entlang führt und dabei die Autobahnen 5, 81 und 96 miteinander verbindet. Letztere erschließt den äußersten Südosten des Landes. Lediglich am Rande des Hochrheins entsteht derzeit nach und nach eine neue Autobahn, die A 98, von der es bereits einige Teilstücke gibt.

Gerade die Autobahnen um die Großstädte Baden-Württembergs werden vor allem während der Stoßzeiten von sehr starkem Verkehr belastet. Staus von über 25 Kilometern Länge sind auch außerhalb von Urlaubszeiten keine Seltenheit.

Die meistbefahrene Kreuzung Baden-Württembergs ist die als "Echterdinger Ei" bekannte Anschlussstelle Stuttgart-Degerloch, welche die Kreuzung der A 8 mit der autobahnähnlich ausgebauten B 27 bildet. Es liegt einige Kilometer östlich des Autobahnkreuzes Stuttgart und wird jeden Tag von 170.000 bis 180.000 Fahrzeugen befahren.

Die Länge der Autobahnen im Land beträgt 1.039 km, die Länge der Bundesstraßen 4.410 Kilometer. Die Landesstraßen sind 9.893 Kilometer lang, die Kreisstraßen 12.074 Kilometer. (Stand 2007)

Das Schienennetz der DB Netz AG im Land umfasst 3.400 Kilometer Strecke, auf denen 6.400 Kilometer Gleise verlegt und 9.500 Weichen eingebaut sind. Rund 1.400 Bahnübergänge sind vorhanden. Auf diesem Netz finden täglich 6.500 Zugfahrten statt, die dabei 310.000 Kilometer zurücklegen.

Weitere Strecken werden von anderen Eisenbahninfrastrukturunternehmen betrieben; die bedeutendsten sind die Württembergische Eisenbahn-Gesellschaft, die Hohenzollerische Landesbahn und die Karlsruher Albtal-Verkehrs-Gesellschaft. Die Nahverkehrsgesellschaft Baden-Württemberg bestellt im Auftrag des Landes den Schienenpersonennahverkehr in Baden-Württemberg. Das Karlsruher Modell als Innovation verbindet technologisch die Systeme Eisenbahn und Straßenbahn und wird an vielen Stellen weltweit nachgeahmt.

Das Land Baden-Württemberg fördert die Realisierung von Schienenwegeprojekten, die eigentlich Sache des Bundes wären, über mehrere Jahre hinweg mit insgesamt rund 2,4 Milliarden Euro, mehr als alle Länder zusammen (Stand: 2017). Zu den geförderten Vorhaben zählen Stuttgart 21, die Neubaustrecke Wendlingen–Ulm, die Ausbau- und Neubaustrecke Karlsruhe–Basel sowie die Südbahn.

Der Rhein hat bis Basel und der Neckar bis Plochingen den Status von Bundeswasserstraßen. Am Zusammenfluss in Mannheim liegt der Hafen Mannheim, einer der bedeutendsten Binnenhäfen Europas. Weitere große Häfen sind die Rheinhäfen Karlsruhe mit dem größten Ölbinnenhafen Europas, der Hafen Heilbronn und der Hafen in Kehl. Auf den Flüssen wird auch Fahrgastschifffahrt im Ausflugs- und Freizeitverkehr betrieben. Auf dem Bodensee verkehren die Autofähren, Personenschiffe und Ausflugsboote der "Weißen Flotte".

Baden-Württemberg verfügt über vier Verkehrsflughäfen. Der internationale Flughafen Stuttgart ist der sechstgrößte Deutschlands. Der Flughafen Karlsruhe/Baden-Baden bei Rastatt erfuhr einen Aufschwung durch die Angebote von Billigfluglinien und ist der zweitgrößte im Bundesland. Ein weiterer Regionalflughafen befindet sich in Friedrichshafen. Die Regionen Oberrhein und Hochrhein-Bodensee profitieren zudem von den grenznahen Flughäfen Flughafen Basel-Mülhausen, Flughafen Straßburg und Flughafen Zürich. Der Black Forest Airport bei Lahr ist ein Frachtflughafen; im Personenluftverkehr hat er zudem die Lizenz als Zubringerflughafen für den Europapark Rust. Mannheim besitzt mit dem City-Airport einen bedeutenden Verkehrslandeplatz.

Mit der Klosterinsel Reichenau im Bodensee, der Zisterzienserabtei Kloster Maulbronn und den Höhlen der ältesten Eiszeitkunst liegen drei Stätten des UNESCO-Welterbes vollständig in Baden-Württemberg. Außerdem hat das Land Anteil an den Prähistorischen Pfahlbauten um die Alpen und am Obergermanisch-Raetischen Limes, die ebenfalls zum Weltkulturerbe zählen. Zwei Häuser in der Stuttgarter Weißenhofsiedlung wurden 2016 als Teil des architektonischen Werks von Le Corbusier in die Welterbe-Liste aufgenommen.

In der Badischen Landesbibliothek in Karlsruhe wird die Ausgabe C des Nibelungenlieds aufbewahrt. Die drei vollständigen Handschriften aus dem 13. Jahrhundert wurden gemeinsam im Juli 2009 zum UNESCO-Weltdokumentenerbe ernannt.

Der Barbarastollen ist ein stillgelegter Versorgungsstollen bei Oberried in der Nähe von Freiburg im Breisgau. Als einziges Objekt in Deutschland unterliegt der Barbarastollen dem Sonderschutz nach den Regeln der Haager Konvention zum Schutz von Kulturgut bei bewaffneten Konflikten. Er dient seit 1975 als "Zentraler Bergungsort der Bundesrepublik Deutschland" zur Lagerung von fotografisch archivierten Dokumenten mit hoher national- oder kulturhistorischer Bedeutung. In Europa ist er das größte Archiv zur Langzeitarchivierung. Seit 1978 ist der Bergungsort auch in das Internationale Register der Objekte unter Sonderschutz bei der UNESCO in Paris eingetragen.

Die Stadt Mannheim ist seit 2014 "UNESCO City of Music".

Im Süden und entlang des Rheins wird die Schwäbisch-alemannische Fastnacht gefeiert. Das Cannstatter Volksfest wird nach dem Münchner Oktoberfest als zweitgrößtes Volksfest der Welt bezeichnet. Seit 1978 werden im Land die Heimattage Baden-Württemberg veranstaltet.

Im nördlichen Teil von Württemberg, in den Gebieten der ehemaligen Markgrafschaft Baden-Durlach und in der Kurpfalz ist die Bevölkerung überwiegend evangelisch. Die meisten anderen Gebiete, vor allem Süd- und Nordbaden sowie Oberschwaben, sind mehrheitlich römisch-katholisch. Wie überall in Deutschland gibt es auch eine wachsende Zahl von Menschen, die sich keiner oder anderen Religionen (z. B. Islam) zugehörig fühlen.

Für Baden-Württemberg insgesamt ergeben sich (Stand Ende 2011) folgende Mitgliederzahlen der Religionsgemeinschaften:

"Siehe auch: Erzbistum Freiburg, Bistum Mainz und Diözese Rottenburg-Stuttgart, Evangelische Landeskirche in Baden und in Württemberg, Neuapostolische Kirche Süddeutschland, Alt-Katholische Kirche in Deutschland#Verbreitung nach Bundesländern"

Amts- und Verkehrssprache ist Deutsch. Zahlreiche weitere Sprachen und Dialekte werden von jenen gesprochen, die aus anderen Sprach- oder Mundartregionen kommen oder einen entsprechenden Migrationshintergrund haben.

Die angestammten Dialekte werden von Sprachwissenschaftlern in alemannische und fränkische Mundarten gruppiert:

Zwischen den Mundarträumen bestehen Übergangsgebiete, die sich keinem der Räume eindeutig zuordnen lassen. Es existieren vor allem fränkisch-schwäbische (unter anderem um Calw, um Pforzheim, Strohgäu, Zabergäu), fränkisch-niederalemannische (um Baden-Baden und Rastatt) und schwäbisch-niederalemannische (Oberschwaben) Übergangsgebiete. Vor allem in diesen Gegenden wird die Unschärfe der germanischen Dialektgliederung deutlich. Neuere Entwicklungen sind das Eindringen schwäbischer Dialektmerkmale nach Heilbronn und Schwäbisch Hall.

Das Land wird auch außerhalb der Landesgrenze mit (vor allem schwäbischen) Dialektsprechern assoziiert. Die Landesregierung unter Erwin Teufel griff dies 1999 auf, indem sie den Werbeslogan „Wir können alles. Außer Hochdeutsch.“ prägte. Bekannte Mundartkünstler sind z. B. die Dichter bzw. Schriftsteller Thaddäus Troll und Harald Hurst, der Volksschauspieler und Komiker Willy Reichert, der Schauspieler Walter Schultheiß und der Kabarettist Christoph Sonntag. Es gibt Fernsehsendungen im Dialekt wie z. B. Hannes und der Bürgermeister. Auch der Kinofilm bzw. die Fernsehserie "Die Kirche bleibt im Dorf" wurden in Mundart verfilmt. Eine Verschriftlichung der Mundart wie in Luxemburg steht aber nicht zur Debatte.

Stuttgart war Spielort der Fußball-Weltmeisterschaften 1974 und 2006. In der Fußball-Bundesliga spielen mit dem SC Freiburg, der TSG 1899 Hoffenheim und dem VfB Stuttgart drei Vereine aus Baden-Württemberg. Der VfB Stuttgart wurde fünf Mal Deutscher Meister und drei Mal DFB-Pokalsieger. In der 2. Bundesliga spielen der SV Sandhausen und der 1. FC Heidenheim. In der 3. Liga sind die SG Sonnenhof Großaspach, der VfR Aalen und der langjährige Erstbundesligist Karlsruher SC aktiv. Die ehemaligen Bundesligisten Stuttgarter Kickers, SV Waldhof Mannheim und SSV Ulm 1846 spielen derzeit alle in der Fußball-Regionalliga Südwest. Aus dem heutigen Baden-Württemberg kommen auch die ehemaligen Deutschen Meister Freiburger FC (1907), FC Phönix Karlsruhe (1909), Karlsruher FV (1910), und VfR Mannheim (1949). Der baden-württembergische Fußball wird von drei regionalen Landesverbänden organisiert: Badischer Fußballverband (BFV), Südbadischer Fußball-Verband (SBFV) und Württembergischer Fußball-Verband (WFV). In der Frauen-Bundesliga spielen der SC Freiburg, der SC Sand und die TSG 1899 Hoffenheim. Ehemalige Bundesligisten sind: VfL Sindelfingen, TSV Crailsheim, SC Klinge Seckach, TSV Ludwigsburg, TuS Binzen und VfL Ulm/Neu-Ulm.

Frisch Auf Göppingen gewann 1960 und 1962 den Europapokal der Landesmeister und zwischen 1954 und 1972 neun Mal die deutsche Meisterschaft. Die Rhein-Neckar Löwen wurden 2016 und 2017 Deutscher Meister. In der Handball-Bundesliga der Männer spielt außerdem der TVB 1898 Stuttgart. In der 2. Bundesliga sind die HBW Balingen-Weilstetten, die SG BBM Bietigheim und die HSG Konstanz aktiv. In der Frauen-Bundesliga sind mit der SG BBM Bietigheim (Deutscher Meister 2016/17), dem TuS Metzingen (deutscher Vizemeister 2015/16), Frisch Auf Göppingen, der Neckarsulmer Sport-Union und dem TV Nellingen fünf Teams vertreten.

In der Basketball-Bundesliga sind die Walter Tigers Tübingen, die MHP Riesen Ludwigsburg und Ratiopharm Ulm beheimatet. In der ProA (zweite Basketball-Bundesliga) spielen die Crailsheim Merlins, die Kirchheim Knights, die MLP Academics Heidelberg, die PS Karlsruhe Lions, das Team Ehingen Urspring und die Ulmer OrangeAcademy.

Der VfB Friedrichshafen gewann 2007 die Volleyball Champions League und wurde je 13 Mal Deutscher Meister und Pokalsieger. In der Volleyball-Bundesliga spielen außerdem der TV Rottenburg und der TV Bühl. Die Mannschaft von Allianz MTV Stuttgart spielt in der Bundesliga der Damen; sie gewann den DVV-Pokal 2011, 2015 und 2017 und wurde 2015, 2016 und 2017 deutscher Vizemeister. CJD Feuerbach gewann von 1989 bis 1991 die deutsche Meisterschaft der Frauen und wurde vier Mal Pokalsieger.

In der Deutschen Eishockey Liga spielen der siebenfache deutsche Meister Adler Mannheim und die Schwenninger Wild Wings. In der DEL2 sind die Ravensburg Towerstars, die Bietigheim Steelers, die Heilbronner Falken sowie der EHC Freiburg vertreten.

Internationale Skisprung-Wettbewerbe werden auf der Hochfirstschanze in Titisee-Neustadt und im Adler-Skistadion Hinterzarten veranstaltet. Eine traditionsreiche Veranstaltung in der Nordischen Kombination ist der Schwarzwaldpokal in Schonach. Aus dem Schwarzwald stammen Olympiasieger und Weltmeister in nordischen Disziplinen wie Georg Thoma, Dieter Thoma und Martin Schmitt. Alpine Skiwettbewerbe finden im Feldberg-Gebiet bei Todtnau-Fahl statt, in der Heimat des ältesten deutschen Skiclubs, des Skiclub Todtnau 1891 e.V.

In Stuttgart finden zwei international bedeutende Tennisturniere statt: Der "MercedesCup" der Männer auf der Anlage des TC Weissenhof ist Teil der ATP World Tour 250. Der "Porsche Tennis Grand Prix" der Frauen in der Porsche-Arena gehört zur WTA Tour.

Das Männerteam des TK Grün-Weiss Mannheim spielt in der 1. Bundesliga. Das Frauenteam des TEC Waldau Stuttgart wurde 2005 Sieger der Bundesliga, das des TC Weissenhof zwischen 1975 und 1989 vier Mal Deutscher Mannschaftsmeister. Der TC Rüppurr aus Karlsruhe gehörte lange der 1. Herren- und aktuell der 1. Damen-Bundesliga an.

Aus dem nordbadischen Landesteil stammen die ehemaligen Weltranglistenersten Steffi Graf und Boris Becker.

Stuttgart war Austragungsort der Leichtathletik-Europameisterschaften 1986 und -Weltmeisterschaften 1993. Von 2006 bis 2008 fand hier das Leichtathletik-Weltfinale statt. Danach wurde die Mercedes-Benz Arena in ein reines Fußballstadion umgebaut. Das Internationale Hochsprung-Meeting Eberstadt wird jährlich ausgetragen.

Der Hockenheimring zählt zu den bedeutendsten Motorsport-Rennstrecken in Deutschland. Er gehört zu den Austragungsorten für den Großen Preis von Deutschland in der Formel 1 und ist Schauplatz des Eröffnungsrennens sowie des Finales der DTM.

In Holzgerlingen, Gaildorf und Reutlingen fanden Läufe zur Motocross-Weltmeisterschaft statt. In Rudersberg werden WM-Läufe mit Seitenwagen veranstaltet. In Berghaupten und Hertingen fanden Läufe zur Langbahn-Welt- und Europameisterschaft statt.

Erfolgreichster Hockeyverein ist der HTC Stuttgarter Kickers, welcher 2005 die deutsche Meisterschaft und 2006 den Europapokal der Landesmeister gewann. Aktuell spielen der Mannheimer HC und der TSV Mannheim sowohl bei den Damen als auch bei den Herren in der Feldhockey-Bundesliga. In der Deutschen Wasserball-Liga ist der SSV Esslingen vertreten. Der SV Cannstatt wurde 2006 Deutscher Meister.

In der Baseball-Bundesliga Süd spielen der deutsche Rekordmeister Mannheim Tornados, der zweifache Deutsche Meister Heidenheim Heideköpfe, die Stuttgart Reds und die Tübingen Hawks. Im American Football gewannen die Schwäbisch Hall Unicorns 2011, 2012 und 2017 den German Bowl. Außerdem spielen in der German Football League die Stuttgart Scorpions. Heidelberg ist neben Hannover das Zentrum des Rugbysports in Deutschland. Die dortigen Vereine Heidelberger RK, RG Heidelberg und SC Neuenheim gewannen in der Bundesliga insgesamt 14 Titel. Aktueller Deutscher Meister ist der TV Pforzheim 1834.

In der Schachbundesliga ist Baden-Württemberg vertreten mit dem SV 1930 Hockenheim und der OSG Baden-Baden, welche von 2006 bis 2015 zehn Mal in Folge die deutsche Meisterschaft gewann. Pferderennen werden seit 1858 auf dem Rennplatz Iffezheim bei Baden-Baden ausgetragen.

In Baden wurde mit dem Mannheimer Schulsystem der Vorläufer des modernen Schulsystems entwickelt. Heute folgt in Baden-Württemberg nach der vierjährigen Grundschule ein vielgliedriges Schulsystem mit Hauptschule und Werkrealschule, Realschule, Gymnasium und Gemeinschaftsschule. Schüler mit und ohne Behinderung werden gemeinsam erzogen und unterrichtet (inklusive Pädagogik). Die sonderpädagogische Beratung, Unterstützung und Bildung findet in den allgemeinen Schulen statt, soweit Schüler mit Anspruch auf ein entsprechendes Bildungsangebot kein sonderpädagogisches Bildungs- und Beratungszentrum besuchen. In ganz Baden-Württemberg gibt es lediglich drei integrierte Gesamtschulen in Freiburg, Heidelberg und Mannheim, die als Schulen besonderer Art im Schulgesetz für Baden-Württemberg eine Sondergenehmigung erhalten haben.
Des Weiteren führt Baden-Württemberg als einziges Bundesland die besondere Form des „sechsjährigen Wirtschaftsgymnasiums“, welches das bundesweit einzige berufliche Gymnasium ist, das bereits mit der gymnasialen Mittelstufe beginnt. Der Besuch dauert von Klassenstufe 8 und endet in Jahrgangsstufe 13 mit der allgemeinen Hochschulreife. Nach dem Regierungswechsel 2011 führte die Landesregierung als neue Schulform in Baden-Württemberg die Gemeinschaftsschule ein, die meist aus ehemaligen Hauptschulen (bzw. Werkrealschulen), vereinzelt aber auch aus Realschulen gebildet wurden. Zum Schuljahr 2013/14 gab es 129 Gemeinschaftsschulen im Land, weitere 81 folgen 2014.

Baden-Württemberg verfolgt eine dezentrale Bildungs-, Hochschul- und Forschungsinfrastruktur. Die Hochschulen sind über das ganze Land verteilt. Insgesamt liegen über ein Viertel aller Hochschulstandorte im ländlichen Raum.

In Baden-Württemberg gibt es 9 staatliche Universitäten, 6 pädagogische Hochschulen (Universitäten gleichgestellt) sowie die private Zeppelin-Universität und 73 staatliche und private Hochschulen.

Die baden-württembergischen Hochschulen gehören zu den renommiertesten in Deutschland. In einem Hochschulranking des Magazins "Focus" (2005) wurden sechs baden-württembergische Universitäten unter die besten zehn eingestuft. In Heidelberg befindet sich die älteste Universität in Deutschland; außerdem gibt es noch Universitäten in Freiburg, Konstanz, Mannheim, Stuttgart, Tübingen, Stuttgart-Hohenheim, Ulm, in Nachfolge der Universität Karlsruhe das Karlsruher Institut für Technologie sowie die private Zeppelin-Universität in Friedrichshafen. 2006 wurde die ehemalige Universität Karlsruhe bei der Exzellenzinitiative des Bundes und der Länder als eine von bundesweit drei zu fördernden Universitäten mit „Zukunftskonzepten“ ausgewählt. In der zweiten Runde der Exzellenzinitiative folgten 2007 die Universitäten Heidelberg, Konstanz und Freiburg als zu fördernde Hochschulen in den exklusiven Kreis der „Eliteuniversitäten“ nach, sodass zeitweise vier von insgesamt neun der durch die Exzellenzinitiative in allen drei Förderlinien geförderten deutschen Universitäten in Baden-Württemberg lagen. Im Zuge der dritten Runde der Exzellenzinitiative im Jahr 2012 verloren jedoch das Karlsruher Institut für Technologie und die Universität Freiburg diesen Status, während die Universität Tübingen erstmals diese Auszeichnung erlangte. Damit liegen derzeit noch drei von insgesamt elf „Eliteuniversitäten“ in Baden-Württemberg.

Die staatlichen Fachhochschulen tragen in Baden-Württemberg seit 2006 den Titel "Hochschule". Neben einer Vielzahl von weiteren Hochschulen, wie "Kunst- und Musikhochschulen" oder "pädagogischen Hochschulen" wird der tertiäre Bildungsbereich durch die Duale Hochschule Baden-Württemberg ergänzt. Bundesweit einzigartig ist die Popakademie Baden-Württemberg. In Ludwigsburg befindet sich die renommierte Filmakademie Baden-Württemberg.




</doc>
<doc id="478" url="https://de.wikipedia.org/wiki?curid=478" title="Betriebssystem">
Betriebssystem

Ein Betriebssystem, auch OS (von engl. "operating system") genannt, ist eine Zusammenstellung von Computerprogrammen, die die Systemressourcen eines Computers wie Arbeitsspeicher, Festplatten, Ein- und Ausgabegeräte verwaltet und diese Anwendungsprogrammen zur Verfügung stellt.
Das Betriebssystem bildet dadurch die Schnittstelle zwischen den Hardware-Komponenten und der Anwendungssoftware des Benutzers. Betriebssysteme bestehen in der Regel aus einem Kernel (deutsch: Kern), der die Hardware des Computers verwaltet, sowie speziellen Programmen, die beim Start unterschiedliche Aufgaben übernehmen. Zu diesen Aufgaben gehört unter anderem das Laden von Gerätetreibern. Betriebssysteme finden sich in fast allen Arten von Computern: Als Echtzeitbetriebssysteme auf Prozessrechnern und Eingebetteten Systemen, auf Personal Computern, Tabletcomputern, Smartphones und auf größeren Mehrprozessorsystemen wie z. B. Servern und Großrechnern.

Die Aufgaben eines Betriebssystems lassen sich wie folgt zusammenfassen:
Benutzerkommunikation; Laden, Ausführen, Unterbrechen und Beenden von Programmen; Verwaltung und Zuteilung der Prozessorzeit; Verwaltung des internen Speicherplatzes für Anwendungen; Verwaltung und Betrieb der angeschlossenen Geräte; Schutzfunktionen z. B. durch Zugriffsbeschränkungen. Die Gewichtung zwischen diesen Aufgaben wandelte sich im Laufe der Zeit, insbesondere wird Schutzfunktionen wie dem Speicherschutz oder begrenzten Benutzerrechten heute eine höhere Bedeutung zugemessen als noch in den 1990er Jahren. Dies macht Systeme allgemein robuster, reduziert z. B. die Zahl der Programm- und Systemabstürze und macht das System auch stabiler gegen Angriffe von außen, etwa durch Computerviren.

"Dieser" Artikel behandelt den Begriff „Betriebssystem“ hauptsächlich im Kontext „allgemein zur Informationsverarbeitung verwendete Computersysteme“. Daneben sind Betriebssysteme (mit ggf. spezialisierter Funktionalität) grundsätzlich in nahezu allen Geräten im Einsatz, in denen Software betrieben wird (wie Spielecomputer, Mobiltelefone, Navigationssysteme, Maschinen der Maschinenbaubranchen u. v. a.). Auch viele Steuerungssysteme (eingebettetes System) die z. B. in Flugzeugen, Autos, Zügen, oder in Satelliten zu finden sind, besitzen spezialisierte Betriebssysteme.

Ein Betriebssystem übernimmt zwei wesentliche Aufgaben, die im Grunde in keinem direkten Zusammenhang zueinander stehen:



Die Gesamtheit aller Programme und Dateien, die sämtliche Abläufe bei Betrieb eines Rechners steuern, wird Systemsoftware genannt. Dazu gehören Betriebssysteme, aber auch systemnahe Software wie Compiler, Interpreter und Editoren. Anwendungssoftware wie beispielsweise Browser oder Buchhaltungssoftware benutzen die Systemsoftware für einen ordnungsgemässen Ablauf. In der Literatur wird der Begriff „Betriebssystem“ innerhalb der Systemsoftware unterschiedlich breit interpretiert.

In der DIN-Sammlung 44300 (veraltet, ersetzt durch ISO/IEC 2382:2015 siehe: Liste der DIN-Normen/DIN 1–49999 unter DIN 44300) geht die Definition von seiner Aufgabe und Stellung in einer Programmhierarchie aus: 

Für Andrew S. Tanenbaum beschränkt sich der Begriff Betriebssystem im Wesentlichen auf den Kernel: „Editoren, Compiler, Assembler, Binder und Kommandointerpreter sind definitiv nicht Teil des Betriebssystems, auch wenn sie bedeutsam und nützlich sind.“ Viele Lehrbücher folgen dieser engeren Sichtweise. Andere Autoren zählen unter anderem auch eine Kommandosprache zum Betriebssystem: „Außer die Hardware zu verwalten [...], bieten moderne Betriebssysteme zahlreiche Dienste an, etwa zur Verständigung der Prozesse untereinander, Datei- und Verzeichnissysteme, Datenübertragung über Netzwerke und eine Befehlssprache.“ Eine noch weitere Fassung des Begriffes, die beispielsweise auch Editoren und Compiler umfasst, geht zum Teil auf ältere Werke des deutschen Sprachraums zurück, lässt sich aber auch in aktueller Literatur noch finden. So zählen die Autoren des Informatik-Dudens auch Übersetzungsprogramme und Dienstprogramme zu den wesentlichen Komponenten eines Betriebssystems. In jüngerer Zeit kann der GNU/Linux-Namensstreit als Beispiel für die Abgrenzungsprobleme angesehen werden.

Ungeachtet dessen, wie weit oder wie eng man den Begriff „Betriebssystem“ fasst, enthalten die Installationsmedien für Betriebssysteme für gewöhnlich zusätzliche Dienst- und Anwendungsprogramme.

Die Entwicklung von Computer-Betriebssystemen verlief und verläuft parallel zur Entwicklung und Leistungsfähigkeit verfügbarer Hardware: Beide Linien bedingten sich gegenseitig und ermöglichten bzw. erforderten Weiterentwicklungen auf der ‚anderen‘ Seite. Die Entwicklung verlief zum Teil in kleinen, manchmal in größeren Sprüngen:

Lochkarten verarbeitende Systeme (gilt sinngemäß auch für Lochstreifen) gehören mittlerweile (seit Anfang der 1970er Jahre) der Vergangenheit an. Jedoch sind sie ein guter Ansatz zur Betrachtung der Systementwicklung: In diesen räumlich relativ großen Systemen gab es noch keine externen elektronischen Speichermedien. Die Programme lagen (in Maschinensprache) in Form von Lochkartenstapeln vor und wurden durch den Operator über den Lochkartenleser in den internen Speicher ‚eingelesen‘. Nach der „Ende-Karte“ wurde das Anwendungsprogramm gestartet, das seine Eingabedaten je nach Aufgabenstellung ebenfalls über den Kartenleser lesen (deshalb der Begriff Stapelverarbeitung, engl. "batch processing", "queued systems") und seine Ergebnisse direkt über einen Drucker und/oder über den Kartenstanzer ausgeben musste. Vor- und nachgelagert waren, mithilfe elektro-mechanischer Geräte (Kartenlocher, Mischer, Sortierer) ausgeführt, Erfassungs-, Misch- und Sortiervorgänge erforderlich. Bereits zu diesem Zeitpunkt war die interne Verarbeitung deutlich schneller als die Ein-/Ausgabegeräte; das Lesen eines Lochkartenstapels (Karton mit 2000 Karten) dauerte ca. 5–10 Minuten, die Arbeitsspeichergrößen solcher Rechner lagen bei ca. 16 bis 64 kB (Beispiel siehe System/360).
Diese Maschinen besaßen kein konventionelles Betriebssystem, wie es heute geläufig ist. Lediglich ein Kontrollprogramm (resident monitor) wurde im Speicher gehalten und sorgte für den reibungslosen Ablauf, indem es die Kontrolle an die momentan auszuführenden Programme übergab. Der Rechner konnte stets nur ein Programm nach dem anderen ausführen.

Eine Weiterentwicklung – Multiprogrammed Batch Systems – konnte zusätzliche Geräte unterstützen (Magnetbandeinheiten, erste Magnetplatten mit z. B. 7,25 MB Speichervolumen), mehrere Programme gleichzeitig ausführen (z. B. in 3 'Partitionen') sowie Programme und Daten auf externen Speichern halten. Eine schnellere Abarbeitung war möglich, da die Zeit für das Lesen und Ausgeben der Kartenstapel entfiel – und die Prozessoren schneller wurden. Hier wurden Mechanismen wie das Spooling (Zwischenausgabe von Druckerdaten auf Magnetband mit verzögertem, parallel möglichem Drucken) und die Möglichkeit des Offline-Betriebs bereits ausgiebig genutzt. Jedoch war ein Programm nötig, welches sich der Aufgaben E/A-Verwaltung, Speicherverwaltung und vor allem CPU-Scheduling etc. annimmt. Ab diesem Zeitpunkt konnte man von ersten Betriebssystemen reden.

Die nächsten Schritte waren dann Folgen der jeweiligen Aufgabenbereiche, die den Systemen zukamen. Folgende Systeme sind entstanden und bis zum heutigen Tage im Einsatz: Parallele Systeme, Verteilte Systeme, Personal-Computer-Systeme, Time-Sharing-Systeme, Real-Time-Systeme und in neuester Zeit auch die Personal Digital Assistants und Smartphones.

Im PC-Bereich sind derzeit die meistgenutzten Betriebssysteme die verschiedenen Varianten von Windows (führend bei Systemen mit GUI), BSD inkl. Apple macOS und GNU/Linux (führend bei Servern). Für spezielle Anwendungen (Beispiel: industrielle Steuerung) werden auch experimentelle Betriebssysteme für Forschungs- und Lehrzwecke eingesetzt.

Neben den klassischen Varianten gibt es noch spezielle Betriebssysteme für verteilte Systeme, bei denen zwischen dem logischen System und den physikalischen System(en) unterschieden wird. Der logische Rechner besteht aus mehreren physikalischen Rechnereinheiten. Viele Großrechner, Number-Cruncher und die Systeme aus dem Hause Cray arbeiten nach diesem Prinzip. Eines der bekanntesten Betriebssysteme im Bereich verteilte Systeme ist Amoeba.

Zu den Aufgaben eines Betriebssystems gehören meist:


Als Gerät aus der Sicht eines Betriebssystems bezeichnet man aus historischen Gründen alles, was über Ein-/Ausgabekanäle angesprochen wird. Dies sind nicht nur Geräte im herkömmlichen Sinn, sondern mittlerweile auch interne Erweiterungen wie Grafikkarten, Netzwerkkarten und anderes. Die (Unter-)Programme zur Initialisierung und Ansteuerung dieser „Geräte“ bezeichnet man zusammenfassend als Gerätetreiber.

Als Betriebsmittel oder Ressourcen bezeichnet man alle von der Hardware eines Computers zur Verfügung gestellten Komponenten, also den Prozessor (bei Mehrprozessorsystemen die Prozessoren), den physikalischen Speicher und alle Geräte wie Festplatten-, Disketten- und CD-ROM-Laufwerke, Netzwerk- und Schnittstellenadapter und andere. Die "Hardware Compatibility List" enthält alle Hardware-Produkte, die im Zusammenhang mit einem bestimmten Betriebssystem auf ihre Funktionalität hin getestet wurden.

Moderne Rechnersysteme besitzen Zeitgeberbausteine (Timer). In frühen PCs wurde z. B. der Baustein 8284 des Unternehmens Intel eingesetzt. Dieser Baustein muss zunächst initialisiert werden. Er kann dann nach Ablauf einer Zeitspanne oder periodisch den Prozessor unterbrechen und ihn zur Abarbeitung einer eigenen Routine veranlassen. Neben der Initialisierung ist eine Unterbrechungsroutine zu erstellen, deren Aufruf in einer dafür geeigneten Sprache (meist Assembler) programmiert werden muss. Da Unterbrechungen asynchron auftreten, sind komplexe Verhältnisse hinsichtlich der Datenstrukturen zu berücksichtigen. Genaue Kenntnisse des Bausteins (Datenblatt), der Computerhardware (Unterbrechungsbehandlung) und des Prozessors sind erforderlich. Die einzelnen Komponenten, die an diesem Prozess beteiligt sind, fasst man unter dem Begriff Rechnerarchitektur zusammen.

Ein modernes Mehrprogrammbetriebssystem verwendet einen solchen Zeitgeberbaustein, um den normalerweise einzigen Prozessor periodisch (normalerweise im Millisekundenbereich) zu unterbrechen und eventuell mit einem anderen Programm fortzufahren (sogenanntes präemptives Multitasking). Die Initialisierung und die Unterbrechungsroutine werden dabei vom Betriebssystem implementiert. Auch wenn nur ein einzelner Prozessor zur Verfügung steht, können mehrere Programme ausgeführt werden, jedes Programm erhält einen Teil der Prozessorzeit (Scheduling). Jedes Programm verhält sich, bis auf die verlangsamte Ausführungszeit, so, als hätte es einen eigenen "virtuellen Prozessor".

Über einen Systemruf, zum Beispiel "alarm", wird jedem Programm darüber hinaus ein eigener "virtueller Zeitgeber" zur Verfügung gestellt. Das Betriebssystem zählt die Unterbrechungen des Original-Zeitgebers und informiert Programme, die den "alarm"-Systemruf verwendeten. Die einzelnen Zeitpunkte werden über eine Warteschlange verwaltet.

Die Hardware des Zeitgebers ist damit vor den Programmen verborgen. Ein System mit Speicherschutz erlaubt den Zugriff auf den Zeitgeberbaustein nur über den Kernel und nur über exakt definierte Schnittstellen (meist Systemrufe genannt, die über spezielle Prozessorbefehle wie TRAP, BRK, INT realisiert werden). Kein Programm kann somit das System gefährden, die Verwendung des "virtuellen Zeitgebers" ist einfach und portabel. Der Anwender oder Programmierer braucht sich nicht um die (komplexen) Details zu kümmern.

So wie Prozessoren und Zeitgeber "virtualisiert" werden, ist dies auch für alle anderen Betriebsmittel möglich. Dabei werden einige Abstraktionen teilweise nur als Software implementiert, andere erfordern spezielle Hardware.

Über Dateisysteme werden die Details der externen Speichersysteme (Festplatten-, Disketten- oder CD-ROM-Laufwerke) verborgen. Dateinamen und Verzeichnisse erlauben den bequemen Zugriff, die eigentlich vorhandene Blockstruktur und die Geräteunterschiede sind vollkommen unsichtbar.

Der interne Speicher (RAM) wird in Blöcke (Kacheln) aufgeteilt und den entsprechenden Programmen auf Anforderung zur Verfügung gestellt. Allenfalls noch vorhandene Daten werden zuvor gelöscht. Über "virtuellen Speicher" wird bei vielen Systemen jedem Programm ein kontinuierlicher (zusammenhängender) Bereich zur Verfügung gestellt. Dieser Speicher ist physikalisch nicht notwendigerweise zusammenhängend, es können sogar unbenutzte Teile auf den externen Speicher ausgelagert sein. Der "virtuelle Speicher" eines Programms kann sogar größer als der reale Speicher sein.

Die Details der Netzwerkzugriffe werden verborgen, indem auf die eigentliche Hardware (Netzwerkkarte) ein Protokollstapel aufgesetzt wird. Die Netzwerksoftware erlaubt beliebig viele "virtuelle Kanäle". Auf der Ebene der Sockets (Programmierung) ist die Netzwerkkarte vollkommen unsichtbar, das Netzwerk hat viele neue Fähigkeiten (bidirektionale, zuverlässige Datenströme, Adressierung, Routing) bekommen.

Als Grafische Benutzeroberfläche (GUI, Abk. für engl. "Graphical User Interface") wird generell eine Bildschirmausgabe beschrieben, wenn sie über eine Eingabeaufforderung hinausgeht.
Mit den richtigen Grafikkarten und Bildschirmen ist die Darstellung von geometrischen Objekten (Linien, Kreisen, Ellipsen, aber auch Schriftattributen und Farben) auf dem Bildschirm möglich, aus denen sich komplexere geometrische Elemente wie Knöpfe, Menüs, etc. Benutzeroberflächen zum einfachen Steuern von Programmen erstellen lassen.

Die Grafikkarte als Hardware ist für den Programmierer und Anwender vollkommen verborgen.

Die ersten Computer kamen ohne echtes Betriebssystem aus, da lediglich ein einziges Programm im Stapelbetrieb geladen sein konnte und die unterstützte Hardware noch sehr überschaubar war. Als Betriebssystem-Vorläufer gilt der 1956 in Gestalt des GM-NAA I/O bei General Motors für die IBM 704 erfundene "resident monitor," ein Stück Software, das nach Beendigung eines Stapelauftrags den Folgeauftrag automatisch startete. 1959 entstand daraus das SHARE Operating System (SOS), das bereits über eine rudimentäre Ressourcenverwaltung verfügte. Dessen Nachfolger IBSYS verfügte bereits über eine einfache Shell mit Kommandosprache.

1961 entstand mit dem Compatible Timesharing System (CTSS) für die IBM 7094 am MIT das erste Betriebssystem für Mehrbenutzerbetrieb. Das ermöglichte die quasi-gleichzeitige Benutzung der Rechenanlage durch mehrere Anwender mittels angeschlossener Terminals. Eine Vielzahl gleichzeitig geladener Programme erforderte es, die von ihnen beanspruchten Speicherbereiche voneinander abzugrenzen. Als Lösung entstand 1956 an der TU Berlin der Virtuelle Speicher und wurde Mitte der 1960er Jahre erstmals in Großrechner-Betriebssystemen umgesetzt.

Damals lieferte meist der Hersteller der Hardware das Betriebssystem, das nur auf einer bestimmten Modellreihe, ja sogar nur auf einem bestimmten System lief, sodass Programme weder zwischen verschiedenen Computern, noch über verschiedene Generationen hinweg portierbar waren. Mit der Einführung der Modellreihe System/360 von IBM führte IBM 1964 das Betriebssystem OS/360 in verschiedenen Versionen (OS/360 für rein lochkartenbasierte Systeme, TOS/360 für Maschinen mit Bandlaufwerken, DOS/360 für solche mit Festplatten) ein. Es war das erste Betriebssystem, das modellreihenübergreifend eingesetzt wurde.

Ab 1963 wurde Multics in Zusammenarbeit von MIT, General Electric und den Bell Laboratories (Bell Labs) von AT&T entwickelt, das jedoch erst ab 1969 bis 2000 im Einsatz war. Multics wurde in PL/I programmiert.
Inspiriert von den Arbeiten an Multics startete eine Gruppe um Ken Thompson und Dennis Ritchie an den Bell Labs 1969 mit der Entwicklung von Unix. Unix wurde in den Jahren 1972–1974 bis auf wenige Teile in der höheren Programmiersprache C mit dem Ziel der Portabilität neu implementiert, um auf der damals neuen PDP-11 lauffähig zu sein. In weiterer Folge entwickelte sich UNIX zu einer ganzen Familie von Systemen für verschiedenste Hardwareplattformen.

Die ersten PCs wie der Altair 8800 von 1975 verfügten zunächst über kein Betriebssystem. Daher mussten sämtliche Aktionen in einem reinen Maschinencode eingegeben werden. Sein erstes Betriebssystem erhielt der Altair 8800 in Form eines BASIC-Interpreters. Dieser stellte sowohl eine Programmierumgebung dar als auch die allgemeine Schnittstelle zwischen dem Benutzer und der Hardware (die dieser Interpreter direkt ansteuerte). Er war Laufzeitumgebung und Benutzerschnittstelle zugleich; über bestimmte Befehle konnte der Benutzer beispielsweise Daten laden und speichern und Programme ausführen. 1974 erfand Gary Kildall CP/M, das als erstes universelles PC-Betriebssystem gilt. Durch seine modulare Bauweise (der plattformunabhängige Kernel BDOS setzte auf einer Hardware-Treiberschicht namens BIOS auf) ließ es sich mit vertretbarem Aufwand auf zahlreiche zueinander inkompatible PC-Plattformen portieren. Eine Programmierumgebung steuerte nun (meistens) nicht mehr die Hardware direkt an, sondern nutzte die Schnittstellen des Betriebssystems. Daher war auch die Programmierumgebung nicht mehr nur auf einer bestimmten Hardware lauffähig, sondern auf zahlreichen PCs.

Für die aufkommende Computergrafik reichten rein textbasierte Benutzerschnittstellen nicht mehr aus. Die 1973 eingeführte Xerox Alto war das erste Computersystem mit einem objektorientierten Betriebssystem und einer grafischen Benutzeroberfläche, was diesen Rechner für Desktop-Publishing geeignet machte und einen großen Fortschritt in Sachen Benutzerfreundlichkeit darstellte.

In den 1980er Jahren wurden Heimcomputer populär. Diese konnten neben nützlichen Aufgaben auch Spiele ausführen. Die Hardware bestand aus einem 8-Bit-Prozessor mit bis zu 64 KiB RAM, einer Tastatur und einem Monitor- bzw. HF-Ausgang. Einer der populärsten dieser Computer war der Commodore C64 mit dem Mikroprozessor 6510 (einer Variante des 6502). Dieser Computer verfügte über einen in einem 8 KiB-ROM Systemkern namens Kernal mitsamt einem BIOS ("Basic Input/Output System"), das die Geräte Bildschirm, Tastatur, serielle IEC-Schnittstelle für Diskettenlaufwerke bzw. Drucker, Kassetteninterface initialisierte und über ein Kanalkonzept teilweise abstrahierte. Über ein 8 KiB-ROM-BASIC, das auf die Funktionen des BIOS aufsetzte, konnte das System bedient und programmiert werden. Das Betriebssystem dieses Computers kann auf der Ebene des BASIC-Interpreters als gute Hardwareabstraktion angesehen werden. Natürlich sind weder Kernel, Speicher- oder sonstiger Hardwareschutz vorhanden. Viele Programme, vor allem auch Spiele, setzten sich über das BIOS hinweg und griffen direkt auf entsprechende Hardware zu.

Xerox entwickelte im Palo Alto Research Center (PARC) das Smalltalk-Entwicklungssystem (Xerox entwickelte mit ALTO (1973) und Star (1981) erste Rechner mit grafischer Benutzeroberfläche). Das Unternehmen Apple bot Xerox an, die Technologie zu kaufen; da PARC aber vor allem ein Forschungszentrum war, bestand kein Interesse an Verkauf und Vermarktung. Nachdem Apple-Chef Steve Jobs Xerox Aktienanteile von Apple anbot, wurde ihm erlaubt, einigen Apple-Entwicklern die Xerox-Demos zu zeigen. Danach war den Apple-Entwicklern auf jeden Fall klar, dass der grafischen Benutzeroberfläche die Zukunft gehörte, und Apple begann, eine eigene grafische Benutzeroberfläche zu entwickeln.

Viele Merkmale und Prinzipien jeder modernen grafischen Benutzeroberfläche für Computer, wie wir sie heute kennen, sind originale Apple-Entwicklungen (Pull-down-Menüs, die Schreibtischmetapher, Drag and Drop, Doppelklicken). Die Behauptung, Apple habe seine GUI von Xerox illegal kopiert, ist ein ständiger Streitpunkt; es existieren jedoch gravierende Unterschiede zwischen einem Alto von Xerox und der Lisa/dem Macintosh.

Mitte der 1990er Jahre steckte das Unternehmen Apple in einer tiefen Krise; es schien kurz vor dem Ruin. Ein dringliches Problem war dabei, dass Apples Betriebssystem Mac OS als veraltet galt, weshalb sich Apple nach Alternativen umzusehen begann. Nach dem Scheitern des wichtigsten Projektes für ein modernes Betriebssystem mit dem Codenamen Copland sah sich Apple gezwungen, Ausschau nach einem für die eigenen Zwecke verwendbaren Nachfolger zu halten. Zuerst wurde vermutet, dass Apple das Unternehmen Be, mit ihrem auch auf Macs lauffähigen Betriebssystem BeOS, übernehmen würde. Die Übernahmeverhandlungen scheiterten jedoch im November 1996, da der frühere Apple-Manager und Chef von Be Jean-Louis Gassée im Falle einer Übernahme 300 Millionen US-Dollar und einen Sitz im Vorstand verlangte. Da Gil Amelio versprochen hatte, bis zur Macworld Expo im Januar 1997 die zukünftige Strategie zu Mac OS zu verkünden, musste schnell eine Alternative gefunden werden. Überraschend übernahm Apple dann noch im Dezember 1996 für 400 Mio. US-Dollar das Unternehmen NeXT des geschassten Apple-Gründers Steve Jobs mitsamt dem Betriebssystem NeXTStep bzw. OPENSTEP, das Apples Grundlage für die nachfolgende neue Betriebssystem-Generation werden sollte. Unter dem Codenamen Rhapsody wurde es weiterentwickelt zu einem UNIX für Heim- und Bürocomputer mit dem Namen „Mac OS X“. Ab Version 10.5 ist es konform mit der Single UNIX Specification; später hieß es einfach „OS X“, inzwischen „macOS“.

Das Betriebssystem OPENSTEP war die erste Implementierung der OpenStep-Spezifikationen, die zusammen mit Sun entwickelt wurden. Deren Entwicklung hatte Einfluss auf Java und somit letztlich auf Android.

Der Ursprung von DOS liegt in CP/M und wurde 1974 von Digital Research eingesetzt. Die Portierung auf den Motorola 68000, genannt CP/M-68k, selbst kein großer kommerzieller Erfolg, wurde zur Grundlage für TOS, dem Betriebssystem des Atari ST. MS-DOS Version 1.0 erschien 1981 als Nachbildung von CP/M und wurde für IBM-PCs eingesetzt. Es setzt auf das BIOS auf und stellt Dateisystemoperationen zur Verfügung.

Die ersten IBM-PCs waren ganz ähnlich wie der C64 aufgebaut. Auch sie verfügten über ein eingebautes BIOS zur Initialisierung und Abstraktion der Hardware. Sogar ein BASIC-Interpreter war vorhanden. Im Gegensatz zum BIOS wurde auf BASIC jedoch in den kompatiblen Rechnern anderer Unternehmen verzichtet.

Der PC konnte mit seinem Intel-8088-Prozessor (16-Bit-Register) bis zu 1 MiB Speicher adressieren, die ersten Modelle waren jedoch nur mit 64 KiB ausgestattet. Diskettenlaufwerke lösten die alten Kassettenrekorder als Speichermedium ab. Sie erlauben vielfaches Schreiben und Lesen einzeln adressierbarer 512-Byte-Blöcke. Die Benutzung wird durch ein "Disk Operating System (DOS)" vereinfacht, das ein abstraktes Dateikonzept bereitstellt. Blöcke können zu beliebig großen Clustern (Zuordnungseinheit – kleinste für das Betriebssystem ansprechbare Einheit) zusammengefasst werden. Dateien (logische Informationseinheiten) belegen einen oder mehrere dieser (verketteten) Cluster. Eine Diskette kann viele Dateien enthalten, die über Namen erreichbar sind.

Auf den ersten PCs war kein Speicherschutz realisiert, die Programme konnten daher an DOS vorbei direkt auf BIOS und sogar auf die Hardware zugreifen. Erst spätere PCs wurden mit dem Intel-80286-Prozessor ausgestattet, der Speicherschutz ermöglichte. MS-DOS stellte auch keine für alle Zwecke ausreichende Abstraktion zur Verfügung. Es ließ sich nur ein Programm gleichzeitig starten, die Speicherverwaltung war eher rudimentär. Ein Teil der Hardware wurde nicht unterstützt und musste von Programmen direkt angesprochen werden, was dazu führte, dass beispielsweise für jedes Spiel die Soundkarte neu konfiguriert werden musste. Die Performance einiger Routinen, speziell zur Textausgabe, war verbesserungswürdig. Viele Programme setzten sich daher über das Betriebssystem hinweg und schrieben z. B. direkt in den Bildschirmspeicher. MS-DOS wurde mit einem Satz von Programmen (sogenannten Werkzeugen) und einem Kommandointerpreter (COMMAND.COM) ausgeliefert.

1983 begann das Unternehmen Microsoft mit der Entwicklung einer grafischen Betriebssystem-Erweiterung („Grafik-Aufsatz“) für MS-DOS namens Windows. Das MS-DOS und BIOS-Design der PCs erlaubten keine Weiterentwicklung in Richtung moderner Serverbetriebssysteme. Microsoft begann Anfang der 1990er ein solches Betriebssystem zu entwickeln, das zunächst als Weiterentwicklung von OS/2 geplant war (an dessen Entwicklung Microsoft zwischen 1987 und 1991 beteiligt war): Windows NT 3.1 (Juli 1993). Für den Consumer-Markt brachte Microsoft am 15. August 1995 Windows 95 heraus; es setzt auf MS-DOS auf. Dieser „Consumer-Zweig“ wurde mit der Veröffentlichung von Windows Millennium (August/September 2000) abgeschlossen.

Aufbau von Windows NT: Über die Hardware wurde eine Abstraktionsschicht, der "Hardware Abstraction Layer (HAL)" gelegt, auf den der Kernel aufsetzte. Verschiedene Gerätetreiber waren als Kernelmodule ausgeführt und liefen wie der Kernel im privilegierten "Kernel Mode". Sie stellten Möglichkeiten der E/A-Verwaltung, Dateisystem, Netzwerk, Sicherheitsmechanismen, virtuellen Speicher usw. zur Verfügung. Systemdienste "(System Services)" ergänzten das Konzept; wie ihre Unix-Pendants, die daemons, waren sie in Form von Prozessen im "User-Mode" ausgeführt.

Über sogenannte "Personalities" wurden dann die Schnittstellen bestehender Systeme nachgebildet, zunächst für Microsofts eigenes, neues Win32-System, aber auch für OS/2 (ohne Grafik) und POSIX.1, also einer Norm, die eigentlich Unix-Systeme vereinheitlichen sollte. Personalities liefen wie Anwenderprogramme im unprivilegierten "User-Mode". Das DOS-Subsystem war in Form von Prozessen implementiert, die jeweils einen kompletten PC mit MS-DOS als virtuelle Maschine darstellten; darauf konnte mit einer besonderen Version von Windows 3.1, dem "Windows-on-Windows", auch Win16-Programme ausgeführt werden. "Windows-on-Windows" blendete dazu die Fenster der Win16-Programme in das Win32-Subsystem ein, das die Grafikausgabe verwaltete. Das System erlaubte daher die Ausführung von Programmen sowohl für MS-DOS wie für die älteren Windows-Betriebssysteme, allerdings unter vollkommener Kontrolle des Betriebssystems. Dies galt aber nur für die Implementierung für Intel-80386-Prozessoren und deren Nachfolger.

Programme, die direkt auf die Hardware zugreifen, blieben aber außen vor. Insbesondere viele Spiele konnten daher nicht unter Windows NT ausgeführt werden, zumindest bis zur Vorstellung von WinG, das später in DirectX umbenannt wurde. Ohne die Möglichkeit eines direkten Zugriffs auf die Grafikhardware bzw. -treiber war die Programmierung von leistungsfähigen Actionspielen zunächst auf die älteren Windows-Versionen beschränkt.

Windows NT erschien in den Versionen 3.1, 3.5, 3.51 und 4.0. Windows 2000 stellte eine Weiterentwicklung von Windows NT dar. Auch Windows XP, Windows Server 2003, Windows Vista, Windows Server 2008, Windows 7, Windows Server 2012, Windows 8 und Windows 10 bauen auf der Struktur von Windows NT auf.

1991 begann Linus Torvalds in Helsinki/Finnland mit der Entwicklung des Linux-Kernels, den er bald danach der Öffentlichkeit zur Verfügung stellte.

Es läuft als portables Betriebssystem auf verschiedenen Rechnerarchitekturen, wurde aber zunächst für PCs mit Intel-80386-Prozessor entwickelt. Das in diesen Rechnern verwendete BIOS dient nur noch zum Initialisieren der Hardware und zum Starten des Bootloaders, da die Routinen des BIOS für Multitaskingsysteme wie Linux ungeeignet sind. Dies kommt zustande, da insbesondere der Prozessor durch Warten belastet wird anstatt durch eine – in der Hardware durchaus vorhandene – geschickte Unterbrechungsverwaltung "(interrupt handling)" auf Ereignisse "(events)" zu reagieren. Linux verwendet daher nach dem Starten des Systems eigene Gerätetreiber.

Es verteilt die Prozessorzeit auf verschiedene Programme (Prozesse). Jeder dieser Prozesse erhält einen eigenen, geschützten Speicherbereich und kann nur über Systemaufrufe auf die Gerätetreiber und das Betriebssystem zugreifen.

Die Prozesse laufen im Benutzermodus "(user mode)", während der Kernel im Kernel-Modus "(kernel mode)" arbeitet. Die Privilegien im Benutzermodus sind sehr eingeschränkt. Ein direkter Zugriff wird nur sehr selten und unter genau kontrollierten Bedingungen gestattet. Dies hat den Vorteil, dass kein Programm z. B. durch einen Fehler das System zum Absturz bringen kann.

Linux stellt wie sein Vorbild Unix eine vollständige Abstraktion und Virtualisierung für nahezu alle Betriebsmittel bereit (z. B. virtueller Speicher, Illusion eines eigenen Prozessors).

Das Unternehmen StatCounter analysiert die Verbreitung von Endanwender-Betriebssystemen anhand von Zugriffsstatistiken diverser Websites.
Sehr viele Jahre war Windows an der Spitze, bis es laut StatCounter 2017 von Android überholt wurde.
Die laut StatCounter am weitesten verbreiteten Endanwender-Betriebssysteme sind: 





</doc>
<doc id="479" url="https://de.wikipedia.org/wiki?curid=479" title="Billy Wilder">
Billy Wilder

Billy Wilder (* 22. Juni 1906 als "Samuel Wilder" in Sucha, Galizien, damals Österreich-Ungarn, heute Sucha Beskidzka, Polen; † 27. März 2002 in Los Angeles, Kalifornien) war ein US-amerikanischer Drehbuchautor, Filmregisseur und Filmproduzent österreichischer Herkunft.

Wilder wirkte stilbildend für die Genres Filmkomödie und -drama und schuf als Regisseur und Drehbuchautor von Komödien wie "Manche mögen’s heiß", "Eins, Zwei, Drei" und "Das Mädchen Irma la Douce," aber auch von dramatischen Filmen wie "Das verlorene Wochenende, Frau ohne Gewissen, Sunset Boulevard" oder "Zeugin der Anklage" Filme von zeitloser Relevanz. Sein Werk umfasst mehr als 60 Filme, die in einem Zeitraum von über 50 Jahren entstanden sind. Er wurde als Autor, Produzent und Regisseur 21-mal für einen Oscar nominiert und sechsmal ausgezeichnet. Allein bei der Oscarverleihung 1961 wurde er als Produzent, Drehbuchautor und Regisseur für den Film "Das Appartement" dreifach ausgezeichnet, was bis heute nur insgesamt sieben Regisseuren widerfahren ist.

Samuel Wilder war der Sohn jüdischer Eltern. Sein Vater Max Wilder betrieb in Krakau das Hotel „City“ sowie mehrere Bahnhofsrestaurants in der Umgebung. Die Mutter rief den Sohn von jeher „Billie“. Samuel nannte sich daher „Billie Wilder“ (deutsch ausgesprochen); in den USA änderte er die Schreibweise dann in „Billy“.

Mitten im Ersten Weltkrieg zog die Familie aus Angst vor der herannahenden russischen Armee 1916 nach Wien. In seiner Jugend war er dort eng mit dem späteren Hollywood-Regisseur Fred Zinnemann befreundet, mit dem er zeitweise in dieselbe Klasse (Privatgymnasium Juranek im 8. Gemeindebezirk) ging und zu dem er sein Leben lang Kontakt hielt. Nach seiner Matura arbeitete er als Reporter für die Wiener Boulevardzeitung "Die Stunde". Als er 1926 den Jazzmusiker Paul Whiteman interviewte, war dieser von ihm so begeistert, dass er ihn einlud, nach Berlin mitzukommen, um ihm die Stadt zu zeigen. Eine Woche später stellte sich heraus, dass "Die Stunde" Wiener Geschäftsleute und Prominente zu jener Zeit mit der Drohung erpresste, unvorteilhafte Artikel über sie zu veröffentlichen. Die Angelegenheit wurde zum größten Medienskandal der Ersten Republik in Österreich und Wilder beschloss, in Berlin zu bleiben und für eine andere Zeitung zu arbeiten.

Wilder wohnte 1927 in Berlin-Schöneberg (Viktoria-Luise-Platz 11) zur Untermiete: „Eineinhalb Jahre. Ein winziges Zimmer mit düsterer Tapete. Wand an Wand mit einer ständig rauschenden Toilette.“ Hier begann auch seine Filmkarriere, als der Direktor einer Filmgesellschaft, Maxim Galitzenstein, sich in Unterhosen aus dem Schlafzimmer der Nachbarin in Wilders Zimmer flüchten musste und deshalb nicht umhinkam, dessen erstes Drehbuch zu kaufen.

Als Ghostwriter für bekannte Drehbuchautoren wie Robert Liebmann und Franz Schulz konnte Wilder sich neben seiner Tätigkeit als Reporter eine zusätzliche Einkommensquelle erschließen. So trug er zu dem Filmklassiker "Menschen am Sonntag" (unter anderem mit Curt Siodmak, Robert Siodmak, Fred Zinnemann und Edgar G. Ulmer) bei. Gemeinsam mit Erich Kästner schrieb er 1931 das Drehbuch für "Emil und die Detektive," die Erstverfilmung von Kästners Roman – damals noch als „Billie Wilder“.

Unmittelbar nach der Machtergreifung der Nationalsozialisten übersiedelte Wilder 1933 nach Paris, wo er sich als Ghostwriter für französische Drehbuchautoren seinen Lebensunterhalt verdiente. Hier inszenierte er auch seinen ersten Film, "Mauvaise graine," mit Danielle Darrieux. 1934 konnte er, von Joe May mit einem Besuchervisum ausgestattet, in die Vereinigten Staaten einreisen. Er nannte sich nun „Billy“, wurde 1936 von Paramount Pictures unter Vertrag genommen und schrieb die Drehbücher zu Komödien wie "Ninotschka," bei dem sein Vorbild Ernst Lubitsch Regie führte, und "Enthüllung um Mitternacht," die beide 1939 veröffentlicht wurden. 1942 führte Wilder in der Komödie "Der Major und das Mädchen" mit Ginger Rogers erstmals in Hollywood Regie, da er mit den ständigen Änderungen an seinen Drehbüchern unzufrieden war und selbst das Heft in die Hand nehmen wollte. Sein zweiter Film "Fünf Gräber bis Kairo" mit Franchot Tone diente 1943 im Zweiten Weltkrieg als Propagandafilm gegen das NS-Regime. Im folgenden Jahr inszenierte er mit "Frau ohne Gewissen" einen bedeutenden Klassiker des Film noir, der Barbara Stanwyck als Femme fatale zeigt. Der Film erhielt sieben Oscar-Nominierungen, unter anderem für Wilder in den Kategorien "Beste Regie" und "Bestes adaptiertes Drehbuch".

1945 erhielt Wilder vom U.S. Army Signal Corps den Auftrag, das umfangreich vorhandene Material des amerikanischen und britischen Militärs u. a. über die Befreiung des KZ Bergen-Belsen zu einem Kurzfilm zu verdichten. Es wurde der einzige Dokumentarfilm unter seiner Aufsicht, "Die Todesmühlen". Trotz aller persönlichen Betroffenheit – seine nächsten Verwandten waren im Holocaust ermordet worden – wollte er keinen „Gräuelfilm“, da er sofort erkannte: „Objektiv gesehen: So unsympathisch die Deutschen sein mögen, sie sind – und jetzt zitiere ich Wort für Wort den guten Onkel in Washington – unsere logischen Verbündeten von morgen.“

Nach seinen Nominierungen für "Frau ohne Gewissen" erhielt er 1946 als Regisseur und als Drehbuchautor je einen Oscar für den Film "Das verlorene Wochenende". Das Drama um einen erfolglosen Schriftsteller (Ray Milland) setzte sich ungewöhnlich realistisch mit den Problemen eines Alkoholikers auseinander. Kurz danach kam Wilder im Auftrag der amerikanischen Regierung im Rang eines Colonels nach Deutschland und inszenierte im kriegszerstörten Berlin 1947/48 den Film "Eine auswärtige Affäre" mit Jean Arthur und Marlene Dietrich in den Hauptrollen, der sich kritisch mit der NS-Vergangenheit im besetzten Deutschland auseinandersetzte. Im selben Jahr führte er zudem Regie beim Filmmusical "Ich küsse Ihre Hand, Madame" mit Bing Crosby.

Nach 1950 war Wilder meist als Produzent an seinen Filmen beteiligt. Er schuf Klassiker wie "Boulevard der Dämmerung" (1950), mit Gloria Swanson als verblendeter Ex-Diva, "Das verflixte 7. Jahr" (1955) und "Manche mögen’s heiß" (1959), beide mit Marilyn Monroe, "Zeugin der Anklage" (1958), erneut mit Marlene Dietrich, sowie "Das Appartement" (1960) und "Das Mädchen Irma la Douce" (1963), beide mit Shirley MacLaine, und die Komödie "Eins, Zwei, Drei" (1961) mit James Cagney, Liselotte Pulver und Horst Buchholz in den Hauptrollen.

Billy Wilders Alter Ego auf der Leinwand verkörperten Jack Lemmon und William Holden. Während Holden vor allem in dramatischen Werken wie "Boulevard der Dämmerung, Stalag 17" oder "Fedora" wirkte, war Lemmon in Komödien wie "Manche mögen’s heiß, Das Mädchen Irma la Douce, Der Glückspilz" und "Extrablatt" zu sehen.

Wilders spätere Werke konnten an die Erfolge seiner Glanzzeit nicht mehr anknüpfen. Ab Mitte der 1980er Jahre beschränkte er sich auf Beratertätigkeiten für United Artists. Wilder, dessen Familie im Holocaust umkam (siehe auch Privatleben), war ursprünglich als Regisseur für "Schindlers Liste" im Gespräch. Aufgrund seines hohen Alters übernahm dann jedoch Steven Spielberg selbst die Regie. Wilder war von Spielbergs Werk tief berührt und ließ ihn das in einem Brief wissen, was Spielberg in seinem Antwortbrief als große Ehrerbietung durch den Altmeister bezeichnete.

1999 übernahm Billy Wilder die Schirmherrschaft über das Bonner „Billy-Wilder-Institute of Film and Television Studies oHG“, das 2002 kurz vor seinem Tod geschlossen werden musste.

Billy Wilder starb am 27. März 2002 in Los Angeles im Alter von 95 Jahren an den Folgen einer Lungenentzündung. Er hatte schon länger mit gesundheitlichen Problemen zu kämpfen, aber immer noch Interviews gegeben. Sein Grab befindet sich im Westwood Village Memorial Park Cemetery.

Im 3. Wiener Bezirk wurde im Jahr 2008 eine Straße nach ihm benannt (Billy-Wilder-Straße).

Wilder war von 1936 bis 1947 mit Judith Coppicus-Iribe verheiratet. Sie hatten eine gemeinsame Tochter, Victoria (* 1939). 1949 heiratete Wilder die Schauspielerin und Sängerin Audrey Young (1922–2012).

1989 ließ Wilder, der insbesondere Picasso und europäische Impressionisten gesammelt hatte, seine umfangreiche Gemäldesammlung versteigern. Der Erlös betrug 32,6 Millionen US-Dollar.

Seine Mutter Gitla starb 1943 im KZ Plaszow, sein Stiefvater Bernhard (Berl) Siedlisker wurde im KZ Belzec ermordet.

Billy Wilder ist als "Weilder" geläufig. Wolfgang Glück berichtete jedoch, Wilder habe sich ihm 1987 als "Wilder" bekanntgemacht und seinen Namen immer in dieser Form ausgesprochen.

Als er bereits zahlreiche Drehbücher geschrieben und sich oft über die Umsetzung geärgert hatte, entschied sich Wilder, bei der Umsetzung seiner Drehbücher selbst die Regie zu übernehmen. Die Idee sei ihm gekommen, als sich Charles Boyer bei den Dreharbeiten zu "Das goldene Tor" weigerte, ein Zwiegespräch mit einer Kakerlake zu führen, wie Wilder es im Drehbuch vorgesehen hatte, und Regisseur Mitchell Leisen danach Wilders Proteste zurückwies. Die Szene war ihm besonders wichtig, weil er Erinnerungen an seine eigene Situation verarbeitet hatte, als er 1934 in Mexicali an der amerikanisch-mexikanischen Grenze darauf warten musste, wieder in die USA einreisen zu dürfen, um endgültig die amerikanische Staatsbürgerschaft zu erlangen. In einem seiner späteren Filme griff Wilder das Motiv in abgewandelter Form auf, als er mit James Stewart in Lindbergh – Mein Flug über den Ozean (1957) den berühmten Piloten bei seinem Flug über den Atlantik zu einer zufällig im Cockpit mitreisenden Fliege sprechen ließ.

Zuvor war es Preston Sturges als erstem Drehbuchautor gelungen, ins Regiefach zu wechseln und das strenge „Kastendenken“ des alten Hollywood zu durchbrechen. Preston Sturges verkaufte sein Drehbuch für "Der große McGinty" für zehn Dollar an Paramount Pictures unter der Bedingung, es selbst verfilmen zu dürfen. Der Film wurde ein Kassenschlager.

Wilders Regiestil ist von seiner Herkunft aus dem schreibenden Fach geprägt; er glaubte wie kaum ein anderer an Macht und Bedeutung des Drehbuchs. Wie Hitchcock ließ er bei den Dreharbeiten kaum Änderungen zu. Er lehnte allzu extravagante Kameraeinstellungen ab, weil sie das Publikum von der Handlung ablenken könnten. Nur wenn das Publikum sich nicht mehr bewusst sei, dass ein Kamerateam anwesend ist, entstehe der Zauber eines guten Films. Dennoch war ihm die Bildgestaltung sehr wichtig. In "Das Appartement" nutzte er das Cinemascope-Breitwandformat geschickt aus, um etwa die Einsamkeit seines Protagonisten filmisch darzustellen. Er liebte den Schwarzweißfilm und nutzte diesen noch, als der Farbfilm längst Standard war. Seine erfolgreichsten Filme hat er in Schwarzweiß gedreht.

In Volker Schlöndorffs TV-Dokumentation "Billy Wilder, wie haben Sie’s gemacht?" erläuterte Wilder einige seiner Grundsätze, die beim Filmemachen zu beachten seien; so beispielsweise, wann Nahaufnahmen "(close-ups)" nicht gemacht werden dürften. Ein Darsteller, der versuche, eine plötzliche Erkenntnis, eine Eingebung darzustellen, sehe immer dumm aus (“looks stupid”). Auch die Nahaufnahme des Gesichts eines Menschen, der gerade eine Todesnachricht erhält, sei unpassend. Es gebe zwei wichtige Elemente eines guten Drehbuchs, die Konstruktion einer Geschichte und die Dialoge. Agatha Christie sei eine ausgezeichnete Konstrukteurin von Geschichten, aber eher schwach in ihren Dialogen gewesen. Raymond Chandler dagegen habe sehr gute Dialoge verfassen können, jedoch von der Konstruktion einer Geschichte keine Ahnung gehabt. Als sein Vorbild betrachtete Wilder Ernst Lubitsch, für den er einige Drehbücher (Ninotchka) verfasst hat. In seinem Büro hing ein Schild mit der Aufschrift: “How would Lubitsch have done it?” (Wie hätte Lubitsch es gemacht?)

In der Struktur bevorzugte Billy Wilder den Aufbau der Handlung in drei Akten aus klassischen Theaterstücken. Wilder legte seine Dreiakter so an, dass die Hauptakteure am Ende des dritten Aktes eine moralische Entscheidung treffen mussten.

Wilders Filme zeichnen sich durch eine straffe Handlung und spritzige, griffige Dialoge aus. In den Handlungen gelang es ihm oft die Grenzen des Unterhaltungsfilmes zu durchstoßen und schlüpfrige Details oder als anstößig geltende Themen in seinen Filmen zu realisieren, um der bigotten Gesellschaft den moralischen Spiegel vor die Nase zu halten. Dabei bediente er sich einer ausgefeilten Symbolsprache und vermeintlich harmloser Formulierungen, um das Hays Office, die Zensurstelle der amerikanischen Filmindustrie, hinters Licht zu führen. Er thematisierte gleich in seiner ersten Regiearbeit ein Liebesverhältnis eines Erwachsenen mit einer Minderjährigen, was besonders im Wortspiel des Originaltitels "The Major and the Minor (Der Major und das Mädchen)" deutlich wurde. Er ließ Männer in Frauenkleidern spielen "(Manche mögen’s heiß)" und schuf so die Grundlage, um eine Fülle anzüglicher und hintergründiger Anspielungen unterzubringen. Ehebruch kommt in seinen Filmen in zahlreichen Variationen vor, ebenso Prostitution oder Homosexualität.

Seine Protagonisten sind keine strahlenden moralischen Helden, sondern oft eher Durchschnittsmenschen mit Fehlern und Schwächen, die aber aufgrund besonderer Herausforderungen in bestimmten Situationen über sich hinauswachsen.

Bestimmte Versatzstücke aus seinen Filmen hat Wilder mehrfach verwendet.

So zum Beispiel














Academy Awards

Golden Globes

Writers Guild of America

Directors Guild of America

Cannes Film-Festival 1946

Laurel Awards

PGA Golden Laurel Awards

British Academy Film Award

Blue Ribbon Award

Bodil

Boulevard der Stars

David di Donatello

Deutscher Filmpreis

Europäischer Filmpreis

Internationale Filmfestspiele Berlin

Fotogramas de Plata
gemeinsam mit "Atlantic City" (1980)

Sindacato Nazionale Giornalisti Cinematografici Italiani

Los Angeles Film Critics Association Award

Venedig Film Festival

American Film Institute

Walk of Fame

Weitere Auszeichnungen und Ehrungen


Zur 60-Jahr-Feier der Filmakademie Wien stiftete Rudolf John den „Billy Wilder Award“, der von der Filmakademie vergeben wird.

Im Jahr 2003 legte die österreichische Post eine Sonderbriefmarke zu seinen Ehren auf.

Von 2000 bis 2016 gab es direkt neben der Deutschen Kinemathek im Berliner Sony Center eine Bar namens „Billy Wilder’s“.

Des Weiteren wurde im Berliner Bezirk Steglitz-Zehlendorf eine Straße mit dem Namen „Billy-Wilder-Promenade“ nach ihm benannt.





</doc>
<doc id="481" url="https://de.wikipedia.org/wiki?curid=481" title="Barry Levinson">
Barry Levinson

Barry Levinson (* 6. April 1942 in Baltimore, Maryland) ist ein US-amerikanischer Filmregisseur. Für "Rain Man" gewann er 1989 den Oscar in der Kategorie "Beste Regie".

Während seines Studiums hatte Levinson unterschiedliche Jobs beim Fernsehen in Baltimore. Um endgültig in der Film- und Fernsehbranche zu arbeiten, ging er nach Los Angeles und nahm Schauspielunterricht. Erste Jobs als Komödiant in Comedy-Clubs brachten ihn zum Schreiben von Drehbüchern. Anfang der 1970er Jahre begann er Drehbücher für Sitcoms und Kinofilme zu schreiben, die unter anderem von Mel Brooks realisiert wurden. Sein Debüt als Regisseur gab er dann 1982 mit "American Diner". 1989 gewann er für "Rain Man" den Oscar für die beste Regie. Obwohl er in seiner Karriere auch immer wieder Flops wie "Toys", "Avalon" oder "Sphere – Die Macht aus dem All" landete, gelangen ihm ebenfalls häufig spektakuläre Erfolge wie "Rain Man" und "Banditen!".

Viele seiner Filme spielen in der Stadt seiner Jugend Baltimore: "American Diner", "Tin Men", "Avalon" und "Liberty Heights". 







http://www.ofdb.de/view.php?page=person&id=30971


</doc>
<doc id="482" url="https://de.wikipedia.org/wiki?curid=482" title="Bernardo Bertolucci">
Bernardo Bertolucci

Bernardo Bertolucci (* 16. März 1940 in Parma) ist ein italienischer Filmregisseur. Zwischen 1962 und 2012 inszenierte er 16 Spielfilme. Sein dem Geheimnis verpflichteter Erzählstil ist oft opernhaft und melodramatisch und lässt Mehrdeutigkeiten viel Raum. Bertolucci inszeniert vielfach Bezugnahmen auf Musik, Malerei und Literatur. Zu seinen meistbeachteten Werken gehören "Der große Irrtum/Der Konformist, Der letzte Tango in Paris, 1900 (Novecento)" sowie "Der letzte Kaiser". Diese Werke entstanden alle mit seinem langjährigen Kameramann, dem befreundeten Vittorio Storaro, der sie durch seinen spezifischen Ausleuchtungsstil mitprägte.

Die Familie wohnte außerhalb Parmas im ländlichen Dorf Baccanelli, wo der kleine Bernardo viel Kontakt mit den Kindern einfacher Bauern hatte. Seine Mutter war Lehrerin für Literatur, der Vater Attilio war ein landesweit bekannter Dichter und lehrte Kunstgeschichte. Bertolucci kam noch vor dem Schulalter mit Poesie in Berührung, die er alltäglich vermittelt bekam, denn die Gedichte des Vaters rankten sich oft um die häusliche Umgebung. Mit sechs Jahren begann der kleine Bernardo Gedichte zu schreiben. Daneben war der Vater als Filmkritiker für die "Gazzetta di Parma" tätig und fuhr regelmäßig nach Parma, um sich Filme anzusehen, über die er schreiben sollte. Oft nahm er Bernardo mit, der daraufhin „Parma“ mit „Kino“ gleichsetzte. Mit 15 Jahren drehte er zwei 16-mm-Amateurkurzfilme.

Als die Familie nach Rom umzog, verlor er die Bezugspunkte und fühlte sich entwurzelt. Er hatte es nun mit Kindern aus dem Kleinbürgertum zu tun: „Was als gesellschaftlicher Aufstieg gedacht war, erlebte ich als Herabstufung des Lebenswandels, denn schließlich haben die Bauern etwas Älterhergebrachtes und daher Aristokratischeres als das Kleinbürgertum.“ In den ersten Jahren in Rom habe er die Stadt abgelehnt. Zum Freundeskreis der Familie zählte dort der Dichter und spätere Filmemacher Pier Paolo Pasolini. Der Vater arbeitete für einen Verleger und half Pasolini 1955, dessen ersten Roman und danach auch Gedichte zu veröffentlichen. Als Belohnung für den erfolgreichen Schulabschluss durfte der junge Bertolucci 1959 einen vollen Monat in Paris verbringen, wo er öfter die Cinémathèque Française aufsuchte. Mit zwanzig habe er den Eindruck gehabt, meinte Bernardo Bertolucci, es sei selbstverständlich, Filme zu drehen. Er brach sein Studium der modernen Literatur an der Universität Rom ab und wurde Regieassistent bei Pasolinis Erstling "Accattone". Da Pasolini als Literat im Film unerfahren war und sich dessen Ausdrucksmittel erst aneignen musste, hatte Bertolucci dabei das Gefühl, der Geburt der filmischen Sprache beizuwohnen. Im Jahr darauf veröffentlichte Bertolucci den Gedichtband "In cerca del mistero" und erhielt dafür bei einem der angesehensten Literaturpreise Italiens, dem Premio Viareggio, die Auszeichnung in der Sektion für das beste Erstlingswerk. Dennoch wandte er sich endgültig von der Schriftstellerei ab.

Bernardos jüngerer Bruder Giuseppe war Theaterregisseur. Sein Vetter Giovanni ist Filmproduzent und hat diese Funktion bei einigen seiner Filme wahrgenommen. Bernardo Bertolucci lernte bei der Entstehung von "Accattone" Adriana Asti kennen, die für die nächsten Jahre seine Lebensgefährtin wurde. Sie spielte in seinem zweiten Spielfilm "Vor der Revolution" die weibliche Hauptrolle.1980 heiratete er die britische Drehbuchautorin Clare Peploe.

Seit mehreren Jahren leidet Bertolucci unter Rückenproblemen. Eine misslungene Bandscheibenoperation zwang ihn in den Rollstuhl. Ihm fiel auf, wie schlecht die Situation von Rollstuhlfahrern in Rom ist, weil auch viele öffentliche Gebäude nicht rollstuhlgängig sind. Also startete er 2012 eine Kampagne zur Verbesserung der Lage.

Wie mehrere andere bedeutende italienische Filmregisseure auch, etwa Pasolini, Visconti und Antonioni, bekannte sich Bertolucci zum Marxismus. Er erzählte, von Kindheit an Kommunist gewesen zu sein, als er viel Zeit unter Bauern verbrachte. Zunächst aus sentimentalen Gründen; als die Polizei eines Tages einen Kommunisten erschoss, habe er sich für deren Seite entschieden. Er trat jedoch erst 1968 der Kommunistischen Partei bei. Trotz der marxistischen Einstellung sprach er sich für Individualismus aus. „Die wichtigste Entdeckung, die ich nach den Ereignissen vom Mai 1968 machte, war, dass ich die Revolution nicht für die Armen gewollt hatte, sondern für mich. Die Welt hätte sich für mich ändern sollen. Ich entdeckte den individuellen Aspekt politischer Revolutionen.“ Er grenzte sich von jenen unter den Marxisten ab, die dem Volk dienen wollen, und fand, es sei dem Volk am besten gedient, wenn er sich selber diene, denn nur dann könne er Teil des Volkes sein. Kino, auch politisches, habe keine politischen Auswirkungen. Ende der Siebziger Jahre sprach er von Schuldgefühlen, sich nicht genügend am Leben der Partei zu beteiligen.

Zu Beginn seines filmischen Schaffens postulierte Bertolucci: "„Ich sehe keinen Unterschied zwischen Kino und Gedichten. Damit meine ich, dass es von der Idee zum Gedicht keine Vermittlung gibt, so wie keine zwischen einer Idee und einem Film besteht.“" Bereits die Idee selbst müsse poetisch sein, sonst könne keine Poesie entstehen. Später schwächte er diese Aussage ab, indem er postulierte, der Film sei dem Gedicht näher als dem Roman. Als er im Alter von 15 Jahren mit einer 16-mm-Kamera bei den Bauern eine traditionelle Schweineschlachtung filmte, stach der Schlachter am Herz vorbei und das toll gewordene Tier entwand sich. Es rannte blutend über den verschneiten Hof, was selbst in Schwarzweiß imposant erschien und in ihm die Überzeugung weckte, beim Drehen dem Zufall so viel Raum wie möglich zu gewähren. Die Drehbücher betrachtete er nur als Skizzen, die er beim Drehen ebenso an die Schauplätze anpasste wie an die Darsteller, die sich nicht in die geschriebenen Figuren verwandeln sollten.

Der visuelle Stil seiner Filme ist gekennzeichnet von pompös langen Kamerafahrten und sehr bewusst gestalteten Farben. Bildausschnitte und Kamerabewegungen legte er als Bestandteil seines Regiestils, seiner persönlichen „Handschrift“ jeweils selbst fest. Sein langjähriger Kameramann Vittorio Storaro, zwischen 1969 und 1993 an acht Werken beteiligt, war nur für die Beleuchtung zuständig. Typisch für Bertoluccis Erzählstil sind Bezüge zu Werken der Malerei, der Oper, der Literatur und des Films oder Zitate daraus. Der Vorspann einiger seiner Filme besteht aus der Einblendung eines oder mehrerer Gemälde. Sämtliche Filme bis 1981 enthalten eine Tanzszene, von denen manche der Handlung eine neue Wendung geben. Die Tänze drücken politische oder sexuelle Konflikte aus, dabei stehen ausgelassener Freudentaumel und strenger Regelgehorsam einander gegenüber. Häufig wird sein Stil als der Oper verwandt beschrieben, weil er deren zur Schau gestellte Melodramatik und musikalische Lyrik anklingen lässt und ganze Handlungen von Verdi-Opern inspiriert sein können. Das Singen von Arien und andere Anspielungen auf Verdi dienen als ironischer Kommentar zum Geschehen. Seine Filme fallen in die unterschiedlichsten Genres, wenn der Genre-Begriff überhaupt angemessen ist. Die Schlüsse seiner Werke sind, zum Missfallen vieler Zuschauer, meist offen, "„Rückzüge, Auflösungen ins Unwirkliche, um keinen Punkt setzen zu müssen.“" Die vordergründige Handlung ist von untergeordneter Bedeutung, sofern sie im Einzelnen überhaupt nachvollziehbar ist.

Autobiografische Elemente haben häufig Eingang in Bertoluccis Filme gefunden; dennoch entwickelte er Drehbücher oft mit Koautoren. Er gewährte unzählige Gespräche, bevorzugt mit Filmfachzeitschriften, in denen er seine Werke und seine darin verfolgten Absichten erklärte. Oft zeigte er die Einflüsse in seinen Filmen auf, solche aus der Kunst wie aus seinem persönlichen Leben und aus seiner Psychoanalyse. Er bemühte sich, als Autor seiner Filme wahrgenommen zu werden.

In Mehrdeutigkeiten, Widersprüchen und Paradoxa sieht Bertolucci etwas Positives, weil Freiheit total sein müsse, sogar wenn dies zu inkohärenten Haltungen führt. „Wir sollten gegen das arbeiten, was wir gemacht haben. Man macht etwas, dann widerspricht man dem, dann widerspricht man dem Widerspruch und so weiter. Lebendigkeit entsteht genau kraft der Fähigkeit, sich selber zu widersprechen …“ Der Titel seines einzigen veröffentlichten literarischen Werks, des Gedichtbandes "In cerca del mistero," bedeutet "Auf der Suche nach dem Geheimnis". Diese Suche zieht sich durch sein ganzes filmisches Schaffen. Er bewegt sich sowohl in der erfahrbaren Welt und wie im Traum, lotet die Beziehung zwischen Vergangenheit und Gegenwart aus, zwischen dem, was man ist, und dem, was man zu sein hat. „Er jongliert zwischen den Genres, Stilen und Theorien und mischt zu jedem Spiel die Karten aufs Neue.“ Er spielt mit allen Formen, meidet Einförmigkeit und versucht beständig Neues, das erstaunt und befremdet. Er möchte für sein Publikum möglichst unvorhersehbar sein und es überraschen, wie er auch sich selbst von Werken anderer gerne überraschen lässt. Überzeugt, dass ein Film erst durch den Zuschauer vollendet wird, hält er jede Deutung eines Werks für richtig. Bertolucci ist kulturell kosmopolitisch, was sich insbesondere in seinen internationalen Großproduktionen ausdrückt, die sich auf vier Kontinenten abspielen. „Bertoluccis Protagonisten sind Gefangene, die aus ihrer Heimat ausbrechen können, aber noch in der Fremde Gefangene ihrer Sehnsucht werden.“

In mathematischen Größen wäre sein Werk durch die Zahl 2 am besten umschrieben, geometrisch durch die Ellipse, die statt eines einzigen Zentrums deren zwei hat. Der Zuschauer, gewohnt seine Aufmerksamkeit auf einen Punkt zu richten, ist gezwungen, zwei Zentren zu beachten. Bertolucci stellt häufig Identitäten infrage; wiederholt tauchen die Motive der Schizophrenie, gespaltener Persönlichkeiten und des Doppelgängers auf. Unterschiedliche Figuren führen parallele Leben oder werden vom selben Schauspieler verkörpert. Am offensichtlichsten ist das Prinzip in "Partner," das auf Dostojewskis Roman "Der Doppelgänger" basiert. Auch in der "Strategie der Spinne" spielt derselbe Darsteller zwei Rollen, Vater und Sohn. Die beiden Figuren Olmo und Alfredo in "1900" sind am selben Tag geboren und Bertolucci unterzieht ihre parallel geschilderten Leben einem Vergleich. In den "Träumern" kommt ein Zwillingspaar vor, ein Bruder und eine Schwester, die voneinander nicht loskommen. Aber auch Ereignisse innerhalb eines Films finden oft in gewandelter Form zweimal statt. Manchmal enthalten die Bilder Spiegelungen und Wirklichkeit und Einbildung bestehen nebeneinander. In Bertoluccis Universum ist eben auch nichts einzigartig.

In Bertoluccis Universum ist nichts fest. Menschen, Gesellschaften, Situationen und die Moral sind allmählichen Verwandlungen unterworfen und vergänglich. Großen Raum nehmen Alterung und körperlicher Verfall ein. Der Tod ist nicht ein brutal eintretender Endpunkt des irdischen Lebens, sondern innerhalb des Lebens präsent als eine in zahlreiche Fragmente zerstückelte Agonie, „ein langer, keuchender Schrei.“ Bertolucci zitierte den Ausspruch, Film zeige den Tod bei der Arbeit. Grundlage aller Poesie sei das Vergehen von Zeit. In auffallend vielen Titeln seiner Filme kommt die Zeit vor, am offensichtlichsten in "Vor der Revolution" und "1900". Der Mond in "La Luna" ist für die Gezeiten verantwortlich. Es gibt den "Letzten Tango" und den "Letzten Kaiser," und der Tod ist präsent in "Todeskampf" und "La Commare Secca".

Seine Figuren unterliegen oft einem Determinismus und können der Vergangenheit nicht entkommen. Es sind zum Beispiel junge Menschen, die aus ihrer Familie und sozialen Klasse ausbrechen wollen, aber unweigerlich nach den Mustern der Vergangenheit leben. Ihnen steht eine männliche Autoritätsfigur gegenüber, die gemischte Gefühle hervorruft und die sie letztlich ablehnen. Die aufbegehrenden Söhne erringen nur vorübergehend Siege, die Strategien der Väter überdauern. Der Vatermord taucht als Motiv mehr oder minder offen in fast allen Filmen der ersten Schaffenshälfte auf.

Bertolucci hatte es in jungen Jahren mit mehreren Vaterfiguren zu tun, die auf sein Leben und Werk Einfluss hatten, von dem er sich befreien wollte. Sein Vater Attilio Bertolucci war ein in Italien bekannter Dichter. Zwar veröffentlichte Bernardo Bertolucci einen eigenen Gedichtband, doch blieb dieser sein einziges literarisches Werk. Nach eigenen Angaben wollte er mit dem Vater wettstreiten, doch er spürte, dass er den Kampf verlieren würde. Daher wich er auf ein anderes Gebiet aus, das Kino. Ein anderes Mal begründete er das Ende seiner dichterischen Arbeit damit, nicht dasselbe in Gedichten und in Filmen mitteilen zu wollen.

Die zweite Figur war Pier Paolo Pasolini. Er wohnte einige Jahre im selben Gebäude; der junge Bernardo war mit ihm befreundet und las ihm regelmäßig seine Gedichte vor; es entwickelte sich eine Beziehung wie zwischen Lehrer und Schüler. Diese setzte sich fort, als ihn Pasolini 1961 zu seinem Regieassistenten ernannte. Stilistisch blieb Pasolinis Einfluss bescheiden.

Die künstlerisch beherrschende Gestalt war der französische Filmregisseur Jean-Luc Godard. Die Filme aus Bertoluccis ersten zwei Jahrzehnten sind von einer ästhetischen und politischen Auseinandersetzung mit Godard und seinem Werk geprägt. Während er Godards Einfluss in "Vor der Revolution" und "Partner" feierte, verwirklichte er in der "Strategie der Spinne" einen eigenen Stil und schritt darauf im Film "Der große Irrtum" zum symbolischen Vatermord. Im "Letzten Tango in Paris" schließlich karikierte er Godards und seine eigene Kinomanie in der Figur des Jungfilmers Tom. Godard radikalisierte sich Ende der 1960er Jahre politisch wie filmästhetisch und stand Bertoluccis Hinwendung zu konventionelleren Großproduktionen ablehnend gegenüber.

Bertolucci nannte Ende der 1960er Jahre Pasolini und Godard als seine bevorzugten Regisseure, zwei große Poeten, die er sehr bewundere, und daher wolle er gegen beide anfilmen. Denn um Fortschritte zu erzielen und um anderen etwas geben zu können, müsse man mit jenen kämpfen, die man am meisten liebe.

Bertolucci thematisiert häufig die Unterdrückung von Söhnen durch Väter beziehungsweise durch das Patriarchat. Im Mittelpunkt des narrativen und emotionalen Geschehens stehen männliche Figuren, während die weiblichen Rollen Ableitungen männlicher Ängste und Hassgefühle sind. Es sind oft niedere oder gar destruktive Rollen, die meist eine sexualisierte Funktion innerhalb der Erzählung einnehmen. An der Figur Caterina in "La Luna" offenbarte Bertolucci seine Sicht, dass sich kreative Arbeit und Muttersein nicht erfolgreich vereinbaren lassen. Die Frauen haben in der Regel keine intellektuellen Interessen und leben für Instinkte und sinnliches Vergnügen. Es sind die männlichen Figuren, die leiden und diese Leiden geistig verarbeiten.

Die ersten Jahre seines filmischen Schaffens waren geprägt von der Suche nach einem eigenen Stil und eigenen Themen. Im Alter von nur 21 Jahren erhielt Bertolucci überraschend die Gelegenheit, einen ersten Film zu drehen. Nach dem Erfolg von Pasolinis Film "Accattone" war der Produzent darauf aus, eine seiner Kurzgeschichten zu verfilmen. Da sich Pasolini bereits dem Projekt Mamma Roma zugewandt hatte, beauftragte der Produzent mit der Ausarbeitung des Stoffes zu einem Drehbuch einen anderen Autor und Bertolucci, dem das dargestellte Milieu römischer proletarischer Kleinkrimineller fremd war. Dennoch bemühte er sich, den Erwartungen des Produzenten nach einem „pasolinesken“ Produkt gerecht zu werden. Nach Fertigstellung bot ihm der Produzent auch die Regie an. Bertolucci versuchte, "La Commare Secca" (1962) zu „seinem“ Film zu machen durch einen eigenen Stil, der von Pasolini unbeeinflusst sei. Besonders auffällig sind die zahlreichen Kamerafahrten, von denen der mitwirkende Kameramann behauptete, er hätte noch in keinem Film so viele durchzuführen gehabt.

In dieser Zeit sah er sich nie ganz dem italienischen Kino zugehörig; näher stand er der französischen Kinematografie, die er am interessantesten fand. Mit der Presse sprach er bevorzugt auf Französisch, weil es die Sprache des Kinos sei. Bei seinem zweiten Film "Vor der Revolution" (1964), produziert von einem filmbegeisterten Mailänder Industriellen, konnte er ein eigenes, persönliches Thema behandeln: die Schwierigkeit eines Intellektuellen bürgerlicher Herkunft, gleichzeitig Marxist und für die proletarischen Massen dazusein, die Angst, auf das eigene Milieu zurückgeworfen zu werden, weil die Wurzeln so stark seien. Er habe für diese Angst vor der eigenen Feigheit noch keine Lösung gefunden, erklärte er später, die einzige Möglichkeit sei, sich der Dynamik und "„unglaublichen Vitalität“" des Proletariats anzuschließen, der echten revolutionären Kraft auf der Welt. Der Stil verrät stilistische Einflüsse von Godard und Antonioni. Das Werk wurde in Italien kaum aufgeführt und erntete dort meist ablehnende Kritiken, während das Echo der internationalen Filmkritik besser war. Beim Dreh begegnete er erstmals Vittorio Storaro, der als Kameraassistent mitwirkte. Storaro hatte den Eindruck, dass Bertolucci ein ungeheures Wissen hatte, besonders für jemanden in seinem Alter, jedoch auch viel Überheblichkeit zeigte.

In den folgenden Jahren lehnte Bertolucci immer wieder Angebote ab, Italo-Western zu drehen. Er wollte keine Kompromisse eingehen und nicht wie manch anderer Regisseur Filme drehen, an die er nur halb glaubte. So vergingen mehrere Jahre, ohne dass er einen langen Spielfilm verwirklichen konnte. Aus dieser Zeit zu verzeichnen sind lediglich die Mitarbeit am Drehbuch von "Spiel mir das Lied vom Tod," eine Fernsehdoku über den Öltransport sowie der Kurzfilm "Todeskampf," ein Beitrag zum Episodenfilm "Liebe und Zorn".

Das filmische Ideal, das er in den 1960er Jahren verfolgte, war ein Kino, das vom Kino handelte, die eigene Sprache reflektierte und erneuerte, im Geiste Godards. Die Filme sollten sich ihrer selbst bewusst sein und das Publikum, das von Film nichts verstehe, lehren. Seine Drehbücher verfasste er damals zusammen mit Gianni Amico, mit dem er die Weltanschauung teilte und der ihn in einer cinéphilen, experimentellen, das gewöhnliche Publikum wenig berücksichtigenden Arbeitsweise bestärkte. So entstand auch "Partner," wo er inhaltlich und formal Godard imitierte. Der wirre Film stieß auf wenig Verständnis; recht bald distanzierte er sich vom "„neurotischen“, „kranken“" Nebenwerk. Später fand er, dass seine intensive Beschäftigung mit Filmsprache ihn zu stilistischer Arroganz verleitet hatte und dass er nach "Partner" ein großes Verlangen nach Publikum für seine Filme verspürte. Er stürzte in eine tiefe Depression und begann 1969 eine Psychoanalyse.

Diese wurde zu einem kreativen Motor und half ihm, sich von den „kranken Theorien“, wie er sie nannte, zu befreien, die sein frühes Schaffen stilistisch geprägt haben. Zwischen der Analyse und der Arbeit bestand eine Ersatzbeziehung: „Wenn ich einen Film drehe, fühle ich mich wohl und brauche keine Analyse.“ Eine erste Frucht seiner neuen Methode war "Die Strategie der Spinne," die er 1969 fürs italienische Fernsehen drehte. Erstmals war Vittorio Storaro für das Licht verantwortlich; Bertolucci sollte fortan für über zwei Jahrzehnte mit ihm arbeiten und sie verband eine enge Freundschaft. Die beiden entwickelten eine unverwechselbare Bildsprache, Bertoluccis spezifischer visueller Stil kam erstmals unverfälscht von Godards Einfluss zur vollen Entfaltung. Dabei befasste er sich mit seinen bevorzugten Themen wie der Verknüpfung von Geschichte und Geschichtsschreibung mit der Gegenwart, dem Ringen des Sohnes mit dem Vater, gespaltenen Identitäten, Faschismus und Widerstand.

Mit dem "Großen Irrtum," auch bekannt als "Der Konformist," betrat Bertolucci die Bühne der internationalen Großproduktionen. Der Film wurde in Frankreich und Italien in geschichtlichen Kulissen aufwendig gedreht, mit dem Star Jean-Louis Trintignant in der Hauptrolle. Wieder lotete er Italiens faschistische Vergangenheit aus und konstruierte einen psychosexuellen Erklärungsansatz für das Handeln der Hauptfigur. Das Werk zeichnet sich durch eine zeitlich fragmentierte Handlung und einen visuellen Stil aus, der inhaltliche Aussagen mit Hilfe des eingesetzten Lichts unterstreicht. Der große Erfolg etablierte Bertolucci als Filmemacher von Weltrang. Mit dem "Großen Irrtum" begann Bertoluccis Zusammenarbeit mit Franco Arcalli, die bis zu dessen Tod 1978 andauerte. Dank Arcalli erkannte er, dass er mit dem Schnitt neue Sichtweisen gewinnen kann. Da er Arcalli als stimulierend und ideenreich erlebte, der Schnitt gleichsam zu einer Revision des Drehbuchs geriet, avancierte Arcalli auch zum Drehbuchautor.

Einen weiteren Höhepunkt erreichte Bertolucci mit dem Drama "Der letzte Tango in Paris" (1972), mit Marlon Brando in der Hauptrolle. Der nur vordergründig unpolitische Film schildert den Daseinsschmerz eines Mannes im mittleren Alter und sein Leiden an der Unterdrückung des Lustprinzips in der westlichen Kultur. Wegen seiner drastischen, unverblümten Darstellung von Sex war der Film ein beherrschendes Gesprächsthema. Die Skandalisierung und Zensurversuche bewirkten einen enormen Kassenerfolg, in den USA blieb er auf Jahre hinaus der einträglichste europäische Film.

Mit diesen Erfolgen im Rücken konnte Bertolucci ein Vorhaben in Angriff nehmen, das oft als größenwahnsinnig beschrieben wird: die Erzählung eines halben Jahrhunderts italienischer Geschichte in dem über fünfstündigen "1900". Von den US-amerikanischen Filmstudios war der Film auf sieben Millionen US-Dollar veranschlagt, und am Ende kostete er über acht Millionen. Obwohl ganz in Italien angesiedelt, werden mehrere Rollen von internationalen Stars gespielt, darunter Burt Lancaster, Gérard Depardieu, Donald Sutherland, Dominique Sanda und Robert De Niro. Es ist ein Hohelied auf die bäuerliche Schicht und auf den Kommunismus. Der Film überraschte die Kritik mit seiner konventionellen Form; Bertolucci wählte bewusst eine ans Massenkino angelehnte einfache und emotionale Filmsprache, um seine Botschaft breiter streuen zu können. Wegen des politischen Gehalts wollten die Studios "1900" in den Vereinigten Staaten nicht wie vorgesehen in den Vertrieb nehmen. Um die zu veröffentlichende Fassung kam es zu einem auch gerichtlich ausgetragen Streit zwischen Bertolucci, dem Produzenten Grimaldi und den Studios. Diese setzten für die US-Version starke Kürzungen durch, werteten den Film dort kaum aus, und bei der US-Kritik stieß das Werk als Propaganda auf völlige Ablehnung. In Europa lief es nur leicht gekürzt. Bertolucci erkannte später den Widerspruch zwischen den kapitalistischen, multinationalen Produktionsbedingungen und der naiven, lokal verwurzelten utopischen Vision.

Mehrere geplante Filmprojekte, darunter die Verfilmung von Dashiell Hammetts Roman "Red Harvest," scheiterten daran, dass er Produzenten nicht von ihnen überzeugen konnte. Er wollte seinen ersten amerikanischen Film auf Basis eines Klassikers der US-Literatur machen. Das Projekt kam für ihn nur als politischer Film infrage, die amerikanischen Produzenten hatten jedoch kein Verständnis für seine Lesart des Romans. Bei der Dimensionierung seiner nächsten beiden Filmprojekte musste Bertolucci viel bescheidener sein. Das Drama "La Luna" (1979) um eine inzestuöse Mutter-Sohn-Beziehung entstand mit einem wesentlich kleineren Budget und wenigen Darstellern. Gedreht wurde vor allem in Rom, teilweise auch in Parma und New York, und in den Hauptrollen ist es mit US-amerikanischen Darstellern besetzt. Bertolucci hat darauf hingewiesen, dass es in diesem Werk kein ständiges Fragen nach dem Wesen von Film und Kino mehr gibt; "La Luna" schäme sich nicht des Vergnügens. Doch genau dieses Beharren auf einem konventionellen, konsumierbaren Erzählstil enttäuschte die Kritik. Noch stärker auf seine Heimatregion um Parma konzentriert blieb er in der "Tragödie eines lächerlichen Mannes" (1981), wo er die nebulöse politische Lage in Italien in den Vordergrund rückte. Bis auf Anouk Aimée mit Italienern besetzt und in italienischer Sprache, war es innerhalb des Vierteljahrhunderts seiner Zusammenarbeit mit Storaro der einzige Streifen, für den er einen anderen Kameramann beizog. Dem metafilmischen Diskurs ließ er so viel Raum wie seit "Partner" nicht mehr und verweigerte dem Publikum eine nachvollziehbare Handlung und die Identifikation mit einer Figur. Das Werk rief bei der Kritik sehr unterschiedliche Bewertungen hervor – einige stellten eine resignative Haltung fest – und fand beim Publikum kaum Beachtung. Manche Filmpublizisten, die sein ganzes Werk überblicken, meinen, die "Tragödie" sei das möglicherweise am meisten unterschätzte Werk Bertoluccis.

Zu Beginn der 1980er Jahre zeigte sich Bertolucci sehr enttäuscht von Italien und dem politischen System. Er verlor auch das Interesse an der Gegenwart. Das führte ihn auf die Suche nach etwas ganz Anderem und die Besonderheiten nicht-westlicher Kulturen. Als Ergebnis drehte er, wieder mit Storaro, drei Filme, in denen er seine Themen vor dem Hintergrund Chinas, Nordafrikas und des Buddhismus behandeln konnte. Diese drei Filme haben gemein, dass der Regisseur gegenüber Vaterfiguren versöhnlicher geworden war, die nun als Lehrer und Weise erschienen. Das erste Projekt war "Der letzte Kaiser" (1987), bei dem er als cineastischer Marco Polo das Leben des letzten chinesischen Kaisers Pu Yi nachzeichnete – zum ersten Mal hatte es die chinesische Regierung einer westlichen Filmproduktion erlaubt, in der Verbotenen Stadt zu drehen. Mit seinem Erfolg bei Kritik und Publikum stellt das mit Auszeichnungen überhäufte Geschichtsdrama einen weiteren Höhepunkt in Bertoluccis Schaffen dar. Der darauffolgende Film "Himmel über der Wüste" (1990) führte ihn nach Marokko und in die Sahara und ist mit den Stars Debra Winger und John Malkovich besetzt. Er handelt von einem Paar, das vor der westlichen Zivilisation flüchtet und in der Begegnung mit dem Fremden und der Sahara seine Identität verliert. Die Kritik war sich nicht einig, wie gut Bertolucci Themen und Figuren des Romans von Paul Bowles getroffen hat; doch selbst einige der negativen Kritiken bescheinigten dem Werk, sein Blick auf die Landschaften und Städte Nordafrikas sei sinnlich und überwältigend.

Die dritte der „exotischen“ Produktionen ist "Little Buddha" aus dem Jahr 1993, mit dem sich Bertolucci an Kinder jeden Alters wenden wollte. Es ergründet Buddhismus und Reinkarnation. Viele Kritiker empfanden das Werk als zu einfach und vermissten die Vielschichtigkeit seiner bisherigen Werke. Es war sein erster Film, der nicht um einen Konflikt politischer, psychologischer oder zwischengeschlechtlicher Art gebaut ist. Der Regisseur bezeichnete sich als skeptischen Amateur-Buddhisten, als nur an der ästhetischen und poetischen Seite dieser Philosophie interessiert. Nach dem Untergang der kommunistischen Utopie des gewohnten Raums für Träume verlustig gegangen, fand er im Buddhismus ein neues inspirierendes Feld. Die Umstellung sei ihm leichtgefallen, denn der Buddhismus habe mit Marx und Freud gemein, dass sie alle nicht Gottheiten, sondern den Menschen in den Mittelpunkt stellen.

Als er die fünfzig überschritten hatte, waren Konflikte mit Vaterfiguren kein wichtiger Bestandteil mehr. Die Themen seiner früheren Filme kommen in "Gefühl und Verführung" (1995) zwar in Gestalt melancholisch zurückgezogener 68er vor, doch die Hauptfigur ist ein junges unerfahrenes Mädchen. Sein nächstes Werk, das Kammerspiel "Shandurai und der Klavierspieler" (1998), fand bei der Kritik viel Zustimmung, doch in Deutschland zunächst keinen Verleih. Es gelangte erst in die Kinos, als das Interesse an Bertolucci mit dem kommerziellen Erfolg der "Träumer" 2003 wieder erwachte. Die Träumer sind drei junge Menschen, die in Paris 1968 eine Dreierbeziehung leben und sich von den Ereignissen auf der Straße abkapseln. Bertoluccis Spätwerke ernteten bei einem Teil der Kritik den Vorwurf, Film gewordene Altherrenfantasien zu sein, bei denen er sich an den Körpern junger Frauen delektierte.

Nach den "Träumern" konnte er fast zehn Jahre lang keinen weiteren Film vollenden. Das Projekt "Bel Canto," die Verfilmung des gleichnamigen Romans von Ann Patchett über eine Geiselnahme in Südamerika, musste auf unbestimmte Zeit verschoben werden. 2007 wandte er sich einem anderen Vorhaben zu, einem Drama über den italienischen Musiker und Mörder Carlo Gesualdo, der im 16. Jahrhundert lebte. Erst 2012 wurde mit "Io e te" seine erste Spielfilmregiearbeit nach zehn Jahren Pause außer Konkurrenz bei den Filmfestspielen von Cannes uraufgeführt.

→ "Erweiterte Filmografie"


Darüber hinaus hat Bertolucci einige Kurz- und Dokumentarfilme gedreht.

Bertolucci erhielt zwei Oscars und zwei Golden Globes für "Der letzte Kaiser" (1987). Zudem erhielt er je eine Oscarnominierung für "Der große Irrtum" (1970) und "Der letzte Tango in Paris" (1972). Zum 75-jährigen Bestehen des Filmfestivals von Venedig erhielt Bernardo Bertolucci 2007 einen Goldenen Ehrenlöwen für sein Lebenswerk  als „vielleicht berühmtester italienischer Regisseur der Gegenwart, und einer der wichtigsten und einflussreichsten in der Geschichte des Kinos.“ 2011 wurde ihm die Ehrenpalme der Filmfestspiele von Cannes zuerkannt, ein Jahr später der Europäische Filmpreis für ein Lebenswerk. Bereits 2008 wurde für ihn ein Stern auf dem Hollywood Walk of Fame angebracht, den er jedoch erst im November 2013 einweihen konnte.




</doc>
<doc id="484" url="https://de.wikipedia.org/wiki?curid=484" title="Liste von Videofachbegriffen">
Liste von Videofachbegriffen

Alphabetische Aufstellung der im Videobereich verwendeten Fachbegriffe mit deren Abkürzung.
























</doc>
<doc id="485" url="https://de.wikipedia.org/wiki?curid=485" title="Internationale Filmfestspiele Berlin">
Internationale Filmfestspiele Berlin

Die Internationalen Filmfestspiele Berlin, kurz Berlinale, sind ein jährlich in Berlin stattfindendes Filmfestival. Sie zählen neben denen von Cannes und Venedig zu den wichtigsten Filmfestivals und als eines der weltweit bedeutendsten Ereignisse der Filmbranche.

Die 68. Berlinale fand vom 15. bis 25. Februar 2018 statt. 2019 soll die 69. Auflage vom 7. bis 17. Februar 2019 ausgerichtet werden.

Die im Wettbewerb erfolgreichen Filme werden von einer internationalen Jury mit dem Goldenen und den Silbernen Bären ausgezeichnet. Mehr als 400 Filme werden in verschiedenen Sektionen präsentiert. Mit mehr als 325.000 verkauften Eintrittskarten und etwa 490.000 Kinobesuchern insgesamt (inklusive akkreditierter Fachbesucher) ist die Berlinale das größte Publikumsfestival der Welt. Rund 16.000 Fachbesucher aus 130 Ländern nehmen an dem Festival teil. Etwa 3.700 Journalisten aus mehr als 80 Ländern berichten über die Zeit der Festspiele.
Während der Berlinale findet zeitgleich der European Film Market (EFM) statt. Der EFM gehört zu den international wichtigsten Treffen der Filmindustrie und hat sich zu einem bedeutenden Marktplatz für Produzenten, Verleiher, Filmeinkäufer und Co-Produktionsagenten etabliert.

Gegründet wurde die Berlinale im Jahr 1951. Festivaldirektor ist seit 2001 Dieter Kosslick. Seit 2002 sind die Internationalen Filmfestspiele Berlin ein Geschäftsbereich der "Kulturveranstaltungen des Bundes in Berlin GmbH". Sie erhalten eine institutionelle Förderung der Beauftragten der Bundesregierung für Kultur und Medien.

Die Berlinale findet seit 1951, zunächst im Sommer, seit 1978 im Februar, in Berlin statt. Sie geht auf eine Initiative von Oscar Martay zurück. Martay war Film Officer der Militärregierung der Vereinigten Staaten und beaufsichtigte und förderte in dieser Funktion die Berliner Filmindustrie, unter anderem mit mehreren Darlehen der amerikanischen Militärregierung, mit denen die Finanzierung der Filmfestspiele in den ersten Jahren sichergestellt wurden. Unter dem Motto „Schaufenster der freien Welt“ eröffnete die erste Berlinale am 6. Juni 1951 mit Alfred Hitchcocks "Rebecca" im "Titania-Palast". Zum ersten Festspielleiter wurde der Filmhistoriker Alfred Bauer berufen, der nach dem Krieg die britische Militärregierung in Filmangelegenheiten beraten hatte. Im Ostteil der Stadt gab es als Reaktion auf die Berlinale das Festival des volksdemokratischen Films, auf dem hauptsächlich Filme aus dem damaligen Ostblock gezeigt wurden. Dieses Filmfest fand ebenfalls erstmals 1951, eine Woche nach dem Ende der Berlinale statt.

Seit der ersten Berlinale wird der – nach einer Vorlage der Bildhauerin Renée Sintenis gestaltete – Goldene Berliner Bär verliehen. Die Preisträger wurden in den ersten Jahren teilweise durch Publikumswahl bestimmt. Nachdem die FIAPF (Fédération Internationale des Associations de Producteurs de Films) die Berlinale offiziell mit den Festivals in Cannes, Venedig und Locarno gleichgestellt hatte, änderte sich dies entsprechend den FIAPF-Richtlinien: Die Berlinale wurde zu einem "A-Festival" und berief 1956 erstmals eine internationale Jury ein, die den „Goldenen“ und die „Silbernen Bären“ vergab. Die frühe Berlinale war vor allem ein Publikums- und Glamour-Festival, auf dem sich zahlreiche Filmstars präsentierten (etwa Gary Cooper, Sophia Loren, Jean Marais, Richard Widmark, Jean Gabin, Michèle Morgan, Henry Fonda, Errol Flynn, Giulietta Masina, David Niven, Cary Grant, Jean-Paul Belmondo und Rita Hayworth).

Die Ausrichtung des Festivals änderte sich ab Ende der 1960er Jahre auch aufgrund der gesellschaftlichen und politischen Polarisierung. So kam es etwa auf der Berlinale 1970 durch den Vietnamkriegs-Film "o.k." von Michael Verhoeven zu heftigem Streit, so dass die Jury zurücktrat und das Wettbewerbsprogramm abgebrochen wurde. Auf der Berlinale 1971 wurde daraufhin neben dem traditionellen Wettbewerb mit dem "Internationalen Forum des jungen Films" eine ehemalige Gegenveranstaltung in das Festival integriert, das junge und progressive Filme vorstellen sollte. Durch die Veränderungen Infolge der Ostpolitik Willy Brandts, die mit einer kulturellen Öffnung der Ostblockstaaten einherging, kamen 1974 mit „Hundert Tage nach der Kindheit“ von Sergej Solowjow und 1975 mit „Jakob der Lügner“ zum ersten Mal ein sowjetischer und ein DDR-Film ins Programm.
1976 wurde der bisherige Festivalleiter Alfred Bauer durch den Filmpublizisten Wolf Donner abgelöst. Donner führte zahlreiche Änderungen und Modernisierungen des Festivals ein, etwa die Verlegung vom Sommer in den Winter. Einer der Gründe für diese Änderung war der Termin der damaligen "Filmmesse" (des heutigen "European Film Market"), der sich im Winter weniger mit den Terminen anderer Filmmärkte überschnitt. Donner etablierte neue Sektionen wie die "Deutsche Reihe" und das "Kinderfilmfest", die ehemalige "Informationsschau" wurde zum "Panorama" in seiner heutigen Form. Seit Donners Zeit gilt die Berlinale vor allem als „Arbeitsfestival“ und weniger als Bühne für Stars und „Sternchen“.

Wolf Donner wurde 1979 durch Moritz de Hadeln abgelöst, der die Berlinale bis 2001 leitete. Seit 2000 ist das Theater am Potsdamer Platz mit 1800 Sitzplätzen Hauptspielstätte. Während der Berlinale wird das Theater in "Berlinale Palast" umbenannt. Neben den Filmpremieren der Wettbewerbsfilme findet hier auch der Eröffnungsfilm und die Preisverleihung statt.

Die Berliner Filmfestspiele werden seit dem 1. Mai 2001 von Dieter Kosslick geleitet. Auch unter Kosslick gab es einige Veränderungen: So wurde die neue Reihe "Perspektive Deutsches Kino" eingeführt, 2003 entstand für die Nachwuchsförderung der "Berlinale Talent Campus" und zur Berlinale 2007 wurde mit den "Berlinale Shorts" eine weitere neue Sektion vorgestellt.

Der Wettbewerb ist die zentrale Sektion der Filmfestspiele; im Wettbewerbsprogramm werden die Hauptpreise – der Goldene Berliner Bär und die Silbernen Bären – verliehen. Im Wettbewerb werden (entsprechend den FIAPF-Richtlinien) ausschließlich Filme gezeigt, die innerhalb der letzten 12 Monate vor Festivalbeginn produziert und noch nicht außerhalb ihrer Ursprungsländer aufgeführt wurden.

Etwa 20 Filme stehen jedes Jahr im Wettbewerb. Die Nominierung der Filme sowie die Auswahl der Jurymitglieder ist Aufgabe der Festivaldirektion. Die Preisträger werden von einer internationalen Jury unter Führung eines Jury-Präsidenten gewählt und zum Ende des Festivals verkündet. Aktuelle Spielstätten des Wettbewerbs sind u. a. der Berlinale Palast am Potsdamer Platz sowie die Kinos CinemaxX, Kino International und das Haus der Berliner Festspiele. 2009 kam der Friedrichstadtpalast als Spielstätte hinzu. Jury-Präsidentin der Berlinale 2009 war die Schauspielerin Tilda Swinton. 2010 hatte dieses Amt der Regisseur Werner Herzog inne, gefolgt von Isabella Rossellini (2011), Mike Leigh (2012), Wong Kar-Wai (2013), Produzent und Drehbuchautor James Schamus (2014), Darren Aronofsky (2015), Meryl Streep (2016) und Paul Verhoeven (2017).

Das "Internationale Forum des Jungen Films" (kurz: "Forum") findet seit 1971 statt; der inhaltliche Schwerpunkt liegt traditionell im Bereich des politisch engagierten Kinos. Das "Forum" geht zurück auf eine Initiative des von Gero Gandert gegründeten 1963 Vereins "Freunde der Deutschen Kinemathek". Die Gründer waren Gero Gandert, Erika Gregor, Ulrich Gregor, Heiner Roß und Manfred Salzgeber. Ulrich Gregor war von 1971 bis 1979 der Sprecher des Forums, ab dann Leiter für 20 Festivals. Das Forum bot jungen Regisseuren wie Raúl Ruiz, Derek Jarman und Peter Greenaway eine erste Gelegenheit, sich international zu präsentieren. Es gibt auch Filmen mit ungewöhnlichen Formaten eine Plattform, so etwa den überlangen Produktionen "Taiga" von Ulrike Ottinger (8 Stunden 21 Minuten) oder "Satanstango" von Béla Tarr (7 Stunden 16 Minuten).

Einen weiteren Schwerpunkt des Forums bildet der außereuropäische Film. In den 1970er/80er Jahren konzentrierte man sich auf US-Independentfilme, Filme aus Lateinamerika und internationale Avantgarde-Filme. In den 1980er/90er Jahren widmete man sich dem unabhängigen Kino Asiens. Leiter des Forums ist seit 2001 der Berliner Filmjournalist Christoph Terhechte. "„Das Internationale Forum, immer noch das wichtigste Nebenprogramm der Berliner Filmfestspiele, ist für die Neugierigen unter den Cineasten schon seit Jahren zu deren Hauptprogramm geworden.“" (Peter W. Jansen). Spielstätten des Forums sind die Kinos "Delphi-Palast", "Arsenal" (mittlerweile am Potsdamer Platz), "CineStar", "CinemaxX" und "Cubix", sowie die Botschaft von Kanada. 2015 wird erstmals das Gesamtprogramm von "Forum Expanded" in der Akademie der Künste gezeigt, nachdem sie bis 1999 schon einmal Spielort der Berlinale war.

Die filmhistorische Retrospektive wird seit 1951 durchgeführt und seit 1977 in deutlich nichtkommerzieller Intention von der "Stiftung Deutsche Kinemathek" (heute: "Filmmuseum Berlin – Deutsche Kinemathek)" organisiert. Im Rahmen der "Retrospektive" wird jährlich eine "Hommage" veranstaltet. Leiter der "Retrospektive" ist Rainer Rother. Aktuelle Spielstätten der Retrospektive sind das "CinemaxX" und das "Zeughauskino".

Verzeichnis der Retrospektiven:


Verzeichnis der Hommagen:


Das "Panorama" gehört zum offiziellen Programm der Berlinale und wird seit 1986 veranstaltet. Vorläufer war in der Anfangszeit der Berlinale die so genannte "Informationsschau". Leiter war zunächst Manfred Salzgeber, der 1992 von Wieland Speck abgelöst wurde. Schwerpunkte sind das Arthouse-Kino und der Autorenfilm, alle Filme werden als Welt- oder Europa-Premiere gezeigt. Das Hauptprogramm bietet jährlich etwa 18 Spielfilme, zahlreiche weitere Produktionen bilden das Rahmenprogramm. Subsektionen sind die Reihen "Dokumente", "Panorama Special" und "Panorama-Kurzfilme". Inhaltlich widmet sich das Panorama eher gesellschaftlichen als direkt politischen Themen: So werden traditionell viele schwul-lesbische Filme gezeigt. Spielstätten des Panoramas sind das "CinemaxX", das "Kino International" und das "CineStar".

Seit 1978 widmet die Berlinale eine Sektion speziell Kindern und Jugendlichen und zeigt dort eine aktuelle Auswahl internationaler Spiel- und Kurzfilme. Die Sektion gilt in diesem Bereich als eine der wichtigsten Markt- und kulturellen Plattformen weltweit. Die Sektion wurde 2004 durch den Jugendfilmwettbewerb "14plus" ergänzt. Der frühere Name „Kinderfilmfest“ wurde zur Berlinale 2007 in "Generation" umbenannt. Entsprechend heißen die Wettbewerbe heute "Generation Kplus" und "Generation 14plus".

Im Wettbewerb "Generation Kplus" verleiht eine elfköpfige Kinderjury den Gläsernen Bären an je einen Spiel- und einen Kurzfilm. Eine Internationale Jury von Filmfachleuten vergibt zudem die mit Geld dotierten Preise des Deutschen Kinderhilfswerks. Den Gläsernen Bären für den besten Spielfilm im Wettbewerb "Generation 14plus" vergibt eine siebenköpfige Jury von Jugendlichen.

Die Devise, keinen „Kinderkitsch“ zu zeigen, bedeutet, anspruchsvolle Filme aus der ganzen Welt ins Programm aufzunehmen, die nah am Alltag und am Erleben von Kindern und Jugendlichen bleiben und auch eine Realität abbilden, die manchmal harter Tobak ist.
Mit ihrem Programm möchte die Sektion Generation für ein erweitertes Verständnis von Filmen für junge Menschen werben. Die Wettbewerbsbeiträge beschränken sich nicht auf klassische Kinder- oder Jugendfilmproduktionen; sie schließen vielmehr auch Filme ein, die zwar nicht für diese Zielgruppen konzipiert wurden, aber Kinder oder Jugendliche thematisch und formal ansprechen.

Im Rahmen von "Cross Section" werden seit 2006 für Kinder und Jugendliche geeignete Filme aus den anderen Sektionen der Berlinale außer Konkurrenz in „Generation“ wiederholt und so einem Publikum unter 18 Jahren zugänglich gemacht. Seit 2008 wird "Generation" von Maryanne Redpath und Florian Weghorn geleitet. Hauptspielstätte von "Generation Kplus" war der "Zoo Palast". Seit 2012 ist das Haus der Kulturen der Welt die Hauptspielstätte von "Generation 14plus". Weitere Vorstellungen gibt es in den Kinos "Colosseum" (Prenzlauer Berg), im "Filmtheater am Friedrichshain" sowie im "Cinemaxx" am Potsdamer Platz.

"Berlinale Special" ist eine 2004 neu eingeführte Reihe im offiziellen Programm, in der sowohl aktuelle Arbeiten großer Filmemacher als auch Wiederaufführungen von klassischen Werken der Filmgeschichte und Produktionen zu Festivalschwerpunkten oder aktuell-brisanten Themen gezeigt werden sollen. Aufführungsort ist der Friedrichstadt-Palast.

Die unter Dieter Kosslick eingeführte Sektion "Perspektive Deutsches Kino" widmet sich der aktuellen deutschen Filmproduktion und ergänzt die geschlossene Reihe "German Cinema"; gezeigt werden etwa ein Dutzend Spiel-, Dokumentar- und Experimentalfilme, die aus ca. 250 Bewerbungen ausgewählt werden. Leiterin ist Linda Söffker.

Im Jahr 2006 wurde mit den "Berlinale Shorts" eine eigene Sektion für kurze Filme eingerichtet. Je ein Goldener und ein Silberner Bär werden seit 1955 an die besten kurzen Filme vergeben, seit 2003 von einer eigens dafür einberufenen internationalen Jury. In den "Berlinale Shorts" laufen jährlich ca. 30 Kurzfilme, die um die Bären konkurrieren. Die Sektion wird seit 2007 von Maike Mia Höhne geleitet. Kurzfilme und mittellange Filme sind darüber hinaus in den Sektionen "Generation", "Perspektive Deutsches Kino" und im "Forum expanded" zu sehen.

2015 wurde die Berlinale zum ersten A-Festival weltweit, das eine eigene Reihe für Fernsehserien ins offizielle Programm einführte. Zuvor war es bereits ab 2010 zu Premieren von Serienformaten (u. a. "Top of the Lake", "Im Angesicht des Verbrechens") gekommen. Im Rahmen der "Berlinale Special Series" kam es u. a. mit "Better Call Saul", "The Night Manager" oder "4 Blocks" zu Welt- oder internationalen Premieren. 2018 wurde die Reihe in "Berlinale Series" umbenannt und hat mit dem Zoo Palast eine veränderte zentrale Spielstätte erhalten.


Der "European Film Market (EFM)" ist wichtiger Handelsplatz für Produzenten, Verleiher, Filmeinkäufer und Co-Produktionsagenten. Als erster Filmmarkt im Jahr gehört der "EFM" neben dem "Marché du Film" in Cannes im Mai und dem "American Film Market" im November zu den drei bedeutendsten Branchentreffen der Filmindustrie.

Der "EFM" ist die Nachfolgeveranstaltung der "Filmmesse". Sie wurde von 1980 bis 1987 von Aina Bellis geleitet; 1988 übernahm Beki Probst die Leitung für diese im Sinne der Filmwirtschaft an die Berlinale angegliederte Veranstaltung. Seit 2014 leitet der Niederländer Matthijs Wouter Knol den European Film Market. Die langjährige EFM-Direktorin Beki Probst steht dem EFM dabei als Präsidentin vor. Veranstaltungsort des "EFM" waren bis 2000 die Räume in der Budapester Straße, danach fand der Filmmarkt im Atrium des Debis-Hauses am Potsdamer Platz statt, das eine Fläche von rund 2.500 m² bot.

Die stetige Expansion der Veranstaltung führte 2006 zum Umzug in den Martin-Gropius-Bau und das Marriott Hotel am Potsdamer Platz. Im Rahmen des "EFM" 2016 wurden 1124 Vorführungen von mehr als 780 angemeldeten Filmen durchgeführt (davon ca. 530 Marktpremieren). Dafür wurden vorwiegend die Kinos "CinemaxX" und "CineStar" genutzt. Es waren über 9000 Teilnehmer aus 110 Ländern gemeldet.



Des Weiteren verleihen vom Festival unabhängige Jurys folgende Preise:




Seit 2002 eröffnet ein 50-sekündiger Trailer („Opener“) die Vorführungen in allen Sektionen des Festivals. Die Computeranimation entstand in Zusammenarbeit des Regisseurs Uli M Schueppel mit der Filmproduktionsfirma "Das Werk" Berlin.




</doc>
<doc id="486" url="https://de.wikipedia.org/wiki?curid=486" title="Bundesbeauftragter für die Stasi-Unterlagen">
Bundesbeauftragter für die Stasi-Unterlagen

Die Behörde des Bundesbeauftragten für die Unterlagen des Staatssicherheitsdienstes der ehemaligen Deutschen Demokratischen Republik (BStU), verkürzend nach den amtierenden Bundesbeauftragten auch Gauck-, Birthler- bzw. Jahn-Behörde genannt, verwaltet und erforscht die Akten und Dokumente des Ministeriums für Staatssicherheit (kurz "MfS" oder "„Stasi“") der DDR. Die Einrichtung der Behörde wurde von Mitgliedern der Bürgerkomitees und Freiwilligen der Bürgerrechtsbewegung im Zuge der friedlichen Revolution von 1989 erwirkt. Die Amtszeit des Bundesbeauftragten beträgt fünf Jahre, eine einmalige Wiederwahl ist gemäß § 35 Abs. 4 Stasi-Unterlagen-Gesetz (StUG) möglich. Das 1991 in Kraft getretene StUG bildet die Rechtsgrundlage der Behörde. Die Behörde ist Mitglied der Platform of European Memory and Conscience.

Am 3. Oktober 1990 übernahm der ehemalige Rostocker Pastor Joachim Gauck das Amt des Sonderbeauftragten der Bundesregierung für die Stasi-Unterlagen. Mit Inkrafttreten des Stasi-Unterlagen-Gesetzes am 29. Dezember 1991 wurde er der Bundesbeauftragte für die Stasi-Unterlagen. Wegen ihres langen amtlichen Titels wurde die Behörde daraufhin kurz Gauck-Behörde genannt.

Als Marianne Birthler die Leitung der Behörde im Oktober 2000 übernahm, hieß die Behörde in den Medien auch "Birthler-Behörde". Sie wird als "Bundesbehörde für die Stasi-Unterlagen" (BStU) bezeichnet und unterliegt heute der Dienstaufsicht durch den Bundesbeauftragten für Kultur und Medien (BKM, bekannter als Kulturstaatsminister). Es gibt keine Fachaufsicht durch ein Ministerium, die BStU berichtet regelmäßig an den Deutschen Bundestag.

Der Deutsche Bundestag hat auf Vorlage der Bundesregierung im Rahmen der Fortschreibung des Gedenkstättenkonzeptes<ref name="Bundestags-Drucksache 16/9875">BT-Drs. 16/9875 (PDF)</ref> beschlossen, dass eine Expertenkommission eingesetzt werden soll. Diese wird die Aufgaben der BStU analysieren und dem Deutschen Bundestag Vorschläge zur Zukunft der Behörde machen. Die Regierungsparteien CDU/CSU und FDP entschieden, dass die BStU noch mindestens bis 2019 tätig bleiben soll.

In dem Gedenkstättenkonzept ist außerdem festgehalten, dass die Struktur und Anzahl der BStU-Außenstellen in den Regionen zeitnah verändert wird, um trotz zurückgehenden Personalbestands die Arbeitsfähigkeit gewährleisten zu können. Hierzu lagen 2009 erste Planungen vor, die mit dem BKM, dem Deutschen Bundestag und den Ländern diskutiert wurden.

Am 28. Januar 2011 wurde der Journalist Roland Jahn vom Deutschen Bundestag zum neuen Leiter der Behörde gewählt. Das Amt trat er im März 2011 an.

Am 29. Dezember 1991 trat das Stasi-Unterlagen-Gesetz (StUG) in Kraft; der Deutsche Bundestag hatte es mit großer Mehrheit verabschiedet. Das zentrale Anliegen dieses Gesetzes ist die Öffnung der Akten des ehemaligen Staatssicherheitsdienstes für die Aufarbeitung, insbesondere der Zugang der Betroffenen zu den Informationen, die der Staatssicherheitsdienst zu ihnen gespeichert hat. Erstmals bekamen damit Bürger die Gelegenheit, Unterlagen einzusehen, die eine Geheimpolizei über sie angelegt hatte. Das ist in der Geschichte ohne Beispiel. Zudem ist es ein Auftrag des Gesetzes, die Persönlichkeitsrechte der Menschen, über die Stasi-Unterlagen existieren, zu schützen. Vier Tage nach dem Inkrafttreten des Gesetzes, am 2. Januar 1992, begann die Akteneinsicht.

Das Ministerium für Staatssicherheit der DDR sammelte Material über Millionen von Menschen – in erster Linie über DDR-Bürger, aber auch über viele Bürger der Bundesrepublik Deutschland und über Bürger anderer Staaten.
Viele Lebensläufe – nicht nur in der DDR – wurden im Laufe der Jahre durch die Staatssicherheit entscheidend beeinflusst. Das MfS beeinflusste den beruflichen Auf- oder Abstieg, erstellte systematische Pläne zur so genannten Zersetzung, drang in die Privatsphäre seiner Opfer ein und verwendete auch intime Informationen für seine Zwecke. Die Stasi verletzte Grundrechte der Bürger wie die ärztliche Schweigepflicht, Bank- und Postgeheimnis, die Unverletzlichkeit der Wohnung, auch wenn sie in der Verfassung der DDR festgelegt waren.

Insgesamt wurden bis Ende 2011 6.680.934 Anträge auf Akteneinsicht gestellt, darunter:

2007 erschien ein als vertraulich deklariertes „Gutachten über die Beschäftigung ehemaliger MfS-Angehöriger bei der BStU“, das der Kulturstaatsminister Bernd Neumann bei dem ehemaligen Verfassungsrichter Hans Hugo Klein und dem Historiker Klaus Schroeder in Auftrag gegeben hatte. Das Gutachten erhebt schwere Vorwürfe gegenüber der Behörde unter der Führung von vor allem Joachim Gauck. So sollen 1991 mindestens 79 ehemalige Stasimitarbeiter, darunter fünf ehemalige IMs, in der Behörde tätig gewesen sein. Im Gutachten heißt es: "„Nahezu alle ehemaligen MfS-Bediensteten hatten in den ersten Jahren des Aufbaus der Behörde die Möglichkeit des Missbrauchs. Sie konnten Akten vernichten, verstellen oder herausschmuggeln, denn sie hatten als Wachschützer, als Archivare, als Magazinmitarbeiter oder als Rechercheure zum Teil ungehinderten und unbeaufsichtigten Zugang zu erschlossenem, aber auch zu unerschlossenem Material.“"
Marianne Birthler begrüßte grundsätzlich die Beauftragung der Bundesregierung zur Erstellung eines Gutachten, wies aber die im Gutachten angegebenen Zahlen an Mitarbeitern mit MfS-Vergangenheit zurück und bemängelte, dass das Gutachten unbelegte Darstellungen enthalte.
Der derzeitige Amtsinhaber Roland Jahn bezeichnete in seiner Antrittsrede am 14. März 2011 die Beschäftigung ehemaliger MfS-Mitarbeiter als „unerträglich“ und erklärte es zu seinem Ziel, diese Mitarbeiter umsetzen zu lassen.

Nach der Auflösung des MfS 1990 und der anschließende Offenlegung seiner Arbeitsweise wurde das MfS zum Gegenstand eines breiten öffentlichen Interesses und intensiver Forschung seit 1991. Die Dokumentation der BStU über Strukturen, Mitarbeiter und Methoden eines Nachrichtendienstes stellt einen bislang einmaligen Sonderfall und Chance in der deutschen Geschichte dar. Die Behörde eröffnete den Betroffenen erstmals und einzigartig Einsicht in die über sie gespeicherten Informationen.

Die Rechtsgrundlage für die Einsichtnahme in die Unterlagen des Staatssicherheitsdienstes der ehemaligen DDR bildet das Stasi-Unterlagen-Gesetz (StUG). Das Antragsformular für die Einsichtnahme kann auf der Internetseite der BStU heruntergeladen werden.

Jeder Einzelne hat das Recht, von dem Bundesbeauftragten Auskunft darüber zu verlangen, ob in den erschlossenen Unterlagen Informationen zu seiner Person enthalten sind. Ist das der Fall, kann er Auskunft, Einsicht in Unterlagen und Herausgabe von Kopien erhalten.

Die Unterlagen zu vermissten oder verstorbenen nahen Angehörigen sind nur für einen festgelegten Personenkreis und nur zu bestimmten Zwecken eingeschränkt zugänglich (§ 15 StUG).

Der Bundesbeauftragte unterstützt die Forschung, Presse, Rundfunk und Film (Medien) sowie Einrichtungen zur politischen Bildung bei der historischen und politischen Aufarbeitung der Tätigkeit des Staatssicherheitsdienstes, der Herrschaftsmechanismen der ehemaligen DDR bzw. der ehemaligen sowjetischen Besatzungszone sowie bei der Aufarbeitung der nationalsozialistischen Vergangenheit. Im Rahmen eines zulässigen Antrags werden die Unterlagen des Staatssicherheitsdienstes zur Verfügung gestellt.

Die Überprüfung von Personen auf eine frühere hauptamtliche oder inoffizielle Tätigkeit für den Staatssicherheitsdienst ist auf der Grundlage eines schriftlichen Ersuchens öffentlicher und nicht-öffentlicher Stellen möglich. Das sind etwa die Landtage oder Regierungen, aber auch Kreistage oder Bürgermeister. Der Kreis der überprüfbaren Personen wurde mit dem Siebten Gesetz zur Änderung des Stasi-Unterlagen-Gesetzes vom 21. Dezember 2006 stark eingeschränkt.

Ebenso werden u. a. Ersuchen zu Rentenangelegenheiten, zu offenen Vermögensfragen oder zu Ordensangelegenheiten bearbeitet. Für Zwecke der Strafverfolgung und Gefahrenabwehr sowie zur Rehabilitierung und Wiedergutmachung stellt der Bundesbeauftragte auch Unterlagen zur Verfügung.

Unter Berufung auf das Informationsfreiheitsgesetz stellte der Bürgerrechtler Heiko Stamer im Oktober 2012 einen Antrag auf Akteneinsicht bezüglich der Nutzung der vom MfS gesammelten Daten durch nationale und internationale Institutionen. Im Februar 2013 erhielt er eine 63-seitge Liste jener, die beim BStU in den Jahren 2000 bis 2012 Auskunft ersucht hatten. Auf der Liste finden sich allen voran nationale Institutionen wie Landeskriminalämter (519), der Generalbundesanwalt (354), das Bundeskriminalamt (349), die regionalen Polizeibehörden (311) und die regionalen Staatsanwaltschaften (233). Ebenso finden sich auf der Liste internationale Institutionen wie das US-Justizministerium, die NSA, die britische sowie die amerikanische Botschaft und verschiedene Verteidigungsministerien.

Unter den Unterlagen der Behörde nimmt die „Rosenholz“-Datenbank in der Öffentlichkeit eine Sonderstellung ein. Sicher haben politische Diskussionen und die Medienberichterstattung um die Verstrickung Westdeutscher in das Staatssicherheitssystem der DDR daran ihren Anteil. Diese Datenbank wird im Karteibereich der Zentralstelle geführt.

Bei den „Rosenholz“-Unterlagen handelt es sich um mikroverfilmte Karteien der Hauptverwaltung Aufklärung (HV A) des MfS, die für die Auslandsspionage zuständig war. Diese Verfilmung fertigte das MfS 1988 im Zuge der allgemeinen Mobilmachungsbereitschaft an. Auf unbekanntem Wege gelangten diese Unterlagen 1989/1990 in die USA. Die amerikanische Regierung ermöglichte 1993 Mitarbeitern des Bundesamtes für Verfassungsschutz Einblick in diese mikroverfilmten Karteikarten. Die deutschen Behörden sollten in die Lage versetzt werden, Spione zu enttarnen und ggf. unter Anklage zu stellen. Diese Aktion des Bundesamtes für Verfassungsschutz, in deren Folge tausende Westdeutsche überprüft wurden, lief damals unter dem Codewort „Rosenholz“. Inzwischen wird der Name auch als Bezeichnung für die Unterlagen selbst benutzt.

Nach langwierigen Verhandlungen der Bundesregierung mit den zuständigen Stellen in den USA wurden die Unterlagen mit westdeutschem Bezug seit dem Sommer 2000 bis Mai 2003 schrittweise an die Stasi-Unterlagen-Behörde zurückgegeben. Seit Juni 2003 können die Unterlagen entsprechend dem Stasi-Unterlagen-Gesetz verwendet werden.

In zahlreichen Spionageprozessen und Ermittlungsverfahren, die in den 1990er Jahren stattfanden, hat der Generalbundesanwalt Erkenntnisse aus den „Rosenholz“-Unterlagen verwendet. Daher ist aktuell mit strafrechtlich relevanten Entdeckungen kaum noch zu rechnen. Darüber hinaus ist die Überlieferungslage der „Rosenholz“-Datenbank nicht eindeutig. Oft werden sowohl IM (Inoffizieller Mitarbeiter), also Spione, wie deren Quellen, die von den IM ausgeforscht oder abgeschöpft wurden, unter dem gleichen Vorgang erfasst. Deshalb lässt sich oft nicht mehr eindeutig zuordnen, wer Täter und wer Opfer war. Da die HV A 1990 fast alle wichtigen Akten vernichten konnte, sind auch kaum Möglichkeiten zur Gegenprüfung in den Stasi-Unterlagen gegeben.

Seit 1995 besteht bei der BStU eine Projektgruppe zur manuellen Rekonstruktion zerrissener Unterlagen des Staatssicherheitsdienstes. In ihr arbeiten überwiegend Mitarbeiter des Bundesamtes für Migration und Flüchtlinge (BAMF) in Zirndorf bei Nürnberg. Derzeit sind dort sechs Vollzeit- und zwei Teilzeitkräfte unter Leitung eines Archivars der BStU tätig. Seit Beginn der Arbeiten im Jahr 1995 ist Schriftgut aus 400 Säcken im Umfang von 105 laufenden Regalmetern wiederhergestellt worden; etwa 75 Prozent der erschlossenen Unterlagen betreffen Vorgänge aus den letzten fünf Jahren der DDR. Hinzu kommen erhebliche Mengen an Teilrekonstruktionen (Blattfragmente), die erst vervollständigt werden können, wenn in anderen Säcken die zugehörigen Teile gefunden worden sind.

Inhaltlicher Schwerpunkt der Arbeiten war etwa die Wiederherstellung von Schriftgut der Abteilung XV (Auslandsaufklärung) der Bezirksverwaltung Leipzig. Zusammen mit den früheren, seit Beginn der Bearbeitung im Herbst 2004 erfolgten Rekonstruktionen, konnte so Schriftgut im Umfang von etwa 8 laufenden Regalmetern wiederhergestellt werden. Diese Unterlagen sind insoweit von besonderer Bedeutung, als 1989/90 beinahe alle Unterlagen zur Auslandsspionage des Staatssicherheitsdienstes vernichtet worden waren. Die rekonstruierten Dokumente haben deshalb nicht nur einen hohen Informationswert, sondern zeichnen sich auch durch ihren Evidenzwert aus, denn sie gehören zu den wenigen Zeugnissen der Tätigkeit, Struktur und Wirkungsweise der Auslandsspionage des Staatssicherheitsdienstes. Die zusammengesetzten Dokumente stammen überwiegend aus der Schlussphase der DDR. Etwa die Hälfte der erschlossenen Unterlagen konnte Vorgängen zugeordnet werden, die 1989 vom MfS noch aktiv geführt wurden. Die rekonstruierten Unterlagen belegen, dass und wie der Staatssicherheitsdienst für Zwecke der Auslandsspionage, insbesondere in der alten Bundesrepublik, auf allen Ebenen inoffizielle Mitarbeiter aus anderen Diensteinheiten des MfS rekrutierte, um diese zusätzlich zum eigenen Agentennetz einzusetzen. Im Juli 2008 wurde entschieden, die manuelle Rekonstruktion der Unterlagen zur Leipziger Abteilung XV zunächst einzustellen, weil einerseits der Arbeits- und Zeitaufwand für die Rekonstruktion der besonders kleinteiligen Schnipsel extrem hoch war und andererseits die Aussicht auf eine computergesteuerte Zusammensetzung der stark zerstörten Unterlagen besteht. Deshalb werden die verbleibenden Materialien – zusammen mit den zerrissenen Akten der Auslandsaufklärung aus den übrigen Bezirksverwaltungen der Staatssicherheit – in das Pilotverfahren zur virtuellen Rekonstruktion eingebracht. Das Pilotverfahren hat somit auch positive Effekte auf die Arbeitsabläufe der manuellen Rekonstruktion: Erstmals besteht die Aussicht, Material differenziert nach Schadensklassen in verschiedene Arbeitsgänge einsteuern zu können. Durch diese neue arbeitsorganisatorische Option wird dem vom Deutschen Bundestag formulierten Ziel, die Rekonstruktion zerrissener Stasi-Unterlagen zu beschleunigen, zugleich auch bei der manuellen Zusammensetzung der Unterlagen entsprochen. Neben den Materialien der Auslandsaufklärung wurden schwerpunktmäßig weiterhin Unterlagen der HA XX (Staatsapparat, Kultur, Kirche, Untergrund) des MfS zusammengesetzt.

Einen dritten Arbeitsschwerpunkt bildet seit Ende 2006 die Rekonstruktion von Unterlagen der Bezirksverwaltungen Cottbus und Frankfurt (Oder). Beide Überlieferungen sind durch hohe Verluste von Unterlagen gekennzeichnet, doch ist für einige Bereiche zerrissenes Schriftgut in nennenswertem Umfang erhalten. Daher besteht die Aussicht, Überlieferungslücken im Wege der Rekonstruktion ausgleichen zu können.

Für die Haushaltsjahre 2007 und folgende bewilligte der Deutsche Bundestag bis zu 6,3 Millionen Euro für ein Pilotverfahren zur virtuellen Rekonstruktion der zerrissen überlieferten Unterlagen des Staatssicherheitsdienstes. Das Pilotverfahren lehnt sich an eine Machbarkeitsstudie des Fraunhofer-Instituts für Produktionsanlagen und Konstruktionstechnik (IPK) aus dem Jahr 2003 an. In dem im Frühjahr 2007 zwischen dem Beschaffungsamt des Bundesministeriums des Innern (für die BStU) und der Fraunhofer-Gesellschaft (für das IPK) abgeschlossenen Forschungsauftrag wird dem Institut eingeräumt, Projektteile von Unterauftragnehmern ausführen zu lassen. Dies trifft insbesondere für das Scannen und die sogenannte Rahmensoftware zur eigentlichen Rekonstruktionssoftware zu. Die für die Verwahrung und Behandlung von MfS-Unterlagen geforderten Sicherheitsstandards werden vom IPK gewährleistet.

Derzeit befindet sich das Pilotverfahren in der Entwicklungsphase. Da die Entwicklung der verschiedenen Module wesentlich zeitaufwändiger war, als das IPK beim Projektstart angenommen hat, kam es zu einer bisher über zweijährigen Verzögerung. Dennoch ist das Verfahren weiter auf einem guten Weg. Durch die Verzögerung entstehen keine Mehrkosten, da die Leistungen des IPK zu einem Festpreis erfolgsabhängig vergütet werden.

Das Rekonstruktionsverfahren konnte mittlerweile in der ersten lauffähigen Version, die die Basismodule beinhaltet, abgenommen werden. In der sich daran anschließenden Testphase (Testlauf im Realbetrieb) mit den verbliebenen Säcken sollen die verschiedenen Arbeitsschritte auf ihre Zuverlässigkeit hin überprüft und verfeinert werden. Auch während des geplanten Testlaufs werden Beschäftigte der BStU beim IPK begleitend eingesetzt.

Das Pilotverfahren besteht aus zwei Hauptbausteinen. Der erste Hauptbaustein umfasst die vom IPK zu realisierende Entwicklungs- und Testphase, in der die Schnipsel aus einigen „Probesäcken“ von insgesamt 400 nach archivfachlichen Kriterien ausgewählten Säcken mit zerrissenen Unterlagen im IPK gescannt und automatisiert bzw. interaktiv (durch Mitarbeiter am Bildschirm) virtuell rekonstruiert werden. Die hierbei gewonnenen Erfahrungen dienen der Weiterentwicklung der Software und der optimalen Ablaufgestaltung. Mitarbeiter der BStU unterstützen und begleiten die Arbeitsvorbereitung des Scan-Verfahrens sowie die Qualitätssicherung für das Zusammensetzen und die interaktive virtuelle Rekonstruktion.

Der zweite Hauptbaustein ist vom BStU zu realisieren und schließt sich an das technisch erfolgreich abgeschlossene Pilotverfahren an. In der archivischen Bearbeitungsphase werden die vom IPK gelieferten virtuell rekonstruierten Einzelseiten zu Dokumenten bzw. Vorgängen formiert und nach Archivstandards erschlossen. Das Ergebnis der darauf folgenden Auswertungsphase wird ein Bericht an den Deutschen Bundestag sein. Gefordert werden belastbare Aussagen zur Machbarkeit und Prozessmodellierung im Realbetrieb sowie zu den Kosten eines möglichen Hauptverfahrens. Darüber hinaus sollen die im Pilotverfahren gewonnenen Erkenntnisse zum Mehrwert der rekonstruierten Unterlagen im Vergleich zu den vorhandenen Unterlagen bezogen auf die jeweils bearbeiteten Teilbestände dargestellt werden. Auf der Grundlage dieser Aussagen soll das Parlament über den weiteren Umgang mit den zerrissenen Unterlagen entscheiden können.

Der Bundesrechnungshof kritisierte in seinem Jahresbericht 2016 die Kosten der Rekonstruktion zerrissener Stasi-Akten mit der Stasi-Schnipselmaschine. In acht Jahren konnten nur Schnipsel aus 23 Säcken digitalisiert und der Inhalt von 11 Säcken rekonstruiert werden. Dabei lagern rund 15 000 Säckn mit Schnipsel in der Behörde. Der Bundesrechnungshof stellte fest, dass für das Projekt beim BStU Ausgaben von mehr als 14 Millionen Euro angefallen waren, „ohne dass absehbar war, wann und wie eine relevant höhere Zahl von Säcken bearbeitet werden könnte“ und forderte den BStU auf, „in einer Neu-Konzeption die fachlichen, technischen und finanziellen Voraussetzungen für eine Weiterführung des Projekts darzulegen“ und empfahl, weil die 2014 vom Bundestag eingesetzte Expertenkommission für die Zukunft des BStU die „Durchführbarkeit der virtuellen Rekonstruktion skeptisch“ sah, eine Kosten-Nutzen-Abwägung.

Auslöser für den Aktenstreit waren unterschiedliche Auffassungen über die Berechtigung der BStU, Stasi-Akten, die Helmut Kohl betreffen, an Journalisten herauszugeben. Der Streit begann Ende 1999 im Zuge der CDU-Spendenaffäre, als bekannt wurde, dass Abhörprotokolle und wörtliche Protokolle des DDR-Geheimdienstes zu diesem Thema vorliegen. Nach Entscheidungen durch das Bundesverwaltungsgericht darf die BStU Unterlagen zu Helmut Kohl nur eingeschränkt herausgeben.

Der "Bundesbeauftragte" ist Roland Jahn, der "Direktor beim Bundesbeauftragten" Björn Deicke. Sie haben einen Stabsbereich, dem die Pressesprecherin, der Referent des Bundesbeauftragten, die Büroleiterin und die interne Revision angehören. Die Behörde ist in vier Abteilungen und diese wieder in Referate unterteilt; zudem gibt es zwölf Außenstellen. Der BStU hat zur Abstimmung und Beratung einen Beirat und ein wissenschaftliches Beratungsgremium.

Diese Abteilung ist zuständig für Personalangelegenheiten und Aus- und Fortbildung (Referat ZV 1), Organisation (Referat ZV 3), Haushalt und Beschaffung (Referat ZV 4), Informations- und Telekommunikationstechnik (Referat ZV 5) sowie Innerer Dienst und Liegenschaftswesen (Referat ZV 6). Abteilungsleiterin ist Alexandra Titze.

Die Abteilung gliedert sich in das Grundsatzreferat (AR 1), das Karteireferat (AR 2), den Magazindienst (Referat AR 3), die Erschließungsreferate AR 4, AR 5 und AR 6, das Referat zur Erschließung und technischen Bearbeitung audiovisueller Medien und maschinenlesbarer Daten (AR 7) und das Referat für Kommunikation und Öffentlichkeitsarbeit (AR K). Abteilungsleiterin von AR ist Birgit Salamon.

Die Abteilung besteht aus einem Grundsatzreferat (AU G) sowie verschiedenen Auskunftsreferaten (AU 1–6). Die Abteilung Auskunft bearbeitet die Anträge auf Akteneinsicht der Bürger, aber auch Anträge auf Überprüfung (Sicherheit, Parlamentarier u. a., Ordensangelegenheiten) für Renten-, Rehabilitations- und Wiedergutmachungsfragen. Die Anträge von Forschung und Medien werden in den Referaten AU 5 und AU 6 bearbeitet. Abteilungsleiter ist Joachim Förster.

Die Abteilung setzt sich zusammen aus dem wissenschaftlichen Forschungsbereich (BF 1), dem Servicebereich (BF 2), u. a. verantwortlich für die hauseigene Bibliothek und Publikationen sowie dem Fachbereich politische Bildung (BF 3), der für die Information der Öffentlichkeit durch Ausstellungen und Veranstaltungen zuständig ist. Abteilungsleiter ist Helge Heidemeyer.

In der Abteilung R sind die zwölf Außenstellen der Stasi-Unterlagen-Behörde zusammengefasst. Sie befinden sich in den früheren Bezirksstädten der DDR und verwahren die Unterlagen der Bezirksverwaltungen des Staatssicherheitsdienstes. Abteilungsleiter ist Jens Boltze.
In folgenden Städten gibt es BStU-Außenstellen:
Gemäß § 39 StUG begleitet der Beirat die inhaltliche Arbeit des Bundesbeauftragten in beratender Funktion. Der Bundesbeauftragte unterrichtet den Beirat über grundsätzliche und andere wichtige Angelegenheiten und erörtert sie mit ihm. Dem Beirat gehören acht Mitglieder an, die vom Deutschen Bundestag gewählt werden sowie neun Mitglieder, die von den jeweiligen Landtagen in den neuen Bundesländern gewählt werden. Damit werden in Anbetracht der fachlichen Unabhängigkeit des Bundesbeauftragten eine zusätzliche Begleitung ihrer Tätigkeit ermöglicht und die besonderen Interessen der neuen Bundesländer berücksichtigt.

Mitglieder im Beirat sind neben dem Vorsitzenden Richard Schröder und den stellvertretenden Vorsitzenden Ulrike Poppe und Rainer Eppelmann folgende vom Deutschen Bundestag gewählte Personen: 

Von den neuen Bundesländern wurden
benannt.

Schon als 1991 das Stasi-Unterlagen-Gesetz (StUG) in Kraft trat, war klar, dass die damit gegründete Bundesbehörde für die Stasi-Unterlagen ihren Aktenbestand irgendwann in das Bundesarchiv überführen würde. Da im Stasi-Unterlagen-Gesetz aber ein Unterschied beim Zugang zu den Täterunterlagen gemacht wird, den es im Bundesarchivgesetz nicht gibt, und weil für die Akten der Stasiopfer ein größerer Datenschutz besteht als im Bundesarchiv, ist diese Überführung nicht ohne Weiteres möglich.

Eine Änderung des StUG und die mögliche Auflösung und Eingliederung der BStU in das Bundesarchiv wurde 2006 und 2007 öffentlich diskutiert. Der Deutsche Bundestag hat auf Vorlage der Bundesregierung im Rahmen der Fortschreibung des Gedenkstättenkonzeptes im Jahr 2008 die Einsetzung einer Expertenkommission durch den 17. Deutschen Bundestag beschlossen. Diese sollte die gesetzlich zugewiesenen Aufgaben der BStU analysieren und Vorschläge zur Zukunft der Behörde unterbreiten.
Grundsätzlich haben die Regierungsparteien CDU/CSU und FDP entschieden, dass die BStU mindestens bis zum Jahr 2019 weiterarbeiten soll. Dies hatte die SPD schon in der vergangenen Legislaturperiode gefordert. Am 27. November 2014 wurde die Expertenkommission zur Zukunft der Behörde des BStU offiziell eingesetzt.

In der Fortschreibung der Gedenkstättenkonzeption wurden auch Entscheidungen über die Zukunft der Außenstellen und die Arbeit in der Fläche festgehalten:

„Die Struktur der Außenstellen wird zeitnah verändert, um eine effizientere Arbeit trotz zurückgehenden Personalbestandes gewährleisten zu können.“

Erste Planungen wurden inzwischen vorgenommen. Da die Außenstellen Rostock, Magdeburg, Gera und Suhl geschlossen werden sollen, hat bereits eine intensive Diskussion darüber eingesetzt.
Die Schließung betrifft die Archivstandorte, vor Ort sollen Bürgerbüros die Dienstleistungen für die Bürger – Akteneinsicht, politische Bildungsarbeit, Informationsveranstaltungen etc. – weiter anbieten.

Anfang 2016 kam es zu Kritik des Bundesrechnungshof wegen einer unzulässigen Anzahl von Überstunden von Dagmar Hovestädt, Pressesprecherin der Behörde. Kritisiert wurde, dass die Bezahlung der Überstunden erfolgte ohne dass diese in Akten dokumentiert waren. Die Bezahlung der Überstunden war so groß, dass sie das Gehaltsgefüge der Einrichtung verändert hat. Laut Behörde waren die Überstunden der Pressesprecherin dienstlich notwendig gewesen. Eine rechtliche Prüfung habe stattgefunden. In der Verwaltung sei es zu verschiedenen Fehlern gekommen. 2017 wurde bekannt, dass die Pressesprecherin in vier Jahren 100.000 Euro Bezahlung für Überstunden erhielt. Prüfer vom Bundesrechnungshof rügten in einem Bericht, dass Kosten einer Mitarbeiterfeier unter dem Haushaltstitel "Veröffentlichung und Dokumentation" verbucht wurden. Tatsächlich habe man Catering, musikalische Umrahmung, Blumenschmuck und Taxifahrten bezahlt. Die Behörde wollte die Mängel abstellen und Finanzentscheidungen intensiver prüfen. Michael Leutert (Die Linke), MdB kommentierte: „Dass ausgerechnet die Wächter über Stasiakten ihre eigenen Personalakten offensichtlich nicht sauber führen, wäre fast ironisch, wenn dabei dem Steuerzahler kein Schaden entstünde.“

Die Behörde hat zahlreiche Dokumentationen und Analysen zu allen möglichen Aspekten rund um die DDR und deren Ministerium für Staatssicherheit veröffentlicht – Angaben dazu sind auf deren Internetseite und in der Deutschen Nationalbibliothek zu finden.




</doc>
<doc id="487" url="https://de.wikipedia.org/wiki?curid=487" title="BND">
BND

BND steht für:


</doc>
<doc id="488" url="https://de.wikipedia.org/wiki?curid=488" title="Bor">
Bor

Bor ist ein chemisches Element mit dem Elementsymbol B und der Ordnungszahl 5. Im Periodensystem steht es in der 3. Hauptgruppe, bzw. der 13. IUPAC-Gruppe, der Borgruppe, sowie der zweiten Periode. Das dreiwertige, seltene Halbmetall kommt in Form seiner Sauerstoffverbindungen als Borax und Kernit angereichert in einigen abbauwürdigen Lagerstätten vor. Bor existiert in mehreren Modifikationen. Amorphes Bor ist ein braunes Pulver. Vom kristallinen Bor sind mehrere allotrope Modifikationen bekannt.

Borverbindungen finden vielfältige Anwendungen in verschiedenen Industriezweigen. Die Waschmittelindustrie verwendet Borverbindungen wie Natriumperborat im großtechnischen Maßstab als Bleichmittel. Die Glasindustrie nutzt Bor in Form seiner Boraxverbindungen für die Produktion von Gläsern und Keramiken mit hoher Chemikalienresistenz und Temperaturwechselbeständigkeit. Elementares Bor wird in der Halbleiterindustrie zur Dotierung verwendet. Borpolymere und -keramiken spielen eine Rolle für die Herstellung hochfester Leichtbau- und feuerfester Materialien. Borcarbid weist eine hohe Härte auf und wird als Schleifmittel verwendet. Zum Hartlöten werden Borverbindungen als Flussmittel genutzt. In der Hydroborierung dienen Borreagenzien der Synthese organischer Feinchemikalien. Natürliches Bor besteht aus zwei stabilen Isotopen, von denen Bor als Neutronenabsorber geeignet ist.

Borate haben geringe Toxizität für Säugetiere, sind aber giftig für Gliederfüßer und werden als Insektizide verwendet. Borsäure wirkt schwach antimikrobiell; es sind natürliche, Bor enthaltende Antibiotika bekannt. Bor ist möglicherweise ein essentielles Spurenelement. In der Landwirtschaft verbessert Bordüngung die Stabilisierung der pflanzlichen Zellwände und hat eine wichtige Funktion bei der Zellteilung, Zelldifferenzierung, Zellstreckung und Gewebebildung der Pflanzen sowie im Nukleinsäurestoffwechsel, der Eiweißsynthese und beim Energiestoffwechsel.

Borverbindungen (von "bura" über "buraq" und griech. βοραχου bzw. lat. "borax" „borsaures Natron“, „Borax“) sind seit Jahrtausenden bekannt. Im alten Ägypten nutzte man zur Mumifikation das Mineral Natron, das neben anderen Verbindungen auch Borate enthält. Seit dem 4. Jahrhundert wird Boraxglas im Kaiserreich China verwendet. Borverbindungen wurden im antiken Rom zur Glasherstellung verwendet.

Erst 1808 stellten Joseph Louis Gay-Lussac und Louis Jacques Thénard Bor durch Reduktion von Bortrioxid mit Kalium, unabhängig hiervon etwas später Sir Humphry Davy durch Elektrolyse von Borsäure her. 1824 erkannte Jöns Jakob Berzelius den elementaren Charakter des Stoffes. Die Darstellung von reinem kristallisiertem Bor gelang dem amerikanischen Chemiker W. Weintraub im Jahre 1909 durch Reduktion von gasförmigem Bortrichlorid mit Wasserstoff im Lichtbogen.

Wie die beiden im Periodensystem vorangehenden Elemente Lithium und Beryllium ist auch Bor ein im Sonnensystem auffallend seltenes Element. Die Seltenheit dieser drei Elemente erklärt sich daraus, dass sie keine Produkte der stellaren Kernfusionen sind, die zur Elemententstehung (Nukleosynthese) führen. Das Wasserstoffbrennen führt zu Heliumatomen, das darauffolgende Heliumbrennen (der Drei-Alpha-Prozess) schon zu Kohlenstoffatomen. Bor entsteht ausschließlich bei der Spallation schwerer Atomkerne durch kosmische Strahlung. 

Bor kommt auf der Erde nur in sauerstoffhaltigen Verbindungen vor. Große Lagerstätten befinden sich in Bigadiç, einem Landkreis der Provinz Balıkesir im Westen der Türkei, an der Mojave-Wüste in den USA und in Argentinien. Staßfurter Kalisalze enthalten geringe Mengen vergesellschafteten Boracit.

Die größten Boratminen befinden sich bei Boron (Kalifornien) (die "Kramer-Lagerstätte") und in Kırka (Türkei). Abgebaut werden die Mineralien Borax, Kernit und Colemanit.

In Wasser kommt Bor überwiegend als undissoziierte Borsäure vor.

Bor kommt im Meerwasser in einer Konzentration von 4-5 mg/l vor.
In Meeresluft wurden 0,17 μg/m gemessen (WHO, 1996).

In Mineralwässern wurden durchschnittlich 500 μg/l Bor gemessen, mit einem Wertespektrum zwischen weniger als 20 μg/l und 3,23 mg/l.

Der Gehalt im Grundwasser sowie in Binnengewässern liegt in Deutschland im Bereich von 10 bis 50 μg/l, wobei in Baden-Württemberg von einem Hintergrundwert (ohne anthropogene Beeinflussung) im Grundwasser von 50 μg/l ausgegangen wird.

In der Außenluft sind in Deutschland im Durchschnitt 16 ng/m³ und im Trinkwasser Werte von 10 bis 210 μg/l gemessen worden. Im Boden liegt die Konzentration an Borax zwischen 88 und 177 mg/kg bezogen auf das Trockengewicht.

In der Schweiz wird von natürlichen Borgehalten im Flusswasser von rund 10 μg/l und im Grundwasser von bis zu 40 μg/l ausgegangen, während die tatsächlichen Werte in Flüssen und Seen bis über 200 μg/l betragen können und das Trinkwasser durchschnittlich rund 20 μg/l und höchstens 60 μg/l Bor enthält.

Pflanzen benötigen Bor und der Gehalt in der Trockenmasse beträgt 30–75 ppm. Menschen nehmen Bor über Trinkwasser und Nahrung auf. Im Körper liegt ein Gehalt von etwa 0,7 ppm vor.

Amorphes Bor wird durch die Reduktion von Bortrioxid, BO, mit Magnesiumpulver hergestellt:
Derartig gewonnenes Bor besitzt nach Abtrennen der Beimengungen eine Reinheit von 98 %. Die Reinheit des Stoffes kann erhöht werden, indem das Bor als Reinstoff aus einer Platinschmelze bei 800–1200 °C auskristallisiert wird.

Kristallines Bor lässt sich auch durch andere Verfahren darstellen: Das Element lässt sich meist aus seinen Halogeniden als Reinstoff gewinnen. Mittels eines 1000–1400 °C heißen Wolfram- oder Tantaldrahts kann durch Reduktion von Bortrichlorid oder Bortribromid mit Wasserstoff das Element in sehr hoher Reinheit dargestellt werden. Um Bortrifluorid mit Wasserstoff zu reduzieren, wären Reaktionstemperaturen von 2000 °C erforderlich, sodass diese Verbindung nicht als Ausgangsstoff zur Darstellung genutzt wird.

Eine weitere Möglichkeit stellt die thermische Zersetzung von Diboran bei 600–800 °C bzw. von Bortriiodid bei 800–1000 °C an einer Tantal-, Wolfram-, oder Bornitrid-Oberfläche dar.

Die vermutlich thermodynamisch stabilste Form ist die β-rhomboedrische Modifikation (β-Bor). Sie hat eine komplizierte Struktur mit mindestens 105 Boratomen pro Elementarzelle, wobei hier noch Boratome hinzukommen, die sich auf teilbesetzten Lagen befinden. Die Anzahl der Boratome pro Elementarzelle wird mit 114 bis 121 Atomen angegeben. Die Struktur dieser Modifikation kann man mit einem 60-Ecken-Polyeder beschreiben.

Die einfachste allotrope Modifikation ist die α-rhomboedrische Form des Bors (α-Bor). Die in dieser Modifikation des Bors dominierende Struktureinheit ist das B-Ikosaeder mit 12 Boratomen im Ikosaeder. Diese sind in Schichten angeordnet ähnlich wie in einer kubisch flächenzentrierten Packung. Die Ikosaeder einer Schicht sind durch Dreizentrenbindungen und die Ikosaeder benachbarter Schichten durch Zweizentrenbindungen miteinander verknüpft.

α-tetragonales Bor (auch als γ-Bor bezeichnet), die als erstes dargestellte kristalline Form des Bors, enthält 50 Bor-Atome in der Elementarzelle (gemäß der Formel (B)B), kann beispielsweise aber auch, abhängig von den Herstellungsbedingungen, als Einschlussverbindung BC oder BN vorliegen. Im fremdatomfreien α-tetragonalen Bor verbindet ein einzelnes Boratom immer vier B-Ikosaeder miteinander. Jedes Ikosaeder hat Verbindungen zu je zwei einzelnen Boratomen und zehn anderen Ikosaedern. Seit der ersten Beschreibung dieser Struktur ist es nie wieder gelungen diese Modifikation rein herzustellen. Man geht mittlerweile davon aus, dass reines α-tetragonales Bor in der beschriebenen Struktur nicht existiert.

Das elementare Bor ist schwarz, sehr hart und bei Raumtemperatur ein schlechter Leiter. Es kommt nicht in der Natur vor.

Forscher an der ETH in Zürich stellten aus äußerst reinem Bor einen ionischen Kristall her. Dazu musste das Material einem Druck von bis zu 30 Gigapascal und einer Temperatur von 1500 °C ausgesetzt werden. Dieselbe Arbeitsgruppe veröffentlichte mittlerweile ein Addendum, wonach sie die Bindungssituation in dieser Modifikation als kovalent bezeichnen.

Einem Forschungsteam an der Universität Bayreuth ist es 2011 gelungen, α-rhomboedrisches Bor eindeutig als thermodynamisch stabile Phase von Bor zu identifizieren.
In Hochdrucklaboratorien wurde eine Serie unterschiedlicher Borkristalle bei Temperaturen bis zu 2300 Kelvin und Drücken bis zu 15 Gigapascal synthetisiert. Von besonderem Interesse für die Forschung und für industrielle Anwendungen, wie die Halbleitertechnik, sind hierbei α-Bor-Einkristalle.

Wegen der hohen Ionisierungsenergie sind von Bor keine B-Kationen bekannt. Die komplizierten Strukturen in vielen Borverbindungen und deren Eigenschaften zeigen, dass die Beschreibung der Bindungsverhältnisse als kovalent, metallisch oder ionisch stark vereinfachend sind und durch einen Molekülorbital(MO)-Ansatz ersetzt werden müssen.

Die Elektronenkonfiguration 1s2s2p des Bors zeigt, dass nur die drei Elektronen der zweiten Schale für die Ausbildung von kovalenten Bindungen mit s, p, p und p-Orbitalen zur Verfügung stehen. Dieser Elektronenmangel wird durch Ausbildung von Mehrzentrenbindungen, insbesondere einer Dreizentrenbindung, und Elektronenakzeptorverhalten (Lewis-Acidität) kompensiert. Es ist gelungen, eine Borverbindung mit einer Bor-Bor-Dreifachbindung herzustellen.

Bor ist durchlässig für Infrarotlicht. Bei Raumtemperatur zeigt es eine geringe elektrische Leitfähigkeit, die bei höheren Temperaturen stark ansteigt.

Bor besitzt die höchste Zugfestigkeit aller bekannten Elemente sowie die zweithöchste Härte, nur übertroffen von der Kohlenstoffmodifikation Diamant. Bormodifikationen haben physikalische und chemische Ähnlichkeit mit Hartkeramiken wie Siliciumcarbid oder Wolframcarbid.

Bis 400 °C ist Bor reaktionsträge, bei höheren Temperaturen wird es zu einem starken Reduktionsmittel. Bei Temperaturen über 700 °C verbrennt es in Luft zu Bortrioxid BO. Von siedender Salz- und Fluorwasserstoffsäure wird Bor nicht angegriffen. Oxidierend wirkende, konzentrierte Schwefelsäure greift Bor erst bei Temperaturen über 200 °C an, konzentrierte Phosphorsäure hingegen erst bei Temperaturen über 600 °C.

Löst man BO in Wasser, so entsteht die sehr schwache Borsäure. Deren flüchtige Ester, am deutlichsten Borsäuretrimethylester, färben Flammen kräftig grün.

Die Fähigkeit des Bors, über kovalente Bindungen stabile räumliche Netzwerke auszubilden, sind ein weiterer Hinweis auf die chemische Ähnlichkeit des Bors mit seinen Periodennachbarn Kohlenstoff und Silicium.

Eine wichtige Forschungsdisziplin der heutigen anorganischen Chemie ist die der Verbindungen des Bors mit Wasserstoff (Borane), sowie mit Wasserstoff und Stickstoff, die den Kohlenwasserstoffen ähneln (isoelektronisch), z. B. Borazol BNH („anorganisches Benzol“). Eine Reihe organischer Borverbindungen sind bekannt, beispielsweise Boronsäuren.

Es sind insgesamt 13 Isotope zwischen B und B des Bors bekannt. Von diesen sind zwei, die Isotope B und B stabil und kommen in der Natur vor. Das Isotop mit dem größeren Anteil an der natürlichen Isotopenzusammensetzung ist B mit 80,1 %, B hat einen Anteil von 19,1 %. Alle künstlichen Isotope haben sehr kurze Halbwertszeiten maximal im Millisekundenbereich. 

Die wirtschaftlich wichtigste Verbindung ist Borax (Natriumtetraborat-Decahydrat, NaBO · 10 HO) zur Herstellung von Isolierstoffen und Bleichstoffen (Perborate). Weitere Anwendungen:



Bor ist möglicherweise ein essentielles Spurenelement, das unter anderem positiven Einfluss auf Knochenstoffwechsel und Gehirnfunktion hat.

Menschen nehmen Bor über Trinkwasser und Nahrung auf. Im Körper liegt ein Gehalt von etwa 0,7 ppm vor. Die World Health Organization (WHO) stellte 1998 in einer Studie fest, dass weltweit von einer durchschnittlichen Aufnahme von 1 - 2 mg Bor pro Tag ausgegangen werden kann und empfiehlt einen Richtwert (Guideline value) von 2,4 mg/l Trinkwasser.

Pflanzen reagieren zum Teil sehr empfindlich auf Bor, so dass bestimmte sensible Pflanzen (Weiden, Obstbäume, Artischocken) bei Konzentrationen von mehr als 1 mg/l Bor zu Borchlorosen neigen (Krankheitsbild gekennzeichnet durch vermehrte Bildung von braunen Flecken) und schließlich absterben können. Pflanzen reagieren aber auch empfindlich auf zu wenig Bor, der Gehalt in der Trockenmasse liegt meist zwischen 30 und 75 ppm.

Elementares Bor in geringen Dosen ist nicht giftig. Für Bor gibt es keine Hinweise auf genotoxische oder kanzerogene Wirkungen; von der Deutschen Gesellschaft für Ernährung ist kein Referenzwert für Bor als Zufuhrempfehlung aufgeführt.

Dosen über 100 mg/Tag können jedoch Vergiftungserscheinungen hervorrufen. Die US-amerikanische Behörde EPA gibt einen täglichen Grenzwert (RfD - Reference Dose) von 0,2 mg pro Kilogramm Körpergewicht für Bor und Borate an, geht jedoch nicht von einer Karzinogenität aus.

Bortrioxid, Borsäure und Borate werden mit der 30. ATP in der EU seit Sommer 2009 als fortpflanzungsgefährdend eingestuft. Bei Borsäure und Borax wurde dieser Effekt bislang jedoch lediglich bei der Verabreichung von höheren Dosen an Mäuse beobachtet.

Einige Borverbindungen wie die Borane (Borwasserstoffverbindungen) sind hochgradig toxisch und müssen mit größter Sorgfalt gehandhabt werden.

Bor lässt sich in der analytischen Chemie mit der Curcumin-Methode quantitativ in Form des rot gefärbten Komplexes Rosocyanin nachweisen. Hierzu wird eine Probe des Bor-haltigen Materials oxidativ aufgeschlossen. Die durch den Aufschluss gebildete Borsäure kann anschließend kolorimetrisch bestimmt werden.



</doc>
<doc id="489" url="https://de.wikipedia.org/wiki?curid=489" title="Barium">
Barium

Barium (von griech. βαρύς "barýs": „schwer“, wegen der hohen Dichte des Bariumminerals Baryt) ist ein chemisches Element mit dem Elementsymbol Ba und der Ordnungszahl 56. Im Periodensystem steht es in der sechsten Periode und der 2. Hauptgruppe bzw. der 2. IUPAC-Gruppe und zählt damit zu den Erdalkalimetallen. Bariumoxid wurde erstmals 1774 von Carl Wilhelm Scheele dargestellt. Barium ist metallisch-glänzend und von silbrig-weißer Farbe. Es kommt in der Natur wegen seiner hohen Reaktivität nicht elementar vor; u. a. ist es leicht entzündlich. Wasserlösliche Bariumverbindungen sind giftig.

Erstmals wurden bariumhaltige Minerale im Jahr 1602 durch den italienischen Schuhmacher und Alchemisten Vincenzo Casciarolo (1571–1624) untersucht, dem glänzende Steinchen auffielen, die nach dem Erhitzen im Dunkeln leuchteten. Sie wurden durch die Publikationen des Ulisse Aldrovandi einem größeren Publikum als „Bologneser Stein“ bekannt. Es handelte sich dabei um Baryt, der beim Erhitzen mit organischen Substanzen phosphoresziert.

1774 wurde von dem schwedischen Chemiker Carl Wilhelm Scheele bei der Untersuchung von Gips erstmals Bariumoxid BaO identifiziert, das zunächst "neue alkalische Erde" genannt wurde. Zwei Jahre später fand Johan Gottlieb Gahn die gleiche Verbindung bei ähnlichen Untersuchungen. Ebenfalls im 18. Jahrhundert war dem englischen Mineralogen William Withering in Bleibergwerken Cumberlands ein schweres Mineral aufgefallen, bei dem es sich nicht um ein Bleierz handeln konnte und dem er die Bezeichnung „terra ponderosa“ gab. Es ist heute als Witherit (Bariumcarbonat BaCO) bekannt.

Metallisches Barium, jedoch nicht in Reinform, wurde erstmals 1808 von Sir Humphry Davy in England durch Elektrolyse eines Gemisches aus Bariumoxid und Quecksilberoxid hergestellt. Daraufhin erfolgte die Namensgebung Barium nach dem Bariummineral Baryt.

Die erste Reindarstellung des Bariums erfolgte 1855 durch Robert Bunsen und Augustus Matthiessen durch Schmelzelektrolyse eines Gemisches aus Bariumchlorid und Ammoniumchlorid. 1910 wurde von Marie Curie das schwerere Radium unter Ausnutzung seiner chemischen Ähnlichkeit mit Barium isoliert. Eine wichtige Rolle spielte das Metall auch 1938 bei den kernchemischen Experimenten Otto Hahns und Fritz Straßmanns, die Uran mit langsamen Neutronen beschossen und zu ihrem Erstaunen das viel leichtere Element Barium in den Reaktionsprodukten fanden. Dieser Befund wurde von ihnen korrekt als Spaltung des Urankerns gedeutet.

Barium kommt wegen seiner hohen Reaktivität in der Natur nicht elementar, sondern nur in Verbindungen vor. Mit einem Anteil von etwa 0,039 % ist Barium das 14.-häufigste Element der Erdkruste.

Barium wird vor allem in den Mineralen Baryt (oder Schwerspat, kristallisiertes Bariumsulfat) und Witherit (Bariumcarbonat) gefunden, und meist aus Baryt gewonnen. Die Weltjahresproduktion an Baryt ist innerhalb der letzten 30 Jahre von etwa 4,8 Millionen Tonnen (1973) auf 6,7 Millionen Tonnen (2003) gestiegen, die weltweiten Reserven werden auf etwa zwei Milliarden Tonnen geschätzt. Die deutschen Vorkommen von Bariumverbindungen liegen im Sauerland, im Harz und in Rheinland-Pfalz. Abbauwürdige Vorkommen von Bariumverbindungen gibt es weltweit: Die Hauptförderländer von Barium sind die Volksrepublik China, Mexiko, Indien, Türkei, USA, Deutschland, Tschechien, Marokko, Irland, Italien und Frankreich.

Jährlich werden etwa 4 bis 6 Millionen Tonnen Bariumsulfat gewonnen. Davon wird jedoch nur ein kleiner Teil zu Bariummetall weiterverarbeitet. Die Gewinnung aus Bariumcarbonat ist zwar einfacher, allerdings ist Bariumcarbonat in der Natur seltener zu finden als Bariumsulfat. Technisch wird Barium aus Baryt über Bariumsulfid und Bariumcarbonat in Bariumoxid umgewandelt, das dann mit Silicium oder Aluminium bei 1200 °C im Vakuum zum Reinmetall reduziert wird. Die Reaktionsgleichungen lauten:





Hochreines Barium wird durch Elektrolyse von geschmolzenem Bariumchlorid hergestellt und einer anschließenden Hochvakuumsublimation unterzogen.

Barium ist ein festes, paramagnetisches Erdalkalimetall, das in einem kubisch-raumzentrierten Gitter kristallisiert. Seine silberweiße Farbe wird an der Luft schnell mattgrau, weil sich eine Oxidschicht bildet.

Barium weist eine grüne bis fahlgrüne Flammenfärbung mit den charakteristischen Spektrallinien von 524,2 und 513,7 nm auf. Barium hat eine Dichte von 3,62 g/cm (bei 20 °C) und zählt damit zu den Leichtmetallen. Mit einer Mohshärte von 1,25 ist es vergleichsweise weich und auch das weichste der Erdalkalimetalle. Der Schmelzpunkt liegt bei 727 °C, der Siedepunkt bei 1637 °C. Das elektrochemische Standardpotenzial beträgt −2,912 V.

In den chemischen Eigenschaften ähnelt es Calcium und den anderen Erdalkalimetallen. Es reagiert heftiger als die meisten anderen Erdalkalimetalle mit Wasser und mit Sauerstoff und löst sich leicht in fast allen Säuren – eine Ausnahme bildet konzentrierte Schwefelsäure, da die Bildung einer Sulfatschicht (Passivierung) die Reaktion stoppt. Barium kann deshalb als eines der unedelsten Metalle bezeichnet werden. Wegen dieser hohen Reaktivität wird es unter Schutzflüssigkeiten aufbewahrt.

Es reagiert direkt mit Halogenen, Sauerstoff, Stickstoff und Schwefel. Dabei bildet es immer Verbindungen, in denen es als zweiwertiges Kation vorliegt. Beim Erhitzen an der Luft verbrennt das Metall mit der typischen grünen Flammenfärbung zu Bariumoxid.

Als sehr unedles Metall reagiert Barium mit Wasser unter Wasserstoff- und Hydroxidbildung. Bariumhydroxid bildet sich auch schon beim Kontakt des Metalls mit feuchter Luft.

Im Gegensatz zu anderen Erdalkalimetallen bildet Barium nur eine dünne, wenig passivierende Oxidschicht und kann sich daher in feuchter Luft selbst entzünden.

In der Natur kommen sieben stabile Bariumisotope vor, wobei Ba mit 71,8 % das häufigste Isotop ist. Des Weiteren sind von Barium 33 radioaktive Isotope mit Halbwertszeiten zwischen 10,5 Jahren bei Ba und 150 Nanosekunden bei Ba bekannt, wobei die meisten jedoch innerhalb von wenigen Sekunden zerfallen. Barium hat durchgängig Isotope von 58 bis maximal 97 Neutronen (von Ba bis Ba).

Stabile Bariumisotope entstehen bei verschiedenen Zerfallsreihen, beispielsweise des I in Ba. Die radioaktiven Isotope zerfallen in Lanthan-, Xenon-, Caesium- und Iodisotope.

Folgend zwei Beispiele für Kernspaltungen, bei denen radioaktive Isotope des Bariums entstehen:

Außerdem kann mit einem Cäsium-Barium-Generator das metastabile Isomer Barium-137m aus dem Zerfall von Caesium-137 erzeugt werden. Barium-137m zerfällt mit einer Halbwertszeit von 153.1(1) Sekunden unter Abgabe von Gammastrahlung zu stabilem Barium-137.

Elementares Barium findet nur in kleinem Umfang Verwendung, und die Produktion liegt bei nur wenigen Tonnen pro Jahr. Die wichtigste Anwendung ist diejenige als Gettermaterial in Vakuumröhren, beispielsweise von Fernsehern oder als Sonnenkollektoren. Es reagiert schnell mit unerwünschten Restgasen wie Sauerstoff, Stickstoff, Kohlenstoffdioxid oder Wasserdampf. Barium vermag auch unreaktive Gase einzuschließen und so aus der Vakuumröhre zu entfernen. Gleichzeitig besitzt das Metall bei den verwendeten Temperaturen einen niedrigen Dampfdruck.

Von der Verwendung als Gettermaterial abgesehen, findet Barium nur wenige Anwendungen. Mit Barium legiertes Nickel wird in Zündkerzen eingesetzt. Weiterhin wird es Bleilegierungen für Lagermetalle zugesetzt, da dies die Härte deutlich erhöht.

In Verbindung mit Eisen als Bariumferrit (BaFe) findet es Verwendung als Material für Magnetbänder hoher Kapazität.

Pflanzen nehmen Bariumkationen aus dem Boden auf und reichern sie an. Die höchste Konzentration in einer Nutzpflanze findet sich mit einem Anteil von 1 % entsprechend 10.000 ppm (Millionstel Anteilen) bei der Paranuss.

Auf Barium richtiggehend angewiesen sind dagegen die Zieralgen (Desmidiaceae), eine Familie von einzelligen, etwa einen Millimeter großen Grünalgen (Chlorophyta), die in kalten, nährstoffarmen Süßgewässern, insbesondere in Hochmooren, vorkommen. In ihren Zellen befinden sich flüssigkeitsgefüllte Hohlräume, in denen sich winzige Bariumsulfatkristalle befinden. Das dazu notwendige Barium wird offenbar selektiv dem Wasser entzogen, selbst bei verschwindend geringen Konzentrationen von nur 1 ppb. Auch um Größenordnungen darüber liegende Konzentrationen des chemisch ähnlichen Erdalkalimetalls Calcium ändern daran nichts. Umgekehrt werden für andere Organismen tödliche Bariumkonzentrationen von bis zu 35 ppm (Millionstel Anteile) toleriert. Die biologische Funktion der Kristalle ist noch unklar, eine Rolle bei der Schwerewahrnehmung wird jedoch vermutet. Dass Barium für die Pflanzen essenziell ist, zeigt sich dadurch, dass sie bei Entzug nicht mehr weiter wachsen.

Auch im menschlichen Körper kommt Barium vor, der durchschnittliche Gewebeanteil liegt bei 100 ppb, in Blut und Knochen mit jeweils bis zu 70 ppb etwas niedriger. Mit der Nahrung wird täglich etwa ein Milligramm Barium aufgenommen.

Alle wasser- oder säurelöslichen Bariumverbindungen sind giftig. Die maximale Arbeitsplatzkonzentration (MAK-Wert) liegt bei 0,5 mg/m. Eine Dosis von 1 bis 15 Gramm ist abhängig von der Löslichkeit der jeweiligen Bariumverbindung für einen Erwachsenen tödlich. Das in der Röntgenologie verwendete wasserunlösliche Kontrastmittel Bariumsulfat, das zur Darstellung des Magen-Darm-Trakts beziehungsweise des Schluckakts in der Videokinematographie eingesetzt wird, muss deshalb frei von löslichen Bariumverbindungen sein, das heißt, als Reinsubstanz zugeführt werden. Zu beachten ist hier auch die Verwechslungsmöglichkeit bei den im Sprachgebrauch der Apotheken verwendeten lateinischen Begriffen „Barium sulfuricum“ (Bariumsulfat) und „Barium sulfuratum“ (Bariumsulfid). Bariumvergiftungen erfolgen meist am Arbeitsplatz oder in der Nähe Barium-verarbeitender Industriezweige. Dabei kann es eingeatmet werden oder über das Trinkwasser in den Organismus gelangen.

Abgelagert wird Barium in der Muskulatur, den Lungen und den Knochen, in die es ähnlich wie Calcium, jedoch schneller, aufgenommen wird. Seine Halbwertszeit im Knochen wird auf 50 Tage geschätzt. Nachdem Calcium auch an der Zellmembran der Muskulatur durch Barium ersetzt werden kann, wird – bei niedriger Dosierung – die Membrandurchlässigkeit erhöht und die Muskelkontraktion gesteigert, was zu einer Blutdrucksteigerung und Senkung der Herzfrequenz, aber auch zu Muskelkrämpfen führen kann. Höhere Dosen führen zu Muskelschwäche bis hin zu -lähmung, die auf die Beeinträchtigung des Zentralen Nervensystems zurückgeführt wird. Herzrhythmusstörungen (Extrasystole und Kammerflimmern), Tremor, allgemeines Schwächegefühl, Schwindel, Angst und Atemprobleme können auftreten. Bei akuten wie subakuten Vergiftungen können Störungen des Magen-Darm-Trakts wie Leibschmerzen, Erbrechen und Durchfall auftreten. In hohen Konzentrationen inaktiviert Barium die passiven Kaliumkanäle in der Membran der Muskelzellen. Kalium kann so die Muskelzellen nicht mehr verlassen. Da die Natrium-Kalium-ATPase unvermindert Kalium in die Zellen pumpt, kommt es zum Abfall des Kaliumspiegels im Blut. Die resultierende Hypokaliämie führt zu Ausfall der Muskelreflexe (Areflexie), schlaffer Muskellähmung und Atemlähmung.

Erste Hilfe kann durch Gabe von Natriumsulfat- oder Kaliumsulfatlösung erfolgen, wodurch die Bariumionen als schwerlösliches und damit ungiftiges Bariumsulfat gebunden werden. Im Krankenhaus kann Barium durch Dialyse entfernt werden.

Eine Nachweisreaktion ist das Umsetzen mit verdünnter Schwefelsäure, woraufhin weißes Bariumsulfat ausfällt:

Befindet sich Barium in Gesellschaft mit anderen Elementen, die ebenfalls schwerlösliche Sulfate bilden, so kann dieses Verfahren nicht angewendet werden. Trennung und Nachweis erfolgen dann, sofern nur Erdalkalielemente vorhanden sind, nach dem Chromat-Sulfat-Verfahren (siehe unter Ammoniumcarbonatgruppe). Im Rahmen dieses Verfahrens wird die Bariumlösung mit Kaliumchromatlösung versetzt, und es entsteht ein gelber Niederschlag von Bariumchromat. Sind noch andere Elemente mit schwerlöslichen Sulfaten vorhanden, muss ein geeigneter Kationentrenngang durchgeführt werden.

Ein zum Nachweis von Barium geeignetes Verfahren ist die Atomspektroskopie. Der Nachweis von Barium und Bariumsalzen erfolgt hierbei über das charakteristische Spektrum. Gebräuchliche Gerätetypen hierzu sind beispielsweise das Flammenatomabsorptionsspektrometer oder das Atomemissionsspektrometer mit induktiv gekoppeltem Hochfrequenzplasma. Damit können selbst geringe Spuren von Barium nachgewiesen werden. Wenn kein Spektrometer vorhanden ist, kann man unter Umständen auch einfach eine Probe in eine Bunsenbrennerflamme halten und die grüne Flammenfärbung beobachten. Die Anwendung dieser Methode ist nicht aussagekräftig, wenn Elemente bzw. deren Verbindungen mit ähnlichen Flammenfarben vorhanden sind.

Bariumverbindungen liegen fast ausschließlich in der Oxidationsstufe +II vor. Diese sind meist farblose, salzartige Feststoffe. Charakteristisch für Bariumverbindungen ist die grüne Flammenfärbung.

Es existieren zwei verschiedene Barium-Sauerstoff-Verbindungen, Bariumoxid und Bariumperoxid. Bariumoxid adsorbiert Wasser und Kohlenstoffdioxid und wird dementsprechend eingesetzt. Bariumperoxid, das aus Bariumoxid hergestellt werden kann, ist ein starkes Oxidationsmittel und wird in der Pyrotechnik verwendet. Es ist daneben ein mögliches Edukt für die Herstellung von Wasserstoffperoxid. Wird Bariumoxid in Wasser gelöst, bildet sich die starke Base Bariumhydroxid, die unter anderem als Nachweis für Carbonat-Ionen dient.

Mit den Halogenen bildet Barium jeweils Verbindungen des Typs BaX. Mit Ausnahme von Bariumfluorid, das in der Fluoritstruktur kristallisiert, kristallisieren sie in der Blei(II)-chlorid-Struktur. Bariumfluorid besitzt einen großen transparenten Bereich und wird in der optischen Industrie eingesetzt, das giftige und gut lösliche Bariumchlorid ist Grundstoff für andere Bariumverbindungen und dient als Fällungsmittel für Sulfat, etwa zum Nachweis oder zur Enthärtung.

Bariumsulfat ist die technisch wichtigste Bariumverbindung. Es besitzt im Vergleich zu anderen Bariumverbindungen den Vorteil, auf Grund der sehr geringen Löslichkeit ungiftig zu sein. Es wird vor allem in der Erdölförderung für die Erhöhung der Dichte von Bohrschlämmen eingesetzt. Daneben dient es als Füllstoff für Kunststoffe, als Röntgenkontrastmittel und wird als Anstrichfarbe eingesetzt.

Bariumcarbonat ist ein wirksames Rattengift, es wird auch als Rohstoff zur Glasherstellung sowie bei der Produktion hartmagnetischer Ferrite verwendet.

Bariumnitrat, Bariumiodat und Bariumchlorat werden wegen ihrer brandfördernden Eigenschaften und der grünen Flammenfärbung in der Pyrotechnik benutzt.

Weitere Bariumverbindungen finden sich in der 




</doc>
<doc id="490" url="https://de.wikipedia.org/wiki?curid=490" title="Beryllium">
Beryllium

Beryllium ist ein chemisches Element mit dem Symbol Be und der Ordnungszahl 4. Der Name lässt sich vom Mineral Beryll, einem berylliumhaltigen Schmuckstein, ableiten ( "beryllos", ). Im Periodensystem steht Beryllium in der zweiten Hauptgruppe, bzw. der 2. IUPAC-Gruppe, und zählt daher zu den Erdalkalimetallen. Als Element der zweiten Periode zählt es zu den leichten Erdalkalimetallen. Bemerkenswert ist jedoch, dass es eine höhere Dichte als seine beiden Homologen Magnesium und Calcium hat. Das stahlgraue Leichtmetall ist sehr hart und spröde und wird meist als Legierungszusatz verwendet. In Verbindungen ist es zweiwertig.

Durch einen Hinweis von Abbé R. J. Haüy isolierte der französische Chemiker Louis-Nicolas Vauquelin 1798 das Beryllium in Form seines Oxids aus den Edelsteinen Beryll und Smaragd. Kurz darauf stellte Martin Heinrich Klaproth die gleiche Verbindung her, welche er Beryllium (nach dem Mineral Beryll) nannte.

Das chemische Symbol Be wurde 1814 von J.J. Berzelius eingeführt.

Erst 1828 gelang es Friedrich Wöhler und Antoine Bussy, das Element durch die Reduktion des Berylliumchlorids mit Kalium darzustellen.

Reines Beryllium wurde erstmals 1899 von Paul Marie Alfred Lebeau durch Schmelzflusselektrolyse von Natriumtetrafluoridoberyllat (Na[BeF]) hergestellt. Nach dem Ersten Weltkrieg wurde Beryllium gleichzeitig von Siemens & Halske AG (Alfred Stock und Hans Goldschmidt) in Deutschland und von Union Carbide and Carbon Corporation (Hugh S. Cooper) in den Vereinigten Staaten produziert.

Wegen des süßen Geschmackes der Berylliumsalze wurde in Frankreich noch bis 1957 Vauquelins Namensvorschlag "Glucinium" (griech. γλυκύς = süß) verwendet.

Im Altertum und Mittelalter dienten durchsichtige Beryllstücke vielfach als Zauberglas, da es, wie eine Lupe, Schriftzüge und Bilder beim Lesen vergrößert. Vom Wort Beryll leitet sich die Bezeichnung "Brille" (lat. ) ab, ursprünglich für ein Augenglas aus Beryll. 1945 wurde Beryllium zusammen mit dem Alphastrahler Polonium als Neutronenquelle in der Atombombe "Little Boy", die über Hiroshima abgeworfen wurde, eingesetzt.

Mit einem Massenanteil von 5,3 ppm in der Erdhülle steht Beryllium an 48. Stelle der Elementhäufigkeit. Das seltene Element kommt in rund 30 verschiedenen Mineralien vor. Die wichtigsten sind Bertrandit (4 BeO·2 SiO·HO) (Vereinigte Staaten) und Beryll (BeAl(SiO)) (Volksrepublik China, Russland und Brasilien). Die schönsten und wertvollsten beryllhaltigen Mineralien sind die Schmuck- und Edelsteine Aquamarin, Smaragd, Roter Beryll, Euklas, Gadolinit, Chrysoberyll, Phenakit und Alexandrit. Berylliumerz-Lagerstätten finden sich bevorzugt im Äquatorialgürtel. Frühere, mittlerweile erschöpfte Lagerstätten lagen nördlich zu Fuße der Hohen Tauern um Bramberg in Österreich. In den USA werden niedrighaltige Lagerstätten von Berylliumoxid-Erz in der Nevada-Wüste abgebaut. Die geschätzten Vorräte an förderbarem Beryllium liegen weltweit bei etwa 80.000 Tonnen.

Elementares Beryllium lässt sich durch Reduktion von Berylliumfluorid mit Magnesium bei 900 °C herstellen.

Die Herstellung hochreinen, metallischen Berylliums erfolgt durch Schmelzflusselektrolyse von Berylliumchlorid oder -fluorid:

Die Weltjahresproduktion an Beryllium-Metall betrug 2004 etwa 100 t.

Beryllium besitzt für ein Leichtmetall einen bemerkenswert hohen Schmelzpunkt. Neben der sehr hohen spezifischen Wärmekapazität von 1,825 kJ/(kg·K) besitzt es einen um ein Drittel höheren Elastizitätsmodul als Stahl. Die molare Wärmekapazität ist mit 16.45 J/(mol·K) jedoch deutlich kleiner als die der meisten anderen Metalle. Auch die Schwingungsdämpfung ist sehr hoch. Da es nur vier Elektronen pro Atom hat, ist es sehr durchlässig für Röntgenstrahlen. Alphastrahlung kann aus Beryllium Neutronen freisetzen:


</doc>
<doc id="491" url="https://de.wikipedia.org/wiki?curid=491" title="Berkelium">
Berkelium

Berkelium ist ein künstlich erzeugtes chemisches Element mit dem Elementsymbol Bk und der Ordnungszahl 97. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Berkelium wurde nach der Stadt Berkeley in Kalifornien benannt, in der es entdeckt wurde. Bei Berkelium handelt es sich um ein radioaktives Metall mit einem silbrig-weißen Aussehen. Es wurde im Dezember 1949 erstmals aus dem leichteren Element Americium erzeugt. Es entsteht in geringen Mengen in Kernreaktoren. Seine Anwendung findet es vor allem zur Erzeugung höherer Transurane und Transactinoide.

So wie Americium (Ordnungszahl 95) und Curium (96) in den Jahren 1944 und 1945 nahezu zeitgleich entdeckt wurden, erfolgte in ähnlicher Weise in den Jahren 1949 und 1950 die Entdeckung der Elemente Berkelium (97) und Californium (98).

Die Experimentatoren, Glenn T. Seaborg, Albert Ghiorso und Stanley G. Thompson, stellten am 19. Dezember 1949 die ersten Kerne im 60-Zoll-Cyclotron der Universität von Kalifornien in Berkeley her. Es war das fünfte Transuran, das entdeckt wurde. Die Entdeckung wurde zeitgleich mit der des Californiums veröffentlicht.

Die Namenswahl folgte naheliegenderweise einem gemeinsamen Ursprung: Berkelium wurde nach dem Fundort, der Stadt "Berkeley" in Kalifornien, benannt. Die Namensgebung folgt somit wie bei vielen Actinoiden und den Lanthanoiden: Terbium, das im Periodensystem genau über Berkelium steht, wurde nach der schwedischen Stadt Ytterby benannt, in der es zuerst entdeckt wurde: "It is suggested that element 97 be given the name berkelium (symbol Bk) after the city of Berkeley in a manner similar to that used in naming its chemical homologue terbium (atomic number 65) whose name was derived from the town of Ytterby, Sweden, where the rare earth minerals were first found." Für das Element 98 wählte man den Namen "Californium" zu Ehren der Universität und des Staates Kalifornien.

Als schwerste Schritte in der Vorbereitung zur Herstellung des Elements erwiesen sich die Entwicklung entsprechender chemischer Separationsmethoden und die Herstellung ausreichender Mengen an Americium für das Target-Material.

Die Probenvorbereitung erfolgte zunächst durch Auftragen von Americiumnitratlösung (mit dem Isotop Am) auf eine Platinfolie; die Lösung wurde eingedampft und der Rückstand dann zum Oxid (AmO) geglüht.

Nun wurde diese Probe im 60-Zoll-Cyclotron mit beschleunigten α-Teilchen mit einer Energie von 35 MeV etwa 6 Stunden beschossen. Dabei entsteht in einer sogenannten (α,2n)-Reaktion Bk sowie zwei freie Neutronen:

Nach dem Beschuss im Cyclotron wurde die Beschichtung mittels Salpetersäure gelöst und erhitzt, anschließend wieder mit einer konzentrierten wässrigen Ammoniak-Lösung als Hydroxid ausgefällt und abzentrifugiert; der Rückstand wurde wiederum in Salpetersäure gelöst.

Um die weitgehende Abtrennung des Americiums zu erreichen, wurde diese Lösung mit einem Gemisch von Ammoniumperoxodisulfat und Ammoniumsulfat versetzt und erhitzt, um vor allem das gelöste Americium auf die Oxidationsstufe +6 zu bringen. Nicht oxidiertes restliches Americium wurde durch Zusatz von Flusssäure als Americium(III)-fluorid ausgefällt. Auf diese Weise werden auch begleitendes Curium als Curium(III)-fluorid und das erwartete Element 97 (Berkelium) als Berkelium(III)-fluorid ausgefällt. Dieser Rückstand wurde durch Behandlung mit Kalilauge zum Hydroxid umgewandelt, welches nach Abzentrifugieren nun in Perchlorsäure gelöst wurde.
Die weitere Trennung erfolgte in Gegenwart eines Citronensäure/Ammoniumcitrat-Puffers im schwach sauren Medium (pH ≈ 3,5) mit Ionenaustauschern bei erhöhter Temperatur.

Die chromatographische Trennung konnte nur aufgrund vorheriger Vergleiche mit dem chemischen Verhalten der entsprechenden Lanthanoide gelingen. So tritt bei einer Trennung das Terbium vor Gadolinium und Europium aus einer Säule. Falls das chemische Verhalten des Berkeliums dem eines Eka-Terbiums ähnelt, sollte das fragliche Element 97 daher in dieser analogen Position zuerst erscheinen, entsprechend vor Curium und Americium.

Der weitere Verlauf des Experiments brachte zunächst kein Ergebnis, da man nach einem α-Teilchen als Zerfallssignatur suchte. Erst die Suche nach charakteristischer Röntgenstrahlung und Konversionselektronen als Folge eines Elektroneneinfangs brachte den gewünschten Erfolg. Das Ergebnis der Kernreaktion wurde mit Bk angegeben, obwohl man anfänglich auch Bk für möglich hielt.

Im Jahr 1958 isolierten Burris B. Cunningham und Stanley G. Thompson erstmals wägbare Mengen, die durch langjährige Neutronenbestrahlung von Pu in dem Testreaktor der National Reactor Testing Station in Idaho erzeugt wurden.

Von Berkelium existieren nur Radionuklide und keine stabilen Isotope. Insgesamt sind 12 Isotope und 5 Kernisomere des Elements bekannt. Die langlebigsten sind Bk (Halbwertszeit 1380 Jahre), Bk (9 Jahre) und Bk (330 Tage). Die Halbwertszeiten der restlichen Isotope liegen im Bereich von Millisekunden bis Stunden oder Tagen.

Nimmt man beispielhaft den Zerfall des langlebigsten Isotops Bk heraus, so entsteht durch α-Zerfall zunächst das langlebige Am, das seinerseits durch erneuten α-Zerfall in Np übergeht. Der weitere Zerfall führt dann über Pu zum U, dem Beginn der Uran-Actinium-Reihe (4 n + 3).

"→ Liste der Berkeliumisotope"

Berkeliumisotope kommen auf der Erde wegen ihrer im Vergleich zum Alter der Erde zu geringen Halbwertszeit nicht natürlich vor.

Über die Erstentdeckung von Einsteinium und Fermium in den Überresten der ersten amerikanischen Wasserstoffbombe, Ivy Mike, am 1. November 1952 auf dem Eniwetok-Atoll hinaus wurden neben Plutonium und Americium auch Isotope von Curium, Berkelium und Californium gefunden, darunter das Bk, das durch den β-Zerfall in Cf übergeht. Aus Gründen der militärischen Geheimhaltung wurden die Ergebnisse erst später im Jahr 1956 publiziert.

In Kernreaktoren entsteht vor allem das Berkeliumisotop Bk, es zerfällt bereits während der Zwischenlagerung (vor der Endlagerung) fast komplett zum Californiumisotop Cf mit 351 Jahren Halbwertszeit. Dieses zählt zum Transuranabfall und ist daher in der Endlagerung unerwünscht.

Berkelium wird durch Beschuss von leichteren Actinoiden mit Neutronen in einem Kernreaktor erzeugt. Die Hauptquelle ist der 85 MW High-Flux-Isotope Reactor (HFIR) am Oak Ridge National Laboratory in Tennessee, USA, der auf die Herstellung von Transcuriumelementen (Z > 96) eingerichtet ist.

Berkelium entsteht in Kernreaktoren aus Uran (U) oder Plutonium (Pu) durch zahlreiche nacheinander folgende Neutroneneinfänge und β-Zerfälle – unter Ausschluss von Spaltungen oder α-Zerfällen.

Ein wichtiger Schritt ist hierbei die (n,γ)- oder Neutroneneinfangsreaktion, bei welcher das gebildete angeregte Tochternuklid durch Aussendung eines γ-Quants in den Grundzustand übergeht. Die hierzu benötigten freien Neutronen entstehen durch Kernspaltung anderer Kerne im Reaktor. In diesem kernchemischen Prozess wird zunächst durch eine (n,γ)-Reaktion gefolgt von zwei β-Zerfällen das Pu gebildet. In Brutreaktoren wird dieser Prozess zum Erbrüten neuen Spaltmaterials genutzt.

Letzteres wird hierzu mit einer Neutronenquelle, die einen hohen Neutronenfluss besitzt, bestrahlt. Die hierbei möglichen Neutronenflüsse sind um ein Vielfaches höher als in einem Kernreaktor. Aus Pu wird durch vier aufeinander folgende (n,γ)-Reaktionen Pu gebildet, welches durch β-Zerfall mit einer Halbwertszeit von 4,96 Stunden zu Am zerfällt. Das durch eine weitere (n,γ)-Reaktion gebildete Am zerfällt wiederum durch β-Zerfall mit einer Halbwertszeit von 10,1 Stunden letztlich zu Cm. Aus Cm entstehen durch weitere (n,γ)-Reaktionen im Reaktor in jeweils kleiner werdenden Mengen die nächst schwereren Isotope.

Die Entstehung von Cm auf diesem Wege ist jedoch sehr unwahrscheinlich, da Cm nur eine kurze Halbwertszeit besitzt und so weitere Neutroneneinfänge in der kurzen Zeit unwahrscheinlich sind.

Bk ist das einzige Isotop des Berkeliums, das auf diese Weise gebildet werden kann. Es entsteht durch β-Zerfall aus Cm – das erste Curiumisotop, welches einen β-Zerfall eingeht (Halbwertszeit 64,15 min).

Durch Neutroneneinfang entsteht zwar aus Bk auch das Bk, dies zerfällt aber schon mit einer Halbwertszeit von 3,212 Stunden durch β-Zerfall zu Cf.

Das langlebigste Isotop, das Bk, kann somit nicht in Kernreaktoren hergestellt werden, so dass man sich oftmals mit dem eher zugänglichen Bk begnügen muss. Berkelium steht heute weltweit lediglich in sehr geringen Mengen zur Verfügung, weshalb es einen sehr hohen Preis besitzt. Dieser beträgt etwa 185 US-Dollar pro Mikrogramm Bk.

Das Isotop Bk wurde 1956 durch Beschuss mit 25-MeV α-Teilchen aus einem Gemisch von Curiumnukliden hergestellt. Seine Existenz mit dessen Halbwertszeit von 23 ± 5 Stunden wurde durch das β-Zerfallsprodukt Cf festgestellt.

Bk wurde 1965 aus Cm durch Beschuss mit α-Teilchen hergestellt. Ein eventuell entstandenes Isotop Bk konnte nicht nachgewiesen werden.

Das Berkeliumisotop Bk wurde 1979 durch Beschuss von U mit B, U mit B, sowie Th mit N bzw. N erzeugt. Es wandelt sich durch Elektroneneinfang mit einer Halbwertszeit von 7,0 ± 1,3 Minuten zum Cm um. Eine Suche nach einem zunächst vermuteten Isotop Bk blieb ohne Erfolg.

Die ersten Proben von Berkeliummetall wurden 1969 durch Reduktion von BkF bei 1000 °C mit Lithium in Reaktionsapparaturen aus Tantal hergestellt.

Elementares Berkelium kann ferner auch aus BkF mit Lithium oder durch Reduktion von BkO mit Lanthan oder Thorium dargestellt werden.

Im Periodensystem steht das Berkelium mit der Ordnungszahl 97 in der Reihe der Actinoide, sein Vorgänger ist das Curium, das nachfolgende Element ist das Californium. Sein Analogon in der Reihe der Lanthanoide ist das Terbium.

Berkelium ist ein radioaktives Metall mit silbrig-weißem Aussehen und einem Schmelzpunkt von 986 °C.

Die bei Standardbedingungen auftretende Modifikation α-Bk kristallisiert im hexagonalen Kristallsystem in der mit den Gitterparametern "a" = 341,6 ± 0,3 pm und "c" = 1106,9 ± 0,7 pm sowie vier Formeleinheiten pro Elementarzelle, einem Metallradius von 170 nm und einer Dichte von 14,78 g/cm. Die Kristallstruktur besteht aus einer doppelt-hexagonal dichtesten Kugelpackung (d. h.c.p.) mit der Schichtfolge ABAC und ist damit isotyp zur Struktur von α-La.

Bei höheren Temperaturen geht α-Bk in β-Bk über. Die β-Modifikation kristallisiert im kubischen Kristallsystem in der Raumgruppe  mit dem Gitterparameter "a" = 499,7 ± 0,4 pm, einem Metallradius von 177 nm und einer Dichte von 13,25 g/cm. Die Kristallstruktur besteht aus einer kubisch dichtesten Kugelpackung mit der Stapelfolge ABC, was einem kubisch flächenzentrierten Gitter (f.c.c.) entspricht.

Die Lösungsenthalpie von Berkelium-Metall in Salzsäure bei Standardbedingungen beträgt −600,2 ± 5,1 kJ·mol. Ausgehend von diesem Wert erfolgte die erstmalige Berechnung der Standardbildungsenthalpie (Δ"H") von Bk auf −601 ± 5 kJ·mol und des Standardpotentials Bk / Bk auf −2,01 ± 0,03 V.

Zwischen 70 K und Raumtemperatur verhält sich Berkelium wie ein Curie-Weiss-Paramagnet mit einem effektiven magnetischen Moment von 9,69 Bohrschen Magnetonen (µ) und einer Curie-Temperatur von 101 K. Beim Abkühlen auf etwa 34 K erfährt Berkelium einen Übergang zu einem antiferromagnetischen Zustand. Dieses magnetische Moment entspricht fast dem theoretischen Wert von 9,72 µ.

Berkelium ist wie alle Actinoide sehr reaktionsfähig. Es reagiert allerdings nicht schnell mit Sauerstoff bei Raumtemperatur, was möglicherweise auf die Bildung einer schützenden Oxidschicht zurückzuführen ist. Jedoch reagiert es mit geschmolzenen Metallen, Wasserstoff, Halogenen, Chalkogenen und Penteliden zu verschiedenen binären Verbindungen.

In wässriger Lösung ist die dreiwertige Oxidationsstufe am beständigsten, jedoch kennt man auch vierwertige und zweiwertige Verbindungen. Wässrige Lösungen mit Bk-Ionen haben eine gelbgrüne Farbe, mit Bk-Ionen sind sie in salzsaurer Lösung beige, in schwefelsaurer Lösung orange-gelb. Ein ähnliches Verhalten ist für sein Lanthanoidanalogon Terbium zu beobachten.

Bk-Ionen zeigen zwei scharfe Fluoreszenzpeaks bei 652 nm (rotes Licht) und 742 nm (dunkelrot – nahes Infrarot) durch interne Übergänge in der f-Elektronen-Schale.

Berkelium eignet sich anders als die benachbarten Elemente Curium und Californium auch theoretisch nur sehr schlecht als Kernbrennstoff in einem Reaktor. Neben der sehr geringen Verfügbarkeit und dem damit verbundenen hohen Preis kommt hier erschwerend hinzu, dass die günstigeren Isotope mit gerader Massenzahl nur eine geringe Halbwertszeit haben. Das einzig infrage kommende geradzahlige Isotop, Bk im Grundzustand, ist nur sehr schwer zu erzeugen, zudem liegen derzeit (9/2008) keine ausreichenden Daten über dessen Wirkungsquerschnitte vor.

Bk ist im Prinzip in der Lage, eine Kettenreaktion aufrechtzuerhalten, und damit für einen schnellen Reaktor oder eine Atombombe geeignet. Die kurze Halbwertszeit von 330 Tagen zusammen mit der komplizierten Gewinnung und dem hohen Bedarf vereiteln entsprechende Versuche. Die kritische Masse liegt unreflektiert bei 192 kg, mit Wasserreflektor immer noch 179 kg, ein Vielfaches der Weltjahresproduktion.

Bk kann sowohl in einem thermischen als auch in einem schnellen Reaktor eine Kettenreaktion aufrechterhalten und hat mit 1380 Jahren eine ausreichend große Halbwertszeit, um sowohl als Kernbrennstoff als auch Spaltstoff für eine Atombombe zu dienen. Es kann allerdings nicht in einem Reaktor erbrütet werden und ist damit in der Produktion noch aufwendiger und kostenintensiver als die anderen genannten Isotope. Damit einher geht eine noch geringere Verfügbarkeit, was angesichts der benötigten Masse von mindestens 35,2 kg (kritische Masse mit Stahlreflektor) als Ausschlusskriterium angesehen werden kann.

Die Verwendung für Berkeliumisotope beruht hauptsächlich in der wissenschaftlichen Grundlagenforschung. Bk ist ein gängiges Nuklid zur Synthese noch schwererer Transurane und Transactinoide wie Lawrencium, Rutherfordium und Bohrium. Es dient auch als Quelle für das Isotop Cf, welches Studien über die Chemie des Californiums ermöglicht. Es hat den Vorzug vor dem radioaktiveren Cf, welches ansonsten durch Neutronenbeschuss im High-Flux-Isotope Reactor (HFIR) erzeugt wird.

Eine 22-Milligramm-Probe Bk wurde im Jahr 2009 in einer 250-Tage-Bestrahlung hergestellt und dann in einem 90-Tage-Prozess in Oak Ridge gereinigt. Diese Probe führte zu den ersten 6 Atomen des Elements Tenness am Vereinigten Institut für Kernforschung (JINR), Dubna, Russland, nach einem Beschuss mit Calcium-Ionen im U400-Zyklotron für 150 Tage. Diese Synthese war ein Höhepunkt der russisch-amerikanischen Zusammenarbeit zwischen JINR und Lawrence Livermore National Laboratory bei der Synthese der Elemente 113 bis 118, die 1989 gestartet wurde.

"→ Kategorie: "

Obwohl das Isotop Bk die längste Halbwertszeit ausweist, ist das Isotop Bk leichter zugänglich und wird überwiegend für die Bestimmung der chemischen Eigenschaften herangezogen.

Von Berkelium existieren Oxide der Oxidationsstufen +3 (BkO) und +4 (BkO).

Berkelium(IV)-oxid (BkO) ist ein brauner Feststoff und kristallisiert im kubischen Kristallsystem in der Fluorit-Struktur in der mit den Koordinationszahlen Cf[8], O[4]. Der Gitterparameter beträgt 533,4 ± 0,5 pm.

Berkelium(III)-oxid (BkO) entsteht aus BkO durch Reduktion mit Wasserstoff:

Es ist ein gelbgrüner Feststoff mit einem Schmelzpunkt von 1920 °C. Es bildet ein kubisch-raumzentriertes Kristallgitter mit "a" = 1088,0 ± 0,5 pm.

Halogenide sind für die Oxidationsstufen +3 und +4 bekannt. Die stabilste Stufe +3 ist für sämtliche Verbindungen von Fluor bis Iod bekannt und auch in wässriger Lösung stabil. Die vierwertige Stufe ist nur in der festen Phase stabilisierbar.

Berkelium(IV)-fluorid (BkF) ist eine gelbgrüne Ionenverbindung und kristallisiert im monoklinen Kristallsystem und ist isotyp mit Uran(IV)-fluorid.

Berkelium(III)-fluorid (BkF) ist ein gelbgrüner Feststoff und besitzt zwei kristalline Strukturen, die temperaturabhängig sind (Umwandlungstemperatur: 350 bis 600 °C). Bei niedrigen Temperaturen ist die orthorhombische Struktur (YF-Typ) zu finden. Bei höheren Temperaturen bildet es ein trigonales System (LaF-Typ).

Berkelium(III)-chlorid (BkCl) ist ein grüner Feststoff mit einem Schmelzpunkt von 603 °C und kristallisiert im hexagonalen Kristallsystem. Seine Kristallstruktur ist isotyp mit Uran(III)-chlorid (UCl). Das Hexahydrat (BkCl · 6 HO) weist eine monokline Kristallstruktur auf.

Berkelium(III)-bromid (BkBr) ist ein gelbgrüner Feststoff und kristallisiert bei niedrigen Temperaturen im PuBr-Typ, bei höheren Temperaturen im AlCl-Typ.

Berkelium(III)-iodid (BkI) ist ein gelber Feststoff und kristallisiert im hexagonalen System (BiI-Typ).

Die Oxihalogenide BkOCl, BkOBr und BkOI besitzen eine tetragonale Struktur vom PbFCl-Typ.

Berkelium(III)-sulfid (BkS) wurde entweder durch Behandeln von Berkelium(III)-oxid mit einem Gemisch von Schwefelwasserstoff und Kohlenstoffdisulfid bei 1130 °C dargestellt, oder durch die direkte Umsetzung von metallischem Berkelium mit Schwefel. Dabei entstanden bräunlich-schwarze Kristalle mit kubischer Symmetrie und einer Gitterkonstanten von "a" = 844 pm.

Die Pentelide des Berkeliums (Bk) des Typs BkX sind für die Elemente Stickstoff, Phosphor, Arsen und Antimon dargestellt worden. Ihre Herstellung erfolgt durch die Reaktion von entweder Berkelium(III)-hydrid (BkH) oder metallischem Berkelium mit diesen Elementen bei erhöhter Temperatur im Hochvakuum in Quarzampullen. Sie kristallisieren im NaCl-Gitter mit den Gitterkonstanten 495,1 pm für BkN, 566,9 pm für BkP, 582,9 pm für BkAs und 619,1 pm für BkSb.

Berkelium(III)- und Berkelium(IV)-hydroxid sind beide als Suspension in 1 M Natronlauge stabil und wurden spektroskopisch untersucht. Berkelium(III)-phosphat (BkPO) wurde als Feststoff dargestellt, der eine starke Fluoreszenz bei einer Anregung durch einen Argon-Laser (514,5 nm-Linie) zeigt.

Weitere Salze des Berkeliums sind bekannt, z. B. BkOS, (BkNO) · 4 HO, BkCl · 6 HO, Bk(SO) · 12 HO und Bk(CO) · 4 HO. Eine thermische Zersetzung in einer Argonatmosphäre bei ca. 600 °C (um eine Oxidation zum BkO vermeiden) von Bk(SO) · 12 HO führt zu raumzentrierten orthorhombischen Kristallen von Berkelium(III)-oxisulfat (BkOSO). Diese Verbindung ist unter Schutzgas bis mindestens 1000 °C thermisch stabil.

Berkeliumhydride werden durch Umsetzung des Metalls mit Wasserstoffgas bei Temperaturen über 250 °C hergestellt. Sie bilden nicht-stöchiometrische Zusammensetzungen mit der nominalen Formel BkH (0 < x < 1). Während die Trihydride eine hexagonale Symmetrie besitzen, kristallisiert das Dihydrid in einer "fcc"-Struktur mit der Gitterkonstanten "a" = 523 pm.

Berkelium bildet einen trigonalen (η–CH)Bk-Komplex mit drei Cyclopentadienylringen, die durch Umsetzung von Berkelium(III)-chlorid mit geschmolzenem Be(CH) bei etwa 70 °C synthetisiert werden können. Es besitzt eine gelbe Farbe und orthorhombische Symmetrie mit den Gitterkonstanten "a" = 1411 pm, "b" = 1755 pm und "c" = 963 pm sowie einer berechneten Dichte von 2,47 g/cm. Der Komplex ist bis mindestens 250 °C stabil und sublimiert bei ca. 350 °C. Die hohe Radioaktivität bewirkt allerdings eine schnelle Zerstörung der Verbindungen innerhalb weniger Wochen. Ein CH-Ring im (η–CH)Bk kann durch Chlor ersetzt werden, wobei das dimere [Bk(CH)Cl] entsteht. Das optische Absorptionsspektrum dieser Verbindung ist sehr ähnlich zum (η–CH)Bk.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.




</doc>

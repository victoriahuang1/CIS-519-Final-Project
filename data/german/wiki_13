<doc id="1370" url="https://de.wikipedia.org/wiki?curid=1370" title="Émile Durkheim">
Émile Durkheim

David Émile Durkheim [] (* 15. April 1858 in Épinal, Frankreich; † 15. November 1917 in Paris) war ein französischer Soziologe und Ethnologe. Er war 1887 als Lehrbeauftragter für Soziologie und Pädagogik in Bordeaux der erste mit einer akademischen Stelle an einer französischen Universität. Er gilt heute als ein Klassiker der Soziologie, der mit seiner Methodologie die Eigenständigkeit der Soziologie als Fachdisziplin zu begründen gesucht hat, und dessen empirische Studie zum Thema Selbsttötung zum Paradigma empirischer Soziologie wurde.

Émile Durkheim war der Sohn des Rabbiners in Épinal (Lothringen) Moise Durkheim (1805–1896) und Enkel des Abraham Israel Durkheim, geboren 1766 im heutigen Bad Dürkheim. Durkheim studierte in Paris an der École normale supérieure, nachdem er zweimal bei der Aufnahmeprüfung durchgefallen war. Er traf dort auf eine Reihe von später ebenfalls sehr renommierten Männern, darunter Lucien Lévy-Bruhl und Jean Jaurès.

Nach seinem Abschluss war Durkheim zunächst als Lehrer für Philosophie an Gymnasien tätig. Nach einem Studienaufenthalt in Deutschland in den Jahren 1885–1886 publizierte er zwei Artikel über seine Stipendienzeit in Berlin und Leipzig. Sie machten ihn bekannt und führten dazu, dass er vom Leiter der Hochschulabteilung im Erziehungsministerium 1887 einen Lehrauftrag für Sozialwissenschaft in Bordeaux erhielt, wo er schließlich Professor für Pädagogik und Soziologie wurde – die erste Dozentur für Soziologie an einer französischen Universität.

In seiner Zeit in Bordeaux verfasste Durkheim drei seiner wichtigsten Schriften: "Über soziale Arbeitsteilung" (seine Dissertationsschrift, 1893), "Die Regeln der soziologischen Methode" (1895) und "Der Selbstmord" (1897). 1896 gründete er die Zeitschrift "L’Année Sociologique", von der er zwölf Jahrgänge herausgab und zu der eine Gruppe von Gleichgesinnten und Durkheims Schülern wesentlich beitrugen.

1902 nahm Durkheim eine Lehrtätigkeit an der Pariser Universität Sorbonne auf, wo er 1906 einen Lehrstuhl für Erziehungswissenschaft erhielt, der 1913 in "Erziehungswissenschaft und Soziologie" umbenannt wurde.

Bereits in seiner ersten, in Latein verfassten und 1892 abgeschlossenen Dissertation setzt sich Durkheim mit Montesquieu und Jean-Jacques Rousseau auseinander.

In "De la division du travail social" (1893) entwirft Durkheim ein grundlegendes Modell von Gesellschaft entlang der folgenden Frage:

Nach Durkheim unterscheiden sich Gesellschaftsstrukturen durch unterschiedliche Formen der Solidarität, wobei er in zweierlei Arten unterteilt:


Das Prinzip der „organischen Solidarität“ versteht Durkheim als Gegenposition zum Utilitarismus, namentlich desjenigen Herbert Spencers. So geartete moderne Kollektive bezeichnet Durkheim als "„nicht-segmentäre“ Gesellschaften". Die Industriegesellschaft hat nach Durkheim eine differenzierte, hochentwickelte und komplexe Arbeitsteilung von solchen Ausmaßen, dass der Einzelne sie nicht mehr überblicken kann. Tatsächlich ist der Einzelne in dieser arbeitsteiligen Gesellschaft überaus abhängig, jedoch entwickelt er eine Ideologie, die genau das Gegenteil sagt – nämlich den Individualismus. Durkheim zeigte dieses Paradoxon der Industriegesellschaft erstmals auf. Andere, wenig oder nicht-industrialisierte Gesellschaften kennzeichnet eine viel einfachere und überschaubarere Arbeitsteilung.

Durkheim geht in diesem Werk davon aus, dass „soziale Fakten als Dinge (zu) behandeln“ sind, d. h. der soziale Tatbestand stellt für ihn die Grundlage aller soziologischen Analyse dar und ist keine bloße „Nebenerscheinung“ von menschlichem Zusammenleben, sondern als Struktur mit eigenem Stellenwert zu betrachten.

Eine soziale Struktur erklärt sich also für Durkheim nicht aus der Summe der Vorstellungen der beteiligten Akteure und existiert unabhängig von denen, die sie erschaffen haben (Emergenzphänomen). Sie wirkt als „Gesellschaft“ von oben auf die Menschen ein und kann von der Soziologie als solche aufgedeckt und durch funktionale (= Wirkung) und historische (= Entstehung) Analyse erklärt werden. Nach Durkheim sind beide Aspekte unbedingt zu beachten. Die moderne Schichtung der Gesellschaft kann also zum Beispiel nicht lediglich dadurch erklärt werden, dass Berufspositionen mit verschiedenen Entlohnungen versehen werden, um sie attraktiver zu machen, weil dabei nur die Wirkung betrachtet würde.

Durkheim gibt drei Kriterien für soziale Strukturen („Gesellschaft“) an:

Das kollektive Gewissen oder auch kollektive Bewusstsein "(conscience collective)" der Gesellschaft, in der man geboren wurde, wird durch Erziehung in den Einzelnen hineingetragen und schlägt sich in dessen Moralvorstellungen, Sitten und Glauben nieder.

Nach Durkheim ist der kollektive Zwang nicht direkt beobachtbar, aber in der negativen Sanktionierung von abweichenden, d. h. regelwidrigen Verhaltensweisen feststellbar und messbar. Wenn diese Abweichung in der Gesellschaft zur Regel wird, das kollektive Gewissen also nicht mehr in der Lage ist, für die Aufrechterhaltung der Ordnung zu sorgen, spricht man von „Anomie“. Dies bedeutet, dass die Gesellschaft vom „Normalzustand“ in einen „pathologischen“ Zustand übergegangen ist.

Im sechsten Kapitel („Regeln der Beweisführung“) bestimmt Durkheim die Methode der kulturvergleichenden Sozialforschung als „die einzige, welche der Soziologie entspricht.“ (1991, S. 205), vgl. Vergleich (Philosophie). Im ersten Abschnitt (I) setzt sich Durkheim kritisch mit Comte und John Stuart Mill auseinander. Im zweiten Abschnitt (II) untersucht Durkheim vier verschiedene Verfahren der vergleichenden Methode:
Die ersten drei Verfahren eignen sich Durkheim zufolge nicht für die Untersuchung sozialer Phänomene, da solche Phänomene zu komplex sind. Dagegen hält Durkheim das Verfahren der parallelen Variationen für ein „ausgezeichnete[s] Instrument der soziologischen Forschung“ (1991, S. 211). Im dritten und letzten Abschnitt (III) behandelt Durkheim den Vergleich mehrerer Gesellschaften.

In "Le suicide" untersucht Durkheim verschiedene Hypothesen zu den unterschiedlichen Suizidraten von Katholiken und Protestanten. Er benutzt hierzu empirische Daten aus verschiedenen Quellen, vor allem aus Moralstatistiken von Adolph Wagner oder Henry Morselli (1852–1929) und untersucht Korrelationen mit unterschiedlichen Parametern, der konfessionellen Zugehörigkeit, dem Berufs- und Vermögensstand der Betroffenen bis hin zum Wetter, zur Jahreszeit und zur Wirtschaftssituation des Landes. Er erklärt die niedrigere Selbsttötungsrate bei Katholiken durch die stärkere soziale Kontrolle und die stärkere soziale Integration. In den Quellen waren die Ergebnisse mit größerer Zurückhaltung dargestellt worden. Außerdem war der Unterschied zwischen den Konfessionen nur in den deutschsprachigen Gebieten Mitteleuropas beobachtbar und kann seinerseits Ausdruck anderer Faktoren gewesen sein.
Durkheims Ergebnisse waren:

In diesem Zusammenhang entwickelt er den Begriff der Anomie, die er als Situation definiert, in der Verwirrung über soziale und/oder moralische Normen herrscht, weil diese unklar oder überhaupt nicht vorhanden sind. Dies führt nach Durkheim zu abweichendem Verhalten. Durkheim nennt in diesem Zusammenhang drei Grundtypen (Idealtypen) des Suizids: die egoistische, die anomische und die altruistische Selbsttötung. Nur in einer Fußnote erwähnt Durkheim einen vierten Typ, die fatalistische Selbsttötung.
Der egoistische Selbstmord ist Ausdruck der mangelnden Integration in eine Gemeinschaft, also das Ergebnis einer Schwächung der sozialen Bindungen des Individuums. Als Beispiel führt Durkheim Unverheiratete an, besonders Männer, die in höherer Zahl Selbstmord begehen als Verheiratete.

Altruistische Selbsttötung ist demgegenüber Ausdruck einer zu starken Bindung an Gruppennormen. Dies findet er vor allem in Gesellschaften, in denen die Bedürfnisse des Einzelnen dem Ziel der Gemeinschaft untergeordnet sind.

Anomische Selbsttötung spiegelt die moralische Verwirrung des Individuums wider, seinen Mangel an gesellschaftlicher Orientierung, oft verbunden mit dramatischem sozialem und ökonomischem Wandel. Er ist die Folge moralischer Deregulierung und fehlender Definition legitimer Ziele durch eine soziale Ethik, die dem Bewusstsein des Einzelnen Sinn und Ordnung vermitteln könnte. Es fehlt hier nach Durkheim vor allem eine wirtschaftliche Entwicklung, die soziale Solidarität produziert. Die Menschen wissen nicht, wo ihr Platz in der Gesellschaft ist. In der entsprechenden moralischen Desorientierung kennen die Menschen nicht mehr die Grenzen ihrer Bedürfnisse und befinden sich in einem Dauerzustand der Enttäuschung. Dies geschieht vor allem bei drastischen Veränderungen der materiellen Bedingungen der Existenz, wirtschaftlicher Ruin oder auch plötzlicher unerwarteter Reichtum: Durch beides werden bisherige Lebenserwartungen infrage gestellt und neue Orientierungen werden erforderlich, bevor die neue Situation und ihre Grenzen richtig eingeschätzt werden können.

Fatalistische Selbsttötung ist das Gegenteil des anomischen. Hier ist ein Mensch in extremem Maße eingeschränkt und erfährt seine Zukunft als vorbestimmt, seine Bedürfnisse werden erstickt. Dies geschieht in geschlossenen und repressiven Gruppen, in denen Menschen den Tod dem Weiterleben unter den gegebenen und nicht zu verändernden Bedingungen vorziehen. Als Beispiel führt Durkheim Gefängnisinsassen an.

Alle vier Typen von Selbsttötung basieren auf hohen Graden von Ungleichgewicht zwischen zwei gesellschaftlichen Kräften: Integration und moralischer Regulierung. Durkheim berücksichtigte bei seiner Untersuchung die Wirkungen von Krisen auf soziale Gefüge, zum Beispiel den Krieg als Ursache für vermehrten Altruismus, wirtschaftlichen Aufschwung oder Depression als Ursache verstärkter Anomie.

Die 1912 erschienenen "Les formes élémentaires de la vie religieuse" (Die elementaren Formen des religiösen Lebens) befassen sich mit der Frage nach dem Wesen der Religion. Mit diesem Werk hat Durkheim diverse Ziele. Zunächst probiert er zu verstehen was die Religion ist, woher es kommt, und was Effekte es in der Gesellschaft hat. Er probiert auch andere soziale Institutionen durch das Studium elementarer menschlichen Beziehungen zu verstehen. Unter anderen, versucht er die soziale Ursprung von Sprache, die Wahrheit, und die Kategorien des Denkens zu verstehen. Durkheim stellt auch die Grundlage für eine funktionalistische Betrachtung der Religion, indem er als ihr wesentliches Kernelement ihre Funktion zur Stiftung gesellschaftlichen Zusammenhalts und gesellschaftlicher Identität ausmacht. 

Als Methodologie, studiert er die einfachste Religion, insbesondere das Totemismus der austalischen Arrernte (Aranda) aber auch nordamerikanische Indianerstämme, die er finden kann. Er probiert nicht die absolute, metaphysische "Ursprung" der Religion zu entdecken, aber will die soziale Konditionen verstehen, die zur Geburt einer Religion führen. Um das zu tun, muss er die Religion in seinem einfachsten Zustand studieren, um das Kern zu wahrnehmen. Diese Methodologie führt zu seiner Analyse des Rituals, wodurch kollektive Energie auf einem heiligen Objekt projiziert wird, um das Symbole der Gruppe und den Kern der Identität der Gruppe zu werden.

Seine Werke wurde kritisiert. Trotzdem, haben seine Theorien und Einblicke eine dauernde Werte und Einfluss. Gemäß Robert Alun Jones, Durkheims Analysen von Religion "have stimulated the interest and excitement of several generations of sociologists irrespective of theoretical 'school' or field of specialization."

Bekannte Schüler Durkheims waren u. a. Marcel Mauss, der Neffe Durkheims, und Maurice Halbwachs. Die Schule um Durkheim und die "Année Sociologique" werden manchmal dafür verantwortlich gemacht, dass Forscher, die Durkheim nicht folgten, wie Gabriel Tarde und Arnold van Gennep, unverdient in Vergessenheit gerieten. Auch nach seinem Tod wirkte Durkheim in Frankreich auf zahlreiche Denker, unter anderem auf die Gründer des Collège de Sociologie (Georges Bataille, Michel Leiris, Roger Caillois) sowie Claude Lévi-Strauss, Michel Foucault und andere aus dem Umfeld des französischen Strukturalismus. Auch Pierre Bourdieu greift wiederholt auf Durkheim zurück.

In Großbritannien setzte sich insbesondere die dortige auch als Sozialanthropologie bekannte Strömung der Ethnologie intensiv mit Durkheim auseinander. Insbesondere die funktionalistischen Spielarten der britischen Sozialanthropologie bei Bronisław Malinowski und Alfred Radcliffe-Brown setzten sich mit Durkheims Werk auseinander.

Durkheims Erbe wurde für die moderne Soziologie vor allem durch Talcott Parsons fruchtbar gemacht, der die Kritik des Utilitarismus in den Vordergrund rückte.

Im deutschsprachigen Raum, wo Durkheim lange Zeit weniger rezipiert wurde als etwa Max Weber und Karl Marx, haben insbesondere René König (unter anderem durch Übersetzung einiger Werke Durkheims) sowie Alphons Silbermann (Mitte der 1970er Jahre in Bordeaux) auf Durkheims Bedeutung hingewiesen.

In jüngster Zeit kam man auf Durkheim wieder zurück, wenn es um eine Theorie geht, die den Wertewandel in der Gesellschaft sowie die Herausbildung der moralischen Autonomie des Individuums erklären kann.

Vor einer vorschnellen Kategorisierung soziologischer Klassiker wie Weber und Durkheim hat Siegwart Lindenberg gewarnt, da diese häufig „doppelsinnig“ arbeiteten. So sei Webers „subjektiv gemeinter Sinn“ nicht individuell, sondern intersubjektiv verstehbar. Und Durkheim habe programmatisch als methodologischer Kollektivist psychologische Erklärungen abgelehnt und dennoch zur Erklärung der Prozesse, die intersubjektiv Sinn erzeugen, psychologische Ansätze herangezogen.






</doc>
<doc id="1373" url="https://de.wikipedia.org/wiki?curid=1373" title="Eisbrecher">
Eisbrecher

Ein Eisbrecher ist ein Schiff, das speziell dafür konstruiert und ausgerüstet ist, durch die zugefrorene See oder zugefrorene Flüsse fahren zu können. Diese Fahrt kann für den Eisbrecher selbst mit seiner Ladung erfolgen, oder um anderen Schiffen eine Fahrrinne freizubrechen und schiffbar zu halten.

Mehrere Bedingungen muss ein Eisbrecher gegenüber normalen Schiffen erfüllen:

Der Rumpf eines im Eis fahrenden Schiffes bedarf einer Eisverstärkung: Stärkere Beplankung innerhalb des Tauchbereichs des Rumpfes, durchgängig doppelte Verschweißung der Außenhaut, verstärkte Innenspanten und engerer Spantenabstand. Eisbrecher sind im Verhältnis zu ihrer Größe besonders breite Schiffe, um eine möglichst breite Fahrrinne zu erzeugen.

Der Bug ist normalerweise so geformt, dass das Eis nicht von einer scharfen Bugkante wie von einem Messer zerschnitten, sondern von der flachen und gewölbten Bugunterseite nach unten gedrückt wird, so dass sich der Eisbrecher auf das Eis schiebt und es unter seinem Gewicht zerbricht. Die Form des Bugs muss gewährleisten, dass die Eisbruchstücke um den Schiffsrumpf weit herumgedrückt werden und nicht den Propeller oder das Ruder beschädigen. Ein Auftürmen des gebrochenen Eises zu Schollen vor dem Bug würde den Eisbrecher stark behindern oder zum Stillstand zwingen. Durch verbesserte Bugformen brauchen moderne Eisbrecher nur noch die Hälfte der Maschinenleistung früherer eisbrechender Schiffe.

Ein anderer Weg wird bei den Thyssen-Waas-Eisbrechern gegangen. Thyssen-Waas-Eisbrecher besitzen eine patentierte Bugform, mit der ein völlig eisfreier Kanal hinter dem Schiff geschaffen werden kann. Bei diesen Eisbrechern wird das Eis von zwei links und rechts des Rumpfs angebrachten Schneiden zerschnitten. Die Bruchstücke werden dann mit Pressluft unter das benachbarte Eis geschoben. Bisher wurden zwei Eisbrecher mit diesem Spezialbug erfolgreich umgebaut. Mit dem optimierten Bug kann dabei mit gleicher Motorenleistung die zweifache Eisbrechleistung der doppelten Eisdicke erreicht werden. Ein weiterer Vorteil ist, dass diese Eisbrecher sowohl in Salz- als auch in Süßwasser eingesetzt werden können. Da sie das Eis schneiden und sich nicht auf das Eis schieben, ändert die unterschiedliche Eintauchtiefe in Salz- oder Süßwasser den zugrundeliegenden Mechanismus nicht. 

Diese Eisbruchmechanik ist eine Weiterentwicklung der Dreischneiden-Technik, bei der durch einen breiten Bug, der Seitenschneiden, der Kiellinie und einer speziellen Gestaltung des Unterschiffs das Eis im Wesentlichen möglichst nur in zwei Plattenbrüche zerlegt und durch den Bug und das Unterschiff unter das benachbarte Eis verdrängt wird. Diese Technik sichert seit den 1990er Jahren eine eisfreie Rinne wesentlich länger als frühere Techniken, die den Nachteil haben, dass Eisbruchstücke in der Rinne schnell wieder zusammenfrieren können. Unter das Nachbareis verdrängter Eisbruch liefert keine Kristallisationskeime in der gebrochenen Fahrrinne.

Die Motorleistung eines Eisbrechers ist höher im Vergleich zu anderen Schiffen gleicher Größe, um das Eis vor ihm brechen zu können. Abhängig von ihren eisbrechenden Leistungen werden Eisbrecher in Eisklassen eingeteilt.

Sollte das Gewicht des Schiffs alleine nicht ausreichen, um die Eismassen zu zerbrechen, kann noch ein besonderer Stampfmechanismus zur Unterstützung zugeschaltet werden. Eine Methode, das Stampfen zu erzeugen, besteht darin, große Wassermassen zwischen Bug und Heck des Eisbrechers hin- und herzupumpen, wodurch das Schiff ins Schwingen gerät (Nickschwingungen, „Stampfen“) und der Druck auf das Eis verstärkt wird. Eine weitere Variante verschiedener atomgetriebener russischer Eisbrecher ist es, durch im Kühlkreislauf erzeugten überspannten Wasserdampf das vorderschiffs liegende Eis anzuschmelzen. Unter günstigsten Voraussetzungen kann damit (auf Kosten der Geschwindigkeit) die maximal überwindbare Eisdicke nahezu verdoppelt werden. Dieses Verfahren ist allerdings umstritten und nicht in allen Gewässern zugelassen, da dadurch die direkt unter dem Eis liegende Meeresflora besonders auf viel befahrenen Routen stark beschädigt wird.

Als weitere Stampf-Methode wird noch ein sogenannter Hammer eingesetzt. Dies sind zwei sehr schwere Gewichte, die in entgegengesetzter Richtung rotieren. Somit heben sich die Schwungkräfte die meiste Zeit des Umlaufes auf. Wenn die Gewichte gleichzeitig oben oder unten sind, wird der Bug entsprechend in die Höhe gerissen oder auf das Eis gestampft.
Ein moderner Eisbrecher wird durch geschützte Schrauben beidseitig an Bug und Heck angetrieben und zusätzlich durch seitliche Strahldüsen stabilisiert. Aus unter der Wasserlinie liegenden Löchern im Rumpf kann zusätzlich Luft gepumpt werden, um durch die aufsteigenden Blasen Eis zu brechen.

Beide Stampftechniken, also das schnelle Umpumpen großer Wassermengen aus den Trimmtanks sowie das Stampfen mittels schwerer Unwuchtgewichte, haben einen weiteren Vorteil: Die eingetauchten Flächen der Bordwand sind beim Auftauchen mit Wasser benetzt, das wie ein Schmierfilm zwischen Bordwand und Eis wirkt. Die Stampfmechanismen haben auch die Aufgabe, den Eisbrecher selbst zu befreien, wenn er sich festgefahren hat. 

Bei großen Eisstärken oder im Packeis kann die Schiffsgeschwindigkeit durch den hohen Eiswiderstand gegen Null gehen. In diesem Fall muss der Eisbrecher zurücksetzen und einen neuen Anlauf fahren. Dieses unter Umständen mehrfache Zurück- und Vorausgehen nennt man „Boxen“.

Ein Hubschrauber gehört heutzutage bei großen Eisbrechern zur Ausrüstung, um im Notfall die Verbindung zum Festland zu gewährleisten, aber vor allem, um die Eisverhältnisse zu erkunden und so die optimale Route des Schiffes zu bestimmen.

Auf Grund des schlechten Verhältnisses von Breite zur Länge, eines kurzen Kiels und des auf Eisbruch ausgelegten Antriebs verhält sich ein Eisbrecher auf offener See ausgesprochen ungemütlich. Er neigt stark zum Rollen und ist in stürmischer See schwer zu manövrieren. Man versucht bei einigen neuen Eisbrechern, dieses Problem mit Ballasttanks zu minimieren. 

Auch die breite Form des Bugs, der nicht in der Lage ist, hohe Wellen elegant zu durchschneiden, fördert die Neigung eines Eisbrechers, in Wellenberge einzutauchen.

Neue Schiffstechnologien der „Pod-Antriebe“, ein unter dem Rumpf elektrisch angetriebener Propeller, der um 360 Grad drehbar ist, ergeben auch für Eisbrecher mehr Fahrsicherheit auf hoher See, zusätzlich bessere Manövrierfähigkeit beim Eisbruch. Mit dieser Technik und mit einem normalen, schnittigen Bug ausgerüstete Schiffe besitzen ein Heck, das wie ein Eisbrecherbug geformt ist. Bei Eisgang drehen diese Schiffe um und brechen rückwärts mit voller Leistung durch das Eis. Diese neue Technologie wird bereits von den Schiffen Mastera und "Tempera" benutzt.

Die Häfen der Ostsee und des Nordmeers waren in der Vergangenheit im Winter häufig zugefroren. Die Schifffahrt kam zum Erliegen, die Schiffe wurden aufgelegt, die Besatzungen abgemustert. Nach der industriellen Revolution hingegen musste das in Form von Schiffen eingesetzte Kapital jedoch „arbeiten“, und ein mehrmonatiges Aufliegen war unter dem Druck der Märkte und der Konkurrenz wirtschaftlich nicht mehr tragbar. Man begann zu überlegen, wie die zugefrorenen Wasserwege auch im Winter offen gehalten werden konnten. Anfangs versuchte man mit reiner Handarbeit, also mit Sägen und kleinen Holzbooten mit Schneidkanten (sogenannten Eisewern) und schließlich sogar mit Sprengstoff, Fahrrinnen und Fahrwasser offen zu halten. Das war eine äußerst mühsame Arbeit, die oft über Nacht wieder zunichtegemacht wurde, wenn ein frischer Wind neue Packeismassen aufgetürmt hatte. Erst nach der Erfindung der Dampfmaschine und deren Einführung auch in der Schifffahrt wurden erste einfache, aber dennoch wirkungsvolle Eisbrecher entwickelt. 

Der erste Eisbrecher war wahrscheinlich der Dampfschlepper "City Ice Boat No. 1" von 1837, der erste eiserne die "Pilot", die 1864 in Kronstadt zum Eisbrecher umgebaut wurde. Die positiven Erfahrungen führten schon 1868 zu einem Neubau nach dem Prinzip der "Pilot": der mit 183 kW Leistung erheblich stärkeren "Boi". Trotz der Erfolge und des starken Interesses in einigen Ostseeanrainerstaaten hatten die beiden Schiffe zunächst keine direkten Nachahmer. Im Eiswinter 1869/70 führte das Einfrieren von neun Dampfern auf der Elbe allerdings zu dem – erfolglosen – Versuch, die "Pilot" für Hamburg zu erwerben. Erst der folgende und nochmals erheblich schwerere Eiswinter führte in Hamburg zum Bau des Eisbrechers "Comité/Eisbrecher No. 1", der im Dezember 1871 in Dienst gestellt wurde. Er war von dem Schiffskonstrukteur Carl Ferdinand Steinhaus entworfen und hatte einen löffelförmig geschwungenen flachen Bug, mit dem sich das Schiff aufs Eis schieben und danach das Eis durch sein Eigengewicht brechen konnte.

Als einer der ersten seegehenden Eisbrecher gilt der nach Plänen des Admirals Makarow entworfene Eisbrecher "Ermak". Zu Beginn des 20. Jahrhunderts nahmen mehrere Länder speziell als Eisbrecher gebaute Schiffe für die Küstengewässer in Betrieb.

Der älteste erhaltene maschinenangetriebene Eisbrecher der Welt ist die finnische "Tarmo", die 1907 in Newcastle upon Tyne gebaut wurde und letztmals 1970 zum Einsatz kam. Sie hat eine Antriebsleistung von 3.850 PS und eine Verdrängung von 2.300 t.

Mit Bau der "Stettin" kam dann erstmals der in Finnland entwickelte sogenannte Runeberg-Steven nach Deutschland.

Das erste zivile Reaktorschiff der Welt war der 1958 in Dienst gestellte sowjetische Eisbrecher "Lenin" (44.000 PS, 19.240 BRT, 3 Reaktoren à 90 MW thermischer Leistung). Zur sowjetischen „Arktika“-Klasse gehören die größten und leistungsstärksten atomgetriebenen Eisbrecher der Welt, mit einer Leistung von rund 55.000 kW (75.000 PS, 2 Reaktoren à 171 MW thermischer Leistung). In dieser Leistungsklasse wurden in der Sowjetunion zwischen 1975 und 1992 die "Rossiya", "Arktika", "Sibir", "Sovetskiy Soyuz" und die "Yamal" gebaut. Sie können Eis von fünf Metern Dicke durchbrechen. Die "Arktika" erreichte 1977 als erstes Überwasserschiff den Nordpol. Zuvor war dies nur mit U-Booten gelungen.

Der größte je gebaute Eisbrecher ist die amerikanische "Manhattan", ein zum Eisbrecher umgebauter Tanker. Sie durchfuhr 1969 als erster Tanker die Nordwestpassage.

Das deutsche Polarforschungsschiff "Polarstern" ist ebenfalls als Eisbrecher gebaut und kann 1,5 Meter dickes Eis durchfahren. Am 7. September 1991 erreichte dieses Schiff als erstes konventionell angetriebenes Schiff den Nordpol.

Um in arktischen Regionen Frachtschiffe bei mittleren Eisbedingungen auch ohne Unterstützung eines Eisbrechers einsetzen zu können, wurden in jüngerer Zeit erste eisbrechende Containerschiffe gebaut. So kann das russische Schiff Aker ACS 650 in bis zu 1,5 m dickem Eis selbständig operieren. Auch die "Sevmorput" ist als nuklear getriebenes Frachtschiff in der Lage Eis bis zu einer Dicke von 1 m zu brechen.




</doc>
<doc id="1374" url="https://de.wikipedia.org/wiki?curid=1374" title="Emmy Noether">
Emmy Noether

Amalie Emmy Noether (* 23. März 1882 in Erlangen; † 14. April 1935 in Bryn Mawr, Pennsylvania) war eine deutsche Mathematikerin, die grundlegende Beiträge zur abstrakten Algebra und zur theoretischen Physik lieferte. Insbesondere hat Noether die Theorie der Ringe, Körper und Algebren revolutioniert. Das nach ihr benannte Noether-Theorem gibt die Verbindung zwischen Symmetrien von physikalischen Naturgesetzen und Erhaltungsgrößen an.

Emmy Noether stammte aus einer gutsituierten jüdischen Familie. Heute erinnert eine Tafel in der Erlanger Hauptstraße an ihr Geburtshaus. Ihr Vater Max Noether hatte einen Lehrstuhl für Mathematik an der Universität Erlangen inne. Ihr jüngerer Bruder, der Mathematiker Fritz Noether, floh vor den Nationalsozialisten in die Sowjetunion, wo er im Zuge des Großen Terrors wegen angeblicher antisowjetischer Propaganda verurteilt und erschossen wurde.

Emmy Noether zeigte in mathematischer Richtung keine besondere Frühreife, sondern hatte in ihrer Jugend Interesse an Musik und Tanzen. Sie besuchte die Städtische Höhere Töchterschule – das heutige Marie-Therese-Gymnasium – in der Schillerstraße in Erlangen. Mathematik wurde dort nicht intensiv gelehrt. Im April 1900 legte sie die Staatsprüfung zur Lehrerin der englischen und französischen Sprache an Mädchenschulen in Ansbach ab. 1903 holte sie in Nürnberg die externe Abiturprüfung am Königlichen Realgymnasium – dem heutigen Willstätter-Gymnasium – nach.

1903 wurden Frauen erstmals an bayerischen Universitäten zum Studium zugelassen, was auch Emmy Noether die Immatrikulation an der Universität Erlangen erlaubte. Vorher hatte sie bereits mit Erlaubnis einzelner Professoren als Gasthörerin Vorlesungen an der Universität Göttingen besucht, musste jedoch aufgrund einer Krankheit zurück nach Erlangen. Dort promovierte sie 1907 in Mathematik bei Paul Gordan. Sie war damit die zweite Deutsche, die an einer deutschen Universität in Mathematik promoviert wurde. 1908 wurde sie Mitglied des "Circolo Matematico di Palermo," 1909 trat sie der Deutschen Mathematiker-Vereinigung bei.

Im gleichen Jahr wurde sie von Felix Klein und David Hilbert an die Georg-August-Universität nach Göttingen gerufen, da sie auf dem Forschungsgebiet der Differentialinvarianten mittlerweile eine Größe war. Göttingen galt zu dieser Zeit als das führende mathematische Zentrum in der Welt. Durch Klein und Hilbert ermutigt, stellte Noether am 20. Juli 1915 einen Antrag auf Habilitation in Göttingen. Der Antragstellung folgten intensive kontroverse Diskussionen in der Fakultät, bei denen sich viele Fakultätsangehörige grundsätzlich gegen eine Habilitation von Frauen aussprachen. Letztlich konnten sich aber Hilbert und Klein durchsetzen; berühmt wurde die in diesem Zusammenhang gefallene Äußerung Hilberts, „eine Fakultät sei doch keine Badeanstalt“.

Da die Habilitation von Frauen an preußischen Universitäten durch einen Erlass vom 29. Mai 1908 untersagt war, stellte die mathematisch-naturwissenschaftliche Abteilung der philosophischen Fakultät der Universität zu Göttingen am 26. November 1915 einen offiziellen Antrag an den preußischen Minister:

Explizit wurde hinzugefügt, dass es keinesfalls um Aufhebung des Habilitationsverbots für Frauen ginge, sondern nur um eine einmalige Ausnahmegenehmigung für „Frl. Dr. Noether“:

In der abschlägigen Antwort des Ministers vom 5. November 1917 hieß es:

Emmy Noether blieb daraufhin nichts anderes übrig, als ihre Vorlesungen unter dem Namen von Hilbert anzukündigen, als dessen Assistentin sie fungierte.

Nach dem Ersten Weltkrieg und dem Zusammenbruch des Kaiserreichs kam es in der Weimarer Republik zu einer allgemeinen rechtlichen Besserstellung der Frauen. Neben dem Wahlrecht wurde auch die Habilitationsordnung so geändert, dass auch Frauen zur Habilitation zugelassen werden konnten. So konnte sich Emmy Noether 1919 als erste Frau in Deutschland in Mathematik habilitieren. Sie war außerdem die erste Frau in Deutschland, die eine (nichtbeamtete) Professur erhielt. Dennoch bekam sie erst 1922 eine außerordentliche Professur und erst 1923 ihren ersten bezahlten Lehrauftrag. Bis zur Hyperinflation im selben Jahr lebte sie sehr sparsam von einer Erbschaft. 1928/29 übernahm sie eine Gastprofessur in Moskau, 1930 in Frankfurt am Main. Bei ihrer Rückkehr aus der Sowjetunion äußerte sie sich sehr positiv über die dortige Lage, weshalb ihr die Nationalsozialisten später unterstellten, eine Kommunistin zu sein. Emmy Noether bekannte sich zum Pazifismus und war von 1919 bis 1922 Mitglied der USPD, danach bis 1924 der SPD. Zusammen mit Emil Artin erhielt sie 1932 den Ackermann-Teubner-Gedächtnispreis für ihre gesamten wissenschaftlichen Leistungen. 1932 hielt sie einen Plenarvortrag auf dem Internationalen Mathematikerkongress in Zürich "(Hyperkomplexe Systeme und ihre Beziehungen zur kommutativen Algebra und zur Zahlentheorie)."

1933 wurde Emmy Noether durch das sogenannte Gesetz zur Wiederherstellung des Berufsbeamtentums vom Naziregime ihre Lehrerlaubnis entzogen. Sie emigrierte daraufhin in die USA. Vor dieser Entscheidung zog sie auch in Betracht, nach Moskau zu gehen. Doch die Bemühungen ihres dortigen Freundes, des Topologen Pawel Alexandrow, bei den sowjetischen Behörden eine Bewilligung zu erwirken, zogen sich zu lange hin. In den USA half ihr ehemaliger Göttinger Kollege Hermann Weyl, eine Stelle für sie zu finden. Ende 1933 erhielt sie eine Gastprofessur am Women’s College Bryn Mawr in Pennsylvania. Ab 1934 hielt Emmy Noether auch Vorlesungen am Institute for Advanced Study. Dort beeinflusste sie Oscar Zariski und wahrscheinlich Nathan Jacobson (und sie beeinflusste mit ihrem neuen Zugang zur Algebra auch Abraham Adrian Albert). Sie kam 1934 noch einmal nach Europa und besuchte Emil Artin und ihren Bruder Fritz in Deutschland. Emmy Noether verstarb am 14. April 1935 an den Komplikationen einer Unterleibsoperation, die wegen eines Tumors notwendig geworden war. Sie fand ihre letzte Ruhestätte unter dem Kreuzgang der M. Carey Thomas Library auf dem Campus des Bryn Mawr College.

Emmy Noether gehört zu den Begründern der modernen Algebra. Ihre mathematische Profilierung entwickelte sich in der Zusammenarbeit und Auseinandersetzung mit dem Erlanger Professor Paul Gordan, der auch ihr Doktorvater wurde. Man nannte diesen gerne den „König der Invarianten“. Die Invariantentheorie beschäftigte Emmy Noether bis in das Jahr 1919 entschieden.

Abweichend von Gordans Interessensschwerpunkten wandte sich Noether der Auseinandersetzung mit den abstrakten algebraischen Methoden zu. Gordan hatte Hilberts Beweis seines Basistheorems, der viele Resultate Gordans verallgemeinerte, aber ein reiner Existenzbeweis war, mit den Worten kommentiert, dass dies nicht Mathematik, sondern Theologie sei.

Ab 1920 verlegte sie ihren Forschungsschwerpunkt auf die allgemeine Idealtheorie. In Göttingen gründete sie eine eigene Schule: Seit Mitte der 1920er Jahre fand sie eine Reihe von hochbegabten Schülern aus aller Welt, die sich um sie scharten. Ihre Studenten nannte sie ihre „Trabanten“ oder die „Noether-Knaben“. Zu ihren Doktoranden zählen Grete Hermann, Jakob Levitzki, Max Deuring, Ernst Witt, dessen offizieller Betreuer Herglotz war, Heinrich Grell, Chiungtze Tsen, Hans Fitting, Otto Schilling und zu ihrem Schülerkreis Bartel Leendert van der Waerden. Andere bedeutende Algebraiker in Deutschland, die mit der Schule verbunden waren, waren Emil Artin, Helmut Hasse (mit dem sie den wichtigen Satz von Brauer-Hasse-Noether in der Theorie der Algebren bewies) und Wolfgang Krull.

In Göttingen, damals Weltzentrum mathematischer Forschung, baute sie eine eigene mathematische Schule auf. Van der Waerden schrieb in seinem berühmten zweibändigen Algebrawerk (Moderne Algebra), dass es auch auf Vorlesungen von Emil Artin und Emmy Noether aufbaute. Noether wird auch eine entscheidende Rolle bei der Durchsetzung abstrakter algebraischer Methoden in der Topologie zugeschrieben, fast ausschließlich durch mündliche Beiträge zum Beispiel in den Vorlesungen von Heinz Hopf 1926/27 in Göttingen und in ihren eigenen Vorlesungen um 1925. Das beeinflusste auch den Topologen Pawel Sergejewitsch Alexandrow, der Göttingen besuchte.

Auch in der theoretischen Physik leistete sie Außerordentliches und legte 1918 mit dem Noether-Theorem den Grundstein zu einer neuartigen Betrachtung von Erhaltungsgrößen. 50 Jahre nach ihrem Tod, im letzten Viertel des 20. Jahrhunderts, entwickelte sich das Noether-Theorem zu einer der wichtigsten Grundlagen der modernen Physik.

Nach Emmy Noether sind folgende mathematische Strukturen und Sätze benannt:
Weiter sind nach Emmy Noether benannt:
Weitere Ehrungen:





</doc>
<doc id="1375" url="https://de.wikipedia.org/wiki?curid=1375" title="Enzym">
Enzym

Ein Enzym, früher "Ferment", ist ein Stoff, der aus biologischen Riesenmolekülen besteht und als Katalysator eine chemische Reaktion beschleunigen kann. Die meisten Enzyme sind Proteine, eine Ausnahme bildet die katalytisch aktive RNA (Ribozym), wie z. B. snRNA oder (natürlich nicht vorkommende künstlich hergestellte) katalytisch aktive DNA (Desoxyribozym). Ihre Bildung in der Zelle erfolgt daher, wie auch bei anderen Proteinen, über Proteinbiosynthese an den Ribosomen. Enzyme haben wichtige Funktionen im Stoffwechsel von Organismen: Sie steuern den überwiegenden Teil biochemischer Reaktionen – von der Verdauung bis hin zur Transkription (RNA-Polymerase) und Replikation (DNA-Polymerase) der Erbinformationen.

Menschen nutzen seit mehreren tausend Jahren die Wirkung von Enzymen wie jener von Hefen und Bakterien; so ist bekannt, dass die Sumerer bereits 3000 v. Chr. Bier brauten, Brot backten und Käse herstellten. Für den Gebrauch von Bier- oder Backhefe, wie beim Maischen oder im Hefeteig, und die damit eingeleiteten Vorgänge der Gärung entstand die Bezeichnung „Fermentation“, noch ohne Kenntnis der Existenz von Bakterien (bzw. der mikrobiellen Hefepilze) und ihrer Wirkung durch Enzyme.

Die Wörter "Fermentation" und "Ferment" hielten im 15. Jahrhundert Einzug in die deutsche Sprache, sie gehen auf das lateinische Wort "fermentum" zurück. Diesen Ausdruck verwendet Columella etwa 60 n. Chr. auch für das Auflockern und Quellen des Bodens, während Seneca etwa gleicher Zeit in seinen "Epistulae" damit einen Gärungsvorgang bezeichnet, den er für die Bildung von Honig als nötig ansah. Mit dieser Bedeutung als „Gärungsmittel“ oder „Sauerteig“ wurde das Wort "Ferment" aus dem Lateinischen entlehnt, und davon "fermentieren", Fermentation sowie Fermenter abgeleitet.

Die ersten Gärungsprozesse beschrieben Paracelsus und Andreas Libavius. Die ersten Versuche zur Erklärung kamen von Johann Baptist van Helmont und Georg Ernst Stahl. Nachdem René Réaumur 1752 die Verdauung bei Vögeln untersucht und herausgestellt hatte, dass Greifvögel keinen Körner zerkleinernden Muskelmagen haben, sondern im Magen eine Flüssigkeit absondern, konnte Lazzaro Spallanzani 1783 belegen, dass allein deren Magensaft schon hinreicht Fleisch zu verflüssigen. Damit war die Theorie eines nur mechanischen Verdauungsprozesses widerlegt.

Die erste unmittelbare Nutzung von Enzymen ohne die Mitbeteiligung von Mikroorganismen erfolgte durch den deutschen Apotheker Constantin Kirchhoff im Jahre 1811, als er entdeckte, dass man durch Erhitzen von Stärke unter Beigabe von Schwefelsäure größere Mengen Zucker herstellen kann. Der französische Chemiker Anselme Payen verfeinerte 1833 den Prozess; da man zu dieser Zeit annahm, dass man den Zucker lediglich von der Stärke trenne, bezeichnete man diesen Prozess als „Diastase“ (griechisch für trennen); heute wird der Begriff „Diastase“ synonym zu Amylase verwendet. Es folgte die Entdeckung von Erhard Friedrich Leuchs im Jahre 1831, dass der menschliche Mundspeichel Stärke scheinbar verzuckere. 1833 wurde von Eilhard Mitscherlich der Begriff „Ferment“ im Zusammenhang mit einem Stoff gebraucht, der bei einer Reaktion nicht verwandelt wird, aber zum Kontakt für eine Reaktion erforderlich ist. 1835 wurde die Diastase vom schwedischen Chemiker Jöns Jakob Berzelius als chemischer Prozess mit der Einwirkung von katalytischen Kräften vermutet.

Erst 1837 entdeckten drei Wissenschaftler (C. Cagniard de la Tour, T. Swann und F. Kuetzing) unabhängig voneinander, dass Hefe aus Mikroorganismen besteht. Louis Pasteur wies 1862 nach, dass Mikroorganismen für die Fermentation verantwortlich sind; er schlussfolgerte, dass die Fermentation durch eine vitale Kraft erfolge, die in der Schimmelzelle vorhanden sei, welche er „Fermente“ nannte, die nicht mit dem Tod der Schimmelzelle an Wirkung verlieren.

1878 führte Wilhelm Friedrich Kühne das heutige neoklassische Kunstwort "Enzym" ( "énzymon") ein, abgeleitet von "en-", „in-“, und "zýmē", welches ebenfalls „der Sauerteig“ oder „die Hefe“ bedeutet, der Sinn ist daher „das in Sauerteig/Hefe Enthaltene“ (nämlich der die Gärung auslösende oder beeinflussende Stoff). Dieser Begriff hielt dann Einzug in die internationale Wissenschaft und ist nun auch Bestandteil der neugriechischen Sprache.

Kühne grenzte den Begriff "Enzyme" als Bezeichnung für außerhalb lebender Zellen wirksame Biokatalysatoren jedoch von "Fermenten" ab, die ihre Wirkung gemäß Pasteur nur innerhalb lebender Zellen entfalten können.

Einen weiteren Meilenstein stellen die Untersuchungen zur Enzymspezifität von Emil Fischer dar. Er postulierte um 1890 dass Enzyme und ihr Substrat sich wie ein Schloss und der passende Schlüssel verhalten. 1897 entdeckte Eduard Buchner anhand der alkoholischen Gärung, dass Enzyme auch ohne die lebende Zelle katalytisch wirken können; 1907 erhielt er für den Nachweis einer Zell-freien Fermentation den Nobelpreis. 1903 schafften Eduard Buchner und Jakob Meisenheimer es, Mikroorganismen, die Milch- und Essigsäuregärung auslösten, abzutöten, ohne ihre Enzymwirkung zu beeinflussen. Der deutsche Chemiker Otto Röhm isolierte 1908 erstmals Enzyme und entwickelte Verfahren zur enzymatischen Ledergerbung, Fruchtsaftreinigung sowie eine Reihe diagnostischer Anwendungen.

Anfang des 20. Jahrhunderts war die chemische Komposition von Enzymen noch unbekannt. Man vermutete, dass Enzyme aus Protein bestehen und ihre enzymatische Aktivität mit ihrer Struktur assoziiert sei. Andere Wissenschaftler wie Richard Willstätter argumentierten jedoch, dass Proteine nur Träger der „echten Enzyme“ wären und von sich aus unfähig wären eine katalytische Reaktion einzuleiten. James B. Sumner zeigte 1926, dass das Enzym Urease ein pures Protein ist und war fähig es zu kristallisieren. Die letzten Zweifel zur Komposition von Enzymen wurden von John H. Northop und Wendell M. Stanley ausgeräumt, als diese 1930 nachwiesen, dass Pepsin, Trypsin und Chymotrypsin aus purem Protein bestehen. Northrop und Stanley erhielten dafür 1946 den Nobelpreis für Chemie.

Die Erkenntnis, wie man Enzyme kristallisiert, erlaubte es den Forschern nun durch Kristallstrukturanalyse die Struktur und die Funktionsweise von Enzymen auf atomarem Level aufzuklären. In den Jahren 1930 bis 1939 konnten die Kristallstrukturen von elf weiteren Enzymen aufgedeckt werden. Die erste Aminosäuresequenz, die von einem Enzym komplett entschlüsselt war, ist die der Ribonuklease. Dieser Schritt gelang Stanford Moore und William Howard Stein. 1969 synthetisierte Robert Bruce Merrifield dann die gesamte Sequenz der Ribonuklease mit der nach ihm benannten Technik (Merrifield-Synthese). Gleichzeitig schafften dies auch R. G. Denkewalter und R. Hirschmann.

In den 1980er Jahren wurden katalytische Antikörper von Richard Lerner entdeckt, die eine Enzymaktivität aufwiesen, nachdem gegen ein dem Übergangszustand nachempfundenes Molekül immunisiert wurde. Linus Pauling hatte bereits 1948 vermutet, dass Enzyme dem Übergangszustand ähnliche Moleküle besonders gut binden. Ende der 1980er Jahre wurde entdeckt, dass auch RNA im Organismus katalytische (enzymatische) Aktivität entfalten kann (Ribozym). 1994 wurde das erste Desoxyribozym, GR-5, entwickelt.

Forscher wie Leonor Michaelis und Maud Menten leisteten Pionierarbeit in der Erforschung der Enzymkinetik mit der Formulierung der Michaelis-Menten-Theorie.

Die IUPAC und die IUBMB haben zusammen eine sogenannte Nomenklatur der Enzyme erarbeitet, die diese homogene und zahlreiche Vertreter enthaltende Gruppe der Moleküle klassifiziert. Hierzu erarbeitete die IUPAC Prinzipien der Nomenklatur:

Außerdem wurde ein Codesystem, das EC-Nummern-System, entwickelt, in dem die Enzyme unter einem Zahlencode aus vier Zahlen eingeteilt werden. Die erste Zahl bezeichnet eine der sechs Enzymklassen. Listen aller erfassten Enzyme gewährleisten ein schnelleres Auffinden des angegebenen Enzymcodes, z. B. bei BRENDA. Zwar orientieren sich die Codes an Eigenschaften der Reaktion, die das Enzym katalysiert, in der Praxis erweisen sich Zahlencodes jedoch als unhandlich. Häufiger gebraucht werden systematische, nach den oben genannten Regeln konzipierte Namen. Probleme der Nomenklatur ergeben sich etwa bei Enzymen, die mehrere Reaktionen katalysieren. Für sie existieren deshalb manchmal mehrere Namen. Einige Enzyme tragen Trivialnamen, die nicht erkennen lassen, dass es sich bei der genannten Substanz um Enzyme handelt. Da die Namen traditionell eine breite Verwendung fanden, wurden sie teilweise beibehalten (Beispiele: die Verdauungsenzyme Trypsin und Pepsin des Menschen).

Enzyme werden entsprechend der von ihnen katalysierten Reaktion in sechs Enzymklassen eingeteilt:


Manche Enzyme sind in der Lage, mehrere, zum Teil sehr unterschiedliche Reaktionen zu katalysieren. Ist dies der Fall, werden sie mehreren Enzymklassen zugerechnet.

Enzyme lassen sich anhand ihres Aufbaus unterscheiden. Während viele Enzyme aus nur einer Polypeptidkette bestehen, so genannte Monomere, bestehen andere Enzyme, die Oligomere, aus mehreren Untereinheiten/Proteinketten. Einige Enzyme lagern sich mit weiteren Enzymen zu sogenannten Multienzymkomplexen zusammen und kooperieren oder regulieren sich gegenseitig. Umgekehrt gibt es auch einzelne Proteinketten, welche mehrere, verschiedene Enzymaktivitäten ausüben können "(multifunktionelle Enzyme)". Eine weitere mögliche Einteilung hinsichtlich ihres Aufbaus berücksichtigt das Vorhandensein von Kofaktoren:


Eine spezielle Gruppe bilden die Protein-RNA-Komplexe bzw. Protein-Ribozym-Komplexe, Beispiele hierfür sind die Telomerasen. Auch die Ribosomen sind solche Komplexe.

Enzyme sind Biokatalysatoren. Sie beschleunigen biochemische Reaktionen, indem sie die Aktivierungsenergie herabsetzen, die überwunden werden muss, damit es zu einer Stoffumsetzung kommt. Theoretisch ist eine enzymatische Umsetzung reversibel, d. h., die Produkte können wieder in die Ausgangsstoffe umgewandelt werden. Die Ausgangsstoffe (Edukte) einer Enzymreaktion, die Substrate, werden im so genannten "aktiven Zentrum" des Enzyms gebunden, es bildet sich ein "Enzym-Substrat-Komplex". Das Enzym ermöglicht nun die Umwandlung der Substrate in die Reaktionsprodukte, die anschließend aus dem Komplex freigesetzt werden. Wie alle Katalysatoren liegt das Enzym nach der Reaktion wieder in der Ausgangsform vor. Enzyme zeichnen sich durch hohe "Substrat-" und "Reaktionsspezifität" aus, unter zahlreichen Stoffen wählen sie nur die passenden Substrate aus und katalysieren genau eine von vielen denkbaren Reaktionen.

Die meisten biochemischen Reaktionen würden ohne Enzyme in den Lebewesen nur mit vernachlässigbarer Geschwindigkeit ablaufen. Wie bei jeder spontan ablaufenden Reaktion muss die freie Reaktionsenthalpie (formula_1) negativ sein. Das Enzym beschleunigt die Einstellung des chemischen Gleichgewichts – ohne es zu verändern. Die katalytische Wirksamkeit eines Enzyms beruht einzig auf seiner Fähigkeit, in einer chemischen Reaktion die Aktivierungsenergie "formula_2" zu senken: das ist der Energiebetrag, der zunächst investiert werden muss, um die Reaktion in Gang zu setzen. Während dieser wird das Substrat zunehmend verändert, es nimmt einen energetisch ungünstigen "Übergangszustand" ein. Die Aktivierungsenergie ist nun der Energiebetrag, der benötigt wird, um das Substrat in den Übergangszustand zu zwingen. Hier setzt die katalytische Wirkung des Enzyms an: Durch nicht-kovalente Wechselwirkungen mit dem Übergangszustand stabilisiert es diesen, so dass weniger Energie benötigt wird, um das Substrat in den Übergangszustand zu bringen. Das Substrat kann wesentlich schneller in das Reaktionsprodukt umgewandelt werden, da ihm gewissermaßen ein Weg „geebnet“ wird.

Für die katalytische Wirksamkeit eines Enzyms ist das "aktive Zentrum" (katalytisches Zentrum) verantwortlich. An dieser Stelle bindet es das Substrat und wird danach „aktiv“ umgewandelt. Das aktive Zentrum besteht aus gefalteten Teilen der Polypeptidkette oder reaktiven Nicht-Eiweiß-Anteilen (Kofaktoren, prosthetische Gruppen) des Enzymmoleküls und bedingt eine Spezifität der enzymatischen Katalyse. Diese Spezifität beruht auf der Komplementarität der Raumstruktur und der oberflächlich möglichen Wechselwirkungen zwischen Enzym und Substrat. Es kommt zur Bildung eines "Enzym-Substrat-Komplexes".

Die Raumstruktur des aktiven Zentrums bewirkt, dass nur ein strukturell passendes Substrat gebunden werden kann. Veranschaulichend passt ein bestimmtes Substrat zum entsprechenden Enzym wie ein Schlüssel in das passende Schloss (Schlüssel-Schloss-Prinzip). Dies ist der Grund für die hohe "Substratspezifität" von Enzymen. Neben dem Schlüssel-Schloss-Modell existiert das nicht starre "Induced fit model": Da Enzyme flexible Strukturen sind, kann das aktive Zentrum durch Interaktion mit dem Substrat neu geformt werden.

Bereits kleine strukturelle Unterschiede in Raumstruktur oder Ladungsverteilung des Enzyms können dazu führen, dass ein dem Substrat ähnlicher Stoff nicht mehr als Substrat erkannt wird. Glucokinase beispielsweise akzeptiert Glucose als Substrat, deren Stereoisomer Galactose jedoch nicht. Enzyme können verschieden breite Substratspezifität haben, so bauen Alkohol-Dehydrogenasen neben Ethanol auch andere Alkohole ab und Hexokinase IV akzeptiert neben der Glucose auch andere Hexosen als Substrat.

Die Erkennung und Bindung des Substrats gelingt durch nicht-kovalente Wechselwirkungen (Wasserstoffbrücken, elektrostatische Wechselwirkung oder hydrophobe Effekte) zwischen Teilen des Enzyms und des Substrats. Die Bindung des Enzyms muss stark genug sein, um das oft gering konzentrierte Substrat (mikro- bis millimolare Konzentrationen) zu binden, sie darf jedoch nicht zu stark sein, da die Reaktion nicht mit der Bindung des Substrates endet. Wichtig ist eine noch stärkere Bindung des Übergangszustandes der Reaktion und damit dessen Stabilisierung. Nicht selten nehmen zwei Substrate an einer Reaktion teil, das Enzym muss dann die richtige Orientierung der Reaktionspartner zueinander garantieren. Diese letzteren mechanistischen Eigenheiten einer enzymatischen Reaktion sind die Grundlage der "Wirkungsspezifität" eines Enzyms. Es katalysiert immer nur eine von vielen denkbaren Reaktionen der Substrate. Die Aktivität von Enzymen wird teilweise durch Pseudoenzyme (Varianten von Enzymen ohne Enzymaktivität) reguliert.

Obwohl die Mechanismen enzymatischer Reaktionen im Detail vielgestaltig sind, nutzen Enzyme in der Regel eine oder mehrere der folgenden katalytischen Mechanismen.

Die Enzymkinetik beschäftigt sich mit dem zeitlichen Verlauf enzymatischer Reaktionen. Eine zentrale Größe hierbei ist die Reaktionsgeschwindigkeit. Sie ist ein Maß für die Änderung der Substratkonzentration mit der Zeit, also für die Stoffmenge Substrat, die in einem bestimmten Reaktionsvolumen pro Zeiteinheit umgesetzt wird (Einheit: mol/(l·s)). Neben den Reaktionsbedingungen wie Temperatur, Salzkonzentration und pH-Wert der Lösung hängt sie von den Konzentrationen des Enzyms, der Substrate und Produkte sowie von Effektoren (Aktivatoren oder Inhibitoren) ab.

Im Zusammenhang mit der Reaktionsgeschwindigkeit steht die Enzymaktivität. Sie gibt an, wie viel aktives Enzym sich in einer Enzym-Präparation befindet. Die Einheiten der Enzymaktivität sind Unit (U) und Katal (kat), wobei 1 U definiert ist als diejenige Menge Enzym, welche unter angegebenen Bedingungen ein Mikromol Substrat pro Minute umsetzt: 1 U = 1 µmol/min. Katal wird selten benutzt, ist jedoch die SI-Einheit der Enzymaktivität: 1 kat = 1 mol/s. Eine weitere wichtige Messgröße bei Enzymen ist die spezifische Aktivität (Aktivität pro Masseneinheit, U/mg). Daran kann man sehen, wie viel von dem gesamten Protein in der Lösung wirklich das gesuchte Enzym ist.

Die gemessene Enzymaktivität ist proportional zur Reaktionsgeschwindigkeit und damit stark von den Reaktionsbedingungen abhängig. Sie steigt mit der Temperatur entsprechend der RGT-Regel an: eine Erhöhung der Temperatur um ca. 5–10 °C führt zu einer Verdoppelung der Reaktionsgeschwindigkeit und damit auch der Aktivität. Dies gilt jedoch nur für einen begrenzten Temperaturbereich. Bei Überschreiten einer optimalen Temperatur kommt es zu einem steilen Abfallen der Aktivität durch Denaturierung des Enzyms. Änderungen im pH-Wert der Lösung haben oft dramatische Effekte auf die Enzymaktivität, da dieser die Ladung einzelner für die Katalyse wichtiger Aminosäuren im Enzym beeinflussen kann. Jenseits des pH-Optimums vermindert sich die Enzymaktivität und kommt irgendwann zum Erliegen. Ähnliches gilt für die Salzkonzentration bzw. die Ionenstärke in der Umgebung.

Ein Modell zur kinetischen Beschreibung einfacher Enzymreaktionen ist die Michaelis-Menten-Theorie (MM-Theorie).
Sie liefert einen Zusammenhang zwischen der "Reaktionsgeschwindigkeit v" einer Enzymreaktion sowie der Enzym- und Substratkonzentration "[E]" und "[S]". Grundlage ist die Annahme, dass ein Enzym mit einem Substratmolekül einen Enzym-Substrat-Komplex bildet und dieser entweder in Enzym und Produkt oder in seine Ausgangsbestandteile zerfällt. Was schneller passiert, hängt von den jeweiligen Geschwindigkeitskonstanten "k" ab.

Das Modell besagt, dass mit steigender Substratkonzentration auch die Reaktionsgeschwindigkeit steigt. Das geschieht anfangs linear und flacht dann ab, bis eine weitere Steigerung der Substratkonzentration keinen Einfluss mehr auf die Geschwindigkeit des Enzyms hat, da dieses bereits mit Maximalgeschwindigkeit "V" arbeitet. Die MM-Gleichung lautet wie folgt:

Die Parameter "K" (Michaeliskonstante) und "k" (Wechselzahl) sind geeignet, Enzyme kinetisch zu charakterisieren, d. h., Aussagen über ihre katalytische Effizienz zu treffen. Ist "K" beispielsweise sehr niedrig, heißt das, das Enzym erreicht schon bei niedriger Substratkonzentration seine Maximalgeschwindigkeit und arbeitet damit sehr effizient. Bei geringen Substratkonzentrationen ist die Spezifitätskonstante "k/ K" ein geeigneteres Maß für die katalytische Effizienz. Erreicht sie Werte von mehr als 10 bis 10 M s, wird die Reaktionsgeschwindigkeit nur noch durch die Diffusion der Substrat- und Enzymmoleküle begrenzt. Jeder zufällige Kontakt von Enzym und Substrat führt zu einer Reaktion. Enzyme, die eine solche Effizienz erreichen, nennt man „katalytisch perfekt“.

Einige Enzyme zeigen nicht die hyperbolische Sättigungskurve, wie sie die Michaelis-Menten-Theorie vorhersagt, sondern ein sigmoides Sättigungsverhalten. So etwas wurde erstmals bei Bindeproteinen wie dem Hämoglobin beschrieben und wird als positive Kooperativität mehrerer Bindungsstellen gedeutet: die Bindung eines Liganden (Substratmolekül) beeinflusst weitere Bindungsstellen im gleichen Enzym (oft aber in anderen Untereinheiten) in ihrer Affinität. Bei positiver Kooperativität hat ein Bindeprotein mit vielen freien Bindungsstellen eine schwächere Affinität als ein größtenteils besetztes Protein. Bindet derselbe Ligand an alle Bindungszentren, spricht man von einem "homotropen Effekt". Die Kooperativität ist bei Enzymen eng mit der Allosterie verknüpft. Unter Allosterie versteht man das Vorhandensein weiterer Bindungsstellen "(allosterischen Zentren)" in einem Enzym, abgesehen vom aktiven Zentrum. Binden Effektoren (nicht Substratmoleküle) an allosterische Zentren, liegt ein "heterotroper Effekt" vor. Die Allosterie ist zwar begrifflich von der Kooperativität zu unterscheiden, dennoch treten sie oft gemeinsam auf.

Die bisherigen Überlegungen gelten nur für Reaktionen, an denen ein Substrat zu einem Produkt umgesetzt wird. Viele Enzyme katalysieren jedoch die Reaktion zweier oder mehrerer Substrate bzw. Kosubstrate. Ebenso können mehrere Produkte gebildet werden. Bei reversiblen Reaktionen ist die Unterscheidung zwischen Substrat und Produkt ohnehin relativ. Die Michaelis-Menten-Theorie gilt für eines von mehreren Substraten nur, wenn das Enzym mit den anderen Substraten gesättigt ist.

Für Mehrsubstrat-Reaktionen sind folgende Mechanismen vorstellbar:


Als Enzymhemmung (Inhibition) bezeichnet man die Herabsetzung der katalytischen Aktivität eines Enzyms durch einen spezifischen Hemmstoff (Inhibitor). Grundlegend unterscheidet man die "irreversible Hemmung", bei der ein Inhibitor eine unter physiologischen Bedingungen nicht umkehrbare Verbindung mit dem Enzym eingeht (so wie Penicillin mit der "D-Alanin-Transpeptidase"), von der "reversiblen Hemmung", bei der der gebildete Enzym-Inhibitor-Komplex wieder in seine Bestandteile zerfallen kann. Bei der reversiblen Hemmung unterscheidet man wiederum zwischen


Enzyme wirken im lebenden Organismus in einem komplexen Geflecht von Stoffwechselwegen zusammen. Um sich schwankenden inneren und äußeren Bedingungen optimal anpassen zu können, ist eine feine Regulation und Kontrolle des Stoffwechsels und der zugrundeliegenden Enzyme nötig. Unter "Regulation" versteht man Vorgänge, die der Aufrechterhaltung stabiler innerer Bedingungen bei wechselnden Umweltbedingungen (Homöostase) dienen. Als "Kontrolle" bezeichnet man Veränderungen, die auf Grund von externen Signalen (beispielsweise durch Hormone) stattfinden. Es gibt schnelle/kurzfristige, mittelfristige sowie langsame/langfristige Regulations- und Kontrollvorgänge im Stoffwechsel:

Schnelle Veränderungen der Enzymaktivität erfolgen als direkte Antwort der Enzyme auf veränderte Konzentrationen von Stoffwechselprodukten, wie Substrate, Produkte oder "Effektoren" (Aktivatoren und Inhibitoren). Enzymreaktionen, die nahe am Gleichgewicht liegen, reagieren empfindlich auf Veränderungen der Substrat- und Produktkonzentrationen. Anhäufung von Substrat beschleunigt die Hinreaktion, Anhäufung von Produkt hemmt die Hinreaktion und fördert die Rückreaktion "(kompetitive Produkthemmung)". Allgemein wird aber den irreversiblen Enzymreaktionen eine größere Rolle bei der Stoffwechselregulation und Kontrolle zugeschrieben.

Von großer Bedeutung ist die "allosterische Modulation". Substrat- oder Effektormoleküle, die im Stoffwechsel anfallen, binden an allosterische Zentren des Enzyms und verändern seine katalytische Aktivität. Allosterische Enzyme bestehen aus mehreren Untereinheiten (entweder aus gleichen oder auch aus verschiedenen Proteinmolekülen). Die Bindung von Substrat- oder Hemmstoff-Molekülen an eine Untereinheit führt zu "Konformationsänderungen" im gesamten Enzym, welche die Affinität der übrigen Bindungsstellen für das Substrat verändern. Eine "Endprodukt-Hemmung" "(Feedback-Hemmung)" entsteht, wenn das Produkt einer Reaktionskette auf das Enzym am Anfang dieser Kette allosterisch hemmend wirkt. Dadurch entsteht automatisch ein Regelkreis.

Eine häufige Form der Stoffwechselkontrolle ist die kovalente Modifikation von Enzymen, besonders die Phosphorylierung. Wie durch einen molekularen Schalter kann das Enzym beispielsweise nach einem hormonellen Signal durch phosphat-übertragende Enzyme (Kinasen) ein- oder ausgeschaltet werden. Die Einführung einer negativ geladenen Phosphatgruppe zieht strukturelle Änderungen im Enzym nach sich und kann prinzipiell aktive als auch inaktive Konformationen begünstigen. Die Abspaltung der Phosphatgruppe durch Phosphatasen kehrt diesen Vorgang um, so dass eine flexible Anpassung des Stoffwechsels an wechselnde physiologische Anforderungen möglich ist.

Als langfristige Reaktion auf geänderte Anforderungen an den Stoffwechsel werden Enzyme gezielt abgebaut oder neugebildet. Die "Neubildung" von Enzymen wird über die Expression ihrer Gene gesteuert. Eine solche Art der "genetischen Regulation" bei Bakterien beschreibt das Operon-Modell von Jacob und Monod. Der kontrollierte "Abbau" von Enzymen in eukaryotischen Zellen kann durch Ubiquitinierung realisiert werden. Das Anheften von Polyubiquitin-Ketten an Enzyme, katalysiert durch spezifische Ubiquitin-Ligasen, markiert diese für den Abbau im Proteasom, einem „Müllschlucker“ der Zelle.

Enzyme haben eine nicht zu unterschätzende biologische Bedeutung, sie spielen "die" zentrale Rolle im Stoffwechsel aller lebenden Organismen. Nahezu jede biochemische Reaktion wird von Enzymen bewerkstelligt und kontrolliert. Bekannte Beispiele sind Glycolyse und Citrat-Zyklus, Atmungskette und Photosynthese, Transkription und Translation sowie die DNA-Replikation. Enzyme wirken nicht nur als Katalysatoren, sie sind auch wichtige Regulations- und Kontrollpunkte im Stoffwechselgeschehen.

Die Bedeutung der Enzyme beschränkt sich jedoch nicht auf den Stoffwechsel, auch bei der Reizaufnahme und -weitergabe sind sie wichtig. An der Signaltransduktion, also der Vermittlung einer Information innerhalb einer Zelle, sind häufig Rezeptoren mit enzymatischer Funktion beteiligt. Auch Kinasen, wie die Tyrosinkinasen und Phosphatasen spielen bei der Weitergabe von Signalen eine entscheidende Rolle. Die Aktivierung und Deaktivierung der Träger der Information, also der Hormone geschehen durch Enzyme.

Weiterhin sind Enzyme an der Verteidigung des eigenen Organismus beteiligt, so sind zum Beispiel diverse Enzyme wie die Serinproteasen des Komplementsystems Teil des unspezifischen Immunsystems des Menschen.

Fehler in Enzymen können fatale Folgen haben. Durch solche Enzymdefekte ist die Aktivität eines Enzyms vermindert oder gar nicht mehr vorhanden. Manche Enzymdefekte werden genetisch vererbt, d. h., das Gen, das die Aminosäuresequenz des entsprechenden Enzyms codiert, enthält eine oder mehrere Mutationen oder fehlt ganz. Beispiele für vererbbare Enzymdefekte sind die Phenylketonurie und Galaktosämie.
Enzyme sind wertvolle Werkzeuge der Biotechnologie. Ihre Einsatzmöglichkeiten reichen von der Käseherstellung (Labferment) über die Enzymatik bis hin zur Gentechnik. Für bestimmte Anwendungen entwickeln Wissenschaftler heute gezielt leistungsfähigere Enzyme durch Protein-Engineering. Zudem konstruierte man eine neuartige Form katalytisch aktiver Proteine, die katalytischen Antikörper, die aufgrund ihrer Ähnlichkeit zu den Enzymen Abzyme genannt wurden. Auch Ribonukleinsäuren (RNA) können katalytisch aktiv sein; diese werden dann als Ribozyme bezeichnet.

Enzyme werden unter anderem in der Industrie benötigt. Waschmitteln fügt man Lipasen (Fett spaltende Enzyme), Proteasen (Eiweiß spaltende Enzyme) und Amylasen (Stärke spaltende Enzyme) zur Erhöhung der Reinigungsleistung hinzu, weil diese Enzyme die entsprechenden Flecken zersetzen. Enzyme werden auch zur Herstellung einiger Medikamente und Insektenschutzmittel verwendet. Bei der Käseherstellung wirkt das Labferment mit, ein Enzym, das aus Kälbermägen gewonnen wurde. Viele Enzyme können heute mit Hilfe von gentechnisch veränderten Mikroorganismen hergestellt werden.

Die in rohen Ananas, Kiwifrüchten und Papayas enthaltenen Enzyme verhindern das Erstarren von Tortengelatine, ein unerwünschter Effekt, wenn beispielsweise ein Obstkuchen, der rohe Stücke dieser Früchte enthält, mit einem festen Tortengelatinebelag überzogen werden soll. Das Weichbleiben des Übergusses tritt nicht bei der Verwendung von Früchten aus Konservendosen auf, diese werden pasteurisiert, wobei die eiweißabbauenden Enzyme deaktiviert werden.

Beim Schälen von Obst und Gemüse werden pflanzliche Zellen verletzt und in der Folge Enzyme freigesetzt. Dadurch kann das geschälte Gut (bei Äpfeln und Avocados gut ersichtlich) durch enzymatisch unterstützte Reaktion von Flavonoiden oder anderen empfindlichen Inhaltsstoffen mit Luftsauerstoff braun werden. Ein Zusatz von Zitronensaft wirkt dabei als Gegenmittel. Die im Zitronensaft enthaltene Ascorbinsäure verhindert die Oxidation oder reduziert bereits oxidierte Verbindungen (Zusatz von Ascorbinsäure als Lebensmittelzusatzstoff).

In der Medizin spielen Enzyme eine wichtige Rolle. Viele Arzneimittel hemmen Enzyme oder verstärken ihre Wirkung, um eine Krankheit zu heilen. Prominentester Vertreter solcher Arzneistoffe ist wohl die Acetylsalicylsäure, die das Enzym Cyclooxygenase hemmt und somit unter anderem schmerzlindernd wirkt.

Die folgende Tabelle gibt einen Überblick über die Einsatzgebiete von Enzymen. Zur Herstellung siehe Protein.

Die Diagnostik verwendet Enzyme, um Krankheiten zu entdecken. In den Teststreifen für Diabetiker befindet sich zum Beispiel ein Enzymsystem, das unter Einwirkung von Blutzucker einen Stoff produziert, dessen Gehalt gemessen werden kann. So wird indirekt der Blutzuckerspiegel gemessen. Man nennt diese Vorgehensweise eine „enzymatische Messung“. Sie wird auch in medizinischen Laboratorien angewandt, zur Bestimmung von Glucose (Blutzucker) oder Alkohol. Enzymatische Messungen sind relativ einfach und preisgünstig anzuwenden. Man macht sich dabei die Substratspezifität von Enzymen zu Nutze. Es wird also der zu analysierenden Körperflüssigkeit ein Enzym zugesetzt, welches das zu messende Substrat spezifisch umsetzen kann. An der entstandenen Menge von Reaktionsprodukten kann man dann ablesen, wie viel des Substrats in der Körperflüssigkeit vorhanden war.

Im menschlichen Blut sind auch eine Reihe von Enzymen anhand ihrer Aktivität direkt messbar. Die im Blut zirkulierenden Enzyme entstammen teilweise spezifischen Organen. Es können daher anhand der Erniedrigung oder Erhöhung von Enzymaktivitäten im Blut Rückschlüsse auf Schädigungen bestimmter Organe gezogen werden. So kann eine Bauchspeicheldrüsenentzündung durch die stark erhöhte Aktivität der Lipase und der Pankreas-Amylase im Blut erkannt werden.



</doc>
<doc id="1376" url="https://de.wikipedia.org/wiki?curid=1376" title="Element">
Element

Element ( ‚Grundstoff‘) bezeichnet:
Elemente bezeichnet:
Siehe auch:


</doc>
<doc id="1377" url="https://de.wikipedia.org/wiki?curid=1377" title="Eskimo-aleutische Sprachen">
Eskimo-aleutische Sprachen

Die eskimo-aleutischen Sprachen bilden eine kleine Sprachfamilie, deren Idiome von etwa 105.000 Menschen in Nordost-Sibirien, Alaska, Nord-Kanada und Grönland gesprochen werden. Zu den Eskimo-Sprachen gehören das Inuktitut oder auch Eastern Eskimo, das im Norden Alaskas, in Kanada und Grönland verbreitet ist, sowie die Yupik-Sprachen im Westen Alaskas und in Sibirien. Der aleutische Zweig besteht aus der Einzelsprache Aleutisch. Das Eskimo und die Yupik-Sprachen bilden jeweils ein Dialektkontinuum.

Die heute oft verwendete Bezeichnung "Inuit" für alle Eskimovölker und Eskimosprachen ist falsch, da hierbei die Yupik-Völker nicht berücksichtigt werden. Außerdem ist die früher für abwertend gehaltene Bezeichnung – sie stammt aus den Algonkin-Sprachen – in Wirklichkeit neutral: sie bedeutet nicht – wie früher angenommen – "Rohfisch-Esser", sondern eher "Schneeschuh-Knüpfer" (Campbell 1997:394).

Nach der aktuellen Literatur (u. a. Campbell 1997, Mithun 1999, Holst 2005) lassen sich die sechs Eskimosprachen und das Aleutische wie folgt klassifizieren:


Die Sprecherzahlen stammen aus Ethnologue 2009 und Holst 2005. Der Verwandtschaftsgrad der Eskimo-Sprachen untereinander ist etwa mit dem der romanischen Sprachen vergleichbar; das Aleutische verhält sich zu den Eskimo-Sprachen ungefähr wie eine baltische Sprache zu den romanischen Sprachen (Einschätzung nach Holst 2005).

Die Darstellung in Ethnologue, dass das Inuit in fünf separate Sprachen zerfällt – von denen dann jeweils zwei sogar zu Makrosprachen zusammengefasst werden –, wird in der Fachliteratur nicht geteilt.

Eine besondere genetische Nähe der sibirischen tschuktscho-kamtschadalischen Sprachen und der eskimo-aleutischen Sprachen wurde von einer Reihe von Forschern angenommen, ist aber nie wirklich nachgewiesen worden. Diese These wurde im größeren Zusammenhang der eurasiatischen Makrofamilie von Joseph Greenberg wiederbelebt.
Nach Joseph Greenberg (2001) stellen die eskimo-aleutischen Sprachen eine Komponente seiner hypothetischen eurasiatischen Makrofamilie dar.

Nach Greenbergs Amerika-Theorie (1987) repräsentieren die eskimo-aleutischen Sprachen, die Na-Dené-Sprachen und der ganze Rest der indigenen amerikanischen Sprachen (zusammengefasst unter der Bezeichnung Amerind) die drei genetisch unabhängigen indigenen Sprachfamilien Amerikas, die auch separaten Einwanderungswellen von Nordostsibirien entsprechen.

Nach neueren Theorien (z. B. Holst 2005) sind die eskimo-aleutischen Sprachen mit den Wakash-Sprachen genetisch verwandt. Holst begründet das durch eine Liste von 62 Wortgleichungen und die Herleitung einiger Lautgesetze. Diese Beziehung überschreitet die von Joseph Greenberg gezogene Grenze zwischen den eskimo-aleutischen und den Amerind-Sprachen und wäre – falls sie sich bestätigen lässt – ein starkes Argument gegen Greenbergs grundsätzliche Einteilung der amerikanischen Sprachen in die drei Gruppen Eskimo-Aleutisch, Na-Dene und Amerind.

Die eskimo-aleutischen Sprachen haben eine agglutinierende Morphologie und sind polysynthetisch. Die Wort- und Formenbildung erfolgt durch Serien von Suffixen. Die Grundwortstellung ist SOV (Subjekt – Objekt – Verb). Die Eskimo-Sprachen sind ergativisch, der Agens eines transitiven Verbums wird durch den Ergativ, der Agens eines intransitiven Verbs und der Patiens ("das Objekt") des transitiven Verbes durch den Absolutiv gekennzeichnet. (Da der Ergativ auch noch die Funktion des Genitivs übernimmt, wird er in den Grammatiken der Eskimo-Sprachen meist "Relativ" genannt.) Beim Aleutischen ist die Frage der Ergativität bisher nicht eindeutig geklärt. Das Substantiv geht seinen bestimmenden Ergänzungen (Attributen) voraus, allerdings steht der Genitiv "vor" seinem Substantiv („des Mannes Haus“). Es werden Postpositionen (keine Präpositionen) verwendet. Wegen der polysynthetischen Struktur ist die Unterscheidung der Kategorien "Wort" und "Satz" problematisch.

Die eskimo-aleutischen Sprachen besitzen – im Gegensatz zu den benachbarten Sprachen Nordasiens – keine Vokalharmonie. Die Kategorie Genus existiert nicht, es werden keine Artikel verwendet. Die 1. Person Plural unterscheidet nicht – wie die Mehrzahl benachbarter Indianersprachen – zwischen inklusiven und exklusiven Formen (je nachdem, ob der Angesprochene mit einbezogen wird oder nicht). Die Wortart Adjektiv existiert nicht, sie wird durch Partizipien von Zustandsverben ersetzt.

Es ist ein weit verbreiteter Irrtum, dass die eskimo-aleutischen Sprachen zahlreiche Wörter für Schnee hätten. Das Gerücht wurde 1940 von Benjamin Lee Whorf in die Welt gesetzt. Tatsächlich gibt es beispielsweise im Westgrönländischen nur zwei Wörter für Schnee: "qanik" »Schnee in der Luft, Schneeflocke« und "aput" »Schnee auf dem Boden«.



Im Plural gibt es keine Unterscheidung zwischen den Formen des Absolutivs und Ergativs:



Die Formen des Ergativs und Genitivs fallen im Grönländischen zusammen, weswegen man diesen Fall zusammenfassend Relativ nennt. Der Genitivbezug wird doppelt gekennzeichnet: einmal durch die Verwendung des vorangestellten Relativs (Genitivs), zusätzlich durch ein Possessivsuffix am Besitz. (Vergleichbar ist die umgangssprachliche deutsche Bildung „dem Mann sein Haus“, nur dass hier der Dativ für den Besitzer verwendet wird.)





</doc>
<doc id="1382" url="https://de.wikipedia.org/wiki?curid=1382" title="Edinburgh">
Edinburgh

Edinburgh (schottisch-gälisch "Dùn Èideann" []; ; amtlich "City of Edinburgh") ist seit dem 15. Jahrhundert die Hauptstadt von Schottland (bis dahin war es Perth). Seit 1999 ist Edinburgh außerdem Sitz des Schottischen Parlaments.

Edinburgh ist mit etwa 493.000 Einwohnern nach Glasgow die zweitgrößte Stadt Schottlands und seit 1996 eine der 32 schottischen Council Areas. Die Stadt liegt an Schottlands Ostküste im Vereinigten Königreich Großbritannien und Nordirland auf der Südseite des Firth of Forth.

Edinburgh ist wahrscheinlich benannt nach dem gododdinischen König Clydno Eiddyn (siehe auch Cynon fab Clydno) als Din Eiddyn (Eiddyns Festung), weniger wahrscheinlich nach König Edwin (auch Eadwine oder Æduini) von Northumbria. Anderen Quellen zufolge leitet sich der Name von der gälischen Bezeichnung "Dùn Èideann" ab, was „Festung am Hügelhang“ bedeutet. Oft wird die Stadt auch „Athen des Nordens“ (nach einem Zitat von Theodor Fontane), „Stadt der sieben Hügel“ oder „Festival-Stadt“ genannt. Sir Walter Scott nannte sie "My own romantic town". Überholt ist der Beiname "Auld Reekie", „Alte Verräucherte“, den Edinburgh seinen früher beständig rauchenden Fabrikschornsteinen verdankte.

Schottische Auswanderer haben den Namen Edinburghs in die Welt getragen. Heute findet sich etwa:

Mehrere Städte haben den gälischen Namen Edinburghs, Dunedin, gewählt:

Es gibt eine Menge vorgeschichtlicher Relikte im Edinburgher Stadtgebiet. Vor der Trockenlegung des Bereichs gab es Seen und Sümpfe zwischen den Hügeln, auf denen die Wohnplätze und Siedlungen lagen. Während der letzten zwei Jahrhunderte wurden prähistorische Grabstätten (Arthur’s Seat) und Horte mit Bronzeartefakten entdeckt. In der Straße "Caiystane View" steht in Richtung auf die "Oxgangs Road" ein großer Menhir (engl. „Standing stone“) mit Schälchen (engl. Cuprnarks). Neben dem Newbridge Kreisverkehr, auf der Westseite der Stadt, liegt das bronzezeitliche Ritualzentrum am Huly Hill Cairn. Es gibt eisenzeitliche Befestigungen aus dem 1. Jahrtausend v. Chr. auf dem Wester Craiglockhart Hill und auf dem Hillend, dem nächstgelegenen der Pentland Hills. Mesolithische Spuren und die eines römischen Kastells liegen in Cramond, einem Dorf am Rande von Edinburgh. Die Statue einer Löwin, die einen Mann verschlingt, wurde in der Mündung des Almond gefunden. Ein Piktischer Symbolstein wurde in den Princes Street Gardens als Teil der Clapper bridge verwendet wiedergefunden.

Zum Ende des 1. Jahrhunderts landeten die Römer in Lothian und entdeckten einen keltisch-britannischen Stamm, den sie Votadini nannten. Irgendwann vor dem 7. Jahrhundert n. Chr. errichten die Gododdin, die wahrscheinlich Nachkommen der Votadini waren, die Hügelfestung von "Din Eldyn" oder "Etin". Obwohl die genaue Position nicht bekannt ist, ist anzunehmen, dass sie einen die Umgebung überragenden Standort wie Castle Rock, Arthur's Seat oder Calton Hill gewählt haben.

Zunächst war Scone (heute "Old Scone") das Zentrum des vereinigten Königreichs von Alba (→ Königreich Schottland). Es verlor im späteren Mittelalter an Bedeutung, und das nur 1½ km flussabwärts gelegene Perth übernahm seine Rolle. Auch andere „burghs“ (Freistädte) wie Stirling spielten für die schottische Geschichte eine bedeutende Rolle. Nach der Ermordung Jakobs I. 1437 fiel die Rolle der Hauptstadt Schottlands dann Edinburgh zu. Die Hauptstadtfunktion im Mittelalter ergab sich aus der häufigen, lange dauernden Anwesenheit des königlichen Hofes, der an verschiedenen Orten Station machte. Das historische Parlament von Schottland tagte ebenfalls an unterschiedlichen Orten.

Im Jahr 1093 wird eine Burg in Edinburgh erwähnt, aus der sich das die Stadt dominierende Edinburgh Castle entwickelte.

Die Kirche des heiligen Ägidius, genannt St Giles’ Cathedral, wurde zum Mittelpunkt der wachsenden Ortschaft. Ihre erste urkundliche Erwähnung stammt aus dem Jahr 854, das noch heute existierende Gebäude wurde etwa seit dem Jahr 1120 gebaut. Im 16. Jahrhundert predigte John Knox in St Giles, die heute die "High Kirk of Edinburgh" der Church of Scotland ist.

1128 wurde das Chorherrenstift Holyrood Abbey von König David I. errichtet, allerdings weit außerhalb der damaligen Stadt. Zwischen Edinburgh und der "Abtei des Heilgen Kreuzes" („holy rood“) lag auch noch die Stadt "Canongate". (Mit „"canon"“ in ihrem Namen sind Kanoniker gemeint.) Neben Holyrood Abbey, von der heute nur noch Ruinen zeugen, wurde in der Folge Holyrood Palace erbaut, dieser ist als "Palace of Holyroodhouse" offizielle Residenz des britischen Monarchen und bildet den östlichen Abschluss der „Royal Mile“.

1583 wurde in Edinburgh eine Universität gegründet, die allerdings in der geschichtlichen Folge erst die vierte in Schottland ist; die University of St Andrews geht auf das Jahr 1450 zurück.

Zur wechselvollen Geschichte der Stadt gehört auch der sogenannte Bischofskrieg von 1639. König Karl I. von England und Schottland versuchte, seinen Willen der "Kirche von Schottland" durch ihm genehme Bischöfe aufzuzwingen und auch ein nach der englischen Liturgie geschaffenes Gebetbuch einzuführen. Es kam zu Aufständen, als deren Initiatorin die Marktfrau "Jenny Geddes" genannt wird, die in der St. Giles' Kathedrale einen Stuhl nach dem Pfarrer warf.

Zu den wichtigsten Daten der Geschichte Edinburghs und ganz Schottlands gehört das am 1. Mai 1707 in Kraft getretene Vereinigungsgesetz, der Act of Union. Dieses Gesetz schuf die Grundlage für die Vereinigung des Königreichs England und des Königreichs Schottland.

Das wieder errichtete schottische Parlament konstituierte sich nach fast 300 Jahren am 12. Mai 1999 in Edinburgh.

Die meisten Einwohner Edinburghs sind Schotten, daneben gibt es viele Iren und auch Deutsche, Polen, Italiener, Ukrainer, Pakistaner, Sikhs, Bengalen, Chinesen und Engländer. Innerhalb dieser Mischung klingt auch Ulster häufig durch. Es gibt Schulen für katholische und protestantische Kinder. Im Juli findet in Edinburgh jedes Jahr einer der größten "Orange Walks" außerhalb Nordirlands statt (zum Gedenken an den protestantischen Sieg in der Schlacht am Boyne).

Edinburgh ist die historische Hauptstadt von Schottland und der früheren Grafschaft "Edinburghshire", die heute Midlothian heißt. Neben Glasgow, Dundee und Aberdeen war Edinburgh seit 1890 eines der vier "Counties of cities" in Schottland. 1975 wurde Edinburgh zu einem District der Region Lothian und 1996 wurde die Stadt im Rahmen der Einführung einer einstufigen Verwaltungsstruktur zur Council Area „City of Edinburgh“. Edinburgh ist auch eine der Lieutenancy Areas von Schottland.

Der Edinburgh City Council umfasst 63 Sitze. Seit der Kommunalwahl 2017 besitzt mit 19 Sitzen die Scottish National Party die Mehrheit.

Oberbürgermeister ("Lord Provost") ist seit der Wahl 2012 Frank Ross (Scottish National Party).
Edinburgh hatte schon seit dem 14. Jahrhundert ein Stadtwappen, es wurde aber erst 1732 vom "Lord Lyon King of Arms" offiziell erwähnt. Nach der Verwaltungsreform 1975 gab der "City of Edinburgh District Council" nach historischer Vorlage ein neues Wappen in Auftrag: Im Schild, über dem die schottische Krone und ein Admiralitätsanker prangen, ist der schwarze Basaltfelsen mit der Burg zu erkennen, deren Türme rote Fahnen tragen. Das Stadtmotto „Nisi Dominus Frustra“, dem 127. Psalm entnommen, proklamiert, dass ohne die Hilfe Gottes nichts von Dauer sein kann. Schildhalter sind ein Mädchen und eine Hirschkuh, das Symbol des heiligen Ägidius, des Schutzpatrons der Stadt. Die Burg war im Mittelalter als "Castrum Puellarum" – Burg der Mädchen – bekannt, der Überlieferung nach ein sicherer Hort für Prinzessinnen.

Edinburgh unterhält offizielle bilaterale Beziehungen mit anderen Städten. Diese Kooperationen haben zum Ziel, den Austausch von Informationen und Fachwissen in Bereichen von gemeinsamem Interesse zu ermöglichen.
Insgesamt gibt es elf Gemeindepartnerschaften: 
Traditionell ist Edinburgh ein wichtiges Handelszentrum, das Schottland mit Skandinavien und Kontinentaleuropa verbindet. Die Bedeutung des Hafens von Leith hat allerdings in den letzten Jahrzehnten stetig abgenommen.

Edinburgh hat nach London die zweitstärkste Wirtschaft aller Städte im Vereinigten Königreich und mit 53 % der Bevölkerung den höchsten Anteil an Arbeitnehmern mit einem beruflichen Abschluss. Im "UK Competitiveness Index 2013", der die Wettbewerbsfähigkeit britischer Städte vergleicht, lag Edinburgh auf Platz 4 aller Großstädte im Vereinigten Königreich. Es liegt bei den Verdiensten und bei der Arbeitslosigkeit auf dem 2.  Platz hinter London.

Während im 19. Jahrhundert vor allem das Brauereiwesen, Banken und Versicherungen sowie Druck- und Verlagswesen prägende Wirtschaftszweige waren, liegt der Schwerpunkt im 21. Jahrhundert vor allem auf Finanzdienstleistungen, wissenschaftlicher Forschung, Hochschulbildung und Tourismus. Im Jahr 2014 betrug die Arbeitslosigkeit in Edinburgh 4,3 % und lag damit deutlich unter dem schottischen Durchschnitt von 6,3 %.

Das Bankwesen ist seit über 300 Jahren eine Hauptstütze der Wirtschaft Edinburghs. Die Bank of Scotland (heute Teil der Lloyds Banking Group) wurde 1695 durch das schottische Parlament gegründet. Heute ist die Stadt durch die Finanzdienstleistungsbranche mit ihrem besonders starken Versicherungs- und Investmentsektor das zweitgrößte Finanzzentrum in Großbritannien und eines der größten in Europa. Edinburgh ist der Sitz von Scottish Widows, Standard Life, Bank of Scotland, Halifax Bank of Scotland (HBOS), Tesco Bank und AEGON UK. Die Royal Bank of Scotland eröffnete ihren neuen Hauptsitz in Gogarburn im Westen der Stadt im Oktober 2005. Verschiedene Finanzdienstleister haben im Vorfeld des Referendums über den Verbleib Schottlands im Vereinigten Königreich 2014 angekündigt, dass sie im Fall einer Selbstständigkeit Schottlands ihren Firmensitz nach London verlegen würden.

Die größten Arbeitgeber der Stadt waren 2014: National Health Service Lothian (19.500 Mitarbeiter), City of Edinburgh Council (19.260), University of Edinburgh (12.650), Lloyds Banking Group (9000), The Royal Bank of Scotland Group (8000), Standard Life (5000), Scottish Government (4000), Tesco and Tesco Bank (2600) und AEGON UK (2100). Das Durchschnitts-Bruttoeinkommen eines Arbeitnehmers betrug 2012 £ 19.100 (etwa 26.700 Euro). Damit lag Edinburgh auf dem zweiten Platz hinter London (£ 21.400). Auch im Ranking der Bruttowertschöpfung pro Einwohner 2012 lag Edinburgh mit £ 38.100 auf dem zweiten Platz hinter London (£ 40.200).

Der Tourismus ist ein weiteres wichtiges Element der Wirtschaft Edinburghs. Es ist die am meisten von ausländischen Besuchern besuchte Stadt im Großbritannien nach London. Altstadt und Neustadt von Edinburgh sind im Jahr 1995 in die Liste des UNESCO-Weltkulturerbes aufgenommen worden. Im Jahr 2013 besuchten 3,45 Mio. Touristen die Stadt, davon 1,3 Mio. aus dem Ausland. Die größte Gruppe der ausländischen Touristen waren US-Amerikaner (192.000) vor Deutschen (174.000).

Edinburgh ist als wichtiger Verkehrsknotenpunkt mit Eisenbahn- und Straßenverbindungen mit dem übrigen Schottland und mit England verbunden.

Der öffentliche Personenverkehr innerhalb der Stadt wird durch ein umfassendes Busnetz bedient, das den größten Teil der Verbindungen ohne Umsteigen (Einzelfahrscheine berechtigen nicht zum Umsteigen) abdeckt. Nach positivem Ausgang der Abstimmung im Schottischen Parlament im Juni 2007 (gegen die Vorbehalte der SNP-Minderheitsregierung) begann der Bau der Straßenbahn Edinburgh, die den Flughafen und Granton via Zentrum und "Leith Walk" verbinden soll. Aufgrund von Finanzierungsproblemen wurde die ursprüngliche Strecke auf den Abschnitt vom Flughafen bis in die Innenstadt reduziert. Diese Strecke wurde am 31. Mai 2014 eröffnet.

Zentral in der Stadt gelegen ist der Bahnhof Edinburgh Waverley an der "East Coast Main Line", der teilweise als Durchgangsbahnhof und teilweise als Kopfbahnhof genutzt wird. Fernverkehr besteht Richtung Mittelengland und London, ScotRail bedient die Verbindungen innerhalb Schottlands. Im September 2015 ist das Streckennetz des regionalen Schienenverkehrs um die von Edinburgh bis Tweedbank wiederaufgebaute Waverley Line ("Borders Railway") ergänzt worden.

Der internationale Flughafen Edinburgh befindet sich 13 Kilometer westlich der Stadt. Neben den meist innerbritischen Flügen gibt es auch Verbindungen zu europäischen Zielen und in jüngerer Zeit ein paar tägliche Transatlantikflüge.

Neben den Autobahnen M8 nach Glasgow und M9 nach Stirling hat Edinburgh eine umfassende Fernstraßen-Anbindung an das Straßennetz von Großbritannien und ist beispielsweise Endpunkt der A1 von London.

Edinburgh verfügt über keine überregionale Fährverbindung; der nächste Fährhafen ist das rund 19 Kilometer entfernte Rosyth.

Edinburgh hat drei international bekannte Universitäten, die Edinburgh Napier University, die Heriot-Watt University mit der Edinburgh Business School und die Universität Edinburgh (ebenfalls mit Business School), wobei letztere neben Universitäten wie Oxford oder Cambridge zu den besten Großbritanniens zählt. Seit 2007 kann sich das Queen Margaret University College im Vorort Musselburgh auch Universität nennen.

Hier hat die 1834 gegründete "Edinburgh Geological Society" (Edinburger Geologische Gesellschaft) ihren Sitz. In der Stadt hat auch der British Geological Survey seine Hauptfiliale für Schottland.

In Edinburgh findet jeden Sommer das Edinburgh Festival statt, das aus einer Vielzahl – zum Teil namhafter – kultureller Veranstaltungen besteht. Internationale Bedeutung im Bereich Theater und Musik hat dabei sowohl das Edinburgh International Festival für die Hochkultur, als auch das Edinburgh Festival Fringe für experimentelle Spielformen erlangt. Ebenfalls ein großer Publikumsmagnet ist das Edinburgh Military Tattoo.

Die "National Gallery of Scotland" beherbergt repräsentative Sammlungen der europäischen Malerei mit einigen bekannten Highlights und zeigt wechselnde Sonderausstellungen. Die Sammlungen der "National Galleries of Scotland" sind in fünf Galerien im Stadtgebiet von Edinburgh verteilt:

In Edinburgh gibt es eine Vielzahl von Museen, wie z. B. die National Museums of Scotland, das Royal Museum, die National Library of Scotland, das National War Museum of Scotland, das Museum of Edinburgh, das Museum of Childhood und die Royal Society of Edinburgh.

Die Usher Hall ist eine Konzerthalle für klassische Musik im Westteil der Stadt an der Lothian Road. Hier spielt regelmäßig auch das Royal Scottish National Orchestra.

Es gibt zwei Multiplex-Kinocenter sowie das Edinburgh Filmhouse, wo das jährliche Edinburgh Film Festival ausrichtet wird.

Über Edinburghs Gassen und Friedhöfe kursieren diverse Legenden und Geistergeschichten. Deshalb werden für schaulustige Touristen auf der "Royal Mile" fast allabendlich Gruseltouren (sogenannte "Ghost Tours") von verschiedenen Veranstaltern angeboten. Die Touren führen etwa auf den Greyfriars Graveyard oder in den Untergrund.

Edinburgh ist darüber hinaus bekannt für seine Pubs.


Zu den markantesten Sehenswürdigkeiten der Stadt zählen das Edinburgh Castle, der Palace of Holyroodhouse, die National Gallery of Scotland, die National Museums of Scotland, die Princes Street, die königliche Yacht "Britannia" sowie die Royal Mile.

Die "Royal Mile" besteht aus den Straßen "Canongate", "High Street" und "Castlehill" und hat tatsächlich etwa die Länge einer schottischen Meile, rund 1,8 km. Im Westen beginnt sie am "Edinburgh Castle" und führt über die ehemalige Highland Tolbooth Church, die St. Giles Cathedral (geweiht dem Stadtheiligen Ägidius von St. Gilles), das People’s Story Museum, das Museum of Edinburgh und das John-Knox-Haus bis zum "Palace of Holyroodhouse". Gegenüber diesem Palast befindet sich der moderne Bau des schottischen Parlaments. Quer zur "Royal Mile" verlaufen im Fischgrätenmuster kleine, häufig extrem steile Gassen, die "closes", "courts" oder auch "wynds" genannt werden. In der Altstadt befinden sich außerdem mehrere große Marktplätze.

Der Park Princes Street Gardens erstreckt sich zwischen dem "Castle Rock", auf dem die Burg erbaut wurde und der "Princes Street". Wo heute zwischen beiden der Park und der Bahnhof liegen, umgaben einst Sumpfland und Seen den Berg, wie noch auf älteren Gemälden zu sehen.

Den höchsten Punkt der Altstadt markiert ein „ehemaliger“ Kirchturm. Im 19. Jahrhundert wurde die "Highland Tolbooth Church" erbaut, 1979 wurde die Kirche geschlossen. „The Hub“ fand inzwischen vielfache neue Verwendungen, unter anderem als "Festivalzentrale".
Mit der "Princes Street" beginnt die georgianische „New Town“, die sich mit ihren rechtwinklig angelegten Straßen nördlich der Eisenbahnanlagen erstreckt. Entlang dieser Prachtstraße aus dem 18. Jahrhundert reihen sich mehrere Denkmäler und Monumente.

In der „New Town“ befinden sich am "Picardy Place" die römisch-katholische Kathedrale von Edinburgh, die St. Mary’s Cathedral, ein neugotischer Bau von 1814. 1874 wurde der Grundstein für eine weitere Bischofskirche gelegt, ebenfalls Maria geweiht, es ist die episkopale St Mary’s Cathedral im "West End".

Der Royal Botanic Garden Edinburgh befindet sich nördlich des Stadtzentrums. Westlich des Botanischen Gartens liegen der Zoo und die "Gallery of Modern Arts".

Eine Aussicht über die Stadt bietet sich von dem 251 Meter hohen "Arthur’s Seat", der vulkanischen Ursprungs ist. Die "Salisbury Crags" sind am Fuß des Berges.

Im Hafen von Leith, am Ocean Terminal, liegt die ehemalige königliche Yacht "Britannia", die besichtigt werden kann. Der getrennte Hafen und der Burgberg haben dazu beigetragen, dass Edinburgh auch „Athen des Nordens“ (Vergleich Piräus – Akropolis) genannt wird.




</doc>
<doc id="1383" url="https://de.wikipedia.org/wiki?curid=1383" title="Eschborn">
Eschborn

Eschborn ist eine Stadt im südhessischen Main-Taunus-Kreis. Sie gehört zur Stadtregion Frankfurt und grenzt nordwestlich an das Stadtgebiet von Frankfurt am Main.

Eschborn liegt im Rhein-Main-Gebiet am östlichen Rand des Main-Taunus-Kreises. Es ist mit den Frankfurter Stadtteilen Sossenheim und Rödelheim benachbart und gehört zur Stadtregion Frankfurt. Die Frankfurter Innenstadt ist 7 km, der internationale Flughafen Frankfurt am Main 15 km entfernt. Zu Eschborn gehört der Stadtteil Niederhöchstadt, der im Norden an Eschborn angrenzt.

Eschborn grenzt im Norden an die Städte Kronberg im Taunus und Steinbach (Taunus) (beide Hochtaunuskreis), im Osten und Süden an die kreisfreie Stadt Frankfurt am Main sowie im Westen an die Stadt Schwalbach am Taunus (Main-Taunus-Kreis).

Im Jahr 770 wurde Eschborn als "Aschenbrunne" in einer Schenkungsurkunde an das Kloster Lorsch erstmals urkundlich erwähnt:

Der Name Aschenbrunne bedeutet so viel wie „Brunnen an der Esche“. Am 3. Juli 875 zerstörte eine Überschwemmung den Ort, wobei 88 Menschen und fast der gesamte Viehbestand getötet wurden. Die Eschborner Turmburg wurde im 11. Jahrhundert erbaut, sie wird mit den Ende des 12./Anfang des 13. Jhs. auftauchenden Herren von Eschborn in Verbindung gebracht, die ihren Hauptsitz kurz darauf nach Kronberg verlegten. Eschborn gehörte als Reichslehen den Herren von Kronberg, bis diese 1704 ausstarben und ihre Herrschaft an Kurmainz fiel.

1389 fand die Schlacht bei Eschborn statt. Im Rahmen des Krieges des rheinischen Städtebundes gegen den Pfalzgrafen zog die Stadt Frankfurt gegen die Ritter von Kronberg zu Felde. Die Kronberger und ihre zu Hilfe geeilten Verbündeten (der Pfalzgraf und die Hanauer) siegten und nahmen zahlreiche Gefangene, unter anderem auch den Bürgermeister von Frankfurt, die sie erst gegen eine Zahlung von 73.000 Goldgulden Lösegeld freigaben. Bei einem Gefecht bei Höchst 1622 zwischen General Tilly und Christian von Braunschweig während des Dreißigjährigen Kriegs wurden die alte Burganlage und nahezu der gesamte Ort zerstört (siehe dazu Näheres unter Schlacht bei Höchst).

Durch den Frieden von Lunéville 1801 gelangte der Fürst von Nassau-Usingen in den Besitz der Herrschaft Kronberg. 1806 gingen Kronberg und Niederhöchstadt gemeinsam als Teil des Fürstentums Nassau-Usingen im neu formierten Herzogtum Nassau unter der Führung des Hauses Nassau-Usingen auf. Nach der Annektierung des Herzogtums durch Preußen im Jahre 1866 fielen Eschborn und Niederhöchstadt an das Königreich Preußen. Mit der Gründung des Deutschen Kaiserreichs gehörte Eschborn ab 1871 zum Deutschen Reich. Mit der Eröffnung der Bahnstrecke Rödelheim-Kronberg der Kronberger Bahn am 19. August 1874 erhielt Eschborn den ersten Eisenbahnanschluss.
In Eschborn lebten bei Kriegsbeginn 1914 1549 Menschen; davon rückten 205 Männer zum Kriegsdienst ein. Bis zu 50 Kriegsgefangene aus Frankreich und Russland lebten in Eschborn.

Am 1. Januar 1939 startete der Bau des Militärflugplatzes Frankfurt-Sossenheim (späterer Name: Fliegerhorst Eschborn), der nie vollendet wurde. Der Militärflugplatz, den die Deutschen Ende der 1930er Jahre unter dem Tarnnamen „Schafweide“ errichteten, bestand zunächst nur aus einer großen Wiesenfläche und wenigen Baracken. Es entstanden mehrere große aus Stein gebaute Hangars, von denen heute noch einer steht. Die Kommandantur sowie die Mehrzahl der übrigen Gebäude wurden nur als Baracken gebaut. Der Flugplatz war unter der Bezeichnung Eschborn während des gesamten Krieges im Einsatz. Insbesondere wurden dort auf Lastenseglern Piloten ausgebildet. Von April 1945 bis Kriegsende (Mai 1945) waren auf dem Platz, der unter deutscher Verwaltung keine befestigte Start- und Landebahn hatte, amerikanische Jägereinheiten stationiert, die von dort aus Einsätze gegen das Reich flogen. Nach Kriegsende kam dem "Flugplatz Eschborn" große Bedeutung als Ausweichflugplatz für den noch nicht wiederhergestellten Flughafen Frankfurt am Main zu. Mit der Wiederinbetriebnahme des Frankfurter Flughafens wurde der Flugbetrieb in Eschborn eingestellt. Auf dem Gelände, welches zu einem Teil auf der Gemarkung von Schwalbach liegt, verblieb bis 1992 die US-Kaserne "Camp Eschborn", wo amerikanische Streitkräfte (Pioniereinheit mit schwerem Gerät) stationiert waren. Das Gelände wurde nach dem Abzug von der Stadt gekauft und zum Gewerbegebiet "Camp-Phönix-Park" umgebaut. Die 1948 bis 1972 betriebene Empfangsfunkstelle Eschborn nutzten vor allem US-Soldaten zur Überseetelefonie in die Heimat.

Das Zweite Deutsche Fernsehen (ZDF) nahm unter seinem ersten Intendanten Karl Holzamer am 1. April 1963 seinen Betrieb in Eschborn auf, zog jedoch 1964 nach Wiesbaden und 1974 nach Mainz.

Zur 1200-Jahr-Feier 1970 wurde Eschborn das Stadtrecht verliehen.

Im Zuge der Gebietsreform in Hessen wurde die Nachbargemeinde Niederhöchstadt freiwillig durch den Eingemeindungsvertrag vom 15. September 1971 am 31. Dezember 1971 in die Stadt Eschborn eingemeindet.

In historischen Dokumenten ist der Ort im Laufe der Jahrhunderte unter wechselnden Ortsnamen belegt:


Die folgende Liste zeigt die Territorien bzw. Verwaltungseinheiten denen Eschborn unterstand im Überblick:

Ursache für den hohen Anstieg der Einwohnerzahl im Jahr 1972 war die Eingemeindung des Stadtteils Niederhöchstadt.

Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:

Amtierender Bürgermeister ist seit dem 16. Februar 2014 Mathias Geiger (FDP, unabhängig kandidiert).

Die Bürgermeisterwahl am 2. September 2007 hatte folgendes Ergebnis:

Die Wahlbeteiligung lag bei 51,3 Prozent (2001: 50,1). Gemeindekennziffer: 436003

Die mit den Bundestags- und hessischen Landtagswahlen gleichzeitige Bürgermeisterwahl am 22. September 2013 hatte im ersten Wahlgang folgendes Ergebnis:

Die Wahlbeteiligung lag bei 75,6 Prozent. Gemeindekennziffer: 436003

Die am 6. Oktober 2013 folgende Stichwahl zwischen Wilhelm Speckhart (CDU) und Mathias Geiger (Unabhängig) hatte folgendes Ergebnis:

Die Wahlbeteiligung lag bei 58,2 Prozent. Gemeindekennziffer: 436003

Die Vorgänger waren u. a. Edwin Mämpel (SPD) die Jahre 1945/46, Heinrich Graf (SPD) von 1946 bis 1961, danach Hans Georg Wehrheim (SPD) von 1962 bis 1979, Jochen Riebel (CDU) von 1979 bis 1984, Manfred Tomala (CDU) von 1984 bis 1990 und Martin Herkströter (CDU) von 1990 bis 2001. Bürgermeister von Niederhöchstadt war zum Zeitpunkt der Eingemeindung Heinz Henrich, der 1966 auf Helmut Neumann folgte. Erster Nachkriegsbürgermeister war Wilhelm Bauer (1945 bis 1964).

Blasonierung: „In Rot aus einer goldenen Krone im Schildfuß wachsend ein silberner offener Flug, unten rechts und oben links belegt mit jeweils vier blauen Eisenhütchen, 2:2 gestellt.“

Das aktuelle Wappen wurde der Gemeinde in dieser Form am 8. Februar 1937 vom Oberpräsidenten der preußischen Provinz Hessen-Nassau, Philipp Prinz von Hessen, verliehen. Bis längstens zu diesem Termin galt ein Vorgängerwappen.


Seit 1985 unterhält Eschborn eine Partnerschaft mit dem französischen Montgeron, das ca. 17 km von Paris entfernt liegt. Im Jahr 2001 unterzeichneten die vier Städte Eschborn (D), Montgeron (F), Póvoa de Varzim (PT) und Żabbar (MLT) einen Freundschaftsvertrag. Im Mai 2010 hat Eschborn mit Póvoa de Varzim und Żabbar eine offizielle Städtepartnerschaft besiegelt.

Die so genannten "Freizeitparks" liegen entlang des Westerbachs, dem einzigen fließenden Gewässer in Eschborn. Es handelt sich dabei um vorwiegend landwirtschaftliche genutzte Flächen, d. h. Ackerbau, Weiden, vereinzelt ein paar Streuobstwiesen. Für Fußgänger und Radfahrer führen durch diese Gebiete gern benutzte Wege als Verbindung zwischen den beiden Eschborner Stadtteilen bzw. zu den Nachbarstädten. Durch die Kirchwiesen (beginnend hinter der Grundschule in Niederhöchstadt) gelangt man zu Fuß in ca. einer Stunde nach Kronberg, durch die Unterwiesen (beginnend hinter dem ehemaligen Bauhof in Eschborn) nach Frankfurt-Rödelheim. Die Oberwiesen liegen zwischen Niederhöchstadt und Eschborn, hier ist auch der beliebte "Traktor­spielplatz", der nach dessen Hauptattraktion benannt ist. Zusammen bilden diese die „grüne Lunge“ der Stadt, wo keine Bauten und Versiegelungen erfolgen dürfen.

Das Arboretum Main-Taunus ist ein ca. 76 ha großer Baum- und Sträucherpark. Hier sind ca. 600 Baum- und Sträucherarten aus allen Teilen der Erde angepflanzt. Das Arboretum liegt auf der Gemarkung der Städte Schwalbach am Taunus, Sulzbach (Taunus) und Eschborn. Durch das Arboretum führen mehrere Rad- und Wanderwege; es ist ganzjährig öffentlich zugänglich.

Aufgrund seiner direkten Nachbarschaft zu Frankfurt ist Eschborn eine finanziell wohlhabende Stadt im sogenannten Frankfurter „Speckgürtel“, was sich nicht zuletzt in vielen Bauvorhaben widerspiegelt. Weiterhin finanziert die Stadt Eschborn annähernd die Hälfte der Kreisumlage des Main-Taunus-Kreises.


Die Skulpturenachse Eschborn ist eine Sammlung von acht Skulpturen im öffentlichen Raum in Eschborn.
In den folgenden Abbildungen sind die sieben Einzelskulpturen "Travel a Head", "Hua", "Phönix", "Fulcrum", "Drei Säulen", "Versatzstück", "Steine für Eschborn" und die Doppelskulptur "Adam und Eva" zu sehen.



Einer der erfolgreichsten Vereine in Eschborn ist der seit Januar 2006 in Insolvenz befindliche Fußballverein 1. FC Eschborn 1930.

Ein weiterer, sehr erfolgreicher Eschborner Verein ist der Tennisclub "tennis 65 eschborn", der neben vielen Mannschaften in den Regionalligen, Landesligen, Bezirksklassen und den Kreisligen auch einen deutschen Meister hat. Die Mannschaft Herren 55+ wurde 2010 und 2011 Deutscher Mannschaftsmeister.

Zu den größten Vereinen in der Stadt zählen u. a. die "TuRa Niederhöchstadt" und der "Turnverein Eschborn 1888". Zahlreiche weitere Vereine ergänzen ein breites, insbesondere sportliches Angebot.

Zwischen den Stadtteilen befindet sich seit Anfang 1970er ein Hallen- und Freibad. Das sog. Wiesenbad wurde am 1. September 2001 nach einer einjährigen Sanierung und Renovierung wieder eröffnet und bietet Sauna-Bereich und Hallenbad sowie ein Freibad je mit 25-Meter-Bahn.

Zudem wird in Eschborn jährlich zum 1. Mai (im Jahr 2011 zum 50. Mal) ein international bekanntes Radrennen gestartet, das bis 2008 als "Rund um den Henninger Turm" bekannt war. Seitdem wechselt der Name abhängig von den Sponsoren, enthält aber über "Eschborn-Frankfurt City Loop" zu "Rund um den Finanzplatz Eschborn-Frankfurt" den Namen Eschborns; außer Radprofis starten in separaten Wertungen auch Amateure und Nicht-Organisierte, zuletzt nicht mehr von Eschborn aus.

Eschborn bietet u. a. aufgrund seiner Nähe zu Frankfurt am Main und seiner guten Erreichbarkeit zirka 30.000 Arbeitsplätze (Stand: 2011). Sie konzentrieren sich vor allem im Gewerbegebiet Süd sowie in den Groß- und Einzelhandelsunternehmen im Osten und Westen der Stadt. Knapp 90 Prozent sind im Dienstleistungsbereich angesiedelt, u. a. bei den Unternehmen Vodafone (ehem. Arcor), Deutsche Bank, Deutsche Börse, VR Leasing, Siemens, Deutsche Telekom, Ernst & Young, IBM, Techem und Randstad Deutschland,

sowie bei folgenden Behörden und Verbänden:
und zahlreichen weiteren Consulting-, Marketing- und Softwareunternehmen. Das Möbelunternehmen Mann Mobilia unterhält außerdem eine große Filiale in Eschborn. Eurest ist ein 1974 gegründeter Betreiber von Betriebsrestaurants.

Der angekündigte Umzug eines Großteils der Mitarbeiter der Deutschen Börse von Frankfurt am Main in ein Übergangsgebäude im Eschborner Gewerbegebiet Süd sorgte 2008 für viel Aufsehen in der Regionalpresse. Bereits 2010 bezogen die meisten Mitarbeiter der Deutschen Börse ein neu errichtetes Gebäude in Eschborn. Der Grund für den Ortswechsel waren die finanziellen Vorteile, vor allem der deutlich geringere Gewerbesteuerhebesatz mit nur 280 Prozent (seit 2016: 330 Prozent) gegenüber dem benachbarten Frankfurt am Main mit 460 Prozent. Offizieller Firmensitz der Deutschen Börse bleibt jedoch Frankfurt am Main.

Bekannte Unternehmen mit ehemaligem Sitz in Eschborn sind u. a. die Unternehmensgruppe Georg von Opel, Arthur Andersen und Linotype AG, heute Monotype GmbH.

Es existieren in Eschborn zwei Zeitungen, die ausschließlich oder vorwiegend in Eschborn erscheinen. Alle zwei Wochen werden die "Eschborner Nachrichten", jede Woche der "Eschborner Stadtspiegel" herausgegeben. Unabhängig von diesen Verlagen existieren drei lokale Online-Medien, die Eschborner Zeitung, das Eschborner OnlineMagazin und das Eschborner Stadtmagazin. Ergänzt werden die vier Medien durch die regionale Presse Höchster Kreisblatt, Frankfurter Rundschau sowie Frankfurter Allgemeine Zeitung.

In Eschborn gibt es vier Schulen:

Die nächsten Autobahnanschlüsse zur A 66 und A 5 sind etwa 1,5 km entfernt, die S-Bahn-Linien S3 und S4 fahren durch Eschborn (Haltepunkte Eschborn Süd, Eschborn und Niederhöchstadt) und bieten eine Direktverbindung in die Frankfurter Innenstadt.

Seit Anfang 2013 lässt das Frankfurter Verkehrsdezernat eine mögliche Verlängerung der U-Bahn Linie U6 nach Eschborn prüfen und hat eine entsprechend konkrete Untersuchung des Projekts in Auftrag gegeben. Dabei geht es um die Verlängerung der U-Bahn-Linie U6, die derzeit an der Heerstraße in Frankfurt-Praunheim endet. Auch eine Haltestelle im Gewerbegebiet Helfmann-Park ist unter Umständen möglich. Diese würde laut dem Bürgermeister von Eschborn „den Standort Eschborn noch weiter aufwerten“. Einen genauen Zeitplan für das Projekt gebe es noch nicht. Ein Treffen mit Vertretern aus Eschborn, Frankfurt und Oberursel zum U-Bahn-Thema soll stattfinden. Bündnis 90/Die Grünen im Hochtaunuskreis haben sich für einen Ausbau der U6 über Eschborn hinaus mit Haltestellen in Steinbach und Kronberg ausgesprochen. Die Stadt Oberursel im Hochtaunuskreis ist seit den 1970er-Jahren über die U-Bahn-Linie U3 an das Frankfurter U-Bahn-Netz angeschlossen.





</doc>
<doc id="1384" url="https://de.wikipedia.org/wiki?curid=1384" title="Eppstein">
Eppstein

Eppstein ist eine Stadt im Main-Taunus-Kreis im Land Hessen in Deutschland. Der Verwaltungssitz befindet sich im Stadtteil Vockenhausen.

Die Stadt Eppstein liegt westlich von Frankfurt am Main und etwa 15 km nordöstlich der Landeshauptstadt Wiesbaden im Vordertaunus.

Die Region ist geprägt durch den "Eppsteiner Schiefer", schwach metamorphe Gesteine wie Phyllite, Grünschiefer und Serizit-Gneise. Sie sind durch Chlorit und Epidot grünlich gefärbt. Ausgangsprodukte dieser Gesteine waren Tonsteine und Vulkanite.

Am 29. Juni 2010 gab es um 2:42 Uhr ein Erdbeben in ca. 5 Kilometern Tiefe mit einer Stärke von 3,5.

Der Jahresniederschlag beträgt 721 mm, was in etwa dem deutschlandweiten Durchschnitt entspricht. Der trockenste Monat ist der April, die meisten Niederschläge fallen im Juni. Die Niederschläge variieren nur minimal und sind extrem gleichmäßig übers Jahr verteilt. An nur 1 % der Messstationen werden niedrigere jahreszeitliche Schwankungen registriert.

Eppstein grenzt im Norden an die Stadt Niedernhausen (Rheingau-Taunus-Kreis) und die Gemeinde Glashütten (Hochtaunuskreis), im Osten an die Stadt Kelkheim (Taunus), im Süden an die Stadt Hofheim am Taunus sowie im Westen an die kreisfreie Stadt Wiesbaden und die Gemeinde Niedernhausen (Rheingau-Taunus-Kreis).

Eppstein besteht aus fünf Stadtteilen: Eppstein, Bremthal, Ehlhalten, Niederjosbach sowie Vockenhausen. Für jeden der Stadtteile besteht ein Ortsbezirk mit Ortsbeirat und Ortsvorsteher. Die Grenzen der Ortsbezirke folgen den seitherigen Gemarkungsgrenzen.

Eine erste urkundliche Erwähnung der Burg Eppstein stammt aus dem Jahre 1122. Deren Besitzer, die Herren von Eppstein, waren es auch, die den Ort begründeten (Ersterwähnung 1299). Nur knapp 20 Jahre später wurden Alt-Eppstein bereits die Stadtrechte zugesprochen. Die eine Hälfte von Eppstein wurde schon 1492 an den hessischen Landgrafen verkauft, die andere Hälfte mit den Orten Bremthal, Ehlhalten, Niederjosbach und Vockenhausen fiel 1581 Kurmainz zu, nachdem die Linie der Eppsteiner 1535 ausgestorben war. Diese Spaltung in zwei unterschiedliche Herrschaftsbereiche blieb einige Jahrhunderte lang bestehen bis zum Jahr 1803, als unter französischer Herrschaft alle geistlichen Staaten in Deutschland aufgelöst wurden. Von nun an konnten sich die heutigen Eppsteiner Stadtteile wieder in gleicher Weise entwickeln. Nach dem Ende der französischen Herrschaft gelangten sie zunächst an das Herzogtum Nassau und dann 1866 an das Königreich Preußen. Nach dem Ende des Zweiten Weltkrieges 1945 wurden sie Bestandteil des Landes Hessen innerhalb der Bundesrepublik Deutschland.

Im Januar 1951 wurde der Gemeinde Eppstein durch das Hessische Staatsministerium das Recht zur Führung der Bezeichnung „Stadt“ verliehen.

Im Zuge der Gebietsreform in Hessen schloss sich die Stadt Eppstein am 1. Januar 1977 mit den bis dahin selbstständigen Gemeinden Bremthal, Ehlhalten und Vockenhausen zur neuen Stadt Eppstein zusammen. Niederjosbach wurde schon 1971 zu Bremthal eingemeindet und wurde dadurch am 1. Januar 1977 auch ein Ortsteil von Eppstein.

Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:

Bürgermeister war seit Juni 2009 Peter Reus (parteilos), er hatte die Nachfolge von Ralf Wolter angetreten, der das Amt seit dem Jahr 2000 bekleidet hatte.
Seit 1. April 2013 befindet sich Peter Reus im Ruhestand. Neuwahlen fanden am 22. September 2013 statt,
gewählt wurde Alexander Simon. Die Stadtverordnetenversammlung berief ihn am 14. November 2013 zum Bürgermeister.

Die Flagge wurde am 13. April 1955 durch das Hessische Innenministerium genehmigt.

Flaggenbeschreibung: „Im weißen, von 2 roten Streifen eingefaßten Feld steht das Eppsteiner Stadtwappen, das im gespaltenen Schild vorne in Blau den linksgekehrten rot-weiß-gestreiften steigenden Löwen und hinten im silbernen Feld 3 rote Sparren zeigt.“

Es gibt verschiedene Städtepartnerschaften: Langeais in Frankreich (seit 1986), Kenilworth in England (seit 1994), sowie Aizkraukle in Lettland (seit 1998). Mit Schwarza in Thüringen ist Eppstein seit 1989 befreundet. Die Städtepartnerschaften werden von dem Verein "Europart Eppstein e. V." betreut.

Eppstein hat S-Bahn-Anbindung nach Frankfurt (S2 Richtung Frankfurt bzw. Dietzenbach und Niedernhausen, Main-Lahn-Bahn) und liegt ca. 3 km von der A 3 entfernt. Durch Eppstein hindurch führt die B 455. Der Flughafen Frankfurt am Main ist ca. 20 km vom Stadtzentrum Eppsteins entfernt.

In Eppstein existiert eine therapeutische Einrichtung für Drogen- und mehrfachabhängige Frauen und Männer. Über ein eigenes Krankenhaus verfügt Eppstein nicht mehr; bis zum 20. März 1970 gab es ein Kreiskrankenhaus mit zuletzt etwa 250 Betten. Die medizinische Versorgung wird durch die Kliniken des Main-Taunus-Kreises in Bad Soden und Hofheim gesichert. Darüber hinaus gibt es niedergelassene Ärzte fast aller Fachrichtungen.

Einer der größten Arbeitgeber der Stadt ist die Firma A. M. Ramp & Co. GmbH, RUCO Druckfarben. Gegründet bereits 1857, ist das Unternehmen heute ein weltweit operierender Hersteller von Spezialdruckfarben für die Kunststoff-, Druck- und Papierindustrie.

Eppstein ist eine der wenigen Städte, in denen Stanniol hergestellt wird. Die Stanniolfabrik Eppstein GmbH & Co KG firmiert seit 2008 nach einem Management-Buy-out als EppsteinFOILS GmbH & Co. KG. Sie befindet sich im Ortsteil Eppstein unterhalb der Burg.

Bis 2013 wurde Lametta aus Stanniol hergestellt. Neben Stanniol produziert das Unternehmen auch andere NE-Metallfolien für hochspezialisierte Anwendungen in aller Welt. Unter anderem lieferte das Unternehmen Bleifolie, die in der US-Sendung Mythbusters verwendet wurde, um einen flugfähigen Ballon komplett aus Blei herzustellen.

Das Unternehmen wurde 1852 als Bleifolienzieherei von Conrad Sachs gegründet und hatte seinen Sitz ursprünglich in der Hintergasse. 1870 wurde der heutige Standort bezogen. Hier befand sich seit 1482 die Bannmühle der Eppsteiner Grafen. Das Kontorgebäude wurde 1904 vom Architekten Carl Wilhelm Plöcker erbaut. 


In Eppstein gibt es zwei Grundschulen und eine Gesamtschule. Außerdem ist hier die Sparkassenakademie Hessen-Thüringen ansässig. Des Weiteren bieten die Musikschule Eppstein-Rossert und der Musikverein Eppstein e. V. musikalische Schulung an diversen Instrumenten.

In Eppstein-Vockenhausen, auf dem Schul- und Sportzentrum am Bienroth liegt die Burg-Schule. 275 Schüler, verteilt auf 14 Klassen der Jahrgangsstufen 1–4, besuchen die Burg-Schule. Das Kollegium besteht aus 17 Lehrerinnen und Lehrern (Stand Schuljahr 2017/2018). Eine weitere Grundschule, die Comenius-Schule, liegt am Ortsausgang des Stadtteils Bremthal in Richtung Niederjosbach.

Auf dem Sport- und Schulzentrum am Bienroth befindet sich die Freiherr-vom-Stein-Schule. 490 Schüler, verteilt auf Gymnasium, Realschule und Hauptschule, besuchen die Gesamtschule in sechs Jahrgangsstufen – von der fünften bis zur zehnten Klasse. Besonderheit der Schule sind die jährlich stattfindenden Schüleraustausche mit der Partnerstadt Kenilworth (England) und einer Schule in Tours (Frankreich). Alle zwei Jahre veranstaltet die Schule außerdem einen Austausch mit dem Bornova Anadolu Lisesi in Izmir (Türkei).

Auf die Schule wurde am 3. Juni 1983 ein Anschlag verübt. Ein Frankfurter Wachmann drang in einem Amoklauf bewaffnet in die Schule ein, verletzte vierzehn Personen schwer und tötete fünf weitere. Im Anschluss daran beging der Täter Suizid. Ein Gedenkkreuz auf dem Gelände der Schule erinnert an die Tat.

Der Alte Friedhof, ursprünglich außerhalb der Stadt gelegen, ist heute eine kleine Grünanlage, die einige historische Grabsteine aufweist. Weiterhin ist das Kriegerdenkmal für die Gefallenen im Ersten Weltkrieg in die Anlage integriert. Der Friedhof wurde 1591 bis 1891 als Friedhof genutzt. Die heutige Gestaltung stammt aus dem Jahr 1985. Das älteste Kreuz datiert aus dem Jahr 1591/92. Das vordere Kreuz soll ein Sühnekreuz für eine überstandene Pest 1635 darstellen. Jedoch wird heute davon ausgegangen, dass es älter ist als der Friedhof und anlässlich eines Totschlags gestiftet wurde. Im Vordergrund der Anlage befindet sich ein eiserner Brunnen.

Das jährlich am ersten Mai stattfindende Radrennen Rund um den Finanzplatz Eschborn-Frankfurt (früher: Rund um den Henninger-Turm) führt durch Eppstein.

Seit 1986 richtet die TSG Eppstein immer im Juni oder Juli den „Eppsteiner Burglauf“ aus. Dieser führt über eine Altdeutsche Meile (7,7 km) vom Sportplatz „Am Bienroth“ um die Burg herum durch die Altstadt und wieder hinauf zum Sportplatz. Die Läufer müssen dabei mehrere hundert Höhenmeter überwinden.

Von 2002 bis 2014 fand im Sommer ein Mountain-Bike-Marathonrennen (Taunus-Trails bzw. EppsteinTrails) für jedermann statt.

Alle drei Jahre organisiert ein Arbeitskreis des Kulturkreises Eppstein ein Holzbildhauersymposium. Das vierte Symposium findet im Mai 2009 statt. Acht bis neun ausgewählte Künstler arbeiten dann eine Woche lang an Kunstwerken, aufmerksam und interessiert beobachtet von Besuchern. Das Ganze findet an der Grünanlage zwischen Eppstein und Niederjosbach statt.

Die Foto-Gruppe Eppstein, ein Treff innerhalb des Kulturkreises Eppstein, richtet alle zwei Jahre die Eppsteiner Fototage aus.

Auf der Burg gibt es regelmäßige Veranstaltungen, zum Beispiel im Sommer die Burgfestspiele. Im Innenhof der Burg präsentieren dabei Schauspielgruppen Freilufttheater mit Werken bekannter Klassiker, aber auch Neuinszenierungen.

Stets am dritten Advent (Samstag und Sonntag) findet in der Altstadt, zu Füßen der Burg, der traditionelle Eppsteiner Weihnachtsmarkt statt.




Der Fernsehfilm Klassentreffen (2004) wurde unter anderem in Eppstein gedreht.



</doc>
<doc id="1386" url="https://de.wikipedia.org/wiki?curid=1386" title="Verwitterung">
Verwitterung

Verwitterung bezeichnet in den Geowissenschaften die natürliche Zersetzung von Gestein infolge dessen exponierter Lage an oder nahe der Erdoberfläche. Dabei spielen mehrere Prozesse zusammen, die eine physikalische Zerstörung und/oder die chemische Veränderung des Gesteins – abiotisch oder biotisch verursacht – herbeiführen. Je nach Art der Verwitterung bleiben die gesteinsbildenden Minerale erhalten (physikalische Verwitterung) oder werden aufgelöst oder umgewandelt (chemische Verwitterung).

In ebenem Terrain bilden die Produkte der Gesteinsverwitterung lockere Oberflächenschichten, die als "Regolith" bezeichnet werden. Der Regolith geht zur Tiefe in das unveränderte Gestein über, das allgemein als "anstehendes Gestein" (kurz "das Anstehende") bezeichnet wird. Die Bodenkunde spricht hierbei vom C-Horizont. In Hanglagen werden die Verwitterungsprodukte hingegen zügig abgeführt (Erosion) und an anderer Stelle als Sediment wieder abgelagert.

Die Verwitterungsprozesse an Bau- und Kunstwerken aus Naturstein werden populär auch als Steinfraß bezeichnet.

Allgemeinsprachlich wird unter „Verwittern“ die natürliche Zersetzung von Materialien, die dem direkten Einfluss der Witterung ausgesetzt sind, verstanden. Dies betrifft neben Gestein auch organische Materialien wie Holz sowie metallische Werkstoffe, Glas, Keramik und Kunststoffe. Bei organischen Materialien fällt diese Form der „Verwitterung“ unter den Oberbegriff Verrottung, bei Metallen, Glas, Keramik und Kunststoffen unter den Oberbegriff Korrosion.

Verrottung und Gesteinsverwitterung sind die wichtigsten Prozesse der Bodenbildung.

Die Gestalt der Erdoberfläche wird sowohl von Prozessen innerhalb und unterhalb der Erdkruste geformt (endogene Faktoren) als auch von Prozessen, die an oder nahe der Oberfläche wirken und zu einem Großteil von den jeweils herrschenden klimatischen Bedingungen abhängen (exogene Faktoren). Die wichtigsten endogenen Faktoren sind Vulkanismus und Tektonik. Die Verwitterung gehört zusammen mit Erosion sowie Sedimenttransport und -ablagerung zu den exogenen Faktoren (siehe auch → Kreislauf der Gesteine).

Die Verwitterungseinflüsse werden üblicherweise gegliedert in:

Eine scharfe Trennung zwischen diesen drei Verwitterungsformen ist nicht immer möglich. So ist die biogene Verwitterung durch Pflanzen teils physikalischer (Turgordruck), teils chemischer Natur (Ätzwirkung). Außerdem setzt die Wirksamkeit einer Verwitterungsform häufig andere vorher angreifende Verwitterungsformen voraus: Chemische Verwitterung ist effektiver in einem durch physikalische Prozesse (die allerdings auch endogen sein können) bereits stark zerrütteten Gesteinskörper. An von Gletschereis glatt polierten Gesteinsoberflächen zeigen sich hingegen auch nach Jahrtausenden oft keine nennenswerten Anzeichen chemischer Verwitterung.

Physikalische Verwitterung (auch "physische" oder "mechanische Verwitterung") ist ein breiter Begriff, der mehrere recht verschiedene physikalische Prozesse einschließt. Ihre Gemeinsamkeit besteht darin, dass sie alle das harte, massive anstehende Gestein in Fragmente zerlegen, deren Größe von großen Blöcken bis zu feinem Sand und Schluff reichen kann. Da dies auch durch die reibende und zermalmende Wirkung der Arbeit von Flüssen, Wellen und Strömungen, Wind und Gletschereis passiert, werden auch diese Prozesse bisweilen der physikalischen Verwitterung zugeordnet. Weil es sich dabei aber um "externe" mechanische Einwirkungen handelt, sollte dabei eher von Erosion statt von Verwitterung gesprochen werden.

Die Frostverwitterung (auch "Frostsprengung") wird durch die Volumenausdehnung gefrierenden, im Poren- und Kluftraum befindlichen Wassers hervorgerufen und gehört zu den wichtigsten Prozessen der physikalischen Verwitterung. Entsprechend ist ihr Auftreten auf Gebiete mit kalten Wintern beschränkt, d. h., auf höhere geographische Breiten sowie größere Höhen in Gebirgsregionen.

Bei der Frostsprengung kann ein Druck von über 200 MPa auftreten. Bei −5 °C beträgt der Druck 50 MPa. Bei −22 °C ist mit 211,5 MPa das Druckmaximum erreicht. Dabei kommt es zu einer Volumenzunahme von bis zu 9 %. Bei noch höherem Druck geht das Eis in eine andere, weniger Raum beanspruchende Form über.

Nahezu überall ist das anstehende Gestein von Spalten durchzogen, den sogenannten Klüften. Erstarrungsgesteine sind nur selten frei von Klüften, durch die das Wasser ins Innere des Gesteins gelangen kann (Spaltenfrost). In Sedimentgesteinen bilden die Schichtflächen eine natürliche Serie von Ebenen relativ geringer Widerständigkeit im Gestein; die Schichtflächen und die Klüfte kreuzen sich im rechten Winkel zueinander. Vergleichsweise geringe Kräfte genügen, um von Klüften und Schichtflächen begrenzte Blöcke aus dem anstehenden Gesteinsverband zu trennen, während viel mehr Kraft vonnöten ist, um im festen anstehenden Gestein neue, frische Spalten zu erzeugen. Der Prozess der Abtrennung von Blöcken aus dem Anstehenden heißt "Blockzerfall".

Wenn grobkörniges Erstarrungsgestein durch chemische Zersetzung geschwächt wird, kann Wasser längs der Grenzflächen zwischen den Mineralkörnern in das Gestein eindringen; hier kann das Wasser gefrieren und durch den starken Druck der dabei auftretenden Volumenvergrößerung die Mineralkörner voneinander trennen. Dieser Prozess wird "körniger Zerfall" genannt. Das dabei entstehende Produkt ist ein Feinkies oder grober Sand, in dem jedes Korn aus einem einzelnen Mineralpartikel besteht, das von seinen Nachbarn längs der ursprünglichen Kristall- oder Korngrenze getrennt worden ist.

Die Wirkung der Frostverwitterung ist in allen Klimaten zu beobachten, die eine winterliche Jahreszeit mit vielen Frostwechseln besitzen. Wo das anstehende Gestein an Felsen und Berggipfeln entblößt ist, werden Blöcke durch Wasser, das in den Klüften gefriert, von Gestein abgetrennt. Unter besonders günstigen Bedingungen, wie sie an hohen Berggipfeln und in der arktischen Tundra vorkommen, sammeln sich große, kantige Gesteinsbrocken in einer Schuttschicht an, die das darunterliegende anstehende Gestein völlig zudeckt. Der Name Felsenmeer bezeichnet solche ausgedehnten Decken aus groben Gesteinsblöcken.

Von Felswänden im Hochgebirge trennt die Frostverwitterung Gesteinsfragmente ab, die zum Fuß der Wand hinunterfallen. Wo die Produktion dieses Schutts mit einer hohen Rate geschieht, sammeln sich die Fragmente am Fuß der Felswände zu Schutthalden an. Frostverwitterung ist ein vorherrschender Prozess in der arktischen Tundra und ein Faktor in der Entwicklung einer großen Vielzahl verschiedener dort vorkommender Bodenstrukturen und Landformen.

Der Wirkung der Frostverwitterung durch wachsende Eiskristalle sehr ähnlich ist der Effekt des Wachstums von Salzkristallen in Spalten und Poren des Gesteins. Dieser "Salzsprengung" genannte Prozess ist in trockenen Klimaten weit verbreitet. Während langer Trockenperioden wird Wasser aus dem Inneren des Gesteins durch Kapillarkräfte an die Oberfläche gezogen. Dieses Wasser enthält gelöste Mineralsalze. Bei seiner Verdunstung bleiben winzige Salzkristalle zurück.

Der Wachstums- oder auch "Kristallisationsdruck" dieser Kristalle ist imstande, den körnigen Zerfall der äußeren Gesteinsschale zu verursachen. Das Auskristallisieren aus übersättigten Lösungen erzeugt eine Druckwirkung von 13 MPa, und beim Wachstum von Salzkristallen 4 MPa. Denselben Prozess kann man auch an Bausteinen und Beton in den Städten beobachten. Streusalz, das im Winter auf Straßen ausgestreut wird, führt zu beachtlichem Zerfall des bodennahen Bereichs von Stein- und Betonbauten.

Sandsteinfelswände sind für Gesteinszerfall durch Salzsprengung besonders anfällig. Tritt am Fuß einer Sandsteinwand Sickerwasser aus, da es nicht in eine dichtere, undurchlässige Gesteinsschicht (Tonschiefer zum Beispiel) eindringen kann, hinterlässt die dort auftretende andauernde Verdunstung dieses Wassers die mitgeführten Salze in den oberflächennahen Poren des Sandsteins. Der Druck der wachsenden Salzkristalle reißt vom Sandstein kleine Schuppen und Splitter ab. Abgetrennte Sandkörner werden von Windstößen weggetragen oder von Regenwasser weggespült, das über die Felswand abläuft.

Mit dem Zurückweichen des Wandfußes entsteht dort allmählich eine Nische oder flache Höhle. In den südwestlichen USA (zum Beispiel im Mesa-Verde-Nationalpark) waren viele solcher Nischen von Indianern bewohnt; mit Steinmauern schlossen sie die natürlichen Hohlformen ein. Diese Felsnischensiedlungen (englisch: "cliff dwellings") waren nicht nur vor schlechtem Wetter geschützt, sondern auch vor feindlichen Angriffen.

Die Salzverwitterung ist allgemein typisch für Regionen mit aridem Klima, da die hohen Verdunstungsraten und die geringen Niederschlagsmengen die Ausfällung von Salzen im Porenraum des Gesteins begünstigen. In humidem Klima tritt diese Form der Verwitterung vor allem an Meeresküsten auf, besonderes bei Mauern oder Felsen, die direkt in das Meer ragen.

Unter Hydrationsverwitterung versteht man die Sprengung des ursprünglichen Gesteinsgefüges infolge der Volumenzunahme von Mineralkörnern durch die Einlagerung von Wassermolekülen in das Kristallgitter der entsprechenden Minerale (Hydratation oder Hydration). Die Hydrationsverwitterung darf nicht verwechselt werden mit der Hydrolyse, bei der die Minerale durch chemische Reaktionen mit Wasser-Ionen umgewandelt werden (chemische Verwitterung).

Rostverwitterung (auch "Rostsprengung") kommt nur bei Gesteinen vor, die (nicht-oxische) Eisenerzminerale enthalten. Entsprechende Mineralkörner erfahren bei Kontakt mit meteorischem Wasser eine Volumenzunahme durch Oxidation und damit der Bildung von Eisenoxiden, -hydroxiden, -oxidhydroxiden und -oxidhydraten. Die Volumenzunahme sprengt das ursprüngliche Gesteinsgefüge, wobei die Sprengwirkung sehr ausgedehnte Bereiche eines Gesteinskörpers betreffen kann. In gebirgigen Gegenden kann es infolge von Rostsprengung zu schweren Steinschlägen und auch Lawinen kommen. Rostsprengung zerstört auch häufig steinerne Kulturgüter, da in früheren Zeiten häufig Eisendübel und Eisenanker bei der Installation in Bauwerken eingesetzt worden sind.

Durch quellfähige Tonminerale kommt es beim Wechsel zwischen Durchfeuchtung und Trocknung zu Volumenschwankungen, die den Gesteinsverband zerstören können.

Ein eigentümlicher, weitverbreiteter Prozess, der mit der physikalischen Verwitterung verwandt ist, entsteht durch Druckentlastung: die Reaktion des Gesteins auf die Verminderung vorher vorhandener, den Gesteinskörper einengender Druckkräfte, wenn überlagernde Gesteinsmassen abgetragen werden.

Gesteine, die in großer Tiefe unter der Erdoberfläche gebildet wurden (besonders Erstarrungs- und metamorphe Gesteine), befinden sich in einem komprimierten Zustand wegen der Last des sie überlagernden Gesteins. Wenn diese Gesteine an die Oberfläche gelangen, dehnen sie sich etwas aus; dabei brechen dicke Gesteinsschalen von der darunter befindlichen Gesteinsmasse los. Dieser Vorgang wird auch "Exfoliation" genannt. Die Trennflächen zwischen den Schalen bilden ein System von Spalten, die als "Druckentlastungsklüfte" bezeichnet werden.

Diese Kluftstruktur ist am besten in massiven, vorher kluftarmen Gesteinen ausgebildet, wie zum Beispiel in Granit; denn in einem bereits engständig geklüfteten Gestein würden die Expansion lediglich zu einer Erweiterung dieser vorhandenen Klüfte führen.

Die Gesteinsschalen, die von der Druckentlastung erzeugt werden, liegen im Allgemeinen parallel zur Landoberfläche und sind deshalb zu den Talsohlen hin geneigt. An Granitküsten sind die Schalen an allen Punkten seewärts geneigt. Die Druckentlastungsklüftung ist sehr gut in Steinbrüchen zu sehen, wo sie den Abbau großer Gesteinsblöcke stark erleichtert.
Wo sich die Druckentlastungsklüfte über dem Gipfelbereich eines einzelnen großen, massiven Gesteinskörpers entwickelt haben, entsteht eine "Exfoliationskuppe" (englisch: "exfoliation dome"). Diese Kuppen gehören zu den größten Landformen, die hauptsächlich durch Verwitterung erzeugt worden sind. In der Region des Yosemite Valley in Kalifornien, wo solche Kuppen eindrucksvoll das Landschaftsbild prägen, besitzen einzelne Gesteinsschalen Dicken von sechs bis 15 Metern.

Andere Arten von großen, glatten Felskuppeln ohne solchen Schalenbau sind keine echten Exfoliationskuppen, sondern entstanden durch den körnigen Zerfall der Oberfläche einer einheitlichen Masse eines harten, grobkörnigen intrusiven Erstarrungsgesteins, dem Klüfte fehlen. Beispiele sind der Zuckerhut von Rio de Janeiro und Stone Mountain in Georgia (USA). Diese glatten Bergkuppen ragen in auffälliger Weise über ihrer Umgebung aus weniger widerständigem Gestein auf.

Die thermische Verwitterung (Insolationsverwitterung) zählt zu den physikalischen Verwitterungsarten, wird aber meist als spezielle Kategorie geführt. Sie wird in festen Materialien durch räumliche und zeitliche Temperaturunterschiede und dadurch verursachte Volumenänderungen hervorgerufen. Diese können

Unter der chemischen Verwitterung wird die Gesamtheit all jener Prozesse verstanden, die zur chemischen Veränderung oder vollständigen Lösung von Gesteinen unter dem Einfluss von Niederschlägen und oberflächennahem Grundwasser bzw. Bodenwasser führen. Dabei ändern sich mit dem Mineralbestand meist auch die physikalischen Eigenschaften des Gesteins. Durch das Wasser werden Elemente oder Verbindungen aus den Mineralen gelöst (bis hin zur vollständigen Auflösung) oder im Wasser bereits gelöste Elemente oder Verbindungen in die Minerale neu eingebaut. Weil chemische Verwitterung an Wasser gebunden ist, spielt sie nur in Regionen mit humidem Klima eine bedeutende Rolle. In Regionen mit großem Wasserüberschuss werden die aus dem Gestein gelösten Stoffe oft in Fließgewässern abgeführt und gelangen so letztlich ins Meer.

Die Lösungsverwitterung ist die Lösung von Gesteinen aus Mineralen, die in reinem Wasser löslich sind, z. B. Gips (CaSO•2HO), Halit (NaCl) oder Sylvin (KCl). Diese Gesteine sind daher in humidem Klima nur selten auf natürliche Weise aufgeschlossen, da sie meist bereits unterhalb der Geländeoberfläche aufgelöst werden. Spezielle Verwitterungserscheinungen der Lösungsverwitterung sind der Salzspiegel und der Gipshut im Dachbereich von Salzstöcken.

Da Lösung traditionell zur Chemie gezählt wird, ordnet man die Lösungsverwitterung der chemischen Verwitterung zu. Da sie aber prinzipiell reversibel ist und die chemische Zusammensetzung des Gesteins nicht verändert wird, sondern lediglich die Kristallstruktur zerstört wird, kann sie auch als physikalische Verwitterungsart aufgefasst werden.

Calciumcarbonat (CaCO, Calcit, Aragonit) ist nur sehr schlecht in reinem Wasser löslich. Verbindet sich das Wasser jedoch mit Kohlenstoffdioxid (CO) aus der Luft,
bildet sich Kohlensäure. Sie wandelt das Carbonat nach der Reaktionsgleichung
in Calciumhydrogencarbonat um, das in Wasser stets vollständig gelöst vorliegt. Dieser Vorgang wird Carbonatisierung genannt, weil ein Salz der Kohlensäure "noch einmal" mit Kohlensäure reagiert. Aus dem gleichen Grund wird Hydrogencarbonat auch als "Bi-" oder "Doppelcarbonat" bezeichnet. CO kann in stärkerer Konzentration auch von Bodenlebewesen oder aus der Zersetzung organischer Substanzen stammen (siehe auch chemisch-biotische Verwitterung).

Die Reaktion der Kohlensäure mit Karbonatgesteinen (Kalkstein, Dolomit, Karbonatit, Marmor) erzeugt in kleinem Maßstab viele interessante Oberflächenformen. Die Oberfläche entblößten Kalksteins ist typischerweise mit einem komplexen Muster von Pfannen, Rillen, Furchen und anderen Vertiefungen überzogen. An einigen Stellen erreichen sie das Ausmaß tiefer Furchen und hoher, wandartiger Gesteinsrippen, die von Mensch und Tier nicht mehr in normaler Weise überquert werden können. So entstehen in Gebieten, deren Oberflächengeologie von Kalkstein dominiert wird, bizarre Karstlandschaften. Die Auflösung von Carbonatgestein ist jedoch nicht auf die Geländeoberfläche beschränkt, sondern erfolgt auch unter der Erde durch versickertes (kohlensaures) Oberflächenwasser. Dies führt zur Bildung ausgedehnter Höhlen und Höhlensysteme und nachfolgend von Dolinen und Poljen. Die chemische Stabilität des Calciumhydrogencarbonats ist jedoch abhängig von Druck und Temperatur. Erwärmt sich die Lösung oder erfährt eine Druckentlastung, so verschiebt sich das chemische Reaktionsgleichgewicht zuungunsten von Kohlensäure und Calciumhydrogencarbonat. Im Zuge dessen zerfällt das Hydrogencarbonat unter Abgabe von CO, und Calciumcarbonat fällt aus. Auf diese Weise entstehen u. a. Quellkalke und Tropfsteine in Kalksteinhöhlen.

Die Wirkung der Kohlensäure ist ein dominierender Faktor für die Denudation in Kalksteingebieten mit feuchtem Klima, nicht zuletzt wegen der dort intensiven biotischen CO-bildenden Prozesse. In feuchtem Klima sind Kalksteine daher relativ verwitterungsanfällig und können große Talzonen und andere Bereiche niedrigen Geländes bilden, während benachbarte Rücken und Plateaus aus verwitterungresistenterem Gestein bestehen. Die Untersuchung eines in Kalkstein eingeschnittenen Tals in Pennsylvania ergab, dass die Landoberfläche allein durch die Wirkung der Kohlensäure im Durchschnitt um 30 cm in 10.000 Jahren tiefergelegt worden ist.

Das Umgekehrte trifft auf Trockenklimate zu. Dort ist der Einfluss der Kohlensäureverwitterung wegen der Abwesenheit flüssigen Wassers und der damit zusammenhängenden geringeren biotischen Aktivität sehr viel geringer, und Kalkstein und Dolomit bilden hohe Rücken und Plateaus. Zum Beispiel sind die Ränder des Grand Canyon und die angrenzenden Plateaus von Dolomitschichten unterlagert. Sandsteinschichten aus Quarzkörnern, die durch Calciumcarbonat miteinander verkittet wurden (sogenannte "karbonatzementierte Sandsteine") verwittern in einem Trockenklima ebenfalls relativ langsam.

Eine weitere für Kohlensäureverwitterung anfällige Calciumverbindung ist das in der Natur eher seltene Calciumhydroxid (Ca(OH), Portlandit). Es verwittert nach der Reaktionsgleichung
zu Calciumcarbonat, das nachfolgend weiter verwittert. Calciumhydroxid ist als "Löschkalk" allerdings ein bedeutender Bestandteil von Beton. Bei Stahlbeton begünstigt die ebenfalls als "Carbonatisierung" bezeichnete Reaktion von Kohlensäure mit Calciumhydroxid, bei der jedoch Calciumcarbonat "erzeugt" statt zersetzt wird, die Korrosion der Bewehrung, woraus schwerwiegenden Bauschäden resultieren können.

Neben Calciumcarbonat und Calyciumhydroxid kann beispielsweise auch Olivin, das Bestandteil vieler vulkanischer Gesteine ist, nach der Reaktionsgleichung
fast vollständig aufgelöst werden, wobei vorstehende Gleichung einen mehrphasigen Prozess mit mehreren Einzelreaktionen zusammenfasst.

In den feuchten Klimaten der niederen Breiten wird so mafisches Gestein, insbesondere Basalt, intensiv von größtenteils biogenen Bodensäuren angegriffen. Im Zusammenspiel mit chemischer Verwitterung durch Hydrolyse (siehe unten) entstehen Landformen, die als sogenannter "Silikatkarst" dem Karbonatkarst sehr ähnlich sind. Die Effekte der chemischen Verwitterung von Basalt zeigen sich beispielsweise in den eindrucksvollen Furchen, Felsrippen und -türmen an den Hängen tiefer Bergnischen in Teilen der Hawaii-Inseln.

Auch diese Verwitterungsform betrifft hauptsächlich Gesteine mit größeren Anteilen an Calciumcarbonat (Kalkstein, Kalksandstein, Marmor). Saurer Regen enthält infolge der Aufnahme von Schwefeldioxid (SO) und Schwefeltrioxid (SO) aus höheren Luftschichten geringe Mengen schwefliger Säure (HSO) bzw. Schwefelsäure (HSO). Beide Schwefeloxide entstammen überwiegend menschgemachten und vulkanischen Emissionen. Bestimmte Bakterien können den Anteil der Schwefelsäure im Regenwasser nach dessen Auftreffen auf Böden oder anderen Oberflächen erhöhen, indem sie darin enthaltene schweflige Säure oxidieren.

Bei Kontakt von Karbonatgesteinen mit saurem Regen verdrängt die Schwefelsäure die schwächere Kohlensäure aus deren Calciumsalz. Aus Calciumcarbonat (Calcit) entsteht Calciumsulfat (Gips) und Kohlendioxid (CO):
Die Wasserlöslichkeit von Gips ist wesentlich besser als die von Calcit, und das Gestein wittert deshalb nach der Vergipsung schneller ab.

Da sie CO erzeugt anstatt dass, wie bei der Kohlensäureverwitterung und der anschließenden biogenen Fällung von Calciumcarbonat in den Meeren, atmosphärisches CO gebunden wird, kann die Schwefelsäureverwitterung den Kohlenstoffkreislauf beeinflussen. Damit kommt der Reduktion menschgemachter Schwefeloxidemissionen eine gewisse Relevanz in der Debatte um die globale Erwärmung zu, denn zumindest regional trägt Schwefelsäureverwitterung heute in erheblichem Maße zur natürlichen Karbonatverwitterung bei.

In urbanen Gebieten sorgt die Schwefelsäureverwitterung für eine beschleunigte Alterung und Zerstörung von historischen Gebäudefassaden, Denkmälern und dergleichen. So verlieren Marmorskulpturen als erstes sichtbares Anzeichen den typischen Glanz ihrer polierten Oberfläche. Nachfolgend büßen sie ihre Konturenschärfe ein und können im Extremfall die gesamte bildhauerisch bearbeitete Oberfläche verlieren. Da Gips hygroskopisch ist, können im Regen enthaltene Rußpartikel in die vergipste Oberfläche eingebunden werden – sogenannte Schwarzkrusten entstehen. Diese sind dichter als der Marmor und vermindern die Wasserdampf­diffusions­fähigkeit des Gesteins. Es entstehen dann parallel zur Oberfläche verlaufende Schadzonen und irgendwann platzt die Schwarzkruste großflächig ab – auch dabei geht die bildhauerisch bearbeitete Oberfläche verloren. Wegen des schwefelsauren Regens sind mittlerweile die meisten Marmorskulpturen in Museen verlagert und durch Abgüsse aus Material ersetzt worden, das gegen sauren Regen unempfindlich ist.

Bei der Hydrolyse (hydrolytische Verwitterung) werden die Ionen im Kristallgitter bestimmter Minerale an H- und OH-Ionen, die in Wasser durch Autoprotolyse permanent entstehen, gebunden, wodurch das Ionengitter zerfällt. Die Hydrolyse ist ein wichtiger Prozess der Bodenbildung, denn sie bildet die Initialreaktion der Umwandlung häufiger Silikatminerale (z. B. Feldspäte und Glimmer) in Tonminerale (z. B. Illit, Kaolinit, Montmorillonit, Smectit). So zerfällt beispielsweise Kalifeldspat nach der Reaktionsgleichung
in alumosilizische Säure und Kaliumhydroxid. Letztgenanntes wird durch Reaktion mit Kohlensäure in Kaliumcarbonat („Pottasche“, KCO) überführt und, da es gut wasserlöslich ist, mit dem Kluft-, Poren- oder Oberflächenwasser aus dem Gestein abgeführt. Die alumosilizische Säure reagiert mit Wasser nach der Reaktionsgleichung
zu Kaolinit und Orthokieselsäure. Letztgenannte ist wiederum löslich und wird abgeführt. Ändert sich jedoch unterwegs das chemische Milieu, kann aus dieser Verwitterungslösung SiO ausfallen und bildet dann Chalcedon­krusten (Silcretes).

Allgemein gilt: je feuchter das Klima, je höher die Temperatur und je geringer der pH-Wert, umso intensiver ist die Hydrolyse. In den warmen und feuchten Klimaten der tropischen und subtropischen Zone werden magmatische Gesteine und metamorphe Gesteine durch Hydrolyse und Oxidation oft bis zu Tiefen von 100 Metern verwittert. Geologen, die solche Tiefenverwitterung des Gesteins zuerst in den südlichen Appalachen untersuchten, nannten diese Verwitterungsschicht Saprolith (wörtlich „verfaultes Gestein“). Für den Bauingenieur bedeutet tiefgründig verwittertes Gestein ein Risiko beim Bau von Autobahnen, Dämmen oder anderen schwerlastigen Bauwerken. Zwar ist Saprolith weich und kann ohne viel Sprengarbeit von Baggern bewegt werden, jedoch besteht die Gefahr, dass das Material unter schwerer Belastung nachgibt, da es wegen seines hohen Gehalts quellfähiger Tonminerale unerwünschte plastische Eigenschaften besitzt.

Unter biotischer Verwitterung (auch biologische oder biogene Verwitterung genannt) versteht man Verwitterung durch den Einfluss lebender Organismen sowie ihrer Ausscheidungs- bzw. Zersetzungsprodukte. Diese Wirkungen können physikalischer Natur sein (Beispiel: Wurzelsprengung) oder in einer chemischen Einwirkung bestehen. Biotische und abiotische Verwitterung ist dabei in manchen Fällen schwer abzugrenzen. Die biotischen Verwitterungsvorgänge werden in der Literatur mitunter auch in den Kategorien der physikalischen bzw. chemischen Verwitterung eingeordnet.

Mechanisch-biotische Verwitterung ist hauptsächlich die Wurzelsprengung. In Klüfte des Gesteins und in winzige Spalten zwischen Mineralkörnern hineinwachsende Pflanzenwurzeln üben durch ihr Dickenwachstum eine Kraft aus, deren Tendenz es ist, diese Öffnungen zu erweitern. Man sieht gelegentlich Bäume, deren unterer Stamm und deren Wurzeln fest in einer Kluft des massiven Gesteins eingekeilt sind. Es bleibt im Einzelfall offen, ob der Baum es tatsächlich geschafft hat, die Gesteinsblöcke zu beiden Seiten der Kluft weiter auseinanderzutreiben, oder ob er lediglich den bereits vorhandenen Raum der Spalte ausgefüllt hat. In jedem Fall sicher ist jedoch, dass der Druck, den das Wachstum winziger Wurzeln in Haarrissen des Gesteins ausübt, unzählige kleine Gesteinsschuppen und Körner lockert. Anheben und Zerbrechen von Beton-Gehwegplatten durch das Wachstum von Wurzeln naher Bäume ist ein allgemein bekannter Beweis für den wirksamen Beitrag von Pflanzen zur mechanischen Verwitterung.

Chemisch-biotische Verwitterung wird durch Mikroorganismen, Pflanzen und Tiere verursacht, und gehört zu jenen Phänomenen, die unter dem Begriff "Biokorrosion" zusammengefasst werden. Beispielsweise greifen die von Pflanzenwurzeln abgesonderten organischen Säuren Minerale an und zerlegen das Gestein dadurch in einzelne Bestandteile. Der aus mikrobiell teilweise abgebauten Resten abgestorbener Pflanzen und Tieren bestehende Humus enthält einen großen Anteil an Huminsäuren, die gesteinszerstörend wirken. Durch mikrobielle Säurebildung, Oxidationen und Reduktionen kann es zur Auflösung von Mineralen kommen.

Die Wirkung der Kohlensäure wird in vielen Fällen durch die Wirkung einfacher organischer Säuren verstärkt. Sie entstehen bei der mikrobiellen Zersetzung von abgestorbener organischer Substanz oder werden von den Wurzeln lebender Pflanzen abgegeben. Sie gehen mit Metallen, vor allem Eisen (Fe), Aluminium (Al) und Magnesium (Mg), sehr stabile, zum Teil wasserlösliche, zum Teil wasserunlösliche Verbindungen ein, so genannte metallorganische Komplexe ("Chelatkomplexe", "Chelate"). Diese Chelatbildung ist eine wichtige Verwitterungsreaktion. Das Wort „Chelat“ bedeutet „ähnlich einer Krebsschere“ und bezieht sich auf die sehr enge Bindung, die organische Moleküle mit Metall-Kationen eingehen.

Im Falle der löslichen Komplexe werden diese im Bodenprofil mit der Sickerwasserbewegung verlagert und dem Verwitterungsmechanismus entzogen. Chelatisierende Stoffe, die vor allem bei mikrobiellen Abbauprozessen freigesetzt werden, sind unter anderem Citronensäure, Weinsäure und Salicylsäure.

Des Weiteren können Mikroorganismen und die Atmung der Pflanzenwurzeln durch Kohlenstoffdioxid-Bildung den Kohlensäuregehalt im Boden erhöhen und dadurch Lösungsvorgänge beschleunigen. Anaerobe Bakterien bewirken teilweise Reduktionsprozesse, indem sie bestimmte Stoffe als Elektronenakzeptoren für ihren Energiestoffwechsel verwenden und dadurch wasserlöslich machen, beispielsweise durch die Reduktion von Eisen von der dreiwertigen zur zweiwertigen Form. Verbindungen des zweiwertigen Eisens sind in Wasser wesentlich leichter löslich als die des dreiwertigen, weshalb Eisen relativ leicht durch mikrobielle Reduktion mobilisiert und verlagert werden kann.

Als Wollsackverwitterung wird die durch verschiedene Verwitterungsprozesse erfolgende Ausbildung typischer Formen im anstehenden Gestein bezeichnet. Dabei bildet sich zunächst ein annähernd rechtwinkliges Kluftnetz im Gestein, was auf physikalische Verwitterung zurückgehen kann, sich aber bei magmatischen Gesteinen auch durch Volumenabnahme beim Erkalten ausbilden kann. Wasser dringt in den Klüften ins Gestein vor und setzt chemische Verwitterungsprozesse (z. B. die Hydrolyse von Feldspäten) in Gang. Von den Klüften her rückt die Zersetzung in das Gestein vor, was an Ecken und Kanten besonders schnell geht, da dort das Verhältnis von Angriffsfläche zu Gesteinsvolumen am größten ist. Bei Exponierung an der Oberfläche wird das von der Verwitterung angegriffene Gestein bevorzugt erodiert, was den bis dahin noch unverwitterten, freiliegenden Kernen der Blöcke eine gerundete, wollsackähnliche Form gibt.

Durch Hydrolyse der Feldspäte und Glimmer oder durch Temperaturverwitterung zerfällt das Gesteinsgefüge granitischer Gesteine (Granit, Granodiorit) in einzelne Mineralkörner. Dieses von der Korngröße her sandig bis feinkiesige Material wird Grus genannt und der entsprechende Vorgang heißt "Vergrusung" oder "Abgrusung". Vergrusung geht oft mit Wollsackverwitterung einher.

Die Mechanismen hinter der Alveolarverwitterung sind nicht genau geklärt. Vermutlich entsteht sie, abhängig von den vor Ort herrschenden Bedingungen, durch verschiedene Verwitterungsarten (Salzverwitterung, Kohlensäureverwitterung) im Zusammenspiel mit Erosion durch Wind und Wasser. Betroffen sind in erster Linie Sandsteine. Die dabei entstehenden wabenartigen Gebilde werden als "Tafoni" bezeichnet.



</doc>
<doc id="1388" url="https://de.wikipedia.org/wiki?curid=1388" title="Safran">
Safran

Der Safran (von arabisch/persisch , „das Gelbe“, „Safran“), wissenschaftlicher Name "Crocus sativus", ist eine Krokus-Art, die im Herbst violett blüht. Aus den Narben ihrer Blüten wird das ebenfalls "Safran" genannte Gewürz gewonnen.

Diese Pflanzenart ist eine triploide Mutante des auf den ägäischen Inseln beheimateten "Crocus cartwrightianus". Sie ist wegen des dreifachen Chromosomensatzes unfruchtbar und kann nur vegetativ durch Knollenteilung vermehrt werden. Die Stammform "Crocus cartwrightianus" besitzt deutlich kürzere, aber ebenfalls aromatische Narben.

Jede Blüte enthält einen sich in drei Narben verzweigenden Griffel. Nur diese süß-aromatisch duftenden Griffel werden getrocknet als Gewürz verwendet. Um ein Kilogramm von ihnen zu gewinnen, benötigt man etwa 150.000 bis 200.000 Blüten aus einer Anbaufläche von ca. 10.000 Quadratmetern; die Ernte ist reine Handarbeit, ein Pflücker schafft 60 bis 80 Gramm am Tag. Hinzu kommt, dass Safran nur einmal pro Jahr im Herbst (und das nur für einige Wochen) blüht. Deshalb zählt Safran zu den teuersten Gewürzen. Im Einzelhandel zahlt man zwischen 4 und 30 Euro pro Gramm.

Die Chromosomenzahl beträgt 2n = 24, selten 16.

Die Safran-Pflanze stammt aus der Familie der Schwertliliengewächse und ist eine mehrjährige Krokusart. Die Safranknolle treibt erst im Herbst und überdauert den Rest des Jahres im Boden.

Wegen der äußerlichen Ähnlichkeit der Safranknolle mit einer Zwiebel wird Safran fälschlicherweise oft in die Kategorie der Zwiebelgewächse eingeteilt, jedoch handelt es sich beim Safran um eine Knollenpflanze. Demnach werden oft auch die Safranknollen selbst genauso unzutreffend als Zwiebeln bezeichnet.

Die Blüte der Safran-Pflanze ist aus 6 fliederfarbenen Perigonblättern aufgebaut, welche in der Blütenröhre münden. Jede Safran-Pflanze produziert jährlich einen hellgelben Griffel, der sich innerhalb der Blütenröhre befindet. Dieser hellgelbe Griffel teilt sich am oberen Ende der Blüte in drei bis sechs 2,5 cm – 4,5 cm lange rote Narbenäste. Diese Narbenäste stellen nach der Ernte das fertige Safrangewürz dar.

Angebaut wird Safran in Afghanistan, Iran, Kaschmir, Südfrankreich, Spanien, Marokko, Griechenland (um Kozani), Türkei (in Safranbolu), Italien (Sardinien, Abruzzen, Toskana) und – seit 2006 bzw. 2007 wieder – in Österreich (Pannonischer Safran (Crocus Austriacus); Wachauer Safran). In Italien ist der Safrananbau seit dem 13., in Deutschland seit dem 15. Jahrhundert belegt. Ein kleines Anbaugebiet von 18.000 Quadratmetern existiert im Schweizer Dorf Mund, wo pro Jahr zwischen 1,5 und 2 Kilogramm Safran geerntet werden – abhängig vom Wetter und den Temperaturen. 
Seit 2012/13 wird auch in Deutschland wieder Safran angebaut, auf dem "Doktorenhof" in Venningen (Pfalz) sowie in Sachsen nahe Dresden (Saxen-Safran) und in Bittenfeld (Baden-Württemberg). „Pro Jahr werden ungefähr 200 Tonnen Safran produziert. Wenn man nach Produktionsmengen beurteilt, so steht Iran mit ca. 170 bis 180 Tonnen jährlich an erster Stelle. Dies macht bis zu 91 % des Marktanteils aus.“

Safran schmeckt bitter-herb-scharf, was bei normaler Dosierung – anders als der typische Duft – nicht zum Tragen kommt. Er enthält Carotinoide, vor allem Crocin, sodass sich mit Safran gewürzte Gerichte intensiv goldgelb färben. Weiter enthält er den Bitterstoff Safranbitter, aus dem sich beim Trocknen teilweise der für das Safranaroma verantwortliche Aldehyd Safranal bildet. Weitere Aromastoffe sind unter anderem Isophorone. Bekannte Gerichte mit Safran sind Bouillabaisse, Risotto alla milanese, Lussekatter und Paella. In der persischen Küche werden besonders Reisgerichte gerne mit Safran verfeinert.

Safran muss vor Licht und Feuchtigkeit geschützt in fest schließenden Metall- oder Glasgefäßen aufbewahrt werden, da das Gewürz am Licht schnell ausbleicht und sich das ätherische Öl relativ leicht verflüchtigt. Safran wurde auch als Farbmittel eingesetzt; der wasserlösliche Farbstoff Crocetin ist in der Pflanze glycosidisch an das Disaccharid Gentiobiose gebunden; diese Verbindung wird als Crocin (siehe oben) bezeichnet. Bereits Plinius der Ältere erwähnt Safran als Farbmittel. Es wurde auch eingesetzt, um Goldschriften zu imitieren, oder um Zinn oder Silber wie Gold erscheinen zu lassen. Es wurde auch in Mischungen mit anderen Pigmenten oder Farbstoffen verwendet.

Um den aromatischen Duft zu bewahren, sollte Safran nicht lange gekocht werden. Es empfiehlt sich, die Narbenschenkel einige Minuten in etwas warmem Wasser einzuweichen und mit der Flüssigkeit gegen Ende der Garzeit dem Gericht zuzugeben. Eine noch intensivere Färbung erhält man, wenn die Safranfäden frisch gemörsert werden.

Von Zeus wird in einer Sage der griechischen Mythologie berichtet, er habe auf einem Bett aus Safran geschlafen. Und bereits die Phönizier verwendeten Safran als Heil- und Gewürzmittel. Kennengelernt hatten sie ihn vermutlich von den Indern. Schon in der Antike war er ein Luxusartikel. Auf das Fälschen oder Verschneiden von Safran standen hohe Strafen. 

Homer berichtete, dass jeder geforderte Preis für Safran bezahlt wurde. Im Mittelalter wiederum war er dreimal so teuer wie Pfeffer. In vielen Kulturen war es Brauch, den Hochzeitsschleier mit Safran gelb zu färben. Reiche Römer streuten Safranfäden auf ihre Hochzeitsbetten. 

Mit Safran wurden Salben, Arzneipflaster ("oxycroceum": Essig-Safran-Pflaster), Balsame und Duftöle zubereitet und Speisen gewürzt, über deren intensives Aroma neben Cicero u. a. Petronius in der "Cena Trimalchionis" berichtet: "omnes enim placentae omniaque poma etiam minima vexatione contacta coeperunt effundere crocum, et usque ad os molestus umor accidere" („Alle Kuchen und alle Äpfel fingen, wenn man sie auch nur ganz sanft berührte, an, Safranwasser zu verspritzen, bis uns die unangenehme Flüssigkeit im Gesicht traf.“). Er diente, wie Plinius der Ältere vermerkt, als Arznei und Weinzusatz, Kaiser Heliogabal soll bevorzugt in mit Safran vermischtem Wasser gebadet haben.

Mittelalterlichen und neuzeitlichen Anekdoten bzw. Berichten zur berauschenden Wirkung des Safrans, die sich seit der Antike in medizinischer und botanischer Literatur finden, verdanken sich die Wendungen "in sacco croci dormivit" („Er hat auf einem Sack Safran geschlafen“), "crocum edisse" („Safran gegessen haben“) und das in der frz. Volkssprache des Mittelalters belegte, sprichwörtliche "Le fol na que faire de saffren", in seiner lat. Fassung "croco stultus non eget" („Der Dumme/Irre/Narr braucht keinen Safran <mehr>“). Dadurch wird ein sehr ausgelassenes, 'übergeschnapptes' Verhalten bezeichnet, das an das unter dem Einfluss hoher Dosen Safran Stehender erinnern soll. So schreibt Peter Lauremberg (1585–1639) in seinem "Apparatus plantarius":

„De hilaritudine in pectus concitanda, a croci esu, res est apud Medicos & Botanicos celebratissima, apud quos experimento comprobatum est, drachmas circiter tres, cum vino haustas, tanta laetitia homines perfundere, ut iis contingat nimio risu exsolvi, ebriis similes reddi, saepe etiam dejici de bona mente, & ridendo aut finire vitam, aut vehementer periclitari. Amatus Lusitanus citat ad Testimonium exemplum Mercatoris, qui plus nimio assumens, tam profuse in risum solutus est, ut fere illi prae cachinno rupta fuerint ilia. Idemque scribit se observasse in alio sacco croci pleno indormierat. Hoc est quod Galenus annotavit Lib. II Med. Loc. Crocum caput opplere, & perturbare arcem rationis ...“

Johann Ferdinand Hertodt von Todenfeld verfasste im 17. Jahrhundert mit der "Crocologia seu curiosa Croci Regis vegetabilium enucleatio" ein umfangreiches Werk, das unzählige pharmazeutische Rezepte zur Behandlung diverser Krankheiten von Durchfall, Wassersucht bis zur Hypochondrie durch die Safranpflanze versammelt.

Zu Beginn des 20. Jahrhunderts war Österreich das Anbauzentrum Mitteleuropas. Der Safran höchster Qualität wurde auch als Crócus austriacus bezeichnet.
Noch heute ist das Fälschen von Safran weit verbreitet: Fälschungen können aus einer Kurkuma-Mischung bestehen. Safranfäden werden auch gefälscht, aber wer mit Aussehen und Geruch vertraut ist, kann den Unterschied erkennen. Ein einigermaßen sicherer chemischer Nachweis ist das Zugeben von Natronlauge zu einer Lösung von etwas „Safranpulver“: Handelt es sich um reinen Safran, so bleibt die Lösung gelb, enthält sie Kurkuma-Anteile, so wird sie trüb und verfärbt sich rot. Dieser Test war schon vor Jahrhunderten bei den Gewürzhändlern üblich. Er beruht auf den verschiedenen chemischen Eigenschaften der in Safran und Kurkuma enthaltenen Farbstoffe.

Falscher Safran (Saflor) ist eine Bezeichnung für die Färberdistel ("Carthamus tinctorius"), die früher zum Färben von Seide verwendet wurde. Dieses Gewürz färbt das Gericht schwächer als echter Safran und bringt kein eigenes Aroma ein. Die Röhrenblüten der Färberdistel lassen sich schon mit bloßem Auge von den fadenförmigen Narbenlappen des Safrans unterscheiden. Beim echten Safran müssen die Narbenschenkel ungefähr zwei bis drei Zentimeter lang, trichterförmig eingerollt und oben eingekerbt sein.

Die Färberdistel wurde in Antike und Mittelalter unter den Bezeichnungen "crocus ortensis" und "crocus ortolanus" auch als schädliche Säfte abführendes Mittel angesehen.

Safran für homöopathische Anwendungen ist im Europäischen Arzneibuch
monographiert und enthält die gängigen Tests auf Identität und Reinheit von Safran.

Zur Eindämmung von Fälschungen und Ersatzprodukten, sowie zur Erhöhung der Konsumentensicherheit, sind Qualitätskriterien definiert. Merkmale wie Färbekraft, Aromakonzentration (Safranal) und Konzentration von Bitterkeit (Picrocrocin) werden zusammengefasst in vier Kategorien unterteilt. Neben dem internationalen ISO-Standard 3632 gibt es auch nationale Normen.

Für den Safran bestehen bzw. bestanden auch die weiteren deutschsprachigen Trivialnamen Chruogo (althochdeutsch), Croc (althochdeutsch), Broze (althochdeutsch), Brugo (althochdeutsch), Gewürzsafran, Kruago (althochdeutsch), Saffaran (mittelhochdeutsch), Saffart (mittelhochdeutsch), Saffaren (mittelhochdeutsch), Safferain (mittelhochdeutsch), Safferen (mittelhochdeutsch), Safferon (mittelhochdeutsch), Safferntblume (Bern), Saffran (mittelhochdeutsch), Saffrat (mittelhochdeutsch), Saffrath (mittelhochdeutsch), orientalischer Safran (lateinisch "Crocus orientalis"), Safrich (schwäbisch), Schaffner (mittelhochdeutsch), Seydfarb (mittelhochdeutsch), Sintvarwe (althochdeutsch), Soffraen (mittelhochdeutsch) und Suffran (mittelhochdeutsch).







</doc>
<doc id="1389" url="https://de.wikipedia.org/wiki?curid=1389" title="Erich Kästner">
Erich Kästner

Emil Erich Kästner (* 23. Februar 1899 in Dresden; † 29. Juli 1974 in München) war ein deutscher Schriftsteller, Publizist, Drehbuchautor und Verfasser von Texten für das Kabarett.

Seine publizistische Karriere begann während der Weimarer Republik mit gesellschaftskritischen und antimililitaristischen Gedichten, Glossen und Essays in verschiedenen renommierten Periodika dieser Zeit. Nach Beginn der NS-Diktatur war er einer der wenigen prominenten intellektuellen Gegner des Nationalsozialismus, die in Deutschland blieben, obwohl seine Werke verboten wurden und zu den „verbrannten Büchern“ zählten. Trotz verschiedener Repressalien konnte er sich unter Pseudonym beispielsweise mit Drehbucharbeiten für einige komödiantische Unterhaltungsfilme und Einkünften durch die Veröffentlichung seiner Werke im Ausland „über Wasser“ halten. Nach 1945 war Kästner wieder eine freiere Entfaltung möglich. Von 1951 bis 1962 war er Präsident des westdeutschen P.E.N.-Zentrums. Mit einer pazifistischen Grundhaltung äußerte er sich öffentlichkeitswirksam bei verschiedenen Gelegenheiten auch politisch gegen die Regierungspolitik der Adenauer-Ära in den 1950er und 1960er Jahren. 

Populär machten ihn vor allem seine Kinderbücher wie beispielsweise "Emil und die Detektive" (1929), "Das fliegende Klassenzimmer" (1933) und "Das doppelte Lottchen" (1949), sowie seine mal nachdenklich, mal humoristich, oft satirisch formulierten gesellschafts- oder zeitkritischen Gedichte und Aphorismen, deren bekannteste Sammlung unter dem Titel "Doktor Erich Kästners Lyrische Hausapotheke" erstmals 1936 im Schweizer Atrium Verlag erschien.

Erich Kästner wuchs in kleinbürgerlichen Verhältnissen in der Königsbrücker Straße in der Äußeren Neustadt von Dresden auf. In der Nähe befindet sich am Albertplatz im Erdgeschoss der damaligen Villa seines Onkels Franz Augustin heute das Erich Kästner Museum.

Sein Vater Emil Richard Kästner (1867–1957) war Sattlermeister in einer Kofferfabrik. Die Mutter, Ida Kästner geb. Augustin (1871–1951), war Dienstmädchen und Heimarbeiterin und wurde mit Mitte dreißig Friseurin. Zu seiner Mutter hatte Kästner eine äußerst intensive Beziehung. Schon als Kind erlebte er ihre Liebe als geradezu ausschließlich auf ihn bezogen – ein anderer Mensch spielte in ihrem Leben eigentlich keine Rolle. In seiner Leipziger und Berliner Zeit verfasste er täglich vertrauteste Briefe oder Postkarten an sie. Auch in seinen Romanen lässt sich immer wieder das Mutter-Motiv finden. Später kamen nie bestätigte Gerüchte auf, dass der jüdische Arzt Emil Zimmermann (1864–1953) – der Hausarzt der Familie – sein leiblicher Vater gewesen sei.

Kästner besuchte ab 1913 das Freiherrlich von Fletchersche Lehrerseminar in der Marienallee in Dresden-Neustadt, brach die Ausbildung zum Volksschullehrer jedoch drei Jahre später kurz vor Ausbildungsende ab. Viele Details aus dieser Schulzeit finden sich in dem Buch "Das fliegende Klassenzimmer" wieder. Seine Kindheit beschrieb Kästner in dem 1957 erschienenen autobiographischen Buch "Als ich ein kleiner Junge war", dort kommentiert er den Beginn des Ersten Weltkriegs mit den Worten: „Der Weltkrieg hatte begonnen, und meine Kindheit war zu Ende.“

Zum Militärdienst wurde er 1917 einberufen und absolvierte seine Ausbildung in einer Einjährig-Freiwilligen-Kompanie der schweren Artillerie. Die Brutalität der Ausbildung prägte Kästner und machte ihn zum Antimilitaristen; zudem zog er sich durch den harten Drill seines Ausbilders Waurich eine lebenslange Herzschwäche zu. Waurich wurde hierfür in einem Gedicht Kästners "(Sergeant Waurich)" kritisch bedacht. Nach dem Ende des Ersten Weltkriegs absolvierte er das Strehlener Lehrerseminar und ein Jahr später das Abitur am König-Georg-Gymnasium mit Auszeichnung und erhielt dafür das Goldene Stipendium der Stadt Dresden.
Im Herbst 1919 begann Kästner an der Universität Leipzig das Studium der Geschichte, Philosophie, Germanistik und Theaterwissenschaft. Als Student wohnte er 1922 zur Untermiete im Leipziger Musikviertel, Hohe Straße 51. Aufgrund der Inflation und seiner schwierigen finanziellen Situation nahm Kästner mehrere Nebenjobs an; u. a. verkaufte er Parfüm und sammelte die Börsenkurse für einen Buchmacher. Kästner schrieb eine Dissertation zum Thema „Friedrich der Große und die deutsche Literatur“ und wurde 1925 promoviert. Sein Studium finanzierte Kästner schon bald aus eigenen Einnahmen als Journalist und Theaterkritiker für das Feuilleton der "Neuen Leipziger Zeitung".

Dem kritischer werdenden Kästner wurde 1927 gekündigt, nachdem seinem von Erich Ohser illustrierten erotischen Gedicht "Nachtgesang des Kammervirtuosen" Frivolität vorgeworfen worden war. Im selben Jahr zog Kästner nach Berlin, von wo aus er unter dem Pseudonym „Berthold Bürger“ weiter als freier Kulturkorrespondent für die "Neue Leipziger Zeitung" schrieb. Kästner veröffentlichte später noch unter vielen anderen Pseudonymen (z. B. „Melchior Kurtz“, „Peter Flint“ oder „Robert Neuner“).

In der Kinderbeilage der im Leipziger Verlag Otto Beyer erschienenen Familienzeitschrift "Beyers für Alle" (seit 1928 "Kinderzeitung von Klaus und Kläre") wurden von 1926 bis 1932 unter den Pseudonymen „Klaus“ und „Kläre“ fast 200 Artikel – Geschichten, Gedichte, Rätsel und kleine Feuilletons – geschrieben, die nach heutigem Stand der Forschung wohl großteils von Kästner stammen. Sein erstes größeres Werk, "Klaus im Schrank oder Das verkehrte Weihnachtsfest", entwarf er im Juli 1927. Die Endfassung schickte er noch im selben Jahr an mehrere Verlage, die das Stück allerdings als zu modern ablehnten.

Kästners Berliner Jahre von 1927 bis zum Ende der Weimarer Republik 1933 gelten als seine produktivste Zeit. In wenigen Jahren stieg er zu einer der wichtigsten intellektuellen Figuren Berlins auf. Er publizierte seine Gedichte, Glossen, Reportagen und Rezensionen in verschiedenen Periodika Berlins. Regelmäßig schrieb er als freier Mitarbeiter für verschiedene Tageszeitungen, wie das "Berliner Tageblatt" und die "Vossische Zeitung" sowie für die Zeitschrift "Die Weltbühne". Unterstützt wurde er ab 1928 von seiner Privatsekretärin Elfriede Mechnig, die ihm 45 Jahre lang die Treue hielt.

Hans Sarkowicz und Franz Josef Görtz, die Herausgeber der Gesamtausgabe von 1998, nennen im Nachwort des der Publizistik Kästners gewidmeten Bandes über 350 nachweisbare Artikel von 1923 bis 1933; die tatsächliche Zahl dürfte höher liegen. Dass so vieles heute verloren ist, mag damit zusammenhängen, dass Kästners Wohnung im Februar 1944 völlig ausbrannte.

Kästner veröffentlichte 1928 sein erstes Buch, "Herz auf Taille", eine Sammlung von Gedichten aus der Leipziger Zeit. Bis 1933 folgten drei weitere Gedichtbände. Mit seiner Gebrauchslyrik avancierte Kästner zur wichtigsten Stimme der Neuen Sachlichkeit.

Im Oktober 1929 erschien mit "Emil und die Detektive" Kästners erstes Kinderbuch. Die Detektivgeschichte entstand auf Anregung von Edith Jacobsohn. Das Buch wurde allein in Deutschland über zwei Millionen Mal verkauft und bis heute in 59 Sprachen übersetzt. Für die Kinderliteratur der damaligen Zeit mit ihren „aseptischen“ Märchenwelten äußerst ungewöhnlich war, dass der Roman in der Gegenwart der Großstadt Berlin spielte. Mit "Pünktchen und Anton" (1931) und "Das fliegende Klassenzimmer" (1933) schrieb Kästner in den folgenden Jahren zwei weitere gegenwartsbezogene Kinderbücher. Einen wesentlichen Anteil am Erfolg der Bücher hatten die Illustrationen von Walter Trier.

Gerhard Lamprechts Verfilmung von "Emil und die Detektive" wurde 1931 ein großer Erfolg. Kästner war jedoch mit dem Drehbuch unzufrieden, das Lamprecht und Billy Wilder geschrieben hatten. In Folge arbeitete er als Drehbuchautor für die Studios in Babelsberg.

Kästners 1931 veröffentlichter Roman "Fabian – Die Geschichte eines Moralisten" ist in fast filmischer Technik geschrieben: Schnelle Schnitte und Montagen sind wichtige Stilmittel. Er spielt im Berlin der frühen 1930er Jahre. Am Beispiel des arbeitslosen Germanisten Jakob Fabian beschreibt Kästner darin das Tempo und den Trubel der Zeit wie auch den Niedergang der Weimarer Republik. Auch seine eigene Tätigkeit als Werbetexter in Berlin spiegelt sich in der Figur Fabians.

Von 1927 bis 1929 wohnte Kästner in der Prager Straße 17 (heute etwa Nr. 12) in Berlin-Wilmersdorf, danach bis Februar 1944 in der Roscherstraße 16 in Berlin-Charlottenburg.

Im Gegensatz zu fast allen seinen regimekritischen Kollegen emigrierte Kästner nach der NS-Machtergreifung am 30. Januar 1933 nicht. Zwar fuhr er unmittelbar danach für kurze Zeit nach Meran und in die Schweiz, wo er auch bereits emigrierte Kollegen traf; dann jedoch kehrte er nach Berlin zurück. Kästner begründete diesen Schritt unter anderem damit, dass er vor Ort Chronist der Ereignisse sein wolle. Tatsächlich sammelte er Material aus der Zeit und machte sich umfangreiche Notizen in Gabelsberger-Kurzschrift in einem geheimen Tagebuch für einen künftigen Roman über das Dritte Reich. Dieses blau eingebundene Buch versteckte er in seiner Bibliothek, nahm es aber bei Bombenalarm mit in den Luftschutzkeller, weshalb es – anders als seine viertausend Bücher – erhalten blieb. Mindestens genauso wichtig dürfte aber sein, dass er seine Mutter nicht alleine lassen wollte. Mit dem Epigramm "Notwendige Antwort auf überflüssige Fragen" (aus: "Kurz und bündig") lieferte er gewissermaßen selbst auch eine Antwort:

Kästner wurde mehrmals von der Gestapo vernommen und aus dem Schriftstellerverband ausgeschlossen. Seine Werke wurden bei der Bücherverbrennung als „wider den deutschen Geist“ verbrannt, was er selbst aus nächster Nähe beobachtete. Der Aufnahmeantrag Kästners in die Reichsschrifttumskammer wurde wegen seiner „kulturbolschewistischen Haltung im Schrifttum vor 1933“ abgelehnt, was sich vor allem auf seine Unterzeichnung des „Dringenden Appells“ des Internationalen Sozialistischen Kampfbundes vom Juni 1932 bezieht. Dies war gleichbedeutend mit einem Publikationsverbot für das Deutsche Reich. Der mit Kästner befreundete Verleger Kurt Leo Maschler übernahm die Rechte vom Berliner Verlag Williams & Co. Bücher von Kästner erschienen nun in der Schweiz in dem von Maschler gegründeten Atrium Verlag.

Allerdings hat Kästner (im Gegensatz zu dem, was er selbst und seine Biographen über seine Arbeit in der Zeit des Nationalsozialismus berichten) unter Pseudonym sehr viel und sehr erfolgreich gearbeitet. Kästner stand nach Ansicht von Hermann Kurzke auf dem Höhepunkt seiner Produktivität und lieferte der Unterhaltungsindustrie des „Dritten Reiches“ Theatertexte und diverse Filmdrehbücher (teilweise als Mitautor). Besonders erfolgreich war "Das lebenslängliche Kind"; im Ausland und in der Nachkriegszeit als Buch bzw. Film unter dem Namen "Drei Männer im Schnee" vermarktet.

Mit einer Ausnahmegenehmigung lieferte Kästner 1942 unter dem Pseudonym „Berthold Bürger“ das Drehbuch zu "Münchhausen", dem prestigeträchtigen Jubiläumsfilm der Ufa. Der Anteil Kästners an dem mit Bobby E. Lüthge und Helmut Weiss verfassten Drehbuch zu dem Heinz-Rühmann-Film "Ich vertraue Dir meine Frau an" lässt sich heute nicht mehr abschätzen.

1943 musste Kästner wegen der Evakuierung der Zivilbevölkerung seine Berliner Stadtwohnung verlassen. Er kam bei Freunden in Neubabelsberg unter. Kästners Wohnung in Charlottenburg wurde 1944 durch Bomben zerstört. Anfang 1945 gelang es ihm, mit einem Filmteam zu angeblichen Dreharbeiten nach Mayrhofen in Tirol zu reisen und dort das Kriegsende abzuwarten. Diese Zeit hielt er in einem 1961 unter dem Titel "Notabene 45" veröffentlichten Tagebuch fest.

Nach dem Ende des Zweiten Weltkriegs zog Kästner nach München, wo er bis 1948 das Feuilleton der "Neuen Zeitung" leitete und die Kinder- und Jugendzeitschrift "Pinguin" herausgab. Gleichzeitig widmete er sich verstärkt dem literarischen Kabarett. So arbeitete er für „Die Schaubude“ (1945–1948) sowie „Die Kleine Freiheit“ (ab 1951) und für den Hörfunk. In dieser Zeit entstanden zahlreiche Nummern, Lieder, Hörspiele, Reden und Aufsätze, die sich mit dem Nationalsozialismus, dem Krieg und der Realität im zerstörten Deutschland auseinandersetzten, u. a. das "Marschlied 1945", das "Deutsche Ringelspiel" und das Kinderbuch "Die Konferenz der Tiere".

Kästners Optimismus der unmittelbaren Nachkriegszeit wich umso mehr der Resignation, wie die Westdeutschen mit Währungsreform und Wirtschaftswunder versuchten, zur Tagesordnung überzugehen. Hinzu kamen die bald erstarkenden Stimmen für eine Remilitarisierung. Seinem Anti-Militarismus blieb Kästner treu – er trat bei Ostermärschen als Redner auf und wandte sich später auch entschieden gegen den Vietnamkrieg. Sein Engagement richtete sich zudem gegen staatliche Maßnahmen, die er als Einschränkung der Pressefreiheit sah. So protestierte er 1952 etwa gegen das „Gesetz über die Verbreitung jugendgefährdender Schriften“ und zählte 1962 zu den ersten Intellektuellen, die sich gegen die Durchsuchungen und Verhaftungen während der Spiegel-Affäre wandten.

Er veröffentlichte jedoch immer weniger, wozu auch sein zunehmender Alkoholismus beitrug. Kästner fand keinen Anschluss an die Nachkriegsliteratur und wurde in den 1950er und 1960er Jahren überwiegend als Kinderbuchautor wahrgenommen und gewürdigt. Die Wiederentdeckung seines literarischen Werks aus der Zeit der Weimarer Republik begann erst ab den 1970er Jahren; "Fabian" wurde z. B. erst 1980 verfilmt.
Dennoch war Kästner sehr erfolgreich. Seine Kinderbücher wurden in zahlreiche Sprachen übersetzt und verfilmt, er selbst wurde vielfach geehrt. Kästner wurde 1951 Präsident des westdeutschen P.E.N.-Zentrums, ein Amt, das er bis 1962 innehatte; 1965 wurde er zum Ehrenvorsitzenden gewählt. Außerdem war er einer der Begründer der Internationalen Jugendbibliothek in München.

Kästner blieb lebenslang unverheiratet; er hatte allerdings zum Teil langjährige Liebesbeziehungen und Affären. Im Jahr 1957 wurde sein Sohn Thomas geboren. Von 1964 bis 1969 lebte Kästner mit seiner Freundin Friedel Siebert (1926–1986) und dem gemeinsamen Sohn in einer Villa in der Parkstraße 3a in Berlin-Hermsdorf am Waldsee. Kästner pendelte zwischen der Freundin in Berlin und der Lebensgefährtin Luiselotte Enderle in München. Viel Zeit verbrachte er außerdem in Sanatorien.

1969 feierte Kästner seinen 70. Geburtstag am Waldsee in Berlin-Hermsdorf. Im selben Jahr trennte sich Friedel Siebert von Kästner und übersiedelte mit Thomas in die Schweiz. Im Jahr 1977 wurde die Sammlung "Briefe aus dem Tessin", die Kästner in den 1960er Jahren an seinen Sohn und dessen Mutter geschrieben hatte, veröffentlicht. Für Thomas verfasste er auch seine beiden letzten Kinderbücher "Der kleine Mann" und "Der kleine Mann und die kleine Miss".

Kästner war häufig auch Rezitator seiner Werke. Bereits in den 1920er Jahren besprach er Schellackplatten mit seinen zeitkritischen Gedichten. In den Verfilmungen seiner Kinderbücher war er mehrfach der Erzähler, so zum Beispiel in der Verfilmung seines Buches "Das doppelte Lottchen" 1950 und in der ersten Hörspielbearbeitung von "Pünktchen und Anton" aus dem Jahr 1963. Des Weiteren sprach er für das "Literarische Archiv" der Deutschen Grammophon eine Auswahl seiner Gedichte, auch "Epigramme", und nahm seine Till-Eulenspiegel-Bearbeitung für die Sprechplatte auf. Nicht zuletzt bestritt Kästner diverse literarische Solo-Abende, so auch im Münchner Cuvilliés-Theater, und las für den Hörfunk aus seinem Werk, wie etwa "Als ich ein kleiner Junge war".

Ab 1965 zog Kästner sich fast ganz aus dem Literaturbetrieb zurück. Kurz vor seinem Tod gab er die Genehmigung, das Erich Kästner Kinderdorf nach ihm zu benennen. Kästner starb am 29. Juli 1974 im Klinikum Neuperlach an Speiseröhrenkrebs und wurde nach seiner Einäscherung auf dem Bogenhausener Friedhof in München beigesetzt.

In Dresden-Neustadt (Antonstraße 1 am Albertplatz) befindet sich in der Villa Augustin das "Erich Kästner Museum", für das sich ein Förderverein engagiert. Dort wurde auch auf einer Mauer eine Bronze-Skulptur gesetzt, die Kästner als sitzenden Jungen darstellt. An seinem Geburtshaus in der nahegelegenen Königsbrücker Straße 66 ist eine Erinnerungstafel angebracht, ebenso am Haus Parkstraße 3a in Berlin. Ein Denkmal, einige von Kästners Büchern symbolisierend, dazu ein Hut und ein Aschenbecher, steht in Dresden am Albertplatz.

Dutzende von Erich-Kästner-Schulen sind nach Kästner benannt. Entgegen den ansonsten gültigen Rechtschreibregeln zur Durchkopplung benutzen diese die Schreibweise „Erich Kästner-Schule“ oder „Erich Kästner Schule“ und folgen damit einem ausdrücklichen Wunsch Kästners.

Das Erich Kästner Kinderdorf in Oberschwarzach bei Schweinfurt bewahrt nach dem Wunsch Erich Kästners und Luiselotte Enderles seit Anfang der 1990er Jahre den Nachlass Kästners, darunter 8200 Bücher aus seiner Privatbibliothek und zahlreiche Gegenstände aus seinem Alltag.

Kästners schriftlicher Nachlass liegt im Deutschen Literaturarchiv Marbach. Teile davon sind im Literaturmuseum der Moderne in Marbach in der Dauerausstellung zu sehen, dazu gehören die Typoskripte seiner Romane "Emil und die Detektive" und "Fabian".

Marcel Reich-Ranicki meint zu den Büchern Kästners: „Kästner liebte das Spiel mit vertauschten Rollen“, er sah die Leser seiner Essays als Kinder und die Leser seiner Kinderbücher als Erwachsene an. Diejenigen, die in seinen Büchern über einen gesunden Menschenverstand verfügen, sind die Kinder und Halbwüchsigen. Sie verfolgen und fassen den Dieb, und die Ordnung wird hierdurch wieder hergestellt ("Emil und die Detektive"). Nicht die Eltern erziehen ihre Kinder – Erzieher sind die Kinder, die ihre Eltern zur Räson bringen ("Das doppelte Lottchen"). Kinder empfanden die meisten seiner Kinderbücher als wahr, weil sie oft das Milieu zeigten, das ihnen vertraut war. Seien es die Höfe Berlins oder einfach ‚Dem Volk aufs Maul geschaut‘. Er habe die Alltagssprache in seinen Büchern fixiert und damit den Kinderroman ‚Emil und die Detektive‘ in die Neue Sachlichkeit eingebunden.


Nach Kästner sind in Deutschland zahlreiche Straßen benannt worden. Der Asteroid (12318) Kastner erhielt den (anglisierten) Namen von Erich Kästner.

Zum 100. Geburtstag Kästners gab die Deutsche Post im Briefmarken-Jahrgang 1999 ein Sonderpostwertzeichen mit einem Motiv aus "Emil und die Detektive" und dem Nennwert 3 Deutsche Mark heraus (Michel-Nr. 2035).

Einer der ersten neuen Intercity-Express-Züge (ICE 4) wurde im Oktober 2017 nach "Erich Kästner" benannt.



Mehr als 40 Filme sind in vielen Ländern nach Kästners Werken entstanden, die bekanntesten sind:


Über Kästner

Bibliografie

Weitere Literatur


2015 gründete sich der "Förderverein Erich Kästner Forschung e. V." mit Sitz in München, der unter dem Reihentitel "Erich Kästner-Studien" Publikationen über Kästner herausgibt. Der Verein fördert wissenschaftliche und kulturelle Aktivitäten zu Kästners Leben, Werk und Wirkung, darunter Tagungen, Vorträge, Workshops und kulturelle Veranstaltungen.



Datenbanken

Portale

Biographien

Artikel

Verschiedenes


</doc>
<doc id="1391" url="https://de.wikipedia.org/wiki?curid=1391" title="ETH Zürich">
ETH Zürich

Die Eidgenössische Technische Hochschule Zürich, kurz ETH Zürich, ist eine technisch-naturwissenschaftliche universitäre Hochschule in Zürich. Die ETH Zürich zählt zu den zehn besten Universitäten der Welt und wird als beste kontinentaleuropäische Universität bewertet. Sie wurde 1855 als "Eidgenössisches Polytechnikum" gegründet. Ihre Gebäude verteilen sich auf zwei Standorte, einen im Zentrum der Stadt Zürich sowie den Standort Hönggerberg, ausserhalb des Stadtzentrums. Sie zählt zu den renommiertesten Universitäten Europas beziehungsweise weltweit.

Die Hochschule ist in 16 Departemente gegliedert und bietet 23 Bachelor- und 42 Master-Studiengänge an. Weiterführende Studien für ein Doktorat im technischen, mathematischen und naturwissenschaftlichen Bereich sind zahlreich möglich. Derzeit sind knapp 18'000 Studenten und Doktoranden eingeschrieben. Die ETH Zürich beschäftigt über 10'000 Personen. Von den 498 Professuren, einschliesslich 80 Assistenzprofessuren, sind 60 (13 %) von Frauen besetzt. Mit der ETH assoziiert sind 21 Nobelpreisträger. ETH-Präsident ist seit 2015 Lino Guzzella, Rektorin ist Sarah Springman.

Die ETH Zürich ist eingebunden in den ETH-Bereich, der die Technische Hochschule in Zürich und diejenige in Lausanne sowie vier weitere Forschungsanstalten (Paul Scherrer Institut, Eidgenössische Forschungsanstalt für Wald, Schnee und Landschaft [WSL], Eidgenössische Materialprüfungs- und Forschungsanstalt [Empa] und Eidgenössische Anstalt für Wasserversorgung, Abwasserreinigung und Gewässerschutz [Eawag]) umfasst.

Nach einer langen Debatte in dem noch jungen Bundesstaat Schweiz, ob neben den kantonalen Universitäten auch eine nationale, eidgenössische Hochschule zu gründen sei, wurde am 7. Februar 1854 das "Gesetz über die eidgenössische polytechnische Schule in Verbindung mit einer Schule für das höhere Studium der exakten, politischen und humanistischen Wissenschaften" erlassen, und die ETH nahm als "Eidgenössische polytechnische Schule" am 16. Oktober 1855 in Zürich ihren Betrieb auf. Jedoch wurde das Angebot auf rein technische Fächer beschränkt, da die katholischen, ländlichen Kantone, die eigene Universitäten unterhielten, ein intellektuelles Monopol der protestantischen, städtischen Kantone verhindern wollten.

Die neue Bildungsstätte sollte ein eigenes Gebäude erhalten, und so schrieben die Stadt und der Kanton Zürich einen Architekturwettbewerb aus, an dem sich unter anderem die beiden Professoren Ferdinand Stadler und Gottfried Semper aus Zürich sowie der Badener Architekt Joseph Caspar Jeuch beteiligten. Der deutsche Architekt Semper, der u. a. auch die nach ihm benannte Dresdner Oper entwarf, gewann schliesslich den Wettbewerb und durfte das Gebäude zwischen 1858 und 1864 nach seinen Plänen errichten. Im Südflügel des Gebäudes war anfänglich die bereits bestehende Zürcher Universität einquartiert. 

Das Polytechnikum wuchs rasch, und jedes Jahrzehnt kam ein neues Gebäude dazu: 1886 das Chemiegebäude an der Universitätsstrasse, von 1887 bis 1890 wurde an der Gloriastrasse 35 inmitten von Rebbergen das Physikgebäude gebaut (abgebrochen im Jahre 1974). Weitere Gebäude waren 1874 für die Forstwirtschaft erstellt worden, nördlich des Hauptgebäudes folgte 1900 das Maschinen-Labor und 1915 die Naturwissenschaft.

Als Folge der sogenannten Aussonderungsverträge 1905 und 1908 wurden die Gebäude der Zürcher Universität und der ETH 1914 voneinander getrennt. Nach dem Auszug der Universität baute der führende Architekt des Historismus in Zürich, Gustav Gull, zwischen 1915 und 1925 das Hauptgebäude von Semper um und veränderte die innere und äussere Gestalt des Gebäudes durchgreifend. Lediglich die gegen die Stadt weisende Südfassade blieb unverändert. Gegen die Rämistrasse wurde die Gestalt des Gebäudes durch eine neue Schaufassade und die heute charakteristische Kuppel ergänzt.

1908 erwarb das Polytechnikum das Recht, Doktortitel zu verleihen. 1911 wurde es nach einer längeren Reformdebatte in "Eidgenössische Technische Hochschule" umbenannt; informell blieb der Name „Poly“ bis in die 70er in Gebrauch. Gleichzeitig wurden die Studienpläne etwas gelockert und den Studenten mehr Freiheiten gewährt.

Bereits in den 1950er Jahren wurde klar, dass das ETH-Zentrum zu wenig Raum zur Verfügung hat. Bereits damals mussten zahlreiche Gebäude in der Nachbarschaft dazugekauft und Wohnungen für Institute umgenutzt werden (teilweise werden diese auch heute noch von der ETH genutzt). Jedoch konnte diese Strategie die Bedürfnisse der Hochschule nicht befriedigen, sodass die ETH 1957 beschloss, einen zweiten Standort in Zürich zu suchen. Zur Diskussion standen Areale auf der Allmend Fluntern, am Irchel, auf dem Adlisberg und auf dem Hönggerberg. Nur letzteres stellte sich als geeignet heraus, und die Planung für den Hönggerberg begann 1959. Für das Projekt wurde der renommierte Architekt Albert Heinrich Steiner gewonnen. Seit den sechziger Jahren wurden dort zusätzliche Gebäude erstellt, und seit Beginn des 21. Jahrhunderts läuft das Ausbauprojekt "Science City", in dessen Rahmen man unter anderem auch Studentenwohnungen auf dem Campus am Hönggerberg erstellen will.

Die Umsetzung erfolgte rasch, und Ende der 70er wurde die erste Etappe mit den Bauten für Physik (HPH und HPP), Molekularbiologie (HPM, HPK…) und der Infrastruktur abgeschlossen. Nicht mehr unter der Leitung von Steiner und teilweise sogar gegen seinen Willen erfolgte in einer zweiten Ausbauetappe (1972–1976) der Bau des Gebäudes für Architektur und Bauwissenschaften (HIL) unter den Architekten Max Ziegler und Erik Lanter. Die dritte Ausbauetappe wurde 1988 mit der Ausschreibung des Ideenwettbewerbs für den Richtplan eingeleitet; den Wettbewerb gewann der Architekt Ben Huser in der Weiterbearbeitung 1989. Nachdem die Architekten Mario Campi und Franco Pesina 1990 die Ausschreibung für die dritte Etappe (1996–2004) gewonnen hatten, wurde das Projekt von Steiner sogar vor Bundesgericht bekämpft, jedoch wurde seine Klage 1994 abgewiesen. Damit war der Weg frei für das Gebäude der Abteilungen Chemie und Werkstoffe sowie Pharmazie und Mikrobiologie (HCI).

Der Standort Hönggerberg beheimatet heute folgende Departemente: Architektur (D-ARCH), Bau, Umwelt und Geomatik (D-BAUG), Chemie und Angewandte Biowissenschaften (D-CHAB), Physik (D-PHYS), Biologie (D-BIOL) sowie Materialwissenschaften (D-MATL). Er kann mit den VBZ-Buslinien 37, 80 und 69 sowie mit zwei Shuttle-Linien direkt vom Zürich Hauptbahnhof und vom Hauptgebäude erreicht werden. Eine weitere Variante zur direkten Erschliessung der Science City über eine S-Bahnlinie und einem unterhalb der Science City angelegten Tunnelbahnhof wurde im Herbst 2014 vorgeschlagen. Die ergänzende Bahnlinie von 4,25 km Länge beginnt verzweigend beim Letziviadukt und führt als Tunnel von 3,5 km Länge unter dem Käferberg in die Gegend Aspholz, wo sie in die Linie nach Regensdorf mündet.

Derzeit wird der Standort Hönggerberg im Rahmen des Projektes "Science City" weiter ausgebaut. Die Idee ist es, einen Hochschul-Campus zu bilden, bei dem Forschen und Wohnen auf demselben Areal stattfinden, der aber auch offen ist für die Öffentlichkeit. 2008 wurde das "Information Science Lab" sowie das "Sport Center" fertiggestellt, das im Mai 2009 eröffnet wurde. Von 2013 bis 2016 wurden Studentenwohnungen gebaut. Diese umfassen insgesamt 63 Wohngemeinschaften mit 485 Zimmern, sowie zwölf Studios. Im selben Zuge wurden von der Luzerner Pensionskasse in Zusammenarbeit mit Losinger Marazzi AG das Projekt "livingscience" realisiert. Dies stellt 404 Studentenzimmer in ein, zwei oder secher Wohngemeinschaften zu Verfügung. Geplant wurden zudem ein Zeichensaal für die ETH, von Studenten nutzbare Gemeinschaftsräume, eine Kinderkrippe sowie dem studentischen Wohnen dienende Ergänzungsräume. Geplant sind zudem noch der Bau einer "Life Science Platform" sowie die Erstellung eines "Akademischen Gästehauses" als Aufstockung des bisher 14-stöckigen Physikturms. Das "Information Science Lab" wurde mit 23 Millionen Franken von dem Unternehmer Branco Weiss unterstützt, und die Zürcher Kantonalbank steuerte 12 Millionen Franken zum "Sport Center" bei. Im Dezember 2006 stimmte der Zürcher Gemeinderat dem Masterplan für die weiteren baulichen Entwicklungsschritte von Science City fast einstimmig zu. Die Sonderbauvorschriften sind im Herbst 2007 in Kraft getreten.

Energiepolitisch hat sich Science City das Ziel gesteckt, ihren CO-Ausstoss gemäss den Richtlinien des Kyoto-Protokolls zu reduzieren, um in Science City die Idee der 2000-Watt-Gesellschaft zu realisieren.

An der ETH Zürich entstehen jährlich ca. 700 Dissertationen und über 1800 (Master-)Diplome.

Begonnen hat die ETH mit 68 Studenten im Jahr 1855, seither hat sich ihre Zahl stetig vermehrt. Seit 1968 hat die Zahl der weiblichen Studenten stark zugenommen und betrug im Herbstsemester 2014 31,8 %.
Derzeit hat die ETH 23 Bachelor-Studiengänge mit 8500 Studierenden, 42 Master-Studiengänge mit 5100 Studierenden, und es sind 3900 Doktorierende eingeschrieben.

Im Juli 2015 hat Rektorin Sarah Springman bekannt gegeben, dass die Aufnahmefähigkeit der ETH bei 20'000 Studenten liege, sodass man den Zuwachs der aktuell bei 18600 liegenden Studentenanzahl begrenzen müsse. Springman zufolge gäbe es genügend gute Schweizer Studierende, sodass die Hochschule nicht auf ausländische angewiesen sei.

Die ETH Zürich ist in 16 Departemente unterteilt. Das jüngste Departement, Gesundheitswissenschaften und Technologie, wurde 2012 durch die Zusammenführung der Bewegungswissenschaften, Biomechanik, Lebensmittelwissenschaften und Neurowissenschaften etabliert.

Dazu kommt noch das Departement Management, Technologie und Ökonomie (D-MTEC), verantwortlich für einige Master-Studiengänge und Doktorate.

Die Schulleitung organisiert Leitung, Aufbau und Organisatorisches der ETH Zürich.

Das Hochschularchiv der ETH Zürich enthält Dokumente zur Geschichte der ETH Zürich und dient der wissenschafts- und technikgeschichtlichen Forschung. Es enthält Nachlässe und Autographen vieler namhafter Wissenschaftler und Nobelpreisträger.

Die Zulassung an die ETH erfolgt mit einer schweizerischen Maturität ohne weitere Bedingungen. Ausländische Studierende müssen abhängig von ihrem Schulabschluss eine Aufnahmeprüfung ablegen oder werden direkt aufgenommen.

Die Semestergebühren an der ETH Zürich betragen 580 Franken sowie zusätzliche 64 Franken Semesterbeiträge pro Semester. Nicht eingeschlossen sind dabei diverse zusätzliche Auslagen wie für Bücher, Praktika, Exkursionen etc. Für finanziell benachteiligte Studenten stehen Stipendien zur Verfügung.

An der ETH werden hauptsächlich naturwissenschaftliche und technische Fächer gelehrt. Die ETH kennt keine Nebenfächer, in allen Studiengängen sind jedoch Lehrveranstaltungen im Bereich der Geistes-, Sozial- und Staatswissenschaften (GESS) obligatorisch zu absolvieren.

Der akademische Sportverband Zürich (ASVZ) bietet ein breites Hochschulsportangebot mit über 80 Sportarten an, von Aerobic über Kletterkurse bis zu Yoga. Die wichtigsten Sportanlässe sind dabei die SOLA-Stafette, die über eine Distanz von 120 Kilometern im Grossraum Zürich durchgeführt wird, sowie der jährliche Rudermatch Uni-Poly zwischen der ETH und der Universität Zürich, der 2008 zum 57. Mal ausgetragen wurde. Die ETH führt bei den Herren noch mit 33 zu 22, die Universität gewann das Duell zuletzt jedoch 17 Mal in Folge. Zweimal musste das Rennen abgebrochen werden.

An der ETH Zürich ist ebenfalls der akademische Teil der Militärakademie (MILAK) der Schweizer Armee beheimatet. Im Rahmen ihrer Ausbildung besuchen angehende Berufsoffiziere den eigens dafür konzipierten Studiengang Staatswissenschaften.


Die Studenten der ETH Zurich sind im Verband der Studierenden an der ETH (VSETH) organisiert. Dieser Verein nach Schweizerischem Recht vertritt durch einen Rahmenvertrag die Studierenden innerhalb und ausserhalb der Hochschule und vereinigt in sich auch Studienfach spezifische Fachvereine, die ein breites Angebot an Veranstaltungen und Dienstleistungen für Studenten anbieten.

Die Doktorierenden sind in der Vereinigung der Assistenten, Wissenschaftlichen Mitarbeiter und Doktoranden der ETH Zürich (AVETH) organisiert.

Die ETH arbeitet in verschiedenen Verbünden mit anderen Hochschulen zusammen. Sie ist Gründungsmitglied der IDEA League, einer strategischen Allianz aus fünf führenden technischen Universitäten in Europa, deren Ziel es ist, Europa wieder an die Weltspitze in Technologie und Wissenschaft zu führen. Sie ist auch Mitglied im Netzwerk Top Industrial Managers for Europe, einem Zusammenschluss von 51 technisch orientierten Universitäten, das Austauschprogramme fördert und den Studenten Doppeldiplome ermöglicht. 2006 gründete sie zudem mit neun weltweit führenden Forschungsuniversitäten die International Alliance of Research Universities.

Die ETH hat 84 ERC-Grants für Grundlagenforschung auf höchstem Niveau eingeworben, 586 Millionen Franken aus dem 7. Forschungsrahmenprogramm der EU flossen an den Forschungsstandort Zürich. Nachdem die Schweizer Regierung in Umsetzung der Volksinitiative «Gegen Masseneinwanderung» das bereits ausgehandelte Personenfreizügigkeitsabkommen mit Kroatien nicht unterzeichnete, setzte die EU die Verhandlungen über das 8. Forschungsrahmenprogramm aus. Der ehemalige ETH-Präsident Ralph Eichler sagte, wenn die Schweiz nicht mehr an den EU-Forschungsrahmenprogrammen teilnehmen könne, sei das, «wie wenn der FC Basel nicht mehr in der Champions League spielen könnte».

Im Times Higher Education World University Rankings belegte die ETH im Jahre 2015 den 9. Platz weltweit und den 4. Platz unter den europäischen Universitäten (den ersten Platz in Kontinentaleuropa). Im Bereich "Engineering & Technology" belegte sie im Times Ranking den 8. Platz und im Bereich "Physical Science" den 11. Platz. Im Shanghai-Ranking 2015 belegte die ETH weltweit den 20. Platz (den vierten innerhalb Europas und den ersten auf dem europäischen Festland). Die ETH belegte weltweit den 8. Platz in der Kategorie "Natural Sciences and Mathematics". In den QS World University Rankings des Jahres 2015 belegte die ETH insgesamt den 9. Platz.


Zahlreiche berühmte Ingenieure und Wissenschaftler haben an der ETH Zürich gelehrt oder studiert. So stehen nach offiziellen Angaben der ETH 21 Nobelpreisträger mit der Hochschule in Verbindung:

Darüber hinaus erhielt Niklaus Wirth, Entwickler mehrerer Programmiersprachen, 1984 den Turing Award, die höchste Auszeichnung der Informatik, und die beiden ETH-Absolventen Jacques Herzog und Pierre de Meuron erhielten 2001 den Pritzker-Preis, den renommiertesten Architektur-Preis. Der Mathematiker Wendelin Werner ist Träger der Fields-Medaille, die als höchste Auszeichnung der Mathematik gilt. Weitere berühmte Persönlichkeiten in Verbindung mit der ETH Zürich werden in der Liste bekannter Persönlichkeiten der ETH Zürich aufgeführt.





</doc>
<doc id="1393" url="https://de.wikipedia.org/wiki?curid=1393" title="Editor">
Editor

Editor [] (von ‚herausgeben‘, ‚Herausgeber‘) steht für

Editor [] (von [Texte etc.] ‚herausgeben‘, ‚redigieren‘, ‚bearbeiten‘, aber auch [Filme] ‚schneiden‘) steht für
Siehe auch:



</doc>
<doc id="1395" url="https://de.wikipedia.org/wiki?curid=1395" title="Eiweiß">
Eiweiß

Eiweiß bezeichnet:

Siehe auch:


</doc>
<doc id="1397" url="https://de.wikipedia.org/wiki?curid=1397" title="El Torito">
El Torito

El Torito ist eine Spezifikation, die angibt, wie CD-ROMs formatiert sein sollen, damit Computer, die dafür durch ein entsprechendes BIOS eingerichtet sind, direkt von CD-ROM starten können, ohne dass die vorherige Installation eines Betriebssystems auf dem Festplattenlaufwerk erforderlich ist.

Die Spezifikation wurde Januar 1995 von Phoenix Technologies und IBM herausgegeben und ist heute weitgehend akzeptiert. Sie ermöglicht das Booten auf drei verschiedene Arten:

Beim Start der Festplattenkopie bezeichnet ein MS-DOS- oder Windows-kompatibles Betriebssystem das Laufwerk mit der CD-ROM als C:, alle weiteren Festplatten beginnen bei D:. Die robustere Variante ist der Start von einem Floppy-Abbild (auf der CD). In diesem Fall wird die CD-ROM mit A: von einem MS-DOS- oder Windows-kompatiblen Betriebssystem bezeichnet. Das ursprüngliche Floppy-Laufwerk A: kann dann mit B: angesprochen werden. 

Der Programmcode dagegen wird beim Starten direkt in den Speicher geladen und dort ausgeführt. Diese Option wird gern von Bootloadern verwendet.



</doc>
<doc id="1400" url="https://de.wikipedia.org/wiki?curid=1400" title="Endoskop">
Endoskop

Ein Endoskop ( und ‚beobachten‘) ist ein Gerät, mit dem das Innere von lebenden Organismen, oder technischen Hohlräumen untersucht und manipuliert werden kann. Die Endoskopie wurde ursprünglich für die humanmedizinische Diagnostik entwickelt. Heute wird sie auch für minimal-invasive operative Eingriffe an Mensch und Tier, sowie in der Technik zur Sichtprüfung schwer zugänglicher Hohlräume eingesetzt.

Zu den Endoskopen zählen die "starren" und "flexiblen" Endoskope und deren Unterarten.
Für Endoskope werden herstellerbezogen oft unterschiedliche Namen verwendet, so z. B. für starre Endoskope: Boreskope oder auch Boroskope, Technoskope, Autoskope, Intraskope; für flexible Glasfaser-Endoskope u. a. Fiberskope oder Flexoskope.

Gebräuchliche Arbeitsdurchmesser von starren Boreskopen sind 1,6 bis 19 mm. Halbstarre Boreskope (auch elastische oder semiflexible genannt) sind ab 1,0 mm, flexible Endoskope von 0,3 bis 15 mm und Videoendoskope von 3,8 bis 12 mm erhältlich.

Ein "starres" Endoskop ("engl./techn. Rigid Borescope") leitet die Bildinformationen des zu untersuchenden Objektes bzw. Raumes durch ein Linsensystem im Inneren des Endoskopschaftes an das Okular weiter. Beispiele sind das technische Boreskop (siehe unten) und med. das Arthroskop und Zystoskop.

Stark verbreitet ist das von Harold H. Hopkins entwickelte Stablinsensystem. Hier wird das Licht durch Stablinsen aus Quarzglas geleitet und an Luftlinsen zwischen den Stäben gebrochen. Diese sehr lichtstarke Bauweise ermöglicht kleinere Linsendurchmesser. Die meisten aktuellen Endoskope bieten durch einen Fokussierungsring in der Nähe des Okulars die Möglichkeit das Bild auch für Brillenträger auf die optimale Schärfe einzustellen. Das für die Untersuchung/Inspektion notwendige Licht der Lichtquelle wird über den angeschlossenen Lichtleiter, ebenfalls im Inneren des Schaftes durch Glasfaserbündel an die Spitze des Endoskopes transportiert. Der Preis eines starren Endoskopes hängt von der Güte der verwendeten Linsen, den Blick/Sichtwinkeln des Objektivs und der Arbeitslänge bzw. dem Arbeitsdurchmesser ab. Im Mittel handelt es sich hier um einen Betrag im eher niedrigeren vierstelligen Eurobereich.

Starre Endoskope mit objektivseitig reflektierendem Schwenkprisma können in Hohlräumen zur Seite blicken. Durch Drehen des Endoskops in seiner Hauptachse und Schwenken des die Blickrichtung davon ablenkenden Prismas lässt sich ein größerer Raumwinkel im Hohlraum abtastend betrachten. Ähnliches leistet schon ein kleiner polierter Metallspiegel, der durch biegsamen Draht und Aufsteckhülse mit dem Endoskopobjektivkopf verbunden ist.

Bei einem "flexiblen" Endoskop (bzw. Flexoskop oder engl. , die Namensgebung ist z. T. Herstellerabhängig) werden Bild und Licht über Glasfaserbündel übertragen. Beispiele sind das technische Flexoskop (siehe unten) und das med. Gastroskop, Koloskop und Bronchoskop.

Ab einem praktikablen Durchmesser sind Fiberskope/Videoskope auch mit auswechselbaren statt festmontierten Objektiven ("Vor/Seit" oder "Rückwärts") sowie Arbeitskanälen zum Einführen von mikromechanischen Geräten (kleine Zangen oder Greifer) in den Untersuchungs- bzw. Inspektionsraum erhältlich. Flexible Glasfaser-Endoskope (Fiberskope) und Video-Endoskope (Videoskope) besitzen meist eine über eingebaute Bowdenzüge fernsteuerbare Gerätespitze. Diese kann je nach Modell und Durchmesser nach 2 ("auf-ab") oder nach 4 Seiten ("auf-ab" und "rechts-links") teilweise bis zu 180° abgewinkelt werden. Die Länge dieser abwinkelbaren Spitze kann je nach Durchmesser zwischen 30 und 70 mm liegen. Im Handgriff des Gerätes ist eine Mechanik eingebaut, die über Kippbügel oder Handräder auf die Bowdenzüge einwirkt und diese Bewegung der Spitze ermöglicht.

"Siehe auch: Medizinische Endoskopie und Mikromechanik"

Die jüngste Unterart der flexiblen Endoskope bilden die Videoendoskope, oft auch Videoskope genannt (engl. Videoscope bzw. Videoprobe), wobei die Namensgebung herstellerabhängig ist. Videoendoskope eröffnen ein neues Kapitel in der modernen Endoskopie, da sie zur Bilderzeugung und -übertragung digitale Technologien nutzen. Ein am Objektiv des Videoendoskopes angebrachter Chip (siehe auch: Digitalkamera) erzeugt ein Bild des Untersuchungsobjektes. Beim CMOS-Chip findet die Digitalisierung des Bildsignals auf dem Videochip statt, so dass das an den Videoprozessor übertragene Bild weniger durch elektromagnetische Störungen beeinflusst werden kann, als bei Endoskopen mit CCD-Chip, wo das analoge Signal des Chips erst außerhalb des Endoskopes im sogenannten Videoprozessor zur weiteren Verarbeitung digitalisiert wird. Ein Videoprozessor bereitet die Bildinformation auf und verbindet sie mit Untersuchungsdaten und Patienteninformationen, bevor die Bilder oder Videosequenzen auf dem Betrachtungsbildschirm angezeigt oder auf einem Speichermedium gesichert werden. Auch eine Übertragung ins Kliniknetzwerk kann von hier aus erfolgen. Genau wie Fiberendoskope besitzen auch Videoendoskope eine fernsteuerbar abwinkelbare Gerätespitze, die je nach Verwendungszweck in 2 oder 4 Richtungen bewegt werden kann. Die Steuerung erfolgt mechanisch oder elektronisch. Die mechanische Steuerung erfolgt über ein kleines Getriebe über Kipphebel oder Drehräder. Einige Videoskope haben anstelle der Mechanik kleine Elektromotoren eingebaut, die über einen Joystick die Bowdenzüge steuern.

Videoendoskope haben grundsätzlich eine um ein Vielfaches höhere Auflösung als Fiberendoskope. Die Bildqualität wird maßgeblich von der Qualität des Linsensystems und des Videochips sowie der Beleuchtung des Untersuchungsgebeites und der Nachbearbeitung des Bildsignals im Videoprozessor beeinflusst. Das letzte entscheidende Glied in der Kette der Bildwiedergabe ist der Betrachtungsbildschirm. 
Über viele Jahre bot der CCD-Chip die beste Videoqualität. Aufgrund der kompakten Bauform werden in Videoendoskopen auch heute noch meist Chips dieses Typs verwendet. Neueste CMOS-Chips ermöglichen höhere Bildwiederholraten als CCD bei gleichzeitig höherer Auflösung, so dass erste Videoendoskope heute eine Darstellung in 1080p erreichen.
Geräte, die LEDs in der Gerätespitze zur Ausleuchtung nutzen, erreichen aufgrund der geringeren Lichtintensität aktuell noch nicht die gleiche Bildqualität wie jene, die mittels Lichtleiter ausleuchten. Batteriebetriebene LED-Lichtquellen, die direkt am Endoskop befestigt werden stellen aufgrund der noch schwächeren Ausleuchtung nur eine Sonderlösung für Ausnahmesituationen dar. Die neueste Entwicklung der Lichttechnologie stellt die Multi-LED-Lichtquelle dar. Hier wird in einer externen Lichtquelle das Licht von mehreren LEDs gebündelt und wie bei der altbekannten Xenon-Lichtquelle über einen oder mehrere Lichtleiter auf das Untersuchungsgebiet abgestrahlt. Diese Technologie ermöglicht eine Ausleuchtung, die mindestens den gleichen Ansprüchen genügt, wie Xenon-Licht, bei gleichzeitig um ein vielfaches längerer Haltbarkeit der Leuchtmittel und deutlich geringerem Energieverbrauch. 

Bekannte Hersteller digitaler Videoendoskope sind Olympus, Pentax Medical, Fujifilm und Karl Storz.

Zu einem einfachen Endoskopset gehören:
Einzelne Komponenten verschiedener Hersteller lassen sich in der Regel nicht ohne weiteres kombinieren. Ein Lichtleiter oder Endoskop des einen Herstellers kann beispielsweise nicht ohne weiteres an einer Lichtquelle eines anderen Herstellers betrieben werden. Namhafte Hersteller bieten hierfür auf Nachfrage passende Adapter an. Zur Erleichterung der praktischen Arbeit mit Endoskopen werden von der Industrie verschiedene Haltearmsysteme angeboten.

Insbesondere die Nutzung digitaler Bildübertragungstechniken ("Videoendoskopie") mittels CCD-Chips machte den Einsatz teurer Xenon-Lampen notwendig. Deren Lichtstärke ist zwar exzellent, ihre Standzeit wird jedoch stark von den jeweiligen Ein/Ausschaltzyklen bestimmt. Es gilt: "Je mehr Zyklen desto geringer die Standzeit."

Leuchtmittel wie Xenon-Lampen entwickeln während des Betriebes enorm viel Wärme, welche zum größten Teil durch den Infrarotanteil des Leuchtmittelspektrums verursacht wird. Daher muss verhindert werden, dass der IR-Anteil zum Lichtaustritt des Endoskopes gelangt. Moderne Lichtquellen sind daher in der Lichtstärke regelbar, durch einen Ventilator gekühlt und die infrarote Strahlung wird durch dichroitische Hohlspiegel, sowie (zusätzlich) durch Wärmeschutzfilter vor dem Lichtleiter, weitgehend aus dem Lichtspektrum entfernt. Diese Systeme werden als Kaltlichtquellen und die Leuchtmittel als Kaltlichtspiegellampen bezeichnet.
Eine weitere, und aufgrund des niedrigen Strom/Kühlungsbedarfs, von Vorteil geprägte Entwicklung sind Geräte mit Leuchtdioden (Light Emitting Diode, LED) als Lichtquelle. Die Lichtleistung von LEDs kann sich jedoch zurzeit noch nicht mit der von Xenonlampen messen. Dennoch öffnet diese Technik neue Einsatzgebiete und bietet speziell für Lichtquellen im Akkubetrieb eine interessante Alternative.

Für endoskopische Lichtleiter werden hauptsächlich Glasfasern verwendet. Es gibt aber auch Lichtleiter, die das Licht mittels eines Gels als Transportmedium leiten können. Gellichtleiter oder auch Flüssigkeits-Lichtleiter genannt, bieten eine stärkere Lichtausbeute was besonders für große Räume und die digitale Endoskopie im Allgemeinen von Vorteil ist. Die Flüssigkeitslichtleiter sind in der Regel besser für die Übertragung von UV-Licht geeignet, als Glasfasern. Gel/Flüssig-Lichtleiter sind in der Verwendung etwas unhandlicher und nicht so biegsam, sowie etwas teurer, als Glasfaser-Lichtleiter. Ohne angeschlossenen Lichtleiter sieht man zwar ein Bild durch das Endoskop, dieses ist jedoch zu dunkel um in geschlossenen Räumen verwertbare Ergebnisse zu erzielen.

Bildleiter sind aus vielen Tausend einzelnen Glasfasern, mit einem Durchmesser von 7 bis 10 µm aufgebaut. Dies entspricht einer Auflösung je nach Durchmesser von 3.000 bis 42.000 oder 75 × 45 bis 240 × 180 Bildpunkten (Pixeln). Pro Faser kann jeweils eine Helligkeits- und Farbinformation übertragen werden. Der Moiré-Effekt, der durch die Überlagerung des Faserrasters mit dem CCD-Raster entsteht, kann die Qualität des Bildes vermindern, weshalb vermehrt Videoskope bzw. Video-Endoskope mit eingebautem CCD Chip am distalen Ende verwendet werden.

Auf dem Gebiet der Messtechnik bieten verschiedene Hersteller Messverfahren an. Jede Messtechnik birgt für sich, für den jeweiligen Verwendungszweck, Vor- und Nachteile. Insbesondere in der technischen Endoskopie werden in vielen Anwendungsgebieten Messsysteme eingesetzt, die heute zum Teil erstaunlich genaue Ergebnisse liefern können. Anwendungsgebiete hierfür sind Flugzeugturbinen oder Kraftwerksbereiche.
Es gibt derzeit vier verwendete berührungslose Messverfahren die in Videoendoskopen, teilweise auch in starren Endoskopen eingesetzt werden:
Diese Messarten liefern nur bei einer senkrechten Ausrichtung des Messendoskopes auf die zu messenden Oberfläche eine exakte Messung.

Viel genauer sind die Messverfahren:

Aktuelle Forschung befasst sich mit der Möglichkeit 3D-Daten endoskopisch zu erfassen. Hierzu wird meist der Ansatz der Streifenprojektion als Variante der flächigen Triangulation eingesetzt. Abhängig von den eingesetzten Optiken können technische Innengeometrien mit Auflösungen im unteren µm-Bereich erfasst und ausgewertet werden.

Wie bei jedem Messgerät hängt die Messgenauigkeit eines Endoskopmesssystems entscheidend von der Schulung und Erfahrung des Anwenders ab.
Ein komplett ausgestattetes, messfähiges Videoendoskop kann einen hohen fünfstelligen Eurobetrag kosten.

Im Zusammenhang mit dem Arbeitsdurchmesser eines Endoskopes gilt:

"Je größer der Durchmesser, desto heller und weiter das Bild."

Gemäß den Gesetzen der Optik ergibt sich weiter folgender Zusammenhang zwischen Sichtwinkel und Vergrößerungsfaktor:

"Großer Sichtwinkel = geringe Vergrößerung (Weitwinkel in der Fotografie)"
"Kleiner Sichtwinkel = starke Vergrößerung (Teleobjektiv in der Fotografie)"

Im Zusammenhang zwischen der Vergrößerung und dem Abstand von Objektiv und dem zu untersuchenden Gegenstand gilt:

"Der Vergrößerungsfaktor beschreibt die Objektgröße im Bild relativ zur realen Objektgröße"
und weiter:
"Der Vergrößerungsfaktor verhält sich invers proportional dem Abstand: Objektiv/Gegenstand" ("abhängig von weiteren Faktoren")

Endoskope werden mit einem Schlüssel ihrer Merkmale gekennzeichnet, dieser findet sich in der Regel
am Schaft oder dem Griffstück als Gravur wieder. Es gilt:

Ein Endoskop mit folgenden Angaben: 6-70-67 hätte demzufolge die Daten:
Arbeitsdurchmesser = 6,00 mm, Blickwinkel = 70°, Sichtwinkel = 67°.
Ein eher vergrößerndes Endoskop mit einem vorausblickenden Objektiv.


Pionierunternehmen der Endoskopie sind Olympus, Karl Storz und die Richard Wolf GmbH.

Das Einsatzspektrum der Endoskopie ist breit gefächert. Endoskope werden neben den bekannten Einsatzgebieten in Medizin und Technik auch in diversen anderen Bereichen eingesetzt, wie z. B.:

Die technischen Einsatzgebiete sind weitreichend, um nur einige zu nennen:
In der Luftfahrt wird die Endoskopie seit den 1950er-Jahren für die Wartung zum Beispiel von Flugtriebwerken eingesetzt. Unter Zuhilfenahme des Arbeitskanals und von Mikrowerkzeugen können auch kleinere Reparaturen an Triebwerkschaufeln durchgeführt werden. So etablierte sich in diesem Bereich der Begriff Boroskopie (v. engl. "Borescope; bore" „Bohrloch/Bohrung“). Das Flexoskop findet im Englischen seine Entsprechung als Flexiscope oder Flexoscope. "Endoskopie" (engl. "Endoscopy") stellt den Oberbegriff für diese Technologie dar.

Das Rundblickendoskop ist ein spezielles technisches Endoskop (engl. "Borescope" für Industrie-Endoskop, im Gegensatz zum medizinischen Endoskop) für die Inspektion von zylindrischen Hohlräumen. Ein Endoskop erzeugt im Normalfall ein rundes, scheibenförmiges Bild, ein Rundblickendoskop liefert dagegen ein ringförmiges Bild, d. h. in der Mitte der Austrittspupille befindet sich keine Bildinformation. Dies unterscheidet das Rundblickendoskop grundlegend auch vom Fischaugenobjektiv, bei dem sich die wesentliche Bildinformation in der Mitte der Austrittspupille befindet. Ein Fischaugenobjektiv schaut jedoch hauptsächlich nach vorn, das Rundblick-Prisma jedoch mehr zur Seite, dies jedoch ringsum (siehe Illustration).

Im Zuge steigender Material- und Qualitätsanforderungen werden industrielle Bauteile immer häufiger einer optischen Serienprüfung unterzogen. Bei unzugänglichen Oberflächen werden technische Endoskope als Hilfsmittel eingesetzt. Zur Innenprüfung zylindrischer Objekte, z. B. von einem Hydraulikzylinder, sind dies Endoskope mit einer seitlichen Blickrichtung, d. h. die optische Achse wird mittels Spiegel oder Prisma umgelenkt (vergleichbar mit einem Periskop).
Zur vollständigen Erfassung der Oberfläche müssen Objekt und Endoskop zueinander linear verschoben und rotiert werden. Um die aufwändige Rotationsbewegung zu vermeiden, wurde versucht, die Umlenkspiegel durch kegelförmige Spiegel zu ersetzen, die mit der Spitze auf das Endoskop aufgesetzt wurden. Diese Konstruktionen waren mechanisch, optisch und in der Anwendungspraxis unbefriedigend und haben sich nicht etabliert.
In der Endoskopie werden generell Umlenkprismen gegenüber Spiegeln bevorzugt. Spiegel sind sehr empfindlich bei Staub oder Schmutz und beeinträchtigen das Bild erheblich. Wenn man ein solches Prisma virtuell um die optische Achse des Endoskops rotiert, entsteht ein einfaches Rundblickprisma.

Erstmals wurde 1985 ein Geradeausblick-Endoskop vorn an der Spitze mit einem speziellen Prisma versehen. Das war eine Glaskugel, in die von vorn ein Kegel eingeschliffen wurde, der optisch verspiegelt wurde. Über diesen „Kegelspiegel“ konnte nun die Oberfläche eines Segmentes ringsum auf einen Blick betrachtet werden. Die Anforderung war damals die 100 %-Innenkontrolle an Auto-Hauptbremszylindern. Diese Teile, mit ihren gehonten Innenflächen mussten einwandfrei und ohne Lunker und Kratzer sein. Da es sich um ein wichtiges Sicherheitsteil am Auto handelt, war eine 100 % Kontrolle unumgänglich.
Im Laufe der Jahre wurde diese Technik noch verfeinert und auch anderen Anwendungen und Anforderungen angepasst. Schleift man z. B. anstelle des Kegels einen Radius in die Glaskugel ein, dann erreicht man sogar einen Rückblick und das ringsum. Diese Rundblickprismen, die ein erheblich besseres Bild als Spiegel liefern, wurden ständig weiterentwickelt und erfüllen heute höchste Ansprüche, sind sogar für automatische Bildverarbeitung geeignet.

Das Rundblickprisma besteht aus mehreren ineinander gefügten sphärischen Glasflächen. Es sammelt die gesamte 360°-Bildinformation von einem Längenabschnitt des Zylinders. Die Länge des Abschnittes hängt vom Bildwinkel (Sehfeld, FOV) des Rundblickprismas und dem Abstand der Oberfläche vom Endoskop ab. Bei direkter visueller Betrachtung oder Verwendung einer industrieüblichen Matrixkamera erscheint das Bild radial verzerrt. Die Verzerrung kann entweder über eine nachgeschaltete Bildverarbeitung beseitigt werden oder durch Verwendung einer ringförmigen Zeilenkamera, deren Streifenbilder aneinandergefügt eine verzerrungsfreie Abwicklung der Zylinderinnenwand liefern.

Medizinische Endoskope haben die Untersuchung des Magen-Darmtraktes, der Lunge und auch der Gebärmutter revolutioniert. Sogar die ableitenden Tränenwege können endoskopisch untersucht werden.
Die ältesten und einfachsten noch im Gebrauch befindlichen Endoskope bestehen aus einem starren Rohr, durch welches das notwendige Licht hineingespiegelt wird und wodurch man mit dem bloßen Auge sieht. Daher spricht man volkstümlich von „Spiegelung“. Die längeren Geräte waren zusätzlich mit Linsen in einem Schlauch am vorderen Ende ausgestattet und ermöglichten erstmals passiv geringe Bewegungen.
Eine erste Weiterentwicklung bestand darin, ortsfern erzeugtes Licht mit Glasfaserbündeln an die Rohrspitze zu bringen. Der nächste Entwicklungsschritt war, auch die Bildinformation über flexible, geordnete Glasfaserbündel, die Bildleiter, zum Auge des Untersuchers zu übertragen. Erst hiermit wurde das Endoskop wirklich flexibel. Die aktive Steuerung des Gerätes erfolgt seither über vier eingearbeitete Bowdenzüge.

Eine medizinische Endoskopieeinheit umfasst über die unter Basis beschriebenen Komponenten hinaus:

Heutzutage wird, vor allem unter stationären Bedingungen, das Bild nicht mehr direkt mit dem Auge (weder am starren Rohrendoskop, noch am Okular des flexiblen Endoskops) betrachtet, sondern an einem oder mehreren modernen Monitoren, die die Farbinformation möglichst wenig verfälschen, und die die Arbeit und das Lehren (Kibitzen) ohne Qualitätsverlust bei Tageslicht ermöglichen. Dadurch eröffnet sich zusätzlich auch die Möglichkeit der Aufzeichnung auf Videoträger oder eine Übertragung in Hörsäle.

Eine interessante neuere Entwicklung ist die „"Endoskoppille"“ oder Kapselendoskopie: Eine Minikamera, die peroral in Form einer Pille eingenommen und durch die natürliche Peristaltik durch den Verdauungstrakt transportiert wird, nimmt in fortlaufender Serie Aufnahmen des Darms auf. Die Kapsel ist für den Einweggebrauch (once disposable) konzipiert. Diese Technik wie auch die Auswertung sind aufwendig, aber im Falle verborgener Blutungen oder kleiner Tumore im Dünndarm als „ultima ratio“ äußerst hilfreich. Ein zeitgleicher therapeutischer Eingriff wie bei den anderen endoskopischen Methoden ist derzeit nicht möglich.

Von großer Wichtigkeit ist die Desinfektion der flexiblen Geräte, die hitzeempfindlich und daher einfachen Methoden nicht zugänglich sind. Heute wird durch moderne Desinfektionsgeräte die Keimarmheit der Endoskope garantiert. Das erste Desinfektionsgerät wurde 1976 von einer Arbeitsgruppe um S.E. Miederer entwickelt.

Bei den meisten endoskopischen Untersuchungen erfolgt für den Betroffenen zur Erleichterung eine Prämedikation, das heißt, es wird ein Beruhigungsmittel, zum Beispiel Midazolam, oder das Narkosemittel Propofol gegeben.


Aktuell wird in Zusammenarbeit von Forschungsgesellschaften und Herstellern an Endoskopen mit sehr kleinen Arbeitsdurchmessern gearbeitet. Durchmesser vergleichbar der Dicke eines menschlichen Haares sollen helfen, das Einsatzgebiet der Endoskopie in neue Bereiche auszudehnen, z. B.:

Eventuell werden bald auch CMOS-Bildsensoren in Videoendoskopen eingesetzt. Diese Art von Bildsensoren verspricht eine kostengünstigere Fertigung und weitere Vorteile in der Bildbearbeitung.

LEDs werden in ihrer Leistungsfähigkeit und Lichtausbeute immer besser, so dass es bereits Hersteller gibt, die sie in starren Videoendoskopen verbauen. LEDs erreichen heute eine Lichtausbeute von über 200 lm/W und der Stromverbrauch – wichtig bei akkubetriebenen Lichtquellen – ist geringer als bei herkömmlichen Lichtquellen.

Da die Handhabung von Instrumenten hohe Anforderungen an den Endoskopierenden hinsichtlich der Koordination der Instrumentes im Raum stellt, wird von Seiten der Industrie seit einigen Jahren die 3D-Technik zur Verfügung gestellt. Hierzu sind geeignete Instrumente Monitore und Brillen zur einwandfreien Bilddarstellung notwendig.





</doc>
<doc id="1401" url="https://de.wikipedia.org/wiki?curid=1401" title="Elefantenvögel">
Elefantenvögel

Die Elefantenvögel (Aepyornithidae, auch Madagaskar-Strauße oder madagassisch Vorompatras) sind eine ausgestorbene Familie der Laufvögel mit den zwei Gattungen "Aepyornis" und "Mullerornis". Sie ist durch Fossilien und zahlreiche subfossile Eifunde von der Insel Madagaskar vor der Ostküste Afrikas bekannt. Die erstmalige Entdeckung eines Fossils dieser Vögel gelang dem Franzosen Alfred Grandidier während einer seiner Forschungsreisen auf der Insel zwischen 1865 und 1870.

Die Aepyornithiden wiesen die typische cursoriale (auf Schnelläufigkeit abgestimmte) Anatomie eines Bodenvogels auf: Die Beine waren lang und die Zehen kurz, während der Flugapparat nahezu gänzlich zurückgebildet wurde. Die Armknochen sind bis auf den Oberarmknochen verloren, das Brustbein ist flach und ungekielt. Der Hals ist lang und trägt einen relativ gesehen kleinen Schädel. "Aepyornis maximus" war der größte Vertreter des Taxons und dürfte zwischen 2,7 und 3 m Höhe und über 400 kg Gewicht erreicht haben. Damit handelt es sich um einen der größten bekannten Vögel der Erdgeschichte und den größten Vogel in historischer Zeit, nur übertroffen von dem etwa gleich hohen, jedoch mutmaßlich schwereren "Dromornis stirtoni" aus dem Miozän oder vergleichbar mit "Brontornis burmeisteri", der zwar ebenfalls etwa gleich hoch wurde, jedoch ein geringer geschätztes Gewicht von ca. 350 bis 400 kg erreichte. Die Riesenmoa ("Dinornis") mit einer Scheitelhöhe von bis zu 3,6 m und einem geschätzten Gewicht von um die 250 kg, die am Ende des 14. Jahrhunderts ausgerottet wurden, waren die einzigen Vögel, die während historischer Zeit ähnliche Ausmaße erreichten. Von "Aepyornis maximus" sind auch subfossile Eierschalen und komplette Eier bekannt, deren Umfang in einigen Fällen bis zu einem Meter und die Länge 34 cm beträgt.

Die Aepyornithiden waren auf Madagaskar endemisch. Madagaskar trennte sich bereits in der Kreidezeit vom Kontinent Afrika, was einige auf einen Verlust der Flugfähigkeit der Vorompatra in situ schließen ließ. Phylogenomische Studien stützen die Paraphylie der Laufvögel in klassischer Auffassung und suggerieren einen unabhängigen Verlust der Flugfähigkeit innerhalb der Laufvögel. Mitochondriale DNA legt nahe, dass Elefantenvögel innerhalb der Urkiefervögel nahe mit einem Taxon der Kiwis, Emus und Kasuaren verwandt sind.

Es gibt keine direkten Belege der Nahrung, die Elefantenvögel üblicherweise zu sich nahmen. Die meisten heute lebenden Laufvögel sind Omnivoren. Es gibt jedoch dick beschalte Regenwaldfrüchte auf Madagaskar, die nach Meinung einiger auf die Verdauung durch Laufvögel ausgelegt waren. So weist etwa die Frucht der heute stark bedrohten Kokospalme "Voaniola gerardii" eine derartige dicke Schale auf, und einige madagassische Palmfrüchte zeigen eine dunkelblau-violette Farbe (z. B. "Revenea louvelii" und "Satranala decussilvae"), ähnlich jenen Früchten, die von Kasuaren bevorzugt werden.

Die Familie Aepyornithidae wird üblicherweise in zwei Gattungen unterteilt, "Aepyornis" mit vier Arten und "Mullerornis" mit drei Arten.

"Aepyornis" 
"Mullerornis" 

Vorompatras waren einst häufig auf Madagaskar und über die ganze Insel verbreitet. Man stimmt weitestgehend überein, dass das Aussterben des Elefantenvogels auf menschliche Einflüsse zurückzuführen ist. Womöglich dienten die großen Vögel als wichtige Fleischlieferanten. Es gibt mehrere Belege von geschlachteten Elefantenvögeln. Insbesondere die Eier der Vorompatras dürften gefährdet gewesen sein, es gibt Belege, dass diese als Mahlzeiten zubereitet wurden. Wahrscheinlich spielte auch die von den Ureinwohnern betriebene Brandrodung eine zusätzliche Rolle, da sie große Flächen des Lebensraums des Elefantenvogels zerstörte. Des Weiteren ist möglich, dass Krankheiten, übertragen durch eingeführtes Geflügel, eine Rolle spielten.

Das Datum des Aussterbens der Elefantenvögel ist nicht sicher belegt. Archäologische Beweise belegen ein Fortleben bis mindestens zum Jahr 1000. Gelegentlich wird über ein Überleben des Vorompatra bis in das 17. Jahrhundert spekuliert. Der erste Gouverneur Madagaskars, Étienne de Flacourt, berichtete von einem großen Vogel, der die Eier eines Straußes legte und die Ampatres bewohnte. Dieser Vogel suche sich die entlegensten Regionen, um nicht von Menschen bedroht zu werden.

Oft wird angenommen, dass die Aepyornithidae der Ursprung der Legenden um den Vogel Roch (andere Schreibweisen: Vogel Ruch oder Rock) sind. Ein historischer Hinweis darauf findet sich in Megiser (1623). Die Bezeichnung Elefantenvogel rührt von Marco Polos Beschreibungen des Roch her, in welchen ein adlerähnlicher Vogel von der Größe eines Elefanten geschildert wurde. Eier des Vorompatra wurden möglicherweise irrtümlich als die eines riesigen Greifvogels interpretiert, was "Aepyornis maximus" diesen Namen einbrachte. Eine weitere Möglichkeit ist, dass der Roch auf Sichtungen des heute ausgerotteten madagassischen Verwandten des Kronenadlers basiert, "Stephanoaetus mahery".

"Vorompatra" ist die alte madagassische Bezeichnung für den "Aepyornis maximus", von der mehrere Schreibweisen existieren. "Vorompatra" bedeutet „Vogel der Ampatres“. Die Ampatres sind die heutige Androy-Region. Dies deckt sich mit de Flacourts Bericht, dem zufolge die angeblichen Reliktbestände des Elefantenvogels die Ampatres bewohnen würden.




</doc>
<doc id="1405" url="https://de.wikipedia.org/wiki?curid=1405" title="Elektrochemische Zelle">
Elektrochemische Zelle

Elektrochemische Zelle ist ein Oberbegriff für folgende Zelltypen:



</doc>
<doc id="1406" url="https://de.wikipedia.org/wiki?curid=1406" title="Erfundenes Mittelalter">
Erfundenes Mittelalter

Das Erfundene Mittelalter, auch Phantomzeit-Theorie, ist eine These, gemäß der 300 Jahre des Mittelalters erfunden wurden. So soll auf das Jahr 614 in Wahrheit direkt das Jahr 911 gefolgt sein. 

Der deutsche Publizist, Germanist und Verleger Heribert Illig stellte 1991 die These auf, man könne durch die Entfernung angeblich erfundener Jahre die Chronologie des Mittelalters korrigieren. Der Ingenieur und Professor für Technikgeschichte, Hans-Ulrich Niemitz, der sich dieser Vorstellung anschloss, nannte den Zeitraum dann "Phantomzeit", weil das Fränkische Reich nach Chlodwig I. ein Produkt der Fantasie oder der Täuschung gewesen sei. Insbesondere hätten laut dieser These Personen wie Karl der Große und die anderen Karolinger vor Karl III. dem Einfältigen entweder überhaupt nicht existiert, oder sie seien vor 614 beziehungsweise nach 911 einzuordnen.

In der Öffentlichkeit hat diese These ein gewisses Interesse gefunden. Wiederholt wurde sie als Verschwörungstheorie bezeichnet. Von Geschichtswissenschaftlern und Mediävisten wird sie als Pseudowissenschaft zurückgewiesen, da die Hypothese auf nachgewiesenen Irrtümern und methodischen Fehlern beruhe. 

Die These des "erfundenen" Mittelalters gehört zum Themenkomplex der Chronologiekritik und betrifft Kalenderkunde, Astronomie, Diplomatik, Textüberlieferung, Archäologie, Architekturgeschichte und historische Geographie.

Illig geht davon aus, dass innerhalb der Chronologie der Historischen Wissenschaften eine Zirkelreferenz vorliege: Moderne absolute Datierungen wie die Radiokarbonmethode oder die Dendrochronologie seien an der als korrekt angenommenen Chronologie ausgerichtet und dürften daher nicht als Beleg für deren Richtigkeit angesehen werden. Eine neue Chronologie würde vielmehr zu einer Neufestlegung absoluter Datierungen und dadurch zu einer Neujustierung dieser Datierungsmethoden führen.

Heribert Illig nimmt an, dass die bei der Kalenderreform durch Papst Gregor XIII. im Jahr 1582 vorgenommene Berichtigung des julianischen Kalenders (mittlere Jahreslänge = 365,25 Tage) von zehn Tagen um drei Tage zu kurz ausgefallen sei. Die tatsächliche Jahreslänge beträgt ca. 365,2422 Tage. Die Gesamtabweichung seit Einführung des julianischen Kalenders im Jahr 46 v. Chr. hätte sich bis 1582 auf insgesamt 12,70 Tage (0,0078 Tage × 1628) summiert. Aufgrund der Tatsache, dass 1582 diese drei Tage nicht korrigiert werden mussten, leitete Illig die "fehlenden drei Jahrhunderte" ab, die er in der Ausgabe "Zeitensprünge", Heft 3/1993, auf genau 297 Jahre berechnete und den in Frage kommenden Zeitraum auf September 614 bis August 911 eingrenzte.

Kritiker entgegnen, Illig habe übersehen, dass das Datum der Tag-und-Nacht-Gleiche zur Einführung des julianischen Kalenders nicht überliefert ist und der 21. März als Frühlingsbeginn erst beim ersten Konzil in Nicäa im Jahr 325 n. Chr. für die weiteren Berechnungen des Osterdatums festgelegt wurde. Dieses Jahr, und nicht die Einführung des julianischen Kalenders, müsse daher der Ausgangspunkt der Chronologie sein. Bis zur Kalenderreform im Jahre 1582 hatte sich in den folgenden 1257 Jahren der astronomische Frühlingsbeginn vom 21. März um 9,73 Tage auf den 11. März verschoben, weshalb Papst Gregor XIII. die Kalenderreform im Jahr 1582 in der maßgeblichen päpstlichen Bulle "" verfügte und den 11. März mit der zehntägigen Korrektur nach vorne auf den 21. März verlegte. Somit widerspricht die Kalenderkorrektur um 10 Tage keineswegs der bestehenden Jahreszählung.

Illig behauptet, dass Originalurkunden aus dem besagten Zeitraum sehr spärlich seien und von Personen meist nur sehr unspezifisch sprächen. Überdies seien vom 10. Jahrhundert bis in die Zeit von Friedrich II. (Anfang 13. Jh.) zahlreiche Urkunden von Majuskelschrift auf Minuskelschrift umgestellt, also neu geschrieben worden, wonach man die alten Urkunden vernichtet habe. Eine Verfälschung um rund 300 Jahre sei dabei möglich gewesen.

Nach dem Kenntnisstand der historischen Wissenschaften existieren jedoch aus dem fraglichen Zeitraum etwa 7000 Originaldokumente. Für die monastische Literatur sei das 9. Jahrhundert an Autoren und Manuskripten sogar das reichste des gesamten frühen Mittelalters. Das Abschreiben war für die mittelalterlichen Zeitgenossen die einzige Möglichkeit, Texte zu kopieren. Eine pauschale Abwertung der Texte des Frühmittelalters als Fälschungen, wie sie bei Illig zu finden ist, ist wissenschaftlich nicht haltbar.

Die dritte Grundlage der Hypothese ist die Archäologiekritik. Sie basiert auf der Behauptung, dass es nur wenige archäologische Denkmäler aus dem Frühmittelalter gebe und dass diese falsch in die Zeit zwischen dem 7. und 10. Jahrhundert n. Chr. datiert worden seien. Hierzu wurden von Illig insbesondere Beispiele aus Bayern angeführt.

Fachwissenschaftlichen Publikationen kann dagegen entnommen werden, dass es für die fragliche Epoche eine große Zahl von archäologischen Funden gibt. In diversen Museen sind einige davon für die Öffentlichkeit zugänglich. Die Schichten zur Karolingerzeit lassen sich (etwa in Paderborn) eindeutig nachweisen. Auch die Ergebnisse der Dendrochronologie sprechen gegen Illigs Thesen.

Die Astronomiekritik gehört nicht zu den Ursprungs- und Kernelementen der These Illigs. Durch astronomische Untersuchungen ist aber auch sie mittlerweile widerlegt. Als Gegenargument führt Illig an, dass seine Thesen durch astronomische Rückrechnungen „nicht streng widerlegbar seien“, weil diese seiner Meinung nach für den betreffenden Zeitraum auf zu „unsicheren Quellen“ beruhten. Er erklärt, dass es zwar Belege in Form astronomischer Beobachtungen gegen seine These gebe, beruft sich aber auf ein Zitat des Astronomen Dieter B. Herrmann, das sich nur auf Sonnenfinsternisse bezieht. Das Zitat ist allerdings aus dem Zusammenhang gerissen, Herrmann selbst verwahrt sich gegen die Benutzung seiner Worte durch Illig. 

Astronomische Ereignisse der Vergangenheit sind zwar im Einzelfall nur schwer eindeutig einem Datum zuzuordnen, die Betrachtung vieler historischer Beobachtungen ergibt aber ein konsistentes Bild. Wie Dieter B. Herrmann anführt, sind die Berichte von Hydatius von Aquae Flaviae über zwei totale Sonnenfinsternisse, die in Aquae Flaviae (heute Portugal) innerhalb eines Abstands von 29,5 Jahren auftraten, durch astronomische Berechnungen sehr genau. Ebenso eine Sonnenfinsternis im Jahr 59 n. Chr. und mehrere Beschreibungen des Halleyschen Kometen. Sie sind eindeutig einem Zeitpunkt zuzuordnen und widerlegen somit Illigs These. 

Da es sich nach Illig bei der Phantomzeit um eine bewusste Täuschung handeln soll, stellt sich die Frage nach den Urhebern. Um diese Fälschung durchzuführen, wäre große Macht eines kleinen, zusammenarbeitenden Kreises nötig gewesen. Illig schlussfolgerte, dass es sich hierbei nur um den römisch-deutschen sowie den byzantinischen Kaiser und den Papst handeln könnte. 2005 stellte Illig die These auf, dass die Fälscher Otto III., Konstantin VII. und Silvester II. waren. Nur in dieser kurzen Zeit, 990 bis 1009 waren sich die Herrscher einig genug, eine solche Täuschung zu vollziehen. Ihr Motiv sei gewesen, dass sie das Bedürfnis gehabt hätten, selbst im Jahr 1000 zu leben. Otto III. habe sich überdies in Gestalt von Karl dem Großen einen ruhmreichen Vorgänger auf dem Thron ausgedacht.

Vorwürfe großangelegter Urkundenfälschungen in Europa wurden immer wieder erhoben. Wilhelm Kammeier beschrieb in den 1920er und 1930er Jahren angeblich groß angelegte Urkundenfälschungen, die er zeitlich im 15. Jahrhundert ansiedelte. Ihm ginge es aber weniger um Chronologiekritik, sondern um Ideologiekritik. Einzelne Aussagen und gar Quellenangaben von Illig lassen aber erkennen, dass er sie offenbar von Kammeier übernommen hat, ohne dass dies den Lesern von Anfang an deutlich gemacht wurde.

In der Öffentlichkeit wurde die These vom erfundenen Mittelalter wiederholt als Verschwörungstheorie bezeichnet. Der deutsche Philosoph Karl Hepfer benennt sie als typisch für den großen Wert, den Verschwörungstheorien auf die Zweckrationalität der mutmaßlichen Verschwörer legen, wobei die Rationalität der Zwecke selbst – hier die Erfindung von 300 Jahren Mittelalter – aber nicht hinterfragt werde.




</doc>
<doc id="1407" url="https://de.wikipedia.org/wiki?curid=1407" title="Eiderenten">
Eiderenten

Die Eiderenten sind die Arten der Vogelgattung "Somateria" in der Familie der Entenvögel (Anatidae). Die deutschsprachige Bezeichnung dieser Gattung bürgerte sich durch den Handel mit Daunen der Eiderente, der vermutlich bekanntesten Vertreterin dieser Gattung, ein. Eiderdaunen gelten als ein Material mit sehr guter Wärmedämmung und wurden insbesondere für das Füllen von Bettdecken verwendet. Bis heute wird auf Island Eiderdaunen kommerziell geerntet und verarbeitet, über lange Zeit waren Eiderdaunen eines der wichtigsten Exportgüter dieser Insel. Sowohl die Bezeichnung für den Vogel als auch seine Federn (Eiderdaunen) sind dem isländischen "æðr" entlehnt. 

Es handelt sich um große, auf dem Land plumpe und schwerfällige Enten. Der keilförmige Schnabel trägt bei zwei der drei Arten einen Frontalfortsatz. Der Geschlechtsdimorphismus ist besonders in der Färbung erheblich. Die Erpel haben im Prachtkleid eine grüne Färbung am Kopf, einen schwarzen Bauch und eine überwiegend weiße Oberseite. Im Ruhekleid dagegen sind sie schwarzbraun wie die Weibchen.

Unter den Entenvögeln sind die Eiderenten die Gattung, die sich am meisten einem Leben auf dem Meer angepasst haben. Nichtbrütende Vögel leben ausschließlich auf dem Meer.

Sie kommen in den nördlichen Küstenbereichen der Holarktis vor. Im Winter ziehen sie nach Süden in eisfreie Gewässer. Eiderenten leben im Allgemeinen in Küstennähe und tauchen nach Muscheln, Krabben, Krebsen und anderen Kleintieren. Sie sind gute Schwimmer und Taucher, denen selbst starker Seegang wenig ausmacht.

Die Gattung besteht aus drei Arten:



</doc>
<doc id="1408" url="https://de.wikipedia.org/wiki?curid=1408" title="Eiderente">
Eiderente

Die Eiderente ("Somateria mollissima") ist eine Vogelart, die zur Familie der Entenvögel (Anatidae) gehört. Es ist eine große, massig wirkende Meerente, die an der arktischen Küste des Atlantiks und des Pazifiks lebt. In Europa kommt sie vor allem in Skandinavien vor. Die Brutpopulation der Nordseeküste ist wesentlich kleiner. Im Sommer finden sich im Wattenmeer jedoch große Scharen nichtbrütender Eiderenten ein, denen sich im Spätsommer auch noch große Scharen an Mauservögeln hinzugesellen.

Die deutschsprachige Bezeichnung dieser Ente bürgerte sich durch den Daunenhandel ein. Sowohl die Bezeichnung für den Vogel als auch seine Federn (Eiderdaunen) sind dem isländischen "æðr" entlehnt. Im deutschen Sprachgebrauch wird sie gelegentlich auch als "Eidergans" oder "St.-Cuthbertsente" bezeichnet. Die lateinische Artbezeichnung "Somateria mollissima" weist auf die weichen und wärmenden Daunen dieser Entenart hin. Übersetzt bedeutet der wissenschaftliche Name "„die Allerweichste mit dem schwarzen Körper“".

Die Eiderente ist mit einer Körperlänge von durchschnittlich 58 Zentimetern etwas größer als eine Stockente und erreicht durchschnittlich ein Körpergewicht von 2,2 Kilogramm. Männchen werden bei dieser Entenart in der Regel älter, größer und schwerer als Weibchen. An Land wirkt die Ente plump und schwerfällig, sie ist jedoch ein guter Schwimmer und Taucher, der selbst mit starkem Seegang gut zurechtkommt. Aufgrund der hohen Schnabelwurzel, die direkt in die Stirn übergeht, wirkt der Kopf der Eiderente keilförmig. Sie ist dadurch von anderen Entenarten gut zu unterscheiden, da dieses Profil nur bei dieser Entenart vorkommt. Während des Fluges ist die Eiderente an ihrer kräftigen Gestalt, dem dicken und kurzen Hals sowie der auffallenden Kopfform deutlich zu erkennen.

Die Eiderente zeigt in der Gefiederfärbung einen deutlichen Geschlechtsdimorphismus. Das Brutkleid des männlichen Vogels, der wie bei allen Enten als Erpel bezeichnet wird, ist am Rücken und an der Brust überwiegend weiß. An der Brust ist das Gefieder leicht rosafarben überhaucht. Der Bauch, die Flanken, die Bürzelmitte, der Schwanz, die Ober- und Unterschwanzdecke sowie die Kopf-Oberseite sind schwarz gefiedert. Am Nacken ist das Gefieder dagegen hell moosgrün. Die Nackenfedern sind leicht verlängert, so dass sie eine kleine Holle bilden. Der Schnabel des Erpels ist beim Prachtkleid gelbgrün, ansonsten blaugrau bis grüngrau. Die äußeren Armschwingen sind schwarz, die inneren sind weiß und sichelförmig gebogen. Als Ruhekleid trägt das Männchen dagegen ein dunkelbraunes Gefieder, das stellenweise mit weißen Gefiederpartien durchsetzt ist. Die Bänderung des Gefieders ist allerdings etwas weniger auffällig als bei den Weibchen.

Das Weibchen trägt während des gesamten Jahres ein unauffällig dunkel- bis gelblichbraunes Gefieder, durch das sich am Körper dichte schwarze Gefiederbänder ziehen. Hals und Kopf sind dagegen stärker einfarbig braun. Das Gefieder hat dort nur eine feine, braunschwarze Strichelung. Sie ähnelt damit im Gefieder den Weibchen vieler anderer Entenarten, durch die auffällige Kopfform ist sie jedoch leicht als Eiderente identifizierbar. Der Schnabel der Eiderente ist beim Erpel grünlich gefärbt, der der weiblichen Eiderente ist dunkelgrün. Die Schnabelspitze ist heller und weist eine breite und verhornte Spitze auf. Die Augenfarbe ist bei beiden Geschlechtern braun.

Jungvögel beider Geschlechter gleichen in ihrer Gefiederfärbung den Weibchen. Sie sind jedoch etwas dunkler in ihrer Gefiederfarbe und weniger stark gebändert. Junge Erpel tragen das voll ausgebildete Prachtkleid des Männchens im 3. oder 4. Lebensjahr. Bereits im Prachtkleid des 2. Lebensjahres zeigen sie jedoch die deutlich die Schwarz-Weiß-Kontrastierung, wie sie für adulte Erpel typisch ist. Zu diesem Zeitpunkt finden sich im Kopf- und Halsbereich noch Federn mit gelbbraunen Rand. Teile des Rückengefieders sind noch schwarzbraun.

Die Eiderente kommt entlang der nördlichen Küsten von Europa, Nordamerika und Ostsibirien vor. Sie brütet von der Arktis bis in die gemäßigten Klimazonen, in Europa nach Süden etwa bis zum Wattenmeer und ins nordwestliche Frankreich. An der nordamerikanischen Atlantikküste reicht das Brutgebiet bis nach Maine, am Pazifik reicht das Brutgebiet bis nach Südalaska. Der Schwerpunkt des Brutgebietes der Eiderenten liegt auf Island, wo etwa 450.000 Paare brüten, sowie an der Ostsee, wo sich bis zu 600.000 Paare zur Brut versammeln. Als Brutplätze nutzt die Eiderente kleine vegetationslose Felseninseln und Schären, bewachsene oder bewaldete Inseln, geschützte und ruhige Meeresbuchten mit flachen Ufern. Der nordamerikanische Bestand wird auf 750.000 bis 1 Million Paare geschätzt. Die IUCN schätzt den Gesamtbestand der Eiderente auf 2,5 bis 3,6 Millionen Tiere und stuft die Art als „nicht gefährdet“ ein.

Vögel aus den nördlichsten Brutgebieten, etwa aus Spitzbergen, ziehen zum Überwintern in die gemäßigten Breiten, wo sie in geeigneten Küstengewässern große Trupps bilden können. Sie überwintern damit in den südlicheren Regionen des Verbreitungsgebiets dieses Vogels. Die südlichen Populationen sind dagegen weitgehend Standvögel.

Im Winter taucht die Eiderente regelmäßig in geringer Zahl auch in großer Entfernung zum Meer an den größeren Alpenseen auf. Seit den 70er Jahren übersommern hier immer wieder einige Vögel. Am Zeller See im Land Salzburg gelang 1972 sogar ein Brutnachweis. Auch in der Schweiz ist die Eiderente in Ausnahmefällen ein Brutvogel. 1988 brütete die Eiderente erstmals am Zürichsee, in den Folgejahren kam es auch zu weiteren Bruten am Neuenburger-, Vierwaldstätter- und Walensee.

Die gesellig lebende Eiderente gehört zu den tagaktiven Enten mit ausgeprägter Tauchfähigkeit. Sie lebt überwiegend von Muscheln bis zu einer Größe von 40 Millimetern und frisst außerdem Schnecken, Krebstierchen sowie – im Gegensatz zu anderen Entenarten –Fische. An der Nordseeküste nutzt sie vor allem die Miesmuschelbänke. Im Binnenland frisst die Eiderente außerdem die eingebürgerten Dreikantmuscheln. Pflanzliche Nahrung spielt bei dieser Ente keine große Rolle. Allerdings frisst das Weibchen während der Brutzeit auch Vegetabilien und nimmt dabei besonders die Pflanzen auf, die in der Nähe des Nestes wachsen.

Muscheln erbeutet die Eiderente, indem sie entweder den Wattboden absucht oder sie im Wasser ertaucht. Mit Hilfe ihres kräftigen Schnabels ist sie in der Lage, Muscheln von ihrer Unterlage abzureißen oder nach ihnen im Wattboden zu graben. Angespülter Seetang wird von ihr gleichfalls nach Wasserinsekten, Muscheln und Schnecken abgesucht. Die Eiderente taucht gewöhnlich nach Muscheln bis zu einer Gewässertiefe von sechs Metern und bleibt etwas mehr als eine Minute unter Wasser. Unter Wasser nutzt sie dabei ihre Flügel zur Fortbewegung. Einzelne Beobachtungen sprechen davon, dass die Eiderente auch wesentlich tiefere Meeresböden erreichen kann. Tauchgänge in Tiefen bis zu 50 Meter wurden bereits beobachtet.

Die Muscheln werden mit den Schalen gefressen. In ihrem starken Kaumagen werden sie geknackt; die Schalentrümmer scheidet die Ente anschließend als Speiballen aus. Das mit der Nahrung aufgenommene Salz wird über Salzdrüsen in der Stirn wieder abgegeben. Die Eiderente nutzt die Gezeitenwechsel gezielt aus, um auch solche Meeresregionen nach Nahrung abzusuchen, die für sie bei Flut nicht erreichbar wären.

Die Weibchen der Eiderente erreichen ihre Geschlechtsreife bereits in ihrem zweiten Lebensjahr. Nur ein Teil der zweijährigen Weibchen kommt allerdings auch schon zur Brut. Die Erpel dagegen beteiligen sich an der Balz erst in ihrem dritten Lebensjahr. Erst dann ist bei ihnen das Gefieder der erwachsenen Erpel weitgehend ausgebildet. Die Erpel beginnen mit ihrer Balz im Dezember. Erst im Spätwinter beteiligen sich auch die Weibchen daran. Es handelt sich um eine Gesellschaftsbalz, bei der sich bis zu 10 Männchen in der Nähe eines Weibchens versammeln. Junge, noch nicht geschlechtsreife Erpel halten sich häufig in der Nähe solcher balzenden Erpel auf und zeigen auch bereits erstes Balzverhalten.

Während der Balz ruft das Männchen ein weiches, dumpfes zwei- bis dreisilbiges "„ahoo“" oder "hu-huúuu", das über das Watt oder die Wasserflächen sehr weit zu hören ist. Junge Männchen beherrschen diesen Ruf noch nicht. Ihr Ruf klingt heiserer und ist lautmalerisch mit "gro-gro-ó" umschrieben. Das Weibchen antwortet auf die Balzrufe des Männchens mit gockelndem "„goggoggoggog“" und knarrendem "„krrr“".

Der Erpel zeigt während der Balzrufe eine charakteristische Körperbewegung, die gelegentlich auch als „"eine Verbeugung nach hinten"“ beschrieben wird.
Dabei legt der Erpel seinen Kopf weit in den Nacken und wölbt die Brust vor. Gewöhnlich umwerben mehrere Männchen ein Weibchen. Zu den typischen Balzhaltungen der Erpel gehören ein Imponierschwimmen, bei dem der Kopf langsam von rechts nach links gedreht wird, sowie das Strecken des Körpers aus dem Wasser, bei dem die Flügel nach hinten weggespreizt werden.

Seine Paarungsbereitschaft signalisiert das Weibchen, indem es sich flach auf das Wasser legt. Zur Paarung schwimmt der Erpel auf die Ente, drückt sie dabei fast völlig unter Wasser und beißt ihr mit dem Schnabel in den Nacken. Die Paarung selbst dauert nur wenige Sekunden.

Bei der Ankunft im Brutgebiet ist die Mehrzahl der Weibchen verpaart. Eine Paarbindung besteht in der Regel nur für ein Jahr. Die ortstreuen Weibchen verpaaren sich aber gelegentlich mit dem gleichen Erpel im nächsten Jahr erneut, wenn dieser in dasselbe Revier zurückkehrt.

Eiderenten brüten einzeln oder in kleinen Gruppen. Häufig befinden sich in den Brutgebieten aber auch größere Kolonien. Kolonien von bis zu 1.000 Paaren kommen beispielsweise auf Island vor. An geeigneten Plätzen können sich zwei bis drei Nester je Quadratmeter befinden. Eiderenten meiden Steilufer, schroffe Felsen und windexponierte Stellen. Steigt das Ufer sanft an, befinden sich die Kolonien mitunter mehrere hundert Meter von der Küstenlinie entfernt, so dass die Nester auch bei Hochwasser nicht vom Wasser erreicht werden.

Der Neststandort ist abhängig von den jeweiligen örtlichen Gegebenheiten. Auf vegetationslosen Brutplätzen errichtet das Weibchen das Nest zwischen dem Geröll. Das Nest ist dann nicht mehr als eine flache Mulde, die aber windgeschützt liegt. Ist eine krautige Vegetation oder Gebüsch vorhanden, liegen die Nester in ihrem Schutz. Gelegentlich nutzt das Weibchen auch alte Möwennester als Nistplatz. Auf bewaldeten Inseln errichten die Eiderenten ihre Nester auch im Schutz von Bäumen. Eiderenten nutzen regelmäßig ihre alten Brutplätze wieder, was die Vegetation in ihrem Brutgebiet beeinflusst. Bedingt durch den abgesetzten Entenkot sind die Stellen um die Nester krautig oder mit Zwergsträuchern bewachsen.

Die Brutzeit liegt je nach Region und Wetterbedingungen im Zeitraum von Anfang April bis Mitte Mai. Das Weibchen legt in der Regel vier bis sechs grünlich-graue Eier in die mit Bauchdaunen ausgepolsterte Nistmulde. Das Legeintervall beträgt 24 Stunden. Sind mehr als neun Eier im Nest, handelt es sich in der Regel um Mehrfachgelege, die bei Eiderenten wie bei anderen in Kolonien brütenden Enten und Halbgänsen häufig vorkommen. Verlässt das Weibchen während der Brut die Eier, bedeckt es diese mit Daunen, um den Wärmeverlust zu vermindern. Durch Störungen aufgeschreckte Weibchen spritzen beim Auffliegen Kot über die Eier. Die Eier werden während einer Dauer von 25 bis 26 Tagen ausschließlich durch das Weibchen bebrütet, das während dieser Zeit fastet. Das Männchen hält sich während dieser Zeit in der Nähe des Nestes auf. Es schränkt in dieser Zeit sogar die Nahrungsaufnahme ein, so dass die Erpel an Körpergewicht verlieren. Ist die Brut jedoch hinreichend weit fortgeschritten, wandern die Männchen zu den Mauserplätzen ab.

Die Jungvögel werden nach dem Schlüpfen von dem Weibchen geführt. Auf dem Meer schwimmend betreut das Weibchen die Jungvögel bis in den Spätsommer hinein. Diese Führungszeit beträgt etwa 65 bis 75 Tage. Während dieser Führungszeit kommt es häufig zur Vergesellschaftung mit mehreren Familien, die sich wieder auflösen, sobald die Jungvögel flugfähig sind.
Eiderenten sind verhältnismäßig standorttreue Tiere, die zum Teil in ihren Brutrevieren auch überwintern. Der überwiegende Teil der Population nutzt allerdings separate Mauser- und Überwinterungsquartiere, wobei überwiegend nur kurze Strecken gezogen werden.

Zur Mauser ziehen die Vögel nach der Brut in ihre Mauserquartiere, viele Vögel sind dann beispielsweise im Wattenmeer anzutreffen. Dabei bevorzugen die nur eingeschränkt flugfähigen Eiderenten Gebiete, in denen sie weitgehend ungestört sind. Ihre Fluchtdistanz gegenüber Menschen erhöht sich in dieser Zeit von normalerweise 100 bis 300 Meter auf 500 bis 1.000 Meter. Der Mauserzug ist daher dadurch bedingt, dass sie große Ruhezonen benötigen. Küstenbereiche, in denen sie sich sonst aufhalten, die ihnen aber nicht ausreichend Rückzugsmöglichkeiten bieten, werden während dieser Zeit von den Eiderenten gemieden. Mittlerweile nutzen die Eiderenten auch einige größere Alpenseen als Quartier für ihre Mauser. So sind Eiderenten während dieser Zeit beispielsweise auch am Bodensee zu beobachten, wo sich bis zu hundert Vögel versammeln. Gelegentlich dienen die Mauserquartiere auch als Überwinterungsort – so beispielsweise im Wattenmeer. Gelegentlich suchen sie aber ab Oktober bis November separate Überwinterungsquartiere auf, von denen sie ab Februar bis März in Richtung ihrer Brutgebiete zurückkehren. Die auf Island und Spitzbergen brütenden Vögel erreichen ihre Brutplätze in den Monaten April bis Mai.

In den nördlichsten Regionen ihres Verbreitungsgebietes zählen die Schneeeule und der Polarfuchs zu den Fressfeinden der Eiderente. In den südlicheren Verbreitungsgebieten gehören der Uhu, der Seeadler und der Rotfuchs zu den Arten, die in der Lage sind, die schwere Ente zu erlegen.

Küken und Eier sind außerdem durch Möwen sowie verschiedene Rabenvögel (beispielsweise Raben- und Nebelkrähe sowie Kolkrabe) gefährdet. Gefährdet sind die Jungvögel jedoch auch durch den Befall mit Parasiten, von denen einige sich auf die Eiderente als Zwischenwirt spezialisiert haben. Viele der Jungvögel leiden beispielsweise an Saugwürmern, die zu einer Schwächung der Jungvögel und gelegentlich zu ihrem Tod führen. Zu einem Massensterben von Eiderenten kann es außerdem kommen, wenn in strengen Wintern die Meeresküsten vereisen und die Eiderenten nicht mehr in der Lage sind, die Muscheln auf dem Meeresboden zu erreichen.

In dem großen Verbreitungsgebiet der Eiderente werden sechs Unterarten unterschieden, wobei Übergangs- und Mischpopulationen die genaue Abgrenzung der Unterarten schwierig machen:

Die Jagd auf die Eiderente ist in den skandinavischen Ländern bis auf Island sowie in Russland erlaubt. Sie wird dort zum Teil sehr stark bejagt. In Norwegen sind die großen Brutgebiete der Eiderente allerdings inzwischen geschützt. Auf Island wurde sie 1786 teilweise und seit 1847 völlig geschützt.

Neben der Jagd kommt es auch zu Verlusten von Gelegen und Küken, wenn Eiderenten durch Menschen gestört werden. Dies trifft vor allem auf die Küstenabschnitte zu, die stark touristisch genutzt werden. Eiderenten leiden außerdem an der Verschmutzung der Meere durch Pestizide. Die Niederlande wurden beispielsweise ab dem Jahre 1925 von Eiderenten besiedelt. Der Bestand wuchs relativ schnell auf 6.000 Individuen, brach jedoch dann auf Grund von Pestizidbelastungen stark ein. Bei Ölunfällen gehört sie zu den Arten, die aufgrund der Verschmutzung des Gefieders und dem Entzug der Nahrungsgrundlage in großer Anzahl sterben. Im Jahre 1970 kamen im Kattegat beispielsweise nach einem Ölunfall 30.000 Eiderenten ums Leben.

Eine der bekanntesten Kolonien von Eiderenten befindet sich auf den Farne Inseln vor Northumberland, Großbritannien. Die dort brütenden Vögel waren Gegenstand eines der ältesten Vogelschutzgesetze der Welt, das der Heilige Cuthbert im Jahre 676 n. Chr. erließ, daher rührt auch der Name St.-Cuthberts Ente. Heute brüten noch etwa 1.000 Entenpaare auf diesen Inseln. Da der Heilige Cuthbert der Schutzpatron von Northumberland ist, wurde die Eiderente zum Wappentier dieses Landkreises. Eiderenten werden dort gelegentlich auch "Cuddy’s ducks" genannt, da "Cuddy" der Kosename für Cuthbert ist.
Die Eiderente ist der Lieferant der Eiderenten-Daune, die eine hohe Wärmespeicherkapazität besitzt. Eiderdaunen galten über lange Zeit als das beste Material, das für die Füllung von Bettdecken verwendet werden konnte. Eine gezielte kommerzielle Ausbeute dieser Eiderdaunen begann bereits vor dem 10. Jahrhundert.

Bis in die Mitte des 20. Jahrhunderts stellten Eiderdaunen eines der wichtigsten Exportartikel Islands dar. Auch heute kommt dort der Eiderente aufgrund dieser Daunen eine größere wirtschaftliche Bedeutung zu. Mit den weichen und warmen Daunen werden Kissen und Bettdecken gefüllt. Die Ernte dieser Daunen ist dabei durchaus mit dem Artenschutz verträglich, da normalerweise die Federn verwendet werden, mit denen Eiderenten ihre Nester auspolstern und diese Federn geerntet werden können, nachdem die jungen Enten das Nest verlassen haben. Ein Daunennest wiegt im Schnitt nur rund 20 Gramm. Die Reinigung der Daunen von Pflanzenteilen ist eine zeitintensive Arbeit, die Stunden in Anspruch nimmt. Nur etwa 1,5 Gramm verwendbarer Daunen pro Nest bleiben nach diesem Reinigungsprozess über, so dass etwa die Ernte von 700 Nestern gebraucht wird, um ein Kilogramm handelbarer Eiderdaunen zu erhalten.

Eiderenten werden aufgrund des attraktiven Brutkleides der Männchen zunehmend in Gehegen gehalten. Es sind friedfertige Vögel, die sich gut mit anderen Wasservögeln vertragen. Für ihr Wohlbefinden brauchen diese Enten jedoch hinreichend tiefe Teiche mit sauberem Wasser.




</doc>
<doc id="1409" url="https://de.wikipedia.org/wiki?curid=1409" title="Euklidischer Algorithmus">
Euklidischer Algorithmus

Der euklidische Algorithmus ist ein Algorithmus aus dem mathematischen Teilgebiet der Zahlentheorie. Mit ihm lässt sich der größte gemeinsame Teiler zweier natürlicher Zahlen berechnen. Das Verfahren ist nach dem griechischen Mathematiker Euklid benannt, der es in seinem Werk „Die Elemente“ beschrieben hat.

Der größte gemeinsame Teiler zweier Zahlen kann auch aus ihren Primfaktorzerlegungen ermittelt werden. Ist aber von keiner der beiden Zahlen die Primfaktorzerlegung bekannt, so ist der euklidische Algorithmus das schnellste Verfahren zur Berechnung des größten gemeinsamen Teilers.

Der euklidische Algorithmus lässt sich nicht nur auf natürliche Zahlen anwenden. Vielmehr kann damit der größte gemeinsame Teiler von zwei Elementen eines jeden euklidischen Rings berechnet werden. Dazu zählen beispielsweise Polynome über einem Körper.

Euklid berechnete den größten gemeinsamen Teiler, indem er nach einem gemeinsamen „Maß“ für die Längen zweier Linien suchte. Dazu zog er wiederholt die kleinere der beiden Längen von der größeren ab. Dabei nutzt er aus, dass sich der größte gemeinsame Teiler zweier Zahlen (oder Längen) nicht ändert, wenn man die kleinere von der größeren abzieht.

Ist die Differenz von formula_1 und formula_2 sehr groß, sind unter Umständen viele Subtraktionsschritte notwendig.
Hippasos von Metapont benutzte schon vor Euklid diese so genannte "Wechselwegnahme" geometrisch für den Beweis der Inkommensurabilität bei gewissen regelmäßigen n-Ecken: Im Quadrat oder im regelmäßigen Fünfeck etwa gibt es keinen gemeinsamen Teiler (Maß) einer Seite mit der Diagonalen.

Heutzutage wird in der Regel der weiter unten beschriebene Divisions-Algorithmus verwendet, bei dem die Schritte 2 und 3 dadurch ersetzt werden, dass man, an Stelle der Differenz von formula_3 und formula_4, für formula_5 den Rest bei der Division von formula_3 durch formula_4 nimmt. Ein weiterer Vorteil dieser Variante ist, dass man sie auf beliebige euklidische Ringe (zum Beispiel Polynomringe über einem Körper) übertragen kann, in denen der klassische Algorithmus nicht funktioniert.

Der klassische Algorithmus hier in Pseudocode für nichtnegative ganze Zahlen a und b dargestellt:

codice_1

Dieser Algorithmus kann auch in einer rekursiven Version angegeben werden:

codice_2

Heutzutage ersetzt man die im klassischen Algorithmus auftretenden wiederholten Subtraktionen eines Wertes jeweils durch eine einzige Division mit Rest. Der moderne euklidische Algorithmus führt nun in jedem Schritt solch eine Division mit Rest aus. Er beginnt mit den beiden Zahlen formula_1 und formula_11, deren größter gemeinsamer Teiler bestimmt werden soll.

In jedem weiteren Schritt wird mit dem Divisor und dem Rest des vorhergehenden Schritts eine erneute Division mit Rest durchgeführt. Und zwar so lange, bis eine Division aufgeht, das heißt, der Rest Null ist.

Der Divisor formula_17 der letzten Division ist dann der größte gemeinsame Teiler.

Da sich die Zahlen in jedem zweiten Schritt mindestens halbieren, ist das Verfahren auch bei großen Zahlen extrem schnell.

Der größte gemeinsame Teiler von 1071 und 1029 wird mit dem Euklidischen Algorithmus wie folgt berechnet:

Der größte gemeinsame Teiler von 1071 und 1029 ist somit 21.

Im Folgenden wird der moderne Euklidische Algorithmus sowohl in einer rekursiven als auch einer iterativen Variante beschrieben. Dabei sind formula_1 und formula_2 jeweils die beiden Zahlen, deren größter gemeinsamer Teiler berechnet werden soll.

codice_3

codice_4

In jedem Schritt des Algorithmus wird eine Division mit Rest ausgeführt.

Die Division mit Rest hat die Eigenschaft, dass

gilt.

Im letzten Schritt des Algorithmus

ist formula_30 und es gilt deshalb

Da im ersten Schritt formula_32 und formula_33 war, ist

Der euklidische Algorithmus ist der älteste bekannte nicht-triviale Algorithmus. Das Verfahren wurde von Euklid um 300 v. Chr. in seinem Werk "Die Elemente" beschrieben. In Buch VII (Proposition 1 und 2) formulierte er den Algorithmus für ganze Zahlen und in Buch X (Proposition 2 und 3) für reelle Zahlen. Die letztere Version ist ein geometrischer Algorithmus und Euklid nannte ihn "anthyphairesis" (Wechselwegnahme). Er suchte ein größtes gemeinsames „Maß“ zweier Strecken: eine dritte Strecke, sodass die Länge der beiden ursprünglichen Strecken Vielfache der Länge der dritten Strecke sind.

Das Verfahren wurde wahrscheinlich nicht von Euklid erfunden, da er in den Elementen die Erkenntnisse früherer Mathematiker zusammenfasste. Der Mathematiker und Historiker Bartel Leendert van der Waerden vermutet, dass Buch VII ein schon von den Pythagoreern verwendetes Lehrbuch der Zahlentheorie ist. Hippasos von Metapont führte etwa 500 v. Chr. vermutlich seinen Beweis der Inkommensurabilität von gewissen Strecken und Diagonalen auf Grundlage des euklidischen Algorithmus durch, und auch Eudoxos von Knidos (um 375 v. Chr.) kannte wohl das Verfahren. Aristoteles (um 330 v. Chr.) wies auf dieses Verfahren in seinem Werk "Topik" (158b, 29-35) hin.

Jahrhunderte später wurde der euklidische Algorithmus voneinander unabhängig in Indien und China entdeckt, um damit hauptsächlich diophantische Gleichungen aus der Astronomie zu lösen und genaue Kalender zu erstellen. Im fünften Jahrhundert beschrieb der indische Mathematiker und Astronom Aryabhata den Algorithmus als „Pulverisator“, wahrscheinlich aufgrund seiner Effektivität beim Lösen diophantischer Gleichungen. Zwar hat schon der chinesische Mathematiker und Astronom Sun Tzu einen Spezialfall des chinesischen Restsatzes beschrieben, die allgemeine Lösung wurde jedoch von Qin Jiushao 1247 in seinem Buch "Shushu Jiuzhang" () veröffentlicht. Im neuzeitlichen Europa wurde der euklidische Algorithmus erstmals wieder in der zweiten Auflage von Bachets "Problèmes plaisants et délectables, qui se font par les nombres" beschrieben. Der Algorithmus wurde in Europa zum Lösen diophantischer Gleichungen und zur Berechnung der Kettenbruchentwicklung verwendet. Nicholas Saunderson veröffentlichte den erweiterten euklidischen Algorithmus und schrieb ihn Roger Cotes zu als Methode zur effizienten Berechnung von Kettenbrüchen.

Im 19. Jahrhundert gab der euklidische Algorithmus den Anstoß zur Entwicklung neuer Zahlensysteme wie den gaußschen Zahlen und den Eisenstein-Zahlen. 1815 verwendete Carl Friedrich Gauß den euklidischen Algorithmus, um die eindeutige Faktorisierung der gaußschen Zahlen zu zeigen. Seine Arbeit wurde jedoch erst im Jahr 1832 veröffentlicht. Gauß erwähnte den Algorithmus zudem in seinem 1801 veröffentlichten Werk Disquisitiones Arithmeticae, allerdings nur als Methode zur Berechnung von Kettenbrüchen. Peter Gustav Lejeune Dirichlet scheint der Erste zu sein, der den euklidischen Algorithmus als Grundlage eines großen Teils der Zahlentheorie beschrieben hat. Er bemerkte, dass viele Ergebnisse der Zahlentheorie, wie beispielsweise die eindeutige Faktorisierung, auch für andere Zahlensysteme gelten, in denen der euklidische Algorithmus angewendet werden kann. Dirichlets "Vorlesungen über Zahlentheorie" wurden von Richard Dedekind herausgegeben und erweitert, der den euklidischen Algorithmus für das Studium algebraischer Zahlen nutzte, einer neuen allgemeineren Zahlenart. Dedekind war beispielsweise der Erste, der Pierre de Fermats Zwei-Quadrate-Satz mit der eindeutigen Faktorisierung der gaußschen Zahlen bewies. Dedekind führte das Konzept des euklidischen Rings ein, ein Zahlensystem, in dem eine verallgemeinerte Variante des euklidischen Algorithmus angewendet werden kann. In den letzten Jahrzehnten des 19. Jahrhunderts trat der euklidische Algorithmus allmählich hinter Dedekinds allgemeinere Theorie der Ideale zurück.

Jacques Charles François Sturm entwickelte 1829 die sturmschen Ketten zur Berechnung der Anzahl der Nullstellen eines Polynoms in einem vorgegebenen Intervall. Dabei wird eine Variante des euklidischen Algorithmus verwendet, um die einzelnen Glieder einer Kette zu bestimmen.

In der Vergangenheit gab es zahllose Versuche, den euklidischen Algorithmus auf mehr als zwei natürliche Zahlen zu verallgemeinern, beispielsweise um außer ihrem größten gemeinsamen Teiler auch optimale (etwa kleinstmögliche) Multiplikatoren zu finden, die in der Linearkombination mit den Zahlen diesen Teiler liefern. Der moderne Stand der Forschung hierzu wurde von Havas, Majewski und Matthews dargestellt.

Der euklidische Algorithmus war der erste Algorithmus zur Berechnung von Ganzzahlbeziehungen kommensurabler reeller Zahlen. In den vergangenen Jahren wurden weitere Algorithmen für diese Aufgabenstellung entwickelt, beispielsweise der Ferguson–Forcade-Algorithmus aus dem Jahr 1979 und verwandte Algorithmen, der LLL-Algorithmus, der HJLS-Algorithmus (nach den Autoren Håstad, Just, Lagarias und Schnorr) und der PSLQ-Algorithmus (nach "partial sum of squares" plus "LQ matrix decomposition"). Im Jahr 2001 wurde gezeigt, dass die von einigen Autoren berichtete Instabilität des HJLS-Algorithmus lediglich auf einer unzweckmäßigen Implementierung beruhte und dass dieser Algorithmus äquivalent zum PSLQ-Algorithmus ist. Enger an den eigentlichen euklidischen Algorithmus angelehnt sind seine mehrdimensionalen Verallgemeinerungen von George Szekeres (1970), Helaman Ferguson und Rodney Forcade (1981), Just (1992), von Rössner und Schnorr (1996) sowie der sehr allgemeine Ansatz von Lagarias (1994).

1969 entwickelten Cole und Davie das Zwei-Spieler-Spiel „Euklid“, das auf dem euklidischen Algorithmus basiert. Bei diesem Spiel gibt es eine optimale Strategie. Die beiden Spieler beginnen mit zwei Stapeln von formula_1 und formula_2 Steinen. In jeder Runde nimmt ein Spieler formula_3-mal soviele Steine vom größeren Stapel, wie der kleinere Stapel groß ist. Auf diese Weise kann der nächste Spieler den größeren Stapel mit formula_38 Steinen auf formula_39 Steine verkleinern, wobei formula_40 die Größe des kleineren Stapels ist. Es gewinnt der Spieler, der einen Stapel komplett abträgt.

Mit dem euklidischen Algorithmus kann man den ggT mit verhältnismäßig geringem Aufwand (im Vergleich zur Berechnung der Primfaktorzerlegung der Zahlen "a" und "b") berechnen. Bei der Laufzeitanalyse stellt sich heraus, dass der schlimmste Eingabefall zwei aufeinander folgende Fibonacci-Zahlen sind. Bei aufeinander folgenden Fibonacci-Zahlen ergibt sich als Rest immer die nächstkleinere Fibonacci-Zahl. Die Anzahl der benötigten Divisionen beträgt im schlimmsten Fall Θ(log("ab")), wobei log("ab") proportional zur Anzahl der Ziffern in der Eingabe ist "(siehe Landau-Symbole)".

Da die für die Division zweier Zahlen benötigte Zeit ihrerseits von der Anzahl der Ziffern der Zahlen abhängt, ergibt sich eine tatsächliche Laufzeit von "O"("log(ab)^3") bei naiver Ausführung der Division.
Durch die vollständige Überführung der eigentlichen Berechnung in den Frequenzbereich mittels einer speziellen schnellen Fourier-Transformation wie sie im Schönhage-Strassen-Algorithmus Verwendung findet, schneller Reziprokwertberechnung mit dem Newton-Verfahren (im Frequenzbereich) für die Division und anschließender Rücktransformation mittels inverser schneller Fourier-Transformation kommt man so zu einer theoretischen Untergrenze von "O"("n⋅"log("n")²), wobei "n" die maximale Anzahl an Ziffern von "a" und "b" ist.

Die von Schönhage entwickelte Variante des euklidischen Algorithmus konnte durch Parallelisierung auf einem Multi-Prozessor-System weiter beschleunigt werden.

Die Quotienten, die im euklidischen Algorithmus auftreten, sind genau die "Teilnenner", die in der Kettenbruchzerlegung von formula_41 vorkommen. Hier für das obige Beispiel mit hervorgehobenen Ziffern:

Hieraus lässt sich der Kettenbruch entwickeln:

Dieses Verfahren lässt sich auch für jede beliebige reelle Zahl formula_5 anwenden. Ist formula_5 nicht rational, so endet der Algorithmus einfach nie. Die so gewonnene Folge an Quotienten stellt dann die unendliche Kettenbruchzerlegung von formula_5 dar.

Wie oben beschrieben wird der euklidische Algorithmus zur Berechnung des größten gemeinsamen Teilers zweier natürlicher Zahlen verwendet. Der Algorithmus lässt sich jedoch auch auf reelle Zahlen und exotischere Zahlensysteme wie Polynome, quadratische Zahlen und die nicht-kommutativen Hurwitzquaternionen verallgemeinern. Im letzten Fall wird der euklidische Algorithmus dazu verwendet, die wichtige Eigenschaft einer eindeutigen Faktorisierung zu zeigen. Das heißt, dass eine solche Zahl eindeutig in irreduzible Elemente, der Verallgemeinerung von Primzahlen, zerlegt werden kann. Die eindeutige Faktorisierung ist grundlegend für viele Beweise der Zahlentheorie.

Wie schon von Euklid im Buch 10 seines Werks „Die Elemente“ beschrieben, kann der euklidische Algorithmus auch auf reelle Zahlen angewandt werden. Das Ziel des Algorithmus ist es dann, eine reelle Zahl formula_46 zu finden, sodass die beiden Zahlen formula_1 und formula_2 ganzzahlige Vielfache dieser Zahl sind. Diese Aufgabenstellung ist gleichbedeutend mit der Suche nach einer Ganzzahlbeziehung zwischen den beiden reellen Zahlen formula_1 und formula_2, also der Berechnung zweier ganzer Zahlen formula_51 und formula_52, für die formula_53 gilt. Euklid verwendete diesen Algorithmus bei der Betrachtung der Inkommensurabilität von Strecken.

Der euklidische Algorithmus für reelle Zahlen unterscheidet sich in zwei Punkten von seinem Gegenstück für ganze Zahlen. Zum einen ist der Rest formula_54 eine reelle Zahl, obwohl die Quotienten formula_55 weiterhin ganze Zahlen sind. Zum anderen endet der Algorithmus nicht immer nach einer endlichen Anzahl von Schritten. Wenn er dies jedoch tut, dann ist der Bruch formula_56 eine rationale Zahl; es gibt also zwei ganze Zahlen formula_3 und formula_4 mit

und kann als Kettenbruch formula_60 geschrieben werden. Wenn der Algorithmus nicht endet, dann ist der Bruch formula_56 eine irrationale Zahl und mit dem unendlichen Kettenbruch formula_62 identisch. Beispiele für unendliche Kettenbrüche sind die Goldene Zahl formula_63 und die Wurzel aus 2 formula_64. Im Allgemeinen ist es unwahrscheinlich, dass der Algorithmus anhält, da fast alle Verhältnisse formula_56 zweier reeller Zahlen irrationale Zahlen sind.

Polynome in einer Variablen über einem Körper bilden einen euklidischen Ring. Die Polynomdivision ist für diese Polynome also eine Division mit Rest und der euklidische Algorithmus kann genauso wie bei den ganzen Zahlen durchgeführt werden. Die Berechnung des größten gemeinsamen Teilers der Polynome formula_66 und formula_67 in formula_68 gestaltet sich beispielsweise folgendermaßen:

Damit ist formula_71 (oder das dazu assoziierte Polynom formula_72) ein größter gemeinsamer Teiler von formula_73 und formula_46.

Wir halten einen faktoriellen Ring (d. h. einen Ring mit bis auf Einheiten eindeutiger Primfaktorzerlegung) formula_75 fest und betrachten Polynome aus dem Polynomring formula_76, also Polynome in einer Variablen formula_38 mit Koeffizienten aus formula_75. Im Spezialfall formula_79, wobei formula_80 ein Körper sei, erhalten wir so den Ring formula_81 der Polynome in zwei Variablen über formula_80.

In formula_76 ist Division mit Rest nicht mehr allgemein durchführbar. Seien z. B. formula_84 und formula_85 in formula_86. Polynomdivision in formula_87 liefert den Quotienten formula_88, der nicht in formula_86 liegt. Wir können allerdings eine Pseudodivision wie folgt definieren: Seien formula_73 und formula_46 Polynome aus formula_76 mit Grad formula_93 bzw. formula_94, sei formula_95 der Leitkoeffizient des Polynoms formula_46, und formula_97. Dann gibt es Polynome formula_98, so dass

wobei wieder formula_5 von geringerem Grad ist als formula_46. Durch wiederholte Durchführung der Pseudodivision lässt sich der ggT von formula_73 und formula_46 bestimmen, allerdings ist das Verfahren in der Praxis ineffizient, da die Faktoren formula_104 die Koeffizienten der Zwischenergebnisse exponentiell anwachsen lassen. Um das zu vermeiden kann nach jedem Schritt der Inhalt des Rests formula_5 entfernt werden, was allerdings wiederum ggT-Berechnungen in formula_75 erfordert. Effizienter lässt sich der ggT mit dem Subresultantenverfahren berechnen.

Von Josef Stein stammt der nach ihm benannte steinsche Algorithmus, der ohne die aufwändigen Divisionen auskommt. Er verwendet nur noch Divisionen durch Zwei, die von einem Rechner sehr schnell durchzuführen sind. Aus diesem Grund wird dieser Algorithmus auch binärer euklidischer Algorithmus genannt. Der Performancevorteil auf realen Rechnern zeigt sich aber erst, wenn der Integertyp die Registerbreite des Prozessors überschreitet.

Merkt man sich beim euklidischen Algorithmus die Quotienten formula_107 der Zwischenschritte, dann lässt sich damit eine Darstellung

mit ganzen Zahlen formula_51 und formula_52 finden. Dies nennt man den erweiterten euklidischen Algorithmus. Damit lassen sich die Inversen in Restklassenringen berechnen.

Eine andere Erweiterung ist der Algorithmus, der hinter dem Quadratischen Reziprozitätsgesetz steckt. Mit diesem lässt sich das Jacobi-Symbol effizient berechnen.



</doc>
<doc id="1410" url="https://de.wikipedia.org/wiki?curid=1410" title="Enterobakterien">
Enterobakterien

Die Enterobakterien oder Enterobacteriaceae (vorläufig die einzige Familie in der Ordnung Enterobacteriales) sind eine große Gruppe innerhalb der Domäne Bacteria. Nach dem phylogenetischen System gehören sie zur Klasse der Gammaproteobacteria in der Abteilung ("Divisio", bei den Prokaryoten auch als "Phylum" bezeichnet) Proteobacteria und bilden dort eine eigene Familie. 

Der Name Enterobakterien leitet sich von "enteron" ( ‚Darm‘) ab, weil viele von ihnen typische Darmbewohner sind. Aber auch viele freilebende, nicht darmbewohnende Bakterienarten gehören in diese Familie.

Sie sind stäbchenförmig und gewöhnlich 1 bis 5 µm lang und besitzen einen Durchmesser von etwa 0,5 - 1,0 µm. Von ähnlichen Bakterien können sie durch das Fehlen von Oxidase unterschieden werden. Die meisten können sich mit Flagellen aktiv bewegen, es kommen jedoch auch Gattungen vor, die sich nicht aktiv bewegen können. Da die Zellwand aus wenigen Mureinschichten und einer zweiten, äußeren Membran aus Phospholipiden und Lipopolysacchariden besteht, sind die Enterobakterien gramnegativ. 

Ihr Stoffwechsel ist fakultativ anaerob, daher können sie sowohl über Oxidation unter Anwesenheit von Sauerstoff Stoffe abbauen, als auch unter anaeroben Bedingungen (kein Sauerstoff) Gärung betreiben. Zwei wichtige anaerobe Stoffwechselwege, die zur Unterscheidung der einzelnen Gattungen genutzt werden, sind die 2,3-Butandiol-Gärung und die gemischte Säuregärung ("mixed acid fermentation"). Bei der gemischten Säuregärung treten als End- und Nebenprodukte vorwiegend Säuren, wie Essigsäure, Milchsäure und Bernsteinsäure (Succinat), aber kein Butandiol auf. Bei der 2,3-Butandiol-Gärung entstehen aus der Gärung von Glucose als End- und Nebenprodukte geringere Mengen von Säuren, aber vor allem in großen Mengen der Alkohol 2,3-Butandiol. Ein weiteres Merkmal der 2,3-Butandiol-Gärung ist das Zwischenprodukt Acetoin und die wesentlich höhere Gasproduktion (CO). Man findet Butandiolgärung z. B. bei "Enterobacter", "Klebsiella", "Erwinia" und "Serratia". Gemischte Säuregärung nutzen u. a. Gattungen wie "Escherichia coli", "Salmonella" und "Proteus".

Zur Bestimmung der einzelnen Gattungen wird eine Vielzahl von Diagnosetests genutzt. Zum Beispiel wird mit Hilfe des Voges-Proskauer-Tests das Zwischenprodukt Acetoin der 2,3-Butandiol-Gärung nachgewiesen.

Viele Enterobakterien sind Teil der gesunden Darmflora von Menschen und Tieren; sie kommen jedoch auch überall in der Umwelt vor (Boden, Wasser). Einige sind Krankheitserreger bei Mensch und Tier. Sie kommen vielfach als nosokomiale Erreger vor („Krankenhauskeime“) und befallen Menschen mit schwachem Immunsystem. 

Der wahrscheinlich wichtigste Vertreter der Enterobakterien ist "Escherichia coli", einer der wichtigsten Modellorganismen der Genetik und Biochemie sowie der Mikrobiologie. Auffällig ist des Weiteren die Gattung "Proteus", bei der man das sogenannte „Schwärm-Phänomen“ beobachtet. Wenn sich wachsende Kolonien dieser Bakterien auf einer Agar-Platte ausbreiten, sieht man einen Bakterienrasen mit konzentrischen Ringen.

Zu den Enterobakterien gehören:







</doc>
<doc id="1411" url="https://de.wikipedia.org/wiki?curid=1411" title="Ewigkeit">
Ewigkeit

Unter Ewigkeit oder etwas Ewigem versteht man etwas, das weder einen zeitlichen Anfang noch ein zeitliches Ende besitzt bzw. unabhängig von dem Phänomen Zeit existiert.

Die ursprüngliche Bedeutung war wohl „langer Zeitraum“, vom ahd. "ēwe" „Lebenszeit“ stammend (ursprünglich wohl von ie. *əiw- „Lebenszeit, Ewigkeit“). Noch im 16. Jahrhundert wurde gebetet "von ewen zu ewen". Umgangssprachlich verstand man daher unter Ewigkeit einen langen Zeitraum („Das dauert ja ewig“, als Übertreibung). Daraus ist ersichtlich, dass „endlos“ ursprünglich nur eine von mehreren möglichen Bedeutungen des heutigen Worts "ewig" war.

Durch theologische Einflüsse – insbesondere durch die Zeitauffassung des Augustinus – hat der Begriff „Ewigkeit“ später vor allem die Bedeutung der „Zeitlosigkeit“ angenommen.

Das Konzept der Ewigkeit ist wissenschaftlich nicht definiert, da die bekannten physikalischen Theorien, die sich mit Fragen der Kosmologie befassen, den Begriff des Unendlichen nicht sinnvoll formulieren. ("Siehe auch:" Steady-State-Theorie)

Philosophisch sieht man Konzepte der Logik oder Mathematik als zeitlos, und in diesem Sinne als ewig an. Der Begriffsinhalt von „unendlicher Zeit“, wurde von Platon entwickelt und von Plutarch und der jüngeren Stoa übernommen. Sie ist die Bezeichnung für das Grenzenlose, in dem alle Phänomene angesiedelt sind, deren Anfang oder Ende nicht gedacht werden kann. Die Ewigkeit gilt Platon als die wahrhafte Form des Seins, d. h. als Seinsweise der Ideen, die frei von allem Werden sind. Für die antiken Denker war die Welt unendlich, d. h. auch anfangslos. 

Ewige Dinge ("ewig" im Sinne von ‚zeitunabhängig‘) scheinen vom Anfang bis zum Ende der uns bewussten Zeit unverändert anzudauern, sofern wir sie überhaupt wahrnehmen. Dennoch ist ewig nicht mit statisch gleichzusetzen.

Insbesondere monotheistische Religionen (Jüdischer Glaube, Christentum, Islam) sprechen vom ewigen Gott oder vom ewigen Gottesreich sowie von ewigem Leben. Die Ewigkeit als Attribut Gottes drückt seine Existenz unabhängig und über zeitlichen Begriffen wie Anfang und Ende aus. Mit einem Konzept seiner Unveränderlichkeit ist sie nicht zwangsläufig verbunden. 

Das dem Menschen verheißene ewige Leben führt zu einer Teilhabe an dieser Ewigkeit Gottes. Manche denken dies allerdings ausgehend von einem Anfang, nach dem zeitlichen Tod. Die Bibel hingegen (z. B. ) versteht Ewigkeit als Qualitätsbegriff. Ewigkeit beginnt demnach nicht erst nach dem Tod, sondern beginnt mit der Erkenntnis Gottes und Jesu Christi und ereignet sich in der Beziehung des Glaubenden mit Gott. Für den so Glaubenden ist der leibliche Tod ein Übergang in das vollendete ewige Leben, das seinen Anfang aber schon im „Neugeborenwerden“ (vgl. ) genommen hat.

Im Urtext der Bibel wurde der Zeitbegriff Äon (griechisch "aion", "aionion") z. B. von Luther neben „Welt“ im zeitlichen Sinn (der Ursprungsbedeutung) zusätzlich auch mit Ewigkeit/ewig übersetzt, was umstritten ist, da die Wahl im Einzelfall willkürlich erscheint und erhebliche theologisch-eschatologische Konsequenzen hat. Das „ewige“ Leben wäre also vielmehr ein „äonisches Leben“ mit Anfang und Ende. Wenn das Neue Testament zweifelsfrei „Ewigkeit“ im Sinn von Endlosigkeit ausdrücken will, werden im griechischen Grundtext Negierungen wie "Unsterblichkeit", "Unvergänglichkeit" oder auch "Unauflöslichkeit" benutzt.

Für viele mittelalterliche Philosophen und Theologen, insbesondere für viele „Mystiker“, und auch für einige Ausprägungen des Buddhismus bedeutet „Ewigkeit“ ein Leben in einer – ewigen, „stehenden“, von zeitlichen Differenzen befreiten – Gegenwart. 

So schreibt Meister Eckhart:
Der frühneuzeitliche Autor Andreas Gryphius formuliert:
Man kann ähnliche Auffassungen auch Ludwig Wittgenstein zuschreiben. Dieser schreibt in seinem Tractatus 6.4311:
In der deutschen Rechtssprache wird der Begriff "Ewigkeit" durch die Bezeichnungen „Ewigkeitsklausel“ oder „Ewigkeitsgarantie“ für den Abs. 3 Grundgesetz verwendet. Danach sind das föderalistische Organisationsprinzip der Bundesrepublik Deutschland und zwei Artikel des Grundgesetzes wie die Unantastbarkeit der Menschenwürde (Art. 1 GG) sowie die am 23. Mai 1949 in Kraft getretenen allgemeinen Verfassungsgrundsätze (Art. 20 GG) für "ewig" vor Änderung und Abschaffung geschützt.




</doc>
<doc id="1412" url="https://de.wikipedia.org/wiki?curid=1412" title="Europäisches Patentübereinkommen">
Europäisches Patentübereinkommen

Das Europäische Patentübereinkommen (EPÜ; , "EPC"; , "CBE") ist ein internationaler Vertrag, durch den die Europäische Patentorganisation (EPO) geschaffen wurde und die Erteilung Europäischer Patente geregelt wird. Durch das EPÜ bilden seine Vertragsstaaten auch einen Sonderverband gemäß der Pariser Verbandsübereinkunft zum Schutz des gewerblichen Eigentums (PVÜ), müssen also dessen Bestimmungen einhalten (z. B. zur Priorität).

Das Europäische Patentübereinkommen wurde am 5. Oktober 1973 auf einer Konferenz in München von 16 europäischen Staaten unterzeichnet und trat für Belgien, Frankreich, Deutschland, Luxemburg, die Niederlande, die Schweiz, und das Vereinigte Königreich am 7. Oktober 1977 in Kraft. Weitere Staaten ratifizierten das Abkommen in der Folgezeit.

1991 fand eine weitere Konferenz der Mitgliedstaaten statt, auf der die Laufzeit eines Europäischen Patents auf zwanzig Jahre festgelegt wurde. Diese Änderung trat für die Mehrheit der Mitgliedstaaten am 4. Juli 1997 in Kraft. 

Eine grundlegende Überarbeitung des Übereinkommens erfolgte im Jahre 2000. Ziel der Überarbeitung war, das Übereinkommen flexibler zu machen, an neuere Internationale Verträge anzupassen und Bedürfnisse der Anmelder besser zu berücksichtigen. Das geänderte Übereinkommen, nach dem Jahr seiner Unterzeichnung kurz als EPÜ 2000 bezeichnet, trat für die überwiegende Mehrheit der Mitgliedstaaten am 13. Dezember 2007 in Kraft. Ein großer Teil der zwischenzeitlich beigetretenen Mitgliedstaaten hat nur die letzte revidierte Fassung des Jahres 2000 angenommen.

Mit dem Beitritt Serbiens am 1. Oktober 2010 gehören nun 38 Vertragsstaaten dem Europäischen Patentübereinkommen an.

Das Übereinkommen wurde geschlossen, um die Patenterteilung innerhalb Europas zu zentralisieren und das Patentrecht seiner Vertragsstaaten zu harmonisieren. Statt in jedem Staat, in dem ein Patentschutz gewünscht wird, nationale Patentanmeldungen einzureichen, braucht nach dem EPÜ nur noch eine Anmeldung eingereicht zu werden, die vom Europäischen Patentamt (EPA), einem Organ der Europäischen Patentorganisation (EPO) zentral bearbeitet wird. In der Anmeldung müssen die Vertragsstaaten angegeben werden, für die ein Europäisches Patent beantragt wird. 

Ein Europäisches Patent kann auch beantragt werden durch eine Internationale Anmeldung nach dem Patentzusammenarbeitsvertrag (PCT = Patent Cooperation Treaty) und Einleiten der regionalen EP-Phase nach Abschluss der Internationalen Phase.

Die zentrale Bearbeitungsphase vor dem Europäischen Patentamt enthält außer dem eigentlichen Erteilungsverfahren evtl. noch ein Einspruchsverfahren, falls innerhalb von neun Monaten nach der Bekanntmachung der Erteilung eines Patents Einspruch dagegen erhoben wird.

Danach ist das Europäische Patentamt nicht mehr zuständig; das Europäische Patent „zerfällt“ in ein Bündel nationaler Patente in den in der Anmeldung benannten Vertragsstaaten, die den durch nationale Patentämter erteilten Patenten gleichwertig sind. Nichtigkeitsklagen gegen Europäische Patente können daher nur vor den nationalen Gerichten eingereicht werden.

Das Europäische Patentübereinkommen besteht aus mehreren Teilen:



Das Europäische Patentübereinkommen wurde von 38 Vertragsstaaten unterzeichnet. Darunter befinden sich alle 28 Mitgliedstaaten der EU sowie 10 weitere Staaten (Stand: Dez. 2014). Als bisher letztes Mitglied trat am 1. Oktober 2010 Serbien der Organisation bei: Vertreter der in der Tabelle mit (*) gekennzeichneten Staaten haben an der diplomatischen Konferenz zur Gründung der Organisation teilgenommen. Diese Staaten waren daher berechtigt, der Organisation durch Ratifikation beizutreten. Island hat das Abkommen allerdings erst 2004 ratifiziert, in Norwegen ist es am 1. Januar 2008 in Kraft getreten.

Alle Vertragsstaaten des EPÜ sind auch Vertragsstaaten des Europarates. Dieses gibt Anlass zur Debatte, ob die Europäische Patentorganisation dem Europarat beitreten kann. Derzeit wird geprüft, wie die EPÜ dem Europarat beitreten kann. 

Außerdem hat die Europäische Patentorganisation in den Jahren 1993 bis 2009 mit einigen Staaten, die (damals) nicht dem EPÜ angehör(t)en, Abkommen über die Erstreckung des Schutzes europäischer Patente geschlossen. Es ist daher möglich, beim Europäischen Patentamt die Erstreckung einer europäischen Patentanmeldung auf die Erstreckungsstaaten zu beantragen. Mit dem Antrag sind Erstreckungsgebühren zu entrichten. Die Patentanmeldung hat dann in den Erstreckungsstaaten dieselbe Wirkung wie eine nationale Patentanmeldung und kann nach ihrer Erteilung auch dort als Patent eingetragen werden. Derzeit kann die Erstreckung für Bosnien und Herzegowina (BA) und Montenegro (ME) beantragt werden. Einige frühere Erstreckungsstaaten sind mittlerweile zu Vertragsstaaten geworden. Für eine in der Zeit eingereichte Patentanmeldung, als diese noch Erstreckungsstaaten waren, kann auch für diese Staaten noch eine Erstreckung beantragt werden.

Seit 2010 wurde dieses Instrument durch die Validierungsabkommen abgelöst, die nicht auf Europa beschränkt sind. Drei davon sind in Kraft, das Abkommen mit Tunesien ist seit 01. Dezember 2017 in Kraft.





</doc>
<doc id="1413" url="https://de.wikipedia.org/wiki?curid=1413" title="Eibengewächse">
Eibengewächse

Die Eibengewächse (Taxaceae) bilden eine Pflanzenfamilie in der Ordnung der Koniferen (Coniferales). Einige Arten werden weltweit als Zierpflanzen in Parks und Gärten verwendet.

Der in Mitteleuropa bekannteste Vertreter der Familie ist die Europäische Eibe ("Taxus baccata"), die auch in Parks oder Gärten angepflanzt wird. Die vegetativen Teile der Pflanze und ihre Samen sind sehr giftig. Der Arillus hingegen ist jedoch ungiftig und kann gegessen werden. Nach dem Verzehr von mehreren „Beeren“ samt Samen sollte ein Arzt aufgesucht werden. Das Gift bezeichnet man als Taxin, es enthält unter anderem das Diterpen Taxol.

Es sind immergrüne Bäume und Sträucher. Meist sind sie nicht harzhaltig und duften nicht aromatisch. Die Borke ist schuppig oder faserig. Die Blätter sind, bis auf "Amentotaxus", wechselständig oder spiralig, wirken aber oft wie zweizeilig, an den Zweigen angeordnet und verbleiben dort mehrere Jahre. Die nadelförmigen Blätter sind abgeflacht. Die Sämlinge besitzen zwei Keimblätter (Kotyledonen).

Die Arten der Taxaceae sind getrenntgeschlechtig: einhäusig (monözisch), aber meist zweihäusig (diözisch). Die männlichen zapfenförmigen Blüten stehen achselständig einzeln oder zu mehreren zusammen an einjährigen Zweigen; sie sind kugelig bis eiförmig. Die Sporophylle besitzen zwei bis zwölf Mikrosporangien (Pollensäcke). Die kugeligen Pollen sind nicht geflügelt. Die Verbreitung des Pollens erfolgt durch den Wind. Die achselständig an einjährigen Zweigen stehenden, weibliche Zapfen sind zu einer einzigen Zapfenschuppe reduziert, die ein oder zwei Samenanlagen besitzt.

Je Zapfen wird nur ein Same gebildet. Der Same besitzt eine harte Samenschale und ist zur Reifezeit von einem Samenmantel (Arillus) (einer fleischigen Hülle, die einer Beere etwas ähnlich sieht) teilweise oder ganz ("Torreya") umgeben. Der Arillus kann intensiv gefärbt sein und saftig, fleischig oder ledrig sein. Von der Farbe und dem Geschmack des Arillus werden vor allem Vögel (Ornithochorie) angelockt. Der Arillus wird meist mit dem darin verborgenen Samen gefressen und der Same wird dann unverdaut wieder ausgeschieden.

Die Gattungen der Familie der Eibengewächse kommen hauptsächlich auf der Nordhalbkugel vor. Das Verbreitungsgebiet reicht südlich bis zu den Philippinen und Mexiko und mit der Art "Austrotaxus spicata" in Neukaledonien auch auf die Südhalbkugel. In China kommen vier Gattungen mit elf Arten vor.

Die Familie Taxaceae umfasst fünf Gattungen mit insgesamt 17 bis 20 Arten. Man unterscheidet zwei Tribus; unter anderem auf Grund der Morphologie des Arillus:

Die Abtrennung von "Austrotaxus" (Austrotaxaceae ), "Amentotaxus" (Amentotaxaceae ) und "Torreya" (Torreyaceae ) als jeweils eigene Familien hat sich nicht durchgesetzt. Der Umfang der beiden Familien Eibengewächse und Kopfeibengewächse (Cephalotaxaceae) wird diskutiert. Es gibt auch Autoren, bei denen beide Familien jeweils drei Gattungen enthalten. Auch eine Zusammenlegung beider Familien mit dann sechs Gattungen in den Taxaceae s. l. stand zur Diskussion. Eine phylogenetische Untersuchung bestätigt jedoch die hier beschriebene innere Systematik der Eibengewächse und den Status der Kopfeiben, verdeutlicht durch folgendes Kladogramm:



</doc>
<doc id="1417" url="https://de.wikipedia.org/wiki?curid=1417" title="Emulator">
Emulator

Als Emulator (von , „nachahmen“) wird in der Computertechnik ein System bezeichnet, das ein anderes in bestimmten Teilaspekten nachbildet.

Das nachgebildete System erhält die gleichen Daten, führt vergleichbare Programme aus und erzielt die möglichst gleichen Ergebnisse in Bezug auf bestimmte Fragestellungen wie das zu emulierende System.

Software-Emulatoren sind Programme, die einen Computer nachbilden und es so ermöglichen, Software für diesen Computer auf einem Computer mit einer anderen Architektur zu verwenden. So können z. B. Spiele für ältere Spielekonsolen auf einem PC oder einer neueren Spielekonsole ablaufen. Ein weiteres Beispiel: Ein Softwareentwickler kann bei der Entwicklung eines Programmes für ein Gerät (z. B. ein Handy), das eine andere Architektur als der Entwicklungs-Rechner hat, dieses im Emulator testen und korrigieren, ohne es jedes Mal auf das Gerät kopieren zu müssen.

Ein Hardware-Emulator ist ein elektronisches Gerät, das ein System wie einen Drucker oder einen Prozessor (CPU) funktionell, elektrisch oder mechanisch (Gehäuse und Pins) nachbilden kann. Die Verbindung zur Prozessorbaugruppe wird mittels Sockel und passendem Stecker erstellt. Er wird auch als "In-Circuit-Emulator" (ICE) bezeichnet.

Ein Terminalemulator ist eine Software, welche die Funktion eines Terminal (Dateneingabe/Bildschirmausgabe) nachbildet, so dass man z. B. von einem PC auf eine entsprechende Anwendung zugreifen kann.

Emulatoren gehören zu den Interpretern.

Im Jahre 1962 wurde erstmals ein (Prozessor-)Emulator eingesetzt. IBM arrangierte zahlreiche Tests in La Grande (Frankreich), um die Kompatibilität ihrer neuen Produkte zu den Vorgängern zu prüfen. Dazu nutzte man eine Kombination aus Hard- und Software, die vom IBM-Ingenieur Larry Moss als „Emulator“ bezeichnet wurde. 1965 schließlich wurde die System/360-Linie offiziell veröffentlicht. Sie umfasste auch den ersten Emulator – der „7070 Emulator“ erlaubt die Verwendung von Programmen für das ältere Modell IBM 7070.
1985 erschien der Atari ST mit einer für den Heimcomputermarkt neuen 68000-CPU und dem Betriebssystem Atari-TOS. Anfänglich gab es für diese Hardware noch wenig Anwendungs-Software. Der Übergang von der damals weit verbreiteten 8-Bit-Software unter CP/M zur neuen 16-Bit-Welt wurde von Atari durch den kostenlos mitgelieferten CPMZ80-Emulator erleichtert. Dieser reine Softwareemulator erzeugte auf der 68000-Hardware eine virtuelle, vollständige Z80-CPU sowie ein mit CP/M 2.2 kompatibles Betriebssystem. Dadurch war ein problemloser Betrieb populärer wie vorhandener Software möglich.
"MegaDrive" war 1994 der erste veröffentlichte Videospielemulator, welcher die gleichnamige Konsole nachbildete. Dieser unterstützte nur rudimentär das Spiel Sonic the Hedgehog; Die Entwicklung kam zum Erliegen, nachdem der Programmierer den Quelltext verloren hatte. Im selben Jahr wurde von Chris George die initiale, aber funktionsunfähige Version von „VSMC“ veröffentlicht, womit erstmals das Super Nintendo Entertainment System emuliert wurde.

Die heute wohl in der EDV häufigsten Emulationen sind Drucker- oder Plotter-emulationen. Fast alle hochwertigen Laserdrucker emulieren zurzeit einen Hewlett-Packard-LaserJet Drucker (HP-PCL), aber auch Rasterdrucker werden emuliert. Häufig sind auch nach wie vor die Emulationen Epson ESC/P, IBM-Proprinter und Andere.

Eine klassische Terminalemulation erlaubt die Interaktion mit textorientierten Programmen, die auf einem entfernten Rechner laufen, über eine externe Schnittstelle, meist eine serielle Leitung oder eine Modemverbindung. Heute hingegen sind Netzwerkverbindungen via TCP/IP die Regel. Terminalemulationen wurden programmiert, um das Verhalten eines „dummen“ Terminals, also eines einfachen Datensicht- und Eingabegerätes, nachzuahmen. Neben den textorientierten Terminalemulationen werden heute vermehrt Lösungen zur Remotebearbeitung mit graphischer Oberfläche (Citrix, MS-Remotedesktop, X-Terminal) eingesetzt. Durch diese graphischen Emulationen können beispielsweise Unix-Benutzer direkt von ihrem Arbeitsplatz aus Programme benutzen, die nur für Windows verfügbar sind (und umgekehrt). Auch die Administration erleichtert sich, da die wesentlichen Wartungs- und Installationsarbeiten nur an einem System, dem Terminal-Server, erfolgen.

Eine virtuelle Maschine (kurz: VM) wird oft fälschlicherweise ebenfalls als Emulator bezeichnet. Diese Spezialsoftware erzeugt auf einem Gastgeberrechner eine Laufzeitumgebung, die eigentliche virtuelle Maschine, die die Hardwareschnittstellen des Rechners (oder eines ähnlichen Rechners) abbildet. Ein Gastbetriebssystem läuft – wie üblich – auf der CPU des Gastgeberrechners, jedoch werden alle Zugriffe auf die Ein- und Ausgabehardware auf Softwareschnittstellen des Gastgeberbetriebssystems umgeleitet. Dadurch ist es möglich, unter dem vorhandenen Betriebssystem ein weiteres in einem Fenster auszuführen. Bei professionellen Anwendungen laufen unter einem Hypervisor, einer speziellen Form der VM, gar parallel mehrere Gastbetriebssysteme auf nur einer vorhandenen Hardware; faktisch wird dabei also ein einzelner Rechner in mehrere unterteilt.


Streng genommen ebenfalls vom Emulator zu unterscheiden ist die „Kompatibilitätsschicht“, die nicht versucht, ein ganzes System zu emulieren, sondern sich auf die Emulation von Softwareschnittstellen beschränkt. Ein bekanntes Beispiel ist Wine, das unter Unix-artigen Betriebssystemen eine Vielzahl der Softwareschnittstellen von Windows bereitstellt, so dass etliche Windows-Programme unter dem eigentlich fremden Betriebssystem lauffähig werden.

"Siehe auch:" Laufzeitumgebung und Programmierschnittstelle

Emulatoren werden für verschiedene Zwecke eingesetzt:

"Siehe auch:" Simulation, Bochs, DOSBox, MESS, PearPC, QEMU

Hardware-Emulatoren ermöglichen das Entwickeln von maschinennaher Software, da keine Emulations-Software der in Entwicklung befindlichen Software das Zielsystem „vorgaukelt“, sondern in der Regel eine besondere Hardware ermöglicht, dass die Software in einer „echten“ Umgebung läuft. Die Emulations-Hardware bietet zumeist Möglichkeiten, die Software anzuhalten, Haltebedingungen zu setzen etc. ohne das Laufzeitverhalten der Software zu verändern. Die meisten Möglichkeiten bietet in der Regel ein In-Circuit-Emulator, bei dem ein besonders ausgerüsteter Mikroprozessor in der echten Zielhardware zur Softwareentwicklung genutzt wird.






VMware Server, Microsoft Virtual Server und Windows Virtual PC (die Version für Microsoft Windows Systeme) sind alles gemischte Systeme, bei denen im Wesentlichen nur der Prozessor virtualisiert wird. Der Rest der PC-Plattform, wie z. B. Netzwerkkarte, BIOS usw. hingegen wird emuliert.

Unabhängig von der real installierten Hardware (z. B. NE2000) kann z. B. beim VMWare Server entweder eine 100BaseTX-PCI-Netzwerkkarte von AMD, alternativ eine 1000BaseTX-(Gigabit)-PCI-Netzwerkkarte von Intel oder eine virtuelle Karte mit VMWare-eigenen Treibern emuliert werden. Als BIOS wird immer eine Phoenix-Variante emuliert.

Die von Microsoft Virtual PC emulierte LAN-Karte basiert, ebenfalls unabhängig vom Chipsatz der Karte des Virtual-PC-Hosts, immer auf einem DEC/Intel-21*4*-(TULIP)-Chip. Genauso basiert die Soundkarte immer auf einem Sound Blaster 16.

Folgende Software – überwiegend Virtualisierungslösungen – wird fälschlicherweise oft für Emulations-Software gehalten:


Emulatoren existieren für fast jedes System. Beliebt sind Emulatoren für Heimcomputer, wie zum Beispiel der VICE für den Commodore 64 oder der UAE für den Amiga. Es existieren jedoch auch weitere unzählige Emulatoren für Computer, Handhelds, Arcade-Automaten und Spielkonsolen, siehe auch MESS

In letzter Zeit spielen Emulatoren auch in der Freeware-Szene eine bedeutende Rolle. So bietet etwa der Game Boy Advance durch seine relativ einfache Programmierbarkeit die Möglichkeit, Spiele und Anwendungen zu entwickeln, die dann auch auf einem Emulator genutzt werden können.

Für den Nutzer, der Emulatoren z. B. zum Ausführen von alten, kommerziellen Computerspielen einsetzt, ist problematisch, dass diese auch dann noch unter dem Schutze des Urheberrechts stehen, wenn es sie seit mehreren Jahren nicht mehr zu kaufen gibt.

Gegenüber der echten, ursprünglichen Hardware besitzen Spielkonsolen-Emulatoren einige Vorteile. Dazu zählen die exzellente Bildqualität, der digital verarbeitete und somit verlustlos aufnehmbare Ton. Weitere, die Benutzerfreundlichkeit der eigentlichen Systeme erweiternde Aspekte sind z. B. das Verbessern der Videoausgabe (z. B. Weichzeichnen und Filtern von Grafiken bei Konsolen wie Super Nintendo oder PlayStation, obwohl diese Systeme niemals diese Techniken unterstützten, geschweige denn berechnen könnten) oder das Verwenden von Savestates zum schnellen Speichern und Laden von Spielständen – jederzeit während der Laufzeit des Spieles.

Der größte Nachteil der Software-Emulation ist, dass sie eine hohe Rechenlast auf dem emulierenden System erzeugen. So können, selbst auf modernen Rechnern, zum Beispiel alte Spieleklassiker teilweise nicht flüssig laufen. Die Software-Entwicklung für solche Emulationen ist sehr aufwendig.

Ein weiterer Nachteil besteht darin, dass Spiele ohne Frame-Limiter zu schnell ablaufen können, wenn die Systemleistung ausreicht, das Spiel mit deutlich mehr Bildern pro Sekunde darzustellen als ursprünglich vorgesehen. Die meisten Emulatoren bieten jedoch die Möglichkeit, die emulierte Rechenleistung zu begrenzen.

Software älterer Computersysteme, besonders der Spielekonsolen, ist häufig nur in Form von ROM-Bausteinen verfügbar. Da sich ROMs relativ einfach auslesen lassen, arbeiten Emulatoren in der Regel problemlos mit sogenannten ROM-Dateien (oder auch "ROM Images"), die in verschiedenen Dateiformaten vorliegen. Ein Hindernis bei der freien Verwertung und Verteilung ist allerdings, dass ROM-Inhalte (Software, hier Spiele) in der Regel urheberrechtlich geschützt sind und manche sogar noch kommerziell genutzt werden. Manche Emulatoren können auch komprimierte Dateien (z. B. im Zip-Format) lesen, die mehrere Dateien enthalten können.

Entpackt verweisen verschiedene Dateiendungen auf bestimmte ROM-Formate, beispielsweise:


Ähnlich verhält es sich mit Kopien von Software, die auf Bändern oder Disketten ausgeliefert wurden. Auch hier sind "Tape Images" bzw. "Disk Images" für die Benutzung mit einem Emulator verbreitet.


Manche Emulatoren (zum Beispiel M.E.S.S.) können auch echte Töne von Cassetten als wav-Datei einlesen.

Ein beliebter Emulator ist MAME („Multiple Arcade Machine Emulator“), der tausende bekannte Arcade-Spiele wie Pac-Man emuliert.

Die Spiele-ROMs sind bei dem quelloffenen Emulator jedoch nicht enthalten.





</doc>
<doc id="1419" url="https://de.wikipedia.org/wiki?curid=1419" title="Endokrinologie">
Endokrinologie

Die Endokrinologie (von griech. ἔνδον "endon" ‚innen‘ und κρίνειν "krinein" ‚entscheiden‘, ‚abscheiden‘, ‚absondern‘, und -logie) ist die „Lehre von der Morphologie und Funktion der Drüsen mit innerer Sekretion (Endokrine Drüsen) und der Hormone“. Der Begriff wurde 1909 von N. Pende (1880–1970) erstmals benutzt.

Endokrin heißen Hormondrüsen, die ihr Produkt nach innen, direkt ins Blut abgeben und im Gegensatz zu exokrinen Drüsen (z. B. Speichel-, Talgdrüsen) keinen Ausführungsgang haben.

Die medizinische Endokrinologie ist ein Teilgebiet der Inneren Medizin, siehe auch Endokrine Chirurgie.

Die Endokrinologie beschäftigt sich unter anderem mit folgenden Krankheiten, sog. Endokrinopathien:





</doc>
<doc id="1420" url="https://de.wikipedia.org/wiki?curid=1420" title="Europäische Eibe">
Europäische Eibe

Die Europäische Eibe ("Taxus baccata"), auch Gemeine Eibe oder nur Eibe genannt, früher auch Bogenbaum, Eue, Eve, Ibe, If, Ifen, ist die einzige europäische Art in der Pflanzengattung der Eiben ("Taxus"). Sie ist die älteste (Tertiärrelikt) und schattenverträglichste Baumart Europas. Sie kann ein sehr hohes Alter erreichen. Bis auf den bei Reife durch Karotinoide lebhaft rot gefärbten Samenmantel, den Arillus, der becherartig den Samen umgibt und den Eibenpollen, sind alle Pflanzenteile der Europäischen Eibe stark giftig. In allen europäischen Ländern gehört die Europäische Eibe zu den geschützten Pflanzenarten. In Deutschland steht sie auf der Roten Liste der gefährdeten Arten (Gefährdungsklasse 3: gefährdet) und war im Jahre 1994 Baum des Jahres sowie Giftpflanze des Jahres 2011. In Österreich war sie im Jahr 2013 Baum des Jahres.

Oft wird der Rückgang der Eibe in Zusammenhang mit der Ausbreitung der Buche ("Fagus"), zu Beginn des Klimawechsels vor ca. 2000 Jahren in Verbindung gebracht. Allerdings kann die starke Ausbreitung der Buche nicht allein für das Verschwinden der Eibe verantwortlich sein, da man die Eibe oft auch in Buchenwäldern antrifft, wo sie im Unterstand der Buche wächst. Möglicherweise hat die Buche ihren Teil zum Verschwinden der Eibe beigetragen, ihre Gefährdung jedoch ist in einer jahrhundertelangen Übernutzung durch den Menschen begründet.

Das Holz der Eibe wurde seit jeher vom Menschen geschätzt, da es aufgrund des langsamen Wuchses des Baumes eine außergewöhnliche Härte und Zähigkeit aufweist. Vor ca. 2000 Jahren begann nicht nur die Buche sich auszubreiten, sondern mit dem Ende der Bronzezeit breitete sich auch der Mensch mehr und mehr in Europa aus. Die Nutzung von Eibenholz durch den Menschen geht aber noch weiter zurück. Den ältesten Nachweis für die Verwendung von Eibenholz als Werkzeug bildet die Lanzenspitze von Clacton-on-Sea aus der Holsteinwarmzeit vor etwa 300.000 Jahren. Aus der Eem-Warmzeit vor etwa 130.000 Jahren stammt die Lanze von Lehringen. Auch der berühmte „Ötzi“, die Gletschermumie, die 1991 in den Ötztaler Alpen gefunden wurde, lebte vor 5200 Jahren und trug einen Bogenstab von ca. 1,80 Meter Länge aus Eibenholz bei sich. Auch der Stiel seines Kupferbeiles war aus Eibenholz.

Während die Nutzung der Eiben in der Forstwirtschaft heute keine wirtschaftliche Bedeutung mehr hat, werden die schnittverträglichen Eiben seit der Renaissance häufig in der Gartengestaltung eingesetzt. Sie wurden und werden vor allem als immergrüne, geschnittene Hecken gepflanzt.

Die immergrüne Europäische Eibe ist in ihrer Gestalt eine sehr variable Art, die je nach Standortbedingungen als Baum oder Strauch wächst. An extremen Standorten wie etwa im Hochgebirge oder in Felswänden wächst sie sogar als Kriechstrauch.

Mit zunehmendem Alter verändert sich das Aussehen der Eibe. Junge Eiben besitzen meist schlanke Stämme mit einer regelmäßigen Beastung. Die Krone ist bei jungen Bäumen breit kegelförmig und entwickelt sich mit zunehmendem Alter des Baumes zu einer runden, eiförmigen oder kugeligen Form. Oft sind freistehende Eiben bis an den Boden beastet. Auch sind ältere Exemplare nicht selten mehrgipfelig und mehrstämmig.

Charakteristisch und auffällig ist die dünne grau- bis rotbraune Schuppenborke der Eibenstämme. Anfangs tragen die Stämme junger Eiben eine rötlichbraune glatte Rinde, die später zu einer graubraunen, sich in Schuppen ablösenden Borke wird. In Mitteleuropa erreichen nur sehr wenige Bäume Wuchshöhen über 15 Meter. Im Norden der Türkei wachsen allerdings monumentale Eiben, die Wuchshöhen von 20 Meter erreichen, und in den Mischwäldern des Kaukasus gibt es vereinzelt Eiben, die eine Wuchshöhe bis 32 Meter haben.

Junge Eiben weisen in der Regel einen Stamm mit einer deutlichen Hauptachse auf, während geschlechtsreife Eiben häufig mehrstämmig sind. In der Jugend wächst die Eibe extrem langsam. Bei ungünstigen Bedingungen verharrt sie in einer Höhe von 10 bis 50 Zentimetern und bildet eine Kleinkrone. Bei günstigsten Bedingungen dauert es mindestens 10–20 Jahre, bis sie aus dem Äser des Rehwildes herausgewachsen ist. Danach wächst sie bei guten Bedingungen bis zu 20 Zentimeter jährlich.

Ab einem Alter von ca. 90 Jahren kulminiert das Höhenwachstum der Eibe. Dagegen hören Dicken- und Kronenwachstum nie auf. So sind Stammdurchmesser von über einem Meter möglich. Auf Grund ihres hohen vegetativen Reproduktionsvermögens sind Wurzelschösslinge, Triebstämmlinge und die Bewurzelung von Ästen, die den Boden berühren, für die Europäische Eibe charakteristisch. Durch die Verwachsung einzelner Stämme können bis zu 1 Meter dicke Komplexstämme entstehen.

Ab einem Alter von etwa 250 Jahren setzt bei Eiben häufig eine Kernfäule im Stammesinneren ein, die im Laufe von Jahrhunderten zu einer fast vollständigen Aushöhlung des Baumes führen kann. Die Kernfäule macht eine genaue Altersbestimmung von alten Eiben fast unmöglich, da im Stammesinneren keine Jahresringe mehr vorhanden sind, an denen das Alter eines Baumes abgelesen werden könnte. Das Alter wird daher meistens geschätzt.

Charakteristisch für die Altersphase von Europäischen Eiben ist, dass der Baum trotz des ausgehöhlten Stammes zunächst eine vollentwickelte Baumkrone aufweist, bis der ausgehöhlte Stamm das Kronengewicht nicht mehr tragen kann und Teile des Baumes wegbrechen. Es verbleiben dann kreis- oder halbkreisförmig stehende Stammfragmente, die unter günstigen Umständen durch neue Triebe aus dem Baumstumpf oder dem Wurzelsystem ergänzt werden.

Alte Eiben haben zwei Strategien zur Verfügung, durch die sie einen von innen heraus wegfaulenden Stamm ersetzen können: Im hohlen Stammesinneren bilden sie gelegentlich Innenwurzeln aus, die sich zu einem neuen Stamm entwickeln können. Alternativ können stammbürtige Triebe außen am Primärstamm senkrecht emporwachsen, so dass sehr alte Eiben gelegentlich nur noch aus einem solchen Kranz stark verdickter und miteinander verwachsener Triebstämme bestehen.

Die weichen und biegsamen Eibennadeln haben eine linealische Form, die mitunter leicht sichelförmig gebogen ist. Sie stehen an den Leittrieben spiralförmig, während sie an den Seitenzweigen zweizeilig angeordnet sind. Eibennadeln sind zwischen 1,5 und 3,5 Zentimeter lang und zwischen 2 und 2,5 Millimeter breit und erreichen ein Alter von drei bis acht Jahren, bis der Baum sie abwirft.

Eibennadeln werden auch als dorsiventral bezeichnet, was bedeutet, dass sie eine deutlich unterscheidbare Ober- und Unterseite haben. Auf ihrer Oberseite sind sie glänzend dunkelgrün und haben einen erhobenen Mittelnerv, der zur Spitze hin ausläuft. An der Unterseite sind sie dagegen hell- oder olivgrün gefärbt. Während Eibennadeln auf der Oberseite keine Spaltöffnung aufweisen, befinden sich an der Unterseite zwei undeutliche, blassgrüne Stomabänder.

Eibennadeln besitzen mehrere auffällige Charakteristika. Sie haben keine durch Sklerenchym mechanisch verstärkte Unterhaut (Hypodermis) und es fehlen Harzkanäle.

Europäische Eiben haben ein sehr weitläufiges, tiefreichendes und dichtes Wurzelsystem. Die Entwicklung dieses Wurzelsystems hat dabei beim Heranwachsen des Baumes Priorität vor dem Dicken- und Höhenwachstum. Europäische Eiben vermögen dabei auch in stark verdichtete Böden vorzudringen.

Das im Vergleich mit anderen Baumarten stark entwickelte Wurzelsystem ermöglicht auch die hohe Regenerationsfähigkeit des Baumes, bei der selbst nach einem kompletten Stammverlust noch Wurzelschösslinge nachwachsen. Mit ihrem vielfältigen und flexiblen Wurzelsystem ist die Eibe unempfindlich gegen Wechselfeuchte, zeitweilige Vernässung und Luftarmut im Boden. Dies zeigt ihre hohe individuelle Anpassungsfähigkeit an unterschiedliche Standorte und Lebensbedingungen.

In Felsregionen ist die Europäische Eibe in der Lage, mit ihren Wurzeln in wasserführende Senken und Klüfte einzudringen, während sie sich an nackte Felsen klammert.

Die Eibe hat eine Pilzwurzel vom Typ einer VA-Mykorrhiza, daher gehören Eiben zu den wenigen Waldbäumen, deren Wurzeln keine Symbiose mit Fruchtkörper bildenden Mykorrhiza-Pilzen eingehen.

Unter optimalen Standortbedingungen tragen Eiben das erste Mal weibliche Zapfen, wenn sie ein Lebensalter von 15 bis 30 Jahren erreicht haben. Unter weniger guten Standortbedingungen kann sich die Geschlechtsreife deutlich hinauszögern. In dichten Baumbeständen stehende Eiben, die kein ausreichendes Licht erhalten, erreichen ihre Geschlechtsreife mitunter erst mit 70 bis 120 Jahren. Die Anlage der Zapfen erfolgt bereits im Spätsommer. Die Blütezeit liegt im Spätwinter oder im frühen Frühjahr des nächsten Jahres, im Normalfall zwischen Februar und März, in kälteren Regionen erst zwischen April und Mai.

Die Europäische Eibe ist normalerweise zweihäusig (diözisch): männliche und weibliche Zapfen befinden sich auf unterschiedlichen Bäumen. Ausnahmefälle sind einhäusige (monözische) Exemplare, bei denen sich Zapfen beider Geschlechter an einem Baum befinden. Meist weist nur ein einzelner Ast Blüten mit einem anderen Geschlecht auf.

Die zahlreichen männlichen Zapfen stehen an 1 bis 2 mm langen, blattachselständigen Trieben. Sie haben eine kugelige Form mit einem Durchmesser von etwa 4 mm und enthalten 6 bis 14 schildförmige Staubblätter, die jeweils 6 bis 8 gelbliche Pollensäcke tragen. Wenn sich die Pollensäcke durch Wärme öffnen, werden die Pollenkörner bereits durch geringe Windbewegungen fortgetragen. Obwohl die Pollenkörner der Europäischen Eibe keine Luftsäcke aufweisen, ist wegen ihres geringen Gewichtes ihre Sinkgeschwindigkeit mit 1,6 cm pro Sekunde so gering, dass sie durch Luftbewegungen sehr weit fortgetragen werden können. Die frühe Blütenzeit, die in einen Zeitraum fällt, in dem Laubbäume in der Regel noch keine Blätter tragen, stellt sicher, dass dieser Pollenflug weitgehend ungehindert stattfinden kann, selbst wenn die jeweilige Eibe von Laubbäumen überdacht ist.

Die weiblichen Zapfen sind nur 1 bis 1,5 mm groß, stehen jeweils als Kurztriebe in den Blattachseln jüngerer Zweige und sind auf Grund ihrer grünlichen Farbe unscheinbar. Sie bestehen aus sich überlappenden Schuppen, von denen nur die oberste fruchtbar ist und nur eine Samenanlage trägt. Zur Blütezeit bildet sich an der Spitze des umhüllenden Deckblattes ein Bestäubungstropfen aus. Dieser nimmt die anfliegenden Pollenkörner auf und bringt, wenn er verdunstet ist, die Pollenkörner an den Nucellus, sodass die Zapfen bestäubt werden. An der Basis der Samenanlage findet sich ein ringförmiger Wulst, der sich bei befruchteten Blüten zu einem fleischigen, schleimigen Samenmantel, dem Arillus, auswächst. Dieser umgibt den Samen becherförmig, seine Farbe wandelt sich mit zunehmender Reife von Grün zu einem auffallenden Rot. Aufgrund des Arillus wird der Eibensamen oft fälschlicherweise als Frucht oder sogar Beere bezeichnet. Dies ist botanisch nicht korrekt, da es bei den Nacktsamigen Pflanzen keinen Fruchtknoten gibt, der zur Fruchtentwicklung erforderlich wäre. Der rote Samenmantel ist essbar und ungiftig, nur die Samen sind giftig. Die Blütenknospen werden im Laufe der zweiten Sommerhälfte ausgebildet.

Der bläulich-braune und eiförmige Samen ist 6 bis 7 mm lang und 3 bis 5 mm breit. Das Gewicht des Samens liegt zwischen 43 und 77 mg. Die Ausbildung des Samenmantels haben Europäische Eiben mit den anderen Arten aus der Familie der Eibengewächse gemeinsam. Die Samen reifen von August bis Oktober und keimen erst im zweiten Frühjahr. Die Samenverbreitung erfolgt durch Vögel, die vom süßen Arillus angelockt werden. Der Arillus wird verdaut und der Samen passiert unbeschadet den Verdauungstrakt. Auf diese Weise sorgen Vögel für die Ausbreitung der Eibensamen.

Für die generative Vermehrung durch Aussaat werden die Samen gesammelt, sobald sich der Arillus rot und der Samen braun verfärben. Der Samenmantel wird mit einem Wasserstrahl entfernt und die Samen dann bis zum nächsten Herbst gelagert. Der Keimerfolg ist größer 50 %, wenn die Samen vor der Aussaat stratifiziert werden, das heißt einer mehrmonatigen Wärme- und Kältebehandlung, die den Wechsel der Jahreszeiten nachahmt, unterzogen werden. 

Die Chromosomenzahl der Art ist 2n = 24.

Die "Taxaceae" (Eibenartige) werden den "Gymnospermen" (Nacktsamern) und innerhalb dieser den Koniferen (Nadelbäumen) zugeordnet. Interessant ist das Fehlen der für diese Gruppe typischen Zapfen bei den Eiben; der fleischige Arillus (fälschlich umgangssprachlich „Beere“) entsteht aus dem Stiel der Samenanlage. Zur Familie der Eibengewächse gehören insgesamt fünf Gattungen ("Amentotaxus", "Austrotaxus", "Pseudotaxus", "Torreya", "Taxus"), die alle Samen mit einem Arillus bilden. Die Gattung "Taxus" gilt als taxonomisch schwierige Gruppe, die verschiedenen Arten besitzen meist aneinandergrenzende, aber nicht überlappende Areale (parapatrische Verbreitung), sind aber morphologisch nur schwierig auseinanderzuhalten. Dies gilt insbesondere für die Vorkommen im Himalaya und China. Hier wurden von verschiedenen Taxonomen schon zwischen 2 und 24 Arten unterschieden. Durch neuere Untersuchungen wurde klar, dass die von früheren Botanikern angegebenen Vorkommen von "Taxus baccata" im West-Himalaya nicht dieser Art zugehören, sondern eine eigene Art bilden, "Taxus contorta" (syn. "Taxus fuana" Nan Li & R.R.Mill) Diese bildet, nach molekularen Daten (Vergleich homologer DNA-Sequenzen) die Schwesterart von "Taxus baccata". Den genetischen Daten zufolge ist die Europäische Eibe, trotz des großen Verbreitungsgebiets, eine monophyletische Einheit und die einzige in Europa heimische Art.

Das Vorkommen von "Taxus baccata" L. beschränkt sich nicht auf Europa, ihr Verbreitungsgebiet reicht von den Azoren, dem Atlasgebirge in Nordwestafrika über Europa, Kleinasien bis in den Kaukasus und den Nordiran. Im Norden verläuft die Verbreitungsgrenze von den Britischen Inseln über Norwegen bis nach Schweden und Finnland. Die östliche Verbreitung reicht von Lettland, entlang der russisch-polnischen Grenze, bis zu den östlichen Karpaten und endet im Norden der Türkei. Im Süden verläuft die Verbreitungsgrenze südlich von Spanien, über Teile Marokkos und Algeriens, bis zur Südtürkei und von dort bis ins Landesinnere des Nordirans.

In Europa ist das Verbreitungsgebiet nicht zusammenhängend, sondern zerfällt in mehrere Teilareale und ist stark zerrissen. Oft kommt die Eibe nur noch in kleinen Beständen oder als Einzelbaum vor. Die Ursache dieser Disjunktion (Zerrissenheit) ist mit großer Wahrscheinlichkeit die anthropogene Übernutzung der Eibenbestände in früherer Zeit.

Natürliche Eibenvorkommen existieren vor allem in Nordportugal, Spanien, der Bretagne und der Normandie im Norden Frankreichs, auf den Britischen Inseln, im südlichen Skandinavien, im Baltikum, den Karpaten, auf der nördlichen Balkanhalbinsel, in Nord- und Mittelitalien, Korsika und Sardinien. Sie fehlt dagegen unter anderem in Dänemark, im nördlichen Belgien und Holland sowie entlang der unteren und mittleren Elbe und Saale. Sie fehlt auch im Landesinneren von Polen, während sie in der Küstenregion der Ostsee vorkommt.

Das Verbreitungsgebiet der Europäischen Eibe wird wesentlich durch ihre geringe Frosthärte bestimmt. Ihre Nordgrenze verläuft bei 62 Grad 30 Minuten N in Norwegen und 61 Grad N in Schweden etwa auf der Januar-Isotherme von −5 Grad Celsius. Sie gedeiht vor allem dort, wo sich das Klima durch milde Winter, kühle Sommer, viel Regen und eine hohe Luftfeuchtigkeit auszeichnet. In den Bayerischen Alpen kommt sie bis in eine Höhe von vor, im Wallis bis in eine Höhe von .

Die Europäische Eibe wird in der Roten Liste der IUCN als „nicht gefährdet“ (least concern) mit einem „ansteigenden“ Trend (increasing) gelistet.

In Deutschland wird die Eibe in der Roten Liste als „gefährdet“ (Stufe 3) geführt, wobei als Hauptgefährdungsgründe Altersklassenbewirtschaftung mit Kahlschlagbetrieb und Wildverbiss angegeben werden. Seit dem Inkrafttreten der Bundesartenschutzverordnung (1. Januar 1987) stehen wild lebende Populationen der Eibe unter "besonderem Schutz". Im Auftrag der Bundesanstalt für Landwirtschaft und Ernährung (BLE) wurden im Rahmen des Projekts "Erfassung und Dokumentation genetischer Ressourcen seltener Baumarten in Deutschland" in den Jahren 2010 bis 2012 die Vorkommen der Europäischen Eibe in den deutschen Wäldern erfasst. Es wurden insgesamt 342 Eibenvorkommen mit zusammen 60.045 Bäumen aufgenommen. Die eibenreichsten Bundesländer waren Thüringen mit 33.200 Eiben und Bayern mit 14.700 Eiben. Die Verbreitungsschwerpunkte liegen im Mitteldeutschen Trias-Berg- und Hügelland, in der Schwäbischen Alb, in der Frankenalb und im Oberpfälzer Jura sowie in der Schwäbisch-Bayerischen Jungmoräne.

In der Schweiz wird die Europäische Eibe in der Roten Liste des Bundesamtes für Umwelt (BAFU) als „nicht gefährdet“ eingestuft. Sie ist aber regional (kantonal) geschützt.

Die Eibe ist standortvage, d. h. sie gedeiht auf feuchten, wechselfeuchten und sehr trockenen, sowie auf sauren und basischen Standorten. Das Ökogramm der Eibe zeigt die sehr große physiologische Amplitude dieser Baumart, die im trockenen Bereich sogar über waldfähige Standorte hinausgeht und selbst wechselnde Bedingungen erträgt. Die Eibe kommt oft auf frischem, humosem oder sandigem Lehm vor, sie gedeiht jedoch ebenso auf wechselfeuchten und sogar sandigen Standorten. Wie bei allen anderen Baumarten ist jedoch das Wachstum der Eibe auf gut durchwurzelbaren und nährstoffreichen Böden begünstigt. Sie kommt auf kalkhaltigen Standorten, Silikatgesteinsböden sowie auf organischen Substraten gleichermaßen vor. Die Europäische Eibe bevorzugt frische, nährstoffreiche, oft basische Böden in ozeanischer, feuchter Klimalage. Ihr Niederschlagsoptimum liegt bei über 1000 mm/Jahr. Ihren Wasserbedarf vermag sie aber auch aus nassen oder moorigen Sonderstandorten in generell niederschlagsärmeren Gebieten zu decken. Selbst in Flussauen ist sie zu finden, was auf eine Toleranz gegen Sauerstoffmangel im Boden hindeutet.

Die Europäische Eibe ist die schattenverträglichste Baumart Europas. Bei einer Temperatur von 20 Grad kann sie bei einer Beleuchtungsstärke von 300 Lux noch überleben. Junge Eiben sind obligate Schattenpflanzen, das heißt, sie gedeihen nur im Schatten, vor allem im schattigen Unterstand unter anderen Bäumen. Herangewachsene Eiben vertragen dagegen auch volle Sonne. Während Europäische Eiben in Wäldern mit einem völlig geschlossenen, immergrünen Kronendach, wie es für einen reinen Fichtenbestand typisch ist, nicht gedeihen, reichen ihnen noch fünf Prozent der Lichtmenge des Freilandes, um erfolgreich Blüten und Samen zu bilden. Am besten gedeihen sie in lichten Mischwaldbeständen, vor allem in Eichen-, Buchen-, Tannen- und Edellaubholzmischwäldern, aber nur wenn der Wildbestand so niedrig ist, dass nachwachsende Jungpflanzen nicht sofort verbissen werden. Dabei stellen sie beispielsweise in den Karpaten 12,4 Prozent der Stammzahl, 13,5 Prozent der Grundfläche und 4 Prozent des Holzvorrates. Die Europäische Eibe zählt dabei zu den sogenannten Klimaxwald-Baumarten, das heißt, sie kann sich in einer Pflanzengemeinschaft, die sich am Ende einer Sukzessionsfolge entwickelt hat, erfolgreich natürlich verjüngen. Reine Eibenbestände sind dagegen selten. Sie entstehen meist, weil das hohe Lebensalter, das Eiben erreichen können, sie die anderen Baumarten überdauern lässt, in deren Schatten sie zuvor wuchsen.

Europäische Eiben finden sich heute wegen früherer Übernutzung, gezielter Ausrottung und Wildverbiss oft nur noch in unzugänglichen Schluchtwäldern und an Steilhängen. Sie wurden häufig als „Unholz“ und Pferde- und Hühnergift bekämpft. Weitere Gründe für die Seltenheit der Eibe sind die Umstellung der Forstwirtschaft von plenterartigen Eingriffen zur schlagweisen Wirtschaft, die die langsamwachsende, gegen plötzliche Freistellung empfindliche Eibe benachteiligt. Ein hoher Wildbestand behindert wegen Verbisses gleichfalls eine natürliche Bestandsverjüngung. Ihre letzten Rückzugsorte sind vielfach schattige und steile, auch vom Wild gemiedene Berghänge, die aber wasserzügig sein müssen.

Das Regenerationsvermögen der Eibe ist im Vergleich zu allen anderen heimischen Nadelbäumen am stärksten ausgeprägt. Die hohe Regenerationsfähigkeit der Eibe zeigt sich einerseits darin, dass sie als einzige Nadelbaumart die Fähigkeit besitzt, aus dem Stock auszuschlagen. Andererseits schafft sie es durch ihre sehr gute Wundheilung (Wundüberwallung), auch große Schäden zu überstehen. Bis ins hohe Alter ist die Eibe in der Lage, durch die Bildung von Reiterationen auf mechanische, aber auch Frost- oder Sonnenbrandschäden zu reagieren. Diese Wiederholungstriebe dienen der Erneuerung der Krone und verschaffen Bäumen die Möglichkeit, alternde Äste zu ersetzen. Eine weitere Überlebensstrategie ist die vegetative Vermehrung. Diese ungeschlechtliche Vermehrung beruht auf der mitotischen Zellteilung. Die Tochtergeneration unterscheidet sich in ihrem genetischen Material daher nicht von der Muttergeneration; sie ist ein Klon. Das hohe vegetative Reproduktionsvermögen zeigt sich durch folgende Fähigkeiten: Durch die Bildung von Astsenkern können sowohl zusätzliche Nährstoffe aufgenommen werden, als auch eine vollständige Verjüngung eines einzelnen Eibenbaumes stattfinden. Bei umgestürzten Bäumen treiben sofort senkrechte Äste aus. Astteile, die mit dem Boden in Berührung kommen, beginnen Wurzeln auszuschlagen.

Obwohl die Nadeln der Eibe weder über sklerenchymatische Verstärkungen noch schützende Wachstropfen in den Spaltöffnungen verfügen, gilt sie als ausgesprochen dürreresistent. So kann die Eibe ähnlich hohe relative Wasserverluste ertragen wie die Gemeine Kiefer ("Pinus sylvestris"). Beide besitzen ähnlich hohe absolute Wasserreserven (auf gleiches Gewicht bezogen) wie krautige, saftreiche Pflanzen, obwohl ihre Wasserkapazität (Wassergehalt bei Sättigung) vergleichsweise gering ist. Dadurch hat die Eibe in Relation zum Trockengewicht die Möglichkeit, weit höhere Wasserverluste, sogar bis zu 45 % ihres Gewichtes, zu überstehen. Eine weitere Fähigkeit, die die Eibe vor dem Austrocknen schützt, ist das schnelle Schließen der Stomata. So stellen vergleichende Untersuchungen von Tannen- und Eibennadeln fest, dass die Eibe auf ein Wassersättigungsdefizit viermal schneller mit einem Stomataverschluss reagiert als die Tanne ("Abies").

Die Winterüberdauerungsstrategie der Eibe beruht auf zwei Komponenten. Zum einen wird die Transpiration im Vergleich zum Sommer auf ein Fünftel bis zu einem Zwanzigstel eingeschränkt. Die Einschränkung ist umso höher, je kälter die Umgebungstemperatur ist. Zum anderen hebt die Eibe die Zellsaftkonzentration an. Dadurch kommt es zu einer Absenkung des Gefrierpunktes. Gemeinsam mit dem Gefrierpunkt verringert sich auch das Temperaturminimum für die Nettoassimilation von ca. −3 °C auf ca. −8 °C. Solange die Eibe ausreichend Vorbereitungszeit auf die Kälteeinwirkung hat, um ihre Zellsaftkonzentration entsprechend zu steigern, kommt es nur bei sehr tiefen Temperaturen von unter −20 °C zu Gefrierschäden. Wesentlich häufiger kommt es zu Schäden durch Frosttrocknis, die unter anderem auf den relativ schlechten Transpirationsschutz der Eibennadeln zurückzuführen sind. Diese Vertrocknungsschäden kommen aber meistens nur bei exponierten, freistehenden Bäumen vor. Des Weiteren ist die Eibe unempfindlich gegen Spätfröste. Sie erreicht das dadurch, dass die im Laufe des Winters erhöhte Zellsaftkonzentration nur langsam abgebaut wird. Dadurch bleibt diese winterliche Abhärtung lange in die Vegetationszeit hinein bestehen. Die Normalwerte des Vorjahres werden erst im Juni wieder erreicht.
Die Eibe gilt als ausgesprochen schattentoleranter Baum. Sie ist in der Lage, auch völlig überschirmt im Nebenbestand zu überleben. Sie verträgt im Vergleich zu den klassischen Schattenbaumarten wie Tanne und Buche deutlich mehr Beschattung. Wie beim Auftreten eines Wasserdefizites, schließen sich die Stomata auch bei Verdunkelung schnell. Sie öffnen sich erst nach der Überschreitung des Lichtkompensationspunktes. Allerdings kann die Eibe schon bei geringer Lichtintensität eine positive Nettoassimilation erreichen. Der Lichtkompensationspunkt, also der Punkt bei dem gerade noch eine positive Nettoassimilation möglich ist, beträgt bei der Eibe bei einer Temperatur von 20 °C etwa 300 Lux. Im Vergleich dazu kommen andere schattenertragenden Baumarten wie die Buche auf 300–500 Lux und die Tanne ("Abies") auf 300–600 Lux. Eine typische Lichtbaumart wie die Weißkiefer ("Pinus sylvestris") benötigt hingegen Werte von 1000 bis 5000 Lux zum Überschreiten des Lichtkompensationspunktes.

Holz, Rinde, Nadeln und Samen enthalten toxische Verbindungen, die in ihrer Gesamtheit als Taxane oder Taxan-Derivate (Diterpene) bezeichnet werden. Im Einzelnen lassen sich unter anderem Taxin A, B, C sowie Baccatine und Taxole nachweisen. Der Gehalt an toxischen Verbindungen ist in den unterschiedlichen Baumteilen verschieden hoch und schwankt in Abhängigkeit von der Jahreszeit und individuellem Baum. Der Samenmantel des Baumes ist hingegen nicht giftig und schmeckt süß.

Die toxischen Verbindungen werden beim Menschen und anderen Säugetieren rasch im Verdauungstrakt aufgenommen. Vergiftungserscheinungen können beim Menschen bereits 30 Minuten nach der Einnahme auftreten. Die toxischen Verbindungen wirken dabei schädigend auf die Verdauungsorgane, das Nervensystem und die Leber sowie die Herzmuskulatur. Zu den Symptomen einer Vergiftung zählt eine Beschleunigung des Pulses, Erweiterung der Pupillen, Erbrechen, Schwindel und Kreislaufschwäche, Bewusstlosigkeit. Bereits eine Aufnahme von 50 bis 100 Gramm Eibennadeln kann für den Menschen tödlich sein. Der Tod tritt durch Atemlähmung und Herzversagen ein. Menschen, die eine solche Vergiftung überleben, tragen in der Regel einen bleibenden Leberschaden davon.

Pferde, Esel, Rinder sowie Schafe und Ziegen reagieren in unterschiedlichem Maße empfindlich auf die in Eiben enthaltenen toxischen Verbindungen. Pferde gelten als besonders gefährdet – bei ihnen soll schon der Verzehr von 100 bis 200 Gramm Eibennadeln zum Tode führen. Bei Rindern treten Vergiftungserscheinungen bei etwa 500 Gramm auf. Gefährdet sind Weidetiere vor allem dann, wenn sie plötzlich größere Mengen aufnehmen. Dagegen scheinen zumindest Rinder, Schafe und Ziegen eine Toleranz gegen die Toxine der Europäischen Eibe zu entwickeln, wenn sie daran gewöhnt sind, regelmäßig kleinere Mengen davon zu fressen. Bei Kaninchen sollen bereits weniger als 2 Gramm der Nadeln zum Tode führen. Unempfindlich gegenüber den Giften der Eiben und deshalb Verursacher von Schäden durch Wildverbiss sind Rehe und Rothirsche.

Als Heildroge dienen die frischen Zweigspitzen, Taxus baccata (HAB)

Wirkstoffe sind:
Diterpen-Alkaloide vom Taxan-Typ, Baccatin III (das Gemisch wurde als „Taxin“ bezeichnet), cyanogene Glykoside, wie Taxiphyllin, Biflavonoide, wie Sciadopitysin und Ginkgetin.

Anwendungen:
Die arzneiliche Anwendung von Eibennadeln in der Volksheilkunde z. B. bei Wurmbefall, als Herzmittel oder zur Förderung der Menstruation, auch als Abtreibungsmittel, war wegen der Giftigkeit risikoreich und gehört inzwischen der Vergangenheit an.

Heute nutzt man noch homöopathische Zubereitungen; zu den Anwendungsgebieten gehören: Verdauungsschwäche und Hautpusteln.

Seit den 1990er-Jahren genießt die Art wieder hohe Wertschätzung, nachdem es gelungen war, die zellteilungshemmende Substanz Paclitaxel, die man bisher nur aus der Rinde der Pazifischen Eibe, Taxus brevifolia, isolieren konnte, teilsynthetisch aus den Taxan-Verbindungen der Nadeln, speziell dem Baccatin III darzustellen sowie später eine weitere Substanz, das Docetaxel.

Sie sind derzeit zur Behandlung von metastasierendem Brust- und Eierstockkrebs sowie von bestimmten Bronchialkarzinomen zugelassen, wegen der schweren Nebenwirkungen jedoch erst nach Versagen anderer Therapien.

Typische Begleitbaumarten der Europäischen Eibe sind in Mitteleuropa Eiche, Hainbuche, Esche, Ulme, Linde, Tanne, sowie der Bergahorn. Dabei findet sie ihr Optimum in Laubwäldern mit tiefgründigen, frischen, nährstoffreichen Böden, etwa in niederschlagsreichen Tannen-Buchen- oder in Stieleichen-Auenwäldern. Im trockenen Klima der Mittelmeerländer wächst sie in der Gesellschaft mediterraner Eichenarten wie der Steineiche, oder der Platanen.

Im offenen Kulturland wachsen Europäische Eiben oft zwischen dornigen Heckengebüschen wie Schlehe oder Heckenrose heran, welche die jungen Pflanzen vor dem Verbiss durch Wild- und Weidetiere schützen.

Besteht die Krautschicht in Eiben-Mischwäldern neben Farnen und Moosen häufig aus Bingelkraut, Walderdbeere, Gundermann, Efeu, Brombeere und Veilchen, sind in Eiben-Buchenwäldern eher Einblütiges Perlgras, Waldmeister, oder Kalk-Blaugras anzutreffen. Wo die Europäische Eibe vor allem mit Eichen vergesellschaftet ist, finden sich in der Krautschicht oft auch Schlüsselblume und Pfirsichblättrige Glockenblume.

Bei Vogelarten, die die Europäische Eibe als Nahrungspflanze nutzen, wird zwischen Samenverbreitern, die nur an dem süßen Arillus interessiert sind und den Samen wieder ausscheiden, sowie Samenfressern unterschieden. Zu den Samenverbreitern zählen vor allem Star, Singdrossel, Amsel und Misteldrossel sowie Wacholder-, Rot- und Ringdrossel. Misteldrosseln zeigen dabei ein territoriales Verhalten und verteidigen ab Spätsommer „ihre“ Eibe gegen andere Vögel, so dass von Misteldrosseln besetzte Eiben noch bis Januar und Februar rote Samenbecher aufweisen. Dieses Verhalten trifft auch auf Singdrosseln zu. Diese zeigen jedoch eine weniger große Verteidigungsbereitschaft als Misteldrosseln. Arillen werden außerdem vom Sperling, Gartenrotschwanz und der Mönchsgrasmücke sowie Eichel- und Tannenhäher, Seidenschwanz und Jagdfasan verzehrt. Alle diese Vogelarten sind maßgeblich an der Verbreitung der Europäischen Eibe beteiligt und sorgen dafür, dass Eibenschösslinge auch weit entfernt von etablierten Eibenbeständen und an unzugänglichen Stellen wie etwa steilen Felshängen wachsen.

Zu den Samenfressern zählen vor allem der Grünfink sowie in geringerem Maße Dompfaff, Kohlmeise, Kernbeißer, Kleiber, Grünspecht, Buntspecht und gelegentlich auch die Sumpfmeise. Kleiber reiben den Samenmantel an Baumrinden ab, bevor sie wie die Spechte das Samenkorn in Ritzen verkeilen, um es aufzuhämmern. Der Grünfink löst dagegen den Arillus mit dem Schnabel, entfernt die glykosidhaltige Samenhülle und frisst dann das Sameninnere.

Bilche wie Sieben- und Baumschläfer klettern in Eiben, um an die roten Arillen zu gelangen. In der Regel fressen Säugetiere jedoch die Samenbecher, die auf den Erdboden gefallen sind. Kleinnager wie Rötel-, Wald- und Gelbhalsmaus gehören zu den Arten, die sich unter anderem daran gütlich tun. Ihre Anwesenheit zieht Raubsäuger wie Rotfuchs und Wiesel und Iltisse an. Rotfüchse fressen allerdings ebenso wie Dachse, Braunbären und Wildschweine gerne die Arillen und auch für Baummarder ist dies schon beschrieben worden.

Kaninchen und Feldhasen verbeißen junge Eibenkeimlinge und behindern so ein Höhen- und Breitenwachstum junger Bäume. Weit größerer Äsungsdruck geht jedoch von Rotwild aus, das ähnlich wie Kaninchen und Hasen unempfindlich für die in der Eibe enthaltenen toxischen Verbindungen ist. Insbesondere ein hoher Bestand an Rehen verhindert die natürliche Verjüngung des Eibenbestandes: Junge Schösslinge reißen sie beim Weiden mit den Wurzeln aus. Die Zweige von Eibenbäumen werden bis zu einer Höhe von etwa 1,4 Metern abgefressen. Auch Ziegen und Schafe weiden an Eibenbäumen. Als ein nennenswerter Eibenschädling hat sich auch das aus Nordamerika nach Europa eingeführte Graue Eichhörnchen erwiesen. Es schält die Rinde auch älterer Eiben ab, sodass die Bäume durch Wundinfektionen gefährdet sind.

Auf Europäischen Eiben finden sich, im Vergleich zu anderen europäischen Baumarten, nur verhältnismäßig wenig Wirbellose. Zu den wichtigsten zählt die Eibengallmücke ("Taxomyia taxi"), deren Larven sich in den Knospen der Triebspitzen einnisten und die dort mitunter zu einer Überproduktion von Eibennadeln führt, sodass sich eine an Artischocken erinnernde Galle bildet. Zwei parasitäre Wespen, nämlich "Mesopolobus diffinis" und "Torymus nigritarsus", wiederum legen ihre Eier in die Gallen beziehungsweise in die vollentwickelten Larven und Puppen der Eibengallmücke. Die Schmetterlingsraupen "Ditula angustiorana " (Wickler) und "Blastobasis vittata" (Blastobasidae) fressen unter anderem Eibenlaub. Im Splintholz der Eiben sind mitunter die Larven des Hausbocks ("Hylotrupes bajulus") sowie des Gescheckten Nagekäfers ("Xestobium rufovillosum") zu finden. Der zu den Rüsselkäfern zählende Gefurchte Dickmaulrüssler ("Otiorhynchus sulcatus") schädigt einjährige Eibentriebe sowie Wurzeln junger Sämlinge und ihre Wipfeltriebe. Ebenfalls anzutreffen ist mitunter die gelblich bis braun gefärbte Eiben-Napfschildlaus ("Eulecanium cornicrudum"), die an jungen Trieben saugt.

Die Europäische Eibe ist ein Kernholzbaum. Kernholz bezeichnet die im Stammquerschnitt physiologisch nicht mehr aktive, dunkle, innere Zone, die sich deutlich vom äußeren, hellen Splintholz unterscheidet. Der schmale Splint ist gelblich-weiß und etwa zehn bis zwanzig Jahresringe stark. Das Kernholz weist eine rötlichbraune Farbe auf. Das wegen des langsamen Wachstums feinringige Holz ist sehr dauerhaft, dicht, hart und elastisch. Die Dauerhaftigkeit des Kernholzes resultiert aus der Einlagerung von Gerbstoffen, welche das Holz imprägnieren. Eibenholz ist, trotz der Dauerhaftigkeit, von dem Gemeinen Nagekäfer angreifbar. Ein Kubikmeter Eibenholz wiegt zwischen 640 und 800 Kilogramm. Im Vergleich dazu wiegt ein Kubikmeter Holz des Mammutbaums 420, der Kiefer 510 und der Buche und Eiche jeweils 720 Kilogramm. Eibenholz trocknet sehr gut, schwindet dabei nur mäßig und lässt sich leicht verarbeiten. Die Europäische Eibe hat heute allerdings keine wesentliche forstwirtschaftliche Bedeutung mehr. Das im Holzhandel nur selten angebotene Holz wird für Furnierarbeiten sowie für Holzschnitzereien und Kunstdrechslerei sowie zum Bau von Musikinstrumenten verwendet.

In der Geschichte der Menschheit hat Eibenholz eine wesentlich größere Bedeutung gehabt, als dem Holz heute beigemessen wird. Das harte und elastische Holz ist besonders für den Bau von Bögen und Speeren geeignet: Bei den nach den "Schöninger Speeren" ältesten bekannten hölzernen Artefakten handelt es sich um zwei Speere, die jeweils aus Eibenholz gefertigt sind. Der ältere Speer wurde in der Nähe von Clacton-on-Sea, Essex gefunden und wird auf ein Alter von 150.000 Jahren datiert. Der zweite Fund stammt aus dem niedersächsischen Lehringen, wo im Brustkorb eines in einer Mergelgrube konservierten Waldelefantenskeletts eine 2,38 m lange Eibenholzlanze gefunden wurde, die den mittelpaläolithischen Neandertalern zugeschrieben und auf ein Alter von 90.000 Jahren geschätzt wird. Zwischen 8000 und 5000 Jahre alt sind acht Eibenbögen, die in verschiedenen Ausgrabungsorten in Norddeutschland gefunden wurden. Ein ebenfalls sehr gut erhaltener und 183 Zentimeter langer Eibenbogen wurde 1991 bei der Ötztaler Gletschermumie gefunden. Auch dieser Bogen ist 5000 Jahre alt.

Jungsteinzeitliche Funde weisen die Verwendung von Eibenholz für die Herstellung von Gebrauchsgegenständen wie Löffeln, Tellern, Schalen, Nadeln und Ahlen nach. Drei bronzezeitliche Schiffe, die in der Mündung des Flusses Humber in Yorkshire gefunden wurden, bestehen aus Eichenplanken, die mit Eibenholzfasern miteinander verbunden waren. Auch die Reste bronzezeitlicher Pfahlbauten z. B. am Mondsee zeugen von dieser frühen Wertschätzung des Eibenholzes, das äußerst feuchtigkeitsbeständig ist.

Zunächst nur aus dem Kernholz der Eibe gebaut, wurden etwa ab dem 8. Jahrhundert die unterschiedlichen Eigenschaften von Splint- und Kernholz zum Bogenbau genutzt. Als Englischer Langbogen wird ein Stabbogentyp des Spätmittelalters bezeichnet, der vor allem durch den massenhaften Einsatz in spätmittelalterlichen Schlachten bekannt wurde. Der aus einem Stück gefertigte Stab ist etwa so lang wie der Schütze, also um 180 Zentimeter, und besteht ungefähr aus 1/3 Splintholz und 2/3 Kernholz auf der Außen- bzw. Innenseite.

Die englischen Bogenschützen waren keine Leibeigenen, die zum Kriegsdienst eingezogen wurden, sondern bestens ausgebildete Soldaten, die für eine bestimmte Zeit vertraglich verpflichtet und gut bezahlt wurden. Sie konnten den Feind über eine Entfernung von über 400 Metern bekämpfen. Mit ihnen konnten englische Heere zahlenmäßig überlegene Streitmachten schlagen.

Ein früher Einsatz zahlreicher Bogenschützen ist für die Schlacht von Hastings am 14. Oktober 1066 belegt, in der die Normannen unter Wilhelm I. den englischen König Harald besiegten. Auf dem Bilderteppich von Bayeux sind Bogenschützen auf beiden Seiten zu erkennen. Im 13. Jahrhundert gingen die Eibenbestände der Insel stark zurück. Der erste Hinweis auf einen Import stammt von einer Zollrolle aus Dordrecht, die auf den 10. Oktober 1287 datiert ist. Für den 8. Januar 1295 ist für Newcastle die Ankunft von sechs Schiffen aus Stralsund belegt, die unter anderem 360 „"Baculi ad arcus"“ oder Bogenstäbe geladen hatten.
Der Hundertjährige Krieg, ab 1337, trug entscheidend zur Bildung des Nationalbewusstseins bei Franzosen und Engländern bei; die Bevölkerung wurde stärker beteiligt. So verordnete Eduard III. 1339: „Hiermit befehlen Wir, dass jeder Mann von Leibes Gesundheit in der Stadt London zur Mußezeit und an den Feiertagen Bogen und Pfeile benützen und die Kunst des Schießens erlerne und übe.“ (SCHEEDER 1994, S. 43) Gleichzeitig wurden Spiele wie Steinstoßen, Holz- oder Eisenwerfen, Handball, Fußball und Hahnenkämpfe unter Androhung von Gefängnis verboten. Jeder Mann zwischen dem siebten und dem sechzigsten Lebensjahr war verpflichtet, einen Bogen und zwei Pfeile zu besitzen. Wegen der Holzknappheit und der starken Nachfrage mussten Höchstpreise festgelegt werden, damit sich jeder einen Bogen leisten konnte. „Da die Verteidigung des Reiches bisher in den Händen der Bogenschützen lag und nun Gefahr droht, befehlen Wir, daß jedermann 2 Schilling Buße je Bogen an den König entrichten muß, der einen solchen für mehr als drei Schilling sechs Pence verkauft“ (SCHEEDER 1994, S. 7).

Jedes Handelsschiff, das ab 1492 in England Handel treiben wollte, musste eine bestimmte Anzahl Eibenrohlinge mit sich führen. Das führte dazu, dass alle europäischen Eibenbestände so stark zurückgingen, dass diese sich bis heute nicht richtig erholt haben. Allein zwischen 1521 und 1567 wurden aus Österreich und Bayern zwischen 600.000 und eine Million zwei Meter lange und 6 cm breite Eibenstäbe für die Weiterverarbeitung zu Bögen ausgeführt. 1568 musste Herzog Albrecht dem kaiserlichen Rat in Nürnberg mitteilen, dass Bayern über keine schlagreifen Eiben mehr verfüge. In England erfolgte aufgrund der Eibenholzverknappung die Anordnung, dass jeder Bogenmacher pro Eibenholzbogen vier aus dem weniger geeigneten Holz des Bergahorns herzustellen habe, und Jugendlichen unter 17 Jahren wurde das Führen eines Eibenholzbogens verboten. Anordnungen aus dieser Zeit lassen darauf schließen, dass England, nachdem die mittel- und südeuropäischen Eibenvorkommen erschöpft waren, Eibenholz aus den Karpaten und dem nordöstlichen Baltikum bezog. 1595 ordnete die englische Königin Elisabeth I. die Umstellung des englischen Heeres von Langbögen auf Musketen an. Fritz Hageneder vertritt in seiner Monographie über die Eibe die Ansicht, dass diese Umstellung, die zu einem Zeitpunkt erfolgte, als der Langbogen der Muskete in Reichweite, Treffsicherheit und Schussgeschwindigkeit noch weit überlegen war, allein erfolgte, weil der Rohstoff Eibe für die Herstellung von Langbögen nicht mehr zur Verfügung stand.

Die Verwendung von Eiben war nicht nur auf die Herstellung von Langbögen begrenzt. Neben verschiedenen Gebrauchsgegenständen wie Webschiffchen, Kästchen, Eimern, Kämmen und Axtholmen wurde das feuchtigkeitsbeständige Holz unter anderem für die sogenannten Sohlbalken verwendet, die direkt auf dem Steinfundament von Häusern auflagen und besonders leicht Feuchtigkeitsschäden ausgesetzt waren. Ebenso wurde das Holz für Fasspipen und Wasserleitungen gebraucht. Das elastische Holz wurde bis ins 20. Jahrhundert bei der Herstellung von Peitschen verwendet. Anders als beim Langbogenbau war Eibenholz bei diesen Verwendungen jedoch einfacher zu ersetzen.

Die Giftigkeit der Eibe ist bereits Thema der griechischen Mythologie: Die Jagdgöttin Artemis tötet mit Eibengiftpfeilen die Töchter der Niobe, die sich ihr gegenüber ihres Kinderreichtums gerühmt hatte. Auch die Kelten verwendeten Eibennadelabsud, um ihre Pfeilspitzen zu vergiften und Julius Caesar berichtet in seinem Gallischen Krieg von einem Eburonen-Stammesfürst, der lieber mit Eibengift Selbstmord beging, als sich den Römern zu ergeben. Zur Giftigkeit der Europäischen Eibe äußern sich Paracelsus, Vergil und Plinius der Ältere. Dioskurides berichtete von spanischen Eiben mit einem so hohen Giftgehalt, dass sie schon denen gefährlich werden konnten, die nur in ihrem Schatten saßen oder schliefen.

In der Medizin spielten Eibenzubereitungen ab dem frühen Mittelalter eine Rolle. Mit ihnen wurden unter anderem Krankheiten wie Epilepsie, Diphtherie und Rheumatismus sowie Hautausschläge und Krätze behandelt. Eibennadelsud wurde auch als Abortivum eingesetzt.

Neben der Verwendung als Gift- und Heilpflanze wurden Eibenbestandteile sogar als Nahrungspflanze verwendet: Der rote und süßliche Samenmantel, der ungiftig ist, lässt sich zu Marmelade einkochen, sofern die giftigen Samen entfernt werden. Eibenlaub wurde in geringem Maße traditionell den Futterpflanzen des Viehs beigemischt, um so Krankheiten vorzubeugen. In einigen Regionen wie etwa Albanien wird dies bis heute praktiziert.

Als einzige europäische Nadelholzart besitzt die Eibe ein gutes Ausschlagsvermögen. Die Schnittverträglichkeit und der dichte Wuchs führen dazu, dass Eiben sehr gerne als dichte Sichtschutzhecken verwendet wurden und werden. Eiben eignen sich auch sehr gut für geometrische oder figürliche Formschnitte. Beginnend mit der Renaissance wurden die immergrünen Eibenbäume daher in der Gartengestaltung eingesetzt. Schnitthecken aus Eiben waren besonders in Barockgärten sehr beliebt. Zu den bekanntesten barocken Gartenanlagen, in denen Eibenhecken eine große Rolle spielen, zählen die Gärten von Versailles. Auch der Residenzgarten von Würzburg weist zahlreiche Eibenskulpturen auf. In England wurden gerne begehbare Labyrinthe aus Eibenhecken gestaltet. Den 114 Meter langen und 52 Meter breiten Irrgarten von Longleat House säumen mehr als 16.000 Eiben. Mit der Hinwendung zum Englischen Landschaftsgarten begann ein zunehmendes Interesse für ausgefallene Züchtungen was bis heute zu mehr als siebzig verschiedenen bekannten Zuchtformen der Europäischen Eibe führte. Zu diesen zählen unter anderem:

"Taxus" × "media" (Bechereibe) = "Taxus baccata" × "Taxus cuspidata"

Der "Eibenbaum," auch "Ibenbaum" (kurz "Ibaum", auch "Ybaum") ist namensgebend für verschiedene geografische Orte. Auf historische Eibenbestände weisen Toponyme wie Eiben, Eibenberg, Ibenberg, Iberg, Yberg, Iberig und Ibach hin.

In der Nähe von Klöstern besteht heute die größte Aussicht, noch alte Eibenbestände zu finden.


Eines der größten natürlichen Eibenvorkommen Europas mit rund 80.000 Eiben findet sich auf der Bergkette des Albis und dort besonders im Gebiet des Uetlibergs.
Der Grund für diesen Bestand geht auf die Liberalisierung der Jagdgesetze in der Schweiz nach der Französischen Revolution zurück. Von 1798 bis 1850 wurden die Nutzwildpopulationen – im Speziellen Paarhufer – bis an die Grenze der Ausrottung bejagt. Eibenschösslinge werden vom Rehwild bevorzugt und haben bei einem größeren Rehwildbestand keine Chance aufzuwachsen. Die beinahe Ausrottung des Rehwilds um 1860 ermöglichte den Aufwuchs der Eiben, die heute fast alle über 150 Jahre alt sind. 1997 fand die internationale Eiben-Tagung in Zürich statt.





</doc>
<doc id="1421" url="https://de.wikipedia.org/wiki?curid=1421" title="Enziangewächse">
Enziangewächse

Die Enziangewächse (Gentianaceae) sind eine Pflanzenfamilie in der Ordnung der Enzianartigen (Gentianales). Die etwa 80 bis 87 Gattungen mit 900 bis 1655 Arten sind weltweit vertreten.

Es sind meist ein-, zweijährige oder ausdauernde krautige Pflanzen, seltener verholzende Pflanzen: Sträucher, Bäume oder Lianen. Alle Pflanzenteile sind unbehaart.

Die meist gegenständigen, ungestielten Laubblätter sind einfach und glattrandig. Bei einigen Taxa sind die sich gegenüberstehenden Blätter teilweise durch eine verdickte Leiste verbunden. Bei manchen krautigen Arten sind die Laubblätter grundständig konzentriert. Nebenblätter fehlen.

Die Blüten stehen einzeln, in end- oder in achselständigen Blütenständen, meist handelt es sich um ein einfaches oder zusammengesetztes Dichasium, mit oder ohne Hochblätter. Die zwittrigen Blüten sind fast immer radiärsymmetrisch und meist vier- oder fünfzählig. Die vier oder fünf (selten bis zu zwölf) Kelchblätter sind mindestens an der Basis verwachsen. Die vier oder fünf (selten bis zu zwölf) meist großen und auffällig gefärbten Kronblätter sind mindestens an der Basis verwachsen (Sympetalie). Es ist nur ein Kreis mit vier oder fünf (selten bis zu zwölf) Staubblättern vorhanden; sie sind mit der Kronröhre verwachsen. Kelch-, Kron- und Staubblätter sind je Blüte immer in gleich großer Anzahl vorhanden. Zwei Fruchtblätter sind zu einem oberständigen Fruchtknoten verwachsen. Einige Arten sind heterostyl. Die Bestäubung erfolgt meist durch Insekten (Entomophilie).

Es werden meist zweiklappige Kapselfrüchte gebildet mit meist vielen Samen; sehr selten sind Beeren mit wenigen Samen. Die kleinen Samen enthalten Öl und können geflügelt oder ungeflügelt sein.

Besonders bei den krautigen Taxa sind häufig Bitterstoffe vorhanden, es handelt sich um Seco-Iridoide.

Diese Familie wurde 1789 unter dem Namen „Gentianae“ durch Antoine Laurent de Jussieu in "Genera Plantarum", 141 aufgestellt. Typusgattung ist "Gentiana" . Die Taxa der früheren Familien: Chironiaceae , Coutoubeaceae , Potaliaceae , Saccifoliaceae sind heute in der Familie der Gentianaceae enthalten.

Die Familie Gentianaceae wird in sechs Tribus gegliedert mit etwa 80 bis 87 (bis 95) Gattungen und 900 bis 1655 Arten:








Einige Gattungen werden bei verschiedenen Autoren auch in die Familie der Loganiaceae eingeordnet.




</doc>
<doc id="1422" url="https://de.wikipedia.org/wiki?curid=1422" title="EVU">
EVU

EVU steht als Abkürzung für:



</doc>
<doc id="1423" url="https://de.wikipedia.org/wiki?curid=1423" title="Elektrizitätsversorgungsunternehmen">
Elektrizitätsversorgungsunternehmen

Ein Elektrizitätsversorgungsunternehmen ("EVU", "EltVU", auch "Stromversorgungsunternehmen"; kurz "Elektrizitätsversorger", "Stromversorger" oder "Stromanbieter") ist ein Unternehmen, das seine Kunden mit elektrischer Energie (historisch und umgangssprachlich „Elektrizität“ oder „Strom“ genannt) versorgt, d. h. beliefert.

Im "engeren" Sinne werden nur solche Unternehmen als "Elektrizitätsversorger" bezeichnet, die einen Endverbraucher direkt beliefern, insbesondere solche, die hierfür ein Verteilungsnetz betreiben. Im "weiteren" Sinne sind alle Unternehmen der Elektrizitätswirtschaft, also der gesamten Versorgungskette von der Erzeugung über den Handel, die Übertragung (Ferntransport) und die Verteilung bis zum Verbraucher unter diesem Begriff mit eingeschlossen.

Als Stromanbieter wird jede natürliche oder juristische Person bezeichnet, die elektrischen Strom an Letztverbraucher liefert.

In Deutschland kann man die Elektrizitätsversorgungsunternehmen unterteilen in

Die Elektrizitätsversorgungsunternehmen sind in Deutschland durch Kooperation - auch auf europäischer Ebene - in der Lage, eine hohe Versorgungssicherheit zu gewährleisten. So gibt es Netze, bei denen Leitungen verschiedener Spannungsebenen (110 kV, 220 kV und 380 kV) auch von verschiedenen EVU betrieben werden. Daneben werden manche Hochspannungsleitungen gemeinsam von den EVU und der Deutschen Bahn betrieben.

Nachfolgend einige bekannte deutsche Verbände:

Die Strompreise in Deutschland können mitunter stark variieren, wobei die Preise vor allem im Süd-Westen und Osten Deutschlands höher liegen. Der Preisunterschied ist jedoch aufgrund verschiedener Berechnungen oft nicht auf den ersten Blick ersichtlich (Öko/Klimatarife, Pakettarife, Sonderabschläge, Haupt- und Nebenzeit). Bedingt ist dieser Umstand einerseits durch die Preisfreiheit der Grundversorger, andererseits auch durch die Qualität der Anbindungen an überregionale Stromnetze.

Gemäß Energiewirtschaftsgesetz (EnWG) wird unterschieden zwischen Betreibern von Elektrizitätsversorgungsnetzen und Betreibern von Elektrizitätsverteilernetzen. Der Begriff Energieversorgungsnetz bezieht sich demnach auf Elektrizität und Gas. Nach  Nr. 18 EnWG sind Energieversorgungsunternehmen „natürliche oder juristische Personen, die Energie an andere liefern, ein Energieversorgungsnetz betreiben oder an einem Energieversorgungsnetz als Eigentümer Verfügungsbefugnis besitzen“. Sie übernehmen also Aufgaben der Erzeugung, der Verteilung und des Vertriebs. Hierunter fallen neben den Elektrizitätsversorgungsunternehmen zum Beispiel auch Erdgas- und Fernwärmeversorgungsunternehmen.

Die §§ 6–10 EnWG schreiben in Umsetzung des europäischen Gemeinschaftsrechtes eine Entflechtung der sogenannten vertikal integrierten EVU vor. Vertikal integrierte Elektrizitätsversorgungsunternehmen sind nach der Legaldefinition des  Nr. 38 EnWG solche Unternehmen oder eine Gruppe von Unternehmen, die im Elektrizitätsbereich mindestens eine der Funktionen Übertragung oder Verteilung und mindestens eine der Funktionen Erzeugung oder Vertrieb wahrnehmen. Das bedeutet zum Beispiel, dass der Netzbetrieb bei marktbeherrschenden Unternehmen rechtlich, operationell, informationell und buchhalterisch unabhängig von anderen Tätigkeiten im Bereich der Energieversorgung organisiert werden muss.

In Deutschland gibt es rund 300 Erzeuger und 1150 Lieferanten für elektrischen Strom (Stand: Januar 2013). Die weitaus höchsten Gesamtumsätze erzielen folgende Energiekonzerne:

Zusammen beherrschen die ersten vier etwa 80 Prozent des deutschen Strommarktes, wobei zu bemerken ist, dass E.ON selbst nur noch Erzeuger ist und den elektrischen Strom hauptsächlich über konzerneigene Vertriebsgesellschaften verkauft. 
Die DB Energie versorgt die Fahrzeuge der Deutschen Bahn und anderer Eisenbahnverkehrsunternehmen mit Einphasenwechselstrom der Frequenz 16,7 Hz. Da dies nur für den Bahnbetrieb geschieht, zählt man sie insofern nicht zu den eigentlichen EVU, obwohl sie ein umfangreiches Hochspannungsleitungsnetz betreibt, welches auch nach Österreich und in die Schweiz führt. Indem die DB Energie jedoch auch zahlreiche Gewerbebetriebe an den Bahnhöfen mit 230 V 50 Hz Netzstrom versorgt, der überwiegend von außen zugeliefert wird, ist sie in diesem Aspekt ein relativ großes EVU.

Daneben gibt es einige EVU, die elektrischen Strom vorwiegend aus erneuerbaren Energiequellen erzeugen bzw. verteilen. Die vom Umsatz her größten, von den großen EVU unabhängigen Ökostrom-Anbieter waren 2013:

Vor 2007 gab es in Frankreich nur EdF sowie lokale Verteilungsunternehmen wie 'Electricité de Marseille' (zusammenfassend 'fournisseurs historiques' genannt). 
Mitte 2007 liberalisierte die französische Regierung den Elektritizitätsmarkt, seitdem gibt es zahlreiche 'fournisseurs alternatifs', z. B. Engie.

In Österreich gibt es zwei nationale Erzeuger-Gesellschaften sowie eine Reihe von regionalen Gesellschaften, die oft aber nur als Verteiler bzw. Stromhändler tätig sind (s. u.). Sie sind meist Gesellschaften mit Anteilen der einzelnen Bundesländer, einige (z. B. Energieversorgung Niederösterreich, Wien Energie) haben auch eigene Elektrizitätswerke, um Spitzenbedarf abzudecken.

Die wichtigsten Stromerzeuger sind:

Bahnstrom

Die wichtigsten Ökostrom-Erzeuger sind:

Die Verteilergesellschaften sind:

Die Übertragungsnetzbetreiber sind:

Im Zuge der Deregulierung des Strommarktes 2001 sind neue Anbieter dazugekommen, die Endkonsumenten und Unternehmen beliefern. Allerdings besitzen sie keine Infrastruktur.

Die großen Elektrizitätsversorgungsunternehmen in der Schweiz sind:


Die Schweizer Elektrizitätsversorgungsunternehmen sind großmehrheitlich im Verband Schweizerischer Elektrizitätsunternehmen organisiert.



</doc>
<doc id="1424" url="https://de.wikipedia.org/wiki?curid=1424" title="E-Plus">
E-Plus

Die E-Plus Mobilfunk GmbH war ein deutscher Mobilfunkanbieter und Teil der E-Plus-Gruppe. Diese gehörte vom Jahr 2000 bis zum 1. Oktober 2014 dem niederländischen KPN-Konzern, bevor sie Teil der Telefónica Deutschland Holding wurde. E-Plus war bis dato mit rund 25,5 Millionen Kunden der drittgrößte Mobilfunknetzbetreiber Deutschlands.

Viele Marken und Unternehmen der E-Plus-Gruppe wurden nach der Übernahme auf die Telefónica Germany GmbH & Co. OHG verschmolzen. Dazu gehörte auch die E-Plus Mobilfunk GmbH, die in die Teilbereiche "E-Plus Mobilfunk" in Düsseldorf und "E-Plus Service" in Potsdam gegliedert war, aber auch eigenständige Tochterunternehmen wie die Base Company, Simyo oder die Blau Mobilfunk. Im Zuge der Übernahme durch Telefónica wurde Yourfone bereits zum 1. Februar 2015 an den Mobilfunk-Service-Provider Drillisch verkauft. Großkunden von E-Plus wurden von O Business übernommen.

E-Plus war des Weiteren Mitglied in der Europäischen Bewegung Deutschland.

Das Unternehmen wurde 1992 als "E-Plus Mobilfunk GmbH" gegründet und erhielt 1993 eine Mobilfunklizenz für DCS 1800 (E1-Netz) durch den damaligen Bundespostminister Wolfgang Bötsch (CSU). Lizenzinhaber wurde ein Konsortium um E-Plus, dessen Hauptgesellschafter die VEBA Telecom, RWE Telliance, Thyssen Telecom und BellSouth waren.



Spätestens seit dem Jahr 2005 verfolgte E-Plus eine Schwerpunktstrategie am Mobilfunkmarkt, also eine Konzentration des Angebots auf besonders häufig nachgefragte Dienste, die zu vergleichsweise niedrigen Preisen verkauft wurden. Innovative Produkte wurden erst mit Verzögerung angeboten, wenn durch die Erfolge von Konkurrenzunternehmen absehbar war, dass sich Investitionen in diese rentieren würden. Bei E-Plus resultierte diese Strategie in eher günstigeren Tarifen für Telefonie, eine Preisführerschaft in diesem Gebiet wurde angestrebt. Das UMTS-Netz wurde schrittweise weiter ausgebaut, um neue Gebiete zu versorgen und die Kapazitäten zu erweitern. Des Weiteren wurde das GSM-Netz deutschlandweit auf E-GSM erweitert, so dass sich die Netzabdeckung stark verbessert hatte.

E-Plus betrieb bundesweite Mobilfunknetze im E-GSM-, DCS-1800-, UMTS- und LTE-Standard. Das GSM-Netz war seit 1994 in Betrieb. Zum Ende des Jahres 2013 befanden sich noch 19.316 GSM-Basisstationen und 12.280 UMTS-Basisstationen inklusive Mobilfunkrepeater in Betrieb. Nach eigenen Angaben erreichte E-Plus damals mit seinem GSM-Netz 99,9 %, mit EDGE 84 % und mit seinem UMTS-Netz 80 % der Bevölkerung.

Der Aufbau des eigenen UMTS-Netzes erfolgte seit dem Jahr 2004. Zum Einsatz kamen hierbei neben gewöhnlichen Standorten mittlerer Höhe wie kleineren Masten und Hausdächern auch sogenannte Ultra High Sites (UHS). Dies bezeichnete Antennenstandorte mit einer Montagehöhe von mehr als 100 Meter, zum Beispiel Industrieschornsteine oder Fernsehtürme. UHS erlaubte die Versorgung größerer Flächen mit weniger Investitionen, als dies gewöhnlich notwendig wäre. Allerdings verringerte sich durch diese Bauweise auch die Kapazität, da trotz der größeren Fläche immer noch die gleiche Anzahl der gleichzeitig durchgestellten Gespräche blieb. Das UMTS-Netz enthielt teilweise Technik des ehemaligen Netzes von Mobilcom, dessen Ausbau noch vor der kommerziellen Inbetriebnahme gestoppt wurde. Mobilcom verkaufte einen Großteil seiner Technik im Jahr 2003 für 20 Mio. Euro an E-Plus.

Anfang 2006 baute das Unternehmen E-GSM-Basisstationen auf oder vorhandene DCS-1800-Stationen zu E-GSM/DCS-1800-Dualband-Stationen um. Die E-GSM-Frequenzen wurden den beiden Netzbetreibern E-Plus und O von der Bundesnetzagentur zugewiesen, um auch ländliche Gebiete besser und effizienter versorgen zu können und den Empfang innerhalb von Gebäuden zu verbessern. Damit sollten die bisher nur auf 1800 MHz sendenden Netzbetreiber eine technische Chancengleichheit erhalten.

Das GSM-Netz unterstützte den Dienst GPRS für den paketorientierten Datentransfer. In fast allen Regionen konnte zudem der Datendienst EDGE zum Surfen genutzt werden, wodurch ein schnellerer Datendurchsatz erreicht wurde als bei GPRS.

Die Sprachübertragung wurde im GSM-Netz mit dem Adaptive Multirate Codec (AMR) und bei älteren Mobiltelefonen mit dem Enhanced Full Rate Codec (EFR) realisiert.

Von Juli 2010 bis Ende 2012 baute E-Plus sein Datennetz mit dem Datenbeschleuniger HSPA+ aus.

In einer Untersuchung der Datenübertragungsrate in deutschen Mobilfunknetzen durch die Stiftung Warentest im Juni 2011 hatte E-Plus am schlechtesten abgeschnitten. Auch beim Netztest der Zeitschrift Connect landete E-Plus im Jahr 2011 unter allen elf Netzbetreibern in Deutschland, Österreich und der Schweiz auf dem letzten Platz.

Im Januar 2013 gab E-Plus bekannt, noch bis Ende 2013, als letzter deutscher Mobilfunknetzbetreiber, seine LTE-Einführung zu vollziehen.

Anfang März 2014 führte E-Plus als letzter deutscher Netzbetreiber LTE in Berlin, Leipzig und Nürnberg ein. Der Netzbetreiber griff dabei auf das 2010 von der Bundesnetzagentur erworbene 1800-MHz-Frequenzspektrum zurück. Mitte März 2014 schaltete E-Plus HD Voice für bessere Sprachqualität im UMTS-Netz frei.

Ab Mitte Januar 2015 wurde auch das "Netzroaming" zwischen O₂ und E-Plus getestet. Im Februar 2015 kündigte Telefonica an, Mitte April 2015 das National Roaming für alle Kunden freizuschalten. Ziel war es, weiße Flecken in der Netzabdeckung zu schließen und Datendienste in nicht mit LTE versorgten Gebieten zu verbessern.

Am 1. Dezember 2015 kündigte Telefonica Deutschland an, dass ab Januar 2016 mit der Zusammenlegung der GSM- und UMTS-Netze begonnen wird und diese bis Ende 2017 abgeschlossen sein soll. Ende Juni 2016 schaltete Telefónica das LTE-Netz von E-Plus ab.

E-Plus war der Netzanbieter für mehrere Mobilfunk-Discounter wie Aldi Talk, Blau Mobilfunk, MTV Mobile, Ortel Mobile GmbH und simyo, sowie den MVNE Sipgate Wireless und ehemals auch Telogic. Nach der Übernahme von E-Plus übernahm Telefónica die Zusammenarbeit mit den Anbietern sowie die Betreuung des Kundenstamms.

Die E-Plus-Gruppe unterstützte die Sozialkampagne „iCHANCE“ vom Bundesverband Alphabetisierung und Grundbildung. „iCHANCE“ ist eine multimedial ausgerichtete Kampagne mit dem Ziel, Vorurteile gegenüber (funktionalen) Analphabeten abzubauen und Menschen mit Grundbildungsbedarf dazu zu ermutigen, Lernangebote wie Lese- und Schreibkurse wahrzunehmen.

Sperrung bestimmter Festnetzrufnummern

Im Juni 2008 wurde bekannt, dass E-Plus – ähnlich wie O₂ und Vodafone – die Erreichbarkeit der Rufnummern bestimmter Sprach-Chat-Systeme erheblich einschränkte. Bei diesen Rufnummern handelt es sich um normale Festnetzrufnummern, die im Rahmen von Festnetz-Flatrates gratis angerufen werden können und in der Praxis ein hohes Gesprächsaufkommen verursachen. Dabei nahm E-Plus im Unterschied zu O nicht nur eine Limitierung des Anrufvolumens vor, sondern sperrte die betroffenen Rufnummern komplett.

Im Februar 2009 wurden diese Sperren erweitert und umfassten seitdem auch einige Anbieter von geschäftlichen Telefonkonferenzen. Ebenso betroffen waren bestimmte Anbieter von Calling-Cards, über deren Rufnummern Gespräche ins Ausland und zu Sonderrufnummern kostengünstiger geführt werden konnten. Durch die Sperrung war es den Kunden unmöglich, diese Dienstleistungen in Anspruch zu nehmen, was jedoch durch die AGB abgedeckt war. Die Festnetz-Flatrates der meisten Mobilfunkanbieter schließen „Dienstenutzung“ über Festnetznummern in der Regel aus. Ebenfalls ist es seitens der Bundesnetzagentur nicht vorgesehen, auf Endteilnehmeranschlüssen Massendienste anzubieten.

Gewinn vor Finanzergebnis, außerordentlichem Ergebnis, Steuern und Firmenwertabschreibungen (EBITDA):
E-Plus unterhielt Betriebsniederlassungen in Hamburg, Essen, Berlin, Leipzig, Hannover, Kassel, Ratingen, Frankfurt am Main, Karlsruhe, Nürnberg, Stuttgart und München. Der Hauptsitz des Unternehmens war in Düsseldorf.

Die E-Plus Mobilfunk GmbH & Co. KG gehörte von 2000 bis September 2014 zur niederländischen KPN N.V. und wurde nach der kartellrechtlichen Freigabe durch die EU-Kommission am 1. Oktober 2014 von der Telefónica Deutschland Holding übernommen. Durch die Fusion mit E-Plus ist Telefónica Deutschland zum nach Kundenzahlen größten deutschen Mobilfunkanbieter aufgestiegen. Seitdem arbeitet der Konzern kontinuierlich an einer schrittweisen Zusammenführung seiner beiden Tochtergesellschaften "E-Plus" und "O₂". Am 26. Januar 2015 wurde die "E-Plus Mobilfunk GmbH & Co. KG" in "E-Plus Mobilfunk GmbH" umfirmiert und seit dem 4. Februar 2015 besteht zwischen der "E-Plus Mobilfunk GmbH" und der "Telefónica Germany GmbH & Co. OHG" als „herrschendem Unternehmen“ ein Beherrschungs- und Gewinnabführungsvertrag. 

Die 2011 gegründete "E-Plus Retail GmbH" verwaltete das Ladengeschäft, das aus eigenen Filialen, Franchisenehmern und freien Händlern bestand und die Marken Base und MTV Mobile führten. Anfang Dezember 2016 wurde sie in "Telefónica Germany Retail GmbH" umfirmiert. MTV Mobile wurde dabei mit eingestellt. Sie hatte ihren Sitz in Düsseldorf und wurde von Marcus Epple geleitet.



</doc>
<doc id="1426" url="https://de.wikipedia.org/wiki?curid=1426" title="Einlagefazilität">
Einlagefazilität

Eine Einlagefazilität ist eine Möglichkeit für Geschäftsbanken im Euroraum, kurzfristig nicht benötigtes Geld bei der Europäischen Zentralbank (EZB) anzulegen. Als Verzinsung erhalten bzw. zahlen sie den von der Zentralbank vorgegebenen Einlagesatz. Es handelt sich somit um ein Wahlrecht zur Geldanlage, welches von der Zentralbank gewährt wird und stellt ein wichtiges geldpolitisches Instrument der EZB dar. 

Die Einführung der Einlagefazilität durch die Europäische Zentralbank hat in Deutschland in ihrer Funktion die Diskontpolitik der Deutschen Bundesbank ersetzt.

Die Initiative zu Einlagegeschäften geht von den Geschäftsbanken aus. Sind diese für Transaktionen mit der EZB zugelassen, so können sie bei der Zentralbank kurzfristig nicht benötigtes Geld anlegen. Aufgrund der kurzen Fristigkeit solcher Geschäfte bezeichnet man diese Form der Finanzierung auch als Übernachtanlage oder Overnight-Money. 

Hat die Bank am Tagesende offene Habensalden auf den ESZB-Konten, werden diese automatisch zu Einlagefazilitäten. Als Preis für die Inanspruchnahme der Einlagefazilität erhalten bzw. zahlen sie den Einlagesatz (teilweise auch Einlagefazilitätssatz). 

Einlagefazilitäten werden dauerhaft und in unbegrenztem Volumen angeboten; daher bezeichnet man sie auch als ständige Fazilität.

Der Einlagesatz wird üblicherweise als einer der drei Leitzinsen der EZB bezeichnet. Der Zinssatz wird vom EZB-Rat festgelegt und bildet die Untergrenze des Zinskorridors. In der Regel liegt er dabei immer einen Prozentpunkt unter dem Hauptrefinanzierungssatz. Davon wich die EZB jedoch kurz nach der Euroeinführung und während der Finanzkrise ab 2007 ab. Die Einlage ist das Gegenstück der Spitzenrefinanzierungsfazilität. Längerfristige Liquidität wird den Banken vor allem über das Hauptrefinanzierungsinstrument zur Verfügung gestellt.

Mit dem Übergang der Zuständigkeit für die Geldpolitik auf die EZB hat die Einlagefazilität die früheren Rediskontkontingenten abgelöst.

Die Einlagefazilität erfüllt vor allem zwei Funktionen: 

Die erste Bedeutung dieses Instruments liegt darin, dass die Geschäftsbanken von sich aus jederzeit Liquidität anlegen und damit Liquiditätsüberschüsse vermeiden können. 

Zweitens hat die Einlagefazilität eine geldpolitische Bedeutung: Grundsätzlich können Geschäftsbanken auch über den Geldmarkt (Interbankenmarkt) Übernachtanlagen tätigen. Allerdings müssen dort getätigte Übernachtanlagen zwangsläufig teurer (das heißt höher verzinst) sein als die Einlagefazilität, da ansonsten auf dem Interbankenmarkt keine Geschäfte zustandekommen. Daher bildet der Einlagesatz die untere Grenze der für Übernachtanlagen erhobenen Zinsen. Erhöht (senkt) die EZB den Einlagesatz, so werden auch die Geschäftsbanken ihren Zins für Übernachtanlagen erhöhen (senken) – folglich dient der Einlagesatz auch zur Durchsetzung der Zinspolitik am Markt.



</doc>
<doc id="1427" url="https://de.wikipedia.org/wiki?curid=1427" title="Eris (Mythologie)">
Eris (Mythologie)

Eris (, auf einer Vase mit der Beischrift bezeichnet; Personifikation von ) ist in der griechischen Mythologie die Göttin der Zwietracht und des Streites. Sie ist Tochter der Nyx, einer der fünf direkt aus dem Ur-Chaos entstandenen Götter. Sie gilt manchmal auch als Schwester des Ares. Eris wurde aus der griechischen in die römische Mythologie als Discordia („Zwietracht“) übernommen.

Sie ist bekannt durch den goldenen „Apfel der Zwietracht“ (den sprichwörtlichen „Zankapfel“ oder „Erisapfel“), den sie auf der Hochzeit des Peleus und der Thetis, zu der sie nicht eingeladen war, unter die Gäste warf. Auf diesem Apfel war die Widmung eingraviert, das bedeutet „der Schönsten“. Aphrodite, Athene und Hera begannen, um den Apfel zu streiten. Auf den Rat des Zeus führte Hermes die drei zu Paris; dieser solle ihn der schönsten der drei Göttinnen geben. Paris entschied sich für Aphrodite, die ihm die schönste Frau (der Welt) versprach. Es erwies sich aber danach, dass sie bereits verheiratet war: Helena, die Frau des Königs von Sparta Menelaos. Ihre Entführung durch Paris löste dann den Trojanischen Krieg aus.

Eris erscheint oft als hinkende, zusammengeschrumpelte, kleine Frau. Erst wenn sie es schafft, den Neid und den Hass der Menschen zu wecken, erblüht sie zu ihrer wahren Gestalt. Homer schreibt über sie in der "Ilias": „... die rastlos lechzende Eris ..., die erst klein von Gestalt einherschleicht; aber in kurzem trägt sie hoch an den Himmel ihr Haupt, und geht auf der Erde. Diese nun streuete Zank zu gemeinsamem Weh in die Mitte, wandelnd von Schar zu Schar, das Geseufz’ der Männer vermehrend.“

In "Werke und Tage" des Hesiod ist neben der zänkischen Eris auch noch eine „gute“ angeführt, die den Menschen zur Arbeit anspornt.

In Hesiods "Theogonie" werden als Nachkommen der Eris genannt:

Eine postmoderne Rezeption des von Eris vertretenen Prinzips findet sich in der Religion des Diskordianismus, der vor allem durch die Romantrilogie "Illuminatus!" von Robert Shea und Robert Anton Wilson bekannt wurde.

Nach der Göttin der Zwietracht ist auch der Zwergplanet Eris benannt, dessen Entdeckung zu einem Streit um die Neudefinition des Begriffs „Planet“ und der kontrovers diskutierten Aberkennung des Planetenstatus von Pluto geführt hatte. Der Zwergplanet wurde so metaphorisch zum Zankapfel der Astronomen.





</doc>
<doc id="1428" url="https://de.wikipedia.org/wiki?curid=1428" title="Enzyklopädie">
Enzyklopädie

Eine Enzyklopädie, früher auch aus dem Französischen: Encyclopédie (griechisch ἐγκύκλιος παιδεία, „Kreis der Bildung“), ist ein besonders umfangreiches Nachschlagewerk. Der Begriff "Enzyklopädie" soll auf Ausführlichkeit oder eine große Themenbreite hinweisen, wie beispielsweise bei einem Menschen, dem enzyklopädisches Wissen nachgesagt wird. Es wird eine Zusammenfassung des gesamten Wissens dargestellt. Die Enzyklopädie ist demzufolge eine überblickende Anordnung des Wissens einer bestimmten Zeit und eines bestimmten Raumes, welche Zusammenhänge darstellt. Daneben werden als Fachenzyklopädien solche Werke bezeichnet, die nur ein einzelnes Fach- oder Sachgebiet behandeln.

Die Verwendung des Begriffes Enzyklopädie ist fließend; Enzyklopädien standen zwischen Lehrbüchern einerseits und Wörterbüchern andererseits. Als älteste vollständig erhaltene Enzyklopädie gilt die "Naturalis historia" aus dem ersten nachchristlichen Jahrhundert. Vor allem die große französische "Encyclopédie" (1751–1780) hat die Bezeichnung „Enzyklopädie“ für ein Sachwörterbuch durchgesetzt. Aufgrund der alphabetischen Anordnung werden Enzyklopädien oft als Lexika bezeichnet.

Die heutige Form des Nachschlagewerkes hat sich vor allem seit dem 18. Jahrhundert entwickelt; dabei handelt es sich um ein umfangreiches Sachwörterbuch über alle Themen für eine breite Leserschaft. Im 19. Jahrhundert kam der typische neutral-sachliche Stil hinzu. Die Enzyklopädien wurden besser strukturiert und beinhalteten neue Texte, keine bloßen Übernahmen älterer (fremder) Werke. Eines der bekanntesten Beispiele im deutschsprachigen Raum war lange Zeit die "Brockhaus Enzyklopädie" (ab 1808), im englischsprachigen die "Encyclopaedia Britannica" (ab 1768).

Seit den 1980er-Jahren werden Enzyklopädien ferner in digitaler Form angeboten, auf CD-ROM und im Internet. Teilweise sind es Fortführungen älterer Werke, teilweise neue Projekte. Ein besonderer Erfolg war die 1993 erstmals auf CD-ROM herausgegebene "Microsoft Encarta". Die 2001 gegründete "Wikipedia" entwickelte sich zur größten Internet-Enzyklopädie.

Die Althistorikerin Aude Doody nannte die Enzyklopädie eine Gattung, die man nur schwer definieren könne. Enzyklopädismus sei das Streben nach universellem Wissen oder auch die Summe des allgemeinen Wissens (einer bestimmten Kultur). Konkret sei die Enzyklopädie ein Buch, „das entweder die gesamte Garnitur des allgemeinen Wissens oder ein erschöpfendes Spektrum an Material über einen spezialistischen Gegenstand versammelt und ordnet.“ Die Enzyklopädie beanspruche, einfachen Zugang zu Informationen über alles zu verschaffen, das der Einzelne über seine Welt wissen muss.

Für das Selbstverständnis von Enzyklopädien werden oftmals die Vorworte der Werke ausgewertet. Im 18. und vor allem 19. Jahrhundert betonten sie, dass sie Wissen zusammenfassen, und zwar nicht für Fachleute, sondern für ein breiteres Publikum. Im Vorwort des "Brockhaus" etwa hieß es 1809:

Der Bibliothekswissenschaftler und Enzyklopädie-Experte Robert Collison schrieb um 1970 für die "Encyclopaedia Britannica" einleitend im entsprechenden Macropaedia-Artikel:

Der moderne Begriff „Enzyklopädie“ setzt sich aus zwei griechischen Wörtern zusammen: ἐγκύκλιος (enkýklios), im Kreis herumgehend, auch: umfassend, allgemein, sowie παιδεία (paideia), Erziehung oder Unterricht. Das daraus zusammengesetzte ἐγκύκλιος παιδεία verwies auf die „chorische Erziehung“, meinte also ursprünglich die musische Ausbildung junger freigeborener Griechen im Kreis des Theaterchores. Eine verbindliche Auflistung der vermittelten Fächer gab es bei den Griechen nicht. Moderne Forscher ziehen es vor, den griechischen Ausdruck als allgemeine Erziehung zu übersetzen, im Sinne einer grundlegenden Bildung.

Der Römer Quintilian (35 bis ca. 96 nach Christus) übernahm den Begriff und meinte, bevor die Jungen als Redner ausgebildet würden, sollten sie den Bildungsweg (den "orbis ille doctrinae", wörtlich: Kreis der Fächer) durchlaufen. Römische Autoren, wie Quintilian und Vitruv, bezeichneten mit ἐγκύκλιος παιδεία eine Vorbildung für die von ihnen vorgestellte Spezialisierung, wie Redner oder Architekt. Dementsprechend variierten die genannten Fächer. Quintilian erwähnt für Redner beispielsweise die Bereiche Geometrie und Musik.

Unklar bleibt, was Plinius gemeint hat, als er die τῆς ἐγκυκλίου παιδείας im Vorwort zu seiner "Naturalis historia" (ca. 77 n. Chr.) erwähnte. Das liegt nicht nur an der Unbestimmtheit der möglichen Fächer, sondern auch an Undeutlichkeiten der Textstelle. Die ἐγκύκλιος παιδεία wurde schließlich zu einer Sammelbezeichnung für die sich ausbildenden sieben freien Künste, die "artes liberales".

Das Wort Enzyklopädie geht auf eine fehlerhafte Rückübersetzung der Stelle bei Quintilian zurück. Dieses "tas Encyclopaedias" in Plinius-Ausgaben seit 1497 setzte dann den Ausdruck durch. Es wurde als griechische Übersetzung von "orbis doctrinae" angesehen. In Nationalsprachen erschien der Ausdruck dann in den 1530er-Jahren. In der Mitte des 16. Jahrhunderts konnte man das Wort ohne weitere Erklärung in Buchtiteln für Werke verwenden, „in denen die Gesamtheit der Wissenschaften nach einer bestimmten Ordnung dargestellt wird“, so Ulrich Dierse. Die Betonung lag dabei nicht auf Gesamtheit, sondern auf Ordnung.

Guillaume Budé verwendete die lateinische Neuschöpfung 1508 im Sinne einer allesumfassenden Wissenschaft oder Gelehrtheit. Wohl zum ersten Mal in einem Buchtitel erschien das Wort 1527. Damals veröffentlichte der südniederländische Pädagoge Joachim Sterck van Ringelbergh: "Lucubrationes, vel potius absolutissima kuklopaideia, nempe liber de ratione studii" (Nachtarbeiten, oder vielmehr vollständigste "kuklopaideia", sicherlich ein Buch über die Vernunft des Lernens). Als Titel eines Buches tauchte es zuerst 1559 auf: "Encyclopaedia, seu orbis disciplinarum" ("Encyclopaedia", oder der Kreis der Fächer) des Kroaten Pavao Skalić.

Die englische "Cyclopaedia" von 1728 war ein alphabetisch geordnetes Nachschlagewerk, ein "dictionary of the arts and sciences". Der Durchbruch des Namens Enzyklopädie kam erst mit der großen französischen "Encyclopédie" (1751 und Folgejahre). Nach dem Vorbild dieses Werkes etablierte sich der Begriff für ein allgemeines Sachwörterbuch.

Daneben wurde das Wort auch für die Erkenntnis von der Einheit des Wissens verwendet; in diesem Sinne beschrieb der Philosoph Christian Appel seinen 1784 an der Universität Mainz eingerichteten „Lehrstuhl für allgemeine Enzyklopädie“. In der Erziehung gehe man von einfachen sinnlichen Eindrücken und Erfahrungen aus, dann komme man über einen Abstraktionsprozess zu zusammenhängenden wissenschaftlichen Weisheiten. Diese seien aber verstreut, daher brauche man eine Zusammenfassung. So solle die Enzyklopädie nicht am Anfang des Universitätsstudiums stehen, sondern am Ende, als Krönung. Für die Erforschung der Enzyklopädien wiederum hat sich der Begriff Enzyklopädik eingebürgert.

Während bei den Römern die Titel von Nachschlage- und Lehrwerken meistens eher nüchtern waren, überwogen seit der Spätantike bis in die Frühe Neuzeit Metaphern:

Alphabetisch angeordnete Enzyklopädien hießen oder heißen "Dictionarium", "Wörterbuch" oder "Lexikon". Weitere Bezeichnungen lauten: "Enzyklopädisches Wörterbuch", "Sachwörterbuch", "Realwörterbuch", dazu "Reallexikon" und "Realenzyklopädie", "Konversationslexikon", "Universallexikon" usw.

Im Englischen und Französischen war "dictionary" beziehungsweise "dictionnaire" weit verbreitet, oft in der Zusammenfassung "dictionary of the arts and sciences" beziehungsweise "dictionnaire des arts et des sciences". Im Deutschen spiegelt sich das im Titel der "Allgemeinen Encyclopädie der Wissenschaften und Künste" von Ersch-Gruber (1818–1889) wider. Als Künste sind gängigerweise die mechanischen und handwerklichen Künste zu verstehen, und der Begriff der Wissenschaft sollte nicht zu eng aufgefasst werden, so wurde die Theologie damals noch selbstverständlich zur Wissenschaft gezählt. "Real" oder "Realia" steht für Sachen im Gegensatz zu Begriffen oder Wörtern, ein "Realwörterbuch" ist also ein Sachwörterbuch und kein Sprachwörterbuch.

Literarische Gattung und Begriff laufen in der Geschichte der Enzyklopädie nicht parallel zueinander. Darum lässt sich darüber streiten, ob es vor der Neuzeit überhaupt Enzyklopädien gegeben hat. Zumindest waren sich die antiken und mittelalterlichen Autoren einer solchen literarischen Gattung nicht bewusst. Es herrscht weite Übereinstimmung, beispielsweise die "Naturalis historia" aus dem Alten Rom als Enzyklopädie anzusehen. Dabei besteht allerdings die Gefahr einer anachronistischen Sichtweise, nämlich ein antikes Werk mit modernen Augen zu sehen und es unangemessen zu interpretieren, warnt Aude Doody.

Die Historiker sind sich nicht darüber einig, welches Werk als die erste Enzyklopädie anzusehen ist. Das liegt einerseits daran, dass viele Werke verloren gegangen sind und nur noch aus kurzen Beschreibungen oder Bruchstücken bekannt sind. Andererseits gibt es keine verbindliche Definition einer Enzyklopädie, manche Historiker berücksichtigen auch einen enzyklopädischen Ansatz im Sinne eines Strebens nach Umfassendheit.

Als ein geistiger Vater der Enzyklopädie wird der griechische Philosoph Platon genannt. Er hat zwar selbst keine Enzyklopädie verfasst, aber mit seiner Akademie zu Athen verschrieb er sich dazu, die ganze Bildung jedem intelligenten jungen Mann zur Verfügung zu stellen. Von einem enzyklopädischen Werk von Platons Neffen Speusippos (gestorben 338 v. Chr.) sind nur noch Fragmente erhalten. Einen enzyklopädischen Ansatz, im Sinne von umfassend, hat man auch Aristoteles nachgesagt.

Die Griechen sind für ihre intellektuellen Erkundungen und ihre philosophische Originalität bekannt. Sie haben ihr Wissen aber nicht in einem einzelnen Werk zusammengefasst. So gelten die Römer als die eigentlichen Erfinder der Enzyklopädie. In der Römischen Republik gab es bereits die Briefserie "Praecepta ad filium" (etwa 183 v. Chr.), mit der Cato der Ältere seinen Sohn unterwies.

Vor allem entstand die Enzyklopädie in der Kaiserzeit, da sie den weiten Horizont solcher Menschen brauchte, die ein Weltreich beherrschten. Die erste der eigentlichen Enzyklopädien war das "Disciplinarum libri IX" von Marcus Terentius Varro († 27 v. Chr.). Die zweite Enzyklopädie waren die "Artes" des Arztes Aulus Cornelius Celsus (gestorben um 50 n. Chr.). Varro war der Erste, der die allgemeinbildenden Fächer zusammengefasst hat, aus denen später die freien Künste wurden. Zusätzlich zu jenen Fächern, die dann im Mittelalter zum Kanon wurden, nahm er Medizin und Architektur auf. Die "Hebdomades vel de imaginibus" sind siebenhundert Kurzbiografien großer Griechen und Römer; davon sind nur vereinzelte Bruchstücke überliefert, ebenso wie vom "Discliplinarum". Varro hatte großen Einfluss auf Autoren der ausgehenden Antike.

Von überragender Bedeutung jedoch war die "Naturalis historia" des Politikers und Naturforschers Plinius. Der Verwalter Plinius war es gewohnt, die Welt in Einheiten und Untereinheiten eingeteilt zu sehen. Sein Werk wurde um das Jahr 77 n. Chr. geschrieben und gilt nun als einzige Enzyklopädie des Altertums, die vollständig erhalten ist. Im Mittelalter fand man sie in fast jeder anspruchsvollen Bibliothek. Das Besondere an ihr war die beanspruchte und immer wieder thematisierte Universalität. Sie diente Plinius auch als Erklärung dafür, dass er vieles nur sehr kurz beschreiben konnte.

Ein anderer römischer Enzyklopädist mit weitreichendem Einfluss war Martianus Capella aus Nordafrika. Er verfasste zwischen 410 und 429 n. Chr. eine Enzyklopädie, die oft "Liber de nuptiis Mercurii et Philologiae" („Die Hochzeit der Philologie mit Merkur“) genannt wird und zum Teil in Versen geschrieben wurde. Die sieben Brautjungfern entsprachen den Kapiteln des Werks und diese wiederum den sieben freien Künsten.

Nach dem Untergang des Römischen Reiches bewahrte der Politiker Cassiodor mit seiner Kompilation "Institutiones divinarum et saecularium litterarum" (543–555 n. Chr.) Teile des antiken Wissens im Frühmittelalter. Dazu hatte er sich in ein von ihm selbst gegründetes Kloster im Süden Italiens zurückgezogen. Während Cassiodor noch Weltliches und Geistliches voneinander trennte, integrierte zwei Generationen später Bischof Isidor von Sevilla die christliche Lehre in die antike Gelehrsamkeit.

Isidors Enzyklopädie "Etymologiae" (um 620) wollte die Welt dadurch deuten, dass er Begriffe samt Wortherkunft erklärte. Durch das Erkennen des wahren Sinn eines Wortes wurde der Leser im Glauben unterwiesen. Isidor gab allerdings zu, dass manche Wörter willkürlich gewählt sind. Die Forschung hat viele Vorlagen Isidors ermittelt. Seine eigene Leistung bestand darin, daraus ausgewählt sowie eine klare, gut angeordnete Darstellung in einfachem Latein abgeliefert zu haben. Brüche im Text lassen vermuten, dass Isidor sein Werk nicht vollendet hat.

Rabanus Maurus, der 847 zum Mainzer Erzbischof geweiht wurde, stellte ein Werk "De universo" zusammen, das großteils Isidors Text übernahm. Rabanus begann jedes seiner 22 Kapitel mit einer geeigneten Textstelle Isidors und ließ vieles weg, das ihm für das Verständnis der Heiligen Schrift unnötig erschien. Dazu gehörten für ihn insbesondere die freien Künste. Viele spätere Werke des Mittelalters folgten außerdem seinem Beispiel, mit Gott und den Engeln zu beginnen.

Auf den antiken und frühmittelalterlichen Enzyklopädien bauten die Werke des europäischen Hochmittelalters auf (um 1050 bis 1250). Das größte enzyklopädische Werk aus der Mitte des 13. Jahrhunderts war das "Speculum maius" des Vincent von Beauvais mit fast zehntausend Kapiteln in achtzig Büchern. Es deckte nahezu alle Themen ab: im ersten Teil "Naturale" Gott und die Schöpfung, einschließlich der Naturgeschichte; in "Doctrinale" das praktische moralische Handeln sowie das scholastische Erbe; in "Historiale" die Geschichte der Menschen von der Schöpfung bis ins dreizehnte Jahrhundert. Ein vierter Teil, "Morale", war nach Vincents Tod hinzugefügt worden und basierte vor allem auf Thomas von Aquins Werken.

Der Südniederländer Jacob van Maerlant verteilte sein enzyklopädisches Wissen auf mehrere Werke: Im Alexanderroman "Alexanders Geesten" (um 1260) band er tausend Verse ein, die einen gereimten Weltatlas ausmachen. In "Der naturen bloeme" (um 1270) behandelt er die Natur, und im "Spiegel historiael" (um 1285) die Weltgeschichte. Er war der erste europäische Enzyklopädist, der in einer (nichtromanischen) Volkssprache geschrieben hat. Seine Werke sind vor allem Bearbeitungen lateinischer Vorlagen, wie "De natura rerum" von Thomas von Cantimpré und "Speculum historiale" von Vincent von Beauvais, doch lässt er viele Details weg, wählt aus, fügt Inhalte von anderen Autoren hinzu und schöpft zu einem geringen Teil auch aus eigenem Wissen von der Welt. Er moralisierte und glaubte zum Beispiel an die Zauberkraft von Edelsteinen. Dennoch steht Maerlant für eine vergleichsweise moderne, kritisch-forschende Naturauffassung im Geiste des Albertus Magnus. Zu den mittelalterlichen Vorläufern heutiger Enzyklopädien zählt auch das im 13. Jahrhundert entstandene Werk "De proprietatibus rerum" von Bartholomaeus Anglicus.

Im Spätmittelalter und in der Renaissance (ca. 1300–1600) zog teilweise bereits eine Darstellung ein, die wissenschaftlicher auftrat und weniger auf dem Christentum beruhte. So befreite sich das anonyme "Compendium philosophicae" (um 1300) von den Legenden, wie sie seit Plinius durch die Enzyklopädien wanderten; der spanische Humanist Juan Luis Vives baute in "De disciplinis" seine Argumente auf der Natur, nicht auf religiöser Autorität auf. Vives wollte nicht über die Natur spekulieren, sondern die Natur beobachten, um für sich und seine Mitmenschen etwas Praktisches zu lernen. Trotz dieser Ansätze bevölkerten bis ins 18. Jahrhundert Wundertiere und Monster die Enzyklopädien, wo sie unproblematisch der Natur zugerechnet wurden.

Mehr noch als die westlichen waren die chinesischen Enzyklopädien Zusammenstellungen bedeutender Literatur. Im Laufe der Jahrhunderte wurden sie eher weitergeführt als erneuert. Oft vor allem für die Ausbildung von Beamten bestimmt, folgten sie normalerweise einer traditionellen Anordnung. Die erste bekannte chinesische Enzyklopädie war der „Kaiserspiegel“ "Huang-lan", der etwa 220 nach Christus auf Befehl des Kaisers erstellt wurde. Aus diesem Werk ist nichts überliefert.
Das "T’ung-tien", etwa 801 fertiggestellt, behandelte Staatskunst und Wirtschaft und wurde mit Ergänzungsbänden bis ins 20. Jahrhundert weitergeführt. Eine der wichtigsten Enzyklopädien, "Yü-hai", wurde etwa 1267 zusammengestellt und erschien 1738 in 240 gedruckten Bänden. Als erste moderne chinesische Enzyklopädie gilt die "Tz’u-yüan" (1915), sie gab die Richtung für spätere Werke vor.

Der persische Gelehrte und Staatsmann Muhammad ibn Ahmad Al-Chwarizmi stellte 975–997 einen arabischen „Schlüssel zu den Wissenschaften“ zusammen, "Mafatih al-’Ulum". Er war zweifellos mit den Grundzügen der griechischen Geisteswelt bekannt und bezog sich teilweise auf Werke des Philo, Nikomachos oder Euklid. Seine Enzyklopädie teilt sich in einen „einheimischen“, arabischen Teil, darunter das Meiste, das heute als Geisteswissenschaften angesehen wird, und einen „fremden“.

Die Brüder der Reinheit in Basra (heutiger Irak), ein Orden von Sufi-Mystikern, waren vor allem 980–999 aktiv und arbeiteten gemeinsam an einer Enzyklopädie. Ihre Kompilation wird meist "Rasa’ulu Ikhwan al safa" genannt. Auch sie kannten die griechischen Gelehrten und hatten ausgesprochene Vorlieben. Umgekehrt gibt es kaum Anzeichen dafür, dass die westlichen Enzyklopädie-Autoren die arabisch-islamischen Quellen gekannt hätten. Die chinesischen Enzyklopädien wiederum waren sowohl vom christlichen als auch vom islamischen Kulturkreis getrennt.

"Margarita Philosophica" von Gregor Reisch (1503) war eine weit verbreitete allgemeine Enzyklopädie, ein Lehrbuch für die sieben freien Künste. Sie war die erste Enzyklopädie, die nicht in Handschriften, sondern sofort gedruckt erschien. Ebenso wie die "Encyclopaedia" von Johannes Aventinus (1517) und die "Encyclopaedia Cursus Philosophici" von Johann Heinrich Alsted (1630) folgte sie einer systematischen Ordnung.

Das "Grand dictionaire historique" (1674) von Louis Moréri war das erste große, nationalsprachliche, alphabetische Nachschlagewerk für die Themenbereiche Geschichte, Biografie und Erdkunde. In seiner Tradition steht das eigentümliche "Dictionnaire historique et critique" (1696/1697) von Pierre Bayle, das Moréris Werk ursprünglich korrigieren und ergänzen sollte. Zu eher knappen Artikeln lieferte Bayle einen überaus ausführlichen und kritischen Apparat von Anmerkungen. Da Bayle in erster Linie diejenigen Gegenstände behandelte, die ihn persönlich interessierten, ist sein Werk als ein Ego-Dokument, eine intellektuelle Autobiografie anzusehen. Es war eher neben, nicht anstelle einer allgemein gehaltenen Enzyklopädie zu verwenden.

Denkt man bei Enzyklopädien heutzutage vor allem an biografisches und historiografisches Wissen und weniger an naturwissenschaftliches, so war dies um 1700 umgekehrt. Damals entstanden die "dictionnaires des arts et des sciences", Wörterbücher der (mechanischen, handwerklichen) Künste und der Wissenschaften. Biografische und historiografische Informationen fehlten großteils. Als Wörterbücher brachen sie, im Unterschied zu den meisten früheren Werken, mit der thematischen Anordnung. Mit Antoine Furetières "Dictionnaire universel des arts et sciences" (1690) begann diese neue Richtung in der Geschichte der Enzyklopädie. Vergleichbar waren das "Lexicon technicum" (1704) von John Harris und dann die "Cyclopaedia" (1728) von Ephraim Chambers.

Doch schon in direkter Nachfolge dieser erfolgreichen Werke kam es zu einem weiteren Schritt, der Überbrückung des Gegensatzes von naturwissenschaftlich-philosophischem und biografisch-historischem Nachschlagewerk. Hier ist nicht zuletzt das eben in diesem Sinne benannte "Universal-Lexicon" (1732–1754) von Johann Heinrich Zedler hervorzuheben. Das in 64 Bänden herausgegebene Großwerk war die erste Enzyklopädie mit Biografien noch lebender Personen.

Die mit Abstand berühmteste Enzyklopädie der Geschichte ist die große französische "Encyclopédie" (1751–1772). Sie führte zwar kaum eigentliche Neuerungen ein, wurde aber gerühmt wegen ihres Umfanges, der thematischen Breite, der systematischen Unterbauung, der vielen Abbildungen, nämlich zweitausendfünfhundert, während die Konkurrenten allenfalls einige hundert Abbildungen aufwiesen. Dennoch war sie weniger erfolgreich und einflussreich als oft angenommen, denn allein schon wegen ihrer schieren Größe erreichte sie relativ wenige Leser, verglichen etwa mit der weitverbreiteten und mehrfach wiederaufgelegten "Cyclopaedia".

Vor allem gilt sie mit ihrer kritischen und weltlichen Einstellung als Schmuckstück der Aufklärung, der gesamteuropäischen Bildungsoffensive. Angriffe von Seiten der Kirche und Schwierigkeiten mit der Zensur überschatteten ihre Entstehung ebenso wie späteren Streitigkeiten zwischen den Herausgebern Denis Diderot und Jean-Baptiste le Rond d’Alembert. Diderot und viele seiner Mitautoren brachten an verschiedenen Stellen in der Enzyklopädie Kritik gegen bestimmte Vorstellungen in der herrschenden Gesellschaft an. Das Werk war als solches das Ergebnis der Leistung vieler Enzyklopädisten und konnte wohl aber letztlich nur dank des Einsatzes von Louis de Jaucourt endgültig fertiggestellt werden, letzterer stellte sogar auf eigene Kosten Sekretäre ein. In den letzten zehn Bänden, die er großteils selbst geschrieben hat, gibt es weniger polemische Fundstellen als in den ersten sieben, was sie für den heutigen Leser weniger interessant machen könnte.

Im englischsprachigen Raum blühte die "Encyclopaedia Britannica", zunächst in Schottland herausgegeben, ab dem 20. Jahrhundert in den USA. Die erste Auflage (1768–1771) bestand aus drei Bänden und war in Qualität und Erfolg eher bescheiden. Die Qualitätsverbesserung der zweiten Auflage trug zum Erfolg der dritten bei, die bereits 18 Bände umfasste. Wenn die "Encyclopaedia Britannica" die Zeiten überdauerte, während die große französische "Encyclopédie" ihren letzten, bescheidenen und umgeformten Nachfolger 1832 hatte, lag dies am Mut der Herausgeber, Neuerungen zuzulassen. Außerdem war die politische Entwicklung in Großbritannien ruhiger als in Frankreich, das unter den Folgen der Revolution von 1789 zu leiden hatte.

Um 1800 trat ein neuer erfolgreicher Typus der Enzyklopädien auf. Entstanden war er aus dem Konversationslexikon, das zunächst Renatus Gotthelf Löbel mitgestaltet hatte. 1808 wurde sein unvollendetes Werk, 1796 begonnen, von Friedrich Arnold Brockhaus aufgekauft. Es behandelte zeitgenössische Themen über Politik und Gesellschaft, um eine gebildete Unterhaltung in einer sozial durchaus gemischten Gruppe zu ermöglichen. Mit den Auflagen von 1824 und 1827 ging der Verlag F. A. Brockhaus dazu über, zeitlosere Themen aus der Geschichte, später auch aus Technik und Naturwissenschaft zu bevorzugen, da die stete Erneuerung der Bände mit aktuellen Themen zu teuer wurde.

Im "Brockhaus" waren die Themen auf viele kurze Artikel aufgeteilt, wodurch das Lexikon schnell über einen Begriff informieren konnte. Ähnlich machte es auch die "Britannica", die anfänglich noch teilweise aus langen Artikeln bestanden hatte. Während der "Brockhaus" von den Geisteswissenschaften her kam und die Naturwissenschaften später integrierte, war es bei der "Britannica" umgekehrt.

In jenem Jahrhundert wurde das Schulwesen in den europäischen Ländern erheblich ausgeweitet. Zusammen mit drucktechnischen Verbesserungen führte dies dazu, dass immer mehr Menschen lesen konnten. Gab es um 1800 im deutschsprachigen Raum 470 Verlagsbuchhandlungen, so waren es hundert Jahre später im Deutschen Reich 9360. Entsprechend wurden Enzyklopädien nicht mehr in Auflagen zu mehreren Tausend, sondern zu mehreren Zehntausend oder gar Hunderttausend gedruckt. Von 1860 bis 1900 bemühten die Enzyklopädien sich um eine gleichmäßigere Behandlung und um Standardisierung. Die Wertschätzung für statistisches Material war groß.

In Deutschland teilten sich vor allem der "Brockhaus", der "Meyer", der "Pierer" und für das katholische Publikum der "Herder" den Markt. "Brockhaus" und "Meyer" hatten je ein Drittel Marktanteil. Daneben gab es Ende des 19. Jahrhunderts etwa fünfzig weitere Verlage, die Enzyklopädien anboten. Manche Enzyklopädien schlossen mit ihrem Namen bewusst an einen berühmten Vorläufer an, so die "Chambers’ Encyclopaedia" der Brüder Chambers, die nur dem Namen nach an die "Cyclopaedia" von Ephraim Chambers erinnerte.

Um 1900 verfügten die meisten westlichen Länder über wenigstens eine umfangreiche und neuere Enzyklopädie. Manche konnten eine Tradition von fünfzig oder gar hundert Jahren vorweisen. Fachleute behandelten in der Sprache des betreffenden Landes viele Themen. Die Beiträge waren in alphabetischer Reihenfolge und schlossen Biografien lebender Personen mit ein, ebenso Bebilderungen, Landkarten, Querverweise, Indizes und Literaturlisten am Ende längerer Artikel. Wich eine Enzyklopädie von diesem Konzept ab, überlebte sie nicht lange. Doch auch die übrigen kamen nur über ein oder zwei Auflagen hinaus, wenn fähige Herausgeber dahinter standen. Ferner konnten Revolutionen und Weltkriege gute Enzyklopädien zu Fall bringen.

Der Erste Weltkrieg hat die Entwicklung teilweise unterbrochen, und unter anderem in Deutschland erschwerte die Inflation zunächst die Wiederaufnahme. Bei Meyer etwa führte dies zu der Entscheidung, den "Großen Meyer" von 20 auf zwölf Bände zu verkleinern, wodurch ein neuer, mittelgroßer Enzyklopädie-Typ entstand. In den 1920er-Jahren wandten die Großenzyklopädien sich an ein deutlich breiteres Publikum als vor dem Krieg und legten noch mehr Wert auf die sachliche Darstellung. Das Layout war moderner, es gab mehr Abbildungen; beim "Brockhaus" (ab 1928) wurden farbige Bilder per Hand eingeklebt. Die Werbung wurde erheblich ausgeweitet, in Kundenzeitschriften und Informationsbroschüren stellte Brockhaus nicht nur das Produkt, sondern auch Idee und Beteiligte vor; Marktanalysen wurden eingeführt.

Eine Herausforderung eigener Art waren die totalitären Regime. Beispielsweise im nationalsozialistischen Deutschland (1933–1945) wurde der Angestelltenbereich des Brockhaus-Verlags gleichgeschaltet, inhaltlich musste man Zugeständnisse an die parteiamtliche Prüfungskommission machen. So nahm der 1933 neu aufgelegte "Kleine Brockhaus" aktualisierte Biografien zu Hitler, Göring und anderen NS-Größen auf, ebenso neue politische Begriffe. Die Parteiideologen waren damit nicht zufrieden, aber der Verlag verwies auf das internationale Ansehen des "Brockhaus", das auch aus wirtschaftlichen Gründen nicht gefährdet werden dürfe. Wesentlich weniger zurückhaltend war das Bibliographische Institut. Seine Vorstandsmitglieder schlossen sich rasch der NSDAP an, 1939 bewarb man den "Meyer" als einziges parteiamtlich empfohlenes Großlexikon.

In den Jahrzehnten nach dem Zweiten Weltkrieg boomten Enzyklopädien und ihre Verlage. Im deutschsprachigen Raum führte das dazu, dass die beiden bedeutendsten Enzyklopädie-Verlage, F. A. Brockhaus und Bibliographisches Institut (Meyer), eine starke Konkurrenz von Seiten anderer Verlage erlebten. Vor allem Großverlage erschlossen mit populären Nachschlagewerken eine breitere Leserschaft und einen erheblichen Marktanteil bei den kleinen und mittelgroßen Enzyklopädien. Piper brachte 1972 ein Jugendlexikon heraus, Bertelsmann kam mit der zehnbändigen "Lexikothek" (1972, mit thematischen Zusatzbänden), Droemer-Knaur zwei Jahre später ebenfalls mit einem zehnbändigen Werk. Die Einzelhandelsketten Kaufhof und Tchibo boten einbändige Lexika an. Brockhaus und Bibliographisches Institut fusionierten 1984; im Jahre 1988 kam Langenscheidt als Mehrheitsaktionär hinzu, womit einem großzügigen Angebot von Robert Maxwell begegnet wurde.

Bereits in der ersten Hälfte des 20. Jahrhunderts gab es Ideen zu einer neuartigen Form der Enzyklopädie. Der Science-Fiction-Autor H. G. Wells träumte um 1938 beispielsweise von einer "World Encyclopaedia", die keine hastig geschriebenen Artikel anbieten solle, sondern sorgfältig zusammengestellte Auszüge, die beständig von Experten überprüft werden. Wells glaubte an den damals neuen Mikrofilm als billiges und universelles Medium.

Dreißig Jahre später kommentierte der Enzyklopädie-Experte Robert Collison, dass die perfekte Enzyklopädie sich wohl nie in der von Wells vorgestellten Form verwirklichen lasse. Es gebe diese perfekte Enzyklopädie bereits in der unperfekten Form der großen Bibliotheken, mit Millionen von Büchern, durch Indizes und Kataloge erschlossen. Eine Schar von Bibliothekaren und Bibliografen stellten das alles Einzelpersonen oder Gruppen der Öffentlichkeit zur Verfügung. Täglich lieferten Autoren und Herausgeber neue Bücher und Artikel.
In den 1980er-Jahren kamen die "PCs" in die Privathaushalte. Doch die elektronische beziehungsweise digitale Herausforderung wurde von den Enzyklopädie-Verlagen lange Zeit nicht erkannt. Im Vorwort der 26-bändigen niederländischen "Winkler Prins" von 1990 heißt es, die Redaktion habe die eventuelle Anwendung neuer, elektronischer Medien untersucht. Doch für das Hintergrundwissen, wie diese Enzyklopädie es anbiete, sei und bleibe die klassische Buchform das handlichste Medium.

1985 wollte die Software-Firma Microsoft eine Enzyklopädie auf CD-ROM herausbringen. Der gewünschte Partner, "Encyclopaedia Britannica", schlug eine Zusammenarbeit jedoch aus. Damals hatten nur vier bis fünf Prozent der US-Haushalte einen Computer, außerdem fürchtete der Britannica-Verlag um das aufgebaute intellektuelle Image der eigenen Enzyklopädie.
In den 1990er-Jahren kam dann der große Durchbruch der elektronischen Enzyklopädien. Der "Brockhaus" sah 2005/2006 jedoch auch einen rückläufigen Trend: Enzyklopädien würden wieder gedruckt werden. Er verwies auf sich selbst sowie auf die französische "Encyclopædia Universalis" (2002) und die "Encyclopaedia Britannica" (2002/2003). Es sei von einer dauerhaften doppelgleisigen Entwicklung mit elektronischen und Printenzyklopädien auszugehen.

1985 erschien bereits eine reine Text-Enzyklopädie auf CD-ROM, "Academic American Encyclopedia" von Grolier, auf der Basis des Betriebssystems DOS. Dann brachte im April 1989 der Britannica-Verlag eine CD-ROM-Enzyklopädie heraus, allerdings nicht das Flaggschiff unter eigenem Namen. Man veröffentlichte vielmehr eine Multimedia-Version der erworbenen "Compton’s Encyclopaedia".

Microsoft seinerseits hatte 1989 die auslaufende "Funk and Wagnalls Standard Reference Encyclopedia" aufgekauft, die billig in Supermärkten angeboten worden war. Mit einem sehr kleinen Mitarbeiterstab wurden die Texte aufgefrischt und erweitert, auch mit Bildern und Audio-Dateien versehen. 1993 kamen sie dann als "Microsoft Encarta" heraus. Die Kunden erhielten sie zusammen mit dem Computer-Betriebssystem Windows, sonst kostete sie hundert Dollar. Damals besaßen schon zwanzig Prozent der US-Haushalte einen Computer.

Ein Jahr später folgte Britannica mit einer CD-ROM-Version der "Encyclopaedia Britannica". Diese erhielt man als Zugabe zur Druckversion oder aber für stattliche 1200 Dollar. Bis 1996 senkte Britannica den Preis auf zweihundert Dollar, doch da beherrschte die "Microsoft Encarta" den Markt für digitale Enzyklopädien bereits. Britannica war von dem Ansehen seiner Enzyklopädie so überzeugt gewesen, dass es den neuartigen Konkurrenten nicht ernst genommen hatte. Von 1990 bis 1996 sanken die Einkünfte aus der "Encyclopaedia Britannica" von 650 Millionen auf nur noch 325 Millionen Dollar jährlich. Der Eigentümer verkaufte sie 1996 für 135 Millionen an einen Schweizer Investor.

Schon 1983 erschien mit der "Academic American Encyclopedia" die erste Enzyklopädie, die sich online präsentierte und ihren Inhalt über kommerzielle Datennetze wie CompuServe anbot. Als das Internet einen eigentlichen Massenmarkt erschloss, waren die ersten Online-Enzyklopädien 1995 die "Academic American Encyclopedia" sowie die "Encyclopaedia Britannica".

Jene Enzyklopädien waren nur gegen Bezahlung aufrufbar. Normalerweise zahlte der Kunde ein Jahresabonnement für den Zugang. Daneben kam es zu Vorschlägen für Online-Enzyklopädien auf der Grundlage Freien Wissens: Die Inhalte sollten unter gewissen Bedingungen wie der Herkunftsnennung frei und kostenlos bearbeitbar und weiterverbreitbar sein. Dieser Gedanke tauchte zwar noch nicht ausdrücklich in Rick Gates’ Aufruf zu einer "Internet Encyclopedia" von 1993 auf, wohl aber in Richard Stallmans Ankündigung (1999) einer "Free Universal Encyclopaedia" im Rahmen des GNU-Software-Projektes.

Als der Internet-Unternehmer Jimmy Wales und sein Angestellter Larry Sanger im Jahre 2000 die "Nupedia" online stellten, war das Echo gering. Nennenswerten Andrang erhielt eine „freie“ Internet-Enzyklopädie erst, als Wales und Sanger das Wiki-Prinzip einführten. Bei einer solchen Website kann der Leser selbst unmittelbar Veränderungen anbringen. Der 15. Januar 2001 gilt als der Geburtstag der "Wikipedia", die seitdem zur mit Abstand größten Enzyklopädie angewachsen ist. Sie wird fast ausschließlich von Freiwilligen geschrieben, die Kosten werden durch Spenden an die Betreiber-Stiftung gedeckt, die gemeinnützige Wikimedia Foundation.

Anfänglichen Zweifeln an der Zuverlässigkeit der "Wikipedia" wurde von mehreren Studien begegnet, dass die Fehlerrate vergleichbar mit der in traditionellen Enzyklopädien sei. Kritischer sind Vergleiche mit Fachenzyklopädien und Fachliteratur. Qualität hat aber nicht nur mit sachlicher Korrektheit zu tun, wie der Historiker Roy Rosenzweig 2006 anführte, sondern auch mit gutem Stil und Prägnanz. Hier lasse die "Wikipedia" noch oft zu wünschen übrig.

Außer der "Wikipedia" existieren weitere Online-Enzyklopädien, teils auf anderen Grundlagen beruhend. So verlangt "Citizendium" (seit 2006) beispielsweise die namentliche Registrierung der Autoren, die ausgewiesene Fachleute für ihr Thema sein sollen. "Google Knol" (2008–2011) überschreitet die Grenzen einer Enzyklopädie und gibt den Autoren größte Freiheit, inhaltlich und bezüglich der Eigentümerschaft ihrer Texte. "Wissen.de" (seit 2000) hat ein breites Angebot auch von nicht unbedingt enzyklopädischen Inhalten, mit Quizfragen und viel Multimedia.

Dadurch ist die Nachfrage nach Printenzyklopädien und kostenpflichtigen elektronischen Enzyklopädien stark zurückgegangen. 2009 gab die "Microsoft Encarta" auf, die "Britannica Online" bemüht sich, mit Anzeigen zu überleben. Dabei hat sie sich teilweise der "Wikipedia" angepasst, denn sie ist kostenlos zugänglich und ruft die Leser zu Verbesserungen auf, die allerdings von Angestellten kontrolliert werden. Der "Brockhaus" wurde 2009 von der Bertelsmann-Tochter Wissen Media übernommen; das Bundeskartellamt hatte trotz der marktbeherrschenden Position von Bertelsmann die Übernahme genehmigt, da der Lexikonmarkt zu einem Bagatellmarkt geschrumpft sei.

Das Internet liefert zwar eine Vielzahl von Informationen, stellten zwei Autoren 1998 fest. Der anfängliche Enthusiasmus sei allerdings einer Ernüchterung gewichen, denn der Zugang zu immer mehr Informationseinheiten erhöhe nicht den Gebrauchswert. Suchanfragen lieferten viele und zufällige Treffer, so dass die Informationsmenge nur durch Kontexte und konsistente Vernetzungen bewältigt werden könne. Das erfordere aber eine oft unterschätzte Leistung menschlicher Intelligenz.

Das Wort "allgemein" bei "allgemeines Nachschlagewerk" bezieht sich sowohl auf das allgemeine Publikum als auch auf die Allgemeinheit (Universalität) des Inhalts. Fachenzyklopädien (auch Spezialenzyklopädien genannt) beschränken sich auf ein bestimmtes Fach wie die Psychologie oder ein Themengebiet wie die Dinosaurier. Oft, wenn auch nicht notwendig, sprechen sie eher ein Fachpublikum an als ein allgemeines Publikum, denn vor allem Fachleute interessieren sich für das Fach in besonderem Maße. Zur Abgrenzung von der Fachenzyklopädie nennt man die allgemeine Enzyklopädie zuweilen auch Universalenzyklopädie. Definiert man eine Enzyklopädie allerdings als ein fächerübergreifendes Nachschlagewerk, dann ist "Universalenzyklopädie" ein Pleonasmus und "Fachenzyklopädie" ein Oxymoron.

Wenngleich die meisten Fachenzyklopädien ebenso wie die allgemeinen Enzyklopädien nach dem Alphabet geordnet sind, so hat sich bei Fachenzyklopädien die thematische Anordnung noch etwas stärker gehalten. Allerdings erhalten fachlich begrenzte Nachschlagewerke in thematischer Anordnung normalerweise die Bezeichnung Handbuch. Die systematische Anordnung bietet sich an, wenn das Fach bereits selbst stark einer Systematik folgt, wie die Biologie mit der binären Nomenklatur.

Als vielleicht erste Fachenzyklopädie kann die "Summa de vitiis et virtutibus" (12. Jahrhundert) angesehen werden. Darin behandelte Raoul Ardent die Theologie, Christus und die Erlösung, das praktische und asketische Leben, die vier Haupttugenden, das menschliche Verhalten.
Von einzelnen Ausnahmen abgesehen entstanden Fachenzyklopädien vor allem seit dem 18. Jahrhundert, und zwar auf dem Gebiet der Biografie, wie das "Allgemeine Gelehrten-Lexicon" (1750/1751). Fachenzyklopädien folgten oft dem Aufstieg des entsprechenden Faches, so kam es im späten 18. Jahrhundert zum "Dictionary of Chemistry" (1795) und auch danach zu vielen weiteren Chemiewörterbüchern. Vergleichbar war der Publikationsreichtum nur auf dem Gebiet der Musik, beginnend mit dem "Musikalischen Lexikon" (1732) des Komponisten Johann Gottfried Walther. Auf ihrem Gebiet ohnegleichen ist die "Real-Encyclopädie der classischen Altertumswissenschaft" (1837–1864, 1890–1978).

Eine der bekanntesten populären Fachenzyklopädien wurde "Brehms Thierleben", begründet von dem Sachbuchautor Alfred Brehm 1864. Es erschien im Bibliographischen Institut, das auch "Meyers Konversations-Lexikon" herausbrachte. Die große Ausgabe aus den 1870er-Jahren hatte bereits 1.800 Abbildungen bei über 6.600 Seiten und zusätzlich Bildtafeln, die auch gesondert, zum Teil eingefärbt, erhältlich waren. Die dritte Auflage 1890–1893 setzte 220.000 Exemplare ab. 1911 brachten Tiermalerei und Naturphotographie ein neues Niveau der Abbildungen mit sich. Das Werk wurde, schließlich auch digital, bis ins 21. Jahrhundert weitergeführt.

Seit dem Ende des 19. Jahrhunderts erschienen ferner Enzyklopädien über bestimmte Länder oder Regionen. Dabei sind die geografischen Enzyklopädien von den Nationalenzyklopädien zu unterscheiden, die sich auf ihr eigenes Land konzentrieren. Beispiele sind das "Deutsche Kolonial-Lexikon" (1920), "The Modern Encyclopaedia of Australia and New Zealand" (1964) und das "Magyar életrajzi lexikon" (1967–1969). Der letzte Band der "Großen Sowjetischen Enzyklopädie" (1. Auflage) hatte sich ausschließlich mit der Sowjetunion beschäftigt, er wurde 1950 als zweibändige "Enzyklopädie der Union der Sozialistischen Sowjetrepubliken" in der DDR veröffentlicht. Der "Fischer Weltalmanach" (seit 1959) behandelt die Länder der Welt in alphabetischer Reihenfolge, und zwar in aktuell gehaltenen Bänden pro Jahr.

Das größte jemals in deutscher Sprache gedruckte Lexikon hatte 242 Bände. Das Werk mit dem Titel "Oeconomische Encyclopädie" wurde zwischen 1773 und 1858 großteils von Johann Georg Krünitz herausgegeben. Die Universität Trier hat dieses Werk vollständig digitalisiert und online verfügbar gemacht.

Enzyklopädien hatten bis in die Frühe Neuzeit eher den Charakter von Sach- oder Lehrbüchern. Schwieriger noch scheint die Unterscheidung zwischen Enzyklopädien und Wörterbüchern zu sein. Es gibt keine scharfe Trennung nach Sachverhalten und Wörtern, denn kein Sprachwörterbuch kommt ohne Sacherklärung aus, kein Sachwörterbuch wie eine Enzyklopädie kann auf sprachliche Hinweise verzichten.

Die einzelnen Beiträge zu einer Enzyklopädie sind entweder alphabetisch oder nach einem anderen System geordnet. Im letzteren Fall spricht man häufig von einer „systematischen“ Anordnung, wenngleich auch das Alphabet als System angesehen werden kann und daher der Ausdruck „nichtalphabetisch“ korrekter wäre. Die systematisch angeordneten Enzyklopädien kann man ferner danach unterscheiden, ob die Einteilung eher pragmatischer oder gar willkürlicher Art ist, oder ob ein philosophisches System dahinter steckt. Anstelle von „systematisch“ verwendet man oft auch den Ausdruck „thematisch“.

Für den wahren Gelehrten sei allein die systematische Anordnung zufriedenstellend, schrieb Robert Collison, weil sie nahe verwandte Themen nebeneinanderlege. Dabei ging er davon aus, dass die Enzyklopädie als ganze oder zumindest in großen Stücken gelesen wird. In der Natur gibt es aber keine zwingenden Zusammenhänge. Systeme sind beliebig, weil sie durch einen menschlichen Reflexionsprozess zustande kommen. Dennoch hat eine systematische Darstellung einen didaktischen Wert, wenn sie logisch und praktikabel ist.

Plinius beispielsweise hat viele verschiedene Ordnungsprinzipien verwendet. In der Erdkunde beginnt er mit der vertrauten Küstenlinie Europas und schreitet dann fort zu exotischeren Erdteilen; die Menschen behandelt er vor den Tieren, da die Menschen wichtiger seien; in der Zoologie beginnt er mit den größten Tieren; bei den Seelebewesen mit denen des Indischen Ozeans, weil diese am zahlreichsten seien. Der erste behandelte römische Baum ist die Weinrebe, da sie am nützlichsten ist. Die Künstler erscheinen in der chronologischen Reihenfolge, Edelsteine nach ihrem Preis.

Eine systematische Anordnung war traditionell die übliche, bis seit dem 17./18. Jahrhundert die alphabetische sich durchsetzte. Dennoch gab es auch noch danach einzelne größere nichtalphabetische Werke, wie die unvollendet gebliebene "Kultur der Gegenwart" (1905–1926), die französische "Bordas Encyclopédie" von 1971 und die "Eerste Nederlandse Systematisch Ingerichte Encyclopaedie" (ENSIE, 1946–1960). In der ursprünglich zehnbändigen ENSIE sind einzelne namentlich gezeichnete Großbeiträge nach thematischer Ordnung aufgeführt. Für die Suche nach einem einzelnen Gegenstand muss man das Register bemühen, das wiederum eine Art Lexikon für sich ist.

Nachdem die Enzyklopädien meist alphabetisch angeordnet wurden, brachten viele Autoren doch noch im Vorwort oder in der Einleitung eine Wissenssystematik an. Die "Encyclopaedia Britannica" hatte (wie schon der "Brockhaus" 1958) seit 1974 einen einführenden Band namens "Propaedia". Darin legte der Herausgeber Mortimer Adler einleitend die Vorzüge eines thematischen Systems dar. Damit könne man einen Gegenstand finden, selbst wenn man die Bezeichnung nicht kennt. Der Band schlüsselte das Wissen auf: zunächst in zehn Großthemen, innerhalb dieser in eine Vielzahl an Sektionen. Am Ende der Sektionen wurde auf entsprechende konkrete Artikel verwiesen. Später fügte die "Encyclopaedia Britannica" jedoch noch zwei Index-Bände hinzu. Bei der "Propaedia" heißt es, sie diene vor allem dazu zu zeigen, welche Themen behandelt werden, während der Index zeige, wo diese behandelt werden.

1985 ergab eine Umfrage unter amerikanischen wissenschaftlichen Bibliotheken, dass 77 Prozent die neue Anordnung der "Britannica" weniger nützlich fanden als die alte. Eine Antwort kommentierte, die "Britannica" käme mit einer vierseitigen Anleitung daher. „Alles, das so viel Erklärung benötigt, ist verdammt nochmal zu kompliziert.“

Keine Enzyklopädie an sich, aber doch enzyklopädischer Art sind Sachbuchreihen, in denen nach einem einheitlichen Konzept viele verschiedene Themen behandelt werden. International zu den bekanntesten gehört die 1941 gegründete französische Reihe "Que sais-je ?" mit über dreitausend Titeln. In Deutschland erscheint bei C. H. Beck die Reihe "C. H. Beck Wissen".

Lange Zeit gab es überhaupt nur wenige Texte in alphabetischer Anordnung. Es handelte sich im Mittelalter vor allem um Glossare, also kurze Wörtersammlungen, oder Listen wie zum Beispiel von Arzneien. Glossare entstanden seit dem 7. Jahrhundert, und zwar dadurch, dass Leser sich schwierige Wörter auf Einzelblättern (nach Anfangsbuchstaben) notierten und dann daraus eine Liste machten. Die alphabetische Anordnung befolgte man meist nur nach dem ersten oder höchstens dritten Buchstaben, wobei man nicht sehr konsequent vorging. Viele Wörter hatten zudem noch keine einheitliche Schreibweise. Selbst im 13. Jahrhundert war die strenge alphabetische Reihenfolge noch selten.

Als einige der wenigen frühen alphabetischen Enzyklopädien werden unter anderem genannt: "De significatu verborum" (2. Hälfte des 2. Jahrhunderts) von Marcus Verrius Flaccus; "Liber glossarum" (8. Jahrhundert) von Ansileubus; und vor allem die "Suda" (um 1000) aus dem Byzantinischen Reich. Sie haben allerdings eher den Charakter von Sprachwörterbüchern; bezeichnenderweise sind die Einträge in der "Suda" meist sehr kurz und befassen sich oft mit sprachlichen Themen, etwa mit Redewendungen. Nach den alphabetischen Werken des 17. Jahrhunderts war es dann vor allem die große französische "Encyclopédie" (1751–1772), die den Begriff „Enzyklopädie“ endgültig mit der alphabetischen Anordnung verband.

Für den schnellen Zugriff ist die alphabetische Anordnung von Vorteil und von den meisten Lesern erwünscht. Eine dieser Enzyklopädien, die "Grote Oosthoek", meinte 1977 im Vorwort, es handele sich um eine Frage der Nützlichkeit, nicht des wissenschaftlichen Prinzips. Die schnelle Information aus fremden Fachgebieten erhalte man durch einen großen Reichtum an Stichwörtern, so spare man Zeit und Energie. Laut Umfrage von 1985 ist "ready reference", das schnelle Nachschlagen, der wichtigste Zweck einer Enzyklopädie, während das systematische Selbststudium wesentlich seltener genannt wurde.

Selbst diejenigen Enzyklopädisten, die für die systematische Einteilung plädierten, folgten aus praktischen Gründen dem Alphabet. Dazu gehörte auch Jean-Baptiste le Rond d’Alembert von der großen französischen "Encyclopédie". Ein späterer Herausgeber und Bearbeiter dieses Werks, Charles-Joseph Panckoucke, wollte wieder eine thematische Anordnung durchsetzen. Doch er verteilte die Artikel nur auf verschiedene Sachgebiete, und innerhalb dieser Sachgebiete erschienen die Artikel in alphabetischer Reihung. Diese "Encyclopédie méthodique par ordre des matières" war also eine Sammlung von 39 Sachwörterbüchern.

Für den Herausgeber war es einfacher, wenn ein größeres Werk thematisch aufgeteilt war. Thematisch abgegrenzte Bände konnten einer nach dem anderen geplant werden, während alphabetische bis zum letzten Eintrag (zumindest theoretisch) von vorneherein feststehen mussten. Man musste alle Lemmata (Stichwörter) kennen und die Querverweise vereinbaren.

Auch innerhalb der alphabetisch angeordneten Werke gibt es immer noch eine Reihe von unterschiedlichen Möglichkeiten. So können Artikel zu Einzelthemen lang oder kurz sein. Das ursprüngliche Konversationslexikon "Brockhaus" ist das typische Beispiel für eine Kurze-Artikel-Enzyklopädie, mit vielen, dafür kurzen, einen einzelnen Gegenstand beschreibenden Artikeln. Für den Zusammenhang sorgen Querverweise auf andere Artikel oder vereinzelte zusammenfassende Beiträge.

Lange-Artikel-Enzyklopädien hingegen enthalten große, an lehrbuchartige Abhandlungen erinnernde Artikel zu relativ weiten Themen. Ein Beispiel ist der "Macropaedia" genannte Teil der "Encyclopaedia Britannica" in den 1970er- bis 1990er-Jahren. Hier ist es für den Leser nicht immer deutlich, in welchem Großartikel er den ihn interessierenden Gegenstand suchen muss. Als Nachschlagewerk gut nutzbar ist eine solche Enzyklopädie daher nur mit einem Index, ähnlich wie bei einer systematischen Anordnung.

Die Idee, lange, überblickende Artikel zu verwenden, hatte erstmals möglicherweise Dennis de Coetlogon mit seiner "Universal history". Sie diente wohl der "Encyclopaedia Britannica" als Vorbild (diese hatte ursprünglich zum Teil lange Artikel, "treatises" oder "dissertations" genannt). Längere Artikel waren auch eine Gegenbewegung zum immer definitorischer und stichwortartiger werdenden Lexikon. Allerdings konnten lange Artikel nicht nur einer bewussten Abkehr von den eher kurzen "dictionnaire"-Artikeln entstammen. Manchmal waren sie Folge einer schwachen Redaktionspolitik, welche die Schreiblust der Autoren wenig einschränkte oder Texte einfach kopierte.

Für die praktische Nutzung einer Enzyklopädie wurden im Laufe der Zeit verschiedene Hilfsmittel entwickelt. Schon im Altertum war es gängig, einen langen Text in Kapitel aufzuteilen. Entsprechende Inhaltsverzeichnisse sind hingegen eine relativ späte Entwicklung. Sie entstanden aus Titeln der Werke. Vor dem 12. Jahrhundert waren sie noch sehr selten und wurden erst im 13. Jahrhundert geläufig.

So hat die "Naturalis historia" ein von Plinius verfasstes "summarium", eine Übersicht. In manchen Handschriften findet man das "summarium" ungeteilt am Beginn, manchmal auf die einzelnen Bücher zerteilt, wie es wohl im Zeitalter der Buchrollen am praktischsten war. Manchmal steht der Text sowohl am Anfang als auch noch einmal später vor den einzelnen Büchern. Wie Plinius selbst es gehandhabt hatte, ist heute nicht mehr festzustellen. Während Plinius in Prosa den Inhalt des Werkes beschrieb, machten später manche Druckausgaben daraus eine Tabelle, einem modernen Inhaltsverzeichnis ähnlich. Dabei gingen sie durchaus frei mit dem Text um und passten ihn an die vermuteten Bedürfnisse der Leser an.

Indizes, also Register von Stichwörtern, tauchten ebenfalls im 13. Jahrhundert auf und verbreiteten sich rasch. In einer Enzyklopädie hatte zuerst Antonio Zara in seiner "Anatomia ingeniorum et scientiarum" (1614) eine Art Index verwendet; wirklich taugliche Indizes kamen erst im 19. Jahrhundert in die Enzyklopädien.

Eines der ersten Werke mit Querverweisen war der "Fons memorabilium" von Domenico Bandini (ca. 1440). Spätestens im 18. Jahrhundert wurden sie gängig. Im 20. Jahrhundert gingen einige Enzyklopädien nach dem Vorbild des "Brockhaus" dazu über, den Verweis mithilfe eines Pfeilsymbols zu realisieren. Im digitalen Zeitalter verwendet man Hyperlinks.

Ein häufig wiederkehrendes Thema in der Forschung ist die Balance zwischen den Fachgebieten in einer Enzyklopädie. Diese Balance oder Ausgewogenheit fehlt zum Beispiel, wenn in einem Werk die Geschichte oder Biografie viel Raum erhalten, Naturwissenschaften und Technik hingegen deutlich weniger. In einer Fachenzyklopädie wird die mangelhafte Balance kritisiert, wenn etwa in einem altertumswissenschaftlichen Werk die politische Geschichte sehr viel ausführlicher behandelt wird als die Sozialgeschichte.

Zuweilen bezieht sich die Kritik auf einzelne Artikel, wobei gemessen wird, welches Lemma mehr Raum erhalten hat als ein anderes. Harvey Einbinder fand an der "Encyclopaedia Britannica" von 1963 beispielsweise den Artikel über William Benton bemerkenswert. Dieser amerikanische Politiker ist der Enzyklopädie zufolge im Senat „ein Verfechter der Freiheit für die gesamte Welt“ geworden. Der Artikel ist länger als der über den ehemaligen Vizepräsidenten Richard Nixon; wie Einbinder mutmaßt, weil Benton auch Herausgeber der "Encyclopaedia Britannica" war. Einbinder kritisierte auch, dass der Artikel „Music“ zwar Béla Bartok und Heinrich Schütz hoch lobte, diese Komponisten aber keine eigenen Artikel erhalten haben.

Auch vormoderne Enzyklopädien hatten in der Regel einen universalen Anspruch. Dennoch brachten die Interessen beziehungsweise die Fähigkeiten des Autors oftmals eine Begrenzung mit sich. So umfasste die "Naturalis historia" zwar Abhandlungen zur Völkerkunde und Kunst, der Schwerpunkt jedoch lag auf Wissensgebieten, die man heutzutage als naturwissenschaftlich einordnet. Im 18. Jahrhundert begannen Universalenzyklopädien damit, den Gegensatz zwischen mehr geisteswissenschaftlichen und mehr naturwissenschaftlichen Werken aufzuheben. Zum Teil sah man einem Werk seine Herkunft noch an, oder der Herausgeber entschied sich bewusst dafür, das Profil durch ein bestimmtes Gebiet oder eine bestimmte Herangehensweise zu schärfen: Der "Ersch-Gruber" folgte dem historischen Ansatz, wegen dessen Anschaulichkeit, der "Meyer" hingegen bevorzugte das Naturwissenschaftliche.

Die Frage der Ausgewogenheit ist nicht zuletzt von Bedeutung bei Werken, für die der Leser bezahlen muss. Er dürfte unzufrieden sein, wenn eine Universalenzyklopädie seiner Meinung nach zu viel Raum solchen Themen lässt, die ihn persönlich wenig interessieren, die er aber mitbezahlt. Robert Collison verweist auf die Ironie, dass die Leser möglichst vollständige Abrisse haben wollten und „unhinterfragt für Millionen von Wörtern bezahlt haben, die sie wahrscheinlich niemals lesen“, während die Enzyklopädie-Macher ebenfalls nach Vollständigkeit gestrebt und Einträge über kleine Themen geschrieben haben, die kaum jemand liest.

Die Ausgewogenheit wird aber selbst noch bei frei zugänglichen Enzyklopädien wie der "Wikipedia" diskutiert. So geht es zum Beispiel über die Frage, ob es nicht etwas über die Seriosität des Gesamtwerkes aussagt, wenn Themen der Popkultur (angeblich oder tatsächlich) überdurchschnittlich vertreten sind. Zumindest, betonte der Historiker Roy Rosenzweig, ist die Ausgewogenheit stark abhängig davon, aus welchem Erdteil und welcher sozialen Schicht die Autoren stammen.

Im Abendland war Latein lange Zeit die Sprache der Bildung und damit der Enzyklopädien. Das hatte den Vorteil, dass die Enzyklopädien auch in anderen Ländern als dem Ursprungsland gelesen werden konnten. Allerdings waren sie dadurch für die große Bevölkerungsmehrheit unzugänglich. Etwa seit dem Beginn des 13. Jahrhunderts erreichte das Wissen auch das Volk in dessen Sprachen. Französisch ist an erster Stelle zu nennen, seit etwa 1300 an zweiter Stelle in Europa Mittelhochdeutsch. Gerade Frauen haben eher in den Volkssprachen Wissen vermittelt. Ende des 15. Jahrhunderts waren volkssprachliche Enzyklopädien kein Wagnis mehr, sondern Routine.

Einige Enzyklopädien wurden übersetzt, wie zum Beispiel "Imago mundi" (ca. 1122) von Honorius Augustodunensis ins Französische, Italienische und Spanische. "De natura rerum" (ca. 1228–1244) erhielt eine Übersetzung ins Flämische und Deutsche, der "Speculum maius" (Mitte 13. Jahrhundert) ins Französische, Spanische, Deutsche und Niederländische. Später, als das Latein eine weniger große Rolle spielte, wurden erfolgreiche Enzyklopädien von einer Volkssprache in die andere übersetzt. Ab 1700 war es dann undenkbar, noch eine Enzyklopädie auf Latein herauszugeben.

Im 19. Jahrhundert waren etwa der "Brockhaus" und der "Larousse", vor allem in den kleineren Ausgaben, Vorbild für Enzyklopädien in anderen Sprachen oder wurden in diese übersetzt. Dies hatte allerdings Grenzen, da man den Inhalt an die jeweilige Sprache beziehungsweise an das jeweilige Land anpassen musste. Ein Beispiel dafür ist die "Encyclopedia Americana" (1827–1829), ein weiteres das "Enzyklopädische Wörterbuch von Brockhaus und Efron" (1890–1906), eine vom Brockhaus-Verlag mitherausgegebene Kurze-Artikel-Enzyklopädie auf Russisch. Trotz der Anpassungen wurde in beiden Fällen von Rezensenten kritisiert, die amerikanische beziehungsweise russische Geschichte und Kultur seien nicht ausreichend berücksichtigt worden.

Wissenschaftliche Forschung bezieht sich in erster Linie auf die Natur und die Handlungen des Menschen. Die Grundlage sind dann je nach Fach zum Beispiel Phänomene der Natur, Experimente, Umfragen oder historische Quellen. Darauf aufbauend verfassen Wissenschaftler Fachliteratur, oder sie reflektieren in ihren Arbeiten andere Fachliteratur. Erst nach dieser eigentlich wissenschaftlichen, nämlich forschenden Arbeit kommen Hilfsmittel an die Reihe, wie Einstiegslektüre, Atlanten oder Wörterbücher. Diese Abfolge von Quellen, Fachliteratur und Hilfsmitteln heißt im Englischen "primary", "secondary" und "tertiary sources".

Enzyklopädien sind demnach Hilfsmittel, die dem Leser einen ersten Zugang zu einem Thema verschaffen sollen. Ähnliches gilt für Lehrbücher und Wörterbücher, die historisch und der literarischen Gattung nach mit Enzyklopädien auch verwandt sind. Daraus wiederum ergeben sich der Charakter von Enzyklopädien und ihr Nutzen im Wissenskontext.

Dass Enzyklopädien sich eher am Ende der Wissensproduktion befinden, hat den Vorteil, dass die Aussagen in der Regel bereits etabliertes und kaum noch umstrittenes Wissen darstellen. Das beinhaltet aber ebenso den Nachteil, dass neue oder unkonventionelle Ideen ausgefiltert worden sind. Außerdem können sich von den Grundlagen über die Fachliteratur bis hin zu den Hilfsmitteln Fehler oder zu grobe Vereinfachungen eingeschlichen haben. Aus diesen Gründen ist immer wieder diskutiert worden, ob allgemeine Enzyklopädien von Schülern oder Studenten als Autorität zitiert werden dürfen.

An der Universität ist die Meinung verbreitet, dass allgemeine Nachschlagewerke in wissenschaftlichen Arbeiten nicht zu zitieren sind. Einbinder zufolge fanden einige Lehrer und Professoren, dass die "Encyclopaedia Britannica" keine zuverlässige Informationsquelle sei; sie warnten ihre Schüler davor, dieses Material blind in ihre eigenen Hausarbeiten eingehen zu lassen. Hingegen meint Thomas Keiderling in seiner Geschichte des "Brockhaus", in den 1920er-Jahren hätten Wissenschaftler diese Enzyklopädie für durchaus zitierfähig gehalten.

Der sprachliche Stil einer Enzyklopädie hängt vom Zweck des Werkes und bisweilen auch vom persönlichen Geschmack des Autors ab. In den Werken des Altertums ist oftmals erkennbar, dass sie Lehr- beziehungsweise Sachbücher waren und ursprünglich aus solchen zusammengestellt wurden. Bei Plinius heißt es beispielsweise im Abschnitt über die Insekten:
Im europäischen Mittelalter waren volkssprachliche Werke in Reimen verfasst, so dass die Leser den Inhalt leichter aufnehmen und sich besser merken konnten. Ein Beispiel aus "Der naturen bloeme" von Jacob van Maerlant, um 1270:

Solche Darstellungsweisen ordnen den Gegenstand in einen größeren, auch philosophischen Zusammenhang ein. Dabei können sich leicht Wertungen einschleichen, die eventuell durchaus gewollt waren. In der großen französischen "Encyclopédie" gab sich der Artikel „Philosophe“ (Philosoph) mal ironisch, mal pathetisch:
Im 19. Jahrhundert bildete sich dann der später als enzyklopädisch bekannte Stil heraus. Sprachwissenschaftlich lässt er sich nicht genau von anderen Gattungen wie wissenschaftliche Aufsätze unterscheiden. Der Autor wird unsichtbar gemacht, man verwendet Passivkonstruktionen, neigt zur Verallgemeinerung. „Ein insgesamt expositorischer Charakter der Artikel“ sei ebenfalls typisch, schreibt Ulrike Spree. Allgemeine Enzyklopädien bemühen sich um ganze Sätze, normalerweise fehlt nur im ersten Satz eines Artikels das Verb. Außer dem Lemma selbst werden zahlreiche weitere Wörter abgekürzt. Ein Beispiel aus der "Brockhaus Enzyklopädie":
Das Wissenschaftsverständnis ist meist empirisch und positivistisch, nicht deduktiv. In alphabetischen Nachschlagewerken gibt es zwar Verweise, dennoch stehen die Artikel in keinem Kontext. Diesen Kontext muss der Leser erst herstellen. So kann ein und derselbe Text bei unterschiedlichen Lesern verschiedene Assoziationen hervorrufen. Obwohl ein gewisser Telegrammstil erkennbar ist, gibt es aus didaktischen Gründen auch die gegenteilige Tendenz. Mit erhöhter Redundanz, Anschaulichkeit und Beispielen nähern Artikel sich an Lehrbücher an.

Normalerweise erheben Enzyklopädien den Anspruch, objektiv zu sein und nicht für eine Interessengruppe oder Partei zu sprechen. Im 19. Jahrhundert etwa hielt man es für möglich, die absolute Wahrheit zu ergründen und vermitteln, auch wenn einzelne Irrtümer möglich seien. Seltener haben Enzyklopädisten wie Denis Diderot den Zweifel zum methodischen Prinzip erheben wollen.

Innerhalb des Wahrheitsanspruchs sind eine Reihe an Positionen denkbar:
Oder aber Enzyklopädien ergreifen ausdrücklich Partei für eine bestimmte Gruppe, wie die gebildeten Stände, die Arbeiterklasse oder die Katholiken. Dabei sollen Interessen berücksichtigt und Irrtümer berichtigt werden. Selbst dann aber wird der Allgemeingültigkeitsanspruch nicht aufgegeben.

Enzyklopädien richten sich meist nicht gegen die bestehenden grundlegenden Vorstellungen in ihrer Gesellschaft. Pierre Bayle und Denis Diderot waren Ausnahmen. Eine ausgesprochen politische Zielsetzung hatten später beispielsweise der anti-monarchische "Grand dictionnaire universel du XIXe siècle" von Larousse, das konservative "Staats- und Gesellschaftslexikon" von Hermann Wagener, das liberale "Staatslexikon" (1834–1843) von Karl von Rotteck und Carl Theodor Welcker sowie das sozialdemokratische "Volks-Lexikon" von 1894. Solche Tendenzschriften waren allerdings eher selten.

Wenn Historiker versuchen zu erfahren, wie die Menschen in einer bestimmten Epoche über etwas gedacht haben, ziehen sie oft die damaligen Enzyklopädien zu Rate. Eine Aussage muss allerdings nicht unbedingt tatsächlich für die Gesellschaft repräsentativ sein, vielleicht spiegelt sie nur die Meinung des Autors, der Herausgeber oder einer bestimmten Bevölkerungsschicht wider.

Einige Beispiele:

Harvey Einbinder listet eine Vielzahl von Artikeln der "Encyclopaedia Britannica" auf, deren Neutralität oder Objektivität er bezweifelt. Moderne Künstler würden kurzerhand für wertlos erklärt, aus Prüderie würden wichtige Handlungselemente etwa im Theaterstück "Lysistrata" weggelassen werden oder sexuelle Themen hinter Fachausdrücken versteckt. Der Judenmord werde unverständlicherweise nicht mit der nationalsozialistischen Ideologie in Verbindung gebracht, der moralische Aspekt der Atombombenabwürfe auf Hiroshima und Nagasaki kaum diskutiert. Letzteres geschehe seiner Vermutung nach, um den Amerikanern ein unangenehmes Thema zu ersparen.

Die Herausgeber von Enzyklopädien hatten zuweilen ausdrücklich gesellschaftspolitische Ziele. Beispielsweise setzte sich insbesondere der Ergänzungsband von 1801 bis 1803 zur "Encyclopaedia Britannica" kämpferisch mit der Französischen Revolution auseinander. Widmungen an den regierenden Monarchen waren nicht ungewöhnlich, doch damals hieß es darin:
Später im 19. Jahrhundert setzte der "Meyer" sich, nach eigenem Bekunden, für eine intellektuelle Gleichheit der Menschen ein, den Lesern ein besseres Leben ermöglichen. Revolutionärem Denken sollte jedoch kein Vorschub geleistet werden. Im Gegensatz zu dieser eher liberalen Haltung wollte "Sparners Illustriertes Konversations-Lexikon" (1870) sozialdisziplinierend auf die Unterschicht einwirken.

Allgemein sehen Enzyklopädien sich oft dem Vorwurf ausgesetzt, nicht neutral zu sein. Einige Kritiker hielten die "Encyclopaedia Britannica" für prokatholisch, andere für kirchenfeindlich. Um 1970 lobten manche Rezensenten am "Brockhaus" dessen angeblich konservativen Grundton im Vergleich zum „linkslastigen“ "Meyer", andere sagten, es sei genau andersherum. Thomas Keiderling findet es überhaupt problematisch, Pauschalurteile solcher Art zu fällen.

Die niederländische "Katholieke Encyclopedie" stellte sich 1949 bewusst nicht in die Tradition der Aufklärung, sondern des christlichen Mittelalters. Wie ihre Schwester, die Universität, sei die Enzyklopädie aus katholischem Hause. Ein Prospekt, bereits aus dem Jahre 1932, nennt Unparteilichkeit gerade in einer Enzyklopädie gefährlich. Schließlich bräuchten Themen wie „Spiritismus“, „Freudianismus“, „Freimaurerei“, „Protestantismus“ oder „Liberalismus“ eine kritische Behandlung und absolute Verwerfung. „Es ist doch eindeutig, dass Neutralität keine Position beziehen kann. Aber zahlreiche Themen können ohne feste Basis nicht beurteilt werden.“ In den sogenannten neutralen Enzyklopädien erhalte Buddha mehr Aufmerksamkeit als Jesus Christus.

Die "Enciclopedia Italiana" (1929–1936) entstand in der Zeit des Faschismus und der Diktator Benito Mussolini hatte mehr oder weniger persönlich zum Thema „Faschismus“ beigetragen (vgl. "La Dottrina Del Fascismo"). Im Allgemeinen jedoch war das Werk international und objektiv. In Deutschland musste sich der "Brockhaus" in den letzten Teilen seiner Großausgabe von 1928 bis 1935 politisch anpassen. Als ausgesprochen nationalsozialistisch gefärbt gilt der sogenannte „braune Meyer“ von 1936 bis 1942 (unvollendet).

Die "Große Sowjetische Enzyklopädie" richtete sich nicht etwa an die Massen der Arbeiter und Bauern, sondern an die „Hauptkaderleute, die den sowjetischen Aufbau betreiben“. Ihre politische Ausrichtung beschrieb sie im Vorwort von 1926 so:
Noch nach dem Erscheinen musste eine sowjetische Enzyklopädie verändert werden, wenn eine Person plötzlich politisch unerwünscht wurde. Als 1953 Lawrenti Beria entmachtet wurde, schickte man den Käufern der "Großen Sowjetischen Enzyklopädie" ein Blatt unter anderem mit Informationen über die Beringsee, das man anstelle der alten Seite mit Beria einkleben sollte.

Traditionell waren Enzyklopädien eher von begrenztem Umfang. Moderne Buchausgaben antiker oder mittelalterlicher Enzyklopädien bleiben meist auf einen oder wenige Bände beschränkt. Die für das Altertum monumentale "Naturalis historia" hatte beispielsweise in einer Ausgabe um das Jahr 1900 fünf Bände. Nach eigener Zählung bestand das Werk aus 37 "libri" (Büchern), wobei ein „Buch“ hier vom Umfang her als ein Kapitel zu verstehen ist. Die "Etymologiae" des Isidor machen ein je nach Ausgabe mehr oder weniger dickes Buch aus.

Zu vielbändigen Enzyklopädien kam es erst seit dem 18. Jahrhundert, allerdings gab es gleichzeitig immer auch Nachschlagewerke in nur einem oder wenigen Bänden. Diese haben im 19. und 20. Jahrhundert, als Enzyklopädien sich massenweise verbreiteten, wesentlich mehr Käufer gefunden als die großen Ausgaben. Thomas Keiderling verwendet für das 20. Jahrhundert eine Einteilung von kleinen Ausgaben mit ein bis vier Bänden, mittleren Ausgaben von fünf bis zwölf Bänden und großen darüber. Für einen genaueren Vergleich des Umfangs müsse man jedoch zusätzlich Buchformate, Seitenanzahlen, Schriftgröße usw. hinzuziehen.

Als größte Enzyklopädie der Geschichte wird zuweilen das chinesische Werk "Yongle Dadian" (auch: "Yung-lo ta-tien") aufgeführt. Es stammt aus dem 15. Jahrhundert und umfasste 22.937 Bücher auf mehr als fünfhunderttausend Seiten. Es handelte sich jedoch mehr um eine aus älteren Texten zusammengestellte Lehrbuchsammlung.

Längere Zeit das umfangreichste Nachschlagewerk war der "Zedler" mit seinen 64 Bänden. Dieses Mammutwerk war folglich für viele Käufer, die sowieso nur einer kleinen, reichen Oberschicht entstammen konnten, unerschwinglich. Selbst viele Lesegesellschaften haben sich den "Zedler" nicht angeschafft.

Im 19. Jahrhundert war der "Ersch-Gruber" die größte allgemeine Enzyklopädie. Das 1818 begonnene Werk wurde aber nicht fertiggestellt, nach 167 Bänden gab der neue Herausgeber (Brockhaus) 1889 auf. Die größte vollständige gedruckte Enzyklopädie wurde dann im 20. Jahrhundert die spanischsprachige "Espasa" mit insgesamt neunzig Bänden. Die Großwerke des 18. und 19. Jahrhunderts erscheinen also umfangreicher als die des 20. Jahrhunderts mit ihren 20–30 Bänden, dabei ist aber das wesentlich dünnere Papier der späteren Werke zu berücksichtigen.

Eine populäre Enzyklopädie wie die "Etymologiae" des Isidor brachte es im Mittelalter auf über tausend Handschriften. Das "Elucidiarium" von Honorius Augustodunensis gab es in mehr als 380 Handschriften.

Jeff Loveland zufolge hat man im 18. Jahrhundert etwa zweihundert bis dreihundert Exemplare von einer Enzyklopädie verkauft; Ulrike Spree zufolge betrug die Auflage hingegen 2000–4000 Exemplare. Vom "Zedler" (1737) wurden vermutlich nur die 1500 Subskriptionsexemplare angeschafft, also diejenigen, die zahlungskräftige Kunden zuvor bestellt hatten. Von der ersten Auflage der (damals dreibändigen) "Encyclopaedia Britannica" (1768–1771) verkaufte man insgesamt dreitausend Exemplare, von der 18-bändigen dritten Auflage (1787–1797) dreizehntausend.

Das 19. Jahrhundert sah wesentlich höhere Auflagen. Die "Encyclopaedia Britannica" in der 7. Auflage (1828) kam auf dreißigtausend Exemplare, "Meyers Conversations-Lexikon" hatte 1848/1849 siebzigtausend Subskribenten. Da das Erscheinen langsam war und die Bandanzahl hoch, ging dies allerdings auf unter vierzigtausend zurück. Von der 2. Auflage der "Chambers Encyclopaedia" verkaufte man 1874–1888 allein in Großbritannien über 465.000 "sets".

"Brockhaus" verkaufte von seiner 13. Auflage (1882–1887) 91.000 Exemplare, von der 14. Auflage bis 1913 mehr als 300.000. Die 17. Auflage des großen "Brockhaus" von 1966 hatte eine Gesamtauflage von 240.000 Exemplaren (Komplettsets). Auf dem Gebiet der kleineren Lexika erlebte Brockhaus jedoch starke Konkurrenz. So verlief der Verkauf des einbändigen "Volks-Brockhaus" von 1955 schleppend: Er kostete 19,80 DM, während Bertelsmann sein "Volkslexikon" für 11,80 DM auf den Markt brachte und über seinen Lesering eine Million Exemplare verkaufte.

In der DDR hatte das achtbändige "Meyers Neues Lexikon" (1961–1964) eine Auflage von insgesamt 150.000 Exemplaren, die zweibändige Ausgabe kam 1956–1958 in drei Auflagen auf 300.000 Exemplare. Zwar war die DDR deutlich kleiner als die Bundesrepublik, der VEB Bibliographisches Institut hatte aber keine Konkurrenz.

Fehlende Konkurrenz führte auch in anderen kleinen Ländern, einschließlich westlichen, zu hohen Auflagen im Vergleich zur Bevölkerungsanzahl. Das sechsbändige "Uj Magyar Lexikon" erschien im kommunistischen Ungarn 1959–1962 in 250.000 Exemplaren. In Norwegen verkaufte sich das fünfzehnbändige "Store Norske" von 1977 bis 2011 in 250.000 Exemplaren bei einer Bevölkerung von nur vier Millionen Norwegern.

Von der 21. Auflage der "Brockhaus Enzyklopädie" aus den Jahren 2005/2006 wurden nur „ein paar Tausend Exemplare“ verkauft, wie der FOCUS berichtete. Der "FAZ" zufolge habe die Gewinnschwelle bei 20.000 verkauften Exemplaren gelegen, davon sei die Hälfte erreicht worden. Diese letzte gedruckte Auflage der "Brockhaus Enzyklopädie" bestand aus dreißig in Leinen gebundenen Bänden mit Goldschnitt, die fast 25.000 Seiten beinhalteten. Sie kostete 2670 Euro.

Aus den antiken Werken sind so gut wie keine Illustrationen überliefert, sondern nur der Text. Nachträglich erhielten sie Abbildungen in einigen mittelalterlichen Handschriften. Diese Illustrationen unterschieden sich meist von Handschrift zu Handschrift; dann brachte der Buchdruck die Möglichkeit, auch Abbildungen genau zu vervielfältigen. Das Mittelalter kannte bereits Bilder von Menschen, Tieren oder Pflanzen, ebenso schematische Darstellungen und Weltkarten. Sie waren allerdings selten.

In der Frühen Neuzeit gab es dann eine große Bandbreite von unterschiedlichen Illustrationen. Auf Titelblättern und Frontispizen reflektierte man über die Grundlagen des in der Enzyklopädie gesammelten Wissens, indem man die sieben freien Künste allegorisch darstellte. Baumdiagramme veranschaulichten den Zusammenhang der einzelnen Fächer, Funktionsdiagramme zeigten zum Beispiel, wie ein Flaschenzug funktioniert. Widmungen präsentierten einen reichen Gönner oder Schirmherr, Kupferstiche leiteten einen neuen Band ein. Beliebt waren auch Tabellen, zum Beispiel zu Planetenbewegungen.

Bilder wurden entweder im Text an die geeignete Stelle eingefügt oder auf gesonderten Bildtafeln geliefert; der Brockhaus-Verlag brachte 1844–1849 und auch noch später eigens einen "Bilder-Atlas zum Conversationslexikon" heraus und nannte ihn im Untertitel "Ikonographische Encyclopädie der Wissenschaften und Künste". Bildtafeln oder gar Bildbände wurden oftmals der Qualität wegen gesondert vom Rest gedruckt, da Bilder zuweilen einen besonderen Druck oder besonderes Papier verlangten. Mit der zunehmenden Verbesserung der Drucktechnik kamen mehr und mehr Bilder in die Enzyklopädien. Schließlich wurden im 20. Jahrhundert reich illustrierte Werke nicht mehr ausdrücklich als „illustriert“ angepriesen, so selbstverständlich war die Bebilderung geworden. Etwa seit den späten 1960er-Jahren waren die Abbildungen einiger Enzyklopädien vollständig in Farbe gehalten.

Die 19. Auflage des "Brockhaus" (1986–1994) hatte 24 Bände mit insgesamt 17.000 Seiten. Darin befanden sich 35.000 Abbildungen, Karten und Tabellen. Ein dazugehöriger Weltatlas beinhaltete 243 Kartenseiten.

Seit dem 18. Jahrhundert erhielten größere Enzyklopädien, wenn schon keine neue Auflage zustande kam, Ergänzungsbände, "Supplemente". Der "Brockhaus" veröffentlichte Mitte des 19. Jahrhunderts Jahrbücher als Ergänzung oder Weiterführung des eigentlichen Lexikons. Ab 1907 gab "Larousse" die Monatsschrift "Larousse mensuel illustré" heraus. Mehr zur Kundenbindung diente die Zeitschrift "Der Brockhaus-Greif", die der Verlag von 1954 bis 1975 unterhielt. Ein Sonderband konnte dazu dienen, besondere geschichtliche Ereignisse zu behandeln, wie den Deutsch-Französischen Krieg 1870/1871 oder den Ersten Weltkrieg.

Anhänge in eigenen Bänden konnten auch Bildbände, Atlanten oder Wörterbücher sein, die aus der Enzyklopädie ein umso vollständigeres Kompendium machten. CD-ROMs, Internetzugänge und USB-Sticks schließlich wurden zunächst als Zugabe für die gedruckte Version angeboten. Ein Versuch, den Wert des Gesamtwerks zu erhöhen, stellten die Künstlerausgaben des "Brockhaus" dar, wie die seit 1986 von Friedensreich Hundertwasser gestaltete, auf 1800 Exemplare limitierte Ausgabe. Der Ladenpreis betrug 14.000 DM (gegenüber etwa 4000 DM für die normale Ausgabe). Die Einbände zeigten, nebeneinander auf dem Regal stehend, zusammen ein neues Bild.

In der Regel wurden Bücher in fertigem Zustand dem Käufer überreicht, und die Zahlung erfolgte zeitnah. Bei größeren Projekten jedoch war es im 18. Jahrhundert üblich, zunächst Subskribenten zu werben und erst danach das Werk zu drucken; eventuell wurde es stückweise in Raten ausgeliefert. Hatte der Käufer alle Auslieferungen beisammen, konnte er damit zu einem Buchbinder gehen. Ein Subskribent (wörtlich: jemand, der unterschreibt) bezahlte im Voraus. So hatte der Verleger bereits ein Kapital, mit dem er erste Ausgaben bewältigen konnte. Je nach Subskriptionsmodell zahlte der Subskribent möglicherweise eine Anzahlung und dann noch pro ausgeliefertem Teil. Zusätzlich hoffte der Verleger, dass weitere Kunden das Werk kauften. Die Veröffentlichung bekannter Subskribenten vorne im Werk sollte verkaufsfördernd wirken, ähnlich wie die Widmung des Werkes an eine hochgestellte Persönlichkeit.

Im Falle der ersten Ausgabe der "Encyclopaedia Britannica" kündigte im Juli 1767 ein Prospekt das Vorhaben der Öffentlichkeit an. Im Februar 1768 ließen die Verleger verlautbaren, dass das Werk in einhundert wöchentlichen Auslieferungen kommen sollte, jeweils mit 48 Seiten. Am Ende sollten es, gebunden, sechs Bände im Oktavformat werden. Auf einfachem Papier kostete eine Auslieferung sechs Pence und acht auf besserem. Bald darauf änderten die Herausgeber das Format auf Quarto, woraus sich drei Bände ergaben. Der Grund dafür war das höhere Prestige von Quarto und vielleicht auch der indirekte Einfluss eines Konkurrenzproduktes. Im Dezember 1768 kam der erste Teil heraus, und nach der Auslieferung des letzten 1771 erhielt man Vorwort und Titelseiten für jeden der drei Bände sowie eine Anleitung für den Buchbinder. Im August 1771 konnte man das gesamte "set" für zwei Pfund und zehn Schillinge kaufen (drei Pfund, sieben Schillinge bei besserem Papier).

Im 19. Jahrhundert konnte man beispielsweise bei "Meyers Konversations-Lexikon" zwischen mehreren Liefermodellen wählen. Die dritte Auflage von 1874 bis 1878 bestand aus fünfzehn Bänden. Der Käufer erhielt wöchentlich eine Lieferung von 64 Seiten, die fünfzig Pfennig kostete; oder aber man bezahlte je 9,50 Mark pro Band. Den "Brockhaus" in der Jubiläumsausgabe von 1898, siebzehn Prachtbände zu je zehn Mark, zahlte man in Monatsraten von drei bis fünf Mark oder in Vierteljahresraten von neun bis fünfzehn Mark. Es gab keine Anzahlung, erst nach drei Monaten musste man die erste Rate zahlen. Subskriptionsmodelle kannte man letztlich bis ins 21. Jahrhundert. Es war aber seit dem 20. Jahrhundert gängig, dass man fertig gebundene Bände erhielt.

"Nelson’s perpetual loose-leaf encyclopaedia" von 1920 war eine Loseblattsammlung in zwölf Bänden. Zweimal im Jahr erhielt der Käufer einige neue Seiten geliefert, mit denen er Seiten veralteten Inhalts ersetzen konnte. Die "Encyclopédie française" (1937–1957) nahm die Idee auf, die sich aber nicht durchsetzen konnte.

Meyers Enzyklopädisches Lexikon in 25 Bänden brauchte für die Produktion und Auslieferung insgesamt acht Jahre von 1971 bis 1979. In den Bänden 4, 7, 10, 13, 16, 19 und 22 wurden Nachträge angefügt, welche die zwischenzeitlichen Aktualisierungen zu den vorangegangenen Bänden enthielten. 1985 erschien schließlich noch ein Nachtragsband (Band 26).

Der Autor einer Enzyklopädie heißt Enzyklopädist oder Enzyklopädiker, wobei dieser Begriff auch für einen Wissenschaftler der Enzyklopädik gebraucht wird, der keine Enzyklopädie schreibt, sondern Enzyklopädien und ihre Entstehung erforscht. Herausgeber und Mitarbeiter der Encyclopédie (Frankreich 1782 bis 1832) wurden Enzyklopädisten genannt.

Ein Urheberrecht im modernen Sinne gab es vor dem 19. Jahrhundert nicht. Dennoch besteht seit der Antike der Begriff des Plagiats, als ungekennzeichnete Übernahme fremder Texte. Bis ins 18. Jahrhundert war es gängig, Enzyklopädien vor allem als Zusammenstellung älterer Texte zu sehen. Dabei wurden die Autoren manchmal genannt, oft aber auch nicht. In der Antike und im Mittelalter stand der Gedanke im Vordergrund, sich bei den alten Weisen zu bedienen und von deren reinem, unverfälschtem Wissen zu lernen. Mit der Renaissance wurde die Vorstellung eines originalen Autors wichtiger.

Plagiate galten beispielsweise im 18. Jahrhundert teilweise als anrüchig, waren aber nicht verboten. Allenfalls anhand des Druckprivilegs konnte der Herausgeber Nachdrucke untersagen. Dabei handelte es sich um eine obrigkeitliche Erlaubnis, überhaupt ein bestimmtes Buch drucken zu dürfen. Nachdrucke konnten aber höchstens im eigenen Land unterbunden werden und wurden oftmals im Ausland gedruckt und dann zum Teil über Schmuggel verbreitet.

Dennis de Coetlogon zum Beispiel gab zwar zu, kopiert zu haben, behauptete aber trotzdem, er sei der Autor seiner "Universal history". Nimmt man dies wörtlich, so hat er sie anscheinend selbst mit der Hand, ohne Helfer, geschrieben. Wenn in der ersten Ausgabe der "Encyclopaedia Britannica" eine „List of Authors“ erschien, dann war damit nicht etwa gemeint, dass jene Personen bewusst für diese Enzyklopädie geschrieben hätten. Vielmehr hatte der Redakteur William Smellie sich aus ihren Werken bedient.

Im Artikel „Plagiaire“ beschrieb die große französische "Encylopédie" das Phänomen des Plagiats. Man beeilte sich anzumerken, dass Lexikografen sich wohl nicht an die üblichen Gesetze des Mein und Dein halten müssten, jedenfalls nicht jene, die ein "dictionnaire des arts et des sciences" verfassten. Schließlich gäben sie nicht vor, Originales zu schreiben. Der Text hatte größte Ähnlichkeit mit dem Artikel „Plagiary“ in Chambers’ "Cyclopaedia" eine knappe Generation zuvor, und dieser wiederum ging auf das "Dictionnaire" von Antoine Furetière (1690) zurück.

Der "Zedler" schreibt unter dem Lemma „Nachdruck derer Bücher“:
Dieser Text selbst war einem zeitgenössischen Buch entnommen worden. Im 19. Jahrhundert war es dann nicht mehr möglich, eine Enzyklopädie mit der Schere zu verfassen, wie William Smellie noch über sich selbst gescherzt haben soll. Zumindest bei den allgemeinen Enzyklopädien gab es dies nach 1860 nicht mehr. Trotzdem war die gegenseitige Beeinflussung der konkurrierenden Verlage groß, auch, weil Fakten an sich (wie die Höhe eines Berges) nicht urheberrechtlich geschützt sind.

Gilt bei antiken Werken meist eine Person als der Autor, so ist im Mittelalter der Autor nicht immer leicht greifbar. Mit dem antiken Argument der "modestia" (Bescheidenheit) bezeichnen sich die Autoren des Mittelalters oft als zu unwürdig, um ihren Namen zu nennen. Sie sahen sich als bloße Vermittler gottgewollten Wissens. Gerade Laien aber, wie König Alfons der Weise oder der Notar Brunetto Latini, neigten im Gegenteil zur Selbststilisierung. Einige Werke sind in Arbeitsgemeinschaften entstanden, wobei dann die leitende Persönlichkeit stellvertretend für die Mitarbeiter genannt wurde.

Die Autoren verstanden sich als Kompilatoren (Zusammensteller), als Übersetzer, die bewährte lateinische Werke einem größeren Publikum eröffneten. Eine neue Generation um 1300 brachte dann auch eigene Gedanken ein. Dies waren ebenfalls oft Laien, oft aus Italien, wo der Klerus eine weniger große Rolle als anderswo spielte. Die Autoren waren meist Männer; Frauen waren nur innerhalb von Klöstern enzyklopädisch tätig.

Im 19. Jahrhundert kam es nicht nur zum modernen Begriff des Autors, sondern auch zu einer erheblichen Spezialisierung. Die erste Auflage der "Encyclopaedia Britannica" wurde noch großteils von den Herausgebern geschrieben (beziehungsweise abgeschrieben). Doch Archibald Constable, der sie 1810 gekauft hatte, setzte auf wissenschaftliche Autoritäten, die auch namentlich genannt wurden. In Deutschland war die Entwicklung beim Brockhaus vergleichbar. Für nicht gekennzeichnete Artikel war die Redaktion verantwortlich. Generell mussten die Autoren sich dem Gesamtwerk unterordnen. Vor allem nach 1830 bemühten sich die Verlage um Experten. Waren die Autoren nicht genannt (wie bei den meisten Enzyklopädien), konnte es damit zu tun haben, dass diese Werke allzu sehr aus älteren Werken abgeschrieben waren. Beliebt war der Trick, als Herausgeberin eine „Gesellschaft von Gelehrten“ anzugeben.
Ulrike Spree: „Der universalistisch gebildete Lexikonautor, der Artikel zu einer ganzen Palette von Themengebieten bearbeitete, gehörte immer mehr der Vergangenheit an.“ Das gab es allenfalls noch bei ein- oder zweibändigen Werken. Trotz einiger großer Namen waren die meisten genannten Autoren unbekannte Leute. Viele haben für mehrere Enzyklopädien geschrieben. Eine der seltenen Enzyklopädien mit Autorennennung war der "Ersch-Gruber" und im 20. Jahrhundert beispielsweise "Collier’s Encyclopedia".

Thomas Keiderling zufolge seien im "Brockhaus" die Autoren anonym geblieben, weil die Artikel objektiv sein und keine Meinung einzelner wiedergeben sollten. Einige Autoren wollten nicht genannt werden, weil sie kontroverse Themen behandelten. Außerdem haben Redakteure die Beiträge überarbeitet und sind so Mitautoren geworden. Die Namensnennung hielt man nur bei namhaften Autoren für sinnvoll, es sei aber weder möglich noch wünschenswert gewesen, für jeden Artikel die herausragendsten Wissenschaftler zu engagieren. Bei einem solchen Anspruch wären redaktionelle Eingriffe wiederum bedenklich gewesen.

1879 beschrieb eine Wochenzeitschrift, wie bei Meyer das Konversations-Lexikon erstellt wurde. In der Hauptleitung in Leipzig wurden die 70.000 Artikel aus der vorherigen Ausgabe ausgeschnitten und auf Papier geklebt. Notizensammler werteten circa fünfzig Zeitungen aus und erfragten Daten von Behörden und Institutionen. In verschiedenen Universitätsstädten gab es Spezialredaktionen, und Autoren, die für ein jeweiliges Fachgebiet angeworben waren, arbeiteten die Artikel um. Weibliche Autoren gab es immer noch kaum. Eine Ausnahme machte die britische "Chambers Encyclopaedia", die aus einer Übersetzung hervorgegangen war: Übersetzungen waren oft Frauenarbeit.
Da die Hochschulen überfüllt waren, war die enzyklopädische Mitarbeit für viele Absolventen attraktiv. Typischerweise sah ein Lexikonredakteur sich als nicht öffentlich in Erscheinung tretender Generalist. Das Mitarbeiterverzeichnis des "Meyer" im Jahre 1877 führte 32 Autoren im Fach Geschichte namentlich auf. Alle waren promoviert, 14 davon Professoren. An der Konzeption der 15. Auflage des "Großen Brockhaus" (zwanzig Bände, 1928–1935) waren 57 Personen beteiligt: 22 Redakteure, zehn Büroangestellte, fünf Mitarbeiter der Bildabteilung, 15 Sekretärinnen, drei Kontorburschen. Über tausend Autoren verfassten 200.000 Artikel mit 42.000 Abbildungen, davon waren vierhundert gelegentliche und sechshundert regelmäßige Autoren. Der Verlag standardisierte Anschreiben, informierte die Autoren mit Rundschreiben und Merkblättern zu Rechtschreibfragen, Literaturangaben, Abkürzungen und Sonderzeichen. Man erhielt ein Bogenhonorar oder Pauschalbeträge nach Umfang. Weiterhin war die Anonymität vertraglich festgeschrieben.

Für das altertumswissenschaftliche Fachlexikon "Der Neue Pauly" stellte ein Erfahrungsbericht 1998 fest, dass die Anzahl der Mitarbeiter sehr hoch gewesen sei – des großen Spezialisierungsdrucks wegen: „Es gibt zahlreiche von mehreren Autoren verfaßte 'Komposit-Artikel', da sich für übergreifende Themen oder 'Dachartikel' kaum noch 'Generalisten' finden lassen. Die Einheitlichkeit der Konzeption eines Artikels – um von dem Werk im ganzen zu schweigen – ist hierdurch gefährdet.“ Neunzehn Fachredaktionen erarbeiteten gemeinsam eine Gesamtlemmaliste und koordinierten die Kommunikation mit den über siebenhundert Autoren.

Die "Wikipedia" wird von Freiwilligen geschrieben und redigiert. Sie beteiligen sich aus Interesse an einem Thema oder aus Idealismus. Außerdem schließen sie sich einer Gemeinschaft an, in der sie Wertschätzung erhalten. Die "Wikipedia"-Freiwilligen sind höhergebildet und etwa zur Hälfte unter dreißig Jahren alt.

Im 19. und 20. Jahrhundert kam es dazu, dass Enzyklopädien bekannte Wissenschaftler oder andere Prominente gewonnen haben. Berühmte Autoren der "Encyclopaedia Britannica" waren unter anderem der Schriftsteller Walter Scott, der Bevölkerungswissenschaftler Robert Malthus und der Wirtschaftswissenschaftler David Ricardo. Im deutschsprachigen Raum der 1970er-Jahre beispielsweise integrierte "Meyers Konversations-Lexikon" längere Beiträge von Prominenten. Einleitend schrieb der Wissenschaftstheoretiker Jürgen Mittelstraß „Vom Nutzen der Enzyklopädie“. Der ehemalige SPD-Bundesminister Carlo Schmid verfasste den Beitrag „Demokratie – die Chance, den Staat zu vermenschlichen“, und der ehemalige FDP-Bundeswirtschaftsminister Hans Friedrichs schrieb über die „Weltwirtschaft“.

Zu einem Problem wird dies, wenn die Prominenten Teil des öffentlichen Diskurses über ihr Fach sind. Es fällt ihnen möglicherweise schwer, einen neutralen, überblickenden Standpunkt zu beziehen. In einem Erweiterungsband der "Encyclopaedia Britannica" (1926) schrieb Leo Trotzki den Artikel über Lenin. Der ehemalige Kriegskommissar Trotzki war Lenins enger Mitarbeiter gewesen, und die Bezugnahme auf den bereits verstorbenen Lenin war ein wichtiges Instrument im politischen Streit zwischen Trotzki, Stalin und weiteren sowjetischen Politikern.

Generell wurden die Mitarbeiter von Enzyklopädien eher schlecht bezahlt. William Smellie erhielt für die Arbeit an der ersten Ausgabe der "Encyclopaedia Britannica" die Summe von zweihundert Pfund. Für vier Jahre Teilzeitarbeit war dies weder großzügig noch armselig, so Jeff Loveland, aber im Vergleich weniger, als Diderot für die größere und langwierigere Arbeit an der "Encyclopédie" bekam. Bei Chambers im 19. Jahrhundert lag das Jahresgehalt der Redakteure bei der unteren Grenze des Mittelstandes.

Im 20. Jahrhundert, berichtet Einbinder von der "Encyclopaedia Britannica", hätten sich viele Gelehrte gern beteiligt, konnten es sich aber nicht leisten, für so wenig Geld (zwei Cent pro Wort) zu schreiben. Das gelte besonders für die Geisteswissenschaften. Zwar sei die Mitarbeit aus Gründen des Prestiges sehr begehrt, doch viele hätten nur einen Artikel beitragen wollen. Überhaupt bemängelte Einbinder einen vorrangig kommerziellen Charakter der "Encyclopaedia Britannica", bei der die Gutverdiener des Verlages die Haustürverkäufer waren, und nicht etwa die Autoren.

Ein Text kann nur dann Leser finden, wenn Menschen des Lesens kundig sind, wenn sie Zeit zum Lesen haben und wenn sie sich den Lesestoff leisten können. Das hat den Kreis der möglichen Leser historisch stark eingeschränkt, noch davon abgesehen, ob die Menschen überhaupt Interesse am Inhalt hatten. Dennoch gab es Wege, die Barrieren zu überwinden: Texte wurden früher laut gelesen, so dass Leseunkundige mithören konnten, reiche Leute stellten ihre Bibliotheken einem größeren Kreis zur Verfügung, oder Gruppen von Menschen schafften sich gemeinsam Bücher an. Erst im 19. Jahrhundert weitete sich der Kreis in Europa wesentlich aus, dank staatlich geförderter Schulen und billigeren Büchern: Um 1900 konnten neunzig Prozent der Deutschen, Franzosen, Engländer und US-Amerikaner lesen. Andere Erdteile blieben zurück, in Russland war dazu nur ein Drittel der Männer in der Lage.

Plinius schrieb die "Naturalis historia" für die Volksmassen, wie Bauern und Handwerker, so behauptete er es in der an den Kaiser gerichteten Widmung. Jedenfalls sei sie für jeden, der die Zeit aufbringe, zu lesen. Seine Aussage ist so zu interpretieren, dass er an diejenigen dachte, die ein einfaches Leben in der Natur führen, entsprechend den von ihm geschätzten römischen Tugenden. Insgesamt wollte er jedoch alle Bürger des Reiches ansprechen, so wie sein Werk das Reich universal beschrieb.

Auch die Verfasser von mittelalterlichen Enzyklopädien haben sich meist an einen offenen Leserkreis gerichtet, zumindest den Vorworten zufolge. Alle Leser sollten angesprochen sein, nicht gefiltert nach ihrem sozialen Status oder ihrem Bildungsgrad. In der Praxis jedoch ist beispielsweise das "Elucidarium" anscheinend fast nur von Geistlichen gelesen worden. Der "Livre de Sidrac" hingegen wurde nur von Adligen rezipiert, jedenfalls befand das Buch sich (den Besitzvermerken zufolge) nie in Klosterbibliotheken. Einen sehr kleinen Adressatenkreis hatte der "Hortus Deliciarum": Die Äbtissin Herrad von Landsberg ließ ihn im 12. Jahrhundert nur für ihre Nonnen schreiben. Erst 350 Jahre später wurde das reich illustrierte Werk außerhalb der Klostermauern bekannt.

Dennis de Coetlogon hat sich für seine "Universal history" (1745) wohl eine gehobene Leserschaft vorgestellt, mit Themen wie der Falknerei, die für Adlige gedacht waren. Über Handwerker, Diener und die niederen Stände schrieb De Coetlogon wiederholt despektierlich. Dennoch waren unter den Subskribenten nicht nur Kaufleute, Beamte und Geistliche, sondern auch einige Handwerker, die ungewöhnlich wohlhabend gewesen sein müssen.

Die große französische "Encyclopédie" wurde eher im städtischen als im ländlichen Frankreich gelesen, eher in alten Städten mit kirchlichen und staatlichen Bildungseinrichtungen als in den neuen Städten, in denen sich bereits Industrie ansiedelte. Die Leser gehörten zur Oberschicht, zu den Vertretern des kirchlichen und des adligen Standes. Sie waren Beamte, Offiziere und nur selten Unternehmer. Spätere, billigere Ausgaben wurden zum Teil auch Besitz von Anwälten und Verwaltern in der Mittelschicht. Paradoxerweise erreichte dieses Werk des Fortschritts vor allem die Stände, die unter der Revolution von 1789 zu leiden hatten. Außer in Frankreich verkaufte sich die "Encyclopédie" (besonders in den späteren Auflagen) auch in den angrenzenden französischsprachigen Gebieten, Italien, den Niederlanden und Westdeutschland, weniger in London oder Kopenhagen, wenngleich einige "sets" sogar nach Afrika und Amerika kamen.

Großenzyklopädien wie die von Brockhaus und Meyer im 19. Jahrhundert richteten sich an das Bildungs- und Besitzbürgertum; diese Schichten waren nicht zuletzt wegen der Kreditwürdigkeit bevorzugte Zielgruppen für die Haustürverkäufer. Beim 17-bändigen "Meyers Konversations-Lexikon" von 1893 bis 1897 waren von je hundert Käufern: 20 Verkehrsbeamte, 17 Kaufleute, 15 Militärs, 13 Lehrer, neun Baubeamte/Techniker, sechs Verwaltungsbeamte, fünf Gutsbesitzer, drei Justizbeamte, drei Künstler, drei Privatiers, zwei Wirte, 1,5 Ärzte, ebenfalls 1,5 Studenten und ein Rechtsanwalt.

Noch 1913 meinte Albert Brockhaus: Wenn man von hundert Millionen Deutschsprachigen in Europa als möglichen Käufern ausgehe, müsse man bereits fünfzig Millionen Frauen und fünfundzwanzig Millionen Kinder abziehen. Damals setzten Brockhaus und Meyer zusammen gerade einmal dreißig- bis vierzigtausend Exemplare um. Doch schon in den Jahren nach dem Ersten Weltkrieg richtete der Brockhaus-Verlag sich zunehmend an Frauen sowie die ärmere Bevölkerung und versuchte Begriffe verständlicher einzuführen. Konfessionell getrennte Darstellungen bei religiösen Stichwörtern kamen bei Katholiken gut an. Man konzipierte in den 1920er-Jahren auch Volksausgaben. Die Auflage des "Großen Brockhaus" von 1928 bis 1935 wurde in erster Linie von Hochschullehrern gekauft, auf den nächsten Rängen folgten Apotheker, Rechtsanwälte, Studienräte, Ärzte, Volksschullehrer, Zahnärzte, Geistliche und Architekten, auf Platz zehn standen die Ingenieure.

Für den "Großen Brockhaus" in den 1950er-Jahren galt, dass fast ein Drittel seiner Käufer Lehrer waren oder aus kaufmännischen Berufen stammte. Bundespräsident Theodor Heuss berichtete 1955, er habe den "Großen Brockhaus" in seinem Arbeitszimmer hinter sich, und neben sich auf dem Schreibtisch den kleinen.

Eine besondere Zielgruppe konnten Frauen sein, so bei den Frauenzimmerlexika, wie das "Damen Conversations Lexikon" von 1834, die eine Tradition des 18. Jahrhunderts fortführten. Sie sollten nicht ermüdend Tatsachen aufzählen, sondern anschaulich und romantisch sein, ausführlich dort, wo die Themen die weibliche Sphäre berührten. Staat und Politik fehlten in ihnen völlig. Auch entstanden ab dem frühen 19. Jahrhundert sogenannte Hauslexika, die sich speziell Themen der praktischen Lebensbereiche widmeten.

Eigene Nachschlagewerke gab es auch für Kinder, wenngleich sie lange Zeit selten waren (rechnet man nicht eigentliche Lehrbücher hinzu). Vor dem 19. Jahrhundert war wohl die "Pera librorum juvenilium" (Sammlung von Büchern für die Jugend, 1695) von Johann Christoph Wagenseil das einzige Werk dieser Art. Dann brachte Larousse 1853 die "Petite Encyclopédie du jeune âge" heraus, aber die nächste erschien im Verlag erst 1957. Arthur Mee (1875–1943) brachte 1910/1912 eine moderne Kinderenzyklopädie auf Englisch heraus, die in Großbritannien "The Children’s Encyclopaedia" und in den USA "The Book of Knowledge" genannt wurde. Die reich bebilderten Artikel waren lebhaft geschrieben. Von großem Erfolg war auch die "World Book Encyclopedia" (seit 1917/1918). Der Erste Weltkrieg unterbrach die Planung für eine "Britannica Junior", sie erschien erst 1934. Der Britannica-Verlag kam dann noch mit mehreren Kinderenzyklopädien hervor. "Mein erster Brockhaus" war in den 1950er-Jahren ein großer Publikumserfolg trotz relativ hohem Preis.

Als Enzyklopädien nicht mehr als Lehrbücher, sondern als Nachschlagewerke verstanden wurden, wurde befürchtet, dass die Leser faul würden. Im Vorwort zur "Deutschen Encyclopädie" (1788) beispielsweise setzte man sich mit dem Gedanken auseinander, dass manche Enzyklopädien Unterricht ohne Mühe, ohne Grundlagenwissen versprächen. Goethe ließ im Lustspiel "Die Vögel" jemanden sagen: „Hier sind die großen Lexica, die großen Krambuden der Literatur, wo jeder einzelne sein Bedürfnis pfennigweise nach dem Alphabet abholen kann.“

Gerade die Befürworter einer systematischen Anordnung meinten, bei einer alphabetischen Anordnung könnte der Leser sich mit knappem, oberflächlichem Wissen zufriedengeben. Die Antwort der Lexikonmacher lautete, ihre Leser seien schon gebildet.

1896 machte der Journalist Alfred Dove sich über die Oberflächlichkeit lustig, die die Konversationslexika in die Konversation gebracht hätten. Dabei sei es unerheblich, ob man sich dem "Brockhaus" oder dem "Meyer" anvertraue, sie seien einander gleich an Charakter und Wert.

Auf die Gläubigkeit gegenüber der gedruckten Autorität ging ein Theaterstück ein, das 1905 im Rahmen des 100. Jubiläums von Brockhaus’ Konversations-Lexikon aufgeführt wurde. Ein Stadtrat liest sich Kenntnisse über die Gasanstalt an und beeindruckt damit seine Zuhörer. Danach gesteht er gegenüber dem Bürgermeister: „Kinder, was ist der große Brockhaus doch für ein herrliches Buch, sogar wenn man falsch aus ihm abschreibt, klingt’s doch noch richtig.“

Auch bei grundsätzlich für hochwertig angesehenen Produkten wurde die Kritik laut, dass der Inhalt veraltet sei. Mit dem wissenschaftlichen Fortschritt vor allem seit dem 17. Jahrhundert war dies an sich kaum vermeidbar. Wenn der letzte Band eines Großwerkes erschien, war der erste oft schon mehrere Jahre, wenn nicht Jahrzehnte alt. Überholte Darstellungen waren jedoch auch ein Versäumnis des Autors oder Herausgebers, der sich nicht um die neueste Fachliteratur bemüht hatte.

So behauptete Dennis de Coetlogon in seiner "Universal history" von 1745 fälschlicherweise, die von ihm verwendeten astronomischen Tafeln seien aktuell. Das hatte zum Teil damit zu tun, dass er aus der "Cyclopaedia" von 1728 abschrieb. Unter „Agriculture and Botany“ meinte de Coetlogon, dass der Saft in Pflanzen zirkuliere, so wie das Blut in Tieren. Diese Ansicht war bereits im vorherigen Jahrzehnt durch Stephen Hales' Experimente widerlegt worden.

Ihrer eigenen Reklame zufolge war die "Encyclopaedia Britannica" stets sehr aktuell. Harvey Einbinder listete in den 1960er-Jahren jedoch zahlreiche Artikel auf, die seit sechs Jahrzehnten oder länger nicht oder kaum verändert wurden. Beispielsweise die Artikel über Hesiod und Mirabeau seien aus den Jahren 1875–1889. In der Ausgabe von 1958 hieß es noch, dass in der polnischen Stadt Tarnopol 35.831 Menschen leben, davon vierzig Prozent Juden. Um das Alter der Artikel zu verbergen, entfernte die "Encyclopaedia Britannica" die Initialen von Autoren, die bereits verstorben waren. Das Alter war aber zum Teil an den veralteten Literaturhinweisen erkennbar, etwa, wenn 1963 der Artikel „Punic War“ (Punischer Krieg) angeblich aktuelle Forschung vermeldete, dies sich aber auf Veröffentlichungen von 1901 und 1902 bezog.

Einbinder erklärte die veralteten Artikel damit, dass der Britannica-Verlag wesentlich mehr Geld für Reklame ausgab als für die Verbesserung des Inhalts. Selbst bei einer großzügigen Schätzung betrugen die Kosten für Beitragende um 1960 weniger als eine Million Dollar, der Werbe-Etat allein für die USA sah jedoch vier Millionen vor.

Paul Nemenyi schrieb über die Ausgabe von 1950, dass die naturwissenschaftlichen Artikel im Durchschnitt fünfzehn bis dreißig Jahre alt seien. Als Diana Hobby von der "Houston Post" 1960 die Kritik von Einbinder wiedergab, erhielt sie in der Folge vom Britannica-Verlag einen Brief, dass sie nur wegen ihres Alters, ihres Geschlechts und ihrer Unschuld eine so bösartige Kritik ernst nehmen könne.

Die Herausgeber von Enzyklopädien versuchten, die Aktualität mithilfe von Ergänzungsbänden aufrechtzuerhalten. 1753 erschienen beispielsweise zwei Ergänzungsbände (Supplements) für die 7. Auflage der "Cyclopaedia". Der "Brockhaus" kam dann für seine Auflage von 1851 bis 1855 mit einem Jahrbuch (1857–1864), das in monatlichen Stücken erschien. Als um das Jahr 2000 gedruckte Enzyklopädien seltener wurden, sind oftmals die Jahrbücher weiterhin erschienen, auch wenn das eigentliche Werk bereits sein Ende gefunden hat.

Laut Umfrage von 1985 fanden Mitarbeiter von wissenschaftlichen Bibliotheken in den USA die Aktualität einer Enzyklopädie für ähnlich wichtig wie Aufbau und Zugänglichkeit, und nur noch die Zuverlässigkeit wichtiger. Als allgemeine, ungeschriebene Regel galt, dass man alle fünf Jahre eine neue Enzyklopädie anschaffen müsse. Viele Bibliotheken kauften etwa einmal im Jahr eine neue Enzyklopädie, so dass sie reihum ein relativ aktuelles "set" der wichtigsten Enzyklopädien anbieten konnten. Eine Ausnahme war die "Britannica" in der umstrittenen Anordnung der frühen siebziger Jahre; bei einem Viertel der Respondenten war ihr "set" mindestens neun Jahre alt. Die Bibliothekare klagten nicht über die Aktualität, und es gab Hinweise, dass sie für neuere Informationen andere Werke oder die Zeitung empfahlen.

Zwar haben Enzyklopädien normalerweise den Anspruch, allgemeinverständlich auch für Laien zu sein, können ihn aber gerade bei naturwissenschaftlichen Themen nicht immer einhalten. Spezialisten neigen dazu, in ihren Artikeln zu sehr ins Detail zu gehen, anstatt die allgemeinen Aspekte darzustellen. So berichtete Robert Collison in den 1960er-Jahren von einem Techniker, dem anhand von wohlausgewählten Beispieltexten eine Großenzyklopädie aufgeschwatzt wurde. Sie erwies sich aber als zu schwierig für sein intellektuelles Niveau, so dass er sie bald wieder mit Verlust verkaufte.

Die Reklame für die "Encyclopaedia Britannica" verwendete gegenüber Eltern gern das Verkaufsargument, mit dieser Enzyklopädie könne man das Bildungsniveau der Kinder erhöhen und ihnen bessere Chancen im Vergleich zu anderen Kindern geben. Allerdings wurde die Enzyklopädie nicht für Kinder, sondern für Erwachsene geschrieben. Collisons Vermutung, dass die meisten Kinder (und Erwachsenen) ihre für viel Geld erworbene Enzyklopädie gar nicht verwenden, wurde von Untersuchungen des Britannica-Verlags bestätigt. Der durchschnittliche Käufer hat weniger als einmal pro Jahr in seine "Encyclopaedia Britannica" geschaut.

Dementsprechend wurde von den Kritikern auch wiederholt die Frage gestellt, ob Großenzyklopädien nicht ein „kostspieliger Luxus“ (Anja zum Hingst) sind, mehr ein Statussymbol für kaufkräftige Schichten als ein Instrument zur persönlichen Bildung. Betrachtet man nur die echten (gebundenen) Großenzyklopädien mit mindestens zehn Bänden, nicht älter als zwanzig Jahre, so gab es diese in den 1980er-Jahren allenfalls in fünf bis acht Prozent der Haushalte. Den Verdacht des Statussymbols schürten nicht zuletzt die Luxus-, Jubiläums- und Künstlerausgaben, die noch einmal deutlich teurer waren als die normalen, die bereits hochwertig gebunden und auf gutem Papier gedruckt waren.




</doc>
<doc id="1429" url="https://de.wikipedia.org/wiki?curid=1429" title="Philosophie der Antike">
Philosophie der Antike

Die Philosophie der Antike war eine philosophiegeschichtliche Epoche. Sie dauerte mehr als 1100 Jahre, von etwa 600 v. Chr. (als ältester Vertreter wurde Thales um 624 v. Chr. geboren) bis ins 6. Jahrhundert n. Chr., als die letzten Neuplatoniker wirkten. Ihre Hauptschauplätze waren das antike Griechenland und das Römische Reich.

Die Philosophie der Antike war geographisch auf den Mittelmeerraum beschränkt. Andere wichtige philosophische Traditionen des Altertums waren die Chinesische Philosophie (seit 1000 v. Chr.) und die Indische Philosophie (seit 1000 v. Chr.), einflussreich waren die Kultur des Judentums, des alten Ägyptens, des Perserreichs und Mesopotamiens. In Europa folgte auf die Philosophie der Antike die Philosophie des Mittelalters.

Die Philosophen der Antike lassen sich grob in verschiedene Gruppen einteilen. Diejenigen, die vor Sokrates gewirkt haben, bezeichnet man als die Vorsokratiker (etwa 600 bis 400 v. Chr.). Sie haben das damalige von Mythen und Göttern geprägte Weltbild durch ansatzweise philosophische und naturwissenschaftliche Erklärungsversuche ersetzt. Mit Sokrates beginnt die griechische Klassik (etwa 500 bis 300 v. Chr.). Zu dieser Zeit war Athen das geistige Zentrum Griechenlands. Sokrates’ Schüler Platon und dessen Schüler Aristoteles wurden zu zwei der wichtigsten und bis heute einflussreichsten Philosophen. Zur Klassik kann man auch die Sophisten, die Kyniker, die Epikureer, die Kyrenaiker und die Stoiker rechnen. Auf die Klassik folgte die Philosophie der hellenistischen Zeit, auf diese die Philosophie der Spätantike.

Die Einteilung der Philosophiegeschichte in Perioden kann nach verschiedenen Gesichtspunkten (Zeit, Ort, Strömung usw.) geschehen. Bis heute hat sich keine allgemeinverbindliche Einteilung etablieren können. Einigermaßen durchgesetzt hat sich der Begriff der Vorsokratiker, wobei allerdings manche Autoren die Sophisten zu den Vorsokratikern, andere zur griechischen Klassik zählen. Hier eine ungefähre Einteilung:

Mit Thales von Milet beginnt im 6. Jahrhundert v. Chr. die abendländische Philosophiegeschichte. Wie die aller anderen Vorsokratiker ist seine Lehre aber nur bruchstückhaft überliefert. Man geht davon aus, dass seit Thales langsam damit begonnen wurde, das von Mythen und Göttern geprägte Weltbild durch wissenschaftlichere Erklärungen zu ersetzen. Dazu passt, dass Thales auch Mathematiker und Astronom war. Thales zählt mit Anaximander und Anaximenes zu den sogenannten Milesiern (auch: ältere ionische Naturphilosophen). Aristoteles berichtet, dass die Milesier versucht haben, einen Urgrund "(archē)" aller Dinge zu finden. Für Thales soll dieser Urgrund das Wasser gewesen sein, für Anaximander war es das Unbegrenzte "(apeiron)" und für Anaximenes die Luft.

Pythagoras gründete im 6. Jahrhundert v. Chr. die philosophische Gemeinschaft der Pythagoreer. Ihre Philosophie war nicht von der Suche nach einem Urstoff, sondern stark von der ebenfalls betriebenen Mathematik geprägt. So sahen sie in Zahlen und mathematischen Verhältnissen den Schlüssel zu einer umfassenden Weltbeschreibung und -erklärung. Die Pythagoreer betätigten sich auch politisch und stellten Theorien in den Bereichen Geometrie, Musiktheorie, Kalenderrechnung und Astronomie auf.

Die von Heraklit überlieferten literarischen Bruchstücke gelten als schwer verständlich. Es handelt sich um sentenzenähnliche Sätze, die an Rätsel erinnern. So wurde er bereits in der Antike „der Dunkle“ genannt. Aus dem Feuer entsteht nach Heraklit die Welt, die in allen ihren Erscheinungsformen eine den meisten Menschen verborgene vernunftsgemäße Fügung gemäß dem Weltgesetz des Logos erkennen lässt. Alles befindet sich in einem ständigen, fließenden Prozess des Werdens, welches vordergründige Gegensätze in einer übergeordneten Einheit zusammenfasst. Aus dieser Auffassung entstand später die verkürzende Formulierung „Alles fließt“ "(panta rhei)."

Parmenides zählt wie Zenon von Elea zu den Eleaten. Er unterscheidet zwischen dem, was den Sterblichen wahr zu sein scheint, und einer sicheren Wahrheit. Sicher wahr seien die Existenz des Seins und die Nichtexistenz des Nichtseins. Daraus müsse geschlussfolgert werden, dass das Sein unveränderbar sei, da die einzige Form der Veränderung für das Sein die wäre, Nichtsein zu werden. Dies sei aber undenkbar und somit sei die Annahme irgendeiner Form der Veränderung des Seins bloße Meinung und purer Schein, im Gegensatz zu einer Erfassung des Seins durch die Vernunft setzt.

Demokrit schließlich führte den Atomismus des Leukipp weiter, indem er behauptete, dass die gesamte Natur aus kleinsten unteilbaren Einheiten, aus Atomen "(atomoi)" zusammengesetzt sei. Die Dinge schienen nur eine Farbe oder Geschmack zu haben, in Wirklichkeit gebe es nur Atome im leeren Raum.

Xenophanes ist für seine kritische Auseinandersetzung mit dem herkömmlichen anthropomorphen Götterbild seiner Zeit bekannt. Empedokles wurde für seine Vier-Elemente-Lehre bekannt, wonach alles aus den Elementen Feuer, Wasser, Luft und Erde bestehe. Anaxagoras gilt als derjenige, der im Zuge seiner Übersiedlung nach Athen die Philosophie ebendahin mitbrachte.

Die fünf Jahrzehnte zwischen den Perserkriegen und dem Peloponnesischen Krieg bildeten Athens klassische Blütezeit, in der die attische Demokratie ihre Vollendung fand. In dieser gesellschaftspolitischen Umbruchphase bestand entsprechender geistiger Orientierungsbedarf, den die sophistische Aufklärung zu decken suchte. Die seit 450 v. Chr. auftretenden Sophisten richteten ihre Überlegungen weg von der Natur auf den Menschen und suchten nach Methoden, das Individuum geistig und körperlich zu stärken. So brachten sie den Jugendlichen Rhetorik und Kampfkünste bei, doch waren sie nicht so spitzfindig, wie man ihnen häufig unterstellt. Wichtige Sophisten waren: Antiphon, Gorgias, Hippias von Elis, Kritias, Prodikos, Protagoras. Von Letzterem stammt der berühmte Satz: „Der Mensch ist das Maß aller Dinge, derer die sind, dass sie sind, und derer die nicht sind, dass sie nicht sind.“

Philosophie wurde so zur öffentlichen Angelegenheit, die auf dem Marktplatz "(agora)" und in interessierten Zirkeln betrieben wurde. Hier entfaltete sich die Freiheit des Denkens in einem friedlichen Wettstreit "(agon)" durch den Austausch der Ansichten und Argumente. Einen besonderen und bis heute fortwirkenden Eindruck hinterließ Sokrates mit seiner Lehrweise und Haltung zum Leben. Er pflegte seine Gesprächspartner in ihrem vorgeblichen Wissen zu erschüttern, indem er durch bohrendes Nachfragen gedanklich-logische Lücken freilegte, um dann in fortgesetzten Dialogen neue Erkenntnisse bei seinen Partnern zu Tage zu fördern, ein Vorgehen, das er Hebammenkunst (Mäeutik) nannte. Die Unerschrockenheit und Festigkeit seines Auftretens in dem gegen ihn als vermeintlichen Verderber der Jugend geführten Prozess und die Art, wie er das Todesurteil hin- und angenommen hat, haben ihn zum Urbild philosophischer Daseinsbewältigung werden lassen.

Da Sokrates selbst nichts Schriftliches hinterlassen hat, ist sein Bild in der Philosophiegeschichte wesentlich von seinem Schüler Platon bestimmt, der die Methode und die Gehalte der sokratischen Lehre nach seinem Verständnis in Dialogform aufgezeichnet und damit überliefert hat. Dazu entwickelte er jedoch seine eigenen Lehren, sodass heute sokratische und platonische Anteile dieses philosophischen Gebäudes, wie es in den platonischen Dialogen vorliegt, schwer zu trennen sind. Berühmt ist Platons Höhlengleichnis: Ohne Kenntnis der Ideen, die die Wahrheit hinter den Dingen darstellen, sind wir wie Menschen, die in einer Höhle sitzen, nie die Sonne gesehen haben und unsere Schatten für das echte, das wahre Leben halten. Dabei nahm Platon an, dass die Ideen selbstständig in einer höheren Welt existierten. (Der Mathematiker und Philosoph Alfred North Whitehead bemerkte einmal, dass alle späteren Entwürfe der europäischen Philosophie im Grunde nur Fußnoten zu Platon seien.)

Als Aristoteles seinem Lehrer Platon philosophisch nur noch teilweise zustimmen konnte, bekannte er, zu Platon empfinde er Freundschaft, zur Wahrheit aber noch mehr als zu diesem. Während Platons Philosophie im Kern auf eine unser sinnliches Wahrnehmungsvermögen der Welt transzendierende Ideenlehre zielte, suchte Aristoteles die erfahrbare Wirklichkeit von Natur und menschlicher Gesellschaft umfassend zu erforschen und wissenschaftlich zu ordnen. Im Gegensatz zu Platon sah er die Ideen als "in" den Dingen befindlich und gab der realen Welt so wieder mehr Gewicht. Hierbei hat er u. a. für Biologie und Medizin, aber auch für die politische Empirie und Theorie Enormes geleistet. In seinem enzyklopädischen Wissensdrang als Philosoph beschäftigten ihn zudem u. a. Dynamik (δύναμις), Bewegung (κίνησις), Form und Stoff. Aristoteles begründete die klassische Logik mit ihrer Syllogistik, die Wissenschaftssystematik und die Wissenschaftstheorie. Die Autorität, die Aristoteles als Forscher und Denker noch im europäischen Mittelalter besaß, war so groß, dass sein Name für den Begriff des Philosophen schlechthin stand.

Eine philosophische Richtung, die sich vor allem an der bedürfnisarmen Lebensweise des Sokrates orientierte, bildeten die Kyniker. Ihr berühmtester Vertreter, Diogenes von Sinope („Diogenes in der Tonne“), soll Alexander dem Großen, als dieser ihn besuchte und nach seinen Wünschen fragte, beschieden haben: „Geh mir aus der Sonne!“

Im Hellenismus wurden die klassischen Denkansätze weiter fortgeführt. Eine besondere Rolle dabei spielten die hellenistischen „Musenhöfe“. So entstand in Alexandria die sehr einflussreiche Alexandrinische Schule, während die Peripatetiker die Denkansätze des Aristoteles weiter entwickelten und die platonische Akademie Platon folgte.

Am Übergang vom 4. zum 3. Jahrhundert v. Chr. entstanden mit Stoa und Epikureismus zwei philosophische Schulen, die weit hinaus über Zeit und Ort ihrer Entstehung ausstrahlten und ethische Grundpositionen für ein glückendes Leben markierten. Ihr Wirkungspotential ist bis heute noch keineswegs erschöpft, wie neuere Veröffentlichungen zu Glück und Lebenskunst zeigen. Während der Epikureismus das individuelle Glück durch optimal dosierte Genüsse zu fördern trachtet und in öffentlichen Angelegenheiten Zurückhaltung empfiehlt, wendet sich die Stoa gegen die Versklavung der Seele in der Sucht nach Bedürfnisbefriedigung, unterstellt sich ganz der Vernunftkontrolle und sieht das Individuum als Teil einer menschlichen Gemeinschaft und eines kosmischen Ganzen, denen gegenüber Pflichten bestehen, die im Handeln zu berücksichtigen sind. Charakteristisch für Stoiker ist eine schicksalsbejahende Grundhaltung im Einklang mit der Ordnung des Universums.

Vermittelt durch Panaitios von Rhodos und Poseidonios fanden stoische Leitlinien Eingang in das Denken führender Kreise des republikanischen und kaiserzeitlichen Rom. Im Kontakt mit der politischen Wirklichkeit des Römischen Reiches ist von der Strenge und Absolutheit des stoischen Ausgangsentwurfs dies und jenes abgeschliffen worden (etwa die völlige Missachtung des Leibes und der Emotionen). Stoisch inspirierte Römer wie Cicero in der Zeit der ausgehenden Republik und Seneca in der frühen Kaiserzeit, bezogen Elemente anderer philosophischer Schulen mit ein; das tat auch Lukrez, der sich aber auf Epikur berief. Mag es einem solchen als eigene philosophische Richtung geführten Eklektizismus an Originalität fehlen, so hat er doch Lebenstauglichkeit und Praktikabilität der philosophischen Lehren zweifellos erhöht. Im Zenit des Prinzipats wurde die Stoa zur Richtschnur und Meditationsgrundlage des römischen Kaisers Mark Aurel, des „Philosophen auf dem Kaiserthron“ in seinen "Selbstbetrachtungen". Er wurde im 2. Jahrhundert n. Chr. gleichsam zur Verkörperung der da schon 500 Jahre alten platonischen Idee vom zur Herrschaft berufenen Philosophen.

Die dritte neben Stoa und Epikureismus zwar an Mitgliederzahl weit unterlegene, aber philosophiegeschichtlich höchst bedeutende philosophische Strömung des Hellenismus und der Kaiserzeit bilden die so genannten skeptischen Schulen. Zu unterscheiden sind derer drei: Der Ältere Pyrrhonismus, durch Pyrrhon von Elis begründet, lehrte eine generelle Ununterschiedenheit und Ununterscheidbarkeit aller Dinge und Meinungen (Indifferentialismus), woraus er v. a. ethische Konsequenzen zog. Mehr oder minder unabhängig davon entwickelte sich später auch in der platonischen Akademie eine erkenntniskritische Richtung: Arkesilaos, mit dem die sog. Mittlere Akademie begann, lehrte nach Sokrates' Vorbild einen strikten Agnostizismus. Dieser wurde von Karneades, dem Begründer der sog. Neuen Akademie, zu einer Art Wahrscheinlichkeitslehre gemildert, welche über seinen Nachfolger Philon von Larisa insbesondere Cicero beeinflusste und noch den jungen Augustinus von Hippo beeindrucken sollte. Schließlich begründete Ainesidemos, wohl ein ehemaliger Anhänger der Akademie, den seit langem erloschenen Pyrrhonismus neu: der Neupyrrhonismus, der v. a. in den Schriften des Sextus Empiricus beschrieben wird, verband die systematische Erkenntniskritik der Neuen Akademie mit der ethischen Motivation des Älteren Pyrrhonismus zu einer skeptischen Haltung, die durch die Enthaltung von jeglichem Erkenntnisurteil (die sog. epoché) den Kampf der Meinungen beenden wollte und gerade dadurch Seelenruhe (Ataraxie) sowie die ersehnte Glückseligkeit (Eudaimonie) zu finden hoffte.

In der Spätantike wurde, obgleich es nach wie vor auch Vertreter von Richtungen wie etwa dem Kynismus gab, der Neuplatonismus als philosophische Richtung maßgeblich, der in einem wohl wechselseitig verschränkten Prozess anregend und befruchtend auch auf das Denken der christlichen Kirchenväter einwirkte.

Der philosophische Impuls, der Roms Herrschaftseliten über Jahrhunderte ethische Orientierung geboten hatte, erlahmte, als der äußere Druck auf die Grenzen zunahm und deren Verteidigung immer mehr Menschen und Mittel band; nun stiegen immer öfter Männer in die Führungsschicht auf, die dem Militär entstammten und häufig wenig Verständnis für feingeistige Dinge aufbrachten. Dennoch versiegte er, vor allem im östlichen Teil des Reiches, nicht. Der Drang von Philosophen wie Plotin und später Proklos zur Vereinheitlichung (Suche nach dem Einen, dem Göttlichen) mündete in eine Rückwendung zu Platon und in eine Neuausrichtung der platonischen Ideenlehre. Daraus ergaben sich Verknüpfungsmöglichkeiten zwischen Neuplatonismus und christlicher Religion.

Wichtige Vertreter der antiken christlichen "Apologetik" waren im 2. Jahrhundert Justinus der Märtyrer, im 3. Jahrhundert Klemens von Alexandrien († nach 215) und Origenes († 253) sowie im 5. Jahrhundert Augustinus von Hippo († 430) mit seinem Werk "Über den Gottesstaat". Das Denken des Augustinus spiegelte die spätantike Umbruchphase und legte das Fundament für die Philosophie des Mittelalters. Gestaltete sich in Platons Parmenides-Dialog die Suche nach dem Einen noch sehr rätselhaft, so glaubten die frühen christlichen Kirchenlehrer in Gott das Eine (und alles, "Hen kai pan") gefunden zu haben, das alle Rätsel löst. Im 4. Jahrhundert hatte dann etwa die Theurgie, die teils sehr kritisch betrachtet wurde, starken Zulauf.

Im Oströmischen Reich wirkten noch im 5. und 6. Jahrhundert bedeutende Philosophen wie Isidor, Simplikios, der wichtige Aristoteles-Kommentare verfasste, und sein Lehrer Damaskios. Von einem völligen Niedergang der Philosophie in der Spätantike kann somit nicht die Rede sein. Die Philosophie war im Ostteil des Reiches auch Rückhalt für nichtchristliche Traditionen (was die „heidnische Renaissance“ zu Zeiten des Kaisers Julian verdeutlichte, der selber ein Anhänger des Neuplatonismus war). Aber auch mehrere Christen traten als bedeutende Philosophen hervor, wie beispielsweise im 6. Jahrhundert Johannes Philoponos in Alexandria oder im Westen der neuplatonisch geschulte Anicius Manlius Severinus Boëthius, dessen Werk "Trost der Philosophie" zu den bemerkenswerten Werken in der ausgehenden Spätantike zählt.

Die faktische Schließung der platonischen Akademie in Athen durch Kaiser Justinian im Jahre 529 (oder etwas später) machte den dortigen philosophischen Studien ein Ende, die christianisierte Schule von Alexandria bestand allerdings fort und ging erst infolge der Perser- und Araberkriege unter, die Byzanz im 7. Jahrhundert zu bestehen hatte (siehe Römisch-Persische Kriege und Islamische Expansion). Bald nach der Mitte des 6. Jahrhunderts erlosch die Tradition der antiken-heidnischen Philosophie endgültig, wenngleich in Byzanz die Beschäftigung mit ihr nicht abriss. Einer der letzten bedeutenden spätantiken Neuplatoniker, der Christ Stephanos von Alexandria, wirkte dann zu Beginn des 7. Jahrhunderts in Konstantinopel.

Das Christentum, das das mittelalterliche Weltbild Europas bestimmte, hat in seine Lehren viele Elemente antiker Philosophie integriert. Die dogmatischen Diskussionen und Streitigkeiten, die das spätantike Christentum dann vom 4. bis 6. Jahrhundert prägten und der Religion ihre heutige Form gaben, sind ohne den Hintergrund der griechischen Philosophie nicht verständlich. Den weltanschaulichen Pluralismus, wie er in den nebeneinander bestehenden antiken Philosophieschulen und Religionen vorhanden war, hat der christliche Monotheismus allerdings von der Spätantike bis in das Zeitalter der Aufklärung hinein nicht mehr zugelassen.

Dem griechischen Philosophiehistoriker Diogenes Laertios aus dem dritten nachchristlichen Jahrhundert ist es zu verdanken, dass viele antike Philosophen trotz der Zerstörung der wohl bedeutendsten antiken Bibliothek in Alexandria nicht ganz in Vergessenheit gerieten: In lateinischer Übersetzung blieb sein Werk dem Mittelalter bekannt. Für den lateinischen Westen war vor allem Boethius von kaum zu überschätzender Bedeutung, da er unter anderem die Regeln der aristotelischen Logik in eine Form brachte, die das mittelalterliche Denken entscheidend prägen sollte.

Nach dem 6. Jahrhundert geriet ansonsten zumindest in Europa der größte Teil der antiken Philosophie in Vergessenheit. Die Weitervermittlung antiker Philosophie geschah in der Folgezeit hauptsächlich durch arabisch-islamische Denker wie Avicenna (980–1037) und Averroes (1126–1198) sowie durch den jüdischen Philosophen und Arzt Maimonides (1135–1204). Über solche Umwege gewann die Philosophie der Antike, insbesondere die des Aristoteles, auf die Philosophie des Mittelalters bei Scholastikern wie Albertus Magnus († 1280) und Thomas von Aquin († 1274) sowie bei Denkern der Frührenaissance allmählich wieder an Bedeutung. Ein zweiter Schub erfolgte im 15. Jahrhundert, als im Zuge der Renaissance westliche Gelehrte in den byzantinischen Osten reisten und Handschriften antiker griechischer Denker mitbrachten (so unter anderem Giovanni Aurispa) bzw., als byzantinische Gelehrte vor den Osmanen in den Westen flohen und als Vermittler antiker Bildung im Westen mitwirkten.


Einführungen

Gesamtdarstellungen

Companions

Prosopographisches Lexikon

Begriffswörterbücher

Philosophie vor dem Hellenismus

Hellenistische und kaiserzeitliche Philosophie

Spezielle Themen

Fachzeitschriften



</doc>
<doc id="1430" url="https://de.wikipedia.org/wiki?curid=1430" title="Ei (Begriffsklärung)">
Ei (Begriffsklärung)

Ei bezeichnet:

Zusammensetzungen mit Ei
Mit Herkunftsbezeichnung
EI steht als Abkürzung für:
EI als Unterscheidungszeichen auf Kfz-Kennzeichen:

Siehe auch:


</doc>
<doc id="1432" url="https://de.wikipedia.org/wiki?curid=1432" title="Elisabeth">
Elisabeth

Elisabeth (zuweilen auch "Elisabet") ist ein weiblicher Vorname.

Der Name leitet sich über griechisch "Elizabeth" von der Bibel her und bezieht sich auf Elisabet, Mutter Johannes’ des Täufers.

Hebräisch bedeutet etwa „Gott schwört“ oder „Gott des Schwures/Eides“. Da das hebräische Wort für "Schwur/schwören" mit dem Zahlwort für "Sieben" verknüpft ist, finden sich auch Deutungen wie „Gott ist Sieben“ (im Sinne von „Gott ist Glück“) und weitere mit dieser Zahl in Verbindung gebrachte Deutungen wie „Gott ist heilig“, „Gott ist Vollkommenheit/Fülle“ und ähnliches. Der Bindevokal zwischen den beiden hebräischen Wortteilen wird gelegentlich als das Possessivpronomen "mein" (über)interpretiert und dementsprechend wird auch „mein Gott schwört“, „Gott ist ihr Eid“, „welcher Gott schwur“ und so weiter übersetzt.

Der Name Elisabeth war Ende des 19., Anfang des 20. Jahrhunderts sehr populär in Deutschland, bis in die 1920er Jahre war er unter den zehn meistvergebenen Mädchennamen des jeweiligen Jahrgangs zu finden. Seine Beliebtheit ließ dann allmählich nach und erreichte Anfang der 1980er einen Tiefpunkt. Seitdem wird der Name wieder häufiger vergeben.


Weitere Tage siehe unten, bei heilige Elisabeth



Im Alten Testament ist eine Elischeba die Frau des Aaron und damit Stammmutter des Priestergeschlechts .

Am Anfang des Evangeliums nach Lukas ist – in gräzisierter Form – Elisabet, die Frau des Priesters Zacharias, die Mutter Johannes des Täufers .

"Heilige Elisabeth:"

"Selige Elisabeth:"














</doc>
<doc id="1442" url="https://de.wikipedia.org/wiki?curid=1442" title="E-Netz">
E-Netz

Das E-Netz (Funktelefonnetz-E/E1) ist ein telefonieorientiertes, digitales Mobilfunknetz in Deutschland, das auf dem GSM-Standard basiert und den DCS-1800-Frequenzbereich bei 1800 MHz nutzt. Der wesentliche Unterschied zu Netzen, die den GSM-900-Frequenzbereich nutzen, besteht in der geringeren Sendeleistung der Endgeräte und Basisstationen. 

Die weltweit erste DCS-1800-Installation war das One2One-Netz (später T-Mobile UK) in Großbritannien.

Bundespostminister Wolfgang Bötsch (CSU) vergab 1993 die Lizenz für den Aufbau eines dritten digitalen Mobilfunknetzes, des E1-Netzes. Die Bezeichnung leitet sich aus den Namenskonventionen für die analogen Autotelefonnetze A-, B- und C-Netz sowie des ersten digitalen Netzes, dem D-Netz, ab.

Als erstes E-Netz in Deutschland ging im Mai 1994 das E-Plus-Netz, welches auch als E1-Netz bezeichnet wird, an den Markt. Hauptgesellschafter waren Vebacom und Thyssen Telecom. E-Plus war damit neben Mannesmann der zweite private Betreiber eines öffentlichen Telekommunikationsdienstes. 1997 folgte als zweite E-Netzbetreiberin (E2-Netz) die Viag Interkom (heute O).

Mittlerweile hat die niederländische KPN Mobile N.V. E-Plus an die spanische Telefónica verkauft. Telefónica, als Eignerin des ehemaligen Viag-Interkom-Netzes (übernommen von der britischen O plc), fusioniert beide Netze. Diese sollen nach Planungen bis in das Jahr 2021 abgeschlossen sein.

Neben E-Plus und O, welche jeweils 112 Frequenzen ersteigert haben, funken auch T-Mobile und Vodafone im DCS-1800-Band. Diese haben 1999 25 (T-Mobile) und 27 (Vodafone) Mobilfunkfrequenzen in dem Band erworben, um Engpässe im P-GSM-Bereich besser ausgleichen zu können und zusätzliche Kapazitäten für das starke Kundenwachstum anbieten zu können.

Bis 1998 waren deutlich mehr als die geplanten 75 Prozent der Bevölkerung in Deutschland mit E-Plus-Funkabdeckung versorgt. Die maximale Sendeleistung der E-Netz-Mobiltelefone beträgt 1 Watt und soll insgesamt zu einem geringen Batterieverbrauch und damit relativ langen Gesprächs- und Bereitschaftszeiten führen. Wegen der geringeren Sendeleistung und der höheren Freiraumdämpfung würden die E-Netze mehr Funkstationen als die GSM-900-Netze benötigen, um die gleiche Versorgung wie die D-Netze zu gewährleisten. Die E-Netze müssten daher engmaschiger „geknüpft“ werden. Dennoch haben die D-Netze (T-Mobile und Vodafone) mehr Stationen in Betrieb als die E-Netz-Betreiber E-Plus und O. Grund hierfür ist, dass die D-Netz-Betreiber wesentlich mehr Kunden versorgen und dementsprechend mehr Stationen aus Kapazitätsgründen gebaut haben.

Da die Signale mit höheren Frequenzen stärker gedämpft werden, ist die Reichweite geringer als im 900-MHz-Bereich (D-Netz). Die systembedingte maximale Reichweite, bis zu der Signallaufzeiten ausgeglichen werden können, beträgt bei GSM-Netzen allgemein ca. 35 km.

In Österreich war E-Netz bis 1996 die Bezeichnung für das Netz der Telekom Austria nach GSM-Standard im 900-MHz-Bereich. Dieses Netz wurde im Juni 1996 in A1 umbenannt.



</doc>
<doc id="1443" url="https://de.wikipedia.org/wiki?curid=1443" title="Ernährung des Menschen">
Ernährung des Menschen

Die Ernährung des Menschen, bestehend aus Getränken und Nahrungsmitteln, dient dem Mensch zum Aufbau seines Körpers sowie der Aufrechterhaltung seiner Lebensfunktionen. Sie beeinflusst auch sein körperliches, geistiges, physiologisches und soziales Wohlbefinden. Der bewusste Umgang mit der Zufuhr von fester Nahrung und Flüssigkeit ist zudem ein fester Bestandteil der menschlichen Kultur und vieler Religionen.

Der menschlichen Ernährung dienen rohe, gekochte oder anders zubereitete, frische oder konservierte Nahrungsmittel "(siehe auch Ökotrophologie)". Fehlfunktionen bei der Nahrungsaufnahme werden als Ernährungsstörungen bezeichnet.

Der heutige Mensch ernährt sich omnivor und weist bezüglich des Verdauungstrakts mehr Ähnlichkeiten mit fleischfressenden Primatenarten auf als mit pflanzenfressenden. Die Individuen der Gattung "Australopithecus", aus der die Gattung des Menschen (die Gattung "Homo") hervorging, ernährten sich hingegen vor drei bis vier Millionen Jahren noch überwiegend pflanzlich.

Aus dem Abrieb und aus anderen Merkmalen ihrer Zähne wurde geschlossen, dass die frühen Vertreter der Hominini ("Australopithecus anamensis", "Australopithecus afarensis", "Australopithecus africanus" und "Homo rudolfensis") sich von einer überwiegend pflanzlichen Kost ernährten, vergleichbar mit den heutigen Pavianen. Die möglicherweise frühesten Hinweise auf Fleischverzehr sind 3,3 Millionen Jahre alte, als Schnittspuren gedeutete Einkerbungen an Wildtierknochen. Sie stammen aus Dikika und werden "Australopithecus afarensis" zugeschrieben.

Erst "Homo habilis", mit dessen rund zwei Millionen Jahre alten Fossilien Steinwerkzeuge und als gesichert geltende Schnittspuren an Knochen gefunden wurden, wird heute zugeschrieben, dass er in etwas größerem Maße als die Individuen früherer Arten der Hominini das Fleisch großer Wirbeltiere verzehrt hat. Offenbar wurden damals mit Hilfe von Steinwerkzeugen zusätzliche Nahrungsquellen – Fleisch und Knochenmark – erschlossen. Dies geht jedenfalls aus 1,95 Millionen Jahre alten Knochenfunden hervor, die in Kenia geborgen wurden und bezeugen, dass damals bereits neben Antilopenfleisch auch das Fleisch zahlreicher im Wasser lebender Tiere – darunter Schildkröten, Krokodile und Fische – verzehrt wurde. Krankhafte Veränderungen an einem 1,5 Millionen Jahre alten, den Hominini zugeschriebenen Schädelknochen eines Kleinkindes (Olduvai Hominid OH 81) wurden zudem als Folge einer Anämie interpretiert, eine Erkrankung, die mit Eisenmangel in Verbindung gebracht wird. Hier gibt es Spekulationen, diese Anämie könnte darauf hinweisen, dass zu diesem Zeitpunkt bereits eine Anpassung an einen regelmäßigen Verzehr von Fleisch stattgefunden habe.

Im weiteren Verlauf der Stammesgeschichte des Menschen, insbesondere in der Spätphase des "Homo erectus", nahm das Hirnvolumen immer weiter zu. Viele Wissenschaftler gehen von einem erhöhten Bedarf an Proteinen in dieser Phase aus, die in tierischer Kost leichter zugänglich sind. "Homo erectus" erlernte zudem den Umgang mit Feuer und begann es zur Erschließung zusätzlicher Nahrungsquellen zu nutzen.

Spätestens vor 450.000 Jahren gab es Jagdaktivitäten, wie Funde von Waffenresten von "Homo heidelbergensis" in Europa eindeutig belegen. Es wird ein stetig wachsender Fleischanteil in der Ernährung vermutet. was in der Fachwelt aber nicht unwidersprochen ist. Zum einen könnten Knollen und Zwiebeln doch einen höheren Anteil an der Nahrung des späten "Homo erectus" (= "Homo heidelbergensis") gehabt haben, zum anderen könnte vor allem das Sammeln und Fangen von Kleintieren, wie Nager oder Schildkröten, zur Deckung des Nahrungsbedarfs gedient haben. Womöglich wird die Bedeutung der Jagd also überschätzt. An Funden aus der Höhle von Arago bei Tautavel in Südfrankreich wurde beispielsweise die Abnutzung der Zähne von "Homo heidelbergensis" mikroskopisch untersucht. Die Ergebnisse ließen auf eine raue Nahrung schließen, die zu mindestens 80 Prozent aus pflanzlichen Anteilen bestand – dies entspricht ungefähr einer Zusammensetzung der Nahrung, wie sie auch bei heutigen Jägern und Sammlern üblich ist. Zu beachten ist hier, dass aus dem europäischen "Homo heidelbergensis" zwar der Neandertaler hervorging, nicht aber der anatomisch moderne Mensch ("Homo sapiens"). Jedoch wird dem afrikanischen "Homo rhodesiensis", der vermutlich zum Formenkreis des sogenannten archaischen "Homo sapiens" gehört, wegen seiner stark abgenutzten Zähne ebenfalls der Verzehr von überwiegend sehr rauer pflanzlicher Nahrung zugeschrieben.

Die mehr als 150.000 Jahre alten Hinterlassenschaften der afrikanischen Pinnacle-Point-Menschen verweisen auf eine intensive Nutzung von Meeresfrüchten. Der älteste Beleg für Fischfang auf dem offenen Meer stammt aus Osttimor und wurde auf ein Alter von 42.000 Jahren datiert.

Nach heutigem Kenntnisstand des Verlaufs der Hominisation ist der anatomisch moderne Mensch ("Homo sapiens") demnach „von Natur aus“ weder ein reiner Fleischfresser (Carnivore) noch ein reiner Pflanzenfresser (Herbivore), sondern ein Allesfresser (Omnivore). Die omnivore Lebensweise erleichterte es dem modernen Menschen, sich nahezu jedes Ökosystem der Erde als Lebensraum zu erschließen. Während sich einige kleinere Bevölkerungsgruppen wie die Evenki in Sibirien, die Eskimos und die Massai auch heute noch überwiegend fleischlich ernähren, leben große Teile der südasiatischen Bevölkerung sowie bäuerliche Völker in den Anden in erster Linie oder sogar ausschließlich von pflanzlichen Nahrungsmitteln. Angaben über die Ernährungssituation heute noch bestehender Jäger-und-Sammler-Völker sind widersprüchlich. Einerseits wird behauptet, dass weit über die Hälfte der Kost von Tieren stammt, andererseits wird auf Studien über die Ernährungsgewohnheiten verwiesen, die auch hier zeigen, dass das Sammeln die Grundlage der Ernährung bildet und die Jagd eher Luxus ist, also eher von Sammlern und Jägern gesprochen werden müsste.

Vor rund 10.000 Jahren führte die Verbreitung des Ackerbaus zur sogenannten neolithischen Revolution. Diese kulturell äußerst bedeutsame Entwicklung ermöglichte dem Menschen die Sesshaftigkeit und führte durch die planvolle Nutzung der Natur zu einer größeren Unabhängigkeit von äußeren Bedingungen. Teilweise verschlechterte dies allerdings die Ernährungslage der Menschen durch eine drastische Verengung des Nahrungsangebots auf wenige Feldfrüchte.

In erster Linie ist das, was der Mensch isst, wie er es zubereitet (Kochkunst) und zu sich nimmt (Esskultur), sowie das, was er "nicht" isst (Nahrungstabu), von seinem Lebensraum und seiner Kultur abhängig, und damit starken regionalen Unterschieden unterworfen. Trotz der teils extremen Unterschiede der traditionellen Regionalküchen wird der Bedarf an Nährstoffen in der Regel gedeckt.

Da sich aber vor allem in den Industrieländern die Ernährungsweise von den traditionellen Formen wegentwickelt und sich durch die Zunahme sitzender Tätigkeiten und abnehmender körperlicher Betätigung der Lebensstil und damit der Energie- und Nährstoffbedarf insgesamt verändert hat, gibt es heutzutage bei vielen Menschen ein Missverhältnis zwischen Nährstoffbedarf und Nährstoffzufuhr. Deshalb wird die Frage nach der richtigen Ernährung wegen der Bedeutung für die Gesundheit in Abhängigkeit von der Lebensweise durch die Diätetik wissenschaftlich erforscht.

Insbesondere die Zunahme an Zivilisationskrankheiten wird der modernen Fehlernährung zugeschrieben. Dies hat dazu geführt, dass es eine Vielzahl von Ansichten, Theorien und Lehren über die richtige Ernährung gibt. Beispiele sind die Theorien von der Vollwerternährung, die Rohkost-Lehre, die Ernährung nach den 5 Elementen aus der traditionellen chinesischen Medizin, die Ayurveda-Lehre, dem Pescetarismus, der Vegetarismus und der Veganismus, die Makrobiotik (Ernährungswissenschaft aus der Perspektive von Yin und Yang), die Trennkost-Lehre und die Steinzeiternährung, die in die Richtung der Low-Carb-Ernährung geht. Die Antworten auf die Frage nach einer richtigen Ernährung sind oft weltanschaulich beeinflusst.

Die Deutsche Gesellschaft für Ernährung hat Regeln zur Zusammenstellung formuliert, die sie als „vollwertige Ernährung“ bezeichnet.

Die Nahrung wird chemisch in ihre Grundbestandteile aufgetrennt.

Als Makronährstoffe werden Substanzen zusammengefasst, die der Körper in verhältnismäßig großen Mengen benötigt und die entweder als Energielieferanten oder als Baustoffe dienen. Es gibt vier Makronährstoffgruppen: Proteine, Fette, Kohlenhydrate und Ballaststoffe.

Als Mikronährstoffe werden Substanzen zusammengefasst, die der Körper in geringen Mengen benötigt und z. B. Teil von Enzymen sind. Mikronährstoffe sind Gegenstand der Lebensmittelforschung.

Trotz der Wichtigkeit von Mikronährstoffen für den Körper – konzentriert sich die Züchtung und Verarbeitung von Lebensmitteln in der Landwirtschaft auf die Makronährstoffe (Kohlenhydrate, Proteine und Fette) und visuelle Aspekte. Für den Konsumenten besteht in der Praxis keine Möglichkeit, die Qualität von Lebensmitteln in dieser Hinsicht (etwa visuell oder geschmacklich) zu bewerten.

Proteine sind vor allem für den Muskel- und Zellaufbau nötig. Auch können sie im Körper zur Energiegewinnung verwertet werden, die DGE empfiehlt hier, dass mindestens 10 % des Energiebedarfs aus Proteinen und Aminosäuren gedeckt werden. Da die Anteile der verschiedenen Aminosäuren aus tierischen Quellen eher dem Bedarf des Menschen entsprechen, besitzen tierische Quellen eine höhere biologische Wertigkeit. Die Annahme, dass 10 % reichen, trifft jedoch nur unter sehr engen Voraussetzungen zu (wenig Körpergewicht, kein Sport, keine körperliche Arbeit etc.), da für die Aufrechterhaltung der Proteinstrukturen des Körpers 0,8 g/kg Körpergewicht als angemessen gelten. Soll nun mit Training auch noch Muskelmasse aufgebaut oder im Rahmen einer Diät ("Low Carb") Protein im Energiestoffwechsel eingesetzt werden, so reichen die 0,8 g/kg bei weitem nicht aus. Bis ca. 4 g/kg Körpergewicht kann die Leber am Tag verstoffwechseln. Wo dazwischen die individuell richtige Menge liegt, hängt von der körperlichen Belastung ("Training") ab.

Proteinreiche Lebensmittel enthalten mindestens 10 g/100 g verzehrbare Masse. Die folgenden Werte stammen aus der "Lebensmitteltabelle für die Praxis" von Souci, Fachmann, Kraut.

<nowiki>*</nowiki> gehört zu den Leguminosen

Da tierische Proteinquellen allerdings etwa in der veganen Ernährung nicht vorkommen, gilt proteinreichen Pflanzen ein besonderes Augenmerk. Da pflanzliche Proteinquellen die Proteine nicht in derselben Zusammensetzung (essenzielles Aminosäure-Spektrum) liefern, wie dies der menschliche Körper benötigt, gilt es verschiedene Pflanzen zu kombinieren und durch eine etwas größere Menge und durch die Kombination verschiedener Quellen den Mangel zu kompensieren.

Kohlenhydrate stellen eine der drei Quellen der Energiegewinnung dar, sind jedoch im Gegensatz zu den anderen beiden, Proteinen und Fettsäuren, kein essenzieller Nahrungsbestandteil. Laut Empfehlung der Deutschen Gesellschaft für Ernährung (DGE) sollen 55 % des Energiebedarfs aus Kohlenhydraten gedeckt werden.

Die DGE empfiehlt vor allem Kohlenhydrate aus ballaststoffreichen Pflanzen, da diese langsamer vom Körper aufgenommen werden (niedriger glykämischer Index). Aufgrund der geringen Energiedichte ballaststoffreicher Pflanzen sind entsprechend große Mengen zu konsumieren, wodurch diese mengenmäßig die Hauptbestandteile der Ernährung ausmachen sollten.

Einfachzucker gelangen zügig ins Blut, von dort in die Zellen und bieten sich als schnell verfügbare Energiequelle an. Allerdings ist diese nicht lange im Blut verfügbar, da der Körper auf große Mengen Zucker im Blut mit entsprechend großen Mengen an Insulin reagiert. Das Insulin sorgt u. a. dafür, dass die überschüssige Energie in Form von Fett in den Fettzellen eingelagert wird. Der Regelkreislauf dafür ist recht komplex und wird im Artikel Energiebilanz der Ernährung näher erläutert.

Die weltweit wichtigsten Lieferanten von Kohlenhydraten zur menschlichen Ernährung stellen

Unter der Bezeichnung Ballaststoff werden chemische Verbindungen zusammengefasst, hauptsächlich Kohlenhydrate, die vom Menschen (und einigen Tieren) entweder gar nicht oder nicht vollständig metabolisiert werden können. Anders als die Bezeichnung vermuten lässt, sind Ballaststoffe ein sehr wichtiger Bestandteil der menschlichen Nahrung und erfüllen wesentliche Funktionen bei der Verdauung.

Da Fette eine sehr hohe Energiedichte besitzen, werden vom Körper geringere Mengen an fetthaltigen Lebensmitteln benötigt um Energie zu gewinnen. Einige wenige Fettsäuren sind essenziell und dienen der Synthetisierung weiterer Substanzen.

Fette sind der Hauptbestandteil von Biomembranen und dienen auch der Energiegewinnung. Es kommen viele Fettsäuren in der Natur vor, aber unter den Gesichtspunkten der menschlichen Ernährung sind folgende Klassen von Fettsäuren relevant:

Essenzielle Fettsäuren sind Fettsäuren, die der Körper nicht selbst aus anderen Stoffen herstellen kann, sondern durch die Nahrung aufgenommen werden müssen, und gehören damit – neben den Essenziellen Aminosäuren und einigen Mineralien – zur Gruppe der Essenziellen Stoffe. Die Essenziellen Fettsäuren sind Linolsäure (eine Omega-6-Fettsäure) und Linolensäure (eine Omega-3-Fettsäure).

Die essenziellen Fette sind am Transport von Nährstoffen und Stoffwechselprodukten beteiligt und werden damit auch für die Regeneration der Zellen benötigt. Die Omega-3-Fettsäuren werden hierbei insbesondere für den Herzkreislauf, das Immun- und das Nervensystem benötigt. Ein Mangel an Omega-3-Fettsäuren kann Krankheiten wie hohen Blutdruck, hohe LDL-Cholesterinwerte, Herzerkrankungen, Diabetes mellitus, Rheumatoide Arthritis, Osteoporosis, Depression, Bipolare Störung, Schizophrenie, Aufmerksamkeitsdefizit, Hautkrankheiten, entzündliche Darmerkrankungen, Asthma, Darmkrebs, Brustkrebs und Prostatakrebs begünstigen.

Bei den Omega-3 Fetten gilt es zudem zu beachten, dass pflanzliche Quellen α-Linolensäure (ALA) enthalten, während Fisch Eicosapentaensäure (EPA) und Docosahexaensäure (DHA) liefert. Gesundheitliche Vorteile ergeben sich sowohl durch die pflanzliche als auch die tierische Variante, da der menschliche Körper enzymatisch ALA in EPA und EPA in DHA umwandeln kann. Ein gesundheitlicher Vorteil durch die Ergänzung von Omega-3 über die Einnahme von Fischölkapseln ist hingegen nicht nachgewiesen.

Die Fette werden auch für Ausdauersport benötigt. Da der Körper nur eine geringe Menge an Kohlenhydraten speichern kann, können bei entsprechender sportlicher Betätigung bereits nach 30 Minuten die Kohlenhydrat-Reserven aufgebraucht sein. Für längere Betätigung greift der Körper auf Fette zu, weshalb für Ausdauersport eine höhere Menge an Essenziellen Fettsäuren konsumiert werden muss.

Viele Stoffe werden nach der Menge, in der sie im Körper vorkommen oder benötigt werden, unterschieden. Hierbei sind "Mengenelemente" im menschlichen Körper zu formula_1 enthalten, während "Spurenelemente" in geringerer Konzentration vorkommen. Diese Konzentration unterscheidet sich von der Häufigkeit der chemischen Elemente auf der Erde und ist somit vom Begriff der chemischen Spurenelemente zu unterscheiden.

Elektrolyte sind elektrisch leitfähige Salze aus Mineralien. Hierbei sind insbesondere die Elemente Calcium, Chlor, Magnesium, Kalium und Natrium beteiligt. Die Salze werden in allen Körperflüssigkeiten und allen Stoffwechselvorgängen benötigt. Eine besondere Rolle spielen sie beim Transport von Nährstoffen und Stoffwechselprodukten im Blut sowie den Nervenfunktionen. Die wichtigste Verbindung ist das Kochsalz.

Elektrolyte werden insbesondere über die Nieren mit dem Urin sowie durch Schweiß ausgeschieden. Bei einem hohen Konsum von Wasser mit geringem Elektrolytgehalt sowie starker körperlicher Betätigung und Hitze kann es zu einem Mangel an Elektrolyten und damit zur Wasservergiftung kommen.

Für den Menschen gibt es essentielle Spurenelemente, die nur in geringen Mengen zugeführt werden sollen. Sowohl ein Mangel als auch ein Überangebot kann gesundheitliche Folgen haben. Oft werden diese Elemente fälschlich als „Mineralien“ bezeichnet. Manche werden z.T. künstlich über Speisesalz zugeführt (Jod und Fluor). Das Eisen wird wegen seiner Wirkungsweise zu den Spurenelementen gezählt, obwohl im menschlichen Körper etwa 60 mg/kg enthalten sind und das Element selbst das vierthäufigste auf der Erde ist.

Vitamin B (Cobalamin) wird ausschließlich in Mikroorganismen (etwa im Dickdarm) gebildet und über verunreinigte Lebensmittel aufgenommen. Vitamin B wird in der Leber gespeichert. Die tierische Leber stellt daher auch die potenteste Quelle dar. Da der Bedarf an diesem Vitamin für den Körper jedoch nur sehr gering ist, kann der Körper notfalls mit dem in der Leber gespeicherten Vorrat mehrere Jahre auskommen.

Die Europäische RDA geben die Menge an, die nach dem aktuellen wissenschaftlichen Kenntnisstand für ausreichend angesehen wird, um den täglichen Bedarf des Menschen zu decken.

Neben den sogenannten Spurenelementen gibt es empfohlene Mengen auch für andere Inhaltsstoffe.

Als "pflanzliche Stoffe" werden in Pflanzen enthaltene chemische Substanzen zusammengefasst, deren Bedeutung in der menschlichen Ernährung unklar ist, die jedoch günstige Eigenschaften haben könnten. Dazu gehören etwa Flavonoide, pflanzliche Sterole sowie pflanzliche Schwefelverbindungen (z. B. aus Knoblauch und Lauch).

Aufgrund von Stoffwechselvorgängen, durch UV-Strahlung und Schadstoffe aus der Umwelt können freie Radikale im menschlichen Körper entstehen, die die Zellen chemisch angreifen. Um diese freien Radikale zu binden und sich davor zu schützen, werden vom Körper Antioxidantien benötigt. Zu den natürlich vorkommenden Antioxidantien gehören Vitamin C, Vitamin E, und Carotinoide.

Der Körper benötigt Wasser vor allem aufgrund von Verlusten durch die Atmung, für Stoffwechselvorgänge und Kühlung durch Verdunstung über die Haut. Der tägliche Wasserbedarf eines Menschen bezogen auf das Körpergewicht "ρ" beträgt etwa formula_2.


Da der Körper bei heißem Wetter und bei körperlicher Betätigung zusätzliche Wärme über Verdunstung abführen muss, kann der Wasserbedarf auch höher liegen. Ein Liter Wasser kann 600 kcal an Wärme durch Verdunstung abführen. Da die abzuführende Energiemenge abhängig von den Wetterbedingungen, der konkreten Betätigung und den physischen Gegebenheiten des Menschen individuell verschieden ist, stellt der angegebene Wert nur einen Richtwert dar.

Ein Mensch benötigt in der Stunde etwa 1 kcal (= 4,1868 kJ) Energie je Kilogramm Körpergewicht an Grundumsatz.


Aufgrund von Aktivitäten hat der Körper einen zusätzlichen Energieverbrauch, den Leistungsumsatz. Der Gesamtumsatz ist die Summe aus Grundumsatz und Leistungsumsatz.

In einer ausgewogenen Ernährung sollte – über einen Zeitraum von mehreren Tagen gemittelt – etwa 55 % des Energiebedarfs aus Kohlenhydraten, mindestens 15 % aus Proteinen und 30 % aus Fetten stammen. Für Low-Carb-Diäten kann auch der Anteil der Fette höher und im Gegenzug der Anteil an Kohlenhydraten niedriger ausfallen. Die Voraussetzung bilden jedoch besonders hochwertige Fette.

Wird Sport oder körperlich anstrengende Arbeit ausgeübt, muss aufgrund des höheren Energieverbrauchs zusätzliche Energie zugeführt werden. Abhängig von der Intensität der Aktivität – und damit der Belastungszone – werden vom Körper unterschiedliche Energiequellen benötigt.

Hierbei ist

Der Energiegehalt "E" eines Lebensmittels berechnet sich aus der Masse "m" des Inhaltsstoffes multipliziert mit dessen Brennwert "H". Für Proteine und Kohlenhydrate beträgt der Brennwert etwa formula_5, während der Brennwert von Fetten etwa formula_6 beträgt. Ethanol hat einen Energiegehalt von etwa formula_7. Der Brennwert anderer Inhaltsstoffe kann in der Praxis meist vernachlässigt werden.


Vor allem bei Personen mit Mangelerscheinungen (das heißt. auch Fettleibigkeit) empfiehlt es sich eine überschlagsmäßige Berechnung der in einer Woche konsumierten Lebensmittel durchzuführen. Verschiedene ballaststoffreiche Gemüse mit geringer Energiedichte können und sollen in nahezu beliebiger Menge ergänzt werden.

Mit den Besonderheiten der Ernährung bei Krankheit beschäftigt sich die Ernährungsmedizin. Bei bestimmten Krankheiten werden zusätzlich zur medikamentösen Therapie Diäten verordnet, um den Krankheitsverlauf zu begünstigen. In der Medizin unterscheidet man prinzipiell:


Alle Tiere, sind auf eine Reihe von Nährstoffen angewiesen, die ihr Körper nicht selbst synthetisieren kann. Diese Nährstoffe nennt man essenziell (lebensnotwendig). Dazu zählen auch Vitamine. Vitamine (lateinisch: vita = Leben) werden in geringsten Mengen (µg/kg pro Tag) benötigt. Sie wirken meist als Cofaktoren zu Enzymen. Während Pflanzen keine Vitamine benötigen, kann der Mensch manche Stoffe nicht selbst bilden und ist daher obligatorisch auf deren Zufuhr angewiesen. Von essenziellen Aminosäuren und den essenziellen ungesättigten Fettsäuren Linol- und Linolensäure benötigte der Mensch täglich größere Mengen (mg/kg pro Tag).

Entspricht die Menge oder die Zusammenstellung einer Ernährung nicht den Anforderungen des menschlichen Organismus, so spricht man von Fehl- oder Mangelernährung. Diese Bezeichnungen werden gelegentlich synonym verwendet; Fehlernährung ist allerdings weiter gefasst als Mangelernährung, da Fehlernährung sowohl eine Unter- als auch eine Überversorgung mit Nahrungsbestandteilen beschreibt. Mangelernährung bedeutet dagegen stets eine Unterversorgung mit bestimmten, essenziellen Nahrungsbestandteilen. Eine Fehlernährung durch Überversorgung, insbesondere mit Nahrungsenergie, wird im Allgemeinen mit der Ernährungssituation in Industrieländern in Verbindung gebracht, während eine Mangelernährung als typisch für sogenannte Entwicklungsländer gesehen wird. Trotz der allgemeinen Überversorgungen ist die mangelhafte Versorgung mit einzelnen Nahrungsbestandteilen aber auch in Industrieländern eine häufige Krankheitsursache. Hier wird sie durch eine falsche Nahrungszusammensetzung verursacht, tritt aber auch als sekundärer Effekt, zum Beispiel als Folge krankheitsbedingter Malabsorption auf. Spezielle Ernährungsformen wie Vegetarismus sind dagegen an sich keine Ursache von Mangelernährung, sie sind, im Gegenteil, oft sogar mit einem besseren Ernährungsstatus verknüpft.

In den Industrieländern ist die Überernährung, als häufigster Faktor der Fehlernährung, für einen großen Teil der hohen und stetig steigenden Kosten im Gesundheitswesen verantwortlich. Übergewicht erhöht das Risiko von Herz-Kreislauferkrankungen und zwar sowohl direkt, als auch indirekt über die Begünstigung weiterer Risikofaktoren, wie zum Beispiel hohe Cholesterinwerte, Bluthochdruck oder Diabetes mellitus. Sowohl Über- als auch Unterversorgung mit Nahrungsenergie haben zudem einen negativen Einfluss auf das Immunsystem und reduzieren die Infektionsresistenz. Unter den Mangelernährungen ist die Protein-Energie-Malnutrition (PEM), mit den Krankheitsbildern Marasmus und Kwashiorkor, die häufigste Form der Fehlernährung und vor allem in industriell weniger entwickelten Ländern anzutreffen. Weitere in größerem Umfang anzutreffende Formen der Mangelernährung sind Mikronährstoffmängel, insbesondere Anämien sowie Vitamin-A- und Jodmangel. Seltener treten dagegen der Vitamin-D-Mangel mit dem Krankheitsbild der Rachitis, der Vitamin-C-Mangel (Skorbut), Thiaminmangel (Beriberi) und Niacinmangel (Pellagra) auf.

Fehl- und Mangelernährung können ihrerseits Krankheiten verursachen oder begünstigen, etwa Skorbut bei Vitamin-C-Mangel, Beriberi bei Vitamin-B1-Mangel oder Diabetes mellitus bei Adipositas (starkem Übergewicht). Für diese und andere Krankheiten, vor allem für die Mangelerkrankungen, ist der Zusammenhang mit Fehl- oder Mangelernährung wissenschaftlich bewiesen.

Des Weiteren gibt es eine große Zahl an Krankheiten, insbesondere die Zivilisationskrankheiten, für die diskutiert wird, ob sie durch die moderne Ernährungsweise zumindest mitverursacht werden, zum Beispiel Arteriosklerose, Bluthochdruck und Krebs. Einen wissenschaftlichen Nachweis dieser Annahme gibt es bisher nur für wenige Erkrankungen. Generell sind Zusammenhänge zwischen Ernährung und Krankheit, methodisch bedingt, schwierig nachzuweisen. Für die meisten Zivilisationskrankheiten gibt es höchstwahrscheinlich nicht nur eine einzige Ursache, sondern eine Kombination von Ursachen, darunter genetische Veranlagung, unzureichende körperliche Aktivität, Ernährung und Umwelteinwirkungen.

Im globalen Maßstab befasst sich die "Food and Agriculture Organization" (FAO) der Vereinten Nationen (UNO) mit für die Menschheit zentralen ernährungspolitischen Fragen. Besonders in den sogenannten Entwicklungsländern bekämpft die FAO mit unterschiedlichen Projekten Mangel- und Unterernährung. Dabei werden auch traditionelle Nahrungsquellen neu erschlossen, wie im Projekt "Edible Forest", das in tropischen und subtropischen Regionen für den Verzehr von Insekten zur ausreichenden Versorgung mit tierischem Eiweiß wirbt.

In Deutschland spielt sowohl auf Bundesebene als auch auf der jeweiligen Landesebene das Thema Ernährung politisch eine Rolle. Während es gegenwärtig auf Bundesebene beim Bundesministerium für Ernährung und Landwirtschaft angesiedelt ist, gibt es in den Bundesländern unterschiedliche Zuständigkeiten, teilweise ist es dort dem Verbraucherschutz zugeordnet. Wichtigste Entwicklung in der Ernährungspolitik ist der Nationale Aktionsplan IN FORM – "Deutschlands Initiative für gesunde Ernährung und mehr Bewegung". Es handelt sich dabei um eine gemeinsame Initiative von Bund, Ländern und Kommunen zur Verbesserung des Ernährungs- und Bewegungsverhaltens der gesamten deutschen Bevölkerung. Koordiniert wird dieser auf Kabinettsbeschluss von Juni 2008 beruhende Aktionsplan auf Bundesebene in Ernährungsfragen vom BMELV mit Sitz in Bonn.








</doc>
<doc id="1445" url="https://de.wikipedia.org/wiki?curid=1445" title="Proteinbiosynthese">
Proteinbiosynthese

Proteinbiosynthese (PBS) ist die Neubildung von Proteinen in Zellen und damit der für alle Lebewesen zentrale Prozess einer Genexpression, bei der nach Vorgabe genetischer Information Proteine aus Aminosäuren aufgebaut werden.

Die eigentliche Synthese eines Proteins aus seinen Bausteinen, den proteinogenen Aminosäuren, findet an den Ribosomen statt und wird auch als Translation bezeichnet, da hierbei die Basenfolge einer messenger-RNA (mRNA) in die Abfolge von Aminosäuren eines Peptids übersetzt wird. Dies geschieht, indem fortlaufend je einem Codon der mRNA das Anticodon einer transfer-RNA (tRNA) zugeordnet wird und deren jeweils einzeln transportierte Aminosäure an die benachbarte gebunden wird (Peptidbindung), sodass eine Kette mit charakteristischer Aminosäuresequenz entsteht. Dieses Polypeptid kann sich im umgebenden Medium zu einem Gebilde strukturierter dreidimensionaler Form auffalten, dem "nativen Protein". Häufig wird es durch Abspaltungen, Umbauten und Anbauten danach noch verändert, posttranslational modifiziert.

Während bei prokaryoten Zellen die ringförmige DNA frei im Zytosol vorliegt und die ribosomale Proteinsynthese zumeist unmittelbar und prompt an der gerade eben transkribierten mRNA-Vorlage erfolgt, sind die Verhältnisse bei eukaryoten Zellen komplizierter. Für das auf mehrere Chromosomen verteilte Genom ist hier mit dem Zellkern (Nukleus) ein eigenes Kompartiment geschaffen, in dessen Karyoplasma auch die Transkription stattfindet. Die primär gezogene RNA-Kopie (hnRNA) wird zunächst stabilisiert, überarbeitet und auf den Kernexport vorbereitet, bevor sie als mRNA eine Kernpore passiert und ins Zytoplasma gelangt, das die Untereinheiten der Ribosomen enthält. Diese räumliche Aufteilung und der mehrschrittige Prozessweg erlauben somit zusätzliche Weisen, eine (hn)RNA posttranskriptional zu modifizieren und darüber die Genexpression zu regulieren beziehungsweise bestimmte RNA-Vorlagen von der Proteinbiosynthese auszuschließen (Gen-Stilllegung).

Einige Arten von Bakterien, Archaeen und Pilzen können über ribosomale Proteinsynthese besondere Proteine aufbauen, die als Multienzymkomplexe eine nichtribosomale Peptidsynthese ermöglichen (NRPS).

Im ersten Schritt für eine Proteinbiosynthese in einer Zelle werden Abschnitte von Genen auf der doppelsträngigen DNA aufgesucht, abgelesen und in einzelsträngige RNA-Moleküle umgeschrieben. Bei diesem Vorgang werden der vorliegenden Folge von Nukleinbasen der DNA (Adenin, Guanin, Cytosin, Thymin) komplementär die Nukleinbasen von RNA-Bausteinen (Uracil, Cytosin, Guanin, Adenin) zugeordnet. In dem dann zum Strang verknüpften RNA-Transkript kommt Ribose anstelle der Desoxyribose und Uracil anstatt Thymin vor. Die genetische Information ist in der Basenfolge enthalten, ein Codogen auf der DNA wird transkribiert zu einem Codon auf der Boten- oder Messenger-Ribonukleinsäure, kurz mRNA genannt.

Für die Transkription eines Gens ist neben mehreren anderen Faktoren eine RNA-Polymerase nötig als Enzym, das den fortlaufenden Aufbau des RNA-Polymers abhängig von der DNA-Vorlage katalysiert. Die der Vorlage basenpaarend zugeordneten Ribonukleosidtriphosphate (UTP, CTP, GTP und ATP) als Bausteine werden – jeweils unter Abspaltung zweier Phosphatgruppen der Triphosphate – miteinander zum Polynukleotid einer RNA verknüpft. Dabei kann es unterschiedliche Typen der RNA-Polymerase geben für die Transkription von Genen, die mittels einer mRNA für Proteine codieren, und für die anderer Gene, beispielsweise für die Bildung einer rRNA oder einer tRNA.

Bei Eukaryoten findet die Transkription im Zellkern statt; daher muss die mRNA aus dem Kern in das Cytosol exportiert werden, da dort die Translation durchgeführt wird. Prokaryoten hingegen haben kein Kernkompartiment, die Transkription findet hier neben der Translation im Zellplasma statt.





Unter Translation versteht man die Übersetzung der Basensequenz der mRNA in die Aminosäuresequenz des Proteins, die an den Ribosomen geschieht. 
In der mRNA bilden jeweils drei aufeinander folgende Basen, ein Basentriplett, innerhalb des offenen Leserahmens ein Codon, welches für eine Aminosäure codiert (siehe hierzu genetischer Code). Am Ribosom werden die Codons entsprechend ihrer Abfolge in Aminosäuren translatiert und diese sequentiell zu einem Polypeptid verknüpft.

Zur Ausbildung einer Peptidbindung zwischen zwei Aminosäuren müssen sie in räumliche Nähe zueinander gebracht werden. Dazu wird die Oberfläche einer großen supramolekularen Struktur benötigt. Diese Aufgabe erfüllen die Ribosomen, zusammengesetzt aus einer kleinen und einer großen Untereinheit, welche zwei nebeneinanderliegende Bindungsstellen formt: die A-Stelle und die P-Stelle.

Da es keine strukturelle Verwandtschaft zwischen einem Codon und der dazugehörigen Aminosäure gibt, wird ein Zwischenstück benötigt, das einerseits die Aminosäure bindet und andererseits das zugehörige Codon auf der mRNA erkennt. Diese vermittelnde Funktion übernehmen Transfer-Ribonukleinsäure-Moleküle, verschiedene tRNAs, als Aminosäuren-„Transporter“ mit Erkennungsregion. Sie besitzen zwei auseinanderliegende exponierte Bindungsstellen: die Aminosäurebindungsstelle und das Anticodon. Die Aminosäurebindungsstellen der tRNAs werden durch die Aminoacyl-tRNA-Synthetasen spezifisch mit der passenden Aminosäure beladen. Die tRNA erkennt mit dem Anticodon das komplementäre Codon auf der mRNA und bindet sich spezifisch daran.

Der Translationsprozess als solcher lässt sich in drei Phasen unterteilen: die Initialphase, Elongationsphase und schließlich die Termination:




Die Polypeptidketten einiger Proteine werden schon während der Translation (cotranslational) durch spezielle Enzyme verändert, in den meisten Fällen aber werden Proteine erst nach Abschluss der Translation (posttranslational) modifiziert. Während Chaperone den formgebenden Prozess der Proteinfaltung beeinflussen, von dem auch die Assoziation zu Proteinkomplexen abhängt, verfügt eine Zelle daneben über eine Vielzahl an Möglichkeiten, die Struktur eines Proteins spezifisch abzuwandeln, derart auch funktionell andere Proteinspezies zu schaffen und so durch Modifikationen das Proteom zu erweitern.

Zu diesen Modifikationen gehören die Abspaltung von einzelnen endständigen Aminosäuren oder auch die längerer Peptidsequenzen bei Präkursor-Proteinen, die Einführung zusätzlicher Bindungen, z. B. Disulfidbrücken zwischen Cysteinresten, oder funktioneller Gruppen, wie Hydroxylierungen von Aminosäuren (Prolin zu 4-Hydroxyprolin durch die Prolyl-4-Hydroxylase, Lysin zu Hydroxylysin durch die Lysylhydroxylase), sowie Oxidationen (z. B. kovalente Quervernetzungen mittels Lysinresten durch die Lysyloxidase), Carboxylierungen oder Decarboxylierungen und zahlreiche weitere. Beispielsweise entstehen durch Glykosylierungen Glykoproteine, durch Acylierungen und Prenylierungen Lipoproteine.

Die einzelnen Schritte von Modifizierungen werden jeweils durch besondere Enzyme katalysiert, deren Vorkommen oft auf bestimmte Organellen, Zellen oder Gewebe beschränkt ist. Außerdem kann die Abfolge modifizierender Schritte bzw. deren zeitlicher Verlauf variiert werden, abhängig von Zellmilieu, Entwicklungsphase oder Umgebungsbedingungen. Das Kollagenmolekül etwa durchläuft eine Reihe posttranslationaler Modifikationen, von denen einige erst im Extrazellularraum stattfinden.

Da viele Proteine als Zielort (engl. "target") nicht das Zytosol, sondern den Extrazellularraum, die Zellmembran, die Organellen wie Chloroplasten, Mitochondrien, Peroxisomen, Zellkern oder endoplasmatisches Retikulum haben, hat die Zelle verschiedene Mechanismen, die Proteine dorthin zu verbringen. Diese Proteine enthalten meist eine N- oder auch C-terminale Signalsequenz, die je nach Targetmechanismus sehr unterschiedlich aufgebaut sein kann. In einigen Fällen gibt es keine terminale Signalsequenz, sondern interne Signale der Peptidkette, die über den Zielort des Proteins bestimmt.


Neben den oben beschriebenen Signalsequenzen ermöglicht eine Glykosylierung ein Targeting für den Einbau in die Zellmembran bzw. für die Exozytose. Beide Wege führen meist über Golgi-Vesikel.

Ein-Gen-ein-Polypeptid-Hypothese



</doc>
<doc id="1446" url="https://de.wikipedia.org/wiki?curid=1446" title="Edsger W. Dijkstra">
Edsger W. Dijkstra

Edsger Wybe Dijkstra ( ; * 11. Mai 1930 in Rotterdam; † 6. August 2002 in Nuenen) war ein niederländischer Informatiker. Er war der Wegbereiter der strukturierten Programmierung. 1972 erhielt er den Turing Award für grundlegende Beiträge zur Entwicklung von Programmiersprachen.

Edsger Dijkstra wurde als Sohn eines Chemikers und einer Mathematikerin geboren. Nach dem Besuch des Gymnasiums Erasmianum in Rotterdam studierte er ab 1948 Mathematik und theoretische Physik an der Universität Leiden. 1951 erreichte er den Bachelor-Grad und besuchte im Anschluss einen Programmierkurs bei Maurice V. Wilkes an der University of Cambridge. Er setzte sein Studium in Leiden fort, arbeitete fortan aber nebenbei am "Mathematisch Centrum" (heute Centrum Wiskunde & Informatica – Zentrum für Mathematik und Informatik) in Amsterdam. Sein Betreuer dort war Direktor Adriaan van Wijngaarden, der ihn auch überredete, gänzlich zum Programmieren zu wechseln, statt ernsthafter theoretischer Physiker zu werden. 
1956 erreichte er den Master-Grad und ging als Vollzeitangestellter zum Mathematisch Centrum. Dijkstra wird als erster Programmierer der Niederlande bezeichnet und schrieb 1959 an der Universität von Amsterdam seine Doktorarbeit über die vom Mathematisch Centrum entwickelte Electrologica X1, deren grundlegende Software er schrieb.

1962 wurde Dijkstra Mathematikprofessor an der Technischen Hochschule Eindhoven. An anderswo bereits angebotene Informatik-Lehrstühle wollte er nicht, da er hierfür noch keine ausreichende wissenschaftliche Grundlage sah. Dennoch bot er seinen Studenten die Möglichkeit, sich nach mindestens drei Jahren mathematischem Studium auf Themen der Informatik zu spezialisieren. Er blieb weiter der Ansicht, dass ein Studium der Informatik stark mathematisch geprägt und etwa ein Einführungskurs für Programmierung eine formalmathematische Veranstaltung frei von Programmiersprachen zu sein habe. Ab 1973 schränkte er seine Tätigkeit an der Universität auf eine außerordentliche Professorenstelle, repräsentiert durch den von ihm etablierten "Eindhoven Tuesday Afternoon Club", ein, wo er dienstagnachmittags mit Kollegen wissenschaftliche Probleme und die neuesten Papers besprach, und wurde hauptamtlich "Research Fellow" der Burroughs Corporation. 1984 wechselte er auf den "Schlumberger Centennial Chair in Computer Sciences" an der University of Texas at Austin. 1999 wurde er emeritiert.

Dijkstra starb an Krebs in seinem Heim in Nuenen. Er hinterließ seine Frau Ria, welche er 1957 geheiratet hatte, sowie drei Kinder.

Zu seinen Doktoranden gehören Arie Habermann und Martin Rem.

Unter seinen Beiträgen zur Informatik finden sich der Dijkstra-Algorithmus zur Berechnung eines kürzesten Weges in einem Graphen (1959 in einem dreiseitigen Artikel veröffentlicht), die erstmalige Einführung von Semaphoren zur Synchronisation zwischen Threads und das damit zusammenhängende Philosophenproblem sowie der Bankieralgorithmus. Des Weiteren stammt der Shunting-yard-Algorithmus, ein Algorithmus um mathematische Terme von der Infixnotation in die umgekehrte polnische Notation oder in einen abstrakten Syntaxbaum zu überführen, von ihm.

Basierend auf diesen Erfahrungen entwarf er das Multitasking-Betriebssystem THE (nach "Technische Hogeschool Eindhoven"), das für seine Schichtenstruktur bekannt wurde. Niklaus Wirth berichtet, dass Dijkstra im Rahmen dieser Arbeit erkannte, nicht für Teamarbeit geeignet zu sein, und fortan nur noch alleine arbeitete.

Ende der 1950er Jahre war Dijkstra am Entwurf von Algol 60 beteiligt, 1960 stellte er den ersten Compiler dafür fertig. Ferner entwarf er den Sortieralgorithmus Smoothsort und entdeckte den Algorithmus von Prim (auch "Prim-Dijkstra-Algorithmus" oder "Algorithmus von Jarnik, Prim und Dijkstra") wieder.

Dijkstra schrieb über 1300 Manuskripte fachlicher und privater Natur, die er fotokopierte und jeweils an etliche Kollegen postalisch versendete, meist aber nicht veröffentlichte. Heute sind viele dieser sogenannten EWD-Manuskripte (nach seinen Initialen) in einem Online-Archiv gesammelt. Für die Burroughs Corporation schrieb er über 500 wissenschaftliche Berichte. Seine populärste Abhandlung ist "Go To Statement Considered Harmful" über den Goto-Befehl und warum er nicht benutzt werden sollte. Er führte den Begriff der strukturierten Programmierung in die Informatik ein und popularisierte in seiner Turing-Lecture "The Humble Programmer" auch den Begriff der Softwarekrise, den er als regelmäßiger Redner an Friedrich L. Bauers "International Summer School Marktoberdorf" dort aufgenommen hatte.





</doc>
<doc id="1447" url="https://de.wikipedia.org/wiki?curid=1447" title="Eichen">
Eichen

Die Eichen ("Quercus") sind eine Pflanzengattung in der Familie der Buchengewächse (Fagaceae).

Eichen-Arten sind sommergrüne oder immergrüne Bäume, seltener auch Sträucher.

Die wechselständigen und spiralig an den Zweigen angeordneten Laubblätter sind meist in Blattstiel und Blattspreite gegliedert. Die dünnen bis ledrigen, einfachen Blattspreiten sind gelappt oder ungelappt. Die Blattränder sind glatt oder gezähnt bis stachelig gezähnt. Die unscheinbaren, extrapetiolaren Nebenblätter fallen früh ab (nur bei "Quercus sadleriana" sind sie auffälliger).

Eichen-Arten sind einhäusig getrenntgeschlechtig (monözisch). Die meist zu mehreren an der Basis junger Zweige sitzenden Blütenstände sind eingeschlechtig. Die Blüten sind sehr einfach gebaut, wie es bei windbestäubten (anemophilen) Taxa häufig der Fall ist. Die männlichen Blüten sind in hängenden Blütenständen (Kätzchen) zusammengefasst. Die Blütenhüllblätter sind verwachsen. Die männlichen Blüten enthalten meist sechs (zwei bis zwölf) Staubblätter. Die weiblichen Blüten enthalten meist drei (bis sechs) Fruchtblätter und Stempel. Jede Cupula enthält nur eine weibliche Blüte.

Eichen sind insbesondere an ihrer Frucht, der Eichel, zu erkennen und in den einzelnen Arten zu unterscheiden. Die Eichel ist eine Nussfrucht. Sie reifen im ersten oder zweiten Jahr nach der Bestäubung. Jede Nussfrucht ist von einem Fruchtbecher eingeschlossen, der "Cupula" genannt wird.

Die Chromosomengrundzahl beträgt x = 12.

Schon von Alters her ist den Menschen aufgefallen, dass Eichen eine ungewöhnliche Vielfalt von Insekten beherbergen (bis zu 1000 Arten in einer Krone). Die Spezialisierung zahlreicher Insekten-Arten auf "Quercus"-Arten gilt als ein Zeichen des hohen entwicklungsgeschichtlichen Alters (Koevolution).

Eichen-Arten sind Nahrungshabitat der Raupen von vielen Schmetterlingsarten. Sie wird in Mitteleuropa nur von der Salweide übertroffen. Beide beherbergen über 100 Arten.


"Siehe auch:" 

In Deutschland nehmen die Eichen nach der Dritten Bundeswaldinventur (2012) mit einer Fläche von 1,1 Millionen Hektar einen Anteil von 11,6 Prozent an der Waldfläche ein. Die Eichenfläche in den deutschen Wäldern hat sich zwischen 2002 und 2012 um 70.000 Hektar vergrößert. Die Eichen sind damit nach der Rotbuche die zweithäufigste Laubbaumgattung in Deutschland. Es handelt sich dabei hauptsächlich um die einheimischen Eichenarten Stieleiche und Traubeneiche. Die aus Nordamerika eingeführte Roteiche nimmt mit einer Fläche von 55.000 Hektar nur einen Anteil von 0,5 Prozent ein. 

Eichen-Arten traten bereits im Tertiär auf. Sie finden sich fossil schon vor zwölf Millionen Jahren, etwa in Sedimenten der Niederrheinischen Bucht. Das im oligozänen/eozänen Baltischen Bernstein sehr häufige Sternhaar wird ebenfalls Eichen zugeschrieben. Auch Eichenblüten sind im Baltischen Bernstein nicht selten. Die Zuordnung einiger kreidezeitlicher Pflanzenfossilien zu "Quercus" bzw. "Quercophyllum" ist indes umstritten.

Bei dem römischen Autor Quintus Ennius (239–169 v. Chr.) findet sich der früheste literarische Beleg für den lateinischen Namen einer "Quercus"-Art, 'quercus'.

"Quercus"-Arten gibt es in Nordamerika, Mexiko, auf den Karibischen Inseln, in Zentralamerika, in Südamerika nur in Kolumbien, in Eurasien und in Nordafrika. "Quercus" ist die wichtigste Laubbaumgattung der Nordhalbkugel. Ein Schwerpunkt der Artenvielfalt ist Nordamerika.

Die Gattung "Quercus" enthält etwa 400 bis 600 Arten, davon bis zu 450 in der Untergattung "Quercus" und bis zu 150 in der Untergattung "Cyclobalanopsis". 
Die Gattung wird in die Untergattungen "Quercus" (die wiederum in Sektionen, darunter Weißeichen, Zerreichen und Roteichen gegliedert ist) und "Cyclobalanopsis" unterteilt: Hier eine Arten-Auswahl:







Die in Mitteleuropa heimischen Stiel- und Trauben-Eichen sind typische Vertreter der Weißeichen, wobei diese beiden Arten in weiten Bereichen gemeinsam vorkommen und zur Bastardisierung neigen, daher häufig nicht eindeutig zu differenzieren sind.
Sie haben Blätter mit abgerundetem Rand. Sie sind sogenannte Lichtbaumarten, das heißt, sie benötigen im Wachstum mehr Licht als etwa die Rotbuche und bilden selbst offene, lichte Kronen. Die Nutzung von Wäldern zur Waldweide (Hutewald) hat deshalb die Ausbildung von Eichenwäldern gefördert, weil die weidenden Tiere den Nachwuchs der Rotbuchen gehemmt haben. Das verkernende Holz der Weißeichen ist sehr dauerhaft und wurde viel im Schiffbau verwendet. Die heimischen Arten bieten etwa 350 Insektenarten einen Lebensraum.

Die ursprünglich im östlichen Nordamerika heimische Roteiche wird erst seit etwa 100 Jahren in Mitteleuropa angebaut. Man findet die Roteiche in Mitteleuropa in Parks und Botanischen Gärten, seltener werden sie in Forsten angebaut.

Roteichen zeichnen sich durch spitze Blätter aus sowie durch Eicheln, die innerhalb von zwei Jahren reifen. Das Holz der Roteichen ist aufgrund von Porengängen nicht wasserdicht und daher weniger wertvoll als das der Weißeichen. Es wird aufgrund der lebhaften Maserung vielfach für Möbel verwendet.





Eine Besonderheit stellt die Mooreiche dar. Dabei handelt es sich nicht um eine Baumart, sondern um Eichenstämme, die über Jahrhunderte in Mooren, Sümpfen oder in Flussufern gelegen hatten und ausgegraben wurden. Die Gerbsäure des Eichenholzes verbindet sich mit den Eisensalzen des Wassers, wodurch das Holz sehr hart wird und sich stark verfärbt. Die Verfärbung kann sehr unregelmäßig sein und variiert von hellgrau über dunkelgelb, dunkelbraun, blaugrau bis tiefschwarz. Diese subfossilen Eichen können 600 bis 8500 Jahre alt sein.

Die Früchte (Eicheln) sind reich an Kohlenhydraten und Proteinen und wurden zur Eichelmast genutzt. Man trieb die Schweine zur Waldweide in die Wälder. In ur- und frühgeschichtlicher Zeit sowie in Notzeiten wurden Eicheln von Menschen als Nahrungsmittel genutzt. Von nordamerikanischen Indianern (z. B. den Maidu) wurden Eicheln regelmäßig als Grundnahrungsmittel genutzt.

Zur Verwendung als Nahrung müssen die geschälten und zerstoßenen Eicheln durch mehrmaliges Baden in Wasser allmählich von den wasserlöslichen Gerbstoffen befreit werden, was sich durch die ausbleibende Verfärbung des Wassers leicht erkennen lässt, wobei eine höhere Temperatur den Vorgang beschleunigt. Sie enthalten in hohen Mengen Tannine. Danach können sie, zum Beispiel als Mehlersatz für Breie und Kuchen oder als Kaffeeersatz „Muckefuck“, verarbeitet werden, wobei bei letzterer Verwendung die Gerbsäure wahlweise auch nicht oder nicht vollständig entzogen werden kann, etwa aus medizinischen Gründen.

Aus der jungen, glatten Rinde wurden Gerbstoffe für die Lohgerberei gewonnen (Eichenschälwald). Die Borke der Korkeiche ("Quercus suber") wird als Kork zur Herstellung von Korken, Korkfußböden und mehr verwendet. Von allen Eichenarten eignen sich nur ungefähr 180 zur Herstellung von Weinfässern, siehe auch Barrique. In der Volksheilkunde wurde borkenlose Eichenrinde genutzt, um Entzündungen im Mund und der Schleimhäute zu heilen.

Aus den Galläpfeln, die von der gemeinen Eichengallwespe hervorgerufen werden, hat man früher dokumentenechte Eisengallustinte gewonnen.

Alle Teile der Eiche sind wegen der enthaltenen Gerbstoffe leicht giftig und können zu gastrointestinalen Symptomen (Magenschleimhautreizung, Erbrechen, Durchfälle) führen (siehe dazu den Artikel: Liste giftiger Pflanzen). Als Heilpflanze wurde und wird die Eiche allerdings geschätzt. Auch die bis ins Mittelalter für die Frucht des Eiche gehaltene Eichenmistel fand magische und therapeutische Verwendung.

Die im Eichenholz enthaltenen Tannine und Aldehyde können beim Einatmen allergische Reaktionen (Rhinitis, Asthma) hervorrufen.

Die älteste Eiche in Europa soll die 1000-jährige Eiche Bad Blumau (Oststeiermark) sein. Zumindest wurde sie schon im Jahr 990 erstmals urkundlich erwähnt und wird auf etwa 1200 Jahre geschätzt. Ihr Stammumfang beträgt 8,75 Meter. Nach anderen Angaben soll eine Stieleiche in Bulgarien im Ort Granit, Bezirk Stara Zagora mit 1640 Jahren wohl der älteste Laubbaum Europas sein. Die älteste Eiche Deutschlands ist die Femeiche in Raesfeld-Erle, Kreis Borken, deren Alter auf bis zu 1500 Jahre geschätzt wird (Quelle: Brockhaus Enzyklopädie). Einen Stammumfang von 9,5 Metern hatte 1924 eine mächtige Eiche am Rande der Pommerschen Schweiz in der Gegend von Bad Polzin, die in einer Urkunde von 1321 als Grenzmal zwischen dem Land Belgard und dem bischöflichen Land Arnhausen genannt wurde und die als eine der ältesten von ganz Pommern galt. Als ihr Standort wird 1924 der (halbstündige) Wanderweg angegeben, der von Neu-Lutzig ("Nowe Ludzicko") über den Bahnhof Lutzig (Bahnhof "Stare Ludzicko") nach Dewsberg ("Dziwogóra") führt.






</doc>
<doc id="1448" url="https://de.wikipedia.org/wiki?curid=1448" title="Europa (Mythologie)">
Europa (Mythologie)

Europa (, altgriechische Aussprache ; Näheres zum Namen siehe unter Europa), eine Gestalt der griechischen Mythologie, ist die Tochter des phönizischen Königs Agenor und der Telephassa. Zeus verliebte sich in sie. Er verwandelte sich wegen seiner argwöhnischen Gattin Hera in einen Stier. Sein Bote Hermes trieb eine Kuhherde in die Nähe der am Strand von Sidon spielenden Europa, die der Zeus-Stier auf seinem Rücken entführte. Er schwamm mit ihr nach Matala auf der Insel Kreta, wo er sich zurückverwandelte. Der Verbindung mit dem Gott entsprangen drei Kinder: Minos, Rhadamanthys und Sarpedon. Auf Grund einer Verheißung der Aphrodite wurde der fremde Erdteil nach Europa benannt.

Die älteste literarische Referenz auf Europa ist in der Ilias von Homer zu finden, wo sie die Tochter des Phoinix ist. Antike Erzählungen des Europa-Mythos finden sich in der „Europa“ des Moschos und in den „Metamorphosen“ des Ovid. Es gibt viele verschiedene Sagen von der Entführung Europas.

Nach Ovid verwandelt sich Jupiter (röm. für Zeus) in einen Stier, ein besonders kräftiges, aber sehr friedlich aussehendes Exemplar mit reinem, schneeweißen Fell und kleinen Hörnern, die aussehen, als habe sie ein Künstler angefertigt. Jupiter mischt sich unter eine Herde königlicher Stiere, die Mercurius (röm. für Hermes) zuvor zum Strand getrieben hat, und nähert sich so Europa, die mit ihren Gefährtinnen am Strand ist. Europas Furcht ist bald überwunden, sie spielt mit dem Stier, füttert ihn, streichelt ihn und umwindet seine Hörner mit Blumen. Schließlich traut sie sich, auf seinen Rücken zu steigen – da geht der Stier ins Wasser und schwimmt aufs offene Meer hinaus. Er bringt sie nach Kreta, wo er seine Stiergestalt ablegt und sich offenbart.

Agenor schickte seine Söhne aus, ihre Schwester Europa zu suchen, doch die Nachforschungen bleiben erfolglos. Schließlich befragt Kadmos das Orakel von Delphi und wird von diesem angewiesen, die Suche nach seiner Schwester aufzugeben und stattdessen die böotische Stadt Theben zu gründen.

Nach anderen Quellen soll Europa nach der Affäre mit Zeus drei Söhne geboren haben. Anschließend wurde sie von Asterios, dem König von Kreta, geheiratet und wurde so zur Königin von Kreta. Asterios, der selbst keine Kinder hatte, adoptierte auch ihre drei Söhne.

Gerold Dommermuth-Gudrich weist darauf hin, dass die römische Fassung dieser Sage im Kern eine orientalische sei: Europa sei „nichts anderes als die Verkörperung der Ischtar oder Astarte, der babylonisch-syrischen Liebesgöttin, die die Griechen mit Aphrodite gleichsetzen. Noch zur Zeit des klassischen Griechentums wurde Europa als Europa-Astarte von den Phöniziern in Sidon verehrt“.

Die Historikerin Annette Kuhn hält dem durch die Ovid-Überlieferung patriarchal geprägten Mythos eine alternative Lesart entgegen, die das frühe Matriarchat einbezieht. So sieht sie das Matriarchat am Wirken, als die Mutter Europas, Telephassa, über Zeus eine Strafe für sein liebestolles Verhalten gegenüber Europa verhängt, und zwar die Verweigerung der Liebe Europas und das Sterben der Natur. Sie interpretiert den Mythos dahingehend, dass Zeus überhaupt erst in der „Verkleidung“ als Stier, welcher als ein Symbol für matriarchalische Ordnung fungierte – hervorgegangen aus dem damals noch weit verbreiteten mythologischen Symbol der „kosmischen Kuh“ –, sich Europa annähern konnte. „Liebe, so lautet die einfache Botschaft, kann nicht erzwungen werden.“

Dabei verweist Kuhn u. a. auf den Mythenforscher Robert von Ranke-Graves, der „in seiner quellenkritischen Nacherzählung der Europageschichte [schon] auf [eine] matriarchale Tradition hingewiesen“ habe.

Die ältesten entdeckten Vasenmalereien, welche eindeutig Europa abbilden, stammen bereits aus dem 7. Jahrhundert vor Christus. Spätere bildliche Darstellungen zeigen Europa meist, Ovids Beschreibung folgend, wie sie vom Zeus-Stier entführt wird. Sie ist meist nur leicht bekleidet oder ganz nackt, sitzt rittlings (in älteren Darstellungen), seitwärts oder halb-liegend (in jüngeren Darstellungen) auf dem Rücken des weißen Stieres, hält sich an ihm fest und zeigt dabei keine Zeichen von Furcht.

Nachdem Europa schon 1948 auf dem 5-DM-Schein zu sehen war, ist seit dem 2. Mai 2013 der Kopf der Sagengestalt auf dem 5-€-Schein im Wasserzeichen und im Hologramm abgebildet, dieser ist der erste Schein der zweiten Serie „Europa-Serie“ der Eurobanknoten. Die neue 10-€-Banknote, ebenfalls mit der Abbildung der Europa, befindet sich seit dem 23. September 2014 im Umlauf. Das abgebildete Porträt stammt von einer über 2000 Jahre alten Vase aus Süditalien, die im Pariser Louvre besichtigt werden kann.





</doc>
<doc id="1449" url="https://de.wikipedia.org/wiki?curid=1449" title="Elektrische Feldkonstante">
Elektrische Feldkonstante

</math>
Die elektrische Feldkonstante formula_4, auch Permittivität des Vakuums, elektrische Konstante, Dielektrizitätskonstante des Vakuums, oder Influenzkonstante, ist eine physikalische Konstante, die eine Rolle bei der Beschreibung von elektrischen Feldern spielt. Sie gibt das Verhältnis der elektrischen Flussdichte zur elektrischen Feldstärke im Vakuum an. Der Kehrwert der elektrischen Feldkonstanten tritt als Proportionalitätsfaktor im Coulomb-Gesetz auf.

Die Konstante formula_4 wird nach dem derzeitigen deutschsprachigen Entwurf des Internationalen Größensystems als "elektrische Feldkonstante" bezeichnet. Die Permittivität des Vakuums hat denselben Wert und wird noch als Erläuterung erwähnt. Weitere Bezeichnungen sind:

Aus den Maxwell-Gleichungen ergibt sich im Internationalen Einheitensystem (SI) ein einfacher Zusammenhang zwischen der elektrischen Feldkonstante, der magnetischen Feldkonstanten formula_2 und der Lichtgeschwindigkeit formula_3:

Nachdem 1948 erst die magnetische Feldkonstante und 1983 auch die Lichtgeschwindigkeit auf einen exakten Wert festgelegt wurden, ist auch der Wert der elektrischen Feldkonstante exakt festgelegt. Dieser Wert ist:

Die Einheit von formula_4 kann auf verschiedene Weisen durch die abgeleiteten SI-Einheiten V = Volt, C = Coulomb und F = Farad ausgedrückt werden:


</doc>
<doc id="1450" url="https://de.wikipedia.org/wiki?curid=1450" title="Elektrolyse">
Elektrolyse

Elektrolyse nennt man einen Prozess, bei dem elektrischer Strom eine Redoxreaktion erzwingt. Sie wird beispielsweise zur Gewinnung von Metallen verwendet, oder zur Herstellung von Stoffen, deren Gewinnung durch rein chemische Prozesse teurer oder kaum möglich wäre. Beispiele wichtiger Elektrolysen sind die Gewinnung von Wasserstoff, Aluminium, Chlor und Natronlauge.

Eine Elektrolyse erfordert eine Gleichspannungsquelle, welche die elektrische Energie liefert und die chemischen Umsetzungen vorantreibt. Ein Teil der elektrischen Energie wird in chemische Energie umgewandelt. Genau dem umgekehrten Zweck, der Umwandlung von chemischer Energie in elektrische, dienen Batterien, Akkumulatoren oder Brennstoffzellen: sie dienen als Spannungsquelle. Wenn man einen Akkumulator lädt, läuft eine Elektrolyse ab, die die chemischen Vorgänge während der Entladung rückgängig macht. Elektrolysen können daher der Energiespeicherung dienen, beispielsweise bei der Elektrolyse von Wasser, die Wasserstoff und Sauerstoff ergibt, die als Energieträger einer Wasserstoffwirtschaft vorgeschlagen wurden. Durch die Umkehrung der Wasserelektrolyse in einer Brennstoffzelle kann etwa 40 % der ursprünglich eingesetzten elektrischen Energie wieder zurückgewonnen werden.

Die Abscheidung von Metallen aus einer Lösung, die die entsprechenden Metallionen enthält, durch einen von außen aufgeprägten Strom ist ebenfalls eine Elektrolyse. Dies kann zur Erzeugung von Metallschichten dienen, beispielsweise beim Verchromen; diese Art der Elektrolysen sind Gegenstand der Galvanotechnik. Die elektrolytische Auflösung und Wiederabscheidung von Metallen dient der Reinigung, z. B. von Kupfer, und wird elektrolytische Raffination genannt.

Bei den chemischen Reaktionen, die bei der Elektrolyse ablaufen, werden Elektronen übertragen. Es sind daher immer Redoxreaktionen, wobei die Oxidation an der Anode (elektrischer Pol), die Reduktion an der Kathode ablaufen; Oxidations- und Reduktionsprozesse sind also räumlich zumindest teilweise voneinander getrennt.

Die Elektrolyse wurde im Jahr 1800 entdeckt, wobei die von Alessandro Volta erfundene erste brauchbare Batterie verwendet wurde, die Voltasche Säule. Die neu entdeckte Elektrolyse ermöglichte es Humphry Davy, in den Jahren 1807 und 1808 mehrere unedle Metalle erstmals elementar herzustellen, beispielsweise Natrium und Calcium. Michael Faraday untersuchte die Elektrolyse genauer und entdeckte ihre Grundgesetze, nämlich die Abhängigkeit der umgesetzten Massen von der Ladungsmenge und der Molmasse. Auf seine Anregung hin wurden auch die Begriffe Elektrolyse, Elektrode, Elektrolyt, Anode, Kathode, Anion und Kation geschaffen. Nach der Erfindung leistungsfähiger elektrischer Generatoren führten Elektrolysen Ende des 19. Jahrhunderts zu einer stürmischen Entwicklung in Wissenschaft und Technik, z. B. bei der elektrolytischen Gewinnung von Aluminium, Chlor und Alkalien, und bei der Erklärung des Verhaltens der Elektrolyte, zu denen auch Säuren und Basen zählen.

Durch zwei Elektroden wird ein elektrischer Gleichstrom in eine leitfähige Flüssigkeit (siehe Elektrolyt) geleitet. An den Elektroden entstehen durch die Elektrolyse Reaktionsprodukte aus den im Elektrolyt enthaltenen Stoffen.

Die Spannungsquelle bewirkt einen Elektronenmangel in der mit dem Pluspol verbundenen Elektrode (Anode) und einen Elektronenüberschuss in der anderen, mit dem Minuspol verbundenen Elektrode (Kathode). Die Lösung zwischen der Kathode und Anode enthält als
Elektrolyte positiv und negativ geladene Ionen. Positiv geladene Ionen (Kationen) oder elektroneutrale Stoffe nehmen an der Kathode Elektronen auf und werden dadurch reduziert. An der Anode läuft der entgegengesetzte Prozess ab, die Abgabe von Elektronen in die Elektrode, wobei Stoffe, z. B. Anionen, oxidiert werden. Die Menge der an der Anode übertragenen Elektronen ist gleich der an der Kathode übertragenen.

Der Transport der Stoffe an die Elektroden erfolgt durch konvektiven Stoffübergang (Diffusion innerhalb der Flüssigkeit mit überlagerter Strömung der Flüssigkeit) und, soweit es Ionen betrifft, zusätzlich durch Migration (Wanderung durch Einwirkung des elektrischen Feldes zwischen den Elektroden).

Die Spannung, die zur Elektrolyse mindestens angelegt werden muss, wird als Zersetzungsspannung (U oder E) bezeichnet. Diese oder eine höhere Spannung muss angelegt werden, damit die Elektrolyse überhaupt abläuft. Wird diese Mindestspannung nicht erreicht, wirkt der Elektrolyt beziehungsweise seine Grenzflächen zu den Elektroden, die auch elektrochemische Doppelschicht genannt werden, isolierend.

Für jeden Stoff, für jede Umwandlung von Ionen zu zwei- oder mehratomigen Molekülen kann die Zersetzungsspannung, das Abscheidepotential anhand des Redoxpotentials ermittelt werden. Aus dem Redoxpotential erhält man noch weitere Hinweise, wie zur elektrolytischen Zersetzung von Metallelektroden in Säure oder zur Verminderung von Zersetzungsspannung durch Abänderung des pH-Wertes. So lässt sich durch das Redoxpotential berechnen, dass die anodische Sauerstoffbildung bei der Wasserelektrolyse von Wasser in basischer Lösung (Zersetzungsspannung: 0,401 V) unter geringerer Spannung abläuft als in saurer (Zersetzungsspannung: 1,23 V) oder neutraler (Zersetzungsspannung: 0,815 V) Lösung, an der Kathode hingegen bildet sich Wasserstoff leichter unter sauren Bedingungen als unter neutralen oder basischen Bedingungen.

Sind in einer Elektrolytlösung mehrere reduzierbare Kationen vorhanden, so werden zunächst die Kationen reduziert, die in der Redoxreihe (Spannungsreihe) ein positiveres (schwächer negatives) Potential haben. Bei der Elektrolyse einer wässrigen Kochsalzlösung bildet sich an der Kathode normalerweise Wasserstoff und nicht Natrium.
Auch beim Vorliegen von mehreren Anionenarten, die oxidiert werden können, kommen zunächst diejenigen zum Zuge, die in der Redoxreihe möglichst nahe am Spannungsnullpunkt liegen, also ein schwächeres positives Redoxpotential besitzen.

Nach Überschreiten der Zersetzungsspannung wächst mit Spannungszunahme proportional auch die Stromstärke. Nach Faraday ist die Gewichtsmenge eines elektrolytisch gebildeten Stoffs proportional zu der geflossenen elektrischen Ladung (Stromstärke multipliziert mit der Zeit, siehe Faradaysche Gesetze).
Für die Bildung von 1 g Wasserstoff (etwa 11,2 Liter, bei der Bildung eines Wasserstoffmoleküls werden zwei Elektronen benötigt) aus wässriger Lösung wird eine elektrische Ladung von 96485 C (1 C = 1 A·s) benötigt. Bei einem Strom von 1 A dauert die Bildung von 11,2 Litern Wasserstoff also 26 Stunden und 48 Minuten.

Neben dem Redoxpotential ist noch die Überspannung (das Überpotential) von Bedeutung. Auf Grund von kinetischen Hemmungen an Elektroden benötigt man häufig eine deutlich höhere Spannung als sich dies aus der Berechnung der Redoxpotentiale errechnet. Die Überspannungseffekte können – je nach Materialbeschaffenheit der Elektroden – auch die Redoxreihe ändern, so dass andere Ionen oxidiert oder reduziert werden als dies nach dem Redoxpotential zu erwarten gewesen wäre.

Kurz nach Abschaltung einer Elektrolyse kann man mit einem Amperemeter einen Stromausschlag in die andere Richtung feststellen. In dieser kurzen Phase setzt der umgekehrte Prozess der Elektrolyse, die Bildung einer galvanischen Zelle ein. Hierbei wird nicht Strom für die Umsetzung verbraucht, sondern es wird kurzzeitig Strom erzeugt; dieses Prinzip wird bei Brennstoffzellen genutzt.

Mitunter ist es ratsam, zur Vermeidung unerwünschter chemischer Reaktionen Kathodenraum und Anodenraum voneinander zu trennen und den Ladungsaustausch zwischen Anoden- und Kathodenraum nur durch ein poröses Diaphragma - häufig ein Ionenaustauscherharz - stattfinden zu lassen. Bei der technischen Elektrolyse zur Herstellung von Natronlauge ist dies recht wichtig. 
Zur Verfolgung von Stoffumsatz, Wanderungsgeschwindigkeiten von Ionen kann auch das Wissen von molaren Grenzleitfähigkeiten wichtig sein.

Wenn man durch eine Elektrolyse eine Trennung einzelner Moleküle oder Bindungen erzwingt, wirkt gleichzeitig ein galvanisches Element, dessen Spannung der Elektrolyse entgegenwirkt. Diese Spannung wird auch als Polarisationsspannung bezeichnet.

Es gibt nur wenige Anodenmaterialien, die während der Elektrolyse inert bleiben, also nicht in Lösung gehen, z. B. Platin und Kohlenstoff. Einige Metalle lösen sich trotz stark negativem Redoxpotential nicht auf, diese Eigenschaft wird als „Passivität“ bezeichnet. In saurer Lösung müssten sich nach der Nernst'schen Gleichung die Mehrzahl der Metalle unter Kationen- und Wasserstoffbildung auflösen. Bis auf Kupfer, Silber, Gold, Platin, Palladium besitzen fast alle Metall/Metallkationenpaare ein negatives Redoxpotential und wären für Elektrolysen in saurem Milieu ungeeignet, da sich das Gleichgewicht (Metallatom und Protonen) zur Kationenbildung und Wasserstoff verschiebt. Im schwefelsauren Milieu ist Blei ein preiswertes und beliebtes Kathodenmaterial, als Anode kann sowohl Blei als auch Bleioxid verwendet werden (Verwendung auch in Autobatterien). Bleisulfat ist schlecht löslich, so dass die Bleielektroden sich kaum auflösen.

Eisen und Nickel können wegen der Passivität als Anoden manchmal auch in saurem Milieu verwendet werden, jedoch werden auch diese Anodenmaterialien vorzugsweise im basischen Milieu verwendet. Eine Eisenanode, die mit konzentrierter Salpetersäure behandelt wurde, löst sich nicht auf, durch die Passivierung gehen keine Eisen(II)- oder Eisen(III)-ionen in Lösung. Es hat sich eine sehr dünne und stabile Eisenoxidschicht (ähnlich wie beim Aluminium) gebildet, die die weitere Auflösung der Elektrode verhindert.
Chloridionen oder höhere Temperaturen können jedoch die Passivität aufheben.

Eisenanoden weisen im Vergleich zu anderen Anodenmaterialien nur eine sehr geringe Überspannung bei der Sauerstoffentwicklung auf, daher werden sie vorzugsweise bei der Erzeugung von Sauerstoff eingesetzt.

Hemmungserscheinungen an der Anode, die bei der Sauerstoffbildung zu einer Überspannung führen, beobachtet man bei Kohle- und Platinanoden. Die Überspannung kann genutzt werden, um bei der Elektrolyse von wässriger Kochsalzlösung Chlor statt Sauerstoff zu erzeugen.

An Zink-, Blei- und besonders Quecksilberkathoden zeigen Protonen eine erhebliche Überspannung und die Bildung von Wasserstoff erfolgt erst bei einer viel höheren Spannung. Die erhebliche Überspannung von Wasserstoff an der Quecksilberkathode, an der Natrium als Amalgam gebunden wird und so dem Gleichgewicht entzogen wird, nutzt man zur technischen Herstellung von Natronlauge. Durch die erhebliche Überspannung an dieser Elektrode bei der Wasserstoffbildung ändert sich die Redoxreihe, statt Protonen werden nun Natriumionen an der Quecksilberkathode reduziert.

Geeignete Elektrodenmaterialien:
(++) Gut geeignet,
(+) geeignet,
(−) nicht geeignet

Sowohl an der Kathode als auch an der Anode können Überspannungen auftreten und somit die benötigte Spannung gegenüber den Berechnungen nach der Nernst-Gleichung erhöhen. Die Überspannungen sind bei Gasbildungen (z. B. Wasserstoff- und Sauerstoffbildung) mitunter beträchtlich. Die aufgebrachte Überspannungsenergie geht als Wärme verloren, trägt also nicht zum Stoffumsatz bei. Je nach Metallart und Oberflächenbeschaffenheit der Elektroden variieren die Überspannungen. Stromstärke und Temperatur beeinflussen ebenfalls die Überspannung. Eine wachsende Stromstärke erhöht leicht die Überspannung, eine Temperaturerhöhung senkt dagegen die Überspannung.

Die nachfolgenden Tabellen geben einen kurzen Überblick bezüglich der
Überspannung bei der anodischen Sauerstoffentwicklung und der kathodischen Wasserstoffentwicklung (die Versuche wurden jedoch bei verschiedenen pH-Werten ausgeführt, zur Berechnung von pH-Änderungen siehe Nernst-Gleichung)

Überspannung Sauerstoffbildung

Konditionen: 1 N-wäss. KOH, 20 °C, Messung nach 20 min.

Überspannung Wasserstoffbildung

Konditionen: 1 N wäss. HCl, 16 °C.

Bei anderen elektrolytischen Reduktionen (ohne Gasbildung) kann auch die Diffusionsüberspannung wichtig werden. Falls nach einigen Minuten die Konzentration des elektrolytisch umzusetzenden Stoffes vor der Elektrode absinkt, muss mehr Spannung aufgebracht werden, um die gleiche Stromstärke zu erzielen. Durch kontinuierliches Rühren oder mit rotierenden Scheiben-, Zylinderelektroden kann die Diffusionsüberspannung gesenkt werden.

Die Wasserstoff- und die Sauerstoffüberspannung bleiben an vielen Metallen nicht konstant.
Sie steigen mitunter sogar noch nach 60 Minuten leicht an.

Der elektrische Widerstand einer Elektrolysezelle behindert den Stromfluss (ohmsches Gesetz) und sollte daher minimiert werden, andernfalls geht Energie in Form von Wärme verloren. Der Widerstand einer Elektrolysezelle hängt vom Elektrodenabstand, von der Größe der Elektrodenfläche und von der Leitfähigkeit ab.

Allgemein gilt für die Berechnung des Widerstands einer Elektrolysezelle:

In destilliertem Wasser ist die Leitfähigkeit sehr gering – der Widerstand also sehr hoch – und eine Elektrolyse schlecht möglich.

Die Leitfähigkeiten von Lösungen geringer Konzentrationen lassen sich über die spezifische Elektrolytische Leitfähigkeit bzw. die Äquivalentleitfähigkeiten der Ionen berechnen. Die Leitfähigkeit von Lösungen sehr hoher Konzentration muss experimentell bestimmt werden. 
Obwohl bei starken Säuren die Leitfähigkeit höher als in basischen Lösungen gleicher Konzentration ist, werden viele Elektrolysen – aufgrund der anodischen Auflösungsvorgänge bzw. der verzögerten Sauerstoffbildung bzw. Halogenoxidation im sauren Bereich – vorwiegend in basischem Medium ausgeführt.

Um die Wirtschaftlichkeit von elektrolytischen Verfahren zu steigern, sollten die Verfahren bei möglichst hohen Stromdichten durchgeführt werden. Dies erreicht man, indem man die Leitfähigkeit durch Salzzugabe oder durch Temperaturerhöhung (je Grad Temperaturzunahme steigt die spezifische Leitfähigkeit etwa um 1-2 %) erhöht. Häufig wird die Stromdichte durch den Diffusionsgrenzstrom limitiert. Aus Kenntnis des Diffusionsgrenzstromes lassen sich dimensionslose Kennzahlen ermitteln, um den Umsatz auch für größere Anlagen berechnen zu können. Es gibt für jede Elektrolyse eine kalkulatorisch optimale Stromdichte, sie ist größtenteils nicht die maximale Stromdichte.

Um möglichst saubere, kompakte Metallabscheidungen zu erhalten, sollte bei geringer Stromdichte gearbeitet werden. Dies ist insbesondere für Gold-, Silber- und Kupferbezüge wichtig.
Metallabscheidungen bei hohen Stromdichten bilden sogenannte Spieße, Stangen, Bäume aus und diese können zu Kurzschlüssen führen.

Häufig – besonders in der organischen Chemie – sind thermische Verfahren aufgrund des höheren Stoffumsatzes pro Zeiteinheit den elektrolytischen Verfahren überlegen.

Während der Elektrolyse können Kationen an der Kathode reduziert und an der Anode Anionen oxidiert werden. Da dicht vor der Elektrode Ladungsänderungen durch Reduktion oder Oxidation auftreten, muss die Ladungsdifferenz im Elektrodenraum durch Wanderungsprozesse ausgeglichen werden.
Kationen und Anionen müssen im Elektrodenraum in identischer Konzentration vorliegen, es darf keinen Überschuss an positiven oder negativen Ionen geben. Der Ausgleich von Ionen in einer Elektrolysezelle wird durch die Ionenwanderung bewirkt. Die Wanderungsgeschwindigkeit ist abhängig von der angelegten Zellspannung und der Art der Ionen. Der Verlust an Kationen vor der Kathode kann durch die Wanderung von überschüssigen Kationen aus dem Anodenraum oder umgekehrt von überschüssigen Anionen aus dem Kathodenraum kompensiert werden. In der Regel stellt sich ein Kompromiss aus beiden Wanderungsrichtungen ein. Die Wanderungsgeschwindigkeiten lassen sich aus den Grenzleitfähigkeiten der Ionenarten berechnen.
Mit der Überführungszahl kann die Änderung der Ionenzusammensetzung direkt bestimmt werden.

Es gibt Ionen wie H+ oder OH-, die sehr schnell in einer Elektrolytlösung wandern. Aufgrund der unterschiedlichen Wanderungsgeschwindigkeiten können sich Ionenarten während der Elektrolyse in den Halbzellen der Elektrolysezelle anreichern.

Bei einer Temperaturerhöhung um 1 °C nimmt die Leitfähigkeit um ca. 1–2,5 % zu. Die Zunahme der Wanderungsgeschwindigkeit könnte mit einer geringeren Viskosität der Solvathülle um die Ionen oder gar mit einer Abnahme der Solvathülle um die Ionen begründet werden.

Die Elektrolyse von Wasser zerlegt dieses in die Elemente Sauerstoff und Wasserstoff. Wie alle Elektrolysen besteht sie aus zwei Teilreaktionen, die an den beiden Elektroden (Kathoden- und Anodenräumen) ablaufen. Das Gesamt-Reaktionsschema dieser Redoxreaktion lautet:

Die Elektroden tauchen in Wasser ein, welches durch die Zugabe von Säure oder Lauge besser leitend gemacht wird. Die Teilreaktionen lauten

"Kathodenraum":
2 HO + 2 e → H + 2 HO (für saure Lösungen)
oder :
2 HO + 2 e → H + 2 OH (für basische Lösungen)

"Anodenraum:"
6 HO → O + 4 HO + 4 e (für saure Lösungen)
oder :
4 OH → O + 2 HO + 4 e (für basische Lösungen)

Als Demonstrationsexperiment kann diese Reaktion im Hofmannschen Wasserzersetzungsapparat ausgeführt werden.

Die Wasserelektrolyse kann zur Gewinnung von Wasserstoff als lagerbarer Energieträger an Bedeutung gewinnen. Der energetische Wirkungsgrad der Elektrolyse von Wasser liegt bei über 70 %.

Die Elektrolyse von Zinkiodid zerlegt dieses in die Elemente Zink und Iod. Wie alle Elektrolysen besteht auch diese aus zwei Teilreaktionen, die an den beiden Elektroden (Kathoden- und Anodenraum) ablaufen. Das Gesamt-Reaktionsschema dieser Redoxreaktion lautet:

Die Reaktionen an den einzelnen Elektrodenräumen lauten :

"Kathodenreaktion":
Zn + 2 e → Zn

"Anodenreaktion:"
2 I → I+ 2 e

Durch die Energiezufuhr bewegen sich die einzelnen Ionen in Richtung Elektroden. Die Zink-Kationen wandern zur Kathode, es werden von den Zink-Kationen zwei Elektronen aufgenommen (Reduktion) und es bildet sich elementares Zink. Die Iod-Anionen wandern zur Anode und werden zu elementarem Iod oxidiert.

Die Metalle Aluminium und Magnesium werden elektrolytisch mithilfe der Schmelzflusselektrolyse hergestellt. Elektrochemisch werden ferner Kupfer, Silber und Gold gewonnen, sowie zu großen Teilen auch Zink und Nickel. Weitere Alkalimetalle und die meisten Erdalkalimetalle werden ebenfalls durch Schmelzflusselektrolyse gewonnen.

Sowohl dabei als auch bei Elektrolyse in wässrigen Medien werden je nach Ausgangsstoff die Halogene Fluor, Brom und Chlor frei, die in großem Maßstab für weitere Synthesen verwendet werden.

In der Chloralkali-Elektrolyse wird aus Steinsalz Chlor, Wasserstoff und Natronlauge hergestellt.

Elektrolytische Metallabscheidungen gehören zu den wichtigsten Anwendungen, entweder zur Erzeugung von metallischen Überzügen bei der Galvanik (galvanisches Verzinken, Verchromen usw.) oder zur Herstellung und Verstärkung von Leiterbahnen in der Leiterplattenproduktion.

Die "elektrolytische Raffination" ist ein Verfahren zur Reinigung von Metallen. Die Reinigung wird dadurch erreicht, dass sich durch Elektrolyse eine Anode aus einem Rohmetall löst und sich an einer Kathode selektiv als reines Metall abscheidet. Verunreinigungen bleiben im Elektrolyt gelöst oder fallen als Anodenschlamm aus. Der Anodenschlamm und Elektrolyt werden wegen ihrer wertvollen Bestandteile aufgearbeitet. Elektrolytische Raffination wird insbesondere für die Reinigung von Kupfer, Nickel, Silber und Blei verwendet.

Bei der elektrolytische Raffination von Kupfer wird Elektrolytkupfer mit einer Reinheit von >99,5 % gewonnen und wird hauptsächlich für elektrische Leiter verwendet. Bei der Kupferraffination beträgt die Zellspannung wenige Zehntel Volt (hauptsächlich verursacht durch Überspannungen und den Zellwiderstand), die Stromdichte liegt im Bereich von 150 bis 240 A/m. Der Anodenschlamm enthält insbesondere die Edelmetalle Gold und Silber, aber auch Selen und Antimon. Die unedleren Metalle, wie Eisen, Nickel, Cobalt und Zink, verbleiben im Elektrolyt.

Bei der elektrolytische Bleiraffination dient die Raffination von Rohblei zur Abtrennung von Arsen, Antimon und Bismut.

Die Kolbe-Elektrolyse ist das älteste Beispiel einer organischen elektrochemischen Reaktion. Bei dieser Elektrolyse werden zwei Carbonsäuremoleküle unter CO-Abspaltung gekuppelt.






Nach Angaben des Statistischen Bundesamtes wurden im Jahr 2007 die folgenden Mengen an Metallen oder Chemikalien in Deutschland hergestellt.

In den USA liegen die hergestellten Elektrolyseprodukte um den Faktor 2–3 höher. Dort werden ca. 5 % der gesamten Stromproduktion für die Elektrolyse benötigt. Die weltweite Gesamtmenge an mittels Elektrolyse hergestelltem Aluminium liegt bei etwa 50 Millionen Tonnen jährlich.




</doc>
<doc id="1451" url="https://de.wikipedia.org/wiki?curid=1451" title="Elektrische Kapazität">
Elektrische Kapazität

Die elektrische Kapazität (Formelzeichen formula_1, von ; Adjektiv kapazitiv) ist eine physikalische Größe aus dem Bereich der Elektrostatik, Elektronik und Elektrotechnik.

Die elektrische Kapazität zwischen zwei voneinander isolierten elektrisch leitenden Körpern ist gleich dem Verhältnis der Ladungsmenge formula_2, die auf diesen Leitern gespeichert ist (formula_3 auf dem einen und formula_4 auf dem anderen), und der zwischen ihnen herrschenden elektrischen Spannung formula_5:

Sie wird dabei festgelegt durch die Dielektrizitätskonstante des isolierenden Mediums sowie die Geometrie der Körper, dazu zählt auch der Abstand.

Bei Akkumulatoren sowie Batterien benutzt man den Begriff „Kapazität“ für die maximale Ladungsmenge formula_2, welche in ihnen gespeichert werden kann. Sie wird in Amperestunden (Ah) angegeben. Diese Kapazität der elektrischen Ladung hat jedoch weder etwas mit der hier dargestellten elektrischen Kapazität (Farad) noch mit der Leistungskapazität (Watt) zu tun.

Eine technische Anwendung findet die Kapazität in Form von elektrischen Kondensatoren, welche durch die Angabe einer bestimmten Kapazität charakterisiert werden. Der Begriff „Kapazität“ wird umgangssprachlich auch synonym für das elektrische Bauelement Kondensator selbst () verwendet.

Kondensatoren stellen eine Leiteranordnung mit zwei Elektroden zur getrennten Speicherung von elektrischer Ladung formula_3 und formula_4 dar. In physikalischer Sicht rührt der elektrische Fluss formula_10 von den getrennten elektrischen Ladungen formula_3 und formula_4, die von der externen Spannungsquelle mit der Spannung formula_5 auf die Elektroden transportiert werden, womit sich:

ergibt. Formal erfolgt dieser Zusammenhang über das Gaußsche Gesetz. Die elektrische Kapazität eines Kondensators kann dann als das Verhältnis der Ladungsmenge formula_2 zur angelegten Spannung formula_5 ausgedrückt werden:

Dabei ist formula_1 üblicherweise eine konstante Kenngröße, die sich wie folgt ergibt.

Ein Körper, auf den eine positive elektrische Ladung gegeben wird, hat dadurch ein elektrisches Feld, das der Bewegung einer weiteren positiven elektrischen Ladung auf den Körper entgegenwirkt. Befindet sich nun aber ein Körper in der Nähe, der negativ geladen ist, so wird das abstoßende elektrische Feld des positiven Körpers geschwächt (die auf den Körper zu bewegende positive Ladung spürt auch die Kraft der anziehenden negativen Ladung). Damit wird weniger Spannung benötigt um die weitere positive Ladung auf den bereits positiv geladenen Körper zu bewegen, als ohne den zweiten negativ geladenen Körper. Der erste Körper hat also eine höhere Kapazität. Das Gleiche gilt natürlich auch für den zweiten Körper. Die Abschwächung des elektrischen Feldes durch den einen geladenen Körper auf den anderen geladenen Körper wird beeinflusst durch deren Geometrie und die Permeabilität des isolierenden Mediums zwischen den beiden Körpern.

In einer vereinfachten Analogie entspricht die Kapazität dem Volumen eines Druckluftbehälters mit konstanter Temperatur. Der Luftdruck ist dabei analog zur Spannung formula_5 und die Luftmenge analog zur Ladungsmenge formula_2. Daher ist die Ladungsmenge im Kondensator proportional zur Spannung.

Diese Gesetzmäßigkeit gilt auch für die sogenannte Pseudokapazität, einer innerhalb enger Grenzen spannungsabhängigen elektrochemischen bzw. faradayschen Speicherung elektrischer Energie, die mit in einer Redoxreaktion und mit einem Ladungsaustausch an Elektroden von Superkondensatoren verbunden ist, wobei allerdings im Gegensatz zu Akkumulatoren an den Elektroden keine chemische Stoffänderung eintritt.

Unter anderem die Physikalisch-Technische Bundesanstalt (PTB) befasst sich mit Kapazitätsnormalen.

Die elektrische Kapazität wird in der abgeleiteten SI-Einheit Farad gemessen. Ein Farad (1 F) ist diejenige Kapazität, die beim Anlegen einer Spannung von 1 Volt eine Ladungsmenge von 1 Coulomb (1 C = 1 As) speichert:

Ein Kondensator der Kapazität 1 Farad lädt sich bei einem konstanten Ladestrom von 1 Ampere in 1 Sekunde auf die Spannung 1 Volt auf. Die SI-Einheit Farad, genannt zu Ehren des englischen Physikers und Chemikers Michael Faraday, hat sich heutzutage international überall durchgesetzt.

Bis Mitte des 20. Jahrhunderts wurde die Kapazität von Kondensatoren häufig mit der Kapazitätseinheit "cm" beschriftet. Diese Angabe in Zentimetern rührt daher, dass die Kapazität im heute praktisch kaum noch gebrauchten Gaußschen Einheitensystem in der Längendimension ausgedrückt wird. So weist eine Metallkugel mit 5 cm Radius gegenüber einer sich im Unendlichen befindlichen Gegenelektrode eine Kapazität von 5 cm auf.

Die nebenstehende Abbildung zeigt einen Papierkondensator der Marke "SATOR" der ehemaligen Firma "Kremenezky, Mayer & Co" von Johann Kremenezky aus dem Jahr 1950 mit einer Kapazität von „"5000 cm"“ bei einer Prüfspannung von „"2000 V"“. Dies wäre eine Kapazität von ca. 5,6 nF im heute üblichen SI-Einheitensystem. Das entspricht der Kapazität einer Metallkugel von 5000 cm Radius.

Eine Kapazität von 1 cm im Gaußschen Einheitensystem entspricht ca. 1,1 pF im SI-Einheitensystem, der Umrechnungsfaktor ist 4"π""ε". Diese Umrechnung kommt durch die Definition der Feldkonstante im Gaußschen Einheitensystem zustande:

Für die Kapazität einer Reihe von einfachen Leiteranordnungen gibt es analytische Lösungen oder konvergente Reihenentwicklungen. Die folgende Tabelle zeigt einige Beispiele:

Hierin bezeichnet ggf. "A" die Fläche der Elektroden, "d" deren Abstand, "l" deren Länge, formula_23 sowie formula_24 deren Radien. Es gilt formula_25, wobei formula_26 für die elektrische Feldkonstante des Vakuums und formula_27 für die dielektrische Leitfähigkeit des Dielektrikums steht. In der schematischen Darstellung sind die Leiter hellgrau bzw. dunkelgrau und das Dielektrikum blau gefärbt.

Allgemeine Gleichungen für die Bestimmung der Kapazität gelten für Strom, Spannung und Ladung an einer elektrischen Kapazität:

Ein Ausdruck für die Kapazität einer beliebigen Elektrodenanordnung oder Ladungsverteilung lässt sich mittels des Gaußschen Satzes herleiten:

Dabei beträgt die dielektrische Verschiebung formula_32, also:

Für ein Vakuum vereinfacht sich diese Gleichung wegen formula_34 zu:

Eine Berechnung der Kapazität erfordert die Kenntnis des elektrischen Feldes. Hierfür ist die Laplace-Gleichung formula_36 mit einem konstanten Potential formula_37 auf den Leiteroberflächen zu lösen. In komplizierteren Fällen existiert keine geschlossene Form der Lösung.

Das Messen der Kapazität dient nicht nur der Kontrolle der Kapazität eines Kondensators (Bauteil), sondern wird beispielsweise in kapazitiven Abstandssensor zur Abstandsbestimmung herangezogen. Auch weitere Sensoren (Druck, Feuchte, Gase) beruhen oft auf einer Kapazitätsmessung.

Entsprechend den oben genannten Zusammenhängen kann die Kapazität folgendermaßen bestimmt werden:
Insbesondere das letztgenannte Verfahren wird in Kapazitätsmessgeräten angewendet, wobei nicht nur die Größe des Stromes, sondern auch seine Phasenlage zur Spannung erfasst wird. Auf diese Weise kann auch die Impedanz und der Verlustwinkel bzw. der Gütefaktor des Kondensators bestimmt werden.


</doc>
<doc id="1452" url="https://de.wikipedia.org/wiki?curid=1452" title="Elektrischer Strom">
Elektrischer Strom

Der elektrische Strom, oft auch nur Strom, ist ein Phänomen der Elektrizitätslehre. In der alltäglichen Bedeutung des Begriffs ist damit der Transport von elektrischen Ladungsträgern gemeint, also beispielsweise von Elektronen in Leitern oder Halbleitern oder von Ionen in Elektrolyten. Diese Form des Stroms bezeichnet man auch genauer als Konvektionsstrom. Er macht sich gemäß dem ampereschen Gesetz durch ein magnetisches Feld bemerkbar und führt meist zu einer Erwärmung des Leiters. In einem elektrischen Stromkreis fließt ein Strom, sobald zwischen den Anschlüssen der Quelle eine elektrische Spannung herrscht und eine leitende Verbindung besteht. 

Darüber hinaus zählt man zum elektrischen Strom den so genannten Verschiebungsstrom. Dieser entsteht nicht durch die Bewegung elektrischer Ladungen, sondern durch die Änderungen des Flusses eines elektrischen Feldes. Er tritt z. B. zwischen den Platten eines Kondensators beim Be- oder Entladen auf und erzeugt genau wie der Konvektionsstrom ein Magnetfeld. 

Die physikalische Größe für die Intensität des elektrischen Stroms ist die elektrische Stromstärke. Ihr Formelzeichen ist formula_1 und ihre gesetzliche Einheit das Ampere.

Bereits Thales von Milet soll im 6. Jahrhundert v. Chr. entdeckt haben, dass Bernstein leichte Körper anzieht, wenn er vorher mit Tüchern gerieben wird. Eine Erklärung dafür konnte er zwar nicht finden, das Wort Elektrizität (vom griechischen „elektron“ für „Bernstein“) weist aber immer noch auf diese antike Entdeckung zurück.

Die technische Nutzung des elektrischen Stromes begann in der Mitte des 19. Jahrhunderts mit der Telegrafie und der Galvanik. Für beide Anwendungen reichte zunächst die Leistung von Batterien aus. Um 1866 fand Werner von Siemens das dynamoelektrische Prinzip und nutzte es bei der Entwicklung des ersten elektrischen Generators, den er als Zündmaschine für die Zündung von Sprengladungen vermarkten konnte. Ab 1880 entwickelten sich diese Generatoren immer mehr zu Großmaschinen, um den Strombedarf der immer größer werdenden Stromnetze befriedigen zu können. In erster Linie dienten diese Netze zur Bereitstellung von elektrischem Strom für die Beleuchtung mit Bogen- und Glühlampen in der Öffentlichkeit und den ersten Privathaushalten. Eine weitere Anwendung des elektrischen Stromes bestand in seinem Einsatz in Leuchttürmen, da die Bogenlampe eine wesentlich höhere Lichtstärke besitzt als die zuvor verwendeten Kerzen oder Petroleumlampen. Infolgedessen entstanden die ersten Kraftwerke, die zunächst noch mit einfachen Wasserturbinen und Dampfmaschinen angetrieben wurden. Seit Beginn des 20. Jahrhunderts stehen leistungsfähige Dampfturbinen zur Verfügung, die bis in die Gegenwart als Kraftmaschinen bei der Stromerzeugung dominieren.

In den letzten Jahren des 19. Jahrhunderts fiel nach dem sogenannten Stromkrieg die Entscheidung zwischen Gleichstrom- und Wechselstromsystem zugunsten des Wechselstroms.

Für quantitative Angaben zum elektrischen Strom verwendet man die physikalische Größe "Stromstärke".

Elektrischer Strom kann unterschiedliche Ursachen haben:

Wenn – beispielsweise zwischen den Polen einer Batterie – eine Potentialdifferenz besteht, spricht man von einer elektrischen Spannung. Aufgrund des dann bestehenden elektrischen Feldes wird eine Kraft auf die Ladungsträger ausgeübt; sie erfahren dadurch eine Beschleunigung, wenn sie beweglich sind. Das geschieht beispielsweise, wenn eine Glühlampe über Metalldrähte an die Pole angeschlossen ist. Die Driftgeschwindigkeit der Ladungsträger bei dieser gerichteten Bewegung entsteht im Wechselspiel mit Streuprozessen. Die Stromdichte lässt sich berechnen durch Multiplikation der Driftgeschwindigkeit mit der Raumladungsdichte.
Der "Driftstrom" wächst trotz der Beschleunigung nicht beliebig an; bei einer gegebenen Spannung formula_2 stellt sich eine begrenzte Stromstärke formula_1 ein. Diese Beobachtung erklärt man mit einem elektrischen Widerstand formula_4. Definiert wird er durch das Verhältnis
In vielen Leitermaterialien ist die Stromstärke bei konstanter Temperatur "proportional" zur Spannung. In diesem Fall wird der Zusammenhang als ohmsches Gesetz bezeichnet, bei dem der Proportionalitätsfaktor formula_4 von der Spannung und Stromstärke unabhängig ist.

In einem Stromkreis mit einer Spannungsquelle bestimmen deren feststehende elektrische Spannung und der Widerstand die konkrete Stromstärke. Hingegen baut bei Verwendung einer Stromquelle deren feststehende Stromstärke am Widerstand die konkrete Spannung auf. In der Praxis kommen allerdings Spannungsquellen viel häufiger als Stromquellen vor, wie beispielsweise in Stromversorgungen, weshalb sich der konkrete Wert der elektrischen Stromstärke nach dem Verbraucher (genauer: dessen Widerstand) richtet.

In Metallen sind ein Teil der Elektronen, die sogenannten Leitungselektronen, nicht jeweils an ein bestimmtes Atom gebunden, sondern ‘gehören’ allen Atomen gemeinsam, siehe metallische Bindung. Nach dem Drude-Modell ist die Leitfähigkeit von Metallen proportional zur Zahl der Leitungselektronen und ihrer Beweglichkeit. Realistischer ist das Bändermodell.

Der Stromtransport ist bei einem Ionenleiter an einen stofflichen Transport von beweglichen, elektrisch positiv oder negativ geladenen Atomen oder Molekülen (also Ionen) gebunden. Das unterscheidet diese Leiter von Leitern 1. Klasse wie den Metallen, in denen die Elektronen den elektrischen Strom tragen. Als Ionenleiter kommen vor allem ionisierte Gase und elektrisch leitfähige Flüssigkeiten in Frage. Man nennt diese Ionenleiter Elektrolyte oder Plasma. Auch Festkörper können Ionenleiter sein, siehe Festelektrolyt. 

Bei Ionenleitern kommt es bei "Gleichstrom" im Gegensatz zu Metallen im Regelfall zu einer stofflichen Veränderung des elektrischen Leiters. Dieser Effekt wird bei der Elektrolyse ausgenutzt. Solche chemischen Vorgänge können die Beschaffenheit des Leiters so verändern, dass sich die elektrolytische Leitfähigkeit allmählich ändert. Ist ein solcher Materialtransport (beispielsweise bei einer Gasentladung) unerwünscht, kann er durch Wechselstrom weitgehend unterbunden werden.

Als Gleichstrom (, abgekürzt "DC") wird jener elektrische Strom bezeichnet, der über die Zeit seine Richtung und Stärke nicht ändert, also zeitlich konstant ist.

Praktisch alle elektronischen Geräte im Haushalt wie Radio- und Fernsehempfänger, Computer oder auch die Steuerungen heutiger Waschmaschinen benötigen für ihre Stromversorgung Gleichstrom. Aber auch in der Energietechnik werden Gleichströme eingesetzt, beispielsweise in der Schmelzflusselektrolyse zur Aluminiumgewinnung, für gut drehzahlregelbare Gleichstrommotoren (inzwischen zunehmend durch Stromrichter und Asynchronmotoren ersetzt), als Zwischenkreis in Stromrichtern, in Sendeanlagen und in Kraftfahrzeug-Bordnetzen.

Gleichstrom kann durch Gleichrichter aus Wechselstrom gewonnen werden. Diese werden daher überall dort eingesetzt, wo Gleichstrom benötigt wird, aber nur der Wechselstrom des öffentlichen Stromnetzes zur Verfügung steht. Seltener, weil erheblich teurer, verwendet man auch direkte Gleichstromquellen, wie z. B. galvanische Zellen und photovoltaische Zellen. Kuriose Sonderfälle ohne technische Bedeutung sind elektrische Maschinen, die direkt ohne Gleichrichter mittels der Unipolarinduktion Gleichstrom herstellen können.

Bei Wechselstrom (, abgekürzt "AC") kommt es zu einer periodischen Änderung der Stromrichtung. Jede Periode besteht aus aufeinanderfolgenden Zeitspannen mit positiven und negativen Augenblickswerten, die sich zu einer mittleren Stromstärke null ergänzen. Ausschlaggebend für den Erfolg des Wechselstroms zum Energietransport war, dass die Spannung mit Hilfe von Transformatoren sehr einfach geändert werden kann. Alle öffentlichen Stromversorgungsnetze werden mit Wechselspannung betrieben,– in Europa und vielen weiteren Ländern mit der Netzfrequenz 50 Hz, in anderen Teilen der Welt 60 Hz, siehe Länderübersicht Steckertypen, Netzspannungen und -frequenzen.

Eine besondere Form von Wechselstrom ist der Dreiphasenwechselstrom (umgangssprachlich Stark-, Dreh- oder Kraftstrom), wie er in öffentlichen Stromnetzen zur elektrischen Energieverteilung großer Leistungen Verwendung findet. Diese Stromart ermöglicht besonders einfach gebaute und robuste Elektromotoren.

Weitere Beispiele für Wechselstrom

Eine Kombination aus Wechselstrom und Gleichstrom wird Mischstrom genannt. Dabei kommt es nicht unbedingt zu einer Richtungsänderung des Mischstromes, sondern der zeitlich konstante Gleichstromanteil wird durch den zusätzlich aufgebrachten Wechselstrom in seiner Stärke periodisch geändert (pulsierender Gleichstrom). Dieser Mischstrom tritt beispielsweise bei Gleichrichtern auf und wird mit Glättungskondensatoren oder Glättungsdrosseln in Netzteilen geglättet. Der dabei übrigbleibende (meist unerwünschte) Wechselanteil wird als Restwelligkeit bezeichnet, die mit einer Brummspannung verkoppelt ist.

Weitere Beispiele für Mischstrom

Von einem eingeprägten Strom spricht man, wenn die Stromstärke in einem weiten Bereich unabhängig vom Wert des Lastwiderstands ist. Dabei kann es sich um Gleichstrom oder um Wechselstrom beliebiger Frequenz und Kurvenform handeln. 

Sogenannte Labornetzteile verfügen sowohl über eine einstellbare Begrenzung der Ausgangsspannung als auch über eine einstellbare Begrenzung der Ausgangsstromstärke und weisen so eine Rechteckkennlinie auf. Welche der beiden Begrenzungen erreicht wird, hängt von der Größe der Belastung ab. Wenn beispielsweise die Begrenzungen auf 30 V und 1,0 A eingestellt sind, dann wird bei einem Lastwiderstand von über 30 Ω (bis zum Leerlauf) die Spannungsbegrenzung erreicht. Ändert sich der Widerstand innerhalb des angegebenen Bereichs, so ändert sich nur die Stromstärke entsprechend. Die davon unverändert bleibende Spannung bezeichnet man als eingeprägte Spannung. Bei einem Lastwiderstand von weniger als 30 Ω (bis zum Kurzschluss) wird die Strombegrenzung erreicht. Ändert sich der Widerstand innerhalb des angegebenen Bereichs, so ändert sich nur die Spannung, die sich dazu passend auf Werte unterhalb von 30 V einstellt, während der trotz Belastungsänderung unverändert fließende Strom einen "eingeprägten Strom" darstellt.

Elektrischer Strom dient in Alltag und Haushalt zur Energieversorgung zahlreicher elektronischer, elektrischer und elektromechanischer Geräte und Anlagen aller Größen, von beispielsweise Armbanduhren bis zu Fahrstühlen. Typischerweise wird er bei kleinen Geräten von einer ins Gerät eingelegten Batterie direkt geliefert, bei großen über das Stromnetz von einem Elektrizitätswerk. In den Industriestaaten ist das gesamte Leben von Bezug und Umformung dieser Energieform durchdrungen.

Der umgangssprachliche Ausdruck „Strom verbrauchen“ ist, ähnlich wie der Begriff „Energieverbrauch“, physikalisch gesehen nicht richtig. Denn
aufgrund der Ladungserhaltung fließt genau der Strom, der in ein Gerät hinein fließt, auch wieder hinaus - sofern im Gerät keine elektrischen Ladungen gespeichert werden.
Gemeint ist mit Stromverbrauch in aller Regel die von einem elektrischen Bauelement, Stromkreis oder Gerät umgesetzte elektrische Energie, oft auch pro Zeitspanne gerechnet, also die elektrische Leistung.

Obwohl die Stromstärke pro Fläche – also die elektrische Stromdichte – und deren Einwirkdauer für die Auswirkungen eines Stromunfalls verantwortlich ist, wird oft die Spannung als Gefahrenquelle angegeben, da sich mithilfe des ohmschen Gesetzes über den Körperwiderstand die Stromstärke bzw. Stromdichte im Körper berechnen lässt. Der Weg des elektrischen Stroms (also beispielsweise rechte Hand – Fuß) ist dabei maßgeblich für die Gefährlichkeit der Spannung, bei einem kürzeren Weg wie etwa Brust – Rücken können geringere Spannungen lebensgefährlich werden. Zusätzlich gibt die Höhe der Spannung Auskunft über den erforderlichen Mindestabstand zu blanken, nicht isolierten Hochspannungsleitungen. 

Elektrische Wechselströme im Bereich der Netzfrequenz sind ab 0,5 mA für den menschlichen Organismus spürbar und bei höheren Stromstärken über 10 mA, welche länger als 2 s einwirken, gefährlich, für Kinder möglicherweise bereits tödlich. Gleichströme sind ab 2 mA spürbar und ab 25 mA, welche länger als 2 s einwirken, gefährlich. Man spricht dann auch von einem Stromschlag. 

Diese und folgende Werte gelten jedoch nur, wenn sich der Strom über den Körperwiderstand im Körper verteilt und nicht z. B. auf den Herzmuskel konzentriert; bei Elektroden unter der Haut gelten sehr viel kleinere Werte. So genügen direkt für den Herzmuskel sehr geringe Stromstärken, um Herzkammerflimmern auszulösen. 

Die anschließende Tabelle gibt die Gefährlichkeit von Wechselstrom von 50–60 Hz wieder:

Bei elektrischen Energieversorgungsnetzen und vor allem Bereiche und Anlagen, die mit Hochspannung betrieben werden, wie etwa Umspannwerke, Freileitungen, aber auch Oberleitungen für Bahnen, kommen auch Stromunfälle durch Spannungsüberschläge und Lichtbögen vor. Der Stromunfall mit Lichtbogeneinwirkung ist fast ausnahmslos zusätzlich mit Verbrennungen verbunden und es entstehen in der Brandwunde meist toxische Verbrennungsprodukte.

Außerdem führen Hochspannungsunfälle (bei ausreichender Stromstärke) häufiger und rascher zu einem Herz- und Kreislaufstillstand.

Elektrostatische Entladungen, also auch Blitze, können Menschen verletzen oder töten. Bei diesen Entladungen treten kurzzeitig (etwa 10 ns bis 1 ms) sehr hohe Ströme von etwa 10 A bis über 100 kA auf. Es entstehen Muskelzuckungen, lokale oder großflächige Verbrennungen.

Jenseits der Zivilisation tritt elektrischer Strom unter anderen auf:





</doc>
<doc id="1453" url="https://de.wikipedia.org/wiki?curid=1453" title="Edelgard Bulmahn">
Edelgard Bulmahn

Edelgard Bulmahn (* 4. März 1951 in Petershagen) ist eine deutsche Politikerin (SPD). Sie war von 1998 bis 2005 Bundesministerin für Bildung und Forschung und von 2005 bis 2009 Vorsitzende des Ausschusses für Wirtschaft und Technologie des Deutschen Bundestages. Zu den von ihr als Bildungsministerin initiierten bzw. umgesetzten Reformen gehören der Ausbau der Ganztagsschule, die Bologna-Reform und die Exzellenzinitiative. In der 17. Wahlperiode war sie Mitglied des Auswärtigen Ausschusses des Deutschen Bundestages. Von 2013 bis 2017 war sie Vizepräsidentin des Deutschen Bundestages. Mit Ablauf der Legislaturperiode hat sie ihre parlamentarische Arbeit 2017 beendet.

Die Tochter eines Binnenschiffers und einer Friseurin wechselte nach acht Jahren Volksschule an das Aufbaugymnasium Petershagen. Nach dem Abitur 1972 verbrachte Edelgard Bulmahn zunächst ein Jahr im Kibbuz „Bror Chail“ in Israel. Danach begann sie ein Lehramtsstudium der Politikwissenschaft und der Anglistik in Hannover. 1978 bestand sie das erste und 1980 das zweite Staatsexamen für das Lehramt an Gymnasien. Seitdem war sie als Studienrätin an der Lutherschule Hannover tätig. Edelgard Bulmahn ist seit 1979 mit Joachim Wolschke-Bulmahn verheiratet. 

Seit 1969 ist Edelgard Bulmahn Mitglied der SPD. Von 1981 bis 1986 war sie Bezirksratsfrau im Stadtbezirk Linden-Limmer. Seit 1987 war sie Mitglied des Deutschen Bundestages, von 1987 bis 1989 stellvertretende Vorsitzende der Enquete-Kommission „Technikfolgen-Abschätzung und -Bewertung“. Seit 1991 war sie Mitglied im Vorstand der SPD-Bundestagsfraktion, von 1993 bis 2011 Mitglied im SPD-Parteivorstand und von 2001 bis 2011 Mitglied im Präsidium der SPD. Zudem fungierte sie als Vorsitzende des Wissenschaftsforums der Sozialdemokratie. Von 1998 bis 2003 war sie SPD-Landesvorsitzende in Niedersachsen. 

Edelgard Bulmahn ist stets als direkt gewählte Abgeordnete des Wahlkreises Stadt Hannover II in den Bundestag eingezogen. Bei der Bundestagswahl 2005 erreichte sie hier 54,3 % der Erststimmen. Bei der Bundestagswahl 2009 erreichte sie hier 39,8 % der Erststimmen, bei der Bundestagswahl 2013 42,8 % der Erststimmen. Bei der konstituierenden Sitzung des Bundestages am 22. Oktober 2013 wurde sie zu einer der Vizepräsidenten des Bundestages gewählt.

Von 1995 bis 1996 war sie Vorsitzende des Ausschusses für Bildung, Wissenschaft, Forschung, Technologie und Technikfolgenabschätzung und von 1996 bis 1998 Fraktions-Sprecherin für Bildung und Forschung. 2005 bis 2009 war sie Vorsitzende des Wirtschaftsausschusses des Deutschen Bundestages, zuletzt ordentliches Mitglied im Auswärtigen Ausschuss. Zur Bundestagswahl 2017 ist Bulmahn wie angekündigt nicht wieder angetreten.

Seit dem 27. Oktober 1998 war Edelgard Bulmahn in der von Bundeskanzler Gerhard Schröder geführten Bundesregierung Bundesministerin für Bildung und Forschung. Nach dem Kabinett Schröder I gehörte
sie auch dem Kabinett Schröder II an.

In ihrer Amtszeit brachte sie grundlegende Reformen der deutschen Bildungs- und Forschungslandschaft auf den Weg. Das von ihr initiierte Forum Bildung, in dem erstmals neben Vertreter(inne)n von Bund und Ländern auch Sozialpartner, Kirchen, Eltern, Schüler/-innen, Auszubildende, Studierende und Wissenschaftler mitwirkten, legte am 19. November 2001 nach zweijähriger Arbeit 12 Empfehlungen zur Neugestaltung des deutschen Bildungswesens vor. Diese zielten auf eine Verbesserung der schulischen Ausbildungsqualität, die Gewährleistung von Chancengleichheit, eine bessere individuelle Förderung und beeinflussten die bildungspolitische Diskussion der darauffolgenden Jahre. Die Empfehlung zum Ausbau der Ganztagsschule setzte Bulmahn gegen den heftigen Widerstand der unionsgeführten Länder mit dem mit vier Milliarden Euro ausgestatteten Investitionsprogramm Zukunft Bildung und Betreuung um. Das Aufstiegsfortbildungsförderungsgesetz wurde von Bulmahn mit der Novellierung vom 20. Dezember 2001 deutlich ausgebaut, was sich in rasch steigenden Teilnehmerzahlen niederschlug. Ihre darüber hinausgehenden Vorschläge zur Reform des deutschen Bildungswesens scheiterten jedoch am Widerstand in den Bundesländern. Nur im Hinblick auf den von Bulmahn vorgeschlagenen Nationalen Bildungsbericht gelang noch eine Verständigung. Bei der beruflichen Bildung kam es in Bulmahns Amtszeit zu einer Vielzahl von Neuerungen, speziell in den Informations- und Kommunikationstechnologien. Das Berufsbildungsgesetz (Deutschland) wurde zum 1. April 2005 erstmals nach 35 Jahren grundlegend novelliert. Ein Nationaler Pakt für Ausbildung und Fachkräftenachwuchs sollte ein ausreichendes Lehrstellenangebot sichern.

Auch das Bundesausbildungsförderungsgesetz (BAföG) wurde von Bulmahn reformiert. Die Freibeträge und Bedarfssätze wurden deutlich angehoben, die Darlehensschulden gedeckelt und die Beschränkungen für ein Auslandsstudium aufgehoben. Mit der Einführung eines vom Elterneinkommen unabhängigen Ausbildungsgeldes, das Kindergeld und familienbezogene Leistungen hätte zusammenfassen sollen, scheiterte sie allerdings am Einspruch von Bundeskanzler Schröder. Nur einen kurzfristigen Erfolg hatte sie mit ihrem Vorhaben, Studiengebühren gesetzlich auszuschließen und eine Studierendenvertretung gesetzlich im Hochschulrahmengesetz zu verankern, da das Bundesverfassungsgericht hierin einen unzulässigen Eingriff in die Gesetzgebungskompetenz der Länder sah. Nachhaltigen Einfluss auf die Entwicklung der deutschen Hochschullandschaft übte Bulmahn mit dem Ausbau der Frauenförderung und der Nachwuchsförderung (Graduiertenkolleg, Graduiertenschule, Emmy Noether-Programm), der Einführung der Juniorprofessur, der Reform der Professorenbesoldung (Besoldungsordnung W), der Bologna-Reform und der Exzellenzinitiative aus. Eine von ihr angestrebte bessere Breitenförderung der Hochschulen scheiterte an den Ländern, die den vorgeschlagenen Pakt für Hochschulen als Eingriff in die Kulturhoheit ablehnten und den Hochschulbau für sich reklamierten.

In der Forschungspolitik setzte Bulmahn Akzente mit dem Programm „Forschung für Nachhaltigkeit“, der erstmaligen Förderung der sozialökologischen Forschung, der Gründung der Deutschen Stiftung Friedensforschung, der Entwicklung eines spezifischen Programms für die Neuen Länder (InnoRegio) und dem Ausbau der Gesundheitsforschung. Neuland beschritt sie mit der gezielten Förderung des Dialogs zwischen Wissenschaft und Bevölkerung im Rahmen von Wissenschaft im Dialog und der Etablierung der Wissenschaftsjahre. Durch den Pakt für Forschung und Innovation erhielten die außeruniversitären Forschungseinrichtungen und die Deutsche Forschungsgemeinschaft finanzielle Planungssicherheit. Insgesamt gelang es Bulmahn trotz der angespannten Haushaltslage, einen deutlichen Mittelzuwachs von knapp 36 % für die Förderung von Bildung und Forschung durchzusetzen. Mit der Wahl von Angela Merkel zur Bundeskanzlerin schied sie am 22. November 2005 aus dem Amt.

Im Rückblick zeigt Bulmahn Verständnis für Kritik an den Auswirkungen von Teilen der in ihre Amtszeit fallenden Hochschulreform: Es sei etwas aus der Balance geraten zwischen Projektfinanzierung auf der einen und Grundfinanzierung auf der anderen Seite. In der Wissenschaft würden zugleich kurzfristiger Wettbewerb und langfristige Planung gebraucht. „Wenn Wissenschaftler aber nur noch Anträge schreiben müssen und gar nicht mehr die Kraft haben, kreativ zu sein und langfristige Forschungsinteressen zu verfolgen, dann ist eine Schieflage entstanden.“ Das gelte ebenso für die Kurzfristigkeit vieler Beschäftigungsverhältnisse: „2006, also kurz nach dem Ende meiner Amtszeit, ist passiert, wogegen ich mich als Ministerin gesperrt habe: dass Drei-, Fünf- oder Neunmonatsverträge en masse erlaubt wurden.“ Zu dem von ihr Erreichten zählt Bulmahn eine stärkere Profilierung und Vernetzung von außeruniversitären Forschungseinrichtungen und Hochschulen. „Das große Problemthema der neunziger Jahre war ja das Nebeneinander und die Erstarrung der einzelnen Einrichtungen. Diese sogenannte Versäulung haben wir dann durchbrochen.“ Als „die hochschulpolitische Notwendigkeit unserer Zeit“ bezeichnet es Bulmahn, die mangelnde Grundfinanzierung der Hochschulen mit Bundesmitteln zu beheben. Bei der Ausgestaltung der Bologna-Reform macht sie Defizite bezüglich der Personalausstattung und des Weiterbildungsangebots aus. 

Edelgard Bulmahn war stellvertretende Vorsitzende der Naturfreunde Deutschlands und gehörte dem Vorstand von Eurosolar an. Sie war Mitglied der Kuratorien von Öko-Institut, der Arbeitsgemeinschaft industrieller Forschungsvereinigungen „Otto von Guericke“, der Fraunhofer-Gesellschaft und der Volkswagenstiftung. Weitere Engagements übernahm sie als Senatorin der Stiftung Niedersachsen sowie in den Kuratorien Deutsche Telekom Stiftung, Deutsches Institut für Wirtschaftsforschung und in der Stiftung Lesen. Sie ist auch Mitglied des Stiftungsrates der von Georg Zundel gegründeten "Berghof Foundation for Conflict Studies" sowie stellvertretende Vorsitzende des deutsch-amerikanischen Netzwerks Atlantik-Brücke e. V.

Bulmahn engagierte sich in der Trilateralen Kommission in Europa und 
als Jurymitglied des „Innovationswettbewerbs Top 100“, einer Auszeichnung für die innovativsten Unternehmen im deutschen Mittelstand.




</doc>
<doc id="1454" url="https://de.wikipedia.org/wiki?curid=1454" title="Euklidische Geometrie">
Euklidische Geometrie

Die euklidische Geometrie ist zunächst die uns vertraute, anschauliche Geometrie des Zwei- oder Dreidimensionalen. Der Begriff hat jedoch sehr verschiedene Aspekte und lässt Verallgemeinerungen zu. Benannt ist dieses mathematische Teilgebiet der Geometrie nach dem griechischen Mathematiker Euklid von Alexandria.

Im engsten Sinne ist euklidische Geometrie die Geometrie, die Euklid in dem Werk "Die Elemente" dargelegt hat.
Über zweitausend Jahre lang wurde Geometrie nach diesem axiomatischen Aufbau gelehrt. Die Redewendung „more geometrico“ (lateinisch: „auf die Art der (euklidischen) Geometrie“) dient noch heute als Hinweis auf eine streng deduktive Argumentation.

Euklid geht dabei folgendermaßen vor:

Das Buch beginnt mit einigen Definitionen, beispielsweise:

Ähnlich werden Ebene, Winkel u. a. definiert.

Außer diesen mehr oder weniger anschaulichen Definitionen von "Grundbegriffen" gibt es auch Definitionen, die im modernen Sinne als "Worteinführungen" zu verstehen sind, weil sie im folgenden Text abkürzend gebraucht werden, so zum Beispiel für Parallelen: „Parallel sind gerade Linien, die in derselben Ebene liegen und dabei, wenn man sie nach beiden Seiten ins Unendliche verlängert, auf keiner Seite einander treffen.“

Insgesamt geben die "Elemente" 35 Definitionen.

Nach den eher beschreibenden Definitionen folgen die fünf eher festlegenden Postulate. Gefordert wird hier,

An die fünf aufgeführten geometrischen Postulate schließen sich mehrere logische Axiome an, zum Beispiel:

Hierauf aufbauend behandelt Euklid nun Probleme …

… und Theoreme 

Zur Lösung eines Problems oder zum Beweis eines Theorems werden grundsätzlich nur die Definitionen, Postulate und Axiome sowie vorher bewiesene Theoreme und die Konstruktionen aus vorher gelösten Problemen verwendet.

Als Platoniker war Euklid davon überzeugt, dass die von ihm formulierten Postulate und Axiome die Wirklichkeit wiedergeben. Gemäß Platons Ideenlehre gehören sie einer ontologisch höherrangigen Ebene an als die in den Sand gezeichneten Figuren, die ihre Abbildungen sind. Das Verhältnis zwischen einem unvollkommen gezeichneten Kreis und der vollkommenen Idee des Kreises illustriert den Unterschied zwischen der sinnlich wahrnehmbaren Welt und der intelligiblen (nur geistig erfassbaren) Welt, der in Platons Höhlengleichnis veranschaulicht wird.

Aus heutiger Sicht genügen "Die Elemente" nicht dem Anspruch an eine axiomatische Theorie:

Hieraus folgt, dass die Schlüsse notgedrungen eine Vielzahl von unausgesprochenen Annahmen verwenden.

In einem anderen Sinne ist euklidische Geometrie eine am Ende des 19. Jahrhunderts entstandene, streng axiomatische Theorie. Die oben genannten Probleme wurden deutlich, als sich Bertrand Russell, David Hilbert und andere Mathematiker um eine strengere Grundlegung der Mathematik bemühten. Sie wurden von Hilbert gelöst, der die Ergebnisse in seinem Werk "Grundlagen der Geometrie" (1899) veröffentlichte. Vorläufer waren Hermann Graßmann, Moritz Pasch, Giuseppe Peano und andere. Auch nach Hilbert wurden mehrere andere Axiomensysteme für die euklidische Geometrie aufgestellt.

David Hilbert verwendet „drei verschiedene Systeme von Dingen“, nämlich Punkte, Geraden und Ebenen, von denen er nur sagt: „Wir denken (sie) uns“. Diese Dinge sollen „in drei grundlegenden Beziehungen“ zueinander „gedacht werden“, nämlich „liegen“, „zwischen“ und „kongruent“. Zur Verknüpfung dieser „Dinge“ und „Beziehungen“ stellt er dann 20 Axiome in fünf Gruppen auf:

Als ein Vertreter des Formalismus erklärt Hilbert es für irrelevant, was diese Punkte, Geraden und Ebenen mit der Wirklichkeit zu tun haben. Die Bedeutung der Grundbegriffe sei dadurch bestimmt, dass sie die Axiome erfüllen. So beginnt er den Abschnitt über die Axiome der Verknüpfung mit dem Satz: „Die Axiome dieser Gruppe stellen zwischen den oben eingeführten Dingen: Punkte, Geraden und Ebenen eine "Verknüpfung" her und lauten wie folgt:…“ Die Definitionen der Grundbegriffe erfolgen also implizit.

Andererseits erklärt Hilbert in der Einleitung zu seinem Werk: „Die vorliegende Untersuchung ist ein neuer Versuch, für die Geometrie ein "vollständiges" und "möglichst einfaches" System von Axiomen aufzustellen…“. Mit diesem Bezug auf "die Geometrie" stellt er klar, dass es ihm nicht um einen beliebigen Formalismus geht, sondern um eine Präzisierung dessen, was Euklid mit „Geometrie“ gemeint hat und was wir alle als die Eigenschaften des uns umgebenden Raumes kennen. – Diese Präzisierung ist Hilbert vollständig gelungen, und sie erweist sich als viel aufwändiger, als Euklid ahnte.

Später aufgestellte Axiomensysteme sind grundsätzlich äquivalent zu dem Hilberts. Sie berücksichtigen die Weiterentwicklung der Mathematik.

Eine mögliche Axiomatisierung ist gegeben durch die Axiome der absoluten Geometrie zusammen mit dem folgenden Axiom, das unter Voraussetzung der übrigen Axiome der absoluten Geometrie gleichwertig zum Parallelenaxiom ist:

Weiterhin dient der Begriff euklidische Geometrie als Gegenbegriff zu den nichteuklidischen Geometrien.

Den Impuls gab dabei die Auseinandersetzung mit dem Parallelenpostulat. Nachdem jahrhundertelang zuvor vergeblich versucht worden war, dieses fünfte Postulat des Euklid auf ein einfacheres zurückzuführen, schlussfolgerten der Ungar János Bolyai und der Russe Nikolai Iwanowitsch Lobatschewski um 1830, dass eine Verneinung dieses fünften Postulates zu logischen Widersprüchen führen müsse, wenn dieses tatsächlich auf einfachere Aussagen zurückgeführt werden könne. Also verneinten die beiden Mathematiker dieses Postulat und definierten jeweils eigene (Ersatz-)Postulate, die wider Erwarten zu einem logisch völlig einwandfreien geometrischen System führten – den nichteuklidischen Geometrien: „Nicht der Beweis war indes so beunruhigend, sondern vielmehr sein rationales Nebenprodukt, das schon bald ihn und fast alles in der Mathematik überschatten sollte: Die Mathematik, der Eckstein wissenschaftlicher Gewissheit, war auf einmal ungewiss geworden. Man hatte es jetzt mit "zwei" einander widersprechenden Visionen unantastbarer wissenschaftlicher Wahrheit zu tun“, was zu einer tiefen Krise in den Wissenschaften führte (Pirsig, 1973).

Die genaue Formulierung des „hyperbolischen“ Axioms, das in der Geometrie von Lobatschewski, der hyperbolischen Geometrie, an die Stelle des Parallelenaxioms tritt, lautet: „… durch einen auf einer Gerade nichtliegenden Punkt gehen mindestens zwei Geraden, die mit dieser in einer Ebene liegen und sie nicht schneiden …“

Ob nichteuklidische Geometrien (es gibt verschiedene) den realen Raum beschreiben können, wird unterschiedlich beantwortet. Meist werden sie als rein abstrakt-mathematische Theorien verstanden, die nur durch die Ähnlichkeit der Begriffe und Axiomensysteme den Namen „Geometrie“ verdienen.

Diese Theorien haben sich inzwischen allerdings in der theoretischen Physik als sehr relevant für die Beschreibung der "Realität" unseres Weltalls erwiesen.

In einem Koordinatensystem lässt sich ein Punkt darstellen als ein Paar (in der ebenen Geometrie) oder als ein Tripel von reellen Zahlen. Eine Gerade oder Ebene ist dann eine Menge von solchen Zahlenpaaren (bzw. -tripeln), deren Koordinaten eine lineare Gleichung erfüllen. Die hierauf aufgebaute analytische Geometrie der reellen Zahlenebene formula_1 oder des reellen Zahlenraums formula_2 erweist sich als völlig äquivalent zu der axiomatisch definierten.

Man kann die analytische Geometrie als ein Modell für die axiomatische Theorie ansehen. Dann liefert sie einen Beweis der Widerspruchsfreiheit des Axiomensystems (wobei man allerdings eine widerspruchsfreie Begründung der reellen Zahlen als gegeben voraussetzen muss).

Man kann den analytischen Zugang aber auch als eine selbstständige (und bequemere) Begründung der Geometrie ansehen; aus dieser Sicht ist der axiomatische Zugang nur noch von geschichtlichem Interesse. Bourbaki zum Beispiel (und ebenso Jean Dieudonné) verzichtet vollständig auf die Verwendung originär geometrischer Begriffe und hält mit der Behandlung der topologischen Vektorräume das Thema für erledigt.

Euklidische Geometrie ist auch die Geometrie, in der "Strecken" und "Winkeln" Maße zugeordnet werden.

Im axiomatischen Aufbau der euklidischen Geometrie kommen "Zahlen" scheinbar überhaupt nicht vor. Es ist allerdings festgelegt, wie man an eine Strecke eine "kongruente" in der gleichen Richtung anfügt, diese also "verdoppelt" - und folglich auch mit einer beliebigen natürlichen Zahl "vervielfacht". Es gibt auch eine Konstruktion, um eine gegebene Strecke in "n" gleiche Teile zu teilen. Wird nun noch eine beliebige Strecke als Einheitsstrecke ausgezeichnet, so ist es damit möglich, Strecken zu konstruieren, deren Maßzahl eine beliebige "rationale Zahl" ist. Dies ist der wesentliche Gegenstand der altgriechischen "Arithmetik".

Bei anderen Konstruktionen ergeben sich Strecken, die keine rationale Zahl als Maßzahl haben. (Etwa die Diagonale des Quadrats über der Einheitsstrecke oder ihre Abschnitte bei der Teilung nach dem goldenen Schnitt.) Dies nachgewiesen zu haben, zeugt von dem unglaublich hohen Niveau der griechischen Mathematik schon zur Zeit der Pythagoreer. Somit wird die Einführung von irrationalen Zahlen erforderlich. 2000 Jahre später stellt Hilberts Vollständigkeitsaxiom sicher, dass alle reellen Zahlen als Maßzahlen für Strecken auftreten können.

Die Festlegung von Maßzahlen für Winkel verläuft ähnlich. Die Festlegung eines „Einheitswinkels“ entfällt, da mit dem "Vollwinkel" (oder dem "Rechten Winkel") ein objektives Maß existiert. Andererseits ist die Teilung des Winkels in formula_3 gleiche Teile wesentlich problematischer; längst nicht zu jedem rationalen Winkelmaß lässt sich ein Winkel konstruieren. Schon die Dreiteilung des Winkels misslingt im Allgemeinen.

Die so eingeführte Metrik ist äquivalent zu der durch die euklidische Norm induzierten euklidische Metrik des „analytischen“ formula_1 oder formula_2. Für die durch ihre Koordinaten gegebenen Punkte formula_6 und formula_7 ist also
formula_8.

Maßzahlen für Winkel lassen sich in der analytischen Geometrie über das Skalarprodukt von Vektoren definieren.

Als analytische Geometrie lässt sich die euklidische Geometrie ohne weiteres für eine beliebige (auch unendliche) Anzahl von Dimensionen verallgemeinern.

Zu den Geraden und Ebenen treten dann höherdimensionale lineare Punktmengen, die als Hyperebenen bezeichnet werden. (In einem engeren Sinne ist eine Hyperebene eines formula_3-dimensionalen Raumes ein möglichst „großer“, also formula_10-dimensionaler Teilraum.)

Die Zahl der Dimensionen ist dabei nicht beschränkt und muss auch nicht endlich sein. Zu jeder Kardinalzahl lässt sich ein euklidischer Raum dieser Dimension definieren.

Räume mit mehr als drei Dimensionen sind für unser Vorstellungsvermögen grundsätzlich unzugänglich. Sie wurden auch nicht mit dem Anspruch entworfen, menschliche Raumerfahrung darzustellen. Ähnlich wie bei den nichteuklidischen Geometrien fanden sich aber auch hier Bezüge zur theoretischen Physik: Die Raumzeit der speziellen Relativitätstheorie lässt sich als vierdimensionaler Raum darstellen. In der modernen Kosmologie gibt es Erklärungsansätze mit noch erheblich mehr Dimensionen.

Verzichtet man auf das 3. und 4. euklidische Postulat (also auf die Begriffe „Kreis“ und „Rechter Winkel“) oder beschränkt man sich, für eine präzisere Definition, auf Hilberts Axiome der "Verknüpfung" und der "Parallelen", so erhält man eine affine Geometrie. Sie wurde von Leonhard Euler erstmals entwickelt. Die Begriffe „Abstand“ und „Winkelmaß“ kommen hier nicht vor, wohl aber Strecken"verhältnisse" und Parallelität.

Ersetzt man das Parallelenaxiom durch die Festsetzung, dass zwei in einer Ebene gelegene Geraden immer einen Schnittpunkt haben sollen, so entsteht aus der affinen eine projektive Geometrie.

Wenn die Anordnungs- und Stetigkeitsaxiome wegfallen, können affine und projektive Geometrien auch aus endlich vielen Punkten bestehen.

In der synthetischen Geometrie wird der Begriff einer "euklidischen Ebene" so verallgemeinert, dass genau die Ebenen, deren affine Koordinaten in einem euklidischen Körper liegen, euklidische Ebenen sind.



</doc>
<doc id="1457" url="https://de.wikipedia.org/wiki?curid=1457" title="Elbląg">
Elbląg

Elbląg ( ), , ist eine kreisfreie Stadt in der polnischen Woiwodschaft Ermland-Masuren nahe der Ostseeküste im früheren Westpreußen (1920–1939 Ostpreußen). Die Stadt hat rund 122.000 Einwohner.

Die Stadt liegt im ehemaligen Westpreußen, rund 55 Kilometer ostsüdöstlich von Danzig am Südwestrand der Elbinger Höhe in der Elbinger Niederung nahe der Mündung der Flüsse Elbląg "(Elbing)" und Nogat in das Frische Haff (Zalew Wiślany).

Als Vorläufer der Stadt kann der altprußische Handelsort Truso betrachtet werden, der in unmittelbarer Nähe gelegen war. Truso wurde vom angelsächsischen Reisenden Wulfstan im Jahre 890 als am Flusse "Ilfing" befindlicher Handelsplatz erwähnt. Archäologischen Funden zufolge kam dem Ort überregionale Bedeutung zu, war er doch nicht nur an der Bernsteinstraße gelegen, sondern an den Handelsrouten zwischen Skandinavien und dem Mittelmeerraum. Hier mündete im 10. Jahrhundert der Weichselarm Nogat in eine Bucht des Frischen Haffes. Diese Bucht entspricht dem heutigen Druzno ("Drausensee", prußisch "Drusin"), der durch Verlandung vom Haff abgetrennt wurde. Die Ausgrabungen haben ergeben, dass Truso durch Seeräuber zerstört wurde.

Im Jahr 1237 errichtete der Deutsche Orden unter dem Landmeister Hermann von Balk in der Nähe des Drausensees eine Festung.
Die Stadt wurde im Jahr 1237 als Elbing in Pogesanien, damals Teil des Deutschordenslandes, unter dem Schutz des Deutschen Ordens durch aus Lübeck stammende Handwerker und Kaufleute gegründet. Es wurde zunächst eine Siedlung mit rasterförmigem Straßennetz angelegt. Das Zentrum bildete der spätere „Alte Markt“, der an dem großen Handelsweg zwischen Thorn und dem Samland gelegen war. Vor 1238 wurde die Stadtpfarrkirche St. Nikolai erbaut. 1238 ließ Landmeister Hermann von Balk die Liebfrauenkirche und ein Dominikanerkloster errichten. Bis 1246 erfolgte die Einwanderung von weiteren Bürgern, die ebenfalls überwiegend aus Lübeck stammten. 1246 erhielt Elbing das Stadtrecht nach Lübischem Recht und erhielt das Privileg, eigene Münzen zu schlagen. Im Süden der Stadt wurde während der 1240er Jahre das Ordensschloß mit einem Heilig-Geist-Hospital errichtet. In den Jahren 1251 bis 1309 war das Elbinger Ordensschloss der stellvertretende Hauptsitz des Ordensstaates (Hauptsitze waren damals Akkon und später Venedig) und Sitz der Landmeister von Preußen und des Großspittlers, gleichzeitig Residenz des ermländischen Bischofs Anselm, der hier 1274 starb. 

Die Kirche zum Heiligen Jakob (Filiale der Stadtpfarrkirche) entstand 1256. Die Corpus-Christi-Kirche mit einem Aussätzigenhospital wurde 1292 erbaut. Der Orden erbaute um 1300 die Befestigungen der Stadt mit 14 Wehrtürmen. In dieser Zeit war Elbing zu einer bedeutenden Handelsstadt angewachsen, die bedeutende Handelsprivilegien bei den Königen von Polen, den Herzögen von Pommern, den skandinavischen Herrschern und sogar bei König Philipp IV. von Frankreich erworben hatte. Im 13. Jahrhundert wurde die "schola senatoria" (Ratsschule). gegründet, und 1314 wurde der Elbinger Stadtturm erbaut.

Elbing entwickelte sich gemeinsam mit Danzig und Thorn zu einer der führenden Hansestädte im östlichen Mitteleuropa. Anfang des 14. Jahrhunderts war die Stadt so angewachsen, dass 1337 durch den Elbinger Komtur Siegfried von Sitten vor den Toren die Elbinger Neustadt angelegt wurde. Sie verfügte über einen eigenen Rat und wurde nach Lübischem Recht regiert. Dieser Neustadt erteilte am 25. Februar 1347 der Hochmeister Heinrich Dusemer das Privilegium.

Ab 1350 beteiligte sich die Elbinger Flotte an den Kämpfen der Hanse gegen norwegische und dänische Seeräuber in der Ostsee. 1360 wütete in Elbing die Pest, der etwa 13.000 Einwohner (etwa 90 %) zum Opfer fielen.

1367 trat Elbing mit Kulm und Thorn der Kölner Konföderation bei. Die Kirche zur Heiligen Brigitta von Schweden wurde nach 1379 erbaut. 1397 entstand der Eidechsenbund: Der Aufstand des Adels und der Städte gegen die Herrschaft des Ordens begann. Nach der Schlacht bei Tannenberg wurde Elbing acht Wochen lang von polnischen Truppen besetzt. Polnische Truppen belagerten 1414 das Elbinger Ordensschloss, jedoch ohne Erfolg.

1440 gründeten die preußischen Hansestädte, unter ihnen Elbing, gemeinsam mit den Landesständen den Preußischen Bund, der gegen die Herrschaft des Ordens gerichtet war und eine autonome Selbstverwaltung unter der Oberhoheit des polnischen Königs anstrebte. 1452 ließen sie sich ihre Rechte und Privilegien von Kaiser Friedrich III. bestätigen. Im daraufhin einsetzenden Dreizehnjährigen Krieg des Preußischen Bundes gemeinsam mit Polen gegen den Deutschen Orden (1453–1466) nahmen die Bürger Elbings an der Belagerung des Ordensschlosses durch die Polen teil und zerstörten das Schloss nach dessen Kapitulation. Die Ruinen des Schlosses wurden 100 Jahre später abgetragen. Ein Teil steht bis heute. Die Stadt huldigte 1454 dem Jagiellonen Polenkönig Kasimir IV. als Schutzherrn. Er und seine Nachfolger bestätigten der Stadt sämtliche alten Privilegien und verliehen viele neue. 1478 schlossen sich die bis dahin eigenständigen Stadthälften der Alt- und Neustadt Elbings zusammen.

Der Dreizehnjährige Krieg endete 1466 mit dem Zweiten Thorner Frieden, bei dem der Orden Pommerellen, das Culmer Land und Ermland sowie Danzig, Elbing und Marienburg verloren geben musste. Diese Gebiete unterwarfen sich als Preußen Königlichen Anteils freiwillig der polnischen Krone. Dadurch entstand eine Zweiteilung Preußens in einen westlichen polnischen Teil und einen östlichen Teil des Deutschen Ordens, der allerdings die polnische Oberhoheit anerkennen musste. Das Ordensland wurde 1525 in das weltliche Herzogtum Preußen umgewandelt. Das Heer des letzten Hochmeisters Albrecht von Brandenburg-Ansbach belagerte noch 1521 unter Führung von Komtur Kaspar von Schwalbach die Stadt Elbing. Die Belagerung konnte abgewehrt werden. Der Tag des Sieges wurde mehrere Jahrhunderte am ersten Freitag nach Sonntag Laetare als „Freudetag“ in der Stadt gefeiert.

Im Jahr 1536 wurde das erste evangelische Gymnasium von Willem van de Voldersgraft bzw. Wilhelm Fullonius, einem Glaubensflüchtling aus Den Haag, eingerichtet. Christoph Hartknoch beschrieb in seiner "Acta Borussica III" dessen Leben oder "Vita Guilielmi Gnaphei". In Hartknochs Arbeiten sind ebenfalls die preußischen Städte einschließlich Elbing dargestellt. Der Rektor des Elbinger Gymnasiums musste auf Grund des Erlasses des katholischen Fürstbischofs von Ermland Elbing verlassen und wurde dann Rat des Herzogs Albrecht von Preußen sowie Rektor und Professor der Universität Königsberg. 1576 bestätigte König Stephan Báthory das Privileg der protestantischen Schule, die bis zum Direktorat Johann Wilhelm Süverns 1803 einen akademischen Anspruch hatte. 1558 sicherte König Sigismund II. August der protestantischen Stadt Elbing die vorläufige Religionsfreiheit zu.

Durch sein staatsstreichartiges Dekret vom 16. März 1569 auf dem Lubliner Reichstag kündigte König Sigismund II. August die Autonomie Westpreußens jedoch unter Androhung herber Strafen einseitig auf, weshalb die Oberhoheit des polnischen Königs in diesem Teil des ehemaligen Gebiets des Deutschen Ordens von 1569 bis 1772 als Fremdherrschaft empfunden wurde.

1567 konnte die Stadt die volle religiöse Autonomie durchsetzen und verwies die Jesuiten der Stadt. Die Lutheraner übernahmen 1577 die Nikolaikirche. Seit dieser Zeit sind auch Kirchenbücher mit Eintragungen der Elbinger Taufen, Heiraten und Bestattungen vorhanden.

Ab 1579 unterhielt die Stadt enge Handelsbeziehungen zu England, das freien Handel in Elbing ausüben konnte. Viele englische und schottische Kaufleute kamen und wurden Bürger der Stadt Elbing. Sie organisierten sich in der "Fellowship of Eastland Merchants". Die Church of Scotland gründete die "Bruderschaft der Schottischen Nation in Elbing". Familiengräber mit Namen Ramsay, Slocombe waren noch bis 1945 auf dem St.-Marien-Friedhof in der Altstadt Elbings zu finden. Andere Familien aus diesem Kreis waren unter anderem die Lamberts, Paynes, Lardings, Wilmsons.

Der Aufruhr der Danziger gegen König Stephan Báthory von Polen wurde 1580 von den Elbingern, die dem König treu blieben, geschickt ausgenutzt. Für Polen spielte Elbing nun eine Schlüsselrolle im Überseehandel. Über die Nogat, die damals tiefer war als die Weichselmündung bei Danzig, erfolgte der polnische Getreideexport nach Westeuropa und umgekehrt der Import westlicher Luxuswaren bis weiter nach Polen. Die Stadt zählte im Jahr 1594 30.000 Einwohner, und der Umsatz von Waren, die von Elbinger Handelsleuten in diesem Jahre verkauft wurden, erreichte die für damalige Zeiten hohe Summe von 1.247.850 Talern. 
Die Stadtpfarrkirche wurde 1617 dem katholischen Klerus übergeben.

Um 1620 trat die Stadt aufgrund ihrer starken Handelsbeziehungen mit England aus der Hanse aus. 1625 folgte ein Ausbruch der Pest, in dessen folge 3.608 Menschen starben. Die Truppen des Schwedenkönigs Gustav II. Adolf nahmen 1626 die Stadt ein und hielten sie bis 1635 als Hauptquartier im Kampf zur Unterstützung der Evangelischen gegen die Katholischen im Dreißigjährigen Krieg. Der schwedische König setzte seinen Vertrauten und Reichskanzler Axel Oxenstierna in Elbing als Generalgouverneur für die neuen schwedischen Besitzungen ein. Dieser führte von 1626 bis 1631 neben den regionalen Geschäften auch einen Teil seiner nationalen Aufgaben von Elbing aus. In den etwa 1500 erhaltenen Briefen Oxenstiernas aus Elbing spiegeln sich militärische, ordnungs-, wirtschafts- und außenpolitische Themen der Zeit. Die Schweden nahmen Preziosen, Möbel, Bücher als Kriegsbeute und schickten diese in ihre Heimat.

1646 dokumentierte der Elbinger Stadtschreiber Daniel Barholz, dass der Elbinger Stadtrat Bernsteindreher (Paternostermacher) angestellt habe. Spätere Mitglieder der Familie Barholz waren prominent als Stadtrat und Bürgermeister. Auch der Barockdichter Daniel Bärholz gehörte dieser Familie an. Die Verarbeitung von Bernstein (preußisches Gold), nicht nur zu Schmuck und kirchlichen Artikeln, sondern als Heilmittel und zu Polierlack, war ein wichtiger Wirtschaftsfaktor jener Zeit. Die Gildemitglieder der Paternostermacher unterstanden besonderen Gesetzen. 
In den Jahren 1655 bis 1660 wurde Elbing im Zuge des Zweiten Nordischen Krieges ein zweites Mal durch schwedische Truppen unter Karl X. Gustav besetzt. Karl X. Gustav verfuhr dabei auf ähnliche Weise wie sein Onkel Gustav Adolf.

Der polnische König Johann II. Kasimir verpfändete Elbing und dessen Territorium 1657 im Vertrag von Wehlau an den Großen Kurfürsten für die Summe von 400.000 Talern und sicherte ihm außerdem die Souveränität über das Herzogtum Preußen zu. Als die polnische Krone die obige Summe nicht erstattet hatte, machte der Nachfolger des Großen Kurfürsten, Friedrich I. (Preußen) in Preußen, von seinem Recht Gebrauch und nahm 1703 das Elbinger Territorium in Besitz, das mithin preußisch wurde.

Die ansehnlichen Erträge, die bis dahin aus dem Territorium an die Stadt geflossen waren, wurden durch diesen Schritt erheblich beschnitten, was zu einer Lähmung der Wirtschaft und einem damit einhergehenden Rückgang der Bedeutung der Stadt führte. Hinzu kam, dass die Stadt Elbing zwar ihre Autonomie wahrte, doch in den folgenden Jahrzehnten mehrfach Besatzungen über sich ergehen lassen musste und damit einhergehende Kontributionen zu leisten hatte. So wurde Elbing während des Großen Nordischen Krieges nacheinander durch schwedische (1703–1710), russische (1710–1712) und sächsische Truppen (1712) besetzt. Während des Siebenjährigen Krieges wurde die Stadt 1758 von russischen Truppen erobert und bis 1762 besetzt gehalten.

Der kaiserliche Mathematiker und Geograph Johann Friedrich Endersch vollendete 1755 eine Karte Ermlands mit dem Titel "Tabula Geographica Episcopatum Warmiensem in Prussia Exhibens". Diese Karte zeigt Stadt und Land Elbing westlich des Ermlands und jedes Dorf in der Gegend. Die Karte von 1755 führt auch den Namen "Prussia Orientalis" (auf Deutsch: Ostpreußen). Endersch fertigte ebenfalls einen Kupferstich des Segelschiffes (Galiot), benannt D.Stadt Elbing (D=der Erbauer), später auch als "Die Stadt Elbing" bekannt, welches 1738 in Elbing erbaut worden war.

1772 kam Elbing im Rahmen der ersten polnischen Teilung zum Preußischen Staat. Zwar verlor Elbing dadurch seine städtische Autonomie und einige damit einhergehende Privilegien, doch war nun die vollständige Unabhängigkeit der deutschen Stadt von der polnischen Krone wiederhergestellt.

Friedrich II. unterstützte Elbing durch viele Steuererleichterungen, und der Handel begann wieder aufzublühen. 1807 besetzten Napoleons Truppen Elbing und erzwangen innerhalb von vier Tagen eine Kontribution von 200.000 Talern. Am 8. Mai 1807 hielt Napoleon I. in Elbing eine große Truppenparade ab. Vom Dezember 1812 bis Januar 1813 musste die Stadt nach seinem gescheiterten Russlandfeldzug 60.000 zurückflutende französische Soldaten, 8.000 Offiziere und 22.000 Pferde ernähren.

Nach den Stein-Hardenbergschen Verwaltungsreformen war Elbing ab 1815 Teil des Kreises Elbing im Regierungsbezirk Danzig der Provinz Westpreußen. Elbing blieb bis 1945 Verwaltungssitz dieses Landkreises, wurde aber 1874 ein Stadtkreis (kreisfreie Stadt) und unterstand seither nicht mehr der Zuständigkeit des Landratsamts.

Industrialisierung und Verkehrswegebau bestimmten das Schicksal der Stadt im 19. Jahrhundert. 1828 stellten die Elbinger das erste Dampfschiff Ostpreußens in Dienst. 1837 wurden die Schichau-Werke gegründet. 1840 bis 1858 wurde der Oberländische Kanal zwischen Deutsch Eylau, Osterode und Elbing nach Plänen und unter Leitung des Königlich-Preußischen Baurats Georg Steenke angelegt. Am 23. Oktober 1844 erfolgte die Gründung der Baptistengemeinde Elbing.

In den 1840er Jahren wurde zusätzlich zu dem bereits bestehenden Gymnasium eine Realschule ins Leben gerufen. Um die Mitte des 19. Jahrhunderts verfügten die im Hafen von Elbing vertretenen Reeder über 14 Handelsschiffe. 1853 wurde die Eisenbahnlinie nach Königsberg fertiggestellt. 1858 bis 1918 erfolgte ein großer wirtschaftlicher Aufschwung der Stadt. Die Stadt hatte viele Fabriken: die Schichau-Werke, die jetzt auch unter anderem Lokomotiven herstellten, die Zigarrenfabrik Loeser & Wolff, eine große Brauerei und Schnapsbrennerei, eine Schokoladefabrik, die Autofabrik Komnick und viele andere Betriebe. Am Anfang des 20. Jahrhunderts hatte Elbing sieben evangelische Kirchen, eine katholische Kirche, vier Bethäuser verschiedener Freikirchen und Glaubensgemeinschaften sowie eine Synagoge.

In der Industriestadt Elbing erhielt die SPD stets die Mehrheit der Wählerstimmen, bei den Reichstagswahlen 1912 sogar 51 %. Laut der preußischen Volkszählung von 1905 waren in den Kreisen Elbing Stadt und Elbing Land 94.065 Personen deutschsprachig und 280 Personen polnisch- bzw. kaschubischsprachig.

Nach dem Ersten Weltkrieg musste Deutschland aufgrund der Bestimmungen 
des Versailler Vertrags 1920 den größten Teil Westpreußens zum Zweck der Einrichtung des Polnischen Korridors an Polen abtreten. Gleichzeitig wurde die ethnisch deutsche, politisch aber von Polen abhängige Freie Stadt Danzig gebildet und ebenfalls vom Reichsgebiet abgetrennt. Die westlich der Nogat gelegenen Teile des Landkreises Elbing fielen an den neuen Freistaat Danzig. Die Stadt Elbing gehörte zu den Gebieten, die bei Deutschland verblieben, und wurde nach Auflösung der Provinz Westpreußen an das benachbarte Ostpreußen angegliedert. Die neu hinzugekommenen westpreußischen Gebiete bildeten dort den Regierungsbezirk Westpreußen, dessen Verwaltungssitz sich in Marienwerder befand, in dem Elbing jedoch die größte Stadt war. 1926 wurde eine Pädagogische Akademie zur Ausbildung von Volksschullehrern eingerichtet.

Die Weltwirtschaftskrise nach 1929 beeinflusste Elbings Situation sehr ungünstig. In den Jahren der Weimarer Republik war Elbing eine Hochburg der KPD. Die auf Deutschlands Aufrüstung gerichtete Politik der NSDAP brachte ab 1933 einen großen wirtschaftlichen Aufschwung für Elbing, hauptsächlich durch den Ausbau der Schichau-Werke, den Bau einer Flugzeugfabrik und die Eröffnung vieler neuer Schulen. 1937 hatte die Stadt 76.000 Einwohner. Nach dem Überfall auf Polen 1939, durch den die 1920 entnommenen Territorien wieder an das Reichsgebiet zurückkamen, wurde Elbing an den Regierungsbezirk Danzig im Reichsgau Danzig-Westpreußen angegliedert.

Während des Zweiten Weltkriegs wurden in Elbing fünf Arbeitslager für vornehmlich polnische Zwangsarbeiter errichtet, die dem KZ Stutthof als Außenlager unterstellt waren. Außerdem gab es im Kreis Elbing 15 weitere Zwangsarbeitslager, die für die Rüstungsproduktion arbeiteten. Um den 23. Januar 1945 begann die Belagerung Elbings durch die Rote Armee. Die Stadt mit ihrer strategisch wichtigen Lage wurde bis zum 10. Februar verteidigt. Am Ende lagen 60 Prozent der Gebäudesubstanz der Stadt in Trümmern (insgesamt 5255 Gebäude). Alle Baudenkmäler waren stark beschädigt, nur sechs Häuser in der Altstadt blieben stehen, darunter das "Kramer-Zunfthaus" und das Postamt. Etwa 5000 deutsche Soldaten fielen, viele Zivilisten ertranken im Frischen Haff während der Flucht aus der belagerten Stadt.

Im Jahre 1918 hatte Elbing Bücherschätze von europäischem Rang beherbergt. Im "Stadtarchiv", das im 17. Jahrhundert gegründet worden war, befanden sich viele wertvolle Pergamente aus dem 13. Jahrhundert und wertvolle historische Sammlungen aus dem 15. Jahrhundert. Die "Bibliothek am Gymnasium" (15.000 Bände) besaß unter anderem ein polnisches Gesetzbuch aus dem 13. Jahrhundert, die "Bibliothek an der Nikolaikirche" (gegründet vor 1403) 23 alte Handschriften und 1.478 alte theologische Werke. Die "Bibliothek an der Marienkirche" verfügte über eine herausragende Sammlung von Musikhandschriften – 520 Werke aus der Zeit vom 16. bis zum 19. Jahrhundert. Die "Stadtbibliothek" (gegründet 1601) hatte die wertvollste Sammlung: 30.000 Bände, darunter 214 Handschriften, 123 Inkunabeln und 770 Landkarten. Das "Stadtmuseum" beherbergte die ehemalige Bibliothek der Dominikaner, unter anderem 50 Handschriften und 15 Inkunabeln. Alle diese Bücherschätze sind seit 1945 verschollen.

Bis zum Ende des Zweiten Weltkrieges hatte die Stadt rund 100.000 vorwiegend evangelische Einwohner deutscher Nationalität. Nachdem Elbing im Frühjahr 1945 von der sowjetischen Besatzungsmacht unter polnische Verwaltung gestellt worden war, wurde von den Polen die Ortsbezeichnung "Elbląg" eingeführt. Die Einwohner wurden in der Folgezeit vertrieben und durch Polen ersetzt.

Die ersten Vertreter der kommunistischen Behörden erschienen im März 1945 in Elbing. Sämtliche Maschinen in den Fabriken, die unzerstört geblieben waren – zum Beispiel in den Schichau-Werken – wurden zwischen 1945 und 1946 demontiert und als Reparationsleistung in die Sowjetunion abtransportiert. Auch Küchenherde, Kachelöfen, Badewannen, Junkers-Öfchen, Türschlösser und -klinken aus unzerstörten Privathäusern wurden dorthin verbracht.

Von 1946 bis 1947 erfolgte die Vertreibung der verbliebenen deutschen Bevölkerung, vor allem in die britische Besatzungszone Deutschlands. Gleichzeitig begann die Migration von Polen, zum Teil aus den Gebieten östlich der Curzon-Linie aber auch aus Zentralpolen. Die sowjetischen Militärbehörden gaben 1946 den Seehafen an die polnische Stadtverwaltung. Da die Ausfahrt zur Ostsee bei Baltijsk ("Pillau") nunmehr unter sowjetischer Kontrolle stand, war die Nutzung des Hafens nur sehr eingeschränkt möglich.

1948 hatte die Stadt 40.000 Einwohner. Ab 1950 begann der Wiederaufbau der Elbinger Industrie. Die Stadt wurde wieder zu einem wichtigen Zentrum der Maschinen- und Transportindustrie, außerdem besitzt die Stadt Holz-, Lebensmittel- und Textilindustrie. Die Stadt hatte im Jahr 1962 eine Anzahl von 81.400 Einwohnern. Viele Bewohner von Elbląg beteiligten sich 1970 zusammen mit Bürgern in Danzig und Stettin am Aufstand gegen die kommunistische Regierung in Polen.

Die Stadt wurde im Zuge der Polnischen Verwaltungsreform 1970 Hauptstadt der gleichnamigen Woiwodschaft. Die Streiks im August 1980 führten zum Aufbau der freien Gewerkschaft Solidarność unter Beteiligung vieler Einwohner Elblągs.

Ab 1990 wurde die Altstadt unter Verwendung historistischer Bauformen wie spitze Giebel zur Straße sowie von Fachwerkimitationen wieder aufgebaut. Nach dem Jahr 2000 stehen wieder viele Gebäude nahe, aber nicht direkt an der Elbląger „Waterkant“. Die Stadt wurde 1992 zum Sitz des katholischen Bistums Elbląg erhoben, das zum neugeschaffenen Erzbistum Ermland gehört. Der Hafen bekam 1994 seine Rechte als Seehafen mit eingeschränkten Nutzungsmöglichkeiten zurück, da die Ausfahrt zur offenen Ostsee unverändert über russisches Hoheitsgebiet durch die Pillauer Meerenge in der Frischen Nehrung verläuft.

Elbląg verlor bei der Verwaltungsreform 1998 seinen Rang als Hauptstadt einer Woiwodschaft, gehört seitdem zur von Olsztyn "(Allenstein)" aus verwalteten Woiwodschaft Ermland-Masuren und ist dort wieder Stadtkreis und Sitz der Kreisverwaltung für den Powiat Elbląski. Die Stadt erhielt 1999 den EU-Preis für Umweltpflege. Die Stadt erhielt 2000 die internationale Auszeichnung „Europäische Fahne“.

Blasonierung: „Von Silber und goldgegittertem Rot geteilt, oben und unten je ein Kreuz in verwechselten Tinkturen.“

Der noch erhaltene Bronzestempel des 1242 gebrauchten SIGILLVM BVRGENSIVM IN ELVIGGE zeigt auf Wellen eine von einem Schiffer linkshin gesteuerte Kogge, über der ein Kreuzlein schwebt. Auch der silberne Stempel des zweiten großen Siegels ist noch vorhanden; hierbei steht das Kreuzchen in der Flagge, während das dritte Schiffssiegel (15. Jahrhundert) darin die beiden Kreuze aufweist, die schon das Dekret des 14. Jahrhunderts im Dreieckschilde zeigt und die alle späteren Siegel enthalten.

Elbląg unterhält mit 14 Städten bzw. Orten Partnerschaften:

Bis 1945 führte die Reichsstraße 1 durch die Stadt, auf deren Trasse heute die Droga krajowa 22, die Droga wojewódzka 500 und die Droga wojewódzka 504 angelegt sind.

Elbląg liegt an den Droga krajowa 7 (ehemalige deutsche Reichsstraße 130) (Danzig–Warschau) und 22 (ehemalige Reichsstraße 1) nach Gorzów Wielkopolski "(Landsberg an der Warthe)" bzw. Kaliningrad ("Königsberg (Preußen)").

Elbląg besitzt einen Bahnhof an der Strecke Bahnstrecke Malbork–Braniewo ("Marienburg"–"Braunsberg"), der ehemaligen Preußischen Ostbahn. In Elbląg beginnt die größtenteils stillgelegte Bahnstrecke Elbląg–Braniewo, die frühere Haffuferbahn, und die schon seit 1945 stillgelegte Bahnstrecke Elbing–Miswalde.

Elbląg verfügt über einen Verkehrslandeplatz im Stadtteil Nowe Pole ("Neustädterfeld"). Der nächste internationale Flughafen ist der Lech-Wałęsa-Flughafen in Danzig.

Für den Schiffsverkehr wurde im Juni 2006 ein neuer Seehafen am Fluss Elbląg in Betrieb genommen, in dem jährlich bis zu 750.000 Tonnen Güter umgeschlagen werden können. Der Hafen ist auch für den Personen- und Autofährverkehr auf der Ostsee vorgesehen. Des Weiteren wurde der Jachthafen modernisiert. Elbląg verfügt jedoch über keinen freien Zugang zur Ostsee, der Schifffahrtsweg führt über das Frische Haff (polnisch "Zalew Wislany", russisch "Kaliningradski Zaliw") durch russische Hoheitsgewässer (Oblast Kaliningrad). Im Mai 2006 wurde dieser Weg von russischer Seite für den internationalen Verkehr gesperrt. Um diesem Problem aus dem Weg zu gehen, gibt es das Projekt, einen Kanal durch die Frische Nehrung (polnisch "Mierzeja Wiślana") zur Ostsee zu bauen.

Nach fünf Jahren ohne Schiffsverkehr fuhren ab Anfang 2011 wieder russische Frachtschiffe den Hafen an.

Die ehemaligen Schichau-Werke wurden 1945 in ELZAM umbenannt und gehören seit 1990 zum Asea Brown Boveri-Konzern (dann Alstom, heute General Electric). Der Betrieb produziert Turbinen und Elektromotoren. Die Brauerei Elbrewery (Marke "EB") ist der zweitgrößte Arbeitgeber der Stadt. Außerdem besitzt die Stadt bedeutende Transportmittelfabriken, eine Schiffswerft, und es haben sich Milch-, Fleisch-, Leder-, Textil- und Möbelindustrie angesiedelt.

Bewohner und Besucher der Stadt treffen an Straßen und Plätzen auf Skulpturen polnischer und internationaler Künstler und Künstlerinnen. Seit 1965 die erste Biennale der "Räumlichen Formen" stattfand, sind zahlreiche bleibende Werke entstanden, die das Stadtbild von Elbląg mitprägen.

Prominenteste Teilnehmerin der ersten Biennale war Magdalena Abakanowicz mit der Stahlplastik "Standing Shape". 1973 fanden die Ausstellungen erst einmal ein Ende. Seit 1986 gibt es sie wieder.

Eine wichtige Rolle bei der Durchführung der Biennale spielt die "Galeria-EL", (Pani Marii), die sich in dem Gebäude der ehemaligen St. Marien-Kirche, der ältesten Kirche Elbings, befindet. Diese entstand im 13. Jahrhundert als Kirchengebäude des Dominikaner-Ordens. Bis 1945 evangelische Kirche, wurde sie nach 1945 nicht mehr als Kirche genutzt. Die Stadtverwaltung hat hier eine Kunstgalerie eingerichtet, in der Bilder und Skulpturen zeitgenössischer Künstler gezeigt werden, die neben erhaltenen Grabplatten und Grabinschriften der Marienkirche ausgestellt sind und an die Verdienste ehemaliger Adels- und Kaufmannsfamilien, Stadtpatrizier und Geistlicher erinnern.

In Elbląg wirken folgende Lehranstalten (Stand in den 2010er Jahren):

Der Fußballverein Olimpia Elbląg spielt in der zweiten polnischen Liga.



Die Stadt ist Verwaltungssitz der gleichnamigen Landgemeinde Elbląg, gehört ihr aber als eigenständige Stadtgemeinde nicht an. Die Landgemeinde Elbląg ist Teil des Powiat Elbląski ("Elbinger Kreis") und bildet einen Gürtel um die kreisfreie Stadt Elbląg. Die Gemeinde zählt 7.239 Einwohner (30. Juni 2014) auf einer Fläche von 191 km² und gliedert sich in 37 Ortschaften, davon 24 mit einem Schulzenamt.

Partnergemeinden der Gmina Elbląg sind Barßel in Niedersachsen seit 2001 sowie Chechelnyk in der Ukraine seit 2004.




</doc>
<doc id="1458" url="https://de.wikipedia.org/wiki?curid=1458" title="Eric Hoffer">
Eric Hoffer

Eric Hoffer (* 25. Juli 1902 in New York City; † 21. Mai 1983 in Kalifornien) war ein sozialkritischer US-amerikanischer Philosoph und Autor. Seine Ideen hat er in zehn Büchern dargestellt, deren erstes, "The True Believer", sowohl von ihm selbst als auch von der Kritik als sein bestes und wichtigstes angesehen wird. Im Februar 1983 wurde er von Ronald Reagan mit der Presidential Medal of Freedom ausgezeichnet.

Hoffer wurde in New York City als Sohn elsässischer Einwanderer geboren. Mit fünf Jahren konnte er englisch und deutsch lesen, erblindete aber, nachdem seine Mutter mit ihm im Arm eine Treppe hinabgestürzt war. Im Alter von fünfzehn Jahren erlangte er sein Augenlicht zurück. Aus Angst, erneut zu erblinden, begann er so viel wie möglich zu lesen. Er verlor sein Augenlicht nicht, behielt aber die Angewohnheit, viel zu lesen, sein Leben lang bei.

Nach dem frühen Tod seiner Eltern suchte Hoffer eine Beschäftigung, die ihm genug Zeit zum Lesen lassen würde. Er begab sich nach Kalifornien, wo er (nach Gerüchten) aus medizinischen Gründen nicht zum Militärdienst zugelassen wurde. Seine tiefe Ablehnung des Nationalsozialismus motivierte ihn, im "San Francisco Naval Shipyard" am Bau von Kriegsschiffen mitzuarbeiten. Während dieser Zeit begann er seine lebenslange Angewohnheit, sich neben seiner Tätigkeit als Gelegenheitsarbeiter (später in der Landwirtschaft, als Goldsucher oder Hafenarbeiter) umfassend literarisch zu bilden. Nachdem er eher zufällig die "Essays" von Michel de Montaigne in einer Gebrauchtbuchhandlung gefunden hatte, fühlte er sich zum Schreiben berufen.

Hoffer war einer der ersten, die das Selbstwertgefühl als von zentraler Bedeutung für das psychische Wohlbefinden des Einzelnen erkannten. Im Gegensatz zum heute betonten Nutzen eines hohen Selbstwertes betrachtete Hoffer die Folgen eines mangelnden Selbstwertes. Er versuchte, die Ursachen totalitärer Massenbewegungen (exemplarisch in Hitlers Nationalsozialismus und Stalins Sowjetkommunismus) aus der psychischen Aufmachung der jeweiligen Anhänger zu verstehen. Allgemein sah er Fanatismus und Selbstgerechtigkeit durch Unsicherheit und Selbstzweifel hervorgerufen. Wie er in "The True Believer" ausführt, kompensieren nach seinen Beobachtungen Menschen die Inhaltsleere des eigenen Lebens durch eine leidenschaftliche Hinwendung an die äußere Welt oder eine Führerperson oder -ideologie. Obwohl Hoffer seine Vorstellungen in erster Linie an den totalitären Massenbewegungen seiner Zeit entwickelte, scheute er sich nicht, auch weniger extreme Bewegungen religiöser oder politischer Ausrichtung als Anlaufpunkt unsicherer Menschen zu bezeichnen.

Hoffers Thesen waren nicht nur neu, sondern ohne jegliche Anlehnung an die psychologischen Lehren seiner Zeit (Freud'sche Psychologie). Bewunderer von Hoffers Ideen schreiben seiner Unabhängigkeit von der akademischen Welt und der Freiheit von Zwängen die Originalität seiner Ideen zu.

Hoffer war zu seiner Zeit einer der den USA zugeneigtesten Schriftsteller. Er verstand sich nicht als „Intellektueller“, insbesondere da er die Bezeichnung negativ für anti-amerikanische Akademiker verstand. Seiner Ansicht nach waren Akademiker in erster Linie machthungrig und kompensierten die ihnen in demokratischen Staaten (aber nicht totalitären Staaten) verweigerte Macht, indem sie sich durch übertriebene Kritik wichtig machten.

Hoffer selbst sah seine eigene Herkunft aus einem bescheidenen Milieu als ermutigend. Er sah sich selbst als Außenseiter und verstand Außenseiter als Vorreiter der Gesellschaft. Obwohl er links eingestellten Akademikern deutlich kritisch gegenüberstand, kann man ihn nicht als Konservativen bezeichnen. Er stand außerhalb der Strömungen seiner Zeit und verstand sich als Hafenarbeiter, dessen Schreiben aus seinen Lebensumständen hervorgebracht worden sei. Vielleicht verband ihn deshalb mit einer anderen großen Außenseiterin des akademischen Betriebs, Hannah Arendt, eine besondere Sympathie.


deutsche Ausgaben:



</doc>
<doc id="1459" url="https://de.wikipedia.org/wiki?curid=1459" title="Elektromagnetische Welle">
Elektromagnetische Welle

Als elektromagnetische Welle bezeichnet man eine Welle aus gekoppelten elektrischen und magnetischen Feldern. Beispiele für elektromagnetische Wellen (auch als elektromagnetische Strahlung oder kürzer "Strahlung" bezeichnet) sind Radiowellen, Mikrowellen, Wärmestrahlung, Licht, Röntgenstrahlung und Gammastrahlung. Elektromagnetische Wellen im Vakuum sind Transversalwellen. Die Wechselwirkung elektromagnetischer Wellen mit Materie hängt von ihrer Frequenz ab, die über viele Größenordnungen variieren kann. 

Anders als zum Beispiel Schallwellen benötigen elektromagnetische Wellen "kein" Medium, um sich auszubreiten. Sie können sich daher auch über weiteste Entfernungen im Weltraum ausbreiten. Sie bewegen sich im Vakuum unabhängig von ihrer Frequenz mit Lichtgeschwindigkeit fort. Elektromagnetische Wellen können sich aber auch in Materie ausbreiten (etwa einem Gas oder einer Flüssigkeit), ihre Geschwindigkeit ist dabei allerdings verringert. Der Brechungsindex gibt das Verhältnis an, um das die Phasengeschwindigkeit von elektromagnetischen Wellen in Materie geringer als die Lichtgeschwindigkeit im Vakuum ist.

Als Transversalwellen zeigen elektromagnetische Wellen das Phänomen der Polarisation. Im freien Raum stehen die Vektoren des elektrischen und des magnetischen Feldes senkrecht aufeinander und auf der Ausbreitungsrichtung. Die Transversalität ist unter Umständen verletzt, wenn – wie bei Plasmaschwingungen (Plasmonen) – Träger chemischer Eigenschaften, z. B. metallische oder gebundene Elektronen, beteiligt sind. Entsprechend unterscheiden sich die Quellen, Ausbreitungseigenschaften und Wirkungen der Strahlung in den verschiedenen Bereichen des elektromagnetischen Spektrums.

Elektromagnetische Wellen können durch unterschiedliche Ursachen entstehen:

Empfänger für elektromagnetische Strahlung nennt man Sensoren oder Detektoren, bei Lebewesen Photorezeptoren. Radiowellen können durch Antennen detektiert werden.

An einer elektromagnetischen Welle lässt sich die Wellengeschwindigkeit messen, einerseits die im Vakuum universale Konstante Lichtgeschwindigkeit formula_1, sowie davon abweichende Werte für die Phasengeschwindigkeit formula_2 in einem durchlässigen (durchsichtigen) Medium. Messbar ist ferner die Intensität, gleich bedeutend mit der Leistung, bzw. mit der pro Zeit-Einheit durch einen bestimmten Querschnitt transportierten Energie.

Um die Wellenlänge zu messen, gibt es unterschiedliche Methoden, je nachdem, ob es sich um kürzere oder längere Wellenlängen handelt. Wellenlänge formula_3 und Frequenz formula_4 lassen durch
ineinander umrechnen.

Physikalisch betrachtet handelt es sich bei elektromagnetischen Wellen um sich ausbreitende Schwingungen des elektromagnetischen Feldes. Hierbei stehen elektrisches und magnetisches Feld bei linear polarisierten Wellen senkrecht aufeinander und haben ein festes Größenverhältnis, welches durch die Wellenimpedanz gegeben ist. Insbesondere verschwinden elektrisches und magnetisches Feld an denselben Orten zur selben Zeit, so dass die häufig gelesene Darstellung, dass sich elektrische und magnetische Energie zyklisch ineinander umwandeln, im Fernfeld "nicht" richtig ist. Sie stimmt allerdings zum Beispiel für das Nahfeld eines elektromagnetische Wellen erzeugenden elektrischen Dipols oder Schwingkreises.

Die Entstehung elektromagnetischer Wellen erklärt sich aus den maxwellschen Gleichungen: Die zeitliche Änderung des elektrischen Feldes ist stets mit einer räumlichen Änderung des magnetischen Feldes verknüpft. Ebenso ist wiederum die zeitliche Änderung des magnetischen Feldes mit einer räumlichen Änderung des elektrischen Feldes verknüpft. Für periodisch (insbesondere sinusförmig) wechselnde Felder ergeben diese Effekte zusammen eine fortschreitende Welle.

"Beispiele für Experimente, in denen der Wellencharakter zum Tragen kommt:"

Für bestimmte Eigenschaften elektromagnetischer Wellen (z. B. photoelektrischer Effekt) genügt das oben beschriebene Wellenmodell nicht, um alle beobachtbaren Phänomene zu beschreiben, vielmehr treten die Teilcheneigenschaften einzelner Photonen, der Quanten des elektromagnetischen Feldes, in den Vordergrund. Der Wellencharakter (etwa Interferenz) bleibt aber voll erhalten. Man spricht deshalb vom Dualismus von Teilchen und Welle.

Im Rahmen dieser Teilchenvorstellung des Lichtes wird jeder Frequenz formula_6 die Energie eines einzelnen Photons formula_7 zugeordnet, wobei formula_8 das plancksche Wirkungsquantum ist. Andererseits haben auch Teilchen, wie zum Beispiel über mehrere Atome hinweg bewegte Elektronen, Welleneigenschaften (siehe auch Elektrischer Strom).
Beide Aspekte elektromagnetischer Wellen lassen sich im Rahmen der Quantenelektrodynamik erklären.

"Beispiele für Wirkungen, in denen der Teilchencharakter zum Tragen kommt:"

Photonen mit genügender Energie (etwa von einigen Elektronvolt aufwärts) wirken auf Materie ionisierend und können chemische ("photochemische") Wirkungen auslösen, wenn die Bindungsenergien überschritten werden (Fotochemie). Diese chemische Wirksamkeit wird auch als Aktinität bezeichnet.

Die Phasengeschwindigkeit formula_9 mit der sich eine monochromatische Welle in einem Medium bewegt, ist typischerweise geringer als im Vakuum. Sie hängt in linearer Näherung von der Permittivität formula_10 und der Permeabilität formula_11 des Stoffes ab,
und ist damit abhängig von der Frequenz der Welle (siehe Dispersion) und bei doppelbrechenden Medien auch von ihrer Polarisation und Ausbreitungsrichtung. Die Beeinflussung der optischen Eigenschaften eines Mediums durch statische Felder führt zur Elektrooptik bzw. Magnetooptik.

Eine direkte Krafteinwirkung (z. B. Richtungsänderung) auf eine sich ausbreitende elektromagnetische Welle kann nur durch das Ausbreitungsmedium erfolgen (siehe Brechung, Reflexion, Streuung und Absorption) bzw. vermittelt werden (siehe Nichtlineare Optik und Akustooptischer Modulator).

Elektromagnetische Wellen sind im elektromagnetischen Spektrum nach der Wellenlänge eingeteilt. Eine Liste von Frequenzen und Beispiele elektromagnetischer Wellen gibt es im entsprechenden Artikel.

Das sichtbare Licht stellt nur einen geringen Teil des gesamten Spektrums dar und ist, mit Ausnahme der Infrarotstrahlung (Wärme), der einzige Bereich, der von Menschen ohne technische Hilfsmittel wahrgenommen werden kann. Bei niedrigeren Frequenzen ist die Energie der Photonen zu gering, um chemische Prozesse auslösen zu können. Bei höheren Frequenzen hingegen beginnt der Bereich der ionisierenden Strahlung (Radioaktivität), bei der ein einziges Photon Moleküle zerstören kann. Dieser Effekt tritt bereits bei Ultraviolett-Strahlung auf und ist für die Bildung von Hautkrebs bei übermäßiger Sonnenexposition verantwortlich.

Beim Licht bestimmt die Frequenz die Farbe des Lichtes und nicht, wie oft fälschlicherweise angenommen, die Wellenlänge in einem Medium bei der Ausbreitung. Die Frequenz wird anders als die Wellenlänge beim Übergang in optisch dichtere Medien nicht beeinflusst. Da sich die Farbe aber beim Durchgang durch ein Medium nicht ändert, ist nur die Frequenz charakteristisch für die Farbe des Lichts. In Spektren wird aus historischen Gründen jedoch die Wellenlänge als charakteristische Eigenschaft angegeben. Der Zusammenhang zwischen Farbe und Wellenlänge gilt nur im Vakuum und in guter Näherung in Luft. Monochromatisches Licht, also Licht mit nur einer einzigen Wellenlänge, hat stets eine Spektralfarbe.

Bei der Wechselwirkung von elektromagnetischer Strahlung mit biologischer Materie muss zwischen ionisierender Strahlung (größer 5 eV) und nicht-ionisierender Strahlung unterschieden werden. Bei der ionisierenden Strahlung reicht die Energie aus, um Atome oder Moleküle zu ionisieren, d. h. Elektronen herauszuschlagen. Dadurch werden freie Radikale erzeugt, die biologisch schädliche Reaktionen hervorrufen.
Erreicht oder übersteigt die Energie von Photonen die Bindungsenergie eines Moleküls, kann jedes Photon ein Molekül zerstören sodass beispielsweise eine beschleunigte Alterung der Haut oder Hautkrebs auftreten kann. Chemische Bindungsenergien stabiler Moleküle liegen oberhalb von etwa 3 eV pro Bindung. Soll es zu Moleküländerungen kommen, müssen Photonen mindestens diese Energie besitzen, was violettem Licht oder höherfrequenter Strahlung entspricht.

Bei der Wechselwirkung von nicht-ionisierender Strahlung unterscheidet man zwischen thermischen Effekten (Strahlung wirkt erwärmend, weil sie durch das Gewebe absorbiert wird), direkte Feldeffekte (induzierte Dipolmomente, Änderung Membran-Potentialen), Quanten-Effekte und Resonanzeffekte (Synchronisation mit Schwingung der Zellstruktur). Darüber hinaus werden weitere indirekte Effekte diskutiert.

Ein Photon mit einer Wellenlänge von 700 nm oder kürzer kann im Molekül Rhodopsin die Änderung der Konformation hervorrufen. Im Auge wird diese Änderung aufgenommen und als Signal vom Nervensystem weiter verarbeitet. Die Wellenlänge mit der größten Empfindlichkeit ändert sich bei Modifikationen des Rhodopsins. Dies ist die biochemische Grundlage des Farbsinns. Photonen von Licht mit einer Wellenlänge über 0,7 µm haben eine Energie unter 1,7 eV. Diese Wellen können keine chemischen Reaktionen an Molekülen bewirken, die bei Zimmertemperatur stabil sind. Aus diesem Grund können Tieraugen normalerweise keine Infrarot- oder Wärmestrahlung sehen. 2013 entdeckten Forscher jedoch, dass der Buntbarsch "Pelvicachromis taeniatus" im Nah-Infrarotbereich sehen kann. Es gibt außerdem andere Sinnesorgane für Infrarotstrahlung, wie das Grubenorgan bei Schlangen.

Photonen können Schwingungen von Molekülen, oder im Kristallgitter eines Festkörpers anregen. Diese Schwingungen machen sich im Material als thermische Energie bemerkbar. Zusätzliche durch elektromagnetische Wellen angeregte Schwingungen erhöhen die Temperatur. Das Material wird erwärmt. Anders als bei chemischen Bindungen gibt es für die Schwingungen keine grundsätzliche Schwelle an Energie, die Photonen überschreiten müssen. Daher kann auch langwellige Wärmestrahlung einen Körper erhitzen. Durch Hitzedenaturierung kann langwellige elektromagnetische Strahlung auf indirekte Weise biologische Stoffe ändern.

Wie schnell sich Licht ungefähr ausbreitet, war seit 1676 bekannt. Allerdings fehlte bis 1865 jeder Zusammenhang zu anderen physikalischen Erscheinungen. Diesen konnte James Clerk Maxwell in den Jahren 1861 bis 1862 durch die von ihm gefundenen Maxwell-Gleichungen herstellen, welche die Existenz elektromagnetischer Wellen vorhersagen. Deren Geschwindigkeit stimmte mit der damals bekannten Lichtgeschwindigkeit so gut überein, dass sofort ein Zusammenhang hergestellt wurde. Diese Wellen konnte Heinrich Hertz in den 1880er-Jahren experimentell nachweisen.

In der klassischen Mechanik werden Wellen (in Ausbreitungsrichtung formula_13) durch die Wellengleichung
beschrieben. Hierbei bezeichnet formula_15 die Auslenkung der Welle und formula_1 ihre Phasengeschwindigkeit, die hier als Ausbreitungsgeschwindigkeit der Welle interpretiert werden kann.

Aus den Maxwellgleichungen lässt sich im Vakuum für die elektrische Feldstärke formula_17 die Beziehung:
herleiten (in SI-Einheiten; siehe Abschnitt Mathematische Beschreibung). Die elektrische Feldstärke verhält sich in dieser Beziehung also wie eine Welle; die Größe
tritt als Ausbreitungsgeschwindigkeit auf. Diese Geschwindigkeit formula_1 ist ausschließlich aus Naturkonstanten zusammengesetzt, die unabhängig vom Bezugssystem des Betrachters sind, was sich folglich auf die Größe formula_1 überträgt.
Grundlage der klassischen Mechanik ist das galileische Relativitätsprinzip, das besagt, dass die Naturgesetze in allen Inertialsystemen – solchen Bezugssystemen, in denen Körper, auf die keine Kraft wirkt, sich geradlinig fortbewegen – dieselbe Form haben ("Galilei-Invarianz"). Ein sich zu einem Inertialsystem mit konstanter Geschwindigkeit fortbewegendes Bezugssystem ist ebenfalls ein Inertialsystem.

Nach diesem Relativitätsprinzip wäre nun zu erwarten, dass ein Beobachter, der sich mit einer konstanten Geschwindigkeit relativ zur elektromagnetischen Welle bewegt, eine unterschiedliche Ausbreitungsgeschwindigkeit misst, wie etwa ein mit konstanter Geschwindigkeit laufender Spaziergänger am Rande eines Teiches eine andere Ausbreitungsgeschwindigkeit einer Wasserwelle auf dem Teich feststellen würde als ein ruhender Beobachter. Die Maxwellgleichungen sagen aber für beide Beobachter die gleiche Ausbreitungsgeschwindigkeit voraus – sie sind nicht Galilei-invariant.

Dieser Widerspruch zur klassischen Mechanik löst sich zu Gunsten der Maxwellgleichungen auf: Die Tatsache, dass sich elektromagnetische Wellen in allen Inertialsystemen mit der gleichen Geschwindigkeit ausbreiten – die vielzitierte "Konstanz der Lichtgeschwindigkeit" – bildet ein Postulat in Einsteins 1905 veröffentlichter spezieller Relativitätstheorie. An Stelle der Galilei-Invarianz tritt die sogenannte "Lorentz-Invarianz".

Die elektromagnetische Wellengleichung ergibt sich direkt aus den Maxwellgleichungen sowie der Divergenzfreiheit elektromagnetischer Wellen und lautet im Vakuum

Betrachtet man die Ausbreitung elektromagnetischer Wellen in polarisierbaren Medien, so muss zusätzlich die Polarisation formula_23 betrachtet werden:

Die zur Wellenausbreitung gehörigen mathematischen Beziehungen lassen sich auf Basis der maxwellschen Gleichungen nachvollziehen. Insbesondere lässt sich dieselbe Form der Wellengleichung herleiten, mit der sich andere Arten von Wellen, beispielsweise Schallwellen, ausbreiten.

Im Vakuum, also im ladungsfreien Raum unter Ausschluss von dielektrischen, dia- und paramagnetischen Effekten sind die Materialgleichungen der Elektrodynamik formula_25 und formula_26. Außerdem sind die Stromdichte formula_27 und Ladungsdichte formula_28 null.

Ausgehend von der dritten maxwellschen Gleichung

wendet man auf beide Seiten den Rotationsoperator an. Dadurch erhält man:

Setzt man darin die vierte maxwellsche Gleichung (mit formula_30) ein,
ergibt sich

Dazu gilt ganz allgemein die vektoranalytische Beziehung
Dabei ist mit formula_33 die Anwendung des vektoriellen Laplace-Operators auf das Vektorfeld formula_34 gemeint. In kartesischen Koordinaten wirkt der vektorielle Laplace-Operator wie der skalare Laplace-Operator formula_35 auf jede Komponente von formula_34.

Wendet man diese Beziehung auf formula_17 an und berücksichtigt, dass der ladungsfreie Raum betrachtet wird, in dem nach der ersten maxwellschen Gleichung die Divergenz von formula_38 null beträgt, so folgt:
Setzt man nun (2) und (3) zusammen, ergibt sich folgende Wellengleichung:

Fast alle Wellen lassen sich durch Gleichungen der Form
beschreiben, wobei formula_40 die Ausbreitungsgeschwindigkeit der Welle ist.
Für die Ausbreitungsgeschwindigkeit elektromagnetischer Wellen, die Lichtgeschwindigkeit formula_1, gilt daher:

Damit erhält man aus (4) die Gleichung

Analog kann man für die magnetische Flussdichte formula_44 die Beziehung
herleiten. Die Lösungen dieser Gleichungen beschreiben Wellen, die sich im Vakuum mit Lichtgeschwindigkeit formula_1 ausbreiten. Breitet sich die elektromagnetische Welle in isotropem Material mit der Dielektrizitätskonstante formula_10 und der Permeabilität formula_11 aus, beträgt die Ausbreitungsgeschwindigkeit

Darin sind aber im Allgemeinen die Materialkonstanten nicht linear, sondern können von der Feldstärke oder der Frequenz abhängen. Während Licht sich in der Luft fast mit Vakuumlichtgeschwindigkeit formula_1 ausbreitet (die Materialkonstanten sind in guter Näherung 1), gilt das für die Ausbreitung in Wasser nicht, was unter anderem den Tscherenkow-Effekt ermöglicht.

Das Verhältnis der Vakuumlichtgeschwindigkeit zur Geschwindigkeit im Medium wird als Brechungsindex formula_51 bezeichnet.
wo formula_53 und formula_54 die relative Permeabilität und die relative Permittivität des Mediums bezeichnen.

Mit Hilfe der Maxwellgleichungen lassen sich aus der Wellengleichung noch weitere Schlüsse ziehen. Betrachten wir eine allgemeine ebene Welle für das elektrische Feld
wo formula_56 die (konstante) Amplitude ist, formula_6 eine beliebige C-Funktion, formula_58 ein Einheitsvektor, der in Propagationsrichtung zeigt, und formula_59 ein Ortsvektor. Zunächst sieht man durch Einsetzen in die Wellengleichung, dass formula_60 die Wellengleichung erfüllt, dass also

Damit formula_62 nun eine elektromagnetische Welle beschreibt, muss es aber nicht nur die Wellengleichung erfüllen, sondern auch die Maxwellgleichungen. Das bedeutet
Das elektrische Feld steht also stets senkrecht zur Propagationsrichtung, es handelt sich also um eine Transversalwelle.
Einsetzen von formula_62 in eine weitere Maxwellgleichung ergibt
Die magnetische Flussdichte in der elektromagnetischen Welle steht also ebenfalls senkrecht zur Propagationsrichtung und auch senkrecht zum elektrischen Feld. Außerdem stehen ihre Amplituden in einem festen Verhältnis. Ihr Quotient ist die Lichtgeschwindigkeit formula_1
In natürlichen Einheiten (formula_70) haben beide Amplituden den gleichen Wert.

Mit dieser Beziehung lässt sich eine Aussage über die Energiedichte
des elektromagnetischen Felds für den Fall der elektromagnetischen Welle herleiten:

Nicht jede elektromagnetische Welle hat die Eigenschaft, dass ihre Ausbreitungsrichtung sowie die Richtungen des elektrischen als auch des magnetischen Feldes paarweise orthogonal zueinander sind, die Welle also eine reine Transversalwelle ist, auch TEM-Welle genannt. Die hier demonstrierten ebenen Wellen sind von diesem Typ, daneben existieren aber auch Wellen, in denen nur einer der beiden Feldvektoren senkrecht auf der Ausbreitungsrichtung steht, der andere aber eine Komponente in Ausbreitungsrichtung hat
(TM- und TE-Wellen). Ein wichtiger Anwendungsfall für solche nicht rein transversale elektromagnetische Wellen sind zylindrische Wellenleiter. Das Gesagte gilt aber vor allem in Kristallen mit Doppelbrechung. Allerdings gibt es keine rein longitudinalen elektromagnetischen Wellen.




</doc>
<doc id="1462" url="https://de.wikipedia.org/wiki?curid=1462" title="Exobiologie">
Exobiologie

Die Exobiologie (aus ‚außer, außen‘ und Biologie) ist eine interdisziplinäre Naturwissenschaft, welche die Möglichkeit der Entstehung und Existenz von außerirdischem Leben erforscht und sich mit der allgemeinen Frage von Leben im All beschäftigt (siehe auch: Panspermie).

Die Bezeichnung "Exobiologie" wurde vor allem in der Frühzeit der Raumfahrt für den heute als Astrobiologie bezeichneten Begriff verwendet, gilt aber heute nur noch für einen Teilbereich der Astrobiologie. Sie wurde von dem Biologen Joshua Lederberg in den 1960er Jahren geprägt und wird oft von einem biologischen Standpunkt aus benutzt. Die ESA benutzt diese Bezeichnung bevorzugt, und auch die NASA betreibt ein Exobiologie-Programm. Eine weitere Bezeichnung, "Kosmobiologie", stammt von dem Physiker John Desmond Bernal, wird aber selten benutzt.

Nach Untersteiner (2006) ist die Exobiologie jener interdisziplinäre Wissenschaftszweig, der den Ursprung, die Verbreitung und die Evolution des Lebens im Universum untersucht. Das NASA Astrobiology Institute (NAI) definiert Exobiologie schlicht als „das Studium des lebenden Universums“. Damit schließt dieser Wissenschaftsbereich auch Fragen zur weiteren Evolution des irdischen Lebens und seiner möglichen Verbreitung im Universum mit ein. In diesem Rahmen
hat der deutsche Physiker Claudius Gros vorgeschlagen
dass die Technologie der Breakthrough Starshot Initiative
benutzt werden könnte, um eine Biosphäre einzelliger Lebewesen
auf einem ansonsten nur zeitweilig bewohnbaren Exoplaneten zu etablieren.
Die Exobiologie versucht auf drei Arten Leben außerhalb der Erde nachzuweisen.






</doc>
<doc id="1464" url="https://de.wikipedia.org/wiki?curid=1464" title="Elefanten">
Elefanten

Die Elefanten (Elephantidae) (, Stamm von ἐλέφᾱς "eléphās" ‚Elefant‘) bilden eine Familie der Rüsseltiere. Diese Familie umfasst alle heute noch lebenden Vertreter der Rüsseltiere.

Elefanten sind die größten noch lebenden Landtiere. Signifikante Merkmale sind neben dem Rüssel vor allem bei erwachsenen Tieren die beiden Stoßzähne des Oberkiefers und der horizontale Zahnwechsel. Je nach Art kann ein Elefant im Durchschnitt zwischen 2 und 6,6 Tonnen Körpergewicht und eine Schulterhöhe von 2,1 bis 3,7 m erreichen. Das größte wissenschaftlich vermessene Exemplar, ein Tier aus Angola, besaß eine Schulterhöhe von 4 m und wog rund 10 Tonnen. Der älteste Elefant lebte im Zoo von Taipeh und wurde 86 Jahre alt. Die Tragzeit ist mit 20 bis 22 Monaten die längste aller Landsäugetiere. Bei der Geburt wiegt ein Kalb bis zu 100 Kilogramm.

Von den Rüsseltieren leben heute nur noch drei Arten, die alle zu den echten Elefanten gehören. Diese sind:


Eine Minderheit von Elefantenforschern und Kryptozoologen hat den Zwergelefanten ("Loxodonta pumilio") als Art vorgeschlagen; diese ist jedoch in der Fachwelt nicht akzeptiert. Er soll neben dem großen Waldelefanten als kleinere Art im tropischen Regenwald (Gabun, Kongo, Kamerun) vorkommen. Auch genetische Untersuchungen an verschiedenen Exemplaren der zentralafrikanischen Region brachten keine Hinweise auf eine vierte Elefantenart.

Die einzig bekannte Hybride zwischen einer asiatischen Elefantenkuh und einem afrikanischen Elefantenbullen wurde 1978 im Zoo von Chester geboren. Es handelte sich um das Bullenkalb „Motty“. Es starb zwei Wochen nach seiner Geburt.

Unter den heute noch lebenden Tieren sind die Seekühe die nächsten Verwandten der Elefanten.

Das Verbreitungsgebiet des Asiatischen Elefanten erstreckte sich früher im südlichen Asien durchgängig von Syrien bis zu weiten Teilen Chinas. Heute ist er noch in Vorder- und Hinterindien, Sri Lanka sowie einigen der großen Sundainseln zu finden.

Der Afrikanische Elefant (= Steppenelefant) lebte früher auf dem gesamten afrikanischen Kontinent, heute befindet sich die nördlichste Grenze seines Verbreitungsgebietes im Süden des Sudans. Er kommt heute in vier getrennten Populationen vor: in den Savannen des östlichen und südlichen Afrika, in Westafrika, in der nördlichen Namib (Südwestafrika) und im zentralafrikanischen tropischen Regenwald. Im südlichen Afrika ist er vor allem in Nationalparks zu finden.

Der Waldelefant lebt in den Regenwäldern Westafrikas, unter anderem in Kamerun, der Demokratischen Republik Kongo und der Zentralafrikanischen Republik.

Alle heute noch lebenden Elefanten sind stark in ihrem Bestand gefährdet, da ihr Lebensraum beständig schrumpft und sie bis in die jüngste Zeit wegen ihrer aus wertvollem Elfenbein bestehenden Stoßzähne gejagt wurden.

Anfang des 20. Jahrhunderts soll es in Afrika mehrere Millionen Elefanten gegeben haben. Der Bestand hat sich nach Daten des Kenya Wildlife Service Research Center in Tsavo-East von 1,2 Millionen im Jahre 1981 auf etwa 0,6 Millionen 1997 reduziert. Nach groben Schätzungen der Weltnaturschutzunion IUCN gab es 2012 zwischen 423.000 und 660.000 Afrikanische Elefanten. Der Bestand des Asiatischen Elefanten soll sich laut WWF von etwa 160.000 Tieren Mitte des 20. Jahrhunderts über 34.000 bis 53.700 im Jahre 1990 auf schätzungsweise 25.600 bis 32.750 (zuzüglich etwa 15.000 Tiere in Gefangenschaft) im Jahre 2006 reduziert haben.

Da die verschiedenen Elefantenarten eine unterschiedliche Anzahl Brust-, Lenden-, Kreuzbein-, und Schwanzwirbel besitzen, besteht ein Elefantenskelett aus 326 bis 351 Knochen.

Etwa 394 Skelettmuskeln bewegen den Elefantenkörper. Die inneren Organe des Elefanten sind im Verhältnis nicht größer als bei anderen Säugetieren. Das Gehirn wiegt etwa vier bis fünf Kilogramm, das Herz – je nach Alter – zwischen zwölf und einundzwanzig Kilogramm. Es schlägt etwa 30 Mal pro Minute. Die Blutmenge entspricht etwa 10 Prozent des Körpergewichtes. Die Körpertemperatur eines gesunden Elefanten beträgt circa 36,5 °C. Die Haut ist etwa zwei Zentimeter dick.

Im Gegensatz zu den meisten anderen Säugetieren befindet sich das Gesäuge bei den Elefantenkühen wie bei den Primaten und Seekühen zwischen den Vorderextremitäten. Die Geburtsöffnung der Elefantenkuh liegt nicht unter dem Schwanzansatz, sondern zwischen den Hinterbeinen. Durch diese anatomische Lage beträgt die Fallhöhe des Kalbes bei der Geburt nur rund 70 Zentimeter statt 1,70 Meter.

Während seit mindestens der Spätantike die Ansicht vertreten war, Elefanten hätten keine Ellenbogen- und Kniegelenke, ist mittlerweile bekannt, dass diese lediglich völlig frei sind. Das Kniegelenk des Elefanten zeigt eine erweiterte Ruheposition, sodass beim Stehen der Winkel zwischen Oberschenkel und Schienbein fast 180° beträgt. Dies ist für Quadrupeden ungewöhnlich und nur beim bipeden Menschen anzutreffen. Es gibt weitere Parallelen: Das Oberschenkelgelenk des Elefanten zeigt große Ähnlichkeit mit dem des Menschen. Die Menisken sind sehr schmal und dünn und das Kreuzbandsystem ist ebenfalls vorhanden. Die Bewegungsmuster der gewichtstragenden hinteren Gliedmaßen erinnern ebenfalls mehr an den Menschen als an cursoriale Quadrupeden. Die Hauptbewegung des Kniegelenks ist eine Extension-Flexion mit einem Aktionsradius von 142°. Im fortgeschrittenen Alter sind die Kniegelenke anfällig für Arthrose.

Elefanten sind Zehenspitzengänger. Sie haben zur Unterstützung einen sechsten „Zeh“ an der Hinterseite des Fußes. Unter der Ferse befindet sich ein Polster aus fetthaltigem Bindegewebe, welches das Körpergewicht gleichmäßig auf die breite behornte Fußsohle verteilt. 

Eine weitere anatomische Besonderheit der Elefanten betrifft ihren Pleuraspalt. Sie sind die einzigen Säugetiere, bei denen dieser durch lockeres Bindegewebe überbrückt ist. Dadurch sind die Pleurablätter trotzdem weiterhin gegeneinander verschiebbar, aber bei Weitem nicht so empfindlich. Dies ermöglicht es Elefanten beispielsweise, einen Fluss zu durchqueren und währenddessen mit ihrem langen Rüssel zu „schnorcheln“. Dabei atmen sie Luft mit atmosphärischem Druck ein, während sich ihr Körper, und damit insbesondere die Lunge, etwa 2 Meter unter Wasser befindet. Diese Druckdifferenz würde bei jedem anderen Säugetier (mit „normalem“ Pleuraspalt) dazu führen, dass die Blutgefäße, die das Wandblatt der Pleura versorgen, förmlich „ausgequetscht“ und zerstört würden.

Elefanten besitzen zwei Arten von Zähnen: Die zu Stoßzähnen gewandelten, hypertrophierten mittleren Oberkieferschneidezähne (I) und die Backenzähne. Die Backenzähne sind hochkronig ("hypsodont") und weisen relativ engständige, lamellenartige Schmelzfalten auf. Dieser Zahnbau wird entsprechend als "lamellodont" bezeichnet. Am größten ist der jeweils letzte Backenzahn, der bis zu 5 kg wiegen kann und beim Afrikanischen Elefanten bis zu 13, beim Asiatischen bis zu 24 Schmelzlamellen besitzt. Das ausgestorbene Wollhaarmammut ("Mammuthus primigenius") hatte als am stärksten spezialisierte Elefantenart Backenzähne mit bis zu 30 Schmelzlamellen. Zwischen den Stoßzähnen und den Backenzähnen befindet sich ein zahnfreier Bereich, der allgemein als Diastem bezeichnet wird. Ein solches Diastem ist typisch für das Gebiss pflanzenfressender Säugetiere. Über das ganze Leben hinweg betrachtet, verfügt ein Elefant über 6 Backenzähne pro Kieferhälfte: 3 Prämolaren und 3 Molaren, wobei die Prämolaren den Milchbackenzähnen entsprechen, die Molaren den auch bei anderen Säugetieren üblichen hinteren Backenzähnen (das Gebiss ausgewachsener Elefanten hat demzufolge keine Prämolaren mehr), also insgesamt 24 Backenzähne.

Da die Kiefer der Elefanten relativ kurz und die Backenzähne relativ groß sind, trägt eine Kieferhälfte immer nur zwei bis drei Backenzähne gleichzeitig, wobei aber nur ein Teil durchgebrochen, also sichtbar ist. Die Kaufläche wird stets nur von dem Backenzahn oder den Backenzähnen gebildet, die sich nahe dem Diastem befinden (also im vorderen Bereich des Kiefers). Ausgewachsene Elefanten haben so maximal anderthalb Backenzähne je Kieferast gleichzeitig in Funktion. Beim Kauen bzw. Zermahlen der relativ harten Pflanzennahrung nutzen sich die Zähne stark ab. Um zu gewährleisten, dass die Mahlleistung stets gleich bleibt, wandert vom hinteren Ende des Kiefers kontinuierlich, wie auf einem sehr langsamen Fließband, „frisches“ Zahnmaterial zum Diastem hin. Diese Wanderung wird durch Resorption und Neuaufbau von Kieferknochensubstanz ermöglicht. Bei den stark abgenutzten Zahnteilen unmittelbar am Diastem wird die Wurzel resorbiert, sodass sie absterben, keinen Halt mehr im Kiefer haben und schließlich abbrechen. Nachdem die ersten drei Backenzähne des Jugendstadiums ausgefallen sind, erfolgt der vollständige Durchbruch des vierten im Alter von etwa 10 bis 14 Jahren, der des fünften mit 26 bis 27 Jahren und des sechsten und letzten mit 34 bis 37 Jahren (jeweils gerechnet auf das Lebensalter eines Afrikanischen Elefanten). Wenn ein Elefant noch zu Lebzeiten alle seine 24 Backenzähne verschlissen hat, muss er verhungern (für in Gefangenschaft gehaltene Tiere wird gelegentlich Zahnersatz angefertigt). Diese spezielle Form der Erneuerung von Zahnsubstanz wird "horizontaler Zahnwechsel" genannt und kommt fast ausnahmslos bei Elefanten vor. Sie hat sich innerhalb der Rüsseltiere schon stammesgeschichtlich sehr früh entwickelt und ist erstmals bei der Gattung "Eritreum" im Oberen Oligozän vor rund 27 Millionen Jahren nachgewiesen.

Die aus Elfenbein bestehenden Stoßzähne werden vor allem zum Entrinden der Bäume sowie als Waffe gegen Feinde eingesetzt, wobei die Stoßzähne mehr dem Imponiergehabe als dem wirklichen Kampf dienen.

Ein Rüssel ist eine verlängerte Nase mit Nasenlöchern (Rüsselloch). Bei Elefanten ist der Rüssel ein äußerst feinfühliges und langes Organ, das im Lauf der Phylogenese aus Oberlippe und Nase entstand. Etwa 40.000 zu Bündeln verflochtene Muskeln machen den Rüssel sehr beweglich. Der Rüssel enthält kein Nasenbein oder andere Knochen. Er besteht ausschließlich aus Muskelgewebe und ist das auffälligste anatomische Merkmal der Elefanten.

Der Rüssel ist ein Multifunktionsorgan, welches als Tast- und Greiforgan, zur Atmung und Geruchswahrnehmung sowie als Waffe und Drohmittel und als Saug- und Druckpumpe beim Trinken dient. Während der Rüssel des afrikanischen Elefanten in zwei fingerartigen Fortsätzen endet, ist es beim indischen Elefanten nur einer.

An seiner Spitze befinden sich empfindliche Tasthaare, die kleinste Unebenheiten wahrnehmen. Dadurch eignet sich der Rüssel auch zum Tasten. Bei der Kontaktaufnahme zu Artgenossen in der Herde wird der Rüssel eingesetzt: gegenseitiges Umschlingen der Rüssel als Liebes- und Freundschaftszeichen und beim Spiel. Mit dem Rüssel werden Staub und Schmutz auf der Haut verteilt, was zum Schutz vor der starken Sonneneinstrahlung und vor Insekten geschieht. Der Rüssel wird zum Greifen von Gegenständen benutzt, beispielsweise, um sie zum Mund zu führen. Ausgebildete Arbeitselefanten können in Zusammenarbeit mit dem Elefantenführer Gegenstände von erheblichem Gewicht mit Hilfe des Rüssels – mit Unterstützung der Stoßzähne – manipulieren, heben und bewegen. Mit Hilfe des Rüssels kann ein Elefant Äste und Pflanzen aus bis zu sieben Meter Höhe erreichen. Ähnlich einem Giraffenhals verdoppelt er damit seine Streckhöhe.

Gelegentlich wird der Elefantenrüssel beim Baden oder Schwimmen als eine Art Schnorchel eingesetzt, zum Riechen wird er hoch in die Luft gehalten.

Durch Heben des Kopfes und des ausgestreckten Rüssels 30 bis 40 Grad über die Horizontale nimmt ein Elefant eine dominante Haltung ein. Der Rüssel kann ebenso als Schlagwaffe dienen, indem er beispielsweise eingerollt (ähnlich einem aufgewickelten Schlauch, zur Innenseite hin – Richtung Mund) und dann zum Schlag mit viel Kraft wieder gestreckt wird; seitliche Hiebe mit dem schwingenden Rüssel sind sehr kraftvoll.

Es passen pro Zug zirka 8 bis 10 Liter Wasser in den Rüssel. Mit Hilfe seines Rüssels kann ein Elefantenbulle in 5 Minuten 200 Liter Wasser trinken.

Das Gehirn eines heutigen Elefanten hat 257 Milliarden Nervenzellen, etwa die 3-fache Menge des menschlichen. In der Großhirnrinde sind davon aber nur 5,6 Milliarden; dort hat der Mensch das Dreifache an Zellen.

In den Zellkernen des Afrikanischen Elefanten befindet sich ein doppelter Chromosomensatz mit zweimal 27 Autosomen und zwei Geschlechtschromosomen. Das Genom wurde erstmals im Jahre 2009 vollständig sequenziert. Das Referenzgenom besteht aus 3.196.743.967 Basenpaaren. Die genaue Zahl der Gene ist noch unbekannt.

Elefanten sind ausnahmslos Pflanzenfresser. Sie nehmen täglich etwa 200 Kilogramm Nahrung zu sich. Dazu brauchen sie 17 Stunden am Tag. Sie fressen vor allem Gras und Blätter, gefolgt von Früchten, Wurzeln, Zweigen und Rinde. Der Rüssel ist bei der Nahrungsaufnahme als Greiforgan enorm nützlich. Mit den fingerartigen Fortsätzen können sie einzelne Halme und Gräser aufnehmen. Ihre Nahrung verwerten sie zu etwa 40 Prozent, da sie ein weniger effizientes Verdauungssystem haben als etwa die Wiederkäuer. Wasser ist für die Elefanten ein wichtiger Lebensfaktor. Sie trinken 70 bis 150 Liter Wasser am Tag. Ihre Harnblase fasst dabei etwa 18 Liter. Täglich benötigen Elefanten etwa 250.000 Kilokalorien.

Der Elefantenmagen fungiert primär als Reservoir für die Nahrung, die in dem sauren Milieu bei einem pH-Wert von ca. 2 vorverdaut wird. Der wesentliche Teil der Verdauung findet erst nach Passage des Magens im Blinddarm und Colon durch Mikroorganismen (Bakterien und Protozoen) statt. Ähnlich wie bei Equidae kann der Kot teilweise wieder aufgenommen werden, damit die enthaltenen Nahrungsstoffe besser genutzt werden.

Durch ihre Größe und ihr Leben im Herdenverband haben Elefanten wenige natürliche Feinde. Nur den größten Raubkatzen wie Löwen und Tigern gelingt es bisweilen, Jungtiere zu erbeuten. In einigen Gebieten Afrikas scheinen Elefanten häufiger von Löwen erbeutet zu werden als bisher angenommen. Im Eiszeitalter hatten Elefanten darüber hinaus noch die mittlerweile ausgestorbenen Säbelzahnkatzen zu fürchten. Insbesondere für die Gattung "Homotherium" konnte zumindest lokal eine Nahrungspräferenz für junge Präriemammuts nachgewiesen werden.

Elefanten leben in Herden, bestehend aus Kühen und Kälbern. Die Herden werden von einer Leitkuh angeführt. Dabei handelt es sich meist um eine ältere, mittlerweile unfruchtbar gewordene Kuh im Alter zwischen 40 und 50 Jahren. Fehlen Vorbilder wie diese Leitkuh, etwa weil sie getötet wurden, so kann dies schwerwiegende Folgen haben: Die Kälber werden von den verbliebenen jungen Müttern aufgezogen, und daher werden viele Verhaltensweisen nicht vermittelt.

Die Größe der Herde variiert mit dem Nahrungsangebot. Ist reichlich Nahrung vorhanden, so schließen sich kleinere Familienherden zu größeren zusammen. Bei akutem Nahrungsmangel kommt es oft vor, dass einzelne Familien – bestehend aus ein bis zwei Kühen mit ihren Nachkommen – „Miniherden“ bilden und allein unterwegs sind. Ebenfalls von der Nahrung abhängig ist das Wanderverhalten der Herden. Während sie bei hohem Nahrungsangebot relativ ortsbeständig sind, bewegen sie sich in Trockengebieten mit schlechter Nahrungsversorgung oft über lange Strecken, um Nahrung zu finden. Dabei bilden sich sogenannte „Elefantenstraßen“, die bereits lange Zeit bestehen und immer wieder benutzt werden.

Im Alter von etwa zwölf Jahren trennen sich Bullen von den übrigen Herdentieren. Nur zur Brunftzeit stoßen sie zu den Herden, um sich zu paaren. Außerhalb der Paarungszeit bestreiten sie ihr Leben entweder als Einzelgänger oder in losen Gruppen. In der Vergangenheit kam es zumindest in Afrika mehrfach vor, dass solche Gruppen als aggressiv auffielen, indem sie beispielsweise Nashörner töteten. Ursache dieses Verhaltens könnte eine Art posttraumatische Belastungsstörung durch Zusammenstöße mit Wilderern sein. Die Gehirnstruktur solcher Elefanten weist Parallelen zur Gehirnstruktur traumatisierter Menschen auf.

Elefantenbullen erleben periodisch eine Verhaltensänderung, deren Dauer stark variieren kann und die als Musth bezeichnet wird.

Durch eine Versuchsreihe im New Yorker Bronx-Zoo ließen sich Anzeichen ermitteln, die nahelegen, dass Elefanten über ein Ich-Bewusstsein verfügen. Asiatische Elefanten wurden hierfür einem Spiegel-Selbsterkennungstest unterzogen. Die Ergebnisse zeigten, dass Elefanten wie Elstern, Delfine und Menschenaffen dem Anschein nach die Fähigkeit besitzen, sich selbst im Spiegel zu erkennen. Dies deutet auf das Vorhandensein eines Ichbewusstseins hin. Außerdem konnte nachgewiesen werden, dass Elefanten in der Lage sind, zu zählen und einfachste Additionsaufgaben zu lösen. Wahrscheinlich erlaubt ihnen diese Fähigkeit, die Vollständigkeit der Herde zu überprüfen.

Elefanten sind in der Lage, Stoßzähne und Knochen von toten Artgenossen zu erkennen. Dies ergab eine Studie im Amboseli-Nationalpark, die von der University of Sussex durchgeführt wurde. Sie scheinen sich auch für den Verbleib von toten Artgenossen zu interessieren. So suchen sie Dörfer auf, deren Bewohner einen Elefanten ihrer Herde getötet haben.

Entgegen der weitverbreiteten Ansicht kommunizieren Elefanten überwiegend nicht durch Trompetenlaute. Diese erzeugen sie nur in bestimmten Stimmungslagen (z. B. Aufregung, Angst, Aggressivität). Zur Verständigung mit Artgenossen nutzen sie zu ⅔ Infraschall-Laute. Diese für Menschen unhörbaren Schwingungen werden durch die Luft und durch das Erdreich über dutzende Kilometer übertragen. Auf diese Weise können die Tiere mit weit entfernten Artgenossen kommunizieren. Elefanten erkennen an der Stimme, ob Menschen ihnen gefährlich werden können. Verzerrte und verfremdete Stimmen erkennen sie.

Stammesgeschichtlich sind Elefanten eine relativ junge Familie der Rüsseltiere (Proboscidea). Die frühesten Rüsseltiere stammen aus dem Paläozän Nordafrikas und sind etwa 60 Millionen Jahre alt. Die Rüsseltiere bildeten verschiedene Familien aus, dazu zählen die Moeritherien, die Deinotherien, die Barytherien, die Gomphotherien, die Mammutiden und die Stegodonten.

Diese ausgestorbenen Rüsseltiere waren keine Elefanten, obwohl sie teilweise ganz ähnlich aussahen. Einige dieser Familien wie die Stegodonten, die Gomphotherien und die Mammutiden waren bis ins Eiszeitalter Zeitgenossen der Elefanten. Die Elefanten entwickelten sich aus der Familie der Stegodonten. Die Entwicklungslinie zu den Elefanten begann im späten Miozän vor etwa 7 Millionen Jahren durch Gattungen wie "Primelephas" und "Stegotetrabelodon", die sich durch die fehlende Zahnschmelzhülle um ihre vier Stoßzähne und das Vorhandensein von Schmelzlamellen auf den Backenzähnen von anderen Rüsseltieren unterschieden. Die Schmelzlamellen gelten als besonders charakteristisches Merkmal für Angehörige der Elephantidae und stellen eine Anpassung an Grasnahrung dar. Eine andere Rüsseltierfamilie, die Stegodonten, entwickelte unabhängig ähnliche Strukturen, die den Schmelzlamellen der Elefanten entsprachen.

Auf "Primelephas" gehen wohl die späteren Gattungen der Elefanten, "Elephas, Loxodonta" und "Mammuthus" zurück. Das vereinfachte Kladogramm der Familie sieht so aus:

Die Gattung "Elephas" entstand im späten Miozän Afrikas, molekulargenetischen Analysen zufolge spalteten sich "Elephas" und "Loxodonta" vor 7,6 Millionen Jahren auf, und verbreitete sich von dort aus über Eurasien. Die früheste bisher bekannte Art mit einem Alter von rund 6,5 Millionen Jahren ist "Elephas nawataensis". Heute lebt als einzige Art der Indische Elefant ("Elephas maximus"). Weitere mit dem heutigen Asiatischen Elefanten verwandte Formen finden sich in "Elephas planifrons" und "Elephas platycephalus". Mit dem Ende des Pleistozäns blieb "Elephas" ausschließlich auf Asien beschränkt. 

Die zweite noch existierende Gattung "Loxodonta" entstand ebenfalls im ausgehenden Miozän Afrikas und ist gegenwärtig durch zwei Arten, den Afrikanischen Steppenelefanten ("Loxodonta africana") und den Waldelefanten ("Loxodonta cyclotis"), repräsentiert. Sie trat erstmals mit "Loxodonta cookei" vor etwa 7 Millionen Jahren in Erscheinung. Mit ihr nahe verwandt ist die Gattung "Palaeoloxodon", zu der auch der Europäische Waldelefant ("Palaeoloxodon antiquus") als einer der bekanntesten Vertreter gehört. Von ihm stammen die Zwergelefanten (beispielsweise der Sizilianische Zwergelefant) ab, die im Pleistozän auf einigen Inseln des Mittelmeeres und Südostasiens verbreitet waren, eine kleine Population überlebte noch bis um 1300 v. Chr. auf der Mittelmeerinsel Tilos.

Die bekanntesten fossilen Elefanten sind zweifellos die Mammuts ("Mammuthus" bzw. "Mammonteus"). Diese Gattung hatte sich laut molekulargenetischen Untersuchungen vor 6,7 Millionen Jahren von "Elephas" getrennt. Erste Nachweise stammen mit der frühpliozänen Art "Mammuthus subplanifrons" ebenfalls aus Afrika. Die ursprüngliche Form "Mammuthus meridionalis", auch unter dem Namen Südelefant bekannt, erreichte vor mindestens drei Millionen Jahren Eurasien und verbreitete sich von dort aus bis nach Nordamerika. Zu Beginn der Eiszeit lebte das Steppenmammut "Mammuthus (=Mammonteus) trogontherii," welches später durch das Kaltsteppen- oder Wollhaarmammut "Mammuthus (=Mammonteus) primigenius" abgelöst wurde. Dieses wird umgangssprachlich auch als Echtes Mammut bezeichnet. Teile von Wollhaarmammuts werden heute noch häufig im sibirischen Dauerfrostboden gefunden. Dabei handelt es sich vor allem um Knochen und Haare, gelegentlich finden sich jedoch auch erstaunlich gut erhaltene Mammuts im Eis. 

Die Menschen im späten Pleistozän jagten die Tiere, und nach verschiedenen Theorien waren sie möglicherweise für das Aussterben der letzten Mammuts verantwortlich. Alternative Theorien gehen von einer Abfolge vieler schwerer Winter und Nahrungsknappheit aus. In den meisten Gebieten Eurasiens und Nordamerikas verschwanden die Mammuts vor etwa 10.000 Jahren; die letzten lebten vor etwa 3700 Jahren auf der Wrangelinsel.

Die Beziehung des Menschen zum Elefanten ist einerseits durch eine jahrtausendealte Liebe, Verehrung und Bewunderung geprägt, andererseits wurden Größe, Schönheit und Kraft der Elefanten nicht nur genutzt, sondern auch ausgenutzt. Ein besonderes Problem stellt bis heute das wirtschaftliche Interesse, ja die Gier mancher Menschen nach dem wertvollen Elfenbein der Stoßzähne dar. Schon das Mammutelfenbein wurde von alters her vor allem in China und Russland zur Elfenbeinschnitzerei genutzt.

Die ältesten Darstellungen von Elefanten stammen aus dem 3. Jahrtausend v. Chr. aus der Ruinenstadt Mohenjodharo im heutigen Pakistan: Es handelt sich um kleine Siegel aus Speckstein, die darauf hindeuten, dass zu dieser Zeit bereits Elefanten gezähmt und möglicherweise als Nutz- und Arbeitstiere gehalten wurden.

Im antiken Ägypten waren Elefanten bekannt, spielten aber im Alltag keine Rolle; sehr begehrt war allerdings das Elfenbein der Stoßzähne. 3100 v. Chr. gab es noch kleine Elefantenpopulationen in Oberägypten, die jedoch vermutlich aufgrund von Klimaveränderungen immer weiter nach Süden abwanderten. Es ist bekannt, dass Thutmosis I. und Thutmosis III. wegen der Stoßzähne in Syrien auf Elefantenjagd gingen. Auch Assyrische Königsinschriften des 8. bis 7. Jahrhunderts v. Chr. berichten von der Elefantenjagd im heutigen Syrien, was entsprechende Knochenfunde belegen.

Eine besondere Rolle spielte und spielten Elefanten seit dem Altertum in Indien, wo sie aufgrund ihrer großen Kraft als Nutztier gehalten und zu Arbeitselefanten ausgebildet wurden. Man setzte sie auch als Kriegselefanten ein.

Die Griechen kannten zunächst nur das Elfenbein als Handelsobjekt. Erste Begegnung mit Elefanten hatten sie bei der Schlacht von Gaugamela. Später wurden in allen Diadochen-Heeren bis zu 500 Kriegselefanten gleichzeitig eingesetzt. Einer von ihnen, Pyrrhus, setzte sie bei seinen Kämpfen gegen Rom ein. Nach der Region, in der der erste Kampf stattfand, nannten die Römer die Elefanten anfänglich „lukanische Ochsen“ ("boves Lucas").

Der karthagische Feldherr Hannibal überquerte 218 v. Chr. mit Kriegselefanten die Alpen.

In Rom waren die ersten Elefanten im Triumphzug 275 v. Chr. zu sehen. Seit 169 v. Chr. wurden sie im Zirkus zur Schau gestellt. Im 1. Jahrhundert n. Chr. wurden Elefantenschaukämpfe gegen Tiere und Menschen Mode, die in der späteren Kaiserzeit durch artistische Darstellungen abgelöst wurden. Im Triumphzug mit vorgespannten Elefanten (Quadriga) scheiterte Pompeius 81 v. Chr. an den engen Stadttoren Roms. Dies sollte später Severus Alexander und Gordian I. gelingen.

Einige Elefanten sind namentlich in die europäische Geschichte eingegangen: Abul Abbas, ein Geschenk des Kalifen Harun ar-Raschid an Karl den Großen; im 16. Jahrhundert lebten Hanno, das Lieblingstier Papst Leos X., und Soliman, der erste Elefant in Wien; Hansken (1630–1655) war eine gelehrte Elefantendame des 17. Jahrhunderts, und Jumbo (ca. 1861–1885), der "König der Elefanten". Ludwig IX., "der Heilige," brachte 1255 nach einem Kreuzzug einen Elefanten nach Frankreich, den er an Heinrich III. von England weiterverschenkte. Ludwig XIV. von Frankreich hielt 13 Jahre lang ein seltenes afrikanisches Exemplar in Versailles.

In Indien und anderen asiatischen Ländern wurde und wird der Elefant sehr verehrt, man setzte ihn nicht nur als Nutz- und Arbeitstier ein, sondern auch bei religiösen Zeremonien und als Staatselefant. Zu diesem Zwecke wurde und wird er bemalt und mit kostbaren Stoffen geschmückt. Steinerne Elefantenskupturen zieren Tempel und Paläste. Die Mogulherrscher hatten hunderte, ja tausende erlesener Elefanten in ihren Ställen; die Tiere des Moghulkaisers waren in sieben Rangklassen geordnet, und besondere Verdienste wurden manchmal mit dem Geschenk eines Elefanten belohnt. Die Elefanten wurden auch in der Moghulzeit als Reittier bei der Jagd, und als Kriegselefanten verwendet. Es gab sogar Elefantenschaukämpfe, zu deren Zweck die Tiere künstlich mittels von Drogen aggressiv aufgeputscht wurden. In Asien gibt es noch heute Elefantenschulen, in denen Elefanten für die Arbeit oder für Touristen abgerichtet werden. In Surin (Thailand) wird jährlich das "größte Elefantenfest Asiens" veranstaltet, bei denen auch Elefantenrennen und -fußballspiele veranstaltet werden. Indische, thailändische und sri-lankische Elefantenführer werden Mahut (auch: Mahout) genannt.

Es gibt heute noch Menschen, die den Elefanten wegen seiner Stoßzähne illegal jagen. Asiatische Elefanten sind nach wie vor Nutztiere und werden in einigen Ländern beispielsweise zur Lastenbeförderung eingesetzt. Dabei kommt es jedoch häufig zu Unfällen, da Nutztier-Elefanten auch äußerst aggressiv werden können.

Wegen ihrer Intelligenz und beeindruckenden Größe sind Elefanten ferner als Dressurtiere im Zirkus verbreitet. In Zoos zählen die Dickhäuter zu den beliebtesten Attraktionen und ziehen jährlich ganze Scharen von Besuchern an. Die Haltung der großen Tiere ist allerdings problematisch und kann zu Unfällen zwischen Elefant und Mensch führen. Die "European Elephant Group", ein „Zusammenschluss von Menschen, denen das Schicksal von Elefanten, die als Folge nicht tiergerechter Haltung mittel- oder unmittelbar durch Schäden an ihrer physischen bzw. psychischen Gesundheit bedroht sind, am Herzen liegt“,
bezeichnet die grauen Riesen als die gefährlichsten Wildtiere in Menschenhand, da es immer wieder zu Unfällen mit dem Pflegepersonal kommt. Sie berichtet allein von 51 Toten und mehr als 100 Verletzten in Zoos und Safariparks seit 1980. Elefanten können zahlreiche Verhaltensstörungen entwickeln, von denen das rhythmische Hin- und Herbewegen (Weben) vielleicht die bekannteste ist.

In freier Wildbahn kommt es, insbesondere durch die zunehmende Einengung der Lebensräume des Elefanten, immer wieder zu Konflikten zwischen Elefant und Mensch, die durch die „Human-Elephant Conflict“-Statistik (HEC) erfasst werden. Elefanten leben in freier Wildbahn in 37 afrikanischen und 12 asiatischen Staaten. Nach Schätzungen werden weltweit jährlich über 500 Menschen durch Elefanten getötet, davon allein 300 in Indien. Gleichzeitig werden jedoch tausende Elefanten durch Menschen getötet – vielfach durch Bauern, die ihre Felderträge schützen wollen, oder als Vergeltung für menschliche Todesopfer, und in zunehmendem Maße auch durch Wilderei. Im Jahr 2009 wurde die Zahl der für den Elfenbeinhandel in Afrika gewilderten Elefanten auf 38.000 Tiere pro Jahr geschätzt. Darüber hinaus werden Elefanten durch die Auswirkungen menschlicher Auseinandersetzungen, etwa durch Landminen in Sri Lanka, getötet.

Der Elefant gilt als weise, stark und keusch, seltener auch als böse. Er ist auch ein Symbol für die Mäßigkeit. 
Sowohl in Afrika (z. B. bei den Ashantis) als auch in Indien galt der Elefant als königliches Tier, und er wurde oft besungen, wie in dem folgenden indischen Text:

Der Elefant ist das Wappentier mehrerer asiatischer und afrikanischer Staaten sowie das politische Wappentier der Republikaner in den USA. Der höchste dänische Orden heißt Elefantenorden.

In der Symbolik spielt auch der weiße Elefant eine besondere Rolle, der in der Natur nur sehr selten vorkommt. Die Seekriegsflagge Thailands ziert ein weißer Elefant, der dort als Zeichen von Macht verehrt wird.

In der indischen Mythologie ist Airavata der erste Elefant. Gott Ganesha erscheint mit dem Kopf eines Elefanten. Er ist eine der beliebtesten Gottheiten des Hinduismus und gilt als Verkörperung von Weisheit und Wohlstand und als Helfer in schwierigen Lebenssituationen. Die buddhistische Überlieferung kennt eine Legende, nach der Mahamaya, der Mutter Siddhartha Gautamas, vor dessen Geburt ein weißer Elefant erschien.

Im Gleichnis „Die blinden Männer und der Elefant“, das im Sufismus, Jainismus, Buddhismus und Hinduismus erzählt wird, geht es um die begrenzte Fähigkeit des Menschen, die Realität – symbolisch durch den Elefanten dargestellt – so zu erkennen und zu verstehen, wie sie "wirklich" ist.

In China erscheint ein weißer elefantenköpfiger Gott, der eine jungfräuliche Göttin, Moye, schwängert. Moye gebiert den Helden Fu-Hi. In China gelten Elefanten noch heute als Symbol für männliche Potenz. Im Judentum bzw. Christentum taucht der elefantenköpfige Dämon Behemoth auf, und auch er symbolisiert die „Fleischlichkeit“ (sexuelle Energie). Die Verbindung zwischen Elefanten und sexueller Energie existierte in Heldenepen des europäischen Mittelalters, in denen „Hörner aus Elfenbein“ eine wichtige Rolle spielten (z. B. das Horn "Olifant" im Rolandslied). Gemahlenes Elfenbein gilt noch heute bei einigen asiatischen Völkern als Potenzmittel. In eine andere Richtung deutet dagegen die Legende vom Elefantenfriedhof.

Seit dem indischen Chaturanga bestand in vielen historischen Schachformen eine Elefantenfigur.

Schließlich ist der Begriff des Elefanten teilweise auch negativ besetzt. So knüpft die Redensart „wie ein Elefant im Porzellanladen“ an die Vorstellung an, Elefanten seien ungeschickt. In der Fernsehsendung "Das will ich wissen", ZDF, 6. November 2009, wurde jedoch das reale Verhalten eines Elefanten in einem Porzellanladen gezeigt. Das Tier bewegte sich äußerst behutsam und vorsichtig, entnahm einer Kristallschale eine Frucht und nicht ein Stück Porzellan ging zu Bruch.

In den Medien sind Elefanten zu finden, die sich meist speziell an Kinder richten. So gibt es Hörspiele wie Benjamin Blümchen und Zeichentrickfiguren wie Dumbo oder Babar und den kleinen blauen Elefanten aus der Sendung mit der Maus. Auf Elefanten basieren Otto Waalkes' Ottifanten. Einen Elefanten im Logo verwenden Hersteller von Spielzeugen wie die Simba-Dickie-Group und Jumbo.

In Werken Salvador Dalís sind Elefanten häufiger zu finden, beispielsweise in seinen Gemälden "Schwäne spiegeln Elefanten" (1937) und "Die Elefanten" (1948). Berühmt ist Berninis Elefant: eine Skulptur, die vom italienischen Bildhauer Gian Lorenzo Bernini entworfen und 1667 durch seinen Schüler Ercole Ferrata geschaffen wurde. Es handelt sich um einen Elefanten, der einen Obelisken auf dem Rücken trägt (siehe Obelisco della Minerva). Dieses barocke Werk vor der römischen Kirche Santa Maria sopra Minerva wurde vermutlich durch den Renaissance-Roman "Hypnerotomachia Poliphili" inspiriert, in dem ein Gebäude in Form eines Elefanten auftaucht, der ebenfalls einen Obelisken trägt.





</doc>
<doc id="1465" url="https://de.wikipedia.org/wiki?curid=1465" title="Enderbit">
Enderbit

Enderbite sind metamorphe Gesteine, und zwar Hypersthen führende Tonalite aus der Gruppe der Charnokite. Sie sind nach dem Fundort der ersten Gesteinsproben dieses Materials, der Typlokalität, dem Enderbyland in der Antarktis, benannt.

Vorkommen finden sich z. B. in Simbabwe und Südindien.




</doc>
<doc id="1466" url="https://de.wikipedia.org/wiki?curid=1466" title="Enzyklopädist">
Enzyklopädist

Ein Enzyklopädist ist eine Person, die dazu beiträgt, das Wissen ihrer Zeit in Form einer Enzyklopädie zusammenzufassen. Weiter fallen unter den Begriff auch die Herausgeber einer Enzyklopädie.

Eine zeitlich umrissene Gruppe solcher Personen waren die Beiträger zur französischen Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers (1751–1780), die als Enzyklopädist (Encyclopédie) zusammengefasst werden.

In der Latinistik werden die römischen Autoren Marcus Porcius Cato der Ältere, Marcus Terentius Varro, Aulus Cornelius Celsus und Gaius Plinius Secundus als Enzyklopädisten zusammengefasst.



</doc>
<doc id="1467" url="https://de.wikipedia.org/wiki?curid=1467" title="Ethologie">
Ethologie

Als Ethologie ( „Gewohnheit, Sitte, Brauch“ und -logie als Lehre bzw. Wissenschaft) wird im deutschen Sprachraum traditionell die „klassische“ vergleichende Verhaltensforschung bezeichnet, gelegentlich aber auch ganz generell die Verhaltensbiologie.

Die Ethologie ist somit ein Teilgebiet der Zoologie und eine Nachbardisziplin der Psychologie, aber innerhalb der Zoologie auch eine Ergänzung zu den "vergleichenden" Ansätzen von Morphologie, Anatomie und Physiologie im Dienst einer systematischen Verwandtschaftsforschung. Die ethologische Forschung ist eng verbunden mit den Arbeiten von Oskar Heinroth, Erich von Holst, Konrad Lorenz, Günter Tembrock, Nikolaas Tinbergen und Irenäus Eibl-Eibesfeldt, dem Entwurf einer Instinkttheorie sowie mit dem ehemaligen Max-Planck-Institut für Verhaltensphysiologie. Als bedeutender Vorläufer kann Jean-Henri Fabre betrachtet werden, der den Instinkt bei den Insekten untersuchte.

Die Bezeichnung "Ethologie" ist abgeleitet von den griechischen Wörtern "ethos" und "logos". Dabei steht "ethos" für ‚Charakter, Sitte, Gewohnheit, Vorkommen‘. "Ethologie" bedeutet dem Wortsinne nach „die Lehre von den Gewohnheiten“.

Schon Charles Darwin hatte aufgrund jahrelanger eigener Zuchtexperimente (u. a. an Tauben) erkannt, dass die häufig sehr komplexen Verhaltensweisen der Tiere aufgrund der gleichen Gesetzmäßigkeiten entstanden sein müssen wie ihre anatomischen Merkmale: also aufgrund von zufälliger Variabilität der einzelnen Merkmale und deren Bedeutung im „Überlebenskampf“ ihrer Träger.

Noch bis ins frühe 20. Jahrhundert hinein hielten sich aber u. a. so genannte vitalistische Anschauungen, die angeborenes Instinktverhalten zwar nicht leugneten und sogar dessen Zweckmäßigkeit aufzeigten. Sie beantworteten jedoch die Frage nach dem Entstehen dieser Zweckmäßigkeit mit der Annahme einer "Lebenskraft" (lateinisch "vis vitalis", daher: Vitalismus), einer "Naturkraft" oder der "göttlichen Lenkung". Diese Unterstellung letztlich übernatürlicher Kräfte blockierte lange Zeit jede naturwissenschaftliche Ursachenforschung. Ein prominenter Vertreter dieser Richtung war Alfred Russel Wallace. Wallace gilt neben Darwin als der Begründer der modernen Evolutionstheorie; er entfernte sich aber weit von allen evolutionsbiologischen Denkweisen, sobald es um das Entstehen der Instinkte ging.

In scharfem Gegensatz zu den vitalistischen Richtungen standen die sogenannten Mechanisten, die alles Verhalten als das gleichsam passive Reagieren auf Außenreize deuteten, als eine Kette von Reflexen („Reflexkettentheorie“). Ihre Anschauungen fußten vor allem auf den Forschungsergebnissen des Nobelpreisträgers Iwan Pawlow und verneinten innere Antriebe bzw. schlossen sie als Gegenstand wissenschaftlicher Untersuchung aus. Ein prominenter Vertreter dieser Richtung war neben Pawlow der US-amerikanische Psychologe John B. Watson, der Begründer des klassischen Behaviorismus.

Im deutschen Sprachraum entwickelte sich nach dem Ersten Weltkrieg eine eigenständige Forschungsrichtung, die zum einen – im Unterschied zum Behaviorismus – das spontane Auftreten von angeborenem Verhalten aufgrund innerer, zentralnervöser Ursachen („Triebe“) betonte. Zum anderen verglichen ihre Vertreter, aufgrund der unterstellten Vererbbarkeit solcher Verhaltensweisen, das Verhalten verwandter Arten in ähnlicher Weise miteinander, wie Anatomen anatomische Merkmale miteinander vergleichen (daher auch "vergleichende Verhaltensforschung"). 1937 schuf sich diese Forschungsrichtung mittels der "Zeitschrift für Tierpsychologie" ein eigenes Publikationsorgan. Erst nach dem Zweiten Weltkrieg wurde die Bezeichnung Tierpsychologie durch die Bezeichnung "Ethologie" abgelöst, da die "Tierpsychologie" inzwischen im Ruf einer bloßen Liebhaberei stand. Das Kunstwort "Ethologie" war allerdings schon Mitte des 19. Jahrhunderts in Frankreich von Isidore Geoffroy Saint-Hilaire geprägt worden, und Friedrich Dahl hatte bereits 1898 vorgeschlagen, das französische Wort für die Lebensgewohnheiten der Tiere ins Deutsche zu übernehmen. Doch erst nachdem William Morton Wheeler 1902 "ethology" in den englischen Sprachraum eingeführt und sich diese Bezeichnung dort durchgesetzt hatte, gelangte sie über diesen Umweg wieder zurück nach Deutschland.

Noch in den 1970er Jahren wurden die Bezeichnungen "Ethologie", "Instinktforschung" und "vergleichende Verhaltensforschung" von den Forschern dieses Fachgebiets als Synonyme verwendet.

In dem Maße, in dem die aus der traditionellen vergleichenden Verhaltensforschung hervorgegangene Instinkttheorie aufgrund von neueren behavioristischen und verhaltensökologischen sowie neurobiologischen Befunden als überholt angesehen wurde, benutzten viele Verhaltensforscher seit den frühen 1980er Jahren auch die Bezeichnung "Ethologie" immer weniger und ersetzten sie durch die als neutraler empfundene Bezeichnung "Verhaltensbiologie".

Außerhalb des deutschen Sprachraums stehen heute hingegen beispielsweise "ethology" (engl.), "éthologie" (französ.), etología (spanisch), etologia (italienisch), etoloji (türkisch) und etologi (dänisch) ganz allgemein für "Verhaltensbiologie". Deshalb wurde die 1937 von Konrad Lorenz mitbegründete "Zeitschrift für Tierpsychologie", neben "Behaviour" und "Animal Behaviour" jahrzehntelang die bedeutendste verhaltensbiologische Fachpublikation, ab 1986 von Wolfgang Wickler – dem internationalen Sprachgebrauch folgend − in "Ethology" umbenannt.

Eine grundlegende Wendung nahm die Verhaltensforschung durch Oskar Heinroth, in dessen 1911 publiziertem Vortrag vor dem 5. Internationalen Ornithologen-Kongress das Wort "Ethologie" erstmals im heutigen, in Deutschland gebräuchlichen Sinne auch vor großem Fachpublikum verwendet wurde. Heinroth hatte zunächst das Verhalten von diversen Gänse- und Entenarten studiert und dabei festgestellt, dass bestimmte Bewegungsweisen (beispielsweise bei der Balz) von Tieren gleichen Geschlechts und gleicher Art mit immer denselben Gesten und Körperhaltungen ausgeführt werden. Heinroth nannte solche formkonstanten Bewegungen "arteigene Triebhandlungen" und konnte aufzeigen, dass verwandte Arten mehr oder weniger starke Abwandlungen solcher Verhaltensweisen besitzen. Von diesen genauen Verhaltensbeobachtungen zu einer evolutionären Deutung ihres Entstehens war es dann weder für Heinroth noch für dessen späteren Schüler Konrad Lorenz ein großer Schritt. Lorenz griff 1931 die Bezeichnung "Ethologie" erstmals auf, als er einen umfangreichen Aufsatz über die „Ethologie sozialer Corviden“ veröffentlichte.

Das ethologische Instinktkonzept besagt, dass Instinktbewegungen im Erbgut verankert sind und durch Schlüsselreize ausgelöst werden können, solange eine innere aktionsspezifische Energie vorhanden ist. Die Zweckmäßigkeit dieses Ineinandergreifens von äußerem Auslöser, Handlungsbereitschaft und spezifischer Verhaltensweise habe sich im Prozess der Evolution entwickelt und diene letztlich der Arterhaltung.

Ein häufig zitiertes Beispiel für eine solche Instinktbewegung ist die "Eirollbewegung der Graugans": Wenn ein Ei (der Schlüsselreiz) außerhalb des Nestes gerät, reckt die Gans ihren Schnabel über das Ei hinweg und rollt das Ei mit Hilfe ihres Schnabels zurück ins Nest. Diese Bewegung läuft immer auf die gleiche Weise ab und wird selbst dann zu Ende geführt, wenn das Ei während des Vorgangs von einem Versuchsleiter entfernt wird. Diese starre, angeborene Form des Verhaltens gilt als eine "arteigene Triebhandlung" im Sinne von Oskar Heinroth und wurde von Konrad Lorenz als Erbkoordination bezeichnet.

Weitere Fachbegriffe der Instinkttheorie sind u. a. Angeborener Auslösemechanismus, Appetenz, Leerlaufhandlung und Übersprungbewegung sowie das Prägungskonzept.

Kennzeichnend für die ethologische Instinktforschung ist zum einen die Betonung der Freilandforschung, also das Beobachten des Verhaltens unter natürlichen Umweltbedingungen, zum anderen sogenannte Ethogramme: Das sind exakte Beschreibungen aller bei einer Tierart beobachtbaren Verhaltensweisen. Anhand dieser Ethogramme können "Verhaltensprotokolle" erstellt werden, in denen die Häufigkeit der Verhaltensweisen und ihre zeitliche Abfolge aufgelistet werden (z. B.: Nahrungsaufnahme, Schlafen, Sich-Putzen, schnelles Weglaufen, Eintragen von Jungtieren zum Nest). Hierdurch wird es möglich, sowohl die Häufigkeit als auch das Aufeinanderfolgen von Verhaltensweisen qualitativ und quantitativ zu beschreiben.

Mit der Bezeichnung Instinktbewegung war bis Ende der 1960er-Jahre die Auffassung verbunden, es handele sich bei den so gedeuteten Verhaltensweisen um rein angeborene Aktivitäten. Inzwischen hat die Forschung aber immer mehr Anhaltspunkte dafür gefunden, dass solche starren Reaktionen auf externe Reize ein "Ausnahmefall" sind, dass Erbe und Umwelt auch in Bezug auf einzelne Verhaltensweisen eng miteinander verzahnt sind (siehe Reaktionsnorm).

Zentrale Konzepte der klassischen Ethologie wurden 1990 von Wolfgang Wickler, einem Schüler von Konrad Lorenz, und 1992 von Hanna-Maria Zippelius, einer Schülerin von Karl von Frisch, kritisiert (vergleiche hierzu unter anderem Übersprunghandlung und Leerlaufhandlung).

Das Konzept des Instinktverhaltens in drei Phasen (ungerichtete Appetenz, Taxis, Endhandlung) ist zwar zur Beschreibung des Nahrungserwerbes von Beutegreifern geeignet, versagt aber schon bei Pflanzenfressern.





Schwierig zu klären ist eine Grundfragestellung der klassischen Ethologie, ob ein bestimmtes Verhalten angeboren oder erlernt ist. Die hierfür entwickelten Untersuchungsmethoden lieferten nur selten eine eindeutige Entscheidung. In der modernen Verhaltensforschung geht man davon aus, dass jegliches Verhalten, wie alle phänotypischen Eigenschaften eines Organismus, eine genetische Grundlage hat und stets zugleich durch Umwelteinflüsse moduliert wird.




Als Nutztierethologie wird von einem Teil der Agrarwissenschaft die Erforschung des Verhaltens von Nutztieren mit dem Ziel einer Optimierung der Haltungsbedingungen in der Nutztierhaltung bezeichnet.




</doc>
<doc id="1469" url="https://de.wikipedia.org/wiki?curid=1469" title="Extension und Intension">
Extension und Intension

Extension und Intension ( ‚Ausdehnung, Spannweite, Verbreitung‘ und ‚Mühe, Spannung, Anspannung‘) sind Begriffe aus der Semantik, mit denen verschiedene Dimensionen der Bedeutung sprachlicher Ausdrücke (Prädikate, Sätze) oder logischer Entitäten (Mengen, Begriffe, Propositionen) bestimmt werden. Das Begriffspaar stammt aus dem Umfeld der aristotelischen Logik und wurde als ' und ' („Umfang und Inbegriff der Idee“) durch die "Logik von Port-Royal" etabliert. In der Sprachphilosophie, den Sprachwissenschaften, verschiedenen Kalkülen der Logik und der Mathematik werden Extension und Intension oftmals unterschiedlich konzipiert. Für Prädikate und Begriffe sind die Ausdrücke Begriffsumfang und Begriffsinhalt unproblematische Übersetzungen.

In der traditionellen Logik (Begriffslogik) verstand man unter der Extension oder dem Umfang eines Begriffs die Gesamtheit der Dinge, auf die er sich erstreckt (die unter ihn fallen, die er umfasst). Demnach war die Extension des Begriffes „Mensch“ die Gesamtheit aller Menschen. Seit der pyrrhonischen Skepsis besteht allerdings auch Zweifel an solcher Begriffspotenz. Mit dem Aufkommen empirischer Wissenschaften gerieten die Taxonomien mehr in die einzelwissenschaftlichen Verantwortungsbereiche und ihre philosophisch- bzw. theologisch-syllogistische Verwaltung wurde obsolet. In der traditionellen Logik war nie eine hinreichend komplexe Ontologie gelungen, um praxistaugliche Überprüfungs- und Entscheidungsverfahren zu ermöglichen, als Beispiel dafür sei nur die mannigfach diskutierte Frage genannt, was zur Gesamtheit aller Menschen gehört und was nicht (z. B. verstorbene Menschen, Versehrte, Leichname, zukünftige Menschen, nur möglicherweise existierende Menschen. Zum Problem siehe auch Präsentismus, Aktualismus). Der letzte Verteidiger einer solchen Begriffslogik war Bruno von Freytag-Löringhoff.

In der klassischen Logik fasst man Begriffe oft als einstellige Prädikate auf, das heißt als Aussageformen mit einer Leerstelle. Aus der Aussageform „… ist ein Mensch“ entsteht dann eine wahre Aussage, wenn man in die Leerstelle den Eigennamen oder die Kennzeichnung eines Menschen einsetzt. Extension eines solchen Prädikates ist dann die Menge der Referenten all jener Eigennamen und Kennzeichnungen, die in die Leerstelle eingefügt eine wahre Aussage ergeben. Die Extension ist demnach die Menge der Gegenstände, denen die durch das Prädikat ausgedrückte Eigenschaft zukommt. Entsprechendes gilt für mehrstellige Prädikate (Relationen): Die Extension des zweistelligen Prädikats „… hat denselben Vater wie …“ besteht aus der Menge aller Geschwister- und (väterlichen) Halbgeschwisterpaare.

Darüber, was Intension und Begriffsinhalt sind, gehen die Meinungen in der Logik auseinander. Nach einer häufig vertretenen Auffassung besteht die Intension eines Begriffes aus der Gesamtheit der Merkmale oder Eigenschaften – die Terminologie ist hier uneinheitlich –, die den Dingen, die er umfasst, "faktisch" gemeinsam sind oder die die Schnittmenge ihrer "notwendigen" Merkmale ausmachen. Demnach enthält die Intension des Begriffes „Mensch“ die Merkmale "belebt", "sterblich", "auf zwei Beinen gehend", "ungefiedert", "vernunftbegabt", "Werkzeuge produzierend" etc.

Begriffsmerkmale treten hauptsächlich bei der Definition eines Begriffs in Erscheinung:
Oder:
Keine dieser Definitionen macht von allen Merkmalen Gebrauch, die allen Menschen gemeinsam sind; beide kommen z. B. ohne das Merkmal "sterblich" aus. Trotzdem erfüllen sie ihren Zweck, nämlich aus einem Diskursuniversum, das nur physische Dinge umfasst, trennscharf diejenigen herauszufiltern, die unter den Begriff „Mensch“ fallen. Wäre dagegen von einer Welt die Rede, in der auch für vernunftbegabte Unsterbliche Platz ist, z. B. für die Göttinnen und Götter des Olymp, so müsste die zweite Definition, um diese Funktion zu erfüllen, durch die Hinzunahme des Merkmals "sterblich" verengt werden.

Die Beispiele zeigen außerdem, dass Begriffe mit verschiedener Intension im selben Diskursuniversum dieselbe Extension haben können: „auf zwei Beinen gehende ungefiederte Lebewesen“ und „vernünftige Lebewesen“ sind extensional gleiche Begriffe. Das Umgekehrte gilt nicht: Begriffe mit verschiedener Extension besitzen im selben Diskursuniversum stets verschiedene Intension.

Bekanntlich sind viele Wörter mehrdeutig: Das Wort „Bank“ kann eine Sitzgelegenheit oder ein Geldinstitut bezeichnen. Bei beiden Bedeutungen handelt es sich um verschiedene Begriffe. Was konstituiert die Verschiedenheit dieser Begriffe und wie erkennt man Gleichheit und Verschiedenheit von Begriffen? Ein einfacher Antwortversuch auf diese Frage wird als Extensionalitätsthese bezeichnet, der zufolge Begriffe durch ihren Extensionalbereich vollständig bestimmt seien. Offensichtlich ist die Menge aller Sitzgelegenheiten eine andere Menge als diejenige aller Geldinstitute.

Diese Extensionalitätsthese hat unter anderem das bekannte Problem, zu erklären, wie es sich bei Bezeichnungen wie „Abendstern“ und „Morgenstern“ verhält. Die Extension beider Bezeichnungen ist identisch: Beide beziehen sich auf den Planeten Venus. Trotzdem scheint plausibel, dass, wer an den Abendstern denkt, einen anderen Begriff verwendet als jener, welcher an den Morgenstern denkt. Der Unterschied liegt, so die klassische Formulierung von Gottlob Frege, nicht in der Extension, sondern in der Weise der Bezugnahme auf das bezeichnete Objekt, also der Intension. Frege selbst spricht nicht von Extension, sondern von "Bedeutung", und nicht von Intension, sondern von "Sinn". Zieht man auch die Intension zur Individuation von Begriffen heran, muss die Extensionalitätsthese verworfen werden.

Versteht man die Intension als eine Menge von Merkmalen und die Extension als eine Menge von Gegenständen, welche diese Merkmale besitzen, so verhalten sich Intension und Extension offensichtlich in folgender Weise gegensätzlich zueinander: je umfangreicher die Intension, desto kleiner die Extension und umgekehrt. Ein Begriff wie „Substanz“ umfasst nach der aristotelischen Ontologie alles überhaupt Seiende, ein Begriff wie „körperliche Substanz“ entsprechend weniger, und ein Begriff wie „vernunftbegabte beseelte körperliche Substanz“ noch weniger Objekte. Solche Beispiele existieren in großer Zahl und lassen die folgende grundlegende Gesetzmäßigkeit vermuten:

Mit dem Aufkommen der modernen Logik wurde die Allgemeingültigkeit dieser Regel auf verschiedene Weise angezweifelt. Der Grund dafür lag in der erwähnten Unbestimmtheit des Begriffs der Intension und in der Vielzahl der Möglichkeiten, ihn in die formale Sprache eines Logikkalküls zu übersetzen. Den ersten erfolgreichen Versuch zu einer solchen Übersetzung unternahm Paul Weingartner. Weingartner konnte zeigen, dass „bei entsprechender Definition des intensionalen Enthaltenseins“ die oben formulierte Grundregel ein Theorem der Klassenlogik darstellt.

Geprägt wird die Gegenüberstellung zwischen Extension und Intension, deren Wurzeln auf die aristotelische Logik zurückgehen, in der Logik von Port-Royal. Beispielhaft sei auch eine kompakte Formulierung von Leibniz zitiert: „Das Lebewesen umfasst mehr Individuen als der Mensch, aber der Mensch enthält mehr Ideen oder Formeigenschaften; das eine hat mehr Exemplare, das andere mehr Wirklichkeitsgrad; das eine hat mehr Extension, das andere mehr Intension.“

Im Lauf der Philosophiegeschichte wurde das Konzept der Extension und Intension von unterschiedlichen Autoren aufgebracht, wobei man bei der Gleichsetzung der Begriffspaare äußerst vorsichtig sein sollte, zumal einige Autoren sie als Eigenschaften von mentalen Entitäten (Begriffen, Urteilen), andere als Eigenschaften sprachlicher Ausdrücke behandeln. Die folgende Tabelle zeigt einige dieser Bezeichnungen.

Dabei ist zu beachten, dass in besonderem Maß bei Frege Vorsicht geboten ist, den Ausdruck „Bedeutung“ mit der Extension gleichzusetzen. Die Unterscheidung zwischen Extension und Intension findet grundsätzlich bei Begriffswörtern („Planet“) Verwendung, während Frege die Unterscheidung zwischen Sinn und Bedeutung auch auf Eigennamen (wobei der Sinn die Art des Gegebenseins eines Gegenstandes ist, die Bedeutung der entsprechende Gegenstand) und ganze Sätze (der Sinn ist hier der Gedanke, die Bedeutung das Wahre/Falsche) anwendet. Darüber hinaus ergeben sich auch bei Anwendung auf Begriffswörter Unterschiede: Während die Extension von „Planet“ die Planeten des Sonnensystems umfasst, ist für Frege die Bedeutung von „Planet“ der abstrakte Begriff „() ist ein Planet“. Zudem wird in ungeraden Kontexten oder opaken Kontexten der ursprüngliche Sinn zur Bedeutung des Ausdrucks. Was an die Stelle des Sinns rückt, lässt Frege offen.

Nach verbreiteter, umstrittener, von Gottlob Frege begründeter Auffassung ist die Extension eines Aussagesatzes sein Wahrheitswert.

Frege selbst:
Die Intension eines Satzes (bei Frege: der Sinn eines Satzes) sind nach verbreiteter, umstrittener Auffassung sein Sinn, Inhalt oder Gedanke oder seine Proposition. Nach Frege ist der Sinn eines Satzes sein Gedanke (in einem objektiven Sinn). Nach Rudolf Carnap ist die Intension eines Satzes die durch den Satz bezeichnete Proposition.

Zum Alltagsgeschäft von Juristen gehört es, konkrete Sachverhalte mit Rechtsnormen zu verknüpfen, bei denen Begriffe, insbesondere unbestimmte oder vage Begriffe, eine zentrale Rolle spielen. Dabei geht es einerseits darum, die Intension eines anzuwendenden Begriffs so zu bestimmen, dass in der Praxis trennscharfe Unterscheidungen vorgenommen werden können, und so gleichzeitig die potenzielle Extension anzugeben: Fälle mit dem Merkmal x (Intensionsbestimmung) gehören zur Menge (Extensionsbestimmung) der mit dem Begriff „y“ bezeichneten normierten Sachverhalte.

Beispiel: Abs. 1 des deutschen Strafgesetzbuches lautet: Der Begriff „Diebstahl“ kann nicht auf Fälle des Anzapfens elektrischer Energie angewandt werden (eingeschränkte Extension des Begriffs „Diebstahl“), da Strom keine „Sache“ ist (eingeschränkte Intension des Begriffs „Sache“). Die auf diese Weise entstandene Gesetzeslücke wurde geschlossen, indem ins StGB der eingefügt wurde, durch den die „Entziehung elektrischer Energie“ unter Strafandrohung gestellt ist.

Die Frage, ob das, was ein Begriff als sprachliches Zeichen bezeichnet, existiert oder nicht, kann nicht nur als empirische behandelt werden, sondern auch als onto- oder mythologische. Dann bekommen Begriffe wie „Gott“, „Teufel“, „Engel“ nicht einfach eine Null-Extension, sondern eine komplexere Intension. Götter wie Zeus „gibt es“ jedenfalls als Teilelement des Begriffs „griechische Mythologie“.







</doc>
<doc id="1537" url="https://de.wikipedia.org/wiki?curid=1537" title="Liste von Filmmusik-Komponisten">
Liste von Filmmusik-Komponisten

Diese Liste enthält Personen, die durch ihre langjährige Haupttätigkeit als Komponisten von Musik für Film und Fernsehen Bekanntheit erlangten oder für ihre Filmmusik mit international oder national führenden Preisen ausgezeichnet wurden oder deren Filmmusik auf andere Weise besonders erfolgreich war.




























</doc>
<doc id="1559" url="https://de.wikipedia.org/wiki?curid=1559" title="Freeware">
Freeware

Freeware ([]; von „kostenlos“ und „Ware“) bezeichnet im allgemeinen Sprachgebrauch Software, die vom Urheber zur kostenlosen Nutzung zur Verfügung gestellt wird. Freeware ist meistens proprietär und steht damit laut der Free Software Foundation im Gegensatz zu Freier Software (engl. ), die weitläufigere Freiheiten gewährt, etwa Veränderungen an der Software.
Die Programmierer verzichten bei Freeware nur auf eine Nutzungsvergütung, aber nicht auf das Urheberrecht. Den Benutzern wird nur ein Nutzungsrecht eingeräumt; Änderungen der Software oder die Nutzung der Teile des Programms (wie etwa Codeschnipsel) werden untersagt. Freewareprogramme können frei kopiert und weitergegeben werden.
Der Unterschied zu Public-Domain-Software ist, dass diese völlig frei nutzbar und veränderbar ist.
Der Begriff "Freeware" wurde von dem US-amerikanischen Programmierer Andrew Fluegelman begründet, der sein Kommunikationsprogramm PC-Talk 1982 jenseits der üblichen und kostenintensiven Distributionswege vertreiben wollte. Die heutige Bedeutung des Begriffs Freeware ist jedoch eine andere als die damalige, nach heutiger Terminologie würde man bei dem damaligen Vertriebsmodell für "PC-Talk" von Shareware reden.

Ein Autor kann nach dem Urheberrecht bei einer Weitergabe seines Werks die vertraglichen Bedingungen in weitem Umfang festlegen. So ist Freeware kein genau definierter, rechtsgültiger Begriff. Es ist in jedem Einzelfall anhand der in einem Endbenutzer-Lizenzvertrag festgelegten Lizenzbedingungen zu prüfen, welche konkreten Rechte der Urheber dem Anwender gewährt. Typische Vertragsbedingungen vom Autor sind etwa, dass die Verbreitung gegen ein Entgelt untersagt ist oder die Nutzung nur für Privatpersonen kostenlos ist, d. h. der Einsatz im kommerziellen Umfeld bedarf einer Lizenzgebühr. Ob bei solchen oder noch weitergehenden Einschränkungen der Nutzung der Begriff Freeware noch zutreffend angewendet wird, ist zumindest unter dem Aspekt der allgemeinen Nutzungsfreiheit strittig.

Eine spezielle Form von Freeware liegt bei Software-Produkten vor, die auf ein kostenpflichtiges Betriebssystem aufbauen, wie z. B. Internet Explorer und Microsoft Windows Media Player. Hierbei ist die kostenlose Nutzung an den Besitz anderer Microsoft-Lizenzen und die Zustimmung zur Rechteerweiterung der bestehenden Lizenzen für Microsoft gebunden.

Für die folgenden Lizenzmodelle wird "Freeware" in der Regel als Oberbegriff verwendet, teilweise auch synonym. Sie sind allerdings mit Einschränkungen verbunden, die sich aus dem Namen des Lizenzmodells ergeben:

Der Begriff Freeware ist in einigen Gebieten anzutreffen: Einmal bei den Computerzeitschriften, die Freeware gerne als vereinfachenden Oberbegriff verwenden (z. B. für freie Software oder Lite-Versionen) und bei Hobbyprogrammierern, die ihre kleineren Software-Projekte auf ihrer Homepage zum kostenlosen Herunterladen anbieten und sich auch nicht mit Software-Lizenzrecht befassen wollen. Viele gängige Webanwendungen werden von Open-Source-Communitys (z. B.: Mantis oder Typo3 Community) programmiert bzw. weiterentwickelt. Die Nutzung dieser Softwares ist kostenlos, die Entwickler bitten die Nutzer um eine freiwillige Spende.

Eine andere Quelle von Freeware ist ehemalige kommerzielle Software, die am Ende ihrer kommerziellen Vermarktung der Nutzergemeinde als Freeware zur Verfügung gestellt wird, teilweise als Promotionaktion für eine neue Software. Teilweise wird frei herunterladbare Freeware zur Verfügung gestellt, um zu verhindern, dass Software nicht mehr erhältliche Abandonware wird, beispielsweise Borland gab einige seiner Legacy-Produkte deswegen frei, z. B. Turbo Pascal oder Diversions Entertainment das Computerspiel One Must Fall.

Obwohl der Begriff Freeware am häufigsten auf kleinere Software-Produkten angewandt wird, existieren einige Beispiele für größere kostenlose Software-Produkte, z. B. der Webbrowser Opera oder das Weltsimulations-Computerspiel .

Auch die meiste Open-Source-Software, wenn auch nicht jede, ist neben ihren weiter reichenden Qualitäten ebenfalls Freeware; ein Beispiel ist der Mozilla Firefox.

Neben einzelnen Autoren gibt es auch Gruppen, sogenannte "coding groups", die hobbymäßig Software programmieren und diese als Freeware anbieten, z. B. im Computerspielbereich Homebrews oder Fangames.
Es gibt auch so genannte Filehosting-Dienste bzw. Open-Source-Communitys (z .B.: SourceForge GNU Savannah und GitHub), wo Programmierer ihre Softwareprojekte erstellen, verwalten und anderen Usern anbieten können.




</doc>
<doc id="1561" url="https://de.wikipedia.org/wiki?curid=1561" title="Fritz Lang">
Fritz Lang

Fritz Lang (* 5. Dezember 1890 in Wien; † 2. August 1976 in Beverly Hills, Kalifornien; eigentlich "Friedrich Christian Anton Lang") war ein österreichisch-deutsch-US-amerikanischer Schauspieler, Filmregisseur und Drehbuchautor. Nach seiner Heirat mit der deutschen Drehbuchautorin Thea von Harbou erwarb der Österreicher 1922 auch die deutsche und nach seiner Emigration 1939 die US-amerikanische Staatsbürgerschaft.

Lang prägte die Filmgeschichte mit, indem er – vor allem in der Ära des späten Stummfilms und des frühen Tonfilms – neue ästhetische und technische Maßstäbe setzte. Seine Stummfilme erzählen zumeist utopische und fantastische Geschichten, die in einer expressiv düsteren Atmosphäre inszeniert wurden. In seinen Tonfilmen rückte er einzelne Menschen und deren innere Beweggründe in den Mittelpunkt; ihre Themen waren dem Alltagsleben entnommen und basierten häufig auf Presseberichten. Der Stummfilm "Metropolis" (1927) und der Tonfilm "M" (1931) gehören zu den Meilensteinen der deutschen und internationalen Filmgeschichte.
Fritz Lang wuchs in Wien als Sohn des Architekten und Stadtbaumeisters Anton Lang und dessen Frau Pauline, geb. Schlesinger, auf. Nach dem Abschluss der Realschule begann er 1907 auf Wunsch des Vaters ein Architekturstudium an der Technischen Hochschule in Wien. 1908 wechselte er an die Wiener Akademie der bildenden Künste, um dort Malerei zu studieren. Außerdem studierte er an der Staatlichen Gewerbeschule in München. Während des Studiums trat er nebenbei als Kabarettist auf. Von 1909 bis 1919 hatte Lang eine Wohnung in der Zeltgasse 1 im achten Bezirk, wo heute eine Gedenktafel angebracht ist.

Von 1910 an unternahm Lang Reisen in die Mittelmeerländer und nach Afrika. 1911 ging er nach München, um an der Kunstgewerbeschule zu studieren, blieb dort nur kurz und ging erneut auf Reisen. 1913/14 setzte er seine Ausbildung in Paris beim Maler Maurice Denis fort und entdeckte für sich dort den Film.

Nach dem Beginn des Ersten Weltkriegs kehrte Lang 1914 nach Wien zurück. Im August 1914 lebte er im Landhaus seiner Eltern in Gars am Kamp, wovon ein Brief zeugt, in dem er detailliert seine letzten Tage in Paris und die turbulente Rückkehr nach Österreich beschreibt. Er meldete sich als Kriegsfreiwilliger und zeichnete sich bei seinem ersten Einsatz an der Front durch große Tapferkeit aus.

Die Zeit von Juni bis Dezember 1915 verbrachte er bei der Ausbildung zum Reserveoffizier in Luttenberg in der Steiermark (dem heutigen Ljutomer im Osten Sloweniens). Bedingt durch seinen militärischen Rang wohnte er privat im Hause des Anwalts Karl Grossmann, eines typischen Intellektuellen, der zahlreichen Interessen, auch der Fotografie, nachging und auch drei Kurzfilme drehte. Lang selbst arbeitete in dieser Zeit, angeregt durch örtliche, traditionelle Töpfereien, auch in Terrakotta. Zwei seiner (Selbstporträt?-)Büsten und zwei Gartenvasen (z. T. signiert und datiert) werden von der Familie Grossmann bewahrt. Es handelt sich wahrscheinlich um Langs einzige erhaltene Werke der bildenden Kunst. Spätere Filmideen und Ausstattungsmotive Langs lassen sich auf Anregungen durch die Bibliothek und die Sammlungen Grossmanns wie auch auf die Architektur und Archäologie der Stadt Luttenberg und ihrer Umgebung zurückführen.

1916 erlitt Lang eine Kriegsverletzung und sein Genesungsurlaub führte ihn zurück nach Wien, wo er Kontakte zu Filmleuten knüpfte und ab 1917 als Drehbuchautor für Joe May zu arbeiten begann (u. a. später bei "Die Herrin der Welt" und "Das indische Grabmal"). Während seiner Tätigkeit für May lernte er seine spätere Frau Thea von Harbou kennen. 1917 musste Lang wieder in den Krieg zurückkehren, wurde jedoch 1918 nach einer zweiten Verwundung für kriegsuntauglich erklärt. Im Rahmen der Truppenbetreuung war er bei einer Theatergruppe zum ersten Mal als Regisseur tätig.

Nach Kriegsende zog Fritz Lang nach Berlin, wo er am 13. Februar 1919 vor dem Standesamt Charlottenburg die Schauspielerin Elisabeth Rosenthal heiratete. Am 25. September 1920 fand seine erste Ehefrau den Tod durch einen Schuss aus Langs Browning-Pistole. Es wird davon ausgegangen, dass sie sich spontan das Leben nahm, nachdem sie Zeugin der Affäre ihres Mannes mit Thea von Harbou geworden war. Die genauen Umstände bleiben jedoch im Dunkeln, als Todesursache wurde „Unglücksfall“ statt „Selbsttötung“ angegeben.

Lang hielt seine erste Ehe sein weiteres Leben lang geheim. Deren Beendigung hat mutmaßlich seine zukünftigen Filmthemen von Schuld, Verstrickung, Tod und Suizid stark beeinflusst. In dem 2001 anlässlich der Lang-Retrospektive bei den Berliner Filmfestspielen herausgegebenen Kinemathek-Buch "FL. Fritz Lang" wurde dieses Kapitel aus Langs Privatleben durch Dokumente belegt, ohne den Tod von Lisa Rosenthal restlos aufzuklären.

Die Abschaffung der Zensur in der Weimarer Republik befreite nach dem Ersten Weltkrieg die Produktionsbedingungen für den Film von äußeren Zwängen. Außerdem machten die generell guten Exportchancen für Stummfilme und die Schwäche der Reichsmark im Deutschland der frühen 1920er Jahre den Dreh auch von monumentalen Filmwerken rentabel, weil allein mit den Deviseneinnahmen aus dem Auslandsgeschäft der größte Teil der Produktionskosten gedeckt werden konnte. In dieser Situation startete Fritz Lang seine Karriere als Filmregisseur, als der er bis Mitte der 1920er Jahre über die Decla-Film bzw. Decla-Bioscop AG und die UFA für den Produzenten Erich Pommer arbeitete.

Langs Erstlingswerk als Regisseur war 1919 das Melodram "Halbblut", das, wie auch der Nachfolger "Der Herr der Liebe", heute als verloren gilt. Der bekannteste und wahrscheinlich auch qualitativ daraus hervorragende Film des Frühwerks ist der ursprünglich als Vierteiler konzipierte Abenteuerfilm "Die Spinnen". Der Erfolg des ersten Teils dieses Films zwang Lang dazu, schnellstmöglich den zweiten nachzuliefern, wodurch ihm nach eigener Aussage die Regie für den zur selben Zeit entstandenen Klassiker "Das Cabinet des Dr. Caligari" entging.
"Der müde Tod" und vor allem der Zweiteiler "Dr. Mabuse, der Spieler" bescherten dem Regisseur 1921/22 schließlich auch auf internationaler Ebene den künstlerischen und kommerziellen Durchbruch. Im August 1922 heiratete er Thea von Harbou.

1924 konnte er mit dem Helden-Epos "Die Nibelungen" einen weiteren großen Publikumserfolg feiern. Während einer mehrmonatigen Kreativpause bereiste er anschließend gemeinsam mit Harbou die USA, besuchte New York und die großen Filmstudios in Hollywood.

Das Erlebnis der Stadt New York inspirierte vermutlich die Wolkenkratzer-Ästhetik von Fritz Langs bekanntestem Film, dem 1927 uraufgeführten Science-Fiction-Klassiker "Metropolis". Dieser erzählt die Geschichte einer zum Moloch mutierten Riesenstadt und brachte durch seine ausufernden Kosten und seinen Misserfolg an den Kinokassen die Universum Film AG an den Rand des finanziellen Ruins. Seine nächsten beiden Filme musste Lang selbst produzieren: 1928 folgte aus diesem Grund mit "Spione" ein relativ schmal budgetierter, aber kommerziell erfolgreicher Agentenfilm. Auch das nachfolgende Projekt, der Science-Fiction-Streifen "Frau im Mond", war 1929 ein kommerzieller Erfolg, obwohl seine filmhistorische Bedeutung bereits von der Einführung des Tonfilms überschattet wurde – das Werk ging als einer der letzten deutschen Stummfilme in die Filmgeschichte ein.

Langs erster Tonfilm war "M" für die Nero-Film AG. Er handelte von einem triebhaften Kindermörder (gespielt von Peter Lorre), der von der kriminellen Unterwelt und der Polizei gleichermaßen, wenn auch aus unterschiedlichen Gründen, gejagt wird. Auch hier setzte Lang mittels einer neuen Technik der Tonwiedergabe Akzente: Die stets vom Mörder apathisch gepfiffene Melodie (In der Halle des Bergkönigs aus der Peer-Gynt-Suite von Edvard Grieg) wird von einem blinden Luftballonverkäufer wiedererkannt, worauf der Mörder schließlich überführt werden kann. Mit dem Element Ton ging Lang in "M" auch darüber hinaus sehr geschickt um, indem er die bereits aus seinen früheren Filmen bekannten Überlappungen verschiedener Szenen zu Montagen auf einen Höhepunkt trieb: In einer Schnittmontage zwischen einer Konferenz der Polizei und einer Konferenz der Unterweltgrößen wurde so geschickt zwischen beiden Seiten hin- und hergeschnitten, dass die jeweils letzten Worte vor dem Schnitt sich mit den ersten Worten der anderen Seite nach dem Schnitt nahtlos zu Sätzen vervollständigen.

Die Figur des "Dr. Mabuse", über den Lang eine ganze Reihe von Filmen in verschiedenen Epochen drehte, ist der Prototyp des kriminellen Genies, das danach trachtet, die Welt einer „Herrschaft des Verbrechens“ zu unterwerfen. In "Das Testament des Dr. Mabuse", Langs zweitem, 1933 ebenfalls für die Nero-Film gedrehten Tonfilm, schreibt die Titelfigur, während sie in einer Zelle in der Psychiatrie einsitzt, ein Handbuch für Verbrecher. Siegfried Kracauer sah darin eine deutliche Anspielung auf Hitlers in Festungshaft entstandenes Buch "Mein Kampf". Fritz Lang selbst bestritt in späteren Jahren, "Das Testament des Dr. Mabuse" als Anspielung auf Hitler konzipiert zu haben, räumte jedoch ein, der "Mabuse"-Gestalt teils wörtliche Zitate der Nationalsozialisten in den Mund gelegt zu haben. Das noch vor der Uraufführung verhängte Verbot des Films "Das Testament des Dr. Mabuse" durch Reichspropagandaminister Joseph Goebbels trug in der Folge zur Legendenbildung bei. Im Umgang mit dem Tonfilm zeigte sich Lang auch hier sehr einfallsreich und weitete die bereits aus "M" bekannte Szenenüberleitung durch Vorwegnahme des Tons der folgenden Szene noch aus.

"M" und "Das Testament des Dr. Mabuse" gelten als Glanzlichter nicht nur des frühen Tonfilms und werden oft als handwerkliche Höhepunkte in Langs filmischem Schaffen bezeichnet.

Die Machtübernahme der Nationalsozialisten 1933 schien Langs Karriere zunächst nicht zu berühren, doch wollte er sich künstlerisch den Nationalsozialisten nicht unterordnen. Anfang April 1933 meldete die Zeitschrift "Kinematograph", dass Lang zusammen mit Carl Boese, Victor Janson und Luis Trenker die Abteilung Regie in der Nationalsozialistischen Betriebszellenorganisation (NSBO) gegründet habe. Diese Aussage lässt sich aber nicht belegen. Lang selber erklärte 1962 in einem Interview, dass er keine leitende Funktion in einer der NSDAP nahestehenden Organisation bekleidet hatte.

Nach späteren Angaben Fritz Langs versuchte Goebbels ihn zu überreden, sein Können in den Dienst der Nazis zu stellen. Goebbels soll ihm 1933 in einem persönlichen Gespräch die Leitung des Deutschen Films angeboten haben, nachdem er sich zuvor ihm gegenüber als großen Bewunderer des Regisseurs zu erkennen gegeben hatte. Lang erbat sich einen Tag Bedenkzeit, entschloss sich nach eigener Aussage noch am selben Tag zur Emigration und bestieg einen Nachtzug nach Paris. Ohne Geld will der inzwischen Zweiundvierzigjährige die Flucht angetreten haben, da die Bankschalter bereits geschlossen waren und er sein Konto nicht mehr auflösen konnte. Diese Aussage Langs wird aber weder durch Zeugen, noch durch schriftliche Belege, noch durch Einträge des Tagebuchschreibers Goebbels gestützt – tatsächlich pendelte Lang etwa drei Monate lang zwischen Berlin, London und Paris und tauschte in dieser Zeit auch Devisen bei seiner Bank.

In Frankreich traf Lang auf Erich Pommer und realisierte mit ihm 1934 den Film "Liliom" mit Charles Boyer in der Hauptrolle. Die Adaption des gleichnamigen Theaterstücks von Ferenc Molnár wurde sowohl in einer französischsprachigen als auch in einer deutschsprachigen Version gedreht. Noch im selben Jahr siedelte Lang in die USA über, wobei ihn seine neue Lebensgefährtin Lily Latté an Bord der "Île de France" begleitete. Seine ohnehin seit langem zerrüttete Ehe mit Thea von Harbou – Lang hatte 1928 eine Affäre mit der "Spione"-Hauptdarstellerin Gerda Maurus begonnen – war bereits im April 1933 geschieden worden. 

In Hollywood setzte Fritz Lang seine Karriere fort, schaffte es dort insgesamt nicht mehr, an seine großen Erfolge des Deutschen Kinos anzuknüpfen. Erheblichen Anteil hatte er an der Gründung der Anti-Nazi League. Nach einigen abgelehnten Projekten drehte er mehrere Filme, in denen er seine europäisch geprägten Ansätze erfolgreich mit US-amerikanischen Themen zu verbinden wusste. In seinem ersten US-Film "Blinde Wut" ("Fury") mit Spencer Tracy zeichnete er ähnlich wie in "M" die psychische Situation eines vom Mob Gejagten nach. Es folgten "Gehetzt" ("You Only Live Once", 1937) mit Henry Fonda und zwei Western.

In den 1940er Jahren realisierte Lang mehrere Filme, die dem Genre des Anti-Nazi-Films zuzurechnen sind, wie 1941 den Spionage-Film "Menschenjagd" ("Man Hunt") und 1943 "Auch Henker sterben" ("Hangmen also die"), einen Film über das Heydrich-Attentat. Letzterer entstand zusammen mit anderen Emigranten, unter anderem Bert Brecht, mit dem es allerdings Auseinandersetzungen gab. 1944 folgte "Ministerium der Angst" ("Ministry of Fear") nach der Vorlage von Graham Greene.

Ebenfalls Beachtung fanden zwei Filme mit Edward G. Robinson in der Hauptrolle, "Gefährliche Begegnung" ("The Woman in the Window", 1944) und "Straße der Versuchung" ("Scarlet Street", 1945), während unter Langs Kinobeiträgen der 1950er Jahre der Polizeifilm "Heißes Eisen" ("The Big Heat", 1953) mit Glenn Ford herausragte.

Von Anfang an hatte Lang in den USA mit Einschränkungen zu kämpfen. So durfte er in „Blinde Wut“ (1936) keine schwarzen Opfer und keine Kritik am Rassismus darstellen. Wegen seiner antinazistischen Filme und seiner Bekanntschaft mit Brecht und Hanns Eisler geriet er ins Blickfeld des vorgeblichen Kommunistenjägers McCarthy.

1956 kehrte Lang nach Europa zurück und drehte für den Produzenten Artur Brauner seine letzten Filme. Dem Zweiteiler "Der Tiger von Eschnapur" / "Das indische Grabmal" (1959), der auf einem stark abgewandelten Lang-Drehbuch von 1921 basierte, folgte mit "Die 1000 Augen des Dr. Mabuse" (1960) ein weiterer "Mabuse"-Film. In letzterem zeichnete Lang ein Sittenbild der frühen Bundesrepublik Deutschland: Große, scheinbar tote, vergessene Verbrecher, die im Hintergrund weiter wirken; ein Hotel als Beobachtungsapparat und Metapher für Totalitarismus; willige Handlanger und Vollstrecker; ein scheinbarer Frieden, der nur mühsam schwelende Konflikte verdeckt; eine Atmosphäre der Künstlichkeit und großspurig gespielten Lockerheit. Die drei gemeinsamen Filme mit Brauner erwiesen sich vor allem als kommerzielle, jedoch nicht als künstlerische Erfolge. Lang kehrte wieder in die USA zurück.

Seine letzte Regiearbeit vollzog sich innerhalb des Films eines anderen Regisseurs: In "Die Verachtung" ("Le mépris") von Jean-Luc Godard verkörperte Lang 1964 mit wienerisch gefärbtem Französisch sich selbst als Filmregisseur, der einen Film nach Homers "Odyssee" zu realisieren hat. Die entsprechenden Szenen inszenierte er selbst. Im Jahr zuvor, 1963, hatte er einen Ehrenpreis bei der Verleihung des Deutschen Filmpreises erhalten.

In seinen letzten Lebensjahren war Fritz Lang nahezu blind. 1971 heiratete er seine langjährige Lebensgefährtin Lily Latté. 1976 starb er in Beverly Hills und wurde im Forest Lawn Memorial Park in Hollywood beigesetzt.

Die Liste ist möglicherweise nicht vollständig und enthält nur Filme, bei denen Lang nicht selbst Regie führte.








</doc>
<doc id="1562" url="https://de.wikipedia.org/wiki?curid=1562" title="Formalisierung">
Formalisierung

Formalisierung bedeutet den Vorgang oder das Ergebnis des Formalisierens einer Sache. Etwas wird formalisiert, indem ihm eine (strenge) Form gegeben, es in einer (strengen) Form dargestellt oder bei seiner Durchführung eine vorgegebene (strenge) Form eingehalten wird. Mit strenger Form ist eine Schriftform gemeint, deren Zeichen in einer festgelegten Reihenfolge auf festgelegte Art und Weise verarbeitet werden. 

Von dieser allgemeinen Bedeutung können eine wissenschaftstheoretische und eine kulturwissenschaftliche Bedeutung unterschieden werden.

Wissenschaftstheoretisch bedeutet Formalisierung im weiteren Sinn „die Generalisierung einer (wissenschaftlichen) Aussage unter Absehung ihrer konkret-empirischen Bezüge“. In dieser Bedeutung ist die Formalisierung mit der Abstraktion verwandt.

Im engeren Sinn bedeutet Formalisierung die Beschreibung eines Phänomens oder die Formulierung einer Theorie in einer formalen Sprache, deren Axiomatisierung und – als letzte Stufe – die Kalkülisierung (siehe Formalisierte Theorie).

So ist die mathematische Logik durch Formalisierung gekennzeichnet. Man formalisiert ein System der Logik, indem man von der vorgegebenen Intension der in ihm vorkommenden Ausdrücke absieht und diese Ausdrücke in genau dem Sinn verwendet, den die Axiome bzw. die Regeln dieses Systems diesem vorschreiben. „Die Aussagenlogik und die Prädikatenlogik lassen sich als Formalisierungen des alltäglichen logischen Schließens ansehen.“

In der Sprachwissenschaft gibt es Versuche, durch formale Grammatiken wie zum Beispiel die generative Transformationsgrammatik die natürliche Sprache zu beschreiben.

Im kulturwissenschaftlichen Sinn kann unter Formalisierung die Auflösung zielgerichteter Handlungen in wiederholbare und übertragbare Verfahrensschritte bezeichnet werden, wie es durch die Regelung einer Ablauforganisation geschieht. Die Philosophin Sybille Krämer spricht in enger Anlehnung an die Mathematik von einem „typographischen, schematischen und interpretationsfreien Symbolgebrauch“, der Handlungen automatisierbar mache.

Die damit verbundene Einschränkung eines persönlichen und willkürlichen Vorgehens kann verschiedene Absichten haben: Sie kann aus Gründen der Transparenz und Gleichberechtigung geschehen (Politik, Recht, Mathematik) oder aus Gründen der Rationalisierung und Automatisierung (Wirtschaft, Militär, Technik, Informatik). Aufzeichnungen sind sowohl Grundlage als auch Resultat von Formalisierungen.

Verwandt mit der Formalisierung und historisch nicht von ihr trennbar ist die Ritualisierung, bei der streng festgelegte Abläufe zu einer Gewohnheit werden, die emotionale Sicherheit verleiht (vgl. Spielregel). Der Linguist Wolfgang Wildgen spricht davon, dass „(sinnentleerte) Teilhandlungen“ durch Formalisierung und Ritualisierung eine „Sinnfunktion“ bekommen.



</doc>
<doc id="1564" url="https://de.wikipedia.org/wiki?curid=1564" title="Fraktal">
Fraktal

Fraktal ist ein vom Mathematiker Benoît Mandelbrot 1975 geprägter Begriff ( ‚gebrochen‘, von ‚ (in Stücke zer-)‚brechen‘), der bestimmte natürliche oder künstliche Gebilde oder geometrische Muster bezeichnet. 

Diese Gebilde oder Muster besitzen im Allgemeinen keine ganzzahlige Hausdorff-Dimension, sondern eine gebrochene – daher der Name – und weisen zudem einen hohen Grad von Skaleninvarianz bzw. Selbstähnlichkeit auf. Das ist beispielsweise der Fall, wenn ein Objekt aus mehreren verkleinerten Kopien seiner selbst besteht. Geometrische Objekte dieser Art unterscheiden sich in wesentlichen Aspekten von gewöhnlichen glatten Figuren.

Der Begriff Fraktal kann sowohl substantivisch als auch adjektivisch verwendet werden. Das Gebiet der Mathematik, in dem Fraktale und ihre Gesetzmäßigkeiten untersucht werden, heißt "fraktale Geometrie" und ragt in mehrere andere Bereiche hinein, wie Funktionentheorie, Berechenbarkeitstheorie und dynamische Systeme. Wie der Name schon andeutet, wird der klassische Begriff der euklidischen Geometrie erweitert, was sich auch in den gebrochenen und nicht natürlichen Dimensionen vieler Fraktale widerspiegelt. Neben Mandelbrot gehören Wacław Sierpiński und Gaston Maurice Julia zu den namensgebenden Mathematikern.

In der traditionellen Geometrie ist eine Linie eindimensional, eine Fläche zweidimensional und ein räumliches Gebilde dreidimensional. Für die "fraktalen Mengen" lässt sich die Dimensionalität nicht unmittelbar angeben: Führt man beispielsweise eine Rechenoperation für ein fraktales Linienmuster tausende von Malen fort, so füllt sich mit der Zeit die gesamte Zeichenfläche (etwa der Bildschirm des Computers) mit Linien, und das eindimensionale Gebilde nähert sich einem zweidimensionalen.

Mandelbrot benutzte den Begriff der verallgemeinerten Dimension nach Hausdorff und stellte fest, dass fraktale Gebilde meist eine nicht-ganzzahlige Dimension aufweisen. Sie wird auch als fraktale Dimension bezeichnet. Daher führte er folgende Definition ein:

Jede Menge mit nicht-ganzzahliger Dimension ist also ein Fraktal. Die Umkehrung gilt nicht, Fraktale können auch ganzzahlige Dimension besitzen, beispielsweise die Peano-Kurve oder die Sierpinski-Pyramide.

Besteht ein Fraktal aus einer bestimmten Anzahl von verkleinerten Kopien seiner selbst und ist dieser Verkleinerungsfaktor für alle Kopien derselbe, so verwendet man die Ähnlichkeitsdimension formula_1, die in solchen einfachen Fällen der anschaulichen Berechnung der Hausdorff-Dimension entspricht.

Die Selbstähnlichkeit kann aber auch nur im statistischen Sinn bestehen. Man spricht dann von Zufallsfraktalen.

Etwas abstrakter betrachtet wird diese Dimension, wenn man folgende Größen einführt:

Selbstähnlichkeit, eventuell im statistischen Sinn, und zugehörige fraktale Dimensionen charakterisieren also ein fraktales System bzw. bei Wachstumsprozessen sog. „fraktales Wachstum“ (z. B. Diffusionsbegrenztes Wachstum).

Die einfachsten Beispiele für selbstähnliche Objekte sind Strecken, Parallelogramme (u. a. Quadrate) und Würfel, denn sie können durch zu ihren Seiten parallele Schnitte in verkleinerte Kopien ihrer selbst zerlegt werden. Diese sind jedoch keine Fraktale, weil ihre Ähnlichkeitsdimension und ihre Lebesgue’sche Überdeckungsdimension übereinstimmen. Ein Beispiel für ein selbstähnliches Fraktal ist das Sierpinski-Dreieck, welches aus drei auf die Hälfte verkleinerten Kopien seiner selbst aufgebaut ist. Es hat somit die Ähnlichkeitsdimension formula_5, während die Lebesgue’sche Überdeckungsdimension gleich 1 ist.

Die Selbstähnlichkeit muss nicht perfekt sein, wie die erfolgreiche Anwendung der Methoden der fraktalen Geometrie auf natürliche Gebilde wie Bäume, Wolken, Küstenlinien usw. zeigt. Die genannten Objekte sind in mehr oder weniger starkem Maß selbstähnlich strukturiert (ein Baumzweig sieht ungefähr so aus wie ein verkleinerter Baum), die Ähnlichkeit ist jedoch nicht streng, sondern stochastisch. Im Gegensatz zu Formen der euklidischen Geometrie, die bei einer Vergrößerung oft flacher und damit einfacher werden (etwa ein Kreis), können bei Fraktalen immer komplexere und neue Details auftauchen.

Fraktale Muster werden oft durch rekursive Operationen erzeugt. Auch einfache Erzeugungsregeln ergeben nach wenigen Rekursionsschritten schon komplexe Muster.

Dies ist zum Beispiel am Pythagoras-Baum zu sehen. Ein solcher Baum ist ein Fraktal, welches aus Quadraten aufgebaut ist, die so angeordnet sind wie im Satz des Pythagoras definiert.

Ein weiteres Fraktal ist das Newton-Fraktal, erzeugt über das zur Nullstellenberechnung verwendete Newton-Verfahren.

Beispiele für Fraktale im dreidimensionalen Raum sind der Menger-Schwamm und die Sierpinski-Pyramide auf Basis des Tetraeders (so wie das Sierpinski-Dreieck auf dem gleichseitigen Dreieck basiert). Entsprechend lassen sich auch in höheren Dimensionen Fraktale nach Sierpinski bilden – bspw. basierend auf dem Pentachoron im vierdimensionalen Raum.

Durch ihren Formenreichtum und den damit verbundenen ästhetischen Reiz spielen sie in der digitalen Kunst eine Rolle und haben dort das Genre der Fraktalkunst hervorgebracht. Ferner werden sie bei der computergestützten Simulation formenreicher Strukturen, beispielsweise realitätsnaher Landschaften, eingesetzt. Um in der Funktechnik verschiedene Frequenzbereiche zu empfangen, werden Fraktalantennen genutzt.

Fraktale Erscheinungsformen findet man auch in der Natur. Dabei ist jedoch die Anzahl der Stufen von selbstähnlichen Strukturen begrenzt und beträgt oft nur drei bis fünf. Typische Beispiele aus der Biologie sind die fraktalen Strukturen bei der grünen Blumenkohlzüchtung Romanesco und bei den Farnen. Auch der Blumenkohl hat einen fraktalen Aufbau, wobei man es diesem Kohl auf den ersten Blick häufig gar nicht ansieht. Es gibt aber immer wieder einige Blumenkohlköpfe, die dem Romanesco im fraktalen Aufbau sehr ähnlich sehen.

Weit verbreitet sind fraktale Strukturen ohne strenge, aber mit statistischer Selbstähnlichkeit. Dazu zählen beispielsweise Bäume, Blutgefäße, Flusssysteme und Küstenlinien. Im Fall der Küstenlinie ergibt sich als Konsequenz die Unmöglichkeit einer exakten Bestimmung der Küstenlänge: Je genauer man die Feinheiten des Küstenverlaufes misst, umso größer ist die Länge, die man erhält. Im Falle eines mathematischen Fraktals, wie beispielsweise der Kochkurve, wäre sie unbegrenzt.

Fraktale finden sich auch als Erklärungsmodelle für chemische Reaktionen. Systeme wie die Oszillatoren (Standardbeispiel Belousov-Zhabotinsky-Reaktion) lassen sich einerseits als Prinzipbild verwenden, andererseits aber auch als Fraktale erklären. Ebenso findet man fraktale Strukturen auch im Kristallwachstum und bei der Entstehung von Mischungen, z. B. wenn man einen Tropfen Farblösung in ein Glas Wasser gibt. Die Lichtenberg-Figur zeigt ebenfalls fraktale Struktur.

Das Auffasern von Bast lässt sich über die fraktale Geometrie von Naturfaserfibrillen erklären. Insbesondere ist die Flachsfaser eine fraktale Faser.

Fraktale können auf viele verschiedene Arten erzeugt werden, doch alle Verfahren enthalten ein rekursives Vorgehen:


Es gibt fertige Programme, sogenannte Fraktalgeneratoren, mit denen Computeranwender auch ohne Kenntnis der mathematischen Grundlagen und Verfahren Fraktale darstellen lassen können.

Das "optionale", also nicht notwendige "F" wird im Allgemeinen als Strecke benutzt, die durch eine Anweisungsfolge ersetzt wird. Wie das "F" stehen auch andere groß geschriebene Buchstaben wie "R" und "L" für einen Streckenabschnitt, der ersetzt wird. "+" und "−" stehen für einen bestimmten Winkel, der im Uhrzeigersinn oder gegen den Uhrzeigersinn läuft. Das Symbol "|" bezeichnet eine Kehrtwendung des Zeichenstiftes, also eine Drehung um 180°. Gegebenenfalls setzt man dafür ein entsprechendes Vielfaches des Drehwinkels ein.

 F → R
 R → +R--L+

F ist eine einfache Strecke zwischen zwei Punkten. F → R heißt, dass die Strecke F durch R ersetzt wird. Dieser Schritt ist notwendig, da es zwei rekursive Ersetzungen R und L besitzt, die sich gegenseitig enthalten. Im Weiteren wird wie folgt ersetzt:

Ab einem bestimmten Abschnitt muss dieser Ersetzungsprozess abgebrochen werden, um eine Grafik zu bekommen:

Dabei stellen r und l jeweils eine fest vorgegebene Strecke dar.

Daneben spielen in der Natur auch „Zufallsfraktale“ eine große Rolle. Diese werden nach probabilistischen Regeln erzeugt. Dies kann etwa durch Wachstumsprozesse geschehen, wobei man beispielsweise diffusionsbegrenztes Wachstum (Witten und Sander) und „Tumorwachstum“ unterscheidet. Im ersten Fall entstehen baumartige Strukturen, im letzten Fall Strukturen mit runder Form, je nachdem, in welcher Weise man die neu hinzukommenden Teilchen an die schon vorhandenen Aggregate anlagert. Wenn die fraktalen Exponenten nicht konstant sind, sondern z. B. von der Entfernung von einem zentralen Punkt des Aggregats abhängen, spricht man von sog. "Multifraktalen".




</doc>
<doc id="1565" url="https://de.wikipedia.org/wiki?curid=1565" title="Francis Ford Coppola">
Francis Ford Coppola

Francis Ford Coppola (* 7. April 1939 in Detroit, Michigan) ist ein US-amerikanischer Regisseur und Produzent. Als Regisseur von Klassikern wie "Der Pate" und "Apocalypse Now" zählt er zu den bedeutendsten Filmschaffenden des US-amerikanischen Kinos.

Da er dem etablierten Studiobetrieb Hollywoods kritisch gegenüberstand, gründete Coppola 1969 das unabhängige Filmstudio American Zoetrope, wo unter anderem die ersten Filme von George Lucas verwirklicht wurden. Nachdem er mit dem Studio in finanzielle Schwierigkeiten geraten war, inszenierte Coppola ab Mitte der 1980er Jahre auch kommerziell ausgerichtete Filme mit weniger künstlerischem Anspruch.

Francis Ford Coppola wurde als Sohn von Carmine Coppola und Italia Pennino in Detroit im US-Bundesstaat Michigan geboren. Sein Vater war Musiker und Komponist, seine Mutter eine italienische Schauspielerin. Francis hat zwei Geschwister, den älteren Bruder August Floyd Coppola, Vater des 1964 geborenen Schauspielers Nicolas Cage, sowie die jüngere Schwester Talia Shire (geb. Talia Rose Coppola). Sein zweiter Name stammte aus einer früheren Arbeit seines Vaters als offizieller Arrangeur für die "„Ford Sunday Evening Hour“" des CBS Radio. Unmittelbar nach Coppolas Geburt zog die Familie nach New York, NY um.

Im Alter von zehn Jahren erkrankte Coppola während eines Ausflugs der Pfadfinder an Polio, wodurch sich eine Lähmung seiner linken Körperhälfte einstellte. Die folgenden neun Monate im Bett verbringend, schaute Coppola häufiger Fernsehen und führte seine ersten Marionettenspiele zur Unterhaltung anderer vor. Coppola akzeptierte die Tatsache, dass er nie wieder werde laufen können, sein Vater jedoch besorgte ihm einen Physiotherapeuten, mit dessen Hilfe sich Coppolas Zustand allmählich so verbesserte, dass er wieder in der Lage war, die Schule zu besuchen. Nachdem sich Coppola nahezu vollständig erholt hatte, bekam er von seinen Eltern eine Super-8-Kamera geschenkt, mit der er seine ersten Filmerfahrungen machte.

Francis Ford Coppola schrieb sich in die New York Military Academy ein, in der Erwartung seines Vaters, dort das Spielen einer Tuba zu erlernen. In der Schule brachte er stets gute bis sehr gute Leistungen, zu seinen Kameraden verhielt er sich vorlaut, aber hilfsbereit. Von den Unterrichtsmethoden in der New York Military Academy war Coppola dennoch enttäuscht, insbesondere verabscheute er das Schulfach "Sport". Nach 18 Monaten auf der Highschool brach er den Unterricht ab und zog sich in Manhattan zurück. Nach seiner Rückkehr auf die Highschool spielte er wieder Tuba und schrieb auch seine ersten eigenen Stücke.

Nach seinem Abschluss erhielt Coppola einen Platz für ein Stipendium in der Hofstra University, wo zuvor bereits sein Bruder studiert hatte. Dort belegte Coppola verschiedene Theaterseminare und inszenierte erste Bühnenauftritte. Ein Film, der Coppola zu dieser Zeit inspirierte und beeindruckte, war Sergei Michailowitsch Eisensteins "Zehn Tage, die die Welt erschütterten" (1927).

Nach dem Studium im Jahr 1959 entschloss sich Coppola, auf der Filmhochschule University of California, Los Angeles weiterzustudieren. Während des Studiums traf Coppola auf den unabhängigen Autorenfilmer Roger Corman und wurde dessen Assistent. Er half Corman bei der Produktion einiger seiner Filme, darunter zum Beispiel "The Terror – Schloß des Schreckens", nahm dabei die Aufgaben eines Schriftstellers, Produktionsassistenten, Tonmeisters und Regisseurs wahr und eignete sich so praktische Erfahrungen im Filmbereich an. Während seiner Studienzeit war er auch an Produktionen mehrerer kleiner Horror- und Erotikfilme beteiligt. Sein Regiedebüt gab Coppola 1961 mit dem Western "Das gibt es nur im Wilden Westen", danach drehte er 1963 den Horrorfilm "Dementia 13". Das Drehbuch dieses Schwarz-Weiß-Films, der von einem Serienmörder handelt, schrieb Coppola selbst. "Dementia 13" wurde mit einem Budget von rund 20.000 US-Dollar in Irland gedreht. Seine Filmstudien auf der University of California in Los Angeles beendete Coppola im Jahr 1968 mit der besten Note seines Jahrgangs.

Francis Ford Coppola nahm eine Stelle im Drehbuchteam von Warner Bros.-Seven Arts ein und wurde mit der Verfilmung des Broadway-Hits "Finian's Rainbow" von 1947 beauftragt. Im Juni 1968 begann Coppola mit den Dreharbeiten zu "Der goldene Regenbogen" und begegnete dort George Lucas, der an dem Samuel-Warner-Memorial-Stipendium teilnahm. Coppola nahm Lucas zu seinem Assistenten für Dreharbeiten an "Der goldene Regenbogen". Dabei ging Lucas mit einer Sofortbildkamera über das Set und schoss Bilder möglicher Kameraeinstellungen. Der Film floppte an den Kinokassen und der Umstand, dass ein Weißer dadurch bestraft wird, dass er zu einem Schwarzen wird, trug ihm zusätzlich den Vorwurf des Rassismus ein.

Bis zum Ende der Dreharbeiten waren Coppola und Lucas gute Freunde geworden. Der Regisseur erzählte Lucas, dass er plane, einen Road-Movie nach der Vorlage einer eigenen Kurzgeschichte namens "Echoes" zu drehen. "Liebe niemals einen Fremden" erzählt die Geschichte einer schwangeren Frau, die ihren Ehemann eines Nachts verlässt und sich mit einem Tramper auf eine ziellose Reise begibt. Dabei wollte Coppola selbst zusammen mit Lucas während einer Odyssee von New York nach Nebraska den Film drehen, möglichst unabhängig von Hollywoods Filmstudios. Da Lucas zu dieser Zeit sein eigenes Projekt mit "THX 1138" beginnen wollte, machte Coppola Lucas ein Angebot. Er versprach, eine Absprache mit Warner Bros.-Seven Arts zu treffen, damit Lucas vom Studio dafür bezahlt würde, ein Drehbuch für "THX 1138" zu schreiben. So konnte Lucas ihn als Assistenten bei "Liebe niemals einen Fremden" begleiten und während der Reise gleichzeitig an "THX 1138" arbeiten.

Coppola, der sich schon längere Zeit von Hollywoods Filmindustrie abwenden wollte, schlug Lucas vor, ein unabhängiges Studio zu gründen. Nach einer Tagung mit dem Filmemacher John Korty, der von seiner dritten Produktion in seinem eigenen Studio berichtete, beschlossen Lucas und Coppola das Studio Korty Films zu besichtigen. Coppola schaute sich noch andere unabhängige Studios an. So besichtigte er eine kleine Firma namens Lanterna Films in Dänemark und bekam ein Zoetrop als Andenken an seinen Besuch geschenkt. Bald darauf bestellte er zusammen mit Lucas hochwertige Filmausrüstung und kümmerte sich um Räumlichkeiten, die er 1969 in San Francisco gefunden hatte. Seinem Studio gab er den Namen American Zoetrope.

Dann wurde ihm „ein Angebot gemacht, das er nicht ablehnen konnte“ (CBS News): das Mafiaepos "Der Pate" ("The Godfather") von 1972, nach einem Roman von Mario Puzo. Dies wurde sein Durchbruch als Regisseur. Coppolas letzter großer Erfolg war 1979 das Vietnam-Epos "Apocalypse Now", in dem unter anderem auch Marlon Brando und Robert Duvall aus "Der Pate" mitspielten. Nach einer Drehdauer von 1976 bis 1979 konnte Coppola den Antikriegsfilm endlich fertigstellen. In der Produktionszeit kam es zu Differenzen mit dem Filmverleih United Artists. Trotzdem wurde es zu einem finanziellen und künstlerischen Erfolg. "Apocalypse Now" ist bis heute einer der einflussreichsten Filme der vergangenen fünfzig Jahre. Von dem grandiosen Flop "Einer mit Herz" konnte er sich jedoch nicht mehr erholen. Filme wie "Die Outsider" und "Rumble Fish" drehte er, um seine Schulden abzubezahlen. Die von ihm eingesetzten Schauspieler wurden als Brat Pack die Jungstars ihrer Generation. Sein Wunschfilm "Cotton Club" wurde erneut ein Misserfolg, und Coppola drehte "Peggy Sue hat geheiratet". Diesen Teufelskreis aus Flops und Auftragsarbeiten konnte er bis heute nicht durchbrechen. So sah er sich entgegen vorherigen Beteuerungen doch gezwungen, "Der Pate – Teil III" zu drehen. Gleiches gilt für den Kinderfilm "Jack" mit Robin Williams und die John-Grisham-Verfilmung "Der Regenmacher". Ein danach geplantes Projekt, das ihm schon lange am Herzen lag, "Megalopolis", kam nicht zustande. Dafür veröffentlichte er 2007 den weitaus persönlicheren "Jugend ohne Jugend", über Alter, Tod, Angst, Liebe, Sprache, Frühgeschichte, Traum, Reinkarnation und Atomwaffen. Mit dem vollständig digital gedrehten Film brachte er es auf die Titelseite der renommierten Cahiers. Die Kritik in den Vereinigten Staaten reagierte ablehnend.

Coppola produzierte zahlreiche Kinofilme anderer Regisseure, darunter George Lucas’ "American Graffiti" (1973), Akira Kurosawas "Kagemusha" (1980), Godfrey Reggios "Koyaanisqatsi" (1983), Paul Schraders "Mishima – Ein Leben in vier Kapiteln" (1985) und Tim Burtons "Sleepy Hollow" (1999). Für Jack Claytons mehrfach ausgezeichneten Film "Der große Gatsby" (1974) schrieb er das Drehbuch. Coppola gelang damit eine werkgetreue und dennoch Hollywood-gerechte Adaption von Fitzgeralds Roman an der unter anderem Truman Capote gescheitert war.

Finanzielle Einnahmen erzielt Coppola vor allem mit seinem Weingut.

Francis Ford Coppola ist seit 1963 mit der Dokumentarfilmerin Eleanor Coppola verheiratet. Er ist der Onkel der US-amerikanischen Schauspieler Nicolas Cage und Jason Schwartzman, Vater der Regisseure Sofia Coppola ("Lost in Translation") und Roman Coppola (Musikvideos und Kinofilme) sowie Bruder der Schauspielerin Talia Shire. Sein ältester Sohn, Gian-Carlo, kam 1986 bei einem Bootsunfall ums Leben.


Francis Ford Coppola erhielt bisher insgesamt fünf Oscars bei vierzehn Nominierungen. Er erhielt die Auszeichnung 1971 für das Drehbuch für (das nicht von ihm inszenierte) "Patton – Rebell in Uniform", 1972 für das Skript zu "Der Pate" sowie 1975 für Produktion, Regie und Drehbuch für "Der Pate – Teil II".





Coppola ist einer von nur vier Regisseuren, denen es gelang zwei Filme zu drehen, welche mit dem Oscar für den Besten Film ausgezeichnet wurden. ("Der Pate" und "Der Pate 2"). Die anderen Regisseure sind Clint Eastwood, Frank Capra und Miloš Forman.



(bester Film – Comedy/Musical)
(bester Film – Drama)
(bester Film – Drama)
(bester Film – Drama)
(bester Film – Drama)




</doc>
<doc id="1566" url="https://de.wikipedia.org/wiki?curid=1566" title="Friedrich Wilhelm Murnau">
Friedrich Wilhelm Murnau

Friedrich Wilhelm Murnau, auch F. W. Murnau, (* 28. Dezember 1888 als "Friedrich Wilhelm Plumpe" in Bielefeld; † 11. März 1931 in Santa Barbara, Kalifornien) gilt als einer der bedeutendsten deutschen Filmregisseure der Stummfilmära. Sein vom Expressionismus beeinflusstes Schaffen, seine psychologische Bildführung und die damals revolutionäre Kamera- und Montagearbeit Murnaus eröffneten dem jungen Medium Film neue Möglichkeiten. Zu seinen berühmtesten Werken zählen "Nosferatu – Eine Symphonie des Grauens" (1922), "Der letzte Mann" (1924), "Faust – eine deutsche Volkssage" (1926) und "Sonnenaufgang – Lied von zwei Menschen" (1927).

Friedrich Wilhelm Plumpe wuchs in einer wohlhabenden Bürgerfamilie auf; der Vater war Tuchfabrikant, die Mutter Lehrerin. 1892 zog seine Familie nach Kassel um. Von 1898 bis 1902 lebte Plumpe in der Kasseler Elfbuchenstraße. Nach dem Besuch des Gymnasiums in Kassel, begann er ein Studium der Philologie und Kunstgeschichte in Berlin und Heidelberg. Dort wurde bei einer Studentenaufführung der berühmte Regisseur Max Reinhardt auf ihn aufmerksam. Reinhardt ermöglichte ihm den Besuch der Max-Reinhardt-Schauspielschule und beschäftigte ihn als Schauspieler und Regieassistenten. Plumpe nahm den Künstlernamen Friedrich Wilhelm Murnau (nach dem Ort Murnau am Staffelsee) an. Dies war, neben dem künstlerischen Aspekt, auch ein klares Zeichen für den Bruch mit seinen Eltern, die seine Homosexualität genauso wie seine Schauspiel- und Regieambitionen nicht akzeptieren wollten. Zu seinen Künstlerfreunden gehörten unter anderem die Autorin Else Lasker-Schüler und die expressionistischen Maler der Gruppe Der blaue Reiter.

Am Ersten Weltkrieg nahm Murnau als Leutnant im 1. Garderegiment zu Fuß und ab 1917 als Kampfflieger teil, bis er absichtlich oder durch einen Navigationsfehler auf dem Gebiet der neutralen Schweiz landete. Dort wurde er zunächst in Andermatt interniert, konnte aber nach dem Gewinn eines Inszenierungswettbewerbs für das patriotische Schauspiel "Marignano" am Luzerner Theater arbeiten. Die Kriegserlebnisse waren für Murnau wie für viele seiner Generation prägend; sein damaliger Lebensgefährte Hans Ehrenbaum-Degele fiel an der Ostfront. Einige Kritiker sehen in Filmen wie "Nosferatu" noch Spuren der Kriegseindrücke.

1919 kehrte Murnau nach Berlin zurück und begann für den Film zu arbeiten. Sein erster Spielfilm, Der Knabe in Blau nach Motiven des Gemäldes The Blue Boy, ist heute wie auch einige seiner späteren Filme verschollen. Mit dem Film "Der Bucklige und die Tänzerin" begann eine höchst fruchtbare Zusammenarbeit mit dem Drehbuchautor Carl Mayer, der in der Folge noch für sechs weitere Filme Murnaus die Bücher schrieb. Andere Künstler, mit denen Murnau bevorzugt zusammenarbeitete, sind die Drehbuchautorin Thea von Harbou, der Kameramann Carl Hoffmann und der Schauspieler Conrad Veidt. Sein berühmtester Film aus dieser Zeit ist "Nosferatu, eine Symphonie des Grauens" von 1922 mit Max Schreck in der Titelrolle, eine Verfilmung von Bram Stokers "Dracula", die aber aufgrund von Lizenzproblemen umbenannt werden musste.

Der Erfolg seiner Filme brachte Murnau einen Vertrag bei der Universum Film (UFA) ein. Für die UFA inszenierte er als erstes 1924 den Film "Der letzte Mann", in dem Emil Jannings einen Hotelportier verkörpert, der zum Toilettenmann degradiert wird und daran zerbricht. Die in diesem Film von Murnau und dem Kameramann Karl Freund verwendete „entfesselte“ oder auch „fliegende“ Kamera befreite die Kamera von ihrer Statik und ermöglichte neue Perspektiven (um z. B. den Rauch einer Zigarette zu verfolgen, schnallte Freund die Kamera an eine Feuerwehrleiter und bewegte diese). Ferner führte Murnau in diesem Film die „subjektive Kamera“ ein, die das Geschehen mit den Augen einer handelnden Person wiedergibt. Murnaus Fähigkeit, mit rein filmischen Mitteln eine Geschichte zu erzählen, zeigt sich auch darin, dass er in diesem Film fast ganz auf Zwischentitel verzichten konnte, was für die Stummfilmzeit ungewöhnlich ist. Die Reihe seiner in Deutschland geschaffenen Filme schloss Murnau 1926 mit "Tartüff" nach Molière und "Faust – eine deutsche Volkssage" ab.

Murnaus Erfolge in Deutschland und vor allem die amerikanische Fassung von "Der letzte Mann" im Jahre 1925 hatten Hollywood auf ihn aufmerksam gemacht. Murnau erhielt ein Vertragsangebot des amerikanischen Produzenten William Fox, der ihm volle künstlerische Freiheit zusicherte. Sein erster in den USA inszenierter Film "Sunrise" nach der Erzählung "Die Reise nach Tilsit" von Hermann Sudermann gewann bei der allerersten Oscarverleihung 1929 drei Oscars, erfüllte jedoch die kommerziellen Erwartungen nicht ganz. Aus diesem Grunde und wegen der immer schwieriger werdenden wirtschaftlichen Situation der Firma Fox und der Lage in Hollywood an der Schwelle zum Tonfilm musste Murnau bei seinen folgenden Filmen zunehmend Eingriffe in sein künstlerisches Konzept hinnehmen; bei dem Film "City Girl" wurde er sogar als Regisseur abgelöst und ohne seinen Einfluss wurde nachträglich eine Tonfassung hergestellt.

Von den Zwängen Hollywoods enttäuscht, kündigte Murnau 1929 den Vertrag mit Fox. Nach einem ergebnislosen Versuch, wieder in Berlin mit der UFA ins Geschäft zu kommen, kaufte er sich eine Segelyacht, fest entschlossen, seinen nächsten Film allein nach seinen eigenen Vorstellungen zu realisieren, und fuhr nach Tahiti, um dort mit dem Regisseur und Dokumentarfilmer Robert J. Flaherty den Film "Tabu" zu drehen. Während der Dreharbeiten gab es erhebliche Schwierigkeiten mit der die Drehkosten finanzierenden Filmmaterial-Firma. Schließlich trennte sich Murnau von Flaherty, der stärkere Dokumentarfilmambitionen hatte, und produzierte den Film auf eigene Kosten. Der auf der Insel Bora Bora ausschließlich mit einheimischen Laiendarstellern gedrehte Film wurde zu einer stilbildenden Mischung aus Dokumentation und Melodram. Der Vertrieb des von Murnau selbst finanzierten Films, für den er sein gesamtes Vermögen aufgewendet und sich hoch verschuldet hatte, wurde von der Firma Paramount übernommen, die von dem Film so beeindruckt war, dass sie Murnau einen Zehnjahresvertrag anbot.

Die Premiere von "Tabu" am 18. März 1931 erlebte Murnau nicht mehr. Am 11. März 1931, kurz vor einer Europa-Promotion-Tour Murnaus, verlor sein Diener, der 14-jährige Filipino Garcia Stevenson, auf der Küstenstraße südöstlich von Santa Barbara (Kalifornien) die Beherrschung über das Auto, in dem die beiden fuhren, so dass es frontal mit einem Lkw zusammenstieß. Murnau starb wenige Stunden später an seinen Verletzungen. Nur elf Personen haben von ihm am 19. März Abschied genommen, darunter Greta Garbo.

Sein Leichnam wurde nach Deutschland überführt und auf dem Südwestkirchhof Stahnsdorf beigesetzt. Carl Mayer und der Regisseur Fritz Lang hielten die Grabreden. Unter den Trauergästen waren unter anderem Robert J. Flaherty, Emil Jannings, Erich Pommer und Georg Wilhelm Pabst. Seinen Grabstein gestaltete Karl Ludwig Manzel. Das Grab befindet sich im Block Schöneberg, Feld 3a, Erbbegräbnis 5. Es ist als Ehrengrab der Stadt Berlin gewidmet. Im Juli 2015 wurde das Grab von Grabräubern geöffnet und der einbalsamierte Kopf Murnaus entwendet. Der Schauspieler Gerd J. Pohl lobte daraufhin eine Belohnung aus, der Kopf blieb aber bislang verschollen.








</doc>
<doc id="1567" url="https://de.wikipedia.org/wiki?curid=1567" title="Frederic Vester">
Frederic Vester

Frederic Vester (* 23. November 1925 in Saarbrücken; † 2. November 2003 in München) war ein deutscher Biochemiker, Systemforscher, Umweltexperte, Universitätsprofessor und populärwissenschaftlicher Autor.

Frederic Vester studierte Chemie in Mainz, Paris und Hamburg, war Postdoktorand in Yale, Saarbrücken und München und habilitierte sich 1969 in Biochemie. Er gründete 1970 die „Studiengruppe für Biologie und Umwelt Frederic Vester GmbH“ in München, nach seinem Tod in „frederic vester GmbH“ umfirmiert, die sich durch staatliche und privatwirtschaftliche Beratungsaufträge finanzierte. Von 1982 bis 1989 war Vester Professor am "Lehrstuhl für Interdependenz von Technik und Gesellschaft" der Universität der Bundeswehr München, von 1989 bis 1991 Gastprofessor für angewandte Ökonomie an der HSG in St. Gallen. Als Autor und über seine Präsenz in den Medien hat Vester das Systemverständnis und das „Vernetzte Denken“ im deutschen Sprachraum populär gemacht.

Frederic Vester ist Vater der nach seiner Mutter benannten Schauspielerin Saskia Vester und ihrer Schwester Madeleine Vester, die ebenfalls als Schauspielerin tätig ist. Frederic Vester gehörte auch zu den Gründungsmitgliedern des BUND und war im deutschsprachigen Raum einer der Pioniere der Umweltbewegung. Vester ist auf dem Nordfriedhof in München begraben.

Unter Berufung auf die Kybernetik (bzw. Biokybernetik) hat Vester "systemisches" („"vernetztes"“) "Denken" propagiert, ein Ansatz, in dem die Eigenschaften eines Systems als ein vernetztes Wirkungsgefüge gesehen werden. Die einzelnen Faktoren verstärken oder schwächen andere Größen des Systems (Rückkopplung). Diese den ungeübten („linear denkenden“) Betrachter verwirrende Vernetzung kann mit Hilfe der Methodik seines "Sensitivitätsmodells" in mehreren Arbeitsschritten mit Softwareunterstützung analysiert und begreifbar gemacht werden. Auf diese Weise können z. B. positive, selbstverstärkende und negative, selbstregulierende Rückkopplungskreisläufe sicher erkannt werden. Einflussgrößen werden in ihrer Systemqualität sichtbar und bewertet (z. B. als stabilisierend, kritisch, puffernd oder empfindlich für äußere Einflüsse usw.). Durch Simulationen können langfristige oder spezielle Verläufe von Eigenschaften betrachtet werden. Auf der Grundlage eines so erarbeiteten Modells können Fragen nach sinnvollen Eingriffsmöglichkeiten und Steuerhebeln, zukünftiger Entwicklung oder möglichen Systemverbesserungen beantwortet werden. Die von Vester entwickelten Methoden und Bewertungen sind in seinem „"Sensitivitätsmodells Prof. Vester"“ zusammengefasst, das in Seminaren und als Software vermarktet wird und seit ca. 1980 in zum Teil umfangreichen Studien eingesetzt worden ist. Die Idee der in diesem Vorgehen zentralen Einflussmatrix wurde von anderen Autoren in der Prioritätenmatrix übernommen.

In seinem Bestseller „"Denken, Lernen, Vergessen"“ hat Vester Hypothesen formuliert, wonach verschiedene Typen von Lernern (Lerntypen) verschiedene „"Kanäle"“ (auditiv, visuell, haptisch, verbal-abstrakt) bevorzugen. Nach eigenen Untersuchungen stellte Vester eine Theorie der "Lernbiologie" auf, die die jedem Menschen eigenen Besonderheiten beim Aufnehmen, Verknüpfen und Speichern von Informationen beschreibt. Allerdings treten nach Vester die vier Lerntypen dieses Modells selten als idealtypische Ausprägungen auf, sondern meistens als Mischformen mit schwerpunktmäßiger Veranlagung (so zum Beispiel der "audio-visuelle Typ"). Diese Klassifikation ist wissenschaftlich umstritten, aber von Pädagogen im deutschen Sprachraum in großem Umfang rezipiert worden.

Vester wurde im Jahr 1993 in den "Club of Rome" aufgenommen. 1999 wurde Vesters Buch "Die Kunst, vernetzt zu denken. Ideen und Werkzeuge für den Umgang mit Komplexität" als "Der neue Bericht an den Club of Rome" veröffentlicht (aktuell 9. Auflage dtv 2013). In dem Buch fasst Vester seine wesentlichen Aussagen zusammen und zeigt das Vorgehen sowie das breite Anwendungsspektrum des Sensitivitätsmodells in praktischen Projekten auf.









</doc>
<doc id="1569" url="https://de.wikipedia.org/wiki?curid=1569" title="Frame">
Frame

Frame (engl. „Rahmen, Gestell“) steht für:

Weiteres:

Frame ist der Familienname folgender Personen:
Siehe auch:


</doc>
<doc id="1571" url="https://de.wikipedia.org/wiki?curid=1571" title="Film noir">
Film noir

Film noir [] (französisch für „schwarzer Film“) ist ein Terminus aus dem Bereich der Filmkritik. Ursprünglich wurde mit diesem Begriff eine Reihe von zynischen, durch eine pessimistische Weltsicht gekennzeichneten US-amerikanischen Kriminalfilmen der 1940er und 1950er Jahre klassifiziert, die im deutschen Sprachraum auch unter dem Begriff „Schwarze Serie“ zusammengefasst werden. Üblicherweise wird "Die Spur des Falken" von 1941 als erster und "Im Zeichen des Bösen" von 1958 als letzter Vertreter dieser klassischen Ära angesehen.

Die Wurzeln des Film noir liegen in erster Linie im deutschen expressionistischen Stummfilm und der US-amerikanischen Hardboiled-Kriminalliteratur der 1920er und 1930er Jahre. Dementsprechend sind die Filme der klassischen Ära üblicherweise durch eine von starken Hell-Dunkel-Kontrasten dominierte Bildgestaltung, entfremdete oder verbitterte Protagonisten sowie urbane Schauplätze gekennzeichnet.

Stil und Inhalte des Film noir fanden auch nach 1958 Verwendung. Diese später produzierten Filme mit Charakteristika der klassischen Ära werden häufig als „Neo-Noir“ bezeichnet. Die Verwendungsbeschränkung des Begriffs Film noir auf Filme US-amerikanischer Herkunft wurde zunehmend aufgegeben, so dass das Produktionsland für die Einordnung heutzutage oft keine Rolle mehr spielt.

Im Gegensatz zu anderen Filmgattungen wie Horrorfilm, Thriller oder Western wurde der Begriff "Film noir" auf Seiten der Filmpublizistik entwickelt und fasst rückwirkend eine Gruppe an vormals eher in losem Zusammenhang wahrgenommenen Filmen zusammen.

Erstmalige Verwendung fand die Formulierung in einem im August 1946 erschienenen Artikel des französischen Filmkritikers Nino Frank. Dieser behandelte eine Reihe von Hollywood-Filmen der frühen 1940er Jahre, die aufgrund eines Importverbots erst nach Ende des Zweiten Weltkriegs den Weg in die französischen Kinos gefunden hatten. Unter anderem befanden sich darunter auch die Filme "Die Spur des Falken" (1941), "Frau ohne Gewissen" (1944), "Laura" (1944) und "Murder, My Sweet" (1944). In diesen vier Produktionen glaubte Frank eine neue „dunklere“ Spielart des Kriminalfilms zu entdecken, die grundsätzlich mehr Augenmerk auf die Charakterisierung der Figuren als auf die Handlung legte. Er wies dabei u. a. auf den Einsatz von Off-Camera-Kommentaren hin, welche die Handlung fragmentieren und die „lebensechte“ Seite des Films hervorheben. Frank kreierte die Formulierung "Film noir" wahrscheinlich in Anspielung auf den Titel der Buchreihe "Série noire" des Pariser Gallimard-Verlags, in der seit 1945 Übersetzungen amerikanischer Hardboiled-Kriminalromane veröffentlicht wurden. Bis Ende der 1960er Jahre blieb die Verwendung des Begriffs allerdings im Wesentlichen auf Frankreich begrenzt. In den USA selbst wurden die betreffenden Filme bis dahin üblicherweise als "psychological melodrama" oder "psychological thriller" bezeichnet.

Bis heute besteht in der Filmwissenschaft kein Konsens darüber, wie der Film noir zu klassifizieren ist. In einem der ersten das Thema betreffenden von den Franzosen Raymond Borde und Étienne Chaumeton verfassten Essay aus dem Jahr 1955 heißt es dazu:

In ihrem Buch "Hollywood in the Forties" von 1968 bezeichneten die Briten Charles Higham und Joel Greenberg den Film noir indessen als eigenständiges Genre:

Dagegen postulierte im Jahr 1970 der ebenfalls britische Kritiker Raymond Durgnat:

Eine ähnliche Ansicht vertrat 1972 der Amerikaner Paul Schrader, indem er den Film noir mit Stilrichtungen wie der Nouvelle Vague oder dem Italienischen Neorealismus verglich. Wie diese sei auch der Film noir als ein zeitlich begrenztes und vorrangig durch motivische und stilistische Merkmale gekennzeichnetes Phänomen zu sehen.

Da keine eindeutige Definition des Film noirs existiert, ist naturgemäß auch die Eingrenzung des Begriffes relativ unscharf. Die Meinungen, ob ein bestimmter Film als Film noir einzuordnen ist, können zum Teil sehr unterschiedlich ausfallen.

So wird zwar beispielsweise der zeitliche Rahmen der klassischen Ära üblicherweise auf die 1940er und 1950er Jahre begrenzt, doch sowohl Vorläufer (z. B. "Gehetzt" [1937]) als auch jüngere Filme (z. B. "Explosion des Schweigens" [1961]) können unter den Begriff fallen. Auch bezüglich der Herkunft als definierender Faktor besteht keine Einigkeit. Zwar zählen viele Film-noir-Experten wie beispielsweise Paul Schrader oder Alain Silver und Elizabeth Ward ausdrücklich nur Hollywood-Produktionen zum klassischen Film noir, manche Filmwissenschaftler wie James Naremore oder Andrew Spicer fassen den Begriff aber weiter und beziehen insbesondere auch britische und französische Produktionen mit ein (siehe Abschnitt „Film noir außerhalb der Vereinigten Staaten“).

Auch inhaltlich existieren keine definitiven Kriterien für die Zuordnung. Selbst die weit verbreitete Ansicht, dass es sich bei Films noirs ausschließlich um Kriminalfilme handle, ist keineswegs allgemein akzeptiert. So wendete schon Nino Frank den Begriff explizit auch auf Billy Wilders "Das verlorene Wochenende" (1945) an, ein „Problemfilm“-Melodrama über einen Alkoholiker. Zwar wurde dieser Film in späteren Abhandlungen zum Thema weitgehend ignoriert, dafür wurden im Lauf der Zeit unter anderem sogar einige Western (z. B. "Verfolgt" [1947]) oder Historienfilme (z. B. "Dämon von Paris" [1949]) dem klassischen Film-noir-Kanon zugeordnet.

Die Ästhetik des Film noir ist stark vom expressionistischen Stummfilm der 1920er Jahre mit seinen scharfen Hell-Dunkel-Gegensätzen, Kamerafahrten mit „entfesselter Kamera“ und verzerrten Kameraperspektiven geprägt. Expressionistische Werke wie "Der letzte Mann" (1924) oder "Metropolis" (1927) stießen in der amerikanischen Filmindustrie auf große Bewunderung und beeinflussten dort schon in den frühen 1930er Jahren die Horrorfilme der Universal Studios. Einen bedeutenden Anteil an dieser Entwicklung hatte der "Metropolis"-Kameramann Karl Freund, der u. a. bei "Dracula" (1931) und "Mord in der Rue Morgue" (1932) mitwirkte. Später fotografierte Freund die Films noirs "Der unbekannte Geliebte" (1946) und "Gangster in Key Largo" (1948).

Während der 1930er Jahre führten die vielversprechenden Möglichkeiten Hollywoods und der aufkommende Nationalsozialismus zur Auswanderung zahlreicher weiterer Filmemacher deutscher und österreichisch-ungarischer Herkunft. Diese brachten ihre Techniken der Bildgestaltung mit nach Hollywood und verstärkten dort den expressionistischen Einfluss. Besonders hervorzuheben ist hier der Regisseur Fritz Lang, dessen erste beiden Hollywood-Filme "Blinde Wut" (1936) und "Gehetzt" (1937) bereits viele visuelle und thematische Elemente des Film noir vorwegnehmen. Früh prägend für den visuellen Stil des Film noir war auch der Kameramann Theodor Sparkuhl, dessen Low-key-Fotografie in "Zum Leben verdammt" (1941), "Der gläserne Schlüssel" und "Der schwarze Vorhang" (beide 1942) mit der kontrastarmen Beleuchtung vieler Gangsterfilme der 1930er Jahre brach.

Weitere für den Film noir wichtige Hollywood-Immigranten waren vor allem die Regisseure Robert Siodmak, Billy Wilder, Otto Preminger, John Brahm, Max Ophüls, Fred Zinnemann, William Dieterle, Edgar G. Ulmer, Curtis Bernhardt, Rudolph Maté, Anatole Litvak und Michael Curtiz, die zahlreiche bedeutende Filme des klassischen Kanons schufen, sowie der stilbildende Kameramann John Alton, der unter anderem die Films noirs von Anthony Mann und Joseph H. Lewis fotografierte.

Ebenfalls wegweisend für den Film noir war die Bewegung des poetischen Realismus im französischen Film der späten 1930er Jahre, der die bitteren sozialen und politischen Realitäten der unteren sozialen Schichten thematisierte. Werke wie Julien Duviviers "Pépé le Moko – Im Dunkel von Algier" (1936), Jean Renoirs "Bestie Mensch" (1938) oder Marcel Carnés "Hafen im Nebel" (1938) und "Der Tag bricht an" (1939) (alle mit Jean Gabin in der Hauptrolle) mischten romantische Krimihandlungen mit heroischen, dem Untergang geweihten Protagonisten. Wenn auch nicht ganz so misanthropisch und zynisch wie der Film noir, gibt es doch eine klare Verwandtschaft zwischen beiden Bewegungen. Folgerichtig lieferten "Bestie Mensch" und "Der Tag bricht an" die Vorlagen für die Films noirs "Lebensgier" (1954) und "Die Lange Nacht" (1947). "Straße der Versuchung" (1945) basiert ebenfalls auf einem Film Renoirs.

Im Gegensatz zu ihren deutschen Kollegen spielten die französische Filmemacher selbst im Hollywood der 1940er und 1950er Jahre allerdings kaum eine Rolle. In Bezug auf den Film noir ist hier lediglich Jean Renoirs Melodrama "Die Frau am Strand" (1947) zu nennen, das von einigen Kritikern dem klassischen Zyklus zugeordnet wird. Der bedeutende Film-noir-Regisseur Jacques Tourneur wiederum war zwar von Geburt Franzose, erlernte sein Handwerk aber in Hollywood.

Nach dem Ende des Zweiten Weltkriegs nahm auch der italienische Neorealismus mit seiner quasi-dokumentarischen Authentizität Einfluss auf die Entwicklung des Film noir. Filmemacher wie Jules Dassin "(Stadt ohne Maske, Gefahr in Frisco)" und Elia Kazan "(Bumerang, Unter Geheimbefehl)" verlegten ihre Drehorte von den Filmstudios hinaus auf die Straße.

Neben den dokumentarischen Stilmitteln des Neorealismus hinterließen auch authentische Dokumentarfilme ihre Spuren im Film noir. Louis de Rochemont, der mit der Dokumentarreihe The March of Time Bekanntheit erlangt hatte, produzierte mit "Das Haus in der 92. Straße" (1946) den ersten Vertreter der sogenannten Semidocumentaries (etwa „halbdokumentarische Filme“). Dessen Erfolg zog eine Serie von Spielfilmen nach sich, in denen die Ermittlungsarbeit von Polizisten und anderen Staatsbediensteten gezeigt und von einem Sprecher kommentiert wurde, darunter "Geheimagent T" (1947), "Stadt ohne Maske" (1948) und "Unter Geheimbefehl" (1950).

Den hauptsächlichen literarischen Einfluss auf den Film noir hatte die „Hardboiled“-Schule amerikanischer Kriminalliteratur, die von Autoren wie Dashiell Hammett seit den 1920er Jahren geprägt und meist in Pulp-Magazinen wie "Black Mask" publiziert wurde. Die frühen Film noir-Meisterwerke "Die Spur des Falken" (1941) und "Der gläserne Schlüssel" (1942) basieren beide auf Romanen von Hammett. Bereits 1931, ein Jahrzehnt vor Beginn der „klassischen Ära“, diente eine von Hammetts Geschichten als Grundlage des Gangsterfilms "Straßen der Großstadt" von Regisseur Rouben Mamoulian und Kameramann Lee Garmes. Dieser Film gilt heute aufgrund seines visuellen Stils als wichtiger Vorläufer des Film Noir.

Bald nach Erscheinen seines ersten Romans "The Big Sleep" im Jahr 1939 wurde Raymond Chandler zum bekanntesten Autor der "Hardboiled"-Schule. Mehrere Romane Chandlers wurden zu Films noirs verarbeitet, beispielsweise "Murder, My Sweet" (1944), "Tote schlafen fest" (1946) und "Die Dame im See" (1947). Chandler war für den Film noir auch ein wichtiger Drehbuchautor, der die Skripte zu "Frau ohne Gewissen" (1944) und "Die blaue Dahlie" (1946) sowie die Erstfassung von "Der Fremde im Zug" (1951) verfasste.

Während Hammett und Chandler die meisten ihrer Geschichten auf die Figur des Privatdetektivs ausrichteten, stellte James M. Cain in seinen Werken weniger heldenhafte Protagonisten dar und fokussierte mehr auf die psychologische Studie als auf die Aufklärung des Verbrechens. Cains Romane lieferten unter anderem die Vorlage für die Films noirs "Frau ohne Gewissen" (1944), "Solange ein Herz schlägt" (1945) und "Im Netz der Leidenschaften" (1946).

Ein weiterer einflussreicher Autor war in den 1940er Jahren Cornell Woolrich (auch unter den Pseudonymen George Hopley und William Irish bekannt). Kein anderer Autor steuerte so viele Vorlagen zu Films noirs bei wie er: Silver & Ward nennen insgesamt 13, darunter "Zeuge gesucht" (1944), "Vergessene Stunde" (1946), "Die Nacht hat tausend Augen" (1948) und "Das unheimliche Fenster" (1949).

Ähnlich essenziell für den Film noir ist das Werk von W. R. Burnett. Burnetts erster Roman "Little Caesar" diente als Vorlage für den Gangsterfilm-Klassiker "Der kleine Cäsar" (1931), außerdem schrieb er die Dialoge für "Scarface" (1932). Ebenfalls im Jahr 1932 entstand "The Beast of the City" nach einer Burnett-Vorlage. Trotz seines frühen Entstehungsjahres wird dieser Film von manchen Kritikern bereits als Film noir angesehen. Während der „klassischen Ära“ lieferte Burnett dann als Roman- oder Drehbuchautor die Basis für sechs weitere Filme, die heute als Films noirs angesehen werden, darunter "Entscheidung in der Sierra" (1941) und "Asphalt-Dschungel" (1950).

Weitere wichtige Autoren, die nicht der Hardboiled-Schule angehörten aber die literarischen Vorlagen für Films noirs lieferten, waren z. B. Eric Ambler ("Von Agenten gejagt" [1943], "Die Maske des Dimitrios" [1944]), Graham Greene ("Die Narbenhand" [1942], "Ministerium der Angst" [1944]) und Ernest Hemingway ("Rächer der Unterwelt" [1946], "Menschenschmuggel" [1950]).

Nach Ansicht der meisten Experten ist John Hustons Detektivfilm "Die Spur des Falken" (1941) als erster klassischer Film noir anzusehen. (Allerdings wird in diesem Zusammenhang gelegentlich auch der kurz zuvor erschienene, aber weniger bekannte "Stranger on the Third Floor" aus dem Jahr 1940 genannt.) Der Film, in dem Humphrey Bogart und Mary Astor als Hauptdarsteller auftreten, erscheint visuell zwar noch konventionell, besitzt jedoch mit dem Privatdetektiv und einer Reihe exzentrischer Protagonisten typische Merkmale späterer Films noirs.

Damit beginnt auch die erste von drei Phasen des amerikanischen Film noir, die der Kritiker Paul Schrader unterscheidet, die „wartime period“ („Phase der Kriegsjahre“) etwa von 1941 bis 1946. In diesen Jahren ist der Charakter des Privatdetektivs oder des sogenannten „einsamen Wolfs“ vorherrschend. Bücher von Chandler und Hammett bilden häufig die Vorlage, und eine Reihe von Darstellern etabliert sich im Film noir, so Humphrey Bogart und Lauren Bacall ("Tote schlafen fest", 1946) und Alan Ladd und Veronica Lake ("Die Narbenhand" und "Der gläserne Schlüssel", beide 1942). Während einige Films noirs von Regisseuren stammten, die bereits etabliert waren, wie zum Beispiel Tay Garnett ("Im Netz der Leidenschaften," 1946), Fritz Lang ("Gefährliche Begegnung," 1944) oder Howard Hawks ("Tote schlafen fest," 1946), dienten andere ihren Regisseuren als Karrieresprungbrett, darunter John Huston, Otto Preminger ("Laura", 1944), Billy Wilder ("Frau ohne Gewissen", 1944) und Edward Dmytryk ("Murder, My Sweet", 1944). Wilders "Frau ohne Gewissen" markiert für Schrader den Übergang von der ersten zur zweiten Phase des Film noir.

Als zweite Phase der „Schwarzen Serie“ nennt Schrader die „post-war realistic period“ („realistische Nachkriegsphase“) etwa von 1945 bis 1949. Die Filme aus dieser Zeit handeln stärker vom Verbrechen auf der Straße, von korrupten Politikern und von alltäglicher Ermittlungsarbeit. Die Helden sind weit weniger romantisch als ihre Vorgänger, und das urbane Erscheinungsbild der Filme realistischer. Vertreter dieser Phase sind "Das Haus in der 92. Straße" und "Rächer der Unterwelt" (beide 1946), "Der Todeskuß" und "Geheimagent T" (beide 1947) sowie "Stadt ohne Maske" (1948).

Die dritte und letzte Phase, etwa von 1949 bis 1953, ist laut Schrader die „period of psychotic action and suicidal impulse“ („Phase der psychotischen Handlungen und selbstmörderischen Triebe“). Die Persönlichkeit der Figuren löst sich auf, psychotische Mörder sind nun oft die Hauptfiguren. Ästhetisch und soziologisch sind dies für Schrader die durchdringendsten Filme, die häufig als B-Produktionen entstehen. Zu dieser Phase zählt Schrader Joseph H. Lewis’ "Gefährliche Leidenschaft", Otto Premingers "Faustrecht der Großstadt", Gordon Douglas’ "Den Morgen wirst du nicht erleben" (alle 1950) und Fritz Langs "Heißes Eisen" (1953).

Als 1955 "Rattennest" von Robert Aldrich herauskam, war das Phänomen Film noir „zum Stillstand gekommen“ (Schrader). Das Amerika der Eisenhower-Ära wollte positivere Abbildungen seines „way of life“ sehen, und zugleich waren Fernsehen und Farbfilm auf dem Vormarsch. Orson Welles’ Meisterwerk "Im Zeichen des Bösen" aus dem Jahr 1958 wird meistens als Schlusspunkt des klassischen Film noir angesehen. Allerdings wurden und werden auch seit Ende der klassischen Phase gelegentlich immer noch Filme produziert, die thematische, visuelle oder andere Elemente des Film noir aufgreifen. Diese werden heutzutage meist mit dem Begriff „Neo-Noir“ beschrieben. Darüber hinaus wurden bereits während der klassischen Phase auch außerhalb der Vereinigten Staaten Filme produziert, die mittlerweile von vielen Experten als Film noir klassifiziert werden (siehe Abschnitt „Film noir außerhalb der Vereinigten Staaten“).

Kriminalität, insbesondere Mord, ist ein Kernelement fast aller Films noirs, wobei häufig Motive wie Geldgier oder Eifersucht zum Tragen kommen. Die Aufklärung des Verbrechens, mit der ein Privatdetektiv, ein Polizeikommissar oder eine Privatperson befasst sein kann, ist ein häufiges, aber dennoch nicht vorherrschendes Thema. In anderen Plots mag es um einen Überfall, um Betrügereien oder um Verschwörungen und Affären gehen.

Films noirs drehen sich tendenziell um Helden (eigentlich Antihelden), die ungewöhnlich lasterhaft und moralisch fragwürdig sind. Sie werden häufig als "alienated" (dt. veräußert, entfremdet) beschrieben, oder in den Worten von Alain Silver und Elizabeth Ward, als „erfüllt von existenzieller Verbitterung“. Unter den archetypischen Figuren des Film noir finden sich hartgesottene "(„hardboiled“)" Detektive, Femmes fatales, korrupte Polizisten, eifersüchtige Ehemänner, unerschrockene Versicherungsangestellte sowie heruntergekommene Schriftsteller. Von diesen sind, wie das Gros der Neo-Noirs zeigt, der Detektiv und die Femme fatale diejenigen Charaktere, die am ehesten mit Film noir assoziiert werden, obwohl bei weitem nicht alle der klassischen Films noirs diese beiden Charaktere zeigen. Das Menschenbild ist pessimistisch. Oftmals steht jeder gegen jeden, alle sind nur auf ihren eigenen Vorteil bedacht und wer anderen vertraut, hat das Nachsehen.

Typisch für den Film noir sind die urbanen Schauplätze, wobei Los Angeles, San Francisco, New York City und Chicago wohl zu den beliebtesten zählen. Die Stadt steht meist sinnbildlich für ein Labyrinth, in dem die Protagonisten gefangen sind; Bars, Nachtclubs und Spielhöllen, heruntergekommene Fabrikhallen und einsame Straßenschluchten sind die üblichen Schauplätze der Handlung. Besonders die Spannungshöhepunkte in einigen Filmen liegen in komplexen, oft industriellen Szenerien, so zum Beispiel die Explosion in "Sprung in den Tod" (1949).

Rückblenden und Voice-over (=eine Erzählerstimme) zählen zu den häufig angewandten Erzähltechniken im Film noir. Oft dient ein Voice-over zur Unterstreichung der Ausweglosigkeit einer Situation oder eines Protagonisten und nimmt bereits zu Beginn des Films den fatalen Ausgang der Geschichte vorweg. Der Voice-over kann jedoch auch eine dem Zuschauer Sicherheit vermittelnde Funktion haben: „In der tödlich instabilen Noir-Welt dient der Voice-over oft als Halt […] er ist unser Wegweiser durch das Noir-Labyrinth“ (Foster Hirsch).

Des Weiteren sind Films noirs stärker als andere Hollywood-Produktionen auf die subjektive Sicht des Protagonisten fixiert, so beispielsweise in der Traum- und Halluzinationsszene in "Murder, My Sweet". "Die Dame im See" und "Die schwarze Natter" (beide 1947) sind über weite Strecken aus dem Blickwinkel der jeweiligen Hauptfigur gefilmt, sodass diese sich dem Betrachter lediglich in Spiegelbildern zeigt.

Der Stil des Film noir ist geprägt von einer Low-Key-Beleuchtung, die kräftige Hell-dunkel-Kontraste und auffällige Schattenbilder erzeugt. Weitere visuelle Eigenarten des Film noir sind sein Gebrauch von schrägen Kameraperspektiven, extreme Unter- oder Aufsichten und der häufige Einsatz von Weitwinkelobjektiven. Aufnahmen von Personen im Spiegel und durch gewölbtes Glas hindurch, sowie andere bizarre Effekte kennzeichnen den Film noir. In den späten 1940er Jahren entstanden zudem viele Filme an Originalschauplätzen. Dies wurde durch zunehmend empfindlicheres Filmmaterial und leichteres Equipment ermöglicht.

Dennoch ist der visuelle Stil im Film noir keineswegs homogen: So wird zwar die Schwarzweißfotografie häufig als essenziell angesehen, doch existieren mit "Todsünde" (1945), "Niagara" (1953) oder "Das Mädchen aus der Unterwelt" (1958) Beispiele für Farbfilme, die als Film noirs anerkannt sind. Auch Filme, die überwiegend in hellem Tageslicht fotografiert wurden, werden dem Film noir zugerechnet, beispielsweise "M" und "Reporter des Satans" (beide 1951).

Film noir ist grundsätzlich pessimistisch. In den Geschichten, die als charakteristisch angesehen werden, finden sich die Figuren in unvorhergesehenen Situationen gefangen und kämpfen gegen das Schicksal, das ihnen in der Regel ein schlimmes Ende beschert. Die Filme beschreiben eine Welt, der die Korruption innewohnt. Von vielen Filmtheoretikern wird Film noir mit der Gesellschaft seiner Zeit in den USA, die infolge des Zweiten Weltkriegs von Angst und Befremdung gekennzeichnet ist, in Verbindung gebracht. Nicholas Christopher umschreibt es so: „Es ist, als hätten der Krieg und die in seinem Gefolge auftretenden sozialen Umwälzungen Dämonen freigelassen, die in der Psyche der Nation eingesperrt gewesen waren.“

Anstatt sich auf einfache Gut-und-Böse-Konstruktionen zu beschränken, baut der Film noir moralische Zwickmühlen auf, die ungewöhnlich uneindeutig sind – zumindest für das typische Hollywood-Kino. Es sind keine Charaktere, die ihre Ziele nach klaren moralischen Vorgaben verfolgen: Der Ermittler in "Die Spur des Fremden" (1946), der wie besessen einen Nazi-Verbrecher aufspüren will, bringt andere Personen in Lebensgefahr, um die Zielperson zu fassen.

Die pessimistische Stimmung des Film noir bezeichnen Kritiker auch als dunkel und „überwältigend schwarz“ (Robert Ottoson). Paul Schrader schrieb, dass Film noir durch seine Stimmung definiert sei, eine Stimmung, die er als „hoffnungslos“ bezeichnet. Auf der anderen Seite sind gewisse Filme der Schwarzen Serie jedoch berühmt für die Schlagfertigkeit ihrer "Hardboiled"-Figuren, die mit sexuellen Anspielungen und selbstreflektivem Humor gespickt ist.

Viele Films noirs, insbesondere die auf Vorlagen der "Hardboiled"-Autoren basierenden, beleuchten das Milieu sozial benachteiligter Schichten oder stellen unterschiedliche soziale Schichten kontrastierend gegenüber. Filme wie "Der gläserne Schlüssel" (1942) und "Murder, My Sweet" (1944) sind laut Georg Seeßlen von „einer latenten gesellschaftskritischen Tendenz durchzogen, wie sich überhaupt an den Filmen, mehr noch aber an den an ihnen beteiligten Personen belegen läßt, daß das Genre der "private-eye"-Filme durchaus ein Derivat einer «linken» Strömung in Hollywood sein mochte.“ Regisseure, die sich mit sozialkritischen Themen hervortaten, waren Edward Dmytryk "(Murder, My Sweet, Cornered, Im Kreuzfeuer)", Robert Rossen "(Johnny O’Clock, Jagd nach Millionen)", Jules Dassin "(Zelle R 17, Die nackte Stadt)" und Joseph Losey "(Dem Satan singt man keine Lieder, M)". Mit Beginn der antikommunistischen McCarthy-Ära endeten die Karrieren dieser Regisseure jäh. Thom Andersen schuf für diese um „größeren psychologischen und sozialen Realismus“ bemühten Filme den Begriff Film gris.

James Naremore erweitert die Liste sozial engagierter Regisseure um Orson Welles und John Huston und sieht den Film noir der 1940er Jahre in zwei Lager gespalten, den des „Humanismus und politischen Engagements“ auf der einen, den des „Zynismus und der Misanthropie“ (vertreten etwa durch Alfred Hitchcock und Billy Wilder) auf der anderen Seite. Die „Schwarze Liste“ der McCarthy-Ära bedeutete das Ende einer „wichtigen Bewegung der kulturellen Geschichte Amerikas“, denn „ohne die Generation der Roten der 1940er Jahre würde die Tradition des Film noir kaum existieren.“ (Naremore)

Ende der 1940er und Anfang der 1950er Jahre schlug sich das geänderte politische Klima in den USA auch im Film noir nieder. Es entstand eine Reihe von Filmen, die vor der Gefahr kommunistischer „Unterwanderung“ warnten wie "The Woman on Pier 13" oder "I Was a Communist for the F.B.I." Detaillierte Milieuschilderungen wie in "Steckbrief 7-73" von John Berry (der ebenfalls auf die Schwarze Liste gesetzt wurde) verschwanden aus dem Film noir. Eine Ausnahme stellte Samuel Fullers "Polizei greift ein" dar, der eine antikommunistische Botschaft mit dem Porträt von Vertretern der amerikanischen Unterschicht verband. Im amerikanischen Detektivroman wurde Mickey Spillanes „faschistischer Rächer“ (Naremore) Mike Hammer zur populärsten Figur jener Jahre.

Im Großbritannien der Nachkriegszeit entstand eine ganze Reihe von Filmen, die dem Film noir stilistisch nahestehen, was zur Prägung des Begriffs „British Noir“ führte. Prominente Beispiele sind "Brighton Rock" von John Boulting nach einem Roman von Graham Greene, "Sträfling 3312" von Alberto Cavalcanti und "Ausgestoßen" von Carol Reed, alle aus dem Jahr 1947. Mit "Der dritte Mann" (1949, ebenfalls nach einem Graham-Greene-Roman) inszenierte Carol Reed auch den wohl berühmtesten Vertreter des British Noir. Aufgrund der Repressionen der McCarthy-Ära emigrierten auch einige Hollywood-Regisseure nach Großbritannien und schufen dort ebenfalls herausragende Werke des British Noir, wie z. B. "Der Wahnsinn des Dr. Clive" (1949) von Edward Dmytryk, "Teuflisches Alibi" von Joseph Losey und "Duell am Steuer" von Cy Endfield (beide 1957).

Auch in Frankreich wurden während der klassischen Ära zahlreiche Filme produziert, die in der Tradition des amerikanischen Film noir stehen. Einer der bekanntesten davon ist "Rififi" (1955) von dem aus den USA nach Frankreich emigrierten Jules Dassin. Weitere herausragende Beispiele sind Henri-Georges Clouzots "Unter falschem Verdacht" (1947) und "Die Teuflischen" (1954), Jacques Beckers "Goldhelm" (1952) und "Wenn es Nacht wird in Paris" (1954) sowie Jean-Pierre Melvilles "Drei Uhr nachts" (1956). Mit "Der Teufel mit der weißen Weste" (1962) und "Der zweite Atem" (1966) schuf Melville auch noch nach Ende der klassischen Ära bedeutende französische Film noirs.

Auch die Regisseure der Nouvelle Vague hatten in ihrem Kampf gegen das etablierte und langweilige französische „Qualitätskino“ der 1950er Jahre eine Vorliebe für die schnell und billig produzierten Filme der „Schwarzen Serie“. Besonders die jüngeren Vertreter wie Samuel Fuller und Robert Aldrich wurden von ihnen bewundert, und der Film noir geriet in ihrem Schaffen zur Inspirationsquelle. Louis Malles "Fahrstuhl zum Schafott" (1958), ein Vorläufer der Nouvelle Vague, nutzt ausgiebig die thematischen und stilistischen Vorgaben des Film noir, und François Truffauts "Schießen Sie auf den Pianisten" (1960) basiert auf einem Roman des Noir-Autoren David Goodis.

Sogar in Deutschland und Österreich entstanden einige Filme mit Noir-Attributen. Als bedeutendster davon gilt heute Peter Lorres "Der Verlorene" (1951). Weitere Beispiele sind "Epilog – Das Geheimnis der Orplid" (1950) von Helmut Käutner, der thematisch und stilistisch an "Der dritte Mann" anknüpft, "Abenteuer in Wien" (1952) von Emil-Edwin Reinert und Robert Siodmaks "Nachts, wenn der Teufel kam" (1957).

Mit dem Begriff "Neo-Noir" werden Filme zusammengefasst, die seit dem Ende der Ära des klassischen Film noir entstanden und die die typischen visuellen und narrativen Elemente des Film noir variieren oder nur reproduzieren.

Als der Film noir in den Kinos seinem Ende entgegenging, begann das neue Medium Fernsehen, das damals noch schwarzweiß war, sich rasant zu verbreiten. Trotz des völlig anderen Seherlebnisses und des nicht geschlossenen Formats der Serie wagten es einige Filmemacher, Fernsehsendungen im Noir-Stil zu produzieren. Den Anfang machte die Detektivserie "China Smith" (1952–1954), zu der auch Robert Aldrich wenige Episoden beisteuerte. Die Hauptfigur war gekennzeichnet durch Sarkasmus, Heimatlosigkeit und einen eigenen Moralkodex. Auch im Noir-Stil präsentierte sich die Serie "Four Star Playhouse" (1952–1956), an der neben Robert Aldrich auch Drehbuchautor Blake Edwards und Schauspieler Dick Powell beteiligt waren. Weitere „hartgesottene“ Detektivserien in den 1950ern waren "Mickey Spillane’s Mike Hammer" (1956–1959) und "The Man with a Camera" (1958–1959).

"Peter Gunn" (1958–1961), die einen emotional abgeschotteten Detektiv als Hauptfigur zeigt, ist in besonderem Maße düster, fatalistisch und gewaltvoll. Von "Peter Gunn" inspiriert ist die nur wenig später erschienene Serie "Johnny Staccato" (1959–1960), die ebenfalls von Low-Key-Beleuchtung und Ich-Erzähler Gebrauch macht, allerdings stärker von der verwendeten Jazzmusik geprägt ist. Bei einigen Episoden von "Johnny Staccato" führte John Cassavetes Regie.

Die erfolgreichste aller Noir-Serien war "Auf der Flucht" (1963–1967) nach einem Konzept von Roy Huggins. Sie beruht hauptsächlich auf den Romanen David Goodis’. Die Hauptfigur Dr. Kimble (gespielt von David Janssen), die von Entfremdung und Angst gezeichnet ist, ist ein typischer Antiheld.

Danach verschwanden die Noir-Serien auch wieder und kamen erst in der Blütezeit des Neo-Noir erneut zum Vorschein. Michael Manns "Miami Vice" (1984–1989), Robert B. Parkers "Spenser: For Hire" (1985–1988), das Remake "Mickey Spillane’s Mike Hammer" (1984–1987) sowie "Fallen Angels" (1993–1995) sind Beispiele für jüngere Hommagen an den Film noir. Auch beim Zeichentrick gibt es mit "Noir" (2001) eine stark vom Film noir inspirierte Serie.

In der Fernsehserie "Veronica Mars" (2004–2007) übernimmt eine Schülerin die Rolle des hartgesottenen Detektivs. Als Tochter eines Privatdetektivs und ehemaligen Polizeichefs hilft sie diesem außerhalb ihrer Schulzeit bei diversen Fällen aus. Sie ist eine Außenseiterin, geprägt von persönlichen Traumata wie der alkoholabhängigen Mutter und der eigenen Vergewaltigung durch einen Mitschüler und besitzt eine desillusionierte Sicht auf das Leben in ihrer Schule und Heimatstadt.

Der grafische Stil des Film noir hielt schon früh Einzug in die Comicwelt. Will Eisners Comicreihe "The Spirit," die bereits 1940 erstmals und 1952 zum letzten Mal erschien, war in dieser Hinsicht wegweisend. Deren Held, Privatdetektiv Danny Colt, verfügt über keinerlei übernatürliche Kräfte, sondern wandelt mit einer bloßen Augenmaske durch die bedrohlich-düstere Großstadt. Nebenfiguren in Außenseiterrollen treten in den Mittelpunkt der Geschichten.

Inspiriert durch "The Spirit" entstanden in den vergangenen Jahrzehnten weitere zahlreiche Comics im Noir-Stil, von denen "Watchmen" von Alan Moore, "Daredevil" von Stan Lee sowie "Batman" von Bob Kane zu den wichtigsten zählen. Eines der neusten Beispiele liefert Comicautor Frank Miller mit der Serie "Sin City," die 2005 verfilmt wurde. Als europäischer Vertreter ist der französische Comic-Zeichner Jacques Tardi zu nennen.

Mittlerweile gibt es eine Reihe von Computerspielen, die auf bekannten Vorbildern des Film noir basieren oder sich in der Tradition des Film noir sehen. Dies sind meistens Adventures, da sie sowohl gut die Handlung als auch typische Optikelemente darstellen können. Es finden sich jedoch, wenn auch selten, einzelne Stilelemente in anderen Spielgenres wie dem Ego-Shooter oder dem Third-Person-Shooter wieder.

Der getreuste Vertreter des Film noir findet sich in dem Adventurespiel Private Eye, welches eine Adaption des Phillip Marlowe Romans „The Little Sister“ („Die kleine Schwester“) ist. Im Spiel kann der originale Plot des Romans oder ein alternativer Story-Verlauf gespielt werden.

Ein weiterer Vertreter des Genres ist Grim Fandango von LucasArts, ein Adventure, welches in einem Totenreich spielt, und eine sehr außergewöhnliche Optik bietet. Ebenso ist Sam aus dem Adventurespiel Sam & Max eine dem Film Noir entlehnte Figur des Hardboiled Detective, auch die Ausdrucksweise ist dem Film noir entnommen. In einer Episode ist es sogar möglich „Noir“ als Option für eine Antwort in Dialogen zu geben.

"L.A. Noire" ist ein 2011 erschienenes Computerspiel von Rockstar Games, das sich ebenfalls dem Film noir verschrieben hat. Es mischt Action- und Open-World-Elementen mit adventureartigen Ermittlungspassagen. Neben "L.A. Noire" gilt auch das storylastige Adventure "Heavy Rain" als moderner Vertreter des Film noir. Hier wird die Stimmung aufgrund des durchweg fallenden Regens und des dunklen Grundtons bestimmt.

Einen anderen Ansatz bietet Discworld Noir von Perfect Entertainment. Hier werden klassische Film-noir-Beispiele (wie zum Beispiel "Die Spur des Falken") stark referenziert, aber in anderer, dem Scheibenwelt-Universum angepasster Form dargestellt. In dem Spiel Max Payne werden Kinoelemente aus dem Actiongenre im Stile John Woos mit starken Einflüssen des Film noir vor allem in der Rahmenhandlung kombiniert, und mit entsprechender Musik unterlegt. für das Nintendo DS ist eine Mischung aus interaktivem Film und Adventure mit klassischen Film noir-Elementen.




</doc>
<doc id="1572" url="https://de.wikipedia.org/wiki?curid=1572" title="Fred Zinnemann">
Fred Zinnemann

Fred Zinnemann (* 29. April 1907 als "Alfred Zinnemann" in Rzeszów , Österreich-Ungarn; † 14. März 1997 in London) war ein österreichisch-US-amerikanischer Filmregisseur.

Fred Zinnemann, Sohn eines Arztes, kam im Nordosten Österreich-Ungarns in einer jüdischen Familie zur Welt und wuchs im 3. Wiener Bezirk auf. In seiner Jugend war er eng mit dem späteren Hollywood-Regisseur Billy Wilder befreundet, mit dem er zeitweise in dieselbe Klasse ging und ein Leben lang Kontakt hielt. Zinnemann maturierte 1925 am Franz-Joseph-Gymnasium Stubenbastei und begann, nachdem er sich zunächst für eine musikalische Ausbildung interessiert hatte, ein Studium der Rechtswissenschaften.

1927 nahm er nach großem Widerstand seiner Eltern und Verwandten in Paris an der Ecole Technique de Photographie et de Cinématographie eine Kameraausbildung auf. Ab 1928 in Berlin tätig, war er 1929 Kameraassistent bei einem Stummfilm mit Marlene Dietrich. Seine dritte Kameraassistenz absolvierte er im Sommer 1929 bei dem Film "Menschen am Sonntag" von Edgar G. Ulmer und Billy Wilder, „der bald einmal berühmt gewordenen Außenseiterproduktion der Brüder Siodmak“.

Zinnemann verließ im Oktober 1929 Deutschland und ging nach Hollywood. Dort arbeitete er als Regieassistent und Kurzfilmregisseur. Er hatte einen Auftritt als Kleindarsteller in dem Spielfilm "Im Westen nichts Neues," es kam jedoch zu keiner Schauspielkarriere. Zinnemann wurde erst einmal Assistent des österreichischen Regisseurs Berthold Viertel und lernte durch ihn Robert J. Flaherty kennen. Dieser vermittelte Zinnemann eine erste Regie beim dokumentarischen Spielfilm "Netze (Redes)" über die Ausbeutung mexikanischer Fischer. Der Film, an dem der Fotograf Paul Strand mitwirkte, entstand in den Jahren 1934 bis 1936. Zinnemann nahm 1936 die amerikanische Staatsbürgerschaft an und begann 1937 seine Tätigkeit in der Kurzfilmabteilung von Metro-Goldwyn-Mayer (MGM). Für seinen dritten Kurzfilm "That Mothers Might Live" über den ungarischen Arzt Ignaz Semmelweis erhielt er 1938 seinen ersten Oscar.

In den 1940er Jahren konnte sich Zinnemann nach seinen ersten Erfolgen schließlich dem Spielfilm zuwenden. Während des Zweiten Weltkriegs wurde er von MGM an den Produzenten Lazar Wechsler ausgeliehen. In dieser Zeit entstand unter anderem "Die Gezeichneten", der mit zwei Oscars ausgezeichnet wurde. Bis 1948 arbeitete Zinnemann für MGM, danach drehte er für verschiedene amerikanische Studios. Seine Unabhängigkeit bewahrte er sich später, indem er seine Filme selbst produzierte.

1951 drehte er mit dem Westernklassiker "Zwölf Uhr mittags" seinen vielleicht bekanntesten Film. Der Film erhielt 1953 vier Oscars und brachte Zinnemann die von der "New Yorker Filmkritik" verliehene Auszeichnung als bester Regisseur des Jahres ein. Die Szene in "Der Pate," in der ein Filmproduzent den enthaupteten Kopf seines Lieblingspferdes in seinem Bett vorfindet, soll auf einen realen Vorfall im Zusammenhang mit einem Filmprojekt Zinnemanns Bezug nehmen. Frank Sinatra, der enge Kontakte zur amerikanischen Cosa Nostra suchte und pflegte, soll 1953 für den Film "Verdammt in alle Ewigkeit" in ähnlicher Weise als Rollenbesetzung durchgesetzt worden sein.

Ein typischer Filmemacher Hollywoods war Zinnemann jedoch nicht. Nur einen Bruchteil seiner knapp sechs Jahrzehnte andauernden Karriere brachte er in Hollywood zu. Dennoch ermöglichte er während dieses Zeitraums späteren Hollywoodstars wie Montgomery Clift, Marlon Brando, Grace Kelly, Rod Steiger, Meryl Streep in seinen Filmen erste Auftritte. Zinnemann wurde insgesamt fünf Mal mit einem Oscar ausgezeichnet, war weitere sechs Mal nominiert und gilt als einer der besten Regisseure des 20. Jahrhunderts.

Zinnemann erlag 1997 in London im Alter von 89 Jahren einem Herzanfall. 2008 wurde in Wien-Landstraße (3. Bezirk) der "Fred-Zinnemann-Platz" nach ihm benannt.

Academy Award (Oscar):





</doc>
<doc id="1573" url="https://de.wikipedia.org/wiki?curid=1573" title="Frank Capra">
Frank Capra

Frank Capra, (* 18. Mai 1897 in Bisacquino, Sizilien als "Francesco Rosario Capra"; † 3. September 1991 in La Quinta, Kalifornien) war ein US-amerikanischer Filmregisseur, Produzent und Autor italienischer Herkunft. Er zählte zu den erfolgreichsten Regisseuren seiner Generation.

Aus einer ärmlichen Auswandererfamilie stammend, gehörte Capra in den 1930er- und 1940er-Jahren zu Hollywoods populärsten und angesehensten Regisseuren. Mit den Filmen "Es geschah in einer Nacht" (1934), "Mr. Deeds geht in die Stadt" (1936) und "Lebenskünstler" (1938) gewann er dreimal den Oscar für die Beste Regie. Weitere bedeutende Filmklassiker unter seiner Regie sind "Mr. Smith geht nach Washington" (1939), "Arsen und Spitzenhäubchen" (1944), "Ist das Leben nicht schön?" (1946) sowie sein letzter Spielfilm "Die unteren Zehntausend" (1961), ein Remake seines eigenen Films "Lady für einen Tag" (1933). Häufig drehte er Tragikomödien im Kontext der Great Depression, die – stets auf der Seite des „Kleinen Mannes“ – zu gesellschaftlichen und sozialen Themen Stellung bezogen. Als sein Erfolg zunehmend nachließ, zog er sich 1964 nach 54 Filmen in den Ruhestand zurück.

Frank Capra wurde in Bisacquino, einem Dorf in der Nähe von Palermo, geboren und auf den Namen Francesco Rosario getauft. Er war das jüngste von sieben – überlebenden – Kindern des Obstpflückers Salvatore Capra und seiner Ehefrau Sarah. Die Familie war römisch-katholisch und eng mit der Kirche verbunden.

Die Familie emigrierte 1903 in die Vereinigten Staaten, als er fünf Jahre alt war. Die dreizehntägige Schiffsreise über den Atlantik musste die Familie wegen Geldmangels im billigen Zwischendeck zubringen, das Elend dort beschrieb Capra später als eine seiner schlimmsten Lebenserfahrungen. Die Familie siedelte sich im östlichen Teil von Los Angeles an, in einem Stadtteil, den Capra später als „Italienisches Ghetto“ beschrieb. Sein Vater arbeitete wie bereits in Italien als Obstpflücker, während Frank für zehn Jahre nach der Schule als Zeitungsjunge Geld nebenher verdienen musste.

Frank Capra schloss die Highschool nach zehn Jahren ab und besuchte anschließend das California Institute of Technology. Für die Finanzierung des Studiums musste er sich mit verschiedenen Jobs Geld dazuverdienen: Er spielte Banjo in Nachtclubs, arbeitete in der Waschküche seiner Universität sowie als Kellner, außerdem putzte er Maschinen in einem lokalen Kraftwerk. Capra schloss sein Studium im Chemieingenieurwesen im Frühling 1918 ab. Bald darauf schloss er sich der US-Armee als Second Leutnant in den Ersten Weltkrieg an. Als Soldatenlehrer unterrichtete er an der Fort Point National Historic Site in San Francisco das Fach Mathematik. Doch Capra erkrankte an der damals wütenden Spanischen Grippe und musste zu seiner Mutter zurückgeschickt werden. Wenig später verstarb sein Vater bei einem Unfall. Als er mit seiner Mutter und seinen Geschwistern in ihrer Wohnung lebte, war er zwar das einzige Familienmitglied mit einem College-Abschluss, allerdings auch das einzige Familienmitglied ohne festes Einkommen. Nach einem Jahr ohne Arbeit verfiel er zeitweise in eine Depression.

Nach Überwindung seiner Depression lebte Capra die nächsten Jahre in verschiedenen Unterkünften um San Francisco; in Zügen oder mit Soldatentruppen reiste er quer durchs Land. Um sein Leben einigermaßen finanzieren zu können, arbeitete Capra als Aushilfe auf Farmen, als Filmstatist, als Pokerspieler und als Handelsvertreter, als der er unter anderen die Werke des Philosophen Elbert Hubbard verkaufte. Mit 24 Jahren drehte Capra die 32-minütige Dokumentation "La Visita Dell'Incrociatore Italiano LIBYA a San Francisco." Dies war sein erster Kontakt zum Film, wenngleich ihm die kleine Dokumentation keinerlei Aufmerksamkeit oder gar Erfolg brachte.

1922 stieß Capra während seiner Zeit als Handelsvertreter auf eine Zeitungsanzeige, dass Regisseure für ein neues Filmstudio in San Francisco gesucht würden. Am Telefon erzählte er den Produzenten, dass er direkt aus Hollywood kommen würde und Erfahrung im Filmgeschäft hätte. Der Studiogründer Walter Montague (1855–1924) stellte Capra ein und gab ihm 75 US-Dollar, um einen zehnminütigen Stummfilm zu drehen. Der angehende Regisseur drehte mit einem Kameramann und Amateurdarstellern in nur zwei Tagen den Film "Fultah Fisher’s Boarding House", dessen Handlung auf einem Gedicht von Rudyard Kipling basierte. Nachdem Walter Montagues Filmstudio wenig Erfolg hatte und schnell wieder aufgelöst wurde, suchte Capra in der Folgezeit nach neuen Anstellungen im Filmgeschäft und erhielt schließlich eine feste Anstellung in einem anderen kleinen Filmstudio in San Francisco.

Hier erweckte Capra die Aufmerksamkeit des Hollywood-Produzenten Harry Cohn, welcher ihn zu seinem neuen Filmstudio in Los Angeles lotste. Er arbeitete während dieser Zeit gleichzeitig als Requisiteur, Filmeditor, Titelschreiber und Assistenzregisseur; durch diese vielfältigen Aufgaben erlernte er jedoch die Grundlagen des Filmgeschäfts. 1924 arbeitete Capra kurz in der Drehbuchabteilung des Produzenten Hal Roach und schrieb unter anderem Gags für die Kleinen Strolche. Ende desselben Jahres konnte Roachs größter Konkurrent Mack Sennett Capra als Autor für die Filme des Komikers Harry Langdon abwerben. Langdons Komikerfigur zeichnete sich durch grenzenlose Langsamkeit, Naivität und Unschuld aus, weshalb sie mit der großen, harten Welt meistens nicht zurechtkam. Auch Capra, der sich nun endgültig zum Komödienspezialisten entwickelte, hatte Anteil daran, dass Langdon sich Mitte der 1920er Jahre zu einem der erfolgreichsten Komiker Hollywoods emporschwang.

Als Langdon von Mack Sennetts Studio zu First National wechselte, um dort Langfilme herstellen zu können, nahm er Capra als seinen wichtigsten Autoren und als Regisseur mit sich. 1926 und 1927 drehten sie zwei Langfilme, von denen der erste, "The Strong Man", sowohl beim Publikum als auch bei Kritikern großen Erfolg hatte. Langdon wurde während dieser Zeit mit Charlie Chaplin, Harold Lloyd und Buster Keaton verglichen. Während der Dreharbeiten des zweiten Films kam es jedoch zu einem Streit zwischen Capra und Langdon und das Erfolgsduo trennte sich. Die Reaktionen auf den Film waren negativ und Langdons Stern begann zu sinken. Nach der Trennung von dem Komiker drehte Capra für First National mit "For the Love of Mike" noch eine weitere Stummfilmkomödie, in welcher auch die zu diesem Zeitpunkt noch nahezu unbekannte Claudette Colbert eine Rolle innehatte. Auch dieser Film wurde mit schlechten Kritiken bedacht und Capra erhielt keinen weiteren Vertrag bei First National.

Der arbeitslose Regisseur wurde 1928 vom Produzenten Harry Cohn, der ihn einst nach Hollywood geholt hatte, erneut für dessen Filmstudio Columbia Pictures verpflichtet. Columbia gehörte zu dieser Zeit zu den sogenannten Poverty Row-Filmstudios in Hollywood, die mit den großen Filmstudios und deren hohen Budgets nicht konkurrieren konnten, Columbia Pictures wollte allerdings an Bedeutung und Größe gewinnen und zu den größeren Filmstudios aufschließen, doch dafür mussten erfolgreiche Langfilme produziert werden. Hierfür wurde Capra verpflichtet. Insgesamt drehte der Regisseur zwanzig Filme für Columbia Pictures, neun davon gleich im ersten Jahr. Die meisten dieser neun Filme Capras stellten sich als sehr erfolgreich heraus, sodass Cohn dessen Gehalt von 1000 US-Dollar pro Film auf das Jahresgehalt von 25.000 US-Dollar vergrößerte. Er drehte unter anderem das Drama "The Younger Generation" (1929), über eine jüdische Familie in New York, deren Sohn seine jüdischen Wurzeln verleugnet, um weiter in der Gunst seiner Freundin zu bleiben.

Ende der 1920er-Jahre hielt der Tonfilm Einzug in Hollywood; Capra begrüßte diese Neuerung im Gegensatz zu vielen seiner Kollegen und hielt den Tonfilm ebenfalls nicht für eine vorübergehende Modeerscheinung. Bei der Produktion seiner ersten Tonfilme war ihm das Ingenieurstudium ein Vorteil, denn – wie sich später Capras langjähriger Kameramann Joseph Walker erinnerte – verstand er im Gegensatz zu dem meisten anderen Regisseuren auch etwas von der technischen Materie und hatte sich schnell an die technischen Neuerungen des Tonfilmes gewöhnt. Neben dem Kameramann Joseph Walker, welcher mit dem Regisseur bei insgesamt 18 Filmen zusammenarbeitete, wurde auch der Drehbuchautor Robert Riskin regelmäßig von Capra verpflichtet. In vielen Capra-Filmen schrieb Riskin die witzigen und scharfen Dialoge und beide wurden zu „Hollywoods umjubelsten Regisseur/Autor-Team“.

Nach Achtungserfolgen wie dem Abenteuerfilm "Das Luftschiff" (1931) und vor allem der Komödie "Vor Blondinen wird gewarnt" (1931), die Jean Harlows Karriere erheblich voranbrachte, drehte Capra im Jahre 1933 die Komödie "Lady für einen Tag". Mit der 75-jährigen May Robson – welche eine arme Apfelverkäuferin spielt, die sich als Dame verkleidet – hatte der Film eine eher ungewöhnliche Hauptdarstellerin. Dennoch wurde er für vier Oscars nominiert, darunter auch in der Kategorie Beste Regie für Capra, ging aber bei der Verleihung leer aus. Dennoch gilt "Lady für einen Tag" als Capras erster großer Wurf, außerdem war es Columbias erster Streifen, welcher für einen Oscar als Bester Film nominiert wurde.

Im folgenden Jahr sollte Capra seinen Erfolg von "Lady für einen Tag" allerdings übertreffen: Seine Roadmovie-Komödie "Es geschah in einer Nacht" (1934) gewann als erster Film alle Oscars in den fünf Hauptkategorien (Bester Film, Bester Regisseur, Bester Hauptdarsteller, Beste Hauptdarstellerin, Bestes Drehbuch). Claudette Colbert spielt im Film eine verwöhnte Millionenerbin, welche vor ihrem Vater zu einem snobhaften Geliebten flieht und auf diesem Weg nicht nur das Elend der normalen Bevölkerung in der Great Depression kennenlernt, sondern sich dabei auch in einen bodenständigen Reporter (Clark Gable) verliebt. Trotz dieser eigentlich komischen Handlung zeigt Capra also ebenso die Probleme der amerikanischen Durchschnittsbürger in der Weltwirtschaftskrise auf. "Es geschah in einer Nacht" gilt heute als Gründungsfilm der Screwball-Komödie schlechthin und beförderte Columbia Pictures in die Reihe der großen Hollywood-Studios. Auch bei Capras folgenden Film "Broadway Bill" über Pferderennen handelte es sich um eine Screwball-Komödie, allerdings war geriet nicht so erfolgreich wie "Es geschah in einer Nacht".

Nach "Broadway Bill" entschied sich Capra, künftig noch mehr Einfluss auf seine Drehbücher zu nehmen und in seinen Filmen auch politische, gesellschaftliche oder moralische Botschaften an die Öffentlichkeit zu senden. Der erste Film dieser Art war "Mr. Deeds geht in die Stadt" mit Gary Cooper und Jean Arthur in den Hauptrollen. Cooper spielt einen Glückskarten-Dichter, ein freundlich-naives Landei, das ein Millionenvermögen erbt und mit seinem Geld viel Gutes tut, ehe seine geldgierigen und betrügerischen Anwälte ihn für verrückt erklären wollen. Der Kritiker Alistair Cooke schrieb bei "Mr. Deeds", dass Capra beginnen würde, mehr Filme über Themen als über Menschen zu drehen. Für "Mr. Deeds geht in die Stadt" gewann Capra seinen zweiten Oscar als Bester Regisseur. Von Capras vorigen Filmen unterschied sich dagegen der Abenteuerfilm "In den Fesseln von Shangri-La" (1937) mit Ronald Colman, welcher auf dem utopischen Roman "Der verlorene Horizont" von James Hilton basierte. Der Dreh verbrauchte die damals hohe Summe von 1,5 Millionen US-Dollar, unter anderem weil der Film in einem exotischen, wunderschön-utopischen Tal im Himalaya spielt – die Kulissen des Filmes waren daher äußerst aufwendig.

Am 5. Mai 1936 war Capra, inzwischen bestbezahlter Regisseur in Hollywood, Gastgeber der Oscarverleihung 1936.

Mit seinem dritten Oscar als Bester Regisseur in fünf Jahren wurde Capra für die Komödie "Lebenskünstler" ausgezeichnet, außerdem gewann die Produktion den Oscar für den Besten Film des Jahres. "Lebenskünstler" basiert auf dem Erfolgsstück "You Can’t Take It with You", welches am Broadway ab Dezember 1937 insgesamt 837 Vorstellungen hatte. Jean Arthur und James Stewart spielen in den Hauptrollen ein junges Liebespaar, deren äußerst unterschiedliche Familien – die einen knallharte Geschäftsleute, die anderen alternative Exzentriker – sich kennenlernen. 1939 drehte Capra mit der Politiksatire "Mr. Smith geht nach Washington" einen seiner bekanntesten Filme, erneut mit Arthur und Stewart in den Hauptrollen. Capra machte Stewart mit den Hauptrollen in "Lebenskünstler" und "Mr. Smith" zum Hollywood-Star. In "Mr. Smith geht nach Washington" wird Capras Patriotismus deutlich, er zeigt „das Individuum im demokratischen System beim Kämpfen gegen zügellose Korruption in der Politik“. Bei seiner Veröffentlichung war der Film bei Kritikern und Publikum beliebt, doch viele Politiker waren erzürnt. So bat der damalige US-Botschafter in Großbritannien, Joseph P. Kennedy, Columbia-Boss Harry Cohn, dass der Film nicht in Europa gezeigt werden sollte. An US-Präsident Franklin D. Roosevelt schrieb Kennedy, dass der Film im Ausland den falschen Eindruck erzeuge, in den USA würden Bestechung, Korruption und Gesetzlosigkeit herrschen. Solch ein Eindruck sei insbesondere im Hinblick auf den beginnenden Zweiten Weltkrieg fatal für Amerika.
Capra und Cohn ignorierten die Kritik am Film und der Regisseur verteidigte seinen Film mit den Worten: „Je unsicherer die Menschen auf der Welt sind, desto mehr verstreuen und verlieren sich ihre schwergewonnenen Freiheiten im Wind der Risiken, umso mehr brauchen sie eine tönendes Statement für Amerikas demokratische Ideale.“ "Mr. Smith geht nach Washington" war für elf Oscars nominiert, konnte aber gegen starke Konkurrenz nur einen Preis für die Beste Originalgeschichte entgegennehmen. Als das Vichy-Regime im Zweiten Weltkrieg US-amerikanische Filme in den Kinos verbot, zeigten viele Kinobesitzer als letzten amerikanischen Film vor dem Inkrafttreten des Gesetzes "Mr. Smith" als Zeichen für Demokratie. 

Nach diesem Film verließ Capra Columbia Pictures und drehte seine nächsten beiden Filme bei Warner Bros. Sein erster Film dort, "Hier ist John Doe", erschien 1941. Der Filmheld, gespielt von Gary Cooper, ist ein ehemaliger Baseballspieler, der ohne Arbeit und Geld durchs Land zieht, ehe er von Nachrichtenreportern zum amerikanischen Durchschnittsbürger „John Doe“ ausgerufen wird und zum Held der Massen mutiert. Kurz vor dem Eintritt der USA in den Zweiten Weltkrieg veröffentlicht, zeigt "Hier ist John Doe" erneut patriotische Züge und enthält außerdem eine Botschaft gegen Faschismus: ein skrupelloser und offensichtlich faschistischer Industrieller mit diktatorischen Absichten nutzt John Doe für seine Zwecke aus und will zum Präsidenten werden. Viele Filmhistoriker sehen diesen Film als einen der persönlichsten von Capra an, vor allem weil dieser wie sein Filmheld einen rasanten Aufstieg hingelegt hatte, jedoch auch von Unsicherheiten geprägt war.

Nur vier Tage nach dem Angriff auf Pearl Harbor am 7. Dezember 1941 trat Capra in die US-Armee im Rang eines Majors ein. Bei seiner Einberufung gab er nicht nur vorerst seine Hollywood-Karriere, sondern auch den Vorsitz der Directors Guild of America auf. Biografen schrieben später, dass er mit seinem Eintritt als Einwanderer seinen Patriotismus zu Amerika beweisen wollte; nach eigener Aussage wollte er sich auch etwas vom Glanz und Geld Hollywoods entfernen, der im Gegensatz zu den Inhalten seiner Filme stand. In den nächsten Jahren war er am Dreh zahlreicher Kriegsdokumentationen für Amerika beteiligt. Sein Ziel war es zu erklären, warum die US-Soldaten für ihr Land in den Krieg zogen und was ihre Ziele waren. Damit distanzierte er sich bewusst von Propagandafilmen aus Deutschland oder Japan.

Zu einem großen Erfolg wurde seine siebenteilige, preisgekrönte Dokumentarfilmreihe von 1942 bis 1945, "Why We Fight", die Capra als Antwort auf Leni Riefenstahls Nazi-Propagandafilm "Triumph des Willens" verstand. Zusätzlich produzierte er mit Walt Disneys Hilfe insgesamt 28 drei- bis fünfminütige Schwarz/Weiß-Cartoons über „Private Snafu“, die zu Unterrichtszwecken in den Streitkräften eingesetzt wurden. Für die Hintergrundmusik von Capras Dokumentationsfilmen waren namhafte Hollywoodkomponisten wie Alfred Newman und Dimitri Tiomkin verantwortlich. General George C. Marshall, unter dessen Kommando Capra stand, sagte nach der Sichtung von Capras erstem Armee-Film: „Colonel Capra, wie haben sie das nur hingekriegt? Das ist die wunderschönste Sache!“ Capras Dokumentarfilme wurden in der Folge in Französisch, Spanisch, Portugiesisch und Chinesisch übersetzt und Winston Churchill verlangte, dass alle "Why we Fight"-Filme im britischen Kino gezeigt werden sollen.

1944 wurde Capras "Arsen und Spitzenhäubchen" mit Cary Grant in der Hauptrolle veröffentlicht, den Capra allerdings bereits 1941 abgedreht hatte. Die mit Schwarzem Humor angereicherte Komödie basiert auf dem Theaterstück "Spitzenhäubchen und Arsenik" ("Arsenic and Old Lace") von Joseph Kesselring und wurde von dem Drehbuchautor Julius J. Epstein adaptiert. Der Erfolg der Bühnenfassung am Broadway verzögerte die Uraufführung des Films bis ins Jahr 1944, denn die Produzenten hatten sich vertraglich verpflichtet, mit der Auswertung des Filmes bis nach Absetzung des Stückes am Broadway zu warten. Der Film erhielt exzellente Kritiken und war auch beim Publikum erfolgreich. Heute gilt er als Klassiker der Schwarzen Komödie.

Nach Ende des Weltkrieges gründete Capra zusammen mit seinen Regiekollegen William Wyler und George Stevens seine eigene Produktionsfirma Liberty Films, um noch mehr Kontrolle über seine Werke zu erhalten. Jedoch wurde die Firma nach Capras Filmen "Ist das Leben nicht schön?" (1946) und "Der beste Mann" (1948) bereits wieder aufgelöst. Die aufwendige Tragikomödie "Ist das Leben nicht schön?" ist heute wahrscheinlich Capras berühmtester Streifen. Dabei war der Film mit James Stewart in der Hauptrolle bei seiner Veröffentlichung, obwohl er für fünf Oscars nominiert wurde, ein finanzieller Misserfolg. Erst durch zahllose Ausstrahlungen im US-Fernsehen wurde der Film ab den 1970er-Jahren wiederentdeckt und gilt heute als Weihnachtsklassiker. Der Film handelt von einem Mann, der sich um die Bevölkerung seiner Kleinstadt verdient gemacht hat, aber nach einem Missgeschick an einem Weihnachtsabend seinen Lebensmut verliert und sich von der Brücke stürzen will, sodass ein Engel ihm helfen muss. In seinem „optimistischen Glauben an das Gute im Menschen“ und seiner „einfallsreichen Machart“ sei es ein typischer Capra-Film. "Ist das Leben nicht schön?" wurde vom American Film Institute zum inspirierendsten amerikanischen Film aller Zeiten gewählt.

Bei "Ist das Leben nicht schön?" warfen Kritiker Capra erstmals eine übersentimentale und überidealistische Art vor. Seine Filmthemen aus der Great Depression passten zunehmend nicht mehr zum Geschmack der Öffentlichkeit, der sich gewandelt hatte und seine Filme waren nicht mehr so erfolgreich. Capra selbst machte dafür die steigende Macht der Filmstars verantwortlich, die sich in seine Regiearbeit einmischten. Nach "Der beste Mann" mit dem Paar Katharine Hepburn und Spencer Tracy waren die meisten seiner Filme auch kommerziell nicht mehr erfolgreich. Nach zwei belanglosen, wenig erfolgreichen Filmen mit Bing Crosby produzierte er in den 1950er-Jahren im Rahmen der "Bell Laboratory Science Series" vier naturwissenschaftliche Dokumentarfilme fürs Fernsehen, die später häufig in amerikanischen Schulklassen gezeigt wurden. Bei der Berlinale 1958 fungierte Capra als Jurypräsident. Erst 1959 drehte Capra mit der Komödie "Eine Nummer zu groß" mit Frank Sinatra und Edward G. Robinson wieder einen Spielfilm, der jedoch nur durchwachsene Ergebnisse einfuhr. Sein letzter Spielfilm "Die unteren Zehntausend" war ein Remake seines eigenen Filmes "Lady für einen Tag" von 1933, wobei Bette Davis diesmal die Rolle der Apfelverkäuferin spielte. Immerhin erhielt der Film drei Oscar-Nominierungen, dennoch halten die meisten Kritiker den Originalfilm für besser als das Remake. Anschließend drehte Capra nur noch den Dokumentarfilm "Rendezvous in Space" (1964) für den Konzern Martin Marietta.

Frank Capra heiratete 1923 die Schauspielerin Helen Howell, die Ehe wurde 1928 geschieden. Noch im selben Jahr heiratete er Lucille Warner. Der Ehe, die 1984 durch Capras Tod endete, entstammen eine Tochter und drei Söhne − ein Sohn starb bereits als Säugling. Ein Sohn war der Filmproduzent Frank Capra junior, dessen Sohn ist der Assistenzregisseur Frank Capra III. (* 1959).

Zu den Interessen Capras gehörten Jagen, Fischen und Klettern, zudem besaß er eine vier Quadratkilometer große Ranch in Fallbrook. Er sammelte ebenfalls seltene und alte Bücher, so brachte ihm eine Auktion im Jahre 1948 insgesamt 68.000 US-Dollar für ein Buch. Im hohen Alter spielte er Gitarre und schrieb einige Kurzgeschichten und Songtexte. Capra hegte ein besonderes Interesse an Naturwissenschaften und Mathematik.

Zwischen 1935 und 1939 fungierte Capra als Vorsitzender der Academy of Motion Picture Arts and Sciences, außerdem war er drei Jahre Präsident der Directors Guild of America, an deren Gründung er beteiligt war. Politisch war der patriotische Capra Republikaner und Konservativer sowie Gegner von Franklin D. Roosevelt und seiner Politik in der Great Depression. Weil Capra es aus schlechten Verhältnissen zu so großem Erfolg gebracht hatte, glaubte Capra, dass alle Menschen dies ebenfalls mit harter Arbeit schaffen könnten und sah keinen Grund für Staatshilfen. Er stand allerdings gegen Korruption und war auch der Wirtschaft gegenüber kritisch eingestellt. Zudem arbeitete er häufig mit linken Drehbuchautoren zusammen, weshalb viele seiner Filme einen „linken Anstrich“ hatten und etwa Kapitalismuskritik übten. Hinter "Ist das Leben nicht schön?" wurde sogar vom FBI „subversive kommunistischen Propaganda“ vermutet.

Nach seinem letzten Film 1964 verbrachte Capra viele Jahre im Ruhestand und kümmerte sich ebenfalls um den Nachruhm seiner Filme. 1971 veröffentlichte er seine Autobiografie "The Name Above The Titel". Er war ebenfalls als Zeitzeuge an Dokumentationen beteiligt und nahm verschiedene Auszeichnungen entgegen. Er befand sich bis Ende der 1980er-Jahre bei guter Gesundheit, ehe er eine Reihe von Herzanfällen erlitt. Frank Capra verstarb 1991 im Alter von 94 Jahren in La Quinta an einem Herzinfarkt. Er wurde auf dem Coachella Valley Public Cemetery im Coachella Valley beigesetzt. Seine Aufzeichnungen und Schriften befinden sich als Nachlass in der Wesleyan University.

Frank Capra gilt bis heute als einer der einflussreichsten Regisseure Hollywoods. In den 1930er- und 1940er-Jahren zählte er zu den weltweit zwei bis drei berühmtesten und erfolgreichsten Regisseuren. Er gehörte zu den wenigen Regisseuren, deren Namen auch einem breiteren Kinopublikum bekannt waren. Als besondere Auszeichnung verstand er selbst, dass sein Name über dem Titel des Films stand. "The Name Above the Title" nannte er darum auch seine Autobiographie. Zu diesem Erfolg verhalf Capra insbesondere der Fakt, dass seine Filme eine bestimmte Handschrift trugen und so ein Capra-Film fast immer auch als solcher zu erkennen war. Damit stand er im Gegensatz zu vielen anderen Regisseuren im Studiosystem, welche die Filme drehten, die sie gerade von ihren Produzenten „vorgesetzt“ bekamen. Er verlangte möglichst viel Unabhängigkeit von Produzenten und arbeitete deshalb zum Beispiel jahrelang mit einem kleineren Studio wie Columbia zusammen, bei dem er etwas mehr Freiheiten hatte.

Capras Genre war die Komödie, jedoch sind viele seiner Komödien mit tragischen und sentimentalen Momenten, bissiger Gesellschaftskritik und Pathos durchsetzt. Häufig wird eine positive Geschichte in einem negativen Umfeld gezeigt, etwa die Liebesgeschichte in "Es geschah in einer Nacht" mitten zwischen verbitterten, von der Great Depression gezeichneten Menschen. Er setzte stets den guten „Kleinen Mann“ als Hauptfigur ein, welcher vor allem von James Stewart oder Gary Cooper gespielt wurde. Seine Filme enthielten oft Botschaften über das Gute in der menschlichen Natur und zeigen, was durch Uneigennützigkeit und harte Arbeit erreicht werden kann. Diese Figur behauptet sich durch seine wohlwollende, etwas naive und idealistische Art gegen die Schurken. Wohlhabende oder mächtige Menschen erschienen dagegen häufig als Bösewichte, ein gutes Beispiel sind Edward Arnolds Verkörperungen von machtgierigen Industriellen in "Mr. Smith geht nach Washington" und "Hier ist John Doe". Die mächtigen Schurken seiner Filme wurden allerdings am Ende nur selten bestraft, weil er dies für realistischer hielt. Ein weiterer Aspekt in seinem Film ist der American Dream, verbunden mit einem gewissen Patriotismus und dem Glauben an. Seine besten Filme zeichnen sich durch schlagfertige, aber dennoch natürlich wirkende Dialoge, markante und oftmals komische Figuren, ein hohes Tempo und etwas Action aus. Am Ende steht trotz vieler Probleme ein positives, fast märchenhaftes Happy End.

Capra improvisierte sehr viel bei seinen Filmen und kam häufig nur mit ungenauen Scripts ans Filmset. Im Gegensatz zu vielen fast diktatorischen Regisseuren zur damaligen Zeit herrschte bei ihm am Set meistens eine angenehme, aber künstlerische Atmosphäre. Obwohl er durchaus dynamische und schnelle Kamerabewegungen und Schnitte in seinen Filmen hatte, stand Capra eher ablehnend gegen technische Spielereien wie experimentelle Kameraperspektiven, was er als schlechte Ablenkung des Publikums verstand. Einige Kritiker entdeckten einen gewissen Purismus in seiner Arbeit. Hinter der Kamera vertraute er meistens über viele Jahre auf dieselben Leute und viele Schauspieler wie H. B. Warner besetzte er in gleich mehreren seiner Filme. Abwertend werden seine Filme oft „Capra-Corn“ genannt, der positive Gegenbegriff ist „Capraesque“. Mit seinen Werken beeinflusste er viele spätere, bedeutende Regisseure maßgeblich.

Oscar

Capra ist einer von nur vier Regisseuren, denen es gelang zwei Filme zu drehen, welche mit dem Oscar für den Besten Film ausgezeichnet wurden. ("Es geschah in einer Nacht" und "Lebenskünstler"). Die anderen Regisseure sind Clint Eastwood, Francis Ford Coppola und Miloš Forman.

Golden Globes

American Film Institute

Internationale Filmfestspiele von Venedig

Hollywood Walk of Fame

1986 wurde Frank Capra mit der National Medal of Arts ausgezeichnet.





</doc>
<doc id="1574" url="https://de.wikipedia.org/wiki?curid=1574" title="First National">
First National

Der First National Exhibitor's Circuit entstand 1917 als Zusammenschluss 26 regionaler Verleihfirmen unter der Federführung von Thomas L. Tally. Ursprünglich war der Zweck der Firma, Filme zu finanzieren und anschließend den Verleih zu übernehmen, doch schon bald kam eine eigene Produktion hinzu. First National war eine Reaktion auf die marktbeherrschende Stellung von Paramount, die das Geschäft immer mehr monopolisierte (bezeichnenderweise wurde W.W. Hodkinson, der vom neuen Paramount-Eigentümer Adolph Zukor geschasst wurde, der Direktor der neuen Firma). 

Der Plan ging ursprünglich gut auf. Mit der Anwerbung von Mary Pickford und Charlie Chaplin für jeweils eine Million Dollar pro Film hatte man die wichtigsten Stars auf seiner Seite und kontrollierte zudem 1919/1920 ca. 3400 Kinos, was 15 bis 20 % des amerikanischen Marktes entsprach. 

Da es aber nicht gelang, die Stars längerfristig zu binden (sie gründeten 1919 United Artists, mussten aber aufgrund der laufenden Verträge noch einige Filme gedreht werden, weshalb die Firma 1920 noch relativ gut dastand), und da auch die geplante Fusion mit Paramount spektakulär scheiterte, ging es mit First National rapide bergab. Paramount kaufte nach und nach die einzelnen zusammengeschlossenen Firmen auf, bis First National im September 1928 mit Warner Bros. fusionierte.


</doc>
<doc id="1575" url="https://de.wikipedia.org/wiki?curid=1575" title="Fluor">
Fluor

Fluor [] ist ein chemisches Element mit dem Symbol F und der Ordnungszahl 9. Im Periodensystem steht es in der 7. Hauptgruppe und gehört damit zur 17. IUPAC-Gruppe, den Halogenen, von denen es das leichteste ist. Es liegt unter Normalbedingungen in Form des zweiatomigen Moleküls F gasförmig vor, ist das reaktivste aller Elemente und sehr giftig. Bereits in geringen Konzentrationen kann sein durchdringender Geruch bemerkt werden. Fluor ist farblos und erscheint stark verdichtet blassgelb. Es ist das elektronegativste aller Elemente und hat in Verbindungen mit anderen Elementen – mit wenigen Ausnahmen – stets die Oxidationsstufe −1. Es reagiert mit allen Elementen mit Ausnahme der Edelgase Helium und Neon.

Der Name des Elementes leitet sich von lat. "fluores" ab. Dieser Begriff bezeichnete das wichtigste natürlich vorkommende Mineral Fluorit ("Flussspat"), das in der Metallurgie als Flussmittel zur Herabsetzung des Schmelzpunktes von Erzen verwendet wird (im Originalkontext: "lapides igni liquescentes (fluores)").

Elementares Fluor ist sehr giftig und stark ätzend. Seine Salze (Fluoride und diverse Fluorokomplexsalze wie Natriummonofluorphosphat) sind in höherer Konzentration ebenfalls sehr giftig, werden in Spuren aber zur Prophylaxe von Zahnkaries verabreicht. Sie werden deswegen (Fluor bzw. Fluoride sind beteiligt an der Bildung von Knochen und Zähnen) teilweise dem Trinkwasser oder Speisesalz zugesetzt (Fluoridierung).

Das erste beschriebene Fluorsalz war das natürlich vorkommende Calciumfluorid (Flussspat). Es wurde 1530 von Georgius Agricola beschrieben und 1556 von ihm als Hilfsmittel zum Schmelzen von Erzen erwähnt. Es macht Erzschmelzen und Schlacken dünnflüssiger, lässt sie fließen (Flussmittel).

Carl Wilhelm Scheele beschäftigte sich ab 1771 erstmals eingehender mit Flussspat und seinen Eigenschaften, sowie der daraus bei Säurebehandlung gebildeten Flusssäure. Er erforschte die Reaktionen bei Einwirkung von Flusssäure auf Glas unter Bildung von Siliciumtetrafluorid und Fluorkieselsäure. Eine weitere Eigenschaft, die er an Flussspat entdeckte, war die Fluoreszenz, die nach dem Mineral benannt ist.

In einem Leserbrief, der lediglich mit dem Kürzel „E.B.“ signiert ist, beklagte 1808 der Schreiber das seiner Meinung nach inkonsequente Vorgehen bei der Namensgebung für neue Elemente. In einem Nachtrag schlug er bei dieser Gelegenheit für den in der Flusssäure (engl.: "fluoric acid") gebundenen Grundstoff den Namen "Fluor" vor. In einem Brief vom 25. August 1812 an Humphry Davy wurde von André-Marie Ampère erstmals der Gedanke geäußert, dass wie in der Salzsäure auch in der Flusssäure das Radikal (engl.: "fluorine." gelegentlich auch "fluorin." in Analogie zu "chlorine" für Chlor) an Wasserstoff gebunden sei. Danach versuchten viele Chemiker das Element zu isolieren. Aufgrund der Schwierigkeiten, die durch die Reaktivität und Giftigkeit entstanden, dauerte es bis zum 28. Juni 1886, als es Henri Moissan erstmals gelang, elementares Fluor herzustellen und zu charakterisieren. Er erhielt es durch Elektrolyse einer Lösung von Kaliumhydrogendifluorid in flüssigem Fluorwasserstoff bei tiefen Temperaturen in einer speziell entwickelten Apparatur (teilweise aus Flussspat). Für diese Leistung bekam Moissan den Nobelpreis für Chemie im Jahr 1906 verliehen.

Aufschwung nahm die Fluorherstellung im Zweiten Weltkrieg, einerseits durch die Entwicklung der Atomwaffen in den USA (Manhattan-Projekt), da die Isotopenanreicherung von Uran über gasförmiges Uranhexafluorid (UF) erfolgt, das mit Hilfe von elementarem Fluor hergestellt wird. Andererseits betrieb damals die I.G. Farben in Gottow eine Fluorelektrolyse-Zelle, deren Produkt angeblich nur zur Herstellung eines neuen Brandmittels (Chlortrifluorid) für Brandbomben dienen sollte. Ob es in Deutschland damals möglich gewesen wäre, mit Hilfe dieser Fluorproduktion Uran anzureichern, wird kontrovers diskutiert.

In der Erdkruste ist Fluor mit 525 ppm ein relativ häufiges Element. Es kommt aufgrund seiner Reaktivität in der Natur nicht elementar, sondern gebunden als Fluorid in Form einiger Minerale vor. Eine Ausnahme bildet Stinkspat (eine uranhaltige Fluorit-Varietät) u. a. aus Wölsendorf, in dem geringe Mengen elementares Fluor durch Radiolyse entstehen, was bei mechanischen Bearbeitungen einen starken Geruch durch freigesetztes Fluor verursacht. Meerwasser enthält wenig gelöste Fluoride, da bei Anwesenheit von Calcium die Löslichkeit durch Bildung von schwerlöslichem Calciumfluorid eingeschränkt wird. Die häufigsten Fluorminerale sind der Fluorit CaF und der Fluorapatit Ca(PO)F. Der größte Teil des Fluorits ist in Fluorapatit gebunden, jedoch enthält dieser nur einen geringen Massenanteil Fluor von ca. 3,8 %. Daher wird Fluorapatit nicht wegen seines Fluorgehaltes, sondern vor allem als Phosphatquelle abgebaut. Die Hauptquelle für die Gewinnung von Fluor und Fluorverbindungen ist der Fluorit. Größere Fluoritvorkommen existieren in Mexiko, China, Südafrika, Spanien und Russland. Auch in Deutschland, etwa bei Wölsendorf in der Oberpfalz findet sich Fluorit.

Ein weiteres natürlich vorkommendes Fluormineral ist Kryolith NaAlF. Die ursprünglich bedeutenden Kryolithvorkommen bei Ivigtut auf Grönland sind ausgebeutet. Das in der Aluminiumproduktion benötigte Kryolith wird heute chemisch hergestellt.

Fluorid-Ionen kommen daneben auch in einigen seltenen Mineralen vor, in denen sie die Hydroxidgruppen ersetzen. Beispiele sind Asbest sowie der Schmuckstein Topas AlSiO(OH, F), Sellait MgF und Bastnäsit (La,Ce)(CO)F. Eine Übersicht gibt die .

Einige wenige Organismen können Fluoride in fluororganische Verbindungen einbauen. Der südafrikanische Busch Gifblaar und weitere Pflanzenarten der Gattung "Dichapetalum" können Fluoressigsäure synthetisieren und in ihren Blättern speichern. Dies dient zur Abwehr von Fressfeinden, für die Fluoressigsäure tödlich wirkt. Die Giftwirkung wird durch Unterbrechung des Citratzyklus ausgelöst.

Das Ausgangsmaterial für die Gewinnung elementaren Fluors und anderer Fluorverbindungen ist überwiegend Fluorit (CaF). Aus diesem wird durch Reaktion mit konzentrierter Schwefelsäure Fluorwasserstoff gewonnen.

Eine weitere Quelle für Flusssäure ist die Phosphatgewinnung, bei der Flusssäure als Abfallprodukt bei der Verarbeitung von Fluorapatit entsteht.

Nur ein kleiner Teil der produzierten Flusssäure wird zu elementarem Fluor weiterverarbeitet. Der größte Teil wird direkt zu anderen fluorierten Verbindungen verarbeitet. Wenn dies nicht möglich ist, wird elementares Fluor benötigt.

Da Fluor eines der stärksten Oxidationsmittel ist, kann es auf chemischem Weg nur sehr umständlich und nicht wirtschaftlich gewonnen werden. Stattdessen wird ein elektrochemisches Verfahren eingesetzt. Die Bruttoreaktion verläuft gemäß:

Das Verfahren wird nach Henri Moissan benannt. Dabei wird kein reiner Fluorwasserstoff zur Elektrolyse verwendet, sondern eine Mischung von Kaliumfluorid und Fluorwasserstoff im Verhältnis von 1:2 bis 1:2,2. Der Hauptgrund für die Verwendung dieser Mischung liegt darin, dass die Leitfähigkeit der Schmelze im Vergleich zu reinem Fluorwasserstoff, der wie reines Wasser Strom nur sehr wenig leitet, stark erhöht ist. Für die Elektrolyse ist es wichtig, dass die Schmelze komplett wasserfrei ist, da sonst während der Elektrolyse Sauerstoff anstatt Fluor entstehen würde.

Technisch wird das sogenannte "Mitteltemperatur-Verfahren" mit Temperaturen von 70 bis 130 °C und einer Kaliumfluorid-Fluorwasserstoff-Mischung von 1:2 angewendet. Bei höheren Fluorwasserstoffgehalten würde ein größerer Dampfdruck entstehen, so dass bei tiefen Temperaturen und aufwändiger Kühlung gearbeitet werden müsste. Bei niedrigeren Gehalten (etwa 1:1) sind die Schmelztemperaturen höher (1:1-Verhältnis: 225 °C), was den Umgang erheblich erschwert und die Korrosion fördert. Die Elektrolyse findet mit Graphit-Elektroden in Zellen aus Stahl oder Monel statt, die zusätzliche Eisenbleche zur Trennung von Anoden- und Kathodenraum enthalten, um eine Durchmischung der entstehenden Gase zu verhindern. An die Elektroden wird eine Spannung von etwa 8–12 Volt angelegt. Der bei der Elektrolyse verbrauchte Fluorwasserstoff wird kontinuierlich ersetzt.

Das Rohfluor, das die Elektrolysezelle verlässt, ist mit Fluorwasserstoff verunreinigt, enthält aber auch Sauerstoff, Tetrafluormethan (CF) und andere Perfluorcarbone, die durch Reaktion von Fluor und dem Elektrodenmaterial entstehen. Diese Verunreinigungen können durch Ausfrieren und Adsorption von Fluorwasserstoff an Natriumfluorid entfernt werden.

Im Labor kann Fluor durch Zersetzung von Mangantetrafluorid (MnF) dargestellt werden. Hierzu wird zunächst KMnF mit SbF versetzt, wobei das instabile blauviolette MnF freigesetzt wird. Dieses Mangantetrafluorid zerfällt bei Temperaturen über 150 °C in F und MnF.

Fluor ist bei Raumtemperatur ein blassgelbes, stechend riechendes Gas. Die Farbe ist von der Schichtdicke abhängig, unterhalb von einem Meter Dicke erscheint das Gas farblos, erst darüber ist es gelb. Unterhalb von −188 °C ist Fluor flüssig und von „kanariengelber“ Farbe. Der Schmelzpunkt des Fluor liegt bei −219,52 °C Von festem Fluor sind zwei Modifikationen bekannt. Zwischen −227,6 °C und dem Schmelzpunkt liegt Fluor in einer kubischen Kristallstruktur mit Gitterparameter a = 667 pm vor (β-Fluor). Unterhalb von −227,6 °C ist die monokline α-Modifikation mit den Gitterparametern a = 550 pm, b = 328 pm, c = 728 pm und β = 102,17° stabil. Fluor ist mit einer Dichte von 1,6959 kg/m³ bei 0 °C und 1013 hPa schwerer als Luft. Der kritische Punkt liegt bei einem Druck von 52,2 bar und einer Temperatur von 144,2 K (−129 °C).

Fluor liegt im elementaren Zustand wie die anderen Halogene in Form zweiatomiger Moleküle vor. Die Bindungslänge im Fluormolekül ist mit 144 pm kürzer als die Einfachbindungen in anderen Elementen (beispielsweise Kohlenstoff-Kohlenstoff-Bindung: 154 pm). Trotz dieser kurzen Bindung ist die Dissoziationsenergie der Fluor-Fluor-Bindung im Vergleich zu anderen Bindungen mit 158 kJ/mol gering und entspricht etwa der des Iodmoleküls mit einer Bindungslänge von 266 pm. Die Gründe für die geringe Dissoziationsenergie liegen vor allem darin, dass sich die freien Elektronenpaare der Fluoratome stark nähern und es zu Abstoßungen kommt. Diese schwache Bindung bewirkt die hohe Reaktivität des Fluors.

Durch die Molekülorbitaltheorie lässt sich die Bindung im Fluormolekül ebenfalls erklären. Dabei werden die s- und p-Atomorbitale der einzelnen Atome zu bindenden und antibindenden Molekülorbitalen zusammengesetzt. Die 1s- und 2s-Orbitale der Fluoratome werden jeweils zu σ und σ*- bindenden und antibindenden Molekülorbitalen. Da diese Orbitale vollständig mit Elektronen gefüllt sind, tragen sie nichts zur Bindung bei. Aus den 2p-Orbitalen werden insgesamt sechs Molekülorbitale mit unterschiedlicher Energie. Dies sind die bindenden σ-, π- und π-, sowie die entsprechenden antibindenden σ*-, π*- und π*-Molekülorbitale. Die π-Orbitale besitzen dabei gleiche Energie. Werden Elektronen in die Molekülorbitale verteilt, kommt es dazu, dass sowohl sämtliche bindenden als auch die antibindenden π*-Orbitale vollständig besetzt sind. Dadurch ergibt sich eine Bindungsordnung von (6–4)/2 = 1 und ein diamagnetisches Verhalten, das auch beobachtet wird.

Fluor gehört zu den stärksten bei Raumtemperatur beständigen Oxidationsmitteln. Es ist das elektronegativste Element und reagiert mit allen Elementen außer Helium und Neon. Die Reaktionen verlaufen meist heftig. So reagiert Fluor im Gegensatz zu allen anderen Halogenen ohne Lichtaktivierung selbst als Feststoff bei −200 °C explosiv mit Wasserstoff unter Bildung von Fluorwasserstoff. Fluor ist das einzige Element, das mit den Edelgasen Krypton, Xenon und Radon direkt reagiert. So bildet sich bei 400 °C aus Xenon und Fluor Xenon(II)-fluorid.

Auch viele andere Stoffe reagieren lebhaft mit Fluor, darunter viele Wasserstoffverbindungen wie beispielsweise Wasser, Ammoniak, Monosilan, Propan oder organische Lösungsmittel. Mit Wasser reagiert Fluor bei verschiedenen Bedingungen unterschiedlich. Werden geringe Mengen Fluor in kaltes Wasser geleitet, bilden sich Wasserstoffperoxid und Flusssäure.
Bei der Reaktion eines Fluor-Überschusses mit geringen Wassermengen, Eis oder Hydroxiden entsteht dagegen als Hauptprodukte Sauerstoff und Sauerstoffdifluorid.

Mit festen Materialien reagiert Fluor dagegen wegen der kleineren Angriffsfläche langsamer und kontrollierter. Bei vielen Metallen führt die Reaktion mit elementarem Fluor zur Bildung einer Passivierungsschicht auf der Metalloberfläche, die das Metall vor dem weiteren Angriff des Gases schützt. Da die Schicht bei hohen Temperaturen oder Fluordrücken nicht dicht ist, kann es dabei zu einer Weiterreaktion von Fluor und Metall kommen, die zur Aufschmelzung des Materials führt. Da beim Aufschmelzen ständig frisches Metall freigelegt wird, das dann wieder zur Reaktion mit Fluor bereitsteht, kann es letztlich sogar zu einem unkontrollierten Reaktionsverlauf kommen (so genanntes "Fluorfeuer").

Auch Kunststoffe reagieren bei Raumtemperatur zumeist sehr kontrolliert mit elementarem Fluor. Wie bei den Metallen, führt auch beim Kunststoff die Reaktion mit Fluor zur Bildung einer fluorierten Oberflächenschicht.

Glas ist bei Raumtemperatur gegen Fluorwasserstoff-freies Fluor inert. Bei höherer Temperatur wird jedoch eine mehr oder weniger schnelle Reaktion beobachtet. Verantwortlich hierfür sind Fluoratome, die durch die thermische Dissoziation des molekularen Fluors gebildet werden und dadurch besonders reaktionsfreudig sind. Produkt der Reaktion ist gasförmiges Siliciumtetrafluorid. Spuren von Fluorwasserstoff führen dagegen auch ohne Erhitzen zu einer schnellen Reaktion.

Fluor ist eines von 22 Reinelementen. Natürlich vorkommendes Fluor besteht zu 100 % aus dem Isotop F. Daneben sind weitere 16 künstliche Isotope von F bis F sowie das Isomer F bekannt. Außer dem Isotop F, das eine Halbwertszeit von 109,77 Minuten besitzt, zerfallen alle anderen künstlichen Isotope innerhalb von Zeptosekunden (10s) bis etwas über einer Minute.

F wird in der Krebsdiagnostik in Form von Fluordesoxyglucose, Fluorethylcholin, Fluorethyltyrosin bzw. F-Fluorid als Radionuklid in der Positronen-Emissions-Tomographie (PET) eingesetzt.

"Siehe auch: Liste der Fluor-Isotope"

Aufgrund der hohen Reaktivität und des schwierigen Umgangs mit Fluor kann elementares Fluor nur eingeschränkt verwendet werden. Es wird überwiegend zu fluorierten Verbindungen weiterverarbeitet, die auf andere Weise nicht hergestellt werden können. Der größte Teil des produzierten Fluors wird für die Herstellung von Uranhexafluorid benötigt, was infolge seiner Leichtflüchtigkeit die Anreicherung von U mit Gaszentrifugen oder durch das Gasdiffusionsverfahren ermöglicht. Dieses Isotop ist für die Kernspaltung wichtig. Ein zweites wichtiges Produkt, das nur mit Hilfe von elementarem Fluor hergestellt werden kann, ist Schwefelhexafluorid. Dieses dient als gasförmiger Isolator beispielsweise in Hochspannungsschaltern und gasisolierten Rohrleitern.

Fluor dient zudem zur Oberflächenfluorierung von Kunststoffen. Dies wird unter anderem bei Kraftstofftanks in Automobilen eingesetzt, wobei sich eine fluorierte Barriereschicht ausbildet, die unter anderem eine niedrigere Benzindurchlässigkeit bewirkt. Diese Anwendung der Fluorierung steht in Konkurrenz zur Koextrusionstechnologien und dem Metalltank. Eine zweite Wirkung der Fluorierung ist, dass die Oberflächenenergie vieler Kunststoffe erhöht werden kann. Dies findet vor allem Anwendungen wo Farben, Lacke oder Klebstoffe auf ansonsten hydrophobe Kunststoffoberflächen (Polyolefine) aufgebracht werden sollen. Die Vorteile der Fluorierung von Kunststoffoberflächen liegen in der Behandelbarkeit von Körpern mit ausgeprägten 3D-Strukturen und Hohlräumen. Zudem lassen sich Kleinteile als Schüttgut behandeln und der Effekt bleibt über eine lange Zeit erhalten. Die Fluorierung wird eingesetzt, wenn weiter verbreitete und kostengünstigere Methoden, wie z. B. die Beflammung nicht einsetzbar sind. Weiter mögliche Effekte die durch Fluorierung von Kunststoffoberflächen erreicht werden können sind: verbesserte Faser-Matrix-Haftung, verringerte Reibung und verbesserte Selektivitäten in der Membrantechnik.

Werden Fluor und Graphit zusammen erhitzt, entsteht Graphitfluorid, das als Trockenschmiermittel und Elektrodenmaterial eingesetzt werden kann.

Für Fluoridionen existieren mehrere Nachweise. Bei der sogenannten "Kriechprobe" wird in einem Reagenzglas aus Glas eine fluoridhaltige Substanz mit konzentrierter Schwefelsäure versetzt.

Es steigen Fluorwasserstoffdämpfe auf, die das Glas anätzen. Gleichzeitig ist die Schwefelsäure aufgrund der Veränderung der Oberfläche nicht mehr in der Lage, das Glas zu benetzen.
Eine zweite Nachweismöglichkeit ist die sogenannte "Wassertropfenprobe". Dabei wird die fluoridhaltige Substanz mit Kieselsäure und Schwefelsäure zusammengebracht. Es entsteht gasförmiges Siliciumtetrafluorid. Über das Gefäß mit der Probe wird ein Wassertropfen gehalten. Durch Reaktion von Siliciumtetrafluorid mit dem Wasser bildet sich Siliciumdioxid, das als charakteristischer weißer Rand um den Tropfen kristallisiert.

"Siehe auch: Nachweise für Fluorid"

In der modernen Analytik, insbesondere für organische Fluorverbindungen spielt die NMR-Spektroskopie eine große Rolle. Fluor besitzt den Vorteil, zu 100 % aus einem Isotop (Reinelement) zu bestehen, das durch NMR-Spektroskopie nachweisbar ist.

Es ist umstritten, ob Fluor ein für den menschlichen Organismus essentielles Spurenelement ist. Laut einem im Jahr 2013 veröffentlichten Gutachten der Europäischen Behörde für Lebensmittelsicherheit (EFSA) ist Fluorid kein essentieller Nährstoff, da es weder Wachstumsprozessen noch der Zahnentwicklung dient und Zeichen eines Fluoridmangels nicht identifiziert werden konnten.

Im Körper sind etwa 5 g Fluoride (bei 70 kg Körpergewicht) enthalten. Es ist sehr ungleichmäßig verteilt, der weitaus größte Teil ist in den Knochen und Zähnen enthalten.

Fluorid kann vor Zahnkaries schützen und den Zahnschmelz härten. Durch den Einbau von Fluorid- anstatt von Hydroxid-Ionen in den Hydroxylapatit der Zähne entsteht Fluorapatit. Letzterer ist schwerer in Wasser löslich und damit stabiler gegenüber dem Speichel. Fluorid wirkt durch die geringe Löslichkeit des Fluorapatits remineralisierend, indem der durch Säuren aufgelöste Apatit in Anwesenheit von Fluorid wieder ausgefällt wird. Außerdem wirkt Fluorid hemmend auf bestimmte Enzyme und bewirkt eine Unterbrechung der Glykolyse in kariesverursachenden Bakterien, was deren Wachstum hemmt.

Die Aufnahme von Fluorid auf natürlichen Wegen erfolgt in der Regel mit dem Trinkwasser oder der Nahrung. Nehmen Kinder während der Zahnentwicklung zu viel Fluorid auf, kann sich eine Dentalfluorose bilden. Diese bewirkt punktförmige bis fleckig-braune Verfärbungen ("mottled teeth" oder "mottled enamel") auf der Zahnoberfläche, zudem ist der Zahn fragiler und weniger widerstandsfähig. Die maximal empfohlenen Höchstmengen an Fluorid, die ein Mensch täglich zu sich nehmen sollte, betragen für Säuglinge bis sechs Monate 0,7 mg, von 7–17 Monaten 0,9 mg bei Kindern bis drei Jahren 1,3 mg. Kinder von vier bis acht Jahren sollten nicht mehr als 2,2 mg Fluorid pro Tag aufnehmen. Wenn danach die Zahnentwicklung abgeschlossen ist, verträgt der Mensch höhere Dosen bis zu 10 mg Fluorid pro Tag.

In Deutschland ist die Trinkwasserfluoridierung nicht zulässig, in der Schweiz wurde sie bis 2003 in Basel durchgeführt. Bis zum Jahr 2000 durfte darum in Basel auch kein fluoridhaltiges Salz verkauft werden.

Da Fluorid ähnlich Selen in größeren Mengen toxisch wirkt, existiert nur ein kleiner Bereich, in dem Fluorid im Körper vorkommen darf, ohne toxisch zu wirken.

Fluor und viele Fluorverbindungen sind für den Menschen und andere Lebewesen sehr giftig, die letale Dosis (LC, eine Stunde) liegt bei elementarem Fluor bei 150–185 ppm. Elementares Fluor wirkt auf Lunge, Haut und besonders auf die Augen stark verbrennend und verätzend. Schon bei einem fünfminütigen Kontakt mit 25 ppm Fluor kommt es zu einer erheblichen Reizung der Augen. Gleichzeitig entsteht durch Reaktion mit Wasser (Luftfeuchtigkeit, Hautoberfläche) der ebenfalls giftige Fluorwasserstoff. Eine akute Fluorvergiftung äußert sich je nachdem, über welchen Weg, in welcher Verbindung und Dosis das Fluor in den Körper gelangt ist, mit unterschiedlichen Beschwerden. Eine gastrointestinal entstandene akute Vergiftung mit Fluoriden führt zu Schleimhautverätzungen, Übelkeit, anfänglich schleimigem, später blutigem Erbrechen, unstillbarem Durst, heftigen Leibschmerzen und blutigem Durchfall. Teilweise versterben Betroffene. Bei Aufnahme von Fluorwasserstoff und staubförmigen Fluoriden mit der Atemluft folgen Tränenfluss, Niesen, Husten, Atemnot, Lungenödem bis hin zum Tod unter Krämpfen. Eine über die Haut entstandene Vergiftung mit Fluorwasserstoff (auch in sauren Lösungen von Fluoriden) hat tiefgreifende Nekrosen und schlecht heilende Ulzera zur Folge.

Als schwach dissoziiertes Molekül wird Fluorwasserstoff leicht durch die Haut aufgenommen. Es kommt zu schmerzhaften Entzündungen, später zu hartnäckigen, schlecht abheilenden Geschwüren. Außerdem bildet HF starke Wasserstoffbrückenbindungen aus und ist so in der Lage, die Tertiärstruktur von Proteinen zu verändern. Mit Aluminium-Ionen bildet Fluorid Fluoridoaluminat-Komplexe, die eine Phosphat-analoge Struktur haben und so zur Deregulierung von G-Proteinen beitragen. Resultat ist ein Eingriff in die rezeptorgekoppelte Signalübertragung und – via signalabhängige Phosphorylierung /Dephosphorylierung – in die Aktivität vieler Enzyme. Bekanntestes Beispiel für eine Enzym-Hemmung durch Fluorid ist Enolase, ein Enzym der Glykolysekette.

Die hochtoxischen Fluoracetate und Fluoracetamid werden nach der Resorption zu Fluorcitrat metabolisiert. Diese Verbindung führt zur Blockade des für den Citratzyklus wichtigen Enzyms Aconitase. Dies bewirkt eine Anreicherung von Citrat im Blut, was wiederum die Körperzellen von der Energiezufuhr abschneidet. Perfluorierte Alkane, die als Blutersatzstoffe in der Erprobung sind, und die handelsüblichen Fluorcarbone, wie PTFE ("Teflon"), PVDF oder PFA gelten als ungiftig.

Das schwerlösliche Calciumfluorid, das sich bei der Reaktion mit Calcium – etwa in den Knochen – bildet, wurde früher für inert und daher harmlos gehalten. Zumindest die Stäube von Calciumfluorid haben sich jedoch sowohl im Tierversuch, als auch beim Menschen, als toxisch erwiesen. Ob in vivo bei akuter Fluoridvergiftung tatsächlich schwerlösliches Calciumfluorid gebildet wird, wie so oft vermutet, konnte im Rahmen gezielter Untersuchungen nicht bewiesen werden.

Die Aufnahme von mehr als 20 mg Fluorid pro Tag führt zu einer chronischen Fluorvergiftung, die auch Fluorose genannt wird. Symptome sind Husten, Auswurf, Atemnot, eine Dentalfluorose mit Veränderung von Struktur und Farbe des Zahnschmelzes, eine Fluorosteopathie und teilweise eine Fluorokachexie. Die Fluorosteopathie führt durch Vermehrung des Knochengewebes zu Elastizitätsverlust und erhöhten Knochenbrüchigkeit (Osteosklerose) bis hin zum völligen Versteifen von Gelenken oder gar der Wirbelsäule. Da gleichzeitig mit Hilfe hoher Fluoriddosen das Knochenwachstum stimuliert werden kann, verwendet man Fluoride zur Behandlung verschiedener Formen der Osteoporose.

Eine Studie untersuchte einen möglichen Zusammenhang von Fluoridaufnahme durch Trinkwasser und dem Auftreten von Osteosarkomen, einer Krebsart. Die aufgenommene Fluoridmenge wurde geschätzt anhand der bevorzugten Trinkwasserquelle. Eine statistische Analyse hierzu fand eine positive Korrelation von Fluoridaufnahme und Krebsrate, allerdings nur bei Männern, nicht bei Frauen. Ein unabhängiger Kommentar zu dieser Analyse bemerkt, dass der Zusammenhang lediglich bei der ersten Gruppe der untersuchten Personen feststellbar war, bei einer späteren zweiten Gruppe hingegen nicht mehr. Die Autoren kommen darin zu der Einschätzung, dass daraus kein erhöhtes Krebsrisiko abgeleitet werden kann. Die Internationale Agentur für Krebsforschung war in ihrer Bewertung 1982 zu dem Ergebnis gekommen, dass es keine Anzeichen einer krebserzeugenden Wirkung von anorganischen Fluoriden "im Zusammenhang mit der Trinkwasserfluoridierung" gibt.

Schäden, die durch die Arbeit mit Fluoriden entstehen, wie Skelettfluorose, Lungenschäden, Reizung des Magen-Darm-Trakts oder Verätzungen sind als Berufskrankheiten anerkannt. Im Berufskrankheiten-System sind sie unter Bk Nr. 13 08 erfasst.

Auf Grund seiner hohen Reaktivität muss Fluor in speziellen Behältnissen aufbewahrt werden. Die Werkstoffe müssen dabei so beschaffen sein, dass sie durch den Kontakt mit Fluor eine Passivierungsschicht ausbilden und so eine Weiterreaktion verhindern. Beispiele für geeignete Werkstoffe sind Stahl oder die Nickel-Kupfer-Legierung Monel. Nicht geeignet sind beispielsweise Glas, das durch entstandenen Fluorwasserstoff angegriffen wird, oder Aluminium. Brennbare Stoffe wie Fett dürfen ebenfalls nicht in Kontakt mit Fluor kommen, da sie unter heftiger Reaktion verbrennen.

Fluor brennt zwar selbst nicht, wirkt aber brandfördernd. Brände bei Anwesenheit von Fluor können nicht mit Löschmitteln gelöscht werden, es muss zunächst der weitere Zutritt von Fluor verhindert werden.

Als elektronegativstes aller Elemente kommt Fluor in Verbindungen fast ausschließlich in der Oxidationsstufe −I vor. Es sind von allen Elementen außer Helium und Neon Fluorverbindungen bekannt.

Fluorwasserstoff ist ein stark ätzendes, giftiges Gas. Die wässrige Lösung des Fluorwasserstoffs wird Flusssäure genannt. Während wasserfreier, flüssiger Fluorwasserstoff zu den stärksten Säuren, den so genannten Supersäuren zählt, ist Flusssäure nur mittelstark. Fluorwasserstoff ist eine der wenigen Substanzen, die direkt mit Glas reagieren. Dementsprechend ist die Verwendung als Ätzlösung in der Glasindustrie eine Anwendung von Flusssäure. Darüber hinaus ist Fluorwasserstoff das Ausgangsmaterial für die Herstellung von Fluor und vielen anderen Fluorverbindungen.

Fluoride sind die Salze des Fluorwasserstoffs. Sie sind die wichtigsten und verbreitetsten Fluorsalze. In der Natur kommt vor allem das schwerlösliche Calciumfluorid CaF in Form des Minerals Fluorit vor. Technisch spielen auch andere Fluoride eine Rolle. Beispiele sind das unter Verwendung erwähnte Uranhexafluorid oder Natriumfluorid, das unter anderem als Holzschutzmittel verwendet wird und früher als Rattengift und Insektizid patentiert und vermarktet wurde.

Ein in der organischen Chemie häufig verwendetes Fluorid ist das Tetrabutylammoniumfluorid (TBAF). Da TBAF in organischen Lösungsmitteln löslich ist und das Fluoridion nicht durch Kationen beeinflusst wird (sogenanntes „nacktes Fluorid“) wird es als Fluoridquelle in organischen Reaktionen benutzt. Eine weitere wichtige Reaktion des Tetrabutylammoniumfluorids ist die Abspaltung von Silylethern, die als Schutzgruppe für Alkohole verwendet werden.

Es existiert eine Reihe von organischen Fluorverbindungen. Eine der bekanntesten fluorhaltigen Stoffgruppen sind die Fluorchlorkohlenwasserstoffe (FCKW). Die niedermolekularen FCKW mit einem oder zwei Kohlenstoffatomen sind gasförmige Stoffe und dienten früher als Kältemittel in Kühlschränken und Treibgas für Spraydosen. Da diese Stoffe den Ozonabbau verstärken und somit die Ozonschicht schädigen, ist ihre Herstellung und Verwendung mit dem Montreal-Protokoll stark eingeschränkt worden. Dagegen sind Fluorkohlenwasserstoffe für die Ozonschicht ungefährlich. Eine weitere umweltschädliche Auswirkung fluorhaltiger organischer Verbindungen ist ihre Absorptionsfähigkeit für Infrarotstrahlung. Daher wirken sie als Treibhausgase.

Eine aus dem Alltag bekannte organische Fluorverbindung ist Polytetrafluorethen (PTFE), die unter dem Handelsnamen "Teflon®" als Beschichtung von Bratpfannen verwendet wird. Perfluorierte Tenside, die unter anderem bei der Herstellung von PTFE verwendet werden, und andere perfluorierte Verbindungen verfügen über eine äußerst stabile Kohlenstoff-Fluor-Bindung. Diese Bindung verleiht den Stoffen eine hohe chemische und thermische Beständigkeit, was aber auch dazu führt, dass die Substanzen in der Umwelt persistent sind und kaum abgebaut werden.

"Siehe auch "

Mit den anderen Halogenen bildet Fluor eine Reihe von Interhalogenverbindungen. Ein wichtiges Beispiel hierfür ist Chlortrifluorid, ein giftiges Gas, das vor allem als Fluorierungsmittel eingesetzt wird.

Fluor ist elektronegativer als Sauerstoff, weshalb die Verbindungen zwischen Fluor und Sauerstoff nicht wie die anderen Halogen-Sauerstoffverbindungen als Halogenoxide, sondern als Sauerstofffluoride bezeichnet werden.

Im Gegensatz zu den schwereren Halogenen existiert nur eine Fluorsauerstoffsäure, die Hypofluorige Säure HOF. Der Grund hierfür liegt darin, dass Fluor keine Drei-Elektronen-vier-Zentren-Bindungen ausbildet.

Fluor bildet auch mit den Edelgasen Krypton, Xenon, und Argon einige Verbindungen wie Xenon(II)-fluorid. Kryptondifluorid, die einzige bekannte Kryptonverbindung, ist das stärkste bekannte Oxidationsmittel. Weitere Edelgasverbindungen des Fluor enthalten oft auch noch Atome anderer Elemente, wie beispielsweise das Argonhydrogenfluorid (HArF), die einzige bekannte Argonverbindung.




</doc>
<doc id="1576" url="https://de.wikipedia.org/wiki?curid=1576" title="Fermium">
Fermium

Fermium ist ein ausschließlich künstlich erzeugtes chemisches Element mit dem Elementsymbol Fm und der Ordnungszahl 100. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Fermium ist ein radioaktives Metall, welches aber aufgrund der geringen zur Verfügung stehenden Mengen bisher nicht als Metall dargestellt wurde. Es wurde 1952 nach dem Test der ersten amerikanischen Wasserstoffbombe entdeckt und Enrico Fermi zu Ehren benannt, der jedoch persönlich mit der Entdeckung von bzw. Forschung an Fermium nichts zu tun hatte.

Fermium wurde zusammen mit Einsteinium nach dem Test der ersten amerikanischen Wasserstoffbombe, Ivy Mike, am 1. November 1952 auf dem Eniwetok-Atoll gefunden. Erste Proben erhielt man auf Filterpapieren, die man beim Durchfliegen durch die Explosionswolke mitführte. Größere Mengen isolierte man später aus Korallen. Aus Gründen der militärischen Geheimhaltung wurden die Ergebnisse zunächst nicht publiziert.

Eine erste Untersuchung der Explosionsüberreste hatte die Entstehung eines neuen Plutoniumisotops Pu aufgezeigt, dies konnte nur durch die Aufnahme von sechs Neutronen durch einen Uran-238-Kern und zwei folgende β-Zerfälle entstanden sein.

Zu der Zeit nahm man an, dass die Absorption von Neutronen durch einen schweren Kern ein seltener Vorgang wäre. Die Identifizierung von Pu ließ jedoch den Schluss zu, dass Urankerne viele Neutronen einfangen können, was zu neuen Elementen führt.

Die Bildung gelang durch fortgesetzten Neutroneneinfang: Im Moment der Detonation war die Neutronenflussdichte so hoch, dass die meisten der zwischenzeitlich gebildeten – radioaktiven – Atomkerne bis zum jeweils nächsten Neutroneneinfang noch nicht zerfallen waren. Bei sehr hohem Neutronenfluss steigt also die Massenzahl stark an, ohne dass sich die Ordnungszahl ändert. Erst anschließend zerfallen die entstandenen instabilen Nuklide über viele β-Zerfälle zu stabilen oder instabilen Nukliden mit hoher Ordnungszahl:

Die Entdeckung von Fermium ("Z" = 100) erforderte mehr Material, da man davon ausging, dass die Ausbeute mindestens eine Größenordnung niedriger als die von Element 99 sein würde. Daher wurden kontaminierte Korallen aus dem Eniwetok-Atoll (wo der Test stattgefunden hatte) zum University of California Radiation Laboratory in Berkeley, Kalifornien, zur Verarbeitung und Analyse gebracht. Die Trennung der gelösten Actinoid-Ionen erfolgte in Gegenwart eines Citronensäure/Ammoniumcitrat-Puffers im schwach sauren Medium (pH ≈ 3,5) mit Ionenaustauschern bei erhöhter Temperatur. Etwa zwei Monate später wurde eine neue Komponente isoliert, ein hochenergetischer α-Strahler (7,1 MeV) mit einer Halbwertszeit von etwa einem Tag. Mit einer derart kurzen Halbwertszeit konnte es nur aus dem β-Zerfall eines Einsteiniumisotops entstehen, und so musste ein Isotop des Elements 100 das neue sein: Es wurde schnell als Fm identifiziert (t = 20,07 Stunden).

Im September 1953 war noch nicht abzusehen, wann die Ergebnisse der Teams in Berkeley, Argonne und Los Alamos veröffentlicht werden könnten. Man entschied sich dazu, die neuen Elemente durch Beschussexperimente herzustellen; gleichzeitig versicherte man sich, dass diese Ergebnisse nicht unter Geheimhaltung fallen würden und somit veröffentlicht werden konnten. Einsteiniumisotope wurden kurz danach am "University of California Radiation Laboratory" durch Beschuss von Uran (U) mit Stickstoff (N) hergestellt. Dabei merkte man an, dass es Forschungen zu diesem Element gebe, die bislang noch unter Geheimhaltung stehen. Isotope der beiden neu entdeckten Elemente wurden durch Bestrahlung des Plutoniumisotops Pu erzeugt, die Ergebnisse wurden in fünf kurz aufeinander folgenden Publikationen veröffentlicht. Die letzten Reaktionen ausgehend von Californium sind:

Das Team in Berkeley war zudem besorgt, dass eine andere Forschergruppe die leichteren Isotope des Elements 100 durch Ionenbeschuss entdecken und veröffentlichen könnte, bevor sie ihre unter Geheimhaltung stehende Forschung hätten veröffentlichen können. Denn im ausgehenden Jahr 1953 sowie zu Anfang des Jahres 1954 beschoss eine Arbeitsgruppe des Nobel-Instituts für Physik in Stockholm Urankerne mit Sauerstoffkernen; es bildete sich das Isotop mit der Massenzahl 250 des Elements 100 (Fm). Die zweifelsfreie Identifizierung konnte anhand der charakteristischen Energie des beim Zerfall ausgesandten α-Teilchens erlangt werden.

Das Team in Berkeley veröffentlichte schon einige Ergebnisse der chemischen Eigenschaften beider Elemente. Schließlich wurden die Ergebnisse der thermonuklearen Explosion im Jahr 1955 freigegeben und anschließend publiziert.

Letztlich war die Priorität des Berkeley-Teams allgemein anerkannt, da ihre fünf Publikationen der schwedischen Publikation vorausgingen, und sie sich auf die zuvor noch geheimen Ergebnisse der thermonuklearen Explosion von 1952 stützen konnten. Damit war das Vorrecht verbunden, den neuen Elementen den Namen zu geben. Sie entschieden sich, diese fortan nach berühmten, bereits verstorbenen Wissenschaftlern zu benennen. Man war sich schnell einig, die Namen zu Ehren von Albert Einstein und Enrico Fermi zu vergeben, die beide erst vor kurzem verstorben waren: „We suggest for the name for the element with the atomic number 99, einsteinium (symbol E) after Albert Einstein and for the name for the element with atomic number 100, fermium (symbol Fm), after Enrico Fermi.“ Die Bekanntgabe für die beiden neu entdeckten Elemente "Einsteinium" und "Fermium" erfolgte durch Albert Ghiorso auf der 1. Genfer Atomkonferenz, die vom 8. bis 20. August 1955 stattfand.

Später wurde das Element zeitweilig mit dem systematischen Namen "Unnilnilium" bezeichnet.

Sämtliche bisher bekannten 19 Nuklide und 3 Kernisomere sind radioaktiv und instabil. Die bekannten Massenzahlen reichen von 242 bis 260. Die mit Abstand längste Halbwertszeit hat das Isotop Fm mit 100,5 Tagen, so dass es auf der Erde keine natürlichen Vorkommen mehr geben kann. Fm hat eine Halbwertszeit von 3 Tagen, Fm von 5,3 h, Fm von 25,4 h, Fm von 3,2 h, Fm von 20,1 h und Fm von 2,6 h. Alle übrigen haben Halbwertszeiten von 30 Minuten bis unterhalb einer Millisekunde.

Nimmt man den Zerfall des langlebigsten Isotops Fm heraus, so entsteht durch α-Zerfall zunächst das Cf, das seinerseits durch β-Zerfall in Es übergeht. Der weitere Zerfall führt dann über Bk, Cf, Cm, Am, Pu, Am zum Np, dem Beginn der Neptunium-Reihe (4 n + 1).

Als "Fermiumbarriere" bezeichnet man den Umstand, dass die Fermiumisotope Fm, Fm und Fm zum Teil schon nach Bruchteilen von Sekunden durch Spontanspaltung zerfallen (t = 370 µs, 1,5 s bzw. 4 ms). Fm ist ein α-Strahler und zerfällt zu Cf. Zudem zeigt keines der bislang bekannten Fermiumisotope β-Zerfälle, was die Bildung von Mendelevium durch Zerfall aus Fermium verhindert. Diese Tatsachen vereiteln praktisch jede Bemühung, mit Hilfe von Neutronenstrahlung, zum Beispiel mit Hilfe eines Kernreaktors, Elemente mit Ordnungszahlen über 100 bzw. Massenzahlen größer als 257 zu erzeugen. Fermium ist somit das letzte Element, das durch Neutroneneinfang hergestellt werden kann. Jeder Versuch, weitere Neutronen zu einem Fermium-Kern hinzuzufügen, führt zu einer Spontanspaltung.

Fermium wird durch Beschuss von leichteren Actinoiden mit Neutronen in einem Kernreaktor erzeugt. Die Hauptquelle ist der 85 MW High-Flux-Isotope Reactor am Oak Ridge National Laboratory in Tennessee, USA, der auf die Herstellung von Transcuriumelementen (Z > 96) eingerichtet ist.

In Oak Ridge sind größere Mengen an Curium bestrahlt worden, um Dezigramm-Mengen an Californium, Milligramm-Mengen an Berkelium und Einsteinium sowie Pikogramm-Mengen an Fermium zu erzeugen. Nanogramm- und Mikrogramm-Mengen von Fermium können für bestimmte Experimente vorbereitet werden. Die Mengen an Fermium, die in thermonuklearen Explosionen von 20 bis 200 Kilotonnen entstehen, bewegen sich vermutlich in der Größenordnung von einigen Milligramm, obwohl es mit einer riesigen Menge von Explosionsresten gemischt ist; 40 Pikogramm Fm wurden aus 10 Kilogramm der Explosionsreste aus dem Hutch-Test vom 16. Juli 1969 isoliert.

Nach der Bestrahlung muss Fermium von den anderen Actinoiden und den Lanthanoid-Spaltprodukten getrennt werden. Dies wird üblicherweise durch Ionenaustauschchromatographie erreicht, das Standardverfahren läuft mit Kationenaustauschern wie Dowex 50 oder T, man eluiert mit einer Lösung von Ammonium-α-hydroxyisobuttersäuremethylester. Kleinere Kationen bilden stabilere Komplexe mit den α-Hydroxyisobuttersäuremethylester-Anionen, daher werden sie bevorzugt von der Säule eluiert. Eine schnelle fraktionierte Kristallisationsmethode wurde ebenfalls beschrieben.

Obwohl das stabilste Isotop des Fermiums das Fm mit einer Halbwertszeit von 100,5 Tagen ist, basieren die meisten Studien auf Fm (t = 20,07 Stunden). Dieses Isotop kann leicht isoliert werden, es ist ein Zerfallsprodukt des Es (t = 39,8 Tage).

Geringe Mengen an Einsteinium und Fermium wurden aus Plutonium isoliert und abgetrennt, welches mit Neutronen bestrahlt wurde. Vier Einsteiniumisotope wurden gefunden (mit Angabe der damals gemessenen Halbwertszeiten): Es (α-Strahler mit t = 20,03 Tage, sowie mit einer Spontanspaltungs-Halbwertszeit von 7×10 Jahren); Es (β-Strahler mit t = 38,5 Stunden), Es (α-Strahler mit t = ∼ 320 Tage) und Es (β-Strahler mit t = 24 Tage). Zwei Fermiumisotope wurden gefunden: Fm (α-Strahler mit t = 3,24 Stunden, sowie mit einer Spontanspaltungs-Halbwertszeit von 246 Tagen) und Fm (α-Strahler mit t = 21,5 Stunden).

Durch Beschuss von Uran mit fünffach ionisierten Stickstoff- und sechsfach ionisierten Sauerstoffatomen wurden gleichfalls Einsteinium- und Fermiumisotope erzeugt.

Im Periodensystem steht das Fermium mit der Ordnungszahl 100 in der Reihe der Actinoide, sein Vorgänger ist das Einsteinium, das nachfolgende Element ist das Mendelevium. Sein Analogon in der Reihe der Lanthanoide ist das Erbium.

Das Metall wurde bislang nicht dargestellt, hingegen erfolgten Messungen an Legierungen mit Lanthanoiden, ferner liegen einige Berechnungen oder Vorhersagen vor. Die Sublimationsenthalpie ist direkt mit der Valenzelektronenstruktur des Metalls verbunden. Die Sublimationsenthalpie von Fermium wurde direkt durch Messung des Partialdrucks des Fermiums über Fm-Sm- und Fm/Es-Yb-Legierungen im Temperaturbereich von 642 bis 905 K bestimmt. Sie gelangten zu einem Wert von 142(13) kJ·mol. Da die Sublimationsenthalpie von Fermium ähnlich ist zu denen des zweiwertigen Einsteinium, Europium und Ytterbium, wurde der Schluss gezogen, dass Fermium einen zweiwertigen metallischen Zustand besitzt. Vergleiche mit Radien und Schmelzpunkten von Europium-, Ytterbium- und Einsteinium-Metall führten zu geschätzten Werten von 198 pm und 1125 K für Fermium.

Das Normalpotential wurde als ähnlich zum Ytterbium Yb/Yb-Paar eingeschätzt, also etwa −1,15 V in Bezug auf die Standard-Wasserstoffelektrode, ein Wert, der mit theoretischen Berechnungen übereinstimmt. Auf der Grundlage polarographischer Messungen wurde für das Fm/Fm-Paar ein Normalpotential von −2,37 V festgestellt. Fm kann relativ leicht zu Fm reduziert werden, z. B. mit Samarium(II)-chlorid, mit dem Fermium zusammen ausfällt.

Die Chemie des Fermiums konnte bisher nur in Lösung mit Hilfe von Tracertechniken untersucht werden, feste Verbindungen wurden nicht hergestellt. Unter normalen Bedingungen liegt Fermium in Lösung als Fm-Ion vor, welches eine Hydratationszahl von 16,9 besitzt und eine Säurekonstante von 1,6 · 10 (pK = 3,8). Fm bildet Komplexe mit einer Vielzahl von organischen Liganden mit harten Donoratomen wie Sauerstoff; und diese Komplexe sind in der Regel stabiler als die der vorhergehenden Actinoide. Es bildet auch anionische Komplexe mit Liganden wie Chlorid oder Nitrat; und auch diese Komplexe scheinen stabiler zu sein als die von Einsteinium oder Californium. Es wird angenommen, dass die Bindung in den Komplexen der höheren Actinoide meist ionischen Charakter hat: das Fm-Ion ist erwartungsgemäß kleiner als die vorhergehenden An-Ionen – aufgrund der höheren effektiven Kernladung von Fermium –; und damit würde Fermium voraussichtlich kürzere und stärkere Metall-Ligand-Bindungen bilden.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.

Fermium wird – in Form seiner Verbindungen in Lösung – in erster Linie in geringen Mengen zu Studienzwecken gewonnen. Verbindungen des Fermiums wurden in fester Form bislang nicht dargestellt.




</doc>
<doc id="1577" url="https://de.wikipedia.org/wiki?curid=1577" title="Francium">
Francium

Francium [] ist ein radioaktives chemisches Element mit dem Elementsymbol Fr und der Ordnungszahl 87. Das Element ist ein Metall, steht in der 7. Periode, 1. IUPAC-Gruppe (Gruppe der Alkalimetalle) und gehört damit zum s-Block.

Francium besitzt von allen Elementen bis zur Ordnungszahl 104 die in ihrer Gesamtheit instabilsten Isotope. Selbst das langlebigste Francium-Isotop Fr besitzt eine Halbwertszeit von nur 21,8 Minuten. Wegen dieser Eigenschaft und des Fehlens einer effizienten Kernreaktion zur Herstellung von Francium (Fr entsteht in 1 % beim Zerfall von Ac), kann es nicht in Mengen hergestellt werden. Francium kann nur als Salz in verdünnten Lösungen und hoch verdünnt als Amalgam studiert werden.

Experimente zeigen, dass Francium ein typisches Alkalimetall ist und seinem leichteren Homologon Caesium sehr ähnlich ist. So ist es in wässriger Lösung positiv einwertig und lässt sich analog zum Caesium in Form schwerlöslicher Salze, z. B. als Perchlorat, Tetraphenylborat und Hexachloroplatinat, ausfällen.

Im Jahre 1871 wurde von Dmitri Iwanowitsch Mendelejew die Existenz eines Elementes vorhergesagt, das den zu diesem Zeitpunkt noch leeren Platz innerhalb seines Periodensystem einnehmen würde. Er beschrieb es als Alkalimetall und gab ihm den Namen "Eka-Caesium".

Vor der eigentlichen Entdeckung des Franciums gab es drei irreführend als Entdeckung dieses Elements bezeichnete Ereignisse:

Im Jahre 1925 veröffentlichte Dmitri Dobroserdow eine Theoriestudie, in der er Voraussagen über das Atomgewicht sowie chemische und physikalische Eigenschaften machte. Er nannte das Element "Russium".

Ein Jahr danach, im Jahre 1926, beobachteten die zwei englischen Chemiker Gerald Druce und Frederic H. Loring die Spektrallinien des Elements bei Untersuchungen von Mangansulfat. 1929 meldete der amerikanische Physiker Fred Allison die Entdeckung des Elementes bei Untersuchungen von Mineralen und benannten das Element "Virginium" nach seinem Heimatstaat Virginia.

1936 wiederum gingen der Rumäne Horia Hulubei und die Französin Yvette Cauchois davon aus, das Element entdeckt zu haben und gaben ihm den Namen "Moldavium". Allerdings konnte keine dieser Entdeckungen bei anderen Wissenschaftlern Bestätigung finden.

Erst 1939 konnte Marguerite Perey das Element als ein Isotop Fr als Zerfallsprodukt von Actinium Ac zweifelsfrei nachweisen. Es wurde zunächst "Actinium-K" genannt und 1946 in "Francium" (von franz. "France" „Frankreich“, dem Vaterland der Entdeckerin) umbenannt. Der Name wurde 1949 von der Internationalen Vereinigung der Chemiker akzeptiert.

Die physikalischen Eigenschaften sind im Wesentlichen Schätzungen, die durch Extrapolation der Eigenschaften der Alkalimetalle oder durch Modellrechnungen bestimmt wurden. Sie sind daher mit großer Vorsicht zu behandeln. Messungen an Festkörpern sind durch die geringen herstellbaren Mengen (wenige Attogramm, ~10.000 Atome) und die hohe Radioaktivität (Aktivität ist etwa 2-Mio.-mal höher als die der gleichen Menge von Pu: sichtbare Mengen würden sofort verdampfen) meist mit den heutigen Mitteln der Technik unmöglich.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.



</doc>
<doc id="1579" url="https://de.wikipedia.org/wiki?curid=1579" title="Fuß (Einheit)">
Fuß (Einheit)

Ein Fuß (engl. "," Plural ') bzw. Schuh ist ein früher in vielen Teilen der Welt verwendetes Längenmaß, das je nach Land meist 28 bis 32 cm maß, in Extremfällen auch 25 und 34 cm. Es ist neben der Fingerbreite, der Handbreite, der Handspanne, der Elle, dem Schritt und dem Klafter eine der ältesten Längeneinheiten.

Das einzige heute noch übliche Fußmaß, der "englische Fuß," beträgt 1 ft = 30,48 cm (12 Zoll). Obwohl es sich nicht um eine SI-Einheit handelt, wird die Einheit Fuß auch international noch häufig verwendet, vor allem in der See- und Luftfahrt.

Seit wann der Fuß als Maßeinheit verwendet wird, ist umstritten. Sichere Schlüsse können aus den frühesten Funden von Maßstäben gezogen werden. Das älteste unbeschädigte Fundstück dieser Art ist die sogenannte Nippur-Elle aus Mesopotamien. Durch Einkerbungen erschließen sich Untereinheiten zu 30 Fingerbreit ("digiti" à 1,73 cm), woraus sich die Maßeinheiten "Fuß" mit 16 "digiti" (27,6 cm) sowie die Handbreite ("palmus" = 4 "digiti") ergeben. Versuche, die Länge der Elle an Gebäuden zu überprüfen, führten zu einem Mittelwert von 518,65 mm.

Ob daraus – wie im Fall des megalithischen Yard – ein gemeinsames Urmaß abgeleitet werden kann, ist in der Fachwelt umstritten. Identische Längen oder ihre Untereinheiten in verschiedenen Kulturen könnten auch eine Folge der Einheitlichkeit von Körpermaßen sein, auf die sie zurückgehen.

Noch vor der IV. Pharaonen-Dynastie teilten ägyptische Geometer die Nippur-Elle nur noch in 28 Teile. Dadurch wuchs der Fuß als Maß auf 51,8 cm ÷ 28 · 16 ≈ 29,6 cm. Genau diese Länge hatte auch das römische Fußmaß. Demnach unterhalten der megalithische bzw. Nippur-Fuß und der römische Fuß ein Verhältnis von genau 28 zu 30.

Ein Fuß (lat. "pes" ≈ 29,6 cm) ist also vier Handbreit (lat. "palmus" ≈ 7,4 cm) bzw. sechzehn Fingerbreit (lat. "digitus" ≈ 1,85 cm). Neben dem offiziellen "pes monetalis" wurde in einigen Teilen der römischen Nordwestprovinzen auch der sogenannte "pes drusianus" (≈ 33,27 cm) verwendet, der gegenüber dem offiziellen Fußmaß um etwa 2 "digiti" länger war. Er wurde benannt nach dem Feldherrn Nero Claudius Drusus. Das „Vier-Fuß-Maß“ nannte man in der Spätantike auf Lateinisch "ulna" (Elle). Das „Maß von 1½ Fuß“ ist die natürliche Elle (lat. "cubitus"). Das „Fünf-Fuß-Maß“ ist der Doppelschritt (lat. "passus"). Das englische "yard" hat genau drei Fuß.

Im alten Griechenland zum Beispiel gab es neben dem hauptsächlich verwendeten, eigentlichen Fuß (griechisch "pous") zu 16 Fingerbreit auch eine sogenannte Pygme zu 18 Fingerbreit. Diese Pygme (Unterarm bis zum Handgelenk) wurde oft in Übersetzungen in Ermangelung eines geeigneten Wortes auch als „Fuß“ bezeichnet. Dennoch kann festgehalten werden, dass über die gesamte Zivilisationsgeschichte hinweg der Fuß stets 16 Fingerbreit beträgt, wobei der „Finger“ als die eigentliche Grundeinheit angesehen werden kann.

Unter den mannigfaltigen, stets voneinander abgeleiteten griechischen Systemen sind vor allem der gemeingriechische Fuß zu nennen (wissenschaftlich seit Heron auch als Pous metrios bezeichnet), welcher später österreichischer Fuß wurde, sowie der besonders für die Erdvermessung des Eratosthenes relevante griechisch-kyrenaische Fuß der Antike.

Griechische Fußmaße

Erst im Mittelalter mit seiner Vorliebe für das Duodezimalsystem wurde der Fuß statt in sechzehn in zwölf Untereinheiten geteilt. Dadurch ergab sich die Daumenbreite, das sogenannte Zoll (lat. "uncia," engl. "inch," frz. "pouce"). Auch in anderen Kulturkreisen, z. B. in Japan oder China, sind Längenmaße in Größe des menschlichen Fußes bekannt.

Ein karolingischer Fuß maß 32,24 cm, der „Pariser Königsfuß“ 32,48 cm (vermutlich vom "pes drusianus" abgeleitet) und der weit verbreitete Rheinfuß knapp 31,4 Zentimeter.

Bei nahezu allen Bauwerken des gesamten Mittelalters wurde der Fuß als Grundbaumaß verwendet. Er lässt sich aus den Breitenmaßen der jeweiligen Bauwerke durch Teilung ermitteln. Die verschiedenen Dombauhütten verwendeten jeweils ihren eigenen Fuß, die entweder antike Fußmaße oder deren Ableitungen waren.

Eine Anweisung zur Grenzvermessung zwischen der Grafschaft Nassau und der Landgrafschaft Hessen-Darmstadt aus dem Jahre 1719 legt unter Punkt 5 fest, dass zur Vermessung „eine Rute zu 18 Schuh, der Schuh zu 12 Zoll“ verwendet werden solle.

Wie man in der frühen Neuzeit versuchte, „ein gerechtes Meßrut“ als Mittelung zu schaffen, zeigt folgender Text des Rechenmeisters, Feldmessers und Stadtschreibers Jakob Köbel aus Oppenheim von 1535:

Mit der Einführung des dezimalen Meters in Frankreich im Jahre 1793 brach man erstmals in der Menschheitsgeschichte mit der Verwendung aller konkret auf den Menschen bezogenen Grundmaße sowie mit der traditionellen Bezugnahme auf andere, schon bestehende Maße. Die "neue Referenz" sollte nun der Erdumfang sein. In bestimmten Bereichen, etwa der Landvermessung und Schifffahrt, wurden allerdings schon zuvor verschiedene „geografische Meilen“ (z. B. in Deutschland  Äquatorgrad lang) und davon abgeleitete Größen verwendet. Das Meter wurde rein abstrakt als zehnmillionster Teil der Entfernung vom Pol zum Äquator definiert. In der Folge verschwand das klassische menschliche Fußmaß im Geltungsbereich des Meters.

Zur Vereinfachung und besseren Akzeptanz der Umstellung auf das Meter wurde im 19. Jahrhundert da und dort das alte Fußmaß auf runde Werte des neuen Systems gebracht. Dieser erneuerte Fuß entsprach im Großherzogtum Hessen genau 25 cm, im Großherzogtum Baden sowie in der Schweiz (siehe auch: Schweizer Fuss) genau 30 cm und im Herzogtum Nassau genau 50 cm. Diese Einheiten wurden dann meistens in zehn statt wie zuvor in zwölf Zoll geteilt. Andere Staaten beschränkten sich darauf, ihren Fuß und andere Maßeinheiten in Abhängigkeit zum metrischen Systems zu definieren.

Die verschiedenen alten deutschen Fußmaße sind durch den Norddeutschen Bund und die Übernahme seiner Gesetze bei der Gründung des Deutschen Reiches (1871) sowie den darauf folgenden deutschen Beitritt zur internationalen Meterkonvention (1875) ganz aufgegeben worden.
In Österreich galt überwiegend der pous metrios zu 31,61 cm.

Terminologisch entsprach dem Fuß regional der "Schuh."

Beispiele der Fußmaße in einigen deutschen Städten und Ländern (gerundet):

Reformierte Fußmaße im Rheinbund (ab 1806) bzw. nach dem Wiener Kongress (1814/15) sowie in der Schweiz (gemäß Konkordat von 1835):

Spezielle Fußmaße:

Mit "limprandischer Fuß" wurde neben dem normalen oder ordinären Fuß das Längenmaß Fuß in Alessandria bezeichnet. Der Unterschied in der Länge war
(Quelle unter)

Das internationale Einheitenzeichen ist heute ft für engl. "foot" bzw. "feet," oft auch abgekürzt mit dem Minutenzeichen ′ (in Unicode das „PRIME“-Zeichen U+2032), ersatzweise das halbe typografische Anführungszeichen '. Gemeint ist damit immer der "internationale Fuß" („angelsächsischer Kompromissfuß“, 1959), der einem Drittel Yard oder zwölf internationalen Zoll je 2,54 cm entspricht, also exakt 30,48 cm misst:

In der US-Landesvermessung hält sich daneben die frühere Definition, bei der exakt 39,37 Zoll in ein Meter passen (statt 39  / = etwa 39,370 078 740). Damit ist der "international foot" exakt 0,999 998 mal so groß wie der "US survey foot," d. h. 0,0002 % oder ein Fünfhunderttausendstel kleiner.


In der Wissenschaft gilt international das dezimale metrische Maßsystem, und selbst in den USA wird in diesem Bereich der "foot" nicht mehr verwendet.

Die alten Einheiten führen jedoch bis heute zu Umrechnungsfehlern. So wurde beim Bau des Hubble-Weltraumteleskops die Luftdichte im Labor falsch auf das Vakuum reduziert und erforderte eine spätere Reparatur durch Astronauten.

In vielen technischen Bereichen gibt es weiterhin Fuß und insbesondere Zoll, um zum wichtigen nordamerikanischen Markt kompatibel zu sein. Da in den meisten Ländern die Verwendung des metrischen Systems gesetzlich vorgeschrieben ist, tauchen diese Einheiten nur noch in Gattungsbezeichnungen (z. B. 17″-Monitor) auf, oder die eigentlichen Fuß- bzw. Zollmaße werden metrisch umgerechnet und dann auch oft gerundet.

ANSI und der normgebende Berufsverband der Maschinenbauingenieure in den USA, ASME, geben die Nennweite bei Rohrmaßen nach "Reihe C, ASME-BPE 1997" aktuell in Zoll an (¼", ⅜", ½", ¾", 1", 1½", …). In Deutschland sollen hingegen nach DIN EN ISO 228-1 metrische Angaben gemacht werden. Dennoch werden im Rahmen der Heizungs- und Sanitärtechnik bei Handel und Handwerk ausschließlich Zoll-Maße verwendet. Bei Gewinden oder Schrauben führen insbesondere unbemerkte kleine Differenzen zu großen Problemen.

Viele Angaben sind in Fuß und Zoll angegeben, so z. B. die Abmessungen der weltweit verbreiteten ISO-Container. Insbesondere die Längenmaße sind hier von Bedeutung, da sie die Grundlage der Klassifikationen darstellen aus denen sich alle weiteren Maße im Wesentlichen ableiten. Der Standard sind 20′-, 40′- und 45′-Container.

Direkte Werte in Fuß sind am häufigsten in der Luftfahrt anzutreffen, wo sie als "feet" die gebräuchlichste Maßeinheit der Flughöhe darstellen. Bei geografischen Höhenangaben in Luftfahrtkarten (speziell für Flugplätze und Berge) wird im Zusammenhang mit der Angabe in Fuß der Begriff „Elevation“ (ELEV) verwendet. Im kontrollierten Luftraum werden die Flugflächen (engl.: "flight level", FL) nach ihrer Höhe in Vielfachen von 100 Fuß benannt. Beispiel: Die Höhenangabe „FL120“ bedeutet: „12000 ft über der Bezugsfläche“.

Viele Maße basieren auf Fuß und Zoll oder werden immer noch direkt darin angegeben. Maße bei Bauvorschriften für z. B. Durchstiege, Raumhöhen oder die Seereling wurden in Fuß oder Zoll definiert und werden heute als Millimeterwerte angegeben (Höhe der Reling 2 Fuß entspricht 610 mm, Abstand der Stützen höchstens 7 Fuß entspricht 2134 mm). Grenzwerte für Schiffsklassen sind oft ganze Fußmaße. Eine Registertonne sind 100 Kubikfuß. Tiefgangs-Marken, die sogenannten Ahmings, die am Bug und Heck eines Seeschiffes und bisweilen auch mittschiffs angebracht sind, sind oft in Fuß skaliert. Bei der Fertigung von Kettengliedern wird zumeist der Zoll verwendet, während die Produkte in Millimeter ausgezeichnet werden (die Glieder einer ¼-Zoll-Kette sind 6,35 mm dick, werden aber als 6-Millimeter-Kette bezeichnet; ⅜ Zoll sind 9,53 mm, werden aber als 10 mm verkauft; und ½ Zoll sind 12,7 mm, heißen aber 13 mm). Auf allen amtlichen amerikanischen Seekarten und nautischen Veröffentlichungen werden Wassertiefen in Fuß angegeben. Hingegen sind die Seekarten der britischen Admiralität inzwischen fast durchgehend metrisch.

Auch wenn es nicht exakt stimmt, nutzen auch Nichtengländer Fußwerte im Namen eines Bootstyps oder einer Marke, um das Boot genauer zu spezifizieren (Boote der Klasse Melges 24 sind 750 cm lang, die Swan 48 hat 1483 cm und nicht korrekte 1463,04 cm = 48 ft).

In manchen Sportarten sind Maße ursprünglich runde Fußwerte, werden inzwischen aber häufig in Metern spezifiziert und dabei nur manchmal auf glatte Werte gerundet. Der Basketballkorb beispielsweise hängt 10 ft hoch (umgerechnet 3,048 m) Die Abmessungen eines Fußballtors stammen aus der Zeit, als in England erste Regeln wie die Acht-Acht-Regel aufgestellt wurden. 8 ft (= 2,44 m) hoch und 8 yd (24 ft = 7,32 m) breit. Die britischen Maße werden auch heute noch im FIFA-Regelwerk parallel zu den metrischen Maßen angegeben. Alle vorgegebenen Abmessungen eines Baseball-Spielfeldes sind grundsätzlich Fuß-Maße.

Im Orgelbau wird der Fuß heute für die Angabe der Tonhöhe von Orgelpfeifen verwendet. Die sogenannte "Fußtonzahl" gibt die klingende Tonhöhe eines Orgelregisters an. Bei einem 8′-Register lässt die Taste C auch den Ton C erklingen, bei einem 4′-Register den Ton c usw. Dabei wird von einer theoretischen Standardpfeife für die Taste C ausgegangen. Für die heutige Stimmung hat im Orgelbau ein Fuß etwa 32 cm. Abhängig von der Bauart weicht die tatsächliche Länge einer Pfeife bei gleicher Tonhöhe jedoch von der in Fuß angegebenen Länge ab.

Auch die Unterschiede der regional üblichen Fußmaße im historischen Orgelbau spielten bei der Nennung der Fußtonzahl keine Rolle, da sie nur grob angegeben wurde. So wurde beispielsweise das Maß 2⅔′ regelmäßig als 3′ geschrieben.





</doc>
<doc id="1580" url="https://de.wikipedia.org/wiki?curid=1580" title="Furlong">
Furlong

Der Furlong (dt. „“) ist ein altes angloamerikanisches Längenmaß mit Herkunft aus der Landwirtschaft. Es ist nicht metrisch und daher nicht SI-konform.


Die alte Flächeneinheit acre (4046,86 m²) entspricht einem streifenförmigen Feld von 1 Furlong × 0,1 Furlong, das ein Ochsengespann in etwa einem Tag pflügen konnte. Ähnliche Dimensionen haben die Einheiten Morgen und Joch, die in Europa bis heute in der bäuerlichen Bevölkerung lebendig sind.

Der Furlong wird insbesondere in Großbritannien für die Strecken von Pferderennen genutzt. So geht das Pferderennen von Ascot über 20 Furlongs und damit zweieinhalb englische Meilen.

Der alte schottische Furlong wich vom angloamerikanischen Maß ab und war etwa 25 Meter größer. Die Länge dieses Furlongs war 226,7671 Meter. Der Umrechnungsfaktor kann mit 1,127 angesetzt werden.



</doc>
<doc id="1581" url="https://de.wikipedia.org/wiki?curid=1581" title="Nautischer Faden">
Nautischer Faden

Der Nautische Faden (vom englischen: „fathom“, zu Deutsch: „Faden“, auch „Klafter“ genannt) ist eine nicht SI-konforme Maßeinheit der Länge, welche insbesondere noch in der englischsprachigen Schifffahrt – in der Nautik – für Tiefenangaben in Gebrauch ist. Ursprünglich handelt es sich bei dem Maß um die Spannweite der Arme eines ausgewachsenen Mannes, historisch sechs Fuß gleichgesetzt, dem Klafter.

Gelegentlich wird auch eine neuere, nicht genormte Definition benutzt:

In der EG-Richtlinie 80/181/EWG ist die erste Definition zugrundegelegt, jedoch der Zahlenwert zu 1,829 Meter gerundet.

Der preußische Faden im Seewesen war

Die Pariser Linie ist hier mit 2,2558 mm gerechnet.




</doc>
<doc id="1582" url="https://de.wikipedia.org/wiki?curid=1582" title="Farad">
Farad

Farad ist die SI-Einheit für die elektrische Kapazität. Sie wurde nach Michael Faraday benannt.

Ein Kondensator mit einer Kapazität von einem Farad (F) kann durch das Aufladen auf eine Spannung von einem Volt (V) eine Ladung von einem Coulomb (C) speichern:

Anschaulich beschreiben kann man diesen Zusammenhang auch durch:

Kondensatoren wie Folien- oder Elektrolytkondensatoren haben Kapazitäten zwischen einigen Pikofarad (pF) und einigen hundert Mikrofarad (μF). Doppelschichtkondensatoren weisen besonders hohe Kapazitätswerte von bis zu einigen tausend Farad auf. 


</doc>
<doc id="1584" url="https://de.wikipedia.org/wiki?curid=1584" title="Fluid dram">
Fluid dram

Fluid dram und Fluid drachm (»Flüssigdrachme«) sind Maßeinheiten des Raums (Flüssigkeit, Apothekermaß), die vor allem in Nordamerika gebräuchlich sind. 

Das Einheitenzeichen für "Fluid dram" ist "US.fl.dr." <br>
Das Einheitenzeichen für "Fluid drachm" ist "Imp.fl.dr."

1 US.fl.dr. = 60 US.min. = 0,2255859375 cubic inch = 3,6966911953125 cm³ <br>
1 US. gallon = 231 cubic inch = 1024 US.fl.dr.

1 Imp.fl.dr. = 0,96076 US.fl.dr.

1 Imp.fl.dr. = 60 Imp.min. = 0,2167339453125 cubic inch = 3,55163303280844 cm³ <br>
1 Imp.gallon = 277,41945 cubic inch = 1280 Imp.fl.dr. 

"Siehe auch:" 



</doc>
<doc id="1585" url="https://de.wikipedia.org/wiki?curid=1585" title="Fluid ounce">
Fluid ounce

Fluid ounce („Flüssigunze“) ist eine Maßeinheit des Raums (Flüssigkeit, Apothekermaß) des angloamerikanischen Maßsystems. Es wird unterschieden zwischen "Imperial fluid ounce," einer imperialen Maßeinheit, welche ihren Ursprung in Großbritannien hat, und "United States fluid ounce", einer US-amerikanischen Maßeinheit, welche Teil der United States customary units ist. Das Einheitenzeichen ist entweder Imp.fl.oz. oder US fl.oz. 

Soweit der Zusammenhang klar ist, wird manchmal auch nur kurz "oz" geschrieben, womit generell aber eine Verwechslungsgefahr mit der so abgekürzten Gewichtseinheit Unze (englisch "ounce") entsteht.

In Amerika ist z. B. eine typische Verkaufsgröße für Bierdosen 12 fl.oz., was 0,3549 Litern entspricht.


<br>

1 Imp.fl.oz. = 0,96076 US.fl.oz. <br>
1 Imp.min. = 0,96076 US.min.


</doc>
<doc id="1586" url="https://de.wikipedia.org/wiki?curid=1586" title="Foot-pound">
Foot-pound

Das foot-pound oder foot-pound force ist eine britische und US-amerikanische Einheit sowohl der Energie als auch des Drehmoments (dort auch Pound-foot). Das Einheitenzeichen ist ft·lb oder ft·lbf, auch lbf·ft.

Ein foot-pound entspricht genau 1,355 817 948 331 400 4 J (Joule) und entspricht der Energie, die aufgebracht werden muss, um einen Körper mit einer Masse von einem Pfund um einen Foot gegen seine Gewichtskraft anzuheben:
550 Foot-pound in der Sekunde entsprechen einem hp (etwa 745,7 Watt). 


</doc>
<doc id="1587" url="https://de.wikipedia.org/wiki?curid=1587" title="Friedrich Nietzsche">
Friedrich Nietzsche

Friedrich Wilhelm Nietzsche (Aussprache: oder ; * 15. Oktober 1844 in Röcken; † 25. August 1900 in Weimar) war ein deutscher klassischer Philologe. Erst postum machten ihn seine Schriften als Philosophen weltberühmt. Im Nebenwerk schuf er Dichtungen und musikalische Kompositionen. Ursprünglich preußischer Staatsbürger, war er seit seiner Übersiedlung nach Basel 1869 staatenlos.

Im Alter von 24 Jahren wurde Nietzsche unmittelbar im Anschluss an sein Studium an der Universität Basel Professor für klassische Philologie. Bereits zehn Jahre später legte er 1879 aus gesundheitlichen Gründen die Professur nieder. Von nun an bereiste er – auf der Suche nach Orten, deren Klima sich günstig auf seine Migräne und Magenleiden auswirken sollte – Frankreich, Italien, Deutschland und die Schweiz. Ab seinem 45. Lebensjahr (1889) litt er unter einer schweren psychischen Krankheit, die ihn arbeits- und geschäftsunfähig machte. Seinen Anfang der 1890er Jahre rasch einsetzenden Ruhm hat er deshalb nicht mehr bewusst erlebt. Er verbrachte den Rest seines Lebens als Pflegefall in der Obhut zunächst seiner Mutter, dann seiner Schwester, und starb 1900 im Alter von 55 Jahren.

Den jungen Nietzsche beeindruckte besonders die Philosophie Schopenhauers. Später wandte er sich von dessen Pessimismus ab und stellte eine radikale Lebensbejahung in den Mittelpunkt seiner Philosophie. Sein Werk enthält scharfe Kritiken an Moral, Religion, Philosophie, Wissenschaft und Formen der Kunst. Die zeitgenössische Kultur war in seinen Augen lebensschwächer als die des antiken Griechenlands. Wiederkehrendes Ziel von Nietzsches Angriffen ist vor allem die christliche Moral sowie die christliche und platonistische Metaphysik. Er stellte den Wert der Wahrheit überhaupt in Frage und wurde damit Wegbereiter postmoderner philosophischer Ansätze. Auch Nietzsches Konzepte des „Übermenschen“, des „Willens zur Macht“ oder der „ewigen Wiederkunft“ geben bis heute Anlass zu Deutungen und Diskussionen.

Nietzsches Denken hat weit über die Philosophie hinaus gewirkt und ist bis heute unterschiedlichsten Deutungen und Bewertungen unterworfen. Nietzsche schuf keine systematische Philosophie. Oft wählte er den Aphorismus als Ausdrucksform seiner Gedanken. Seine Prosa, seine Gedichte und der pathetisch-lyrische Stil von "Also sprach Zarathustra" verschafften ihm Anerkennung als Schriftsteller.

Friedrich Nietzsche wurde am 15. Oktober 1844 in Röcken geboren, einem Dorf nahe Lützen im Kreis Merseburg in der preußischen Provinz Sachsen, heute Sachsen-Anhalt. Seine Eltern waren der lutherische Pfarrer Carl Ludwig Nietzsche und dessen Frau Franziska. Seit der Reformation im 16. Jahrhundert ist die Familie Nietzsche in Sachsen als evangelisch dokumentiert. In den Familien beider Elternteile gab es einen hohen Anteil protestantischer Pfarrer. Seinen Vornamen gab ihm sein Vater zu Ehren des preußischen Königs Friedrich Wilhelm IV., an dessen 49. Geburtstag er geboren wurde. Nietzsche selbst behauptete in seinen späten Jahren, in väterlicher Linie von polnischen Edelleuten abzustammen, was jedoch nicht bestätigt werden konnte.

Die Schwester Elisabeth kam 1846 zur Welt. Nach dem Tod des Vaters 1849 und des jüngeren Bruders Ludwig Joseph (1848–1850) zog die Familie nach Naumburg. Der spätere Justizrat Bernhard Dächsel wurde formal zum Vormund der Geschwister Friedrich und Elisabeth bestellt.

Von 1850 bis 1856 lebte Nietzsche im „Naumburger Frauenhaushalt“, das heißt zusammen mit Mutter, Schwester, Großmutter, zwei unverheirateten Tanten väterlicherseits und dem Dienstmädchen. Erst die Hinterlassenschaft der 1856 verstorbenen Großmutter erlaubte der Mutter, für sich und ihre Kinder eine eigene Wohnung zu mieten. Der junge Nietzsche besuchte zunächst die allgemeine Knabenschule, fühlte sich dort allerdings so isoliert, dass man ihn auf eine Privatschule schickte, wo er erste Jugendfreundschaften mit Gustav Krug und Wilhelm Pinder, beide aus angesehenen Häusern, knüpfte. Ab 1854 besuchte er das Domgymnasium Naumburg und fiel bereits dort durch seine besondere musische und sprachliche Begabung auf. 1857 bereitete Pastor Gustav Adolf Oßwald, ein enger Freund seines Vaters, ihn in Kirchscheidungen für die Aufnahmeprüfung in Schulpforta vor. Am 5. Oktober 1858 wurde Nietzsche als Stipendiat in die Landesschule Pforta aufgenommen, wo er als bleibende Freunde Paul Deussen und Carl Freiherrn von Gersdorff kennenlernte. Seine schulischen Leistungen waren sehr gut, in seiner Freizeit dichtete und komponierte er. In Schulpforta entwickelte sich zum ersten Mal seine eigene Vorstellung von der Antike und, damit einhergehend, eine Distanz zur kleinbürgerlich-christlichen Welt seiner Familie. In dieser Zeit lernte Nietzsche den älteren, einstmals politisch engagierten Dichter Ernst Ortlepp kennen, dessen Persönlichkeit den vaterlosen Knaben beeindruckte. Von Nietzsche besonders geschätzte Lehrer, mit denen er nach seiner Schulzeit noch in Verbindung blieb, waren Wilhelm Corssen, der spätere Rektor Diederich Volkmann und Max Heinze, der 1897, als Nietzsche entmündigt war, zu dessen Vormund bestellt wurde.

Gemeinsam mit seinen Freunden Pinder und Krug traf sich Nietzsche ab 1860 auf der Burgruine Schönburg, wo er mit ihnen über Literatur, Philosophie, Musik und Sprache diskutierte. Mit Ihnen gründete er dort die künstlerisch-literarische Vereinigung „Germania“. Die Gründungsfeier fand am 25. Juli 1860 statt: „… bei Naumburger Rotwein (die Flasche zu 75 Pfennige) leisteten die drei sechzehnjährigen Vereinsmitglieder ihren Bundesschwur. Gedichte, Kompositionen, Abhandlungen mußten regelmäßig geliefert werden. Man wollte dann gemeinsam darüber diskutieren.“ Die Versammlungen fanden vierteljährlich statt. Auf ihnen wurden Vorträge gehalten. Es gab eine Gemeinschaftskasse, aus der Bücher beschafft wurden. Bereits in dieser Zeit entwickelte Nietzsche seine Leidenschaft für die Musik Richard Wagners. Zu Nietzsches frühen Werken, die vor dem Hintergrund der Schönburger Germania entstanden sind, zählen die "Synodenvorträge", "Kindheit der Völker", "Fatum und Geschichte", sowie "Über das Dämonische in der Musik". 1863 wurde die "Germania" aufgelöst, nachdem Pinder und Krug ihr Interesse daran verloren hatten.

Im Wintersemester 1864/65 begann Nietzsche an der Universität Bonn das Studium der klassischen Philologie und der evangelischen Theologie unter anderem bei Wilhelm Ludwig Krafft. Zusammen mit Deussen wurde er Mitglied der Bonner Burschenschaft Frankonia. Er bestritt freiwillig eine Mensur, von welcher er einen Schmiss auf dem Nasenrücken zurückbehielt. Nach einem Jahr verließ er die Burschenschaft, weil ihm das Verbindungsleben missfiel. Neben seinem Studium vertiefte er sich in die Werke der Junghegelianer, darunter "Das Leben Jesu" von David Friedrich Strauß, "Das Wesen des Christentums" von Ludwig Feuerbach und Bruno Bauers Evangelienkritiken. Diese bestärkten ihn (zur großen Enttäuschung seiner Mutter) in dem Entschluss, das Theologiestudium nach einem Semester abzubrechen.

Nietzsche wollte sich nun ganz auf die klassische Philologie konzentrieren, war jedoch mit seiner Lage in Bonn unzufrieden. Daher nahm er den Wechsel des Philologieprofessors Friedrich Ritschl nach Leipzig (in Folge des Bonner Philologenstreits) zum Anlass, zusammen mit seinem Freund Gersdorff ebenfalls nach Leipzig zu ziehen. In den folgenden Jahren sollte Nietzsche zu Ritschls philologischem Musterschüler werden, obwohl er in Bonn noch dessen Konkurrenten Otto Jahn zugeneigt war. Ritschl war für Nietzsche zeitweise eine Vaterfigur, ehe später Richard Wagner (s. u.) diese Stelle einnahm.

Im Oktober 1865, kurz bevor Nietzsche das Studium in Leipzig aufnahm, verbrachte er zwei Wochen in Berlin bei der Familie seines Studienfreundes Hermann Mushacke. Dessen Vater hatte in den 1840er Jahren zu einem Debattierzirkel um Bruno Bauer und Max Stirner gehört. Dass Nietzsche bei diesem Besuch mit Stirners 1845 erschienenen Buch "Der Einzige und sein Eigentum" konfrontiert wurde, liegt nahe, lässt sich aber nicht belegen. Jedenfalls wandte Nietzsche sich unmittelbar danach einem Philosophen zu, der Stirner und dem Junghegelianismus denkbar fernstand: Arthur Schopenhauer. Ein weiterer Philosoph, den er in seiner Leipziger Zeit für sich entdeckte, war Friedrich Albert Lange, dessen "Geschichte des Materialismus" 1866 erschien. In erster Linie setzte Nietzsche jedoch zunächst sein philologisches Studium fort. In dieser Zeit knüpfte er eine enge Freundschaft mit seinem Kommilitonen Erwin Rohde. Mit diesem zusammen beteiligte er sich 1866 an der Gründung des Klassisch-philologischen Vereins an der Universität Leipzig.

Hatte er im sogenannten Deutschen Krieg zwischen Preußen und Österreich, in dessen Verlauf auch Leipzig preußisch besetzt wurde, noch seine militärische Einberufung vermeiden können, so wurde Nietzsche nun (1867) als Einjährig-Freiwilliger bei der preußischen Artillerie in Naumburg verpflichtet. Nach einem schweren Reitunfall im März 1868 dienstunfähig geworden, nutzte er die Zeit seiner Kur zu weiteren philologischen Arbeiten, die er in seinem letzten Studienjahr fortsetzte. Von großer Bedeutung sollte sein erstes Zusammentreffen mit Richard Wagner im Jahre 1868 werden.

Auf Empfehlung Friedrich Ritschls und Betreiben Wilhelm Vischer-Bilfingers wurde Nietzsche 1869, noch bevor er seine Promotion ("honoris causa") erhalten und seine Habilitation absolviert hatte, zum außerordentlichen Professor für klassische Philologie an die Universität Basel berufen. Zu seiner Tätigkeit gehörte auch das Lehren am Basler Gymnasium am Münsterplatz ("Pädagogium"). Als seine wichtigste Erkenntnis auf dem Gebiet der Philologie sah er die Entdeckung an, dass die antike Metrik, im Gegensatz zur modernen, akzentuierenden Metrik, ausschließlich auf der Länge von Silben basierte (quantitierendes Prinzip).

Auf eigenen Wunsch wurde Nietzsche nach seiner Übersiedlung nach Basel aus der preußischen Staatsbürgerschaft entlassen und blieb für den Rest seines Lebens staatenlos. Allerdings diente er im Deutsch-Französischen Krieg für kurze Zeit als Sanitäter auf deutscher Seite. In dieser Zeit zog er sich eine schwere Dysenterie- und Diphtherieerkrankung zu, deren Rekonvaleszenz von längerer Dauer war. Die Gründung des Deutschen Reichs und die anschließende Ära Otto von Bismarcks nahm er von außen und mit einer grundsätzlichen Skepsis wahr.

In Basel begann 1870 die bis in die Zeit von Nietzsches geistiger Umnachtung andauernde Freundschaft zu seinem Kollegen Franz Overbeck, einem atheistischen Theologieprofessor. Nietzsche schätzte auch den älteren Kollegen Jacob Burckhardt, der ihm gegenüber jedoch höflich, aber bestimmt Distanz wahrte.

Bereits im Jahre 1868 hatte Nietzsche in Leipzig Richard Wagner und dessen spätere Frau Cosima kennengelernt. Er verehrte beide zutiefst und war seit Beginn seiner Zeit in Basel häufig Gast im Haus des „Meisters“ in Tribschen bei Luzern. Dieser nahm ihn zwar zeitweise mit in seinen engsten Kreis auf, schätzte ihn aber vor allem als Propagandisten für die Gründung des Bayreuther Festspielhauses.
1872 veröffentlichte Nietzsche sein erstes größeres Werk, "Die Geburt der Tragödie aus dem Geiste der Musik", eine Untersuchung über den Ursprung der Tragödie, in der er die exakte philologische Methode durch philosophische Spekulation ersetzte. Er entwickelte darin seine Kunstpsychologie, indem er die griechische Tragödie aus dem Begriffspaar apollinisch-dionysisch zu erklären versuchte. Die Schrift wurde von den meisten seiner altphilologischen Kollegen – auch von Ritschl – abgelehnt und mit Schweigen übergangen. Durch Ulrich von Wilamowitz-Moellendorffs Polemik "Zukunftsphilologie!" kam es zwar doch noch zu einer kurzen öffentlichen Kontroverse, in die Rohde, inzwischen Professor in Kiel, und sogar Wagner auf Nietzsches Seite eingriffen; aber Nietzsche wurde sich seiner Isolation in der Philologie noch mehr bewusst, derentwegen er sich schon Anfang 1871 um den freiwerdenden Basler philosophischen Lehrstuhl Gustav Teichmüllers beworben hatte. Dieser wurde aber mit Rudolf Eucken besetzt.

Auch die vier "Unzeitgemäßen Betrachtungen" (1873–1876), in denen er eine von Schopenhauer und Wagner beeinflusste Kulturkritik übte, fanden nicht die erhoffte Resonanz. Im Umkreis Wagners hatte Nietzsche inzwischen Malwida von Meysenbug und Hans von Bülow kennengelernt, und auch die Freundschaft mit Paul Rée, dessen Einfluss ihn vom Kulturpessimismus seiner ersten Schriften abbrachte, begann. Seine Enttäuschung über die ersten Bayreuther Festspiele von 1876, wo er sich von der Banalität des Schauspiels und der Niveaulosigkeit des Publikums abgestoßen fühlte, nahm Nietzsche zum Anlass, sich von Wagner zu entfernen. Seine frühere Leidenschaft schlug in Ablehnung und schließlich radikale Gegnerschaft um.

Derselbe Prozess fand statt mit Schopenhauer. Nietzsche begann am 6. Dezember Philipp Mainländers 200 Seiten lange Kritik der Philosophie Schopenhauers zu lesen – wenige Tage später schrieb er, mit Schopenhauer gebrochen zu haben. Mit der Publikation von "Menschliches, Allzumenschliches" (1878) wurde die Entfremdung von Wagner und von der Schopenhauerschen Philosophie offenbar. Auch die Freundschaften zu Deussen und Rohde hatten sich inzwischen merklich abgekühlt. In dieser Zeit unternahm Nietzsche mehrere vergebliche Versuche, eine junge und vermögende Ehefrau für sich zu finden, worin er vor allem von der mütterlichen Gönnerin Malwida von Meysenbug unterstützt wurde. Außerdem nahmen die seit seiner Kindheit auftretenden Krankheiten (Migräneanfälle und Magenstörungen sowie eine starke Kurzsichtigkeit, die letztlich praktisch bis zur Blindheit führte) zu und zwangen ihn zu immer längeren Urlauben von seiner Lehrtätigkeit. 1879 musste er sich deswegen schließlich vorzeitig pensionieren lassen.

Getrieben von seinen Krankheiten auf der ständigen Suche nach für ihn optimalen Klimabedingungen, reiste er nun viel und lebte bis 1889 als freier Autor an verschiedenen Orten. Dabei lebte er vor allem von der ihm gewährten Pension; zudem erhielt er mitunter Zuwendungen von Freunden. Im Sommer hielt er sich meist in Sils-Maria, im Winter vorwiegend in Italien (Genua, Rapallo, Turin) und in Nizza auf. Hin und wieder besuchte er die Familie in Naumburg, wobei es mehrfach zu Zerwürfnissen und Versöhnungen mit seiner Schwester kam. Sein früherer Schüler Peter Gast (eigtl. Heinrich Köselitz) wurde zeitweilig zu einer Art Privatsekretär. Köselitz und Overbeck waren Nietzsches beständigste Vertraute.

Aus dem Wagnerkreis war ihm vor allem Meysenbug als mütterliche Gönnerin erhalten geblieben. Kontakt hielt er außerdem mit dem Musikkritiker Carl Fuchs und zunächst auch mit Paul Rée. Anfang der 1880er erschienen mit "Morgenröte" und "Die fröhliche Wissenschaft" weitere Werke im aphoristischen Stil von "Menschliches, Allzumenschliches".
1882 lernte er durch Vermittlung von Meysenbug und Rée in Rom Lou von Salomé kennen. Nietzsche fasste schnell weitreichende Pläne für die „Dreieinigkeit“ mit Rée und Salomé. Die Annäherung an die junge Frau gipfelte in einem mehrwöchigen gemeinsamen Aufenthalt in Tautenburg, mit Nietzsches Schwester Elisabeth als Anstandsdame. Nietzsche sah in Salomé bei aller Wertschätzung weniger eine gleichwertige Partnerin als eine begabte Schülerin. Er verliebte sich in sie, hielt über den gemeinsamen Freund Rée um ihre Hand an, doch Salomé lehnte ab. Unter anderem aufgrund von Intrigen Elisabeths zerbrach die Beziehung zu Rée und Salomé im Winter 1882/1883. Nietzsche, der angesichts neuer Krankheitsschübe und seiner nunmehr beinahe vollständigen Isolation – mit Mutter und Schwester hatte er sich wegen Salomé überworfen – von Suizidgedanken geplagt wurde, flüchtete nach Rapallo, wo er in nur zehn Tagen den ersten Teil von "Also sprach Zarathustra" zu Papier brachte.

Die Gedanken zum dritten Teil entwickelte er bei seinem Aufenthalt im Bergdorf Èze in der Nähe von Nizza. Eine Straße und eine Gedenktafel erinnern an Nietzsches Tage in Èze.

Waren ihm schon nach dem Bruch mit Wagner und der Philosophie Schopenhauers nur wenige Freunde erhalten geblieben, so stieß der völlig neue Stil im "Zarathustra" selbst im engsten Freundeskreis auf Unverständnis, das allenfalls durch Höflichkeit überdeckt wurde. Nietzsche war sich dessen durchaus bewusst und pflegte seine Einsamkeit geradezu, wenn er auch oft darüber klagte. Den kurzzeitig gehegten Plan, als Dichter an die Öffentlichkeit zu treten, gab er auf. Daneben plagten ihn Geldsorgen, denn seine Bücher wurden so gut wie nicht gekauft. Den vierten Teil des "Zarathustra" gab er 1885 nur noch als Privatdruck mit einer Auflage von 40 Exemplaren heraus, die als Geschenk für „solche, die sich um ihn verdient machten“, gedacht waren und von denen Nietzsche letztlich lediglich sieben verschenkte.
1886 ließ er "Jenseits von Gut und Böse" auf eigene Kosten drucken. Mit diesem Buch und den 1886/87 erscheinenden Zweitauflagen von "Geburt", "Menschliches", "Morgenröte" und "Fröhlicher Wissenschaft" sah er sein Werk als vorerst abgeschlossen an und hoffte, dass sich bald eine Leserschaft entwickeln würde. Tatsächlich stieg das Interesse an Nietzsche, wenn auch sehr langsam und von ihm selbst kaum bemerkt.

Neue Bekanntschaften Nietzsches in diesen Jahren waren Meta von Salis und Carl Spitteler, auch ein Treffen mit Gottfried Keller war zustande gekommen. 1886 war seine Schwester, inzwischen verheiratet mit dem Antisemiten Bernhard Förster, nach Paraguay abgereist, um die „germanische“ Kolonie Nueva Germania zu gründen – ein Vorhaben, das Nietzsche lächerlich fand. Im brieflichen Kontakt setzte sich die Abfolge von Streit und Versöhnung fort, persönlich sollten sich die Geschwister aber erst nach Friedrichs Zusammenbruch wiedersehen.

Nietzsche hatte weiterhin mit wiederkehrenden schmerzhaften Anfällen zu kämpfen, die ein konstantes Arbeiten unmöglich machten. 1887 schrieb er in kurzer Zeit die Streitschrift "Zur Genealogie der Moral". Er wechselte Briefe mit Hippolyte Taine, dann auch mit Georg Brandes, der Anfang 1888 in Kopenhagen die ersten Vorträge über Nietzsches Philosophie hielt.

Im selben Jahr schrieb Nietzsche fünf Bücher, teilweise aus umfangreichen Aufzeichnungen für das zeitweise geplante Werk "Der Wille zur Macht". Sein Gesundheitszustand hatte sich vorübergehend gebessert, im Sommer war er in regelrechter Hochstimmung. Seine Schriften und Briefe ab Herbst 1888 jedoch lassen bereits auf seinen beginnenden Größenwahn schließen. Die Reaktionen auf seine Schriften, vor allem auf die Polemik "Der Fall Wagner" vom Frühjahr, wurden von ihm maßlos überbewertet. An seinem 44. Geburtstag entschloss er sich, nach der Vollendung der "Götzen-Dämmerung" und des zunächst zurückgehaltenen "Antichrist", die Autobiographie "Ecce homo" zu schreiben. Im Dezember begann ein Briefwechsel mit August Strindberg. Nietzsche glaubte, kurz vor dem internationalen Durchbruch zu stehen, und versuchte, seine alten Schriften vom ersten Verleger zurückzukaufen. Er plante Übersetzungen in die wichtigsten europäischen Sprachen. Überdies beabsichtigte er die Veröffentlichung der Kompilation "Nietzsche contra Wagner" und der Gedichte "Dionysos-Dithyramben".

Anfang Januar 1889 erlitt er in Turin einen geistigen Zusammenbruch. Kleine Schriftstücke, sogenannte „Wahnzettel“, die er an enge Freunde, aber auch zum Beispiel an Cosima Wagner und Jacob Burckhardt und sogar Umberto I. von Italien sandte, waren von einer psychischen Erkrankung gezeichnet. Als Ursache für den Zusammenbruch wurde damals eine progressive Paralyse als Folge von Syphilis diagnostiziert, was heute als umstritten gilt.
Der durch die Wahnzettel an Burckhardt und ihn selbst alarmierte Overbeck brachte Nietzsche zunächst in die von Ludwig Wille geleitete "Irrenanstalt Friedmatt" in Basel. Von dort wurde der inzwischen geistig vollständig Umnachtete von seiner Mutter in die Psychiatrische Universitätsklinik in Jena unter Leitung Otto Binswangers gebracht. Ein Heilungsversuch Julius Langbehns, der von sich aus Kontakt zur Mutter aufgenommen hatte, scheiterte. 1890 durfte die Mutter ihn schließlich bei sich in Naumburg aufnehmen. Zu dieser Zeit konnte er zwar gelegentlich kurze Gespräche führen, Erinnerungsfetzen hervorbringen und unter einige Briefe von der Mutter diktierte Grüße setzen, verfiel jedoch schnell und plötzlich in Wahnvorstellungen oder Apathie und erkannte auch alte Freunde nicht wieder.

Über das weitere Verfahren mit den teilweise noch ungedruckten Werken berieten zunächst Overbeck und Köselitz. Letzterer begann eine erste Gesamtausgabe. Gleichzeitig setzte eine erste Welle der Nietzsche-Rezeption ein.

Elisabeth Förster-Nietzsche kehrte nach dem Suizid ihres Mannes 1893 aus Paraguay zurück, ließ die bereits gedruckten Bände der Köselitzschen Ausgabe einstampfen, gründete das Nietzsche-Archiv und übernahm von der betagten Mutter Zug um Zug die Kontrolle sowohl über den pflegebedürftigen Bruder als auch über dessen Nachlass und die Herausgabe seiner Werke. Mit Overbeck zerstritt sie sich, während sie Köselitz für eine weitere Zusammenarbeit gewinnen konnte.

Nietzsche selbst, dessen Verfall sich fortsetzte, bekam von alldem nichts mehr mit. Nach dem Tod seiner Mutter 1897 lebte er in der "Villa Silberblick" in Weimar, wo seine Schwester ihn pflegte. Ausgewählten Besuchern – etwa Rudolf Steiner – gewährte sie das Privileg, zu dem dementen Philosophen vorgelassen zu werden. So berichtete das Jenaer Volksblatt unter Berufung auf eine Naumburger Zeitung:

Von Steiner stammt eine weitere, ausführliche Schilderung des umnachteten Nietzsche. Nach mehreren Schlaganfällen war Nietzsche allerdings teilweise gelähmt und konnte weder stehen noch sprechen. Am 25. August 1900, im Alter von 55 Jahren, starb er an den Folgen einer Frontotemporalen Demenz in Weimar. Er wurde an der Röckener Dorfkirche im Familiengrab beigesetzt.

Nietzsche begann sein Werk als Philologe, begriff sich selbst aber zunehmend als Philosoph oder als „freier Denker“. Er gilt als Meister der aphoristischen Kurzform und des mitreißenden Prosa-Stils. Die Werke sind zuweilen mit einer Rahmenhandlung, Vor- und Nachwort, Gedichten und einem „Vorspiel“ versehen. Einige Interpreten halten selbst die scheinbar wenig strukturierten Aphorismenbücher für geschickt „komponiert“.

Nietzsche hat wie kaum ein zweiter Denker die Freiheit der Methode und der Betrachtung gewählt. Eine definitive Einordnung seiner Philosophie in eine bestimmte Disziplin ist daher schwierig. Nietzsches Herangehensweise an die Probleme der Philosophie ist teils die des Künstlers, teils die des Wissenschaftlers und teils die des Philosophen. Viele Stellen seines Werks können auch als psychologisch bezeichnet werden, wobei dieser Begriff erst später seine heutige Bedeutung bekam. Zahlreiche Deuter sehen einen engen Zusammenhang zwischen seinem Leben und seinem denkerischen Werk, sodass über Nietzsches Leben und Persönlichkeit weit mehr geforscht und geschrieben wird, als dies bei anderen Philosophen der Fall ist.

Oft wird Nietzsches Denken und Werk in bestimmte Perioden eingeteilt. Die folgende Aufteilung geht in Grundzügen auf Nietzsche selbst zurück und ist seit dem Nietzschebuch Lou Andreas-Salomés (1894) in ähnlicher Form von fast allen Interpreten verwendet worden.

Es gibt allerdings einige Überschneidungen und Brüche in diesem Schema. So fügte Nietzsche den Zweitauflagen der "Geburt der Tragödie" und der "Fröhlichen Wissenschaft" von 1887 ein selbstkritisches Vorwort beziehungsweise ein fünftes Buch hinzu. Bedeutsam ist auch die erst 1896 erschienene Schrift "Über Wahrheit und Lüge im außermoralischen Sinne" aus dem Sommer 1873, in der Nietzsche viele seiner späteren Gedanken vorwegnimmt. Einige Themen – etwa das Verhältnis von Kunst und Wissenschaft – behandelt Nietzsche in allen Zeiträumen, wenn auch aus unterschiedlichen Perspektiven und mit entsprechend unterschiedlichen Antworten.

Neben seinen philosophischen Betrachtungen veröffentlichte Nietzsche Gedichte, in denen seine philosophischen Gedanken bald heiter, bald dunkel und schwermütig ausgedrückt werden. Sie hängen mit den Prosawerken zusammen: Die "Idyllen aus Messina" (1882) gingen in die zweite Auflage der "Fröhlichen Wissenschaft" ein, während einige der "Dionysos-Dithyramben" (1888/89) Überarbeitungen von Stücken aus "Also sprach Zarathustra" sind.

Lange Zeit umstritten war die Bedeutung von Nietzsches "Nachlass", dessen Rezeption zudem von der fragwürdigen Publikation durch das Nietzsche-Archiv erschwert wurde (vergleiche Nietzsche-Ausgabe). Extrempositionen bezogen hier einerseits Karl Schlechta, der zumindest im vom Archiv publizierten Nachlass nichts fand, was nicht auch in Nietzsches veröffentlichten Werken zu finden sei; und andererseits etwa Alfred Baeumler und Martin Heidegger, die Nietzsches veröffentlichtes Werk nur als „Vorhalle“ sahen, während sich die „eigentliche Philosophie“ im Nachlass befinde. Inzwischen herrscht eine mittlere Position vor, die den Nachlass als Ergänzung der veröffentlichten Werke begreift und darin ein Mittel sieht, Nietzsches Denkwege und Entwicklungen besser nachzuvollziehen.

Nietzsches Denken ist auf viele unterschiedliche Weisen interpretiert worden. Es enthält Brüche, verschiedene Ebenen und fiktive Standpunkte lyrischer Personen („Ein Fälscher ist, wer Nietzsche interpretiert, indem er Zitate aus ihm benutzt. […] Im Bergwerk dieses Denkers ist jedes Metall zu finden: Nietzsche hat alles gesagt und das Gegenteil von allem.“, Giorgio Colli). Eine kanonische Wiedergabe ist sehr schwierig.

Die Frage, ob das weitgehende Fehlen einer Systematik von Nietzsche beabsichtigt war, somit Ausdruck seiner Weltsicht ist, hat man in der Rezeption ausführlich diskutiert. Seit der zweiten Hälfte des 20. Jahrhunderts wird sie vorwiegend bejaht. Vergleiche hierzu unten den Abschnitt Kritik an Religion, Metaphysik und Erkenntnistheorie.

Eines der wichtigsten Objekte von Nietzsches Kritik spätestens seit "Menschliches, Allzumenschliches" ist die Moral im Allgemeinen und die christliche Moral im Besonderen. Nietzsche wirft der bisherigen Philosophie und Wissenschaft vor, herrschende Moralvorstellungen unkritisch übernommen zu haben; wahrhaftig freies und aufgeklärtes Denken habe sich dagegen, wie der Titel eines Buchs sagt, "Jenseits von Gut und Böse" zu stellen. Dies hätten alle abendländischen Philosophen seit Platon, insbesondere Kant, versäumt. Nietzsche untersucht oft Werturteile nicht auf ihre vermeintliche Gültigkeit hin, sondern beschreibt Zusammenhänge zwischen der Erschaffung von Werten durch einen Denker oder eine Gruppe von Menschen und deren biologisch-psychologischer Verfassung. Es geht ihm also um die Frage des Werts von moralischen Systemen überhaupt:

Diese Form der Kritik auf einer Meta-Ebene ist ein typisches Kennzeichen von Nietzsches Philosophie. "Vergleiche: Metaethik."

Er selbst führt diese Kritik mit Methoden der Geschichts-, Kultur- und Sprachwissenschaft exzessiv aus und legt dabei ein besonderes Augenmerk auf die Herkunft und Entstehung moralischer Denkweisen, etwa in "Zur Genealogie der Moral". Wichtige Begriffe seiner Moralkritik sind:


Solche Gedankengänge werden von Nietzsche zu einer immer radikaleren Kritik am Christentum, etwa in "Der Antichrist", gebündelt. Dieses sei nicht nur nihilistisch in dem Sinne, dass es der sinnlich wahrnehmbaren Welt jeden Wert abspreche – eine Kritik, die in Nietzsches Verständnis auch den Buddhismus treffe –, sondern im Gegensatz zum Buddhismus auch aus Ressentiment geboren. Das Christentum habe jede höhere Art Mensch und jede höhere Kultur und Wissenschaft behindert. C. A. Bernoulli hebt hervor, dass Nietzsches Anti-Christentum vornehmlich antisemitisch bestimmt ist und dass, wo er ehrlich spricht, „seine Urteile über die Juden allen Antisemitismus an Schärfe weit hinter sich lassen.“ In den späteren Schriften steigert Nietzsche die Kritik an allen bestehenden Normen und Werten: Sowohl in der bürgerlichen Moral als auch im Sozialismus und Anarchismus sieht er Nachwirkungen der christlichen Lehren am Werk. Die ganze Moderne leide an "décadence". Dagegen sei nun eine „Umwertung aller Werte“ nötig. Wie genau allerdings die neuen Werte ausgesehen hätten, wird aus Nietzsches Werk nicht eindeutig klar. Diese Frage und ihr Zusammenhang mit den Aspekten des "Dionysischen", des "Willens zur Macht", des "Übermenschen" und der "Ewigen Wiederkunft" werden bis heute diskutiert. Die extremsten Aussagen Nietzsches zur "Energie der Größe" und zum "Anti-Humanismus" finden sich in einem Nachlass-Fragment von 1884: „…-jene ungeheure "Energie der Größe" zu gewinnen, um, durch Züchtung und anderseits durch Vernichtung von Millionen Mißrathener, den zukünftigen Menschen zu gestalten und "nicht zu Grunde" zu gehen an dem Leid, das man "schafft", und dessen Gleichen noch nie da war!“

Mit dem Stichwort „Gott ist tot“ wird oft die Vorstellung verbunden, dass Nietzsche den Tod Gottes beschworen oder herbeigewünscht habe. Tatsächlich verstand sich Nietzsche eher als Beobachter. Er analysierte seine Zeit, vor allem die seiner Auffassung nach inzwischen marode gewordene (christliche) Zivilisation. Er war zudem nicht der erste, der die Frage nach dem „Tod Gottes“ stellte. Bereits der junge Hegel äußerte diesen Gedanken und sprach von dem „unendlichen Schmerz“ als einem Gefühl, „worauf die Religion der neuen Zeit beruht – das Gefühl: Gott selbst ist tot“.

Die bedeutendste und meistbeachtete Stelle zu diesem Thema ist der Aphorismus 125 aus der "Fröhlichen Wissenschaft" mit dem Titel „Der tolle Mensch“. Der stilistisch dichte Aphorismus enthält Anspielungen auf klassische Werke der Philosophie und Tragödie. Dieser Text lässt den Tod Gottes als bedrohliches Ereignis erscheinen. Dem Sprecher darin graut vor der Schreckensvision, dass die zivilisierte Welt ihr bisheriges geistiges Fundament weitgehend zerstört habe:

Dieser unfassbare Vorgang werde gerade wegen der großen Dimension lange brauchen, um in seiner Tragweite erkannt zu werden: „Ich komme zu früh, sagte er dann, ich bin noch nicht an der Zeit. Diess ungeheure Ereigniss ist noch unterwegs und wandert, – es ist noch nicht bis zu den Ohren der Menschen gedrungen.“ Und es wird gefragt: „Ist nicht die Grösse dieser That [Gott getötet zu haben] zu gross für uns? Müssen wir nicht selber zu Göttern werden, um nur ihrer würdig zu erscheinen?“ Unter anderem aus diesem Gedanken heraus erscheint später die Idee des „Übermenschen“, wie sie vor allem im "Zarathustra" dargestellt wird: „Todt sind alle Götter: nun wollen wir, dass der Übermensch lebe.“

Das Wort vom Tod Gottes findet sich auch in den Aphorismen 108 und 343 der "Fröhlichen Wissenschaft", und es taucht auch mehrmals in "Also sprach Zarathustra" auf. Danach verwendete Nietzsche es nicht mehr, befasste sich aber weiter intensiv mit dem Thema. Beachtenswert ist hier etwa das nachgelassene Fragment "Der europäische Nihilismus" (datiert 10. Juni 1887), in dem es heißt: „,Gott‘ ist eine viel zu extreme Hypothese.“

Nietzsche kommt zu dem Schluss, dass mehrere mächtige Strömungen, vor allem das Aufkommen der Naturwissenschaften und der Geschichtswissenschaft, daran mitgewirkt haben, die christliche Weltanschauung unglaubwürdig zu machen und damit die christliche Zivilisation zu Fall zu bringen. Durch die Kritik der bestehenden Moral, wie Nietzsche selbst sie betreibt, werde die Moral hohl und unglaubwürdig und breche schließlich zusammen. Mit dieser radikalisierten Kritik steht Nietzsche einerseits in der Tradition der französischen Moralisten, wie etwa Montaigne oder La Rochefoucauld, die die Moral ihrer Zeit kritisieren, um zu einer besseren zu gelangen; andererseits betont er mehrfach, er bekämpfe nicht nur die Heuchelei von Moral, sondern die herrschenden „Moralen“ selbst – im Wesentlichen immer die christliche. In diesem Sinne bezeichnet er sich selbst als „Immoralisten“.

Es besteht heute weitgehende Übereinstimmung, dass Nietzsche sich nicht als Befürworter des Nihilismus verstand, sondern ihn als Möglichkeit in der [nach]christlichen Moral, vielleicht auch als eine geschichtliche Notwendigkeit sah. Über den Atheismus Nietzsches im Sinne des Nichtglaubens an einen metaphysischen Gott sagen diese Stellen wenig aus. (Siehe hierzu den Abschnitt "Kritik an Religion, Metaphysik und Erkenntnistheorie".)

Das Begriffspaar „apollinisch-dionysisch“ wurde zwar schon von Schelling verwendet, fand aber erst durch Nietzsche Eingang in die Philosophie der Kunst. Mit den Namen der griechischen Götter Apollon und Dionysos bezeichnet Nietzsche in seiner frühen Schrift "Die Geburt der Tragödie aus dem Geiste der Musik" zwei gegensätzliche Prinzipien der Ästhetik. Apollinisch ist demnach der Traum, der schöne Schein, das Helle, die Vision, die Erhabenheit; dionysisch ist der Rausch, die grausame Enthemmung, das Ausbrechen einer dunklen Urkraft. In der attischen Tragödie ist Nietzsche zufolge die Vereinigung dieser Kräfte gelungen. Das „Ur-Eine“ offenbare sich dem Dichter dabei in der Form von dionysischer Musik und werde mittels apollinischer Träume in Bilder umgesetzt. Auf der Bühne sei die Tragödie durch den Chor geboren, der dem Dionysischen Raum gibt. Als apollinisches Element komme der Dialog im Vordergrund und der tragische Held hinzu.
Die griechische Tragödie sei durch Euripides und den Einfluss des Sokratismus zugrunde gegangen. Hierdurch sei vor allem das Dionysische, aber auch das Apollinische aus der Tragödie getrieben worden, sie selbst sei zu einem bloß dramatisierten Epos herabgesunken. Die Kunst habe sich in den Dienst des Wissens und sokratischer Klugheit gestellt und sei zur reinen Nachahmung geworden. Erst im Musikdrama Richard Wagners sei die Vereinigung der gegensätzlichen Prinzipien wieder gelungen.

In späteren Schriften rückt Nietzsche von dieser Position ab; insbesondere sieht er in den Werken Wagners jetzt keinen Neuanfang mehr, sondern ein Zeichen des Verfalls. Auch seine grundsätzlichen ästhetischen Betrachtungen variiert er: In den Schriften der „positivistischen“ Periode tritt die Kunst deutlich hinter die Wissenschaft zurück. Nunmehr ist für Nietzsche „der wissenschaftliche Mensch die Weiterentwickelung des künstlerischen“ ("Menschliches, Allzumenschliches"), ja sogar „[d]as Leben ein Mittel der Erkenntnis“ ("Die fröhliche Wissenschaft" ).

Erst nach "Also sprach Zarathustra" greift Nietzsche wieder deutlicher auf seine frühen ästhetischen Ansichten zurück. In einem Notizbuch von 1888 heißt es:

In den späten Schriften entwickelt er auch den Begriff des Dionysischen weiter. Die Gottheit Dionysos dient zur Projektion mehrerer wichtiger Lehren, und "Ecce homo" schließt mit dem Ausruf: „Dionysos gegen den Gekreuzigten!“ Das Thema des Dionysos ist eine der entscheidenden Konstanten im Leben und Werk Nietzsches, von seiner "Geburt der Tragödie" bis in den Wahnsinn hinein, wo er mit "Dionysos" unterschreibt und Cosima Wagner zu seiner "Ariadne" wird.

Mit der Kritik der Moral hängt eine Kritik bisheriger Philosophien zusammen. Gegen metaphysische und religiöse Konzepte ist Nietzsche grundsätzlich skeptisch. Die Möglichkeit einer metaphysischen Welt sei zwar nicht widerlegbar, aber sie gehe uns auch nichts an:

Alle metaphysischen und religiösen Spekulationen seien dagegen psychologisch erklärbar; sie hätten vor allem der Legitimation bestimmter Moralen gedient. Die jeweilige Art zu denken, die Philosophien der Philosophen sind nach Nietzsche aus deren körperlicher und geistiger Verfassung sowie ihren individuellen Erfahrungen abzuleiten.

Nietzsche wendet diese These auch in seinen Selbstanalysen an und weist wiederholt darauf hin, dass wir die Welt notwendigerweise stets perspektivisch wahrnehmen und auslegen. Schon die Notwendigkeit, sich in Sprache auszudrücken und damit Subjekte und Prädikate anzusetzen, sei eine vorurteilsbehaftete Auslegung des Geschehens ("Über Wahrheit und Lüge im außermoralischen Sinne"). Damit behandelte Nietzsche Fragen, die in Ansätzen von der modernen Sprachphilosophie wieder aufgenommen wurden.

Er würdigt die Skeptiker als den einzigen „anständigen Typus in der Geschichte der Philosophie“ ("Der Antichrist" ) und äußert grundsätzliche Vorbehalte gegen jede Art von philosophischem System. Es sei unredlich zu meinen, die Welt lasse sich in eine Ordnung einpassen:

In seiner Autobiographie "Ecce homo" beschreibt er ein letztes Mal sein Verhältnis zu Religion und Metaphysik:


In den Werken Nietzsches lässt sich zeigen, dass er schon in jungen Jahren einen Zugang zu den Themen der Metaphysik, der Religion und der Moral, später auch des Ästhetischen, aus einem historisch-kritischen Blickwinkel forderte. Alle Erklärungsmuster, die auf etwas Transzendentes, Unbedingtes, Universales abzielen, sind nichts als Mythen, die in der Geschichte der Erkenntnisentwicklung jeweils auf der Grundlage des Wissens ihrer Zeit entstanden sind. Dieses aufzudecken ist Aufgabe der modernen Wissenschaft und Philosophie. In diesem Sinne verstand sich Nietzsche als Verfechter eines radikalen Aufklärungsgedankens. „[…] erst nachdem wir die historische Betrachtungsart, welche die Zeit der Aufklärung mit sich brachte, in einem so wesentlichen Puncte corrigirt haben, dürfen wir die Fahne der Aufklärung — die Fahne mit den drei Namen: Petrarca, Erasmus, Voltaire — von Neuem weiter tragen. Wir haben aus der Reaction einen Fortschritt gemacht.“ Den Begriff der Genealogie verwendete er erstmals im Titel der "Genealogie der Moral". Die Methodik wird dort insbesondere in der zweiten Abhandlung in den Abschnitten 12 bis 14 ausgeführt. Die dahinter stehende Methode beschrieb und praktizierte er bereits in "Menschliches, Allzumenschliches" (Aphorismen 1 und 2), und bereits in der "Zweiten Unzeitgemäßen Betrachtung" reflektierte er den Wert des Historischen kritisch, zeigte dessen Grenzen, aber auch seine Unhintergehbarkeit. Genealogie bedeutet für Nietzsche nicht historische Forschung, sondern kritische Erklärung von Gegenwartsphänomenen anhand von (spekulativen) theoretischen Ableitungen aus der Geschichte. Im Mittelpunkt steht eine „Deplausibilisierung“ bisheriger Narrative in Philosophie, Theologie und den kulturwissenschaftlichen Fragen durch historisch gestützte psychologische Thesen. Großen Einfluss hat dieses Konzept Nietzsches auf Michel Foucault. Josef Simon setzte die Methode mit der modernen Dekonstruktion gleich.

Aus seiner Kritik von Metaphysik, Erkenntnistheorie, Moralphilosophie und Religion heraus entwickelte Nietzsche selbst ein pluralistisches Weltbild. Indem er die Welt und auch den Menschen als einen im ständigen Werden befindlichen Organismus auffasste, in dem eine Vielzahl von Elementen im ständigen Gegeneinander ihrer Kräfte danach ringt, sich durchzusetzen, löste er sich vom traditionellen Substanzdenken und von jeglichen kausal-mechanistischen sowie teleologischen Erklärungen. „Alle Einheit ist nur als Organisation und Zusammenspiel Einheit: nicht anders als wie ein menschliches Gemeinwesen eine Einheit ist: also Gegensatz der atomistischen Anarchie; somit ein Herrschafts-Gebilde, das Eins bedeutet, aber nicht eins ist.“ In diesem Organismus als Totalität wirken die verschiedensten Kräfte im Kampf gegeneinander; sie folgen ihrem jeweiligen Willen zur Macht (s. u.). „Leben wäre zu definiren als eine dauernde Form von Prozeß der Kraftfeststellungen, wo die verschiedenen Kämpfenden ihrerseits ungleich wachsen.“ Jeder Organismus führt seinen Kampf aus seiner eigenen Perspektive.

Die subjektive Sicht, die zur Perspektive führt, bedeutet nun weder Willkür noch Relativismus. Die jeweils eingenommene Perspektive führt vielmehr dazu, dass der Mensch die Welt, wie sie ihm erscheint, zu einem Bild, zu einer Interpretation zusammenfügt.
Der „Wille zur Macht“ ist "erstens" ein Konzept, das zum ersten Mal in "Also sprach Zarathustra" vorgestellt und in allen nachfolgenden Büchern zumindest am Rande erwähnt wird. Seine Anfänge liegen in den psychologischen Analysen des menschlichen Machtwillens in der "Morgenröte". Umfassender führte es Nietzsche in seinen nachgelassenen Notizbüchern ab etwa 1885 aus.

"Zweitens" ist es der Titel eines von Nietzsche auch als "Umwertung aller Werte" geplanten Werks, das nie zustande kam. Aufzeichnungen dazu gingen vor allem in die Werke "Götzen-Dämmerung" und "Der Antichrist" ein.

"Drittens" ist es der Titel einer Nachlasskompilation von Elisabeth Förster-Nietzsche und Peter Gast, die nach Ansicht dieser Herausgeber dem unter Punkt zwei geplanten „Hauptwerk“ entsprechen soll.

Die Deutung des "Konzepts" „Wille zur Macht“ ist stark umstritten. Für Martin Heidegger war es Nietzsches Antwort auf die metaphysische Frage nach dem „Grund alles Seienden“: Laut Nietzsche sei alles „Wille zur Macht“ im Sinne eines inneren, metaphysischen Prinzips, so wie dies bei Schopenhauer der „Wille (zum Leben)“ ist. Die entgegengesetzte Meinung vertrat Wolfgang Müller-Lauter: Danach habe Nietzsche mit dem „Willen zur Macht“ keineswegs eine Metaphysik im Sinne Heideggers wiederhergestellt – Nietzsche war ja gerade Kritiker jeder Metaphysik –, sondern den Versuch unternommen, eine in sich konsistente Deutung allen Geschehens zu geben, die die nach Nietzsche irrtümlichen Annahmen sowohl metaphysischer „Sinngebungen“ als auch eines atomistisch-materialistischen Weltbildes vermeide. Um Nietzsches Konzept zu begreifen, sei es angemessener, von "den" (vielen) „Willen zur Macht“ zu sprechen, die im dauernden Widerstreit miteinander stehen, sich gegenseitig bezwingen und einverleiben, zeitweilige Organisationen (beispielsweise den menschlichen Leib), aber keinerlei „Ganzes“ bilden, denn die Welt sei ewiges Chaos.

Zwischen diesen beiden Interpretationen bewegen sich die meisten anderen, wobei die heutige Nietzscheforschung derjenigen Müller-Lauters deutlich näher steht. Gerade der Begriff Macht weist jedoch bei Nietzsche (mit seiner stets auf das gesunde Individuum ausgerichteten Weltanschauung) auf neuere positive Verständnisformen voraus, wie wir sie z. B. bei Hannah Arendt finden – hier jedoch bezogen auf den Menschen in der Gesellschaft: die grundsätzliche Möglichkeit aus sich heraus gestaltend „etwas zu machen“.

Nietzsches zuerst in "Die fröhliche Wissenschaft" auftretender und in "Also sprach Zarathustra" als Höhepunkt vorgeführter „tiefster Gedanke“, der ihm auf einer Wanderung im Engadin nahe Sils-Maria kam, ist die Vorstellung, dass alles Geschehende schon unendlich oft geschah und unendlich oft wiederkehren wird. Man solle deshalb so leben, dass man die immerwährende Wiederholung eines jeden Augenblickes nicht nur ertrage, sondern sogar begrüße. „Doch alle Lust will Ewigkeit – will tiefe, tiefe Ewigkeit“ lautet folglich ein zentraler Satz in "Also sprach Zarathustra". Eng mit der „Ewigen Wiederkunft“, für die Nietzsche trotz seiner nur sehr oberflächlichen naturwissenschaftlichen Bildung auch wissenschaftliche Begründungen zu geben versuchte, hängt wohl der "Amor fati" (lat. „Liebe zum Schicksal“) zusammen. Dies ist für Nietzsche eine Formel zur Bezeichnung des höchsten Zustands, den ein Philosoph erreichen kann, die Form der höchstgesteigerten Lebensbejahung.

Über die „ewige Wiederkunft“, ihre Bedeutung und Stellung in Nietzsches Gedanken herrscht keine Einigkeit. Während einige Deuter sie als Zentrum seines gesamten Denkens ausmachten, sahen andere sie bloß als fixe Idee und störenden „Fremdkörper“ in Nietzsches Lehren.

An einen Fortschritt in der Geschichte der Menschheit – oder in der Welt überhaupt – glaubt Nietzsche nicht. Für ihn ist folglich das Ziel der Menschheit nicht an ihrem (zeitlichen) Ende zu finden, sondern in ihren immer wieder auftretenden höchsten Individuen, den "Über"menschen. Die Gattung Mensch als Ganzes sieht er nur als einen Versuch, eine Art Grundmasse, aus der heraus er „Schaffende“ fordert, die „hart“ und mitleidlos mit anderen und vor allem mit sich selbst sind, um aus der Menschheit und sich selbst ein wertvolles Kunstwerk zu schaffen. Als negatives Gegenstück zum Übermenschen wird in "Also sprach Zarathustra" der "letzte Mensch" vorgestellt. Dieser steht für das schwächliche Bestreben nach Angleichung der Menschen untereinander, nach einem möglichst risikolosen, langen und „glücklichen“ Leben ohne Härten und Konflikte. Das Präfix „Über“ in der Wortschöpfung „Übermensch“ kann nicht nur für eine höhere Stufe relativ zu einer anderen stehen, sondern auch im Sinne von „hinüber“ verstanden werden, kann also eine Bewegung ausdrücken. Der Übermensch ist daher nicht unbedingt als Herrenmensch über dem letzten Menschen zu sehen. Eine rein politische Deutung gilt der heutigen Nietzscheforschung als irreführend. Der „Wille zur Macht“, der sich im Übermenschen konkretisieren soll, ist demnach nicht etwa der Wille zur Herrschaft über andere, sondern ist als Wille zum Können, zur Selbstbereicherung, zur Selbstüberwindung zu verstehen.

Aus seiner Jugend im Pfarrhaus und im kleinbürgerlich-frommen „Frauenhaushalt“ ergaben sich Nietzsches erste praktische Erfahrungen mit dem Christentum. Schon sehr bald entwickelte er hier einen kritischen Standpunkt und las Schriften von Ludwig Feuerbach und David Friedrich Strauß. Wann genau diese Entfremdung von der Familie begann und welchen Einfluss sie auf Nietzsches weiteren Denk- und Lebensweg hatte, ist Gegenstand einer andauernden Debatte in der Nietzsche-Forschung.

Auch der frühe Tod des Vaters dürfte Nietzsche beeinflusst haben, jedenfalls wies er selbst oft auf dessen Bedeutung für ihn hin. Dabei ist zu beachten, dass er ihn selbst kaum kannte, sondern sich aus Familienerzählungen ein wohl idealisiertes Bild des Vaters machte. Als freundlicher und beliebter, andererseits körperlich schwacher und kranker Landpfarrer taucht er in Nietzsches Selbstanalysen immer wieder auf.

Schon in seiner Jugend war Nietzsche von den Schriften Ralph Waldo Emersons und Lord Byrons beeindruckt, den seinerzeit tabuisierten Hölderlin erkor er zu seinem Lieblingsdichter. Auch Machiavellis Werk "Der Fürst" las er bereits privat in der Schulzeit.

Wie stark der Einfluss des Dichters Ernst Ortlepp oder die Ideen Max Stirners beziehungsweise des ganzen Junghegelianismus auf Nietzsche waren, ist umstritten. Der Einfluss Ortlepps ist vor allem von Hermann Josef Schmidt hervorgehoben worden. Über den Einfluss Stirners auf Nietzsche wird bereits seit den 1890ern debattiert. Einige Interpreten sahen hier höchstens eine flüchtige Kenntnisnahme, andere dagegen, allen voran Eduard von Hartmann, erhoben einen Plagiatsvorwurf. Bernd A. Laska vertritt die Außenseiter-These, Nietzsche habe infolge der Begegnung mit dem Werk Stirners, das ihm vom Junghegelianer Eduard Mushacke vermittelt worden sei, eine „initiale Krise“ durchgemacht, die ihn zu Schopenhauer führte.

Im Philologiestudium bei Ritschl lernte Nietzsche neben den klassischen Werken selbst vor allem philologisch-wissenschaftliche Methoden kennen. Dies dürfte einerseits die Methodik seiner Schriften beeinflusst haben, was insbesondere in der "Genealogie der Moral" deutlich wird, andererseits aber auch sein Bild von der strengen Wissenschaft als mühselige Arbeit für mittelmäßige Geister. Seine eher negative Haltung zum Wissenschaftsbetrieb an den Universitäten beruhte zweifellos auf eigenen Erfahrungen sowohl als Student als auch als Professor.

An der Universität versuchte Nietzsche den von ihm geschätzten Jacob Burckhardt zu Gesprächen zu gewinnen, las einige von dessen Büchern und hörte sich Vorlesungen des Kollegen an. Mit dem Freund Franz Overbeck hatte er in der Basler Zeit einen regen Gedankenaustausch, auch später half ihm Overbeck in theologischen und kirchengeschichtlichen Fragen weiter.

Werke bekannter Schriftsteller wie Stendhal, Tolstoi und Dostojewski machte Nietzsche sich für sein eigenes Denken ebenso zunutze wie solche heute eher unbekannter Autoren wie William Edward Hartpole Lecky oder Fachgelehrter wie Julius Wellhausen. Zu seinen Ansichten über die moderne "décadence" las und bewertete er etwa George Sand, Gustave Flaubert und die Brüder Goncourt.

Schließlich lässt sich Nietzsches Interesse an Wissenschaften von der Physik (besonders Roger Joseph Boscovichs System) bis zur Nationalökonomie belegen. Auf die besondere Bedeutung der kritischen Auseinandersetzung mit dem Buch "Der Ursprung der moralischen Empfindungen" (1877) von Paul Rée verwies Nietzsche in der Vorrede zur "Genealogie der Moral" Für sein Wissen über die Physiologie, auch in der kritischen Auseinandersetzung mit dem Darwinismus, stützte sich Nietzsche stark auf das Werk "Der Kampf der Theile im Organismus. Ein Beitrag zur Vervollständigung der mechanischen Zweckmäßigkeitslehre" des Anatomen Wilhelm Roux. Er meinte auch, durch seine Krankheiten ein besseres Wissen über Medizin, Physiologie und Diätetik erlangt zu haben als manche seiner Ärzte.

Ab Mitte der 1860er übten die Werke Arthur Schopenhauers großen Einfluss auf Nietzsche aus; dabei bewunderte Nietzsche aber schon zu Beginn weniger den Kern der Schopenhauerschen Lehre als die Person und den „Typus“ Schopenhauer, das heißt in seiner Vorstellung den wahrheitssuchenden und „unzeitgemäßen“ Philosophen. Eine weitere wesentliche Inspiration war dann die Person und die Musik Richard Wagners. Die Schriften "Richard Wagner in Bayreuth" (Vierte "Unzeitgemäße Betrachtung") und vor allem die "Geburt der Tragödie" feiern dessen Musikdrama als Überwindung des Nihilismus ebenso wie eines platten Rationalismus. Diese Verehrung schlug spätestens 1879 nach Wagners vermeintlicher Hinwendung zum Christentum (in "Parsifal") in Feindschaft um. Nietzsche rechtfertigte seinen radikalen Sinneswandel später in "Der Fall Wagner" und in "Nietzsche contra Wagner". Dass Nietzsche sich auch lange nach Wagners Tod 1883 beinahe zwanghaft mit dem einstigen „Meister“ beschäftigte, hat einige Aufmerksamkeit gefunden: Über das komplizierte Verhältnis zwischen Nietzsche und Wagner (sowie Wagners Frau Cosima) gibt es viele Untersuchungen mit teilweise unterschiedlichen Ergebnissen. Neben den von Nietzsche genannten weltanschaulichen und kunstphilosophischen Differenzen haben sicherlich auch persönliche Gründe eine Rolle bei Nietzsches „Abfall“ von Wagner gespielt.

Auch Schopenhauer sah er nun kritischer und meinte, gerade in dessen Pessimismus und Nihilismus ein zeittypisches und daher rückwärtsgewandtes Phänomen zu erkennen. Freilich fand er auch 1887 noch lobende Worte für Schopenhauer, der „als Philosoph der erste eingeständliche und unbeugsame Atheist [war], den wir Deutschen gehabt haben“:

Sein Wissen über Philosophie und Philosophiegeschichte hat Nietzsche sich nicht systematisch aus den Quellen angeeignet. Er hat es vornehmlich aus Sekundärliteratur entnommen: vor allem aus Friedrich Albert Langes "Geschichte des Materialismus" sowie Kuno Fischers "Geschichte der neuern Philosophie" zu späteren Autoren. Platon und Aristoteles waren ihm aus der Philologie bekannt und auch Gegenstand einiger seiner philologischen Vorlesungen, aber besonders Letzteren kannte er nur lückenhaft. Mit den Vorsokratikern befasste er sich zu Anfang der 1870er Jahre intensiv, vor allem auf Heraklit kam er noch später zurück.

Für die "Ethik" Spinozas, die Nietzsche zeitweise anregte, war ihm Fischers Werk die Hauptquelle. Kant lernte er ebenfalls durch Fischer (und Schopenhauer, s. oben) kennen; im Original las er vermutlich nur die "Kritik der Urteilskraft". Zum deutschen Idealismus um Hegel übernahm er für einige Zeit die scharfe Kritik Schopenhauers. Später ignorierte er die Richtung. Bedenkenswert ist, dass sich bei Nietzsche zu den Junghegelianern (Feuerbach, Bauer und Stirner) keine nennenswerten Äußerungen finden, obwohl er sie als Denker einer „geistesregen Zeit“ ansah, auch keine zu Karl Marx, obwohl er sich verschiedentlich über den politischen Sozialismus äußerte.

Weitere von Nietzsche rezipierte Quellen waren die französischen Moralisten wie La Rochefoucauld, Montaigne, Vauvenargues, Chamfort, Voltaire und Stendhal. Die Lektüre Blaise Pascals vermittelte ihm einige neue Einsichten zum Christentum.

Hin und wieder setzte sich Nietzsche polemisch mit den seinerzeit populären Philosophen Eugen Dühring, Eduard von Hartmann und Herbert Spencer auseinander. Vor allem von Letzterem und den deutschen Vertretern der Evolutionstheorie um Ernst Haeckel bezog er sein Wissen um die Lehren Charles Darwins.

Die intensive Quellenforschung der letzten Jahrzehnte hat zahlreiche Bezüge in Nietzsches Werken ermittelt, unter anderem zu den beiden heute weniger bekannten Denkern African Spir und Gustav Gerber, deren Sprach- und Erkenntnistheorie überraschende Ähnlichkeiten mit der Nietzsches aufweisen.

Vereinzelt ist in der Nietzsche-Forschung darauf hingewiesen worden, dass Nietzsches Kritik an anderen Philosophien und Lehren auf Missverständnissen beruhe, eben weil er sie nur durch entstellende Sekundärliteratur kannte. Dies betreffe insbesondere Nietzsches Aussagen zu Kant und der Evolutionslehre. Aber auch dieses Thema ist umstritten.

Eingeklammerte Jahreszahlen geben das Jahr der Entstehung, mit Kommata abgetrennte das Jahr der Erstveröffentlichung an.




Seit seiner Jugend musizierte Nietzsche und komponierte zahlreiche kleinere Stücke.

Bedeutend sind:






Seit 2012 wird der auf 12 Bände angelegte, erste Standard-Kommentar zu Nietzsches Werken publiziert:
Heidelberger Akademie der Wissenschaften (Hrsg.): "Historischer und kritischer Kommentar zu Friedrich Nietzsches Werken". Berlin / Boston: Walter de Gruyter 2012 ff.
Erschienen sind bisher sechs Bände:



Im Jahr 1951 erschien in den USA ein Buch mit dem Titel "My Sister and I", als dessen Autor Friedrich Nietzsche angegeben wurde. Nietzsche soll diese angeblich autobiographische Schrift 1889–1890 während seines Aufenthalts in der Jenaer Nervenklinik verfasst haben. Ein Originalmanuskript ist jedoch nicht überliefert. Als Übersetzer ins Englische ist Oscar Levy genannt, der Herausgeber der ersten englischsprachigen Ausgabe von Nietzsches Werken. Levy war aber schon 1946 verstorben, und seine Erben bestritten, dass er etwas mit dem Werk zu tun habe. Da keine Belege für die behauptete Urheberschaft Nietzsches oder Levys vorlagen, wurde die Schrift von der Fachwelt überwiegend als Fälschung zurückgewiesen oder ignoriert. In den 1980er Jahren stellten einzelne Stimmen diese Zurückweisung infrage. Teile der Kontroverse um die Autorschaft sind in Neuauflagen des Buches abgedruckt, etwa in:






MDR Figaro sendete am 30. Mai 2015 ein fast einstündiges Feature von Katrin Schumacher mit dem Titel "Musikalische Stille – Von einem Philosophen, einem Hotel und der Musik am Ende der Welt", das auch Nietzsche und seine Zeit im Schweizer Hotel Waldhaus zum Thema hatte.

Zum 100. Todesjahr Nietzsches gab die Deutsche Post im Jahr 2000 ein Sonderpostwertzeichen mit seinem Porträt zum Nennwert 110 Pfennige heraus (Michel-Nr. 2131).


</doc>

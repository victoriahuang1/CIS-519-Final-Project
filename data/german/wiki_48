<doc id="5453" url="https://de.wikipedia.org/wiki?curid=5453" title="Versicherungsverein auf Gegenseitigkeit">
Versicherungsverein auf Gegenseitigkeit

Ein Versicherungsverein auf Gegenseitigkeit (VVaG) ist eine der in Deutschland rechtlich zulässigen Rechtsformen für einen Versicherer. Die Versicherungsnehmer sind Mitglieder und Träger des Vereins. In Ausnahmefällen können Versicherungsvereine auf Gegenseitigkeit auch einzelne Verträge abschließen, bei denen der Versicherungsnehmer nicht Mitglied wird (Nichtmitgliederversicherung).

Der VVaG ist eine besondere Rechtsform für Versicherer, nämlich ein Verein, der die Versicherung seiner Mitglieder nach dem Grundsatz der Gegenseitigkeit betreiben will ( VAG). Diese besondere, nur für Versicherer zulässige Rechtsform des VVaG, ist im vierten Kapitel des zweiten Teils des VAG geregelt. Daneben gelten verschiedene weitere Vorschriften aus dem Vereinsrecht, dem Handelsrecht, dem Aktienrecht sowie dem Genossenschaftsrecht. Der VVaG unterscheidet sich von der Genossenschaft dadurch, dass diese durch die Zielsetzung gekennzeichnet ist, der VVaG durch das Gegenseitigkeitsprinzip. In Deutschland ist Genossenschaften das Versicherungsgeschäft verwehrt. Gegenüber der Aktiengesellschaft, bei der das Kapitalelement dominiert, überwiegt das personale Element im Sinne eines Personenvereins. Der VVaG ist getragen von den Bedürfnissen seiner Mitglieder ( VAG). Das sichert ihm Marktnähe und Innovationskraft. Ähnlich dem Entscheidungsgremium der Hauptversammlung für die Aktionäre einer Aktiengesellschaft hat der Versicherungsverein für seine Mitglieder als oberstes Organ die Mitgliedervertreterversammlung, teilweise auch Hauptversammlung genannt ( VAG).

Das Prinzip der „Gegenseitigkeit“ unter den Mitgliedern tritt bei großen VVaG eher in den Hintergrund. Dort steht dagegen die Beziehung zwischen dem einzelnen Mitglied und der juristischen Person der VVaG im Vordergrund.

Bei den VVaG gibt es vereinfachte Vorschriften für kleinere Vereine, die nur einen sachlich, örtlich oder dem Personenkreis nach begrenzten Wirkungskreis haben ( VAG). Kleinere Vereine sind z. T. sehr bodenständige, lokale Versicherer, die von kleinen Gruppen von Versicherungsnehmern zum direkten gegenseitigen Nutzen betrieben werden. Diese sind zwar nicht besonders reich an Angeboten, aber im Vergleich zur großen Unternehmen sehr flexibel und sehr spezialisiert. Oft gehen die dann für den gesamten Versicherungsmarkt gültigen Regelungen als Innovationen von diesen aus (Nichtrauchertarife, Krankenversicherungstarife für Vegetarier oder die Versicherung von Windausfall für Windkraftanlagen).

Ähnlich dem VVaG, aber keine Versicherungsgesellschaften oder VVaG sind die nicht eingetragenen und eingetragene Vereine, die z. B. als Unterstützungskassen im Gesundheitswesen tätig sind. Diese dürfen jedoch ihre Leistungen nicht garantieren und gelten daher nicht als stets eine bestimmte Entschädigung garantierende Versicherer. Es gibt hier nur dann und in dem Umfang eine Entschädigung für einen Schaden, wenn bzw. wie der Vereinskassenstand eine Auszahlung zulässt. In einigen Fällen kaufen die Vereine Rückdeckung bei einem Versicherer. Bei einigen Vereinen wird durch einen erhöhten Beitrag eine privilegierte Mitgliedschaft erzielt, in der garantierte Leistungen durch einen traditionellen Versicherer enthalten sind (wie z. B. Verkehrsclubs).

VVaG sind die Urform der modernen Versicherer. Die Idee des VVaG als Rechtsform geht auf James Dodson (1710–1757) zurück, der die erste altersabhängige Beitragstabelle für Lebensversicherungsverträge berechnete. Auf seine Initiative hin wurde 1762 der erste professionelle, auf mathematischer Basis arbeitende Lebensversicherer der Welt gegründet, zugleich auch der erste VVaG, die "Society for Equitable Assurances on Lives and Survivorships". Der erste VVaG in Deutschland geht auf den Kaufmann Ernst-Wilhelm Arnoldi zurück. Als Arnoldi im Jahre 1820 in Gotha die "Gothaer Feuerversicherungsbank des Deutschen Handelsstandes", die heutige Gothaer Versicherung, ins Leben rief, verwirklichte er damit die Idee der gegenseitigen Hilfe: Alle tragen gemeinsam die Last des Einzelnen. Dabei sind die Versicherungsnehmer gleichzeitig Eigentümer des Unternehmens. Ähnlich wie bei genossenschaftlichen Banken existieren keine ausschließlich Kapital gebenden Eigentümer, weshalb eine kontinuierliche und von Kapitalgebern unabhängige Geschäftspolitik im Interesse der Mitglieder garantiert ist. Notwendige unternehmerische Entscheidungen können schnell getroffen und umgesetzt werden.

Seit einer Reihe von Jahren werden weltweit, etwas verzögert auch in Deutschland, immer mehr VVaG in Aktiengesellschaften umgewandelt (Demutualisierung). Hierbei ist es immer wieder zu Unstimmigkeiten über die den bisherigen Eigentümern, den Versicherungsnehmern, zu zahlende Entschädigung gekommen. Das Bundesverfassungsgericht hat 2005 hierzu in einem Urteil Änderungen der gesetzlichen Vorschriften vorgeschrieben (26. Juli 2005, 1 BVR 782/94 und 1 BVR 957/96), aber keine Änderung der bisher erfolgten Vorgänge verlangt.

In Deutschland waren im Jahre 2015 ca. 90 VVaGs im Handelsregister eingetragen, einschließlich nicht eintragungspflichtiger kleiner VVaGs gab es 254 Vereine dieser Rechtsart (Stand: 2014).

In der Sparte Schaden/Unfall betrug ihr Marktanteil in der Vergangenheit 28 % (Beitragsvolumen 1999: 48 Mrd. Euro) und in der Sparte Leben 23 % (59 Mrd. Euro). In der Sparte der Krankenversicherung (Gesamtbeitragsvolumen 20 Mrd. Euro) waren die VVaG mit 52 % Marktanteil den Aktiengesellschaften sogar eindeutig voraus. In der Sparte Schaden/Unfall verbuchten die VVaG einen über dem Durchschnitt liegenden Beitragszuwachs von 0,6 %, in den Bereichen Leben gar 9,4 % und in der Sparte Kranken 3,0 %. Die durchschnittliche Verzinsung der Kapitalanlage 1999 von über 179 Mrd. Euro betrug 7,4 %. Insgesamt verteilt sich das auf dem deutschen Markt eingenommene Beitragsvolumen zu 58 % auf Versicherungsunternehmen mit der Rechtsform Aktiengesellschaften, 11 % öffentlich-rechtliche Versicherungsunternehmen sowie 29 % Versicherungsvereine auf Gegenseitigkeit einschließlich ihrer Töchter.

Die Marktbedeutung der VVaG äußert sich auch in ihrem wirtschaftlichen Erfolg. Wirtschaftlicher Erfolg lässt sich in Kennzahlen messen. VVaG haben im Vergleich zu Aktiengesellschaften höhere Zuwachsraten bei den Versicherungsverträgen, sind in Sparten mit niedrigerem Risiko tätig, weisen geringere Kostensätze aus und erwirtschaften mehr Gewinn. Die Bilanzen der VVaG, gemessen an der Bilanzsumme, haben höhere Zuwachsraten, weisen nach einer Studie aus dem Jahr 1997 (Farny, Köln) im Fall von Schaden- und Unfallversicherern mehr sichtbares Eigenkapital aus, beinhalten mehr stille Reserven und bedecken im Übrigen mehr Solvabilität. VVaG verwenden ihren Bruttoüberschuss vor Steuern durchschnittlich mit 28 % für das Eigenkapital, während Aktiengesellschaften hierfür nur 10 % verwenden.
Eine Ursache geringerer Kostenquoten (Vergleichsgröße v. a. bei Lebensversicherungen) von Versicherungsvereinen ist die frühe Nutzung des Direktvertriebes, den diese lange vor Aufkommen der internetbasierten Direktabschlüsse betrieben. Durch geringere Abschlusskosten (Einsparung an Provisionen für Versicherungsvertreter und Makler) konnten diese ihre Kostenquoten niedrig halten.

Der wirtschaftliche Erfolg des VVaG bedeutet eine der wesentlichen Stärken seiner Rechtsform und begründet seine außerordentliche Bedeutung für den Versicherungsmarkt.

Da ein VVaG keine fremden Eigentümer hat, die Ansprüche auf den erzielten Gewinn haben, verbleiben erwirtschaftete Überschüsse im Unternehmen oder kommen den Versicherungsnehmern als Vereinsmitgliedern zugute. Andererseits kann ein VVaG nicht ohne Weiteres Kapital auf dem Kapitalmarkt aufnehmen, sondern muss die benötigten Sicherheitsmittel aus seinen Gewinnen selbst erwirtschaften. Dies schränkt wiederum seine Möglichkeiten, Gewinne zugunsten der Versicherungsnehmer zu verwenden, ein. Zudem kann ein VVaG im Rahmen dieser Beschränkung sein Geschäft nicht ohne Weiteres ausdehnen, da er die für zusätzliches Geschäft benötigten Sicherheitsmittel erst aus dem bestehenden Geschäft bilden muss.

Im internationalen Wettbewerb erweist sich die Rechtsform des VVaG jedoch immer mehr als Hemmschuh, weshalb entsprechend ambitionierte VVaG Umwandlungen (Demutualisierung) oder Holdingstrukturen anstreben.

VVaG müssen die zur Absicherung von Schwankungen des Versicherungsverlaufs benötigten Sicherheitsmittel selbst systematisch erwirtschaften, während Aktiengesellschaften diese bei Bedarf kurzfristig auf den Kapitalmärkten aufnehmen können. VVaG halten daher regelmäßig besonders hohe Sicherheitsmittel vor, die sie systematisch aufbauen; nur hierfür nicht benötigte Beträge können zugunsten der Versicherungsnehmer verwendet werden. Der Aufbau der Sicherheitsmittel des VVaG muss mit dem Wachstum des Geschäfts Schritt halten; insofern unterliegt der VVaG Wachstumsschranken. VVaG weisen daher in der Sparte Schaden/Unfall im Durchschnitt erheblich über dem Soll liegende Solvabilität auf, so dass eine Vielzahl von VVaG einen Deckungsgrad der Ist-Solvabilität zwischen 250 und 450 % aufweisen. In der Lebensversicherung liegt der Schnitt zwischen 150 und 200 % und in der Sparte Kranken zwischen 200 und 300 %. Andererseits haben VVaG für diese hohen Reserven aber keine Finanzierungskosten, wie sie von Aktiengesellschaften für auf dem Kapitalmarkt aufgenommene Beträge in erheblichen Umfang in Form von Dividenden zu leisten sind. Daher halten VVaG soviel Reserven wie sinnvoll bzw. nötig, Aktiengesellschaften so wenig Reserven wie vertretbar bzw. möglich. Soweit der VVaG gleichmäßig wächst und keine größeren Schwankungen im Versicherungsergebnis hat, kann er daher Versicherungsschutz günstiger anbieten als Aktiengesellschaften. Hingegen kann hohes Wachstum, größere Diversifikation und der Ausgleich von starken Schwankungen besser von Aktiengesellschaften geleistet werden.

VVaG führen also in größerem Umfang als Aktiengesellschaften Gewinne dem Eigenkapital zu, zahlen dafür aber keine Dividenden an Aktionäre. Zusätzlich bieten sich für VVaG als Alternative zur Kapitalbeschaffung börsennotierte Genussrechte an. Eine weitere Möglichkeit, den Kapitalmarkt zu nutzen, besteht durch die Schaffung von Tochterunternehmen in Form von Aktiengesellschaften und die Bündelung von Aktivitäten in Zwischenholdings. Dies bietet bei ausreichendem Volumen sogar die Möglichkeit eines Börsengangs.

Über 60 deutsche Versicherungsvereine auf Gegenseitigkeit sind in der „Arbeitsgemeinschaft der Versicherungsvereine auf Gegenseitigkeit e. V.“ (ARGE VVaG) mit Sitz in Köln organisiert. Hauptzweck der ARGE VVaG ist die Vertretung derjenigen Interessen der VVaG, die in ihrer spezifischen Rechtsform begründet sind. Zu den Zielen zählt hierbei, die Vorteile des VVaG-Gedanken stärker in die öffentliche Diskussion einzubringen sowie die Nachteile der VVaG gegenüber anderen Rechtsformen abzubauen.





</doc>
<doc id="5454" url="https://de.wikipedia.org/wiki?curid=5454" title="Vanillin">
Vanillin

Vanillin ("4-Hydroxy-3-methoxybenzaldehyd", FEMA 3107) ist der Hauptaromastoff in den Kapselfrüchten der Gewürzvanille ("Vanilla planifolia") sowie ein naturidentischer Aromastoff. Die organische chemische Verbindung mit der Summenformel CHO ist ein Derivat des Benzaldehyds mit je einer zusätzlichen Hydroxy- und Methoxygruppe.

Vanillin ist der Hauptbestandteil des natürlichen Vanilleextrakts, einer Mischung aus mehreren hundert verschiedenen Verbindungen. Aus diesem eher raren Naturprodukt wurde Vanillin schon Mitte des 19. Jahrhunderts isoliert, 1874 gelang die erste Synthese aus dem Naturstoff Coniferin. Die ersten kommerziellen Herstellungsverfahren von Vanillin gingen später von Eugenol aus. Heute wird Vanillin als „naturidentisch“ kostengünstig aus Guajacol synthetisiert oder aus Lignin gewonnen, einem Bestandteil von Holz und dem häufigsten Nebenprodukt der industriellen Papierherstellung. Das Vanillin im Lignin trägt auch zum typischen Geruch alten Papiers bei. Daneben sind inzwischen mehrere biotechnologische Verfahren etabliert, deren Produkte dürfen als „natürlich“ deklariert werden.

Vanillin ist weltweit mengenmäßig der wichtigste Aromastoff, der zudem preisgünstig hergestellt werden kann. Er wird in Lebensmitteln, Getränken, Speiseeis, Backwaren und Schokolade, sowie in der Parfüm- und Pharmaindustrie verwendet.

Vanille wurde als Aroma von präkolumbianischen Völkern Mittelamerikas angebaut; zum Zeitpunkt ihrer Eroberung durch Hernán Cortés benutzten die Azteken es als Aromastoff für Schokolade. Den Europäern wurde sowohl Schokolade als auch Vanille um das Jahr 1520 bekannt.

Vanillin wurde erstmals im Jahre 1858 als relativ reine Substanz von Nicolas-Théodore Gobley isoliert; dies geschah durch vollständiges Eindampfen eines Vanilleextrakts und anschließendes Umkristallisieren aus heißem Wasser. Im Jahr 1874 gelang erstmals dem Chemiker Wilhelm Haarmann zusammen mit Ferdinand Tiemann in Holzminden die Herstellung von Vanillin aus Coniferin, das im Rindensaft von Nadelhölzern (Coniferen) vorkommt.

1876 synthetisierte Karl Reimer erstmals aus Guajacol (1) das Vanillin (2). In der später als Reimer-Tiemann-Reaktion benannten Synthese wird Guajacol im Alkalischen mit Chloroform umgesetzt. Dabei reagiert zuerst Chloroform mit der Base zu Dichlorcarben. Dieses lagert sich am Phenolat-Anion des Guajacols an.

Vanillin findet sich am häufigsten in den meist falsch als Schoten bezeichneten Kapselfrüchten der Gewürzvanille ("Vanilla planifolia") (1,5–4 %), ferner auch in Styrax, Gewürznelken und anderen Pflanzen. Die frisch geernteten grünen Samenkapseln enthalten Vanillin in Form seines β--Glucosids "Vanillosid". Die grünen Hülsen besitzen nicht den Geschmack oder Geruch von Vanille. Relativ reines Vanillin kann sich als weißer Staub oder „Frost“ auf der Außenseite der Hülsen abscheiden.

In niedrigeren Konzentrationen trägt Vanillin zum Geschmack und Aroma von Lebensmitteln in vielfältiger Weise bei: in Olivenöl, Butter, Himbeeren und Lychee-Früchten. Bei der Reifung von Weinen und Spirituosen in Eichenfässern trägt Vanillin gleichfalls zum Geschmacksprofil bei. In anderen Lebensmitteln entsteht durch Wärmebehandlung Vanillin aus anderen vorhandenen Inhaltsstoffen. Auf diese Weise trägt Vanillin zum Geschmack und Aroma von geröstetem Kaffee bei, ferner in Ahornsirup und Vollkornprodukten, einschließlich Mais-Tortillas und Haferflocken.

Die bis zu 30 cm langen Kapselfrüchte der Gewürzvanille werden kurz vor der Reife geerntet. Diese haben noch nicht das typische Aroma und den Geschmack des fertigen Produkts. Zur Gewinnung werden die Früchte der sogenannten "Schwarzbräunung" unterzogen. Zuerst werden die Kapselfrüchte heißwasser- oder wasserdampfbehandelt, anschließend folgt eine Fermentation in luftdichten Behältern. Durch die Trocknungs- und Fermentierungsprozesse wandeln sich die β--Glucoside des Vanillins in Vanillin und Glucose um.

Ein Großteil des Vanillins wird aus den bei der Papierherstellung anfallenden Sulfitabfällen gewonnen. Die hierin enthaltene Ligninsulfonsäure wird bei erhöhter Temperatur und erhöhtem Druck mit Oxidantien und Alkalien behandelt, wobei unter anderem Vanillin entsteht, das durch Extraktion, Destillation und Kristallisation gereinigt wird. Die Ausbeuten betragen je nach Holzart 7–25 %. Dieses künstliche Vanille-Aroma auf Lignin-Basis besitzt ein reicheres Geschmacksprofil. Dies ist auf die Anwesenheit von Acetovanillon als Lignin-Folgeprodukt zurückzuführen – eine Verunreinigung, die in Vanillin aus einer Guajacolsynthese nicht auftritt.




Alternativ stehen verschiedene biotechnologische Methoden zur Verfügung. Vanillin kann beispielsweise durch "Amycolatopsis"- oder "Streptomyces"-Stämme aus Ferulasäure hergestellt werden. Die Ferulasäure kann ebenfalls biotechnologisch mit Hilfe von "Pseudomonas"-Stämmen aus Eugenol im Fed-Batch-Verfahren (Eugenol ist toxisch für die Zellen) hergestellt werden. Eugenol ist ein gut verfügbarer Rohstoff und stammt aus Nelkenöl. Ebenfalls dient Curcumin als Präkursor von Vanillin, mit Hilfe der Bakterien "Rhodococcus rhodochrous" wird dieses durch Biotransformation gewonnen. Möglich ist auch die Gewinnung aus Glucose durch genetisch modifizierte "Escherichia coli" Bakterien und nachfolgender Dehydrogenase.

Es kann auch über den Shikimisäureweg aus Hefekulturen hergestellt werden.

Vanillin als Produkt des Shikimisäurewegs (1). Endprodukte dieses Wegs chemischer Reaktionen sind die Aminosäuren Phenylalanin, Tyrosin und Tryptophan. Phenylalanin (2) wird biosynthetisch mit Hilfe des Enzyms Phenylalanin-Ammoniak-Lyase (PAL) unter Freisetzung von Ammoniak (NH) zu Zimtsäure (3) umgesetzt. Dies ist der erste Schritt in der Biosynthese der Phenylpropanoide.

Zwei wesentliche Wege sind in Diskussion, wie auf Basis von Phenylpropanoidverbindungen die Schritte zum Vanillin verlaufen: der "Ferulasäureweg" und der "Benzoatweg". Beide gehen zunächst von einer "p"-Hydroxylierung der Zimtsäure zur "p"-Cumarsäure (4-Hydroxyzimtsäure) (4) aus. Danach folgen drei Reaktionsschritte, deren Reihenfolge unterschiedlich ist, aber letztlich zum Zielmolekül führen.


Die Hydroxylierungen von 3 nach 4 und von 8 nach 9 werden vom Enzym Diphenolase katalysiert. Die Diphenolase wirkt bei letzterer Reaktion als Monophenoloxidase; diese Aktivität hat momentan eine unterschiedliche EC-Nummer (), es handelt sich aber um dasselbe Enzym.

Im Gegensatz zur chemischen Herstellung („naturidentisch“) darf das biotechnologisch hergestellte Vanillin als „natürlich“ deklariert werden.

Biosynthetisches Vanillin ist etwa 60-mal teurer als synthetisches (2015), aber immer noch billiger als natürliches.

Vanillin tritt in Form farbloser, charakteristisch süßlich riechender Nadeln auf, die an feuchter Luft allmählich zu Vanillinsäure oxidieren. Es löst sich schlecht in Wasser (10 g/l bei 25 °C) hingegen gut in Ethanol und Diethylether. Es schmilzt bei 82 °C und siedet bei 285 °C bei Normaldruck in einer CO-Atmosphäre bzw. 154 °C bei Unterdruck (13 hPa). Es kristallisiert im monoklinen Kristallsystem in der mit den Gitterparametern "a" = 1404,9 pm, "b" = 787,4 pm, "c" = 1501,7 pm, β = 115,45° und vier Formeleinheiten pro Elementarzelle.

Die Substanz leitet sich strukturell sowohl vom Benzaldehyd als auch vom Guajacol ("2-Methoxyphenol") ab. Infolge seines bifunktionalen Charakters ist Vanillin sehr reaktionsfreudig. Durch Veretherung, Veresterung oder Aldolkondensation sind sehr viele Derivate synthetisierbar. Durch Angriff am aromatischen Ring sind weitere Reaktionen möglich. Eine katalytische Hydrierung von Vanillin führt zu Vanillylalkohol bzw. zu 2-Methoxy-4-methylphenol. Vanillin kann enzymatisch zur Vanillinsäure oxidiert werden. Eine wässrige Lösung von Eisen(III)-chlorid bildet mit Vanillin eine blauviolette Färbung.

Der pK-Wert der phenolischen OH-Gruppe beträgt 7,40 (25 °C). Dieser Wert ist gegenüber dem Phenol mit 9,99 deutlich niedriger; die elektronenziehende Aldehydgruppe erhöht durch ihren −M-Effekt die OH-Acidität; die phenolische OH-Bindung wird zunehmend polarisiert. Der pK-Wert des 4-Hydroxybenzaldehyds bewegt sich bei einem ähnlichen Wert und beträgt 7,66; die fehlende Methoxygruppe macht hier kaum einen Unterschied aus. Zum Vergleich besitzt auch das Guajacol ("2-Methoxyphenol") mit seinem pK-Wert von 9,98 praktisch keinen Unterschied zum Phenol mit 9,99.

Isovanillin ("3-Hydroxy-4-methoxybenzaldehyd") ist ein Isomer und unterscheidet sich vom Vanillin durch die Stellung der Methoxygruppe. Anstatt an Position 3 ist diese hier an Position 4 vorzufinden. Hydroxy- und Methoxygruppe tauschen im Vergleich zum Vanillin die Plätze.

"ortho"-Vanillin ("2-Hydroxy-3-methoxybenzaldehyd") ist gleichfalls ein Isomer und unterscheidet sich vom Vanillin durch die Stellung der Hydroxygruppe. Die Vorsilbe "ortho-" kennzeichnet hier die Position der Hydroxygruppe im Substitutionsmuster bezüglich der Aldehydgruppe; im Vanillin befinden sich diese beiden Gruppen in "para-"Stellung.

Ethylvanillin ("3-Ethoxy-4-hydroxybenzaldehyd") ist ein struktureller Verwandter und unterscheidet sich vom Vanillin, indem man die Methylgruppe gegen eine Ethylgruppe austauscht. Es kommt nicht natürlich vor, sondern wird auf dem Syntheseweg hergestellt. Heute wird es oft als künstlicher Aromastoff anstelle des teureren Vanillins verwendet, da es zudem 2- bis 4-mal intensiver in Geschmack und Aroma ist.

Acetovanillon ("4-Hydroxy-3-methoxyacetophenon", auch "Apocynin") ist gleichfalls ein struktureller Verwandter und unterscheidet sich vom Vanillin, indem man die Aldehydgruppe gegen eine Acetylgruppe austauscht. Es entsteht in künstlichen Vanille-Aromen auf Lignin-Basis.

Vanillin und Ethylvanillin besitzen einen ähnlichen Geruch, der des Isovanillins ist hingegen kaum merkbar. Vanillin und Ethylvanillin lassen sich mit Gemischen aus Hexan und Essigsäureethylester per Dünnschichtchromatographie gut trennen.

Veratrumaldehyd ("3,4-Dimethoxybenzaldehyd"), auch Methylvanillin ist gleichfalls ein struktureller Verwandter mit der gleichen Summenformel.

Als Ersatzstoff wird auch Propenylguaethol (Vanitrope) verwendet, es besitzt ein typisches Vanillinaroma und dessen Geruch ist etwa 15-mal intensiver als der von Vanillin.

Die zuverlässige qualitative und quantitative Bestimmung von Vanillin in unterschiedlichen Untersuchungsmaterialien gelingt nach hinreichender Probenvorbereitung durch den Einsatz der Gaschromatographie oder HPLC gekoppelt mit der Massenspektrometrie.

Die Bestimmung von Vanillin kann auch zur Überprüfung der Güteklassen von Olivenöl als Markersubstanz herangezogen werden. Dieser analytische Einsatz zur Prüfung auf Authentizität in extrem fetthaltiger Matrix erfordert jedoch besondere Verfahren.

Vanillin ist mengenmäßig der wichtigste Aromastoff weltweit, nicht zuletzt, da er technisch preisgünstig hergestellt werden kann. Man geht von einem Verbrauch von etwa 15.000 Tonnen im Jahr aus (2004). Die rund 2.000 Tonnen Kapselfrüchte echter Vanille, die jährlich weltweit geerntet werden, enthalten aber nur etwa 40 Tonnen Vanillin (der Vanillingehalt einer handelsüblichen Vanille-Schote beträgt zwischen 1,6 und 2,4 % laut ISO-Norm 5565-1:1999). Über 99,7 % des in Verkehr gebrachten Vanillins sind also nicht natürlichen Ursprungs.

Vanillezucker ist eine Zubereitung mit dem natürlichen Vanillearoma von mindestens 1 g gemahlenen Vanilleschoten oder deren Extrakten auf 16 g Zucker (Saccharose), Vanillinzucker enthält die aromatisierende Beimengung von mindestens 0,17 g Vanillin auf die gleiche Zuckermenge. Vanillin wird als Aromastoff in verschiedenen Lebensmitteln verwendet, unter anderem in Speiseeis, Backwaren und Schokolade. Daneben ist Vanillin einer von vielen Duftstoffen bei der Parfümherstellung und zur Geschmacksverbesserung von Pharmazeutika und Vitaminpräparaten, wo er in kleinen Mengen zur Abrundung und Fixierung von süßen, balsamischen Düften verwendet wird.

Auch in der chemischen Industrie wird Vanillin verwendet, beispielsweise als Ausgangsstoff oder Zwischenprodukt bei der Synthese von verschiedenen Arzneistoffen, wie beispielsweise Levodopa, Methyldopa und Papaverin. Es ist außerdem Bestandteil von Günzburgs Reagenz – einer alkoholischen Lösung von Phloroglucin und Vanillin zum qualitativen Nachweis der freien Salzsäure im Magensaft.

Vanillin wird in der Histologie bei der Vanillin-HCl-Färbung zum Färben von Tanninen verwendet. Vanillin kann als Nachweisreagenz zur Derivatisierung von Verbindungen bei der Dünnschichtchromatographie verwendet werden. Dabei wird die entwickelte Platte durch Aufsprühen oder Tauchen mit einer Vanillin-Schwefelsäure-Lösung benetzt und erhitzt. Einige Verbindungen zeigen dabei charakteristische Farbreaktionen, anhand derer sie identifiziert werden können.





</doc>
<doc id="5455" url="https://de.wikipedia.org/wiki?curid=5455" title="Verflüssigung">
Verflüssigung

Verflüssigung ist das Überführen eines Stoffes in den flüssigen Aggregatzustand. Dies kann durch zwei Prozesse erfolgen:

Die Eigenschaften eines Materials verändern sich beim Phasenübergang von der festen oder gasförmigen zur flüssigen Phase.



</doc>
<doc id="5456" url="https://de.wikipedia.org/wiki?curid=5456" title="Viren">
Viren

Viren (Singular: "das" Virus, außerhalb der Fachsprache auch "der" Virus; lat. "" „Schleim“, „Saft“, „Gift“) sind infektiöse Partikel, die sich als Virionen außerhalb von Zellen (extrazellulär) durch Übertragung verbreiten, aber als Viren nur innerhalb einer geeigneten Wirtszelle (intrazellulär) vermehren können. Sie selbst bestehen nicht aus einer Zelle. Alle Viren enthalten das Programm (einige Viren auch weitere Hilfskomponenten) zu ihrer Vermehrung und Ausbreitung, besitzen aber weder eine eigenständige Replikation noch einen eigenen Stoffwechsel und sind deshalb auf den Stoffwechsel einer Wirtszelle angewiesen. Daher sind sich Virologen weitgehend darüber einig, Viren nicht zu den Lebewesen zu rechnen. Man kann sie aber zumindest als „dem Leben nahestehend“ betrachten, denn sie besitzen allgemein die Fähigkeit zur Replikation und Evolution.

Von den tatsächlichen Lebewesen sind bislang etwa 1,8 Millionen verschiedene rezente Arten bekannt, vermutlich existieren sehr viel mehr. Zu jeder Art könnte es mehrere Virenarten geben, die an diese Art angepasst sind. Bislang sind jedoch lediglich um die 3.000 Virenarten identifiziert worden (Virusklassifikation). Viren befallen Zellen von Eukaryoten (Pflanzen, Pilze, alle Tiere einschließlich des Menschen) und Prokaryoten (Bakterien und Archaeen). Viren, die Prokaryoten als Wirte nutzen, werden Bakteriophagen genannt; für Viren, die speziell Archaeen befallen, wird aber teilweise auch die Bezeichnung Archaeophagen verwendet.

Die Wissenschaft, die sich mit Viren und Virusinfektionen beschäftigt, ist die Virologie.

Erst seit dem späten 19. Jahrhundert sind Viren als eigene biologische Einheit bekannt. Die Beschreibungen von Viruskrankheiten sind aber sehr viel älter, ebenso die ersten Behandlungsmethoden. Aus Mesopotamien ist ein Gesetzestext aus der Zeit um 1780 v. Chr. überliefert, der von der Bestrafung eines Mannes handelt, dessen wahrscheinlich von Tollwut befallener Hund einen Menschen beißt und dadurch tötet (Codex Eschnunna §§ 56 und 57). Aus ägyptischen Hieroglyphen sind Darstellungen bekannt, die vermutlich die Folgen einer Polio-Infektion zeigen.

Die Bezeichnung „Virus“ wurde zum ersten Mal von Cornelius Aulus Celsus im ersten Jahrhundert v. Chr. verwendet. Er bezeichnete den Speichel, durch den Tollwut übertragen wurde, als „giftig“. Im Jahr 1882 führte Adolf Mayer bei Experimenten mit der Tabakmosaikkrankheit erstmals unwissentlich eine virale Erregerübertragung (Transmission) durch, indem er den Pflanzensaft infizierter Pflanzen auf gesunde Pflanzen übertrug und bei diesen so ebenfalls die Krankheit auslöste.

Diese Übertragung war bereits im 18. Jahrhundert mit dem Wort Virus assoziiert. So beschreibt die Londoner Times in einem Nachruf auf einen Arzt dessen Virusinfektion: Beim Zunähen einer sezierten Leiche hatte er sich in die Hand gestochen, (wobei ein wenig Virussubstanz übertragen wurde, oder anders gesagt, ihm wurde Fäulnis eingeimpft).

Dimitri Iwanowski wies unabhängig von Mayer im Jahr 1892 in einem Experiment nach, dass die Mosaikkrankheit bei Tabakpflanzen durch einen Stoff ausgelöst werden kann, der durch Filtration mittels bakteriendichter Filter (Chamberland-Filter) nicht entfernt werden konnte und dessen Partikel deshalb deutlich kleiner als Bakterien sein mussten.
Der erste Nachweis eines tierischen Virus gelang 1898 Friedrich Loeffler und Paul Frosch, die das Maul-und-Klauenseuche-Virus entdeckten (siehe auch virologische Diagnostik). Die Größe vieler Viren wurde in den 1930er Jahren durch William Joseph Elford mit Methoden der Ultrafiltration bestimmt.

Der bislang älteste – indirekte – Beleg für eine durch Viren verursachte Erkrankung wurde aus den deformierten Knochen eines 150 Millionen Jahre alten, kleinen zweibeinigen Dinosauriers ("Dysalotosaurus lettowvorbecki") abgeleitet, der im Berliner Museum für Naturkunde verwahrt wird und Symptome von Osteodystrophia deformans aufweist, die auf eine Infektion mit Paramyxoviren zurückgeführt werden.

Viren haben keinen eigenen Stoffwechsel, denn sie besitzen kein Zytoplasma, das ein Medium für Stoffwechselvorgänge darstellen könnte, und ihnen fehlen sowohl Ribosomen wie auch Mitochondrien. Daher können sie allein keine Proteine herstellen, keine Energie umwandeln und sich auch nicht selbst replizieren. Im Wesentlichen ist ein Virus also eine Nukleinsäure, auf der die Informationen zur Steuerung des Stoffwechsels einer Wirtszelle enthalten sind, insbesondere zur Replikation der Virus-Nukleinsäure und zur weiteren Ausstattung der Viruspartikel (Virionen). Die Replikation des Virus kann daher nur innerhalb der Wirtszelle erfolgen.

Viren kommen in zwei Erscheinungsformen vor:

Ein Viruspartikel außerhalb von Zellen bezeichnet man als Virion (Plural Viria, Virionen). Virionen sind Partikel, die aus Nukleinsäuren, und zwar entweder Desoxyribonukleinsäuren (DNA) oder Ribonukleinsäuren (RNA), und meistens aus einer Protein-Hülle (Kapsid) bestehen. Letzteres fehlt jedoch z. B. beim Influenzavirus, welches stattdessen ein Ribonucleoprotein aufweist. Einige Virionen sind zusätzlich von einer mit viralen Membranproteinen durchsetzten Lipiddoppelschicht umgeben, die als Virushülle bezeichnet wird. Viren, die vorübergehend bis zum Beginn der Replikationsphase zusätzlich zum Kapsid eine Virushülle aufweisen, werden als behüllt bezeichnet, Viren ohne derartige Hülle als unbehüllt. Einige Virionen besitzen andere zusätzliche Bestandteile.

Der Durchmesser von Virionen beträgt etwa 15 nm (beispielsweise Circoviridae) bis 440 nm (Megavirus chilensis). Virionen sind deutlich kleiner als Bakterien, jedoch etwas größer als Viroide, welche weder ein Kapsid noch eine Virushülle besitzen.

Das Proteinkapsid kann unterschiedliche Formen haben, zum Beispiel ikosaederförmig, isometrisch, helikal oder geschossförmig.

Serologisch unterscheidbare Variationen eines Virus nennt man Serotypen.

Virionen dienen der Verbreitung der Viren. Sie dringen ganz oder teilweise (mindestens ihre Nukleinsäure) in die Wirtszellen ein (infizieren sie) und die Virus-Nukleinsäure programmiert danach deren Stoffwechsel zur Vermehrung der Virus-Nukleinsäure und zur Produktion der anderen Virionen-Bestandteile um.

Viren sind im Wesentlichen bloße stoffliche Programme zu ihrer eigenen Reproduktion in Form einer Nukleinsäure. Sie besitzen zwar spezifische genetische Informationen, aber nicht den für ihre Replikation notwendigen Synthese-Apparat. Ob Viren als Lebewesen bezeichnet werden können, ist abhängig von der Definition von Leben. Eine allgemein anerkannte, unwidersprochene Definition gibt es bislang nicht. Die meisten Wissenschaftler stufen Viren nicht als Lebewesen ein – wobei die wissenschaftliche Diskussion noch nicht abgeschlossen ist, da beispielsweise bei der Genomgröße des Cafeteria-roenbergensis-Virus eine Abgrenzung anhand der Größe des Genoms zu verwischen beginnt.

Viren werden normalerweise auch nicht zu den Parasiten gerechnet, weil Parasiten Lebewesen sind. Einige Wissenschaftler betrachten Viren dennoch als Parasiten, weil sie einen Wirtsorganismus infizieren und seinen Stoffwechsel für ihre eigene Vermehrung benutzen. Diese Forscher definieren Viren als „obligat intrazelluläre Parasiten“ (Lebensformen, die immer Parasiten innerhalb einer Zelle sind), die mindestens aus einem Genom bestehen und zur Replikation eine Wirtszelle benötigen. Man kann sich – unabhängig von der Klassifizierung als Lebewesen oder Nicht-Lebewesen – darauf einigen, dass das "Verhalten" von Viren dem von gewöhnlichen Parasiten sehr ähnlich ist. Viren können wie Prionen, funktionslose DNA-Sequenzen und Transposons in diesem Sinne als „parasitär“ bezeichnet werden.

Ein Virus selbst ist zu keinen Stoffwechselvorgängen fähig, daher braucht es Wirtszellen zur Fortpflanzung. Der Replikationszyklus eines Virus beginnt im Allgemeinen, wenn sich ein Virion an ein Oberflächenprotein auf einer Wirtszelle anheftet (Adsorption), das vom Virus als Rezeptor verwendet wird. Bei Bakteriophagen erfolgt dies durch Injektion seines Erbmaterials in eine Zelle, bei Eukaryoten werden die Virionen durch Endozytose eingestülpt und durchdringen dann die Endosomenmembran, z. B. durch ein fusogenes Protein. Nach der Aufnahme muss ein Virion vor der Replikation erst von seinen Hüllen befreit werden (uncoating). Das Erbmaterial des Virus, seine Nukleinsäure, wird anschließend in der Wirtszelle vervielfältigt und die Hüllproteine sowie gegebenenfalls weitere Bestandteile der Virionen werden anhand der Gene des Virusgenoms ebenfalls von der Wirtszelle synthetisiert (Proteinbiosynthese/Genexpression).
So können in der Zelle neue Viren gebildet werden (Morphogenese), die als Virionen freigesetzt werden, indem entweder die Zellmembran aufgelöst wird (Zell-Lyse, lytische Virusvermehrung), oder indem sie ausgeschleust (sezerniert) werden (Virusknospung, budding), wobei Teile der Zellmembran als Bestandteil der Virushülle mitgenommen werden. Mit Hilfe von Immunoevasinen wird die Immunabwehr des Wirtes unterdrückt. Die Anzahl an neugebildeten Virionen einer infizierten Wirtszelle wird als burst size (engl. für ‚Berstgröße‘) bezeichnet.

Eine weitere Möglichkeit ist der Einbau des Virus-Genoms in das des Wirtes (Provirus). Dies ist der Fall bei temperenten Viren, wie zum Beispiel dem Bakteriophagen "Lambda".

Die Auswirkung der Virusvermehrung auf die Wirtszelle nennt man Zytopathischen Effekt (CPE). Es gibt verschiedene Arten des zytopathischen Effekts: Zelllyse, Pyknose (Polioviren), Zellfusion (Masernvirus, HSV, Parainfluenzavirus), intranucleäre Einschlüsse (Adenoviren, Masernvirus), intraplasmatische Einschlüsse (Tollwutvirus, Pockenvirus).

Die Verbreitungswege von Viren sind vielfältig. So können humanpathogene Viren zum Beispiel über die Luft in Form von Tröpfcheninfektion (z. B. Grippeviren) oder über kontaminierte Oberflächen durch Schmierinfektion (z. B. Herpes simplex) übertragen werden. Bei Pflanzenviren erfolgt die Übertragung häufig durch Insekten oder auch durch mechanische Übertragung zwischen zwei Pflanzen, bzw. über kontaminierte Werkzeuge in der Landwirtschaft. Eine abstrakte Sicht auf die epidemiologische Kinetik von Viren und anderen Krankheitserregern wird in der Theoretischen Biologie erarbeitet.

Der Ursprung der Viren ist nicht bekannt. Die meisten Forscher nehmen heute an, dass es sich bei Viren nicht um Vorläufer des zellulären Lebens handelt, sondern um Gene von Lebewesen, die sich aus Lebewesen lösten. Es werden noch immer mehrere Möglichkeiten diskutiert, wobei es im Prinzip zwei verschiedene Ansätze gibt:

Daraus abgeleitet sind drei Theorien formuliert worden.


Für eine Evolution eines Virus (bzw. irgendeines Gens) ist seine Variabilität und Selektion von Bedeutung. Die Variabilität ist (wie bei allen Organismen) durch Kopierfehler bei der Replikation des Erbgutes gegeben und dient unter anderem der Immunevasion und der Änderung des Wirtsspektrums, während die Selektion oft durch die (Immun)-Antwort des Wirtes durchgeführt wird.

Höher organisierte Lebewesen haben per Rekombination und Crossing-over bei der geschlechtlichen Fortpflanzung eine sehr effektive Möglichkeit der genetischen Variabilität besonders in Richtung einer Umweltanpassung und damit Weiterentwicklung ihrer jeweiligen Art entwickelt. Virionen beziehungsweise Viren zeigen als überdauerungsfähige "Strukturen", die für ihre Vermehrung und damit auch Ausbreitung auf lebende Wirte angewiesen sind, ohne geschlechtliche Fortpflanzung allein mit ihrer Mutationsfähigkeit eine mindestens ebenbürtige Möglichkeit für eine genetische Variabilität.

Dabei ist es dann letztlich unerheblich, dass diese Mutationen im Genom der Viren im Grunde zuerst auf Kopierfehlern während der Replikation innerhalb der Wirtszellen beruhen. Was zählt, ist allein der daraus für die Arterhaltung resultierende positive Effekt der extremen Steigerung der Anpassungsfähigkeit. Während Fehler dieser Art zum Beispiel bei einer hochentwickelten Säugetierzelle zum Zelltod führen können, beinhalten sie für Viren sogar einen großen Selektionsvorteil (siehe dazu Evolution).

Kopierfehler bei der Replikation drücken sich in Punktmutationen, also im Einbau von falschen Basen an zufälligen Genorten aus. Da Viren im Gegensatz zu den höherentwickelten Zellen nur über wenige oder keine Reparaturmechanismen verfügen, werden diese Fehler nicht korrigiert.

Sonderformen der genetischen Veränderung bei Viren werden beispielsweise bei den Influenza-Viren mit den Begriffen Antigendrift und Antigenshift (genetische Reassortierung) dort genau beschrieben.

Eine Infektion mit Viren erzeugt in ihren Wirten verschiedene Formen der Abwehrreaktion. Viren werden ausschließlich intrazellulär repliziert, denn sie verwenden zur Replikation die dafür notwendigen Bausteine und Enzyme aus dem Cytosol einer Wirtszelle. Daher sind verschiedene intrazelluläre Abwehrmechanismen entstanden, die als Restriktions- oder Resistenzfaktoren bezeichnet werden. Während Bakterien unter anderem das CRISPR und Restriktionsenzyme zur Abwehr von Bakteriophagen innerhalb einer Zelle verwenden, gibt es in Eukaryoten z. B. den Myxovirus-Resistenzfaktor Mx1, die PAMP-Rezeptoren, den dsRNA-aktivierten Inhibitor der Translation "DAI", das Melanom-Differenzierungs-Antigen 5 (MDA-5), die Oligoadenylatsynthase OAS1, das Langerin, das Tetherin, das ""-Protein (SAMHD1), das RIG-I, das APOBEC3, das TRIM5alpha, die Proteinkinase R und die RNA-Interferenz.

In Tieren und insbesondere in Wirbeltieren hat sich darüber hinaus eine Immunantwort herausgebildet, die sich adaptiv an verändernde Viren anpasst und eine Gedächtniswirkung besitzt. Bei einer viralen Infektion besteht die Immunantwort in Säugetieren aus einem angeborenen und einem erworbenen Anteil. Im Zuge der adaptiven Immunantwort entstehen Antikörper und zytotoxische T-Zellen, die an einzelne Bestandteile des Virus (Antigene) binden können und dadurch Viren und Virus-infizierte Zellen erkennen und beseitigen können.

Im Jahre 1962 wurde von André Lwoff, Robert W. Horne und Paul Tournier entsprechend der von Carl von Linné begründeten binären Klassifikation der Lebewesen eine Taxonomie der Viren eingeführt, die folgende Stufen umfasst (Muster für Namensendungen der Taxa in Klammern):

Damit einher geht eine Zuordnung in Gruppen, die sich an den Wirten orientieren

Die meisten Viren gehören nur zu einer der obigen vier Gruppen, doch Virusarten der Familie Rhabdoviridae und Bunyaviridae können sowohl Pflanzen als auch Tiere infizieren. Einige Viren vermehren sich nur in Vertebraten, werden jedoch auch von Invertebraten mechanisch übertragen (siehe Vektor), vor allem von Insekten. Viren, die auf die Nutzung von Genen anderer Viren („Mamaviren“) während der gemeinsamen Infektion einer Wirtszelle angewiesen sind, werden Virophagen genannt.

Das International Committee on Taxonomy of Viruses (ICTV) hat ein Klassifizierungssystem entwickelt, um eine einheuitliche Unterteilung in Familien zu gewährleisten. Der neunte ICTV-Report definiert ein Konzept mit Virus-Spezies als unterstem Taxon in einem hierarchischen Systemn sich verzweigender Viren-Taxa.

Die taxonomische Struktur ist im Prinzip wie bei der herkömmlichen Virusklassifikation (siehe oben). Es gibt in diesem Vorschlag keine Definition von Unterarten (Subspecies), Stämmen (im Sinn von Varietäten, wie 'Bakterienstamm', englisch: strain) oder Isolaten.

Mit Stand von 2013 wurden sieben Ordnungen vorgeschlagen:

Die vom Nobelpreisträger und Biologen David Baltimore vorgeschlagene Klassifizierung basiert darauf, in welcher Form genau das Virusgenom vorliegt und wie daraus die Boten-RNA (mRNA) erzeugt wird. Das Virus-Genom kann in der Form von DNA oder RNA vorliegen, Einzelstrang (englisch: single-stranded, ss) oder Doppelstrang (englisch double-stranded, ds). Ein Einzelstrang kann als Original (englisch: sense, +) oder in komplementärer Form (englisch: antisense, −) vorliegen. 
Unter Umständen wird zur Vervielfältigung ein RNA-Genom übergangsweise in DNA umgesetzt (Retroviren) oder umgekehrt ein DNA-Genom übergangsweise in RNA transkribiert (Pararetroviren); in beiden Fällen wird die RNA mit einer Reversen Transkriptase (RT) in DNA zurückgeschrieben.

Die gesamte Virosphäre wird folgenden sieben Gruppen definiert:


Moderne Virusklassifikationen benutzen eine Kombination von ICTV und Baltimore.

Der offizielle internationale, wissenschaftliche Name eines Virus ist die englischsprachige Bezeichnung, nach der sich stets auch die international gebräuchliche Abkürzung richtet, wie bei " Lagos bat virus" (LBV). Diese Abkürzung wird unverändert auch im Deutschen verwendet. Folgerichtig lautet die Abkürzung für die deutsche Virusbezeichnung "Lagos-Fledermaus-Virus" ebenfalls "LBV".

In den englischen Virusnamen wie zum Beispiel bei "West Nile virus " werden normalerweise keine Bindestriche benutzt und "virus" wird kleingeschrieben. Der Bindestrich taucht im Englischen nur bei Adjektiven auf, also bei "Tick-borne encephalitis virus" oder "Avian encephalomyelitis-like virus".

Im Deutschen werden die Virusnamen teilweise mit Bindestrichen geschrieben, also "West-Nil-Virus", "Hepatitis-C-Virus", "Humanes Herpes-Virus", "Lagos-Fledermaus-Virus", "Europäisches Fledermaus-Lyssa-Virus", teilweise auch zusammen. Vor den Nummern von Subtypen steht (wie im Englischen) ein Leerzeichen, bei den Abkürzungen ein Bindestrich, z. B. "Europäisches Fledermaus-Lyssa-Virus 1" (EBLV-1), "Herpes-simplex-Virus 1" (HSV-1) und "Humanes Herpes-Virus 1" (HHV-1).

Beim Menschen können eine Vielzahl von Krankheiten durch Viren verursacht werden. Allein diese "humanpathogenen Viren" sind hier hinsichtlich Genom und Behüllung klassifiziert und in ihrer Taxonomie nach ICTV aufgelistet.








Die Gruppe der „Onkoviren“, der wichtigsten beim Menschen krebserzeugenden (karzinogenen) Viren, ist weltweit für 10 bis 15 Prozent aller Krebserkrankungen des Menschen verantwortlich, nach Schätzung der amerikanischen Krebsgesellschaft sogar für etwa 17 % der Krebsfälle.

Da Viren beziehungsweise Virionen im Gegensatz zu Bakterien keine Zellen sind, können sie auch nicht wie solche abgetötet werden. Es ist lediglich möglich, eine virale Infektion und die Virusvermehrung durch Virostatika zu be- oder zu verhindern. Besonders die biochemischen Vermehrungsabläufe können von Virusart zu Virusart sehr unterschiedlich sein, was die Findung eines hemmenden oder unterbindenden Wirkstoffes erschwert.

Da die Vermehrung der Viren im Inneren von normalen Zellen stattfindet und sich dort sehr eng an die zentralen biochemischen Zellmechanismen ankoppelt, müssen die in Frage kommenden antiviralen Wirkstoffe entweder

Andererseits müssen diese gesuchten Wirkstoffe jedoch auch für den Körperstoffwechsel, den Zellverband und/oder den internen Zellstoffwechsel insgesamt verträglich sein, da sonst nicht nur beispielsweise die Virusvermehrung in den Zellen zum Erliegen kommt, sondern schlimmstenfalls auch das (Zell-)Leben des gesamten behandelten Organismus.

Weil diese Bedingungen sehr schwer zu vereinbaren sind, sind die bisher entwickelten antiviralen Medikamente auch oft mit schweren Nebenwirkungsrisiken verbunden. Diese Gratwanderung stellte die Medizin vor schwierige Aufgaben, die bislang meist ungelöst blieben.

Verschärft wird die Entwicklung von effektiven antiviralen Medikamenten außerdem durch die Resistenzentwicklung von Seiten der zu bekämpfenden Viren gegenüber einem einmal gefundenen, brauchbaren Wirkstoff, zu der sie auf Grund ihres extrem schnell ablaufenden Vermehrungszyklus und der biochemischen Eigenart dieser Replikation gut in der Lage sind.

Aktuell wird verstärkt an Therapien geforscht, bei denen Viren zur Heilung von Krankheiten eingesetzt werden. Diese Forschungen umfassen den Einsatz viraler Vektoren unter anderem als onkolytische Viren zur Bekämpfung von Tumoren, als Phagentherapie zur gezielten Infektion und Lyse von zum Teil antibiotikaresistenten Bakterien, als Impfstoff zur Prophylaxe und Therapie von Infektionskrankheiten, zur Erzeugung von induzierten pluripotenten Stammzellen oder zur Gentherapie von Gendefekten.






</doc>
<doc id="5457" url="https://de.wikipedia.org/wiki?curid=5457" title="Viroid">
Viroid

Viroide sind die kleinsten bekannten infektiösen Krankheitserreger, 80 bis 100-fach kleiner als die kleinsten Viren. Prione sind noch kleiner und infektiös, aber nicht autonom replizierend, da Prione im Zuge der normalen Proteinbiosynthese mit einer zellulären mRNA als Vorlage gebildet werden und somit nicht selbstreplizierend sind. Manche Viroide sind bedeutende Pflanzenpathogene der Landwirtschaft.

In einer im Jahr 2000 erschienenen Zusammenstellung der wichtigsten Meilensteine des vergangenen Millenniums in der Pflanzenpathologie hat die American Phytopathological Society Theodor Otto Dieners Entdeckung der Viroide im Jahr 1971 als eine der zehn wichtigsten Erreger-Entdeckungen des Millenniums anerkannt.

Zur Entdeckung des ersten Viroids führten Bestrebungen, den Erreger der Spindelknollensucht der Kartoffelpflanze zu isolieren und zu charakterisieren. Anfangs der 1920er Jahre wurde in den US-Bundesstaaten New York und New Jersey diese Erkrankung von Kartoffeln erstmals beobachtet.<ref name="DOI10.1094/PHI-I-2009-0804-01">R. A. Owens, J. Th. J. Verhoeven: "Potato Spindle Tuber." In: "Plant Health Instructor." 2009, .</ref> Die infektiöse Natur der Krankheit wurde 1923 von Eugene S. Schultz und Donald Folsom erkannt, doch blieb die besondere Natur des Erregers zunächst unbekannt und er wurde den Pflanzenviren zugeordnet.

Dem am Pioneering Laboratory for Plant Virology des "Agricultural Research Service" in Beltsville, Maryland tätigen Pflanzenpathologen Theodor O. Diener gelang 1971 die Isolierung und Charakterisierung des Erregers als eine zum damaligen Kenntnisstand ungewöhnlich kleine, proteinfreie und unabhängig replizierende RNA. Diener schlug den Namen "Viroid" – also Virus-ähnlich – für diese Erregergruppe vor.

In den folgenden Jahren klärten Diener und andere Gruppen viele der molekularen Eigenschaften von Viroiden auf. So konnten Stollar und Diener im Jahr 1971 mit immunologischen Versuchen zeigen, dass Viroide nicht doppelsträngige, sondern einzelsträngige RNA sind und dass keine Viroid-spezifischen RNA-DNA Hybride existieren. Sogo et al. untersuchten 1973 Viroid-RNA mit Hilfe der Elektronenmikroskopie und bestätigten die einsträngige Natur der RNA und die ringförmige Sekundärstruktur der RNA.

Heinz L. Sänger et al. konnten 1976 am Institut für Virologie der Justus-Liebig-Universität Gießen die Struktur der Viroide als kovalent geschlossene, einzelsträngige RNA-Ringe mit spezifischen Basenpaarungen innerhalb des RNA-Stranges nachweisen. Henco et al. wiesen helikal gewundene Abschnitte innerhalb des RNA-Ringes nach, mit denen die besondere energetische Stabilität der RNA erklärbar war. Im Jahr 1978 publizierten Hans Joachim Gross et al. die erste Nukleotidsequenz eines Viroids – des PSTVd. Seitdem werden Viroide mit der üblichen drei- oder vier-stelligen Abkürzung benannt, zu der ein „d“ zugefügt wird – also PSTVd für Potato Spindle Tuber Viroid.

Viroide bestehen nur aus einer ringförmig geschlossenen, einzelsträngigen Ribonukleinsäure (RNA). Im Gegensatz zu Viren besitzen Viroide keine zusätzlichen Proteine oder Lipide zur Verpackung in Form einer Hülle oder eines Kapsids. Viroide können sich nur innerhalb lebender Zellen vermehren (obligat intrazelluläre Parasiten), weshalb sie auch zusammen mit den Viren, Virusoiden und Prionen die Gruppe der subzellulären Erreger bilden. Für Viroide, zusammen mit Viroid-ähnlichen Satelliten-RNA, hat das International Committee for Taxonomy of Viruses (ICTV) eine eigene Ordnung von subviralen Erregern definiert.

Die RNA der Viroide ist zwischen 241 und 401 Nukleotide lang. Typischerweise wird von dieser genomischen RNA während der Vermehrung in der Zelle keine mRNA transkribiert und damit keine Viroid-eigenen Proteine hergestellt. Die Viroid-RNA besitzt eine eigene katalytische Aktivität unter anderem in Form eines Ribozyms. Der Mechanismus der RNA-Vermehrung und Wechselwirkung mit zellulären Bestandteilen ist ein Modell zum Studium der Funktionalität von RNA in der Zelle. Diese Mechanismen der Viroide werden von vielen als ein "molekulares Fossil" aus einer frühen Stufe der chemischen Evolution angesehen.

Die RNA der Viroide enthält viele komplementäre Bereiche, wodurch sich doppelsträngige, lineare Strukturen ausbilden, die im Elektronenmikroskop als circa 50 nm lange stäbchenförmige Strukturen beobachtet werden können.

Das Genom des PSTVd wird in fünf Bereiche unterteilt: "terminal left" (TL), "pathogenicity" (P), "central" (C), "variable" (V) und "terminal right" (TR). Innerhalb der "central"-Region liegt die konservierte Sequenz der "central conserved region" (CCR), in der ein UV-sensitives E-Schleifenmotiv mit ungewöhnlicher Basenpaarung liegt. Innerhalb der "central conserved region" liegt die stark konservierte RNA-Sequenz GAAAC, die in allen Viroiden vorkommt.

Bei einer Dichtegradientenzentrifugation liegen die Sedimentationskoeffizienten für PSTVd bei 6,7 ± 0,1 S (359 nt Länge), für CEVd bei 6,7 ± 0,1 S (371 nt Länge), für CCCVd-1s bei 5,9 S (247 nt Länge), für CCCVd-1l bei 6,3 ± 0,1 S (302 nt Länge), für CCCVd-2s bei 7,6 ± 0,1 S (494 nt Länge) und für CCCVd-2l bei 8,2 S (604 nt Länge). Unter der Annahme einer ellipsoiden Form der Viroide beträgt das Axialverhältnis (a/b) 20. Das partielle spezifische Volumen v der Viroide wurde als 0,53 cm / g bestimmt.

Im Gegensatz zu Viren codieren Viroide keine Proteine. Deshalb sind sie bei ihrer Replikation und ihrem Transport ausschließlich auf Enzyme der Wirtspflanze angewiesen. Dies unterscheidet Viroide grundlegend von Satelliten-Viren, die bei ihrer Replikation auf Helferviren angewiesen sind.

Die Replikation erfolgt per "rolling circle replication" durch die zelluläre RNA-Polymerase II, die sonst normalerweise DNA als Vorlage verwendet. Die Mutationsrate bei der Replikation von Viroiden ist die höchste unter allen Nukleinsäure-Replikons. Durch die Ungenauigkeit der Replikation kommen Viroide innerhalb eines Organismus als Quasispezies vor. Allerdings wirken sich viele Mutationen auf die Infektiosität und Pathogenität aus. Durch die "rolling circle replication" entsteht ein langer RNA-Strang aus mehreren seriellen Kopien des Genoms (ein Multimer), der vermutlich erst durch die zelluläre RNase (DCL1 und DCL4 hydrolysieren Pospiviroidae, DCL2 und DCL3 bei allen Viroiden) in die einzelnen Genome geschnitten werden (zum Monomer). Die Viroide aus der Gruppe der "Avsunviroidae" können sich zudem autokatalytisch hydrolysieren. Der Ringschluss der neu erzeugten Viroid-RNA erfolgt vermutlich durch eine zelluläre DNA-Ligase, die sonst normalerweise DNA als Vorlage verwendet. Bei Avsunviroidae erfolgt eine Ligation autokatalytisch.

Das Hepatitis-D-Virus (HDV) besitzt eine Ribozymaktivität ähnlich zu den Viroiden.

Der genaue Mechanismus, über den die Viroid-RNA bei den Pflanzen pathogen wirkt, ist bisher nicht bekannt. Es sind verschiedene Modelle vorgeschlagen worden (z .B. RNA silencing). Eine Überexpression von einer nichtinfektiösen shRNA mit einer bestimmten Sequenz des PSTVd führt zu ähnlichen Symptomen wie eine Infektion mit PSTVd. Zudem ist eine Teilsequenz des PSTVd komplementär zu bestimmten mRNA von Pflanzen. Während einer Infektion mit Viroiden werden im Zuge der pflanzlichen Abwehr des Viroids durch Dicer-artige und "AGO"-Proteine der Pflanze kurze RNA-Fragmente gebildet ("vd-sRNA"). Durch diese kurzen Fragmente wird die Resistenz der Wirtspflanze geschwächt.

Viroide werden mechanisch bzw. durch eine Schmierinfektion, insbesondere nach Verletzung, aber auch durch Pollen und Samen übertragen, wobei der Anteil der Übertragungswege unbekannt ist. Bei einer Koinfektion der Pflanze mit PSTVd und dem "potato leafroll virus" (PLRV, ein Polerovirus) oder dem "velvet tobacco mottle virus" (VTMoV, ein Sobemovirus) wird die Transmission von PSTVd verstärkt, vermutlich durch eine Verpackung des Viroids im Viruskapsid. Daneben wurde eine Transmission durch Röhrenblattläuse (Aphididae) oder Hummeln vermutet.<ref name="DOI10.1007/bf01976653">J. A. Bokx, P. G. M. Piron: "Transmission of potato spindle tuber viroid by aphids." In: "Netherlands Journal of Plant Pathology." 87, 1981, S. 31, .</ref> Die Ausbreitung innerhalb einer Pflanze erfolgt über Plasmodesmen. Die Infektion ist lebenslang (persistent).

Auf der Grundlage von Sequenzvergleichen bereits charakterisierter Viroid-RNA (derzeit 29 Arten und eine Vielzahl von Varianten) und ihrer unterschiedlichen katalytischen und strukturellen Eigenschaften werden die Viroide in zwei Virusfamilien und acht Gattungen eingeteilt. Der prototypische Vertreter der Pospiviroide ist das PSTVd und der für die Avsunviroide ist "Avocado sunblotch viroid" (ASBVd). Während Pospiviroide sich im Zellkern replizieren, entstehen Avsunviroide im Chloroplasten.



Viroide treten nur als Krankheitserreger von Gefäßpflanzen auf (Pflanzenpathogene). Über 30 Krankheiten von Gefäßpflanzen werden durch Viroide verursacht. Die Infektionen betreffen viele verschiedene Kulturpflanzen, wobei die resultierende Erkrankung je nach Umweltbedingung und Wirt-Erreger-Verhältnis entweder nicht oder nur milde ausbrechen oder aber zu schweren Schädigungen und Absterben der Pflanzen führen kann. Die wirtschaftlichen Schäden durch Viroide betreffen vorwiegend Kartoffelpflanzen, Tomaten, Zitrusfrüchte, Weintrauben und Zierpflanzen. In Tieren wurden Viroide bisher nicht entdeckt. Ein typisches Beispiel ist das Kartoffelspindelknollen-Viroid "Potato Spindle Tuber Viroid" (Abk. PSTVd), welches Kartoffeln, Tomaten und viele andere Pflanzenarten befällt und großen wirtschaftlichen Schaden anrichtet.

Die Identifikation von einzelnen Viroiden erfolgt über einen Northern Blot mit einer Hybridisierungssonde und zunehmend auch per RT-PCR und qRT-PCR. Die Therapie besteht aus Exklusion oder Eradikation der infizierten Pflanzen.

Die Tenazität der Viroide ist stammabhängig. PSTVd wird durch wiederholtes Einfrieren und Auftauen inaktiviert. Viroide sind generell empfindlich gegenüber Bleichmitteln, Chaotropen in höheren Konzentrationen und Denaturierung durch Erhitzen, wobei die Struktur der Viroide durch zweiwertige Magnesiumionen stabilisiert wird und die Denaturierungstemperatur konzentrationsabhängig um bis zu 30 °C steigt.




</doc>
<doc id="5458" url="https://de.wikipedia.org/wiki?curid=5458" title="Volkskunde">
Volkskunde

Volkskunde ist eine Kultur- und Sozialwissenschaft, die sich vorwiegend mit der Geschichte und Gegenwart von Erscheinungen der menschlichen Alltags- und Populärkultur beschäftigt. An deutschsprachigen Hochschulen wird das Fach auch geführt als "Europäische Ethnologie, Populäre Kulturen, Empirische/Vergleichende Kulturwissenschaft" oder als "Kulturanthropologie". Der Schwerpunkt liegt dabei im europäischen Kulturraum, wobei Prozesse wie Globalisierung oder Transnationalisierung den Blick über die Grenzen Europas hinaus notwendig machen; dabei ergeben sich Überschneidungen mit den weltweit forschenden Fachrichtungen beispielsweise der Völkerkunde und der Sozialanthropologie.

Die Volkskunde untersucht kulturelle Phänomene der materiellen Kultur (wie z. B. Arbeitsgeräte, Bräuche, Volkslieder) sowie die subjektiven Einstellungen der Menschen zu diesen. Die Arbeitsfelder des so genannten traditionellen Kanons (z. B. Brauch, Volkslied, Sage, Hausforschung etc.) mit ihrem Fokus auf ländliche Bevölkerungsschichten standen lange im Mittelpunkt volkskundlicher Forschung. Seit ihrer Neuorientierung in den 1960er- und 1970er-Jahren versteht sich die Volkskunde als eine Kulturwissenschaft, die Kultur in einem weiten und dynamischen Sinn als den gesamten Lebenszusammenhang einer bestimmten (sozialen, religiösen oder ethnischen) Gesellschaft oder gesellschaftlichen Gruppe versteht. Durch ihre Quellenvielfalt (empirische Methoden, Bildanalyse, Objektanalyse, schriftliche Quellen) kann so der räumliche, soziale und historische Kontext stets mit berücksichtigt werden.

Aufgrund der Fülle an Kulturphänomenen gibt es eine große Anzahl volkskundlicher Arbeitsfelder: "Arbeiter-, Bild-, Brauchforschung, Erzähl-, Familien-, Gemeinde- und Stadt(teil-)forschung, Geräte-, Geschlechter-" (bzw. "Frauenforschung"), "Interethnische Forschung, Kleidungs-" (ursprünglich "Trachtenforschung"), "Leser- und Lesestoff-Forschung, Lied- und Musikforschung, Medien-, Medialkultur-, Nahrungsforschung, Reise- und Tourismusforschung, Volksfrömmigkeits-" sowie "Volksschauspielforschung". Weitere Schwerpunkte sind u. a. "Bodylore", "Interkulturelle Kommunikation", "Rechtliche Volkskunde", "Wohnen und Wirtschaften" sowie Museologie und Sachkulturforschung.

Museen stellen nach wie vor eines der wichtigsten volkskundlicher Arbeitsfelder dar. Die Forschungsergebnisse werden dabei in einigen Museumsarten entweder als Schwerpunkte präsentiert u. a. in Volkskundemuseen, Freilichtmuseen, Heimatmuseen, Bauernhofmuseen oder bilden einen wichtigen Bestandteil z. B. in vielen Regional-, Landes- und Nationalmuseen.

Meist von Problemen der Gegenwart ausgehend, ohne sich jedoch auf solche zu beschränken, thematisiert sie Kulturkontakte, -entwicklungen oder -strömungen und geht dabei sowohl empirisch als auch hermeneutisch vor. Die Beschäftigung mit Fragen des beschleunigten Wissenstransfers, der gesellschaftlichen Mobilität, der Multikulturalität und des Kulturtransfer sowie der Migration, Integration und Ausgrenzung sind einige Beispiele für moderne Forschungsthemen.

Wichtige Nachbardisziplinen der Volkskunde sind im gegenständlichen Bereich Literatur-, Kunst- und Musikwissenschaft; bezüglich der Betrachtungsweise Kultur-, Alltags-, Sozial- und Wirtschaftsgeschichte, Geographie, Kultursoziologie und Sozialpsychologie; hinsichtlich des Forschungsziels Ethnologie, Kulturanthropologie sowie, teilweise, die Politikwissenschaft.

Mit der Vielfalt der Forschungsfelder geht ein methodenpluralistischer Ansatz einher. Dieser umfasst die archivalische Quellenforschung und die Analyse materieller Kultur ebenso wie die Bildforschung, die Foto- und Filmanalyse, sowie die Diskurs- und die Medienanalyse. Als Wissenschaft mit vor allem empirischer Vorgehensweise, verwendet sie außerdem qualitative Methoden, wie die Feldforschung und die Teilnehmende Beobachtung sowie wissenschaftliche Interviews, wie das narrative Interview oder Oral History.

Als zur Zeit des Humanismus in Deutschland die "Germania" des Tacitus von Gelehrten wiederentdeckt wurde, begann man sich auch für die Lebensumstände des „einfachen Volkes“ zu interessieren, indem man die Inhalte seines Werkes mit der Gegenwart verglich. Wie viele andere geisteswissenschaftliche Fächer, entstand auch die Volkskunde aus den am Beginn der Moderne maßgeblichen Strömungen Aufklärung und Romantik. Im Zusammenhang mit der Aufklärung entstand um 1750 die Kameralistik, Statistik und Staatenkunde. Sie sah ihre Aufgabe in einer umfassenden Landesbeschreibung, die dem absolutistischen Herrscher detailliertes Wissen über dessen Länder und Bevölkerung im Sinne der bestmöglichen Regierbarkeit und Optimierung der Wirtschaftlichkeit liefern sollte. Im Umkreise der Statistik kam um 1780 die Bezeichnung Volks- und Völkerkunde erstmals auf – die frühste belegbare Begriffserwähnung stammt aus der Hamburger Zeitschrift "Der Reisende" von 1782 – beide Begriffe wurden anfangs als Synonym verwendet. Nachhaltig prägend wirkte die Romantik, deren Suche nach Natürlichem, Authentischem und Nationalem eine intensive Auseinandersetzung mit der eigenen Geschichte und Vergangenheit forderte. Hierauf fußt das frühe Interesse beispielsweise an Mythologie, Poesie, Märchen, Sagen oder Volksliedern, wobei Johann Gottfried Herder theoretische Grundlagen und Konzepte lieferte. Wichtige Vertreter dieser Phase sind beispielsweise Achim von Arnim, Clemens Brentano oder die Brüder Grimm.

So verstanden ist die Volkskunde sowohl ein Produkt als auch ein Symptom der Moderne: Die durch die Industrialisierung beschleunigten und oft als Bedrohung empfundenen gesellschaftlichen und kulturellen Veränderungen führten zu einer Beschäftigung mit scheinbar stabilen Elementen in der Kultur, die man hauptsächlich im ländlichen Milieu zu finden glaubte.

Ab der Mitte des 19. Jahrhunderts begann sich das Fach zu institutionalisieren: 1852 rief Hans von und zu Aufseß das Germanische Nationalmuseum in Nürnberg für kulturgeschichtliche Sammlungen des Mittelalters sowie der frühen Neuzeit ins Leben. Sechs Jahre später (1858) begann Wilhelm Heinrich Riehl sich für eine „Volkskunde als Wissenschaft“ stark zu machen. Gut drei Jahrzehnte darauf (1889) gründet Rudolf Virchow in Berlin das (spätere) "Museum für Deutsche Volkskunde", das heute Museum Europäischer Kulturen heißt; im Jahr darauf (1890) gründete Karl Weinhold, ebenfalls in Berlin, den ersten Verein für Volkskunde, der ab 1891 die "Zeitschrift für Volkskunde" herausgab. Weitere Vereine und Museen entstanden in Österreich, Bayern und der Schweiz. Im Jahr 1919 wurde die Volkskunde schließlich zu einem universitären Lehrfach. Otto Lauffer erhielt den ersten volkskundlichen Lehrstuhl im Deutschen Reich an der Universität Hamburg, aber der erste (damals noch unbezahlte) Professor für Volkskunde im deutschen Sprachraum wurde 1931 Viktor von Geramb an der Karl-Franzens-Universität in Graz.

Grundsätzliche Fragen – zum Beispiel nach einer Definition für "Volk" oder nach der Entstehung volkstümlicher Kulturgüter – wurden erstmals 1900 in Basel von Eduard Hoffmann-Krayer, John Meier und anderen erläutert. Anfang der 1920er Jahre formulierte Hans Naumann seine darauf aufbauende Theorie vom "gesunkenen Kulturgut" und "primitiven Gemeinschaftsgut". Wie Hoffmann-Krayer vertrat Naumann eine Zweischichtentheorie – anders als jener glaubte er jedoch, dass wesentliche Erscheinungsformen kulturellen Lebens stets von gehobenen sozialen Schichten geschaffen und von niedrigeren lediglich übernommen werden.

Auf dem Feld der Erzählforschung war die Finnische Schule für die erste Jahrhunderthälfte tonangebend. Die Kulturraumforschung konnte sich ab 1926 vom Rheinland aus in großen Teilen des deutschen Sprachraums etablieren. Ende der 1920er Jahre bereicherte die Schwietering-Schule mit ihrer soziologisch-funktionalistischen Betrachtungsweise die Volkskunde. Eine eher psychologische Herangehensweise vermittelte Adolf Spamer von 1936 an in Berlin.

In der Zeit des Nationalsozialismus wurde eine rassistisch und volkserzieherische Volkskunde, die ihren Anspruch auf Wissenschaftlichkeit völlig verlor, zur dominierenden Lehre. Ältere Vorstellungen eines dauerhaften, in Rasse und Lebensraum wurzelnden National- und Stammescharakters, wie sie unter anderem von Martin Wähler vertreten wurden, kamen dieser Instrumentalisierung entgegen. Nach Ende des Zweiten Weltkriegs wurde vor allem von soziologischer Seite die Forderung laut, dem Fach seine Eigenständigkeit abzuerkennen.

Eine neue Hoffnung brachte jedoch bereits 1946 Richard Weiss’ "Volkskunde der Schweiz" mit sich, und zwar aufgrund seiner (für die damalige Zeit überaus beispielhaften) psychologisch-funktionellen Sichtweise. In der Bundesrepublik Deutschland und ebenso in Österreich tat man sich in der Folgezeit ungeachtet dessen schwer, die Instrumentalisierung des eigenen Faches durch die Nationalsozialisten kritisch zu reflektieren. Nicht zuletzt deshalb erschien es einzelnen Instituten wichtiger, den Gegenstandsbereich der Volkskunde neu zu definieren bzw. zu ergänzen. So stellte Hermann Bausinger in seiner 1961 publizierten Arbeit "Volkskultur in der technischen Welt" das Selbstverständnis des Faches als Erforschung vor allem bäuerlicher Traditionen und Kulturinhalte in Frage. Insbesondere sei der Begriff „Volkskultur“ zu hinterfragen, da er eine scheinbar unveränderliche, ursprüngliche Kultur postuliere. Im Anschluss an Bausingers Kritik entwickelten sich neue Forschungsansätze und -schwerpunkte, die vor allem den Bereich der zeitgenössischen Alltagskultur in den Fokus brachten. Konrad Köstlin kritisierte allerdings, dass diese „moderne Volkskunde“ in vielen Fällen lediglich eine idealisierende Darstellung der Arbeiterschicht (als Träger der Volkskultur) gebracht hätte, während man andererseits den „alten“ Volkskundlern vorwerfe, die bäuerliche Kultur idealisiert zu haben – die isolierte Betrachtungsweise, so Köstlin, sei aber in beiden Fällen die gleiche. 

Im Jahr 1970 fand die Arbeitstagung der "Deutschen Gesellschaft für Volkskunde" (DGV) in Falkenstein ("Falkensteiner Tagung") statt, hierbei wurde kritisch über Theorien, das Selbstverständnis, die Fachgeschichte und bislang tragende volkskundliche Grundbegriffe wie Volk, Stamm, Gemeinschaft, Tradition, Kontinuität und Sitte diskutiert, mit dem Ergebnis einer Neupositionierung und eines Paradigmenwechsels: Man lehnte das damalige Verständnis von Volkskultur ab und wollte stattdessen stärker gegenwartsbezogen forschen und sich soziokulturellen Problemen widmen. Zudem bildeten sich zwei Positionen bezüglich des wissenschaftlichen Umgangs mit dem Begriff Kultur. Die Fachvertreter des ehemaligen Instituts für Volkskunde in Tübingen, das zu diesem Zeitpunkt bereits in das "Institut für empirische Kulturwissenschaft" umbenannt worden war, plädierten für die Soziologie als neue Leitdisziplin. Die Vertreter des Institutes in Frankfurt am Main hingegen betonten die inhaltliche Nähe der Volkskunde zu ethnologischen Disziplinen wie der Ethnologie (Völkerkunde) und der angelsächsischen "Cultural Anthropology". Mehrheitlich schloss man sich der ersten Gruppe an, innerhalb derer Kultur nun primär als Regulationsmodell des Alltags verstanden wird. Manifestiert hat sich diese Diskussion in der (im Übrigen bis heute andauernden) Debatte darüber, wie das Fach neu zu benennen sei, um solchermaßen auch nach außen hin ein Signal der selbst verordneten Neuorientierung zu setzen. Institutsumbenennungen waren die Konsequenz: Berlin, Freiburg und Marburg entschieden sich für "Europäische Ethnologie," Frankfurt am Main für "Kulturanthropologie," Göttingen für "Kulturanthropologie/Europäische Ethnologie," Tübingen für "Empirische Kulturwissenschaft," Regensburg für "Vergleichende Kulturwissenschaft". Andernorts beließ man es bei dem alten Namen oder wählte eine Doppelbezeichnung, zum Beispiel "Volkskunde/Europäische Ethnologie" in München und Münster, "Volkskunde/Kulturgeschichte" in Jena, "Europäische Ethnologie/Volkskunde" in Innsbruck, Würzburg und Kiel, "Kulturanthropologie/Volkskunde" in Mainz sowie "Volkskunde und Kulturanthropologie" in Graz. Derzeit gibt es 28 Universitätsinstitute im deutschen Sprachraum (Stand: 2005). Die Deutsche Gesellschaft für Volkskunde "(DGV)," die 1963 in Marburg im Sinne der Volkstumsforschung gegründet wurde, führt nach eigenen Angaben die Arbeit des "Verbandes der Vereine für Volkskunde" (gegründet 1904) fort.

Die Volkskunde wird an deutschsprachigen Hochschulen bisher als eigenständiges Fach auch unter den Namen Europäische Ethnologie oder Kulturanthropologie geführt und untersucht das Andere in der eigenen (deutschen bzw. europäischen) Kultur. Betont werden bei einer "volkskundlichen" Herangehensweise Phänomene der Alltagskultur. Der Schwerpunkt liegt dabei im europäischen Raum, wobei Prozesse wie Globalisierung oder Transnationalisierung den Blick über die Grenzen Europas hinweg notwendig gemacht und zu einer größeren Schnittmenge mit der Ethnologie geführt haben. Diese bis heute anhaltenden inhaltlichen wie methodischen Annäherungen haben in den letzten Jahren zu Debatten um die Demarkationslinien der sozial- und kulturwissenschaftlichen Fächer geführt.

Anders als die Bezeichnung "Europäische Ethnologie" vermuten lässt, ist das Fach jedoch bis heute ausschließlich im deutschen Sprachraum verankert. Der griechische Volkskundler und Philologe Nikolaos Politis (1852–1921) hat den Neologismus "Laographie" (von : Folkloristik) geprägt. Er entspricht in etwa der deutschen Volkskunde als Integrationsbegriff der Kulturforschung. Die Folkloristik wird im griechischen Sprachraum u. a. als Studium kleiner Gruppen von Menschen in ihrer natürlichen Umgebung begriffen (vgl. Ethnographie) und untersucht Sitten und Bräuche als prägend für einen Ort und seine Kultur.

Gegenwärtige beschäftigen sich Vertreter des Faches mit folgenden Themen, die auch in Kommissionen der "Deutschen Gesellschaft für Volkskunde" repräsentiert sind (Stand: 2017):


Einführungen

Aktuelle Diskussion zur Orientierung des Faches

Atlanten

Nachschlagewerke

Periodika

Zeitschriften



</doc>
<doc id="5459" url="https://de.wikipedia.org/wiki?curid=5459" title="VGA">
VGA

VGA steht als Abkürzung in der Technik für:
VGA steht weiterhin für folgende Organisationen und Unternehmen:
Die Abkürzung VgA steht für:
Die Abkürzung vGA steht für:
Siehe auch:


</doc>
<doc id="5460" url="https://de.wikipedia.org/wiki?curid=5460" title="Vorbenutzung">
Vorbenutzung

Die Vorbenutzung im Patentrecht regelt in mehreren Länderm, beispielsweise Deutschland und Österreich, das Recht eines Benutzers einer technischen Erfindung, der diese, unveröffentlicht, vor Anmeldung bzw. dem Prioritätstag eines Patentes oder eines Gebrauchsmusters bereits genutzt hat. Dieser Vorbenutzer verliert durch das Patent nicht das Recht den Gegenstand herzustellen, zu besitzen oder in Verkehr zu bringen. Eine Vorbenutzung kann im Patentregister verbüchert werden. 

Allerdings ist dieses Recht nicht veräußer-, aber vererbbar. Dies ist insbesondere dann problematisch, wenn an einem Vertriebsweg mehrere Partner beteiligt sind, von denen lediglich einer den Status eines Vorbenutzers hat, da auch der Transport, das Feilhalten und Verkaufen einen Eingriff in ein bestehendes Schutzrecht darstellen, wobei die gängige Rechtsprechung in einzelnen Ländern stark unterschiedlich ist.

In der Praxis ist der Nachweis einer Vorbenutzung meist schwierig, hinzu kommt, dass Änderungen am Gegenstand nur möglich sind, wenn sie das entsprechende Patent nicht verletzen.

Eine offenkundige Vorbenutzung besteht dann, wenn ein Teil der Öffentlichkeit, vor der Anmeldung bzw. dem Prioritätstag von der Vorbenutzung informiert wurde, also beispielsweise eine Führung durch das Firmengelände veranstaltet wurde, bei der die Details offenbart wurden. Wenn es sich hierbei lediglich um Geschäftspartner oder Angestellte handelt, welche zur Geheimhaltung verpflichtet sind, ist die Vorbenutzung nicht offenkundig. 

In den meisten Ländern bewirkt eine offenkundige Vorbenutzung, sei es durch den Anmelder selbst oder durch einen dritten vorgenommen, dass der offenkundig vorbenutzte Teil zum Stand der Technik gehört und nicht mehr patentierbar ist.


</doc>
<doc id="5461" url="https://de.wikipedia.org/wiki?curid=5461" title="Vorname">
Vorname

Der Vorname einer Person ist der Teil des Namens, der nicht die Zugehörigkeit zu einer Familie ausdrückt, sondern individuell identifiziert.

Die Vornamen eines Menschen werden nach seiner Geburt von seinen Eltern bestimmt. In manchen Ländern, so in den deutschsprachigen Ländern, gibt es Reglementierungen, die die Freiheit der Wahl des Vornamens einschränken.

Im Deutschen und in den meisten anderen europäischen Sprachen stehen die Vornamen (als individuelle Namen) "vor" dem Familiennamen (von regionalen Ausnahmen abgesehen), während beispielsweise im Ungarischen, Vietnamesischen, Chinesischen, Japanischen oder Koreanischen der von den Eltern bestimmte individuelle Name "hinter" dem Familiennamen steht. Im Deutschen bezeichnet man als "Rufnamen" den- oder diejenigen Vornamen, unter denen eine Person angesprochen wird.

Im anglo-amerikanischen Sprachraum sind Zwischennamen gebräuchlich, die auch "Mittelnamen (middle names)" genannt und meistens mit dem Anfangsbuchstaben abgekürzt werden "(middle initials)". Auch im Ostfriesischen gibt es Zwischennamen. Im Russischen steht der Vatersname zwischen dem Vor- und dem Familiennamen.

In vielen Personennamenssystemen dient der Vorname innerhalb einer Familie zur Unterscheidung der Familienmitglieder; im Unterschied zum Familiennamen, der die Zugehörigkeit zu einer Familie ausdrückt.

Die Namensgebung im deutschen Sprachraum ist von germanischen, lateinischen und christlich-religiösen Traditionen bestimmt. So wurden lange bevorzugt die Namen christlicher Heiliger oder biblische Namen vergeben.

In vielen asiatischen und afrikanischen Kulturen wird ähnlich wie in Süddeutschland oder Ungarn zuerst der Familienname und danach erst der individuelle Eigenname des Familienmitglieds genannt, sodass der Ausdruck „Vorname“ in diesen Namenssystemen eigentlich nicht passt.

In einigen Ländern wird zusammen mit Vornamen und Familiennamen auch ein Vatersname benutzt, zum Beispiel im Russischen, wo der Vatersname zwischen den beiden Elementen steht. Der Vatersname ist vom Vornamen des Vaters abgeleitet. Als Rufname dient im Russischen oft die Kombination von Vor- und Vatersnamen, zum Beispiel "Iwan Wassiljewitsch".

Aus vielen Vornamen haben sich im Lauf der Zeit Familiennamen entwickelt. Andererseits leiten sich auch manche Vornamen von gebräuchlichen Familiennamen ab. Die wissenschaftliche Disziplin der Namenforschung beschäftigt sich mit der Bedeutung, Herkunft und Verbreitung von Namen.

Die Wahl des Vornamens hängt von der Muttersprache und vom Geschlecht des Kindes ab. Es gibt jedoch eine Anzahl weiterer Einflussfaktoren, zum Beispiel familiäre, nationale oder regionale Traditionen, Gebräuche oder zeitbedingte Vorlieben. Oft wird von den Namensgebern ein Name mit möglichst „passender“ Bedeutung oder Anmutung gewählt, der die Eigenschaften des Kindes, die Wünsche oder Erwartungen des Umfelds oder auch eine politische oder weltanschauliche Programmatik transportiert. Auch die Benennung nach bekannten Persönlichkeiten, Idolen, Verwandten oder Vorbildern innerhalb und außerhalb des eigenen Familienkontextes ist häufig. Faktoren wie der Wohlklang (Euphonie) eines Namens oder seine Originalität, mit der sich die Individualität des Namensträgers unterstreichen lässt, spielen ebenfalls eine Rolle, die je nach Kultur und Epoche unterschiedlich stark einwirkt. Auch die Erfahrungen mit dem eigenen Namen sind als Motiv bei der Namensvergabe von Bedeutung.

Vornamen sind schon seit früheren Zeiten in Verwendung. Der Begriff „Vorname“ mag aber Verwirrung stiften, da eine Person mehrere Vornamen besitzen kann, der VOR-Name und einen NACH-Namen voraussetzt. Die Bezeichnung „Rufname“ ist also vielleicht für die Zeit der Einnamigkeit geeigneter, da bis ins Mittelalter im deutschen Sprachraum nur ein einziger Name üblich war. Allenfalls gab es einen individuellen Beinamen zur Unterscheidung, woraus sich, zusammen mit den Übernamen, die heutigen vererbten Familiennamen entwickelten, die aber in der Realität noch längere Zeit durch Veränderungen wechseln konnten.

Die germanischen Rufnamen waren bis zum 4. Jahrhundert nach dem Prinzip aufgebaut, zwei Namenglieder sinnvoll zu verbinden; z. B.: "Gud-run, Sieg-run" ("run" = Zauber, Geheimnis), "Ger-hart, Ger-not" ("ger" = Speer, "hart" = hart/streng). Viele Namensteile waren nur einseitig verwendbar, das heißt, sie waren entweder nur als Erstglied (z. B. "man") oder nur als Zweitglied (z. B. "run") in Gebrauch. Etliche von ihnen können sowohl als Vorder- wie auch als Hinterglied des zusammengesetzten Namens fungieren (z. B. "her" und "bert" wie in "Walt-her", "Her-bert", "Bert-hold"). Außerdem wohnte manchen Namenglieder nur ein Geschlecht inne, wohingegen andere sowohl für weibliche als auch für männliche Namen verwendet werden konnten (z. B. "Sieg" in "Sieglinde" und "Siegfried"). Die anfänglich inhaltliche Wichtigkeit hielt sich aber nicht; mit der Zeit wurde der Rufname mit mehr Augenmerk auf Wohlklang und Abstammung gewählt.

Nicht-germanische Namen waren, nach der Römerzeit des Südens, erst ab dem 7./8. Jahrhundert wirklich präsent; man findet in dieser Zeit vorwiegend Namen, die der Bibel entlehnt sind; z. B. "Christian, Elisabeth" oder "Daniel" etc.

Im 12. Jhdt. (dem „christlichen“ Mittelalter) waren Namen aus dem Neuen Testament verbreitet, die dem Deutschen oft angepasst oder verkürzt wurden, z. B.:

Auch Heiligennamen breiteten sich zu dieser Zeit vom Westen und Süden nach Norddeutschland aus, wobei dies von den Verehrungsgebieten abhing, da, je nach Region, bestimmten Heiligen mehr Wichtigkeit beigemessen wurde; z. B.: "Benedikt, Andreas, Elisabeth, Florian, Anton(ius)."

Mit der Renaissance fanden unter dem Einfluss des Humanismus griechische und lateinische Namen aus der Antike Eingang in die deutsche Namenwelt wie "Hektor", "Agrippa", "Claudius", "Julius", "Augustus". Hohenzollernfürsten hießen zu dieser Zeit "Albrecht Achilles", "Albrecht Alcibiades", "Johann Cicero". Vornamen wie Nachnamen von Gebildeten wurden gewöhnlich latinisiert wie beispielsweise "Henricus", "Martinus", "Joachimus". Humanisten der damaligen Zeit waren auch am germanischen Altertum interessiert und verbreiteten somit Namen wie "Hildebrand", "Hartmann" oder "Reinhold".

Die Reformation führte zu einem allgemeinen Rückgang im Gebrauch von Heiligennamen und es wurden bis in das 18. Jahrhundert alttestamentliche Namen wie "Benjamin", "Jonas", "Daniel", "David", "Rebekka" oder "Martha" bevorzugt. Auf katholischer Seite bestimmte dagegen der 1566 erstmals herausgegebene Catechismus Romanus, dass man (weiterhin) Namen von Heiligen wählen sollte. Eine ebensolche Empfehlung findet sich im 1614 erschienen Rituale Romanum. Bestimmte Namen entwickelten sich dadurch zu ausgesprochen katholischen Vornamen wie "Ignaz" / "Ignatius", "Vincenz", "Xaver", "Franz", "Josef", "Maria". Maria entwickelte sich auch zu einem beliebten zweiten Vornamen bei Männern. Vielleicht bekanntestes Beispiel dafür ist Rainer Maria Rilke.

Zu den meistverbreiteten Vornamen in Deutschland zählten im 16. Jahrhundert: Johann/Johannes, Georg, Heinrich, Hans, Christoph, Friedrich, Philipp, Wilhelm, Andreas, Jakob/Jacob, Joachim, Hermann, Martin, Michael, Ludwig, Peter, Caspar, Paul, Anton, Christian; Anna, Maria/Marie, Elisabeth, Katharina/Catharina, Dorothea, Agnes, Magdalene/Magdalena, Sophie, Christine/Christina, Hedwig, Sibylle/Sibylla, Sophia, Barbara, Margarete/Margaretha, Johanna, Eleonore, Ursula, Charlotte, Eva.

Im 17. und 18. Jahrhundert wurden dann auch französische Vornamen (z. B. "Charlotte, Babette") sowie englische (z. B. "Alfred, Edith") vergeben, die aber erst im 20. Jahrhundert im deutschsprachigen Raum an Beliebtheit zunahmen.

Die calvinistische Vorliebe für alttestamentliche Namen überdauerte das 18. Jahrhundert nicht und während dieses Jahrhunderts entwickelte sich dort eine Vorliebe für deutsche Namensbildungen mit christlichem Anklang, wie beispielsweise "Gottfried", "Gotthold", "Gotthelf/Gotthilf", "Fürchtegott" oder "Liebfried".

Zu den meistverbreiteten Vornamen in Deutschland zählten im 17. Jahrhundert: Johann, Friedrich, Heinrich, Georg, Christian, Christoph, Wilhelm, Ludwig, Ernst, Philipp, Karl/Carl, Franz, Joachim, Hans, Anton, August, Otto, Adam, Hermann, Andreas, Bernhard; Maria/Marie, Anna, Elisabeth, Sophie, Dorothea, Charlotte, Katharina/Catharina, Eleonore, Amalie/Amalia, Christine, Magdalena, Luise, Henriette, Hedwig, Johanna, Juliane, Sibylla, Sophia, Wilhelmine, Barbara.

Zu den meistverbreiteten Vornamen in Deutschland zählten im 18. Jahrhundert: Johann, Karl/Carl, Friedrich, Georg, Wilhelm, Heinrich, Christian, Franz, Ludwig, August, Ernst, Joseph, Ferdinand, Philipp, Anton, Gustav, Christoph, Hans, Peter, Otto; Maria/Marie, Caroline/Karoline, Luise/Louise, Charlotte, Sophie, Anna, Friederike, Henriette, Amalie, Elisabeth/Elise, Johanna, Wilhelmine, Marianne, Auguste, Dorothea, Christiane, Juliane, Katharina, Julie, Therese.

„Im großen und ganzen bereitete der Protestantismus eine Rückkehr zu germanischen Namen vor.“

Ende des 19. Jahrhunderts nahmen die "Doppelnamen" (auch „Bindestrichnamen“ genannt) an Zahl zu. Diese erfreuten sich besonders in den 1930ern und 1950ern großer Beliebtheit, z. B. "Hans-Peter, Eva-Maria, Klaus-Dieter". In den Folgejahren existierten einige dieser Doppelnamen früher oder später dann auch in zusammengeschriebener Form (Hanspeter 1810er, Evamaria 1880er, Klausdieter 1930er).

Zu den meistverbreiteten Vornamen in Deutschland zählten im 19. Jahrhundert: Karl/Carl, Friedrich/Fritz, Wilhelm, Hans, Heinrich, Hermann, Otto, Ernst, Paul, Georg, Max, Franz, Ludwig, August, Rudolf, Adolf, Gustav, Richard, Johann, Julius, Theodor; Marie/Maria, Elisabeth/Else, Anna, Margarete, Helene, Gertrud, Luise/Louise, Hedwig, Auguste, Johanna, Sophie, Charlotte, Clara, Mathilde, Emma, Martha, Ida, Bertha, Frieda, Julie, Käthe.

Zu den meistverbreiteten Vornamen in Deutschland zählten im 20. Jahrhundert: Hans, Peter, Wolfgang, Klaus, Michael, Karl, Jürgen, Heinz, Thomas, Werner, Walter, Ernst, Franz, Paul, Kurt, Helmut, Herbert, Hermann, Andreas, Dieter; Barbara, Ursula, Maria, Susanne, Elisabeth, Monika, Petra, Karin, Sabine, Claudia, Renate, Eva, Gabriele, Anna, Brigitte, Helga, Christine, Gisela, Ruth, Ulrike.

Die Welt der Vornamen wurde im 20. Jahrhundert immer internationaler. Nach dem Zweiten Weltkrieg gingen die germanischen Namen eher unter "(was auch als Reaktion auf den Nationalsozialismus zu interpretieren ist)", dagegen nahmen die hebräischen, griechischen und lateinischen Namen ihren Platz ein; in weiterer Folge herrschte ein starker anglo-amerikanischer Einfluss. Vor allem durch internationale Medien wie Fernsehen und Rundfunk oder Literatur kam man mit vielen fremdsprachigen Namen in Kontakt und übernahm sie ins Deutsche. Heute ist auch die Entlehnung aus allen europäischen Ländern – von Skandinavien bis zum Balkan "(Björn bis Dragan)" – gängig.

Als Kontrast zur internationalen Namenvielfalt entwickelte sich teilweise eine Gegenströmung zur Bewahrung der alten germanischen Namen.

Seit den 1950er-Jahren gewannen anglophone und romanische Vornamen wie "Jennifer", "Mike" oder aber "Natalie" und "Marco" an Bedeutung. Obwohl sich in beiden Teilen Deutschlands verschiedene Namen der größten Beliebtheit erfreuten ("Peggy", "Mandy" und "Ronny" sind oft zitierte Beispiele für die DDR), war die Tendenz in beiden Staaten gleich.

In der zweiten Hälfte des 20. Jahrhunderts nahm in Deutschland die Beliebtheit des französischen Vornamens Nadine schlagartig zu. "(Der Grund dafür ist in der ab Ende 1974 von der ARD ausgestrahlten TV-Serie "Härte 10" zu finden, mit Olga-Georges Picot als "Nadine Mercier" in der weiblichen Hauptrolle)."

Vor allem folgende Faktoren sind für diese Änderungen verantwortlich:

Einflussfaktoren, die ausgeschlossen werden können:

Bei der Übernahme fremder Namen war von jeher eine lautliche Anpassung zu beobachten. Zuerst wurden Namen adaptiert, die an traditionelle phonetische Gewohnheiten anschlussfähig waren. So wurde aus "Johannes" im Mittelalter "Hans", aus "Christian" wurde "Christen" und aus "Marcus" zunächst "Marx". Manche Namen wurden auch in ihrer geschriebenen Form übernommen, obwohl die Aussprache in den Herkunftsgebieten eine andere war: So wurde span. "Xavier" als "Xaver" übernommen und nicht als "Chabier" und norweg. "Harald" als Harald und nicht als "Harall".

In Österreich darf eine Person mehrere Vornamen tragen. Für die Namenswahl gelten folgende Einschränkungen:

Zur Wahl des Vornamens eines Kindes sind dessen Eltern berechtigt, bei unehelicher Geburt ist es das Recht der Mutter. Beim zuständigen Standesamt muss dafür schriftlich die Erklärung des Vornamens eingereicht werden; sie ist Voraussetzung für die Ausstellung der Geburtsurkunde. Wird die Erklärung nicht gleich bei der Anzeige der Geburt abgegeben, muss sie spätestens innerhalb eines Monats nach der Geburt beim Standesamt erfolgen. Können sich die Eltern eines ehelich geborenen Kindes nicht auf den oder die Vornamen einigen, oder geben sie unzulässige oder gar keinen Vornamen an, wird das Pflegschaftsgericht verständigt.

Im Jahr 2010 wurden Neugeborenen unter den österreichischen Staatsangehörigen am häufigsten die Vornamen Anna und Lukas gegeben. Lukas ist dabei bereits seit 1996 der häufigste Name.

Nach Schweizer Namensrecht gibt es Vornamen wie Andrea, die, wie im Italienischen, das Geschlecht nicht eindeutig bestimmen. Solche Vornamen müssen mit einem anderen, eindeutig männlichen oder weiblichen Vornamen kombiniert werden (z. B. Andrea Luigi, Andrea Franziska), oder man muss ausweichen auf eine eindeutig das Geschlecht bezeichnende Namensvariante (z. B. Andreas, Andre, André für Knaben bzw. Andrée, Andreina, Andrina, Andrietta für Mädchen). Weitere Beispiele solcher Namen sind Dominique, Gabriele, Sascha und Simone.

Wie in anderen Teilen des deutschen Sprachraums, so sind auch in der Deutschschweiz einige Vornamen üblich, die im übrigen deutschen Sprachraum so gut wie nicht vorkommen. Dazu gehören Beat (in Deutschland bekannte weibliche Variante Beate), Reto, Urs und Regula oder Solange.

In Griechenland werden zumeist christliche, seltener antike Vornamen vergeben. Traditionell wurde bei der ersten Tochter immer der Vorname der Großmutter väterlicherseits und beim ersten Sohn der Vorname des Großvaters väterlicherseits vergeben. Entsprechend bei den zweiten Kindern die Namen der Großeltern mütterlicherseits. Modenamen sind eher selten und ein Phänomen der letzten Jahre (auch hier oft antike Namen wie Iason "(Jason)" oder Danae, aber kaum je solche z. B. aus dem angelsächsischen Raum).

Während hier mehrere Vornamen unüblich sind, wird der Vorname des Vaters (in der Genitivform) als Mittelname geführt und auch in Identitätspapieren angegeben.

Die zehn beliebtesten Namen für Neugeborene in Italien im Jahr 2007 waren bei Mädchen Giulia, Sofia, Martina, Sara, Chiara, Aurora, Giorgia, Alessia, Francesca, Alice, und bei Jungen Alessandro, Andrea, Matteo, Lorenzo, Gabriele, Mattia, Luca, Davide und Riccardo.

Aufgrund des hohen Bevölkerungsanteils von Katholiken sind dort viele Vornamen an den Namen von Heiligen und der Jungfrau Maria orientiert.

In einigen italienischen Regionen ist es Tradition, den ersten Sohn nach dem Großvater väterlicherseits, den zweiten Sohn nach dem Großvater mütterlicherseits, die erste Tochter nach der Großmutter väterlicherseits und die zweite Tochter nach der Großmutter mütterlicherseits zu benennen. Dies führt zu einer starken Verbreitung traditioneller Vornamen.

Siehe auch: Italienische Personennamen germanischer Wurzel.

In Polen werden die einem neugeborenen Kind vergebenen Vornamen gemäß dem Gesetz über Standesakten (pln. "Prawo o aktach stanu cywilnego") gesetzlich wie folgt eingeschränkt:

Bis sechs Monate nach der Geburt des Kindes darf der Vorname bzw. dürfen die Vornamen durch die Eltern auf standesrechtlichem Weg nachträglich geändert werden. Sollte kein Elternteil innerhalb der Frist von 14 Tagen nach der Geburt den oder die Vornamen für das Kind bestimmt haben, hat der Standesbeamte über den Vornamen zu entscheiden und einen "in Polen üblichen" Vornamen seiner Wahl einzutragen.

Grundsätzlich muss jeder einzutragende Vorname gemäß der "Verordnung über die Einzelheiten der Standesamtakten" der in Polen üblichen Rechtschreibnorm entsprechen. Dies bedeutet insbesondere, dass Vornamen mit den im polnischen Alphabet traditionell nicht vorhandenen Buchstaben "Q", "V" und "X" nicht eingetragen, bzw. in der Rechtschreibung entsprechend polonisiert werden. So wird aus "Kevin" der Vorname "Kewin" und aus "Roxana" der Vorname "Roksana". Auch sonstige vom Polnischen abweichende Schreibweisen werden entsprechend der Aussprache angeglichen. Aus "Jessica" wird "Dżesika" und aus "Brian" wird "Brajan". Eine gesetzlich verankerte Liste der eintragungsfähigen Vornamen besteht nicht, allerdings wird von den Standesämtern in Zweifelsfällen üblicherweise die Liste des Rates der Polnischen Sprache als Referenz verwendet.

Da die gesetzlichen Bestimmungen nur die Eintragung, jedoch nicht die Führung der Vornamen betreffen, kann es vorkommen, dass polnische Bürger Namen tragen, die nicht den obigen Regelungen entsprechen. Dies kann sich beispielsweise durch die Geburt im Ausland oder eine Einbürgerung ergeben. Ferner besteht die Einschränkung auf höchstens zwei Vornamen erst seit 1952 und die davor geborenen Personen können weiterhin auch drei oder mehr Vornamen tragen.

In China, Korea, Vietnam und anderen ostasiatischen Staaten haben Vornamen eine andere Funktion. Sie identifizieren ihren Träger weit mehr als in Europa, was dort notwendig ist, da sich die Bevölkerung in diesen Ländern nur wenige Familiennamen teilt. Der Vorname kann beliebig aus einem oder zwei Morphemen der Sprache gebildet werden, die klassisch jeweils als chinesische Schriftzeichen geschrieben werden. Es besteht also eine fast unbeschränkte Anzahl an zulässigen Eigennamen. In vielen Familien wird ein Morphem des Vornamens identisch an alle Nachkommen derselben Generation vergeben (Generationenname).

Anders als bei europäischen Vornamen gibt es keine festgelegte Zuordnung von Eigennamen zum Geschlecht des Trägers (bis auf Modewellen, die gewisse Eigennamen gehäuft auftreten lassen und manchmal ein bestimmtes Geschlecht des Trägers vermuten lassen). Die Bezeichnung „Vorname“ für die ostasiatischen Eigennamen ist irreführend, da sie in Ostasien durchweg "hinter" den Familiennamen gestellt werden. Auch die Bezeichnung „Rufname“ ist unpassend, da der Eigenname in Ostasien (außer im engsten Familienkreis) fast nie zur Anrede verwendet wird. Zur formalen Anrede wird entweder der vollständige Name gebraucht oder der Familienname, ggf. ergänzt durch eine Funktionsbezeichnung (zum Beispiel „Kollege“). Im Freundeskreis werden meistens der Familienname mit dem Zusatz „ehrwürdiger/junger“ zur Anrede verwendet oder aber Spitznamen, und unter Verwandten ist die Anrede mit dem Verwandtschaftsgrad üblich, wofür es sprachlich differenziertere Begriffe als in Europa gibt (zum Beispiel chinesisch 妹妹 "mèimèi" = ‚jüngere Schwester‘, 大伯 "dàbó" = ‚älterer Bruder des Vaters‘ etc.).





</doc>
<doc id="5468" url="https://de.wikipedia.org/wiki?curid=5468" title="Vier-Farben-Satz">
Vier-Farben-Satz

Der Vier-Farben-Satz (auch Vier-Farben-Theorem, früher auch als Vier-Farben-Vermutung oder Vier-Farben-Problem bekannt) ist ein mathematischer Satz und besagt, dass vier Farben immer ausreichen, eine beliebige Landkarte in der euklidischen Ebene so einzufärben, dass keine zwei angrenzenden Länder die gleiche Farbe bekommen. Der Satz findet Anwendung in der Graphentheorie, Topologie und Kartografie.

Dies gilt unter den Einschränkungen, dass isolierte gemeinsame Punkte nicht als „Grenze“ zählen und jedes Land aus einer zusammenhängenden Fläche besteht, also keine Exklaven vorhanden sind.

Der Satz wurde erstmals 1852 von Francis Guthrie als Vermutung aufgestellt, als er in einer Karte die Grafschaften von England färben wollte. Es war offensichtlich, dass drei Farben nicht ausreichten – siehe Abbildung rechts – und man fünf in keinem konstruierten Beispiel brauchte. In einem Brief des Londoner Mathematikprofessors Augustus De Morgan vom 23. Oktober 1852 an den irischen Kollegen William Rowan Hamilton wurde die Vermutung erstmals diskutiert und veröffentlicht: „Genügen vier oder weniger Farben, um die Länder einer Karte so zu färben, dass benachbarte Länder verschiedene Farben tragen?“

Der englische Mathematiker Arthur Cayley stellte das Problem 1878 der mathematischen Gesellschaft Londons vor. Innerhalb nur eines Jahres fand Alfred Kempe einen scheinbaren Beweis für den Satz. Elf Jahre später, 1890, zeigte Percy Heawood, dass Kempes Beweisversuch fehlerhaft war. Ein zweiter fehlerhafter Beweisversuch, 1880 von Peter Guthrie Tait veröffentlicht, konnte ebenfalls elf Jahre lang nicht widerlegt werden. Erst 1891 zeigte Julius Petersen, dass auch Taits Ansatz nicht korrekt war. Heawood gab im Jahre 1890 mit der Widerlegung von Kempes „Vier-Farben-Beweis“ zusätzlich einen Beweis für den Fünf-Farben-Satz an, womit eine obere Grenze für die Färbung von planaren Graphen zum ersten Mal fehlerfrei bewiesen wurde. In Kempes fehlerhaftem Versuch steckten bereits grundlegende Ideen, die zum späteren Beweis durch Appel und Haken führten.
Heinrich Heesch entwickelte in den 1960er und 1970er Jahren einen ersten Entwurf eines Computerbeweises, der aber mangels verfügbarer Rechenzeit nicht verwirklicht wurde. Dieser konnte von Kenneth Appel und Wolfgang Haken an der University of Illinois 1976 verbessert werden. Der Beweis reduzierte die Anzahl der problematischen Fälle von Unendlich auf 1936 (eine spätere Version sogar 1476), die durch einen Computer einzeln geprüft wurden. Nach Kritiken an diesem Beweis veröffentlichten Appel und Haken 1989 eine ausführliche Beschreibung mit einem 400-seitigen Anhang auf Mikrofilm.

1996 konnten Neil Robertson, Daniel Sanders, Paul Seymour und Robin Thomas einen modifizierten Computerbeweis finden, der die Fälle auf 633 reduzierte. Auch diese mussten per Computer geprüft werden.

2005 haben Georges Gonthier und Benjamin Werner einen formalen Beweis des Satzes in dem Beweisassistenten Coq konstruiert.

Der Vier-Farben-Satz war das erste große mathematische Problem, das mit Hilfe von Computern gelöst wurde. Deshalb wurde der Beweis von einigen Mathematikern nicht anerkannt, da er nicht direkt durch einen Menschen nachvollzogen werden kann. Schließlich muss man sich auf die Korrektheit des Compilers und der Hardware verlassen. Auch der Mangel an mathematischer Eleganz des Beweises wurde kritisiert.

Wie viele offene Probleme der Mathematik hat der Vier-Farben-Satz eine Menge fehlerhafter Beweise und Gegenbeweise provoziert. Manche hielten der öffentlichen Prüfung über Jahrzehnte stand, bis sie als falsch erkannt wurden. Viele andere, hauptsächlich von Amateuren entwickelte, sind niemals veröffentlicht worden.

Häufig enthalten die einfachsten „Gegenbeispiele“ eine Region, welche alle anderen Regionen berührt. Dies erzwingt, um mit vier Farben auszukommen, die restlichen Regionen mit nur drei Farben auszufüllen. Die Gegenbeispiele übersehen dabei, dass durch Umfärbung des inneren Bereiches ebendieses erreicht werden kann, da sie sich zu sehr auf das äußere Gebiet stürzen.

Dieser Trick kann verallgemeinert werden; es ist leicht, Karten zu konstruieren, auf denen es unmöglich ist, mit vier Farben auszukommen, wenn die Farben einiger Regionen im Voraus festgelegt wurden. Ein oberflächlicher Überprüfer des Gegenbeispiels wird oft nicht daran denken, diese Regionen umzufärben.

Andere falsche Gegenbeweise verletzen die Annahmen des Satzes, wie zum Beispiel durch Verwendung von Regionen, die aus mehreren getrennten Bereichen bestehen, oder durch Verbieten von gleichfarbigen Regionen, die sich nur an einem Punkt berühren.

Formal lässt sich das Problem am einfachsten mit Hilfe der Graphentheorie beschreiben. Man fragt, ob die Knoten jedes planaren Graphen mit maximal vier Farben so gefärbt werden können, dass keine benachbarten Knoten die gleiche Farbe tragen. Oder kürzer: „Ist jeder planare Graph 4-färbbar?“ Dabei wird jedem Land der Karte genau ein Knoten zugewiesen; die Knoten angrenzender Länder werden miteinander verbunden.

Das Vier-Farben-Problem ist ein Spezialfall der Heawood-Vermutung. Das klassische Vier-Farben-Problem betrifft Landkarten, die auf einer Ebene oder Kugeloberfläche liegen. Die Heawood-Vermutung stellt die analoge Frage für allgemeine Oberflächen, etwa die Kleinsche Flasche (6 Farben), das Möbiusband (6 Farben), die Projektive Ebene (6 Farben) und den Torus (7 Farben). Interessanterweise ist die Verallgemeinerung – abgesehen vom Spezialfall für Ebenen oder Kugeloberflächen – wesentlich leichter zu beweisen als der Vier-Farben-Satz und kommt ohne Computerhilfe aus. J. W. Ted Youngs und Gerhard Ringel konnten im Jahr 1968 erstmals die Heawood-Vermutung für alle anderen Fälle beweisen (Satz von Ringel-Youngs). Der Vier-Farben-Satz wird also nicht durch diesen Beweis verifiziert, sondern muss gesondert behandelt werden.
Erweitert man die Aufgabenstellung des Vier-Farben-Satzes von Oberflächen auf den dreidimensionalen euklidischen Raum, dann gibt es keine Obergrenze für die Anzahl der Farben. Anstelle der „Länder“ treten dreidimensionale Gebiete („Körper“) auf, die unterschiedliche Farben haben sollen, wenn sie eine gemeinsame Grenzfläche besitzen. Für jede Zahl formula_1 lässt sich ein Beispiel konstruieren (Heinrich Tietze), das mindestens formula_1 Farben benötigt. Man denke sich formula_1 „lange“ kongruente Quader („Riegel“) nebeneinanderliegend, die zusammen einen Quader quadratischer Grundfläche bilden. Darauf liegen noch einmal formula_1 zu den ersten kongruente Quader nebeneinander, aber senkrecht zu den unteren, so dass alle unteren Quader alle oberen Quader berühren. Nun sei jeder der unteren mit genau einem der oberen verbunden, so dass beide gemeinsam kreuzweise "einen" Körper bilden. Jeder dieser Körper berührt jeden anderen; man braucht also formula_1 Farben und formula_1 war beliebig.

Wenn (so wie in der Realität häufig der Fall) ein Land auf mehrere nicht-angrenzende Gebiete verteilt ist (Kolonien, Exklaven, …), dann ist der zugehörige Graph nicht notwendigerweise planar und es sind möglicherweise mehr als vier Farben zur Färbung notwendig. Auf Planarität kann man gegebene Graphen sehr schnell testen. Nach dem Satz von Kuratowski gibt es bestimmte Untergraphen, die die Planarität von Graphen verhindern. Es sind dies genau zwei Grundformen, die sogenannten Kuratowski-Minoren formula_7 und formula_8, und darüber hinaus ihre Unterteilungen. Durch eine geschickte Wahl der Datenstrukturen kann man diese „Untergraphen“ finden bzw. feststellen, dass es sie nicht gibt, indem man jeden Knoten und jede Kante nur konstant oft betrachtet.

Die kleinste mögliche Färbung in allgemeinen Graphen formula_9 zu finden, mit anderen Worten die sogenannte "Chromatische Zahl" formula_10 zu bestimmen, ist sehr aufwändig (genauer: eine NP-vollständige Aufgabe). Nach den Aussagen von Tutte wäre sie gelöst, wenn man im Dualgraphen formula_11 eine kleinste Gruppe gefunden hat, sodass eine gruppenwertige Strömung (das ist ein „Fluss ohne Anfang und Ende“), die nirgends das Nullelement annimmt, existiert. Diese Gruppenordnung heißt "Flusszahl" formula_12 und es ist für beliebige Graphen formula_13. Die Lösbarkeit dieses nach wie vor NP-vollständigen Problems ist unabhängig von der Struktur der vorgegebenen Gruppe und hängt nur von der Gruppenordnung ab.

Es gibt weitere Zusammenhänge des Vier-Farben-Problems mit Problemen der Diskreten Mathematik, sodass man auch Methoden der Algebraischen Topologie anwenden kann.

Einige bekannte Mathematiker haben sich an dem Beweis versucht. So berichtet Max Born, dass Hermann Minkowski über mehrere Wochen einen Beweisversuch in einer Einführungsvorlesung für Topologie unternahm (mit den einführenden Worten, dies würde sich gut als Einführung in die Topologie eignen und daran hätten sich bisher nur Mathematiker dritten Ranges versucht), bis er schließlich aufgab. Born erinnert sich, dass damals ein Gewitter herrschte und Minkowski halb scherzhaft meinte, der Himmel würde über seine Vermessenheit zürnen.

Auch Ernst Witt versuchte sich als Student an dem Beweis und präsentierte ihn Richard Courant; sein Freund Heinrich Heesch fand aber einen Fehler, was der Beginn seiner eigenen Beschäftigung mit dem Problem war.

Andere Mathematiker, die sich mit dem Problem beschäftigten und bedeutende Teilresultate erzielten, waren Øystein Ore (der die Mindestanzahl der Gebiete, die mit vier Farben einfärbbar sind, auf 40 erhöhte) und Hassler Whitney (in seiner Dissertation).

Es gibt auch algebraisch äquivalente Formulierungen (Howard Levi, Juri Wladimirowitsch Matijassewitsch, M. Mnuk, Noga Alon).




</doc>
<doc id="5469" url="https://de.wikipedia.org/wiki?curid=5469" title="Vielfliegerprogramm">
Vielfliegerprogramm

Die Vielfliegerprogramme der großen Fluggesellschaften sollen die Kundenbindung verstärken, indem sie die häufige Nutzung derselben Gesellschaft mit Rabatten in Form von Freiflügen oder „Prämien“ belohnen. Betriebswirtschaftlich bezeichnet man diesen Vorgang als Lock-in-Effekt.

Bei der Teilnahme an einem solchen Programm werden die bei einer Fluggesellschaft zurückgelegten Meilen gespeichert und nach Erreichen einer bestimmten Menge mit Prämien belohnt. Dies können kostenlose Flüge (Freiflüge), ermäßigte Flüge, Erhöhung der Buchungsklasse (Upgrade), Zutrittsberechtigung für Lounges, Ermäßigungen bei Hotels, Golfclubs, Restaurants, Bergbahnen usw. oder auch Sachleistungen sein. Ab einem bestimmten Umsatz vergeben manche Gesellschaften einen höheren Mitgliederstatus wie z. B. silber oder gold oder bestimmte Namen (z. B. „Senator“, „Executive“). Hierbei werden den Kunden weitere Vorteile wie höherer Meilenzuwachs und kostenlose Dienstleistungen (z. B. Chauffeurdienste) gewährt.

Manchmal lassen sich im Tausch gegen Meilen Plätze neben freien Sitzplätzen oder mit mehr Beinfreiheit reservieren. Teilweise wird diesen Kunden sogar Platz auf ausgebuchten Flügen angeboten; ‚normale‘ Fluggäste werden dann auf einen späteren Flug verlagert. Nach einer bestimmten Zeit können angesammelte Meilen oder der erreichte Mitgliedsstatus verfallen.

Alle großen internationalen Fluggesellschaften bieten Vielfliegerprogramme an. Aufgrund von Vereinbarungen zwischen Fluggesellschaften oder -allianzen lassen sich diese Bonus-Meilen auch bei anderen Gesellschaften sammeln oder von einer Gesellschaft zur anderen übertragen. Häufig arbeiten die Gesellschaften auch mit anderen Unternehmen wie Hotels oder Autovermietern zusammen. Bei Erwerb eines Produktes oder einer Dienstleistung dieser Unternehmen werden dann ebenfalls eine bestimmte Menge „Meilen“ oder Punkte auf einem Kundenkonto gutgeschrieben. Die Teilnehmer am Programm erhalten meist eine Kundenkarte, die sie teilweise bei der Nutzung vorlegen müssen.

In der Regel sind die verschiedenen Vielfliegerprogramme von Fluglinien, die zusammenarbeiten, untereinander kompatibel. Das trifft auf Oneworld Alliance, Star Alliance und SkyTeam zu. So kann man die Meilen, die mit einer beliebigen Fluglinie einer dieser Allianzen geflogen wurden, auf das entsprechende Vielfliegerprogramm der Partnerfluglinie buchen, bei der man angemeldet ist; nicht immer jedoch zu den gleichen Bedingungen wie bei der durchführenden Fluggesellschaft.

Ökonomisch setzen die Betreiber der populären Systeme auf die Erwartung, dass ein Großteil dieser Punkte niemals gegen Waren oder Dienstleistungen eingetauscht wird. Es ist möglich, Meilen an Makler zu verkaufen. In der Vergangenheit kam es daher bisweilen zu rechtlichen Auseinandersetzungen beim Versuch, Punkte verschiedener Teilnehmer gemeinschaftlich zu nutzen oder über Tauschbörsen und Versteigerungsplattformen zu veräußern.

Bereits 1979 rief die Fluggesellschaft Texas International Airlines die erste derartige Aktion ins Leben. Das Konzept des Vielfliegerprogramms wird jedoch meist dem ehemaligen Chef der American Airlines, Robert Crandall, zugeschrieben. Er fand Anfang der 1980er Jahre heraus, dass fünf Prozent seiner Kunden für 40 Prozent des Umsatzes sorgten. Am 1. Mai 1981 wurde das Programm AAdvantage eingeführt. Die Teilnehmer erhielten dann mit jedem Flug Meilen, die sie zunächst nur für Flüge der American Airlines, und später auch gegen andere Prämien einlösen konnten. Delta Air Lines "(Skymiles)", TWA "(Aviators)" und United Airlines "(MileagePlus)" folgten kurz darauf mit ähnlichen Programmen, British Airways (Executive Club) führte 1982 ein Vielfliegerprogramm ein. Miles & More, das Vielfliegerprogramm der Lufthansa, wurde am 1. Januar 1993 gestartet.

In Anlehnung an die Vielfliegerprogramme haben auch mehrere Bahngesellschaften ein Vielfahrerprogramm aufgelegt. Unter anderem sind dies:


In einer Railteam genannten Kooperation sollen diese oben genannten Vielfahrerprogramme untereinander kompatibel werden.

Daneben beteiligt sich die österreichische Privatbahn Westbahn am Miles & More Programm. Auch in den AIRail-Zügen der Deutsche Bahn AG können Miles-&-More-Meilen erworben werden.

Da auch bei Geschäftsreisen Punkte erworben werden, die dann allerdings personengebunden dem Reisenden und nicht dem Zahlenden zugutekommen, stellt die korrekte Abrechnung von Bonusmeilen eine gewisse Hürde dar. Kritiker sprechen schlicht von Bestechung, die dazu führt, dass nicht der günstigste Fluganbieter gewählt wird, sondern derjenige, der dem Reisenden die meisten Punkte anbietet. Viele Arbeitgeber legen im Arbeitsvertrag oder in einer Anordnung fest, dass die „Dienst-Meilen“ auch nur für Dienstreisen verwendet werden dürfen. Hält sich der Arbeitnehmer nicht daran, kann er abgemahnt und sogar gekündigt werden. Mittlerweile hat das Bundesarbeitsgericht hierzu ein Urteil veröffentlicht.

Andere Firmen verzichten jedoch zu Gunsten des Arbeitnehmers auf eine solche Regelung, und auch bei Freiberuflern, die für unterschiedliche Auftraggeber unterwegs sind, ist dies üblich. Manche Fluggesellschaften fordern aber, dass Prämien nur privat genutzt werden dürfen. Zu Rücktritten führte 2002 die sogenannte Bonusmeilen-Affäre, bei der Abgeordnete des Deutschen Bundestages, darunter der Grünen-Politiker Cem Özdemir sowie der Linken-Politiker Gregor Gysi, mit dienstlich angesammelten Bonuspunkten private Flugreisen durchführten.

In Deutschland können die Fluggesellschaften die Lohnsteuer aus Vereinfachungsgründen mit einem Pauschalsteuersatz von 2,25 % berechnen. Bemessungsgrundlage sind die insgesamt an inländische Kunden ausgeschütteten Prämien. Die Höhe des Steuersatzes berücksichtigt, dass ein Teil der Prämien keinen Arbeitslohn darstellt und ein anderer Teil wegen des Rabattfreibetrags steuerfrei wäre.

Ein "Mileage run" ist eine Flugreise mit dem Ziel, in möglichst kurzer Zeit möglichst viele Vielfliegermeilen zu sammeln. Beweggründe hierfür sind das Sammeln von Bonusmeilen, um diese gegen Flug- und Sachprämien einzutauschen sowie die (Wieder-)Erreichung eines Statuslevels.




</doc>
<doc id="5471" url="https://de.wikipedia.org/wiki?curid=5471" title="Volumen">
Volumen

</math>

Das Volumen (Pl. "Volumen" oder "Volumina"; von lat. "volumen" „Windung, Krümmung“, aus "volvere" „wälzen, rollen“), auch: Raum- oder Kubikinhalt, ist der räumliche Inhalt eines geometrischen Körpers. Übliches Formelzeichen ist "V".

In der Physik bezeichnet man mit dem Volumen die Ausdehnung (den "Platzbedarf") eines Körpers. Die (kohärente) SI-Einheit für das Raummaß ist der Kubikmeter (Einheitenzeichen m). Vereinzelt liest man noch die veralteten Abkürzungen cbm für m³ und ccm für cm³. Die Einheit Liter ist für Gase und Flüssigkeiten gebräuchlich und als 1 dm (10×10×10 cm³) definiert.

Technisch muss unterschieden werden:

Die ersten bekannten Formeln zur Volumenbestimmung (auch Stereometrie) stammen schon aus dem frühen Ägypten. Das Moskauer Papyrus ist eine Sammlung von Rechenaufgaben und ist etwa auf das Jahr 1850 v. Chr. datiert. Unter anderem sind hier die Formeln für die Bestimmung der Volumina für Rechteckkegel beschrieben. Die Bestimmung wurde durch Analyse und anschließender Synthese erreicht. Das heißt, der Körper wurde in mehrere bekannte Körper zerlegt und die Einzelvolumina addiert.

Im Laufe der Zeit haben sich ganz unterschiedliche Methoden zur Bestimmung von Volumina entwickelt:

Mathematisch gesehen ist das Volumen (der Rauminhalt) ein Maß für eine messbare Teilmenge des gewöhnlichen dreidimensionalen Raums. In der Theorie kann aus bekannten Ausmaßen und Form des Körpers das Volumen durch Rechnung nach für den entsprechenden Körper gültigen Formeln bestimmt werden.

Beispiele:


Man kann ein Volumen auch über mehrdimensionale Mannigfaltigkeiten definieren, siehe dazu auch Volumenform. Nach dieser Verallgemeinerung ist das Volumen eines Teilraumes des zweidimensionalen euklidischen Raumes sein Flächeninhalt und Entsprechendes gilt auch in höherdimensionalen euklidischen Räumen. Beispielsweise hat ein n-dimensionaler Hyperwürfel mit Kantenlänge formula_1 ein Volumen von formula_24.

Das Volumen einer orientierbaren Riemannschen Mannigfaltigkeit ist definiert durch Integration der Volumenform über die Mannigfaltigkeit.

Ein Hohlraum ist ein mathematisches, ein physikalisches oder ein natürliches Objekt. Ein Hohlraum hat ein Volumen, das man als Hohlvolumen bezeichnet. Ein in einer Struktur eingeschlossenes Volumen kann ein Hohlraum sein. Dabei verändert die Existenz von Hohlräumen oft die umliegende Struktur, z. B. in Hinsicht auf Festigkeit oder Elastizität (Siehe Porosität).

Ein natürlicher Hohlraum enthält ein Vakuum oder ist mit Gasen, Flüssigkeiten oder anderen Stoffen gefüllt, was wiederum die umschließende Struktur beeinflussen kann. Insbesondere kann die Grenzfläche zwischen Hohlraum und Struktur sich verändern, schwer zu erkennen sein oder auch nur auf gedanklicher Ebene existieren. Auch ein Hohlraum, der eine oder mehrere Öffnungen hat, also nicht vollständig von der umschließenden Struktur umgeben ist, wird umgangssprachlich so bezeichnet.

Die Größe des umschlossenen Volumens kann oft errechnet oder experimentell bestimmt werden. In manchen Fällen ist dies allerdings prinzipiell nicht möglich.

Hohlraumbildung ist ein oft auftretendes Phänomen bei geologischen und sonstigen physikalischen und chemischen Prozessen.

Evakuierte Hohlräume haben mehrere universelle Eigenschaften, eine davon ist die Hohlraumstrahlung.





</doc>
<doc id="5473" url="https://de.wikipedia.org/wiki?curid=5473" title="VB">
VB

VB, Vb, vb oder vB steht als Abkürzung für:


Informatik:

Jura:

Wirtschaft:

Politik:

Behördenwesen:

Verkehrswesen:
VB steht auf Kfz-Kennzeichen für:
Vb (‚Fünf-b‘) steht für:
Siehe auch:


</doc>
<doc id="5474" url="https://de.wikipedia.org/wiki?curid=5474" title="Volt">
Volt

\.</math>

Da die obige Definition in der Praxis nur schwer als genaue Referenz eingesetzt werden kann, wird seit 1990 ein Volt mittels des Josephson-Effekts und der Josephson-Konstante, des Kehrwerts des magnetischen Flussquantums, festgelegt. Diese Konstante beträgt:
Damit kann mit Referenzmessungen die Festlegung von einem Volt auf eine sehr genaue Frequenzmessung (Zeitmessung) zurückgeführt werden.

Früher wurde die Definition des Volt vom Weston-Normalelement abgeleitet. Dieses Element liefert bei einer Temperatur von 20 °C eine Spannung von genau 1,01865 V.

Die Einheit Volt ist mit verschiedenen Vorsätzen für Maßeinheiten (SI-Präfixe) in Verwendung, beispielsweise: 


</doc>
<doc id="5475" url="https://de.wikipedia.org/wiki?curid=5475" title="Vektor (Begriffsklärung)">
Vektor (Begriffsklärung)

Vektor (von ‚Träger‚ Fahrer‘; zu "vectum", PPP von "vehere" ‚fahren, führen, tragen, bringen‘) bezeichnet

in den Naturwissenschaften:

bei der Datenverarbeitung:

im Militärwesen:

des Weiteren:
Siehe auch:


</doc>
<doc id="5476" url="https://de.wikipedia.org/wiki?curid=5476" title="Vektorraum">
Vektorraum

Ein Vektorraum oder linearer Raum ist eine algebraische Struktur, die in vielen Teilgebieten der Mathematik verwendet wird. Vektorräume bilden den zentralen Untersuchungsgegenstand der linearen Algebra. Die Elemente eines Vektorraums heißen "Vektoren." Sie können addiert oder mit Skalaren (Zahlen) multipliziert werden, das Ergebnis ist wieder ein Vektor desselben Vektorraums. Entstanden ist der Begriff, indem diese Eigenschaften ausgehend von Vektoren des euklidischen Raumes abstrahiert wurden, sodass sie dann auf abstraktere Objekte wie Funktionen oder Matrizen übertragbar sind.

Die Skalare, mit denen man einen Vektor multiplizieren kann, stammen aus einem Körper. Deswegen ist ein Vektorraum immer ein Vektorraum "über" einem bestimmten Körper. Sehr oft handelt es sich dabei um den Körper formula_1 der reellen Zahlen oder den Körper formula_2 der komplexen Zahlen. Man spricht dann von einem reellen Vektorraum bzw. einem komplexen Vektorraum.

Eine Basis eines Vektorraums ist eine Menge von Vektoren, die es erlaubt, jeden Vektor durch eindeutige Koordinaten darzustellen. Die Anzahl der Basisvektoren in einer Basis wird Dimension des Vektorraums genannt. Sie ist unabhängig von der Wahl der Basis und kann auch unendlich sein. Die strukturellen Eigenschaften eines Vektorraums sind eindeutig durch den Körper, über dem er definiert ist, und seine Dimension bestimmt.

Eine Basis ermöglicht es, Rechnungen mit Vektoren über deren Koordinaten statt mit den Vektoren selbst auszuführen, was manche Anwendungen erleichtert.

Es seien formula_3 eine Menge, formula_4 ein Körper, formula_5 eine innere zweistellige Verknüpfung, genannt Vektoraddition, und formula_6 eine äußere zweistellige Verknüpfung, genannt Skalarmultiplikation. Man nennt dann formula_7 einen "Vektorraum über dem Körper formula_8" oder kurz "formula_8-Vektorraum," wenn für die Vektoraddition die Eigenschaften

und weiter für die Skalarmultiplikation die Eigenschaften

für alle formula_22 und formula_23 erfüllt sind.

Anmerkungen


Für alle formula_48 und formula_49 gelten folgende Aussagen:

Ein anschaulicher Vektorraum ist die zweidimensionale Euklidische Ebene formula_55 (in rechtwinkligen kartesischen Koordinatensystemen) mit den Pfeilklassen (Verschiebungen oder Translationen) als Vektoren und den reellen Zahlen als Skalaren.
Die Summe zweier Verschiebungen ist wieder eine Verschiebung, und zwar diejenige Verschiebung, die man erhält, indem man die beiden Verschiebungen nacheinander ausführt:

Der Nullvektor formula_59 entspricht der Verschiebung, die alle Punkte an ihrem Platz belässt, d. h. der identischen Abbildung.

Durch die Streckung der Verschiebung formula_60 mit einem Skalar formula_61 aus der Menge der reellen Zahlen erhalten wir das Dreifache der Verschiebung:

Alles zu diesem Beispiel Gesagte gilt auch in der reellen affinen Ebene.

Ist formula_8 ein Körper und formula_64 eine natürliche Zahl, so bildet das formula_64-fache kartesische Produkt
die Menge aller formula_64-Tupel mit Einträgen in formula_8, einen Vektorraum über formula_8.
Die Addition und die skalare Multiplikation werden komponentenweise definiert; für
formula_70, formula_48 setzt man:
und
Häufig werden die formula_64-Tupel auch als Spaltenvektoren notiert, das heißt, ihre Einträge werden untereinander geschrieben.
Die Vektorräume formula_75 bilden gewissermaßen die Standardbeispiele für endlichdimensionale Vektorräume. Jeder formula_64-dimensionale formula_8-Vektorraum ist isomorph zum Vektorraum formula_75. Mit Hilfe einer Basis kann jedes Element eines Vektorraums eindeutig durch ein Element des formula_75 als Koordinatentupel dargestellt werden.

Ist formula_8 ein Körper, formula_3 ein formula_8-Vektorraum und formula_83 eine beliebige Menge, so kann auf der Menge formula_84 aller Funktionen formula_85 eine Addition und eine skalare Multiplikation punktweise definiert werden:
Für formula_86 und formula_48 sind die Funktionen formula_88 und formula_89 definiert durch
Mit dieser Addition und skalaren Multiplikation ist formula_84 ein formula_8-Vektorraum.
Insbesondere gilt dies für formula_96, wenn also als Zielraum der Körper formula_8 selbst gewählt wird.
Weitere Beispiele für Vektorräume erhält man als Untervektorräume dieser Funktionenräume.

In vielen Anwendungen ist formula_98, der Körper der reellen Zahlen, oder formula_99, der Körper der komplexen Zahlen, und formula_83 ist eine Teilmenge von formula_1, formula_102, formula_2 oder formula_104.
Beispiele sind etwa der Vektorraum aller Funktionen von formula_1 nach formula_1 und die Unterräume formula_107 aller stetigen Funktionen und formula_108 aller formula_109-mal stetig differenzierbaren Funktionen von formula_1 nach formula_1.

Ein einfaches Beispiel für einen Funktionenraum ist der zweidimensionale Raum der reellen linearen Funktionen, das heißt der Funktionen der Form
mit reellen Zahlen formula_113 und formula_114. Dies sind diejenigen Funktionen, deren Graph eine Gerade ist. Die Menge dieser Funktionen ist ein Untervektorraum des Raums aller reellen Funktionen, denn die Summe zweier linearer Funktionen ist wieder linear, und ein Vielfaches einer linearen Funktion ist auch eine lineare Funktion.

Zum Beispiel ist die Summe der beiden linearen Funktionen formula_115 und formula_116 mit
die Funktion formula_119 mit
Das 3-fache der linearen Funktion formula_115 ist die lineare Funktion formula_122 mit

Die Menge formula_124 der Polynome mit Koeffizienten aus einem Körper formula_8 bildet, mit der üblichen Addition und der üblichen Multiplikation mit einem Körperelement, einen unendlichdimensionalen Vektorraum. Die Menge der Monome formula_126 ist eine Basis dieses Vektorraums. Die Menge der Polynome, deren Grad durch ein formula_127 nach oben beschränkt ist, bildet einen Untervektorraum der Dimension formula_128. Beispielsweise bildet die Menge aller Polynome vom Grad kleiner gleich 4, also aller Polynome der Form
einen 5-dimensionalen Vektorraum mit der Basis formula_130.

Bei unendlichen Körpern formula_8 kann man die (abstrakten) Polynome mit den zugehörigen Polynomfunktionen identifizieren. Bei dieser Betrachtungsweise entsprechen die Polynomräume Unterräumen des Raums aller Funktionen von formula_8 nach formula_8. Zum Beispiel entspricht der Raum aller reellen Polynome vom Grad formula_134 dem Raum der linearen Funktionen.

Ist formula_135 ein Oberkörper von formula_8, so ist formula_135 mit seiner Addition und der eingeschränkten Multiplikation formula_138 als skalare Multiplikation ein formula_8-Vektorraum. Die dazu nachzuweisenden Regeln ergeben sich unmittelbar aus den Körperaxiomen für formula_135. Diese Beobachtung spielt eine wichtige Rolle in der Körpertheorie.

Beispielsweise ist formula_2 auf diese Weise ein zweidimensionaler formula_1-Vektorraum; eine Basis ist formula_143. Ebenso ist formula_1 ein unendlichdimensionaler formula_145-Vektorraum, bei dem eine Basis jedoch nicht konkret angegeben werden kann.

Lineare Abbildungen sind die Funktionen zwischen zwei Vektorräumen, die die Struktur des Vektorraums erhalten. Sie sind die Homomorphismen zwischen Vektorräumen im Sinne der universellen Algebra. Eine Funktion formula_146 zwischen zwei Vektorräumen formula_147 und formula_3 über demselben Körper formula_8 heißt genau dann "linear," wenn für alle formula_150 und alle formula_151
erfüllt sind. Das heißt, formula_115 ist kompatibel mit den Strukturen, die den Vektorraum konstituieren: der Addition und der Skalarmultiplikation. Zwei Vektorräume heißen "isomorph," wenn es eine lineare Abbildung zwischen ihnen gibt, die bijektiv ist, also eine Umkehrfunktion besitzt. Diese Umkehrfunktion ist dann automatisch ebenfalls linear. Isomorphe Vektorräume unterscheiden sich nicht bezüglich ihrer Struktur als Vektorraum.

Für endlich viele formula_155 und formula_156 bezeichnet man die Summe

als Linearkombination der Vektoren formula_158. Dabei ist formula_159 selbst wieder ein Vektor aus dem Vektorraum formula_3.

Ist formula_161 eine Teilmenge von formula_3, so wird die Menge aller Linearkombinationen von Vektoren aus formula_161 die lineare Hülle von formula_161 genannt. Sie ist ein Untervektorraum von formula_3, und zwar der kleinste Untervektorraum, der formula_161 enthält.

Eine Teilmenge formula_161 eines Vektorraums formula_3 heißt linear abhängig, wenn sich der Nullvektor auf nicht-triviale Weise als eine Linearkombination von Vektoren formula_169 ausdrücken lässt. „Nicht-trivial“ bedeutet, dass mindestens ein Skalar (ein Koeffizient der Linearkombination) von null verschieden ist. Andernfalls heißt formula_161 linear unabhängig.

Eine Teilmenge formula_171 eines Vektorraums formula_3 ist eine Basis von formula_3, wenn formula_171 linear unabhängig ist und die lineare Hülle von formula_171 der ganze Vektorraum ist.

Unter Voraussetzung des Auswahlaxioms lässt sich mittels des Lemmas von Zorn beweisen, dass jeder Vektorraum eine Basis hat (er ist "frei"), wobei diese Aussage im Rahmen von Zermelo Fraenkel äquivalent zum Auswahlaxiom ist. Dies hat weitreichende Konsequenzen für die Struktur eines jeden Vektorraums: Zunächst einmal lässt sich zeigen, dass je zwei Basen eines Vektorraums dieselbe Kardinalität haben, sodass die Kardinalität einer beliebigen Basis eines Vektorraums eine eindeutige Kardinalzahl ist, die man als "Dimension" des Vektorraums bezeichnet. Zwei Vektorräume über demselben Körper sind nun genau dann isomorph, wenn sie dieselbe Dimension haben, denn aufgrund der Gleichmächtigkeit zweier Basen von zwei Vektorräumen existiert eine Bijektion zwischen ihnen. Diese lässt sich zu einer bijektiven linearen Abbildung, also einem Isomorphismus der beiden Vektorräume, fortsetzen. Ebenso lässt sich zeigen, dass beliebige lineare Abbildungen durch die Bilder von Elementen einer Basis festgelegt sind. Dies ermöglicht die Darstellung jedweder linearer Abbildungen zwischen endlichdimensionalen Vektorräumen als Matrix. Dies lässt sich auf unendlichdimensionale Vektorräume übertragen, wobei jedoch sichergestellt werden muss, dass jede verallgemeinerte „Spalte“ nur endlich viele von Null verschiedene Einträge enthält, damit jeder Basisvektor auf eine Linearkombinationen von Basisvektoren im Zielraum abgebildet wird.

Mittels des Basisbegriffs hat sich das Problem, ein "Skelett" in der Kategorie aller Vektorräume über einem gegebenen Körper zu finden, darauf reduziert, ein Skelett in der Kategorie der Mengen zu finden, das durch die Klasse der Kardinalzahlen gegeben ist. Ein jeder formula_176-dimensionale Vektorraum lässt sich auch als die formula_176-fache direkte Summe des zugrunde liegenden Körpers auffassen. Die direkten Summen eines Körpers bilden also ein Skelett der Kategorie der Vektorräume über ihm.

Die Linearfaktoren der Darstellung eines Vektors in den Basisvektoren heißen Koordinaten des Vektors bezüglich der Basis und sind Elemente des zugrunde liegenden Körpers. Erst durch Einführung einer Basis werden jedem Vektor seine Koordinaten bezüglich der gewählten Basis zugeordnet. Dadurch wird das Rechnen erleichtert, insbesondere wenn man statt Vektoren in „abstrakten“ Vektorräumen ihre zugeordneten „anschaulichen“ Koordinatenvektoren verwenden kann.

Ein "Untervektorraum" (auch "linearer Unterraum") ist eine Teilmenge eines Vektorraums, die selbst wieder ein Vektorraum über demselben Körper ist. Dabei werden die Vektorraumoperationen auf den Untervektorraum vererbt. Ist formula_3 ein Vektorraum über einem Körper formula_8, so bildet eine Teilmenge formula_180 genau dann einen Untervektorraum, wenn die folgenden Bedingungen erfüllt sind:

Die Menge formula_147 muss also abgeschlossen bezüglich der Vektoraddition und der Skalarmultiplikation sein. Jeder Vektorraum enthält zwei triviale Untervektorräume, nämlich zum einen sich selbst, zum anderen den Nullvektorraum formula_188, der nur aus dem Nullvektor besteht. Jeder Unterraum ist Bild eines anderen Vektorraums unter einer linearen Abbildung in den Raum und Kern einer linearen Abbildung in einen anderen Vektorraum. Aus einem Vektorraum und einem Untervektorraum kann man durch Bildung von Äquivalenzklassen einen weiteren Vektorraum, den "Quotientenraum" oder "Faktorraum," bilden, was maßgeblich mit der Eigenschaft eines Unterraums zusammenhängt, ein Kern zu sein, siehe auch Homomorphiesatz.

Zwei oder mehrere Vektorräume können auf verschiedene Weisen miteinander verknüpft werden, sodass ein neuer Vektorraum entsteht.

Die direkte Summe zweier Vektorräume formula_189 über dem gleichen Körper besteht aus allen geordneten Paaren von Vektoren, von denen die erste Komponente aus dem ersten Raum und die zweite Komponente aus dem zweiten Raum stammt:

Auf dieser Menge von Paaren wird dann die Vektoraddition und die Skalarmultiplikation komponentenweise definiert, wodurch wiederum ein Vektorraum entsteht. Die Dimension von formula_191 ist dann gleich der Summe der Dimensionen von formula_3 und formula_193. Häufig werden die Elemente von formula_191 statt als Paar formula_195 auch als Summe formula_196 geschrieben. Die direkte Summe kann auch auf die Summe endlich vieler und sogar unendlich vieler Vektorräume verallgemeinert werden, wobei im letzteren Fall nur endlich viele Komponenten ungleich dem Nullvektor sein dürfen.

Das direkte Produkt zweier Vektorräume formula_189 über dem gleichen Körper besteht, wie die direkte Summe, aus allen geordneten Paaren von Vektoren der Form

Die Vektoraddition und die Skalarmultiplikation werden wieder komponentenweise definiert und die Dimension von formula_199 ist wieder gleich der Summe der Dimensionen von formula_3 und formula_193. Bei dem direkten Produkt unendlich vieler Vektorräume dürfen jedoch auch unendlich viele Komponenten ungleich dem Nullvektor sein, wodurch es sich in diesem Fall von der direkten Summe unterscheidet.

Das Tensorprodukt zweier Vektorräume formula_189 über dem gleichen Körper wird durch

notiert. Die Elemente des Tensorproduktraums haben dabei die bilineare Darstellung

wobei formula_205 Skalare sind, formula_206 eine Basis von formula_3 ist und formula_208 eine Basis von formula_193 ist. Ist formula_3 oder formula_193 unendlichdimensional, dürfen hierbei wieder nur endlich viele Summanden ungleich null sein. Die Dimension von formula_203 ist dann gleich dem Produkt der Dimensionen von formula_3 und formula_193. Auch das Tensorprodukt kann auf mehrere Vektorräume verallgemeinert werden.

In vielen Anwendungsbereichen in der Mathematik, etwa der Geometrie oder Analysis, ist die Struktur eines Vektorraums nicht hinreichend, etwa erlauben Vektorräume an sich keine Grenzwertprozesse, und man betrachtet daher Vektorräume mit bestimmten zusätzlich auf ihnen definierten Strukturen, die mit der Vektorraumstruktur in gewissen Sinnen kompatibel sind. Beispiele:

Bei all diesen Beispielen handelt es sich um topologische Vektorräume. In topologischen Vektorräumen sind die analytischen Konzepte der Konvergenz, der gleichmäßigen Konvergenz und der Vollständigkeit anwendbar. Ein vollständiger normierter Vektorraum heißt Banachraum, ein vollständiger Prähilbertraum heißt Hilbertraum.








</doc>
<doc id="5478" url="https://de.wikipedia.org/wiki?curid=5478" title="Voynich-Manuskript">
Voynich-Manuskript

Das Voynich-Manuskript (benannt nach Wilfrid Michael Voynich, der das Manuskript 1912 erwarb) ist ein Schriftstück, das sich einmal im Besitz des Kaisers Rudolf II. des Heiligen Römischen Reichs befand. 

Es ist in einer bislang nicht identifizierten Schrift und Sprache geschrieben. Sein Inhalt konnte bis heute nicht entschlüsselt werden und es ist nach wie vor unklar, ob der Text überhaupt einen sinnvollen Inhalt transportiert. Im Manuskript vorhandene Abbildungen erinnern an botanische, anatomische und astronomische Zusammenhänge und wurden mit Sorgfalt gezeichnet. 

Das Manuskript befindet sich seit 1969 unter Katalognummer MS 408 im Bestand der Beinecke Rare Book and Manuscript Library der Yale University.

1962 datierte ein Expertenteam die Handschrift aufgrund von Material und Schreibstil auf etwa 1500 n. Chr. Doch die Provenienz (die Folge der Vorbesitzer) konnte bislang nur lückenhaft und nicht mit Sicherheit ermittelt werden.

Da der Inhalt bisher nicht entschlüsselt werden konnte, stützt die Datierung des Manuskripts sich lediglich auf die Illustrationen. Aufgrund der Hinweise aus Kleidung und Haartracht sowie einiger weiterer Anhaltspunkte wird das Manuskript von den meisten Experten in den Zeitraum zwischen 1450 und 1520 datiert.

Erst 2009 wurden an Instituten in Chicago und Arizona kleinste Proben von vier verschiedenen Seiten untersucht. In einer Radiokarbonanalyse konnte das Alter des verwendeten Pergaments mit großer Wahrscheinlichkeit auf den Zeitraum zwischen 1404 und 1438 bestimmt werden. Vermutlich sind alle Seiten gleichen Ursprungs. Ferner haben Experten des McCrone-Forschungsinstitutes zu Chicago festgestellt, dass die Tinte nicht wesentlich später aufgetragen wurde.
Details in den Illustrationen, insbesondere die Schwalbenschwanzzinnen, ließen die Redakteure einer ORF-Sendung eine Entstehung der Handschrift in Oberitalien vermuten, da diese Zinnenform in der fraglichen Zeit nur dort belegt sei. Die Frührenaissance Norditaliens war auch eine Hochburg der frühneuzeitlichen Universalgelehrten und der Kryptologie.

Aus dem kaum leserlichen und wohl nicht eigenhändigen Namenseintrag "Jacobj ’a Tepenece" auf der ersten Seite des Manuskripts lässt sich, falls sie echt ist, schließen, dass der böhmische Hofpharmazeut Jakub Horčický z Tepence das Exemplar zur Lektüre in Händen hatte oder sogar sein Eigentümer war. Da schon sein Adelstitel verwendet wird, müsste dieser Eintrag erst nach 1608 entstanden sein. In einem mit dem Manuskript gefundenen Brief schreibt dessen vermeintlicher Verfasser, der spätere Besitzer Johannes Marcus Marci, um jenen Zeitpunkt sei Rudolf II. von Habsburg, Kaiser des Heiligen Römischen Reiches, Gerüchten zufolge Besitzer dieses Manuskriptes gewesen, nachdem er es für die damals hohe Summe von 600 Dukaten einem unbekannten Händler abgekauft habe. Entweder war Jakub Horčický dieser Händler, oder – und diese Theorie gilt als wahrscheinlicher – das Manuskript wurde ihm von Rudolf II. für weitere Analysen anvertraut, da er als erfolgreicher Chemiker und Pharmazeut bekannt war.

Marci berief sich bei dieser Geschichte auf seinen Freund Rafael Mišovský, einen Rechtsanwalt und Dichter, der unter Rudolf II. an den Prager Hof gekommen war, wo er den späteren Kaiser Ferdinand II. unterrichtete. Marci berichtete auch, Kaiser Rudolf habe geglaubt, Roger Bacon, der franziskanische Universalgelehrte des 13. Jahrhunderts, sei der Autor des Manuskripts gewesen.

Der nächste bekannte Besitzer war nach dem Begleitbrief Georg Baresch, ein wenig bekannter Alchemist, der zu Beginn des 17. Jahrhunderts in Prag lebte. Baresch hatte versucht, den Text zu entschlüsseln, war jedoch damit gescheitert. Er wandte sich daher an Athanasius Kircher, einen jesuitischen Universalgelehrten und seinerzeit eine Berühmtheit, dem es angeblich gelungen war, die Hieroglyphenschrift der alten Ägypter zu lesen. Dass die kirchersche Lesung völlig irrig war, stellte sich erst nach der erfolgreichen Entschlüsselung der Hieroglyphen durch Champollion heraus. Zu seiner Zeit galt Kircher jedoch als Kapazität im Dechiffrieren rätselhafter Texte, weshalb Baresch ihm eine Kopie der Manuskripttexte zusammen mit der Bitte um eine Expertise zusandte. Kircher scheint darauf jedoch nie reagiert zu haben. Der erste Brief Bareschs scheint verloren, ein weiterer Brief Bareschs an Kircher vom 27. April 1639 konnte jedoch von René Zandbergen im Archiv der Korrespondenz Kirchers gefunden werden.

Als nächster Besitzer erbte der bereits erwähnte Johannes Marcus Marci das Manuskript von dem mit ihm befreundeten Baresch (kurz vor 1666). Marci war der Autor des dem Manuskript beigelegten Briefes an Kircher, in dem er Kircher erneut um Hilfe bei der Entschlüsselung der Geheimschrift bat. Zu diesem Zweck wollte er diesmal keine Kopie senden, sondern das Manuskript selbst. Es ist jedoch nicht belegt, dass das Manuskript je in Kirchers Hände gelangte, denn in keinem der nach Kirchers Tod angefertigten Kataloge über seinen wissenschaftlichen Nachlass wird etwas von jenem Manuskript erwähnt.

Was in den über 200 Jahren zwischen 1666 und 1870 mit dem Manuskript geschah, ist bislang unbekannt. Doch da es (nach Aussage Voynichs) Teil einer Bibliothek des Jesuitenordens war, kann vermutet werden, dass das Manuskript sich zusammen mit dem Nachlass Kirchers im Besitz des Jesuitenordens befand, also zunächst der Bibliothek des "Collegium Romanum" (heute die Päpstliche Universität Gregoriana) gehörte.

Dort blieb es vermutlich, bis der Kirchenstaat im Zuge des Risorgimento von den Truppen Viktor Emanuels II. 1870 annektiert wurde und kirchliches Eigentum von Konfiskation bedroht war. Die Bestände der päpstlichen Universitätsbibliothek wurden eilig den Mitgliedern der Fakultät übertragen, da privater Besitz nicht vom Zugriff des italienischen Staates bedroht war. Darunter befand sich auch der Nachlass Kirchers, der dem damaligen Ordensgeneral Pierre Jean Beckx übergeben wurde. Das Voynich-Manuskript gehörte ausweislich eines Exlibris von Beckx zu diesem Bestand. Beckx’ „Privatbibliothek“ ging schließlich in die Bücherbestände des 1865 gegründeten Jesuitenkollegs "Nobile Collegio Mondragone" in der Villa Mondragone bei Frascati ein.

Dort wurde es vermutlich 1912 von Wilfrid Michael Voynich entdeckt, der es zusammen mit etwa 30 anderen wertvollen Manuskripten den Jesuiten abgekauft haben will. Dazu Voynichs Fundbericht:

Nach Voynichs Tod im Jahre 1930 erbten seine Frau Ethel und Anne Nill, seine langjährige Sekretärin, das Manuskript. Nach dem Tod von Ethel 1960 war Anne Nill seine alleinige Besitzerin. Sie verkaufte es für 25.000 US-$ an den Buchhändler Hans P. Kraus. Dieser wollte es gewinnbringend weiterverkaufen, fand jedoch keinen Käufer und stiftete das Manuskript schließlich 1969 der Yale-Universität, wo es heute zum Bestand der "Beinecke Rare Book & Manuscript Library" gehört.

Es ist umstritten, auf welche Weise das Manuskript in Voynichs Besitz überging. Voynich selbst schwieg sich zeitlebens über die genaue Herkunft des Manuskripts aus. Erst durch einen nach ihrem Tode zu öffnenden Brief von Voynichs Witwe Ethel Lilian Voynich an ihre Erbin und Lebensgefährtin Anne Nill wurde die Herkunft des Manuskripts aus dem Mondragone-Kolleg bekannt.

Das Voynich-Manuskript hat die Form eines Kodex, also eines Buches, das aus mehreren Lagen von Pergament-Blättern zusammengeheftet ist. Das Manuskript bestand ursprünglich aus (mindestens) 20 Lagen, von denen zwei (16 und 18) heute verloren sind. Die meisten Lagen sind Quaternionen, umfassten also ursprünglich acht Blätter, entsprechend 16 Seiten. Die Blätter wurden (vermutlich zu einem späteren Zeitpunkt) mit einer handschriftlichen Zählung (Foliierung) versehen, die von 1 bis 116 läuft. Ausgehend von dieser Foliierung kann ein seither eingetretener Verlust von Lagen und Blättern (nicht alle Lagen sind vollständig) festgestellt werden. Zum heutigen Zeitpunkt besteht der Kodex nicht mehr aus 116, sondern nur noch aus 102 Blättern. Verweise auf Teile des Manuskripts beziehen sich im Allgemeinen auf diese alte Blattzählung.

Einzelne Blätter wurden wegen ihrer Größe mehrfach gefaltet, wodurch sich Unterseiten ergeben (zum Beispiel ist „f. 67r2“ die zweite Unterseite auf der Vorderseite (recto) von Blatt 67). Das Manuskript umfasst gegenwärtig 102 Blätter, darunter fünf Doppel-, drei Dreifach-, ein Vierfach- und ein Sechsfach-Blatt. Das Seitenformat ist ca. 225 mal 160 mm.

Das Manuskript ist in Pergament gebunden. Der Einband trägt weder Titel noch Autorenvermerk.

Da der Text nicht gelesen werden kann, lässt sich eine Gliederung des Inhalts nur auf die Art der Illustrationen stützen. Das Manuskript enthält eine große Zahl von Abbildungen, die in Tinte ausgeführt und nachträglich koloriert wurden. Die Abbildungen entstanden offenbar vor der Niederschrift des Textes, der sich der Form der Abbildungen anpasst und sie umfließt.

Vermutungen über den Inhalt der Abschnitte sind insofern mit Unsicherheiten behaftet, als der kontextuell-ideengeschichtliche Hintergrund unsicher bis unbekannt ist. Die Abbildung eines Löwen in einem Buch über Tierkunde ist beispielsweise ganz anders zu deuten als in einer Sammlung von Fabeln oder in einem alchemistischen Werk. Der sogenannte „balneologische“ Abschnitt etwa enthält zahlreiche Abbildungen nackter Frauen in Wannen (oder vielleicht auch Teichen), die durch komplexe Röhrensysteme miteinander verbunden sind. Je nach Kontext könnten hier dargestellt sein:
oder etwas ganz Anderes.

Entsprechend der offensichtlichen Gruppierung einander ähnlicher Illustrationen wird das Manuskript üblicherweise wie folgt in Abschnitte gegliedert:

Der Abschnitt enthält vorwiegend ganzseitige Abbildungen einzelner Pflanzen, die zwar uns bekannten Pflanzen ähneln, sich jedoch häufig durch entscheidende Details von diesen unterscheiden. Einige Abbildungen erscheinen als größere und genauere Versionen von Abbildungen aus dem Abschnitt „Pharmazie“. Die Gestaltung der Seiten entspricht der von mittelalterlichen und frühneuzeitlichen Kräuterbüchern bekannten Gestaltung.

Hier sind ganzseitige, kreisförmige Diagramme mit Sonne, Mond und Sternen abgebildet. Abgesehen von der Beschriftung der Diagramme enthalten die Seiten nur wenig Text. Eine Folge von zwölf Seiten (f. 70v2–73v) stellt offenbar Tierkreiszeichen dar. Im Zentrum befindet sich eine das jeweilige Tierkreiszeichen darstellende Abbildung, die umgeben ist von konzentrischen Ringen, auf denen sich je einen Stern haltende Frauen im Uhrzeigersinn bewegen. Teilweise sitzen die Frauen in Zubern oder Fässern, teilweise sind sie nackt. Die Folge der Sternzeichen beginnt mit „Fische“ (statt wie üblich mit „Widder“), darüber hinaus sind die Zeichen „Widder“ und „Stier“ zweimal repräsentiert. Die Darstellungen der Sternzeichen „Wassermann“ und „Steinbock“ fehlen und befanden sich vermutlich auf dem fehlenden Blatt 74.

Der sowohl rätselhafteste als auch faszinierendste Abschnitt des Manuskripts stellt auf fast jeder Seite Gruppen nackter Frauen mit gewölbten Bäuchen dar, die in Becken oder Wannen sitzen, die durch Leitungen oder Röhren verbunden sind. Die Leitungen münden häufig in teils organisch, teils mechanisch wirkende End- und Verbindungsstücke. Diese Ambivalenz führte dazu, den Inhalt des Abschnitts sowohl mit anatomischen Gegenständen (z. B. der menschlichen Reproduktion) zu verknüpfen, als auch (dem Augenschein folgend) ihn schlicht als „bäderkundlichen“ (balneologischen) Abschnitt zu bezeichnen.

Die Bezeichnung dieses Abschnitts ist eher eine Verlegenheitsbezeichnung. Sie rührt von der oberflächlichen Ähnlichkeit der Abbildungen mit jenen aus der „astronomischen“ Sektion her. Es handelt sich um kreisförmige, rosettenähnliche Darstellungen, die von teils umfangreichem Textmaterial begleitet sind. Besonders bekannt ist die sogenannte „Rosettenseite“ (f. 85v–86r), die auseinandergefaltet eine quadratische Anordnung von neun miteinander verbundenen „Rosetten“ zeigt.

Zu sehen sind Abbildungen von Pflanzen und Pflanzenteilen mit Beschriftungen sowie von Gefäßen, die an von Apothekern verwendete Behältnisse erinnern, versehen mit einigen kurzen Texten. Vor allem wegen der bunten Gefäße wurden in diesem Abschnitt pharmakologische Inhalte vermutet.

Hier sind kurze Textabschnitte ohne Illustrationen zu finden, die jeweils mit einem Stern-Symbol eingeleitet werden. Man hat vermutet (insbesondere, da diese Sektion auf die „pharmakologischen“ Seiten folgt), dass es sich um Rezepte für Medikamente oder sonstige kurzgefasste Vorgehensanweisungen handelt.

Auf der letzten Seite (f. 116v) findet sich der sogenannte „Schlüssel“: ein dreizeiliger Text, bestehend aus Zeichen, die einem im 15. Jahrhundert in Deutschland verwendeten Schrifttyp ähneln. Dieser kurze Text diente Newbold (siehe unten) als Einstieg für seinen Entschlüsselungsversuch. Er enthält auch angeblich den Namen Roger Bacons in Form eines Anagramms.

Die Gestalt des Textes als solche erscheint nicht ungewöhnlich: geschrieben wurde von links nach rechts (was an dem etwas ungleichmäßigeren rechten Rand erkennbar ist); die einzelnen Schriftzeichen sind durch kleine Zwischenräume voneinander abgehoben; durch größere Zwischenräume gliedert der Text sich in „Wörter“, und es ist bei längeren Textsequenzen so etwas wie eine Absatzgliederung zu erkennen.

Der Schriftduktus erscheint flüssig, als wäre der Schreiber in Sprache und Schrift des Manuskriptes geübt gewesen, im Gegensatz zu den beim „Abmalen“ der Zeichen einer unbekannten Schrift üblichen Unsicherheiten. Das Fehlen von Korrekturen ist ein Indiz dafür, dass eine Vorlage des Textes existierte, von der abgeschrieben wurde. Nach den Untersuchungen von Prescott Currier in den 1970er Jahren lassen sich zwei oder mehrere Schreiber und Schriftstile unterscheiden. Neuere Analysen stellen die Richtigkeit dieser Beobachtung in Frage. Ein anderer Handschriftenexperte, der das Manuskript in Augenschein nahm, konnte nur eine Hand erkennen.

Der Text insgesamt umfasst etwa 170.000 einzelne Glyphen. Da bei manchen Glyphen nicht klar ist, ob sie Repräsentationen eigenständiger Zeichen oder Ligaturen mehrerer Zeichen sind und ob Variationen einzelner Glyphen unterschiedliche Zeichen repräsentieren (wie z. B. „1“ und „l“ in der lateinischen Schrift) oder ob es sich um Formvarianten eines Zeichens handelt (wie z. B. bei „t“ und „t“), kann das dem Voynich-Text zugrundeliegende Alphabet nicht mit Sicherheit bestimmt werden. Insgesamt scheint der Text mit einem Alphabet von 20 bis 30 Zeichen weitestgehend dargestellt werden zu können.

Im Zusammenhang mit der Frage nach dem Voynich-Alphabet stand das Problem der Transkription des Textes. Insbesondere eine Untersuchung des Textes mit Hilfe von Computern setzte eine möglichst adäquate Kodierung der Voynich-Zeichen voraus. Erste Ansätze in dieser Richtung wurden von William und Elisabeth Friedman und ihren Arbeitsgruppen unternommen. In Folge haben sowohl Bennett an der Yale University als auch Prescott Currier eigene Alphabete und Transkriptionsschemata entwickelt. Auf dem Voynich-Symposium von 1976 wurde von Mary D’Imperio eine Vereinheitlichung der Transkription vorgeschlagen, woraufhin man sich auf das von Currier entwickelte Schema einigte.

Es zeigte sich aber, dass dieses Alphabet bei der Darstellung seltener Zeichen und von Ligaturen noch zu wünschen übrig ließ. Dementsprechend wurden neue Alphabete entwickelt, als erstes das von Jacques Guy vorgeschlagene "Frogguy"-Alphabet. Mittlerweile hat sich aufgrund eines breiten Konsenses das sogenannte EVA (European Voynich Alphabet) etabliert. Zu diesem Alphabet wurde auch eine entsprechende Computerschrift "(EVA Hand 1)" entwickelt, mit der die Darstellung transkribierter Voynich-Texte auf dem Computer vereinfacht wird.

Der Text des Manuskripts enthält ca. 35.000 „Wörter“. Diese Wörter weisen phonotaktische Charakteristika ähnlich denen einer natürlichen Sprache auf, d. h.,

Die statistische Analyse des Textes offenbart weitere Ähnlichkeiten mit natürlichen Sprachen:

Andere Eigentümlichkeiten des Voynich-Textes finden sich jedoch in europäischen Sprachen nirgends. Zum Beispiel gibt es kaum Wörter mit mehr als zehn, aber auch kaum welche mit weniger als drei Zeichen. Weiter scheint es initiale und finale Buchstabenformen zu geben, also Sonderformen von Zeichen am Wortanfang und -ende, wie sie in semitischen Sprachen gebräuchlich sind. Und schließlich erscheinen unmittelbare Wiederholungen des gleichen Wortes oder kleinere Varianten mit ungewöhnlicher Häufigkeit.

Voynich war angesichts des Marci-Briefes schnell zu der Überzeugung gelangt, Roger Bacon (gest. 1292/94) sei der Autor des Manuskripts. In den folgenden Jahren bemühte er sich, die Provenienz des Manuskripts zu klären. Von der Annahme, Bacon sei der Autor, gelangte er zu der Hypothese, der englische Mathematiker und Mystiker John Dee sei in den Besitz der Handschrift gelangt – und jener Unbekannte, der das Manuskript später an Rudolf II. verkaufte. Eine Annahme, die auf der Kenntnis basierte, dass Dee eine Sammlung von Schriften Bacons besaß und sich zusammen mit dem Spiritisten Edward Kelley in den 1580er Jahren am Hof Rudolfs II. aufhielt.

Eine Entschlüsselung des Textes hatte Voynich nicht versucht. Er verschickte vielmehr ab 1919 Kopien des Manuskripts an verschiedene Fachleute. Einer von diesen war Newbold.

Newbold war Dozent für Philosophie an der University of Pennsylvania in Philadelphia. Er hörte schon 1915 von dem Manuskript, beschäftigte sich damit aber erst nach 1919, nachdem er von Voynich drei Seiten in Photokopie erhalten hatte. Schon nach wenigen Stunden meinte er, einen Schlüssel gefunden zu haben.

In der Folge entwickelte er die Theorie einer Mikroschrift. Demnach sollte der eigentliche Inhalt des Manuskripts in mikroskopisch kleinen Unregelmäßigkeiten der Voynich-Zeichen versteckt sein. Bei genauer Betrachtung würden darin altgriechische Kurzschriftzeichen erkennbar. Der so gelesene Text wurde von Newbold einem weiteren Dechiffrierungsschritt unterzogen. Das Resultat bestätigte ihm nicht nur die Urheberschaft Bacons, darüber hinaus verriet es angeblich auch, dass Bacon nicht nur über ein Mikroskop verfügt habe, sondern dass ihm schon die Spiralstruktur des Andromedanebels bekannt gewesen sei.

Über ihre Ergebnisse berichteten Voynich und Newbold im April 1921 in mehreren Vorträgen vor dem "College of Physicians" und der "American Philosophical Society" in Philadelphia. Obwohl erste (vermeintliche) Erfolge sich schnell eingestellt hatten, gestaltete sich die weitere Entzifferung ausgesprochen mühsam. Bevor Newbold eine vollständige Decodierung an Voynich liefern konnte, starb er überraschend im September 1926.

Kent, ein Freund Newbolds und Professor für vergleichende Philologie an der University of Pennsylvania, kann nicht als Voynich-Forscher im engeren Sinn gelten. Vielmehr unterzog er sich der Aufgabe, den Nachlass seines früh verstorbenen Freundes Newbold zu ordnen und zu edieren. 1928 erschien der von ihm herausgegebene Band "The Cipher of Roger Bacon", der dem wissenschaftlichen Ruf seines Freundes erheblich schaden, der Voynich-Forschung jedoch sehr nützen sollte, da der Band erstmals Reproduktionen des Manuskriptes im Druck verfügbar machte. Er rief allerdings auch Kritiker auf den Plan.

Manly, Professor für englische Sprache an der University of Chicago und während des Ersten Weltkrieges Kryptoanalytiker im militärischen Nachrichtendienst der USA, hatte die Forschungen Newbolds schon einige Zeit mit Interesse, aber auch mit Skepsis verfolgt, was aus einem 1921 veröffentlichten Artikel „Das geheimnisvollste Manuskript der Welt“ in der US-Zeitschrift „Harpers“ ersichtlich wird. Auf die Publikation der „Ergebnisse“ meinte er reagieren zu müssen, da er befürchtete, unwidersprochen würden die Thesen Newbolds ungefiltert Eingang in die Geistesgeschichte finden. 1931 veröffentlichte er daher eine vernichtende Kritik an Newbolds Methoden und Ergebnissen.

Er zeigte darin, dass die Mikroschrift nur in der Phantasie Newbolds vorhanden war, dass es sich vielmehr bei den vermeintlichen Kürzeln um Unregelmäßigkeiten bei Auftrag und Abblättern der Tinte auf dem rauen Schreibmaterial handelte. Darüber hinaus wies er darauf hin, dass das von Newbold verwendete Verfahren der Dechiffrierung eine sichere Wiederherstellung eines Originaltextes gar nicht zuließ, vielmehr musste der Dechiffrierer den zu dechiffrierenden Inhalt schon kennen (was eben bei Newbold der Fall war, der genau das fand, was er zu finden hoffte).

Feely, ein Anwalt aus Rochester in Maine, stützte seinen Entschlüsselungsversuch lediglich auf eine Abbildung der Manuskriptseite 78r in Newbolds Buch. Er kam zu dem Ergebnis, es handele sich um eine Chiffrierung durch Alphabetsubstitution (d. h., jedes Zeichen des Alphabets wird regelhaft durch ein bestimmtes anderes Zeichen ersetzt, in diesem Fall durch ein Voynich-Zeichen). Als Klartextsprache nahm er Latein an. Eine solch einfache Verschlüsselung könnte natürlich bei der vorhandenen Textmenge aufgrund von Häufigkeitsanalysen auch ohne Computer dechiffriert werden, wie Edgar Allan Poe in seiner Erzählung "Der Goldkäfer" vorführt.

Feely nahm daher weiter an, zuvor seien die lateinischen Wörter durch willkürliches Weglassen von Buchstaben abgekürzt worden. Das angenommene Element der Willkür in der Verschlüsselung hat zur Folge, dass die Entschlüsselung auf einem gehörigen Maß an Subjektivität beruht und damit Irrtümer ermöglicht. Dass der von Feely entschlüsselte Text keinen Sinn ergab, wäre angesichts der üblichen Hermetik frühneuzeitlicher alchemistischer Texte zu tolerieren gewesen. Hätte Feelys Entschlüsselung jedoch zugetroffen, hätte sie auf den von ihm nicht analysierten Seiten ebenfalls zu akzeptablen Lesungen führen müssen. 

O’Neill war ein Botaniker an der Catholic University of America und hatte von einem Kollegen einen Satz Photokopien des Voynich-Manuskripts erhalten. Er versuchte, die in den botanischen Abschnitten abgebildeten Pflanzen zu identifizieren, was bei mittelalterlichen Manuskripten häufig schwierig, im Fall des Voynich-Manuskripts nahezu unmöglich ist. Dennoch meinte O’Neill zwei Pflanzen eindeutig bestimmen zu können, nämlich auf Blatt 93r eine Sonnenblume und auf Blatt 101v eine Art des Spanischen Pfeffers.

Das Bemerkenswerte bei diesen Identifizierungen war, dass beide Gewächse in der Alten Welt vor Kolumbus nicht heimisch waren, das Manuskript demnach erst nach 1493 entstanden sein könnte. Das wiederum hieße, dass Roger Bacon nicht der Autor sein kann.

William Friedman war wohl der erste ausgewiesene Experte für Kryptologie, der sich mit dem Voynich-Manuskript befasste. Er war Gründer des Signals Intelligence Service der US-Armee (einer der Vorläuferorganisationen der heutigen NSA). Unter seiner Leitung wurde während des Zweiten Weltkriegs der japanische PURPLE-Code entschlüsselt.

Friedman hatte in den Kriegsjahren einen Vortrag Newbolds gehört und später mit Manly an der Widerlegung der Theorien Newbolds gearbeitet. Im Mai 1944 gründeten die beiden eine Arbeitsgruppe, deren Aufgabe die maschinenlesbare Transkription des Voynich-„Textes“ mittels Lochkarten sein sollte. Die Aufgabe wurde nicht vollendet, da die Gruppe mit Kriegsende auseinanderfiel. Unter Voynich-Forschern ist die Gruppe um Friedman (und das von ihr entwickelte Transkriptionsschema) als "FSG" "(First Study Group)" bekannt.

Das Voynich-Manuskript scheint Friedman und seine Ehefrau Elisabeth weiter beschäftigt zu haben, da er Ende der 50er Jahre in der Fußnote eines Aufsatzes eine als Anagramm verschlüsselte Hypothese zum Voynich-Code publizierte. Die Auflösung wurde erst nach seinem Tod 1970 bekannt:
Unter einer künstlichen oder universellen Sprache versteht man eine Plansprache oder logische Sprache. Vom „A-priori“-Typ ist eine solche Sprache dann, wenn sie sich nicht allgemeiner Verständlichkeit halber an existierende Sprachen anlehnt, sondern wenn sie in ihrer Konstruktion logisch-philosophischen Prinzipien folgt.

Konsequenzen dieser Hypothese für den Voynich-Text wären:

Im September 1962 initiierten die Friedmans eine weitere Arbeitsgruppe "(SSG, Second Study Group)" mit dem Ziel, automatische Datenverarbeitung zur Entschlüsselung des Voynich-Codes einzusetzen. Dieses Mal sollte ein RCA-301-Computer eingesetzt werden, zu dem die Gruppe außerhalb der normalen Betriebszeiten Zugang hatte. Sie wären damit die ersten Voynich-Forscher gewesen, die einen Computer zur Entschlüsselung verwendeten. Es kam jedoch nicht dazu, da RCA die Nebennutzung für diesen Zweck untersagte. Die Gruppe löste sich im Sommer 1963 auf.

Robert Brumbaugh war Professor für Philosophie des Mittelalters an der Yale University, hatte also im Gegensatz zu anderen Voynich-Forschern die Möglichkeit, das Dokument im Original in Augenschein zu nehmen – zu einer Zeit, in der nur wenige Seiten als (schwarz-weißes) Faksimile publiziert bzw. als Photokopie in Umlauf waren, ein unschätzbarer Vorteil. Darüber hinaus gelang es ihm, einen Forschungsauftrag für die Untersuchung des Manuskripts zu erhalten. Er veröffentlichte in den 1970er Jahren eine Reihe von Artikeln zum Thema und fasste in der 1978 erschienenen Monographie "The Most Mysterious Manuscript" den damaligen Stand der Forschung zusammen. Brumbaugh selbst entwickelte aufgrund der Ähnlichkeit einiger Voynich-Zeichen mit altertümlichen Ziffernformen die Theorie, dass die Voynich-Zeichen (dezimale) Ziffern seien, wobei jeder Ziffer mehrere Buchstaben des lateinischen Alphabets zugeordnet seien. Ähnlich wie beim Ansatz von Feely enthielte auch eine solche Kodierung ein Element der Mehrdeutigkeit, entsprechend enthalten die Dekodierungen ein stark subjektives Element. Auch die von Brumbaugh vorgelegten „Entschlüsselungen“ ergaben keinen (offensichtlichen) Sinn.

Prescott Currier war ursprünglich Sprachwissenschaftler (B. A. in Romanistik und Diplom in vergleichenden Sprachwissenschaften). Ab 1935 begann er sich mit Kryptologie zu beschäftigen. 1940 in der US-Marine dienstverpflichtet, arbeitete er 1941 als amerikanischer Liaison-Offizier in Bletchley Park in England, um die kryptoanalytischen Bemühungen der amerikanischen und englischen Dienste zu koordinieren. Von 1948 bis 1950 war er Direktor der "Naval Security Group."

Currier hatte in England die Bekanntschaft von John Tiltman gemacht, der wiederum von Friedman zur Beschäftigung mit dem Voynich-Manuskript angeregt worden war. Auch Currier sollte sich über viele Jahre mit dem Rätsel des Manuskripts beschäftigen. Wichtigstes Resultat seiner Untersuchungen war, dass – anders, als bis dahin stets angenommen – das Manuskript mehr als einen Schreiber hat. Currier stellte fest, dass zwei Schreibstile – und mehr noch: zwei „Sprach“stile – deutlich unterscheidbar sind. Diese beiden Voynich-Varianten werden heute mit "Currier-A" bzw. "Currier-B" bezeichnet. Er stellte seine Ergebnisse 1976 auf einem von Mary D’Imperio veranstalteten Seminar vor.

Die Mathematikerin Mary D’Imperio war wie Friedman Kryptoanalytikerin (zeitweise Beraterin der NSA). Persönlich bekannt mit John Tiltman (der zusammen mit Friedman die These aufgestellt hatte, dass dem Voynich-Manuskript eine künstliche Sprache zugrundeliege) und Prescott Currier, begann sie Ende der 1970er, sich intensiv mit dem Voynich-Manuskript zu beschäftigen. Sie organisierte das erste wissenschaftliche Symposium zum Thema Voynich, das im Jahr 1976 stattfand, und veröffentlichte die Resultate in einem Tagungsband sowie in dem heute noch als beste Überblicksarbeit geschätzten Band "The Voynich Manuscript: An Elegant Enigma." In ihren Arbeiten zum Voynich-Manuskript befasste sie sich mit Fragen der Transkription und des Zeichenvorrates. Sie wies unter anderem auf die Ähnlichkeiten zwischen den Voynich-Zeichen und einigen in Mittelalter und Renaissance gebräuchlichen lateinischen Kürzeln hin.

Gordon Rugg von der britischen Keele-Universität beschäftigte sich etwa ab 1997 mit der Frage, wie der Text des Voynich-Manuskripts entstanden sein könnte. Dazu erstellte Rugg eine Tabelle mit zufälligen Zeichenkombinationen, die dann als Vor-, Mittel- oder Nachsilben neuer „Wörter“ dienten. Über diese Tabelle schob er ein sogenanntes Cardan-Gitter, eine Schablone mit drei Fenstern, wie sie im 16. Jahrhundert zur Verschlüsselung von Texten verwendet wurde. Die Zeichenfolgen, die jeweils in den drei Fenstern erschienen, wurden transkribiert, und eine dreisilbige unverständliche „Sprache“ entstand, die große Ähnlichkeit mit dem Text des Voynich-Manuskriptes aufwies. Im Dezember 2003 gab Rugg seine Forschungsergebnisse bekannt. Seiner Ansicht nach handelt es sich bei dem Voynich-Manuskript um einen mittelalterlichen Schabernack, um wirres Geschwafel ohne Sinn und Gehalt.

Die Schabernack-Hypothese wird auch durch eine Textanalyse des österreichischen Wissenschaftlers Andreas Schinner gestützt: Er entdeckte unnatürliche Regelmäßigkeiten in der Wortfolge des Manuskripts, die in Texten, die in natürlichen Sprachen verfasst sind, nicht vorkommen. Der theoretische Physiker kommt daher ebenfalls zu dem Schluss, dass das Voynich-Manuskript das raffinierte Werk eines Schelms ist und lediglich bedeutungslosen Unsinn enthalte.

Im Jahr 2009 konnte mittels einer Radiokarbonanalyse die Entstehung des Pergamentbandes mit höchster Wahrscheinlichkeit auf zwischen 1404 und 1438 bestimmt werden. Die Verschlüsselungstechnik mit dem Cardan-Gitter müsste also auf einem schon damals älteren Pergament angewandt worden sein. Außerdem erscheint die Schabernack-Hypothese insofern, als die Anfertigung dieses Manuskriptes nicht nur ein extrem kostspieliges Unterfangen war (damals sehr teures Pergament, sehr teure, hochqualitative Tintenfarben), sondern auch viele Jahre in Anspruch genommen haben muss, als äußerst unwahrscheinlich.

Marcelo Montemurro von der University of Manchester und Damián Zanette vom Centro Atómico Bariloche e Instituto Balseiro veröffentlichten 2013 eine Arbeit mit dem Titel „Keywords and Co-Occurrence Patterns in the Voynich Manuscript: An Information-Theoretic Analysis“ bei PLOS ONE. In dem Artikel geben sie an, semantische Muster im Voynich-Manuskript identifiziert zu haben. Demnach könnte das Manuskript einen Geheimtext mit einer „authentischen Botschaft“ darstellen.

Nach Ansicht zweier amerikanischer Botaniker, Arthur O. Tucker und Rexford H. Talbert, zeigt das Voynich-Manuskript Pflanzen mittelamerikanischer Herkunft.

Dies könnte darauf hindeuten, dass das Voynich-Manuskript in Mittelamerika gezeichnet und in einer mittelamerikanischen Sprache geschrieben wurde, also möglicherweise in einer Sprache, die heute nicht mehr gesprochen wird.

Nick Pelling stellt seine Theorie zur Autorenschaft des Voynich-Manuskript in seinem Buch „The Curse of the Voynich“ vor. Aufgrund der Illustrationen im Voynich-Manuskript – speziell jene auf den ausklappbaren Rosetten-Folios – vermutet Pelling, dass das Manuskript aus der Gegend von Mailand stammt und um die Mitte des 15. Jahrhunderts oder etwas später datiert. Aufgrund biographischer Hinweise vermutet er als möglichen Autor den Architekten Antonio Averlino, auch Filarete genannt. Neben dieser Theorie hat Nick Pelling weitere wichtige Beobachtungen zum Manuskript gemacht (z. B. was die Reihenfolge und Bindung der einzelnen Folios im historischen Ablauf betrifft sowie zur Schrift, zur Chiffre und den möglichen kryptologischen Erklärungen dafür).

Stephen Bax, ein Professor für Angewandte Linguistik, gibt an, er habe in dem Text insgesamt 10 Wörter, nämlich verschiedene Pflanzennamen sowie die Namen von Sternbildern, entziffert. Seiner Meinung nach ist der Text in einer semitischen Sprache verfasst, und die Entdeckung dieser Namen könnte ähnlich wie bei den ägyptischen Hieroglyphen den Durchbruch zur Entschlüsselung des Textes darstellen.

2016 veröffentlichte eine Gruppe um Andronik Aramowitsch Artjunow vom Keldysch-Institut der RAN eine neue Lösung. Nach ihnen seien Vokale und Leerzeichen entfernt worden. 30 % des Textes seien in Dänisch oder Deutsch, der Rest in einer romanischen Sprache (Latein oder Spanisch) verfasst.

Greg Kondrak, ein Professor für Computerlinguistik an der Universität von Alberta, nutzte Verfahren aus dem Bereich der künstlichen Intelligenz, um verschlüsselte Texte zu analysieren, und wandte diese auch auf das Voynich-Manuskript an. Die Ergebnisse wurden 2016 in Form eines Artikels präsentiert, der von Kondrak und einem seiner Studenten verfasst wurde. Dem zufolge sei die Sprache des Manuskripts mit einer gewissen Wahrscheinlichkeit hebräisch, wobei die einzelnen Wörter Anagramme mit fehlenden Vokalen seien. Allerdings konnten auf diese Weise nur kleine Fragmente entschlüsselt und übersetzt werden. Erst nach einigen manuellen Korrekturen konnte ein einzelner Satz ermittelt werden, der halbwegs Sinn ergab. Die beiden räumten außerdem ein, dass Experten für mittelalterliche Manuskripte nicht von den Ergebnissen überzeugt waren.

Das Voynich-Manuskript war in der ersten Hälfte des 20. Jahrhunderts nur wenigen Spezialisten bekannt. Im Laufe der letzten Jahrzehnte jedoch stieg der Bekanntheitsgrad, wodurch das Voynich-Manuskript Eingang in Werke der populären Kultur fand und Büchern, Bildern, Musik bis hin zu Computerspielen als Inspiration diente:



Die Kurzgeschichte "The Return of the Lloigor" von Colin Wilson gehört zum Kreis der Werke um den Cthulhu-Mythos, einem fiktiven Mythenkreis, basierend auf den Erzählungen von Howard Phillips Lovecraft. In diesen Erzählungen taucht immer wieder ein Buch auf, das grausige Necronomicon des wahnsinnigen Arabers Abdul Al’Hazred. Das Necronomicon enthält in verrätselter Form Beschwörungsformeln, mit deren Hilfe dämonische Wesen aus grausiger Urzeit auf die Welt der Menschen losgelassen werden können. In der Erzählung von Wilson entpuppt sich das Voynich-Manuskript als eine unvollständige Kopie des Necronomicons. Seitdem wurde die Verbindung des fiktiven Necronomicons zum realen Voynich-Manuskript von anderen Autoren der Horrorliteratur weiter ausgebaut.

Das Necronomicon erscheint in den Erzählungen von H. P. Lovecraft erstmals 1922 in der Erzählung "The Hound," zwei Jahre, nachdem Voynich Kopien an interessierte Forscher versandt hatte, und ein Jahr nachdem die ersten Ergebnisse von Voynich und Newbold durch die Vorträge in Philadelphia publik gemacht worden waren. Die zeitliche Nähe regt zwar zu Spekulationen an, jedoch ist eine Erwähnung des Voynich-Manuskripts in der sehr umfangreichen Korrespondenz Lovecrafts nicht belegt.

Immerhin erscheint John Dee in der fiktiven Publikationsgeschichte des Necronomicons als Übersetzer, was allerdings wenig besagt, da Dee unabhängig vom Voynich-Manuskript in esoterischen Kreisen – ähnlich darin Bacon – eine prominente Figur ist. Sollte das Voynich-Manuskript, über das in der amerikanischen Presse vielfach berichtet wurde (allein in der New York Times erschienen 1921 vier Artikel), Lovecrafts Aufmerksamkeit entgangen sein, wäre das einigermaßen erstaunlich.

Das Voynich-Manuskript ist ein wesentlicher Bestandteil des Romans "Indiana Jones und der Stein des Weisen". Es soll hier den Weg zu dem Grab des Hermes weisen, wo sich der Stein des Weisen befinden soll.

Im Jahr 2015 veröffentlicht das Autorenpaar Achim Engstler und Astrid Dehe den Roman "Unter Schwalbenzinnen", in dem sie die Geschichte des Entstehens des Manuskripts erzählen. Eine florentinische Patriziertochter „malt“ erzählend Bilder, die sie einem Kopisten im Jahr 1442 berichtet. Der Kopist begreift nicht jedes Bild und kann auch nicht einhundertprozentig folgen. Er zeichnet deswegen bestimmte Schilderungen (siehe obige Bilder). Der Kopist besitzt ein altes Buch in unbekannter Sprache. Die Visionen sind nicht ungefährlich in einer Zeit der Ketzerverfolgung und der uneingeschränkten Macht der Familie der Medici.

Im Jahr 2017 erschien „Das verdammte Manuskript“ des österreichischen Autors Harald A. Jahn im Wiener Verlag PROverbis. In dem als Mystery-Thriller deklarierten Roman entdeckt ein Wissenschaftler im Paris des ausklingenden 21. Jahrhunderts alte Drucklettern mit Glyphen des Voynich-Alphabets und Pergamente, die älter sind als das Manuskript, aber dieselben Zeichen enthalten. Bei seiner Recherche entdeckt er den mittelalterlichen Entstehungsort und kommt mithilfe einer rätselhaften Helferin dem Geheimnis auf die Spur.








</doc>
<doc id="5480" url="https://de.wikipedia.org/wiki?curid=5480" title="Vietnam">
Vietnam

Vietnam (vietnamesisch "Việt Nam" [in Hanoi ], Bedeutung: "Viet des Südens", amtlich "Sozialistische Republik Vietnam", vietnamesisch "Cộng hòa Xã hội chủ nghĩa Việt Nam", Chữ Nôm "共和社會主義越南" [in Hanoi ]) ist ein langgestreckter Küstenstaat in Südostasien. Er grenzt an China, Laos, Kambodscha, den Golf von Thailand und das Südchinesische Meer.

Das erste historisch belegte Königreich auf dem Gebiet des heutigen Vietnams entstand im 1. Jahrtausend v. Chr. Danach entwickelte sich ein friedliches Zusammenleben zwischen den Yues und den Han während der Trieu-Dynastie. 111 v. Chr. kam die Dynastie unter die Kontrolle der Han-Chinesen als Provinz der Han-Dynastie und blieb dies – unterbrochen von kurzen Zeiträumen der Unabhängigkeit – bis 938 n. Chr., als sie nach der Schlacht am Bạch Đằng-Fluss die Unabhängigkeit errang. Danach folgte eine Blütezeit der Kultur, Gesellschaft, Wirtschaft und Politik. In den folgenden Jahrhunderten expandierte Vietnam nach Süden. Im 19. Jahrhundert kam das Gebiet nach und nach als Teil von Französisch-Indochina unter französische Kolonialherrschaft.

Im Zweiten Weltkrieg besetzte Japan die Region, in einem mehrjährigen Krieg versuchte Frankreich anschließend ohne Erfolg, die Kolonialherrschaft wiederherzustellen. Als Folge der französischen Niederlage wurde Vietnam 1954 in das sozialistische Nordvietnam (Hauptstadt Hanoi) und das von den Westmächten unterstützte Südvietnam (Hauptstadt Saigon) geteilt. 1976 wurden die beiden Staaten nach dem Vietnamkrieg unter kommunistischer Führung wiedervereinigt. Seit 1986 laufen im Rahmen des "Đổi mới" marktwirtschaftliche Reformen, die aber bislang nur in Ansätzen zu einer politischen Liberalisierung führten. Hanoi wurde 1976 Hauptstadt des wiedervereinigten Vietnams, größte Stadt nach Einwohnern ist Ho-Chi-Minh-Stadt (Saigon); Haiphong, Cần Thơ und Đà Nẵng sind ebenfalls bedeutende Metropolen des Landes.

Vietnams Fläche entspricht ungefähr 93 % jener Deutschlands. Das Land umfasst die weiten Ebenen der Flussdeltas von Rotem Fluss und Mekong, die gesamte östliche Festlandküste Südostasiens sowie die langen Gebirgszüge und Hochebenen des Hinterlandes. Die Nord-Süd-Ausdehnung beträgt etwa 1650 km, die Ost-West-Breite bis zu 600 km, während die schmalste Stelle in Mittelvietnam nur 50 km breit ist.

Die Geographie Vietnams wird auch als „Bambusstange mit zwei Reisschalen“ beschrieben: Im Norden und Süden liegen zwei fruchtbare reisliefernde Flussdeltas, dazwischen als Verbindung ein schmales, eher karges, von Wald und Gebirge geprägtes Gebiet.
Insgesamt ist Vietnam zu von Bergen und Hochebenen überzogen.

Von Nord nach Süd werden fünf Landschaften unterschieden:


Das Klima unterscheidet sich erheblich zwischen Nord- und Südvietnam. Der Norden weist ein gemäßigtes tropisches Wechselklima auf, es gibt eine kühle Jahreszeit von November bis April und eine heiße von Mai bis Oktober. Der Süden ist tropisch: warm bis sehr heiß während des ganzen Jahres, etwas kühler von November bis Januar, heiß von Februar bis Mai und mit einer Regenzeit zwischen Mai und Oktober. Die Wetterscheide zwischen diesen Gebieten bildet der "Wolkenpass" nördlich von Đà Nẵng.

Während der Regenzeit wüten häufig Taifune, die besonders im Mekongdelta, aber auch in anderen Küstenregionen Überschwemmungen anrichten können.

Die zwei mit Abstand wichtigsten Städte sind die Hauptstadt Hanoi ("Hà Nội") und die größte Stadt Vietnams Ho-Chi-Minh-Stadt ("Thành phố Hồ Chí Minh", früher "Saigon"). Während letztere eine der schnellstwachsenden Boomstädte der Welt ist und als wirtschaftliches Zentrum des ASEAN verstanden wird, hat Hà Nội den Ruf, ruhiger und eleganter zu sein. In der Tat ist Hà Nội in wirtschaftlichen Belangen gegenüber der südlichen Metropole recht weit im Hintertreffen.

Weitere wichtige Städte sind die Hafenstädte Đà Nẵng, Hải Phòng und Nha Trang, die in ihrem Stadtbild teilweise stark französisch geprägt sind. Dies ist unter anderem an den Kirchen und Villen der Städte zu erkennen. Die Städte Huế als Hauptstadt während der letzten Kaiserdynastie und die kaiserliche Sommerresidenz Đà Lạt im südlichen Hochland sind von großer geschichtlicher Bedeutung und ziehen viele Besucher an. Für Touristen ist auch die Handelsstadt Hội An interessant, da ihre zum UNESCO-Weltkulturerbe erklärte Altstadt sehr gut erhalten ist. Reine Industriestädte sind hingegen Vinh, Ninh Bình, Mỹ Tho oder Bến Tre.

Die gesamte Küste ist mit touristisch teils unerschlossenen Stränden übersät. Beispiele dafür sind Mũi Né, Long Hải und Vũng Tàu am Südchinesischen Meer sowie Hà Tiên am oder die Insel Phú Quốc im Golf von Thailand.

Der Einsatz von Umweltgiften durch die USA während des Vietnamkrieges hat die vietnamesische Natur nachhaltig geschädigt. Vor allem dioxinhaltige Herbizide wie Agent Orange, von dem die US-Luftwaffe etwa 40 Millionen Liter über dem Land versprühte, zeigen in großen Landstrichen nach wie vor Wirkung, da sie sich nur sehr langsam zersetzen und eine Halbwertszeit von etwa einem Jahrzehnt haben. So wurde während des Krieges etwa die Hälfte der Mangrovensümpfe zerstört, die sich nicht selbst regenerieren können. Die entlaubten Hänge im Landesinneren können nach wie vor nicht aufgeforstet werden, denn es können sich nur sehr widerstandsfähige Gräser halten, die während der Trockenzeit sehr anfällig für Flächenbrände sind. In der Regenzeit kommt es in diesen Regionen daher zu extrem starker Erosion.

Unter den Spätfolgen des Dioxineinsatzes haben nicht nur jene immer noch zu leiden, die damals direkt damit in Berührung kamen (Hautverätzungen, Chlorakne, Krebs). Das Gift fand auch seinen Weg in die Nahrungskette, was, durch die dadurch verursachte Schädigung des Erbgutes, unter anderem in signifikant erhöhten Zahlen an Fehl-, Tot- und Missgeburten seinen Niederschlag findet.

Neben Umweltgiften sind in den ländlichen Gebieten auch noch eine große Zahl von Blindgängern und Landminen zu finden. Nach wie vor werden jedes Jahr Bauern und Altmetallsucher von explodierender Munition getötet oder verletzt.

Millionen Hektar der tropischen Wälder, die zuvor bereits unter den Herbiziden zu leiden hatten, wurden seit den 1960er Jahren durch Brandrodung und Abholzung zerstört. Besonders betroffen hiervon ist der teils schwer zugängliche Norden. Zwar versucht die Regierung dem Einhalt zu gebieten, aber der Druck der schnell wachsenden Bevölkerung und die Armut in den Bergprovinzen veranlassen die Leute immer wieder dazu, Wald niederzubrennen, um Ackerland zu gewinnen. Tropenhölzer, wie das Teakholz, werden in Vietnam wie in ganz Südostasien trotz inzwischen strenger gesetzlicher Regelungen nach wie vor illegal gewonnen, um daraus Möbel für den europäischen, US-amerikanischen und japanischen Markt zu fertigen.

Es gibt Programme mit teils großer ausländischer Hilfe, die das Umweltbewusstsein der Vietnamesen stärken sollen. Regierung und Umweltorganisationen setzen große Hoffnungen in die Entwicklung des Ökotourismus. Sie haben bereits mehrere Nationalparks eingerichtet – den ältesten davon schon 1962 –, und einige Landschaften des Landes stehen unter besonderem Schutz der UNESCO.

Vietnam hat eine artenreiche Tierwelt, die jedoch durch die fortschreitende Zerstörung der Wälder und Wilderei bedroht ist. So leben nach neueren Schätzungen nur mehr rund 200 Tiger und weniger als 60 Asiatische Elefanten dort, deren Überleben fraglich ist. Die Java-Nashörner, die in Vietnam lange nur noch auf das Gebiet des Cat-Tien-Nationalparks beschränkt waren, sind bereits 2010 durch Wilderei ausgerottet worden. Außerhalb Vietnams leben die seltenen Tiere nur noch im Ujung Kulon-Nationalpark auf der Insel Java. Weitere Säugetiere umfassen Primaten (Schopfgibbons, Plumploris, Languren, Makaken), Raubtiere (darunter Malaienbären, Marmorkatzen sowie etliche Schleichkatzenarten), Paarhufer (Kantschile, Muntjaks, Hirsche, Bantengrinder, Gaure) sowie zahlreiche Fledermaus- und Nagetiergattungen. Die Vogelwelt ist ebenfalls artenreich, dazu gehören Fasane, Nashornvögel, Eulen, Greifvögel, Reiher und zahlreiche Singvögel. Auch Krokodile, Schlangen, Echsen und Frösche sind in diesem Land beheimatet, dazu zahllose Arten von Insekten und Wirbellosen. In den 1990er-Jahren wurden mehrere neue Arten Vietnams beschrieben, darunter das Vu-Quang-Rind und mehrere Muntjakarten. Das Vu-Quang-Rind wird im Vu-Quang-Nationalpark geschützt.

Die Bevölkerungszahl Vietnams beträgt Schätzungen zufolge zwischen 86,9 Millionen und 91,5 Millionen. Die Bevölkerung ist im Schnitt sehr jung: Landesweit waren 2005 etwa 32 % der Menschen unter 14 Jahre alt und nur etwa 5,6 % sind über 65. Das Bevölkerungswachstum wird auf 1,3 % bis 1,4 % geschätzt. Tendenziell sinkt die Geburtenrate (2005: 17,07 Geburten pro 1000 und 1,94 Kinder pro Frau), während aufgrund verbesserter medizinischer Bedingungen die Sterberate ebenfalls sinkt (2005: 6,2 pro 1000). Die Lebenserwartung lag im Zeitraum von 2010 bis 2015 bei insgesamt 75,7 Jahren (70,7 Jahren für Männer und 80,3 Jahren für Frauen).

Während die vietnamesische Bevölkerung von westlichen Beobachtern als durchweg jung wahrgenommen wird, beginnt Vietnam sich darauf einzustellen, dass die Bevölkerung in eine Phase der Alterung eingetreten ist. Am 1. April 2010 erreichte die Zahl der über 60-Jährigen den Stand von 8,1 Millionen; das sind 9,4 % der Gesamtbevölkerung und bedeutet einen Zuwachs von 4 % gegenüber 2009. Vietnam gehört damit zu den Ländern mit einer außergewöhnlich schnellen Alterung der Gesamtbevölkerung. Während es in Schweden 85 Jahre, in Japan 26 Jahre und in Thailand 22 Jahre dauerte, um den Status "alternde Bevölkerung" nach den Richtlinien der UNFPA (United Nations Population Fund) zu erreichen, dauerte dies in Vietnam nur 20 Jahre. Die schnelle Alterung der Bevölkerung ist darauf zurückzuführen, dass die Fertilitätsrate von über 5 Kinder pro Frau in den 70er Jahren auf heute noch 2,0 Kinder zurückgegangen ist.

Die Mehrheit der Bevölkerung lebt in den dichtbesiedelten Gebieten der Mündungsdeltas von Rotem Fluss und Mekong, in denen Landwirtschaft vorherrscht. Trotz der agrarischen Prägung lebten 2016 bereits rund 34 % der Vietnamesen in den urbanen Regionen der großen Städte (in den 1980er Jahren waren es nur 15 %), und die Zuwanderung aus den wirtschaftlich wenig entwickelten ländlichen Gebieten nimmt stetig zu. Dazu kommt eine Wanderungsbewegung von Norden in Richtung Süden. In Vietnam existiert selbst kein privates Eigentum an Grund und Boden. Der vietnamesische Staat erteilt Landnutzungsrechte, deren durchschnittliche bewilligte Nutzungsdauer rund 50 Jahre beträgt.

Knapp 2,5 Millionen leben im Ausland, die meisten davon sind im Vietnamkrieg geflüchtet oder mussten das Land aufgrund von politischer Verfolgung verlassen. Knapp 1,3 Millionen davon leben in den Vereinigten Staaten und 125.000 in der ehemaligen Kolonialmacht Frankreich. Die Rücküberweisungen der Exil-Vietnamesen sind eine äußerst wichtige Einnahmequelle für die Verwandten in der Heimat. In Vietnam selbst sind nur 0,1 % der Einwohner im Ausland geboren, womit das Land eines der homogensten weltweit ist.

Entwicklung der Bevölkerung über Zeit

Quelle: UN, Zahlen für 2030 und 2050 sind Prognosen

Etwa 88 % der Bevölkerung sind ethnische Vietnamesen ("Việt" oder "Kinh"). Daneben sind 53 ethnische Minderheitengruppen anerkannt. Die größte davon sind die „Auslandschinesen“ (vietnam.: "Hoa"), deren Zahl auf etwa 1,2 Millionen geschätzt wird. Die Mehrzahl von ihnen sind Nachfahren von Einwanderern, die 1644, nach dem Zusammenbruch der Ming-Dynastie, ins Land gekommen waren. Weitere Volksgruppen sind Thái, Khmer (vor allem im Süden, der Region des Mekongdelta, die über Jahrhunderte zu Kambodscha gehörte) und die unter der Sammelbezeichnung „Bergvölker“ (Montagnards) bekannten Bewohner der Bergregionen. Letztere, die als die ursprünglichen Bewohner des kontinentalen Südostasien gelten, wurden im Verlauf der Geschichte in Vietnam, Thailand, Myanmar und Laos von den zugewanderten Mehrheitsvölkern aus den fruchtbareren Regionen der Flussebenen und Küsten in die unzugänglichen Bergregionen verdrängt.

Da einige Angehörige der „Bergvölker“ im Indochinakrieg und im Vietnamkrieg jeweils auf Seiten Frankreichs bzw. der USA kämpften, gab es nach der Wiedervereinigung Vietnams Repressionen gegen diese Völker und sie sind in der vietnamesischen Gesellschaft teils nicht gut angesehen. Aber auch Minderheitenvölker, die auf vietnamesischer Seite gekämpft haben, finden kaum positive Beachtung. Diese Völker sind bis heute von der wirtschaftlichen Entwicklung des Landes weitgehend abgeschnitten und leben vergleichsweise in Armut. Kultur und Sprache der Minderheiten unterscheiden sich meist sehr stark von jener der Vietnamesen.

Die Amtssprache ist Vietnamesisch, das 88 % der Bevölkerung als Muttersprache beherrschen. Geschrieben wird die vietnamesische Sprache seit 1945 in einer eigenen, lateinbasierten Schrift. Aus vietnamesischer Sicht werden die zahlreichen ethnischen Minderheiten anerkannt, die Sprachen der Minderheiten erlaubt und auch gefördert.

Die französische Sprache hatte nach der französischen Kolonialzeit schrittweise ihren offiziellen Status verloren, hat aber weiterhin hohe Bedeutung, da sie in vielen Schulen als erste Fremdsprache unterrichtet wird. Vietnam ist zudem Vollmitglied der Gemeinschaft frankophoner Staaten. Viele Vietnamesen sind während der Indochinakriege nach Frankreich ausgewandert und bilden dort eine französischsprachige Diaspora. Russisch – und in geringerem Ausmaß auch Deutsch, Tschechisch und Polnisch – werden von vielen Vietnamesen beherrscht, die während des Kalten Krieges in den Staaten des Ostblocks studiert oder gearbeitet haben. Mittlerweile werden die russische und die französische Sprache durch das Englische aus dem öffentlichen und dem Schulleben verdrängt, weil viele Touristen aus dem angelsächsischen Raum kommen und der Handel mit dem ehemaligen „Erzfeind“ USA zunimmt. Das Erlernen des Englischen ist heute in den meisten Schulen obligatorisch, obwohl Französisch immer noch in manchen Bildungseinrichtungen angeboten wird.

Genaue Angaben über die Religionszugehörigkeit in Vietnam sind schwer zu machen. Die große Mehrheit der Vietnamesen bekennt sich zu keinem Glauben. Laut einer 2004 veröffentlichten Studie sind 81,5 Prozent der Vietnamesen Atheisten. Schätzungen gehen von ca. 20 Millionen Buddhisten und 6 Millionen Katholiken aus. Weitere Konfessionen sind Cao Dai (2 Millionen Anhänger), Hoa Hao (1 Million), Protestantismus (500.000) und Islam (50.000). Im Religionsverständnis der Vietnamesen gibt es keine strikte Trennung verschiedener Konfessionen. Die Religiosität ist zumeist eine historisch gewachsene Mischung mit vielen Aspekten unterschiedlicher religiöser Ursprünge. Es ist für Vietnamesen nicht unüblich, regelmäßig buddhistische Pagoden zu besuchen und ihre Ahnen zu verehren.

Die Alltagsreligiosität – bzw. vielmehr die Lebensweise – ist im Allgemeinen am ehesten durch den Theravada- und/oder Mahayana-Buddhismus, den Taoismus, den Konfuzianismus, sowie animistische Vorstellungen und insbesondere auch einen Ahnenkult beeinflusst, ohne dass es dabei zu Dogmen kommt. Geisterglaube ist verbreitet. Rituelle Handlungselemente der unterschiedlichen Einflüsse können beim Einzelnen je nach Alltagssituation auftreten. In den ursprünglich konfuzianistisch geprägten Volksreligion "Đạo Mẫu" und "Cao Đài" gibt es auch heute noch Stadtschamanen "(Dong)", die vielfältige Rituale des Opfers und der Inspiration ausführen. Besonders beliebt bei allen Vietnamesen unabhängig von ihrer Konfession ist das "Lên đồng-Ritual", bei dem die Schamanin die Geister in Trance um Gesundheit und Wohlstand für die Gastgeber des Rituals bittet. Dabei spielt das Kostüm eine wichtige Rolle: Es spiegelt die klassische Hoftracht der Vormoderne und wird dem Geist „angezogen“, um ihn in dieser Weise zu ehren. Der Geist tritt dann über das Medium mit den Anwesenden in Kontakt, um Opfergaben in Empfang zu nehmen und die Musik zu genießen.

Anders als in anderen asiatischen Staaten existiert in (Süd-)Vietnam seit 1963 auch eine zentrale Vereinigung von Ordensleuten und Laien aller buddhistischen Schulen, die „Kongregation der Vereinigten Vietnamesischen Buddhistischen Kirche“ (KVVBK).

Die Verfassung Vietnams sieht generell eine Religions-/Glaubensfreiheit vor. Da religiöse Institutionen aber immer auch eine gewisse Konkurrenz zum staatlichen Einfluss auf die Bevölkerung darstellen, wurden Religion und deren Institutionen zumindest in der Vergangenheit seitens der Kommunistischen Partei Vietnams mit Misstrauen behandelt.

Der katholische Glaube kam erstmals im 16. Jahrhundert mit französischen, spanischen und portugiesischen Missionaren ins Land. Er wurde unter Druck der französischen Kolonialherrschaft verbreitet. Nachdem der Katholizismus in den ersten Jahren der kommunistischen Herrschaft aktiv bekämpft wurde, bemüht sich die Regierung nun um ein besseres Verhältnis zum Heiligen Stuhl. Der Besuch des damaligen Premierministers Nguyễn Tấn Dũng bei Papst Benedikt XVI. 2007 hat die Hoffnung auf eine weitere Öffnung hin zu einer größeren Religionsfreiheit gestärkt, aber die katholische Kirche wird weiterhin als „reaktionär“ angesehen.

Die frühesten Spuren menschlicher Aktivität auf dem Gebiet des heutigen Vietnam lassen sich bis vor 300.000 bis 500.000 Jahren zurückdatieren. Die älteste bisher bekannte Kultur dieser Region war die mehr als 30.000 Jahre alte Dieu-Kultur südlich von Hanoi, von wo aus sich auch die 16.000 Jahre alte Hoa-Binh-Kultur weit ausbreitete. Die letzte altsteinzeitliche Kultur der Region war die Bac-son-Kultur (ca. 10.000 v. Chr.), die auch bereits Keramik anfertigte. Der Bewässerungsanbau von Reis war etwa ab 3000 v. Chr. bekannt.

Die Bronzezeit begann hier etwa 1500 v. Chr. mit der Sa-Huynh-Kultur, deren Mitglieder die Küstenregion bevölkerten. Zugleich existierte im Delta des Roten Flusses die Dong-Son-Kultur, bekannt vor allem für ihre reich verzierten Bronzetrommeln. Aus dieser Kultur ging Mitte des 1. Jahrtausends v. Chr. das erste bekannte Königreich der Việt () hervor, das den größten Teil des heutigen Nordvietnam umfasste.

Im 3. Jahrhundert v. Chr. wanderten Âu Việt aus dem Gebiet des heutigen Südchina ein und vermischten sich mit den ansässigen Lạc Việt. Im Jahr 258 v. Chr. gründete Thục Phán das Königreich Âu Lạc (aus der Vereinigung von Âu Việt und Lạc Việt) und erklärte sich selbst zum König. Nach einem langen Krieg mit den Qín wurde er 208 v. Chr. von dem Qín-General (vietnamesisch: Triệu Đà) besiegt. Dieser rief sich selbst zum König aus und nannte sein Königreich "Nam Việt" ( = "Südviệt" oder "Südyuè").

Im Jahr 111 v. Chr. wurde Nam Việt von Truppen Hàn Wǔdìs erobert und als Präfektur ( (quận)) (Giao Chỉ) in das chinesische Reich eingegliedert. Unter dieser Herrschaft wurden technische Errungenschaften im Reisanbau, in der Viehhaltung und in der Baukunst übernommen. Es kam aber auch zu zahlreichen Aufständen und kurzen Phasen der Unabhängigkeit. Im Jahr 679 wurde die Präfektur in "An Nam" () umbenannt.

Am Anfang des 10. Jahrhunderts brach in China die Tang-Dynastie zusammen. Annam nutzte die Schwächephase, um sich der chinesischen Macht zu entziehen. Der erste vietnamesische Staat entstand 938 unter dem Strategen Ngô Quyền. Bis 968 wurde der Staat unter Đinh Bộ Lĩnh konsolidiert; bis 1009 wechselten sich jedoch mehrere kurzlebige Dynastien an der Macht ab.

Von 1010 bis 1225 wurde der Staat "Dai Viet" von der Lý-Dynastie beherrscht. Ihr Gründer Ly Thai To verteidigte ihn erfolgreich gegen Chinesen, Khmer und Cham. Die Ly stärkten das Staatswesen nach chinesischem Vorbild und passten es an vietnamesische Bedürfnisse an.

Nach Unruhen übernahm im Jahr 1225 die Trần-Dynastie die Macht. Sie verteidigte in Allianz mit den Cham das Land erfolgreich gegen die Chinesen unter der Yuan-Dynastie des Kublai Khan. Unter der Führung von Trần Hưng Đạo gelang es den Vietnamesen, eine Armee von angeblich 500.000 Mongolen zu besiegen und die Unabhängigkeit Vietnams zu sichern. Um 1400 löste die Hồ-Dynastie die Trần ab und es kam zu einer kurzzeitigen chinesischen Herrschaft unter den Ming. Diese versuchten, Vietnam bewusst weiter zu sinisieren, beispielsweise wurde das vietnamesische Literaturerbe systematisch zerstört.
Im Jahr 1427 gründete Lê Lợi die Lê-Dynastie, die bis 1789 regierte. Unter den Le wurden wieder die vietnamesischen Traditionen bewusst betont, dennoch blieb der Konfuzianismus die dominante Säule der Staatsorganisation. Champa wurde erobert und die vietnamesische Macht bis an den Mekong ausgedehnt. Bereits ab dem Ende des 15. Jahrhunderts erodierte die Macht des Königshauses. Nutznießer waren einflussreiche Händlerfamilien (vor allem die "Trinh" und "Nguyen") und die seit 1516 präsenten Europäer. Das vietnamesische Königshaus musste zahlreiche Jesuiten und Franziskaner im Land dulden. Die europäischen Missionare brachten neben der neuen Religion auch neue Technologien ins Land, beispielsweise entwickelte der Jesuit Alexandre de Rhodes die bis heute gebräuchliche, auf den lateinischen Buchstaben basierende vietnamesische Schrift Quốc ngữ.

Im Jahr 1765 brach die Tây-Sơn-Rebellion aus. Aus dem nachfolgenden Bürgerkrieg ging 1789 der Prinz Nguyễn Ánh aus der einflussreichen Händlerfamilie "Nguyễn" mit französischer Hilfe als Sieger hervor. Er rief sich zum Kaiser Gia Long aus, verlegte die Hauptstadt des Landes nach Huế und initiierte erstmals die Namensgebung "Việt Nam" für das Land. 1802 ersuchte er den chinesischen Kaiser Jiāqìng um die Erlaubnis, das Land von Đại Việt 大越 in Nam Việt 南越 umbenennen zu dürfen. Dieser tauschte allerdings die beiden Silben zu Việt Nam 越南, um Verwechslungen mit dem alten Königreich Nam Việt unter Qín-General Zhào Tuó (vietn.: Triệu Đà) zu verhindern, da dieses Reich einen Teil des Gebietes umfasste, das später Südchina wurde.

Unter der Herrschaft Gia Longs wurden mit französischer Beratung große Infrastruktur- und Verteidigungsprojekte in Angriff genommen, die die Staatskasse leerten. Das Territorium des Reiches wurde erweitert, ab 1834 gehörten Teile des heutigen Kambodscha als Provinz "Trấn Tây thành" zu Vietnam.

Ab Mitte des 19. Jahrhunderts verstärkten die Franzosen ihren Druck auf die Nguyen-Kaiser, was zu Ausschreitungen der verarmten Bevölkerung gegen französische Missionare führte. Um als Schutzmacht der christlichen Missionen Stärke zu demonstrieren, griffen französische Kanonenboote 1858 den Hafen Đà Nẵng und das Mekongdelta an und tauchten auch auf dem Parfüm-Fluss auf, der durch die Hauptstadt Huế fließt. Ab 1862 musste Vietnam Gebiete an die Franzosen abtreten. Bis 1883 wurden drei Protektorate namens Annam, Cochinchina und Tonkin gegründet, die der vietnamesische Kaiser akzeptieren musste. Damit stand Vietnam unter französischer Kolonialherrschaft. Infolge der Einführung der Geldwirtschaft schritt die Verarmung der Bevölkerung voran, während auf dem Land eine schmale Großgrundbesitzerschicht entstand. Die chinesische Minderheit dominierte die Ökonomie des Landes. Bereits ab 1905 waren vietnamesische nationalistische Freiheitskämpfer um Phan Bội Châu (1868–1940) und Cuong De in Japan und Südchina aktiv.

In der Folgezeit kamen vietnamesische Studenten und Intellektuelle in Europa, vor allem in Frankreich, mit den Ideen des Nationalismus und Kommunismus in Kontakt. Der bedeutendste unter ihnen war Hồ Chí Minh (1890–1969), der 1929 die in Annam, Cochinchina und Tonkin tätigen kommunistischen Parteien zu einer Einheitspartei vereinigte. Die Partei wurde 1930 nach dem missglückten Yen-Bai-Aufstand und der Hinrichtung vieler ihrer Mitglieder dezimiert und geschwächt.

Während des Zweiten Weltkrieges geriet 1941 ganz Indochina und damit auch Vietnam verstärkt unter den Einfluss Japans (geteilte Herrschaft mit dem Vichy-Regime). Nachdem Hồ Chí Minh 1941 aus dem Exil zurückgekehrt war, wurde bald aus über 40 lokalen Widerstandsgruppen eine "Liga für die Unabhängigkeit Vietnams" unter der Kurzbezeichnung "Việt Minh" zur Abwehr des japanischen Imperialismus und französischen Kolonialismus gebildet. Im März 1945 besetzten die Japaner Indochina, beendeten die französische Kolonialverwaltung und setzten Kaiser Bảo Đại ein. Die USA unterstützten die Việt Minh, die bei der Bekämpfung der japanischen Okkupation einige Erfolge erzielten. Nach der Kapitulation Japans musste Bảo Đại am 25. August 1945 abdanken. Am 2. September 1945 proklamierte Hồ Chí Minh nach der erfolgreichen Augustrevolution die "Demokratische Republik Vietnam". Die Unabhängigkeitserklärung berief sich auf die Unabhängigkeitserklärung der Vereinigten Staaten von 1776 und auf die Deklaration der Menschen- und Bürgerrechte der französischen Revolution. Vietnam war damit die erste unabhängige Republik Südostasiens.

Nach der Potsdamer Konferenz fiel Vietnam in den Herrschaftsbereich der Briten. Diese mussten jedoch die besiegten Japaner bitten, im aufständischen Süden einzuschreiten. Im Norden wiederum marschierten ab September 1945 nationalchinesische Truppen mit dem Auftrag ein, die Japaner zu entwaffnen. Trotz eines Friedensvertrages mit den Việt Minh erzwangen die Franzosen am 23. September 1945 die Wiedererrichtung ihres kolonialen Regimes in Südvietnam, so dass am 5. Oktober französische Truppen in der Stadt Saigon landeten. Chinesen und Briten übergaben Vietnam wieder an Frankreich.

Der Versuch Frankreichs, sich auch das inzwischen unabhängige Nordvietnam wieder botmäßig (untertan) zu machen, führte 1946 zum Ausbruch des Ersten Indochinakrieges. In Südvietnam wurde 1948 eine unter französischer Aufsicht stehende Gegenregierung eingesetzt, der ab 1949 der ehemalige Kaiser Bảo Đại als Staatsoberhaupt vorstand. Nach jahrelangem Guerillakampf gelang es den Việt Minh unter General Võ Nguyên Giáp am 7. Mai 1954, die Franzosen in der Schlacht um Điện Biên Phủ zu besiegen. Dieser Sieg markierte das Ende der französischen Kolonialherrschaft in Indochina. Es folgten ein Waffenstillstand und die Genfer Konferenz vom 21. Juli 1954, auf der die Teilung Vietnams entlang des 17. Breitengrades in die (nördliche) Demokratische Republik Vietnam (Hauptstadt Hanoi) und die (südliche) Republik Vietnam (Hauptstadt Saigon) beschlossen wurde. Bis Mai 1955 hatte Frankreich alle Truppen aus Indochina abzuziehen.

In Südvietnam beauftragte Bảo Đại am 16. Juni 1954 den Katholikenführer Diệm mit der Regierungsbildung. Im Jahr darauf entmachtete Diệm Bảo Đại und erhob sich selbst zum Staatschef. Landreformen, die die Việt Minh veranlasst hatten, wurden zurückgenommen. Die Regierung Diệms war unpopulär, Studenten und Buddhisten protestierten gegen die Regierungspolitik. Die USA verstärkten ihre Unterstützung für Südvietnam, um den Sturz des Regimes zu verhindern. Bis 1960 versank Südvietnam immer mehr in Korruption und Chaos. Am 1. November 1963 wurde Diệm gestürzt und ermordet. Darauf folgten mehrere kurzlebige Militärregierungen, bis sich ab 1967 unter dem von den USA protegierten Präsidenten Nguyễn Văn Thiệu eine neue, stabile Regierung etablieren konnte.

Am 2. und 4. August 1964 ereignete sich der Zwischenfall im Golf von Tonkin. Die USA starteten ab 1965 massive „Vergeltungsangriffe“ auf Nordvietnam. Die erst 1971 veröffentlichten sogenannten Pentagon-Papiere zeigten auf, dass die USA diesen Krieg unter anderem seit längerem geplant hatten, um in Südvietnam eine Beteiligung der Kommunisten an der Regierung zu verhindern. Ab 1965 führten die USA einen systematischen Luftkrieg gegen Nordvietnam; im Süden operierten US-Bodentruppen. Bis 1968 eskalierte der Krieg, obwohl die USA Nordvietnam militärisch weit überlegen galten. Auf der Seite der Nationalen Front für die Befreiung Südvietnams kämpften rund 230.000 Partisanen und 50.000 Angehörige der offiziellen nordvietnamesischen Streitkräfte. Ihnen standen rund 550.000 Amerikaner, ungefähr die gleiche Zahl ARVN-Soldaten, 50.000 Südkoreaner und kleinere Kontingente Verbündeter (darunter auch aus Australien und Neuseeland) gegenüber.

Am 31. Januar 1968 gelang den Viet Cong ein politisch wichtiger Sieg: In der Tet-Offensive nahmen die kommunistischen Partisanen Südvietnams vorübergehend Teile Saigons und weiterer Städte ein, die gut gesicherte Botschaft der USA in Saigon wurde angegriffen. In den USA konnte nun die Regierung nicht mehr behaupten, dass der Konflikt unter Kontrolle sei. Es war offensichtlich, dass der Krieg nicht mehr gewonnen werden konnte, die öffentliche Meinung in den USA schwenkte um, nicht zuletzt aufgrund von Presseberichten und Bildreportagen über Kriegsgräuel, Massaker und Napalm-Opfer. Die USA beschlossen deshalb 1969 die "Vietnamisierung des Krieges" und den Abzug ihrer Truppen in mehreren Schritten. Die Bombardierungen und Luftangriffe, insbesondere die Verwendung von Entlaubungsmitteln, dauerten bis 1973 an.
Am 3. September 1969 starb Hồ Chí Minh, der Präsident Nordvietnams. Am 27. Januar 1973 vereinbarten Henry Kissinger und Lê Đức Thọ, der Nachfolger von Hồ Chí Minh, einen Waffenstillstand. Damit endete die direkte Kriegsbeteiligung der USA, die Waffenlieferungen an Südvietnam gingen jedoch weiter. Die Nordvietnamesen setzten den Kampf gegen Südvietnam erfolgreich fort. Am 21. April 1975 stand Saigon vor dem Fall, Staatschef Nguyễn Văn Thiệu legte sein Amt nieder, die letzten verbliebenen Vertreter der USA wurden evakuiert. Am 30. April wurde Saigon eingenommen, Südvietnam kapitulierte bedingungslos am 1. Mai 1975, der Vietnamkrieg war damit zu Ende. Bis zur Wiedervereinigung übernahm eine Provisorische Revolutionäre Regierung die Macht im Süden.

Am 2. Juli 1976 wurden Nord- und Südvietnam unter dem Namen "Sozialistische Republik Vietnam" wiedervereint. Saigon, die ehemalige Hauptstadt Südvietnams, wurde in Ho-Chi-Minh-Stadt (Thành phố Hồ Chí Minh) umbenannt.

Das in der Folge des Vietnamkrieges entstandene kommunistisch-maoistische Regime der Roten Khmer in Kambodscha und vor allem deren Attacken auf vietnamesisches Gebiet veranlassten Vietnam, in Kambodscha einzumarschieren. Anfang 1979 eroberten vietnamesische Truppen Phnom Penh und errichteten einen von Vietnam abhängigen „Revolutionären Volksrat“ unter Heng Samrin. Die Volksrepublik China, die die Regierung der Roten Khmer unterstützt hatte, provozierte daraufhin angesichts der moskautreuen Politik Vietnams entlang der Grenze zu Vietnam bewaffnete Auseinandersetzungen, die als Erziehungskrieg bekannt wurden. Während der mehrwöchigen Kämpfe erlitten beide Seiten hohe Verluste. China zog sich schließlich wieder zurück und gab an, seine Ziele erreicht zu haben. Der Konflikt endete ohne klaren Sieger. Erst 1989 zog sich Vietnam aus Kambodscha zurück.

1983 befanden sich rund 2000 sowjetische Militärberater im Land, die Luft- und Seestützpunkte (u. a. in Cam Ranh) sowie eine Abhörstation betrieben, deren Nutzung vertraglich vereinbart war.

1986 veranlasste die Kommunistische Partei Vietnams wirtschaftliche Reformen, genannt Đổi mới (Erneuerung).
Während der 1990er Jahre wuchs die Wirtschaft stark und Vietnam wurde wieder in die internationale Staatengemeinschaft aufgenommen.
Am 3. Februar 1994 hob die Regierung Clinton das seit dem Vietnamkrieg bestehende Handelsembargo auf.
1995 nahmen Vietnam und die USA wieder diplomatische Beziehungen auf; 2001 trat ihr bilaterales Handelsabkommen in Kraft.

Vietnam ist ein Einparteienstaat, in welchem die Kommunistische Partei Vietnams die Einheitspartei darstellt und somit das Monopol auf die Macht innehat. Die Menschenrechtslage ist problematisch. Die Presse wird entsprechend der Regierungsmeinung zensiert und die Zivilgesellschaft stark überwacht. Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „unfrei“ bewertet. In der Kategorie „politische Rechte“ erhält Vietnam die Note 7, bei der Wahrung der Bürgerrechte erhält das Land die Note 5 (die Note 1 ist die Beste und die 7 die Schlechteste).

Vietnam wird hauptsächlich von einem Kollegium aus drei Personen geführt, welches aus dem Generalsekretär der KPV, dem Premierminister und dem Staatspräsidenten besteht. Alle drei sind Parteifunktionäre und treffen ihre Entscheidungen in der Regel einstimmig. Der Generalsekretär ist nicht nur Leiter des Sekretariats, sondern in der Regel auch Vorsitzender des Politbüros der KPV, welches momentan aus 14 Mitgliedern besteht.

Laut Verfassung ist die Nationalversammlung, das Einkammerparlament Vietnams, das höchste Organ staatlicher Macht. Die 493 Abgeordneten werden für eine Legislaturperiode von fünf Jahren gewählt. Mindestens zweimal jährlich muss die Nationalversammlung eine Vollversammlung abhalten. In der übrigen Zeit werden ihre Aufgaben vom Ständigen Ausschuss der Nationalversammlung (SANV) ausgeführt. Die Nationalversammlung ernennt den Staatspräsidenten, den Premierminister und die Regierung (Exekutive), sowie die Prokuratur des Obersten Volksgerichtshofes und des Obersten Volkskontrollamtes (Judikative). Die Nationalversammlung hat seit den letzten Verfassungsänderungen stark an politischem Einfluss gewonnen. Sie kann jetzt Gesetze ändern, kann Minister zur Verantwortung ziehen und muss den Staatshaushalts- und Produktionsplänen zustimmen. Die größte politische Macht liegt weiterhin bei der kommunistischen Partei, welche durch die Vietnamesische Vaterlandsfront – einem Dachverband für Massenorganisationen – den Wahlprozess unter ihrer Kontrolle hat. Sie steuert mit ihrem Zentralkomitee und dem Politbüro die Politik des Landes. Durch den etwa 90-prozentigen Anteil an KPV-Mitgliedern in der NV sind alle ranghohen Regierungsmitglieder ebenfalls Teil der KPV.

Wahlen finden in Vietnam alle fünf Jahre auf mehreren Ebenen statt: Auf Zentralebene (Nationalversammlung) sowie auf Provinz-, Distrikts- und Gemeindeebene (Volksräte). Die Kandidaten, die sich zur Wahl stellen wollen, werden von der Vietnamesischen Vaterlandsfront und der Kommunistischen Partei nach strengen Kriterien ausgewählt. Trotzdem sind momentan ca. 10 % der Abgeordneten keine Parteimitglieder, nachdem bei der Wahl 2002 ungefähr 15 % Nicht-Parteimitglieder zugelassen wurden. Allerdings hatten sich zuvor 69 Unabhängige beworben, und nur 13 wurden angenommen. Seit 2003 müssen von Rechts wegen in jedem Wahlkreis mindestens zwei Kandidaten mehr antreten als Mandate zu vergeben sind.

Die erste Verfassung Vietnams wurde im November 1946 verabschiedet. Sie legte die Unteilbarkeit des Landes sowie die Gleichheit aller Bürger des Landes vor dem Gesetz fest. Seitdem gab es 1959, 1980 und 1992 neue Verfassungen. Die heutige vietnamesische Verfassung gilt in ihrer Version vom 15. April 1992, welche 2001 modifiziert wurde. Mit einem zusätzlichen Abschnitt im Artikel 4 stellt sich die kommunistische Partei, im Unterschied zur Verfassung von 1980, formell unter die Verfassung und das Gesetz, während sie bis dahin die Autorisierung dazu hatte, alles zu tun, was sie zum Aufbau des Sozialismus für notwendig erachtete. Die heutige Verfassung hat ihren Schwerpunkt in Richtung der Entwicklung von Wirtschaft, Bildung, Wissenschaft und Technologie und des Schutzes des privaten Sektors und von ausländischen Investoren verschoben.

Artikel 4 der Verfassung legt jedoch nach wie vor die führende Rolle der Kommunistischen Partei Vietnams fest und verbietet alle Oppositionsparteien. Die Präambel der Verfassung beschreibt die Partei als Führer, das Volk als Herrscher und den Staat als Verwalter.

Die vor dem Verbot bestehenden oder nach dem Verbot gegründeten Parteien im Ausland bestehen weiter. Diese haben zwar keinen Einfluss auf das politische Geschehen in Vietnam, veranstalten aber viele Demonstrationen im Inland und Ausland. Zudem besitzen manche Parteien eigene Parteizeitungen, die zumeist kritische Enthüllungen gegen die kommunistische Regierung in Vietnam beinhalten.

Des Weiteren räumt die Verfassung Vietnams formell allen Bürgern Grundrechte wie z. B. Redefreiheit, Pressefreiheit, Versammlungsfreiheit und Glaubensfreiheit ein. Aufgrund der staatlichen Zensur und Kontrolle durch die kommunistische Partei ist es den Bürgern allerdings nur in beschränktem Umfang möglich, diese Grundrechte in Anspruch zu nehmen. So wurden bereits mehrere kritische Blogger verhaftet. Der bekannte Blogger und Dissident Le Quoc Quan, ein Anwalt, der sich für die Menschenrechte einsetzt, wurde im Herbst 2013 aufgrund des Vorwurfs der Steuerhinterziehung zu einer zweieinhalbjährigen Haftstrafe verurteilt. Daraufhin kam es zu einer Protestdemonstration.

Am 19. September 2015 wurde die Bloggerin Ta Phong Tan nach drei Jahren Haft unter bloßer Suspendierung des Rest auf zehn Jahre Haftstrafe ins Exil gezwungen. Pen International fordert freie Einreise und Erlassen dieser Strafe für sie genauso wie Freilassung einer Reihe anderer Blogger.

In der Rangliste der Pressefreiheit 2017, die von Reporter ohne Grenzen herausgegeben wird, belegte Vietnam Platz 175 von 180 Ländern.

Vietnam verfügt de facto über keine unabhängige Judikative. Die im vietnamesischen Rechtssystem handelnden Personen sind alle unmittelbar oder mittelbar durch die kommunistische Partei bzw. die Vietnamesische Vaterlandsfront ausgewählt, wobei politische Zuverlässigkeit ein wichtiges Auswahlkriterium darstellt. Die Partei nimmt auch auf Rechtsentscheidungen Einfluss, welche die Monopolstellung der KPV in Frage stellen könnten. Darüber hinaus fehlt es an Richtern und Anwälten mit adäquater Ausbildung. Allerdings haben die Schöffen in Vietnam im Gegensatz zum deutschen System eine juristische Ausbildung.

Die oberste Instanz des vietnamesischen Rechtssystems ist der Oberste Volksgerichtshof, welcher der Nationalversammlung unterstellt ist und dessen Mitglieder auf Vorschlag des Staatspräsidenten von der Nationalversammlung ernannt werden. Die Nationalversammlung bestimmt auch das Budget der Judikative. Dem Obersten Volksgerichtshof sind die Volksgerichte auf Distrikts- und Provinzebene, die Militärtribunale sowie die Verwaltungs-, Wirtschafts- und Arbeitsgerichte unterstellt.

Die Todesstrafe ist in Vietnam nicht abgeschafft; sie wird unter anderem gegen Personen verhängt, die der Korruption oder des Drogenhandels überführt wurden.

Während des Vietnamkrieges und danach war Vietnam in Südostasien weitgehend isoliert. Die USA hatten ein Wirtschaftsembargo verhängt und drängten auch andere Staaten, Vietnam zu boykottieren. Speziell nach dem Einmarsch in Kambodscha (1978–1989) waren auch die Beziehungen zur Volksrepublik China so gespannt, dass an der vietnamesisch-chinesischen Grenze ein Krieg ausbrach. Vietnam integrierte sich deshalb sehr stark in den Rat für gegenseitige Wirtschaftshilfe ("RGW"). Aus der Isolierung kam das Land erst nach dem Rückzug aus Kambodscha 1991 heraus.

In den 1990er Jahren entspannten sich die Beziehungen zu allen Nachbarstaaten. Im Jahr 1991 nahm das Land wieder diplomatische Beziehungen zu China sowie den meisten Ländern Europas und Ostasiens auf. Unterhielt Vietnam vor dem Ende des Kalten Krieges nur zu 23 nicht-kommunistischen Staaten diplomatische Beziehungen, sind es heute 172. Es gibt Handelsabkommen mit 76 Ländern sowie eine ebenso hohe Anzahl an Ländern mit Meistbegünstigtenstatus. Die USA haben ihr Embargo gegen Vietnam aufgehoben und so wurde der Beitritt zur Weltbank, dem Internationalen Währungsfonds und zur Asiatischen Entwicklungsbank möglich. Im Juli 1995 trat Vietnam der ASEAN bei, 1998 der APEC. Seit dem 11. Januar 2007 ist Vietnam 150. Mitglied der WTO. 2008 wurde das Land zu einem von zehn nicht-ständigen Mitgliedern des UN-Sicherheitsrat gewählt.

Von besonderem Interesse für Vietnam sind die Beziehung zur asiatisch-pazifischen Region, und hier besonders zu China, als ebenfalls sozialistischem Staat und Hauptordnungsmacht in der Region. Auch mit Deutschland gibt es einige Kooperationen. Die Konrad-Adenauer-Stiftung, die Rosa-Luxemburg-Stiftung und das Goethe-Institut haben Außenstellen in Vietnam. Im Jahr 2010 riefen mehrere Organisationen das Veranstaltungsjahr „Deutschland in Vietnam“ aus, bei dem diverse Veranstaltungen deutscher Kultur in Vietnam stattfanden. Parallel dazu wurde auch ein Veranstaltungskalender „Vietnam in Deutschland“ erstellt. Die mutmaßliche Entführung des vietnamesischen Managers Trịnh Xuân Thanh, der sich als Asylbewerber in Deutschland aufhielt, am 23. Juli 2017 durch den vietnamesischen Geheimdienst, veranlasste die Bundesregierung, einen vietnamesischen Diplomaten auszuweisen.

Grenzstreitigkeiten gibt es mit einer Reihe von Staaten um die Paracel-Inseln sowie die Spratly-Inseln im Südchinesischen Meer.

Vietnam ist in 58 Provinzen und fünf Munizipalitäten unterteilt. Unter dieser Ebene folgen Städte, Distrikte und Dörfer. Die Volksräte der Provinzen und Munizipalitäten sind direkt der Zentralregierung unterstellt. Auf Distrikts- und Gemeindeebene gibt es ebenfalls gewählte Volksräte, gegenüber welchen die lokalen Behörden bis zu einem gewissen Maß gebunden sind. Die Volksräte wählen außerdem die Volkskomitees, welche die regionalen Regierungen darstellen.

Städte
2016 lebten 34,2 % der Bevölkerung in Städten oder städtischen Räumen. Die 5 größten Städte sind (Stand 2016):

Die Vietnamesische Volksarmee ging auf die Gründung eines vietnamesischen kommunistischen Staates während der Augustrevolution zurück. Die Streitkräfte spielten im Indochinakrieg und Vietnamkrieg die entscheidende Rolle zum Erreichen der Unabhängigkeit und Einheit des Landes im Rahmen eines kommunistischen Staates. Im Kambodschanischen Bürgerkrieg besetzten die Streitkräfte Teile des Nachbarlandes. 1979 verteidigten die Streitkräfte den Nordteil des Landes gegen eine chinesische Invasion. Die Streitkräfte unterliegen einer rigorosen politischen Kontrolle durch die kommunistische Partei. Darüber hinaus besitzen die Streitkräfte eigene Unternehmen.

Die Landstreitkräfte haben eine Stärke von etwa 412.000 Mann; es existiert eine allgemeine Wehrpflicht für alle Männer, die in der Regel zwei Jahre dauert. Die Marine hat 42.000 Mann; die modernste Teilstreitkraft Vietnams ist die Luftwaffe mit 30.000 Mann. Ihre Hauptstärke besteht aus 124 MiG-21, 53 Su-22, 12 Su-27 und 24 Su-30.

Vietnam sieht sich momentan keinen Bedrohungen von außen gegenübergestellt. Die Regierung hat deshalb in den vergangenen Jahren die Truppenstärke und Verteidigungsausgaben reduziert. Es wird geschätzt, dass 2003 2,3 Milliarden Dollar für Verteidigungszwecke aufgewendet wurden. Trotzdem gehört das vietnamesische Militär zu den mächtigsten und schlagkräftigsten in der Region. Auch innenpolitisch ist das Militär stark, viele ranghohe Militärs nehmen einflussreiche Positionen in Partei- und Staatsführung ein. Nach den militärischen Auseinandersetzungen mit Frankreich, den USA und China hat es in der Bevölkerung starken Rückhalt.

Neben der regulären Armee gibt es paramilitärische Reserveeinheiten, deren Stärke auf 4 bis 5 Millionen Mann geschätzt wird. Hierzu gehören die Selbstverteidigungskräfte und die Volksmiliz.

Die zwei größten Städte des Landes, Hanoi und Ho-Chi-Minh-Stadt haben internationale Flughäfen, die von wenigen europäischen, aber den meisten asiatischen Großstädten direkt angeflogen werden. Daneben gibt es Eisenbahnverbindungen von und nach China und Straßenverbindungen in alle Nachbarländer. Die Grenzübergänge sind meist nur am Tag geöffnet. Ausländer können, sofern sie alle notwendigen Papiere haben, jeden beliebigen Grenzübergang zur Einreise benutzen.

Vietnams Straßen haben eine Länge von insgesamt etwa 210.000 Kilometern, wovon nur etwa 13,5 % in einem guten Zustand und 29 % asphaltiert sind. 10 % der vietnamesischen Dörfer sind jährlich wegen unpassierbarer Straßen mehr als einen Monat von der Außenwelt abgeschnitten.

Durch starke Anstrengungen im Tiefbau wächst der Anteil, der internationalen Standards entspricht, stetig; vorerst meist in den Einzugsgebieten von Großstädten. Trotz der fortschreitenden Asphaltierung ist der größere Teil in eher schlechtem Zustand. Straßen in einer Qualität, die man als Autobahn bezeichnen könnte, gibt es nur wenige. Die wichtigste Straße Vietnams, die über 2100 km als verkehrstechnisches Rückgrat durch das gesamte Land von der chinesischen Grenze bis ins Mekongdelta verläuft, ist die Nationalstraße 1, vietnamesisch "Quốc lộ 1A".

Derzeit wird an einer zweiten Nord-Süd-Verbindung gebaut, dem so genannten Ho-Chi-Minh-Highway, der auf weiten Strecken entlang der Strecke des berühmten Ho-Chi-Minh-Pfads verläuft. Nach ihrer Fertigstellung soll diese 1690 km lange und 500 Millionen US-Dollar teure Straße Hanoi mit Ho-Chi-Minh-Stadt verbinden. 2006 waren bereits 960 km der Strecke zwischen den Orten Khe Co (Provinz Ha Tinh) und Ngoc Hoi (Provinz Kon Tum) in Form einer meist zweispurigen Asphaltstraße fertig gestellt. Nach der Fertigstellung wird diese Route eine attraktive Alternative zur Nationalstraße 1 darstellen. Zum einen wird der Verkehr weniger dicht sein, zum anderen führt die geplante Strecke durch reizvolle Landschaften. Dabei wird sie allerdings auch einige der letzten, bisher unberührten Wildnisgebiete und Nationalparks an der laotischen Grenze durchschneiden.

In Vietnam herrscht offiziell Rechtsverkehr. In der Regel wird jedoch gefahren, wo gerade Platz ist. Kreuzungen, die mit Ampeln geregelt sind, kommen eher in den Städten vor. Auch wenn die Regierung versucht, den Busverkehr zu fördern, ist das bedeutendste Nahverkehrsmittel das Moped, das bei wohlhabenderen Familien zunehmend vom Auto abgelöst wird. Die bis vor wenigen Jahren allgegenwärtigen Fahrrad-Rikschas ("Cyclo") richten sich in den Großstädten heute meist an die Touristen. Auch andere Formen des „Taxifahrens“ sind bei Touristen populär, so in den Innenstädten das Mopedtaxi ("Xe Ôm"), das von seriösen Fahrern nur mit Helm angeboten wird. Seit 2007 gilt in Vietnam die Helmpflicht, und die Bußgelder sind recht hoch. Es ist erlaubt, maximal zwei schwere oder drei leichte Personen mit einem Moped zu befördern.

Das vietnamesische Eisenbahnnetz besteht aus sechs Linien mit 3260 Kilometern Schiene, stammt größtenteils aus der Kolonialzeit und wird nur langsam modernisiert. Die längste Linie führt von Hanoi nach Ho-Chi-Minh-Stadt; für die 1730 Kilometer benötigt der "Reunification Express" 29,5 Stunden. Das Bahnnetz ist überwiegend in Meterspur ausgeführt, zwischen Hanoi und der chinesischen Grenze gibt es ein Dreischienengleis mit Normalspur. Grenzüberschreitende Verbindungen gibt es zurzeit nur nach China über den Grenzübergang Đồng Đăng.

Die Züge sind ausschließlich dieselbetrieben. Ältere Fahrzeuge stammen großteils aus sowjetischer Produktion, in den letzten Jahren ist China der Hauptlieferant. Pläne zur Errichtung einer Hochgeschwindigkeitsstrecke wurden aufgrund zu hoher Kosten nicht umgesetzt.

Fahrkarten werden in verschiedenen Klassen verkauft (drei Sitzplatzkategorien, Schlafabteile mit zwei bis sechs Plätzen pro Abteil). Alle Klassen außer der billigsten sind klimatisiert. Die Züge fahren recht langsam, sind dafür sicher und vergleichsweise pünktlich. Für längere Fahrten empfehlen sich Liege- oder Schlafwagen, die man längere Zeit im Voraus buchen sollte.

In Hanoi und Ho-Chi-Minh-Stadt werden U-Bahn-Systeme gebaut und geplant, die ab 2018 bzw. 2020 in Betrieb gehen sollen.

Die wichtigste Fluglinie des Landes ist die staatliche Vietnam Airlines. Sie bietet zahlreiche Flüge in andere asiatische Länder sowie einige Interkontinentalstrecken an und bedient die meisten Flughäfen in Vietnam. Besonders hier sind neben den Strecken zwischen den großen Städten des Landes auch die abgelegenen kleineren Städte im eher schwach erschlossenen Bergland von Bedeutung, die hier zumeist über einen eigenen Flugplatz verfügen. Die Flotte von Vietnam Airlines entspricht internationalem Niveau. Im regionalen Flugverkehr Südvietnams ist die Vietnam Air Service Company (VASCO) aktiv, eine Tochtergesellschaft der Vietnam Airlines.

Überwiegend auf den Inlandsmarkt sind die Billigfluglinien Jetstar Pacific Airlines und VietJet Air fokussiert. Sie besitzen westlichem Standard entsprechende Flotten mit nur einer Beförderungsklasse und haben relativ günstige Tarife, die jedoch die Gepäckbeförderung nicht einschließen.

Vietnam verfügt über etwa 5000 Kilometer Wasserstraßen, die ganzjährig befahrbar sind. Besonders im Mekongdelta ist der Wassertransport wichtig, und die Straßen werden durch zahlreiche Flussarme unterbrochen, die mittels Fähre überbrückt werden müssen.

Die wichtigsten Seehäfen sind Ho-Chi-Minh-Stadt, Hải Phòng, Đà Nẵng, Quang Ninh, Qui Nhon sowie Cần Thơ. 2005 wurden etwa 15 Millionen Tonnen Fracht umgeschlagen, nach 4,5 Millionen im Jahr 1993.

Im Telefonnetz Vietnams gab es in den letzten Jahren viele Investitionen. Wo investiert wurde, kommt modernste Technologie zum Einsatz, dementsprechend zuverlässig und komfortabel ist das Netz. Wo noch nicht investiert wurde, ist das Telekommunikationsnetz weit zurückgeblieben. Für Mitte 2004 wurden 4,9 Millionen Festnetzanschlüsse, 3,4 Millionen Mobiltelefone und 5,1 Millionen Internet-Benutzer gezählt. Das Internet wurde 2016 von 52,0 % der Bevölkerung genutzt.

Im Jahr 2016 gibt es in fast allen größeren Orten Breitbandinternet. Standardmäßig kommt hier Glasfaserkabel bis zum Endkunden zum Einsatz. Fast überall finden sich offene WLANs. In Lokalen oder Hotels gibt es standardmäßig Gratis-WLAN.

Es existieren mehrere gut ausgebaute Mobilfunknetze mit 3G- und HSDPA-Internet. Sim-Karten mit reichlich Datenvolumen werden praktischerweise an jeder Ecke angeboten. Die Tarife sind auch für Einheimische ziemlich günstig. Praktisch jeder Vietnamese verfügt inzwischen über ein Mobiltelefon oder zunehmend über Smartphones.

Die Internet-Cafés, von denen es im ganzen Land eine hohe Anzahl gibt, werden überwiegend für Onlinespiele besucht. Ähnlich wie in China ist die Regierung besorgt, dass durch das Internet das staatliche Informationsmonopol untergraben wird und letzten Endes die Legitimität der Alleinregierung der Kommunistischen Partei in Frage gestellt werden könnte. Deshalb kommt für das ganze Land ein Gateway ("Vietnam Data Communications") mit Filtersystem zum Einsatz, welches unerwünschte Inhalte blockieren soll. Dazu gehörte in der Vergangenheit mehrmals die vietnamesischsprachige Webpräsenz der BBC.

Im Jahr 2000 wurden laut Schätzungen 92 % aller Kinder eingeschult. Jedoch nur zwei Drittel absolvierten die fünf Grundschuljahre. Speziell auf dem Land verlassen viele Kinder vorzeitig die Schule, wobei die Gründe in den Kosten für Schulmaterial, Bücher und Uniformen sowie der Notwendigkeit, Geld für den Familienunterhalt verdienen zu müssen, zu suchen sind. Regional gibt es riesige Unterschiede: In einigen ländlichen Gegenden gehen nur 10 bis 15 % der Kinder länger als drei Jahre zur Schule, während in Ho-Chi-Minh-Stadt 96 % der Schüler die Grundschuljahre beenden. Nur 62,5 % der Kinder beginnen die Mittelschule.

Etwa 6 % der Einwohner über 15 Jahre sind Analphabeten; Analphabetismus betrifft 3,7 % der Männer und 7,2 % der Frauen, insgesamt liegt sie bei 4,5 % (Stand 2015). In Vietnam gibt es keine Schulpflicht. Da die Ausbildung selbst bezahlt werden muss und einige Familien dafür nicht genug Geld haben, schicken sie ihre Kinder nicht in die Schule.

Die Grundschule geht bis zur 5. Klasse, die Mittelschule bis zur 9.; dann muss man eine Prüfung bestehen, um in die Oberschule zu kommen (10., 11. und 12. Klasse). Wird diese nicht bestanden, bleibt man immer wieder sitzen. Dies gilt für Gymnasium und Realschule (vorausgesetzt, man bricht die Ausbildung nicht ab).

Besucht man ein Gymnasium bzw. eine Realschule, kann und darf man nicht mehr wechseln.

Es gibt staatliche und private Universitäten, die renommiertesten davon sind die Staatliche Universität Hà Nội und die Staatliche Universität Hồ-Chí-Minh-Stadt; der Zugang wird durch eine Aufnahmeprüfung der jeweiligen Universität geregelt.

Die verbreitetste Fremdsprache in Vietnam ist heute Englisch. Aus Gründen, die mit der Geschichte des Landes und der früheren Einbindung in den Ostblock zusammenhängen, trifft man oft Leute an, die Französisch, Russisch oder Deutsch sprechen; so haben etwa 100.000 Vietnamesen in der DDR studiert, gearbeitet oder eine Ausbildung genossen. Immer mehr Vietnamesen lernen auch Japanisch und Chinesisch.

Das vietnamesische Gesundheitssystem ist gut, was die Lebenserwartung von 75,6 Jahren und die Kindersterblichkeit von 2,2 % belegen. Im Jahr 2001 gab die Regierung nur 0,9 % des BIP für das Gesundheitssystem aus. Im Jahr 2000 gab es demnach auch nur 14,8 Krankenhausbetten pro 10.000 Einwohner, was selbst für Asien ein sehr niedriger Wert ist. 80 % aller Aufwendungen für das Gesundheitssystem stammen von den Patienten selbst.

Nachdem in den 1980er und 1990er Jahren Krankheiten wie Malaria, Tuberkulose, Denguefieber, Typhus und Cholera große Probleme darstellten, hat Vietnam ausländische Hilfe angenommen und diese Epidemien weitgehend zurückgedrängt. Die HIV-Prävalenz lag 2005 offiziell bei 0,35 %, was dem weltweiten Durchschnitt entspricht. HIV/AIDS-Patienten werden gesellschaftlich geächtet, was eine effiziente Bekämpfung der Epidemie erschwert.

Eine Gesundheitsstudie aus dem Jahr 2007 zeigt, dass 87 % der Vietnamesen aus der Altersgruppe der 60- bis 69-Jährigen an Krankheiten leidet. In den noch älteren Bevölkerungsschichten ist die Krankheitsrate noch größer.

Nach den zahlreichen Kriegen in Vietnams Vergangenheit sind 5 Millionen Vietnamesen bzw. 6 % der Bevölkerung behindert.

Der Anteil der unterernährten Bevölkerung konnte von 24,3 % im Jahr 2000 auf 10,7 % im Jahr 2015 gesenkt werden.

Entwicklung der Lebenserwartung seit 1950
Quelle: UN

Vietnam gehört zu jenen Staaten, die sich in einer Transformation von der Zentralverwaltungswirtschaft zur sozialistischen Marktwirtschaft befinden. Dieser Prozess hat in Vietnam ein rasantes Wirtschaftswachstum ausgelöst und das Land zu einem attraktiven Investitionsstandort für internationale Unternehmen werden lassen. Die Weltbank stuft Vietnam seit Beginn 2011 als Schwellenland ein.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Vietnam Platz 55 von 137 Ländern (Stand 2017–2018). Im Index der Wirtschaftlichen Freiheit belegte das Land 2017 Platz 147 von 180 Ländern.

Nach der Wiedervereinigung Vietnams stand die Wirtschaft des Landes vor dem Problem, in zwei Hälften geteilt zu sein, die nach komplett verschiedenen Mustern organisiert waren: Im Norden gab es die kommunistische, planwirtschaftlich organisierte Hälfte, deren Landwirtschaft in Kooperativen betrieben wurde und dessen Land zudem durch die Armee der USA im Vietnamkrieg stark zerbombt worden war. Der Süden hingegen war einige Zeit marktwirtschaftlich organisiert, hatte aber während der vergangenen zwei Jahrzehnte eine Wirtschaft entwickelt, die vollständig vom Zustrom amerikanischen Geldes abhing, das bedingt durch die Militärpräsenz zufloss.

Der Süden wurde nach sowjetischem Vorbild restrukturiert, die Landwirtschaft kollektiviert und die Betriebe wurden verstaatlicht. Im Jahr 1978 trat Vietnam dem Rat für gegenseitige Wirtschaftshilfe bei, während die USA ein Wirtschaftsembargo über Vietnam verhängten, das nicht nur Amerikanern verbot, mit Vietnam Handel zu treiben, sondern auch den IWF, die Weltbank und ähnliche Organisationen daran hinderte, Vietnam Aufbaukredite zu geben.
Das Resultat aus der Unproduktivität der Staatsbetriebe und der kollektivierten Landwirtschaft, den Handelshindernissen und den massiven Umweltschäden aus dem Vietnamkrieg war schreckliche Armut. Repressionen der kommunistischen Führung gegen die früheren Feinde, Armut und Enteignungen der Privatwirtschaft im Süden veranlassten mehr als eine halbe Million Vietnamesen dazu, als Boatpeople unter Lebensgefahr das Land zu verlassen. Die Anzahl Überlebender wird auf nur 20 % bis 40 % geschätzt. Angesichts offensichtlicher ökonomischer Probleme entschied sich die Kommunistische Partei 1979 dazu, private Wirtschaftssubjekte stärker zu fördern und 1981 wurden in der Landwirtschaft die ersten Reformschritte gesetzt. Weitere Reformen blieben jedoch wirkungslos, es kam zu wirtschaftlicher Stagnation und Hyperinflation sowie zu schwerwiegenden Versorgungsengpässen. 1980 lag die Reisproduktion pro Kopf mit 265 kg unter der Subsistenzgrenze von 300 kg pro Kopf und Jahr. Das Einzige, was Vietnam halbwegs am Leben hielt, war Wirtschaftshilfe der RGW-Staaten, die sich auf geschätzte drei Milliarden Dollar jährlich belief.

Im Jahr 1986 starb Lê Duẩn und machte Platz für eine reformorientierte, jüngere Generation. Unter Nguyễn Văn Linh wurde Đổi mới ("Wirtschaftserneuerung") angekündigt und ab 1989 wurden die ersten Maßnahmen dieser Reformpolitik verwirklicht. Das bedeutete, dass die zentrale Planung aufgegeben, die Kollektivierung schrittweise abgeschafft und marktwirtschaftliche Reformen eingeführt wurden. Allerdings gab die KPV keineswegs ihre sozialistische Prägung auf, denn es wurde betont, dass eine sozialistische Marktwirtschaft aufgebaut werde, welche die erste Stufe des Übergangs zum Kommunismus sei. Ausländischen Firmen wurde erlaubt, in Vietnam zu investieren. Als Vietnam am Beginn der 1990er Jahre aus der internationalen Isolation fand, in die es durch die Intervention in Kambodscha gekommen war, und die Amerikaner 1993 ihr Wirtschaftsembargo aufhoben, flossen so viele ausländische Investitionen und Finanzhilfe in das Land, dass das Wirtschaftswachstum zeitweise 10 % pro Jahr überstieg. Aus dem früheren Mangelland Vietnam wurde, speziell durch die Reformen in der Landwirtschaft, der zweitgrößte Reisexporteur der Welt. 2003 lag die Reisjahresproduktion pro Kopf bei rund 470 kg.
Ein beträchtlicher Teil der Wirtschaftsleistung wird durch finanzielle Unterstützung, Waren und Investitionen von Auslandsvietnamesen (vor allem aus den USA) erbracht; für das Jahr 2000 wurde dieser Betrag auf eine Milliarde US$ geschätzt.

Vietnam hat 2009 die Grenze von 1000 USD Jahreseinkommen pro Kopf überschritten und ist seitdem ein „Middle Income Country“. 2016 betrug das Bruttoinlandsprodukt 198 Mrd. USD, demnach 2215 USD pro Kopf. Allerdings ist das Volkseinkommen zwischen Stadt und Land sehr ungleich verteilt. Nach wie vor leben 60 Prozent der Bevölkerung auf dem Land, erwirtschaften dort aber nur 20 Prozent des Volkseinkommens. Die Inflationsrate lag in Vietnam 2016 bei 2,66 %.

Das um die Kaufkraftparität bereinigte BIP pro Person lag 1999 noch bei 410 US$ (Stadt 640, Land 180), 2016 schon bei etwa 6530 US$, was ca. 18 Dollar/Tag entspricht. Immer noch etwa 6 % der Bevölkerung verdienen weniger als einen US$ pro Tag.

Auf dem X. Parteikongress der KPV, der vom 18.–25. April 2006 in Hanoi stattfand, verabschiedeten 1178 Delegierte den Fünf-Jahres-Plan für den Zeitraum 2006–2010 ("Socio-Economic Development Plan for the Five Year Period 2006–2010"). Gemäß diesem Plan soll Vietnam bis 2020 ein Industrieland werden; das Wirtschaftswachstum soll bis dahin zwischen 8 und 8,5 % bleiben. Bezeichnenderweise hielt sich gleichzeitig der ehemalige Microsoft-Chef Bill Gates auf Einladung der vietnamesischen Regierung ebenfalls in Hanoi auf.

Im Mai 2006 wurde bekannt, dass Vietnam und die USA im Juni 2006 ein bilaterales Handelsabkommen abschließen wollen. Im November 2006 fand in Hanoi zudem das Gipfeltreffen der APEC-Staaten statt, an dem auch US-Präsident George W. Bush teilnahm. Zum 11. Januar 2007 trat Vietnam der Welthandelsorganisation WTO bei.

Vietnam war bis vor wenigen Jahren ein fast ausschließlich agrarisch geprägtes Land. Bis heute sind in der Landwirtschaft 65 % der Arbeitskräfte Vietnams tätig, jedoch trägt dieser Sektor nur mehr ein Fünftel des BIPs bei. Für 2007 verzeichnete man einen Zuwachs von 3,4 %, trotz zahlreicher Naturkatastrophen.

Der von den französischen Kolonialherren 1857 in Vietnam eingeführte Kaffeeanbau hat sich in den letzten 25 Jahren rasant entwickelt, von einer Anbaufläche von 22.000 Hektar 1980 auf heute eine halbe Million Hektar. Damit ist Vietnam hinter Brasilien der weltweit zweitgrößte Kaffeeproduzent geworden. Auslöser für diese Entwicklung war die DDR. Wegen der in den 1980er Jahren stetig gestiegenen Preise für Rohkaffee und dem immensen Bedarf der DDR an diesem wurde ein Ausweg aus dem Problem gesucht, wertvolle harte Währung für Kaffee ausgeben zu müssen. Das damalige sozialistische Bruderland Vietnam bietet gute klimatische Voraussetzungen für den Kaffeeanbau in mittlerweile auch weltmarktfähiger Qualität. Eines der Zentren des vietnamesischen Kaffeeanbaus ist die südliche Hochland-Provinz Đắk Lắk (durchschnittlich bei einer Höhe von ).

Vietnam verfügt über Erdölreserven, die auf 270 bis 500 Millionen Tonnen geschätzt werden. 2004 wurden mehr als 400.000 Barrel täglich gefördert, der Peak dürfte aber schon überschritten sein. Vietnam verfügt bis heute über keine nennenswerten Raffineriekapazitäten und muss daher Rohöl exportieren und Ölprodukte importieren. Drei Anlagen zur Raffinierung von Erdöl sind im Bau. In Vietnam gibt es weiterhin große Vorkommen von Anthrazitkohle (Reserven von 3.116 Mt) und Erdgas sowie Antimon, Bauxit, Chrom, Gold, Eisen, Phosphaten, Zinn und Zink.

2014 war Vietnam mit 41 Mio. t weltweit dreizehntgrößter Steinkohleförderer.

2005 wurden in Vietnam 51,33 Milliarden Kilowattstunden elektrischer Energie erzeugt, wohingegen es zehn Jahre zuvor nur 14,31 Milliarden waren. 21,24 Milliarden davon entfielen auf Wasserkraft, der Rest auf Wärmekraft.

Die Industrie trug 2007 42 % zum BIP bei und ist Hauptsäule des Wirtschaftswachstums des Landes mit 10,6 % Zuwachs in diesem Jahr. Der wichtigste Industriezweig ist die Herstellung von Textilien und Schuhen, daneben sind die Herstellung von Zement, Stahl und die Montage von Automobilen bedeutend. Etwa 40 % der Industriebetriebe Vietnams befinden sich nach wie vor in staatlicher Hand und mindestens ein Viertel davon arbeitet defizitär; trotzdem hat die Regierung 2002 beschlossen, dass alle Betriebe, die in sensitiven Bereichen tätig sind, zu 100 % unter staatlicher Kontrolle bleiben. Die vietnamesischen Betriebe sind in der Regel sehr klein und kapitalschwach. Es wird erwartet, dass viele davon die schnell fortschreitende wirtschaftliche Öffnung Vietnams nicht überleben werden.

Vor der Einführung von Đổi mới waren private Unternehmen, abhängig vom Wirtschaftssektor, entweder verboten oder vernachlässigbar. Nur Familienbetriebe waren legal. Einige Zeit nach dem Beginn der Reformen, im Jahr 2002, betrug der Anteil des privaten Sektors am BIP etwa 40 %, wobei der Anteil in der Landwirtschaft besonders hoch war und der Anteil an der Industrieproduktion etwa ein Drittel ausmachte.

Die Asienkrise 1998 hat auch Vietnam stark getroffen und das Wirtschaftswachstum (2001: etwa 5 %) sowie das Interesse ausländischer Investoren hatten zwischenzeitlich merklich nachgelassen. Die Regierung muss nun eine Reihe von Reformen umsetzen, um der Wirtschaft weiterhin ein starkes Wachstum zu ermöglichen. Dies beinhaltet vor allem eine Reform der Rechtsordnung, denn rechtliche Unsicherheit schreckt viele potentielle Investoren ab. Ebenso ist die Frage von Eigentum an Grund und Boden nicht restlos geklärt und die Unmöglichkeit, landwirtschaftliche Flächen in Industrieflächen umzuwidmen, hat dazu geführt, dass die Preise für Industrieland jene in Japan zeitweise überstiegen.

Die staatlichen Unternehmen stellen für die vietnamesische Wirtschaft ein Problem dar: sie sind meist unrentabel, international nicht konkurrenzfähig und haben eine hohe Menge an Krediten, die sie wahrscheinlich nicht zurückzahlen werden können und damit das ganze Bankensystem bedrohen. Eine Anzahl von Staatsbetrieben wurde bereits mit anderen Staatsbetrieben fusioniert, oder geschlossen. Der Prozess läuft aber wegen der sozialen Auswirkungen (Arbeitslosigkeit) recht schleppend.

Die Wirtschaft ist durch einen starken Unterschied zwischen dem Norden und dem Süden geprägt, wobei die Wirtschaft im Süden bedeutend dynamischer ist als im Norden. Dies wird meist damit begründet, dass die strategische Lage des Südens besser ist und dass dort Đổi mới – aufgrund der kürzer zurückliegenden Erfahrung mit den Marktmechanismen – schneller gegriffen hat als im Norden.

Die Inflation, die in den 1980er Jahren ein großes Problem darstellte, ist mittlerweile unter Kontrolle. Als Erinnerung an die Inflation bleiben astronomisch wirkende Preise mit vielen Nullen. Es gibt Scheine von 500 bis 500.000 Dong Nennbetrag und mittlerweile auch Münzen ab 500 Dong. Ein Euro ist etwa 25.000 Dong wert (2017), der größte Schein also nur gut 20 €, so dass es normal ist, dass man es bei großen Beträgen mit Bündeln, in Geschäften und Banken bei der Abrechnung auch mit Säcken von Geldscheinen zu tun hat.

Vietnams Außenhandel hat sich in den Jahren seit seiner Integration in die Weltwirtschaft rasant entwickelt und die vietnamesische Volkswirtschaft hat einen Offenheitsgrad erreicht, der etwa dem Thailands entspricht. 2016 wurden Waren im Wert von 176,6 Milliarden US-Dollar exportiert, was gegenüber 2015 einer Steigerung von 9,0 % entspricht. Wichtigste Exportprodukte sind Rohöl, Güter der Leichtindustrie, wie etwa Textilien, Schuhe oder Elektro- und Elektronikgeräte, Holzprodukte und landwirtschaftliche Erzeugnisse wie Meeresfrüchte, Fisch, Reis und Kaffee. Im Jahr 2008 war Vietnam der zweitgrößte Kaffeeproduzent der Welt. Hauptabnehmer für vietnamesische Erzeugnisse sind die USA, die EU-Länder, China und die anderen ASEAN-Staaten.

Vietnams Importe machten 2016 einen Wert von 174,1 Milliarden US-Dollar aus, sie stiegen gegenüber 2016 um 5,0 %. Importiert werden vor allem Maschinen und Fahrzeuge, Erdölprodukte, Eisen und Stahl, Textil- und Ledermaterialien sowie Computer und IT-Ausrüstungen. Die wichtigsten Lieferanten sind die VR China, die anderen ASEAN-Staaten, die EU, Südkorea und Japan. Die USA spielen als Lieferanten für Vietnam eine nur sehr untergeordnete Rolle.

Vietnam hat ein relativ hohes Handelsbilanzdefizit, das durch die Einnahmen aus dem Tourismus, durch Zuflüsse von ausländischen Direktinvestitionen, Entwicklungshilfe (2007: 5,4 Milliarden US-Dollar) und Überweisungen von Auslandsvietnamesen (2007: mehr als 5,5 Milliarden US-Dollar) ausgeglichen wird. Deshalb sind Leistungs- und Zahlungsbilanz unter Kontrolle. Aufgrund seiner Attraktivität als Produktionsstandort ist die Handelsbilanz Vietnams inzwischen positiv (Stand 2016). Sogar Chinesische Unternehmen haben ihre Produktion nach Vietnam verlagert, aufgrund der niedrigeren Lohnkosten.

Vietnam bleibt ein bevorzugtes Ziel für ausländische Direktinvestitionen. Der größte Investor Vietnams 2016 war Südkorea mit einem neu registrierten Investitionsbetrag in Höhe von 5,518 Mrd. USD, entsprechend 36,3 % des gesamten Investitionsvolumens. Danach folgten Singapur mit 1,59 Mrd. USD, Hong Kong mit 1,1 Mrd. USD und Japan 868 Mio. USD. Deutschland steht mit ca. 300 in Vietnam aktiven Unternehmen und einem kumulierten Investitionsbetrag von 1,357 Mrd. USD auf Rang 21 der Investorenliste.

Die Auslandsverschuldung ist mit etwa 16,6 Milliarden US-Dollar bzw. 37 % des BIP (2005) relativ niedrig. Dies liegt vor allem daran, dass Vietnam bis 1993 fast keine Kredite aus dem westlichen Ausland bekommen konnte. Bis 2016 stieg sie auf knapp 60 % des BIP an.

Die vietnamesische Währung ist inoffiziell an den US-Dollar gekoppelt ("Crawling Peg").

In Europa wurde Vietnam eher mit Vietnamkrieg, Kommunismus und Armut assoziiert und zählte zunächst nicht zu den klassischen Urlaubsländern. Bis vor wenigen Jahren wurde Vietnam deshalb fast ausschließlich von Leuten besucht, die sich für die Kultur interessieren, Abenteuer erleben wollten oder mit dem Land nach dem Vietnamkrieg in der einen oder anderen Art emotional verbunden waren.

Seit etwa 1999 erlebt Vietnam einen Boom im Tourismus. Neben Studienreisenden kommen auch immer mehr Rucksack-, Pauschal- und Badetouristen, letztere vor allem aus anderen asiatischen Ländern. Dies beruht z. T. auf einem „Ausweich-Effekt“, der mit der anhaltenden Gewalt und den Terroranschlägen auf den Philippinen und in Indonesien begründet ist, wohingegen Vietnam ein sicheres Land mit niedriger Kriminalität ist. Mittlerweile fahren auch Kreuzfahrtschiffe vietnamesische Häfen an bzw. ankern vor der Küste und bieten Tagesausflüge nach Ho-Chi-Minh-Stadt, Nha Trang, Đà Nẵng oder Huế an.

In den letzten Jahren wurden in einigen Fischerdörfern eilig einige internationale Hotels und Resorts hochgezogen, Restaurants für Ausländer eröffnet und der Aufbau einer touristischen Infrastruktur in Angriff genommen. Mehrere hunderttausend Menschen sind bereits im Tourismus beschäftigt.

Die wichtigen Wirtschaftskennzahlen Bruttoinlandsprodukt, Inflation, Haushaltssaldo und Außenhandel entwickelten sich in den letzten Jahren folgendermaßen:

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 57,2 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 48,0 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 4,5 % des BIP.

Die Staatsverschuldung betrug 2016 126 Mrd. US-Dollar oder 62,4 % des BIP.

Die Staatsausgaben entfielen in % des BIP unter anderem auf folgende Bereiche:


Die vietnamesische Kultur hat ihre Anfänge in der Dong-Son-Kultur vor etwa 3000 Jahren. Sie war anderen südostasiatischen Kulturen sehr ähnlich.

Die heutige vielfältige Kultur Vietnams ist eine Mischung aus originären lokalen Kulturen der Vietnamesen und anderer Völker des Landes, sowie chinesischen und westlichen Elementen.

Alle Medien Vietnams werden vom Staat und damit der Kommunistischen Partei Vietnams gesteuert, und nur Informationen, die von der Regierung genehmigt sind, dürfen veröffentlicht werden. Zeitungen, die sich diesen Regeln entzogen haben, sind wiederholt geschlossen worden; ebenso sind Dissidenten, die kritische Informationen über das Internet verbreitet haben, in Haft genommen worden. Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Vietnam Platz 175 von 180 Ländern. In Vietnam sitzen zwanzig Online-Aktivisten und Bürgerjournalisten im Gefängnis. Die Situation der Pressefreiheit im Land wird von Reporter ohne Grenzen als „schwierig“ eingestuft. 

In den Büchereien der Großstädten ist ausländische Literatur in verschiedenen Sprachen als Lehrmaterial erhältlich. Auch englischsprachige Printmedien werden in Vietnam angeboten. Dies sind zum einen Zeitschriften, die sich an Touristen richten und Reise- oder Unterhaltungsmöglichkeiten bewerben. Die meisten englischsprachigen Publikationen richten sich an Geschäftsleute und verkünden die neuesten Errungenschaften der Wirtschaftspolitik Vietnams. Ausländische Publikationen werden nicht zensiert, sind aber für die durchschnittlichen Vietnamesen sehr teuer. Man findet sie dort, wo sich Touristen konzentrieren. Alte Exemplare von ausländischen Zeitungen werden häufig von Straßenhändlern angeboten.
Das vietnamesische Radio und Fernsehen strahlt mehrere teils landesweite, teils regionale Programme aus. Im Fernsehen VTV 1 bis 7 gibt es am späteren Abend englische Kurznachrichten, der Rest des Programmes wird mit vietnamesischen Shows und ausländischen Filmen bestritten, die meist in der Originalsprache mit vietnamesischen Untertiteln gezeigt werden. Zudem sind ausländische Fernsehsender (z. B. ESPN, BBC, CNN, TV5 oder Deutsche Welle TV) mit entsprechenden digitalen Decodern empfangbar.

Es gibt einen vietnamesischen Kurzwellensender namens "Voice of Vietnam", der seit der Augustrevolution existiert und während des Vietnamkrieges hauptsächlich Propaganda gegen die Vereinigten Staaten ausstrahlte. Heute werden halbstündige Programme auf Englisch, Französisch, Russisch, und seit dem 1. März 2006 auch in deutscher Sprache produziert, die auch in Europa gehört werden können.

Bücher

Zeitschriften



</doc>
<doc id="5482" url="https://de.wikipedia.org/wiki?curid=5482" title="Wirtschaftswissenschaft">
Wirtschaftswissenschaft

Die Wirtschaftswissenschaft, auch Ökonomik und veraltend Ökonomie, ist die Wissenschaft von der Wirtschaft (Ökonomie). Die Wirtschaftswissenschaft untersucht den rationalen Umgang mit Gütern, die nur beschränkt verfügbar sind.

Eine erste systematische ökonomische Theorie stammt von Aristoteles, für den zielgerichtetes Handeln in der Ökonomie (; ) legitim ist. Er unterschied zwischen der Verwendung der materiellen Mittel für das gute Leben () und dem (naturgemäßen oder naturwidrigen) Erwerb dieser Mittel (). Die Hauptkonturen seiner ökonomischen Theorie finden sich in seinen Werken Polis und Nikomachische Ethik.

Die großen Denker der Antike und des Mittelalters gelten als Ahnherren der Wirtschaftswissenschaft. Auch das Mittelalter kannte bereits einige ökonomische Darstellungen. Thomas von Aquin behandelte in seinem zwischen 1265 und 1273 entstandenen Hauptwerk Summa theologica auch ökonomische Fragestellungen. Er stufte Landwirtschaft und Handwerk höher ein als den Handel, Gewinnstreben darf nicht den Schwächeren oder die Allgemeinheit schädigen. Kernstück bildete seine Lehre vom gerechten Preis. Joseph Schumpeter schrieb in seiner "History of Economic Analysis" (1954) die Entwicklung der wissenschaftlichen Untersuchung ökonomischer Zusammenhänge bereits den Spätscholastikern (im 14. und 15. Jahrhundert) zu. Allerdings unterschied sich die alteuropäische Ökonomik in ihrer Grundkonzeption stark von der heutigen Sichtweise. Nikolaus Kopernikus beschäftigte sich in der Folge der Inflation der Bauernkriegszeit mit Geldtheorie. Bei Johannes Buridan (1300–1385) finden sich erste Ansätze deskriptiver Ökonomie, indem er im Metallismus den Geldwert durch seinen Metallwert bestimmt sieht. Von Nicolaus Oresmius (1325–1385) stammte mit dem "Traktat über Geldabwertungen" (1373) das erste rein ökonomische Werk. 

Als „frühmoderne“ (auch „vorklassische“) Ökonomen werden die Merkantilisten und die Physiokraten eingeordnet. Seit etwa 1450 setzte der Merkantilismus ein, während dessen der Ökonom James Denham-Steuart mit seinem merkantilistischen "Summa" (1767) die erste Gesamtdarstellung der Ökonomie vorlegte und so die Ökonomie als eigenständige Wissenschaft begründete. Die „natürliche Ordnung“ () der Physiokratie war geprägt von Freiheit, Wettbewerb und Privateigentum, was am besten in ihrem 1751 von Vincent de Gournay geprägten Schlagwort „laufen lassen und geschehen lassen“ () zum Ausdruck kommt. Als Geburtsstunde der Wirtschaftswissenschaft in der heute verstandenen Form als Forschungsdisziplin mit eigenständigen Theoriegebilden wird häufig das Jahr 1758 genannt, in welchem der französische Arzt Francois Quesnay sein Hauptwerk "Tableau économique" veröffentlichte. Der zum Zeitpunkt der Veröffentlichung 64-jährige Gelehrte verstand die Abhängigkeiten von Geld- und Güterströmen als Kreislauf.

Auch der Schotte Adam Smith wird als Begründer der modernen Wirtschaftswissenschaft angesehen. Er veröffentlichte 1776 sein Buch "Der Wohlstand der Nationen" ("An Inquiry into the Nature and Causes of the Wealth of Nations") und kritisierte dort den bis dahin zumeist vorherrschenden Merkantilismus. Sein weitverbreitetes Werk fand in Großbritannien und den USA große Anerkennung und vermittelte erstmals die Idee einer neuen Wissenschaftsrichtung zur Untersuchung des wirtschaftlichen Handelns.

Seit David Ricardos Schrift "Principles of Political Economy and Taxation" (1821) setzte sich die „deduktive Methode“ mit quantitativer Betrachtung durch. Mit Verbreitung dieser Methode wurden die sozialen Rahmenbedingungen zunehmend aus der Untersuchung der Politischen Ökonomie eliminiert; es setzte sich zunächst ein rein logisch-mathematisches Verständnis der Marktverhältnisse durch (Volkswirtschaftslehre als Naturwissenschaft).

Unter dem Eindruck der Industrialisierung im 19. Jahrhundert entwickelten Karl Marx und Friedrich Engels die marxistische Wirtschaftstheorie. Die Klassische Nationalökonomie wurde, beginnend gegen Ende des 19. Jahrhunderts, abgelöst durch die Neoklassische Theorie, die die moderne Volkswirtschaftslehre bis heute prägt. Über mehrere Jahrzehnte – bis in die 1960er/70er Jahre – dominierten allerdings die Veröffentlichungen von John Maynard Keynes die Diskussion.

Seit der zweiten Hälfte des 20. Jahrhunderts gewinnen zunehmend die an Smith anknüpfenden Ideen des wirtschaftlichen Liberalismus Verbreitung. Als einer der bedeutendsten Ökonomen dieser Richtung gilt Milton Friedman. Daneben existieren unter anderem mit dem Sammelbegriff Heterodoxen Ökonomie oder Plurale Ökonomik kritische Positionen zum Mainstream.

Die Betriebswirtschaftslehre (BWL) als reine Beschreibung von Tätigkeiten und deren Zwecken in einzelnen Unternehmen begann schon im 15. Jahrhundert in Italien. Dort wurde im Jahre 1494 auch für das Rechnungswesen der BWL die Technik der doppelten Buchführung durch Luca Pacioli entwickelt und veröffentlicht. 

Als Begründer der "Handlungswissenschaft" gilt der Franzose Jacques Savary, der im Jahre 1675 das erste systematisch gegliederte Lehrbuch zur Betriebswirtschaft veröffentlichte: "Le parfait Négociant". Darin fasste er das gesamte kaufmännische Wissen seiner Zeit zusammen, beschrieb das Handelsgeschäft und die damit verbundenen Risiken und schlug unter anderem vor, zur bilanziellen Bewertung des betrieblichen Vermögens das Niederstwertprinzip anzuwenden und für den periodengerechten Abschluss transitorische Posten vorzusehen. Savary hatte großen Einfluss auf Paul Jacob Marperger aus Nürnberg, der in seinem 1714 veröffentlichten Hauptwerk "Nothwendige und nützliche Fragen über die Kauffmannschafft" ebenfalls das Handelsgeschäft beschrieb und die Handelsspanne rechtfertigte. Als Erster begründete er den wissenschaftlichen Anspruch des Fachs, indem er forderte, auf Universitäten öffentliche "Professores Mercaturae" zu verordnen.

Als Savarys eigentlicher Nachfolger im deutschen Sprachraum aber gilt jedoch Carl Günther Ludovici, der „sein Augenmerk allein auf das Zusammentragen und systematische Aufbauen des Stoffes“ richtete und mit seinem Werk „"Eröffnete Akademie der Kaufleute oder vollständiges Kaufmannslexikon" die beste Sammlung seiner Zeit schuf“ (Eduard Weber), in deren Anhang sich mit dem "Grundriss eines vollständigen Kaufmanns-Systems" eine systematische Darstellung der Handlungswissenschaft findet, die den Stoff gliedert in die Arten der Handels- und Handelshilfsbetriebe, die produktiven Faktoren (Waren, Personen, Sachmittel) sowie die Handelstätigkeit als Ein- und Verkauf.

Nach einer Zeit des Niederganges der Betriebswirtschaftslehre und der Verdrängung durch die Volkswirtschaftslehre nahm ihre Bedeutung seit Beginn des 20. Jahrhunderts erheblich zu.

Im deutschen Sprachraum wird die Wirtschaftswissenschaft üblicherweise in die Bereiche Betriebswirtschaftslehre (BWL) und Volkswirtschaftslehre (VWL, Nationalökonomie) unterteilt. Die zugehörigen Berufsbezeichnungen sind Wirtschaftswissenschaftler, Volkswirt und Betriebswirt (oder auch Ökonom).

Die Volkswirtschaftslehre untersucht grundlegende wirtschaftliche Zusammenhänge und Gesetzmäßigkeiten in einer Gesellschaft, sowohl in Bezug auf einzelne wirtschaftende Einheiten (Mikroökonomie) als auch gesamtwirtschaftlich (Makroökonomie). Erkenntnisobjekte sind das Wirtschaften, also der planmäßige und effiziente Umgang mit knappen Ressourcen zwecks bestmöglicher Bedürfnisbefriedigung in der Umgebung der Wirtschaft. Die Betriebswirtschaftslehre befasst sich mit den wirtschaftlichen Zusammenhängen und Gesetzmäßigkeiten einzelner Unternehmen und liefert Erkenntnisse über betriebliche Strukturen und Prozesse.

Um wirtschaftstheoretische Modelle empirisch zu überprüfen und ökonomische Phänomene quantitativ zu analysieren, werden ökonometrische Methoden eingesetzt, die letztlich auf mathematischen Modellen beruhen. Diese Modelle dienen auch dazu Prognosen für wirtschaftliche Entwicklungen zu erstellen.

Zur Wirtschaftswissenschaft im weiteren Sinne zählen auch interdisziplinäre Bereiche wie

Die Wirtschaftswissenschaft zählt zu den Sozialwissenschaften. Wirtschaftswissenschaftliche Aspekte werden auch in anderen sozialwissenschaftlichen Bereichen untersucht.





</doc>
<doc id="5492" url="https://de.wikipedia.org/wiki?curid=5492" title="Woody Allen">
Woody Allen

Woody Allen (* 1. Dezember 1935 als Allan Stewart Konigsberg in Brooklyn, New York), bürgerlich seit 1952 Heywood Allen, ist ein US-amerikanischer Komiker, Filmregisseur, Autor, Schauspieler und Musiker. Neben den über 50 Filmen als Drehbuchautor und Regisseur hat er zahlreiche Erzählungen, Theaterstücke und Kolumnen geschrieben. Darüber hinaus ist er passionierter Jazzmusiker.

Allen ist einer der produktivsten Filmregisseure unserer Zeit. Er war 24-mal für einen Oscar nominiert und erhielt die Auszeichnung viermal: 1978 für "Der Stadtneurotiker" in den Kategorien Bester Regisseur und Bestes Drehbuch, 1986 mit "Hannah und ihre Schwestern" und 2012 mit "Midnight in Paris" jeweils für das beste Drehbuch. Allen nahm die jeweilige Auszeichnung nie persönlich entgegen.

Allan Stewart Konigsberg wurde als Sohn jüdischer Eltern im New Yorker Stadtteil Brooklyn geboren. Seine jüngere Schwester Ellen (* 1943) ist Filmproduzentin. Beide Eltern, der Vater Martin Konigsberg, ein Diamantschleifer (* 25. Dezember 1900; † 13. Januar 2001) und die Mutter Nettie Cherry Konigsberg (* 8. November 1906; † 27. Januar 2002), waren in der Lower East Side von Manhattan aufgewachsen. Die Familie lebte in Flatbush, einem jüdisch geprägten Viertel. Die Großeltern waren deutsch- und jiddisch-sprechende Immigranten aus Russland und Österreich-Ungarn. In Allens Familie war Jiddisch neben dem Englischen noch geläufig. Obwohl die Eltern keine orthodoxen Juden waren, schickten sie ihren Sohn acht Jahre lang auf eine hebräische Schule. Später auf seine jüdische Herkunft angesprochen, äußerte Allen:

Danach absolvierte er die Public School 99 und die Midwood High School, wo "Red", so der Spitzname des schmächtigen Rotschopfs, zum ersten Mal auf sich aufmerksam machte – durch sein herausragendes Talent im Kartenspiel (ein geflügeltes Wort in der Midwood High war „Never play cards with Konigsberg“).
Er entwickelte ein gewisses Interesse fürs Theater, vor allem aber für das Kino und die Radioshows der 1940er Jahre, wie "Duffy’s Tavern" oder "The Great Gildersleeve". Allen bezeichnet sich selbst als Sportfan. Er spielte täglich bis zu zwei Stunden Klarinette.

Um sein Taschengeld aufzubessern, begann er damit, für die Agentur David O. Alber Gags zu schreiben, die an Kolumnisten großer Tageszeitungen verkauft wurden. Dank seines Talents und den von ihm geknüpften Beziehungen durfte er bald Entertainment-Stars wie Sid Caesar zuarbeiten. Als 16 Jahre alter Frischling im Showbusiness beschloss Konigsberg, fortan den Künstlernamen „Woody Allen“ zu tragen, wovon der Vorname von dem Klarinettisten Woody Herman entlehnt war. 1952, im Alter von 17 Jahren, ließ er seinen bürgerlichen Namen in "Heywood Allen" ändern. Den Vornamen "Heywood" wählte er in Referenz an den Jazzpianisten Eddie Heywood.

Trotz seines einträglichen Jobs belegte er – seinen Eltern zuliebe – einen "Communications Arts Course" an der New York University, wo er allerdings kaum je gesehen wurde. Ein prägendes Ereignis seiner Studienzeit war vermutlich, dass er, dem Rat seines Dekans folgend, einen Psychoanalytiker aufsuchte.

Er heiratete die sechzehnjährige Philosophiestudentin Harlene Rosen. Das junge Paar zog nach Manhattan, und Woody stieg vom Gagzulieferer zum Drehbuchautor auf.

Die Ed Sullivan Show, die Tonight Show und einige andere gehörten zu seinen Abnehmern. 1957 trat er, nominiert für den Emmy, erstmals aus dem Schatten seiner Auftraggeber und vor die Kamera. Ungefähr zu der Zeit ging seine Ehe mit Harlene in die Brüche. Bis sie ihn 1969 auf zwei Millionen Dollar verklagte, war sie der Hauptgegenstand seiner Gags, die er mittlerweile auch in Form von Prosa veröffentlichte. Er fing nun an, Theaterstücke zu schreiben und aufzuführen, aber sein ganzer Ehrgeiz galt dem Plan, Stand-up-Comedian zu werden, ein Alleinunterhalter, der Mitte der 1950er Jahre in Mode gekommen war.

Sein erster Auftritt 1960 im Nachtclub Duplex (Greenwich Village) geriet zum Fiasko. Seine Manager bezeichneten ihn als schlechtesten Komiker, den sie je gesehen hatten, und dennoch gelang es ihnen, aus dem schüchternen und linkischen Auftreten eine Masche zu machen und damit einen unverwechselbaren Stil zu kreieren, mit dem Allen zum Geheimtipp avancieren sollte. Es brauchte seine Zeit und sicher einige Mühen, aber Allen schuf daraus im Laufe der Jahre die Kunstfigur „Woody“, die bis vor kurzem nahezu unverändert in den meisten seiner Filme zu sehen war.

Vor seiner ersten Filmproduktion 1965 (Drehbuch zu "Was gibt’s Neues, Pussy?") schrieb Allen bereits 14 Jahre lang Witze, die er größtenteils als Stand-up-Comedian benutzte oder verkaufte. Er war auf dem besten Wege, mit seinem ungewöhnlichen intellektuellen Stil und den erfundenen Geschichten aus seinem Privatleben zu einer nationalen Berühmtheit aufzusteigen. Seine ersten Schritte in dem neuen Medium tat er nach demselben Rezept, das ihm auf der Bühne so großen Erfolg beschert hatte.

Etwa zu dieser Zeit lernte er die junge Schauspielerin Louise Lasser kennen, die er 1966 heiratete. 1971 wurde die Ehe wieder geschieden, aber Lasser durfte in "Bananas" und "Was Sie schon immer über Sex wissen wollten, aber bisher nicht zu fragen wagten" noch größere Rollen spielen. Seine Filme zwischen 1965 und 1975 zeichnen sich vor allem durch ihre Kombination von absurdem Sprach- und Bildwitz aus.

Mangels eigener formaler Mittel bediente sich Allen bereits vorhandener Erzählkonzepte, die er durch Satire neu beleuchtete, wie etwa bei "Was Sie schon immer über Sex wissen wollten, aber bisher nicht zu fragen wagten", einer Travestie der Aufklärungsfilme der 1960er Jahre. Charakteristisch für sein Frühwerk ist außerdem, dass (zumindest seiner eigenen Aussage zufolge) bis zu fünfzig Prozent des jeweiligen Films improvisiert wurde. Viele der eingesetzten Stilmittel, vor allem die Travestie und die teilweise arg surrealen Inhalte, sind auch in seinen 1971, 1973 und 1980 in Buchform veröffentlichten Kurzgeschichten zu finden.

Ab Mitte der 1970er Jahre wurde der humoristische Anteil in seinen Filmen zugunsten einer dramatisch anspruchsvolleren Handlung zurückgedrängt. In dieser Zeit agierte seine damalige Lebensgefährtin Diane Keaton häufig als Spielpartnerin und weibliche Hauptdarstellerin. Zum Ende des Jahrzehnts schließlich trat Mia Farrow in sein Leben, die fortan in sehr unterschiedlichen Rollen in seinen Filmen auftrat.

Der Film "Ehemänner und Ehefrauen" markiert das Ende der beruflichen Zusammenarbeit von Allen und Mia Farrow, die seit Anfang der 1980er Jahre ein Paar waren. Ihre Beziehung ging in die Brüche, als Mia Farrow 1992 von Allen aufgenommene Nacktfotos ihrer 21-jährigen Adoptivtochter Soon-Yi Previn (* 1970 oder 1972; das genaue Geburtsdatum ist nicht bekannt, da ihr Alter zum Zeitpunkt der Adoption geschätzt wurde) entdeckte und Allen daraufhin das Verhältnis mit dieser eingestand. Die nachfolgende gerichtliche Auseinandersetzung um das Sorgerecht für die übrigen Kinder brachte Allen stark in Bedrängnis.

Mia Farrow hatte Soon-Yi Previn zusammen mit ihrem damaligen Mann André Previn adoptiert, weshalb Allen Soon-Yi – im Gegensatz zu Farrows anderen Adoptivkindern Dylan und Moses – nicht hatte adoptieren dürfen. Farrow und Allen hatten zudem seit 1987 den gemeinsamen Sohn Satchel Farrow, der sich später in Ronan Farrow umbenannte. Nach Angaben der Schauspielerin ist der biologische Vater von Ronan Farrow möglicherweise ihr Ex-Ehemann Frank Sinatra.

Mia Farrow gewann die gerichtliche Auseinandersetzung; sie erhielt am 7. Juni 1993 das alleinige Sorgerecht für Dylan und Satchel zugesprochen. Allens Adoptivsohn Moses durfte selbst entscheiden und lehnte einen weiteren Kontakt zu Allen ab (erst ca. 20 Jahre später und nach einer Entfremdungsphase von Mia Farrow nahm er wieder Kontakt zu Allen und Soon-Yi auf). „Das Gericht stellte in so gut wie allen Punkten seine elterliche Eignung infrage und nannte Allens Verhalten den Kindern gegenüber 'missbrauchend und gefühllos'“, hieß es dazu am 8. Juni 1993 in der "New York Times".

Zur Beziehung von Allen zu Mia Farrows Adoptivtochter Soon-Yi ergibt sich aus den Gerichtsdokumenten im Sorgerechtsstreit zwischen Allen und Farrow und nach Mia Farrows eigener Erinnerung, dass Allen bis 1990 (Soon-Yi war damals etwa 18 bis 20 Jahre alt) „had little to do with any of the Previn children, (but) had the least to do with Soon-Yi“. Erst Mia Farrow soll Allen damals bestärkt haben, den Kontakt zu ihrer Adoptivtochter Soon-Yi zu verstärken. Dabei ist erwähnenswert, dass Farrow während ihrer Beziehung zu Allen mit ihren Kindern in einer Wohnung am Central Park West lebte. Allen hingegen wohnte in seiner Wohnung an der Fifth Avenue. Angeblich übernachtete Allen in den zwölf Jahren seiner Beziehung zu Mia Farrow kein einziges Mal in deren Wohnung. Im Dezember 1997 heirateten Woody Allen und Soon-Yi Previn, die nun ihrerseits zwei Töchter adoptierten.

Ein kontroverses Element des Sorgerechtsstreit waren die seit August 1992 gegen Allen erhobenen Vorwürfe, er habe seine damals siebenjährige Adoptivtochter Dylan sexuell missbraucht. Zu einer juristischen Klärung kam es jedoch nicht: Die Untersuchungsbehörden konnten keine Beweise für die von dem Mädchen geschilderten sexuellen Übergriffe feststellen. Ein rechtsmedizinisches Gutachten kam zu dem Ergebnis, dass Dylan Farrow nicht missbraucht worden war. Der zuständige Staatsanwalt, der die Vorgehensweise der Gutachter in mehreren Punkten für unzureichend hielt, stellte in seiner abschließenden Beurteilung dagegen einen hinreichenden Verdacht fest, dass der Missbrauch stattgefunden habe, was eine staatliche Anklageerhebung gerechtfertigt hätte. Er begründete seinen letztlichen Verzicht auf ein Strafverfahren gegen Allen mit seiner Sorge um das zu schützende Wohl des Kindes: Der bereits durch die Trennung der Eltern und den Sorgerechtsstreit geschädigten Dylan wolle er die zu erwartenden negativen Konsequenzen eines solchen Verfahrens ersparen. 

Seit 2013 werden die Missbrauchsvorwürfe gegen Woody Allen erneut öffentlich diskutiert, seit Dylan Farrow, die zuvor diesbezüglich von ihrer Mutter vertreten worden war, sich in einem Gespräch mit dem Magazin "Vanity Fair" erstmals selbst öffentlich über ihre Erfahrung als Missbrauchsopfer äußerte. Anfang 2014 konkretisierte sie die Vorwürfe in einem offenen Brief auf der Webseite der New York Times. 

Moses Farrow, der gemeinsame Adoptivsohn des ehemaligen Paares, widersprach anschließend der Darstellung seiner Schwester und schilderte diese als Produkt einer von Mia Farrow betriebenen, innerfamiliären Rache- und Entfremdungskampagne gegen Allen. Allen selbst veröffentlichte zwei Tage später in der "New York Times" eine in eigenen Worten verfasste, abschließende Stellungnahme, in der er diese Darstellung bekräftigte und seinerseits Vorwürfe gegen Mia Farrow erhob. Dylans weiterer Bruder Ronan Farrow ergriff mit einem ausführlichen Artikel für den "Hollywood Reporter" im Mai 2016 Partei für seine Schwester. Anlass war ein vom Magazin kurz zuvor abgedrucktes langes Interview mit Woody Allen, in dem die Missbrauchsvorwürfe nur in einer fehlerhaften Randnotiz erwähnt worden waren, der eine spätere Korrektur nachgereicht wurde. Farrow beklagte in seinem Text neben der oft unkorrekten Darstellung der Tatsachen im konkreten Fall seiner Schwester eine grundsätzlich häufig mangelnde öffentliche Unterstützung für Opfer sexuellen Missbrauchs und wies dabei auf Parallelen zum Fall des prominenten Komikers Bill Cosby hin.

Schon Woody Allens frühe Komödien tragen oft dunklere Untertöne, etwa "Die letzte Nacht des Boris Gruschenko", "Der Schläfer" oder "Bananas". Sein Film "Der Stadtneurotiker" lässt sich dennoch als Bruch zu seinen vorherigen Filmen werten. Er verweist in Form oder Inhalt weniger auf filmische Vorbilder, zeigt aber stärker als zuvor eine autobiografische Färbung. Zusammen mit Kameramann Gordon Willis, mit dem Allen hier zum ersten Mal zusammenarbeitete, schuf Allen mit ungewöhnlichen Methoden Stilbrüche, die für sein weiteres Werk bezeichnend sind. Allen, mittlerweile 40 und frisch getrennt von Diane Keaton, zieht erstmals Bilanz über sein bisheriges Leben.

1978 stellte er seinen ersten ernsten Film vor, "Innenleben". Allen, der in diesem Film nicht als Schauspieler auftritt, erzählt vom Zerfall einer bürgerlichen Großfamilie. Die Leere und Anonymität der Innenräume – daher auch der Originaltitel "Interiors" – kontrastiert die emotionalen Verwirrungen der darin wohnenden Protagonisten. "Innenleben" gilt als deutliche Hommage an den schwedischen Regisseur Ingmar Bergman.

In "Manhattan" bezog Allen 1979 wieder stärker komödiantische Elemente ein. Anders als bei seinen meisten Filmen beginnt "Manhattan" nicht mit dem typischen Allen-Vorspann – Schwarzbild mit weißen Anfangstiteln, unterlegt mit Jazzmusik. Stattdessen sieht man eine Abfolge von New-York-Ansichten in Schwarzweiß, wobei Allen hier zum ersten und bisher einzigen Mal als Regisseur auf Breitwandbilder in Panavision setzte (2,35:1). Man hört dazu Allen, der mehrmals versucht, einen Anfang zu formulieren, abbricht und wieder neu beginnt und schließlich erklärt: „New York was his town, and it always would be.“ Dann erklingt große symphonische Musik von George Gershwin. Allen ist hier als krisengeplagter Fernsehautor Isaac Davis zu sehen, der zwischen verschiedenen Frauen steht und sich erst am Ende entscheiden kann. "Der Stadtneurotiker" und "Manhattan" gelten als Allens bis dato größte Erfolge.

Der letzte Teil der sogenannten New-York-Trilogie, "Stardust Memories" aus dem Jahr 1980, ist stark an Federico Fellinis "8½" angelehnt. Wie die beiden Vorgänger ist "Stardust Memories" deutlich autobiografisch angehaucht, auch wenn Allen später behauptete, es gebe zwischen dem Protagonisten seines Films und ihm selbst keinerlei Parallelen. Er spielt in New York und handelt von einem Filmschaffenden und dessen großer Verachtung für sein Publikum.

Unter manchen Filmkritikern herrscht die Meinung vor, dass Allens Filme in dieser Periode nur mit sowjetischen oder polnischen Beispielen zu vergleichen seien, tatsächlich hat er sich noch nie sehr viel aus Hollywood und der US-amerikanischen Filmbranche gemacht. Er war nicht einmal bei der Oscarverleihung, als "Der Stadtneurotiker" vier Academy Awards gewann.

Was seine Filme betrifft, lassen sich zwei Linien unterscheiden. Auf der einen Seite entwickelt er seine Komödien weiter, die nun zu Tragikomödien werden. Die Story wird aufwendiger, sie spielt oft in mehreren Handlungs- und Realitätsebenen. Die Filme sind auch aussagekräftiger als seine frühen Komödien, ihre Aussage wird nicht mit plumpen Gags kaschiert. Fast alle haben ein trauriges Ende (engl. "sad ending"), wie zum Beispiel "The Purple Rose of Cairo", in dem die Protagonistin am Ende noch unglücklicher und einsamer ist als zu Anfang. Bei "Eine Sommernachts-Sexkomödie" zeigt sich Allen 1982 von William Shakespeares "Ein Sommernachtstraum" und dessen amourösen Verwicklungen beeinflusst. Mit "Zelig" präsentiert er 1983 eine Satire über einen Menschen, der sich chamäleonartig ständig an seine Umwelt anpasst, der unter chassidischen Juden zum chassidischen Juden und unter Nazis zu einem Nazi wird. Allen porträtiert in diesem Film, der wie ein Dokumentarfilm scheinbar mit Ausschnitten aus Wochenschauen inszeniert ist, einen Menschen, dessen Unpersönlichkeit ihn durch die Zeiten treiben lässt. 1984 erzählt Allen in "Broadway Danny Rose" von einem New Yorker Künstleragenten, der sich in mafiöse Kreise verirrt, und porträtiert das Milieu von Standup-Komikern, das die Basis für seine eigene Karriere war.

Mit seiner Komödie "Hannah und ihre Schwestern" erzielte Allen noch einmal einen großen Erfolg, sowohl in kommerzieller Hinsicht als auch bei den Kritikern. Der Film erhielt drei Oscars; der für das beste Drehbuch ging an Woody Allen. Andererseits experimentiert er mit alternativen Formaten; so dreht er beispielsweise mit "September" oder "Eine andere Frau" Dramen, die ohne jede Komik auskommen. Kritiker warfen ihm vor, mit solchen Filmen nur den von ihm verehrten Bergman kopieren zu wollen; sie übersahen jedoch Allens eigenständigen Ansatz. Auch ein Film über seine Kindheit, "Radio Days", am einfachsten zu beschreiben als Kostümfilm, entsteht. Im Gegensatz zum ebenfalls autobiografischen Theaterstück "The Floating Lightbulb" durfte dieser sogar nach seiner Uraufführung weiter gezeigt werden, das Stück zog Allen nämlich wieder zurück. Bei "Verbrechen und andere Kleinigkeiten" schildert er eine tödliche Mordintrige, vermeidet jedoch nicht gelegentliche komödiantische Elemente. Bei diesem Film arbeitete Allen zum ersten Mal mit dem Bergman-Kameramann Sven Nykvist zusammen.

Nach "Verbrechen und andere Kleinigkeiten" fand Allen allmählich zu einem neuen Stil. An die Stelle seiner schwarzseherischen Tragikomödien traten nun andere, die wieder leichter und beschwingter anmuten. In Filmen wie "Alice" hat er zwar nach der Meinung vieler Kritiker Schwierigkeiten, zu einem überzeugenden Ende zu kommen. Aber diese neueren Filme stehen dennoch für eine im Gegensatz zu den frühen Komödien dramatisch fundierte Entwicklung, mit einem im Gegensatz zu den Filmen der achtziger Jahre positiven Tenor.

In "Ehemänner und Ehefrauen" schloss Allen 1992 die Reihe seiner Filme mit Mia Farrow ab. Der Film handelt von der Liebe und der Beziehungsfähigkeit, wobei der Treue eine eindeutige Absage erteilt wird.

Kurzfristig ersetzte Diane Keaton Mia Farrow 1993 bei "Manhattan Murder Mystery", der im weitesten Sinne eine Fortsetzung des "Stadtneurotikers" ist. Die Handlung war als Nebenhandlung für diesen Film vorgesehen, wurde aber aus Zeitgründen gestrichen. Danach verhalf Allen gleich in zwei aufeinanderfolgenden Filmen einer Schauspielerin zu einem Oscar (Dianne Wiest und Mira Sorvino), später bei "Sweet and Lowdown" Sean Penn und Samantha Morton zu Nominierungen.

1995 drehte Allen "Geliebte Aphrodite". Formal sehr streng, in oft langen Plansequenzen, erzählt der Regisseur (und Hauptdarsteller) vom langweiligen Leben mit seiner Frau (Helena Bonham Carter) und vom Seitensprung mit einem süßen, aber ziemlich einfältigen Callgirl (Mira Sorvino), die, ohne es zu wissen, die Mutter seines Adoptivsohnes ist. Der Film brilliert mit einem sporadisch auftretenden griechischen Chor, der in einem Original-Amphitheater, und zwar dem in Taormina auf Sizilien, gedreht wurde. Der Chor übernimmt dabei – skandierend und tanzend – das Erzählen der Rahmenhandlung, wird aber zusehends in das Geschehen, das sich in New York abspielt, involviert. Irgendwann sitzt der in Lumpen gekleidete griechische Chorführer in Allens Upper-East-Side Luxus-Apartment und hilft ihm beim Ehebruch, indem er den Zettel mit einer Hand vor dem Verrutschen sichert, auf dem Allen, verdeckt vor seiner Frau telefonierend, die Telefonnummer des Callgirls notiert, mit der er später ein Verhältnis haben wird. Eine geradezu typische Allen-Szene, in der banale Realität mit Kunstfiguren anderer Epochen vermischt wird. Etwas Ähnliches hatte er auch in "Mach’s noch einmal, Sam" mit Humphrey Bogart konstruiert. Dabei setzte er dort wie hier das umstrittene, oft ernste Image dieser Figuren zur Steigerung der Komik ein.

Mit Julia Roberts, Goldie Hawn, Drew Barrymore und anderen drehte er 1996 in New York, Venedig und Paris das auf bekannte Jazz-Standard-Songs aufgebaute Musical "". Bei "Harry außer sich" wurde Allen 1997 in der Bildsprache dem Originaltitel "Deconstructing Harry" – der nicht zufällig auf den Dekonstruktivismus anspielt – gerecht. Er dekonstruiert die physische Umgebung, verwendet kurze "Jump Cuts" und erzählt von einem Mann, der von anderen Menschen nur noch unscharf („out of focus“) gesehen wird.

1998 drehte Allen "Celebrity – Schön. Reich. Berühmt.", eine Gesellschaftskomödie, in der er selber nicht auftrat, jedoch einen selbstironischen Leonardo DiCaprio vorführte, der einen abgehobenen Hollywoodstar spielte und damit seine eigene reale Existenz satirisch brach. Im selben Jahr lieh Allen der Ameise "Z" in dem Warner-Brothers-Streifen "Antz" seine Stimme. In der deutschen Synchronfassung übernahm dies sein Standardsynchronsprecher Wolfgang Draeger. Z-4195 – so die genaue Bezeichnung – weist dabei viele Facetten der von Woody Allen bekannten, von ihm konzipierten und verkörperten Charaktere auf; zu Beginn des Films liegt Z auf der Couch eines Psychiaters.

1999 erschien der schon oben genannte Film "Sweet and Lowdown", des Weiteren im Jahr 2000 "Schmalspurganoven" mit Hugh Grant, 2001 "Im Bann des Jade Skorpions", 2002 "Hollywood Ending", 2003 "Anything Else" und 2004 "Melinda und Melinda". In einigen deutschen Kinos wurden die Filme mit Verzögerung im Original mit Untertiteln gezeigt; so wurde "Anything Else" erst im September 2004 dem deutschen Publikum vorgestellt.

Der Thriller "Match Point" aus dem Jahr 2005 wurde auf Festivals bejubelt. Viele Kritiker sprachen von einem neuen, wiedererstarkten Allen. Es war sein erster Film, der ausschließlich in London spielte und produziert wurde und damit der erste Film seiner „europäischen Phase“. Auch seine beiden nächsten Filme, die Krimi-Komödie "Scoop – Der Knüller" (2006) und das Drama "Cassandras Traum" (2007), spielten in London. In letzterem spielten Colin Farrell und Ewan McGregor zwei Brüder im Londoner Arbeitermilieu, die in die Kriminalität abdriften. 2008 folgte die Komödie "Vicky Cristina Barcelona" mit Scarlett Johansson, Rebecca Hall, Javier Bardem und Penélope Cruz, die für ihre Leistung einen Oscar als beste Darstellerin in einer Nebenrolle gewann.

Anfang September 2008 gab Allen mit einer Inszenierung von Giacomo Puccinis Einakter "Gianni Schicchi" sein hochgelobtes Debüt als Opernregisseur. Die in Zusammenarbeit mit der "Los Angeles Opera" produzierte Inszenierung entstand auf Vermittlung von Generaldirektor Plácido Domingo, dem Allen zwei Jahrzehnte zuvor eine Oper versprochen hatte.

2009 folgte die Komödie "Whatever Works" mit dem Komiker Larry David in der Hauptrolle. Der Film eröffnete das Tribeca Filmfestival in New York. 2010 folgte die Tragikomödie "Ich sehe den Mann deiner Träume (You Will Meet A Tall Dark Stranger)," die in Cannes Premiere feierte. Auf der Besetzungsliste stehen Freida Pinto, Josh Brolin, Lucy Punch, Anthony Hopkins, Antonio Banderas und Naomi Watts.

Allen dreht weiterhin pro Jahr einen Film, so dass auch sein Spätwerk einen beachtlichen Umfang annimmt. 2010 drehte er mit Owen Wilson und Marion Cotillard "Midnight in Paris". Die Liebeskomödie um einen erfolgreichen US-amerikanischen Drehbuchautor (Wilson), der ins Paris der 1920er Jahre zurückversetzt wird, eröffnete 2011 die 64. Filmfestspiele von Cannes. Der Film war sowohl an der Kinokasse als auch bei der Kritik ein großer Erfolg. Für das Drehbuch erhielt Allen 2012 einen Oscar und einen Golden Globe. Darüber hinaus erhielt "Midnight in Paris" Oscar-Nominierungen in den Kategorien „Bester Film“, „Beste Regie“ und „Bestes Szenenbild“.

2012 folgte die Episodenkomödie "To Rome With Love". Auf der Besetzungsliste stehen Jesse Eisenberg, Ellen Page, Penélope Cruz, Alec Baldwin, Roberto Benigni, Judy Davis, Greta Gerwig und Alison Pill. Auch Woody Allen selbst übernahm wieder eine Rolle. Der Film erzählt episodisch vier skurrile Geschichten, die allesamt in Rom stattfinden. Dabei orientiert sich der Film an Boccaccio 70 aus dem Jahr 1962, der von Federico Fellini, Luchino Visconti, Mario Monicelli und Vittorio De Sica gedreht wurde.

Im selben Jahr übernahm Allen auch eine Rolle als Schauspieler in John Turturros Komödie "Fading Gigolo". 2013 kam sein Film "Blue Jasmine", eine freie Adaption von Tennessee Williams' Theaterstück "Endstation Sehnsucht," in die Kinos. Die Hauptrolle übernahm Cate Blanchett, die für ihre Leistung mit dem Oscar als beste Hauptdarstellerin ausgezeichnet wurde. Sally Hawkins erhielt eine Nominierung als beste Nebendarstellerin, Allen eine Nominierung für das beste Originaldrehbuch.

2014 lief sein 43. Film, die in Südfrankreich gedrehte Komödie "Magic in the Moonlight" mit Emma Stone und Colin Firth in den Hauptrollen, in den deutschen Kinos an. Im selben Jahr drehte er in Newport (Rhode Island) "Irrational Man", der beim Internationalen Filmfestival in Cannes 2015 uraufgeführt wurde. In den Hauptrollen sind Joaquin Phoenix und, wie schon in "Magic in the Moonlight", Emma Stone zu sehen. US-Kinostart war im Juli 2015, in die deutschen Kinos kam er am 15. November 2015.

Am 11. Mai 2016 eröffnete Allen mit seinem Film "Café Society", der außerhalb des Wettbewerbs gezeigt wurde, die 69. Internationalen Filmfestspiele von Cannes. Allen hatte bereits 2002 mit "Hollywood Ending" und 2011 mit "Midnight in Paris" die Filmfestspiele eröffnet. Seit "Manhattan" (1979) ist "Café Society" der 14. Film, den der Regisseur außerhalb des Wettbewerbs in Cannes zeigen lässt. Der Film handelt von einem jungen Mann (dargestellt von Jesse Eisenberg), der während der 1930er Jahre nach Hollywood geht, um dort sein Glück beim Film zu versuchen. Dort verliebt er sich und lernt die titelgebende Café-Society-Kultur kennen. In weiteren Rollen sind Kristen Stewart, Blake Lively, Parker Posey und Steve Carell zu sehen. Der deutsche Kinostart war am 10. November 2016.

Im Frühjahr 2016 hat Allen außerdem für Amazon Video die sechsteilige Serie "Crisis in Six Scenes" gedreht, die seit dem 30. September 2016 verfügbar ist. 
Allens 46. Film "Wonder Wheel", mit Justin Timberlake und Kate Winslet in den Hauptrollen feierte am 15. Oktober 2017 seine Weltpremiere beim "New York Film Festival" und kam am 11. Januar 2018 in die deutschen Kinos.

Am 11. September 2017 begannen in New York die Dreharbeiten zu Allens jüngsten Film " A rainy day in New York" mit "Jude Law" und "Selena Gomez" in den Hauptrollen.

Von 1965 bis 2006 wurde Allen von Wolfgang Draeger als Standardsprecher synchronisiert. In dieser Zeit wurde er nur in zwei Filmen von anderen Schauspielern gesprochen. In "Casino Royale" (1967) lieh ihm Horst Sachtleben seine Stimme, in der Kinoversion von "Was Sie schon immer über Sex wissen wollten, aber bisher nicht zu fragen wagten" wurde er von Harald Juhnke gesprochen. Für die Fernsehauswertung dieses Films im ZDF 1987 wurde sein Part jedoch wiederum von Draeger synchronisiert. Woody Allen war sogar der Meinung, die deutsche Synchronstimme von Wolfgang Draeger passe besser zu ihm als seine eigene.

Für den Film „To Rome With Love“ (2012) wurde Draeger als Sprecher durch Freimut Götsch ersetzt, da Draegers Stimme von Allen als zu alt empfunden wurde. Seither hat Draeger aber Allen wieder synchronisiert.

Allen spielt regelmäßig Klarinette in der "Eddy Davis New Orleans Jazz Band", einer Jazz Band im New Orleans Stil (Traditional Jazz), Zuerst spielte die Band jeden Montag im Club „Alexander's“, von den 1970er bis 1990er Jahren in Michael’s Pub, danach lange Zeit im Carlyle Hotel in Manhattan.

Mit seiner Band geht Allen regelmäßig auf Tournee, so war er im März 2010 und im März 2011 für jeweils drei Konzerte in Deutschland, auch im Juli 2017 führt ihn seine Tournee wieder nach Deutschland. Die Europa Tour von 1996 war Gegenstand des Dokumentarfilms "Wild Man Blues" von Barbara Kopple (auch als DVD erschienen und der Soundtrack als CD bei RCA). Die Band tourte auch schon durch Griechenland, die Türkei sowie durch Südamerika. Gelegentlich tritt Allen mit ihr auf Festivals auf, wie etwa 2008 auf dem Montreal Jazz Festival.

In seinen Filmen setzt er regelmäßig Jazzmusik ein, so etwa in dem Film "Sweet and Lowdown" um einen an Django Reinhardt und dessen kurze Zeit in den USA angelehnten Jazzgitarristen. Ebenfalls Swing-orientiert war der Soundtrack zum Film "Radio Days", der in den 1940er Jahren spielt. In dem Film "Der Schläfer" trat Allen selbst als Jazzmusiker auf, mit dem Preservation Hall Orchester in New Orleans und dem New Orleans Funeral Ragtime Orchestra.

Zwei mit seiner Lebensgefährtin Soon-Yi Prévin adoptierte Kinder nannte er nach den Jazzmusikern Manzie Johnson und Sidney Bechet Manzie und Bechet.

Woody Allen schreibt alle seine Drehbücher auf einer alten Schreibmaschine "Olympia SM 3" von 1952. Textkorrekturen schreibt er jeweils neu, schneidet diese danach mit einer Schere aus und tackert sie dann über den alten Text.

Während seiner Zeit auf der Highschool trainierte er mehrere Monate lang für das Amateurboxturnier Golden Gloves. Allerdings verweigerten seine Eltern dann die erforderliche schriftliche Zustimmung für seine Teilnahme.

Im Januar 2006 erwarb Allen für sich und seine Familie ein Townhouse in der Upper East Side von Manhattan, 118 East 70th Street. Der Kaufpreis von 25,9 Mio. Dollar – für ein vergleichsweise bescheidenes Domizil – galt selbst für New Yorker Verhältnisse als sehr hoch. Seine Nachbarin ist Susan Weber Soros, die Ex-Frau des Milliardärs George Soros.

Legende: B – Buch, D – Darsteller, R – Regie


Legende: UA – Uraufführung


















Außerdem eine dreistellige Anzahl von Nominierungen für die oben genannten und andere Filmpreise sowie der Prinz-von-Asturien-Preis 2002. Zudem wurde er 2001 in die American Academy of Arts and Sciences und 2010 in die American Philosophical Society gewählt.





</doc>
<doc id="5494" url="https://de.wikipedia.org/wiki?curid=5494" title="Wörterbuch">
Wörterbuch

Ein Wörterbuch ist ein Nachschlagewerk, das Wörter oder andere sprachliche Einheiten in einer meist alphabetisch sortierten Liste verzeichnet und jedem Eintrag (Lemma) erklärende Informationen oder sprachliche Äquivalente zuordnet.

Ein Wörterbuch im engeren Sinn dient zum Nachschlagen sprachlicher Information, während der Ausdruck in der weiteren Bedeutung auch andere nach Stichwörtern gegliederte Nachschlagewerke mit primär sachbezogener Information sowie Mischformen beider Typen umfasst.

Der Ausdruck "Wörterbuch" ist im Deutschen eine durch das Niederländische "(woordenboek)" beeinflusste Lehnübersetzung des griechischen Wortes "lexikon (biblion)": „Wörter betreffendes Buch“. Es ist ein zusammengesetzter Begriff, der nach einem Fremdwort gebildet wurde, indem alle Bestandteile des Fremdwortes einzeln ins Deutsche übersetzt wurden. Bis ins 17. Jahrhundert hinein wurden die Begriffe "Lexikon" und "Dictionarium" bevorzugt verwendet; dann trat "Dictionarium" zugunsten der neu eingeführten Übersetzung "Wörterbuch" (auch "Wortbuch") zurück; der Begriff "Lexikon" blieb erhalten. Seit der Einführung des Wortes hat sich die Bedeutung von "Wörterbuch" im allgemeinen Sprachgebrauch überwiegend auf „Sprachwörterbuch“, die Bedeutung von "Lexikon" hingegen in der Tendenz auf „Sachwörterbuch“ verengt, wobei "Lexikon" oft auch als Synonym für „Enzyklopädie“ verwendet wird. Im Ergebnis treten damit "Wörterbuch" einerseits und "Lexikon" bzw. "Enzyklopädie" andererseits oft als Gegenbegriffe auf.

Fachsprachlich, besonders in der Lexikographie (Wörterbuchforschung), wird jedoch "Wörterbuch" auch noch in der weiteren Bedeutung als Oberbegriff für alle Arten von Nachschlagewerken mit einer Gliederung nach Stichwörtern beibehalten. Ebenso wird der Begriff "Lexikon" in fachsprachlichen Zusammenhängen weiterhin in seiner weiteren Bedeutung, also unter Einschluss sprachlexikographischer Werke, und in der Sprachwissenschaft auch speziell für das Inventar der Lexeme eines Sprechers oder einer Sprachgemeinschaft gebraucht.

Ein Wörterbuch im engeren Sinn (auch "Sprachwörterbuch") dient der Vermittlung sprachlichen Wissens. Seine Auswahl der Lemmata (Morpheme, Lexeme, Phrasen und Phraseologismen) soll den Wortschatz einer Einzelsprache oder einen Teilwortschatz dieser Sprache (etwa einen Dialekt, Soziolekt oder Idiolekt) abdecken. Bei den zugeordneten Informationen handelt es sich primär um sprachliche Informationen, die die Schreibung, Aussprache und grammatischen Eigenschaften wie Wortart, Genus und Flexion des Lemmas, seine Herkunft, Bedeutung, Verwendungsweise und Übersetzbarkeit betreffen. Sie werden in Form von erklärenden Angaben oder durch Zuordnung vergleichbarer Einheiten aus der gleichen Sprache, etwa in einem "Synonymenwörterbuch" und einem "Reimwörterbuch", oder aus einer oder mehreren anderen Sprachen in einem "Übersetzungswörterbuch" dargestellt. Sachinformationen zu den von den Wörtern bezeichneten Realien können dabei ebenfalls einbezogen werden, wenn das für die Erklärung der Wortbedeutung oder Wortgeschichte erforderlich ist, sie bilden jedoch keinen Selbstzweck. Eigennamen (Personen und Orte) werden in einem Sprachwörterbuch normalerweise nicht erklärt, abgesehen von Nachschlagewerken zur Namenforschung und Ortsnamenforschung (Onomastik und Toponomastik).

Ein neuer Ansatz ist es, nicht einzelne Wörter, sondern nur Sätze zu übersetzen („Sätzebuch“). Die Suche nach einzelnen Wörter geschieht mithilfe einer elektronischen Suchmaschine, die das Wort im Satzzusammenhang präsentiert; ein Beispiel hierfür ist Tatoeba.

Bei einem Sachwörterbuch (auch "Realwörterbuch, Reallexikon, Realenzyklopädie") steht demgegenüber die Vermittlung von Sach- und Weltwissen statt Sprachwissen im Vordergrund (siehe "Enzyklopädie"). Das Lemma ist nicht als Element eines Wortschatzes Gegenstand sprachlicher Information, sondern beschreibt als thematisches Stichwort das Thema der Sachinformation. Sprachliche Eigenschaften des Lemmas werden nicht oder nur insoweit einbezogen, wie es dem Verständnis der vom Lemma bezeichneten Sache dient. Die Auswahl der Lemmata bezieht in einem Sachwörterbuch in der Regel auch Eigennamen ein und dient der Abdeckung und Strukturierung eines bestimmten Sach- oder Wissensbereiches, der ein spezielles Fachgebiet sein oder dem Anspruch nach auch das gesamte verfügbare Wissen über die Welt umfassen kann.

Ein enzyklopädisches Wörterbuch (auch "Sprach- und Sachwörterbuch, integriertes Wörterbuch"), in Deutschland seit den 1930er-Jahren vereinzelt auch Allbuch genannt und dem Typ nach besonders in Frankreich verbreitet, stellt sprachlexikographische und sachlexikographische Information gleichrangig nebeneinander und will Grundfunktionen beider Wörterbucharten erfüllen. Auch sonst treten die Typen von Sprach- und Sachwörterbuch oft in gemischter Form auf, besonders im Bereich der Fachlexika, wo fachsprachliche Wörterbücher oft auch einen eigengewichtigen Anteil an Sachinformation, oder fachspezifische Sachwörterbücher zusätzlich sprachliche Informationen zum Lemma und zu dessen Übersetzbarkeit integrieren.

Ausgehend vom allgemeinsprachlichen Gesamtwörterbuch, das ein umfassendes Informationsprogramm für den Kern der Allgemeinsprache bietet (Beispiel: Duden Universalwörterbuch), kann man eine Reihe von Wörterbuchtypen unterscheiden, die im Verhältnis zu diesem Wörterbuchtyp entweder in der Lemmaauswahl oder im Informationsprogramm beschränkt sind:









Ein Wörterbuch besteht in der Regel aus "Außentexten" und dem "Wörterverzeichnis".

Im Wörterverzeichnis wird zwischen Makrostruktur und Mikrostruktur unterschieden:
Unter der Makrostruktur versteht man die Auswahl der Lemmata, ihre Anordnung sowie die Anordnung der Außentexte. Eine der wichtigsten makrostrukturellen Entscheidungen ist, wie die Lemmata angeordnet werden sollen. In den meisten Wörterbüchern geschieht das alphabetisch.

Wie der einzelne Wörterbucheintrag organisiert ist, d. h. die Anordnung der Wortinformation, ist Teil der Mikrostruktur.

Alphabetische Sortierung

In der alphabetischen Anordnung gibt es unterschiedliche Möglichkeiten:

Beispiel für eine Wortstrecke in unterschiedlicher Sortierung:
Die nestalphabetische Anordnung ist besonders platzsparend (bei gedruckten Wörterbüchern ein wichtiger Faktor) für die Benutzer jedoch auch am verwirrendsten, da Wörter aus verschiedenen Wortfamilien in einem Nest zusammengefasst werden. Als am benutzerfreundlichsten gilt die glattalphabetische Sortierung, da jedes Stichwort auf einer separaten Zeile steht und einen eigenen Eintrag eröffnet, was allerdings kostbaren Druckraum in Anspruch nimmt. Insofern stellt die nischenalphabetische Anordnung einen Kompromiss dar. Sie gruppiert Lemmata, ohne die alphabetische Reihenfolge zu durchbrechen.
Anhand der unterschiedlichen Auflagen des "Duden Großwörterbuch" kann man die verschiedenen Anordnungsformen beobachten: Bildete das sechsbändige Großwörterbuch noch Nester, so sind diese in der zweiten, achtbändigen Ausgabe durch Nischen ersetzt und die zehnbändige Ausgabe von 1999 verfolgt einen glattalphabetischen Ansatz.


Nichtalphabetische Sortierung


Weitere makrostrukturelle Überlegungen betreffen die "Auswahl" der Lemmata und das "Datensortiment", also welche Angaben es beispielsweise zu bestimmten Wortarten geben soll.

Unter der Mikrostruktur dagegen versteht man die konkreten Angaben, die zu einem Lemma gemacht werden. Diese Angaben sind in den meisten Sprachwörterbüchern durch "Textverdichtung" (Abkürzungen, Paraphrasen etc.) gekennzeichnet, um möglichst viele Angaben auf möglichst wenig Raum zusammenzufassen. In Sachwörterbüchern werden dagegen sehr unterschiedlich lange Texte geboten. In vielen Fällen werden den Artikeln Definitionen vorangestellt, dann folgen die Informationen in fortlaufender Prosa. Im Text oder am Ende des Textes werden Querverweise auf andere Artikel gegeben, die inhaltlich mit dem Text in Beziehung stehen. Am Schluss des Artikels werden – vor allem bei Enzyklopädien und Konversationslexika – vielfach Literaturhinweise gegeben.

Wörterbuchbenutzung

Wörterbücher sollen den Nutzern helfen, lexikalische Wissenslücken zu schließen, müssen also so strukturiert sein, dass Informationen schnell und gezielt nachgeschlagen werden können. Sowohl ein- als auch mehrsprachige Wörterbücher verlangen vom Nutzer zwei Voraussetzungen:

Die "Wörterbuchbenutzungsforschung" – oder auch "Wörterbuchdidaktik" – beschäftigt sich mit Benutzererwartungen an Wörterbücher "(„Welche Fragen werden in welchem Wörterbuch beantwortet?“)" und untersucht die Bedingungen erfolgreicher Wörterbuchbenutzung. Die Erkenntnisse fließen in die Erstellung neuer Wörterbücher ein, oder in existierende Wörterbücher, die auf andere Medien übertragen werden.

Einen ausführlichen Überblick über deutsche Wörterbücher und Lexikographie gibt Haß-Zumkehr (2001).






</doc>
<doc id="5495" url="https://de.wikipedia.org/wiki?curid=5495" title="Werner Herzog">
Werner Herzog

Werner Herzog (* 5. September 1942 in München; eigentlich "Werner Herzog Stipetić") ist ein deutscher Regisseur, Produzent, Schauspieler und Schriftsteller. Er zählt zu den bedeutendsten Vertretern des „Neuen Deutschen Films“ und des internationalen Autorenfilms. Die Zeitschrift "Time" rechnete ihn 2009 zu den hundert einflussreichsten Personen der Welt.

Herzog wuchs in dem bayerischen Dorf Sachrang unweit der Grenze zu Österreich auf; die Familie war vor den Bombenangriffen auf München dorthin geflohen. Mit zwölf Jahren zog er mit seiner aus einem kroatischen Offiziersgeschlecht stammenden Mutter Elisabeth Herzog (geb. Stipetić, 1912–1984) nach München. Sein Vater war Dietrich Herzog (1910–1989). Sein Großvater war der Philologe Rudolf Herzog, der Bekanntheit durch die Ausgrabung des Asklepieions auf der Insel Kos erlangte. Werner Herzog hat einen älteren Bruder Tilbert Herzog und einen jüngeren Halbbruder Lucki Stipetić, der bis heute mit ihm als Produzent zusammenarbeitet und eine jüngere Schwester Sigrid Herzog, die Regisseurin ist. In seiner Kindheit war ihm die Existenz des Kinos lange nicht bewusst, bis er im Alter von elf Jahren in der Dorfschule seinen ersten Film sah.

Kurzzeitig bewohnte er mit seiner Familie in München eine Pension in der Elisabethstraße mit Klaus Kinski, der bereits zu dieser Zeit mit exzentrischen Allüren auffiel. Während der Gymnasialzeit arbeitete Herzog in Nachtschicht als Punktschweißer in einer Stahlfabrik. Er machte am Maximiliansgymnasium in München das Abitur und studierte, neben seinen ersten Filmproduktionen, Geschichte, Literatur und Theaterwissenschaft an der Universität München. Professor Werner Vordtriede war einer seiner Lehrer. Ein Stipendium, das er aber bereits nach einer Woche abbrach, brachte ihn nach Pittsburgh in die Vereinigten Staaten.

1962 veröffentlichte Herzog, 19-jährig, seinen ersten Film, den zwölfminütigen Kurzfilm "Herakles". 1963 gründete er seine eigene Produktionsfirma "Werner Herzog Filmproduktion" in München. Seinen ersten abendfüllenden Spielfilm "Lebenszeichen" drehte Herzog im Alter von 24 Jahren. Für diesen Film erhielt er finanzielle Unterstützung vom Kuratorium junger deutscher Film. In der Kategorie "Bester erster Film" wurde ihm dafür ein Deutscher Filmpreis verliehen. Der Film erschien 1968.

Ende 1974 ging er in 22 Tagen zu Fuß von München nach Paris, um die kranke Filmkritikerin Lotte Eisner zu besuchen und sie damit – in seiner Sichtweise – zu retten; darüber schrieb er das Buch "Vom Gehen im Eis".

Bei den Internationalen Filmfestspielen von Berlin 2010 bekleidete Herzog das Amt des Jurypräsidenten. 2012 wurde ihm das Bundesverdienstkreuz 1. Klasse verliehen.

Viele seiner Filme drehte Werner Herzog auf Englisch. In fünf seiner bekanntesten Filme besetzte er die Hauptrolle mit Klaus Kinski. Über die oftmals schwierige Beziehung der beiden drehte er 1999 den Dokumentarfilm "Mein liebster Feind".

Werner Herzogs Werk beinhaltet neben seinen Spielfilmen auch zahlreiche dokumentarische Arbeiten. Nach "Cobra Verde" drehte er nur wenige Spielfilme, aber zahlreiche Dokumentationen für Fernsehen und Kino. Auch in früheren Jahren drehte er regelmäßig Dokumentarfilme. Der vielleicht bemerkenswerteste davon ist "Gasherbrum", über eine Doppel-8000er-Besteigung von Reinhold Messner und Hans Kammerlander, in dem Herzogs Verständnis vom Dokumentarfilm Ausdruck findet: Er verweigert sich dem "Cinéma vérité" und der Einschätzung, dass Kameras Authentizität reproduzieren könnten. Vielmehr geht es in den dokumentarischen Arbeiten auch immer um die eigene Perspektive auf den Gegenstand, um Herzog selbst also. Dies geht so weit, dass er manchen dokumentierten Personen Wörter und Aussprüche in den Mund legt und die Arbeiten zudem stark ästhetisiert.

Der Kurzfilm "Werner Herzog Eats His Shoe" dokumentiert Herzogs Einlösen einer Wette. Herzog ermutigte damit Errol Morris, seinen ersten Film "Gates of Heaven" tatsächlich fertigzustellen. Herzog kochte seine Schuhe und aß einen bis auf die Sohle auf. Regie führte Les Blank, der später auch eine vielgerühmte Dokumentation über die beschwerlichen Dreharbeiten von "Fitzcarraldo" drehte "(Burden of Dreams)".

Mitte der achtziger Jahre wandte er sich der Oper zu und debütierte 1985 mit der Inszenierung von Ferruccio Busonis "Doktor Faust" am Teatro Comunale in Bologna. Bekannt wurde er durch seine Aufführungen von Wagner, insbesondere "Lohengrin" bei den Bayreuther Festspielen im Jahr 1987, und Beethovens "Fidelio" an der Mailänder Scala.

2009 erhielten Herzogs Spielfilme "Bad Lieutenant – Cop ohne Gewissen" und "My Son, My Son, What Have Ye Done" Einladungen in den Wettbewerb der 66. Filmfestspiele von Venedig. Damit ist Herzog der zweite Filmemacher, der mit zwei Werken um den Goldenen Löwen konkurrierte.

Am 18. September 2012 führte Herzog bei einem Konzert der amerikanischen Rockband The Killers in New York Regie. Das Konzert wurde live im Internet übertragen.

Auf der New Yorker Whitney Biennale 2012 führte Herzog die Videoinstallation "Hearsay of the Soul" (Hörensagen der Seele) auf. 2015 wird die Arbeit im Rahmen einer Sonderausstellung in Köln gezeigt.

Im Rahmen der Arbeiten an seinem Film "Aguirre, der Zorn Gottes" hatte sich Herzog am 24. Dezember 1971 am Flughafen von Lima mit allen Mitteln bemüht, für sich und ein Drehteam Plätze auf dem später verunglückten LANSA-Flug 508 zu bekommen, allerdings vergeblich: Nachdem am Vortag alle Flüge auf dieser Strecke wetterbedingt ausgefallen waren (auch er und sein Team waren ursprünglich für den 23. gebucht) und es am 24. nur diesen einzigen Flug nach Pucallpa gab, war der Flug so überlaufen, dass er keine Plätze mehr bekam und so um Haaresbreite der Katastrophe entging. Jahrzehnte später drehte er nach persönlichen Gesprächen mit Juliane Koepcke, der einzigen Überlebenden dieses Fluges, seinen 1999 erschienenen Dokumentarfilm "Julianes Sturz in den Dschungel" über dieses Unglück und Koepckes Rettung.

Bei den "Simpsons" lieh er in der 15. Folge der 22. Staffel einem deutschen Pharmaunternehmer im englischen Original und in der deutschen Synchronisation seine Stimme.

Der niederländische Autor und Biologe Maarten ’t Hart warf Werner Herzog 2010 Tierquälerei vor. Herzog hatte ’t Hart für den Film "Nosferatu" als Berater beigezogen, jedoch gegen dessen Rat eine Massenszene mit Ratten durchführen lassen. Die Tiere wurden auf Wunsch Herzogs schwarz gefärbt. Im Zusammenhang damit seien sie in kochendes Wasser getaucht worden. Laut ’t Hart seien dabei 50 % der Ratten umgekommen.




Adolf-Grimme-Preis

Academy Awards

Berlinale

British Academy Film Award

BBC Four World Cinema Awards

Bayerischer Filmpreis

Bayerischer Poetentaler

Boulevard der Stars

Cannes Film Festival

César

Directors Guild of America

Emmy

Europäischer Filmpreis

International Documentary Film Festival Amsterdam

Festival International de Programmes Audiovisuels, Biarritz

Krakowski Festiwal Filmowy

Deutscher Filmpreis

Gotham Award

Gilde-Filmpreis

Internationale Hofer Filmtage

Independent Spirit Award

International Documentary Association

Las Vegas Film Critics Society Awards

Internationales Filmfestival von Locarno

Los Angeles Film Critics Association Award

Internationales Filmfestival Mannheim-Heidelberg

Melbourne International Film Festival

National Society of Film Critics

New York Film Critics Circle Award

San Francisco International Film Festival

Festival Internacional de Cine de San Sebastián

Sundance Film Festival

São Paulo International Film Festival

Association Française de la Critique de Cinéma

Filmfestspiele von Venedig

Verdienstorden der Bundesrepublik Deutschland








</doc>
<doc id="5496" url="https://de.wikipedia.org/wiki?curid=5496" title="Werner Nekes">
Werner Nekes

Werner Nekes (* 29. April 1944 in Erfurt; † 22. Januar 2017 in Mülheim an der Ruhr) war ein deutscher Filmregisseur und Sammler historischer optischer Objekte.

Werner Nekes wuchs in Duisburg-Hamborn auf und ging in Oberhausen und Mülheim (Ruhr) zur Schule. Er studierte ab 1963 Sprachwissenschaft und Psychologie in Freiburg und Bonn, wo er einen studentischen Filmclub leitete. Ab 1965 begann er mit 8-mm-, dann mit 16-mm-Film erste Experimentalfilme zu drehen. Er lernte die damals noch malende Dore O. kennen, sie zogen nach Hamburg und heirateten 1967. Dore O. war an den meisten seiner Filme v. a. als Darstellerin beteiligt und begann auch eigene Experimentalfilme zu drehen. Mit Franz Winzentsen, Helmut Herbst, Thomas Struck, Klaus Wyborny und Heinz Emigholz gründeten sie die Filmmacher-Cooperative Hamburg.

1968 erhielt sein 10-minütiger Kurzfilm "schwarzhuhnbraunhuhnschwarzhuhnweißhuhnrothuhnweiß oder put-putt" den Internationalen Filmpreis in São Paulo. 1969 erhielt er einen Bambi für sein bisheriges Werk. und im Folgejahr das Filmband in Silber für "jüm-jüm" (1967). Im Jahr 1972 wurden Filme von Werner Nekes auf der Documenta 5 in Kassel in der Abteilung „Filmschau: New European Cinema“ gezeigt.

1980 drehte er mit "Uliisses" seinen ersten Langfilm. 1986 entstand Nekes’ bis heute bekanntester Film, die Schlagerfilm-Parodie "Johnny Flash" mit Helge Schneider als aufsteigendem Schlagerstar in der Hauptrolle. Christoph Schlingensief war als Aufnahmeleiter, Kamera-Assistent und Darsteller beteiligt. Sie hatten sich 1982 kennengelernt. Beide lehrten an der Hochschule für Gestaltung in Offenbach (1982–1984). Anfang der 70er Jahre hatte Nekes schon einmal eine Professur für Experimentalfilm an der Hochschule für bildende Künste Hamburg (1970–1972). 

Nekes sammelte alles, was mit der Vorgeschichte des Films zu tun hat, wie optische Spielzeuge, Laternae magicae, Panoptiken und vieles mehr, deren Techniken er auch in seinen Filmen verwendete. So trug er im Laufe der Jahre eine umfangreiche Sammlung von internationaler Bedeutung zusammen, die schon mehrfach ausgestellt sowie in Büchern, der Fernsehserie "Media Magica" (1996) und in Nekes’ Dokumentarfilm "Was geschah wirklich zwischen den Bildern?" (1985) dokumentiert wurde. 

Versuche, diese Sammlung in einer Dauerausstellung zu präsentieren, scheiterten bisher. Das bislang letzte Projekt sah vor, die Sammlung in einem ausgedienten Wasserturm auf dem Gelände der ehemaligen Landesgartenschau in Mülheim zu zeigen. In dem Turm befindet sich seit 1992 eine Camera obscura.

2009 wurde Werner Nekes in die Klasse der Künste der Nordrhein-Westfälischen Akademie der Wissenschaften und der Künste aufgenommen.

Werner Nekes lebte zuletzt in Mülheim an der Ruhr.

Die Filmemacherin Ulrike Pfeiffer drehte 2016 den Dokumentarfilm "Werner Nekes. Das Leben zwischen den Bildern", der am 16. Februar 2017 im Rahmen einer Gedenkveranstaltung für Nekes während der Berlinale uraufgeführt wurde. 





</doc>
<doc id="5500" url="https://de.wikipedia.org/wiki?curid=5500" title="Wicca">
Wicca

Wicca (manchmal ausgesprochen [], für männlich „Hexer“; Aussprache entsprechend dem englischen Vorbild []) ist eine neureligiöse Bewegung. Sie versteht sich als neu gestaltete, naturverbundene Spiritualität und als Mysterienreligion. Um eine Mysterienreligion handelt es sich in dem Sinne, dass auch auf Erkenntnis des eigenen Lebens und innere Transformation Wert gelegt wird. Wicca hat seinen Ursprung in der ersten Hälfte des 20. Jahrhunderts und ist eine Glaubensrichtung des Neuheidentums. Wicca sieht sich auch als „Religion der Hexen“, und die meisten Anhänger bezeichnen sich selbst als Hexen. Viele der unterschiedlichen Wicca-Richtungen sind im Gegensatz zu den meisten neuheidnischen Bewegungen explizit synkretistisch.

Es bestehen zahlreiche Parallelen zum Kult der Großen Göttin; im Wicca sind hingegen ein weiblicher und ein männlicher Gott gleichberechtigte Partner und Repräsentanten einer polaren Natur.

Gemeinsam ist fast allen Wicca das Feiern der acht Jahreskreisfeste und die am Mondzyklus orientierten magischen Rituale (Esbats).

Der Begriff "Wicca" wurde aus dem Angelsächsischen übernommen, wo das Wort "wicca" (männliche Form) ‚Hexer‘, ‚Zauberer‘ bedeutet; die weibliche Form dieses Wortes ist "wicce" (manchmal auch ausgesprochen [], Mehrzahl für beide Geschlechter "wiccan" []). Das heutige englische Wort "witch" für ‚Hexe‘ geht sprachgeschichtlich auf "wicce" bzw. "wicca" zurück. Der moderne Begriff erscheint zunächst in der Form "Wica" in Gardners "Witchcraft Today" (1954) als Bezeichnung für die Vertreter der von ihm beschriebenen „Alten Religion“ (und nicht für Wicca selbst). Die Schreibweise "Wicca" ist erstmals 1969 belegt.

Völlig unabhängig von Gerald Gardner verwendete J. R. R. Tolkien den Begriff "wicca" für die beiden Zauberer Gandalf und Saruman bereits 1942 in seinem ersten Manuskript zum Band "Die zwei Türme" des Werkes "Herr der Ringe". Dies ist durch eine Fußnote im Kapitel 20, "The Riders of Rohan", im siebten Band der zwölfbändigen Dokumentation seines Schaffens durch seinen Sohn Christopher belegt. In der 1969 gedruckten Ausgabe des "Herr der Ringe" ersetzte Tolkien jedoch den Begriff "wicca" durch "wizard".

Über die Wortherkunft des altenglischen "wicca", "wicce" gibt es verschiedene Ansichten. Das palatalisierte /ʧ/ spricht für die Ableitung des maskulinen aus dem femininen Substantiv oder aber Ableitung von beiden aus dem zugehörigen Verb "wiccian", ‚behexen‘. Möglicherweise verwandt ist "wigle", ‚Wahrsagen‘, das seinerseits auf dem rekonstruierten indoeuropäischen Wortstamm *"weg" beruht, der vermutlich Wachsamkeit und Lebhaftigkeit bezeichnete. Dafür spricht auch die heute noch im Niederdeutschen gebräuchliche Bezeichnung "Wicker" für einen Hellseher oder Zauberer. Jacob Grimm verbindet das Wort dagegen mit Gotisch "weihs", ‚heilig‘.

Gerald Gardner und andere Wicca-Autoren haben stattdessen eine Herkunft von dem altenglischen Wort "wita" für ‚weiser Mensch‘ oder "witan" für ‚wissen‘ vorgeschlagen, was darauf Bezug nimmt, dass Hexen ursprünglich als weise Frauen galten. Aufgrund dieser Ableitung wird Wicca heute manchmal auch als "„Craft of the Wise“" bezeichnet. Diese Etymologie wurde bereits im 19. Jahrhundert von Walter William Skeat vorgeschlagen und würde "wicca" als eine Abwandlung des früheren "witga" deuten. Da keine altenglische Schreibung mit "t" bekannt ist, gilt diese Etymologie aber als unwahrscheinlich.

Robert Graves (1948) schlägt eine Verbindung zur Wortwurzel *"wei" vor, die ‚biegen‘ bedeutet, und sieht daher Wortverwandtschaften mit "willow" (‚Weide‘) und "wicker" (geflochtene Korbwaren). Der Bezug zur Hexerei sei demnach die Verwendung von Magie zum „biegen“ bzw. „beugen“ der Kräfte der Natur oder auch in abstrakterer Form als Bezug zur Vorstellung der „Schicksalsweber“ (siehe auch Nornen). Auch Graves Vermutung gilt als unwahrscheinlich.

In Deutschland war „Wicca“ kurzzeitig ein markenrechtlich geschützter Begriff, die Eintragung wurde jedoch nach einigen Monaten gelöscht. Ebenfalls üblich sind die Bezeichnungen „Alte Religion“ „Alter Pfad“, die ausdrücken, dass Wicca sich in der Tradition ursprünglicher Kulte sieht, die Magie und Religion nicht trennen. Auch Charles Leland bezeichnete in seinem Buch "Aradia – Die Lehre der Hexen" von 1899 den Hexenkult als „la Vecchia Religione“, also als „die Alte Religion“.

Wicca gehört zu den mitgliederreichsten Gruppierungen im Spektrum des Neopaganismus und ist insbesondere im angloamerikanischen Raum besonders stark verbreitet. In den USA ist Wicca seit 1994 als Religion staatlich anerkannt. Die Schätzungen über die Zahl der Menschen in den USA, die sich der Wicca-Religion zugehörig fühlen, gehen weit auseinander, da die Abgrenzungen gegen andere neuheidnische Richtungen nicht eindeutig sind und manche Menschen verschiedene Formen der Spiritualität praktizieren. Um das Jahr 1990 schätzten seriöse Quellen die Zahl auf mehr als 200.000 in den USA, 30.000 in Großbritannien und weltweit auf 800.000. Der "American Religious Identification Survey" (ARIS) zeigte einen Anstieg der Wicca-Bekenner in den USA von 134.000 im Jahre 2001 auf 342.000 sieben Jahre später 2008.

Die Wicca-Religion wurde begründet durch Gerald Brousseau Gardner (1884–1964). Er gab an, er sei in einen bestehenden Hexencoven, den "New Forest Coven", initiiert worden. Ab 1969 wurde erstmals die Schreibweise „Wicca“ verwendet. In späteren Jahren waren neben Gardner Vivianne Crowley, Doreen Valiente und Eleanor Bone weitere führende Personen, die zur Entwicklung dieser Glaubensrichtung beitrugen. Valiente verfasste sehr viele der heute als „traditionell“ bekannten Texte (z. B. "The Charge of the Goddess" und "The Wiccan Creed"), ebenso überarbeitete sie verschiedene Passagen des "Buches der Schatten", die von Gerald Gardner aus den Werken von Aleister Crowley übernommen wurden. Gardner und Valiente trennten sich wegen Unstimmigkeiten, und Patricia Crowther trat ihre Nachfolge an.
Die Alexandrische Linie wurde von Alex Sanders gegründet, wobei nicht zweifelsfrei geklärt ist, von wem Sanders initiiert wurde. Die Gründung der Alexandrischen Linie galt zunächst als Schisma, gehört aber mittlerweile zum Wicca im engeren Sinne.

Nach Amerika wurde Wicca durch Raymond und Rosemary Buckland – Initianten der Gardnerischen Linie – gebracht, wo Buckland in späteren Jahren auch eine eigene Linie, das "Seax Wica", gründete. In Amerika wurde Wicca unter anderem durch Zsuzsanna Budapest und Miriam Simos (besser bekannt als Starhawk) mit Elementen feministischer Göttinnen-Spiritualität angereichert (Dianic Wicca, Reclaiming-Tradition), während durch Selena Fox neoschamanistische Einflüsse ihren Einzug fanden, so z. B. afrikanische und indianische Traditionen wie Trommeln, ekstatische Tänze und Visionsarbeit.

Janet und Stewart Farrar veröffentlichten in den 1970er und 1980er Jahren zahlreiche bis dahin nicht allgemein bekannte Rituale in ihren Büchern, insbesondere die meisten der heutigen acht Sabbat-Rituale. Den Vorwurf, sie hätten mit dieser Bekanntmachung geheimer Rituale gegen das Schweigegebot der Wicca-Coven verstoßen, wies Janet Farrar zurück. Sie entgegnete, dass diese Rituale entweder schon zuvor durch Gerald Gardner selbst oder durch Doreen Valiente öffentlich gemacht worden waren oder sie erst durch die Farrars auf der Grundlage älterer Quellen neu geschaffen worden waren.

Sowohl Raymond Buckland als auch Janet Farrar und ihr neuer Ehemann Gavin Bone (sie heirateten circa ein Jahr nach Stewarts Tod) lehnten die Ausschließlichkeit der Initiation innerhalb der bestehenden Traditionslinien als Dogmatismus ab. Sie befürworteten eine Selbstinitiation, die eigentlich eine Initiation durch Göttin und Gott sei, wie schon Doreen Valiente in ihrem Buch 1978 hervorgehoben habe. Auch Vivianne Crowley lehnte Selbstinitiation nicht grundsätzlich ab, hielt sie aber nur für eine Notlösung. Das moderne eklektische Neowicca der sogenannten freifliegenden Hexen, die selbstinitiiert und nicht in Coven organisiert sind, führte unter anderem durch die Bestseller von Scott Cunningham und Silver RavenWolf zu einer enormen Steigerung der Verbreitung von Wicca seit dem Ende der 1980er-Jahre. Gleichzeitig kam es aber auch zu gegenläufigen Bestrebungen, die gardnerianischen und alexandrischen Traditionen zu bewahren.

Die traditionellen Wicca-Anhänger schließen sich üblicherweise einem Konvent oder "Coven" (englisch für Hexenzirkel) an, einem Arbeits- und Anbetungskreis, einer Organisationsform, die auf den schottischen Hexenglauben zurückgehen soll, aber wahrscheinlich ältere Quellen hat.

Keine zwei Coven gleichen einander in ihren Glaubensansichten, und selbst die beschriebene Grundstruktur ist nicht notwendigerweise in allen Coven gleich. Die Stilrichtungen reichen von ägyptisch über keltisch bis hin zu indianisch oder synkretistischen Mischungen – in der Ethnologie wird dergleichen als eklektische Vorgehensweise bezeichnet. Die meisten Coven legen Wert auf eine Verbindung zu vorchristlichen Religionen.

Ein Coven besteht nach traditioneller Ansicht idealerweise aus 13 Personen (eine Zahl, die bei Wicca eine besondere Bedeutung hat), die möglichst in Arbeitspaare aus Frau und Mann gegliedert sind. Das Paar Hohepriesterin/Hohepriester wird als eine Einheit und quasi als eine Person gesehen. Größere Coven neigen in der Regel dazu, sich über kurz oder lang aufzuspalten und einen Tochter-Coven hervorzubringen. Ein Coven hat in der Wicca-Tradition eine Hohepriesterin und einen (mindestens zweitgradig initiierten) Hohepriester als Leiter sowie eine "Maiden" („das Mädchen“ oder „die Jungfrau“) als Stellvertreterin der Hohepriesterin. Allen dreien fallen in den meisten Ritualen bestimmte Aufgaben zu. Der Coven trifft sich insbesondere an den Feiertagen (Sabbat) und den Vollmondtagen (Esbat). Einige, aber nicht alle Coven praktizieren „im Himmelskleid“ (von engl. "skyclad", d. h. rituelle Nacktheit), als ein Zeichen ihrer Verbundenheit zur Natur und der persönlichen Freiheit. Andere wiederum tragen besondere Gewänder oder wieder andere nur Alltagskleidung.

Falls die Mitgliederzahl eines Coven zu groß wird, gründet ein Arbeitspaar des Muttercovens mit beliebig vielen mitgenommenen Mitgliedern einen neuen Kreis, traditionell mindestens drei Meilen (etwa fünf Kilometer) vom Muttercoven entfernt. Die Entfernung soll dem Entstehen einer organisierten Religion bzw. Kirche vorbeugen.

Die Zeremonie zur Initiation, d. h. der offiziellen religiösen Einweihung in einen Hexen-Coven, wird von einem/einer gegengeschlechtlichen Hohepriester(in) abgehalten. Durch die Initiation erfolgt (unter anderem) die freiwillige Widmung des Initianten an ein bestimmtes göttliches Wesen und die an antike Mysterienkulte angelehnte Vermittlung von Wissen und tieferer Einsicht. Im Wicca existieren insgesamt drei Grade: Der erste Grad ist der Grad der Göttin und des Wassers. Der zweite Grad ist der Grad des Gottes. Mit dem zweiten Grad wird der Wicca zum Hohepriester geweiht. Mit diesem Rang wäre es bereits möglich, einen eigenen Coven zu leiten. Die Verbindung mit der Universalenergie, die Vereinigung von Gott und Göttin – dem Animus und der Anima (nach Ansicht von Vivianne Crowley, 2004) – ist die Arbeit im dritten Grad, was jedoch von manchen Wicca auch anders gesehen wird.

Im Rahmen der Initiation erfolgt auch die Weitergabe eines handschriftlich angelegten „Buchs der Schatten“. Dieses innerhalb eines Covens weitergegebene Buch ist aber nicht statisch, sondern wird im Laufe der Zeit ergänzt oder erweitert. In einigen Traditionen wird zusätzlich die Urversion der jeweiligen Linie tradiert. Diese enthält vor allem Rituale, weniger Dogmen oder feste Richtlinien. Ferner haben viele Coven noch ein eigenes „Buch der Schatten“, in das geheimes Wissen, Rituale, Anrufungen und eigene Erfahrungen eingetragen werden können.

Wicca glauben daran, dass alles im Grunde eine Einheit und miteinander verbunden ist (Holismus). Sie verehren die Natur als heilig, da sie eins ist mit dem göttlichen Urgrund und dem Menschen in körperlicher und geistiger Hinsicht Kraft spendet (Chthonismus). Ein wichtiges Glaubensprinzip ist die Regel „Wie oben, so unten“ ("„As above, so below“"), die besagen soll, dass in allen Bereichen des Kosmos, im Großen wie im Kleinen, die gleichen polaren Ordnungsprinzipien am Werk seien und dass sich auch im Kleinsten stets das Ganze widerspiegele. Der Satz stammt ursprünglich aus der hermetischen Schrift "Tabula Smaragdina". Er repräsentiert inzwischen eine weit verbreitete Auffassung im Bereich der Esoterik und des New-Age-Denkens. Wicca ist also keine dualistische Religion, die Gott und Schöpfung als voneinander getrennt betrachtet und somit eher panentheistisch statt theistisch zu verstehen.
Die beiden polaren Mächte, die im Mittelpunkt stehen, werden als dreifache Mond-Göttin (Jungfrau, Mutter, Weise) oder Erdmutter bzw. Muttergöttin sowie als dualer Gehörnter Gott (Fruchtbarkeitsgott und Todesgott; häufig assoziierte Aspekte: geopferter Jahresgott, Grüner Mann, Himmelsvater, Sonnengott) personifiziert. Ähnlich wie in der Psychologie von Carl Gustav Jung werden diese Gottheiten jedoch von vielen Wicca nur als Archetypen des kollektiven Unbewussten oder als Symbole für Anima und Animus im individuellen Unterbewusstsein angesehen.
Typischerweise steht die Göttin für das passive und lunare weibliche Prinzip (Yin) und der Gehörnte Gott für das aktive und solare männliche Prinzip (Yang), wobei das Horn Zeugungskraft, Macht und Stärke symbolisiert. Diese beiden Prinzipien sind gleichberechtigt und beide notwendig, denn das allganze Göttliche wird als Vereinigung dieser Polaritäten verstanden. Wicca ist daher für einige Theologen nur vordergründig duo-theistisch, da zumindest gewisse Traditionen und Strömungen im Wicca auch monotheistische oder non-duale Aspekte aufweisen.
So werden die Göttin und der Gott oftmals auch als polare Aspekte eines allumfassenden, ungeschlechtlichen und monistischen Eins gesehen, das von Patricia Crowther als "Dryghten" bezeichnet wurde. Dieses altgermanische Wort findet sich als "Dryghtyn" als Bezeichnung für Gott in manchen alten englischen Bibeln. Es ist verwandt mit dem althochdeutschen "trôthin" und "trëuga", basierend auf dem Proto-Indoeuropäischen "*trw". Diese Vorstellung eines "All-Einen", die aber nicht von allen Wicca geteilt wird, ähnelt dem hinduistischen Konzept des Brahman oder dem buddhistischen Shunyata sowie dem taoistischen Tao oder dem alchemistischen Azoth. Da Wicca, wie Anhänger vieler anderen Formen des Neuheidentums auch, im Sinne eines philosophischen Panpsychismus daran glauben, dass alles in der Welt lebendig und beseelt ist und durch eine Weltseele verbunden ist, gibt es auch deutliche Bezüge zum Animismus ursprünglicher „Naturreligionen“.
Aus den fernöstlichen Religionen und der Theosophie übernommen werden oftmals auch die Vorstellungen von der Existenz mehrerer Schichten von Energiekörpern (Ätherleib, Astralleib etc.), die durch die sieben Chakras sowie die sogenannte Silberschnur mit dem physischen Körper wechselwirken sollen. Energiearbeit ist somit ein zentraler Bestandteil der magischen Rituale. Astralreisen werden ebenfalls als Mittel der Magie gesehen, wobei die Flugsalben früherer Hexen solche Erfahrungen durch Drogen unterstützt haben sollen.

Wichtiger als die Frage Mono- oder Polytheismus ist für viele Anhänger von Wicca und anderen Formen des Hexentums oder der Göttinnenspiritualität jedoch die Vorstellung einer dreifaltigen Göttin, die sich im Jahreskreis wandelt und der bestimmte Feste zugeordnet sind:


Alle drei Gestalten bilden eine Gottheit; sie sind nie völlig voneinander getrennt. Diese Vorstellung von der dreifaltigen, den Jahreskreis durchwandernden Göttin gehen in erster Linie auf Schriften des 19. Jahrhunderts, u. a. der „Cambridge Ritualists“ Jane Ellen Harrison, Gilbert Murray, F. M. Cornford und A. B. Cook zurück, wurden aber vor allem durch James Frazer in seinem Werk „Der Goldenen Zweig“ und Robert Graves „Die weiße Göttin“ popularisiert.

Der männliche Gegenpol, der gehörnte Gott, wird hingegen entweder in drei- oder aber in zweifacher Form verehrt, in den Gestalten des jugendlichen Lichtgottes oder göttlichen Kindes und des wilden Mannes oder Herrn der Tiere sowie als Herr der Unterwelt. Wie die Große Göttin, so durchwandert auch der Gehörnte Gott den Jahreskreis:


Wicca „arbeiten“ in ihren Ritualen häufig mit verschiedenen Gottheiten. Ebenso wie Hindus, aber im Gegensatz zu echten Polytheisten sehen Wicca diese Gottheiten aber nur als verschiedene Erscheinungsformen oder Facetten ihrer zwei großen Hauptgottheiten, der Göttin und des Gottes. Es gilt bei vielen der Grundsatz „Alle Göttinnen sind eine Göttin und alle Götter sind ein Gott“ (erstmals zu finden in Dion Fortunes Roman "Die Seepriesterin" 1938, offenbar als Entlehnung einer Passage aus "„Der goldene Esel“" des antiken Autors Apuleius). Individuen wählen als Identifikationspunkt ihre persönliche Gottheit aus diversen Gottheiten verschiedener Götterwelten, deren Geschichte sie als besonders inspirierend erachten und auf die sie sich zur persönlichen Verehrung am meisten beziehen wollen. Ähnlich werden Coven einige Gottheiten als Gruppenfokus wählen. Manchmal werden diese spezifischen Gottheiten auch geheim gehalten oder einem stärker pantheistischen Ansatz gefolgt und auf die Verehrung personaler Gottheiten ganz verzichtet. Wicca legen Wert auf Freiheit und sehen sich als gleichberechtigte Partner der Gottheiten an, die Demutsgesten nicht als angemessenes Mittel der Verehrung betrachten.

Häufig verehrte Gottheiten sind:

Im britischen Wicca ist oft auch noch der Name Herne für den gehörnten Gott gebräuchlich. Weniger gebräuchlich hingegen ist die Verehrung von:

Allen gemeinsam ist der Kern, dass eine ekstatische Vereinigung mit der Natur und die Kommunikation mit dem (personifizierten oder abstrakten) Göttlichen angestrebt wird. Hier scheinen wieder schamanische Ideen durch; siehe Mircea Eliade in seinem Buch "Schamanische Ekstasetechniken".

Im Gegensatz zu manchen großen Weltreligionen (z. B. Christentum, Islam und Hinduismus), die das eigentliche Heil eher im Jenseits bzw. nicht im Diesseits suchen und die materielle Welt zum Teil als unrein oder leidvoll betrachten, ist Wicca eine freudvolle, lustbetonte und dieseitsbejahende Religion, die den Körper nicht als ein zu überwindendes Übel ansieht und Körperlichkeit und Natur auch nicht als sündhaft, sondern als im höchsten Maße heilig erachtet.

Da sie alles in der Welt als kreisläufige Prozesse von Werden und Vergehen verstehen, gehen Wicca-Anhänger auch von einer Wiedergeburt der Seele aus. Im Gegensatz zu den fernöstlichen Wiedergeburtslehren betrachtet Wicca den Kreislauf der Reinkarnation (Samsara) jedoch nicht als etwas Negatives, dessen Folgen nur Leiden sind, die durch spirituelle Entwicklung (Moksha) überwunden werden sollten, sondern als natürlichen und ewigen Kreislauf, der heilig und auch erstrebenswert ist.

Auf Grund der Regel der dreifachen Wiederkehr glauben sie auch an ein Karma, aber nicht daran, dass jedes persönliche Unglück durch schlechtes Karma selbstverschuldet sei. Sie glauben auch nicht an Determinismus und Prädestination, sondern vertreten im Gegenteil die Überzeugung, dass die eigene Seele frei sei und man selbst die Verantwortung für sein Leben trage. Zwischen den Wiedergeburten soll sich die Seele für gewisse Zeit im „Sommerland“ genannten Jenseits ausruhen. Diese Jenseitsvorstellung beruht auf keltischen, hinduistischen (Devachan) und theosophischen Wurzeln, hat jedoch auch gewisse Anklänge an die christlichen und islamischen Paradiesvorstellungen.

Die Glaubensinhalte im Wicca besitzen teilweise Ähnlichkeiten mit Vorstellungen, wie sie im Neuplatonismus, der christlichen Mystik, der jüdischen Mystik (Kabbala) und der islamischen Mystik (Sufismus), im Hinduismus (Advaita Vedanta), Buddhismus (Tantra, Zen) und Taoismus sowie in der Theosophie und Anthroposophie zu finden sind. Es handelt sich bei Wicca jedoch nicht um eine Weiterentwicklung dieser älteren Konzepte anderer Glaubensrichtungen, sondern nur um oberflächliche Entlehnungen oder zufällige Übereinstimmungen, auch wenn zuweilen solche Ähnlichkeiten irrtümlich als Eigenschaften einer universalen „Ewige Philosophie“ (Philosophia perennis) betrachtet wurden. Vereinzelt wurde auch ein Zusammenhang zwischen Wicca und der untergegangenen Religion der Sabier aus Harren hergestellt, was jedoch historisch nicht belegbar ist. Zu den spirituellen Vorlagen für Entlehnungen im modernen Wicca-Glauben zählen nach Ansicht der Religionshistoriker (z. B. Hutton, 2001) insbesondere das Rosenkreuzertum und die Freimaurerei sowie die Hermetik, Alchemie, Zeremonialmagie und Ritualmagie (z. B. das mittelalterliche Grimoire "Clavicula Salomonis", dem viele von Gardners Ritualen entlehnt sind). Diese religionsgeschichtlichen Erkenntnisse stoßen jedoch noch immer bei einigen Wicca auf Widerspruch, die Gardner eher für einen Autor halten, der nur „Offensichtliches“ einer weit älteren Religion wieder hervorgebracht habe – so dass nach ihrer Meinung der Einfluss genau umgekehrt gewesen sein soll: Die „Alte Religion“ habe die Freimaurer und Rosenkreuzer beeinflusst, und die Entwicklung in unserer modernen Zeit sei nur ein "Flashback". Für derartige Ansichten gibt es aus wissenschaftlicher Sicht aber keinerlei Hinweise. Für die Entwicklung von Wicca war insbesondere der Hermetic Order of the Golden Dawn von großer Bedeutung. In jüngerer Zeit sind auch neo-schamanistische Einflüsse unverkennbar sowie eine stärkere Orientierung am keltischen und germanischen Heidentum.

Wicca versteht sich als eine tolerante Religion und erhebt keinen Anspruch auf Alleingültigkeit. Dogmatismus, Fanatismus und Diskriminierungen anderer Religionen werden abgelehnt. Wicca sieht sich als eine zeitgemäße Alternative zu anderen Formen der Spiritualität. Entsprechend dem Verständnis von eigener Verantwortung dient Magie im Wicca-Kult nur dazu, natürliche Energien zu lenken und notwendige Veränderungen zum Positiven anzustoßen. Gewaltfreiheit und Naturverbundenheit haben einen hohen Stellenwert.

Die ethischen Grundsätze im Wicca basieren auf der Weisung (Rede) „Solange es niemandem schadet, tu was du willst“ ("An ye harm none do as ye will") und der Regel der (dreifachen) Wiederkehr „Alles, was von dir ausgeht, fällt dreifach auf dich zurück“.

Der Begriff "Rede" ist vom altenglischen Wort "roedan" abgeleitet, das "führen" oder "anleiten" bedeutet. Der unmittelbare Ursprung der Wiccan-Rede ist Gerald Gardners Buch "The Meaning of Witchcraft" (1959), in dem er den Ausspruch des Königs Pausole "„Do what you like so long as you harm no one“" erwähnt, der aus dem Roman "The Adventures of King Pausole" (1901) von Pierre Louÿs stammt. Es wird aber allgemein angenommen, dass die Rede auch durch Aleister Crowleys "Gesetz von Thelema" inspiriert wurde, das lautet: „Tu, was du willst, soll sein das ganze Gesetz. Liebe ist das Gesetz, Liebe unter Willen“. Crowley könnte dabei seinerseits durch die Augustinus-Worte „Liebe und tue, was du willst“ beeinflusst gewesen sein oder durch die Worte "„Do as thou wilt …“" des Autors François Rabelais aus dem Roman "Gargantua" (1534). Ein bislang weitgehend übersehener, möglicher Ursprung des Wortlautes der Wiccan-Rede könnte die französische Verfassung von 1791 sein, in der definiert wird: „Die Freiheit besteht darin, alles tun zu können, was einem anderen nicht schadet“. Die heutige acht-Worte-Fassung der Rede stammt von einer Ansprache Doreen Valientes am 3. Oktober 1964 und wurde noch im selben Jahr von Gerard Noel veröffentlicht.

Die „Rede“ wird oft auch von Wicca-Anhängern dahingehend interpretiert, dass sie alle Handlungen verbiete, die irgendwie Schaden irgendwelcher Art bewirken könnten. Es gibt aber auch die Ansicht, dass die Freiheit zu tun, was man will, nur unter der Voraussetzung gilt, dass man dabei niemandem schadet. Handlungen, die schaden, sind also nicht in gleicher Weise frei, sondern müssen gegebenenfalls hinreichend gerechtfertigt und in ihrem Ausmaß angemessen sein. Andere Interpretationen sehen in der Rede eine Forderung, seinen wahren Willen zu erkunden und diesen auszuüben unter der Einschränkung, dabei keinem (anderen oder sich selbst) zu schaden.

Die Regel der dreifachen Wiederkehr stützt sich auf das Prinzip von Ursache und Wirkung und den Gedanken, dass alles, was man tut, auch Konsequenzen hat, wobei kleine Ursachen auch große Wirkungen haben können. Diese Regel ist aber nur entfernt verwandt mit dem östlichen Konzept des Karmas. Die Kombination von „Rede“ und der Regel der Wiederkehr erfüllt in der Wicca-Ethik weitgehend die Funktion der „Goldenen Regel“.

Die Aussage der „Rede“ ist einer der Faktoren, weshalb christliche und andere Gruppen kritisieren, dass Wicca eine hedonistische Religion für die Konsum- und Spaßgesellschaft sei. Ihre Anhänger hingegen sehen Wicca als eine Religion, deren Ethik sehr stark auf dem Gedanken der Eigenverantwortung basiert. Jeder Mensch muss selbst abwägen und überlegen, um wirklich moralisch handeln zu können, und muss für sein Handeln auch die Konsequenzen tragen (Regel der dreifachen Wiederkehr). Diese Betonung von eigener Verantwortung unterscheidet die Ethik neopaganer Religionen im Allgemeinen grundsätzlich von der auf Gottesfurcht und Gehorsam basierenden Ethik der großen monotheistischen Religionen, aber hebt sich auch deutlich ab von denjenigen ethischen Maßstäben, die den Willen oder das Wohlergehen der Mehrheit in den Mittelpunkt stellen, wie z. B. Rechtspositivismus und Utilitarismus. Die Wiccan-Rede korrespondiert mit ihrer Betonung von Freiheit, Selbstverantwortung und Schädigungsverbot weitgehend mit der naturrechtlichen Ethik der menschlichen Freiheit und dem Grundprinzip des Aggressionsverzichts im Individualanarchismus und im modernen Libertarismus. Im Gegensatz zu diesen individualistisch-anarchistischen Strömungen betont Wicca jedoch nicht nur die individuelle Freiheit, sondern auch die soziale Eingebundenheit jedes Einzelnen in die gesellschaftliche Gemeinschaft.

Der Jahreskreis, auch Jahresrad genannt, bezeichnet das in der Wicca-Religion gebräuchliche System von acht jahreszeitlichen Festtagen, das den Zyklus von Werden und Vergehen in der Natur symbolisiert. Es hat große Ähnlichkeiten mit dem keltischen Jahreskreis. Das System von Wicca ist eine moderne Verbindung der vier keltischen Hochfeste mit heidnischen Sonnenwendfesten in bronzezeitlichen Kulturen. Die Namen für zwei der Festtage (Litha und Mabon) sowie viele der Rituale sind moderne Neuschöpfungen.

Die acht Haupt-Feiertage, genannt Sabbats, richten sich nach dem Jahreslauf (die teils nach festen Daten, teils nach natürlichen oder astrologischen Ereignissen berechnet werden).

Vier Feste gelten als die „höheren“ Festtage und werden daher auch als Große Sabbate bezeichnet (andere Bezeichnungen hierfür sind teilweise Licht- bzw. Feuer-Feste oder Mondfeste). Diese liegen jeweils genau in der Mitte zwischen zwei solaren Festen, und werden deshalb auch als Kreuz-Viertel-Tage bezeichnet. Für die Bestimmung der genauen Daten der Drei-Viertel-Tage gibt es je nach Tradition unterschiedliche Vorgehensweisen. Traditionellerweise werden die mit den Kalenden übereinstimmenden Daten verwendet, in anderen Traditionen werden diese auch kalendarisch genau zwischen den solaren Festen gefeiert:
Die vier solaren Feste, die nach astronomischen Konstellationen des Sonnenstandes bestimmt werden, werden auch als Kleine Sabbate bezeichnet (eine andere Bezeichnung hierfür ist teilweise Sonnenfeste) und sind:

Neben diesen acht Sabbat-Festen gibt es die 13 sogenannten Esbats, die zu Ehren der Göttin bei Vollmond (manchmal auch Schwarzmond) veranstaltet werden. Bei diesen handelt es sich um magische Arbeitstage.

Die traditionellen Ritual-Werkzeuge innerhalb der Wicca-Bewegung sind:

In einigen Traditionen finden zum Teil folgende Utensilien Verwendung:

Athame und Zauberstab werden mit der rechten Hand (bei Linkshändern der linken Hand) gehalten. Diese Hand, die auch „Schutzhand“ genannt wird, steht symbolisch für den Punkt, an dem die persönliche Kraft aus dem Körper strömt. Die linke Hand (bei Linkshändern die rechte Hand) dagegen heißt „rezeptive Hand“, weil durch sie Energie in unseren Körper strömt. In manchen Ritualen symbolisiert der Kelch das weibliche Prinzip (den Schoß) und die Athame das männliche Prinzip (den Phallus), ganz im Sinne von Riane Eislers „Kelch und Schwert“, bei einigen Traditionen/Pfaden ist die Bedeutung von Athame und Stab (und die jeweilige Zuordnung zur Himmelsrichtung) vertauscht.

Im traditionellen Wicca werden Rituale oftmals nackt (im „Himmelskleid“, engl. "skyclad") ausgeführt, da Kleidung die magischen Energien und die Verbindung mit der Erde behindern soll. Häufige Rituale sind:





Aufgrund des panentheistischen Charakters im Wicca wird beim Ernten von Blumen, Kräutern usw. versucht, zunächst durch Visualisierung eine Verbindung mit den entsprechenden Pflanzen herzustellen. Erst dann wird das Benötigte geschnitten, häufig mit der Bolline (einem weißen Messer). Beispielsweise Einsatzmöglichkeiten für Kräuteranwendung sind hierbei:


Zur Verwendung an Wicca-Festen haben sich besondere Sabbatkräuter eingebürgert:


Die klassische Elementelehre ist ein wesentlicher Bestandteil der Weltsicht von Wicca. Jede manifeste Form wird als Ausprägung der vier archetypischen Elemente "Erde", "Wasser", "Luft" und "Feuer" verstanden, die unterschiedlich interpretiert werden (manchmal materialistisch als Aggregatzustände, meist aber esoterisch als subtile Energien). Hinzu kommt als fünftes Element der "Äther", der von Wicca als Geist interpretiert wird, im Gegensatz zum klassischen fünften Element wie Quintessenz oder Akasha. Die fünf Spitzen des Pentagramms symbolisieren im Wicca diese fünf Elemente. Bei der Beschwörung des magischen Kreises werden an den Kardinalpunkten neben den vier Himmelsrichtungen und Kreisvierteln oft auch die vier Elemente angerufen, die in Form von vier Elementeherrschern ("Elementekönige" oder "Wachtürme") personalisiert werden, die über die entsprechenden Elementargeister gebieten sollen. Beeinflusst durch seine Beziehungen zum Hermetic Order of the Golden Dawn, wurde das Konzept der Elementarmagie und der "Wachtürme" von Gerald Gardner aus dem System der henochischen Magie von John Dee und Edward Kelley entlehnt. Die Wicca-Vorstellungen über die Elementargeister basieren weitgehend auf den Werken von Paracelsus sowie der Theosophie bzw. der daraus hervorgegangenen Anthroposophie Rudolf Steiners.

→ "Hauptartikel: Wicca-Traditionen"

British Traditional Wicca oder Traditionelles Wicca:

Einige häufig in den USA verwendete Begriffe zur Selbstbezeichnung derjenigen Wicca, die sich auf eine bis auf Gerald Gardner zurückgehende Initiationslinie berufen können und sich eng an dessen Lehren orientieren:

Wicca Traditionslinien:

Unter Traditionen oder Linien versteht man in Coven organisierte und darin initiierte Wicca, die sich noch relativ eng am British Traditional Wicca orientieren, wie beispielsweise:

Unter Eclectic Wicca versteht man Wicca-Anhänger, die keiner speziellen Tradition angehören, sich aber bei verschiedenen Traditionen und Kulturen bedienen. Eklektiker können sowohl in einem Coven initiiert werden als auch eine Selbstinitiation gegenüber dem Gott und der Göttin durchführen.

Solitary-Wicca („Freifliegende Hexen“) ist ein besonders „freier“ Wicca-Stil, der die persönliche Freiheit betont und eine hierarchische Struktur vermeidet. Es umfasst alle Wicca, die nicht in Coven organisiert sind und eigenständig lernen.

Neo-Wicca ist keine Tradition im eigentlichen Sinne, sondern der Sammelbegriff für moderne und offenere Interpretationen von Wicca – mit geringerer Betonung von Themen wie Sexualität und Tod, und meist auf der Grundlage von Selbstinitiation.

Typische Vertreter von Neo-Wicca sind:

Ernsthafte Vertreter der Wicca-Religion und anderer neopaganer Kulte verwenden im englischen Sprachraum gerne den Begriff "Fluffy Bunnies" als zumeist abwertende Bezeichnung für Neulinge, meistens Mädchen im Teenager-Alter, die durch Fernseh-Serien wie "Buffy" oder "Charmed" zu Wicca gekommen sind und sich nur oberflächlich aus wenigen Büchern (z. B. von Silver RavenWolf) informiert haben. Kennzeichnend für Fluffy Bunnies ist u. a., dass sie eine relativ einfältige Auffassung von Magie und Wicca-Ethik haben sowie immer wieder längst widerlegte Geschichtsmythen unkritisch zitieren (z. B. Wicca sei eine uralte Göttinnenreligion aus dem vorgeschichtlichen Matriarchat oder angeblich seien neun Millionen Hexen während der „Burning Times“ verbrannt worden oder in Salem seien Hexen verbrannt worden usw.).

Die ersten englischsprachigen, wissenschaftlichen Abhandlungen zur geschichtlichen Entwicklung und zum Glaubenssystem von Wicca waren die Bücher der amerikanischen Journalistin Margot Adler (1987) und der amerikanischen Anthropologin Tanya Luhrmann (1989). Erste englischsprachige Dissertationen zu Wicca stammen von Gini Graham Scott (1976), Joan Ludecke (1989) und der Sozialanthropologin Susan Greenwood (2000).

Der Wicca-Autor Aidan Kelly vertrat 1991 in einem Buch die These, dass Wicca eine Kreation von Gerald Gardner sei und die von diesem behaupteten älteren Traditionen (z. B. hinsichtlich der Person von Dorothy Clutterbuck und des New Forest Coven) nicht authentisch seien. Diese Arbeit wurde jedoch auch aus wissenschaftlicher Sicht kritisiert. Kelly (2008) bekräftigte seine Ansichten und belegte sie ausführlicher in einem neuen Buch.

Der Historiker Ronald Hutton (1999) lieferte die erste unabhängige und ausführliche wissenschaftliche Abhandlung zur Entstehungsgeschichte der modernen Hexenreligion. Auch Hutton kam zu dem Ergebnis, dass Wicca weitgehend eine Neuschöpfung Gerald Gardners sei, aufbauend auf bestehenden magischen Traditionen und Organisationen (Isis Mysterienkult, Theosophische Gesellschaft, Rosicrucian Society, Rosicrucian Order Crotona Fellowship, Hermetic Order of the Golden Dawn, Ordo Templi Orientis OTO, Thelema, Ancient Druid Order, Orden der Barden, Ovaten und Druiden OBOD etc.), Riten der Freimaurerei (insbesondere die, auch Frauen zulassende, Co-Masonry), der romantischen Woodcraft-Bewegung (insbesondere des "Order of Woodcraft Chivalry") und sogar des Nudismus sowie den Büchern von Michelet, Leland, Murray, Frazer und Graves.

Ein neueres Buch von Philip Heselton (2003), zu dem Ronald Hutton das Vorwort verfasste, kommt auf Grund weiterer Nachforschungen hinsichtlich Dorothy Clutterbuck und einigen weiteren Punkten zu anderen Schlussfolgerungen. Eine persönliche „Innenansicht“ der Geschichte von Wicca wurde von Frederic Lamond (2005) in seinem Buch "Fifty Years of Wicca" veröffentlicht, kann aber nicht als wissenschaftlich neutrale Studie gelten und beansprucht dies auch nicht. Die Geschichte von Wicca in Amerika wurde auch von dem Autor Chas Clifton (2006) beleuchtet. Im Jahre 2008 veröffentlichte die Ethnografin Sabina Magliocco ihre Studie über die neuheidnische „Hexen-Kultur“ in Amerika.

Im deutschen Sprachraum gibt es bislang auch nur wenige wissenschaftliche Arbeiten zur Entwicklung und gesellschaftlichen Bedeutung von Wicca:







Die religionswissenschaftliche Forschung geht davon aus, dass Wicca durch eine Synthese und Neukombination zahlreicher Elemente zustande gekommen ist. Im 19. und frühen 20. Jahrhundert bestanden günstige Voraussetzungen für eine Wiederbelebung der alten „Naturreligionen“. Durch die Romantik am Beginn des 19. Jahrhunderts wurde ein großes Interesse an vorchristlichen Kulturen Europas geweckt. Auch später noch gab es einflussreiche kulturelle Strömungen, die die Sehnsucht nach der Verbindung von Mensch, Kultur und Natur ausdrückten (z. B. Woodcraft-Bewegung). Für viele Menschen, die den Glauben an das Christentum verloren hatten, stellte die Natur die Verbindung zwischen dem Menschen und alten heidnischen Religionen dar. Hinzu kommt, dass im Jahr 1951 in Großbritannien der Witchcraft Act aufgehoben wurde.

Die Organisation in Coven, die drei Initiationsgrade und auch einige Symbole stammen aus der Welt der Freimaurerei. Mitglieder der Freimaurerlogen wurden zur Verschwiegenheit verpflichtet, wie heute noch einige Wicca in Bezug auf Einzelheiten ihrer Rituale. Aus der Freimaurerei entwickelte sich schließlich die Gesellschaft der Rosenkreuzer und der Hermetic Order of the Golden Dawn, dessen Hauptzweck das Studium und die Arbeit mit Ritualmagie war. Namentlich Gardener hatte nach 1938 Kontakt mit Mitgliedern der Rosenkreuzer. Allerdings handelt es sich bei Anleihen aus der Freimaurerei um die Adaption einer äußeren Struktur, die mit anderen Inhalten gefüllt wurde.

Andere Elemente des Wicca stammen tatsächlich aus vorchristlicher Zeit, so die Verehrung von Göttinnen, teilweise auch die einer dreifachen Göttin und viele der Feste des Jahreskreises. So wurden Imbolc, Beltaine, Lugnasadh und Samhain bei keltischen Stämmen der britischen Inseln tatsächlich gefeiert. Sonnenwendfeiern sind bei den Germanen überliefert. Allerdings ist über Einzelheiten dieser Feste nur wenig bekannt. Die Vorstellung vom Sommerland und der Anderswelt stammen ebenfalls von den Kelten. Die Aussagen verschiedener Quellen zum Thema Wiedergeburtsglauben bei den Kelten sind widersprüchlich, aber die aktuelle wissenschaftliche Auffassung ist, dass es bei den Kelten zwar den Glauben an Gestaltwandler, Wiedergänger und Wiederbelebung von Toten gab, es aber keine gesicherten Belege für den Glauben an Reinkarnation oder Seelenwanderung gibt.

Ein für Wicca einflussreiches Werk aus der klassischen Antike ist der Roman "Der goldene Esel" (ca. 150 n. Chr.) des römischen Schriftstellers Apuleius, der die Einweihung in die Isis-Mysterien beschreibt. Die erste Erwähnung nachtfahrender Frauen findet sich im Canon episcopi im Jahr 906. Die Vermutung, dass es sich dabei um einen überlebenden heidnischen Kult oder eine Religion handelt, ist strittig.

Auch einige moderne Werke übten starken Einfluss auf Wicca aus. Das erste Buch, in dem gesichert die Hexenreligion als überlebende alte heidnische Tradition dargestellt wurde, war offenbar "La Sorcière" (1862) von Jules Michelet. Im Jahre 1899 veröffentlichte Charles Godfrey Leland ein Buch, in dem er die magischen Traditionen italienischer Hexen niederschrieb. Im Jahre 1921 veröffentlichte die britische Anthropologin Margaret A. Murray das Buch "The Witch-cult in Western Europe" über die angeblich lange Traditionslinie der „weisen Frauen“ in Europa und bezog sich dabei auf die Hexen als eigenen religiösen/magischen Kult, die in ständiger Konfrontation mit dem Christentum (als Antagonist) lebten und sich gegen den Niedergang ihrer Tradition behaupten mussten. Ein wichtiger Einfluss ging auch von James Frazers religionsgeschichtlichem Buch "The Golden Bough" („Der goldene Zweig“) (1890) aus sowie von Robert Graves und seinem Buch "The White Goddess" („Die weisse Göttin“) (1948). Beide Autoren glaubten aus den überlieferten Mythen und Märchen eine vorgeschichtliche, heidnische Religion rekonstruieren zu können. Auch das Buch "Der Heros in tausend Gestalten" (1949) des Mythenforschers Joseph Campbell übte nicht unerheblichen Einfluss auf den Wicca-Kult aus, insbesondere auf die Vorstellung des geopferten Jahresgottes. Die in den erwähnten Werken von Michelet, Leland, Murray, Frazer und Graves vertretenen Thesen gelten aus heutiger ethnologischer Sicht jedoch teilweise als widerlegt oder als wenig plausibel. Dies gilt auch für die angebliche Wiederentdeckung alter walisischer Druidentraditionen durch den britischen Autor Edward Williams (1747–1826) (bekannt als Iolo Morganwg), aus dessen "Barddas" eine ganze Reihe von Ritualen, Mythen und Begriffen im heutigen Wicca entlehnt wurden.

Seit den 1960er Jahren dokumentierte der italienische Historiker Carlo Ginzburg Belege für die These, dass die Hexensabbate zumindest teilweise auf ein Substrat schamanischer Fruchtbarkeits- und Totenkulte in Verbindung mit der Einnahme psychotroper Pflanzen zurückzuführen sind und somit die pauschale Kritik an Murray etwas relativiert werden muss. Ginzburgs Thesen eines überlebenden, vorchristlichen, heidnischen Substrats wurden seitdem durch verschiedene weitere seriöse historische Untersuchungen bestätigt und unterstützt.

Seit den 1970er Jahren ging ein wichtiger Einfluss auf Wicca auch von der feministisch geprägten Göttinnenspiritualität aus, insbesondere von dem Buch "Der Hexenkult als Ur-Religion der Großen Göttin" (1983, engl. 1979) von Starhawk und "Herrin der Dunkelheit, Königin des Lichts" (1987, engl. 1975) von Zsuzsanna Budapest. Die moderne Verehrung der Großen Göttin stützt sich u. a. auch auf Werke der Archäologie und Matriarchatsforschung, wie "Die Sprache der Göttin" (1995, engl. 1989) und "Die Zivilisation der Göttin" (1996, engl. 1991) von Marija Gimbutas oder im deutschsprachigen Raum das Buch "Die Göttin und ihr Heros" (1980) von Heide Göttner-Abendroth. Schließlich beeinflussten auch einige Erzählungen und Romane, wie "Puck of Pook's Hill" (1906) von Rudyard Kipling, "Die Seepriesterin" (1938) von Dion Fortune oder die "Die Nebel von Avalon" (1982) von Marion Zimmer Bradley, die Entwicklung und Popularität des Neuen Heidentums, der Göttinnenspiritualität und von Wicca.

Die Kulturwissenschaftlerin Katrin Fischer führte insgesamt zwölf Interviews mit Wicca-Anhängern aus dem deutschsprachigen Raum durch. Sie stellt dar, wie sich ihre Lebenswelt, der damit zusammenhängende Alltag, Fremd- und Eigenbilder zu einer gruppenspezifischen sozialen Wirklichkeit formen. Anhand dieser Kriterien fand Fischer drei Typen von Wicca, die sich in bestimmten Merkmalen unterscheiden.

Priester und Priesterin

Diese Personen gehören dem British traditional Wicca an. Der Weg zu Wicca führte sie über eine religiöse Suche, da ihnen das Christentum keine Antworten auf für sie wichtige Lebensfragen mehr geben konnte. Für sie steht ihr Priestertum im Mittelpunkt des Interesses. Wicca stellt ihre Religion dar, in der sie ihre Überzeugungen und ihren Glauben leben. Das bedeutet für sie, dass sie sich auf einem lebenslangen spirituellen Weg befinden, der sie zu ihrem Selbst führen soll. Wicca ist ihr Lebensinhalt. Personen dieses Typs sehen keinen Anlass, ihr Glaubenssystem öffentlich zu vertreten, und ziehen sich meistens zurück. Sie sind in Coven integriert und legen großen Wert auf klare Strukturen, auch in Bezug auf Glaubensinhalte. Sie sind diejenige Gruppe, die am stärksten auf Geheimhaltung ihrer Kulthandlungen bestehen. Feminismus und die historische Hexenverfolgung haben in ihrem Denken keinen besonderen Stellenwert. Magie soll Wicca dieses Typs nicht berechtigen, Macht auszuüben, und sie wünschen dies auch nicht.

Feministische Wicca

Hierzu gehören Wicca, die sich besonders der Göttin zuwenden. Sie gehören meistens unkonventionellen Wicca-Traditionen wie dem Dianic Wicca an. Personen dieses Typs sind entweder durch die Frauenbewegung zu Wicca gekommen oder haben sich durch dieses politisiert. Die Verbindungen der Mitglieder im Coven sind sehr eng und persönlich. Im Unterschied zum ersten Typus hat für sie der Wiedergeburtsglaube einen hohen Stellenwert. Die historische Hexenverfolgung wird als ein Vernichtungsfeldzug gegen Frauen interpretiert, der sich gegenwärtig aber nicht wiederhole. Personen dieses Typs stehen häufig in der Öffentlichkeit, üben Wahlämter aus oder sind in der Frauen-, Ökologie- und neuerdings der globalisierungskritischen Bewegung engagiert. Sie sehen keinen Grund, ihre Überzeugungen zu verschweigen, was allerdings häufiger zu Konflikten mit konservativen Teilen der Gesellschaft führt.

Freifliegende Hexen

Wichtig für diese Gruppe ist der Bezug zur Natur und der animistischen Welt. Sie zeichnen sich durch eine starke Verbindung zu den heimatlichen Wurzeln und durch Bezugnahme auf Flora und Fauna aus. Das Erbhexentum ist für die Angehörigen dieses Typus ausschlaggebend für das Gefühl, Hexe zu sein. Germanische oder keltische Wurzeln dienen als Verbindungselement zwischen der Religion und ihren Fähigkeiten als Hexe. Für sie ist die historische Hexenverfolgung ein Vernichtungsfeldzug gegen das alte Wissen der weisen Frauen. Dieser könne sich auch heute noch wiederholen. Sie fühlen sich davon persönlich betroffen. Eine interviewte Wicca legt z. B. regelmäßig Blumen auf das Denkmal für die letzte in ihrer Stadt verbrannte Hexe. Sie halten ihren Glauben teilweise geheim, teilweise provozieren sie ihre konservative Umwelt aber auch bewusst. Feminismus spielt für sie keine Rolle. Hexen dieses Typs glauben, durch die Magie über Macht zu verfügen. Diese setzen sie auch gegen Fremde als Verteidigung ein.





</doc>
<doc id="5513" url="https://de.wikipedia.org/wiki?curid=5513" title="Woody Harrelson">
Woody Harrelson

Woody Harrelson (* 23. Juli 1961 als "Woodrow Tracy Harrelson" in Midland, Texas) ist ein US-amerikanischer Schauspieler und Regisseur.

Er wuchs mit seiner Mutter Diane Lou (geb. Oswald) in Lebanon, Ohio, auf. Sein Vater Charles Voyde Harrelson verließ Frau und Kinder 1968.

Harrelson studierte Englisch und Schauspiel am "Hanover College" in Indiana und ist Mitglied der Studentenverbindung Sigma Chi.

Seine erste Hauptrolle war die des "Billy Hoyle" in der Filmkomödie "Weiße Jungs bringen’s nicht". Bekannt wurde Woody Harrelson durch seine Rolle des "Woody" in der Fernsehserie "Cheers". Zeitweilig spielte er den Lebensgefährten der weiblichen Titelfigur in der US-Sitcom "Will & Grace". Auf der Kinoleinwand wurde er vor allem durch den Film "Natural Born Killers" von Oliver Stone bekannt, in dem er mit Juliette Lewis ein Serienmörder-Duo darstellte. Für die Titelrolle in "Larry Flynt – Die nackte Wahrheit" von Miloš Forman erhielt er 1997 eine Oscarnominierung als bester Hauptdarsteller. Eine weitere Nominierung als bester Nebendarsteller erhielt er 2010 für seine Darstellung eines traumatisierten US-Soldaten in dem Drama "The Messenger – Die letzte Nachricht".
In seinem 2011 veröffentlichen Album singt Harrelson in dem Song "Wild and Free" mit Ziggy Marley (Ziggy Marley ft. Woody Harrelson).

Er wird seit "After the Sunset" (2004) von dem deutschen Schauspieler Thomas Nero Wolff gesprochen.

1985 heiratete Harrelson in Tijuana Nancy Simon. Zehn Monate später ließen sie sich scheiden. Am 28. Dezember 2008 heiratete er seine ehemalige Assistentin Laura Louie Harrelson, mit der er seit 1987 zusammenlebt. Am 28. Februar 1993 wurde die erste Tochter des Paares geboren, die zweite Tochter kam am 22. September 1996 zur Welt; am 3. Juni 2006 wurden sie erneut Eltern einer Tochter. Harrelson setzt sich unter anderem für Tierrechte ein und ist Veganer. 2010 unterstützte er gemeinsam mit Jennifer Aniston, Courteney Cox, Ben Stiller und anderen Hollywood-Stars die Kampagne "The Cove PSA – My Friend Is …" zum Schutz von Delfinen.

Woody Harrelson bezeichnet sich selbst als Anhänger des Anarchismus. Er ist Unterstützer des 9/11 Truth Movement und befürwortet eine Wiederaufnahme der Untersuchungen über die Terroranschläge am 11. September 2001. Er befürwortet außerdem die Cannabis-Legalisierung und beantragte Anfang 2016 zwei Lizenzen für Marihuana-Dispensaries auf Hawaii.

Oscar

Golden Globe Award

British Academy Film Award

Screen Actors Guild Award

Emmy

Goldene Himbeere



</doc>
<doc id="5542" url="https://de.wikipedia.org/wiki?curid=5542" title="Warren Beatty">
Warren Beatty

Henry Warren Beatty [] (* 30. März 1937 in Richmond, Virginia) ist ein US-amerikanischer Schauspieler, Regisseur, Drehbuchautor, Produzent und politischer Aktivist der Demokratischen Partei. Seine berühmteste Rolle spielte er als Gangster Clyde Barrow in dem Filmklassiker "Bonnie und Clyde".

Als Kinder einer Schauspiellehrerin standen Warren Beatty und seine ältere Schwester Shirley MacLaine schon in jungen Jahren häufig auf der Bühne. Nach der High School und einem Studium an der Northwestern University wirkte Beatty in zahlreichen Theaterproduktionen mit. Ende der 1950er Jahre wurde er von dem Regisseur und Produzenten Joshua Logan entdeckt, der ihm Engagements am New Yorker Broadway verschaffte.

1959 war Beatty für ein Jahr in der beliebten Fernsehserie "The Many Loves of Dobie Gillis" zu sehen, bevor ihm zwei Jahre später mit seinem Leinwanddebüt in Elia Kazans "Fieber im Blut" der Durchbruch gelang. Während der Dreharbeiten lernte er Natalie Wood kennen, die zu der Zeit noch mit Robert Wagner verheiratet war. Mit ihr war er die nächsten acht Jahre zusammen.

Nach weiteren Filmauftritten und einem Golden Globe 1962 als Bester Nachwuchsdarsteller, produzierte Beatty 1967 das Gangsterdrama "Bonnie und Clyde", in dem er auch die männliche Hauptrolle des Clyde Barrow übernahm. Bei einem Budget von nur 2,5 Millionen Dollar spielte der Film weltweit ca. 70 Millionen Dollar ein und war für zehn Oscars nominiert, von denen er zwei gewann. Der Film etablierte Beatty für die nächsten Jahrzehnte als eine der führenden Hollywood-Persönlichkeiten. Er war als Schauspieler, Regisseur, Produzent und Drehbuchautor tätig.

Trotz des Erfolgs von "Bonnie und Clyde" war Beatty in den folgenden Jahren nur relativ selten als Schauspieler zu sehen. Während der 1970er Jahre wirkte er in sieben, in den 1980er Jahren sogar nur in zwei Filmen mit. In den 1990er Jahren drehte er vier Filme. Zu seinen Projekten zählten Kassenschlager wie "Shampoo" oder sein Regiedebüt "Der Himmel soll warten". An beiden Filmen war er auch als Co-Autor und Produzent beteiligt. 1971 spielte er zusammen mit Gert Fröbe in "Der Millionenraub".

1975 lernte er am Set von "Mitgiftjäger" "(The Fortune)" Jack Nicholson kennen, mit dem er seitdem eng befreundet ist. Für den Film "Reds" (Drehbuch, Produktion, Regie, Hauptrolle), einem Biopic über den Journalisten John Reed, gewann Beatty 1982 nach zehn Nominierungen in fünf verschiedenen Kategorien seinen ersten Oscar als bester Regisseur.
Nach "Reds" drehte er in den 1980er Jahren als Schauspieler lediglich den Film "Ishtar" an der Seite von Dustin Hoffman, der jedoch überwiegend negative Kritiken erhielt. Erst Anfang der 1990er Jahre konnte er mit der Comic-Adaption "Dick Tracy" und dem Gangsterfilm "Bugsy" an frühere Erfolge anknüpfen.

1992 heiratete der frühere Frauenheld seine Schauspielkollegin Annette Bening, mit der er vier Kinder hat. Zuvor war er u. a. mit Natalie Wood, Julie Christie, Diane Keaton, Joan Collins, Isabelle Adjani und Madonna (zumeist ehemalige Filmpartnerinnen) liiert.

Zuletzt war Beatty in den Komödien "Bulworth" (1998) und "Stadt, Land, Kuss" (2001) im Kino zu sehen. Bei der Oscarverleihung 2000 überreichte ihm Jack Nicholson den "Irving G. Thalberg Memorial Award" für sein Lebenswerk, 2007 erhielt Beatty bei der Golden-Globe-Verleihung den Ehrenpreis. Erst 2016 trat er erneut in "Regeln spielen keine Rolle" in Erscheinung, den er auch inszenierte.

Obwohl Beatty in rund 40 Jahren nur 23 Filme gedreht hat, bleiben seine Erfolge und sein Allround-Talent erwähnenswert. Die ersten vier Filme, die unter seiner Regie entstanden sind, erhielten insgesamt 29 Oscarnominierungen, sieben Oscars und weitere Preise. Hinzu kommen 24 weitere Oscarnominierungen und sechs Auszeichnungen für drei Filme, die er produziert oder für die er das Drehbuch geschrieben hat.

Warren Beatty ist aktives Mitglied der Demokratischen Partei in Kalifornien und bekannt für sein links-progressives Engagement. Er gestand einmal, dass er im Jahr 2000 mit dem Gedanken spielte, für das Amt des US-Präsidenten zu kandidieren.






</doc>
<doc id="5543" url="https://de.wikipedia.org/wiki?curid=5543" title="Western">
Western

Der Western (, ) ist ein Kino-Genre, in dessen Mittelpunkt der zentrale US-amerikanische Mythos der Eroberung des (wilden) Westens der Vereinigten Staaten im neunzehnten Jahrhundert steht. Entsprechende Werke der Literatur werden meist als Trivialliteratur gewertet. Wesentliche Merkmale sind Handlungsort und Zeit: der westliche Teil des nordamerikanischen Kontinents während seiner Besiedlung durch die von Osten kommenden Siedler. Mit einem Einspielergebnis von rund 424 Mio. US-Dollar ist "Der mit dem Wolf tanzt" der erfolgreichste Western an den Kinokassen.

Der klassische Western ist in seinen handelnden Figuren, erzählenden Elementen, Orten und Stilmitteln stark festgelegt. Im Mittelpunkt stehen meist der gute, zuweilen naiv wirkende, aber wehrhafte Cowboy oder Sheriff und sein Konterpart, der skrupellose Bösewicht. Zwischen den Hauptakteuren steht häufig eine Frau, um die ein Kampf zumeist mit Revolvern oder Fäusten ausgetragen wird. Das Fort oder die kleine Stadt, der Saloon mit Whiskey und Kartenspiel, Pferde, Wagen, die weite Landschaft, die in gewaltigen Totalen eingefangen wird, und das Indianerdorf sind typische Orte der Handlung. Wichtige Elemente sind häufig auch ein Bankraub oder ein Postkutschenraub. Aufgelöst wird der Konflikt am Ende durch einen "Shootout" oder Showdown auf der Hauptstraße. Reizvolle, weil ungewöhnliche Kontraste zum verbreiteten Einerlei des Genres stellen die intelligent-bescheidenen Auftritte von Schauspielern wie Glenn Ford, James Stewart, Clint Eastwood und James Garner (Westernkomödie: "Auch ein Sheriff braucht mal Hilfe") dar.

Zwei zentrale Motive bestimmen das Genre: Zum einen die (Selbst-)Erfahrung an der Grenze, dem „Frontier Land“, beispielhaft in "Der mit dem Wolf tanzt", in dem der Soldat John Dunbar nach einem missglückten und missverstandenen Selbstmordversuch während einer Schlacht im Bürgerkrieg die Armee verlässt, "„um den Wilden Westen zu sehen, solange es ihn noch gibt“". Zum anderen die Erneuerung einer Gesellschaft durch Gewalt, die Wiederherstellung einer neuen, vitaleren und zivileren Ordnung, nachdem die alte Ordnung durch Gewalt zerstört wurde. Die vier Phasen der Geschichte der Eroberung des Westens – frühes Vordringen in die Wälder des Ostens während der englisch-französischen Besatzung mittels Pfadfindern und Indianer-Scouts, Landnahme des Westens durch Planwagen-Trecks und kleine Siedler, Übergang zur zivilisierten Gesellschaft und schließlich Beendigung der Entwicklung durch Eisenbahnbau, Indianerkriege und Bürgerkrieg – schlagen sich in den einzelnen Filmen entsprechend nieder.
Allen vier Phasen gemeinsam ist das Spannungsfeld zwischen dem Faustrecht einerseits und dem es ablösenden Prinzip des staatlichen Rechts als Grundlage einer zivilisierten Gesellschaft andererseits. Dieses Spannungsfeld greift der Endzeit-Western auf, der in einer meist nicht näher bezeichneten Zukunft nach einer auch meist nur angedeuteten apokalyptischen Katastrophe angesiedelt ist. Ist im klassischen Western der Westen noch nicht zivilisiert, ist er es im Endzeit-Western nicht mehr (wobei ein Endzeit-Western nicht zwingend 'im Westen' angesiedelt sein muss). Auf beiden Prämissen baut die (Wieder-)Herstellung einer gesellschaftlichen Ordnung auf, in der zunächst das Recht des Stärkeren gilt, mit dem Ressourcen erkämpft werden: Im klassischen Western sind es z. B. die Ressourcen Siedlungsgebiet (Landnahme) und Bodenschätze (Goldrausch), im Endzeit-Western geht es zum Beispiel um Wasser oder Benzin. So unterscheidet sich der Endzeit-Western vor allem in Bezug auf die Zeit der Handlung und daraus folgend dem Interieur, Kostümen etc., die grundlegenden Erzählstrukturen, Themen und Motive bleiben sich aber gleich. Ein anschauliches Beispiel findet sich im zweiten Film der Reihe Mad Max: Dort greift eine Horde Punks auf Motorrädern einen fahrenden Tanklastzug an. Ersetzt man gedanklich die Punks durch Indianer, die Motorräder durch Pferde und den Tanklastzug durch Postkutsche, Planwagen oder Eisenbahn, könnte die Sequenz genauso gut in einen klassischen Western passen.

Am 1. Dezember 1903 kam mit "Der große Eisenbahnraub" der erste Western ins Kino, zu einer Zeit also, als es den wilden Westen fast noch tatsächlich gab. Von da an liefen jede Woche neue, zumeist einfache Produktionen im Stile von Broncho Billy in den Kinos an, die sich auf die action- und gewaltgeladene Konfrontation zwischen den Hauptfiguren konzentrierten und sich wenig mit Psychologie, komplexen Charakteren und Handlungen beschäftigten. Bis in die siebziger Jahre hinein kann man den Western als das wichtigste Genre der Filmproduktion der USA bezeichnen, wobei die vierziger und fünfziger Jahre als der Höhepunkt der Entwicklung gelten dürfen.

Die Ikonografie des frühen Western hat besonders von dem Maler Frederic Remington (1861–1909) gelebt, der seinerseits die Fotografien Eadweard Muybridges kannte. Ford schätzte später diese „volkstümliche und unakademische, aber durchaus artifizielle Ästhetisierung des Traditionellen“  seiner Werke, wie auch Hawks eine umfangreiche Sammlung von Drucken und Gemäldekopien nach Werken von unter anderem Remington und Charles M. Russell besaß. Ford übernahm Remingtons Kadrierung und die Behandlung des Raumes seiner Bilder. Der Maler Charles Schreyvogel, der sein Handwerk noch als Zuschauer in Buffalo Bills Wild-West-Show lernte, nahm auf monumentalen Leinwänden die Wirkungsweise des Filmbilds vorweg. Die Maler Thomas Moran und Albert Bierstadt waren lyrischer, zugleich aufrichtiger. 

Mit nur drei Western gelang es John Ford, das Genre grundsätzlich zu verändern. Ist sein Darsteller John Wayne zuerst noch ein makelloser Held oder väterlicher Offizier, bleibt am Ende von "Der schwarze Falke", einem der komplexesten und vielschichtigsten Western, nur ein rachsüchtiger Einzelgänger, der genauso verloren und heimatlos ist wie die von ihm erbittert gejagten Feinde. Von nun an konnte es keine einfache Schwarz-Weiß-Zeichnung der Protagonisten mehr geben.

Eine davon optisch und inhaltlich deutlich zu unterscheidende Sonderrolle nehmen die 1962 mit "Der Schatz im Silbersee" beginnenden Karl-May-Verfilmungen der im wilden Westen spielenden Romane von Karl May ein, die sich neben der durch die anderen Drehorte bedingten Bildästhetik auch durch ein deutlich indianer-freundlicheres Bild von den US-amerikanischen Produktionen abheben und in denen vor allem die Heldenfiguren Winnetou und Old Shatterhand beispielhaft für die mögliche Verständigung zwischen Indianern und "Bleichgesichtern" stehen. Noch deutlicher auf die Seite der Indianer schlugen sich die 1966 mit "Die Söhne der großen Bärin" beginnenden DEFA-Indianerfilme.

Aufgrund der Unverrückbarkeit seiner Elemente nahm der Western zunehmend eine Entwicklung nach innen, in die Tiefe. Dies geschieht oft durch fast unmerkliche Verschiebungen. Der Showdown in "Spiel mir das Lied vom Tod" steht immer noch im Zentrum des Films, untersucht man jedoch die Szene genau, stellt man fest, dass das anachronistische Duell zwischen Henry Fonda und Charles Bronson nicht auf der Hauptstraße der Stadt stattfindet – dort wird gerade die Eisenbahn gebaut, Symbol für die neue Zeit –, sondern auf einem Nebenschauplatz, dem Hinterhof einer Farm.

Ohne Orte, Figuren und Handlungsablauf anzutasten, entstanden der epische, der psychologische Western und schließlich, Ende der 1960er und Anfang der 1970er nahtlos an das Ende des klassischen Westerns anknüpfend, der Spät-Western und Italo-Western, die wesentlich schonungsloser, zynischer und auch zuweilen realistischer mit ihrem Sujet umgehen. Seitdem folgte mit den Filmen "Der mit dem Wolf tanzt", "Erbarmungslos", "Dead Man", "The Missing", "True Grit" und nicht zuletzt "Django Unchained" in unregelmäßigen Abständen ein Revival dieses immer wieder totgesagten Genres.

Im Zentrum des Western steht die Besiedlung der sogenannten "frontier" ("frontier land"). Mit dieser "frontier" ist allerdings nicht nur die sich stetig nach Westen verschiebende Grenze der Trapper, Goldsucher, Siedler und Viehzüchter gemeint, tatsächlich geht es um die Konfrontation mit dem eigenen Ich – um eine Grenzerfahrung in zweifachem Sinne also, die sich auf einer geografischen Ebene einerseits und einer metaphysischen, individuellen Ebene andererseits abspielt.

Der Westernheld (Cowboy oder Trapper), dessen Urtypus Davy Crockett und Daniel Boone darstellen, und der stets im Mittelpunkt der Handlung steht, ist eine in seinen moralischen Werten vom mittelalterlichen Ritter abgeleitete, zutiefst romantische Figur. So wie sich der Ritter von Reiter ableitet, und der Chevalier von Cheval (dt.: Pferd), ist der Westmann ohne sein Pferd undenkbar. Damit und mit seinem fransengeschmückten Lederanzug ist er der Natur näher als der bürgerlichen Gesellschaft, die sich in der Zeit der Industrialisierung und der Besiedlung des Westens durch Weiße krakenartig ausbreitet. Auch wenn er für sie auskundschaftet und ihr vorauseilt, ihre Planwagen-Trecks anführt, sie damit unweigerlich hinter sich herzieht, so lehnt er sie doch im Herzen ab und befindet sich im gleichen Maße auf der Flucht vor ihr. Die Gesetze, denen er folgt, bezieht er nicht aus den Gesetzbüchern der Städte, er leitet sie scheinbar direkt von Gott und aus der Natur ab. Der Begriff "Outlaw", jemand also, der sich außerhalb der Gesetze stellt, hat nicht nur auf Grund der tief im amerikanischen Bewusstsein verankerten Vorliebe für Gesetzesbrecher und Gangster eine positive Färbung, auch der positive Held des Western ist auf seine Art stets ein "Outlaw" – und damit muss er unweigerlich in Konflikt mit der Gesellschaft geraten. So wird der „gute“ "Outlaw" zum Alter Ego des „bösen“ Westernhelden. In vielen Western wie "Der schwarze Falke", "Der Mann, der Liberty Valance erschoß" und "High Noon" wird diese enge Verwandtschaft zwischen den Gegenspielern bewusst zum Thema des Films gemacht. In "Der Mann aus dem Westen" trifft Gary Cooper sogar auf seinen ehemaligen Ziehvater, um mit ihm in existenzielle Konflikte zu geraten.

Diese Freiheit, in der Konfrontation mit dem eigenen Ich jenseits der alles regelnden Zivilisation zu triumphieren, ist der Kern der zum Gründungsmythos der Vereinigten Staaten im Western verklärten Besiedelung des Westens. Im Spätwestern wird der Verlust dieser Freiheit immer wieder thematisiert: Das letzte Stück Land ist besiedelt, das letzte Wildpferd gefangen, Automobile und Maschinengewehre halten Einzug.

Michael Cimino stellte mit dem wirtschaftlichen Debakel "Heaven’s Gate" (1980) den Johnson-County-Krieg 1892 in Wyoming dar, wo das Land verteilt war, und das Großkapital den Siedlern die Zähne zeigte (zudem änderte er die Produktionsbedingungen von Hollywood in den Folgejahren entscheidend).

In Sam Peckinpahs "The Wild Bunch" (1969), der nicht zufällig in Mexiko spielt, etwa 1914, geht es um eine von "Pike" angeführte Bande scheinbar gewissenloser Outlaws, die von seinem alten Freund "Thornton", einem ehemaligen Mitglied der "Wild Bunch", gejagt werden. In jeder Szene jedoch merkt man Thornton an, dass er lieber an Pikes Seite reiten würde, statt eine Horde zwar auf der Seite des Gesetzes stehender, aber zutiefst unmoralischer Kopfgeldjäger anzuführen. Thornton, Pike und seine "Wild Bunch" sind Dinosaurier (in einer Szene fällt Pike sogar vom Pferd), die von der modernen Zeit überlebt wurden. Im zentralen Moment wählen sie in einem Augenblick persönlicher Freiheit mit einem kurzen „let’s go“ den sicheren Tod.

Die Westernhelden und Cowboys, wie sie typischerweise in Western-Filmen dargestellt werden, sind eine Kunstschöpfung der Populärkultur, die nicht nachträglich, sondern im selben Moment erfunden wurden, als ihre Vorbilder im Westen das Land erkundeten. Der Stenograf des Revolverhelden Duke of Death in "Erbarmungslos" ist historische Realität. Der berühmte Kit Carson wurde von einem solchen Begleiter für die Groschenhefte der Ostküste zur Romanfigur aufgebaut. Eine reale Figur namens "Deadwood Dick" hat es nicht gegeben, aber als zunehmend Leser der Deadwood-Dick-Geschichten nach Deadwood zu pilgern begannen, nahm man dort die Produktion von Postkarten mit dem vermeintlichen Konterfei des Westerners auf.

Eine übergeordnete Rolle in der Geschichte der Entwicklung der amerikanischen Populärkultur nimmt Buffalo Bill mit seinem Zirkus und seinem "Rough Rider Congress" ein. Unklar blieb oft, ob die Cowboys ihre Revolver-, Lasso- und Pferdetricks nur übten, um in Buffalo Bills Show aufzutreten, oder dies tatsächlich Bestandteil ihres täglichen Lebens im Wilden Westen war. Buffalo Bills Wild West Show gastierte mit ihrem Programm auch in Europa und sorgte damit lange vor dem Film für eine weite Verbreitung des Wild-West-Mythos, und kann damit, zusammen mit den Dime and Nickel Novels der Ostküste aus dem Hause Beadle & Adams, als Vorläufer des Western-Films angesehen werden.

In der Soziologie und der Kulturwissenschaft wird das Genre Western auch unter dem Aspekt der nationalen Mythen und Legendenbildung untersucht. Dabei weisen die klassischen, nicht satirischen Western eine Grundstruktur auf: „So erzählen sie immer, wie ein Kontinent ‚zivilisiert‘ wird durch den Mut und die Kraft eines Mannes. Und am Ende steht immer die Herstellung oder Wiederherstellung von staatlicher Ordnung bzw. Staatlichkeit.“ (Rudolf Walther)

Martin Weidinger beschreibt diesen Sachverhalt in seiner Studie „Nationale Mythen – männliche Helden. Politik und Geschlecht im amerikanischen Western“. Die Helden des Western sind „Ikonen des Machismo“ und kämpfen gegen alle Widerstände für eine neue Ordnung oder wollten eine alte zurückgewinnen.

Geschlecht, Religion, Hautfarbe und sexuelle Orientierung sind im klassischen Western streng hierarchisch geordnet und festgelegt. Walther schreibt dazu in einer FR-Rezension zur Studie Weidingers: „Bis in die sechziger Jahre des 20. Jahrhunderts hinein hat diese stereotype Hierarchisierung die USA entscheidend geprägt. Das Ende des Western fällt zusammen mit dem Aufkommen von Studenten-, Bürgerrechts-, Frauen- und Antikriegsbewegung.“ 






</doc>
<doc id="5546" url="https://de.wikipedia.org/wiki?curid=5546" title="Wismut">
Wismut

Wismut steht für:

Namensbestandteil von Sportvereinen:
Siehe auch:


</doc>
<doc id="5547" url="https://de.wikipedia.org/wiki?curid=5547" title="Wasserstoff">
Wasserstoff

Wasserstoff ist ein chemisches Element mit dem Symbol H (für „Wassererzeuger“; von ' „Wasser“ und ' „werden, entstehen“) und der Ordnungszahl 1. Im Periodensystem steht es in der 1. Periode und der 1. IUPAC-Gruppe, es nimmt also den ersten Platz ein.

Wasserstoff ist das häufigste chemische Element im Universum, jedoch nicht in der Erdrinde. Er ist Bestandteil des Wassers und beinahe aller organischen Verbindungen. Somit kommt gebundener Wasserstoff in sämtlichen lebenden Organismen vor.

Wasserstoff ist das chemische Element mit der geringsten Atommasse. Sein häufigstes Isotop, das auch als Protium bezeichnet wird, enthält kein Neutron, sondern besteht aus nur einem Proton und einem Elektron. Unter Bedingungen, die normalerweise auf der Erde herrschen (siehe auch Normalbedingungen), kommt nicht dieser "atomare Wasserstoff" H vor, sondern der "molekulare Wasserstoff" H, ein farb- und geruchloses Gas. Bei bestimmten chemischen Reaktionen tritt Wasserstoff vorübergehend atomar als H auf, bezeichnet als naszierender Wasserstoff. In dieser Form reagiert er besonders stark mit anderen Verbindungen oder Elementen.

Entdeckt wurde Wasserstoff vom englischen Chemiker und Physiker Henry Cavendish im Jahre 1766, als er mit Metallen (Eisen, Zink und Zinn) und Säuren experimentierte. Cavendish nannte das dabei entstandene Gas wegen seiner Brennbarkeit „inflammable air“ ("brennbare Luft"). Er untersuchte das Gas eingehend und veröffentlichte seine Erkenntnisse darüber noch im selben Jahr. Auf ähnliche Weise (Einwirkung von Säuren auf Metalle) erzeugten allerdings schon im 17. Jahrhundert Théodore Turquet de Mayerne (um 1620) und Robert Boyle (um 1670) Knallgas.

Eine genauere Analyse geschah durch Antoine Laurent de Lavoisier, der den Wasserstoff als „Wasser erzeugenden Stoff“ oder „Hydrogen“ bezeichnete und ihm damit seinen heutigen Namen gab. Cavendish hatte inzwischen eine Beobachtung von Joseph Priestley aufgreifend erkannt, dass bei der Verbrennung von Wasserstoff Wasser entsteht (veröffentlicht erst 1784). Lavoisier erfuhr von den Experimenten von Cavendish beim Besuch von dessen Assistenten Charles Blagden 1783. Cavendish war Anhänger der Phlogistonlehre und sein Wasserstoff war für ihn ein Kandidat für diese hypothetische Substanz. Lavoisier zeigte in aufsehenerregenden Experimenten, dass es ein eigenständiges Element war und Bestandteil des Wassers, das man damals vielfach selbst noch für elementar gehalten hatte gemäß der alten Vier-Elemente-Lehre. Lavoisier führte seine Experimente quantitativ aus unter Verwendung der von ihm postulierten Massenerhaltung. Er leitete Wasserdampf in einer abgeschlossenen Apparatur über glühende Eisenspäne und ließ ihn an anderer Stelle kondensieren. Dabei stellte er fest, dass die Masse des kondensierten Wassers etwas geringer war als die der ursprünglichen Menge. Dafür entstand ein Gas, dessen Masse zusammen mit dem Gewichtszuwachs des oxidierten Eisens genau der „verloren gegangenen“ Wassermenge entsprach. Sein eigentliches Experiment war also erfolgreich.

Lavoisier untersuchte das entstandene Gas weiter und führte die heute als Knallgasprobe bekannte Untersuchung durch, wobei das Gas verbrannte. Er nannte es daher zunächst „brennbare Luft“. Als er in weiteren Experimenten zeigte, dass sich aus dem Gas umgekehrt Wasser erzeugen lässt, taufte er es hydro-gène (griechisch: "hydro" = Wasser; "genes" = erzeugend). Das Wort bedeutet demnach: „Wassererzeuger“. Die deutsche Bezeichnung lässt auf die gleiche Begriffsherkunft schließen.

Nachdem man gemäß der Schule von Lavoisier lange Sauerstoff für den Säurecharakter verantwortlich gemacht hatte, änderte sich dies, als Humphry Davy 1808 Chlorwasserstoff darstellte und nachwies, dass darin kein Sauerstoff enthalten war. Danach erkannte man, dass statt Sauerstoff Wasserstoff für den Säurecharakter verantwortlich war.

Wasserstoff ist das häufigste chemische Element in der Sonne und den großen Gasplaneten Jupiter, Saturn, Uranus und Neptun, die über 99,99 % der Masse des Sonnensystems in sich vereinen. Wasserstoff stellt 75 % der gesamten Masse beziehungsweise 93 % aller Atome des Sonnensystems. Im gesamten Weltall wird (unter Nichtbeachtung dunkler Materie) ein noch höherer Anteil an Wasserstoff vermutet.

Kurz nach der Entstehung des Universums waren nach der mutmaßlichen Vernichtung der Antimaterie durch ein geringes Übermaß der Materie und der Kondensation eines Quark-Gluon-Plasmas zu Baryonen nur mehr Protonen und Neutronen (nebst Elektronen) vorhanden. Bei den vorherrschenden hohen Temperaturen vereinigten sich diese zu leichten Atomkernen, wie H und He. Die meisten Protonen blieben unverändert und stellten die zukünftigen H-Kerne dar.
Nach ungefähr 380.000 Jahren war die Strahlungsdichte des Universums so gering geworden, dass sich Wasserstoff-Atome einfach durch Zusammenschluss der Kerne mit den Elektronen bilden konnten, ohne gleich wieder durch ein Photon auseinandergerissen zu werden.

Mit der weitergehenden Abkühlung des Universums formten sich unter dem Einfluss der Gravitation und ausgehend von räumlichen Dichteschwankungen allmählich Wolken aus Wasserstoffgas, die sich zunächst großräumig zu Galaxien und darin zu Protosternen zusammenballten. Unter dem wachsenden Druck der Schwerkraft setzte schließlich die Kernfusion ein, bei der Wasserstoff zu Helium verschmilzt. So entstanden erste Sterne und später die Sonne.

Sterne bestehen weit überwiegend aus Wasserstoff-Plasma. Die Kernfusion von Wasserstoff H zu Helium He erfolgt hauptsächlich über die Zwischenstufen Deuterium H und Helium He oder über den Bethe-Weizsäcker-Zyklus. Die dabei frei werdende Energie ist die Energiequelle der Sterne. Der in unserer Sonne enthaltene Wasserstoff macht den größten Teil der gesamten Masse unseres Sonnensystems aus.

Auch die Gasplaneten bestehen zu großen Teilen aus Wasserstoff. Unter den extremen Drücken, die in großen Tiefen in den großen Gasplaneten Jupiter und Saturn herrschen, kann er in metallischer Form existieren. Dieser "metallische" Kern ist elektrisch leitfähig und erzeugt vermutlich das Magnetfeld der Gasplaneten.

Außerhalb von Sternensystemen kommt Wasserstoff auch in Gaswolken vor. In den so genannten H-I-Gebieten liegt das Element molekular und nichtionisiert vor. Diese Gebiete emittieren Strahlung von etwa 1420 MHz, die sogenannte 21-cm-Linie, auch HI- oder Wasserstofflinie genannt, die von Übergängen des Gesamtdrehimpulses herrührt. Sie spielt eine wichtige Rolle in der Astronomie und dient dazu, Wasserstoffvorkommen im All zu lokalisieren und zu untersuchen.

Ionisierte Gaswolken mit atomarem Wasserstoff nennt man dagegen H-II-Gebiete. In diesen Gebieten senden Sterne hohe Mengen ionisierender Strahlung aus. Mit Hilfe der H-II-Gebiete lassen sich Rückschlüsse auf die Zusammensetzung der interstellaren Materie ziehen. Wegen ständiger Ionisation und Rekombination der Atome senden sie mitunter sichtbare Strahlung aus, die oft so stark ist, dass man diese Gaswolken mit einem kleinen Fernrohr sehen kann.

Auf der Erde ist der Massenanteil wesentlich geringer. Bezogen auf die Erd-Gesamtmasse bestehen etwa 0,12 % und bezogen auf die Erdkruste etwa 2,9 % aus Wasserstoff. Außerdem liegt der irdische Wasserstoff im Gegensatz zu den Vorkommen im All überwiegend gebunden und nur selten in reiner Form als unvermischtes Gas vor. Die bekannteste und am häufigsten auftretende Verbindung ist das Wasser. Neben diesem sind auch Erdgase wie z. B. Methan sowie das Erdöl wichtige wasserstoffhaltige Verbindungen auf der Erde. Auch in mehr als der Hälfte aller bisher bekannten Minerale ist Wasserstoff enthalten.

Der größte Anteil irdischen Wasserstoffs kommt in der Verbindung Wasser vor. In dieser Form bedeckt er über zwei Drittel der Erdoberfläche. Die gesamten Wasservorkommen der Erde belaufen sich auf circa 1,386 Milliarden km. Davon entfallen 1,338 Milliarden km (96,5 %) auf Salzwasser in den Ozeanen. Die verbliebenen 3,5 % liegen als Süßwasser vor. Davon befindet sich wiederum der größte Teil im festen Aggregatzustand: in Form von Eis in der Arktis und Antarktis sowie in den Permafrostböden vor allem in Sibirien. Der geringe restliche Anteil ist flüssiges Süßwasser und findet sich meist in Seen und Flüssen, aber auch in unterirdischen Vorkommen, etwa als Grundwasser.

In der Erdatmosphäre liegt Wasserstoff hauptsächlich als gasförmiges Wasser (Wasserdampf) vor. Wie viel Wasserdampf eine Volumeneinheit Luft enthält, hängt neben dem Vorhandensein von Wasser von der Lufttemperatur ab. Beispielsweise kann Luft von 30 °C Temperatur bis zu 4,2 Volumenprozent Wasserdampf aufnehmen. Die relative Luftfeuchtigkeit beträgt dann 100 %, da der Sättigungsdampfdruck des Wassers erreicht ist.

Die Häufigkeit von molekularem Wasserstoff in der Atmosphäre beträgt nur 0,55 ppm. Dieser niedrige Anteil kann mit der hohen thermischen Geschwindigkeit der Moleküle und dem hohen Anteil an Sauerstoff in der Atmosphäre erklärt werden. Bei der mittleren Temperatur der Atmosphäre bewegen sich die H-Teilchen im Durchschnitt mit fast 2 km/s. Das ist rund ein Sechstel der Fluchtgeschwindigkeit auf der Erde. Aufgrund der Maxwell-Boltzmann-Verteilung der Geschwindigkeiten der H-Moleküle gibt es aber dennoch eine beträchtliche Zahl von Molekülen, welche die Fluchtgeschwindigkeit erreichen. Die Moleküle haben jedoch nur eine extrem geringe freie Weglänge, sodass nur Moleküle in den oberen Schichten der Atmosphäre tatsächlich entweichen. Weitere H-Moleküle kommen aus darunter liegenden Schichten nach, und es entweicht wieder ein bestimmter Anteil, bis letztlich nur noch Spuren des Elements in der Atmosphäre vorhanden sind. Zudem wird der Wasserstoff in den unteren Schichten der Atmosphäre durch eine photoaktivierte Reaktion mit Sauerstoff zu Wasser verbrannt. Bei einem geringen Anteil stellt sich ein Gleichgewicht zwischen Verbrauch und Neuproduktion (durch Bakterien und photonische Spaltung des Wassers) ein.

Einfache chemische Prozesse zur Produktion von H sind die Reaktion verdünnter Säuren mit unedlen Metallen (z. B. Zink) oder die Zersetzung des Wassers durch Alkalimetalle. Diese im chemischen Laboratorium für kleine Mengen üblichen Methoden sind aber für die industrielle Herstellung ungeeignet und unwirtschaftlich.

Eine Methode zur industriellen Gewinnung von molekularem Wasserstoff ist die Dampfreformierung. Unter hoher Temperatur und hohem Druck werden Kohlenwasserstoffe mit Wasser umgesetzt. Dabei entsteht Synthesegas, ein Gemisch aus Kohlenstoffmonoxid und Wasserstoff. Das Mengenverhältnis kann dann durch die sogenannte Wassergas-Shift-Reaktion eingestellt werden. Diese Methode wird hauptsächlich für industrielle Hochdrucksynthesen eingesetzt. Die zweite gängige Methode in der Industrie ist die partielle Oxidation. Hierbei reagiert meistens Erdgas mit Sauerstoff unter Bildung von H und Kohlenmonoxid.

Eine alte und effiziente Möglichkeit zur Wasserstoffgewinnung ist die Elektrolyse von Wasser. Dabei wird Wasser mit Hilfe von elektrischem Strom in Wasserstoff und Sauerstoff gespalten.

Meist wird dem Wasser ein wenig Säure zur Katalyse der Reaktion zugesetzt. An der Kathode entsteht Wasserstoffgas, an der Anode Sauerstoffgas, im Mol- und Volumenverhältnis 2:1. Diese Methode wird heute nur noch in sehr geringem Umfang eingesetzt, vor allem zur Gewinnung von „schwerem Wasser“, das sich bei der Elektrolyse im nicht umgesetzten Rest anreichert.

Eine moderne Methode ist das Kværner-Verfahren. Dabei zerlegt ein Plasmabrenner Kohlenwasserstoffe zu Kohlenstoff und Wasserstoff und erreicht dabei enorm hohe Wirkungsgrade.

Atomarer Wasserstoff kann durch Zufuhr der Dissoziationsenergie aus dem molekularen Element erzeugt werden. Methodisch wird dieses bewerkstelligt durch Erhitzung auf mehrere tausend Grad, elektrische Entladung bei hoher Stromdichte und niedrigem Druck, Bestrahlung mit Ultraviolettlicht, Beschuss mit Elektronen bei 10 bis 20 Elektronenvolt oder Mikrowellenstrahlung. Allerdings reagiert atomarer Wasserstoff (z. B. an Behälterwänden) sehr schnell wieder zu molekularem Wasserstoff. Es stellt sich somit ein Fließgleichgewicht ein, das in der Regel weit auf der Seite des molekularen Wasserstoffs liegt.

Zur Darstellung von größeren Mengen atomaren Wasserstoffs sind das Woodsche Darstellungsverfahren (Robert Williams Wood, 1898) und dasjenige von Irving Langmuir, die Langmuir-Fackel besonders geeignet.

Wasserstoff ist das Element mit der geringsten Dichte. Molekularer Wasserstoff (H) ist etwa 14,4-mal so leicht wie Luft. Flüssiger Wasserstoff wiegt 70,8 Gramm pro Liter. Sein Schmelzpunkt liegt bei 14,02 K (−259 °C), der Siedepunkt bei 21,15 K (−252 °C). Wasserstoff ist in Wasser und anderen Lösungsmitteln schlecht löslich. Für Wasser beträgt die Löslichkeit 18,2 mL/L bei 20 °C und Normaldruck. Dagegen ist die Löslichkeit (genauer maximale Volumenkonzentration) in Metallen deutlich höher.

Einige thermodynamische Eigenschaften (Transportphänomene) sind aufgrund der geringen Molekülmasse und der daraus resultierenden hohen mittleren Geschwindigkeit der Wasserstoffmoleküle (1770 m/s bei 25 °C) von besonderer Bedeutung, (wie z. B. beim Oberth-Effekt-Raketentreibstoff). Wasserstoff besitzt bei Raumtemperatur das höchste Diffusionsvermögen, die höchste Wärmeleitfähigkeit und die höchste Effusionsgeschwindigkeit aller Gase. Eine geringere Viskosität weisen nur drei- oder mehratomige reale Gase wie zum Beispiel "n"-Butan auf.

Die Mobilität des Wasserstoffs in einer festen Matrix ist, bedingt durch den geringen Molekülquerschnitt, ebenfalls sehr hoch. So diffundiert Wasserstoff durch Materialien wie Polyethylen und glühendes Quarzglas. Ein sehr wichtiges Phänomen ist die außerordentlich hohe Diffusionsgeschwindigkeit in Eisen, Platin und einigen anderen Übergangsmetallen, da es dort dann zur Wasserstoffversprödung kommt. In Kombination mit einer hohen Löslichkeit treten bei einigen Werkstoffen extrem hohe Permeationsraten auf. Hieraus ergeben sich technische Nutzungen zur Wasserstoffanreicherung, aber auch technische Probleme beim Transportieren, Lagern und Verarbeiten von Wasserstoff und Wasserstoffgemischen, da nur Wasserstoff diese räumlichen Begrenzungen durchwandert (siehe Sicherheitshinweise).

Wasserstoff hat ein Linienspektrum und je nach Temperatur des Gases im sichtbaren Bereich ein mehr oder weniger ausgeprägtes kontinuierliches Spektrum. Letzteres ist beim Sonnenspektrum besonders ausgeprägt. Die ersten Spektrallinien im sichtbaren Bereich, zusammengefasst in der so genannten Balmer-Serie, liegen bei 656 nm, 486 nm, 434 nm und 410 nm. Daneben gibt es weitere Serien von Spektrallinien im Infrarot- (Paschen-Serie, Brackett-Serie und Pfund-Serie) und eine im Ultraviolettbereich (Lyman-Serie) des elektromagnetischen Spektrums. Eine besondere Bedeutung in der Radioastronomie hat die 21-Zentimeter-Linie (HI-Linie) in der Hyperfeinstruktur.

In einem magnetischen Feld verhält sich H sehr schwach diamagnetisch. Das bedeutet, die Dichte der Feldlinien eines extern angelegten Magnetfeldes nimmt in der Probe ab. Die magnetische Suszeptibilität ist bei Normdruck formula_3 = −2,2 · 10 und typischerweise einige Größenordnungen unter der von diamagnetischen Festkörpern.

Gegenüber elektrischem Strom ist H ein Isolator. In einem elektrischen Feld hat er eine Durchschlagsfestigkeit von mehreren Millionen Volt pro Meter.

Der Atomradius von Wasserstoff wurde zu 37 Pikometer bestimmt. In höchstangeregten Wasserstoffatomen, siehe Rydberg-Zustand, wie sie unter den Vakuumbedingungen interstellarer Nebel vorkommen, fliegen deren Elektronen auf Bahnen mit Atomradien von bis zu 0,339 Millimetern.

Bei Temperaturen unterhalb von 21,15 K kondensiert Wasserstoff zu einer klaren, farblosen Flüssigkeit. Dieser Zustand wird als LH abgekürzt (engl. "liquid", „flüssig“). Unterhalb von 14,02 K (−259,2 °C) bildet Wasserstoff einen kristallinen Festkörper mit hexagonal dichtester Kugelpackung (hcp), dort ist jedes Molekül von zwölf weiteren umgeben. Am Gefrierpunkt bildet sich beim Abkühlen ein schlammartiges Zweiphasengemisch, ein sogenannter Slush.

Anders als bei Helium tritt beim Verflüssigen von einfachem Wasserstoff (H) keine Suprafluidität auf; prinzipiell kann aber das Isotop Deuterium (H) suprafluid werden.

Der Tripelpunkt des Wasserstoffs, bei dem seine drei Aggregatzustände gleichzeitig vorkommen, ist einer der Fixpunkte der Internationalen Temperaturskala. Er liegt bei einer Temperatur von exakt 13,8033 K und einem Druck von 7,042 kPa. Der kritische Punkt liegt bei 33,18 K und 13,0 bar, die kritische Dichte beträgt 0,03012 g/cm (die niedrigste kritische Dichte aller Elemente).

Unter extremen Drücken, wie sie innerhalb von Gasplaneten herrschen, wird wahrscheinlich metallischer Wasserstoff, d. h. in metallischer Form, ausgebildet. Dabei wird er elektrisch leitend (vgl. Leitungsband).

Ein einzelnes Wasserstoffatom besteht aus einem positiv geladenen Kern und einem negativ geladenen Elektron, das über die Coulomb-Wechselwirkung an den Kern gebunden ist. Dieser besteht stets aus einem einzelnen Proton (Hauptisotop H) und seltener je nach Isotop einem oder zwei zusätzlichen Neutronen (H bzw. H-Isotop). Das Wasserstoffatom H spielte aufgrund seines einfachen Aufbaus in der Entwicklung der Atomphysik als „Modellatom“ eine herausragende Rolle.

So entstand 1913 aus Untersuchungsergebnissen am Wasserstoff das bohrsche Atommodell, mit dessen Hilfe eine vergleichsweise einfache Beschreibung vieler Eigenschaften des Wasserstoffatoms möglich ist. Man stellt sich dazu vor, dass das Elektron den Kern auf einer bestimmten Kreisbahn umläuft. Nach Bohr kann das Elektron auch auf andere, im Abstand zum Kern genau definierte Bahnen springen, so auch auf weiter außen liegende, wenn ihm die dazu nötige Energie zugeführt wird (z. B. durch Stöße im erhitzten Gas oder in der elektrischen Gasentladung). Beim Rücksprung von einer äußeren auf eine innere Bahn wird jeweils eine elektromagnetische Strahlung oder Welle einer bestimmten, der frei werdenden Energie entsprechende Wellenlänge abgegeben. Mit diesem Modell lassen sich die Spektrallinien des H-Atoms erklären, die im sichtbaren Licht bei Wellenlängen von 656 nm, 486 nm, 434 nm und 410 nm liegen (Balmer-Serie); im ultravioletten Bereich liegt die Lyman-Serie mit Wellenlängen von 122 nm, 103 nm, 97 nm und 95 nm. Wichtige Serien im Infraroten sind die Paschen-Serie (1,9 µm; 1,3 µm; 1,1 µm und 1 µm) und die Brackett-Serie (4,1 µm; 2,6 µm; 2,2 µm und 1,9 µm) (in allen Serien sind hier nur die ersten vier Linien angegeben). Das Bohrsche Modell reicht aber bei der Betrachtung von Details und für andere Atome zur Erklärung der dabei beobachteten bzw. gemessenen Phänomene nicht aus.

Physikalisch korrekter ist die quantenmechanische Beschreibung, die dem Elektron anstelle der flachen bohrschen Bahnen räumlich ausgedehnte Atomorbitale zuschreibt. Das H-Atom ist das einzige, für das sich das Eigenwertproblem sowohl der nichtrelativistischen Schrödingergleichung als auch der relativistischen Diracgleichung analytisch, das heißt ohne den Einsatz numerischer Verfahren, lösen lässt. Das ist sonst nur für die ebenfalls ausgiebig untersuchten wasserstoffähnlichen Ionen möglich, denen lediglich ein Elektron verblieben ist (He, Li usw. bis U).

Andere quantenmechanische Phänomene bewirken weitere Effekte. Die Feinstruktur der Spektrallinien kommt u. a. daher, dass Bahndrehimpuls und Spin des Elektrons miteinander koppeln. Berücksichtigt man darüber hinaus den Kernspin, kommt man zur Hyperfeinstruktur. Eine sehr kleine, aber physikalisch besonders interessante Korrektur ist die Lambverschiebung durch elektromagnetische Vakuumfluktuationen. Durch all diese Korrekturen wird bereits das Spektrum des Wasserstoffs zu einem komplexen Phänomen, dessen Verständnis viel theoretisches Wissen in Quantenmechanik und Quantenelektrodynamik erfordert.

Unter normalen Bedingungen ist Wasserstoffgas H ein Gemisch von Molekülen in vier Kernspin-Zuständen, die sich durch die Symmetrie ihrer Kernspins voneinander unterscheiden. Sie lassern sich weiter in zwei Formen von Wasserstoff unterscheiden, welche als "ortho"- und "para"-Wasserstoff bezeichnet werden (kurz o- und p-Wasserstoff). Bei o-Wasserstoff haben die Kernspins symmetrische Konfiguration, während sie beim p-Wasserstoff einen antisymmetrischen Zustand einnehmen. o-Wasserstoff ist die energiereichere Form. Die beiden Molekülzustände hängen über folgende, temperaturabhängige Gleichgewichtsbeziehung miteinander zusammen:

Im reinen Gas dauert bei tiefen Temperaturen die Einstellung des Gleichgewichts Monate, da die Wechselwirkungen zwischen den Kernen und der Hülle extrem schwach sind. Für diese Zeiten liegt damit praktisch eine Mischung von zwei unterschiedlichen Gasen vor. Trotz gleicher chemischer Zusammensetzung H unterscheiden sie sich sogar makroskopisch durch deutlich verschiedenen Temperaturverlauf der spezifischen Wärme. Abgesehen hiervon sind die physikalischen Eigenschaften von o- und p-Wasserstoff aber nur geringfügig verschieden. Beispielsweise liegen der Schmelz- und Siedepunkt der p-Form etwa 0,1 K unter denen der o-Form.

Am absoluten Nullpunkt findet man ausschließlich p-Wasserstoff. Da es für antiparallele Kernspins (Gesamte Spinquantenzahl S=0) nur einen Spinzustand gibt, bei parallelen Kernspins (S=1) aber drei Zustände verschiedener Orientierung im Raum, liegen im Gleichgewicht unter Standardbedingungen 25 % des Wasserstoffs als p-Form und 75 % als o-Form vor. Über diesen Anteil hinaus kann der Anteil der o-Form im thermodynamischen Gleichgewicht nicht gesteigert werden.

Bei der industriellen Herstellung von flüssigem Wasserstoff spielt der Übergang zwischen o- und p-Wasserstoff eine wichtige Rolle, weil bei der Temperatur der Verflüssigung das Gleichgewicht schon stark zur p-Form hin tendiert und sich spätestens im flüssigen Zustand dann schnell einstellt. Damit die dabei frei werdende Wärme nicht gleich einen Teil der gewonnenen Flüssigkeit wieder verdampfen lässt, beschleunigt man die Einstellung des neuen Gleichgewichts schon im gasförmigen Zustand durch den Einsatz von Katalysatoren.

Im Periodensystem steht Wasserstoff in der I. Hauptgruppe, weil er 1 Valenzelektron besitzt. Ähnlich wie die ebenfalls dort stehenden Alkalimetalle hat er in vielen Verbindungen die Oxidationszahl +1. Allerdings ist sein Valenzelektron auf der K-Schale, die nur maximal 2 Elektronen haben kann und somit die Edelgaskonfiguration bereits mit 2 Elektronen und nicht mit 8, wie bei den anderen Schalen, erreicht.

Durch Aufnahme eines Elektrons kann er also die Edelgaskonfiguration des Heliums erreichen, was nur mit sehr unedlen Metallen möglich ist. Er hat dann die Oxidationszahl −1 und diese Verbindungen haben einen Halogencharakter; sie werden Hydride genannt.

Diese Stellung quasi „in der Mitte“ zwischen zwei Edelgaskonfigurationen, in der er die gleiche Anzahl Elektronen aufnehmen oder abgeben kann, ist eine Eigenschaft, die der IV. Hauptgruppe ähnelt, was seine Elektronegativität erklärt, die eher der des ebenfalls „in der Mitte“ stehenden Kohlenstoffs als der des Lithiums gleicht.

Aufgrund dieser „gemäßigten“ Elektronegativität sind die für die I. Hauptgruppe typischen Bindungen des Wasserstoffs in der Oxidationszahl "+1" keine Ionenbindungen wie bei den Alkalimetallen, sondern kovalente Molekülbindungen.

Zusammenfassend sind die Eigenschaften des Wasserstoffs für die I. Hauptgruppe atypisch, da aufgrund der Tatsache, dass die K-Schale nur 2 Elektronen aufnehmen kann, auch Eigenschaften anderer Gruppen hinzukommen.

Bei Zündung reagiert Wasserstoff mit Sauerstoff und Chlor heftig, ist sonst aber vergleichsweise beständig und wenig reaktiv. Bei hohen Temperaturen wird das Gas reaktionsfreudig und geht mit Metallen und Nichtmetallen gleichermaßen Verbindungen ein.

Mit Chlor reagiert Wasserstoff exotherm unter Bildung von gasförmigem Chlorwasserstoff, der in Wasser gelöst Salzsäure ergibt. Beide Gase reagieren dabei mit gleichen Stoffmengenanteilen:

Diese Reaktion ist unter dem Namen Chlorknallgasreaktion bekannt, die sich schon durch die Bestrahlung mit Licht zünden lässt. Für die Knallgasreaktion (Wasserstoff und Sauerstoff) bedarf es einer Zündung.

Die aggressivste Reaktion bei niedrigen Temperaturen geht jedoch Wasserstoff mit Fluor ein. Wird Wasserstoffgas bei −200 °C auf gefrorenes Fluor geleitet, reagieren die beiden Stoffe sofort explosiv miteinander.

Wird der molekulare Wasserstoff ionisiert, so spricht man vom Diwasserstoff-Kation. Dieses Teilchen tritt z. B. in Niedertemperatur-Plasmaentladungen in Wasserstoff als häufiges Ion auf.

Wasserstoff "in statu nascendi", d. h. im "Zustand des Entstehens" unmittelbar nach einer Wasserstoff erzeugenden Reaktion, existiert für Sekundenbruchteile in Form der einzelnen, sehr reaktiven H-Atome. Je zwei der Atome reagieren dann zum Molekül, das sich aber nach dem Zusammenschluss für kurze Zeit noch in einem angeregten Zustand befindet. Nascierender Wasserstoff kann – abweichend vom „normalen“ chemischen Verhalten – verschiedene Reaktionen bewirken, die mit molekularem Wasserstoff nicht möglich sind.

So gelingt es zum Beispiel nicht, mit Hilfe von im Kippschen Apparat erzeugtem Wasserstoffgas in einer angesäuerten, violetten Kaliumpermanganatlösung (KMnO) oder gelben Kaliumdichromatlösung (KCrO) den die Reduktion anzeigenden Farbwechsel hervorzurufen. Mit direkt in diesen Lösungen durch Zugabe von Zinkpulver erzeugtem Wasserstoff "in statu nascendi" gelingt diese reduktive Farbänderung.

Um molekularen Wasserstoff in die Atome zu zerlegen, muss Energie von etwa 4,5 eV pro Molekül oder genauer 436,22 kJ/mol aufgewendet werden (der Chemiker spricht von Enthalpie); beim Zusammenschluss zu Wasserstoffmolekülen (H) wird diese Energie wieder freigesetzt:

Das Gleichgewicht dieser Reaktion liegt unter Normalbedingungen vollkommen auf der rechten Seite der dargestellten Gleichung, denn atomarer Wasserstoff reagiert sehr rasch (z. B. an Behälterwänden) und stark exotherm zu molekularem Wasserstoff (oder mit anderen Reaktionspartnern, wenn solche in der Nähe sind).

Eine Anwendung findet diese Reaktion beim Arcatom-Schweißen.

Auch im Weltraum liegt bei niedrigen Temperaturen in der Regel molekularer Wasserstoff vor. In der Nähe heißer Sterne wird molekularer Wasserstoff jedoch von deren Strahlung aufgespalten, so dass dort die atomare Form überwiegt. Diese ist zwar sehr reaktiv und geht schnell neue Verbindungen ein, vor allem mit anderen Wasserstoffatomen, die jedoch von der Strahlung ebenfalls wieder gespalten werden. "Siehe dazu auch H-II-Gebiet."

Anmerkung: Wasserstoff in den Sternen liegt nicht nur atomar vor, sondern auch als Plasma: Die Elektronen sind infolge der dort herrschenden hohen Temperaturen je nach Temperatur von den Protonen mehr oder weniger abgetrennt. Die Oberfläche der Sonne hat jedoch nur eine Temperatur von ungefähr 6000 °C. Bei dieser Temperatur ist immer noch der größte Teil des Wasserstoffes nicht ionisiert und sogar molekular, d. h. das Gleichgewicht liegt weit auf der Seite des molekularen Wasserstoffes. Die thermische Energie ist bei 6000 °C weit unter der Energie von 4,5 eV, die zur Auflösung der molekularen Bindung erforderlich ist. Die Sonne ist jedoch in der Korona mit mindestens einer Million Kelvin wesentlich heißer. Daher sind im Sonnenlicht die Übergänge der Elektronen im atomaren Wasserstoff erkennbar. Chemische Verbindungen können sich bei so hohen Temperaturen kaum bilden und zerfallen sofort.

Eine wichtige Eigenschaft des Wasserstoffs ist die so genannte Wasserstoffbrückenbindung, eine anziehende elektrostatische Kraft zwischen zwei Molekülen. Ist Wasserstoff an ein stark elektronegatives Atom, wie zum Beispiel Fluor oder Sauerstoff, gebunden, so befindet sich sein Elektron eher in der Nähe des Bindungspartners. Es tritt also eine Ladungsverschiebung auf und das H-Atom wirkt nun positiv polarisiert. Der Bindungspartner wirkt entsprechend negativ. Kommen sich zwei solche Moleküle nahe genug, tritt eine anziehende elektrische Kraft zwischen dem positiven H-Atom des einen Moleküls und des negativen Teils des jeweiligen Partners auf. Das ist eine Wasserstoffbrückenbindung.

Da die Wasserstoffbrückenbindung mit nur 17 kJ/mol bis 167 kJ/mol schwächer ist als die Bindungskraft innerhalb eines Moleküls, verbinden sich die Moleküle nicht dauerhaft. Vielmehr bleibt die Wasserstoffbrücke wegen ständiger Bewegung nur Bruchteile einer Sekunde bestehen. Dann lösen sich die Moleküle voneinander, um erneut eine Wasserstoffbrückenbindung mit einem anderen Molekül einzugehen. Dieser Vorgang wiederholt sich ständig.

Die Wasserstoffbrückenbindung ist für viele Eigenschaften verschiedener Verbindungen verantwortlich, wie etwa DNA oder Wasser. Bei Letzterem führen diese Bindungen zu den Anomalien des Wassers, insbesondere der Dichteanomalie.

Es existieren drei natürlich vorkommende Isotope des Wasserstoffs. Von allen Elementen unterscheiden sich beim Wasserstoff die Isotope in ihren chemischen Reaktionsfähigkeiten am deutlichsten voneinander. Das liegt an dem vergleichsweise großen Unterschied der Atommasse (Deuterium H doppelt, Tritium H dreimal so schwer wie Wasserstoff H). In jüngerer Zeit gelang es, vier weitere Wasserstoffisotope nachzuweisen (H, H, H und H). Diese haben aber alle sehr kurze Lebensdauern (< 10 s).

Durch die Einbeziehung von Myonen, negativ geladenen instabilen Elementarteilchen mit ungefähr 10 % der Masse eines Protons, können exotische kurzlebige Strukturen erstellt werden, die sich chemisch wie ein Wasserstoffatom verhalten. Da Myonen selten natürlich vorkommen und ihre Lebensdauer lediglich 2 µs beträgt, werden solche Wasserstoffisotope künstlich an Teilchenbeschleunigern hergestellt.

Das Myonium besteht aus einem Elektron und einem positiv geladenen Antimyon, das die Rolle des Protons (also des Atomkerns) einnimmt. Auf Grund seiner Kernladungszahl von 1 e handelt es sich bei Myonium chemisch um Wasserstoff. Wegen der geringen Atommasse von 0,1 u (1/10 von H) treten Isotopeneffekte bei chemischen Reaktionen besonders stark in Erscheinung, so dass damit Theorien für Reaktionsmechanismen gut überprüft werden können.

Ein exotischer Wasserstoff mit einer Masse von 4,1 u entsteht, wenn in einem He-Atom eines der Elektronen durch ein "Myon" ersetzt wird. Auf Grund seiner gegenüber dem Elektron wesentlich höheren Masse ist das Myon dicht am He-Kern lokalisiert und schirmt eine der beiden Elementarladungen des Kerns ab. Zusammen bilden He-Kern und Myon effektiv einen Kern mit einer Masse von 4,1 u und einer Ladung von 1 e, so dass es sich chemisch um Wasserstoff handelt.

Jedes Jahr werden weltweit mehr als 600 Milliarden Kubikmeter Wasserstoff (rd. 30 Mio. t) für zahllose Anwendungen in Industrie und Technik gewonnen. Wichtige Einsatzgebiete sind:


Die beiden natürlichen Isotope haben spezielle Einsatzgebiete.

Deuterium findet (in Form von schwerem Wasser) in Schwerwasserreaktoren als Moderator Verwendung, d. h. zum Abbremsen der bei der Kernspaltung entstehenden schnellen Neutronen auf thermische Geschwindigkeit.

Deuterierte Lösungsmittel werden in der magnetischen Kernresonanzspektroskopie benutzt, da Deuterium einen Kernspin von Eins besitzt und im NMR-Spektrum des normalen Wasserstoff-Isotops nicht sichtbar ist.

In der Chemie und Biologie helfen Deuteriumverbindungen bei der Untersuchung von Reaktionsabläufen und Stoffwechselwegen (Isotopenmarkierung), da sich Verbindungen mit Deuterium chemisch und biochemisch meist nahezu identisch verhalten wie die entsprechenden Verbindungen mit Wasserstoff. Die Reaktionen werden von der Markierung nicht gestört, der Verbleib des Deuteriums ist in den Endprodukten dennoch feststellbar.

Ferner sorgt der erhebliche Massenunterschied zwischen Wasserstoff und Deuterium für einen deutlichen Isotopeneffekt bei den massenabhängigen Eigenschaften. So hat das schwere Wasser einen messbar höheren Siedepunkt als Wasser.

Das radioaktive Isotop Tritium wird in Kernreaktoren in industriell verwertbaren Mengen hergestellt. Außerdem ist es neben Deuterium ein Ausgangsstoff bei der Kernfusion zu Helium. In der zivilen Nutzung dient es in Biologie und Medizin als radioaktiver Marker. So lassen sich beispielsweise Tumorzellen aufspüren. In der Physik ist es einerseits selbst Forschungsgegenstand, andererseits untersucht man mit hochbeschleunigten Tritiumkernen schwere Kerne oder stellt künstliche Isotope her.

Mit Hilfe der Tritiummethode lassen sich Wasserproben sehr genau datieren. Mit einer Halbwertszeit von etwa zwölf Jahren eignet es sich besonders für die Messung relativ kurzer Zeiträume (bis zu einigen hundert Jahren). Unter anderem lässt sich so das Alter eines Weines feststellen.

Es findet Verwendung als langlebige, zuverlässige Energiequelle für Leuchtfarben (im Gemisch mit einem Fluoreszenzfarbstoff), vor allem in militärischen Anwendungen, aber auch in Armbanduhren. Weitere militärische Verwendung findet das Isotop in der Wasserstoffbombe und gewissen Ausführungen von Kernwaffen, deren Wirkung auf Spaltung beruht.

Wasserstoff gilt als ein Energieträger der Zukunft.

Wasserstoff als Energieträger verursacht keine schädlichen Emissionen, insbesondere kein Kohlendioxid, wenn er aus erneuerbaren Energien wie Wind, Sonne oder Biomasse gewonnen wird. Derzeit (2012) erfolgt die Wasserstoffherstellung fast ausschließlich aus fossilen Primärenergien, vorrangig Erdgas.

Wasserstoffgas enthält mehr Energie pro Gewichtseinheit als jeder andere chemische Brennstoff, allerdings deutlich weniger Energie pro Volumeneinheit, selbst in flüssigem Zustand. Wasserstoff ist, wie auch elektrische Energie, keine Primärenergie, sondern muss wie Strom aus Primärenergie hergestellt werden. (→ "Siehe auch Hauptartikel: Wasserstoffherstellung")

Die technischen Probleme bei der Speicherung von Wasserstoff gelten heute als gelöst. Verfahren wie Druck- und Flüssigwasserstoffspeicherung und die Speicherung in Metallhydriden befinden sich im kommerziellen Einsatz. Daneben existieren weitere Verfahren, die sich noch im Stadium der Entwicklung oder in der Grundlagenforschung befinden. (→ "Siehe auch Hauptartikel: Wasserstoffspeicherung")

Die verschiedenen Speichermethoden werden nach ihren Eigenschaften und den spezifischen Anforderungen der Fahrzeuge (z. B. PKW, Bus, Schiff, Flugzeug) eingesetzt:

Die ersten beiden Methoden erlauben eine einfache Wiedergewinnung des Wasserstoffs. Drucktanks aus kohlenstofffaserverstärktem Kunststoff mit bis zu 800 bar sind Behälter, die allen Sicherheitsanforderungen der Fahrzeughersteller entsprechen und vom TÜV abgenommen sind.

Da sich das Sicherheitsventil für Überdruck innerhalb des Tanks befindet, wird Wasserstoff im Notfall schrittweise abgegeben und verflüchtigt sich schnell. Wenn eine Zündquelle in der Nähe ist, kann sich der Wasserstoff entzünden, verbrennt aber schnell und mit geringer Wärmeabstrahlung. Eine Explosion ist nahezu unmöglich, da die Konzentration des Wasserstoffs in der Luft nicht ausreicht. Reiner Wasserstoff ist nicht explosiv.

Speicherung in Hydriden oder Nanoröhren stellen die sichersten Methoden dar. Die Tanks sind jedoch schwerer. In einem 200-kg-Tank können nur etwa 2 kg Wasserstoff gespeichert werden, was energetisch etwa 8 Litern Benzin entspricht. Auch ist die Rückgewinnung gasförmigen Wasserstoffs durch Wärmezuführung aufwendiger. Diese Form der Speicherung ist kostenintensiver als die Speicherung in Druck- und Flüssiggastanks.

Auf die Masse bezogen:

Auf das Volumen bezogen:

Schon bald nach den Anfängen der Kernphysik im ersten Viertel des 20. Jahrhunderts wurde die Aufmerksamkeit der Physiker auf die Energiegewinnung gelenkt. Neben der Kernspaltung wurde der Weg einer Verschmelzung der Kerne, die Kernfusion, erforscht. Die ersten gefundenen Reaktionen sind die Proton-Proton-Reaktionen, bei denen Wasserstoffkerne direkt zu Helium verschmelzen. Das konnte die Energiegewinnung in leichten Sternen wie unserer Sonne größtenteils erklären. Zwischen 1937 und 1939 entwickelten Hans Bethe und Carl Friedrich von Weizsäcker eine Theorie zur Kernfusion in sehr schweren Sternen, den nach ihnen benannten Bethe-Weizsäcker-Zyklus. Darin spielt Wasserstoff die überwiegende Rolle in der Energiegewinnung. Er wird aber nicht direkt zu Helium verschmolzen, sondern fusioniert in verschiedenen Reaktionen mit Kohlenstoff, Stickstoff und Sauerstoff. Am Ende des Zyklus entsteht Helium; die anderen Elemente wirken als Katalysatoren.

Während des Kalten Krieges bauten die Großmächte ihre nuklearen Waffenarsenale aus. Der Schritt zu den Fusionswaffen gelang zuerst den USA: basierend auf der Atombombe, die ihre Energie aus der Kernspaltung bezieht, konstruierten amerikanische Forscher unter Edward Teller die Wasserstoffbombe. In ihr wird durch die Kernfusion ein Vielfaches der Energie einer Uranbombe freigesetzt. 1952 testeten die Vereinigten Staaten die erste Wasserstoffbombe auf einer kleinen Pazifikinsel. Brennstoff war nicht Wasserstoff, sondern das Isotop Deuterium. In der Bombe liefen vor allem folgende Kernreaktionen ab:

Das entstandene Tritium und Helium-3 können noch weiter reagieren:

In Summe entstehen aus drei Deuteronen ein Heliumkern sowie ein Neutron und ein Proton.

Da Deuterium wie Wasserstoff schwer zu speichern ist, wird bei den meisten Fusionswaffen inzwischen auf Lithium-Deuterid (LiD) als Brennstoff zurückgegriffen. Durch die bei der Primärreaktion von Deuterium entstehenden Neutronen wird aus dem Lithium Tritium erbrütet:

Bei der Reaktion mit Lithium-6 wird zudem noch Energie frei, während die Reaktion mit Lithium-7 Energie verbraucht, dafür aber wieder ein Neutron erzeugt, das für die weitere Tritium-Produktion zur Verfügung steht.

Physiker forschen aber auch an einer friedlichen Nutzung der Kernverschmelzung zur Energiegewinnung. Am weitesten fortgeschritten sind Versuche, die Reaktion in einem Plasma kontrolliert ablaufen zu lassen. Die dazu nötigen sehr hohen Temperaturen sind schwierig zu realisieren. Ab etwa 1970 wurden die ersten entsprechenden Versuchsanlagen errichtet. Unter den heute (2016) führenden Anlagen sind beispielsweise JET und ITER (im Bau) in Europa, ein deutscher Tokamak-Reaktor in Garching sowie der Stellarator Wendelstein 7-X am Max-Planck-Institut für Plasmaphysik (IPP) in Greifswald.

Falls die Experimente erfolgreich verlaufen, sollen die gewonnenen Erkenntnisse für den Bau eines Demonstrationskraftwerks (DEMO) dienen. Die gegenwärtigen Planungen gehen von der Inbetriebnahme von DEMO etwa 2040 und der möglichen kommerziellen Nutzung ab etwa 2050 aus. Solche kommerziellen Reaktoren werden aber anders als Wasserstoffbomben voraussichtlich nur die Deuterium-Tritium-Reaktion zur Energiegewinnung nutzen können. Sie sind somit unbedingt auf Lithium zur Erbrütung des eigentlichen Brennstoffs Tritium angewiesen. Während Deuterium in den Weltmeeren in fast beliebiger Menge zur Verfügung steht, sind die bekannten Lithium-Vorräte beschränkt.

Mit Wasserstoffbrennen wird die Kernfusion von Wasserstoff in Helium im Inneren von Sternen (z. B. einer Nova, auf der Oberfläche eines Weißen Zwergs) bezeichnet. Diese Reaktion stellt in normalen Sternen während des Großteils ihres Lebenszyklus die wesentliche Energiequelle dar. Sie hat trotz ihres historisch bedingten Namens nichts mit einer chemischen Verbrennung zu tun.

Der Prozess der Kernfusion kann beim Wasserstoffbrennen auf zwei Arten ablaufen, bei denen auf verschiedenen Wegen jeweils vier Protonen, die Atomkerne des Wasserstoffs, in einen Heliumkern He umgewandelt werden:

Für die exakte Berechnung der freigesetzten Energie ist zu berücksichtigen, dass in Teilreaktion der Proton-Proton-Reaktion und auch des Bethe-Weizsäcker-Zyklus zwei Positronen freigesetzt werden, die bei der Annihilation mit einem Elektron 1,022 MeV entsprechend den Ruhemassen von Elektron und Positron freisetzen. Zur Massendifferenz der vier Protonen und des Heliumkerns ist folglich die zweifache Elektronenmasse zu addieren. Diese Massendifferenz ist identisch der Differenz der vierfachen Atommasse von Protium, Wasserstoff bestehend aus Protonen und Elektronen und der Atommasse von He. Diese Atommassen sind näherungsweise, aber nicht exakt identisch mit den Atommassen von Wasserstoff und Helium, da es verschiedene Isotope dieser Elemente gibt. Ferner verlässt ein kleiner Teil der Energie die Sonne in Form von Neutrinos.

Insgesamt wird beim Wasserstoffbrennen etwa 0,73 % der Masse in Energie umgewandelt, was man als Massendefekt bezeichnet. Die aus der Massendifferenz erzeugte Energie ergibt sich aus der einsteinschen Beziehung "E" = "mc"².
Sie resultiert aus der Kernbindungsenergie der Nukleonen, der Kernbausteine.

Die Fusion von Wasserstoff zu Helium ist am ergiebigsten; die nächste Stufe stellarer Fusionsreaktionen, das Heliumbrennen, setzt pro erzeugtem Kohlenstoffkern nur noch etwa ein Zehntel dieser Energie frei.

Wasserstoff ist in Form verschiedenster Verbindungen essentiell für alle bekannten Lebewesen. An vorderster Stelle zu nennen ist hier Wasser, welches als Medium für alle zellulären Prozesse und für alle Stofftransporte dient. Zusammen mit Kohlenstoff, Sauerstoff, Stickstoff (und seltener auch anderen Elementen) ist er Bestandteil derjenigen Moleküle aus der organischen Chemie, ohne die jegliche uns bekannte Form von Leben schlicht unmöglich ist.

Wasserstoff spielt im Organismus auch aktive Rollen, so bei einigen Koenzymen wie z. B. Nicotinamid-Adenin-Dinucleotid (NAD/NADH), die als Reduktionsäquivalente (oder „Protonentransporter“) im Körper dienen und bei Redoxreaktionen mitwirken. In den Mitochondrien, den Kraftwerken der Zelle, dient die Übertragung von Wasserstoffkationen (Protonen) zwischen verschiedenen Molekülen der so genannten Atmungskette dazu, einen Protonengradienten durch chemiosmotisches Membranpotenzial zur Generierung von energiereichen Verbindungen wie Adenosintriphosphat (ATP) bereitzustellen. Bei der Photosynthese in Pflanzen und Bakterien wird der Wasserstoff aus dem Wasser dazu benötigt, das fixierte Kohlendioxid in Kohlenhydrate umzuwandeln.

Bezogen auf die Masse ist Wasserstoff im menschlichen Körper das drittwichtigste Element: Bei einer Person mit einem Körpergewicht von 70 kg, sind rund 7 kg (= 10 Gew.-%) auf den enthaltenen Wasserstoff zurückzuführen. Nur Kohlenstoff (ca. 20 Gew.-%) und Sauerstoff (ca. 63 Gew.-%) machen einen noch größeren Gewichtsanteil aus. Bezogen auf die Anzahl der Atome ist der sehr leichte Wasserstoff sogar das mit Abstand häufigste Atom im Körper eines jeden Lebewesens. (Die 7 kg beim Menschen entsprechen 3,5·10 Mol Wasserstoff mit je 2·6·10 Atomen, das sind rund 4,2·10 Wasserstoffatome).

In biologischen Systemen reagiert molekularer Wasserstoff mit reaktiven Sauerstoffspezies und wirkt so als Antioxidans. Im Tierversuch führt die Anreicherung von Trinkwasser mit molekularem Wasserstoff nach Nierentransplantation zu einem besseren Überleben des Transplantates, zu einem verminderten Auftreten einer chronischen Schädigung des Transplantates, zu einer Verminderung der Konzentration an reaktiven Sauerstoffspezies und zu einer Hemmung von Signalwegen, welche die entzündliche Aktivität verstärken (proinflammatorische Signalwege).

Wasserstoff ist hochentzündlich. Er brennt mit reinem Sauerstoff oder Luft sowie mit anderen gasförmigen Oxidationsmitteln wie Chlor oder Fluor mit heißer Flamme. Da die Flamme kaum sichtbar ist, kann man unabsichtlich hinein geraten. Gemische mit Chlor oder Fluor sind schon durch Ultraviolettstrahlung entzündlich (siehe "Chlorknallgas"). Außer der nach GHS vorgeschriebenen Kennzeichnung (siehe Info-Box) müssen H-Druckgasflaschen nach DIN EN 1089-3 mit roter Flaschenschulter und rotem Flaschenkörper versehen sein.

Wasserstoff ist ungiftig und schädigt auch nicht die Umwelt. Daher ist auch kein MAK-Wert festgelegt. Atem- oder Hautschutz sind nicht erforderlich. Erst wenn hohe Konzentrationen eingeatmet werden, können durch den Mangel an Sauerstoff ab etwa 30 Vol.-% Bewegungsstörungen, Bewusstlosigkeit und Ersticken auftreten.

Gemische aus Luft und 4 bis 76 Vol.-% Wasserstoff sind brennbar. Ab einer Konzentration von 18 % in Luft ist das Gemisch explosiv (Knallgas). Die Zündtemperatur in Luft beträgt 560 °C. Bei der Handhabung ist der Wasserstoff von Zündquellen, einschließlich elektrostatischen Entladungen, fernzuhalten. Die Lagerung der Behälter sollte fern von oxidierenden Gasen (Sauerstoff, Chlor) und anderen brandfördernden Stoffen erfolgen.

Wasserstoff kann wegen dessen geringer Atomgröße durch viele Feststoffe hindurchdiffundieren, das heißt, Gas kann langsam durch ungeeignete Materialien (z. B. Plaste) austreten. Die für Gastanks und Leitungen verwendeten Materialien und -stärken berücksichtigen das, sodass keine größeren Risiken bestehen als z. B. mit Benzin. Wasserstofffahrzeuge mit Drucktanks können problemlos in Parkhäusern und Tiefgaragen geparkt werden. Es existiert keine gesetzliche Bestimmung, die das einschränkt ("siehe dazu": Wasserstoffspeicherung).

Molekularen Wasserstoff kann man durch die Knallgasprobe nachweisen. Bei dieser Nachweisreaktion wird eine kleine, beispielsweise während einer Reaktion aufgefangene Menge eines Gases, in einem Reagenzglas entzündet. Wenn danach ein dumpfer Knall, ein Pfeifen oder ein Bellen zu hören ist, so ist der Nachweis positiv (das heißt, es war Wasserstoff in dem Reagenzglas). Der Knall kommt durch die Reaktion von Wasserstoffgas mit dem Luftsauerstoff zustande:

Mit der gleichen Reaktion verbrennt Wasserstoff mit einer schwach bläulichen Flamme, wenn man ihn gleich an der Austrittsstelle entzündet (Pfeifgas).

Die Knallgasprobe ist die „klassische“ Methode zum Nachweis und ist besonders in Schulversuchen beliebt.

Wasserstoff geht mit den meisten chemischen Elementen Verbindungen mit der allgemeinen Summenformel EH ("n" = 1, 2, 3, 4) ein. Einige wenige dieser Elementwasserstoffe sind nur in Form so genannter Addukte bekannt, wie L · EH (L steht für einen Liganden).
Die Folgende Abbildung bietet eine Übersicht über wichtige Grundreaktionen des Wasserstoffs. Auf genaue Reaktionsbedingungen und Stöchiometrie ist hier nicht geachtet.

Wasserstoff kann in Verbindungen sowohl positive als auch negative Ladungsanteile tragen. Das ist abhängig davon, ob der Bindungspartner eine höhere oder eine niedrigere Elektronegativität als Wasserstoff (2,2) besitzt. Zwischen den beiden Verbindungstypen lässt sich im Periodensystem keine scharfe Grenze ziehen, da zum Beispiel das Säure-Base-Verhalten mit berücksichtigt werden muss. Eine mehr oder weniger willkürliche Betrachtung besagt, dass in den Wasserstoffverbindungen der Elemente Bor, Silicium, Germanium, Zinn und Blei sowie allen links davon der Wasserstoff negativ polarisiert ist, in Verbindungen mit Kohlenstoff, Phosphor, Arsen, Antimon, Bismut und allen Elementen rechts davon positiv. Entsprechend lässt sich bei Monosilan (SiH) die Oxidationszahl für Silicium auf +4 (Wasserstoff dementsprechend −1), in Methan (CH) für Kohlenstoff auf −4 (Wasserstoff +1) festlegen.

Zur Darstellung von Wasserstoffverbindungen EH werden hauptsächlich drei verschiedene Verfahren genutzt:

In Verbindung mit Metallen kann Wasserstoff jeweils ein Elektron aufnehmen, so dass negativ geladene Wasserstoffionen (Hydridionen, H) entstehen, die mit Metallkationen Salze bilden. Diese Verbindungen werden Hydride genannt. Salzartige Elementwasserstoffe sind von den Alkali- und, mit Ausnahme von Beryllium, den Erdalkalimetallen bekannt. Außerdem zählt man die Dihydride des Europiums und Ytterbiums (EuH und YbH) dazu.

Metallhydride reagieren sehr heftig mit Wasser unter Freisetzung von molekularem Wasserstoff (H) und können sich an der Luft selbst entzünden, wobei sich Wasser und das Metalloxid bilden. In der Mehrzahl sind sie aber nicht explosiv. Minerale, die (an Sauerstoff gebundenen) Wasserstoff enthalten, sind Hydrate oder Hydroxide.

In metallartigen Wasserstoffverbindungen – mit wenigen Ausnahmen sind das die Übergangsmetallhydride – ist atomarer Wasserstoff in der entsprechenden Metallstruktur eingelagert. Man spricht in diesem Fall auch von Wasserstoff-Einlagerungsverbindungen, obwohl sich bei der Aufnahme des Wasserstoffs die Struktur des Metalls ändert (was eigentlich nicht der Definition für Einlagerungsverbindungen entspricht). Das Element besetzt die oktaedrischen und tetraedrischen Lücken in den kubisch- bzw. hexagonal-dichtesten Metallatompackungen.

Die Löslichkeit von Wasserstoff steigt mit zunehmender Temperatur. Man findet jedoch selbst bei Temperaturen über 500 °C selten mehr als 10 Atomprozente Wasserstoff im betreffenden Metall. Am meisten Wasserstoff können die Elemente Vanadium, Niob und Tantal aufnehmen. Bei Raumtemperatur sind folgende Stöchiometrien zu beobachten: VH, NbH und TaH. Ab 200 °C findet man bei diesen Metallen eine 1:1-Stöchiometrie (MH) vor. Das kubisch-raumzentrierte Kristallgitter bleibt dabei unangetastet.

Verbindungen, bei denen Wasserstoff der elektropositivere Partner ist, haben einen hohen kovalenten Anteil. Als Beispiele seien Fluorwasserstoff (HF) oder Chlorwasserstoff (HCl) genannt. In Wasser reagieren diese Stoffe als Säuren, da der Wasserstoff sofort als Proton (H-Ion) von umgebenden Wassermolekülen abgespalten werden kann. Isolierte H-Ionen verbinden sich in wässriger Lösung sofort mit Wassermolekülen zu HO-Ionen; dieses Ion ist verantwortlich für die saure Eigenschaft von wässrigen Chlorwasserstofflösungen.

Die kovalenten Wasserstoffverbindungen der Elemente der IV. bis VII. Hauptgruppe des Periodensystems sowie Borwasserstoffe sind Säuren nach der Definition von Johannes Nicolaus Brønsted, geben also Protonen an andere Verbindungen ab.

Die Säurestärke der Verbindungen nimmt dabei in den Hauptgruppen von oben nach unten und in den Perioden von links nach rechts zu. Ebenso steigt sie mit der Zahl der Element-Element-Bindungen bei Wasserstoffverbindungen eines bestimmten Elements. So ist zum Beispiel Wasser (HO) eine schwächere Säure als Wasserstoffperoxid (HO), Ethan (CH) in der Säurestärke schwächer als Ethen (CH) und Ethin (CH).

Umgekehrt können kovalente Elementwasserstoffe als Basen fungieren. Wasserstoffverbindungen der Elemente aus Hauptgruppe V bis VII können Protonen aufnehmen, da sie über freie Elektronenpaare verfügen.

Ursache für die Acidität oder Basizität einer wässrigen Lösung ist die Stoffmengenkonzentration an Protonen (H-Ionen). Den negativen dekadischen Logarithmus dieser Konzentration nennt man pH-Wert. Zum Beispiel bedeutet eine Konzentration von 0,001 mol H-Ionen pro Liter Wasser „pH 3,0“. Dieses Beispiel trifft auf eine Säure zu. Wasser ohne jeden Zusatz hat bei Normalbedingungen den pH 7, Basen haben pH-Werte bis 14.

Wasserstoffoxide (auch Hydrogeniumoxide) sind Verbindungen, die nur aus Wasserstoff und Sauerstoff bestehen, von größter Wichtigkeit ist das Wasser (Wasserstoffoxid); von technischer Bedeutung ist daneben Wasserstoffperoxid, früher Wasserstoffsuperoxid genannt. Ein weiteres, aber selteneres Oxid ist das Dihydrogentrioxid.

Von außerordentlicher Bedeutung für alles Leben auf der Erde sind auch Alkohole und Saccharide sowie Carbonsäuren, die (nur) Wasserstoff, Sauerstoff und Kohlenstoff enthalten.

Wasserstoff bildet mit Kohlenstoff die kovalenten Kohlenwasserstoffe, deren Studium sich die Kohlenwasserstoffchemie verschrieben hat.

Chemie

Technik

Bedeutung



</doc>
<doc id="5549" url="https://de.wikipedia.org/wiki?curid=5549" title="Watt (Einheit)">
Watt (Einheit)

Das Watt ist die im internationalen Einheitensystem (SI) für die Leistung (Energieumsatz pro Zeitspanne) verwendete Maßeinheit. Sie wurde nach dem schottischen Wissenschaftler und Ingenieur James Watt benannt. Als Einheitenzeichen wird der Großbuchstabe „W“ verwendet.

Das Watt ist eine abgeleitete Einheit. Sie lässt sich aus den Basiseinheiten kg, m und s zusammensetzen:

Die Einheit ist nach James Watt benannt, der für seine Verbesserung des Wirkungsgrades von Dampfmaschinen bekannt ist. Sie wurde 1882 von der British Science Association vorgeschlagen und auf ihrem zweiten Kongress im Jahr 1889 als Einheit für Leistung anerkannt. Gegenüber der vorher üblichen Einheit „ampere.volt“ hatte die neue Einheit einen deutlicheren Bezug zur Mechanik, sodass sie bei Elektromotoren auch als Ersatz für die Einheit PS (Pferdestärke) akzeptiert wurde. 
Im Jahr 1948 wurde die Einheit dann nochmals durch die neunte Generalkonferenz für Maß und Gewicht definiert, und 1960 in das Internationale Einheitensystem aufgenommen.

Ein Watt ist gleich der Leistung, um

Wobei zu beachten ist, dass die unterschiedlichen Leistungsarten ebenso wie Energieformen nicht immer direkt ineinander überführbar sind.

Die Einheit Watt ist mit verschiedenen dezimalen Vielfachen (auch SI-Präfixen) in Verwendung, beispielsweise: 




</doc>
<doc id="5550" url="https://de.wikipedia.org/wiki?curid=5550" title="Weber (Einheit)">
Weber (Einheit)

Das Weber (auch: die Voltsekunde) ist die Maßeinheit des magnetischen Flusses. Benannt wurde sie nach Wilhelm Eduard Weber.

Im veralteten CGS-System ist Mx (Maxwell) die Einheit des magnetischen Flusses.


</doc>
<doc id="5553" url="https://de.wikipedia.org/wiki?curid=5553" title="Wiki">
Wiki

Ein Wiki (hawaiisch für „schnell“), seltener auch "WikiWiki" oder "WikiWeb" genannt, ist eine Website, deren Inhalte von den Besuchern nicht nur gelesen, sondern auch direkt im Webbrowser geändert werden können (Web-2.0-Anwendung). Das Ziel ist häufig, Erfahrung und Wissen gemeinschaftlich zu sammeln (kollektive Intelligenz) und in für die Zielgruppe verständlicher Form zu dokumentieren. Die Autoren erarbeiten hierzu gemeinschaftlich Texte, die ggf. durch Fotos oder andere Medien ergänzt werden (Kollaboratives Schreiben, E-Collaboration). Ermöglicht wird dies durch ein vereinfachtes Content-Management-System, die sogenannte "Wiki-Software" oder "Wiki-Engine". Wiki-Seiten werden meist in Form von Wikitext gespeichert. Das bekannteste Wiki ist die Online-Enzyklopädie "Wikipedia", welche die Wiki-Software "MediaWiki" einsetzt. Zudem nutzen auch viele Unternehmen Wikis als Teil des Wissensmanagementsystems in ihrem Intranet (standortübergreifend), siehe Enterprise Wiki.
Als wesentlicher Unterschied zu anderen Content-Management-Systemen (CMS) bietet Wiki-Software weniger Gestaltungsmöglichkeiten für Layout und Design der Webseiten. Primäre Funktion ist hingegen ein Bearbeitungsmodus für jede Wiki-Seite, der es auch einem Neuling erlaubt, ohne große Einarbeitung Text und Inhalt der Seite zu ändern. Hierzu wird die Wiki-Seite im Bearbeitungsmodus häufig in einem WYSIWYG-Editor geöffnet oder in einer einfach erlernbaren, vereinfachten Auszeichnungssprache (beispielsweise Wikitext) angezeigt (oder wahlweise beides). Beide Varianten ermöglichen in der Regel insbesondere Schriftauszeichnung, Verlinkung, Listen und Aufzählungen sowie teils auch die Möglichkeit von Transklusionen für wiederholende Inhalte.

Im Unterschied zu den Content-Management-Systemen (CMS) mit ihren teils genau geregelten Arbeitsabläufen (engl. "workflows") etwa in Redaktionssystemen, setzen Wikis auf die Philosophie des offenen Zugriffs: idealerweise kann jeder Nutzer jeden Eintrag lesen und bearbeiten. Wikis gelten als gegenüber einem klassischen CMS dann im Vorteil, wenn eine hohe Anzahl an Nutzern Informationen einstellt, so dass im Medium eine kritische Masse erreicht wird und es zu einem „Selbstläufer“ wird.
Es gibt aber auch Wiki-Systeme, die eine Zugriffssteuerung (etwa via Access Control List) für bestimmte Seiten und Benutzergruppen (z. B. Abteilungen eines Unternehmens) erlauben.

Eine wesentliche Funktion der meisten Wiki-Produkte ist die Versionsverwaltung, die es den Nutzern im Fall von – durch den offenen Zugriff kaum vermeidlichen – Fehlern oder Vandalismus erlaubt, eine frühere Version einer Seite schnell wiederherzustellen.

Wie bei Hypertexten üblich, sind die einzelnen Seiten eines Wikis durch Querverweise (Hyperlinks) miteinander verbunden; dabei dient der Titel einer Seite meist auch als Linkadresse. Links auf nichtexistente Seiten werden dann nicht als Fehler angezeigt, sondern es erscheint ein Formular, um die neue Seite anzulegen. Eine Vernetzung mit anderen populären Wiki-Diensten wird teils durch sog. "InterWiki"-Verweise ermöglicht.

Die meisten Systeme sind als freie Software veröffentlicht, oft unter einer Version der gebräuchlichen GNU General Public License (GPL). Viele Wiki-Software Systeme sind modular aufgebaut und bieten eine eigene Programmierschnittstelle, die dem Benutzer ermöglicht, eigene Erweiterungen zu schreiben, ohne den gesamten Quellcode zu kennen.

Ein Wiki kann öffentlich zugänglich im World Wide Web verfügbar sein, in lokalen Netzwerken nur für eine bestimmte Nutzergruppe (z. B. als Intranet) eingesetzt werden oder auch auf einem einzelnen Rechner zur persönlichen Informationsorganisation verwendet werden, etwa in Form eines Desktop-Wikis. Beispiele für Desktop-Wiki-Software sind "AcroWiki" für Palm OS, "Tomboy" und "Zim" für Linux, "VoodooPad" für macOS, "Gluebox" (plattformunabhängig), "ConnectedText" und "WikidPad" für Windows, sowie TiddlyWiki, das client-seitig (ohne Server) als JavaScript in jedem Browser läuft.

Die Entwicklung des Wikis als Medium ist eng mit dem World Wide Web verbunden. Es wurde erst durch dieses zu einem Erfolgsmodell, auch wenn seine Vorläufer bis in die 1970er Jahre zurückgehen. Die Änderbarkeit der Seiten durch jedermann setzt zudem eine ursprüngliche Idee des World Wide Web konsequent um.

In der Softwareentwicklung wurde der immense Nutzen von Wikis für ein effektives Wissensmanagement in einem kollaborativen Umfeld zuerst erkannt, was vermutlich auf die Technikaffinität der Mitarbeiter zurückzuführen ist.
Ein Wiki-System kann in der Softwareentwicklung insbesondere der Erstellung von Dokumentationen, zur Verwaltung von Softwarefehlern oder der Koordination unter den Software-Entwicklern dienen.
So wurden die ersten Wikis Mitte der 1990er Jahre von Software-Designern zur Produktverwaltung in IT-Projekten entwickelt. Insbesondere in Entwicklungsprojekten von Open-Source-Software – etwa bei Apache oder OpenOffice.org –, bei denen Menschen, die über Kontinente verstreut sind, zusammenarbeiten, fällt den Wikis eine Schlüsselrolle zu. Heute kommen Wikis in einer Vielzahl von Anwendungen zum Einsatz, bei denen inhaltliche Flexibilität mehr zählt als ein repräsentatives Layout. Dazu gehören Dokumentationen in Wirtschaft, Wissenschaft und Kultur.

Einer der ersten Vorläufer des Wikis war das in der Carnegie-Mellon University 1972 entwickelte ZOG-Datenbanksystem, das für mehrere Nutzer ausgelegt war und die Daten in strukturierten Textrahmen darstellte, verbunden waren sie durch Hyperlinks. Dieses System wurde 1981 von Donald McCracken und Robert Akscyn zum Knowledge Management System (KMS) erweitert, bei dem Änderungen an den Datenblättern im gesamten Netzwerk sofort sichtbar wurden. In diesem System waren bereits Grafiken und Bilder integrierbar, auch sie konnten mit Hyperlinks versehen werden.

Ebenfalls auf ZOG basierte der Document Examiner von Janet Walker, der ab 1985 zur Darstellung von Computer-Anleitungen verwendet wurde. Dieses Hypertextsystem, bei dem die Texte in einem scrollbaren Bildschirmfenster dargestellt wurden, wurde im selben Jahr von Xerox zum Note Cards-System weiterentwickelt, aus dem schließlich 1987 das HyperCard-System von Apple (zunächst unter dem Namen "WildCard") hervorging. Dieses System beeinflusste Ward Cunningham bei seinem WikiWikiWeb entscheidend, da es beispielsweise bereits verschiedene Typen von "Cards" ermöglichte, von denen eine Gruppe für Benutzer, eine für Projekte und eine für die Ideen selbst stehen konnte. Ebenfalls war in Cunninghams Weiterentwicklung des Systems das Anlegen neuer Karten durch das Klicken auf Links auf nichtvorhandene Inhalte möglich.

Tim Berners-Lee, der ab 1989 entscheidende Beiträge zu HTML und zum World Wide Web leistete, hatte zu Beginn seiner Arbeiten an Hypertextsystemen ähnliche Ideen verfolgt, da seiner Meinung nach dieses Instrument vor allem zur kollaborativen Erstellung von Texten in der Wissenschaftsgemeinde verwendet werden sollte. Konsequenterweise war Berners-Lees erster Webbrowser WorldWideWeb (1990/1991) sowohl zum Darstellen als auch zum Bearbeiten von Websites geeignet. In historischer Perspektive beschreibt er seine Ideen in seinem Buch „Weaving The Web“ (deutsche Lehnübertragung „Der Web-Report“). Dennoch setzte sich im Web zunächst die nichtkollaborative Erstellung von Websites durch, die durch restriktive Benutzerrechte für die Seiten auf den Servern erreicht wurde.

Das erste im Web gehostete wirkliche Wiki, "WikiWikiWeb", wurde vom US-amerikanischen Softwareautor Ward Cunningham als Wissensverwaltungswerkzeug im Rahmen der Entwurfsmuster-Theorie 1994 auf Basis der HyperCard-Systeme konzipiert. Es befasste sich mit Softwaredesign im Rahmen der objektorientierten Programmierung. Am 25. März 1995 wurde es über das Internet der Öffentlichkeit verfügbar gemacht.
Den Namen wählte Cunningham, da er bei der Ankunft am Flughafen auf Oʻahu die Bezeichnung "Wiki Wiki" für den dortigen Schnellbus kennengelernt hatte. Dabei übernahm er die Verdoppelung, die im Hawaiischen für eine Steigerung („sehr schnell“) steht. Cunningham betrachtet "Wiki" weiterhin als eine Abkürzung für den eigentlichen Namen "WikiWikiWeb".

Cunninghams Konzept stieß in der Software-Entwicklergemeinde auf reges Interesse, das schnell anwuchs. So umfassten die Seiten des WikiWikiWeb im Dezember 1995 bereits 2,4 MB Speicherplatz, Ende 1997 waren es 10 MB und Ende 2000 62 MB.

Bereits kurze Zeit nach der Inbetriebnahme des WikiWikiWeb entstanden erste Klone der Software. Wikis entwickelten sich schnell zu einem beliebten Instrument in der Szene rund um die freie Software, in der sie als Instrument zur Unterstützung der Kommunikation und der Ideenorganisation unter den Entwicklern genutzt wurden. Auch Cunningham unterstützte diese Entwicklung, indem er einen eigenen Klon seiner Software, "Wiki Base" genannt, veröffentlichte. Dennoch kam es bald zu Spannungen zwischen WikiWikiWeb und einigen Klonen, da Cunningham erwartete, dass die Nutzer von Wiki Base eigene Verbesserungen in den Quellcode seines eigenen Wikis einfügen, was aber nur selten geschah.

Einer der bedeutendsten Klone von Wiki Base war das 1997 von Peter Merel geschriebene "CvWiki", aus dem 1999 das UseModWiki hervorging, das bis heute im MeatballWiki, einem der populärsten Software-Wikis verwendet wird. UseModWiki war auch in der Anfangszeit der Wikipedia deren Wiki-Engine, bis es 2002 von MediaWiki abgelöst wurde. 1998 wurde mit TWiki die erste Wiki-Software auf Basis von Textdateien veröffentlicht, dieses System eignet sich vor allem für kleinere Wikis (z. B. Desktop- und Firmenwikis), in denen so eine höhere Performance erreicht werden kann. 1999 erschien mit PhpWiki die erste Wiki-Engine auf Basis der Programmiersprache PHP.

Bis 2001 waren Wikis als Medium weitgehend auf die Software-Entwicklerszene beschränkt, weshalb das öffentliche Interesse an ihnen außerhalb dieser spezialisierten Szene begrenzt war. Dennoch wurden mit anderen Softwarekonzepten bereits kollaborative Webportale mit ähnlichen Zielen, wie Everything2, entwickelt. Das erste echte Wiki-Portal, das zu einem anderen Thema als Software entwickelt wurde, war der Online-Reiseführer World66, gegründet im Jahr 1999 von einem niederländischen Unternehmen, das als eines der ersten das Konzept der freien Inhalte in ein profitables Geschäftsmodell zu integrieren versuchte.

Zwischen 1998 und 2000 kam es im WikiWikiWeb selbst zu Spannungen, als sich die Beiträge immer weiter vom ursprünglichen Thema des Wikis entfernten. Es kam so zu einer Konfrontation zwischen zwei Gruppen: Während die "WikiReductionists" den Schwerpunkt des Wikis weiterhin auf der objektorientierten Softwareprogrammierung sehen wollten, sollte nach der Meinung der "WikiConstructionists" auch Platz für andere, allgemeinere Themen im WikiWikiWeb sein, insbesondere für solche, die das Wiki-Konzept als solches betrafen (sogenannte "WikiOnWiki"-Themen). Dies führte im Jahr 2000 zur Spaltung und zur Gründung des MeatballWiki, das sich neben der Diskussion der Wiki-Idee selbst auch mit allgemeineren Themen wie dem Urheberrecht oder der Cyberpunk-Bewegung befasste. Das MeatballWiki und einige andere in diesem Streit entstandenen Websites wurden als "SisterSites" bezeichnet und vom WikiWikiWeb aus direkt verlinkt. Aus diesem Wiki stammen zahlreiche Ideen, die die Popularisierung der Wiki-Idee fördern sollten, wie der TourBusStop, eine Tour durch verschiedene Wikis, der WikiNode als Knotenpunkt eines Wikis und der WikiIndex als Datenbank möglichst aller Wikis.

Die Popularisierung des Wiki-Konzeptes geht auf die Online-Enzyklopädie "Wikipedia" zurück. Zwischen 1999 und 2000 hatte das US-amerikanische Unternehmen "Bomis" die Idee einer im Internet erstellten Enzyklopädie entwickelt. Dem Nupedia-Projekt, das 2000 gestartet wurde, war jedoch zunächst kein Erfolg beschieden, da der Prozess der Erstellung der Einträge auf dem Peer-Review-Prozess fußte und damit sehr langwierig war. Gegen Ende des Jahres wurde daher von den Bomis-Gründern Jimmy Wales und Larry Sanger eine Wiki-Erweiterung entwickelt, die am 15. Januar 2001 auf der separaten Domain "wikipedia.com" online ging und sich noch im Laufe des Jahres, besonders nach einer Meldung im Onlinemagazin Slashdot, zu einem großen Erfolg entwickelte. Im selben Jahr wurden andere Sprachversionen gestartet. Bis 2005 wuchs die Zahl der Seiten auf über eine Million an und Wikipedia wurde zu einer der meistbesuchten Webseiten überhaupt.

Um die wachsenden Ansprüche der Wikipedia erfüllen zu können, wurde 2002 die MediaWiki-Software entwickelt. Diese führte als Neuerung ein, dass die Links erstmals freien Text erhalten konnten, davor war die sogenannte CamelCase-Schreibweise üblich, in der die Wörter nicht durch Leerzeichen getrennt wurden. MediaWiki war besonders auf Skalierbarkeit angelegt, um die schnell steigenden Nutzerzahlen bewältigen zu können.

In den Folgejahren wurden, zum Teil aus der Wikipedia-, zum Teil aber auch aus der Meatball-Community heraus, neue Webportale auf Wiki-Basis gegründet. Darunter fiel die Enciclopedia Libre, eine bereits 2002 gegründete Abspaltung der spanischsprachigen Wikipedia, Susning.nu, eine schwedischsprachige Mischung aus Enzyklopädie und Webforum, der 2003 gegründete Online-Reiseführer Wikitravel, das SourceWatch-Projekt zur Dokumentation von Lobby-Organisationen sowie die als Schwesterprojekte der Wikipedia bezeichneten Wikis "Wikinews", "Wiktionary", "Wikibooks", "Wikisource", "Wikiquote" und "Wikispecies". Das Wiki-Konzept wurde so an verschiedene Arten von Texten angepasst, mit unterschiedlichem Erfolg. Eine erste nennenswerte Abwandlung des Wikipedia-Konzeptes wurde ab 2003 mit Wikinfo entwickelt, in dem verschiedene Sichtweisen auf die verschiedenen Themen zugelassen waren, der Erfolg blieb aber hinter dem der Wikipedia deutlich zurück.

Kommerzielle Wikifarmen, die ihre Dienste oft kostenlos anbieten, führten dazu, dass es nach und nach nahezu zu jedem möglichen Thema ein eigenes Wiki gibt. Ein besonders großer Erfolg wurden die sogenannten Fanwikis, die – neben der lexikalischen Abhandlung – eine neue Form der kollaborativ erstellten Fan-Fiction ermöglichten. Insbesondere im Science-Fiction- (z. B. Memory Alpha), Fantasy- und Comicbereich konnten einige Wikis hohe Artikel- und Teilnehmerzahlen erreichen. Auch im Bereich Humor haben sich Wikis wie Uncyclopedia, Stupidedia und Kamelopedia etabliert.

Der Erfolg von Wikipedia führte zu verschiedenen Bestrebungen, das Wiki-Konzept zu verbessern. Im Bereich der als Enzyklopädie konzipierten Wikis entwickelten Ulrich Fuchs und Larry Sanger unabhängig voneinander die Projekte Wikiweise und Citizendium, bei denen das Wiki-Konzept eingeschränkt wird und stattdessen durch ein näher an der traditionellen redaktionellen Arbeitsweise orientiertes System eine Qualitätssteigerung erzielt werden soll. So hat bei Citizendium jeder Artikel einen eigenen verantwortlichen Betreuer, der mit Klarnamen bekannt ist. Beiden Projekten blieb jedoch bisher ein durchschlagender Erfolg verwehrt.

Eine weitere Entwicklung ist die Erweiterung von traditionellen Web-Portalen verschiedener Art durch Wiki-Funktionen. So kann im Wissensportal Google Knol jeder Interessierte Texte einstellen und bestimmen, ob er seine Inhalte zur kollaborativen Bearbeitung nach Wiki-Art freigibt oder nicht. Auf einem ähnlichen Konzept basiert das wissenschaftliche Wiki Scholarpedia, das auf wenige Spezialthemen beschränkt ist und die Teilnahmemöglichkeiten Fachfremder stark einschränkt.

Auf Wiki-Basis wurden weiterhin etwa seit 2005 computergenerierte Datenbanken erstellt, die von den Web-Benutzern bearbeitet und so verbessert werden können. Diese Wikis sind meist stark strukturiert und nutzen in hohem Maße Vorlagen. Bekannte Vertreter dieser Wiki-Form sind das Web-Verzeichnis AboutUs.org, die Open Directory Project-Erweiterung Chainki und die proprietäre Musikdatenbank CDWiki. Selbst zur Vermarktung von Internetwerbung wurden Wikis verwendet, wie bei "WikiFox" (inzwischen eingestellt) und "ShoppiWiki".

Durch Softwareerweiterungen wurde das Wiki-Konzept um die Darstellung von ab 2005 populären Inhalten wie Web-Videos erweitert sowie auf zukünftig erwartete Internetphänomene wie das Semantische Web vorbereitet.

Im März 2007 wurde das Wort "wiki" in das Oxford English Dictionary aufgenommen.

Eine Reihe von Wikis, Regiowikis genannt, wurden speziell für einzelne Städte und deren Themenkreise eingerichtet. Das größte der Welt ist das Stadtwiki Karlsruhe.

Durch den Erfolg von Wikipedia beflügelt, haben viele Unternehmen begonnen, Unternehmenswikis aufzubauen, um das Wissen ihrer Mitarbeiter unternehmensintern zu sammeln und transparent zu machen (Wissensmanagement). Hierbei ist gegebenenfalls das Engagement der Mitarbeiter unverzichtbar. Der finanzielle Aufwand ist dagegen meist niedriger als bei herkömmlichen Systemen der Wissenskonservierung. Erfolgversprechend ist der Einsatz von Wikis tendenziell eher bei flachen Hierarchien und in einer möglichst offenen Unternehmenskultur.

Im Jahr 2008 nutzten oder testeten beispielsweise 41 % der finnischen Top-50-Unternehmen Wikis, weitere 18 % standen einem Wiki-Einsatz offen gegenüber.

Laut einer Studie von Forrester Research wird sich der Einsatz von Unternehmenswikis im Rahmen von Enterprise 2.0 von 2007 bis 2013 in etwa verzehnfachen. Die Unternehmensberatung Gartner schätzte, dass 2009 ungefähr die Hälfte der Unternehmen ein Wiki installiert haben.

Grundsätzlich können unternehmensinterne Wikis in zwei Gruppen eingeteilt werden:

Einige Wikis kombinieren beide Typen und ermöglichen die Einrichtung von sogenannten Spaces, um Projekte voneinander inhaltlich und benutzerrechtlich zu trennen.

Mittlerweile nutzen viele Schulen und Hochschulen eigene Wikis. Im Jahre 2010 gab es in Deutschland WikiWebs an mehr als 34 % aller Hochschulen.

Die weltweit größte technische Berufsgesellschaft IEEE mit den Schwerpunktsbereichen Elektrotechnik und Informatik gründete 2008 eine Unterorganisation mit dem Namen "IEEE Global History Network". Es entstand eine wikibasierte, englischsprachige, frei zugängliche Online-Datenbank mit historischen Inhalten der Technikgeschichte dieser Fachbereiche. Dazu gehören Meilensteine der Entwicklung wie auch mündliche und schriftliche persönliche Erfahrungsberichte. 2015 wurde das IEEE Global History Network in die breiter abgestützte Organisation Engineering and Technology History Wiki der wichtigsten technischen US-Berufsgesellschaften eingebracht und wird dort weitergeführt.

In Österreich wurde nach dem Beschluss des Regierungsprogramms der großen Koalition zwischen SPÖ und ÖVP im Jahre 2007 von der Parlamentsfraktion der Grünen eine Webseite zur zivilgesellschaftlichen Neuformulierung des Regierungsprogrammes eingesetzt. Die Partei verspricht die darin formulierten Anregungen zu berücksichtigen, sollte sie in Regierungsverhandlungen eintreten.
Am 11. Juli 2009 wurde in Portugal vom Institut für portugiesische Demokratie (IDP) das Projekt "Constituição 2.0" lanciert. Dabei soll, nach dem Vorbild der Wikipedia, eine neue, mit dem Wiki-System kollektiv erstellte, portugiesische Verfassung entstehen. In einem Artikel in der israelischen Tageszeitung "Haaretz" wird die Idee als Möglichkeit zur Schaffung einer Verfassung Israels aufgegriffen.

Wiki-Software kann dabei helfen, Wissen innerhalb einer Organisation zu strukturieren und dokumentieren und dadurch leichter verfügbar und nutzbar zu machen. Wissenstransfer wird auf diese Weise unabhängiger von unmittelbarem zwischenmenschlichen Kontakt. Ebenso kann ein Wiki dazu dienen, Organisationsstrukturen bis hin zu informellen Netzwerken und Experten als Ansprechpartnern transparent zu machen. Die Wahl der geeigneten Wiki-Software hängt dabei sowohl von der Struktur der Organisation als auch vom konkreten Einsatzzweck ab. Zu den Anforderungen an eine Wiki-Software können insbesondere die folgenden zählen.



Unter Umständen ist es notwendig vorzusehen, dass zu bestimmten Seiten nur bestimmten Nutzergruppen lesenden Zugang haben. Bestimmte Nutzer wird man zudem erweiterte Rechte zuteilen können müssen,
um z. B. die Rolle eines Administrators übernehmen zu können.

Zur langfristigen Sicherung der Investition trägt eine weite Verbreitung der Wiki-Software bei.
Sie kann daran abgelesen werden, wie viele Referenzen es gibt und ob es eine aktive Nutzer-Community gibt.





</doc>
<doc id="5557" url="https://de.wikipedia.org/wiki?curid=5557" title="Wirbeltiere">
Wirbeltiere

Wirbeltiere (Vertebrata) sind Chordatiere mit einer Wirbelsäule. Zu diesem Unterstamm gehören fünf traditionell als Klassen geführte Großgruppen: Säugetiere, Vögel, Reptilien, Amphibien sowie Fische (Knochen- und Knorpelfische), als urtümliche Vertreter zudem die Rundmäuler. Ihnen wird die informelle Gruppe der Wirbellosen oder Invertebrata (das sind alle übrigen Tiere) gegenübergestellt, die keine Wirbelsäule haben.

Von vielen Zoologen wird heute der Begriff Schädeltiere (Craniota) für dieses Taxon bevorzugt. Diese Auffassung berücksichtigt, dass die Rundmäuler, wie auch einige andere Wirbeltiere, als Achsenskelett keine Wirbelsäule, sondern eine Chorda dorsalis haben. Doch allen Wirbeltieren gemein ist ein verknöcherter oder knorpeliger Schädel; sein Vorhandensein gehört somit zu den gemeinsam abgeleiteten Merkmalen (Synapomorphien) dieser Chordaten-Gruppe.

Die Monophylie der Wirbeltiere wird durch eine Reihe gemeinsamer abgeleiteter (neuer) Grundplanmerkmale (Synapomorphien) unterstützt:


Die Wirbeltiere haben in der konventionellen biologischen Systematik den Rang eines Unterstamms. Zusammen mit den Manteltieren und den artenarmen Schädellosen bilden sie den Stamm der Chordatiere (Chordata).

Nach der "Notochordata-Urochordata-Hypothese" gelten die Wirbeltiere als Schwestergruppe der Schädellosen (Acranier/Cephalochordata), daher werden sie oft auch als „Schädeltiere“ (Craniota oder Craniata) bezeichnet. Die alternative, später erschienene "Olfactores-Cephalochordata-Hypothese" besagt hingegen, dass die Manteltiere (Urochordata/Tunicata) die Schwestergruppe der Wirbeltiere ist. Welche Hypothese stimmt, ist bis heute noch nicht klar.

Früher wurden die Wirbeltiere nach dem Kriterium unterteilt, ob ein Kiefer vorhanden ist oder nicht. Dieser Ansatz ist überholt: Den Kiefermäulern (Kiefertieren) werden heute nicht mehr die Kieferlosen (Agnatha) gegenübergestellt, sondern die Rundmäuler (Cyclostomata).

Die innere Systematik der Wirbeltiere bleibt jedoch umstritten, insbesondere die Frage, ob ein Schwestergruppenverhältnis zwischen Kiefermäulern und Neunaugen besteht oder zwischen Schleimaalen und Neunaugen:

Die folgende Darstellung berücksichtigt auch ausgestorbene Gruppen. Die klassischen Großgruppen sind fett hervorgehoben.

Wirbeltiere (Vertebrata): über 70.300 Arten

Die ausgestorbenen, oft stark gepanzerten, kieferlosen Taxa werden als Ostracodermi zusammengefasst, die gepanzerten, kiefertragenden als Placodermi. Beide Gruppen sind jedoch keine monophyletischen Taxa, ebenso wenig die Acanthodii, die teilweise basal zu Knorpelfischen oder zu den Knochenfischen stehen.

Die Zugehörigkeit der ausgestorbenen Conodonten zu den Vertebrata ist umstritten.

Wirbeltiere sind weltweit verbreitet. Sie leben auf allen Kontinenten einschließlich der Antarktis, im Meer bis in die Tiefsee, in Süßgewässern, und an Land in allen Biotopen einschließlich der Hochgebirge. Vögel und Fledermäuse verfügen über die Fähigkeit zum aktiven Flug, was die Ausbreitung begünstigt. Die Artenvielfalt ist in den tropischen Regenwäldern am höchsten (Amazonasgebiet, Gebiete in Afrika und Südostasien).

Heute gibt es über 70.000 Wirbeltierarten, mehr als die Hälfte davon sind Fische. Dies sind nach Schätzungen etwa ein Prozent aller Wirbeltierarten, die im Verlauf der Evolution erschienen sind. Die Zahl liegt deutlich höher als in älteren Quellen angegeben wurde, z. B. gab die IUCN für 2004 noch 57.739 bekannte Wirbeltierarten an. Jedes Jahr werden mehrere hundert Wirbeltierarten neu entdeckt, so sind seit 1982 etwa 1246 neue Säugetierarten, seit 1996 etwa 7407 neue Fischarten, seit 2004 etwa 2010 Amphibienarten und seit 2008 etwa 1716 Reptilienarten bis zum Jahr 2016 neu beschrieben worden. Daneben sind weltweit bisher mehrere zehntausend fossile Arten entdeckt worden.

Wirbeltiere sind insgesamt betrachtet deutlich größer als wirbellose Tiere. Die meisten wirbellosen Tiere werden nur wenige Zentimeter groß, sehr häufig werden die Größen in Millimeter angegeben. Ausnahmen unter den Wirbellosen sind nur die Kopffüßer, einige Krebstiere (Hummer, Langusten) und Riesenmuscheln. Wirbeltiere von wenigen Zentimetern Größe gehören dagegen immer zu den kleinsten Arten ihres Taxons.

Die kleinsten im Wasser lebenden Wirbeltiere sind einige Grundeln (z. B. "Schindleria brevipinguis") und Karpfenfische (z. B. "Paedocypris progenetica" mit einer Länge von 7,9 mm beim Weibchen und 10 mm beim Männchen), kleinstes Landwirbeltier der Frosch "Paedophryne amauensis" (mit einer Länge von 7,7 mm). Die Etruskerspitzmaus ("Suncus etruscus") mit einer Rumpflänge von 2 cm und einem Gewicht von 1 g und die Hummelfledermaus ("Craseonycteris thonglongyai") mit einem Gewicht von 1,5 bis 3 g gelten als die kleinsten Säugetiere.

Das größte Wirbeltier ist der Blauwal ("Balaenoptera musculus") mit einer Maximallänge von 30 Metern und einem Maximalgewicht von 200 Tonnen. Das größte rezente an Land lebende Wirbeltier ist der Afrikanische Steppenelefant ("Loxodonta africana") mit einem Maximalgewicht von 7 Tonnen. Die größten ausgestorbenen Wirbeltiere des Festlandes waren die Sauropoden (Sauropoda), eine sehr artenreiche Gruppe der Dinosaurier.

Voraussetzungen für diese Größenzunahme bei den Wirbeltieren waren ihr einzigartiges, aus Knochen und Knorpel bestehendes Innenskelett, die Entwicklung einer sehr leistungsfähigen Muskulatur und das geschlossene Herz-Kreislauf-System.

Einige Wirbeltiere erreichen ein Lebensalter, das weit über das übliche Maß der höheren Tiere hinausgeht. Seit einiger Zeit ist bekannt, dass Grönlandwale mehr als 200 Jahre alt werden können. Neuerdings wurde für den Grönlandhai eine Lebenslänge von über 270 Jahren festgestellt, es gilt als wahrscheinlich, dass die Tiere sogar mehr als 400 Jahre alt werden können.





</doc>
<doc id="5558" url="https://de.wikipedia.org/wiki?curid=5558" title="World Trade Center">
World Trade Center

Das World Trade Center [] () war ein Bürokomplex im Financial District an der Südspitze von Lower Manhattan in New York City, der aus sieben Gebäuden bestand und 1973 eröffnet wurde. Sein Kernstück bildeten die weltbekannten Zwillingstürme ("Twin Towers", WTC 1 und 2). Sie gehörten mit jeweils 110 Stockwerken von insgesamt 417 und 415 Metern Höhe zu den höchsten Gebäuden New Yorks und prägten die Skyline der Stadt.

Infolge der Terroranschläge am 11. September 2001 stürzten die Zwillingstürme sowie das WTC 7 vollständig ein. Dabei starben 2753 Menschen. Das als Hotel dienende 72 Meter hohe WTC 3 wurde von herabstürzenden Trümmern der kollabierenden Zwillingstürme vollständig zerstört. Das WTC 4 mit Sitz der weltgrößten Warenterminbörse, das WTC 5 und das WTC 6 wurden so schwer beschädigt, dass sie später abgerissen wurden.

Auf dem als Ground Zero bekannten Gelände wird seit 2006 ein neues "Welthandelszentrum" errichtet, das aus sechs Wolkenkratzern, einer unterirdischen Shoppingmall und dem National September 11 Memorial and Museum bestehen wird. Teile davon wurden inzwischen eröffnet.

Die Idee eines Welthandelszentrums in New York entstand 1939 zu Beginn des Zweiten Weltkriegs, als die Wirtschaft der USA und der internationale Handel wuchsen. Bei der 1939 New York World’s Fair wurde ein "World Trade Center" eingerichtet, das unter dem Motto „Weltfrieden durch Handel“ stand.

Auf Initiative des Bankiers Winthrop W. Aldrich ermöglichte der Staat New York 1946 durch ein Gesetz den Bau eines WTC. Im selben Jahr wurde die "World Trade Corporation" gegründet. Der New Yorker Gouverneur Thomas E. Dewey berief eine Arbeitsgruppe ein, die das WTC planen sollte. Die Architekten John und Drew Eberson entwarfen einen Plan für 21 Gebäude über einen Zehn-Block-Bereich. Die Kosten dafür wurden auf 150 Millionen Dollar geschätzt. 1949 löste die Legislative des Staates New York die World Trade Corporation auf, und die WTC-Pläne wurden zurückgestellt.

In der Nachkriegszeit verpasste Lower Manhattan den Wirtschaftsaufschwung. Moderne große Schiffe fanden in den südlichen Docks keinen Platz mehr. Containerschiffe beförderten immer mehr Fracht, die im neu erbauten Elizabeth Port in New Jersey gelöscht wurde. Die Gebäude Südmanhattans waren nicht mehr zweckmäßig und verwahrlosten vielfach. Ende der 1950er Jahre setzte sich der Bankier und Unternehmer David Rockefeller, ein Neffe von Aldridge, mit aller Kraft für die Erneuerung dieses Stadtteils ein. Mit dem Bau des One Chase Manhattan Plaza im Financial District belebte er dessen Entwicklung und setzte dies mit seinem Einsatz für das WTC fort. 

1958 gründete Rockefeller dazu die "Downtown-Lower Manhattan Association" (DLMA). Diese beauftragte das Architekturbüro Skidmore, Owings and Merrill (SOM) mit der Planung. 1960 stellte SOM die Pläne öffentlich vor: Danach sollte für 250 Millionen Dollar ein WTC auf einem 13 Acres (53.000 Quadratmeter) großen Grundstück entlang des East River, vom Old Slip zur Fulton Street und zwischen der Water Street und der South Street, errichtet werden. Der Komplex sollte eine 900 Fuß (275 m) lange Halle und ein 70-stöckiges Büro-Hotel-Gebäude, eine internationale Handels-Mart für die Ausstellung von Waren, eine Wertpapierbörse, einen Arcade-Einzelhandel, sowie ein in das herkömmliche Straßennetz integriertes Theater, Geschäfte und Restaurants enthalten. Rockefeller schlug New Yorks Bürgermeister Robert F. Wagner junior, seinem Bruder und damaligen Gouverneur des Staates New York Nelson A. Rockefeller und dem Gouverneur von New Jersey Robert B. Meyner die Port Authority of New York and New Jersey als Träger des Projekts vor, weil sie mit der DLMA und SOM zusammen ausreichende Kreditkapazitäten, bewährte Fachkompetenz und Erfahrung besitze. Er behauptete, das WTC werde dazu beitragen, dass der zunehmende internationale Handel über den Hafen von New York laufe. 

Der Entwurf wurde erweitert und sah nun eine Gesamtgeschossfläche von über 1 Million Quadratmetern in einem 72-stöckigen "World Trade Mart" mit einem Hotel, einem "World Trade Institute", einem Ausstellungsgebäude, einem 30-stöckigen "World-Commerce-Exchange"-Gebäude mit Ämtern und Agenturen, sowie einem 20-stöckigen "Trade-Center-Gateway"-Gebäude mit internationalen Banken und sonstigen unternehmensbezogenen Dienstleistern vor. Als Vorstandsvorsitzender der Chase Manhattan Bank wollte David Rockefeller den Bau der Zwillingstürme finanzieren. Der Direktor der Hafenbehörde Austin J. Tobin strebte das weltweit bedeutendste WTC an. Am 11. März 1961 stimmte die Hafenbehörde dem Projekt zu, dessen Gesamtkosten nun auf 335 Millionen Dollar veranschlagt wurden. 

Gouverneur Robert B. Meyner versagte dem Projekt jedoch die erforderliche Genehmigung, weil er wirtschaftliche Nachteile für New Jersey und den weiteren Niedergang der Hudson & Manhattan Railroad befürchtete. Nach ergebnislosen Verhandlungen mit Meyners schlug Tobin dessen Nachfolger Richard J. Hughes im Dezember 1961 vor, das WTC-Projekt zum Hudson Terminal an der Westseite Manhattans zu verlegen, um so eine Verkehrsanbindung an New Jersey zu gewährleisten. Am 22. Januar 1962 einigten sich die Staaten New York und New Jersey und erlaubten der Hafenbehörde (Port Authority), Südmanhattan neu zu überbauen mit der Auflage, die marode Hudson & Manhattan Railroad zu übernehmen. Die Hochhäuser über dem unterirdischen Hudson Terminal und etwa 164 weitere Gebäude sollten für den Bau des WTC eingerissen werden. Die in diesem Bereich angesiedelten zahlreichen Unternehmen und Einzelhändlern der New Yorker Elektronikindustrie („Radio Row“) sollten zur Umsiedlung gezwungen und finanziell entschädigt werden. 

Als ausführender Architekt wurde Minoru Yamasaki und als Partner (associates) Emery Roth & Sons verpflichtet. 

1962 kaufte die Hafenbehörde das WTC-Baugelände vom Staat New York. Im Juni 1962 gingen etwa 1325 betroffene Geschäftsinhaber und Kleinunternehmer gerichtlich gegen die Enteignung vor und versuchten, die Zwangsumsiedlung mit Protestaktionen zu stoppen. Der Rechtsstreit zwischen Hafenbehörde und lokalen Unternehmern durchlief alle Gerichtsinstanzen. Im April 1963 bestätigte der New York State Court of Appeals die Urteile der Vorinstanzen und sprach der Hafenbehörde das Recht auf Enteignung zu, da der Bau des WTCs einen öffentlichen Zweck erfülle. Am 12. November 1963 lehnte der United States Supreme Court ein Revisionsverfahren dazu ab und bekräftigte, die Hafenbehörde müsse die Unternehmer im Rahmen des bundesstaatlichen Rechts finanziell entschädigen und bei der Umsiedlung helfen. 

Private Stadtsanierer und Mitglieder des "Real Estate Board of New York" fürchteten, der Bau des WTC werde das bestehende Überangebot an freien Büroflächen vergrößern, zuviele Subventionen erzwingen und so den privaten Sektor zu stark benachteiligen. Der Ausschuss „Reasonable World Trade Center“ forderte weniger WTC-Büroflächen und somit deutlich niedrigere Zwillingstürme. Der Organisator Lawrence A. Wien wollte damit den Titel „Höchstes Gebäude der Welt“ für sein Empire State Building bewahren. Im Januar 1964 vereinbarte die Hafenbehörde mit dem Staat New York, WTC-Büros auch an staatliche Behörden zu vermieten. Ab Frühjahr 1964 schloss sie Mietverträge mit Privatunternehmen und Banken für das WTC ab, 1965 folgte ein Mietvertrag mit dem "United States Customs Service". Ab dem 21. März 1966 wurden mehr als 300 Fachgeschäfte für Unterhaltungselektronik auf dem Baugelände abgerissen. Am 3. August 1966 vereinbarte die Hafenbehörde mit der Stadt New York, anstelle von Steuern für den privat vermieteten Teil des WTC eine jährliche Zahlung an die Stadt zu leisten. Weil ein Anstieg dieser Jahrespauschale wegen einer Grundsteuererhöhung absehbar war, stimmte der New Yorker Bürgermeister dem Vertrag letztlich zu.

Am 5. August 1966 wurde der Grundstein für das WTC gelegt. Der Baugrund war nicht naturgegeben, sondern das Ergebnis von Aufschüttungen früherer Generationen. Festen Grund fand man auf dem feuchten Boden erst in 21 Metern Tiefe. Das gesamte Gelände musste demzufolge zunächst mit einer innerhalb von 14 Monaten gegossenen 90 cm dicken und 21 m hohen Betonwanne gegen eindringendes Wasser aus dem Hudson River geschützt werden. Mit dem eine Million Kubikmeter umfassenden Aushub für die sechsgeschossige Sockelzone – das entspricht 100.000 LKW-Ladungen – wurde das westlich vom "World Trade Center"-Grundstück liegende Hafengelände zugeschüttet. Dadurch erweiterte man dessen Baufläche um 9,2 ha, sodass ab 1982 die vier Turmbauten des World Financial Center zu Füßen des "World Trade Centers" errichtet werden konnten.

Typisch für die nicht unumstrittenen Entwürfe des unter Höhenangst leidenden Architekten Yamasaki waren die nur 46 cm schmalen Fenster, die die Zwillingstürme fensterlos erschienen ließen und lediglich 30 Prozent der Fassadenfläche bildeten. Insgesamt wurden bei den Zwillingstürmen 43.600 Fenster verbaut. Die Außenhaut der Türme bestand vor allem aus einem stabilen Netz von je 59 Stahlpfeilern pro Gebäudeseite. Anders als bei üblichen Hochhausbauten, deren Fassade wie eine Gardine an der inneren Tragstruktur aufgehängt war, besaß das aus jeweils 236 Stahlpfeilern gefertigte äußere Skelett der Zwillingstürme eine tatsächliche Tragfunktion. Die Stahlpfeiler der Fassade wurden durch Aluminiumprofile verkleidet. Der innere Kern der Zwillingstürme bestand aus jeweils 47 Stahlpfeilern. Beim Bau der Twin Towers wurden 200.000 Tonnen Stahl und 325.000 m³ Beton verarbeitet. Die Konstruktion sollte Orkanen mit Windgeschwindigkeiten von 300 km/h standhalten, den schlimmsten Erdbeben und dem Einschlag einer mit der Höchstgeschwindigkeit von 965 Kilometern pro Stunde aufprallenden Boeing 707, des größten Passagierflugzeugs der damaligen Zeit.

Im Zuge des Baus des "World Trade Centers" wurde 1969 die in New York ansässige "World Trade Center Association" (WTCA) gegründet, um die Botschaft "„Frieden durch Handel und Wohlstand“" weltweit zu promoten. Die lizenzierungspflichtigen Namensrechte wurden mittlerweile an 320 weltweit existierenden "World Trade Center" vergeben.

Für das Datum der Fertigstellung der von der Tishman Construction Corporation in Kooperation mit der "Karl Koch Erecting Company" errichteten Zwillingstürme gibt es verschiedene Angaben. Während am 23. Dezember 1970 der letzte Träger des Nordturms ("WTC 1") hochgehievt wurde, waren in der Woche zuvor bereits die ersten Mieter eingezogen. Am 19. Juli 1971 wurde die Errichtung des Rohbaus vom Südturm ("WTC 2") abgeschlossen, im September bezogen die ersten Unternehmen auch hier ihre Büros. Die Zwillingstürme wurden offiziell am 4. April 1973 eingeweiht. Die vollständige Fertigstellung dauerte noch bis 1977. Zwischen 1972 und 1975 wurden auch die "World Trade Center"-Gebäude 4, 5 und 6 fertigstellt.

Der 417 Meter hohe Nordturm löste bei seiner Vollendung 1972 das 381 Meter hohe Empire State Building als höchstes Gebäude der Erde ab, das 41 Jahre lang diesen Titel für sich beanspruchen konnte. Schon 1974 musste das prestigeträchtige Attribut an den Sears Tower in Chicago abgetreten werden. Bis zum Jahr 2000, als durch Verlängerung der Antennen die Höhe des Sears Towers von 520 Metern auf 527 Meter gesteigert wurde, war der Nordturm mit dem 526,7 Meter hoch aufragenden Antennenmast allerdings das insgesamt höchste Bauwerk der Welt. Die Zwillingstürme waren die herausragenden und bekanntesten Gebäude des Bürokomplexes und prägten die New Yorker Skyline bis zu ihrem Einsturz am 11. September 2001.

Etwa 50.000 Menschen arbeiteten in 430 Unternehmen aus 28 Ländern im New Yorker "World Trade Center". Hinzu kamen täglich bis zu 200.000 Besucher, vor allem aufgrund der Aussichtsmöglichkeiten und der Restaurants in den Zwillingstürmen. Der auf 4,5 ha stehende World-Trade-Center-Komplex umfasste die Zwillingstürme ("WTC 1" und "WTC 2"), fünf weitere Gebäude ("WTC 3" bis "WTC 7") sowie unter großen Teilen des "WTC 4", "WTC 5" und des "Plaza" eine aus sechs Untergeschossen bestehende "Mall" mit Einkaufspassage und Restaurants. "The Mall" war das größte Einkaufszentrum in Lower Manhattan. Zudem konnte hier trockenen Fußes zwischen den verschiedenen New Yorker U-Bahn-Linien umgestiegen werden. Der U-Bahnhof World Trade Center und eine Linie der "PATH" nach New Jersey waren Mittelpunkt des lokalen Personennahverkehrs. Unter dem "WTC 3" und Teilen der Zwillingstürmen lag eine Tiefgarage für 2000 Fahrzeuge. Der World-Trade-Center-Komplex besaß mit der Adresse "10048" eine eigene Postleitzahl.

Mitte der 1980er Jahre pachtete der Immobilienunternehmer Larry Silverstein ein Grundstück gegenüber dem "World Trade Center"-Komplex und erbaute darauf das 1987 eröffnete "WTC 7". Im November 1995 wurde von der "Port Authority of New York and New Jersey" das als "The Hotel" bekannte "WTC 3" für 141,5 Millionen US-Dollar an den Hotelbetreiber Marriott International verpachtet.

1998 entschied die landesbehördliche Port Authority of New York and New Jersey als Eigentümer des "World Trade Centers" auch die Zwillingstürme ("WTC 1" und "WTC 2"), das "WTC 4", das "WTC 5", sowie 40.000 m² Einzelhandelsflächen der untergeschossigen "Mall" zu privatisieren, um sich wieder verstärkt auf das Kerngeschäft für infrastrukturelle Verbesserungen konzentrieren zu können. Zu diesem Zeitpunkt war das "World Trade Center" vollständig belegt, konnte jährliche Mieteinnahmen von 200 Millionen US-Dollar generieren und war eines der weltweit profitabelsten Immobilienobjekte. Die im Jahr 2000 erfolgte Ausschreibung konnte nach einem massiven Bieterwettstreit ein Konsortium von Larry Silverstein, Joseph Cayre und Lloyd Goldman in Kooperation mit dem Einkaufszentrumsbetreiber Westfield America für sich entscheiden. Der 99 Jahre gültige Pachtvertrag mit einem Volumen von 3,2 Milliarden US-Dollar war die größte Immobilientransaktion der amerikanischen Geschichte und wurde am 24. Juli 2001 unterzeichnet.

Das "World Trade Center" lag im Financial District, dem Zentrum der Finanzwelt von New York City, an der Südwestspitze des Bezirks Downtown Manhattan.

Das Gelände des World-Trade-Center-Vierecks wurde im Norden von der Vesey Street umschlossen, von der Church Street im Osten, von der Liberty Street im Süden und der West Street Avenue im Westen. In der Mitte der Westfront stand der Nordturm ("WTC 1" oder "1 World Trade") mit der Autoauffahrt von der West Street Avenue. Darüber ging vom WTC 6 eine Fußgängerbrücke zu den markanten Gebäuden des gegenüberliegenden World Financial Center (WFC) mit seinem vorgelagerten "Wintergarden". In der südwestlichen Ecke des "World-Trade-Center"-Vierecks befand sich das als Marriott World Trade Center bekannte WTC 3. Mit seinem verhältnismäßig schmal wirkenden, leicht angewinkelten Grundriss verdeckte das Hotel von Westen den Blick auf die unteren Etagen des Südturms. In der Nähe des WTC 3 überquerte eine weitere Fußgängerbrücke die West Street Avenue Richtung St. Nicholas Greek Orthodox Church. Die Kirche war neben dem Deutsche Bank Building und dem WTC 7 eines der drei außerhalb des "World Trade Center"-Vierecks liegenden Gebäude, die bei den Terroranschlägen am 11. September 2001 zerstört bzw. danach abgerissen wurden.

Das WTC 7 stand außerhalb des eigentlichen World-Trade-Center-Vierecks nördlich der Vesey Street, war aber in Höhe des 3. Stockwerks mittels einer Promenade sowie einer Fußgängerbrücke mit dem nach "Austin J. Tobin" benannten Außenplatz, dem "World Trade Center Plaza", verbunden. Auf dem "Plaza" befand sich die auch nach dem 11. September 2001 noch erhaltene Bronzeskulptur "The Sphere" des deutschen Bildhauers Fritz Koenig, die dem ursprünglichen "World Trade Center"-Motto "„Weltfrieden durch Handel“" gewidmet ist. Die neunstöckigen WTC 4 und WTC 5 schlossen das "World Trade Center"-Quadrat in Richtung Osten ab.

Der 1972 als Teil der Zwillingstürme fertiggestellte Nordturm (, ) war 417 Meter hoch und kam dank eines im Jahre 1978 auf dem Dach installierten und 109,7 Meter hohen Antennenmasts auf eine Gesamthöhe von 526,7 Meter.

Der 417 Meter hohe Nordturm löste bei seiner Vollendung 1972 das 381 Meter hohe Empire State Building als höchstes Gebäude der Erde ab, das 41 Jahre lang diesen Titel für sich beanspruchen konnte. Schon 1974 musste das prestigeträchtige Attribut an den Sears Tower in Chicago abgetreten werden. Allerdings war der Nordturm mit dem 526,7 Meter hoch aufragenden Antennenmast das immer noch insgesamt höchste Gebäude der Welt. Erst im Jahr 2000 konnte der Sears Tower auch diesen Titel für sich beanspruchen, als durch Verlängerung der Antennen die Höhe des Sears Towers von 520 Metern auf 527 Meter gesteigert wurde.

Das WTC 1 hatte wie auch sein Zwillingsturm Grundabmessungen von 63,4 × 63,4 Meter, stand auf 21 Meter tiefen Fundamenten und hatte eine vermietbare Bürofläche von über 418.000 m², die sich auf die 110 Stockwerke verteilten. Jedes Geschoss war 3,65 Meter hoch und besaß eine Fläche von 4000 m².
Der Nord- und Südturm verfügte über jeweils 99 Fahrstühle, von denen 23 als "Shuttle-Express" eine Geschwindigkeit von bis zu 8 m/s (28,8 km/h) erreichten. Die Zubringer-Aufzüge fuhren nonstop zu den "Sky Lobbys" auf der 44. und 78. Etage, um von hier aus auf einen lokalen Fahrstuhl umsteigen zu können. Zudem garantierten die Expressfahrstühle einen schnellen Transport zum "Windows on the World"-Restaurant und der zugehörigen "Greatest Bar on Earth" auf der 106. und 107. Etage. Durch die Panoramascheiben des "Windows on the World" konnte man über Queens, New Jersey, Brooklyn und den John-F.-Kennedy-Flughafen blicken. An klaren Tagen war eine Sicht bis zu 80 Kilometern möglich.

Jeweils 8 Stockwerke waren ausschließlich mit Gebäudetechnik belegt. Hier befanden sich alle für die Gebäudeanlage nötigen Versorgungseinrichtungen wie Wasser- und Luftversorgung, Klimaanlagen und Elektrik. In der 110. Etage des Nordturms hatten zahlreiche Fernseh- und Radiostationen ihre Sendeanlagen. Unter den Mietern befand sich neben zahlreichen Unternehmen aus der Finanz- und Versicherungsbranche auch die Botschaftsvertretung von Thailand.






Der 1973 als Teil der Zwillingstürme fertiggestellte Südturm () war 415 Meter hoch. In der 110. Etage des Südturms befand sich die weltweit höchstgelegene öffentlich zugängliche Besucherterrasse. Auf dem 107. Stockwerk war eine innen liegende Aussichtsplattform. Die Aufteilung und technische Ausstattung entsprach der des Nordturms. Unter den Mietern befanden sich zahlreiche Unternehmen der Finanz-, Investment- und Versicherungsbranche.

Vom 11. September bis 10. Oktober 1995 wurde auf der Aussichtsplattform der 107. Etage des Südturms die Schachweltmeisterschaft 1995 zwischen Titelverteidiger Garri Kasparow und Herausforderer Viswanathan Anand veranstaltet. Kasparow gewann das in einer schallgedämmten Glaskabine ausgetragene Duell mit 10,5 zu 7,5 Punkten.

Das 1981 unter dem Namen "Vista Hotel" eröffnete "Marriott World Trade Center" (kurz "WTC 3") hatte 821 Hotelzimmer auf 22 Etagen und eine Höhe von 72 Meter. Der Entwurf stammte von Skidmore, Owings and Merrill.

Das Hotel der AAA-Kategorie hatte einen schmalen, der West Street folgenden Grundriss direkt zu Füßen der Zwillingstürme. Das Hotel hatte Verbindungen mit dem Nordturm (WTC 1) und dem Südturm (WTC 2). In den Verbindungen waren Modehäuser, Cafés und Restaurants untergebracht. Als Shops gab es in der Passage zu den beiden Türmen ein Greenhouse Cafe, den Tall Ships Bar & Grill, Times Square Gifts, von der Grayline New York Tours Bus ein Verkaufscenter sowie der Friseursalon Olga’s. Neben Restaurants gab es 2400 m² für Geschäftstreffen. Eine weitere Fußgängerbrücke überquerte die West Street in Richtung St. Niklas. Das ehemalige "Vista-Hotel" wurde von der "Port Authority of New York and New Jersey" im November 1995 für 141,5 Millionen US-Dollar an die Marriott-Gruppe verpachtet.

Das "Marriott World Trade Center" wurde am 11. September 2001 zunächst durch herabfallende Trümmer des einstürzenden Südturms beschädigt und schließlich durch den Kollaps des Nordturms zerstört. In der Halle des Hotels war nach 9.00 Uhr eine mobile Einsatzleitung der New Yorker Feuerwehr errichtet worden. Alle Gäste und das Personal wurden evakuiert.

Der britische Fernsehsender Channel 4 produzierte den Dokumentarfilm "The 9/11 Hotel" über 14 Mitarbeiter und Hotelgäste des "Marriott World Trade Center", die die Terroranschläge am 11. September 2001 überlebten.

Das 1975 eröffnete "WTC 4" (auch "South Plaza Building" oder "Commodities Exchange Center") war ein 36 Meter hohes neunstöckiges Gebäude, das im gleichen Design und im gleichen Stil wie die "World Trade Center 5" und "World Trade Center 6" konstruiert wurde.

Hauptmieter des "WTC 4" war auf der 7.,8. und 9. Etage das synonymgebende "Commodities Exchange Center" mit der weltgrößten Warenterminbörse für Rohstoffe, New York Mercantile Exchange (NYMEX) sowie drei weiteren Warenterminbörsen des ICE Futures U.S. ("Commodity Exchange" (COMEX), "New York Cotton Exchange" (NYCE), "Coffee, Sugar and Cocoa Exchange, Inc."). Im 8. Stockwerk des "WTC 4" war das damals weltweit größte Handelsparkett für Warentermingeschäfte mit Gold, Silber, Platin, Kupfer, Aluminium, Rohöl, Heizöl, Erdgas, Propangas, Baumwolle, Zucker, Kakao und konzentrierten gefrorenen Orangensaft beheimatet. Die "Rohstoffbörse für gefrorenes Orangensaftkonzentrat" (F.C.O.J.) diente als Kulisse für die Komödie Die Glücksritter mit Eddie Murphy und Dan Aykroyd. Die Stockwerke 4,5 und 6 mit insgesamt über 25.000 m² waren vollständig von der Deutschen Bank angemietet. An der zur Liberty Street gewandten Seite des "WTC 4" befand sich der Eingang zu "The Mall" im Untergeschoss des "World Trade Centers".

In den Katakomben des "World Trade Center 4" waren Rohstoffdepots in zum Teil zweistöckigen Tresorräumen eingerichtet. Allein die ScotiaMocotta lagerte hier zum Zeitpunkt der Anschläge am 11. September etwa 11,2 t Gold und 850 t Silber im Gesamtwert von über 200 Millionen US-Dollar, die verwendet wurden, um Gold- und Silber-Terminkontrakte an der im "WTC 4" ansässigen Warenterminbörse NYMEX auszugleichen. Die etwa 380.000 Unzen Gold und 30 Millionen Unzen Silber konnten mit Hilfe von Polizei und Feuerwehr geborgen werden.

Das 1975 eröffnete "WTC 5" (auch "North Plaza Building") war ein 36 Meter hohes neunstöckiges Bürogebäude. Das L-förmig angelegte "WTC 5" hatte ein Maß von 100 × 130 Meter. Jede Etage hatte eine Grundfläche von etwa 11.000 m².

Das US-amerikanische Finanzinstitut Morgan Stanley belegte mehr als 30.000 m² Bürofläche auf den gesamten Etagen 3, 4 und 5. Ein weiterer Hauptmieter war die Credit-Suisse-Investmentbank "Credit Suisse First Boston" mit über 16.000 m² auf den Stockwerken 7, 8 und 9.

Bei den Zusammenstürzen der Zwillingstürme am 11. September 2001 wurden die oberen sechs Stockwerke durch herabgefallene Trümmer und Brände schwer beschädigt oder waren zusammengestürzt. Auf dem Dach des "World Trade Center 5" wurden Flugzeugteile von der in den Südturm eingeschlagenen Boeing 767 geborgen. Die unteren drei Etagen blieben unbeschädigt.

Das "WTC 5" wurde im Januar 2002 vollständig abgerissen. Auf diesem Grundstück wird das neue Two World Trade Center stehen.

Das 1975 eröffnete "WTC 6" (auch "U.S. Customshouse", ) befand sich an der nordwestlichen Ecke des "World-Trade-Center"-Komplexes zu Füßen des Nordturms. Das "WTC 6" war mit 32 Metern Höhe und acht Stockwerken das niedrigste Gebäude des World-Trade-Center-Komplexes. In dem Bürobau mit einer Gesamtnutzungsfläche von 49.953 m² befanden sich Regierungsinstitutionen, wie die Zoll- und Grenzschutzbehörde, das Amt für Alkohol, Tabak, Schusswaffen und Sprengstoffe (ATF), das Handelsministerium, das Arbeitsministerium, das Landwirtschaftsministerium, sowie die Export-Import Bank of the United States.

Am 11. September 2001 waren bereits 12 Minuten nach dem Einschlag des Flugzeugs in den Nordturm alle 800 Arbeitnehmer aus dem "WTC 6" erfolgreich evakuiert. Das "WTC 6" wurde durch Trümmer und lang anhaltende Brände so stark beschädigt, dass es wie alle anderen Gebäude des weltgrößten Bürokomplexes letztlich vollständig abgerissen werden musste. Unter anderem war ein schätzungsweise 1000 Tonnen schweres und 40 Meter breites Segment des einstürzenden Nordturms bis in die Kellergeschosse des "WTC 6" herabgestürzt. Auf dem Grund des "WTC 6" wurde der neue Wolkenkratzer One World Trade Center errichtet.

Das WTC 7 wurde zwischen 1984 und 1987 auf einem Grundstück der New Yorker Hafenbehörde nördlich der Vesey Street gegenüber dem eigentlichen "World-Trade-Center"-Viereck, aber als Teil des Gesamtkomplexes, errichtet. Es war 186 Meter hoch, hatte 47 Stockwerke und war in Höhe des dritten Stockwerks mit einer Promenade und einer Fußgängerbrücke mit dem WTC-Hauptkomplex verbunden. Bauherr und Eigner war die Immobiliengesellschaft Silverstein Properties, der Entwurf stammte vom Architekturbüro Emery Roth & Sons, die Bauarbeiten wurden wie bei den Zwillingstürmen von Tishman Construction durchgeführt. Das WTC 7 hatte eine Gesamtnutzfläche von 200.000 m², von denen 174.000 m² als Bürofläche konzipiert wurden.

1989 erfolgte im Auftrag des neuen Hauptmieters Salomon Brothers ein umfangreicher 200 Millionen US-Dollar teurer Umbau zu einem bis dahin einmaligen „Gebäude-im-Gebäude“. Dabei wurden zusätzlich über 375 Tonnen Stahl verbaut, um die Stockwerke für die Sonderausstattung der Salomon Brothers zu verstärken. Danach wurde das WTC 7 auch "Salomon Brothers Building" genannt. Wie auch im WTC 6 gehörten zahlreiche Regierungsbehörden zu den Mietern, wie die US-Steuerbehörde (IRS), die US-Börsenaufsicht (SEC), das US-Verteidigungsministerium, der U.S. Secret Service, die Central Intelligence Agency (CIA) sowie das Office of Emergency Management der Stadt New York.

Der Gebäudekomplex war erstmals am 26. Februar 1993 Ziel eines Bombenanschlags islamistischer Terroristen. Damals hatten Terroristen einen gemieteten Ryder-Van auf der Ebene B2 der Tiefgarage des Nordturms des World Trade Centers abgestellt. In ihm hatten sich etwa 700 kg des Sprengstoffs Harnstoffnitrat befunden sowie etliche Druckgasbehälter mit Wasserstoff, die die Wucht bei der Explosion des Fahrzeugs noch verstärken sollten. Die Explosion riss ein 30 Meter großes Loch in vier der sechs Untergeschosse (in Betonbauweise).

Sieben Stockwerke wurden besonders schwer beschädigt, sechs davon unter der Erde. Dem Anschlag fielen sechs Menschen zum Opfer, über tausend weitere wurden verletzt. Daraus resultierte einer der größten Rettungseinsätze in der Stadt New York, wobei etwa 45 % des diensthabenden Personals der Feuerwehr zu diesem Schadensereignis gerufen wurden. Sechs islamistische Terroristen wurden 1997 bzw. 1998 dieses Attentats für schuldig befunden und zu je 240 Jahren Freiheitsstrafe verurteilt.

Im Zuge der Anschläge wurden neue Sicherheitsbestimmungen für das "World Trade Center" erlassen und die "Port Authority of New York and New Jersey" investierte in den folgenden acht Jahren 700 Millionen US-Dollar für Sicherheitsmaßnahmen in den Gebäuden. Zudem wurde im Juni 1999 im 23. Stockwerk des "WTC 7" das hochtechnisierte und 13 Millionen US-Dollar teure Office of Emergency Management ("OEM") vom OEM-Direktor und Antiterrorexperten Jerome Hauer eingeweiht. Das bombensichere Notfall- und Katastrophenzentrum war rund um die Uhr besetzt und sollte bei einem Terroranschlag, Katastrophenfall oder einem großflächigen Stromausfall die New Yorker Feuerwehr, Polizei und sonstige Notdienste koordinieren. In den Zuständigkeitsbereich des OEM fielen außerdem tägliche Notfallübungen.

Am Morgen des 11. Septembers 2001 entführten 19 Terroristen des Netzwerks al-Qaida vier Flugzeuge auf Inlandslinienflügen. Zwei davon wurden von jeweils fünf Entführern in die Zwillingstürme gesteuert. Um 8:46 Uhr schlug American-Airlines-Flug 11 in den Nordturm (WTC 1), um 9:02 Uhr schlug United-Airlines-Flug 175 in den Südturm (WTC 2) ein. Das explodierende Kerosin löste anhaltende Gebäudebrände auf vielen Stockwerken aus. Der Südturm kollabierte um 9:59 Uhr, der Nordturm um 10:28 Uhr Ortszeit. 

Die 9/11-Kommission klärte von Dezember 2002 bis August 2004 Entstehung, Planung und Verlauf der Anschläge auf. Das National Institute of Standards and Technology (NIST) klärte von 2002 bis 2008 die physikalischen Ursachen der Einstürze von WTC 1, 2 und 7 auf und gab mehrere Untersuchungsberichte dazu heraus. Nach dem Abschlussbericht für die Türme durchtrennte der erste Einschlag zwischen dem 93. und 99. Stockwerk 35 von 236 Außenpfeilern sowie sechs von 47 Innenpfeilern des WTC 1 und entfernte an 43 davon die Brandschutzbeschichtung. Drei Treppenhäuser stürzten ein und unterbrachen die Fahrstuhlverbindungen oberhalb des 60. Stockwerks. Geschätzte 15 % des getankten Kerosins gingen in einem Feuerball auf, etwa 50 % liefen unverbrannt im Gebäude aus. Brennendes Kerosin schoss nach oben und unten durch die Aufzugsschächte, sprengte Türen und Wände auf mehreren Stockwerken bis hinein in den Keller sowie viele Fenster der Lobby im Erdgeschoss und durchfegte die Eingangshalle bis zum Ausgang zum WTC 3. Unmittelbar danach riefen viele Betroffene den Notruf an. 

Der zweite Flugzeugeinschlag erfolgte mit 870 km/h zwischen dem 77. und dem 85. Stockwerk des WTC 2 und durchtrennte 33 von 236 Außenpfeilern, darunter den südwestlichen Eckpfeiler, sowie zehn von 47 Innenpfeilern und löste an 39 davon die Brandschutzbeschichtung ab. Auch hier verbrannten weniger als 15 % des Kerosins sofort, mindestens die Hälfte lief unverbrannt im Gebäude aus. Das Gewicht des oberen Gebäudeteils verteilte sich auf die restlichen intakten Außen- und Innenpfeiler. Die Zugkraft der einsackenden Stockwerke 79 bis 84 bog die östlichen Außenpfeiler schon 18 Minuten später nach innen. Als die geschwächte Baustruktur und versagende Stützpfeiler die Gewichte nicht mehr tragen konnten, kippte der obere Teil nach Süden und Osten ab. Um 9:58:59 Uhr kollabierte das WTC 2 in neun Sekunden. Seit 10:06 Uhr erwartete das New York City Police Department auch den Einsturz des WTC 1. Um 10:23 Uhr gaben die Etagen auf der Südseite nach; um 10:28 Uhr kippte der Gebäudeteil über der Einschlagszone nach Süden und durchschlug dann alle Stockwerke darunter in 12 Sekunden. Hauptursachen der Einstürze waren laut NIST direkte Schäden an der Baustruktur durch die Flugzeugeinschläge, großflächige anhaltende Brände auf mehreren Stockwerken, die die nunmehr unbeschichteten Stahlträger des Innenkerns rasch erhitzten, aufweichten und ihre Belastung auf die äußeren Stützpfeiler übertrugen, bis diese nach innen einsackten und die Stockwerke darüber nicht mehr tragen konnten. 

Rund 15.000 Personen konnten die Türme rechtzeitig verlassen. Bei den Flugzeugeinschlägen und Gebäudeeinstürzen starben insgesamt 2753 Menschen. 16 Personen überlebten in einem Treppenhaus des WTC 1. 

Der Einsturz des WTC 2 beschädigte zudem das WTC 3, zerstörte einen Großteil des WTC 4 und begrub die 11 Meter hohe St. Nicholas Greek Orthodox Church vollständig unter den Trümmern. Auch der unbeschädigte Teil des WTC 4 wurde für Neubauten abgerissen.

Trümmer des WTC 1 fielen auf das 110 Meter weit entfernte WTC 7 und verursachten unkontrollierbare Brände, da Hauptwasserleitungen zerstört wurden, eine Sprinkleranlage ausgefallen war, die Brandherde unzugänglich waren und die Feuerwehr aus Sicherheitsgründen gegen 14:30 Uhr jenes Tages abgezogen wurde. Um 17:20 Uhr stürzte das WTC 7 vollständig ein. Die NIST-Untersuchung zu WTC 7 ergab, dass Stahlträger und -stützen durch ihre von den unkontrollierten Bränden bewirkte Wärmeausdehnungen verbogen und verschoben wurden. Dadurch brach im 13. Stockwerk eine Verbindung zwischen einem Träger und einer Stütze. Das Stockwerk stürzte an dieser Stelle ein und riss die Stockwerke darunter bis zum 5. Stockwerk mit sich. Der nun über die Länge von acht Stockwerken freistehende Stützpfeiler wurden an dieser Stelle überlastet und knickte noch weiter ein, was in der Folge zum Totaleinsturz führte. Gebäude mit gleichartiger Stahlrahmenkonstruktion waren bei ähnlichen Brandumständen bis dahin nicht eingestürzt, hatten aber ein strukturelles Design, das sich von dem des WTC 7 unterschied.

Seit dem 11. September wird die World Trade Center Site auch „Ground Zero“ genannt. Im Mai 2002 wurden die Aufräumarbeiten dort beendet. Die meisten Trümmer und der Aushub wurden auf die Bauschuttdeponie Fresh Kills in Staten Island, N.Y. transportiert. Bei der Entfernung des Schutts und des Aushubs wurde versucht, die Überreste von Opfern des Anschlags herauszufiltern und genetisch zu identifizieren. In der Folge gab es in den USA eine Debatte um den Umgang mit dem World-Trade-Center-Grundstück und der Deponie.

Zum Gedenken an die Opfer wird jährlich am 11. September das Tribute in Light („Ehrerbietung in Licht“) ausgerichtet. Dabei werden die Umrisse der Zwillingstürme als Lichtsäulen am Abend- und Nachthimmel nachgebildet.

Verschwörungstheorien zum 11. September 2001 behaupten andere Ursachen der Gebäudeeinstürze, meist eine kontrollierte Sprengung mit Explosivstoffen, die vor den Flugzeugeinschlägen heimlich im Gebäude platziert worden sein sollen. Diese These wiesen die NIST-Berichte detailliert zurück. Antisemitische Verschwörungstheorien stellen Larry Silverstein, den jüdischstämmigen Pächter der Türme und Eigentümer des WTC 7, häufig als Profiteur der Gebäudeeinstürze dar und behaupten: Er müsse die Anschläge erwartet haben, da er die WTC-Türme erst kurz zuvor zu sehr günstigen Konditionen erworben und für ihn günstige Versicherungspolicen abgeschlossen habe. Tatsächlich musste Silversteins Unternehmen die Leasingraten für die zerstörten Gebäude weiterzahlen, erhielt nach langem Rechtsstreit nur etwa die Hälfte der angestrebten Versicherungssumme und war rechtlich verpflichtet, diese in den Neubau zu investieren.

Am 7. Mai 2002 erfolgte der Spatenstich für das 7 World Trade Center, dessen Neubau 2006 abgeschlossen war. Als leitenden Architekten setzte Bauherr Larry Silverstein seinen Stammarchitekten David Childs und dessen Architekturbüro Skidmore, Owings and Merrill gegen Daniel Libeskind durch, dessen Entwürfe zunächst den Ausschreibungswettbewerb gewonnen hatten. Silverstein warf den emotional geprägten Bauvisionen Libeskinds fehlende Wirtschaftlichkeit vor. Childs pflegte eine längere Geschäftsbeziehung zu Silverstein und hatte schon das 7 WTC für ihn entworfen. Zudem hatte Silverstein ihn Anfang August 2001 mit Sanierungskonzepten für die Zwillingstürme beauftragt. Kurz nach deren Zerstörung beauftragte Silverstein ihn, erste Entwürfe für eine Neubebauung des Ground Zero anzufertigen.

Im Verlaufe der Bauarbeiten kam es zwischen Silverstein, der Stadt New York und der Hafenbehörde zu mehreren Rechtsstreitigkeiten, die zu temporären Baustopps und erheblichen Verzögerungen führten.

Von November 2002 bis Mai 2006 wurde zunächst das "7 World Trade Center" auf dem Grundstück des am 11. September 2001 eingestürzten "WTC 7" erbaut. Da das Grundstück außerhalb des eigentlichen World-Trade-Center-Vierecks liegt, konnte schon frühzeitig mit dem Bau begonnen werden.

Am 6. September 2005 begannen am Ground Zero die Bauarbeiten des vom spanischen Architekten Santiago Calatrava entworfenen neuen World-Trade-Center-U-Bahnhofs. Die Kosten für das aus öffentlicher Hand finanzierte Bauprojekt verdoppelten sich nach über zehnjähriger Bauzeit von veranschlagten knapp zwei Milliarden auf 3,85 Milliarden US-Dollar. Die am 4. März 2016 eröffnete "World Trade Center Transportation Hub" ist damit der teuerste Bahnhof der Welt.

Im Herbst 2006 wurde das Designkonzept der übrigen Bürotürme des neuen World-Trade-Center-Komplexes vorgestellt. Am 19. Dezember 2006 erfolgte auf dem als Ground Zero bekannten Gelände die Grundsteinlegung für den Bau des One World Trade Centers des Architekten David Childs. Zum 10. Jahrestag der Terroranschläge wurde am 11. September 2011 die nationale Gedenkstätte eingeweiht.

Das am 10. Mai 2013 fertigstellte "One World Trade Center" ist mit einer symbolischen Höhe von 541,3 Metern das höchste Gebäude der westlichen Hemisphäre sowie das vierthöchste der Welt. Das hauptsächlich von der New Yorker Hafenbehörde mit Steuergeldern finanzierte "One World Trade Center" entstand aus einem Joint Venture mit dem New Yorker Immobilienunternehmen "Durst Organization" und ist mit Baukosten von 3,8 Milliarden US-Dollar das mit Abstand teuerste Bürogebäude der Welt.

Im November 2013 wurde das Four World Trade Center eröffnet. 2018 sollen die Bauarbeiten für das Three World Trade Center abgeschlossen sein. 2021 soll planmäßig das neue Two World Trade Center folgen. Die Gebäude "2", "3" und "4 World Trade Center" sind Eigentum der "Silverstein Properties". Der Baubeginn des Five World Trade Center wurde mehrfach verschoben und ist derzeit ungewiss.

Für August 2016 ist die Eröffnung der unterirdischen Shoppingmall "Westfield World Trade Center" geplant. Die Westfield Group investierte 1,4 Milliarden US-Dollar in die doppelgeschössige Einkaufspassage, die über hundert Geschäften insgesamt 34.000 m² Verkaufsfläche bietet. Zusätzlich werden in den jeweils fünf unteren Stockwerken des "Three World Trade Centers" und "Four World Trade Centers" Geschäfte und Boutiquen eingerichtet.

Neue Gebäude auf dem Gelände des World Trade Centers (Baustatus: Stand 2016):





</doc>
<doc id="5560" url="https://de.wikipedia.org/wiki?curid=5560" title="Wall Street">
Wall Street

Die Wall Street (dt. Wallstraße) ist eine Straße im New Yorker Stadtbezirk Manhattan, in der sich zahlreiche Kreditinstitute und die weltgrößte Wertpapierbörse, die New York Stock Exchange, befinden.

1647 entschloss sich die Niederländische Westindien-Kompanie in der Stadt Neu Amsterdam für Ordnung zu sorgen. Diese Aufgabe sollte Petrus Stuyvesant übernehmen. Während seiner 17 Jahre dauernden Amtszeit als Gouverneur wurden das erste Krankenhaus, das erste Gefängnis und die erste Schule gebaut. Als Schutz vor Überfällen der Indianer ließ er 1652 im Norden der Stadt quer über die Insel einen Wall aufschütten, der später der dort verlaufenden Straße ihren Namen gab. Auf der Wall Street befand sich ab 1711 der erste Sklavenmarkt New Yorks.

Die rund 1100 Meter lange Straße bildet das Zentrum des New Yorker Finanzdistrikts. Zugleich wird mit dem Begriff "Wall Street" auch die US-amerikanische Finanzindustrie als Ganzes bezeichnet.

Am Ende der Straße liegt die bekannte Trinity Church. An die Zeit, in der New York City die Hauptstadt der USA war, erinnert die Federal Hall, in der der erste Kongress tagte und dort unter anderem die Bill of Rights verabschiedete.



</doc>
<doc id="5563" url="https://de.wikipedia.org/wiki?curid=5563" title="William Hurt">
William Hurt

William Hurt (* 20. März 1950 in Washington, D.C.) ist ein US-amerikanischer Schauspieler und Oscar-Preisträger.

Seine Schauspielausbildung absolvierte Hurt gemeinsam mit Christopher Reeve und Robin Williams an der Juilliard School, damals geleitet von dem früheren Schauspieler John Houseman. Hurt, Williams und Reeve waren die einzigen Bewerber im Jahr 1973, die die Aufnahmeprüfung (man musste vor einer Jury, inklusive Mr. Houseman, eine Szene aus einem klassischen und einem modernen Theaterstück spielen) bestanden. Trotz aller Filmerfolge, deren Höhepunkt sich in den 1980er Jahren abzeichnete (1986 Oscar für die Hauptrolle in "Kuß der Spinnenfrau"), blieb Hurt immer dem Theater treu. Dabei feierte er Erfolge auch bei Off-Broadway-Aufführungen, etwa in der Hauptrolle in "My Life" mit Christopher Reeve in einer Nebenrolle. Nachdem Attraktivität und Bedeutung der ihm angebotenen Rollen in seinen Augen nachgelassen haben, konzentriert sich Hurt heute hauptsächlich auf Theaterproduktionen.

Von 1971 bis 1982 war Hurt mit Mary Beth Hurt verheiratet. Zwischenzeitlich lebte er mit der gehörlosen Schauspielkollegin Marlee Matlin zusammen, die ihn in ihrer 2009 erschienenen Autobiografie "I'll Scream Later" der Gewalttätigkeit ihr gegenüber bezichtigte. Mit seiner zweiten Ehefrau Heidi Henderson (1989 bis 1992) hat er zwei Kinder; aus der Beziehung mit Sarah Jennings stammt ein weiteres Kind. Schließlich lebte Hurt von 1992 bis 1997 mit der französischen Filmschauspielerin Sandrine Bonnaire zusammen, mit der er ebenfalls ein gemeinsames Kind hat.

Hurts deutsche Synchronstimme stammte überwiegend von Randolf Kronberg, der nach seinem Tod durch Thomas Fritsch und Jürgen Heinrich ersetzt wurde.




</doc>
<doc id="5564" url="https://de.wikipedia.org/wiki?curid=5564" title="Wolfgang Pauli">
Wolfgang Pauli

Wolfgang Ernst Pauli (* 25. April 1900 in Wien; † 15. Dezember 1958 in Zürich) war ein österreichischer Wissenschaftler und Nobelpreisträger, der zu den bedeutendsten Physikern des 20. Jahrhunderts zählt. Er formulierte 1925 das später nach ihm benannte Pauli-Prinzip, das eine quantenmechanische Erklärung des Aufbaus eines Atoms darstellt und weitreichende Bedeutung auch für größere Strukturen hat.

Pauli wurde in Wien als Sohn eines Arztes und Universitätsprofessors für Kolloidchemie, Wolfgang Josef Pauli (1869–1955), geboren, der aus einer jüdischen Prager Verleger-Familie stammte, aber zum Katholizismus konvertiert war (sein ursprünglicher Name war Wolf Pascheles). Seine Mutter Berta „Maria“ war Journalistin und Frauenrechtlerin. Pauli hatte eine Schwester Hertha Pauli, die Schauspielerin und Schriftstellerin war. Mit zweitem Vornamen wurde Pauli nach seinem Patenonkel benannt, dem Physiker Ernst Mach.

Bereits auf dem Gymnasium in Wien (BG XIX, Gymnasiumstraße 83, 1190 Wien) galt Pauli als mathematisches Wunderkind. 1918 veröffentlichte er gleich nach der Matura seine erste Arbeit über Hermann Weyls Erweiterung von Albert Einsteins "Allgemeiner Relativitätstheorie" (Weyls Buch "Raum-Zeit-Materie" war im gleichen Jahr gerade erschienen).

Ab 1919 studierte er Physik an der Ludwig-Maximilians-Universität München bei Arnold Sommerfeld, wo er in kürzestmöglicher Zeit 1921 mit einer Arbeit über das Wasserstoffmolekül-Ion (das einfachste Molekül) "summa cum laude" promoviert wurde. Eine Theorie, die alle Phänomene erklärte, war noch nicht entwickelt und das Ergebnis war aus seiner Sicht eine Enttäuschung, zeigte es doch deutlich die Grenzen des Bohrschen Atommodells, an dem er auch 1921/22 als Assistent von Max Born in Göttingen weiterarbeitete (Anwendung der Methoden der Himmelsmechanik, insbesondere der Störungstheorie, wie sie Born in seinem Buch "Atomphysik" darstellt). 1922/23 ging er für ein weiteres Jahr zu Niels Bohr nach Kopenhagen. 1923 bis 1928, also in der entscheidenden „Sturm-und-Drang-Zeit“ der Quantenmechanik, war er Professor in Hamburg. Die Hamburger Zeit betrachtete er im Rückblick als die wohl glücklichste Zeit seines Lebens, sicher auch deswegen, weil er hier in dem Physiker Otto Stern, dem Mathematiker Erich Hecke und dem Astronomen Walter Baade gleichgesinnte Kollegen fand, mit denen er den wissenschaftlichen und freundschaftlichen Austausch pflegen konnte.

1928 wechselte Pauli an die ETH in Zürich. Ab 1935 arbeitete er intermittierend in den USA, wo er u. a. 1935/36 am Institute for Advanced Study in Princeton forschte und wo er ab 1940 und während des Zweiten Weltkriegs wieder war. Nach dem Anschluss Österreichs wurde er automatisch deutscher Staatsbürger. Er stellte daraufhin einen Antrag auf Einbürgerung in die Schweiz, der abgelehnt wurde. Nach Beginn des Zweiten Weltkrieges stellte Pauli einen zweiten Einbürgerungsantrag. Auch dieser wurde abgewiesen. In der Begründung der Polizeibehörde hieß es:

Pauli schrieb daraufhin im Mai 1940 einen Brief an Frank Aydelotte, den Direktor des Institute for Advanced Studies in Princeton, in dem er schilderte, dass er nach deutschem Recht als „Dreiviertel-Jude“ gelte und im Falle einer zu befürchtenden deutschen Invasion der Schweiz auch eine entsprechende Behandlung zu befürchten habe. In einem solchen drohenden Fall würde er auf jeden Fall versuchen, nach Frankreich zu fliehen, um nach Amerika zu gelangen. Pauli lehrte am Institute for Advanced Study, wo er damals und auch mehrmals später Mitglied war, und an der Princeton University und war 1942 Gastprofessor an der Purdue University. Seine Professur an der ETH Zürich behielt er aber nach wie vor bei.

In den USA arbeitete er nicht an kriegswichtigen Projekten mit. Als sein Rockefeller-Stipendium, mit dem er sich ab 1940 finanzierte, 1942 reduziert wurde, bemühte er sich allerdings, in kriegsbedingte Projekte einbezogen zu werden, und wandte sich an Robert Oppenheimer, der ihm allerdings in einem merkwürdigen Antwortbrief davon abriet und ihm den Vorschlag machte, stattdessen die Fahne der Grundlagenforschung hochzuhalten und auch Artikel aus seiner eigenen Feder unter dem Namen von Kollegen wie Hans Bethe, Edward Teller und Robert Serber, die an geheimer Forschung arbeiteten, zu veröffentlichen, um das Misstrauen der Deutschen über das Versiegen wissenschaftlicher Publikationen dieser Wissenschaftler zu dämpfen. Oppenheimer begründete den merkwürdigen Vorschlag mit dem bekannten Hang Paulis zu "Burlesken," Pauli lehnte aber ab. 1946 wurde er amerikanischer Staatsbürger, ging aber im selben Jahr zurück an die ETH in Zürich. Dort hatte man ihm zwar seine Professorenstelle zunächst noch freigehalten, es gab aber Widerstände wegen seiner Abwesenheit, und einflussreiche Kräfte in der Schweiz wollten seine Kündigung. Es entspann sich ein bitterer Briefwechsel während des Krieges mit Pauli, der sich dagegen wehrte. Das endete, als bekannt wurde, dass Pauli den Nobelpreis erhalten würde, und er konnte 1946 auf seinen Lehrstuhl an der ETH Zürich zurückkehren. 1949 wurde er endlich Schweizer Staatsbürger. Auch in den 1950er Jahren kehrte er regelmäßig zu Gastvorlesungen nach Princeton zurück. Pauli war an der Gründung des CERN beteiligt. 1958 starb er überraschend an einem Pankreas-Krebs in einem Zürcher Spital in einem Zimmer mit der Nummer „137“, was er bei seiner Einweisung dort als schlechtes Omen angesehen hatte (siehe Wert der Feinstrukturkonstante).

Pauli war ein ausgesprochener „Gesellschaftsmensch“. Er war schon in seiner Studienzeit bekannt dafür, dass er sich gerne bis spät in die Nacht in verschiedenen Kneipen aufhielt und deswegen oft erst spät am nächsten Morgen zur Arbeit erschien. In seiner Jugendzeit war Pauli strikter Abstinenzler gewesen. Er war jedoch in seiner Hamburger Zeit im Zusammensein mit seinen Freunden, dem Astronomen Walter Baade, dem Physiker Otto Stern und dem Mathematiker Erich Hecke auf einen anderen Geschmack gekommen und meinte dazu später: „Als ich nach Hamburg kam, wechselte ich unter dem Einfluss von Stern direkt vom Mineralwasser zum Champagner.“ In Hamburg war er ein häufiger Besucher des Nachtlebens in St. Pauli und war dort auch, da er unter zu viel Alkohol die Kontrolle verlor, in Streitigkeiten verwickelt. Sein Göttinger Mentor Max Born schrieb im Jahr 1920 über seinen ehemaligen Assistenten an Einstein: „Der Bericht über den ‚kleinen Pauli‘ ist nicht ganz vollständig. Ich erinnere mich, dass er lange zu schlafen liebte und mehr als einmal die Vorlesung um 11 Uhr verpasste. Wir schickten dann unser Hausmädchen um halb 11 zu ihm, um sicher zu sein, dass er auf sei. Er war ohne Zweifel ein Genius ersten Ranges; aber meine Besorgnis‚ einen so guten Assistenten werde ich nie mehr kriegen, war doch unberechtigt. Sein Nachfolger Heisenberg war ebenso genial und dabei gewissenhafter: ihn brauchten wir nicht wecken zu lassen oder sonst an seine Pflichten erinnern.“ Der so beschriebene Werner Heisenberg erhielt 1932 den Nobelpreis für Physik.

Was Physik betrifft, war Pauli als Perfektionist bekannt. Dies beschränkte sich nicht nur auf seine eigene Arbeit, sondern er geißelte auch Fehler seiner Fachkollegen unerbittlich. So wurde er zum "Gewissen der Physik," bezeichnete Arbeiten oft unverblümt als „ganz falsch“ oder steigerte seine Ablehnung etwa wie folgt: „Das ist nicht nur nicht richtig, es ist nicht einmal falsch!“. In Kollegenkreisen kursierten deshalb Witze wie etwa der folgende: „Nach Paulis Tod gewährte Gott ihm eine Audienz. Pauli fragte Gott, warum die Feinstrukturkonstante den Wert 1/137 habe. Gott nickte, ging zur Tafel und begann, Gleichung nach Gleichung in rasender Geschwindigkeit abzuleiten. Pauli sah zunächst mit großer Genugtuung zu, aber bald schon begann er heftig und entschieden, seinen Kopf zu schütteln …“ Bei einer Faust-Parodie, die die Physiker des Niels-Bohr-Instituts 1932 in Kopenhagen unter Leitung von dessen Autor Max Delbrück aufführten (das Skript hatte Illustrationen von George Gamow), standen Bohr für Gott (gespielt von Felix Bloch) und Pauli für Mephistopheles (gespielt von Léon Rosenfeld), das Neutrino war Gretchen.

Die Zeit Ende der 1920er Jahre war geprägt von persönlichen Problemen. Seine Mutter beging aufgrund einer Affäre seines Vaters Selbstmord, und mit der zweiten Frau seines Vaters kam er nicht zurecht. Pauli trat aus der Kirche aus, ging eine kurze Ehe mit einer Tänzerin ein und hatte Alkoholprobleme. Er begab sich von 1932 bis 1934 in psychoanalytische Behandlung bei einer Assistentin von Carl Gustav Jung, Erna Rosenbaum (1897–1957), einer englischen Ärztin, die sich gerade dem Kreis C. G. Jungs angeschlossen hatte. Erst die 1934 geschlossene Ehe mit Franziska „Franca“ Bertram (1901–1987) brachte Ruhe in sein Leben. Sie hatten keine Kinder.

Pauli war gefürchtet und berüchtigt wegen seiner oft schonungslos und respektlos auch gegenüber Freunden oder Fachautoritäten vorgetragenen Kritik. So schrieb er 1929 über die Arbeiten Albert Einsteins an seinen Kollegen Pascual Jordan in Hamburg: „Einstein soll im Berliner Kolloquium schrecklichen Quatsch über einen Fernparallelismus verzapft haben!“ und rezensierte 1931 dessen erneuten Versuch der Konstruktion einer vereinheitlichten Feldtheorie: „Es ist schon eine kühne Tat der Redaktion, ein Referat über eine neue Feldtheorie Einsteins unter die Ergebnisse der exakten Naturwissenschaften aufzunehmen. Beschert uns doch seine nie versagende Erfindungsgabe sowie seine hartnäckige Energie beim Verfolgen eines bestimmten Zieles in letzter Zeit durchschnittlich etwa eine solche Theorie pro Jahr – wobei es psychologisch interessant ist, dass die jeweilige Theorie vom Autor gewöhnlich eine Zeitlang als die ‚definitive Lösung‘ betrachtet wird.“ Mit seinem Kollegen Paul Ehrenfest, der wie Pauli einen Artikel in der "Enzyklopädie der mathematischen Wissenschaften" verfasst hatte, verband ihn eine herzliche Freundschaft, die die beiden aber nicht am Austausch bissiger "Bonmots" hinderte:Ehrenfest: „Herr Pauli, Ihr Enzyklopädieartikel gefällt mir besser als Sie selbst!“, daraufhin Pauli: „Das ist doch komisch, mir geht es mit Ihnen gerade umgekehrt!“Auch sonst machte Pauli gerne amüsierte oder maliziöse Kommentare über seine Kollegen. Über seinen Assistenten Rudolf Peierls meinte er: „Der Peierls, der spricht so schnell; bis man verstanden hat, was er sagt, behauptet er schon das Gegenteil!“ Eine andere Anekdote berichtet davon, dass der immer optimistische Werner Heisenberg seine von ihm aufgestellte "Einheitliche Feldtheorie" – über die er mit Pauli diskutiert hatte, der sich aber zunehmend davon distanzierte – im Radio als „Heisenberg-Pauli-Theorie“ vorstellte und sagte, sie stünde kurz vor der Vollendung, es fehlten „nur ein paar Details“. Pauli schickte darauf an George Gamow am 1. März 1958 eine Postkarte, auf der nur ein Quadrat gezeichnet war mit der Bemerkung „Ich kann malen wie Tizian.“ Darunter stand in kleiner Schrift: „Es fehlen nur die Details.“

Die einzige Person, die er von seiner Kritik ausnahm, war sein Lehrer Arnold Sommerfeld, den er verehrte und in dessen Gegenwart er wie ausgewechselt war: Er sprach ihn mit Herr Geheimrat an und war äußerst zuvorkommend und diplomatisch, wenn er eine abweichende Meinung formulierte.

Berüchtigt war Pauli bei Experimentalphysikern für seine handwerkliche Ungeschicklichkeit, ja sie argwöhnten sogar im Scherz, dass seine bloße Anwesenheit im Raum oder auch nur in derselben Stadt Laborgeräte zum Versagen brachte (oft thematisiert: „Pauli-Effekt“ genannt). Pauli besuchte in Wien das Bundesgymnasium XIX in der Gymnasiumstraße 83, 1190 Wien. In seiner Klasse war Richard Kuhn, der 1938 den Nobelpreis für Chemie erhielt. Man erzählt sich, dass in einer Physikstunde der Professor an der Tafel einen Fehler machte, diesen jedoch auch nach langem Suchen nicht fand. Zur großen Erheiterung der Klasse habe er dann verzweifelt gerufen: „Pauli, jetzt sagen Sie mir schon, wo der Fehler liegt, Sie wissen es doch längst.“

Paulis Kritik hatte bisweilen aber auch negative Folgen, in mehr als einem Fall hinderte sie andere Physiker, die sich auf sein Urteil verließen, an der Publikation bedeutender Resultate. Bekannt ist der Fall von Ralph Kronig im Fall des Spins.

Pauli war mit dem Tiefenpsychologen Carl Gustav Jung befreundet und diskutierte mit ihm dessen Arbeiten. Jung war an Paulis reichhaltigen Traum-Erfahrungen interessiert und Pauli regte der Kontakt mit Jung umgekehrt zu wissenschaftshistorischen und wissenschaftsphilosophischen Arbeiten an. Im Briefwechsel der beiden Forscher während der Jahre von 1932 bis 1958 wird deutlich, dass Wolfgang Pauli großen Anteil an der Konzeption des Begriffes Synchronizität hat, wie er von C. G. Jung eingeführt wurde, und darüber hinaus an der Konkretisierung der für Jungs Werk zentralen Begriffe des kollektiven Unbewussten sowie der Archetypen. Pauli interessierte sich besonders für die Genese von Johannes Keplers Ideen. Die bisherige Untersuchung seiner Aufzeichnungen belegt, dass Paulis Auseinandersetzung mit diesen Themen nicht einem rein akademischen Interesse entsprang, sondern in tiefgehendem eigenem Erleben wurzelte – der existentiellen Auseinandersetzung mit dem archetypischen „Geist der Materie“.

Pauli war auch nach seiner Scheidung in den 1930er Jahren bei Jung aufgrund von Beziehungs- und Alkoholproblemen in psychoanalytischer Behandlung, die 1934 abgeschlossen wurde. Die Behandlung übernahm er nach ersten Sitzungen, in denen er Pauli als ernsthaft gefährdet erkannte aufgrund einseitiger intellektueller Ausrichtung und Verlust des Kontakts zu seinem Gefühlsleben, nicht selbst, sondern beauftragte damit seine junge Schülerin Erna Rosenbaum. Der Grund war, dass er Paulis Träume, die nach Jung an Archetypen reich waren, unbeeinflusst von seinem (Jungs) eigenem Vorwissen aufgezeichnet haben wollte. Später diskutierte er die Traum-Archetypen allerdings mit Pauli persönlich. Nach dem Krieg nahm Pauli den Dialog mit Jung wieder auf, teilte mit diesem ein Interesse an Alchemie und hatte außerdem enge wissenschaftliche Kontakte zur Jung-Schülerin Marie-Louise von Franz. Diese war auch zeitweise seine Psychoanalytikerin.

Pauli lieferte viele wesentliche Beiträge zur modernen Physik, speziell auf dem Gebiet der Quantenmechanik. Sein Perfektionsdrang führte dazu, dass er vor der Publikation zögerte und seine Resultate stattdessen in intensiven Briefwechseln mit seinen Kollegen, insbesondere mit Niels Bohr, Werner Heisenberg (der seinerseits seine meisten Arbeiten vor der Publikation Pauli vorlegte) und Pascual Jordan, mit denen er eng befreundet war, weitergab (von ihm sind „nur“ 93 Artikel und 11 Bücher, aber über 2000 wissenschaftliche Briefe erhalten). Dass seine Ergebnisse so in die „Folklore“ der Physik eingingen, reichte Pauli oft völlig aus („Ich kann es mir leisten, nicht zitiert zu werden“). Wichtige Arbeiten sind u. a.:


In späteren Jahren ist er auch auf die Allgemeine Relativitätstheorie zurückgekommen und arbeitete an Kaluza-Klein-Theorien. Auch seine ETH-Vorlesungen aus den 1950er Jahren fanden weite Verbreitung.

Seine Assistenten waren u. a. Ralph Kronig, Felix Bloch, Rudolf Peierls, Hendrik Casimir, Markus Fierz, Josef-Maria Jauch, Nicholas Kemmer, Victor Weisskopf, Charles Enz, Res Jost. Robert Oppenheimer war ein Schüler von ihm.

An der ETH Zürich finden jedes Jahr „Wolfgang-Pauli-Vorlesungen“ statt.

Im Jahr 1969 wurde in Wien Penzing (14. Bezirk) die "Wolfgang-Pauli-Gasse" nach ihm benannt.
Die "Wolfgang-Pauli-Strasse" führt durch den Campus Hönggerberg der ETH Zürich.

Nach Wolfgang Pauli ist ein Mondkrater benannt.

An der Universität Hamburg ist der größte Hörsaal der Physikalischen Institute nach Wolfgang Pauli benannt.

Dem Roman "Gehen, ging, gegangen" von Jenny Erpenbeck ist ein Zitat von Wolfgang Pauli vorangestellt: „Gott schuf das Volumen, der Teufel die Oberfläche.“

Wolfgang Pauli war in seiner Gymnasialzeit ein Klassenkamerad von Richard Kuhn, wodurch sich das Kuriosum ergibt, dass aus ein und derselben Klasse zwei Nobelpreisträger hervorgingen.






</doc>
<doc id="5565" url="https://de.wikipedia.org/wiki?curid=5565" title="World Wide Web">
World Wide Web

Das World Wide Web [] () ( für „weltweites Netz“, kurz Web, WWW, selten und vor allem in der Anfangszeit und den USA auch W3) ist ein über das Internet abrufbares System von elektronischen Hypertext-Dokumenten, sogenannten Webseiten. Sie sind durch Hyperlinks untereinander verknüpft und werden im Internet über die Protokolle HTTP oder HTTPS übertragen. Die Webseiten enthalten meist Texte, oft mit Bildern und grafischen Elementen illustriert. Häufig sind auch Videos, Tondokumente und Musikstücke eingebettet.

Umgangssprachlich wird das World Wide Web oft mit dem Internet gleichgesetzt, es ist jedoch jünger und stellt nur eine von mehreren möglichen Nutzungen des Internets dar. Andere Internet-Dienste wie E-Mail, IRC oder Telnet sind nicht in das World Wide Web integriert.

Zum Aufrufen von Inhalten aus dem World Wide Web wird ein Webbrowser benötigt, der z. B. auf einem PC oder einem Smartphone läuft. Mit ihm kann der Benutzer die auf einem beliebigen, von ihm ausgewählten Webserver bereitgestellten Daten herunterladen und auf einem geeigneten Ausgabegerät wie einem Bildschirm oder einer Braillezeile anzeigen lassen. Der Benutzer kann dann den Hyperlinks auf der angezeigten Webseite folgen, die auf andere Webseiten verweisen, gleichgültig ob diese auf demselben Webserver oder einem anderen gespeichert sind. So ergibt sich ein weltweites Netz aus Webseiten. Das Verfolgen der Hyperlinks wird auch als „Surfen im Internet“ bezeichnet.

Mit dem so genannten Web 2.0 wurden ab etwa den 2000er Jahren Webseiten populär, deren Inhalt der Nutzer nicht nur wie etwa bei Nachrichten-Seiten passiv ansehen, sondern selbst ändern und ergänzen kann, z. B. um eigene Inhalte zu veröffentlichen oder mit anderen Nutzern zu kommunizieren. Dazu zählen Blogs als private Meinungsseiten, von einer losen Autorengemeinschaft geschaffene Seiten nach dem Wiki-Prinzip und Soziale Netzwerke. Serverseitige Techniken und (Skript-)Sprachen, die diese Interaktivität umsetzen, sind vor allem CGI, Python, ASP, Apache Wicket, JSF, ColdFusion, Ruby und SSI. Zu clientseitigen Techniken, die z. B. über Filter die Inhalte individualisieren, gehören unter anderem CSS, JavaScript oder Java, wobei Java hauptsächlich zur plattformneutralen Ausführung von Programmen dient, die oft als Webanwendungen über das Internet geladen werden und mit internetbasierenden Datenbanken (z. B. SAP-Clients) kommunizieren. Mit der Interaktivität wurde der Einsatz von Suchmaschinen möglich, die die bis dato vorhandenen Webverzeichnisse ergänzten und bis heute weitgehend verdrängten.

Mit der zunehmenden Komplexität von Formaten, Protokollen und Techniken entstanden neue Berufsbilder, wie z. B. Webdesigner und Mediamatiker. Zu ihren Aufgaben gehört neben der Programmierung von Inhalten auch die Auswertung von Nutzerverhalten im Rahmen der Logdateianalyse.

Das WWW wurde unter Weiterentwicklung bekannter ähnlicher Konzepte 1989 von Tim Berners-Lee und Robert Cailliau am europäischen Forschungszentrum CERN in Genf entwickelt. Berners-Lee entwickelte dazu das HTTP-Netzwerkprotokoll und die Textauszeichnungssprache HTML. Zudem programmierte er den ersten Web-Browser und die erste Webserver-Software. Er betrieb auch den ersten Webserver der Welt auf seinem Entwicklungsrechner vom Typ NeXTcube. Das Gesamtkonzept wurde der Öffentlichkeit 1991 unter Verzicht auf jegliche Patentierung oder Lizenzzahlungen zur freien Verfügung gestellt, was erheblich zur heutigen Bedeutung beitrug.

Die weltweit erste Webseite "info.cern.ch" wurde am 6. August 1991 veröffentlicht. Eine Nachbildung dieser Seite ist über nachfolgenden Link erreichbar:

Das WWW führte zu umfassenden, oft als revolutionär beschriebenen Umwälzungen in vielen Lebensbereichen, zur Entstehung neuer Wirtschaftszweige und zu einem grundlegenden Wandel des Kommunikationsverhaltens und der Mediennutzung. Es wird in seiner kulturellen Bedeutung, zusammen mit anderen Internet-Diensten wie E-Mail, teilweise mit der Erfindung des Buchdrucks gleichgesetzt.

Das Web entstand 1989 als Projekt an der Forschungseinrichtung CERN, in der Nähe von Genf auf schweizerischem und französischem Gebiet liegend, an dem Tim Berners-Lee ein Hypertext-System aufbaute.
Die Idee hierzu stellte er erstmals am 12. März 1989 in der Forschungseinrichtung vor. Das Konzept wurde von dem Belgier Robert Cailliau mitentworfen. Das ursprüngliche Ziel des Systems war es, Forschungsergebnisse auf einfache Art und Weise mit Kollegen auszutauschen. Eine Methode dafür war das „Verflechten“ von wissenschaftlichen Artikeln – also das Erstellen eines Webs. In Berners-Lees eigenen Worten:

Das dem Hypertext zugrunde liegende Konzept stammt von früheren Entwicklungen ab, wie Ted Nelsons Projekt Xanadu, Vannevar Bushs „memex“ Maschinenidee und dem Note Code Project.

Das World Wide Web unterscheidet sich von damaligen Hypertext-Systemen (Note Code benutzte beispielsweise eine einfache und lesbare Syntax und semantische Deskriptoren). Das WWW benötigt nur unidirektionale Links statt bidirektionaler, was es ermöglicht, einen Link auf eine Ressource zu setzen, ohne dass deren Besitzer eingreifen muss. Zudem, anders als andere Protokolle wie HyperCard oder Gopher, baut das World Wide Web auf einem freien Protokoll auf, was die Entwicklung von Servern und Clients ohne Beschränkungen durch Lizenzen möglich machte. Tim Berners-Lee machte das "World Wide Web"-Projekt am 6. August 1991 mit einem Beitrag zur Newsgroup "alt.hypertext" öffentlich und weltweit verfügbar.

Das erste Web-Anzeigeprogramm, das eher ein Browser-Editor-Hybrid war, nannte Berners-Lee einfach „WorldWideWeb“. Er hatte es im Herbst 1990 auf einem NeXT-Computer geschrieben. Später benannte er es – um Verwechslungen mit dem World Wide Web (mit Leerzeichen) zu vermeiden – in „Nexus“ um. Es konnte damals nur Text anzeigen, aber spätere Browser wie Pei Weis Viola (1992) fügten die Fähigkeit Grafiken anzuzeigen hinzu. Marc Andreessen vom NCSA veröffentlichte im Jahre 1993 einen Browser namens „Mosaic für X“, der bald dem Web und auch dem gesamten Internet ungekannte Popularität jenseits der bisherigen Nutzerkreise und ein explosionsartiges Wachstum bescherte. Marc Andreessen gründete die Firma „Mosaic Communications Corporation“, später „Netscape Communication“. Mittlerweile können moderne Browser auch zusätzliche Merkmale wie dynamische Inhalte, Musik, Animationen und Videos wiedergeben.

In Berners-Lees erstem Projektentwurf vom März 1989 hieß das Web noch "Mesh" (engl. "Geflecht"). Der Name wurde aber schnell verworfen, da er zu sehr an "Mess" (engl. "Unordnung") erinnert. Die folgenden Benennungsversuche "Mine of Information" ( "Informations-Mine") oder "The Information Mine" hatten keinen Bestand, da die Abkürzungen MOI ( "ich") und TIM zu egozentrisch wirkten. Außerdem war eine Mine ein nur teilweise geeignetes Bild, da man aus ihr bloß etwas herausholen kann, das Web dagegen sowohl Informationen liefern als auch mit ihnen befüllt werden sollte.

Schließlich legte Berners-Lee sich auf "Web" und "World Wide Web" fest, obwohl er von Kollegen gewarnt wurde, dass die im Englischen und Französischen zungenbrecherische Abkürzung "WWW" den Projekterfolg gefährden würde. "Web" erschien ihm als Bild besonders passend, da es in der Mathematik ein Netz von Knoten ( "Nodes") bezeichnet, von denen jeder mit jedem verbunden sein kann.

Das WWW basiert auf drei Kernstandards:

Folgende Standards kamen später dazu:

Das World Wide Web Consortium (W3C), das heute vom Erfinder des WWW Tim Berners-Lee geleitet wird, und andere entwickeln den HTML- und den CSS-Standard; andere Standards kommen von der Internet Engineering Task Force, der ECMA und Herstellern wie Sun Microsystems. Nicht vom W3-Konsortium standardisiert ist die am weitesten verbreitete Skript- oder Makrosprache von Webbrowsern:

Das WWW wurde und wird durch andere Techniken ergänzt. Schon früh wurden Bilder zur Illustration benutzt; man verwendete die Formate GIF für Grafiken und Animationen und JPEG für Fotos. Später wurde GIF mehr und mehr von PNG verdrängt, da für dessen Verwendung – im Gegensatz zu GIF – keine Lizenzgebühren anfielen.

Zudem können in Browsern zahlreiche weitere Dateitypen durch Browsererweiterungen, so genannte Plug-ins, dargestellt werden. Dadurch lassen sich Multimediainhalte von Animationen bis hin zu Musik und Videos oder ganze Anwendungen wie zum Beispiel Versicherungsrechner oder Navigationsoberflächen darstellen. Ferner ermöglichen Java-Applets das Einbetten von Programmen, die auf dem Computer des WWW-Benutzers ablaufen.

Weitere beliebte Formate sind PDF zum Anzeigen von Dokumenten und Flash für interaktive Inhalte oder Animationen.

Mit Hilfe der dynamischen WWW-Seiten kann das WWW als Oberfläche für verteilte Programme dienen: Ein Programm wird nicht mehr konventionell lokal auf dem Rechner gestartet, sondern ist eine Menge von dynamischen WWW-Seiten, die durch einen Webbrowser betrachtet und bedient werden können. Vorteilhaft ist hier, dass die Programme nicht mehr auf den einzelnen Rechnern verteilt sind und dort (dezentral) administriert werden müssen.

Dynamische Webanwendungen werden entweder am Webserver oder direkt im Browser ausgeführt.

Nachteilig sind die begrenzten Ausdrucksmöglichkeiten von WWW-Seiten, so dass Programme in Form von Internetseiten im Allgemeinen nicht so einfach bedient werden können wie konventionelle Programme. Ein Trend, der versucht, beides in Einklang zu bekommen, sind Rich Internet Applications.

Zurzeit ist zu beobachten, dass immer mehr Dienste, die ursprünglich vom WWW getrennt waren und als eigenes Programm liefen, über das WWW angeboten werden und mittels eines Browsers genutzt werden können:

So wird Webmail oft als E-Mail-Client oder WebFTP als FTP-Client genutzt; Webforen ersetzen das Usenet und Webchats den IRC.

Oft führten Browser-Hersteller neue Möglichkeiten ein, ohne auf eine Standardisierung zu warten. Umgekehrt werden jedoch immer noch nicht alle Teile von Standards wie HTML oder CSS korrekt implementiert. Das führt zu Inkompatibilitäten zwischen bestimmten Webseiten und manchen Browsern. Besonders „hervorgetan“ durch solche Inkompatibilitäten hatte sich zu Beginn des Internet-Booms die Firma Netscape, heute vor allem das Unternehmen Microsoft mit seinem Internet Explorer.

Außerdem ging durch die Vielzahl der Ad-Hoc-Erweiterungen von HTML ein wesentlicher Vorteil dieser Sprache verloren – die Trennung von Inhalt und Darstellung. Durch diese Trennung können die in HTML ausgezeichneten Inhalte optimal für das jeweilige Ausgabegerät – ob Bildschirm, Display des Mobiltelefons oder Sprachausgabe (für Benutzer mit Sehschwierigkeiten) – aufbereitet werden.

Das W3C und andere Initiativen treiben daher die Entwicklung in die Richtung XHTML/XML und CSS voran, um diese Vorteile von HTML wiederzuerlangen. Die fortschreitenden Bemühungen um die Barrierefreiheit von Internetseiten unterstützen diesen Trend.





</doc>
<doc id="5567" url="https://de.wikipedia.org/wiki?curid=5567" title="Weide">
Weide

Weide steht für:

Weide, geographische Objekte:

Weide, Personen:

Siehe auch:


</doc>
<doc id="5569" url="https://de.wikipedia.org/wiki?curid=5569" title="Washington, D.C.">
Washington, D.C.

Der District of Columbia oder Washington, D.C. [] ist Bundesdistrikt, Regierungssitz und seit 1800 die Hauptstadt der Vereinigten Staaten. Der Distrikt ist kein Bundesstaat und gehört auch zu keinem, er ist vielmehr dem Kongress der Vereinigten Staaten direkt unterstellt. Trotz Namensgleichheit mit dem Bundesstaat Washington wird Washington, D.C. im deutschen Sprachraum meist nur „Washington“ genannt. D.C. steht dabei für District of Columbia.

Bei der Volkszählung 2010 hatte Washington, D.C. 601.723 Einwohner. Das United States Census Bureau schätzte die Einwohnerzahl zum 1. Juli 2015 auf 672.228 Einwohner, nach einem stetigen Abfall der Bevölkerung seit 1950 der erste größere Zuwachs. Der Großraum Washington hatte 5.582.170 Einwohner. Zusammen mit dem benachbarten Großraum Baltimore hatte die Region laut Zensus insgesamt 8.572.971 Einwohner.

Mit dem Weißen Haus als Amts- und Wohnsitz des Präsidenten und dem Kapitol, das den Kongress (bestehend aus Senat und Repräsentantenhaus) beherbergt, sowie dem Obersten Gerichtshof befinden sich die Spitzen aller drei verfassungsmäßigen Gewalten in der Stadt. Washington ist darüber hinaus Sitz des Internationalen Währungsfonds, der Weltbank und der Organisation Amerikanischer Staaten.

Die offizielle Bezeichnung der amerikanischen Hauptstadt ist "District of Columbia". "Columbia", abgeleitet vom Namen des Seefahrers Kolumbus, war zur Zeit der Namensgebung eine gebräuchliche poetische Bezeichnung für Amerika. Die Stadt Washington ist nach George Washington benannt, dem Oberbefehlshaber im Unabhängigkeitskrieg und ersten Präsidenten der Vereinigten Staaten. Als die Städte Washington und Georgetown sowie der Washington County im District of Columbia Organic Act von 1871 aufgehoben wurden, wurde festgelegt, dass der Teil des Distrikts, in dem die bisherige Stadt Washington lag, weiterhin als Washington bezeichnet werden solle. Als pars pro toto setzte Washington, D.C. sich dann als übliche Bezeichnung für den gesamten Distrikt durch.

Washington, D.C. befindet sich nahe der Ostküste des Landes, etwa 35 km westlich der Chesapeake Bay, einer Bucht des Atlantischen Ozeans. Die Höhe über dem Meeresspiegel variiert zwischen 0 und 125 m. Die Stadt liegt an der Mündung des Anacostia River in den Potomac River, und zwar am linken Ufer des Potomac zwischen den Bundesstaaten Maryland im Nordosten und Virginia im Südwesten.

Der District of Columbia wurde aus von Maryland und Virginia abgetretenem Land gebildet, um die Bundesregierung und den Kongress dem Zugriff der damals noch sehr mächtigen Einzelstaaten zu entziehen und eine städtebaulich durchgeplante, moderne und repräsentative Hauptstadt der neuen Republik zu bilden.

Der ursprüngliche Distrikt hatte eine Fläche von 100 Quadratmeilen (258,9 km²), was die in der Verfassung der Vereinigten Staaten vorgeschriebene Obergrenze darstellt. Er war ein Quadrat mit 10 Meilen (16,1 km) Kantenlänge, dessen Ecken genau in die vier Himmelsrichtungen zeigen.

Das auf dem westlichen Ufer des Potomac gelegene Gebiet, welches ursprünglich vom Staat Virginia abgetreten worden war, wurde 1846 an diesen zurückgegeben, da die Stadt weniger schnell gewachsen war als erwartet (heute Arlington County und teilweise Alexandria). Hierdurch verkleinerte sich die Fläche auf 177 km². Der Distrikt besteht seitdem nur noch aus Gebieten, die ursprünglich von Maryland kamen.

Geographisch ist Washington in die vier Quadranten Northwest (NW), Southwest (SW), Northeast (NE) und Southeast (SE) eingeteilt, deren Grenzen am Kapitol aufeinandertreffen. Politisch ist die Stadt in acht Bezirke "(wards)" eingeteilt, die jeweils eigene Repräsentanten in den Stadtrat wählen.

Die Straßen sind überwiegend gerade angelegt und durchnummeriert. Die von Ost nach West verlaufenden Straßen sind alphabetisch geordnet (vgl. Mannheimer Quadrate), die in Nord-Süd-Richtung sind durchnummeriert. Die Nummerierung bzw. Alphabetisierung beginnt in allen Richtungen am Kapitol. Die großen diagonal verlaufenden Straßen werden als Avenues bezeichnet und sind zumeist nach Bundesstaaten benannt.

Die bekanntesten Gebäude sind das Weiße Haus und das Kapitol. Das ebenso bekannte Pentagon liegt jedoch außerhalb der Stadt in Arlington. Es gibt in Washington keine Wolkenkratzer, weil kein Gebäude höher sein darf als die Breite der angrenzenden Straße plus 6,1 m. Drei Gebäude fallen jedoch nicht unter diese Regelung, da sie fertiggestellt oder zumindest geplant waren, ehe das Gesetz Anfang des 20. Jahrhunderts in Kraft trat: das Washington Monument, der Turm des Old Post Office und die Washington National Cathedral.

Washington befindet sich in der subtropischen Klimazone mit kontinentalen Einflüssen im Winter und hat nach Köppen ein feucht-subtropisches Klima (effektive Klimaklassifikation: Cfa).

Die durchschnittliche Jahrestemperatur beträgt 14 °C, die Niederschlagssumme 981 mm. Die größten Niederschlagsmengen werden im Juli und August erreicht. Im Schnitt sind 36,7 Tage heißer als 32 °C und 64,4 Nächte kälter als 0 °C.

Als im 17. Jahrhundert die Europäer erstmals auf dem Gebiet des heutigen District of Columbia ankamen, war es von einem Indianerstamm, den Nacotchtank, bewohnt, die am Anacostia River siedelten. 1749 wurde am Potomac die Stadt Alexandria als Teil der Kolonie Virginia gegründet, 1751 etwas weiter nördlich und auf der anderen Flussseite die nach König Georg II. benannte Stadt Georgetown als Teil der Kolonie Province of Maryland.

1788 argumentierte James Madison in den Federalist Papers (Nr. 43), dass die zukünftige Bundesregierung die Kontrolle über die Bundeshauptstadt haben müsse. In der Verfassung der Vereinigten Staaten wurde dem Kongress das Recht zugesprochen, einen „District“ mit einer Größe von 10 auf 10 Meilen für den Regierungssitz per Gesetz festlegen zu können. Die erste Hauptstadt nach der Ratifizierung der Verfassung war New York City (1788–1790). George Washington legte den Amtseid als erster Präsident der Vereinigten Staaten auf dem Balkon des alten Rathauses ab. Im Residence Act von 1790 wurde beschlossen, Philadelphia für zehn Jahre zur Hauptstadt zu machen und sich in der Zwischenzeit nach einem permanenten Platz am Potomac umzusehen. Präsident Washington wählte ein Gebiet, das sowohl Teile von Maryland als auch von Virginia umschloss. Zu jener Zeit bestand das Gebiet in erster Linie aus Wiesen- und Sumpfland. Es wurde geplant, dass der Kongress in der neuen Hauptstadt am ersten Montag im Dezember 1800 tagen sollte. Washington ist also eine Planhauptstadt.

1791 wurde das Gebiet des District of Columbia aus den Staaten Maryland und Virginia herausgenommen. Es ist beiderseits des Potomac gelegen und war ursprünglich ein Quadrat von genau 10 Meilen mal 10 Meilen (16,09 km mal 16,09 km) Seitenlänge. Die Lage kam durch einen Handel von Thomas Jefferson, der aus Virginia stammte, mit Alexander Hamilton, dessen Heimat New York City ursprünglich Sitz der Regierung war, zustande: Jefferson unterstützte Hamiltons Pläne einer Nationalbank, dafür stimmte dieser einer in den Südstaaten gelegenen Hauptstadt zu.

Pierre Charles L’Enfant wurde beauftragt, die „Federal City“ zu gestalten. Als Inspiration legte ihm Thomas Jefferson verschiedene Stadtpläne vor, die er von seiner Europareise 1788 mitgebracht hatte, darunter Pläne von Frankfurt am Main, Karlsruhe, Amsterdam, Paris, Orléans, Montpellier, Turin und Mailand. L’Enfant entwickelte eine erste Version für einen Stadtplan, überwarf sich dann aber mit den Auftraggebern aus dem Kongress, so dass er von dem Projekt abberufen wurde. Die weitere Planung wurde dann in die Hände des Landvermessers Andrew Ellicott gelegt, der die ursprünglichen Pläne von L’Enfant stark modifizierte. Da die Straßen aber vom Kapitol und von der Union Station abgehen, weist der verwirklichte Plan deutliche Parallelen zum Fächergrundriss von Karlsruhe auf, wo Thomas Jefferson während seiner Deutschlandreise am 15. April 1788 war.

Der Bau der neuen Hauptstadt wurde mit dem künftigen Amtssitz der US-Präsidenten, dem Weißen Haus, am 13. Oktober 1792 begonnen. Noch heute vermerkt dort eine Gedenktafel: „Dieser Grundstein des Hauses des Präsidenten wurde im 17. Jahr der Unabhängigkeit der Vereinigten Staaten von Amerika am 13. Oktober 1792 gelegt. Präsident: George Washington, Kommissionäre: Thomas Johnson, Doktor Stewart, Daniel Carroll, Architekt: James Hoban, Baumeister: Collen Williamson. Vivat Republica“

Am 11. Juni 1800 wurde Washington ständige Hauptstadt der Vereinigten Staaten. Präsident John Adams siedelte mit seiner Regierung im Juni 1800 nach Washington um. Im November 1800 trat der Kongress zum ersten Mal in der neuen Hauptstadt zusammen.

Mit dem District of Columbia Organic Act von 1801 kam der District of Columbia unter die direkte Verwaltung des Bundeskongresses. Die Städte Alexandria und Georgetown wurden in den District of Columbia integriert, das Gebiet nordöstlich des Potomac wurde als Washington County organisiert, das Gebiet südwestlich des Potomac als Alexandria County.

Da Wahlen damals von den Bundesstaaten organisiert wurden, hatten die Bewohner des District of Columbia keinerlei Wahlrecht. Da es sich zum großen Teil um Beamte bzw. Regierungsangestellte handelte, entsprach dies in gewisser Weise auch dem Prinzip der Gewaltenteilung. Mit dem Wachsen der Stadt Washington und der Zunahme nicht bei der Regierung beschäftigter Einwohner wurde dies aber zunehmend als undemokratischer Anachronismus gesehen.

Am 24. August 1814, während des Britisch-Amerikanischen Krieges, wurde die Stadt von einer 4500 Mann starken britischen Streitmacht aus Armee- und Marineeinheiten erobert. Dabei wurde unter anderem das Kapitol zerstört und das Weiße Haus beschädigt. Präsident James Madison musste mit seiner Regierung nach Virginia fliehen. Die Washington-Kampagne der Briten vom 19. bis 29. August 1814 hatte eher symbolischen Charakter und sollte den Amerikanern verdeutlichen, sich nicht mit Großbritannien anzulegen (“Britain is not a country to mess around with”).

Seit den 1830er Jahren gab es Bestrebungen, das westlich des Potomac gelegene Alexandria County wieder Virginia anzugliedern. Gründe waren der Verlust des Wahlrechts durch den besonderen Status des District of Columbia, der wirtschaftliche Niedergang durch den ausschließlichen Bau von Bundesgebäuden auf der Maryland zugewandten Seite des Potomac und die Angst, die wirtschaftlich bedeutende Sklaverei könne im District of Columbia verboten werden. Nachdem die Virginia General Assembly im Februar 1846 zugestimmt hatte, das Gebiet zurückzunehmen, beschloss der Kongress im Juli 1846, ein Referendum über die Rückgabe abzuhalten. Im September 1846 stimmten die Einwohner von Alexandria mit 763 zu 222 für die Rückgabe, die Einwohner des Alexandria County mit 106 zu 29 dagegen; Präsident James K. Polk proklamierte daraufhin die Rückgabe. Wegen Vorbehalten, dass die Einwohner von Alexandria County nicht ausreichend berücksichtigt worden seien, akzeptierte Virginia die Rückgabe erst nach langer Debatte am 13. März 1847.

Im Kompromiss von 1850 wurde für den District of Columbia der Sklavenhandel, nicht aber die Haltung verboten. Im Sezessionskrieg von 1861 bis 1865 stieg die Einwohnerzahl durch den gestiegenen Bedarf an Bundesbeamten und durch geflohene Sklaven stark an. 1862 wurde durch den Compensated Emancipation Act die Sklaverei durch obligatorischen staatlichen Freikauf beendet, noch vor der Emanzipations-Proklamation im folgenden Jahr.

Im District of Columbia Organic Act von 1871 wurden die Städte Washington und Georgetown sowie der Washington County aufgehoben und der District of Columbia unter einheitliche Verwaltung gestellt.

1902 stellte eine Senats-Kommission einen Masterplan für die Entwicklung von Washington vor, der nach dem Vorsitzenden der Kommission, James McMillan aus Michigan als McMillan Plan bekannt wurde. Dabei wurden insbesondere die bisherigen viktorianischen Parkanlagen durch die National Mall in ihrer heutigen Form als offener Raum mit flankierenden öffentlichen Gebäuden ersetzt. Im Zuge des New Deal wurden in Washington ab den 1930er Jahren zahlreiche neue Gebäude errichtet oder saniert.

Die Stadt hat seit 1974 einen Stadtrat und wählt einen Bürgermeister. Jedoch hat diese Volksvertretung nur eingeschränkte Kompetenzen. Der Kongress hat jederzeit die Möglichkeit, über diese lokale Volksvertretung hinweg Beschlüsse für die Hauptstadt zu fassen. Auch kann er den Stadtrat auflösen.

Die Wahlrechte der Bürger von Washington sind auch auf nationaler Ebene eingeschränkt. Erst seit dem 23. Verfassungszusatz, der 1961 in Kraft trat, dürfen die Einwohner des District of Columbia den Präsidenten mitwählen. Die Zahl der ihnen zustehenden Wahlmänner ist jedoch auf die des bevölkerungsärmsten Staates beschränkt. Dadurch stellt der District of Columbia drei Wahlmänner; allerdings wären es momentan auch ohne diese Klausel nicht mehr. Im Repräsentantenhaus ist der District seit 1970 mit einem nicht stimmberechtigten Beobachter vertreten, im Senat gar nicht. Dadurch ergibt sich die weltweit einmalige Besonderheit, dass die Bewohner der Hauptstadt eines demokratischen Staates ihr Parlament nicht mitwählen dürfen.

1978 wurde ein Verfassungszusatz vom Kongress verabschiedet, der den Bürgern Washingtons die gleiche Vertretung im Kongress gestattet hätte, als wäre der District of Columbia ein Staat. Statt der erforderlichen Dreiviertelmehrheit von 38 Staaten wurde der Verfassungszusatz aber nur von 16 Staaten innerhalb der siebenjährigen Frist ratifiziert. Im Januar 2009 wurde ein Gesetzesvorschlag für den „District of Columbia House Voting Rights Act of 2009“ in den Kongress eingebracht, der den District of Columbia zu einem Wahlkreis für das Repräsentantenhaus machen würde. Eine Repräsentation im Senat ist jedoch nicht vorgesehen. Der Gesetzesvorschlag wurde vom Senat mit deutlicher Mehrheit beschlossen, kam jedoch ins Stocken, als Senator John Ensign aus Nevada einen Gesetzeszusatz vorschlug, der dem District of Columbia das Recht, Waffenbesitz zu beschränken, entziehen würde. Dieser Zusatz wurde ebenso beschlossen. Im Repräsentantenhaus konnte bislang keine Einigkeit darüber erzielt werden, wie mit den beiden Vorschlägen verfahren werden soll.

Im Jahr 2015 wurde die Bevölkerung auf 672.228 Personen geschätzt, eine Zunahme seit dem letzten Census 2010 von 11,7 %. Washington, D.C. hatte 2010 offiziell 601.723 Einwohner (US Census 2010), davon 38,5 % Weiße, 50,7 % Schwarze oder Afroamerikaner, 3,5 % Asiaten, 0,3 % Indianer, 0,1 % Hawaiianer oder von anderen Pazifikinseln stammend. 2,9 % zwei oder mehr Gruppen zugehörig. 9,1 % der Gesamtbevölkerung waren Hispanics oder Latinos jedweder Ethnie.

Die mitgliederstärksten Religionsgemeinschaften waren im Jahre 2000 die römisch-katholische Kirche mit 160.048, die American Baptist Churches USA mit 51.836, die Southern Baptist Convention mit 38.852 und die anglikanische Episcopal Church mit 19.698 Anhängern. 60.479 Einwohner waren islamischen und 25.500 jüdischen Glaubens.

Durch seinen einzigartigen Status als Bundesdistrikt ist die Politik von Washington, D.C. und die politische Vertretung der Bewohner durch einige Besonderheiten ausgezeichnet:

Washington untersteht direkt dem Kongress der Vereinigten Staaten, der die endgültige Entscheidungsgewalt hat.

Von 1802 bis 1871 hatte der Distrikt eine Form der kommunalen Regierung, bei der die Verwaltungsstrukturen von Georgetown, einer zum Stadtteil gewordenen ehemaligen Stadt auf Distriktsgebiet, erhalten blieben. Hierdurch gab es getrennte Verwaltungen für Georgetown, die Stadt Washington, den Bezirk Washington und – bis zur Zurückgabe an Virginia – die Stadt Arlington. Bestimmte Aufgaben wurden jedoch gemeinsam übernommen, wie z. B. die Verwaltung der 1861 gegründeten Stadtpolizei. Dies erwies sich als ineffizient, so dass die Infrastruktur mit dem wachsenden Washington nicht mithielt und der Lebensstandard sank.

1871 wurde daher die Stadtregierung durch den Kongress reformiert, bei dem eine gemeinsame Regierung für den gesamten Distrikt geschaffen wurde. Diese bestand aus einem vom Präsidenten ernannten elfköpfigen Oberhaus sowie einem vom Volk gewählten Unterhaus mit 22 Mitgliedern. Weiterhin gab es eine Modernisierungsbehörde. Wie in Bundesstaaten gab es auch einen Gouverneur, der aber vom Präsidenten ernannt wurde. Die umfänglichen Modernisierungsmaßnahmen führten schnell zum finanziellen Kollaps der Stadt, weswegen schon nach zwei Gouverneuren im Jahr 1874 die vom Volk gewählte Regierung abgeschafft wurde.

Von 1874 bis 1967 wurde die Stadt von einem dreiköpfigen Komitee geführt: zwei vom Präsidenten nach Zustimmung des Senats ernannten Kommissaren und einem Ingenieur vom United States Army Corps of Engineers. Einer der drei wurde als Vorsitzender bestimmt und übernahm die vormalige Rolle des Gouverneurs. Ab 1967 ernannte der Präsident einen Bürgermeister und 9 Stadträte. Mehrere Versuche, eine echte Volksvertretung einzuführen, scheiterten in den Jahren 1948 bis 1968.

Erst 1973 wurde ein Gesetz verabschiedet, das der Stadt einen Bürgermeister und einen Stadtrat mit 13 Stadträten gab. Die Stadt ist in 8 Stimmbezirke aufgeteilt, die jeweils einen Stadtrat wählen. Die restlichen 5 Räte werden von der ganzen Stadt gewählt. Weiterhin gibt es beratende Nachbarschaftskomitees.

Alle Gesetze, die vom Stadtrat beschlossen werden, bedürfen der nachfolgenden Zustimmung des Kongresses. Bestimmte Befugnisse sind dem Stadtrat auch ausdrücklich genommen. So dürfen z. B. die Zuständigkeiten der Gerichte im Distrikt nicht verändert werden. Die gesetzliche Höhenbeschränkung für Gebäude im Distrikt darf ebenso nicht verändert werden.

Der Distrikt wählt einen Delegierten für das Repräsentantenhaus, der zwar in Ausschüssen mitstimmen darf, aber nicht bei allgemeinen Abstimmungen. Im Senat hat der Distrikt keinerlei Vertretung.

Die Einwohner Washingtons dürfen seit 1964 an den Wahlen zum Präsidenten teilnehmen. Die Zahl der zu bestimmenden Wahlmänner wird hierbei anhand der Bevölkerung so berechnet, als sei der Distrikt ein Bundesstaat. Jedoch darf er keinesfalls mehr Delegierte haben als der kleinste Staat. Mehr stünden der Stadt bei der derzeitigen Bevölkerungszahl aber auch ohne diese Beschränkung nicht zu.

Im Verlauf der relativ kurzen Präsidentschaftswahlgeschichte Washingtons sprach sich die Mehrheit der Bevölkerung stets für die demokratischen Kandidaten aus. Bei der Wahl 2008 erhielt Barack Obama hier 94 Prozent der abgegebenen Stimmen. Die Bürgermeisterwahlen verlaufen in der Regel ähnlich.

Mit Ausnahme der Teilnahme an Präsidentschaftswahlen haben die Einwohner Washingtons keine garantierten Wahlrechte und sind im Vergleich zu den Bewohnern der 50 Bundesstaaten erheblich eingeschränkt. Dies wird immer wieder bemängelt, vor allem von den Einwohnern und Kommunalpolitikern in Washington. Auf den Kfz-Kennzeichen Washingtons wurde hierzu auch der Slogan "Taxation without representation" („Besteuerung ohne Repräsentation“) verwendet. Dieser lehnt sich an den Slogan "No taxation without representation" aus der amerikanischen Unabhängigkeitsbewegung an, der die fehlende politische Vertretung der britischen Kolonien in Nordamerika anprangerte, obwohl diese genauso Steuern entrichteten. Analog wird hiermit in Washington darauf hingewiesen, dass die Bewohner Washingtons Bundessteuern zahlen, ohne eine politische Vertretung im Bund zu haben.

Daher gab und gibt es immer wieder Vorschläge, diese Ungleichbehandlung zu beseitigen. Ein Hindernis ist, dass der Distrikt eine Hochburg der Demokraten ist, wodurch verbesserte Wahlrechte der Bewohner gleichbedeutend mit einem Stimmengewinn für diese Partei wären, was wiederum nicht im Interesse der anderen großen Partei, der Republikaner, ist.

Eine Variante, die schon zahlreiche Male in den Kongress eingebracht wurde, ist die Herstellung einer Repräsentation per Gesetz. Ein Versuch, den Distrikt wie einen Bundesstaat entsprechender Größe zu behandeln, scheiterte zuletzt 2003 schon früh im Gesetzgebungsprozess. Spätere Vorschläge beschränkten sich darauf, eine Vertretung im Repräsentantenhaus herzustellen. Teilweise wurde versucht, dem Bundesstaat Utah, einer republikanischen Hochburg, weitere Mandate zu geben, um parteipolitische Verschiebungen zu kompensieren. Der letzte Anlauf hierzu wurde 2009 unternommen und scheiterte an Zusätzen, die nicht für beide Kammern des Kongresses annehmbar waren. Zudem ist die Verfassungsmäßigkeit eines solchen Wahlgesetzes umstritten.

Ein verfassungsrechtlich unzweifelhafter Vorschlag ist, einen Verfassungszusatz zu verabschieden, der den Einwohnern des District of Columbia Wahlrechte zum Kongress gibt. Dies wurde schon erfolgreich im Jahr 1961 beim 23. Verfassungszusatz für das Wahlrecht in den Präsidentschaftswahlen durchgeführt.

Ein entsprechender Zusatz wurde 1978 mit großer Mehrheit von beiden Kammern des Kongresses angenommen. Dieser sah die Aufhebung des 23. Verfassungszusatzes vor und eine Gleichstellung des Distrikts bei Wahlen zum Kongress und Präsidentschaftswahlen, indem Washington wie ein Staat entsprechender Größe behandelt worden wäre. Um in Kraft zu treten, bedurfte dies der Zustimmung von drei Vierteln aller Bundesstaaten, also 38 von 50, innerhalb von sieben Jahren. Dies wurde jedoch deutlich verfehlt. Bis zum Stichdatum im Jahr 1985 hatten lediglich 16 Staaten zugestimmt.

Zuletzt wurde 2009 ein solcher Vorschlag von der Senatorin Lisa Murkowski eingebracht.

Es gibt auch die Idee, den District wieder zu einem Teil Marylands zu machen und die Bewohner somit als Bürger Marylands an Wahlen teilnehmen zu lassen. Der District of Columbia wurde ursprünglich aus Teilen der Staaten Virginia und Maryland geformt. 1846 wurde der von Virginia genommene Teil wieder zurückgegeben. Dies könnte auch mit dem von Maryland abgetretenen Land geschehen, wobei man ggf. bestimmte Flächen im Zentrum, auf denen die Regierungsgebäude stehen, ausnehmen könnte. Es gibt allgemein verfassungsrechtliche Bedenken, da ein Distrikt als Regierungssitz in der Verfassung vorgesehen ist, so dass diese Pläne unter Umständen einen Verfassungszusatz benötigen.

Eine andere Variante dieses Vorschlags sieht vor, die Bürger Washingtons bei nationalen Wahlen wie Bürger Marylands zu behandeln und die Zahl der Abgeordneten im Repräsentantenhaus ggf. entsprechend zu erhöhen. Zwischen 1790 und 1801 wurde dies genau so gehandhabt, sodass der Kongress dies beschließen könnte. Ein entsprechender Vorschlag scheiterte 2004 schon früh im Gesetzgebungsprozess.

Ein weiterer Vorschlag ist, den Distrikt in einen Bundesstaat umzuwandeln. Dem Kongress obliegt das Recht, neue Staaten in die Union aufzunehmen. In den 1980er Jahren gab es zwei Versuche, eine Verfassung für einen neuen Staat „New Columbia“ zu verabschieden. Dies wurde auch von den Einwohnern Washingtons ratifiziert. Im Kongress konnten derartige Pläne keine Zustimmung finden. Zuletzt lehnte 1993 das Repräsentantenhaus mit deutlicher Mehrheit einen derartigen Vorschlag ab. Es gibt nicht zuletzt auch hier verfassungsrechtliche Bedenken, da ein Distrikt als Regierungssitz in der Verfassung vorgesehen ist.

Das Siegel des District of Columbia zeigt eine Personifizierung der Göttin Justitia, die einen Kranz an einer Statue des ersten US-Präsidenten George Washington ablegt. Im Hintergrund rechts ist das Kapitol, Sitz der Legislative der Vereinigten Staaten von Amerika, zu sehen. Im Hintergrund links fährt ein Zug über ein Viadukt vor einer aufgehenden Sonne.

Auf einem Spruchband steht das lateinische Motto:

Die Jahreszahl 1871 verweist auf das Jahr, in dem der District of Columbia durch den "District of Columbia Organic Act" in seiner gegenwärtigen Form eingerichtet wurde.

Die National Mall zwischen dem Kapitol und dem Lincoln Memorial dient des Öfteren zum Aufmarsch von Massenkundgebungen und für andere Großveranstaltungen: Garten und Vorgarten des Weißen Hauses grenzen an die Prachtallee und vom Washington Monument hat man einen guten Blick auf den Amtssitz des Präsidenten.

Washington ist Sitz der Organisation Amerikanischer Staaten (OAS), der Weltbank, des Internationalen Währungsfonds (IWF) und des Regionalbüro USA der Weltgesundheitsorganisation (WHO).

Von den fünf Universitäten der Stadt sind die bekanntesten die Georgetown University (gegründet 1789), die George Washington University (gegründet 1821) und die Howard University (gegründet 1867 und eine der ältesten Universitäten für die afroamerikanische Bevölkerung). 

Das Bruttoinlandsprodukt pro Kopf im Distrikt betrug 2016 160.472 US-Dollar womit das BIP pro Kopf höher liegt als in allen Bundesstaaten der USA. Seinen Wohlstand verdankt Washington D.C. vor allem seinem Status als administratives und politisches Zentrum der Vereinigten Staaten. Zu den wichtigsten Bereichen der lokalen Wirtschaft gehören Tourismus, Finanzen, Bildung, Gesundheit und Forschung. Der Staat beschäftigt einen großen Teil der Arbeitnehmer in Washington D.C. Diplomatische Institutionen und private Stiftungen beschäftigen ebenfalls eine hohe Anzahl an Einwohnern. Die Arbeitslosenrate lag im November 2017 bei 6,4 % (Landesdurchschnitt: 4,1 %).

Trotz des hohen Durchschnittseinkommens ist die Stadt für ihre sozialen Probleme bekannt. 2013 lebten 18,9 % der Bevölkerung in Armut, was an der extrem hohen Einkommensungleichheit liegt. Die Rate der Einwohner, die Lebensmittelhilfen vom Staat beziehen, war im selben Jahr die höchste des Landes. Eine hohe Kriminalitätsrate macht Washington D.C zudem zu einer der gefährlichsten Großstädte der USA.

Der Virginia Avenue Tunnel wurde 1872 fertiggestellt.

Am 20. November 1990 wurde die letzte Linie der Metro Washington in Betrieb genommen. Das System ist mit 176,32 km das zweitgrößte der USA. Heute besteht das Netz aus sechs Linien. Daneben besteht eine Vielzahl von Busverbindungen nach Maryland, Delaware und Arlington. Das Nahverkehrssystem in Washington wird von "Washington Metropolitan Area Transit Authority" betrieben.

Das ursprünglich sehr umfangreiche Straßenbahnsystem wurde 1962 stillgelegt. Ein neues Straßenbahnsystem existiert seit Februar 2016. Derzeit besteht es aus einer Linie, eine weitere ist in Bau. Im Endausbau könnte das Straßenbahnnetz acht Linien umfassen. Der erste Abschnitt der H Street/Benning Road Line wurde mit zehnjähriger Verspätung am 27. Februar 2016 um 10 Uhr Ortszeit in Betrieb genommen. Die Linie ist insgesamt 2,4 Meilen (ungefähr 3,9 km) lang und hat 9 Haltestellen. Für diese Strecke braucht die Tram planmäßig 20 Minuten. Laut Plan fährt der Zug alle 10 bis 15 Minuten. An Sonntagen ist vorerst kein Betrieb, dies soll aber möglicherweise zu einem späteren Zeitpunkt eingeführt werden. Für eine gewisse Zeit ist die Benutzung der Straßenbahn kostenlos.

Die Strecke führt vom Hauptbahnhof (Washington Union Station) Richtung Osten entlang der H Street, biegt am Ende dieser Straße leicht rechts in die Benning Road ab und endet direkt am "Langston Golf Course" in unmittelbarer Nähe zum Robert F. Kennedy Memorial Stadium.
Der Fahrzeugpark besteht aus je drei Fahrzeugen der Hersteller "United Streetcar" aus Portland (Oregon) sowie "Inekon" aus Tschechien. Aufgrund der massiven Verzögerung beim Bau, der langen Testphase sowie der deutlich höheren Kosten geriet das Projekt oft in die Schlagzeilen. Insgesamt wurden hierfür mehr als 200 Millionen US-Dollar ausgegeben.

Bis zum 11. September 2001 konnten Fahrgäste direkt von der Station Pentagon in das Gebäude des Verteidigungsministeriums gelangen; dieser Ausgang wurde nach den Terroranschlägen vom 11. September 2001 auf unbestimmte Zeit geschlossen.

Der nächstgelegene Flughafen ist der Ronald Reagan Washington National Airport. Er liegt am rechten (westlichen) Ufer des Potomac River in Virginia und wird in der Regel nur für den nationalen Flugverkehr verwendet. Washington Dulles International Airport liegt 45 km westlich der Stadt in Virginia und Baltimore Washington International Airport 65 km nordöstlich in Maryland. Von den beiden letzteren starten sowohl nationale als auch internationale Flüge.

In den US-amerikanischen Profi-Ligen kommen fünf Teams aus Washington. Die Washington Redskins (NFL), Washington Nationals (seit 2005) (MLB) sowie die Washington Capitals (NHL), Washington Wizards (NBA) und D.C. United (MLS). Damit ist Washington in allen jeweiligen Wettkampfklassen der fünf beliebtesten Männersportarten des Landes vertreten.

 Denkmäler und Gedenkstätten




Nationalarchiv

Weitere Institutionen

Areale

Washington hat vierzehn Partnerstädte:



</doc>
<doc id="5571" url="https://de.wikipedia.org/wiki?curid=5571" title="Wacholder">
Wacholder

Die Wacholder ("Juniperus") sind eine Pflanzengattung in der Unterfamilie Cupressoideae aus der Familie der Zypressengewächse (Cupressaceae). Mit den etwa 50 bis 70 Arten, die dieser Gattung zugerechnet werden, stellen sie fast 40 Prozent der Arten innerhalb der Zypressengewächse. In Mitteleuropa kommen in freier Natur nur zwei Arten vor, nämlich der Gemeine Wacholder und der Sadebaum.

Der deutsche Name "Wacholder" (von althochdeutsch "wechalter", mittelhochdeutsch "wëcholtër") hat verschiedene etymologische Deutungen erfahren. Sicher ist, dass der Teil „-der“ der Reflex der indogermanischen Baumbezeichnung ist, wie sie in "Holunder", "Affolter" (Apfelbaum), "Flieder", "Heister" und anderen vorkommt. Unsicher ist der erste Teil. Hier wird entweder ein Zusammenhang mit "wachsen" angenommen, unter Verweis auf den immergrünen Baum, oder mit "wickeln", nach einer (spekulativen) Verwendung zum Binden bzw. für rituell genutzte Wacholdersträuße. Die hin und wieder anzutreffende Deutung als "Wach-Halter" ist hingegen wohl eine Volksetymologie.

Im Niederdeutschen wird der Wacholder auch als Machandelbaum bezeichnet. Daraus hat sich dann als Nebenform die Bezeichnung Machangelstrauch (oder nur "Machangel") entwickelt. Diese Bezeichnung ist vor allen Dingen in Grimms Märchen und einigen Gedichten anzutreffen.

Der botanische Name ist ebenfalls nicht sicher gedeutet. Favorisiert wird die Lesart als lateinisch *"iūni-perus" aus älterem *"iuveni-paros" in der Bedeutung „(zu) früh gebärend, abortierend“ nach der Verwendung von "Juniperus sabina".

Der heute nur selten vorkommende Name "Juniper", welcher als Vor- und Nachname existiert, leitet sich von "Juniperus" ab.

In Deutschland finden sich in althochdeutschen Glossen vorwiegend ab dem 10. Jahrhundert aus den von Spohra/Spurcha entwickelten Namensformen der Begriff „Spurk“ für Wacholder.

Unter anderem in Österreich und Teilen Bayerns ist der Wacholder unter "Kranewitt" bekannt, das über mhd. "kranewite" auf ahd. "kranawitu", "chranawita", welches Kranichholz bedeutet, zurückgeht. Den gleichen Ursprung besitzt das gleichbedeutende "Krammet".

Der Wacholderschnaps ist demgemäß in Österreich unter "Kranewitter" bekannt.

Wacholder-Arten sind immergrüne Sträucher oder Bäume. Als größtes Einzelexemplar gilt ein Syrischer Wacholder ("Juniperus drupacea") in der Türkei, mit einer Wuchshöhe von 40 Metern. Das Holz besitzt einen schmalen Splint und einen rötlich-braunen Kern und duftet oft aromatisch. Die Zweige sind rund oder vier- bis sechsflügelig. Die Blätter sind im Allgemeinen kurz und liegen eng an den Zweigen an. Sie sind in der Jugend nadelförmig, später schuppen- oder nadelförmig. Die Blätter sind in gegenständigen Paaren in vier Reihen oder in wechselständigen Quirlen in drei bis sechs Reihen oder selten in Quirlen mit vier bis acht Reihen an den Zweigen angeordnet.

Die Sämlinge besitzen zwei bis acht Keimblätter (Kotyledonen).

Die zu den Nacktsamigen Pflanzen gehörenden Arten sind meist zweihäusig (diözisch), selten einhäusig (monözisch) getrenntgeschlechtig. Die männlichen Zapfen besitzen drei bis vier Paare oder Trios Sporophylle. Jedes Sporophyll besitzt zwei bis acht Pollensäcke.

Die beerenförmigen, ei- bis kugelförmigen weiblichen Zapfen, oft als Beeren bezeichnet, sind 0,3 bis 2 Zentimeter groß. Sie benötigen bis zur Reife ein bis zwei Jahre, bleiben geschlossen und werden bläulich. Die meist dicken, fleischigen Zapfenschuppen sind aus Deck- und Samenschuppen verwachsen und besitzen ein bis drei Samen. Die ungeflügelten, hartschaligen Samen sind rund bis kantig. Die beerenförmigen Zapfen werden von Vögeln als ganzes geschluckt und die Samen verlassen den Darmtrakt unversehrt. Der bittere Geschmack der Zapfen (bei den meisten Arten) ist wohl eine Anpassung gegen Fraß durch Säugetiere.

Die Chromosomengrundzahl beträgt x = 11.

Wacholder-Arten kommen vorwiegend auf der Nordhalbkugel der Erde vor. Nur das Verbreitungsgebiet von "Juniperus procera" reicht im östlichen Afrika bis 18° Süd.

Wacholder-Arten sind sehr anpassungsfähig. Sie gedeihen in Klimaregionen, die von der subarktischen Tundra bis zu Halbwüsten reichen. Nahezu alle Arten sind gut an regenarme Zeiten angepasst. In Bergregionen sind es häufig Wacholder-Arten, die noch an der Baumgrenze gedeihen. Der auf den Azoren gedeihende Kurzblättrige Wacholder ist die einzige Nadelholzart, die sich auf einer mitten im Ozean liegenden Inselkette vulkanischen Ursprungs etablieren konnte. Die Samen der Vorfahren dieser Art gelangten vermutlich im Verdauungstrakt von Vögeln dorthin.

Sie kommen vielfach auf trockenen Böden (Sand, Heide, Steppe, Halbwüste) vor.

In vielen semiariden Gebieten wie in den westlichen USA, im nördlichen Mexiko, im zentralen und südwestlichen Asien sind sie die dominante Waldbedeckung in weiten Bereichen der Landschaft. Die Untergattung Juniperus ist hauptsächlich eurasisch, mit einer holarktischen Art ("Juniperus communis"). Sie ist auch die einzige Art dieser Untergattung in Nordamerika und Mitteleuropa und überhaupt die am weitesten verbreitete Koniferenart. Die Untergattung "Caryocedrus" ist endemisch in Südwestasien und Südosteuropa. Die Untergattung "Sabina" besiedelt fast alle Areale, die auch für die ganze Gattung gelten – außer nördlich von 50° Nord in Europa und 60° Nord in Asien.

In stark beweideten Gebieten ist Wacholder, aufgrund seiner Unverträglichkeit für Weidetiere, oft der einzige vorkommende Baum.

Die häufigere der beiden Wacholder-Arten in Deutschland, der Heide-Wacholder, war der Baum des Jahres 2002.

Die Gattung Wacholder ("Juniperus") enthält etwa 50 bis 70 Arten. Die wissenschaftlichen Diskussionen über die Artenzahlen, die Rangzuordnungen nach Varietäten, Unterarten oder Formen werden teilweise kontrovers geführt. Untersuchungen auf DNA-Basis (RAPD und Genetischer Fingerabdruck – Fingerprinting) und bezüglich der Blattölzusammensetzung halten die Diskussion in Bewegung; auch in der Feldforschung gefundene neue Arten lassen die Taxonzahlen schwanken.

Hier wird meist und vorzugsweise den Ausführungen von Robert P. Adams gefolgt, der 2008 annähernd 70 Arten und 27 Varietäten anerkennt, aber die Kategorie Unterart nicht verwendet.

Die Gattung "Juniperus" wird in drei Sektionen eingeteilt, die in der Literatur manchmal auch als Untergattungen geführt werden:




Die Sektion "Sabina" wird gelegentlich auch als eigenständige Gattung angesehen; manche Botaniker nehmen an, dass die Arten dieser Sektion/Gattung eine eigenständige Entwicklung mit einem anderen stammesgeschichtlichen Ursprung darstellen.

Obwohl die Wacholder-Arten generell gut angepasst und auch weit verbreitet sind, gibt es dennoch viele Arten, die die Weltnaturschutzunion IUCN in der Roten Liste gefährdeter Arten führt, aber als nicht gefährdet („Least Concern“) bezeichnet. Darüber hinaus wird die Bedrohungssituation vielfach als evaluierungsbedürftig angegeben. Von insgesamt 52 gelisteten Arten werden 12 Arten mit einer Gefährdungskategorie versehen. Das sind ausschließlich Arten, die auf karibischen sowie den atlantischen Inseln der Azoren und der Kanaren oder in Mexiko und Guatemala beheimatet sind:
Ursachen der Bedrohung sind in vielen Fällen andauernde Abholzung und Überweidung.

In der Roten Liste der Schweiz werden der Gemeine Wacholder "Juniperus communis" s. str., "Juniperus communis" subsp. "nana" als Synonym für den Alpen-Wacholder oder Zwerg-Wacholder "Juniperus communis" var. "saxatilis" und der Sadebaum "Juniperus sabina" aufgelistet und als nicht gefährdet (LC) bezeichnet.

Auf europäischer Ebene wurde mit der Berner Konvention Appendix I des Europarats der Kurzblättrige Wacholder oder auch Azoren-Wacholder ("Juniperus brevifolia") als streng geschützte Wildpflanze ausgewiesen.

Mit der Fauna-Flora-Habitat-Richtlinie Nr. 92/43/EWG in der aktualisierten Fassung vom 1. Januar 2007 der Europäischen Union (FFH-RL) Anhang 1 werden Schutzgebietausweisungen für folgende Lebensraumtypen, denen Wacholder-Arten angehören, gefordert:

In der Bundesrepublik Deutschland wird der Zedern-Wacholder "Juniperus cedrus" in der Bundesartenschutzverordnung (BArtSchV) durch Ausweisung als streng geschützte Art unter Schutz gestellt.

Auf dem Nordamerikanischen Kontinent führt die USA über verschiedene Bundesstaaten acht Wacholder-Arten als gefährdete und zu schützende Arten an .

In Gärten und in Park- sowie Friedhofsanlagen werden heute viele Zuchtformen des Wacholder angepflanzt. In Asien sind Wacholder-Arten schon seit Jahrhunderten beliebte Zierpflanzen (→ Steingärten).

Die Ausbreitung der Wacholder-Arten als pflegeleichte und immergrüne Pflanzen in Ziergärten hat zur zunehmenden Ausbreitung des Birnengitterrosts geführt. Diese Pilzkrankheit ist auf Wacholder als Wirtspflanze angewiesen, wobei der einheimische Gemeine Wacholder wohl weniger anfällig ist. Dadurch ist der Bestand an Birnbäumen stark zurückgegangen.

Wacholder als Tee fördert die Verdauung, Harnausscheidung und wirkt gegen Sodbrennen. Er unterstützt die Rheuma- und Gicht-Therapie.

Wacholder ist als Diuretikum allerdings so nicht zugelassen. Die diuretische Wirkung kommt durch die nierenreizenden Inhaltsstoffe der Scheinfrüchte zustande. Wacholder darf deshalb nur in Kombination mit anderen Diuretika eingesetzt werden, da die Verwendung des Wacholder als Einzeldroge sonst leicht zu einer Überdosierung und daraus resultierenden Nierenschäden führen kann.

Im Mittelalter fanden Wacholderbeeren unter anderem als Zutat zu Salben bei der Behandlung von Gelenkerkrankungen Verwendung.

Die Beeren sind ein wichtiger Rohstoff bei der Alkoholherstellung. Es entsteht Wacholderschnaps beziehungsweise Gin. Auch Spirituosen wie Krambambuli, Steinhäger und Genever gibt die Wacholderbeere die spezielle Geschmacksnote.

Ferner werden Wacholderbeeren auch als Aromastoff für Limonaden, wie zum Beispiel Root Beer oder in Schweden Enbärsdricka eingesetzt.

Junge Triebe des Wacholders werden in Skandinavien bei der Bierherstellung eingesetzt.

In einigen Regionen der Schweiz wird aus Wacholderbeeren ein Konzentrat (Saft) hergestellt, das dann zusammen mit Glukosesirup, Rohzucker, Wasser und Karamellzucker zu dem Brotaufstrich Latwerge verarbeitet wird. Das Rezept für diesen Brotaufstrich wird seit langem mündlich überliefert.
Seit ein paar Jahren wird Latwerge bei Großverteilern und in Reformhäusern angeboten.

Die Zapfen des Syrischen Wacholders werden von anatolischen Bergbauern gesammelt und als vitamin- und zuckerreiches Mus namens Andiz Pekmezi genutzt.

Im getrockneten Zustand wird die Wacholderbeere (Kronwittbirl), auch Krammatbeere und gebietsweise Gewürzbeere genannt, gerne bei der Zubereitung von Sauerkraut, wie auch bei vielerlei Fleischzubereitungen (Sauerbraten, Wildbraten) verwendet.

Gleichfalls ist sie wichtig bei der Herstellung von geräuchertem Fleisch oder Fisch. Die Beeren werden in zerstoßenem Zustand den Pökelmischungen beigegeben, sowohl in die Salzmischungen als auch in wässrige Pökellake. Der Geschmack der Wacholder-Beere fördert die geschmackliche Entwicklung beim Räuchern von Fleisch oder Fisch. In alten Rezepten findet man Angaben wie diese: 8–12 Wacholderbeeren je Kilogramm Speck oder Schinken.

Auch das Holz des Wacholder-Strauches wird in Form von Spänen zu den üblichen Räuchermehlen gegeben, um eine Aromatisierung über den Rauch zu erreichen. In alten Rezepten findet man häufig, man solle Kranewitt-Zweige (Wacholderzweige) zur Räucherglut beigeben, um den Geschmack zu verbessern.

Eine zu hohe Dosierung von Beeren oder Holz führt allerdings zu einer seifigen Geschmacksnote.

Wacholder-Holz, -Zweige und -Beeren werden gerne zum Verräuchern verwendet. Wacholder-Rauch gilt als reinigend und desinfizierend und wurde schon im Mittelalter verwendet. Er riecht sehr holzig und gleichzeitig frisch; die Rauchentwicklung ist mäßig bis stark.

Wacholder wird im Bogensport als sogenanntes Bogenholz verwendet, wobei das Holz dann für mindestens 2 Jahre abgelagert wird. Danach wird der vorbereitete Stamm geviertelt und danach weiterverarbeitet. Die weitere Verwendung ist meistens als sogenanntes Laminat zur Zierde von Bögen und Wurfarmen.

Beeren und Nadeln des Wacholder enthalten leicht giftige ätherische Öle, die bei Hautkontakt und Verzehr zu Reizerscheinungen führen können.
Im Gegensatz zu den anderen Wacholdergewächsen ist beim "Juniperus communis" nur die Beere ("Fructus Juniperi") giftig. Reif ist sie im zweiten Jahr. Die Hauptwirkstoffe sind 0,2–2,9 % ätherische Öle aus alpha-Pinen, Terpineol, Sabinen, Myrcen, Flavonoiden und anderen. Eine Überdosierung führt zu Nierenschmerzen, Nierenversagen, Hautreizungen und Leberschädigung. Ferner wird die Herztätigkeit und Atmung gesteigert; seltener können Krämpfe auftreten. Äußerlich kommt es zur Rötung der Haut bis zur Blasenbildung. Die maximale Dosierung liegt bei 2 g.







</doc>
<doc id="5572" url="https://de.wikipedia.org/wiki?curid=5572" title="Walmart">
Walmart

Wal-Mart Stores Inc. ist ein weltweit tätiger US-amerikanischer Einzelhandelskonzern, der einen großen Teil des US-Marktes beherrscht. Walmart ist in der Liste Fortune Global 500 auf Platz eins der umsatzstärksten Unternehmen der Welt verzeichnet. Walmart beschäftigt weltweit über zwei Millionen Angestellte und ist damit der größte private Arbeitgeber der Welt.

Walmarts Umsatz im Geschäftsjahr 2016 betrug 485,9 Mrd. US-Dollar. Der Gewinn lag bei 13,6 Mrd. US-Dollar. Die gesamte Marktkapitalisierung des Unternehmens belief sich Anfang 2017 auf 221,1 Milliarden US-Dollar, womit es das derzeit wertvollste Einzelhandelsunternehmen ist. Bedeutende Wettbewerber sind Carrefour und Tesco. Weltweit betreibt Walmart etwa 11.600 Filialen (Stand: Januar 2016).

Der Unternehmensname leitet sich von seinem Gründer Sam Walton ab (Walton’s Market). Am 2. Juli 1962 eröffnete Sam Walton den ersten Walmart in Rogers, Arkansas, nachdem er bereits 1950 seinen ersten Laden, einen Dime & Nickle Store am Town Square der Kleinstadt Bentonville, Arkansas eröffnet hatte. Hier befindet sich heute neben der Firmenzentrale auch ein Museum über die Unternehmensgeschichte. 1972 ging das Unternehmen an die Börse, was ihm das zur Expansion nötige Kapital gab. Der große Aufstieg begann im Jahre 1987, als Walmart seine ersten Hypermärkte eröffnete, mit einer gegenüber dem damaligen Kaufhausdurchschnitt zehnmal so großen Handelsfläche. Die erste Filiale außerhalb der Vereinigten Staaten wurde 1991 in Mexiko-Stadt eröffnet. Während der Konzern in Kanada erfolgreich agiert, in Mexiko mittlerweile zum Marktführer geworden und in Großbritannien zum zweitgrößten Konzern nach Tesco aufstieg, ist seine Lage in den anderen Auslandsmärkten schwieriger.

Walmart beherrscht mittlerweile einen großen Teil des US-Einzelhandels und ist mit einem Umsatz von 485,9 Milliarden US-Dollar (2016) das umsatzstärkste Unternehmen der Welt. Der Konzern ist auch der mit Abstand größte private Arbeitgeber der Welt, mit über zwei Millionen Beschäftigten (2015).

In Deutschland war Walmart ab 1997 präsent, konnte sich aber nicht durchsetzen. Die 85 Filialen wurden im Juli 2006 an die Metro AG verkauft und anschließend auf die Marke Real umgeflaggt. Walmart verbuchte mit dem Verkauf einen Gesamtverlust von einer Milliarde USD. Näheres siehe Abschnitt Walmart in Deutschland.

Walmart besitzt in den USA 3702 Filialen und ist ebenfalls mit größeren Investitionen in Mexiko, Großbritannien (Asquith Dairies), Japan (Seiyu), Kanada und der Volksrepublik China vertreten, während die Märkte in Deutschland und Südkorea 2006 aufgegeben wurden. In Mexiko ist Walmart über das Tochterunternehmen Walmex tätig.

Der größte Konkurrent, die französische Carrefour-Gruppe, ist nicht einmal halb so groß wie Walmart. Acht von zehn amerikanischen Haushalten kaufen mindestens einmal im Jahr bei Walmart ein, jede Woche betreten weltweit 138 Millionen Kunden ein Geschäft des Konzerns. Allerdings kontrolliert der Konzern nur acht Prozent des amerikanischen Einzelhandelsmarktes; in vielen anderen Ländern haben die Marktführer in dieser Branche einen Marktanteil von über 30 %.

Am 31. Mai 2009 eröffnete Walmart seinen ersten Cash-and-Carry Store in Indien unter dem Namen "Best Price Modern Wholesale". Dieser ist der erste von insgesamt 10 bis 15 geplanten Großhandelsmärkten, die in den nächsten sieben Jahren in Indien aufgebaut werden sollen.

Im Juni 2011 übernahm Walmart für 16,5 Milliarden Rand 51 % des südafrikanischen Handelskonzerns Massmart, eines der größten Afrikas. Zu den Marken gehören unter anderem Game, Makro, Cash & Carry und Builders Warehouse. Im August 2016 übernahm Walmart den erst 2014 gegründeten und seit Juli 2015 aktiven Amazon-Klon "Jet.com" für 3,3 Milliarden US-Dollar.

Der Konzern erwirtschaftet sein Geld durch sehr niedrige Preise bei niedrigen Gewinnmargen, die allerdings durch den riesigen Umsatz und die gegenüber anderen Ketten deutlich niedrigeren Löhne und Gehälter trotzdem Profit abwerfen. Allerdings erhalten die Angestellten bei Walmart (nicht als „employees“, sondern als „associates“, also frei übersetzt „Beteiligte“ bezeichnet) seit Mitte der 1970er Jahre einen Anteil des Profits der Gesamtgruppe. Dabei kann gewählt werden, ob Geld oder Aktienbeteiligungen ausgegeben werden sollen. Diese „Zusatzbezahlung“ wird auf ein Konto eingezahlt und erst bei Ausscheiden des Mitarbeiters an diesen mit Zins und Zinseszins ausgezahlt. Manche Mitarbeiter sind so über die Zeit trotz geringen Grundeinkommens Millionäre geworden.

Entscheidend ist auch, dass Walmart nahezu kein Lager an Waren unterhält – eine ausgefeilte Logistik sorgt dafür, dass die Produkte vom LKW direkt in Logistikzentren geliefert werden und nicht zwischengelagert werden müssen (Just-in-time-Produktion). Der Konzern beschäftigt 2000 Analysten, um das Verhalten seiner Kunden vorauszusagen.

Analysten bemängeln die zu geringe Kapitalrendite im Ausland. Walmart zog sich 2006 aus den verlustreichen Märkten Südkorea (Verkauf an Shinsegae) und Deutschland zurück. In Großbritannien hat die 1999 erworbene Tochter ASDA 2005 ihre Umsatz- und Profitziele nicht erreichen können. Als Gründe dafür wird die alleinige Fokussierung auf den Preis und die Unterschätzung der Bio-Lebensmittel genannt. Die Netto-Ertragskraft beträgt etwa 16 Milliarden US-Dollar.

Die Walton-Familie zählt zu den reichsten Firmeninhabern der Welt; auf der Forbes-Liste der reichsten Menschen der Welt nehmen Sam Waltons Erben mit einem Vermögen von je etwa 17 Milliarden US-Dollar im Jahre 2007 die Plätze 23 und 24 ein.Walmart ist der größte Energieverbraucher und der größte Bauträger der USA. Um die Stromkosten zu senken, hat das Unternehmen ihre Dachflächen für die Installation von Solaranlagen freigegeben. Unternehmen wie SolarCity haben Solaranlagen installiert und verkaufen den Strom direkt an Walmart. Walmart profitiert durch den günstigeren Strom, der zusätzlich durch einen langen Liefervertrag gebunden ist. Bereits 327 Walmart-Märkte sind mit Solaranlagen versehen. Bis 2020 sollen doppelt so viele mit Solaranlagen ausgerüstet sein.

Mitte der 1990er Jahre versuchte Walmart mit großem finanziellen Aufwand, auch in Deutschland Fuß zu fassen. 1997 übernahm Walmart 21 Wertkauf-SB-Warenhäuser für rund 750 Millionen Mark, 1998 74 Interspar-Häuser zu einem Preis von 1,3 Milliarden Mark. Walmarts Deutschland-Zentrale war in Wuppertal auf dem Gelände der früheren Justizvollzugsanstalt Wuppertal neben der ehemaligen Wicküler-Brauerei angesiedelt. Die Logistik wurde von einer Tochtergesellschaft abgewickelt, die in Grolsheim und in Bingen-Kempten zwei Logistikzentralen unterhielt.

Der Konzern machte in Deutschland ausschließlich Verluste; insgesamt geschätzte 3 Mrd. Euro. Allein 2003 fiel ein operativer Verlust von 487 Millionen Euro an, über die folgenden Verluste machte das Unternehmen keine Angaben. 

Walmart gelang es nie, sich auf die deutschen Marktbedingungen einzustellen. Während Walmart Deutschland Verluste anhäufte, konnte die Kaufland-Gruppe im gleichen Zeitraum und Marktsegment stark wachsen. Walmart traf in Deutschland auf ein Einzelhandelsoligopol, dessen Firmen nach ähnlichen Geschäftsprinzipien wie sie selbst arbeiten. So hatte der Konzern von Anfang an keinen Wettbewerbsvorteil. Dazu kam, dass die Walmart-Unternehmenskultur (u. a. Begrüßungspersonal am Eingang, Einpacken der Ware an der Kasse, vorgeschriebene Freundlichkeitsfloskeln der Mitarbeiter) in Deutschland weder von Mitarbeitern noch von Kunden positiv angenommen wurde.

Die im Februar 2005 intern herausgegebene Ethikrichtlinie „Statement of Ethics“ gab Anlass für öffentliche Diskussionen. Insbesondere der in den Medien oft als „Flirt-Paragraph“ bezeichnete Abschnitt, demzufolge „lüsterne Blicke, zweideutige Witze und sexuell deutbare Kommunikation jeder Art“ sowie Beziehungen unter und private Treffen von Mitarbeitern untersagt wurden, wurde viel kritisiert und diskutiert. Auch gab es eine Telefonnummer, bei der Mitarbeiter Verstöße durch Kollegen auch anonym melden konnten. Bei Herausgabe der Richtlinie wurde laut Walmart darauf hingewiesen, dass die Landesgesetze Vorrang vor dem Leitfaden hätten. Entgegen der Medien-Darstellung seien Beziehungen zwischen Mitarbeitern nicht verboten, solange sie das Arbeitsverhältnis nicht negativ beeinflussen.

Die Einmischung in persönliche Beziehungen von Mitarbeitern sei jedoch ein schwerwiegender Eingriff in die Privatsphäre und mit dem deutschen Recht nicht vereinbar, stellte das Wuppertaler Arbeitsgericht fest. In zweiter Instanz scheiterte Walmart vor dem Düsseldorfer Landesarbeitsgericht im November 2005 ebenfalls. Der zuständige Richter sagte: „Dies greift tief in die Persönlichkeitsrechte ein und verstößt gegen die Artikel 1 und 2 des Grundgesetzes.“ Zudem müssten weitere Klauseln der Richtlinie mit dem Gesamtbetriebsrat abgestimmt werden.

In Deutschland spielte Walmart 2006 auch zehn Jahre nach dem Markteintritt fast keine Rolle. Der Marktführer Edeka setzte mit gut 29 Milliarden Euro mehr als zehnmal so viel um wie Walmart. Auch Aldi (gut 24 Mrd.) und Lidl (gut 21 Mrd.) erzielten einen wesentlich höheren Umsatz. Deutschland ist, anders als die USA, seit Jahrzehnten ein Lebensmittel-Billigland; im Discountbereich ergeben sich Margen von nur ungefähr zwei Prozent (vgl. USA circa 5 %). Der ausbleibende Erfolg zwang Walmart schließlich dazu, das Geschäft in Deutschland aufzugeben.

Obwohl Walmart-Deutschland-Chef David Wild einen Rückzug noch am 18. Juni 2006 dementierte, gab Walmart am 28. Juli 2006 den Rückzug vom deutschen Markt bekannt. Die 85 Märkte in Deutschland wurden von der Metro AG bzw. deren Supermarktkette Real übernommen.

Im Oktober 2006 gab das Bundeskartellamt die Übernahme der von Walmart in Deutschland betriebenen Selbstbedienungs-Verbrauchermärkte durch die Metro AG frei. Sie übernahm gleichzeitig 19 Filialimmobilien, deren Wert nach eigenen Angaben den nicht genannten Kaufpreis übersteigt. Die Europäische Kommission, die aufgrund der Umsatzschwellen der Unternehmen für die Fusion zuständig gewesen wäre, hatte den Fall auf Antrag der beteiligten Unternehmen an das Bundeskartellamt verwiesen, da von dem Verfahren ausschließlich Märkte innerhalb Deutschlands betroffen waren. Die Fusion wurde vom Bundeskartellamt ohne Auflagen genehmigt, weil sie nicht zur Entstehung oder Verstärkung einer marktbeherrschenden Stellung führte.

Ende Dezember 2006 wurden 15 Warenhäuser mit 1.200 Mitarbeitern sowie die Wuppertaler Hauptverwaltung geschlossen und die verbleibenden 70 Filialen als Real-Märkte weitergeführt. Die Firma Walmart Germany wurde am 4. April 2007 aus dem Handelsregister gelöscht. Die Kosten für den Rückzug aus Deutschland wurden mit 863 Millionen US-Dollar (680 Millionen Euro, 2006) beziffert.

Bei Walmart herrscht offiziell eine Unternehmenskultur, die Arbeitnehmer als gleichberechtigte Partner des Unternehmens bezeichnet. Diese geht, insbesondere in den USA, mit einer starken Anti-Gewerkschafts-Politik des Unternehmens einher. Nur von 10 Angestellten einer Fleischereiabteilung im Osten der USA ist bekannt, dass sie in einer Gewerkschaft organisiert sind. In Kanada hingegen schloss man gleich ein ganzes Supercenter, nachdem sich alle Angestellten dort zu einer Gewerkschaft zusammengeschlossen hatten. In den USA verdienen neue Angestellte bei Walmart im Schnitt nur zwei Drittel eines gewerkschaftlich organisierten Kollegen bei einem anderen Supermarkt. Ebenso gibt es keine Zusatzleistungen wie zum Beispiel eine durch den Betrieb getragene Krankenkasse. Im Schnitt muss der Konzern 44 % seiner Arbeitskräfte jährlich ersetzen, das bedeutet für ihn jedes Jahr 600.000 Neueinstellungen. Im Schnitt laufen zu jedem gegebenen Zeitpunkt etwa 1.500 Klagen gegen Walmart, die sich hauptsächlich gegen Verletzungen des amerikanischen Arbeitsrechts wenden.

Walmart wird vorgeworfen, direkt und indirekt die Verbreitung von Ausbeutungsbetrieben zu unterstützen, in denen die Walmart-eigenen Marken unter sehr umstrittenen Arbeitsbedingungen hergestellt werden.

Es gibt immer wieder Demonstrationen gegen die Eröffnung von Walmart-Filialen. Die Demonstranten werfen Walmart Preiskrieg vor, mit dem sie kleine Geschäfte vernichten und dadurch Arbeitsplätze und die Vielfalt einschränken. Außerdem werden die Arbeitsbedingungen der Angestellten kritisiert. Walmart selbst sieht sich als „Hecht im Karpfenteich“, der andere Wettbewerber „dazu zwingen würde, ihre Geschäftspolitik zu ändern und so erfolgreicher zu werden“.

Immer wieder kam es auch zu Kritik, da Walmart indirekt Druck auf Herausgeber von Zeitschriften ausübte, ihre Inhalte an die sehr konservative Unternehmensphilosophie anzupassen, um sie weiterhin bei Walmart anbieten zu können. Kritiker betrachteten das als Einflussnahme auf die Pressefreiheit, zumal Zeitschriften, die sich dem Druck nicht beugten (wie z. B. Maxim) aus den Regalen genommen oder zumindest in neutrale Umschläge gesteckt wurden. Walmart weigerte sich auch, bestimmte CDs in seinen Regalen anzubieten, weil diese dem „familienfreundlichen“ Bild, mit dem sich Walmart gerne selbst vermarktet, entgegenstehen würden.

Im Dezember 2005 wurde Walmart von einem Gericht in Kalifornien zu einer Zahlung in Höhe von 57 Millionen US-Dollar verurteilt. Das Geld geht an 116.000 frühere und derzeitige Mitarbeiter, denen Walmart eine vorgeschriebene 30-minütige Pause verwehrte. Des Weiteren wurde Walmart zu einer Strafzahlung in Höhe von 115 Millionen US-Dollar verurteilt, weil das Unternehmen den Mitarbeitern keine Mittagspause zugestand.

Im Oktober 2006 wurde der Konzern aufgrund unbezahlter Mehrarbeit zu einer Zahlung von 78,5 Millionen US-Dollar an seine Mitarbeiter im US-Bundesstaat Pennsylvania verurteilt. Nach Medienberichten sind weitere 70 Verfahren anhängig.

Wegen der unsachgemäßen Entsorgung von Sondermüll wurde Walmart im Mai 2013 zu einer Geldstrafe in Höhe von 81,6 Millionen US-Dollar verurteilt. Giftige Stoffe wurden von Mitarbeitern in die Kanalisation gekippt oder im Hausmüll entsorgt.

Ein Berufungsgericht in San Francisco hat eine Sammelklage von 1,6 Millionen Frauen wegen sexueller Diskriminierung gegen die Einzelhandelskette Walmart für zulässig erklärt.

Walmart lehnt jegliche Verantwortung für die Arbeitsbedingungen bei seinen Zuliefererfirmen, vorwiegend in Asien, ab und bekam dafür 2005 den "Public Eye Award" verliehen.

"„Die gnadenlose Ausbeutung des schwachen Arbeitrechts in den USA durch das Unternehmen Walmart vereitelt die Gründung von Gewerkschaften und verletzt die Rechte seiner amerikanischen Arbeiter“", so Human Rights Watch in ihrem am 1. Mai 2007 erschienenen 210 Seiten langen Bericht „Discounting Rights: Wal-Mart’s Violation of US Workers’ Right to Freedom of Association“. Von zentraler Bedeutung, so der Bericht, sei das Ausmaß und die Aggressivität an Gewerkschaftsfeindlichkeit. Das Verhalten des Unternehmens sei auch deshalb besonders bedenklich, weil es sich um das zu dieser Zeit zweitgrößte Unternehmen der Welt handle.

Allein der Reingewinn in dem im Januar 2007 beendenden Geschäftsjahr stünde bei 11,2 Milliarden US-Dollar. Human Rights Watch fand heraus, dass kein Arbeiter des größten privaten Arbeitgebers der USA durch eine Gewerkschaft vertreten sei und dass dies zum Prinzip des Unternehmens gehöre. Schon im „Manager’s Toolbox“ würden den Managern Maßnahmen genannt, wie gewerkschaftlicher Einfluss zu verhindern sei. Denunziation, Bespitzelung, Lauschangriffe, massiver Druck und die Angst, den Arbeitsplatz zu verlieren, seien bei Walmart gängige Geschäftspraxis.

Walmart hatte in der Vergangenheit immer wieder mit dem Tod von vermeintlichen Ladendieben für Schlagzeilen gesorgt. So wurde unter anderem im September 2009 eine 37-jährige Kundin der Walmart-Filiale in Jingdezhen (China) zu Boden geschlagen, weil sie nach einer Kontrolle ihren Kassenbon zurückverlangt hatte. Sie erlag drei Tage später ihren Verletzungen. In Houston wurde 2005 ein Mann von Walmart-Mitarbeitern zu Boden gedrückt und gefesselt, er starb wenig später am Boden liegend an einem Herzinfarkt.

JibJab hat die Kritik an Walmart in der Flash-Animation "Big Box Mart" verarbeitet.

Im November 2017 wird Walmart in den Veröffentlichungen der Paradise Papers aufgelistet.






</doc>
<doc id="5574" url="https://de.wikipedia.org/wiki?curid=5574" title="Microsoft Windows 95">
Microsoft Windows 95

Microsoft Windows 95 ist ein Betriebssystem mit grafischer Benutzeroberfläche für Personal Computer (PC). Es war das erste Betriebssystem der Windows-Reihe von Microsoft, das den 32-Bit-Betrieb des Prozessors weitreichend unterstützte, ohne auf die Abwärtskompatibilität zu den damals noch weit verbreiteten DOS-Programmen zu verzichten. Diese wurden (und werden bis heute) unter NT-Systemen lediglich in einer Virtual DOS Machine ausgeführt, was z. B. direkte Hardwarezugriffe, die viele dieser Programme voraussetzen, konsequent verhindert. Wie auch Windows NT ist Windows 95 abwärtskompatibel für 16-Bit-Windowsprogramme.

Nach seiner Einführung am 24. August 1995 entwickelte sich Windows 95 zum bis dahin erfolgreichsten Betriebssystem auf dem Markt und begründete die Windows-9x-Reihe.

Im Februar 1995 wurde an eine Handvoll Personen eine Testversion des bis dahin geheimen Windows 95 verteilt. Davor kannte man es nur als Windows 4.0 oder unter seinem Arbeitstitel Windows „Chicago“. Jeder, der an den Testphasen teilnehmen durfte, musste einen Geheimhaltungsvertrag unterzeichnen.
Am 24. August 1995 gab Microsoft nach weiteren zahlreichen Tests die Endversion zum Verkauf frei. Deren Versionsnummer war 4.00.950. Microsoft begann die größte Produkteinführung der Konzerngeschichte.

Damit läutete Microsoft auf breiter Front das Ende der 16-Bit-Architektur ein. Im 16-Bit-Modus der x86-Linie laufen unter anderem DOS und die ersten Versionen von Windows bis einschließlich Microsoft Windows 3.1 (Windows 3.1 ist teilweise 32-Bit-fähig). Windows 95 setzt, ebenso wie seine direkten Nachfolger Windows 98 und Windows Me, auf MS-DOS auf, das zum Starten und für einige wichtige Systemprozesse und Treiber benötigt wird.

Umstritten ist, ob Windows 95 als „grafische Oberfläche für DOS“ zu betrachten sei oder als weitgehend eigenständiges Betriebssystem:

Der offizielle Support von Windows 95 von Microsoft mit Aktualisierungen und Korrekturen endete am 31. Dezember 2001.

Im August 1995 führte Microsoft das Produkt mit seiner bis dahin größten Werbekampagne ein. Die Einführung des „Start“-Knopfes wurde bei Fernsehspots mit dem Lied „Start me Up“ von den Rolling Stones untermalt.

Die eigens für Windows 95 geschaffene Startmelodie wurde 1994 von Brian Eno komponiert, nachdem er von Mark Malamud und Erik Gavriluk (Senior-Entwickler des Microsoft Chicago-Projekts) angesprochen worden war.
Microsoft wollte ein Musikstück, das inspirierend, universell, optimistisch, futuristisch, gefühlvoll und emotional sei, und noch weiteren Attributen gerecht werden sollte. Auch sollte es maximal 3¼ Sekunden lang sein. Schließlich wurden daraus jedoch sechs Sekunden.

Um sowohl computerunerfahrenen als auch Nutzern älterer Windowsversionen eine schnelle Eingewöhnung in die neue Oberfläche des Betriebssystems, seiner Bedienung und der neuen Multimediamöglichkeiten zu ermöglichen, lag vielen vorinstallierten Computern eine CD-Rom mit dem Titel "Windows 95 Start!" bei, ein interaktiver Computerkurs.

Nach Angaben von Microsoft existieren unter anderem folgende Verbesserungen gegenüber Windows 3.1:

Windows 95 ist nach Windows NT 3.1 und 3.5 (beide mit der Benutzeroberfläche von Windows 3.x) das erste Microsoft-Betriebssystem, das zum größeren Teil auf der 32-Bit-Architektur (im x86-kompatiblen Schutzmodus) basiert. IBM mit OS/2, das diese Technik schon längere Zeit beherrschte, konnte sich auf dem Markt gegen Windows nicht durchsetzen. Dabei wurden von Microsoft die 16-Bit-DOS-, 16-Bit-Windows- und 32-Bit-Windowsarchitekturen (mit ihren spezifischen Speicherschutzmodi) in einer Art Symbiose vereint. Die meiste Software lief damals noch unter DOS, was eine konsequente Windows-NT-Entwicklung unattraktiv machte.

Mit Windows 95 konnte nun auch ein Win-3.x-Nachfolger mehrere Programme gleichzeitig ausführen. Bisher mussten Programme (unter Windows 3.x) warten, bis das Vorgängerprogramm den Prozessor freigab. Multitasking ist zwar schon in vorigen Windows-Versionen vorhanden, jedoch handelt es sich dort noch um kooperatives Multitasking, es läuft also immer nur ein Programm gleichzeitig, die anderen werden lediglich im Speicher gehalten und solange angehalten. Das präemptive Multitasking im 32-Bit-Modus ermöglicht nun einen systemkontrollierten Quasi-Parallelbetrieb im Zeitscheibenverfahren (vgl. auch Scheduling), allerdings aus Gründen der Abwärtskompatibilität nur mit eingeschränktem Speicherschutz.

Mit der Registrierungsdatenbank wurde ein zentraler, systemweit eindeutiger und auch konkurrierend erreichbarer Platz für Konfigurationsinformationen eingeführt; sie löste das System der Initialisierungsdateien von Windows 3.1 fast vollständig ab. Jedoch verwenden auch heute gelegentlich noch Anwendungsprogramme Konfigurationsdateien anstatt Registry-Einträge, besonders Portable Software.

Durch die Dateisystem-Erweiterung VFAT erlaubt das System erstmals die Nutzung längerer Dateinamen unter Windows, wodurch die von DOS bekannte Begrenzung auf 8+3 Zeichen für den Namen entfällt. Jetzt sind 255 Zeichen erlaubt, jedoch "inklusive" des Pfadnamens, was beim Kopieren in Unterordner zusätzliche Probleme verursachen kann. Dabei unterscheidet Windows zwar keine Groß- und Kleinbuchstaben, behält aber die vom Benutzer vergebene Schreibweise bei. Mit VFAT wollte Microsoft das neue Dateisystem kompatibel zum alten machen, sodass jeder lange Name noch einen automatisch generierten DOS-kompatiblen Namen erhält, z. B. „DOKUME~1.DOC“ neben „Dokumentation des neuen Projekts.doc“ (hinter der Tilde werden mehrfach vorhandene Namen einfach durchnummeriert). Dadurch können alle auf VFAT erstellten Dateien auch von DOS-Nutzern und Nutzern von Windows bis Version 3.11 verwendet werden (wenn das zugrundeliegende Dateisystem das von DOS unterstützte FAT12 oder FAT16 ist, nicht jedoch FAT32).

Windows 95B unterstützt erstmals FAT32, wodurch ein erweiterter Adressraum zur Verfügung steht. Die Verbesserung gegenüber FAT16 besteht hauptsächlich in der Unterstützung größerer Festplattenpartitionen (mehr als 2 GB) und in kleineren Speichersektoren, wodurch der ungenutzte Speicher vor allem bei kleinen Dateien reduziert wird.

Die Online-Datenkomprimierung DriveSpace aus DOS 6.22/DOS 7 ist erstmals auch mit einer grafischen Bedienoberfläche konfigurierbar. Zusammen mit Microsoft Plus! wurde die Effektivität dieser Komprimierung durch bessere Algorithmen (HiPack und UltraPack) in DriveSpace 3 (die dritte Version) nochmals gesteigert und in Windows 95B auch ohne Zusatzsoftware ins Betriebssystem integriert. Die problematische Datensicherheit und andere Nachteile führten jedoch dazu, dass das Programm seit der Verfügbarkeit großer Festplatten zu günstigen Preisen schnell an Bedeutung verlor.

Umfangreich sind auch die Neuerungen im grafischen Bereich, allen voran das "Windows-Startmenü". Microsoft hat mit Windows 95 die Grafische Benutzeroberfläche so weiterentwickelt, dass sie ähnlich zu bedienen ist wie das anfänglich gemeinsam mit IBM entwickelte Betriebssystem OS/2. Auch die Taskleiste am unteren Rand des Bildschirms war unter Windows neu. Klickt der Nutzer auf „Start“, so erhält er ein Menü, in dem er die verfügbaren Programme abrufen, die zuletzt benutzten Dokumente aufrufen, Einstellungen ändern, Hilfe aufrufen sowie den Computer ausschalten kann (oft zitiert wurde die kurios anmutende Aufforderung: „Klicken Sie auf "Start", um zu beenden“). Die Startleiste (das „Band“ neben diesem „Start“-Knopf) zeigt die aktuell laufenden Programme an, mit einem Klick kann man zwischen diesen wechseln. Der alte Programm-Manager aus Windows 3.1 wurde ersetzt durch den so genannten Desktop, eine Oberfläche, auf der sich mit entsprechenden Anwendungen verknüpfte Symbole („Verknüpfungen“) befinden. Der alte Programm-Manager war dennoch ebenso wie der alte Datei-Manager im Lieferumfang enthalten, die entsprechenden Programmdateien befinden sich als „Progman.exe“ und „Winfile.exe“ im Windows-Installationsverzeichnis. Bei der Installation des ersten Windows-95-Betriebssystems (Windows 95A) konnte man die alte Benutzeroberfläche sogar noch alternativ als Standardoberfläche auswählen.

Der von Windows 3.x bekannte Datei-Manager wurde durch den neuen Windows-Explorer ersetzt. Neben der eigentlichen Dateiverwaltung ist er auch für die Symbole (auf dem Desktop), die Fenster, die Taskleiste und einiges mehr zuständig. Neu für Windows sind auch die Kontextmenüs. So kann man praktisch alles mit der rechten Maustaste anklicken, um zu sehen, welche Aktionen man auf dem jeweiligen Objekt durchführen kann; so zeigen sich beispielsweise Unterschiede der möglichen Aktionen im Kontextmenü zwischen Textdateien und etwa Worddokumenten. Unter Windows 3.x ist die rechte Maustaste – im Gegensatz zu vielen Anwendungsprogrammen, beispielsweise WordPerfect – meist ohne Funktion.

Abgesehen vom "Datei-Manager" (ein Überbleibsel von Windows 3.x) ist Windows 95 vollständig Jahr-2000-kompatibel. Für diesen ist jedoch von Microsoft ein Update erschienen. Im Service Pack 1 (etwa Februar/März 1996) befindet sich bereits der nachinstallierbare Internet Explorer, Version 2.0.

Mit Windows 95 gab es erstmals WordPad, in allen Vorgängerversionen war nur der WordPad-Vorgänger Microsoft Write enthalten.

In der Betaversion können mit dem virtuellen Gerätetreiber „cdfs.vxd“ (Größe: 77,2 KB) Musik-CDs noch wie ein gewöhnlicher Windows-Ordner geöffnet werden. Dort werden die einzelnen Musikstücke als kopierbare WAV-Dateien in Mono und Stereo in jeweils drei Qualitätsstufen angezeigt. Ein Rippen von Musikdateien war damit nicht nötig. Die „cdfs.vxd“ wurde in der Verkaufsversion durch eine nur noch 57,7 KB große Datei ersetzt, die nur noch Verknüpfungen anzeigt (*.cda-Dateinamen). Die „cdfs.vxd“ der Beta-Version war bis einschließlich Windows ME funktionsfähig. Sie wurde von verschiedenen Computerzeitschriften in beigelegten CD-ROMs oder online zum Download angeboten.

Neben der ebenfalls neuen (und wegen Fehlern fast unbrauchbaren) USB-Unterstützung unterstützt das Betriebssystem ab der B-Version erstmals auch AGP-Grafikkarten.

Windows 95 hatte seit jeher Probleme mit der stetig wachsenden Leistungsfähigkeit der Hardware. Bei zu schnellen Prozessoren kam es aufgrund eines Timingfehlers zu einem Absturz; dieser Fehler wurde, da er zuerst beim AMD K6 auftrat, auch „AMD-K6-Bug“ genannt. Ein weiterer Fehler in einer anderen Systemkomponente, der von Microsoft nicht behoben wurde, sorgt für einen Absturz, wenn der Prozessor schneller als 2,1 GHz ist. Auch werden Festplatten, die größer als 32 GB sind, von Windows 95 nicht unterstützt.

Von „Windows 95“ wurden vier Versionen entwickelt, von denen sich die letzte nochmals in verschiedene Versionen unterteilt. Allerdings war nur die erste Version im Handel erhältlich, wahlweise auch als Diskettensatz, alle anderen waren nur als OEM-Versionen vorinstalliert und mit neu gekauften Rechnern und auf CD-ROM (nicht bootfähig, mit zusätzlicher Startdiskette) erhältlich.

Unter DOS melden sich alle OSR 2.x-Versionen mit 4.00.1111. Unter Windows ohne USB-Unterstützung ebenfalls, sie sind nur am „B“- bzw. „C“-Eintrag erkennbar.

Nach der Entwicklung von Windows 95 (Ur- bzw. A-Version) erschien eine Systemaktualisierung unter dem Codename „Nashville“. Sie ist eine unter Windows 95 installierbare Betaversion, die sich in der Software-Systemsteuerung als „Windows 96“ ausweist. Sie kam später jedoch nicht unter diesem Namen in den Handel, sondern wurde als aktualisiertes Windows 95B verkauft. Die Neuerungen betrafen hauptsächlich die Unterstützung neuer Hardware, wie Infrarot- und USB-Schnittstellen.

Für Windows 95 (Ur-Version) und Windows 95a (OEM-Service Release 1) gelten folgende Mindest-Systemvoraussetzungen:

Für Windows 95b und Windows 95c (OEM-Service Release 2, 2.1 und 2.5) gelten folgende Mindest-Systemvoraussetzungen:

Windows 95 brachte nicht nur Neuerungen, sondern auch Probleme mit sich. Ziel der Architektur war vollständige 16-Bit-Kompatibilität zu Windows 3.11 und DOS unter gleichzeitiger Verwendung der neuen 32-Bit-Architektur, was jedoch nur teilweise erreicht wurde. Auch aufgrund dieses Kompatibilitätsansatzes reichte Windows 95 bei Weitem nicht an die Stabilität der Windows-Versionen der NT-Linie heran.

Durch die Unterstützung sowohl von alten 16-Bit- als auch von neuen 32-Bit-Programmen wurde der Kernel signifikant komplexer als bei der Vorgängerversion 3.1x, was in deutlich geringerer Ausführungsgeschwindigkeit von 16-Bit-Programmcode – insbesondere beim Bildschirmaufbau – resultiert. Die Windows-Kerneldateien greifen bei 16-Bit-Programmcode weiterhin wie bei DOS oder Windows 3.1 auf grundlegende Ein-/Ausgabefunktionen des DOS-Systemkernels IO.SYS zu.

Bei der B- und C-Version gibt es zudem einige Probleme mit der vorher nicht vorhandenen USB-Unterstützung, die sich als fehlerhaft erwies. Auch einige Grafikkartentreiber verweigern unter Version C ihren Dienst, laufen jedoch mit der älteren Version B anstandslos.



</doc>
<doc id="5575" url="https://de.wikipedia.org/wiki?curid=5575" title="Microsoft Windows 98">
Microsoft Windows 98

Windows 98 (Codename: "Memphis") ist ein ab 25. Juni 1998 von Microsoft vertriebenes Betriebssystem. Microsoft beendete den Support für Windows 98, Windows 98 SE und Windows ME ab 11. Juli 2006. Diese Betriebssysteme sind im Wesentlichen eine stetige Weiterentwicklung von Windows 95.

Die Beta-Version „Memphis“ durchlief insgesamt drei Beta-Phasen und eine Pre-Beta-Phase. Die Pre-Beta erschien im Dezember 1996 und ist ab einem bestimmten Datum nicht mehr lauffähig. Als Boot-Logo der Pre-Beta wird „Microsoft Memphis Developer Release“ angezeigt. Sie bietet keine sichtbaren Neuerungen, selbst in den Systemeigenschaften heißt sie noch Windows 95. Allerdings bietet sie z. B. USB-Unterstützung.

Später erschien die Beta 1. Sie beinhaltet folgende Neuerungen:

Mit der Beta 2 wurde „Memphis“ in Windows 98 umbenannt. Da ursprünglich bereits ein Release im Jahr 1997 geplant war, war vorher der Name Windows 97 geplant. Das Setup hat (bis auf ein paar Bilder während des Kopiervorgangs) das Aussehen des Setups der finalen Version. Das Boot-Logo trägt unter dem Schriftzug „Windows 98“ den Untertitel „Microsoft Internet Explorer“, der Untertitel wurde in der finalen Version entfernt. Der Internet Explorer 4.0 ist enthalten und bringt alles mit, was in der finalen Version dabei ist:

Zudem befindet sich in der Systemsteuerung der Punkt „Benutzerverwaltung“. Außerdem können Ordner mit einem Klick geöffnet werden.

Ursprünglich sollte Windows 98 im November 1997 – und nicht ein 95 C – veröffentlicht werden, allerdings verschob sich dieser Termin bis zum Frühling 1998. Microsoft verkündete offiziell, dass diese Verzögerung dazu diene, ein Upgrade von Windows 3.1 auf Windows 98 zu ermöglichen, in Wirklichkeit integrierte Microsoft währenddessen ihren Internet Explorer 4.01 unlöschbar tief ins Betriebssystem, um so Netscape im laufenden Browserkrieg aggressiv vom Markt zu verdrängen. Erst unter Schadensersatz-Klagen gegen Microsoft im Jahr 1999 wurden unfaire Details öffentlich.

Windows 98 ist, wie bereits sein Vorgänger, ein 16-Bit/32-Bit-Hybrid-System. Es verwendet, wie alle Betriebssysteme der Win9x-Reihe, ein 16-Bit MS-DOS-Betriebssystem zum Start und basiert teilweise darauf. Windows 98 unterstützt, wie schon der Vorläufer Windows 95, echtes präemptives Multitasking. Das bedeutet, dass das Betriebssystem bei der Zuteilung von CPU-Zeit an aktive Programme eine strikte Kontrolle durch feste Zeitscheiben ausübt. Eingeschränkt ist diese Konsequenz nur durch Hardwarezugriffe, die länger als die zugeteilte Zeitscheibe dauern, z. B. Timeouts. Konsequenter Speicherschutz ist bei Windows 98 nicht gewährleistet, sondern erst ab NT/2000/XP/Vista (dort mit dem Nachteil des Verlustes der Abwärtskompatibilität zu älterer Software, die direkten Zugriff auf physikalische Adressen benötigt). Windows 98 ermöglicht mit dem mitgelieferten MS-DOS 7.1 den Betrieb von DOS- und damals noch nicht häufigen 32-Bit Windows-Programmen. Windows 98 ist wesentlich größer als sein Vorgänger. Windows 98 erstellt beim ersten erfolgreichen Systemstart eines jeden Tages automatisch oder manuell eine Sicherungs-Kopie der Registrierung. Es verkaufte sich als Upgrade zu Windows 95 von Anfang an gut.

Windows 98 ist das erste DOS-basierte (größtenteils) 32-Bit Betriebssystem, das im Gegensatz zu Windows 95 nicht mehr einzelne Hintergrundprogramme im 16-Bit-Modus ausführt. Aber selbst hier kommen immer noch einzelne Dienstprogramme im 16-Bit-Modus zum Einsatz.

Mit Windows 98 führte Microsoft das „Win32 Driver Model“ ein („WDM“), welches auf dem Gerätetreibermodell von NT basierte. Das 2001 folgende Model hieß Windows Driver Foundation.
Weitere Neuerungen waren z. B. bessere AGP- und USB-Unterstützung (beides bereits ab Windows 95 B, aber die USB-Unterstützung ist dort so fehlerhaft, dass die meisten Hardwaretreiber sich erst ab Windows 98 installieren lassen), Unterstützung von ACPI, Festplattenpartitionen größer als 2 GB mit FAT32-Dateisystem (bereits ab Windows 95 B). Einige Funktionen, die sich bei Windows 95 mit dem Internet Explorer 4.0 nachrüsten lassen, sind bei Windows 98 bereits integriert. Dazu gehören die Integration des Internet Explorers in die Benutzeroberfläche, der Active Desktop, der verbesserte Windows Explorer (bessere Bedienung, UNC-Pfade und Netzwerkrechner lassen sich über die Adresszeile ansteuern).

Ferner war Windows 98 das erste grafische Betriebssystem von Microsoft, das mehrere Monitore unterstützte.

Windows 98 SE "(Second Edition)" (Build 2222) wurde am 5. Mai 1999 veröffentlicht. Die deutsche Version war am 10. Juni 1999 verfügbar.

Entscheidende Verbesserungen gegenüber der Erstausgabe sind unter anderem – neben einer weiter verbesserten USB-Unterstützung – wesentliche Erweiterungen in der Netzwerkunterstützung, wie z. B. die sogenannte Internetverbindungsfreigabe (ICS), welche die gemeinsame Nutzung einer einzigen Verbindung ins Internet durch mehrere Rechner ermöglichte. Allerdings war diese Funktion problembehaftet, schwer zu aktivieren und nicht immer stabil. Weiter neu war die Möglichkeit einer unbeaufsichtigten Installation.

Andere SE-Funktionen wie DirectX 6.1, Internet Explorer 5.0, Windows Media Player 6.1, MDAC (Datenbankanbindung), MSI (Microsoft-Installer) etc. sind im Vorgänger installierbar.

Als Voraussetzungen gibt Microsoft an:

Diese Systemvoraussetzungen nennt Microsoft als Mindestvoraussetzungen für Installation und (sinnvollen) Betrieb. Tatsächlich kann das System mit noch geringerer Ausstattung betrieben werden oder (insbesondere unter Zuhilfenahme eines anderen PCs) auf anderem Weg installiert werden. So ist, mithilfe von ein paar Modifikationen, auch eine Installation über oder sogar auf einem beliebigen USB-Stick möglich; Voraussetzung ist eine nicht vorhandene RAM-Begrenzung im MS-DOS, die wiederum vom Prozessor abhängig ist. Bei USB-Sticks, die sich als USB-Festplatte ausgeben, ist eine Installation immer möglich, da diese für DOS normale Festplatten sind.

2003 entschied Microsoft in Anbetracht der häufigen Nutzung des Systems weltweit, den Support mit Patches statt wie geplant bis Januar 2004 bis zum 11. Juli 2006 zu liefern. Danach wurde zudem die Windows-Update-Funktionalität beendet, wodurch auch die bereits erschienenen Updates nicht mehr über Windows Update bezogen werden konnten.

Alle als PC-kompatibel geltende Chipsätze und gängige Hardware, die zum Zeitpunkt der Auslieferung auf dem Markt waren, funktionieren mit Windows-Standardtreibern. Neuere Hardware läuft ebenfalls unter Windows 98, sofern ein solcher Support durch den Hersteller vorgesehen ist. Ab Mitte 2006 unterstützen Hauptplatinen-Chipsätze Windows 9x nicht mit angepassten Treibern.

Eine Besonderheit der "Second Edition" ist, dass diese sowohl alte Windows Gerätetreiber als auch neue WDM-Treiber unterstützt. In Windows Me werden nur WDM-Treiber unterstützt. Damit bietet SE die Möglichkeit, sowohl alte Hardware, für die es keine WDM-Treiber gibt, als auch neue Hardware mit WDM-Treibern zu kombinieren oder aber je nach Stabilität den alten Windows-Treiber oder den neueren WDM-Treiber einzusetzen.

Windows 9x kann MBR-formatierte Festplatten über 128 GB mit den Standardtreibern ohne 48-Bit-LBA-Unterstützung ansteuern, aber ein Schreibzugriff auf eine Datei oberhalb dieser Grenze führt durch Überschreiben zum Datenverlust. Die 128 GB gelten dabei pro tatsächlich vorhandener Festplatte (physikalisches Laufwerk) und nicht pro partitioniertem logischem Laufwerk. Das gilt auch für extern angeschlossene USB- und FireWire-Festplatten. Einige Hersteller bieten Treiber an, mit denen der Betrieb großer Medien problemlos möglich ist.

Um mehr als 512 MB Arbeitsspeicher betreiben zu können, sind kleine Veränderungen, z. B. an der codice_1, notwendig. Dazu wird der VCache begrenzt. Ab 1 bis 2 GB muss der adressierbare physische Speicher MaxPhysPage begrenzt werden, um die Stabilität des Systems aufrechtzuerhalten.

Etliche Softwarehersteller haben Windows 9x Jahre weiter unterstützt, nachdem Microsoft den Support beendet hatte. Die letzte installierbare Version von Microsoft Office ist Office XP, OpenOffice.org unterstützt Windows 98 bis einschließlich Version 2.4.3. Internet Explorer 6 ist die letzte Version für Windows 98. Firefox 2 wurde bis zur Version 2.0.0.20, Opera bis zur Version 10.5 gepflegt. Die letzte flash-Version ist 9.262.

2005 gab es inoffiziellen Support in Form von Service-Packs, die von der Windows-98-User-Community erstellt wurden. Die Supportseite con Creopard wird weiterhin aktuell gehalten.

Durch Verwendung des Open-Source-Patchs "KernelEx" ist es möglich, manche nur für Windows XP geschriebene Programme unter Windows 98 zu verwenden (etwa Firefox 3). Das wird durch umfangreiche Anpassungen von Windows-DLL-Systemdateien erreicht, mit denen fehlende API-Funktionen nachgerüstet werden. Da KernelEx ein inoffizieller Patch ist, gibt es keine offizielle Unterstützung und der Benutzer arbeitet auf eigenes Risiko.

Eine direkte Installation und Ausführung von Windows 98 auf aktueller Hardware ist mangels Treiber und inkompatibler Geräte, vorallem aber zu moderner PC-Hardware (Arbeitsspeicher von über 1,5 GB, EFI statt BIOS, NVMe) nur mehr stark eingeschränkt oder gar nicht mehr möglich. Da PCs jedoch prinzipiell IBM-PC-kompatibel sind, und wenn das jeweilige EFI mit einem CSM ( – eine BIOS-Emulation) ausgestattet ist, ist es prinzipiell möglich. Was fehlt sind Treiber für Chipsätze, Grafikkarten, verbaute Controller-Chips wie USB-Hostcontroller, womit ein Teil der Hardware nicht benutzbar ist. Windows 98 läuft dann im BIOS-kompatiblen Modus, die Grafik ist auf Super-VGA (800 × 600, 16-Bit-Farbe) beschränkt. Alle modernen Peripheriegeräte, aber auch Zusatzfunktionen von erweiterten Tastaturen und Mäusen, die einen Treiber erfordern, funktionieren ebenfalls nicht. Für einige Probleme mit Speichern und Prozessoren sind Lösungen zur Anpassung dokumentiert bzw. inoffizielle Patchprogramme verfügbar.
Eine einfachere Möglichkeit stellt der Betrieb von Windows 98 in einer virtuellen Maschine (VM) dar: Treiber für moderne Peripheriegeräte fehlen zwar weiterhin, aber die Installation von Windows selbst wird einfacher. Nicht alle VMs und Emulatoren unterstützen jedoch Windows 98 und so ist auch hier oft einiges an Handarbeit nötig.

Da meist wichtige Treiber fehlen oder nicht funktionieren, ist ein vollwertiger Einsatz sowohl nativ als auch in einer VM oder in einem Emulator nicht möglich, meist zudem instabil. lassen sich OpenGL- und DirectX-Programme und -Spiele meist nicht ausführen.

Bei der Vorführung des Betriebssystems durch Bill Gates auf der CES 1998 stürzte das Betriebssystem beim Anschluss eines Scanners über USB mit einem Blue Screen of Death ab, was sichtlich zum Amüsement des Publikums beitrug.

Nutzung in Virtuellen Maschinen und Emulatoren:


</doc>
<doc id="5578" url="https://de.wikipedia.org/wiki?curid=5578" title="Microsoft Windows Millennium Edition">
Microsoft Windows Millennium Edition

Microsoft Windows Me ist das letzte von Microsoft veröffentlichte Betriebssystem aus der Windows-9x-Linie, die auf MS-DOS aufsetzt. Die Abkürzung "Me" steht für die offizielle Schreibweise "Millennium Edition" (zu Deutsch "Jahrtausend-Ausgabe"). 

Ursprünglich sollte Windows 98 das letzte Betriebssystem der Windows-9x-Linie werden; ein Nachfolger von "Windows NT 5.0", dem späteren Windows 2000, sollte die NT- und 9x-Linien zusammenführen. Die Entwicklung von NT 5.0 war jedoch von massiven Verzögerungen betroffen, die auch die Veröffentlichung des Nachfolgers in weite Ferne rücken ließ. Im März 1999 organisierte Microsoft seine Unternehmensstruktur neu und spaltete vom bisherigen Windows-Team, das mit der Entwicklung von Windows 2000 beschäftigt war, ein Entwicklerteam ab, das sich auf Windows für Heimanwender konzentrieren sollte. Die Öffentlichkeit interpretierte dies zunächst als Plan, eine Version von Windows 2000 für Heimanwender zu entwickeln, aber am 7. April 1999 kündigte Microsoft völlig überraschend an, nun doch einen Nachfolger von Windows 98 zu veröffentlichen, der unter der Bezeichnung "Millennium" bekannt wurde. Dieser plötzliche Umschwung hatte mehrere Gründe. Die Systemanforderungen von Windows 2000 galten weiterhin als zu hoch für Heimanwender, und die Hardware- und Softwareunterstützung war nicht so gut wie bei Windows 9x. Außerdem benötigte die Zielgruppe der Heimanwender nach Ansicht der Entwickler keine der erweiterten Funktionen von Windows 2000 wie etwa einen Verzeichnisdienst, die das Betriebssystem nur aufblähten.

Kurz darauf versuchten die Entwickler, Ideen zu sammeln, die in das neue Produkt einfließen könnten. Diese resultierten in der ersten Vorversion des Betriebssystems, das am 23. Juli 1999 präsentiert wurde. Die Ziele, die sich das Entwicklerteam setzte, waren eine bessere Multimediaunterstützung, eine tiefere Einbindung in das Internet und eine einfachere Einrichtung von Heimnetzwerken. Das Produkt sollte funktionieren, ohne dass sich der Benutzer tiefer mit dem Betriebssystem beschäftigen muss. Um dies zu gewährleisten, sollte das Betriebssystem sogenannte Aktivitätszentren (englisch "") beinhalten. Diese sollten durch eine intuitive, HTML-basierte Bedienung auch seltener benutzte Funktionen, wie das Bearbeiten von Bildern, vereinfachen. Aufgrund von Problemen mit der Einbindung dieser Aktivitätszentren in das Betriebssystem musste das Konzept jedoch verworfen werden; lediglich die Windows-Hilfe sowie die Systemwiederherstellung stellen letzte Überreste der Aktivitätszentren dar. 

Im September 1999 folgte der erste Betatest des Betriebssystems. Ausführliche Berichte über das Betriebssystem lehnte Microsoft jedoch mit der Begründung ab, diese Beta-Version stelle noch nicht den Funktionsstand der Endversion dar. Dies änderte sich erst am 24. November 1999, als die zweite Beta-Version des Betriebssystems veröffentlicht wurde. Die Entwicklung stockte kurz danach, da sich die Entwickler entschieden, den TCP/IP-Protokollstapel von Windows 2000 zu portieren, was viel Zeit beanspruchte. Am 1. Februar 2000 schließlich kündigte Microsoft den endgültigen Namen des Betriebssystems, "Microsoft Windows Millennium Edition", an. Die enge Konzentrierung auf Heimanwender brachte dem Betriebssystem während der Entwicklung Kritik von zahlreichen Fachzeitschriften ein. Ursprünglich sollte das Betriebssystem nicht an MSDN-Abonnenten verteilt werden, was Microsoft aber nach Protesten änderte. Auch der NetWare-Client würde nun doch Bestandteil des Betriebssystems werden. 

Am 11. April 2000 folgte ein dritter Betatest. Probleme mit dem Windows Media Player und dem Internet Explorer sorgten für Verzögerungen im Entwicklungsprozess, sodass das Entwicklungsstadium nicht wie ursprünglich vorgesehen im Mai, sondern erst am 19. Juni 2000 erreicht wurde. Die Veröffentlichung des Betriebssystems folgte am 14. September 2000. Das Betriebssystem kostete, genau wie Windows 98 zuvor, 209 US-Dollar als Vollversion und 109 USD als Upgrade-Variante. Für Benutzer von Windows 98 gab es ein Sonderangebot, sodass sie die Upgrade-Version bereits für 60 USD erwerben konnten.

Der Mainstream-Support für Windows Me lief am 31. Dezember 2003 aus. Ursprünglich sollte der Extended Support ein Jahr später enden, Microsoft verlängerte jedoch den Support von Windows Me zusammen mit Windows 98 bis zum 30. Juni 2006.

Microsoft behauptete anfangs, dass Windows Me, ähnlich wie die Betriebssysteme der Windows-NT-Reihe, nicht mehr auf DOS basiere. Dies stellte sich jedoch schnell als falsch heraus, wenngleich Microsoft zahlreiche Möglichkeiten, den MS-DOS-Modus aufzurufen, aus dem Betriebssystem entfernte. Das in Windows Me vorhandene MS-DOS wurde mit dem Ziel einer kürzeren Startzeit modifiziert. Die Dateien AUTOEXEC.BAT und CONFIG.SYS werden vom Betriebssystem ignoriert und nur Definitionen von Umgebungsvariablen werden ausgewertet, indem diese Einstellungen in die Windows-Registrierungsdatenbank übertragen werden. Die vormals eigenständigen Dateien HIMEM.SYS und SmartDrive wurden in die IO.SYS integriert und diese komprimiert, sodass sie schneller in den Arbeitsspeicher geladen wird. Die Windows-Registrierungsdatenbank selbst wurde ebenfalls optimiert; von den bisherigen Dateien "SYSTEM.DAT" und "USER.DAT" wurde die "CLASSES.DAT" abgespalten, die den Inhalt des Hive codice_1 enthält. So werden während des Startvorgangs nur die nötigen Teile der Registrierung geladen.

Windows Me enthält einige neue Programmierschnittstellen, die vor allem auf die Bedürfnisse von Heimanwendern abzielen. Windows Image Acquisition (WIA) dient dem automatischen Erkennen von Scannern und Digitalkameras, etwa um den Assistenten zum Einscannen eines Bildes zu starten. Auch das Versenden von Bildern per E-Mail direkt vom Scanner, ohne das Bild auf der Festplatte speichern zu müssen, ist so möglich. Über DirectPlay Voice können Spieler über ein Mikrofon direkt miteinander reden, sofern das Spiel für diese Schnittstelle programmiert wurde. Mithilfe von Universal Plug and Play (UPnP) können kompatible Geräte vom Betriebssystem aus konfiguriert werden.

Bereits beim ersten Start des Betriebssystems wird ein Assistent geladen, der den Benutzer durch die ersten Schritte der Einrichtung des Betriebssystems führt. Die Figur "Merlin", ein Zauberer aus Microsoft Agent, dient während dieses Prozesses als Hilfefunktion. Neben einem Tutorial zur Benutzung der Maus sind dies insbesondere die Regionseinstellungen und die Eingabe des Lizenzschlüssels. Danach geht der Startprozess nahtlos in einen Willkommensbildschirm über, der die neuen Funktionen von Windows Me vorstellt.

Die Benutzeroberfläche von Windows Me entspricht größtenteils dem Windows-2000-Pendant. Windows Me enthält den Internet Explorer 5.5 sowie den Windows Media Player 7.0, der sich stark von der vorherigen Version unterscheidet. Neu ist der Windows Movie Maker, ein einfaches Videoschnittprogramm, mit dem Videos aufgenommen und bearbeitet werden können. Aus "Plus! für Windows 98" übernommen wurde die Funktion Komprimierte Ordner, die ZIP-Dateien auch ohne Programme von Drittherstellern unterstützt. Zu den Spielen wurden, neben dem ebenfalls aus Plus! stammenden Spiel "Spider Solitär" sowie dem Spiel "3D Pinball: Space Cadet" aus Windows NT 4.0, einige simple Online-Spiele hinzugefügt, die nur im Internet gespielt werden können. Ein neuer Assistent erleichtert das Einrichten eines Heimnetzwerkes. Die Hilfefunktion wurde in Windows Me komplett überarbeitet, sie vereint nun die Hilfedateien aller Windows-Bestandteile und erlaubt auch das Stellen von Supportanfragen über das Internet.

Ähnlich wie Windows 2000 enthält Windows Me die Systemdateiüberprüfung, die wichtige Systemdateien des Betriebssystems überwacht und diese durch Sicherungskopien ersetzt, wenn sie verändert oder gelöscht werden. Die neu eingeführte Systemwiederherstellung sichert in regelmäßigen Abständen, wenn Anwendungen installiert werden oder auf Wunsch des Benutzers die wichtigsten Dateien des Betriebssystems und erlaubt im Bedarfsfall die Wiederherstellung auf einen früheren Stand. Sie ersetzt das ältere Programm Microsoft Backup, welches dennoch auf der Windows Me-CD vorhanden ist. Über die neue Funktion "Automatische Updates" kann das Betriebssystem automatisch auf den neuesten Stand gehalten werden.

Die Mindestvoraussetzungen zur Installation von Windows Me sind ein Intel-Pentium-Prozessor mit 150 MHz, 32 MB Arbeitsspeicher, 320 MB freier Festplattenspeicher, ein CD-ROM-Laufwerk sowie ein Diskettenlaufwerk, eine VGA-kompatible Grafikkarte und eine Soundkarte mitsamt Lautsprecher. Neben der Vollversion war auch eine Upgrade-Version erhältlich, mit der ein bestehendes Windows 95 oder Windows 98 auf Windows Me aktualisiert werden konnte.

Die Resonanz auf Windows Me war zunächst gemischt. Es wurde bemängelt, dass zahlreiche Neuerungen von Windows Me, wie etwa der neue Internet Explorer, auch für ältere Betriebssysteme verfügbar seien, wodurch es weniger Anreize für eine Aktualisierung des Betriebssystems gäbe. Kritisiert wurde zudem die Tatsache, dass sich der Windows Media Player und der Movie Maker nicht deinstallieren ließen, und das obwohl genau zu dieser Zeit Microsoft rechtliche Konsequenzen wegen Ausnutzung seiner Monopolstellung im Browserkrieg drohten.

Bald nach der Veröffentlichung jedoch kippte die Meinung stark ins Negative, denn zahlreiche Fehler im Betriebssystem brachten Windows Me einen schlechten Ruf ein. Bereits am Tag der Veröffentlichung wurde eine Sicherheitslücke bekannt, durch die Windows Me zum Absturz gebracht werden konnte. Vor allem Instabilität und Kompatibilitätsprobleme mit Anwendungen und Treibern sorgten für Unmut bei den Anwendern. Aber auch neue Funktionen von Windows Me waren von den Fehlern betroffen; die Systemwiederherstellung etwa stellte aufgrund eines Fehlers ihren Dienst nach dem 8. September 2001 ein, sodass neuere Wiederherstellungspunkte nicht mehr funktionierten.



</doc>
<doc id="5579" url="https://de.wikipedia.org/wiki?curid=5579" title="Microsoft Windows NT">
Microsoft Windows NT

Windows NT (ursprünglich von "N-Ten", einem Simulator, auf dem das System in der Anfangsphase betrieben wurde und später für "New Technology") ist ein Kernel, der bei Betriebssystemen der Windows-Reihe des US-amerikanischen Unternehmens Microsoft zum Einsatz kommt. Seit seiner Version 5.0 wird Windows NT nicht mehr als Teil des Produktnamens, sondern nur noch als internes Versionskürzel verwendet.

Die Entwicklung an Windows NT begann, als die Allianz zwischen dem US-amerikanischen Unternehmen IBM und Microsoft zur Entwicklung des Betriebssystems OS/2 zerbrach.

Leiter des NT-Projekts wurde David N. Cutler. Er galt als einer der renommiertesten Entwickler von Betriebssystemen überhaupt und war maßgeblich an der Entwicklung des Betriebssystems VMS beteiligt gewesen, weshalb der Windows NT-Kernel viele Ähnlichkeiten mit VMS aufweist. Microsoft warb ihn und Mitglieder seines Teams von DEC ab und setzte sie auf die Entwicklung eines neuen Betriebssystems an. Diese Abwerbung beantwortete DEC mit einer Klage, die Microsoft durch die Zahlung von 150 Millionen US-Dollar und die Zusage, mit Windows NT auch Alpha-Prozessoren zu unterstützen, beilegen konnte.

Cutler setzte sich zwei wesentliche Ziele für Windows NT. Ihm ging es darum, "Zuverlässigkeit" zu erreichen – eine abstürzende Anwendung sollte nicht mehr das gesamte System zum Absturz bringen können. Diese Stabilität war unter Betriebssystemen wie VMS oder unixoiden Systemen längst üblich. Auch wichtig war "Portabilität" – Windows NT sollte auf allen modernen Computerarchitekturen lauffähig sein. Außerdem sollte Windows NT, ähnlich wie es der Mach-Kernel konnte, als Basis für verschiedene Betriebssysteme gleichzeitig dienen und so z. B. Windows-, MS-DOS-, OS/2- und POSIX-Programme gleichzeitig ablaufen lassen können. Der Arbeitstitel während der Entwicklung hieß demnach auch "Portasys."

Nach Aussage des früheren Microsoft-Mitarbeiters Mark Lucovsky stand NT ursprünglich für "N-Ten." Dies war der Codename für den in Entwicklung befindlichen Prozessor Intel i860. Er war als Plattform für NT gedacht, lag jedoch nicht bei Microsoft vor. Deshalb wurde auf einem Emulator entwickelt. Zu Vermarktungszwecken wurde das Kürzel später in "New Technology" umgedeutet.

Die erste ausgelieferte Version hatte die Versionsnummer 3.1. So sollte ein Bezug zu Windows 3.1 hergestellt werden, das die gleiche grafische Benutzeroberfläche besaß und beim Erscheinen von Windows NT die aktuell auf dem Markt erhältliche DOS-basierte Windows-Version darstellte.

Nach Windows NT 4.0 wurden das Kürzel NT und die Versionsnummer im Produktnamen fallen gelassen. Die Nachfolgeversionen werden Windows 2000, Windows XP, Windows Server 2003, Windows Vista, Windows Server 2008 (sowie R2), Windows 7, Windows 8, Windows Server 2012 (sowie R2), Windows 8.1, Windows 10 und Windows Server 2016 genannt. Alle geben in der Umgebungsvariablen codice_1 als Betriebssystem codice_2 an. Windows 2000 weist noch im Startbildschirm mit dem Text „Auf NT-Technologie basierend“ auf die Verwandtschaft hin.

Cutler hatte seine zwei Primärziele erreicht: Das neue Betriebssystem war stabil, lief aufgrund seiner modularen Entwicklung auf mehreren Plattformen (MIPS und x86, später auch PowerPC und Alpha) und bot verschiedenen Programmarten Unterstützung. Es liefen sowohl 16-Bit-Windows-3.x-Programme als auch Programme für das neue 32-Bit-Windows-NT-API sowie textbasierte OS/2-Software und POSIX-1.0-kompatible Programme. Über die Jahre fand hier allerdings wieder eine Rück- bzw. Weiterentwicklung statt. Die OS/2- und POSIX-Versionen wurden zunächst nicht weiter gepflegt und später entfernt. Die Versionen für PowerPC, MIPS und Alpha wurden eingestellt, dafür kamen später IA-64- und x64-Versionen und mit Windows RT auch eine ARM-Version hinzu, wobei letztere die Ausführung von Win32-Anwendungen, die nicht durch Microsoft signiert worden sind, nicht mehr unterstützt.

In den ersten NT-Versionen läuft das GDI zusammen mit den anderen Subsystemen auf Ring 3 der Intel-Privilegstufe außerhalb des Kernel-Bereichs. Dadurch ist der Kernel selbst vor Abstürzen in den Programmen geschützt. Ab NT 4.0 läuft das Grafiksubsystem aus Geschwindigkeitsgründen teilweise direkt im Kernel, womit Fehler in Grafiktreibern moderne Windows-NT-Versionen zum Absturz bringen können. Windows Vista verwendet mit dem neuen Grafiktreiber-Modell allerdings wieder Userspace-Treiber.

Windows NT besitzt einen modularen Aufbau. Die unterste Ebene bildet die Hardwareabstraktionsschicht (engl. "Hardware Abstraction Layer," abgekürzt HAL). Darauf bauen der eigentliche Kernel (ein Hybridkernel) und die Subsysteme auf. Der Kernel kümmert sich um die Vergabe von Arbeitsspeicher und Rechenzeit. Auf den Kernel setzen die Subsysteme auf. Dem Win32-Subsystem kommt dabei die größte Bedeutung zu, da es sich auch um den Aufbau der grafischen Benutzeroberfläche kümmert und die Signale der Eingabegeräte verarbeitet. In den Enterprise- und Ultimate-Editionen von Windows Vista sind die Microsoft Windows Services for UNIX in Form eines POSIX-kompatiblen Subsystems für UNIX-basierte Applikationen enthalten.

Aus Kompatibilitäts- und Geschwindigkeitsgründen, vor allem für Spiele, entwickelte Microsoft die DOS-basierte Betriebssystemlinie Windows 3.x/9x neben NT zunächst weiter. Erst mit dem Erscheinen von Windows XP wurde die DOS-basierte Linie aufgegeben, wobei Windows XP (wie der Vorläufer Windows 2000) einen reinen NT-Kernel hat.

Bereits die erste Windows-NT-Version war vollständig von MS-DOS losgelöst. Aus Gründen der Abwärtskompatibilität konnten allerdings ältere 16-Bit-DOS-Programme wie beispielsweise der MS-DOS-Kommandozeileninterpreter COMMAND.COM in einer Virtual DOS Machine ausgeführt werden. Programme, die direkt (also ohne das Subsystem von Windows) auf die Hardware zugreifen, werden aus Sicherheitsgründen nicht mehr ausgeführt. Zusätzlich stand dem Anwender ein weiterentwickelter, vollständig 32-Bit-fähiger Kommandozeileninterpreter namens cmd.exe zur Verfügung. Außerdem unterstützte Windows NT bereits in der Version 3.1 das Dateisystem NTFS "(New Technology File System)" und verfügt seit jeher über einen 32-Bit-Kernel.

 RTM Build Final




</doc>
<doc id="5580" url="https://de.wikipedia.org/wiki?curid=5580" title="Microsoft Windows 2000">
Microsoft Windows 2000

Windows 2000, kurz "W2K" oder "Win 2k" (von Kilo: „2k“ = 2000), ist ein Betriebssystem von Microsoft. Es ist eine Weiterentwicklung von Windows NT 4.0 und der Vorgänger von Windows XP. Die interne Bezeichnung bei Microsoft lautet Windows NT 5.0.

Die Planungen für Windows NT 5.0, der ursprüngliche Name von Windows 2000, begannen kurz nach der Veröffentlichung von Windows NT 4.0. Mit dem neuen Betriebssystem wollte Microsoft die Administrationskosten für Unternehmen senken, das hauptsächlich durch die Einführung eines Verzeichnisdienstes namens Active Directory geschehen sollte. Das Betriebssystem sollte Ende 1997 veröffentlicht werden. Anfang 1997 verteilte Microsoft eine Vorversion von Active Directory an Entwickler, gleichzeitig kündigte das Unternehmen an, dass sich die Fertigstellung des Betriebssystems in das Jahr 1998 verschiebe. Diese erste Verzögerung wurde zunächst begrüßt, da die Presse sich davon ein stabileres Betriebssystem erhoffte und viele Unternehmen ohnehin mit der Migration auf den Vorgänger Windows NT 4.0 beschäftigt seien.

In darauffolgenden Presseständen von Microsoft, unter anderem auf der CeBIT im März 1997 und auf der WinHEC im Mai 1997, erläuterte das Unternehmen die Ziele des neuen Betriebssystems. Windows NT 5.0 sollte die Windows-9x- und Windows-NT-Linien vereinigen und in diesem Zuge Funktionen wie Plug and Play und USB-Unterstützung beinhalten. Neben der bisherigen 32-Bit-Version sollte es erstmals auch eine 64-Bit-Version von Windows für den Alpha-Prozessor von DEC und einem Prozessor von Intel mit dem Codenamen "Merced" (dem späteren Intel Itanium) geben. Wie NT 4.0 sollte NT 5.0 in einer Workstation-, Server- und Enterprise-Edition erscheinen. Microsoft lizenzierte am 12. Mai 1997 eine Mehrbenutzertechnologie von Citrix, die neben NT 4.0 (in Form der Terminal Server Edition) auch Bestandteil von NT 5.0 sein sollte.

Auf der COMDEX im Frühjahr 1997 kündigte Microsoft einen Betatest im Zeitraum August/September und eine Veröffentlichung Anfang 1998 an, was in etwa dem Entwicklungszeitraum von Windows NT 4.0 entsprach. Das Betatest-Datum wurde später auf den September 1997 festgesetzt, was sich später als großer Fehler herausstellte, da die Entwickler weit hinter dem Zeitplan lagen und nicht in der Lage waren, in so kurzer Zeit eine Beta-Version des Betriebssystems mit den zuvor versprochenen Funktionen fertigzustellen. Als am 20. September 1997 die erste Beta-Version schließlich veröffentlicht wurde, galt sie als instabil und unausgereift; zahlreiche Neuheiten des Betriebssystems waren in dieser Vorversion nicht vorhanden oder funktionsuntüchtig. Der Termin für den zweiten Betatest, der für den 15. Dezember 1997 vorgesehen war, musste in das Jahr 1998 verschoben werden. In diesem Zuge war auch der geplante Veröffentlichungstermin Anfang 1998 nicht zu halten und musste zum Ende des Jahres verschoben werden. Einige Zeitschriften schrieben gar, dass mit einer Fertigstellung erst 1999 zu rechnen sei. Der zweite Betatest wurde zunächst für den April 1998 versprochen, aber auch dieser Termin fiel schließlich und so veröffentlichte das Unternehmen zunächst nur eine Vorversion im März.

Im Februar 1998 bestätigte Microsoft, dass zu ambitionierte Ziele Schuld an den massiven Verzögerungen im Entwicklungsprozess seien. Zu den Plänen, die für Windows NT 5.0 vorgesehen waren, zählten etwa eine TV-Funktion (die später unter der Bezeichnung "WebTV" Bestandteil von Windows 98 wurde) sowie ein Projekt mit dem Codenamen "Chrome", das DirectX und HTML kombinieren sollte, um Multimediainhalte im Web bereitzustellen, aber letztendlich nie realisiert wurde. Spekulationen, wonach gar das von Anfang an versprochene Active Directory dem Entwicklungsprozess zum Opfer fallen könnte, dementierte Microsoft klar. Der zweite Betatest wurde auf den Juni 1998 festgesetzt, das Endprodukt sollte nunmehr tatsächlich Anfang 1999 erscheinen. Der endgültige Termin für den zweiten Betatest war, nach weiteren Verzögerungen, der 18. August 1998. Zwar enthielt diese Version laut Microsoft alle für das Endprodukt vorgesehenen Funktionen, sie galt jedoch auch als instabil und unausgereift. Aufgrund dessen plante Microsoft einen dritten Betatest zu einem noch unbestimmten Zeitpunkt.

Am 27. Oktober 1998 wurde dann der Name "Windows 2000" durch Microsoft offiziell festgelegt. Dieser Schritt war in der Öffentlichkeit äußerst kontrovers, da Windows NT bislang der Name für Business-Betriebssysteme war, während hingegen die Bezeichnung Windows ohne Zusatz mit Consumer-Betriebssystemen assoziiert wurde. Diese Entscheidung sollte sich erst im Nachhinein als richtig herausstellen, denn viele Nutzer sahen Windows 2000 nach seiner Veröffentlichung als ein besseres Betriebssystem als Windows NT an, obwohl Windows 2000 letztlich auch nur eine Version von Windows NT ist. Gleichzeitig wechselten die drei Versionen des Betriebssystems ihren Namen; sie hießen nunmehr "Professional", "Server" und "Advanced Server". Der Windows 2000 Server würde anders als die bisherigen Server-Versionen von Windows NT nur noch zwei statt vier Prozessoren unterstützen, der Advanced Server nur noch vier statt acht Prozessoren. Neu angekündigt wurde der "Datacenter Server", eine Version für große Rechenzentren, die bis zu 16 Prozessoren und 64 Gigabyte Arbeitsspeicher unterstützen werde.

Im Januar 1999 erklärte Microsoft, dass das Endprodukt erst am 25. Februar 2000 erscheinen werde; der dritte Betatest sollte im April 1999 stattfinden. Zu dieser erneuten Verzögerung trugen zahlreiche Faktoren bei: die parallel verlaufende Hardwareentwicklung und die dadurch entstehende Notwendigkeit, Treiber für diese neue Hardware zu schreiben (etwa den Pentium-III-Prozessor), Vorsorgemaßnahmen aufgrund des Jahr-2000-Problems, die zuvor erfolgte Namensänderung sowie die parallele Arbeit an der 64-Bit-Version. Aufgrund dessen entstanden kurzzeitig Gerüchte um eine Version des Betriebssystems, der zwar bestimmte Funktionen, wie das Upgrade einer bestehenden Windows-NT-Domäne auf Active Directory, fehlen würden, die aber die zeitliche Lücke bis zur endgültigen Veröffentlichung schließen sollte. Der dritte Betatest, an dem 650.000 Betatester teilnahmen, startete schließlich am 30. April 1999. Diese Version löste die Probleme, die in vergangenen Betatests auftraten, und brachte große Hoffnungen auf die Endversion.

Am 1. Juli 1999 folgte der Release Candidate von Windows 2000. Auch wenn das Betriebssystem nochmals stabiler war als beim letzten Betatest, zeigten sich immer noch Probleme im Zusammenhang mit Active Directory. Am 18. August 1999 entschied Microsoft, dass die Server-Varianten von Windows 2000 die doppelte Anzahl an Prozessoren unterstützen würden - 4 beim Server, 8 beim Advanced Server und 32 beim Datacenter Server. Damit revidierte Microsoft seine frühere Entscheidung, die Anzahl der unterstützten Prozessoren im Vergleich zu NT 4.0 zu reduzieren und reagierte auf das zu erwartende Erscheinen von Systemen mit acht Prozessoren.

Der zweite Release Candidate, der ursprünglich am 6. September folgen sollte, erschien schließlich wenige Tage später am 15. September. Doch auch dies sollte nicht der letzte Release Candidate sein; Microsoft brachte am 17. November eine dritte Version heraus, und am 15. Dezember erreichte Windows 2000 schließlich den Status Release to Manufacturing. Am 17. Februar 2000 erschienen schließlich Windows 2000 Professional, Server und Advanced Server. Der Start von Windows 2000 drohte zunächst, überschattet zu werden: laut einer internen Nachricht von Microsoft solle Windows 2000 63.000 Fehler haben. Es stellte sich jedoch heraus, dass dies nur das Resultat eines Programms sei, das den Quelltext von Windows 2000 automatisiert überprüfe und daher nichts mit der Anzahl der Fehler im Betriebssystem zu tun habe. Der Windows 2000 Datacenter Server kam am 26. September 2000 auf den Markt.

Der "Mainstream Support" von Windows 2000 lief am 30. Juni 2005 aus. Der "Extended Support", in dessen Rahmen Sicherheitsaktualisierungen veröffentlicht wurden, endete am 13. Juli 2010.

Parallel zur 32-Bit-Version arbeitete ein separates Entwicklerteam, geführt von David N. Cutler, an der 64-Bit-Version des Betriebssystems, die Anfang 2000 für den Alpha-Prozessor und später gemeinsam mit der Veröffentlichung des Itanium-Prozessors auch für diese Architektur erscheinen sollte. Diese Version sollte nicht nur mehr als die bei 32-Bit-Prozessoren adressierbaren 4 Gigabyte an Arbeitsspeicher unterstützen, sondern noch einige zusätzliche Funktionen beinhalten, um sie für Großunternehmen attraktiver zu machen. Compaqs Ankündigung, die Entwicklung von Alpha-Prozessoren zu beenden, brachte jedoch nicht nur das Ende für die 32-Bit-Version, die sich bereits in der Release Candidate-Phase befand, sondern auch für die 64-Bit-Version. Da jedoch funktionierende Prototypen des Itanium-Prozessors fehlten und es auch keine anderen 64-Bit-Systeme gab, die für Windows 2000 in Frage gekommen wären, arbeiteten die Entwickler vorerst weiter mit Alpha-Rechnern.

Im August 1999 demonstrierten Microsoft und Intel erstmals Windows 2000 auf einem Prototyp eines Itanium-Systems. Im Juni 2000 erschien eine Vorversion der 64-Bit-Version von Windows 2000; diese erhielten die Besitzer der 5.000 bis dahin ausgelieferten Itanium-Prototypen. Danach endeten die Arbeiten an der 64-Bit-Version von Windows 2000; diese wurde fortan auf Basis des Nachfolgers, Windows Whistler, entwickelt.

Für Windows 2000 erschienen insgesamt vier Service Packs. Diese erschienen erstmals in zwei Version. Zum einen ist dies die "Webinstallation", die automatisch den Versionsstand des Betriebssystems überprüft und nur die Dateien herunterlädt, die aktualisiert werden müssen. Zum anderen ist dies die "Netzwerkinstallation", die wie bisher sämtliche Dateien enthält. Zudem konnte wie bisher das Service Pack als CD bestellt werden.

Eine Neuheit der Service Packs für Windows 2000 ist das sogenannte Slipstreaming. Dabei können die Dateien des Service Packs in das Installationsverzeichnis von Windows 2000 integriert werden, sodass bei einer Neuinstallation des Betriebssystems die Installation des Service Packs nicht mehr notwendig ist.

Das erste Service Pack für Windows 2000 kam am 31. Juli 2000 heraus. Das Service Pack selbst beschränkte sich dabei größtenteils auf die Behebung der Programmfehler, die seit dem Erscheinen von Windows 2000 entdeckt wurden. Eine Neuheit, die nur auf der Service Pack-CD enthalten war, aber auch separat aus dem Internet heruntergeladen werden konnte, war der "Terminal Services Advanced Client", eine Erweiterung der Terminaldienste des Windows 2000 Servers. Enthalten waren ein ActiveX-Client, mit dem eine Verbindung auch über das Internet mittels des Internet Explorers hergestellt werden konnte, ein Snap-In der Terminaldiensteverwaltung für die Microsoft Management Console, sowie ein Windows-Installer-Paket, mit dem das Clientprogramm auf Windows-2000-Clients installiert werden kann.

Das Service Pack 2 folgte am 16. Mai 2001. Da mit diesem Service Pack die Exportbeschränkungen der USA bezüglich Kryptografieverfahren entfielen, aktualisierte das Service Pack die Verschlüsselungsverfahren auf 128 Bit, einschließlich der Systeme außerhalb der USA, die bisher auf eine maximale Schlüssellänge von 56 Bit beschränkt waren.

Mit dem Service Pack 2 unterstützte das Betriebssystem erstmals den Kompatibilitätsmodus, der Probleme mit Anwendungen lösen soll, die für Windows NT 4.0 oder Windows 95 geschrieben wurden und unter Windows 2000 standardmäßig nicht korrekt ausgeführt werden. Der Kompatibilitätsmodus ist standardmäßig deaktiviert, kann aber bei Bedarf aktiviert werden. Zudem wird er nur auf Windows 2000 Professional installiert, für die Serverversionen konnte der Kompatibilitätsmodus allerdings aus dem Internet heruntergeladen werden.

Am 1. August 2002 veröffentlichte Microsoft das Service Pack 3. Mit diesem Service Pack erhielt Windows 2000 die Funktion "Automatische Updates", die im Hintergrund automatisch nach verfügbaren Aktualisierungen sucht und den Anwender informiert, falls neue Aktualisierungen verfügbar sind. Zudem können mit dem Service Pack 3 die Standardprogramme wie Webbrowser und E-Mail-Programm konfiguriert werden. Der in Windows 2000 installierte Windows Installer wird mit diesem Service Pack auf die Version 2.0 aktualisiert.

Mit dem Service Pack 3 unterstützt Windows 2000 48-Bit-LBA und kann damit korrekt mit Festplatten umgehen, die größer sind als 137 GB. Die Unterstützung muss jedoch manuell in der Windows-Registrierung aktiviert werden. Zudem können mit dem Service Pack 3 Computercluster erstmals in Active Directory integriert werden.

Das letzte Service Pack für Windows 2000 erschien am 26. Juni 2003. Mit diesem Service Pack unterstützte Windows 2000 erstmals USB-2.0-Controller. Zudem führte das Service Pack 4 eine Unterstützung für drahtlose Netzwerke nach dem IEEE-802.11-Standard ein, die dem Nachfolgebetriebssystem Windows XP entnommen wurde und ähnlich funktioniert, aber im Vergleich zu diesem einige Einschränkungen besitzt. So muss zur Herstellung einer Verbindung ein Programm des Adapterherstellers verwendet werden, außerdem kann immer nur ein drahtloser Netzwerkadapter verwendet werden und nicht mehrere gleichzeitig.

Das Service Pack 4 enthält im Gegensatz zu früheren Versionen keine Aktualisierungen der Microsoft Virtual Machine mehr, diese können jedoch manuell heruntergeladen und installiert werden. Die Service Pack-CD enthält zudem Updates für das Windows 2000 Resource Kit; diese betreffen die Netzwerkdiagnoseprogramme sowie das Programm Sysprep.

Nach dem Service Pack 4 plante Microsoft zunächst ein Service Pack 5. Im November 2004 kündigte Microsoft jedoch an, dass es kein Service Pack 5 mehr geben würde, stattdessen sollten die neuesten Aktualisierungen in Form eines Update-Rollup-Pakets erscheinen. Dieses Update-Rollup-Paket kam am 28. Juni 2005 heraus, setzte ein installiertes Service Pack 4 voraus und enthielt alle seitdem erschienen Hotfixes. Da das Update-Rollup-Paket einige Fehler enthielt, erschien am 13. September 2005 eine aktualisierte Version.

Windows 2000 wurde in vier Versionen veröffentlicht: "Professional", "Server", "Advanced Server" und "Datacenter Server". Eine Embedded-Version wie zuvor bei Windows NT 4.0 war zwar geplant, Microsoft gab aber am 24. April 2000 das Ende der Entwicklungsarbeiten an dieser Version bekannt.


Die Benutzeroberfläche von Windows 2000 entspricht der des zuvor erschienenen Windows 98; sie profitiert zudem von einigen Verbesserungen durch den im Betriebssystem enthaltenen Internet Explorer 5.0. Darüber hinaus enthält Windows 2000 nur kleinere Neuheiten; so unterstützt Windows 2000 personalisierte Menüs, das heißt, selten benutzte Verknüpfungen im Startmenü werden automatisch ausgeblendet. Windows 2000 ermöglicht zudem wie Windows 98 SE die Internetverbindungsfreigabe.

Windows 2000 enthält eine neue Version des Dateisystems NTFS. Zu den neuen Funktionen dieser Version zählen etwa Datenträgerkontingente, mit denen der von einem Benutzer beanspruchbare Festplattenspeicher festgelegt werden kann, sowie das Encrypting File System, mit dem Dateien auf der Festplatte verschlüsselt werden können. Zudem unterstützt NTFS mit dieser Version erstmals Sparse-Dateien. Ältere Versionen von Windows sind mit der neuen Version von NTFS nicht kompatibel, das Service Pack 4 für Windows NT 4.0 enthält jedoch einen Patch, der das Lesen und Schreiben von mit Windows 2000 erstellten NTFS-Partitionen ermöglicht. Windows 2000 unterstützt zudem das FAT32-Dateisystem, welches bereits in den Consumer-Versionen von Windows Verwendung fand.

Die Systemdateiüberprüfung überwacht wichtige Systemdateien des Rechners und ersetzt sie automatisch, falls sie beschädigt oder gelöscht werden sollten. Windows 2000 beinhaltet zudem erstmals ein Defragmentierungsprogramm, eine beschränkte Version des Programms Diskeeper von "Executive Software". Im Gegensatz zu Windows NT 4.0, das nur bestimmte Komponenten von DirectX implementierte, bietet Windows 2000 eine vollständige Unterstützung von DirectX.

Eine der größten Neuheiten der Serverversionen von Windows 2000 ist Active Directory. Dabei handelt es sich um einen auf LDAP basierenden Verzeichnisdienst, in dem alle Ressourcen des Netzwerks, wie Benutzer, Gruppen und Computer zentral hierarchisch verwaltet werden. Active Directory verwendet Dynamisches DNS, um die Netzwerkressourcen zu adressieren. Im Gegensatz zu Windows NT 4.0 kann jeder Server zu einem Domänencontroller werden, ohne dass das Betriebssystem neu installiert werden muss. Mit Active Directory kommt auch Kerberos, ein ticket-basiertes System zur Authentifizierung von Personen. Ähnlich wie Windows 98 können Benutzer sich in Windows 2000 mittels einer Smartcard authentifizieren.

Windows 2000 unterstützt Gruppenrichtlinien. Damit können Berechtigungen für einen Computer gesetzt werden, etwa das Recht, die Systemsteuerung aufzurufen. Zudem führt Windows 2000 das verteilte Dateisystem DFS ein, das es ermöglicht, Ressourcen, die sich auf mehreren Servern befinden, unter einem Namen zusammenzufassen.

Mit "Routing und RAS" enthält Windows 2000 eine Erweiterung des bis Windows NT 4.0 enthaltenen RAS-Dienstes. Dieses enthält eine verbesserte und vereinfachte Benutzeroberfläche und ermöglicht erstmals Network Address Translation (NAT), ähnlich wie es bereits bei der Internetverbindungsfreigabe verwendet wird. Für VPNs bietet Windows 2000 das L2TP-Protokoll, welches auf IPsec basiert und sicherer ist als das ältere PPTP-Protokoll. Zudem entfällt die Beschränkung von 256 gleichzeitigen Verbindungen, prinzipiell können sich beliebig viele RAS-Clients mit einem Windows-2000-Server verbinden. In einem kleinen Netzwerk kann Windows 2000 mithilfe von APIPA die IP-Adressen automatisch ohne administrative Konfiguration zuweisen.

Die Fernsteuerungsfunktionen, die erstmals mit der Windows NT 4.0 Terminal Server Edition eingeführt wurden, sind Bestandteil aller Serverversionen von Windows 2000. Diese führt zudem eine neue Version des Remote Desktop Protocols ein, mit der Daten vom Server auf dem Drucker des Clients gedruckt werden können und eine auf Text und Dateien beschränkte gemeinsame Zwischenablage ermöglicht wird. Die Terminaldienste unter Windows 2000 unterstützen zwei Modi: den Remoteverwaltungsmodus, der lediglich zur Administration des Servers gedacht ist und nur bis zu zwei eingehende Verbindungen ermöglicht, und der Anwendungsservermodus, der zur Einrichtung einer Thin-Client-Umgebung dient und die Ressourcenverteilung des Servers entsprechend anpasst. Clients, die sich mit einem Terminalserver im Anwendungsservermodus verbinden wollen, benötigen eine Lizenz von einem Lizenzserver; dieser muss innerhalb von 90 Tagen bei Microsoft aktiviert werden, danach warnt das System bei jeder Remoteanmeldung, dass die Lizenz abgelaufen ist.

Windows 2000 besitzt einen modularen Aufbau. Die unterste Ebene bildet der HAL. Darauf bauen der eigentliche Betriebssystem-Kern und die Subsysteme auf. Der HAL selbst wurde für frühere Windows-NT-Versionen hardwareunabhängig entwickelt. Der Betriebssystemkern kümmert sich um die Vergabe des Arbeitsspeichers und der Rechenzeit. Auf den Kern setzen die verschiedenen Subsysteme (Win32, OS2 und POSIX) auf. Dem Win32-Subsystem kommt dabei die größte Bedeutung zu, da es sich auch um den Fensteraufbau kümmert und die Signale der Eingabegeräte verarbeitet. Mit Windows NT 4.0 hat Microsoft Teile des GDI-Systems mit in den Kernel-Bereich genommen.

Windows 2000 führt zwei neue Module des Betriebssystemkerns ein. Dies ist zum einen der "PnP-Manager", der Plug and Play implementiert und es so Windows 2000 ermöglicht, ähnlich wie Windows 95 und Windows 98 angeschlossene Hardware automatisch zu erkennen und zu installieren. Zum anderen ist dies der "Power-Manager", der die Stromsparfunktionen des ACPI-Standards implementiert, dadurch kann Windows 2000 erstmals in den Standby-Modus oder den Ruhezustand geschaltet werden. Dies erfordert allerdings neue Gerätetreiber, die mit dem Power-Manager kompatibel sind – werden ältere Gerätetreiber, etwa für Windows NT 4.0, verwendet, stehen die Stromsparfunktionen nicht zur Verfügung.

Mit Windows 2000 führt Microsoft Unterstützung für Physical-Address Extension (PAE) ein, um Arbeitsspeicher über 4 GB adressieren zu können. Die Address Windowing Extension bietet Programmen durch neue Programmierschnittstellen die Möglichkeit, auf diesen zusätzlichen Arbeitsspeicher zugreifen zu können, indem die entsprechenden Speicherbereiche in den virtuellen Speicher des Programms eingeblendet werden. Zwar ist diese Funktionalität in allen Versionen von Windows 2000 vorhanden, jedoch können nur der Advanced Server und der Datacenter Server mehr als 4 GB Arbeitsspeicher nutzen.

Die Subsysteme arbeiten in der Regel nur auf Ring 3 (Privilegierungsstufe). Dadurch ist der Betriebssystemkern selbst vor Abstürzen in den Programmen geschützt.

Windows 2000 unterstützt das Windows Driver Model, mit dem es unter anderem möglich ist, Gerätetreiber zu schreiben, die sowohl mit Windows 2000 als auch mit Windows 98 kompatibel sind. Das Betriebssystem enthält zahlreiche neue Gerätetreiber, unter anderem unterstützt es erstmals in der NT-Reihe USB-Geräte.

Die Systemvoraussetzungen für Windows 2000 Professional sind ein Pentium-Prozessor mit 133 MHz, 64 MB Arbeitsspeicher, eine 2 GB große Festplatte mit mindestens 650 MB freiem Speicherplatz und ein CD-ROM-Laufwerk. Eine Aktualisierung ist sowohl von Windows NT Workstation 4.0 und 3.51, als auch von Windows 95 und 98 möglich. Die Systemvoraussetzungen für Windows 2000 Server und Advanced Server sind ähnlich, sie benötigen allerdings 128 MB Arbeitsspeicher und 1 GB freier Festplattenspeicher. Mit Windows 2000 Server kann eine bestehende Installation von Windows NT Server 3.51 und 4.0 sowie der Terminal Server Edition aktualisiert werden, Windows 2000 Advanced Server erlaubt zusätzlich eine Aktualisierung der Windows NT Server 4.0 Enterprise Edition.

Systeme, die mit Windows 2000 Datacenter Server ausgeliefert werden sollen, müssen mindestens acht Prozessoren unterstützen; soll das System in einer Clusterumgebung verwendet werden, müssen auch tatsächlich acht Prozessoren vorhanden sein. Ansonsten sind mindestens ein Pentium III Xeon-Prozessor, 256 MB Arbeitsspeicher, eine 2 GB große Festplatte mit mindestens 1 GB freiem Speicherplatz sowie ein CD-ROM-Laufwerk erforderlich. Da der Datacenter Server ausschließlich auf dafür spezialisierter Hardware verwendet werden soll, ist ein Upgrade eines bestehenden Betriebssystems nicht vorgesehen.

Sollte der Rechner nicht in der Lage sein, von einer CD zu starten, enthält Windows 2000 Professional einen Diskettensatz bestehend aus vier Startdisketten.

Microsoft unterstützte Windows 2000 bis 13. Juli 2010 mit sicherheitskritischen Korrekturen („"Extended Support"“). Viele Firmen gingen davon aus, dass das System bis dahin noch ausreichend war, so waren in Deutschland Ende 2009 rund 61.000 Installationen mit Windows 2000 Server im Betrieb. Um 2010 wurde Windows 2000, neben Windows XP, noch oft in Kontoauszugsdruckern verwendet. Nach dem Ende des "Extended Support" hat Microsoft automatische Aktualisierungen über Windows Update für Windows 2000 eingestellt, ein frisch installiertes Windows 2000 lässt sich daher nicht mehr automatisch auf den letzten Stand bringen.

Bis zum Ende des Supports konnte das Betriebssystem – gegebenenfalls durch Softwarekomponenten anderer Hersteller – in allen wichtigen Anwendungsbereichen Office, Internet und Multimedia mit der aktuellen Entwicklung Schritt halten. Das letzte unter Windows 2000 nutzbare Microsoft Office ist die Version 2003. OpenOffice.org unterstützt das System bis Version 3.3, LibreOffice bis Version 3.6.7 (vom 10. Juli 2013). Die letzten Firefox-Versionen sind 12.0 und 10.0.12esr. Das jüngste unterstützte Internet Explorer war 6, .NET wird bis zur Version 2.0 unterstützt.

In vielen Fällen lässt sich Hardware, die noch bis 2010 verkauft wurde, problemlos nutzen. Gerätetreiber für Windows 2000 sind oft identisch mit denen für Windows XP. Bei vielen mit Windows-XP-Treiber verkauften Multimedia-Komponenten wie TV-Karten, Kameras und Scanner gibt es hingegen meist keinen kompatiblen Treiber. DirectX wird bis Version 9.0c unterstützt. Einige Komponenten (z. B. ASPI-Treiber) wurden nicht durch Service Packs nachgerüstet und müssen von Fremdanbietern den Treibern beigefügt werden.





</doc>
<doc id="5581" url="https://de.wikipedia.org/wiki?curid=5581" title="Microsoft Windows XP">
Microsoft Windows XP

Windows XP ("„eXPerience“," für "Erlebnis," "Erfahrung") ist ein Betriebssystem von Microsoft. Es basiert auf dem
Windows-NT-Kernel und ist der technische Nachfolger von Windows 2000 und der Vorgänger von Windows Vista. Das interne Versionskürzel lautet Windows NT 5.1 und der interne Codename in der Entwicklungsphase war "Whistler." Windows XP kam am 25. Oktober 2001 auf den Markt. Es löste Windows ME der MS-DOS-Linie in der Version „Home Edition“ als Produkt für Heimanwender und Privatnutzer ab.

Ursprünglich plante Microsoft noch, Windows 2000 in zwei Richtungen weiterzuentwickeln: zum einen "Neptune," welches als Nachfolger von "Millennium" hauptsächlich Endverbraucher ansprechen sollte und für welches die Funktionen vorgesehen waren, die architekturbedingt nicht in Millennium implementiert werden konnten, und zum anderen "Odyssey," welches für Firmenkunden bestimmt war. Eine Vorversion von Neptune erreichte am 27. Dezember 1999 die Betatester, doch schlussendlich gab Microsoft die Pläne auf.

Am 21. Januar 2000 erreichte die Presse die Meldung, dass die Projekte Neptune und Odyssey zusammengelegt würden. Das so entstandene Projekt erhielt den neuen Codenamen "Whistler." Auf der WinHEC im April 2000 stellte Microsoft das neue Betriebssystem erstmals vor und kündigte bereits einige neue Funktionen für das Betriebssystem an, etwa die Möglichkeit, ohne Abmeldung zwischen verschiedenen Benutzerkonten zu wechseln. Whistler sollte außerdem modular sein, sodass es auch auf kleinen Mobilgeräten lauffähig sein sollte.

Auf der Professional Developers Conference im Juli 2000 kündigte Microsoft die Veröffentlichung von Whistler für die zweite Jahreshälfte von 2001 an. Am 13. Juli erschien die erste Vorversion von Whistler, die bereits die neue Design-Funktion der Benutzeroberfläche demonstrierte. Die hier verwendete und zunächst für das Endprodukt vorgesehene Benutzeroberfläche, zunächst "Professional" und später "Watercolor" genannt, gab Microsoft erst im Februar 2001 zugunsten von "Luna" auf. Am 24. August folgte eine zweite Vorversion, die das neue Startmenü demonstrierte und erstmals die Windows-Firewall enthielt. Während sich der erste Betatest, der ursprünglich für September vorgesehen war, immer weiter nach hinten verschob, kam am 3. Oktober die nächste Vorversion heraus, die jedoch lediglich kleinere Verbesserungen enthielt. Erst die zwei Wochen später folgende Vorversion zeigte eine neu gestaltete Installationsroutine sowie ein neues Hilfesystem.

Am 31. Oktober 2000 startete schließlich der Betatest von Whistler. Auf der darauffolgenden COMDEX stellte Microsoft den Tablet PC vor, der mit einer speziell angepassten Version von Whistler erscheinen sollte. Am 18. Dezember kündigte Microsoft mit "Whistler Embedded" das neue Betriebssystem der Embedded-Reihe an, das Windows NT 4.0 Embedded ablösen sollte.

Die am 4. Januar 2001 ausgelieferte Vorversion beinhaltete erstmals die Produktaktivierung. Mit einem Beispiel-Design demonstrierte Microsoft das Wechseln des Designs im laufenden Betrieb, das Unternehmen hielt aber an der Meinung fest, dass es für Endverbraucher keine Möglichkeit geben werde, ein eigenes Design zu kreieren. Am 16. Januar folgte bereits die nächste Vorversion und am 23. Januar schließlich eine weitere Vorversion.

Am 13. Februar stellte Microsoft erstmals die neue Benutzeroberfläche "Luna" vor, gleichzeitig gab es den endgültigen Namen für das Produkt bekannt, "Windows XP." Die zeitgleich veröffentlichte Vorversion enthielt bereits diese Neuerungen. Nach heftiger Kritik an der neuen Benutzeroberfläche besserte Microsoft vereinzelt nach und reduzierte unter anderem die Größe der Symbole in der Schaltflächenleiste. Nach zahlreichen Verzögerungen und mehreren Vorversionen startete der zweite Betatest schließlich am 26. März mit einer offiziellen Ankündigung auf der WinHEC. Große Kritik erhielt Microsoft für die Entscheidung, Unterstützung für USB 2.0 nicht mit Windows XP auszuliefern, an der das Unternehmen dennoch festhielt. Die Entwicklung der Server-Version nahm ihren eigenen Verlauf und am 30. April nannte Microsoft die Server-Version provisorisch "Windows 2002."

Am 5. Mai 2001 erschien eine weitere Vorversion. Neu war eine Sicherheitsfunktion, die bei Benutzerkonten mit einem leeren Passwort lediglich die lokale Anmeldung ermöglicht und unter anderem eine Anmeldung über das Netzwerk in solchen Fällen sperrt. Am 24. Mai erschien die letzte Version, die neue Funktionen implementierte, die nachfolgenden Versionen konzentrierten sich auf die Behebung von Programmfehlern. Pläne, Windows XP mit AOL zu bündeln, zerschlugen sich im Juni, stattdessen kündigte Microsoft den Windows Messenger offiziell an. Ebenso plante Microsoft, dass die Home Edition nur einen Monitor unterstützten sollte, auch das änderte sich.

Mit der am 21. Juni veröffentlichte Vorversion band Microsoft das Betriebssystem stärker an das Microsoft-Passport-System an. Am 2. Juli 2001 eröffnete Microsoft die Release-Candidate-Phase. Der endgültige Veröffentlichungstermin wurde auf den 25. Oktober festgesetzt. Aufgrund eines Urteils eines US-Gerichts musste Microsoft entgegen ursprünglicher Pläne Erstausrüstern () erlauben, eigene Icons auf dem Desktop zu platzieren; ebenso können diese den Internet Explorer und Outlook Express aus dem Startmenü entfernen und der Internet Explorer konnte in der Systemsteuerung deinstalliert werden, wenngleich dies lediglich verschiedene Icons entfernte, den IE-Kern allerdings im Betriebssystem beließ. Ebenso fuhr Microsoft die Beschränkungen bei der Produktaktivierung drastisch zurück.

Am 27. Juli erschien der zweite Release Candidate. Durch die neueste Gerichtsentscheidung versuchte AOL nun, Erstausrüster dazu zu bringen, Windows XP trotz der zuvor gescheiterten Verhandlungen ausschließlich mit AOL auszuliefern. Microsoft konterte mit der Pflicht, einen Link zum Microsoft-Network-Dienst auf dem Desktop zu platzieren, falls Erstausrüster eigene Icons dort platzieren wollen. Am 24. August 2001 erreichte Windows XP schließlich den RTM-Status, und am 25. Oktober 2001 erschien das Betriebssystem wie geplant im Handel.

Mit Windows XP wollte Microsoft die Benutzerfreundlichkeit des Betriebssystems erhöhen. Im Gegensatz zu älteren Heimanwender-Betriebssystemen von Microsoft basiert Windows XP auf einem Windows-NT-Kernel. Dieser Wechsel sollte für eine verbesserte Stabilität sorgen. Zudem wurde Augenmerk auf die Verbesserung der Sicherheit gelegt.
Die mit einem gekennzeichneten Elemente waren bereits unter Windows ME verfügbar, nicht jedoch unter Windows 2000.

Die für Benutzer auffälligste Neuheit in Windows XP ist die Benutzeroberfläche „Luna“, die im Auslieferungszustand eine buntere und verspieltere Desktop-Oberfläche bietet als bei älteren Windowsversionen; wahlweise steht auch eine leicht modifizierte Version der Oberfläche aus Windows 2000 zur Verfügung („klassische Darstellung“). Die „Luna“-Oberfläche enthält auch den voreingestellten Bildschirmhintergrund „Grüne Idylle“, ein Bild einer grünen Wiese unter blauem Himmel. Sie weckte während und auch noch nach der Vertriebsperiode des Betriebssystems Assoziationen mit dem Szenenbild aus der Kinderfernsehserie Teletubbies.

Das Startmenü wurde erweitert: So ist es in Windows XP in zwei statt bisher einer Spalte angeordnet. Während in der linken Spalte die zuletzt benutzten Programme angezeigt werden, bietet es rechts zusätzliche Einträge, etwa zum „Arbeitsplatz“ oder zu Benutzerordnern wie dem Ordner „Eigene Dateien“ oder „Eigene Musik“. Neu sind dort auch ein Link zu den eingerichteten „Netzwerkverbindungen“ sowie eine Schaltfläche zum Einstellen von „Programmzugriffen und -standards“.

Im Windows-Explorer wurden Funktionen zur Unterstützung von digitaler Fotografie eingebaut. So wird nun z. B. die "Windows Bild- und Faxanzeige" mitgeliefert, mit der gängige Bildformate geöffnet und rudimentär bearbeitet werden können. Auch ist die Bildanzeige als Bildschirmpräsentation ohne Zusatzsoftware möglich. Auch Musikdateien werden besser unterstützt: Die sogenannten ID3-Tags, die Informationen wie z. B. Interpret, Titel usw. in der Musikdatei speichern, werden im Explorer angezeigt und können über das Eigenschaftenmenü direkt bearbeitet werden. Der Explorer beinhaltet nun auch eine einfache Funktion zum Brennen von CDs. Software von Drittanbietern ist insoweit nicht mehr notwendig. Auch kann der Explorer ZIP-komprimierte Dateien erstellen und verwalten.

Die Systemwiederherstellung ist eine Funktion, welche es dem Benutzer mit Hilfe sogenannter Wiederherstellungspunkte ermöglicht, das System in Hinsicht auf System- und Konfigurationsdateien in einen früheren Zustand zurückzuführen. Dies soll vor allem bei fehlgeschlagenen Treiber- oder Software-Installationen weiterhelfen. Diese erstmals mit Windows ME eingeführte Technik wurde verbessert und mit Windows XP in die Windows-NT-Linie übernommen.

Windows XP enthält auch Kompatibilitätsoptionen für Anwendungen, die für ältere Windowsversionen geschrieben wurden. Diese Funktion wurde zwar bereits mit dem "Service Pack 2" unter Windows 2000 eingeführt, muss dort nach der Service-Pack-Installation aber erst im System registriert werden und steht nur Administratoren zur Verfügung. Unter Windows XP steht sie standardmäßig zur Verfügung und kann für jede Anwendung einzeln festgelegt werden.

Während die meisten Windowsversionen bisher lediglich das Laufwerksdateisystem FAT verwenden konnten, kommen nun für alle Anwender die bisher nur unter Windows NT/2000 bekannten Funktionen des NTFS-Dateisystems hinzu. Das sind beispielsweise Dateigrößen über 4 GB, Metadaten-Journaling, Datenträgerkontingente oder eine zuverlässigere und einfachere Datenträgerkomprimierung als DriveSpace (Windows 9x) bzw. "Doublespace" (DOS). Einige NTFS-Funktionen sind in der Homevariante allerdings nicht nutzbar, so etwa die Verschlüsselung und standardmäßig (d. h. ohne Fremdsoftware im laufenden Betrieb) die Vergabe von Dateizugriffsberechtigungen.

Um Software-Piraterie einzudämmen, verwendet Microsoft bei Windows XP erstmals das System der Produktaktivierung. Bei diesem Verfahren tauscht das Betriebssystem im Zuge der Installation bestimmte Daten mit Microsoft aus, bevor eine dauerhafte Verwendung gestattet wird. Die ausgetauschten Daten enthalten vor allem Informationen über die verwendete Hardware. Falls sich diese Daten ändern, zum Beispiel durch Austausch oder Erweiterung von Hardware-Komponenten, kann das Betriebssystem in einigen Fällen eine erneute Aktivierung verlangen.

Die übertragenen Daten enthalten nach Angaben Microsofts einen Hash-Wert der folgenden Merkmale in verschlüsselter Form:

Mittlerweile wurde bekannt, dass bei einer Aktivierung bestimmter Notebooks, deren Hardware nicht ohne weiteres geändert werden kann, nicht alle obengenannten Daten ausgetauscht werden. Für Kunden, Partner und Entwickler mit großem Installationsaufwand, welche oft automatisierte Installationsroutinen verwenden, gibt es für die erworbene "Corporate Edition" oder MSDN-Version einen firmenweiten Lizenzschlüssel, der keine weitere Produktaktivierung erforderlich macht. Wurde eine weite unerlaubte Verbreitung entdeckt, wurden diese Schlüssel im Zuge der Produktupdates gesperrt oder Online-Produktupdates verweigert.

Für die Umgehung der Aktivierung gibt und gab es sogenannte Cracks, die geläufigsten darunter waren:

Microsoft versucht außerdem seit geraumer Zeit, durch Studien zu belegen, wie transparent sich die Produktaktivierung verhält und wie sie funktioniert. Der deutsche TÜViT hat die Anonymität des Aktivierungsverfahrens „bestätigt“, wobei TÜViT gerade an entscheidender Stelle nicht selbst nachprüfte, sondern den Angaben seines Auftraggebers Glauben geschenkt hat.

Ungültige Seriennummern werden beim Windows Update durch ein ActiveX-Programm namens Windows Genuine Advantage (WGA) zurückgewiesen. Da in alternativen Browsern kein ActiveX unterstützt wird, musste dazu in der Vergangenheit die ausführbare Datei "GenuineCheck.exe" heruntergeladen werden. Sie generierte eine Nummer, die im Download Center und bei Windows Updates eingegeben werden musste. Diese Nummer wurde aus der Seriennummer und einem Code, der in den Systemeigenschaften einzusehen ist, errechnet. Diese Methode wurde von Softwarepiraten schnell geknackt, indem der windowseigene Kompatibilitätsmodus genutzt wurde. Dieses Verfahren wurde durch die ausführbare Datei "legitcheck.hta" ersetzt, die manuell heruntergeladen und ausgeführt werden muss. Mit ihr entfiel die manuelle Eingabe einer Nummer.

Die Windows-Firewall wurde neu eingeführt. Sie dient dem Schutz gegen Internetangriffe und wurde mit dem Service Pack 2 stark erweitert. Die Funktion „Schneller Benutzerwechsel“ erlaubt es nun, dass mehrere Benutzer gleichzeitig angemeldet sind. Zwischen diesen kann dann besonders schnell gewechselt werden. Windows XP ermöglicht mit der Remoteunterstützung die Fernwartung über Terminal Services (Remote Desktop Protocol). Die Möglichkeiten, das System per Kommandozeile zu verwalten, wurden vereinheitlicht und erweitert. Zudem wurde die Kantenglättung für Schriften (ClearType) eingeführt.

Die CD-ROM von Windows XP ist bootfähig, im Gegensatz zu Windows 2000 oder Windows 98 liegen keine Startdisketten bei. Sollte das System keine Möglichkeit bieten, von einer CD zu starten, können Abbilder eines Diskettensatzes, bestehend aus sechs Disketten, aus dem Internet heruntergeladen werden, um die Installation des Betriebssystems zu ermöglichen.

Für Windows XP wurde das Windows-2000-System für Intel-Prozessoren als Grundlage übernommen. Es sollte aber darüber hinaus auch die alten MS-DOS-basierten Windows-Versionen ersetzen. Daher mussten Möglichkeiten geschaffen werden, weitere ältere, nicht unter Windows NT lauffähige Programme auszuführen.
Eine weitere Ergänzung ist ein Kompatibilitätsmodus genanntes "Personality," der bei Bedarf Routinen aus älteren Systemen emuliert. Damit soll das Ausführen von Programmen ermöglicht werden, die an Vorgängerversionen angepasst wurden.

Andere Programme nehmen den vollen Speicherschutz von Windows in Anspruch. Das System ist daher vergleichsweise zuverlässig und, insbesondere im Vergleich mit Windows 98, stabil.

Windows XP formatiert Partitionen, wie schon Windows 2000, standardmäßig mit dem Dateisystem NTFS. Für große Festplatten ist es möglich, die Verwaltung im Modus mit 48-bit-LBA zu aktivieren. Es ist auch in der Lage, mit FAT-Partitionen umzugehen.

Die "Home Edition" ist primär für den privaten Einsatz zu Hause gedacht. Da mit Windows XP die Weiterentwicklung der DOS-basierenden „9x“-Systeme eingestellt wurde, tritt die Home Edition an ihre Stelle und wurde trotz der abweichenden technischen Basis als Nachfolger dieser Systeme beworben. Als solche fehlen ihr zahlreiche Funktionen, die nur in einer Firmenumgebung relevant sind. Im Gegenzug wurde z. B. die Verwendung des Kompatibilitätsmodus für Endanwender vereinfacht, der unter Windows 2000 (ab SP2) eingeführt wurde, aber Administratorrechte erforderte und standardmäßig deaktiviert war.

Die Home Edition kann keiner Domäne beitreten, ebenso fehlt der Remote-Desktop, die Gruppenrichtlinienverwaltung sowie die Möglichkeit, Zugriffsrechte über den Windows-Explorer zu setzen. Ebenso fehlt der Home Edition das verschlüsselte Dateisystem EFS. Das Programm NTBackup fehlt standardmäßig in der Home Edition, kann aber von der CD nachinstalliert werden. Außerdem sind der Internet Information Server und zahlreiche Administrationsprogramme in der Home Edition nicht verfügbar. Windows XP Home Edition unterstützt zudem nur einen einzigen Prozessor.

Die "Professional Edition" tritt die direkte Nachfolge von Windows 2000 Professional an. Diese Version kann im Gegensatz zur Home Edition nicht nur von Windows 98 und Me, sondern auch von Windows NT 4.0 und Windows 2000 aktualisiert werden. Windows XP Professional unterstützt bis zu zwei Prozessoren. Die meisten anderen Versionen von Windows XP basieren auf der Professional Edition.

Im Zuge eines Verfahrens mit der Europäischen Kommission im März 2004, infolgedessen Microsoft zur Zahlung von 497 Millionen Euro verurteilt wurde, musste das Unternehmen außerdem eine Version von Windows XP ohne den Windows Media Player in den Handel bringen. Nach zähen Verhandlungen einigten sich Microsoft und die EU-Kommission auf den Namen "Windows XP N." Da Microsoft den Preis für Windows XP N genauso hoch setzte wie für das normale Produkt, verzichteten die meisten Erstausrüster darauf, das Produkt in den Handel zu bringen, sodass es kaum verbreitet war.

Nach einem Untersuchungsverfahren der südkoreanischen Kartellbehörde musste Microsoft die normalen Versionen von Windows XP vom Markt nehmen und zwei neue Versionen von Windows XP für den südkoreanischen Markt veröffentlichen: zum einen "Windows XP K," welches zusätzlich Links zu Medienspielern und Instant Messengern von Drittanbietern enthält, und "Windows XP KN," welches sowohl den Windows Media Player als auch den Windows Messenger nicht enthält.

Die "Media Center Edition" basiert ebenfalls auf der „Professional Edition“ und enthält spezifische Erweiterungen für auf multimediale Inhalte sowie deren Wiedergabe spezialisierte Computer, die in der Regel mit einer TV-Karte ausgestattet sind. Ein Merkmal ist die Möglichkeit der vereinfachten Bedienung durch die Darstellung auf einem normalen Fernsehapparat und die Steuerung mit einer Fernbedienung. Microsoft versuchte damit erstmals, die Lücke zwischen einem reinen Computer und einer Medienzentrale für das Wohnzimmer zu schließen. Windows XP Media Center Edition erfuhr 2003 die erste Aktualisierung, die letzte XP-Version ist die Media Center Edition 2005. Während die erste Version der Windows XP Media Center Edition nur im Paket mit entsprechenden Computern vertrieben und nicht als Einzelprodukt verfügbar war, sind die aktualisierten Fassungen auch einzeln über den Vertriebskanal "System Builder" zu erwerben. Seit der letzten Version können Endbenutzergeräte wie z. B. DVD-Recorder, die Xbox 360 von Microsoft und weitere über eine Netzwerkverbindung mit dem Betriebssystem kommunizieren. Dafür ist in diesen Endgeräten ein Windows XP Media Center Edition als „embedded Version“ oder ein zur Media Center Edition kompatible Benutzerschnittstelle implementiert.

Am 9. November 2002 erschien die Windows XP Tablet PC Edition. Damit erhoffte sich das Unternehmen, den seit Jahren produzierten, aber kaum erfolgreichen Tablet-PCs zum Durchbruch zu verhelfen. Dazu veröffentlichte Microsoft die Microsoft-Tablet-PC-Spezifikation, die bestimmte Kriterien an Tablet-PCs stellte, die mit dem neuen Betriebssystem ausgeliefert werden sollten. Das Betriebssystem selbst basiert auf Windows XP Professional mit integriertem Service Pack 1, enthält aber zusätzlich Funktionen zur Handschrifterkennung. Zusätzlich veröffentlichte Microsoft ein Add-On zur Integration von Office XP sowie zahlreiche Programmierschnittstellen, mit denen Entwickler die Stiftfunktionen in ihren eigenen Programmen nutzen konnten. Die Windows XP Tablet PC Edition war nicht im Handel erhältlich, sondern wurde nur mit passender Hardware verkauft; lediglich MSDN-Abonnenten und Volumenlizenzkunden konnten die Tablet PC Edition auch ohne Tablet erhalten.

Zusammen mit dem Service Pack 2 veröffentlichte Microsoft eine aktualisierte Version unter der Bezeichnung "Windows XP Tablet PC Edition 2005." Besitzer der älteren Tablet PC Edition konnten kostenlos auf die neue Version aktualisieren, außerdem lag sie neuen Tablet-PCs bei. Die Tablet PC Edition 2005 bot hauptsächlich eine verbesserte Handschrifterkennung sowie eine Integration mit Office 2003. Die neue Version unterstützte das .NET Framework, sodass auch Managed Code für die Tablet PC Edition geschrieben werden konnte. Die Tablet PC Edition 2005 wurde schnell durch ein Speicherleck in den Stiftfunktionen bekannt, das dazu führen konnte, dass das Betriebssystem wegen fehlendem freien Arbeitsspeicher unbenutzbar wurde.

Das Betriebssystem und die zugehörigen Tablet-PCs konnten sich jedoch kaum auf dem Markt durchsetzen. Vor allem aufgrund des hohen Preises und der schlechten Vermarktung war die Verbreitung auf wenige Nischen beschränkt. Bis 2005 konnten weltweit lediglich 650.000 Tablet-PCs verkauft werden.

Windows XP "Embedded" wird primär im industriellen Umfeld, aber auch in medizinischen Geräten, Geldautomaten oder für Kassenterminals eingesetzt, seltener in Haushalts- und Unterhaltungselektronik oder in Voice-over-IP-Komponenten. Diese Version basiert ebenfalls auf der Professional Edition.

Die "Windows XP 64-Bit Edition" war eine spezielle Version von Windows XP für den Intel-Itanium-Prozessor. Sie erschien zeitgleich mit den 32-Bit-Versionen von Windows XP; bereits die ersten ausgelieferten Itanium-Prozessoren waren mit einer Vorversion von Windows XP ausgestattet, die von Microsoft offiziell unterstützt wurde. Der 64-Bit-Edition fehlten zahlreiche Funktionen des 32-Bit-Pendants, darunter der Windows Media Player, NetMeeting, sowie Unterstützung für alte DOS- und 16-Bit-Anwendungen, ansonsten war sie jedoch ein vollwertiges Betriebssystem, das bis zu 16 GB Arbeitsspeicher verwalten konnte.

Im März 2003 folgte zusammen mit der Veröffentlichung von Windows Server 2003 die neue "Windows XP 64-Bit Edition Version 2003" für die neuen Itanium-2-Prozessoren. Nachdem als letzter Hersteller Hewlett-Packard im September 2004 die Auslieferung von Workstations mit Itanium-Prozessoren einstellte, beendete Microsoft im Januar 2005 die Unterstützung der Windows XP 64-Bit Edition. Insgesamt waren die Verkaufszahlen enttäuschend und die Windows XP 64-Bit Edition hatte praktisch keinerlei Verbreitung.

Die Windows XP „x64 Edition“ (Codename „Anvil“) ist eine Version, die ausschließlich für Prozessoren mit AMD64- oder Intel-64-Erweiterung entwickelt wurde. Sie läuft nicht auf 64-Bit-Prozessoren anderer Hersteller und ähnelt Windows XP Professional zwar, basiert aber auf dem Kernel von Windows Server 2003 und besitzt somit eine modernere Basis (NT 5.2). Die x64-Edition erschien am 25. April 2005. Im Zusammenspiel zwischen Prozessor und Betriebssystem kann auch eine konventionelle 32-Bit-Software – über den "Windows on Windows 64-x86-Emulator" (WOW64) – ausgeführt werden. Somit müssen auszuführende Programme nicht als 64-Bit-Versionen vorliegen. Dieses Verfahren der x64-Prozessoren wird "Mixed-Mode" genannt – dem gleichzeitigen Ausführen von 64- und 32-Bit-Software auf einem Prozessor. Für die einwandfreie Funktion der Hardware werden 64-Bit-Gerätetreiber vorausgesetzt. Die Treiber werden in der Regel vom Hardware-Hersteller für das Betriebssystem her- und bereitgestellt. Besonders zu beachten ist, dass sämtliche Programme auf 16-Bit-Basis unter Windows XP x64 nicht funktionieren, da der Betriebsmodus „Long Mode“ der x64-Architektur dies nicht mehr unterstützt. Dies betrifft alle Programme, die – teilweise oder ausschließlich – für MS-DOS oder Windows 3.x entwickelt worden sind.

Die Vorteile dieser 64-Bit Version gegenüber XP mit 32-Bit-Architektur sind:

Das letzte "Service Pack" für Windows XP Professional x64 ist das Service Pack 2 vom 12. März 2007, die "Service Packs" der Windows-XP-32-Bit-Editionen sind nicht mit der x64-Variante kompatibel.

Am 11. August 2004 kündigte Microsoft die "Starter Edition" von Windows XP an. Diese Version sollte in Zusammenarbeit mit Erstausrüstern die Verbreitung von PCs mit Windows in Schwellenländern fördern. Zunächst startete das Projekt in Thailand, Malaysia, und Indonesien, später kamen noch weitere Länder wie Indien und Mexiko dazu.

Mit Preisen um die 30 US-Dollar war die Starter Edition weit günstiger als andere Versionen von Windows XP, dafür hatte sie einige einschneidende Einschränkungen. So lief das System ausschließlich auf den Prozessoren Intel Celeron, AMD Duron und AMD Sempron und verweigerte den Dienst auf anderen Prozessoren. Außerdem unterstützte diese Edition lediglich eine Bildschirmauflösung von 800×600 und enthielt weder Netzwerkfunktionen noch Unterstützung für mehrere Benutzerkonten. Es konnten zudem lediglich drei Programme gleichzeitig ausgeführt werden.

Der Verkauf der Starter Edition kam in den Zielmärkten nur schleppend voran. Vor allem durch die in diesen Ländern weit verbreitete Produktpiraterie waren vollwertige Versionen von Windows XP für weniger Geld erhältlich, zumal die Starter Edition nicht im Handel erhältlich war und nur zusammen mit einem entsprechenden PC erworben werden konnte. Außerdem sah Microsoft keine Möglichkeit vor, die Starter Edition auf die Home oder Professional Edition zu aktualisieren.

Windows "Fundamentals for Legacy PCs" (Windows FLP) ist ein Betriebssystem/Thin Client, das basierend auf Windows XP Embedded für ältere und weniger leistungsstarke PC optimiert wurde. Die Codenamen waren „Eiger“ und „Mönch“. Microsoft wollte mit dieser Version grundlegende Dienste auf älteren Computern zur Verfügung stellen. Es wurden viele Kerneldienste des Service Packs 2 für Windows XP übernommen, beispielsweise die Windows-Firewall, Gruppenrichtlinienverwaltung, automatische Aktualisierungen und andere Verwaltungssysteme. Es wurde speziell für Büroanwendungen und für die Fernverbindung ("Remote Desktop") optimiert. Windows FLP kann leicht zu einer Diskless-Arbeitsstelle umgebaut werden. Diese Version wird ausschließlich an Kunden mit „Microsoft Software Assurance“ (Volumenlizenz) abgegeben.

Anders als in den vorherigen Windows-NT-Versionen gibt es keine Server-Variante von Windows XP. Die Serverprodukte zu Windows XP sind in der separaten "Windows-Server-2003"-Produktfamilie zusammengefasst. Die einzelnen Server-Versionen gliedern sich dabei in "Standard Edition," "Enterprise Edition," "Datacenter Edition," "Web Edition" und "Small Business Server," wobei die Datacenter Edition ausschließlich als OEM-Lizenz (Lizenz für Kunden von Erstausrüstern) in Verbindung mit entsprechender Hardware erhältlich ist.

Microsoft bietet mit der Herausgabe seiner Produkte wie Windows XP befristeten Support an. Bei der Befristung wird nach dem Anwender (z. B. Privatanwender) und nach Phasen unterschieden. Microsoft unterscheidet zwei Phasen:

Den bis zu zwei- oder fünfjährigen "Mainstream Support" und den darauffolgenden, bis zu fünfjährigen "Extended Support." Aktualisierungen wurden nach bestimmten Zeitabständen zusammengefasst und als Service Pack bereitgestellt. Diese Service Packs enthielten teilweise auch neue Funktionen, wie beispielsweise die verbesserte Firewall im Service Pack 2. Microsoft stellte für Windows XP bisher drei Service Packs zur Verfügung. Mit dem 14. April 2009 ging die Produktunterstützung von Windows XP vom Mainstream- in den Extended Support über. Dieser Supportzyklus beinhaltete Aktualisierungen, die bis zum 8. April 2014 erfolgten. In dieser Phase wurden keine neuen Funktionen mehr geliefert, sondern ausschließlich Sicherheitslücken behoben. Bei der Aktualisierung konnte zwischen automatischer und manueller Installation unterschieden werden. Für Geschäftskunden wurden auch nach April 2009 kostenpflichtige Serviceverträge (Support für Fehlerbehebungen) angeboten. Die Aktualisierung der Signaturen für Schadprogramme erfolgte noch bis Juli 2015, wodurch die Rechner noch gegen bestimmte Angriffe geschützt werden konnten, allerdings keine vollständige Sicherheit hergestellt werden konnte.

Wie bei Computersystemen üblich werden häufig Sicherheitslücken und Fehler entdeckt, die oft schon kurz nach dem Bekanntwerden von Angreifern direkt (z. B. Cracker) oder indirekt (z. B. Virenprogrammierer) ausgenutzt werden, um anfällige Systeme für eigene Zwecke zu missbrauchen, anderweitig zu manipulieren oder außer Funktion zu setzen.

Um Benutzern das Installieren entsprechender Sicherheitsaktualisierungen zu erleichtern, stellt der Hersteller seit Windows 98 eine Funktion zur automatischen Aktualisierung per Internet zur Verfügung. Das erweitert die bisherige Strategie der Verbreitung von Service Packs und Hotfixes durch manuelles Herunterladen. Der automatische Prozess erleichtert die Verteilung entsprechender Aktualisierungen und erhöht so Verbreitungsgeschwindigkeit und -grad von Updates. Er kann in vier Stufen angepasst werden (Bestätigung des Downloads, Bestätigung der Installation, vollautomatisch oder deaktiviert).

Die Updates können aber auch zwangsweise eingespielt und aktiviert werden, denn die konfigurierbaren Update-Stufen gelten nicht für den Update-Dienst selbst, was viele Benutzer überrascht. Das Gleiche gilt nach den Lizenzbedingungen für die integrierte Digitale Rechteverwaltung.

Das "Service Pack 1" für Windows XP, das vor allem alle bis dahin veröffentlichten Sicherheitspatches in einem einzelnen Paket vereinte, wurde am 9. September 2002 veröffentlicht. Hardwareseitig kamen der standardmäßige Support von Festplattengrößen jenseits von 137 GB sowie die uneingeschränkte Nutzung der USB-2.0-Schnittstelle hinzu.

Microsofts Unterstützung für Windows XP mit SP1 oder SP1a lief zum 10. Oktober 2006 aus. Seit diesem Datum liefert Microsoft für Windows XP mit Service Pack 1 keinerlei Sicherheitsaktualisierungen mehr aus.

Das "Service Pack 2" wurde am 9. August 2004 öffentlich verfügbar gemacht und zielte vor allem auf eine verbesserte Systemsicherheit ab. Ursprünglich sollte das Service Pack 2 schon im Juni 2004 von Microsoft herausgegeben werden, es stellte sich aber heraus, dass noch einige Fehler zu beheben waren, was die Veröffentlichung um zwei Monate verzögerte. Zum ersten Mal fügte Microsoft mit einem Service Pack Windows XP neue Funktionen hinzu, wie etwa eine überarbeitete Windows-Firewall, die Unterstützung für die Datenausführungsverhinderung, mehr Software zur Unterstützung drahtloser Netze und einen Pop-up-Blocker für den Internet Explorer 6.0, der nach dem Aufspielen des Service Packs ebenfalls aktualisiert wird und in der Version 6.0 SP2 vorliegt. Durch das neu hinzugekommene Sicherheitscenter können eine Vielzahl von Personal Firewalls und Antivirenprogrammen überwacht, indem Hersteller dieser Programme die mit diesem Service Pack neu eingeführten APIs benützten, und die Funktion „automatische Updates“ eingerichtet werden. Microsoft unterstützte das Service Pack 2 bis zum 13. Juli 2010.

Das "Service Pack 3," das zugleich das letzte für Windows XP ist, sollte am 29. April 2008 veröffentlicht werden. Es gab jedoch ein Kompatibilitätsproblem mit Microsofts Dynamics Retail Management System (RMS), sodass es erst am 6. Mai über das Microsoft Download Center und Windows Update verfügbar wurde. Die Nutzer von Microsofts kostenpflichtigem MSDN sowie Nutzer mit Volumenlizenzverträgen hatten schon vorab die Möglichkeit, sich das Service Pack 3 herunterzuladen.

Bei der 313 MB umfassenden Aktualisierungsdatei handelt es sich um eine Sammlung aller Software-Aktualisierungen und Fehlerbereinigungen, die seit dem Erscheinen von Windows XP veröffentlicht wurden. (Bei Download über Windows Update hat das Service Pack eine geringere Größe, da ausschließlich für die laufende Windows-Version benötigte Dateien heruntergeladen werden müssen.) Aus Support-Gründen lässt sich das SP3 nur installieren, wenn mindestens das Service Pack 1 bereits zuvor installiert wurde; die Slipstream-Integration in eine Installationsquelle ist dagegen in jedem Fall möglich. Zusätzlich zu den Aktualisierungen beinhaltet das Service Pack 3 auch einige weitere aktualisierte Programme, wie den Background Intelligent Transfer Service (BITS) 2.5, Windows Installer 3.1, Management-Console (MMC) 3.0 und die Core XML Services 6.0. Programmaktualisierungen des Internet Explorers 7 und Media Players sind nicht enthalten. Das Update erlaubt die Verwendung von Windows XP als Gastsystem in Microsofts Virtualisierungssystem Hyper-V. Ebenfalls enthalten ist eine Clientkomponente für das von Windows Server 2008 bereitgestellte NAP-System.
Weiterhin wird nun die Erkennung von „Black-Hole“-Routern unterstützt. Das Sicherheits-Center wartet zusätzlich mit besseren Beschreibungen auf und es wurde ein Windows-Kryptographie-Modul (FIPS) implementiert, das im Kernel-Modus läuft.
Nach der Installation des Service Pack 3 verschwindet die Möglichkeit, die Adress-Symbolleiste in die Taskbar einzubinden. Microsoft sah sich nach eigenen Angaben zu diesem Schritt gezwungen, da regulierende Behörden das gefordert hätten. Microsoft empfiehlt, auf die Windows Desktop Search umzusteigen.

Nachdem Microsoft den Extended-Support-Zeitraum für Windows XP im Jahr 2007 bis zum April 2014 verlängert hatte, endete er nach 13 Jahren am 8. April 2014 endgültig mit Ausnahme der Embedded-Versionen, bei denen der Extended-Support am 12. Januar 2016 endete. Microsoft weist darauf hin, dass es nach diesem Termin keinerlei Sicherheitsaktualisierungen und technischen Support mehr gibt. Für Großkunden mit einem gesonderten, kostenpflichtigen Supportvertrag wird Microsoft jedoch auch über dieses Datum hinaus für eine begrenzte Zeit Aktualisierungen zur Verfügung stellen. Da ein Jahr vor dem Supportende laut Netapplication der Marktanteil von Windows XP noch immer über 38 % lag, hat Microsoft die Get2Modern-Kampagne ins Leben gerufen, die kleine und mittlere Unternehmen dabei unterstützen soll, auf Windows 7 oder Windows 8 umzusteigen.

Mitte Januar 2014 gab der Konzern bekannt, dass die Microsoft Security Essentials, die System Center Endpoint Protection, sowie Forefront Client Security, Forefront Endpoint Protection und Windows Intune auch nach dem XP-Supportende am 8. April 2014 mit Updates versorgt werden. Dieser Teil-Support wurde bis zum 14. Juli 2015 aufrechterhalten.

Am 2. Mai 2014 veröffentlichte Microsoft trotz ausgelaufenem Support-Lifecyle ein weiteres Sicherheitsupdate für Windows XP. Microsoft begründete dies mit der zeitlichen Nähe zum Supportende.

Nach dem Supportende kursierte im Internet eine Beschreibung für eine Modifikation an der Registrierungsdatenbank, durch die man über das Supportende hinaus Updates für Windows XP via Windows Update erhalten könne. Durch diese Änderung identifiziert sich das System als Windows Embedded POSReady 2009, ein auf Windows XP basierendes Kassenbetriebssystem, dessen Support-Lifecycle erst am 9. April 2019 endet. Von Seiten der Fachpresse wird jedoch von dieser Modifikation abgeraten, da diese Updates nicht für Windows XP entwickelt und getestet wurden.

Aufgrund eines schwerwiegenden Cyber-Angriffs auf ungepatchte Windows-Systeme im Mai 2017 mit dem Schadprogramm WannaCry, welches eine Sicherheitslücke in der Implementierung des SMB-Prokolls zur wurmartigen Verbreitung ausnutzte, veröffentlichte Microsoft am 12. Mai 2017 ein weiteres, außerplanmäßiges Sicherheitsupdate für Windows XP unter der Bezeichnung KB4012598. Ein weiteres außerplanmäßiges Update wurde von Microsoft im Juni 2017 unter der Bezeichnung KB4012583 veröffentlicht.

Einige Nutzergruppen stellen die öffentlich verfügbaren Systemaktualisierungen (z. B. Sicherheitsaktualisierungen) gebündelt als sogenannte inoffizielle Service Packs zur Verfügung. Die Verwendung dieser inoffiziellen Service Packs wird von Microsoft nicht unterstützt und birgt die Gefahr einer Infektion, etwa mit Schadprogrammen.

Verwendet ein Benutzer standardmäßig ein uneingeschränktes Benutzerkonto, so werden alle Programme im Sicherheitskontext eines Administratorkontos ausgeführt. Damit hat auch Schadsoftware (Viren, Würmer, Trojaner, Spyware, Adware usw.) alle Möglichkeiten, Veränderungen am System vorzunehmen. Oftmals werden diese Veränderungen so umgesetzt, dass der Anwender des befallenen Computers diese zunächst nicht bemerkt (z. B. wird eine Schadsoftware als Systemdienst eingerichtet und dann automatisch ständig ausgeführt).

Zur Lösung dieses Problems bietet Windows XP die Möglichkeit, den Computer standardmäßig mit einem eingeschränkten Benutzerkonto zu verwenden.

Zur Markteinführung von Windows XP waren viele Programme nicht an Windows-NT-Systeme angepasst, sie waren von Konzept und Realisierung her nur auf die nun beendete Windows-9x-Linie abgestimmt. Daher funktionierten sie oft nicht richtig, wenn der angemeldete Anwender nicht alle Administrator-Berechtigungen hatte. Später entwickelte Programme ließen sich dagegen auch vollständig mit einem „eingeschränkten Benutzerkonto“ benutzen. Für die systemweite Installation von Programmen ist ein Administratorkonto notwendig, da besondere Berechtigungen nötig sind, wenn Teile des Betriebssystems, dessen Konfiguration oder Einstellungen anderer Benutzer modifiziert werden. Auf Administratorrechte kann bei der Installation eines Programms nur verzichtet werden, wenn das Programm ausschließlich für das Benutzerprofil des angemeldeten Benutzers installiert wird. Unter Windows XP (auch Windows 2000 und Windows NT) können sehr detaillierte Berechtigungen auf Dateien und weitere Systemobjekte (z. B. Registry-Schlüsseln, Pipes etc.) vergeben werden.

Neben älteren Spielen betraf diese Problematik weitere Programme, die nicht nur für den privaten Gebrauch vorgesehen waren.

Wie andere Microsoft-Produkte steht auch Windows XP unter der Kritik, durch den Kauf werde ein „Quasi-Monopolist“ unterstützt. Tatsächlich ist die Dominanz von Windows auf dem Heimcomputer-Betriebssystem-Markt unübersehbar, so erfordern viele Anwendungsgebiete Microsoft-Produkte und der Einsatz von Windows XP oder anderer Windows-Betriebssysteme ist dort – zumindest sekundär – zwingend.

Gerade Windows XP integrierte viele Anwendungen, die bisher durch andere Anbieter bereitgestellt worden waren, und wurde dafür stark kritisiert und teilweise streng beobachtet. Solche Anwendungen sind zum Beispiel Mediaplayer (Windows Media Player), Instant Messenger (Windows Messenger) oder die enge Bindung an das Microsoft-Passport-Netzwerk, das in der Fachwelt teilweise als ein Sicherheitsrisiko und eine potentielle Bedrohung der Privatsphäre angesehen wird. Das wird ebenso als eine Fortführung von Microsofts traditionell wettbewerbsbeschränkendem Verhalten angesehen.

Obwohl die jüngste Kritik vor allem diese drei Programme im Blick hatte, waren auch in früheren Windows-Versionen – beispielsweise Windows 95 – schon Komponenten so in das System integriert, dass sie mit herkömmlichen Mitteln nicht mehr trennbar waren (Unmöglichkeit der Deinstallation) und laut Microsoft auch überhaupt nicht mehr getrennt werden konnten. Vor allem der Webbrowser (Internet Explorer, siehe auch Browserkrieg) fiel dabei oft in Kritik, aber auch der graphische Dateimanager (Windows Explorer) oder der TCP/IP-Stack.

Microsoft argumentiert zudem, dass solche Systemwerkzeuge nicht mehr Spezialanforderungen bedienen, sondern in den Bereich allgemeinen Interesses gerückt seien und damit ihre Existenzberechtigung als allgemeine Komponenten des Betriebssystems verdienen würden. Als Bestätigung dieser Auffassung findet sich zudem fast kein anderes Betriebssystem, das nicht ebenfalls Systemwerkzeuge integriert hat.

Ebenso werden Neuentwicklungen für Windows von Microsoft teilweise nur für neuere Windows-Betriebssysteme verfügbar gemacht, obwohl diese technisch auch für ältere Windows-Versionen möglich wären, zum Beispiel DirectX oder die .NET-Laufzeitumgebung. Andererseits gibt es keinen Hersteller von Betriebssystemen, der Ergänzungen und Erweiterungen stets für alle älteren Versionen herausgibt.

Microsoft erfüllte manchmal nur notdürftig Gerichtsanordnungen bezüglich gebündelter Software durch Veröffentlichung von speziellen Downgrades oder Versionen ohne den betreffenden Software-Teil. Es wird dabei kritisiert, dass Microsoft diese Komponenten häufig nicht vollständig entfernt habe, auch wenn das technisch möglich gewesen wäre. Microsoft rechtfertigte diesen Schritt mit der Tatsache, dass Schlüsselfunktionen von Windows von dieser Software abhängen würden, so das HTML-Hilfesystem oder die Windows-Schreibtischoberfläche (Desktop).

Ein weiterer Kritikpunkt an Windows XP und seinen Komponenten ist die Übermittlung von Daten an den Hersteller. Windows XP sendet regelmäßig Daten an Microsoft. Laut Microsoft handelt es sich dabei um Daten, deren Art veröffentlicht ist, Kritiker bezweifeln das jedoch. Keine Studie überprüfte bisher, welchen Inhalt diese in verschlüsselter Form übertragenen Datenpakete tatsächlich haben. Kritiker befürchten, dass kaum nur die Daten übermittelt werden, die Microsoft offiziell angibt; dafür seien die Pakete nach der Meinung mancher zu groß. Gegen eine Darstellung des Spiegels und des Heise-Verlags im Jahre 2002, dass beispielsweise der Windows Media Player die genutzten Medieninhalte an Microsoft-Server übermittle, protestierte Microsoft nicht öffentlich.

Windows XP wurde seit seinem Erscheinen häufig mit dem freien Betriebssystem Linux verglichen. Es wurde argumentiert, dass die Anforderungen an die Hardware zu hoch und die von Microsoft herausgegebenen Mindestanforderungen unrealistisch für ein produktives Arbeiten seien. Ein paar Jahre später hat die Hardware-Entwicklung diese Aussage eingeholt, da auch preisgünstige Rechner genügend Leistung bringen. Tatsächlich wurde neben Linux auch Windows XP auf vielen Netbooks eingesetzt, auf denen der Windows-XP-Nachfolger Vista wegen dessen höherer Hardware-Anforderung nicht brauchbar gewesen wäre. Obwohl Windows Vista das aktuelle Windows-Betriebssystem war, verkaufte Microsoft ein besonders günstiges Windows XP speziell für Netbooks bis mindestens 2009. Erst dann waren einerseits etwas bessere Netbooks und andererseits mit Windows 7 Starter eine günstige Windows-Version auch für Netbooks verfügbar.

Da die SATA-Schnittstelle bei der Produkteinführung noch sehr neu war, beinhaltet die Installations-CD noch keine generischen Treiber für diese Controller. Durch das Einstellen des IDE-Modus für den SATA-Controller im BIOS lässt sich Windows XP auch ohne SATA-Treiber installieren und bietet praktisch die gleiche Performance wie über den AHCI-Modus. Wenn das BIOS des Rechners keinen Modus für IDE-Kompatibilität bietet, kann auf die zur Installation vorgesehene Festplatte nicht ohne Weiteres zugegriffen werden. Wie bei anderen speziellen (SCSI, RAID) oder neuen Kontrollern kann man den benötigten Treiber mit einer Diskette – und nur mit dieser – während der Installation zur Verfügung stellen. Viele neue Computer verzichten aber auf ein Diskettenlaufwerk und eine Routine für einen CD-Wechsel oder das Laden über USB ist nicht vorgesehen. Es muss daher entweder ein Diskettenlaufwerk nachgekauft oder eine eigens angepasste Installations-CD erstellt werden.

Die Systemvoraussetzungen für Windows XP "Home" und "Professional" Edition werden wie folgt angegeben:

Dabei ist zu beachten, dass diese Voraussetzungen für eine grundlegende Installation ohne zusätzliche Programme und sonstige Patches und auf Festplatten von maximal etwa 2 TB gelten.

Seit etwa 2010 werden allerdings auch für den nicht-professionellen Einsatz in zunehmendem Umfang Festplatten von mehr als 2 Tebibyte (TiB) Gesamtgröße angeboten. Deren Partitionen können nicht mehr durch den seit der Einführung von DOS üblichen Master Boot Record (MBR) verwaltet werden, sondern dies erfolgt beispielsweise durch eine GUID Partition Table (GPT). Microsoft verweist darauf, dass dann je nach Version von Windows XP Einschränkungen sowohl hinsichtlich der Installierbarkeit des Systems als auch hinsichtlich der Nutzbarkeit der Kapazität gelten. Einschränkungen gelten laut Microsoft je nach Version von Windows XP auch für Festplatten, bei denen – unabhängig von der Größe des gesamten Mediums – die physische Größe der Sektoren nicht 512 Bytes, sondern beispielsweise 4 Kibibyte (KiB) beträgt. Software-Anpassungen an 4 KiB-Sektoren-Platten gibt es durch Microsoft nur für Windows 7 und jüngere Betriebssysteme. Daher wird die Kompatibilität solcher Platten mit Sektoren von mehr als 512 Bytes unter Windows XP herstellerseitig entweder (unabhängig vom Nutzer) mit besonderer Firmware oder (vom Nutzer anzuwenden) mit Anpassungs-Programmen (beispielsweise für Platten von Western Digital) erreicht.

Messungen des tatsächlichen Nutzungsanteils eines Betriebssystems sind schwierig, so dass verschiedene Erhebungen deutlich unterschiedliche Ergebnisse liefern können. Laut der StatCounter, welches Webzugriffe analysiert, sei XP bis 2011 das am meisten eingesetzte Betriebssystem gewesen, ehe es im Laufe des Jahres 2011 von Windows 7 überholt worden sei. Auswertungen von Net Applications, das ebenfalls Webzugriffe analysiert, ergaben, dass Windows XP noch bis September 2012 das führende Betriebssystem gewesen sei.

Geplant wollte Microsoft die Auslieferung im Januar 2008 beenden, da aber ihr Nachfolger Vista viel zu hohe Hardwareanforderungen an preisgünstige und mobile Rechner stellte, verschob der Konzern sein Aus bis zum 30. Juni 2008. Für Subnotebooks und Netbooks wurde Windows XP sogar bis 2010 ausgeliefert, um in diesem Marktsegment – trotz schlankerem Windows 7 ab 2009 – nicht an Konkurrenten zu verlieren. Erst später konnten viele Subnotebooks und Netbooks mit Windows 8 / 8.1, welches ähnliche Hardwareanforderungen wie das sechs bzw. fünf Jahre ältere Vista hat, ausgeliefert werden.




</doc>
<doc id="5586" url="https://de.wikipedia.org/wiki?curid=5586" title="William Gibson">
William Gibson

William Ford Gibson (* 17. März 1948 in Conway, South Carolina) ist ein US-amerikanischer, in Kanada lebender Science-Fiction-Autor. Bekannt wurde er mit seinem 1984 erschienenen Roman Neuromancer, der in diesem Jahr alle gängigen SF-Preise erhielt: Den Philip K. Dick Award, den Nebula Award sowie den Hugo Award.
In diesem Buch prägte er unter anderem den Begriff Cyberspace, der noch immer häufig für elektronische Netze wie das World Wide Web verwendet wird, sowie das Subgenre des Cyberpunk und den Begriff der Matrix, welche durch ein globales Informationsnetzwerk gebildet wird und so den Cyberspace ermöglicht.

William Ford Gibson wurde am 17. März 1948 in Conway, South Carolina, als einziger Sohn eines höheren Managers einer Baufirma geboren. Einhergehend mit der Stelle des Vaters musste die Familie häufig umziehen, während der Vater oft zusätzlich auf Dienstreisen unterwegs war. Als Gibson sechs Jahre alt war, erstickte sein Vater während einer solchen Dienstreise in einem Restaurant an seinem Essen, und die nun verwitwete Mutter zog mit ihm in das Dorf in Südwest-Virginia, aus dem sie und Gibsons Vater stammten.

Als Gibson 15 Jahre alt war, schickte ihn seine Mutter in ein Jungeninternat in Arizona. In Gibsons 18. Lebensjahr verstarb seine Mutter, und einige Zeit später verließ er die Schule ohne einen Abschluss. 1967 zog er nach Kanada um, wo er in engem Kontakt mit vielen fahnenflüchtigen Amerikanern stand, die sich der Einberufung zum Vietnamkrieg entzogen. Nach eigenen Angaben fühlte er sich in deren Gesellschaft nie völlig wohl, weil er deren Hintergrund nicht teilte und jederzeit in die USA zurückgehen konnte. 1972 zog er mit seiner Freundin und heutigen Frau nach Vancouver, British Columbia, wo er an der University of British Columbia einen Hochschulabschluss in Englisch machte. Gibson lebt noch heute mit seiner Frau und zwei Kindern in Vancouver.

Im Jahr 1999 wurde die Dokumentation "No Maps for These Territories" über Gibson veröffentlicht. 2008 wurde er in die Science Fiction Hall of Fame aufgenommen.


Eine Reihe von Kurzgeschichten wurden gesammelt unter dem Titel "Burning Chrome" (dt. 1988 Cyberspace) veröffentlicht.




Drehbücher

Literarische Vorlage

Darsteller




</doc>
<doc id="5589" url="https://de.wikipedia.org/wiki?curid=5589" title="Weichsel">
Weichsel

Die Weichsel ( (), , "Vistula") ist ein 1048 Kilometer langer Strom und der längste Fluss in Polen. 
Das Einzugsgebiet umfasst auch Teile der Slowakei, Weißrusslands und der Ukraine. Der längste Gewässerlauf in ihrem Flusssystem sind die 1213 Flusskilometer von der Quelle des Westlichen Bug bis zur Ostsee.

Auf alten Landkarten findet man auch die Schreibweisen "W(e)ixel" oder "Wissel".

Der Strom entspringt in den Schlesischen Beskiden auf einer Höhe 1107 m ü. NN bzw. 1080 m. ü. NN am südwestlichen Hang der Barania Góra (deutsch: "Widderberg") aus den Bächen Czarna Wisełka und Biała Wisełka, die nach neun beziehungsweise sieben Kilometern in den Stausee Zbiornik Czerniański münden. Nach Verlassen des Gebirges wendet sie sich nach Osten und bildet ein Stück weit die historische Grenze zwischen Oberschlesien und Kleinpolen. Unterhalb des Goczałkowice-Stausees ist sie schiffbar.

Sie fließt ostwärts durch in eine tektonische Senke, die nördlich von der Krakau-Tschenstochauer Höhe und dem Kielcer Bergland begrenzt wird, südlich vom Beskidenvorland und dann östlich vom Lubliner Hügelland. Etwa 70 km nach dem Stausee fließt die Weichsel durch die alte Königsstadt Krakau (Kraków). Von Niepołomice 25 km östlich von Krakau bis etwas unterhalb der Mündung des San war die Weichsel von 1815 bis 1916 die Grenze zwischen dem österreichischen Galizien und dem Russischen Reich. 70 km hinter dem Ort mündet der Karpatenfluss Dunajec. Bald hinter Sandomierz mündet von den Karpaten her der San in die Weichsel. Weiter nördlich erreicht sie das polnische Tiefland. Kurz hinter Warschau ("Warszawa") gelangt sie in den Bereich überwiegend ostwestlich ausgerichteter Urstromtäler, durch die ihr von Osten, kurz vorher vereint, Bug und Narew zufließen. Hier wendet sie sich stark nach Westen und passiert Płock, Dobrzyń nad Wisłą, Włocławek und Toruń ("Thorn"). Bei Włocławek besteht seit 1970 ein großes Stauwehr mit Wasserkraftwerk. Bei Bydgoszcz ("Bromberg"), dessen Stadtzentrum nicht am Strom liegt, verlässt sie das große ostwestliche Urstromtal und durchbricht in einem kleineren den baltischen Landrücken. Dieser umfasst zwischen Weichsel und Oder die Pommersche Seenplatte, zwischen Weichsel und Njemen die Masurische Seenplatte. In den letzten beiden Jahrzehnten des 20. Jahrhunderts wurden am Mittellauf umfangreiche Regulierungsmaßnahmen durchgeführt, um die Schiffbarkeit zu verbessern.

Der Einstrom des Flusses in die Ostsee wurde natürlicherweise durch den Dünenrücken der "Danziger Binnennehrung" versperrt, des westlichen Teils der Frischen Nehrung (Mierzeja Wiślana). Zwischen Landrücken und Dünenrücken hat sich ein Delta gebildet. Kurz hinter Gniew ("Mewe") zweigt nach Osten die Nogat ab, die erst 1371 durch ein Hochwasser vom selbständigen Fluss (wieder) zum Weichselarm wurde und im Bereich der Elbinger Niederung in das Frische Haff (polnisch "Zalew Wiślany", also "Weichselhaff") mündet. Kurz vor dem Dünenrücken verzweigte sich natürlicherweise der Hauptstrom der Weichsel in die "Elbinger oder Königsberger Weichsel (Szkarpawa)", die ebenfalls ins Frische Haff mündet und bis Anfang des 19. Jahrhunderts der Hauptstrom war, und die Danziger Weichsel, die nahe der Stadt Danzig den Dünenrücken durchbrach und die in die Danziger Bucht (polnisch "Zatoka Gdańska") mündete. Im Jahre 1840 entstand bei einem Hochwasser ein neuer Dünendurchbruch auf halbem Wege zwischen der Gabelung und Danzig, woraufhin der westliche untere Teil des alten Mündungsarms versandete. In den Jahren 1889 bis 1895 wurde dann bei der Gabelung der Dünenrücken durchstochen, um die Hochwassergefährdung des Weichseldeltas zu vermindern. Seit der größte Teil des Weichselwassers durch diesen "Weichseldurchstich", polnisch "Przekop Wisły", in die Ostsee strömt, versandet die Danziger Weichsel insgesamt und wurde zunehmend Tote Weichsel genannt, polnisch "Martwa Wisła".

Der östliche Weichselarm Szkarpawa hat seinerseits ein Delta ausgebildet. Dessen nördlichster Arm wird weiterhin "Wisła Królewiecka (Königsberger Weichsel)" genannt.

Es besteht Uneinigkeit darüber, ob der Name Weichsel indoeuropäischer oder prä-indoeuropäischer Herkunft ist.

Die ersten schriftlichen Erwähnungen der Weichsel sind etwas mehr als 2000 Jahre alt und stammen von römisch-antiken Autoren. 

Pomponius Mela nannte 44 n. Chr. im dritten Buch der "Chorographia" (3.27) die "Visula" als Grenze zwischen Germanien und Sarmatien. Plinius nannte 77 n. Chr. in seiner Naturgeschichte (4.52, 4.89) ausdrücklich zwei Namen: „"Visculus" sive "Vistla"“. Der Vistla-Fluss floss demnach in das "Mare Suebicum", das heute als Ostsee bekannt ist.

Plinius bezeichnete gleichfalls die Weichsel als den Grenzfluss zwischen dem germanischen und sarmatischen Einflussgebiet. Die zu seiner Zeit im Weichselgebiet lebenden Ostgermanen bezeichnete Plinius als "Vandili" (Vandalen) und nannte als Teilstämme "Burgodiones (Burgunder)", "Varinnae", "Charini" und "Gutones (Goten)". Die Goten hatten sich erst im letzten Jahrhundert vor der Zeitenwende an der unteren und mittleren Weichsel angesiedelt, begannen aber schon um 200 n. Chr. wieder abzuwandern und sind ab dem 5. Jahrhundert nicht mehr dort nachzuweisen.

Abgesehen von den Wanderungsbewegungen änderten sich auch die Bezeichnungen: Tacitus bezeichnete in seiner "Germania" die östlich der Weichselmündung wohnenden "Aesti" oder "Aisti" (wohl gleichbedeutend mit der heutigen Bezeichnung "Balten") als Germanen, wies aber darauf hin, dass sie eine dem Britischen (Keltisch) ähnelnde Sprache sprechen und unterschied sie von den "Suebi".

Ab dem 5./6. Jahrhundert nach Chr. sind slawische Siedlungen an der Weichsel nachgewiesen. Zwischen germanischer und slawischer Siedlungsperiode ist in diesen Gebieten ein erheblicher Abwanderungsverlust festzustellen. In der germanischen schriftlichen Tradition der Weichselwälder "(Widsith, Vers 121)" – „die Leute, die an Weichsel leben“ (auf dem Gebiet der Przeworsk-Kultur) – sind die Heimat der Sachsen und anderen Germanen :

"Wulfhere sohte ic ond Wyrmhere; ful oft þær wig ne alæg,"

"þonne Hræda here heardum sweordum"

"ymb Wistlawudu wergan sceoldon"

"ealdne eþelstol ætlan leodum."

Als Jordanes im 6. Jahrhundert eine Chronik der Goten, Getica, erstellte, benannte er den Fluss Vistula. Er beschrieb ebenfalls zwei weitere Flüsse mit dem Namen Viscla. Dieser Name bezieht sich auf den Nebenfluss Wisłoka und den Nebenfluss des San, Wisłok.

Etwa im Jahre 850 n. Chr. wurden die meist östlich der Mündung der Weichsel wohnenden Prußen vom Bayrischen Geograph als „Bruz“ erwähnt.

Weichsel und Warthe haben nach der dritten Teilung Polens als Freiheitssymbole Eingang in die polnische Nationalhymne Mazurek Dąbrowskiego gefunden.

Schon der erste polnische Chronist Wincenty Kadłubek beschrieb die Weichsel als den Heimatort der Wandalen, von denen er die polnische Sage Wanda herleitete.

Der polnische Name Wisła leitet sich vom lateinischen Vistla ab. Das Buchstabenpaar -tl- wurde schlicht durch das polnische -ł- ersetzt. So heißt dann auch die erste Stadt, durch welche die Weichsel fließt.

Im Mai 2010 kam es in Polen (und in anderen Ländern Mitteleuropas) zu großen Hochwassern. Im September 2012 war der Wasserstand der Weichsel historisch niedrig und lag bei 58 cm. Dieser Zustand dauerte an bis 2015. Während dieser Zeit fand man auf dem Flussboden jahrhundertealte Artefakte.

Reihenfolge flussabwärts, Großstädte in Fettsatz, Einwohnerzahlen vom 31. Dezember 2016

Reihenfolge flussabwärts mit Längenangabe (ab 300 km Fettsatz), Abfluss und Größe des Einzugsgebietes; eingerückt mündungsnahe Nebenflüsse der Nebenflüsse






</doc>
<doc id="5591" url="https://de.wikipedia.org/wiki?curid=5591" title="Weltbank">
Weltbank

Die Weltbank (engl. ) bezeichnet im weiten Sinne die in der US-amerikanischen Hauptstadt Washington, D.C. angesiedelte "Weltbankgruppe", eine multinationale Entwicklungsbank. Die Weltbankgruppe hatte ursprünglich den Zweck, den Wiederaufbau der vom Zweiten Weltkrieg verwüsteten Staaten zu finanzieren.

Die Weltbankgruppe umfasst die folgenden fünf Organisationen, die jeweils eine eigene Rechtspersönlichkeit besitzen:


Die Organisationen der Weltbankgruppe sind durch verwaltungsmäßige Verflechtungen und durch einen gemeinsamen Präsidenten (im Fall der ICSID als Vorsitzender des Verwaltungsrates) verbunden.

Die Weltbank ist nicht mit dem Internationalen Währungsfonds (IWF) zu verwechseln, auch wenn beide Institutionen zeitgleich gegründet wurden und in enger Nachbarschaft ihren Sitz haben. Vereinfacht lässt sich sagen, dass die Weltbank-Gruppe Finanzierungsinstrumente für langfristige Entwicklungs- und Aufbauprojekte im Bereich der Realwirtschaft bereitstellt. Der IWF stellt demgegenüber für Länder, die – oft auf Grund von Zahlungsbilanzschwierigkeiten – Bedarf an Fremdwährung haben, Brückenfinanzierung bereit. Die Tätigkeit des IWF ist daher eher auf den Bereich der Finanzwirtschaft als auf die Finanzierung der Realwirtschaft gerichtet. Die Unterscheidung hat allerdings in jüngerer Zeit etwas an Gewicht verloren, da auch der IWF begonnen hat, Kreditlinien (sogenannte Fazilitäten) bereitzustellen, die auf die entwicklungspolitischen Ziele ärmerer Länder zugeschnitten sind.

Die gemeinsame Kernaufgabe dieser Institutionen ist es, die wirtschaftliche Entwicklung von weniger entwickelten Mitgliedstaaten "durch finanzielle Hilfen, Beratung sowie technische Hilfe" zu fördern und so zur "Umsetzung der internationalen Entwicklungsziele" beizutragen (vor allem den Anteil der Armen an der Weltbevölkerung bis zum Jahr 2015 um die Hälfte reduzieren zu helfen). Sie dienen auch als Katalysator für die Unterstützung durch Dritte. Die Weltbankgruppe hat im Geschäftsjahr 2008 38,2 Milliarden USD an Darlehen, Zuschüssen, Beteiligungen, Investitionen und Garantien an ihre Mitgliedstaaten sowie Privatinvestoren vergeben.

Dies geschieht vornehmlich durch die Vergabe von langfristigen Darlehen zu marktnahen Konditionen (IBRD) beziehungsweise zinslosen, langfristigen Krediten (IDA) für Investitionsprojekte, umfassende Reformprogramme und technische Hilfe, zunehmend auch durch die Förderung der privatwirtschaftlichen Entwicklung durch die Beteiligung an Firmen (IFC) und durch die Übernahme von Garantien (MIGA).

1974 haben die Gouverneursräte der Weltbank und des IWF auf Wunsch der Entwicklungsländer einen gemeinsamen Ministerausschuss über den Transfer von finanziellen Ressourcen in Entwicklungsländer eingesetzt – das "Entwicklungskomitee" (, "DC"). Das DC hat 24 Mitglieder, die die gesamte Mitgliedschaft der Weltbankgruppe und des IWF vertreten, und tagt zweimal im Jahr. Seine Aufgabe ist es, die Gouverneursräte der Bank und des IWF über "wichtige Entwicklungsfragen" und über die für die wirtschaftliche und soziale Entwicklung in den Entwicklungsländern "erforderlichen Ressourcen" zu beraten. Im Laufe der Zeit hat das DC auch Fragen des Handels und des globalen Umweltschutzes in seine Beratungen einbezogen.

Die Weltbank veröffentlicht jährlich den Weltentwicklungsbericht (), der jeweils einem übergreifenden und für die aktuelle Entwicklungsdiskussion bedeutsamen Thema gewidmet ist. Er fasst in seinen detaillierten Analysen nicht nur den Diskussionsstand zusammen, sondern gibt vor allem auch entscheidende Anstöße und bringt die internationale Debatte über Entwicklung um wichtige Schritte voran. Weitere zentrale Weltbank-Berichte sind der , und .

Das " (PSD)" ist eine Strategie der Weltbank, die Privatwirtschaftsentwicklung in Entwicklungsländern voranzutreiben. PSD ist für sämtliche Teile der Weltbank verbindlich und alle anderen Strategien müssen hierauf abgestimmt sein. Auch die Vergabe von Krediten ist an grundlegende Reformen im Sinne der PSD geknüpft. Hierzu zählt die Förderung einer privaten Herstellung von Infrastruktur. Dies wird mit einer häufigen Bevorzugung öffentlicher Unternehmen durch die öffentliche Hand begründet, welche Wettbewerb verhindere (vgl. Strukturanpassungsprogramme).

Ebenso wie der Internationale Währungsfonds (IWF) sind IBRD, IDA und IFC Sonderorganisationen der Vereinten Nationen. Mitglieder bei der IBRD können nur Staaten werden, die bereits dem IWF angehören und alle damit verbundenen Verpflichtungen übernommen haben. Die Mitgliedschaft bei der IBRD ist wiederum Voraussetzung für den Beitritt zur IDA, zur IFC, zur MIGA und zum ICSID.

Oberstes Organ der IBRD (wie auch bei IFC, IDA und MIGA) ist der "Gouverneursrat", für den jeder Mitgliedstaat einen Gouverneur (in der Regel den Wirtschafts- oder Finanzminister) und einen Stellvertreter ernennt. Das "Exekutivdirektorium" besteht bei der IBRD, der IDA und der IFC seit November 2010 aus 25 Personen; davon werden sechs von den Mitgliedern mit den höchsten Kapitalanteilen (darunter Deutschland) ernannt, die übrigen 19 werden alle zwei Jahre durch die Gouverneure anderer Mitgliedstaaten gewählt. Mit Ausnahme Saudi-Arabiens, das durch einen eigenen Exekutivdirektor repräsentiert wird, vertreten die übrigen gewählten Direktoren jeweils mehrere Mitgliedstaaten (Stimmrechtsgruppen). Die Exekutivdirektoren nehmen – im Auftrage ihrer Gouverneure – das Tagesgeschäft wahr.




Die laufenden Geschäfte führt der Präsident entsprechend den Beschlüssen des Direktoriums. Er wird von den Exekutivdirektoren für jeweils fünf Jahre gewählt und darf weder Gouverneur noch Exekutivdirektor sein. Er hat den Vorsitz im Direktorium (ohne Stimmrecht, außer bei Stimmengleichheit) und ist Leiter des Mitarbeiterstabs (die Weltbank hat etwa 10.000 Mitarbeiter). Präsident der IBRD und ihrer Schwestergesellschaften IDA und MIGA war zuletzt von 2005 bis 2007 der Amerikaner Paul Wolfowitz, seit Juni 2007 Robert Zoellick, und nun ab dem 1. Juli 2012 Jim Yong Kim. Bei der IFC und bei der MIGA bestehen organisatorische Besonderheiten insofern, als sie einen eigenen, von der IBRD und IDA getrennten Mitarbeiterstab und einen eigenen geschäftsführenden Vizepräsidenten haben.

Bei Abstimmungen im Gouverneursrat und Direktorium richtet sich das Stimmengewicht des einzelnen Landes im Wesentlichen nach der Höhe seines Kapitalanteils. Wie beim IWF verfügen alle Mitglieder – über eine bestimmte Anzahl von Basisstimmen hinaus – über Stimmrechte entsprechend ihrer finanziellen Beteiligung.

Lange Zeit (bis 2007) war es gängige Praxis, dass die USA den Präsidenten der Weltbank und Europa den Vorsitzenden des Internationalen Währungsfonds stellten. Hier eine Aufstellung der Präsidenten mit ihrer jeweiligen Amtszeit:
Die Stimmrechte sind nach Anteilseigentum verteilt. Im Jahr 2010 wurde die Verteilung neu gewichtet, wodurch Schwellenländer, v. a. China an Einfluss gewannen. Den größten Stimmenanteil haben die USA (15,85 %), gefolgt von Japan (6,84 %), China (4,42 %), Deutschland (4,00 %), dem Vereinigten Königreich (3,75 %), Frankreich (3,75 %), und Indien (2,91 %). Die Länder, die unter der „Voice Reform – Phase 2“ genannten Reform signifikant hinzugewonnen haben, sind Südkorea, die Türkei, Mexiko, Singapur, Griechenland, Brasilien, Indien, und Spanien. Der Stimmanteil der meisten Industriestaaten wurde reduziert, während Nigeria, USA, Russland und Saudi-Arabien unverändert blieben.

Die Weltbank wird seit langen von Nicht-Regierungsorganisationen, wie der Organisation zur Vertretung der Rechte Eingeborener Survival International, und von verschiedenen Ökonomen wie Henry Hazlitt und Ludwig von Mises kritisiert, darunter sogar ihr früherer Chefökonom und Wirtschaftsnobelpreisträger Joseph Stiglitz. Henry Hazlitt argumentierte, die Weltbank fördere gemeinsam mit dem monetären System, dessen Teil sie ist, die weltweite Inflation und „eine Welt, in der der internationale Handel von Staaten dominiert“ werde. Stiglitz meinte, dass die von der Bank verfochtene, auf einen freien Markt hin orientierte Reformpolitik die wirtschaftliche Entwicklung vielfach schädige, wenn sie schlecht oder zu schnell (in Form einer „Schocktherapie“), in der falschen Reihenfolge oder in schwachen, nicht konkurrenzfähigen Wirtschaftsräumen umgesetzt würde.

Häufige Kritik wird an der Form der Führung der Weltbank geübt. Obwohl die Bank 188 Länder repräsentiert, wird sie von einer kleinen Zahl von Ländern angeführt. Diese Länder (die auch den Großteil ihres Budgets finanzieren) wählen die Präsidenten und die führenden Manager der Organisation, sodass ihre Interessen in der Bank dominieren. Der Bank wurde sogar vorgeworfen, die ungleiche Machtverteilung bei den Stimmrechten zugunsten der westlichen Länder und die Rolle der Bank in Entwicklungsländern würde sie vergleichbar machen zur South African Development Bank während des Apartheidregimes, und damit zu einer Säule der „globalen Apartheid“.

In den 1990ern propagierten die Weltbank und der IWF den „Washington Consensus“, ein Bündel wirtschaftspolitischer Maßnahmen, wozu Deregulierung und Marktliberalisierung, Privatisierungen und der Rückzug des Staates gehörten.
Obwohl der Washington Consensus die wirtschaftliche Entwicklung fördern sollte, wurde er dafür kritisiert, Fairness, Beschäftigungspolitik und die tatsächliche Umsetzung der Reformen außer Acht zu lassen. Joseph Stiglitz meinte, der Consensus messe dem Wachstum des Bruttoinlandsproduktes zu viel Bedeutung zu und achte zu wenig auf die Nachhaltigkeit dieses Wachstums oder darauf, ob damit auch ein Beitrag zum Wachstum des Lebensstandards geleistet werde.

Einer der größten Kritikpunkte an der Weltbank bezieht sich auf die Auswirkungen der von ihr und vom Internationalen Währungsfonds von armen Ländern geforderten Strukturanpassungsprogramme. In einigen Ländern, vor allem im Subsahara-Raum, führten diese Maßnahmen zu einem Rückgang des Wirtschaftswachstums und zu höherer Inflation. Da Armutsreduzierung kein Ziel der Programme war, verschlimmerten sich vielfach die Lebensbedingungen der Armen in Folge der Reduktion von Sozialausgaben und der Erhöhung der Lebensmittelpreise.

Im Jahr 2009 waren unter den mehr als eintausend US-amerikanischen Angestellten der Weltbank nur vier Afroamerikaner. Dem Vorwurf, die Weltbank betreibe rassistische Diskriminierung, entgegnete die Unternehmensleitung, es gäbe einfach nicht genug qualifizierte Afroamerikaner. Bereits 1998 hatte ein interner Untersuchungsausschuss festgestellt: „In unserer Einrichtung gibt es rassistische Diskriminierung.“ Außerdem würden Afrikaner und Afroamerikaner in einem gesonderten Gebäude „ghettoisiert“.





</doc>
<doc id="5595" url="https://de.wikipedia.org/wiki?curid=5595" title="Warschau">
Warschau

Warschau (polnisch Warszawa ) ist seit 1596 die Hauptstadt Polens und die flächenmäßig größte sowie mit über 1,7 Mio. Einwohnern (2013) bevölkerungsreichste Stadt des Landes. Als eines der wichtigsten Verkehrs-, Wirtschafts- und Handelszentren Mittel- und Osteuropas genießt Warschau große politische und kulturelle Bedeutung. In der Stadt befinden sich zahlreiche Institutionen, Universitäten, Theater, Museen und Baudenkmäler. 

Beidseitig am Strom der Weichsel (pln. "Wisła") in der Woiwodschaft Masowien gelegen, stellt sie mit rund 3,5 Mio. Einwohnern das Zentrum der zweitgrößten Agglomeration Polens dar. Ihr Stadtgebiet gliedert sich in 18 Stadtbezirke, unter denen Śródmieście ("Stadtmitte") die Innenstadt ausmacht und das UNESCO-Welterbe der wiederaufgebauten Warschauer Altstadt beherbergt.

Warschau liegt an der mittleren Weichsel, im Urstromtal der Weichsel, sowie an der Mittelmasowschen Senke auf durchschnittlich 100 Metern über dem Meeresspiegel. Die Stadt breitet sich beidseitig der Weichsel aus und liegt in etwa der Mitte zwischen den Karpaten und der Ostsee – es sind jeweils ca. 350 km. Der historische Stadtkern liegt am linken, westlichen Weichselufer auf dem langgezogenen Weichselkliff Skarpa Wiślana, welches relativ steil etwa 15 bis 30 Meter über die Weichsel emporragt. Eine der ersten Brücken in Europa mit einer Länge von mehreren hundert Metern verband die beiden Ufer bereits im 16. Jahrhundert. Das begünstigte die Ausdehnung der Stadtbebauung auf das rechte Weichselufer, welches seit jeher den Namen "Praga" trägt. Im Stadtgebiet gibt es mehrere eiszeitliche Moränenhügel sowie von Menschenhand geschaffene Anhöhen. Die Weichsel ist im Gebiet von Warschau schiffbar. Die Stadt hat am rechten Weichselufer den Binnenhafen Żerań. Allerdings beschränkt sich der Schiffsverkehr auf kleinere Schiffe und Boote, da die Flusstiefe oftmals drei Meter nicht übersteigt.

Warschau befindet sich in der Übergangszone vom maritimen zum kontinentalen Klima. Die Jahresdurchschnittstemperatur liegt bei 8,5 °C. Der kälteste Monat ist Januar mit einer Durchschnittstemperatur von −1,9 °C und der wärmste ist Juli mit 19 °C. Die Sommer sind warm bis heiß, die Winter kühl und teilweise auch eisig kalt. Die Summe der jährlichen Niederschlagsmenge übersteigt nicht 550 mm. Eine dicke Schneeschicht ist in den Wintermonaten keine Seltenheit und die Gewässer in den Parkanlagen sowie die Weichsel können vollständig zufrieren.
Seit der letzten Verwaltungsreform im Jahre 2002 ist Warschau wieder eine einheitliche Stadtgemeinde, die gleichzeitig den Status eines Landkreises ( "Powiat") hat. Dieser Status ist in etwa mit einer kreisfreien Stadt in Deutschland vergleichbar. Vorher war Warschau ein aus mehreren unabhängigen Gemeinden "(gminy)" bestehender relativ loser Kommunalverband. Jetzt gliedert sich die Stadt in 18 Stadtbezirke "(dzielnice)", die einer gesamtstädtischen Verwaltung recht stark untergeordnet sind. Die meisten der neuen Bezirke sind aus den alten Gemeinden hervorgegangen, jedoch mit zwei Ausnahmen:

Nachfolgend die derzeitigen Bezirke Warschaus in Zahlen (Stand: 2014):

Warschau wurde erstmals 1241 in der lateinischen Ausfertigungsformel einer Schenkungsurkunde erwähnt ("actum et datum Varschevie", also „verfügt und ausgegeben zu Warschau“), aus dem späteren Mittelalter überlieferte Namensformen sind unter anderem "Warseuiensis" (1321, lateinisches Adjektiv), "Varschewia" (1342) und "Warschouia" (1482). 

Die Etymologie des Toponyms ist ungeklärt. Am gängigsten ist die unter anderem von Aleksander Brückner favorisierte Hypothese, dass der Name der Stadt auf die Genitivform des Eigennamens "Warsz" zurückgeht und folglich so viel wie „[Dorf/Gut] des Warsz“ bedeutet; allerdings ist dieser Vorname seinerseits kaum belegt und in seiner Herleitung unsicher, möglicherweise handelt es sich um eine Kurzform von Warcisław. Der mutmaßliche Eigentümer und Namensgeber der Siedlung dürfte im Bereich der heutigen Stadtviertel Solec und Mariensztat begütert gewesen sein und mag ein Angehöriger des Adelsgeschlechts der Rawa oder Rawicz gewesen sein. 

Andere Autoren vermuten einen baltischen Ursprung des Namens, wobei sich eine ganze Reihe Etyma zum Vergleich anbieten, etwa das Adjektiv *virš-ī’n- „oberer“ (vgl. litauisch "viršùs" „Höhe, Spitze, Gipfel“); Simas Karaliūnas' zufolge geht "Warszawa" vielmehr auf ein litauisches "Ãpvaršuva" zurück, das so viel wie „Ort, mit Gastungspflicht [gegenüber dem visitierenden König]“ bedeuten (vgl. litauisch "apvaišinti", „alle bewirten“) und auch den Namen des königlichen Guts Opvoišovo bei Pajūris in Tauroggen ergeben haben soll.

Die erste befestigte Siedlung auf dem Gebiet des heutigen Warschau war das Stare Bródno im 9. Jahrhundert. Hier befanden sich eine Wallburg und ein Dorf. Diese Siedlung wurde Ende des 11. Jahrhunderts aufgegeben. Eine neue befestigte Anlage entstand im 12. Jahrhundert in Jazdów auf dem Weichselkliff am linksseitigen Weichselufer. Diese kleine Anlage war einer der Sitze der masowischen Herzöge. Es wird vermutet, dass sie in der Gegend des heutigen Ujazdowski-Schlosses lag und 1262 von den Litauern unter Mindaugas zerstört wurde. In der Nähe wurden weitere kleinere Ansiedlungen in Kamion, Gocław und Solec angelegt. Sie alle lagen im von westslawischen Stämmen besiedelten Gebiet Masowien, das im 10. Jahrhundert von dem polnischen Herzog Mieszko I. aus der Dynastie der Piasten erobert wurde. Die wichtigste Stadt in Masowien war zu jener Zeit das ca. 100 Kilometer weichselabwärts gelegene Płock, das im 11. Jahrhundert für kurze Zeit die Hauptstadt Polens unter Władysław I. Herman war. Nach dem Tod von Bolesław III. Schiefmund wurde in Polen die Senioratsverfassung eingeführt, welche das Staatsgebiet unter den Söhnen Boleslaw III. Schiefmunds aufteilte und der jeweils Älteste das Seniorat über die Juniorherzöge hatte. Masowien wurde an dessen zweitältesten Sohn übergebe. Seit 1146 wurde Senior Bolesław IV., der die Linie der masowischen Piasten begründete und das Land von Płock aus regierte, Herr über diese Ländereien.

Die Aufteilung Polens in Senioratsherzogtümer 1188 schwächte das ganze Land, was zu zahlreichen Einfällen der Ruthenen und Litauer in Masowien führte. Daraufhin wurde der Handelsweg, der vom Schwarzen Meer bis zur Ostsee verlief, vom westlichen Bug auf das linke Weichselufer verlegt. Dies führte zu einer wirtschaftlichen Blüte der Warschauer Siedlung Jazdów, in der die masowischen Herzöge eine ihrer Burgen errichteten. Jazdów wurde jedoch 1262 von den Litauern zerstört, sodass die Bewohner ihre Siedlung drei Kilometer weiter nördlich auf dem Gebiet der heutigen Warschauer Altstadt errichteten. Der Herzog von Masowien Bolesław II. Mazowiecki gab die Burg in Jazdów (heute befindet sich dort das Ujazdowski-Schloss) ebenfalls auf und errichtete ein Schloss innerhalb der Altstadt (das heutige Warschauer Königsschloss). Sein Hauptsitz blieb aber Płock. Zwischen 1281 und 1321 wurde Warschau urkundlich öfters erwähnt. Die Lokationsurkunde ist aber nicht mehr erhalten. Im Jahr 1334 verlieh Trojden I. Warschau das Kulmer Stadtrecht und viele Kaufleute aus Thorn siedelten sich in der Stadt an. 1339 fand in Warschau ein bedeutender Rechtsstreit zwischen Kasimir III. von Polen und dem Deutschen Orden statt. Im Jahr 1356 wurde durch Siemowit III. das erste Kloster des Augustiner-Ordens in Warschau gegründet. Zu dieser Zeit entstanden die meisten Gebäude in der Altstadt, allen voran die gotische Johanneskathedrale und das Schloss der masowischen Herzöge, das spätere Königsschloss.

Mit der Wiedervereinigung Polens durch König Władysław I. Ellenlang 1320 wurde die Senioratsverfassung aufgehoben. Gleichwohl gehörte Masowien zu diesem Zeitpunkt nicht zu Polen, wurde jedoch um die Mitte des 14. Jahrhunderts polnisches Lehen. Es zerfiel weiter in die Einzelherzogtümer Płock, Rawa und Czersk. Warschau gehörte zu letzterem. Die Warschauer Altstadt wurde 1350 mit einem ersten und 1380 mit einem weiteren Mauerring umgeben. Um 1380 entstand nördlich der Altstadt ebenfalls am Weichselufer die Warschauer Neustadt, die 1408 das Kulmer Stadtrecht erhielt. Janusz I. Starszy verlegte 1413 die Hauptstadt des masowischen Herzogtums Czersk von Czersk nach Warschau. Nach der polnisch-litauischen Union von 1386 entwickelte sich Warschau dank der zentralen Lage zwischen den beiden Hauptstädten Krakau und Wilna sehr schnell. Insbesondere die Regierungszeit von Fürst Janusz I. von 1374 bis 1429 war eine erste Blütezeit Warschaus. Aus dieser Zeit sind mehrere gotische Gebäude und Kirchen in der Alt- und Neustadt erhalten geblieben, unter anderem das Portal des Bürgerhauses am Marktplatz der Altstadt mit der Hausnummer 21. 1454 wurden zur Regierungszeit von Bolesław IV. die St.-Anna-Kirche und das Bernhardinerkloster südlich des Krakauer Tors erbaut. Im Jahr 1469 bestätigten die masowischen Fürsten die Privilegien der jüdischen Gemeinde, die seit dem Anfang des 14. Jahrhunderts in Warschau existierte. Mit dem Aussterben der jeweiligen Piastenherzöge kamen Rawa im Jahr 1462, Płock 1496 und Czersk-Warschau 1526 direkt an Polen, wobei die letzten masowischen Fürsten wahrscheinlich 1524 (Stanislaus I.) und 1526 (Janusz III.) auf Geheiß der polnischen Königin Bona Sforza vergiftet wurden. Beide sind neben ihrem Lehrer Stanislaus aus Strzelec in der Johanneskathedrale bestattet. Die prächtigen Renaissancegrabplatten der beiden Fürsten und des Kanonikers wurden von ihrer Schwester Fürstin Anna Odrowąż gestiftet. Nach ihr ist auch die St.-Anna-Kirche benannt, da sie das Bernhardinerkloster großzügig unterstützte.

Mit dem Anschluss an Polen erhielt das Warschauer Bürgertum von Sigismund I. viele Handelsprivilegien, die die Entwicklung der Stadt beschleunigten. Mit der Wiedererlangung Danzigs und des Weichseldeltas 1466 durch Polen wurde die Weichsel der wichtigste polnische Handelsweg für den Export und Import nach und aus Westeuropa. Das an der Weichsel gelegene Warschau profitierte wirtschaftlich sehr davon. Nach dem Tod Sigismunds I. verlegte seine Witwe Bona Sforza 1548 ihren Hof vom Krakauer Wawel ins Schloss Ujazdowski in Warschau. Ihr Sohn Sigismund II. August regierte Polen-Litauen jedoch weiterhin von Krakau aus, obwohl er immer öfter in Warschau zu Gast war. 1569 und 1573 wurde in der Union von Lublin bzw. den Articuli Henriciani festgelegt, dass das polnische Parlament, der Sejm, in Warschau tagen und die Königswahl in Kamion bzw. Wola vor den Toren Warschaus stattfinden sollte. So wurden in Kamion Heinrich von Valois 1573 und in Wola 1574 Stephan Báthory und 1587 Sigismund III. Wasa zu polnisch-litauischen Königen gewählt. Auch die Abnahme des Lehnseides der preußischen Herzöge wurde seit Stephan Báthory vor der St.-Anna-Kirche in Warschau statt auf dem Krakauer Marktplatz vollzogen. Nach dem Brand des Krakauer Wawels 1596 entschloss sich Sigismund III. Wasa aus dem schwedischen Hause Wasa die Residenz der polnischen Könige nach Warschau zu verlegen, weil er zugleich König von Schweden war und Ambitionen auf den Moskauer Zarenthron hegte. Der etappenweise Umzug ging einher mit dem Ausbau des Sitzes der masowischen Herzöge zum polnischen Königsschloss ab 1598 durch italienische Baumeister. Nach der Rückkehr vom erfolgreichen Feldzug nach Moskau 1611 blieb Sigismund III. Wasa endgültig in Warschau. Gleichwohl blieb Krakau rechtlich weiterhin Hauptstadt, da es keinen Rechtsakt gab, der den Umzug legalisieren würde. Warschau stand bis 1795 nur der Titel des Königssitzes zu.
Mit der Erlangung der Rolle der Hauptstadt begann für Warschau eine Blütezeit im Frühbarock unter der Dynastie der Wasas, die bis zur Zerstörung der Stadt durch die Schweden 1655 andauerte. Nach dem Brand der Altstadt 1607 wurde diese im manieristischen Stil wieder aufgebaut. Warschau wuchs im 16. Jahrhundert weit über die mittelalterlichen Stadtmauern der Alt- und Neustadt hinaus und hatte über 50.000 Einwohner. Es entstanden neue Stadtviertel beiderseits der Weichsel. 1568 bis 1573 entstand die erste über 500 m lange Weichselbrücke auf 18 Pfeilern. Es war eine der längsten Brücken im damaligen Europa. Im Jahr 1648 erhielt Praga, der Stadtteil auf dem rechten Weichselufer, die Stadtrechte. Andere Stadtviertel (Jurydyka) waren Privateigentum einzelner Magnaten, reicher Adeliger "(Szlachta)", Geistlicher und Klöster. Sie waren von der städtischen Gerichtsbarkeit ausgenommen. Sie entstanden zahlreich um die neuen frühbarocken Paläste des Adels, der die Nähe zum Königshof suchend von Krakau herzog. Die Magnaten stifteten auch zahlreiche frühbarocke Kirchen und Klöster. 1597 kamen beispielsweise die Jesuiten nach Warschau. Sigismund III. Wasa ließ das Königsschloss das Schloss Ujazdowski und den Kazimierz-Palast im frühbarocken Stil um- und ausbauen. Entlang der Krakauer Vorstadt entstanden am Königsweg prachtvolle Paläste des Adels wie der Koniecpolski-Palast, der Potocki-Palast oder der Krasicki-Palast. Als eines der luxuriösesten Palais Europas galt der 1641 errichtete Ossoliński-Palast. 1637 eröffnete Władysław IV. das erste dauerhafte Theater im Königsschloss. Für seinen 1632 verstorbenen Vater Sigismund III. Wasa ließ er 1643 auf dem Schlossplatz die Sigismundssäule errichten, das erste profane Denkmal Warschaus. Ab 1661 erschien in Warschau die erste polnische Tageszeitung, der „Polnische Merkur“. In der ersten Hälfte des 17. Jahrhunderts war Warschau eines der führenden Zentren der frühen Neuzeit in Europa. In den Jahren 1655–1657, während des Zweiten Nordischen Krieges, wurde Warschau von den Schweden, Brandenburgern und Siebenbürgern zerstört. Die reichen Paläste wurden ausgeraubt und niedergebrannt und die geplünderten Kunstschätze und Bücherbestände nach Schweden verschifft. Die Verwüstungen waren so groß, dass diese Jahre als die "Schwedische Sintflut" in die Geschichte der Stadt eingingen und der 23. Juni als Festtag zur Erinnerung an den Rückzug der feindlichen Truppen 1657 begangen wurde. Aus der Wasa-Zeit sind gleichwohl wertvolle frühbarocke Baudenkmäler erhalten geblieben oder rekonstruiert worden, wie beispielsweise das Königsschloss, das Schloss Ujazdowski, die Jesuitenkirche und die Dominikanerkirche sowie der Gianotti-Palast in der Altstadt. Die Wasa-Zeit endete 1668 endgültig mit der Abdankung von Johann II. Kasimir.

Eine erneute Blütezeit begann für Warschau unter der Herrschaft Jan Sobieskis ab 1673, der vor allem als großzügiger Mäzen und Kunstliebhaber die südlichen Stadtteile ausbauen ließ. Er folgte dem nur fünf Jahre in Warschau regierenden Michael I. Korybut Wiśniowiecki. Ab 1677 baute Sobieski den Wilanów-Palast an dem nach Süden verlängerten Königsweg im Versailler Stil mit einem großen französischen Garten. 1687 stiftete er ebenfalls im Süden Warschaus die Antoni-Padewski-Kirche als Votum für den Sieg bei Wien 1683. Sobieski holte zwei der genialsten europäischen Baumeister des Hochbarock nach Warschau, Tylman van Gameren und Andreas Schlüter, sowie zahlreiche Künstler, unter denen vor allem Jan Reisner zu erwähnen ist. Sobieskis Ehefrau Marysieńka baute ab 1692 eines der damals weltweit größten Handelszentren, das Marywil, an dessen Stelle sich heute das Große Theater und der Theaterplatz befinden. In dieser Zeit entstanden unter der Anleitung von Tylman van Gameren auch der Krasiński-Palast, das Ostrogski-Palast, die Sakramentinnenkirche, die Kapuzinerkirche und die Karmeliterkirche. Für Marysieńka wurde außerhalb der Stadt auch das Palais Marymont errichtet, heute ein dicht besiedelter Wohndistrikt Warschaus.

Nach dem Tod von Sobieski 1696 und der Wahl Augusts II. 1697 begann die Sachsenzeit. Diese Zeit der sächsischen Könige begann für Warschau sehr unglücklich mit dem Großen Nordischen Krieg ab 1702. Während dieses Krieges und des später folgenden Polnischen Thronfolgekriegs ab 1734 wurde Warschau erneut von schwedischen bzw. russischen Truppen besetzt und zerstört. Zuletzt verflocht August III. Polen in den Siebenjährigen Krieg ab 1756. In der sächsischen Zeit unter August II. und August III. wurde ab 1713 die Sächsische Achse mit dem Sächsischen Palais, dem Brühlschen Palais und dem Sächsischen Park senkrecht zum Königsweg geschaffen. 1726 wurde der Sächsische Park für die Öffentlichkeit geöffnet. 1740 entstand das Collegium Nobilium, eine Aristokratenschule der Piaristen, aus der die Warschauer Universität hervorgehen sollte. 1748 wurde die Warschauer Oper eröffnet. Auch die Palais an der Senatorska- und der Miodowa-Straße (z. B. das von Ephraim Schröger erbaute Palais Lelewel) gehen auf die Sachsenzeit zurück. Die Wettiner holten hervorragende Dresdner Baumeister und Künstler, wie Johann Georg Plersch und Johann Sigmund Deybel, und die Mode für Meissner Porzellan an den Warschauer Hof. Diese gestalteten Warschau im Stil des Spätbarock und Rokoko um. 1732 führte August II. eine der größten Militärparaden auf den Czerniaków-Feldern vor Warschau zu Ehren seiner Tochter Anna Orzelska, für die er auch das Blaue Palais errichten ließ, durch. Aufgrund der großen Verwüstungen in den zahlreichen Kriegen ging die Sachsenzeit dennoch als eine der schwärzesten Perioden in die Warschauer Geschichte ein.

Zur erneuten Blütezeit kam es während der polnischen Aufklärung unter Stanislaus August Poniatowski ab 1764, der viele Warschauer Gebäude im klassizistischen Stil umbauen bzw. errichten ließ. Unter seiner Herrschaft wurde Warschau zu einem der bedeutendsten Zentren der Aufklärung und des Klassizismus in Europa. Er verlängerte den Königsweg südlich der Krakauer Vorstadt um die "Neue Welt" und gründete dort neue „Jurydykas“. Südlich des Schlosses Ujazdowski ließ er den Łazienki-Komplex mit vielen Gärten und Schlössern erbauen. Zahlreiche Paläste aus der Stanislaus-Zeit finden sich auch an der Długa- und der Senatorska-Straße. Unter seiner Zeit hatte Warschau mehr als 150.000 Einwohner und war damit eine der größten Städte in Europa. Er begann seine Regierungszeit sehr ambitioniert. Bereits im ersten Regierungsjahr 1765 gründete er die Ritterschule, die Münzerei und das Große Nationale Theater in Warschau unter der Regie von Wojciech Bogusławski. Seit 1770 wurde die Stadt neu geordnet und alle Straßen erhielten Straßennamen und Hausnummern. Ab 1772 wurde der Łazienki-Komplex umgebaut. Ab 1776 wurde die Altstadt durch eine weitere Weichselbrücke mit dem rechts der Weichsel gelegenen Stadtteil Praga verbunden. Ab 1773 hatte das erste Bildungsministerium der Welt, die Kommission für Nationale Erziehung, ihren Sitz in Warschau und zwischen 1788 und 1792 tagte im Warschauer Königsschloss der Große Sejm, der am 3. Mai 1791 die erste moderne Verfassung Europas verabschiedete. Dem war 1789 die sogenannte Schwarze Prozession vorausgegangen, in der die Städte mehr politische Mitspracherechte verlangten. Sie führte schließlich zur Einbeziehung der Stadtverfassung vom 20. April 1791 in die Verfassung vom 3. Mai 1791. Für Warschau hatte dies unter anderem zur Folge, dass die Jurydykas aufgehoben und dass eine einheitliche Stadtverwaltung eingeführt wurde. Daher ist auch der 21. April (der Tag, an dem Warschau die Städteverfassung ratifizierte) der Stadtfeiertag. Die Verfassung vom 3. Mai 1791 hatte zur Folge, dass russische und preußische Truppen 1792 Polen besetzten und das Land 1793 zum zweiten Mal geteilt wurde. Bei einem zunächst erfolgreichen Aufstand in Warschau im April 1794 unter der Führung des Schustermeisters Jan Kilińskis innerhalb des ganzpolnischen Kościuszko-Aufstandes, an dem die ganze Bevölkerung Warschaus regen Anteil nahm, wurde die russische Garnison vernichtet und mehr als 4000 russische Soldaten und Zivilisten getötet. Die russischen Truppen Suworows veranstalteten 1794 nach der Schlacht bei Warschau als Rache ein Massaker unter der Bevölkerung des rechtsufrigen Stadtteils Praga. Dabei kamen mehr als zehntausend Zivilisten ums Leben. 1795 wurde Polen zum dritten Mal geteilt. Nach der Abdankung Stanisław August Poniatowskis, der 1798 in Grodno verstarb, wurde Warschau 1796 von preußischen Truppen besetzt und für 11 Jahre Sitz der neuen preußischen Provinz Südpreußen, die Warschau, Posen und Kalisz umfasste. Die Bevölkerungszahl sank rapide auf 115.000 Einwohner 1806 und die wirtschaftliche Lage verschlechterte sich. 1800 gründete Stanisław Staszic die Polnische Akademie der Wissenschaften "(Polska Akademia Nauk)" in Warschau, die bis heute ihren Sitz im Staszic-Palast an der Krakauer Vorstadt hat.

Nach dem Frieden von Tilsit wurde 1807 aus den beiden preußischen Teilungsgebieten von 1793 und 1795 – mit Ausnahme von Danzig – das Herzogtum Warschau mit Warschau als Hauptstadt gebildet. 1807 erhielt es eine neue liberale Verfassung und der polnische Sejm wurde nach zwölf Jahren Unterbrechung wieder in Warschau einberufen. Im selben Jahr trat ein der polnischen Rechtstradition angepasster Code Napoléon "(Code civil)" in Kraft, eines der ersten zusammengefassten Zivilrechtsbücher Europas. Als Grundlage für den 1964 erlassenen "Kodeks Cywilny" prägt der Code Civil bis heute die polnische Zivilrechtsordnung. Der Neffe des letzten polnischen Königs, General Józef Antoni Poniatowski, baute die polnische Armee in Warschau wieder auf, die bald schon 200.000 Mann zählte. Im Fünften Koalitionskrieg wurde das Herzogtum von Österreich angegriffen. Anfänglich unterlegen (→ Schlacht von Raszyn), konnte das Herzogtum sich gegen die Habsburger behaupten und erhielt im Frieden von Schönbrunn mit Westgalizien und Krakau die Gebiete, die bei der Dritten Polnischen Teilung an Österreich gefallen waren. Truppen des Herzogtums beteiligten sich 1812 am Russlandfeldzug Napoleons sowie im Jahr darauf an der Völkerschlacht bei Leipzig, in der Józef Poniatowski den Tod in der Weißen Elster fand. Er wurde danach in der Wawel-Kathedrale in Krakau feierlich als Nationalheld bestattet. Poniatowski wurde zur Symbolfigur des Herzogtum Warschaus, obwohl er nicht Herzog war, sondern nur der Oberbefehlshaber der Armee. Die Warschauer gaben ihm bereits Anfang des 19. Jahrhunderts ein Denkmal in Auftrag, das vom berühmten dänischen Bildhauer des Klassizismus Bertel Thorvaldsen angefertigt wurde. Im Zuge der Neuordnung Europas 1814/15 durch den Wiener Kongress wurde das Herzogtum Warschau aufgehoben.

Nach dem Wiener Kongress wurde Warschau die Hauptstadt des Königreiches Polen (Kongresspolen), das vom russischen Zaren in Personalunion mit Russland regiert wurde. Es erhielt 1817 eine relativ liberale Verfassung, und der polnische Sejm in Warschau hatte weitgehende Befugnisse. Die Exekutivmacht lag beim Statthalter Großfürst Konstantin, dem Bruder des Zaren Alexander I.

1816 wurde auf Grundlage des Collegium Nobilium von 1740 die Warschauer Universität, ein Jahr später die Warschauer Wertpapierbörse als erste moderne polnische Wertpapierbörse (1818 wurde eine Wertpapierbörse in Krakau errichtet) gegründet. Bereits seit dem 17. Jahrhundert ist der börsenmäßige Handel mit Wertpapieren (vor allem Wechsel) in Warschau belegt. Auch in anderen polnischen Handelsstädten (Danzig, Krakau, Posen, Lemberg etc.) bestand unregelmäßiger Börsenhandel seit dem Mittelalter, der von niederländischen und italienischen Kaufleuten nach Polen gebracht wurde. Doch die erste Börse mit einer öffentlich-rechtlichen Börsenordnung war die besagte Warschauer Wertpapierbörse von 1817.

In dieser Zeit setzte auch die Industrialisierung in Warschau ein, und die ersten großen Fabriken entstanden in der Stadt. Außerhalb der Tore wurde 1792 der Powązki-Friedhof angelegt, eine der größten und schönsten Nekropolen des 19. Jahrhunderts, und 1825 wurde unter Führung von Antonio Corazzi mit dem Bau des Großen Theaters, des damals größten in Europa, begonnen. Hier spielten unter anderem Helena Modrzejewska und Pola Negri.

In den 1810/20ern lebte und konzertierte der junge Frédéric Chopin in Warschau, der in der Nähe der Stadt in den Gutshof der Familie seiner Mutter Żelazowa Wola geboren wurde. Bereits zu Anfang der 1820er Jahre wurde klar, dass der Zar sich nicht an die Verfassung halten würde und autokratisch über seinen Statthalter zu regieren gedachte. Dies änderte sich auch nach dem Dekabristenaufstand in Russland 1825 nicht.

1830 wurde bekannt, dass der Zar polnische Truppen gegen die Revolutionäre in Belgien einsetzen wollte. So brach am 30. November 1830 mit der Erstürmung des Belvedere-Palastes "(Belweder)" in Warschau durch Aufständische der Novemberaufstand los. Der Großfürst Konstantin musste nach wenigen Tagen aus der Stadt fliehen, und der polnische Sejm setzte den Zaren als polnischen König ab. Der Aufstand hatte in den ersten Monaten Erfolg, und die russischen Truppen mussten Warschau und das Umland räumen. Nach über einem Jahr Krieg mussten jedoch die Aufständischen kapitulieren. Mit der großen Emigration flohen ca. 30.000 Warschauer und andere Kongresspolen nach Westeuropa und in die Vereinigten Staaten. Zu ihnen gehörten unter anderem Frédéric Chopin und Adam Mickiewicz.

1832 wurden die Verfassung und der Sejm aufgehoben, und es begann eine Zeit der politischen Repressalien. Im selben Jahr wurde als Antwort auf den Novemberaufstand nördlich der Neustadt die Zitadelle, die auch ein Gefängnis für politische Gefangene enthielt, errichtet. In der sich anschließenden romantischen Epoche wurde Warschau ausgebaut.

Seit 1840 erreichte die Eisenbahn Warschau, und bald war eine erste Verbindung nach Wien fertiggestellt. Während des Völkerfrühlings 1848 blieb es in Warschau, anders als in den preußischen und österreichischen Teilungsgebieten, relativ ruhig, denn die Verschwörer, die einen gesamtpolnischen Aufstand planten, wurden zuvor verhaftet. In dieser Zeit wurde auch die Textilindustriestadt Łódź etwa 80 km südwestlich von Warschau in Kongresspolen an der Eisenbahnstrecke nach Wien aufgebaut und stieg bald zu einer der führenden Industriemetropolen Europas auf.

Im Januar 1863 brach der Januaraufstand gegen das Zarenregime los. In einem Partisanenkrieg konnten die Warschauer zwei Jahre lang Widerstand leisten, bis sie Ende 1864 aufgeben mussten. Das Königreich Polen wurde endgültig aufgelöst und Russland einverleibt. Somit wurde Warschau nach Moskau und St. Petersburg die drittgrößte Stadt im Zarenreich. Der Wegfall der Zollgrenze zu Russland brachte einen rasanten Wirtschaftsaufschwung, der bis zum Ersten Weltkrieg andauerte.

Das wirtschaftliche Zentrum der Stadt verlagerte sich vom Königsweg auf die westlich von ihm gelegene prächtige Marszałkowska-Straße. 1866 fuhr die erste von Pferden gezogene und 1908 die erste elektrische Straßenbahn in Warschau. Hier entstanden zahlreiche repräsentative Miets- und Handelsgebäude sowie Kultureinrichtungen im Stil des Historismus, der Sezession und des Eklektizismus.

Nach den Zerstörungen des Zweiten Weltkriegs ist dieser Teil der Stadtgeschichte vollständig verloren gegangen. Reste der historischen Bebauung des 19. Jahrhunderts findet man in der Lwowska-Straße und teilweise in den Ujazdów- und Jerusalemer-Alleen.

Ab 1881 wurde ein modernes Kanalisationssystem gebaut. Ende des 19. Jahrhunderts entstanden die beiden Fortgürtel der Warschauer Festung. 1900 errichtete man das prächtige Gebäude der Warschauer Philharmonie im Jugendstil, in der in der ersten Hälfte des 20. Jahrhunderts Ignacy Paderewski und Jan Kiepura auftraten. 1867 wurde die doppelte und erste weibliche Nobelpreisträgerin Marie Skłodowska-Curie in der Neustadt geboren. Der Zeit des Warschauer Positivismus hat vor allem der Schriftsteller Bolesław Prus in seinen dem Realismus treuen Romanen ein Denkmal gesetzt. Allen voran ist hier der Roman „Lalka“ zu nennen, in dem Prus den Werdegang und den Fall eines Warschauer Unternehmers beschreibt. Ein anderer Vertreter des Warschauer Positivismus, Henryk Sienkiewicz, erhielt 1905 den Literaturnobelpreis. Er wurde später in einer Krypta der Warschauer Kathedrale bestattet. Auch Teodor Józef Korzeniowski (Pseudonym Joseph Conrad) wohnte im 19. Jahrhundert in Warschau "(Nowy Świat 47)". Im selben Jahr fand als Reaktion auf den verlorenen Krieg Russlands gegen Japan und den Blutsonntag in St. Petersburg eine kurze sozialistische Erhebung statt, die von Rosa Luxemburg, die aus einer jüdischen Familie in Zamość (südliches Kongresspolen) stammte und in Warschau aufgewachsen war, mitorganisiert wurde. Um die Jahrhundertwende waren 36 % der Warschauer Wohnbevölkerung Juden. 1909 waren 36,9 % Juden, 2,4 % Protestanten und 0,4 % Mariaviten. Ein Jahr nach Beginn des Ersten Weltkriegs wurde Warschau von deutschen Truppen besetzt. Ein vorläufiger Regierungsrat eines von Deutschland und Österreich abhängigen Satellitenstaates wurde in Warschau eingesetzt. Im selben Jahr wurde die Warschauer Universität wiedereröffnet. Nach der Oktoberrevolution in Russland musste dieses im Friedensvertrag von Brest-Litowsk auf die Gebiete aus den Teilungen Polens im 18. Jahrhundert verzichten.

Im Ersten Weltkrieg nahmen deutsche Truppen am 5. August 1915 Warschau ein. Die russische Armee hatte bei ihrem Abzug noch strategische Gebäude und Brücken niedergebrannt. Warschau wurde unter der deutschen Okkupation Hauptstadt des bis 1918 bestehenden Generalgouvernements Warschau mit Hans von Beseler als Generalgouverneur und Ernst Reinhold Gerhard von Glasenapp als Polizeipräsident. Das Deutsche Reich und Österreich-Ungarn errichteten ein provisorisches Königreich mit der Hauptstadt in Warschau und stellten der polnischen Bevölkerung ein unabhängiges Polen in Aussicht. Am 1. und 2. Mai 1916 fand in der unter deutscher Verwaltung stehenden Stadt ein hauptsächlich von Militärärzten besuchter außerordentlicher Kongress der Inneren Medizin statt.

Nach der Niederlage der Mittelmächte 1918 unterstützten die Westmächte diesen Plan.
Die deutsch-österreichischen Truppen in Warschau wurden entwaffnet. Der Tag, an dem Marschall Piłsudski in Warschau eintraf, der 11. November 1918, gilt als der Unabhängigkeitstag Polens und Beginn der Zweiten Polnischen Republik.

1920 drohte die Rote Armee, Warschau im polnisch-sowjetischen Krieg einzunehmen. Sie konnte jedoch durch Marschall Piłsudski durch das "Wunder an der Weichsel" vom 13.–16. August 1920 zurückgedrängt werden. Seit dieser Zeit war Warschau wieder die Hauptstadt des erneut unabhängigen Polens. An der Aleje Ujazdowskie (seitdem Regierungs- und Botschaftsviertel) wurden in den 1920er Jahren ein neues Sejm-Gebäude sowie verschiedene Ministerienpaläste und Botschaften errichtet. In derselben Zeit wurde auf dem Pole Mokotowskie der erste polnische Flughafen eingeweiht.

In der Zwischenkriegszeit erlebte Warschau einen neuen Bauboom, und das kulturelle Leben blühte auf. In dieser Zeit wirkte hier u. a. der spätere Nobelpreisträger Czesław Miłosz. Die Warschauer Bohème dieser Zeit ist unter anderem in den Bildern von Józef Rapacki festgehalten worden. 1926 kam es bei dem Piłsudski-Maiputsch zu Straßenkämpfen in Warschau, die auf der Poniatowski-Brücke anfingen. Nachdem jedoch ziemlich früh klar wurde, dass die amtierende Regierung unter Stanisław Wojciechowski weder in der Armee noch in der Stadtbevölkerung einen Rückhalt hatte, gab sie nach zwei Tagen auf.

Unter Stadtpräsident Stefan Starzyński (seit 1934 im Amt) erlebte Warschau eine kulturelle Blütezeit. Warschaus Flughafen Okęcie erhielt nationale und internationale dauerhafte Flugverbindungen. Das Straßenbahn- und Busnetz wurde ausgebaut, und neue Straßenzüge in den Außenbezirken entstanden. 1939 hatte Warschau bereits über 1.350.000 Einwohner.

Im September 1939 begann mit dem deutschen Überfall auf Polen der Zweite Weltkrieg. Im Verlauf des Konflikts wurde die Stadt Warschau Zentrum erbitterter Kämpfe. Die Reste der in der Schlacht an der Bzura geschlagenen polnischen Armee verschanzten sich im Stadtgebiet und verteidigten dieses zäh. Kurz darauf wurde Warschau von deutschen Truppen vollständig eingeschlossen, nachdem diese die Front am Narew durchbrochen hatten und nun auch im Osten vor Warschau standen. Während dieser schweren Kämpfe wurde das Stadtgebiet von Warschau, insbesondere die Außenbezirke, sowohl von deutscher Artillerie beschossen als auch aus der Luft bombardiert. Hierbei fanden weit mehr als zehntausend Zivilisten den Tod. Bei der deutschen Belagerung von Warschau verlor die Stadt rund 10 % ihrer Bebauung, das zerbombte Warschauer Königsschloss brannte aus. Am 28. September 1939 musste Warschau kapitulieren und wurde von deutschen Truppen besetzt. Der Stadtpräsident Stefan Starzyński wurde von der Gestapo verhaftet und 1943 im KZ Dachau ermordet.

Vier Wochen nach Beginn des Polenfeldzugs marschierten am 28. September 1939 Truppen der Wehrmacht in Warschau ein und eine über fünfjährige verheerende Besatzungszeit brach an. Der Terror der Besatzer traf von Anfang an auf einen entschiedenen Widerstand weiter Teile der Bevölkerung. Der organisierte Widerstand nahm verschiedene Formen an, von geheimen Bildungseinrichtungen sowie kleinen und großen Sabotagen bis zu Attentaten. Warschau war von Beginn der Besatzung an das Zentrum des Polnischen Untergrundstaates mit der Geheimverwaltung der Londoner Exilregierung und der Heimatarmee.

Am 31. Juli 1944 erreichte im Zuge der Operation Bagration die Rote Armee den Warschauer Stadtteil Praga. In den folgenden Monaten bildete die Weichsel die Frontlinie, die östlichen Teile der Stadt standen unter Kontrolle der Roten Armee. Im Rahmen der Weichsel-Oder-Operation konnte erst über fünf Monate später am 17. Januar 1945 auch der westliche Teil Warschaus eingenommen werden.

Nach der Besetzung Warschaus durch die deutsche Wehrmacht wurden ab November 1940 die Juden der Stadt und der Umgebung im Warschauer Ghetto eingesperrt. Es war das größte jüdische Ghetto im besetzten Europa. Mindestens 300.000 jüdische Bürger Warschaus wurden von dort deportiert und ermordet. 1941 wurde für Juden die Ghettopflicht eingeführt und das Verlassen des Ghettos sowie jegliche Hilfe für die jüdischen Warschauer unter Todesstrafe gesetzt. Gleichwohl konnten die Organisation Żegota sowie viele Privatleute hunderte Juden vor dem Tod retten.

Am 18. April 1943 kam es zum Aufstand im Warschauer Ghetto unter der Führung von Mordechaj Anielewicz und Marek Edelman als Reaktion auf die Liquidierung des Ghettos durch die Waffen-SS. Am 8. Mai 1943 nahmen sich die meisten jüdischen Anführer im Versteck in der Miła-Straße 18 das Leben. Einigen jüdischen Einheiten, unter anderem Marek Edelman, gelang die Flucht zum polnischen Untergrund. Nach dem Ghettoaufstand wurden von der SS am 16. Mai 1943 die Große Synagoge in Warschau zerstört, ein ganzes Stadtviertel niedergebrannt und die meisten der überlebenden Juden im KZ Treblinka ermordet. International wurde dieser Widerstand gegen die deutsche Besatzung registriert.

Der Warschauer Aufstand, getragen von der polnischen Heimatarmee, begann am 1. August 1944 unter der Führung von Tadeusz Bór-Komorowski. Er war die größte Erhebung gegen die Okkupanten im besetzten Europa während des Zweiten Weltkrieges. Fast die gesamte verbliebene Stadtbevölkerung beteiligte sich an den Kriegshandlungen, deren Ziel ein von Nazideutschland und der Sowjetunion unabhängiges Polen sein sollte. In den ersten Augusttagen wurden die polnischen Medien und eine Pfadfinderpost wiederhergestellt und die Erhebung hatte zunächst Erfolg, als sich die deutschen Truppen aus weiten Teilen der Innenstadt zurückziehen mussten. Aufgrund mangelnden Nachschubs jedweder Form kam der Aufstand schnell in eine kritische Situation. Die der Wehrmacht zu diesem Zeitpunkt weit überlegene Rote Armee war am rechten Weichselufer stehen geblieben und leistete dem Widerstand keine Unterstützung. Außerdem verweigerten die Sowjets den Westalliierten Flugplätze, von denen aus sie mehr Hilfsgüter und Waffen hätten einfliegen können. Die Heimatarmee musste am 2. Oktober 1944 kapitulieren. Im Warschauer Aufstand, der überwiegend durch Einheiten der Waffen-SS niedergeschlagen wurde, kamen fast 200.000 polnische Soldaten und Zivilisten ums Leben. Als Repressalie wurde die Mehrzahl der noch vorhandenen Gebäude auf dem linken Weichselufer von den deutschen Truppen planmäßig gesprengt und Warschau weitgehend zerstört. Die überlebende Bevölkerung wurde in Konzentrationslager oder zur Zwangsarbeit deportiert.

Am 17. Januar 1945 marschierte die Rote Armee in eine Ruinenstadt ohne Einwohner ein. Die befreite Bevölkerung kam zum großen Teil nach Warschau zurück. Den Soldaten der Heimatarmee blieb jedoch eine Rückkehr verwehrt. Viele mussten emigrieren. Die Stadtverwaltung wurde von der sich bildenden kommunistischen Partei "(Polska Partia Robotnicza)" eingesetzt. Bald wurde der Beschluss gefasst, Warschau detailgetreu wieder aufzubauen. 1945 wurde ein Fonds für den Wiederaufbau Warschaus gegründet. Im Februar 1945 begann eine Kommission unter der Leitung von Roman Piotrowski erste Rekonstruktionsarbeiten. Die Altstadt, die Neustadt und die Krakauer Vorstadt wurden ab 1946 bis 1953 in einer historischen Rekonstruktion wieder aufgebaut (1980 als Weltkulturerbe der UNESCO ausgezeichnet). Die Aufbauarbeiten stellen bis heute weltweit die größte geplante Rekonstruktion einer Bebauung dar. Gleichzeitig wurde auch die Bebauung der Straßenzüge "Miodowa", "Długa" und "Senatorska" sowie der Plätze "Teatralny" und "Bankowy" rekonstruiert. Die Arbeiten orientierten sich dabei zu einem großen Teil an Gemälden des italienischen Malers Bernardo Bellotto (Canaletto), der im 18. Jahrhundert viele Stadtpanoramen Warschaus geschaffen hatte. 

Von 1947 bis 1949 wurde unter Teilen der Altstadt der Tunnel der „Ost-West-Arterie“ gebaut. Im Jahre 1971 entstand ein Komitee zum Wiederaufbau des Warschauer Königsschlosses unter der Leitung von Stanisław Lorentz. Dieser Wiederaufbau wurde in den 1970er und 1980er Jahren durchgeführt. 

Mehrere Stadtteile entstanden im realsozialistischen Stil. Von 1952 bis 1955 wurde der Warschauer Kulturpalast errichtet, das damals zweithöchste Gebäude Europas. Daneben wurden die Stadtviertel Marienstadt und Marszałkowska Dzielnica Mieszkaniowa "(MDM)" im realsozialistischen Stil (wieder) errichtet. Die Blütezeit dieser Stilrichtung in Polen datiert auf die Jahre von 1949 bis 1955 und verschmolz in ihrer Warschauer Abwandlung mit der polnischen Architektur der 1930er Jahre, die wiederum stark von dem Warschauer Klassizismus beeinflusst war. Lange Zeit verschmäht, wird die Architektur des Sozrealismus in jüngster Zeit allmählich wiederentdeckt. Stadtansichten dieses Stils malte unter anderem Helena Krajewska.

Im Mai 1955 wurde in Warschau unter dem Diktat der Sowjetunion das Militärbündnis Warschauer Pakt gegründet. Im selben Jahr fand hier das Weltjugendfestival statt. Als Reaktion auf den Polnischen Oktober 1956 wurde Władysław Gomułka zum Ersten Parteisekretär ernannt; die Zeit des Stalinismus ging zu Ende (siehe auch hier). Gomułka hielt im selben Jahr am Defiladenplatz eine Rede vor über einer Million Menschen, die den ersehnten Umbruch einläuten sollte. Im März 1968 kam es zu einer Studentenrevolte, die von dem Verbot der Aufführung des Theaterstücks "Totenfeier" von Adam Mickiewicz in Warschau ausgelöst wurde. Dies war der Beginn des Endes der Ära Gomułka, der im Dezember 1970 nach einem Arbeiteraufstand von Edward Gierek abgelöst wurde.
Neben dem Brief der polnischen Bischöfe an ihre deutschen Amtskollegen 1965 war Willy Brandts Kniefall von Warschau am 7. Dezember 1970 vor dem Mahnmal für den Ghettoaufstand 1943 einer der wichtigsten Eckpfeiler für die deutsch-polnische Aussöhnung. 1976 wurde in Warschau das KOR "(Komitee zur Verteidigung der Arbeiter)" gegründet, aus dem später die Gewerkschaft Solidarność hervorgehen sollte, die am 10. November 1980 beim Woiwodschaftsgericht in Warschau registriert wurde. Entscheidende Bedeutung für den Untergang des Kommunismus hatte der Papstbesuch Johannes Paul II. am 2. Juni 1979 in Warschau, der mitursächlich für die Gründung der ersten unabhängigen Gewerkschaft im Ostblock war, sowie seine Messe vor über einer Million Warschauern auf dem Defiladenplatz im Jahr 1987. Mit der Ausrufung des Kriegsrechts durch General Jaruzelski am 13. Dezember 1981 wurde Warschau von motorisierten Spezialeinheiten (ZOMO) mit Panzern und schwerem Kriegsgerät besetzt.
Nach der Solidarność-Bewegung in den 1980er Jahren kam es Februar bis April 1989 zu den Gesprächen am Runden Tisch in Warschau; als ein Ergebnis daraus wurden die ersten (fast) freien Wahlen in einem Warschauer-Pakt-Staat eingeleitet (4. und 18. Juni 1989 Parlamentswahlen; Präsidentschaftswahl 19. Juli 1989).

Mit dem Gesetz über den Warschauer Verwaltungsaufbau vom 18. Mai 1990 wurde die Warschauer Selbstverwaltung wiedereingeführt und am 27. Mai 1990 wurde nach über 50 Jahren wieder ein Stadtparlament gewählt. Zum Präsidenten von Warschau wurde Stanisław Wyganowski gewählt, der bereits seit Januar 1990 diese Funktion vorläufig innehatte. Am 7. April 1991 wurde nach einem halben Jahrhundert die Warschauer Wertpapierbörse als zweite Kapitalmarktinstitution dieser Art in einem ehemaligen Ostblockstaat wiedereröffnet, die sich in den folgenden Jahren zu der führenden Börse in Ostmitteleuropa entwickelte. Sie bekam – was durchaus auch Symbolcharakter hatte – ihren Sitz in dem Gebäude der ehemaligen Polnischen Vereinigten Arbeiterpartei und zog später in ein neuerrichtetes Gebäude an der Aleje Ujazdowskie. 1994 wurden elf Stadtteile aus dem Stadtgebiet gebildet und 1995 wurde das erste Teilstück der U-Bahn in Betrieb genommen. 2002 wurde das Gesetz über den Warschauer Verwaltungsaufbau modernisiert, so dass Warschau wieder eine einheitliche Gemeinde der Woiwodschaft Masowien mit 18 Untereinheiten wurde. In den 1990er Jahren entstanden viele moderne Wolkenkratzer und Bürogebäude im Zentrum und Stadtteil Wola und Warschau wurde zum führenden Finanzzentrum in Ostmitteleuropa.

Der in der Volksrepublik begonnene Wiederaufbau Warschaus dauert auch heute noch an. In den nächsten Jahrzehnten sollen die Königsgärten rekonstruiert werden und der Brühlsche und Sächsische Palast wiedererrichtet werden. Gleichwohl werden die allermeisten Gebäude des alten Warschaus nicht mehr neu entstehen können. Die heutigen Straßenzüge verlaufen weitgehend anders als vor 1939. Die reiche Sezessionsarchitektur der Marszałkowska-Straße und der Jerusalemer Alleen ist unwiederbringlich verloren.

Wie andere Zentren Mitteleuropas auch, profitiert Warschau von der Wende 1989. Die Stadt beansprucht den Titel "größte Baustelle Europas", denn in der Innenstadt sind in den letzten Jahren viele Läden, Einkaufszentren, Bürohochhäuser und Freizeitmöglichkeiten geschaffen worden. Warschau hat sein Blockbauten-Image abgelegt und ist nun neben Frankfurt, London, Rotterdam und Paris die „höchste“ Stadt Europas.

Warschau ist der größte Investitionsschwerpunkt in Polen. In der Stadt entstehen neue Bürohochhäuser, wie beispielsweise der 1999 fertiggestellte 208 Meter hohe Warsaw Trade Tower oder der 2016 eröffnete Warsaw Spire mit einer Höhe von 220 Metern. Beide machen dem 237 Meter hohem Kulturpalast (erbaut 1955) in der Skyline den Platz streitig. 2016 begann zudem der Bau des voraussichtlich höchsten Gebäudes der Europäischen Union, dem Varso Tower, der 310 Meter hoch sein wird.

Warschau ist Sitz verschiedener Hochschulen, darunter der Warschauer Universität und der Kardinal-Stefan-Wyszyński-Universität. Zudem ist Warschau seit 2005 Sitz der Europäischen Agentur für die operative Zusammenarbeit an den Außengrenzen (FRONTEX).

Den politischen Aufbau der Stadtverwaltung regelt das Gesetz über die Verwaltungsstruktur der Hauptstadt Warschau vom 15. März 2002 Das ist der dritte Gesetzestext dieser Art nach den Beschlüssen von 1990 und 1994. Das Gesetz von 2002 hat unter anderem folgendes festgelegt:

Der Stadtpräsident oder die Stadtpräsidentin ist das Exekutivorgan der polnischen Hauptstadt. Ihm oder ihr unterstehen die Beschäftigten der Stadtverwaltung, der Leiter der städtischen Organisationseinheiten wie auch anderer Diensteinheiten wie zum Beispiel die Warschauer Feuerwehr. Der Präsident oder die Präsidentin wird in allgemeiner, geheimer, gleicher und unmittelbarer Wahl von den Bewohnern der Stadt gewählt. Zur gleichen Zeit finden die Kommunalwahlen statt. Vor der Reform von 2002 hat der Stadtrat den Präsidenten oder die Präsidentin gewählt.

Der Präsident/die Präsidentin beruft die wichtigsten Amtsträger der Stadt, darunter seine eigenen Stellvertreter, die für bestimmte Sachbereiche zuständig sind und mit dem Präsidenten/der Präsidentin das Stadtmagistrat bilden "(zarząd miasta)". Zusätzlich wirkt er/sie bei der Berufung der Bezirksbürgermeister mit.

Der Stadtrat besteht aus 60 Abgeordneten und wird alle vier Jahre gewählt. Seine Hauptaufgabe besteht zum einen darin, die Hauptsatzung der Stadt zu verabschieden und zum anderen, den Haushaltsplan zu beschließen und dessen Einhaltung zu kontrollieren.

"Hauptsatzung der Stadt Warschau:"

Der Stadtrat ist vor der Verabschiedung der Hauptsatzung der Stadt Warschau verpflichtet, die Meinungen der einzelnen Bezirksräte einzuholen. Sollte sich ein Bezirksrat innerhalb von 14 Tagen nicht äußern, gilt die Einholung der Meinung als erfolgt.

Zusätzlich muss der Satzungsentwurf mit dem Premierminister/der Premierministerin abgesprochen werden, was spätestens 30 Tage nach Einbringung der Sache passieren muss. Sollte der Premierminister/die Premierministerin nach 30 Tagen keine Handlung vorgenommen haben, gilt die Besprechung als erfolgt.

Das aktuell gültige Wappen wurde im Rahmen eines Wettbewerbs im Jahre 1938 beschlossen. Der Autor schöpfte aus einer historischen Darstellung des Jahres 1390.

Blasonierung: Auf rotem Grund ist die Figur einer Frau mit einem Fischschwanz und erhobenem Schwert in der rechten Hand und einem runden Schild in der linken Hand zu sehen. Über dem Wappen befindet sich eine goldene Königskrone. Die Haare der Seejungfer, des Schildes, des Schwertes und der Krone sind goldfarben.

In Warschau gibt es etwa 30 das ganze Jahr durch arbeitende Theater. Die beiden wichtigsten sind das 1765 gegründete Nationaltheater "(Teatr Narodowy)" sowie die Staatsoper "Opera Narodowa" im Teatr Wielki (dt. "Großes Theater") von 1833, die die lange Warschauer Theatertradition verkörpern. Darüber hinaus sind die wichtigsten Schauspielhäuser: Teatr Studio, Teatr Polski, Teatr Rozmaitości und Teatr Ateneum. Populäre Musicals wie Phantom of the Opera werden im Teatr Roma gespielt.

Berühmte Warschauer Theaterregisseure: Jerzy Grzegorzewski, Grzegorz Jarzyna, Adam Hanuszkiewicz.

Berühmte Warschauer Theaterschauspieler: Gustaw Holoubek, Daniel Olbrychski, Zbigniew Zapasiewicz, Krystyna Janda, Andrzej Seweryn.

In Warschau gibt es zahlreiche staatliche wie auch private Museen. Die meisten wurden während des Zweiten Weltkriegs zerstört und in der Nachkriegszeit wiederaufgebaut, wobei große Teile der Museumsbestände dem Krieg zum Opfer fielen und bis heute verschollen sind. Nachfolgend eine kleine Liste ausgewählter Museen.

Die wichtigsten Galerien mit wechselnden und dauerhaften Ausstellungen sind die Galeria Zachęta, das Zentrum für Zeitgenössische Kunst "(Centrum Sztuki Współczesnej)" im Schloss Ujazdowski, Galerie Foksal, Galerie XXI, Galerie Test, Galerie Zapiecek und die Galerie Kordegarda.

Seit 1927 wird in der Warschauer Nationalphilharmonie der Internationale Chopin-Wettbewerb veranstaltet.

Die Warschauer Altstadt wurde 1980 als UNESCO-Weltkulturerbe ausgezeichnet. Der repräsentativste Platz der Altstadt ist der große dreieckige Schlossplatz. Ursprünglich wurde seine Südseite durch das Krakauer Tor begrenzt, von dem nur noch eine gotische Brücke übrig geblieben ist. Die Ostseite bildet die Westfassade des Königsschlosses "(Zamek Królewski)", die Nordwestseite des Schlossplatzes wird von der Häuserfront der Altstadt eingenommen. Unterhalb des Schlossplatzes verläuft seit 1949 der Tunnel der Trasa W-Z. In der Mitte des Platzes steht die 1643/44 aufgestellte Sigismundssäule "(Kolumna Zygmunta)".

Das jetzige frühbarocke Gebäude mit dem 60 m hohen Uhrturm stammt aus den Jahren 1598–1619, der Flügel zur Weichsel dagegen aus dem Rokoko. Im Inneren dominieren Elemente des Klassizismus. Das Schloss brannte 1939 während der Bombardierung Warschaus aus und wurde 1944 von der Wehrmacht gesprengt. Nach dem Krieg blieb die Ruine jahrzehntelang stehen. Sie wurde 1971–1988 aus Spendenmitteln originalgetreu rekonstruiert.

Östlich unterhalb des Schlossplatzes neben dem Königsschloss befindet sich der spätbarocke Palast unter dem Blechdach, der nach dem Material seines Daches benannt wurde. Er wurde 1698–1701 als Stadtresidenz der Lubomirskis errichtet. Später diente er als Residenz des Königs Stanislaus II. August. Der Palast wurde nach dem Warschauer Aufstand niedergebrannt und 1948–1949 wiederaufgebaut.

Historischer und geografischer Mittelpunkt der Altstadt ist der Marktplatz "(Rynek)", in dessen Mitte die Skulptur der Flussjungfrau Syrenka steht. Seit dem Mittelalter befand sich in der Mitte des Platzes das Rathaus, das 1817 abgerissen wurde. Der Marktplatz wurde im 13. Jahrhundert auf einer Fläche von 90 × 73 Meter angelegt. Um ihn herum wohnten vor allem Kaufleute und Handwerker. Später siedelten sich auch viele Künstler hier an. Der Großteil der Patrizierhäuser wurde nach dem Stadtbrand von 1607 wiederaufgebaut, wobei sich gotische Elemente vor allem in den Fundamenten erhalten konnten. Die Sgraffitomalereien und die Polychromien an einigen Häusern wurden nach der Zerstörung im Zweiten Weltkrieg von Jan Seweryn Sokołowski rekonstruiert. Die Ostseite, auch Brass-Seite genannt, war am stärksten zerstört, so dass einer Rekonstruktion der Abriss einiger Fundamente vorausgehen musste. Dagegen blieb auf der Nordseite, der Dekert-Seite, die meiste ursprüngliche Bausubstanz erhalten. In der ganzen Häuserzeile befindet sich das Historische Museum der Stadt Warschau. Besonders treten hier das "Baryczka-Haus" und das "Haus zum Negerlein" hervor, dessen Renaissancefassade von Santi Gucci stammt. Die West- oder Kołątaj-Seite besticht durch eine Neorenaissancewanduhr und das "Haus zum Frieden und zur Gerechtigkeit". Die berühmteste Ansicht des Marktplatzes bietet die Südseite (Zakrzewski-Seite), die vom Turm der Jesuitenkirche überragt wird. Besondere Beachtung verdienen hier das Haus zum Löwen und die Sonnenuhr von Tadeusz Przypkowski. Die zwei Brunnen aus dem 18. Jahrhundert wurden in den 1970er Jahren wiederhergestellt.

Vom Marktplatz aus verlaufen jeweils zwei Seitenstraßen in alle Himmelsrichtungen. Die nördliche Nowomiejska-Straße führt zur Barbakane, einer massiven Verteidigungsanlage auf einer gotischen Brücke um das Neustädter Stadttor, die im 15. Jahrhundert von Jan Baptysta errichtet wurde. Die Świętojańska-Straße führt nach Südosten zum Schlossplatz.

An dieser Straße befindet sich die Johanneskathedrale. Sie stammt aus der zweiten Hälfte des 15. Jahrhunderts, wurde aber in den folgenden Jahrhunderten umgebaut. Zuletzt wurde sie, dem Zeitgeist des 19. Jahrhunderts entsprechend, im Stil der englischen Neugotik umgestaltet, was ihr historisches Aussehen zerstörte. Im Zweiten Weltkrieg wurde das Bauwerk bis auf die Fundamente verwüstet, lediglich der alte, niedrige Glockenturm blieb teilweise erhalten. Da die Kirche von Grund auf rekonstruiert werden musste, entschied man sich, sie als Neuschöpfung in Anlehnung an die Masowische Gotik neu erstehen zu lassen.

Gleich daneben steht die der Muttergottes geweihte Jesuitenkirche, die 1609–1629 im Übergangsstil des Manierismus zum Barock erbaut wurde. Außerdem ist das im Renaissancestil gehaltene "Haus zum Schiff" eine weitere Sehenswürdigkeit an der Świętojańska-Straße. Weitere gut erhaltene Bürgerhäuser, wie das "Haus zu den Tauben", das "Haus mit der Christus-Statue" oder das "Burbach-Patrizierhaus", finden sich an den Straßen Szeroki Dunaj ("Breite Donau" – ehemaliger Bach innerhalb der Altstadt), Wąski Dunaj "(Schmale Donau)", der Piwna-Straße, der Brzozowa-Straße und der Rycerska-Straße. An der Piwna-Straße befindet sich die 1356 erbaute Martinskirche mit einem gotischen Chor und einer barocken Fassade. Steil zur Weichsel hinab führt die malerische Ulica Kamienne Schodki "(Steinerne Treppengasse)", die zum ehemaligen Weißen Tor führte. Der Kanonikerplatz im Osten der Altstadt wird von manieristischen Bürgerhäusern gesäumt, die ursprünglich dem Kanoniker Orden gehörten. In seiner Mitte steht die 1646 gegossene Warschauer Erzglocke. Ein besonders schöner Blick auf die Altstadt bietet sich vom Pragaer Weichselufer.

Die Neustadt "(Nowe Miasto)" schließt sich im Norden an die Altstadt an und liegt ebenfalls auf einer Uferdüne an der Weichsel. Sie wurde im 14. Jahrhundert außerhalb der Stadtmauern angelegt. Die Neustadt wurde, nachdem sie im Zweiten Weltkrieg völlig zerstört wurde, zusammen mit der Altstadt in den frühen 1950er Jahren wiederaufgebaut. Das Zentrum ist der dreieckige Neustädter Marktplatz. Er besaß so wie der Altstädtische Markt ein Rathaus, das 1818 abgebrochen wurde.

An der Südseite des Platzes befindet sich die 1688–1692 von Tylman van Gameren zu Ehren des Siegers der Schlacht am Kahlenberg (1683), König Jan Sobieski errichtete barocke Sakramentinnenkirche des heiligen Kasimir. Sie diente während des Warschauer Aufstandes als Lazarett. Während eines Bombenangriffs der deutschen Luftwaffe fanden hunderte Verwundete, Ärzte und Krankenschwestern den Tod, als ihre Kuppel einstürzte.

Die älteste Neustädter und eine der ältesten Warschauer Kirchen ist die Anfang des 15. Jahrhunderts erbaute spätgotische Marienkirche, deren charakteristischer Turm das Weichselpanorama dominiert. Daneben gibt es drei weitere ursprünglich gotische Kirchen, die in der Barockzeit umgebaut wurden – die Franziskaner-, die Paulaner- und die Dominikanerkirche. Der barocke Adelspalast der Magnatendynastie Sapieha, der Sapieha-Palast, überragt den nördlichen Teil der Neustadt. Als besonders schön gelten die Straßenzüge der Ulica Freta und der Ulica Mostowa. In einem Haus an der ersteren wurde im 19. Jahrhundert die zweifache Nobelpreisträgerin Marie Curie geboren. An der letzteren steht das Brückentor aus der Renaissance, das zur ersten Warschauer Weichselbrücke aus dem 16. Jahrhundert führte. Im ehemaligen Mostowski-Palast in der Ulica Kościelna befindet sich seit dem Jahr 2004 das 5-Sterne-Hotel Le Regina. Sehenswert ist auch der sogenannte Mokronowski-Palast, der 1771 von Giacomo Fontana erbaut wurde.

Der Warschauer Königsweg "(Trakt Królewski)" beginnt am Königsschloss und führt in südlicher Richtung etwa 10 km lang bis zur Stadtresidenz Wilanów König Jan Sobieskis und ist eine der längsten Repräsentationsstraßen der Welt. Er setzt sich aus mehreren repräsentativen Straßenzügen zusammen, der Krakauer Vorstadt, der "Neuen Welt" (Ulica Nowy Świat) und den Ujazdowski-Alleen (von Norden nach Süden). Der Königsweg verläuft entlang der Weichsel und bildete zusammen mit der senkrecht zu ihm verlaufenden Sächsischen Achse die Hauptachse der urbanen Entwicklung Warschaus. Er wurde bereits zu Beginn der Stadtgeschichte bebaut und verband die ehemalige Siedlung Jazdów mit der Altstadt. Nachdem Königin Bona Sforza Anfang des 16. Jahrhunderts auf den Fundamenten der Burg Jazdów ein Schloss errichtete und sich dort nach dem Tod ihres Ehemannes König Sigismund I. niederließ, entstand an dem Weg vom Krakauer Tor zum Schloss Ujazdowski eine dauerhafte Bebauung. Der Königsweg war eine der ersten Warschauer Straßen, die gepflastert wurden. Im Gegensatz zur engen Altstadt wurde dieser Teil der Stadt weiträumig angelegt und es dominieren weiträumige Gärten und Parkanlagen sowie große Paläste und Wohnhäuser. Seinem Ruf als Prachtstraße werden auch die vielen Regierungs- und Verwaltungsgebäude in ehemaligen Adelspalästen gerecht, die den Königsweg säumen. Weite Teile sollen in den nächsten Jahren für den Straßenverkehr gesperrt und mit ihren zahlreichen Geschäften zur Flaniermeile ausgebaut werden.

Die Krakauer Vorstadt beginnt am Königsschloss und führt neben dem Koniecpolski-Palast (Amtssitz des Präsidenten) bis zum Staszic-Palast.
Am nördlichen Ende der Krakauer Vorstadt befand sich im Mittelalter das Krakauer Tor. Heute steht hier markant die Sigismundssäule auf dem Schlossplatz. Die ersten Gebäude der Krakauer Vorstadt wurden während des Krieges zerstört und im Zuge des Baues der Unterführung Ost-West nicht wieder aufgebaut. Heute ist der Turm der St.-Anna-Kirche, von dem sich ein schönes Panorama auf die Stadt bietet, das nördlichste Gebäude der Krakauer Vorstadt. Die St.-Anna-Kirche ist eine Synthese aus gotischem, barockem und klassizistischem Baustil. Sie wurde 1454 von der masowischen Fürstin Anna Mazowiecka zu Ehren ihrer Namenspatronin für den Bernhardiner-Orden gestiftet. Der Chor, das Sternengewölbe und der Saal im Kirchenkloster sind im Stil der Gotik gehalten und überstanden spätere Umbauten. In der Renaissance wurde sie nach Westen ausgebaut und im 17. Jahrhundert barockisiert. Stanisław Kostka Potocki und Chrystian Piotr Aigner haben 1788 eine spätbarocke Fassade mit Skulpturen von Jakob Monaldi und Franz Pinck entworfen. Aigner hat später 1819–1821 auch die klassizistische Kolonnade entworfen. Im Zweiten Weltkrieg wurde die Kirche nur zu einem geringen Teil zerstört. Infolge des Baus des Ost-West-Tunnels drohte sie jedoch beschädigt zu werden und die Böschung musste mit Pfählen aus Eisenbeton gestützt werden. Heute wird die St.-Anna-Kirche als Universitätskirche von der Hochschulgemeinde genutzt. Zu den wichtigsten Palais in der Krakauer Vorstadt zählen das 1686 von Tylman van Gameren errichtete Palais Czapski, der 1693 von Józef Piola erbaute Potocki-Palast und der im 17. Jahrhundert geschaffene Kazimierz-Palast.

Die Nowy Świat "(Neue Welt)" beginnt am Staszic-Palast und führt über das Rondo de Gaulle bis zum Platz der Drei Kreuze. Sie ist eine der beliebtesten Flanier- und Einkaufsmeilen Warschaus. Hier befinden sich der Kossakowski-Palast, das Sanguszko-Palais und Branicki-Palast. Auf dem Rondo de Gaulle steht eine künstliche Palme. Hier kreuzen sich die Nowy Świat und die Jerusalemer Alleen, die zur Poniatowski-Brücke führen. Am Rondo stand vor dem Ersten Weltkrieg das Palais Opalinski, an dessen Stelle 1928 bis 1931 der Sitz der Bank für Landeswirtschaft von Rudolf Świerczyński erbaut wurde. Das Gebäude gilt als eines der besten Beispiele der Architektur der Zwischenkriegszeit in Polen. Am Ende der Nowy Świat steht auf dem Platz der Drei Kreuze die Aleksander-Kirche von Chrystian Piotr Aigner.

Die Aleje Ujazdowskie beginnen am Platz der Drei Kreuze, in dessen Mitte die Alexanderkirche steht. Südöstlich zweigt die Wiejska-Straße ab, an der sich die Regierungsgebäude des Sejm und Senat befinden. Die eigentlichen Aleje Ujazdowskie zweigen vom Platz der Drei Kreuze direkt nach Süden ab und gehen schon bald in eine Parklandschaft über. Insbesondere an der östlichen Seite befinden sich die wichtigsten Parkanlagen Warschaus. Der Łazienki-Park, mit seinen Hauptattraktionen, dem Łazienki-Palast und dem Chopin-Denkmal, sowie der Palast Belweder "(Belvedere)" liegen an den Aleje Ujazdowskie. In der nahen Umgebung gibt es den 1896 geöffneten Ujazdowski-Park mit dem Schloss Ujazdowski, wo sich heute das "Zentrum für zeitgenössische Kunst" befindet.

Im 19. Jahrhundert errichtete die russische Besatzungsmacht zunächst die Zitadelle und in Folge zwei Gürtel von Befestigungsanlagen (im Wesentlichen Artillerieforts) um Warschau und baute die Stadt so zur Festung Warschau aus, da sie einen Angriff westeuropäischer Großmächte befürchtete. Ähnlich verfuhren die Österreicher mit Krakau und Przemyśl sowie Deutschland mit Lötzen und Toruń, da sie jeweils einen russischen Angriff voraussahen. Im Ersten Weltkrieg kamen diese Bastionen teilweise zum Einsatz. Neben der Zitadelle sind einige dieser Warschauer Forts (sowie die Festung Modlin, die zum "Polnischen Festungsdreieck" gehörte) erhalten geblieben und können besichtigt werden.

Seit dem 16. Jahrhundert verlegten viele Magnatenfamilien ihren Hauptsitz nach Warschau, wo der Sejm tagte und die Königswahl stattfand. Wer an der großen Politik teilhaben wollte, musste vor Ort anwesend sein. Obwohl Warschau in der Adelsrepublik nie Hauptstadt wurde, sondern stets „nur“ Königssitz und Sejmstadt war, wurden hier die politisch wichtigen Entscheidungen getroffen. Insofern blieb die Hauptstadt Krakau nach 1611 nur formell Hauptstadt. Viele der repräsentativen Paläste wurden entlang der Hauptachsen der Stadt (Königsweg, Sächsische Achse, Ulica Senatorska, Ulica Miodowa, Ulica Freta) im Stil des Barock und Klassizismus errichtet. Einige der größten Paläste mit ausgedehnten Gartenanlagen entstanden etwas außerhalb der Hauptstraßen, wie zum Beispiel der Wilanów-Palast. Zu den wichtigen Palais Warschaus zählen auch:


Gegen Ende des 14. Jahrhunderts entstanden in Warschau die ersten gemauerten Kirchen. Kaum eine der gotischen und Renaissancekirchen hat jedoch die Verwüstungen durch die Schweden im Ersten Nordischen Krieg 1655 bis 1660 unbeschadet überstanden. Nach dem Krieg wurden die zerstörten Kirchen wiederaufgebaut und im Zuge der Ausdehnung des Stadtgebiets nach Süden neue errichtet. Dieser großen Bautätigkeit verdankt Warschau seine zahlreichen Kirchen des Hochbarock und des Klassizismus, die heute das Stadtbild prägen. Bedeutendster Baumeister des Warschauer Barocks war Tylman van Gameren. Im Stil des Klassizismus schufen Domenico Merlini, Chrystian Piotr Aigner und Simon Gottlieb Zug. Auch die Neorenaissance und Neugotik hinterließen in Warschau zahlreiche Sakralbauten. Während der russischen Besetzung im 19. Jahrhundert wurden zahlreiche orthodoxe Kirchen in Warschau errichtet, von denen sich bis heute nur zwei erhalten konnten. Die meisten wurden nach der polnischen Unabhängigkeit 1918 abgerissen oder im Zweiten Weltkrieg zerstört.

Auch die stammten aus dem 19. Jahrhundert, zwei davon sind erhalten (siehe Nożyk-Synagoge).
Viele der Warschauer Gotteshäuser wurden 1944 gesprengt oder schwer beschädigt, nach dem Krieg aber rekonstruiert.

Die deutsche Besatzungsmacht ließ die Große Synagoge am 16. Mai 1943 nach dem Aufstand im Warschauer Ghetto sprengen. Die Pragaer Synagoge wurde 1961 abgerissen.


1661 war der Wiederaufbau abgeschlossen, der der Kirche ihre heutige Form verlieh. Die 1825 angebaute neugotische Eingangshalle wurde bei der Rekonstruktion der Kirche nach den Zerstörungen des Zweiten Weltkriegs nicht wieder aufgebaut. Im Innern findet sich ein stuckiertes "Lubliner Gewölbe".






Die Warschauer Friedhöfe sind als Zeitzeugen einzigartig, da sie die einzigen Teile der Stadt sind, die im Zweiten Weltkrieg nicht vollständig zerstört wurden. Die ältesten erhaltenen Friedhöfe stammen aus dem 18. Jahrhundert und sind mit wunderschönen Grabmälern aus dem 18. und 19. Jahrhundert ausgestattet. Hervorzuheben ist der 43 Hektar große Powązki-Friedhof mit Grabstätten vieler berühmter Polen. Der in der Nähe liegende jüdische Friedhof ist einer der größten Europas.

Aufgrund der oft tragischen Stadtgeschichte gibt es in Warschau sehr viele Gedenkstätten an Opfer von Fremd- und Gewaltherrschaft. Hierzu gehören vor allem das Grabmal des unbekannten Soldaten, das Warschauer Ghetto-Ehrenmal, das Denkmal des Warschauer Aufstandes, der X. Pavillon in der Zitadelle, Pawiak sowie zahlreiche kleinere Gedenktafeln und -steine, auf die der aufmerksame Besucher an fast jeder Straßenkreuzung trifft.

In den Nachkriegsjahren wurden in Warschau über 200 Gedenktafeln an die Opfer und Ereignisse während der deutschen Besatzung platziert. Der polnische Bildhauer Karol Tchorek (1904–1985) entwarf sie 1948 im Rahmen eines Wettbewerbs. 1949 erhielt er dafür den ersten Preis. 2013 gab es noch über 160 dieser "Tchorek-Gedenktafeln" in Warschau. Etliche waren der Modernisierung und dem Ausbau der Stadt zum Opfer gefallen. Im Jahr 1962 entschied das damalige Bürgerkomitee für Denkmalschutz, die Standorte dieser Tafeln zu katalogisieren. Die Durchführung oblag den Schulen, Unternehmen, gemeinnützigen Organisationen, Berufsverbänden und öffentlichen Einrichtungen. Die Tafeln sind aus Sandstein gefertigt. Als zentrales Motiv ist das Malteserkreuz abgebildet. In der Regel trägt das Schild in der Mitte die Inschrift: „MIEJSCE UŚWIĘCONE KRWIĄ POLAKÓW POLEGŁYCH ZA WOLNOŚĆ OJCZYZNY“ („dieser Ort ist durch das Blut der Polen geheiligt, die im Kampf um die Freiheit ihrer Heimat gefallen sind“). Darunter sind Informationen über das jeweilige Ereignis festgehalten. Garantien für die exakte Belegung der Angaben, wie Anzahl der Todesopfer oder das Datum des Ereignisses, können jedoch nicht gegeben werden. Auch grammatikalische Fehler sowie Fehler in der Zeichensetzung oder der Rechtschreibung kommen vor. Der Wortlaut des Textes wurde von politischem Druck bestimmt. Die meisten der Gedenktafeln beziehen sich auf den Warschauer Aufstand, ein Ereignis, welches bei dem kommunistischen Regime Polens umstritten war. Aus diesem Grund wurden die Inschriften sorgfältig entsprechend der damaligen offiziellen Propaganda formuliert. In manchen Fällen wurde der Warschauer Aufstand indirekt umschrieben, was man in dem formulierten Text beispielsweise unter „aufständisch “oder „aufständisches Krankenhaus“ wiederfindet. Das Nazitum wurde immer mit „Hitlerowcy“ („Hitleristen“) bezeichnet.

Die Architektur des Sozialistischen Realismus wird teilweise bis in die Gegenwart als von der Sowjetunion aufgezwungen und fremd empfunden. Mittlerweile wird jedoch dieser Baustil als Teil der Architekturgeschichte der Stadt akzeptiert. Was für den künstlerisch und baulich anspruchsvolleren Stil der 1950er Jahre gilt, kann jedoch nicht für die Plattenbauten der 1970er Jahre gelten, die vor allem in den Außenbezirken entstanden sind.

Der dominanteste "sozrealistische" Bau in der Innenstadt ist der 1952–1955 errichtete Pałac Kultury i Nauki oder kurz "Pałac Kultury" ("Palast der Kultur und Wissenschaft" bzw. "Kulturpalast"). Er verbindet den Zuckerbäckerbaustil mit Elementen der traditionellen polnischen Architektur, wie der polnischen Attika, ähnelt jedoch in seinem Kubus auch dem Empire State Building in New York. Daneben gelten auch der Platz der Verfassung, das Viertel MDM, Marienstadt und die Ostwand als charakteristische Architekturbeispiele dieser Periode. Auch das Zentralratsgebäude der PVAP entstand in diesem Stil. Spätere Bauwerke aus der sozialistischen Zeit weisen einen mehr international ausgerichteten Stil auf, der sich zum Beispiel bei dem von einem schwedischen Architektenbüro entworfenen Novotel Warszawa Centrum (früher: "Hotel Forum"), dem Hotel Marriott, dem Intraco I, dem "Intraco II" (heute Oxford Tower genannt) und anderen Wolkenkratzern der real-sozialistischen Zeit manifestierte. Der ehemals größte Basar Europas im Stadion "10-lecia" wirkte wie eine Reminiszenz an die frühe Nachwendezeit.

Seit 1989 vollzog sich eine Wende in der monumentalen Warschauer Architektur und immer mehr "gläserne Gebäude" entstanden. Den Anfang machte der Blaue Wolkenkratzer (Blue Tower), der Anfang der 1990er Jahre am Bankenplatz an der Stelle der ehemaligen Hauptsynagoge fertiggestellt wurde. Insbesondere seit der Wende legt Warschau immer mehr sein "Plattenbau-Image" ab und selbst das höchste Gebäude der Stadt, der Kulturpalast, wird langsam von modernen Hochhäusern verdrängt. Die interessantesten modernen Gebäude entstanden entlang der Johannes-Paul-II.-Allee und der Emilia-Plater-Straße westlich des Kulturpalastes. Einzelne hervorragende Architekturbeispiele befinden sich auch außerhalb des Finanzviertels, wie zum Beispiel der Warsaw Trade Tower oder das Metropolitan. Meisterwerke der letzten Jahre sind das Rondo 1-B, die Złote Tarasy "(Goldene Terrassen)", das Gebäude des Obersten Gerichtshofes und die neue Universitätsbibliothek. Gebaut wird gerade an dem monumentalen Tempel der Göttlichen Vorsehung im Stadtteil Wilanów, deren Grundstein bereits 1792 gelegt wurde. Interessante zukünftige Projekte sind das Hochhaus Złota 44 von Daniel Libeskind an der Johannes-Paul-II.-Allee und das Wissenschaftszentrum Kopernikus an der Weichsel. Zudem wird lebhaft über den Wiederaufbau des Sächsischen Schlosses und des Brühlschen Palastes sowie eines Denkmals zu Ehren Johannes Pauls II. auf dem Piłsudski-Platz diskutiert.

Viele Gebäude konnten nach dem Zweiten Weltkrieg nicht wieder aufgebaut werden, insbesondere die ganze Sächsische Achse. Sie stand senkrecht in Ost-West-Richtung auf dem Königsweg, kreuzte sich mit diesem in der Höhe des Hotel Bristol. Zu ihr gehörten der Piłsudski-Platz (vor 1918 Sächsischer Platz, 1939–1945 Adolf-Hitler-Platz, 1945–1989 Platz des Sieges) mit seiner barocken Bebauung (unter anderem orthodoxe Kirche mit Turm bis ca. 1920), das Sächsische Palais, das Brühlsche Palais, der Sächsische Garten und die Paläste der Siedlung hinter dem Eisernen Tor. Von der Bebauung des Piłsudski-Platzes ist nichts übrig geblieben. Heute wird seine Südseite vom Hotel Viktoria und die Nordseite vom modernen Bürogebäude des Stararchitekten Norman Foster Metropolitan gesäumt. Die Ostseite erinnert noch etwas an die Vorkriegszeit, obwohl auch diese Gebäude nach dem Krieg nicht originalgetreu rekonstruiert wurden. Einziger Überrest vom Sächsischen Palais und dem Brühlschen Palais ist ein Teil der Säulenfront, wo sich das Grabmal des Unbekannten Soldaten befindet. Die Vorarbeiten zum Wiederaufbau des Sächsischen Palais begannen mit archäologischen Grabungen 2006.
Der Gebäude- und Pflanzenbestand des Sächsischen Gartens ist 1944 fast völlig niedergebrannt. Das Theater und die Orangerie wurden nicht wieder aufgebaut. Nur der Arkadenbrunnen und der Sybillentempel zeugen wieder von der einstigen Pracht. Große Teile des ehemaligen Parks wurden Bauland und gingen damit für den Park verloren. Das großartige Eiserne Tor und Paläste im Westen gibt es nicht mehr. Der einzige Palast, der teilweise rekonstruiert wurde, ist das Lubomirski-Palast, der 1967 nach dem Wiederaufbau um ca. 90 Grad auf Schienen gedreht wurde, so dass er jetzt die gedachte Sächsische Achse an ihrem jetzigen westlichen Ende verschließt. Ursprünglich führte sie weit in die Siedlung hinter dem Eisernen Tor, wo heute Plattenbauten die repräsentative Bebauung von vor 1939 ersetzten.

Warschau ist das wirtschaftliche Zentrum Polens. Etwa 15 % des polnischen Bruttoinlandsproduktes werden in der Stadt erwirtschaftet. Viele ausländische Investoren nutzen Warschau als Ausgangspunkt für Geschäfte in Mittel- und Osteuropa, was an den vielen neuen Bürohochhäusern und Hotels erkennbar ist.

Seit April 1991 gibt es wieder die Warschauer Wertpapierbörse. Die "Giełda Papierów Wartościowych w Warszawie" (GPW) ist einer der schnellstwachsenden Börsenplätze der Welt und ist die größte Börse im östlichen Mitteleuropa.

Ab 2005 wurde auf einem drei Hektar großen Areal in der Nähe des Zentralbahnhofes "(Warszawa Centralna)" mit dem Bau des Einkaufszentrums Złote Tarasy (dt.: "Goldene Terrassen") begonnen, welches nach einer Bauzeit von 37 Monaten am 7. Februar 2007 eröffnet wurde. Es umfasst eine Nutzfläche von über 200.000 m². Mit 57.000 m² Verkaufsfläche gehört es zu den größten in Osteuropa. Eine Besonderheit ist das 10.000 m² umfassende Atrium, welches von einem wellenartigen Glasdach überspannt wird. In dem Komplex befinden sich zudem ein Kinocenter und ein Parkhaus mit 1.700 Stellplätzen.

In Warschau haben die polnischen Sender TVN, Telewizja Polska, Polsat, TV 4, TV Puls und Canal+ ihren Hauptsitz. Daneben haben Unternehmen wie Viacom International Media Networks und Discovery Networks einen der Hauptsitze in Warschau. Des Weiteren sendet Polskie Radio i Telewizja und diverse weitere Privatsender aus der Hauptstadt. Polens bedeutendste Boulevardzeitung Fakt hat hier ihren Sitz, aber auch die Gazeta Wyborcza und die Rzeczpospolita.

Warschau ist wichtiger Verkehrsknotenpunkt im Schnittpunkt der Verkehrswege Paris/London–Berlin–Warschau–Minsk/Kiew/Moskau und Nordeuropa–Balkan.

Im Bereich des öffentlichen Personennahverkehrs verfügt Warschau über ein Bus- und Straßenbahnnetz, das vor allem außerhalb des Zentrums überlastet ist. Das 121 km lange Streckennetz der Warschauer Straßenbahn wird von 27 Linien bedient. Es ist technisch teilweise veraltet und wird zurzeit modernisiert. Das Busnetz in Warschau setzt sich aus 219 Linien zusammen und bedient ein Streckennetz von ca. 2600 km Gesamtlänge.

Seit April 1995 verkehrt in Warschau eine U-Bahn, deren Fertigstellung sich immer wieder verzögert hat. Derzeit umfasst das Liniennetz zwei Linien mit einer Länge von 29,2 Kilometern und 28 Stationen.
Die Linie M1 verkehrt in Nord-Süd-Richtung und umfasst eine Gesamtlänge von 23,1 Kilometern mit 21 Stationen. Die im März 2015 eröffnete Linie M2 mit 6,1 Kilometer Länge und 7 Stationen verläuft in West-Ost-Richtung und unterquert dabei die Weichsel.

Für den Regional- bzw. Vorortverkehr gibt es, neben einigen Vorort- und Überlandbussen, die Warschauer Vorortbahn "(Warszawska Kolej Dojazdowa)".

Seit 1. Juli 2005 sind S-Bahnen der Warschauer S-Bahn "(Warszawska Szybka Kolej Miejska)" in Betrieb. Das S-Bahn Netz wurde zum 1. Juni 2012 auf vier Linien erweitert.

Für den Fernverkehr gibt es einen unterirdischen Zentralbahnhof (Warszawa Centralna), die Bahnhöfe Warszawa Wschodnia "(Ost)" und Warszawa Zachodnia "(West)" und mehrere kleinere Bahnhöfe.

Über ein umfangreiches System von Fußgängertunneln ist der Zentralbahnhof mit den beiden Stationen der Vorortbahn "(Warszawa Śródmieście)" und der U-Bahn "(Centrum)" verknüpft, die einige hundert Meter entfernt liegen. Vom Zentralbahnhof aus verkehren hauptsächlich die Fernzüge in alle polnischen Großstädte sowie die meisten Hauptstädte der Nachbarstaaten wie Berlin, Minsk, Moskau, Kiew, Wien und Prag.

Die PKP hat die Fernstrecken Warschau–Danzig-Gdynia und Warschau-Krakau bzw. Kattowitz für den EIP (Pendolino) ausgebaut. Andere Strecken werden mit dem Express InterCity (EIC) bedient. – Der Bau einer Hochgeschwindigkeitsstrecke ins Baltikum (Rail Baltica) soll bis 2023 realisiert sein.

Warschau besitzt einen großen Busbahnhof, und zwar den .
Er ist vom Zentralbahnhof aus mit den Stadtbuslinien 158 und 588 zu erreichen. Von dort verkehren hauptsächlich Fernbusse der PKS, die in Polen mit der Bahn gleichwertig angesehen sind. Der Busbahnhof für den Stadtbusverkehr befindet sich auf der Vorderseite von Warszawa Zachodnia. Nationaler und internationaler Busverkehr wird im rückwärtigen Bereich des Bahnhofgebäudes abgewickelt.

Warschau besitzt mit dem Chopin-Flughafen den wichtigsten und größten internationalen Flughafen Polens. Der Flughafen ist der Heimatflughafen der Polskie Linie Lotnicze "(LOT)".
Der Chopin-Flughafen liegt etwa 10 km vom Stadtzentrum entfernt im Ortsteil Okęcie des Stadtbezirks Włochy. Ungefähr 9 Millionen Passagiere werden jährlich am Flughafen abgefertigt. Der Flughafen besitzt vier Terminals.

Der zweite Flughafen namens Flughafen Modlin liegt etwa 50 km nordwestlich von Warschau und soll den Chopin-Flughafen entlasten. Er wird hauptsächlich von Billigfluggesellschaften genutzt.

Die polnische Hauptstadt ist an das landesweite polnische Fernstraßennetz angebunden, das Schnellstraßennetz im Raum Warschau befindet sich noch im Aufbau. Derzeit verlaufen die Droga krajowa 2, 7, 8 und die 61 durch die Innenstadt.

Warschau besitzt keine Umfahrungen, daher herrscht viel Verkehr in der Innenstadt. Im Bau und Planung befindet sich die sogenannte "Schnellstraßenumfahrung" ( "Ekspresowa Obwodnica Warszawy") mit einer Länge von ungefähr 85 Kilometern, die den Fernverkehr aus dem Stadtzentrum fernhalten soll. Sie besteht aus den Schnellstraßen S2 (E30), S7 (E77), S8 (E67) und der S17 (E372). Die Schnellstraße S2 soll die Lücke der Autostrada A2 zwischen den Autobahnkreuzen „Warschau-Konotopa“ und „Warschau-Lubelska“ schließen. Die Schnellstraße S79 soll in Zukunft den Chopin-Flughafen mit der Schnellstraßenumfahrung verbinden.

Das Fahrradverleihsystem in Warschau heißt Veturilo.

Als Hauptstadt Polens ist Warschau neben Krakau auch das Bildungszentrum des Landes. In der Stadt studieren ungefähr 255.000 Studenten.
Die wichtigsten Hochschulen der Stadt sind:

Im Stadtteil Natolin befindet sich der polnische Campus des College of Europe.

In Warschau gibt es seit 1810 die Fryderyk-Chopin-Musikuniversität und seit 1932 die wichtigste Schauspielschule in Polen – Aleksander-Zelwerowicz-Theaterakademie Warschau. Zahlreiche der berühmtesten polnischen Schauspieler absolvierten hier ihr Studium.

Neben der Polnischen Nationalbibliothek und der Universitätsbibliothek Warschau dient als größte öffentliche Bibliothek auch die Warschauer Stadtbibliothek der literarischen Versorgung der Bürger.

Die bekanntesten Fußballvereine der Stadt sind Legia und Polonia, die jeweils mehrmals die polnische Meisterschaft gewinnen konnten. Der frühere Armeeklub Legia trägt seine Spiele in der Pepsi Arena aus, die über 30.000 Zuschauern Platz bietet. Polonia spielt, wie auch der American-Football-Klub Warsaw Eagles, im deutlich kleineren Polonia-Warschau-Stadion.

Die größte Arena der Stadt ist das Stadion Narodowy, das anlässlich der Fußball-Europameisterschaft 2012 gebaut wurde und etwa 58.000 Plätze hat. Es ist die Heimstätte der Polnischen Herren-Fußballnationalmannschaft, die ihre Heimspiele zuvor im Stadion Dziesięciolecia austrug. Seit 2015 findet hier auch jährlich ein WM-Grand Prix zur Speedway-Einzel-Weltmeisterschaft statt.

Seit 1979 findet jedes Jahr der Warschau-Marathon statt.

Der Pferderennsport hat eine große Tradition in Warschau. Zwischen 1841 und 1939 bestand die Pferderennbahn Pole Mokotowskie; diese wurde durch die Pferderennbahn Służewiec ersetzt, die heute noch existiert.

In der "Liste von Persönlichkeiten der Stadt Warschau" sind die in der Stadt geborenen Persönlichkeiten aufgeführt sowie solche, die ihren Wirkungskreis in Warschau hatten.

Die von der Stadt zu Ehrenbürgern ernannten Persönlichkeiten sind in der "Liste der Ehrenbürger von Warschau" zu finden.

Nach Warschau ist der Asteroid (1263) Varsavia benannt.

Vom 24. bis 30. Juli 1989 wurden in Warschau die IX. Internationalen Feuerwehrwettkämpfe des Weltfeuerwehrverbandes CTIF (Feuerwehrolympiade) veranstaltet. Zum Programm gehörten Traditionelle Internationale Feuerwehrwettbewerbe, Internationale Feuerwehrsportwettkämpfe und Internationale Jugendfeuerwehrwettbewerbe.





</doc>
<doc id="5598" url="https://de.wikipedia.org/wiki?curid=5598" title="Warnstreik">
Warnstreik

Der Warnstreik ist ein Unterfall der üblichen Arbeitsniederlegung von Arbeitnehmern in Form eines kurzen Streiks in einem Betrieb in sachlichem und zeitlichem Zusammenhang mit laufenden Tarifverhandlungen (BAGE 28,295). Zweck eines Warnstreiks ist es, durch die Ausübung von Druck Tarifverhandlungen zu erzwingen oder aber festgefahrene Tarifverhandlungen zu beleben. Ein Warnstreik kann ohne Urabstimmung stattfinden.

Streiks, also auch Warnstreiks, sind in Deutschland als Mittel des Arbeitskampfes durch das Grundgesetz gewährleistet. Der Art. 9 GG schützt verfassungsrechtlich die sogenannte Tarifautonomie und die Maßnahmen, die hierfür erforderlich sind. Der Streik ist ein Grundrecht zur Durchsetzung tariflicher Forderungen.

Das Bundesarbeitsgericht (BAG) hat festgelegt, dass auch bei Warnstreiks das sogenannte Ultima-Ratio-Prinzip gilt (siehe BAG GS v. AP Nr. 43 zu Art. 9 GG Arbeitskampf), welches besagt, dass Arbeitskampfmaßnahmen erst nach Scheitern der Tarifverhandlungen ergriffen werden dürfen.

Eine förmliche Erklärung, dass die Tarifverhandlungen gescheitert sind, ist hierfür nicht nötig. Allein die Tatsache, dass Gewerkschaften zu Warnstreiks aufrufen, macht klar, dass sie die Verhandlungen zurzeit als gescheitert betrachten (Entscheidung des BAG vom 21. Juni 1988).

Die Teilnahme an einem rechtmäßigen (Warn-)Streik stellt keine Verletzung des Arbeitsvertrages dar. Maßregelungen durch den Arbeitgeber wegen der Teilnahme an einem Streik sind verboten. Der bestreikte Arbeitgeber darf deshalb dem streikenden Arbeitnehmer nicht kündigen. Nach Ende des Streiks besteht ein Anspruch auf Weiterbeschäftigung.

Während des Streiks ruht das Arbeitsverhältnis.

Der Arbeitnehmer braucht keine Arbeitsleistung zu erbringen. Ein Anspruch auf Arbeitsentgelt besteht für die Dauer des Streiks nicht.


</doc>
<doc id="5599" url="https://de.wikipedia.org/wiki?curid=5599" title="Weltwunder">
Weltwunder

Weltwunder oder die sieben Weltwunder waren bereits in der Antike eine Auflistung besonderer Bauwerke oder Standbilder. Die älteste Überlieferung einer Liste von Weltwundern geht auf den Geschichtsschreiber Herodot zurück (etwa 450 v. Chr.).

Die erste vollständige Liste der bekannten „sieben Weltwunder“ findet sich in einem Epigramm des Schriftstellers Antipatros von Sidon (2. Jahrhundert v. Chr.), der einen Reiseführer des Mittelmeerraums und Vorderasiens schrieb. Die Griechen nannten sie: , "" – „die sieben Sehenswürdigkeiten der bewohnten [Erde]“. Philon von Byzanz beschrieb sie in der Schrift "De septem mundi miraculis".

Dass die Liste in Vorderasien entstand, ist naheliegend: Vier der Weltwunder fanden sich dort. Da zu jener Zeit viele imposante Bauwerke be- und entstanden, wurden vor allem solche in der Umgebung des Schreibers angeführt.

Diese Liste wurde im Laufe der Jahre oft geändert und den Reisegewohnheiten der jeweiligen Gesellschaften angepasst. Schon in klassischer Zeit gab es Alternativen, wie das Kapitol in Rom, den „Hörneraltar der Artemis auf Delos“, den „Hadrianustempel des Zeus in Kyzikos“ (südliches Marmarameer) und viele mehr. Im 13. Jahrhundert wurden die gesamte Stadt Rom, die Hagia Sophia in Konstantinopel (heute Istanbul, Türkei) und sogar die Arche Noah aufgenommen. Aus der anfänglich kurzen Reiseliste entstand zeitweise ein ganzer Reisekatalog, der alle bedeutenden Bauwerke wie Tempel oder Skulpturen enthielt. Doch diese zerfielen mit der Zeit, und im Gedächtnis blieb vor allem der Mythos der ursprünglichen Weltwunder.

Auch heute noch inspirieren die klassischen „sieben Weltwunder“ Autoren, immer wieder neue Listen von „Weltwundern“ in den verschiedensten Bereichen zu erstellen. Darunter fallen zeitgenössische Bauwerke ebenso wie auch außergewöhnliche Aufzählungen von Naturereignissen oder Kunstwerken.

In der Antike beschrieb der erwähnte Antipatros die heute geläufige Liste der klassischen sieben Weltwunder in seinem Reiseführer. Genannt wurden darin die imposantesten und prunkvollsten Bauwerke seiner Zeit und seines Kulturkreises:


Die Liste umfasst sieben Weltwunder, weil die Zahl Sieben in der Antike als „vollkommen“ galt. Diese festgelegte Zahl sollte die Bauwerke in ihrer Bedeutung erhöhen.

Heute existieren von diesen Weltwundern nur noch die Pyramiden von Gizeh. Die anderen wurden durch Erdbeben und Kriege zerstört oder zerfielen im Laufe der Zeit. Die ursprünglich aufgelisteten Stadtmauern von Babylon wurden z. B. schon von Antipatros aus der Liste entfernt, da sie zerstört waren, und durch den Leuchtturm von Alexandria ersetzt. In späteren Listen waren die Mauern von Babylon aber teilweise noch verzeichnet. Erst Gregor von Tours strich sie im 6. Jahrhundert endgültig aus der Liste.

Antike Darstellungen der Weltwunder gibt es recht wenige, jedoch wurden einige Münzprägungen mit dem Helioskopf (der Koloss von Rhodos war eine Statue des Sonnengottes Helios), mit der Zeusstatue im Profil oder mit dem Leuchtturm von Alexandria gefunden. Beschreibungen des Mausoleums liegen vor. In der Renaissancezeit fertigten Künstler wie der Niederländer Maerten van Heemskerck und im Barock der österreichische Architekt Johann Bernhard Fischer von Erlach Darstellungen der Wunder nach ihren Vorstellungen an.

Da die ursprünglichen Weltwunder weitgehend zerstört wurden, wurden Versuche unternommen, neue Listen zu erstellen. Die sieben Weltwunder inspirierten Autoren, andere Bauwerke in die klassische Liste einzufügen, wobei sie auch „jüngere“ Bauwerke wie das Taj Mahal berücksichtigten. Die meisten gehören zum Weltkulturerbe.

In jüngster Vergangenheit ernannte man auch modernere Bauwerke zu „Weltwundern“, die sich durch ihre hohe Baukunst oder auch ihr außergewöhnliches Äußeres von anderen abhoben. Diese Liste beruht nicht auf Rekorden, sondern auf architektonischen Meisterleistungen, die sich im Lauf der Zeit durchgesetzt haben. 1995 erstellte die American Society of Civil Engineers eine Liste der „Sieben Wunder der modernen Welt“:


Ein größeres Medienecho erreichte Mitte der 2000er Jahre die 1998 von dem Schweizer Bernard Weber gegründete Stiftung „NewOpenWorld Foundation“ mit der Wahl der sogenannten „New 7 Wonders of the World“. Ziel war laut Weber, Menschen aus aller Welt durch ihr gemeinsames kulturelles Erbe zu verbinden. Die Wahl erfolgte in einer Kombination aus Online-Wahl und Juryentscheidung in drei Phasen.

In der ersten Phase standen 200 Bauwerke zur Auswahl, über die, nach Angabe der Veranstalter, ca. 20 Millionen Internetnutzer abstimmten. Aus den 77 Bestplatzierten wurden in der zweiten Phase von einer Jury, bestehend aus sieben Architekten (unter anderem Zaha Hadid, Tadao Andō, César Pelli und Harry Seidler) unter dem Vorsitz des ehemaligen UNESCO-Generaldirektors Federico Mayor Zaragoza, 21 Finalisten ausgewählt.

Protest kam aus Ägypten: Kulturminister Farouk Hosny und der Generalsekretär der ägyptischen Altertümerverwaltung Zahi Hawass stellten fest, dass die Wahl keinerlei wissenschaftlichen oder offiziellen Hintergrund habe, und bezeichneten sie als „Unfug“. Daraufhin wurden die Pyramiden von Gizeh aus der Liste genommen und als „ewiges Weltwunder“ deklariert. In der dritten Phase von Januar 2006 bis Juni 2007 konnte per Internet, Telefon oder SMS abgestimmt werden. Laut Veranstalter wurden insgesamt 100 Millionen Stimmen abgegeben. Schloss Neuschwanstein verpasste den Einzug in die Liste nur knapp und wurde auf Platz 8 gewählt.

Am 7. Juli 2007 wurden in Lissabon im Rahmen einer aufwendig inszenierten Fernsehshow die „New 7 Wonders of the World“ bekannt gegeben:

Kritische Reaktionen lösten die Wahlbedingungen aus. Die Umfrage wurde als eindeutig unwissenschaftlich („decidedly unscientific“) kritisiert. In mehreren Ländern gab es Kampagnen von Tourismusministerien, Politikern oder Geschäftsleuten, um die Menschen zur Stimmabgabe für ein bestimmtes Bauwerk aufzurufen. Dabei wurden sie ausdrücklich zur Mehrfachstimmabgabe ermutigt. Prominente Figuren des öffentlichen Lebens betrieben in vielen Ländern Werbung für ihren Kandidaten.

Ein weiterer Kritikpunkt bestand darin, dass die Teilnahme ausschließlich online oder per Telefon möglich war. So konnte zwar grundsätzlich jeder Mensch am Abstimmungsprozess teilnehmen, die Mehrheit hat jedoch keinen Zugang zu diesen Technologien („digitale Kluft“). Weber hielt dem entgegen, dass aus Mali innerhalb einer Woche mehr Stimmen abgegeben worden seien als aus Deutschland insgesamt. Zudem wurde kritisiert, dass auf die telefonische Stimmabgabe Gebühren erhoben wurden.

Nach anfänglicher Unterstützung von Webers Initiative durch das "Office for Partnerships" der Vereinten Nationen distanzierte sich die UNESCO von der Umfrage und stellte fest, dass es sich dabei um eine private Medienkampagne handle, die weder wissenschaftlichen Kriterien folge noch dessen Erhalt und Erforschung diene (im Gegensatz zur Aufnahme eines Bauwerks in das UNESCO-Welterbe).

Aus Ländern Asiens und der so genannten Dritten Welt erntete Weber auch Anerkennung für seine Kampagne, da die sieben antiken Weltwunder ausschließlich im Mittelmeerraum und in Vorderasien standen und die „Neuen Weltwunder“ deshalb als eine gerechtere Darstellung bzw. Auswahl gesehen werden.

Analog zu den von Menschen geschaffenen „Weltwundern“ werden auch verschiedene Naturerscheinungen wie der Grand Canyon in den Vereinigten Staaten oder das Great Barrier Reef vor der Küste Australiens manchmal als solche bezeichnet. Viele davon gehören auch zum Weltnaturerbe.

Auch hier wurde eine weltweite Umfrage des New7Wonders Projekts (siehe oben) durchgeführt. Dabei herausgekommen sind folgende:




</doc>
<doc id="5602" url="https://de.wikipedia.org/wiki?curid=5602" title="Währung">
Währung

Eine Währung () ist im weiteren Sinne die Verfassung und Ordnung des gesamten Geld­wesens eines Staates, die insbesondere die Festlegung des Münz- und Notensystems innerhalb des Währungsraums betrifft. Der Währungsraum ist dabei der Geltungsbereich einer Währung. Sie ermöglicht den Transfer von Waren und Dienstleistungen, ohne eine Gegenleistung in Form von anderen Waren und Dienstleistungen zu liefern.

Als Währung oder Währungseinheit wird auch die vom Staat anerkannte Geldart (das gesetzliche Zahlungsmittel eines Landes) bezeichnet. In diesem Fall ist Währung dann eine Unterform des Geldes. Die meisten Währungen werden an den internationalen Devisenmärkten gehandelt. Der sich dort ergebende Preis wird als Wechselkurs bezeichnet. Nahezu alle gängigen Währungen basieren inzwischen auf dem Dezimalsystem, das heißt, es gibt eine Haupteinheit und eine Untereinheit, wobei die Untereinheit ein dezimaler Bruchteil (i. d. R. ein Hundertstel) des Wertes der Haupteinheit verkörpert (Dezimalwährung).

In den jeweiligen Staaten üben der Finanzminister oder die staatliche Zentralbank Kontrolle über die Währung beziehungsweise die Währungspolitik aus. Die Zentralbanken besitzen in nahezu allen westlichen Staaten ein großes Maß an Autonomie, das heißt die Regierung kann gar nicht oder nur in sehr geringem Maße beziehungsweise indirekt auf die Zentralbank einwirken.

Ist eine Währung weltweit handel- und umtauschbar, so wird von ihrer Konvertibilität gesprochen. Wird eine Währung durch Gold und/oder Silber hinterlegt und ist der Umtausch von Banknoten in das jeweilige Metall jederzeit möglich, so ist auch in diesem Zusammenhang Konvertibilität gegeben.

Derzeit gibt es weltweit über 160 offizielle Währungen, aber nur der US-Dollar und in zunehmendem Maße auch der Euro gelten als internationale Leitwährungen. Daneben gibt es noch Komplementärwährungen, die nur regional neben dem offiziellen Geld als Tauschmittel akzeptiert werden.

Hat eine Währung innerhalb der Bevölkerung stark an Vertrauen verloren, so bilden sich oft Ersatzwährungen wie Zigaretten (z. B. Zigarettenwährung in Deutschland nach dem Zweiten Weltkrieg), die dann als Zahlungs- und Tauschmittel dienen. Auch sogenanntes Notgeld dient in Krisenzeiten als Ersatz für die offizielle Währung. Oftmals werden auch Währungen anderer Staaten zur Ersatzwährung. Ein bekanntes Beispiel ist der Gebrauch der „Westmark“ in der DDR neben der DDR-Mark. Insbesondere die sogenannten „blauen Fliesen“ (100-DM-Scheine) waren ein beliebtes Tauschmittel auf dem Schwarzmarkt.

Der Begriff Währung bezeichnet in einem weiten Sinne die Währungsverfassung, also die gesetzliche Ordnung des Geldwesens eines Staates. Häufiger bezeichnet Währung jedoch das gesetzliche Zahlungsmittel eines Staates. Die meisten Länder haben eine eigene nationale Währung. Eine Ausnahme bildet der Euroraum mit dem Euro als gemeinsamer Währung für 19 Länder (Währungsunion).

Währungen werden von einem Emittenten herausgegeben, heutzutage i. d. R. durch die Zentralbank. Sie ist üblicherweise gesetzlich mit der Herstellung und der Emission der Währung beauftragt. Die als gesetzliches Zahlungsmittel konzipierte Währung ist innerhalb des Staates mit einem gesetzlichen Annahmezwang ausgestattet, das heißt dass ein Gläubiger verpflichtet ist, die Tilgung einer Geldschuld mit dem gesetzlichen Zahlungsmittel zu akzeptieren, sofern nichts anderes wirksam vereinbart wurde. Dadurch wird ihr Wert als Zahlungsmittel gewährleistet. In Deutschland und den übrigen teilnehmenden Mitgliedsstaaten der Europäischen Wirtschafts- und Währungsunion ist seit dem 1. Januar 2002 das Euro-Bargeld gesetzliches Zahlungsmittel: gemäß § 14 Absatz 1 Satz 2 Bundesbankgesetz sind hierbei die von der EZB ausgegebenen Euro-Scheine das einzige unbegrenzte gesetzliche Zahlungsmittel.

Für viele Währungen werden eigene Schriftzeichen (vorwiegend mit Doppelstrich) oder Abkürzungen, die Währungssymbole einer Währungseinheit verwendet, zum Beispiel:

Üblicherweise gibt es zwei verschiedene Abkürzungen: Zum einen ein Zeichen oder ein Buchstabenkürzel ohne genormten Aufbau (z. B. „Fr.“, „SFr.“ oder „sfr“ für Schweizer Franken), das vorwiegend im Inland verwendet wird; zum anderen eine genormte, aus drei Buchstaben bestehende Abkürzung gemäß dem ISO-Standard 4217 (z. B. „CHF“), die vor allem im internationalen Währungshandel verwendet wird.

Um im Ausland einkaufen zu können, muss man i. d. R. das inländische Zahlungsmittel gegen das ausländische Zahlungsmittel tauschen. Auch wenn z. B. ein deutscher Exporteur Waren im Ausland verkauft hat und dafür Geld in ausländischer Währung erhielt, wird er es i. d. R. in inländische Währung umtauschen. Der Umtausch erfolgt zum jeweils gültigen Wechselkurs. Der Wechselkurs ist das Austauschverhältnis zweier Währungen.

Der An- und Verkauf von Währungen erfolgt am Devisenmarkt. Im Rahmen des Tauschs einer Währung in eine andere entstehen Transaktionskosten. Neben Kreditinstituten sind wesentliche Marktteilnehmer auf dem Devisenmarkt auch größere Industrieunternehmen, private Devisenhändler, Devisenmakler und Handelshäuser. Auch die Zentralbanken verschiedener Länder können durch Devisenmarktinterventionen aus wirtschaftspolitischen Gründen in den Devisenmarkt eingreifen. Aufgrund der zunehmenden internationalen Verflechtung hat der internationale Handel mit Währungen am Devisenmarkt in den letzten Jahrzehnten stark an Bedeutung gewonnen. Währungen werden sowohl zu spekulativen Zwecken als auch zu realwirtschaftlich begründeten Tauschzwecken gehandelt.

Seit 1999 ermittelt die Europäische Zentralbank Euro-Referenzkurse für ausgewählte Währungen. Daneben haben die deutschen Banken das Euro-Fixing eingeführt, d. h., es werden täglich Referenzkurse für acht wichtige Währungen (USD, JPY, GBP, CHF, CAD, SEK, NOK, DKK) festgestellt, die als Grundlage für die Währungsgeschäfte der am Euro-Fixing beteiligten Banken dienen.

Währungspolitik sind alle Maßnahmen zur Gestaltung des inneren und äußeren Geldwertes. Währungspolitik im engeren Sinne (= Gestaltung des äußeren Geldwertes) ist die Gestaltung der Währungsbeziehungen mit dem Ausland und die Sicherung des außenwirtschaftlichen Gleichgewichts. Die auf das Inland gerichteten währungspolitischen Maßnahmen werden auch als Geldpolitik bezeichnet. Die Währungspolitik im engeren Sinne kann verschiedene Ziele verfolgen:

Welche dieser zum Teil gegenläufigen Ziele ein Land verfolgt, zeigt sich auch schon in der Wahl des Wechselkurssystems:

Bei einem festen Wechselkurs ist die Zentralbank verpflichtet, den Kurs der eigenen Währung am Devisenmarkt je nach Marktlage durch Käufe oder Verkäufe von Devisen (Devisenmarktinterventionen) stabil zu halten. Beispielsweise haben heutzutage einige Länder ihre nationale Währung an den Wert des Dollar oder des Euro gebunden. Der Vorteil eines festen Wechselkurses ist die Planungssicherheit für international operierende Unternehmen. Wechselkurse sind ein wichtiger Kalkulationsfaktor für den Handel und Kapitalverkehr mit dem Ausland. Wenn z. B. eine Rechnung auf eine Fremdwährung lautet und diese wertet bis zur Bezahlung aufgrund von Wechselkursschwankungen auf, dann wird die erworbene Ware real teurer als zunächst kalkuliert. Der Nachteil von festen Wechselkursen ist, dass es für eine Zentralbank schwer bis unmöglich wird eine eigenständige (nationale) Geldpolitik zu verfolgen.

Heutzutage haben die meisten Währungen flexible Wechselkurse. Der Wechselkurs bildet sich also am Devisenmarkt im Wechselspiel von Angebot und Nachfrage. Währungsschwankungen führen zu Unsicherheit und reduziert die Planungs- und Kalkulationssicherheit international operierender Unternehmen. Durch eine Aufwertung der heimischen Währung verlieren
inländische Unternehmen an Wettbewerbsfähigkeit weil ausländische Waren und Dienstleistungen relativ billiger werden, während gleichzeitig Exporte relativ teurer werden.

Als Währungskrise wird eine volkswirtschaftliche Krise in Form der schnellen und unerwarteten Währungsabwertung bezeichnet. Sie wird durch das ungewollte Aufgeben eines festen Wechselkurses zu einer oder mehreren anderen Währungen oder zum Gold ausgelöst. Ursache oder Folge von Währungskrisen können Finanz- und Wirtschaftskrisen sein.

Obwohl Währungskrisen immer verschieden geartet sind, lassen sich einige "Frühindikatoren" ausmachen, die sehr häufig auftreten. Hierunter fallen (anhaltende) Leistungsbilanzdefizite, starke Devisenzuflüsse in der Kapitalbilanz, ein Anwachsen der kurzfristigen Auslandsverbindlichkeiten, hohes Kreditwachstum sowie starke Preissteigerungen bei Vermögenswerten (insbesondere Immobilien und Aktien).

Nach Ausbruch einer Währungskrise lassen sich wiederum typische "Krisensymptome" feststellen. Dazu gehören zunehmend kürzere Fristen in der Auslandsverschuldung, verstärktes Begleichen von Auslandsverbindlichkeiten mit Fremdwährungen, höhere Zinssätze für Kreditnehmer im Schuldnerland, hohe Werteinbußen von Aktien und Immobilien, Umkehrung der Kapitalströme (Kapitalflucht) sowie starke Verluste an Währungsreserven.

Beispiele für Währungskrisen nach Ende des Bretton-Woods-Systems sind unter anderem die Dollarkrise 1971, die lateinamerikanische Schuldenkrise von 1982/83, die Mexikokrise von 1994/95 (Tequila-Krise), die südostasiatische Finanz- und Währungskrise 1997 (Asienkrise) sowie die Brasilienkrise 1999.

Die klassischen Geldfunktionen (Tauschmedium, Zahlungsmittel, Wertmesser und Wertaufbewahrungsmittel/Wertspeicher) wurden bereits zu Beginn des 3. Jahrhunderts v. Chr. durch Metalle wie Kupfer, Silber, Zinn und Gold erfüllt. Daneben fungierte Getreide als Tauschmittel und Wertmesser. Jedoch war die Palastwirtschaft in Verbindung mit der Oikoswirtschaft sowie die mit ihnen verbundene Subsistenzproduktion hinderlich für die Entwicklung einer Geldwirtschaft, da nicht selbst produzierte Güter meist im Wege des Tausches oder der Dienstverpflichtung beschafft wurden. Münzgeld setzte sich daher erst später und zunächst nur in einigen Wirtschaftszweigen durch.

In Afrika existierten zu jener Zeit die verschiedensten Formen von Währungen. Allen gleich war ihre Funktion als Wertspeicher. So fungierten z. B. Perlen, Elfenbein, Vieh oder auch die Manilla-Währung als Zahlungsmittel. Im 15. Jahrhundert, mit Aufkommen des Sklavenhandels, waren insbesondere die Manilla-Ringe, die als Bezahlung für Sklaven dienten, von Bedeutung.

Im antiken Griechenland existierte zunächst eine ganze Klasse von Gütern, die jeweils einzelne Geldfunktionen verkörperten.

Im Laufe der Zeit setzte sich genau abgewogenes ungemünztes Edelmetall als Zahlungsmittel in den griechischen Poleis durch. Es ist davon auszugehen, dass Geld für die standardisierten öffentlichen Zahlungen in der Polis von entscheidender Bedeutung war. Die ersten richtigen Münzen datieren um ca. 600 v. Chr. und wurden in Westanatolien geprägt. Diese Münzen bestanden aus einer natürlich vorkommenden Silber-Gold-Legierung und wurden höchstwahrscheinlich nur lokal verwendet. Die Verwendung von Münzen setzte sich aber schnell in ganz Griechenland durch, wobei (bedingt durch bessere Gewinnungsmöglichkeiten in Bergwerken – im Gegensatz zur Goldwährung im Persischen Reich) in aller Regel Silber als Münzmetall verwendet wurde (im Ausnahmefall auch Gold und Bronze). Das zugesicherte Gewicht wurde hierbei durch Stempel der Polis garantiert. Wichtigste Währung war die Drachme, welche auch nochmals von 1831 bis 2001 als Währung Griechenlands eingesetzt wurde (Griechische Drachme).

Von einer Geldwirtschaft im eigentlichen Sinne kann jedoch erst Anfang des 5. Jahrhunderts v. Chr. gesprochen werden. Zentrum der antiken Monetarisierung war Athen, dessen Währung im gesamten Mittelmeerraum zirkulierte. Gründe hierfür liegen in der demokratischen Struktur sowie in der Handelsmacht Athens. Erst Alexander der Große führte eine neue bedeutende Währung ein, die Athens Vormachtstellung beendete.

Wie im antiken Griechenland gab es auch in Rom verschiedene Geldformen. Eine Vereinheitlichung hin zu einer allgemein gültigen Währung fand um 500 v. Chr. statt. Geld diente hier zunächst zur Festsetzung von Strafen. Im Zuge der Expansion des Römischen Reiches kamen immer größere Gold-, Silber- und Bronzevorkommen als Kriegsbeute nach Rom. Dies förderte die nun aufkommende großflächige Münzprägung. Zunächst wurden Bronze- und Silbermünzen hergestellt. Es dauerte jedoch relativ lange, bis die römische Münzprägung an den Umfang der griechischen anknüpfen konnte. Im Zuge der Punischen Kriege wurde der Metallgehalt der Münzen reduziert, da immer größere Geldmengen für die Finanzierung des Militärs nötig waren. Andererseits verbreitete sich die römische Währung auch immer mehr in ganz Italien, so dass alle anderen italienischen Städte ihre Münzprägung quasi einstellten. In den neu eroberten Gebieten außerhalb Italiens existierten unzählige verschiedene Währungen, die jedoch mit der römischen Hauptwährung konvertierbar waren.

Infolge weiterer Expansionen flossen immer größere Silbermengen nach Rom, sodass ein Großteil der staatlichen Ausgaben durch die Neuprägung von Silbermünzen finanziert wurde, was in den folgenden Jahrhunderten zunächst zur Geldentwertung und im 3. Jahrhundert n. Chr. zum völligen Zusammenbruch der römischen Silberwährung führte. So besaßen in zunehmendem Maße auch die römischen Bürger kein Vertrauen mehr in immer neue Münzformen, die einen tendenziell immer geringer werdenden Silberanteil besaßen. Die Folge war, dass insbesondere ältere Münzen gehortet beziehungsweise eingeschmolzen wurden. Das Geld verlor dadurch stark an Bedeutung, sodass zum Beispiel der Sold der römischen Soldaten direkt in Getreide ausgezahlt wurde. Als Reaktion ersetzte der Kaiser Konstantin der Große die Silberwährung durch eine stabile Goldwährung.

In der Spätantike kam es schließlich zu einer Neuordnung des Geldsystems, wobei wieder Silbermünzen – diesmal jedoch mit hohem Silbergehalt – sowie Bronzemünzen geprägt wurden. Goldmünzen bestanden aber weiterhin. Ungeachtet dessen verlor die Silbermünze dennoch weiter an Bedeutung, so dass das einst auf Silber- und Bronzemünzen basierende Geldsystem Roms durch ein System einer Gold- und Bronzewährung ersetzt wurde.

Als Basis für das byzantinische Währungssystem diente die unter Konstantin I. eingeführte Goldwährung, der sogenannte Solidus. Diese Währung bestand über etwa 1000 Jahre (5. Jahrhundert v. Chr. bis Einführung des Dinar um 700 n. Chr.). Gründe hierfür sind der hohe Goldgehalt und die daraus folgende Stabilität der Goldwährung. Silber verlor im Zuge dieser Entwicklung immer mehr an Bedeutung. Es bestand jedoch, wie auch Bronzegeld, neben der Goldwährung als Zahlungsmittel fort. Geld besaß in der byzantinischen Gesellschaft einen enorm hohen Stellenwert. Es diente in allen Bereichen der Wirtschaft sowie bei öffentlichen Ausgaben und ermöglichte den internationalen Handel. Dieser brach jedoch infolge wachsender Unsicherheiten (auch Piraterie auf den Handelswegen) fast im ganzen byzantinischen Gebiet zusammen.

Anknüpfend an den bereits erwähnten Solidus entwickelte sich unter Karl dem Großen der schwere Silberdenar, der auch Pfennig genannt wurde. Die Goldzirkulation im Rahmen der staatlichen Institutionen an sich nahm jedoch ab. Andererseits entwickelte sich das Geld in zunehmendem Maße zu einem Tauschmittel, das dem Handels- und Marktgeschehen diente. Die ursprüngliche Goldwährung verlor als Zahlungsmittel an Bedeutung und wurde nur noch als eine Art Wertspeicher gehortet. Im 7. bis 8. Jahrhundert vollzog sich dann der Übergang zur reinen Silberwährung, die lediglich noch den reinen Rechenbezug zum Gold aufwies.

Erstmals verwendet wurde Papiergeld in Form von Banknoten in China. Die Einführung war ein langwieriger und stetiger Prozess, der sich ungefähr von 618 bis 1279 erstreckte. So diente Papiergeld im 10. Jahrhundert zunächst nur auf regional sehr beschränkter Ebene als Erleichterung für die Händler in der staatlichen Salzindustrie. Die Banknotenproduktion wurde in der Folge zwar verstaatlicht, jedoch gab es viele regional verschiedene Währungen. Die eigentliche massenweise Produktion von Banknoten wurde erst mit Erfindung des Drucks mit beweglichen Lettern im 11. Jahrhundert ermöglicht. Mitte des 13. Jahrhunderts wurden die vielen verschiedenen Währungen erstmals zu einer staatlichen Währung vereinheitlicht.

In der islamischen Welt entwickelte sich in der Zeit des 7. bis 12. Jahrhunderts eine starke Geldwirtschaft, die vom erhöhten Handelsumschlag und einer stabilen hochwertigen Währung (dem Dinar) profitierte. In jener Zeit wurden erstmals Kredite, Schecks, Schuldscheine und Sparkonten eingeführt. Auch die notwendigen Bankstrukturen entstanden mit dieser Entwicklung.

Im Jahr 1661 wurden in Schweden erstmals auf europäischer Ebene Banknoten offiziell eingeführt. Zwar besaß Schweden reiche Kupfervorkommen, jedoch besaßen Kupfermünzen einen geringen Zahlwert, so dass große und außerordentlich schwere Münzen geprägt werden mussten. Die Benutzung von Papiergeld stellte somit eine enorme Erleichterung dar.

Die Benutzung von Banknoten offenbarte natürlich viele Vorteile, so dass zum Beispiel die Kreditvergabe spürbar erleichtert wurde und auch der sehr riskante Transport von Gold und Silber entfiel. Weiterhin war es nun erstmals möglich Anteile an Unternehmen in Form von Papier auszugeben.

Auf der anderen Seite bestanden jedoch einige Nachteile, so z. B., dass die Regierungen jetzt in der Lage waren theoretisch unbegrenzt Geld nachzudrucken, um ihren Finanzierungsbedarf (vereinfachte Kriegsfinanzierung) zu decken, da, anders als bei Münzen mit genau definierten Edelmetallgehalt, ein fest hinterlegter Wert der Banknoten nun nicht mehr bestand. Eine mögliche Folge dieser Entwicklung wäre das Einsetzen einer starken Inflation.

Endgültig durchgesetzt hatte sich die nicht an Edelmetall gebundene Papierwährung im 20. Jahrhundert – spätestens in der Weltwirtschaftskrise.

Im Hochmittelalter war das Münzprägerecht ein Privileg, das jeder Adlige anstrebte, denn das Münzregal war ein profitables Hoheitsrecht. Dies führte dazu, dass es viele nicht vergleichbare Währungen gab, bei denen der Edelmetallanteil bei einzelnen Münzarten stark schwanken konnte. Denn im Mittelalter waren Kurantmünzen üblich; der Kurswert fremder Münzen wurde anhand des Edelmetallgehaltes ermittelt. Dies wiederum behinderte den überregionalen Handel. Aus diesen zwei Gründen – Handelserleichterung und Machtkonzentration – verstärkte sich die Tendenz zu nationalen Einheitswährungen.

In der frühen Zeit des Kurantgeldes entsprach der Metallgehalt der Münzen ihrem Nominalwert. Da sich die Münzherren jedoch öfter zu Münzverschlechterung verleiten ließen, um ihren Geldbedarf zu decken, kam es in der Frühen Neuzeit mehrfach zur Inflation. So beruhte beispielsweise die sogenannte Kipper- und Wipperzeit zu Beginn des Dreißigjährigen Krieges auf einer Münzverschlechterung.

Treibend in Europa war Frankreich, das mit seiner Zentralregierung die Münzrechte früh sammelte und dem König unterstellte. Die erste wichtige Währungsreform war die große Münzreform unter Ludwig XIII. 1640–1641, als der Louis d’or eingeführt wurde. Mit der Einführung des Französischen Franc 1795 wurde die erste Dezimalwährung etabliert. Durch Napoleons Feldzüge wurde diese Währung und vor allem deren dezimale Stückelung in Europa verbreitet. Dadurch entstanden in und um Frankreich einige Münzsysteme, die ähnlich aufgebaut waren und wegen der Kurantmünzen hohen Reinheitsgrades feste Wechselkurse bildeten. Dies führte dazu, dass am 23. Dezember 1865 die Lateinische Münzunion gegründet wurde; sie war eine Währungsunion, die aus Frankreich, Belgien, Italien, der Schweiz und Griechenland bestand und klare Vorgaben für die Münzenherstellung gab. Die Länder prägten zwar ihre eigenen Münzen, alle 100er Münzen (100 Franc, 100 Franken, 100 Lire, 100 Drachmen) bestanden aber aus 32,26 g Gold und hatten einen Durchmesser von 35 mm. Der Nachteil der Lateinischen Münzunion war der Bimetallismus, also der feste Umtauschsatz zwischen den Gold- und Silbermünzen (Der Ausdruck Hinkende Währung bezeichnete ein Währungssystem, bei dem zwei Metalle (meist Gold und Silber) gesetzliches Zahlungsmittel waren).

Neben dem starken Preisverfall für Silber gegen Ende des 19. Jahrhunderts brachten die bimetallischen Währungen weitere Probleme mit sich, so dass viele Staaten sich entschieden ihre Währung nur mit Gold zu hinterlegen. Mit der Hinterlegung von Währungen durch Gold sollten die Nachteile, welche die Einführung des Papiergeldes mit sich brachte (insbesondere in Bezug auf die erhöhten Inflationsrisiken), abgefedert werden. Großbritannien war für diese Entwicklung Vorreiter und führte bereits 1817 den Goldstandard ein. Deutschland (1871 im Zuge des Deutsch-Französischen Krieges) und die USA (1900) folgten. Jedoch kam es nicht zu einer generellen Angleichung, das heißt, es gab nach 1880 durchaus unterschiedliche Formen der Goldwährung.

Mit Einführung des Goldstandards entstand die sogenannte „Verpflichtung zur Konvertibilität“, das heißt, es war für jeden Bürger zu jeder Zeit theoretisch möglich, sein Bargeld gegen die entsprechende Menge Gold bei der Zentralbank zu tauschen. Die Goldparität bezeichnet hierbei das Umtauschverhältnis. Dieser reine Goldstandard existierte eigentlich nur in der Theorie. Praktisch fungierte die Hinterlegung der Währung mit Gold jedoch nur als eine Art Absicherung vor zu starker Bargeldinflation (Preisstabilisierung).

Mit Beginn des Ersten Weltkriegs erhöhte sich der Geldbedarf seitens der Regierungen dramatisch. Verstärkt wurde diese Entwicklung noch während der Weltwirtschaftskrise und schließlich durch den Ausbruch des Zweiten Weltkrieges. Viele Staaten rückten nun vom reinen Goldstandard ab und restaurierten ihn zu einem Goldkernstandard. Der direkte Umtausch von Banknoten in Gold war damit ausgeschlossen.

Bereits im Jahr 1944 während des Zweiten Weltkriegs entschieden sich 44 Staaten ein neues Währungssystem einzuführen. Kerngedanke war hierbei laut dem White-Plan die Kopplung der internationalen Währungen an den US-Dollar. Seitens der US-Zentralbank bestand gegenüber der Zentralbank anderer Länder des Bretton-Woods-Systems eine Umtauschpflicht des Dollar in Gold zu einem bestimmten Wechselkurs. Somit entstanden feste Wechselkurse zwischen den jeweiligen Währungen und dem US-Dollar als Ankerwährung.

Weiterhin wurden der Internationale Währungsfonds (IWF) und die Weltbank gegründet. Der IWF sollte die Stabilität des internationalen Währungssystems fördern und bei Schieflagen korrigieren. Er überwachte also de facto die festgelegten Wechselkurse. Dazu diente auch die Einführung von Sonderziehungsrechten durch den IWF.

Die Werthaltigkeit des Dollar als Ankerwährung sollte dadurch gesichert sein, dass die Notenbanken der teilnehmenden Staaten gegenüber der FED das Recht hatten Dollars zu einem Umtauschkurs von 35 $/Feinunze in Gold zu tauschen. Die tatsächliche Eintauschmöglichkeit hing von der größe der Goldreserven der FED ab. 1948 hatte die FED Goldreserven im Wert von 25 Mrd $ (71 % der Weltgoldreserven), denen kurzfristige Auslandsschulden von 18,6 Mrd $ gegenüberstanden. Nach dem Zweiten Weltkrieg hatten fast alle Bretton-Woods-Staaten einen großen Nachholbedarf an Investitions- und Konsumgütern, so dass sie lieber Dollarbestände anhäuften als Dollars in Gold zu tauschen. Aufgrund ständiger Handelsbilanzdefizite der Vereinigten Staaten stieg die Auslandsverschuldung immer weiter an. 1961 verfügte die FED noch über 44 % der Weltgoldreserven, die kurzfristig fälligen Auslandsschulden waren aber bereits um eine Mrd $ höher als der Wert der Goldreserven. Bis 1971 sanken die US-Goldreserven auf 12 Mrd $. Die Zentralbanken der anderen Bretton-Woods-Staaten verfügten 1971 über Dollarreserven von mehr als 50 Mrd $. Das System konnte nur noch solange funktionieren, wie die Bretton-Woods-Staaten bereit waren hohe Dollarreserven zu halten, ohne sie in Gold einzutauschen. Anfang der 1970er Jahre wurde das Bretton-Woods-Abkommen aufgegeben, die Institutionen bestanden jedoch mit teils veränderten Zuständigkeiten fort.

Anfang des Jahres 1973 wurden in den meisten westeuropäischen Staaten und in Japan die Dollarkurse freigegeben. Die Wechselkurse wurden flexibel. In diesem Zusammenhang entstand der Begriff des freien Floatens, welches im Gegensatz zu fest fixierten Wechselkursen stand. Jedoch entschieden sich insbesondere kleinere Volkswirtschaften, die mehr vom internationalen Handel abhingen als zum Beispiel Japan oder die USA, feste Wechselkurse zu behalten. Dies wurde aber mit der Zeit immer schwieriger, da internationale Kapitalbewegungen durch neue Entwicklungen auf dem Gebiet der EDV-Technik und der Telekommunikation immer leichter und schneller vonstattengingen. Zudem gestaltete sich auch die Kontrolle schwieriger.

Bereits kurz nach Einführung der flexiblen Wechselkurse wurde das neue System mit zwei Ölpreisschocks konfrontiert. In der Folge ergaben sich erhebliche Leistungsbilanzüberschüsse (OPEC-Staaten) und -defizite (OECD-Staaten). Dies glich sich jedoch mittelfristig wieder aus.

Erst mit dem 2. IWF-Änderungsabkommen wurde den Mitgliedsstaaten die Wahl des Wechselkurssystems selbst überlassen. Jedoch war dies an die Verpflichtung der einzelnen Staaten gebunden, für stabile Währungs- und Wirtschaftsverhältnisse zu sorgen. Gold verlor somit endgültig seine Stellung als Bezugsgröße.

Die Wechselkurse schwankten in der Folge merklich und veränderten sich auch dauerhaft. Insbesondere die voneinander abhängigen Staaten im westlichen Europa versuchten, sich gemeinsam gegen Wechselkursschwankungen abzusichern, und schufen dafür das Europäische Währungssystem (EWS). Sie strebten stabile Wechselkurse auf Basis der "Stufenflexibilität" an.

Von den flexiblen Wechselkursen profitierte insbesondere der internationale Handel, welcher im Vergleich zu Entwicklung der Bruttoinlandsprodukte überproportional wuchs.

Eine generelle Tendenz für die Entwicklung der Inflation ließ sich hingegen nicht feststellen. So wichen die Inflationsraten in Deutschland und in den USA deutlich voneinander ab.

Nach Beschluss der europäischen Staats- und Regierungschefs (Den Haag 1969) sollte die Europäische Gemeinschaft schrittweise zu einer "Wirtschafts- und Währungsunion" (EWWU) ausgebaut werden. Zunächst wurde ein Europäischer Wechselkursverbund geschaffen (1972) und infolgedessen ein Europäisches Währungssystem (EWS 1979). In dem Vertrag von Maastricht (1992) wurde schließlich die vollständige Währungsintegration beschlossen.

Der Europäische Wechselkursverbund diente dabei zunächst als Instrument, um das Bretton-Woods-System der festen Wechselkurse zumindest noch teilweise zu erhalten. Dadurch sollte die Konvertibilität der einzelnen europäischen Währungen gewährleistet werden (Block-Floating).

Dies scheiterte jedoch und wurde durch das EWS ersetzt, dessen vorrangiges Ziel die Wechselkursstabilität innerhalb der Europäischen Gemeinschaft war. Die Einführung des ECU als Recheneinheit (vgl. Europäische Währungseinheit) war Teil dieser Entwicklung. Weiterhin wurde festgelegt, dass die Wechselkurse nur innerhalb einer gewissen Bandbreite schwanken durften. Diese wurden jedoch infolge der Währungskrisen 1992/1993 stark erweitert. Allen voran ist hier die Pfundkrise vom September 1992 zu nennen, die dazu führte, dass Großbritannien das Europäische Wirtschaftssystem verließ. In unmittelbarem Zusammenhang mit der Pfundkrise ist der amerikanische Investor George Soros zu nennen, der massiv gegen das Britische Pfund spekulierte, indem er in sehr großem Maße Pfund gegen Deutsche Mark und Französische Franc tauschte und damit den Wertverfall des Pfunds noch mehr beschleunigte.

Der entscheidende Schritt zur europäischen Einheitswährung wurde jedoch mit dem Vertrag von Maastricht vollzogen, der die Schaffung der europäischen Wirtschafts- und Währungsunion verankerte. 1998 nahm schließlich die Europäische Zentralbank (EZB) ihre Arbeit auf.

Mit der Einführung des Euro, zunächst als Buchgeld am 1. Januar 1999, existierte in den Teilnehmerländern schließlich erstmals eine gemeinsame europäische Währung.

Am 1. Januar 2002 wurde die Europäische Währungsunion schließlich mit der Einführung der Euro-Banknoten und -Münzen in zunächst zwölf Staaten vollendet.

Alle teilnehmenden Staaten haben sich zur Einhaltung der sogenannten Maastricht-Kriterien (offiziell: EU-Konvergenzkriterien) verpflichtet.

Obwohl die Preise innerhalb der Eurozone gut vergleichbar sind, weicht aufgrund von nationalen Besonderheiten die Kaufkraft beziehungsweise der Binnenwert, also die Menge an Waren und Dienstleistungen, die die Bürger in der Eurozone für einen bestimmten Geldbetrag in einem bestimmten Land kaufen können, voneinander ab.

Im Laufe der Zeit sind weitere Staaten der Eurozone beigetreten (zum Beispiel 2001 Griechenland). Voraussetzung dafür ist die erfolgreiche Teilnahme am Wechselkursmechanismus II (WKM II).

Darüber hinaus haben einige Staaten (z. B. Bosnien-Herzegowina, Bulgarien und einige französische Übersee-Départements) ihre Währung anhand des Currency Boards an den Euro gebunden. Der Euro nimmt dabei die Rolle der Ankerwährung ein, indem er einen festen Wechselkurs zur jeweiligen Heimatwährung hat.

Man unterscheidet folgende Arten von Währungen:

Sie sind dadurch gekennzeichnet, dass hinter dem Wert der Geldeinheit eine ganz bestimmte Menge eines bestimmten Mediums steht. Das sind häufig Edelmetalle.

Man unterscheidet hierbei monometallistische und bimetallistische Währungen.

"Monometallistische Währungen"
Hierbei dient nur genau ein Metall als Währungsmetall (oft Gold → Goldwährung; manchmal Silber → Silberwährung).

"Bimetallistische Währungen"
Hierbei dienen genau zwei Metalle (Gold "und" Silber) als Währungsmetalle.

Darüber hinaus kann eine Währung durch ein Currency Board an eine andere Währung oder Währungskorb gebunden sein. Auch gibt es Währungen, die an einen Warenkorb gebunden sind.


Eine Nebenwährung ist eine Alternativwährung einer Volkswirtschaft. Es handelt sich um eine fremde Währung neben der gesetzlich vorgeschriebenen Währung, die vor allem als Zwischentauschmittel und darüber hinaus auch als Recheneinheit innerhalb eines Währungsgebietes verstärkt auftritt und genutzt wird.





</doc>
<doc id="5603" url="https://de.wikipedia.org/wiki?curid=5603" title="Wilhelm Schickard">
Wilhelm Schickard

Wilhelm Schickard (* 22. April 1592 in Herrenberg; † 23. Oktober 1635 in Tübingen) war ein deutscher Astronom, Geodät und Mathematiker. Er lehrte Hebräisch und Astronomie an der Universität Tübingen. Er gebrauchte seinen Namen auch in den Varianten Schickhart, Schickhard, Schickart, Schickardt und daraus latinisierten Formen.

Wilhelm Schickard wurde als Sohn des Schreiners Lucas Schickhardt und der Pfarrerstochter Margarete Gmelin geboren und war der Neffe des Baumeisters Heinrich Schickhardt und von Wilhelm Gmelin. Er besuchte die Klosterschule in Bebenhausen und wurde 1610 in das Tübinger Stift aufgenommen. An der Universität Tübingen erwarb er 1611 den Magistergrad und studierte anschließend Theologie. Ab 1613 war er Vikar an mehreren Orten in Württemberg, bis er 1614 zum Diakon nach Nürtingen berufen wurde. Dort lernte ihn im Jahr 1617 Johannes Kepler kennen, der nach Tübingen gekommen war, um seine Mutter in einem Hexenprozess zu verteidigen. Für Keplers Werk "Harmonice mundi" schuf er einige Kupferstiche und Holzschnitte.

Schickard gehörte in Tübingen zum Freundeskreis des chiliastischen Juristen und Theosophen Tobias Heß. Zu diesem Kreis zählten beispielsweise auch Johann Valentin Andreae, Christoph Besold, Wilhelm Bidembach von Treuenfels, Abraham Hölzel, Thomas Lansius und Samuel Hafenreffer sowie Johann Jakob Heinlin, der nach Schickards Tod zunächst seine Professur an der Tübinger Universität vertrat.

Das hier abgebildete Porträt von 1632 befindet sich in der Sammlung der Professorenportraits der Universität Tübingen.

Im Jahr 1619 wurde er als Professor für Hebräisch an die Universität Tübingen berufen. Bei seiner Lehrtätigkeit suchte er nach einfachen Verfahren, den Schülern das Lernen zu erleichtern. So schuf er die "Rota Hebræa", eine Darstellung der hebräischen Konjugation in Form zweier drehbarer Scheiben, die übereinander gelegt werden und die jeweiligen Formen in Fenstern erscheinen lassen. Zum Studium der hebräischen Sprache schuf er das "Horologium Hebræum", die hebräische Uhr, ein Lehrbuch des Hebräischen in 24 Kapiteln, die jeweils in einer Stunde zu erlernen waren. Dieses Buch war das bekannteste Buch Schickards und wurde bis zum Jahr 1731 immer wieder neu aufgelegt. Im Jahr 1627 schrieb er ein Lehrbuch zum Erlernen des Hebräischen auf deutsch, den "Hebräischen Trichter".

Neben seinem Lehramt für Hebräisch beschäftigte er sich mit Astronomie. 1623 erfand er ein "Astroscopium", einen aus Papier gefertigten Kegel, in dessen Innerem der Sternenhimmel abgebildet war. Er entwickelte u. a. eine Theorie der Mondbahn, welche die genauesten Ephemeriden seiner Zeit lieferten. Er war der erste, der Meteorbahnen aus gleichzeitigen Beobachtungen von verschiedenen Standorten bestimmte. Seine grafischen Methoden zur Berechnung von Finsternissen und im Kopernikanischen System wurden viel benutzt.

Schickard war ein begabter Mechaniker und baute seine Instrumente vielfach selbst – Kepler nannte ihn in einem Brief deshalb auch einen "beidhändigen Philosophen". 1623 baute er die erste Rechenmaschine (von ihm "Rechenuhr" genannt), um astronomische Rechnungen zu erleichtern. Die Maschine beherrschte das Addieren und Subtrahieren von bis zu sechsstelligen Zahlen, einen „Speicherüberlauf“ signalisierte sie durch das Läuten einer Glocke. Um komplexere Berechnungen (Multiplikation, Division) zu ermöglichen, waren Napiersche Rechenstäbchen (auch Nepersche Stäbchen genannt) in Form von Zylindern – ähnlich den späteren Rechenkästen von Caspar Schott – darauf angebracht, die zur Unterstützung der weiteren Rechenschritte auf der Addiermaschine dienten. Die Konstruktion war bis zum 20. Jahrhundert verloren, und erst 1960 wurde eine funktionierende Replik hergestellt. Hinweise auf die Maschine samt Zeichnungen von Schickard fanden sich im Nachlass von Kepler (Schickard versprach Kepler ein Exemplar, das aber durch Feuer vernichtet wurde) und auch im Nachlass von Schickard selbst. Schickard kannte die Schriften von Napier und war selbst ein früher Vertreter der Verwendung von Logarithmen.

Ab 1624 begann er auf seinen Reisen durch Württemberg als Schulaufseher für die Lateinschulen, das Land neu zu vermessen. Damit ihn dabei andere unterstützen konnten, schrieb er im Jahr 1629 eine Anweisung, wie künstliche Landtafeln zu machen seien. Dabei verwendete er die Methode der geodätischen Triangulation, die einige Jahre zuvor Willebrord Snell erfunden hatte.

Im Jahr 1631 starb der Astronomieprofessor Michael Mästlin, und Schickard wurde zu seinem Nachfolger bestellt. Er hielt von nun an die astronomischen Vorlesungen. Eine seiner wichtigsten Arbeiten betraf die Theorie der Mondbewegung. Zur Berechnung der Mondbahn veröffentlichte er 1631 die "Ephemeris Lunaris", mit der man grafisch die Mondstellung am Himmel zu jedem Zeitpunkt bestimmen konnte. Er war überzeugter Anhänger des heliozentrischen Systems und erfand zu seiner Darstellung das erste Handplanetarium, das auf seinem Porträt von 1631 abgebildet ist. Er korrespondierte neben Kepler mit Astronomen und Wissenschaftlern wie Ismael Boulliau und Pierre Gassendi.

Nach der Schlacht bei Nördlingen 1634 besetzten die kaiserlichen Truppen auch Tübingen, mit ihnen kam die Pest. Im Herbst 1634 starb erst Schickards Mutter an Misshandlungen durch Soldaten, dann starben seine Frau und seine drei Töchter an der Pest, ihm blieb nur sein neunjähriger Sohn. Schickard, der zur Jahreswende selbst an der Pest erkrankte und sich wieder erholte, gelang es, sich mit der Besatzungsmacht zu arrangieren. Im Auftrag von Graf Gronsfeld, der sich für seine mathematischen und mehr noch für seine geodätischen Arbeiten interessierte, führte er von Februar bis Juli 1635 im Gebiet Stuttgart–Herrenberg–Tübingen und im Gebiet Sinzheim–Bruchsal–Pforzheim Vermessungen durch. Mitte Oktober erkrankte er erneut, am 23. Oktober 1635 starb er und wurde am folgenden Tag begraben, sein Sohn am Tag darauf. Sein Nachbar und Patenonkel seiner Kinder, Thomas Lansius, bewahrte den Nachlass mehrere Jahre verschnürt in seinem Keller auf, bis ihn Schickards Bruder Lucas entgegennehmen konnte.

Der Mondkrater Schickard wurde bereits 1651 von Giovanni Riccioli auf seiner Mondkarte benannt.
Die hier abgebildete Skizze der Rechenmaschine findet sich im Schickardschen Skizzenbuch in der Württembergischen Landesbibliothek in Stuttgart. Die Maschine wurde von dem Tübinger Logiker Bruno von Freytag-Löringhoff 1957 rekonstruiert. Rekonstruierte Exemplare können im Tübinger Stadtmuseum, im Computermuseum des Wilhelm-Schickard-Instituts für Informatik in Tübingen und im Arithmeum in Bonn besichtigt werden. Den Mechanismus seiner Rechenmaschine kann man auch im Heinz Nixdorf MuseumsForum (Computermuseum) in Paderborn ausprobieren. Das nach ihm benannte Wilhelm-Schickard-Institut für Informatik befindet sich an der Eberhard Karls Universität Tübingen.

Willhelm Schickard ist Namensgeber der Wilhelm-Schickard-Schule in Tübingen (kaufmännische berufliche Schule) und gilt als einer der Namensgeber des 1962 eröffneten Schickhardt-Gymnasiums in Herrenberg.

Im Jahr 1989 wurde die 1955 gegründete Forschungsgesellschaft für Uhren- und Feingerätetechnik zu Ehren von Wilhelm Schickard und Philipp Matthäus Hahn in Hahn-Schickard-Gesellschaft für angewandte Forschung e. V. (HSG) umbenannt.

Im Technologiepark Karlsruhe gibt es seit 1993 eine Wilhelm-Schickard-Straße.






</doc>
<doc id="5604" url="https://de.wikipedia.org/wiki?curid=5604" title="Wasserstoffbrennen">
Wasserstoffbrennen

Mit Wasserstoffbrennen wird die Kernfusion von Protonen (d. h. von Atomkernen des häufigsten Isotops H des Wasserstoffs) zu Helium im Inneren von Sternen (oder, im Fall einer Nova, auf der Oberfläche eines weißen Zwergs) bezeichnet, also mit anderen Worten die stellare Wasserstofffusion. Diese Reaktion stellt in normalen Sternen während des Großteils ihres Lebenszyklus die wesentliche Energiequelle dar. Alle Sterne der Hauptreihe beziehen ihre Energie aus dem Wasserstoffbrennen. Trotz der Bezeichnung handelt es sich nicht um eine "Verbrennung" im Sinne der chemischen Redoxreaktion, eine solche setzt bedeutend weniger Energie frei.

Der Prozess der Kernfusion kann beim Wasserstoffbrennen auf zwei Arten ablaufen, bei denen auf verschiedenen Wegen jeweils vier Protonen in einen Heliumkern He, zwei Positronen und zwei Elektronneutrinos umgewandelt werden:<ref name="DOI10.1103/PhysRevLett.90.131301">John N. Bahcall, M. C. Gonzalez-Garcia, Carlos Peña-Garay: "Does the Sun Shine by pp or CNO Fusion Reactions?" In: "Physical Review Letters." 90, 2003, .</ref>

Bei der Fusion von vier Protonen zum Heliumkern wird einerseits Materie in Form von zwei Positronen erzeugt, andererseits wird Materie in Energie umgewandelt. Diese Äquivalenz von Masse und Energie wird in der bekannten Formel "E = mc"² von Albert Einstein beschrieben, aufgrund des auftretenden Massendefekts wird eine Energie von etwa 25 MeV frei. Bei der Sonne bedeutet das, dass in jeder Sekunde etwa 564 Millionen Tonnen Wasserstoff zu 560 Millionen Tonnen Helium „verschmolzen“ werden, der Massendefekt also 4 Millionen Tonnen beträgt.

Der Massendefekt bei der Fusion von Wasserstoff zu Helium ist der größte aller Fusionsreaktionen und somit bezüglich der Energie am ergiebigsten; die nächste Stufe stellarer Fusionsreaktionen, das Heliumbrennen, setzt pro erzeugtem Kohlenstoffkern nur noch etwa ein Zehntel davon frei. 

Die Energieerzeugungsrate ist bei der Proton-Proton-Reaktion proportional zur 4. Potenz der Temperatur, beim Bethe-Weizsäcker-Zyklus zur 18. Potenz. Mithin bewirkt eine Erhöhung der Temperatur um 5 % eine Steigerung von 22 % bzw. 141 % bei der Energiefreisetzung. Beim Heliumbrennen (27. Potenz) und Kohlenstoffbrennen (30. Potenz) liegen diese Werte nochmals deutlich höher.




</doc>
<doc id="5605" url="https://de.wikipedia.org/wiki?curid=5605" title="Wasserzeichen">
Wasserzeichen

Wasserzeichen sind in Papier durch unterschiedliche Papierstärken eingebrachte, mittels Lichtdurchlass erkennbare Bildmarken und dienten ursprünglich zur Kennzeichnung der herstellenden Papiermühle. Sie werden unter anderem als ein Sicherheitsmerkmal bei Banknoten und Briefmarken sowie in Personaldokumenten eingesetzt. Da Wasserzeichen ein bestimmtes Vorgehen benötigen, um wahrnehmbar zu sein (Papier gegen das Licht halten), werden "nicht wahrnehmbare" Markierungen in digitalen Inhalten, die ebenfalls nur mit einem vorgegebenen Verfahren detektiert werden können, als digitale Wasserzeichen bezeichnet.

Nachweislich seit 1282 im italienischen Bologna kennzeichneten die Papiermühlen ihre Ware, indem sie auf dem Drahtgeflecht des Schöpfsiebes einen dickeren Draht in Form eines Buchstabens oder eines Symbols befestigten. Um 1800 wurde das erste mehrschichtige, 3D-Wasserzeichen hergestellt. Die Einführung der „Dandy Rolle“ in Form eines Egoutteurs im Jahr 1826 von John Marshall revolutionierte den Wasserzeichenprozess und machte es einfacher für die Produzenten. Diese Figur hinterlässt einen Abdruck im Papier; die Faserschicht ist dort dünner, und bei durchscheinendem Licht wird das Wasserzeichen als transparenteres Bild sichtbar. Umgekehrt, konnte man durch ein partielles Vertiefen des Siebes eine Stoffanreicherung erzielen, die dadurch in der Ansicht ein trüberes Aussehen bekam. Durch geschicktes Kombinieren zwischen Vertiefung und Erhöhung lassen sich sogar "Halbtöne" simulieren. Dieses aufwendige Verfahren wird heute noch für Wasserzeichen in Banknoten angewendet. In der Frühzeit der europäischen Papierherstellung waren Wasserzeichen Herkunfts- und Geschäftszeichen der Papiermühlen.

Da die Siebe zur Papierherstellung nach rund zwei Jahren verschlissen waren, ermöglichte die Erfassung von datierbaren Wasserzeichen mit leichten Abweichungen, die durch die handwerkliche Fertigung entstanden, die Datierung des Papiers. Weil Papier wegen seines hohen Preises meist nicht lange gelagert wurde, ergibt sich damit ein vergleichsweise genaues Instrument zur Datierung der auf dem Papier überlieferten Dokumente und Grafiken. Zu diesem Zweck sind seit Beginn des 20. Jahrhunderts umfangreiche historische Wasserzeichensammlungen angelegt worden. Das erste umfangreiche Werk stellt Charles-Moïse Briquets "dictionnaire des filigranes" dar, ihm folgte die bisher noch unvollständig gedruckte Sammlung Gerhard Piccards im Landesarchiv Baden-Württemberg. Weitere große Sammlungen von Wasserzeichen befinden sich im Deutschen Buch- und Schriftmuseum der Deutschen Nationalbibliothek in Leipzig und im Papiermuseum Düren.

Die oben genannten und ein wachsender Kreis weiterer Wasserzeichenkarteien liegen inzwischen teilweise digitalisiert vor. Besonders wird dabei auf folgende Sammlungen hingewiesen:
Durch die Digitalisierung weiterer Sammlungen wird in zunehmendem Maße auch die Lücke bei der Kartierung der im östlichen Europa verwendeten Papiere geschlossen.

Die Datierung der Wasserzeichen erfolgt in erster Linie durch die Bestimmung des verwendeten Symbols gemäß der Systematik der Karteien und zusätzlich durch Höhe und Breite des Wasserzeichens, sowie Abstand der Stegdrähte. Zur Datierung und Lokalisierung der Wasserzeichen ferner auch deren Sitz innerhalb des Schöpfsiebes von Bedeutung, da sich Wasserzeichen einerseits durch Abnutzung verziehen konnten, andererseits Wasserzeichen auch von einem Schöpfsieb abgenommen und auf ein anderes aufgebracht werden konnten; eine Feinbestimmung erfordert ferner auch die Berücksichtigung des Abstandes der Kett- und Schussfäden sowie der Befestigungspunkte des Wasserzeichengebildes auf dem Sieb.

Eine Unterscheidung erfolgt in echte, halbechte und unechte Wasserzeichen.

Echte Wasserzeichen entstehen heutzutage meist in der Siebpartie der Papiermaschine. Die verfilzten Fasern sind noch sehr feucht, und eine fest positionierte rotierende Walze (Egoutteur) auf der Siebpartie verdünnt oder verdichtet partiell die Papierbahn an vorgegebener Stelle. Bei den echten Wasserzeichen wird unterschieden in "Anlagerungswasserzeichen" und "Verdrängungswasserzeichen". Anlagerungswasserzeichen können ausschließlich auf der Rundsiebmaschine oder – so wie im Originalverfahren – auf dem Handsieb gefertigt werden.

Das "Anlagerungswasserzeichen" entsteht durch eine gestörte Ablagerung der Fasern während des Formierungsprozesses (Übergang der Faser vom Schwimmen zum Liegen). Je nach Drahtform und Faserlänge kann das Anlagerungswasserzeichen schärfer oder unschärfer ausfallen. Das typische Anlagerungswasserzeichen am "Runddraht" erkennt man leicht dadurch, dass beim Abgautschen ein Teil der unter den Draht geschwemmten Fasern abgerissen wird und somit der Rand immer etwas unscharf ist. Trapezdrähte oder Rechteckdrähte, die flach auf dem Sieb aufliegen, zeigen ein messerscharfes Abbild (kurzer Zellstoff). Es wird zwischen zwei Arten von Anlagerungswasserzeichen unterschieden:



Das "Verdrängungswasserzeichen" entsteht auf der Egotteurpartie und ist dadurch gekennzeichnet, dass die Faser im Moment der Formierung durch einen von oben einwirkenden Egotteurdraht verdrängt wird. Das dabei entstehende Wasserzeichen ist dadurch gekennzeichnet, dass es immer etwas unscharf (der verwendeten Drahtform und Elastizität des Vlieses wegen) und oftmals mit leichten Quetschrändern versehen ist. Durch den Einlaufdruck dringt Stoffwasser durch das Sieb in das Innere des Egoutteurs ein. Die Strömung wird dadurch so orientiert, dass in der Suspension frei schwimmende Fasern das Gewebe erreichen und dort entflockt werden. Anschließend können die Fein- und Füllstoffe noch leicht verfließen und so eine sehr feine, gleichmäßige Oberfläche bilden.

Um echte "Schatten-Wasserzeichen" zu erzeugen werden nun bei der Herstellung des Egoutteursiebs Stellen erhitzt und eingedrückt. In diesen Aussparungen sammeln sich Papierfasern und eine partielle Verdickung findet statt. "Helle Wasserzeichen" entstehen durch erhabene Stellen auf der Siebwalze. Dazu werden rostfreie Drähte, genannt "Elektrodrähte", auf die Oberseite des Drahtgitters gelötet. Diese verdrängen die feuchten Papierfasern und führen so eine geringere Faserkonzentration bei.

Die Stärke des Wasserzeichens kann gesteuert werden, indem der Druck des Egoutteurs auf das Papiervlies erhöht wird oder indem der Egoutteur mit einer größeren Geschwindigkeit als das Papierflies angetrieben wird.

Im Anlagerungsverfahren lassen sich auch noch Zellstoffe markieren, die im Verdrängungsverfahren nicht mehr sauber markiert werden können (Langfaser). Für beide Wasserzeichenformen gilt: Die gewählte Drahtstärke ist abhängig von der späteren Grammatur und der Stärke des gewünschten Zeichens. Für die Handpapiermacherei mit einem Flächengewicht im Bereich von 40 bis 300 g/m eignen sich Drähte von ca. 0,4 bis 1,2 mm.

Halbechte Wasserzeichen (beispielsweise die "Molette-Wasserzeichen") entstehen durch Einpressen in die bereits wesentlich trockenere Papierbahn nach dem Verlassen der Siebpartie (meist in oder nach der ersten Presse). Sie lassen sich nachträglich durch partielle Einwirkung von Natronlauge oder Wasser größtenteils wieder entfernen. Im Gegensatz zum echten Wasserzeichen wird hier die Papierfaserbahn vorrangig geprägt und nicht wesentlich in der Faserstruktur verändert. Es findet keine nennenswerte Reduzierung oder Vergrößerung der Faserbreidicke statt. Molette-Wasserzeichen werden meist mit auf die Molette aufgezogenen Hartgummiringen (ähnlich wie Stempel oder Buchdrucktypen beschaffen) gefertigt. Typische Anwendungen für Molette-Zeichen sind längs des Randes durchlaufende Schriftzüge parallel zur Laufrichtung des Papiers. Seltener werden Molette-Zeichen als "Passwasserzeichen" gefertigt. Die Verwendung von Ornamenten beschränkt sich bei der Molette auf einfache Linienformen. Flächige oder Schattenwasserzeichen lassen sich auf diese Weise nicht erzeugen.

Die zweite Form der halbechten Wasserzeichen ist das "Kalanderwasserzeichen". Diese werden beim Kalandrieren (Glätten durch Kombination von Friktion und Druck im Walzenwerk [= Kalander], historisch auch ‚Kalender‘ oder ‚Calender‘) des Papiers in die vollständig trockene Papierbahn eingeprägt. Das Papier wird dabei hart verdichtet. Das Kalanderwasserzeichen kann durch Quellung mit Wasser aus dem Papier entfernt werden.

Kalanderwasserzeichen erkennt man an der extrem harten Randkontur und der schnittartigen Einkerbung des Papiers. Molette-Wasserzeichen und Kalanderwasserzeichen sind oftmals nicht genau voneinander abgrenzbar. Kalanderwasserzeichen eignen sich vorzugsweise für Linienformen. Es sind aber auch flächige Formen bekannt.

Unechte Wasserzeichen (chemische Wasserzeichen) sind keine Wasserzeichen im eigentlichen Sinne, da sie nicht bei der Papierherstellung (also nicht durch Wasser) entstehen. Meist werden sie nachträglich aufgedruckt. Sehr häufig wird hier Transparentmasse (z. B. Glycerin, Fettfarbträger oder Schwefelsäurepaste → Merzerisation) verdruckt, die durch Zusatz von Farbpigmenten ein echtes Wasserzeichen simulieren soll. Auch eine nachträgliche Prägung in das Papier (ähnlich dem Kalandrieren), entspricht dem eines unechten Wasserzeichens.

Eine moderne Form der Wasserzeichenfälschung für die schwierig zu kopierenden Schattenwasserzeichen (Anlagerungswasserzeichen) besteht in der Blattfertigung aus zwei Blättern halber Enddicke. Dabei wird eines der Blätter vor dem Verkleben mit dem Motiv bedruckt (später innen liegend), das dann in der Durchsicht wie ein Schattenwasserzeichen erscheint. Derartige Blätter lassen sich einfach mit der Brennprobe oder einem Saugtest prüfen (siehe Karton-/Pappen­fertigung).

Unechte Wasserzeichen sind leicht an der fehlenden Verdünnung/Verdickung des Papiers oder am erkennbaren Farbauftrag erkennbar und lassen sich rückstandsfrei durch chemische Reagenzien (z. B. Alkohole) aus dem Papier entfernen.

Darüber hinaus werden heutzutage Wasserzeichen auf der Basis von Ultraviolettstrahlung (UV) zunehmend als Sicherheitsmerkmal für schützenswerte Papiere eingesetzt. UV-Wasserzeichen sind im Gegenlicht hell sichtbar und erscheinen unter Schwarzlicht fluoreszierend. Zur Erhöhung des Sicherheitslevels können diese UV-Wasserzeichen in gestrichenen und farbigen Papieren eingesetzt werden.

Digitale Wasserzeichen sind in Mediendateien eingebrachte Informationen. Wesentliches Ziel dabei ist meist (wie bei einem traditionellen Wasserzeichen), dass die eigentlichen Informationen und diejenigen, die das Wasserzeichen ausmachen, nicht mehr voneinander getrennt werden können. Es gibt für jeden Medientyp (Bilder, Audio, Video) ein eigenes Verfahren, das an die jeweilige Codierung und das Datenformat angepasst ist. Die angewandten Techniken sind eng verwandt mit der Steganographie.





</doc>
<doc id="5607" url="https://de.wikipedia.org/wiki?curid=5607" title="Wissenschaft">
Wissenschaft

Die Wissenschaft (mittelhochdeutsch wizzen[t]schaft = Wissen, Vorwissen, Genehmigung für lat. "scientia") ist der Inbegriff der Gesamtheit des menschlichen Wissens, der Erkenntnisse und der Erfahrungen einer Zeitepoche, welches systematisch erweitert, gesammelt, aufbewahrt, gelehrt und tradiert wird.

Die Wissenschaft ist ein System der Erkenntnisse über die wesentlichen Eigenschaften, kausalen Zusammenhänge und Gesetzmäßigkeiten der Natur, Technik, Gesellschaft und des Denkens, das in Form von Begriffen, Kategorien, Maßbestimmungen, Gesetzen, Theorien und Hypothesen fixiert wird.<ref name="Klaus/Buhr">Artikel „Wissenschaft“. In: Georg Klaus, Manfred Buhr (Hrsg.): "Philosophisches Wörterbuch." 11. Aufl., Leipzig 1975.</ref>

Die Wissenschaft ist auch die Gesamtheit von Erkenntnissen und Erfahrungen, die sich auf einen Gegenstandsbereich beziehen und in einem Begründungszusammenhang stehen. Das Wissen eines begrenzten Gegenstandsbereichs kennzeichnet die Einzelwissenschaft, die sich in einen theoretischen und einen angewandten Bereich gliedert und mit fortschreitender Differenzierung eine Reihe von Teildisziplinen hervorbringen kann.

Mit Wissenschaft ist auch der methodische Prozess intersubjektiv nachvollziehbaren Forschens und Erkennens in einem bestimmten Bereich gemeint, der nach herkömmlichem Verständnis ein begründetes, geordnetes und gesichertes Wissen hervorbringt. Methodisch kennzeichnet die Wissenschaft entsprechend das gesicherte und im Begründungszusammenhang von Sätzen gestellte Wissen, welches kommunizierbar und überprüfbar ist sowie bestimmten wissenschaftlichen Kriterien folgt. Wissenschaft bezeichnet somit ein zusammenhängendes System von Aussagen, Theorien und Verfahrensweisen, das strengen Prüfungen der Geltung unterzogen wurde und mit dem Anspruch objektiver, überpersönlicher Gültigkeit verbunden ist.

Zudem bezeichnet Wissenschaft auch die Gesamtheit der wissenschaftlicher Institutionen und der dort tätigen Wissenschaftler.

Das deutsche Wort Wissenschaft ist ein Kompositum, das sich aus dem Wort Wissen (von indogermanisch "*u̯e(i)d" bzw. "*weid-" für erblicken, sehen) und dem althochdeutschen Substantiv "scaf(t)" bzw. "skaf(t)" (Beschaffenheit, Ordnung, Plan, Rang) zusammensetzt. Wie viele andere deutsche Komposita mit der Endung "-schaft" auch, ist es im Zuge der substantivischen Wortbildung des Althochdeutschen im Mittelalter entstanden. Dabei wurde das früher selbstständige Substantiv "scaf(t)" bzw. "skaf(t)" zur Nachsilbe. In diesem Sinne bezeichnet es die Beschaffenheit bzw. Ordnung des Wissens.

Die Geschichte und Entwicklung der Wissenschaft wird in der akademischen Disziplin der Wissenschaftsgeschichte erforscht. Die Entwicklung des menschlichen Erkennens der Natur der Erde und des Kosmos und die geschichtliche Entstehung der Naturwissenschaften ist ein Teil davon, zum Beispiel die Geschichte der Astronomie und die Geschichte der Physik. Zudem bestehen Verbindungen zu den Anwendungswissenschaften der Mathematik, Medizin und Technik. Bereits Thales forderte, dass Wissenschaft beweisbar, nachprüfbar bzw. in ihren Ergebnissen wiederholbar und zweckfrei sei. Die philosophische Beschäftigung mit wissenschaftstheoretischen Kenntnissen und Methoden geht geschichtlich zurück bis auf Aristoteles in der Antike, heute Wissenschaftstheorie genannt.

Eine frühe dokumentierte Form eines organisierten wissenschaftsähnlichen Lehrbetriebs findet sich im antiken Griechenland mit der Platonischen Akademie, die (mit Unterbrechungen) bis in die Spätantike Bestand hatte. Wissenschaft der Neuzeit findet traditionell an Universitäten statt, inzwischen auch an anderen Hochschulen, die auf diese Idee zurückgehen. Daneben sind Wissen schaffende Personen (Wissenschaftler) auch an Akademien, Ämtern, privat finanzierten Forschungsinstituten, bei Beratungsfirmen und in der Wirtschaft tätig. In Deutschland ist eine bedeutende öffentliche „Förderorganisation“ die Deutsche Forschungsgemeinschaft, die projektbezogene Forschung an Universitäten und außeruniversitären Einrichtungen fördert. Daneben existieren „Forschungsträgerorganisationen“ wie etwa die Fraunhofer-Gesellschaft, die Helmholtz-Gemeinschaft Deutscher Forschungszentren, die Max-Planck-Gesellschaft und die Leibniz-Gemeinschaft, die – von Bund und Ländern finanziert – eigene Forschungsinstitute betreiben. In Österreich entsprechen der DFG der Fonds zur Förderung der wissenschaftlichen Forschung (FWF) sowie die Österreichische Forschungsförderungsgesellschaft (FFG), in der Schweiz und Frankreich die nationalen Forschungsfonds. Andere Fonds werden z. B. von Großindustrien oder dem Europäischen Patentamt dotiert.

Neben den wissenschaftlichen Veröffentlichungen erfolgt der Austausch mit anderen Forschern durch Fachkonferenzen, bei Kongressen der internationalen Dachverbände und "scientific Unions" (z. B. IUGG, COSPAR, IUPsyS, ISWA, SSRN) oder der UNO-Organisation. Auch Einladungen zu Seminaren, Institutsbesuchen, Arbeitsgruppen oder Gastprofessuren spielen eine Rolle. Von großer Bedeutung sind auch Auslandsaufenthalte und internationale Forschungsprojekte.

Für die interdisziplinäre Forschung wurden in den letzten Jahrzehnten eine Reihe von Instituten geschaffen, in denen industrielle und universitäre Forschung zusammenwirken (Wissenschaftstransfer). Zum Teil verfügen Unternehmen aber auch über eigene Forschungseinrichtungen, in denen Grundlagenforschung betrieben wird.

Die eigentliche Teilnahme am Wissenschaftsbetrieb ist grundsätzlich nicht an Voraussetzungen oder Bedingungen geknüpft: Die wissenschaftliche Betätigung außerhalb des akademischen oder industriellen Wissenschaftsbetriebs steht jedermann offen und ist auch gesetzlich von der Forschungsfreiheit abgedeckt. Universitäten bieten außerdem die voraussetzungslose Teilnahme am Lehrbetrieb als Gasthörer an. Wesentliche wissenschaftliche Leistungen außerhalb eines beruflichen Rahmens sind jedoch die absolute Ausnahme geblieben. Die staatlich bezahlte berufliche Tätigkeit als Wissenschaftler ist meist an die Voraussetzung des Abschlusses eines Studiums gebunden, für das wiederum die Hochschulreife notwendig ist. Leitende öffentlich finanzierte Positionen in der Forschung und die Beantragung von öffentlichen Forschungsgeldern erfordern die Promotion, die Professur die Habilitation. In den USA findet sich statt der Habilitation das Tenure-Track-System, das 2002 in Form der Juniorprofessur auch in Deutschland eingeführt werden sollte, wobei allerdings kritisiert wird, dass ein regelrechter Tenure Track, bei dem den Nachwuchswissenschaftlern für den Fall entsprechender Leistungen eine Dauerstelle garantiert wird, in Deutschland nach wie vor eine Ausnahme darstellt.

Dementsprechend stellt die Wissenschaft durchaus einen gewissen Konjunkturen unterliegenden Arbeitsmarkt dar, bei dem insbesondere der Nachwuchs angesichts der geringen Zahl an Dauerstellen ein hohes Risiko eingeht. Besonders die gestiegene Beteiligung von Frauen an Promotion und Habilitation sowie die mit den neueren hochschulpolitischen Entwicklungen einhergehende Fokussierung und somit Beschneidung der thematischen Breite von Lehre und Forschung führt auf diesem zu einem erhöhten Konkurrenzdruck.

Für die Wissenschaftspolitik an Bedeutung gewonnen hat die Wissenschaftsforschung, die wissenschaftliche Praxis mit empirischen Methoden zu untersuchen und zu beschreiben versucht. Dabei kommen unter anderem Methoden der Scientometrie zum Einsatz. Die Ergebnisse der Wissenschaftsforschung haben im Rahmen der Evaluation Einfluss auf Entscheidungen.

Gesellschaftliche Fragen innerhalb des Wissenschaftsbetriebs sowie die gesellschaftlichen Zusammenhänge und Beziehungen zwischen Wissenschaft, Politik und übriger Gesellschaft untersucht die Wissenssoziologie.

Die Wissenschaftstheorie ist sowohl ein Teilgebiet der Philosophie als auch eine Hilfswissenschaft der einzelnen Fachgebiete, zum Beispiel als Philosophie der Naturwissenschaft. Sie beschäftigt sich mit dem Selbstverständnis von Wissenschaft in Form der Analyse ihrer Voraussetzungen, Methoden und Ziele. Dabei wird besonders ihr Wahrheitsanspruch kritisch hinterfragt. Für die Forschung, die nach neuen Erkenntnissen sucht, ist insbesondere die Frage nach den Methoden und Voraussetzungen der Erkenntnisgewinnung von Bedeutung. Diese Frage wird in der Erkenntnistheorie behandelt.

Die Forschung beginnt mit einer Fragestellung, die sich aus früherer Forschung, einer Entdeckung oder aus dem Alltag ergeben kann. Der erste Schritt besteht darin, die Forschungsfrage zu beschreiben, um ein zielgerichtetes Vorgehen zu ermöglichen. Forschung schreitet in kleinen Schritten voran: Das Forschungsproblem wird in mehrere, in sich geschlossene Teilprobleme zerlegt, die nacheinander oder von mehreren Forschern parallel bearbeitet werden können. Bei dem Versuch, sein Teilproblem zu lösen, steht dem Wissenschaftler prinzipiell die Wahl der Methode frei. Wesentlich ist nur, dass die Anwendung seiner Methode zu einer Theorie führt, die objektive, d. h. intersubjektive nachprüfbare und nachvollziehbare Aussagen über einen allgemeinen Sachverhalt macht und dass entsprechende Kontrollversuche durchgeführt wurden.

Wenn ein Teilproblem zur Zufriedenheit gelöst ist, beginnt die Phase der Veröffentlichung. Traditionell verfasst der Forscher dazu selbst ein Manuskript über die Ergebnisse seiner Arbeit. Dieses besteht aus einer systematischen Darstellung der verwendeten Quellen, der angewendeten Methoden, der durchgeführten Experimente und Kontrollexperimente mit vollständiger Offenlegung des Versuchsaufbaus, der beobachteten Phänomene (Messung, Interview), gegebenenfalls der statistischen Auswertung, Beschreibung der aufgestellten Theorie und die durchgeführte Überprüfung dieser Theorie. Insgesamt soll die Forschungsarbeit also möglichst lückenlos dokumentiert werden, damit andere Forscher und Wissenschaftler die Arbeit nachvollziehen können.

Sobald das Manuskript fertig aufgesetzt wurde, reicht es der Forscher an einen Buchverlag, eine wissenschaftliche Fachzeitschrift oder Konferenz zur Veröffentlichung ein. Dort entscheidet zuerst der Herausgeber, ob die Arbeit überhaupt interessant genug und thematisch passend z. B. für die Zeitschrift ist. Wenn dieses Kriterium erfüllt ist, reicht er die Arbeit für die Begutachtung (Wissenschaftliches Peer-Review) an mehrere Gutachter weiter. Dies kann anonym (ohne Angabe des Autors) geschehen. Die Gutachter überprüfen, ob die Darstellung nachvollziehbar und ohne Auslassungen ist und ob Auswertungen und Schlussfolgerungen korrekt sind. Ein Mitglied des Redaktionskomitees der Zeitschrift fungiert dabei als Mittelsmann zwischen dem Forscher und den Gutachtern. Der Forscher hat dadurch die Möglichkeit, grobe Fehler zu verbessern, bevor die Arbeit einem größeren Kreis zugänglich gemacht wird. Wenn der Vorgang abgeschlossen ist, wird das Manuskript gesetzt und in der Zeitschrift abgedruckt. Die nunmehr jedermann zugänglichen Ergebnisse der Arbeit können nun weiter überprüft werden und werfen neue Forschungsfragen auf.

Der Prozess der Forschung ist begleitet vom ständigen regen Austausch unter den Wissenschaftlern des bearbeiteten Forschungsfelds. Auf Fachkonferenzen hat der Forscher die Möglichkeit, seine Lösungen zu den Forschungsproblemen, die er bearbeitet hat (oder Einblicke in seine momentanen Lösungsversuche), einem Kreis von Kollegen zugänglich zu machen und mit ihnen Meinungen, Ideen und Ratschläge auszutauschen. Zudem hat das Internet, das zu wesentlichen Teilen aus Forschungsnetzen besteht, den Austausch unter Wissenschaftlern erheblich geprägt. Während E-Mail den persönlichen Nachrichtenaustausch bereits sehr früh nahezu in Echtzeit ermöglichte, erfreuten sich auch E-Mail-Diskussionslisten zu Fachthemen großer Beliebtheit (ursprünglich ab 1986 auf LISTSERV-Basis im BITNET).

Lehre ist die Tätigkeit, bei der ein Wissenschaftler die Methoden der Forschung an Studenten weitergibt und ihnen einen Überblick über den aktuellen Forschungsstand auf seinem Fachgebiet vermittelt. Dazu gehören

Zu den Voraussetzungen zur Teilnahme an der Lehre als Student und den Formen sowie Abläufen siehe Studium.
Die Werte der Wissenschaft sind darauf ausgerichtet, eine möglichst präzise und wertefreie Beschreibung des Analysierten zu liefern.


Ein klassisches Ideal – das auf Aristoteles zurückgeht – ist die völlige Neutralität der Forschung. Sie sollte autonom, rein, voraussetzungs- und wertungsfrei sein („tabula rasa“). Dies ist in der Praxis nicht völlig möglich und mitunter kritisierbar. Bereits die Auswahl des Forschungsgegenstandes kann subjektiven Einschätzungen unterliegen, die die Neutralität der Ergebnisse in Frage stellt. Ein Beispiel dafür ist die Tatsache, dass männliche Primatenforscher in den 1950er und 1960er Jahren vor allem Paviane untersuchten, die für ihre dominanten Männchen bekannt sind. Weibliche Primatologinnen in den 1970er Jahren untersuchten hingegen vorzugsweise Arten mit dominanten Weibchen (z. B. Languren). Dass die Absichten der Forscher dabei auf Zusammenhänge zu den Geschlechterrollen der Menschen abzielten, ist offensichtlich.

Karl Popper betrachtete den "Wert der Wertefreiheit" als Paradoxon und nahm die Position ein, dass Forschung positiv von Interessen, Zwecken und somit einem Sinn geleitet sein sollte (Suche nach Wahrheit, Lösung von Problemen, Verminderung von Übeln und Leid). Wissenschaft soll demnach immer eine kritische Haltung gegenüber eigenen wie fremden Ergebnissen einnehmen; falsche Annahmen sind immer einer Kritik zugänglich. Ebenfalls bezweifelt wurde von ihm, dass Wissenschaft begründet und gesichert sei, was von Kritikern wie David Stove bereits als eine Spielart des Irrationalismus betrachtet wird. Kritische Theorien wie der Sozialkonstruktivismus und der Poststrukturalismus und verschiedene Spielarten des Relativismus bestreiten ganz, dass Wissenschaft unabhängig von den Prägungen und Beschränkungen menschlicher Kultur so etwas wie wertfreies und objektives Wissen erlangen könne.

Richard Feynman kritisierte vor allem die nach seiner Ansicht sinnlos gewordene Forschungspraxis der von ihm so bezeichneten "Cargo-Kult-Wissenschaft", bei der Forschungsergebnisse unkritisch übernommen und vorausgesetzt werden, so dass zwar oberflächlich betrachtet eine methodisch korrekte Forschung stattfindet, jedoch die wissenschaftliche Integrität verloren gegangen ist.

Mit Massenvernichtungswaffen, Gentechnik und Stammzellenforschung sind im Laufe des 20. Jahrhunderts vermehrt Fragen über ethische Grenzen der Wissenschaft (siehe Wissenschaftsethik) entstanden.
Bereits Aristoteles gliederte die Wissenschaft in Teilbereiche, so genannte Einzelwissenschaften. Dabei hielt er die Geometrie und Arithmetik für ungeeignet sich mit Lebewesen wissenschaftlich zu befassen. Die klassische neuzeitliche Aufteilung folgt unterschiedlichen Gesichtspunkten. Dem Ziel nach als rein theoretische (Methodenlehre, Grundlagenforschung) oder praktisch angewandte Wissenschaft oder der Erkenntnisgrundlage nach (empirischen) Erfahrungs- oder (rationale) Vernunftwissenschaften. Die Einteilung der Wissenschaft ist insbesondere für organisatorische Zwecke (Fakultäten, Fachbereiche) und für die systematische Ordnung von Veröffentlichungen von Bedeutung (z. B. Dewey Decimal Classification, Universelle Dezimalklassifikation).

Vermehrt gibt es die Bestrebung, disziplinübergreifende Bereiche zu etablieren und so Erkenntnisse einzelner Wissenschaften gewinnbringend zu verknüpfen.

Die Unterscheidung in Natur-, Geistes- und Sozialwissenschaften ist verbreitet. Die Natur- und Sozialwissenschaften werden oft als empirische Wissenschaften () bezeichnet und den Geisteswissenschaften () nach Gegenstand und Methode entgegengesetzt. Mit der zunehmenden Verwissenschaftlichung und Differenzierung kamen immer neuere Wissenschaftszweige hinzu, die eine Klassifizierung erschweren. Die verschiedenen zweckgebundenen Einteilungen sind nicht mehr einheitlich. Bei zunehmendem Trend zur weiteren Spezialisierung ist die gegenwärtige Situation sehr dynamisch und kaum überschaubar geworden. Historisch gesehen sind einzelne Bereiche aus der Philosophie entstanden. So waren insbesondere Naturphilosophie und Naturwissenschaft lange Zeit in der Naturkunde eng verbunden.

Aus dem Bedürfnis heraus, Daten über Forschungseinrichtungen, Forschungsergebnisse statistisch zu erheben und international vergleichbar zu machen, gibt es Versuche, die verschiedenen Wissenschaften zu klassifizieren. Eine der für Statistiker verbindlichen Systematiken der Wissenschaftszweige ist die 2002 von der OECD festgesetzte "Fields of Science and Technology" (FOS).




</doc>
<doc id="5608" url="https://de.wikipedia.org/wiki?curid=5608" title="Wilhelm von Ockham">
Wilhelm von Ockham

Wilhelm von Ockham, englisch "William of Ockham" oder "Occam" (* um 1288 in Ockham in der Grafschaft Surrey, England; † 9. April 1347 in München), war ein berühmter mittelalterlicher Philosoph, Theologe und kirchenpolitischer Schriftsteller in der Epoche der Spätscholastik. Er wird traditionell, aber ungenau als einer der Hauptvertreter des Nominalismus bezeichnet. Sein umfangreiches philosophisches Werk enthält Arbeiten zur Logik, Naturphilosophie, Erkenntnistheorie, Wissenschaftstheorie, Metaphysik, Ethik und politischen Philosophie.

Während die Quellen – vor allem Ockhams eigene Werke – über seine Ansichten und Lehren detailliert informieren, liegen über seine Persönlichkeit und Biographie nur relativ spärliche Nachrichten vor.

Das erste gesicherte Datum aus Ockhams Leben ist seine Weihe zum Subdiakon in Southwark im Februar 1306; damals gehörte er bereits dem Franziskanerorden an. Etwa im Zeitraum von 1300 bis 1308 erhielt er an einer Ordensschule ("studium", Studienhaus) der Franziskaner in London seine Ausbildung in den Artes als Voraussetzung für das Studium der Theologie, das er dann um 1308 an der Universität Oxford begann. Um 1317 erhielt er dort den Grad eines Bakkalaureus und damit die Berechtigung, eine Vorlesung über die "Sentenzen" des Petrus Lombardus zu halten. Den Magistergrad erlangte er aber offenbar nie, da sein mittelalterlicher Beiname "Venerabilis Inceptor" („Ehrwürdiger Beginner“) besagt, dass er sich für den Erwerb des Magistergrades qualifiziert hatte, dieser ihm aber nicht formell verliehen wurde. Die Ursache dafür waren möglicherweise universitätspolitische Konflikte und philosophisch-theologische Gegensätze, doch könnte es auch daran gelegen haben, dass die Anzahl der zulässigen Abschlüsse des Promotionsverfahrens von vornherein auf die Anzahl der zu besetzenden Stellen an der Universität und in den Ordenshäusern begrenzt war. Jedenfalls verließ Ockham Oxford und übersiedelte nach London, wo er etwa ab 1320 im Studienhaus der Franziskaner unterrichtete.

Der Kanzler der Oxforder Universität, der Magister John Lutterell, befand sich zu Beginn der 1320er Jahre in einem heftigen Konflikt mit den dortigen Magistern. Im Sommer 1322 baten die Magister den dafür zuständigen Bischof von Lincoln, den Kanzler abzusetzen. Lutterell wurde entlassen. Ob Ockham in diesem Konflikt bereits eine Rolle spielte, geht aus den Quellen nicht hervor, ist aber zu vermuten, denn der Kanzler war als eifriger Thomist ein entschiedener Gegner der Philosophie und Theologie des franziskanischen Gelehrten. Jedenfalls erlaubte König Eduard II. Lutterell im August 1323, an den päpstlichen Hof in Avignon zu reisen. Dort legte der ehemalige Kanzler eine Anklageschrift gegen Ockham vor, in der er ihn der Häresie bezichtigte. Darauf musste sich Ockham 1324 nach Avignon begeben, um sich dem gegen ihn angestrengten Prozess zu stellen. Lutterells Anklageschrift zählte 56 Lehrsätze auf, die als Irrtümer angeprangert wurden. 1325 wurde eine Kommission eingesetzt, die den Fall zu untersuchen hatte. Sie bestand aus sechs Theologen, unter denen der Ankläger Lutterell war. Die Kommission stellte auf der Grundlage der Anklageschrift eine neue Liste von 51 angeblich häretischen Thesen Ockhams zusammen. 1326 erstellte die Kommission ein abschließendes Gutachten, in dem von den 51 Sätzen Ockhams 29 als häretisch oder irrig, die übrigen 22 als möglicherweise falsch bezeichnet wurden. Unter anderem wurde Ockham des Pelagianismus für schuldig befunden. Damit hätte seiner Verurteilung durch Papst Johannes XXII. nichts mehr im Wege gestanden, zumal sich der Papst schon im Sommer 1325 in einem Brief an Eduard II. scharf gegen Ockhams Lehre ausgesprochen hatte. Obwohl das Verfahren sehr sorgfältig und mit großem Aufwand betrieben wurde und Ockham bis 1328 als Angeklagter in Avignon blieb, kam es aus unbekannten Gründen zu keinem Urteil. Ockham befand sich in Avignon als Angeklagter nicht in Haft; er musste dort bleiben, durfte sich aber frei bewegen und an seiner Verteidigung arbeiten.

Damals war der Armutsstreit im Gang, eine theologische Auseinandersetzung, die mit der Anklage gegen Ockham nicht zusammenhing. Dabei ging es ursprünglich um die Frage, inwieweit die Franziskaner im Sinne des Testaments des Ordensgründers Franz von Assisi verpflichtet waren, in vollkommener Armut zu leben, und wie der Franziskanerorden mit Geschenken – darunter insbesondere Immobilien – umgehen sollte, die er erhielt und die mit dem ursprünglichen Armutsideal schwer vereinbar waren. Strittig war auch, ob Christus und die Apostel privat oder gemeinschaftlich Eigentum besessen hatten; aus der Annahme, dass dies nicht der Fall gewesen war, wurde gefolgert, dass eine konsequente Christus-Nachfolge notwendigerweise mit entsprechender Armut verbunden war. Demnach durften die Mönche individuell ebenso wie kollektiv keine Dinge besitzen, sondern sie nur im unumgänglichen Maß gebrauchen. Obwohl der Streit sich formal nur auf die Lebensweise von Bettelmönchen bezog, konnte die Armutsforderung auch als Kritik am Reichtum des höheren Klerus und besonders der Angehörigen des päpstlichen Hofes verstanden werden.

Papst Johannes XXII. war ein entschiedener Gegner der Armutsthese und verurteilte sie als häretisch. Dadurch geriet er in Konflikt mit dem Ordensgeneral der Franziskaner, Michael von Cesena, den er nach Avignon zitierte. Michael traf am 1. Dezember 1327 in Avignon ein; er wohnte dort wohl in dem Franziskanerkonvent, wo auch Ockham untergebracht war. So sah sich Ockham, der sich bisher auf theologische und philosophische Fragen konzentriert hatte und kirchenpolitisch kaum hervorgetreten war, zur Auseinandersetzung mit dem Armutsstreit veranlasst. Es gelang Michael, den Philosophen von der Auffassung zu überzeugen, dass die Armutsforderung berechtigt war und drei gegenteilige Verordnungen des Papstes von 1322 bis 1324 häretisch waren. Daraus zogen die beiden Franziskaner die Konsequenz, dass der Papst vom wahren Glauben abgefallen sei. Johannes verbot Michael, Avignon zu verlassen. Am 26. Mai 1328 flohen Michael, Wilhelm von Ockham und die Franziskaner Bonagratia von Bergamo und Franz von Marchia aus Avignon und begaben sich auf dem Seeweg nach Pisa. Dort trafen sie auf Kaiser Ludwig IV. den Bayern, der sich damals bereits im Streit mit dem Papst befand. Johannes hatte die Rechtmäßigkeit der Herrschaft Ludwigs bestritten und ihn am 23. März 1324 exkommuniziert, worauf Ludwig den Papst der Häresie beschuldigte und am 18. April 1328 für abgesetzt erklärte. Bei dem Häresievorwurf spielte der Armutsstreit, in dem Ludwig ab 1324 auf der Seite der Armutsbefürworter stand, eine Rolle. Ludwig stellte die flüchtigen Franziskaner unter seinen Schutz; Anfang 1330 traf Ockham mit seinen Gefährten in München ein, wo er bis zu seinem Tod blieb. Ockham, der am 20. Juli 1328 exkommuniziert worden war, wurde nun zu einem Vorkämpfer der Gegner des Papstes. Er begann sich intensiv mit politischen und kirchenrechtlichen Grundsatzfragen zu befassen, insbesondere dem Verhältnis zwischen weltlicher und geistlicher Macht und den Grenzen der Befugnisse des Papstes.

Es gelang den rebellischen Mönchen nicht, ihren Orden für den Kampf gegen Johannes zu gewinnen; die Franziskaner blieben dem Papst treu und wählten einen neuen Ordensgeneral. Auch nach dem Tod des Papstes 1334 kam es nicht zu einer Versöhnung mit dessen Nachfolger Benedikt XII.; die Positionen blieben im Wesentlichen unverändert, und Ockham verfasste einen Traktat gegen Benedikt, um auch den neuen Papst als Häretiker zu erweisen. Zwar konnte Ockham seine Stellung als Berater des Kaisers festigen – er half Ludwig auch im Ehestreit um Margarete von Tirol mit einem Gutachten –, doch der Niedergang von Ludwigs Ansehen und Macht und die Wahl des Gegenkönigs Karl IV. im Juli 1346 bedeuteten für den exkommunizierten Franziskaner eine akute Gefahr. Einer seiner letzten Texte zeigt, dass er mit der Möglichkeit rechnete, dass München in die Hände der Gegner fiele. Ockham hat aber den Tod Ludwigs im Oktober 1347 nicht mehr erlebt. Entgegen früheren Vermutungen, wonach er bis 1349 lebte und sich möglicherweise mit dem Papst aussöhnte, steht nach heutigem Forschungsstand fest, dass er im April 1347 als Exkommunizierter gestorben ist.

Die Werke Ockhams lassen sich in vier Hauptgruppen gliedern:

Drei Grundprinzipien, die Ockham überall konsequent anwendet, prägen sein Denken sowohl auf theologischem als auch auf philosophischem Gebiet:

Ockham wandte sich gegen den im antiken und bisherigen mittelalterlichen Denken vorherrschenden, ursprünglich von Platon formulierten und auch von Aristoteles vertretenen Nezessitarismus (Notwendigkeitslehre). Platon war der Meinung, die bestehende Weltordnung ergebe sich aus dem Zusammenwirken von Notwendigkeit und Vernunft zwangsläufig genau so, wie sie empirisch gegeben ist. Auch Aristoteles hielt alles tatsächlich Existierende für notwendig und meinte, es sei alles soweit überhaupt möglich von der Natur optimiert. Dem stellt Ockham seine Überzeugung von der Kontingenz der Welt und aller ihrer Bestandteile entgegen. Die Welt ist für ihn nur eine unter einer unbegrenzten Menge von möglichen Welten, die Gott hätte schaffen können. Überdies kann Gott, nachdem er die Welt geschaffen hat, jederzeit Naturgesetze ändern oder aufheben, und es ist kein Grund dafür ersichtlich, dass er dies tut oder unterlässt. Diese Auffassung wird mitunter so gedeutet, dass Ockhams Gott willkürlich handle, also ohne rationalen Grund einer Möglichkeit vor anderen den Vorzug gebe. Das hat Ockham aber nicht gemeint, denn das wäre aus seiner Sicht eine unzulässige Einschätzung von Gottes Handeln aus einer begrenzten menschlichen Perspektive. Ockham betrachtet Gottes Handeln als rational, aber nur begrenzt für die menschliche Vernunft einsehbar. Die Frage, warum die Welt so und nicht anders ist, muss demnach offenbleiben.

Eine Hauptforderung der aristotelischen Logik ist der Satz vom Widerspruch, wonach es unmöglich ist, dass dasselbe demselben in derselben Beziehung zugleich zukomme und nicht zukomme. Ockham betont, dass etwas in diesem Sinne Widersprüchliches nicht nur unlogisch ist, sondern auch kein Erkenntnisobjekt sein und schlechterdings nicht existieren könne. Damit begrenzt er die Allmacht Gottes, auf deren Schrankenlosigkeit er ansonsten großes Gewicht legt. Auch für Gott bestehen demnach nur widerspruchsfreie Alternativen, da er nur in diesem Rahmen auf geordnete Weise schaffen kann. Ockham unterscheidet begrifflich (nicht real) zwischen einer absoluten und einer „geordneten“ oder „ordinierten“ Macht Gottes und stellt fest, Gott handle nur nach den Regeln einer von ihm selbst festgelegten Ordnung, die widersprüchliche Akte ausschließt. Auch andere Ordnungen, die Gott hätte einrichten können, müssten widerspruchsfrei sein. Einen Grund dafür, dass Gott Widersprüchliches nicht verwirklichen kann, gibt Ockham aber nicht an. Er hält es für unmöglich, dass Gott etwas real Unendliches oder einen räumlich ausgedehnten unteilbaren Körper erschafft, etwas bereits Geschehenes ungeschehen macht oder real existierende Universalien erzeugt, denn all dies würde nach seiner Überzeugung den Satz vom Widerspruch verletzen. Hingegen ist es für Ockham theoretisch möglich, dass Gott sündigt.

Zahlreiche neuzeitliche Autoren, darunter Leibniz, zitieren das als „Ockhams Rasiermesser“ bekannte Prinzip in der Formulierung: „Entia non sunt multiplicanda sine necessitate“: „Entitäten (als seiend angenommene Dinge) sollen nicht unnötig vervielfacht werden“. Diese erst seit dem 17. Jahrhundert bezeugte Formulierung stammt aber nicht von Ockham. Ontologisch bedeutet das auch als Sparsamkeitsprinzip bezeichnete Prinzip nach einer in der Neuzeit verbreiteten Interpretation, dass Dinge nur dann für existierend gehalten werden sollen, wenn eine Notwendigkeit besteht, ihre Existenz zu behaupten; die „überflüssigen“ Dinge sind als nichtexistent „wegzurasieren“. Das hat Ockham aber nicht gemeint und nicht so ausgedrückt; denn ihm ging es nicht um das Sein oder Nichtsein von Dingen, sondern um die Berechtigung von Aussagen. Er hat auch nicht den Begriff „Rasiermesser“ verwendet. Vielmehr besagt sein Grundsatz, aus dem in der Neuzeit das Rasiermesser-Prinzip abgeleitet wurde, nur, dass in Aussagen unnötige Vervielfachungen zu vermeiden sind: „Umsonst geschieht mit Hilfe einer Mehrheit, was mit weniger bewirkt werden kann“ und „Eine Mehrheit ist nicht ohne Notwendigkeit anzunehmen“. Damit will Ockham verhindern, dass die Schaffung und Verwendung eines überflüssigen Begriffsinstrumentariums zur Entstehung ontologischer Vorstellungen beiträgt, die für die wissenschaftliche Erkenntnis nicht hilfreich sind. In der Formulierung „Umsonst geschieht mit Hilfe einer Mehrheit, was durch eines bewirkt werden kann“ kommt der Grundsatz schon im 13. Jahrhundert bei dem Franziskaner Odo Rigaldus vor, einem Schüler Alexanders von Hales.

Zur Begründung beruft sich Ockham auf Aristoteles, der sich in seiner "Physik" gegen die Annahme einer unendlichen Mannigfaltigkeit von Prinzipien ausspricht. Aristoteles argumentiert, dass es anderenfalls kein Wissen von dem sich aus den Prinzipien Ergebenden geben könne; überdies könne die Annahme einer begrenzten Anzahl von Prinzipien alles leisten, was sich mittels unendlich vieler erreichen lasse. Allerdings geht Ockham weit über das von Aristoteles Gemeinte hinaus. Aristoteles meint nur, dass keine unbegrenzte Mannigfaltigkeit der Prinzipien anzusetzen sei, während Ockham strikt die Eliminierung aller nicht notwendigen Hypothesen oder Theoriebestandteile fordert.

Ockhams Gott ist nicht an das Sparsamkeitsprinzip gebunden; vielmehr gibt es vieles, was er aus unbekanntem Grund mit größerem Aufwand tut, obwohl er es auch mit geringerem Aufwand tun könnte. Dem Philosophen steht es nicht zu, etwas möglicherweise Seiendes mit der Begründung, es sei überflüssig, zu eliminieren. Wohl aber soll er bei seiner eigenen Tätigkeit, dem Formulieren von Aussagen, nicht mehr Annahmen einführen, als er tatsächlich benötigt. Dieses Sparsamkeitsprinzip beinhaltet nicht die Behauptung, dass die Welt möglichst sparsam aufgebaut sei und daher Unnötiges in ihr nicht existiere, sondern es ist eine pragmatische Zweckmäßigkeitsregel für die wissenschaftliche Beschreibung von Phänomenen. Wenn eine Aussage das Sparsamkeitsprinzip verletzt, folgt daraus nicht ihre Unwahrheit, sondern nur, dass sie dem Ziel wissenschaftlicher Erkenntnis nicht angemessen ist. Dies drückt Ockham mit Formulierungen wie „es ist nicht nötig“ oder „es besteht keinerlei Notwendigkeit“ aus.

Die Auffassung des Aristoteles, dass Erkenntnis Sinneswahrnehmung voraussetzt, teilt Ockham nur hinsichtlich der sinnlich wahrnehmbaren Erkenntnisobjekte der Außenwelt, nicht aber hinsichtlich der auf die eigenen Akte des Intellekts bezogenen Erkenntnis. Der Anstoß zur Erkenntnis kommt für ihn stets vom jeweiligen Einzelding ("singulare"). Er lehnt die Auffassung des Thomas von Aquin ab, wonach zwischen dem Einzelding und dem Erkenntnisakt ein eigenständiges vermittelndes Medium, die geistige Erkenntnisform ("species intelligibilis"), stehen muss. Ebenso verwirft er auch die verbreitete Ansicht, Erkenntnis beruhe darauf, dass der Intellekt sich dem Wahrnehmungsobjekt angleiche (Assimilation) und dieses so in ihm abgebildet werde (Repräsentation), was eine strukturelle Ähnlichkeitsbeziehung zwischen ihnen (Affinität) voraussetzt. Dagegen argumentiert er, dass dies zu einem infiniten Regress führen müsse, da die Repräsentation, um Erkenntnisobjekt sein zu können, ihrerseits einer Repräsentation bedürfe.

Ockham betont, dass etwas nur bekannt sein kann, wenn es die Form eines Satzes ("complexum") aufweist, also einer logischen Verknüpfung zwischen dem, worüber etwas ausgesagt wird (Subjektterm), und dem, was darüber ausgesagt wird (Prädikatterm). Ein solcher Satz ist für Ockham nur dann im eigentlichen Sinn ("proprie") wissenschaftlich, wenn seine Aussage mit Notwendigkeit wahr ist, wenn also seine Richtigkeit überprüft und durch einen Syllogismus erwiesen worden ist, dessen Prämissen notwendig sind. Mit „notwendig“ ist dabei nicht eine absolute Notwendigkeit des äußeren Sachverhalts gemeint, auf den sich der Satz bezieht (das würde in Ockhams kontingenter Welt Wissenschaft unmöglich machen), sondern nur die Geltung des Satzes unter der Voraussetzung, dass beabsichtigt ist, die beiden Terme sinnvoll miteinander zu verbinden. Gegenstand einer Wissenschaft sind somit nicht reale, vom Denken unabhängige Objekte der Außenwelt, die der Intellekt im Erkenntnisvorgang assimiliert, sondern nur die Sätze, die über die Objekte ausgesagt werden.

Ockham legt großen Wert auf die saubere Trennung von logischen Aussagen und ontologischen Sachverhalten. Die Prädikation, deren Subjekt ein Allgemeinbegriff ist, versteht er nicht als Vorhandensein einer im Prädikat ausgesagten Eigenschaft in dem Allgemeinbegriff, sondern nur als eine Zuordnung von Subjekt und Prädikat im Rahmen der Aussage. Das Prädikat kommt zwar dem Subjekt zu, verhält sich aber zu ihm nicht wie eine Eigenschaft zu ihrem Träger oder ein Akzidens zu einer Substanz, denn die Zuordnung der Termini im Satz zueinander spiegelt nicht ein Verhältnis zwischen den realen Entitäten, auf die sie sich beziehen.

In seiner Aussagenlogik formuliert Ockham als Axiome für die Konjunktion „und“ und die Disjunktion „oder“ bereits die beiden de Morganschen Gesetze.

Ockham glaubt, dass zukünftige Ereignisse eintreten (d. h. wahr sein) können, selbst wenn die Menschen nicht wissen, dass dem so ist. Diese Denkweise inspirierte im letzten Drittel des 20. Jahrhunderts die Entwicklung verschiedener Kalküle der Computation Tree Logic, einer Form der Zeitlogik, bei der von einer verzweigten Zeitfolge ausgegangen wird. In der Literatur werden diese Kalküle als ockhamsche Ansätze oder ockhamsche Logiken bezeichnet.

Ockhams konsequente Trennung zwischen logischen und ontologischen Aussagen führt ihn zur Verwerfung der Metaphysik des Thomas von Aquin und besonders der im Thomismus vertretenen aristotelischen Vorstellung einer Analogia entis. Dabei geht es um die von Thomas bejahte Frage, ob der Ausdruck „seiend“ von verschiedenen Entitäten wie „Gott“ und „Geschöpf“ in der gleichen Bedeutung (univok) ausgesagt werden kann, sei es im Sinne einer Analogie (aristotelisch) oder im Sinne einer Teilhabe des einen Seins am anderen (neuplatonisch). Ockham verneint dies. Nach seiner Ansicht bezeichnet der Begriff „seiend“ keine für sich real existierende Eigenschaft, die mit einem realen Subjekt wie Sokrates verknüpft werden könnte, indem man sagt „Sokrates ist (bzw. war) ein Seiendes“. Vielmehr ist die Aussage „Sokrates ist (bzw. war) ein Seiendes“ nur deswegen wahr, weil der Terminus „Sokrates“ (Subjekt) und der Terminus „ist seiend“ (Prädikat) im Sinne der Aussagenlogik für ein und dasselbe stehen (supponieren, siehe Supposition).

Das Universalienproblem, die Frage nach dem Wirklichkeitsbezug von Universalien (Allgemeinbegriffen), wurde im Mittelalter seit dem späten 11. Jahrhundert kontrovers diskutiert. Die einander entgegengesetzten Auffassungen waren der Begriffsrealismus (auch als Universalienrealismus oder kurz Realismus bezeichnet) und der Nominalismus. Realisten meinen, dass die Allgemeinbegriffe etwas bezeichnen, was auch extramental (außerhalb des menschlichen Geistes) real existiert, nämlich in den Einzeldingen (Aristotelismus) oder zusätzlich auch unabhängig von diesen in einer Ideenwelt (Platonismus). Nominalisten hingegen halten die Allgemeinbegriffe für bloße Zeichen, die zwar innerhalb des menschlichen Verstandes vorkommen, weil dieser sie für seine Tätigkeit benötigt, darüber hinaus aber keinen Bezug zu irgendeiner Wirklichkeit haben. Sowohl bei den Realisten als auch bei ihren Gegnern gab es verschiedene Abstufungen der Radikalität bzw. Mäßigung, mit der sie ihre Positionen vertraten. Ockham vertrat einen „gemäßigten“ Nominalismus, der mitunter zur Abgrenzung vom „radikalen“ Nominalismus des Johannes Roscelin als Konzeptualismus bezeichnet wird. Zur Vermeidung von Verwechslungen mit neuzeitlichem Konzeptualismus spricht man auch von „zeichentheoretischem Nominalismus“. Der radikale ältere Nominalismus Roscelins – dessen Radikalität allerdings nur aus gegnerischen Darstellungen bekannt ist – erklärt die Allgemeinbegriffe für bloße „Namen“ ("nomina"), also vom Verstand erschaffene Fiktionen, denen nirgends irgendeine Realität zukommt außer derjenigen, dass sie ein „Stimmhauch“ ("flatus vocis") sind. Ockhams gemäßigter Nominalismus oder Konzeptualismus bestreitet zwar ebenfalls die Existenz von Universalien in den äußeren Wahrnehmungsobjekten, betrachtet die Allgemeinbegriffe aber insoweit als existent, als sie Konzepte sind, die innerhalb des menschlichen Geistes tatsächlich vorhanden sind. Demnach hat das Allgemeine eine subjektive, rein mentale Realität im Denken und nur dort. Den Realisten wirft Ockham vor, aus sprachlichen Gegebenheiten Realitäten zu machen und den fundamentalen Unterschied zwischen Existenz und Prädikation zu verwischen; über Einzelnes sage man, dass es existiere oder nicht existiere, über Allgemeines hingegen, dass es ausgesagt (prädiziert) werde oder nicht ausgesagt werde.

Die nominalistische bzw. konzeptualistische Denkweise Ockhams kommt auch in seiner Auffassung vom Staat zur Geltung. Da das menschliche Individuum ein Einzelding ist, das als solches real existiert, die Bürgerschaft oder der Staat hingegen ein Universale, das nur im menschlichen Geist vorhanden ist, kann der Staat kein Selbstzweck sein oder einen übergeordneten Wert darstellen, sondern sein Zweck ist das Wohl der individuellen Bürger, die ihn bilden. Das Gemeinwohl, also dasjenige, was den Individuen zugutekommt, hat Vorrang gegenüber willkürlichen Entscheidungen staatlicher Instanzen. Das Kriterium für die Legitimität von Anordnungen der Obrigkeit ist, ob sie dem Gemeinwohl dienen oder nicht.

Nach Ockhams Ansicht hat der Kaiser seine Kompetenz vom Volk. Das Volk kann ihm aber nur die Kompetenz übertragen, das Allgemeinwohl zu fördern, das heißt das Wohl der von seinen Anordnungen betroffenen Individuen. Es kann nicht eine Person dazu bevollmächtigen, das Allgemeinwohl einzuschränken oder Maßnahmen zu treffen, die anderen Zwecken als dem gemeinsamen Nutzen der Bürger dienen. Erteilt der Herrscher einen Befehl, der nicht in Einklang mit der Gerechtigkeit ist und nicht dem allgemeinen Nutzen dient, so überschreitet er seinen Zuständigkeitsbereich, und in diesem Fall besteht keine Gehorsamspflicht.

Die gleichen Kriterien wie in der Staatstheorie verwendet Ockham auch in der Lehre von der Kirche (Ekklesiologie). Nach seiner Überzeugung bezieht auch das Amt des Papstes seine Legitimation daraus, dass es dem Nutzen aller dient. Dürfte der Papst nach seinem Belieben alles tun, was nicht durch göttliches Gesetz verboten ist, so wären, wie Ockham schreibt, alle Christen seine Sklaven. Die Macht des Papstes ist also nicht nur dadurch eingeschränkt, dass er nicht gegen göttliches Recht oder Naturrecht verstoßen darf, sondern auch durch seine Pflicht, dem Wohl der ihm unterstellten Individuen zu dienen. Außerdem ist er in der Regel nur für geistliche Angelegenheiten zuständig; in den weltlichen Kompetenzbereich des Kaisers darf er sich nur einmischen, wenn er einsichtig machen kann, dass anderenfalls das gemeinsame Wohl gefährdet wäre.

Da Ockham es als erwiesen betrachtete, dass der Papst ein Häretiker war, benötigte er ein vom Papst unabhängiges und gegen ihn verwendbares Kriterium für Wahrheit in Glaubensfragen. Dieses konnte für ihn nicht ein Urteil eines allgemeinen Konzils sein, wie die Konziliaristen meinten, denn er betrachtete auch ein Konzil als grundsätzlich irrtumsfähig. Er hielt zwar an der traditionellen Lehre fest, wonach die Kirche hinsichtlich der Wahrheit theologischer Aussagen die entscheidende Instanz ist, doch definierte er den Begriff Kirche um. Anfangs war bei ihm von der „römischen“ Kirche die Rede, womit er den Apostolischen Stuhl meinte, den er auch ausdrücklich nannte. Später, als er sich von der Kurie distanzierte, berief er sich auf das Urteil der „universalen“ Kirche. Er erörterte die theoretische Möglichkeit, dass alle Kleriker der Welt in einer Glaubensfrage irren könnten. Dazu bemerkte er, in diesem Falle müssten Laien, auch falls sie nur wenige und theologisch gänzlich ungebildet seien, auf ihrem Standpunkt beharren; sie seien dann die Kirche und die qualifizierten Richter der Geistlichkeit. Er hielt es sogar für möglich, dass die gesamte Kirche außer einer einzigen Person, die sogar ein unmündiges Kind sein kann, einer falschen Lehre verfällt. Dann bestehe die wahre Kirche aus dieser einen Person. Die Zusage Christi: „Ich bin bei euch alle Tage bis zum Ende der Welt“ garantiere, dass niemals alle Christen gleichzeitig vom Glauben abfallen können. Daher brauche ein Christ, auch wenn er als einziger Rechtgläubiger allein gegen alle stehe, nicht an seinem Sieg zu verzweifeln. Damit weist Ockham in letzter Konsequenz, im äußersten Extremfall dem einzelnen Christen die Aufgabe zu, mittels seiner eigenen Urteilskraft letztinstanzlich zu entscheiden.

Ockham hatte zwar einige Schüler, darunter Adam Wodeham, aber er hat keine kontinuierlich fortbestehende, auf ein bestimmtes Lehrgebäude festgelegte philosophische oder theologische Schule gegründet. Dennoch spricht man von einem spätmittelalterlichen Ockhamismus, und der Begriff „Ockhamisten“ ("Ockamistae", "Occamici") kommt in mittelalterlichen Quellen vor. Damit ist eine nominalistische Strömung des 14. und des 15. Jahrhunderts gemeint, die sich auf Ockhams Schriften berief. Allerdings hat ein Teil dieser Philosophen (darunter Nikolaus von Autrecourt und Johannes von Mirecourt) Ockhams Positionen radikalisiert, andere kombinierten sie mit entgegengesetzten Ansichten anderer Denker, während die zahlreichen Gegner des Nominalismus Ockhams Auffassungen teils verzerrt wiedergaben. Dadurch entstand in weiten Kreisen ein schiefes Bild von Ockhams Philosophie. Die Vorgehensweise der mehr oder weniger von Ockhams Ansatz ausgehenden Philosophen wurde als „moderner Weg“ ("via moderna") bezeichnet zur Unterscheidung vom „alten Weg“ derjenigen, die auf die eine oder andere Weise Allgemeinbegriffe mit denkunabhängigen Strukturen verbanden.

Im Jahr 1339 wurde an der Pariser Universität zwar nicht die Lektüre von Ockhams Schriften, wohl aber deren Verwendung im Unterricht verboten. Bald darauf wurde dort ein allgemeines Nominalismusverbot verfügt.

In der Frühen Neuzeit wurden Ockhams Werke selten gedruckt, meist kannte man seine Lehren nur aus zweiter oder dritter Hand. Theologisch anregend waren seine Ideen aber für Luther, der sie über eine lehrbuchartige Zusammenfassung kennenlernte, die der Tübinger Professor Gabriel Biel, ein Ockhamist, angefertigt hatte. Luther bekämpfte Biel, schätzte aber Ockham, wobei ihm neben der kirchenpolitischen Aktivität des Franziskaners besonders die fundamentale Kritik an den Lehren führender Theologen der Scholastik zusagte.

In der Moderne hat besonders das als „Ockhams Rasiermesser“ bekannte Sparsamkeitsprinzip Wertschätzung gefunden, so etwa bei Charles S. Peirce und Bertrand Russell. Peirce behauptete, dass die gesamte moderne Philosophie auf dem Ockhamismus gründe. Im Konstruktivismus, besonders auch im Radikalen Konstruktivismus, wird Ockham als wichtiger Vorläufer des konstruktivistischen Ansatzes betrachtet.

Wilhelm von Ockham ist eine der Figuren, die Umberto Eco in seinem Roman "Der Name der Rose" in die Gestalt des William von Baskerville einfließen ließ. Nach ihm ist auch die Programmiersprache Occam und die Occamstraße im Münchner Szeneviertel Schwabing benannt.














</doc>
<doc id="5610" url="https://de.wikipedia.org/wiki?curid=5610" title="Watt">
Watt

Watt steht für:
Watt bezeichnet folgende Orte:
WATT steht als Abkürzung für:

Siehe auch:


</doc>
<doc id="5618" url="https://de.wikipedia.org/wiki?curid=5618" title="Wavelet">
Wavelet

Mit dem Begriff Wavelet werden die einer kontinuierlichen oder diskreten Wavelet-Transformation zugrundeliegenden Funktionen bezeichnet, also die für die Transformation benutzte Basisfunktion, mit der ein zu analysierendes Signal oder Bild – im Allgemeinen eine N-dimensionale Funktion – „verglichen“ wird. Das Wort ist eine Neuschöpfung aus dem französischen "„ondelette“", das „"kleine Welle"“ bedeutet und teils wörtlich („onde“→„wave“), teils phonetisch („-lette“→„-let“) ins Englische übertragen wurde. Der Ausdruck „Wavelet“ wurde in den 1980er Jahren in der Geophysik (Jean Morlet, Alex Grossmann) für Funktionen geprägt, welche die Kurzzeit-Fourier-Transformation verallgemeinern, wird jedoch seit Ende der 1980er Jahre ausschließlich in der heute üblichen Bedeutung verwendet. In den 1990er Jahren entstand ein regelrechter Wavelet-Boom, ausgelöst durch die Entdeckung von kompakten, stetigen (bis hin zu beliebiger Ordnung der Differenzierbarkeit) und orthogonalen Wavelets durch Ingrid Daubechies (1988) und die Entwicklung des Algorithmus der schnellen Wavelet-Transformation (FWT) mit Hilfe der Multiskalenanalyse (MultiResolution Analysis – MRA) durch Stéphane Mallat und Yves Meyer (1989).

Im Gegensatz zu den Sinus- und Kosinus-Funktionen der Fourier-Transformation besitzen die meistverwendeten Wavelets nicht nur Lokalität im Frequenzspektrum, sondern auch im Zeitbereich. Dabei ist „Lokalität“ im Sinne kleiner Streuung zu verstehen. Eine Sinus- oder Kosinus-Funktion ist beispielsweise aufgrund ihrer Periodizität nicht lokal im Zeitbereich. Die Wahrscheinlichkeitsdichte ist das normierte Betragsquadrat der betrachteten Funktion bzw. von deren Fourier-Transformierten. Dabei ist das Produkt beider Varianzen immer größer als eine Konstante, analog zur Heisenbergschen Unschärferelation, siehe auch das WKS-Abtasttheorem. Aus dieser Einschränkung heraus entstanden in der Funktionalanalysis die Paley-Wiener-Theorie (Raymond Paley, Norbert Wiener), ein Vorläufer der diskreten Wavelet-Transformation, und die Calderón-Zygmund-Theorie (Alberto Calderón, Antoni Zygmund), die der kontinuierlichen Wavelet-Transformation entspricht.

Das Integral einer Wavelet-Funktion ist vorzugsweise 0, daher nimmt in der Regel die Waveletfunktion die Form von nach außen hin auslaufenden (kleiner werdenden) Wellen (also „Wellchen“ = Ondelettes = Wavelets) an.

Wichtige Beispiele für Wavelets sind das Haar-Wavelet (Alfréd Haar 1909), die nach Ingrid Daubechies benannten Daubechies-Wavelets (um 1990), die ebenfalls von ihr konstruierten Coiflet-Wavelets und das eher theoretisch bedeutsame Meyer-Wavelet (Yves Meyer, um 1988).

Wavelets gibt es für Räume beliebiger Dimension, meist wird ein Tensorprodukt einer eindimensionalen Waveletbasis verwendet. Aufgrund der fraktalen Natur der Zwei-Skalen-Gleichung in der MRA haben die meisten Wavelets eine komplizierte Gestalt, die meisten haben keine geschlossene Form.

Im angelsächsischen Sprachraum wird der englische Begriff "" weiter gefasst: Dort wird unter eine wellenartige Oszillation mit einer Amplitude beginnend mit Null, einem Amplitudenanstieg und einem anschließenden Amplitudenabfall zurück auf Null verstanden. Eindimensionale Wavelets mit einem von Null abweichenden Integral werden somit von dieser weiter gefassten Definition des Begriffs Wavelet mit umfasst. Solche Wavelets werden beispielsweise in bestimmten Verfahren der digitalen Signalverarbeitung genutzt. Beispielsweise können Distributionen als eine solche Klasse von Wavelets aufgefasst werden, mit denen beispielsweise die Abtastung eines Signals erfolgen kann. Ein besonders wichtiges Beispiel, das in diesem erweiterten Sinne als Extremform eines Wavelets aufgefasst werden kann, ist die Diracsche Deltafunktion. Die Anwendung einer bestimmten Wavelet-Transformation ist daher stets an die Verwendung einer jeweils zugehörigen Wavelet-Untermenge für die Wavelet-Transformation gebunden.

Anwendung finden Wavelets in Methoden der Signalverarbeitung, insbesondere der Signalkompression, welche als ersten Schritt eine diskrete Wavelet-Transformation beinhalten. Diese wurden seit Anfang der 1990er Jahre als Meilenstein der Bildkompression und Audiodatenkompression propagiert. Trotzdem sind außerhalb von Spezialanwendungen, wie z. B. in der Geophysik oder Computertomographie, solche Wavelet-Kompressionsmethoden nur in der JPEG2000-Norm und ihren direkten Vorgängern wie dem DjVu und dem LuraWave-Format implementiert. Bisher ist JPEG2000 wenig verbreitet. In einem weiten Sinne basiert auch das gängige JPEG auf einer Wavelet-Transformation, die verwendete Diskrete Kosinustransformation kann als Haar-Wavelet interpretiert werden. In Methoden der Signalanalyse wird eher die kontinuierliche Wavelet-Transformation in diskretisierter Form verwendet.

Ein Wavelet formula_1 ist hier die erzeugende Funktion eines affinen Systems von Funktionen formula_2, welche eine Hilbert-Basis, d. h. ein vollständiges Orthonormalsystem im Funktionenraum formula_3 bilden. Die Darstellung einer Funktion mittels dieser Funktionen nennt man Wavelet-Transformation:
und inverse Wavelet-Transformation

Das elementarste Beispiel ist das Haar-Wavelet. Es ist hilfreich, wenn die Wavelet-Funktion zu einer Multiskalenanalyse assoziiert ist, da dann in der praktischen Berechnung die Auswertung vieler der Integrale, die hinter den Skalarprodukten stehen, durch wiederholte Faltung von einmal gewonnenen Koeffizientenfolgen mit endlichen Filterfolgen ersetzt werden kann. Dieses beschleunigte Verfahren nennt man dementsprechend schnelle Wavelet-Transformation.

Der Zusammenhang zwischen Wavelets und Filtern zur Signalverarbeitung ist nun recht anschaulich: Die Waveletmaske entspricht der Impulsantwort eines Bandpassfilters mit einer gewissen Schärfe in der Zeit (Filterlänge) und in der Frequenz (Bandbreite). Filterlänge und Bandbreite sind umgekehrt proportional, so wird eine "Streckung" des Filters um den Faktor 2 die Bandbreite halbieren.

Es ist möglich und sinnvoll, andere Skalenfaktoren zu betrachten. So entspricht die DCT-Variante im JPEG-Algorithmus einem Haar-Wavelet zur Blockgröße 8.
Unter weiteren Abschwächungen der analytischen Anforderungen ergeben sich Wavelet-Frames (siehe Rahmen) beziehungsweise Framelets, diese erzeugen eine redundante Signaltransformation, die unter bestimmten Umständen vorzuziehen ist, zum Beispiel bei der Rauschunterdrückung.

Eine in letzter Zeit aufgekommene Variante sind die so genannten Multiwavelets, die nicht eine, sondern einen Vektor von Skalierungsfunktionen in der MRA aufweisen und dementsprechend matrixwertige Skalierungsfolgen.

Der neue JPEG2000-Standard der Bildkomprimierung kann biorthogonale, 5/3 und 9/7 Wavelets verwenden.




</doc>
<doc id="5619" url="https://de.wikipedia.org/wiki?curid=5619" title="Wernher von Braun">
Wernher von Braun

Wernher Magnus Maximilian Freiherr von Braun (* 23. März 1912 in Wirsitz, Provinz Posen, Deutsches Reich; † 16. Juni 1977 in Alexandria, Virginia, Vereinigte Staaten) war als deutscher und später US-amerikanischer Raketeningenieur ein Wegbereiter der Raketenwaffen und der Raumfahrt.

Er genoss aufgrund seiner Pionierleistungen als führender Konstrukteur der ersten leistungsstarken, funktionstüchtigen Flüssigkeitsrakete A4 („V2“) bei den Nationalsozialisten hohes Ansehen und in der westlichen Welt wegen seiner leitenden Tätigkeit beim Bau von Trägerraketen für die NASA-Missionen.

Wernher von Brauns Vater war der ostpreußische Gutsbesitzer und spätere Reichsernährungsminister Magnus Freiherr von Braun. Wernher von Brauns Mutter war Emmy von Braun, Tochter Wernher von Quistorps (1856–1908), eines Gutsbesitzers und Mitglieds des Preußischen Herrenhauses. Wernhers älterer Bruder Sigismund (1911–1998) war ab 1936 im Dritten Reich und auch in der späteren Bundesrepublik im Auswärtigen Amt tätig; der jüngere Bruder Magnus (1919–2003) wurde Ingenieur für organische Chemie.

Schon als Kind interessierte sich von Braun für Musik und Naturwissenschaften. Seine Begeisterung für die Astronomie wurde von seiner Mutter geweckt, die ihm zur Konfirmation ein astronomisches Fernrohr schenkte. Mit 13 Jahren experimentierte er im Berliner Tiergarten mit Feuerwerksraketen. Als er das Buch "Die Rakete zu den Planetenräumen" von Hermann Oberth in die Hände bekam, erlangten die Utopien, die er aus den Abenteuerromanen von Jules Verne und Kurd Laßwitz aufgenommen hatte, etwas Reales. Um das fachwissenschaftliche Buch verstehen zu können, strengte er sich an, seine bis dahin mäßigen Leistungen in Mathematik zu verbessern. Inspiriert wurde er ebenfalls durch das Buch "Das Problem der Befahrung des Weltraums" des slowenischen Astronomen und Astrophysikers Herman Potočnik.
Er besuchte bis 1925 das Französische Gymnasium Berlin und wohnte anschließend im Internat der Hermann-Lietz-Schule auf Schloss Ettersburg bei Weimar. Ab 1928 besuchte er die gerade gegründete Hermann Lietz-Schule Spiekeroog. Aufgrund guter Leistungen konnte er dort vorzeitig mit 18 Jahren im April 1930 die Abiturprüfung ablegen.

Ab 1929 arbeitete er gemeinsam mit Hermann Oberth in Berlin-Plötzensee und – nach dessen Rückkehr nach Siebenbürgen im August 1930 – mit Mitgliedern des Vereins für Raumschiffahrt auf dem Raketenflugplatz Berlin in Reinickendorf an Raketen mit Flüssigkeitstriebwerken. Nach seiner Schulzeit verbrachte er ein sechsmonatiges Praktikum bei der Lokomotivfabrik Borsig in Berlin, welches für das Ingenieurstudium gefordert war. Dort habe er gelernt, „dass es absolut nichts gibt, was präzise und vollendete und gründliche Arbeit übersteigt“, wie er sich Jahrzehnte später erinnerte.

Von Braun studierte ab 1930 an der Technischen Hochschule in Berlin-Charlottenburg und an der ETH Zürich. 1932 erwarb er ein Diplom als Ingenieur für Mechanik an der TH Berlin und trat, gefördert durch Walter Dornberger, als Zivilangestellter in das Raketenprogramm des Heereswaffenamtes ein. Seine Experimente führte er auf dem Gelände der Heeresversuchsanstalt Kummersdorf etwa 30 Kilometer südlich von Berlin durch. 1934 wurde er an der Friedrich-Wilhelms-Universität in Berlin zum Dr. phil. mit einer Arbeit über „Konstruktive, theoretische und experimentelle Beiträge zu dem Problem der Flüssigkeitsrakete“ promoviert. Im gleichen Jahr erreichte das von von Braun konzipierte Aggregat 2, gestartet von der Nordseeinsel Borkum aus, eine Höhe von 2200 Metern. In den Jahren 1935–1937 entwickelte von Braun in enger Zusammenarbeit mit dem Team Ernst Heinkels und dem Testpiloten Erich Warsitz ein Raketentriebwerk, das zuerst in Kummersdorf und später in Neuhardenberg an einem Flugzeug, der Heinkel He 112, erprobt wurde.

Ende 1935 wurde mehr und mehr klar, dass das Gelände in Kummersdorf ungeeignet war, das stark expandierende Raketenprogramm weiterhin zu beherbergen. Zum Test der neuen, deutlich größeren Raketen brauchte man eine mehrere hundert Quadratkilometer große Testzone, wofür nur die Ostsee infrage kam. Luftwaffe und Heer einigten sich darauf, eine gemeinsame Versuchsanstalt auf der Insel Usedom zu errichten.

Von 1937 an war Wernher von Braun der technische Direktor der neuen Heeresversuchsanstalt Peenemünde (HVA). Hier leitete er unter anderem die Entwicklung des Aggregats 4, kurz A4 genannt, einer Großrakete mit Flüssigtreibstoff. Ab 1943 wurde die Rakete anderen Ortes im Reich in Serienfertigung gebaut und nach ihren ersten Einsätzen auf London "V2 (Vergeltungswaffe 2)" genannt. Das Aggregat 4 war eine der ersten einsatzfähigen Boden-Boden-Raketen mit Flüssigkeitstriebwerk überhaupt. Neu war an dieser Rakete auch, schubstarke Flüssigkeitstriebwerke mit einem Kreiselsystem zu koppeln. So gelang es erstmals, die Flugbahn zu stabilisieren und Abweichungen automatisch auszuregeln.

Im Jahr 1942 überschritt ein Prototyp erstmals eine Gipfelhöhe von mehr als 80 km, 1945 wurden um 200 km erreicht. Die Rakete Aggregat 4 war damit nach Definition der FAI das erste von Menschen geschaffene Objekt im Weltraum, indem es eine Höhe von über 100 km erreichte.

In Peenemünde existierte seit Juni 1943 ein KZ-Außenlager. Zusätzlich gab es ein zweites KZ, ein Kriegsgefangenenlager in Karlshagen und die Lager Trassenheide, in denen insgesamt 1400 Häftlinge untergebracht waren. Dazu kamen über 3000 „Ostarbeiter“ aus Polen und der Sowjetunion. Von Braun selbst wird im Protokoll zu einer Besprechung vom 25. August 1943 zitiert: „Die Belegschaft für […] Mittelteile- und Heckfabrikation könnte aus dem Häftlingslager F1 gestellt werden.“

Damit nahm von Braun Bezug auf die Fertigungshalle F1 der Heeresversuchsanstalt Peenemünde, in der die A4-Rakete produziert wurde und in deren Keller 500 Menschen eingepfercht waren, was eine Verwicklung seiner Position als technischer Direktor in den Häftlingseinsatz zeigt.

In einer Aktennotiz vom 16. April 1943 erwähnte der Verantwortliche für den Bau der A4-Fabrik, Arthur Rudolph, später Direktor des Entwicklungsprogramms der Saturn V, die äußerst schlechten Lebens- und Arbeitsbedingungen der Zwangsarbeiter, darunter viele Ostarbeiter und Franzosen. HVA-Leiter Walter Dornberger ließ zum Umfang an beschäftigten HVA-Zwangsarbeitern, im von ihm unterzeichneten Besprechungsprotokoll vom 4. August 1943, festhalten: „Das Verhältnis der deutschen Arbeiter zu den KZ-Häftlingen soll 1:15, höchstens 1:10 betragen“.

Die Briten versuchten die HVA mit der „Operation Hydra“ in der Nacht vom 17. zum 18. August 1943 zu zerstören. Zu den Hauptzielen gehörte auch die Tötung der Wissenschaftler in ihren Unterkünften.
Von Braun konnte sich in einen Bunker retten.

Von Braun beantragte am 12. November 1937 seine Aufnahme in die NSDAP und wurde rückwirkend zum 1. Mai 1937 aufgenommen (Mitgliedsnummer 5.738.692). Am 1. Mai 1940 wurde er außerdem Mitglied der SS (SS-Nr. 185.068), in der er bis zum Sturmbannführer (28. Juni 1943) aufstieg. Lange war dies öffentlich nicht bekannt, Gerüchten wurde wenig Glauben geschenkt. Erst nach seinem Tod wurde seine SS-Mitgliedschaft der Allgemeinheit bekannt.

Mit der Entwicklung des Aggregats 4 hatte er eine Waffe geschaffen, die mit bisher unerreichter Reichweite und Geschwindigkeit eine Tonne Sprengstoff ans Ziel brachte. Die Zielgenauigkeit war stets so gering, dass sie sich primär nur als Terrorwaffe gegen die Zivilbevölkerung eignete. Dies führte später zu schweren Vorwürfen gegen von Braun, da diese Tatsache ihm bereits während der Entwicklung hätte bewusst sein müssen. Dennoch führte er nicht nur die Arbeit fort, sondern warb weiterhin massiv für das Potenzial von Raketen.

Bei einem dieser Werbebesuche von Brauns im Führerhauptquartier Wolfsschanze in Ostpreußen verlieh ihm Hitler persönlich den Professorentitel. Dazu von Braun: „Nach meinem Gespräch mit Hitler sah ich zufällig, dass Speer mit ihm – gleichsam hinter vorgehaltener Hand – etwas besprach. Wenige Augenblicke danach schritt Hitler auf mich zu, reichte mir die Hand und sagte: Professor, ich möchte Ihnen zu Ihrem Erfolg gratulieren.“

Im Februar 1944 wurde von Braun bei Heinrich Himmler vorgeladen. Himmler wollte sich Einfluss über die V2 sichern, was von Braun jedoch abwies. Im März 1944 wurde von Braun auf Betreiben Himmlers von der Gestapo verhaftet. Ihm wurde Verrat und Wehrkraftzersetzung sowie Vorbereitung zur Flucht nach England vorgeworfen, was mit der Todesstrafe geahndet werden konnte. Nur seine besondere Bedeutung im Raketenprogramm ließ ihn nach Intervention von Speer und Dornberger bei Hitler wieder freikommen.

Am 29. Oktober 1944 wurden von Braun und Walter Dornberger nach dem Einsatz der V2 an der Westfront mit dem Ritterkreuz des Kriegsverdienstkreuzes mit Schwertern ausgezeichnet.

In der Nacht vom 17. auf den 18. August 1943 wurde die HVA Peenemünde im Zuge der „Operation Hydra“ bombardiert. Um die Produktion der V2 vor weiteren Bombenangriffen zu schützen und möglichst geheim zu halten, sollte sie unter die Erde verlegt werden. Daraufhin entstand ein neues KZ-Außenlager des KZ Buchenwald mit dem Tarnnamen „Arbeitslager Dora“ am Südrand des Harzes. Die Häftlinge der KZ wurden von der SS, unter menschenunwürdigen Bedingungen, in der Stollenanlage im Kohnstein hauptsächlich im Stollenvortrieb und den untertage gelegenen Werksanlagen der Mittelwerk GmbH eingesetzt.
In Mittelbau-Dora fand nun unter anderem auch die Serienfertigung der A4 statt. Auch dieser Lebensabschnitt von Brauns wird von vielen Historikern sehr kritisch bewertet, da er eine Verantwortlichkeit für diese Produktion schwerlich abweisen konnte. Andere werfen ihm zumindest Opportunismus vor.

So forderte er in einem Schreiben vom 12. November 1943 die Zahl von 1350 Arbeitskräften an, was seinerzeit stets KZ-Häftlinge bedeutete. Einige Insassen des Konzentrationslagers bezeugten später zudem, ihn bei der Besichtigung der Arbeitsstätten gesehen zu haben. Es wird von 5 bis 20 Aufenthalten im Mittelwerk ausgegangen. Von Braun gab diese Zahlen in einem Gerichtsprozess am 14. Oktober 1947 in Texas an. Von Braun selbst erklärte, dass er vom Elend der Zwangsarbeiter nichts gewusst habe und für deren Einsatz nicht verantwortlich gewesen sei. Allerdings berichtete er 1969 in einem Interview, selbst im Mittelwerk gewesen zu sein: „Als die Sprengarbeiten für den Ausbau bereits begonnen hatten, die Produktion aber noch nicht angelaufen war […] damals waren einige Häftlinge in diesen Stollen untergebracht. Ich bin mit der besichtigenden Besuchergruppe durch diese temporären Unterkünfte gegangen.“ Er gab auch zu, dass die, so wörtlich, „Hungergestalten“ in einem „erbarmungswürdigen Zustand“ gewesen seien, Eindrücke, die „schwer auf der Seele jedes anständigen Mannes lasten“ würden. Nach eigenen Angaben schämte er sich damals, dass solche Dinge in Deutschland möglich waren, selbst angesichts der Kriegssituation. Er hatte sie also gesehen, die Zwangsarbeiter, die sogar da unten, wie er es beschreibt „temporär untergebracht“ waren.

Es liegt ein Brief von Brauns vom 15. August 1944 an Albin Sawatzki vor, der für die Planung und Steuerung der V2-Fabrikation verantwortlich war. Dieser belegt, dass von Braun im KZ Buchenwald war und dort selbst Häftlinge aussuchte. Viele Berichte und Dokumente sprechen für seine Involviertheit in die Vorgänge in Mittelbau-Dora. Im Erlebnisbericht von Adam Cabala ist zu lesen: „[…] auch die deutschen Wissenschaftler mit Prof. Wernher von Braun an der Spitze sahen alles täglich mit an. Wenn sie die Gänge entlang gingen, sahen sie die Schufterei der Häftlinge, ihre mühselige Arbeit und ihre Qual. Prof. Wernher von Braun hat während seiner häufigen Anwesenheit in Dora nicht ein einziges Mal gegen diese Grausamkeit und Bestialität protestiert. Selbst der Anblick von Toten haben ihn nicht gerührt: Auf einer kleinen Fläche neben der Ambulanzbude lagen tagtäglich haufenweise die Häftlinge, die das Arbeitsjoch und der Terror der rachsüchtigen Aufseher zu Tode gequält hatten. […] Aber Prof. Wernher von Braun ging daran vorbei, so nahe, dass er die Leichen fast berührte“.

Von Braun wohnte 1944 zeitweise in Bleicherode (20 Kilometer vom Lager Dora-Mittelbau entfernt), das KZ-Außenlager Bleicherode startete am 26. Oktober 1944. Das KZ-Außenlager Kleinbodungen öffnete am 3. Oktober 1944 nur vier Kilometer entfernt im Nachbarort für durchschnittlich etwa 620 KZ-Häftlinge. Rings um das nur acht Kilometer entfernte Nordhausen spann sich ein ganzes Netz von am Ende 40 Außenlagern des KZ Mittelbau. Im Spätsommer 1944 wurde sein Bruder Magnus von Braun direkt nach Dora-Mittelbau versetzt, wo er Gyroskope, Servomotoren und Turbopumpen für die A4 entwickelte.

Im Zusammenhang mit dem Ausbau von Dora-Mittelbau und der anschließenden Fertigung der A4-Rakete und anderer Waffen kamen nach offizieller Zählung in den SS-Akten ca. 12.000 Zwangsarbeiter ums Leben. Neueren Schätzungen zufolge könnte die Zahl der tatsächlichen Todesopfer sogar bis zu 20.000 betragen haben. Der Einsatz der Waffe forderte insgesamt ca. 8000 Opfer, hauptsächlich in der Zivilbevölkerung. Die V2 war somit die einzige Waffe, deren Produktion mehr Opfer forderte als ihr Einsatz.

Auch beim einzigen alliierten Prozess 1947, in dem ausschließlich Verbrechen im KZ Mittelbau-Dora verhandelt wurden, war von Braun weder angeklagt noch als Zeuge geladen. Allerdings sagte sein Bruder als Zeuge dort im Nordhausen-Prozess gegen die Lagerleitung des Konzentrationslagers Dora-Mittelbau aus. Er stand wie Wernher von Braun mittlerweile schon in US-amerikanischen Diensten.

Insgesamt kamen rund 3000 V2-Raketen zum Einsatz, rund ein Drittel davon gegen London, ebenso viele gegen Antwerpen, das mit seinem Hafen von hoher Bedeutung für den alliierten Nachschub war. Ein Angriff richtete sich auch gegen das von den alliierten Streitkräften befreite Paris.

Die Sprengkraft aller abgefeuerten V2-Raketen zusammen indes war kaum stärker als ein einziger mittlerer Bombenangriff im Zweiten Weltkrieg. Die Wirkung war psychologischer Art, weil es gegen diese „Wunderwaffe“ weder Abwehrmöglichkeiten noch Vorwarnung gab – die tatsächliche militärisch-strategische Bedeutung war aber gering.

Am 11. April 1945 besetzten US-Truppen die Produktionsstätten in Bleicherode, das "Mittelwerk". Einhundert A4-Raketen wurden in die USA abtransportiert und bildeten dort die Grundlage des US-amerikanischen Raketenprogramms.

Wenige Tage vorher wurden die Raketenpioniere um Wernher von Braun und General Walter Dornberger auf Befehl Hans Kammlers nach Süddeutschland verlegt, um den anrückenden Besatzern zu entgehen. Sie bezogen daraufhin eine Kaserne in Oberammergau. Später teilte sich die Gruppe auf, Wernher und sein Bruder Magnus kamen nach Weilheim in Oberbayern. Bei der Fahrt hatte sich von Brauns Gipsverband gelockert, woraufhin sie eine Privatklinik in Sonthofen aufsuchen mussten.

Kurz vor Erreichen der französischen Armee in Sonthofen ließ ihn Dornberger nach Oberjoch bringen, wo die Peenemünder Führungsgruppe im Sporthotel "Ingeburg" () Unterschlupf gefunden hatte. Dort verbrachten sie bei bestem Wetter und guter Verpflegung die letzten Kriegstage.

Nach der Besetzung Oberbayerns durch US-amerikanische Truppen kontaktierte der Englisch sprechende Bruder Magnus von Braun die US-Amerikaner, mit deren strategischem Interesse am deutschen Raketen-Know-how sie fest rechnen konnten. Noch zu Kriegszeiten wurden in der Aktion "Operation Overcast" gezielt deutsche Wissenschaftler gesucht, um sich ihres Wissens bemächtigen zu können. Am 2. Mai 1945 stellte sich von Braun zusammen mit einigen Wissenschaftlern aus seinem Team den US-Streitkräften in Reutte in Tirol.

Wegen seiner Mitgliedschaft in der NSDAP, der SS und seiner engen Beteiligung an der Kriegsführung des nationalsozialistischen Deutschlands sowie aufgrund der Konstruktion und des Baus der „Vergeltungswaffe“ V2 unter Einsatz von KZ-Häftlingen und Zwangsarbeitern ist von Brauns Person heute umstritten.

In der folgenden Zeit wurde Garmisch-Partenkirchen das Zentrum vieler von den Amerikanern internierter deutscher Raketenexperten. Dort wurden sie von den verschiedenen Geheimdiensten über das Raketenprogramm und ihre dortige Tätigkeit verhört. Eine weitergehende Aufarbeitung ihrer Vergangenheit seitens der Amerikaner fand nicht in nennenswertem Maße statt. Während des Sommers half von Braun bei der Organisation des Abtransports von Akten und Raketenteilen, die nicht den Sowjets zufallen sollten. Bereits im September 1945 flog er zusammen mit einer kleinen Vorausgruppe als Teil der geheimen Operation Paperclip in die Vereinigten Staaten.

Ihre neue Heimat wurde Fort Bliss, Texas, wo sie unter Aufsicht der US Army standen. Ende 1945/Anfang 1946 erreichten über hundert weitere Peenemünder Fort Bliss, darunter sein jüngerer Bruder Magnus. Eine ihrer ersten Aufgaben war es, die amerikanischen Experten in Funktionsweise und Bau der V2 zu unterrichten. In der Folgezeit starteten sie von White Sands aus regelmäßig V2 zu Testzwecken. Erst im Dezember 1946 wurde ihre Anwesenheit in Amerika öffentlich. Bisher war von Braun in den USA lediglich einem kleinen Kreis bekannt, was sich in den folgenden Jahren drastisch ändern sollte.

Überraschend hatte sich von Braun Ende 1946 mit seiner Cousine Maria von Quistorp (* 1928) schriftlich verlobt. Im Februar 1947 reiste er per Schiff in das besetzte Nachkriegsdeutschland zurück. Während des gesamten Aufenthalts stand er dabei unter militärischer Bewachung, da ein Entführungsversuch seitens der Sowjetunion befürchtet wurde. Am 1. März heiratete er in einer lutherischen Kirche in Landshut. Seine Eltern folgten ihrem Sohn mit Gemahlin auf dem Rückweg nach Amerika, wo sie die nächsten Jahre verbrachten. Am 9. Dezember 1948 wurde die Tochter Iris Careen geboren.

Bei ihrer ersten Ankunft in Amerika war von Braun noch von der raschen Aufnahme eines ambitionierten Raketenprogramms ausgegangen. Die Raketenforschung unterstand jedoch nach wie vor dem Militär und war damit ebenfalls betroffen von der vorherrschenden Demobilisierung. Erst der Koreakrieg konnte die finanzielle Lage verbessern. 1950 zog von Braun mit seinem Team nach Huntsville, um dort die Entwicklung der Redstone aufzunehmen. Die Redstone basierte auf dem Aggregat 4, war jedoch größer und leistungsstärker. Im August 1953 fand ihr erster Testflug statt. Zu der Zeit war von Braun für etwa 1000 Mitarbeiter verantwortlich.

Früh entstanden Pläne, mit der Redstone einen Satelliten in den Erdorbit zu starten. Dazu würden mehrere Loki-Feststoffraketen, gebündelt zu drei Stufen, auf der Redstone starten. Von Braun warb für das Projekt, scheiterte jedoch vor einer Kommission gegen das Konzept der Marine. Ebenso wie Redstone ging die Loki auf eine deutsche Entwicklung, die Taifun-Flugabwehrrakete, zurück.

Seine zweite Tochter, Margrit Cecile, wurde am 8. Mai 1952 geboren. Im Jahr 1952 kehrten außerdem seine Eltern wieder zurück nach Deutschland, wo sie später in Oberaudorf lebten; seine Mutter verstarb 1959, sein Vater starb 1972. Am 14. April 1955 wurden Wernher von Braun und seine Frau US-amerikanische Staatsbürger.

Im November 1955 wurde die Entwicklung einer Nachfolgerakete für die Redstone, der Jupiter, beschlossen. Die neu geschaffene "Army Ballistic Missile Agency" sollte für die Entwicklung zuständig sein. Ihr Leiter wurde Bruce Medaris, von Brauns Vorgesetzter. Zwar waren die Pläne für eine orbitale Redstone offiziell gestoppt, dennoch wollten sie für den Fall eines Scheiterns der Marine gerüstet sein. Dazu wurde das "Reentry Test Vehicle" entwickelt, eine Redstone mit Oberstufe identisch zum vorgeschlagenen Satellitenkonzept, lediglich die letzte Stufe sollte durch einen Gefechtskopf ersetzt werden. Später wurde die Rakete Jupiter-C genannt.

Parallel zu seiner Arbeit bei der Army warb von Braun öffentlich für das Raumfahrtprogramm. Im Oktober 1951 nahm er an der "First Symposium on Space Flight" teil, einer Konferenz, die im Hayden Planetarium in New York stattfand. Zwischen März 1952 und April 1954 veröffentlichte er zusammen mit anderen Autoren eine Serie von Artikeln in der Zeitschrift "Collier’s Weekly". Damit wurde der breiten US-amerikanischen Öffentlichkeit die bemannte Weltraumfahrt als technisch durchführbar vorgestellt.

Am 4. Oktober 1957 startete die Sowjetunion den ersten künstlichen Erdsatelliten Sputnik in eine Umlaufbahn. Inmitten des Kalten Krieges wurde der amerikanischen Öffentlichkeit die sowjetische Überlegenheit auf dem Gebiet der Raketentechnik vor Augen geführt. In der Folge des Sputnikschocks wurden die Raumfahrtausgaben abermals aufgestockt. Nachdem die Vanguard-Rakete der Marine beim Start versagt hatte, brachte am 1. Februar 1958 eine Jupiter-C Explorer 1 ins All. Am 17. Februar erschien von Braun mit der Bezeichnung "Missileman" auf dem Titelbild des "Time Magazine".

Der sowjetische Erfolg hatte in Amerika die unproduktive Konkurrenz der Teilstreitkräfte aufgezeigt. Im Juli 1958 wurde aus diesem Grund die zivile Luft- und Raumfahrtbehörde NASA gegründet. Die Verantwortlichen der NASA wollten von Beginn an von Brauns in der Raketenentwicklung erfahrene Abteilung übernehmen. Das Budget hätte jedoch nur für etwa 2000 der 5000 Angestellten gereicht, erst als sich ein Jahr später die finanzielle Situation verbessert hatte, war die Übernahme der ganzen Abteilung gesichert.

Von Braun und sein Team wurden offiziell im Oktober 1959 der NASA überstellt. Bereits vorher war die Entscheidung zum Bau der Saturn-Rakete (der späteren Saturn I) gefallen. Außerdem wurde das Mercury-Programm vorangetrieben, das erstmals den Flug eines Astronauten in den Weltraum ermöglichen sollte.

Am 2. Juni 1960 kam von Brauns drittes Kind, Sohn Peter Constantin, zur Welt. Im selben Jahr wurde von Braun Direktor des Marshall Space Flight Centers in Alabama, eine Position, die er bis 1970 innehatte. Das Mercury-Raumschiff war immer noch in der Testphase, als im April 1961 Juri Gagarin mit Wostok 1 einmal die Erde umrundete. Erst drei Wochen später folgte Alan Shepard auf einer Redstone, wobei lediglich ein suborbitaler Flug erfolgte. Amerika war abermals von der Sowjetunion geschlagen worden. Am 25. Mai verkündete Präsident Kennedy den bemannten Flug zum Mond innerhalb des Jahrzehnts als Ziel.

Über die nächsten Jahre nahm die Entwicklung rasant an Fahrt auf. Das Mercury-Programm wurde von Gemini abgelöst. Bis zu 400.000 Menschen arbeiteten schließlich am Apollo-Programm. 1967, zwei Jahre vor Kennedys Ultimatum, startete die unter von Brauns Leitung entwickelte Saturn V mit Apollo 4 zu ihrem Erstflug. Der erste bemannte Start im Folgejahr war gleichzeitig der erste Flug von Menschen in den Mondorbit.

Von Brauns größter Erfolg und die Erfüllung langjähriger Träume wurde die bemannte Mondlandung im Jahr 1969. Sein sowjetischer Rivale Sergei Koroljow, der Vater der sowjetischen Raumfahrt, konnte dieses Ereignis nicht mehr erleben – er war bereits 1966 gestorben. Koroljow wurde von Braun erst nach seinem Staatsbegräbnis bekannt, da das sowjetische Raumfahrtprogramm der Geheimhaltung unterlag.

Von 1970 bis 1972 war Wernher von Braun Direktor eines neu geschaffenen Planungsbüros der NASA, welches sich mit der Zukunft der US-Raumfahrt befassen sollte. Von Braun setzte sich für eine bemannte Mars-Mission ein, was jedoch aufgrund von Finanzierungsproblemen – nicht zuletzt durch den andauernden Vietnamkrieg – zunichtegemacht wurde. Daneben drang er auf technische Vereinfachungen des neuen Space-Shuttle-Systems, das zur damaligen Zeit noch deutlich größer und komplexer in Planung war.

Enttäuscht von den starken Budgetkürzungen durch den US-Kongress, verließ er 1972 die NASA und wurde einer der Vizepräsidenten von Fairchild, einem Luft- und Raumfahrtkonzern. Dort trat er unter anderem für neuartige Kommunikationssatelliten ein, welche eine Verbindung in abgelegene Gebiete ermöglichen sollten.

In den ersten Wochen nach Bekanntwerden seines Wechsels zu Fairchild stieg der Aktienkurs der Firma um 30 %. Seine Tätigkeit führte ihn häufig ins Ausland. Er traf dabei die indische Premierministerin Indira Gandhi, den Schah von Persien und den spanischen Thronfolger Juan Carlos. Im Juli 1975 wurde er Mitglied des Aufsichtsrats von Daimler-Benz.

Bei einer routinemäßigen medizinischen Untersuchung Mitte 1973 wurden auf einem Röntgenbild Auffälligkeiten neben seiner linken Niere entdeckt. Am Johns Hopkins Hospital in Maryland wurde ihm wenig später eine tumorbefallene Niere und umliegendes Tumorgewebe entnommen. Bereits nach wenigen Tagen hatte er sich von der Operation erholt, einige Wochen später konnte er wieder seiner Arbeit nachgehen.

Zwei Jahre nach der ersten Krebsoperation wurde bei einer Nachuntersuchung ein Dickdarmtumor entdeckt und entfernt. Sein sich von dort an beständig verschlechternder Gesundheitszustand ermöglichte es ihm ab November 1976 nicht mehr, das Krankenhaus zu verlassen.

Am 31. Dezember 1976 trat Wernher von Braun in den Ruhestand; am 16. Juni 1977 starb er an seiner Krankheit in Alexandria, Virginia, und wurde auf dem dortigen Ivy Hill Cemetery (Sektion T, Grabstelle 29) beigesetzt. Auf dem Grabstein stehen der Name, das Geburts- und das Todesjahr sowie der Hinweis auf den : „Die Himmel erzählen von der Herrlichkeit Gottes; und das Firmament verkündet seiner Hände Werk.“ Von Braun war evangelischer Christ.


1974 hielt er insgesamt 25 Ehrendoktortitel, darunter von den folgenden Hochschulen:

Wernher von Braun erlangte in den USA rasch eine große Popularität, auch wegen der Veröffentlichungen seiner Bücher und öffentlichen Auftritten. Bekannt machten ihn vor allem drei Fernsehproduktionen Walt Disneys: "Man in Space" (1955), "Man and the Moon" (1955) und "Mars and Beyond" (1957). In diesen von Ward Kimball realisierten Kurzfilmen trat von Braun an der Seite Disneys auf und erläuterte seine Theorien. Von Braun war ein geschickter Marketing-Stratege für Raketentechnik und bewerkstelligte es, eine Zusammenarbeit mit der Walt Disney Company zu erreichen. Im Kurzfilm "Man in Space" erklärt von Braun unter anderem die allgemeine Raketen-Funktionsweise und Einflüsse, welche Raumfahrer in der Lage sein müssten auszuhalten. Mit 42 Millionen Zuschauern gilt der Film als zweiterfolgreichste TV-Sendung aller Zeiten im US-Fernsehen.

Sein Buch "Das Marsprojekt" beeinflusste den von George Pal produzierten Science-Fiction-Film "Die Eroberung des Weltalls" ("Conquest of Space", 1955). Und bereits 1960 wurde seine Lebensgeschichte unter dem Titel "" als US-deutsche Koproduktion mit Curd Jürgens in der Titelrolle verfilmt.

Als von Braun zu einer Koryphäe der US-amerikanischen Raumfahrt aufstieg, wurde in der Öffentlichkeit und im Fernsehen gelegentlich nach seiner Vergangenheit im Dritten Reich gefragt. Von Braun distanzierte sich dabei stets vom Nationalsozialismus und wies auch eine Mitschuld an den nationalsozialistischen Verbrechen im Zweiten Weltkrieg von sich.

Zu von Brauns bis heute anhaltender Bezeichnung als Visionär schreibt der Politikwissenschaftler Rainer Eisfeld: „Braun profitierte von seiner Anpassung an den Zeitgeist, der die Implikation eigenen Handelns wegschob, indem er auswich auf eine Vision.“ In seinem 1996 erschienenen Buch "Mondsüchtig" beschreibt Eisfeld die Geschichte der Ingenieure, für die – unter der Leitung von Brauns – die Technik zum Selbstzweck wurde und die ihre tiefe Verstrickung in die Barbarei des Nationalsozialismus bis zuletzt verleugneten.

Von Braun wurde mehrfach musikalisch thematisiert:

Nach den erfolgreichen Apollo-Mondlandungen verfolgte Wernher von Braun weiter mit viel Elan weitreichende Pläne, bis hin zum bemannten Marsflug. Bei der NASA und auch in der US-amerikanischen Öffentlichkeit stieß er damit aber nicht nur auf Begeisterung. Ein Redakteur von "Reader’s Digest" kommentierte: „Wernher von Braun möchte am liebsten weiter Geld ausgeben wie ein volltrunkener Matrose“ (zit. in "Eisfeld").

Anlässlich des 100. Geburtstags im Jahr 2012 wurde auf Initiative des "Polnisch-Deutschen Kulturforums Insel Usedom" die so genannte "Peenemünder Erklärung" veröffentlicht, in der vor einer Idealisierung von Brauns gewarnt wird und eine „wissenschaftlich seriöse Aufarbeitung“ der Rolle von Brauns im Nationalsozialismus gefordert wird. Zu den Erstunterzeichnern gehören Historiker wie Werner Buchholz, Bernd Faulenbach, Anton Schindling und Thomas Stamm-Kuhlmann, aber auch Politiker wie Thomas Freund (Staatssekretär in der Landesregierung) und Karin Timmel (Landrätin).

1994 wurde der Mondkrater "von Braun" durch die Internationale Astronomische Union nach ihm benannt.

Das seit 1979 diesen Namen tragende Wernher-von-Braun-Gymnasium in Friedberg bei Augsburg benannte sich nach jahrelangen Diskussionen, Fernsehberichterstattung, Appellen – unter anderem von Kultusminister Ludwig Spaenle (CSU) – und Forderungen des Kreistages Aichach-Friedberg um. Dies teilte die Schule am 20. Dezember 2013 in einer Stellungnahme mit. Damit verbunden waren auch eine Distanzierung vom Namensgeber sowie die Aussage, dass in Wernher von Braun kein Vorbild für Schüler zu sehen sei. Seit dem 1. Februar 2014 heißt die Schule offiziell „Staatliches Gymnasium Friedberg“.

Die Wernher-von-Braun-Schule in Neuhof bei Fulda, eine seit 1975 nach ihm benannte Gesamtschule, entschloss sich nach längeren Diskussionen ebenfalls zu einer Umbenennung. Sie trägt seit Februar 2015 den Namen „Johannes-Kepler-Schule“.

Gegenwärtig sind noch einige Straßen in deutschen Städten nach Wernher von Braun benannt, wobei es auch hier mancherorts Bestrebungen für eine Umbenennung gibt. So wurde die Wernher-von-Braun-Straße in Memmingen im Juni 2014 in Rudolf-Diesel-Straße umbenannt. Die Wernher-von-Braun-Straße in Neuhof soll analog der gleichnamigen Schule ebenfalls umbenannt werden.

Das Von-Braun-Paradigma ist nach ihm benannt.







</doc>
<doc id="5621" url="https://de.wikipedia.org/wiki?curid=5621" title="Weimarer Verfassung">
Weimarer Verfassung

Die Weimarer Verfassung (auch Weimarer Reichsverfassung, kurz WRV; offiziell: Verfassung des Deutschen Reichs) war die am 31. Juli 1919 in Weimar beschlossene, am 11. August ausgefertigte und am 14. August 1919 verkündete erste effektive demokratische Verfassung Deutschlands. Mit ihr wurde das Deutsche Reich zu einer föderativen Republik mit einem gemischt präsidialen und parlamentarischen Regierungssystem.

Die Weimarer Verfassung löste das am 10. Februar 1919 erlassene Gesetz über die vorläufige Reichsgewalt ab, das die wichtigsten künftigen Verfassungsorgane und ihre Zuständigkeiten beschrieb. Viele ihrer Artikel entstammten direkt der Paulskirchenverfassung von 1849 und flossen ihrerseits in das heute geltende Grundgesetz für die Bundesrepublik Deutschland ein.

Nach dem Ort ihrer Verabschiedung wird das Deutsche Reich für die Dauer seiner demokratischen Periode von 1919 bis 1933 als Weimarer Republik bezeichnet. Der 11. August wurde in den Folgejahren zu deren Nationalfeiertag.

Die Deutsche Revolution 1848/49 war Teil einer europaweiten Revolutionsbewegung. In ihr kam der Widerstand gegen die vorherrschende monarchische Ordnung nach der Restauration zum Ausdruck. In ihrem Gefolge wurde die Verfassung des geplanten Deutschen Reichs am 27. März 1849 in der Paulskirche in Frankfurt am Main von der verfassunggebenden deutschen Nationalversammlung nach langen Diskussionen beschlossen. Amtlich verkündet wurde sie einen Tag später. Aufgrund des Tagungsortes der Nationalversammlung wird sie als "Paulskirchenverfassung" oder auch "Frankfurter Reichsverfassung" bezeichnet.

Die Paulskirchenverfassung sah die Schaffung einer Erbmonarchie mit konstitutionellen Zügen vor. Zu diesem Zweck trug die Kaiserdeputation dem preußischen König Friedrich Wilhelm IV. die deutsche Kaiserkrone an. Dieser berief sich auf das Gottesgnadentum und lehnte ab. Damit scheiterte die Verfassung des Paulskirchenparlaments.

Am 16. April 1871 trat die Bismarcksche Reichsverfassung als Verfassung des neu gegründeten Deutschen Reiches in Kraft. Sie ging aus der Verfassung des Norddeutschen Bundes von 1867 hervor. Die von Otto von Bismarck geprägte Verfassung besaß keinen Grundrechtsteil, sondern beschränkte sich auf Bestimmungen für die Zuständigkeiten der einzelnen staatlichen Organe. Sie sah außerdem weiterhin die Monarchie als Staatsform vor.

Die Bismarcksche Reichsverfassung wurde erst durch das Inkrafttreten der Weimarer Verfassung 1919 abgelöst, die sich an der Paulskirchenverfassung orientierte und wieder einen Grundrechtsteil enthielt.

Staatstheoretisch wurde die Weimarer Verfassung von der Parlamentarismustheorie Robert Redslobs beeinflusst, die über den „Vater“ der Weimarer Verfassung, Hugo Preuß, konkreten Eingang in den Verfassungstext erhielt.

Die offizielle Bezeichnung für das Dokument lautet "Verfassung des Deutschen Reichs". Um es begrifflich von der offiziell genauso genannten Bismarckschen Reichsverfassung abzugrenzen, wird es in Geschichtswissenschaft und Publizistik nach seinem Entstehungsort Weimar als "Weimarer Verfassung" oder "Weimarer Reichsverfassung" bezeichnet.

Am 19. Januar 1919 fanden die Wahlen zur verfassunggebenden Nationalversammlung statt. Frauen besaßen sowohl das aktive als auch passive Wahlrecht. Die Sitze wurden nach dem Verhältniswahlrecht verteilt. Die SPD war die stärkste Fraktion und bildete mit dem Zentrum und der Deutschen Demokratischen Partei (DDP) die so genannte Weimarer Koalition.
Am 6. Februar 1919 trat die Nationalversammlung das erste Mal im Deutschen Nationaltheater in Weimar zusammen. Berlin war nicht der Tagungsort, weil dort Unruhen die Unabhängigkeit und Sicherheit der Abgeordneten gefährdeten. Die Wahl Weimars war wohl auch als Zeichen für die Anknüpfung an die Humanitätsideale der Weimarer Klassik gemeint, hatte aber vor allem militärische Gründe – das zuerst vorgesehene Erfurt wäre im Angriffsfall schlechter zu verteidigen gewesen.

Am ersten Entwurf für eine Verfassung war der Staatssekretär des Reichsamts des Inneren und spätere Reichsminister des Innern Hugo Preuß maßgeblich beteiligt, nachdem die zwischenzeitlichen Erwägungen des Rats der Volksbeauftragten, Max Weber in dieses Amt zu berufen, nicht umgesetzt wurden.

Da fast alle politischen Strukturen der Kaiserzeit wie zum Beispiel der Bundesrat, die in der Reichsverfassung von 1871 festgeschrieben waren, wegfielen oder bedeutungslos wurden, kam es zu Auseinandersetzungen zwischen den Parteien, die Anhänger der Monarchie waren, und denen, welche die Republik befürworteten. Am 31. Juli 1919 beschloss die Nationalversammlung die Verfassung in ihrer endgültigen Form mit 262 zu 75 Stimmen; dabei waren 84 Abgeordnete abwesend. Am 11. August 1919 unterzeichnete Reichspräsident Friedrich Ebert die Weimarer Verfassung in Schwarzburg. Sie trat mit ihrer Verkündung am 14. August 1919 in Kraft (Reichsgesetzblatt 1919, S. 1383). Der 11. August wurde zum Nationalfeiertag der Weimarer Republik, weil er an die „Geburtsstunde der Demokratie in Deutschland“ erinnern sollte.

Die Weimarer Verfassung galt auch nach der Machtübernahme der NSDAP am 30. Januar 1933 formell fort. Sie wurde jedoch materiell weitestgehend außer Kraft gesetzt, zunächst durch die "Verordnung des Reichspräsidenten zum Schutz von Volk und Staat", besser bekannt als „Reichstagsbrandverordnung“ vom 28. Februar 1933. Die Verordnung annullierte die 81 Mandate der Kommunistischen Partei Deutschlands und machte den Weg frei für die notwendige Zweidrittelmehrheit zur Verfassungsänderung, die das "Gesetz zur Behebung der Not von Volk und Reich" („Ermächtigungsgesetz“) ermöglichten. Das zunächst auf vier Jahre befristete Gesetz wurde am 23. März 1933 verabschiedet und später mehrmals verlängert.

Gleichwohl haben führende Kommentatoren der NS-Zeit bereits 1933 die Weimarer Verfassung als außer Kraft gesetzt betrachtet und das Ermächtigungsgesetz als „Vorläufiges Verfassungsgesetz des neuen Deutschlands“ bezeichnet. Der Übergang der verfassungsgebenden Gewalt auf die Reichsregierung (und damit die Beseitigung dessen Vorbehaltes, dass Reichsrat und Reichstag unangetastet bleiben) regelte dann Artikel 4 des "Gesetzes über den Neuaufbau des Reichs" vom 30. Januar 1934. Nach dieser Betrachtungsweise sei die Weimarer Verfassung gegenstandslos geworden. 

Auch mit der Übernahme der Regierungsgewalt durch den Alliierten Kontrollrat am 5. Juni 1945 blieb die Weimarer Verfassung weiterhin außer Funktion. 

, 137, 138, 139 und 141 der Weimarer Verfassung sind 1949 durch Artikel 140 Bestandteil des Grundgesetzes geworden. Die sonstigen Normen der Verfassung galten nach 1949, soweit sie nicht dem Grundgesetz widersprachen, als einfaches Bundesrecht fort; nach einer Rechtsbereinigung in den sechziger Jahren ist heute jedoch nur noch Abs. 3 Satz 2 („Adelsbezeichnungen gelten nur als Teil des Namens und dürfen nicht mehr verliehen werden“) in Kraft.

Die Verfassung war der deutschen Verfassungstradition gemäß funktional in drei Teile aufgeteilt. Einerseits wurde im Außenverhältnis die Zuständigkeit des Reichs von der Zuständigkeit der Reichsländer (die ehemaligen Bundesstaaten des Kaiserreichs) abgegrenzt (Verbandszuständigkeit des Reiches). Andererseits stellte die Verfassung ein Organisationsstatut dar, in dem die Staatsorgane des Reichs benannt und ihre Kompetenzen untereinander festgesetzt wurden (Organzuständigkeit). Soweit die Vorschriften der Reichsverfassung die Organzuständigkeit regelten, stellte die Verfassung Binnenrecht dar. Eine dritte Art von Vorschriften regelte das Verhältnis zwischen den Bürgern und dem Staat. Anders als die Bismarcksche Reichsverfassung enthielt die Weimarer Reichsverfassung im zweiten Hauptteil einen umfassenden Grundrechtskatalog.

Zunächst werden die Zuständigkeiten des Reichs dargestellt, danach ein Überblick über die Staatsorgane (Reichstag, Reichspräsident und Reichsregierung, Reichsrat, Staatsgerichtshof) und ihre Kompetenzen gegeben. Zuletzt wird auf das Verhältnis zwischen Bürgern und Reich eingegangen (Grundrechte, Grundpflichten).


Die Verfassung folgt dem Prinzip der begrenzten Einzelermächtigung. Wo nicht das Reich durch die Verfassung ausdrücklich für zuständig erklärt wurde, waren die Reichsländer berufen („im Zweifel für die Reichsländer“). Die Zuständigkeiten des Reichs wurden aber im Vergleich zu der Bismarckschen Reichsverfassung erheblich ausgeweitet. 

Das Reich konnte nur dort gesetzgeberisch tätig werden, wo die Verfassung ihm ausdrücklich einen Titel zusprach. Dabei wurde zwischen Gesetzgebungstiteln unterschieden, auf deren Sachgebiet nur das Reich regulierend tätig werden durfte (Art. 6 WRV, ausschließliche Gesetzgebung), Titeln, bei denen die Länder Recht setzen konnten, soweit das Reich nicht tätig geworden ist (Art. 7 f. WRV, sog. konkurrierende Gesetzgebung) und Titeln, auf die das Reich nur bei dem Bedürfnis einer reichseinheitlichen Regelung ein Gesetz stützen durfte (Art. 9 WRV). Auch war eine Rahmengesetzgebungskompetenz in Art. 10 WRV vorgesehen. Soweit das Reich Gesetze erlassen hatte, brach Reichsrecht das Landesrecht; das Landesrecht wurde insoweit nichtig. 

Umfasste die ausschließliche Gesetzgebung noch Bereiche, die traditionell dem Reiche oblagen (Staatsverträge und Kolonialwesen, Staatsangehörigkeit, Freizügigkeit im Reichsgebiet, Ein- und Auswanderung, Auslieferung, Wehrrecht, Münzwesen, Zollrecht einschließlich die Einheit des Zoll- und Handelsgebietes und der Freizügigkeit des Warenverkehrs, Post- und Fernmeldewesen), ging die konkurrierende Gesetzgebung weit über das Gewohnte hinaus. Neben den tradierten Gegenständen des Reichsrechts (Justizpolitik: Bürgerliches Recht, Handelsrecht, Strafrecht, Prozessrecht und Strafvollstreckungsrecht; Innenpolitik: Passrecht, Fremdenpolizei, Press-, Vereins-, Versammlungswesen; Sozial- und Arbeitspolitik: Arbeitsrecht, Sozialversicherungen, Einrichtung beruflicher Vertretungen für das Reichsgebiet; Verkehrspolitik: Seeschifffahrt, Eisenbahn, Binnenschifffahrt, Fahrzeugverkehr zu Lande, im Wasser und in der Luft; Wirtschaftspolitik: Versicherungswesen, Bankwesen, Börsenwesen, Gewerberecht, Vergesellschaftung, Enteignungsrecht, Handel, das Maß- und Gewichtswesen, die Ausgabe von Papiergeld) waren Gesetzgebungskompetenzen bezüglich des Armenwesens, der Wandererfürsorge, der Fürsorge für die Kriegsteilnehmer und ihre Hinterbliebenen, Einrichtung beruflicher Vertretungen für das Reichsgebiet, Straßenbau, Bergbau, Gesundheitswesen, Veterinärwesen, Verkehr mit Nahrungs- und Genussmitteln, Küstenfischerei, Pflanzenschutz, Theater- und Lichtspielwesen und insbesondere über das Abgabenrecht (Steuern und Beiträge) einschließlich des dazugehörenden Verfahrensrechts neu. Politisch bedeutete diese Zuständigkeit des Reichs für die Länder, dass das Reich nicht mehr ihr „Kostgänger“ war, sondern es nunmehr die Möglichkeit hatte, die eigenen Einnahmen festzulegen. Es konnte sogar diejenigen Steuern bestimmen, welche den Ländern zuflossen. Das Reich hatte dabei lediglich auf die Lebensfähigkeit der Länder Rücksicht zu nehmen. Machtpolitisch bedeutend konnte auch die Bedürfnisgesetzgebung über das Ordnungs- und Polizeirecht sein, von dem das Reich allerdings keinen Gebrauch machte. Daher blieb das Länderpolizeirecht bestehen. Selbst in traditionellen Länderangelegenheiten wie der Schul- und Hochschulpolitik konnte das Reich Rahmengesetze erlassen. Die Rahmengesetzgebung erstreckte sich auch auf die Rechte und Pflichten der Religionsgesellschaften, das wissenschaftliche Büchereiwesen, das Recht der Beamten der Länder und sonstigen Körperschaften, das Bodenrecht, die Bodenverteilung, das Ansiedlungs- und Heimstättenwesen, die Bindung des Grundbesitzes, das Wohnungswesen, die Bevölkerungsverteilung und das Bestattungswesen.

Völlig neu waren die Elemente der direkten Demokratie in der Weimarer Verfassung. Über Volksbegehren und Volksentscheid hatte das Volk die Möglichkeit, auf die Gesetzgebung einzuwirken. Gemäß Artikel 73 Absatz 3 war ein Volksentscheid durchzuführen, wenn mindestens 10 % der Wahlberechtigten einen solchen mit einem Volksbegehren verlangte. Der Reichstag konnte einen Volksentscheid durch unveränderte Verabschiedung eines Gesetzes mit dem Inhalt des Volksbegehrens abwehren. Durch Volksentscheid konnte ein Beschluss des Reichstags nur außer Kraft gesetzt werden, wenn sich die Mehrheit der Wahlberechtigten an der Abstimmung beteiligte. Der Reichspräsident konnte bestimmen, dass ein Gesetz durch einen Volksentscheid bestätigt werden musste (Art. 73).

Die Reichsverwaltung folgt zunächst der deutschen Verfassungstradition: Reichsgesetze werden durch die Behörden der Länder ausgeführt. Danach war scheinbar die Gesetzgebungszuständigkeit gegenüber der Verwaltungszuständigkeit überschießend geregelt: Landesgesetze wurden durch die Länder in eigenen Angelegenheiten ausgeführt; das Gleiche galt für Reichsgesetze, es sei denn die Reichsverfassung sah einen Vollzug durch Reichsbehörden vor. Völlig abweichend von der Bismarckschen Reichsverfassung und dem Grundgesetz, der heutigen Verfassung Deutschlands, konnte das Reich aber durch einfaches Reichsgesetz die Vollzugszuständigkeit an sich ziehen (Art. 14 WRV). Ein solches Reichsgesetz löste noch nicht einmal die Zustimmungspflicht des Reichsrats aus. Damit stand dem Reich die politische Macht zu, durch Reichsgesetz den Vollzug von Reichsrecht mit der Gesetzgebungszuständigkeit des Reiches gleichzuschalten. 

Die Aufsicht über die Ausführung von Reichsgesetzen durch die Länder stand der Reichsregierung zu. Die Reichsregierung konnte für die Gesetze, die durch die Länder ausgeführt wurden, mit Zustimmung des Reichsrats Verwaltungsvorschriften erlassen. Sie konnte Landesbehörden anweisen. Zum Zwecke der Aufsicht konnte sie zu den obersten Landesbehörden und mit deren Zustimmung zu den mittleren und unteren Behörden Beauftragte entsenden. 

Eine einheitliche Reichsverwaltung von Verfassung wegen bestand z. B. für den Auswärtigen Dienst, die Zoll- und Verbrauchssteuerverwaltung, das Post- und Fernmeldewesen, die Reichsbahn, die Reichswasserstraßenverwaltung. Die Abgabenverwaltung war allerdings Ländersache. Das Reich konnte jedoch den Ländern Weisungen hinsichtlich der Durchführung der Reichsabgabengesetze machen und Kontrollbehörden einrichten.

Den Ländern blieb nur bei der Rechtsprechung die gewohnte Zuständigkeit. Die Länder waren Gerichtsherren, soweit nicht das Reich von Verfassungs wegen Gerichtsherr war. Durch einfaches Reichsgesetz konnte sich das Reich nicht die Zuständigkeit für die Gerichte schaffen. Nach der Verfassung war ein Reichsgericht vorgesehen; es wurde auch ein Staatsgerichtshof für das Deutsche Reich eingerichtet. Die bisher bestehende Militärgerichtsbarkeit des Reiches wurde zugunsten der ordentlichen Gerichtsbarkeit aufgelöst. Auch sollten sowohl beim Reich wie bei den Ländern Verwaltungsgerichte bestehen. Ein Reichsverwaltungsgericht wurde allerdings erst 1942 ins Leben gerufen.

Das Deutsche Reich hatte nach der Weimarer Reichsverfassung den Reichstag, den Reichspräsidenten, die Reichsregierung, den Reichsrat und den Staatsgerichtshof als Staatsorgane. Das Reich handelte durch seine Staatsorgane. Durch Artikel 1 der Verfassung wurde die neue Staatsform einer Republik konstituiert. Die Wahl von Reichstag und Reichspräsident durch das Deutsche Volk, die Möglichkeit des Volkes, über Volksentscheid und Volksbegehren auf die Gesetzgebung einzuwirken, bildete die vom Volk ausgehende Staatsgewalt in Form einer gemischt repräsentativ-plebiszitären Demokratie (Volkssouveränität). Auch das betonte Art. 1 WRV noch einmal. Jedes Land, das Bestandteil des Deutschen Reichs ist, muss eine freistaatliche Verfassung haben, und seine Volksvertretung muss in einer allgemeinen, gleichen, unmittelbaren und geheimen Verhältniswahl von Männern und Frauen bestimmt werden (Art. 17 WRV); damit wurde gesichert, dass die innere Grundstruktur von Reich und Ländern gleich ist. 

Das wichtigste Organ war der vom Volk gewählte Reichstag, welcher die Gesetzgebung (legislative Gewalt) ausübte und die Reichsregierung überprüfte. Die Möglichkeit eines Misstrauensvotums prägte den Parlamentarismus. Der Reichstag wurde auf vier Jahre gewählt. Es wurde das Prinzip der Verhältniswahl angewandt, das heißt: die Zusammensetzung des Parlaments entsprach genau dem Verhältnis der abgegebenen Stimmen. Schon unter der Reichsverfassung von 1871 herrschte ein gleiches Wahlrecht. Die Abgeordneten, die in allgemeiner, geheimer, gleicher und unmittelbarer Verhältniswahl von Personen über 20 Jahren bestimmt werden (Art. 22), sind als Vertreter des Volkes nur ihrem Gewissen unterworfen und nicht an Aufträge gebunden (Art. 21). Der Reichstag kann gemäß Artikel 25 vom Reichspräsidenten aufgelöst werden, jedoch nur einmal aus dem gleichen Anlass. Jedoch kann der Reichstag mit einer Zweidrittelmehrheit eine Volksabstimmung über die Absetzung des Reichspräsidenten beschließen (Art. 43).

Außerdem wurde festgesetzt, dass die Reichsverfassung durch den Reichstag nur bei Anwesenheit von mindestens zwei Dritteln der gesetzlichen Mitgliederzahl mit einer Mehrheit von zwei Dritteln der Anwesenden oder durch eine Mehrheit der Stimmberechtigten bei einem Volksentscheid, der auf Grund eines Volksbegehrens stattfindet, geändert werden kann (Art. 76). Die verfassungsändernde Gewalt war inhaltlich vollkommen frei; sie war insbesondere nicht an bestimmte Staatsstrukturgrundbestimmungen (z. B. Gewaltenteilung, Föderalismus usw.) gebunden. Die Verfassungsänderung musste nicht in der Verfassung selbst erfolgen, sondern konnte auch im Wege von Einzelgesetzen mit Verfassungsrang vorgenommen werden. Verfassungsänderungen konnten zeitlich befristet werden. Diese weitgehende Freiheit des Reichstags versetzte ihn in die Lage, zeitlich befristete Verfassungsänderungen in Einzelgesetzen zu beschließen, welche eine Übertragung der Gesetzgebungsbefugnis auf die Reichsregierung vorsahen (Ermächtigungsgesetz).

Der Reichspräsident wird „vom ganzen deutschen Volke“ gewählt. Er muss mindestens 35 Jahre alt sein (Art. 41). Die Amtszeit des Reichspräsidenten beträgt sieben Jahre, der Reichstag kann mit einer Zweidrittelmehrheit eine Volksabstimmung über die Absetzung des Reichspräsidenten beschließen (Art. 43). Der Reichspräsident ist völkerrechtlicher Vertreter des Reiches (Art. 45), Oberbefehlshaber über die gesamte Wehrmacht des Reichs (Art. 47). Er kann zur Wiederherstellung des Reichsfriedens Grundrechte außer Kraft setzen und die zur Wiederherstellung der öffentlichen Sicherheit und Ordnung nötigen Maßnahmen treffen (Art. 48 Abs. 2). Letztere Kompetenz wurde in Staatspraxis und Rechtswissenschaft als Befugnis verstanden, Notverordnungen zu erlassen.

Um die Macht des Parlaments einzuschränken, wurde das Amt des Reichspräsidenten mit weit reichenden Kompetenzen ausgestattet. Er war in seiner Position mit dem starken Staatsoberhaupt der konstitutionellen Monarchie vergleichbar („Ersatzkaiser“). Der Reichspräsident ernannte und entließ die Mitglieder der Reichsregierung, repräsentierte das Volk, ernannte (auf Vorschlag des Reichsrates) Richter und hatte den Oberbefehl über die Reichswehr. Besonders die Art. 25 (Auflösung des Reichstags) und 48 (Recht, bei Gefährdung der Ordnung Grundrechte außer Kraft zu setzen) zeigten sehr deutlich seine starke Machtposition.

Die Reichsregierung bestand aus dem Reichskanzler und den von diesem vorgeschlagenen Reichsministern, die wie auch der Kanzler selbst vom Reichspräsidenten ernannt (Art. 52 und 53) und nicht vom Reichstag gewählt wurden. Die Reichsregierung bildete ein echtes Kollegialorgan, innerhalb dessen jeder Reichsminister innerhalb seines Sachgebiets selbständig entschied (Ressortprinzip). Nach Art. 56 Abs. 2 leitete jeder Reichsminister den ihm anvertrauten Geschäftszweig selbständig und unter eigener Verantwortung gegenüber dem Reichstag. Die Reichsminister hatten der Reichsregierung alle Gesetzentwürfe, ferner Angelegenheiten, für welche Verfassung oder Gesetz dieses vorschreiben, sowie Meinungsverschiedenheiten über Fragen, die den Geschäftsbereich mehrerer Reichsminister berührten, zur Beratung und Beschlussfassung zu unterbreiten. 

Für grundsätzliche Fragen und Angelegenheiten der Abstimmung zwischen den Ressorts war der Reichskanzler im Rahmen seiner Richtlinienkompetenz zuständig. Alternativ konnte auch das Kabinett mit Stimmenmehrheit entscheiden; bei Stimmengleichheit entschied die Stimme des Reichskanzlers. Die Reichsregierung gab sich mit Genehmigung des Reichspräsidenten eine Geschäftsordnung.

Die Reichsregierung hatte ein Gesetzesinitiativrecht im Reichstag. Auch im Reichsrat besaß sie ein Antragsrecht. 

Sie war überdies oberste Aufsichtsbehörde für die Ausführung der Reichsgesetze durch die Länder. Die Reichsregierung konnte mit Zustimmung des Reichsrats einheitliche Verwaltungsvorschriften erlassen. Sie konnte sogar allgemeine Anweisungen an die Länderbehörden betreffend die Ausführung von Reichsgesetzen im Einzelfall geben. Sie war ermächtigt, zur Überwachung der Ausführung der Reichsgesetze zu den Landeszentralbehörden und mit ihrer Zustimmung zu den unteren Behörden Beauftragte zu entsenden. 

Sowohl der Reichskanzler, als auch die Reichsminister müssen zurücktreten, wenn der Reichstag ihnen das Vertrauen entzieht (Art. 54). Diese Vorschrift, welche ein parlamentarisches Regierungssystem zur Folge hatte, fand ihre Vorläuferregelung in der Oktoberverfassung. Über dieses destruktive Misstrauensvotum konnte der Reichstag jeden einzelnen Reichsminister – und nicht nur die Reichsregierung insgesamt – stürzen, ohne dass für eine neue Reichsregierung oder für einen neuen Reichsminister im Reichstag eine parlamentarische Mehrheit vorhanden wäre. In der Praxis wurde dieses destruktive Misstrauensvotum von der NSDAP und der KPD ab dem Zeitpunkt, ab welchem die Weimarer Koalition keine parlamentarische Mehrheit mehr hatte, genutzt, um die Regierungen zu stürzen, ohne dass die fähig gewesen wären gemeinsam eine Koalitionsregierung zu bilden. Art. 54 trug wesentlich zur Destabilisierung der Republik bei, was sich in insgesamt 21 Regierungen der Weimarer Republik äußerte.

Als weiteres Verfassungsorgan wurde der Reichsrat gebildet. Er vertrat die Länder bei der Gesetzgebung und Verwaltung des Reichs (Art. 60 WRV). Die Anzahl der Stimmen der einzelnen Länder war abhängig von der Größe und Einwohnerzahl des Landes (Art. 61 Abs. 1 WRV). Allerdings durfte nach Art. 61 Abs. 1 S. 4 WRV kein Land durch mehr als zwei Fünftel aller Stimmen vertreten sein. Dies hatte zur Folge, dass Preußen lediglich 26 der insgesamt 66 Stimmen erhielt. Bei strikter Durchführung des proportionalen Prinzips hätten Preußen 53 Stimmen zugestanden. An zweiter Stelle stand Bayern mit 11 Stimmen. Der Reichsrat setzte sich nach Art. 63 Abs. 1 S. 1 WRV aus Vertretern der Landesregierungen zusammen. Jedoch wurde gem. Art. 63 Abs. 1 S. 2 WRV die Hälfte der preußischen Stimmen nach Maßgabe eines Landesgesetzes von den preußischen Provinzialverwaltungen bestellt. Somit entsandte die preußische Staatsregierung lediglich 13 Vertreter, wohingegen die restlichen 13 Stimmen durch je einen Vertreter der 13 preußischen Provinzen wahrgenommen wurden. Die Vertreter der Landesregierungen besaßen ein imperatives Mandat, während die Vertreter der preußischen Provinzen über ein freies Mandat verfügten.

Der Reichsrat besaß das Recht, sein Veto gegen die Beschlüsse des Reichstags einzulegen. Außerdem durfte er Vorschläge für die Besetzung des Reichsgerichts machen. Er hatte im Gegensatz zu Reichspräsident und Reichstag nur einen sehr geringen Anteil an der Macht in der Weimarer Republik; allgemein wird er als schwächer bewertet als der Bundesrat im Kaiserreich bzw. in der Bundesrepublik.

Nach Maßgabe eines Reichsgesetzes wurde ein Staatsgerichtshof für das Deutsche Reich errichtet. Der Staatsgerichtshof war zuständig insbesondere für Verfassungsstreitigkeiten innerhalb eines Landes, in dem kein Gericht zu ihrer Erledigung besteht, sowie über Streitigkeiten nichtprivatrechtlicher Art zwischen verschiedenen Ländern oder zwischen dem Reiche und einem Lande auf Antrag eines der streitenden Teile. Ferner war der Staatsgerichtshof für die Präsidenten-, Reichskanzler- oder Ministeranklage auf Antrag des Reichstags mit der Behauptung zuständig, dass der Reichspräsident, der Reichskanzler oder ein Reichsminister schuldhaft die Reichsverfassung oder ein Reichsgesetz verletzt habe.

Der erste Abschnitt des Zweiten Hauptteiles erklärt die Gleichheit aller Deutschen vor dem Gesetz und die Abschaffung der Standesunterschiede (Art. 109). Rechtsgleichheit ist also noch ein Bürgerrecht, kein Menschenrecht, wie nach dem Grundgesetz. Es werden keine weiteren Adelstitel verliehen, der Staat verleiht keine Orden und Ehrenzeichen, und kein Deutscher darf ausländische Titel oder Orden annehmen (Art. 109). Es werden weiterhin die Unverletzlichkeit der Wohnung (Art. 115) und das Recht auf freie Meinung (und deren Äußerung) zugesichert. Zum ersten Mal in der deutschen Geschichte enthielt die Verfassung zudem einen Artikel, der sogenannten „fremdsprachigen Volksteilen“ (z. B. Litauern, Sorben und Polen) das Recht auf Gebrauch ihrer Sprache zusicherte (Art. 113).

Der zweite Abschnitt setzt den Schutz von Ehe und Mutterschaft (Art. 119), sowie die Versammlungsfreiheit (Art. 123), die Wahlfreiheit (Art. 125) und die Gleichberechtigung weiblicher Beamter (Art. 128) fest. Beamte sind nicht Diener einer Partei, sondern der gesamten Gesellschaft (Art. 130).

Im dritten Abschnitt werden Glaubensfreiheit und Gewissensfreiheit garantiert. Auf eine Staatskirche wird verzichtet; damit war das bis dahin noch geltende „landesherrliche Kirchenregiment“ abgeschafft, nach dem der Landesherr Träger der Regierungsgewalt in der evangelischen Landeskirche war.

Der vierte Abschnitt erklärt, dass der Staat das Schulwesen beaufsichtigt. Es gibt öffentliche Schulen und eine allgemeine Schulpflicht. Gemäß dem Weimarer Schulkompromiss sollte ein ergänzendes Reichsschulgesetz die demokratische Ausgestaltung des Schulwesens näher bestimmen. Im Übrigen wird in diesem Abschnitt der Denkmalschutz als Aufgabe des Staates festgesetzt.

Der fünfte Abschnitt regelt das Wirtschaftsleben und schreibt, was für diese Zeit eher ungewöhnlich war, auch „soziale Rechte“ (Art. 162) fest. So muss laut Artikel 151 Abs. 1 Satz 1 das Wirtschaftsleben „den Grundsätzen der Gerechtigkeit mit dem Ziele der Gewährleistung eines menschenwürdigen Daseins für alle entsprechen“. Die wirtschaftliche Freiheit des Einzelnen wird gewährleistet, findet ihre Grenzen aber an diesen Grundsätzen. Im Artikel 153 Abs. 3 heißt es: „Eigentum verpflichtet. Sein Gebrauch soll zugleich Dienst sein für das Gemeine Beste.“ Zudem wird das Recht auf eine angemessene Wohnung (Art. 155) erwähnt, und Mütter, Kranke und Alte besonders geschützt (Art. 161). Außerdem enthält dieser Abschnitt die Regelung des Erbrechtes und die Schaffung eines einheitlichen Arbeitsrechts. Der Schutz von Urheberrechten (Art. 158) und von Arbeitnehmerrechten wird garantiert, was auch die Bildung von Betriebsräten beinhaltet. Der Verfassungsauftrag, einen "Reichswirtschaftsrat" zu schaffen, blieb bis zum Ende der Weimarer Republik unerfüllt. Lediglich ein Vorläufiger Reichswirtschaftsrat trat 1920 ins Leben (Art. 161 bis 164).

Die Übergangs- und Schlussbestimmungen regeln das Inkrafttreten der einzelnen Artikel der Verfassung. Es wird zudem bestimmt, dass die Nationalversammlung bis zum Zusammentritt des ersten Reichstages dessen Position übernimmt.

1925 lautete eine zeitgenössische, als „rückblickend“ bezeichnete Einschätzung:

Es wird immer wieder diskutiert, inwieweit einzelne Teile der Weimarer Verfassung zum Untergang der Republik beigetragen haben. Dabei wurden den Verfassern der Verfassung Versäumnisse vorgeworfen, die letztendlich mit zum Untergang der ersten deutschen Demokratie beigetragen haben sollen.

Viele der „Konstruktionsfehler“ müssen jedoch kritisch diskutiert werden und die innen- wie außenpolitischen und gesellschaftlichen Umstände, unter denen die Verfassung entstand, berücksichtigt werden. Des Weiteren muss der Umstand Beachtung finden, dass der Parlamentarische Rat von 1949 aus den Fehlern der Weimarer Verfassung lernen konnte, die Verfasser der Weimar Verfassung um den Berliner Staatsrechtler und Kommunalpolitiker Professor Hugo Preuß aber bis auf den Versuch der Paulskirche kein vergleichbares Vorbild hatten. Außerdem muss man beachten, dass eine Verfassung nur einen Rahmen abzugeben vermag, der durch konkrete Politik auszufüllen ist, aber auch unausgefüllt bleiben kann.


Problematisch war auch z. B. die Praxis, so genannte „verfassungsdurchbrechende“ Reichsgesetze zu beschließen. Dabei durften Gesetze der Verfassung widersprechen, wenn sie von einer Zweidrittelmehrheit unterstützt wurden. Die vier Ermächtigungsgesetze gehören zu dieser Entwicklung. Das Grundgesetz schreibt daher vor, dass eine Verfassungsänderung in einer expliziten Änderung des Verfassungstextes bestehen muss. Die Praxis ist jedoch abermals nicht so sehr der Verfassung anzulasten, sondern der Politik.

Allerdings: Ohne die Flexibilität der Weimarer Verfassung bzw. ihrer pragmatischen Anwendung hätte die Republik vielleicht die ersten fünf Jahre nicht überstanden. Die Weimarer Verfassung erschien so erfolgreich, dass in der ersten österreichischen Republik Teile davon (namentlich die Stellung des Präsidenten) durch die Novelle des Bundes-Verfassungsgesetzes von 1929 übernommen wurden.

Die Gründe für das Scheitern der Republik können daher nicht allein in den in der Verfassung angelegten machtstrukturellen Mängeln gesehen werden; hinzu kamen eine große Distanz vieler noch an die Monarchie und die monarchische Vaterfigur gewöhnter Bürger zur parlamentarischen Demokratie, die Uneinigkeit der Demokraten, die wirtschaftlichen Probleme der damaligen Zeit, der Zivilisationsbruch des Weltkrieges, der auch zu einer Verrohung der Menschen geführt hatte, der politische Extremismus und schließlich auch das Handeln der politischen Akteure wie Franz von Papen, Kurt von Schleicher und Reichspräsident Paul von Hindenburg.

Der Verfassungstag am 11. August war von 1921 bis 1932 Nationalfeiertag der Weimarer Republik. Reichspräsident Ebert hatte die Verfassung an seinem Urlaubsort am Esstisch unterzeichnet; eine große, feierliche Zeremonie für die Unterzeichnung wäre seinem Charakter fremd gewesen.

Später aber führte Ebert den Brauch ein, an den Jahrestag in sogenannten Verfassungsfeiern zu erinnern. Die Verfassungstage zogen viele Besucher an und wurden noch am 11. August 1932 abgehalten. Erst die Nationalsozialisten schafften den Brauch ab.

Das Deutsche Kaiserreich war bis zur Oktoberreform von 1918 eine konstitutionelle, danach kurzzeitig eine parlamentarische Monarchie. Das Staatsoberhaupt war der Kaiser, der zugleich preußischer König war. Er hatte die exekutive Gewalt inne: Er ernannte den Reichskanzler, der als einzelner Ministerrang hatte (Chef der "Reichsleitung"), war Oberbefehlshaber des Heeres und bestimmte über die Beamten (Staatssekretäre). Der Deutsche Kaiser berief den Reichstag und Bundesrat („Bundesrath“) ein. Er hatte das Recht, mit Zustimmung des Bundesrates den Reichstag aufzulösen oder anderen Staaten den Krieg zu erklären. Auch wenn der Kaiser die Gesetzgebung stark beeinflussen konnte und jeder Verantwortung gegenüber anderen Staatsorganen enthoben war, bedurften alle Reichsgesetze der ausdrücklichen Zustimmung des Bundesrates. Der Bundesrat erließ nicht nur Verwaltungsvorschriften, sondern war vollwertige Parlamentskammer. Es gab aber sonst keinen Kontrollmechanismus, der innerhalb der Grenzen seiner Befugnisse Missbrauch seitens des Kaisers verhindern oder ihn einschränken konnte. Die Weimarer Republik war eine parlamentarische Demokratie mit einem Reichspräsidenten als Staatsoberhaupt. Er ernannte Reichsleitung und -kanzler, konnte den Reichstag auflösen, verabschiedete per Notverordnung Gesetze, hatte den Oberbefehl über die Reichswehr und ernannte die Richter des Reichsgerichts.

Exekutive war die Reichsleitung. Der Kaiser ernannte die Reichsbeamten, welche, genauso wie der Reichskanzler, dem Kaiser gegenüber verpflichtet waren, und nicht dem Parlament. Das Parlament konnte die Regierung zwar kritisieren oder kontrollieren, jedoch nicht ihr Vertrauen entziehen und somit für eine neue Regierungsbildung sorgen. Der Kaiser selbst konnte das Parlament auflösen, welches somit in seiner Hand war und Gesetzesentwürfen seitens des Kaisers zustimmen musste. Das Parlament war in der Weimarer Verfassung nicht so stark vom Reichspräsidenten eingeschränkt, da es die Legislative bestimmte.

Der Reichstag wurde von Männern ab 25 Jahren auf drei und ab 1888 auf fünf Jahre gewählt. Die Wahl war gleich und geheim. Der Reichstag bildete zusammen mit dem Bundesrat die Legislative. Er machte Gesetzesentwürfe, denen der Bundesrat zustimmen musste. In der Weimarer Republik wurde der Reichstag von Männern und Frauen ab 20 Jahren in allgemeiner, unmittelbarer, gleicher und geheimer Wahl gewählt. Die Legislative war auf den Reichspräsidenten, Reichstag, Reichsrat und das Volk aufgeteilt.

Der Bundesrat setzte sich aus den Vertretern der 25 bundesstaatlichen Regierungen zusammen. Er setzte Verwaltungsvorschriften für das Reich und kontrollierte die Reichsleitung. Es gab insgesamt 58 Stimmen, wovon allein 14 für ein Veto reichten. Allein Preußen besaß 17 Stimmen. Der Reichsrat setzte sich aus den Vertretern der Landesregierungen zusammen und die Stimmenanzahl war von der Größe des jeweiligen Landes abhängig.

Zusammengefasst war das Deutsche Reich bis 1918 eine konstitutionelle Monarchie. Die Stärke des Reichstages bemaß sich, ob es den vertretenen Parteien gelang, das parlamentarische Regierungssystems durchzusetzen, wenn nicht, lag das vor allem an der Uneinigkeit der dort vertretenen Parteien. Das Fehlen der Grundrechte in der Verfassung wurde ersetzt durch die Gewährung der Grundrechte durch die Landesverfassungen oder die Rechtsprechung.

Als der Parlamentarische Rat zwischen dem 1. September 1948 und dem 23. Mai 1949 in Bonn das Grundgesetz für die Bundesrepublik Deutschland (GG) ausarbeitete, orientierte er sich an der Weimarer Verfassung. Man lernte sozusagen aus ihren Fehlern. Das Grundgesetz ähnelt der Weimarer Verfassung in vielen Punkten, enthält aber auch große Unterschiede. So spielt der Bundespräsident nicht die herausragende Rolle wie der Reichspräsident. Insgesamt wurde das Gleichgewicht der Staatsorgane anders austariert. 

Während der Weimarer Republik sah ein großer Teil der Staatsrechtslehrer die Grundrechte lediglich als Staatsziele an, obwohl die Weimarer Reichsverfassung die Grundrechte als solche bezeichnete. Nach dieser Vorstellung banden die Grundrechte nur die Verwaltung, nicht jedoch den Gesetzgeber. Dem Grundgesetz zufolge stellen die Grundrechte hingegen eindeutig unmittelbar geltendes Recht dar (Art. 1 Abs. 3 GG), das die gesamte Staatsgewalt – einschließlich Legislative – bindet.

Darüber hinaus dürfen die Grundrechte in ihrem Wesensgehalt nicht angetastet werden ( Abs. 2 GG). Der verfassungsändernde Gesetzgeber darf die Grundrechtsartikel des Grundgesetzes abändern, nur sind die in den Art. 1 und 20 GG niedergelegten Grundsätze unantastbar ( Abs. 3 GG).

In GG wird weiterhin verfügt, dass die , , , und der Weimarer Verfassung Bestandteile des Grundgesetzes sind. Sie werden auch als „Religionsartikel“ oder „inkorporierte Artikel der Weimarer Reichsverfassung“<ref name="J/P Art. 140">Hans D. Jarass, in: Jarass/Pieroth, "Grundgesetz für die Bundesrepublik Deutschland, Kommentar", 3. Auflage, München 1995, Art. 140 Rn 1.</ref> bezeichnet und bilden den Kern des geltenden Staatskirchenrechts.

In der Weimarer Reichsverfassung standen die Grundrechte nicht am Anfang des Textes, anders als im Grundgesetz für die Bundesrepublik Deutschland von 1949. Bei den sozialen Grundrechten ist das Grundgesetz allerdings zurückhaltender als die Weimarer Verfassung. Während die Weimarer Verfassung in ihrem fünften Abschnitt zum Teil detailliert soziale Rechte festschreibt, übernahm das Grundgesetz im Wesentlichen nur den Satz, dass Eigentum verpflichte ( Abs. 2 Satz 1 GG) und definiert die Bundesrepublik bewusst zurückhaltend als „sozialen Bundesstaat“ ( Abs. 1 GG).

Die Macht des Bundespräsidenten wurde vom Grundgesetz sehr stark eingeschränkt, zugunsten des Bundestags und des Bundeskanzlers. Heute hat der deutsche Bundespräsident vor allem eine repräsentative Funktion. Normalerweise bestätigt er mit seiner Unterschrift nur bereits getroffene Entscheidungen, z. B. vom Parlament beschlossene Gesetze.

Die Stellung der Regierung wurde gestärkt. Sie ist nur vom Deutschen Bundestag abhängig, und nicht wie früher, vom Reichstag und dem Reichspräsidenten. Der Bundestag kann einen Kanzler nur dadurch absetzen, dass er gleichzeitig einen neuen wählt (konstruktives Misstrauensvotum). Dieses Verfahren sorgt für mehr Stabilität, da sich in der Weimarer Zeit politische Gruppierungen zu einer Abwahl des Kanzlers vereinen konnten, ohne jedoch einen eigenen Kandidaten vorschlagen zu müssen. In der Weimarer Republik konnte man auch den Reichsministern das Vertrauen entziehen.

Verfassungsänderungen müssen – anders als in Weimarer Zeit – jetzt explizit sein. Verfassungsdurchbrechende Gesetze, die mit der notwendigen Zweidrittelmehrheit zustandekommen, ändern nicht die Verfassung, notwendig ist eine Verfassungstextänderung. Abs. 3 GG besagt ferner, dass die und sowie Artikel, die die Bundesstaatlichkeit betreffen, nicht verändert werden dürfen. Bundesländer können zwar (nach Volksabstimmungen) in ihrem Gebietsumfang oder in ihrer Zahl verändert werden, jedoch ist eine Abschaffung nicht möglich. Die im Artikel 20 GG festgeschriebene Gewaltenteilung kann nicht außer Kraft gesetzt werden. Die „Ewigkeitsklausel“ des Art. 79 Abs. 3 GG bindet die "pouvoir constitué" (verfasste Gewalt = Staatsgewalt). Ob sie auch die "pouvoir constituant" (verfassungsgebende Gewalt) bindet, ist umstritten.

Die Bundesländer sind durch den Bundesrat stärker in die Gesetzgebung eingebunden als früher durch den Reichsrat. Der Reichsrat besaß zwar ein Vetorecht, jedoch war dies eher schwach.

Den Oberbefehl über die Armee hatte der Reichspräsident, heute der Bundesverteidigungsminister, im Verteidigungsfall der Bundeskanzler. Auch dies sollte man nicht überbewerten; so hat der österreichische Bundespräsident ebenfalls den Oberbefehl, das hat für die Verfassungspraxis aber kaum Bedeutung. Was es in einer ernsten innenpolitischen Krise bedeuten könnte, ist nicht vorhersehbar.

Das Grundgesetz spricht zwar von „Wahlen und Abstimmungen“, allerdings sind Volksentscheide, außer zur Neugliederung der Länder, auf Bundesebene abgeschafft – allein auf Landesebene sind sie vollständig möglich. Diese Partizipationsmöglichkeit wurde eingeschränkt, da sie in der Weimarer Zeit von den Kommunisten, Nationalsozialisten und anderen Parteien zur Propaganda genutzt wurde und da die Alliierten nach dem Zweiten Weltkrieg der deutschen Bevölkerung misstrauten.

Auch in der Verfassung der Deutschen Demokratischen Republik von 1949 finden sich Ausdrücke und Satzteile der WRV – insbesondere im Grundrechtskatalog – wieder. Diese übernommenen Elemente sind jedoch oftmals im Sinne der SED abgeändert worden.

So heißt es über das Wirtschaftsleben in der WRV (Art. 151):

Die DDR-Verfassung (Art. 19) verzichtet auf die „wirtschaftliche Freiheit des Einzelnen“:

Besonders das Regierungssystem der DDR weicht erheblich von dem der WRV ab. Während die Bundesrepublik anstelle des Reichspräsidenten vor allem den Bundeskanzler gestärkt hat, so die DDR-Verfassung (auf dem Papier) das Parlament. Die DDR-Regierung sollte demnach aus Vertretern aller Fraktionen nach Fraktionsstärke zusammengestellt werden.

Die WRV über den Kanzler und die Richtlinien der Politik (Art. 56):

Die DDR-Verfassung (Art. 98) betont die Bedeutung der Volkskammer (des Parlamentes):

Durch das Grundgesetz wurde die WRV nicht explizit aufgehoben, soweit sie ihm nicht (wie im Staatsorganisationsrecht) entgegenstand; sie (z. B. die Grundrechte) galt somit zunächst als einfaches Gesetz weiter und wurde dann im Rahmen von Rechtsbereinigungen weitgehend aufgehoben. Übrig blieb neben den in Art. 140 GG erwähnten Religionsartikeln, die Verfassungsbestandteil sind (Fundstellennachweis-Nr. 100-2), noch der Art. 109 Abs. 3 Satz 2, „Adelsbezeichnungen gelten nur als Teil des Namens und dürfen nicht mehr verliehen werden“, der unter dem Titel „Die Verfassung des Deutschen Reiches“ geltendes (einfachgesetzliches) Bundesrecht und mit einer eigenen Fundstellennachweis-Nummer (401-2) ausgestattet ist.








</doc>
<doc id="5622" url="https://de.wikipedia.org/wiki?curid=5622" title="Wolof (Sprache)">
Wolof (Sprache)

Wolof [] ist eine Sprache der Wolof aus dem nördlichen Zweig der westatlantischen Sprachfamilie, einer Untergruppe der Niger-Kongo-Sprachen. Die am nächsten verwandten Sprachen sind Fulfulde und Serer.

Wolof wird überwiegend in Senegal gesprochen. Etwa 80 % der Senegalesen beherrschen die Sprache und machen Wolof zur faktischen Umgangssprache des Landes. Daneben wird Wolof auch noch in Gambia und Mauretanien gesprochen.

In der älteren französischsprachigen Literatur findet man auch noch die Schreibweise „Ouolof“ statt „Wolof“. In einigen englischsprachigen Publikationen, vor allem solchen, die sich auf das gambische Wolof beziehen, findet man auch die Schreibweise „Wollof“, weil diese Schreibweise bei englischen Muttersprachlern eher zur korrekten Aussprache des Wortes führt. In Publikationen des 19. Jahrhunderts und davor kann man auch den Schreibweisen "„Volof“" oder "„Olof“" begegnen. Sehr selten kommen auch noch die Schreibweisen "„Jolof“", "„Jollof“" und "„Dyolof“" vor. – Der Begriff "Wolof" wird sowohl für das Volk als auch für ihre Sprache und für Dinge und Gegebenheiten aus ihrer Kultur und Tradition benutzt.

Etwa 40 Prozent der Senegalesen (ca. 3,2 Millionen Menschen) sprechen Wolof als Muttersprache. Etwa weitere 40 Prozent der Senegalesen sprechen Wolof als Zweitsprache. In der Region von Dakar bis Saint-Louis sowie westlich und südlich von Kaolack wird Wolof vom überwiegenden Teil der Bevölkerung gesprochen. Im östlichen Senegal und im Landesteil südlich von Gambia (Casamance, Bassari-Land) sprechen nur wenige Wolof, wobei die Situation in den großen Städten wie etwa Ziguinchor wieder eine andere ist: Dort ist Wolof die meistgenutzte Umgangssprache, besonders unter jungen Leuten, ist dort aber noch stärker als im Norden von Wörtern und Floskeln durchsetzt, die aus dem Französischen stammen. Letzteres ist die offizielle Amtssprache für den gesamten Senegal; jedoch wird Wolof neben Serer, Diola, Malinke, Pulaar und Soninké als „Nationalsprache“ gewürdigt.

In Gambia sprechen etwa 15 Prozent der Bevölkerung (ca. 200.000 Menschen) Wolof. Amtssprache in Gambia ist Englisch. Die dominierenden Sprachen Gambias, Mandinka (40 %), Wolof (15 %) und Ful (15 %), werden aber ebenfalls als offizielle Sprachen akzeptiert. In Gambias Hauptstadt Banjul ist etwa jeder zweite Wolof.

In Mauretanien sprechen etwa 7 Prozent der Bevölkerung (ca. 185.000 Menschen) Wolof. Der Sprachgebrauch ist dort nur in der südlichen Küstenregion zu finden. Amtssprache in Mauretanien ist Arabisch, wichtigste Verkehrssprache Französisch.

Wolof wurde seit dem 11. Jahrhundert, seit Beginn der Islamisierung, zunächst in arabischen Buchstaben geschrieben. Zahlreiche Gedichte und religiöse Schriften sind in arabischer Schrift überliefert und sie ist auch heute noch im muslimischen Kontext in Gebrauch.

In der Regel wird Wolof heute jedoch mit den Buchstaben des lateinischen Alphabets geschrieben. Hierfür wurde vom Sprachinstitut Centre de linguistique appliquée de Dakar (CLAD) eine Standard-Orthographie entwickelt, die seit den 70er Jahren offiziell ist. Zuletzt wurde sie 2005 von staatlicher Seite erneut bestätigt. Allerdings findet man im alltäglichen Gebrauch auch immer wieder an die französische Rechtschreibung angelehnte Schreibweisen. So konnte man beispielsweise schon auf einem Werbeplakat lesen: "Thiafka bou mat seuk" „Vollendeter Geschmack“, was in offizieller Orthographie eigentlich "Cafka bu mat sëkk" hätte sein müssen.

Die Aussprache der Buchstaben entspricht im Allgemeinen denen der deutschen Lautwerte. Das "r" wird aber nicht wie im Deutschen oder Französischen als Rachen-R, sondern als Zungenspitzen-R gesprochen, ähnlich wie im Italienischen oder Spanischen. Der Buchstabe "x" wird wie die deutsche Buchstabenkombination "ch" in "Bach" ausgesprochen.

Wolof benutzt für die Vokale zusätzlich diakritische Zeichen um anzuzeigen, ob es sich um einen offenen oder geschlossenen Vokal handelt. Beispiel: "o" ist offen wie im deutschen Wort "offen", "ó" ist geschlossen wie im deutschen Wort "Ofen".

Einzelvokale werden kurz, Doppelvokale lang ausgesprochen. Beispiel: "o" ist kurz wie im deutschen "offen", "oo" ist lang wie das "a" im englischen "call". Wenn ein geschlossener Vokal lang ist, wird das diakritische Zeichen nur über den ersten Vokal gesetzt, also "óo", aber einige Quellen weichen von diesem CLAD-Standard ab und schreiben "óó".

Der sehr häufige Buchstabe "ë" wird wie das deutsche "e" in "Schatten", wie das französische "e" in "le" oder wie der englische unbetonte Artikel "a" ausgesprochen ([ə] = Schwa).

Wenngleich auch die Verben im Wolof nicht flexionslos sind, sondern ihnen Suffixe ganz unterschiedlicher Funktion angehängt werden können, spielen sich große Teile der Konjugation im Wolof eher im Bereich der Personalpronomina ab: Die verschiedenen Formen der Pronomina (s.u.) bringen jeweils unterschiedliche Satzteile in den Fokus und nuancieren dabei teilweise auch Tempus und Aspekt (vgl. hierzu vor allem den Kontrast von "naa" und "dama"). Insbesondere verbinden sie sich auch mit anderen Elementen wie dem Hilfsverb "di" „sein/werden“, welches den unvollendeten Aspekt markiert, oder der Form „a ngi“, die unmittelbares Erleben markiert, zu untrennbaren Einheiten. Die resultierenden Formen werden auch Temporal-Pronomina genannt.

Beispiel: Das Verb "dem" heißt „gehen“; das Temporal-Pronomen "maa ngi" (aus "man" „ich“ + "a ngi" „gerade im Moment (erlebbar)“) bedeutet „ich, hier und jetzt“; das Temporal-Pronomen "dinaa" (aus "di" „sein/werden“ + "naa" „ich“) bedeutet „ich werde (in Zukunft)“. Damit können nun folgende Sätze gebildet werden: "Maa ngi dem." „Ich gehe gerade/jetzt.“ – "Dinaa dem." „Ich werde (in Zukunft) gehen.“

Im Wolof spielen Zeiten wie Präsens (Gegenwart), Präteritum (Vergangenheit) und Futur (Zukunft) eine untergeordnete Rolle. Dagegen ist der Aspekt einer Handlung aus Sicht des Sprechers von entscheidender Bedeutung. Der wichtigste Aspekt ist, ob eine Handlung aus Sicht des Sprechers abgeschlossen oder noch nicht abgeschlossen ist, unabhängig davon, ob sie in der Gegenwart, Vergangenheit oder Zukunft stattfindet. Andere Aspekte sind, ob eine Handlung regelmäßig stattfindet, ob eine Handlung ganz bestimmt eintritt bzw. eingetreten ist, ob eine Handlung mehr das Subjekt, Prädikat oder Objekt eines Satzes betonen möchte, etc. Folglich wird also nicht nach Zeiten, sondern nach Aspekten konjugiert. Dennoch hat sich für die zu konjugierenden Personalpronomen der Begriff Temporalpronomen (Zeit-Fürwörter) eingebürgert; passender wäre wohl die Bezeichnung Aspektpronomen.

Beispiel: Das Verb "dem" heißt „gehen“; das Temporalpronomen "naa" bedeutet „ich bin/habe bereits“; das Temporal-Pronomen "dinaa" bedeutet „ich werde in Zukunft“; das Temporal-Pronomen "damay" (aus "dama" + "di") bedeutet häufig „ich bin/tue regelmäßig“, aber auch „ich werde gleich“. Damit können nun folgende Sätze gebildet werden: "Dem naa." „Ich bin bereits gegangen.“ – "Dinaa dem." „Ich werde (in Zukunft) gehen.“ – "Damay dem." „Ich gehe regelmäßig/üblicherweise/normalerweise.“ bzw. „ich gehe gleich“.
Möchte man unbedingt ausdrücken, dass eine Handlung in der Vergangenheit stattfand, so geschieht das nicht durch Konjugation der Temporalpronomen, sondern durch Anfügen des Suffixes "-(w)oon" an das Verb bzw. erste Hilfsverb. Beim Hilfsverb "di" ergibt dies dann die Form "doon". (Das Temporalpronomen liegt dabei, je nach betrachtetem Aspekt, bereits in konjugierter Form vor.)

Beispiele: "Demoon naa Ndakaaru." „Ich war (bereits) nach Dakar gegangen.“ "Dama doon dem Ndakaaru." „Ich ging nach Dakar.“

Im Wolof unterscheidet man Aktionsverben von Zustandsverben. Aktionsverben drücken Tätigkeiten oder Aktivitäten aus; Zustandsverben beschreiben Zustände oder Eigenschaften. Beispiel: "dem" „gehen“ ist Aktionsverb; "baax" „gut sein“ ist Zustandsverb.

Diese Unterscheidung spielt im Wolof deshalb eine dominante Rolle, weil die passende Übersetzung der Temporal-Pronomen häufig davon abhängt, ob sie mit einem Aktionsverb oder mit einem Zustandsverb benutzt werden: Bei ersterem entspricht der vollendete Aspekt eher unserem Perfekt oder Plusquamperfekt und der unvollendete unserem Präsens oder Präteritum, während bei letzterem bereits der vollendete Aspekt in der Regel mit einer Form im Präsens oder Präteritum übersetzt werden muss. Manche Temporal-Pronomen treten vorwiegend mit einer Verbart zusammen auf; beispielsweise wird das Temporal-Pronomen "maa ngi" „ich, hier und jetzt“ selten zusammen mit einem Zustandsverb verwendet.

Manche Verben können sowohl Aktionsverb als auch Zustandsverb sein, abhängig vom Kontext, in dem sie stehen. Beispiel: "toog" in der Bedeutung „sich hinsetzen“ ist Aktionsverb, in der Bedeutung „sitzen/(hin)gesetzt“ Zustandsverb.

Strenggenommen gibt es im Wolof keine Adjektive (Eigenschaftswörter): diese Rolle wird nämlich durch die Zustandsverben übernommen. Beispiel: "baax" hat als Zustandsverb die Bedeutung „gut sein“, und wird an den Stellen verwandt, wo wir das Adjektiv „gut“ gebrauchen würden. So heißt beispielsweise "baax na" „es/das ist gut“ und "nit ku baax" entspricht unserem „guter Mensch“, müsste man ganz wörtlich jedoch mit: „Mensch, der gut ist“ übersetzen.

Der Anfangskonsonant einiger Wörter richtet sich nach dem Anfangskonsonanten des vorhergehenden Wortes.

Beispiel: Der bestimmte Artikel „der/die/das (hier)“ hat im Singular folgende verschiedene Formen: "bi", "gi", "ji", "mi", "si", "wi", "li", "ki". Welche Form verwendet wird, hängt häufig vom Anfangskonsonanten des vorausgehenden Wortes ab, beispielsweise heißt es "bunt bi" „die Tür“, aber "kër gi" „das Haus/der Hof“.

Strenggenommen ist diese Formenvielfalt das Überbleibsel einer Klasseneinteilung, wie sie für viele afrikanische Sprachen (insbesondere für die Bantusprachen, zu denen Wolof jedoch nicht gehört) üblich ist; hierbei wird die Welt der Substantive in verschiedene Nominalklassen eingeteilt, z. B. in belebte Dinge, unbelebte Dinge, Gebrauchsgegenstände usw.

Auch im Wolof hat sich diese Klasseneinteilung an manchen Stellen gehalten. In diesen Fällen ist dann auch keine Konsonanten-Harmonie mehr zu erkennen, sondern die verschiedenen Artikel bewirken eine Bedeutungsveränderung. So haben Bäume beispielsweise in der Regel den Artikel "gi", deren Früchte jedoch den Artikel "bi": "mango gi" „der Mangobaum“ vs. "mango bi" „die Mango“. Und auch das Diminutiv (Verniedlichung) wird unter anderem durch Artikelveränderung gebildet: "jëkkër ji" „Ehemann“ vs. "njëkkër si" „Ehemännchen“ (i.d.R. negativ).

Grammatikalisch unterscheidet Wolof nicht zwischen maskulin (männlich), feminin (weiblich) und neutrum (sächlich). Beispiel: "bi" kann je nach Übersetzung „der“, „die“ oder „das“ bedeuten. "Mu ngi dem" kann je nach Übersetzung „er geht“, „sie geht“ oder „es geht“ bedeuten. Allerdings erfüllt die im vorherigen Abschnitt behandelte Artikelvielfalt/Klasseneinteilung zum Teil ähnliche Funktionen wie bei uns das grammatische Geschlecht.
Das Wolof-Zahlensystem ist auf der Basis der Zahlen „5“ und „10“ aufgebaut. Beispiel: "benn" „eins“, "juróom" „fünf“, "juróom-benn" „sechs“, "fukk" „zehn“, "fukk ak juróom-benn" „sechzehn“.

Das Wolof-Zahlensystem ist auf der Basis der Zahlen „5“ und „10“ aufgebaut.

Die folgenden Beispiele stammen aus "Kauderwelsch, Wolof für den Senegal – Wort für Wort" von Michael Franke.
Mit dem Lied "7 Seconds," das einen großen kommerziellen Erfolg 1994 in den Hitparaden hatte, kann sich der europäische Zuhörer ein Bild der Sprache machen. Bei dem Duett mit Neneh Cherry und Youssou N’Dour, singt N'Dour die erste Strophe auf Wolof.




</doc>
<doc id="5623" url="https://de.wikipedia.org/wiki?curid=5623" title="Wasser">
Wasser

Wasser (HO) ist eine chemische Verbindung aus den Elementen Sauerstoff (O) und Wasserstoff (H). Wasser ist als Flüssigkeit durchsichtig, weitgehend farb-, geruch- und geschmacklos. Wasser ist die einzige chemische Verbindung auf der Erde, die in der Natur als Flüssigkeit, als Festkörper und als Gas vorkommt. Die Bezeichnung "Wasser" wird dabei für den flüssigen Aggregatzustand verwendet. Im festen Zustand spricht man von Eis, im gasförmigen Zustand von Wasserdampf. Wasser ist Grundlage des Lebens auf der Erde. Natürlich kommt Wasser selten rein vor, sondern enthält meist gelöste Anteile von Salzen, Gasen und organischen Verbindungen.

Das Wort „Wasser“ leitet sich vom althochdeutschen "waȥȥar", „das Feuchte, Fließende“, ab. Die indogermanischen Bezeichnungen "*wódr̥" und "*wédōr" sind bereits in hethitischen Texten des 2. Jahrtausends v. Chr. belegt. Verwandte Wörter finden sich auch in anderen indogermanischen Sprachen, z. B.

Auch das altgriechische Wort ὕδωρ, "hydor", „Wasser“, von dem sich alle Fremdwörter mit dem Wortbestandteil "hydr(o)"- ableiten, gehört zu dieser Familie.

Andere – nach der chemischen Nomenklatur zulässige – Bezeichnungen für Wasser sind:

 mit allen chemischen und physikalischen Daten in der "Infobox", "Verwendung als Chemikalie" und "Dichteanomalie des Wassers".

Wasser besteht aus Molekülen, gebildet aus je zwei Wasserstoffatomen und einem Sauerstoff­atom.

Sauerstoff hat auf der Pauling-Skala mit 3,5 eine höhere Elektronegativität als Wasserstoff mit 2,1. Das Wassermolekül weist dadurch ausgeprägte Partialladungen auf, mit einer negativen Polarität auf der Seite des Sauerstoffs und einer positiven auf der Seite der beiden Wasserstoffatome. Es resultiert ein Dipol, dessen Dipolmoment in der Gasphase 1,84 Debye beträgt.

Tritt Wasser als Ligand in einer Komplex-Bindung auf, so ist Wasser ein "einzähniger" Ligand.

Geometrisch ist das Wassermolekül gewinkelt, wobei die beiden Wasserstoffatome und die beiden Elektronenpaare in die Ecken eines gedachten Tetraeders gerichtet sind. Der Winkel, den die beiden O-H-Bindungen einschließen, beträgt 104,45°. Er weicht aufgrund des erhöhten Platzbedarfs der freien Elektronenpaare vom idealen Tetraederwinkel (~109,47°) ab. Die Bindungslänge der O-H-Bindungen beträgt jeweils 95,84 pm.

Weil Wassermoleküle Dipole sind, besitzen sie ausgeprägte zwischenmolekulare Anziehungskräfte und können sich durch Wasserstoffbrückenbindung zu Clustern zusammenlagern. Dabei handelt es sich nicht um beständige, feste Verkettungen. Der Verbund über Wasserstoffbrückenbindungen besteht nur für Bruchteile von Sekunden, wonach sich die einzelnen Moleküle wieder aus dem Verbund lösen und sich in einem ebenso kurzen Zeitraum erneut – mit anderen Wassermolekülen – verketten. Dieser Vorgang wiederholt sich ständig und führt letztendlich zur Ausbildung von variablen Clustern. Diese Vorgänge bewirken die besonderen Eigenschaften des Wassers:

Wasser hat

Je nach Isotopenzusammensetzung des Wassermoleküls unterscheidet man normales „leichtes Wasser“ (zwei Atome Wasserstoff: HO), „Halbschweres Wasser“ (ein Atom Wasserstoff und ein Atom Deuterium: HDO), „schweres Wasser“ (zwei Atome Deuterium: DO) und „überschweres Wasser“ (zwei Atome Tritium: TO), wobei mit HTO und DTO noch weitere Moleküle mit gemischten Isotopen vorkommen.

Wasser kann unter Hochspannung eine Wasserbrücke zwischen zwei Glasgefäßen ausbilden.

Wasser als chemische Verbindung wurde zum ersten Mal synthetisiert, als Henry Cavendish im 18. Jahrhundert ein Gemisch aus Wasserstoff und Luft zur Explosion brachte (siehe Knallgas-Reaktion).

Wasserstoff gilt als Energieträger der Zukunft.
Wasserstoff ist, wie auch elektrische Energie, keine Primärenergie, sondern muss, analog zur Stromerzeugung, aus Primärenergie hergestellt werden.
Zur Demonstration wird Wasser im Hofmannschen Wasserzersetzungsapparat in seine Bestandteile zerlegt. Reaktionsschema:

Nachweisreaktion: Wasser färbt weißes, kristallwasser­freies Kupfersulfat hellblau, und blaues Cobalt(II)-chlorid­papier wird durch Wasser rot gefärbt.

In der Analytik wird Wasser in Kleinmengen (Feuchte bzw. Trockenheit) überwiegend quantifiziert mittels Karl-Fischer-Titration (nach Karl Fischer). Monographien in Pharmakopoen zum quantitativen Nachweis von Wasser beruhen überwiegend auf der Karl-Fischer-Titration.

Wärmeeinwirkung verursacht eine schnellere Bewegung der Wassermoleküle. Werden an der Stelle der Wärmeeinwirkung 100 °C erreicht, geht es dort (je nach Keim mit mehr oder weniger Siedeverzug) vom flüssigen in den gasförmigen Aggregatzustand (Dampf) über, dessen Volumen um etwa das 1600-fache höher ist (siehe Wasserdampf) und der infolge seiner im Verhältnis zum umgebenden Wasser geringeren Dichte als mehr oder weniger große Blasen aufsteigt: Das Wasser beginnt zu sieden, wobei die Dampfblasen von Schichten noch nicht so heißen Wassers abgekühlt werden und wieder zu flüssigem Wasser kondensieren. Erreicht schließlich die gesamte Wassermenge die Temperatur von 100 °C, so gelangen die nun großen Dampfblasen bis an die Oberfläche: Das Wasser kocht.

Druck und Temperatur sind die bestimmenden Faktoren für die Löslichkeit von Gasen im Wasser. Gasbläschen, die bereits bei geringfügiger Erwärmung sichtbar werden, bestehen nicht aus Wasserdampf, sondern aus gelösten Gasen. Ursache ist die geringere Wasserlöslichkeit von Gasen bei Erwärmung. Wasser, das sich eine Zeit lang in einer unter Druck stehenden Leitung oder Flasche befunden hat, hat oft einen Überschuss an Gasen gelöst. Daher reicht schon das Wegnehmen des äußeren Drucks, dass sich – bevorzugt an Keimen an der Wandung – Gasblasen ausscheiden und bis zu einer Größe von 1-2 mm auch haften bleiben.

Die Geschichte der menschlichen Nutzung des Wassers und somit jene der Hydrologie, der Wasserwirtschaft und besonders des Wasserbaus, ist durch eine vergleichsweise geringe Zahl von Grundmotiven geprägt. Von den ersten sesshaft werdenden Menschen zu den Hochkulturen der Antike über das Mittelalter bis zur Neuzeit stand im Zentrum immer ein Konflikt zwischen einem Zuviel und einem Zuwenig an Wasser. Ihm war man dabei fast immer ausgeliefert, ob durch Dürren die Ernte einging oder Hochwasser Leben und Besitz bedrohte. Es wurde auch zum Gegenstand der Mythologie und der Naturphilosophie. Noch heute kommt dem Wasser in den meisten Religionen der Welt eine Sonderstellung zu, besonders dort, wo die Frage des Überlebens von der Lösung der zahlreichen Wasserprobleme abhängt.

Ziel war es, allen Nutzungsansprüchen gerecht zu werden und dabei jedem Menschen den ihm zustehenden Teil des Wassers zu garantieren. Hierbei diente das Wasserrecht als eine der ersten Rechtsformen zur Mitbegründung der ersten zentralistischen Zivilisationen Mesopotamiens und Ägyptens sowie jener, die in den Flusstälern Chinas und Indiens entstanden.

Die lange Geschichte der Wassernutzung zeigt sich dabei, wie die Menschheitsgeschichte insgesamt, nicht als ein kontinuierlicher Entwicklungspfad. Sie wurde vor allem durch einzelne Zentren hohen wasserwirtschaftlichen Standards sowie durch immer wiederkehrende Brüche geprägt, neben oft jahrhundertelang währenden Stagnationsphasen. So beeindruckend die frühen wasserbaulichen Anlagen dabei auch waren, wie groß sich Innovationskraft und Kreativität unserer Vorfahren auch zeigten, letztlich war und ist man auch heute noch abhängig von der Natur, die man jedoch erst in vergleichsweise jüngster Zeit anfing wirklich zu verstehen.

Aufgrund der großen Bedeutung des Wassers wurde es nicht zufällig bei den frühesten Philosophen zu den vier Urelementen gezählt. Thales von Milet sah im Wasser sogar den Urstoff allen Seins. Wasser ist in der von Empedokles eingeführten und dann vor allem von Aristoteles vertretenen Vier-Elemente-Lehre neben Feuer, Luft und Erde ein Element.

Wasser ist in der taoistischen Fünf-Elemente-Lehre (neben "Holz, Feuer, Erde, Metall") vertreten. Die Bezeichnung "Elemente" ist hier jedoch etwas irreführend, da es sich um verschiedene Wandlungsphasen eines zyklischen Prozesses handelt. Wasser hat verschiedene Orientierungen was zu unterschiedlichen (symbolischen) Strukturen führt.

Im antiken Griechenland wurde dem Element Wasser das Ikosaeder als einer der fünf Platonischen Körper zugeordnet.

Wasser ist in den Mythologien und Religionen der meisten Kulturen von zentraler Bedeutung. Mit den Vorsokratikern begann vor etwa 2500 Jahren das abendländische Denken als eine Philosophie des Wassers. In vielen Religionen des Altertums wurden Gewässer allgemein und vor allem Quellen, als Heiligtum verehrt. Die ungeborenen Kinder wähnte man in Quellen, Brunnen oder Teichen verborgen, aus denen sie die Kindfrauen (Hebammen) holten (Kinderglauben).

Wasser ist der Inbegriff des Lebens. In den Religionen hat es einen hohen Stellenwert. Oft wird die reinigende Kraft des Wassers beschworen, zum Beispiel im Islam in Form der rituellen Gebetswaschung vor dem Betreten einer Moschee, oder im Hindu-Glauben beim rituellen Bad im Ganges.

So gut wie jede Gemeinde im Judentum besitzt eine Mikwe, ein Ritualbad mit fließendem reinen Wasser, das oft aus einem tief reichenden Grundwasserbrunnen stammt, wenn Quellwasser nicht zur Verfügung steht. Nur wer vollständig untertaucht, wird rituell gereinigt. Notwendig ist dies für zum Judentum Bekehrte, für Frauen nach der Menstruation oder einer Geburt, und bei orthodoxen Juden vor dem Sabbat und anderen Feiertagen.
Im Christentum wird die Taufe teils durch Untertauchen oder Übergießen mit Wasser als Ganzkörpertaufe vollzogen, in der westlichen Kirche heute meist durch Übergießen mit Wasser. In der katholischen Kirche, den orthodoxen Kirchen und der anglikanischen Kirche spielt die Segnung mit Weihwasser eine besondere Rolle.

In der Esoterik spielt das Wasser eine Rolle, Kraftorte werden oft an Quellen oder Flüssen gesucht.

In vielen Sagen und Märchen spielt Wasser eine Rolle, zum Beispiel als Wasser des Lebens. Die Bedeutung des Wassers findet sich im geflügelten Wort Kein Wässerchen trüben können.

Der menschliche Körper besteht zu über 70 % aus Wasser. Ein Mangel an Wasser führt daher beim Menschen zu gravierenden gesundheitlichen Problemen (Dehydratation, Exsikkose), da die Funktionen des Körpers, die auf das Wasser angewiesen sind, eingeschränkt werden. Zitat der Deutschen Gesellschaft für Ernährung (DGE): "Geschieht dies "(die Wasserzufuhr)" nicht ausreichend, kann es zu Schwindelgefühl, Durchblutungsstörungen, Erbrechen und Muskelkrämpfen kommen, da bei einem Wasserverlust die Versorgung der Muskelzellen mit Sauerstoff und Nährstoffen eingeschränkt ist."

Wie hoch der tägliche Mindestbedarf liegt ist unklar. Empfehlungen von 1,5 Litern und mehr pro Tag für einen gesunden, erwachsenen Menschen können wissenschaftlich nicht gestützt werden. Bei einem durchschnittlichen Tageskonsum von 2 Litern werden in 80 Jahren über 55.000 Liter Wasser getrunken. Der Wasserbedarf kann bei erhöhter Temperatur größer sein.

Das Trinken exzessiver Mengen an Wasser mit mehr als 20 L/Tag kann ebenfalls zu gesundheitlichen Schäden führen. Es kann eine „Wasservergiftung“ eintreten bzw. genauer zu einem Mangel an Salzen, d. h. zu einer Hyponatriämie mit permanenten neurologischen Schäden oder Tod führen.

In der Medizin wird Wasser (in Form von isotonischen Lösungen) vor allem bei Infusionen und bei Injektionen verwendet. Bei der Inhalation wird aerosolisiertes Wasser zur Heilung, etwa von Husten, benutzt.
Wasser, äußerlich angewendet, hat auf die Gesundheit und die Hygiene sehr günstige Einflüsse. "Siehe auch": Baden, Balneologie, Kneippen, Sauna, Schwimmen, Waschen. Die antiken Römer pflegten aus diesen Gründen eine „Wasserkultur“ im Thermalbad.

Wasser ist eine Grundvoraussetzung für das Leben: ohne Regen keine Trinkwasserversorgung, keine Landwirtschaft, keine Gewässer mit Fischen zum Verzehr, keine Flüsse zum Gütertransport, keine Industrie. Letztere benötigt für alle Produktionsvorgänge viel Wasser, was geklärt in den Kreislauf zurückgeführt wird. Wasser wird wegen seiner hohen Verdampfungswärme in Form von Wasserdampf zum Antrieb von Dampfmaschinen und Dampfturbinen sowie zur Beheizung von chemischen Produktionsanlagen benutzt. Wegen seiner hohen Wärmekapazität und Verdampfungswärme dient Wasser als umlaufendes bzw. verdampfendes Kühlmittel; in Deutschland dienten 1991 allein in Kraftwerken 29 Milliarden m als Kühlwasser. Wasser kann auch als Kältemittel (R-718) in Kältemaschinen eingesetzt werden. Im Salzbergbau wird Wasser als Lösemittel zum Auslaugen, zum Transport, als Sole und zum Reinigen eingesetzt.

Die Wasserversorgung nutzt unterschiedliche Wasservorkommen als Trinkwasser, zum Teil aber auch für Betriebswasserzwecke: Niederschlags­wasser, Oberflächenwasser in Flüssen, Seen, Talsperren, Grundwasser, Mineralwasser und Quellwasser. Die Nutzung der Gewässer wird in Deutschland im Wasserhaushaltsgesetz geregelt. In Mitteleuropa gibt es eine zuverlässige, weitgehend kostendeckende und hochwertige Trinkwasserversorgung. Diese wird meist durch öffentliche Anbieter (kommunale Versorger) gewährleistet, die die ökologische Verantwortung übernehmen und es als Leitungswasser zur Verfügung stellen. Der weltweite Wassermarkt hat ein Wachstum wie kaum eine andere Branche. Deshalb haben private Anbieter großes Interesse, Wasser als Handelsware zu definieren, um diesen Markt zu übernehmen.

Wo normales Trinkwasser keine direkte Handelsware ist, wird mit dem Begriff virtuellem Wasser auf den indirekten Wasserexport verwiesen: "Grünes Wasser" – also meist Bewässerungs­wasser – wird aus Ländern der Dritten Welt in Form der Agrarprodukte zu uns exportiert. Es ist das Wasser, das bei der Aufzucht von Pflanzen und Tieren eingesetzt wurde. Für den Anbau von Bananen etwa sind 1.000 l Wasser für jeden Quadratmeter Boden notwendig. Produktionssteigerungen führen zu einem Verbrauch von Wasser, das damit nicht mehr als Trinkwasser zur Versorgung der örtlichen Bevölkerung zur Verfügung steht.

Als Wasserverbrauch wird die Menge des vom Menschen in Anspruch genommenen Wassers bezeichnet. Der umgangssprachliche Begriff ist – wie „Energieverbrauch“ – nicht korrekt, da nirgends Wasser „vernichtet“ wird: seine Gesamtmenge auf der Erde bleibt konstant; „Wasserbedarf“ wäre treffender. Dieser umfasst den unmittelbaren menschlichen Genuss (Trinkwasser und Kochen) ebenso wie den zum alltäglichen Leben (Waschen, Toilettenspülung etc.) sowie den für die Landwirtschaft, das Gewerbe und die Industrie (siehe Nutzwasser) gegebenen Bedarf. Das ist daher nicht nur eine Kenngröße für die nachgefragte Wassermenge, sondern zumeist auch für die Entsorgung oder Wiederaufbereitung des bei den meisten Wassernutzungen entstehenden Abwassers (Kanalisation, Kläranlage). Die aus der Versorgungsleitung entnommene Wassermenge wird durch einen Wasserzähler gemessen und zur Kostenberechnung herangezogen.

Weltweit liegt der Süßwasserbedarf bei jährlich geschätzt 4.370 km³ (2015), wobei die Grenze der nachhaltigen Nutzung bei 4.000 km³ angegeben wird ("siehe auch" Welterschöpfungstag). Ein dabei bislang unterschätzter Faktor ist die Verdunstung genutzten oder zur Nutzung vorgehaltenen Wassers bspw. durch Pflanzen („Evapotranspiration“), die nach neuer Daten-Analyse mit ca. 20 % des Gesamtverbrauchs angenommen wird.

In Deutschland betrug 1991 der Wasserbedarf 47,9 Milliarden Kubikmeter, wovon allein 29 Milliarden Kubikmeter als Kühlwasser in Kraftwerken dienten. Rund elf Milliarden Kubikmeter wurden direkt von der Industrie genutzt, 1,6 Milliarden Kubikmeter von der Landwirtschaft. Nur 6,5 Milliarden Kubikmeter dienten der Trinkwasserversorgung. Der durchschnittliche Wasserbedarf (ohne Industrie) beträgt rund 130 Liter pro Einwohner und Tag, davon etwa 1–2 Liter in Speisen und Getränken einschließlich des in Fertiggetränken enthaltenen Wassers.

Die Versorgung der Menschheit mit sauberem Wasser stellt Menschen nicht nur in den Entwicklungsländern vor ein großes logistisches Problem. Nur 0,3 % der weltweiten Wasservorräte sind als Trinkwasser verfügbar, das sind 3,6 Millionen Kubikkilometer von insgesamt ca. 1,38 Milliarden Kubikkilometern.

Die Wasserknappheit kann sich in niederschlagsarmen Ländern zu einer Wasserkrise entwickeln. Zur Linderung einer Wasserknappheit sind insbesondere angepasste Technologien geeignet. Es wurden aber auch schon ausgefallen erscheinende Ideen erwogen. So wurde vorgeschlagen, Eisberge über das Meer in tropische Regionen zu schleppen, die unterwegs nur wenig abschmelzen würden, um am Ziel Trinkwasser daraus zu gewinnen.

"Siehe auch:" Wasserverteilungssystem, Wasseraufbereitung, Wasseraufbereitungsanlage, Wasserwirtschaft, Siedlungswasserwirtschaft in Deutschland, Wasserreinhaltung

Weltweit haben etwa 4 Mrd. Menschen bzw. zwei Drittel der Weltbevölkerung mindestens einen Monat im Jahr nicht ausreichend Wasser zu Verfügung. 1,8 bis 2,9 Mrd. Menschen leiden 4 bis 6 Monate im Jahr unter schwerer Wasserknappheit, ca. 0,5 Mrd. Menschen ganzjährig.

Auf Antrag Boliviens erklärte die UN-Vollversammlung am 28. Juli 2010 mit den Stimmen von 122 Ländern und ohne Gegenstimme den Zugang zu sauberem Trinkwasser und zu sanitärer Grundversorgung zu Menschenrechten. 41 Länder enthielten sich der Stimme, darunter USA, Kanada und 18 EU-Staaten. Da Resolutionen der UN-Vollversammlung völkerrechtlich unverbindlich sind, ergeben sich zunächst keine rechtlichen Konsequenzen. Jedoch könnte die neue Resolution nun die Auffassung stützen, dass sauberes Wasser und Sanitäranlagen zu einem „angemessenen“ Lebensstandard gehören und somit aufgrund des völkerrechtlich bindenden Internationalen Paktes über wirtschaftliche, soziale und kulturelle Rechte, der das Recht auf einen angemessenen Lebensstandard enthält, eingeklagt werden. Einige Länder wie Südafrika oder Ecuador haben das Recht auf Wasser in ihre Verfassung übernommen.

Die wasserrechtlichen Grundlagen der Wasserwirtschaft und des öffentlichen Umganges mit den Wasserressourcen bilden in Deutschland das Wasserhaushaltsgesetz und die Europäische Wasserrahmenrichtlinie. Wichtige Behörden und Institutionen sind:

Wasser spielt eine zentrale Rolle in vielen Wissenschaften und Anwendungsgebieten. Die Wissenschaft, die sich mit der räumlichen wie zeitlichen Verteilung des Wassers und dessen Eigenschaften beschäftigt, bezeichnet man als Hydrologie. Insbesondere untersucht die Ozeanologie das Wasser der Weltmeere, die Limnologie das Wasser der Binnengewässer, die Hydrogeologie das Grundwasser und die Aquifere, die Meteorologie den Wasserdampf der Atmosphäre und die Glaziologie das gefrorene Wasser unseres Planeten. In flüssiger Form wurde Wasser bislang nur auf der Erde nachgewiesen. Bereiche der Umweltökonomie befassen sich mit Wasser als Ressource (Water Economics).

Die Wasserchemie befasst sich mit den Eigenschaften des Wassers, seinen Inhaltsstoffen und mit den Umwandlungen, die im Wasser stattfinden oder durch das Wasser verursacht werden, sowie mit dem Stoffhaushalt der Gewässer. Sie behandelt Reaktionen und Auswirkungen im Zusammenhang mit der Herkunft und Beschaffenheit der unterschiedlichen Wassertypen. Sie beschäftigt sich mit allen Bereichen des Wasserkreislaufs und berücksichtigt damit die Atmosphäre und den Boden. Dabei beschäftigt sie sich unter anderem mit der Analyse von im Wasser gelösten Stoffen, den Eigenschaften des Wassers, dessen Nutzung, dessen Verhaltensweise in verschiedenen Zusammenhängen.
Wasser ist ein Lösungsmittel für viele Stoffe, für Ionenverbindungen, aber auch für hydrophile Gase und hydrophile organische Verbindungen. Sogar gemeinhin als in Wasser unlöslich geltende Verbindungen sind in Spuren im Wasser enthalten. Daher liegt Wasser auf der Erde nirgends in reinem Zustand vor. Es hat je nach Herkunft die unterschiedlichsten Stoffe in mehr oder weniger großen Konzentrationen in sich gelöst.
In der Wasseranalytik unterscheidet man unter anderem folgende Wassertypen:

Aber auch bei den wässrigen Auslaugungen (Eluaten) von Sedimenten, Schlämmen, Feststoffen, Abfällen und Böden wird die Wasseranalytik eingesetzt.

Um die Eigenschaften des Wassers und eventuell darin gelöster Stoffe, bzw. damit in Kontakt stehender fester Phasen aufzuklären, kann auch die Molekulardynamik-Simulation sinnvoll sein.

"Siehe auch:" Wasserhärte, Hydrophobie, Hydrophilie

In den Geowissenschaften haben sich Wissenschaften herausgebildet, die sich besonders mit dem Wasser beschäftigen: die Hydrogeologie, die Hydrologie, die Glaziologie, die Limnologie, die Meteorologie und die Ozeanographie.
Besonders interessant für die Geowissenschaften ist, wie Wasser das Landschaftsbild verändert (von kleinen Veränderungen über einen großen Zeitraum bis hin zu Katastrophen, bei denen Wasser innerhalb weniger Stunden ganze Landstriche zerstört), dies geschieht zum Beispiel auf folgende Weisen:

Wasser ist nicht nur ein bedeutender Faktor für die mechanische und chemische Erosion von Gesteinen, sondern auch für die klastische und chemische Sedimentation von Gesteinen. Dadurch entstehen unter anderem Grundwasserleiter.

Auch interessiert Geowissenschaftler die Vorhersage des Wetters und besonders von Regenereignissen (Meteorologie).

"Siehe auch:" Gewässer, Permafrostboden, Binnenmeer, Binnensee, Teich, Meer, Ozean, Bach, Flussaue.

Die verschiedenen strömungstechnischen Eigenschaften und Wellentypen auf mikroskopischer und makroskopischer Ebene werden intensiv untersucht, wobei folgende Fragestellungen im Mittelpunkt stehen:


Der größte Teil der Erdoberfläche (71 %) ist von Wasser bedeckt, besonders die Südhalbkugel und als Extrem die Wasserhalbkugel.
Die Wasservorkommen der Erde belaufen sich auf circa 1,4 Milliarden Kubikkilometer (entspricht dem Volumen eines Würfels mit 1120 km Kantenlänge), wovon der allergrößte Teil auf das Salzwasser der Weltmeere entfällt. Nur 48 Millionen Kubikkilometer (3,5 %) des irdischen Wassers liegen als Süßwasser vor. Das mit 24,4 Millionen Kubikkilometern (1,77 %) meiste Süßwasser ist dabei als Eis an den Polen, Gletschern und Dauerfrostböden gebunden und somit zumindest für prompte Nutzung nicht verfügbar. Einen weiteren wichtigen Anteil macht das Grundwasser mit 23,4 Millionen Kubikkilometern aus. Das Wasser der Fließgewässer und Binnenseen (190.000 km³), der Atmosphäre (13.000 km³), des Bodens (16.500 km³) und der Lebewesen (1.100 km³) ist im Vergleich rein mengenmäßig recht unbedeutend. Dabei ist jedoch nur ein geringer Teil des Süßwassers auch als Trinkwasser verfügbar. Insgesamt liegen 98,233 % des Wassers in flüssiger, 1,766 % in fester und 0,001 % in gasförmiger Form vor. In seinen unterschiedlichen Formen weist das Wasser dabei spezifische Verweilzeiten auf und zirkuliert fortwährend im globalen Wasserkreislauf. Diese Anteile sind jedoch nur näherungsweise bestimmbar und wandelten sich auch stark im Laufe der Klimageschichte, wobei im Zuge der globalen Erwärmung von einem Anstieg des Wasserdampfanteils ausgegangen wird.

Tiefenwasser in schon deutlich wärmeren geologischen Schichten wird direkt oder über Wärmetausch als Wärme-Energiequelle genutzt, wobei sowohl natürliche Thermalquellen und Geysire an der Oberfläche vorliegen als auch der Mensch danach bohrt. Durch den Gebirgsdruck bleibt Wasser in der Tiefe auch bei Temperaturen über dem Siedepunkt bei Normaldruck von 100 °C flüssig. Neue Erkenntnisse lassen vermuten, dass auch in etwa 500 km Tiefe, im Zwischenbereich von oberem und unterem Erdmantel Wasser in flüssiger Form vorliegt.

Die bislang noch fehlende bzw. unzureichende Versorgung eines großen Teils der Weltbevölkerung mit hygienischem und toxikologisch unbedenklichem Trinkwasser, sowie mit einer ausreichenden Menge Nutzwasser, stellt eine der größten Herausforderungen der Menschheit in den nächsten Jahrzehnten dar. Seit 1990 haben rund 2,6 Milliarden weitere Menschen Zugang zu einer sicheren Wasserversorgung erhalten, zum Beispiel mithilfe von Pumpbrunnen oder einem Leitungssystem. Aber immer noch trinken 663 Millionen Menschen jeden Tag Wasser, das verschmutzt ist und krankmachen kann.

Die Herkunft des Wassers auf der Erde, insbesondere die Frage, warum auf der Erde deutlich mehr Wasser vorkommt als auf den anderen inneren Planeten, ist bis heute nicht befriedigend geklärt. Ein Teil des Wassers gelangte zweifellos durch das Ausgasen von Magma in die Atmosphäre, stammt also letztlich aus dem Erdinneren. Ob dadurch aber die Menge an Wasser erklärt werden kann, wird stark angezweifelt. Das Element Wasserstoff ist das häufigste Element im Universum, und auch Sauerstoff kommt in großen Mengen vor, allerdings normalerweise gebunden in Silikaten und Metalloxiden; beispielsweise ist der Mars mit großen Anteilen an Eisen(III)-oxid bedeckt, was ihm seine rote Farbe verleiht. Wasser hingegen ist dort – im Vergleich zur Erde – nur in geringen Mengen zu finden.

Außerhalb der Erde kommt ebenfalls Wasser vor. Beispielsweise wurde Wassereis in Kometen, auf dem Mars, einigen Monden der äußeren Planeten und dem Exoplaneten OGLE-2005-BLG-390Lb nachgewiesen. Allein die Saturnringe enthalten überschlägig etwa 20 bis 30 Mal so viel Wasser, wie auf der Erde vorkommt. Hinweise auf das Vorhandensein von Wassereis in polnahen Meteoritenkratern gibt es beim Erdmond und sogar bei Merkur, dem sonnennächsten Planeten. Als Flüssigwasser wird es unter den eisigen Oberflächen von Europa, Enceladus, ein paar weiteren Monden sowie bei OGLE-2005-BLG-390Lb vermutet. Direkt fotografisch belegt wurde außerirdisches Flüssigwasser bisher aber nur wenige salzwasserhaltige Schlammtröpfchen auf dem Mars. Außerirdischer Wasserdampf konnte unter anderem in den Atmosphäre von Mars und Titan, den höheren Atmosphärenschichten roter Riesensterne, in interstellaren Nebeln und sogar im Licht ferner Quasare nachgewiesen werden.

Wasser beeinflusst entscheidend unser Klima und ist Basis nahezu aller Wetter­erscheinungen, vor allem bedingt durch seine hohe Mobilität und Wärmekapazität. In den Ozeanen wird die einstrahlende Sonnenenergie gespeichert. Diese regional unterschiedliche Erwärmung führt wegen Verdunstung zu unterschiedlichen Konzentrationen der gelösten Stoffe, da diese nicht mitverdunsten (vor allem Salinität (Salzgehalt)). Dieses Konzentrationsgefälle erzeugt globale Meeresströmungen, die sehr große Energiemengen (Wärme) transportieren (z. B. Golfstrom, Humboldtstrom, äquatorialer Strom, mitsamt ihren Gegenströmungen). Ohne den Golfstrom würde in Mitteleuropa arktisches Klima herrschen.

Im Zusammenhang mit dem Treibhauseffekt stellen Ozeane die wirksamste CO-Senke dar, da Gase wie Kohlendioxid in Wasser gelöst werden (siehe Kohlenstoffzyklus). Die mit der globalen Erwärmung einhergehende Temperaturerhöhung der Weltmeere führt zu einem geringeren Haltevermögen an Gasen und damit zu einem Anstieg des CO in der Atmosphäre.
Wasserdampf stellt in der Atmosphäre ein wirksames Treibhausgas dar. (siehe Treibhauseffekt)

Bei der Erwärmung verdunstet Wasser, es entsteht Verdunstungskälte. Als „trockener“ Dampf (nicht kondensierend) und als „nasser“ Dampf (kondensierend: Wolken, Nebel) enthält und transportiert es latente Wärme, die für sämtliche Wetterphänomene entscheidend verantwortlich ist ("siehe auch" Luftfeuchtigkeit, Gewitter, Föhn). Die Wärmekapazität des Wassers und die Phänomene der Verdunstungskälte und latenten Wärme sorgen in der Nähe von großen Gewässern für gemäßigte Klimate mit geringen Temperaturschwankungen im Jahres- und Tagesgang. Wolken verringern zudem die Einstrahlung durch die Sonne und die Erwärmung der Erdoberfläche durch Reflexion.

Der aus Wolken fallende Niederschlag und der Wasserdampf (Auskämmung und Photosynthese bzw. Atmung) bewässern die terrestrischen Ökotope. Auf den Landmassen können so Gewässer oder Eismassen entstehen, die auch meso- und mikroklimatische Wirkungen haben. Das Verhältnis von Evapotranspiration (Gesamtverdunstung eines Gebietes) zu Niederschlag entscheidet, ob sich trockene ("aride", Steppen, Wüsten) oder feuchte ("humide", Wälder, Waldsteppen) Klimate bilden. Auf den Landmassen ist außerdem der Wasserhaushalt der Vegetation eine klimatische Größe.

Wasser ist vermutlich der Entstehungsort des Lebens und eine seiner Bedingungen. In Organismen und in unbelebten Bestandteilen der Ökosphäre spielt es als vorherrschendes Medium bei fast allen Stoffwechsel­vorgängen beziehungsweise geologischen und ökologischen Elementarprozessen eine entscheidende Rolle. Die Erdoberfläche ist zu circa 72 % von Wasser bedeckt, wobei Ozeane hieran den größten Anteil tragen. Süßwasser­reserven bilden lediglich 2,53 % des irdischen Wassers und nur 0,3 % sind als Trinkwasser zu erschließen (Dyck 1995). Durch die Rolle des Wassers in Bezug auf Wetter und Klima, als Landschafts­gestalter im Zuge der Erosion und durch seine wirtschaftliche Bedeutung, unter anderem in den Bereichen der Land-, Forst- und Energiewirtschaft, ist es zudem in vielfältiger Weise mit Geschichte, Wirtschaft und Kultur der menschlichen Zivilisation verbunden. Die Bedeutung des Wassers für das Leben war insofern auch immer Gegenstand der Naturphilosophie.

Das Leben ist nach dem heutigen Erkenntnisstand im Wasser entstanden ("siehe auch" Evolution). Autotrophe Schwefelbakterien (Prokaryoten) produzieren aus Schwefelwasserstoff und Kohlendioxid unter Zufuhr von Lichtenergie organische Kohlenstoffverbindungen und Wasser:

Als Nachfolger nutzten Blaubakterien (Cyanobakterien) und alle späteren autotrophen Eukaryoten das hohe Redoxpotential des Wassers: Unter Zufuhr von Licht produzieren sie aus Wasser und Kohlendioxid Traubenzucker und Sauerstoff:

Durch diesen Prozess reicherte sich im Wasser und in der Atmosphäre immer mehr Sauerstoff an. Damit wurde die Gewinnung von Energie durch Zellatmung (Dissimilation) möglich:

Voraussetzung für die Fähigkeit, mit dem giftigen Sauerstoff (Oxidation der empfindlichen Biomoleküle) umzugehen, waren Enzyme wie die Katalase, die eine strukturelle Ähnlichkeit mit dem Sauerstoff transportierenden Hämoglobin aufweist. Aerobe Purpurbakterien nutzten vielleicht als erstes den giftigen Sauerstoff zum energieliefernden Abbau von organischen Stoffen. Nach der Endosymbiontentheorie nahmen damals noch anaerobe Eukaryoten die aeroben Prokaryoten (wahrscheinlich Purpurbakterien) auf.

Wasser wurde damit zum Medium grundlegender biochemischer Vorgänge (Stoffwechsel) zur Energiegewinnung und -speicherung:

Auf Grund des Dipolmomentes eignet sich Wasser als Lösungsmittel für polare Substanzen und wegen der daraus entspringenden Viskosität und Dichte als Transportmittel. Wasser transportiert Nährstoffe, Abbauprodukte, Botenstoffe und Wärme innerhalb von Organismen (zum Beispiel Blut, Lymphe, Xylem) und Zellen. Die Eigenschaften des Wassers werden bei Pflanzen und Tieren (inklusive Mensch) mannigfaltig, z. B. für die Temperaturregulierung benutzt, in Form von Guttation, Schwitzen, etc., oder z. B. als Basis für antibakterielle Schutzfilme bei Kröten und Fischen.

Pflanzen und Tieren ohne Skelett verleiht der Turgordruck des Wassers Form und Festigkeit. Durch Turgoränderungen können sie sich auch bewegen (zum Beispiel Blattbewegung bei Pflanzen).

Die Stachelhäuter, zu denen die Seeigel, Seesterne und Seewalzen gehören, haben statt eines festen Skeletts ein System hydraulisch arbeitender Gefäße (Ambulacralsystem). Sie bewegen sich durch gezielte Druckänderungen in diesem Gefäßsystem.

Wassergehalt in einigen Nahrungsmitteln:

In terrestrischen Ökosystemen ist Wasser begrenzender Faktor der Produktivität. Es ist essenziell für den Stoffwechsel von Lebewesen (Biosphäre) sowie für die Herausbildung und Prägung ihrer Standorte (Pedosphäre, Erdatmosphäre/Klima). Niederschläge speisen Gewässer und Grundwasser als Ressource für das Pflanzenwachstum und als Trinkwasser für die Tiere.

Die meiste Biomasse und größte Produktivität findet sich in aquatischen Ökosystemen, vor allem in Ozeanen, in denen der begrenzende Produktionsfaktor die Menge der im Wasser gelösten Nährstoffe ist, also vor allem Phosphat, Stickstoffverbindungen (Ammonium, Nitrat) und CO (Kohlendioxid). Die Eigenschaften des Wassers werden mit hoher Effizienz genutzt, z. B. bei der Oberflächenspannung von Insekten, Spinnentieren, bei der Dichte und den optischen Eigenschaften vom Plankton etc.

Die Temperaturabhängigkeit der Wasserdichte führt in Gewässern zu einer Temperaturschichtung, zu Sprungschichten und Ausgleichsströmungen, die vor allem in limnischen (Süßwasser-) Biotopen charakteristisch sind ("siehe" Ökosystem See), aber auch in marinen Ökosystemen anzutreffen sind und genutzt werden (Wale nutzen z. B. die Schallreflexionen an Sprungschichten zur Verbesserung ihrer Kommunikation). Die Dichteanomalie des Wassers ermöglicht auch das Überleben von Lebewesen im Winter, da stehende Gewässer dadurch nicht bis zum Grund durchfrieren (Ausnahme flache Gewässer und „Frosttrocknis“). Zusätzlich bewirkt die Dichteanomalie in tieferen Seen der gemäßigten Zonen im Frühling und Herbst bei Erreichen einer einheitlichen Temperatur eine "Umwälzung" des Wassers und somit einen Austausch von Oberflächen- und Tiefenwasser, der für Nährstoff- und Sauerstoffkreislauf wesentlich ist.

Auch wenn aquatische Ökosysteme durch die Wärmekapazität des Wassers sehr stabile Lebensräume darstellen, haben auch geringere Temperaturschwankungen deutliche Folgen (vgl. Ökosystem See). So wird die Temperaturerhöhung der Ozeane Veränderungen in marinen Ökosystemen zur Folge haben.

Der ökologische Zustand von Fließ- bzw. Oberflächengewässern (wie von Grundwasser) wird in der Europäischen Union (EU) nach der "Richtlinie 2000/60/EG" (EU-Wasserrahmenrichtlinie, WRRL) nach verschiedenen Kriterien analysiert und nach fünf Graden eingeteilt: „sehr gut“, „gut“, „mässig“, „unbefriedigend“, „schlecht“.

Wasser hat in der Technik verschiedene Anwendungsmöglichkeiten. Zu den Wichtigsten zählen die Wärmeübertragung für Heizung oder Kühlung. Die Erzeugung von Kälte durch Verdunstung, etwa in Kühltürmen. Den Betrieb von Kältemaschinen auf Basis Adsorption von Ammoniak oder Wasserdampf in Lithiumbromid-Lösung.

Wasser dient kalt und warm zum reinigenden Waschen (eventuell mit Detergentien oder Laugen), Lösen (Auslaugen von Salzlagerstätten), Trennen über Chromatographie oder Extraktion (Aufgussgetränke), Umkristallisieren (Abbinden von Gips, Zement, (zusammen mit Kohlenstoffdioxid:) Kalk). Als Druckstrahl zum Spülen, Brausen, Hochdruckreinigen eventuell mit abrasivem Zusatzstoff, und zum Wasserstrahlschneiden auch etwa im hygienesensiblen Bereich der Lebensmittelindustrie.

In Form von Gel als Schallübertragungsmedium vom Sensorkopf zum menschlichen Körper bei der Ultraschalldiagnostik. Als Medium mit hoher Oberflächenspannung und guter Verdunstungsrate zum verschieblichen Anklatschen von Beschriftungsfolie auf Schaufenstern, Autokarosserien und anderen glatten zu kaschierenden Oberflächen. Auch als Gleit- und Dichtmittel für Saugnäpfe.

Ursprüngliche Hydraulik verwendet Wasser als Druckübertragungsmedium, als Fontänen in Springbrunnen und Wasserspielen, die auch Verdunstungskühlung und Lichteffekte ermöglichen. Das Aufbrechen von geologischen Schichten beim Fracking ist ebenfalls eine Hochdruckanwendung.

Die Gewichtskraft von Wassermassen wird einerseits in den verschiedenen Mühlen und Wasserkraftwerken zur Gewinnung von mechanischer oder elektrischer genutzt. Beim durch Wasserschlag pumpenden Wasserwidder kommt zusätzlich die geringe Kompressibilität von Wasser dazu. Ballasttanks helfen unbeladene oder ungleichmäßig beladene Schiffe zu stabilisieren, U-Booten auf- und abzutauchen. Es gibt Seilbahnen und Lifte, die noch immer im Gegenzug von Wasserballasttanks gezogen bzw. gehoben werden. Durch Wasser erzeugter Auftrieb erlaubt Schiffen, Bojen und Lebewesen das Schwimmen, siehe auch Dock.

Wasser als Dissoziationsmedium dient für Elektrolyse, Galvanik, Akku- und Batterietechnik, in alten Kraftwerken als Strom-Regelungstank. Weiters als Lösungsmittel aller wässrigen Chemie, ob beim Mikroverfahren der Tüpfelplatte, dem grafisch wirksamen Entwickeln fotografischer Platten und Filme oder der großtechnischen Herstellung von Nitramoncal aus Ammoniak und Salpetersäure.

In der Medizin dient Wasser als lösendes Medium zum Injizieren oder Infundieren von Stoffen in den Körper, um den Wasserhaushalt des Körpers zu korrigieren, zum Aufweichen harter Haut oder von Nägeln oder zum Spülen des Darms. Das Kopfhaar mit Wasser reversibel aufzuquellen und in Wellen und Locken zu formen ist Friseurhandwerk.

Weidenruten, Peddigrohr etc. werden in Wasser gelegt zum Flechten biegsam gemacht. Hartholz unter Wasserdampf zu Bugholzmöbeln geformt. Erbsen quellen durch Wasser auf, um Schädelknochen durch Sprengen zu präparieren.

Wasser kann Infrarotstrahlung aus Glühlampenlicht ausfiltern und absorbiert radioaktive Strahlung im Abklingbecken von Kernkraftwerken.

In Wasserwerfern wird Wasser, mit und ohne chemischem Zusatz, als Munition eingesetzt.









</doc>
<doc id="5624" url="https://de.wikipedia.org/wiki?curid=5624" title="Wavelet-Transformation">
Wavelet-Transformation

Mit Wavelet-Transformation (WT, ) wird eine bestimmte Familie von linearen Zeit-Frequenz-Transformationen in der Mathematik und den Ingenieurwissenschaften (primär: Nachrichtentechnik, Informatik) bezeichnet. Die WT setzt sich zusammen aus der Wavelet-Analyse, welche den Übergang der Zeitdarstellung in die Spektral- bzw. Waveletdarstellung bezeichnet, und der Wavelet-Synthese, welche die Rücktransformation der Wavelettransformierten in die Zeitdarstellung bezeichnet.

Der Begriff Wavelet bezeichnet die für die Transformation benutzte Basisfunktion, mit der das zu analysierende Signal oder Bild – im Allgemeinen eine N-dimensionale Funktion – „verglichen“ wird.

Die Wurzeln der Waveletschule liegen in Frankreich, wo auch der ursprünglich französische Begriff "ondelette" geprägt wurde, dessen englisches Pendant "wavelet" sich jedoch später als Bezeichnung durchgesetzt hat. Ins Deutsche übersetzt bedeutet Wavelet so viel wie "kleine Welle" oder "Wellchen" und drückt den Umstand aus, dass man im Gegensatz zur Fourier-Transformation zeitlich lokalisierte Wellen bzw. Funktionen als Basis benutzt, wodurch die eingangs erwähnte Zeit- und Frequenzauflösung möglich wird. Wie alle linearen Zeit-Frequenz-Transformationen unterliegt auch die Wavelettransformierte der Unschärferelation der Nachrichtentechnik, d. h. ein Ereignis kann nicht gleichzeitig beliebig genau in Zeit und Frequenz lokalisiert werden. Es gibt immer nur einen Kompromiss aus guter zeitlicher Auflösung oder guter Auflösung im Frequenzbereich.

Die Wavelet-Transformation unterteilt sich in erster Linie in zwei Lager, nämlich die kontinuierliche Wavelet-Transformation, welche ihre Hauptanwendung in der Mathematik und der Datenanalyse hat, und die diskrete Wavelet-Transformation, welche eher in den Ingenieurswissenschaften zu finden ist und deren Anwendung im Bereich der Datenreduktion, Datenkompression und Signalverarbeitung liegt (siehe ).

Die Wavelet-Transformation kann als Verbesserung der Kurzzeit-Fourier-Transformation (STFT) angesehen werden.

Bei der STFT wird eine Fensterfunktion auf das zu untersuchende Signal angewendet – etwa die Gaußsche Glockenkurve wie bei der Gabor-Transformation. Für jeden Punkt der STFT wird das Fenster an den zu betrachtenden Zeitpunkt und an die zu betrachtende Frequenz (Modulation im Zeitbereich) verschoben. Die absolute Zeitdauer und Bandbreite des Fensters („Breite“ im Zeit- und Frequenzbereich) – und damit die Auflösung – ändern sich dadurch nicht.

Die Auflösungen im Zeit- und Frequenzbereich sind nur abhängig von der Form des Fensters. Auf Grund der Zeit-Frequenz-Unschärfe ist die Auflösung im Zeitbereich umgekehrt proportional zur Auflösung im Frequenzbereich. Es lässt sich also nicht gleichzeitig im Zeitbereich und im Frequenzbereich die bestmögliche Auflösung erzielen. Enthält nun ein Signal Frequenzanteile sowohl bei hohen als auch bei niedrigen Frequenzen, möchte man bei niedrigen Frequenzen eine gute (absolute) Frequenzauflösung erzielen, da eine kleine absolute Frequenzänderung hier stark ins Gewicht fällt. Bei einer hohen Frequenz ist eine gute Zeitauflösung wichtiger, da eine vollständige Schwingung hier weniger Zeit beansprucht und sich die Momentanfrequenz daher schneller ändern kann.

Für ein Signal mit Frequenzanteilen bei 1 Hz und 1 kHz, für welches die Frequenz auf 10 Prozent genau aufgelöst werden soll, ist bei 1 Hz eine Frequenzauflösung von 0,1 Hz nötig. Bei 1 kHz entspricht dieses einer Auflösung von 0,01 Prozent – eine so gute Auflösung ist hier nicht nötig. Andererseits vollführt das Signal bei 1 kHz zehn vollständige Schwingungen in 10 ms. Um Frequenzänderungen in diesem Zeitraum auflösen zu können, ist eine Zeitauflösung besser als 10 ms nötig. Bei 1 Hz entspricht diese Zeitdauer nur einer hundertstel Schwingung. Eine so gute zeitliche Auflösung ist also hier nicht nötig. Gewünscht ist bei niedrigen Frequenzen also eine gute Frequenzauflösung unter Inkaufnahme einer schlechten Zeitauflösung und bei hohen Frequenzen eine gute Zeitauflösung bei schlechterer Frequenzauflösung. Die Short-Time-Fourier-Transformation leistet dieses nicht.

Wie bei der STFT wird eine Fensterfunktion auf das zu untersuchende Signal angewendet. Anstatt allerdings das Fenster zu verschieben und zu modulieren (Verschiebung im Frequenzbereich) (wie bei der STFT), wird das Fenster verschoben und skaliert. Durch die Skalierung ergibt sich wie durch die Modulation ebenfalls eine Frequenzverschiebung, allerdings wird gleichzeitig mit einer Frequenzerhöhung die Zeitdauer („Breite“ im Zeitbereich) des Fensters verringert. Dadurch ergibt sich bei höheren Frequenzen eine bessere zeitliche Auflösung. Bei niedrigen Frequenzen wird die Frequenzauflösung besser, dafür wird die Zeitauflösung schlechter.

Die kontinuierliche Wavelet-Transformation (CWT, engl. "continuous wavelet transform") ist gegeben durch
Dabei ist

Mit der aus dem Mother-Wavelet formula_3 abgeleiteten Wavelet-Familie
lässt sich die kontinuierliche Wavelet-Transformation kompakt als Skalarprodukt
schreiben.

Ein Wavelet formula_3 ist eine quadratintegrierbare Funktion, welche relativ frei wählbar ist. Im Allgemeinen stellt man eine weitere technische Voraussetzung an ein Wavelet, die "Zulässigkeitsbedingung":
Dabei bezeichnet formula_12 die Fourier-Transformierte von formula_3. Die Zulässigkeitsbedingung wird für den Beweis einiger zentraler Sätze und Eigenschaften benötigt, weshalb sie häufig in die Definition eines Wavelets mit eingeschlossen wird.

Eine unmittelbare Folgerung der Zulässigkeit ist, dass die Fouriertransformierte des Wavelets an der Stelle 0 verschwindet:
Des Weiteren folgt daraus, dass das erste Moment des Wavelets, also sein Mittelwert, verschwindet:

Die ursprüngliche Funktion x(t) kann bis auf eine additive Konstante wieder aus der Wavelettransformierten
zurückgewonnen werden mit der Rekonstruktionsformel 
mit

Als Reproduzierender Kern (engl. "reproducing kernel") wird die Wavelettransformierte des Wavelets selbst bezeichnet. Somit bezeichnet
den Kern des Wavelets formula_19.

Das Attribut "reproduzierend" trägt der Kern, weil sich die Wavelettransformierte unter der Faltung mit dem Kern reproduziert, das heißt, die Wavelettransformierte ist invariant unter der Faltung mit dem Kern. Diese Faltung ist
gegeben durch
Dies ist keine gewöhnliche Faltung, da sie nicht kommutativ ist; sie ist jedoch assoziativ.

Eine weitere wichtige Bedeutung erhält der reproduzierende Kern daher, dass er die minimale Korrelation zwischen zwei Punkten (a,b) und (a',b') im Waveletraum angibt. Dies lässt sich zeigen, indem man die Autokorrelation von weißem Rauschen im Waveletraum betrachtet. Bezeichnen wir mit formula_21 ein Gauss’sches weißes Rauschen mit Varianz 1, so ist dessen Autokorrelation gegeben durch formula_22. Die Korrelation im Waveletraum ist dann (ohne Ausführung der Rechnung)
also gerade gegeben durch den reproduzierenden Kern.



Die Wavelet-Paket-Transformation ist eine Ausweitung der Schnellen Wavelet-Transformation (FWT), indem nicht nur der Tiefpasskanal, sondern auch der Bandpasskanal weiter mittels der Wavelet-Filterbank aufgespalten werden. Dieses kann dazu dienen, aus einer üblichen 2-Kanal-DWT wie z. B. den Daubechies-Wavelets eine M-Kanal-DWT zu erhalten, wobei M eine Potenz von 2 ist; der Exponent wird Tiefe des Paket-Baums genannt. Dieses Verfahren wird in der Breitbanddatenübertragung als Alternative zur schnellen Fourier-Transformation angewandt.

Wird in einem Rekursionsschritt der FWT ein weißes Rauschen als Eingangssignal transformiert, so ist das Ergebnis aufgrund der orthogonalen Natur der DWT wieder ein weißes Rauschen, wobei die Energie (=Quadratsumme der Samples) gleichmäßig auf Tief- und Bandpasskanal verteilt wird. Nimmt man eine möglichst hohe Abweichung von diesem Verhalten, d. h. eine möglichst vollständige Konzentration der Signalenergie auf einen der beiden Kanäle, als Entscheidungskriterium, ob der Eingangskanal aufgespalten werden soll, und setzt man dieses Verfahren für die aufgespaltenen Kanäle fort, so entsteht eine Variante eines Beste-Basis-Verfahrens.






</doc>
<doc id="5626" url="https://de.wikipedia.org/wiki?curid=5626" title="Wavelet-Kompression">
Wavelet-Kompression

Die Wavelet-Kompression ist eine Form der Datenkompression speziell für Bildkompression (teilweise auch Videokompression). 

Als erstes wird eine 2D-Wavelet-Transformation durchgeführt.
Dadurch erhält man genau so viele Koeffizienten, wie das Bild Pixel enthält.
Diese Koeffizienten sind leichter zu komprimieren, da sich der Großteil der wichtigen Informationen auf einige wenige Koeffizienten konzentriert.
Dieses Prinzip heißt auch Transformationskodierung.
Anschließend werden die Koeffizienten quantisiert und die quantisierten Werte entropiekodiert und/oder lauflängenkodiert.

Videokompression
Bildkompression



</doc>
<doc id="5629" url="https://de.wikipedia.org/wiki?curid=5629" title="WHO (Begriffsklärung)">
WHO (Begriffsklärung)

WHO steht als Abkürzung für:

Who steht für:
who steht für:
Siehe auch:


</doc>
<doc id="5630" url="https://de.wikipedia.org/wiki?curid=5630" title="Weltgesundheitsorganisation">
Weltgesundheitsorganisation

Die Weltgesundheitsorganisation (, WHO) ist die Koordinationsbehörde der Vereinten Nationen für das internationale öffentliche Gesundheitswesen.

Es handelt sich dabei um eine Sonderorganisation der Vereinten Nationen mit Sitz in Genf. Die WHO wurde am 7. April 1948 gegründet und zählt heute 194 Mitgliedsstaaten. Sie wird seit Juli 2017 von dem äthiopischen WHO-Generaldirektor Tedros Adhanom Ghebreyesus geleitet.

Die Verfassung der Weltgesundheitsorganisation konstatiert, dass ihr Ziel die Verwirklichung des bestmöglichen Gesundheitsniveaus bei allen Menschen ist. Ihre Hauptaufgabe ist die Bekämpfung der Erkrankungen, mit besonderem Schwerpunkt auf Infektionskrankheiten, sowie Förderung der allgemeinen Gesundheit unter Menschen auf der Welt.

Für ihre Erfolge erhielt die WHO 2009 den Prinzessin-von-Asturien-Preis in der Kategorie "Internationale Zusammenarbeit".

Die Idee einer Weltgesundheitsorganisation wurde 1945 in San Francisco im Rahmen der Gründungskonferenz der Vereinten Nationen formuliert. Am 22. Juli 1946 wurde die Verfassung der Weltgesundheitsorganisation in New York verabschiedet und von 61 Staaten unterzeichnet. Sie trat als „Specialized Agency“ der Vereinten Nationen am 7. April 1948 nach der Ratifikation des 26. Unterzeichnerstaates in Kraft.

Die Mitgliedschaft in der WHO steht allen Staaten offen. Mit Ausnahme des Fürstentums Liechtenstein sind alle Mitgliedstaaten der Vereinten Nationen auch Mitglieder der WHO. Hinzu kommen zwei Nicht-Mitgliedstaaten, Niue und die Cookinseln. Die Länder, die keine Mitgliedstaaten der Vereinten Nationen sind, können zur WHO als assoziierte Mitglieder beitreten. Sie werden ausführlich informiert und haben ein beschränktes Teilnahme- und Abstimmungsrecht. Zu den assoziierten Mitgliedern gehören Puerto Rico und Tokelau.

Die Republik China (Taiwan), die 1971 aus den Vereinten Nationen ausgeschlossen wurde, stellte ein Gesuch um Beitritt zur WHO im Beobachterstatus. Die Weltgesundheitsorganisation lehnte dies 2004 wegen der Ein-China-Politik ab.

Die Geschäfte der WHO werden durch deren Hauptorgane, die Weltgesundheitsversammlung (World Health Assembly) und den Exekutivrat (Executive Board) wahrgenommen.

Die sechs Regionalbüros der WHO haben ihren Sitz in folgenden Städten:

Jedes Regionalbüro ist von einem Regionaldirektor geleitet, der vom Regionalausschuss für den Zeitraum von fünf Jahren gewählt wird. Der Name des Kandidaten zum Posten des Regionaldirektors wird dem Exekutivrat vermittelt, der die Ernennung bestätigt.

Die WHO ist bestrebt, ihre Präsenz in den Mitgliedstaaten zu verstärken. Etwa 200 Kooperationszentren und Forschungseinrichtungen unterstützen durch ihre Tätigkeiten die laufenden Programme der WHO.

Im Zweijahresbudget für die Jahre 2008–2009 betrugen die Einnahmen 3,759 Milliarden US-Dollar und die Ausgaben 3,941 Milliarden US-Dollar.

Die ordentlichen Beiträge der WHO-Mitgliedstaaten beliefen sich auf 940 Millionen US-Dollar. Sie werden nach einem Schlüssel bemessen, wobei sich die Höhe des Beitrags nach der Zahlungsfähigkeit des jeweiligen Landes richtet. Die freiwilligen Beiträge in der Höhe von 2,745 Milliarden US-Dollar wurden zu 52 % von den WHO-Mitgliedstaaten, vor allem den USA, Großbritannien, Kanada, Norwegen und der Niederlande entrichtet. Der Rest der freiwilligen Beiträge stammte hauptsächlich von Stiftungen (21 %), von internationalen Organisationen (17 %) sowie zu je 5 % von NGOs und dem privaten Sektor. Die restlichen Einnahmen stammten aus Dienstleistungen der WHO oder aus der Nachzahlung ausstehender Beiträge.

Die Ausgaben des WHO-Hauptbüros (insbesondere zur Finanzierung der laufenden Programme) beliefen sich auf 1,412 Milliarden US-Dollar. Von den 6 Regionalbüros hatte dasjenige in Afrika mit 1,007 Milliarden US-Dollar die höchsten Ausgaben.

WHO-Projekte werden teilweise als öffentlich-private Partnerschaft finanziert. Darunter fallen:

Nicht mehr im Programmbudget der WHO figuriert der Globale Fonds zu Bekämpfung von AIDS, Malaria und Tuberkulose (GFATM), der 2002 von der G8 gegründet wurde. Die WHO hat 2005 ihre Zusammenarbeit mit dem GFATM in einer Handlungsempfehlung umschrieben. Ebenfalls außerhalb der WHO führen UNAIDS, UNITAID oder das IARC eine eigene Rechnungslegung.
Die Verfassung der WHO statuiert, dass ihr Zweck darin liegt, allen Völkern zur Erreichung des bestmöglichen Gesundheitszustandes zu verhelfen. Zur Verwirklichung dieses Zweckes dient die WHO-Strategie „Gesundheit für alle im 21. Jahrhundert“, die 1998 von der Weltgesundheitsversammlung verabschiedet wurde und die auf der 1978 verabschiedeten „Alma-Ata-Deklaration“ beruht. Es soll ein Grad an Gesundheit erreicht werden, der es allen Menschen ermöglicht, ein sozial und wirtschaftlich produktives Leben zu führen. Gesundheit wird als ein wesentlicher Bestandteil der menschlichen Entwicklung wahrgenommen.

Die Gesundheit wird in der Verfassung der WHO definiert als „ein Zustand von vollständigem physischen, geistigen und sozialen Wohlbefinden, der sich nicht nur durch die Abwesenheit von Krankheit oder Behinderung auszeichnet“. Dieser Gesundheitsbegriff wurde durch das Konzept der Gesundheitsförderung in der Ottawa-Charta von der WHO 1986 weiterentwickelt. Darin wird postuliert, dass zur Erreichung dieses Zustandes sowohl Einzelne als auch Gruppen ihre Bedürfnisse befriedigen, ihre Wünsche und Hoffnungen wahrnehmen und verwirklichen sowie ihre Umwelt meistern bzw. verändern können. In diesem Sinne wird Gesundheit als Zustand des vollständigen körperlichen, geistigen und sozialen Wohlbefindens definiert und als ein wesentlicher Bestandteil des alltäglichen Lebens verstanden – und nicht als vorrangiges Lebensziel.


Eine zentrale Aufgabe ist es, Leitlinien, Standards und Methoden in gesundheitsbezogenen Bereichen zu entwickeln, zu vereinheitlichen und weltweit durchzusetzen. Die Verfassung der WHO sieht dafür drei Instrumente vor: Völkerrechtliche Verträge, Regelungen unmittelbar gestützt auf die WHO-Verfassung und nicht-verbindliche Empfehlungen.

Zusätzlich werden der WHO durch völkerrechtliche Verträge Regelungsaufgaben übertragen, beispielsweise durch das Einheitsabkommen über die Betäubungsmittel von 1961 und die Konvention über psychotrope Substanzen von 1971. Diese werden häufig durch die dafür von der WHO eingesetzte Expertenkommission erfüllt.

Die größten Erfolge hat die WHO bei der Bekämpfung von Infektionskrankheiten erzielt. Dank weltweiter Impfprogramme kann jährlich der Tod oder die Behinderung von mehreren Millionen Menschen verhindert werden.

Nachdem jahrelang die Pocken bekämpft worden waren, erklärte die Weltgesundheitsversammlung der WHO im Mai 1980 auf Empfehlung einer Expertenkommission vom Dezember 1979 die Krankheit für ausgerottet.

Die Entwicklung von Impfstoffen gegen Malaria und Schistosomiasis nähert sich einem Erfolg und die Ausrottung der Kinderlähmung (Polio) in den nächsten Jahren wird angestrebt.

Am 28. Mai 1959 wurde auf der 8. Weltgesundheitsversammlung zwischen der Internationalen Atomenergieorganisation (IAEO) und der WHO die Resolution (WHA12-40) verabschiedet. Der Vertrag legt u. a fest, dass die Verantwortung für Untersuchungen, Entwicklungen und Anwendungen auf dem Gebiet der Kernenergie primär bei der IAEO liegt und die WHO bei entsprechenden Aktivitäten die IAEO zu konsultieren habe und diese einvernehmlich zu regeln seien. Diese Abhängigkeit der WHO von der IAEO, die laut Satzung den friedlichen Einsatz der Kernenergie weltweit fördern soll, wird vielfach kritisiert, da dadurch z. B. die Zahl der weltweiten Opfer der Katastrophe von Tschernobyl von der WHO und der IAEO gemeinsam als viel zu niedrig beziffert würden. Von verschiedenen Seiten, u. a. von der Europaabgeordneten Rebecca Harms, wird deshalb die Auflösung der Resolution WHA12-40 gefordert.

Die WHO stand und steht wegen ihres Verhaltens bei der Pandemiebekämpfung in der Kritik. So wurden nach dem Auftreten des H5N1-Virus (sogenannte Vogelgrippe H5N1) im Mai 2005 – aufgrund der Warnung des damaligen Impfdirektors Klaus Stöhr vor einer möglichen weltweiten Grippeepidemie („bis zu 7 Millionen Tote“) – von Regierungen für Millionen die Grippemittel Tamiflu und Relenza angeschafft. Zwar verbreitete sich das Virus weltweit, jedoch kam es nur selten zu Erkrankungen beim Menschen, sodass weltweit nur 152 Menschen an der „Vogelgrippe H5N1“ verstarben, weit weniger als bei einer saisonalen Grippe. 2007 wechselte Klaus Stöhr von der WHO zum Pharmakonzern Novartis.

Nach dem Auftreten des A/H1N1-Virus (sogenannte Schweinegrippe) erhöhte die WHO mit der Verbreitung der Krankheit die Epidemiewarnstufe schrittweise bis zur höchsten Stufe 6 (Pandemie). Die Regierungen bestellen daraufhin Impfstoffe (alleine in Deutschland für ca. 450 Mio. Euro) und Grippemittel. Kritik löste dabei vor allem aus, dass die derzeitige Direktorin der WHO-Impfstoffabteilung – Frau Marie-Paule Kieny – vor ihrer Tätigkeit bei der WHO beim französischen Pharmaunternehmen Transgene S.A. beschäftigt war, der strategische Partnerschaften zur Impfstoffherstellung mit dem Schweizer Pharmakonzern Roche unterhält. Der Europarat ging dem Verdacht nach, dass es ein enges Zusammenspiel zwischen WHO und Pharmaindustrie gab.

Ein Problem sehen Kritiker in der Finanzierung der WHO. 2014 berichtete Frontal21, dass vom Jahresbudget der WHO von etwa 4 Mrd. USD allein etwa 3 Mrd. USD freiwillige Beiträge sind, darunter auch größere Spenden von Unternehmen, insbesondere aus der Pharmabranche. Laut dem Bericht kritisiert Transparency International die viel zu geringen Pflichtbeiträge der Staaten an die WHO. Dadurch sei ab 2001 die WHO in die Arme der Industrie getrieben worden. Ähnliche Kritik kommt laut dem Bericht von Medico international, welche meint, dass die WHO unterfinanziert sei, um auf eine Krise wie Ebola angemessen reagieren zu können. Die WHO sei mehr und mehr auf Gelder aus der Wirtschaft angewiesen, wodurch die Neutralität der WHO gefährdet sei. Medico international fordert, die privaten Interessen in der WHO zurückzudrängen, die WHO anständig zu finanzieren und zu demokratisieren. Nach dem Bericht von Frontal21 kritisiert der Brite Paul Flynn, der 2010 die Untersuchung im Europarat gegen die WHO geleitet hatte, die WHO wie folgt: „Meiner Meinung nach ist sie (die WHO) auch heute noch exzessiv beeinflusst von der Pharmaindustrie, die sehr geschickt bei der Manipulation von Gesundheitsausgaben vorgeht, zugunsten eigener finanzieller Interessen.“

Die WHO beziehungsweise die WHA hat im Laufe der Zeit zahlreiche Welttage mittels Resolutionen beschlossen oder bereits von anderen internationalen Organisationen initiierte Welttage unterstützt. Weitere Aktionstage mit Bezug zu Gesundheitsthemen finden sich in der Liste der Gedenktage und Aktionstage.




</doc>
<doc id="5631" url="https://de.wikipedia.org/wiki?curid=5631" title="Wertemenge">
Wertemenge

Wertemenge oder Wertebereich steht für:


</doc>

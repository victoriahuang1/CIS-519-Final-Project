<doc id="12641" url="https://de.wikipedia.org/wiki?curid=12641" title="Ebolafieber">
Ebolafieber

Ebolafieber, nach ICD-10 Ebola-Viruskrankheit, ist eine Infektionskrankheit, die durch Viren der Gattung "Ebolavirus" hervorgerufen wird. Die Bezeichnung geht auf den Fluss Ebola in der Demokratischen Republik Kongo zurück, in dessen Nähe diese Viren 1976 den ersten allgemein bekannten großen Ausbruch verursacht hatten. Von 2014 bis 2016 gab es in Westafrika die bislang größte Ebolafieber-Epidemie. Die WHO erklärte die Länder Liberia (Januar 2016), Sierra Leone und Guinea (Ende 2015) für ebolafrei.

Das Ebolafieber verläuft je nach Virusart in etwa 25 bis 90 Prozent aller Fälle tödlich. Als Therapie stehen bislang lediglich Maßnahmen zur Bekämpfung oder Linderung einzelner Krankheitssymptome zur Verfügung. In Deutschland, Österreich, der Schweiz und in vielen anderen Ländern besteht eine Meldepflicht bei Verdacht auf Ebolafieber, bei direktem oder indirektem Erregernachweis, bei Ausbruch der Erkrankung, beim hämorrhagischen Krankheitsverlauf oder bei Tod durch die Ebola-Virus-Krankheit. Für die Erkrankung von Affen besteht in Deutschland Anzeigepflicht nach dem Tiergesundheitsgesetz (TierGesG).

Es werden fünf Arten in der Gattung "Ebolavirus" unterschieden: Zaire-Ebolavirus (EBOV), Sudan-Ebolavirus (SUDV), Reston-Ebolavirus (RESTV), Taï-Forest-Ebolavirus (TAFV, früher Elfenbeinküste- bzw. Côte-d’Ivoire-Ebolavirus) und Bundibugyo-Ebolavirus (BDBV). Außer dem Reston-Ebolavirus lösen alle anderen vier Arten beim Menschen hohes Fieber über 38,5 °C in Verbindung mit Blutungen (hämorrhagisches Fieber) aus. Die Sterblichkeit (Letalität) der Erkrankten durch Ebolafieber liegt bei 50 bis 90 % im Falle von EBOV, bei 41 bis 65 % im Falle von SUDV sowie bei 25 und 36 % bei den beiden bekannten Ausbrüchen des BDBV. Im Falle des TAFV und RESTV ist kein Toter unter den wenigen nachgewiesenen Erkrankungsfällen. Die Weltgesundheitsorganisation (WHO) gibt eine "case fatality rate" (Letalitätsrate) von rund 50 % an, mit einer Spannweite von 25 bis 90 % bei den vergangenen Ausbrüchen. Das Robert Koch-Institut nennt eine Letalität je nach Virusspezies von 30 bis 90 %. Aufgrund der hohen Letalität und Infektionsgefahr wird der Erreger in die höchste Risikogruppe 4 nach der Biostoffverordnung eingeordnet (lediglich die Virusspezies RESTV ist der Risikogruppe 2 zugeordnet).

Das natürliche Reservoir des Virus – der Reservoirwirt – ist bislang unbekannt. Es gibt jedoch Hinweise darauf, dass als Hauptwirt Nilflughunde in Frage kommen. In jüngster Zeit konnten Forscher aus dem Centre International de recherches Médicales de Franceville in Gabun entweder Virusbruchstücke oder Virusantikörper in sechs selbst nicht erkrankten Flughundearten nachweisen. Es handelte sich dabei um die Flughundearten "Epomops franqueti", Hammerkopf ("Hypsignathus monstrosus"), Schmalkragen-Flughund ("Myonycteris torquata"), "Micropteropus pusillus" und Nilflughund ("Rousettus aegyptiacus") sowie die Fledermausart Angola-Bulldoggfledermaus "Mops condylurus" aus einer Gegend, in der zuvor Schimpansen und Gorillas an Ebolafieber verendet waren. In Teilen West- und Zentralafrikas werden diese Tiere als „Buschfleisch“ verzehrt. Es wird hinsichtlich dieses rohen Fleisches daher vom ungeschützten Kontakt und Verzehr abgeraten.

Der Subtyp "Reston" löst in Makaken (eine Affen-Gattung) die Krankheit aus. Eine Krankheitsauslösung beim Menschen wurde jedoch bislang nicht festgestellt.

Der Erreger kann von erkrankten Menschen (durch Körperflüssigkeiten), von Tieren (einschließlich des regional üblichen „Buschfleischs“) und von kontaminierten Gegenständen auf den Menschen übertragen werden. Grundsätzlich lassen sich diese Infektionswege primär durch Desinfektion von Gegenständen und Verzicht auf „Buschfleisch“, sekundär durch Isolierung der Erkrankten und Schutzkleidung für pflegende Angehörige und medizinisches Personal zuverlässig ausschalten. Nicht in allen Gegenden sind allerdings hierfür die nötigen Voraussetzungen erfüllt.

Im Rahmen der Ebolafieber-Epidemie 2014 in Westafrika wurde auch die Möglichkeit in Betracht gezogen, dass eine infizierte Person beispielsweise nach Europa einreist und erst dort erkrankt. Das Robert Koch-Institut (RKI) hat wichtige Faktoren genannt, die bei der Pathogenese von Bedeutung sind. Das dazu verwendete Flussschema soll dem medizinischen Personal als Hilfestellung dienen, einen begründeten Verdacht auf einen importierten Ebolafieber-Fall schnell zu erkennen. Ähnliche Zusammenfassungen wurden ebenso von den Centers for Disease Control and Prevention (CDC) angewendet.

Demnach handelt es sich um einen begründeten Verdachtsfall, wenn
"und" in den 21 Tagen vor Erkrankungsbeginn
"oder"
"oder"

Hinsichtlich der Inkubationszeit wurden verschiedene Angaben veröffentlicht. Im Allgemeinen wird sie mit 2 bis 21 Tagen angegeben, am häufigsten beträgt sie 8–10 Tage. In 3 % aller Fälle sind es 22–42 Tage, über 2 % macht die WHO keine Aussagen.

Bei vielen Patienten lassen sich zwei klinische Phasen der Erkrankung unterscheiden, zwischen denen die Symptome über 24 bis 48 Stunden abklingen (Remission). In der ersten Phase treten Symptome wie bei einer beginnenden Grippe auf, die zweite Phase ist durch hämorrhagisches Fieber gekennzeichnet.

Nach Ablauf der Inkubationszeit treten zunächst grippeähnliche Symptome auf, mit Fieber, Schüttelfrost, Kopf- und Muskelschmerzen, hinzu kommen Übelkeit und Erbrechen oder auch Durchfall. Weitere Symptome, die während der Epidemie in Mosango (Demokratische Republik Kongo) 1995 dokumentiert wurden, sind Kraftlosigkeit, Schmerz im Unterleib, Appetitlosigkeit, Gelenkschmerz, Rötung der Schleimhäute in der Mundhöhle, Schluckstörung und Bindehautentzündung. Darüber hinaus wurden bei der Epidemie 2014 im Kenema-Staatskrankenhaus (Sierra Leone) noch Schwindelanfälle, Halsschmerzen und Ödeme als Symptome dokumentiert. Ein Hautausschlag entwickelt sich in rund einem Viertel bis der Hälfte der Patienten. Nach wenigen Tagen der symptomatischen Erkrankung weisen einige Patienten Blutungen auf. Diese beschränken sich meist auf Hautblutungen wie Petechien, Ekchymosen, Konjunktivalblutungen (Blutungen in der Bindehaut des Auges) oder Blutungen nach Nadeleinstichen, wie bei Injektionen.

Die Dokumentation der Symptome von Ebolafieber-Patienten im Kenema-Staatskrankenhaus in Sierra Leone während der Epidemie 2014 ermöglicht Aussagen über die Häufigkeit der Symptome, dabei ist jedoch zu beachten, dass nur die Daten von 44 Patienten erfasst wurden. Hiernach weisen viele Erkrankte Fieber (89 % der Patienten), Kopfschmerz (80 %), Kraftlosigkeit (66 %), Schwindelanfälle (60 %) und Durchfall (51 %) auf. Weniger häufig auftretende Symptome sind Unterleibsschmerz (40 %), Halsschmerz (34 %), Erbrechen (34 %) und Konjunktivitis (31 %). Bei nur einem Patienten wurden Blutungen dokumentiert, so dass für die Epidemie 2014 hämorrhagische Symptome als selten angesehen werden. Allerdings können diese bei den verbleibenden 43 Patienten nicht ausgeschlossen werden, da eingeschränkt protokolliert wurde.

Die zweite Phase der Erkrankung ist durch hämorrhagisches Fieber gekennzeichnet, hohes Fieber mit > 38,5 °C in Verbindung mit Blutungen. Die Patienten weisen in diesem späteren und schwereren Krankheitsverlauf Schleimhautblutungen vor allem aus dem Magen-Darm-Trakt auf, aber auch aus anderen Organen, z. B. der Niere, was sich durch blutigen Stuhl und Urin äußert. Bei den Patienten zeigen sich Schockzustände und Kreislaufzusammenbrüche. Die Beeinträchtigung der Nieren führt zu Oligurie und schließlich zum Nierenversagen. Auch über neuropsychiatrische Auffälligkeiten wurde berichtet, bei dem Ausbruch von 1995 wurden Krämpfe (Konvulsionen) und Delirium diagnostiziert. Während der Epidemie 2014 wurde außerdem von Verwirrtheit und Hörverlust berichtet. Bei tödlichen Verläufen wird der Tod durch eine Form des septischen Schocks mit multiplem Organversagen hervorgerufen. Durch Obduktionen lassen sich Schädigungen (Nekrosen) an verschiedenen Organsystemen nachweisen, ohne dass sich im Regelfall eine einzelne pathologische Veränderung als Todesursache bestimmen lässt.

Klinisch ist eine Ebolainfektion in ihrem Erscheinungsbild nicht eindeutig von einer Infektion mit dem Marburg-Virus oder anderen Viren, die hämorrhagisches Fieber auslösen, unterscheidbar.

Das Virus kann nur durch Labordiagnostik im Blut, im Urin oder im Speichel zweifelsfrei nachgewiesen werden. Als Standardverfahren hat sich die Reverse Transkriptase-PCR etabliert, bei der bereits sehr wenige Viruskopien für einen sicheren Nachweis ausreichen. Auch ein Nachweis der speziellen Antikörper ist möglich, allerdings werden diese häufig erst im späteren Verlauf der Erkrankung gebildet, ihre Abwesenheit kann daher nicht als Ausschlusskriterium einer akuten Infektion gelten. Der Umgang mit Ebolaviren darf nur in Hochsicherheitslaboren der Schutzstufe 4 erfolgen.

In der Differenzialdiagnose sind andere Tropenkrankheiten auszuschließen, die sich auch durch Fieber äußern können. Dies trifft häufig auf Malaria zu. Weiterhin muss abgeklärt werden, ob es sich nicht um einen Fall von viral hervorgerufenem hämorrhagischen Fieber jenseits von Ebolafieber handelt. Dies ist beispielsweise Denguefieber (verursacht durch das Dengue-Virus), Gelbfieber (verursacht durch das Gelbfieber-Virus), Krim-Kongo-Fieber (verursacht durch das Krim-Kongo-Hämorrhagisches-Fieber-Virus), Lassafieber (verursacht durch das Lassa-Virus), Marburgfieber (verursacht durch das Marburg-Virus) oder Infektionskrankheiten verursacht durch Hantaviren oder das Hepatitis-A-Virus. Ebenso müssen bakterielle Erkrankungen ausgeschlossen werden, wie beispielsweise Typhus abdominalis, Pest, Rickettsiose, Meningokokken-Sepsis oder andere Sepsisformen, Leptospirose, Bakterienruhr oder hämorrhagische Formen des Rückfallfiebers.

Zur Behandlung wird eine symptomatische Therapie durchgeführt. Im Frühstadium gibt es vereinzelt Erfolge mit Rekonvaleszentenserum. Ein wirksames Virostatikum ist bisher nicht bekannt, Ribavirin zeigt gegen Filoviren keine Wirkung. Die Isolierung der Patienten ist von großer Bedeutung, um eine Infektion des medizinischen Personals oder anderer Patienten zu verhindern. Die symptomatische Therapie beinhaltet eine intensivmedizinische Betreuung, bei der fiebersenkende Maßnahmen, Ausgleich des Flüssigkeits- und Elektrolytverlustes sowie Regulierung des Glucosehaushalts im Mittelpunkt stehen.

Im Zuge der Ebolaepidemie 2014 erfolgte erstmals die Behandlung von zwei US-amerikanischen Infizierten mit einem experimentellen Antikörper. Der Name des nicht zugelassenen Medikaments der Firma Mapp Biopharmaceutical aus San Diego lautet ZMapp. Es wurde erfolgreich an einigen Affen getestet und zeigte bei den beiden US-Amerikanern deutliche Verbesserungen innerhalb weniger Stunden.

Mit Ebola-Virus infizierte Rhesusaffen und Makaken konnten noch bis zu 5 Tagen nach der Infektion durch den Einsatz stabiler gegen das L-Protein der RNA-Polymerase, das Virusprotein 24 und das Virusprotein 35 des Ebola-Virus gerichteter kurzer eingreifender Ribonukleinsäuremoleküle, sogenannter small interfering RNA (siRNA), gerettet werden. Von den mit siRNA behandelten 3 Rhesusaffen überlebten 2, von den 7 Makaken überlebten alle.

An folgenden deutschen Kliniken werden Sonderisolierstationen für hochinfektiöse Patienten vorgehalten:


In Österreich verfügt das Kaiser-Franz-Josef-Spital in Wien über eine Sonderisolierstation.

Im Rahmen der Ebolafieber-Epidemie 2014 wurden Infizierte am UKE in Hamburg, am Universitätsklinikum in Frankfurt am Main und am Klinikum St. Georg in Leipzig behandelt. Nach Angaben des Universitätsklinikums in Hamburg-Eppendorf können dort bis zu sechs Patienten gleichzeitig auf der Sonderisolierstation behandelt werden. Für die Betreuung des infizierten Senegalesen wurden 50 Pflegekräfte und 30 Ärzte geschult. Auch das Frankfurter Klinikum gab an, bis zu sechs Patienten gleichzeitig auf der Sonderisolierstation aufnehmen zu können.

Ebolafieber verläuft oft tödlich. Die Todesrate variierte je nach Ausbruch und Virusvariante zwischen 25 und 90 % (siehe Ursache). Ein laborchemischer Prädiktor ist die Menge der durch PCR im Blut nachgewiesenen Virus-RNA. Bei Patienten, welche schließlich an der Krankheit verstarben, war diese um den Faktor 100 (rund 2 Log-Stufen) höher. An der Spezies "Zaire-Ebolavirus" konnte nachgewiesen werden, dass bei Überlebenden des Virus eine gut regulierte Immunantwort auf das Virus stattfindet, während bei Todesopfern eine defiziente Immunantwort mit einer Überaktivierung an Makrophagen und Monozyten vorherrscht. Als Marker für das Überleben wurden Interleukin-1β (IL-1β) und Interleukin-6 (IL-6) identifiziert. Als Marker für eine defiziente Immunantwort konnten Interleukin-10 (IL-10), IL1RA und Neopterin identifiziert werden. Bezüglich der klinischen Erscheinungen ist das Auftreten von Fieber und die Höhe der Körpertemperatur nicht für die Vorhersage des Krankheitsverlaufs geeignet.

Während der Epidemie 2014 wurden am Kenema-Staatskrankenhaus in Sierra Leone Daten zum zeitlichen Verlauf der Krankheit aufgezeichnet. Dabei handelt es sich um eine kleine Gruppe von Patienten, je nach Art des Aufzeichnungszeitpunktes zwischen 39 und 63 Patienten. Die Angaben erfolgen als Mittelwert plus bzw. minus Standardfehler. Demnach vergingen 5,7 ± 0,5 Tage vom Auftreten der ersten Symptome bis zur Einweisung bzw. Aufnahme im Krankenhaus. Bei Patienten, bei denen die Ebola-Viruskrankheit tödlich verlief, lagen 9,8 ± 0,7 Tage zwischen dem Auftreten der ersten Symptome und dem Tod. Bei den Patienten, die die Krankheit überlebten, vergingen 21,3 ± 2,6 Tage zwischen dem Auftreten der ersten Symptome und der Entlassung aus dem Krankenhaus.

Wird die Erkrankung überlebt, können in der Rekonvaleszenz Komplikationen auftreten – beschrieben wurden Psychosen, Myelitis (Rückenmarksentzündung), Hepatitis (Leberentzündung) und Uveitis (Regenbogenhautentzündung). Die Rückenmarksentzündung kann wiederum zu einer Querschnittlähmung führen.

Begleitend zur Behandlung infizierter Patienten sind auch Reinigungs- und Desinfektionsmaßnahmen anzuwenden, um die Übertragung durch kontaminierte Gegenstände und Oberflächen zu unterbinden. Dies dient dem Schutz des medizinischen Personals oder der Angehörigen. Das Robert Koch-Institut empfiehlt für die Desinfektion von Oberflächen wie auch für die Händedesinfektion Mittel mit dem Wirkungsbereich „begrenzt viruzid“ bzw. „viruzid“. Listen mit geeigneten Desinfektionsmitteln sind beim RKI oder beim Verbund für Angewandte Hygiene (VAH) erhältlich. Für den Fall eines möglichen Kontakts mit dem Virus sind zur postexpositionellen Prophylaxe (PEP) Maßnahmen zur Haut- und Schleimhautdesinfektion nötig. Nach Angaben des RKI sollte analog der Deutsch-Österreichischen Leitlinien zur HIV-PEP vorgegangen werden. Ebenso muss das medizinische Personal über eine persönliche Schutzausrüstung verfügen (siehe Übertragung des Ebolavirus).

Bei Patienten, die möglicherweise an Ebolafieber erkrankt sind, ist umgehend zu prüfen, ob es sich um einen begründeten Verdachtsfall handelt (siehe Abschnitt Faktoren der Pathogenese). Das Robert Koch-Institut empfiehlt, den Patienten bis zur Entscheidung im derzeitigen Umfeld zu belassen, weitere Personen (medizinisches Personal oder Angehörige) sollen einen Mindestabstand von einem Meter einhalten oder Schutzkleidung tragen. Sobald klar ist, dass es sich um einen begründeten Verdachtsfall handelt, soll die Isolierung des Patienten in einem dafür eingerichteten Behandlungszentrum erfolgen. Die Infektion ist durch eine Labordiagnostik zu bestätigen. Falls der Zustand des Patienten eine sofortige Behandlung erfordert, darf er in ein Krankenhaus der Regelversorgung eingewiesen werden, unter Beachtung der Regeln des "Barrier Nursing" (übersetzt etwa „Krankenpflege mit Absperrung“). Das Prinzip des „Barrier Nursing“ ist die Beachtung von drei Zonen beim Umgang mit hoch kontagiösen Krankheitserregern. Im ersten Bereich erfolgt die Behandlung des Patienten, hier muss Schutzkleidung getragen werden. Das Zimmer oder die Station ist nur über einen Schleusenbereich zu betreten. In dieser zweiten Zone erfolgt die Dekontamination der Schutzkleidung nach Verlassen des Behandlungsraums, um Infektionen zu vermeiden. Im dritten Bereich findet die Unterstützung des medizinischen Personals statt. Bei Transportfähigkeit ist die Verlegung in ein Kompetenz- und Behandlungszentrum anzustreben.

Weiterhin müssen alle Kontaktpersonen gefunden werden, die direkten Kontakt mit dem Erkrankten hatten oder mit infektiösem Material des Erkrankten in Kontakt gekommen sind. Das Gesundheitsamt ermittelt die Kontaktpersonen und übernimmt eine Einschätzung ihres Expositionsrisiko. Nach dem deutschen Infektionsschutzgesetz (IfSG) kann dabei unter anderem eine Beobachtung ( IfSG) oder eine Quarantäne ( IfSG) angeordnet werden. Durch diese Schutzmaßnahmen ( IfSG) soll die Verbreitung übertragbarer Krankheiten verhindert werden, sie stellen einen Eingriff in die im Grundgesetz für die Bundesrepublik Deutschland festgelegten Grundrechte (z. B. Freiheit der Person) dar. Nach den Empfehlungen des RKI soll bei Kontaktpersonen 21 Tage (Inkubationszeit) lang regelmäßig überwacht werden, ob klinische Erscheinungen (insbesondere Fieber) oder unspezifische Symptome auftreten. Wenn dies der Fall ist, gelten die für begründete Verdachtsfälle beschriebenen Maßnahmen.

Das Umfeld der Patienten ist bei Bestätigung eines Ebolafieberfalls zu desinfizieren, dies gilt beispielsweise für die Wohnung der Person oder Bereiche, in denen die Behandlung stattgefunden hat. Die Rechtsgrundlage in Deutschland für diese „besonderen Maßnahmen“ ist IfSG. Für den Umgang mit Haustieren von an Ebolafieber Erkrankten gibt es zurzeit keine Empfehlung des RKI. Für die USA wurden im Rahmen der Ebolafieber-Epidemie 2014 Empfehlungen durch die CDC veröffentlicht. Im Fall der in Spanien im Oktober 2014 infizierten Krankenschwester wurde ihr Hund vorsorglich getötet.

Das Robert Koch-Institut hat ein Merkblatt mit detaillierten Informationen über Desinfektionsmaßnahmen bei einem begründeten Ebolafieber-Verdachtsfall veröffentlicht. Darin wird unter anderem auch die Wäschedesinfektion beschrieben. Während der Behandlung der Patienten sollte Einmalwäsche verwendet werden. Diese, aber auch Kleidungsstücke und sonstige Wäsche des Erkrankten, die möglicherweise kontaminiert sind, müssen einer geeigneten Abfallentsorgung zugeführt werden.

Alle Abfälle, welche bei der Versorgung eines begründeten Ebolafieber-Verdachtsfalls anfallen, müssen durch eine fachgerechte Abfallentsorgung inaktiviert werden. Neben dem Infektionsschutzgesetz sind auch die Vorgaben der Biostoffverordnung (BioStoffV) zu beachten, insbesondere die in Anhang II BioStoffV festgelegten Schutzmaßnahmen für die Schutzstufen 2, 3 und 4. Diese Schutzmaßnahmen sind für Einrichtungen des Gesundheitsdienstes zwar nicht direkt vorgeschrieben, der Arbeitgeber hat jedoch geeignete Maßnahmen daraus auszuwählen, um Beschäftigte und andere Personen zu schützen ( BioStoffV). Üblicherweise sind kontaminierte feste und flüssige Abfälle vor der endgültigen Entsorgung durch physikalische oder chemische Verfahren zu inaktivieren ().

In den USA wurde berechnet, dass in einem amerikanischen Krankenhaus pro Patient Abfall mit einem Volumen von acht Fässern mit 55 Gallonen Fassungsvermögen täglich entsteht. Das entspricht etwa 2 m³.

Auch für den Fall, dass ein an Ebolafieber Erkrankter stirbt, existieren RKI-Empfehlungen. Wenn eine Obduktion erforderlich ist, soll sie nur unter Bedingungen der Schutzstufe 4 durch besonders qualifiziertes Personal erfolgen. Der Leichnam wird in eine flüssigkeitsdichte Plastikhülle überführt, die in einen verschließbaren und von außen desinfizierten Sarg gelegt wird. Dessen Aufbewahrung muss in einem gesicherten und entsprechend gekennzeichneten Bereich, vorzugsweise einem Kühlraum, erfolgen. Die Bestatter sind über das Infektionsrisiko bei ungeschütztem Kontakt mit dem Leichnam zu informieren. Eine Feuerbestattung (Kremation) wird empfohlen.

Eine serologische Untersuchung von 4.349 Menschen in Gabun zeigt eine hohe Prävalenz von Antikörpern gegen die Virusspezies "Zaire Ebolavirus" (früher als ZEBOV, aktuell als EBOV abgekürzt). Mit Hilfe des ELISA-Verfahrens wurden Blutproben untersucht, und bei 15,3 % der Proben wurde ein gegen EBOV gerichtetes Immunglobulin G (IgG) nachgewiesen. Weitere Untersuchungen zeigen, dass es sich um eine humorale und eine zelluläre Immunantwort handelt. Die untersuchten Personen leben sowohl in Gebieten, in denen es bereits Ausbrüche von Ebolafieber gab, wie auch in Gebieten, in denen bisher keine Erkrankungen registriert wurden. Eine Schlussfolgerung der Studie ist, dass sie durch Kontakt mit dem Virus eine Immunität erlangt haben, ohne dass bei ihnen die Infektionskrankheit Ebolafieber diagnostizierbar ist. Der Kontakt zu Wildtieren als Wirt ist eine Möglichkeit für einen Kontakt mit dem Ebolavirus, erklärt jedoch nicht die hohe Prävalenz. Stattdessen wird angenommen, dass die Personen über Früchte, die mit dem Speichel der entsprechenden Tiere, wie z. B. Fledermäusen kontaminiert waren, mit dem Virus in Berührung gekommen sind. Die Frage, ob ein Mensch durch eine überstandene Ebolafieber-Erkrankung zumindest gegen die infektionsauslösende Ebolavirus-Spezies immun ist, wird durch die Studie nicht geklärt.

Spätestens seit 2012 wird an der Entwicklung von Ebola-Impfstoffen gearbeitet; manche haben sich im Tierversuch sogar bei der Gabe zwei bis drei Tage nach der Infektion noch bewährt und zu einer Heilung geführt.

Ab September 2014 wurde erstmals ein Impfstoff gegen das Ebolavirus von den National Institutes of Health der Vereinigten Staaten auf seine Verträglichkeit an zwanzig gesunden Probanden geprüft. Der Impfstoff besteht aus einem Schimpansen-Adenovirus, dem zusätzlich gentechnisch ein Ebola-Virusprotein eingepflanzt wurde, das eine Immunantwort auslöst. Das Schimpansen-Adenovirus wurde als Vehikel für das Ebola-Virusprotein ausgewählt, da es normalerweise keine Erkrankung beim Menschen auslöst. Der Impfstoff wurde von der schweizerisch-italienischen Biotechfirma Okairos entwickelt, die im Jahr 2013 vom Pharmakonzern GlaxoSmithKline übernommen wurde.

In Kanada wird ebenfalls an der Entwicklung eines Impfstoffs gearbeitet. Das experimentelle Präparat namens VSV-EBOV, ein Präparat aus dem vesikulären Stomatitis-Virus mit Ebola-Antigenen, wurde im staatlichen Canadian National Microbiology Laboratory entwickelt und bereits erfolgreich an Affen getestet. Im Oktober 2014 begannen Tests an Menschen, in der Versuchsphase wird das Mittel 40 gesunden, freiwilligen Probanden in den USA verabreicht. Außerdem stellte die kanadische Regierung den experimentellen Impfstoff der WHO zur Verfügung.

Ende 2016 wurde berichtet, dass der Wirkstoff rVSV-ZEBOV der Firma Merck, Sharp & Dohme laut WHO als erster Impfstoff wirksam und sicher abschließend klinisch getestet wurde.

Wissenschaftlern des USAMRIID gelang es im Jahre 2003, Mäuse durch Injektion von virusähnlichen Partikeln zu immunisieren. Anfang 2005 zeigten Forscher um Steven Jones und Heinz Feldmann (University of Manitoba, Winnipeg, Kanada) eine erfolgreiche Impfung (aktive Immunisierung) bei Javaneraffen ("Macaca fascicularis") mit einem abgeschwächten, lebenden, rekombinanten Vesiculären-Stomatitis-Virus (VSV), das auf seiner Oberfläche ein so genanntes Glycoprotein des Zaire-Ebolavirus-Stammes „Kikwit“ produziert. Nunmehr erhofft man sich eine baldige vorbeugende Impfmöglichkeit auch beim Menschen.

Forscher des US-Militärs haben 2010 ein Medikament, das die Virusreplikation hemmt, indem es sich an die RNA des Virus bindet, erfolgreich an Affen getestet. Dabei überlebten 60 % der Rhesusaffen und 100 % der vorher infizierten Makaken.

2011 verkündeten Forscher des Scripps Research Institute Erfolge mit einem von ihnen entwickelten Antikörper. 

Die Estrogenrezeptormodulatoren Clomifen und Toremifen hemmen die Ebolaerkrankung in infizierten Mäusen.
Laut einer Studie aus dem Jahr 2014 verhindert das Herzmittel Amiodaron das Eindringen von Ebolaviren in Zellen zumindest im Reagenzglas.

Experimentelle Impfstoffe und Wirkstoffe sind z. B. ZMapp, VSV-EBOV, cAd3-ZEBOV, TKM-Ebola, Favipiravir, Brincidofovir, JK-05, FGI-106 und BCX4430.

Seit 1976 kam es wiederholt zu Ausbrüchen des Ebolafiebers. Insbesondere im tropischen Teil Afrikas kosteten diese Ausbrüche zahlreiche Personen das Leben:

"Legende:" BDBV: Bundibugyo Virus (Spezies "Bundibugyo Ebolavirus"), EBOV: Ebola Virus (Spezies "Zaire Ebolavirus"), RESTV: Reston Virus (Spezies "Reston Ebolavirus"), SUDV: Sudan Virus (Spezies "Sudan Ebolavirus") und TAFV: Taï Forest Virus (Spezies "Taï Forest Ebolavirus"), siehe auch aktualisierte Nomenklatur der Spezies in der Gattung "Ebolavirus".

Die Ebola-Epidemie in Westafrika brach zunächst in Guinea in der Präfektur Macenta und der Präfektur Guéckédou aus. Laut WHO und dem Gesundheitsministerium in Guinea war auch die Hauptstadt Conakry betroffen; der Schwerpunkt der Neuerkrankungen verlagerte sich ab Juli 2014 mit 454 Infizierten zunehmend nach Sierra Leone. Eine Behandlungsstation in Telimélé im Westen von Guinea konnte bereits wieder geschlossen werden.

In Liberia wurden hingegen Erkrankungen in sieben Bezirken bestätigt, darunter in der Hauptstadt Monrovia, wo bis Ende Juli ein Ebola-Behandlungszentrum aus Zelten, ergänzend zu den bestehenden Krankenhäusern, aufgebaut wird. Ein Behandlungszentrum besteht bereits in Foya im Bezirk Lofa im abgelegenen Grenzgebiet zu Guinea, wo auch Ärzte ohne Grenzen in Voinjama seine Aktivität verstärkt. Liberia will angesichts des Ausbruchs alle Grenzen zu den Nachbarstaaten schließen. Ausgenommen seien zwei Flughäfen und drei andere Grenzpunkte.

Internationale Hilfemaßnahmen kamen aufgrund politischer und gesellschaftlicher Probleme nur schleppend in Gang. Ein großer Teil der Bevölkerung ist über die zunächst grippeähnlichen Symptome und den Infektionsweg des Virus schlecht oder gar nicht informiert, sodass Infizierte nicht früh genug isoliert werden und gegebenenfalls weitere Personen anstecken. Auch ist das Misstrauen gegenüber ausländischen Ärzteteams und den inländischen Behörden groß, denn es gab Gerüchte, dass Helfer das Virus eingeschleppt hätten. So brach das Rote Kreuz eine Hilfsaktion ab, nachdem Mitarbeiter bedroht worden waren. Ein Krisentreffen der Weltgesundheitsorganisation (WHO) in Ghana am 2. und 3. Juli 2014 sollte die Teilnehmerstaaten dazu bringen, die Epidemie offiziell anzuerkennen, und die Ausbreitung über die Landesgrenzen hinaus verhindern. Inzwischen droht beispielsweise die Regierung von Sierra Leone mit Strafen, falls Kranke versteckt würden. Nötig seien laut WHO eine effektive Analyse und eine Rückverfolgung der Kontakte mit Erkrankten (englisch: "contact tracking") sowie eine grenzüberschreitende Zusammenarbeit. Die WHO sprach aber keine generellen Reisebeschränkungen aus. Liberia hingegen plante zunächst wohl nicht einmal eine Aufklärungskampagne.

Eine hohe Überlebensrate von 63 Prozent im Behandlungszentrum des Donka-Krankenhauses in Conakry, wo seit dem 25. März 59 bestätigte Ebolaerkrankte behandelt wurden, steht im Kontrast zur geringeren Überlebensrate auf dem Land: Gerade wenn erst spät medizinische Hilfe in Anspruch genommen wurde, wie zu Beginn der Epidemie in Guéckédou, habe es eine sehr geringe Überlebenschance gegeben. Behandlungseinrichtungen wurden daher vor allem im Grenzgebiet der drei Staaten abseits der großen Zentren aufgebaut, unter anderem in Kailahun, Koindu und Buedu im Bezirk Kailahun. Dort wurden 200 lokale Gesundheitshelfer ausgebildet.

Zu den am 20. Juli im Labor bestätigten 786 Erkrankungen in Westafrika (1093 insgesamt) mit 660 Toten (davon 442 bestätigt) sind noch weitere inzwischen an Ebola Erkrankte sowie Todesfälle hinzuzuzählen. Die Weltgesundheitsorganisation berichtet regelmäßig in englischer Sprache darüber (mit klarer Übersichtstabelle).

Zwei in Afrika arbeitende US-amerikanische Infizierte sollen laut Medienberichten mit einem experimentellen Serum behandelt worden sein, das bis dahin nur an Affen getestet wurde.

Nachträgliche Untersuchungen von Blutproben aus Sierra Leone, die bereits zwischen 2006 und 2008 zwecks Lassafieber-Diagnostik gewonnen worden waren, ergaben serologische Hinweise darauf, dass Ebola-Infektionen bereits in diesen Jahren auftraten, aber nicht erkannt wurden. Die Ernährungs- und Landwirtschaftsorganisation der Vereinten Nationen (FAO) warnt inzwischen vor dem Verzehr von Fledermäusen, anderen Wildtieren und Kadavern, da diese Überträger des Virus sein können.

Das Auswärtige Amt rät seit dem 1. August 2014 von Reisen in die Gebiete ab. Seit dem 5. August 2014 gilt darüber hinaus auch eine förmliche Reisewarnung für die Länder Liberia, Guinea und Sierra Leone. In der ersten Augustwoche 2014 wurden erste Infektionsfälle in Nigeria bekannt, darunter ein Arzt. Zwei infizierte US-Amerikaner wurden in die Vereinigten Staaten ausgeflogen. Die spanische Luftwaffe evakuierte ebenfalls einen Staatsbürger.

Am 8. August 2014 erklärte die WHO die Epidemie zum internationalen Gesundheitsnotfall. Am 14. August 2014 erklärte die WHO, es gebe Hinweise in den betroffenen Ländern, dass das tatsächliche Ausmaß des Ausbruchs weit größer sei, als die Zahl der Todesopfer und Erkrankungen erkennen lasse.

Am 4. August 2014 gab die Weltbank bekannt, den betroffenen Ländern Guinea, Liberia und Sierra Leone durch eine Nothilfe bis zu 200 Mio. US-Dollar (149 Mio. Euro) zur Verfügung zu stellen. Ebenfalls finanzielle Hilfen wurden von der Afrikanischen Entwicklungsbank (60 Mio. US-Dollar) und der WHO (100 Mio. US-Dollar) zugesagt.

Durch Veruntreuung von mindestens fünf Millionen Euro an Ebola-Geldern entstanden erhebliche finanzielle Schäden. So sind laut einem Finanzprüfungsbericht fast zwei Millionen der Rot-Kreuz-Spenden durch Mitarbeiter der Internationalen Rotkreuz- und Rothalbmond-Bewegung in Sierra Leone mit Hilfe von Bankangestellten hinterzogen worden. Durch überhöhte Preise für Hilfsgüter und maßlose Personalkosten gingen in Liberia circa zwei Millionen Euro verloren. Für die Zollabfertigung in Guinea wurde eine Million Euro zuviel abgerechnet. Weitere Untersuchungen waren 2017 noch nicht abgeschlossen.




</doc>
<doc id="12642" url="https://de.wikipedia.org/wiki?curid=12642" title="Dominanz (Genetik)">
Dominanz (Genetik)

In der Genetik wird zwischen dominanten (, ‚beherrschend‘) und rezessiven Allelen unterschieden. Bei bezüglich eines Merkmals heterozygoten Individuen wird allein das Merkmal des dominanten Allels ausgeprägt, das Merkmal des rezessiven Allels findet sich dagegen nicht im Phänotyp.




</doc>
<doc id="12644" url="https://de.wikipedia.org/wiki?curid=12644" title="Phänotyp">
Phänotyp

Der Phänotyp ( "phaíno" „ich erscheine“ und "týpos" „Gestalt“) oder das Erscheinungsbild ist in der Genetik die Menge aller Merkmale eines Organismus. Er bezieht sich nicht nur auf morphologische, sondern auch auf physiologische Eigenschaften und auf Verhaltensmerkmale.

Der Phänotyp wird durch das Zusammenwirken von Erbanlagen und Umweltfaktoren (Modifikation) bestimmt. Inwieweit der Phänotyp durch Umwelteinflüsse beeinflussbar ist, hängt von der Reaktionsnorm ab. Diese Möglichkeit, auf Umwelteinflüsse zu reagieren, ist genetisch festgelegt. Verfahren, mit denen Rückschlüsse vom Erbgut, d. h., der individuellen Desoxyribonukleinsäure (DNS), auf den Phänotyp eines Individuums geschlossen werden, werden DNA-Phänotypisierung genannt.

Wenn Umwelteinflüsse eine starke Variabilität des Erscheinungsbildes eines Individuums hervorrufen können, spricht man von hoher phänotypischer Plastizität oder großer Variationsbreite oder Modifikabilität. Ist der Phänotyp jedoch weitgehend durch seinen Genotyp vorherbestimmt, deutet dies auf geringe Plastizität hin. Das Konzept der phänotypischen Plastizität beschreibt das Maß, in dem der Phänotyp eines Organismus durch seinen Genotyp vorherbestimmt ist. Ein hoher Wert der Plastizität bedeutet: Umwelteinflüsse haben einen starken Einfluss auf den sich individuell entwickelnden Phänotyp. Bei geringer Plastizität kann der Phänotyp aus dem Genotyp zuverlässig vorhergesagt werden, unabhängig von besonderen Umweltverhältnissen während der Entwicklung. Hohe Plastizität lässt sich am Beispiel der Larven zweier Molcharten beobachten: Wenn diese Larven die Anwesenheit von Räubern wie Libellenlarven wahrnehmen, vergrößern sich Kopf und Schwanz im Verhältnis zum Körper, und die Haut wird dunkler pigmentiert. Larven mit diesen Merkmalen haben bessere Überlebenschancen gegenüber Räubern, wachsen aber langsamer als andere Phänotypen.
Phänotypische Veränderungen aufgrund von Umwelteinflüssen heißen Modifikationen. So können zum Beispiel genetisch identische Pflanzen (zum Beispiel Stecklinge) an unterschiedlichen Standorten völlig unterschiedliche Wuchsformen entwickeln.

Trifft ein exogener (nicht erblicher) Faktor in einer bestimmten Periode der Entwicklung auf einen Organismus, so kann ein Merkmal genau so ausgebildet werden wie im Falle einer genetischen Veränderung (Mutation). In diesem Fall spricht man von Phänokopie.

Wenn ein bestimmter Phänotyp von verschiedenen genetischen Aberrationen kommen kann, spricht man von Heterogenie.

Besonders im biomedizinischen Bereich wird das Modell des Phänotyps von Russel/Burch 1959 um den Dramatyp erweitert. Dieser stellt die Reaktion auf die unmittelbare, augenblickliche Umgebung dar, während sich die Entwicklung des Phänotyps auf einen längeren Zeitraum bezieht. Die Kenntnis um den Dramatyp ist notwendig, um bei Tierversuchen ein möglichst gleichförmiges Ergebnis zu erhalten.





</doc>
<doc id="12645" url="https://de.wikipedia.org/wiki?curid=12645" title="Eispickel">
Eispickel

Der Eispickel ist ein Ausrüstungsgegenstand im Alpinismus auf Hochtouren. Er dient dem Bergsteiger als Gehunterstützung auf Fels, Schnee, Firn und Eis, zum Sondieren der Schneeauflage auf Gletscherspalten, zum Klettern im Eis, zum Abbremsen eines Sturzes auf geneigten Schnee- oder Firnflächen mittels Pickelrettungsgriff, als "Anker" zur Sicherung von Seilpartnern und zur Bergung von in Gletscherspalten Gestürzten. Vor der Einführung der Steigeisen mit Frontalzacken (ab 1932) wurde er außerdem zum Schlagen von Stufen im Eis verwendet (ähnlich einer Spitzhacke); in dieser Funktion kommt der Eispickel heute nur dann zur Anwendung, wenn keine Steigeisen verfügbar sind.

Hervorgegangen ist der Eispickel mit der Entwicklung des Bergführerwesens im Alpenraum aus einem "Küchenbeil" und dem Alpenstock. Er hatte einen Pickelkopf mit Haue und Schaufel aus Stahl sowie einen Holzschaft mit Stahlspitze. Der Schweizer Bergführer Franz Josef Lochmatter brachte um 1860 den ersten Pickel, der in Chamonix in Gebrauch war, ins Mattertal bzw. in die Schweiz.

Dieser Vorgänger des modernen Eispickels wird heute noch – nach dessen Konstrukteur, dem Bergsteiger Peter Aschenbrenner – als „Aschenbrenner-Eispickel“ bezeichnet. Bei modernen Eispickeln besteht der Schaft aus Aluminium, Haue und Schaufel werden aus legierten Stählen gefertigt (z. B. Chrom-Molybdän oder Nickel-Chrom-Molybdän).

Eispickel werden nur noch sehr selten von Hand geschmiedet. Dabei wird ein Stück Maschinenbaustahl (0,53 % Kohlenstoff, 0,95 % Silizium, 1 % Magnesium) in einem Gesenk vorgepresst und danach in mehreren Stufen zu einem Rohling geschmiedet. Dieser wiederum wird geschliffen und hochglanzpoliert, was unter anderem eine bessere Korrosionsbeständigkeit zur Folge hat. Moderne Eispickel werden zum großen Teil im "Gesenk" geschmiedet. Bei diesem Verfahren wird weichgeglühter Stahl bei einer Temperatur von rund 850 °C vorgeformt und in das Gesenk gepresst. Dadurch wird der Faserverlauf nicht unterbrochen, sondern nur ausgerichtet, was eine erhöhte Festigkeit zur Folge hat.

Nach Anwendungszweck unterscheidet man Eispickel für leichte oder mittlere Hochtouren und spezielle Eisgeräte für kombinierte Touren oder zum Eisklettern. Es gibt sie in verschiedenen Schaftlängen von 40 bis 90 cm, wobei davon ausgegangen wird, dass bei Gletscherbegehungen Pickel in der Länge von 60 bis 70 cm ausreichend sind.

Für Eispickel und Eisgeräte gibt es zwei Normen, welche sich in Bezug auf die entsprechenden Prüfungen unterscheiden. So wird in „B“-Geräte (Basisgeräte) und „T“-Geräte (Technikgeräte) unterschieden. Letztere sind für den extremeren Einsatz gedacht und unterliegen höheren Anforderungen.

Der russische Revolutionsführer Leo Trotzki wurde 1940 mit einem Eispickel angegriffen und starb an der Verletzung. Im Jahr 2005 wurde der verschollen geglaubte Eispickel gefunden. Das Mordinstrument wurde im Kriminologischen Museum in Mexiko-Stadt ausgestellt, dann aber wegen Diebstahlsgefahr durch eine Kopie ersetzt. Ein mexikanischer Geheimdienstler, auch ein Mitbegründer des Museums, habe den Originalpickel an sich genommen und aufbewahrt, schrieb die mexikanische Tageszeitung La Jornada. Seine Tochter berichtete, dass ihr Vater viermal vergeblich versucht habe, den Eispickel zurückzugeben. Doch niemand wollte das Original zurückhaben. Dann nahm diese Tochter den Eispickel an sich und präsentierte ihn in einer Radiosendung.

In Literatur und Film tauchen als "Mordinstrument" in der Regel eher (Bar-)Eispickel (engl. "ice pick") auf, die unter anderem zum Zerkleinern von Stangeneis dienen. Sie sind kleiner als die Bergsteiger-Eispickel und in verschiedenen Formen erhältlich, darunter einfache Dornen mit Griff, die auch als Stichwaffe verwendet werden können.


</doc>
<doc id="12646" url="https://de.wikipedia.org/wiki?curid=12646" title="Geschichte Serbiens">
Geschichte Serbiens

Der Artikel Geschichte Serbiens beschäftigt sich mit den historischen Ereignissen auf dem Gebiet der heutigen Republik Serbien sowie, soweit sich dies aus historischen Entwicklungen ergibt, auch auf angrenzenden Gebieten. Dabei wird vor allem der Zeitraum vom Eintreffen der ersten slawischen Stämme in Südosteuropa im 6. Jahrhundert dargestellt. Während dieser Zeit waren die Grenzen der jeweiligen serbischen Herrschaftsgebiete, deren Namen wie auch deren Bewohner starken Veränderungen unterworfen, so dass nur eingeschränkt von einer kontinuierlichen Geschichte gesprochen werden kann.

Der erste serbische Staat, Raška, konstituierte sich in der zweiten Hälfte des 8. Jahrhunderts unter Fürst Višeslav. Das Fürstentum Raška existierte zunächst unter byzantinischer bzw. bulgarischer Suzeränität. Danach entwickelte sich das Land unter den Nemanjiden (1167 bis 1371) zum Serbischen Großreich, dessen Einflusszone den gesamten Balkan umfasste. Das Serbische Großreich zerfiel nach dem Tod des Kaisers Stefan Uroš IV. Dušan, 1355, in mehrere unabhängige Fürstentümer, die sich in der Folge teilweise gegenseitig befehdeten. Dies und die fehlende Staatseinheit unter den Serben begünstigte 1459 die Eroberung durch das Türkisch-Osmanische Reich und die über 350 Jahre währende osmanische Fremdherrschaft. Im Gefolge des Ersten und Zweiten Serbischen Aufstands Anfang des 19. Jahrhunderts entstand 1815 das Fürstentum Serbien, das aber noch dem Osmanischen Reich tributpflichtig war. Erst mit den Ergebnissen des Berliner Kongresses, 1878, wurde Serbien ein unabhängiger Staat. 1882 wurde das Fürstentum Serbien zum Königreich Serbien erhoben, das zunächst im Wesentlichen nur das heutige Zentralserbien umfasste. Mit den Balkankriegen kamen bis 1913 das Kosovo, das heutige Mazedonien sowie der größere Teil des Sandschaks von Novi Pazar dazu.

Am Ende des Ersten Weltkrieges, 1918, ging das Königreich Serbien im "Königreich der Serben, Kroaten und Slowenen" auf, das 1929 in das Königreich Jugoslawien umbenannt wurde. Im Zweiten Weltkrieg war Serbien nach dem deutschen Balkanfeldzug von 1941 bis 1944 ein Satellitenstaat unter der Kontrolle des Deutschen Reiches. Mit dem Sieg der Jugoslawischen Volksbefreiungsarmee unter Josip Broz Tito wurde die Monarchie abgeschafft und 1945 der Grundstein für die Sozialistische Föderative Republik Jugoslawien der Nachkriegszeit gelegt. Während dieser Zeit war Serbien in seinen heutigen Grenzen eine Teilrepublik Jugoslawiens. Zehn Jahre nach dem Tod Titos brach Jugoslawien ab den 1990er Jahren in den Jugoslawienkriegen wieder auseinander. Seit dem Austritt Montenegros 2006 aus der Staatenunion Serbien und Montenegro ist Serbien erstmals seit 1918 wieder ein eigenständiger Staat.

Das Gebiet des heutigen Serbien war ursprünglich von Illyrern, Kelten, Thrakern und etwas später von Griechen besiedelt. Mitte des 2. Jahrhunderts v. Chr. gründeten die Römer die Provinz Illyrien, deren Grenzen ab dem 3. Jahrhundert nach Christus von verschiedenen Nomadenvölkern überschritten wurden. Kurz nach dem Fall Roms wurde Illyrien dem Byzantinischen Reich angeschlossen.

Zur Zeit des Römischen Reiches gehörte das Gebiet des heutigen Serbien zur Provinz Moesia superior. Seit der Teilung des Reiches 395 gehörte es zum oströmischen (byzantinischen) Reich.

Das Banat war Teil der römischen Provinz Dakien.

Die Grenzen eines eigentlichen serbischen Territoriums wurden erst verhältnismäßig spät, nämlich im 15. Jahrhundert umrissen. So wie andere slawische Stammesverbände (Severci, Draguviti, Duljebi) waren die Serben ein Teil des in der „Urheimat“ im Osten Europas lebenden Verbandes slawischer Stämme. Ihre räumliche Verteilung im Anschluss an die Migration von Osteuropa nach Süden ist nur ungefähr bekannt: die Severci lebten zwischen der Donau und dem Balkangebirge, die Draguviti im ägäischen Hinterland in Makedonien und Thrakien; die Kroaten in der Ebene östlich der Alpen und die Serben waren ihre Nachbarn weiter im Osten.

Erst für die Mitte des 10. Jahrhunderts lassen sich in dem Staatshandbuch „De adinistrado imperio“ des byzantinischen Kaisers Konstantinos VII. Porphyrogennetos genauere Angaben über die von den Serben besiedelten Territorien finden. Der Kaiser unterscheidet dabei zwischen den bereits getauften, d. h. zum Christentum bekehrten Serben im Unterschied zu den heidnisch gebliebenen, deren Siedlungsgebiet sich zwischen dem der ebenfalls bereits christianisierten Kroaten und dem Ersten Bulgarischen Reich erstreckte. Die östlichsten von Kroaten bewohnten Gebiete – Livno, Pliva und Imota – markierten den Verlauf der Westgrenze des damaligen Serbien.

Da sich die serbische historische Entwicklung von der slawischen Landnahme bis zur Etablierung eines serbischen Kaiserreiches innerhalb des Territoriums im damals bestehenden oder auf dem ehemaligen Boden des Byzantinischen Reiches vollzog, war in der Geschichte des mittelalterlichen serbischen Staates der byzantinische Einfluss prägend. Die geistigen und politischen Ideen Byzanz' dienten den Regionen des christlichen Balkan als ein für alle Völker primär wirkender Orientierungspunkt. Das christliche byzantinische Reich bestimmte die Richtung der Geschichte der jungen Völker, die sich auf dem Boden seines Reiches niedergelassen hatten – im politischen, insbesondere aber im geistigen Sinne, wo sich in der Rivalität zwischen dem westlichen Papsttum und der konstantinopolischen Orthodoxie das östliche Staats- und Religionsideal selbst nach dem Zerfall des Byzantinischen Reiches hielt. 

Im Osten war die Stadt Rasa (später Ras) der Grenzort zum Ersten Bulgarischen Reich, in dem in der zweiten Hälfte des 10. Jahrhunderts der bulgarische Zar Peter ein Bistum gegründet hatte, dessen Sitz, die Petrova crkva, das älteste sakrale Bauwerk im heutigen Serbien ist. Die Nordgrenze bildete allem Anschein nach die Save, im Süden erstreckte sich Serbien bis zu den im Küstengebiet der Adria gelegenen Fürstentümern: Zahumlje, das zwischen dem Fluss Neretva und Ragusa lag; Travunja, das sich von dort bis zur Bucht von Cattaro (Boka Kotorska) erstreckte, und Duklja ("Dioclea"), das sich bis zum Fluss Bojana hinzog. Die Bewohner der beiden erstgenannten Fürstentümer sollten die Nachfahren von bereits zuvor angesiedelten Serben sein. Der Fürst von Zahumlje behauptete sogar, dass seine Familie von Serben abstammte, die ursprünglich aus der Weichselregion gekommen waren. Den Einwohnern von Duklja wurde hingegen keine serbische Herkunft zugeschrieben.

Den ursprünglichen Stammesnamen der Serben bewahrte nur ein Fürstentum im Inneren der Balkanhalbinsel, dessen Beherrscher seit dem 8. Jahrhundert namentlich bekannt sind. Weitere Angaben über ihre Herrschaft stammen aus dem 9. Jahrhundert. Das damalige Serbien wurde ebenso wie auch andere von slawischen Stämmen besiedelte Territorien von zwei Seiten bedrängt: Einerseits durch das Byzantinische Reich, andererseits durch das Erste Bulgarische Reich, von dem es als Hindernis für seine Expansion nach Westen gesehen wurde. Im Gegensatz zu Byzanz, das sich mit der Oberherrschaft über die slawischen Stämme begnügte, strebten die bulgarischen Khane nach vollständiger Unterwerfung. Die um das Jahr 870 von Byzanz aus erfolgte Christianisierung der serbischen Herrscher stellte einen großen Erfolg für die Byzantiner dar.

Seit dem 6. Jahrhundert siedelten sich Serben auf dem Gebiet des heutigen Serbien an. Sie ließen sich zuerst in einer Gegend nieder, die Raszien genannt wird. Deshalb wurden sie jahrhundertelang außer als Serben auch als Raszier bezeichnet. Byzanz ermutigte slawische Stämme, sich als Föderaten in den Provinzen des Balkans anzusiedeln. Diese sich seit 580 abzeichnende Landnahme der Slawen auf dem Balkan reichte vom Fürstentum Karantanien über das heutige Slowenien und Kroatien, Bosnien und Serbien bis nach Bulgarien und den Peloponnes. Byzanz förderte das Entstehen kleiner Herrschaften als Puffer gegen das Awarenreich im Norden. Die Herrschaft der Steppennomaden bestand von 567 bis 803 im Karpatenbogen bis zur Donau. Es wurde vom Heer des Frankenreichs zerschlagen.

Manche der serbischen Einwanderer nahmen die griechische Kultur an, die meisten aber bewahrten ihre slawisch-serbische Identität. Ihre Stammesführer bildeten mit der Zeit Fürstentümer unter der Oberhoheit von Ostrom. Von ihnen war das weitgehend selbstständige serbische Fürstentum das bedeutendste. Es erlebte mit Župan Vlastimir und der frühen Hauptstadt Ras bei Novi Pazar (daher auch die Bezeichnung Raszien) in der Mitte des 9. Jahrhunderts seine erste Blüte.

Bis ins 9. Jahrhundert lebten die Serben unter nominaler Oberherrschaft des Byzantinischen Reiches und in relativ friedlicher Nachbarschaft mit den Bulgaren. Der oberste Mann im Staat war der sogenannte Groß-Župan, der von den anderen Županen als Anführer anerkannt wurde. 830 schlossen sich die in loser Nachbarschaft lebenden Stämme unter Župan Vlastimir zu einer Stammesföderation zusammen, um sich gegen die nun unter Khan Presian I. gegen Byzanz vordrängenden Bulgaren wehren zu können.

Unter Vlastimir, seinem Sohn Strojimir und seinen Nachfolgern wurde Serbien (Raszien) in der 2. Hälfte des 9. Jahrhunderts wahrscheinlich unter direktem Einfluss der Slawenapostel Kyrill und Method von Byzanz aus orthodox christianisiert.

Im Zuge der Auseinandersetzungen zwischen Bulgarien und Byzanz im 10. Jahrhundert war Serbien eher Byzanz zugeneigt. Um diese Gefahr auszuschalten, gab der bulgarische Zar Simeon I. vor, Časlav, einen am bulgarischen Hof aufgewachsenen Urenkel Vlastimirs als Groß-Župan einsetzen zu wollen. Dies war aber nur ein Vorwand, um Serbien als Provinz zu annektieren. Časlav wurde zusammen mit anderen Županen gefangen genommen. Viele, die nicht schon vorher geflohen waren, flohen nun nach Byzanz und Kroatien.

Nach Simeons Tod 927 kehrte Časlav als Befreier nach Serbien zurück. Er erkannte Byzanz als oberste Autorität an und bekam dafür Hilfe beim Wiederaufbau des Landes. Unter Časlav bekam der Staat, der etwas größer war als unter Vlastimir, wieder inneren Zusammenhalt. Nach seinem Tod bei einem Angriff der Ungarn zerfiel er aber wieder.

Im 11. Jahrhundert gab es das erste in größerem Rahmen anerkannte serbische Königtum unter Mihailo von Zeta. Dieser hatte zuerst – unter anderem durch seine Heirat – eine engere Verbindung zu Byzanz gesucht. Als aber Byzanz im Kampf gegen die Normannen geschwächt war, brach er seine Neutralität und unterstützte einen Aufstand der südslawischen Völker gegen die byzantinische Oberherrschaft. Nachdem dieser gescheitert war, suchte er Unterstützung im Westen, beim Papst. Mitverantwortlich für diese Wende war auch, dass Mihailo ein eigenes Erzbistum und den Königstitel wollte. Der Papst, der nach dem Schisma von 1054 Interesse daran hatte, die Herrscher an den Rändern seines Einflussgebietes für sich zu gewinnen, ernannte Mihailo zum ersten serbischen König (1077) und machte somit sein Land Duklja zum ersten anerkannten serbischen Königtum.

Im 12. Jahrhundert begann unter Stefan Nemanja eine der wichtigsten Perioden für das serbische Nationalbewusstsein. Stefan besiegte in der Schlacht bei Pantino seine Brüder im Kampf um die Herrschaft und schloss mit den beiden überlebenden Brüdern eine Union, in der sie ihn als Groß-Župan anerkannten. So wurde er zum alleinigen Herrscher Rasziens und Dioklitiens (Dukljas). Diese Union und seine Unterstützung für einen Angriff Venedigs und Ungarns auf Byzanz brachten ihn in Konflikt mit dem byzantinischen Kaiser Manuel. In einer erniedrigenden Prozedur musste er sich unterwerfen, um dem Land für einige Jahre Stabilität zu sichern.

Doch nach dem Tod Manuels im Jahr 1180 machte er sich die verworrene Situation in Byzanz zunutze, um dem Reich die Unabhängigkeit und große Gebiete abzuringen – darunter das südliche Kosovo um Prizren und die Gegend um Niš, das zeitweilig zur neuen Hauptstadt wurde. In einem Friedensvertrag mit dem byzantinischen Kaiser Isaak II. Angelos wurde seinen Expansionsbestrebungen Einhalt geboten, aber gleichzeitig blieben die neuen Grenzen des Landes weitgehend unangetastet.

1196 dankte Stefan Nemanja zugunsten seines mittleren Sohnes Stefan ab und entsagte als Mönch "Simeon" allem Weltlichen. Er wirkte aber auch noch als Mönch prägend für die weitere Geschichte Serbiens: Er gründete zahlreiche Kirchen und Klöster (darunter die bedeutenden Klöster Studenica und Hilandar, die beide zum Weltkulturerbe zählen). Nach seinem Tod im Jahr 1200 wurde er ein wichtiger serbischer Nationalheiliger.

Stefan Nemanjić, der Sohn, brauchte einige Jahre, bis er die Herrschaft 1207 endgültig gegen seinen älteren Bruder Vukan behauptet hatte. Eine viel wichtigere Rolle für Stefans über 30-jährige Regierungszeit spielte aber sein jüngerer Bruder Rastko, der als der Heilige Sava von Serbien bekannt wurde.

Als sich die serbische Politik nach anfänglich guten Beziehungen zu Byzanz infolge der Einnahme Byzanz' durch den Vierten Kreuzzug eher dem Westen zuwandte. Damit bekam Stefan den Beinamen "Prvovenčani", der Erstgekrönte – und die von seinem Vater begründete Dynastie der Nemanjiden war bestätigt und gestärkt.

Die wichtigste und folgenreichste Tat Savas aber war, dass er bei einem Besuch des byzantinischen Patriarchen im Nicäanischen Exil das Recht erwirkte, eine autokephale und autonome Serbisch-Orthodoxe Kirche zu gründen. Diese Kirche mit ihren ersten aus Serbien stammenden Heiligen Simeon und Sava sollte – besonders in der langen Zeit der osmanischen Herrschaft – das Fundament für das serbische Selbstbewusstsein bilden. Mit der Schaffung eines Rechtskodex – des sogenannten Nomokanon – schuf Sava zudem die Grundlage für eine enge Verbindung zwischen Kirche und Staat, die ebenfalls sein Geschlecht überdauern sollte.

Während des Nemanjiden-Reichs im 13. Jahrhundert kam es zu wichtigen Veränderungen in der sozialen Struktur des Staates. Aus den Župans, den Sippenführern, wurden Adlige. Die einst freien Bauern gerieten zunehmend in deren Abhängigkeit. Die Städte erhielten Sonderrechte. So wurde aus dem losen Stammesverband ein feudaler Staat mit einem etablierten Herrschergeschlecht, das von Gottes Gnaden legitimiert war, sowie einer starken Nationalkirche.

Eine weitere Konsolidierung erfuhr das Reich unter der langen Herrschaft von Stefan Uroš I. (1243–1276), der als dritter Sohn des Stefan Prvovenčani nach seinen Brüdern Radoslav und Vladislav die Herrschaft antrat. Die außenpolitischen Konflikte hielten sich in Grenzen und so konnte der wirtschaftliche Ausbau voranschreiten. Dieser beruhte vor allem auf dem Bergbau: Bergwerke zum Abbau von Gold und Silber, aber auch Eisen, Kupfer und Blei wurden eröffnet. Um diese herum entstanden Siedlungen, der Handel kam in Schwung. Historikern zufolge stammte in der Hochphase des serbischen Bergbaus im 14. bis 15. Jahrhundert bis zu einem Viertel des in Mittel- und Westeuropa gehandelten Silbers aus serbischen Bergwerken, in der die Grundlage für der Machtentfaltung der Nemanjiden geschaffen wurde. Durch Privilegien für deutsche Bergarbeiter aus Transsilvanien und Handelsleute aus Dubrovnik (Ragusa), das als Hafen für Serbien eine wichtige Rolle spielte, kamen Angehörige anderer Völker nach Serbien.

Mit den sächsischen Bergleuten als eigentliche Initiatoren der Eröffnung der Bergwerke begann ein weitreichender ökonomischer und sozialer Wandel, der letztlich auch zur gesteigerten politischen Bedeutung Serbiens führte. Mit Brskovo (im Tal der Tara) beginnend, waren im späten 13. Jahrhundert zuerst fünf, später sieben Minen in Betrieb. Zu Mitte des 14. Jahrhunderts wurden um die größeren Lagerstätten weitere Minen eröffnet. Die bedeutendsten waren Rudnik (Šumadija), Trepča (bei Kosovska Mitrovica), Janjevo (bei Priština) und Novo Brdo (zwischen Priština und Gnjilane), letztere entwickelte sich zur größten Mine der Balkanhalbinsel. Wurden die Minen zuerst nur von den Sachsen betrieben, so lernten sie die lokalen Serben bald in ihrem Handwerk an, deren Arbeitsleistung für den Aufschwung des bergmännischen Handwerks erheblich wurde. Die Sachsen brachten nach Serbien nicht nur das Bergrecht und hochqualifizierte Juristen mit, sie etablierten insbesondere Formen des autonomen Stadt- und Siedlungsrechts. Ihre Siedlungen blieben katholische Oasen in denen sich schon bald dalmatinische Händler niederließen. Katholische Gemeinden existierten so inmitten des orthodoxen Staates; in einigen Fällen wurden in den größeren Bergwerken sogar zwei bis drei katholische Pfarrkirchen gegründet. Als Erbe der Sachsen wurde die deutsche Bergmannsterminologie und das Minenrecht überliefert, das auch in den späteren serbischen- und osmanischen Rechtskodifizierungen übernommen wurde.

Mit dem Bergbau waren weitere ökonomische Innovationen verbunden. Über die Besteuerung der Erze (10 %) und von geschmolzenen Metall (10 %) erwuchs den Herrschern eine bedeutende neue Einkommensform. Mit der Regierung von Stefan Uroš I. wurden auch die ersten Prägestätten für Silbermünzen (Dinar, Grossus) eingeführt. Waren während der byzantinischen Herrschaft unter den Komnenen auch den mächtigsten balkanischen Opponenten jegliche Münzprägung untersagt, so fiel das imperiale byzantinische Monopol nach 1204. Ein serbischer Hyperper (yperperi Sclavonie) wurde seit 1214 erwähnt, ohne dass sich Münzen überliefert hätten. Unter Radoslav wurden anschließend Kupfermünzen geprägt, die denen der epirischen Angeloi nach in Thessaloniki geprägten Münzen nachempfunden wurden. Mit Uroš I. imitierte man den venetianischen Silber-Grossus sowohl in Aussehen als auch in Größe und Gewicht. Die dem ehemals karolingischen Münzwesen verpflichtete lokale Geldnutzung wurde so unter Uroš I. an die etablierte Geldökonomie angepasst. Hauptmünzen mit weiter Verbreitung waren der Denarii grossi. Im Handel größerer Warenmengen waren auch die Goldmünzen, die in Venedig, Genua und Florenz geprägt wurden, im Umlauf. Serbischen Münzen nutzten sowohl westliche als auch östliche Bezeichnungen. Dinar und Grossus waren Bezeichnungen die aus dem Westen kamen, teils östlichen Ursprungs war der Hyperper, als Attribut der Goldmünzen des 11. Jahrhunderts. Später kam zusätzlich noch der Aspra auf, als im byzantinischen Bereich keine Goldmünzen mehr geprägt wurden.

Aus diesen ökonomischen Impulsen die der Bergbau bot, verband sich im serbischen Reich auch erstmals eine urbane Entwicklung. In der Gründung größerer Siedlungen, in denen sich Händler in weit höherem Ausmaß als vordem ansiedelten, wurde insbesondere im Aufblühen zentraler Handelsmärkte an der Küste eine für die Ökonomie der Balkanhalbinsel wesentliche Instanz etabliert. Die ehemals überwiegend vom Handwerk geprägten merkantilen mediterranen Städte bekamen durch Handel mit serbischen Erzen wichtige Impulse. Vormals bestand aus der Vergabe als Bischofssitz, die allen größeren dalmatinischen Küstenstädten zuteilwurde, eine Egalisierung in der Bedeutung. Mit dem Erzhandel konnten jedoch nur noch diejenigen Städte wetteifern, die über genügend Kapital und Bevölkerung verfügten, und damit die Voraussetzung besaßen, sich an den sich eröffnenden ökonomischen Perspektiven in den Minen in Bosnien und Serbien zu beteiligen. Nur die zwei im Süden gelegenen Orte Dubrovnik und Kotor verfügten über diese Qualitäten und übernahmen damit unter den dalmatinischen Küstenstädten das Primat im Erzhandel, sowie allgemein daraus folgend die zentralen Positionen im ökonomischen Kreislauf der Balkanhalbinsel. Während Kotor direkt dem serbischen König unterstand, war Dubrovnik unter venezianischer Herrschaft. Über 250 Jahre wurden daher zwischen Dubrovnik und den serbischen Königen neben Handelsabkommen, den Rechten freier Bewegung, auch Mechanismen zur Beilegung von Grenzdisputen vereinbart. Neben der verstärkten Migration von befähigten Individuen nach Kotor und Dubrovnik, entwickelten sich auch durch familiäre Bande zwischen beiden Städten enge Bindungen.

Der nächste wichtige Herrscher nach der kurzen Regierungsperiode von Uroš’ Sohn Dragutin (1276–1282) war dessen jüngerer Bruder Stefan Uroš II. Milutin (1282–1321), auch "Uroš der Mächtige" oder "Uroš der Heilige" genannt. Er setzte den wirtschaftlichen Ausbau seines Vaters und die Tradition der Kirchen- und Klostergründungen fort. Unter ihm stieg Serbien zur dominierenden Macht auf dem Balkan auf, unter anderem durch Gebietsgewinne in Makedonien. In Skopje gründete er jenen Hof, der für ihn und seine Nachfolger zum wichtigsten werden sollte. Milutin konnte als erster auch auf den Konsequenzen der ökonomischen Veränderung die sich durch den Erzabbau boten sichtbar Kapital schlagen; für die gesteigerten militärischen Fähigkeiten wurden erstmals auch katalanische Söldner angedient, in der exzeptionellen Bautätigkeit wurden die herausragendsten Bauträger und Freskenmaler sowohl aus Dalmatien wie Thessaloniki bestellt und dem Hofzeremoniell, das dem byzantinischen Herrscherhaus nacheiferte.

Nach anfänglichen Reibereien mit Byzanz schloss Uroš II. 1299 einen Friedensvertrag mit Kaiser Andronikos II. Palaiologos und heiratete dessen Tochter. Er übernahm das byzantinische Hofzeremoniell und sah sich angesichts des geschwächten Byzantinischen Reichs als der legitime Fortführer der byzantinischen Tradition.

Uroš’ Sohn Stefan Uroš III. Dečanski konnte sich in der kurzen Zeit, die er zwischen seinem Vater und seinem Sohn Dušan zum Zug kam, außenpolitisch bewähren. In der Schlacht bei Velbužd (heute Kjustendil) besiegte er die Bulgaren, die ab nun für längere Zeit Verbündete bleiben sollten.

Von seinem Vater war Stefan Dečanski in seiner Kindheit als Geisel zu den Tataren geschickt und später – als er 1314, vom Adel dazu aufgestachelt, sich gegen ihn erhob – geblendet und ins Exil geschickt worden. Von seinem Sohn wurde er 1331 eingesperrt und kurze Zeit später auf mysteriöse Weise ermordet. Das alles war mehr als genug, um ihn heiligzusprechen und als Märtyrer zu verehren.

Mit der Heiligsprechung der ersten Nemanjiden waren alle dynastischen Nachkommen bestrebt, die Beziehungen der Dynastie zur Kirche weiter zu festigen. Als Förderer kirchlicher Schenkungen standen sie daher der Gesellschaft des Staates vor. Aus dem Vorbild Stefan Nemanjas in der Neugründung des Klosters Studenica, dem ein romanischer Entwurf zugrunde lag, sowie der kulturhistorisch weitreichenden Erlaubnis des Byzantinischen Kaisers auf dem Territorium Byzanz das verfallene Kloster Hilandar als serbisches Kloster zu übernehmen, betätigten sich alle Nemanjiden-Herrscher als die primären kirchlichen Patrone. Auch die Nemanja folgenden Monarchen übernahmen, wenn auch vielfach vereinfacht, die Vorgaben im Bauschema Studenicas. Ihre einschiffigen basilikalen Bauwerke besaßen eine Vierungskuppel und waren in den kostspieligeren Stiftungen mit Kalk-, Sandstein oder Marmor verkleidet, die an Fenstern und Türen auch skulpturale Elemente erhielten. So errichtete Vladislav Mileševa, Uroš I. Sopoćani, Helene d'Anjou Gradac und Dragutin Arilje.

Unter Stefan Milutin wurde die Bautätigkeit stark ausgeweitet. Für den Neubau des Katholikons im Kloster Hilandar führten die Baumeister einem Entwurf aus, der sich formal direkt von konstantinopoler Kaiserstiftungen der Komnenen ableiten lässt. Im Bischofssitz der auf byzantinischen Bauplänen beruhenden Kreuzkuppelkirche Gračanica, die zuerst sogar als Herrscher-Mausoleum gedacht war, folgte jedoch auch auf dem Gebiet Serbiens selbst ein augenfälliger Bruch zur Bautradition. In der Verfeinerung des Bauplans, Sorgfalt architektonischer Konzeption und Integration der Baukörper übertraf sie selbst alle damaligen byzantinischen Vorbilder. Damit wurde der plastische Anspruch Milutins, in offener Konkurrenz zu seinem Schwiegervater Kaiser Andronikos II. als bedeutender Patron der Kunst und Architektur zu gelten, offensichtlich. Diese komplette Abkehr der Bau- und Familientradition der Vorgänger Milutins erzeugte auf kirchlicher Seite Widerstand und Milutins eigentlichem Mausoleum, Kloster Banjska, wie auch demjenigen Stefan Dečanskis, Visoki Dečani, wurden wieder romanische Entwürfe zugrunde gelegt.

Durch die Byzantinisierung der Lebensbereiche am Hof, dem Feudalwesen und in der völligen Einbindung in die byzantinische Kirchenhierarchie war die kulturelle Vorbildfunktion Byzanz jedoch soweit etabliert, dass unter Stefan Dušan auch die völlige Hinwendung zur byzantinischen Sphäre endgültig vollzogen wurde und auch die direkte Übernahme byzantinischer Architekturvorbilder, die über Thessaloniki und die Athosklöster importiert wurden, Maß aller Dinge wurde. Mit Dušans Gründung des Erzengelklosters erlangte gleichzeitig ein neues Modell weitere Vorbildfunktion aller nachfolgenden Stiftungen auf dem Territorium Serbiens.

Diese zahlreichen Klostergründungen waren nicht nur dem Andenken der Herrscher gedacht, für die die Mönche Gebete und Homilien zu halten hatten, die Klöster wurden Lehrstätten und Zentren der Kultur, die noch im 12. Jahrhundert die einzigen „urbanen“ Gesamtheiten in Serbien bildeten. Selbst für die regional ansässige Bevölkerung waren die Klöster noch lange Zeit die ihnen einzig bekannten Städte, da sich bis auf die Küstenorte, erst mit der Einverleibung vormaliger byzantinischer Gebiete alte städtische Strukturen in das territoriale Gebiet Serbiens integriert wurden. Noch im 12. Jahrhundert standen die Serben städtischen urbanen Lebensweisen ablehnend gegenüber. Dies resultierte auch darauf, dass innerhalb der serbischen Grenzen, die sich bis zum Ende des 12. Jahrhunderts auf die Einheit der ehemaligen römischen Provinz Dalmatia beschränkten, die auch zur römischen Epoche im Hinterland der Provinz nicht urbanisiert wurde. Die transversalen römischen Straßen verbanden über das Innere der Provinz nur die nächstgelegenen großen urbanen Zentren zwischen den Städten der Adriaküste, den Niederungen Pannoniens und den großen Legionslagern der Provinz Moesia an den longitudinalen Achsen der Via militaris im Einzugsgebiet von Morava und Timok.

Die Klöster waren auch zu aller vorderst Mittelpunkte agrarischer Güter, die von den zum Kloster gehörenden Ländereien versorgt wurden. Darüber hinaus war in den Regularien von Zar Dušans Zakonik bestimmt worden, das 1000 Haushalte für den Unterhalt von 50 Mönchen zuständig wären. Allein das Kloster Dečani besaß 2166 agrarische- und 266 der Herdentierhaltung verpflichteter Haushalte. Bei den Planung der Klosteranlagen waren die Ideen einer idealen städtischen Siedlungen jedoch schon von Anfang an vordergründig. Über die gesamte orthodoxe Welt verbreitet bildeten Klöster die Kerne späterer Städte (insbesondere in der Rus, Kiev und Moskau). Auch in Serbien wurden die Vorbilder athonitischer Lavren nachgebildet, in denen Studenica als erster dieser idealisierten verkleinerten Entwürfe, die Vorstellung eines Abbild Jerusalems oder Konstantinopels bilden sollte.

Unter Stefan Uroš IV. Dušan (1331–1355), dem mächtigsten aller serbischen Herrscher, erreichte das Serbische Reich den Höhepunkt seines politischen Einflusses und seiner Ausdehnung. Nicht nur durch Kriegsführung, sondern auch durch geschicktes Ausnutzen der politischen Machtverhältnisse gewann er weite Gebiete dazu, darunter fast ganz Albanien (mit Ausnahme der Stadt Durrës) und jene Teile Makedoniens, die sich noch nicht unter serbischer Herrschaft befanden (mit Ausnahme Thessalonikis). Sein Reich erstreckte sich schließlich von der Donau im Norden bis zum Golf von Korinth im Süden und von der Grenze zur unabhängigen Republik Dubrovnik im Westen bis kurz vor Sofia im Osten. Die Hauptstadt des damaligen Reiches war Skopje.

Zu Weihnachten 1345 ernannte Stefan Uroš IV. Dušan sich selbst zum "Kaiser Serbiens und des Römerreichs" ("Imperator Rasciae et Romaniae") und ließ sich zu Ostern 1346 krönen. Doch Kaiser konnten nur vom Patriarchen gekrönt werden. Da er mit Byzanz in Fehde lag, ließ er in einem Konzil serbischer und bulgarischer Kirchenmänner den Erzbischof von Peć, Joanikije, zum Patriarchen von Serbien erheben. Der Patriarch von Konstantinopel belegte darauf Dušan, den neuen Patriarchen und die neuen Kirchenfunktionäre mit dem Bann.

Dušans Reich wurde nach byzantinischem Muster unter Führung des serbischen Adels verwaltet. Die weitgehenden Rechte von Adel und Kirche wurden 1349 in einem umfassenden Rechtskodex, dem sogenannten Dušanov zakonik (Kodex des Stefan Dušan) festgelegt. Da der feudale Adel und die Staatsfunktionäre ihre Rechte aber immer wieder missbrauchten, mussten die Gesetze so modifiziert werden, dass sich schließlich für den serbischen und den griechischen Teil separate Verwaltungssysteme ergaben. Auch kulturell erlebte Serbien eine Hochblüte. So löste zum Beispiel die serbische Redaktion des kirchenslawischen das Griechische als Schriftsprache Südosteuropas ab und wurde auch zur Diplomaten- und Kanzleisprache im gesamten Balkanraum. Es blieb dies bis in das 16. Jahrhundert. Die klösterlichen Freskenmalereien werden zu den Höhepunkten europäischer Malerei des 13. und 14. Jahrhunderts gezählt.

Mit seinem neuen Titel als Herrscher von Romania, das heißt Ostrom, erhob Dušan offen Anspruch auf den Thron von Byzanz. Er geriet damit in Konflikt mit Johannes Kantakuzenos, dessen Ansprüche er 1342/1343 noch unterstützt hatte. Kantakuzenos rief die Osmanen gegen die Serben zu Hilfe. Damit waren die Weichen für das Eindringen der Osmanen auf den Balkan und den Niedergang des Großserbischen Reiches gestellt.

Dušan konnte seine Pläne nicht zu Ende führen, da er 1355 plötzlich und auf unbekannte Weise starb.

Sein Sohn und Nachfolger Stefan Uroš V. (1355–1371), genannt "der Schwache", konnte das Reich nicht zusammenhalten. Die Feudalherren wurden immer unabhängiger, manche spalteten sich – teils mit Hilfe äußerer Rivalen Serbiens – völlig ab, andere erkannten Uroš zwar nominell an, gebärdeten sich aber auf ihren Gebieten wie souveräne Herrscher, ließen Münzen prägen, nahmen Steuern ein etc. So hatte das Reich Anfang der 1360er Jahre große Gebiete im heutigen Albanien und Griechenland (Albanien, Epirus und Thessalien) verloren, in der Region Zeta (im Westen an der Adriaküste) hatte eine Familie Balšić die Macht, in Makedonien die Brüder Vukašin und Jovan Uglješa, bekannt als Mrnjavčevićs. Zeta und Makedonien befanden sich offiziell noch unter der Herrschaft Uroš’, der tatsächlich nur mehr Zentralserbien in seiner Hand hatte.

1365 bekam Vukašin Mrnjavčević den Königstitel und alle Rechte eines Mitregenten. Da Uroš V. kinderlos war, bedeutete das die Übergabe der Erbrechte und den Anfang vom Ende der Dynastie der Nemanjiden.

Die relativ kurze Zeit im 14. Jahrhundert, in der die serbischen Zaren einen großen Feudalstaat beherrschten, der sich von der Donau bis an die Küsten der Adria und der Ägäis ausdehnte, wurde im Nachhinein als "Großserbisches Reich" bezeichnet. Unter der jahrhundertelangen osmanische Herrschaft wurde das Serbische Reich zum Inbegriff eines serbischen Idealstaates, wobei gern vergessen wurde, dass das mittelalterliche Großserbien ein Vielvölkerreich war, das an die politische Tradition des byzantinischen Kaisertums anknüpfte, welches sich ebenso wie das im Westen als Universalmonarchie verstand.

Doch die Bedrohung durch die Osmanen verhinderte einen Machtkampf. 1371 besiegten die Osmanen die Serben in der Schlacht an der Marica (einem Fluss im heutigen Bulgarien). Diese Schlacht ist im Bewusstsein der Serben weniger präsent als die durch Legenden verklärte Schlacht auf dem Amselfeld ("Kosovo Polje"), war aber mindestens so entscheidend für das weitere Schicksal Serbiens. Die Mrnjavčevićs fielen in der Schlacht, Uroš V., der nicht teilgenommen hatte, starb unerwartet zwei Monate später.

Die Osmanen überließen zunächst das eroberte Territorium den lokalen Herrschern, von denen sich einige unterwarfen. So wurde auch Kraljević Marko, der Sohn von Vukašin Mrnjačević, zu einem Vasallen des osmanischen Sultans Murad I. Obwohl er für den Sultan kämpfte, wurde er eine der zentralen Figur der serbischen Folklore, die sich ihrem Beherrscher nur widerwillig unterordnet.

Das Territorium, das sich unter der Kontrolle der Osmanen befand, war unter verschiedenen Feudalherren aufgeteilt. Zeta hatte weiterhin die Familie Balšić. Teile Rasziens, des Kosovo und Nordmakedoniens gehörten Vuk Branković, Zentralserbien und Teile des Kosovo hatte Lazar Hrebeljanović in seiner Hand. Dieser Lazar wurde durch den Gewinn von Gebieten, die er gemeinsam mit dem bosnischen Ban Tvrtko I. dem Župan Nikola Altomanović abrang, bald zum mächtigsten unter den Feudalherren und sollte schließlich zu einer der wichtigsten Figuren werden.

Durch geschickte Heiratspolitik – er selbst heiratete eine Nachfahrin der Nemanjiden – gelang es ihm, seine Stellung zu festigen und Allianzen zu schließen. Dabei stand das Bestreben im Vordergrund, die ehemals serbischen Länder gegenüber dem drohenden Ansturm der Osmanen zu konsolidieren. Er war dabei vorsichtig genug, Ban Tvtrko dessen selbst verliehenen Titel "König der Serben und Bosniens" nicht abzusprechen, obwohl er sich selbst Herrscher aller Serben nannte.

Ein wichtiger Schritt zur Konsolidierung der bedrohten Gebiete war die Aufhebung des Kirchenbanns über die abgespaltene serbische Kirche, die Lazar 1375 erwirkte. Auch die Aufnahme von Flüchtlingen aus den osmanisch kontrollierten Gebieten – unter ihnen viele Vertreter der Kirche und der Intelligenz – bedeutete eine Stärkung seiner Position. Auf die Unterstützung der Kirche konnte er rechnen, da ihre Vertreter in ihm den großen Hoffnungsträger für eine Wiedervereinigung der serbisch-orthodoxen Gebiete sahen.

Unterdessen begannen die Osmanen, nachdem sie einige Zeit mit der Konsolidierung ihrer Herrschaft in den gewonnenen Gebieten Bulgariens und Makedoniens beschäftigt gewesen waren, Mitte der 1380er Jahre Serbien selbst anzugreifen. 1386 eroberten sie die wichtige Stadt und Feste Niš, doch schon 1387 gelang es den Serben, eine osmanische Heeresabteilung unter Lala Şahin bei Pločnik vernichtend zu schlagen. Es war abzusehen, dass es zu einer entscheidenden Schlacht kommen müsse.

Am 15. Juni 1389 (nach Julianischem Kalender am 28. Juni, dem St.-Veits-Tag oder Vidovdan), trafen Serben und Osmanen in der Schlacht auf dem Amselfeld ("Kosovo polje") aufeinander. Beide Seiten waren gut gerüstet: Sultan Murad I. führte seine Truppen selbst an und brachte seine beiden Söhne mit, Lazar hatte die Unterstützung von Vuk Branković und der Männer des bosnischen Königs Tvrtko I. Über die Schlacht selbst ist wenig bekannt, außer dass neben vielen anderen auch die beiden Anführer umkamen. Der Ausgang der Schlacht war zunächst unklar. In Europa verbreitete sich das Gerücht über eine Niederlage der Osmanen. Erst später, im Licht der darauf folgenden Ereignisse, gingen die Interpretationen in die Richtung einer Niederlage der serbischen Seite, da die Serben größere Verluste erlitten und im Gegensatz zu den Osmanen kaum mehr Ressourcen für weitere Auseinandersetzungen hatten.

Dies war auch Lazars Witwe Milica klar und deshalb blieb ihr nichts anderes übrig, als sich den Forderungen von Murads Sohn und Nachfolger Bayezid I. nach Unterwerfung zu beugen. Serbien wurde zum Vasallenstaat der Osmanen. Doch auch in diesem musste sich Lazars Clan erst gegen die anderen Feudalherren behaupten, zumal Lazars Sohn Stefan Lazarević 1389 noch zu jung war, um die Nachfolge anzutreten. Milica beauftragte gelehrte Mönche mit dem Verfassen einer Vita Lazars, die eine Heiligsprechung vorbereiten und begründen sollte. Ein wichtiger Grund dafür war, dass die Macht des Stefan Lazarević auf einer wesentlich sichereren Basis stand, wenn er einen Heiligen als Vorfahren vorweisen konnte. Damit begann die Mythenbildung um die Schlacht am Amselfeld und ihren Helden Lazar, die noch heute in den Konflikten um das Kosovo als fester Bestandteil des serbischen Selbstbewusstseins ihre Wirkung zeitigt.

Die baldige Heiligsprechung Lazars konnte seinem Sohn zwar die führende Position im Vasallenstaat sichern, dennoch musste Stefan Lazarević, als er die Volljährigkeit erreicht hatte, zunächst mit seinen Männern Sultan Bayezid militärische Unterstützung leisten. Er verhalf ihm 1396 zum Sieg gegen die Kreuzfahrer bei Nikopolis und war 1402 bei der Niederlage gegen die Mongolen in der Schlacht bei Ankara (Ankara) dabei. Der Sultan wurde dort gefangen genommen, Stefan jedoch nutzte die Chance, um der osmanischen Herrschaft zu entkommen.

Er ließ sich vom byzantinischen Kaiser Manuel II. den Titel Despot (im damaligen Byzanz der höchste Herrschertitel nach dem des Kaisers) verleihen und kehrte nach Serbien zurück, wo er seinem Land unter ungarischer Oberherrschaft zu einer letzten Blütezeit verhalf. Von den Ungarn bekam Stefan – vor allem im Donauraum und in Südungarn – Gebiete dazu, unter anderem auch Belgrad, das er zur neuen Hauptstadt ausbaute. Nach heftigen Bruderkämpfen, sowohl im serbischen als auch im osmanischen Herrscherhaus, schloss Stefan einen Friedensvertrag mit dem neuen Sultan Mehmed I., der es ihm ermöglichte, den Großteil der serbischen Länder noch einmal zu vereinigen.

Stefan Lazarević ist nicht nur als Ritter, sondern auch als humanistisch gebildeter Gelehrter und Dichter in die Geschichte eingegangen, der Serbien den Weg vom Feudalstaat, zu einem an den Idealen von Humanismus und Renaissance orientierten Staat gewiesen hat, in dem besonders die Städte und der Handel einen Aufschwung erlebten. Vor der in späterer Interpretation als bloße Fremdherrschaft und jahrhundertelange Unterdrückung empfundenen Osmanenzeit wurde der Glanz dieser Epoche wohl als besonders stark empfunden.

Trotz der negativen Vorzeichen durch einen doppelten Vasallenstatus gegenüber Ungarn und den Osmanen war das Zeitalter des Despotats Stefans durch ökonomische Prosperität auch eine kulturelle Blütezeit, die auch in der Emigration bedeutender Künstler und Gelehrter aus dem sogenannten byzantinischen Commonwealth nach Serbien geschuldet wurde, gekennzeichnet. Stefan selbst konnte seine Kapitale von Kruševac nach Belgrad verlegen und leitete eine fundamentale Erneuerung der auf antiken römisch-byzantinischen Fundamenten gebauten Stadt ein. In Belgrad entstand mit der Festung von Belgrad die neben Konstantinopel bedeutendste mittelalterliche Wehranlage der Balkanhalbinsel, die selbst den massiven Angriffen von 1456 als einziger umkämpfter Stützpunkt den Osmanen nicht vor dem 16. Jahrhundert in die Hände fallen sollte. Stefans Mausoleum in dem ab 1409 als exemplarisch für die damals hochentwickelte Wehrarchitektur Serbiens errichteten Wehrkloster Manasija war gleichzeitig das letzte Zentrum der mittelalterlichen serbischen Literatur, als deren Hauptvertreter der bulgarische Emigrant Konstantin Kostenecki eine als klassisch angesehene Vita des Despoten verfasste.

In der Architektur vollzogen sich bedeutende Veränderungen, die durch einen grundlegenden Typus einer Dreikonchenanlage zu einem vereinheitlichten byzantinischen Spätstil führten, dessen Besonderheiten im Design des Kirchenkörpers und der Fassadengestaltung wurden. Die Innovationen beruhten auf Vorbildern der Architektur im Zeitalter Zar Dušans sowie Thessalonikis und des Heiligen Berges Athos, wurden aber nicht mehr allein von den Angehörigen Herrscher-Dynastie gestiftet. Bauwerke der „Morava-Schule“ wurden von den höhergestellten feudalen Familien mit eigenen Stiftungen gegründet. Die architektonische Homogenisierung dieser spätbyzantinischen Kunst mit ihren polychromatischen Fassaden, dem mehrkuppeligen Aufriss mit hochstrebenden Tambouren und Kuppeln, übernahmen zudem auch Einflüsse des Westens.

Insbesondere erreichten aber die Freskenmaler ein Niveau, wie es seit fast zwei Jahrhunderten in der byzantinischen Kunst nicht mehr erreicht wurde. Die religiösen Darstellung der Fresken Manasijas sind durch die Pracht der höfischen Kultur am Hofe Stefans geprägt und spiegeln die Lebenswelt der Zeit in realistischen Darstellungen wider. In den ein Jahrzehnt später entstandenen Fresken im Kloster Kalenić erreichte die spirituelle Darstellung aus der Wirkung gedämpfter Farben und intimer Darstellung ihren Höhepunkt, die sich zu den Hauptwerken der europäischen Malerei zu Anfang des 15. Jahrhunderts stellt. Die Fresken Manasijas und Kalenićs sind zugleich die Höchstleistungen der Palaiologischen Renaissance analog der Tafelmalerei Rubljows, den Mosaiken der Chora-Kirche sowie den Fresken Mystras.

Die bedeutenden kulturellen Leistungen waren aber nur unter dem Gesichtspunkt einer florierenden Wirtschaft möglich. Diese basierte wie zuvor auf dem Silbererzhandel. Trat durch den osmanischen Erfolg 1371 ein empfindlicher Eingriff in die Abkommen über den Erzhandel im serbischen Königreich und den Handelshäusern Dubrovniks ein, was mit der Zersplitterung des serbischen Reiches in der Meidung der unsicheren Regionen den Handel einschränkte, so war durch den Krieg zwischen Venedigs gegen Genua auch der Gesamthandel im Mittelmeer betroffen. Eine Folge blieb der bis um 25 % gestiegene Silberpreis, welcher auch bis zum Abschluss der osmanischen Eroberungen verlieren würde. So kostete ein Pfund Silber (ca. 330 g) vorher 6 Dukaten und stieg danach auf 7,5 bis 8 Dukaten.

Der gestiegene Preis beeinflusste die ökonomischen Grundlagen der Metallproduktion und Silberminen wurden begehrte Anlageobjekte. Die Anzahl der neuen Silberminen in Serbien stieg dadurch und zu den schon existierenden Minen um die Lagerstätten des Kopaonik und des Einzugsgebietes der Drina wurden in Železnik (Kučevo), Rudište (Belgrad), sowie am Cer neue Minen eröffnet. In der geografischen Verteilung der Minen, die sich jetzt nicht mehr nur im Bereich Altserbiens und im Kosovo konzentrierten, entwickelte sich eine regional ausgewogene Ökonomie im Staatsgebiet. Nach Ausscheiden Kotors aus dem Wettbewerb übernahm Dubrovnik auch das alleinige Primat über den Erzhandel. Raguser Händler investierten in den Minen oder übernahmen die Schmelzen und organisierten die Produktion, was ihnen im Bergrecht zugestanden wurde. Der Import hochwertiger Güter nach Serbien wurde dadurch bewerkstelligt und für das Jahr 1422 wird ein geschätzter Warenimportwert von 130.000 Dukaten aus Dubrovnik veranschlagt. Einzelne große Handelshäuser in Dubrovnik hatten allein auch über den Zeitraum von 1426 bis 1432 3480 kg Silber nach Italien exportiert. Die Investition in den arbeits- und produktionsmittelintensiven Bergbau wurden zudem oft über Kreditvergabe ermöglicht, was zu einer Kreditkrise der serbischen Bevölkerung beitrug, aber insgesamt die Bedeutung des Kapitalmarkts für die Bergwerksökonomie verstärkte. Aufgrund der andauernden Kreditkrise wurde unter Đurađ Branković 1435 eine monetäre Reform notwendig, die den Tauschwert von 10 neuen zu 16 alten Dinaren regelte. Aufgrund der Verschuldung wurde der Dinar zudem im Gewicht abgewertet und fortan als Asper bezeichnet.

Mit dem Aufstieg des allgemeinen Handels kam es zur Gründung eines neuen Siedlungstyps: der "trg" (Marktplatz). Ein trg hatte zumeist das Privileg eine einmal jährlich stattfindenden Messe "panadjur" (aus dem byzantinischen Reich übernommen) abzuhalten. Viele spätere bedeutendere serbische Städte gingen aus einem trg hervor: unter anderem Zaslon (Šabac), Valjevo, Paraćin, Užice, Čačak – andere wurden erst durch die osmanischen Gründungen von Bazaaren zu städtischen urbanen Siedlungen transformiert. Mit dem Aufschwung des Metallhandels wurde auch die technischen Grundlagen der Metallurgie angespornt. Die Manufaktur von Bombarden und Glocken im Sandgussverfahren wurde vervollkommnet und die Arbeit serbischer Metallgießer ist in Moskau durch einen Hilandar-Mönch in der Einrichtung einer Uhr, die die Stunden anläutete, für 1404 belegt.

Nach seinem Tod 1427 folgte ihm sein Neffe Đurađ Branković. Obwohl er gleich zu Beginn seiner Regierungszeit einige Gebiete an Ungarn und das osmanische Reich verlor, konnte er immer noch die Kräfte eines Gebietes mobilisieren, das von Donau und Save bis zur Adria reichte. Nachdem er Belgrad an die Ungarn zurückgegeben hatte, ließ er an der Donau eine neue Hauptstadt bauen – Smederevo, das bald den Ruhm eines neuen Konstantinopel erlangte.

1438 begann Sultan Murad II., dessen Vasall Đurađ war, mit massiven Angriffen auf serbisches Territorium. Đurađ setzte sich zunächst relativ erfolgreich zur Wehr. Nach der Schlacht bei Warna am 10. November 1444, in der die Ungarn unter Wladyslaw I. und dem Feldherren Johann Hunyadi gegen die Osmanen unterlagen, und der dritten Schlacht auf dem Amselfeld am 17.–19. Oktober 1448, in der Johann Hunyadi erneut gegen die von Murad II. befehligten Osmanen unterlag, wurde Serbien zur Pufferzone und zum Mediator zwischen diesen beiden Parteien.

Auch den Angriffen Sultan Mehmeds II., des Eroberers von Konstantinopel, leistete Đurađ noch Widerstand. Doch als 1456 mit Đurađ und Johann Hunyadi zwei wichtige Anführer des Widerstands gegen die Osmanen ablebten, war die Lage Serbiens trüber als jemals zuvor. Đurađs Nachfolger Lazar, der einzige seiner vier Söhne, der nicht von den Osmanen geblendet worden war, verstarb bereits im Januar 1458. Damit war Serbien ohne Führung. Mit der Einnahme Smederevos 1459 wurde Serbien Teil des Osmanischen Reichs und hörte als Staat für mehrere Jahrhunderte auf zu existieren.

Die Osmanen eroberten Serbien unter dem Vorwand, dem Land Ordnung zu bringen, bis sich die politische Lage in Serbien stabilisiert hätte. Der Grund war, dass der damalige Gouverneur von Serbien, Mihailo Anđelović, gestürzt wurde, der wiederum der Bruder des osmanischen Großwesirs Mahmud-Pascha Anđelović war. Serbien wurde zum Sandschak Smederevo, das nach der osmanischen Einnahme von Belgrad 1521 zum "Sandschak Belgrad" umbenannt wurde. Die südlichen Gebiete wurden zum Sandschak Kosovo, die südwestlichen um das alte Raszien zum Sandschak Novi Pazar, das aber dem Vilâyet von Bosnien (1463 erobert) angeschlossen wurde, die Küstengebiete kamen zum Sandschak von Shkoder/Skutari. Damit wurden Fakten geschaffen, die kulturell und politisch bis heute fortwirken.

Trotzdem brach der serbische Widerstand gegen die Osmanen immer noch nicht ab. Dieser konzentrierte sich in Südungarn, der späteren Vojvodina, wo Matthias Corvinus eine Art Militärgrenze errichtete. Dorthin übersiedelten viele Serben, die unter ihren Despoten oder Herzögen Autonomie erlangten, dafür aber für Ungarn vorwiegend gegen die Osmanen kämpfen mussten.
Die serbischen Despoten wurden von den ungarischen Königen nominell als die "wahren" Herren Serbiens ausgerufen. Mit dem osmanischen Sieg über Ungarn bei Mohács 1526 war auch das Ende des serbischen Fürstentums in Ungarn entschieden.

Die nächsten Jahrhunderte herrschten die Osmanen über Serbien. In dieser Zeit wurden manche serbischen Christen zu Moslems, doch blieb das serbische Nationalgefühl unter anderem durch die serbisch-orthodoxen Klöster erhalten.

In mehreren Aufständen versuchten die Serben, sich von der osmanischen Herrschaft zu befreien, die sie als schweres Joch empfanden. Der erste große serbische Aufstand begann in der Vojvodina 1593, der aber bis 1607 blutig niedergeschlagen wurde. Als die Habsburger die Osmanen aus Ungarn vertrieben (siehe Türkenkriege), wagten die Serben um 1689 nochmals einen Aufstand. Unterstützt von serbischen Aufständischen drangen die Truppen der Habsburger bis nach Sarajevo in Bosnien und Skopje in Mazedonien vor. Als sich jedoch die Österreicher zurückziehen mussten, folgten ihnen auch viele Serben, insbesondere diejenigen, die sich am Aufstand beteiligt hatten und nun die Rache der Osmanen befürchteten, darunter auch der serbische Patriarch Arsenije III. Crnojević. Es kam zu großen Flüchtlingsbewegungen, die in die serbische Geschichte als "seobe" (Wanderungen) eingingen.

Die Serben siedelten sich hauptsächlich in den zumeist entvölkerten Gebieten Südungarns an, einige kamen bis nach Budapest, in dessen Nähe zwei damals bedeutende serbische Siedlungen gegründet wurden: Szentendre nördlich und Ráckeve südlich von Budapest. Andere folgten dem Ruf der russischen Zarin Katharina der Großen und siedelten in der heutigen Ukraine, wo es zwei serbische Provinzen gab: Nova Serbia (russ. Нова Сербія) und Slovjanoserbia (Слов’яаносербія).

Von 1718 bis 1739 war das serbische Gebiet nördlich der Save und westlich der Donau im Besitz des Hauses Österreich.

Es wurde 1718 von den Habsburgern erobert. Jedoch fiel es 1739 wieder unter osmanische Verwaltung.
Die einzigen beiden Herrscher waren General Odijer, der temporäre Administrator von 1718 bis 1720, und Karl Alexander. Er war der Gubernator von Belgrad und sogleich auch der restlichen Provinz.

In diese Zeit fallen erste Versuche, ein serbisches Nationalbewusstsein herauszubilden.

Nach der Eroberung Serbiens durch die Osmanen 1459 verschwand Serbien als eigenständiger Staat von der Landkarte.
Erst knappe 350 Jahre später gelang es den Serben unter Đorđe Petrović, genannt Karađorđe („Schwarzer Georg“), im Ersten Serbischen Aufstand (1804), weite Teile Serbiens zu befreien. Mit dem Aufstand reagierten serbische Revolutionäre auf das im Januar 1804 erfolgte osmanische Massaker an 72 serbischen Knezen (Dorfältesten). Im Laufe des Aufstandes wurden eine eigenständige serbische Regierung (Praviteljstvujušči sovjet serbski, deutsch etwa: regierender Rat der Serben) gebildet, ein serbischer Fürst gekrönt sowie ein Parlament (Skupština) und der Vorläufer der heutigen Universität Belgrad gegründet. Dieser Aufstand wurde jedoch 1813 von den Osmanen niedergeschlagen.

1815 brach der Zweite Serbische Aufstand unter Miloš Obrenović, der das Haus Obrenović begründete, aus.
1816 unterzeichneten die Osmanen einen Vertrag zur Stabilisierung der Beziehungen mit den Serben.
1817 gelang es ihm, Ali Pascha zu einem ungeschriebenen Abkommen zu zwingen, welches den Zweiten Serbischen Aufstand beendete. Im selben Jahr kam Karađorđe nach Serbien zurück, jedoch wurde er auf Obrenovićs Befehl hin ermordet.

Sultan Mahmud II. erkannte 1830 mit einer Urkunde Miloš Obrenović als obersten Knjas der Serben an. Im November 1833 wurden mit einer weiteren Urkunde die Autonomierechte des Fürstentums präzisiert.
1867 gelang es den Serben unter Fürst Mihailo Obrenović, die Osmanen in ihrem Fürstentum endgültig zu besiegen, womit Serbien de facto unabhängig wurde (de jure erst 1878). 1869 erhielt das Land eine liberale Verfassung. In der Folge bildeten sich die ersten Parteien, die jedoch der autoritären, durch Polizeimaßnahmen und willkürliche Ministerwechsel charakterisierten Regierung des Fürsten Milan IV. (ab 1882 als Milan I. König von Serbien) wenig entgegenzusetzen hatten.

Anfangs war das Fürstentum relativ klein; das Gebiet beschränkte sich auf das Paschaluk Belgrad, welches aber in den Jahren 1831 bis 1833 im Osten, Süden und Westen erweitert wurde. Durch den Berliner Kongress 1878 wurde die Unabhängigkeit des Fürstentums Serbien anerkannt. Zudem erhielt Serbien Gebiete im Süden (um Vranje, heute bekannt als Pčinjski Okrug) zugesprochen.

Der Balkan bildete mit der Skandinavischen Halbinsel die letzte Region Europas, die durch den Bau von Eisenbahnen erschlossen wurde. Beide Halbinseln lagen im 19. Jahrhundert an den wirtschaftlichen Grenzräumen Europas, während der Eisenbahnbau in Skandinavien jedoch methodisch und friedlich und nach ökonomischen Gesichtspunkten der Region vorangetrieben wurde, so verblieb die Balkanhalbinsel ein Pfand imperialer Entwürfe und nationaler Ambitionen. Die Ungarischen Eisenbahnen erreichten nach Mitte des 19. Jahrhunderts in Galizien, Transsylvanien und Kroatien die Außengrenzen des k.u.k. Imperiums. Darüber hinaus lagen Serbien, Rumänien und das Osmanische Reich.

Bis 1860 war noch kein Kilometer Schiene südlich Save und Donau gebaut worden. Erst in diesem Jahr begann mit einer Stichbahn in der Dobrudscha und sechs Jahre darauf in Bulgarien der Bau von Schienenwegen im osmanischen Gebiet.

1855 hatte der Sultan die Konstruktion eines Schienenweges von Konstantinopel über Sofia nach Belgrad vorgeschlagen. Es dauerte 14 Jahre, bis dieser Vorschlag von Baron Hirsch, einem bayrischen Finanzier, aufgegriffen wurde. Er plante eine Verbindung zwischen Konstantinopel und dem österreichischen Eisenbahnnetz. Die Trassierung wurde unter Umgehung Serbiens geplant. Von Sofia aus sollte die Strecke westlich über Niš nach Priština und von dort über Novi Pazar und Banja Luka verlaufen. Ebenso sollten Sarajewo und die Save angebunden werden. Die Kontrolle der Eisenbahn wäre damit gänzlich von den Serbien benachbarten Großmächten ausgeübt worden, die diese Trassierung unter wirtschaftlichen und militärischen Gesichtspunkten vorantrieben. Als Gesellschaft zum Betrieb der Bahn gründete Baron Hirsch die Compagnie des Chemins de fer Orientaux, im deutschen Sprachraum auch als Orientbahn bezeichnet. Die Abschnitte zwischen Konstantinopel und Belovo, etwa 80 km östlich von Sofia sowie die sogenannte Sandschakbahn zwischen Banja Luka und Dobrljin waren bis 1874 fertiggestellt. Die schwierigen Geländeverhältnisse und vor allem der Aufstand in der Herzegowina 1875 sowie nachfolgend der Montenegrinisch-Osmanische und der Serbisch-Osmanische Krieg verzögerten und unterbrachen den Weiterbau. Im Ergebnis des Krieges und des Berliner Kongresses wurde Serbien um Gebiete vergrößert, die eigentlich auf der geplanten Trasse von Baron Hirschs Orientbahn lagen und Österreich-Ungarn besetzte Bosnien und Herzegowina. Der Orientbahn verblieben nur mehr die bereits fertigen Abschnitte außerhalb Serbiens und Bosniens.

Diesmal übernahm Österreich das Patronat der Eisenbahntrassierung und man wählte die Route über die Morava zwischen Niš und Belgrad, womit Serbien ins internationale Eisenbahnsystem integriert wurde. Der serbische Abschnitt wurde von einer französischen Privatgesellschaft erbaut, die nach diversen Finanzproblemen 1889 durch die neugegründete Serbische Staatsbahn (ŽS) abgelöst wurde. 1888 nahm der erste Orient-Express zwischen Wien und Konstantinopel seine Fahrt auf.

Dennoch wurde Baron Hirschs Route durch den Sandschak Novi Pazar nicht vergessen. Unter den frühesten osmanischen Projekten befand sich eine dem Tal des Vardar folgende Route von Thessaloniki nach Skopje. Anfang der 1870er Jahre vollendet, wurde sie bis Kosovska Mitrovica in den Kosovo weitergeführt. Gleichzeitig wurde die nördliche Trasse zwischen der ungarischen Grenze nach Banja Luka und Sarajewo vollendet. Trotz großer topographischer Hindernisse konnte diese bis zur nördlichen Grenze zum Sandschak als Schmalspurbahn ausgeführt werden. Nur noch 160 km trennten nun den Endpunkt der Bosnischen Ostbahn bei Uvac und den Terminus der Thessaloniki-Trasse bei Kosovska Mitrovica. Nördlich dieser noch ausstehenden Verbindungstrasse lag Serbien, südlich Montenegro. Diese verfolgten ein konkurrierendes Vorhaben bei der für die Serbischen Eisenbahnen eine Adriabahn strategische Priorität hatte, die über die Südost-Dinariden die Anbindung Serbiens an die Adria und Montenegro zum Ziel hatte. Diese konkurrierenden Eisenbahntrassierungs-Vorhaben bildeten eine der wesentlichen diplomatischen Verstrickungen zwischen Österreich-Ungarn und dem Osmanischen Reich einerseits und Serbien und Montenegro andererseits in der ersten Dekade des 20. Jahrhunderts. Die geplante Verbindung der osmanischen und bosnischen Bahnen wurde jedoch aufgrund der Balkankriege nie vollendet und der Sandschak wurde 1912 zwischen Serbien und Montenegro aufgeteilt.

Alles in allem waren für den Bau der Eisenbahntrassen auf der Balkanhalbinsel nie ökonomische Gesichtspunkte vordergründig, dagegen wurden aber immer strategische Erwägungen zu ihrer Einrichtung offen vorgetragen. Österreich-Ungarn versuchte bis 1914, Serbien davon abzuhalten, eine Adriaverbindung einzurichten. Gleichzeitig wurden die eigenen Trassen zwischen der Donauebene und Mittelmeer ausgebaut. Auch Dubrovnik und die Bucht von Kotor wurden von den Österreichischen Eisenbahnen erreicht. Eine Verbindung von Belgrad mit Kotor blieb in den 1890er Jahren der eigentliche Wunschtraum der Serbischen Eisenbahnen.

Insgesamt wird der Eisenbahnbau auf der Balkanhalbinsel im späten 19. Jahrhundert als ein Beispiel des wirtschaftlichen Imperialismus des Deutschen Reiches und seines Verbündeten Österreich-Ungarn betrachtet. Deutsches Kapital finanzierte zum Großteil die Orientbahn, und es bestand eine Vereinbarung zwischen Deutschland und dem Osmanischen Reich von 1903, die im Rahmen eines politischen Projektes eine Weiterführung nach Kleinasien als sogenannte Bagdadbahn vorsah. Mit ihrem Bau wurde zuerst an eine Neutralisierung Serbiens als Verbündeter Russlands und eine enge Anbindung der Türkei an Deutschland gedacht.

Nach dem Berliner Kongress wurde am 6. März 1882 das Königreich Serbien, mit König Milan I. proklamiert. Das Königreich war der Nachfolgestaat des Fürstentums Serbien.

1908 wurde Bosnien-Herzegowina von Österreich-Ungarn annektiert. Dies führte zu einem ernsten und dramatischen europäischen Konflikt, die sog. Bosnische Annexionskrise: Protest seitens des Osmanischen Reiches; Entrüstung in Serbien, das seine nationalen Pläne durchkreuzt sah und mit einer Mobilmachung antwortete; Russland stieß in der sogenannten Meerengenfrage (Öffnung des Bosporus und der Dardanellen) auf britischen Widerstand, sah sich von Österreich-Ungarn ebenfalls überspielt und stellte sich hinter Serbien; Großbritannien bestärkte nun Russland und forderte eine internationale Konferenz zur Klärung der bosnischen Frage, die aber von Österreich-Ungarn abgelehnt wurde; Italien sprach sich gegen eine Machterweiterung Österreich-Ungarns und zur Erhaltung des Status quo am Balkan aus; Frankreich hielt sich zurück, da es einer militärischen Kraftprobe noch nicht gewachsen zu sein glaubte; und das Deutsche Reich hielt fest zu Österreich-Ungarn, lehnte jedoch die Präventivkriegsabsichten des österreichischen Generalstabs zur sogenannten "Abrechnung mit Serbien" ab. Das Deutsche Reich warnte Russland vor Unterstützung Serbiens in der als Demütigung empfundenen „Petersburger Note“ und zwang es, auf Serbien einzuwirken, die Annexion anzuerkennen. Russland beabsichtigte jedoch, Serbien gegen jede zukünftige Drohung seiner Unabhängigkeit zu verteidigen.

Die Bosnienkrise löste die Mazedonienkrise ab. Schon im 19. Jahrhundert wurde Mazedonien zum bulgarisch-griechisch-serbischen Streitobjekt. Bulgarische, griechische und serbische Freischärler, die Komitadschi, Klephten und Tschetniks kämpften um Einfluss.

1912 vermittelte Russland ein Balkanbündnis zwischen Bulgarien, Griechenland und Serbien, dem sich auch Montenegro anschloss. Es kam im Oktober 1912 zum Ersten Balkankrieg. Mazedonien sollte laut einem nicht offiziellen Einverständnis zwischen Bulgarien und Serbien aufgeteilt werden. Bulgarien sollte den größeren Teil Mazedoniens bekommen, Serbien den Nordwesten sowie einen Zugang zum Meer durch Nordalbanien, das durch eine Unterstützung Bulgariens gedeckt werden sollte.

Die Offensive der Balkanstaaten begann am 17. Oktober 1912. Die serbische und die griechische Armee marschierte in Mazedonien ein, die bulgarische Armee in Thrakien. Die montenegrinischen Truppen spielten kaum eine Rolle. Die griechische Armee eroberte Saloniki, nachdem serbische Truppen die osmanischen in den Schlachten von Kumanovo (24. Oktober) und Monastir (5. November) besiegt hatten. Die bulgarische Armee siegte bei Kirk Kilissa und Lüle Burgas. Im November belagerte sie bereits Konstantinopel.

Eine Friedenskonferenz in London über den Jahreswechsel 1912/13 brachte keine Ergebnisse, da die Großmächte keinen Kompromiss zwischen ihren unterschiedlichen Interessen in der Region finden konnten. Am 23. Januar 1913 putschten sich die Jungtürken unter Enver Bey in Konstantinopel an die Macht und schlossen einen Waffenstillstand. Trotzdem hielten die Belagerungen an, bis auch İşkodra, Janina und Adrianopel kapitulierten.

Im Streit um die Grenzziehung in Mazedonien griff Bulgarien, das seine Kräfte überschätzte, im Juni 1913 Serbien an, um ein "fait accompli" zu schaffen; der Zweite Balkankrieg begann. Bei Bregalnica in Mazedonien wurde die bulgarische Armee geschlagen. Daraufhin erklärten Griechenland und Montenegro Bulgarien den Krieg, dem sich am 15. Juli 1913 auch Rumänien wie auch das Osmanische Reich anschlossen. Das führte zum endgültigen militärischen Zusammenbruch Bulgariens. Österreich-Ungarn drohte zugunsten Bulgariens einzugreifen, wurde aber von seinen Verbündeten Deutschland und Italien zurückgehalten. Im Frieden von Bukarest vom 10. August 1913 kam das heutige Mazedonien zu Serbien.

Aus den Balkankriegen ging Serbien als politisch gestärkte Macht hervor. Dies führte zu Spannungen mit der benachbarten europäischen Großmacht Österreich-Ungarn, das bereits 1908 Bosnien und Herzegowina annektiert hatte. Durch die Erfolge Serbiens und der Verbündeten, die das Osmanische Reich bis an den Rand Europas verdrängt hatten, befürchtete man in Wien, dass als nächstes das Habsburgerreich mit einem Aufflammen nationaler Bestrebungen seiner slawischen Minderheiten zu rechnen habe.
Österreich-Ungarn hatte für den Sommer 1914 ein Großmanöver in Bosnien und Herzegowina anberaumt, das durch den österreichischen Thronfolger Franz Ferdinand abgenommen werden sollte. Nach Meinung des britischen Militärattachés in der britischen Botschaft in Wien sollte dieses Serbien als eine Lektion dienen. Das Manöver wurde mit einer vollständig gerüsteten Armee, als ob ein Krieg mit Serbien anstehe, bei dem ein Angriff Montenegros am rechten Flügel abgewehrt werden müsse, durchgeführt.

Am Vidovdan, dem Nationalfeiertag Serbiens am 28. Juni 1914, wurde durch den jungen serbischen Studenten Gavrilo Princip ein erfolgreiches Attentat auf Franz Ferdinand verübt. Princip war ein Mitglied der bosnisch-serbischen revolutionären Bewegung Mlada Bosna, von der schon während der Balkankriege bis zu 20.000 Freiwillige gekämpft hatten, davon jedoch nur ein geringer Teil aus der Habsburgermonarchie. Eine Fraktion dieser Bewegung plante nach Informationen der österreichischen Gendarmerie einen Aufstand innerhalb der Grenzen des Habsburgerreiches zur Unterstützung Serbiens, falls es zum Krieg kommen sollte. Die seit den Balkankriegen entlassenen Freiwilligen füllten gleichzeitig in voller Kampfausstattung die Cafehäuser Belgrads und warteten auf eine Konfrontation mit Österreich-Ungarn.

In der darauf ausbrechenden Julikrise versicherte das Deutsche Reich am 6. Juli die unbedingte Bündnistreue zu Österreich-Ungarn (Blankovollmacht) im Fall eines Krieges mit Serbien und Russland. Am 23. Juli stellte Österreich-Ungarn an Serbien ein Ultimatum, das bis zum Abend des 25. Juli angenommen werden müsse, in dem u. a. die Bekämpfung der gegen Österreich-Ungarn gerichteten Aktivitäten unter österreichisch-ungarischer Beteiligung und Bestrafung der Schuldigen gefordert wurde. Das scharf formulierte Ultimatum war derart gestaltet, das selbst die deutsche Regierung nicht davon ausging, dass dieses für irgendeine Regierung annehmbar sei. Russland erklärte auf Seiten Serbiens zu stehen.

Die serbische Regierung akzeptierte den größten Teil des Ultimatums, verwahrte sich jedoch gegen die Bedingung der Einschränkung der Souveränität Serbiens in Punkt 6 und beschloss die Teilmobilmachung der Armee. Woraufhin Österreich-Ungarn die Antwort Serbiens für „unbefriedigend“ befand und ebenfalls mit einer Teilmobilmachung begann.

Trotz deutscher und britischer Vermittlungsversuche (Vorschlag einer Botschafterkonferenz und direkte Verhandlungen zwischen Russland und Österreich-Ungarn), erklärte Österreich-Ungarn Serbien am 28. Juli den Krieg. Russlands Mobilmachung zur Unterstützung Serbiens löste eine Reihe von gegenseitigen Ultimaten und Kriegserklärungen der europäischen Großmächte in der ersten Augustwoche aus. Mit dem deutschen Einmarsch in das neutrale Belgien am 5. August war der Weltkrieg unumkehrbar ausgebrochen.
Montenegro wurde öffentlich am 24. Juli von der Nachricht des Ultimatums unterrichtet. König Nikola versicherte Belgrad daraufhin, dass er Serbien beistehe.

Der Krieg wurde in Serbien von der Bevölkerung mit der Losung eines "heiligen Krieges für das Serbentum und Jugoslawentum" begrüßt. Serbien konnte bei einer Bevölkerung von 4.500.000 Einwohnern eine große Armee von 707.000 Soldaten aufstellen. Auf das Schlachtfeld zogen vorerst aber nur 250.000 Soldaten: Montenegro hatte zuerst 38.000, dann 45.000 Soldaten mobilisiert.

Um einen religiösen Konflikt im Krieg zu vermeiden, indem das katholische Habsburgerreich dem orthodoxen Serbenstaat gegenüberstand, wurde insbesondere in Bosnien und Herzegowina, wo sich die antiorthodoxe Stimmung der Katholiken und Muslime seit dem Attentat immer stärker bemerkbar machte, der Ausnahmezustand ausgerufen. Die Losung „Serbien muss sterbien“ fand sich dennoch als Flugblatt auf allen Tischen der Cafés in Sarajevo. In den an Serbien angrenzenden Gebieten des Habsburgerreiches wurde ein „Schutzkorps“ mit 11.000 Kämpfern organisiert, für Strafmaßnahmen gegen die serbische Bevölkerung. Viele Zivilisten der eigenen serbischen Bevölkerung wurden von der k.u.k. Armee ohne Gerichtsverfahren hingerichtet. Am 17. August 1914 kam es im serbischen Städtchen Šabac zu einem Massaker an den Bewohnern. Massenhinrichtungen gab es in den ersten Kriegstagen in zahlreichen nordserbischen Orten, die planmäßig und auf höheren Befehl hin erfolgten. Ermordungen von Zivilisten und verhafteten Serben wurden auch in Čelibiči an der Drina verübt. In Gefangenenlagern wurden alleine ins Lager Arad ab August 45.000 Serben der Habsburgermonarchie verbracht.

Die drei Armeen, die Österreich-Ungarn gegen Serbien aufstellte, umfassten die 5. und 6. Armee in Bosnien, sowie Teile der 2. Armee in Syrmien. Das Serbische Heer war in drei Armeen gegliedert, zusätzlich dazu noch der Užice Korpus, sowie die Verteidigung Belgrads. Den Oberbefehl übernahm der Thronfolger Prinz Alexander I., der Leiter des Stabes war Radomir Putnik, sein Stellvertreter Živojin Mišić. Die Armee Montenegros sollte formell nach einem abgesprochenen gemeinsamen Plan operieren, den Putnik entworfen hatte.

In drei Offensiven der österreichisch-ungarischen Armee gegen Serbien im August, September und November/Dezember 1914 versuchte das Habsburgerreich das an Soldaten und Ressourcen weit schwächere Serbien niederzuwerfen.
Der überraschende serbische Erfolg in der Schlacht von Cer vom 16. bis 24. August bildete den ersten Sieg der Alliierten und offenbarte die operativen Probleme der österreichischen Armee, die gleichzeitig zum Balkanfeldzug der Gefahr, die aus der Mobilisierung der russischen Armee an der Ostfront erwachsen war, gegenüberstand. So waren die 5. und 6. k.u.k. Armee zwar nominell der vereinten serbischen Armee mit den Teilstreitkräften der 1., 2. und 3. Armee deutlich überlegen, jedoch führte die k.u.k.-Armee die Operation fast ausschließlich über die unwegsamen Gebirgsregionen Bosniens durch. Diesem Vorgehen stand jedoch keine adäquate Transportinfrastruktur mit ausreichenden Versorgungstrassen zur Verfügung, da beispielsweise die Bosnische Ostbahn ein Torso geblieben war und keine große operative Bedeutung in einem Großkrieg besaß.
Für die Besetzung der serbischen Gebiete hatte der Befehlshaber der Balkanarmee Oskar Potiorek im Nachgang der Armeen spezielle Einheiten aufgestellt, die die serbischen Gebiete zu verwalten hatten. Dabei sollten Polizeistationen in allen Ebenen von muslimischen Bosniern geleitet werden. Ein weiterer Plan Potioreks sah vor, nach Aussiedlung der Serben im Unterlauf der Drina (Podrinje) auch hier Bosnische Muslime an deren Stelle anzusiedeln.

Nachdem der Feldzug vorerst gescheitert war, musste Potiorek beim Generalstabschef Franz Conrad von Hötzendorf mehrmals die Erlaubnis erbitten, auch einen Teil der 2. Armee nutzen zu dürfen. Diese sollte hauptsächlich an der Ostfront in Galizien zum Einsatz kommen und stand daher Potiorek nicht im vollem Umfang zur Verfügung. Schließlich konnte Potiorek nur auf persönliche Einflussnahme von Kaiser Franz Joseph I. einen Teil der 2. Armee für seine Balkanarmee abzweigen.

Nachdem die erste österreichisch-ungarische Offensive abgewehrt worden war, unternahm die serbische Armee ihrerseits unter Druck der verbündeten Alliierten zur Entlastung der weiteren Fronten eine Gegenoffensive. So wurde die 1. serbische Armee nach Syrmien beordert und mit vereinten Kräften der montenegrinischen Armee war man nach Bosnien eingedrungen, was Potiorek ab dem 6. September veranlasste wieder die Initiative zu ergreifen. Der zweite Feldzug an der Balkanfront wurde in der Schlacht an der Drina am Mačkov kamen im Jagodnja-Gebirge entschieden und endete für beide Seiten mit hohen Verlusten.
Nachdem die zweite Offensive in einen Stellungskrieg gemündet war, versuchte Potiorek in der bedeutendsten Offensive der Balkanfront, der Schlacht an der Kolubara, Serbien endgültig zu unterwerfen. Dabei brachte der Vorstoß der 5. und 6. Armee die serbische Armee an den Rand der Niederlage. Als Živojin Mišić die Leitung der 1. Armee vom verwundeten Petar Bojović übernommen hatte, nahm er unter Missachtung eines ausdrücklichen Befehls Putniks eine operative Rücknahme zur Auffrischung der Reserven und Munition vor. Währenddessen schickte sich Potioreks 5. Armee an Belgrad einzunehmen, welches durch das Zurücknehmen und Verkürzung der Front dem Angreifer überlassen wurde. In einer von Mišić nun für Potiorek völlig überraschenden Offensive entsetzte er die am Rand der Niederlage stehende serbische Armee. Er führte die 1. Serbische Armee gegen die überlegene 6. Armee Österreich-Ungarns, die dessen ungeachtet auf das linksseitige Ufer der Kolubara zurückgenommen werden musste. Potiorek versuchte den unkoordinierten Rückzug der 6. Armee noch aufzuhalten, indem er die 5. Armee eilig aus Belgrad zurückholte, jedoch war es dafür schon zu spät, da diese von der 2. und 3. Serbischen Armee geschlagen worden war. Damit war der Krieg in Serbien vorerst entschieden. Mit dem Sieg an der Kolubara hatte sich Serbien auch ein stärkeres Mitspracherecht bei den Kriegszielen der Alliierten erkämpft.

Diese Kriegsziele wurden in einigen Schlüsselelementen hervorgehoben. Zuerst von Alexander I. am 4. August 1914, dann in der Zirkularnotiz vom 4. September und schließlich in der Nišer-Deklaration des Serbischen Parlamentes vom 7. Dezember 1914. Das Ziel war in einem historischen Ideal in der Vereinigung mit den „Brüdervölkern“ – Serben, Kroaten und Slowenen definiert worden.
Ein alternatives Ziel einer einfachen Vereinigung der štokavisch sprechenden Bevölkerungsteile zu einem Großserbien wurde dagegen in keinem Programm als klares Ziel formuliert. Nur in Kreisen der Schwarzen Hand fanden sich Artikel, die ein solches Projekt befürwortet hätten. Ein genereller Plan für ein Großserbien bestand damit nicht, jedoch wurden Elemente eines solchen von Vertretern ausländischer Regierungen in den Raum gestellt. Edward Grey übermittelte am 3. Mai 1915 der serbischen Regierung die Note, dass der Sieg der Entente Serbien zumindest um Bosnien und Herzegowina, sowie in einem breiten Zugang zur Adria in Dalmatien vergrößern würde. Zur Föderation mit Kroatien sollten jedoch die Kroaten selbst die Entscheidung treffen. Letztlich wurde Serbien aber nur zugesichert, dass es einen Adriazugang bekommen sollte, ein Großserbien sahen die verbündeten Entente-Mächte nicht vor. Für die föderative Verbindung Serbiens und Kroatiens wurde mit Frano Supilo, dem Bildhauer Ivan Meštrović und Ante Trumbić ein sogenannter "Jugoslawischer Ausschuss" (Jugoslovenski odbor) gegründet. Dieser erwuchs aus einer Gruppe Wissenschaftler, die die Serbische Regierung am 29. August zusammenberufen hatte (Ljuba Jovanović, Aleksandar Belić, Jovan Cvijić, Nikola Stojanović, Mirko Laras und Slobodan Jovanović). Nikola Pašić formulierte am 27. Oktober die Leitlinien des Jugoslawischen Ausschusses, die ein zukünftiges Jugoslawien oder ein Serbisch-Kroatischen Staat als Ziele vorgaben. Von Außen wirkten die verbündeten Entente-Mächte daraufhin, das Serbien gegenüber Italien und Bulgarien territoriale Zugeständnisse vornehmen sollte, um diese auf die Seite der Entente zu ziehen.

Den bei Kriegsbeginn rund 460.000 Mann zählenden k.u.k.-Balkanstreitkräften wurden so insgesamt Verluste von mehr als 200.000 Mann zugefügt (rund 30.000 Tote und über 170.000 Verwundete). 70.000 weitere Soldaten gerieten in serbische Kriegsgefangenschaft.
Potiorek, der sich auf dem serbischen Kriegsschauplatz den Weisungen Conrads widersetzt hatte und den Feldzug in Serbiens unabhängig von Conrad in Sarajewo plante, wurde die ganze Schuld am Debakel zugesprochen. Er bat nach der Niederlage um seine Entlassung. Nach seiner Enthebung und Pensionierung gab er an den Nachfolger der Führung der Balkanarmee den Ratschlag weiter, der ein Jahr später auch umgesetzt werden sollte:

Der Weg über Donau und Save nach Serbien vorzudringen, wurde für den Feldzug 1915 im AOK unter Erich von Falkenhayn und Franz Conrad von Hötzendorf im Operationsplan für den Herbst schließlich auch umgesetzt. Zuerst wurde aber Bulgarien als Verbündeter für den neuen Angriff auf Serbien gewonnen.

Besonders die ersten österreichisch-ungarischen Offensiven waren von schweren Übergriffen gegen die serbische Zivilbevölkerung begleitet. Außerdem waren im Land Seuchen ausgebrochen, die ab 1915 die Zivilbevölkerung und Armee dezimierten. Dem Heer fehlte es an fast allem, der Abwehrkampf hatte viele Ressourcen und Kriegsmaterial verschlissen. Man verwendete sogar die Uniformen gefallener Gegner. Dabei stand fast jeder, der eine Waffe halten konnte, an der Front, selbst Frauen wurden Soldaten.

Die Ententemächte verlangten von Serbien eine Entlastungsoffensive gegen Bosnien, um Russland Luft zu verschaffen und die zweite Front gegen Österreich-Ungarn zu verstärken. Die Serben besetzten stattdessen im Juli 1915 das praktisch in Anarchie zurückgefallene Albanien, angeblich um italienischen Ambitionen zuvorzukommen, aber auch um einen eigenen Seezugang zu erhalten. Großbritannien und Frankreich schickten Waffen und Versorgungsgüter, um die serbische Armee für eine kommende Offensive zu stärken.

Am 7. September unterzeichnete Bulgarien den Vertrag des Bündnisses mit den Mittelmächten (Österreich-Ungarn und das Deutsche Reich). Am 6. Oktober befahl das deutsche Oberkommando unter Leitung von Feldmarschall August von Mackensen eine gemeinsame Offensive Deutschlands, Österreich-Ungarns und Bulgariens. Der Hauptschlag erfolgte diesmal über die Donau und Save bei Belgrad und Smederevo.

Die Heeresgruppe Mackensen und die bulgarische 2. Armee zählten zusammen 350 Bataillone und 1400 Geschütze, die serbische Armee konnten 275 Bataillone und 654 Kanonen entgegensetzen. An der Save-Donaufront und an der Drina standen 143 Bataillone und 362 Geschütze der Serben 202 Bataillonen und 990 Geschützen der verbündeten Mittelmächte gegenüber. Die Verteidigung von Belgrad wurde von Mihailo Živković befehligt, für die 20 Bataillone und 75 Kanonen gegen 66 Bataillone und 273 Geschütze (darunter 108 schwere) der Armee Kövess standen.

Mittels Durchbruch der bulgarischen Arme in der Morava-Vardar-Furche, wurde Serbien von den französischen und britischen Expeditionsheer in Thessaloniki abgeschnitten und drohte völlig eingeschlossen zu werden. Keine der mit Serbien verbündeten Entente-Mächte hatte massiv eingegriffen. Die serbischen Armeen mussten trotz heftigen Widerstandes den Rückzug antreten. nur durch komplette Evakuierung von Regierung und verbliebener Armee in Richtung Südwesten entgingen die serbischen Streitkräfte der Einkreisung und völligen Vernichtung. Die sich im Kosovo versammelnden Reste zählten nur noch 300.000 Soldaten, nicht einmal ein Drittel der Gesamtstärke vor Beginn der gegnerischen Offensive, aber beschwert um zahllose Flüchtlinge. Ohne Versorgung und Ruhemöglichkeit zogen sich die Serben durch unwegsames Bergland in winterlichem Wetter unter schwierigsten Bedingungen bis an das Ionisches Meer zurück. Dabei starben durch Hunger, Seuchen, feindliche Angriffe und die Übergriffe albanischer Partisanen viele der Fliehenden, Zivilisten wie Soldaten.

Ein umfassender Operationsplan zur Einnahme Montenegros durch die Armee Österreich-Ungarns wurde Parallel zur Offensive auf Serbien durchgeführt. Die montenegrinische Armee konnte in der Schlacht von Mojkovac vom 6./7. Januar 1916 den Durchbruch zweier österreichischer Divisionen, der 62. und 53., über die Tara zurückschlagen. Damit war die Überquerung der Prokletije und der Durchmarsch durch Albanien über die Gebirgsbarriere bis zum Abzug der letzten serbischen Einheiten auch gesichert worden. Während die österreich-ungarische Armee mit den schwierigen Geländeverhältnissen und dem Winterwetter nur eine ungenügende Versorgung der Operation in Montenegro aufbauen konnte, hatten die Reste der serbischen Armee sich westlich Andrijevica und bei Plav gesammelt und marschierten gegen Skutari und die nordalbanische Küstenebene. Ohne den weiteren Rückhalt Serbiens brach jedoch der Widerstand Montenegros bald zusammen, was mit dem Zusammenbruch der montenegrinischen Armee am 25. Januar 1916 den Vorstoß der Mittelmächte nach Albanien geöffnet hatte. Nachdem die Alliierten zuerst durch Schiffe der italienischen Flotte, sowie nachfolgend der französischen Flotte in Durrës und Vlora einen Brückenkopf aufzubauen begannen, war für die serbische Armee und der mit ihr evakuierten Regierung eine Rettung in greifbarer Nähe.
Der Vorstoß der Mittelmächte durch Nordalbanien wurde durch das schlechte Wetter und das weglose Land behindert, so dass die Serben ihrem Zugriff entkamen. Shkodra erreichten 185.930 Soldaten. Wegen Uneinigkeit über den Abtransport nach Korfu verminderte sich diese Zahl, Valona erreichten noch 150.000, ein Drittel der operativen Stärke von 1914. Selbst nach dem Abtransport der serbischen Armee mitsamt der serbischen Regierung nach Korfu, starben viele an den Folgen der Entbehrungen auf der Flucht. Dieser verlustreiche Rückzug ging als „Golgota Serbiens“ in die serbische Geschichtsschreibung ein.
Bis zum 5. August 1916 wurde die Hauptstreitmacht der serbischen Kräfte an die Salonikifront verbracht. Diesen schlossen sich 65.000 Franzosen und 85.000 Briten mit einer italienischen Division und einer russischen Brigade an.

Bis zum Dezember 1915 besetzten österreichisch-ungarische, deutsche und bulgarische Truppen ganz Serbien. Zwischen Bulgarien und Österreich-Ungarn aufgeteilt, wurde ein Generalmilitär-Gouvernement mit einem Gouverneur und einem Zivilkommissar an seiner Spitze errichtet. Die Administration bildeten vornehmlich Ungarn und radikale kroatische Frankovci. Die Polizeigewalt fiel fast ausschließlich bosnischen Muslimen zu. Die überwiegende Gesamtheit der serbische Bevölkerung fristete damit als okkupierte Gesellschaft ihr Dasein und nahm nicht mehr am öffentlichen Leben teil. 

1916 wurde die serbische Armee mit Unterstützung Frankreichs und Großbritannien reorganisiert (vier serbische Divisionen und eine „jugoslawische“ Freiwilligen-Division, zumeist Serben aus Österreich-Ungarn und Emigranten) und an der Salonikifront eingesetzt. Trotz der erlittenen schweren Verluste war die serbische Armee aufgrund ihrer erfolgreichen Evakuierung ein für die Mittelmächte noch immer ernstzunehmender Faktor geblieben. Die Entente-Verbündeten rechneten damit, dass die Mittelnmächte deshalb 23 Divisionen an die Saloniki-Front stellen müssten. Den 300.000 Soldaten der Entente standen damit 350.000 bulgarische und deutsche Soldaten gegenüber. Diese Zahl stieg später sogar noch weiter an. Nachdem es beim Eintritt Rumäniens auf Seiten der Entente zu Problemen gekommen war, griffen die Bulgaren über die griechische Grenze an. Die serbische Armee musste daher zwischen dem 12. September bis zum 19. November 1916 die Hauptaufgabe der offensiven Bewegungen an der Front übernehmen. Nachdem Živojin Mišić in den Generalstab zurückbeordert worden war und nun auch dessen Leitung übernahm, wurde der Kajmakčalan-Gipfels zwischen dem 12. bis 30. September 1916 genommen. Die Sicherung des Grenzgebirges eröffnete in der Bitola-Offensive (engl. Monastir Offensive) die schließliche Einnahme Bitolas, womit die serbische Armee am 18. November 1916 erstmals wieder auf eigenem Boden stand.

Das historische Hauptereignis des Jahres bildete die Russische Revolution vom 7. März 1917, die zu vielen geheimen und öffentlichen Vorschlägen zur Beendigung des Krieges zwischen den verfeindeten Staaten führte. Eine Welle demokratischer Bestrebungen begann auch die Teile der Habsburgermonarchie zu erreichen. Der Wind des Zusammenbruchs der alten Regime bildete damit einen neuen Hoffnungsschimmer der kleineren Völker. Auch die serbische Regierung unternahm aufgrund dieser globalen Veränderungen einen Versuch einer inneren Neuordnung. Mitglieder der Offiziersgruppe der Schwarzen Hand, unter ihnen ihr Anführer, Oberst Dragutin Dimitrijević, genannt „Apis“, wurden angeblich eines Anschlages gegen den serbischen Thronfolger Alexander verdächtigt und drei Offiziere nebst Apis durch ein Militärgericht zum Tode verurteilt. Neben dem Faktor, dass dies aufgrund des Wunsches eines Separatfriedens mit den Mittelmächten geschehen war, indem der Hauptvertreter einer radikalen österreichfeindlichen Politik und maßgeblich in das Attentat 1914 verwickelten kleinen Offiziersgruppe beseitigt wurde, wird in der serbischen Forschung der wahrscheinlichste Faktor zur Eliminierung des einflussreichen russophilen Offizierselementes gesehen. Die Russophilen waren damit ihres mächtigen inneren Zirkels in der serbischen Armee beraubt worden, der weitere innenpolitische russische Einfluss vollzog sich von nun an über die Übernahme der sozialen Ideologien und der Republikanischen Partei.

Im besetzten Serbien leisteten Aufständische mit Partisanenaktionen heftigen Widerstand, den die Besatzer ihrerseits mit Gräueltaten vergalten. Im Februar 1917 entflammte in Toplica ein Volksaufstand, der sich daraufhin nach Niš, Aleksinac und ins Tal des Timok ausbreitete. 15.000 Bauern nahmen am Aufstand teil. Bei dessen Niederschlagung durch die bulgarische Armee wurden etwa 20.000 Einwohner getötet. Dörfer wurden dem Erdboden gleichgemacht. Deutsche und österreich-ungarische Einheiten halfen bei dem Unternehmen.

Gespräche des serbischen Außenministers Nikola Pašić mit Ante Trumbić als Vertreter des Jugoslawischen Komitees auf Korfu führten am 20. Juli 1917 zur Deklaration von Korfu über die Absicht zur Bildung des späteren Königreichs der Serben, Kroaten und Slowenen.

Aus Angst, dass die serbische Armee unter fremden Kommando an einem Nebenkriegsschauplatz nicht mehr an den Hauptkämpfen beteiligt sei, wodurch das eigene Schicksal von fremden Mächten entschieden werde, forderte die serbische Generalität einen allgemeinen Strategiewechsel zur Beendigung des Weltkrieges. Sie verlangte, dass zuerst die kleineren Gegner ausgeschaltet werden müssten, um dann gegen die größeren vorzugehen. Selbst der serbische Regent schrieb hierzu dem britischen König zwei Memoranden. Nach der großen deutschen Frühjahrsoffensive auf Paris 1918, begannen die Verbündeten diese zuerst als unsinnig erachtete Idee ernst zu nehmen.
Der französische General Franchet d’Esperey wurde zum neuen Kommandanten der alliierten Orientarmee an die Saloniki-Front berufen. Er übertrug der serbischen Armee die Hauptaufgabe für den geplanten Durchbruch. Živojin Mišić hatte hierfür zwei serbische Armeen, daneben noch die Unterstützung eines Freiwilligenkontingents aus Soldaten der südslawischen Länder der Donaumonarchie. Die französische Armee bestand zur Hälfte aus Soldaten der Kolonialgebiete, ein marokkanisches Kavallerieregiment bildete darin das Hauptelement für einen tiefen Vorstoß. 180.000 französische, 150.000 serbische, 135.000 griechische, 120.000 britische und 42.000 italienische Soldaten sowie 1.000 albanische Freiwillige unter Esad Pascha wurden aufgestellt.
Den Hauptteil der Truppen hatte d’Esperey auf eine Linie von 33 km Breite konzentriert. Am 14. September begann mit einem Artillerieangriff auf die Gräben der Mittelmächte das Ende des Weltkrieges an der Salonikifront. Nachdem die 2. Serbische Armee schon am folgenden Tag die Salonikifront erfolgreich durchbrechen konnte, kam es bei den Verteidigern zu kompletten Auflösungserscheinungen. Der lange Marsch nach Belgrad dauerte 46 Tage und die Hauptstadt wurde am 1. November 1918 eingenommen.

Die Rückeroberung Serbiens und der Platz an der Seite der Siegermächte des Ersten Weltkrieges ermöglichten Serbien, am 1. Dezember 1918 das Königreich der Serben, Kroaten und Slowenen zu begründen, das sich ab 1929 Jugoslawien nennen sollte. Schon am 25. November beschloss die Volksversammlung der Vojvodina die Vereinigung mit Serbien, am 29. November folgte dem die Nationalversammlung Montenegros. Zudem erhielt Serbien Gebiete im Osten (Caribrod, Bosilegrad, Strumica), welche Bulgarien als Verlierer des Ersten Weltkrieges abtreten musste.

Das Königreich Serbien zählte Anfang 1914 rund 4,5 Millionen Einwohner. In den vier Kriegsjahren sind nach serbischen Angaben circa. 1,1 Millionen Menschen oder 24 % der Gesamtbevölkerung des Königreichs ums Leben gekommen. Es sind schätzungsweise 60.000 Zivilisten exekutiert worden und bis zu 400.000 weitere aufgrund von Epidemien, Kälte, Hungersnöten und Krankheiten gestorben, viele davon auf dem verlustreichen Rückzug zur Adria. Serbien erlitt neben Montenegro die anteilsmäßig größten Verluste: Von 700.000 Soldaten starben etwa 130.000. Insgesamt verlor Serbien kriegsbedingt rund 540.000 Menschen, etwa 11 % und Montenegro sogar 16 % seiner Bevölkerung.

Das Land selbst war in den Kriegsjahren ausgeplündert worden, die Wirtschaft zerstört. Die solcherart eingetretenen Verluste machten rund die Hälfte des serbischen Volksvermögens aus. Um die Not zu lindern, wurden rigorose Beschlagnahmungen von Nahrungsmitteln und anderen lebenswichtigen Ressourcen in den neuen jugoslawischen Ländern, die vom Krieg verschont geblieben waren, durchgeführt, was zu den ersten Krisen des neuen jugoslawischen Staates führte.

Am Ende des Ersten Weltkrieges wurde Österreich-Ungarn aufgelöst und das Staatsgefüge in Mittel- und Osteuropa vollkommen neu geordnet.

Aus Serbien, dem bis dahin unabhängigen Montenegro sowie den meisten südslawisch besiedelten Ländern Österreich-Ungarns entstand 1918 das "Königreich der Serben, Kroaten und Slowenen", das sich ab 1929 Jugoslawien nannte.

Das Territorium Jugoslawiens wurde in neun Banschaften neugegliedert.
Die Grenzen der Banschaften entsprachen nicht den bis dahin geltenden Grenzen. Ihre Bezeichnungen wurden von den Flüssen, welche sie durchflossen (mit Ausnahme der Primorska Banovina "(Banschaft Küstengegend)", sie lag am Meer und erhielt daher ihren Namen), abgeleitet.

Das Territorium Serbiens vor dem Ersten Weltkrieg entfiel im neuen Staat auf fünf Banovine (Vardar, Morava, Zeta, Drina, Donau).
Die mehrheitlich von Serben bevölkerte Banovina Vrbas "(Banschaft Vrbas)" wurde nach dem Ersten Weltkrieg ebenfalls als eine serbische Banschaft angesehen.

Nach dem schnellen Sieg der Achsenmächte über das Königreich Jugoslawien im Balkanfeldzug wurde das Land in zehn Teile mit unterschiedlichem staatsrechtlichem Status aufgeteilt. Serbien, bestehend aus Altserbien (das ehemalige Gebiet Serbiens innerhalb der Grenzen von 1912, ohne Mazedonien) und dem Westbanat, wurde mit zusammen etwa 4,5 Millionen Einwohnern wegen seiner ökonomischen Bedeutung zur ausschließlich deutschen Einflusszone erklärt und unter Militärverwaltung gestellt. Das Deutsche Reich installierte darauf in Serbien eine Kollaborationsregierung. Der dem Faschismus ideologisch nahe stehende General Milan Nedić, der im Königreich Jugoslawien Verteidigungsminister gewesen war, proklamierte am 1. September 1941 den Staat Serbien.

Am 4. Juli 1941 rief Tito den „Allgemeinen Aufstand“ aus und stellte Partisaneneinheiten auf, denen in Serbien nur unzureichende deutsche Besatzungstruppen gegenüberstanden.

Ein Großteil der Nationalbewegung Zbor unter Dimitrije Ljotić und ihr militärischer Verband Serbisches Freiwilligen-Korps ("Srpski dobrovoljački korpus", SDK) kollaborierte mit den deutschen Besatzern.

Als sowjetische Truppen an die Grenzen Serbiens vorstießen, wurde am 4. Oktober 1944 die Regierung Nedić aufgelöst. Am 20. Oktober 1944 wurde Belgrad gemeinsam von Tito-Partisanen und der Roten Armee eingenommen. Ein kleiner Teil der in Serbien lebenden „Volksdeutschen“ wurde bereits vorher aus dem Gebiet evakuiert.

Während der zweiten Vollversammlung des „Antifaschistischen Rats der Nationalen Befreiung Jugoslawiens“ ("AVNOJ") mit 142 Delegierten am 21. bis 29. November 1943 im bosnischen Jajce wurde bereits das Grundgerüst für den zukünftigen föderativen sozialistischen jugoslawischen Staat gleichberechtigter Völker und Republiken gelegt.

Die Sozialistische Republik Serbien wurde eine von sechs Teilrepubliken in Jugoslawien. Die südserbische Region Makedonien wurde von Serbien getrennt und bekam den Status einer eigenständigen Teilrepublik der Sozialistischen Föderativen Republik Jugoslawien. Montenegro bekam ebenfalls den Status einer eigenständigen Teilrepublik. Innerhalb der Teilrepublik Serbien wurden zwei autonome Provinzen eingerichtet: im Norden die Vojvodina (mit einer ungarischen Minderheit) und im Süden das Kosovo i Metohija (mit einer albanischen Mehrheit). Der Vojvodina wurde die Baranja (heute Teil Kroatiens) entnommen, dafür erhielt sie den größten Teil des hauptsächlich von Serben besiedelten Syrmien (davor Teil Slawoniens). Mit dieser Aufteilung Serbiens versuchten die jugoslawischen Kommunisten das Gleichgewicht zwischen Serbien und den restlichen Teilrepubliken zu halten. Serbien wurde politisch geschwächt.

1987 fordern serbische Intellektuelle im SANU-Memorandum ein Ende der sogenannten „Diskriminierungen des serbischen Volkes“. Das Memorandum propagiert unter anderem einen „Genozid“ am serbischen Volk im Kosovo und eine antiserbische Verschwörung Kroatiens und Sloweniens gegen Serbien. Intellektuelle und Politiker der anderen Völker Jugoslawiens reagierten auf die Forderung aus Belgrad mit eigenen nationalen Programmen. Zwischen den Teilnehmerstaaten breitete sich eine zunehmend vergiftete Atmosphäre aus. In Zusammenhang mit den politischen Umwälzungen in den anderen sozialistischen Staaten Osteuropas 1989/1990 bildeten sich dann auch in Jugoslawien neue Parteien und es kam 1990 zu ersten freien Wahlen in einigen Republiken, die mehrheitlich von nationalistisch agierenden Parteien gewonnen wurden. Am 9. März 1991 kam es zu Protesten in Serbien. Hunderttausend Demonstranten gingen vier Tage lang auf die Straße und protestierten unter der Führung der Partei von Vuk Drašković gegen das Milošević-Regime. Am 25. Juni 1991 proklamierten zunächst Slowenien und Kroatien ihre Unabhängigkeit, was von der Belgrader Führung als Verfassungsbruch angesehen wurde. Die jugoslawische Zentralregierung, die serbisch dominiert war, suchte die Unabhängigkeitsbestrebungen militärisch niederzuwerfen, um den Staat zu erhalten.

Am 25. Juni 1991 löste sich Slowenien aus dem Staatenbund Jugoslawien und erklärte seine Unabhängigkeit, was eine militärische Intervention der Jugoslawischen Volksarmee auslöste. Im sogenannten 10-Tage-Krieg wurde eine Besetzung des Landes durch die Armee jedoch durch relativ gut organisierten Widerstand verhindert. Es kam lediglich zu kleineren Gefechten zwischen slowenischen Polizisten und jugoslawischen Soldaten vor allem an internationalen Grenzübergängen, als die slowenische Landespolizei dort die Kontrolle übernahm. Deshalb kam es auch zu keinen nennenswerten Zerstörungen, was die Entwicklung der slowenischen Wirtschaft nach der Unabhängigkeit begünstigte. Die Gefahr eines Bürgerkriegs, wie er in anderen Teilen Jugoslawiens stattfand, bestand zu keinem Zeitpunkt, da die slowenische Bevölkerung, von kleineren Minderheiten abgesehen, fast ausschließlich aus Slowenen besteht. Unter Vermittlung der UNO und der österreichischen Regierung konnte schließlich ein Kompromiss erzielt werden: Slowenien sollte den Vollzug der Unabhängigkeit für die Dauer von drei Monaten aussetzen und in dieser Zeit mussten sich die dortigen Soldaten, die ihren Präsenzdienst in Slowenien leisteten, zurückziehen. Beide Seiten hielten sich an die Vereinbarung, und so konnte am 8. Oktober 1991 die Unabhängigkeit der Republik Slowenien in Kraft gesetzt werden. Während die Kriegshandlungen in Slowenien schon nach kurzer Zeit eingestellt wurden und Slowenien unabhängig wurde, entbrannte in Kroatien ein Bürgerkrieg.

Kroatien erklärte am 8. Oktober 1991 seine Unabhängigkeit. Militante Serben und die Jugoslawische Volksarmee errichteten daraufhin die sogenannte Serbische Autonome Provinz Krajina (SAO Krajina). Diese umfasste Teile Kroatiens, aus denen Kroaten gewaltsam vertrieben wurden. Die Unterstützung der Krajina-Serben durch Belgrad wurde vom Sicherheitsrat der Vereinten Nationen und von der Europäischen Gemeinschaft seit 1991 mit Sanktionen geahndet, da weltweit angenommen wurde, Serbien wolle auf diese Weise die Republik Serbische Krajina mit der Republika Srpska mit Serbien zu einem Großserbien verbinden.

Es kam zum Kroatienkrieg, der schließlich mit der Militäraktion ("Oluja"/Sturm) unter Führung von Ante Gotovina beendet wurde. Im Zuge dessen floh wiederum ein großer Teil der ansässigen serbischen Bevölkerung bzw. wurde vertrieben.

Auf Betreiben des serbischen Präsidenten Slobodan Milošević beschloss das serbische Parlament im Rahmen der Antibürokratischen Revolution 1989, den Status des Kosovo als autonome Provinz aufzuheben. Es folgten im Kosovo ethnische Unruhen, die zu einigen Dutzend Toten auf beiden Seiten führten. Die Albaner reagierten erst mit einem friedlichen Totalboykott, doch als es auch nach dem Dayton-Abkommen dem 1992 gewählten Anführer der Kosovo-Albaner Ibrahim Rugova nicht gelang, die Probleme im Kosovo zu internationalisieren und der serbische Druck im Kosovo immer mehr zunahm, tauchte 1997 die "Ushtria Çlirimtare Kosovës" (UÇK, "Befreiungsarmee Kosovos") auch öffentlich auf.

Die internationale Staatengemeinschaft stufte die UÇK zunächst als terroristische Organisation ein. Sehr schnell jedoch leiteten die USA entsprechende Verhandlungen um Waffenkäufe ein. In der Folge erlangte die UÇK die Kontrolle über mehrere Gebiete Kosovos. Die Kämpfe zwischen den verschiedenen serbischen Einheiten und der UÇK und vor allem die NATO-Angriffe führten im Endeffekt dazu, dass mehr als 500.000 Einwohner auf der Flucht waren und zunächst im nahen engeren Serbien, Mazedonien oder Albanien, dann zum Teil in Westeuropa Schutz suchten.

Im Frühjahr 1999 durchgeführte NATO-Luftangriffe zwangen Slobodan Milošević schließlich zum Einlenken. Der Rückzug der serbischen Armee beendete die blutigen Auseinandersetzungen im Kosovo, dem bis zu diesem Zeitpunkt bereits tausende Menschen zum Opfer gefallen waren. Das Kosovo wurde vorläufig Protektorat der UNO.

Bei den Präsidentschaftswahlen am 24. September 2000 wurde Vojislav Koštunica zum Jugoslawischen Präsidenten gewählt. Milošević hatte sich zunächst zum Wahlsieger erklärt, musste aber nach mehrtägigen Streiks, Demonstrationen der "Demokratischen Opposition Serbiens" (DOS) und der Besetzung des Parlaments am 5. Oktober schließlich seine Niederlage eingestehen. Bei den Parlamentswahlen im Dezember 2000 errang die DOS eine Zweidrittelmehrheit im Parlament. Im Januar 2001 wurde Zoran Đinđić zum neuen Ministerpräsidenten gewählt. Dies führte u. a. dazu, dass Slobodan Milošević am 29. Juni 2001 an den Internationalen Strafgerichtshof für das ehemalige Jugoslawien (ICTY) in Den Haag ausgeliefert wurde. Am 12. März 2003 wurde Đinđić auf offener Straße von Attentätern aus den Reihen der ehemaligen "„Roten Barette“" ermordet.

Nachdem die übrigen Teilrepubliken des alten Jugoslawien ihre Unabhängigkeit erklärt hatten, schlossen sich Serbien und Montenegro 1992 zur "Bundesrepublik Jugoslawien" zusammen. Diese bestand bis 2003, seitdem bildeten diese beiden Republiken den Staatenbund Serbien und Montenegro, dessen Auflösung am 21. Mai 2006 durch das montenegrinische Unabhängigkeitsreferendum eingeleitet wurde. Montenegro erklärte daraufhin durch einen Parlamentsbeschluss am 3. Juni 2006 seine formale Unabhängigkeit, Serbien folgte am 5. Juni mit einer Unabhängigkeitserklärung, ebenfalls durch einen Parlamentsbeschluss.

Der Ministerpräsident Serbiens ist seit 2014 Aleksandar Vučić. Seine Vorgänger waren Ivica Dačić (2012–2014), Mirko Cvetković (2008–2012) und Vojislav Koštunica (2004–2008). Staatspräsident war von 2004 bis 2012 der liberale, Europa zugewandte Reformer Boris Tadić.

Die nationalistische SRS-Partei des mutmaßlichen Kriegsverbrechers Vojislav Šešelj erlangte bei den Parlamentswahlen im Dezember 2003 mit rund 27 Prozent der Stimmen die meisten Mandate. Sie blieb dennoch in der Opposition, da die demokratischen und liberalen Parteien ein Bündnis und eine Regierung bildeten. Dies blieb auch nach den Parlamentswahlen 2007 so.

Bei den Wahlen 2012 unterlag Tadić seinem Konkurrenten Tomislav Nikolić, der bis 2017 das Amt des Staatspräsidenten ausübte. Zu seinem Nachfolger wurde bei den Wahlen 2017 der seit April 2014 amtierende Ministerpräsident Aleksandar Vučić gewählt.

Nach langen, ergebnislosen Verhandlungen beschloss am 17. Februar 2008 das Parlament des Kosovo, einseitig die Unabhängigkeit der Provinz auszurufen. Serbien erkennt diesen Beschluss nicht an und beruft sich dabei auf die Resolution 1244 des UN-Sicherheitsrats aus dem Jahr 1999, nach der das Kosovo Bestandteil der damaligen Bundesrepublik Jugoslawien unter UN-Verwaltung ist. Eine neue Resolution kam nicht zustande, da Russland sein Veto angekündigt hatte. Nachdem mehrere EU-Staaten Kosovo offiziell anerkannt hatten, zerbrach schließlich die Regierungskoalition im Streit um das Verhältnis zur Europäischen Union, was zu Neuwahlen führte.

Serbien bemüht sich seit der Demokratisierung im Jahr 2000 stärker um die Integration in die Europäische Union. Verhandlungen über ein Stabilisierungs- und Assoziierungsabkommen (SAA) begannen im November 2005. Gefordert wird auch die volle Kooperation Serbiens mit dem Internationalen Strafgerichtshof für das ehemalige Jugoslawien (ICTY) in Den Haag. Zwischenzeitlich wurden die Verhandlungen ausgesetzt, da vermutet wurde, dass die beiden gesuchten Kriegsverbrecher Radovan Karadžić und Ratko Mladić von der Regierung gedeckt wurden. Erst nach der neuen Regierungsbildung 2007 wurden die Verhandlungen fortgesetzt. Im Juli 2008 wurde Radovan Karadžić verhaftet und an das ICTY ausgeliefert. Serbien ratifizierte im September 2008 einseitig das vorläufige Stabilisierungs- und Assoziierungsabkommen mit der EU, da sich die Niederlande gegen eine Ratifizierung von Seiten der EU widersetzten. Am 7. Dezember 2009 wurde von den Außenministern der EU ein Interimsabkommen für Handelserleichterungen mit Serbien freigegeben. Die weitreichendste Veränderung im Verhältnis der EU mit Serbien war die vom 19. Dezember 2009 an gültige Reiseerleichterung für serbische Staatsbürger, die seitdem visafrei in die EU reisen können.

Die serbische Regierung stellte am 22. Dezember 2009 einen Antrag auf Mitgliedschaft in der Europäischen Union und gilt seit dem 1. März 2012 offiziell als Beitrittskandidat. Am 21. Januar 2014 wurden die Beitrittsverhandlungen aufgenommen.





</doc>
<doc id="12647" url="https://de.wikipedia.org/wiki?curid=12647" title="Ideologie">
Ideologie

Ideologie (französisch "idéologie"; zu griechisch ἰδέα "idéa" „Idee“ und λόγος "lógos" „Lehre“, „Wissenschaft“ – eigentlich „Ideenlehre“) steht im weiteren Sinne bildungssprachlich für Weltanschauung. Im engeren Sinne wird damit zum einen auf Karl Marx zurückgehend das „falsche Bewusstsein“ einer Gesellschaft bezeichnet, zum anderen wird in der amerikanischen Wissenssoziologie jedes System von Normen als Ideologie bezeichnet, das Gruppen zur Rechtfertigung und Bewertung eigener und fremder Handlungen verwenden. Seit Marx und Engels bezieht sich der Ideologiebegriff auf „Ideen und Weltbilder, die sich nicht an Evidenz und guten Argumenten orientieren, sondern die darauf abzielen, Machtverhältnisse zu stabilisieren oder zu ändern“.

Der Ideologiebegriff nach Marx, der im westlichen Marxismus eine zentrale Rolle spielt, geht davon aus, dass das herrschende Selbstbild vom objektiv möglichen Selbstbild der jeweiligen gesellschaftlichen Entwicklungsstufe verschieden ist. Da die materiellen Verhältnisse und Interessen das Denken bestimmen, wird nach Marx die Ideologie der Gesellschaft durch die Interessen dominanter gesellschaftlicher Gruppen, z. B. der Bourgeoisie, beeinflusst, um diese zu rechtfertigen. Durch eine Ideologiekritik kann diesen Interessen entgegengewirkt werden, um im Sinne eines allgemeinen Interesses ein nach dem Stand der Erkenntlichkeit korrektes und vollständiges Bild der Gesellschaft zu entwerfen. Eine wichtige Weiterentwicklung erfährt die Theorie der Ideologie bei Georg Lukács, der sie mit einer Theorie des Totalitarismus verknüpft: Die vollständige Vereinnahmung des Individuums durch gesellschaftlich organisierte Aktivitäten und Strukturen führt dazu, dass sich das Individuum nur innerhalb dieser Strukturen verstehen kann und somit selbst eine passende Ideologie entwickelt. 

In der Wissenssoziologie hat sich "Ideologie" hingegen als Bezeichnung für ausformulierte Leitbilder sozialer Gruppen oder Organisationen durchgesetzt, die zur Begründung und Rechtfertigung ihres Handelns dienen – ihre Ideen, Erkenntnisse, Kategorien und Wertvorstellungen. Sie bilden demnach das notwendige „Wir-Gefühl“, das den inneren Zusammenhalt jeder menschlichen Gemeinschaft gewährleistet. Dieser Ideologie-Begriff wird auch auf die Ideensysteme von politischen Bewegungen, Interessengruppen, Parteien etc. angewandt "(→ politische Ideologie)".

Im gesellschaftlichen Diskurs werden die beiden Ideologiebegriffe oft nicht hinreichend voneinander unterschieden.

Zu Beginn des 19. Jahrhunderts prägte Antoine Louis Claude Destutt de Tracy den französischen Begriff "idéologie" als Bezeichnung für das Projekt einer „einheitlichen Wissenschaft der Vorstellungen und Wahrnehmungen“ ("science qui traite des idées ou perceptions"), das sich auf die Erkenntnistheorie von Condillac berief. Die "Idéologistes" setzten zur Vorbeugung gegen eine neue Schreckensherrschaft ein pädagogisches Programm der Breitenaufklärung ins Werk. Durch eine publizistische Kampagne von Napoleon Bonaparte wurde diese Schule jedoch als wirklichkeitsfremdes, spekulatives Systemgebäude angegriffen; aus dieser Tradition leitet sich der Begriff der Ideologie als kohärentes Weltbild auf der Basis unzutreffender Prämissen ab. Erst durch Marx und Engels wurde dieser Begriff dann herrschaftskritisch angewandt. Zuvor war der Ausdruck "Ideologen" im deutschen Sprachraum für eine Orientierung an Ideen (anstatt der Realität), etwa der der Freiheit oder einer republikanischen Verfassung reserviert gewesen.

Der Begriff der Ideologie ist, bis zum Versuch einer funktionalen Beschreibung in der Wissenssoziologie, immer eng mit dem Gedanken Ideologiekritik verbunden.
Neben den hier genannten Positionen sind zu den Ideologiebegriffen u.a. einschlägig: Ferdinand Tönnies, Hans Barth, Ernst Topitsch, Hans Albert, Bertrand Russell, Louis Althusser, Theodor W. Adorno, Hannah Arendt und Jürgen Habermas.

Vorläufer des modernen Ideologiebegriffes ist die Idolenlehre von Francis Bacon. Schon hier ist die Idee einer Aufdeckung von falschen Vorstellungen entscheidend: Die Reinigung des Denkens von "Idolen" (Trugbildern) ist für ihn die Voraussetzung von Wissenschaft. Quellen dieser Trugbilder können Tradition, Sprache, Herkunft und Sozialisation sein.

Eine besondere Rolle spielte die Ideologiekritik in der Aufklärung. Zentrales Ziel der Aufklärung war die Befreiung des Bewusstseins der Menschen von Aberglauben, Irrtümern und Vorurteilen, die nach dieser Sichtweise den mittelalterlichen Machthabern zur Legitimation ihrer Herrschaft dienten. Die französischen Materialisten, u. a. Paul Heinrich Dietrich von Holbach und Claude Adrien Helvétius, kritisierten insbesondere die katholische Kirche und bezeichneten deren – ihrer Meinung nach im Interesse der Machterhaltung verbreiteten – Behauptungen als "Priesterbetrug". Die Aufklärung verlangte die politische Durchsetzung von Vernunft, Wissenschaft, Demokratie und Menschenrechten.

Die Vorstellung des Aufrechterhaltens von für das Individuum oder die Gesellschaft zuträglichen Irrtümern über Selbst und Welt findet sich auch bei Arthur Schopenhauer, Max Stirner, Friedrich Nietzsche, Vilfredo Pareto (dieser als „Derivation“). 

Nach dem sozialistischen Utopisten Saint Simon griffen Mitte des 19. Jahrhunderts Marx und Engels den seit Napoleon stigmatisierten Begriff wieder auf. Ideologie wird hier nicht als bewusste Verführung, sondern als ein sich aus den gesellschaftlichen Verhältnissen ergebender objektiv notwendiger Schein konzipiert: Aus dem Klassencharakter der gesellschaftlichen Verhältnisse ergibt sich nach Marx die Tendenz, dass die Gedanken der herrschenden Klasse, die mit den bestehenden Produktionsverhältnissen im Einklang stehen, auch die herrschenden Gedanken in der Gesellschaft sind. In seinem Hauptwerk, "Das Kapital", bestimmt Marx den Waren- und Geldfetisch als bestimmende Verkehrungsmomente in der kapitalistischen Produktion. Die Menschen nehmen ihre (arbeitsteiligen) Beziehungen zueinander als Beziehungen zwischen Waren wahr.

Im 20. Jahrhundert wurden von westlichen Marxisten ideologische Momente der Verdinglichung diskutiert, so zum Beispiel von Ernst Bloch ("Geist der Utopie", 1918) oder Georg Lukács "(Geschichte und Klassenbewußtsein," 1923), für dessen Verdinglichungsanalyse die Idee einer ideologischen Verblendung zentral war. Demnach sei Ideologie „notwendig falsches Bewusstsein“. Die Bilder von der Wirklichkeit, die das Subjekt sich schafft, sind von subjektiven Faktoren beeinflusst oder bestimmt. Daher sind sie nicht objektiv, sondern verfälschen die Wirklichkeit.

Antonio Gramsci entwickelt in den "Gefängnisheften" einen Ideologiebegriff, der Ideologie als „gelebte, habituelle gesellschaftliche Praxis“ versteht. Ideologie ist bei ihm nicht mehr zu reduzieren auf die Ebene des Bewusstseins, sondern umfasst auch Handlungen der Menschen.

Nach Louis Althusser vermitteln Ideologien dem Individuum Bewusstsein und üben über das Individuum Macht aus, z. B. in Verbindung mit sogenannten ideologischen Staatsapparaten. Zudem ermöglichen Ideologien es den Individuen, sich in der Gesellschaft als Subjekte wiederzuerkennen. Ideologie ist nach Althusser nicht nur Manipulation, sondern konstituiert Subjekte – sie verstünden sich trotz bzw. wegen ihrer Unterwerfungen als frei. Ein wichtiger Gedanke von Althusser ist, dass Ideologien unbewusst sind. Ein zentrales Werk für Althussers Ideologietheorie ist sein Essay "Ideologie und ideologische Staatsapparate" aus dem Jahre 1970.

Max Horkheimer und Theodor W. Adorno, die Begründer der Frankfurter Schule, übernahmen und erweiterten das Konzept der Marx’schen Ideologiekritik (Kapitel "Kulturindustrie" in der "Dialektik der Aufklärung," 1947). Anknüpfend an Georg Lukács’ Verdinglichungsthese sahen sie in Warenfetisch und kapitalistischem Tauschprinzip die Quellen des gesellschaftlich erzeugten Verblendungszusammenhangs. Ideologie ist für sie objektiv notwendiges und zugleich falsches Bewusstsein, in dem sich Wahres und Unwahres verschränke, da Ideologie auf die Idee der Gerechtigkeit als apologetische Notwendigkeit nicht verzichten könne. So verdecke das Grundmodell bürgerlicher Ideologie, der „gerechte Tausch“, dass im kapitalistischen Lohnarbeitsverhältnis nur scheinbar Vergleichbares getauscht werde. In der Kulturindustrie nehme die Ideologie die Form des „Massenbetrugs“ an. Ein Veralten der Ideologie konstatierten die Frankfurter Ideologiekritiker für die Phase des postliberalen Spätkapitalismus und des Faschismus. Im Spätkapitalismus würden die faktischen Verhältnisse zu ihrer eigenen Ideologie, das heißt die Realität rechtfertigt sich durch ihr So-und-nicht-anders-Sein. Da der Faschismus in seinen Proklamationen auf jeden Wahrheitsanspruch verzichte, an dem Ideologie entlarvt werden könnte, triumphiere in seinem Herrschaftsbereich der blanke Zynismus des Machtstaates.

Ideologiekritik ist nach Adorno bestimmte Negation im Hegelschen Sinn, „Konfrontation von Geistigem mit seiner Verwirklichung und hat zur Voraussetzung die Unterscheidung des Wahren und Unwahren im Urteil wie den Anspruch auf Wahrheit im Kritisierten“

In seinem Werk "Die offene Gesellschaft und ihre Feinde" kritisiert Karl R. Popper den totalitären Charakter bestimmter Ideologien, insbesondere des Nationalsozialismus und des Stalinismus.

Totalitäre politische Ideologien mit umfassendem Wahrheitsanspruch weisen oftmals Elemente von Mythenbildung, Geschichtsklitterung, Wahrheitsverleugnung und Diskriminierung konkurrierender Vorstellungen auf. Nach den Erfahrungen mit dem Nationalsozialismus und dem Zusammenbruch des real existierenden Sozialismus ist die Skepsis gegenüber umfassenden und mit Heilsversprechungen durchsetzten Theoriengebäuden gewachsen, insbesondere wenn sie mit Handlungsaufforderungen oder mit der Unterdrückung abweichender Ideen verbunden sind.
Ideologiekritik im Sinne von Karl Popper umfasst dabei insbesondere die Analyse folgender Punkte:

Der Politikwissenschaftler Kurt Lenk schlug in seinem Aufsatz "Zum Strukturwandel politischer Ideologien im 19. und 20. Jahrhundert," den er in seinem Buch "Rechts, wo die Mitte ist" veröffentlichte, eine Klassifizierung der Ideologien vor. Er unterschied zwischen Rechtfertigungsideologien, Komplementärideologien, Verschleierungsideologien und Ausdrucksideologien.

Unter "Rechtfertigungsideologien" verstand Lenk modellbildende Ideologien, die sich auf die gesamten gesellschaftlichen Beziehungen erstrecken. Das zu Grunde liegende Modell ist meist eine auf Rationalität und Wissenschaftlichkeit pochende Deutung der Realität.
Ideologisch sei ein solches Modell, weil es bestrebt ist, seinerseits ein verbindliches Verständnis von Realität – nicht selten unter dem Anspruch der unangreifbaren Anwendung rationaler Argumente und Argumentationsstrukturen – als einzig „vernünftigerweise“ vertretbares zu etablieren.

Lenk beschrieb demgegenüber "Komplementärideologien" als „für jene Gesellschaften lebensnotwendig, in denen der Mehrheit der Menschen ein relativ hohes Maß an Triebverzicht abverlangt werden muss, damit die Reproduktion der Gesellschaften gewährleistet ist.“ Komplementärideologien würden die benachteiligten Gesellschaftsmitglieder vertrösten. Zum einen beinhalte diese Ideologie eine die Realität verleugnende Verheißung auf einen objektiv unmöglichen besseren Zustand. Diese trostspendende Zukunftserwartung soll die eigenständige Interessendurchsetzung der benachteiligten Gesellschaftsmitglieder lähmen und sie zur Gefolgschaft mit ihren Bedrückern verpflichten. Komplementärideologien arbeiten auch mit dem Bezug zur „Ehrlichkeit“, wonach der Zustand der Welt Schicksal sei und menschliches Tun daran nichts ändern könne.

"Verschleierungs- oder Ablenkungsideologien" seien nach Lenk die Erzeugung von Feindbildern, um einer Diskussion über die objektiven Gründe gesellschaftlicher Probleme aus dem Weg zu gehen. Eng angelehnt an diesen Aspekt verwendete er den Begriff "Ausdrucksideologie." Darunter verstand er eine Ideologie, die bei den seelisch tieferen Schichten der Menschen ansetze. Es werde ein Freund-Feind-Bild inszeniert und Behauptungen aufgestellt, an die die Massen fanatisch glauben sollen.

Die Gegenwart wird häufig als „nach-“ oder „postideologisches Zeitalter“ bezeichnet, in dem die Subjekte der Gesellschaft vorwiegend realistisch und pragmatisch – also frei von Ideologien – agieren würden. Der französische Philosoph Jean-François Lyotard begründet dies mit dem heutigen Wissen über die Unmöglichkeit der Letztbegründung. Die Vielfalt der gesellschaftlichen Kräfte "(der Pluralismus)" postmoderner, liberal demokratischer Gesellschaften, die sich permanent gegenseitig kontrollieren, verhindert nach dieser populären Auffassung die Bildung von Ideologien. Verfechter dieser Idee verweisen gern auf das Scheitern der großen ideologisch begründeten Systeme in der jüngeren Geschichte (Nationalsozialismus, Kommunismus). Auf diese Weise wird der Begriff "Ideologie" allein auf die abwertende Konnotation beschränkt und die damit assoziierten negativen Bilder legen den Schluss einer "ideologiefreien" Gegenwart nahe, die solche Entwicklungen überwunden hat. Durch die Transparenz der Politik, die angeblich keinen Fehler unerkannt lässt und umgehend korrigiert, versprechen die Beteiligten „Wahrheit und Ehrlichkeit“: Begriffe, die in einer Ideologie keinen Platz haben.

Mit dieser modernen „Anti-Ideologie“ werden alle gegenwärtigen gesellschaftlichen Entwicklungen (Technologischer Fortschritt, demokratische Systeme, kapitalistische Gesellschaftsordnung, stetig zunehmendes Wirtschaftswachstum u. a.) als „wahr und ehrlich“ legitimiert. Die Philosophen Slavoj Žižek und Herbert Schnädelbach weisen jedoch darauf hin, dass solch technokratisches Denken alles andere als nicht-ideologisch sei: Eine der idealen Grundbedingungen für eine Ideologie sei die Annahme, dass es keine Ideologie gäbe.
Žižek sieht darin gar eine weitaus gefährlichere Ideologie als in den Diktaturen: Despoten legitimieren Enteignung, Vertreibung, Gewalt usw. im Bewusstsein ihrer Machtfülle mit offensichtlichen Unwahrheiten. Demgegenüber ist im modernen Pluralismus ein Konsens der gesamten Gesellschaft notwendig: Tatsächlich ideologische Begründungen würden im alltäglichen Diskurs als unumstößliche Wahrheiten akzeptiert und bestimmten somit ohne offensichtlichen Zwang durch die Politik den sozialen Prozess. Je mehr sich die Bürger mit dieser versteckten Ideologie identifizierten, desto weniger brauche der Staat einzugreifen. Vordenker der Kritik dieser „diskursiven, alles durchdringenden, sich sozial organisierenden Ideologie der Gegenwart“ sind vor allem Ernesto Laclau und Chantal Mouffe.

Die Abgrenzung von der Ideologie wurde im Zuge der Aufklärung zu einem Bestandteil der Wissenschaften, die sich im Gegensatz zu Ideologie und Glaube darum bemühen, wertfrei, neutral und intersubjektiv vorzugehen und die Gültigkeit ihrer Theorien und Hypothesen anhand empirischer Erfahrungstatsachen zu überprüfen (Wissenschaftstheorie, Empirisch-analytischer Ansatz).

Wissenschaftliche Denkmuster, Paradigmen bzw. Ideenschulen können auch einen ideologischen und abwehrenden Charakter entwickeln und damit wissenschaftlichen Fortschritt hemmen. Thomas Kuhn analysierte in seinem Buch "Die Struktur wissenschaftlicher Revolutionen" wissenschaftliche Paradigmen auch unter dem Aspekt als konkurrierende Ideenschulen. Diese legen fest:


Von einzelnen Wissenschaftstheoretikern (u. a. Bruno Latour) wird die Entgegensetzung von Ideologie und objektiver Wissenschaft als Machtmechanismus und Verschleierungstechnik betrachtet. Diese Position wird von Kritikern allerdings wiederum als zur totalen Irrationalität führend heftig kritisiert (Sokal-Affäre).

Auch wenn Naturwissenschaften ideologiefrei sein können, gilt dies nicht unbedingt für Gesellschaftswissenschaften. So finden sich beispielsweise in der Völkerkunde und den Sozialwissenschaften um die Wende vom 19. zum 20. Jahrhundert etliche Beispiele für ideologisch geprägte Vorstellungen. Sehr deutlich wird dies bei den sozialdarwinistischen Schulen, die rassistischen Ideen mit ihren Aufzeichnungen über angeblich „unterentwickelte Naturvölker“ nährten.

Einen Sonderfall stellt nach Hans Albert das Fach Ökonomie dar. Da die Volkswirtschaftslehre sich u. a. mit der Frage beschäftigt, wie die gesellschaftliche Arbeit möglichst optimal organisiert, gesteuert oder beeinflusst werden kann, muss der einzelne Wissenschaftler auch einen Standpunkt zur Frage haben, was gut für die Gesellschaft ist. Das ist, bedingt durch unterschiedliche Partialinteressen, zwangsläufig immer eine ideologische Position.

Politik ist immer mit Ideologie verbunden, eine unideologische, rein technokratische Politik ist realitätsfremd. Politische Programme basieren auf bestimmten Wertesystemen. Die grundlegenden politischen Ideologien sind Liberalismus (Betonung der Freiheit auf Grundlage der Marktwirtschaft), Sozialismus (Betonung der Gleichheit) und Konservatismus (Betonung von gesellschaftlichen Traditionen).

Der Vorwurf einer durch Ideologie bestimmten Argumentation findet sich häufig im politischen Diskurs. Damit wird unterstellt, dass ein Standpunkt deswegen nicht stichhaltig sei, weil er auf einer politischen Ideologie basiere. Der eigene Standpunkt wird demgegenüber implizit oder explizit so dargestellt, dass er auf einer nüchternen Analyse der Wahrheit, dem gesunden Menschenverstand oder auf einer nicht in Frage zu stellende Ethik beruhen würde. Dies könnte indes die jeweilige Gegenseite in vielen Fällen mit dem gleichen Recht für sich in Anspruch nehmen. Unausgesprochene Ideologeme (einzelne Elemente einer Ideologie) beherrschen oft die politische Debatte, ohne dass dies in der Diskussion immer bewusst wird.

Als analytische Kategorie findet neben dem Begriff der "politischen Ideologie" ebenso der Begriff der "religiösen Ideologie" Anwendung in der Wissenschaft. Eine religiöse Ideologie ist eine Ideologie mit transzendentem Bezug, die das Konzept einer Gesamtexistenz von Person und Gesellschaft umfasst und Integrations- sowie Bindungskräfte in bestimmten gesellschaftlichen Gruppen entwickeln kann. Die Entstehung einer religiösen Ideologie kann insbesondere darin begründet sein, dass in Verbindung mit einer oppositionellen politischen Haltung „Konfession“ eine bedeutsame Rolle zu spielen beginnt. Als populäre Beispiele für religiöse Ideologien werden in der Literatur Bezüge zu den Weltreligionen hergestellt und insbesondere der Protestantismus und der Katholizismus als religiöse Ideologien bezeichnet; unabhängig davon, ob die ursprünglichen Motive politisch gewesen sind. Gemeint ist mit einer derartigen Kennzeichnung jeweils nicht eine Religion als Gesamtphänomen, sondern eine bestimmte religiöse und politische Lehre, die eine religiöse Bewegung zur Folge haben kann. In allgemeiner Hinsicht wird der Begriff religiöse Ideologie auch in Zusammenhang mit der Orthodoxie und dem Fundamentalismus gebracht.

Der Politikwissenschaftler Mathias Hildebrandt, der den Begriff politische Ideologie als Fundamentalismus zu fassen versuchte, stellte den traditionalistischen Aspekt von spezifischen religiösen Strömungen innerhalb von Religionen als ein gemeinsames Merkmal heraus. Er schrieb: „Es wird der Anspruch erhoben, zu den ursprünglichen Quellen der eigenen Tradition zurückzukehren und sie von den Verfälschungen ihrer historischen Entwicklung zu befreien, die zumeist als ein Degenerationsprozess begriffen wird.“ Einher ginge diese Auffassung mit einer „"Essenzialisierung der eigenen Tradition, die den Anspruch erhebt, das wahre Wesen der eigenen Religion freigelegt zu haben"“. Das Paradoxe bei den religiösen Ideologien sei allerdings, dass im Gegensatz zum Anspruch, zur wahren Lehre zurückzukehren, „"in den meisten Fällen eine moderne religiöse Ideologie"“ entstehe.

Neben dem Begriff der religiösen Ideologie hat sich in der Religionspolitologie der Begriff "politische Religion" durchgesetzt. Der Akzent liegt bei diesem Begriff weder stark auf dem Politischen noch auf dem Religiösen von bestimmten Ideologien. Einerseits wird mit diesem Begriff die enge Verbindung zwischen religiösen und politischen Denkweisen hervorgehoben, andererseits die Verbindung zwischen Ideologien, die sowohl politische als auch religiöse Elemente und politisch-religiöse Bewegungen erfassen.

Ideologie ist – nach Karl Mannheim – „Funktionalisierung der noologischen Ebene“ und somit Instrumentalisierung der menschlichen Erkenntnisfähigkeit oder konkreter noch – nach Roland Barthes – „Verwandlung von Geschichte in Natur.“

Ideologie sichert die eingeforderte Legitimation für die bestehende Ordnung und befriedigt das Bedürfnis nach Sicherheit und Sinnhaftigkeit, die durch die Religion nicht mehr gewährleistet werden können: „Das Behagliche möchte allzu gern das zufällige Sosein des Alltags, wozu heutzutage romantisierte Gehalte (‚Mythen’) gehören, zum Absoluten hypostatieren und stabilisieren, damit es ihm ja nicht entgleitet. So vollzieht sich die unheimliche Wendung der Neuzeit, dass jene Kategorie des Absoluten, die einst das Göttliche einzufangen berufen war, zum Verdeckungsinstrument des Alltags wird, der durchaus bei sich bleiben möchte.“

Andererseits läuft die Ideologie Gefahr, als geschlossenes Sinnsystem einer komplexen Wirklichkeit letztlich nicht gerecht werden zu können und schlussendlich als Welterklärungsmodell zu scheitern. Da „Ideologie immer selbstreferentiell ist, das heißt sich immer durch die Distanznahme zu einem Anderen definiert, den sie als ‚ideologisch’ ablehnt und denunziert“ löst sie „den Widerspruch des entfremdeten Wirklichen durch eine Amputation, nicht durch eine Synthese“

Karl Mannheims These von der Funktionalisierung der Erkenntnis durch die Ideologie ergänzt Roland Barthes durch die Funktionalisierung des Mythos, den die Ideologie instrumentalisiert: „Die Semiologie hat uns gelehrt, dass der Mythos beauftragt ist, historische Intention als Natur zu gründen. Dieses Vorgehen ist genau das der bürgerlichen Ideologie. Wenn unsere Gesellschaft objektiv der privilegierte Bereich für mythische Bedeutung ist, so deshalb, weil der Mythos formal das am besten geeignete Instrument der ideologischen Umkehrung ist, durch die sie definiert wird. Auf allen Ebenen der menschlichen Kommunikation bewirkt der Mythos die Verkehrung der Antinatur in Pseudonatur.“

Die Geschichte des Ideologiebegriffs ist eng verknüpft mit der Geschichte der bürgerlichen Gesellschaft. Ideologie nach heutigem Verständnis wird erst möglich nach dem „Verschwinden des göttlichen Bezugspunktes“ das sich bereits ankündigt mit dem beginnenden Empirismus in Bacons „Idolae“, die als „Götzenbilder“ und „Täuschungsquellen“ den „Weg zur wahren Einsicht versperren“ Kant – der seiner „Kritik der reinen Vernunft“ ein Bacon-Zitat über die Idolae voranstellt – stellt dann das traditionelle Seinsverständnis mit der in den vier Antinomien und auch in der transzendentalen Dialektik ständig wiederkehrenden Mahnung, das Epistemische nicht als Ontologisches misszudeuten, endgültig infrage und schafft somit „nachdem die objektiv ontologische Einheit des Weltbildes zerfallen war“ die Basis für Hegels dialektisches Weltbild, das „nur auf das Subjekt bezogen konzipierbar“ und nur als „eine im historischen Werden sich transformierende Einheitlichkeit“(ibid.) Gültigkeit beanspruchen konnte. Erst jetzt, nach Beendigung der französischen Revolution, ergibt es einen Sinn, von "bürgerlicher" Ideologie oder generell von einem Ideologie"begriff" zu sprechen, der dann auch sogleich von Napoleon pejorativ auf den eigentlich wertfrei als „Lehre von den Ideen“ von den Spätaufklärern in der Nachfolge Condillacs und der empirischen Tradition aufgebrachten Terminus angewandt wurde. Den wesentlichen Beitrag zum heutigen Ideologieverständnis dürfte schließlich Karl Marx geleistet haben, der im „Elend der Philosophie“ ausführt: „… dieselben Menschen, welche die sozialen Verhältnisse gemäß ihrer materiellen Produktionsweise gestalten, gestalten auch die Prinzipien, die Ideen, die Kategorien gemäß ihren gesellschaftlichen Verhältnissen“.

Auch wenn Mannheim zunächst versucht, zwischen wertfreien und wertenden Ideologien zu unterscheiden, kommt er doch zu dem Fazit, dass der wertfreie Ideologiebegriff „letzten Endes in eine ontologisch-metaphysische Wertung“ „hinübergleitet“ In diesem Zusammenhang spricht Mannheim dann auch vom „falschen Bewußtsein“, das die Ideologie zwangsläufig schafft: „Es sind also in erster Linie überholte und überlebte Normen und Denkformen, aber auch Weltauslegungsarten, die in diese ‚ideologische’ Funktion geraten können und vollzogenes Handeln, vorliegendes inneres und äußeres Sein nicht klären, sondern vielmehr verdecken.“

Die derart resultierende verkürzte Sicht auf die Realität beklagt Roland Barthes denn auch als „Verarmung des Bewußtseins“ die durch die Ideologie als bürgerliche geleistet wird: „Es ist die bürgerliche Ideologie selbst, die Bewegung, durch die die Bourgeoisie die Realität der Welt in ein Bild der Welt, die Geschichte in Natur verwandelt.“





</doc>
<doc id="12649" url="https://de.wikipedia.org/wiki?curid=12649" title="Hierarchie">
Hierarchie

Als Hierarchie (gesprochen [] oder [], altgr. ἱεραρχία "hierarchia", zusammengesetzt aus ἱερός, "hieros", „heilig“ und ἀρχή, "archē", „Führung, Herrschaft“, daraus ab dem 17. Jahrhundert kirchenlateinisch "hierarchia": „Rangordnung der Weihen“) bezeichnet man ein System von Elementen, die einander über- bzw. untergeordnet sind. Im Sinne der Monohierarchie ist dabei jedem Element höchstens ein anderes Element unmittelbar übergeordnet, während bei einer Polyhierarchie auch mehrere über- und untergeordnete Elemente möglich sind.

Mathematisch betrachtet bedarf eine Hierarchie einer Ordnungsrelation, die einen Baum (Monohierarchie) oder gerichteten azyklischen Graphen (Polyhierarchie) definiert. Das Komplement ist die Heterarchie.

Die Einteilung (Klassifizierung) oder Einordnung (Klassierung) von Objekten in eine Hierarchie impliziert häufig eine Wertigkeit, die bereits in der Rangordnung, nach der die Objekte geordnet werden, enthalten ist. Grundsätzlich sind sie allerdings einfacher als komplexe Netzwerkstrukturen zu erfassen.

Aus dem Blickwinkel der Soziologie, auch im Kontext von Sozialen Systemen, sind Hierarchien oft mit Verhältnissen von Herrschaft und Autorität verbunden. Hierarchien gelten auch in operationellen Strukturen, wie beispielsweise in der Linienorganisation eines Unternehmens, bei Behörden, streng hierarchisch grundsätzlich im Strafvollzug im Verhältnis von Bediensteten und Beamten gegenüber Gefangenen, im Militär­wesen oder in der Kirche, vor allem die römisch-katholische Kirche und die orthodoxen Kirchen sind stark hierarchisch strukturiert. Die Organisationsforschung beschäftigt sich mit organisatorischen Phänomenen.

Hierarchische Strukturen kommen auch in der Tierwelt vor, wo über die Rangordnung die hierarchischen Positionen der einzelnen Individuen, vom Alphatier bis zum Omega (rangniedrigstes Individuum), bedingt werden. Unter anderem ist dies Gegenstand der Verhaltensforschung.

Hierarchische Ordnungen in sozialen Systemen basieren häufig auf Bedürfnissen und emotionalen Abhängigkeiten. Idealtypisch leisten Höhere Beiträge zu Gunsten der Mitglieder ihres Systems bzw. "zu Gunsten" des jeweiligen Gesamtsystems (Beiträgshöhen über längere Zeiträume). Jene Beitragshöhen determinieren üblicherweise hierarchische Ordnung. So richtet sich beispielsweise die Stimmverteilung des Internationalen Währungsfonds (IWF) wie auch die Stimmverteilung der Weltbank nach zur Verfügung gestellten Beitragshöhen. „Die Ablehnung immer vorhandener Hierarchien führt nur zu deren Verschleierung und macht sie damit auch unangreifbar.“

Ein klassisches Beispiel für eine Hierarchie sind militärische Dienstgrade. Allerdings spiegeln in der Praxis die Dienstgrade nicht unbedingt das tatsächliche Vorgesetztenverhältnis, jedenfalls aber das jeweilige Rangverhältnis wider.

Auch in Unternehmen werden formelle Hierarchien von Vorgesetzten und Untergeordneten definiert, welche "theoretisch" festlegen, wer wem Weisungen erteilen kann. Allerdings wirken insbesondere in Matrixorganisationen und aus der Tendenz zu Abflachungen von Hierarchien in Organisationen Politisierungsdilemmata, die die inoffiziellen (informellen) Hierarchien in Unternehmen und unklare rangdynamische Entscheidungsstrukturen begünstigen. Daraus resultieren unproduktive Zirkelbezüge und struktureller Egoismus, welche entweder aus Selbstzweck oder aus blinden Flecken oft beibehalten werden, auf diese Weise dysfunktional wirken. Selbstorganisation unter Gleichgestellten funktioniert oftmals nicht.

Idealtypisch orientieren sich Hierarchien in Unternehmen an der jeweiligen Zweckausrichtung. Stefan Kühl geht (nach Luhmann 1973) davon aus, dass die "hierarchische Ordnung" eines Unternehmens mit seiner jeweiligen „Zweck-Mittel-Ordnung“ sinnvollerweise parallel (damit widerspiegelnd) zu schalten ist. 

Im Rahmen des hoheitlichen Strafvollzuges besteht hingegen eine strenge Hierarchieform, die sich im Gegensatz zu den meisten anderen Varianten bereits durch die gesetzlich geregelte unbedingte Gehorsamspflicht der Gefangenen gegenüber den Vollzugsbediensteten (z. B. in Deutschland: § 82 Abs. 2 Strafvollzugsgesetz) klar und undurchdringlich abbildet.

Streng geordnet ist die Gerichtsbarkeit mit ihren Instanzen, die jeweils Urteile der untergeordneten Instanz aufheben können. Zur Ordnung von Informationen sind Klassifikationen ein gängiges Mittel. Auch die hierarchische Ordnung von Dateien in einer Verzeichnisstruktur ist üblich.

Geplante soziale Netzwerke in der Betriebswirtschaftslehre mit wenig Herrschaftselementen nehmen nicht selten „Herrschaftsfreiheit“ in Anspruch, sind aber informell regelmäßig nicht hierarchiefrei. Solche Hierarchien bleiben ambivalent, solange keine Konflikte auftreten.

Weiterhin kann es bei Matrixorganisation auch zu bivalenten Überlagerungen von in der Regel zwei Hierarchien kommen, die weiterer spezifischer Regelung bedürfen. Solche Matrix wird für Projekte definiert, die eine temporäre Zuordnung zu einer zweiten Hierarchie bewirken. Die per definitionem gewünschte Orthogonalität der dabei überlagerten Hierarchien wird in der Praxis nicht erreicht, weil die verfügbaren materiellen und intellektuellen Ressourcen unausweichlich zu Kopplungen zwischen den Hierarchien führen.

Das Idealbild einer hierarchischen Struktur geht davon aus, dass mit der Struktur eine Methodik für Menschenführung und Kooperation definiert ist, die eine bestimmte Strategie der bidirektionalen Kommunikation nutzt. Damit verbunden sind Filterkonzepte, die ein Überborden der Information von unten nach oben (bottom-up) verhindern. Damit wird die Fiktion behindert, die oberen Hierarchen wüssten um Details. Hingegen muss ein Konzept der Eskalation für Konflikte definiert sein, das Informationen von unten nach oben befördert oder den Zugriff auf Information von oben nach unten erlaubt. Hingegen ist nach der klassischen Organisationstheorie ein "Durchregieren" über mehrere Ebenen hinweg kontraproduktiv.

Hierarchien werden auch allgemein zur Ordnung von Objekten verwendet. Bildlich werden Hierarchien häufig mit einer Pyramide oder einem Stammbaum verglichen. Die Elemente lassen sich übersichtlich in Ebenen anordnen, wobei jedes Element (bis auf das oberste) nur mit einem (Monohierarchie) oder mehreren (Polyhierarchie) Elementen der jeweils nächsthöheren Ebene verbunden ist.

Spezielle Arten von hierarchischen Strukturen, die auch häufig synonym verwendet werden, sind Klassifikationen oder Taxonomien. In der Biologie werden Lebewesen nach verschiedenen Kriterien in einer hierarchischen Systematik geordnet, während der Stammbaum einer Person theoretisch eindeutig bestimmbar ist.




</doc>
<doc id="12650" url="https://de.wikipedia.org/wiki?curid=12650" title="Bertrand Russell">
Bertrand Russell

Bertrand Arthur William Russell, 3. Earl Russell (* 18. Mai 1872 bei Trellech, Monmouthshire, Wales; † 2. Februar 1970 in Penrhyndeudraeth, Gwynedd, Wales) war ein britischer Philosoph, Mathematiker und Logiker. Er unterrichtete unter anderem am Trinity College der Universität Cambridge, der London School of Economics, der Harvard University und der Peking-Universität und war Mitglied der Cambridge Apostles. 1950 erhielt er den Nobelpreis für Literatur.

Russell gilt als einer der Väter der Analytischen Philosophie. Er verfasste eine Vielzahl von Werken zu philosophischen, mathematischen und gesellschaftlichen Themen. Zusammen mit Alfred North Whitehead veröffentlichte er die Principia Mathematica, eines der bedeutendsten Werke des 20. Jahrhunderts über die Grundlagen der Mathematik. Russell war Atheist und Rationalist. Als weltweit bekannter Aktivist für Frieden und Abrüstung war er eine Leitfigur des Pazifismus, auch wenn er selbst kein strikter Pazifist war. Sozialistischen Ideen stand er aufgeschlossen gegenüber.

Bertrand Russell wurde am 18. Mai 1872 in eine Familie der englischen Aristokratie geboren. Sein Großvater John Russell, dem 1861 der Titel Earl Russell verliehen wurde, war britischer Premierminister. Bertrand Russells Vater, John Russell, Viscount Amberley, starb, als Bertrand drei Jahre alt war. Die ebenfalls aus einer Adelsfamilie stammende Mutter "Katherine Louisa Stanley" starb noch früher, 18 Monate vor ihrem Mann, an Diphtherie, ebenso wie Bertrands Schwester "Rachel Lucretia Russell". Die ganze Familie Russell gehörte den liberalen Whigs an, Bertrand Russells Eltern waren aber selbst für dieses Umfeld radikal in ihrer Haltung. So hatten sie einen atheistischen Hauslehrer eingestellt, um ihre Kinder vor dem Einfluss der als Übel angesehenen Religion zu bewahren. Russell hatte einen älteren Bruder, Frank Russell, der den Titel Earl 1878 von seinem Großvater erbte; nach dem Tod des Bruders fiel der Titel 1931 an Bertrand. John Stuart Mill, ein Freund seines Vaters, war – in einem nicht-religiösen Sinn – Bertrand Russells Pate.

Nach dem Tod der Eltern wurde Bertrand Russell mit seinem Bruder von den viktorianischen Großeltern aufgenommen und wuchs auf deren Anwesen Pembroke Lodge, Richmond Park auf. Sein Großvater starb 1878, und so wurde Russell hauptsächlich von seiner Großmutter erzogen, einer religiösen Frau, die jedoch fortschrittliche Ansichten in Bezug auf Wissenschaft und soziale Gerechtigkeit hatte und hiermit einen deutlichen Einfluss auf ihn ausübte.

Bertrand Russell verbrachte eine einsame Jugend. Zu den prägenden Ereignissen zählte er ausgedehnte Spaziergänge im Richmond Park, wo er einen großen Teil seiner Zeit verbrachte. Er wurde von Privatlehrern unterrichtet und beschäftigte sich mit Literatur und Mathematik. In seiner Autobiografie schrieb er, dass er damals unglücklich gewesen sei und mitunter an Selbstmord gedacht habe. Davon habe ihn jedoch der Gedanke an seine Familie und die Absicht, etwas zur Mathematik beizutragen, abgehalten.

Russell erhielt ein Stipendium der Universität Cambridge, der Alma Mater seines Vaters, und studierte dort von 1890 bis 1894 Mathematik. Hier fand er einen Kreis von Freunden und Gesprächspartnern, zu dem unter anderem George Edward Moore, Alfred North Whitehead und John Maynard Keynes gehörten. Auf Empfehlung Whiteheads wurde er Mitglied des konspirativen Debattierklubs der Cambridge Apostles. Mit der akademischen Lehre der Mathematik („von den Vorlesungen hatte ich überhaupt nichts“) und Philosophie („Den größten Teil dessen, was ich dort an Philosophie lernte, erkannte ich nach und nach als falsch“) war er dagegen unzufrieden. Später erhielt er ein Fellowship, das ihm ermöglichte, von 1895 bis 1901 ohne Lehrverpflichtungen forschen zu können.

Während seiner Studienjahre lernte Russell Alys Pearsall Smith kennen, die Tochter von Robert Pearsall Smith und Hannah Whitall Smith, einer in der Heiligungsbewegung einflussreichen amerikanischen Quäkerfamilie. Sie verliebten sich und heirateten im Dezember 1894 – gegen den Willen von Russells Familie. Zuvor hatte die Familie ihm einen Posten in der britischen Botschaft in Paris vermittelt, auch um ihn von seiner Verlobten zu trennen. Doch Russell war in Paris nicht glücklich und entschied sich – obwohl ihn schließlich auch die weltgewandte Alys zu einer Botschafterkarriere drängte – für die theoretische Arbeit als Mathematiker, Philosoph und Schriftsteller.

Auf einem mathematischen Kongress lernte Russell 1900 den italienischen Logiker Giuseppe Peano und dessen Werk kennen. Russell eignete sich Peanos Methoden an, erweiterte sie und legte so den Grundstein für die Principia Mathematica, den Versuch, die gesamte Mathematik auf einen begrenzten Satz von Axiomen und Schlussregeln zurückzuführen. Die Arbeit an diesem monumentalen Werk dauerte von 1902 bis 1913, als der dritte und letzte Band erschien. Russell verfasste die "Principia Mathematica" zusammen mit Whitehead, der zeitweise samt Familie in Russells Haus wohnte.

1911 traf Russell erstmals den aus Wien stammenden Philosophen Ludwig Wittgenstein, der ein Studium in Cambridge aufgenommen hatte, und freundete sich mit ihm an.

Die Ehe Russells scheiterte nach seiner Darstellung schon 1902. Das Ehepaar lebte in der Folge getrennt voneinander. Russell fürchtete berufliche Nachteile und ließ sich daher erst 1921 scheiden, als seine spätere zweite Frau schwanger wurde. Er hatte während dieser Zeit mehrere Affären, unter anderem mit Lady Ottoline Morrell, mit der er bis zu ihrem Lebensende in Freundschaft verblieb, wie zahlreiche Briefe bezeugen.

Seit Ende des 19. Jahrhunderts war Russell politisch tätig. Er setzte sich wie seine Mutter für das Frauenwahlrecht ein und kandidierte bei einer Nachwahl im Jahr 1907 – wenn auch erfolglos – für das House of Commons.

Ein einschneidendes Ereignis in Russells Leben war der Erste Weltkrieg. Ab 1914 stellte Russell seine mathematische Forschung zurück und begann, sich als Aktivist und Autor für Frieden und Kriegsdienstverweigerung einzusetzen. Dass er wegen eines Flugblatts zu einer Geldstrafe verurteilt worden war, nahm die Universität Cambridge zum Anlass, ihm die Professur zu entziehen. Er wurde später zu einer sechsmonatigen Gefängnisstrafe verurteilt, weil er in einer Antikriegsdienst-Zeitschrift die Möglichkeit erwogen hatte, dass US-amerikanische Soldaten in England als Streikbrecher eingesetzt werden könnten. Allerdings wurde Russell ermöglicht, im Gefängnis zu lesen und zu schreiben, und so verfasste er während seiner Haft mehrere Bücher. Russell störte seine Inhaftierung kaum, weil er so seine „Selbstachtung“ behielt und Gelegenheit bekam, „über Dinge nach[zu]denken, die weniger schmerzlich waren als die allgemeine Zerstörung.“

Nach dem Ersten Weltkrieg unternahm Russell mehrere Reisen. 1920 besuchte er mit einer Delegation der Labour Party die Sowjetunion und hatte unter anderem die Möglichkeit zu einem Gespräch mit Lenin, welcher ihn stark enttäuschte. Russell kehrte desillusioniert zurück und äußerte sich äußerst negativ über den russischen Sozialismus. So schrieb er in einem Brief: „Bis zum einfachsten Bauern herunter sind sie ein Volk von Künstlern; die Bolschewiken haben sich zum Ziel gesetzt, sie so weit wie möglich zu industrialisieren und zu Yankees zu machen.“ Russell, der zuvor mit dem Sozialismus sympathisiert hatte, war fortan ein ausgesprochener Gegner des Kommunismus.

1920 und 1921 unternahm Russell eine Reise nach China und Japan. Die Universität Peking hatte ihm, der in Cambridge entlassen worden war, eine Gastprofessur angeboten. Russell, den viele Aspekte der chinesischen Kultur tief beeindruckten, fasste die Erlebnisse seiner Reisen in mehreren Büchern zusammen.

Begleitet wurde Russell auf seiner Asienreise von seiner damaligen Geliebten Dora Black. Sie pflegte ihn gesund, als er in China aufgrund einer Lungenentzündung dem Tode nah war. Bei der gemeinsamen Rückkehr nach England war Dora schwanger, woraufhin sich Bertrand Russell 1921 von seiner Frau Alys Pearsall Smith scheiden ließ und kurz darauf Dora Black heiratete.

Gemeinsam gründeten sie für ihre Kinder Kate und John Russell 1927 die libertäre experimentelle Beacon Hill School. Während dieser Jahre arbeitete Russell überwiegend als Schriftsteller und verfasste Bücher zu philosophischen und pädagogischen Themen, aber auch populärwissenschaftliche Abhandlungen über zeitgenössische physikalische Theorien wie Quantenphysik und Relativitätstheorie.

Auf einer Vortragsreise durch die USA lernte Russell 1927 die Schriftstellerin und spätere Gestalttherapeutin Barry Stevens (damals noch Barry Fox) kennen. In Stevens’/Fox’ Worten standen sie sich drei Jahre lang sehr nahe. Stevens’ Tochter Judith besuchte eine Zeit lang Russells Beacon Hill School. Russell schrieb in der Zeit von 1927 bis 1932 34 Briefe an Barry Fox/Stevens. Die Briefe befinden sich mittlerweile im Bertrand Russell-Archiv der McMasters University, Hamilton, Ontario, Kanada.

Auch Russells Ehe mit Dora Black scheiterte schließlich, und 1936 heiratete Russell – bereits 64-jährig – Patricia Helen Spence („Peter“ genannt). Mit ihr hatte er einen Sohn, Conrad Russell. Die Familie zog in die USA, wo Russell zunächst an den Universitäten von Chicago und Los Angeles lehrte.

1939 verließ Russell Los Angeles, um am City College of New York eine Stelle als Dozent anzunehmen. Obwohl er in New York bereits zum Professor ernannt worden war, wurde die New Yorker Universität 1940 dazu gezwungen, ihre Ernennung zurückzuziehen. Grund hierfür waren Proteste fundamentalistischer Christen und Politiker, die der Ansicht waren, Russell spreche sich in seinen Schriften gegen Religion und somit für Unmoral aus und sei deshalb ungeeignet für die Aufgabe, Logik und Grundlagen der Mathematik zu lehren. Besonders kritisierten diese Kreise Russells Buch "Ehe und Moral".

Studenten, Mitglieder der Fakultät und mehrere Intellektuelle (darunter John Dewey und Albert Einstein) protestierten vergeblich gegen diese Einmischung in die Freiheit der Lehre. Es kam zu einem Prozess gegen die Universität, den die Mutter einer Studentin angestrengt hatte. Das umstrittene Urteil untersagte der Universität eine Berufung Russells, weil dieser die Moral der Studenten gefährde, Ehebruch und das „Verbrechen der Homosexualität“ befürworte.

Dies brachte Bertrand Russell, der – wie er in seiner Autobiografie schrieb – zeitweise den Eindruck hatte, nicht mehr öffentlich auftreten zu können, ohne einen „katholischen Lynchmob“ hervorzurufen, in eine finanziell schwierige Situation, da er für die Ausbildung seiner Kinder aufkommen musste. Ihm half in dieser Zeit Albert C. Barnes, der ihm eine Stelle als Dozent bei der Barnes Foundation gab. Allerdings überwarf sich Russell schon bald mit dem als exzentrisch geltenden Barnes, der die Qualität seiner Vorlesungen bemängelte und ihn deshalb vorzeitig aus seinem Fünfjahresvertrag entließ. Barnes unterlag später vor Gericht und musste Nachzahlungen leisten. Die kritisierten Vorlesungen dienten als Grundlage für einen Großteil des 1945 veröffentlichten Werkes History of Western Philosophy ("Philosophie des Abendlandes"), das sehr erfolgreich war und Russell für viele Jahre finanziell absicherte.

Anders als im Ersten Weltkrieg nahm Russell im Zweiten Weltkrieg keine pazifistische Position ein. Kurz nach Kriegsende sprach er sich sogar für einen Präventivkrieg gegen die Sowjetunion aus, welche noch nicht über Atomwaffen verfügte. Damit wollte er einen die Menschheit vernichtenden Atomkrieg verhindern (siehe Politisches und gesellschaftliches Engagement).

1944 kehrte Russell zurück nach England, um wieder am Trinity College in Cambridge zu lehren. In den folgenden Jahren arbeitete er zudem für die BBC an Rundfunkübertragungen.

1948 überlebte er auf dem Weg zu einem Vortrag einen Flugzeugabsturz in Norwegen.

1949 erhielt Russell den Order of Merit, und 1950 wurde ihm der Nobelpreis für Literatur verliehen, insbesondere für "Ehe und Moral", für das er wenige Jahre zuvor stark kritisiert worden war.

Nachdem auch Russells Ehe mit Patricia Helen Spence mit einer Scheidung geendet hatte, ging er 1952 eine vierte Ehe mit Edith Finch ein, die bis an sein Lebensende hielt.

Der mit 78 Jahren weltweit bekannte und vielfach ausgezeichnete Russell zog sich nach 1950 nicht aus der Öffentlichkeit zurück. Ihn bewegte vor allem ein möglicher Dritter Weltkrieg als eine große Gefahr für die Menschheit. So war er die treibende Kraft des Russell-Einstein-Manifests und engagierte sich in verschiedenen politischen Krisen des Kalten Krieges als Vermittler zwischen den Staatschefs. Er war zeitweise Präsident der Campaign for Nuclear Disarmament. Mit anderen Mitgliedern der Organisation wurde er 1961 angeklagt, zum Widerstand gegen die Staatsgewalt aufgerufen zu haben und – mit 89 Jahren – zu einer zweimonatigen Freiheitsstrafe verurteilt. Diese Strafe wurde „auf Grund ärztliche[r] Atteste auf eine Woche herab“gesetzt.

1963 gründete er die Bertrand Russell Peace Foundation. Im Russell-Tribunal untersuchte er US-amerikanische Kriegsverbrechen in Vietnam.

Hochbetagt schrieb er seine Autobiografie, die von 1967 bis 1969 in drei Bänden erschien.

Am 2. Februar 1970 starb Bertrand Russell mit 97 Jahren in Penrhyndeudraeth (Wales) an Influenza.

Bertrand Russells Werk lässt sich grob in drei Themen aufteilen, auf die er – trotz vieler Überschneidungen – in verschiedenen Phasen seines Lebens den Schwerpunkt seiner Arbeit legte. Während er in der ersten Hälfte seines Lebens hauptsächlich an den Grundlagen der Mathematik arbeitete, wandte er sich nach Fertigstellung der "Principia Mathematica" verstärkt philosophischen Fragen zu. Im letzten Drittel seines Lebens spielte sein politisches Engagement die Hauptrolle.

Bei seiner Arbeit auf dem Gebiet der mathematischen Logik stützte sich Russell unter anderem auf Gottfried Wilhelm Leibniz, Giuseppe Peano und Gottlob Frege. Sein erstes mathematisches Werk, "An Essay on the Foundations of Geometry", war durch Immanuel Kants Auffassung von Zeit und Raum beeinflusst, die zu jener Zeit weitgehend akzeptiert war. Später distanzierte sich Russell von diesem Werk, das viel mehr Lob bekommen habe als verdient gewesen sei, und auch von Kant, dessen Philosophie er für unvereinbar mit dem Raumzeitbegriff der Relativitätstheorie hielt.

In Bezug auf die Mengenlehre wurde Russell durch das nach ihm benannte Paradoxon (Russellsche Antinomie) berühmt. Dieses Paradoxon tritt auf, wenn man die Menge aller Mengen betrachtet, die sich nicht selbst als Element enthalten. Diese Menge enthält sich genau dann selbst, wenn sie sich nicht selbst enthält, was ein Widerspruch ist, der zur Folge hat, dass die Menge aller sich selbst nicht enthaltenden Mengen nicht existieren kann. Eine populäre Version dieses Paradoxons ist unter dem Namen Barbier-Paradoxon bekannt.

Mit dieser Entdeckung war Gottlob Freges Grundannahme, jedem Begriff entspreche eine Menge als Begriffsumfang, widerlegt, weil es zu dem Begriff "sich selbst nicht enthaltende Menge" keine entsprechende Menge gibt. Das bedeutete das Ende der "naiven Mengenlehre". Um die von ihm entdeckte Antinomie zu beheben, entwickelte Russell die Typentheorie, die in einer ersten Version in "Principles of Mathematics" (1903) veröffentlicht wurde und die er in seinem Werk "Principia Mathematica" (1910–1913) weiterentwickelte. Seine Typentheorie hat sich in der Mengenlehre nicht dauerhaft durchgesetzt, da sich die Zermelo-Fraenkel-Mengenlehre als leistungsfähiger erwies.

Zusammen mit Alfred North Whitehead schrieb Russell mit Principia Mathematica eines der wichtigsten Werke mathematischer Grundlagenforschung nach den Erschütterungen der Mathematik Anfang des 20. Jahrhunderts. Ziel war es, alle mathematischen Wahrheiten aus einem Satz von Axiomen und Schlussregeln zu konstruieren. Russells Schwerpunkt lag auf philosophischen, Whiteheads auf mathematischen Problemen. Ein angekündigter vierter Band über die Grundlagen der Geometrie wurde nicht vollendet.

Nach dem im Gefängnis verfassten Buch "Introduction to Mathematical Philosophy" (1919), in dem er hauptsächlich frühere Arbeiten und deren philosophische Bedeutung erklärt, wandte sich Russell von Problemen der Mathematik und Logik ab.

Bertrand Russell gilt zusammen mit George Edward Moore als einer der Begründer der Analytischen Philosophie. Sein erster bedeutender Beitrag zur Sprachphilosophie war die Kennzeichnungstheorie, die er 1905 im Essay "On Denoting" vorstellte. Darin vertrat er eine Philosophie der idealen Sprache und beeinflusste den Logischen Positivismus.

Ein weiterer Beitrag Russells ist die Entwicklung in Richtung des Logischen Atomismus innerhalb der Analytischen Philosophie. Seine Hauptaussage ist, dass es eine grundlegende Sprache gebe, auf die gewöhnliche Gesetze zurückgeführt werden könnten und die aus atomischen, nicht weiter reduzierbaren logischen Fakten bestehe. Sein Aufsatz "The Philosophy of Logical Atomism" (1918/1919) sowie der Tractatus Logico-Philosophicus Ludwig Wittgensteins, mit dem Russell befreundet war, sind grundlegende Werke jenes logischen Atomismus.

Ludwig Wittgenstein war zunächst Russells Schüler in Cambridge gewesen, später wurde er zu seinem Gesprächspartner und Freund. Russell erkannte die außergewöhnliche Begabung Wittgensteins rasch und motivierte ihn in Phasen des Zweifelns, seine Arbeit weiterzutreiben.

Zwar hatte Russell mehrere Bücher über Ethik und Moral veröffentlicht, erkannte aber Ethik nicht als Gebiet der Philosophie im eigentlichen Sinne an, da ihre Erkenntnisse nicht als "Wissen" bezeichnet werden könnten. In jungen Jahren durch George Edward Moores Principia Ethica beeinflusst, wonach ethische Fakten objektiv sein können, war er später eher ein Anhänger David Humes subjektiver Ethik und vertrat die Ansicht, dass Ethik sich von den „Leidenschaften“ ableite. Es gebe keine zuverlässige Methode, von Leidenschaften zu Erkenntnis zu gelangen. Er schätzte sein Leben lang das methodische Vorgehen der modernen Wissenschaften als zuverlässige Quelle für Erkenntnis.
Russell änderte verschiedentlich seine Meinung. So gab er angesichts des Zweiten Weltkrieges seinen rigorosen Pazifismus auf. Das erste der 1951 im "New York Times Magazine" veröffentlichten "Zehn Gebote eines Liberalen" lautete: „Fühle dich keiner Sache völlig gewiss!“

Obwohl Russell anfänglich von seiner gläubigen Großmutter erzogen wurde, fühlte er sich nie als Christ. Er glaubte aber in seiner Jugendzeit – in einer Phase, als er sich zu Georg Wilhelm Friedrich Hegels Philosophie hingezogen fühlte – an die Existenz Gottes. Dies änderte sich, unter anderem durch den Einfluss der Philosophie John Stuart Mills, und bald wurde Russell in der Öffentlichkeit als Agnostiker oder Atheist bezeichnet. Er selbst betrachtete sich als Skeptiker und stellte klar, er sei Agnostiker in dem Sinne, dass man die Nichtexistenz irgendeines Dinges – also auch eines „Gottes“ oder etwa der homerischen Götter – nicht beweisen könne. Es sei allerdings die Aufgabe einer Religion, ihrerseits zunächst zu beweisen, dass Gott existiere (vgl. dazu seine bekannte Analogie „Russells Teekanne“). Dies sei aber bislang nie geglückt. Russell bestritt so auch die Gültigkeit eines der verbreitetsten Argumente für die Existenz eines Schöpfergottes, das der angeblichen Notwendigkeit einer Ursache der Welt: „Wenn "alles" eine Ursache haben muss, dann muss auch Gott eine Ursache haben. Wenn es aber etwas geben kann, das keine Ursache hat, dann kann das ebensogut die Welt wie Gott sein, so dass das Argument bedeutungslos wird“. Gegen Ende seines Lebens bezeichnete sich Russell daher auch selbst als Atheist.

Seine Ansichten über Religion hat er in dem langen Essay "Warum ich kein Christ bin" (1927; erw. 1957) zusammengefasst. Religion im Allgemeinen, insbesondere aber das Christentum, hielt Russell für ein Übel, eine „Krankheit, die aus Angst entstanden ist“. Besonders Islam, Judentum und Christentum seien in ihrem Kern überdies „Sklavenreligionen“, die bedingungslose Unterwerfung verlangten: „Die ganze Vorstellung vom herrschenden Gott stammt aus den altorientalischen Gewaltherrschaften. Es ist eine Vorstellung, die eines freien Menschen unwürdig ist.“ Russell kritisierte auch in weiteren Texten die Christliche Ethik, insbesondere die Sexualethik, scharf, die er als „Vergewaltigung der menschlichen Natur“ bezeichnete.

Mit achtzig Jahren machte Russell jedoch noch eine Erfahrung, die ihn zu einer sehr überraschenden Selbsterkenntnis führte: Anlässlich eines Griechenland-Besuchs empfand er unzweideutig, dass er sich in einer alten christlichen Kirche „weit mehr zu Hause“ fühlte als in Gebäuden der „heidnischen Antike“. Dabei wurde ihm „klar, dass christliches Lebensgefühl "(Christian outlook)" weit mehr Einfluss auf mich besaß, als ich geglaubt hatte.“ Den wesentlichen Unterschied zwischen antiker und christlicher Welt sah Russell im „Fehlen jeglichen Sündenbegriffs“ bei den alten Griechen. Er schrieb: „Ich bemerkte nunmehr überrascht, dass dieser Begriff auch meine Gefühle beherrschte.“ Einen Einfluss dieser Erkenntnis seiner eigenen kulturellen Prägung auf seine Anschauungen "(beliefs)" zur Religion verneinte er jedoch.

Schon in frühen Jahren befasste sich Russell mit gesellschaftlichen Themen. So schrieb er sein erstes Buch nicht etwa über ein mathematisches Thema, sondern über die zu der Zeit revolutionäre deutsche Sozialdemokratie (1896) nach einer Berlin-Reise, auf der er unter anderem mit August Bebel und Wilhelm Liebknecht zusammengetroffen war. Im Verlauf seines Lebens veröffentlichte er noch viele gesellschaftskritische und philosophische Studien; schließlich wurde ihm „als eine Anerkennung für seine vielseitige und bedeutungsvolle Verfasserschaft, worin er als Vorkämpfer der Humanität und Gedankenfreiheit hervortritt“, 1950 der Nobelpreis für Literatur verliehen.

Russell beließ es nicht bei der Theorie. Er setzte sich im frühen 20. Jahrhundert für das Frauenwahlrecht und für soziale Gerechtigkeit ein. In "Proposed Roads to Freedom: Socialism, Anarchism and Syndicalism" (1919) sprach er sich für eine moderate Form des Syndikalismus, den englischen Gildensozialismus, aus.

Als Pazifist und Friedensaktivist war Russell seit dem Ersten Weltkrieg bekannt. Ein Pazifist, der Gewalt kategorisch ablehnte, war Russell, der jeder Ideologie gegenüber kritisch eingestellt war, jedoch nicht. Er engagierte sich aber in pazifistischen Organisationen, schrieb einen offenen Brief an den amerikanischen Präsidenten Woodrow Wilson und setzte sich später für eine Organisation zur Unterstützung von Kriegsdienstverweigerern ein. Wegen eines Artikels für eine Zeitschrift dieser Organisation verbüßte er eine Haftstrafe von sechs Monaten. Seine Anstellung am Trinity College hatte er aufgrund seiner politischen Aktivität schon vorher verloren.

Nach dem Ersten Weltkrieg kandidierte er bei Wahlen 1922 und 1923 für die Labour Party, blieb aber erfolglos. Seine gesellschaftliche Aktivität konzentrierte sich bald auf die 1927 mit seiner damaligen Frau Dora Russell gegründete libertären Internatsschule Beacon Hill School, ein Projekt, das der Unzufriedenheit der Russells mit allen damaligen Schulmodellen entsprang. Bertrand Russell sah das in der Öffentlichkeit viel beachtete Experiment der neuen Schule, die nach der Trennung von seiner Frau alleine weitergeführt wurde, im Rückblick kritisch und konstatierte, dass die Freiheit der Kinder in der Schule geringer war, als es vorgegeben wurde.

Bertrand Russell hatte sich 1935 in "Which Way to Peace" (ein Buch, dessen Wiederauflage er bis an sein Lebensende untersagte) noch für eine Appeasement-Politik gegenüber Nazideutschland ausgesprochen. Von dieser Position rückte er 1940 ab, weil er einsah, dass Adolf Hitler besiegt werden musste („Ich stelle fest, dass ich in diesem Krieg meine pazifistische Einstellung nicht beibehalten kann“).

Er gehörte mit Victor Gollancz, George Bell und anderen zu den Unterzeichnern eines am 12. September 1945 in mehreren Londoner Tageszeitungen erschienenen Aufrufs gegen die Vertreibung von Deutschen aus Ostmitteleuropa.

Ein wichtiges Ereignis für Russell, das sein weiteres Leben bestimmen sollte, war der Abwurf der ersten Atombombe 1945. Russell sah die gesamte Menschheit bedroht, wenn die kommunistische Sowjetunion ebenfalls über die entsprechende Technologie verfügen würde. Er sagte voraus, dass die Atombomben billiger werden und dass es in nicht allzu ferner Zukunft eine Wasserstoffbombe geben würde. Seiner Meinung nach (und nach der einer Reihe weiterer westlicher Intellektueller jener Zeit) war es notwendig, eine Weltregierung unter Führung der USA zu bilden.

In mehreren Artikeln – unter anderem in der Abhandlung "Humanity’s Last Chance", die im Oktober 1945 von der Zeitschrift "Cavalcade" veröffentlicht wurde – schlug er vor, mit Hilfe der Atombombe einen Präventivkrieg gegen die Sowjetunion innerhalb der nächsten zwei Jahre zu führen, um diese zu zwingen, eine Weltregierung unter US-amerikanischer Führung zu akzeptieren.

Als 1949 die ersten sowjetischen Atombombentests erfolgten, modifizierte Russell seine Einstellung. Nun sah Russell die einzige Chance zum Überleben der Menschheit darin, einen Dritten Weltkrieg zu verhindern, und widmete diesem Ziel einen Großteil seiner Zeit.

1955 verfasste Russell mit Albert Einstein und anderen namhaften Wissenschaftlern das Russell-Einstein-Manifest, in dem an die Verantwortung von Wissenschaft und Forschung appelliert wurde. Hierauf basierten 1957 die "Pugwash Conferences on Science and World Affairs", wo renommierte Wissenschaftler Fragen der atomaren Bedrohung und Vorschläge zur globalen Sicherheit debattieren.

Als Präsident der 1958 gegründeten Campaign for Nuclear Disarmament engagierte er sich in vielen Interviews, Schriften und Vorträgen für den Frieden. Er versuchte in Briefwechseln, die Präsidenten Dwight D. Eisenhower und Nikita Sergejewitsch Chruschtschow zur Kooperation und zur Abrüstung zu bewegen.

1962 griff Russell durch Telegramme an John F. Kennedy, Chruschtschow, den UN-Generalsekretär Sithu U Thant und den britischen Premier Harold Macmillan in die Kubakrise ein, als die Welt am Rand eines Atomkrieges stand. Chruschtschow schrieb Russell eine lange Antwort, die von der sowjetischen Nachrichtenagentur TASS veröffentlicht wurde und eigentlich an Kennedy und die westliche Welt gerichtet war. Chruschtschow lenkte schließlich ein, wodurch ein Atomkrieg abgewendet wurde.

1962 forderte Russell in einem Telegramm an Chruschtschow, das auch von François Mauriac und Martin Buber unterzeichnet wurde, die Wiederherstellung sämtlicher Bürgerrechte für sowjetische Juden. Russells privater Briefwechsel mit Chruschtschow zu diesem Thema wurde im Februar 1963 in der britischen und der sowjetischen Presse sowie von Radio Moskau veröffentlicht.

Überdies gründete Russell 1963 die Bertrand Russell Peace Foundation, die auch nach seinem Tod den Einsatz für Frieden und Menschenrechte gewährleisten sollte. Er gehörte zur Opposition gegen den Vietnamkrieg und untersuchte noch im hohen Alter im Rahmen der Russell-Tribunale seit 1966 unter anderem mit Simone de Beauvoir, Jean-Paul Sartre, Günther Anders und Peter Weiss Kriegsverbrechen der USA in Vietnam.

1908 wurde Russell als Mitglied („Fellow“) in die Royal Society aufgenommen, die ihm 1934 die Sylvester-Medaille verlieh. Weitere Auszeichnungen:

Bertrand Russell hat in seinem Leben eine große Anzahl von Büchern, Essays, Pamphleten und Briefen veröffentlicht, die hier nicht komplett wiedergegeben werden kann. Für eine detaillierte Bibliographie siehe Literatur.








</doc>
<doc id="12653" url="https://de.wikipedia.org/wiki?curid=12653" title="Gießen (Metall)">
Gießen (Metall)

Das Gießen (auch der Guss oder das Werk) von Metallen und Legierungen ist ein Fertigungsverfahren, bei dem Werkstücke (Gussstücke) aus flüssigem Metall – der Schmelze – hergestellt werden. Beim häufigsten Verfahren – dem in Gießereien vorgenommenen Formguss - wird die Schmelze in eine Hohlform gefüllt, in der sie anschließend erstarrt. Die Innenfläche der Hohlform ist das Negativ der Außenfläche des Gussstücks.

Das Gießen zählt zur Hauptgruppe des Urformens und ist innerhalb dieser die bedeutendste Verfahrensgruppe. Für das Gießen nicht-metallischer Werkstoffe siehe Gießen (Urformen).

Das Einfüllen der Schmelze in die Formen wird als Abguss bezeichnet. Zur gesamten Prozesskette des Formgusses vom Rohmaterial bis zum Gussstück zählt auch der Formenbau, die Fertigung der Modelle, das Schmelzen der Werkstoffe und die Schmelzebehandlung, sowie die Nachbehandlung: Entformen, Wärmebehandeln und das Gussputzen. Zu letzterem zählt die Entfernung von Anschnitt und Speisern, die nur zur Zuführung von Schmelze dienen, aber nicht Bestandteil des Gussteils sind, das Entsanden, Entgraten, Entzundern und das Ausbessern von Gussfehlern.

Bei der Eisengewinnung aus Erz wird das flüssige Metall zu Barren oder Masseln (Blockguss) oder zu endlosen Strängen (Strangguss) gegossen. Die Weiterverarbeitung der Masseln erfolgt in den Gießereien, wobei das Eisen wieder eingeschmolzen wird. Von den dort angewendeten Gießverfahren sind die wichtigsten der Sandguss, wobei Formen aus Sand genutzt werden, und der Druckguss, bei dem die Schmelze von einem Kolben in eine mehrfach genutzte Dauerform gepresst wird.

Die wichtigsten Gießwerkstoffe für Formguss sind Gusseisen mit einem Massenanteil an der Gesamtproduktion in Gießereien von etwa 75 % und Aluminiumlegierungen. Die für das Gießen wichtigen Werkstoffeigenschaften werden zur Gießbarkeit zusammengefasst.

Das Gießen stammt aus der Kupferzeit, der Übergangszeit von der Jungsteinzeit zur Bronzezeit. Erste Metalle waren schon in der Steinzeit bekannt. In gediegener, also metallischer Form kamen in der Natur Gold, Silber und Kupfer vor. Kupfer wurde anfangs durch Hämmern bearbeitet, wodurch es sehr spröde und brüchig wurde. Daher wurde es zunächst nur als Schmuck genutzt; für Werkzeuge waren alle bekannten Metalle wegen der geringen Härte und Festigkeit nicht brauchbar. Mit der Entwicklung von Schmelzöfen (sogenannte Tiegelöfen) um 3000 v. Chr. – zuerst in China und Indien – wurde es möglich, Metalle zu gießen und auch aus Kupfererz Kupfer zu gewinnen. Kupfer bereitete jedoch beim Gießen Probleme, da es dazu neigt, Blasen zu bilden. Außerdem weist es einen hohen Schmelzpunkt auf und ist relativ zähflüssig. Seit 1500 v. Chr. wurden in Ägypten daher Blasebälge statt Blasrohre eingesetzt. Der entscheidende Durchbruch gelang mit der Technik des Legierens: Durch Zulegieren von Zinn entstand Bronze, die sich ausgezeichnet vergießen lässt, deutlich härter und fester ist als Kupfer und sich daher auch für Werkzeuge eignet. Damit löste Bronze den Stein als wichtigsten Werkzeugwerkstoff ab. Bronze, Silber und Gold wurden für den Kunstguss genutzt, Bronze zusätzlich für die Herstellung von Waffen und Werkzeugen. Diese Metalle wurden zwar auch durch Schmieden und Treiben bearbeitet, das wichtigste Verfahren aber war das Gießen. Da sich der Schmelzofen aus dem Keramikofen entwickelte, waren die frühen Metallverarbeiter auch mit dem Bau keramischer Formen vertraut.
In der Antike wurde Bronze durch Eisen als wichtigster Werkstoff abgelöst, das sich in Europa bis ins Mittelalter nicht gießen ließ, während die Chinesen die Technik seit etwa 500 v. Chr. beherrschten. Im Kunsthandwerk wurde nach wie vor der Bronzeguss genutzt und weiterentwickelt. Die Bronzestandbilder wurden im Laufe der Antike immer größer und komplexer. Anfangs wurden nur massive Figuren von etwa 30 cm Höhe hergestellt. Später gelang der Guss von hohlen Gussstücken (Hohlguss), was viel Material einsparte und so bedeutend größere Werkstücke ermöglichte, darunter lebensgroße Reiterstandbilder. Angewandt wurde das Wachsausschmelzverfahren: Über eine Grundmasse aus Ton wurde ein Mantel aus Wachs gelegt, in den die zu fertigende Form eingearbeitet wurde. Danach wurde diese Schicht mit einer weiteren Lage aus Ton umgeben. Die Schmelze wurde in die Zwischenlage aus Wachs gegossen, wodurch dieses verbrannte. Bei einer Weiterentwicklung des Verfahrens wurden die Standbilder in einzelnen Teilen wie Armen oder Köpfen vergossen und zusammengelötet. Wenn die einzelnen Teile misslangen, musste nicht die gesamte Form erneut gefertigt werden.
Im Hochmittelalter gelang dank der neuen Schachtöfen, den Vorläufern der Kupolöfen, erstmals die Herstellung von Gusseisen (gießbares, aber nicht schmiedbares Eisen), das nun im Kunstguss genutzt wurde. Außerdem wurden die Blasebälge immer häufiger durch Wasserkraft angetrieben. Teils wurde in den gleichen Schachtöfen auch Gusseisen und schmiedbares Eisen hergestellt. Teilweise wurden die Formen in der Gießerei gebaut und zu den Öfen transportiert, teils wurde das Gusseisen in den Gießereien erneut eingeschmolzen. Besondere Bedeutung erlangte der Glockenguss.

Die ersten Kanonen der frühen Neuzeit wurden noch aus schmiedeeisernen Blechstreifen geschmiedet, was jedoch aufwendig war. Danach gewann der Bronzeguss an Bedeutung. Die Bronzekanonen wurden zunächst im Vollguss hergestellt und dann gebohrt, später wurden sie über einem Kern gegossen und die vorhandene Bohrung nur noch ausgebohrt, was Gussmasse und Bearbeitungszeit einsparte. Kurz vor der Industrialisierung wurden auch Kanonen aus Gusseisen über einem Kern gegossen und danach ausgebohrt.

Während der Industrialisierung wurde Gusseisen zum wichtigen Konstruktionswerkstoff. Zum Teil wurden ganze Brücken daraus gefertigt. Gegen Ende des 19. Jahrhunderts wurden mit Aluminium und Magnesium neue Gusswerkstoffe entdeckt. Bereits um 1900 wurden Serienteile für die Automobilindustrie aus Aluminium vergossen, größere Anwendungen gab es aber erst Mitte des 20. Jahrhunderts.

In den 1970er Jahren wurde es durch die Entwicklung moderner FEM-Simulation möglich, den Gießprozess zu simulieren und zu optimieren.

Viele Fertigungsverfahren lassen sich alternativ anwenden. Das Gießen konkurriert vor allem mit dem Umformen (Schmieden) und dem Zerspanen (Drehen, Bohren, Fräsen, Schleifen). Diese benötigen allerdings Rohmaterial in fester Form, das üblicherweise durch Gießen hergestellt wurde. Beim Gießen sind auch sehr komplexe Formen herstellbar und es eignet sich für große Serien. Kleine und mittelgroße Werkstücke werden eher geschmiedet oder zerspant.

Werkstoffe mit sehr hohem Schmelzpunkt werden häufig pulvermetallurgisch hergestellt. Statt einer Schmelze wird dabei Metallpulver genutzt. Bei Einzelstücken ist das 3D-Drucken eine Alternative.

Das Gießen komplexer Werkstücke hat gegenüber anderen Produktionsmethoden den Vorteil, dass es nur relativ wenige Prozessschritte umfasst und den Materialverbrauch reduziert, der z. B. beim Fräsen entsteht. Auch bei gewichtsoptimierter Bauteilgeometrie, wie sie im Flugzeugbau oder in der Medizintechnik beim Titan­guss erforderlich ist, erlangt das Gießen gegenüber der Zerspanung eine immer größere Bedeutung. Der Anteil der Produktion der Gießereien an der Gesamtproduktion des produzierenden Gewerbes in Deutschland macht zwar nur etwa ein Prozent aus, es gibt jedoch zahlreiche Branchen, die die Gießereien als Zulieferer benötigen. Hauptabnehmer sind mit über 50 % die Fahrzeugindustrie (mit stark steigender Tendenz in den letzten Jahrzehnten) und der Maschinenbau. Hingegen ging der Bedarf der Montanindustrie an Gussteilen stark zurück.

Die Anzahl der Beschäftigten lag 2011 bei 78.000 in Deutschland, die in etwa 500 Gießereien arbeiteten. Die Produktionsmenge wird in der Gießerei als Gesamtmasse der Werkstücke angegeben. 2011 lag sie für Deutschland bei 5,8 Millionen Tonnen. Die weltweite Jahresproduktion an Gussteilen betrug 2013 über 100 Millionen Tonnen. 2013 war China der wichtigste Produzent mit 42,5 Millionen Tonnen, gefolgt von den USA (12,8 Millionen Tonnen) und Indien (9,3 Millionen Tonnen). Danach folgen Japan, Deutschland und Russland mit 5,3 bis 4,3 Millionen Tonnen.
Allein die deutschen Fahrzeugbauer beziehen etwa 3 Millionen Tonnen aus der Produktion der deutschen Gießereien. Das zeigt, dass die Branche von der Durchsetzung der Elektromobilität, die zum Wegfall schwerer mechanischer Komponenten (Motor, Getriebe usw.) führen wird, stark betroffen sein dürfte. Auch verschiebt sich der Schwerpunkt der Automobilproduktion nach Asien. Ein weiterer wichtiger Trend ist die Entwicklung von Leichtbaugussteilen. Der Handformguss von großen Einzelstücken und Kleinserien ist in Deutschland aus Effizienzgründen weitgehend eingestellt worden, was dazu führt, dass große, von Hand gegossene Einzelstücke heute oft aus dem Ausland (z. B. Brasilien) bezogen werden müssen.

Die erreichbaren Genauigkeiten sind im Allgemeinen gering. Die ISO-Toleranzen liegen zwischen IT16 bis IT11 (kleine sind genauer), mit Sondermaßnahmen auch IT10. Die Genauigkeiten beim Schmieden sind vergleichbar (Präzisionsschmieden bis IT8) beim Zerspanen deutlich besser mit IT7 bis IT6, weshalb Gussteile häufig spanend nachbearbeitet werden. Weiterentwicklungen in der Gießereitechnik versuchen diese Nacharbeit möglichst gering zu halten. Die Oberflächenrauheit ist wie auch beim Schmieden relativ schlecht mit mittleren Rautiefen von 63 µm bis 1000 µm, bei Zerspanen liegen sie zwischen 10 µm und 0,25 µm.

Die Seriengussverfahren wie das Druckgießen sind sehr produktiv. Demgegenüber ist das Vakuumgießen ein Verfahren für den Präzisionsguss von Einzelstücken, Kleinserien oder Prototypen aus Kunststoff.

Die Materialausnutzung ist beim Gießen wie auch beim Schmieden sehr gut. Nur etwa 10 % des Materials geht verloren, beim Zerspanen wird teilweise über die Hälfte des Rohteils in Form von Spänen entfernt. Trotz der großen Energiemengen zum Schmelzen ist das Gießen daher wie auch das Schmieden sehr energieeffizient, wenn die gesamte Prozesskette zum fertigen Bauteil betrachtet wird, während beim Zerspanen etwa die dreifache Energie benötigt wird.

Mit dem Gießen ist eine große Bandbreite an Werkstücken herstellbar. Manche Kleinteile wiegen nur wenige Gramm, die größten über 200 Tonnen. Die Vielfalt der herstellbaren Formen ist kaum begrenzt, vor allem Freiformflächen, also dreidimensionale gekrümmte Flächen sind möglich. Wichtige Produkte sind Glocken (hergestellt durch Glockenguss), Implantate und Prothesen, Bronzestandbilder (per Bronzeguss gefertigt) und sonstiger Kunstguss, Gehäuse für Pumpen, Getriebe und Elektromotoren, Impeller, Schiffspropeller und Turbinenschaufeln für die Luft- und Raumfahrtindustrie aus Titan oder Nickel. Für die wichtigste Abnehmerbranche der Gießerei, die Automobilbranche, werden Autofelgen, Motorblöcke, Kurbelwellen, Zylinderköpfe, Abgaskrümmer und viele weitere Teile gefertigt, häufig per Sandguss (mit Gusseisen) oder Druckguss (mit Aluminium).

Werkstoffe, die in der Gießerei genutzt werden, werden als Gusswerkstoff oder Gusslegierung bezeichnet, ihre Eignung zum Gießen als Gießbarkeit.

Der mit Abstand wichtigste Gusswerkstoff mit einem Anteil von 75 % ist das Gusseisen, eine Eisenlegierung mit mindestens 2 % Kohlenstoff (meist um 4,3 %). Es hat mit 1200 °C einen deutlich geringeren Schmelzpunkt als Stahl (1500 °C), der unter 2 % Kohlenstoff enthält. Außerdem weist Gusseisen eine sehr gute Gießbarkeit auf: Die Schmelze ist sehr dünnflüssig und weist ein gutes Formfüllungsvermögen auf. Die Schwindung und Schrumpfung beim Abkühlen und Erstarren sind gering. Außerdem weist Gusseisen sehr gute Gebrauchseigenschaften auf, darunter die Verschleißbeständigkeit, und die Schwingungsdämpfung. Die meisten Gusseisensorten enthalten noch um 2 % Silicium, das die Gießbarkeit verbessert. Gusseisen wird bevorzugt in Formen aus Sand vergossen (Sandguss).

Der zweitwichtigste Gusswerkstoff ist Aluminiumguss, eine Aluminiumlegierung die noch Silicium, Magnesium oder Kupfer enthält. Sie schmelzen bei etwa 570 °C und lassen sich auch sehr gut vergießen. Aluminiumgusslegierungen können auch für filigrane Bauteile genutzt werden, deren Formen von anderen Werkstoffen nicht vollständig gefüllt werden würden. Aluminiumlegierungen werden bevorzugt für den Druckguss eingesetzt.

Einstellige prozentuale Anteile haben noch der Stahlguss und die verschiedenen Kupferlegierungen (Messing, Bronze, Rotguss). Medizinische Implantate, aber auch Flugzeugteile werden oft aus Titan gegossen.

Es gibt zahlreiche verschiedene Gießverfahren, die nach mehreren Kriterien eingeteilt werden können.

Das Gießen in Formen, die der Form des Fertigteils weitgehend entsprechen, ist das Formgießen, das am häufigsten eingesetzt wird. Daneben gibt es noch das Vergießen zu Barren oder Brammen, den Blockguss und das Gießen von kontinuierlichen, theoretisch endlosen Strängen, dem Strangguss.

Nach der Art der Formfüllung unterscheidet man das Schwerkraftgießen, das Standardverfahren, bei dem die Schmelze durch die Wirkung der Schwerkraft in die Form fällt, sowie das Schleudergießen mit Zentrifugalkräften (für rotationssymmetrische Teile) und das Druckgießen, bei dem die Schmelze durch Kolbendruck in die Form gepresst wird.

Eine besonders wichtige Einteilung unterscheidet zwischen Verfahren mit Formen, die nur einmalig genutzt werden und beim Entnehmen der Gussstücke zerstört werden (verlorene Form) und den Dauerformen, die mehrfach genutzt werden:


Außerdem gibt es noch eine Reihe von Spezialverfahren wie das Thixogießen, Vacuralgießen und Squeeze Casting.

Die Prozesskette beim Gießen besteht aus

Zur Vorbereitung des eigentlichen Gießvorgangs, des Abgusses, werden die Formen gebaut, für die zuvor Modelle gefertigt werden. Um Hohlräume in den Werkstücken fertigen zu können, werden Kerne hergestellt und in die Formen gelegt. Parallel zu diesen Aufgaben wird die Schmelze vorbereitet.

Die Wachsmodelle beim Feingießen werden entweder manuell aus einem Wachsmodell geschnitzt oder mittels eines Urmodells selbst durch Gießen hergestellt. Beim Vollformgießen werden die Styropormodelle aus Blöcken geschnitten und teilweise auch aus Einzelteilen zusammengebaut und verklebt. Dauermodelle für das Sandgießen können aus Holz, Keramik oder Metall bestehen, die verschieden oft genutzt werden können. Billige Holzmodelle können teilweise nur fünfmal genutzt werden, Metallmodelle deutlich öfter.

Dauerformen werden aus Stahl geschmiedet oder gefräst und können mehrere 100.000 Euro kosten. Verlorene Formen bestehen aus Formstoff, für den meist Sand genutzt wird, der mit geringen Mengen von Ton und Wasser zusammengehalten wird. Die Formen können aus verfestigtem Sand gefräst werden (direktes Formstofffräsen). Teilweise werden sie durch 3D-Drucken gefertigt. Üblicherweise werden zum Formenbau aber Modelle genutzt, über die der Formstoff gelegt wird. Der noch lose Formstoff muss dann verfestigt werden, wofür zahlreiche verschiedene Verfahren zum Einsatz kommen können. Dazu zählt einfaches Schütteln und Pressen, was in der Serienfertigung genutzt wird, da es sich automatisieren lässt (Maschinenformen). Beim Maskenformen wird eine relativ dünne Schicht aus Formstoff über das Modell gelegt und mit Harzen durchtränkt. Diese härten im Ofen aus.

Die Formen müssen nicht nur die Form der zu fertigenden Werkstücke enthalten, sondern weitere Öffnungen zum Zuführen der Schmelze. Der Hohlraum, in die die Schmelze gegossen wird, wird als Anschnitt bezeichnet. Nach der Formfüllung verringert sich das Volumen der flüssigen, abkühlenden Schmelze, weshalb weiteres Material in die Form fließen muss. Ein einzelner Anschnitt reicht häufig nicht dafür aus, weshalb sogenannte Speiser angebaut werden, die nach dem Erstarren mit dem Anschnitt wieder entfernt werden. Der Anschnitt besteht im einfachsten Fall aus einer Öffnung direkt über dem Hohlraum für das Werkstück. Bessere Werkstückqualitäten lassen sich erreichen, wenn die Schmelze in einem separaten Schacht auf den Boden fällt und seitlich in das Werkstück fließt. Für die Form, Größe und Anzahl der Speiser und Anschnitte gibt es zahlreiche Varianten, da sie einen großen Einfluss auf die Bauteilqualität haben.

Kerne sind nötig, um Gussteile mit Hohlräumen fertigen zu können. Die Kerne werden in die Formen gelegt und nach dem Erstarren entnommen. Bei Dauerformen bestehen die Kerne meistens ebenfalls aus Metall, bei Sandformen aus Sand. Sie werden ebenfalls nach dem Gießen zerstört. Außerdem werden Kerne für Hinterschneidungen benötigt.

Zur Vorbereitung der Schmelze zählt die Zusammenstellung der Rohstoffe, das Schmelzen in Öfen und die Schmelzebehandlung.

Als Rohstoffe können Metalle direkt von den Hüttenwerken genutzt werden, in der Gießerei werden aber größere Mengen Schrott verarbeitet. Die Branche ist durch eine hohe Recyclingquote geprägt. Ein Teil des Schrotts entsteht in der Gießerei selbst; dazu zählen fehlerhafte Gussstücke sowie die entfernten Speiser und Anschnittsysteme, die als Kreislaufmaterial fungieren. Verarbeitet wird aber auch Altschrott aus gebrauchten und zerkleinerten Bauteilen.

Um eine Legierung mit der gewünschten Zusammensetzung zu erhalten, müssen verschiedene Ausgangsmaterialien gemischt werden. Diese Mischung wird als Gattierung bezeichnet. Mittels spezieller Software kann berechnet werden, welche Mengen in welchen Verhältnissen benötigt werden für die günstigste Gattierung.

Zum Schmelzen der Metalle werden verschiedene Industrieöfen verwendet. Besondere Bedeutung haben der Kupolofen, der Lichtbogenofen und der Induktionsofen. Sie eignen sich für verschiedene Werkstoffe unterschiedlich gut. Kupolöfen werden für Eisenwerkstoffe genutzt, Lichtbogenöfen für Stahl und Nichteisenmetalle, beide aber nur zum Schmelzen. Induktionsöfen und Widerstandsöfen eignen sich auch zum Warmhalten der Schmelze. Etwa 60 bis 70 % des Energiebedarfs einer Gießerei geht auf das Schmelzen zurück.

Danach erfolgt die Schmelzebehandlung. Der Schmelze werden verschiedene Stoffe zugesetzt, die verhindern, dass sie mit dem Sauerstoff der Luft reagiert und sich in sonstiger Weise ungewollt verändert. Bei der Impfung der Schmelzen werden Fremdstoffe zugesetzt, die die Erstarrung beeinflussen und damit die Härte und Festigkeit des fertigen Gussstücks.

Meistens werden die Metalle in der Gießerei geschmolzen, manchmal auch in den Stahl- oder Hüttenwerken und dann per Flüssigmetalltransport zur Gießerei verbracht.

Das Einfüllen der Schmelze in die Form wird als Abguss bezeichnet. Danach und teilweise parallel dazu erstarrt die Schmelze. Diese Vorgänge haben entscheidenden Einfluss auf die Qualität der Gussteile. Die Schmelze kann direkt aus dem Ofen in die Form gefüllt werden, sie kann aber auch in Gießpfannen oder Gießlöffel gefüllt werden und erst anschließend in die Formen.

Die Schmelze kann direkt in die Formen gegossen werden, was vor allem bei nach oben offenen Formen praktiziert wird. Üblich ist das Vergießen in ein spezielles Eingusssystem. Die Schmelze kann von oben in die Form fallen oder seitlich oder von unten in sie hineinfließen. Fallende Schmelzen führen zu Verwirbelungen und turbulenten Strömungen. Die Schmelze vermengt sich dabei mit Luft und nimmt unerwünschte Gase auf, die später als Poren im Gussstück zurückbleiben. Die Formen werden aber relativ schnell befüllt. Bei steigender Formfüllung kommt es zu keinen oder nur geringen Verwirbelungen, was zu laminaren Strömungen führt. Die Gussstücke enthalten dann nur wenige Poren. Beim Kippgießen werden beide Varianten kombiniert.

Die Schmelze erkaltet beim Kontakt mit der Form. Die Erstarrung soll erst beginnen, wenn die Form vollständig gefüllt ist, da es sonst zu Fehlstellen kommen kann. Manche Schmelzen werden beim Erkalten zähflüssig, was Fehlstellen begünstigt. Die entsprechende Gießeigenschaft ist das Formfüllungsvermögen. Manche Formen werden beheizt, um die Schmelze länger warm zu halten oder es werden Formen mit geringer Wärmeleitfähigkeit verwendet. Dies verlängert allerdings die anschließende Erstarrung. Andere Formen vor allem Dauerformen werden gekühlt, um den Prozess zu beschleunigen und die Temperaturbelastung zu verringern.

Spätestens nach der vollständigen Formfüllung erkaltet die Schmelze und verringert dabei ihr Volumen, was als (Flüssig-)Schwindung bezeichnet wird. Aus dem Anschnitt und den Speisern muss Schmelze nachfließen, bis sie erstarrt. Während der Erstarrung kommt es infolge des Schrumpfens ebenfalls zu Volumenänderungen. Diese kann nicht mehr durch Speiser ausgeglichen werden. Danach verringert sich das Volumen weiter, bis das Gussstück Raumtemperatur erreicht hat (Festkörperschwindung).

Der genaue Ablauf der Erstarrung und anschließenden Abkühlung hat entscheidenden Einfluss auf die Mikrostruktur und damit auf die Härte und Festigkeit der Gussteile. In der Schmelze sind verschiedene Stoffe gelöst. Da sich die Löslichkeit beim Abkühlen verringert, werden diese Stoffe ausgeschieden. Bei Gusseisen wird beispielsweise Grafit (Kohlenstoff) ausgeschieden. Die Einteilung der Gusseisensorten erfolgt nach der (mikroskopischen) Form des Grafits in Gusseisen mit Lamellengraphit, Gusseisen mit Vermiculargraphit (Würmchengrafit) und Gusseisen mit Kugelgraphit, die sich in ihrer Härte und Festigkeit unterscheiden. Auch gelöste Gase können aus der Schmelze ausgeschieden werden. Wenn sie nicht aus der Form entweichen können, bleiben sie als Poren oder Lunker zurück. Auch die Abkühlgeschwindigkeit hat Einfluss auf die Härte und Festigkeit der Gussteile. Bei langsamem Abkühlen entsteht beispielsweise das gut zu bearbeitende graue Gusseisen, bei schneller Hartguss.

Zur Nachbehandlung zählt das Entformen, bei dem die Gussstücke aus den Formen entnommen werden. Dies kann geschehen, nachdem sie auf Raumtemperatur abgekühlt sind, oder direkt nach dem Erstarren. Vor allem in der Serienfertigung werden die Gussstücke möglichst früh aus den Formen entnommen, da dadurch einerseits die weitere Schrumpfung nicht durch die Form behindert wird und andererseits die Formen für ein erneutes Abgießen schnell wieder zur Verfügung stehen. Bei Dauerformen werden die Gussstücke mit Ausstoßern entfernt, bei verlorenen Formen wird die Form zerstört.

Die weitere Nachbehandlung beinhaltet vor allem das Putzen sowie manchmal noch eine Wärmebehandlung.

Das Gussputzen beinhaltet das Abtrennen von Anschnitt und Speißern, Entfernen von Kernen, Entzundern (Brandstellen), Entsanden (Entfernen von Formstoffresten), Ausbessern von Gussfehlern und das Reinigen der Oberfläche. Teilweise werden auch noch Bearbeitungszugaben entfernt. Auf die Putzarbeiten entfällt ein großer Teil der Gesamtkosten in der Gießerei, da es sich nur teilweise automatisieren lässt. Eine putzgerechte Konstruktion des Gussstücks ist daher für die Stückkosten entscheidend.

Die Wärmebehandlung soll die mechanischen Eigenschaften des Gussteils verbessern. Bei Temperguss (eine Gusseisensorte) zählt sie zum festen Bestandteil (Tempern ist eine Form der Wärmebehandlung). Auch Stahlguss wird üblicherweise geglüht, da das Gussgefüge sehr grobkörnig ist. Bei anderen Werkstoffen kann die Wärmebehandlung auch entfallen.

Die Gießprozess-Simulation nutzt numerische Methoden, um den gesamten Gießprozess inklusive Formfüllung, Erstarrung und Abkühlung von Gussteilen vorauszusagen und erlaubt auch die quantitative Vorhersage von mechanischen Eigenschaften, thermischen Spannungen und Verzug der Gussteile. Durch Simulation wird die Qualität eines Gussteils bereits vor dem Produktionsbeginn exakt beschreibbar und die Gießtechnik kann auf die gewünschten Teileeigenschaften ausgerichtet werden. Damit lassen sich nicht nur teure Probeabgüsse in der Entwicklung eliminieren. Die genaue Auslegung des gesamten Gießsystems spart darüber hinaus Energie, Material und Werkzeugkosten. 

Software zur Simulation von Gießprozessen unterstützt den Anwender von der Auslegung des Bauteils, der Festlegung der Schmelzpraxis und der Gießtechnik über den Modellbau und die Formherstellung bis hin zur Wärmebehandlung und Nachbearbeitung. Damit können Kosten in der gesamten Fertigungskette konsequent eingespart werden.

Die Gießprozess-Simulation wurde seit den 1970er Jahren zunächst an Hochschulen, insbesondere in Europa und den USA, entwickelt und gilt als bedeutendste Innovation in der Gießereitechnik der letzten 50 Jahre. Seit den späten 1980er Jahren stehen kommerzielle Programme zur Verfügung, die den Gießereien erstmals Einblicke in die Gießtechnik möglich machen, die zuvor eine Blackbox war. Allerdings sind die Optimierung der Geometrie der Gussform und die Ermittlung der richtigen Gusskonfiguration (Material, Temperatur, Zeitdauer des Gießens usw.) relativ komplexe Prozesse, für die nicht durchweg exakte mathematische Modelle zur Verfügung stehen. Daher ist für die Entwicklung von Simulationsmethoden immer noch der Einsatz experimenteller Methoden notwendig.

Der Arbeitsort heißt Gießerei, der Arbeiter Gießer. Die offiziellen Berufsbezeichnungen sind:


Trotz maschineller Hilfsmittel werden relativ hohe körperliche Anforderungen an die Gießer und ihre Helfer gestellt.

In Österreich existiert seit dem 1. Juli 2010 der Lehrberuf Gießereitechnik. Dieser ersetzt die Vorgängerlehrberufe GießereimechanikerIn, FormerIn und GießerIn. Der Lehrberuf Gießereitechnik unterscheidet Eisen- und Stahlguss von Nichteisenmetallguss. Beide Ausbildungen haben eine duale Ausbildungsdauer von vier Jahren. Gießereitechniker mit Schwerpunkt Eisen- und Stahlguss arbeiten in Betrieben des Gießereigewerbes und der Eisen-, Stahl- und Maschinenbauindustrie. Sie stellen Gussteile aus Eisen und Stahl her. Gießereitechniker mit Schwerpunkt Nichteisenmetallguss arbeiten in Betrieben des Gießereigewerbes und der Leichtmetall-, Buntmetallgießereiindustrie und Maschinenbauindustrie. Sie stellen Gussteile aus Nichteisenmetallen und Legierungen, wie z. B. Aluminium, Messing, Bronze, Kupfer her.





</doc>
<doc id="12655" url="https://de.wikipedia.org/wiki?curid=12655" title="Maschine">
Maschine

Eine Maschine (entlehnt aus , von , dieses von "mēchanḗ" „Werkzeug, künstliche Vorrichtung, Mittel“) ist ein technisches Gebilde mit durch ein Antriebssystem bewegten Teilen. Maschinen werden als technische Arbeitsmittel vor allem für mechanische Einwirkung verwendet. In der Vergangenheit stand der Energie- und/oder Stofffluss im Vordergrund. Der Informationsfluss spielte zuerst in feinmechanischen Geräten eine Rolle, ist heute aber in fast allen Maschinen von Bedeutung (Automatisierung). Reizvolle Ziele für die Erfindung von Maschinen waren aus der Sicht eines Arbeiters eine Verstärkung der eigenen Kraft, Zeitgewinn, Genauigkeit, feinere Bearbeitungsmöglichkeit und Fertigung identischer Produkte. Es folgte auch eine Entlastung des Produktionsarbeiters von körperlicher und geistiger Arbeit durch Maschinen und Geräte. Diese modernen Arbeitsmittel übernehmen vor allem Routine- und auch gefährliche Arbeiten.

Jede Maschine enthält individuell angefertigte Einzelteile, wie wenigstens Gestell und Gehäuse. Einen erheblichen Anteil nehmen Teile mit einer Standardfunktion ein; diese Maschinenelemente werden gesondert als Massenartikel produziert. Fixe Maschinenelemente sind zum Beispiel Schrauben und Dichtungen, bewegliche Elemente sind zum Beispiel Zahnräder und Hebel.

Maschinen sind immer Produkte des Menschen. Aufgrund der antiken Bedeutung (vgl. „“) wurde die Maschine bis in die Neuzeit hauptsächlich als Mittel zu einer Täuschung – dem Erzeugen unnatürlicher, also unmöglicher Effekte – und erst in zweiter Linie als Arbeitshilfe verstanden.

In der Renaissance entwickelte sich ein genaueres Konzept über "Mechanismen": Mechanismen sind Komplexe von Bauteilen, bei denen die Bewegung eines Elements zwangsläufig die Bewegung anderer Elemente bewirkt. Sie haben in der Regel bewegliche Komponenten und sind im Vergleich zum "" (dem ‚Werkzeug‘) erheblich komplexer.

In dieser Zeit war ein Mechanismus primär ein "Werk", eine Form von Getrieben, die Kräfte übertragen. Zu den kompliziertesten Mechanismen gehörte die Grande Complication ("Große Komplikation)" in mechanisch-automatischen Uhrwerken. In diesem Sinne ist auch der Mechanismus von Antikythera zu verstehen, ein antikes Artefakt, das die Himmelsmechanik nachbildete.

In dieser Zeit bezeichnete auch "Kunst ()" noch das Ingenieurwesen, und noch im 19. Jahrhundert wurden im Bergbau von Wasserkraft oder Pferden angetriebene Maschinen "Bergmännische Kunst" genannt.

Die Entwicklung der klassischen Mechanik als wissenschaftliche Disziplin seit der Aufklärung führt dazu, nach den Grundelementen mechanischer Systeme zu suchen, im Sinne der ‚Atome‘, der Bauteile, die sich nicht weiter zerlegen lassen – angedacht wurde das schon von den antiken griechischen Ingenieuren (Aristoteles): So werden die "einfachen Maschinen" definiert, nämlich Seil und Stange, Rolle, Hebel, sowie schiefe Ebene (in der Antike noch die Schraube, die sich aber als Stange und schiefe Ebene modellieren lässt).

Mehr oder weniger komplexe Mechanismen kommen in praktisch allen Ingenieurwissenschaften und technischen Disziplinen vor. Das Spektrum möglicher Maschinen reicht von einer einfachen Gerätschaft mit verbundenen, beweglichen Teilen ("Mechanismen") bis zu sich über Kilometer erstreckenden komplexen Bauwerken ("Anlagen").

In der technischen Anwendung haben Maschinen meist einen Antrieb (zum Beispiel einen Motor), der mehr oder minder kontinuierlich Energie liefert. Mit dieser Energie arbeiten die Maschinen, weshalb sie "Arbeitsmaschinen" genannt werden. Der Motor und andere Maschinen, die verschiedene Energieformen in meist rotierende Bewegungsenergie umwandeln, werden als "Kraftmaschinen" bezeichnet.

Weil die Maschine, wenn sie einen kontinuierlichen Antrieb hat, Arbeitsvorgänge in eine Folge wiederholbarer Schritte teilt („formalisiert“), überschneidet sich ihre Bedeutung mit der des "Automaten". Sie kann (unberechenbare) Handlungen von Menschen oder Tieren durch planbare Tätigkeiten (einen Algorithmus) ersetzen. Der "automatos", „der sich aus eigenem Willen bewegt“, ist ursprünglich ein auf Räderwerken aufgebautes, ebenfalls illusionistisches Spielzeug – er entspricht aber den mechanistischen Leitbildern der Aufklärung, die auch die Natur als zwangsläufige, determinierbare Abfolge zu erklären sucht.

19. Jahrhundert:
Verbindungen widerstandsfähiger Körper, welche so eingerichtet sind, dass mittels ihrer mechanische Naturkräfte genötigt werden können, unter bestimmten Bewegungen zu wirken (Reuleaux).Die Verbindung der Körper zu einer Maschine schließt nicht alle und jede Bewegung aus, sondern verhindert nur die für den Zweck der Maschine unnötigen und störenden Bewegungen, sodass die zweckmäßigen Bewegungen als die allein möglichen übrigbleiben.

20. Jahrhundert:
Normungsbestrebungen gehen dahin, zwischen "Maschine", "Gerät", "Apparat", "Automat", "Werkzeug", "Instrument" und "Anlage" zu unterscheiden:

Ende 20. Jahrhundert:
In Europa ist die "Maschine" in der "Maschinen-Richtlinie" definiert.Infolge der Elektronisierung und Automatisierung im 20. Jahrhundert hat sich der Begriff der Maschine auf Computerprogramme ausgedehnt, mit denen Maschinen simuliert werden. In der Ingenieurtechnik wird aber meist zwischen mechanischer Maschine und elektronischen Automaten unterschieden.

Mit Hilfe der Richtlinie 2006/42/EG (Maschinenrichtlinie) wird ein einheitliches Schutzniveau zur Unfallverhütung für Maschinen beim Inverkehrbringen innerhalb des Europäischen Wirtschaftsraumes (EWR) sowie der Schweiz und der Türkei geregelt. Darin ist auch festgelegt, was als Maschine aufgefasst werden muss:

Nach 2006/42/EG Artikel 2 Abs. a (bzw. Umsetzung in nationales Recht, durch 9. ProdSV § 2, Absatz 2) bezeichnet „Maschine“

Eine Maschine ist als eigenständige Einheit im Wesentlichen unabhängig von der Umgebung funktionsfähig, während ihre Einzelkomponenten meist nicht unabhängig von der Maschine sinnvoll verwendbar sind.

Nicht unter den Regelungsbereich der Maschinenrichtlinie fallen jedoch "„Maschinen, deren einzige Kraftquelle die unmittelbar angewandte menschliche Arbeitskraft ist, mit Ausnahme von Maschinen, die zum Heben von Lasten verwendet werden, …“" (2006/42/EG Artikel I Absatz 2a). Diese Eingrenzung des Begriffes grenzt somit viele Geräte aus, die im alltagssprachlichen Sinne Maschinen sind. Im Verordnungstext werden noch weitere Ausnahmen und Ergänzungen definiert.

Die Neufassung der Maschinenrichtlinie 2006/42/EG führt daneben auch "unvollständige Maschinen" auf, die dadurch gekennzeichnet sind, dass sie dazu bestimmt sind, in eine andere Maschine oder andere unvollständige Maschinen eingebaut oder mit ihnen zusammengefügt zu werden, um zusammen mit ihnen eine Maschine im Sinne der Richtlinie zu bilden.

Beispiele, mit den daraus folgenden zugehörigen Anforderungen der EU-Richtlinien:







</doc>
<doc id="12657" url="https://de.wikipedia.org/wiki?curid=12657" title="James Watt">
James Watt

James Watt [] (* in Greenock; † 25. August 1819 in seinem Haus in Heathfield, Staffordshire) war ein schottischer Erfinder. Seine einflussreichste Erfindung war die Verbesserung des Wirkungsgrades von Dampfmaschinen durch Verlagerung des Kondensationsprozesses aus dem Zylinder in einen separaten Kondensator. Watt selbst hielt das von ihm entworfene Gestänge, das Wattsche Parallelogramm, für seine größte Erfindung. Er stattete seine Dampfmaschinen mit den vom Windmühlenbau bekannten Fliehkraftreglern zur Konstanthaltung der Drehzahl bei Belastungsschwankungen aus.

James Watt wurde als Sohn armer, aber sehr gebildeter Eltern geboren. Sein Vater war Zimmermann und Konstrukteur von nautischen Geräten, Watts Großvater war ein Mathematiklehrer und schottischer Nationalist. James war ein kränkliches Kind, das unter anderem unter chronischen Kopfschmerzen litt. Die Eltern unterrichteten ihn deshalb teilweise selbst. Schon als Junge experimentierte er gern und soll die Funktionsweise von jedem Gegenstand, den er in die Finger bekam, erforscht haben. Darüber hinaus war Watt ein eifriger Sammler von Pflanzen und Steinen, las viel und erfand Geschichten.

Für ein Medizinstudium, für welches Watt sich interessierte, waren seine Eltern jedoch zu arm. Deshalb begann Watt in London eine inoffizielle Mechanikerlehre, nachdem er im schottischen Glasgow keinen Lehrherrn finden konnte. Da diese ihm jedoch schon bald nichts Neues mehr zu bieten hatte, brach er sie vor Ablauf der vorgeschriebenen sieben Jahre ab. Eine eigene Werkstatt konnte er wegen der nicht abgeschlossenen Ausbildung nicht eröffnen, da die Glasgower Zünfte Einspruch erhoben. Watt erhielt 1757 eine Stelle als Instrumentenmacher an der Universität von Glasgow. Dort fertigte und reparierte er für die Universität Instrumente wie Kompasse und Quadranten. Sein Einraum-Labor, das er nach einem Jahr um einen zweiten – zur Straße gelegenen – Raum mit Schaufenstern erweiterte, entwickelte sich schon bald zum Treffpunkt von Dozenten und Studenten. Watt fand an der Universität viele Freunde, obwohl er „nur“ ein Handwerker war. Unter anderem war er mit dem Ökonomen Adam Smith befreundet. Er wurde von seinen Zeitgenossen als außerordentlich bescheiden und liebenswürdig beschrieben.

1760 heiratete Watt seine Cousine und Jugendliebe, die 1736 geborene Margaret Miller. Margaret Watt starb 1773 bei der Geburt des sechsten Kindes. Von ihren gemeinsamen Kindern überlebte nur der 1769 geborene Sohn James. 1775 heiratete Watt Anne Macgregor.

1764 erhielt Watt als Universitätsmechaniker den Auftrag, das nur schlecht funktionierende Modell einer Dampfmaschine nach der Bauart von Thomas Newcomen zu reparieren. Watt erkannte, dass das Problem im ungünstigen Wärmehaushalt der Grundkonstruktion begründet war. Ein Großteil des einströmenden Wasserdampfes wurde dabei verbraucht, die Zylinderwandung auf über 100 °C aufzuheizen. Beim Modell trat dieser Sachverhalt aufgrund des unterschiedlichen Verhältnisses von Zylinderoberfläche und -volumen deutlicher zutage als bei den Originalmaschinen. Watt beschloss, die Maschine – basierend auf Vorarbeiten von Denis Papin – zu verbessern. Er lernte neben Französisch und Italienisch auch Deutsch, um deutsche Schriften zur Wärmetheorie zu lesen. Um das fortwährende, wechselweise Aufheizen und Abkühlen des Zylinders zu vermeiden, verlegte er die notwendige Kondensation des Wasserdampfes in einen separaten Behälter, den Kondensator. Zusätzlich ließ er den Zylinder von außen in der sogenannten 'Steam Jacket' (deutsch etwa: Dampfjacke) mit Dampf umspülen, um die Wärmeverluste im Zylinder zu verringern. Diese Dampfumspülung isolierte er nach außen mit senkrecht angebrachten Holzbrettern. Erst wesentlich später konstruierte er den Zylinder auf Doppelwirkung um: Während bei den Vorgängermaschinen der Kolben durch den atmosphärischen Luftdruck nach unten sank, wurde der Vorgang nun durch Dampfkraft unterstützt. Für diese scheinbar kleine Änderung musste der Zylinderdeckel an der Kolbenstange abgedichtet werden und das kraftübertragende Gestänge vollkommen neu konstruiert werden. Watt erfand hierfür das Wattsche Parallelogramm, dessen Bedeutung er noch über der des separaten Kondensators sah.

Watt gab seine Stelle an der Universität Glasgow auf, um sich stärker der Weiterentwicklung der Dampfmaschine zu widmen. Obwohl er nebenbei als Feldvermesser arbeitete, häufte er in den folgenden Jahren Schulden an. Auch war er häufig krank. Erst 1769 fand er in dem Eisenfabrikanten John Roebuck (1718–1794) einen Finanzier und erhielt am 5. Januar 1769 das englische Patent Nr. 913. Watts Verbesserungen ermöglichten gegenüber der von John Smeaton optimierten Newcomen-Dampfmaschine eine Ersparnis an Steinkohle von über 60 Prozent.

Es gelang zunächst nicht, einen dampfdichten Zylinder herzustellen. Über den Versuchen ging sein Förderer John Roebuck pleite. Der Industriebaron Matthew Boulton erklärte sich bereit, Roebucks Nachlass zu übernehmen, wenn er die Verlängerung des Patentes um 25 Jahre bis zum Jahr 1800 erreichen könne. Nach erfolgreicher Lobbyarbeit wurde die Firma Boulton & Watt für die Dauer der Gültigkeit des Patents gegründet. Boulton sicherte sich für seinen Anteil 2/3 der Einnahmen zu. Die erste einsatzfähige Dampfmaschine nach dem Wattschen Prinzip wurde 1776 in der Fabrik von John Wilkinson installiert. Wilkinsons Firma war es gelungen, einen Zylinder aus Eisen in der erforderlichen Qualität zu fertigen: Er nutzte ein von ihm entwickeltes und patentiertes Verfahren zum Bohren von Kanonen in Eisen, bei dem der Bohrmeißel fixiert wird und sich das Werkstück um diesen dreht. In der Folge stellten 'Boulton & Watt' in Soho bei Birmingham wichtige Teile wie Kondensator und Luftpumpe her, zusammengebaut wurden die Dampfmaschinen erst beim Kunden. Wilkinson lieferte die Zylinder direkt an den Aufstellungsort und weitere benötigte Materialien wurden vor Ort dazugekauft und angepasst. Es ist eine Besonderheit, dass 'Boulton & Watt' die Dampfmaschinen anfangs nicht verkaufte, sondern über die Patentlaufzeit verleaste. Als Nutzungsentgelt verlangten sie ein Drittel der gegenüber der optimierten Newcomen-Dampfmaschine gesparten Brennstoffkosten. Zu dessen besserer Berechnung konstruierte Watt eigens einen manipulationssicheren Zähler für die Kolbenbewegungen.

1781 wandelte Watt den Kolbenhub mittels eines Kreisschubgetriebes in eine Drehbewegung um. 1782 konstruierte er eine Dampfmaschine, bei der der Kolben von beiden Seiten durch Dampf bewegt wird. Damit hatte James Watt nun eine Dampfmaschine entwickelt, bei der die komplette Arbeit vom Dampf geleistet wurde, und nicht mehr ein Teil der Arbeit vom relativ niedrigen natürlichen Luftdruck. Auf diese Weise wurden wesentlich stärkere Maschinen möglich. Bei dieser Dampfmaschine drückt der Kolben den Balancier nach oben und zieht ihn nach unten. Damit diese in zwei Richtungen wirkenden Kräfte übertragen werden konnten, wurde statt einer Kette eine Kolbenstange benutzt. Zur linearen Führung der Kolbenstange am pendelnden Balancier erfand Watt 1784 das Wattsche Parallelogramm, das er selbst als seine größte Erfindung ansah.

Seit 1788 stattete er seine Dampfmaschinen mit den bereits erfolgreich in Windmühlen benutzten Fliehkraftreglern zur Regelung der Drehgeschwindigkeit der Antriebsachse unter Belastungsschwankungen aus. Außerdem führte er die Pferdestärke (PS) als Maßeinheit für die Leistung ein.

Die Dampfmaschinen von Watt erreichten schließlich einen Wirkungsgrad von 3 %, das dreifache der optimierten Newcomen-Dampfmaschinen. Der Bau einer Hochdruckdampfmaschine verzögerte sich wegen Watts Angst vor Explosionen und seinem bis zum Jahr 1800 verlängerten Patent über die Dampfkondensation außerhalb des Zylinders. Als Richard Trevithick (1771–1833) im Jahre 1804 eine auf Rädern und Schienen fahrende Hochdruckdampfmaschine konstruierte und mit fünffachem Atmosphärendruck betrieb, wünschte Watt ihm ob des Leichtsinns den Strick um den Hals.

Watt und Boulton behinderten während der Zeit, in der die Watt verliehenen Patente Gültigkeit hatten, erfolgreich die Weiterentwicklung der Dampfmaschine durch konkurrierende Ingenieure. So verklagten sie Jonathan Hornblower, dessen Verbunddampfmaschine einen höheren Wirkungsgrad möglich machte, wegen Patentverletzung und konnten so deren Weiterentwicklung stoppen.
Watt stieg im Jahre 1800 aus seinem Unternehmen aus und übergab seine Anteile an seine Söhne James und Gregory. Er selbst zog sich in sein Haus Heathfield in Handsworth, Birmingham, zurück, wo er an diversen weiteren Erfindungen arbeitete, darunter Dingen, die eher dem Zeitvertreib als dem ernsthaften technischen Einsatz dienten. Er lebte äußerst gesellig und seine Gäste schwärmten, dass man sich mit ihm über alle Themen unterhalten könne. Der schottische Nationaldichter Walter Scott äußerte sich einmal erleichtert darüber, dass Watt Ingenieur geworden war. Dessen Talent zum Geschichtenerzählen hätte ihm sonst ernsthaft Konkurrenz gemacht.

In Birmingham empfing Watt Erzherzog Johann aus der Steiermark, der sich 1815 und 1816 bei Reisen nach England über die Errungenschaften der englischen Industrie informieren wollte.

Am 25. August 1819 starb Watt. Er wurde in der "St. Mary’s Church" in Handsworth bei Birmingham beigesetzt.

In der Westminster Abbey wurde ihm ein Ehrenmal (Kenotaph) errichtet, das folgende fast hymnische Inschrift trägt:

Die SI-Einheit der Leistung wurde mit Watt benannt und ersetzte das bisher verwendete, von ihm eingeführte PS.

Die britische Institution of Mechanical Engineers nannte ihre höchste Auszeichnung die „James-Watt-Medaille“. Diese gilt heute als die weltweit renommierteste Auszeichnung auf dem Gebiet des Maschinenbaus.

Außerdem wurde der Mondkrater Watt 1935 von der IAU nach ihm benannt.

James Watt wurde unter anderem der Ehrendoktor der Universität Glasgow verliehen. Er war korrespondierendes Mitglied der französischen Akademie der Wissenschaften.






</doc>
<doc id="12659" url="https://de.wikipedia.org/wiki?curid=12659" title="397">
397






</doc>
<doc id="12660" url="https://de.wikipedia.org/wiki?curid=12660" title="Heiligenkalender">
Heiligenkalender

Als Heiligenkalender wird das in der römisch-katholischen Kirche verwendete kalendarische Verzeichnis der Heiligen bezeichnet, das diesen jeweils einen Gedenktag zuordnet. Verwandt ist der Heiligenkalender mit dem Martyrologium, einem Verzeichnis von Märtyrern und anderen Heiligen, das meist eine kurze Zusammenfassung ihres Lebens und der Umstände ihres Todes enthält.

Ursprünglich sind Heiligenkalender Bestandteil liturgischer Bücher, die angeben an welchen Tagen und nach welcher Ordnung in den Gottesdiensten bestimmter Heiliger zu gedenken ist. Nach der Vielzahl von liturgischen Traditionen in den verschiedenen Regionen der christlichen Kirche sowie in den liturgischen Büchern, die in bestimmten Ordensgemeinschaften oder einzelnen Klöstern Verwendung finden, gibt es eine Vielzahl von Heiligenkalendern.

Diese werden in der Regel als "Sanctorale" verfasst, das sind Bücher, die neben dem Kalenderdatum und dem Heiligen auch die "Eigentexte" (lateinisch: Proprium) des jeweiligen Heiligengedenkens verzeichnen. Der Heiligenkalender unterscheidet sich vom allgemeinen liturgischen Kalender (lateinisch: Temporale) auch dadurch, dass in ihm, anders als bei den Terminen des vom Osterfest abhängigen Festkreises, die Gedenktage ein festes Datum haben, also unbeweglich sind, wenn sie nicht durch ein anderes Fest verdrängt werden.

Papst Pius V. hatte 1568/1570 im Zug der Umsetzung der Reformen des Konzils von Trient für die westliche katholische Kirche eine Regulierung eingeführt. Grundlage des von ihm eingeführten Kalenders mit 158 Heiligenfesten war der stadtrömische Kalender.
Dieser Kalender galt als Grundlage für die lateinische Liturgie der katholischen Kirche bis zu den Reformen im Zuge des Zweiten Vatikanischen Konzils, als 1969 die "Grundordnung des Kirchenjahres und des neuen Römischen Generalkalenders" eingeführt wurde, und seitdem wieder versucht wird, die Heiligenkalender nach Ländern, Kulturräumen und geistlichen Traditionen in der Kirche (wie Eigenkalender der Ordensgemeinschaften) zu pluralisieren. So gibt es den Regionalkalender für das deutsche Sprachgebiet, der die Heiligenverehrung in den katholischen Gottesdiensten in Deutschland, Österreich, der Schweiz und den Bistümern Luxemburg und Bozen-Brixen regelt.

Der Heiligenkalender enthält die Gedenktage der Heiligen im Kirchenjahr der katholischen Kirche. Als Datum des Heiligengedenkens gilt in der Regel der Todestag des Heiligen, sozusagen als Geburtstag im Himmel; nur bei Johannes der Täufer und der Gottesmutter führt der Heiligenkalender sowohl den Geburts- wie den Todestag auf.

Heiligenkalender gibt es auch in den orthodoxen Kirchen, bei den Anglikanern und auch bei einigen reformatorischen Kirchen (evangelischer Namenkalender). Altritualisten innerhalb dieser Kirchen verwenden in der Regel zusammen mit der älteren Form ihrer Liturgie auch eine entsprechend ältere Version des Heiligenkalenders.

Die Gedenktage der Heiligen werden von denen, die auf den Heiligen oder das Festgeheimnis des betreffenden Tages getauft wurden, auch als Namenstag gefeiert. Früher war es auch üblich, den Tagesheiligen des Geburts- oder Tauftages als Namensgeber zu wählen; Martin Luther z. B. wurde am Tag nach seiner Geburt getauft und daher nach Martin von Tours (11. November) benannt.

Das bäuerliche Brauchtum hat vielen dieser Gedenktage – häufig in Reimform – Wetterregeln zugeordnet, die Aussagen über die Witterungsentwicklung und oftmals deren Auswirkungen auf die Landwirtschaft beschreiben. So lautet beispielsweise die Bauernregel für den Martinstag am 11. November: "Ist Martini klar und rein, bricht der Winter bald herein."





</doc>
<doc id="12661" url="https://de.wikipedia.org/wiki?curid=12661" title="Albessen">
Albessen

Albessen ist eine Ortsgemeinde im Landkreis Kusel in Rheinland-Pfalz an der Grenze zum Saarland. Sie gehört der Verbandsgemeinde Kusel-Altenglan an.

Albessen liegt auf dem Westrich. Nachbargemeinden sind Ehweiler und Konken im Osten und Herchweiler und Selchenbach als Grenzdörfer zum Saarland im Westen.

Der Ort wurde 1436 erstmals als „Albeßen“ urkundlich erwähnt. Im Jahr 1609 gab es 19 Haushalte mit insgesamt 81 Einwohnern im Dorf. Bis Ende des 18. Jahrhunderts gehörte Albessen zum Herzogtum Pfalz-Zweibrücken.

In der sogenannten Franzosenzeit war der Ort von 1798 bis 1814 dem Kanton Kusel im Saar-Departement zugeordnet.

Von 1816 an gehörte Albessen zum Königreich Bayern und war Teil des Landkommissariats Kusel im Rheinkreis. Im Jahr 1840 hatte der Ort 17 Häuser und 156 Einwohner. Er wurde als evangelisches Dorf bezeichnet und war nach Konken eingepfarrt.

Bis 1971 war die Gemeinde Albessen verwaltungsmäßig der Bürgermeisterei Konken zugeordnet und gehörte seit dem 1. Januar 1972 der Verbandsgemeinde Kusel an, die 2018 in der Verbandsgemeinde Kusel-Altenglan aufging.

Der Gemeinderat in Albessen besteht aus sechs Ratsmitgliedern, die bei der Kommunalwahl am 25. Mai 2014 in einer Mehrheitswahl gewählt wurden, und der ehrenamtlichen Ortsbürgermeisterin als Vorsitzender.

Die Baudenkmäler befinden sich in der Liste der Kulturdenkmäler in Albessen

Jeden ersten Mai wird der Maibaum aufgestellt. Er wird neben dem Dorfweiher in einer fest montierten Halterung verankert. Am Stamm befinden sich die Wappen einiger bekannter Familien, Betriebe und Vereine, z. B. dem Volkschor oder der Freiwilligen Feuerwehr. Anschließend werden Bier und Grillgut verkauft und im Kreis des Dorfes und der umliegenden Gemeinden gefeiert.

Ein anderes regelmäßiges Fest ist das Weiherfest direkt am Weiher im Ort. Beim Weiherfest wird das Anlegen des Weihers als Löschwasserteich gefeiert. Jeder Ort hat ein eigenes Sommerfest mit einem anderen Namen. In Kusel beispielsweise ist es die Kuseler Herbstmesse.

In Albessen gibt es eine Gaststätte (in deren Nebenraum einmal im Monat jeweils am zweiten Sonntag ein Gottesdienst der evangelischen Kirche abgehalten wird) und drei Voll- und einen Nebenerwerbsbauern. Eine von Schulklassen und anderen Interessierten oft besuchte Attraktion ist ein Biolandhof, der als Erster in Deutschland als eine Aktiengesellschaft geführt wird. Leiter und Vorstand der AG ist Kornelius Burgdörfer-Bensel. Zusätzlich gibt es noch einen Betrieb, der Ziegenkäse produziert und selbst vermarktet.



</doc>
<doc id="12663" url="https://de.wikipedia.org/wiki?curid=12663" title="Sextant">
Sextant

Ein Sextant (Spiegelsextant, auch "Sixtant") ist ein nautisches und optisches Messinstrument, mit dem man den Winkel zwischen den Blickrichtungen zu relativ weit entfernten Objekten, insbesondere den Winkelabstand eines Gestirns vom Horizont, bestimmen kann. Er wird hauptsächlich zur Höhenwinkel-Messung von Sonne und Sternen für die astronomische Navigation auf See verwendet, seltener auch in der Luftfahrt und bei Expeditionen. Früher wurde er auch in der Astronomie und der Landesvermessung verwendet.

Der Name "Sextant" kommt von dem Geräterahmen, der einen Kreissektor von etwa 60° (ein Sechstel eines Kreises) darstellt, womit infolge des Spiegelgesetzes Winkelmessungen in doppeltem Umfang, also bis 120° möglich sind. Der Sextant hat deshalb eine Skala von mindestens 120°. Demgegenüber hatte sein Vorläufer, der "Oktant", eine Winkelskala von mindestens 90°. Sein Rahmen umfasste 45°, also ein Achtel eines Kreises, wovon sich – analog zum Sextanten – sein Name ableitet. Hingegen ist der "Quadrant" ein Winkelmesser bezüglich der Lotrichtung und misst (ohne Spiegelung) Höhenwinkel bis 90°.

Grundlegende Komponenten des Sextanten sind der Zeigerarm "(Alhidade)", mit dem die Messung vorgenommen wird und dessen genaue Winkeleinstellung am "Gradbogen" abgelesen werden kann; ein Spiegel "(Indexspiegel)", der auf dem Drehpunkt der Alhidade senkrecht zur Instrumentenebene montiert ist und sich mit ihr dreht; und ein feststehender Spiegel "(Horizontspiegel)", der bei Nullstellung der Alhidade parallel zum Indexspiegel steht. Er ist entweder halbdurchlässig "(Vollsichtspiegel)" oder er besitzt eine verspiegelte rechte Seite und eine nicht verspiegelte, lichtdurchlässige linke Seite "(Halbspiegel)". Ein kleines auf den Horizontspiegel gerichtetes "Fernrohr" oder auch nur ein einfaches Rohr ohne Linsen dient zum Anvisieren der Ziele.

Die Alhidade wird durch eine an ihrem unteren Ende sitzende "Schraube" bewegt, die über ein Schneckengetriebe in eine "Verzahnung" am unteren Rand des Gradbogens eingreift (der Gradbogen entspricht dem Teilkreis beim Theodolit). Bei modernen Sextanten trägt die Schraube eine "Trommel" mit eingravierten Gradbruchteilen zur bequemeren Feinablesung (Trommelsextant). Für größere Bewegungen der Alhidade kann die Schnecke mittels einer Sperrklinke aus der Verzahnung gehoben werden.

Weitere Komponenten sind verschiedene "Filter" (Schattengläser), die in den Strahlengang geklappt werden können. Mit dem stärksten Filter kann die Sonne anvisiert werden, mit dem mittleren der Mond, ohne dass das Bild des Horizonts überstrahlt wird.

Einige Modelle besitzen einen "künstlichen Horizont", der nach dem Prinzip einer Wasserwaage funktioniert. Diese Wasserwaage ist dann über zusätzliche Spiegel durch die Einblicköffnung zu sehen und dient als physikalische Richtungsreferenz für die Messung, wenn der Horizont als optische Referenz nicht zur Verfügung steht.

Schaut man durch die Einblicköffnung auf den Horizontspiegel, so sieht man einerseits durch diesen hindurch (wegen seiner Halbtransparenz oder wegen der unverspiegelten Hälfte) in direkter Sicht das eine der beiden Ziele. Der Horizontspiegel ist so geneigt, dass sich der Indexspiegel in ihm spiegelt und so gleichzeitig auch das von diesem reflektierte Bild erkennbar ist. Der Beobachter sieht daher zwei überlagerte Bilder: das eine direkt, das andere nach zweifacher Reflexion. Auch bei einem Halbspiegel stehen die Bilder nicht strikt nebeneinander, sondern überlagern sich teilweise, weil die Grenze zwischen verspiegeltem und unverspiegeltem Teil von dem auf 'unendlich' eingestellten Fernrohr sehr unscharf wiedergegeben wird.

In der Nullstellung der Alhidade sind der Horizontspiegel und der Indexspiegel parallel und beide Bilder daher deckungsgleich. (Genaugenommen gilt dies nur bei unendlich großer Entfernung des betrachteten Zieles, denn es entsteht eine kleine Parallaxe durch die Höhendifferenz zwischen Einblicköffnung und Indexspiegel, bei Gestirnen und entfernten Landmarken kann diese Parallaxe aber vernachlässigt werden.) Um den Winkel zwischen zwei Zielen zu messen, wird mit der Alhidade der Indexspiegel gegenüber dem Horizontspiegel so lange gedreht, bis die Bilder der beiden Ziele zusammenfallen. Wegen des Reflexionsgesetzes "Einfallswinkel = Ausfallswinkel" ist der gesamte Reflexionswinkel doppelt so groß wie der Schwenkwinkel der Alhidade. Die Skala ist deshalb mit dem jeweils doppelten Wert des Schwenkwinkels beschriftet. Aus diesem Grund genügt auch ein 60° großer Gradbogen für einen Messbereich von +100° bis −20°.

Wird der Sextant während der Messung nicht völlig ruhig gehalten, so schwanken beide Bilder "gemeinsam" im Gesichtsfeld hin und her, sodass eine zweifelsfreie Überlagerung beider Ziele und damit eine korrekte Messung nach wie vor möglich, wenn auch etwas erschwert ist. Der Sextant kann daher freihändig und auch in schwankender Umgebung wie z. B. an Bord eines Schiffes verwendet werden.

Bei der Höhenmessung von Gestirnen für die astronomische Navigation wird mit dem Fernrohr durch den Horizontspiegel hindurch der Horizont angepeilt. Bei der Ausführung als Halbspiegel ist in der linken Hälfte des Blickfeldes der Horizont zu sehen, auf der rechten Seite der Horizontspiegel mit dem Bild des Indexspiegels. Moderne Sextanten mit Vollsichtspiegel ermöglichen es (unter Lichtverlust), den Horizont über die gesamte Breite des Blickfeldes zu sehen. Der Beobachter verstellt die Alhidade so lange, bis das gespiegelte Bild des Himmelskörpers im Horizontspiegel sichtbar ist und sich scheinbar auf Höhe des Horizonts befindet. Bei Navigationssternen wird der Winkel zwischen Stern und Horizont gemessen, also Horizont und Stern im Spiegel zur Deckung gebracht. Im Fall des Mondes oder der Sonne wird hierfür die Ober- oder Unterkante herangezogen.

Die Alhidade muss so eingestellt werden, dass das Objekt (im Beispiel der Sonnenunterrand) am untersten Punkt den Horizont berührt. Voraussetzung für eine korrekte Messung ist eine exakt lotrechte Haltung des Sextanten. Bei schräger Haltung würde nicht der Winkel zwischen dem Sonnenunterrand und dem senkrecht darunter gelegenen Punkt des Horizonts gemessen, sondern der größere Winkel zu einem anderen Punkt des Horizonts. Man schwenkt daher den Sextanten seitlich um die Fernrohrachse hin und her und justiert die Alhidade solange, bis der Sonnenunterrand an einem Punkt der Schwenkbewegung den Horizont gerade eben berührt.

In dem Augenblick, in dem die Alhidade richtig eingestellt ist, bestimmt der Beobachter den Zeitpunkt der Beobachtung mit Hilfe einer genau gehenden Uhr (am bequemsten durch Zuruf an einen Assistenten). Der Beobachter kann auch das Bild des Gestirns lediglich in die Nähe des Horizonts bringen und warten, bis seine auf- oder absteigende Bewegung im Zuge der scheinbaren Himmelsdrehung es durch den Horizont trägt. Es ist dann nur noch nötig, den genauen Zeitpunkt dieses Ereignisses zu bestimmen.

Die ganzzahligen Grade des eingestellten Winkels werden am Gradbogen abgelesen, die Gradbruchteile bei Trommelsextanten an der Unterteilung der Ablesetrommel. Bevor der gemessene Winkel zur Navigation verwendet werden kann, sind noch mehrere rechnerische Korrekturen erforderlich, u. a. für (vorher zu bestimmende) Gerätefehler (Indexfehler), die Differenz zwischen Horizont und Waagerechte (so genannte Kimmtiefe, abhängig von der Beobachtungshöhe), die Verfälschung der Gestirnshöhe durch atmosphärische Lichtbrechung und gegebenenfalls die Distanz der Gestirnsober- oder unterkante von der Mitte.

Bei der Verwendung in terrestrischer Navigation oder für Vermessungsaufgaben wird der Sextant analog verwendet, dann jedoch manchmal auch waagerecht gehalten, wenn die Horizontalwinkel zwischen drei der Position nach bekannten Landmarken gemessen werden. Zur Abstandsberechnung durch Messung von Höhenwinkeln an bekannten Bauwerken, z. B. Leuchttürmen, deren genaue Höhe man einem Leuchtfeuerverzeichnis entnehmen kann, wird der Sextant senkrecht gehalten.


Schon zu Zeiten der Segelschifffahrt hatten Sextanten eine Messgenauigkeit von etwa einer Bogenminute (1/60 Grad), was einer Positionsgenauigkeit von einer Seemeile entspricht. Moderne Sextanten können eine mechanische Genauigkeit von 10–20 Bogensekunden erreichen.

In der nautischen Praxis ist die Handhabung des Sextanten v. a. durch Wellengang erschwert, so dass die tatsächliche Messgenauigkeit selten besser als eine Bogenminute ist; unter schwierigen Bedingungen kann eine Messung mit einem Fehler von fünf Bogenminuten noch als gut gelten.
In der Luftfahrt führt die hohe Geschwindigkeit des Flugzeugs zu Ungenauigkeiten, da sich das Flugzeug bereits während des Messvorgangs um einen Betrag bewegt, der weit über der Genauigkeit des Instruments liegt. Durch „Bracketing“ lässt sich dieser Fehler weitenteils eliminieren. Hier wird derselbe Stern am Anfang und Ende der Beobachtungsreihe gemessen, um so einen Mittelwert zu gewinnen.

Das erste Konzept für ein Gerät zur Winkelmessung mit Hilfe von Spiegeln stammt von Isaac Newton, der seinen Entwurf 1700 an die Royal Society einreichte. Seine Skizzen blieben jedoch unbeachtet und wurden erst 1742, nach seinem Tod, veröffentlicht.

Um 1730 entwickelten unabhängig voneinander John Hadley (1682–1744), englischer Astronom und Mathematiker, und Thomas Godfrey (1704–1749), Optiker und Erfinder in den britischen Kolonien in Amerika, den Sextanten und reichten ihre Entwürfe an die Royal Society ein.
Hadleys Konstruktion, damals ein Oktant, erwies sich als die zweckmäßigere und wurde der Vorläufer aller weiteren Sextanten. Beide Versionen galten aber als gleichwertig und so teilten sich beide Erfinder einen Preis, der für die genaue Positionsbestimmung auf See ausgesetzt worden war.

Die ersten Sextanten waren noch aus Holz gebaut. Auf See verzog sich das Holz durch die Luftfeuchtigkeit, so dass die Instrumente bald aus Metall gefertigt wurden. Der Sextant ist ein recht empfindliches Instrument.
Eine kleine Verformung des Zeigers oder eine kleine Verstellung des Spiegels durch ein Fallenlassen kann zu einer falschen Positionsbestimmung führen, die um viele Kilometer neben der tatsächlichen Position liegt. Um sicher sein zu können, dass das Gerät nicht beschädigt ist, wurde ein Sextant in der Regel neu gekauft und nur selten aus der Hand gegeben. Neuere Instrumente lassen sich an den Halterungen der Spiegel justieren. Wichtig ist hier die exakte Parallelstellung der Spiegel zueinander sowie die rechtwinklige Anordnung zur Geräteebene sowie die Nullstellung.

Der Sextant ersetzte schnell den Jakobsstab und das Astrolabium. Im Bereich der Landvermessung wurde er später vom Theodolit abgelöst. Bei der Navigation auf See verlor der Sextant erst mit der Satellitennavigation (GPS) an Bedeutung.
In der Luftfahrt war der Sextant nur kurze Zeit in Gebrauch und wurde bald durch Funknavigation und Trägheitsnavigation ersetzt, die heute oft durch Satellitennavigation ergänzt werden.

Der auf der 10-DM-Banknote abgebildete Sextant – auf Basis eines Quintanten (144° Messbereich) der englischen Firma Troughton – ist von Carl Friedrich Gauß 1821 mit einem dritten Spiegel versehen worden, um ihn in der Landesvermessung als Sonnenspiegel (Licht-Scheinwerfer = Vize-Heliotrop) zur Sichtbarmachung von Vermessungspunkten zu verwenden.




</doc>
<doc id="12667" url="https://de.wikipedia.org/wiki?curid=12667" title="Santo Domingo">
Santo Domingo

Santo Domingo de Guzmán ist die Hauptstadt der Dominikanischen Republik und mit 2.987.013 Einwohnern in der eigentlichen Stadt und 3.813.214 in der Agglomeration (Stand jeweils 2006) zugleich die größte Metropole des Landes. Santo Domingo und das etwa gleich große Havanna, die Hauptstadt Kubas, sind zudem die beiden größten Städte der Westindischen Inseln in der Karibik. Santo Domingo ist die älteste von Europäern errichtete Stadt in der Neuen Welt.

Die Stadt liegt an der Südküste der karibischen Insel Hispaniola an der Mündung des Río Ozama und besitzt den bedeutendsten Hafen der Republik ("La Haina").

Die Durchschnittstemperatur am Tag beträgt 30,25 °C, in der Nacht 21,08 °C. Der jährliche Niederschlag 1385 Millimeter und die Durchschnittstemperatur des Wassers beträgt 26,75 °C.
Luftfeuchtigkeit im Sommer: 90 %
Santo Domingo war seit 1496 von Europäern besiedelt, wurde aber offiziell erst 1498 von Christoph Kolumbus’ Bruder Bartolomeo an der Mündung des Flusses Ozama gegründet und ist somit die älteste von Europäern errichtete Stadt in der Neuen Welt. Sie hieß zunächst "La Nueva Isabela".

1502 wurde sie wegen eines Hurricans und einer Ameisenplage vom damaligen Statthalter der Insel, Nicolás de Ovando, am Westufer des Flusses Ozama neu gegründet und erhielt den heutigen Namen. Die neu gegründete Stadt wies erstmals den Grundriss einer Idealstadt auf, wie ihn die Städtebauer der Renaissance forderten. Das Stadtbild mit seinen rechtwinkelig angelegten Straßen und einem zentral gelegenen Platz ("Plaza de Armas" oder "Plaza Mayor") sollten prägend für alle folgenden Neugründungen in ganz Lateinamerika werden. Im spanischen Kolonialreich war Santo Domingo der Sitz des Gouverneurs und der "Real Audiencia".

Die Kathedrale "Basilica Menor de la Virgen de La Anunciación " ist die älteste Kathedrale Amerikas (Grundsteinlegung 1521, eingeweiht 1540). 1546 wurde sie von Papst Paul III. zur ersten erzbischöflichen Kathedrale der Neuen Welt erhoben. Sie barg bis 1992 die Gebeine von Kolumbus, die anlässlich der 500-Jahr-Feier der Entdeckung Amerikas in den neuen "Faro a Colón" überführt wurden. Die Kathedrale gilt als unvollendet, da die ursprünglich geplanten Glockentürme nicht errichtet wurden.

1538 wurde hier die älteste Universität Amerikas eröffnet. Während der Trujillo-Diktatur hieß Santo Domingo von 1936 bis 1961 "Ciudad Trujillo".

Der internationale Flughafen Las Américas befindet sich in der Nähe von Santo Domingo. Seit dem 30. Januar 2009 verfügt die Stadt über eine U-Bahn, die Metro Santo Domingo. Das Netz hat derzeit (2017) zwei Linien und 30 Stationen.

Die UNESCO erklärte 1990 die historische Altstadt () am westlichen Ufer des Río Ozama zum Weltkulturerbe der Menschheit.

Im Jahr 2003 wurden in Santo Domingo die 14. Panamerikanischen Spiele ausgetragen.

Am "Plaza de España" mit Blick über den Río Ozama liegt der unter Diego Kolumbus von 1510 bis 1514 erbaute Palast des damaligen Vizekönigs. Hier befand sich der Sitz der spanischen Kolonialregierung in der Neuen Welt für sechs Jahrzehnte. Nach dem Tod von María de Toledo, der Frau von Diego Colón, wurde der Palast nicht mehr bewohnt und zerfiel. Erst in den 1950er-Jahren wurde er restauriert und beherbergt nun das Vizekönigliche Museum.

Ruinen des ersten Klosters auf dem amerikanischen Kontinent

Ehemaliger Gouverneurspalast, errichtet als Verwaltungssitz der spanischen Kolonialherren auf dem amerikanischen Kontinent, wird heute gelegentlich für Empfänge und Hochzeiten benutzt.

Älteste Festung in Amerika. Diese Burg aus dem 16. Jahrhundert wurde oberhalb des Hafens von Santo Domingo an der Mündung des Flusses Ozama errichtet, um die Hafeneinfahrt gegen Feinde vom Meer aus zu verteidigen.

Kreuzartiger Monumentalbau aus Beton, ca. 250 m lang von Ost nach West und ca. 65 m in Nord-Süd-Richtung, eingeweiht 1992 anlässlich der 500. Wiederkehr des Jahres der Entdeckung Amerikas, eines der meist besuchten Denkmäler in der Dominikanischen Republik. Beeindruckend, vor allem nachts, wenn vertikale Lichtsäulen von 157 Strahlern das große Kreuz meilenweit sichtbar in den Himmel projizieren. Der Bau soll die Gebeine von Christoph Kolumbus beherbergen, diese Ehre beansprucht jedoch auch Sevilla in Spanien für sich.
Mit vier Universitäten und einem Technologie-Institut ist die Stadt ein bedeutendes Bildungszentrum: (alle externen Weblinks in Spanisch) 


„Santo Domingo“ war ein sehr erfolgreicher Schlagertitel von Wanda Jackson in den 1960er Jahren und wurde von mehreren Künstlern gecovert.





</doc>
<doc id="12668" url="https://de.wikipedia.org/wiki?curid=12668" title="Schrödingergleichung">
Schrödingergleichung

Die Schrödingergleichung ist eine grundlegende Gleichung der Quantenmechanik. Sie beschreibt in Form einer partiellen Differentialgleichung die Zeitentwicklung des quantenmechanischen Zustands eines nichtrelativistischen Systems.
Die Gleichung wurde 1926 von Erwin Schrödinger (1887–1961) zuerst als Wellengleichung aufgestellt und bei ihrer ersten Anwendung erfolgreich zur Erklärung der Spektren des Wasserstoffatoms genutzt.

Die Schrödingergleichung besagt, dass die zeitliche Veränderung eines Zustands durch den Hamiltonoperator bestimmt ist. Wenn das Quantensystem ein klassisches Analogon hat (z. B. Teilchen im dreidimensionalen Raum), lässt sich der Hamiltonoperator schematisch aus der klassischen Hamiltonfunktion erhalten. Für manche Systeme werden Hamiltonoperatoren auch direkt nach quantenmechanischen Gesichtspunkten konstruiert (Beispiel: Hubbard-Modell).

Als Spezialfall der zeitlichen Entwicklung beschreibt die Schrödingergleichung die Zustände eines Quantensystems, bei denen sich das Betragsquadrat der Wellenfunktion mit der Zeit nicht ändert (stationäre Zustände, Eigenzustände des Hamiltonoperators), und ermöglicht die Berechnung der durch solche Zustände definierten Energieniveaus.

Die Schrödingergleichung bildet das Fundament für fast alle praktischen Anwendungen der Quantenmechanik. Seit 1926 gelang mit ihr die Erklärung vieler Eigenschaften von Atomen und Molekülen (bei denen die Elektronenwellenfunktionen als Orbitale bezeichnet werden) sowie von Festkörpern (Bändermodell).

Die nach ihm benannte Gleichung wurde von Schrödinger 1926 postuliert. Ausgangspunkt dabei waren die auf Louis de Broglie zurückgehende Vorstellung von Materiewellen und die Hamilton-Jacobi-Theorie der klassischen Mechanik. Die Wirkung formula_1 der klassischen Mechanik wird dabei mit der Phase einer Materiewelle identifiziert. Sobald typische Abstände kleiner als die Wellenlänge sind, spielen Beugungsphänomene eine Rolle, und die klassische Mechanik muss durch eine Wellenmechanik ersetzt werden.

Die Schrödingergleichung kann nicht aus der klassischen Physik hergeleitet werden, sondern ist ein Postulat.
Formal kann die Schrödingergleichung jedoch aus der Hamiltonfunktion (Ausdruck für die Energie) des betrachteten Problems

abgeleitet werden, indem man die klassischen Größen Energie, Impuls und Ort gemäß dem Korrespondenzprinzip durch die entsprechenden quantenmechanischen Operatoren ersetzt:

Anschließendes Anwenden auf die unbekannte Wellenfunktion formula_4 ergibt die Schrödingergleichung

Auf die gleiche Weise kann die Hamilton-Funktion in einen Hamilton-Operator umgewandelt werden.

Historisch gesehen ging Schrödinger von Louis de Broglies Beschreibung freier Teilchen aus und führte in seiner Arbeit Analogien zwischen Atomphysik und elektromagnetischen Wellen, in Form von De-Broglie-Wellen (Materiewellen), ein:

wobei formula_7 eine Konstante ist. Diese Wellenfunktion ist eine Lösung der eben genannten Schrödingergleichung mit formula_8.
In der statistischen
Interpretation der Quantenmechanik (begründet von Max Born) gibt das Betragsquadrat formula_9 der Wellenfunktion die Aufenthaltswahrscheinlichkeitsdichte des Teilchens an.

Eine andere Möglichkeit, die Schrödingergleichung aufzustellen, benutzt den von Richard Feynman eingeführten Begriff des Pfadintegrals. Diese alternative Herleitung betrachtet die Wahrscheinlichkeiten für die verschiedenen Bewegungen (Pfade) des zu untersuchenden Teilchens von einem Ort formula_7 nach formula_11 und führt damit wieder zu derselben Schrödingergleichung. Auch hierbei spielt die klassische Wirkung formula_1 eine zentrale Rolle. 

Die Schrödingergleichung in ihrer allgemeinsten Form lautet

Dabei bezeichnet formula_14 die imaginäre Einheit, formula_15 die reduzierte Plancksche Konstante, formula_16 die partielle Ableitung nach der Zeit und formula_17 den Hamiltonoperator des Systems.
Der Hamiltonoperator wirkt in einem Hilbertraum, die zu bestimmende Größe formula_18 ist ein Zustandsvektor in diesem Raum. Diese generische Form der Schrödingergleichung gilt auch in der relativistischen Quantenmechanik und in der Quantenfeldtheorie. In letzterem Fall ist der Hilbertraum ein Fockraum.

Ein quantenmechanischer Zustand wird durch einen Vektor in einem komplexen unitären Hilbertraum formula_19 beschrieben. Meist wird die Dirac-Notation mit Bra und Ket verwendet. Die Struktur des Hilbertraums wird durch das betrachtete System bestimmt. Für die Beschreibung des Spins eines Teilchens mit Spin 1/2 ist der Hilbertraum beispielsweise zweidimensional formula_20, für einen harmonischen Oszillator ist seine Dimension abzählbar unendlich formula_21. Ein freies Teilchen wird in einem (uneigentlichen) Hilbertraum mit überabzählbar unendlicher Dimension beschrieben. 

Die durch die Schrödingergleichung beschriebene Zeitentwicklung ist eine unitäre Transformation des Zustandsvektors im Hilbertraum. Da es sich dabei um eine lineare Transformation handelt, gilt das Superpositionsprinzip. Eine weitere Konsequenz ist die Möglichkeit der quantenmechanischen Verschränkung nicht wechselwirkender Teilsysteme.

Die Zeitentwicklung der Zustände wird durch die Anwendung eines Hamiltonoperators formula_17 auf die Zustände beschrieben. „Ausintegriert“ erhält man den Zeitentwicklungsoperator:

Der Zeitentwicklungsoperator hat für zeitunabhängige Hamiltonoperatoren formula_24 die Form:

Die Norm eines Zustands ist gleich der "L"-Norm, die durch das Skalarprodukt induziert wird:

Die Wahrscheinlichkeitserhaltung (Erhaltung der Norm des Zustands) drückt sich durch die Unitarität des Zeitentwicklungsoperators formula_27 aus, was wiederum darauf beruht, dass formula_17 selbstadjungiert ist. Mit formula_29 und formula_30 folgt:

Setzt man die Erhaltung der Wahrscheinlichkeitsdichte in der Theorie voraus, so muss der Zeitentwicklungsoperator unitär sein. Die Änderung eines zeitabhängigen Zustandes formula_23 wird daher durch einen anti-hermiteschen Operator bestimmt, wodurch man bereits vor Kenntnis der Schrödingergleichung ohne Beschränkung der Allgemeinheit 
ansetzen kann. Damit reduziert sich das Postulieren der Schrödingergleichung auf die Bestimmung der Gestalt des hermiteschen Operators formula_17.

Die Hermitezität ist eine Forderung, die an alle Operatoren der Quantenmechanik gestellt wird, die nach dem Korrespondenzprinzip Messergebnisse repräsentieren. Da Messergebnisse stets reell sein müssen, kommen als zugeordnete Operatoren nur hermitesche Operatoren in Frage. Solche Operatoren werden auch Observablen genannt.

Die von Schrödinger aufgestellte Gleichung ist Prototyp und Spezialfall des allgemeinen Schemas. Sie beschreibt die Quantenmechanik von nichtrelativistischen Punktteilchen, der Hilbertraum ist der Raum komplexwertiger Funktionen im Konfigurationsraum.

Die komplexwertige Wellenfunktion formula_35 eines Punktteilchens in einem Potential formula_36 ist eine Lösung der Schrödingergleichung

wobei formula_38 die Masse des Teilchens, formula_39 sein Ort, formula_40 der Laplace-Operator und formula_41 die Zeit sind. 

Die Schrödingergleichung ist eine lineare partielle Differentialgleichung zweiter Ordnung. Aufgrund der Linearität gilt das Superpositionsprinzip: Wenn formula_42 und formula_43 Lösungen sind, so ist auch formula_44 eine Lösung, wobei formula_45 und formula_46 beliebige komplexe Konstanten sind.

Mit dem Hamiltonoperator

lässt sich die Schrödingergleichung in ihrer allgemeinen Form

schreiben.

"Hinweis: Elektrodynamische Größen sind hier im CGS-Einheitensystem angegeben"

Falls das Teilchen, wie im Falle eines Elektrons oder Protons, eine elektrische Ladung besitzt, so verallgemeinert sich bei Anwesenheit eines äußeren elektromagnetischen Feldes der Ein-Teilchen-Hamiltonoperator in der Orts-Darstellung zu
wobei hier formula_50 die elektrische Ladung des Teilchens (formula_51 bei Elektronen), formula_52 die Lichtgeschwindigkeit im Vakuum, formula_53 das Vektorpotential und formula_54 das skalare Potential bezeichnen. Die sich so ergebende Schrödingergleichung tritt dabei an die Stelle der klassischen Gleichung mit Lorentzkraft. Die Potentiale sind durch folgende Beziehungen mit dem elektrischen Feld formula_55 bzw. dem magnetischen Feld formula_56 verknüpft:

Der Hamiltonoperator eines Vielteilchensystems ist die Summe der Ein-Teilchen-Hamiltonoperatoren und der Wechselwirkungsenergien (zum Beispiel der Coulomb-Wechselwirkungen zwischen den Teilchen)

Mehrere Teilchen werden durch eine einzelne Wellenfunktion formula_58 dargestellt. Diese Wellenfunktion hat als Parameter die Positionen aller Teilchen sowie die Zeit.

Mit der Schrödingergleichung wurde die Ad-hoc-Konstruktion des bohrschen Atommodells überwunden (wie zuvor schon mit der umständlicheren Heisenberg'schen Matrizenmechanik). Die diskreten Energieniveaus des Wasserstoffatoms, die im Bohrschen Modell stationären klassischen Bahnen eines Elektrons im Coulombpotential des Atomkerns zugeordnet sind, ergeben sich im Rahmen der Schrödingergleichung als Eigenwerte der Schrödingergleichung für ein Elektron im Potential des Atomkerns.

Während die Bahn formula_60 eines Teilchens in der klassischen Mechanik durch die Newtonsche Bewegungsgleichung bestimmt ist, liefert in der Quantenmechanik die Schrödingergleichung stattdessen eine Wahrscheinlichkeitsverteilung formula_61 für den Aufenthaltsort des Teilchens.
Man spricht auch veranschaulichend davon, dass das Teilchen über den Raum delokalisiert sei. Als umfassendere Theorie muss die Quantenmechanik allerdings die klassische Mechanik enthalten. Eine Form dieser Korrespondenz wird durch das Ehrenfest-Theorem hergestellt. Das Theorem besagt u. a., dass der Mittelwert der Teilchenkoordinate die klassische Bewegungsgleichung erfüllt. Relevant und evident wird die Korrespondenz bei lokalisierten kohärenten Wellenpaketen. Solche Wellenpakete lassen sich bei höheren Quantenzahlen, also z. B. bei höheren Anregungszuständen des Wasserstoffatoms konstruieren.

In der Schrödingergleichung kommen die Wellenfunktion und die Operatoren im sogenannten Schrödinger-Bild vor, in dem eine Bewegungsgleichung für die Zustände betrachtet wird. Im Heisenberg-Bild werden stattdessen Bewegungsgleichungen für die Operatoren selbst betrachtet. Diese Bewegungsgleichungen werden als Heisenbergsche Bewegungsgleichung bezeichnet. Die beiden Formulierungen sind mathematisch äquivalent.

Die Schrödingergleichung ist deterministisch, das heißt, dass ihre Lösungen bei Vorgabe von Anfangsbedingungen eindeutig sind. Andererseits sind die Lösungen der Schrödingergleichung nach der Kopenhagener Deutung statistische Größen, aus denen nur Aussagen über die Mittelwerte von Messergebnissen in gleichartigen Versuchsanordnungen folgen. Nach der Kopenhagener Deutung der Quantenmechanik liegt dies nicht an einem Mangel der Messanordnung, vielmehr ist dies durch die Natur selbst bedingt.

Für die statistische Interpretation der Quantenmechanik ist es notwendig, die Lösungen der Schrödingergleichung so zu normieren, dass

ist. Diese sogenannte "Normierungsbedingung" sagt aus, dass die Wahrscheinlichkeit, dass das Teilchen irgendwo im gesamten Raum zu finden ist, bei 1 liegt. Für die so erhaltenen "normierten" Lösungen entspricht dann formula_63 der Aufenthaltswahrscheinlichkeitsdichte des Teilchens am Ort formula_39 zum Zeitpunkt formula_41. Allerdings ist nicht jede Lösung einer Schrödingergleichung normierbar. Sofern existent, ist diese normierte Lösung bis auf einen Phasenfaktor der Form formula_66 für ein reelles formula_67, das aber physikalisch bedeutungslos ist, eindeutig bestimmt.

Da die Schrödinger-Gleichung invariant ist unter der Phasentransformation formula_68 (U(1)-Symmetrie), folgt aus dem Noether-Theorem die Erhaltung der Normierung; die Wahrscheinlichkeit ist also eine Erhaltungsgröße.

Aus der Wellenfunktion ergeben sich die physikalischen Eigenschaften des Teilchens. Beispielsweise wird der klassische Wert für den Ort des Teilchens formula_60 durch den mittleren Ort des Teilchens zur Zeit formula_41, also
ersetzt, während der klassische Wert für den Impuls des Teilchens durch folgenden Mittelwert ersetzt wird:
Jede klassische Messgröße formula_73 wird so durch eine Mittelung des zugehörigen Operators über den Raum, in dem sich das Teilchen befindet, ersetzt:
Der Ausdruck formula_75 wird als Erwartungswert von formula_76 bezeichnet. Der Erwartungswert der Energie ist gleich formula_77.

Für ein System mit Hamiltonoperator formula_17 ohne explizite Zeitabhängigkeit ist der Ansatz
naheliegend. Hierbei ist die Zeitabhängigkeit des Zustandsvektors durch einen Faktor formula_80 mit konstanter Frequenz formula_81 ausgedrückt. Für den zeitunabhängigen Faktor des Zustandsvektors wird die Schrödingergleichung zur Eigenwertgleichung

Entsprechend der Planckschen Formel hat ein solches System die Energie

Diskrete Eigenwerte entsprechen diskreten Energieniveaus des Systems („Quantisierung als Eigenwertproblem“).

"Anmerkung:" Eine gebräuchliche Ortsraumdarstellung (→ "Abschnitt: Orts- und Impulsdarstellung") der „zeitfreien“ (stationären) Schrödingergleichung lautet:

Die Lösungen der Schrödingergleichung (bzw. Pauligleichung) beinhalten im Prinzip die ganze Festkörperphysik und Chemie (eine Einschränkung: für innere Elektronen schwerer Atome sind relativistische Korrekturen nicht mehr klein).
Lösungen in geschlossener Form gibt es nur für einige 1-Elektron-Systeme (Wasserstoffatom, Potentialbarriere, harmonischer Oszillator, Morse-Potential, …). Ab Heliumatom oder Wasserstoffmolekül ist man auf numerische Techniken angewiesen.

Mit Computerunterstützung und geeigneten Methoden (Störungsrechnung, Variationsansätze, …) lassen sich Systeme bis zu einer Größe von etwa 10 Elektronen numerisch ohne Näherung behandeln, d. h. die Verfahren konvergieren mit steigendem Rechenaufwand gegen die exakte Lösung. Ein Beispiel solcher Verfahren ist Configuration Interaction.

Bei diesen prinzipiell exakten Verfahren ist im "N"-Teilchen-Fall eine Wellenfunktion im "3N"-dimensionalen Konfigurationsraum zu bestimmen. Verwendet man "q" (Stützpunkt- oder Variations-) Werte je Dimension, dann sind "q" Werte zu berechnen.
Im Ergebnis macht diese exponentiell wachsende Anforderung an Speicher und Rechenleistung exakte Rechnungen für die meisten Systeme unmöglich, man denke etwa an eine organische Verbindung mit gerade mal zwei Kohlenstoffatomen und mehr als 24 Elektronen. Walter Kohn hat dieses exponentielle Ressourcenwachstum als „Exponentialbarriere“ bezeichnet. 

Größere Systeme werden daher mit Näherungsverfahren untersucht. Bekannte Verfahren sind die Hartree-Fock-Näherung, Erweiterungen und die Split-Operator-Methode in der Theoretischen Chemie.

Eine Sonderrolle spielt die auf Walter Kohn zurückgehende Dichtefunktionaltheorie, da diese gezielt die Exponentialbarriere umgeht. Damit lassen sich mit ab initio-Rechnungen Gitterkonstanten und Bindungsenergien auch komplizierter Atome und Verbindungen mit Fehlern im Prozentbereich berechnen.

Im eindimensionalen Fall eines freien Teilchens reduziert sich der Laplace Operator zu einer doppelten Ableitung und das Potential formula_36 verschwindet.

Im Fall einer gaussförmigen Amplitudenverteilung ist eine Lösung der eindimensionalen Schrödingergleichung mit verschwindendem Potential:

Hier ist formula_88 die halbe Breite des Wellenpakets und formula_89 die Wellenlänge zum Zeitpunkt formula_90. Die folgenden Bilder zeigen den räumlichen und zeitlichen Verlauf der Wellenfunktion formula_91 für verschiedene Anfangsbedingungen. Im Fall einer reinen Gaussverteilung verbreitert sich die Wellenfunktion zu beiden Seiten. 
Wird die anfängliche Gaussverteilung mit der komplexen Schwingung formula_92 multipliziert, ergibt sich ein bewegtes Teilchen mit Dispersion.

Dieses Beispiel beschreibt ein einfaches Modell für chemische Bindung (siehe Feynman Lectures). Ein Elektron ist an einen Atomkern 1 gebunden und befindet sich im Zustand
formula_93, oder aber an einen Atomkern 2 und befindet sich im Zustand formula_94.
Wenn keine Übergänge möglich sind, gilt jeweils die stationäre Schrödingergleichung.
Wenn Übergänge von formula_93 nach formula_94 möglich sind, muss der Hamiltonoperator bei Anwendung auf Zustand formula_93 eine Beimischung von Zustand formula_94 erzeugen, und analog für Übergänge von formula_94 nach formula_93.
Ein Parameter formula_101 bestimmt die Übergangsrate. Das System wird dann wie folgt modelliert:
Durch Addition und Subtraktion dieser Gleichungen sieht man, dass es "neue" stationäre Zustände in Form von Superpositionen aus formula_93 und formula_94 gibt:
denn für diese findet man mit elementarer Algebra
Die Vorfaktoren der stationären Zustände werden wieder als messbare Energien interpretiert. Eine der beiden Energien
(je nach Vorzeichen von formula_101) ist kleiner als das ursprüngliche
formula_108. Der entsprechende Superpositionszustand ist der Bindungszustand des Moleküls.

Für die Schrödingergleichung in einem Hilbertraum lässt sich mathematisch zeigen, dass der Hamiltonoperator formula_17 selbstadjungiert ist. Dann folgt aus dem Satz von Stone die Existenz einer unitären Gruppe und damit die eindeutige Lösbarkeit des Anfangswertproblems. Dabei ist es aus mathematischer Sicht wichtig, Selbstadjungiertheit formula_110 von der schwächeren Eigenschaft der Symmetrie formula_111 zu unterscheiden. Letztere lässt sich in der Regel durch eine partielle Integration zeigen, für die Selbstadjungiertheit ist eine detaillierte Untersuchung des Definitionsbereichs des adjungierten Operators notwendig. Für beschränkte Operatoren fallen beide Begriffe zusammen, aber Schrödingeroperatoren sind in der Regel unbeschränkt und können nach dem Satz von Hellinger-Toeplitz nicht auf dem ganzen Hilbertraum definiert werden. Danach gilt es das Spektrum von formula_17 zu untersuchen um die Dynamik zu verstehen.

Die Schrödingergleichung ohne Potential (freie Schrödingergleichung)

kann mittels Fourier-Transformation behandelt werden und der freie Schrödingeroperator ist auf dem Sobolev-Raum formula_114 selbstadjungiert. Das Spektrum ist gleich formula_115.

Die Normerhaltung
lässt sich durch Fourier-Transformation zeigen. Sie drückt im Fall formula_117 die Erhaltung der Wahrscheinlichkeiten aus.

Es gilt

das durch Separation in Kugelkoordinaten explizit lösbar ist. Betrachtet man Atome mit mehr als einem Elektron oder Moleküle, so wurde die Selbstadjungiertheit erst später von Tosio Kato bewiesen. Die Struktur des essentiellen Spektrums wird in diesem Fall durch das HVZ-Theorem (nach W. Hunziker, C. van Winter und GM Zhislin) beschrieben. Solche Modelle können in der Regel nur numerisch gelöst werden.

Die eindimensionale Schrödingergleichung ist ein Spezialfall einer Sturm-Liouville-Gleichung.

Die Wechselwirkung des Spins oder Eigendrehimpulses des Teilchens mit einem äußeren Magnetfeld wird in obiger Form der Schrödingergleichung nicht berücksichtigt. Wenn diese Wechselwirkung nicht vernachlässigt werden soll, ist für ein Elektron bei Anwesenheit eines äußeren Magnetfeldes die Pauli-Gleichung zu benutzen.

Die Pauli-Gleichung ist jedoch nicht lorentzinvariant, sondern „nur“ galilei-invariant (nicht relativistisch). Die korrekte relativistische Verallgemeinerung der Schrödinger- und auch der allgemeineren Pauli-Gleichung stellt für Fermionen die lorentzinvariante Diracgleichung dar, die im Gegensatz zur Schrödingergleichung eine partielle Differentialgleichung 1. Ordnung ist.

Eine Reihe von Problemen in der Physik führt auf eine Verallgemeinerung, die nichtlineare Schrödingergleichung

mit einem nichtlinearen Selbstwechselwirkungsterm formula_120. Dabei wurde die explizite Abhängigkeit der Lösungsfunktion formula_121 von Zeit und Ort weggelassen. Speziell im Fall der kubischen, nichtlinearen Schrödingergleichung formula_122, formula_123 und einer Dimension formula_124 handelt es sich um eine integrable Wellengleichung mit Solitonenlösungen. Sie taucht zum Beispiel bei der Beschreibung von Lichtwellen in Glasfasern und Wasserwellen auf. In Dimension formula_125 hat man im kubischen Fall die Gross-Pitaevskii-Gleichung die das Bose-Einstein-Kondensat beschreibt.

Nimmt man eine gravitative Selbstwechselwirkung der Teilchen an, enthält man die nichtlineare Schrödinger-Newton-Gleichung.

Schrödingers Originalarbeiten

Die Schrödingergleichung wird in allen üblichen Lehrbüchern der Quantenmechanik behandelt, zum Beispiel:


Mathematik:



</doc>
<doc id="12672" url="https://de.wikipedia.org/wiki?curid=12672" title="Otto von Bismarck">
Otto von Bismarck

Otto Eduard Leopold von Bismarck-Schönhausen, ab 1865 Graf von Bismarck-Schönhausen, ab 1871 Fürst von Bismarck, ab 1890 auch Herzog zu Lauenburg (* 1. April 1815 in Schönhausen (Elbe); † 30. Juli 1898 in Friedrichsruh bei Aumühle), war ein deutscher Politiker und Staatsmann. Von 1862 bis 1890 – mit einer kurzen Unterbrechung im Jahr 1873 – war er Ministerpräsident des Königreichs Preußen, von 1867 bis 1871 zugleich Bundeskanzler des Norddeutschen Bundes sowie von 1871 bis 1890 erster Reichskanzler des Deutschen Reiches, dessen Gründung er maßgeblich vorangetrieben hatte.

Als Politiker machte sich Bismarck im preußischen Königreich zunächst als Vertreter der Interessen der Junker im Kreis der Konservativen einen Namen und war während der Reaktionsära Diplomat (1851–1862). 1862 wurde er zum preußischen Ministerpräsidenten berufen. Im preußischen Verfassungskonflikt kämpfte er gegen die Liberalen für den Primat der Monarchie. Als Außenminister setzte er im Deutsch-Dänischen Krieg und im Deutschen Krieg zwischen 1864 und 1866 die politische Vorherrschaft Preußens in Deutschland durch. Im Deutsch-Französischen Krieg von 1870/71 war er treibende Kraft bei der Lösung der deutschen Frage im kleindeutschen Sinne und der Gründung des Deutschen Reiches.

Als Kanzler bestimmte er die Politik des neu geschaffenen Reiches – allgemein im Volksmund und in der Geschichtsschreibung wurde Bismarck daher auch der „Eiserne Kanzler“ genannt – und, bis auf eine kurze Unterbrechung, als preußischer Ministerpräsident bis zu seiner Entlassung 1890 entscheidend mit. Er setzte außenpolitisch auf einen Ausgleich der europäischen Mächte (→ Bündnispolitik Otto von Bismarcks).

Innenpolitisch ist seine Regierungszeit nach 1866 in zwei Phasen einteilbar. Zunächst kam es zu einem Bündnis mit den gemäßigten Liberalen. In dieser Zeit gab es zahlreiche innenpolitische Reformen wie die Einführung der Zivilehe, wobei Bismarck Widerstand von katholischer Seite mit drastischen Maßnahmen bekämpfte (→ Kulturkampf). Seit den späten 1870er-Jahren wandte Bismarck sich zunehmend von den Liberalen ab. In diese Phase fällt der Übergang zur Schutzzollpolitik und zu staatsinterventionistischen Maßnahmen. Dazu zählte insbesondere die Schaffung des Sozialversicherungssystems. Innenpolitisch geprägt waren die 1880er-Jahre nicht zuletzt vom repressiven Sozialistengesetz. 1890 führten Meinungsverschiedenheiten mit dem seit knapp zwei Jahren amtierenden Kaiser Wilhelm II. zu Bismarcks Entlassung.

In den folgenden Jahren spielte Bismarck als Kritiker seiner Nachfolger noch immer eine gewisse politische Rolle. Insbesondere durch seine viel gelesenen Memoiren "Gedanken und Erinnerungen" wirkte er selbst maßgeblich und nachhaltig an seinem Bild in der deutschen Öffentlichkeit mit.

In der deutschen Geschichtsschreibung dominierte bis Mitte des 20. Jahrhunderts eine ausgesprochen positive Bewertung von Bismarcks Rolle, die teilweise Züge einer nationalistischen Idealisierung trug. Nach dem Zweiten Weltkrieg mehrten sich jedoch kritische Stimmen, die Bismarck für das Scheitern der Demokratie in Deutschland verantwortlich machten und das von ihm geprägte Kaiserreich als obrigkeitsstaatliche Fehlkonstruktion darstellten. Jüngere Darstellungen überwinden diesen scharfen Gegensatz zumeist, wobei die Leistungen und Mängel von Bismarcks Politik gleichermaßen betont werden, und zeigen ihn als eingebettet in zeitgenössische Strukturen und politische Prozesse.

Otto von Bismarck wurde am 1. April 1815 auf Schloss Schönhausen nahe der Elbe bei Stendal in der Provinz Sachsen als zweiter Sohn des Rittmeisters "Karl Wilhelm Ferdinand von Bismarck" (1771–1845) und dessen Ehefrau "Luise Wilhelmine, geb. Mencken" (1790–1839) geboren. Er war väterlicherseits Spross des alten Adelsgeschlechts Bismarck, einem landsässigen Uradelsgeschlecht der Altmark, das seit Anfang des 18. Jahrhunderts zugleich auch im Kreis Naugard in Hinterpommern drei Güter besaß. Seine Mutter war dagegen als Tochter von Anastasius Ludwig Mencken bürgerlicher Herkunft. Die Familie Mencken hatte in der Vergangenheit Gelehrte und hohe Beamte hervorgebracht. Sein Bruder Bernhard von Bismarck wurde Landrat und Geheimer Regierungsrat.

1816 übersiedelte die junge Familie, ohne das Gut Schönhausen I aufzugeben, auf das hinterpommersche Gut Kniephof, wo Otto von Bismarck die ersten Jahre seiner Kindheit verbrachte.

Die unterschiedliche soziale Herkunft der Eltern hatte erhebliche Folgen für Bismarcks Sozialisation. Vom Vater erbte er den Stolz auf seine Herkunft, die Mutter gab ihm nicht nur seinen scharfen Verstand, den Sinn für rationales Handeln und sprachliche Sensibilität mit, sondern auch den Wunsch, seinem Herkunftskreis zu entkommen. Bismarck hatte es seiner Mutter zu verdanken, dass er eine Bildung genoss, die für einen Landedelmann nicht typisch war. Ihre Söhne sollten nicht nur Junker sein, sondern in den Staatsdienst eintreten. Allerdings führte die streng auf das Rationale abzielende Erziehung der Mutter dazu, dass sich Bismarck, wie er später schrieb, in seinem Elternhaus nie wirklich heimisch fühlte. Während er der Mutter reserviert gegenüberstand, hat er den Vater geliebt.

Im Alter von sechs Jahren begann Bismarcks schulische Ausbildung 1821 auf Wunsch der Mutter in der preußischen Hauptstadt Berlin in der Plamannschen Erziehungsanstalt. Dieses Internat, in das hohe Beamte ihre Söhne zu schicken pflegten, war ursprünglich im Geist von Johann Heinrich Pestalozzi gegründet worden. Zur Zeit Bismarcks war diese Reformphase längst beendet und die Erziehung geprägt von Drill und Deutschtümelei. Der Übergang vom kindlichen Spiel auf dem heimischen Hof zum Internatsleben, das von Zwang und Disziplin geprägt war, fiel Bismarck außerordentlich schwer. In dieser Zeit prägte sich deutlich sein Unwillen aus, Autoritäten anzuerkennen.

1827 wechselte Bismarck auf das Berliner Friedrich-Wilhelms-Gymnasium, ab 1830 besuchte er bis zum Abitur 1832 das humanistische Berlinische Gymnasium zum Grauen Kloster. Außer in Bezug aufs Altgriechische, das Bismarck bald als überflüssig ansah, zeigte er sich in der Schule als ausgesprochen sprachbegabt, wenn auch nicht immer als fleißig.

Bismarck war Angehöriger der lutherischen Konfession. Den Religionsunterricht erhielt er von Friedrich Schleiermacher, der den Sechzehnjährigen in der Berliner Dreifaltigkeitskirche auch konfirmierte. Bismarck befasste sich in dieser Zeit mit Fragen der Religion hauptsächlich vom Verstand her und sah sich in ihr, von Hegel oder Spinoza beeinflusst, rückblickend eher als Deist und Pantheist denn als gläubiger Christ. Ein Atheist war er allerdings nie, auch wenn seine Umgebung ihn zumeist für einen gottlosen Spötter hielt. In der Zeit seines Referendariats schrieb er 1836 an seinen Bruder Bernhard: „Ich bemerke nur, dass Du mir zu wenig Besonnenheit zumutest, wenn Du mich für einen Atheisten hältst.“ Das Christentum griff entscheidend in sein Leben ein, als ihn unerwartet der Tod seiner Freundin Marie von Thadden-Trieglaff traf.

Nach dem Abitur nahm Bismarck als Siebzehnjähriger am 10. Mai 1832 das Studium der Rechtswissenschaften auf (1832–1835), zunächst an der Universität Göttingen (1832–1833). Die politischen Nachwehen im Gefolge der Julirevolution lehnte er nachdrücklich ab. Es war daher auch kein Zufall, dass er sich nicht den damals oppositionellen Burschenschaften, sondern der schlagenden landsmannschaftlichen Studentenverbindung Corps Hannovera Göttingen anschloss. Er blieb zeitlebens ein überzeugter Corpsstudent. An den Burschenschaften missfielen ihm „ihre Weigerung, Satisfaktion zu geben, ihr Mangel an äußerlicher Erziehung und an Formen der guten Gesellschaft, bei näherer Bekanntschaft auch die Extravaganzen ihrer politischen Auffassungen, die auf einem Mangel an Bildung und an Kenntnis der vorhandenen, historisch gewordenen Lebensverhältnisse beruhte.“ Er fasste seine Beobachtungen später zu der Bemerkung zusammen, dass es sich um eine Verbindung von Utopie und Mangel an Erziehung gehandelt habe. Andererseits bezeichnete er sich selbst als keineswegs von preußisch-monarchischen Gedanken beeinflusst. Geschichte und Literatur interessierten ihn, das Jurastudium weniger. Der einzige akademische Lehrer, der ihn beeindruckte und wohl auch beeinflusste, war der Historiker Arnold Heeren, der in seinen Vorlesungen die Funktionsweise des internationalen Staatensystems skizzierte. Engere persönliche Beziehungen baute er zu seinem Corpsbruder Gustav Scharlach und dem späteren amerikanischen Diplomaten John Lothrop Motley auf, der zeit seines Lebens einer seiner wenigen persönlichen Freunde blieb.

Im November 1833 setzte Bismarck sein Studium an der Berliner Friedrich-Wilhelms-Universität fort. 1835 schloss er es mit dem Ersten Staatsexamen ab. Anschließend war er zunächst Auskultator beim Berliner Stadtgericht. Auf eigenen Wunsch wechselte er vom Justiz- in den Verwaltungsdienst. Nicht nur im Kreis um den Novellisten Carl Borromäus Cünzer suchte er Zerstreuung: Vom Büroalltag eines Regierungsreferendars im mondänen Kurort Aachen bald gelangweilt, verliebte er sich im August 1836 in Laura Russell, eine Nichte des Herzogs von Cumberland. Nach der Affaire mit einer (älteren) Französin reiste er im Sommer 1837 mit einer (jüngeren) Engländerin, einer Freundin Lauras, durch Deutschland. Dadurch kam es zu einer mehrwöchigen Überschreitung eines vierzehntägigen Urlaubs, durch die er sein Referendariat verlor.

Bismarck haderte mit Auslagen für Frauen und machte zusätzlich durch den Besuch von Spielkasinos Schulden. Seinen Dienstgeschäften blieb er monatelang fern. Er versuchte später, seine Referendarausbildung in Potsdam fortzusetzen, kehrte dem Verwaltungsdienst aber nach einigen Monaten den Rücken. Er erklärte diesen Schritt rückblickend damit, dass er kein bloßes Rädchen im Getriebe der Bürokratie sein wollte: „Ich will aber Musik machen, wie ich sie für gut erkenne, oder gar keine.“

1838 leistete Bismarck als Einjährig-Freiwilliger seinen Militärdienst ab, zunächst beim Garde-Jäger-Bataillon. Im Herbst wechselte er zum "Jäger-Bataillon Nr. 2" nach Greifswald in Vorpommern, wo er sich an der Königlichen Staats- und landwirtschaftlichen Akademie Eldena auch auf die Führung der Familienbetriebe vorbereitete.

Bismarck bezog nach dem Tod seiner Mutter im Jahr 1839 das hinterpommersche Gut Kniephof und wurde Landwirt. Gemeinsam mit dem um fünf Jahre älteren Bruder Bernhard bewirtschaftete er die väterlichen Güter Kniephof, Külz und Jarchlin im Kreis Naugard. Nachdem Bernhard von Bismarck 1841 zum Landrat gewählt worden war, kam es zu einer vorläufigen Teilung. Bernhard bewirtschaftete nun Jarchlin, Otto Külz und Kniephof. Nach dem Tod des Vaters im Jahr 1845 übernahm Otto die Bewirtschaftung des Familienbesitzes Schönhausen bei Stendal.

Bismarck erwarb schnell gute Kenntnisse in rationaler landwirtschaftlicher Betriebsführung. In den etwa zehn Jahren, in denen er als Verwalter des elterlichen Besitzes fungierte, gelang es ihm nicht nur, die Güter zu sanieren, sondern auch die eigenen Schulden zurückzuzahlen, die er in den zurückliegenden Jahren aufgehäuft hatte.

Einerseits gefiel es ihm, sein eigener Herr zu sein, andererseits füllten ihn die landwirtschaftliche Tätigkeit und das Leben als Landjunker nicht wirklich aus. Er beschäftigte sich nebenher intensiv, aber unsystematisch mit Philosophie, Kunst, Religion und Literatur, ohne dass ihn dies nachhaltig geprägt hätte. 1842 unternahm er eine Studienreise nach Frankreich und England und in die Schweiz. Das Bestreben, in den Staatsdienst zurückzukehren, gab er 1844 auf – erneut aufgrund seiner Abneigung gegen alles Bürokratische. In diesen Jahren war er gerngesehener Gast bei zahlreichen gesellschaftlichen Ereignissen in der Region. Er nahm unter anderem an zahlreichen Jagdveranstaltungen teil, aber auch an ausschweifenden Zechgelagen. Eigenen Bekundungen zufolge hatte er sich in diesem Zusammenhang eine Art Trinkfestigkeit angeeignet; bei den Landjunkern habe er an Ansehen hinzugewonnen, weil er dazu fähig sei, seine „Gäste mit freundlicher Kaltblütigkeit unter den Tisch zu trinken“. Dies wie auch die ihm anhaftende Neigung, bei gesellschaftlichen Ereignissen fast stets im Mittelpunkt zu stehen, brachte ihm den Ruf des „tollen Bismarck“ ein.

Durch Moritz von Blanckenburg, einen Schulfreund aus Berlin, kam Bismarck in Kontakt mit dem pietistischen Kreis um Adolf von Thadden-Trieglaff. Blanckenburg war mit dessen Tochter Marie von Thadden-Trieglaff verlobt. Marie von Thadden und Bismarck fühlten sich als verwandte Seelen, aber für die junge Frau kam eine Auflösung ihrer Verlobung nicht in Frage. Im Oktober 1844 heiratete sie Blanckenburg. Bei der Hochzeitsfeier wählte sie ihre zwanzigjährige Freundin Johanna von Puttkamer als Tischdame für Bismarck aus. Im Sommer 1846 reisten das Ehepaar Blanckenburg, Bismarck und Johanna von Puttkamer gemeinsam in den Harz. Nach dem unerwarteten Tod Maries am 10. November 1846 hielt Bismarck in einem berühmt gewordenen Brautbrief an Heinrich von Puttkamer um die Hand von dessen Tochter an. Der Gutsbesitzer antwortete hinhaltend. Bismarck reiste daraufhin nach Reinfeld bei Rummelsburg in Hinterpommern und überzeugte die Eltern Johannas in einem persönlichen Gespräch. Die Heirat fand im Jahr 1847 in Reinfeld (Landkreis Rummelsburg i. Pom.) statt. Seit dieser Zeit spielte der Glaube an einen persönlichen Gott für Bismarck eine zentrale Rolle.

Aus der Ehe mit Johanna von Bismarck gingen drei Kinder hervor:

Johanna ordnete ihre Bedürfnisse denen ihres Mannes unter und bot ihm zugleich – anders als seine Mutter – eine feste emotionale Bindung. Die Briefe, die die beiden austauschten, gehören zu den Höhepunkten der Briefliteratur des 19. Jahrhunderts.

Bismarck trat politisch zunächst auf kommunaler Ebene hervor. In seiner Zeit auf Gut Kniephof war er Deputierter des Kreises Naugard, wurde 1845 Mitglied des Provinziallandtags der Provinz Pommern und unterstützte in einigen Fällen seinen Bruder bei dessen Tätigkeit als Landrat. Über seinen pietistischen Freundeskreis kam er um 1843/1844 in Kontakt zu führenden konservativen Politikern, insbesondere zu den Brüdern Ernst Ludwig und Leopold Gerlach. Er verpachtete 1845 nicht zuletzt, um diese Verbindung auszubauen, den Kniephof und zog nach Schönhausen. Dieser Ort lag näher bei Magdeburg, dem damaligen Dienstsitz von Ludwig von Gerlach. Bismarck erhielt sein erstes öffentliches Amt 1846 durch die Ernennung zum Deichhauptmann in Jerichow.

Sein Hauptanliegen in dieser Zeit war es, die Vormachtstellung des landbesitzenden Adels in Preußen zu bewahren. Die Konservativen lehnten den absolutistisch-bürokratischen Staat ab und träumten von einer Wiedereinführung der Mitregierung der Stände, insbesondere des Adels. Zusammen mit den Brüdern Gerlach trat Bismarck beispielsweise für die Bewahrung der Patrimonialgerichtsbarkeit ein.

Als Nachrücker im sächsischen Provinziallandtag wurde Bismarck als Vertreter der Ritterschaft der Provinz Sachsen 1847 Mitglied des Vereinigten Landtags. In diesem Gremium, das von der gemäßigten liberalen Opposition dominiert war, fiel er bereits bei seiner ersten Plenarrede als strikt konservativer Politiker auf, als er bestritt, dass es bei den Befreiungskriegen auch um die Durchsetzung liberaler Reformen gegangen war. In der „Judenfrage“ sprach er sich klar gegen die politische Gleichstellung der jüdischen Bevölkerung aus. Diese und ähnliche Positionen führten bei den Liberalen zu empörten Reaktionen. Bismarck fand in dieser Zeit in der Politik ein Betätigungsfeld, das seinen Neigungen entgegenkam: „Die Sache ergreift mich viel mehr als ich dachte.“
Die Leidenschaft des politischen Kampfes ließ ihn kaum essen und schlafen. Am Ende der Versammlung hatte sich Bismarck in den konservativen Kreisen einen Namen gemacht. Auch der König war auf ihn aufmerksam geworden. Wenngleich er eindeutig konservative Positionen vertrat, war Bismarck bereits in dieser Zeit auch Pragmatiker und bereit, vom politischen Gegner zu lernen. Dies kam etwa in dem Plan zum Tragen, als Gegengewicht zur liberalen Deutschen Zeitung eine konservative Zeitung zu gründen.

Bismarck lehnte die Märzrevolution entschieden ab. Als ihn die Nachricht vom Erfolg der Bewegung in Berlin erreichte, bewaffnete er in Schönhausen die Bauern und schlug vor, mit ihnen nach Berlin zu ziehen. Der in Potsdam kommandierende General Karl von Prittwitz lehnte dieses Angebot jedoch ab. Danach versuchte Bismarck, Prinzessin Augusta, die Gattin des späteren Königs Wilhelm I., von der Notwendigkeit einer Gegenrevolution zu überzeugen. Augusta wies das Ansinnen als intrigant und illoyal zurück. Bismarck zog sich durch sein Verhalten die dauerhafte Abneigung der späteren Königin zu. Nach der Anerkennung der Revolution durch Friedrich Wilhelm IV. waren Bismarcks gegenrevolutionäre Pläne vorerst gescheitert.

In die preußische Nationalversammlung wurde Bismarck nicht gewählt. Dafür beteiligte er sich an der außerparlamentarischen Sammlung des konservativen Lagers. Im Sommer 1848 war er an der Gründung und inhaltlichen Ausgestaltung der Neuen Preußischen Zeitung (wegen des Kreuzes auf dem Titelblatt auch „Kreuzzeitung“ genannt) beteiligt. Für das Blatt schrieb er zahlreiche Beiträge. Im August 1848 war er einer der maßgeblichen Initiatoren des sogenannten Junkerparlaments. In diesem versammelten sich mehrere hundert adlige Gutsbesitzer, um gegen den Eingriff in ihr Eigentum zu protestieren.

Diese Aktivitäten führten dazu, dass die konservative Kamarilla um den König Bismarck immer mehr zu schätzen begann. Seine Hoffnung, nach der Gegenrevolution im November 1848 mit einem Ministerposten belohnt zu werden, erfüllte sich jedoch nicht, da er selbst in konservativen Kreisen als zu extrem galt. Der König schrieb auf eine entsprechende Vorschlagsliste als Randbemerkung: „Nur zu gebrauchen, wenn das Bayonett schrankenlos waltet“.

Im Januar und im Juli 1849 wurde Bismarck in die zweite Kammer des preußischen Landtages gewählt. Er beschloss in dieser Zeit, sich ganz der Politik zu widmen, und zog mit seiner Familie nach Berlin. Damit war er einer der ersten Berufspolitiker in Preußen. Im Landtag trat er als Sprachrohr der Ultrakonservativen auf. So verteidigte er die Ablehnung von Kaiserwürde und Reichsverfassung durch Friedrich Wilhelm IV., weil aus seiner Sicht zu befürchten stand, dass Preußen in Deutschland aufginge. Die nationale Frage war für ihn gegenüber der Sicherung der preußischen Macht zweitrangig.

Der König und sein Berater Joseph von Radowitz wollten die deutsche Einheit vor allem durch Absprache mit den Mittelstaaten erreichen. Außerdem sollte die angestrebte Erfurter Union konservativer und föderalistischer sein als das Frankfurter Vorbild. Bismarck hielt dies für unrealistisch und nicht sinnvoll. Im preußischen Parlament machte er aus seiner Kritik an den Plänen keinen Hehl. Seine Rede vom 6. September 1849 veränderte die Haltung interessierter politischer Kreise zu ihm. Er galt fortan wegen seiner abwägenden und flexiblen Argumentation auch in den eigenen konservativen Reihen nicht mehr nur als Scharfmacher. Bismarck empfahl sich damit erstmals für einen Posten im hohen Staatsdienst oder in der Diplomatie. Er wurde trotz seiner Kritik an der Union in das Volkshaus des Erfurter Unionsparlaments gewählt und wurde in ihm Schriftführer.

Obwohl er dem Parlamentarismus grundsätzlich ablehnend gegenüberstand, entwickelte Bismarck sich in Erfurt zu einem der bedeutendsten Parlamentsredner der Zeit, dem auch der politische Gegner wegen seiner bilder- und pointenreichen Sprache Aufmerksamkeit schenkte. Nach dem Scheitern der Unionspläne übernahm Bismarck die schwierige Aufgabe, im preußischen Landtag die Olmützer Punktation zu verteidigen. Er schaffte es dabei, einerseits konservative Standpunkte zu vertreten, sich andererseits aber zu einer staatlichen Machtpolitik fern irgendwelcher Ideologien zu bekennen: „Die einzige gesunde Grundlage eines großen Staates, und dadurch unterscheidet er sich wesentlich von einem kleinen Staate, ist der staatliche Egoismus und nicht die Romantik, und es ist eines großen Staates nicht würdig, für eine Sache zu streiten, die nicht seinen eigenen Interessen angehört.“ Mit seiner Betonung des Staates, der Macht- und Interessenpolitik, entfernte Bismarck sich vom traditionellen Konservatismus, der (in eher defensiver Grundeinstellung) aus der Gegnerschaft zum modernen, zentralen, bürokratischen und absolutistischen Staat entstanden war.

Bismarck wurde am 18. August 1851 auf Betreiben Leopold von Gerlachs durch Friedrich Wilhelm IV. zum preußischen Gesandten beim Bundestag in Frankfurt ernannt. Eine diplomatische Ausbildung hatte er nicht. Die Stellung in Frankfurt war nach seiner Einschätzung zu dieser Zeit der wichtigste Posten in der preußischen Diplomatie. Seine Ernennung wurde in der Öffentlichkeit als Zeichen für den Sieg der sozialen und politischen Reaktion sowie als Kapitulation Preußens gegenüber Österreich gewertet.

In Frankfurt handelte Bismarck sehr eigenständig. Er befand sich zeitweise sogar im Gegensatz zur Berliner Regierungspolitik. Allerdings machte er als Gesandter deutlich, dass er noch immer ein Mann der Hochkonservativen war. Seine Haltung in einer Kammerdebatte führte am 25. März 1852 zum Duell Vincke–Bismarck, bei dem keiner der beiden Duellanten getroffen wurde.

Als das Königreich Preußen und das Kaisertum Österreich nach der Herbstkrise 1850 zusammenarbeiteten, wollte Bismarck sich nicht damit abfinden, dass der österreichische Ministerpräsident Felix zu Schwarzenberg Preußen die Rolle als Juniorpartner zudachte. Ihm und letztlich auch der Regierung in Berlin ging es darum, die Anerkennung Preußens als gleichberechtigte Macht durchzusetzen. Zu diesem Zweck suchte er ständig die Auseinandersetzung mit dem österreichischen Gesandten Friedrich von Thun und Hohenstein, griff Wien scharf an und legte zeitweise sogar die Arbeit des Bundestages lahm, um die Grenzen der österreichischen Kompetenzen in Frankfurt aufzuzeigen. Er trug auch dazu bei, dass Österreichs Wunsch scheiterte, dem Deutschen Zollverein beizutreten. Bismarck lehnte einen Ausbau der Institutionen und überhaupt eine Bundesreform ab, solange Österreich Preußen nicht als gleichberechtigt behandelte.

Die Entscheidung der preußischen Regierung im Jahr 1854 (vor dem Hintergrund des Krimkrieges), das Schutz- und Trutzbündnis mit Österreich zu erneuern, stieß bei Bismarck auf Kritik. Als Österreich sich danach offen gegen Russland wandte, gelang es Bismarck 1855, durch geschicktes Taktieren den Antrag der Österreicher zur Mobilisierung der Bundestruppen gegen Russland abzuwenden. Dieser Erfolg ließ sein diplomatisches Ansehen zunehmen. Nach der Niederlage Russlands im Krimkrieg plädierte er in verschiedenen Denkschriften für eine Anlehnung an das Zarenreich und an Frankreich, durch die er Österreich weiter zu schwächen hoffte. Besonders ausführlich legte er sein außenpolitisches Konzept in der „Prachtschrift“ von 1856 nieder. Seine Äußerungen lösten einen heftigen Konflikt mit den Hochkonservativen um die Gebrüder Gerlach aus, die in Napoleon III. nur einen Vertreter des revolutionären Prinzips und einen „natürlichen Feind“ sahen. Bismarck antwortete, dass ihm die Legitimität der Staatsoberhäupter letztlich egal sei. Für ihn standen nicht die konservativen Grundsätze, sondern die Staatsinteressen im diplomatischen Geschäft im Mittelpunkt. Im Lager der Konservativen galt er nun zunehmend als egoistischer Opportunist.

Der Konflikt mit den Gerlachs hatte aber auch innenpolitische Gründe. Nach der Übernahme der Regentschaft durch Prinz Wilhelm 1857 verloren die Hochkonservativen an Einfluss; stattdessen nahm die Bedeutung der gemäßigt liberal-konservativen Wochenblattpartei zu. In der beginnenden Neuen Ära versuchte auch Bismarck, durch eine gewisse Distanzierung von den extremen Konservativen seine Position zu behaupten. In einer umfangreichen Denkschrift sprach er nunmehr von einer „nationalen Mission“ Preußens und von einem Bündnis mit der national-liberalen Bewegung. Damit vollzog er einen bemerkenswerten Kurswechsel. Allerdings ging es ihm nicht um den Kampf für die deutsche Einheit um ihrer selbst willen, sondern war es sein Ziel, den deutschen Nationalismus einer Stärkung der preußischen Macht dienstbar zu machen.

Die Erwartungen, die er mit der Anpassung an ein verändertes politisches Klima in Preußen verband, erfüllten sich für ihn selbst allerdings zunächst noch nicht. Im Januar 1859 wurde er als preußischer Gesandter nach Sankt Petersburg versetzt; er selbst sprach davon, dass er an der Newa kaltgestellt worden sei. Der Wechsel fiel der Familie schwer; die Eheleute Bismarck hatten in Frankfurt die glücklichste Zeit ihrer Ehe erlebt. Bismarck erweiterte in der neuen Funktion allerdings seine diplomatischen Kenntnisse und erfreute sich des Wohlwollens des russischen Hofes und des Kaiserpaares. Sein Ehrgeiz richtete sich aber zunehmend auf die höchsten Ämter im preußischen Staat. Er beobachtete genau die Entwicklung des preußischen Verfassungskonflikts. Die Hoffnung, bereits im April 1862 zum Ministerpräsidenten ernannt zu werden, erfüllte sich nicht. Stattdessen wurde er Gesandter in Paris, wo er im Palais Beauharnais residierte. Dieser Posten galt ihm jedoch von Beginn an nur als Wartestellung.

In diese Zeit fiel die von seiner Ehefrau geduldete Liebesaffäre mit Fürstin Katharina Orlowa (1840–1875), der Ehefrau des russischen Gesandten in Belgien Nikolai Alexejewitsch Orlow. Am 22. August 1862, kurz vor seiner Berufung zum Ministerpräsidenten, wäre Bismarck in Biarritz mit Katharina Orlowa fast ertrunken und wurde von einem Leuchtturmwärter gerettet. Seiner Frau schreibt er an diesem Tag nur: Es war die letzte private Eskapade Bismarcks, ehe er sich ausschließlich der Politik widmete.

In Berlin verfestigte sich inzwischen die ablehnende Haltung der Liberalen gegen eine geplante Heeresreform. Die Notwendigkeit einer solchen Reform wurde eigentlich von niemandem ernsthaft in Frage gestellt. Im Gegensatz zu den anderen Großmächten war die preußische Armee seit 1815 kaum gewachsen. Selbst im Vergleich mit Österreich waren die preußischen Streitkräfte deutlich schwächer. Die offiziell bestehende Wehrpflicht existierte in der Wirklichkeit nur noch auf dem Papier, und seit längerem gab es Bemühungen, die Landwehr an die reguläre Armee heranzuführen. In der Sache wäre eine Einigung mit den Liberalen bei der Heeresvorlage möglich gewesen. Wilhelm I. jedoch glaubte, dass ein Nachgeben die Krone schwächen würde.

Dies bestärkte die Liberalen in ihrer Kritik, und das Abgeordnetenhaus verweigerte die für die Reform nötigen Finanzmittel. Im März 1862 wurde das Parlament aufgelöst und eine neue Regierung gebildet. Statt der gemäßigten Liberalen der Neuen Ära hatten in dieser Regierung Konservative wie der Kriegsminister Albrecht von Roon das Sagen. Aus den Neuwahlen ging allerdings die neu gegründete Fortschrittspartei als Sieger hervor, während die Zahl der konservativen Abgeordneten stark abnahm. Wilhelm I. erwog in dieser aussichtslos erscheinenden Lage ernsthaft den Rücktritt zu Gunsten seines Sohnes, des späteren Kaisers Friedrich III. Nach einer Auseinandersetzung mit den Ministern der Regierung hatte der König bereits den Entwurf einer Abdankungsurkunde formuliert.

General Roon sah in der Ernennung Bismarcks zum Ministerpräsidenten die einzige Möglichkeit, den Thronwechsel zugunsten des als liberal geltenden Kronprinzen zu verhindern. Mit einem Telegramm – „"Periculum in mora. Dépêchez-vous!"“ („Gefahr im Verzuge. Beeilen Sie sich!“) – rief er Bismarck nach Berlin zurück. Nach 25 Stunden Bahnfahrt traf Bismarck am 20. September 1862 wieder in Berlin ein. Zwei Tage später wurde er von König Wilhelm I. im Schloss Babelsberg empfangen. Über Inhalt und Verlauf der Unterredung liegt nur Bismarcks Bericht vor, der aber im Gegensatz zu anderen Teilen seiner Erinnerungen im Kern korrekt sein dürfte. Bismarck gewann den noch zögernden König, indem er sich als seinen unbedingten Gefolgsmann gab. Er versprach die Durchsetzung der Heeresreform und betonte seinerseits die grundlegende Bedeutung der Auseinandersetzung um sie. Es gelte, um die Entscheidung zwischen „königlichem Regiment oder Parlamentsherrschaft“ zu kämpfen. Um die letztere abzuwenden, befürworte er auch „eine Periode der Diktatur.“ Der König habe Bismarck daraufhin gefragt, ob er bereit sei, sich für die Heeresreform ohne Abstriche einzusetzen und an der Reform festzuhalten, notfalls auch gegen die Mehrheitsbeschlüsse des Abgeordnetenhauses. Als Bismarck beides bejahte, habe der König sich von seiner Entschlossenheit beeindruckt gezeigt: „Dann ist es meine Pflicht, mit Ihnen die Weiterführung des Kampfes zu versuchen und ich abdiziere nicht“ (d. h., "ich danke nicht ab"). Der König ernannte Bismarck zum Ministerpräsidenten und Außenminister.

Das Ernennungsgespräch legte die Grundlage für die außergewöhnliche Beziehung zwischen dem König und Bismarck in den folgenden Jahrzehnten. Bismarck schuf sich die Grundlage für eine außergewöhnliche Vertrauensstellung bei Wilhelm I. und verschaffte sich eine Blankovollmacht, die seinen Handlungsspielraum über das übliche Maß eines leitenden Ministers hinaus erweiterte (Lothar Gall), indem er sich dem Monarchen als „kurbrandenburgischer Vasall“ andiente, der in prekärer Lage kampfesmutig und in unverbrüchlicher Treue zu seinem Lehnsherrn stehen werde. Zwar kam es in den nächsten Jahren immer wieder zu Meinungsverschiedenheiten, doch haben sie das Grundvertrauen des Königs Bismarck gegenüber nicht beeinträchtigt.

Im Einzelnen erhielt Bismarck sehr starke Vollmachten, auf die er sich später berief. Darunter war die, dass seine Minister "nur mit seinem Einverständnis" dem Monarchen einzeln berichten dürfen.

Bismarck blieb zwar ein Konservativer, allerdings ein zunehmend pragmatisch handelnder und nicht an ideologischen Fixierungen klebender Politiker. Ideale, Theorien und Prinzipien waren für ihn nicht vorrangig ausschlaggebend; was vor allem zählte, waren die Interessen der Staaten. Daraus ergab sich die Machterweiterung Preußens als maßgebliches Ziel. Aus Bismarcks Sicht war es nur möglich, den Großmachtanspruch Preußens zu bewahren, wenn dieses eine hegemoniale Stellung in Europa zu Lasten Österreichs gewinnen konnte und die übrigen europäischen Mächte das duldeten. Um Nationalismus im landläufigen Sinn ging es ihm dabei nicht, vielmehr um außenpolitischen Realismus. Er setzte darauf, dass außenpolitische Erfolge sich auch auf seine Innenpolitik günstig auswirken würden. Er wollte die Monarchie und den Obrigkeitsstaat ebenso erhalten wie die besondere Stellung von Militär und Adel. Erste Priorität hatte aber im Zweifelsfall die Macht des Staates. Darauf zielte auch das zeitweilige Bündnis mit der nationalen und liberalen Bewegung.

Am Anfang dominierte in weiten Teilen der politischen Öffentlichkeit bis hinein ins konservative Lager die Ablehnung Bismarcks, der noch immer als extremer Reaktionär galt. Er hatte es daher schwer, geeignete Minister zu finden, und schrieb: „Wir sind froh, wenn wir acht Männer finden und halten.“ Das erste Kabinett Bismarck bestand so denn auch mehrheitlich aus eher zweitrangigen Persönlichkeiten. Unter ihnen waren Carl von Bodelschwingh, Heinrich Friedrich von Itzenplitz und Gustav von Jagow. In seinen Memoiren urteilte Bismarck, dass einige Minister „nicht im Stande [waren,] ihre Ministerien zu leiten“. Sie haben mit Ausnahme Roons kein Verständnis für die politische Gesamtlinie gezeigt, einige sich außerdem als „arbeitsscheu und vergnügungssüchtig“ erwiesen.

Vor diesem Hintergrund war Bismarck die alles entscheidende Persönlichkeit. Als Chef eines Konfliktministeriums berufen, dominierte er klar die Auseinandersetzung mit den Liberalen.

Bismarck versuchte anfangs, die Opposition nicht nur durch Drohungen, sondern auch durch Ausgleichsbemühungen zu neutralisieren. Dies scheiterte, weil er mit einigen seiner Äußerungen erneut das Renommee eines stockkonservativen Politikers bediente. Oft zitiert wurde die Aussage: „Nicht auf Preußens Liberalismus sieht Deutschland, sondern auf seine Macht. […] Nicht durch Reden und Majoritätsbeschlüsse werden die großen Fragen der Zeit entschieden […] – sondern durch Eisen und Blut.“

Eigentlich war die „Blut-und-Eisen“-Rede als weitgehendes Bündnisangebot an die liberale und nationale Bewegung gedacht gewesen. Obwohl auch die liberale Mehrheit des Abgeordnetenhauses der Auffassung war, dass die „Deutsche Frage“ nicht ohne Gewalt durchzusetzen sei, fasste man, insbesondere die (liberale) Presse, „Eisen und Blut“ als eine angekündigte Gewaltherrschaft auf, die sich auf außenpolitische Abenteuer stürze. Dies hat dazu beigetragen, Bismarcks Ruf als Gewaltpolitiker zu festigen. Bismarck gab in der Folge seinen Schlingerkurs auf und bekämpfte die Liberalen mit aller Schärfe. Das Parlament wurde vertagt. Damit regierte Bismarck im Herbst 1862 ohne ordnungsgemäßen Haushalt. Anfang 1863 wurde das Parlament wieder einberufen. Bismarck rechtfertigte sich mit der berühmt gewordenen, heftig umstrittenen Lückentheorie. Danach basiere das normale staatliche Handeln auf Kompromissen zwischen der Krone, dem Herrenhaus und dem Abgeordnetenhaus. Weigere sich eine der Seiten nachzugeben, komme es zu Konflikten, „und Konflikte, da das Staatsleben nicht stillzustehen vermag, werden zu Machtfragen; wer die Macht in den Händen hat, geht dann in seinem Sinne vor, weil das Staatsleben auch nicht einen Augenblick stillstehen kann.“

Dahinter stand Bismarcks Voraussetzung, der Fall eines unauflöslichen Dissenses zwischen Monarch und Parlament sei in der Verfassung nicht geregelt. Demnach liege eine Lücke vor, die durch die Prärogative des Königs geschlossen werden müsse. Diese Auslegung der Rechtslage war nach Auffassung vieler Zeitgenossen schlicht ein Verfassungsbruch. Maximilian von Schwerin-Putzar urteilte, dies bedeute, „Macht geht vor Recht.“ Bislang habe die Größe Preußens und die Anerkennung des Königshauses auf dem Grundsatz beruht „Recht geht vor Macht. Justitia fundamentum regnorum! Das ist der Wahlspruch der preußischen Könige, und er wird es fort und fort bleiben.“

Um gegen die Liberalen zu mobilisieren, verfolgte Bismarck zeitweilig unterschiedliche Pläne. Dazu gehörte auch ein Bündnis mit der sozialdemokratischen Bewegung. 1863 traf er sich mehrfach mit Ferdinand Lassalle, ohne dass dies damals jedoch praktische Auswirkungen gehabt hätte.

Trotz heftiger Proteste – öffentliche Kritik kam sogar vom Thronfolger – und der allgemeinen Erwartung eines Scheiterns der Regierung überlebte Bismarck die Krise politisch. Gegen hohe liberale Beamte, unter ihnen nicht zuletzt Abgeordnete, ging er mit repressiven Mitteln bis hin zu Entlassungen vor. Gleichzeitig wurde die Pressefreiheit in Missachtung der Verfassung praktisch abgeschafft. 1865 forderte Bismarck Professor Rudolf Virchow (ein Mitglied des Preußischen Abgeordnetenhauses) zum Duell, das dieser jedoch ablehnte, weil es keine zeitgemäße Form der Auseinandersetzung sei.

An der verfahrenen politischen Situation änderte sich freilich nichts. Die Verfassungskrise blieb bis 1866 ungelöst und artete in so etwas wie einen Stellungskrieg aus. Bismarck versuchte, die Opposition zu zermürben. Er regierte mit dem Staatsapparat, und lange Zeit wurde das Parlament gar nicht einberufen. Am 9. Mai 1866 wurde es erneut aufgelöst. Bismarck spielte anfangs selbst mit dem Gedanken eines Staatsstreichs durch Abschaffung von Wahlrecht und Verfassung. Je länger der Konflikt andauerte, desto mehr lehnte er solche Forderungen, die von konservativer Seite erhoben wurden, aber ab, da sie keine langfristig stabile politische Ordnung hervorzubringen versprachen.

Bismarck versuchte unterdessen, mit außenpolitischen Erfolgen innenpolitischen Druck auf die Opposition auszuüben. Zunächst ging dieses Kalkül nur sehr bedingt auf. Das erste Abkommen, die Alvenslebensche Konvention vom 8. Februar 1863 zur Unterstützung Russlands gegen den Aufstand in Polen, stieß in Preußen selbst in konservativen Kreisen auf breite Ablehnung. Der Druck von Seiten Großbritanniens und Napoleons III. machte die Konvention überdies wertlos.

Österreich sah Bismarck geschwächt und versuchte das zu nutzen, um eine Reform des Deutschen Bundes zu Gunsten der Habsburgermonarchie durchzusetzen. Nur mit Mühe gelang es Bismarck, dem König die Teilnahme an dem geplanten Fürstentag in Frankfurt auszureden. Der Ministerpräsident legte im Gegenzug die preußischen Vorstellungen einer Bundesreform vor. Sie zielten wie schon früher auf gleiche Rechte für Österreich und Preußen. Neu war aber die Forderung nach einer „aus direkter Beteiligung der ganzen Nation hervorgehenden Nationalvertretung.“ Dies war nicht mehr und nicht weniger als ein Bündnisangebot Preußens an die Nationalbewegung, die eng mit dem Liberalismus verbunden war. Kurzfristig nützte das Bismarck nichts, da er angesichts des Verfassungskonflikts als Partner für die Liberalen nicht in Frage kam. Die Opposition in Preußen konnte bei den Neuwahlen Ende Oktober 1863 ihre Position behaupten.

Die Frage der Bundesreform wurde bald von einer Krise internationaler Größenordnung überdeckt. Nach dem Tod Friedrichs VII. von Dänemark entbrannte ein Streit um die Zukunft der beiden Herzogtümer Schleswig und Holstein. Schleswig war ein Lehen Dänemarks, während Holstein Mitglied des Deutschen Bundes war. Beide Territorien unterstanden jedoch dem dänischen König in Personalunion (Dänischer Gesamtstaat). Friedrich von Augustenburg beanspruchte die Länder für sich. Die deutsche nationale Bewegung unterstützte ihn und forderte die Vereinigung der beiden Herzogtümer und ihre Eingliederung in den Deutschen Bund als eigenständiger Staat. Der neue dänische König Christian IX., der unter dem Druck der Nationalbewegung im eigenen Land stand, unterschrieb stattdessen zögernd die Novemberverfassung, die Schleswig verfassungsrechtlich näher als Holstein an Dänemark band und somit die Bestimmungen des Londoner Protokolls über den Bestand des Gesamtstaates verletzte.

Zur Enttäuschung der nationalen und liberalen Bewegung lehnte Bismarck es ab, den Anspruch Friedrichs von Augustenburg zu unterstützen. Er wandte sich gleichzeitig aber auch gegen die dänische Position und strebte mittelfristig die Einbindung der beiden Herzogtümer in den preußischen Machtbereich an. Dies war zum Zeitpunkt der Krise außenpolitisch allerdings nicht durchsetzbar. Deshalb hegte Bismarck zunächst wie Österreich ein Interesse an einem neuen Augustenburger Staat. Die Österreicher sahen in einer „nationalen Lösung“ der schleswig-holsteinischen Frage eine Gefahr für den eigenen Vielvölkerstaat. Vor diesem Hintergrund konnte es noch einmal zu einer Zusammenarbeit der beiden deutschen Großmächte kommen.

Bismarcks Politik in der schleswig-holsteinischen Krise folgte wie auch bei anderen Gelegenheiten keinem festen Plan. Er ging vielmehr davon aus, dass die Umstände denjenigen am meisten begünstigen würden, der sich von ihnen leiten ließ, ihnen Lösungen abgewann und sie ihnen nicht aufzuzwingen versuchte.

Bismarck trat zunächst als Verteidiger des bestehenden Völkerrechts auf und forderte von Dänemark, wieder auf den Boden der Londoner Verträge von 1852 zurückzukehren. Dadurch beruhigte er die europäischen Großmächte. Österreich stellte sich an die Seite Preußens. Die übrigen deutschen Staaten im Deutschen Bund und der Bundestag wurden dadurch weitgehend an den Rand gedrängt. Tatsächlich erklärten Bismarck und der österreichische Gesandte Alajos Károlyi in Berlin, dass beide Großmächte das Recht beanspruchen, sich über die Beschlüsse des Bundestages hinwegzusetzen. Damit wurde das Fortbestehen des Bundes erstmals von Preußen und Österreich gemeinsam in Frage gestellt.
Der Konflikt um Schleswig und Holstein führte im Dezember 1863 zunächst zu einer Bundesexekution gegen Holstein und Lauenburg und dann -gegen die Proteste des Deutschen Bundes- im Februar 1864 zum Deutsch-Dänischen Krieg zwischen Preußen und Österreich auf der einen und Dänemark auf der anderen Seite. Im Gegensatz zu früheren Kriegen Preußens lag die eigentliche Führung nicht beim König oder den hohen Militärs, sondern beim Ministerpräsidenten, dessen politischem Kalkül die militärischen Schritte untergeordnet wurden. Als sich die Berichte über unüberlegte Befehle des 80-jährigen Oberbefehlshabers General Friedrich von Wrangel häuften und er beim König den Antrag gestellt hatte, Schleswig-Holstein als unabhängige Herzogtümer anzuerkennen, wurde er auf Betreiben Bismarcks abgelöst.

Nach dem Sieg Preußens an den Düppeler Schanzen am 18. April 1864 kam es auf der Londoner Konferenz zu ersten Verhandlungen über die Beilegung des Konflikts, die nicht zuletzt am Taktieren Bismarcks scheiterten. Der Krieg ging weiter und die verbündeten Österreicher und Preußen eroberten Jütland. Damit war Dänemark besiegt. Der Krieg endete mit dem Wiener Friedensvertrag vom 30. Oktober 1864. In diesem verzichtete Dänemark auf die Herzogtümer Schleswig, Holstein und Lauenburg. Die zeitweiligen Überlegungen, einen eigenen Bundesstaat unter den Augustenburgern zu bilden, blieben ergebnislos, weil Bismarck versuchte, einen solchen Bundesstaat zu einer Art preußischem Protektorat zu machen. Stattdessen wurden die Herzogtümer der gemeinsamen Verwaltung durch Österreich und Preußen unterstellt. Diese Konstruktion war für Bismarck nur ein Provisorium. Nicht zuletzt auf Grund seines Ziels der alleinigen Kontrolle über die Herzogtümer trat der preußisch-österreichische Gegensatz wieder hervor.

Innenpolitisch löste der Erfolg in Dänemark kein Nachgeben der Fortschrittspartei im preußischen Parlament aus. Die Liberalen befanden sich Bismarck gegenüber jetzt aber mit verschiedenen Anträgen in der Defensive, wenn sie z. B. wegen des Verfassungsstreits den Ausbau der Marine ablehnten, der von der Mehrheit sachlich gewollt wurde. In der liberalen Bewegung begannen ehemalige Kritiker des Ministerpräsidenten wie Heinrich von Treitschke, ihre Position zu ändern. Die Liberalen begannen in zwei Lager zu zerfallen: jene, die an der Verbindung zwischen nationaler Einigung und politischer Liberalisierung festhielten, und jene, die das erste Ziel auch unter Hintansetzung des zweiten anstrebten.

Nach dem Deutsch-Dänischen Krieg spielte Bismarck noch einige Zeit ernsthaft mit dem Gedanken einer preußisch-österreichischen Übereinkunft unter konservativem Vorzeichen. Als sich zeigte, dass die von Ludwig von Biegeleben bestimmte österreichische Deutschlandpolitik eine Erweiterung der preußischen Macht nicht zuließ, setzte Bismarck auf ein Bündnis mit der liberalen und nationalen Bewegung mit dem Ziel der Schaffung eines kleindeutschen Staates. Allerdings steuerte er keineswegs von Beginn an auf eine kriegerische Auseinandersetzung hin. Vielmehr hielt er sich zunächst mit dem Ziel der alleinigen Kontrolle über Schleswig und Holstein alle Optionen offen. In der Gasteiner Konvention kam es im August 1865 zur Teilung. Holstein wurde österreichisch und Schleswig preußisch verwaltet. Das Herzogtum Sachsen-Lauenburg kam an Preußen. Zum Dank erhielt Bismarck den preußischen Grafentitel. Für ihn war die Auseinandersetzung mit Österreich allerdings nur aufgeschoben.
Bismarck entschied sich letztlich auch deswegen für einen Krieg, weil er hoffte, so den preußischen Verfassungskonflikt beenden zu können, zeichnete sich doch immer deutlicher eine Spaltung des oppositionellen Lagers ab. Die zentrale Weichenstellung fiel auf einer Kronratssitzung am 28. Februar 1866. Bismarck gelang es, den vor einem „Bruderkrieg“ zurückschreckenden König von der Kriegspolitik zu überzeugen, und er schaffte es, Wilhelm I. in den folgenden Monaten von der Änderung seiner Meinung abzuhalten.

Bismarck unternahm nun alles, Österreich zu isolieren und zu provozieren. Er hielt sich aber auch die Möglichkeit offen, den Konfrontationskurs abzubrechen, sollte es zu starke Widerstände der Großmächte geben. Mit Erfolg hielt Bismarck insbesondere Napoleon III. zu einer neutralen Haltung an. Die Unterstützung Italiens sicherte Bismarck sich durch einen befristeten Bündnisvertrag (8. April 1866). Nachdem er erneut die Wahl eines direkt gewählten deutschen Parlaments ins Spiel gebracht hatte, um Österreich zu provozieren, löste er massive Kritik im Lager der preußischen Konservativen aus. Selbst Ludwig von Gerlach distanzierte sich in aller Schärfe von ihm. Die Liberalen hielten Bismarck weiterhin für unglaubwürdig und gingen auf dessen Bündnisangebot nicht ein. Auch in der Öffentlichkeit war ein deutscher Bürgerkrieg höchst unpopulär. Um den Krieg abzuwenden, verübte Ferdinand Cohen-Blind am 7. Mai 1866 sogar ein Pistolenattentat auf Bismarck, was jedoch scheiterte.

Als Österreich am 1. Juni 1866 die Entscheidung über die Zukunft Schleswig-Holsteins dem Bundestag übertrug, ließ Bismarck mit dem Argument, dies sei eine Verletzung der Gasteiner Konvention, die preußische Armee in Holstein einmarschieren. Daher beschloss der Bundestag am 14. Juni auf Antrag Österreichs die Mobilmachung des Bundesheeres. Preußen erklärte daraufhin den Bund für aufgelöst, da ein solcher Beschluss unzulässig sei. Es begann am 16. Juni 1866 mit den militärischen Operationen gegen die Königreiche Hannover, Sachsen und gegen Kurhessen. Ein Erfolg der preußischen Armee war keineswegs sicher. Ein Großteil der Zeitgenossen, so auch Napoleon III., rechneten mit einem österreichischen Sieg. Bismarck setzte somit alles auf eine Karte. „Wenn wir geschlagen werden […] werde ich nicht hierher zurückkehren. Ich werde bei der letzten Attacke fallen.“
Bismarck war bestrebt, den Krieg selbst unter Kontrolle zu halten. Dies stand im Gegensatz zu den Plänen von Generalstabschef Moltke, der einen unbegrenzten Krieg plante. Die Gefahr, das Militär könnte sich der politischen Führung entziehen, kam dann wegen der Kürze des Feldzuges nicht zum Tragen. Aus verschiedenen Gründen – etwa der Zerstrittenheit der Streitkräfte des Deutschen Bundes, der strategischen Nutzung der Eisenbahn und neuer Taktiken auf dem Schlachtfeld – erwies sich die preußische Armee als überlegen und errang am 3. Juli 1866 in der Schlacht von Königgrätz den entscheidenden Sieg.

Während Wilhelm I. und die Militärs darauf drängten, Wien zu erobern und Österreich harte Friedensbedingungen aufzuerlegen, setzte Bismarck gemäßigte Bedingungen durch, da er davon ausging, dass ein geschwächtes Österreich zu einem Bündnis mit Frankreich gezwungen wäre, was zu einem Zweifrontenkrieg gegen Preußen hätte führen können. Im Prager Frieden vom 23. August 1866 brauchte Österreich denn auch keine Gebiete an Preußen abzutreten, musste aber der Abtretung Venetiens an Italien, Auflösung des Deutschen Bundes und der Bildung eines Norddeutschen Bundes unter preußischer Führung zustimmen. Schleswig und Holstein wurden von Preußen ebenso annektiert wie Hannover, Kurhessen, Nassau und die Freie Stadt Frankfurt. Die süddeutschen Staaten blieben zunächst unabhängig.

Bismarck erwarb 1867 von der ihm wegen des erfolgreichen Deutschen Krieges bewilligten Dotation in Höhe von 400.000 Talern das Rittergut Varzin. Auf dessen Gemarkung ließ er die Papierfabrik Hammermühle errichten, die sich bald zum größten Unternehmen Ostpommerns entwickeln sollte, sowie weitere Papierfabriken. Das Gut Kniephof verkaufte er 1868 an seinen Neffen Philipp von Bismarck.

Der Krieg führte unter anderem dazu, dass die Konservativen ihre Position im preußischen Landtag erheblich ausbauen konnten. Um den Konflikt mit den Liberalen endlich beizulegen, ließ Bismarck ankündigen, er wolle den Landtag um „Indemnität“ bitten, also um die nachträgliche Genehmigung der Ausgaben. Dies bedeutete das Eingeständnis, dass er in den Jahren seit 1862 faktisch ohne rechtmäßigen Haushalt regiert hatte. Bismarck wollte dies aber nicht als Schuldeingeständnis gewertet wissen. Verfassungsrechtlich war die Position der Regierung, so der Historiker Heinrich August Winkler, noch immer unhaltbar.

Dennoch lag ein Politikwechsel vor, mit dem niemand gerechnet hatte. Die Frage, wie man das Angebot Bismarcks zu beurteilen habe, führte zur Spaltung der Liberalen. Während die einen argumentierten, von Bismarck seien weitere Fortschritte in der nationalen Frage zu erwarten, meinten andere, liberale Freiheitsrechte müssten Vorrang vor der nationalen Einheit haben. Dieser Konflikt führte zur Abspaltung der gemäßigten und nationalen Liberalen von der Fortschrittspartei und zur Bildung der Nationalliberalen Partei. Ähnliche Veränderungen fanden auch im Lager der Konservativen statt. Von den ideologisch geprägten Altkonservativen um Leopold von Gerlach, die sich schon vor dem Krieg von 1866 von Bismarck abgewandt hatten, trennten sich nunmehr realpolitisch gesinnte Bismarckanhänger und bildeten die Freikonservative Partei. Für seine Politik konnte sich Bismarck in den folgenden Jahren auf Nationalliberale und Freikonservative stützen.

Der Sieg im Deutschen Krieg bewirkte in der deutschen und preußischen Öffentlichkeit einen Wandel in der Beurteilung Bismarcks. Von den Zeitgenossen wurden die Umwälzungen als „Revolution von oben“ wahrgenommen. Bismarck selbst hatte nur mit einer Revolution gedroht, als er fürchtete, Russland würde die Annexionen in Norddeutschland verhindern: „Soll Revolution sein, so wollen wir sie lieber machen als erleiden.“ Gegenüber Napoleon III. hatte er bereits früher gesagt: „Revolutionen machen in Preußen nur die Könige.“

Bei den Annexionen hat Bismarck sich um das für die Konservativen zentrale Prinzip der monarchischen Legitimität nicht gekümmert. Der Reichstag des neuen Norddeutschen Bundes wurde nach demokratischen Grundsätzen gewählt. Die zentralen Aspekte der Verfassung des Bundes wurden von Bismarck in weiten Teilen selbst bestimmt („Putbuser Diktate“), wenngleich er in den parlamentarischen Beratungen auch einigen Kompromissen zustimmen musste. Die Verfassung, die im Kern auch während des Deutschen Kaiserreichs weiter galt, wird daher auch Bismarcksche Reichsverfassung genannt.

Zusammen mit der Position des preußischen Ministerpräsidenten und dem Amt des Außenministers hatte Bismarck als norddeutscher Bundeskanzler nun eine überaus starke Machtstellung inne. Im konstituierenden Reichstag (Februar bis April 1867), dem verfassungsvereinbarenden Gremium, war es den Nationalliberalen zwar gelungen, Bismarck noch einige Zugeständnisse abzuringen. Doch der Militäretat wurde weitgehend dem parlamentarischen Einfluss entzogen. Weder Kanzler noch andere Regierungsmitglieder konnten vom Reichstag zu Fall gebracht werden. Insgesamt ist Bismarck den liberalen Forderungen weit entgegengekommen, er hat aber auch alles dafür getan, zu verhindern, dass aus dem konstitutionellen ein parlamentarisches System werden konnte.

Die inneren Veränderungen gingen aber weit über die Verfassung hinaus. Sie umfassten die allgemeine Rechtsordnung, die Wirtschafts- und Sozialverfassung bis hin zur Verwaltungsstruktur. Bei allen Mängeln ist doch bemerkenswert, dass unter der Verantwortung Bismarcks, der kurze Zeit zuvor noch allgemein als Erzkonservativer gegolten hatte, ein für die Zeit sehr modernes Staatswesen entstand. In weiten Bereichen entsprach dieses liberalen Vorstellungen. Die eigentliche Umsetzung lag in anderen Händen. Insbesondere Rudolph von Delbrück war hier eine prägende Persönlichkeit. Dennoch ist Bismarcks persönlicher Einfluss nicht zu unterschätzen. Der Historiker Lothar Gall sieht die endgültige Durchsetzung des modernen bürokratisch-zentralisierten Anstaltsstaates in Mitteleuropa mit den für die Entfaltung der Industriegesellschaft wichtigen Rechtsformen und Institutionen sogar im Wesentlichen als Bismarcks Werk an.

In Fortführung seines funktionalen Verhältnisses zum nationalen Gedanken wurde die Nation nach 1866 für Bismarck als Integrationsfaktor wichtig. Bismarck erkannte, dass die Monarchie und der damit verbundene Staat auf Dauer nur überlebensfähig waren, wenn Preußen sich selbst an die Spitze der nationalen Bewegung stellte. Gleichzeitig war er aus machtpolitischen Gründen bestrebt, die süddeutschen Staaten mit dem Norddeutschen Bund zu vereinigen. Sein Ziel war nunmehr die Schaffung eines kleindeutschen Nationalstaates unter preußischer Führung.

Zwar wurden mit den süddeutschen Staaten Schutz- und Trutzbündnisse abgeschlossen, aber der Norddeutsche Bund erwies sich nicht als der von Bismarck erhoffte Magnet, der zu einem Anschluss der noch fernstehenden deutschen Länder führte. Die Wahlen zum Zollparlament gewannen in Bayern und Württemberg Gegner eines Anschlusses.

Bismarck war der Meinung, dass nur eine äußere Bedrohung die Stimmung in seinem Sinn verändern könnte. Allerdings versuchte er nicht, eine konkrete Bedrohungssituation selbst herbeizuführen. Zwar hielt er es für wahrscheinlich, dass die deutsche Einigung gewaltsam gefördert werden musste, aber „ein willkürliches, nur nach subjektiven Gründen bestimmtes Eingreifen in die Entwicklung der Geschichte hat immer nur das Abschlagen unreifer Früchte zur Folge gehabt; und daß die deutsche Einheit in diesem Augenblick keine reife Frucht ist, fällt meines Erachtens in die Augen.“

Außenpolitisch rechnete Bismarck von Seiten Frankreichs mit dem stärksten Widerstand gegen einen deutschen Nationalstaat. In der französischen Öffentlichkeit wurden unter der Losung „Rache für Sadowa“ (Königgrätz) territoriale Forderungen gestellt, die zur Luxemburgkrise führten. Mit der Neutralisierung Luxemburgs wurde das Problem im Mai 1867 gelöst. Bismarck nutzte die Gelegenheit, durch Parlamentsreden und in Presseartikeln die antifranzösische Stimmung noch zu verstärken. Napoleon III. sah den Ausgang des Konflikts als Niederlage an und tat danach alles, um weitere preußische Ambitionen zu unterbinden. Unklar ist, ob Bismarck tatsächlich bereit war, den Erwerb Luxemburgs durch Frankreich zu akzeptieren und nur die Umstände dies verhinderten, oder ob das Ergebnis der Krise seinem bewussten Kalkül entsprang. Unabhängig davon standen sich der Norddeutsche Bund und Frankreich nun in aller Schärfe gegenüber.

Ein weiterer Konflikt mit Frankreich entstand Anfang 1870 im Laufe der spanischen Thronfolge-Frage. Bismarck drängte Prinz Leopold von Hohenzollern-Sigmaringen zur Kandidatur. Der Prinz entstammte der katholischen Linie der in Preußen regierenden Hohenzollern, was ihn aus Sicht von Napoleon III. unannehmbar machte. Bismarck ging es zunächst nur darum, einen diplomatischen Sieg zu erringen und sich dabei mehrere Möglichkeiten offen zu halten. Sowohl Bismarck als auch Kaiser Napoleon III. wollte für sich einen Ansehensverlust verhindern, so dass der diplomatische Konflikt zu einer nationalen Frage eskalierte.
In Frankreich erzielte die Hohenzollernkandidatur die von Bismarck erhoffte Wirkung, befürchtete man dort doch, künftig von hohenzollerschen Staaten eingekreist zu werden. Die Krise schien durch den Verzicht des Prinzen zunächst entschärft. Wilhelm I. wies jedoch das Verlangen Frankreichs zurück, er solle im Namen des Hauses Hohenzollern auch für alle Zukunft auf ähnliche Kandidaturen verzichten. Der König informierte Bismarck darüber in der sogenannten Emser Depesche. Dieser nutzte die Gelegenheit und stellte in einer Pressemitteilung die Begegnung von Wilhelm mit dem französischen Botschafter als besonders schroff dar. Napoleon III. war damit vor aller Welt brüskiert worden. Angesichts der Reaktionen in der französischen Öffentlichkeit sah er keine andere Wahl mehr, als Preußen den Krieg zu erklären. Damit erschien Frankreich, wie von Bismarck beabsichtigt, als Aggressor. In Deutschland war die öffentliche Meinung nun ganz auf Seiten Preußens und die süddeutschen Staaten sahen den Bündnisfall als gegeben an. Dagegen war Frankreich außenpolitisch völlig isoliert.

Der Deutsch-Französische Krieg schien zunächst nach gewohntem Muster eine rasche Entscheidung zu bringen. Infolge der Gefangennahme Napoleons III. bei der Schlacht von Sedan brach das Zweite Kaiserreich zusammen. Zu einem schnellen Friedensschluss kam es allerdings nicht, weil die deutsche Seite, mit Bismarck in führender Rolle, die Abtretung von Elsass-Lothringen zur Bedingung machte. Diese territoriale Forderung wurde auch unter dem Eindruck der öffentlichen Meinung in Deutschland gestellt. Kurzfristig führte dies dazu, dass die neu gebildete französische Regierung den Krieg nicht nur fortsetzte, sondern ihn sogar zu einem nationalen Volkskrieg erhob. Langfristig wurden die deutsch-französischen Beziehungen durch die Elsass-Lothringen-Frage schwer belastet. Die dauerhafte Schwächung Frankreichs entwickelte sich zu einem zentralen Ziel der Bismarckschen Außenpolitik.

Der Ministerpräsident mischte sich während des Krieges wiederholt in die Entscheidungen der Militärs ein. Dies führte zu heftigen Konflikten mit der militärischen Führung, die ihren Höhepunkt anlässlich der Frage einer Belagerung oder Beschießung von Paris erreichten. Hier setzte Bismarck sich mit seiner Forderung nach einer Beschießung durch.

Der Krieg hatte die Gegner der deutschen Vereinigung auch in Süddeutschland in die Defensive gedrängt. Seit Mitte Oktober 1870 verhandelte Bismarck in Versailles mit den Delegationen der süddeutschen Länder. Mit einem Bündnis der deutschen Fürsten und freien Städte sollte nicht zuletzt weitergehenden Vorstellungen des nationalen und liberalen Lagers begegnet werden. Bei den Verhandlungen verzichtete Bismarck auf direkten Druck und argumentierte stattdessen mit den Vorteilen eines solchen Zusammenschlusses. Insgesamt setzte er seine Vorstellungen durch.

Als Erste erklärten Baden und Hessen-Darmstadt ihren Beitritt zum Norddeutschen Bund. Württemberg und Bayern machten den Weg zur Gründung des Deutschen Reiches frei, nachdem ihnen Reservatsrechte zugebilligt worden waren. Bismarck selbst verfasste den „Kaiserbrief“, mit dem Ludwig II. von Bayern Wilhelm I. um die Annahme der Kaiserkrone bat. In diesem Zusammenhang bestach Bismarck Ludwig auch mit Mitteln aus dem Welfenfonds. Nur mit Mühe gelang es ihm allerdings, König Wilhelm, der einen Bedeutungsverlust des preußischen Königtums befürchtete, zur Annahme des Kaisertitels zu bewegen.

Am 18. Januar 1871 kam es im Spiegelsaal von Versailles zur „Kaiserproklamation“. Sie markierte die Gründung des Deutschen Kaiserreichs. Wenige Tage später kapitulierte Paris. Der Deutsch-Französische Krieg endete am 10. Mai 1871 mit dem Frieden von Frankfurt.

Bismarck hatte damit den Höhepunkt seiner politischen Laufbahn erreicht. Er wurde in den Fürstenstand erhoben und Wilhelm I. machte ihm den Sachsenwald in der Nähe Hamburgs zum Geschenk. Bismarck gehörte nunmehr zu den großen Grundbesitzern des Reiches und war, auch dank der geschickten Verwaltung seiner Gelder durch Gerson Bleichröder, ein reicher Mann. Den Großteil seines Vermögens erwirtschaftete er über den Verkauf des Holzes aus dem Sachsenwald. Sein Hauptabnehmer Friedrich Vohwinkel erwarb zwischen 1878 und 1886 Holz im Wert von mehr als einer Million Mark aus Bismarcks Wäldereien. Bismarck erwarb ein ehemaliges Hotel in Friedrichsruh im Sachsenwald und ließ es umbauen. Nach 1871 wurde Friedrichsruh zum Mittelpunkt seines Privatlebens.

Das neue Kaiserreich übernahm weitgehend die Verfassung des Norddeutschen Bundes. Als Reichskanzler, Vorsitzender des Bundesrates, preußischer Ministerpräsident und Außenminister blieb Bismarck so der dominierende Politiker. Darüber hinaus konnte er auf sein ungeheures Prestige als Gründer des Reiches bauen. Dieses wog auch gegenüber Wilhelm I. schwer, sodass Bismarck seinen Willen gegenüber dem Deutschen Kaiser meist durchsetzen konnte. Wilhelm klagte daher: „Es ist nicht leicht unter einem solchen Kanzler Kaiser zu sein.“

So sehr Bismarck auch von Leidenschaft zur Politik und der Liebe zur Macht durchdrungen war, so sehr sehnte er sich gleichzeitig nach einer Befreiung von dieser Last. Bereits 1872 klagte er: „Mein Öl ist verbraucht, ich kann nicht mehr.“ Bismarck war in den Jahren seiner Kanzlerschaft nicht nur psychisch belastet, sondern auch körperlich stark angeschlagen. Immer öfter musste er sich deswegen teilweise für Monate auf seine Güter zurückziehen. Bismarck trank und aß im Überfluss. Er wurde immer dicker und wog 1879 247 Pfund, bei einer Körpergröße von 1,90 Meter. Er litt unter zahlreichen teils chronischen Krankheiten wie Rheuma, Venenentzündungen, Verdauungsstörungen, Hämorrhoiden und vor allem unter Schlaflosigkeit, hervorgerufen durch Völlerei. Neben dem Konsum von Alkohol und Tabak berichteten Zeitgenossen wie die Baronin Hildegard von Spitzemberg auch von der Einnahme von Morphium. Erst Ernst Schweninger, sein neuer Arzt, konnte ihn in den 1880er-Jahren zu einer gesunden Lebensweise überreden. Zuvor litt er unter Gesichtneuralgien, weshalb er sich vor Schweningers Behandlung einen Vollbart wachsen ließ, damit er sich nicht rasieren musste.

Im privaten Leben Bismarcks spielte die Familie eine große Rolle. Aber auch in diesem Bereich setzte er stets seinen Willen durch. Als sein Sohn Herbert von Bismarck 1881 die geschiedene Fürstin Elisabeth zu Carolath-Beuthen heiraten wollte – eine Katholikin, die mit zahlreichen Bismarck-Gegnern, etwa Marie Gräfin Schleinitz, verwandt und verschwägert war – verhinderte Bismarck dies letztlich, indem er ihm erst mit Enterbung, dann mit Selbstmord drohte. Herbert fügte sich, war seither aber ein verbitterter Mann.

Die deutsche Reichsgründung veränderte die europäischen Machtverhältnisse grundlegend. Das neue Reich stand zunächst außerhalb der Pentarchie, die sich in den letzten hundert Jahren herausgebildet hatte, besaß es doch eine gänzlich andere machtpolitische Qualität als das recht kleine Preußen. Daher galt das Reich als Störenfried der internationalen Ordnung. Nach einem längeren Lernprozess erkannte Bismarck, dass das allgemeine Misstrauen der übrigen Staaten gegenüber Deutschland nur durch Selbstbeschränkung und den Verzicht auf weitere territoriale Gewinne abgebaut werden konnte. Er versicherte daher, dass das Reich saturiert sei. „Wir verfolgen keine Macht-, sondern eine Sicherheitspolitik“, bekräftigte er 1874.

Ein Grundziel von Bismarcks Außenpolitik blieb es, Frankreich zu schwächen. Um dies zu erreichen, bemühte er sich um gute Beziehungen zu Österreich und zu Russland, ohne dabei eine Seite zu präferieren. Ergebnis dieser Strategie war das Dreikaiserabkommen von 1873. Wie schwierig es für das Deutsche Reich jedoch war, seine neue Position auf Kosten Frankreichs zu festigen, zeigte bereits 1875 die weitgehend von Bismarck selbst provozierte „Krieg-in-Sicht-Krise“. Der Versuch Bismarcks, eine deutsche Hegemonialpolitik gegenüber Frankreich durchzusetzen, scheiterte.

Auch wenn Bismarck dem wiedererstarkten Frankreich lediglich drohen wollte und nicht wirklich einen Krieg plante, war die Krise für ihn lehrreich. Sie zeigte, dass eine Annäherung zwischen Frankreich und Russland nicht grundsätzlich ausgeschlossen war. Die Möglichkeit eines Bündnisses zwischen beiden bereitete ihm für den Rest seiner Amtszeit Sorge. Aber auch England hatte deutlich gemacht, dass es einen weiteren Machtzuwachs Deutschlands nicht akzeptieren werde. Im Zweifelsfall arbeiteten die europäischen Flügelmächte zusammen, um eine Störung des machtpolitischen Gleichgewichts zu verhindern.

Vor allem aus der Krieg-in-Sicht-Krise zog Bismarck den Schluss, dass für das Reich eine defensive Politik die einzig realistische Alternative sei. Durch seine Lage in der Mitte Europas drohte dem Reich, in einen großen europäischen Krieg mit einbezogen zu werden. Bismarck entwickelte vor diesem Hintergrund ein diplomatisches Konzept, das darauf abzielte, die Spannungen zwischen den Großmächten an die Peripherie zu verlagern, um so die Mitte Europas vor Kriegen zu bewahren. Zum ersten Mal zum Tragen kam dieses Konzept bei der Balkankrise zwischen 1875 und 1878. Bismarck förderte dabei einerseits die Spannungen zwischen den Mächten, verhinderte aber gleichzeitig, dass die Konflikte außer Kontrolle gerieten. Seine außenpolitische Strategie fasste er 1877 im Kissinger Diktat zusammen. Dabei ging er von „einer politischen Gesamtsituation [aus], in welcher alle Mächte außer Frankreich unser bedürfen, und von Koalitionen gegen uns durch ihre Beziehungen zueinander nach Möglichkeit abgehalten werden.“

Während des Berliner Kongresses zur Beendigung der Balkankrise präsentierte sich Bismarck 1878 als „ehrlicher Makler“. Dies verstärkte zwar sein außenpolitisches Prestige auch im Ausland, es zeigten sich aber auch sofort die Grenzen seines Konzepts. Zar Alexander II. machte Bismarck dafür verantwortlich, dass Russlands Erfolge eng begrenzt blieben. Dies führte dazu, dass Bismarck die Zusammenarbeit mit Österreich forcierte. Dies wiederum mündete im Zweibundvertrag von 1879. Aus diesem Defensivbündnis gegenüber Russland wurde eine dauerhafte Allianz, die die Außenpolitik während des gesamten Kaiserreichs prägen sollte. Bismarck selbst stilisierte die Verbindung als eine Art zeitgemäße Neuausgabe des Deutschen Bundes und als „Bollwerk des Friedens über lange Jahre hinaus. Populär bei allen Parteien, exklusive Nihilisten und Sozialisten.“

Bismarck gelang es aber auch, die Spannungen zwischen Deutschland und Russland abzubauen und 1881 das Dreikaiserbündnis abzuschließen. Damit war eine enge Verbindung Russlands mit Frankreich zunächst verhindert worden. Das Bündnissystem wurde 1882 durch den Dreibund zwischen Deutschland, Österreich-Ungarn und Italien, sowie 1883 durch den Anschluss Rumäniens an den Zweibund ergänzt.

Mitte der 1880er-Jahre schien Bismarck die diplomatische Absicherung des Reichs erfolgreich abgeschlossen zu haben. Das Konzept der Saturiertheit wurde jedoch durch die imperialistischen Tendenzen der Zeit immer mehr in Frage gestellt. Bismarck selbst war eigentlich Gegner kolonialer Erwerbungen.

Auch in Deutschland bildete sich eine imperialistische Bewegung, die auf den Erwerb von Kolonien drängte. Deren Druck konnte sich Bismarck nicht auf Dauer entziehen. Verschiedene innen- und außenpolitische Gründe führten zu einem Sinneswandel des Reichskanzlers. Dabei spielte auch die von ihm gefürchtete Thronübernahme des liberalen, englandfreundlichen Kronprinzen Friedrich Wilhelm eine Rolle. Da der Erwerb von Kolonien die Beziehungen zu Großbritannien verschlechtern musste, habe die Kolonialpolitik, „nur den Zweck, einen Keil zwischen den Kronprinzen und England zu treiben.“ Bismarck schien 1884 schließlich zur Überzeugung gekommen, dass eine erfolgreiche Kolonialpoilitk doch mehr Chancen, als Risiken berge.

1884 und 1885 kam es zum Erwerb mehrerer Territorien in Afrika und im Stillen Ozean. Da sich die innenpolitischen Konstellationen in Frankreich und Großbritannien änderten, verlor Bismarck jedoch schnell das Interesse an deutscher Kolonialpolitik. Sie blieb zunächst eine Episode. Gegenüber dem Kolonialverfechter Eugen Wolf äußerte Bismarck 1888: „Ihre Karte von Afrika ist ja sehr schön, aber meine Karte von Afrika liegt in Europa. Frankreich liegt links, Russland liegt rechts, in der Mitte liegen wir. Das ist meine Karte von Afrika.“ Jedoch hatte Bismarck ungewollt Kräfte freigesetzt, die sich in der Wilhelminischen Zeit nicht mehr beherrschen lassen sollten.

In der zweiten Hälfte der 1880er-Jahre wurde Bismarcks außenpolitisches System zunehmend bedroht. Ab 1886 nahmen in Frankreich die revanchistischen Tendenzen zu. Zeitweilig drohte ein französisch-russisches Bündnis und damit die Gefahr eines Zweifrontenkriegs für das Deutsche Reich. Bismarck bauschte die Krise mit Frankreich allerdings auf, um seine innenpolitischen Pläne zur Heeresverstärkung durchsetzen zu können.

Fast zeitgleich entstand eine neue Balkankrise. Bismarck versuchte vergeblich, die Spannungen zwischen den beiden Kontrahenten Österreich und Russland auszugleichen. Das Dreikaiserbündnis zerbrach. In Russland nahmen daraufhin die Stimmen für ein Bündnis mit Frankreich weiter zu. Probleme durch die Schutzzollpolitik Bismarcks verschärften die Situation. In Deutschland plädierten einflussreiche Persönlichkeiten aus Militär und Diplomatie wie Friedrich von Holstein, Helmuth Karl Bernhard von Moltke und Alfred von Waldersee für einen Präventivkrieg gegen Russland. Bismarck lehnte solche Ideen strikt ab. Er hielt den Krieg weiter für vermeidbar. Als Macht- und Realpolitiker spielten nationalistische und sozialdarwinistische Vorstellungen für ihn keine Rolle.

Zwar war Bismarcks altes Bündnissystem zerbrochen, doch konnte er die Krise noch einmal entschärfen. Auf dem Balkan weigerte er sich, für England und Österreich „die Kastanien aus dem Feuer zu holen.“ Ohne mit Österreich zu brechen, gelang es ihm, einen offenen Krieg zu verhindern. Im Februar 1887 war Bismarck im Hintergrund am Zustandekommen der Mittelmeerentente zwischen Großbritannien, Österreich und Italien beteiligt. Ihr Ziel war es, den russischen Expansionsdrang zu begrenzen. Kurze Zeit später schloss Bismarck mit Russland den Rückversicherungsvertrag ab, um Russland erneut an Deutschland zu binden.

Wie schon in der Zeit des Norddeutschen Bundes beruhte die Innenpolitik des Deutschen Reiches in den ersten Jahren auf einer Zusammenarbeit Bismarcks mit den Freikonservativen und den Nationalliberalen. Diese übten einen erheblichen Einfluss auf die Vereinheitlichung, Gestaltung und Modernisierung der Wirtschafts- und Rechtsordnung aus, sowohl im Reich wie auch teilweise in Preußen. Bismarck scheute dabei auch zeitweise nicht vor einem Konflikt mit den Konservativen zurück. Als das preußische Herrenhaus sich 1872 weigerte, einer Reform der Kreisordnung zuzustimmen, veranlasste Bismarck Wilhelm I. dazu, zusätzliche Herrenhausmitglieder zu ernennen, um mit Hilfe dieses „Pairsschubes“ das Gesetz durchzubringen. Die Empörung bei den Konservativen war groß und Roon sprach gar von einem Staatsstreich. Dies führte zum Rücktritt Bismarcks vom Posten des preußischen Ministerpräsidenten zu Gunsten Roons. Da dieser sich dem Amt jedoch nicht gewachsen zeigte, übernahm es Bismarck nach kurzer Zeit wieder selbst.

Auf verschiedenen Feldern zeigten sich bald schon erste Grenzen der Zusammenarbeit Bismarcks mit den Liberalen. Zum wichtigsten Streitpunkt wurde ab 1873 der Bereich der Militärorganisation, um den es heftige Auseinandersetzungen gab. Auf den von Bismarck geforderten faktischen Verzicht des Parlaments auf Kontrolle des Militärhaushaltes („Äternat“) konnten sich die Nationalliberalen nicht einlassen. Eine Lösung brachte 1874 ein Kompromissvorschlag von Johannes Miquel. Danach wurden die Ausgaben für jeweils sieben Jahre bewilligt („Septennat“). Trotz dieses relativen Erfolgs hatte Bismarck den Liberalen die Grenzen seiner Kooperationswilligkeit deutlich gemacht, obwohl diese ihm de facto acht Jahre Handlungsfreiheit gaben. Gleichzeitig stärkte die grundsätzliche Einigung mit dem Parlament Bismarcks Stellung gegenüber dem Militär.

Nationalliberale und Bismarck stimmten in ihrer Gegnerschaft zu einer katholischen Partei überein. Für Bismarck spielte dabei auch eine Rolle, dass mit der 1870 gegründeten Zentrumspartei eine seinem Einfluss entzogene, im Kern konservative, katholische Partei entstanden war. Das Zentrum schaffte eine Klammer zwischen katholischer Arbeiterschaft, Honoratioren und Kirche. Bismarck reduzierte es konsequent auf den von ihm gefürchteten Ultramontanismus. Tatsächlich wurde das Zentrum in den ersten Reichstagswahlen von 1871 auf Anhieb zweitstärkste Kraft. Damit sank der Wahlerfolg der Nationalliberalen insbesondere im katholisch-bürgerlichen Lager. Der Kulturkampf hatte für Bismarck zwar vor allem politische Gründe, doch er sah in Ludwig Windthorst, dem herausragenden Politiker der Zentrumspartei, einen persönlichen Gegner: „Mein Leben erhalten und verschönern zwei Dinge, meine Frau und Windthorst. Die eine ist für die Liebe da, der andere für den Haß.“

Bismarck stilisierte die Katholiken zu Reichsfeinden – auch um aufziehender Kritik an seiner Amtsführung entgegenzuwirken. Ab 1872 wurden im Rahmen des sogenannten Kulturkampfes verschiedene Sondergesetze gegen die Katholiken beschlossen und wiederholt verschärft. Im Zuge dieser Auseinandersetzung wurden Rechte und Machtstellung der Kirche durch Reichs- und preußische Landesgesetze beschnitten (Kanzelparagraph, Brotkorbgesetz), aber auch die Zivilehe eingeführt. In diesem Zusammenhang äußerte Bismarck am 14. Mai 1872 vor dem Reichstag: „Seien Sie außer Sorge, nach Kanossa gehen wir nicht, weder körperlich noch geistig.“

Die erste, harte Etappe des Kulturkampfes endete 1878. In diesem Jahr starb Pius IX., sein Nachfolger Leo XIII. signalisierte Verständigungsbereitschaft, an der Bismarck gelegen war, um das Zentrum auszubooten. Eine direkte Verhandlung mit dem Heiligen Stuhl schadete der Partei und verringerte ihr Ansehen bei der katholischen Bevölkerung. Zudem hatte der Kanzler nicht das geschafft, was er vorgehabt hatte. Die katholische Basis und die katholische Partei ließen sich nicht spalten, vielmehr wurde durch die staatlichen Angriffe die Bildung eines katholischen Milieus gefördert. Darüber hinaus unterstützte die katholische Presse die Partei, die zunehmend Mandate im Reichstag gewann. Ein letzter Grund für Bismarck ergab sich aus dem letztlich vollzogenen Bruch mit den Nationalliberalen. Er lotete die Möglichkeit aus, das Zentrum in seine Politik einzubauen und somit eine „blau-schwarze Koalition“ mit den Konservativen zu bilden.

Der Kulturkampf endete im April 1887 mit dem zweiten Friedensgesetz. Bis dahin trugen beide Seiten zur Deeskalation bei. Eine Folge des Kulturkampfes bis heute sind die Zivilehe und die staatliche Schule. Für die zukünftige Politik Bismarcks nicht unwichtig war, dass Windthorst keineswegs ein ultramontaner Eiferer war. Er war zwar preußenkritisch, aber eben auch pragmatisch und konstitutionell ausgerichtet, was Bismarck neue politische Optionen eröffnete.

Die Basis der Zusammenarbeit von Bismarck mit den Liberalen wurde immer schwächer. Mit Aufzug der Gründerkrise begannen zahlreiche Großgrundbesitzer und Industrielle, Forderungen nach Schutzzöllen zu erheben. Bismarck hoffte, dass die Wirtschaftspolitik zur Spaltung der Liberalen führen würde. Obwohl er sich öffentlich nicht zu diesem Thema äußerte, ermutigte er die Interessenvertreter zur Abspaltung, die dann auch vollzogen wurde. In der neu gegründeten Deutschkonservativen Partei sah Bismarck einen möglichen Bündnispartner; das Parteiprogramm wurde mit ihm persönlich abgestimmt. Zum Vorzeichen des aufziehenden Konflikts mit den Liberalen wurde 1876 der Rücktritt Rudolph von Delbrücks vom Amt des Präsidenten des Reichskanzleramtes. Delbrück hatte als Verkörperung der Zusammenarbeit Bismarcks mit den Liberalen sowie als Hauptvertreter des Wirtschaftsliberalismus gegolten.

In Hinblick auf den erwarteten baldigen Thronwechsel stellten die Liberalen für Bismarck eine Gefahr dar. Unter einem Kaiser Friedrich III. stand der Wechsel zu einer liberalen Regierung zu erwarten – nach dem Vorbild der britischen Regierung unter Premierminister William Ewart Gladstone. Bismarck versuchte 1877 Albrecht von Stosch, den Chef der Marine, auszuschalten, da dieser als möglicher Kanzler des künftigen Kaisers galt. Als dies scheiterte, drohte Bismarck mit dem eigenen Rücktritt und zog sich zeitweise auf sein Gut in Varzin zurück. Der Versuch, von dort aus die Nationalliberalen mit Angeboten – etwa ein Ministeramt für Rudolf von Bennigsen – und Zugeständnissen für seine Politik zu gewinnen, war nicht erfolgreich. Ihm wurden Gegenforderungen präsentiert, die seinen Plänen zuwiderliefen, den Parlamentarismus einzudämmen. Daraufhin entschloss er sich zum Bruch mit den Nationalliberalen.

Mit der Forderung der Nationalliberalen, die Reichsverfassung in einem stärker parlamentarischen Sinne umzugestalten, war eine Grenze erreicht worden, die Bismarck nicht zu überschreiten bereit war. Im Reichstag erklärte er diesbezüglich 1879: „Eine Fraktion kann sehr wohl die Regierung unterstützen und dafür einen Einfluss auf sie gewinnen, aber wenn sie die Regierung regieren will, dann zwingt sie die Regierung, ihrerseits dagegen zu reagieren.“ Angesichts der gegenseitigen politischen Blockade sah sich Bismarck zu einer Flucht nach vorn gezwungen. In einer Reichstagsrede kündigte er am 22. Februar 1878 einen innenpolitischen Kurswechsel an. Das dabei von ihm angedeutete Ziel eines staatlichen Tabakmonopols widersprach zentralen wirtschaftsliberalen Prinzipien. Über den konkreten Anlass hinaus fassten die dem Liberalismus nahestehenden Regierungsmitglieder dies als einen ersten Schritt hin zu einer grundlegend veränderten Wirtschaftspolitik auf. Heinrich von Achenbach und Otto von Camphausen legten ihre Ämter nieder. An ihre Stelle traten Personen, die in den Parteien kaum verankert waren und nur geringes politisches Gewicht besaßen.

Seit der Rede von August Bebel im Reichstag am 25. Mai 1871 zu Gunsten der Pariser Kommune sah Bismarck in den Sozialdemokraten eine revolutionäre Bedrohung. Schon damals skizzierte er seine zukünftige Politik so: „1. Entgegenkommen gegen die Wünsche der arbeitenden Klassen, 2. Hemmung der staatsgefährlichen Agitation durch Verbots- und Strafgesetze.“

Nach Bismarcks Ansicht verstärkten die sozialen Auswirkungen der Gründerkrise die revolutionäre Gefahr. Zwei Attentate auf Kaiser Wilhelm I. im Jahr 1878 dienten Bismarck als willkommener Anlass, mit einem Sozialistengesetz gegen die Sozialistische Arbeiterpartei vorzugehen. Er wollte einen „Vernichtungskrieg führen durch Gesetzesvorlagen, welche die sozialdemokratischen Vereine, Versammlungen, die Presse, die Freizügigkeit (durch die Möglichkeit der Ausweisung und Internierung) […] träfen.“

Über den Kampf gegen die Sozialdemokratie hinaus, boten die Attentate für Bismarck aber auch die Gelegenheit, angesichts einer fehlenden parlamentarischen Unterstützung wieder in die politische Offensive zu gehen und zu neuen Mehrheiten zu kommen. Ein erster Gesetzentwurf scheiterte an der überwältigenden Mehrheit des Reichstags. Nach dem zweiten Attentat ließ Bismarck das Parlament auflösen. Er wollte wieder die Rückendeckung der Nationalliberalen gewinnen und darüber hinaus die Regierungsbasis weiter nach rechts verschieben. Nach der Wahl waren die beiden konservativen Parteien zusammen stärker als die Nationalliberalen.

Im neuen Reichstag stimmten schließlich auch die Nationalliberalen, nach einigen Zugeständnissen, dem Sozialistengesetz zu. Es blieb, mehrfach vom Parlament verlängert, bis 1890 in Kraft. Dieses Ausnahmegesetz verbot die sozialistische Agitation, während die politische Arbeit der sozialdemokratischen Parlamentarier davon unberührt blieb. Letztlich verfehlte das Gesetz seinen Zweck und trug ungewollt zur Verfestigung eines sozialistischen Milieus bei, denn erst jetzt setzte sich die marxistische Theorie wirklich durch. Bemerkenswert ist, dass Bismarck dem Thema später in seinen "Gedanken und Erinnerungen" kein einziges Wort widmete.

Vor dem Hintergrund der Wirtschaftskrise wurde im Jahr 1878 der Ruf von Großgrundbesitzern und Schwerindustriellen nach Schutzzöllen lauter. Als sich für diese Forderung eine Mehrheit im Reichstag abzeichnete, sprach sich auch Bismarck, der auf erhöhte Staatseinnahmen hoffte, im so genannten „Weihnachtsbrief“ vom 15. Dezember 1878 für eine Verbindung von Steuerreform und Schutzzollpolitik aus. Dem stimmten letztlich nur wenige Nationalliberale zu. Bismarck stützte sich stattdessen auf die Deutschkonservative Partei, auf die Freikonservativen und auf das Zentrum. Die liberale Ära war damit beendet. Bismarck betonte nunmehr die Bedeutung des Obrigkeitstaates als Garanten der nationalen Einheit und setzte auf eine nationalkonservative Sammlungsbewegung unter Einschluss des Zentrums. Eine feste parlamentarische Basis, wie sie zuvor die Nationalliberalen gestellt hatten, bot diese Parteienkonstellation allerdings nicht. Viele politische Initiativen Bismarcks blieben daher in den folgenden Jahren ergebnislos.

Der Übergang vom Freihandel zum Protektionismus vollzog sich in den folgenden Jahren in mehreren Schritten. Bismarck hoffte, aus seinem Eingehen auf die Wünsche der Verbindung von „Roggen und Eisen“ politisches Kapital schlagen zu können, um die konservative Basis des Reiches auszubauen und seine eigene Position zu festigen.

Angesichts seiner schwierigen parlamentarischen Situation versuchte Bismarck, die bisherige Bedeutung der Parteien zurückzudrängen. Das Feld der Auseinandersetzung sollte die Sozial- und Wirtschaftspolitik werden. Daher übernahm er 1880 selbst das Amt des Handelsministers, das er bis 1890 bekleidete. Um Einfluss auf die Wirtschaftsgesetzgebung zu nehmen, versuchte er einen Volkswirtschaftsrat aus Vertretern der Wirtschaftsverbände zu etablieren, mit dem das Parlament umgangen werden sollte. Dies scheiterte allerdings am Widerstand der Parteien.

Hauptziel von Bismarcks Sozialpolitik war, eine stärkere Staatsbindung zu erzeugen. Die Parteien sollten dabei von ihrer Basis getrennt werden. Bismarck verschleierte sein eigentliches Ziel des Machterhalts dabei keineswegs. Geplant war zunächst nur eine Unfallversicherung, später kamen Versicherungen gegen Krankheit, Invalidität und Altersarmut hinzu. Diese sollten weitgehend staatlich kontrolliert sein – zeitweise sprach Bismarck sogar von Staatssozialismus. Er wollte so „in der großen Masse der Besitzlosen die konservative Gesinnung erzeugen, welche das Gefühl der Pensionsberechtigung mit sich bringt.“

Nicht die Versicherungen an sich, aber Bismarcks persönliche Motive stießen auf heftigen Widerstand. Letztlich strich das Parlament aus der Gesetzesvorlage zur Unfallversicherung alle „staatssozialistischen“ Elemente heraus. Bismarcks Kalkül, nach einer Reichstagsauflösung die Wähler mit der Parole eines „sozialen Königtums“ und mit antiparlamentarischen Tönen zu überzeugen, ging nicht auf. Insbesondere die Linksliberalen gewannen bei der Reichstagswahl am 27. Oktober 1881 deutlich hinzu. Bismarck dachte danach kurzzeitig an Rücktritt, entschied sich aber dagegen und deutete sogar Staatsstreichpläne an.

Anstelle der ursprünglich geplanten Reichsanstalt setzte er später die Berufsgenossenschaften durch. Gedacht als neokorporativer Zusammenschluss jenseits der Parteien, wurden die Genossenschaften von den Unternehmern dominiert. Entgegen dem ursprünglichen Ziel gewannen in ihnen die Vertreter der Rechtsparteien an Gewicht. Die Krankenversicherung wurde dagegen von der Selbstverwaltung der Arbeiter dominiert; Sozialdemokraten dominierten viele der Allgemeinen Ortskrankenkassen.

Mit der Sozialgesetzgebung schuf Bismarck einen Pfeiler des modernen Sozialstaats; seine machtpolitischen Ziele erreichte er aber nicht. Der Versuch, der Sozialdemokratie die „Wurzeln abzugraben“, schlug mittelfristig ebenso fehl wie das Vorhaben, den Obrigkeitsstaat zu Lasten der Parteien auszubauen. Bismarcks Interesse an der Sozialgesetzgebung ließ nach: Die Alters- und Invalidenversicherung von 1889 wickelte er geschäftsmäßig ab.

Bismarck und Innenminister Robert von Puttkamer gelang es, die preußischen Beamten auf eine bedingungslose Unterstützung der Regierungspolitik zu verpflichten. Zugute kam Bismarck, dass sich innerhalb der Nationalliberalen, unter Führung von Johannes Miquel, die Vertreter eines protektionistischen und staatsnahen Kurses durchsetzten. Sie bekannten sich zu wesentlichen Aspekten von Bismarcks Politik. Nicht zuletzt mit dem Ziel, die materiellen Interessen der konservativen Wähler zu bedienen, legte Bismarck 1885 eine protektionistische Zollvorlage vor, mit der die Importe massiv beschränkt wurden.

Auch um nationalistische Emotionen nutzbar zu machen, verstärkte Bismarck die antipolnische Politik in den preußischen Ostprovinzen. Mit der Ausweisung von nichtpreußischen Polen ab 1885 und dem Ansiedlungsgesetz von 1886 setzte eine intensive Germanisierung ein. Die französische Revanchismusbewegung nutzte Bismarck, um mit einer breit angelegten Pressekampagne alle Kritiker als Vaterlandsverräter zu diskreditieren, die sich insbesondere seinen militärpolitischen Plänen entgegenstellten. Nach der Reichstagsauflösung wurde die nationalistische Agitation noch einmal verstärkt.

Aus den Reichstagswahlen vom Februar 1887 ging das Regierungslager aus Konservativen und Nationalliberalen mit absoluter Mehrheit hervor. Bismarck besaß mit den so genannten Kartellparteien nun jene parlamentarische Mehrheit, die er in den vergangenen zehn Jahren angestrebt hatte. Er konnte jetzt sowohl seine militärpolitischen Pläne als auch Begünstigungen für seine konservative Klientel durchsetzen.

Aufgrund von Bismarcks neuer Machtstellung spielte die Thronbesteigung von Friedrich III. im März 1888 kaum noch eine Rolle. Als der todkranke neue Kaiser sich weigerte, einer Verlängerung der Legislaturperiode und des Sozialistengesetzes zuzustimmen, belehrte Bismarck die Kaiserin, dass der Monarch „als solcher kein Faktor der Gesetzgebung“ sei.

Auch wenn Bismarck alles tat, um potenzielle Nachfolger auszuschalten, mehrten sich seit dem Ende der 1880er-Jahre doch die Anzeichen dafür, dass seine politische Führungsrolle sich dem Ende zuneigte. In der politischen Öffentlichkeit wurde der Ruf nach einer Abkehr von der nur bewahrenden Diplomatie Bismarcks zu Gunsten einer dynamischen und risikobereiten Außenpolitik laut. Nach der kurzen Herrschaftszeit von Friedrich III. standen sich mit dem neuen Kaiser Wilhelm II. und Bismarck zwei ungleiche Persönlichkeiten gegenüber. Bismarck hielt Wilhelm für unreif und wenig vorbereitet auf die Übernahme der Verantwortung. Er sei ein „Brausekopf, könne nicht schweigen, sei Schmeichlern zugänglich und könne Deutschland in einen Krieg stürzen, ohne es zu ahnen und zu wollen.“ Für Wilhelm dagegen war Bismarck eine nicht mehr zeitgemäße Person und er machte deutlich, selbst politischen Einfluss nehmen zu wollen: „Sechs Monate will ich den Alten verschnaufen lassen, dann regiere ich selbst.“

Bismarck sah vor diesem Hintergrund in der mutwilligen Verschärfung der innenpolitischen Lage eine Möglichkeit, den neuen Kaiser von seiner Unentbehrlichkeit zu überzeugen. Er brachte daher ein neues, verschärftes und unbefristetes Sozialistengesetz ein, wohl wissend, dass dies die Kartellparteien auseinandersprengen würde, da die Nationalliberalen dies nicht mittragen konnten. Wilhelm, der seine Regierungszeit nicht mit einem solchen Konfliktkurs beginnen wollte, stellte sich den Plänen des Kanzlers entgegen. In einer Sitzung des Kronrates prallten beide am 24. Januar 1890 aufeinander. In den folgenden Monaten versuchte Bismarck verzweifelt, seine Stellung zu halten und spielte erneut mit Staatsstreichgedanken, aber auch mit dem Plan einer engen Zusammenarbeit zwischen Zentrum und Konservativen.

Am 15. März 1890 entzog Kaiser Wilhelm dem Kanzler wegen dessen Konfliktkurses endgültig die Unterstützung. Das Entlassungsgesuch Bismarcks datiert vom 18. März 1890. Die Öffentlichkeit reagierte mehrheitlich erleichtert auf den Rücktritt. Theodor Fontane schrieb: „Es ist ein Glück, dass wir ihn los sind. Er war eigentlich nur noch Gewohnheitsregente (sic!), tat was er wollte, und forderte immer mehr Devotion. Seine Größe lag hinter ihm.“ Als Nachfolger Otto von Bismarcks wählte der Kaiser den politisch unerfahrenen General Leo von Caprivi.

Bismarck zog sich verbittert nach Friedrichsruh zurück, doch verabschiedete er sich damit nicht endgültig von der Politik. „Aber das kann man nicht von mir verlangen, dass ich, nachdem ich vierzig Jahre lang Politik getrieben, plötzlich mich gar nicht mehr damit abgeben soll.“ Seine Unnahbarkeit wurde durch diese Zurückgezogenheit noch gesteigert, sodass bald das Wort vom „Einsiedler im Sachsenwald“ die Runde machte. Bereits einen Tag nach seinem Rücktritt verkündete Bismarck, seine Memoiren verfassen zu wollen. Bismarck versuchte nicht nur, sein Bild für die Nachwelt mitzugestalten, sondern verzichtete auch nicht auf Eingriffe in die Tagespolitik. Bald nach seiner Entlassung begann er eine äußerst umtriebige Pressepolitik. Insbesondere die „Hamburger Nachrichten“ wurden zu seinem Sprachrohr. Bismarck attackierte vor allem seinen Nachfolger Caprivi scharf. Indirekt kritisierte er damit auch den Kaiser, dem er seine Entlassung nicht verziehen hatte. Am 30. April 1891 ließ sich Bismarck auf Initiative des jungen Diederich Hahn im Wahlkreis "Neuhaus (Oste), Hadeln, Lehe, Kehdingen, Jork" für den ausgeschiedenen Abgeordneten Hermann Gebhard in den Reichstag wählen. Wilhelm II. glaubte kurzzeitig sogar an eine Rückkehr des Altkanzlers in die Politik. Allerdings hat Bismarck seinen Wahlkreis nie betreten und von seinem Mandat niemals Gebrauch gemacht; Bei der Reichstagswahl 1893 verzichtete er zugunsten Diederich Hahns auf eine erneute Kandidatur. Die Pressepolitik in eigener Sache war durchaus erfolgreich. Die öffentliche Meinung wandte sich Bismarck verstärkt wieder zu, insbesondere nachdem Wilhelm II. begonnen hatte, ihn öffentlich anzugreifen. Für das Ansehen des neuen Reichskanzlers Caprivi geradezu katastrophal wirkte sich dessen Versuch aus, ein Treffen Bismarcks mit Kaiser Franz Joseph von Österreich zu verhindern. Die Reise nach Wien wurde zu einem Triumphzug des Altkanzlers, der erklärte, keine Verpflichtungen mehr gegenüber der deutschen Regierung zu haben: „Alle Brücken sind abgebrochen.“

Wilhelm II. bemühte sich in der Folge um eine öffentlichkeitswirksame Aussöhnungsgeste. Mehrere Treffen mit Bismarck im Jahr 1894 wurden positiv aufgenommen, eine wirkliche Entspannung brachte dies aber nicht. Wie gering Bismarcks Ansehen im Reichstag war, zeigte die gescheiterte Kampfabstimmung um ein Glückwunschtelegramm anlässlich seines achtzigsten Geburtstags. Daraufhin machten ihn etwa 400 deutsche Städte zum Ehrenbürger, darunter die Mitglieder der im Entstehen begriffenen "Städteverbände" in geschlossener Form, so der badische, der Thüringer und der sächsische. Im Jahr 1896 zog Bismarck durch die Offenlegung des streng geheimen Rückversicherungsvertrages noch einmal die Aufmerksamkeit der deutschen und internationalen Presse auf sich.

Die Erstellung der Memoiren unterstützte Lothar Bucher, ohne dessen Drängen das Werk wahrscheinlich nie fertiggestellt worden wäre. Bucher beklagte nicht nur Bismarcks rasch nachlassendes Interesse an seinen Memoiren, sondern beschrieb auch, wie der Altkanzler in ihnen Tatsachen absichtlich entstellte: „Bei nichts, was misslungen ist, will er beteiligt gewesen sein, und niemand lässt er neben sich gelten.“ Nach Buchers Tod im Oktober 1892 besserte Bismarck an den Manuskripten noch herum, aber das Werk wurde nicht mehr fortgesetzt.

Der Tod seiner Frau im Jahr 1894 traf Bismarck tief. Ab 1896 verschlechterte sich sein Gesundheitszustand immer deutlicher und er war schließlich auf einen Rollstuhl angewiesen. Die Erkrankungen an Altersbrand und anderen Gebrechen, die er gegenüber der Öffentlichkeit und sogar gegenüber seiner Familie verschwieg, führten am 30. Juli 1898 zu seinem Tod. Unmittelbar nach seinem Ableben entstand durch zwei Paparazzi die Fotografie von Bismarck auf dem Sterbebett.

Als Bismarck starb, befand sich Wilhelm II. im Zuge seiner Sommerreise in Norwegen auf der kaiserlichen Yacht "Hohenzollern". Nachdem ihn die Todesnachricht am Morgen des 31. Juli erreicht hatte, sandte er ein Telegramm an Herbert von Bismarck. Darin kündigte Wilhelm eine pompöse Beisetzung Bismarcks in der Hohenzollerngruft im Berliner Dom an, da Bismarck ein Freund seines Großvaters Wilhelm I. gewesen sei und ihm für seine Leistungen der Dank des deutschen Volkes für immer gebühre. Wilhelm II. beauftragte ebenfalls per Telegramm den Bildhauer Reinhold Begas, einen Sarkophag für Bismarck zu entwerfen; August zu Eulenburg sollte das Programm der Feier als nationales Ereignis gestalten. Bismarck hatte indes bereits 1896 in seinem Testament verfügt, er wolle in Friedrichsruh begraben werden. Seine Familie entsprach diesem Wunsch. Nun wollte Kaiser Wilhelm nach seinem Eintreffen in Kiel am 1. August wenigstens am offenen Sarg Bismarcks in Friedrichsruh stehen und begab sich mit seiner Gemahlin dorthin. Als er jedoch am folgenden Tag eintraf, war der Sarg bereits verlötet.

Bismarck fand demnach seine letzte Ruhestätte neben seiner Frau in einem Mausoleum in Friedrichsruh.

Für die Verhältnisse des 19. Jahrhunderts war der Verkaufserfolg der zunächst zweibändig von der Cotta’schen Verlagsbuchhandlung verlegten Erinnerungen sensationell: Die Erstauflage von mehr als dreihunderttausend Exemplaren war schon in den ersten Dezembertagen 1898 vergriffen, ab 1905 erschien sie dann als sogenannte „Volksausgabe“. Die Öffentlichkeit und Geschichtsforschung interessierende Auseinandersetzung mit Kaiser Wilhelm II. und die Entlassung des Reichskanzlers blieben dem dritten, erst 1921 erschienenen, Band vorbehalten.

Nach seiner Entlassung setzte in Deutschland eine beispiellose Bismarck-Verehrung ein, die sich nach dem Tod des Altkanzlers noch verstärkte. Seine Büste wurde in die Walhalla aufgenommen. Zahlreiche Straßen wurden nach ihm benannt. Auch Industrieunternehmen wie die Zeche Graf Bismarck trugen seinen Namen. Aus der Zechenkolonie des Unternehmens ging der Gelsenkirchener Stadtteil Bismarck hervor. Nach dem Reichsgründer wurden auch der Farbstoff Bismarckbraun Y, die Palmenart Bismarckia nobilis, eine Zubereitungsart von Heringsfilets (Bismarckhering) sowie während des Zweiten Weltkrieges das Typschiff der "Bismarck"-Klasse (Schlachtschiff "Bismarck") benannt. Vorher waren bereits die Kriegsschiffe SMS "Bismarck" (1877) und SMS "Fürst Bismarck" (1897) mit seinem Namen in Dienst gestellt worden. Auch einzelne Bäume erhielten seinen Namen ("Bismarcktanne").

Vor allem in den deutschen Kolonien in Afrika und im Stillen Ozean erhielten geografische Gegebenheiten oder Orte Bismarcks Namen (Bismarck-Archipel, Bismarckgebirge, Bismarckberge, Bismarckberg, Bismarck-Gletscher, Bismarcksee, Bismarck-Straße, Bismarckburg, Bismarckplatz beispielsweise in Daressalam, Deutsch-Ostafrika). Aber auch in den Vereinigten Staaten wurden mehrere Siedlungen nach Bismarck benannt. Darunter befand sich bereits seit 1873 die heutige Hauptstadt des Bundesstaates North Dakota.

In Deutschland entstanden Bismarckgesellschaften. Nach seinem Tod wurden in zahlreichen Städten größtenteils durch Spenden finanzierte Bismarckdenkmäler errichtet, vielfach in Form von Bismarcktürmen. Das erste zu Lebzeiten Bismarcks errichtete Standbild entstand im Jahr 1877 im Bad Kissinger Stadtteil Hausen, wo er seit 1874 mehrmals zur Kur weilte (siehe Bismarck-Denkmal (Bad Kissingen)). Das größte Bismarck-Standbild in Deutschland ist das 1906 eingeweihte Bismarckdenkmal in Hamburg. Der Bau eines gigantischen Bismarck-Nationaldenkmals bei Bingerbrück wurde durch den Ausbruch des Ersten Weltkriegs verhindert. Die meisten Bronzebildnisse zeigen Bismarck in Uniform. Diese Form der Darstellung überdeckte Bismarcks Maxime eines außenpolitischen Ausgleichs und spiegelte weniger Bismarcks Person als vielmehr den Zeitgeist der Wilhelminischen Ära wider.

Neben historisierenden Gemälden (z. B. von Franz von Lenbach) und eher privaten, alltäglichen Darstellungen (z. B. von Christian Wilhelm Allers) entstanden auch verklärende und überhöhende, die vor allem die Reichsgründung thematisierten. Auch in patriotischen Gedichten wie den "Bismarckliedern" von Paul Warncke (1895) und Wilhelm Berger wurde der Reichskanzler gefeiert.

Ebenfalls 1895, zu Bismarcks 80. Geburtstag, erschien das großformatige Buch "Unser Bismarck", das im Lauf der Zeit eine Auflage von 100.000 Exemplaren erreichte. Zu seinem 80. Geburtstag wurde er aus Progandazwecken zum Ehrenmitglied des Alldeutschen Verbandes.

In Friedrichsruh bestand seit 1927 ein von der Familie eingerichtetes Bismarck-Museum. Seit 1951 befindet es sich im "Alten Landhaus" (mit Einrichtungsgegenständen, Dokumenten, Gemälde "Proklamation des Deutschen Kaiserreiches" von Anton von Werner), gegenüber dem nach der Zerstörung des Schlosses im Zweiten Weltkrieg, neu errichteten Familiensitz und betreut auch das zugängliche Bismarck-Mausoleum.

Im alten Empfangsgebäude des Bahnhofs Friedrichsruh befindet sich die Otto-von-Bismarck-Stiftung, die 1996 von der Bundesrepublik Deutschland als eine von mittlerweile fünf Politikergedenkstiftungen eingerichtet wurde und dort eine Dauerausstellung zu Bismarck zeigt. Ihr Hauptziel ist die Erarbeitung einer neuen kritischen Ausgabe der Schriften Bismarcks. In Göttingen ist Bismarcks Studentenwohnung, das Bismarckhäuschen, als kleines Museum zugänglich. In Bismarcks Geburtsort Schönhausen erwarb „das deutsche Volk“ im Jahr 1885 von der Familie Gaertner das Gut Schönhausen II und schenkte es Bismarck zum 70. Geburtstag. In diesem, ehemals auch der Familie Bismarck gehörigen Rittergutshaus, wurde ein Bismarck-Museum errichtet, das bis 1948 bestand. 1998 wurde es wieder, mit Mitteln des Landes Sachsen-Anhalt, im erhalten gebliebenen Seitenflügel, dem so genannten Torhaus von Schloss Schönhausen I eingerichtet. Im selben Jahr entstand ein weiteres Bismarckmuseum in Bad Kissingen, wo Bismarck zwischen 1874 und 1893 insgesamt 15 Mal zur Kur geweilt hatte. Am 1. November 2004 wurde in Jever ein weiteres Bismarckmuseum eröffnet.

Mehr als 150 Jahre Bismarck-Rezeption haben eine Vielzahl von Deutungen seiner Persönlichkeit und seiner Handlungen hervorgebracht, die sich oft konträr gegenüberstehen. Bis nach dem Zweiten Weltkrieg überwog dabei in der deutschsprachigen Literatur die Neigung von Autoren, die Wertung von eigenen politischen und religiösen Standpunkten beeinflussen zu lassen. Die Historikerin Karina Urbach bilanzierte 1998: „Mindestens sechs Generationen ist sein Leben schon nahegebracht worden, und man kann abgewogenerweise sagen, dass fast jede zweite Generation in Deutschland einer weiteren Version Bismarcks begegnet ist. Keine andere deutsche politische Figur ist dermaßen für politische Zwecke benutzt und missbraucht worden.“

Kontrovers wurde Bismarck bereits zu Lebzeiten gesehen. Schon in den ersten biografischen Studien, einige davon mehrbändig, wurde die Komplexität und Undurchdringlichkeit von Bismarcks Persönlichkeit hervorgehoben. Der Soziologe Max Weber wertete 1895 in seiner Freiburger Antrittsrede Bismarcks Rolle im deutschen Einigungsprozess kritisch: „Denn dieses Lebenswerk hätte doch nicht nur zur äußeren, sondern auch zur inneren Einigung der Nation führen sollen und jeder von uns weiß: das ist nicht erreicht. Es konnte mit seinen Mitteln nicht erreicht werden.“ Theodor Fontane war, wie Hans-Jürgen Perrey schreibt, „voller Bewunderung für die historischen Leistungen und die historische Größe Otto von Bismarcks, um im selben Atemzuge ebenso schwerwiegende Vorbehalte zu äußern, wenn er auf den Menschen und dessen Charakter schaute.“ „Er ist die denkbar interessanteste Figur, ich kenne keine interessantere, aber dieser beständige Hang, die Menschen zu betrügen, dies vollendete Schlaubergertum ist mir eigentlich widerwärtig, und wenn ich aufrichten, erheben will, so muß ich doch auf andere Helden blicken“, schrieb Fontane am 5. August 1893 seinem Freund August von Heyden

Diese negativen Beurteilungen konnten sich auf Dauer nicht durchsetzen, nicht zuletzt wegen Bismarcks Memoiren, die den Bismarckverehrern neben einem fast unerschöpflichen Vorrat von Zitaten die Grundlagen für das Bild lieferten, das sich viele national gesinnte Deutsche von Bismarck machten; dies erschwerte einen kritischen Blick auf den Reichsgründer. Zu Lebzeiten nahm Bismarck außerdem persönlich Einfluss auf seine Darstellung in der Geschichtsschreibung, indem er den Zugriff von Historikern auf Dokumente regulierte und zum Teil Manuskripte Korrektur las. Nach seinem Tod übernahm der Sohn Herbert von Bismarck für einige Jahre diese Kontrolle über das Bismarck-Bild der Nachwelt.

Die professionelle Geschichtswissenschaft konnte sich vor dem Hintergrund der Reichseinigung der Faszination Bismarcks nicht entziehen und trug zur Idealisierung seiner Person bei. Heinrich von Treitschke wandelte sich von einem politischen Kritiker Bismarcks zu einem glühenden Bewunderer. Bismarcks Reichsgründung galt ihm als heroische Glanztat der deutschen Geschichte. Treitschke und andere Historiker der kleindeutsch-borussischen Schule der Geschichtsschreibung waren fasziniert von der strukturbrechenden Kraft Bismarcks. Der Bismarck-Biograf Erich Marcks schrieb 1909: „Und zu dem Glauben bekenne ich mich gern: dieses Dasein war so groß, in sich so gewaltig, für sein Volk so umfassend bedeutungsreich, daß an ihm alles, soweit es nur Leben hat, historisch wertvoll ist.“ Jedoch betonte Marcks, im Einvernehmen mit anderen Historikern der Wilhelminischen Ära wie Heinrich von Sybel, noch die Zweitrangigkeit der Rolle Bismarcks gegenüber den Leistungen der Hohenzollern. Nicht Bismarck, sondern Wilhelm I. wurde bis 1914 in Schulbüchern als Gründer des Deutschen Kaiserreichs dargestellt.
Der entscheidende Schritt zu einer extremen Überhöhung von Bismarcks Bild in der Historiografie wurde während des Ersten Weltkriegs vollzogen. Anlässlich des 100. Geburtstags von Bismarck 1915 entstanden Weiheschriften, die ihren rein propagandistischen Zweck kaum verhüllten. In patriotischem Überschwang betonten Historiker die Pflicht der deutschen Soldaten, die von Bismarck herbeigeführte Einheit und Größe Deutschlands gegen die anderen europäischen Mächte zu verteidigen, unterschlugen dabei aber Bismarcks beständige Warnungen gegen einen solchen Krieg in Mitteleuropa. Bismarck-Forscher wie Erich Marcks, Max Lenz und Horst Kohl zeichneten Bismarck vielmehr als geistige Leitfigur der deutschen Kriegsanstrengungen.

Die deutsche Niederlage im Krieg und der Wechsel zur Republik von Weimar brachten keinen grundsätzlichen Umschwung in diesem nationalistischen Bismarck-Bild, weil die Elite der Historikerzunft weiter der Monarchie verpflichtet blieb. In einer als demütigend und chaotisch empfundenen Lage Deutschlands wurde Bismarck als Orientierung gebende Vaterfigur porträtiert, an deren Genius angeknüpft werden müsse, um die „Schmach von Versailles“ zu überwinden. Sofern Kritik an seiner historischen Rolle geäußert wurde, bezog sie sich auf die „kleindeutsche“ Lösung der deutschen Frage, nicht auf die kriegerisch und „von oben“ herbeigeführte Einigung per se. Der Traditionalismus verhinderte, dass in dieser Zeit innovative Bismarck-Biografien erschienen. Immerhin ermöglichte die Freigabe weiterer Dokumente in den 1920er-Jahren neue Detailstudien, die Bismarcks diplomatisches Geschick hervorhoben. In einer zukunftsweisenden Monografie analysierte Otto Jöhlinger zudem 1921 erstmals Bismarcks Antisemitismus. Der Historiker betonte dabei, dass der Reichskanzler entsprechende Äußerungen hauptsächlich in reaktionären politischen Kreisen getätigt hatte, sein eigenes Verhalten gegenüber Juden aber von Pragmatismus geprägt war. Die populärste Bismarck-Biografie der Zeit legte 1926 der Schriftsteller Emil Ludwig mit einer kritischen psychologischen Studie vor, in der Bismarck als faustischer Held im Drama der Geschichte des 19. Jahrhunderts porträtiert wurde.

In der Zeit des Nationalsozialismus wurde häufiger eine historische Kontinuitätslinie zwischen Bismarck und Adolf Hitler behauptet, um so den nationalsozialistischen Staat als Vollendung der deutschen Einheitsbewegung (jedoch bei Korrektur der „kleindeutschen Lösung“) zu porträtieren. Erich Marcks, Nestor der Bismarck-Forschung, unterstützte diese ideologisierte Geschichtsdeutung. Auch in Großbritannien wurde Bismarck während des Zweiten Weltkriegs vermehrt als Vorgänger Hitlers gesehen, der Beginn der historiografischen Definition eines Deutschen Sonderwegs. Während des Zweiten Weltkriegs ließ die Berufung der Nationalsozialisten auf Bismarck jedoch nach; vor allem seine bekannten Warnungen vor einem Krieg Deutschlands gegen Russland waren ab 1941 nicht mehr zeitgemäß. Stattdessen erblickten nun konservative Mitglieder des Widerstands in Bismarck eine Leitfigur.

Im Jahr 1944 erschien Arnold Oskar Meyers "Bismarck der Mann und der Staatsmann", in dem Bismarck nationaldeutsch und völkisch gedeutet wurde. Mit diesem Werk erlangte die Bismarck-Verherrlichung in der Tradition des Kaiserreichs einen letzten Höhepunkt. Angesichts der Niederlage im Zweiten Weltkrieg und der Aufteilung Deutschland konnte Meyers überzogene politische Interpretation jedoch keinen größeren Einfluss auf die Bewertung der Rolle Bismarcks durch die Geschichtsschreibung mehr ausüben.

Eine wichtige kritische Stimme erhob der Jurist Erich Eyck mit seiner 1941–1944 im Schweizer Exil veröffentlichten dreibändigen Bismarck-Biografie. Er warf Bismarck machiavellistische Methoden und mangelnden Respekt vor dem Recht vor, verurteilte seinen Zynismus gegenüber demokratischen, liberalen und humanitären Werten und machte ihn für das Scheitern der Demokratie in Deutschland verantwortlich. Bismarcks Bündnissystem sei zwar mit Geschick erbaut worden, aber künstlich und von vornherein zum Scheitern verurteilt gewesen. Jedoch konnte auch Eyck sich der Faszination Bismarcks nicht entziehen: „Aber niemand, wo immer er steht, kann verkennen, daß er die zentrale und beherrschende Figur seiner Zeit ist und mit ungeheurer Kraft und tyrannischer Energie ihr die Wege gewiesen hat. Und niemand kann sich der faszinierenden Anziehungskraft dieses Menschen entziehen, der im guten wie im bösen immer eigenartig und immer bedeutend ist.“

Nach dem Zweiten Weltkrieg hielten einflussreiche deutsche Historiker wie Hans Rothfels und Theodor Schieder, wenn auch differenziert, an einem insgesamt positiven Bismarckbild fest. Viele deutsche Fachrezensionen der Eyck-Biografie, die erst in den 1950er-Jahren erschienen, waren entsprechend äußerst kritisch. Gerhard Ritter warf Eyck in einem Brief vor, lediglich antideutsche Klischees bestätigt zu haben. Demgegenüber argumentierte Friedrich Meinecke, selbst zuvor ein Bismarck-Bewunderer, 1946 in "Die deutsche Katastrophe", das traumatische Scheitern des deutschen Nationalstaates verhindere, Bismarck auf absehbare Zeit zu feiern.

Der Brite Alan J. P. Taylor veröffentlichte 1955 eine psychologisch gefärbte und nicht zuletzt deswegen umstrittene Bismarck-Biografie, in der er die komplexe Persönlichkeit seines Studienobjekts mit dem inneren Kampf zwischen väterlichem und mütterlichem Erbe zu erklären suchte. Taylor kontrastierte Bismarcks politischen Instinkt beim Ringen um eine Friedensordnung in Europa positiv mit der aggressiven deutschen Außenpolitik seit der Wilhelminischen Ära. Die erste deutsche Nachkriegsbiografie Bismarcks von Wilhelm Mommsen unterschied sich von Vorgängern vor allem durch den nüchternen, um eine objektive Perspektive bemühten Stil. Mommsen hob Bismarcks politische Flexibilität hervor und vertrat die Ansicht, dessen innenpolitische Fehler sollten nicht die Errungenschaften eines bedeutenden Staatsmannes überdecken.

In den 1960er- und 1970er-Jahren verlor der auf Biografien „großer Figuren“ zentrierte Ansatz in der westdeutschen Historikerzunft stark an Boden. Demgemäß waren nun nicht mehr Person und Handeln Bismarcks bevorzugtes Studienobjekt, sondern die politischen, sozialen und kulturellen Strukturen, in die er eingebunden war, die er aber selbst auch beeinflusste. In der sozialgeschichtlichen Schule um den bismarckkritischen Hans-Ulrich Wehler wurde unter anderem Bismarcks Praxis der Kampagnen gegen vermeintliche Staatsfeinde (Sozialdemokraten, Jesuiten etc.) problematisiert. In Form einer „negativen Integration“ habe das Schüren von Ängsten dem Reichskanzler dazu gedient, soziale Milieus an das neue Kaiserreich zu binden. Bismarck sei es zudem gelungen, ab 1878 mit einer „Sammlungspolitik“ die Interessen zweier einflussreicher Gruppen, nämlich der führenden Landbesitzer (Junker) und der Großindustriellen, in einer „Allianz gegen den Fortschritt“ zu verbinden. Wehler charakterisierte Bismarcks Herrschaftssystem 1973 als bonapartistische Diktatur. Dazu hätten charismatische, plebiszitäre und traditionelle Elemente gehört. Später versuchte Wehler, Bismarcks Stellung mit Hilfe von Max Webers Konzept der „charismatischen Herrschaft“ zu deuten.

Ende der 1970er-Jahre setzte eine Gegenbewegung zum Verzicht der Sozialhistoriker auf biografische Studien ein. Seitdem sind in regelmäßigem Abstand neue Bismarck-Biografien erschienen, die zumeist ein differenziertes Bild des ersten Reichskanzlers jenseits einer überspitzten Überhöhung oder Dämonisierung zeichnen. Den meisten neueren Biografien ist gemeinsam, dass sie im Versuch einer Synthese zwar die Wirkungsmacht Bismarcks betonen, dessen Person jedoch eingebettet in die zeitgenössischen Strukturen und politischen Prozesse zeigen.

Einen ungewöhnlichen Weg ging dabei Fritz Stern, der 1978 eine Doppelbiografie Bismarcks und seines Bankiers Gerson von Bleichröder vorlegte. Lothar Gall zeichnete 1980, einen von Ludwig Bamberger und Henry Kissinger verwendeten Begriff aufnehmend, das Bild eines „weißen Revolutionärs“. Bismarck war danach ein Erzroyalist, der die konservativen Strukturen bewahren wollte, stürzte zu diesem Zweck aber auch bestehende Ordnungen um und hat modernisierend gewirkt. Am Ende habe er aber die Kräfte, die er gerufen hatte, nicht mehr beherrschen können und bemühte sich um das Zurückdrängen moderner Tendenzen.

Der US-amerikanische Historiker Otto Pflanze legte zwischen 1963 und 1990 eine mehrbändige Biografie Bismarcks vor, die im Unterschied zu anderen Werken weniger Bismarcks Handeln als vielmehr seine Persönlichkeit in den Vordergrund stellte und diese teils mit psychoanalytischen Methoden untersuchte. Pflanze kritisierte Bismarck dafür, die Reichsverfassung und den Umgang mit den Parteien ganz seinen unmittelbaren politischen Zwecken angepasst und dadurch ein wirkungsmächtiges negatives Exempel gesetzt zu haben. Nach Pflanze geht die Darstellung als Einiger der deutschen Nation auf Bismarcks späte Selbststilisierung zurück, derweil er ursprünglich nur den Einfluss Preußens im Konzert der europäischen Mächte habe stärken wollen.

Der DDR-Historiker Ernst Engelberg brachte 1985 den ersten Band seiner Bismarck-Biografie heraus, die in Ost und West auf Verwunderung stieß, weil sie eher liebevoll und, abgesehen von der Sozialistenverfolgung, wenig kritisch mit dem Kanzler umging. Engelberg sah, durchaus in Einvernehmen mit anderen marxistisch-leninistischen Historikern der Zeit, die Reichsgründung als Phase des Fortschritts an, die der Arbeiterklasse einen nationalen Zusammenschluss ermöglicht habe. Engelberg betrachtete Bismarck selbst nicht als Abenteurer, sondern als überlegt handelnden Politiker, dessen Charakterfehler ihm nicht persönlich anzulasten, vielmehr aus seinen sozialen Wurzeln im Junkertum heraus erklärbar seien. Der Erste Weltkrieg sei nicht Bismarcks Erbe, sondern die Schuld seiner Nachfolger gewesen.







</doc>
<doc id="12677" url="https://de.wikipedia.org/wiki?curid=12677" title="Paläolinguistik">
Paläolinguistik

Die Paläolinguistik beschäftigt sich als Randgebiet der Sprachwissenschaft mit dem Sprachursprung und der Weiterentwicklung der Sprache bis zum Beginn der historischen Überlieferung. Helmut Glück bezeichnet sie als „linguistische Vorgeschichtsforschung“. Der Begriff "Paläolinguistik" taucht bereits 1968 auf.

Die Frage danach, wann und wie Sprache erstmals entstanden ist, beschäftigt die Menschen in Europa seit der griechischen Antike. Arens (1969: 18f.) geht auf dieses Thema mehrfach ein, beginnend mit Hinweisen auf Plato, Epikur und Diodor. Aus der Sicht der deutschen Geistesgeschichte kann auch auf Herders Überlegungen hierzu hingewiesen werden (Arens 1969: 123ff.).

Zu den Autoren, die sich mit der Sprachentwicklung vor Einsetzen der historischen Überlieferung befasst haben, gehört August Schleicher, der im Jahr 1853 seine "Stammbaumtheorie" veröffentlichte. Aufgegriffen wurde sie von Nikolai Jakowlewitsch Marr in seiner Japhetitentheorie. Ein neuer Vertreter der Paläolinguistik ist Richard Fester. Seit 1995 erscheint die Zeitschrift "Mother Tongue", die sich mit der Vorgeschichte der Sprachen befasst.

Die methodischen Probleme der Paläolinguistik liegen auf der Hand: Wenn man über Sprachursprung und -entwicklung in der Zeit, bevor Schriftzeugnisse auftauchen, etwas sagen will, haben solche Versuche einen hypothetischen Charakter. 

Die historische Linguistik hat Wege gefunden, aus den bezeugten ältesten Sprachzuständen auf frühere Sprachzustände zu schließen; das Verfahren heißt Rekonstruktion und ist Bestandteil der etymologischen Forschung. Es erlaubt einigermaßen sichere Rückschlüsse auf die Ursprachen einzelner Sprachfamilien, zum Beispiel die indogermanische Ursprache. Rekonstruktionen sind jedoch höchstens über einen Zeitraum von einigen tausend Jahren möglich. Noch frühere Sprachzustände oder gar der Sprachursprung sind damit nicht zugänglich und müssen Gegenstand von Spekulation bleiben.

Umstritten ist, ob die bislang erschlossenen Ursprachen der verschiedenen Sprachfamilien ihrerseits wiederum auf gemeinsame Ursprünge zurückgeführt werden können. Ruhlen (1994) und der Archäologe Renfrew (1995) glauben, diesen Schritt gehen zu können. Sie nehmen an, dass sich alle Sprachen aus einer einzigen Ursprache entwickelt haben (Monoglottogenese oder Monogenese). Zur Unterstützung ihrer Hypothese berufen sie sich zusätzlich auf Erkenntnisse der Humangenetik (Cavalli-Sforza u. a. 1988). Auf eine gemeinsame Ursprache zumindest Europas deuten die nicht unwidersprochen gebliebenen Untersuchungen zu Gewässer- und Ortsnamen hin (Hamel & Vennemann 2002). Die deutliche Mehrheit der historischen Linguisten sieht sich jedoch nicht in der Lage, diesen Argumenten zu folgen, weil sie der Überzeugung sind, dass mit den bekannten Methoden solch weitreichende Schlüsse nicht möglich sind.




</doc>
<doc id="12678" url="https://de.wikipedia.org/wiki?curid=12678" title="Richard Fester (Linguist)">
Richard Fester (Linguist)

Richard Fester (* 1910 in Berlin; † 1982) war ein deutscher Autor und Paläolinguist. In seinem bekanntesten Werk, "Sprache der Eiszeit" (erschienen 1962), stellte er die These auf, dass alle Sprachen der Welt einen einmaligen gemeinsamen Ursprung besitzen, dessen Urwortschatz er glaubte rekonstruieren zu können. Jedoch war Fester in Fragen der Vorgeschichte, der Sprachwissenschaft oder gar Paläoklimatologie kaum bewandert, und seine umfangreichen Wortgleichungen sind durch keinerlei Lautgesetze gestützt.

Aus einem Vergleich von gut 100 verschiedenen Sprachen stellte Fester die Hypothese auf, dass sechs Urformen, die er „Archetypen“ nannte („ba“, „kall“, „tal“, „tag“, „os“ und „acq“), sechs Lebenssituationen entsprechen, und die Basis aller Sprachen bilden.

Er schließt dabei methodisch an die japhetitologische Vierelementenanalyse des sowjetischen Linguisten Nikolai Jakowlewitsch Marr an, der von den vier Ursilben "sal, ber, yon" und "rosch" ausgeht, die bei der Arbeit ausgestoßenen Urlauten entsprechen.

Fester zog aus seinen Untersuchungen weitergehende Schlüsse, die nicht nur linguistischer Natur sind.

Der erste der von ihm postulierten Urlaute, „ba“, steht stellvertretend für die Bildung einer einfachen Silbe aus einem mit Hilfe der Lippen geformten Konsonanten (dazu gehören m, b, p, f und w) und einem offenen Vokal (Mundstellungen von a bis o) und entspricht der seiner Meinung nach einfachsten Möglichkeit, mit unserem Sprechapparat eine Silbe zu bilden. Insofern könnte das erste Wort in der Tat so ähnlich wie „ba“ geklungen haben. Fester stellte Listen mit Wörtern zusammen, die sich aus dieser Lautkombination entwickelt haben könnten. Dabei sind diese im Laufe der Entwicklung oft in komplexere Strukturen eingebettet worden, wobei sie sich auch veränderten. Bei seiner Auflistung, die Wörter wie „bau-en“, „Feu-er“, „Mama“ und „Papa“ enthält, kam er zu dem Schluss, dass das Urwort „ba“ zunächst Dinge bezeichnete, die ausschließlich mit dem Menschen und seinem Umfeld, also mit menschlichen Beziehungen und Daseinsfragen zu tun haben.

Die Grundbedeutung des Urwortes „kall“ (mit Zunge und Gaumen erzeugter Konsonant (g, k) in Verbindung mit einem Vokal und einem Konsonanten wie l, r, m oder n) bezeichnet eine Wölbung, Vertiefung oder einen „umschlossenen Hohlraum“, so dass sich Wörter wie „Halle“, „Zelle“ und „Quelle“ oder deren Umkehrungen „Loch“ und „Lache“ bildeten. Mit den Abkömmlingen des Urwortes „kall“ wird eine Art Gefäß umschrieben, aus dem etwas entspringen kann. Dass sehr viele weitere Ableitungen („Gyn“, „Girl“, „Queen“) die Bedeutung „Frau“ haben, wird damit erklärt, dass die Frau das „Gefäß des Lebens“ ist. Die große Häufigkeit von Wörtern mit diesem Stamm (gegenüber den anderen Urwörtern) ließ für Fester nur einen Schluss zu: Er deutete diese als Beweis für ein Ur-Matriarchat, eine umstrittene These, die in dem Buch "Weib und Macht" gemeinsam mit anderen Autoren diskutiert wurde. Er stellt darin u. a. fest: „Wenn man sich die Zeit des Menschen auf dieser Erde mit 2000 Jahren vorstellt, dann gibt es Männerherrschaft erst seit einem Jahr. Und wenn man das grafisch darstellt, und dazu eine gerade Linie von zwei Metern Länge darstellt, dann ist der letzte Abschnitt, der männerrechtliche nur einen Millimeter lang.“

Die übrigen vier Urlaute bedeuten laut Fester:
„tal“ = Einschnitt oder Spalte im Boden oder im Körper, die Erde, unten
„tag“ = Aufrechter Mensch, Götter, hoch
„os“ = Öffnung
„acq“ = Wasser

In etwa vergleichbarer Weise schrieb der Jurist und Sprachwissenschaftler Arnold Wadler 1935 in der Schweiz das Buch "Der Turm von Babel – die Urgemeinschaft der Sprachen" (1948 in englischer Übersetzung erschienen als "One Language – Sources of all tongues"), in dem er zu recht ähnlichen Ergebnissen kommt wie Fester. Der amerikanische Autor J. P. Cohane, der Festers Bücher offenbar nicht kannte, kam in seinem Buch "The Key" (Crown, New York) 1969 ebenfalls zu einer Rekonstruktion von 6 Urwörtern, die er wie Fester vor allem aus geographischen Begriffen erschloss, und die insbesondere eine religiöse Bedeutung gehabt haben sollen. Cohanes Urwörter lauten: „Oc“ der „Og“ wie in Okeanos, Kronos, Moloch und dem altirischen Gott Oc; „Hawwah“ wie in "Aloha", "Yahweh", "Aqua" und "Erde"; „Mana“; „Ash“ oder „Az“; „Tema“ wie in Thames, Tiamat und Athena; sowie „Eber“ oder „Abar“ wie in "Berber", "Hibernia", "Kalabrien", "Abruzzen", "Hebräisch", "Ares" und "Mars". 

Innerhalb der Sprachwissenschaft werden diese Versuche als wissenschaftlich nicht fundiert abgelehnt. In Ermangelung von Lautgesetzen ist Fester nie in der Lage, Lehnwörter und zufällige Wortähnlichkeiten (z. B. "Deus" im Lateinischen und "Theos" im Griechischen) in völlig unverwandten Sprachen von vielleicht möglichen Urverwandtschaften überzeugend zu unterscheiden.

Auch in der modernen vergleichenden Sprachwissenschaft haben einige renommierte Wissenschaftler (wie z. B. Joseph Greenberg und Merritt Ruhlen) den (ebenfalls nicht unumstrittenen) Versuch unternommen, mit der von Greenberg entwickelten Methode der Mass Lexical Comparison tatsächlich eine Ursprache (Proto-World) und deren Nachfolgesprachen (z. B. Nostratisch und andere so genannte Makrofamilien) zu rekonstruieren. Dabei werden, wie von Fester, auch mögliche Fehlgleichsetzungen ("false cognates") nicht von vornherein von dem Vergleich ausgeschlossen, jedoch nicht aus Unkenntnis, sondern aus bestimmten methodischen Gründen. In dem derart rekonstruierten Grundwortschatz von Proto-World finden sich eine Reihe Parallelen zu Festers Archetypen, z. B. „*aya“ (Mutter, Vater, Großmutter) könnte Festers „ba“ entsprechen, „*k’olo“ (Loch, Grube, Aushöhlung) entspricht offensichtlich Festers „kall“, „*tika“ (Erde, Mensch) entspricht Festers „tag“, „*'ag'wa“ (Wasser) entspricht zweifellos Festers „acq“.
Es scheint daher, dass sich manche moderne Sprachforscher teilweise den unwissenschaftlich entstandenen Thesen Festers annähern.

Die Rekonstruktion einer Ursprache setzt die Annahme einer monogenetischen Entstehung der menschlichen Sprache voraus (vergl. Monoglottogenese), die auch Fester vertreten hat. Eine solche einmalige Entstehung der Sprache vor maximal 200.000 Jahren wird auch durch neuere Ergebnisse der Anthropologie (Out-of-Africa-Theorie) und der Humangenetik, wie z. B. die Entdeckung des „Sprach-Gens“ FOXP2 (siehe Svante Pääbo) nahegelegt. Festers Hypothese, dass schon frühe Urmenschen-Arten lange vor "Homo sapiens" die Fähigkeit zur Sprache besaßen, ist aus heutiger Sicht eher unwahrscheinlich, da hierzu das Vorhandensein der anatomischen und physiologischen Voraussetzungen des Kehlkopfapparates offenbar nicht allein ausreichend ist. Die Mutation zur Sprachfähigkeit scheint der letzte große Schritt in der Evolution innerhalb unserer Art gewesen zu sein und auch zu erklären, warum der moderne Mensch vor etwa 60.000–100.000 Jahren und damit lange nach seiner Entstehung plötzlich einen explosiven Kulturschub erfährt und sich erst dann über die Kulturstufe des "Homo erectus" und des Neandertalers erhebt und über die ganze Welt ausbreitet.

Im Rahmen seiner Gesamtvergleiche meinte Fester auch Wörter der amerikanischen Indianer mit finnischen Wörtern (auch in einigen Ortsnamenbezeichnungen) auf gemeinsame Ursprünge zurückführen zu können. Vermutlich aus Ahnungslosigkeit von der einst bestehenden Landbrücke nach Amerika und darüber hinaus vom Zusammenhang der uralischen Sprachen verstieg er sich in die Annahme einer Besiedlung über den Nordatlantik. Er dachte an eine Eisbrücke, die während der letzten Eiszeit bestanden habe und über die eiszeitliche Jäger vordrangen, als sie Robben nachstellten. Skier sind schon auf alten Felszeichnungen zu sehen, und mit solchen, argumentiert Fester, wäre die Strecke durch Kundschafter zu bewältigen gewesen, denen dann später ihre Sippen folgten. Fester postuliert als Erklärung für eine solche „Weiße Brücke“ eine südlichere Lage des Nordpols (beim Südende Grönlands) während des Magdalénien. Diese These, die in fast identischer Form 1958 von dem Amerikaner Charles Hapgood formuliert wurde, ist wissenschaftlich jedoch zweifelsfrei widerlegt und widerspricht allen bekannten Fakten (dazu zählt auch der von Fester unterstützte Mythos der angeblich „schockgefrosteten“ Mammuts). 

Ergebnisse der modernen Genetik (Beispiele über die diversen Seiten des "Atlas of the human journey") und Linguistik (Joseph Greenberg und andere) legen nahe, dass Amerika in drei sprachlichen Einwanderungswellen (daher drei große amerikanische Sprachfamilien: Amerindisch, Eskimo-Aleutisch und Na-Dené) über die Beringstraße besiedelt wurde.





</doc>
<doc id="12684" url="https://de.wikipedia.org/wiki?curid=12684" title="Clemens Winkler">
Clemens Winkler

Clemens Alexander Winkler (* 26. Dezember 1838 in Freiberg; † 8. Oktober 1904 in Dresden) war ein deutscher Chemiker. Er ist der Entdecker des chemischen Elements Germanium.

Clemens Winkler kam als zweitältester Sohn von drei Söhnen und drei Töchtern des Metallurgen Kurt Alexander Winkler und seiner Frau Elmonde Winkler, geb. Schramm, in Freiberg zu Welt. Er war der Neffe des Mineralogen August Breithaupt und Cousin des Geologen Hermann Theodor Breithaupt. Sein Pate war der Chemiker Ferdinand Reich. Der Chemiker Ferdinand Bischoff war sein Schwager.

Nachdem er zunächst mit seinen Geschwistern Privatunterricht erhielt, kam er mit 12 Jahren auf das Gymnasium in Freiberg, von welchem er auf die Realschule nach Dresden und nach zwei Jahren auf die Gewerbeschule in Chemnitz wechselte. Nach seinen Aufenthalt an der Chemnitzer Gewerbeschule (1853–1856) begann er das Studium an der Freiberger Bergakademie (1857–1859), brach dieses aber nach zwei Jahren auf Wunsch seines kranken Vaters ab. Er arbeitete zunächst als Chemiker in den Blaufarbenfabriken Oberschlema und, nach dem Tod des Vaters im Jahr 1862, in Niederpfannenstiel. In Pfannenstiel gründete er mit Minna Pohl danach eine Familie, aus der vier Söhne und zwei Töchter hervorgingen. In den folgenden 2 Jahren veröffentlichte er neun Arbeiten und wurde 1864 für seine Arbeit "Ueber Siliciumlegirungen und Siliciumarsenmetalle" an der Universität Leipzig promoviert; im gleichen Jahr wurde er Hüttenmeister im Blaufarbenwerk Niederpfannenstiel.
Dort hatte Winkler auch Zeit für eigene Versuche. Er untersuchte die Reaktionen des Elementes Indium – das sein früherer Lehrer Ferdinand Reich entdeckt hatte – bestimmte das Atomgewicht, ermittelte die Zweiwertigkeit und stellte eine Vielzahl von Salzen Indiums dar. Er bestimmte die Atomgewichte von Nickel und Cobalt.

Auf Empfehlung Hermann Kolbes wurde Clemens Winkler im Jahr 1873 der Nachfolger von Theodor Scheerer als Professor für anorganische Chemie an die Bergakademie Freiberg berufen und dass, obwohl er Hüttenmeister war und bis dahin keine akademische Lehrtätigkeit innehatte. Seine ersten Arbeiten handelten von der Mineralanalyse, für die Gewichtsanalyse von Metallarten wendete Winkler die Elektroanalyse an. Winkler führte 1898 die Drahtnetzelektrode aus Platin ein und gilt als Mitbegründer der Elektroanalyse.

Bereits 1831 hatte in England Peregrine Phillips ein Patent zur Erzeugung von Schwefelsäure aus Schwefeldioxid durch Platinkontakt. Winkler entwickelte zu diesem Zweck platinierten Asbest und erhielt eine sehr hohe Ausbeute an Schwefelsäure. 1876 entstand eine Schwefelsäurefabrik bei Freiberg. Winkler war ein Mitbegründer des Kontaktverfahrens und erkannte Arsen als Kontaktgift, das die Wirkung des Katalysators verschlechterte.
Später entwickelte der BASF-Chemiker Rudolf Knietsch das Verfahren weiter, es fand weltweite Anerkennung.

Winkler verbesserte die gasanalytischen Methoden von Robert Bunsen und vereinfachte sie so weit, dass sie weite Anwendung in der Industrie fanden; er entwickelte die nach ihm benannte Gasbürette und gilt gemeinsam mit dem Chemiker Walther Hempel als ein Begründer der technischen Gasanalyse.

Die bedeutendste Leistung Winklers war die Entdeckung des Elementes Germanium am 6. Februar 1886. Bei der Analyse des seltenen Minerals Argyrodit fand er heraus, dass dieses zu ca. 75 Prozent aus Silber, zu 17 Prozent aus Schwefel und zu geringen Anteilen (insgesamt ca. 1 Prozent) aus Eisen, Quecksilber und Zink bestand. Nun fehlten noch sieben Prozent zum Ganzen. Nach mehrmonatiger Arbeit konnte Winkler schließlich ein neues Element mithilfe des Freiberger Aufschlusses isolieren, das er – in Anlehnung der Namensgebung der Elemente Gallium und Scandium – als "Germanium" bezeichnete. Seine Entdeckung bestätigte die theoretische Vorarbeit von Dmitri Iwanowitsch Mendelejew, der die Existenz eines Elementes, das er Eka-Silicium nannte, mit diesen Eigenschaften vorausgesagt hatte.

Im Jahre 1894 traf er sich erstmals mit Mendelejew, mit dem er seit 1886 Briefkontakte pflegte. Von 1896 bis 1899 wirkte Winkler als Direktor der Bergakademie; Berufungen an andere Universitäten lehnte er ab. Er starb im Jahr 1904 im Alter von 65 Jahren an Krebs.

Clemens Winkler war Angehöriger des Weinheimer Corps Franconia Freiberg. Er ist auf dem Trinitatisfriedhof in Dresden begraben.


Das Werk Clemens Winklers umfasst 141 wissenschaftliche Veröffentlichungen. Die folgende Liste stellt eine Auswahl seiner Publikationen dar.





</doc>
<doc id="12687" url="https://de.wikipedia.org/wiki?curid=12687" title="Elektromotor">
Elektromotor

Ein Elektromotor ist ein elektromechanischer Wandler (elektrische Maschine), der elektrische Leistung in mechanische Leistung umwandelt. In herkömmlichen Elektromotoren erzeugen stromdurchflossene Leiterspulen Magnetfelder, deren gegenseitige Anziehungs- und Abstoßungskräfte in Bewegung umgesetzt werden. Damit ist der Elektromotor das Gegenstück zum sehr ähnlich aufgebauten Generator, der Bewegungsleistung in elektrische Leistung umwandelt. Elektromotoren erzeugen meist rotierende Bewegungen, sie können aber auch für translatorische Bewegungen gebaut sein (Linearantrieb). Elektromotoren werden zum Antrieb vieler Gerätschaften, Arbeitsmaschinen und Fahrzeuge eingesetzt.

1820 entdeckte der dänische Physiker und Philosoph Hans Christian Ørsted die magnetische Wirkung des elektrischen Stroms, ein grundlegendes Phänomen des Elektromagnetismus. Ein Jahr später veröffentlichte Michael Faraday seine Arbeitsergebnisse über „elektromagnetische Rotation“. Er konstruierte eine Vorrichtung, bei der ein elektrischer Leiter um einen festen Magneten rotierte und im Gegenexperiment ein beweglicher Magnet um einen festen Leiter. 1822 entwickelte Peter Barlow das nach ihm benannte Barlow-Rad. Der britische Wissenschaftler William Sturgeon erfand 1832 einen weiteren Motorvorläufer. Auf dem europäischen Kontinent wirkten Ányos Jedlik (1827) und Hermann Jacobi an der Weiterentwicklung des Gleichstrom-Elektromotors. So entwickelte Jacobi bereits 1834 den ersten praxistauglichen Elektromotor in Potsdam und stattete 1838 in Sankt Petersburg ein sechs Personen fassendes Boot mit dem von ihm entwickelten 220 Watt starken Motor aus, was somit zugleich die erste Anwendung eines Elektromotors in der Praxis darstellte. Auch der US-amerikanische Grobschmied Thomas Davenport entwickelte in Vermont einen Kommutatormotor. Auf sein Design wurde ihm am 25. Februar 1837 ein Patent erteilt.

Damit war um 1837/1838 die Grundlage für einen elektromotorischen Antrieb bekannt und auch bis zur anwendungstauglichen Arbeitsmaschine entwickelt. Werner von Siemens ließ im Jahre 1866 seine Dynamomaschine patentieren. Sie ermöglichte erstmals eine Erzeugung elektrischer Energie in größerem Umfang. Dies verhalf dem Elektromotor zum Durchbruch für eine praxistaugliche weitverbreiteten Anwendung. Daneben gab es zu jener Zeit auch einige technische Entwicklungen von andersartigen Elektromotoren, welche aber letzten Endes keine Bedeutung erlangten. Dazu zählt unter anderem der Egger-Elektromotor, welcher ähnlich wie eine Dampfmaschine aufgebaut ist, und das elektrische Kraftrad von Johann Kravogl.

Ab etwa 1880 wurden in vielen Staaten Elektronetze und Kraftwerke aufgebaut. In Deutschland war beispielsweise Emil Rathenau mit seiner Allgemeinen Electricitäts-Gesellschaft Vorreiter und in Amerika Thomas Alva Edison. Mit der großflächigen Bereitstellung von elektrischer Energie breitete sich der Elektromotor dann schnell aus. Gemeinsam mit der Chemischen Industrie war diese Elektrifizierung das wichtigste Merkmal der zweiten industriellen Revolution. Die öffentlichen Pferdebahnen wurden durch elektrische Straßenbahnen ersetzt, und im Gewerbe verwendete man nun Elektromotoren anstatt der Dampfmaschine zum Antrieb verschiedenster Arbeitsmaschinen.

Die Drehbewegung eines Elektromotors beruht auf den Anziehungs- und Abstoßungskräften, die mehrere Magnetfelder aufeinander ausüben (Lorentzkraft). Im üblichen Elektromotor gibt es einen feststehenden Außenteil sowie einen sich darin drehenden Innenteil. Entweder besitzt einer davon Permanentmagneten und der andere elektrische Spulen, oder beide Komponenten besitzen Spulen. Jede stromdurchflossene Spule erzeugt ein Magnetfeld, dessen Ausrichtung (Nordpol/Südpol) abhängig von der Stromrichtung ist - fließt der Strom in entgegengesetzter Richtung durch die Spule, so wird auch das Magnetfeld umgedreht. Durch mehrfaches, passendes Umpolen der Spulen während eines Umlaufs wird eine kontinuierliche Drehung des Innenteils erreicht.

Der feststehende, magnetisch wirkende Teil eines Elektromotors wird Stator genannt. Bei Elektromotoren liegt der Stator meistens außen und ist mit dem Gehäuse verbunden; liegt der Stator innen, so nennt man den Motor „Außenläufer“.
Der sich bewegende (meistens: rotierende), magnetisch wirkende Teil eines Elektromotors, der die Motorachse dreht. Er besteht aus der Achse, dem Anker und einer Spule, wenn der Anker kein Permanentmagnet ist.
Eisenkern des Rotors, um den die Rotorspule(n) gewickelt ist/sind.
Schuhform-ähnliche Ausbuchtung des Eisens eines Magnetkerns, die das Magnetfeld an diese Stelle leiten/bündeln soll.

Eine Scheibe mit elektrischen Anschlüssen, die Segmente der Scheibe sind; die Scheibe dreht sich mit der Rotorwelle. An den Anschlüssen sind die Spulen angeschlossen; die Kommutatorscheibe polt während eines Umlaufs die Spulen um. Die genaue Funktionsweise ist im nachfolgenden Abschnitt erklärt.

Der (feststehende) Stator kann bei einem Gleichstrommotor ein Dauermagnet mit Polschuhen sein, jedoch ist auch eine Fremderregung über eine Erregerspule anstatt des Dauermagneten möglich. Bei einem Wechselstrom-Kommutatormotor oder auch Universalmotor befindet sich im Stator hingegen immer eine Erregerspule. Wird Strom durch diese Spule geleitet, baut sich das Erregerfeld (Magnetfeld) auf (Ørsted-Prinzip).

Im Inneren des Stators ist ein Rotor, der in den meisten Fällen aus einer Spule mit Eisenkern (dem sogenannten Anker) besteht, der drehbar im Magnetfeld zwischen den Polschuhen des Stators gelagert ist.

Die Stromzuführung für den Anker erfolgt über einen segmentierten Kommutator und Schleifkontakte (Kohlebürsten). Schickt man durch den Rotor Strom, entsteht auch hier ein Magnetfeld, das jetzt in Wechselwirkung mit dem Magnetfeld des Stators tritt. Er dreht sich somit um seine eigene Achse und schaltet über den sich mitdrehenden Kommutator immer die passenden Wicklungen in den Stromweg und kann so elektrische Arbeit in mechanische Arbeit umwandeln.

Hätte ein solcher Motor keinen Kommutator, würde sich der Anker so weit drehen, bis das Rotormagnetfeld zum Statorfeld gleichgerichtet ist. Damit er an diesem „toten Punkt“ nicht stehen bleibt, wird der Strom in den Ankerspulen mit Hilfe des Kommutators (auch Stromwender oder Kollektor genannt) bei jedem neuen Segment umgeschaltet. Der Kommutator besteht aus Metallsegmenten, die eine durch schmale Streifen nichtleitenden Materials (Kunststoff, Luft) unterbrochene Zylinder- oder Kreisfläche bilden. An den Segmenten sind die Ankerwicklungen angeschlossen. Am Kommutator liegen, durch Federn angedrückt, meist zwei Kohlebürsten an, die den Strom zuführen. Mit jeder Drehung des Rotors wird die Stromrichtung durch die Ankerwicklungen geändert und es gelangen diejenigen Leiter in das Magnetfeld des Stators, deren Stromfluss so gerichtet ist, dass ein Drehmoment erzeugt wird.

Das Magnetfeld im Rotor steht – relativ zum Stator – fest; der Eisenkern des sich drehenden Ankers muss daher zur Vermeidung von Wirbelströmen aus einem Blechstapel bestehen.

Nach diesem Prinzip können auch Wechselstrommotoren gebaut werden, wenn das Erregerfeld mit dem Wechselstrom ebenfalls seine Polung ändert (Universalmotor). Dann muss auch der Stator aus einem Blechpaket bestehen.

Bei Wechselstrom kann auch auf einen Kommutator verzichtet werden, wenn die Umdrehungszahl im Rhythmus des Wechselstromes erfolgt; das dann mit umlaufende Magnetfeld des Rotors wird dann erzeugt:


Solche Motoren besitzen daher kein oder ein geringes Anlaufmoment. Sie benötigen eine Anlaufhilfe, können jedoch mit Wechselstrom mit mehr als nur einer Phase auch selbst starten:




Es gibt einige Arten von Elektromotoren, die heute keine wirtschaftliche Bedeutung haben.

Elektrostatische Motoren verwenden statt Magnetfelder elektrische Felder, die von Ladungen erzeugt werden. Aufgrund der hohen benötigten Spannungen und einem niedrigeren Wirkungsgrad sind diese Motoren jedoch nur für kleine Kräfte und Maßstäbe relevant.

Elektromotoren kommen sowohl ungeregelt als auch geregelt zum Einsatz. In einfachen Fällen kommen ungeregelte Drehstrommotoren mit Stern-Dreieck-Umschaltungen zur Anwendung. Diese sind jedoch nur zur Lösung primitiver Antriebsaufgaben geeignet. In den meisten Fällen in der heutigen Praxis liegen anspruchsvollere Antriebsprobleme vor, sodass die Elektromotoren durch eine Regelung geregelt werden müssen. Handelt es sich dabei um größere Leistungen, die erforderlich sind, so müssen noch leistungselektronische Stellglieder zwischen Regelung und Elektromotor dazwischengeschaltet werden. Kommen Regelung und Elektromotor zusammen und bilden sie gemeinsam eine funktionelle Einheit, so spricht man vom „Elektroantrieb“. Per se ist also ein Elektromotor nicht an eine Regelung gebunden; in vielen praktischen Fällen hat sich jedoch gerade deren Zusammenwirken als zweckmäßig erwiesen.

In der Vergangenheit fanden Elektromotoren zunächst praktische Verwendung als Antrieb von Straßenbahnen und etwas später als Universalantrieb zur Ersetzung von Dampfmaschinen in Fabriken und wurden zu diesem Zweck über Riementriebe zum Antreiben mechanischer Webstühle und dergleichen eingesetzt. Mit der Einführung von Fließbändern in der Industrie wurden Elektromotoren dann zum Antriebsmittel ganzer Industriezweige schlechthin.

Im Bereich Verkehr und Mobilität kamen Elektromotoren erstmals bei Elektrolokomotiven und Elektrischen Bahnen zum Tragen, später in Elektrokarren und in Gabelstaplern. Mit der Weiterentwicklung von Akkus werden heute Elektroautos mit immer größerer Reichweite gebaut und gelten wegen der hohen Effizienz des Elektroantriebs als Alternative zum Verbrennungsmotor in der Zukunft. Entwicklungen in der Leistungselektronik brachten einen weiteren Anwendungsschub – von da ab konnten die wartungsfreien, preiswerten Asynchronmotoren auch für drehzahlvariable Antriebe eingesetzt werden.

Heute werden Elektromotoren in großer Zahl in Maschinen, Automaten, Robotern, Spielzeug, Haushaltsgeräten, Elektronikgeräten (zum Beispiel Videorekorder, Festplatten, CD-Spieler), in Ventilatoren, Rasenmähern, Kränen usw. eingesetzt. Die große Bedeutung des Elektromotors für die heutige moderne Industriegesellschaft spiegelt sich auch im Energieverbrauch wider: Elektromotoren haben einen Anteil von über 50 Prozent am Stromverbrauch in Deutschland.

Elektromotoren werden in Kraftfahrzeugen und Bahnen seit langem angewendet. Gründe hierfür sind:


Trotz dieser Vorteile wird der Elektromotor bisher wenig in Pkw und Lkw eingesetzt. Grund ist insbesondere die begrenzte maximale Reichweite bzw. die hohe Masse der Energiespeicher (Akkumulatoren) sowie deren lange Ladezeit.

Mit einem Elektromotor und einem Akkumulator werden auch manche Modellflugzeuge (Elektroflug), kleine Schiffe, Torpedos und U-Boote angetrieben. Die Elektromotoren anderer U-Boote werden aus Brennstoffzellen oder aus einem mitgeführten kleinen Kernkraftwerk gespeist.

Fahrzeugantriebs-Konzepte mit Elektromotoren, jedoch ohne oder nur teilweiser Energiespeicherung in einem Akkumulator, sind:


Bei elektrischen Bahnen und Oberleitungsbussen wird die Elektroenergie mit Oberleitungen oder Stromschienen zugeführt. Auch hier kann Nutzbremsung stattfinden, wenn das speisende Netz dafür ausgelegt ist oder Akkumulatoren installiert werden. Auch Doppelschichtkondensatoren werden hierbei angewendet.

Eine weitere mobile Anwendung ist der dieselelektrische Antrieb; hier erzeugt ein Dieselaggregat elektrischen Strom, der die Fahrmotoren antreibt. Nutzbremsung ist nicht möglich, wenn nicht zusätzlich Akkumulatoren mitgeführt werden. Dieselelektrische Antriebe finden sich in Schiffen, Lokomotiven und U-Booten (hier ergänzt durch einen Akkumulator).

Die vielfältigen Anwendungsgebiete von Elektromotoren in der Industrie lassen sich in zwölf Gebiete aufteilen. Die ersten vier befassen sich mit dem Materialfluss. Die nächsten vier mit kontinuierlichen oder getakteten Produktionsstraßen und die letzten beiden mit Prozessen, die auf die Werkstücke einwirken.

Technologisch veraltete Elektromotoren führen zu einem erhöhten Energieverbrauch. 1998 wurde eine freiwillige Vereinbarung zwischen dem europäischen Sektorkomitee für elektrische Antriebe CEMEP und der Europäischen Kommission getroffen.
In dieser heute veralteten Vereinbarung wurden drei Wirkungsgradklassen definiert:

Im Jahr 2009 wurde eine neue weltweit geltende Normierung für die Effizienzklassen (EN 60034-30:2009) eingeführt.
Die folgenden Wirkungsgradklassen für Niederspannungs-Drehstrom-Asynchronmotoren im Leistungsbereich von
0,75 kW bis 375 kW sind heute geltend:

Seit 16. Juni 2011 dürfen ungeregelte Motoren (0,75–375 kW) nur noch ab Leistungsklasse IE2 in Verkehr gebracht werden. Der Anteil hocheffizienter Motoren soll stetig ausgebaut werden. Beispiele sind die permanenterregten Synchronmotoren mit höchsten Wirkungsgraden.

Die einzelnen Komponenten des Elektromotors werden unabhängig voneinander hergestellt. Die wichtigsten sind das Gehäuse, der Stator, die Welle und der Rotor. Anschließend erfolgt die Endmontage.

Das eigentliche Gehäuse wird auf beiden Seiten von Deckeln abgeschlossen, die beim Elektromotor als Lagerschilde bezeichnet werden, da sie auch zum Lagern der Motorwelle mittels Kugellagern dienen. Die einzelnen Prozessschritte für die Lagerschilde und das Gehäuse stimmen jedoch überein. Beide werden zunächst durch Gießen oder Fließpressen grob in Form gebracht, danach erfolgt die Feinbearbeitung mit Drehen, Bohren und Schleifen und zuletzt die Reinigung. Die Details hängen von der produzierten Stückzahl ab.

Gießen mit Formen aus Sand wird nur bei geringen Stückzahlen eingesetzt, beispielsweise bei der Prototypenfertigung. Für mittlere und größere Stückzahlen eignen sich der Druckguss und der Schleuderguss sowie das Strangpressen. Der Druckguss ist mit einem Anteil von 60 % das häufigste Verfahren. Hier besteht die Form aus Stahl und kann etwa 80.000-mal abgegossen werden. Die benötigten Maschinen kosten zwischen 700.000 Euro und einer Million Euro, sodass eine Mindeststückzahl von etwa 15.000 erreicht werden muss, um wirtschaftlich zu sein. Schleudergussanlagen kosten dagegen nur etwa 60.000 bis 100.000 Euro. Am teuersten sind Strangpressanlagen mit 8 Mio. Euro. Sie eignen sich daher nur für sehr große Serien, weisen dann aber die niedrigsten Stückkosten auf.

Nach dem Gießen oder Strangpressen werden die Gehäuse entgratet. Die weitere Feinbearbeitung geschieht meist auf Bearbeitungszentren, die auf das Drehen, Bohren, Fräsen und Schleifen spezialisiert sind. Zu den Aufgaben gehört das Ausdrehen der Innenkontur, die Feinbearbeitung von Rändern und das Bohren von Durchgängen oder Gewinden.

Die Reinigung der Gehäuse geschieht bei kleinen Serien meist durch Bestrahlen mit Trockeneis (sogenanntes Trockeneisstrahlen) oder mit kleinen Kugeln (Kugelstrahlen). Dadurch werden Gießrückstände, Späne, Stäube und sonstige Schmutzpartikel entfernt. Bei mittelgroßen Serien geschieht die Reinigung mittels Ultraschallbad. Bei Großserien werden Durchlauf-Reinigungsanlagen eingesetzt, die aus einer Aufgabestation bestehen, aus Reinigungs- und Spülzonen, der Trockenzone und der Übergabestation.

Die eigentlichen leistungserzeugenden Komponenten, also der Rotor und der Stator, werden aus Blechpaketen zusammengebaut. Gegenüber der Vollmaterialbauweise haben Blechpakete den Vorteil, dass sie Wirbelströme verhindern und so den Wirkungsgrad erhöhen. Beim Zusammenbau der Bleche zu Paketen ist es wichtig, Kurzschlüsse zu vermeiden. Die einzelnen Bleche sind daher mit einem Isolator beschichtet. Sie werden aus Elektroband hergestellt. Dabei handelt es sich um Bleche aus einem siliziumhaltigen Stahl, der verbesserte magnetische Eigenschaften aufweist. Da seine Produktion recht aufwendig ist, wird es von Elektromotor-Herstellern eingekauft. Die Fertigung der Blechpakete geschieht in mehreren Schritten: Ausschneiden der Bleche, Stapeln, dauerhaftes Fügen (Kleben, Schweißen, etc.) und eine Nacharbeit.

Für kleinere Serien oder Prototypen wird das Blech mittels Laser- oder Wasserstrahlschneiden getrennt. Bei größeren Serien ist das Stanzen wirtschaftlicher. Anschließend werden die Bleche gestapelt. Beim Stanzen kann dies direkt in der Maschine geschehen, während bei den anderen Verfahren ein weiterer Prozessschritt nötig ist. Zum Fügen der Blechpakete gibt es zahlreiche Möglichkeiten. In der Massenproduktion werden häufig Nasen an einzelnen Blechen nach unten gedrückt, in Aussparungen der darunterliegenden Schichten. Häufig ist dieser Schritt direkt in das Stanzen integriert. Nach dem Stapeln können die einzelnen Lagen auch zusammengeschweißt werden. Dies ist bei deutlich niedrigeren Stückzahlen wirtschaftlich, hat jedoch den Nachteil, dass eine elektrisch leitende Verbindung entsteht, die die Entstehung von Wirbelströmen begünstigt. Da die Schweißnähte an Stellen angebracht werden können, die für das magnetische Feld von geringer Bedeutung sind, sind die Nachteile beim Wirkungsgrad gering. Eine andere Möglichkeit ist die Verwendung von Backlack. Hier werden nach dem Stanzen die einzelnen Bleche mit Backlack beschichtet und gestapelt und anschließend im Ofen gebacken. Dadurch werden einerseits die Schichten zusammengeklebt und andererseits auch isoliert.

Als letzter Schritt kann eine Nachbearbeitung geschehen, die den Wirkungsgrad etwas erhöhen. Dazu zählt das Spannungsarmglühen, Außenrunddrehen, Entgraten und eine Nachlackierung. Da die Effizienzsteigerungen gering sind, wird dies vor allem bei Großmotoren praktiziert.

Der Stator ist mit einem Anteil von 35 % an den Gesamtkosten das teuerste Bauteil. Dies liegt an der aufwendigen Produktion und dem teuren Material. Die einzelnen Prozessschritte sind Isolieren der Komponenten, Wickeln der Spulen, Bearbeiten der Wicklung und Imprägnieren.

Zwischen dem Blechpaket und den Wicklungen der Spulen wird Isolatorpapier verwendet, um Spannungsüberschläge zu vermeiden. Der für die Spulen benötigte Draht wird mittels Drahtziehen hergestellt, anschließend mit einer isolierenden Lackschicht und danach mit einer Gleitschicht überzogen, die das Wickeln erleichtert.

In der Spulenwickeltechnik haben sich zahlreiche Methoden und Verfahren etabliert zur Herstellung der Spulen. Die wichtigsten sind die Linear-, Flyer- und Nadelwickeltechnik. Die Anlagen für die Spulenwicklungen kosten zwischen 150.000 Euro für einfache Maschinen und gehen bis zu 4 Millionen Euro für Anlagen der Großserienproduktion.

Nachdem die Spulen in den Stator eingebaut wurden, werden die Enden der Drähte kontaktiert und geprüft.

Der Kostenanteil der Welle liegt mit nur 5 % sehr niedrig. Die Herstellung geschieht in drei Schritten: Grobbearbeitung im weichen Zustand, Härten und die Feinbearbeitung durch Schleifen.

Die erste Formgebung geschieht bei großen Stückzahlen meist durch Schmieden, insbesondere mittels Gesenkschmieden. Bei kleineren und mittleren Stückzahlen werden Bearbeitungszentren eingesetzt wie bei der Herstellung der Gehäuse. Zum Härten werden konventionelle Wärmebehandlungsmethoden eingesetzt, darunter das Induktionshärten, das Einsatzhärten und das Nitrierhärten. In allen Fällen wird danach durch Hartdrehen oder Schleifen die endgültige Form präzise erzeugt.

Bei Motoren mit Permanentmagneten sind die Fertigungsschritte Magnetisieren, Magnetbestückung, Wellenmontage und Auswuchten austauschbar, verschiedene Reihenfolgen aber mit jeweils eigenen Vor- und Nachteilen behaftet.

Bei Asynchronmotoren wird stattdessen ein Rotorkäfig verwendet. Meistens wird er mittels Druckguss gefertigt. Beim Prototypenbau wird er auch aus Stäben und Ringen zusammengelötet. Hochwertige Käfige bestehen aus Kupfer, das eine größere Leitfähigkeit aufweist als Aluminium aber auch etwa viermal teurer ist und erst bei 1084 °C schmilzt. Aluminiumlegierungen dagegen schmelzen bereits bei 600 °C. Daher lassen sich Alugussformen etwa 50.000-mal abgießen, Formen für Kupfer dagegen nur 100-mal. Üblicherweise wird die Schmelze direkt in die Rotornuten gegossen.

Wegen der Vielfalt der verschiedenen Motorbauarten und möglichen Stückzahlen gibt es bei der Endmontage große Bandbreiten und Varianten von der ausschließlich manuellen Montage bis zur vollautomatischen Montagelinie.

Zuerst wird der Stator in das Gehäuse gebaut. Dies kann mit Aufschrumpfen, Einpressen oder Kleben geschehen. Danach wird das Rotorpaket in den Stator eingebracht.

Der nächste Schritt ist die Montage der Sensorik. Beim Asynchronmotor ist dies ein Drehzahlmesser und bei Motoren mit Permanentmagnet ein Positionsmesser (Inkrementalgeber). Sie werden ebenfalls aufgeschrumpft, eingepresst oder geklebt. Außerdem werden Temperatursensoren verbaut.

Anschließend erfolgt die Kontaktierung der Sensoren und der einzelnen Phasen mit dem Anschlussstecker.

Danach werden die Lagerschilde mit Kugellagern bestückt und am Gehäuse angebracht. Im letzten Schritt erfolgt die Endprüfung auf Sicht sowie die Widerstands-, Isolations-, Funktions- und Hochspannungsprüfung sowie einer Prüfung der Leistungselektronik.





</doc>
<doc id="12688" url="https://de.wikipedia.org/wiki?curid=12688" title="Gleichstrommaschine">
Gleichstrommaschine

Eine Gleichstrommaschine, auch Gleichstrommotor, Kommutatormotor oder Kommutatormaschine, ist eine rotierende elektrische Maschine, die mit Gleichstrom betrieben wird oder Gleichstrom erzeugt. Je nach Richtung des Leistungsflusses wird zwischen dem Gleichstrommotor (ein Elektromotor, dem elektrische Energie zugeführt und mechanische Energie entnommen wird) und dem Gleichstromgenerator (ein elektrischer Generator, dem mechanische Energie zugeführt und elektrische Energie entnommen wird) unterschieden. Gleichstrommaschinen können unter Belastung anlaufen, und ihre Drehzahl ist leicht zu verändern.

Merkmal der klassischen Gleichstrommaschinen ist ein als Kommutator oder Polwender bezeichneter mechanischer Wechselrichter. Mit dessen Hilfe wird im Motorbetrieb im Rotor ein drehzahlabhängiger Wechselstrom erzeugt. Im Generatorbetrieb wandelt er die vom Rotor gelieferte Wechselspannung in eine pulsierende Gleichspannung um. 

Es gibt auch Anwendungsfälle, in denen dieselbe Gleichstrommaschine zeitweise als Motor und zeitweise als Generator benutzt wird.

Eine Sonderform ist die Unipolarmaschine, mit der Gleichstrom ohne Verwendung eines Kommutators erzeugt werden kann.

Begünstigt durch die Entwicklung der ersten galvanischen Elemente in der ersten Hälfte des 19. Jahrhunderts, waren die ersten elektromechanischen Energiewandler Gleichstrommaschinen. Die Urform eines Elektromotors wurde 1829 von Ányos Jedlik entworfen. Im Jahr 1832 baute der Franzose Hippolyte Pixii den ersten Gleichstromgenerator. Antonio Pacinotti baute um 1860 einen Gleichstrommotor mit vielteiligem Kommutator. Friedrich von Hefner-Alteneck entwickelte 1872 den Trommelanker, welcher mit den Arbeiten von Werner von Siemens zu dem dynamoelektrischen Prinzip die Möglichkeit der Selbsterregung und den industriellen Einsatz im Bereich des Großmaschinenbaus eröffnete.

In den folgenden Jahrzehnten verlor die Gleichstrommaschine, bedingt durch die Entwicklung des Dreiphasenwechselstroms, im Großmaschinenbau an Bedeutung. Insbesondere die Synchronmaschinen und für wartungsarme Antriebssysteme der Asynchronmotor lösten die Gleichstrommaschine in vielen Anwendungsbereichen ab. Die Gleichstrommaschine mit Fremderregung lässt sich gut regeln, denn die Ströme durch Anker- und Statorwicklung lassen sich getrennt steuern. Die Gleichstrommaschine hat daher insbesondere im Bereich hochdynamischer Antriebssysteme eine gewisse Bedeutung behalten, beispielsweise zum Antrieb von Werkzeugmaschinen mit präziser Drehzahl- und Drehmomentsteuerung. Im Kleinstleistungsbereich, beispielsweise bei Modellbahnen, ist vor allem der permanenterregte Gleichstrommotor des einfachen Aufbaues wegen üblich.

Die Maschine hat einen unbeweglichen Teil, den Stator. Er besteht aus einem Joch in Form eines Hohlzylinders. Daran befestigt sind die Hauptpole und bei größeren Maschinen auch Wendepole. Der Stator ist nicht geblecht, sondern besteht aus massivem Material, da hier kein magnetisches Wechselfeld wirkt und somit keine Wirbelströme auftreten. Der Hauptpolkern trägt entweder die Hauptpol- oder Erregerwicklung, oder Permanentmagnete erzeugen den nötigen Hauptpolfluss ("permanenterregte" Maschine). In den Polschuhen sitzt bei größeren Maschinen die Kompensationswicklung.

Der drehbar gelagerte Teil der Gleichstrommaschine heißt Rotor oder bei konventionellen Maschinen auch Anker. Der Rotor ist geblecht ausgeführt, da sonst durch die auftretenden Ummagnetisierungen große Wirbelstromverluste auftreten würden. Die meisten Gleichstrommaschinen sind als Innenläufer ausgeführt: Der Rotor ist der innere Teil, der Stator der äußere. Bei Außenläufern ist es umgekehrt. 

Eine oder mehrere Spulen auf dem Anker werden in einem magnetischen Feld des Stators so platziert, dass die Lorentzkraft und teils auch die Reluktanzkraft ein Drehmoment erzeugt. Die Wicklungen des Ankers werden hierzu über einen Kommutator (Polwender) angeschlossen. Die Schleifkontakte am Kommutator (Metallbürsten oder Kohle„bürsten“) wechseln während der Drehung die Polung der Ankerwicklungen so, dass immer diejenigen Wicklungen von Strom entsprechender Richtung durchflossen werden, die sich quer zum Erregerfeld bewegen. An den Bürsten ist eine pulsierende Gleichspannung abgreifbar. Durch mehrere räumlich versetzte Windungsschleifen (Wicklungen) erhält man eine Glättung des Spannungsverlaufs.

Die Bürsten bestehen aus einem Material, das eine verschleißarme gute Kontaktierung bietet (oft selbstschmierender Graphit, teilweise gemischt mit Kupferpulver (siehe Kohlebürste); bei kleinen Motoren, etwa für Kassettentonbandgeräte, auch Edelmetall-Bürsten).

Durch Umkehrung des Prinzips (der Anker wird von außen bewegt) erhält man einen Generator. Zur Stromerzeugung werden fast nur permanent- oder fremderregte Bauweisen benutzt. Der Kommutator wandelt den erzeugten Wechselstrom in Gleichstrom.

Das allgemeine Motorverhalten wird durch die Feldstärke der Feldwicklung und die Eigenschaften der Ankerwicklung (Windungszahl, Anzahl der Pole) bestimmt.

Das Stator-Magnetfeld wird bei kleineren Motoren (Spielzeug, Stellantriebe, Gebläse und Kühler-Ventilatoren in Kraftfahrzeugen, Elektroantrieb an Fahrrädern) oft durch Permanentmagnete erzeugt. Diese Magnete sind mit der Entwicklung der Gleichstrommotoren immer leistungsfähiger geworden und würden auch den Bau größerer Motoren gestatten. Die Kosten großer Permanentmagnete wären jedoch höher als die der Erregerwicklung.

Permanenterregte Motoren haben, wie auch fremderregte Maschinen, sehr hohe Einschaltströme. Ihr Betriebsverhalten ist in den mathematischen Grundlagen erklärt. Permanenterregte Maschinen haben den Vorteil, dass zur Erzeugung des Magnetfeldes keine Energie benötigt wird. Das verbessert besonders bei kleiner Gesamtleistung den Wirkungsgrad. Nachteil ist, dass das Feld nicht geschwächt und somit die Drehzahl weniger variiert werden kann.

Wird das Statorfeld durch einen Elektromagneten erzeugt, spricht man von elektrischer Erregung. Ist die Erregerwicklung vom Ankerstromkreis unabhängig, spricht man von Fremderregung. Sind die Rotor- und Statorwicklung miteinander verbunden, unterscheidet man:

Die "Reihenschlussmaschine" wird auch "Hauptschlussmaschine" genannt,
bei ihr sind Erregerwicklung und Ankerwicklung in Reihe geschaltet. Die Erregerwicklung muss niederohmig sein. Bei einer Speisung mit Wechselspannung wechseln sowohl das Erregerfeld als auch der Ankerstrom die Richtung nach jeder Halbwelle, so dass ein Reihenschlussmotor auch mit Wechselstrom betrieben werden kann. Der Eisenkern des Stators muss in diesem Fall ein Blechpaket sein, damit Wirbelströme vermieden werden.

Mit einphasigem Wechselstrom betriebene Motoren (Einphasen-Reihenschlussmotoren) sind in älteren Elektrolokomotiven (daher der Frequenzkompromiss 16,7 Hz im Bahnstromnetz) sowie in Straßenbahn-Triebwagen zu finden. Sie befinden sich auch unter dem Begriff Universalmotor oder Allstrommotor in vielen Haushaltsmaschinen (Staubsauger, Küchenmaschinen) und Elektro-Handgeräten (Bohrmaschinen). Die Anlasser großer Verbrennungsmotoren (beispielsweise in LKW) sind Reihenschlussmotoren.

Um einen Reihenschlussmotor als Generator (z. B. beim elektrischen Bremsen von Straßenbahnen) betreiben zu können, muss seine Erregerwicklung umgepolt werden, denn sonst hebt der generierte, durch die Feldwicklung fließende Strom das Erregerfeld auf.

Das Drehmoment einer Reihenschlussmaschine ist stark drehzahlabhängig (Reihenschlussverhalten). Bei geringer Drehzahl ist die Gegeninduktionsspannung der Ankerwicklung gering. Daher fließt ein großer Strom durch Anker und Erregerwicklung, und es kann ein großes Drehmoment aufgebracht werden. Mit der Drehzahl steigt die Gegeninduktionsspannung. Strom und Erregung sinken und damit auch das Drehmoment der Maschine.

Reihenschlussmotoren haben, insbesondere bei Wechselspannungsbetrieb („Universalmotor“, z. B. in Staubsaugern), dennoch einen wesentlich geringeren Einschaltstrom als Nebenschluss- oder permanenterregte Motoren. Sie liefern dennoch kurzzeitig ein sehr hohes Anlaufmoment, weswegen sie in Anlassern, Straßenbahnfahrzeugen und Elektrolokomotiven, wo sie im Kurzbetrieb extrem überlastbar sind, verwendet werden.

Bei Betrieb mit Wechselstrom ist das Drehmoment mit doppelter Netzfrequenz pulsierend, so dass bei großen Motoren ausgleichende Elemente zwischengeschaltet werden müssen. Das gilt auch für Einphasen-Synchronmaschinen.

Die Anschlüsse des Ankers werden mit A1 und A2 bezeichnet, die der Erregerwicklung mit D1 und D2. Bei der dargestellten Schaltung dreht der Motor im Uhrzeigersinn, erkennbar am eingezeichneten Pfeil im Anker.

Bei der Nebenschlussmaschine sind Erreger- und Ankerwicklung parallelgeschaltet. Der Erregerstrom wird nur durch den ohmschen Widerstand der Erregerwicklung, die eine hohe Windungszahl und Induktivität hat, begrenzt. Ein Wechselspannungsbetrieb ist nicht möglich, da der Erreger- dem Ankerstrom weit nacheilen würde. Die Drehzahl großer Nebenschlussmaschinen ist nahezu unabhängig vom Drehmoment, weshalb sie sich besonders für Anwendungen eignet, bei denen das Lastmoment schwankt, die Drehzahl aber möglichst konstant sein soll, z. B. bei Förderbändern und Hebezeugen, bei denen auch Asynchronmotoren benutzt werden.

Nebenschlussmotoren können bei Unterbrechung des Erregerkreises "durchgehen", da Drehzahl und Stromaufnahme ohne Erregung drastisch ansteigen.

Nebenschlussmotoren können als Generator (z. B. zur Bremsung) arbeiten, wenn eine Hilfsspannungsquelle oder eine Restmagnetisierung dafür sorgen, dass beim Start des Bremsvorganges eine Erregung vorhanden ist. Mit steigender Erregung oder Drehzahl steigt auch die generierte Spannung – es ist die Spannung, die auch bei Motorbetrieb dem speisenden Strom entgegenwirkt und für eine konstante Drehzahl sorgt. Sie wird daher auch Gegen-EMK genannt. Der Anstieg der Gegen-EMK mit der Erregung, also bei Nebenschlussmotoren mit der Versorgungsspannung, bewirkt, dass ihre Drehzahl wenig von der Spannung abhängt, solange keine magnetische Sättigung eintritt. Mit sinkender Spannung sinkt auch die Drehzahlsteifigkeit. Bei einem fremderregten Gleichstrommotor mit unabhängig versorgtem, konstantem Erregerfeld ist dagegen die Leerlaufdrehzahl proportional zur Ankerspannung.

Die Anschlüsse des Ankers werden mit A1 und A2 bezeichnet, die der Erregerwicklung mit E1 und E2. In der dargestellten Beschaltung dreht der Motor rechtsherum (im Uhrzeigersinn), erkennbar am eingezeichneten Pfeil im Anker.

Das maximal erreichbare Drehmoment wird durch den zulässigen Ankerstrom begrenzt, dieser ist hauptsächlich von den getroffenen Kühlmaßnahmen abhängig. Große Nebenschlussmaschinen in Walzwerken werden fremdbelüftet, um auch bei geringer Drehzahl einen hohen Ankerstrom und damit ein hohes Drehmoment zu ermöglichen.

Wird an eine Nebenschlussmaschine plötzlich ihre Nennbetriebsspannung gelegt, fließt durch den Anker ein sehr hoher Einschaltstrom, der Schutzschaltungen auslösen kann. Große Maschinen müssen daher mit geringerer Spannung angefahren werden. Dadurch wird die Kennlinie parallel zu geringen Drehzahlen hin verschoben, sodass sie die Momentenachse in einem Bereich außerhalb der Überlast schneidet. Das Anlaufmoment sowie der Ankerstrom im Stillstand sind dann begrenzt. Zusammen mit der folgenden Erhöhung der Antriebsdrehzahl kann auch die Spannung erhöht werden. Alternativ können zum Anfahren Vorwiderstände im Ankerkreis verwendet werden; dadurch wird die Kennlinie flacher, sodass sie wiederum die Achse in einem Bereich außerhalb der Überlast schneidet. Der Nachteil dieser Methode ist die Verlustleistung am Widerstand, dieser muss dann gegebenenfalls aktiv gekühlt werden.

Der Doppelschlussmotor (auch "Verbund-" oder "Compound-Motor" genannt) vereinigt die Eigenschaften des Neben- und des Reihenschlussmotors in einer Maschine. Er hat eine Reihenschluss- und eine Nebenschlusswicklung. Je nach Auslegung hat der Doppelschlussmotor unterschiedliches Betriebsverhalten. Bei richtiger Compoundierung hat er ein etwas geringeres Anzugsdrehmoment als ein gleichwertiger Reihenschlussmotor. Seine Drehzahl sinkt dann bei Belastung etwas mehr ab als die eines entsprechenden Nebenschlussmotors. Bei Leerlauf geht er nicht durch. Wird der Doppelschlussmotor überkompoundiert, so hat er vorwiegend Reihenschlussverhalten, also ein hohes Anzugsmoment, aber eine instabile Drehzahl. Bei Unterkompoundierung hat er überwiegend Nebenschlussverhalten, also hohe Drehzahlstabilität, aber geringeres Anzugsmoment. Der Doppelschlussmotor wird wegen seines gleichen Drehzahl-Drehmoment-Verhaltens zum Antrieb z. B. von Pressen und Stanzen verwendet.

Hier werden Anker- und Erregerwicklung aus zwei unterschiedlichen und getrennt voneinander einstellbaren Gleichstromstellern gespeist. Während bei der Nebenschlussmaschine die Erregerspannung gleich der Ankerspannung ist, kann man bei fremderregten Maschinen durch Verringerung des Erregerstroms, man spricht in diesem Fall auch von einer Feldschwächung des magnetischen Flusses Φ, die Drehzahl "n" über die Nenndrehzahl "n" hinaus steigern. Im rechten Diagramm entspricht dies dem Bereich mit "n"/"n" größer als 1. Dabei kommt es gleichzeitig zu einer Reduktion des Drehmoments "M". Andererseits kann bei der fremderregten Maschine die Ankerspannung "U" unabhängig vom Erregerstrom abgesenkt werden. Dadurch kommt es bei konstantem Drehmoment zu einer Leistungsabsenkung und gleichzeitiger Drehzahlreduktion.

Durch die getrennte Regel- bzw. Steuerbarkeit der Anker- und Erregerwicklungen lassen sich verschiedene Betriebspunkte unterhalb der rot gestrichelten Drehmomentkennline durch die Art der Ansteuerung erzielen. Vorteile wie hohes Drehmoment bei Stillstand oder geringen Drehzahlen sind gegeben. Daher spielten insbesondere fremderregte Gleichstrommaschinen im Bereich von hochdynamischen Antriebssystemen, beispielsweise bei Werkzeugmaschinen oder elektrischen Stadtschnellbahntriebzügen wie der DB-Baureihe 420 bis in die 1980er Jahre hinein eine bedeutende Rolle. Gleichstrommaschinen wurden in den Folgejahren in diesen Anwendungsbereichen zunehmend durch Drehstrommaschinen mit vorgeschalteten elektronischen Frequenzumrichtern abgelöst, die in Kombination die gleichen Vorteile für hochdynamische Antriebe bei geringerem Wartungsaufwand bieten.

Fremderregte Gleichstrommaschinen wurden früher auch im Leonardsatz eingesetzt, dem früher einzigen drehzahlvariablen Antrieb für große Leistungen, der aus einem Drehstrom-Asynchronmotor, einem jeweils fremderregten Gleichstrom-Generator und einem Gleichstrommotor bestand.

Kleine Maschinen bis etwa 100 Watt mit Permanentmagneten können auch mit einem hohlen Rotor gebaut werden. Der Rotor ist eisenlos, selbsttragend gewickelt und kunstharzgetränkt.
Der Stator, ein Permanentmagnet, liegt in diesem Fall innerhalb des Rotors. Das außenliegende Motorgehäuse aus Eisen bildet den notwendigen Rückschluss für den magnetischen Fluss des Stators. Der elektrische Aufbau entspricht der ersten Illustration.
Durch den eisenlosen Aufbau des Rotors bildet der Motor kein Rastmoment aus, er lässt sich vollkommen frei drehen.
Da im Gegensatz zu allen anderen Motoren im Betrieb keine Eisenteile ummagnetisiert werden müssen, ist dieser Motor frei von Eisenverlusten und erreicht bei hohen Drehzahlen höhere Wirkungsgrade.
Insbesondere ist jedoch sein Rotations-Trägheitsmoment geringer, weshalb damit hochdynamische, leichte Antriebe realisiert werden können. Mit Glockenankermotor ausgerüstete Modellbahnfahrzeuge zeichnen sich durch ruckfreien Lauf und gute Langsamfahreigenschaften aus.
Nachteilig ist der große Luftspalt im Erregerkreis, der eine verringerte Magnetflussdichte zur Folge hat. Die selbsttragende Bauweise stellt hohe technologische Anforderungen, da die Fliehkräfte aufgenommen werden müssen und ein nachträgliches Auswuchten des Ankers durch Materialabtrag nicht möglich ist. Nötige Stoßfestigkeit limitiert die Ankergröße.

Der Scheibenläufermotor ist ähnlich aufgebaut, allerdings ist die Läufer-Wicklung nicht in Form eines Zylinders, sondern als Scheibe ausgeführt.

Nachteil der Gleichstrommaschinen sind Funken, die bei den Bürsten entstehen („Bürstenfeuer“). Das Bürstenfeuer ist die Hauptursache für hochfrequente Störungen, die der Motor im Betrieb in das Leitungsnetz zurückspeist und die andere elektrische Verbraucher stören. Es begrenzt auch die maximale Drehgeschwindigkeit, da die Bürsten bei hohen Drehzahlen heiß werden und besonders schnell verschleißen. Weiterhin bewirken hohe Drehzahlen auch höhere Induktionsspannungen, die bis hin zum umlaufenden Bürstenfeuer führen können.

Mit der Entwicklung der Elektronik können kleinere permanenterregte Synchronmotoren so betrieben werden, dass sie von außen ähnlich beschrieben werden können wie eine Gleichstrommaschine. Diese Motoren mit Elektronik-Umrichter wurden besonders im englischen Sprachraum als "brushless direct current" (BLDC) beworben, auf Deutsch übersetzt bürstenlose Gleichstrommaschine. Die Maschine wird auch als EC-Motor (EC für "electronically commutated") bezeichnet. Vom Aufbau her sind diese Motoren ungedämpften permanenterregten Synchronmaschinen gleich und können in Anwendungen, die eine genügende Eigendämpfung haben, auch als Synchronmaschine angesteuert werden.

Da der Anker stromdurchflossen ist, bildet sich auch um diesen ein magnetisches Feld. Dieses verstärkt das Hauptfeld auf der einen Seite des Leiters und schwächt es auf der anderen. Insgesamt führt dies dazu, dass sich der "neutrale Bereich", in dem die Polung des Stromes umgeschaltet werden muss, etwas verspätet, d. h. im Generatorbetrieb in Drehrichtung und im Motorbetrieb gegen die Drehrichtung verschiebt. Da sich jedoch der Kommutator nicht anpasst (also stets senkrecht zu den Polschuhen umschaltet und nicht senkrecht zu den „effektiven“ Feldlinien), liegt zu dem Zeitpunkt des Umschaltens (Kommutierung) noch eine Induktionsspannung an den Kohlebürsten an, und es kommt zur Funkenbildung, dem Bürstenfeuer.
In Anlagen, die ein gleichmäßiges Drehmoment verlangen und nur in einer Laufrichtung betrieben werden (z. B. starke Lüfter), kann das Bürstenfeuer verringert werden, indem der Bürstenträger leicht verdreht montiert wird und dann im Betriebszustand doch senkrecht zu den effektiven Feldlinien umschaltet. Dies erfordert jedoch eine Justierung im Betrieb und wird heute aus Kostengründen kaum noch durchgeführt. Stattdessen werden in großen Maschinen Wendepolwicklungen und Kompensationswicklungen eingesetzt, die die Feldlinien gleichsam in die ideale Lage „zurückbiegen“.

Der Anker dreht sich im Motor innerhalb des Statorfeldes. Nach dem Generatorprinzip wird so in dessen Spulen auch bei Motorbetrieb eine Spannung induziert. Diese induzierte Spannung ist wie die Betriebsspannung gepolt und wirkt daher dem Rotorstrom entgegen. Sie wird Gegenspannung oder Gegen-EMK genannt. Sie ist ein wichtiger Parameter von Motoren, mit ihrer Hilfe lässt sich in etwa die Leerlaufdrehzahl permanenterregter Motoren bestimmen.

Der Ankerstrom führt zu einem ohmschen Spannungsabfall am Ankerwiderstand (Kupfer), dieser Spannungsabfall steigt somit mit der Belastung des Motors (steigende Stromaufnahme) an und bewirkt bei Motoren einen Abfall der Drehzahl. Bei großen fremderregten Motoren ist dieser Drehzahlrückgang sehr gering.

Die Gegen-EMK ist streng linear abhängig von der Drehzahl des Ankers und der Stärke der Erregung. Die Gegen-EMK kann von Regelschaltungen genutzt werden, um die Drehzahl permanenterregter Motoren exakt zu stabilisieren; dies wird z. B. bei Kassetten-Tonbandgeräten angewendet.

Die Gegen-EMK macht bei Umkehr der Stromrichtung (Klemmenspannung < EMK) aus dem Motor einen Generator, sie kann zur Bremsung und zur Energierückspeisung (Nutzbremsung) dienen.

Bei Motorstillstand gibt es keine Gegenspannung. Deshalb haben fremd- und permanenterregte Gleichstrommotoren einen hohen Einschaltstrom – der Widerstand der Rotorspulen ist vergleichsweise klein und somit der Strom im Moment des Einschaltens sehr groß.
Ohne Begrenzung des Anlaufstromes werden große Motoren oder das speisende Netz eventuell überlastet, man verwendet daher in Reihe zum Anker Anlasswiderstände, die nach dem Hochlaufen stufenweise kurzgeschlossen werden.
Auch die Reihenschlussmotoren von Straßenbahnen wurden früher über Fahrschalter (Stufenwiderstand) angefahren, heute wird dies verlustärmer über Schaltregler (Chopperbetrieb) erreicht.
Bei Elektrolokomotiven verwendete man Transformatoren mit Stufenschaltern, an denen sich kleinere Stelltransformatoren von Stufe zu Stufe „hangelten“. Auch hier verwendet man heute stattdessen Leistungselektronik (IGBT-Schalter)

Unter Vorgabe des Verbraucher-Zählpfeil-Systems (wie z. B. beim Ohmschen Gesetz vorausgesetzt) gilt
Setzt man den Strom als zeitlich konstant voraus formula_2, folgt
Berücksichtigt man zusätzlich das Induktionsgesetz, wird daraus

Diese Gleichung lässt sich wie folgt deuten:
Für konstantes formula_5 und dem in der Praxis kleinen formula_6 ist die induzierte Spannung formula_7 unwesentlich kleiner als formula_5. Damit ist bei konstantem Fluss formula_9 n ungefähr proportional der Ankerspannung. Im Bereich formula_10 ist damit die Drehzahl über die Ankerspannung steuerbar. Man spricht vom Ankerstellbereich.
Für den Fall formula_11 und formula_12 spricht man vom Typenpunkt. Oberhalb des Typenpunktes ist bei konstanter Ankerspannung formula_5 eine Drehzahlsteigerung durch eine Verringerung des magnetischen Flusses formula_9 über eine Verringerung des Erregerstromes möglich (Feldschwächbereich). Hierbei sind jedoch einige Randbedingungen zu beachten. Die Drehzahl darf einen zugelassenen Maximalwert nicht überschreiten. Wegen der Wirkung der Lorentzkraft gilt formula_15 und folglich wird das zulässige Drehmoment M proportional mit formula_9 kleiner.

Darin ist

Die Gleichungen des mechanischen Systems mit der Annahme, dass der Erregerkreis nicht gesättigt ist:

Darin ist

Einige Gleichungen anders angeschrieben:

dabei ist:




</doc>
<doc id="12689" url="https://de.wikipedia.org/wiki?curid=12689" title="Drehstrom-Asynchronmaschine">
Drehstrom-Asynchronmaschine

Eine Drehstrom-Asynchronmaschine ("Drehstrom-Induktionsmaschine") ist eine Drehstrommaschine, bei der der Rotor (auch "Läufer") dem Drehfeld des Stators als Generator vor- und als Elektromotor nachläuft. Sie besitzt einen passiven Läufer, der entweder ständig (Kurzschlussläufer, Käfigläufer) oder fallweise kurzgeschlossen wird (Schleifringläufer). Beim Einsatz als Generator kann der Läufer dieser Asynchronmaschine auch mit einer abweichenden Frequenz erregt werden (doppelt gespeiste Asynchronmaschine). Einphasig betreibbare Asynchronmotoren sind der Kondensatormotor, der Spaltpolmotor und der Anwurfmotor. Die Drehstrom-Asynchronmaschine wurde 1889 von Michail Ossipowitsch Doliwo-Dobrowolski bei der Firma AEG entwickelt und ist in der elektrischen Antriebstechnik weit verbreitet.

Die Entwicklung des Asynchronmotors geht zurück auf Vorarbeiten von Galileo Ferraris, 1885 und wesentliche Arbeiten von Michail von Dolivo-Dobrowolsky, 1891. Letzterer baute den ersten Einfachkäfigläufer und später auch einen ersten Doppelkäfigläufer.

Der Asynchronmotor ist heute der am meisten verwendete Elektromotor. Drehstrom-Asynchronmaschinen werden mit Leistungen von bis zu mehreren Megawatt hergestellt. Der Vorteil gegenüber anderen Elektromotoren ist das Fehlen von Kommutator und Bürsten. Bürsten verschleißen und erzeugen Funken („Bürstenfeuer“), wodurch das Leitungsnetz mit hochfrequenten Schwingungen gestört wird. Außerdem dürfen Maschinen mit Bürsten wegen möglicher Wirkung des Bürstenfeuers als Zündquelle nicht in explosionsgeschützten Bereichen eingesetzt werden. Allerdings verursachen auch Asynchronmotoren – insbesondere beim Betrieb an einem Frequenzumformer – Oberschwingungen, die auf das Netz zurückwirken.

Der Motor besteht aus zwei Teilen, dem äußeren, feststehenden Ständer oder Stator und dem sich darin drehenden Läufer oder Rotor. Beidseits des schmalen Luftspalts fließen elektrische Ströme im Wesentlichen in axialer Richtung. Die Ströme sind in Spulendrähten konzentriert, die von weichmagnetischem Eisen umgeben sind. Das Eisen ist senkrecht zur Achse geblecht.

Beim Betrieb an Drehstrom beträgt die Zahl der Kupferspulen im Stator drei oder ein Vielfaches davon, siehe Polpaarzahl, mit einer Phasenverschiebung der Ströme in benachbarten Spulen von 120 Grad. Die Statorspulen sind zu drei Wicklungssträngen verbunden, deren Enden herausgeführt sind.

Für den "Läufer" eines Drehstrom-Asynchronmotors existieren zwei Bauformen:


Der "Ständer" oder Stator besteht aus dem Gehäuse, dem Ständerblechpaket und der darin eingelegten Ständerwicklung, die immer als Mehrphasenwicklung ausgeführt ist. Das Gehäuse muss das Drehmoment gegen das Fundament abstützen. Häufig hat das Gehäuse außen Kühlrippen, die vom Lüfter des Läufers angeblasen werden.

Die Wirkungsweise des Drehstrom-Asynchronmotors basiert auf dem Drehfeld, das im Luftspalt zwischen Stator und Rotor radial gerichtet ist.

Bewegt sich der Rotor synchron zum Drehfeld, so ist (bis auf Transienten) der magnetische Fluss durch die Maschen des Käfigs konstant und es wird keine Spannung induziert. Das Drehmoment ist bzw. wird null.

Dreht sich der Rotor langsamer als das Drehfeld, so ändert sich der magnetische Fluss, was eine Spannung induziert, die wiederum einen Strom hervorruft. Solange der Schlupf klein ist, ist der Strom proportional zur Änderungsrate des Flusses, also zum Schlupf. Das mit dem Käfigstrom einhergehende Feld ist noch klein im Vergleich zum Feld des Stators und zu diesem um 90° phasenverschoben. Das dadurch entstehende Drehmoment ist proportional zum Schlupf.

Wird das Gegenfeld des Käfigs spürbar, so steigt der Käfigstrom nicht mehr proportional zum Schlupf an und die Phasenverschiebung nimmt ab. Das Drehmoment erreicht ein Maximum. Der Betriebspunkt liegt zwischen diesem Maximum und der Synchrondrehzahl.

Im anderen Extrem des blockierten Rotors entspricht der Käfig der Sekundärwicklung eines (kurzgeschlossenen) Transformators. Die Stromaufnahme ist begrenzt durch den Streufluss und ohmsche Verluste. Im Anfahrbereich hat der Motor einen schlechten Wirkungsgrad und erwärmt sich stark. Der hohe Anfahrstrom kann durch einen vorgeschalteten Anlasswiderstand gemindert werden. Neben dem Aufwand für zusätzliche Komponenten muss man eine längere Anfahrzeit in Kauf nehmen.

Während des Anfahrens können starke Geräusche auftreten (magnetischer Barkhausen-Effekt). Es kann ein Verharren (Kleben) bei Drehzahlen unterhalb der Nenndrehzahl unter starker Geräuschbildung auftreten, häufig bei 1/7 der Synchrondrehzahl. Durch die Nuten in den Blechpaketen von Stator und Rotor werden Oberschwingungen im Stromnetz erzeugt (Nutenpfeifen).

Das Problem wird umgangen, wenn man die Nuten des Rotors schräg zur Wellenachse anordnet. Das verteuert zwar die Herstellung, phasenverschobene Magnetfelder können so jedoch nicht mehr in Resonanz kommen.

Die Motoren werden meistens durch Schütze gesteuert, je nachdem welche Betriebsart vorgesehen ist. Ein Beispiel ist die Stern-Dreieck-Schaltung. Man kann die Motordrehzahl auch über Umrichter, wie z. B. Frequenzumrichter steuern, indem man die Frequenz erhöht oder reduziert. Das ist sinnvoll bei Anlagen, die eine variable Drehzahl benötigen, ohne dass ein verstellbares Getriebe eingesetzt werden soll.
In der Holzverarbeitung werden beispielsweise Motoren an Fräsmaschinen über einen Frequenzumrichter angeschlossen, um aus der Netzfrequenz von 50 Hz beispielsweise 200 Hz zu generieren, wobei die Drehzahl dann auf über 10.000 min gesteigert werden kann. Die hohen Fliehkräfte, die auf den Rotor wirken, erfordern Sonderausführungen der Maschinen.

Asynchronmotoren besitzen einen hohen Einschaltstrom; ist dieser nicht bekannt, geht man vom Achtfachen des Nennstromes aus. Um das Netz und angeschlossene Getriebe zu schonen, sowie das Auslösen vorgeschalteter Sicherungen zu vermeiden, verwendet man bei Asynchronmotoren spezielle Anlassverfahren. Das am häufigsten verwendete Verfahren ist die Stern-Dreieck-Schaltung. Beim Anlauf in Sternschaltung sind Leistung und Drehmoment auf etwa ein Drittel reduziert. Nach der Hochlaufzeit wird durch Umsteuerung der Schütze auf Dreieckbetrieb umgeschaltet. Frequenzumrichter können bei entsprechender Konfigurierung beziehungsweise Programmierung Asynchronmotoren sanft und lastangepasst hochfahren. Bei stärkeren Motoren muss das jeweilige Anlassverfahren mit dem Netzbetreiber abgestimmt werden.

Bei Käfigläufermotoren wirkt sich beim Anlaufen der Skineffekt günstig aus. Bei hohem Schlupf konzentriert sich der Strom am Rand der Kurzschlussstäbe, wodurch der Widerstand steigt. Über das Profil der Kurzschlussstäbe lässt sich die Kennlinie von Leistung und Drehmoment gegen die Drehzahl beeinflussen.

Früher verwendete man (u. a. bei Fahrgeschäften) Anlasswiderstände, insbesondere auch Wasserwiderstände zum Hochfahren. Letztere bestehen aus einem Wassertank, in den allmählich Elektroden eingetaucht werden.

In der Kältemaschinentechnik ist der Teilwicklungsanlauf ein etabliertes Standardverfahren zur Reduzierung des Anlaufstromes.

Asynchronmaschinen können
betrieben werden.

Unterschiedliche Polzahlen und Frequenzen ergeben folgende Drehzahlen für das Drehfeld: 
Das sind die Ständerdrehfeld-Drehzahlen, also die Drehzahl, die das Netz dem Motor über die Feldwicklungen im Stator aufprägt. Sie werden auch als Synchrone Drehzahl bezeichnet.

Im Motorbetrieb liegen die mechanischen Drehzahlen aufgrund des prinzipbedingten Schlupfes je nach Bauweise und Belastung geringfügig unter diesen Werten (meist 1 - 8 %). Prinzipbedingt deshalb, weil erst die Drehzahldifferenz zwischen Ständerdrehfeld und Rotor eine Spannung im Rotor induziert, durch die ein Strom im Rotor fließt, dessen Magnetfeld in Wechselwirkung mit dem Magnetfeld des Ständers das notwendige Drehmoment erzeugt. Demzufolge ist der Schlupf, also die Verminderung der Rotordrehzahl, auch immer abhängig vom Belastungsmoment.

Wichtige Drehzahlen sind die Leerlaufdrehzahl (Motor läuft ohne Last), die Nenndrehzahl (Motor liefert Nennleistung als Produkt von Nenndrehzahl und Nennmoment), Kippdrehzahl (maximales Drehmoment; wird diese von der Last überschritten, bleibt der Motor stehen) und Kurzschlussdrehzahl (Motor steht, Anlaufmoment, Anlaufstrom).

Wird die Drehstrom-Asynchronmaschine auf eine höhere als die synchrone Drehzahl angetrieben, so speist sie Leistung ins Netz zurück (Generatorbetrieb).


Die Dahlander-Schaltung bietet bei Asynchronmaschinen in Käfigläuferausführung die Möglichkeit der Polumschaltung und damit der Drehzahlumschaltung.

Es ist denkbar, auf einer Welle zwei komplett getrennte Motoren anzuordnen. Elegant ist es dann, wenn diese Motoren in "einem" Gehäuse sind. Dann können auch beide Motoren einen gemeinsamen Rotor (Käfig) haben.
Die Statorwicklungen werden jedoch doppelt ausgeführt. Stator eins ist für die niedrige Drehzahl ausgelegt. Stator zwei ist für die vier- oder sechsfache Drehzahl ausgelegt.
Ein Drehzahlverhältnis von eins zu zwei wird meist mit der oben beschriebenen Dahlanderschaltung realisiert.

Polumschaltbare Motoren haben fast die gleichen Eigenschaften wie die Dahlandermotoren, mit dem Unterschied, dass Dahlandermotoren sogenannte "angezapfte Wicklungen" besitzen (die Wicklungen haben drei Anschlüsse: Anfang, Ende und eine Anzapfung in der Mitte der Wicklung). Sie haben also im Ständerblechpaket nur drei um 120 Grad versetzte Wicklungen. Polumschaltbare Motoren sind mit "getrennten" Wicklungen ausgestattet. Das heißt: Sie haben mindestens sechs Wicklungen im Ständerblechpaket, also nicht ein Polpaar, wie der Dahlandermotor (drei Wicklungen), sondern ab zwei Polpaaren aufwärts (sechs oder mehr Wicklungen).

Nicht immer geht es darum, den Einschaltstrom herabzusetzen. In manchen Fällen geht es auch darum, dass sich ein zu hohes Anzugsmoment, bei direkter Einschaltung, störend auf die Anlage auswirkt.
Die sogenannte KUSA-Schaltung ("Ku"rzschlussläufer-"Sa"nftanlauf) ist eine Schaltung zum Anlassen von Drehstrommotoren mit Käfigläufer bei ca. der Hälfte des Nenndrehmomentes.

Bei der KUSA-Schaltung wird ein Vorwiderstand in einen Außenleiter des Laststromkreis des Drehstrommotors gelegt, der nach einer einstellbaren Zeit oder manuell mittels Kontaktes überbrückt wird. Es ist oft zweckmäßig, den Vorwiderstand anzuzapfen, um verschiedene Beträge des Anlaufmomentes einstellen zu können.
Diese Anlaufart kommt nur bei Leerlauf oder geringem Gegenmoment in Betracht.

Mit dem Siegeszug der Spannungsumformer werden heute nahezu ausschließlich Kurzschluss-Käfigläufermotoren (engl. "squirrel cage induction motor") verwendet. Dieser Ausführungsart verdankt der Asynchronmotor seine Bezeichnung als „Arbeitspferd“ der elektrischen Antriebstechnik.
Kombiniert mit einem entsprechend gesteuerten Frequenzumrichter ist er auch in der Lage, gegen große Gegenmomente von Arbeitsmaschinen anzulaufen. Die Frequenzumrichterbaugruppen übernehmen derzeit zunehmend auch die Aufgabe des Motorschutzes. Außerdem werden Motoren mit angebautem Frequenzumrichter angeboten. Dadurch verringert sich der Verdrahtungs- und Entstöraufwand.

Vorteile


In der Europäischen Gemeinschaft ist die EN 60034 „Drehende elektrische Maschinen“ zu beachten. 

"Normmotoren "

Genormte Anbaumaße werden für Deutschland mit den Normen DIN 42673, 42676 und 42677 vorgegeben. 
Der Leistungsbereich bis ca. 200 kW gehört den Niederspannungs-Normmotoren. 

Die Normmotoren, für die die großen Hersteller Listen mit technischen Daten veröffentlichen, sind nach Drehmomentklassen eingeordnet. Üblicherweise können diese Motoren gegen das doppelte Nennmoment anlaufen. 
Für die Konstruktion ist die Achshöhe ein Richtmaß. Der Normmotorenbereich beginnt bei AH 56 und reicht bis zu AH 315 (ca. 200 kW). Oberhalb von AH 315 beginnt mit AH 355 der Transnormmotorenbereich.

" Sonderbauformen "

Im Generatorbetrieb rotiert der Läufer schneller als das Magnetfeld und speist so Energie in das Netz ein.

Es gibt drei verschiedene Asynchronmaschinen, die als Generator eingesetzt werden.

Alle drei Generatortypen werden in dezentralen Kraftwerken eingesetzt.

Zum Verständnis der Vorgänge einer Drehzahlregelung ist die Betrachtung des Ersatzschaltbilds der Asynchronmaschine notwendig. Das Ersatzschaltbild zeigt eine zur Maschine elektrisch äquivalente Schaltung, wie sie auch ein Frequenzumrichter sieht. 

Auf der linken Seite ist die Ständerwicklung dargestellt, sie besteht aus "R" (Kupferwiderstand und äquivalenter Serienwiderstand der Ummagnetisierungsverluste) und dem Blindwiderstand ihrer Induktivität "X" bei asynchronem Lauf.
Rechts ist der Läufer oder Rotor dargestellt: die Induktivität "X" repräsentiert die bei stillstehendem Motor erscheinende Induktivität, sie ergibt sich aus den am stehenden Kurzschlusskäfig vorbeilaufenden Magnetfeldlinien. Der Wirkwiderstand Rr setzt sich zusammen aus 

Im Leerlauf besteht das Ersatzschaltbild des Asynchronmotors im Wesentlichen also aus "R" und "X", weshalb eine solche Maschine fast nur Blindleistung aufnimmt. Der im Leerlauf aufgenommene Strom ist oft ähnlich hoch wie der Nennstrom, die Maschine hat aufgrund der Kupfer- und Ummagnetisierungsverluste bei Leerlauf oft bereits über die Hälfte der Verlustleistung bei Nennbelastung. Mit zunehmender Belastung steigt der Wirkstrom durch Rr und damit im Kurzschlusskäfig an. Der Phasenwinkel zwischen Strom und Spannung verringert sich von nahezu 90° auf kleinere Werte. Bei hochmagnetisierten Asynchronmotoren findet mit ansteigendem Drehmoment sogar zunächst oft ein Rückgang des Gesamtstroms statt, welcher erst später mit steigendem Drehmoment dann wieder bis zum Nennstrom ansteigt. 

Von der Asynchronmaschine wird also mit "X" ein Blindstrom aufgenommen, welcher für die Magnetisierung der Maschine sorgt. Im Gegensatz zur Drehstrom-Synchronmaschine muss die magnetische Durchflutung in der Asynchronmaschine erst durch den Blindstrom in der Ständerwicklung aufgebaut werden.

Der belastungsabhängige Wirkstrom erzeugt einen Spannungsabfall im Käfig-Anteil des Rr, aber nur einen unwesentlich höheren Spannungsabfall in Rs. Folglich steigen die Verluste bei zunehmender Belastung im Läufer schneller an als im Stator. Der Kupferwiderstand Rs und der „Kupfer“-Widerstand vom Käfigläufer-Anteil von Rr verursachen mit dem Quadrat der Stromaufnahme steigende Verluste, daher nimmt der Wirkungsgrad der Maschine mit steigender Belastung ab. Dazu kommt deren Temperaturabhängigkeit, weshalb die Effizienz der warmen Maschine noch etwas sinkt.

Im Umrichterbetrieb wird bei immer kleiner werdender Frequenz der Blindwiderstand Xs ebenfalls immer kleiner. Bei Einhaltung des Nennstromes muss daher die vom Frequenzumrichter gelieferte Spannung sinken. Damit wird das Verhältnis des Spannungsteilers "R" zu "X" immer ungünstiger und Rs führt zu relativ zur verfügbaren Motorleistung steigenden Verlusten. Bei Dauerbetrieb kann dabei nur annähernd das Nenndrehmoment erzeugt werden, da die Kühlung von Läufer und Stator nicht ausreichend gegeben ist. Bei höheren als der Nenndrehzahl bzw. Nennfrequenz darf ein Asynchronmotor dagegen – unter Berücksichtigung der Isolation – an höheren Spannungen arbeiten und ist effektiver.

Moderne Frequenzumrichter können "R"/"R" selbst messen und sind damit in der Lage, sich selbst automatisch für einen beliebigen angeschlossenen Motor zu konfigurieren und ihn so vor Überlastung zu schützen.
Ein Haltemoment oder Drehzahlen nahe Null können mit einer Vektorregelung erreicht werden. Auch hier fehlt Kühlung, da das Lüfterrad am Läufer dann diesen selbst, die herausragenden Statorwicklungen und den Luftspalt nicht mehr kühlt.

Das Modell unterliegt der Voraussetzung eines rotationssymmetrischen Aufbaus der Maschine sowie dem Fehlen einer Streufeldreluktanz. Um diese kann das Modell erweitert werden. Sie wird hier jedoch (zunächst) nicht berücksichtigt, um das Modell möglichst einfach und verständlich zu halten. Gleiches gilt für die Windungszahl der Ständerwicklung.

Dabei werden die Einträge eines Vektors (x,y) in der Rotationsebene als komplexe Zahl x+iy dargestellt. Das Feld formula_1 sowie die Speisespannung formula_2 sowie der Statorstrom formula_3 sind die rotierenden Zeigergrößen des Ständers, formula_4 ist der Zeiger des Läuferstroms. 
Angeschlossen an die drei Phasen des Elektrizitätsnetzes kann der Zeiger formula_2 als formula_6 dargestellt werden. (Dreiecksschaltung)

Die Maschengleichung des Ständerkreises lautet unter Berücksichtigung des Induktionsgesetzes:

Da der Läufer vorwärts rotiert „sieht“ er das Magnetfeld rückwärts rotieren.

Somit ergibt sich die Maschengleichung des Läuferkreises in mitrotierenden Koordinaten:

Das Magnetfeld ist Ergebnis von Läufer- und Ständerstrom multipliziert mit der Hauptfeldreluktanz formula_11:

Ersetzt man formula_13 durch formula_14 ergibt sich das Gleichungssystem mit den Unbekannten formula_15 und formula_4.

Berücksichtigt man Streufeldreluktanzen in Form der Induktivitäten formula_20 und formula_21 sowie die Windungszahl formula_22 des Ständers erhält man sehr ähnliche Gleichungen:

Das erzeugte Drehmoment ergibt sich aus dem Kreuzprodukt von formula_1 und Läuferstrom. Hier wird das analog zum Zeigermodell in Komplexzahlenrechnung dargestellt.

Zur Glättung des Erregerfelds werden im Regelfall nicht alle Windungen einer Spule in einer Nut konzentriert, sondern in mehreren nebeneinander liegenden Nuten verteilt.

Durch diese Verteilung verringert sich die Spannungsamplitude der Grundwelle, was durch den Zonenfaktor berücksichtigt wird.

Als Sehnung wird bei einer Mehrschichtwicklung die Verschiebung der Wicklungslagen bezeichnet. Diese Verschiebung bewirkt eine Glättung der Erregerkurve und damit eine Reduzierung der Oberschwingungen der induzierten Spannung.

Durch die Sehnung verringert sich die induzierte Spannungsamplitude, was durch den Sehnungsfaktor berücksichtigt wird. Er berechnet sich zu

mit der Polpaarzahl formula_33, Anzahl Nuten formula_34 und dem Wicklungsschritt formula_35.
Dabei beschreibt der Wicklungsschritt formula_35 das Verhältnis von Spulenweite zu Nutteilung.

Das Produkt aus Sehnungs- und Zonenfaktor formula_37 wird als Wicklungsfaktor bezeichnet.

Die Begriffe Nennleistung, Nenndrehzahl und Nenndrehmoment ergeben sich aus den Angaben zu den technischen Daten des Motors und dem zugehörigen Typenschild. In diesem Zusammenhang wird auch von den Auslegungswerten gesprochen.

Das Nennmoment ist in der Regel nicht auf dem Typenschild vermerkt. Es kann aus nachstehender Formel errechnet werden. Siehe auch Leistung bei Technischen Anwendungen.


Die zugehörige Synchrondrehzahl (oder Drehfelddrehzahl) liegt immer knapp über der Nenndrehzahl, die sich aus
ergibt.

Bei 50 Hz ergeben sich so Werte von 3000, 1500 oder 750 Umdrehungen pro Minute mit den Polpaarzahlen 1, 2 oder 4.

Das gezeigte Beispiel für ein Typenschild bezieht sich auf einen Motor, der nur für den Sternbetrieb geplant ist. Bei einer Netzfrequenz von 50 Hertz und einer Nennleistung von 5000 kW und einer Nenndrehzahl von 1480/min ergibt sich:

Eine weitere Methode zur bildlichen Darstellung von Leistung, Drehmoment und Verlust einer Asynchronmaschine im Generator- und Motorbetrieb in Abhängigkeit vom Schlupf stellt der Ossanna-Kreis dar.

Das nebenstehende Bild zeigt den typischen Drehmomentenverlauf in Abhängigkeit von der Drehzahl. Im Dreiecksbetrieb hat der Motor im Vergleich zum Sternbetrieb etwa das dreifache Anzugsmoment. Die Betriebspunkte B oder B liegen jenseits des Kippmomentes K oder K.

Mit P (wie Pumpe) ist als Beispiel die Kurve für das erforderliche Drehmoment einer Kreiselpumpe eingezeichnet.

Es kommt darauf an, dass der Drehzahlbereich von Null bis zum Kipppunkt möglichst schnell durchfahren wird, denn in diesem Bereich hat der Motor einen schlechten Wirkungsgrad und erwärmt sich dementsprechend. Die (kritische) Anlaufzeit hängt von der Trägheit der Arbeitsmaschine und von dem Verhältnis der Anfahrmomente ab.

Das Beispiel zeigt, dass die Pumpe scheinbar auch in Sternschaltung problemlos läuft, denn die Betriebspunkte B1 und B2 liegen dicht beieinander. Dennoch ist es möglich, dass der Motor bei Dauerbelastung in Sternschaltung einen zu hohen Strom bezieht, um das von der Arbeitsmaschine geforderte Moment aufzubringen. Der Motor erwärmt sich dadurch stark, denn in die Berechnung der Wärmeverluste geht der aufgenommene Strom quadratisch ein. Eine Erwärmung über die vom Hersteller angegebene zulässige Temperatur verkürzt die Lebenszeit des Motors stark. Oft ist das geforderte Bemessungsmoment für den Betrieb in Dreieckschaltung aber so groß, dass der Motor es nicht in Sternschaltung aufbringen kann. Der Anlauf und die Umschaltung in Dreieckschaltung müssen also ohne Last erfolgen oder bis zu Lastmomenten, die der Motor noch in Sternschaltung bewältigen kann, ohne sich unzulässig hoch zu erwärmen.

In dem Beispiel ist das antreibende Drehmoment (Stern) im Anfahrbereich etwa zwei- bis viermal größer als das erforderliche Moment der Pumpe. Die Differenz ist der beschleunigende Anteil. Daher könnte hier ein Anlauf der Pumpe mit offenem Schieber erfolgen. Technischer Standard ist der Anlauf einer Pumpe mit geschlossenem Schieber. Dann ist das erforderliche Moment erheblich kleiner und der kritische Anlaufbereich wird schnellstmöglich durchfahren.

Lüfter mit langen Flügeln (z. B. in einem Kühlturm) haben ein großes Massenträgheitsmoment. Ferner ist der Anlauf nur unter Last möglich. Dadurch ergeben sich potentiell sehr lange Anlaufzeiten, so dass die Auslegung der Motor-Lüfter-Kombination sorgfältig erfolgen muss.





</doc>
<doc id="12690" url="https://de.wikipedia.org/wiki?curid=12690" title="Synchronmaschine">
Synchronmaschine

Eine Synchronmaschine ist eine Drehstrommaschine, in der der Rotor (auch: "Läufer") synchron mit dem Drehfeld des Stators läuft. Vom Prinzip her kann jede Synchronmaschine als elektrischer Motor und elektrischer Generator betrieben werden. Synchrongeneratoren dienen in der Energiewirtschaft in einem weiten Leistungsbereich der Bereitstellung von elektrischer Energie. Synchronmotoren finden vielseitigen Einsatz als Antriebsmaschinen in der Industrie, zum Beispiel als Antriebe für Fahrzeuge, Schiffe und Züge.

Die Synchronmaschine trägt ihren Namen wegen der Betriebseigenschaft, dass ihr Rotor exakt mit dem durch die Netzfrequenz vorgegebenen Drehfeld "synchron" umläuft. Das unterscheidet Synchronmaschinen von Asynchronmaschinen, deren Rotor dem Drehfeld im Motorbetrieb nach- und im Generatorbetrieb voreilt. Ein weiteres Unterscheidungsmerkmal ist, dass im Gegensatz zu Asynchronmaschinen für den Betrieb von Synchronmaschinen ein Erregerfeld benötigt wird. Bevor eine Synchronmaschine als Generator ans Netz geschaltet wird, muss sie mit dem Netz synchronisiert werden. Im Generatorbetrieb läuft die Maschine allgemein mit relativ konstanter Drehzahl. Synchronmotoren müssen dagegen oft in ihrer Drehzahl variabel sein. Um einen Synchronmotor stufenlos in der Drehzahl regeln zu können, wird Leistungselektronik wie z. B. Frequenzumrichter verwendet. Ein Drehgeber (Strichgeber, Resolver) erfasst im Betrieb ständig die Läuferstellungsänderung. Daraus ermittelt die Steuerungselektronik die tatsächliche Drehzahl. Bei Belastung läuft der Läufer des Synchronmotors dem Drehfeld im Winkel, dem Polradwinkel, hinterher. Im Generatorbetrieb ist der Polradwinkel positiv in Drehrichtung, eilt also vor. Synchronmaschinen können Blindleistung aufnehmen oder abgeben. Dadurch kann die Maschine zudem zur Blindleistungskompensation verwendet werden. Das Blindleistungsverhalten lässt sich über die Erregung beeinflussen.

Als Vorläufer der dreiphasigen Synchronmaschine kam ab Mitte des 19. Jahrhunderts der einphasige Wechselstromgenerator zur Versorgung von Beleuchtungsanlagen zum Einsatz. 1887 entwickelten Friedrich August Haselwander und der US-Amerikaner Charles Schenk Bradley unabhängig voneinander den dreiphasigen Synchrongenerator. Bei den Entwicklungen bildeten sich die Bauformen der Schenkelpol- und Vollpolmaschine aus. Ein Mitgründer der Brown, Boveri AG, Charles E. L. Brown, gilt als Erfinder des Walzenläufers, mit in Nuten am Umfang verteilter Erregerwicklung. 

Die Weiterentwicklung der Synchronmaschine hing stark mit dem Ausbau der elektrischen Energieversorgung im Rahmen der Elektrifizierung und dem Bedarf von immer leistungsstärkeren Generatoren zusammen. Zuerst entstanden Einzelpol- beziehungsweise Schenkelpolmaschinen, da diese geeignet waren, mit den langsamlaufenden Kolbendampfmaschinen als Antriebsmaschine, Elektrizität zu erzeugen. Als die Dampfturbinen die Kolbendampfmaschinen ersetzten, kamen die schnelllaufenden walzenförmigen Vollpolläufer zum Einsatz. Unabhängig davon wurden in der Industrie schon immer Synchronmaschinen eingesetzt, wenn eine konstante Antriebsdrehzahl oder Phasenschieberbetrieb benötigt wurde.

Synchronmaschinen werden in verschiedenen Bauformen ausgeführt. Sie werden als Außen- oder Innenpolmaschinen gefertigt. Beide Maschinentypen haben gemeinsam, dass sie wie alle Drehstrommaschinen über einen Läufer und einen Ständer verfügen. In jedem Fall wird eine Erregereinrichtung für den Betrieb der Maschinen benötigt. Außerdem erfolgt nochmals eine Unterteilung in Schenkelpol- und Vollpolmaschinen.

Die Ständerwicklung besteht aus drei um 120°/"p" ("p" = Polpaarzahl) versetzten Wicklungssträngen, die mit U, V und W bezeichnet werden. Sie sind in Stern- oder Dreieckschaltung verschaltet. Über die Ständerwicklung wird im Generatorbetrieb elektrische Energie ins Netz gespeist beziehungsweise im Motorbetrieb aus dem Netz bezogen. Der Aufbau des Ständers gleicht dem der Drehstrom-Asynchronmaschine. Der Ständer der Innenpolmaschine wird ebenfalls Anker genannt und die Ständerwicklung dementsprechend Ankerwicklung.

Der Läufer der Innenpolmaschine kann als Schenkelpolläufer oder Vollpolläufer ausgeführt sein. Rotor, Polrad und seltener Induktor sind ebenfalls Bezeichnungen für beide Läuferbauformen. Der Vollpolläufer wird zudem als Walzenläufer und Volltrommelläufer bezeichnet. Dieser ist rotationssymmetrisch aufgebaut und trägt die Erregerwicklung. Die Erregerwicklung ist in die Nuten des Vollpolläufers eingebracht und mit Nutkeilen befestigt. Schenkelpolläufer besitzen ausgeprägte Polschuhe und Schenkel, weswegen sie einen großen Durchmesser besitzen. Die Erregerwicklung ist auf die Schenkel des Läufers gewickelt.

Es gibt verschiedene Prinzipien der Erregung, beispielsweise die statische Erregung. Bei diesem Prinzip sind die Enden der Erregerwicklung über Schleifringe, die sich auf der Läuferwelle befinden, herausgeführt. Über Kohlebürsten wird die Erregerspannung an die Erregerwicklung gelegt. Ein anderes Prinzip ist die bürstenlose Erregung über Außenpol-Synchrongeneratoren und mitrotierende Diodengleichrichter (sog. "RG-Sätze"). Diese Technik wird aber infolge der steigenden dynamischen Anforderungen (Pendeldämpfungsgerät, engl. , PSS) nur noch selten in Kraftwerksneubauten eingesetzt, weil RG-Erregungen wesentlich langsamer als Schleifringerregungen reagieren.
Handelt es sich um eine permanentmagneterregte Synchronmaschine (PSM), trägt der Läufer Permanentmagnete zur Erregung. Die Permanentmagnet-Erregung gewinnt immer mehr an Bedeutung.
Die Hybridsynchronmaschine (HSM) hingegen vereint die Wirkung der elektromagnetischen Reluktanz und der Wirkung von Permanentmagneten zur Drehmomentbildung.

Größere Synchronmaschinen verfügen über eine Dämpferwicklung (Dämpferkäfig). Sie wirkt sich auf das Betriebsverhalten von Synchronmaschinen aus. Bei Vollpolmaschinen sitzt die Dämpferwicklung in den Nuten der Erregerwicklung oder zwischen diesen Nuten in gesonderten Dämpfernuten. Bei Schenkelpolmaschinen sitzt die Dämpferwicklung in gesonderten Dämpfernuten der Polschuhe. Die Dämpferwicklung bei Vollpolmaschinen ähnelt vom Prinzip her dem Aufbau des Kurzschlussläufers einer Asynchronmaschine. Synchronmaschinen können aber in Abhängigkeit von der Bauform ohne eine Dämpferwicklung eine Eigendämpfung aufweisen, die sich ebenfalls auf den Betrieb auswirkt.

Die wichtigste Aufgabe der Dämpferwicklung von Synchronmaschinen besteht darin, mechanische Pendelmomente zu dämpfen. Pendelmomente treten auf durch Asynchronbetrieb, an die Synchronmaschine angekuppelte Maschinen mit periodischem Drehmoment (z. B. Verbrennungsmotoren als Antriebsmaschine oder Kolbenkompressoren als Arbeitsmaschine) und Laststöße. Im unsymmetrischen Betrieb (Schieflast) und im Extremfall bei Einphasenbetrieb tritt ein inverses Drehfeld auf, das ebenfalls gedämpft wird. Ungedämpft hätte das inverse Drehfeld hohe Verluste zur Folge.

Für den Generatorbetrieb ist vor allem die Dämpfung der inversen Felder von Bedeutung. Inverse Felder verursachen einen Strom in der Dämpferwicklung, dessen Frequenz doppelt so groß wie die Netzfrequenz ist. Die Dämpferwicklung wird hierbei mit geringem Widerstand ausgeführt, um die Verluste gering zu halten.

Im Motorbetrieb sind vor allem Pendelmomente zu dämpfen. Bei Belastung mit einem konstanten Lastmoment besteht unter einem konstanten Polradwinkel ein Gleichgewicht zwischen dem durch die Last abgeforderten und dem durch die Maschine zugeführten Drehmoment (siehe auch Federmodell des Polradwinkels einer Synchronmaschine). Durch plötzliche Erhöhung des Lastmomentes (Laststoß) verzögert sich wegen des Massenträgheitsmoments des Läufers seine Drehbewegung über den Polradwinkel hinaus. Das Lastmoment ist nun kleiner als das Motormoment und das verursacht wiederum durch das Massenträgheitsmoment eine Beschleunigung bis zu einem zu geringen Polradwinkel. Dieses Pendeln wiederholt sich mit immer kleiner werdender Amplitude, bis wieder ein Gleichgewicht erreicht ist. Durch die Relativbewegung zwischen Ständerdrehfeld und Läufer wird nach dem Prinzip der Asynchronmaschine ein Drehmoment erzeugt, welches den Pendelbewegungen entgegenwirkt. Ähnlich wirken ebenfalls Massivteile des Läufers wie der massive Läuferballen der Vollpolmaschine oder die massiven Polschuhe der Schenkelpolmaschine. Das heißt, eine gewisse Dämpfung kann auch ohne Dämpferwicklung stattfinden. Neben dem Dämpfen von Pendelmomenten kann die Dämpferwicklung auch zum Selbstanlauf nach dem Prinzip des Asynchronmotors mit Käfigläufer dienen.

Vollpolmaschinen werden mit hohen Drehzahlen betrieben und eignen sich deshalb gut zum Einsatz als Turbogeneratoren. Die Läufer dieser Generatoren werden Turboläufer genannt. Sie sind mit wenigen Polpaaren ausgeführt und laufen bei 50 Hz Netzfrequenz mit bis zu 3000 min. Aufgrund der hohen Drehzahlen und der auf den Läufer wirkenden Kräfte müssen diese Generatoren schlank gebaut werden und sind wegen der Gefahr einer Überdrehzahl nicht leerlauffähig. Die Baugröße von Vollpolmaschinen ist im Durchmesser durch die Fliehkraftgrenze und in der Länge durch die Durchbiegegrenze beschränkt.

Schenkelpolmaschinen werden häufig als niedertourige Generatoren mit großen Durchmessern und geringer Länge eingesetzt. Sie sind mit großer Polpaarzahl ausgeführt und laufen mit Drehzahlen von 60 bis 750 min.

Im Ständer der Außenpolmaschine befinden sich ausgeprägte Polschuhe und Schenkel, welche die Erregerwicklung tragen. Auf dem Läufer, im Fall der Außenpolmaschine ebenfalls Anker genannt, befindet sich die dreisträngige Läuferwicklung. Die Enden der Läuferwicklung sind über Schleifringe herausgeführt. Kohlebürsten nehmen im Generatorbetrieb die bereitgestellte Leistung ab, oder führen die benötigte Leistung im Motorbetrieb zu. Diese Bauform eignet sich nicht für Maschinen mit großer Bemessungsleistung, da die Ströme in Abhängigkeit von der Leistung steigen. Damit verbunden sind der Anstieg der Verluste am Schleifringapparat und die Notwendigkeit, den Schleifringapparat größer auszuführen, um die Ströme tragen zu können. Für große Leistungen kommen Innenpolmaschinen zum Einsatz.

Die Wicklungen von Generatoren sind für die Höhe der Ströme auszulegen, die bei einem Kurzschluss auftreten. Das betrifft ebenso die Erregerwicklung, da auch in ihr im Kurzschlussfall hohe Stromspitzen auftreten. Der höchste Kurzschlussstrom tritt bei einem dreipoligen Klemmenkurzschluss auf, wenn sich die Maschine bei Bemessungsdrehzahl im Leerlauf befindet und bei Bemessungsspannung erregt ist. Laut DIN VDE 0530 darf der Kurzschlussstrom maximal das 21-fache des Effektivwerts und das 15-fache des Scheitelwerts des Bemessungsstroms betragen.

Die Ständerwicklungen und die Erregerwicklung von Synchronmaschinen erwärmen sich im Betrieb durch die auftretenden Ströme. Die Wärme muss abgeführt werden. Im unteren Leistungsbereich geschieht das z. B. über Kühlrippen des Ständers und mittels Lüfter. Dabei zirkuliert die Luft im Gehäuse und um die Wicklungen. Für die Zirkulation ist ein Lüfter auf der Läuferwelle angekuppelt. Möglich ist auch eine Fremdlüftung über einen externen Lüfter. Großgeneratoren erwärmen sich sehr stark. Die Wärme wird hier über eine Wasser- und Wasserstoffkühlung abgeführt. Dabei zirkuliert Deionat durch die als Hohlleiter ausgeführte Ständerwicklung. Anstelle von Luft befindet sich unter Druck stehender Wasserstoff im Gehäuse, das in dem Fall völlig dicht sein und sogar einer Knallgasexplosion standhalten muss. Aufgrund der großen Wärmeleitfähigkeit von Wasserstoff wird so eine wesentlich bessere Kühlung erreicht als mit Luft. In früher Vergangenheit wurden Prototypen mit supraleitender Erregerwicklung getestet. Ziel der Forschung ist u. a. die Erhöhung der Luftspaltdichte und des Strombelages. Mit der Technik soll ermöglicht werden, die aktive Masse der Maschine bei gleicher Leistung zu halbieren.

Hauptanwendungen der Synchronmaschinen sind die Drehstromgeneratoren in Kraftwerken. Fast die gesamte konventionelle Produktion elektrischer Energie erfolgt mit Synchrongeneratoren. In Wärmekraftwerken kommen Vollpolmaschinen mit Leistungen bis fast 2000 MVA und Ausgangsspannungen von 21 bis 27 kV zum Einsatz. Im Mülheimer Siemens-Werk wurde der weltweit größte Generator für das finnische Kernkraftwerk Olkiluoto gefertigt. Er hat eine Bemessungsscheinleistung von 1992 MVA. Diese Generatoren, mit ihren schnell umlaufenden Turboläufern, werden in Einheit mit den Turbinen Turbosätze genannt. Die langsamlaufenden Schenkelpolmaschinen in Wasserkraftwerken werden Wasserkraft- oder Hydrogeneratoren genannt und liefern bei maximal 25 kV Ständerspannung Leistungen bis zu 1000 MVA. Generatoren kleinerer Leistung von 10 kVA bis 10 MVA kommen in Kleinkraftwerken und Dieselgeneratoren zum Einsatz und sind meist ebenfalls als Schenkelpolmaschine ausgeführt. Synchrongeneratoren für Windkraftanlagen werden zurzeit mit bis zu 8 MW Leistung gefertigt. Hinzu kommt der Einsatz bei der Versorgung von lokalen Netzen. So findet der Synchrongenerator auch Verwendung bei der Bereitstellung von Elektroenergie zum Betrieb von Schienenfahrzeugen und Schiffsantrieben sowie wohl zukünftig auch von Straßenfahrzeugen. Eine Sonderbauform der Schenkelpolmaschine bildet die Klauenpolmaschine und kommt vor allem als Kfz-Lichtmaschine (Generator) zum Einsatz.

Drehstrom-Synchronmotoren großer Leistung dienen als Antrieb für Gebläse, Pumpen und Verdichter sowie teilweise als Bahnantriebe (SNCF BB 26000, TGV, AGV). Mit der Möglichkeit, die Drehzahlregelung über Frequenzumrichter vorzunehmen, verdrängte der Synchronmotor große Gleichstrommaschinen, aber auch Gasturbinen zum Antrieb von Turboverdichtern. Im Bereich kleiner und mittlerer Leistung kommen Motoren mit Permanentmagneten für Hilfs- und Fahrzeugantriebe zur Anwendung. Eine Anwendung im Bereich der Automatisierungstechnik stellt die Kombination von zwei Synchronmaschinen dar. Diese Kombination dient als Sensor und Aktuator zur Übermittlung von Winkelpositionen des Läufers und wird auch als Drehmelder oder als Drehmeldetransformator bezeichnet. Neben Synchronmaschinen werden als Drehmelder auch andere Maschinentypen eingesetzt.

Ein Beispiel für die Verwendung einer kleinen Synchronmaschine außerhalb der Energietechnik stellt der Synchronmotor in der Hammond-Orgel dar.

Die Wirtschaftlichkeit einer Maschine wird unter anderem durch die Anschaffungs- und Betriebskosten sowie den Wirkungsgrad bestimmt. Der Wirkungsgrad der Synchronmaschine (ca. 95…99 % in Abhängigkeit von der Baugröße und der nötigen Erregerleistung) liegt aufgrund der synchronen Strom- und Spannungsphasen generell über dem der Asynchronmaschine. Große Synchronmaschinen wie z. B. der Turbogenerator zählen damit zu den effizientesten Energiewandlern. Wegen der Erregereinrichtung der Synchronmaschine ist der Aufbau der Synchronmaschine komplexer als bei der Asynchronmaschine und damit auch teurer. Der Aufwand für die Steuerelektronik ist ähnlich hoch wie bei der Asynchronmaschine. Permanentmagneterregte Synchronmaschinen erreichen noch höhere Wirkungsgrade, da ihnen keine Erregerleistung zugeführt werden muss. Bei gleichbleibender Leistung und größerer Leistungsdichte verringert sich die Masse der Maschinen oder verringert sich die Baugröße. Generatoren dieser Bauart erreichen in Windkraftanlagen einen Wirkungsgrad von über 98 % und liegen damit über dem Wirkungsgrad von Maschinen gleicher Größe mit elektrischer Erregung. Permanentmagneterregung kommt nur bei Maschinen kleiner bis mittlerer Baugröße zum Einsatz. Die Kosten für die Magnete fallen bei größeren Maschinen immer mehr ins Gewicht, so dass die Wirtschaftlichkeit gegenüber Maschinen mit elektromagnetischer Erregung nicht mehr gegeben ist. Die komplizierte Montage der Magnete stellt außerdem einen großen Nachteil dar.

Es gibt diverse Hersteller für elektrische Maschinen und so folgt nur eine Auswahl von Herstellern mit einigen ihrer Produkte im Bereich der Synchronmaschinen:

Unter "Hochspannungsmotoren" oder "Hochspannungsgeneratoren" versteht man Maschinen mit Bemessungsspannungen größer 1 kV. Diese Bezeichnungen kommen zustande, da in den VDE-Vorschriften schon Spannungen größer 1 kV als Hochspannung bezeichnet werden.

Vorteile:

Nachteile:
Damit die Synchronmaschine als elektrischer Generator, also als Drehstrom-Synchrongenerator, arbeiten kann, ist ein Erregerfeld im Läuferkreis notwendig (Innenpolmaschine). Das heißt, durch eine gleichstromerregte Läuferwicklung (Erregerwicklung) oder einen Permanentmagneten muss ein magnetisches Feld (Erregerfeld) erzeugt werden, das in den Strängen der Ständerwicklung eine Ständerspannung formula_1 induziert. Die Stränge der Ständerwicklung sind zum Stern verkettet. Man erhält an den Generatorklemmen (Enden der Stränge U, V, W) eine Dreiphasenwechselspannung, also drei um 120° phasenverschobene Wechselspannungen. Die Ständerstrangspannung formula_1 (auch Klemmenspannung genannt) lässt sich mit Kenntnis der synchronen Reaktanz formula_3, dem Ständerstrom formula_4 und der Polradspannung formula_5 wie folgt berechnen:
Bei der Verwendung einer Erregerwicklung muss zur Erzeugung des Erregerfeldes Erregerleistung zugeführt werden. Dazu gibt es verschiedene Erregersysteme, beispielsweise die statische Erregereinrichtung oder die bürstenlose Erregereinrichtung. Um bei plötzlichem Lastabwurf Schäden am Generator zu vermeiden, ist bei größeren Maschinen eine eigene Entregungsschaltung vorgesehen.

Die Anzahl der Polpaare formula_7 und die Netzfrequenz formula_8 der Erregerwicklung ist ausschlaggebend für die Bemessungsdrehzahl formula_9 des Generators, die so definiert ist:
ist die Drehfelddrehzahl (bei Synchronmaschinen Drehfelddrehzahl = Rotordrehzahl) auf Minuten bezogen, gilt:
So laufen bei einer Frequenz der Ständerspannung von 50 Hz = 50/s · 60 s/min = 3000 min ein zweipoliger (formula_12) Generator mit 3000 min und ein vierpoliger (formula_13) Generator mit 1500 min.

Außerdem ist eine an die Generatorwelle angekuppelte Arbeitsmaschine notwendig wie z. B. ein Verbrennungsmotor oder eine Turbine, die den Läufer mit dem Erregerfeld rotatorisch antreibt. Das bedeutet, die Arbeitsmaschine führt dem Generator mechanische Leistung zu, die der Generator in elektrische Leistung wandelt. Die zugeführte mechanische und die abgegebene elektrische Wirkleistung erhält man rechnerisch wie folgt:
Diese Gleichung ist gültig für Stern- sowie Dreieckgeschaltete Maschinen. Wenn bezogene, nicht absolute Werte für die Berechnung werwendet werden, muss die 3 entfernt werden.

Werden die Verluste vernachlässigt, gilt formula_22. Beim realen Generator treten jedoch Hysterese- und Stromwärmeverluste sowie Reibungsverluste auf. Dividiert man die abgeführte elektrische Leistung durch die zugeführte mechanische Leistung, erhält man den Wirkungsgrad der Maschine, der immer kleiner als 1 ist, also unter 100 % liegt.

Die Flussverkettung formula_23 bildet den Zusammenhang zwischen Rotordrehzahl und Induzierten Polradspannung formula_5. 

Die Flussverkettung kann im Leerlaufversuch ermittelt werden. Dazu treibt man die Maschine mit einer bekannten Winkelgeschwindigkeit formula_26 an und misst die Spannung über einer der Phasen zum Neutralleiter (im Vereinfachten einphasigen Ersatzschaltbild entspricht es der Spannung formula_27). Wird eine zwölfpolige (formula_28) Maschine mit formula_29 angetrieben und eine Leiter-Sternpunkt Spannung von formula_30 gemessen ergibt sich eine Flussverkettung von formula_31.

"Zusammenfassung der Wirkungsweise:"

Die Ständerspannung formula_39 ist lastabhängig. Bei konstantem Erregerstrom formula_40 und konstanter Drehzahl ergeben sich verschiedene Kennlinienverläufe für kapazitive, induktive und ohmsche Lasten. Mit kapazitiver Last ergibt sich eine Spannungsüberhöhung, für ohmsche Last ergibt sich ein schwacher Abfall und für induktive Last ein starker Abfall der Ständerspannung. Um die Ständerspannung konstant zu halten, muss also der Erregerstrom entsprechend der Last geregelt werden. Die "Regulierkennlinie" stellt dar, wie der Erregerstrom entsprechend den verschiedenen Lasten geregelt werden muss: induktive Last bedarf einer starken Erhöhung des Erregerstroms, ohmsche Last einer schwachen. Um der starken Erhöhung der Ständerspannung bei kapazitiver Last entgegenzuwirken, muss der Erregerstrom stark gesenkt werden. Bei Generatoren in Großkraftwerken wird der Erregerstrom konstant gehalten. Hier erfolgt die Spannungsregelung mittels Stufenschalter der nachgeschalteten Maschinentransformatoren.

Synchrongeneratoren können bei asynchroner Netzschaltung Schaden nehmen, wenn keine Sicherheitseinrichtungen wirken. Eine Fehlsynchronisation eines Generators hat Ausgleichsströme zur Folge, die wiederum Drehmomente nach sich ziehen. Kleine Fehlsynchronisationen und damit verbundene Pendelungen (verursacht durch die Drehmomente) dämpfen die Dämpferwicklungen. Große Fehlsynchronisationen führen zu Schäden am Generator, da die damit verbundenen großen Drehmomente auf die Maschine und das Maschinenfundament wirken.
Soll ein Generator Energie in ein Verbundnetz speisen, so sind vor einer Synchronisation Synchronisations- oder auch Parallelschaltbedingungen zu erfüllen:
Für die Synchronisation stehen verschiedene Geräte und Schaltungen zur Verfügung (Hell- oder Dunkelschaltung, Synchronoskop), jedoch wird heute meist auf die automatische Synchronisation durch digitale Steuerungstechnik vertraut. Der Generator wird nach Herstellung der Synchronisationsbedingungen im Leerlauf ans Netz geschaltet und kann danach elektrisch belastet werden, also elektrische Leistung abgeben.

Für den Motorbetrieb ist wie beim Generatorbetrieb auch eine erregte Läuferwicklung (Erregerwicklung) oder ein Permanentmagnet notwendig, um ein Erregerfeld zu erzeugen. Außerdem muss über die Ständerwicklungen elektrische Energie zugeführt werden, damit der Drehstrom-Synchronmotor ein Drehmoment an der Welle abgeben kann. Die aufgenommene elektrische Leistung berechnet sich wie folgt:

Die abgegebene mechanische Leistung entspricht der aufgenommenen elektrischen Leistung, abzüglich des Verlustleistungsanteils formula_46, welcher aus Kupfer- und Eisenverlusten sowie aus Reibungsverlusten besteht.

Das Verhältnis von abgegebener mechanischer Leistung zu aufgenommener elektrischer Leistung drückt den Wirkungsgrad der Maschine aus.

Das vereinfachte Ersatzschaltbild der Synchronmaschine ist im Abschnitt Generatorbetrieb zu finden. Im Artikel Drehstrommaschine ist das Antriebsprinzip durch ein Drehfeld beschrieben, welches sowohl für Synchron-, als auch Asynchronmotoren gilt.
"Zusammenfassung der Wirkungsweise:"

Synchronmotoren mit geringer Dämpfung laufen nicht allein an. Der Läufer eines Synchronmotors besitzt in der Regel ein zu großes Massenträgheitsmoment, um dem Drehfeld aus dem Stillstand zu folgen. Deshalb muss die Motordrehzahl unbelastet in die Nähe der Drehfelddrehzahl gebracht werden. Dann wird die Erregung zugeschaltet und der Läufer des Motors wird in den synchronen Lauf gezogen. Danach kann der Motor belastet werden. Für den Anlauf stehen verschiedene Verfahren zur Verfügung:

Als Phasenschieberbetrieb wird eine Betriebsart der ans Netz synchronisierten Synchronmaschine bezeichnet, bei der fast ausschließlich Blindleistung aus dem Netz bezogen oder in das Netz abgegeben wird. Die Synchronmaschine wird dabei im mechanischen Leerlauf betrieben, die dabei trotzdem aufgenommene, vergleichsweise geringe Wirkleistung dient dazu, die Verluste wie thermische Verluste in den elektrischen Wicklungen oder Verluste zufolge der mechanischen Reibung in den Lagern abzudecken.

Durch Erhöhen oder Absenken des Erregerstroms wird die Höhe der ans Netz abgegebenen oder aus dem Netz aufgenommenen Blindleistung beeinflusst. Bei Übererregung wird induktive Blindleistung abgegeben (Verhalten wie Kondensator) und untererregt nimmt die Synchronmaschine induktive Blindleistung auf (Verhalten wie Spule). Die Abgabe induktiver Blindleistung entspricht einer Aufnahme von kapazitiver Blindleistung und umgekehrt. Die Synchronmaschine als Phasenschieber dient primär der Lastflusssteuerung in vermaschten Stromnetzen und sekundär der Blindleistungskompensation.

In der Regel wird eine Synchronmaschine im Phasenschieberbetrieb übererregt betrieben, da Energienetze meist mehr durch induktive als durch kapazitive Verbraucher belastet werden. Energienetze nehmen kapazitiven Charakter durch Leitungskapazitäten an, wenn nur wenige Verbraucher am Netz sind. In diesem Fall wird die Synchronmaschine im Phasenschieberbetrieb untererregt betrieben.

Es wird zwischen reinen "Phasenschiebern" und "Synchrongeneratoren im Phasenschieberbetrieb" unterschieden.

Phasenschieber sind speziell auf diese Funktion ausgelegt und besitzen als wesentliches Merkmal keine nach außen geführte mechanische Welle. Sie dienen ausschließlich, in Abhängigkeit von der Erregung, der Bereitstellung von induktiver oder kapazitiver Blindleistung im Versorgungsnetz. Phasenschieber finden primär zur Steuerung der Blindleistung in größeren Umspannwerken (Netzknotenpunkten) Anwendung oder werden in der Nähe von größeren Anlagen installiert, deren Blindleistung zu kompensieren ist. Sie können grundsätzlich als Schenkelpolmaschine oder Vollpolmaschine ausgeführt sein.

Synchrongeneratoren im Phasenschieberbetrieb sind herkömmliche Synchronmaschinen und befinden sich beispielsweise in Kraftwerken, die zeitweise und nach Bedarf als Phasenschieber betrieben werden. Beispielsweise laufen Synchronmaschinen in Pumpspeicherkraftwerken, die sich nicht im Pump- oder Generatorbetrieb befinden, im Leerlauf und können so im Phasenschieberbetrieb verwendet werden. Bei Gasturbinenkraftwerken wird der Generator im Phasenschieberbetrieb mittels mechanischer Kupplung von der Gasturbine getrennt, um zusätzliche Wirkleistungsverluste, verursacht durch Kompression in der Gasturbine, zu verhindern.

Betreibt man eine Synchronmaschine mit konstanter Netzspannung im Phasenschieberbetrieb, so lassen sich die nach ihrer Kurvenform benannten V-Kurven aufnehmen. Ändert man bei verschiedenen konstanten Wirkleistungen formula_58 den Erregerstrom formula_40 und über- oder untererregt man damit die Synchronmaschine und trägt man dann die sich ergebenen Ständerströme formula_4 auf, so erhält man die charakteristischen V-Kurven. Die mit Wirkstrom belastete Synchronmaschine kann zusätzlich so viel aus der Über- oder Untererregung folgenden Blindstrom übernehmen, bis der Bemessungsstrom erreicht ist.

Im Bild sind fünf Kurven mit den Minima P bis zu P zu sehen, die sich bei verschiedenen Wirk- zu Bemessungsleistungsverhältnissen P/P ergeben. In den Minima der Kurven wird nur Wirkleistung umgesetzt, links und rechts davon zusätzlich Blindleistung. Bei der Kurve mit dem Minimum P handelt es sich um reinen Phasenschieberbetrieb, dabei wird keine Wirkleistung umgesetzt.

Beim Erreichen der Stabilitätsgrenze fällt die Maschine im Motorbetrieb außer Tritt oder geht im Generatorbetrieb durch.

Mit der Stromortskurve lässt sich das Betriebsverhalten von Synchronmaschinen darstellen. Es lassen sich Aussagen zur Betriebsart, dem Erregergrad und der Betriebsstabilität einer Synchronmaschine treffen. Aus dem vereinfachten Ersatzschaltbild (R=0; siehe Generatorbetrieb) folgt die Formel für die Ständerspannung:
Daraus lässt sich der Ständerstrom ableiten:


</doc>
<doc id="12692" url="https://de.wikipedia.org/wiki?curid=12692" title="Bild (Mathematik)">
Bild (Mathematik)

Bei einer mathematischen Funktion "f" ist das Bild, die Bildmenge oder der Bildbereich einer Teilmenge "M" des Definitionsbereichs die Menge der Werte aus der Zielmenge "Y," die "f" auf "M" tatsächlich annimmt.

Häufig werden dafür auch die Wörter "Wertemenge" oder "Wertebereich" benutzt, die aber bei anderen Autoren zur Bezeichnung der ganzen Zielmenge "Y" verwendet werden.

Für eine Funktion formula_1 und eine Teilmenge formula_2 von formula_3 bezeichnet man die folgende Menge als das "Bild von M unter f:"

Das "Bild von f" ist dann das Bild der Definitionsmenge unter formula_5, also:

Im Allgemeinen nutzt man die übliche Mengennotation, um die Bildmenge darzustellen, in obigem Beispiel:



Wir betrachten die Funktion formula_15 (ganze Zahlen) mit formula_16.

Es sei formula_1 eine Funktion und formula_2 und formula_23 seien Teilmengen von formula_3:

Die Aussagen über Vereinigung und Durchschnitt lassen sich von zwei Teilmengen auf beliebige nichtleere Familien von Teilmengen verallgemeinern.



</doc>
<doc id="12693" url="https://de.wikipedia.org/wiki?curid=12693" title="Achsenmächte">
Achsenmächte

Als Achsenmächte bezeichnet man zunächst die Mächte der „Achse Berlin–Rom“, eine Bezeichnung, die auf eine Absprache zwischen Adolf Hitler und Benito Mussolini am 25. Oktober 1936 zurückgeht. Sie begründete eine Zusammenarbeit zwischen dem nationalsozialistischen Deutschland und dem faschistischen Italien. Die Kapitulation Italiens 1943 beendete diese Zusammenarbeit.
Im Zusammenhang des Zweiten Weltkriegs verstand man unter „Achsenmächte“ das Deutsche Reich und seine Bündnispartner Italien und Japan. Auf dem Höhepunkt ihrer Macht beherrschten die Achsenmächte und ihre Verbündeten große Teile Europas, Nordafrikas, Ostasiens und des Westpazifik.

Die Kriegsgegner dieses Bündnisses werden als die Alliierten unter den kriegführenden Staaten des Zweiten Weltkriegs bezeichnet, die damals global agierende Kriegskoalition trat als Anti-Hitler-Koalition auf.

Im November 1936 gelang es dem Deutschen Reich, Japan mit dem Antikominternpakt als weiteren Verbündeten zu gewinnen. Italien trat ihm im November 1937 bei. Die „Achse Berlin–Rom“ wurde durch den Stahlpakt (1939) auch zum förmlichen Bündnis. Seit dem Abschluss des auf Initiative Hitlers geschlossenen Dreimächtepakts (1940) zwischen dem Deutschen Reich, dem Kaiserreich Japan und dem Königreich Italien wurde – auch von den Vertragspartnern – von der „Achse Berlin–Rom–Tokio“ gesprochen.

Vom Auswärtigen Amt wurde von 1939 bis 1944 die propagandistische, aufwendig gestaltete und zweisprachig (deutsch/italienisch) erscheinende Zeitschrift "Berlin Rom Tokio" herausgegeben, die der Zusammenarbeit dieser drei Staaten publizistischen Ausdruck gab.

Die Bezeichnung "Achsenmächte" geht auf eine Rede Benito Mussolinis am 1. November 1936 zurück, worin er von einer „Achse Berlin–Rom“ sprach, der am 25. Oktober 1936 ein geheimer Freundschaftsvertrag zwischen Italien und dem Deutschen Reich vorausgegangen war. Mussolini erklärte, dass die beiden Länder eine „Achse“ bilden würden, um welche sich die anderen europäischen Staaten drehen würden.

Geschlossen wurde dieses Abkommen, als das faschistische Italien – Deutschland ursprünglich keineswegs freundlich gesinnt – sich aufgrund des Äthiopienkriegs der Kritik des Völkerbundes ausgesetzt sah, von Deutschland jedoch Beistand erhielt. Der Zusammenschluss Italiens mit Großbritannien und Frankreich von Stresa (April 1935) war nur von zweimonatiger Dauer. Hitler bezweckte mit diesem Bündnis, Großbritannien zu einer Annäherung an das Deutsche Reich zu bewegen, während Mussolini damit das Gegenteil beabsichtigte: Er schloss die Achse ab, eben um eine solche engere deutsch-britische Zusammenarbeit zu verhindern, die den italienischen Interessen in Afrika im Wege stehen konnte.

Die Achse war in erster Linie ein Element der Propaganda beider Länder. Auch nachdem sie mit dem Stahlpakt zu einem Militärbündnis ausgeweitet worden war, kam es kaum je zu einer konkreten Zusammenarbeit. Er wurde in Form eines Freundschafts- und Bündnisvertrages von den Außenministern beider Länder, Joachim von Ribbentrop und Galeazzo Ciano, im Beisein Hitlers am 22. Mai 1939 in Berlin unterzeichnet.

In der Praxis zeigte sich bald, dass die militärische und rüstungspolitische Zusammenarbeit zwischen Berlin und Rom trotz des Paktes von einem Konkurrenzverhältnis geprägt war - was die Forschung unter dem Schlagwort "Kooperation als Machtkampf" zusammengefasst hat. Nach dem italienischen Kriegseintritt, als das Bündnis sich im Krieg bewähren musste, wurde rasch deutlich, dass es den Achsenpartnern nicht gelang, effektiv zusammenzuarbeiten: 
"Italienische Wünsche und deutsche Hilfsangebote griffen selten ineinander. Es gelang nicht, die potentielle Kraft, welche die Koalition im Grunde bot, zum Einsatz zu bringen. Mythos und Realität des Bündnisses klafften radikal auseinander. Mit den italienischen Niederlagen drängten sich diese Mängel zunehmend in den Vordergrund. Die Sachzwänge des Koalitionskrieges brachten es mit sich, dass der vielfach beschworene Zusammenhalt des Bündnisses einerseits zusehends an Substanz verlor, während Deutschland und Italien auf der anderen Seite de facto zu einer 'vermeintlich alternativlosen Notgemeinschaft' zusammenwuchsen. Das ineffektive Angebot- und Nachfragespiel des Herbst 1940, in welchem die Koalitionspartner stets die Unterstützung boten oder forderten, die nicht gefragt oder genehmigt wurde, ist beispielhaft für das gegenseitige Misstrauen und die blockierte Zusammenarbeit innerhalb der 'Achse'."

Am 27. September 1940 schlossen die Achsenmächte den Dreimächtepakt:


Kurzzeitig erhielt der Vorname "Roberto" eine neue Bedeutung als Akronym von "Rom-Berlin-Tokio". Zuletzt betonten Achsenvertreter damit am 16. Januar 1942 ihre Partnerschaft und prägten diese Losung.

Im Selbstverständnis von US-Präsident Franklin D. Roosevelt handelte es sich bei der Auseinandersetzung mit den Achsenmächten Deutschland, Italien und Japan nicht nur um einen Konflikt zwischen den „Habenden“ ("beati possidentes") und den drei weltpolitischen und kolonialen „Habenichtsen“. Für ihn war es ein , um sowohl die „Neuen Ordnungen“ in Europa und Asien zu zerstören als auch die USA als zukünftige Weltmacht zu positionieren.

Länder, die sich im Dreimächtepakt mit dem nationalsozialistischen Deutschland verbündeten, gelten als Verbündete der „Achse“. Diesem Vertrag zwischen den Achsenmächten Deutschland, Italien und Japan vom September 1940 schlossen sich kurz darauf eine Reihe weiterer europäischer Länder an. Ungarn, Rumänien und die Slowakei traten noch 1940 bei, Bulgarien 1941. Jugoslawien unterzeichnete zwar am 25. März 1941 den Dreimächtepakt, ratifizierte diesen aber nach dem Staatsstreich vom 27. März nicht und wurde im Balkanfeldzug von den Achsenmächten zerschlagen. Der daraufhin gebildete Unabhängige Staat Kroatien trat im Juni 1941 dem Pakt bei. Während des Zweiten Weltkriegs war "Achsenmächte" die Bezeichnung für alle mit dem Deutschen Reich verbündeten Staaten.

Auch der „Unabhängige Staat Kroatien“ (NDH-Staat), ein nominell unabhängiger Vasallenstaat, der nach der deutschen Eroberung und Aufteilung Jugoslawiens entstanden war, galt als Mitglied der „Achse“. Kroatien war Mitglied des Antikominternpakts und befand sich bis zum Kriegsende am 8. Mai 1945 mit Deutschland in einem umfassenden Militärbündnis.

Thailand trat dem Dreimächtepakt zwar nicht bei, schloss aber 1942 ein militärisches Bündnis mit Japan und erklärte den USA und Großbritannien den Krieg.

Die Finnen bezeichnen sich ungern als ehemalige Verbündete des Deutschen Reichs. Vielmehr wollen sie als „gleichzeitig kriegführender Staat“ gesehen werden. Finnland beteiligte sich am deutschen Überfall auf die Sowjetunion. Finnisches Territorium durfte von der Wehrmacht als Aufmarschgebiet genutzt werden und die finnische Armee wurde gegen die sowjetischen Truppen mobilgemacht. Der Name dieses sogenannten Fortsetzungskriegs spielt auf den finnisch-sowjetischen Winterkrieg an.

Finnland wollte im Fortsetzungskrieg die im vorangegangen Winterkrieg verlorenen Territorien zurückgewinnen und auch Ost-Karelien erobern. Die Bombardierung finnischer Städte durch sowjetische Streitkräfte diente dann am 25. Juni 1941 als Begründung für die finnische Kriegserklärung und den Beginn der finnischen Offensive. Großbritannien erklärte Finnland nach wiederholten Ersuchen, die feindlichen Aktivitäten gegen die Sowjetunion einzustellen, am 6. Dezember 1941 ebenfalls den Krieg.

Finnland schloss sich nie dem Dreimächtepakt an, unterzeichnete 1941 allerdings den Antikominternpakt. Finnland lehnte es ab, seine Streitkräfte unter das Kommando eines gemeinsamen deutsch-finnischen Hauptquartiers zu stellen und bewahrte seine operative Unabhängigkeit. So wurde etwa die Teilnahme an der Leningrader Blockade und am Angriff auf die Murmanskbahn bei Louhi verweigert.

Mit dem insgesamt sechs Wochen währenden Ryti-Ribbentrop-Vertrag wandelte sich das Verhältnis zu einem formalen Militärbündnis, welches als deutsche Bedingung für die Gewährung dringend benötigter Waffen- und Luftunterstützung geschlossen wurde, als sich Finnland bei der Großoffensive der Roten Armee mit der Gefahr einer vollständigen sowjetischen Besetzung konfrontiert sah. Nachdem die Offensive mit Hilfe der deutschen Waffenlieferungen abgewehrt war, wechselte Finnland 1944 zu den Alliierten und ging unter sowjetischem Druck im Lapplandkrieg zum Angriff auf deutsche Streitkräfte über. Präsident Risto Ryti, der sein "persönliches" Ehrenwort für die Einhaltung des Bündnisses gegeben hatte, trat daraufhin von seinem Amt zurück. Sein Nachfolger im Amt, Präsident Mannerheim, erklärte Rytis Abkommen mit den Deutschen für nichtig. Nach Kriegsende drängte die Sowjetunion auf ein Gerichtsverfahren gegen Ryti und einige seiner Kabinettsmitglieder. In dem umstrittenen Prozess wurde Ryti schließlich "ex post facto" zu einer Gefängnisstrafe von zehn Jahren verurteilt.

Nach den Erfahrungen der deutschen Besatzung, die Belgien im Ersten Weltkrieg über sich ergehen lassen musste, war die gesamte Nation beim Gedanken an eine zweite deutsche Besatzung entsetzt, insbesondere, da die belgische Delegation bei der Pariser Friedenskonferenz 1919 wenig Einfluss hatte – auch wenn Belgien bei den Reparationen keineswegs zu kurz gekommen war. Mehr aus diesem Grund als aus politischen Erwägungen heraus war Belgien den Achsenmächten wenig freundlich gesinnt, obwohl sich später Tausende Belgier (sowohl Flamen als auch Wallonen) der Waffen-SS unter dem berüchtigten belgischen Faschisten Léon Degrelle freiwillig anschlossen.

Nachdem die Deutschen Belgien 1940 tatsächlich erneut besetzten, erlebten die niederländisch-sprachigen Flamen von den Deutschen eine gewisse Bevorzugung gegenüber den frankophonen Wallonen. Nach der deutschen Invasion der Sowjetunion erhielten flämische Freiwillige die Erlaubnis zum Beitritt in die Waffen-SS, wohingegen die Wallonen zunächst nur bei der deutschen Wehrmacht aufgenommen wurden.

Belgisch-Kongo stand hingegen auf der Seite der belgischen Exilregierung und beteiligte sich am Krieg gegen die Achsenmächte in Nord- und Ostafrika sowie in Südostasien.

Dänemark wurde von Deutschland am 9. April 1940 im Zuge der Operation Weserübung überfallen und blieb bis zum Kriegsende militärisch besetzt. Die Dänen hegten große Sympathien für die Alliierten, jedoch gab es auch Unterstützung für die Achsenmächte: Die von der deutschen Besatzungsmacht bis 1943 im Amt belassene Regierung trat dem Antikominternpakt bei. Über 6.000 dänische Staatsangehörige dienten bei der Waffen-SS an der Ostfront in verschiedenen Verbänden, davon 1.500 Angehörige der deutschen Minderheit in Dänemark (Zahlen von 1941). Im August 1943 beendete die dänische Regierung die Zusammenarbeit mit der Besatzungsmacht und trat zurück. Dänemark stand fortan nur noch unter deutscher Militärverwaltung, ohne weiterhin Partner der Achse zu sein.

Nach der französischen Kapitulation am 22. Juni 1940 wurde Marschall Philippe Pétain zum neuen Staatschef des sogenannten Vichy-Regimes ernannt. Die Waffenstillstandsbedingungen sahen die militärische Besetzung von mehr als 50 Prozent des französischen Gebietes vor, einschließlich der Hauptstadt Paris. Pétain verlegte den Regierungssitz in den Badeort Vichy in der unbesetzten „freien“ Zone.

Großbritannien befürchtete, dass die französische Marine in deutsche Hände fallen würde und beschlagnahmte sämtliche französischen Schiffe in Häfen unter britischer Kontrolle. Auch wurden bei einem britischen Angriff in Mers-el-Kébir am 3. Juli 1940 mehrere französische Kriegsschiffe versenkt. Nach diesem Angriff brach das Vichy-Regime sämtliche diplomatischen Beziehungen mit Großbritannien ab und zog eine Kriegserklärung in Betracht.

Die Vichy-Regierung übte die Kontrolle über die französischen Kolonialbesitzungen aus und wurde auch von den USA und der Sowjetunion diplomatisch anerkannt. Dem standen die "Forces Françaises Libres" entgegen, deren Exilregierung unter Charles de Gaulle sich in London befand.

Vichy-Frankreich zeichnete den Antikominternpakt von 1941 und entsendete französische Freiwilligenkräfte an die Ostfront. Vichy-beherrschte Kolonien wurden häufig als Aufmarschgebiete für Angriffe der Achsenmächte genutzt. Japan besetzte Französisch-Indochina, das dann Ausgangspunkt der Invasion von Thailand, Malaya und Borneo war.

Streitkräfte der Briten und der "Forces Françaises Libres" bekämpften Vichy-treue Truppen in Völkerbundmandat für Syrien und Libanon 1941 und Madagaskar 1942, US-amerikanische Einheiten beteiligten sich Ende 1942. Die deutsche Wehrmacht besetzte 1942 Südfrankreich und die Vichy-Kolonie Tunesien, nachdem die Vichy-Verteidigungskräfte von den Amerikanern und Briten überrannt und niedergekämpft worden waren.

Die Italienische Sozialrepublik ("Repubblica Sociale Italiana" – RSI) ersetzte 1943 das Königreich Italien als Mitglied der Achsenmächte. Am 25. Juli 1943 enthob König Viktor Emanuel III. in Übereinstimmung mit dem Faschistischen Großrat Benito Mussolini seines Amtes und ließ ihn festnehmen. Italien schloss sich den Alliierten an und erklärte Deutschland den Krieg. In einer spektakulären Kommandoaktion konnte Mussolini jedoch von einer deutschen Fallschirmjägereinheit unter Otto Skorzeny befreit werden.

Norditalien war von der Wehrmacht besetzt, und am 23. September 1943 rief Mussolini (ab 8. September von Hitler als Marionette eingesetzt) dort die Italienische Sozialrepublik („Duce-Italien“) aus. Dieser Staat, dessen Regierungssitz sich in Salò am Gardasee befand, schrumpfte flächenmäßig in dem Maße, in dem die westlichen Alliierten gen Norden vorrückten. Die faschistische Republik von Salò hörte Ende April 1945 auf zu existieren, als die letzten verbliebenen deutschen Streitkräfte auf italienischem Boden sich zurückzogen oder ergaben und sie schließlich kapitulierten.

Japanische Streitkräfte drangen am Morgen des 8. Dezembers 1941 auf thailändisches Gebiet vor. Anfangs leisteten die thailändischen Grenztruppen der Invasion Widerstand, jedoch befahl Feldmarschall und Premierminister Phibul Songkhram die Einstellung des Widerstandes. Am 21. Dezember 1941 wurde ein Militärbündnis mit Japan unterzeichnet, am 25. Januar 1942 folgte die thailändische Kriegserklärung an das Vereinigte Königreich und die Vereinigten Staaten von Amerika. Der siamesische Botschafter in den Vereinigten Staaten, Seni Pramoj, stellte seine Abschrift der Erklärung nicht zu. Auch wenn die Briten die Kriegserklärung erwiderten, indem sie ihrerseits Thailand den Krieg erklärten und es infolgedessen als feindliche Nation betrachteten, geschah dies bei den USA nicht. Die Seri-Thai-Bewegung entstand in dieser Zeit. Im Mai 1942 führten thailändische Streitkräfte ihre größte Offensive des Krieges durch und eroberten die Stadt Kengtung in Nordburma von der chinesischen 93. Division.

Weitere "Seri-Thai"-Strukturen wurden in Großbritannien und innerhalb Thailands aufgebaut. Königin Ramphaiphanee leitete die Sektion in Großbritannien, und der Regent Pridi Banomyong befehligte indirekt den weitaus größten Teil der Bewegung, den im Landesinneren, was "de facto" Hochverrat gleichkam. Unterstützt von Teilen des Militärs, gelang ab 1945 die Errichtung geheimer Flugplätze und Ausbildungslager und die Einschleusung alliierter Agenten.

Mit der fortschreitenden Dauer des Krieges regte sich in der thailändischen Bevölkerung Unmut über die japanische Besetzung. Im Juni 1944 trat Phibul Songkhram nach einer parlamentarischen Abstimmungsniederlage zurück. Die neue Zivilregierung versuchte, sowohl "Seri Thai" zu unterstützen als auch gleichzeitig ein gutes Verhältnis mit den Japanern aufrechtzuerhalten.

Nach Kriegsende verhinderte der maßgebliche Einfluss der USA, dass Thailand als Achsenmacht angesehen wurde, Großbritannien verlangte aber drei Millionen Tonnen Reis als Reparationszahlung und die Rückgabe von Gebieten der britischen Kolonie von Malaya, die während des Krieges und der Invasion annektiert worden waren. Ebenso musste Thailand annektierte Teile von Britisch-Burma, Französisch-Indochina, Französisch-Kambodscha und Französisch-Laos zurückgeben.

Mandschukuo war ein von Japan am 1. März 1932 errichteter Marionettenstaat in der Mandschurei.
Die staatliche Unabhängigkeit Mandschukuos von China wurde vom Völkerbund nicht anerkannt, was Japan veranlasste, aus dem Völkerbund auszutreten. Das Deutsche Reich, Italien und die Neuorganisierte Regierung der Republik China unter Wang Jingwei waren die einzigen bedeutenden Länder, die den von Japan abhängigen Staat diplomatisch anerkannten. Später folgten diese Länder: Costa Rica, El Salvador, Burma unter Ba Maw, Thailand, die Provisorische Regierung des Freien Indien von Subhash Chandra Bose und der Vatikan. 1945 wurde Mandschukuo von sowjetischen Truppen in der Operation Auguststurm besetzt und 1946 an die Republik China zurückgegeben.

Während des Zweiten Sino-Japanischen Krieges 1937–1945 wurde in Nanjing am 29. März 1940 ein kurzlebiger Staat von Wang Jingwei ausgerufen, der auch das Oberhaupt der von Japan kontrollierten Marionettenregierung wurde. Die Staatsembleme der "Regierung der Republik China" waren denen der Republik China und des heutigen Taiwan ähnlich. Nach der japanischen Niederlage am 9. September 1945 wurde das Gebiet der Herrschaft des nationalistischen und Chiang Kai-shek-treuen Generals Ho Ying-ching übergeben.

Zusätzlich wurden von den Japanern weitere kleine „unabhängige“ oder „autonome“ Staaten oder politische Gebilde in besetzten Gebieten des chinesischen Festlands eingerichtet, von der Inneren Mongolei bis nach Guangdong. Mengjiang war einer dieser weiteren Satellitenstaaten in Nordchina. Er wurde am 18. Februar 1936 im Osten der Inneren Mongolei gegründet, seit 1942 war er formal autonomer Teil Nanjing-China. Die Autonomie des Landes war rein theoretischer Natur, da die tatsächliche politische Machtausübung in den Händen der japanischen Besatzer blieb. Mengjiangs Staatsoberhaupt von Japans Gnaden war der mongolische Prinz Demchugdongrub.

Die Provisorische Regierung des Freien Indien ("Arzi Hukumat-e-Azad Hind") war eine Schattenregierung unter Subhash Chandra Bose. Ihr Wirkungskreis beschränkte sich auf jene Teile Indiens, die unter japanischer Kontrolle standen. Bose war ein indischer Freiheitskämpfer, der Gandhis Konzept des gewaltfreien Widerstandes nichts abgewinnen konnte.

Boses Aufstieg lagen mehrere Faktoren zugrunde:

Bose initiierte eine Massenbewegung gegen die Benutzung indischer Ressourcen und Soldaten für den Krieg und schloss mit den in Ostindien vorrückenden Japanern ein Bündnis. Bose und Anand Mohan Sahay, ein weiterer politischer Anführer, erhielten vom Chef der ultranationalistischen Geheimgesellschaft Gen’yōsha Tōyama Mitsuru und japanischen Militärberatern ideologische Unterstützung. Weitere achsen-freundliche indische Politgrößen waren Asit Krishna Mukherji, ein Freund Boses, seine Ehefrau und Wahl-Inderin Savitri Devi, der Pandit Rajwade von Pune und Rash Behari Bose, der Gründer der „Indischen Unabhängigkeits-Liga“. Bose erklärte am 21. Oktober 1943 Indien für unabhängig.

Nach der japanischen Besetzung der Andamanen und Nikobaren wurde Port Blair zur provisorischen Hauptstadt. Die „Provisorische Regierung des Freien Indien“ hielt sich bis zum 18. August 1945, als sie dann offiziell aufgelöst wurde. Während ihres Bestehens wurde sie von neun verschiedenen Staaten anerkannt: Deutschland, Japan, Italien, Kroatien unter Ante Pavelić, die Republik China unter Wang Jingwei, Thailand, Burma unter Ba Maw, Mandschukuo und die Philippinen unter dem de facto (und später auch de jure) Präsidenten José Laurel.

Unter König Ahmet Zogu befand sich das Königreich Albanien seit den 1920er Jahren im Einflussbereich Italiens. Schon vor dem Ersten Weltkrieg wurde die italienische Sprache an albanischen Schulen gelehrt, und nach dem Krieg befand sich das Land unter dem „Schutz“ einer Vielzahl italienischer Festungen.

Am 7. April 1939 marschierten italienische Truppen in Albanien ein, besetzten rasch das Land und zwangen König Zogu ins Exil. Fünf Tage nach der Invasion beschloss das albanische Parlament den Anschluss an Italien in Personalunion, indem die albanische Krone Viktor Emanuel III. angeboten wurde, der somit König von Italien, Kaiser von Äthiopien und auch König von Albanien war. Am 10. Juni 1940 folgte Albanien Italien in den Krieg gegen Großbritannien und Frankreich. Albanien diente als Aufmarschgebiet für die italienische Invasion Griechenlands 1941. Albanische Truppen nahmen am Überfall auf Griechenland teil, und albanische Freiwillige dienten später in der 21. Waffen-Gebirgs-Division der SS „Skanderbeg“ (albanische Nr. 1). 1941 erklärte Albanien auch den USA den Krieg.

Nachdem der italienische Versuch, Äthiopien im Ersten Italienisch-Äthiopischen Krieg 1895/96 zu erobern, gescheitert war, wurde das Kaiserreich von den Italienern im zweiten Krieg 1935–1936 schließlich okkupiert. Der Sieg wurde am 9. Mai 1936 verkündet und der italienische König Viktor Emanuel III. zum "Kaiser von Abessinien" gekrönt.




</doc>
<doc id="12694" url="https://de.wikipedia.org/wiki?curid=12694" title="Marxismus">
Marxismus

Marxismus ist der Name einer von Marx und Engels im 19. Jahrhundert begründeten Gesellschaftslehre. Ihr Ziel besteht darin, durch revolutionäre Umgestaltung anstelle der bestehenden Klassengesellschaft eine klassenlose Gesellschaft zu schaffen.

Der "Marxismus" ist eine einflussreiche politische, wissenschaftliche und ideengeschichtliche Strömung, die sowohl dem Sozialismus als auch dem Kommunismus zugerechnet wird. Als "Marxisten" werden seit der zweiten Hälfte des 19. Jahrhunderts die Anhänger von Karl Marx und Friedrich Engels bezeichnet. Im weiteren Sinne ist Marxismus eine Sammelbezeichnung für die von Marx und Engels entwickelte Wirtschafts- und Gesellschaftstheorie sowie für damit verbundene philosophische und politische Ansichten. Auch Personen und Denkrichtungen, die in spezifischer Weise an das Werk von Marx und Engels anschließen, werden zum Marxismus gerechnet.

Bekannte marxistische Strömungen sind der Orthodoxe Marxismus der frühen Sozialdemokratie (im Wesentlichen Ende des 19./Anfang des 20. Jahrhunderts), der Leninismus, der Marxismus-Leninismus, der Trotzkismus sowie verschiedene Formen des Westlichen oder Neomarxismus, darunter die Frankfurter Schule und der französische Strukturalistische Marxismus, der italienische Operaismus, der jugoslawische Titoismus und der Postmarxismus.

Seine theoretischen Wurzeln hat der Marxismus unter anderem in der kritischen Auseinandersetzung mit der klassischen deutschen Philosophie (Kant, Hegel, Feuerbach), der klassischen englischen Nationalökonomie (Smith, Ricardo), dem französischen Frühsozialismus (Fourier, Saint-Simon, Blanqui, Proudhon) sowie den Historikern der französischen Restauration (Thierry, Guizot, Mignet). Vor allem Engels, Karl Kautsky und Lenin, aber auch Plechanow, Labriola, Trotzki und Rosa Luxemburg haben die weitere Entwicklung des Marxismus nachhaltig beeinflusst. In einer zweiten Phase nach dem Ersten Weltkrieg bis zu den 68er-Bewegungen erfuhr der Marxismus eine weitere Ausdifferenzierung durch Karl Korsch, Georg Lukács, Antonio Gramsci, Ernest Mandel, André Gorz, Herbert Marcuse, Theodor W. Adorno und Louis Althusser.

Mit der Zeit entwickelten sich eine eigenständige marxistische Philosophie und in vielen Disziplinen der Wissenschaften mit gesellschaftlichem Bezug eigene marxistische Strömungen – wie beispielsweise eine marxistische Soziologie, eine marxistische Wirtschaftstheorie, eine marxistische Literaturtheorie oder in der Psychologie der Freudomarxismus.

Der Begriff „Marxismus“ war zunächst nicht Selbstbezeichnung einer Partei oder Gruppe, sondern wurde von außen an sie herangetragen. Schon in den 1850er Jahren gebrauchten Anhänger Weitlings den Begriff „Marxianer“. Innerhalb der Internationalen Arbeiterassoziation (1864–1876) kam es zu Konflikten zwischen Anarchisten („Bakuninisten“) und den dann von diesen so titulierten „Marxisten“. Zu dieser Zeit wurde der Begriff Marxist auch zunehmend von Unterstützern gebraucht. In den späten 1870er Jahren distanzierte sich Marx selbst von einer Jugendfraktion französischer Sozialisten um Paul Lafargue und Jules Guesde, die sich als Marxisten bezeichneten, da sich diese „Jungen“ nach seiner Ansicht zu entschieden gegen die Idee des Reformismus wandten. In diesem Zusammenhang hat Marx laut Engels gesagt, er selbst sei kein Marxist. Der Begriff „Marxismus“ lässt sich ab den 1880er Jahren feststellen. So z. B. in der 1882 erschienenen Schrift "Le Marxisme et l’lnternationale" von Paul Brousse.

Marx und Engels wiederum führten das Begriffspaar „Wissenschaftlicher Sozialismus“ als Alternative zu „Marxismus“ ein. Damit grenzten sie sich von anderen Staats- und Gesellschaftsentwürfen ab, die sie dem „Utopischen Sozialismus“ oder dem Anarchismus zuordneten. Allerdings gelang es Engels nicht, den Begriff „Wissenschaftlicher Sozialismus“ für ihre Anschauungen durchzusetzen. So finden sich von Engels nach Marx Tode viele Briefstellen, in denen er sich abschätzig über den Begriff „Marxismus“ und seine Vertreter äußert. In einem Brief an Lafargue 1890 äußert es sich über die jungen Akademiker innerhalb der SPD, die „alle in Marxismus machen“, jedoch eigentlich auf eine Karriere aus wären, „und von denen Marx sagte: ’Alles, was ich weiß, ist, daß ich kein Marxist bin!' Und wahrscheinlich würde er von diesen Herren das sagen, was Heine von seinen Nachahmern sagte: Ich habe Drachen gesät und Flöhe geerntet.“ An anderer Stelle schreibt er an Lafargue: „Wir haben Euch niemals anders genannt als ’the so-called Marxists’, und ich wüßte nicht, wie man Euch anders nennen sollte. Habt Ihr einen anderen, ebenso kurzen Namen, dann macht ihn bekannt, und wir werden ihn mit Vergnügen und ohne Umstände anwenden.“ Zugleich musste Engels jedoch zunehmend erkennen, dass sich der Begriff Marxismus wohl durchsetzen würde: „Nun, wir waren siegreich, wir haben der Welt bewiesen, daß fast alle Sozialisten in Europa 'Marxisten’ sind (sie werden darüber verrückt werden, daß sie uns diesen Namen gegeben haben!)“ So schrieb er in "Ludwig Feuerbach und der Ausgang der klassischen deutschen Philosophie" (Ausgabe von 1888): „Inzwischen hat die Marxsche Weltanschauung Vertreter gefunden weit über Deutschlands und Europas Grenzen hinaus und in allen gebildeten Sprachen der Welt.“ Und fügt später in einer Fußnote hinzu: „Ohne ihn [Anm.: Marx] wäre die Theorie heute bei weitem nicht das, was sie ist. Sie trägt daher auch mit Recht seinen Namen.“

Neben dem Ausdruck „Wissenschaftlicher Sozialismus“ konnten sich auch später gebildete Synonyme wie Dialektischer Materialismus, Historischer Materialismus, Philosophie der Praxis, Wissenschaftlicher Kommunismus oder Marxismus-Leninismus und ähnliche Wortgruppen nicht gegen die Bezeichnung „Marxismus“ durchsetzen.

Marx und Engels setzten sich mit verschiedenen Denktraditionen „wissenschaftlich-kritisch“ auseinander. Ihre Grundgedanken wurden erst nach ihrem Tod systematisiert. Eine solche Kanonisierung des Marxismus zu einer einheitlichen Lehre findet sich ansatzweise in den Schriften von Franz Mehring, Karl Kautsky, Antonio Labriola und Georgi W. Plechanow. Die Einordnung der Anschauungen von Marx und Engels in eine konsistente Theorie steht unter einem doppelten Vorbehalt:


Vor allem in den letzten Jahren ihrer Schaffensperiode führte zunehmend Engels vor allem mit Zeitungsartikeln eine öffentliche Auseinandersetzung mit Kritikern ihrer Theorien, und setzte sich für die Verbreitung ihrer Ideen in der Arbeiterbewegung ein. Im Gegenzug dazu arbeitete Marx – oftmals in gesundheitlich schlechter Verfassung und in seinen letzten Lebensjahren begriffen – an seinem ökonomischen Spät- und Hauptwerk "Das Kapital". Auf Grund ihrer engen Zusammenarbeit und der gegenseitigen Kenntnis ihrer Schriften ist anzunehmen, dass diese „Arbeitsteilung“ von beiden Seiten gewollt war.

Vor allem die „Orthodoxie“ der klassischen Sozialdemokratie und im Anschluss daran der Marxismus-Leninismus verstehen den Marxismus als theoretisches und praxisorientiertes System und als Weltanschauung. Die marxistische Theorie kann zum besseren Verständnis in drei große Kernbereiche unterschieden werden, die jedoch bei Marx und Engels untrennbar miteinander verflochten sind:


Um die Grundlagen des Marxismus besser zu verstehen, schlägt Lenin eine Einteilung der dafür wichtigsten theoretischen Auseinandersetzungen mit Denkern vor, die Marx und Engels wesentlich beeinflussten:


In seinem bekannten Essay "Drei Quellen und drei Bestandteile des Marxismus" schreibt Lenin:

Die konsequente Fortsetzung des Marxismus durch die Sozialdemokratie und den Marxismus-Leninismus ist umstritten. So lehnten Marx und Engels nationalistische Konzepte laut eigener Aussage ab. Gegenüber dem nationalstaatlichen Denken vieler Zeitgenossen vertraten sie internationalistische Positionen, während z. B. die deutsche Sozialdemokratie 1914 dem Krieg gegen das zaristische Russische Reich zustimmten. Nachdem der Kapitalismus mit seinem Weltmarkt ein international agierendes System ist, sei nach Marx und Engels auch seine vollständige Überwindung letztlich nur im internationalen Rahmen zu verwirklichen (Weltrevolution). Diese Ansicht wurde jedoch später vom Marxismus-Leninismus in den 1930er Jahren endgültig durch die Theorie vom Aufbau des Sozialismus in einem Land (Sowjetunion) verdrängt beziehungsweise zurückgestellt. Die Lage in Mittel- und Westeuropa wurde so eingeschätzt, dass die revolutionäre Bestrebung dort gescheitert sei, eine Weltrevolution war ausgeblieben. In der Kommunistischen Internationale ordneten sich alle Länder der neuen Doktrin unter.

Seit der Begründung des Marxismus durch Marx und Engels haben sich verschiedene marxistisch beeinflusste Richtungen entwickelt, die jeweils das Erbe der „Klassiker“ beanspruchten und sich voneinander abgrenzten. Heute firmieren unter der Bezeichnung „Marxismus“ sehr verschiedene Strömungen, die teilweise nur noch entfernt mit dem Fundament der Werke von Marx und Engels verbunden sind. Diese Strömungen des Marxismus wurden wiederum durch verschiedene Theoretiker vertreten und weiterentwickelt, die sich von unterschiedlichen Denkansätzen her dem vielschichtigen Werk von Marx und Engels genähert und eigene Strömungen des Marxismus begründet oder vorhandene Strömungen nachhaltig beeinflusst haben. Am stärksten innerhalb der universitären Wissenschaft verankert ist der Marxismus zurzeit in den USA (Stand: November 2006).

Der "orthodoxe Marxismus" der klassischen Sozialdemokratie (etwa bis zum Ersten Weltkrieg) orientierte sich eng an den Schriften von Marx und Engels. Mit der Spaltung der russischen Sozialdemokratie in Menschewiki und Bolschewiki und der Gründung des „marxistischen Zentrums“ (Zentrismus (Marxismus)) um Karl Kautsky Anfang des 20. Jahrhunderts spaltete sich der orthodoxe Marxismus in einen reformistischen und einen revolutionären Flügel. Letzter konzentriert sich als "revolutionärer Marxismus" auf die Weiterentwicklung und revolutionäre Umsetzung des Marxismus. Eine besondere Ausformung des orthodoxen Marxismus ist der Austromarxismus, der zwischen Sozialreform und Revolution schwankt und dadurch die Herausbildung (und Abspaltung) eines starken revolutionär-marxistischen Flügels im Österreich der Zwischenkriegszeit verhindern konnte.

Der "Revisionismus" bzw. "Reformismus" um Eduard Bernstein lehnte im Gegensatz zum orthodoxen Marxismus alle radikalen und revolutionären Aspekte des Marxismus ab und erachtete auf Grund der veränderten ökonomischen Bedingungen (Imperialismus) einen gemäßigten Weg zum Sozialismus als möglich. Spätestens nach der Spaltung der sozialdemokratischen Parteien in sozialistische und kommunistische Parteien nach dem Ersten Weltkrieg wurde der Revisionismus mit seiner politischen Praxis des Reformismus zur Hauptströmung innerhalb der Sozialistischen Internationale, deren Sektionen sich in den meisten Ländern inzwischen vollkommen von einer marxistischen Weltanschauung losgesagt haben.

Der "Sowjetmarxismus" oder "Marxismus-Leninismus" (ab 1924) (von Kritikern meist als Stalinismus bezeichnet) berief sich auf den orthodoxen Marxismus und beanspruchte, diesen an die neuen Gegebenheiten (Imperialismus und Monopolkapitalismus) angepasst zu haben. Denselben Anspruch erhebt der "Trotzkismus", der mit seiner Theorie der permanenten Revolution die Theorie vom Sozialismus in einem Land ablehnt und eine kritische Distanz zum Realsozialismus bewahrt. Sowohl der Marxismus-Leninismus als auch der Trotzkismus sehen sich in der Nachfolge der Bolschewiki unter Lenin. Auf den Marxismus-Leninismus beriefen sich auch viele Befreiungsbewegungen in der „Dritten Welt“, aus denen sich oftmals eigenständige politische Systeme entwickelten, wie zum Beispiel die heute noch bestehenden Systeme Chinas (Maoismus), Nordkoreas (Chuch’e-Ideologie), Kubas oder Vietnams.

"Westlicher Marxismus" und "Neomarxismus" sind Sammelbegriffe für Theorien insbesondere der Neuen Linken, die in Abgrenzung zum Realsozialismus versuchen, die Kernaussagen des Marxismus an die inzwischen geänderten sozialen und ökonomischen Bedingungen anzupassen. Es existieren hier die verschiedensten Ausformungen wie zum Beispiel jene der britischen "New Left"-Gruppe (E. P. Thompson, Perry Anderson), eine der frühesten nach dem Ungarn-Aufstand entstanden, des Reform- und Eurokommunismus westeuropäischer kommunistischer Parteien, des italienischen Operaismus sowie der Frankfurter Schule. Unter dem Begriff des Postmarxismus versammeln sich die Antideutschen und die Wertkritiker. Gelegentlich wird auch der Titoismus zum Neomarxismus gezählt. Zentral für den Neomarxismus waren die Schriften von Karl Korsch, Antonio Gramsci, Georg Lukács, Ernst Bloch, Ernest Mandel, Louis Althusser, Roman Rosdolsky, Leo Kofler und anderer.

Obwohl Marx und Engels in erster Linie eine "Philosophiekritik" und "Ideologiekritik" betrieben, welche die Emanzipation des Menschen anstrebte, wird der Marxismus selbst gelegentlich als humanistisch geprägte philosophische Lehre verstanden. Erkenntnis- und wissenschaftstheoretisch ist der Marxismus von zwei wesentlichen Elementen geprägt: Von der "Dialektik" Hegels und vom erkenntnistheoretischen "Materialismus" (Feuerbachs). Lenin bezeichnet den Materialismus als die "Philosophie des Marxismus". Marx hat bereits 1845 die Philosophen in seinem berühmt gewordenen Satz kritisiert:

Im Gegensatz zum philosophischen Idealismus vertritt der Marxismus die Ansicht, dass alle Ideen, Vorstellungen und Gedanken aus der komplexen, insbesondere gesellschaftlichen Realität und den sie beinhaltenden Machtverhältnissen erwachsen, die sich „in letzter Instanz“ aus den jeweils historisch-geographischen Produktionsverhältnissen und materiellen Gegebenheiten entwickeln würden. Marx und Engels übernahmen – von den Junghegelianern beeinflusst – das materialistische Weltbild Feuerbachs, und ergänzten aus dem Werk Hegels die Dialektik und den damit verbundenen Gedanken ständiger Entwicklung.

Marx und Engels überwanden somit die in ihren Augen einseitige Sichtweise der mechanischen Materialisten, die die Welt als "unveränderlich" verstanden. 1843 übernimmt Karl Marx von Hegel die Denkfigur der Dialektik sowie die Annahme einer Gesetzmäßigkeit der Geschichte. Diese führt er jedoch anders als Hegel nicht auf die Entfaltung des „Weltgeists“ zurück, sondern auf materielle, soziale Bedingungen und Auseinandersetzungen innerhalb der Gesellschaft.

Lenin bezeichnet die philosophischen Anschauungen von Marx und Engels als dialektischen Materialismus, obwohl sie diesen Begriff selbst nicht benutzten. Lenin bezeichnet die materialistische Dialektik von Marx und Engels als

In der Entdeckung des Radiums, des Elektrons, sowie der Verwandlung der Elemente sieht Lenin eine Bestätigung dieser Ansichten, die das idealistische Postulat des ewigen Stillstands widerlegen würden. Nach der Hegelschen Dialektik ist das Abbild der Welt im tätigen Begreifen ihrer Zusammenhänge von aufeinander bezogenen Gegensätzen – Thesen und Antithesen – geprägt, die sich gegenseitig im dialektischen Dreischritt zu Synthesen vorwärtsentwickeln. Diese Synthesen treiben die „objektive Wirklichkeit“ voran und „bestimmen“ damit die Zukunft, bis diese keine Widersprüche mehr enthält und im Begriff des „Absoluten“ „aufgehoben“ ist. Für den idealistischen Philosophen ist dieser Fortschritt, der die materielle Welt insgesamt durchwirkt, ein Produkt des menschlichen Geistes, der im Begreifen seiner selbst mit dem absoluten „Weltgeist“ identisch wird.

Marx betrachtet die Hegelsche Dialektik aus Sicht des Materialismus: Er stellt sie „vom Kopf auf die Füße“ und postuliert, dass sich die objektive Wirklichkeit aus ihrer materiellen Existenz und deren Entwicklung erklären lässt und nicht als Verwirklichung einer göttlichen absoluten Idee oder als Produkt des menschlichen Denkens.

Das Universum wird wie in der universalhistorischen Philosophie Hegels als eine Totalität, also als objektiv zusammenhängendes Ganzes gesehen. Aber Marx versteht die im Idealismus bloß geistigen Gegensätze als Ausdruck und Abbild realer, materieller Gegensätze: Auch diese hingen gegenseitig voneinander ab und befänden sich in einer ständigen Bewegung wechselseitiger Beeinflussung. Diese sei insgesamt aufsteigend, d. h., sie komme im Ganzen vom Einfachen zum Komplexen und durchlaufe dabei bestimmte Ebenen, denen bestimmte qualitative Veränderungen entsprächen, so dass sie die Entwicklung vorantrieben.

Eine objektive Realität existiert nach dieser Sichtweise auch außerhalb und unabhängig vom menschlichen Bewusstsein in den materiellen Bewegungen, auf die jedoch die Menschen (selbst ein Teil des Materiellen) "bewusst" zurückwirken. Dies bedeutet aber keineswegs, dass die Menschen ihre Umwelt objektiv richtig erfassen; Marx und Engels wollen gerade der ideologischen Selbsttäuschung, dem "falschen Bewusstsein" von der Umwelt, daher der Problematik der Subjekt-Objekt-Spaltung, entkommen:

Diese Annahme erfährt ihre stärkste Wirkung, wenn man über zukünftige gesellschaftliche Entwicklungen Überlegungen anstelle; in diesem Sinne wird jeglichem Utopismus eine Absage erteilt. Nach einer materialistischen Weltanschauung muss „die Produktion und Reproduktion des wirklichen Lebens“ das „bestimmende Moment in der Geschichte“ werden, die Arbeit daher eine zentrale Kategorie für das Individuum selbst und die gesellschaftliche Entwicklung sein. Daher werden alle Gesellschaftsordnungen maßgebend durch ökonomische Bewegungsgesetze bestimmt:

Die Konsequenz dieser Sichtweise ist eine umfassende Kritik an Religion, Recht und Moral. Diese begreift Marx als Produkte der betreffenden materiellen Verhältnisse, deren Wandel auch sie unterworfen sind. Religion, Recht und Moral hätten also nicht die universelle Gültigkeit, die sie beanspruchen.
Der historische Materialismus ist die Anwendung der Leitsätze des dialektischen Materialismus auf die Erforschung der Gesellschaft und ihre Geschichte.
Demnach lässt sich auch die Entwicklung einer Gesellschaft wissenschaftlich erklären: durch den Klassenkampf befinden sich die sozialen Verhältnisse zwischen den Klassen in einer ununterbrochenen Bewegung. Die Produktivkräfte (Arbeitskräfte und Produktionsmittel) entwickeln sich im Laufe der Zeit, bis sie mit den Produktionsverhältnissen (Arbeitsteilung und Besitzverteilung) in Widerspruch geraten. Marx sieht die Produktionsverhältnisse als „Fesseln“, welche ein Hindernis für die weitere Entwicklung der Produktivkräfte bilden. Die Unterklassen sind stets darauf bedacht, die Produktionsverhältnisse zu ihrem Vorteil zu verändern. Dies hat zur Folge, dass neue herrschende Klassen zustande kommen und der Klassenkampf erneut anfängt.

Marx unterscheidet zwischen folgenden geschichtlichen Entwicklungsstufen der Gesellschaft:

Nach der Überwindung des Kapitalismus folgen zwangsläufig:

Die Geschichte einer Gesellschaft ist eine (naturgesetzliche) Entwicklung vom Einfachen zum Komplexen, von Niederem zu Höherem. Deshalb sei der Kommunismus zukünftig unvermeidbar. Der Kapitalismus führe nach Marx´ Ansicht in immer größere Krisen. Die sozialistische Gesellschaft wird demzufolge die kapitalistische Gesellschaft ersetzen, genauso wie die kapitalistische Gesellschaft die feudale Ordnung ersetzt habe. Der Klassenkampf ende erst in der kommunistischen Ordnung, in der der Gegensatz von Herr und Diener aufgehoben sei.

Nachdem mit dem dialektischen Materialismus eine erkenntnistheoretische Position entwickelt wurde, und mit dem Historischen Materialismus eine "allgemeine" Geschichts- und Gesellschaftstheorie, war Marx seiner Analyse der gegenwärtigen, "konkreten" Gesellschaft bedeutend näher gekommen. Der nächste notwendige Schritt war nun für ihn, die ökonomischen Bewegungsgesetze in kapitalistischen Gesellschaften zu studieren, da nach der Theorie des historischen Materialismus die Produktionsweise einer Gesellschaft bedeutend für ihre Entwicklung ist. Herzstück seines Werks ist die "Kritik der politischen Ökonomie" in den drei Bänden des "Kapitals". Die Gesetzmäßigkeiten der Ausbeutung im herrschenden Kapitalismus, die Entstehung der modernen Klassengesellschaft und der Konzentrationsprozess des Kapitals werden sowohl mikro- wie makroökonomisch differenziert analysiert. Dabei griff Marx auf Vorarbeiten der Nationalökonomie, z. B. von Adam Smith und David Ricardo, zurück. Werttheorie, Verelendungs- und Krisentheorie sind wichtige Bestandteile dieser Analyse.

Das von Marx und Engels entworfene Theoriegebäude war und ist Bezugspunkt für verschiedenste politische und wissenschaftliche Denkrichtungen.
Praktische Anwendung fand der Marxismus zuerst in der Arbeiterbewegung des 19. Jahrhunderts, vor allem der deutschen Sozialdemokratie, welche die Theorien von Marx und Engels zur Grundlage ihrer ersten Programme und Mitgliederschulungen machte. Sodann entwickelte Lenin im Anschluss an Marx seine Imperialismustheorie, die nach der Oktoberrevolution 1917, zusammen mit den Ideen von Marx und Engels, zur neuen Staatsideologie der Sowjetunion wurde. Lenin verstand sich selbst jedoch nicht als Begründer einer neuen Strömung, sondern als Verteidiger des Marxismus. Nach Lenins Tod sprach man dann aber allgemein vom Leninismus, der einen an die russischen Verhältnisse angepassten Marxismus darstellt. Später veränderte Josef Stalin den Leninismus mit der Theorie des „Sozialismus in einem Land“ zum sogenannten Konstrukt des Marxismus-Leninismus.

Dieser Marxismus-Leninismus bestimmte den so genannten real existierenden Sozialismus nach 1945 in weiten Teilen der Welt, vor allem in Ost- und Mitteleuropa, und wirkte auch mit starkem Einfluss auf China, Kuba, Nordkorea, oder Vietnam. Ob und wie weit dieser sich noch aus den Grundideen der „Klassiker“ herleiten lässt oder eine „Fehlentwicklung“ darstellt, ist eine der umstrittensten Fragen innerhalb der marxistischen Theoriebildung. Die praktische Politik dieser Länder wird insbesondere in Nordkorea bis heute vom Stalinismus beherrscht. Heute wird das Gulag-Regime weitgehend als totalitäres System eingeordnet und von fast allen Marxisten abgelehnt.
Gegen die unterschiedlichen Ideologien von Stalin und Mao beansprucht auch der von Leo Trotzki entwickelte Trotzkismus mit seiner Theorie der „permanenten Revolution“ das wahre Erbe von Marx bzw. Lenin.

In Abgrenzung zu Stalinismus und Faschismus entstanden seit den frühen 1930er Jahren die Arbeiten der Frankfurter Schule, die versuchten, die Ideen von Marx auf die veränderten politisch-ökonomischen Bedingungen der Moderne anzuwenden und teils mit der Psychoanalyse zu verbinden.

Aus den Befreiungsbewegungen in der „Dritten Welt“ entwickelten sich oftmals politische Systeme, wie zum Beispiel die heute noch bestehenden Systeme Chinas (früher Maoismus), Vietnams oder Kubas.

In den 1960er Jahren entstanden besonders im Zusammenhang mit der weltweiten Studentenbewegung, den westeuropäischen Arbeiterstreiks und den so genannten Befreiungsbewegungen in der „Dritten Welt“ verschiedene Formen des Neomarxismus, des Eurokommunismus (insbesondere des Operaismus und Titoismus) und des demokratischen Sozialismus.


Die Schriften von Karl Marx und Friedrich Engels sind bis heute theoretisches Gerüst für verschiedene Organisationen und Parteien in allen Teilen der Welt.

In vielen Staaten Europas formierten sich erst kleinere Organisationen und daraus später, Parteien, deren Geschichte Parallelen aufweist. Mit Aufkommen des Nationalsozialismus wurden viele Organisationen aufgelöst und in den Widerstand gedrängt, nach 1945 befanden sich marxistische Organisationen vor allem in einer Auseinandersetzung mit der pluralistischen Demokratie des Westens und der Sozialdemokratie auf der einen Seite, und dem „Realsozialismus“ und der KPdSU auf der anderen. Nach dem Zerfall der Sowjetunion entwickelte sich vornehmlich in Russland ein postsowjetischer Marxismus.

Seit der Veröffentlichung der ersten marxistischen Schriften formierte sich Kritik an fast jedem Teilbereich der Theorie und auch an Wissenschaftlern, die im Marxismus begründete Methoden anwenden. Marx selbst war Kritik gegenüber offen: „Jedes Urteil wissenschaftlicher Kritik ist mir willkommen.“ Zum Beispiel gibt es nicht ganz widerspruchslose Betrachtungen über gesellschaftliche Voraussetzungen für eine sozialistische Revolution. In Marx’ Brief an Wera Sassulitsch (1881) bezog sich Marx auf die Situation im damaligen Russland, welches als rückständiges Agrarland angesehen wurde, in dem es noch keine große Anzahl von Industriearbeitern gab. Betrachtet wurde dabei die russische Dorfkommune, in der bereits Gemeinbesitz vorherrschte, die Marx unter Vorbehalt als möglichen „"Stützpunkt der sozialen Wiedergeburt Rußlands"“ betrachtete. Das Proletariat sollte nach Marx jedoch im Normalfall Wegbereiter einer Revolution sein, davon nahm er auch nie Abstand. Bekanntlich ereignete sich später (1917) in Russland mit der Oktoberrevolution eine Revolution, die sich gegen die kapitalistische Klassengesellschaft richtete, und von Lenin und den Bolschewiki, die sich als "Vorhut der Arbeiterklasse" verstand, angeführt wurde. Allerdings galt Russland zu dieser Zeit weiterhin als ein überwiegendes Agrarland.
Marx schlussfolgerte nicht erst, aber verstärkt, nach den Erfahrungen der Pariser Kommune (1871), dass das Proletariat die Eroberung der politischen Macht anstreben solle und dafür die Konstituierung politischer Parteien notwendig sei. Dazu kam Marx ebenfalls aus den Erfahrungen der Pariser Kommune zu der Erkenntnis, dass „die Arbeiterklasse nicht die fertige Staatsmaschine einfach in Besitz nehmen und sie für ihre eigenen Zwecke in Bewegung setzen kann“ und in "Der achtzehnte Brumaire des Louis Bonaparte" (1852) hatte er bereits geschrieben: „Alle Umwälzungen“ [= der Gesellschaft] „vervollkommneten diese Maschine statt sie zu brechen.“ Manche Formulierungen bei Marx sind also nicht eindeutig. Nach Lenins Interpretation bestand „Der Marx’sche Gedanke […] gerade darin, dass die Arbeiterklasse ‚die fertige Staatsmaschine‘ ZERSCHLAGEN, ZERBRECHEN muss und sich nicht einfach auf ihre Besitzergreifung beschränken darf. […] In diesen Worten: ‚die bürokratisch-militärische Maschinerie zu zerbrechen‘, ist“, nach Lenins Interpretation, „kurz ausgedrückt, die Hauptlehre des Marxismus von den Aufgaben des Proletariats in der Revolution gegenüber dem Staat enthalten.“ Marx machte keine konkreten Angaben zur politischen Ordnung einer kommunistischen Gesellschaft. Die Kritik am Marxismus hat sich im 20. Jahrhundert im Laufe der Entstehung der sich auf Marx berufenden Staatssysteme verschärft. Sie greift vor allem inhumane Politik und ökonomische Ineffizienz im „Realsozialismus“ als Ergebnis marxistischer Theorie an. Neomarxistische Kritiker dagegen wenden die marxsche Theorie auf diese Systeme selber an, um ihre Entwicklung und das praktische Scheitern der behaupteten Gesellschaftsziele zu erklären.








</doc>
<doc id="12695" url="https://de.wikipedia.org/wiki?curid=12695" title="Central Intelligence Agency">
Central Intelligence Agency

Die Central Intelligence Agency, offizielle Abkürzung CIA, ist der Auslandsgeheimdienst der Vereinigten Staaten. Im Gegensatz zu den anderen US-Nachrichtendiensten, insbesondere der National Security Agency (NSA), liegt der Schwerpunkt der CIA eher in der Beschaffung von Informationen von und durch Menschen (Human Intelligence, "HUMINT") als durch Technik (Signals Intelligence, "SIGINT").

Die CIA ist ein ziviler Geheimdienst. Im Gegensatz zu einem Nachrichtendienst, dessen Aufgabe die reine Gewinnung von geheimen Informationen ist, gehören zu den Aufgaben der CIA nicht nur Spionage, Beschaffung und Analyse von Informationen über ausländische Regierungen, Vereinigungen und Personen, um sie den verschiedenen Zweigen der amerikanischen Regierung zur Verfügung zu stellen, sondern auch Geheimoperationen im Ausland. Nicht selten bedient sich die CIA, so wie andere Geheimdienste auch, der Desinformation und illegaler Mittel, um die internationale Politik, die öffentliche Meinung und die Repräsentanten der Vereinigten Staaten zu beeinflussen.

Im Gegensatz zur National Security Agency (NSA) liegt der Beschaffungsschwerpunkt der CIA weniger auf technischer Informationsgewinnung "(Signals Intelligence)", als primär auf der Nutzung menschlicher Quellen "(Human Intelligence)".

Die CIA darf auf Weisung des US-Präsidenten durch verdeckte Operationen politische und militärische Einflussnahme im Ausland betreiben. Die zuständige Abteilung ist der National Clandestine Service, ehemals Directorate of Operations. Die Behörde ist für zahlreiche Fälle bekannt, bei denen aktiv in innere Angelegenheiten fremder Länder eingegriffen wurde (siehe "Bekannte Operationen"). Die Grenze zwischen ihren operierenden Agenten und Kombattanten ist dabei fließend. Ein bekanntes Beispiel ist die fehlgeschlagene Invasion in der Schweinebucht im Jahre 1961. Diese Praxis wird sowohl von Beobachtern in den USA als auch international scharf kritisiert.

Aus Dokumenten, die der Nachrichtendienst auf Geheiß seines Direktors Michael Hayden am 26. Juni 2007 im Internet veröffentlichen ließ, ging hervor, dass die CIA in den sechziger Jahren Kubas Staatspräsident Fidel Castro vergiften lassen wollte. Auf der Liste geplanter politischer Morde des Dienstes standen diesen Unterlagen zufolge auch der Anführer der Unabhängigkeitsbewegung im Kongo, Patrice Lumumba, sowie der Machthaber der Dominikanischen Republik, Rafael Trujillo.

Die CIA ist Teil der United States Intelligence Community (IC). Innerhalb der IC untersteht die CIA direkt dem Director of National Intelligence, der als höchste Instanz die Überwachung innerhalb der IC übernimmt. Die IC ist mit 15 verschiedenen Geheim- sowie Sicherheitsdiensten die größte Vereinigung dieser Art weltweit.

Der Dienst wird von je einem Geheimdienstausschuss des Senats, dem "Select Committee on Intelligence" und des Repräsentantenhauses, dem "House Permanent Select Committee on Intelligence", kontrolliert. Anders als sonstige Behörden braucht die CIA ihren Haushalt nicht zu veröffentlichen. Kritiker sehen dies als einen Widerspruch zur amerikanischen Verfassung.

Die CIA ist in fünf Direktorate unterteilt:

Innerhalb des "National Clandestine Service" arbeiten eine Vielzahl an spezialisierten Diensten, darunter die National Resources Division, die innerhalb der USA arbeitet, und die Special Activities Division, die Verdeckte Operationen in aller Welt ausführt und auch über paramilitärische Einheiten verfügt.

Die CIA wird operativ und technisch hauptsächlich durch den Director of the Central Intelligence Agency (D/CIA) geleitet. Er untersteht wiederum dem Director of National Intelligence (DNI).
Auf Leitungsebene gibt es in der Central Intelligence Agency jedoch noch weitere Beamte, die spezielle – durch den D/CIA zugewiesene – Aufgaben wahrnehmen.


Eine Auflistung aller bisheriger D/CIAs findet sich hier.

Der Dienstsitz der CIA befindet sich seit den 1950er-Jahren in Langley, Virginia, einem Vorort nordwestlich von Washington, D.C., dort im sogenannten "Langley Research Center". Der Campus hat keine offizielle Adresse, die dorthin führenden Straßen keinen Namen. Die CIA wird nach ihrem Dienstsitz häufig als "Langley" bezeichnet, manchmal auch salopp als Firma. Agenten werden gelegentlich spöttisch als "Virginia Farm Boys" bezeichnet, da die Grundausbildung in Camp Peary, einem militärischen Sperrgebiet des US-Verteidigungsministeriums stattfindet, was aber von der CIA genutzt wird. Es befindet sich in York County, Virginia. Das Gelände umfasst 37,53 km², von denen knapp 32,37 km² nicht bebaut sind. Der 400.000 m² große "Bigler’s Millpond" grenzt an den York-Fluss.

Die amerikanische Botschaft in Bern wurde zu einer Europa-Zentrale des US-Geheimdienstes CIA ausgebaut. In Bern befand sich nach dem Zweiten Weltkrieg die Europazentrale der CIA-Vorgängerorganisation OSS. In der Zwischenzeit wurde die CIA-Tätigkeit in Europa von Stuttgart aus koordiniert. Und im sogenannten Berner Club treffen sich seit Jahren Spione aus aller Welt zum Informationsaustausch.

Das Open Source Center ist eine Behörde der CIA, die auf das nachrichtendienstliche Sammeln und Auswerten von öffentlich zugänglichen Quellen spezialisiert ist.

Obwohl es in den vergangenen Jahren so aussah, als ob sich die CIA von der Zusammenarbeit mit der ehemaligen Firma Blackwater (jetzt Academi) distanziert hat, gibt es jetzt wieder gehäufte Berichte, dass immer noch eine sehr enge Zusammenarbeit existiert.

In Wiesbaden befindet sich eine Außenstelle der CIA.

Der Spiegel berichtete 1986 unter Bezug auf das Magazin GEHEIM über den Stützpunkt in Frankfurt: Dabei benannte er das I.G.-Farben-Haus am Grüneburg-Platz als Sitz in Frankfurt am Main, getarnt als : 

Der Nachrichtendienst wurde am 18. September 1947 durch Verabschiedung des National Security Act gegründet. Vorläufer der CIA war im Zweiten Weltkrieg das Office of Strategic Services (OSS), aus dem von 1945 bis 1947 die von Luftwaffengeneral Hoyt S. Vandenberg geleitete "Central Intelligence Group, CIG" im Außenministerium hervorging. Man erkannte jedoch schnell, dass mit dem beginnenden Kalten Krieg ein Rumpf-Spionageapparat für die Weltmacht USA nicht ausreichte.

Erster CIA-Direktor war Admiral Roscoe H. Hillenkoetter. Auf Hillenkoetter folgte 1950 Walter Bedell Smith, der jedoch den OSS-Veteran Allen Welsh Dulles zum Direktor einer Abteilung für spezielle verdeckte Aufgaben machte, die den Charakter der CIA entscheidend prägten. Dulles, der zwischen 1953 und 1961 als Direktor der CIA fungierte, war neben George Tenet am längsten an der Spitze der amerikanischen Auslandsspionage.

Während des Koreakrieges war die CIA für sämtliche nachrichtendienstlichen und auch militärischen Operationen hinter den feindlichen Linien zuständig. Die desaströsen Ergebnisse dieser Unternehmungen führten nach dem Krieg im Verteidigungsministerium zu der Erkenntnis, dass das Militär wieder für militärische Kommandooperationen zuständig sein sollte und zur Aufstellung der Special Forces, einer Truppe für asymmetrische Kriegführung. Diese Einheit bildete fortan die militärische Schnittstelle zur CIA.

Ende der 1950er-Jahre begann die CIA mit Spionageflügen in fremdem Luftraum, hauptsächlich über der Sowjetunion und der Volksrepublik China. Aus nachrichtendienstlicher und technischer Sicht waren die Programme mit Spionageflugzeugen, wie zum Beispiel U-2 und A-12, sehr erfolgreich.

Während des Vietnamkrieges leitete die CIA zahlreiche verdeckte Operationen in Laos, Kambodscha und Nordvietnam, war aber auch auf südvietnamesischem Gebiet sowohl in Pazifizierungs- als auch in Mordprogramme (Operation Phoenix) eingebunden. Bis in die 1970er-Jahre operierte die CIA zudem verdeckt in Lateinamerika und unterstützte unter anderem rechtsgerichtete Militärputsche gegen die demokratisch gewählten Regierungen Guatemalas (1954) , Brasiliens (1964)  und Chiles (1973) .

1978 wurden als Folge der Ermittlungen des Church Committee im "Foreign Intelligence Surveillance Act" die Zuständigkeiten der CIA und ihre Kontrolle durch ständige Ausschüsse des US-Kongresses neu geregelt.

1988 wurde George Bush als erster ehemaliger CIA-Chef zum Präsidenten der Vereinigten Staaten gewählt.

Im Jahr 1995 war das Hauptquartier der CIA das Ziel eines geplanten Attentats. Im Rahmen der Operation Bojinka sollte eine kleine mit Sprengstoff beladene Passagiermaschine gezielt in das Hauptquartier in Langley geflogen werden. Der Attentäter Abdul Hakim Murad nahm im Vorfeld Flugunterricht in North Carolina, um diesen Anschlag durchführen zu können. Der Plan wurde jedoch durch einen Brand im Apartment, in dem die Attentäter wohnten, vereitelt. Als Folge davon änderte al-Qaida ihre Pläne, was dann zu den Terroranschlägen am 11. September 2001 führte.
Seit dem 11. Juli 1997 war George Tenet Direktor der CIA. Er trat am 3. Juni 2004 nach Kritik an der Arbeit des Dienstes im Zusammenhang mit dem Dritten Golfkrieg aus „persönlichen Gründen zurück“. Bis zur Ernennung eines neuen Direktors übernahm sein bisheriger Stellvertreter John E. McLaughlin kommissarisch die Leitung der CIA.

Seit etwa 2004 ist die CIA für viele Operationen mit Drohnen verantwortlich. Im August 2011 legte das Bureau of Investigative Journalism (BIJ) einen Bericht über Drohnenangriffe in Pakistan vor, für den etwa 2000 Medienberichte ausgewertet wurden. Demnach wurden seit 2004 mindestens 291 Einsätze durchgeführt, bei denen zwischen 2292 und 2863 Menschen starben. 126 bewaffnete Anführer der Islamisten, die namentlich bekannt sind, und mehrere hundert militante Islamisten wurden getötet. Etwa 385 bis 775 Unbeteiligte, darunter 164 Kinder, kamen bei den Angriffen ums Leben. Ähnliche Angriffe fanden auch im Jemen und auf dem afrikanischen Kontinent statt.

Vom 24. September 2004 bis 6. Mai 2006 war Porter Goss CIA-Chef, bis er auf Druck der Regierung Bush sein Amt zur Verfügung stellte. Seit dem 30. Mai 2006 leitete Michael V. Hayden, ehemaliger Leiter der NSA, die CIA. Bis April 2005 war der Direktor der CIA jeweils auch als Director of Central Intelligence für die Beratung des US-Präsidenten in Nachrichtendienstfragen zuständig und hatte die Führung der Gemeinschaft der US-Nachrichtendienste (Intelligence Community) inne. Im Zuge der Reform des US-amerikanischen Nachrichtendienstsystems nach den Terroranschlägen vom 11. September wurde diese Funktion auf den Director of National Intelligence übertragen, der gleichzeitig für die Koordinierung der Arbeit der CIA mit der anderer Nachrichtendienste und für die Auslandskooperation zuständig ist. Dadurch wurde die Rolle der CIA als führende Nachrichtendienstinstitution aufgehoben.

Anfang Januar 2009 wurde Leon Panetta vom designierten Präsidenten Barack Obama überraschend als neuer Direktor der CIA nominiert, obwohl er über keine geheimdienstliche Erfahrung verfügte. Nachdem Panetta im Juli 2011 Nachfolger des Verteidigungsministers Robert Gates wurde, wurde General David Petraeus als neuer Direktor der CIA nominiert.

Am 8. November 2012, wenige Tage nach den Präsidentschaftswahlen in den USA, reichte Petraeus ein Rücktrittsgesuch vom Amt ein, das US-Präsident Obama am 9. November 2012 annahm. Petraeus war zuvor als Kandidat für ein Ministeramt in der zweiten Amtszeit von Präsident Obama gehandelt worden. Als Grund für den Rücktritt nannte Petraeus eine außereheliche Affäre. Sein Amt wurde kommissarisch von Michael Morell übernommen, bevor John Brennan im Januar 2013 als neuer Direktor von Barack Obama nominiert wurde.

Im Januar 2017 übernahm Mike Pompeo das Amt des Director of the Central Intelligence Agency.

Es liegt in der Natur der Sache, dass geheimdienstliche verdeckte Operationen in der Regel nicht an die Öffentlichkeit gelangen. Die folgenden Operationen wurden meist durch die geschichtliche Forschung, Recherchen von Enthüllungsjournalisten, offizielle Untersuchungen oder die Freigabe von Dokumenten im Rahmen des Freedom of Information Act bekannt. Eine Vielzahl weiterer Beispiele findet sich in dem Artikel Liste der Militäroperationen der Vereinigten Staaten, an denen die CIA oft maßgeblich beteiligt war.


Die Aktivitäten der CIA haben zu politischen Kontroversen in den Vereinigten Staaten und anderen Ländern geführt. Grund hierfür ist vor allem die erwiesene massive Einmischung in die inneren Angelegenheiten anderer Länder mittels verdeckter Operationen, zu der auch Staatsstreiche gegen demokratisch gewählte Regierungen zählten (siehe oben). Auch die lange praktizierte Installierung und Unterstützung von diktatorischen Regimes wird kritisiert, , wie beispielsweise von Augusto Pinochet in Chile, Manuel Noriega in Panama und Somoza in Nicaragua, oder auch autokratischer Systeme wie die des Schahs im Iran. Ebenso wurde die frühere Zusammenarbeit mit ehemaligen Mitgliedern der Wehrmacht und der SS kritisiert (MK Ultra) oder bei der teilweisen Unterstützung und Duldung der sogenannten Rattenlinien, eines legalen Fluchtweges für NS-Kriegsverbrecher, wie zum Beispiel Klaus Barbie, die mit Papieren des Vatikans und des Roten Kreuzes ausgestattet nach Südamerika entkamen.

Außerdem sahen viele Kritiker die Aufgabe der CIA nicht zuletzt darin, Desinformationen im Heimatland zu verbreiten, etwa was das militärisch-nukleare Bedrohungspotenzial des Ostblocks anging, um den US-Militärhaushalt und das Budget der Agency auf einem möglichst hohen Niveau zu belassen. Dagegen sah sich die CIA nicht in der Lage, den Zusammenbruch der Sowjetunion vorherzusagen.

Als nachteilig wird auch die verdeckte CIA-Operation Operation Cyclone für die Mudschahedin ab 1979 angesehen, die in der Zeit von „Säuberungen“ an Kommunisten durch den Vizepräsidenten Hafizullah Amin in Afghanistan begann, sechs Monate "vor" – und damit entgegen der landläufigen Geschichtsschreibung – der sowjetischen Intervention in Afghanistan. US-Präsident Jimmy Carter, beeinflusst von seinem Sicherheitsberater Zbigniew Brzeziński, unterzeichnet am 3. Juli 1979 die erste Direktive für die Unterstützung der Mudschahedin in Afghanistan, die sowjetische Armee marschiert am 24. Dezember 1979 in Afghanistan ein. Dies geschah im Wissen darum, dass sich die Wahrscheinlichkeit einer militärischen Intervention der Sowjetunion erhöhen würde. Das Kalkül war, der Sowjetunion eine militärische Belastung aufzuzwingen, die mit der Situation der USA während des Vietnamkriegs vergleichbar war. Brzeziński behaupte 1998, er habe Moskau in die „afghanische Falle“ gelockt. Die Sowjetunion rechtfertigte damals ihre Intervention mit der Behauptung, dass sie beabsichtigten, gegen eine geheime Einmischung der Vereinigten Staaten in Afghanistan zu kämpfen. Dabei wurden die radikalen Warlords, die auch im Drogenhandel (Opium) involviert sind, ebenso gefördert wie islamistische Organisationen, nicht zuletzt über den pakistanischen Geheimdienst ISI. So subventionierten die USA ab Mitte der Achtziger Jahre auch den saudi-arabischen Milliardärssohn Osama bin Laden, der seine multinationalen Freiwilligentrupps vorerst nur gegen Kommunisten und noch nicht gegen die Vereinigten Staaten mobilisierte. Nach Ende des Kalten Krieges stellten die USA ihre Kontakte zu den Gruppierungen Anfang der 1990er-Jahre weitgehend ein, während der ISI sie weiter als ein Instrument zur Durchsetzung pakistanischer Interessen nutzte. Dies habe zur Stärkung des internationalen Terrorismus geführt, den die USA heutzutage als die stärkste Bedrohung betrachten. Das Phänomen, dass eine verdeckte Operation auf das Ursprungsland zurückfällt, wird als "Blowback" (engl. für Rückstoß) bezeichnet.

2014 versicherte das Weiße Haus, die CIA würde keine Impfkampagnen mehr zur Informationsbeschaffung nutzen. Die CIA hatte offenbar im Rahmen der Operation Neptune’s Spear 2011 im Rahmen einer Impfkampagne die unzweifelhafte Identität der Bewohner des Anwesens Bin Ladens zu erhalten versucht. Obschon die Ablehnung alles "Westlichen" in islamistischen Kreisen grundsätzlich viel älter ist, führte das Bekanntwerden sicher zu einem zusätzlichen Rückschlag für die Polio-Impfkampagne in Pakistan.

Im Zuge von Untersuchungen der Folter-Vorwürfe kam es zu Auseinandersetzungen zwischen der CIA und dem Geheimdienst-Ausschuss des US-Senats im Umgang mit Verschlusssachen.

Im März 2017 wurden per Wikileaks im großen Maßstab Informationen über Abhörtechniken der CIA im IT-Bereich bekannt und unter dem Stichwort "Vault 7" öffentlich diskutiert.

Die aktuelle Kritik bezieht sich auf die wiederholt nachgewiesenen Verstöße gegen Menschenrechte im Zuge des sogenannten Kriegs gegen den Terror. Seit etwa 2001 sind die USA dazu übergegangen, terrorverdächtige Personen zu entführen und ohne Gerichtsverfahren über längere Zeit in weltweit verteilten Geheimgefängnissen zu inhaftieren, die das US-Militär als Black sites bezeichnet. Dabei haben die USA Verträge missachtet, die sie selbst ratifizierten und die grundsätzlich jedem Individuum ein Recht auf Schutz vor staatlichen Übergriffen garantieren. Es sind mehrere Fälle bekannt geworden, bei denen sich nach mehrmonatiger bis jahrelanger Haft herausstellte, dass die Verhafteten unschuldig bzw. Opfer einer Verwechslung waren. Zu den bekanntesten Fällen gehören die Deutschen Murat Kurnaz und Khaled al-Masri sowie der Kanadier Maher Arar, die angaben, in Pakistan, Afghanistan bzw. Syrien gefoltert worden zu sein.

Da die CIA offiziell keine Folter anwenden darf, wurde es gängige Praxis, die Gefangenen in befreundete Länder auszufliegen, wo sie von „Verhörspezialisten“ dieser Länder vernommen werden. Besonders kritisiert wird in diesem Zusammenhang die auch von US-Stellen mehrfach bestätigte Tatsache, dass dabei Länder bevorzugt werden, die systematisch foltern, etwa Syrien und Ägypten. Im April 2009 wurden von US-Präsident Barack Obama interne Papiere des Geheimdienstes CIA veröffentlicht, die die Existenz eines polnischen Geheimgefängnisses bestätigen und die belegen, dass Chalid Scheich Mohammed allein im März 2003 183 Mal dem "Waterboarding" unterzogen wurde, im Schnitt acht Mal pro Tag.

Im Jahr 2005 berichteten Medien, dass die CIA mit Hilfe von als zivil getarnten Fluggesellschaften Transporte von ohne rechtliche Grundlage verhafteten Terrorverdächtigen vornimmt. Genannt wurden die Airlines "Premier Executive Transport Services", "Tepper Aviation", "Pegasus Technologies" und "Aero Contractors". Dabei soll sie die Frankfurter Rhein-Main Air Base für geheime Gefangenentransporte als Zielort missbraucht haben, um von dort aus die weltweiten Geheimgefängnisse anzufliegen.

Die Berichte führten zu einer offiziellen Untersuchung im Auftrag des Europäischen Parlaments durch den Sonderermittler Dick Marty. Nach seinen Erkenntnissen soll die CIA in Europa über 100 Personen entführt haben. Marty betonte aber, dass er nicht die Kapazitäten besäße, genügend Beweise aufzubringen, um die Folteranlagen zu identifizieren. Dabei warf er einigen europäischen Regierungen vor, in der Angelegenheit eine heimliche Komplizenschaft mit den USA eingegangen zu sein. Insbesondere Polen und Rumänien hätten CIA-Gefängnisse in ihren Ländern ermöglicht.

Die Vorgehensweise deckt sich zumindest teilweise mit der Definition des Verschwindenlassens von Personen, das in dem 2002 in Kraft getretenen Rom-Statut als "Verbrechen gegen die Menschlichkeit" definiert ist. Es bildet eine der Rechtsnormen für die Rechtsprechung des Internationalen Strafgerichtshofs in Den Haag. Die Regierung von Präsident Bush forderte jahrelang eine Immunität für US-Bürger, die der Strafgerichtshof bislang aber nicht gewähren will. Mit mehr als 50 Staaten haben die USA inzwischen bilaterale Abkommen geschlossen, die eine Auslieferung von US-Bürgern aus diesen Ländern nach Den Haag verhindern sollen.

In Deutschland sind im Zusammenhang mit der Entführung des deutschen Staatsbürgers Khaled al-Masri Haftbefehle gegen 10 CIA-Agenten ausgesprochen worden. In Italien werden wegen der Entführung des Imams Abu Omar 26 CIA-Agenten per Haftbefehl gesucht. Im November 2009 wurden 22 CIA-Mitarbeiter und ein Angehöriger der Luftwaffe zu Haftstrafen zwischen 7 und 9 Jahren verurteilt. Die Berufungsverfahren wurden im September 2012 vom höchsten italienischen Gericht endgültig abgewiesen. Die CIA-Mitarbeiter wurden von den italienischen Ermittlern mit Hilfe einer Software namens "Analyst's Notebook" der Firma IBM enttarnt. Die Software analysierte die Verbindungsdaten und andere Metadaten einer großen Anzahl von Mobiltelefonen um Muster und Zusammenhänge zu finden.

Im Jahr 2006 erklärte der Oberste Gerichtshof der USA zahlreiche der oben angeführten Praktiken der US-Regierung für ungesetzlich. Um eine legale Grundlage für ihr weiteres Vorgehen zu schaffen, schuf die Bush-Regierung daher das umstrittene Gesetz Military Commissions Act. In einem in der Öffentlichkeit wenig beachteten Teil enthält das Gesetz eine Art Generalamnestie für von US-Bürgern verübte Verbrechen vor Inkrafttreten des Gesetzes, was von Kommentatoren als auf die oben genannten Praktiken bezogen gedeutet wurde.

Nach offiziellen US-Angaben sind die von der CIA betriebenen Geheimgefängnisse im Laufe des Jahres 2006 geschlossen worden. Laut einem Bericht der Financial Times wurde diese unter anderem vom Menschenrechtsrat der UN lange geforderte Entscheidung dadurch beschleunigt, dass Verhörspezialisten der CIA sich wegen der unklaren Rechtslage geweigert hatten, in diesen Einrichtungen weiterhin Gefangene zu verhören.

Im Jahr 2006 veröffentlichte ein Zusammenschluss von sechs Menschenrechtsorganisationen, darunter Amnesty International und Human Rights Watch, eine Liste mit 36 Personen, die entweder erwiesenermaßen oder mit hoher Wahrscheinlichkeit von US-Behörden wie der CIA unter Terrorverdacht gefangen gehalten wurden, und die „verschwunden“ (engl. "disappeared") seien. Sie seien weder wieder aufgetaucht, noch würden die US-Behörden Fragen zu ihrem weiteren Schicksal oder deren Verbleib beantworten. Diese Situation hatte sich bis zum April 2009 noch nicht wesentlich geändert. Die US-amerikanische Juraprofessorin Margaret Satterthwaite meinte dazu:

Die Verstrickung der CIA in den Drogenhandel ist vielfach öffentlich nachgewiesen worden. Die Aktivitäten dienten unter anderem der finanziellen Unterstützung verbündeter paramilitärischer Gruppen, zum Beispiel in Laos, Nicaragua und Afghanistan, und zur Destabilisierung von gegnerischen Regierungen.


Der amerikanische Professor Christopher Simpson bilanzierte im Jahr 2001 zu den Drogenhandels-Aktivitäten der CIA-Abteilung "Directorate of Operations" (2005 umbenannt in "National Clandestine Service"): 

Der ehemalige US-Außenminister John Kerry sagte 1987 während der Senatsanhörungen zur Iran-Contra-Affäre, zum Thema des von der CIA tolerierten Drogenschmuggels der Contra-Rebellen in die USA:
Zum gleichen Anlass äußerte sich auch der US-Senator Al D’Amato:
Die CIA hat bei der Geldwäsche von Drogengeldern wiederholt mit Banken zusammengearbeitet, die zum Teil eigens zu diesem Zweck gegründet wurden. Eingehend dokumentiert ist der Fall der 1991 spektakulär in Konkurs gegangenen Großbank Bank of Credit and Commerce International (BCCI). Laut einer Untersuchung des amerikanischen Senats war die Bank unter anderem an der Geldwäsche der Drogengewinne der Contras (siehe oben) beteiligt. Senator John Kerry schrieb bereits im April 1989 in einem Bericht: "Die CIA wusste von Anfang an, dass die BCCI ein durch und durch korruptes und kriminelles Unternehmen war. Die CIA hat die BCCI deshalb für ihre geheimen Operationen benutzt." Der Zusammenbruch der Bank war eine direkte Folge von Kerrys Untersuchungskommission, deren Ergebnisse die zwangsweise stattfindende Schließung von Niederlassungen der Bank in mehreren Ländern zur Folge hatten.

Über die BCCI liefen auch die Unterstützungszahlungen der CIA für die afghanischen Mudschahedin in den 1980er-Jahren (Operation Cyclone). Die Warlords stiegen zu den weltweit führenden Opium-Produzenten auf. Dies geschah unmittelbar, nachdem die Opiumernte im südostasiatischen Goldenen Dreieck größtenteils ausgefallen war. Der Rohstoff wurde im Grenzgebiet zu Pakistan massenhaft in Heroin umgewandelt. Die regionalen Kriegsherren konnten ihre Gewinne aus dem Drogenhandel über die Bank waschen, was mit zu der enormen Steigerung der Heroinproduktion in der Region beitrug. Robert Morgenthau, ehemaliger Staatsanwalt von New York, bezeichnete die BCCI als "„eine der größten kriminellen Unternehmungen der Weltgeschichte“". Ständige Kunden der Bank waren auch Manuel Noriega, der über die BCCI die Herkunft der Drogenprofite für das Medellín-Kartell verwischte, der damalige irakische Diktator Saddam Hussein und der palästinensische Terrorist Abu Nidal.

Das Personal der 1973 gegründeten australischen Nugan Hand Bank bestand hauptsächlich aus ehemaligen CIA-Mitarbeitern, die vorher in Vietnam und Laos tätig waren. Als Rechtsberater "(legal counsel)" fungierte der ehemalige CIA-Direktor William Colby. Nach der Insolvenz der Bank wurde der Gründer Frank Nugan, ein australischer Rechtsanwalt, erschossen in seinem Wagen aufgefunden. Der zweite Gründer, der amerikanische Staatsbürger und ehemalige US-Special-Forces-Soldat Michael Hand, floh am 14. Juni 1980 mit Hilfe zweier Ex-CIA-Mitarbeiter aus Australien. Er gilt seitdem als vermisst und wird von den australischen Behörden wegen zahlreicher Delikte gesucht, unter anderem wegen Wäsche von Drogengeldern und Betrugs.

Mit dem "Intelligence Reform and Terrorism Prevention Act" wurde das Amt des D/CIA auf die Leitung der Central Intelligence Agency beschränkt. Er leitet administrativ die Operationen der CIA sowie die Personal- und Haushaltsplanung. Ferner ist er "National Human Source Intelligence" (HUMINT) "Manager". Er untersteht dem Director of National Intelligence (DNI).

"siehe: Director of the Central Intelligence Agency"

Office of the Director of the Central Intelligence Agency

Nach einem Bericht der "Washington Post" betrug das Budget aller Nachrichtendienste der USA 2013 zusammen 52,6 Milliarden US-Dollar. Nachfolgend sind die fünf größten Behörden aufgeführt, deren jeweilige Budgets die "Post" in die vier Kategorien "Unterhalt", "Sammeln von Daten", "Datenverarbeitung und -verwertung" sowie "Datenanalyse" unterteilt hat.
Am 9. Dezember 2014 wurde vom Geheimdienstausschuss des US-Senats der CIA-Folterreport (offiziell: Committee Study of the Central Intelligence Agency’s Detention and Interrogation Program) veröffentlicht. Die 480 Seiten lange Zusammenfassung des 6300-seitigen, als vertraulich eingestuften Senatberichts bestätige lang bekannte Vermutungen, dass auf hoher Ebene „systematische Verbrechen und grobe Verletzungen der internationalen Menschenrechtsgesetze“ begangen worden seien, dass die Praktiken im Wesentlichen wirkungslos waren und dass CIA-Regierungsbeamte das Weiße Haus, das Justizministerium und den US-Kongress über Art, Ausmaß und Erfolg der Verhörmethoden routinemäßig irreführten und mehrfach belogen – so fasste die New York Times Erkenntnisse aus dem Folterreport zusammen.

Eine Kernaussage des Folterreports ist: die angewandten Foltermethoden waren in ihrem Spektrum größer und in ihrer Durchführung weit brutaler und sehr viel intensiver, als dies bis dahin die Öffentlichkeit erfahren hatte. Dabei seien kaum verwertbare Informationen erlangt worden – im Gegenteil sei es oft zu Fehlinformationen gekommen.

Zu den angewandten Foltermethoden zählten Einsatz von Insekten, vorgetäuschtes Begräbnis, Sippenhaft, Schlafentzug bis zur Halluzination, Scheinhinrichtungen, wochenlanges Waterboarding (Ertränkungsfolter bis zur Besinnungslosigkeit), völlige Verwahrlosung, rektale Folterung (puriertes Essen wurde rektal in den Dickdarm verabreicht und rektale Wasserzuführung – ohne medizinische Notwendigkeit), sexuelle Demütigung und Vergewaltigung – dabei konnte die Folterung bis zum Tod gehen, wie im Fall des versehentlich festgenommenen Gul Rahman im berüchtigten Geheimgefängnis „Cobalt“ in Afghanistan, der nach Eisduschen getötet wurde. Ein weiterer war Manadal Al-Jamadi, der starb, nachdem er, den Kopf in einer Plastiktüte, an einer Wand wie am Kreuz aufgehängt worden war (Abu-Ghuraib-Folterskandal). Einer dieser Gefangenen war eine „intellektuell minderbemittelte Person“, die nur verhaftet wurde, um sie als Druckmittel gegen ein anderes Familienmitglied zu benutzen.

Dabei beschreibt der Bericht keine bedauerlichen Einzelfälle, Exzesse oder die Taten von einzelnen Individuen, sondern ein systematisches, organisiertes, in vollen Umfang autorisiertes Programm, das von Präsident George W. Bush und Vizepräsident Dick Cheney genehmigt worden war, als nach den Anschlägen vom 11. September 2001 die „robusten Befragungs-Techniken“ gegen Verdächtige freigegeben wurden. Im Jahr 2002 gab die damalige Nationale Sicherheitsberaterin Condoleezza Rice der CIA die Genehmigung, den Palästinenser Abu Subeida zu foltern – sie wurde 2005 Außenministerin. Der damalige CIA-Direktor George Tenet genehmigte offiziell das Programm, der derzeitige CIA-Direktor John O. Brennan, war Tenets ausführender Assistent. Im Justizministerium von John Ashcroft verfassten die Rechtsberater John Yoo und Jay Bybee, die berüchtigten „Folter-Memos“. Die Psychologen Bruce Jessen und James Mitchell entwickelten die Foltermethoden. Jose Rodriguez, der Antiterrorchef der CIA, genehmigt die Zerstörung von Videobändern, die die Verbrechen dokumentierten.

Auf völkerrechtlicher Ebene sind alle diese Methoden staatlicher Brutalität und Morde durch die Genfer Konventionen, die sie als Kriegsverbrechen einstufen, und das Übereinkommen der Vereinten Nationen gegen Folter und andere grausame, unmenschliche oder erniedrigende Behandlung oder Strafe verboten; das Folterverbot ist absolut, es gibt keine Ausnahmen, und die Unterzeichner verpflichteten sich, gegen Verstöße vorzugehen. Auf staatlicher Ebene verletzen sie auch die amerikanische Verfassung und Gesetze, die Folter und Mordanschläge verbieten. Zur Verschleierung der Folter richtete die CIA klandestine Geheimgefängnisse, „Black Sites“, in Ländern wie Afghanistan, Thailand, Rumänien, Polen oder Litauen ein. Um die Folterungen durchzuführen, arbeitete die CIA dabei mit vielen der repressivsten autoritären Regime der Welt zusammen – gerade auch mit jenen Diktaturen, von denen die US-Regierung behauptet sie zu bekämpfen, einschließlich Ägyptens unter Hosni Mubarak, Syriens unter Baschar al-Assad und Libyens unter Muammar al-Gaddafi.

Der unabhängige Senator Angus King erklärte: „Wir haben Dinge getan, für die wir japanische Soldaten nach dem Zweiten Weltkrieg wegen Kriegsverbrechen vor Gericht gestellt haben.“

Bei den Vereinten Nationen wurden strafrechtliche Konsequenzen verlangt. Der Uno-Hochkommissar für Menschenrechte Zeid Ra'ad Al-Hussein erinnerte an die UN-Antifolterkonvention – danach dürfen auch außergewöhnliche Umstände wie Krieg oder Kriegsgefahr, innenpolitische Instabilität oder ein sonstiger öffentlicher Notstand, "nicht" als Rechtfertigung für Folter geltend gemacht werden. Die USA unterschrieben die Antifolterkonvention 1988 und ratifizierten es nach langen Jahren endlich 1994. Der UNO-Sonderberichterstatter zu Anti-Terror-Maßnahmen und Menschenrechten Ben Emmerson erklärte: „Jetzt ist die Zeit zu handeln – die Verantwortlichen müssen zur Rechenschaft gezogen werden“, diejenigen, die verantwortlich seien, müssten „strafrechtlichen Sanktionen gegenübertreten, die der Schwere ihrer Verbrechen entsprechen“ und sprach von einer „kriminellen Verschwörung“.

Auf dem Gelände der Central Intelligence Agency in Langley steht die Skulptur "Kryptos". Von den vier verschlüsselten Botschaften wurden bisher nur drei entschlüsselt.

Eine der CIA-Publikationen, das "CIA World Factbook", unterliegt nicht den Geheimhaltungsvorschriften und ist ohne Urheberrecht-Beschränkungen frei verwendbar.

Es existiert ein CIA-Museum, das technische Ausrüstungen zeigt, die die CIA im Laufe der Jahre einsetzte. Ebenso befinden sich dort Informationen zur Geschichte des Dienstes. Das Museum ist für die Öffentlichkeit nur online zugänglich.

2006 ist bekannt geworden, dass die CIA eine Reihe von Dokumenten, die im Sinne des "Informationsfreiheitsgesetzes" von der Geheimhaltung befreit waren, über Jahre hinweg der Öffentlichkeit zum Teil rechtswidrig wieder entzogen hat.

Das Direktorat für Wissenschaft und Technologie (Directorate of Science & Technology, DS&T) ist einer der vier Hauptbestandteile des Nachrichtendienstes.

1999 gründete der Nachrichtendienst die Venture Capital Firma In-Q-Tel mit dem ausdrücklichen Geschäftszweck, die CIA und die anderen Nachrichtendienste der Vereinigten Staaten mit der neuesten Technologie zu versorgen.

Im Dezember 2010 gründete die CIA eine Arbeitsgruppe, die "WikiLeaks Task Force", die sich mit den Auswirkungen der Veröffentlichung von Depeschen US-amerikanischer Botschaften durch WikiLeaks befassen soll.






</doc>
<doc id="12696" url="https://de.wikipedia.org/wiki?curid=12696" title="Laurent Gbagbo">
Laurent Gbagbo

Laurent Gbagbo (* 31. Mai 1945 in Gagnoa, Französisch-Westafrika) ist ein ivorischer Politiker der Ivorischen Volksfront (FPI).

Er war Professor für Geschichte an der Universität von Cocody-Abidjan und wurde aus politischen Gründen zweimal inhaftiert.

Vom 2. Dezember 2000 bis 3. Dezember 2005 war er Präsident der Elfenbeinküste. Ab 2005 übte er aufgrund der mehrfachen Verschiebung der Präsidentenwahl das Amt bis zum 4. Dezember 2010 weiterhin kommissarisch aus. Trotz seiner Abwahl hielt Gbagbo an der Macht in der Elfenbeinküste fest, bis er 2011 nach monatelangem Widerstand auf Geheiß von Alassane Ouattara, welcher im Jahr zuvor aus Sicht der Vereinten Nationen und der ehemaligen Kolonialmacht Frankreich als Sieger aus der umstrittenen Stichwahl um die Präsidentschaft hervorgegangen war, in seiner Residenz nahe Abidjan festgenommen wurde.

Von November 2011 bis Februar 2013 befand er sich im Gewahrsam des Internationalen Strafgerichtshofes in Den Haag. 2013 musste er sich wegen seiner verantworten.

Gbagbo war zunächst Lehrer für Geschichte und Erdkunde am Gymnasium Lycée Classique d’Abidjan und erhielt dann einen Forschungsauftrag am Institut für Geschichte und Afrikanische Archäologie (IHAAA). Er war Generalsekretär der Partei FPI (1988–1996), deren Vorstandsvorsitzender (1996–2000), Abgeordneter für den Wahlkreis Ouragahio (1990–2000) und wurde schließlich 2000 Präsident der Republik Côte d’Ivoire.

Nach seinem Baccalauréat mit Schwerpunkt Philosophie am Lycée Classique d'Abidjan 1965 studierte er Geschichte an der Université d’Abidjan und legte seine Licence in diesem Fach 1969 ab. 1970 wurde er Lehrer für Geschichte und Erdkunde am Lycée Classique d'Abidjan.
Sehr aktiv in der Gewerkschaft, trat er heimlich der Opposition bei, zusammen mit Bernard Zadi Zaourou, beide Dozenten an der Université d’Abidjan. Aufgrund seiner politischen Aktivitäten wurde er von März 1971 bis Januar 1973 in Séguéla und Bouaké inhaftiert.

Ab 1974 arbeitete er als Forscher am Institut für Geschichte, Kunst und Afrikanische Archäologie (IHAAA) der Université d’Abidjan und schrieb gleichzeitig an seiner Doktorarbeit mit dem Titel "Les ressorts socio-économiques de la politique ivoirienne: 1940–1960" (zu Deutsch: Die sozioökonomischen Aspekte der ivorischen Politik zwischen 1940 und 1960). Er promovierte 1979 an der Universität Paris VII. 1979 veröffentlichte er sein erstes Buch über das Heldenepos des Königs von Manding. Wenig später erschien ein Essay mit dem Titel "Reflexion sur la Conférence de Brazzaville" (Überlegungen über die Konferenz von Brazzaville). 1980 wurde er Direktor des IHAAA.

Laurent Gbagbo war als aktives Mitglied der Gewerkschaft „Hochschule und Forschung“ SYNARES (Syndicat National de la Recherche et de l'Enseignement Supérieur) an Arbeitskämpfen beteiligt, besonders an dem Streik an den Hochschulen von 1982. Zusammen mit einigen Lehrerkollegen gründeten sie heimlich eine Vereinigung, woraus später die Partei Front Populaire Ivoirien (FPI) werden sollte. Man hielt ihn für den Hauptverantwortlichen im „Komplott der Hochschullehrer“ von 1982, weshalb er freiwillig ins Exil nach Frankreich ging, um gegen die „Diktatur der PDCI“ (Parti démocratique de Côte d'Ivoire) zu kämpfen und für das Mehrparteiensystem zu werben. Er veröffentlichte dazu 1983 ein Buch mit dem Titel: „La Côte d’Ivoire pour une alternative démocratique“ (Eine demokratische Alternative für Côte d’Ivoire), gefolgt von einem gemeinsamen Werk, das die Partei FPI und ihr Programm vorstellte: „Les propositions pour gouverner“. Er erhielt Flüchtlingsstatus und freundete sich mit Guy Labertit an, damals Verantwortlicher im französischen PSU (Parti Socialiste Unifié) für Internationale Zusammenarbeit und Herausgeber der Zeitschrift „Libération Afrique“. Dieser nahm ihn zeitweise bei sich zu Hause auf. Unter der Regierung von Jacques Chirac 1986 wurde er unter Druck gesetzt, in seine Heimat zurückzukehren.

Er kehrte erst 1988 zurück, nachdem er in zahlreichen Verhandlungen mit dem Gesandten des Präsidenten Félix Houphouët-Boigny, Zugeständnisse, u. a. die Effektivität des in der Verfassung festgeschriebenen Mehrparteiensystems, erzielte. Nachdem er beschuldigt wurde, von einer ausländischen Macht beauftragt zu sein, das Regime zu destabilisieren, änderte er seine Strategie und begründete dies mit dem Sprichwort „Ein Baum kann sich nicht gegen einen Vogel wehren“.
Zurück in seiner Heimat, organisierte Laurent Gbagbo am 19. und 20. November 1988 den ersten Kongress seiner immer noch illegalen Partei FPI, als deren Generalsekretär er gewählt wurde. Die Partei definierte sich als eine linke, demokratische politische Kraft und wählte als Logo eine Rose mit Wurzel in der Côte d’Ivoire – angelehnt an die französische PS. In der Präsidentenwahl im Oktober 1990 trat er als einziger Kandidat gegen den amtierenden Félix Houphouët-Boigny an und erhielt dabei 18,3 % der Stimmen. Gbagbo wurde die Galionsfigur der Opposition; die FPI gewann bei den Parlamentswahlen neun Mandate und bei den Kommunalwahlen sechs Bürgermeisterämter.

Laurent Gbagbo wurde bei dem 3. ordentlichen Kongress der FPI vom 9. bis 11. Juli 1999 zum Präsidentschaftskandidaten gewählt und gewann am 22. Oktober 2000 die Wahl gegen General Robert Guéï.

Gbagbos Amtszeit wurde überschattet durch einen Bürgerkrieg, der im September 2002 ausbrach und zu einer Spaltung des Landes führte. Während der Norden der Elfenbeinküste von Rebellen kontrolliert wurde, wurde der Süden von Gbagbo nahestehenden Soldaten beherrscht. Der Bürgerkrieg endete 2007 mit einem Waffenstillstand, der ehemalige Rebellenführer Guillaume Soro wurde zum Premierminister ernannt.

Gbagbos erste Amtszeit als Staatspräsident endete offiziell im Jahr 2005, infolge des Bürgerkrieges wurden die anstehenden Präsidentschaftswahlen aber mehrfach verschoben. Sie fanden schließlich im Herbst 2010 statt. Gbagbo gewann den ersten Wahlgang, musste sich aber dem Oppositionspolitiker Alassane Ouattara in einer Stichwahl stellen. Nach Angaben der Unabhängigen Wahlkommission habe Ouattara diese gewonnen, der Verfassungsrat widerrief aber dieses Wahlergebnis und erklärte Gbagbo zum Wahlsieger. Am 4. Dezember 2010 wurde er ungeachtet internationaler Proteste für eine zweite fünfjährige Amtszeit vereidigt und bildete am Tag darauf die Regierung Aké N’Gbo. Im Nachgang zur Wahl kam es auch im eigenen Land zu Protesten. Medienberichten zufolge kam es zu hunderten Festnahmen. Als Reaktion auf internationale Proteste gegen sein Festhalten am Präsidentenamt rief er die von der UN und Frankreich gestellten Friedenstruppen dazu auf, die Elfenbeinküste zu verlassen.

Am 20. Dezember 2010 verhängte die Europäische Union ein Einreiseverbot gegen Gbagbo und 18 seiner Vertrauten. Auch ein Einfrieren ihrer Konten wurde geplant.
Die französische und belgische Regierung hatten Ouattara als legitimen Staatspräsidenten anerkannt und dessen Gesandte als Botschafter akkreditiert, die Vertrauten Gbabgos waren mittlerweile untergetaucht. Sowohl die westafrikanische Staatengemeinschaft ECOWAS als auch die Afrikanische Union suspendierten die Mitgliedschaft der Elfenbeinküste, forderten Gbagbo offiziell zum Rücktritt auf, die ECOWAS, die regelmäßig Militärinterventionen durchführte, drohte dem Regime „legitime Gewalt“ an und entsandte eine hochrangige diplomatische Delegation, um den abgewählten Präsidenten zur Aufgabe zu überreden.

Laurent Gbagbo war, zusammen mit seiner Frau Simone und seinen Vertrauten Alcide Djédjé, Désiré Asségnini Tagro und Pascal Affi N’Guessan von, am 6. Januar 2011 beschlossenen, Sanktionen der Vereinigten Staaten betroffen. Alle seine Besitztümer wurden eingefroren und Firmen durften keine Geschäfte mit ihm machen.

Am 19. Januar 2011 hatte der Schweizer Bundesrat beschlossen, alle möglichen Vermögenswerte Gbagbos und seines Umfeldes in der Schweiz mit sofortiger Wirkung zu sperren. Diese Gelder sollten nicht über die Schweiz den rechtmäßigen Eigentümern entzogen werden können, begründete der Bundesrat die Entscheidung. Davon betroffen waren 85 natürliche Personen sowie elf juristische Personen, Organisationen und Einrichtungen. Auch die Europäische Union (EU) hatte aufgrund der Lage in der Elfenbeinküste beschlossen, die Vermögenswerte von Laurent Gbagbo und seinem Umfeld einzufrieren. Die USA hatten ebenfalls gewisse Vermögen blockiert.

Im März und April 2011 eskalierte der bewaffnete Konflikt zwischen den verfeindeten Lagern. In zahlreichen Landesteilen konnten sich die Anhänger Ouattaras militärisch durchsetzen, in Abidjan, der einstigen Hochburg Gbagbos, kam es zu heftigen Gefechten. Offenbar verlor der abgewählte Präsident immer mehr an Rückhalt in den Streitkräften, die ihm die Gefolgschaft verweigerten oder zum neu gewählten Präsidenten Ouattara überliefen. Dieser ließ alle Grenzübergänge sperren, um die Flucht seines verhassten Gegners zu verhindern. Am 1. April 2011 tauchte Gbagbo im Zuge der chaotischen Straßenkämpfe in Abidjan unter und ließ noch verlautbaren Seit dieser Zeit hielt er sich im Bunker des Präsidentenpalastes versteckt, den Ouattaras Truppen zunächst vergeblich zu stürmen versuchten. Französische und UN-Helikopter beschossen im Verlauf der Kämpfe mehrfach den Präsidentenpalast. Am 11. April inhaftierten Streitkräfte Ouattaras, die von französischen Soldaten der Opération Licorne und UN-Soldaten unterstützt wurden, Gbagbo in seiner Privatresidenz.

Am 30. November 2011 lieferte ihn die Elfenbeinküste nach Den Haag aus, wo er sich vor dem Internationalen Strafgerichtshof (IStGH) verantworten muss. Ihm werden Verbrechen gegen die Menschlichkeit zur Last gelegt. Konkret wirft ihm die Anklagebehörde die Verantwortung für Mord, Vergewaltigung, unmenschliche Akte sowie Verfolgung und der damit zusammenhängende Tod von mindestens 325 Menschen in der Elfenbeinküste zwischen dem 16. Dezember 2010 und dem 11. April 2011 als „"Indirekter Mittäter"“ vor.

Am 28. Januar 2016 begann sein Prozess. Gbagbo erklärte sich wie sein ebenfalls angeklagter ehemaliger Jugendminister und Milizenchef Charles Ble Goude für unschuldig. Bisher ist der Prozess noch nicht abgeschlossen.

Laurent Gbagbo ist mit Simone Ehivet verheiratet und Mitglied der katholischen Kirche. Die Ehefrau von Gbagbo arbeitet aktiv im Vorstand seiner Partei.

Seine letzten Bücher sind: 1989, „Histoire d’un retour“ (Geschichte einer Rückkehr), 1991, „Agir pour les libertés“ (Handeln für die Freiheiten) und 1995, „Le temps de l’espoir“ (Zeit der Hoffnung).




</doc>
<doc id="12698" url="https://de.wikipedia.org/wiki?curid=12698" title="Papier">
Papier

Papier (von , aus ‚ Papyrusstaude‘) ist ein flächiger Werkstoff, der im Wesentlichen aus Fasern meist pflanzlicher Herkunft besteht und durch Entwässerung einer Fasersuspension auf einem Sieb gebildet wird. Das entstehende Faservlies wird verdichtet und getrocknet.


Höhlenzeichnungen sind die ältesten Dokumente, die der Mensch mit Pigment­farbe auf einen Untergrund gezeichnet hat. Die Sumerer, als Träger der ältesten bekannten Hochkultur, schrieben seit etwa 3200 v. Chr. mit Keilschrift auf weiche Tontafeln, die zum Teil durch Zufälle gebrannt, überliefert sind. Auch aus Ägypten sind Schriftträger aus anorganischen Materialien bekannt, beispielsweise die Narmer-Palette – eine Prunkpalette des Königs Narmer (3100 v. Chr.) aus Schiefer.

Papierähnlichere sind aus Papyrus gefertigt. Dieses Papyrus(papier) besteht aus den flach geschlagenen, über Kreuz gelegten und gepressten Stängeln der am gesamten unteren Nil in ruhigen Uferzonen wachsenden Schilfpflanzen (echter Papyrus), die dünnen, gepressten Schichten werden dann zusammengeklebt (laminiert). Geschrieben wurde darauf mit schwarzer und roter Farbe. Die schwarze Tusche bestand aus Ruß und einer Lösung von Gummi arabicum, die rote Farbe wurde auf Ocker-Basis hergestellt. Das Schreibgerät war ein Pinsel aus Binsen. Papyrus wurde im Alten Ägypten seit dem dritten Jahrtausend v. Chr. als Schreibmaterial benutzt. Zwar gab es Papyrus im antiken Griechenland, jedoch war eine Verbreitung über Griechenland hinaus kaum bekannt. Im 3. Jahrhundert v. Chr. ersetzten die Griechen den Pinsel durch eine gespaltene Rohrfeder.

Im Römischen Reich wurden sowohl Papyrus als auch Wachs­tafeln benutzt. In die Letzteren wurde der Text mittels angespitzter Griffel geritzt. Nach dem Auslesen wurde das Wachs mit einem Schaber geglättet und die Tafel konnte erneut beschrieben werden. Öffentliche Verlautbarungen wurden meist als dauerhafte Inschrift (Steintafeln oder Metallplatten) an Tempeln oder Verwaltungsgebäuden angebracht. Die Römer bezeichneten Papyrus-Rindenbast mit , aus dem sich später die Bezeichnung „Library“ (Bibliothek) entwickelte.

In China wurden Tafeln aus Knochen, Muscheln, Elfenbein und Schildkrötenpanzer benutzt. Später bestanden Schrifttafeln auch aus Bronze, Eisen, Gold, Silber, Zinn, Jade, Steinplatten und Ton oder auch häufig aus organischem Material, wie Holz-, Bambusstreifen und Seide. Pflanzenblätter und Tierhäute wurden noch nicht als Schriftträger benutzt. Orakelknochen wurden mit Griffeln geritzt oder mit Tinte mit Lampenruß oder Zinnober als Pigment beschriftet.

In Indien und Ceylon wurden die Blätter der Talipot-Palme etwa seit 500 v. Chr. benutzt →Palmblattmanuskript, sowie auch Birkenrinde, Holzblöcke, -tafel und Baumwolllappen, sowie auch Steintafeln, -blöcke.

In den Hochkulturen des Alten Orients und des Mittelmeerraumes wurde seit alters her Leder als Beschreibstoff verwendet. Wie Leder wird auch Pergament aus Tierhäuten hergestellt. Durch die Vorteile des Pergaments wurden im mittelalterlichen Europa andere Beschreibstoffe verdrängt. Die Tierhäute werden mit Pottasche oder Kalk gebeizt, gründlich gereinigt und aufgespannt getrocknet, es folgte das Schaben und die Oberflächenbearbeitung.

In der neuen Welt wurde "Huun", Amatl, ein papierähnlicher Beschreibstoff, bereits vor dem 5. Jahrhundert von den Maya hergestellt. Allerdings ist dieses Material, der Herstellungsart nach, eher dem Papyrus verwandt, denn es wird aus kreuzweise verpressten Baststrängen, nicht aber aus aufgeschlossenen Einzelfasern erzeugt. Auch der für die Papierdefinition essenziell wichtige Entwässerungsvorgang erfolgt weder auf einem Sieb noch durch mechanischen Wasserentzug. Insofern wäre es falsch, von einer Erfindung des Papieres in Amerika zu sprechen. Die tatsächliche und unabhängige Urherstellung von Papier lässt sich nur für Asien und Europa nachweisen.

Obwohl es Funde aus China gibt, die auf etwa 140 v. Chr. datiert werden können, wird die Erfindung des Papiers Ts'ai Lun zugeschrieben, der um 105 n. Chr. (Belegdatum der ersten Erwähnung der chinesischen Papierherstellungsmethode) ein Beamter der "Behörde für Fertigung von Instrumenten und Waffen" am chinesischen Kaiserhof war und erstmals das heute bekannte Verfahren, Papier herzustellen, beschrieb. Zu seiner Zeit gab es einen papierartigen Beschreibstoff, der aus Seidenabfällen hergestellt wurde "(Chi)". Diesen mischten die frühen Papiermacher vornehmlich mit Hanf, alten Lumpen und Fischernetzen und ergänzten das Material mit Baumrinde oder Bast des Maulbeerbaumes. Die chinesische Erfindung bestand vor allem in der neuartigen Zubereitung: Die gesäuberten Fasern und Fasernreste wurden zerstampft, gekocht und gewässert. Anschließend wurden einzelne Lagen mit einem Sieb abgeschöpft, getrocknet, gepresst und geglättet. Beim Schöpfen entstand an dem Papier eine „Schönseite“, die an der dem Sieb abgewandten Seite lag, und eine „Siebseite“, die an dem Sieb lag. Der entstehende Brei aus Pflanzenfasern lagerte sich als Vlies ab und bildete ein relativ homogenes Papierblatt. Dies war eine Technik, die in Korea in einer eigenständigen Form vermutlich seit dem 2. Jahrhundert n. Chr. angewandt wurde und seit vielen Jahren unter dem Namen Hanji () ihre Renaissance feiert.

Da Bast ein Material ist, das im Vergleich zu dem verwendeten Holz längere Fasern und dadurch eine hohe zeitliche Haltbarkeit hat, war das Papier von "Ts’ai Lun" nicht nur zum Schreiben verwendbar, sondern auch für Raumdekorationen etwa in Form von Tapeten sowie Kleidungsstücken. Die Verwendung von Maulbeerbast lag nahe, da der Seidenspinner sich von den Blättern des Maulbeerbaums ernährte und somit dieses Material ein ohnehin vorhandenes Nebenprodukt aus der Seiden­produktion war. Wie alt die Verwendung von Bast ist, belegt die Gletschermumie Ötzi (ca. 3300 v. Chr), der Kleidungsstücke aus Lindenbast trägt.

Bereits im 2. Jahrhundert gab es in China Papiertaschentücher, im 3. Jahrhundert Zusatz von Leimstoffen (Stärke), Erfindung der Leimung (dünner Überzug, um Papier glatter und weniger saugfähig zu machen; die Tinte oder Tusche verläuft weniger stark), sowie die Färbung von Papier. Möglicherweise wurde schon die erste Zeitung (Dibao) herausgegeben. Im 6. Jahrhundert wurde Toilettenpapier aus billigstem Reisstrohpapier hergestellt. Alleine in Peking wurden jährlich zehn Millionen Päckchen mit 1000 bis 10.000 Blatt produziert. Die Abfälle an Stroh und Kalk bildeten bald große Hügel, „Elefanten-Gebirge“ genannt. Für Zwecke des chinesischen Kaiserhofes stellte die kaiserliche Werkstatt 720.000 Blatt Toilettenpapier her. Für die kaiserliche Familie waren es noch einmal 15.000 Blatt hellgelbes, weiches und parfümiertes Papier.

Bekannt ist, dass um das Jahr 300 die Thais die Technik des "schwimmenden Siebs" zur Papierherstellung verwendeten. Das Bodengitter des Siebes war fest mit dem Rahmen verbunden. Jedes geschöpfte Blatt musste im Sieb trocknen und konnte erst dann herausgenommen werden. Entsprechend viele Siebe waren nötig.

Um das Jahr 600 gelangte die weiter entwickelte Technik des Schöpfens mit dem Schöpfsieb nach Korea und wurde um 625 in Japan verwendet. Das frisch geschöpfte Blatt kann feucht entnommen und zum Trocknen ausgelegt werden. Diese Technik wird noch bei handgeschöpftem Papier verwendet. Daraus ergibt sich, dass das Schöpfsieb in der Zeit zwischen 300 und 600 erfunden wurde.

In Japan wurde die Technik verbessert, indem der Faserbrei mit Pflanzenschleimen z. B. von "Abelmoschus manihot" aufgewertet wurde. Die Fasern waren gleichmäßiger verteilt, es traten keine Klümpchen auf. Dieses Papier wird als Japanpapier bezeichnet. Die Amtsrobe der japanischen Shintō-Priester, die auf die Adelstracht der Heian-Zeit zurückgeht, besteht aus weißem Papier (Washi), das vorwiegend aus Maulbeerbaum-Bast besteht.

In der Tang-Dynastie wurde die Papierherstellung weiter stark verbessert, es wurde gewachst (Chinawachs, Bienenwachs), gestrichen, gefärbt und kalandriert. Um den steigenden Papierbedarf unter den Tang zu decken, wurden die Bambusfasern zur Papierproduktion eingeführt.
Der chinesische Kaiser Gaozong (650 bis 683) ließ erstmals Papiergeld ausgeben. Auslöser war ein Mangel an Kupfer für die Münzprägung. Seit dem 10. Jahrhundert hatten sich Banknoten in der Song-Dynastie durchgesetzt. Ab etwa 1300 waren sie in Japan, Persien und Indien im Umlauf und ab 1396 in Vietnam unter Kaiser "Tran Thuan Tong" (1388–1398).
Im Jahr 1298 berichtete Marco Polo in seiner Reisebeschreibung („Il Milione“) über die starke Verbreitung des Papiergeldes in China, wo es zu dieser Zeit eine Inflation gab, die den Wert auf etwa ein Prozent des ursprünglichen Wertes fallen ließ. Im Jahr 1425 wurde das Papiergeld allerdings wieder abgeschafft, um die Inflation zu beenden. Um das Inumlaufbringen von Falschgeld zu erschweren, wurde Papiergeld zeitweise aus einem Spezialpapier gefertigt, das Zusätze an Seidenfasern, Insektiziden und Farbstoffen enthielt.

Wann genau das erste Papier in der arabischen Welt produziert wurde, ist umstritten. So wird als Datum 750 oder 751 genannt, als vermutlich bei einem Grenzstreit gefangengenommene Chinesen die Technik der Papierherstellung nach Samarkand gebracht haben sollen. Andererseits gibt es Erkenntnisse, die zu der Annahme führen, dass in Samarkand bereits 100 Jahre früher Papier bekannt war und auch hergestellt wurde. Als Papierrohstoff wurden Flachs und Hanf (Hanfpapier) benutzt. Bald hatten die Araber eine blühende Papierindustrie aufgebaut.

In Bagdad wurde um 795 die Papierherstellung aufgenommen, 870 erschien dort der erste Papiercodex. Papiergeschäfte waren wissenschaftliche und literarische Zentren, die von Lehrern und Schriftstellern betrieben wurden. Das Haus der Weisheit entstand nicht zufällig zu dieser Zeit in Bagdad. In den Kanzleien des Kalifen Hārūn ar-Raschīd wurde auf Papier geschrieben. Es folgten Papierwerkstätten in Damaskus, Kairo, in nordafrikanischen Provinzen bis in den Westen. Die Araber entwickelten die Herstellungstechnik weiter, durch die Einführung der Oberflächenleimung. Man mischte Lumpen und Stricke, diese wurden zerfasert und gekämmt, dann in Kalkwasser eingeweicht, dann zerstampft und gebleicht. Diese Pulpe schmierte man an eine Wand zum Trocknen. Anschließend wurde sie mit einer Stärkemischung glattgerieben und in Reiswasser getaucht um die Poren zu schließen. Genormte Flächenmaße wurden eingeführt, 500 Bogen waren ein Bündel (rizma), worauf der noch in der Papierwirtschaft übliche Begriff Ries zurückgeht. Vom 8. bis zum 13. Jahrhundert dauerte die hohe Blütezeit des islamischen Reiches. Als Kulturzentrum zog Bagdad Künstler, Philosophen und Wissenschaftler, insbesondere Christen und Juden aus Syrien, an.

In Indien wurde das Papier ab dem 13. Jahrhundert unter islamischem Einfluss eingeführt und begann in Nordindien das bis dahin vorherrschende Palmblatt als Schreibmaterial abzulösen. Die indischen Papiermanuskripte sind aber durch das Vorbild der Palmblattmanuskripte beeinflusst. So wurde das Querformat (das bei Palmblattmanuskripten durch die natürlichen Dimensionen der Palmblätter vorgegeben ist) beibehalten. An die Stelle der Löcher für den Bindfaden, der bei Palmblattmanuskripten die einzelnen Blätter zusammenhält, traten bei den Papiermanuskripten rein ornamentale Kreise. Im westlichen Nordindien ersetzte Papier das Palmblatt bis zum 15. Jahrhundert komplett. In Ostindien blieb das Palmblatt bis ins 17. Jahrhundert in Gebrauch. In Südindien konnte sich Papier dagegen nicht durchsetzen. Hier blieb das Palmblatt bis zum Aufkommen des Buchdrucks im 19. Jahrhundert das bevorzugte Schreibmaterial.

Über den Kulturkontakt zwischen dem christlichen Abendland und dem arabischen Orient sowie dem islamischen Spanien gelangte das Schreibmaterial seit dem 11. Jahrhundert nach Europa. Ein bedeutender Teil der Ausgangsmaterialien für die frühe europäische Papiererzeugung bestand aus Hanffasern, Flachsfasern (Leinen) und Nesseltuch, die Papiermühlen kauften die erforderlichen Hadern von den für sie arbeitenden Lumpensammlern. In Xàtiva bei Valencia gab es nach einem Reisebericht von Al-Idrisi bereits in der Mitte des 12. Jahrhunderts eine blühende Papierwirtschaft, die auch in die Nachbarländer hochwertige Produkte exportierte. Auch nach der Vertreibung der Araber aus Spanien blieb das Gebiet um Valencia bedeutend für die Papierwirtschaft, weil dort viel Flachs (Leinen) angebaut wurde, der ein hervorragender Rohstoff für die Papierherstellung ist.

Das sogenannte "Missale von Silos" ist das älteste erhaltene christliche Buch aus handgeschöpftem Papier. Es stammt aus dem Jahr 1151 und wird in der Bibliothek des Klosters Santo Domingo de Silos in der Provinz Burgos (Spanien) aufbewahrt.

Die maschinelle Massenproduktion von Papier begann im mittelalterlichen Europa; europäischen Papiermachern gelang es in kurzer Zeit, den Arbeitsprozess durch die Einführung zahlreicher – den Chinesen und Arabern unbekannter – Innovationen zu optimieren: Der Betrieb wassergetriebener Papiermühlen mechanisierte den bis dahin nur in Handarbeit oder mit Tieren im Kollergang praktizierten Zerkleinerungsvorgang. Derartige Wassermühlen, eisenbewehrte Lumpen-Stampfwerke, sind erstmals ab 1282 bezeugt. Das Reißen der Lumpen mit einem Sensenblatt löste die umständliche Praxis des Reißens von Hand oder Schneidens mit Messer oder Schere ab. Papierpressen, konstruiert in Anlehnung an antike Kelter, trockneten das Papier durch Schraubpressdruck.

Ebenfalls völlig neu war die Konstruktion des Schöpfsiebs, bei dem ein Metallgeflecht an die Stelle der älteren Bambus- oder Schilfsiebe trat. Das starre Schöpfsieb aus Metalldraht war die technische Voraussetzung für das Anbringen des zur Kennzeichnung dienenden Wasserzeichens, einer italienischen Erfindung. Die Verfeinerung der Papierqualität zu erschwinglichen Preisen trug kurze Zeit später wesentlich zum Erfolg des von Johannes Gutenberg erfundenen modernen Buchdrucks bei.

Mit der Ausbreitung der Schriftlichkeit in immer weitere Bereiche der Kultur (Wirtschaft, Recht, Verwaltung und Weitere) trat das Papier gegenüber Pergament seit dem 14. Jahrhundert seinen Siegeszug an. Ab der Mitte des 15. Jahrhunderts begann mit dem Buchdruck auf dem billigeren Papier, das Pergament als Beschreibstoff in den Hintergrund zu treten. Allerdings dauerte es bis ins 17. Jahrhundert, bis es vom Papier weitgehend verdrängt wurde. In der Folge spielte Pergament nur noch als Luxusschreibmaterial eine Rolle.

Die erste deutsche Papiermühle entstand 1389/1390 bei Nürnberg. Gegründet wurde die Gleismühl vom Ratsherrn und Exportkaufmann Ulman Stromer. Stromer unternahm Geschäftsreisen, unter anderem auch in die Lombardei, und kam dort mit der Papierherstellung in Berührung. Stromer ließ Mitarbeiter und Erben einen Eid ablegen, die Kunst der Papierherstellung geheim zu halten. Die Gleismühl bestand aus zwei mit Wasserkraft angetriebenen Werkseinheiten. Die kleinere Mühle wies zwei Wasserräder auf, die größere verfügte über drei. Insgesamt wurden 18 Stampfen angetrieben.

1389 bis 1394 leitete Stromer selbst die Papiermühle und verpachtete sie dann gegen eine Pacht von „30 Ries gross Papier“ an Jörg Tirman, seinen Mitarbeiter. Die Schedelsche Weltchronik von 1493 zeigt sie als früheste Darstellung einer Papiermühle auf der Darstellung der Stadt Nürnberg. Die Gleismühle brannte später ab.

Ab 1393 ist die Papierherstellung in Ravensburg nachgewiesen. Im späten Mittelalter und in der frühen Neuzeit entwickelte sich die oberschwäbischen Reichsstadt zum größten Papierherstellungszentrum im Südwesten. Für das 15. und 16. Jahrhundert wird die Produktion in bis zu sieben Papiermühlen auf etwa 9000 Ries (etwa 4,5 Millionen Blatt) jährlich geschätzt. 

Östlich der Elbe entstanden die ersten Papiermühlen erst Mitte des 17. Jahrhunderts. Francois Feureton aus Grenoble gründete mit Unterstützung des Friedrich Wilhelm zunächst eine Papierfabrik in Burg und dann in Prenzlau.

Die benötigten Zellstofffasern wurden bis in die zweite Hälfte des 19. Jahrhunderts aus abgenutzten Leinentextilien, Lumpen, Hadern (hergeleitet vom althochdeutschen "hadara": „Schafspelz“) gewonnen. Lumpensammler und -händler versorgten die Papiermühlen mit dem Rohstoff. Lumpen waren zeitweise so begehrt und rar, dass für sie ein Exportverbot bestand, das auch mit Waffengewalt durchgesetzt wurde. In den Papiermühlen wurden die Hadern in Fetzen geschnitten, manchmal gewaschen, einem Faulungsprozess unterzogen und schließlich in einem Stampfwerk zerfasert. Das Stampfwerk wurde mit Wasserkraft angetrieben.

Die Rohstoffaufbereitung erfolgte noch im 17. Jahrhundert in handwerklich organisierten Betrieben sowie teilweise in größeren Manufakturen mit einem höheren Grad der Arbeitsteilung. Im frühen 18. Jahrhundert wurden halbmechanische Lumpenschneider eingeführt, die zunächst nach dem „Fallbeilprinzip“ sowie später nach dem „Scherenprinzip“ arbeiteten. In der ersten Hälfte des 19. Jahrhunderts erfolgte der Übergang, statt des Faulens und Reinigens von Hadern, mit Chlor zu bleichen. Der Verlust an Fasern war so geringer, es konnten außerdem auch farbige Stoffe zu weißem Papier verarbeitet werden. Die typische Archivordnung in farbigen Aktendeckeln stammt beispielsweise noch aus der Zeit, als echt gefärbte blaue und rote Lumpen nur zu rosa oder hellblauem Papier verarbeitet werden konnten. Erst im 19. Jahrhundert kommen andersfarbige Aktendeckel (etwa gelb) hinzu.

Aus dem dünnen Papierbrei (Stoff) in der Bütte (= Bottich, daher der Name des Büttenpapiers) schöpfte der Papiermacher das Blatt mit Hilfe eines sehr feinmaschigen, flachen, rechteckigen Schöpfsiebes aus Kupfer von Hand. Das Schöpfsieb zeichnet sich durch einen abnehmbaren Rand, den Deckel, aus. Die Größe des Papierbogens wurde von der Größe des Siebes bestimmt. Nun drückte der "Gautscher" den frischen Bogen vom Sieb auf ein Filz ab, während der Schöpfer den nächsten Bogen schöpfte. Nach dem Gautschen wurden die Bögen in großen trockenen Räumen, vornehmlich auf Speichern und Dachböden, zum Trocknen aufgehängt. Anschließend wurde das Papier nochmals gepresst, geglättet, sortiert und verpackt (eine Pauscht entspricht 181 Bogen Papier). Handelte es sich um Schreibpapier, wurde es geleimt. Dazu wurde es in Leim getaucht, gepresst und getrocknet. Der Leim hindert die Tinte am Verlaufen. Bei Handarbeit, die nur bei Fasern – und somit Papier – hoher Qualität angewendet wird, nehmen die Fasern keine bevorzugte Richtung ein (Isotropie).

Der moderne technische Durchbruch begann sich mit der Erfindung des „Holländers“ um 1670 abzuzeichnen. Es handelt sich um eine Maschine, die den Faserbrei (Pulpe) nicht mehr durch reine Schlageinwirkung aufschließt, sondern durch eine kombinierte Schneid- und Schlageinwirkung. Der Holländer bot aufgrund der hohen Rotationsgeschwindigkeit einen schnelleren Faserdurchgang als das Stampfwerk. Somit stieg die Produktivität der Faseraufbereitung. Üblicherweise wurden Holländer anfangs dort eingesetzt, wo nur geringe Wasserkraft zur Verfügung stand (geringe Antriebsmomente, aber hohe Drehzahlen möglich) und/oder eine Feinzeugaufbereitung einem großen Stampfwerk nachgeschaltet werden sollte. Das Zeitverhältnis für 1 kg Ganzstoff liegt bei etwa 12:1 (Stampfzeit/Holländerzeit), wobei die schonende Stampfung eindeutig den besseren Halbstoff ergibt. Der Holländer wurde in deutschen Papiermühlen ab etwa 1710 umfassend eingesetzt. Durch den höheren möglichen Eintrag im Holländer (ca. 15 kg Stoff im Gegensatz zu 2–5 kg im Stampfwerk) und die geringere erforderliche Mannkapazität verbreitete sich das Gerät schnell. Auch ist der Holländer wartungsärmer als ein Stampfwerk, was sich bei den Reinvestitionskosten erheblich bemerkbar machte. Später wurden dann direkt aus dem Holländerprozess die ersten Stetigmahlerkonstruktionen (Jordan-Mühle "Kegelstoffmühle", Scheibenrefiner) entwickelt.

Ein Papiermacher ist ein Handwerker, der Papier herstellt. In der Gegenwart ist er in einer Papiermühle mit entsprechenden Produktionseinrichtungen (industrielle Papierfabrik) tätig. Seit dem Jahr 2005 heißt der Beruf nach der Klassifikation in Deutschland Papiertechnologe.

In der größten Zahl der Fälle hat jeder leitende Papiermüller ein Wasserzeichen verwendet, das allein für seine Wirkungszeit typisch war. Da die Papiermacher ein Beruf mit einer ausgeprägten Berufstradition innerhalb bestimmter Familien waren, ergänzen sich genealogische und Wasserzeichenforschung gegenseitig. Aus diesem Grunde ist das Deutsche Buch- und Schriftmuseum in der Deutschen Bücherei in Leipzig zugleich Standort einer Papiermacherkartei (siehe Verkartung), in der die Daten von über 8000 Papiermachern, Papiermühlenbesitzern, Lumpensammlern und Papierhändlern samt ihren Familien erfasst worden sind, und einer Kartei der Papiermühlen mit den Papiermachern, die jemals auf ihnen erwähnt worden sind.

Der Mangel an Lumpen, Hadern, die für die Papierherstellung notwendig waren, wurde zum Engpass der Papierherstellung. Deshalb wurde bereits um 1700 nach Alternativen für die Hadern gesucht.

Der französische Physiker René-Antoine Ferchault de Réaumur schrieb 1719 der französischen Akademie der Wissenschaften in Paris:

Einen heute skurril anmutenden Beitrag lieferte der Arzt Franz Ernst Brückmann zu Wolfenbüttel, der sich vornehmlich mit „Erdgewächsen und Mineralien“ befasste. Entsprechend schlug er zur Lösung des Rohstoffproblems "Asbestpapier" vor und ließ 1727 zu Braunschweig einige Exemplare seines Werkes „Historiam naturalem curiosam lapidis …“ oder kurz „Historia naturalis de asbesto“ auf "Asbestpapier" abdrucken. Das Buch enthielt auf diesen unverbrennlichen Bogen auch sein eigenes Bildnis – um „unsterblich“ zu werden.

Frühe und zukunftsweisende Versuche, und sogleich in gewerbsmäßiger Größenordnung, wurden durch den vielseitig genialen braunschweigischen Oberjägermeister Johann Georg von Langen unternommen, denn im Juni 1753 – unter Verweis auf ältere Berichte – gibt er Rechenschaft gegenüber seinem Landesherrn (Carl I.) ab über eine am „Holzminder Bach erbauete Reibe-Mühle, mit Vorstellung des Gebrauchs, so künftig von solcher Mühle zu machen“. Auf dieser Mühle „Porcellain-Masse“ zu mahlen hatte sich gerade zerschlagen, weshalb v. Langen vorschlug, es könne „diese Mühle mit wenig Kosten mit zu Verfertigung des Pack- und anderen Papiers, so aus Holtz gemacht wird, gebrauchet werden.“ Entsprechend hielt er um die herzogliche Konzessionierung an und vermerkte, dass sich „solche (‚Holtz-Papier-Mühle‘) durch Verfertigung einer so gemein nützigen Kauffmanns Waare nicht allein verinteressieren“ würde (wegen der Neuartigkeit dieser Technologie), sondern mit der Zeit auch völlig bezahlt machen würde. Denn er habe „eine neue Art Papier von Holtz Materie erfunden“, so dass um 1760/61 die Aussicht bestand, der Lumpenbedarf werde mit der Zeit spürbar vermindert werden können. Weiteres steht hier leider noch aus. Von Langen hatte sich durchaus auch mit anderen vegetabilischen Stoffen wie 1756 etwa der Verwendung von „Rohr zum Packpapier“ befasst.

Doch umfassendere Experimente führte Jacob Christian Schäffer durch, um Papier aus Pflanzenfasern oder Holz zu gewinnen; dies beschrieb er in sechs Bänden „Versuche und Muster, ohne alle Lumpen oder doch mit einem geringen Zusätze derselben, Papier zu machen“ zwischen 1765 und 1771. Seine Verfahren zur Papierherstellung aus Pappelwolle, Moos, Flechten, Hopfen, Weinreben, Disteln, Feldmelde "Atriplex campestris", Beifuß, Mais, Brennnesseln, Aloe, Stroh, Rohrkolben, Blaukohlstrunken, Graswolle, Maiglöckchen, Seidenpflanzen, Ginster, Hanfschäben, Kartoffelpflanzen, Torf, Waldreben, Tannenzapfen, Weiden- und Espenholz sowie Sägespänen und Dachschindeln ergaben aber kein qualitativ gutes Papier und wurden deshalb von den Papiermüllern nicht verwendet.

Gleichwohl, inspiriert durch die Schäfferschen Versuche, fanden diese im braunschweigischen Räbke bei Helmstedt ihre Neuauflage. Hier wurden im Jahre 1767, unter Anleitung des braunschweigischen Professors Justus Friedrich Wilhelm Zachariae, Experimente mit anderen „vegetabilischen“ Stoffen als den bisher unentbehrlichen weißen Leinen-Lumpen vorgenommen. Dabei wurden von Fachleuten (Papierfabrikanten) Erprobungen mit durchaus aussichtsreichen Materialien wie der Wilden Karde (Weberdistel), Flachs, Hanf, Baumwolle und schließlich gar mit „Pappelweide“ bzw. dem „gemeinen Weidenbaum“ durchgeführt, also auch mit zukunftsweisenden Holzarten.


Friedrich Gottlob Keller erfand Anfang Dezember 1843 das Verfahren zur Herstellung von Papier aus Holzschliff, wobei er auf einem Schleifstein Holz in Faserquerrichtung mit Wasser zu Holzschliff verarbeitete, der zur Herstellung von qualitativ gutem Papier geeignet war. Er verfeinerte das Verfahren bis zum Sommer 1846 durch die Konstruktion von drei Holzschleifermaschinen. Am 11. Oktober 1845 ließ er eine Reihe von Exemplaren der „Nummer 41“ des "Intelligenz- und Wochenblattes für Frankenberg" mit Sachsenburg und Umgebung auf seinem Holzschliffpapier drucken.

Die industrielle Auswertung seiner Erfindung blieb Friedrich Gottlob Keller versagt, weil ihm die Geldmittel zur technischen Erprobung fehlten und die Patentierung des Verfahrens vom Sächsischen Ministerium des Inneren verweigert wurden. So übertrug er am 20. Juni 1846 die Rechte zur Nutzung des Verfahrens gegen ein geringes Entgelt an den vermögenden Papierfabrikanten Heinrich Voelter, der das "Kellersche Holzschliffverfahren" weiterentwickelte, in die Praxis einführte und durch die Entwicklung von Hilfsmaschinen zur großtechnischen Nutzung brachte. Ab 1848 arbeitete Voelter mit dem Heidenheimer Papierfabrikanten Johann Matthäus Voith zusammen mit dem Ziel, Papier zur Massenware zu machen. Voith entwickelte das Verfahren weiter und erfand im Jahr 1859 den Raffineur, eine Maschine, die das "splitterreiche" Grobmaterial des Holzschliffs verfeinert und dadurch eine deutliche Verbesserung der Papierqualität herbeiführt.

Seit etwa 1850 wurde der Holzschleifer eingesetzt, mit dem die Papierherstellung aus dem preiswerten Rohstoff Holz im industriellen Maßstab möglich wurde; um 1879 arbeiteten allein in Deutschland rund 340 solcher Holzschleifereien. Die größte Rohstoffnot wurde durch den Einsatz von Holzschliff zwar gemildert, auf Hadern konnte jedoch nicht zur Gänze verzichtet werden.

Die älteste erhaltene Holzschleiferei ist die Kartonfabrik von Verla in Finnland, die 1882 erbaut wurde. Die 1964 stillgelegte Fabrikanlage wurde 1996 in das Verzeichnis des UNESCO-Weltkulturerbes aufgenommen.

Die Holzschliffpapiere erwiesen sich wegen der in der Schliffmasse enthaltenen Restanteile verschiedener saurer Substanzen als problematisch. Diese Säureanteile stammen aus dem chemischen Aufschlussprozess, der für die Behandlung des zerfaserten Holzstoffes (Lignocellulose) im industriell verbreiteten Sulfitverfahren zwangsläufig benötigt wird. Aus der Schwefligen Säure und ihren Salzen entstehen durch Luftoxidation und Hydrolyse reaktionsrelevante Mengen an Schwefelsäure. Durch die anhaltende Luft- und Luftfeuchteeinwirkung bilden sich weiterhin organische, chemisch sehr aktive Substanzen im Papier. Andere Aufschlussverfahren arbeiten mit Chlorverbindungen und Essigsäure. Diese komplexen Wirkungsmechanismen führen zur Vergilbung sowie zu einer erheblichen Verringerung der Reißfestigkeit, Naßfestigkeit und Biegesteifigkeit im Endprodukt, was sich als „Brüchigkeit“ des Papieres bemerkbar macht. Die verringerte Stabilität im Papier ist eine Folge der durch Säure katalysierten Spaltung des Cellulose­moleküls, die in Form einer fortschreitenden "Kettenverkürzung" abläuft. Hauptursache für das Vergilben des "Holzschliffpapieres" sind das Lignin und seine hierbei entstehenden Zersetzungsprodukte (überwiegend aromatische Verbindungen).

Häufig wird das Holzschliffpapier fälschlicherweise mit säurehaltigem Papier gleichgesetzt. Das säurehaltige Papier ist eine Folge des Herstellungsprozesses und einiger chemischer Zusätze seiner Leimung. Holzschliffpapier vergilbt besonders stark und verliert schnell seine Elastizität. Billiger Holzschliff und die 1806 erfundene Leimung mit verseiften Harzen wurden massenhaft eingesetzt, so dass insbesondere Papiererzeugnisse (Bücher, Graphiken, Zeitungen, Landkarten) seit der Erfindung der Holzschlifftechnologie durch Friedrich Gottlob Keller nach 1846 und in der ersten Hälfte des 20. Jahrhunderts aufgrund beider Ursachen in besonderer Weise den inneren Schadwirkungen unterliegen. Die Restaurierung ist kompliziert und bei hohen Zerfallsraten der Zellulose nur noch durch Massenentsäuerung und nachträglichen Stabilisierungsverfahren wie beispielsweise durch das Papierspaltverfahren möglich.

So hat das Holzschliffpapier nicht nur einen Nutzen für die kostengünstige Herstellung von Papier gebracht, sondern auch einen großen Schaden für die schriftliche Überlieferung des 19. und 20. Jahrhunderts.

Seit den 1980er Jahren wird für den Druck hochwertiger Publikationen und Grafiken überwiegend ein „alterungsbeständiges Papier“ oder „säurefreies Papier“ verwendet. Dieses ist durch geeignete chemische Zusätze frei von freien Säuren und freien Chloriden und wird in der DIN EN ISO 9706 genormt.

Unabhängig von der Faserart kann Papier in "Handarbeit" oder "maschinell" hergestellt werden. Für die maschinelle Erzeugung hat sich die Papierindustrie (Wirtschaftszweig „Herstellung von Papier, Karton und Pappe“) etabliert.

Papier besteht hauptsächlich aus Cellulosefasern, die wenige "Millimeter" bis zu einigen "Zentimetern" lang sind. Die Cellulose wird zunächst weitgehend freigelegt, also von Hemicellulosen, Harzen und anderen Pflanzenbestandteilen getrennt. Der so gewonnene Zellstoff wird mit viel Wasser versetzt und zerfasert. Diesen dünnen "Brei" nennt der Papiermacher „Stoff“ oder „Zeug“. Wenn dieser in einer dünnen Schicht auf ein feines Sieb gegeben wird, hat er einen Wassergehalt von über 99 % (Papiermaschinenauflauf) beziehungsweise etwa 97 % bei der "Handschöpferei". Ein Großteil des Wassers tropft ab. Das Sieb muss bewegt werden, sodass sich die Fasern möglichst dicht über- und aneinander legen und ein "Vlies", das "Papierblatt", bilden. Wenn das Papier getrocknet ist, kann die Oberfläche mit Hilfe von "Stärke", "modifizierter Cellulose" (beispielsweise Carboxymethylcellulose) oder Polyvinylalkohol geschlossen werden. Dieser Vorgang wird als Leimung bezeichnet, obwohl der Begriff Imprägnierung der richtige wäre. "Leimung" erfolgt mit Harzseifen oder Alkylketendimeren innerhalb des Stoffes (Masseleimung in der Papiermaschine oder Bütte).

Wird auf dem "Handschöpf"- oder "Rundsieb" ein Muster aus Draht angebracht, lagern sich an dieser Stelle weniger Fasern ab und das Muster ist beim fertigen Papier zumindest im Gegenlicht als Wasserzeichen zu erkennen. Wasserzeichen werden fast ausschließlich nur noch auf der Papiermaschine als Egoutteurwasserzeichen gefertigt.

Die für das Papier notwendigen Ausgangsstoffe lassen sich in vier Gruppen einteilen. 

Die Faserstoffe unterteilen sich wiederum in zwei Gruppen.

Die Cellulose ist die eigentliche, qualitativ hochwertige Fasergrundlage eines jeden Papieres. Cellulose ist ein Polysaccharid der Kohlenhydrate mit der angenäherten chemischen Formel (CHO), aus dem fast alle Zellwände von Pflanzen und Hölzern bestehen.

Cellulose kann aus Holz, Einjahrespflanzen (beispielsweise Stroh), Hadern, Kunststoff-Fasern und Altpapier (zunehmend zur Hälfte der eingesetzten Stoffe) gewonnen werden. Cellulose besteht aus sehr vielen, kettenförmig miteinander verknüpften Glukoseresten. Die einzelnen Cellulosemoleküle sind also kettenförmige Makromoleküle, deren kleinste Glieder Glukoseeinheiten sind. Das Glukosemolekül (CHO), das Monomer der Cellulose, bildet mit einem weiteren Glukosemolekül durch Lösung eines Wassermoleküls eine Cellobiose. Das Aneinanderreihen solcher Cellobiosen zu einer Kette bildet ein Cellulosemolekül (es entsteht ein Polymer).

Die "Kettenmoleküle" bilden miteinander Mizellen, das sind Molekülbündel, aus denen sich die Fibrillen aufbauen. Erst eine größere Anzahl Fibrillen bildet die sichtbare Cellulosefaser. Die Molekülbündel bestehen aus "kristallinen" Bereichen (regelmäßige Molekül-Führung) und amorphen Bereichen (unregelmäßige Molekülführung). Die "kristallinen" Bereiche sind für die "Festigkeit" und "Steifheit", die "amorphen" Bereiche für die "Flexibilität" und "Elastizität" des Papiers verantwortlich. Die Länge der Kette, also die Anzahl der Monomere, variiert je nach Papierrohstoff und ist für die "Qualität" und "Alterungsbeständigkeit" von großer Bedeutung.

Das nicht aufbereitete Fasermaterial zur Papierherstellung nennt der Papiermacher "Halbstoff".

Zu nahezu 95 % wird Papier aus Holz (in Form von "Holzstoff", "Halbzellstoff", "Zellstoff" oder "Altpapier") hergestellt. Faserbildung und Härte des Holzes spielen bei der Auswahl als Papierrohstoff eine Rolle, nicht jedes Holz ist für jede Papierart gleich gut geeignet. Häufig werden Nadelhölzer wie "Fichte", "Tanne", "Kiefer" und "Lärche" verwendet. Aufgrund der längeren Fasern gegenüber Laubhölzern verfilzen diese Fasern leichter und es ergibt sich eine höhere Festigkeit des Papiers. Aber auch Laubhölzer wie "Buche", "Pappel", "Birke" und Eukalyptus werden gemischt mit "Nadelholz-Zellstoff" eingesetzt. Die Verwendung sehr kurzfaseriger Harthölzer ist auf hoch ausgerüstete Spezialpapiere beschränkt.

Die Verfügbarkeit und die regionalen Gegebenheiten bestimmen hauptsächlich, welche Holzart als "Primärrohstoff" eingesetzt wird, wobei seit den 1960er Jahren große Mengen an Holz für die Papierherstellung mit sogenannten Holzspänetransportern weltweit über See verschifft werden. Allerdings muss auch beachtet werden, dass die Eigenschaften des gewinnbaren Zellstoffes mit der gewünschten Papierbeschaffenheit korrelieren. Schnellwüchsige Hölzer wie Pappeln kommen dem großen Bedarf entgegen, eignen sich jedoch nur für "voluminöse", "weiche" und "weniger reißfeste" Papiere. Zellstoffe aus "Laubhölzern" haben "kürzere" und "dünnere" Fasern als jene aus "Nadelhölzern". Entsprechend den späteren Anforderungen an das Papier werden unterschiedliche Mischungen von diesen "Kurzfaser"- und "Langfaserzellstoffen" beziehungsweise "Hart"- und "Weichfaserstoffen" eingesetzt. Die Steuerung der Eigenschaften kann geringfügig über den Aufschlussprozess und die spätere Mahlung variiert werden. So kann ein "Fichtenzellstoff" sowohl mit Natronlauge hart erkocht werden als auch "langfaserig" und "weicher" im "Sulfatverfahren". Zellstoffe aus "Einjahrespflanzen" zeigen größtenteils Eigenschaften wie die typischen Nadelholzzellstoffe und werden deshalb auch als Surrogate für diese eingesetzt (etwa Espartogras statt Fichte). Alle cellulosehaltigen Stoffe sind grundsätzlich zur Papierherstellung geeignet. Zunehmend ist die Bedeutung von "Altpapier" als Rohstoff. Papierabfälle werden bis zu 100 % für weniger wertvolle Papiersorten eingesetzt. Bei Feinpapieren gewinnt moderner "Deinkingstoff" immer höhere Einsatzanteile. LWC-Papiere enthalten teilweise bis zu 70 % Altpapier-Stoff ohne nennenswerte Einbuße in der Gebrauchsfähigkeit. Der früher wichtigste Rohstoff, die Hadern (Lumpen), findet nur noch in geringen Mengen Verwendung.

Einen Anteil von 61 % hat der Sekundärrohstoff Altpapier in den 2010er Jahren an den in Deutschland eingesetzten Rohstoffen für Papier, Pappe und Kartonagen erreicht. Da Altpapier bereits einmal zu Papier verarbeitet wurde, enthält es jedoch viele Zusatzstoffe und wurde bereits gemahlen. Die Fasern werden durch die erneute Verarbeitung zu Papier weiter geschädigt, der Anteil der Zusatzstoffe im Verhältnis zu den Faserstoffen nimmt zu. In der Praxis werden Papierfasern im Schnitt nur "fünf" bis "sechsmal" rezykliert.

In Europa und Amerika werden vereinzelt Weizen und Roggen zur Strohfasergewinnung genutzt. "Grassorten" aus Nordafrika wie Alfa- und "Espartogras" können verwendet werden. In Japan wird noch immer "Reisstroh" verwendet, in Indien ist es schnell wachsender "Bambus". Mengenmäßig spielen diese Faserstoffe weltweit im Vergleich zu Zellstoff aus Holz keine große Rolle. Für besondere, "dauerhafte" und "stark beanspruchte" Papiere für Banknoten und Wertpapiere werden in geringem Umfang noch Hadern verwendet.

Weißer Holzschliff

Weißschliff entsteht aus geschliffenen Holzstämmen. Dazu werden geschälte Holzabschnitte mit viel Wasser in "Pressenschleifern" oder "Stetigschleifern" zerrieben. (vergleiche auch Holzschleifer) Im gleichen Betrieb wird die stark verdünnte Fasermasse zu Papier verarbeitet oder zum Versand in "Pappenform" gebracht. Dies geschieht mit Entwässerungsmaschinen.

Brauner Holzschliff

Braunschliff entsteht, wenn Stammabschnitte erst in großen Kesseln gedämpft und dann geschliffen werden.

Thermomechanischer Holzstoff

"Thermomechanischer Holzstoff" (TMP) entsteht aus "gehäckselten" Holzabfällen und Hackschnitzeln aus Sägereien. Diese werden im TMP-Verfahren (Thermo-mechanical-Pulp-Verfahren) bei 130 °C gedämpft. Die Lignin-Verbindungen zwischen den Fasern lockern sich dadurch. Anschließend werden die Holzstücke in Refinern (Druckmahlmaschinen mit geriffelten Mahlscheiben) und Zusatz von Wasser gemahlen. Thermomechanischer Holzstoff hat im Vergleich zum "Holzschliff" eine gröbere Faserstruktur. Werden außerdem Chemikalien zugesetzt, handelt es sich um das "chemo-thermomechanische Verfahren (CTMP)". Durch rein mechanische Verfahren gewonnener Holzstoff (RMP) besteht nicht aus den eigentlichen Fasern, sondern aus zerriebenen und abgeschliffenen Faserverbindungen, diese werden "verholzte Fasern" genannt. Um die "elementaren Fasern" zu gewinnen ist eine "chemische Aufbereitung" des Holzes notwendig.

Holzschnitzel werden in einem Kochprozess chemisch behandelt. Die Fasern werden durch zwölf- bis fünfzehnstündiges Kochen von den Inkrusten, den unerwünschten Holzbestandteilen, Begleitstoffen von Cellulose getrennt. Chemisch betrachtet besteht Holz aus:

Es gibt "Natron"-, "Sulfit"- und "Sulfatverfahren", die nach den eingesetzten Kochchemikalien unterschieden werden. Das „Organocell-Verfahren“ ist eine neue Entwicklung. Vor allem enthaltenes "Restlignin" färbt den Zellstoff nach dem Kochen "gelblich" bis "braun", er muss also gereinigt und gebleicht werden. "Restlignin" und andere unerwünschte Stoffe werden beim Bleichen herausgelöst, chemische Aufhellung beseitigt Verfärbungen. Der gebleichte Zellstoff wird entwässert. Er wird nun entweder direkt zu Papier verarbeitet oder zu Rollen aufgewickelt. Die Ausbeute ist bei der Zellstoffherstellung geringer als bei der Holzstoffherstellung. Zellstofffasern aber haben den Vorteil, dass sie länger, fester und geschmeidiger sind. Aus "Nadelholz" gewonnene Zellstofffasern sind ca. 2,5 mm bis 4 mm lang, aus "Laubholz" gewonnene sind etwa 1 mm lang. Der größte Teil, ca. 85 % des benötigten Zellstoffs, vor allem "Sulfatzellstoff", wird aus den skandinavischen Ländern, USA und Kanada importiert. "Sulfatzellstoff" ist im Vergleich zu Sulfitzellstoff "langfaseriger" und "reißfester", somit wird er hauptsächlich für die Herstellung "hochweißer" Schreib- und Druckpapiere verwendet. "Sulfitzellstoff" findet überwiegend Verwendung bei der Herstellung weicher "Hygienepapiere".

Der Faserstoff muss gebleicht werden, damit daraus weißes Papier entstehen kann. Traditionell wurde der Zellstoff mit Chlor gebleicht. Das führt jedoch zu einer hohen Belastung der Abwässer mit "organischen Chlorverbindungen" (AOX). Modernere Verfahren ersetzten "Chlor" durch "Chlordioxid" für ECF-Zellstoffe („elemental chlorine free“ bzw. „Elementar-Chlor-Frei“). Aufgrund der höheren Oxidationswirkung und der besseren Selektivität von Chlordioxid sinkt die AOX Belastung um 60 %–80 %. Wird vollständig auf Chlorverbindungen verzichtet und Sauerstoff, Ozon, Peroxoessigsäure und Wasserstoffperoxid verwendet, wird der Zellstoff mit TCF (totally chlorine free) bezeichnet. Papier aus ECF-Zellstoffen wird als "chlorarm" bezeichnet, (es sind noch Chlorverbindungen vorhanden). "Chlorarme" Druckpapiere sind in hochweißer Qualität schon ab einer flächenbezogenen Masse von 51 g/m herstellbar, chlorfreie erst ab 80 g/m.

TCF-Zellstoff hat eine geringere Faserfestigkeit als chlorgebleichter oder ECF. Vorwiegend aus Holzstoff hergestelltes Papier heißt "holzhaltig", im Handel "mittelfein". Da "Lignin", "Harze", "Fette" und Gerbstoffe im "Faserbrei" verbleiben, sind sie von geringerer Qualität als "holzfreie Papiere".

Das einst im niederbayerischen Kelheim großtechnisch und weltführend umgesetzte Organocell-Verfahren dient der "schwefelfreien" und damit umweltfreundlicheren Zellstoffproduktion. In mehreren Kochstufen werden die "Holzschnitzel" in einem Ethanol-Wasser-Gemisch unter Zusatz von "Natronlauge" bei Temperaturen von bis zu 190 °C unter Druck aufgeschlossen. Dabei lösen sich "Lignin" und "Hemicellulose". Es folgen verschiedene "Waschstufen", in denen der Zellstoff von der Kochflüssigkeit befreit wird sowie "Bleichen" und "Entwässern".

Der Zellstoff wird in drei Stufen gebleicht:
Ethanol und Natronlauge, die Kochchemikalien, werden in einem Recyclingverfahren, welches parallel zur Zellstoffproduktion abläuft, zurückgewonnen. Es werden "schwefelfreies" Lignin und schwefelfreie Hemicellulose gewonnen, die von der chemischen Industrie verwendet werden können.

Durch Zerkleinern und Kochen in Natronlauge wird aus Stroh der Halbstoff "Strohzellstoff" oder, bei anderer Aufbereitung, "gelber Strohstoff".

Im "Kugelkocher" werden Hadern gekocht. Dazu werden sie zunächst sortiert, im Haderndrescher gereinigt. Mit Kalklauge und Soda werden die Hadern unter Dampfdruck von 3 bar bis 5 bar im Kugelkocher gekocht. Dabei werden "Farbstoffe zerstört", "Fett verseift" und "Schmutz gelöst". Während des mehrstündigen Kochens lockert sich das Gewebe der Hadern und sie lassen sich anschließend leicht zu Halbstoff zerfasern.

Der "Pulper" (Stoffauflöser) ist eine Bütte mit rotierendem Propeller. In ihm wird nach "Güteklassen" sortiertes, "zu Ballen" gepresstes Altpapier mit viel Wasser "zerkleinert" und mechanisch "aufgelöst". So werden die Fasern des Altpapiers geschont. Ein Arbeitsgang, der früher häufig mit dem Kollergang durchgeführt wurde. Der pumpfähige "Faserbrei" ist noch verunreinigt. Er gelangt im "Pulper" in einen Zylinder und wird von einem Rotor zerfasert. Dann wird der grob gelöste Stoff durch ein Sieb gedrückt. Infolge der Zentrifugalkraft werden grobe Verunreinigungen ausgeschieden. An der Zylinderachse sammelt sich der leichte Schmutz. Weitere Fremdstoffe wie "Wachse" und "Druckfarben" werden in Spezialanlagen herausgelöst.

Beim "Deinking" werden die Druckfarben mit Hilfe von Chemikalien (Seifen und Natriumsilicat) von den Fasern des Altpapiers gelöst. Durch "Einblasen" von Luft bildet sich an der Oberfläche des "Faserbreis" "Schaum", in welchem sich die Farbbestandteile sammeln und abgeschöpft werden können. Dieses Trennverfahren heißt Flotation.

Der "Halbstoff" wird durch Faserstoffmahlung und Mischung zum "Ganzstoff" verarbeitet. Die "Halbstoffe" werden in Refinern (Kegelstoffmühle) weiter zerfasert. Als "dicker Brei" fließt das Halbfertigprodukt im Refiner zwischen einer Messerwalze und seitlich befestigten Grundmessern hindurch. Die Fasern werden dabei zerschnitten ("rösche Mahlung") oder zerquetscht ("schmierige Mahlung"), je nach Einstellung der Messer. Die Enden der gequetschten Fasern sind "fibrilliert" (ausgefranst), was bei der "Blattbildung" zu einer besseren Verbindung der Fasern führt.


Außerdem können die Fasern bei der Mahlung "lang" oder "kurz" gehalten werden, wobei die "langen" Fasern "stärker verfilzen" als die "kurzen". Es ergeben sich daraus "vier" verschiedene Möglichkeiten der Mahlung. Faserlänge und Mahlart bestimmen Faser- und Papierqualität. Übliche Kombinationen sind "rösch" und "lang" oder "schmierig" und "kurz".

Die Messer des Refiners liegen bei der Kurzfasermahlung sehr eng aneinander, sodass fast kein Zwischenraum vorhanden ist. Das Mischen der verschiedenen "Halbstoffe" sowie die Zugabe von "Füll"-, "Leim"- und "Farbstoffen" gehören zur Herstellung des "Ganzstoffes".

Auf der Papiermaschine wird die "Papierbahn" gebildet. Folgende Maschinenstationen sind hintereinander geschaltet:

Die "Blattbildung" findet bei der industriellen Papierproduktion auf der Papiermaschine statt. Der "gereinigte" und "entlüftete Papierbrei", welcher zu ca. 99 % aus Wasser besteht, wird im "Stoffauflauf" zu einem dünnen, möglichst gleichförmigen Strahl geformt. Dieser trifft bei "Langsiebpapiermaschinen" auf ein rotierendes, endloses Sieb (siehe dazu auch Metalltuch). Innerhalb weniger Sekunden läuft ein sehr großer Teil des Wassers ab und die Papierstruktur entsteht. Hierbei tragen unter dem Sieb angebrachte "Sauger" sowie "Pulsationen" erzeugende "Foils" zur Entwässerung des Faserstoffs bei. Oftmals wird auch versucht, die Temperatur der "Suspension" zu erhöhen (beispielsweise über Dampfblaskästen), was über eine niedrigere Viskosität ebenfalls die Entwässerung fördert. Soll das Papier ein Wasserzeichen enthalten, ist dieses in das Sieb eingearbeitet oder wird von oben mittels einer sogenannten "Egoutteurwalze" aufgebracht.

Auf Langsiebpapiermaschinen gefertigtes Papier hat wegen der einseitigen Entwässerung i. d. R. eine "ausgeprägte Zweiseitigkeit". Diese drückt sich neben den unterschiedlichen Oberflächen (die "Siebseite" und die glatte Filz- oder "Schönseite") meist auch in einer sehr ungleichmäßigen Verteilung der Füllstoffe innerhalb des Papieres aus. Dies hat neben unterschiedlicher Bedruckbarkeit oftmals auch eine Rollneigung (Curl) zur Folge. Abhilfe verschafft hier teilweise die Entwässerung über ein zweites Sieb nach oben (sogenannte Hybrid-Former), die zudem die Gesamtentwässerungsleistung erhöht.

Langsiebpapiermaschinen geraten jedoch spätestens ab Geschwindigkeiten von ca. 1200 m/min an physikalische Grenzen, da die erzeugten "Luftverwirbelungen" über dem Langsieb die Formation zerstören. Moderne Papiermaschinen, insbesondere für graphische Papiere und Tissue, produzieren jedoch mit Geschwindigkeiten von bis zu 2000 m/min bei "Arbeitsbreiten" von mehr als zehn Metern. Daher sind für diese Maschinen andere Stoffauflaufkonzepte entwickelt worden, sogenannte "Gap-Former": Hierbei wird der "Papierbrei" direkt in einen Spalt zwischen zwei rotierende Siebe gespritzt. Neben der höheren Laufgeschwindigkeit bieten "Gap-Former" auch eine deutlich gleichmäßigere Entwässerung und damit verminderte Zweiseitigkeit. Bei Papieren aus maschineller Produktion verlaufen fast alle Fasern parallel zur Siebrichtung.
Am Ende des Siebes wird die weiche Papierbahn auf einen Filz übergeben und gelangt in die "Pressenpartie". Traditionelle Pressenpartien bestehen aus drei bis vier aufeinanderfolgenden Pressen, in denen die Papierbahn mittels gegeneinandergepresster Walzen zwischen Filzen entwässert wird. Seit Anfang der 1990er Jahre hat sich jedoch zunehmend das Konzept der "Schuhpresse" durchgesetzt, bei der eine Walze den Filz und das Papier in einen polymerbespannten 'Schuh' presst. Dies hat eine deutlich größere "Niplänge" zur Folge, womit sich eine schonendere und zugleich stärkere Entwässerung erzielen lässt.

In der "Trockenpartie" findet schließlich die endgültige Entwässerung statt. Hier läuft die Papierbahn durch eine Anzahl dampfbeheizter "Trockenzylinder" und wird anschließend "geglättet" und "aufgerollt". In einigen Fällen ("hochglatte" und "scharf" satinierte Papiere) wird vor dem endgültigen Aufrollen noch ein weiterer Glättungsschritt im Kalander vollzogen.

Der "Papierbrei" enthält außerdem:

Füllstoffe

Neben den Faserstoffen werden bis zu 30 % Füllstoffe dem Ganzstoff hinzugefügt. Diese können sein:

Durch das Ausfüllen der Zwischenräume zwischen den Fasern machen die Füllstoffe das Papier weicher und geschmeidiger und geben ihm eine glatte Oberfläche. Der Massenanteil der Füllstoffe drückt sich in der „Aschezahl“ aus. Bei Spezialpapieren, die, wie im Fall des „Theaterprogrammpapieres“, "raschelfrei" sein sollen, wird ein hoher Aschegehalt mit langen Fasern kombiniert. Auch Zigarettenpapier wird stark gefüllt, damit es glimmt und nicht abbrennt.

Die Zusammensetzung und Kristallstruktur der Füllstoffe bestimmen Transparenz und Opazität eines Papiers sowie die Farbannahme beim Druck mit wegschlagenden Farben. Für die Tintenfestigkeit hingegen ist Leim notwendig. Füllstoffe können teilweise auch die Eigenschaften der Farbstoffe mit übernehmen. Viele Pigmentfarbstoffe sind auch ein effektiver Füllstoff.

Farbstoffe

Auch weiße Papiere enthalten manchmal Farbstoffe, die in unterschiedlichen Mengen zugesetzt werden, denn auch optische Aufheller zählen zu den Farbstoffen. Es werden für Buntfarben vor allem synthetische Farbstoffe verwendet. Wichtig beim Papierfärben ist die Abstimmung des Farbsystems auf die "Fasereigenschaften" und das verwendete "Leimungssystem". Grundsätzlich werden saure (substantielle, selbstaufziehende) Farbstoffe und alkalische oder saure Entwicklungs- also Verlackungsfarbstoffe eingesetzt. Erstere sind einfach in der Anwendung reagieren aber empfindlich auf pH-Wert-Schwankungen mit mangelhafter Fixierung. Letztere neigen, der nötigen Fällungsreaktion wegen, zur Verlackung jenseits der Faser, sodass ein Großteil der Flotte unwirksamen Farbverlust aufweist. Farbstoffe reagieren vorzugsweise auf Cellulose oder Holzbestandteile, selten auf beides. Die Auswahl des richtigen Systems passend zum zu färbenden Zellstoff ist wichtig. Eine Sondergruppe stellen die natürlichen oder Pigmentfarbstoffe (Körperfarben) dar. Beide sind nur begrenzt wirksam, da sie meist durch Einlagerung im Lumen und durch "Kapillarretention" im Blatt gehalten werden. Intensivtönungen sind nur mit Küpenfärbung (Indigo) oder Rotpigmenten (Rotlack, Cochenille) möglich.

Leimungsstoffe

Leim macht das Papier beschreibbar, weil es weniger saugfähig und weniger hygroskopisch wird. Leimung ist in der Papiermacherei die Hydrophobierung der Fasern. Die Leimstoffe sind chemisch modifizierte (verseifte) Baumharze in Kombination mit "sauren Salzen", wie Kalialaun oder Aluminiumsulfat. Auch Polymere auf Basis von Acrylaten oder Polyurethanen werden eingesetzt.

Zunehmend werden neben verschiedenen Harzen ASA (Alkenyl Succinic Acids = Alkenylbernsteinsäureanhydride) und Alkylierte Ketendimere (AKD, Ketenleimung) zur Leimung von Papier eingesetzt. Die früher häufig verwendete, saure Leimung mit Harzsäuren und Alaun ist der Hauptgrund dafür, dass so geleimte Papiere bei der Archivierung zerstört werden. Das statt des "Alauns" benutzte "Aluminiumsulfat" kann durch überschüssige "Restionen" Schwefelsäure bilden, die wiederum die Cellulose zerstört. So wird die "Leimung" meist im "neutralen" oder "schwach" alkalischen pH-Bereich durchgeführt. Einige Papierfarbstoffe verlangen aber eine saure Leimung, wobei die Einstufung "sauer" oder "alkalisch" sich lediglich auf den prozessbedingten pH-Wert der "Bütte" bezieht, nicht auf das fertige Endprodukt. Die Wahl der Papierleimung wird ebenfalls durch nachfolgende Arbeitsschritte beeinflusst. Nach dem Bedrucken kann Bindemittel der Druckfarbe in das Papier wegschlagen, den Leimgrad senken und die "Beschreibbarkeit" des bedruckten Papiers deutlich "verringern".

Prinzipiell wird bei der Leimung zwischen "Masseleimung" und der "Oberflächenleimung" unterschieden. Bei der "Masseleimung" wird das Leimungsmittel der Flotte zugegeben, bei der "Oberflächenleimung" wird das schon fertige Papier beschichtet. "Verseifte Harze", "Alkylketendimere" und "ASA" sind typische Masseleimungsmittel, polymere Leimungsmittel wie Gelatine oder Stärkederivate sind eher als Oberflächenleimungsmittel im Gebrauch. Über den möglichen Einsatz als effektives "Masseleimungsmittel" entscheiden vor allem die Eigenretention und der technisch mögliche Einsatz von "Retentionschemikalien".

Nassfestmittel

"Unbehandeltes" Papier wird "mechanisch unbeständig", wenn es "feucht" oder "nass" wird. Durch die Aufspaltung der Wasserstoffbrücken unter Wasserzutritt verliert das "Faservlies" seinen inneren Zusammenhalt. Papier wird deshalb auch als "hydroplastisch" bezeichnet. Um auch im nassen Zustand eine – wenn auch beschränkte – mechanische Festigkeit zu erhalten, werden dem Papier bei der Herstellung Nassfestmittel (etwa Luresin) zugesetzt. Reißfestes "Küchenkrepp" dürfte das bekannteste Papier dieser Klasse sein, aber auch Kartons, Landkartenpapiere oder Sicherheitspapier für Geldnoten enthalten große Mengen "Nassfestmittel". Nassfestmittel sind im Verarbeitungszustand "wasserlösliche" Polymere, die vorrangig aus Polyaminen und Epichlorhydrinderivaten hergestellt werden und mit den Papierfasern reagieren. Dabei bilden sich "wasserunlösliche Quervernetzungen" zwischen den Fasern, die den "Papierfilz" stabilisieren. Die hydrophobe Verkettung verhindert jedoch ein erfolgreiches Recycling, so dass der zunehmende Einsatz von "Nassfestmitteln" im "Hygienepapierbereich" weitreichende Konsequenzen für die Altpapierverwertung hat. Der Anfall von "unlösbaren Stippen" im normalen Löseprozess ist beständig steigend. Werden Nassfestmittel (ähnlich wie Bitumenklebstoffe) chemisch aufgebrochen, so "degradiert" die Faser untypisch "schnell". Die Altpapierqualität nimmt somit schneller ab als bei normalen Recyclingprozessen. "Nassfestmittel" dürfen nicht mit Leimungschemikalien (beispielsweise AKD) verwechselt werden, da der "chemo-physikalische" Wirkprozess verschieden ist. So ist etwa ein "nassfestes", "ungeleimtes" Papier nach wie vor "hoch" kapillar, wohingegen ein "überleimtes" Papier sich trotzdem nach langem Wasserzutritt zerfasern lässt.

Gestrichenes Papier (auch Kunst- oder Bilderdruckpapier) ist ein Papier, bei dem die Oberfläche mit einer aus "Pigmenten", "Bindemittel" und Additiven bestehenden Streichfarbe („Strich“) veredelt ist. Das Papier bekommt eine "geschlossene", "glatte" und "stabile" Oberfläche, wodurch eine bessere Qualität beim Druck erreicht wird.

Formate nach EN ISO 216, DIN 476:

Die bekanntesten international genormten Papierformate sind diejenigen der "A-Reihe" nach DIN 476 "Papierformat", die seit 2002 teilweise durch EN ISO 216 ersetzt ist. In einigen Ländern wie den Vereinigten Staaten und Kanada werden andere Formate verwendet.

Zur Verarbeitung von Papier, insbesondere dem Zuschneiden auf bestimmte Formate, steht eine Reihe an Werkzeugen zu Verfügung. Von alters her "Schere" und "Papier-Messer", in neuerer Zeit "Papierschneidemaschinen":

Die Papierindustrie unterscheidet folgende vier Großgruppen von Papiersorten:

Weltweit werden jährlich 406 Millionen Tonnen (Stand: 2014) Papier, Karton und Pappe produziert. Die größten Produzenten (Stand: 2014) sind China (108 Millionen Tonnen), die USA (73 Millionen Tonnen), Japan (26 Millionen Tonnen) und Deutschland (22,5 Millionen Tonnen).

Ein Drittel der Kapazitäten für die Papierproduktion weltweit entfällt auf die europäische Papierindustrie. Europa ist führend bei der Herstellung von Druck- und Schreibpapier, gefolgt von Asien und Nordamerika, und hat einen Anteil von knapp 26 % an der gesamten Papier- und Pappeproduktion. Durch die Konsolidierung der europäischen Papierindustrie im letzten Jahrzehnt ist die Zahl der Unternehmen, Papierfabriken und Papiermaschinen in Europa gesunken, die Produktionskapazität jedoch gleichzeitig erheblich gestiegen. Es wird geschätzt, dass die 20 größten Papierhersteller derzeit einen Anteil von fast 40 % an der weltweiten Papier- und Pappeproduktion haben. Der Umsatz der europäischen Papierindustrie betrug 2015 rund 79 Milliarden Euro. 180.000 Menschen arbeiten in der europäischen Zellstoff- und Papierindustrie. Neben großen Papierherstellern wie UPM-Kymmene, Stora Enso, International Paper, Svenska Cellulosa Aktiebolaget (SCA), Metsä Board, Sappi oder der Smurfit Kappa Group existiert eine große Zahl mittelgroßer und kleinerer Papierhersteller wie z. B. die Papierfabrik Palm oder die Kartonfabrik WEIG.

Die deutsche Papierindustrie, deren Interessen durch den Verband Deutscher Papierfabriken (VDP) vertreten werden, ist mit einem Produktionsvolumen von 22,6 Millionen Tonnen (2015) an Papier, Karton und Pappe die Nummer Eins in Europa und steht weltweit hinter China, den USA und Japan an vierter Stelle. Die Produktion gliedert sich in vier Hauptsortengruppen. "Verpackungspapiere" und -"karton" haben einen Anteil von 49 %. 38,4 % des Produktionsvolumens entfällt auf "grafische Papiere". Die restlichen 12,6 % entfallen fast zu gleichen Teilen auf "Hygienepapiere" (6,4 %) sowie auf das breite Spektrum der "technischen- und Spezialpapiere" (6,2 %). Die rund 40.600 Mitarbeiter erwirtschaften in der deutschen Zellstoff- und Papierindustrie in 162 Werken einen Umsatz von 14,4 Milliarden Euro (2015), ein Plus von 0,9 % gegenüber dem Vorjahr.

Gründe zur Papierforschung ergeben sich aus sehr verschiedenen wissenschaftlichen Ansätzen. Neben technischen Fragestellungen der Papierindustrie sind das auch komplexe Themen in historischen Bibliotheks- und Archivbeständen. Dazu gehören beispielsweise die Herkunftsorte historischer Papiere einschließlich ihrer Wasserzeichen sowie das Alterungsverhalten aus "konservatorischer" und "restauratorischer" Sicht. Auf diesem Gebiet sind weltweit zahlreiche "wissenschaftliche Bibliotheken" und einige "private Institutionen" tätig.

Die industrielle Papierforschung wird in Deutschland gebündelt in der Papiertechnischen Stiftung (PTS), die im Jahr 1951 gegründet wurde und von den Unternehmungen der Papierindustrie gefördert wird. Es werden Auftragsforschungen und Dienstleistungen für die Papierindustrie und deren Zulieferfirmen erbracht. Darüber hinaus betreiben verschiedene Zulieferer eigenständige Forschungsanlagen.

Die Technischen Universitäten in Darmstadt und Dresden, die Fachhochschule München sowie die Duale Hochschule Baden-Württemberg in Karlsruhe bilden Papieringenieure aus. Forschungsschwerpunkte in Darmstadt sind Recyclingverfahren sowie Wasserkreisläufe, in Dresden wird vornehmlich zu "Energieeffizienz" sowie "Oberflächeneigenschaften" geforscht.

Eine weitere Forschungsanlage betreibt der größte Hersteller für chemische Produkte zur Papierherstellung, die BASF in Ludwigshafen, teilweise in Partnerschaft mit der Omya.

Naturpapier ist ein Oberbegriff für alle Papiere, die aus Naturfasern ohne Zusatz von synthetischen Fasern und ohne Oberflächenbehandlung "maschinenglatt" oder "satiniert" sind.

Quelle: "Papiereigenschaften"


"Masse, Dichte und Dicke von Schreibpapier"
Die "Masse" (bzw. umgangssprachlich auch das "Gewicht") von Papier wird meist "flächenbezogen" angegeben – konkret in Gramm pro Quadratmeter (g/m).

Die flächenbezogene Masse von normalem Schreibpapier beträgt 80 g/m, ein A4-Blatt hat damit eine Masse von 5 g. Drei dieser Blätter plus Briefumschlag liegen somit gerade unter der für einen Standardbrief erlaubten Höchstmasse von 20 g. 1000 Blatt A4-Papier wiegen daher 5 kg und 200.000 Blatt A4-Papier wiegen schon eine ganze Tonne.

Die auf das Volumen bezogene Dichte von normalem Schreibpapier liegt das Standard-Volumen in der Größenordnung von 800 kg/m, die Dicke also bei 0,1 Millimetern.

Dicke caliper(US), caliper(UK) Stärke (in Ìm; USA: in mil = 1/1000 inch = 25,4 μm) eines Einzelbogens (DIN EN 20534, ISO 534: Papier/Pappe; FEFCO 3: Wellpappe).

Papiere für den Buchblock von literarischen oder wissenschaftlichen Büchern haben üblicherweise 80–100 g/m bei 1,0–1,8-fachem Volumen (abgekürzt: "VOL").
Auch die Dicke eines Buchblocks = Anzahl Blatt * Papiergewicht / (80 g/m) * Volumen * 0,1 mm ist auf das Standard-Volumen bezogen.

Je nach flächenbezogener Masse ändert sich die Bezeichnung für den aus Papier bestehenden Bedruckstoff (umgangssprachlich sind Überschneidungen zwischen Papier, Karton und Pappe möglich):

"Basisgewicht basis weight" im englischen Sprachraum gebräuchliche Angabe des Substratgewichts; während in Amerika noch immer die Masse (1 liber = 453,6 g) von 500 Bogen sowie deren Format (1 square inch = 6,45 cm) angegeben werden, wird im internationalen Papierhandel unter dem "Basisgewicht" heute die flächenbezogene Masse (in g/m, Grammatur) verstanden.

Grundsätzlich ist bei allen Messungen zu beachten, dass Luftfeuchtigkeit und "Temperatur" einen sehr großen Einfluss auf die Messwerte haben. Deshalb findet die Messung immer in Klimaräumen bei einem nach ISO-Normen festgelegten "Normklima" (23 °C, 50 % Luftfeuchtigkeit) statt. Meist wird die Papierprobe vor der Messung 24 Stunden in dem Raum gelagert, um sie zu akklimatisieren. Da die Messungen von der flächenbezogenen Masse des Papiers (auch Flächengewicht oder Grammatur genannt) abhängen, werden so genannte "Laborblätter" mit einer nach ISO-Norm festgelegten flächenbezogenen Masse verwendet.

Die Porosität gibt an, wie viel Luft ein Papier durchlässt. Die Maßeinheit der Porosität lautet "Gurley". Dazu wird das Normblatt in den Prüfapparat eingespannt und der Prüfapparat drückt 100 ml Luft mit 1,23 kPa durch eine Prüffläche von 6,42 cm und misst die dafür benötigte Zeit. Eine Zeitdauer von einer Sekunde entspricht dabei einem Gurley.


Zugfestigkeit

Prüfungen nach DIN EN ISO 1924: Quotient (in kN/m) aus Bruchlast und Breite eines Papierstreifen; abgeleitet: Zugindex/-steifigkeit (in N/m); Zugsteifigkeitsindex (in Nm/kg) als Quotient aus Zugfestigkeit und Grammatur.

Die Zugfestigkeit ist einer der zentralen physikalischen Werte bei der Papierherstellung, bei Kraftpapier ist sie sogar der wichtigste Wert. Die Maßeinheit der auf "die Breite" der Papierprobe bezogenen Zugfestigkeit ist N/m. Da die Zugfestigkeit vorwiegend von der flächenbezogenen Masse abhängt, wird auch der "Zugfestigkeitsindex" (ZFI) mit der Maßeinheit Nm/g verwendet.

Zur Bestimmung dieses Wertes wird eine Zerreißprobe gemacht. Dazu werden Papierstreifen einer genormten Länge und Breite mechanisch eingespannt, der so genannte „Reißapparat“ zieht die Probe auseinander und zeichnet die benötigte Kraft auf. Die im Moment des Zerreißens benötigte Kraft ist die Zugfestigkeit. Um einen Durchschnittswert zu erhalten, werden meist zehn Streifen zerrissen, wovon "5 längs" der Laufrichtung und "5 quer" zur Laufrichtung der Papiermaschine genommen werden. Als Nebenprodukt dieser Messung werden noch die Bruchdehnung und die "Zugbrucharbeit" ermittelt. Die Bruchdehnung wird in Prozent angegeben und gibt an, um wie viel Prozent der Papierstreifen sich im Moment des Bruchs verlängert. Die "Zugbrucharbeit" wird in J/m angegeben und ist die aufgewendete Zugkraft pro Papierfläche.

Spezifischer Weiterreißwiderstand

"Durch-/Weiter-/Fortreißfestigkeit" ISO 1974, DIN 53115: Brecht-Imset, (DIN EN 21974: grammaturbezogener "Elmensdorf-Durchreißindex" in mNm/g).

Die Maßeinheit des "spezifischen Weiterreißwiderstandes" ist mN·m/g. Diese Maßeinheit gibt an, wie leicht ein Papier, das bereits eingerissen ist, weiterreißt. Dazu wird das Papier mit einem Schnitt versehen und in das "Reißfestigkeitsprüfgerät" (nach Elmendorf) eingespannt. Durch einen Knopfdruck wird ein blockiertes Pendel ausgelöst, welches die Probe im Zuge der Pendelbewegung zerreißt und dabei die Kraft misst.

Berstwiderstand

Druck (in kPa), dem ein Substrat nicht mehr standhält; abgeleitet ist der Berstfaktor (Druck durch Grammatur); Berstfestigkeit nach Mullen (DIN ISO 2758: Papier; DIN 53141-1: Pappe), nach Schopper (DIN 53113), an Wellpappe (ISO 2759, DIN/ISO 3689: nass, FEFCO 4).

Der Berstwiderstand gibt den benötigten Druck an, um ein Papier zum Bersten zu bringen. Die Maßeinheit des Berstwiderstandes lautet kPa. Dazu wird das Normblatt in den Prüfapparat eingespannt und eine Membran mit genormter Fläche drückt mit ansteigender Kraft gegen das Papier. Der Druck, der zum Durchstoßen des Papiers erforderlich ist, wird "Berstwiderstand" genannt.

Spaltwiderstand/-festigkeit

Widerstand, den Papier, Karton oder ein Verbund einer senkrecht einwirkenden Dehnung (TAPPI T 541) oder einer Schiebebewegung (Scott-Bond-Test: TAPPI T 833 pm-94 und T 569, Brecht-Knittweis-Spaltwiderstand: DIN 54516) entgegensetzt.

Der "Spaltwiderstand" gibt die aufzubringende Kraft an, welche benötigt wird, die Papierbahn in der Masse zu spalten. Dies wird gewöhnlich bei mehrlagigen Papieren angewandt, bei denen mehrere Papierbahnen nass (25–35 %) vergautscht wurden, so beispielsweise bei Faltschachtelkarton (FSK) oder besonders voluminösen Papieren (Rohdichte <1,5) wie Bierdeckeln.


Prozent-Verhältnis aus den Reflexionsfaktoren eines Einzelbogens über einer "schwarzen Unterlage" und eines Stapels aus mindestens 20 Bogen (DIN 53146, ISO 2471), ferner die Strahlungsdurchlässigkeit im UV-vis-Bereich (DIN 10050-9).

Der "Grad" der Lichtundurchlässigkeit des Papiers bezieht sich auf seine Fähigkeit, Licht nicht durchscheinen zu lassen. Papier ist "lichtundurchlässig", wenn das einfallende Licht "zurückgestreut" oder im Papier absorptiert wird. Je höher die Streuung des Lichts, umso lichtundurchlässiger ist das Papier. Lichtundurchlässigkeit ist eine erwünschte Qualität, die das Durchscheinen des Druckes minimiert. Ein Blatt mit 100%iger Lichtundurchlässigkeit lässt überhaupt kein Licht durchscheinen und damit auch nicht den Druck, sofern die Druckfarbe nicht eindringt. Im Allgemeinen ist die Lichtundurchlässigkeit des Papiers umso geringer, "je niedriger" seine flächenbezogene Masse ist. Der "Weißegrad" und die "Helligkeit" des "Füllstoffs", seine "Kornstruktur" und -"größe", sein Brechungsindex und der "Füllstoffgehalt" sind Faktoren, die die Lichtundurchlässigkeit des Papiers bestimmen.

Die "Helligkeit" ist ein Maß für die Licht "reflektierenden" Eigenschaften des Papiers, die die Wiedergabe von Kontrasten und Halbtönen beeinflussen. Der Unterschied zwischen dem Helligkeitsgrad, der "durch Kaolin" erzielt wird (80 bis 90 auf der ISO-Helligkeitsskala), und dem Helligkeitsgrad, der "durch Calciumcarbonate" erzielt wird (GCC über 90 und PCC 90-95), ist erheblich.


Der Weißgrad ist ein technischer Kennwert für die "Reflexionsfähigkeit" des Papieres für weißes Licht. Er wird idealerweise mit einem Spektralphotometer gemessen. Aus der spektralen Verteilung wird der Zahlenwert nach verschiedenen Formeln berechnet. Für Papier wird meist der Weißgrad nach Berger genutzt. Bei einem normalen Kopierpapier ohne UV-sensible Aufheller liegt der "Weißgrad nach Berger" etwa bei 160. Durch optische Aufheller und Farbstoffe werden die Messergebnisse beeinflusst. Darum wird der Weißgrad üblicherweise unter Normlicht bestimmt, das gegenüber Tageslicht einen geringeren Anteil an kurzwelliger UV-Strahlung hat. Handelsübliche weiße Papiere sind meist "aufgehellt". Unter Normlicht gemessene "neutralweiße" Papiere sehen so unter Glühlampenlicht "gelblicher", im "sonnigen Tageslicht" oder unter Leuchtstofflampen dagegen "bläulich-weiß" aus.

Der Weißgrad gibt lediglich den Unbuntanteil einer gemessenen Fläche bezogen auf eine "ideal weiße" oder "ideal schwarze" Fläche an. Bei zwei Papieren, die messtechnisch den gleichen Weißgrad besitzen, kann ein sichtbarer Farbstich bestehen, der den subjektiven "Weißeindruck" verfälscht. Menschen empfinden leicht "gelbliches" oder "rötliches" Papier als weniger weiß, also "grauer" gegenüber einem leicht "bläulichen" oder "grünlichen" des gleichen Weißgrades.

Der Weißgrad wird als Standardprüfung in der Papierproduktion verwendet. Um unerwünschte "Farbstiche" zu vermeiden, ist vom Anwender neben dem Weißgrad auch der "Farbstich" des Papieres zu beachten. Den Effekt der „Weißgraderhöhung“ durch optische Verschiebung wird unter anderem beim „Bläuen“ des Papieres ausgenutzt. Durch Zugabe "blauer" Pigmente wird ein Gelbstich verringert. Beim sogenannten „Drücken“ wird ein "zu weißes" Papier durch Zugabe "roter" oder "brauner" Pigmente gebrochen. In beiden Fällen nimmt der technische Weißgrad leicht ab, der subjektive "Weißeindruck" jedoch wird beim "Bläuen erhöht" und beim "Drücken verringert".

Während bei der Papierherstellung "von Hand" die Fasern "gleichmäßig" in allen Richtungen liegen, tritt bei der "maschinellen" Papierherstellung, die auf einem Endlossieb erfolgt, eine (teilweise) Ausrichtung der Fasern "längs" des Bandes auf. Die Laufrichtung des Papiers unterscheidet zwischen der Maschinenrichtung (in Laufrichtung) und der Querrichtung (quer zur Laufrichtung). Die "Querrichtung" ist zugleich die Richtung der Faserdicke, so dass in Querrichtung eine etwa "dreifache" Quellung und Schwindung des Papieres gegenüber der Laufrichtung auftritt. Bei Papiermaschinenherstellung liegt die Querrichtung "parallel" zu den "Drehachsen" und "senkrecht" zur "Bewegungsrichtung".

Im Papierhandel und in der Druckerei werden Lauf- und Querrichtung durch die Begriffe "Schmalbahn" und "Breitbahn" einem Format zugeordnet. Die Bezeichnung "schmal" oder "breit" gibt dabei die Richtung der Schnittkante parallel zur Laufrichtung an. Dabei gilt:
Dieses Wissen ist wichtig für die anzuwendende Formatlage bei verschiedenen Maschinenbauarten und zu beachtenden Weiterverarbeitungsprozessen (Falzlagen, späteres Buchformat). So kann der Passer in Umfangsrichtung innerhalb der Druckmaschine verstellt werden, in Querrichtung hingegen nicht. Bei Offsetarbeiten mit hohem Feuchtmittelanfall muss also die erste Platte in der Maschine kürzer eingerichtet werden als die letzte und das Papier muss in Breitbahn laufen, so dass die "Quellung" von Werk zu Werk passgenau ausgeglichen werden kann.

In "Katalogen" und auf "Preisetiketten" wird das Maß quer zur Laufrichtung unterstrichen oder fett ausgezeichnet oder zuerst genannt. Üblich sind auch die Abkürzungen SB (Schmalbahn) und BB (Breitbahn) oder ein Pfeil, der die Laufrichtung markiert.

In Abhängigkeit von der vorherrschenden Faserrichtung beeinflussen "Feuchtigkeit", "Temperatur" und "Alterung" das Papier. Bei einer ungleichmäßigen Ausrichtung ändert somit jede Karte im Laufe der Zeit und mit dem Wechsel der Witterung bzw. des "Raumklimas" ihren genauen Maßstab unterschiedlich in den beiden Richtungen. Nur durch spezielle beziehungsweise geschichtete Papiersorten kann dieser Effekt bei maschinell produzierten Papieren verringert werden.

Bei der Herstellung von Büchern (und anderen aus Papier bestehenden Gegenständen) ist darauf zu achten, dass die Laufrichtung aller Seiten, des Buchdeckel- und Überzugmaterials parallel zum Buchrücken verläuft, da Papier sich immer quer zu seiner Laufrichtung "ausdehnt" bzw. "schrumpft". Andernfalls bricht das Buch leicht an der Bindung auseinander bzw. lässt sich schlecht durchblättern. Wird beim "Verkleben" von Papier und Pappe die "Laufrichtung" der zu kombinierenden Materialien "ignoriert", kommt es zu "wellenartigen" Verwerfungen, die irreversibel sind. Zur Prüfung der Laufrichtung gibt es mehrere praxisbezogene Methoden.

Durch das Aufeinanderkleben mehrerer Papierschichten abwechselnder Laufrichtung entsteht starres Papier (vergleichbar zum Sperrholz), wie bei den mindestens dreilagigen Bristolkarton.

Die Anforderungen bezüglich der Alterungsbeständigkeit von Büchern sind in den so genannten Frankfurter Forderungen der Deutschen Bibliothek und der Gesellschaft für das Buch, sowie in der US-Norm ANSI/NISO Z 39.48–1992 und ISO-Norm 9706, beschleunigte Alterung (Simulation: ISO 5630, DIN 6738) fixiert.
Heute soll ein alterungsbeständiges Papier folgende Kriterien erfüllen:


Als Orientierungshilfe für die Alterungsbeständigkeit von gestrichenen und ungestrichenen Papieren wurden Lebensdauerklassen augearbeitet.


Entgegen der Normung werden auch alterungsbeständige Recyclingpapiere angeboten, da durch Forschungsergebnisse nachgewiesen wurde, dass sich Holzschliff und Alterungsbeständigkeit nicht ausschließen. So sind zum Beispiel "Recycling-Kopierpapiere" auf dem Markt, die die Vorgaben nach der Lebensdauerklasse LDK 24 bis 85 erfüllen und auch über eine "Alkalireserve" in Form von Carbonat verfügen.

Papier wird vorrangig zum Beschreiben und Bedrucken sowie meist als Pappe oder Karton zum Verpacken verwendet. Der Anteil dieser beiden Papiergruppen an der Papierproduktion in Deutschland betrug im Jahr 2011 44 % und 43 %. Mit großem Abstand folgen "Hygienepapiere" wie Toilettenpapier oder "Haushaltstücher" und die "technischen Spezialpapiere" wie Filterpapiere, Dekorpapiere oder Tapeten mit einem Anteil von "sieben" und "sechs" Prozent. Papier wird weitestgehend aus Zellstoff oder aus Holzstoff/ Holzschliff hergestellt. Wiederzuverwertendes Papier in Form von Altpapier stellt mittlerweile die wichtigste Rohstoffquelle in Europa dar. Je nach flächenbezogener Masse (umgangssprachlich Flächengewicht) und Eigenschaften wird zwischen Papier, Karton oder Pappe unterschieden.

Wird der Rohstoff zu "massiven Objekten" statt zu flächigem Papier verarbeitet ist dies Pappmaché. Aufgrund der vielfältigen Kombinationsmöglichkeiten bei den Rohstoffen, der Fertigung, der Verarbeitung und der Verwendung existieren etwa 3000 Papiersorten.

Beim Beschriften wird ein Farbstoff (beispielsweise Tinte, Toner und Druckfarbe) mit einem Gerät auf Papier aufgetragen. Dies kann von Hand mit einer Schreibmaschine, einem Füllfederhalter, einem Bleistift, einem Buntstift, einem Filzstift oder einem Federkiel geschehen. Seit der Erfindung des Buchdrucks gibt es Maschinen, die einen Text seitenweise auf Papier übertragen können. Dies ist mit einer Druckmaschine millionenfach oder mit einem Laserdrucker für nur wenige Seiten möglich. Während anfänglich noch der zur Verfügung stehende Rohstoff die Eigenschaften des Papiers bestimmte, kann mittlerweile Papier weitestgehend den Anforderungen angepasst werden: Gestrichenes Bilderdruckpapier zum Kunstdruck, zum Zeitungsdruck ein billiges, reißfestes Papier und als Kopierpapier holzfreies, ungestrichenes Papier.

Pappmaché ist ein Gemisch aus Papier, Bindemittel und Kreide oder Ton, das im 18. Jahrhundert als Ersatz für Stuck in der Innenausstattung verwendet wurde. So gab es eine Manufaktur, in der aus alten Akten für das Schloss Ludwigslust Deckenverzierungen, Büsten und sogar Statuen, die wenige Monate im Freien aufgestellt werden konnten, hergestellt wurden. Papier findet sich im Modellbau, in der japanischen Papierfaltkunst Origami und bei Collagen und Assemblagen.

Als deutsche zeitgenössische Künstlerin findet Jutta Barth mit ihren Collagen und Assemblagen Anerkennung. Sie arbeitet objekthaft mit dem Werkstoff Papier und fertigt Zeichnungen auf "handgeschöpftem" Recyclingbütten.

Aquarellpapier für Aquarelle hat eine flächenbezogene Masse von bis zu 850 g/m. Fotopapier muss speziell beschichtet werden, damit es als Träger für die Fotoemulsion oder zum Einsatz für Tintenstrahldrucker geeignet ist.

Dies ist die Bezeichnung für "veredelte", "geschmückte" und "verzierte", oft aufwendig bearbeitete Papiererzeugnisse die von etwa 1820/1860 bis 1920/1930 hergestellt wurden, als es eine eigene Luxuspapierindustrie gab. Zur Veredlung wurde eine Reihe von Bearbeitungsverfahren eingesetzt, wie Kolorierung als "Hand-" und "Schablonenkolorierung", Farbendruck als "Chromolithografie", Gold- und "Silberdruck", Prägen (Gaufrieren) und Stanzen, das Aufbringen von Fremdmaterialien, wie Glimmer, Seide sowie das Anbringen von "Laschen", "Klappen" und "Mechanismen" bei Spielzeugen. Unter Luxuspapiere fallen Andachts- und Fleißbildchen, viele Ansichts- (Leporello), Gelegenheits- (Glückwunsch-, Weihnachts- und Neujahrskarten) und Bildpostkarten (Motivkarten), "verzierte" Briefbogen, Etiketten, allerlei Papierspielzeug (Papiertheater), Reklamemarken und Sammelbilder und vieles mehr. Solche Luxuspapiere sind Sammelobjekte.

In Japan und China wird Papier in der Inneneinrichtung in vielfältiger Weise verwendet, beispielsweise die japanischen Shōji, mit durchscheinendem Washi-Papier bespannte Raumteiler.

Pappe hat eine flächenbezogene Masse von mindestens 300 g/m und ca. 1,5 mm Dicke. Dünneres Material, ab 130 g/m, heißt Karton und wird vorwiegend als Kartonage verwendet. Mit einer Kunststoffbeschichtung und eventuell einer Aluminiumfolie als Zwischenlage kann sie als Getränkekarton sogar Flüssigkeiten verpacken. Die am meisten verbreitete Pappe ist die Wellpappe, die in den vielfältigsten Sorten vorkommt. Pappe und Kartons werden vorwiegend aus Recyclingpapier produziert, da es hierbei nicht so sehr auf die Farbe des Materials ankommt. Inzwischen kann jedoch Recyclingpapier mit einer sehr hohen Qualität produziert werden und unterscheidet sich im Weißegrad nur noch sehr schwach von Papier aus neuen Fasern. Das Papier mit der größten relativen Zugfestigkeit wird Kraftpapier genannt. Es besteht zu beinahe 100 % aus langfaserigen Zellstofffasern von Nadelhölzern. Es wird besonders für Papiersäcke verwendet. Es gibt Filterpapiere (Luftfilter für Fahrzeuge und Staubsauger), Kabelisolierpapiere, Medizinische Papiere, Klebezettel, Zigarettenpapier und Thermopapiere. Papiere finden sich ebenfalls in Metallpapierkondensatoren und Elektrolytkondensatoren, wo sie als "Isolator" oder "Träger" des flüssigen Elektrolyts dienen.

Es gibt Flugdrachen aus Papier in China, seitdem es dieses Material gibt. Die 1783 erbaute Montgolfière der Gebrüder Montgolfier war ein Heißluftballon aus Leinwand, der mit einer dünnen Papierschicht "luftdicht" verkleidet war. Im Zweiten Weltkrieg produzierte Japan ca. 10.000 Ballonbomben aus Papier, die mit Lack "gasdicht" gemacht wurden und Brand- und Sprengsätze (5 bis 15 Kilogramm) über den Pazifik nach Amerika transportierten.

Im Flugzeugmodellbau wird Papier als Bespannung (Spannpapier) von Tragflächen in Holm-Rippen-Bauweise und für Flugzeugrümpfe verwendet. Dazu wird es "aufgeklebt", mit "Spannlack getränkt" und "überlackiert", sobald durch Trocknen die nötige Oberflächenspannung erreicht ist.

Des Weiteren wird Papier zum Basteln von Papierfliegern benutzt. Dazu wird das Papier in eine einem Flugzeug ähnelnde Form gefaltet.

Papier kann zu Textilien verarbeitet werden, einerseits direkt aus Papier, andrerseits kann es in Streifen geschnitten, versponnen und zu Textilen verwebt werden. Bei dem in den 1970er Jahren auf den Markt gekommenen „Papierkleid“ handelte es sich allerdings um speziell gefertigte "Vliesstoffe", die billiger als Kleiderstoffe waren.

Bei der Papierproduktion ist vor allem der Verbrauch an Holz, Wasser und Energie Gegenstand der umweltpolitischen Diskussion. Rund 20 % des weltweit eingeschlagenen Holzes werden zu Papier verarbeitet. In Deutschland werden allerdings vornehmlich so genanntes Waldrestholz, Sturmholz oder Industrierestholz verwendet. Kontroverse Standpunkte finden sich vor allem beim Thema nachhaltige Forstwirtschaft, der Forstzertifizierung und der Nutzung von Urwald.

Der "hohe Wasserverbrauch" war bereits vor 1900 problematisch, was die mehrfache Nutzung des Wassers bedingte, um den Verbrauch zu reduzieren. In den ersten Papiermühlen wurden 1200 Liter pro Kilogramm Papier benötigt, um 1900 waren es 100 bis 800 Liter durch bessere Methoden sank dieser Wert auf "sieben" Liter. Ein völliges Schließen des Kreislaufs ist im Regelfall nicht möglich. Auf Grund der Wasserhärte kommt es zu "Verstopfungen" von Maschinenteilen, wenn sich "Calciumcarbonat" anreichert. Nur in Einzelfällen, falls nur Altpapier eingesetzt wird, ist der "komplette Abwasserrücklauf" derzeit möglich.

Der Wasserverbrauch bedingt das Abwasser. Mit der Industrialisierung Ende des 19. Jahrhunderts und der steigenden Nachfrage nach "Zellstoff" und "Chlorbleiche" stieg die Menge der Abwässer und damit der darin enthaltenen Schadstoffe. Die "Hemicellulosen" und das "Lignin" (über 50 % des Holzes) wurden anfangs fast unbehandelt in die Gewässer entsorgt. Aufgrund der Umwelt- und Gesundheitsschäden und des "Verlustes" der "Kochchemikalien" mit der Ablauge wurden zunächst die Laugen zur Energieerzeugung eingesetzt. Moderne Zellstofffabriken arbeiten energieautark, die "Kochchemikalien" werden aus dem Abgas und der Asche "zurückgewonnen".

Die deutschen Umweltvorschriften gehören zu den strengsten weltweit. Abwässer aus Zellstofffabriken dürfen eine Belastung bis 25 Kilogramm CSB (chemischer Sauerstoffbedarf) pro Tonne Zellstoff aufweisen, bei der Papierherstellung sind zwei bis fünf Kilogramm "CSB" pro Tonne Papier zulässig.

Die meisten Papierfabriken betreiben eigene Kläranlagen, die das eingesetzte Wasser in bis zu drei Stufen (mechanisch, biologisch, chemisch) reinigen und erst anschließend in den Vorfluter einleiten. Andere Papierfabriken sind "Indirekteinleiter", die ihr Abwasser gemeinsam mit "kommunalem" Abwasser reinigen lassen.

Der Energieverbrauch zur Herstellung liegt bei etwa 2,7 kWh pro Kilogramm Papier.






Papiergeschichte

Industrieverbände


</doc>
<doc id="12699" url="https://de.wikipedia.org/wiki?curid=12699" title="Fribourg">
Fribourg

Fribourg steht für


Fribourg ist der offizielle französische Name für

Siehe auch:



</doc>
<doc id="12700" url="https://de.wikipedia.org/wiki?curid=12700" title="Homöopathie">
Homöopathie

Die Homöopathie [] (von altgriechisch "hómoios" ‚gleich, gleichartig, ähnlich‘ sowie "páthos" ‚Leid, Schmerz, Affekt, Gefühl‘; wörtlich also „ähnliches Leiden“) ist eine alternativmedizinische Behandlungsmethode, die auf den ab 1796 veröffentlichten Vorstellungen des deutschen Arztes Samuel Hahnemann beruht. Nach Hahnemann ist Krankheit „eine besondre Stimmung“ des Organismus, die der Heilkünstler anhand der Symptome zu erkennen und zu beseitigen habe.

Ihre namensgebende und wichtigste Grundannahme ist das von Hahnemann formulierte Ähnlichkeitsprinzip: „Ähnliches möge durch Ähnliches geheilt werden“ ("similia similibus curentur"). Danach solle ein homöopathisches Arzneimittel so ausgewählt werden, dass die Inhaltsstoffe der Grundsubstanz unverdünnt an Gesunden ähnliche Krankheitserscheinungen ("Symptome") hervorrufen könnten wie die, an denen der Kranke leidet, wobei auch der „gemüthliche und geistige Charakter“ des Patienten berücksichtigt werden solle. Hierzu wurden von Hahnemann und seinen Nachfolgern ausgedehnte Tabellen "(Repertorien)" erstellt, mit deren Hilfe der Homöopath den Patienten und seine Krankheitserscheinungen einem "Arzneimittelbild" zuordnen soll.

Zur Herstellung der homöopathischen Arzneimittel werden die Grundsubstanzen einer sogenannten Potenzierung (Verdünnung) unterzogen, das heißt, sie werden wiederholt (meist im Verhältnis 1:10 oder 1:100) mit Wasser oder Ethanol verschüttelt oder mit Milchzucker verrieben. Die Verdünnung wurde zunächst wegen der Giftigkeit vieler der verwendeten Stoffe durchgeführt. Erst in einer späteren Phase verordnete Hahnemann „Hochpotenzen“. Hahnemann nahm an, dass durch das besondere Verfahren der Potenzierung oder eine wirksam werde. Zur Begründung der Hochpotenzen ging er davon aus, dass sich hier .

Diese behauptete selektive Steigerung erwünschter Wirkungen durch die Prozeduren des Verdünnungsverfahrens, die von einigen Autoren auch als „rituell“ bezeichnet werden, widerspricht naturwissenschaftlichen Erkenntnissen. Die Lehre der Homöopathie wird daher zu den Pseudowissenschaften gezählt. Auch das hahnemannsche Ähnlichkeitsprinzip ist wissenschaftlich nicht haltbar. Klinische Studien nach wissenschaftlichen Standards konnten keine über den Placebo-Effekt hinausgehende Wirksamkeit homöopathischer Arzneimittel nachweisen.
Wahrgenommene Erfolge einer Behandlung werden dem Behandlungsumfeld, nicht dem Mittel selbst zugeschrieben, etwa dem Glauben des Patienten an die Wirksamkeit der Behandlung (Autosuggestion) oder der Qualität der Beziehung zwischen Therapeuten und Patienten. Der Fachbereich Humanmedizin der Philipps-Universität Marburg verwarf die Homöopathie 1992 im Rahmen der „Marburger Erklärung zur Homöopathie“ als „Irrlehre“.

Bei einer 2009 in Deutschland durch das Allensbach-Institut durchgeführten Erhebung brachten 17 % der Befragten homöopathische Arzneimittel mit dem Verdünnungs- oder Ähnlichkeitsprinzip in Verbindung. Im Jahr 2015 wurden in Deutschland Homöopathika für 595 Mio. Euro umgesetzt, das war ein Wachstum von 12,8 Prozent gegenüber dem Vorjahr. 2016 stieg der Umsatz um 4,5 Prozent, die Zahl der abgegebenen Packungen stieg hingegen nur um 0,3 Prozent. Im ersten Halbjahr 2017 war der Absatz geringer als im ersten Halbjahr 2016.

Die Homöopathie ist eine weit verzweigte Praxis mit vielen Varianten. Alle homöopathischen Lehren berufen sich auf Hahnemann und das Ähnlichkeitsprinzip, weichen aber in anderen Punkten teilweise erheblich voneinander ab. Die meisten Homöopathen sehen das Ähnlichkeitsprinzip, die „Arzneimittelprüfung am Gesunden“, die Erhebung des individuellen Krankheitsbildes durch eine ausführliche Anamnese und die „Potenzierung“ bei der Herstellung der homöopathischen Arzneimittel als Grundsätze der Homöopathie an.

Nach dem Ähnlichkeits- oder Simileprinzip – „similia similibus curentur“ („Ähnliches möge durch Ähnliches geheilt werden“) – sollen Krankheiten durch Mittel geheilt werden, die bei einem Gesunden ähnliche Symptome hervorrufen, wie sie bei dem Kranken beobachtet werden:
Die Idee eines Simile-Prinzips lässt sich nicht allein auf Hahnemann zurückführen. Ansatzweise findet sie sich bereits im Corpus Hippocraticum und den Schriften des Theophrast von Hohenheim "(Paracelsus)":

Die Entwicklung zum zentralen Prinzip der Homöopathie geht unter anderem auf einen Selbstversuch Hahnemanns zurück, mit dem er herausfinden wollte, wie die damals schon als Mittel gegen Malaria bekannte Chinarinde wirkt („Chinarindenversuch“). Nach sechs Jahren weiterer Experimente an sich und seinen Familienmitgliedern mit anderen Substanzen formulierte Hahnemann 1796 das Ähnlichkeitsprinzip der Homöopathie in Form eines Postulats, veröffentlicht in Christoph Wilhelm Hufelands "Journal der praktischen Arzneikunde". Hahnemann schrieb hierzu in seinem Grundlagenwerk der Homöopathie, dem Organon der Heilkunst:
Voraussetzungen für die Anwendung des Ähnlichkeitsprinzips in der Homöopathie sind zum einen die Kenntnis der Wirkung der homöopathischen Mittel (siehe „Homöopathische Arzneimittelprüfung“) und zum anderen die exakte Erfassung des Symptombildes des Patienten in der homöopathischen Anamnese (siehe „Wahl des Mittels“).

Schon Zeitgenossen Hahnemanns haben die Beliebigkeit des Ähnlichkeitsprinzips kritisiert, welches aus wissenschaftlicher Sicht nicht nachvollziehbar ist. So bezeichnete Ludwig Griesselich das Simile-Prinzip als Sack, in den „man Alles hineinstecken kann“. Bis heute wurde noch kein Medikament nach diesem Prinzip entdeckt, das Eingang in die evidenzbasierte Medizin gefunden hat. Hahnemanns Chinarindenversuch konnte nie reproduziert werden. Es wird vermutet, dass Hahnemann allergisch auf Chinarinde reagiert hatte.

Bis heute werden in der Homöopathie Arzneimittelprüfungen durchgeführt, die jedoch keineswegs arzneimittelrechtlichen Medikamentenstudien gemäß dem Arzneimittelgesetz (siehe unten) entsprechen, sondern nach Hahnemanns Vorgaben durchgeführt werden: Homöopathische Prüfer, die gesund sein müssen, nehmen ein Mittel ein und notieren anschließend alle Veränderungen und Reaktionen, die sie an sich feststellen.

Die aufgezeichneten Symptome mehrerer solcher Prüfungen werden zu einem homöopathischen "Arzneimittelbild" zusammengefasst und in Verzeichnissen geordnet. Je nach Ordnungskriterium handelt es sich dabei entweder um "Arzneimittellehren" (nach Mitteln geordnet) oder um Repertorien, die nach Symptomen zusammengestellt werden.

Hahnemann selbst hatte schon im Jahre 1805 angebliche Wirkungen von 27 verschiedenen Mitteln an gesunden Menschen beschrieben. Er betonte, dass sämtliche Befindlichkeiten der Probanden während der Wirkungszeit des Mittels als durch dieses ursächlich hervorgerufen gelten müssten, selbst wenn der Prüfer Ähnliches in anderen Zusammenhängen beobachtet habe.

Diese Prüfungen wurden bereits im 19. Jahrhundert als „höchst unwissenschaftlich“ kritisiert. Ihre „Kunst“ bestehe darin, . Schon 1927 war in den Originalschriften von Samuel Hahnemann zu finden, dass Berichte von nicht weniger als 716 Symptomen vorlägen, die Hahnemann durch Reiben mit Magneten an Personen erhalten habe. Eine von einigen Homöopathen durchgeführte Untersuchung von Arzneimittelprüfungen der Jahre 1945 bis 1995 ergab, dass diese Prüfungen sehr unterschiedlich gehandhabt wurden und meist von sehr niedriger Qualität waren.

Es gibt keine einheitlichen Vorschriften für Arzneimittelprüfungen, lediglich Empfehlungen.

Bei der homöopathischen Anamnese eines Patienten, d. h. einer Beobachtung und Befragung, wird versucht, das gesamte Symptombild und die Art der „Verstimmung der Lebenskraft“ zu erfassen. Im Unterschied zur Anamnese in der naturwissenschaftlichen Medizin wird in der homöopathischen Anamnese der Patient auch über eine Vielzahl von Sachverhalten befragt, die aus naturwissenschaftlicher Sicht unerheblich sind. Ziel ist es, dasjenige Mittel herauszufinden, bei welchem die beim gesunden Menschen beobachteten Symptome möglichst mit denen übereinstimmen, die bei der Anamnese des Kranken erfasst wurden (Repertorisierung, Repertorisation).

Ein weiterer Grundsatz der Homöopathie ist die Verwendung „potenzierter“ Mittel. Unter Potenzierung ist die starke Verdünnung bei gleichzeitiger „Dynamisierung“ (Verschüttelung oder Verreibung; siehe unten) zu verstehen. Die Mittel werden durch stufenweise durchgeführtes Potenzieren aus „Urtinkturen“ (pflanzlichen und tierischen Ursprungs: Symbol: Ø oder mineralischen und chemischen Ursprungs: Symbol O) und aus Verdünnungsmitteln wie Ethanol, destilliertem Wasser, Glycerin und Milchzucker hergestellt. Homöopathische Mittel werden flüssig (Dilution) oder als Globuli, in tiefen Potenzen auch in Form von Tabletten angewendet.

Hahnemann führte die Potenzierung um 1798 ein. Nach seinen Anweisungen wurden Homöopathika in Hunderterschritten potenziert („C-Potenzen“). Das heute gebräuchlichere Dezimalsystem zur Potenzierung und Bezeichnung der Potenzen, z. B. D10, wurde vom Heilpraktiker und späteren Arzt Arthur Lutze entwickelt und von Constantin Hering in die Behandlung eingeführt. Aus der Sicht der Homöopathen ist die Wirkung einer bloßen Verdünnung nicht mit der eines potenzierten, also verschüttelten oder verriebenen Mittels vergleichbar. Von Hahnemann wurde schon im "Organon der Heilkunst" (Anmerkung zu § 11) die Wirkung eines potenzierten Mittels nicht der körperlichen Substanz oder physischen Wirkung eines Arzneistoffes, sondern einer diesem innewohnenden, immateriellen, durch das Dynamisieren daraus freigewordenen „spezifischen Arzneikraft“ zugeschrieben. Heutige Homöopathen, die Hochpotenzen anwenden, nehmen an, bei der Potenzierung, auch Dynamisierung genannt, werde durch die Energiezufuhr beim Verschütteln oder Verreiben eine Information an das Lösungsmittel abgegeben und bei jedem Potenzierungsschritt verstärkt, auch wenn keine Moleküle des Arzneimittels mehr in der Lösung vorhanden sind. Es existieren keine Belege für die Existenz der postulierten immateriellen Energiezufuhr durch die rituellen mechanischen Prozeduren. Die mit dieser Methode hergestellten Lösungen unterscheiden sich nicht von einfach nur verdünnten Lösungen.

Die Verdünnung unter die chemische Auflösungsgrenze (ab D23 – siehe auch Avogadro-Konstante) ist kein "zwingendes" Element der Homöopathie. Viele Heilpraktiker und einige Ärzte arbeiten in Deutschland auch mit den Verdünnungen 1:10.000 und 1:1.000.000 (D4 und D6), in denen die Stoffe noch in nennenswerter Konzentration vorliegen. Bei diesen nur schwach verdünnten Mitteln sind die regulären Dosis-Wirkungs-Beziehungen des verwendeten Stoffes zu beachten und diverse Wirkungen möglich. Neben der bekanntesten D-Potenzierungsreihe (1:10) gibt es noch die C-Reihe (1:100), die M- (1:1000) und die LM- oder Q-Reihe (1:50.000).

Die "Fluxionspotenzierung" nach Dellmour ist eine Sonderform der flüssigen Potenzierung, die ohne Verschütteln erfolgt. Durch Turbulenzen der zuzufügenden Flüssigkeit bei der Zugabe ins Potenzierungsgefäß soll das Verschütteln unnötig sein. Jedoch sei der Dynamisierungseffekt geringer, so dass sehr hohe Potenzgrade hergestellt werden müssten.

Bei der "K-Potenzierung" nach Semjon Nikolajewitsch Korsakow, einem russischen Homöopathen, erfolgt die Potenzierung vereinfacht in nur einem Glas. Die Methode wurde zwar nicht in das Homöopathische Arzneimittelbuch (HAB) aufgenommen, dennoch stellen einige Firmen aus Zeit- und Kostengründen Hochpotenzen nach dieser Methode her.

Aus Sicht einiger Homöopathen müsste eine "„Schwache Quantentheorie“" zur Erklärung der Homöopathie heranzuziehen sein. Dabei wird das Phänomen der Verschränkung jedoch ohne klare mathematische Definition auf grundlegend verschiedene Systeme wie ein homöopathisches Mittel und Krankheitssymptome angewandt; die extrem leichte Zerstörbarkeit verschränkter Zustände wird nicht berücksichtigt.

Als Versuch der Erklärung eines „Gedächtniseffekts von Wasser“ werden von einigen Homöopathen strukturelle Veränderungen am Wasser als Lösungsmittel angeführt. Diese Gedächtnisfunktion ist jedoch, selbst nach Ansicht von anderen Vertretern der Homöopathie, nicht mit den Kenntnissen über Wasser vereinbar.

Samuel Hahnemann begründete die Lehre der Homöopathie und spielt damit bis heute eine große Rolle für Anhänger der Homöopathie. Er galt als scharfzüngiger Kritiker der zeitgenössischen Medizin des 18. Jahrhunderts, die Methoden wie Aderlass, Brechkuren und Drastika nutzte.

Gegen die aus dem Mittelalter stammende Medizin, die bis ins 18. Jahrhundert wirkte, setzte Hahnemann die Idee und Praxis der Homöopathie mit dem Ziel, zu einer individualisierten und milden Behandlung zu gelangen. Er wollte damit zeigen, dass Krankheiten auch ohne Blutverlust und Abführmittel in Heilung enden können. Seine Anregung zu experimentellen Untersuchungen über die Arzneiwirkung im gesunden Menschen war neu. Die Ideen und Experimente der Homöopathie blieben, weil ihre Beweisverfahren zu weit von gewissenhafter Kritik, von naturwissenschaftlicher Logik und gründlicher Forschung entfernt waren, für die sich entwickelnde wissenschaftliche Medizin des 19. Jahrhunderts von geringer Bedeutung. Trotz zahlreicher Anläufe wurde die Homöopathie im 19. und 20. Jahrhundert an keiner deutschsprachigen Universität dauerhaft institutionalisiert. Die Einrichtung von Lehrstühlen scheiterte am Widerstand der medizinischen Fakultäten.

Erste Hinweise auf die Arzneimittelprüfung am Gesunden finden sich 1790 in der als Fußnote eingeschobenen Schilderung des „Chinarindenversuchs“ in Hahnemanns Übersetzung von William Cullens Arzneimittellehre.

1805 formulierte Hahnemann seine Sichtweise in der Arbeit "Heilkunde der Erfahrung". Er trat in dieser Schrift als eine Art Reformator auf, der die seit der Antike gewonnenen medizinischen Kenntnisse verwarf: 

Gegen eine Ergründung der Ursachen von Krankheiten setzte er seine Erfahrungsheilkunde: „Wenn wir aber auch die den Krankheiten zum Grunde liegenden, innern Körperveränderungen nie einsehen können, so hat doch die Uebersicht ihrer äussern Veranlassungen einigen Nutzen. Keine Veränderung entsteht ohne Ursache. Die Krankheiten werden ihre Entstehungsursachen haben, so verborgen sie uns auch in den meisten Fällen bleiben.“

Der Arzt habe die Rolle, eine Krankheit über ihre Symptome zu beschreiben: „Mit diesem sorgfältigen Eifer wird der Arzt das reine Bild der Krankheit aufgezeichnet, er wird die Krankheit selbst vor sich haben in Zeichen, ohne welche sich keine verborgene Eigenschaft der Dinge, und eben, so wenig eine Krankheit dem blos nach Wahrnehmungen seiner Sinne erkennenden, irdischen Menschen ausspricht.“ Die nachfolgende Aufgabe sei: „Ist die Krankheit gefunden, so müssen wir das Heilmittel suchen.“

1805 erklärte Hahnemann: 

Gegen diese Reize verwendete er Arzneimittel: 

Heutige Homöopathen sehen den Selbstversuch Hahnemanns mit Chinarinde als Geburtsstunde der Homöopathie an, da er das Ähnlichkeitsprinzip (Simile-Prinzip) belege. 
Hahnemann selbst erwähnte den Versuch außerhalb dieser Beschreibung nur weitere drei Mal, in einem kürzeren Fachartikel, der Beantwortung eines Briefes und seiner "Reinen Arzneimittellehre." (Band 3. 1817/1830).Historisch gesehen spielte der Chinarindenversuch Hahnemanns innerhalb der Homöopathie auch in Bezug auf seine Gültigkeit und Nachvollziehbarkeit nur eine untergeordnete Rolle. Fundstellen in einschlägigen Zeitschriften und Lexika sind dazu selten. Teile der Versuchsbeschreibung Hahnemanns finden sich jedoch auf zahllosen entsprechenden Webseiten.

Hahnemanns Versuch konnte bis heute weder von Homöopathen noch von wissenschaftlichen Medizinern erfolgreich reproduziert werden. Zwei Beispiele: Der Arzt Johann Christian Jörg ließ 1821 vier Medizinstudenten eine nach der Beschreibung Hahnemanns gewonnene Tinktur aus Chinarinde trinken. Bei keinem der Probanden traten die von Hahnemann beobachteten Symptome auf. Die beiden Ärzte Hans-Joachim Krämer und Ernst Habermann führten 1997 einen Selbstversuch nach Hahnemanns Angaben durch, der ebenso keine der Angaben Hahnemanns bestätigte, aber zu Verdauungsproblemen führte.

Der medizingeschichtlich arbeitende Homöopath Georg Bayr wertet den Versuch aus heutiger Sicht: „Der Chinarindenversuch basiert auf Intuition. Er war zufällig. Es war ein zeitbedingter Irrtum. Der Irrtum war fruchtbar, da die Homöopathie daraus entstand.“

Der Wirkmechanismus der Chinarinde bzw. des darin enthaltenen Chinins, das 1820 durch Extraktion mit Alkohol aus der Chinarinde isoliert wurde, auf die Krankheit Malaria ist heute bekannt. Chinin wirkt bei ungeschlechtlichen Formen des Malariaerregers als Hemmer der Nucleinsäuresynthese und hindert so den Erreger an seiner Vermehrung. Chinin wirkt außerdem schmerzstillend, fiebersenkend und örtlich betäubend. Möglicherweise handelte es sich bei Hahnemanns Selbstbeobachtungen um eine allergische Reaktion aufgrund einer Sensibilisierung für Chinin, da er das Mittel bereits früher eingenommen hatte. Andererseits ist Chinin als Diastereomer des Chinidin bekannt, das die Herzfrequenz erhöht. Dieser Effekt der Herzfrequenzerhöhung wurde zu dieser Zeit als Fieber gewertet, da Thermometer für die Fiebermessung nicht üblich waren. Dies könnte ebenfalls Hahnemanns Beobachtung erklären.

Hahnemann verzichtete zunächst auf Versuche einer theoretischen Begründung. In seinem Spätwerk bezog er sich – offensichtlich bemüht um eine nach damaligen Maßstäben wissenschaftliche Begründung – auf vitalistische Vorstellungen („Umstimmung der Lebenskraft“).

Nach jahrelangen praktischen Erfahrungen mit der Homöopathie stellte Hahnemann fest, dass bestimmte chronische Krankheitsverläufe homöopathisch nicht zu heilen waren. Ab 1816 entwickelte er deshalb eine Methode zur Behandlung chronischer Krankheiten. 1828 veröffentlichte er die Ergebnisse seiner Forschung in einem fünfbändigen Werk mit dem Titel "Die chronischen Krankheiten". Nach seiner Theorie liege den chronischen Krankheiten ein Miasma, eine Art tief liegendes „Ur-Übel“, zugrunde. Hahnemann unterteilte die Miasmen in Psora (als Folge der Krätze), Sykosis (Feigwarzenkrankheit als Folge der Gonorrhoe) und Syphilis. Hahnemanns Arbeit nach der Erkenntnis der Miasmen war der Versuch, die Psora auszumerzen, wie er schrieb. Von Gegnern wurde kritisiert, dass er zwölf Jahre weiter die homöopathische Behandlung propagierte, obwohl „die homöopathische Behandlung von sieben Achteln der chronischen Krankheiten eine ganz nutzlose gewesen sei.“

Sein Verständnis der chronischen Krankheiten bewegte sich zwar im Rahmen der damaligen medizinischen Erkenntnisse. Die Miasma-Lehre gilt heute jedoch als überholt. Mit der Entdeckung des Cholera-Erregers 1884 durch Robert Koch wurde das Ende der Miasma-Lehre in der Hochschulmedizin eingeläutet. Syphilis und Cholera werden erfolgreich mit Antibiotika behandelt. Feigwarzen sind Folge einer Infektion mit Humanen Papillomviren.

In der klassischen Homöopathie jedoch werden die praktischen Konsequenzen der Miasma-Lehre bis heute berücksichtigt.

Schon zu Hahnemanns Lebzeiten und später verbreitete sich die Homöopathie international.

Die Ausbreitung in Frankreich wurde durch Hahnemanns Wirken dort befördert. Mélanie d’Hervilly heiratete ihn 1835 und zog mit ihm nach Paris, wo er die dank der Beziehungen seiner Frau florierende Praxis bis zu seinem Tod 1843 betrieb. Seine Frau betrieb die Praxis anschließend ohne Lizenz weiter, bis sie gegen 1870 das Haus verkaufen musste. Gegen Ende ihres Lebens erhielt sie schließlich die ärztliche Zulassung und war damit die erste homöopathische Ärztin.

In Großbritannien praktizierten homöopathische Ärzte schon seit den 1830er Jahren. Die englische Königsfamilie ließ sich seit dem 19. Jahrhundert homöopathisch behandeln und trat öffentlich für diese Therapieform ein.
Während sich die Homöopathie auch in weiteren europäischen Ländern, wie Belgien und den Niederlanden, Österreich und der Schweiz, Spanien, Italien und Griechenland verbreitete, ist sie in Skandinavien vergleichsweise schwach vertreten.

In den 1820er und 1830er Jahren wurden homöopathische Praktiken sowohl innerhalb als auch außerhalb der akademischen Medizin in den Vereinigten Staaten eingeführt. Auch in Ländern Südamerikas, wie Brasilien, Kolumbien, Chile und Argentinien, etablierte sich die Homöopathie. Der Argentinier Tomás Pablo Paschero bezog dabei Methoden der Tiefenpsychologie in die Behandlung ein.

Um 1830 wurde die Homöopathie unter anderem durch den siebenbürgischen Arzt Johann Martin Honigberger nach Indien gebracht, wo sie breit akzeptiert wurde. Allerdings haben auch politische Gründe eine Rolle dabei gespielt: die Homöopathie kam aus dem Deutschen Reich, das politisch der Gegner der britischen Kolonialherren war. Heute ist die Homöopathie im öffentlichen Gesundheitswesen Indiens fest verankert und anerkannt. In den 1980er Jahren waren 17,6 % des lizenzierten medizinischen Personals Homöopathen. 7 % aller Ambulanzen wurden von Homöopathen geleitet. Es existieren 200 Colleges und ein eigener Forschungsrat.

Viele Methoden experimenteller Medizin wurden zuerst in der Auseinandersetzung mit alternativer Medizin entwickelt: So publizierte bereits 1835 der Theologe und Redakteur George Löhner einen unter Beteiligung von Ärzten, Apothekern und anderen Honoratioren in Nürnberg durchgeführten Test der Wirkung einer homöopathischen Kochsalzlösung an einer Gruppe von 55 freiwilligen, gesunden Probanden. 42 Personen hatten „gar nichts Ungewöhnliches“ bemerkt (19 Kochsalz-Potenz, 23 Wasser), 9 Personen hatten „etwas Ungewöhnliches“ bemerkt (6 Kochsalzpotenz, darunter aber einer, der wusste, dass er die Potenz eingenommen hatte, 3 Wasser). Während die Kommission folgerte, dass die Potenzierung keine Wirkung habe, reagierten homöopathische Zeitschriften mit heftigen Polemiken.

Der Medizinhistoriker Michael Stolberg kritisiert 2006, dass eine Verzerrung durch die persönliche Haltung der Versuchsteilnehmer zur Homöopathie nicht ausgeschlossen wurde; durch eine Mitteilung, nichts Ungewöhnliches bemerkt zu haben, konnten diese das Gesamtergebnis negativ beeinflussen. Er würdigt jedoch die Anwendung moderner Elemente des Studiendesigns: Der Versuch sei 

Die medizinische Versorgung erfolgte im 18. Jahrhundert durch Ärzte, überwiegend aber durch Bader und Wundärzte. Zusätzlich trugen viele Laien mit Kenntnissen über die Heilkraft von Pflanzen, Mineralien und anderen Wirkstoffen zu Heilbehandlungen bei. Die Ausbreitung der Homöopathie förderten nicht nur Ärzte, sondern auch Patienten und Laienbehandler. Im 19. Jahrhundert gewann die Homöopathie besonders in Kreisen des Adels und bei gebildeten Bürgern Anhänger und Multiplikatoren. Die Homöopathie stand von Anfang an der Religion nahe. Viele der ersten Homöopathen waren Pfarrerssöhne oder Theologiestudenten. In Frankreich trat der Klerus offen für Hahnemanns Lehre ein. Viele auf dem Land lebende Pfarrer praktizierten Homöopathie, besonders in Österreich. Aber auch Gutsbesitzer, Kaufleute und andere waren an der Verbreitung der Homöopathie beteiligt. Gefördert wurde diese Entwicklung durch die homöopathische Hausarztliteratur, wie Carl Gottlob Casparis "Homöopathischer Haus- und Reisearzt" und Constantin Herings "Homöopathischer Hausarzt", die seit Ende der 1820er Jahre erschien. In ihr wurde die Behandlung häufiger Krankheiten mit einfachen Mitteln geschildert. In diese Zeit fallen die ersten homöopathischen Vereinsgründungen. Ab 1830 gab es auch Zeitschriften, die sich vor allem an Laien richteten. So gab beispielsweise der Paderborner Arzt Peter Meinolf Bolle zwischen 1855 und 1871 die "Populäre Homöopathische Zeitung" heraus. Die bedeutendste homöopathische Laienzeitschrift war die "Leipziger populäre Zeitschrift für Homöopathie", welche ab 1870 erschien.

Laienvereine
Die deutschen homöopathischen Laienvereine sind ein weltweit einmaliges Phänomen. Zwischen 1870 und 1933 wurden 444 solcher Vereine gegründet, vor allem in Württemberg, Sachsen, Preußen und Baden. 1914 waren zwei Prozent der württembergischen Bevölkerung Mitglied in einem homöopathischen Verein. Die Vereine boten neben Geselligkeit und Freizeitgestaltung vor allem Zugang zu homöopathischem Wissen und Behandlung in Form von Selbsthilfe. Sie schafften homöopathische Hausarztliteratur an und machten diese ihren Mitgliedern zugänglich. Herzstücke der Vereine waren die homöopathischen Vereinsapotheken mit teilweise großen Vorräten homöopathischer Arzneien, fast immer in tiefen D-Potenzen. Vereinsmitglieder durften sich kostenlos, abgesehen vom Mitgliedsbeitrag, die gewünschten Mittel herausgeben lassen. Diese Praxis war jedoch von Beginn an juristisch umstritten und wurde schließlich untersagt. Da die Vereine zu den wichtigsten Abnehmern ihrer Produkte gehörten, unterstützten die Arzneimittelhersteller deren Bildungsarbeit. Die Vereine setzten sich zudem für die Einrichtung homöopathischer Lehrstühle an den Universitäten und die Gründung homöopathischer Krankenhäuser ein.
In der „Krise der Medizin“ in den 1920er Jahren fanden Naturheilkunde, Lebensreformbewegung und alternative Heilverfahren verstärkt Zulauf. Die naturheilkundlichen und homöopathischen Laienverbände gewannen viele Anhänger unter Arbeitern und Kleinbürgern. Der Dachverband "Reichsbund für Homöopathie und Gesundheitspflege" umfasste im Jahr 1930 348 Vereine mit 38.200 Mitgliedern. Der Nationalsozialismus griff mit der „Neuen Deutschen Heilkunde“ diese sich zu einer Massenbewegung entwickelnde Tendenz auf und vereinnahmte sie für seine Ziele. Die homöopathischen Laienvereine wurden davon zunächst mit erfasst. Im Laufe der Zeit nahm ihre Aktivität aber deutlich ab. Als die Zeit des Nationalsozialismus endete, war das homöopathische Laienwesen weitgehend zerstört. Die ersten Neu- und Wiedergründungen erfolgten in den 1950er Jahren. Die bereits am 24. Februar 1868 in Stuttgart gegründete "Hahnemannia" ist heute der Dachverband der homöopathischen Laienvereine.

In der Neuen Deutschen Heilkunde sollten die seit Mitte des 19. Jahrhunderts zunehmend naturwissenschaftlich fundierte „Schulmedizin“ und die „biologischen Heilverfahren“ zusammengefasst werden. Die homöopathischen Laienvereine bekannten sich häufig begeistert zur nationalsozialistischen Bewegung. In der Laienzeitschrift „Homöopathische Monatsblätter“ erschienen Aufsätze zur „Rassenhygiene“ und zu Nationalistisch-Völkischem, sogar zum Wert der Homöopathie für die Behandlung von Erbkrankheiten. Der Deutsche Zentralverein homöopathischer Ärzte vollzog 1933 die Gleichschaltung und wurde 1935 Mitglied der „Reichsarbeitsgemeinschaft für eine Neue Deutsche Heilkunde“.

Erstmals in ihrer Geschichte genoss die Homöopathie staatliche Unterstützung. Bei allen vordergründigen Erfolgen und aller Hoffnung von Homöopathen auf Anerkennung gab es jedoch frühzeitig kritische Stimmen, die vor einer Vereinnahmung durch den Nationalsozialismus warnten. Es wurde durch die Zusammenschließung mit anderen Methoden eine Verwässerung der Lehre und ein Verlust der Eigenständigkeit befürchtet. Auf staatlicher Seite erlahmte andererseits das Interesse an der Homöopathie aus unterschiedlichen Gründen. Der wichtigste dürfte eine Untersuchung der Homöopathie im Auftrag des Reichsgesundheitsamts zwischen 1936 und 1939 gewesen sein. Es wurden klinische Versuche, Arzneimittelprüfungen und Quellenstudien zu einzelnen homöopathischen Arzneien durchgeführt. Die klinischen Versuche hatten keinerlei Erfolg gezeigt. Die Nachprüfungen homöopathischer Mittel konnten die Ergebnisse vorheriger Prüfungen nicht reproduzieren.

Über das Schicksal jüdischer Homöopathen ist bisher nur wenig bekannt. In der homöopathischen Presse wurden teilweise eindeutig antisemitische Äußerungen verbreitet. Die 1933 beginnende „Ausschaltung“ jüdischer, sozialdemokratischer und marxistischer Ärzte vollzog sich auch in der Homöopathie. Prominentestes Opfer der Ausschaltung innerhalb der Homöopathie war der jüdische Arzt Otto Leeser (1888–1964). Er galt als Vertreter der naturwissenschaftlich-kritischen Richtung der Homöopathie in Deutschland.

Nach dem Zweiten Weltkrieg wurden einige Laienvereine wiedergegründet, erreichten aber nicht annähernd die frühere Bedeutung. Die Teilung Deutschlands brachte eine unterschiedliche Entwicklung der Homöopathie mit sich.

In der Bundesrepublik erlebte die Homöopathie seit Mitte der 1970er Jahre mit der Zunahme der Beliebtheit alternativer Heilmethoden bei Laien wieder einen Aufschwung. Zu dieser Zeit waren etwa 200 Homöopathen in der Bundesrepublik tätig. Bis 1993 stieg die Zahl auf 2.212 homöopathisch behandelnde Ärzte. 1978 erkannte der deutsche Gesetzgeber im Arzneimittelgesetz die Homöopathie, neben der "Anthroposophisch erweiterten Medizin" und der "Phytotherapie", als „Besondere Therapierichtung“ an. Die Mittel der besonderen Therapierichtungen können zugelassen und dürfen verordnet werden, auch ohne dass für sie ein Wirksamkeitsnachweis erbracht wurde. Die Homöopathische Centralofficin Dr. Willmar Schwabe, einer der bedeutendsten Hersteller homöopathischer Arzneimittel, verlegte 1946 ihren Firmensitz von Leipzig nach Karlsruhe. Der Betrieb in Leipzig wurde in der DDR jedoch weitergeführt. In Karlsruhe wurde 1961 die Homöopathika-Produktion abgespalten und fortan als Deutsche Homöopathie-Union weitergeführt. Die Firma Biologische Heilmittel Heel, 1936 in Berlin gegründet, baute nach dem Krieg ihr Werk in Baden-Baden wieder auf und ist heute international an zehn Standorten tätig.

In der DDR war die Homöopathie in den Anfangsjahren verbreitet, wurde aber zunehmend verdrängt. Unterstützt wurde dies dadurch, dass ab 1949 keine neuen Heilpraktiker zugelassen wurden. 1959 wurde eine Kampagne gegen die Homöopathie geführt. Es wurde eine Wanderausstellung mit dem Titel „Aberglauben und Gesundheit – Ausstellung gegen Aberglauben und Kurpfuscherei“ gestartet. 1961 folgte eine öffentliche Ächtung der Homöopathie. Es wurde zwar kein Behandlungsverbot erlassen, es gab jedoch ein Fortbildungsverbot. Von Laien und den wenigen Heilpraktikern sowie wenigen Ärzten wurde Homöopathie jedoch weiter eingesetzt. Homöopathische Arzneimittel wurden ab 1952 in Leipzig vom VEB Homöopharm Dr. Willmar Schwabe und dessen Nachfolgeunternehmen sowie in den "Bombastus-Werken" in Freital hergestellt.

Nach der deutschen Wiedervereinigung wurde mit Unterstützung der Karl und Veronica Carstens-Stiftung bereits am 28. April 1990 eine erste Weiterbildungsveranstaltung zur Homöopathie für Ärzte in Wittenberg durchgeführt.
In Zusammenarbeit mit der Fakultät für Geistes-, Sozial- und Erziehungswissenschaften der Otto-von-Guericke-Universität Magdeburg hat der "Deutsche Zentralverein homöopathischer Ärzte" ein Konzept für einen berufsbegleitenden zweijährigen Masterstudiengang "„Wissensentwicklung und Qualitätsförderung in der homöopathischen Medizin – Integrated Practice in Homoeopathy“" für Ärztinnen, Ärzte und andere approbierte Heilberufe erarbeitet, der mit dem Erwerb eines "Master of Arts" abgeschlossen werden soll. Es fand sich jedoch bislang keine Hochschule, die diesen Studiengang umsetzen wird. An einigen Universitäten wird Homöopathie, teilweise mit Unterstützung der Karl und Veronica Carstens-Stiftung, als Wahlkurs angeboten. Die Stiftung unterstützt zahlreiche studentische Arbeitskreise "Homöopathie" finanziell.

In Köthen wurde 2009 im restaurierten Gebäude des "Spitals der Barmherzigen Brüder" neben dem Hahnemannhaus die "Europäische Bibliothek für Homöopathie" eingerichtet. Betreiber ist der Deutsche Zentralverein homöopathischer Ärzte (DZvhÄ). Das Gebäude wurde dafür im Rahmen der Internationalen Bauausstellung Stadtumbau Sachsen-Anhalt 2010 für 2,6 Millionen Euro saniert, davon 751.064 Euro aus dem Programm "Stadtumbau Ost", sowie 1,16 Millionen Euro aus dem "Europäischen Fonds für regionale Entwicklung".

2003 wurde auf dem Deutschen Ärztetag die Zusatz-Weiterbildung Homöopathie in der neuen (Muster-)Weiterbildungsordnung neu geordnet. Voraussetzung zum Erwerb der Zusatzweiterbildung Homöopathie ist die Facharztanerkennung. Die Weiterbildung gliedert sich in 6 Monate Weiterbildung bei einem Weiterbildungsbefugten, oder 100 Stunden Fallseminare einschließlich Supervision, und 160 Stunden Kurs-Weiterbildung. Die Zahl der Fachärzte mit dieser Zusatzweiterbildung stieg von 2212 im Jahr 1993 auf 6712 im Jahr 2009.

Homöopathische Arzneimittel sind in Deutschland meist apothekenpflichtig. Homöopathische Behandlungen und Arzneimittel sind nicht im Leistungskatalog der gesetzlichen Krankenversicherung enthalten.
Einige Kassen bieten die Präparate aber als Satzungsleistung an und bezahlen unter bestimmten Bedingungen homöopathische Behandlungen bei Ärzten mit der Zusatzbezeichnung „Homöopathie“, beispielsweise im Rahmen von Verträgen zur Integrierten Versorgung. Eine Studie unter Mitgliedern der Techniker Krankenkasse kam zu dem Ergebnis, dass die Behandlungskosten von zusätzlich homöopathisch behandelten Patienten signifikant höher lagen als die der Kontrollgruppe.

Private Krankenversicherungen übernehmen in Deutschland die Kosten für homöopathische Behandlungen bei allen Ärzten, private Zusatzversicherungen darüber hinaus bei Heilpraktikern (gegebenenfalls abzüglich einer vereinbarten Selbstbeteiligung). Meist werden die Kosten für homöopathische Arzneimittel übernommen.

Zurzeit gibt es in Deutschland etwa 60.000 Ärzte, die homöopathische und anthroposophische Arzneimittel regelmäßig verordnen.

2013 betrug der Anteil homöopathischer Arzneimittel im deutschen Apothekenmarkt am Umsatz 1,3 %, an der Zahl der verkauften Einheiten 3,9 % (4 % im Vorjahr). Homöopathika hatten 2013 mit etwa 482 Mio. Euro einen Anteil von 8.1 % an rezeptfreien Medikamenten. Dabei wurden 1,6 % (92 Mio. Euro) durch Therapeuten verordnet und 6,5 % (386 Mio. Euro) durch Selbstkäufe erworben. Gegenüber 2011 nahm die Verordnungshäufigkeit um 3,8 % zu und der Selbstkauf stieg um 23 %.

Laut einer Umfrage des Instituts für Demoskopie Allensbach aus dem Jahre 2014 haben 60 % der Befragten bereits Homöopathika gegen diverse Indikationen eingenommen. Fast die Hälfte (48 %) berichten von uneingeschränkt positiven Heilerfahrungen. Während 1970 nur knapp jeder vierte Westdeutsche schon einmal selbst Homöopathika genommen hatte (24 Prozent), stieg inzwischen der Anteil in Deutschland mit 60 Prozent auf das 2,5-fache. Nur ein kleiner Teil der Bevölkerung (12 %) schließt die Verwendung homöopathischer Arzneimittel für sich aus.

Die Schweizer Jost Künzli, Adolphe Voegeli und Rudolf Flury spielten nach 1945 als homöopathische Lehrer in Europa eine bedeutende Rolle. Es wurden mehrere Zeitschriften, wie "Homöopathie", "Homoeopathia" und die "Schweizerische Zeitschrift für Homöopathie" begründet und wieder eingestellt. Innerhalb des "Schweizerischen Vereins Homöopathischer Ärztinnen und Ärzte" (SVHA) kam es in den 1970er- und 1980er-Jahren vermehrt zu Flügelkämpfen zwischen den unterschiedlichen Homöopathierichtungen.

Von 1999 bis 2005 wurde die Homöopathie zusammen mit den vier anderen alternativen Heilmethoden provisorisch in den Leistungskatalog der Grundversicherung der Krankenkassen aufgenommen. Die Behandlungen wurden von der Krankenkassen-Grundversicherung übernommen, sofern sie von einem Arzt verschrieben wurden. Am 30. Juni 2005 hat das Bundesamt für Gesundheit, Teil des Eidgenössischen Departements des Inneren, diese Leistungspflicht nach den Ergebnissen der von ihm in Auftrag gegebenen Studie "Programm Evaluation Komplementärmedizin" wieder gestrichen, da die Autoren zu dem Schluss kamen, dass „die vorliegenden placebokontrollierten Studien zur Homöopathie […] keinen eindeutigen Effekt über Placebo hinaus“ belegen. Am 17. Mai 2009 stimmte eine Mehrheit des Schweizer Stimmvolks dafür, dass die Berücksichtigung der Komplementärmedizin in der Bundesverfassung verankert wird. Die Verfassung wurde in Folge um den Artikel 118a „Bund und Kantone sorgen im Rahmen ihrer Zuständigkeiten für die Berücksichtigung der Komplementärmedizin“ erweitert. Zur Umsetzung dieses Verfassungszusatzes wird ab 2012 die Homöopathie neben vier weiteren alternativmedizinischen Behandlungsmethoden unter bestimmten Voraussetzungen von der obligatorischen Krankenpflegeversicherung bezahlt. Diese Regelung gilt provisorisch bis Ende 2017. In dieser Zeit gelten Wirksamkeit, Zweckmäßigkeit und Wirtschaftlichkeit der fünf komplementärmedizinischen Methoden als teilweise umstritten und werden hinsichtlich dieser Kriterien evaluiert.

Für Ärzte besteht die Möglichkeit, einen von der FMH anerkannten Fähigkeitsnachweis Homöopathie zu erwerben. Die Ausbildung dauert zwei Jahre und wird in Form von Seminaren und Supervision absolviert.

Bereits kurz nach dem Krieg gab es in Wien eine Interessenvertretung für Homöopathie. 1953 wurde die "Österreichische Gesellschaft für Homöopathische Medizin" (ÖGHM) als größte Vereinigung homöopathischer Ärzte Österreichs gegründet. Sie hat heute etwa 900 Mitglieder. In Österreich unterliegen homöopathische Arzneimittel dem Arzneimittelgesetz von 1983. In ihm wurde die Homöopathie als Teil der Medizin anerkannt. Seitdem ist die Ausübung der Homöopathie in Österreich Ärzten vorbehalten. Die Österreichische Ärztekammer verleiht dafür seit 1995 ein eigenes Diplom für den Bereich Homöopathie, welches zur Ausübung berechtigt. Zur Erlangung des Diploms ist eine mehrjährige Ausbildung zu absolvieren, die etwa 350 Fortbildungsstunden umfasst. 1991 wurde in Salzburg die "Ärztegesellschaft für Klassische Homöopathie" (ÄKH) gegründet und 1994 die "Österreichische Gesellschaft für Veterinärmedizinische Homöopathie" (ÖGVH). 1995 wurde eine Ausbildung zum "Fachtierarzt für Veterinärmedizinische Homöopathie" durch die "Delegiertenversammlung der Tierärzte Österreichs" beschlossen.

Heute ist die Homöopathie in fast allen Ländern der Erde vertreten. Weltweit liegt der Umsatz mit homöopathischen Arzneimitteln geschätzt in einer Größenordnung von 2 Milliarden Euro. Das sind weniger als ein Prozent des gesamten Arzneimittelmarkts. Die größten Märkte sind Frankreich, USA, Deutschland und Indien. Die Hälfte des Homöopathie-Marktes entfällt auf Europa.

In Großbritannien forderte Anfang 2010 ein Ausschuss des Unterhauses aufgrund fehlender Wirkungsnachweise, Homöopathie nicht mehr mit öffentlichen Mitteln zu finanzieren. Trotz der Empfehlungen des Ausschusses hatte die Regierung Großbritanniens zunächst beschlossen, die Homöopathie weiterhin über das NHS bezahlen zu lassen. Sie stimmte zwar weitgehend mit den Einschätzungen des Ausschusses überein, glaubt aber, dass der informierte Patient und sein Arzt in der Lage seien, die geeignete Behandlung zu finden. Dies schließe auch eine alternative Methode wie die Homöopathie ein. 2017 wurde bekannt, dass die Kosten künftig nicht mehr vom NHS getragen werden sollen.

In der EU gibt es schätzungsweise 50.800 Anbieter homöopathischer Medizin, davon 45.000 Ärzte.

Jährlich findet im April, in der Geburtswoche Hahnemanns, die "World Homeopathy Awareness Week" statt, um auf die Behandlungsmethode aufmerksam zu machen.

Die Wettbewerbsbehörde für Verbraucherschutz in den USA, FTC (Federal Trade Commission) hat 2016 strengere Regeln für frei verkäufliche homöopathische Produkte beschlossen. Die Produkte müssen entweder einen Wirksamkeitsnachweis haben, oder einen Warnhinweis tragen, dass die Wirksamkeit nicht wissenschaftlich belegt ist. Grundsätzlich sagt die Behörde: „Generell basieren die Versprechungen homöopathischer Produkte nicht auf modernen wissenschaftlichen Methoden und werden von modernen Medizinfachleuten nicht akzeptiert“.

Die Homöopathie ist keine einheitliche Lehre. Es gibt verschiedene Richtungen, die sich teilweise gegenseitig bekämpfen. Auch können Heilpraktiker oder Schulmediziner, die Homöopathie anwenden, nicht generell einer Richtung zugeordnet werden. Das große Spektrum an Richtungen verdeutlichen die zahlreichen Eigenbezeichnungen, wie "Klassische Homöopathie", "genuine Homöopathie", die "Bönninghausen-" und "Boger-Methode", die "miasmatische" und "wissenschaftliche Homöopathie", die "naturwissenschaftlich-kritische Richtung", die "prozessorientierte Homöopathie", die "kreative Homöopathie", die "Impuls-" und "Resonanzhomöopathie", die "Elektronische Homöopathie" (frequenzbasiert), die "Seghal-" und "Herscue-Methode", die "central delusion", "C4-Homöopathie", sowie "quantenlogische Homöopathie". Auch Begriffe, wie "organotrope" und "personotrope Homöopathie" werden benutzt. Die Spaltung der Homöopathie begann bereits zu Hahnemanns Zeiten, der Abweichler von seiner reinen Lehre auch als "Bastard-Homöopathen" bezeichnete. Hahnemann forderte bereits 1796: 

Der Begriff „Klassische Homöopathie“ entstand aus dem Bemühen, sich vom großen Spektrum der als „homöopathisch“ bezeichneten Heilmethoden abzugrenzen. Grundlagen der Klassischen Homöopathie sind die Lehre Hahnemanns und die sich daran orientierenden Weiterentwicklungen der Heilmethode (zum Beispiel durch Bönninghausen, Hering, Kent u. a.). Werden die Lehren Hahnemanns dabei besonders stringent eingehalten, werden sie als „genuine Homöopathie“ bezeichnet. Im Gegensatz zu vielen anderen Richtungen der Homöopathie wird in der Klassischen Homöopathie immer "nur ein Mittel" auf einmal verabreicht, meistens in einer mittleren oder hohen Potenz. Arzneimittel werden nach gründlicher Anamnese nach dem individuellen Symptombild des Kranken ausgewählt. Klassische Homöopathen behandeln sowohl akute Krankheiten als auch chronische Leiden (konstitutionelle Behandlung).

Siehe auch: Verband klassischer Homöopathen Deutschlands

Die naturwissenschaftlich-kritische Homöopathie ist eine Richtung der Homöopathie, die homöopathische Arzneimittel als Ergänzung zu schulmedizinischen Therapieformen einsetzt. Häufig werden niedrige Potenzen bis D12 verwendet, in denen noch ein chemisch nachweisbarer Rest der Arzneisubstanz vorhanden ist. Die Behandlung mit Hochpotenzen wird abgelehnt. Arzneimittel werden außerdem nicht nach dem oft sehr komplexen gesamten Symptombild des Kranken, sondern nach Pathologie (Krankheit) verordnet. Das erleichtert besonders die Findung des passenden Arzneimittels, weil zum Beispiel für eine Erkältungskrankheit nur noch aus einer Liste von wenigen Mitteln ausgewählt werden muss. Dieses Vorgehen steht jedoch im Widerspruch zu Hahnemanns Lehre, der in seinem "Organon" einer Vermischung der Homöopathie mit nicht-homöopathischen Behandlungsmethoden entgegentrat und sie als Verrat anprangerte:

Wichtige Vertreter dieser Homöopathierichtung waren Moritz Müller, Ludwig Grießelich, Friedrich Rummel, Franz Hartmann, Otto Leeser, Alfons Stiegele (1871–1956), Julius Mezger und Fritz Donner. Obwohl sie mit der "Hygea" eine eigene Zeitschrift hatten, waren Rummel und Hartmann auch in der Schriftleitung der "Allgemeinen Homöopathischen Zeitung" tätig.

Verbreitet ist auch die Verwendung von „Komplexmitteln“, d. h. einer Vermengung von verschiedenen Mitteln, die für eine bestimmte Krankheit nach organotropen Gesichtspunkten oder auch klinischen Indikationen zusammengestellt wird. Sie enthalten Einzelsubstanzen in sehr unterschiedlicher Dosierung, die sich in ihrer Wirkung verstärken sollen. Der evangelische Geistliche Emanuel Felke gilt als Begründer der Komplexmittelhomöopathie.
Die Therapie mit Komplexmitteln widerspricht ebenfalls dem Wesen der ursprünglichen Homöopathie. Hahnemann schreibt in seinem "Organon":

Neben Emanuel Felke waren der Italiener Cesare Mattei und der Schweizer Emil Bürgi bekannte Vertreter dieser Richtung.

Die erste Publikation zum Thema Tierhomöopathie stammt vom Hofapotheker Donauer aus dem Jahr 1815. Hahnemann selbst vertrat 1829 den Standpunkt, dass „… Thiere … ebenso sicher und gewiß, als die Menschen zu heilen“ wären. Eigene Schritte in Richtung der Entwicklung einer Tierhomöopathie unternahm er allerdings nicht. Zu Beginn des 20. Jahrhunderts verlor diese bis in die 1930er Jahre an Bedeutung. Den wichtigsten Einfluss auf ihre neuerliche Verbreitung hatte in der Nachkriegszeit Hans Wolter.

Homöopathieanhänger behaupten, es gebe Behandlungserfolge bei Tieren und diese ließen sich nicht durch Placeboeffekte erklären, da Tiere nicht an eine Wirksamkeit von Homöopathika „glauben“ könnten. Allerdings sind durchaus placeboähnliche Effekte bei Tieren nachgewiesen worden, die beispielsweise durch klassische Konditionierung oder die fürsorgliche Behandlung erklärt werden. Ferner beeinflusst die Meinung der Tierhalter zur Behandlungsart ihre Beurteilung des Behandlungserfolgs. Diese Effekte werden auch von Tierhomöopathen genutzt.

J. C. L. Genzke veröffentlichte 1837 ein Lehrbuch "Homöopathische Arzneimittellehre für Tierärzte" und berichtete darin über 67 Arzneimittelprüfungen an Hunden, Pferden und Rindern. Die Auswahl der Arzneimittel in der Tierhomöopathie erfolgt jedoch auf der Grundlage von Arzneimittelbildern (AMB), die überwiegend humanhomöopathischer Herkunft sind. Die Übertragbarkeit der AMB vom Menschen auf Tiere wird damit begründet, dass es beim Tier nichts gebe, was nicht auch latent oder homolog im Menschen als Organ, Verhaltensmuster oder Grundform einer Pathologie vorhanden sei.

In der seit dem 1. Januar 2009 geltenden EU-Bioverordnung für die tierische Erzeugung in der Ökologischen Landwirtschaft wird gefordert, dass Krankheiten der Tiere unverzüglich zu behandeln seien. Dabei dürfen „[…] chemisch-synthetische allopathische Tierarzneimittel einschließlich Antibiotika […] erforderlichenfalls unter strengen Bedingungen verwendet werden, wenn die Behandlung mit phytotherapeutischen, homöopathischen und anderen Erzeugnissen ungeeignet ist.“

Große Metaanalysen zur Beurteilung der Wirksamkeit der Homöopathie in der Veterinärmedizin fehlen. Die wenigen bislang durchgeführten, methodisch gut gestalteten klinischen Studien zeigten jedoch keine medizinische Wirksamkeit der Veterinärhomöopathie.

Homöopathische Arzneimittel werden nicht durch Medikamentenstudien gemäß dem deutschen Arzneimittelgesetz untersucht. Es wird auch keine erwartete Wirksamkeit überprüft, stattdessen wird eine Homöopathische Arzneimittelprüfung durchgeführt, bei welcher ein Prüfer durch „Selbstbeobachtung“ erhebt, ob und welche Symptome er durch das homöopathische Mittel erlebt. Der Prüfer wird dabei im homöopathischen Sinne als Proband bezeichnet. Diese Vorgehensweise genügt nicht den für Medikamentenstudien notwendigen wissenschaftlichen Qualitäts- und Objektivitätskriterien.

Die europäische Gesetzgebung sieht seit der Richtlinie 2001/83 ein eigenes Zulassungsverfahren für homöopathische Arzneimittel vor, bei dem wissenschaftliche Tests nicht erforderlich sind. In der Novelle zu dieser Richtlinie (2004/27) wird dieses vereinfachte Zulassungsverfahren für alle Mitgliedsländer verpflichtend. Die Richtlinie verlangt den Aufdruck „Homöopathisches Arzneimittel ohne genehmigte Heilanzeigen“.

Grundlage für die Wahl eines homöopathischen Mittels ist einerseits die Anamnese und andererseits die Kenntnis der Wirkungen und Symptome, die eine Arznei bei einem gesunden Menschen nach Meinung der Homöopathen auslösen könne. Um diese Kenntnisse zu erlangen, werden Arzneimittelprüfungen durchgeführt.

Als Hilfsmittel dienen dabei Arzneimittellehren und Repertorien. In Arzneimittellehren werden die Mittel mit allen bei der Arzneimittelprüfung angeblich beobachteten Symptomen beschrieben. Repertorien sind nach Symptomen hierarchisch gegliedert und verzeichnen alle Mittel, bei denen das jeweilige Symptom beobachtet worden sein soll. Die Wertigkeit eines Mittels (einwertig bis vierwertig) gebe einen Hinweis darauf, wie bewährt das Mittel bei der Heilung dieses Symptoms ist. Eine hohe Wertigkeit im Repertorium erhalte ein Mittel nur, wenn es sowohl bei der Arzneimittelprüfung bei einer hohen Zahl von gesunden Probanden dieses Symptom hervorgerufen habe als auch viele Berichte über Heilungen von Fällen mit diesem Symptom existierten.

Eine klare statistische Definition für eine solche „hohe Anzahl“ gibt es dabei nicht. Deshalb werden in modernen Repertorien auch Kennzeichnungen für bewährte Mittel geführt, die auf die Erfahrung einzelner Homöopathen mit hohem Ansehen zurückgehen. So werden zum Beispiel die "Künzli-Punkte" von vielen Autoren zitiert.

Potenzierte Mittel gibt es in Form von alkoholischen Lösungen, Tabletten und Globuli (mit homöopathischer Lösung imprägnierte Kügelchen aus Zucker). Bei der Einnahme von Lösungen sollte nach Empfehlung von manchen Homöopathen auf die Verwendung eines metallenen Löffels verzichtet werden, da dieser die vermeintlichen „Erinnerungseigenschaften“ der Flüssigkeit beeinflussen könne. Stattdessen kann ein Löffel aus Holz oder Kunststoff verwendet werden. Auch nahm Hahnemann an, dass der Genuss oder Geruch verschiedener Substanzen die Wirkung einiger homöopathischer Mittel beeinträchtigen könne.

Homöopathische Mittel sollen nach Meinung der Homöopathen unter die Zunge geträufelt bzw. unter der Zunge aufgelöst und ca. eine Minute im Mund belassen werden, um die Resorption des Zuckers über die Mundschleimhaut zu verbessern.
Das beste Ergebnis soll erreicht werden können, wenn die homöopathischen Arzneimittel sofort nach dem Auftreten der ersten Symptome eingenommen werden. Homöopathische Hochpotenzen sollen besonders wirksam sein, weshalb von Seiten der Homöopathen gefordert wird, dass diese immer durch einen versierten Homöopathen verordnet werden und der Verlauf beobachtet wird.

Abhängig von Wirkstoff und Trägersubstanz kann es Umstände geben, die auch gegen die Gabe eines bestimmten homöopathischen Mittels sprechen. So sollten beispielsweise trockene Alkoholiker keine alkoholischen Lösungen einnehmen, da diese einen Rückfall auslösen können. Auch Allergien oder Unverträglichkeiten gegen Bestandteile des Mittels, wie beispielsweise Honigbiene als Tiefpotenz bei Bienengiftallergie, können der Einnahme entgegenstehen. Schwangere und Stillende sollten Medikamente erst nach Rücksprache mit einem Arzt einnehmen; das Gleiche gilt für die Behandlung von Kindern.

Als relative Kontraindikation gelten Erkrankungen, die eine Substitutionstherapie erfordern, wie Diabetes mellitus Typ 1, akute Erkrankungen, die aus vitaler Indikation oder zur Vermeidung von Spätfolgen eine rasche schnell wirksame Behandlung erfordern und für die es bewährte Therapien gibt, wie beim akuten Herzinfarkt, bei allergischem Asthma oder allergischem Schock. Organische Erkrankungen, bei denen eine lebensbedrohliche Verschlechterung vorprogrammiert ist, wie bei bösartigen Erkrankungen, sollten ebenfalls nicht homöopathisch behandelt werden.

Als Nebenwirkung sehen Homöopathen eine vorübergehende Verstärkung der Symptome an, welche sie "homöopathische Verschlimmerung" (auch "Erstverschlimmerung") nennen. Die Existenz eines solchen Phänomens ist nicht belegt.

Bei niedrigen Potenzstufen (bis etwa D6) kann eine reguläre unerwünschte Arzneimittelwirkung auftreten, weil im Mittel noch nennenswerte Stoffmengen enthalten sind. So können z. B. durch die Anwendung von Mercurius (Quecksilber), Arsenicum (Arsen) oder Nux vomica (Brechnuss), einer Pflanze, die Strychnin-Alkaloide enthält, Vergiftungen hervorgerufen werden.

Eine internationale Studie aus dem Jahr 2016 kam zu dem Ergebnis, dass Nebenwirkungen durch die Homöopathie in einer ähnlichen Häufung auftreten wie in der evidenzbasierten Medizin. Die amerikanische Food and Drug Administration (FDA, dt. Behörde für Lebens- und Arzneimittel) veröffentlichte 400 Berichte von Nebenwirkungen (als häufigste Beschwerden wurden Krämpfe, Zittern, Fieber, Kurzatmigkeit und Lethargie genannt), die nach der Einnahme von Homöopathika aufgetreten waren.

"siehe ausführlicher:" Repertorium (Homöopathie)

Ein homöopathisches Repertorium enthält eine Sammlung von Symptomen und die dazugehörenden Arzneimittel aus verschiedenen Arzneimittellehren oder Arzneimittelprüfungen.
Der Homöopath repertorisiert anhand der Symptome eines Patienten im Repertorium das am häufigsten vorkommende Mittel und kann daraus das „ähnlichste“ Mittel für den Patienten aussuchen.

Bereits Hahnemann benutzte ein handschriftliches Findebuch. Die ersten gedruckten Repertorien stammen von seinen unmittelbaren Schülern Bönninghausen und Jahr. Ende des 19. Jahrhunderts veröffentlichte der homöopathische Arzt James Tyler Kent ein sehr umfassendes Repertorium in englischer Sprache, das bis heute das meistbenutzte Werk dieser Art ist. Weiterhin existieren auch digitale Repertorien.

Bis heute existiert weder ein formaler, reproduzierbarer Nachweis noch eine akzeptable naturwissenschaftliche Begründung für eine Wirksamkeit homöopathischer Arzneimittel, die über den Placebo-Effekt hinausgeht. Die Homöopathie wird zu den Pseudowissenschaften gezählt. Von der wissenschaftlichen Medizin wird die Homöopathie als wirkungslose, in einigen Fällen sogar riskante Behandlung abgelehnt.

Weiterhin werden auch die theoretischen Grundprinzipien der Homöopathie angegriffen. Das „Ähnlichkeitsprinzip“ (siehe auch Magisches Denken) sei von Hahnemann durch seinen Selbstversuch mit Chinarinde belegt worden. Dieser ist jedoch nicht reproduzierbar, Hahnemann zeigte lediglich die Symptome einer allergischen Reaktion auf Chinarinde. Das zweite wichtige Prinzip der Homöopathie, das besagt, dass homöopathische Mittel in „potenzierter“ Form wirksamer seien als „Urtinkturen“, gilt nach heutigen medizinischen und physikalischen Erkenntnissen als widerlegt, da geringere Wirkstoffkonzentrationen eine geringere Wirkung zur Folge haben.

Die homöopathische Medikation nach dem „Ähnlichkeitsprinzip“ ist aus wissenschaftlicher Sicht nicht nachvollziehbar. Die Beliebigkeit des Prinzips wurde schon zu Zeiten Hahnemanns kritisiert. Eine Unterscheidung in subjektive oder objektive Symptome ist in der Homöopathie kaum von Bedeutung. Während in der Medizin Symptome als Krankheitserscheinungen angesehen werden, sind Symptome in der Homöopathie die Krankheit selbst und nicht bloße Erscheinung derselben. Untersuchungen, wie etwa Röntgenbilder, Ultraschall oder Gewebeproben, werden zur Klärung der Ursache einer Krankheit nicht herangezogen. So werden bei konsequenter Durchführung beispielsweise allergisch, bakteriell oder viral hervorgerufene Erkrankungen gleich behandelt, wenn sie dieselben Symptome zeigen. Hinzu kommt, dass dieselben Symptome Bestandteil verschiedener Arzneimittelbilder sind. Die sich daraus ergebende Vielzahl an übereinstimmenden Bildern macht die Wahl des Mittels willkürlich.

Bereits im 19. Jahrhundert wurde die Homöopathie scharf kritisiert. 1851 bezeichnete der britische Arzt Robert Mortimer Glover sie als die schlimmste Art von Quacksalberei, die es je gegeben hätte und vermutlich je geben würde. Karl Wilhelm Fickel, zeitweilig leitender Oberarzt an der homöopathischen Lehranstalt Leipzig und unter dem Pseudonym Ludwig Heyne Autor homöopathischer Schriften, wandte sich komplett von der Homöopathie ab und veröffentlichte 1840 seine Schrift "Direkter Beweis von der Nichtigkeit der Homöopathie als Heilsystem". Er urteilte: „Als Heilsystem ist die Homöopathie eine Irrlehre, in praktischer Anwendung ein Unding.“ Carl Ernst Bock bezichtigte 1855 Hahnemann der Fälschung und bezeichnete die Homöopathie als „ein Gewebe von Täuschungen, Unwissenheit und Unwahrheiten“.

Auch Lexika fanden schon früh klare Worte:

Fritz Donner, ein Vertreter der naturwissenschaftlich-kritischen Homöopathie, war in den Jahren 1936 bis 1939 an Überprüfungen homöopathischer Arzneimittel beteiligt, die vom damaligen Reichsgesundheitsamt angeordnet worden waren. Die erwartete Wirksamkeit ließ sich dabei nicht nachweisen. Er zitierte Hanns Rabe, den damaligen 1. Vorsitzenden des "Deutschen Zentralverbandes homöopathischer Ärzte" (DZV), mit den Worten: „Wir können doch das gar nicht, was wir behaupten!“
Seine Beobachtungen fasste er in einen Report für die Robert Bosch Stiftung zusammen, der 1969 zunächst in französischer Sprache, jedoch erst 1995 in deutscher Sprache veröffentlicht wurde. Donner untersuchte auch homöopathische Arzneimittelprüfungen und erwähnte dabei unseriöse Praktiken der Prüfer. Bereits während seiner Tätigkeit am Stuttgarter Robert-Bosch-Krankenhaus war bei ihm und einigen seiner Kollegen der Verdacht aufgekommen, dass es bei Arzneimittelprüfungen Placebosymptome gegeben haben könnte.

In mehr als 100 wissenschaftlichen Studien konnte kein Nachweis für die pharmakologische Wirksamkeit homöopathischer Arzneimittel erbracht werden, die über den Placebo-Effekt hinausgeht.

Eine erste Metaanalyse von Klaus Linde und Mitarbeitern aus dem Jahr 1997 kam zwar zu dem Schluss, dass die Gesamtheit der Ergebnisse der Studien nicht vollständig durch den Placeboeffekt erklärbar seien und dass einige der untersuchten Homöopathika folglich wirksam sein müssten. Bei weiteren Untersuchungen fanden die Autoren allerdings, dass Studien geringerer Qualität bessere Ergebnisse für die homöopathische Behandlung zeigten als Studien mit strengen Kriterien. Linde räumte daher ein, dass seine damalige Schlussfolgerung nicht haltbar sei und seine Metaanalyse die Effekte zumindest deutlich überschätzt haben dürfte.

Eine Studie aus dem Jahr 2003, die angeblich einen empirischen Nachweis der Wirksamkeit hochpotenzierter Homöopathika geliefert hatte, wurde Ende 2005 zurückgezogen. Ein Forschungspreis, der den Forschern (der Apothekerin Franziska Schmidt und den Pharmakologen Karen Nieber und Wolfgang Süß) zugesprochen worden war, wurde zurückgegeben. Der Chemiker Klaus Keck (Konstanz), der Mathematiker Gerhard Bruhn und der Geophysiker Erhard Wielandt hatten zuvor öffentlich bemängelt, dass die Ergebnisse der Studie beruhten. Selbst erklärte Befürworter der Homöopathie haben diese Fehler erkannt und bestätigt.

Entgegen der sonstigen Studienlage bestehen Befürworter der Homöopathie darauf, dass eine Behandlung mit homöopathischen Arzneimitteln über den Placebo-Effekt hinaus wirke. Oftmals werden dabei anekdotische Einzelfälle oder gar Selbsttests als „Beleg“ angegeben. Auch kann der Rechtfertigungsdruck, der auf alternativen Methoden lastet, dazu verführen, Erfolgsgeschichten zu verbreiten. Solche Einzelfälle besitzen wissenschaftlich keine Relevanz, weil hierfür eine Doppelblindstudie mit einer Kontrollgruppe, die Placebos erhält, notwendig wäre und die statistische Signifikanz der Ergebnisse überprüft werden müsste, beispielsweise mittels eines Vierfeldertests. An einem Einzelfall lässt sich prinzipiell nicht eruieren, auf welchen Effekt die Genesung zurückzuführen ist, und ob eine andere Behandlung andere Ergebnisse gebracht hätte.

Zudem können Erfolge, die der Homöopathie nachgesagt wurden, nach wissenschaftlichen Anforderungen allein mit methodischen Schwächen und verzerrenden Einflüssen erklärt werden, wie eine Metaanalyse zur Wirksamkeit homöopathischer Behandlungen betont, die 2005 in der renommierten Medizinzeitschrift "The Lancet" von Shang und Mitarbeitern dokumentiert wurde und laut dem Kommentar des Herausgebers das „Ende der Homöopathie“ markiere. Eine schweizerisch-britische Forschergruppe hatte zur Bewertung des Behandlungserfolges bei verschiedenen Erkrankungen 110 Homöopathie-Studien 110 entsprechende Studien aus der konventionellen Medizin gegenübergestellt. Es zeigte sich, dass in der Homöopathie die gemessenen Effekte nicht gegen die Annahme der Nullhypothese (die Homöopathie beruhe einzig auf dem Placebo-Effekt) sprechen. Auch bestätigte die breitangelegte Metauntersuchung die mathematische Erkenntnis, dass Studien mit einigen wenigen Teilnehmern und niedriger Qualität eher nicht-vorhandene Wirkungen vorspiegeln als solche mit einer hohen Teilnehmerzahl und guter Qualität. Diese Metastudie wurde 2006 von dem österreichischen Homöopathie-Befürworter Friedrich Dellmour und dem "Schweizerischer Verein Homöopathischer Aerztinnen und Aerzt"e hauptsächlich mit der Behauptung kritisiert, dass sich die Homöopathie nicht für wissenschaftliche Doppelblindstudien eigne und deshalb keine Wirksamkeit messbar sei. Außerdem wurde die Methodik der Metastudie kritisiert.

Die Arzneimittelkommission der deutschen Ärzteschaft wies in einer Stellungnahme darauf hin, dass Therapieeinrichtungen – darunter die Homöopathie – , die nicht wissenschaftlich fundiert arbeiten, solche Besonderheiten geltend machen, um sich der wissenschaftlichen Prüfung ihrer Hypothesen zu entziehen. Sowohl prinzipiell als auch in der Praxis sei die Durchführung sauber geplanter und durchgeführter placebokontrollierter, doppelblinder Studien möglich. Diese werden auch durchgeführt und es zeige sich kein Unterschied zwischen der homöopathischen Behandlung und der Placebobehandlung. Untersuchungen ohne Kontrollmedikation durch Placebos sind dagegen zum Nachweis der Wirksamkeit ungeeignet. Mit dem oft zitierten Satz „Wer heilt hat recht“ könne die Wirksamkeit nach wissenschaftlichen Prinzipien nicht nachgewiesen werden.

Der Bericht der britischen Regierung zur Homöopathie stellte im Jahr 2010 fest, dass es keinen glaubwürdigen Beweis zur Wirksamkeit der Homöopathie gebe. 2015 kam eine von der australischen Regierung in Auftrag gegebene Studie zum selben Ergebnis. In den USA müssen seit 2017 rezeptfreie Homöopathika mit einem Hinweis auf ihre Wirkungslosigkeit versehen werden.

Aus utilitaristischer Sicht wurde die Homöopathie auch als ethisch inakzeptabel bezeichnet.

Eine Wirkung der homöopathischen Arzneimittel wird von Kritikern der Homöopathie als unplausibel abgelehnt. Sie sehen Erklärungen für die angebliche Wirksamkeit einer homöopathischen Behandlung in:

Bei einer Potenzierung von D24 oder C12 (Verdünnung von 1:10) enthalten nur noch etwa die Hälfte aller Mischungen, die aus einer einmolaren Ausgangslösung hergestellt wurden, überhaupt ein Restmolekül der Ausgangssubstanz (bezogen auf 1 Liter D24-Lösung/Mischung). Eine solche Verdünnung entspricht ungefähr dem Auflösen einer Kopfschmerztablette im Atlantik. Da die Herstellung der homöopathischen Arzneien üblicherweise nicht in einem keim- und staubgefilterten Reinraum durchgeführt wird, muss angenommen werden, dass im Verdünnungsprozess, etwa beim Öffnen des Mischgefäßes und der Zugabe von Verdünnungslösung, die Konzentration der Wirksubstanz zwar abnimmt, aus der Luft aber Verunreinigungen hinzukommen. Dies bewirkt, dass schließlich in den hochpotenzierten Präparaten außer der Trägersubstanz (Wasser, Ethanol oder Milchzucker) nur die Verunreinigung der Trägersubstanzen (alle drei enthalten metallische Verunreinigungen) und die Verunreinigungen aus der Umgebung enthalten sind. Auch die besten Filtrierverfahren lassen manchmal mehr Reststoffe im Wasser zurück, als sich homöopathische Wirkstoffe darin befinden. Somit kann eine Stoff-Wirkungsrelation nicht vernünftig untersucht werden. Zusätzlich zum Einsatz wirkstofffreier Potenzierungen beinhaltet aus pharmakologischer Sicht die behauptete Wirkungssteigerung durch Potenzierung auch, dass stärker verdünnte Lösungen mit weniger gelösten Wirkstoffmolekülen stärker wirken sollen. Der Pharmakologe Klaus Starke vermisst sowohl beim Simile-Prinzip als auch bei der homöopathischen Potenzierung die „biologische Basis“ und ordnet die Homöopathie den „dogmatischen Arzneitherapien“ zu. Die Pharmakologen Lüllmann, Mohr und Hein bewerten die Homöopathie als . Die Homöopathie müsse sich auf Patienten beschränken, deren Erkrankung durch eine nicht besser behandelbar sei.

Homöopathen vertreten die Hypothese, dass eine Wirkung durch im Wasser „gespeicherte“ Information eintrete. Diesen „Gedächtniseffekt von Wasser“ wollte 1988 Jacques Benveniste mittels der Beeinflussung weißer Blutzellen (Leukozyten) durch hochgradig verdünnte Antigene nachgewiesen haben. Die Ergebnisse ließen sich nicht reproduzieren. Auch weitere wissenschaftlichen Nachweise einer angeblichen Informationsspeicherfähigkeit des Wassers konnten nicht bestätigt werden.
In Hahnemanns Organon gab es bereits in den sechs Auflagen von 1810 bis 1842 umfangreiche Einarbeitungen, Streichungen, Änderungen von übernommenen Teilen und Widersprüche. Verschiedene Schulen nutzen unterschiedliche Auflagen als Handlungsanweisung. (siehe Abschnitt „Richtungen in der Homöopathie“)

"Potenzierung der Ursubstanz"
An einigen Stellen sehen Kritiker Widersprüche in der homöopathischen Theorie und Praxis. So wird nicht erklärt, warum nur die gewünschten Eigenschaften eines jeweiligen Stoffes durch eine „Potenzierung“ ihre Wirkung verstärken und nicht auch die unerwünschten Nebenwirkungen bzw. die Wirkungen und Nebenwirkungen all der anderen Spurenelemente, Reststoffe etc., die sich außerdem noch im Alkohol bzw. Wasser oder im Gefäß befunden haben.

Ein Beispiel zu den verunreinigenden Reststoffen im Wasser wurde im Nachgang des Leipziger Skandals 2003 (siehe Abschnitt „Kein Nachweis der Wirksamkeit“) von Wissenschaftlern als Gedankenexperiment errechnet: Wenn auch nur eine einzige Tollkirsche in einen Bach fällt, dessen Wasser in die Leipziger Trinkwasserversorgung führt (34 Millionen m Jahresverbrauch), dann würde dies zu einer Atropinkonzentration im Trink-/Brauchwasser von D17 führen. Dies bedeutet, dass es methodisch gar nicht möglich ist, größere Atropin-Verdünnungen als D17 herzustellen, weil das homöopathische Heilmittel bereits mit D17-Atropin-Wasser hergestellt wird. Selbst das reinste auf der Welt herstellbare destillierte Wasser enthält immer noch einige Moleküle fast aller häufigeren Elemente und zahlreicher chemischer Verbindungen als Verunreinigungen. Da sich in hochpotenzierten Homöopathika jedoch rein mathematisch gar keine Moleküle der Ausgangssubstanz befinden dürften, ist diese Konzentration an Verunreinigungen im fertigen Medikament in jedem Falle höher als die des Homöopathikums, denn durch die Luft gelangen solche Verunreinigungen bei jeder Potenzierung in die Lösung. Die anfänglichen Verunreinigungen werden bei der Potenzierung natürlich ebenso „mitpotenziert“ wie das Homöopathikum, so dass die Verunreinigungen im fertigen Medikament nicht nur als erneute Verunreinigungen durch die Umwelt vorliegen, sondern auch in höchstpotenzierter Form (höherpotenzig als die Wirksubstanz selbst). Jedes Homöopathikum ist also in Wahrheit ein buntes Gemisch aus mehr oder minder hohen Potenzen unterschiedlichster Substanzen, unter denen die vermeintliche Wirksubstanz keinerlei hervorgehobene Rolle mehr spielen kann. Nach Auffassung der Wissenschaftler führt dieser Umstand ein zentrales Prinzip der Homöopathie ad absurdum.

"Heilung der Krankheit oder der Symptome?"
Viele Homöopathen führen an, die Schulmedizin heile keine Krankheiten, sondern unterdrücke nur Symptome, während die Homöopathie die Ursache des Leidens bekämpfe.

Nach Hahnemann kann eine Krankheit aber "nur" durch ihre Symptome erkannt werden, was sich in der Praxis der homöopathischen Anamnese und Verschreibung nach Symptombild widerspiegelt.

Sehr deutlich ist dies in Hahnemanns eigenen Schriften - etwa „Organon der Heilkunst“ - zu erkennen,:

[…] aber bloß die Gesammtheit der Symptome ist die dem Heilkünstler zugekehrte Seite der Krankheit, bloß diese ist ihm wahrnehmbar und das einzige, was er von der Krankheit wissen kann und zu wissen braucht zum Heil-Behufe.

Eine erfolgreiche Behandlung der Symptome ist der Heilung der eigentlichen Krankheit gleichzusetzen:
Es lässt sich nicht denken, auch durch keine Erfahrung in der Welt nachweisen, daß, nach Hebung aller Krankheitssymptome […] etwas anders, als Gesundheit, übrig bliebe oder übrig bleiben könne, so daß die krankhafte Veränderung im Innern ungetilgt geblieben wäre.

Kritiker sehen in der Argumentation, dass das Verschwinden von Symptomen bei schulmedizinischer Behandlung als Unterdrückung, bei homöopathischer als Heilung gewertet wird, ein Messen mit zweierlei Maß.

Im Jahr 2011 wurden in Deutschland Homöopathika für 389 Millionen Euro umgesetzt, weltweit etwa 2 Milliarden. Zahlen über den Rohgewinn dabei werden nicht publiziert.

Im Oktober 2010 wurde ein Betrugsfall in Bangladesch entdeckt bei dem Medikamentenfälscher hochprozentigen Alkohol in Fläschchen füllten, welche dann bloß als unterschiedliche homöopathische Heilmittel etikettiert wurden.

Die Homöopathie wird durch eine massive Lobbyarbeit nach dem Vorbild der übrigen Pharmaindustrie unterstützt. Kritiker der Homöopathie werden dabei zum Teil namentlich an den Netz-Pranger gestellt, und entsprechende Publikationen finanziell gefördert. Der britische Wissenschaftler Edzard Ernst wurde 2005 stark angegriffen, nachdem er öffentlich einen Bericht über Alternativmedizin als „skandalös und voller Fehler“ bezeichnete, nach einer 13-monatigen Untersuchung konnte ihm aber kein Fehlverhalten nachgewiesen werden.

Der Verzicht auf eine normale medizinische Versorgung kann bei akuten Notfällen lebensgefährlich sein, wenn der Einsatz einer wirksamen Therapie verzögert wird. Ein Extrembeispiel ist die von einer Homöopathin bei der Indikation vorgeschlagene alleinige Gabe des homöopathischen Mittels Aconitum in der Potenz C30 oder C200. Aconitum (Blauer Eisenhut) würde in einer Arzneimittelprüfung Herzsymptome oder sogar den Herztod verursachen, also ist dieses Arzneimittel nach homöopathischer Raison bei Herzstillstand sinnvoll.
Da das verstärkte Auftreten der Symptome unter dem Begriff Erstverschlimmerung als Teil der Heilung verstanden wird, könnten wichtige Notfallbehandlungen versäumt oder erst verspätet vorgenommen werden. Eine konventionelle Therapie darf hier nicht durch eine homöopathische Behandlung ersetzt werden.

Auch bei Erkrankungen, die keine Notfallsituationen darstellen, kann die alleinige homöopathische Behandlung durch den Verzicht auf eine nachgewiesen wirksame Behandlung zum Tode führen. 2002 starb in Australien ein neun Monate altes Mädchen, dessen Eltern ein Ekzem des Kindes ausschließlich homöopathisch behandelten. 2005 starb, ebenfalls in Australien, eine 45-jährige Frau an den Folgen einer Darmkrebserkrankung, die auch ausschließlich homöopathisch behandelt wurde.

Pharmakologisch und toxikologisch können niedrigpotente Homöopathika problematisch sein. Unzureichend verdünnte Homöopathika, von „Urtinkturen“ bis zu „Potenzen“ von „D4“, können Wirkstoff enthalten und zu allergischen Reaktionen und zu Vergiftungserscheinungen führen. Die amerikanische Food and Drug Administration (FDA) warnte wiederholt und zuletzt im September 2016 vor sogenannten "Teething Tablets" und "Teething Gels". Das sind vorgeblich homöopathische Präparate, die aus Schwarzer Tollkirsche ("Atropa belladonna") hergestellt werden und bei Kleinkindern die Schmerzen beim Zahnen erträglich machen sollen. Seit ihrer Warnung im Jahr 2010 erreichten die Behörde mehr als 400 Berichte über Atropin-Vergiftungen nach Anwendung derartiger Produkte, darunter auch 10 Todesfälle. Laboruntersuchungen fanden in verschiedenen Produkten der "Teething Tablets" Belladonna-Konzentrationen, die weit über dem deklarierten Gehalt lagen. Zum Schutz der Konsumenten wurde der Hersteller aufgefordert, die betroffenen Produkte zurückzurufen.

Impfgegner lehnen Schutzimpfungen oft auch mit Hinweis auf die Homöopathie ab. Hahnemann selbst zeigte sich jedoch im "„Organon der Heilkunst“" beeindruckt von der Schutzimpfung und hat sie als Indiz für die Existenz des Simile-Prinzips und Beispiel für das Funktionieren desselben angeführt. Homöopathisch tätige deutsche Ärzte mit der Zusatzweiterbildung "Homöopathie" impfen seltener als Nicht-Homöopathen, lehnen jedoch eine Impfung nicht grundsätzlich ab. Impfungen gegen Kinderkrankheiten, bei Risikogruppen und als ineffektiv beurteilte Impfungen werden von homöopathischen Ärzten allerdings weniger akzeptiert und angewandt. Verschiedene homöopathische Fachgesellschaften stehen Schutzimpfungen zwar kritisch gegenüber, halten sie jedoch grundsätzlich für sinnvoll. Gründe für das zurückhaltendere Verhalten bei manchen Impfungen, wie gegen Masern, Mumps, Windpocken und Keuchhusten, ist auch die positive Einschätzung einer möglichen homöopathischen Alternativbehandlung.

Gelegentlich werden von Homöopathen auch „homöopathische Impfungen“ (orale Gaben von Krankheitsprodukten, „Nosoden“, in Potenz) oder „homöopathische Malariaprophylaxe“ angeboten. Solche Angebote werden von den Dachverbänden offiziell abgelehnt. Der Deutsche Zentralverein homöopathischer Ärzte (DZVhÄ) beispielsweise schreibt: . Andererseits stellt derselbe Verband die – wissenschaftlich unbelegte – Möglichkeit einer wie auch eine homöopathische Therapie in Aussicht. Steffen Rabe, Münchner Kinderarzt und Internetbeauftragter im DZVhÄ, hält sogar eine bewusst herbeigeführte Ansteckung mit dem Masernvirus, „Masernpartys“, im Alter zwischen etwa drei und acht Jahren für . Die vorsätzliche Verbreitung menschlicher Krankheiten stellt allerdings eine gefährliche Körperverletzung dar, insbesondere bei unter Umständen tödlich verlaufenden Masern.

Nach einem Artikel des "British Medical Journal" aus dem Jahre 2009 warnt die WHO davor, bestimmte ernste Krankheiten wie Infektionen mit HIV, Tuberkulose und Malaria sowie bei Kindern zusätzlich Durchfall oder Grippe homöopathisch zu behandeln. Diese Warnung war eine Reaktion auf die Befürchtungen einer Gruppe junger Ärzte und Forscher, dass die Anwendung von Homöopathie bei diesen fünf Krankheiten in Entwicklungsländern lebensgefährliche Folgen hätte.
Die Gruppe Homöopathen ohne Grenzen versucht in Entwicklungsländern Malaria homöopathisch zu heilen.
2014 wollten homöopathische Ärzte eine Ebola-Therapie in Liberia testen, dies wurde jedoch von den örtlichen Behörden untersagt.

Der im 20. Jahrhundert populäre Abenteuerromanschriftsteller Karl May beschreibt 1881 in seiner Reiseerzählung "Durch Wüste und Harem", die 1895 in "Durch die Wüste" umbenannt wurde, dass ihm, als seinem Alter Ego "Kara Ben Nemsi", in Kairo eine „noch halb gefüllte homöopathische Apotheke von Willmar Schwabe in die Hand gekommen“ sei. Er schildert, wie er mit einem „Kästchen mit Aconit, Sulphur, Pulsatilla und all' den Mitteln, welche in einer Apotheke von hundert Nummern zu haben sind, ... hier und da bei einem Fremden oder Bekannten fünf Körnchen von der dreißigsten Potenz“ erfolgreich eingesetzt hätte und so in den Ruf eines erfolgreichen Arztes gelangt wäre. Der Pharmazeut Willmar Schwabe war ein Hersteller von Homöopathika (siehe oben).

In Köthen (Anhalt) gibt es das Hahnemannhaus in der Wallstraße 47 (mit Gedenktafel) und in der Wallstraße 48 die europäische Fachbibliothek zur Homöopathie sowie im Historischen Museum im Schloss eine Dauerausstellung zur Homöopathie.
In Hahnemanns Geburtsstadt Meißen findet sich in einer alten Klosteranlage das Hahnemannzentrum e. V. 
Zahlreiche Apothekenmuseen zeigen (auch) homöopathische Mittel.









</doc>
<doc id="12704" url="https://de.wikipedia.org/wiki?curid=12704" title="Residenzpflicht (Begriffsklärung)">
Residenzpflicht (Begriffsklärung)

Residenzpflicht steht für:



</doc>
<doc id="12706" url="https://de.wikipedia.org/wiki?curid=12706" title="AD">
AD

AD steht für:
AD als Unterscheidungszeichen auf Kfz-Kennzeichen:

A/D steht als Abkürzung für:

Ad steht für:

a. D. steht als Abkürzung für:

ad steht als Abkürzung für:


.ad steht für:

Siehe auch:


</doc>
<doc id="12707" url="https://de.wikipedia.org/wiki?curid=12707" title="Mitglied des Europäischen Parlaments">
Mitglied des Europäischen Parlaments

Ein Mitglied des Europäischen Parlaments (kurz MdEP; , kurz "MEP") ist ein gewählter Vertreter im Europäischen Parlament. Deutschsprachige Mitglieder des Europäischen Parlaments bezeichnen sich selbst meist als Europaabgeordnete oder Europaparlamentarier. Dies ist zwar nicht der offizielle Begriff, aber die in der deutschen Sprache am häufigsten verwendete Bezeichnung.

Die Europaabgeordneten vertreten im politischen System der Europäischen Union die Unionsbürger. Sie sind an der Gesetzgebung auf europäischer Ebene beteiligt und kontrollieren die Exekutive der EU, das heißt insbesondere die Europäische Kommission. Im Parlament sind die Abgeordneten in länderübergreifenden Fraktionen organisiert, in denen sich jeweils Abgeordnete mit ähnlicher politischer Ausrichtung vereinen. Um Themen fachkundig behandeln zu können, spezialisieren sich die Abgeordneten und werden dementsprechend in zwanzig ständige Ausschüsse gewählt, die für bestimmte Sachbereiche zuständig sind und die Arbeit der Plenarsitzungen vorbereiten.

Nach der letzten Europawahl hat sich das Parlament am 1. Juli 2014 für die achte Wahlperiode konstituiert. Entsprechend Abs. 2 des Vertrags über die Europäische Union und ergänzender Rechtsvorschriften setzt sich das Europäische Parlament derzeit aus 751 Vertretern der Unionsbürgerinnen und Unionsbürger zusammen, die als "Mitglieder des Europäischen Parlaments" bezeichnet werden, darunter 96 deutsche, 18 österreichische und 6 luxemburgische Abgeordnete. Aufgrund von Rücktritten oder Todesfällen kann die Zahl der Abgeordneten vorübergehend sinken, bis die gemäß jeweiligem Wahlrecht nachrückberechtigte Person ihr Mandat antritt. Die nächsten Direktwahlen aller Abgeordneten durch die stimmberechtigten Bürger aller 28 EU-Staaten finden 2019 statt. In ihren Heimatländern sind die Abgeordneten Mitglieder in über 150 verschiedenen Parteien, die zum größten Teil einer der 12 politischen Parteien auf europäischer Ebene angehören.
Aufgaben, Rechte, Pflichten, Immunität, Bezahlung der Abgeordneten und ähnliches werden geregelt durch:

In der 1952 eingerichteten Parlamentarischen Versammlung der Europäischen Gemeinschaft für Kohle und Stahl, dem Vorgänger des Europäischen Parlaments, wurden die Mitglieder von den Parlamenten der Mitgliedstaaten bestimmt. Seit der Europawahl 1979 werden die Abgeordneten des Europäischen Parlaments jedoch alle fünf Jahre in allgemeinen, unmittelbaren, freien und geheimen Europawahlen gewählt. Diese Wahlen finden in allen Mitgliedstaaten gleichzeitig, aber mit jeweils etwas unterschiedlichen Wahlsystemen und getrennten Listen statt. Jeder Mitgliedstaat entsendet eine feste Anzahl an Europaabgeordneten, wobei nach dem Prinzip der degressiven Proportionalität größere Mitgliedstaaten jeweils mehr Sitze haben als kleinere, kleinere Mitgliedstaaten jedoch mehr Sitze "pro Einwohner" als größere. Im Einzelnen ist die Anzahl der Sitze im AEU-Vertrag festgeschrieben und kann nur durch eine einstimmige Vertragsreform geändert werden.

Wahlberechtigt und wählbar ist jeder Bürger der Europäischen Union. Bürger, die in einem anderen EU-Mitgliedstaat leben als dem, dessen Staatsangehörigkeit sie besitzen, können frei wählen, in welchem dieser Staaten sie ihr Wahlrecht ausüben. Die Altersgrenzen für das aktive und passive Wahlrecht werden jeweils von den Mitgliedstaaten festgelegt.

Im Vergleich zu nationalen Parlamenten hat das Europäische Parlament eine verhältnismäßig große Austauschrate der Parlamentarier. Häufig legen Abgeordnete etwa ihr Mandat nieder, um sich in ihr nationales Parlament wählen zu lassen oder ein nationales Regierungsamt anzutreten. Da zudem insbesondere in kleinen Mitgliedsländern mit wenig Sitzen im Europäischen Parlament kleinere Parteien nur geringe Chancen haben, allein ein Mandat zu erringen, schließen sich diese manchmal zu Listenverbindungen zusammen, mit der vorherigen Vereinbarung, nach einem bestimmten Zeitraum zu „rotieren“. Solche Vereinbarungen haben allerdings keinen rechtlichen Wert; formal dauert das Mandat jedes Abgeordneten jeweils bis zur nächsten Europawahl und kann nur durch seinen eigenen Willen niedergelegt werden. Scheidet ein Parlamentarier aus dem Parlament aus, so wird er durch den Nächstplatzierten auf der jeweiligen nationalen Wahlliste ersetzt, auf der er gewählt wurde. Der einzige derzeit (Stand: März 2011) amtierende Europaabgeordnete, der seit der Europawahl 1979 durchgängig dem Parlament angehörte, ist der Deutsche Hans-Gert Pöttering (CDU).

Bis zur ersten Europawahl 1979 hatten alle Europaabgeordneten ein sogenanntes „doppeltes Mandat“: Sie waren zugleich Abgeordnete des Europäischen und ihres jeweiligen nationalen Parlaments. Auch in den ersten Europawahlen kandidierten häufig noch prominente Abgeordnete der nationalen Parlamente, die dann ein doppeltes Mandat wahrnahmen. Mit den zunehmenden Kompetenzen des Europäischen Parlaments und dem damit verbundenen wachsenden Arbeitsaufwand eines Mandats wurde diese Praxis jedoch immer seltener angewandt und von verschiedenen Parteien und Mitgliedstaaten abgelehnt. Seit der Europawahl 2004 sind doppelte Mandate nicht mehr erlaubt (mit Ausnahmen für Großbritannien [bis 2009, faktisch bis 2005] und Irland [bis 2007]).

Da Unionsbürger jeweils auch im Staat ihres Wohnorts wählbar sind, kommt es immer wieder vor, dass Mitglieder des Europäischen Parlaments nicht die Staatsangehörigkeit des Landes besitzen, für das sie gewählt wurden. Die folgende Tabelle zeigt einen Überblick über diese Abgeordneten bis einschließlich der Europawahl 2004. Drei Abgeordnete, nämlich Daniel Cohn-Bendit, Monica Frassoni und Ari Vatanen, wurden bereits in mehreren Ländern gewählt.

Der Frauenanteil an den Europaabgeordneten ist von 1979 bis 2009 bei jeder Europawahl gestiegen, von zunächst 18 % auf 35 %. Er liegt damit über dem Durchschnitt der nationalen Parlamente in Europa und etwa auf der Höhe des Deutschen Bundestags (mit 33 % Frauen seit der Bundestagswahl 2009). Allerdings variiert er je nach Mitgliedstaat stark. In der Legislaturperiode 2009–2014 war der Frauenanteil der finnischen (61,5 %) und schwedischen Europaabgeordneten (55,6 %) am höchsten, bei den tschechischen (18,2 %) und maltesischen Abgeordneten (0 %) am niedrigsten. Bei den deutschen Abgeordneten lag der Frauenanteil bei 37,4 %, bei den österreichischen bei 41,2 %.

Vor allem in den 1970er Jahren hatte das damals noch weitgehend machtlose Europäische Parlament den Ruf, vor allem als renommierte Versorgungsstätte für nationale Altpolitiker zu dienen (was in Deutschland zu dem Spottspruch „Hast du einen Opa, schick ihn nach Europa“ führte); 1979 lag das Durchschnittsalter der Abgeordneten mit 51 Jahren deutlich über demjenigen des Deutschen Bundestags mit 47 Jahren. Inzwischen liegt das Durchschnittsalter im Europäischen Parlament jedoch auch unter 50 Jahren und etwa auf der Höhe von demjenigen nationaler Parlamente. Das älteste Parlamentsmitglied in der Legislaturperiode 2009–2014 war der Italiener Ciriaco De Mita (* 1928), das jüngste Mitglied war die Schwedin Amelia Andersdotter (* 1987).

Im Europäischen Parlament organisieren sich die Abgeordneten in Fraktionen, die sich jeweils an den europäischen Parteien orientieren, in denen die verschiedenen nationalen Parteien Mitglied sind. In der Wahlperiode 2009–2014 gibt es sieben solche Fraktionen sowie eine Reihe von fraktionslosen Abgeordneten.

Gemäß Art. 6 des Direktwahlakts geben die Abgeordneten „ihre Stimmen einzeln und persönlich ab“ und sind „weder an Aufträge noch an Weisungen gebunden“, sie verfügen also über ein freies Mandat. In der Praxis wird dieses wie auch in anderen Parlamenten durch die Fraktionsdisziplin eingeschränkt. Aufgrund der heterogenen Zusammensetzung des Europäischen Parlaments ist diese Fraktionsdisziplin hier jedoch traditionell eher schwach ausgeprägt; fraktionenübergreifende Mehrheiten zu bestimmten Themen sind häufiger als in den meisten nationalen Parlamenten. Dies ist zum einen damit zu erklären, dass die Aufteilung in Regierungs- und Oppositionsfraktionen im Europäischen Parlament weitgehend entfällt. Zum anderen haben die europäischen Parteien keinen Einfluss auf die Kandidatenaufstellung ihrer nationalen Mitgliedsparteien bei der Europawahl: Die Loyalität der Abgeordneten gilt daher manchmal nicht der Gesamtfraktion, sondern vor allem ihrer nationalen Landesgruppe innerhalb der Fraktion, die einen größeren Einfluss auf ihre Wiederwahl hat. Im Zuge der zunehmenden Kompetenzen und der damit einhergehenden Professionalisierung des Parlaments nahm allerdings auch die Geschlossenheit der Fraktionen zu. So stimmten die Abgeordneten der größeren Fraktionen in der Legislaturperiode 2004–2009 in rund 90 % aller Entscheidungen im Sinne ihrer Fraktion. Die europaskeptischen Fraktionen (UEN und Ind/Dem) hatten hingegen deutlich niedrigere Kohäsionsraten (76 % bzw. 47 %).

Ursprünglich wurden die Mitglieder des Europäischen Parlaments durch ihre jeweiligen Herkunftsstaaten bezahlt. In der Regel erhielten die europäischen Parlamentarier dieselben Vergütungen wie ihre Pendants auf nationaler Ebene. Dies führte dazu, dass im Jahr 2004, am Ende der 5. Wahlperiode, ein spanischer Europaparlamentarier 2600 Euro erhielt, während ein Italiener in der gleichen Position mit 11.000 Euro das Vierfache erhielt. Noch größer waren die Unterschiede, wenn man die Saläre der ab Mitte 2004 die zehn neuen osteuropäischen EU-Mitglieder vertretenden Parlamentarier betrachtet: Ihre monatlichen Vergütungen betrugen teilweise nur 800 Euro. Dies ist insofern problematisch, als alle Europaparlamentarier viel Zeit in Brüssel und Strassburg verbringen und entsprechend dem Preisniveau in diesen Städten ähnlich hohe Ausgaben haben. Osteuropäische Abgeordnete sollen deshalb sogar in ihren Büros im Parlamentsgebäude geschlafen haben.

Für die 6. Wahlperiode wurde deshalb vorgeschlagen, dass alle Abgeordneten ein einheitliches Salär von 8.600 Euro monatlich erhalten sollten. Da dies teilweise drastische Erhöhungen bedeutet hätte und osteuropäische Abgeordnete bis zum dreifachen ihrer nationalen Regierungschefs verdient hätten, bildete sich eine starke Opposition, und der Plan scheiterte schließlich. Für die 7. Wahlperiode ab Juli 2009 einigte man sich schließlich auf eine einheitliche Entlohnung der Abgeordneten, jedoch auf einem deutlich niedrigeren Niveau, als dies fünf Jahre zuvor vorgesehen war. Die Einigung, die 2005 zustande kam, sah ursprünglich ein Grundgehalt von rund 7.000 Euro und eine Spesenpauschale von 3.785 Euro vor. Bis zum Inkrafttreten Mitte 2009 erhöhte sich diese Zahl noch, da die Diäten von den Gehältern der Richter am EuGH abhängig sind. Zudem wurde das viel kritisierte Spesenvergütungssystem geändert, welches in der Vergangenheit teilweise zu überhöhten Spesenforderungen geführt hatte. Insbesondere dürfen keine Verwandten mehr als Mitarbeiter beschäftigt werden, und für die Erstattung von Reisekosten muss nun ein Beleg vorgelegt werden, da teilweise Spesen für nicht durchgeführte Reisen erhoben wurden oder der Tarif für teurere Verkehrsmittel beansprucht wurde, obwohl man mit einem günstigeren Verkehrsmittel unterwegs war.

Ein Europaparlamentarier hat gegenwärtig (Stand 2014) die folgenden Ansprüche:

Der (private, aber zu zwei Drittel von öffentlichen Geldern finanzierte) Pensionsfonds der Abgeordneten des Parlamentes geriet im Zuge der Finanzkrise ab 2007 durch Fehlspekulationen in ein Defizit von etwa 120 Millionen Euro. Nachdem der private Fonds bereits zweimal durch Steuergelder gerettet wurde, votierte das Europäische Parlament 2009 bei der dritten Rettung und größerer Medienaufmerksamkeit mit einer knappen Mehrheit gegen weitere staatliche Hilfen. Die Entscheidung des Parlaments für mehr Transparenz im Fonds wurde vom damaligen Präsidenten Hans-Gert Pöttering (CDU) jedoch verhindert.

Um Interessenskonflikte zu vermeiden, legt Artikel 7 des Direktwahlakts bestimmte Funktionen fest, die die Mitglieder des Europäischen Parlaments nicht ausüben dürfen. Jeder Mitgliedstaat kann zudem weitere Unvereinbarkeiten für die in ihm gewählten Parlamentarier festlegen. Die europaweiten Unvereinbarkeiten umfassen folgende Ämter:

Wenn jemand, der eines dieser Ämter innehat, in das Europäische Parlament gewählt wird, so muss er das Amt vor Aufnahme des Mandats im Parlament abgeben. Eine Sonderregelung gilt für Mitglieder des irischen Parlaments, die bei der Europawahl einen Sitz gewinnen. Diese dürfen bis zur nächsten irischen Parlamentswahl ein Doppelmandat ausüben, jedoch nicht darüber hinaus.

Wenn umgekehrt ein Europaabgeordneter eines der genannten Ämter neu aufnehmen will, muss er zuvor auf sein Mandat im Europäischen Parlament verzichten. Dieses geht an einen Nachrücker auf der Wahlliste über, für die er in das Parlament gewählt wurde. Da es insbesondere nicht selten vorkommt, dass Europaabgeordnete in die nationalen Parlamente oder Regierungen ihres jeweiligen Staates gewählt werden, gibt es im Vergleich zu den meisten nationalen Parlamenten im Europäischen Parlament eine recht hohe Mitgliederfluktuation.

Die Beitrittskandidaten der Europäischen Union entsenden jeweils eine Anzahl von Beobachtern in das Europäische Parlament, die üblicherweise vom nationalen Parlament ernannt werden. Diese Beobachter können bei den Debatten des Parlaments anwesend sein und auf Einladung auch selbst das Wort ergreifen, sie können jedoch nicht an Abstimmungen teilnehmen oder andere offizielle Aufgaben der Parlamentarier wahrnehmen. Nach dem Beitritt erhalten diese Beobachter für eine Übergangszeit den vollen Parlamentarierstatus, bis die nächsten Europawahlen stattfinden oder das Land Nachwahlen organisiert.

Nach der Europawahl 2009 gab es zudem eine Anzahl Beobachter aus denjenigen Staaten, denen nach der im Vertrag von Lissabon vorgesehenen Erweiterung des Parlaments zusätzliche Abgeordnete zustanden. Aufgrund verschiedener rechtlicher Schwierigkeiten wurden diese auch nach Inkrafttreten des Vertrages zunächst keine vollwertigen Abgeordneten (siehe "Liste der Mitglieder des 7. Europäischen Parlamentes#Zusätzliche Mitglieder nach Inkrafttreten des Vertrags von Lissabon").



</doc>
<doc id="12709" url="https://de.wikipedia.org/wiki?curid=12709" title="Marschall">
Marschall

Der Marschall, auch Feldmarschall, ist heute der höchste militärische Dienstgrad. Symbol des Ranges war in Deutschland der Marschallsstab, der formal mitverliehen wurde; der Ausdruck kann aber auch ein zeremonielles Hofamt bezeichnen. Dem Rang des Feldmarschalls entsprach in der Marine jener des "Großadmirals".

Das Wort stammt von Althochdeutsch ', zusammengesetzt aus "," „Pferd, Mähre“ und ' „Knecht, Diener“. Es bezeichnet ursprünglich den „Pferdeknecht (Roßknecht)“. Mit dem Titel seines Herrn steigt – wie auch bei Mundschenk „Tafeldiener“, Kämmerer „Kammerdiener“ – auch seine Bedeutung, zu „Stallmeister“ ("Marstaller") und später zu allgemeiner Bedeutung im Sinne „Kommandeur der Reiterei“.

Der Marschall war im Mittelalter schon eines der vier bzw. fünf alten Hofämter. Aus der Oberaufsicht über die Pferde und damit über das berittene Gefolge entstand einerseits mit dem Aufkommen der Ritterheere der Oberbefehl des Marschalls im Kriege und die Führung der Ritterschaft bzw. der Landstände, andererseits eine Oberaufsicht über das gesamte Hofwesen, was endlich dazu führte, dass der Marschall die Obliegenheiten des Truchsess also Küchenmeisters und des Mundschenks übernahm.

Vor allem war er auch Reisemarschall und hatte für die Gäste zu sorgen. In den meisten deutschen Territorien wurden im späteren Mittelalter diese Funktionen auf verschiedene Beamte verteilt:


Im Heiligen Römischen Reich Deutscher Nation gehörte das Amt des "Erzmarschalls" zu den vier Erzämtern, die mit der weltlichen Kurfürstenwürde verbunden waren. So war der Kurfürst von Sachsen der Erzmarschall des römisch-deutschen Kaisers. Der Titel Erzmarschall wurde durch die Goldene Bulle 1356 als Kurwürde an die Herzöge von Sachsen verliehen. Die Erzämter waren reine Ehrentitel. Die mit den Ämtern verbundenen, tatsächlichen Aufgaben nahmen stellvertretend für die Kurfürsten die Inhaber der sogenannten Reichserbämter wahr. Der Reichserbmarschall war zuständig für das Tragen des Reichsschwertes am Königshof.

Johann Wolfgang Goethe, der am 3. April 1764 Augenzeuge der Krönung Josephs II. zum römisch-deutschen König in Frankfurt war, erwähnt dazu:

Später wurde der Titel eines Marschalls fast nur noch als Ehrentitel für verschiedene Gelegenheiten verliehen. Gutes Beispiel hierfür ist Prinz Philip, der Mann der britischen Königin Elisabeth II. Er ist – ehrenhalber – Marschall der , Feldmarschall der , Feldmarschall der und Marschall der . Mit diesem nicht unbedingt militärischen Titel wurden aber auch erfolgreiche militärische Führer für einen erfolgreich abgeschlossenen, selbständigen Feldzug geehrt. Am bekanntesten hierbei sind der Marschall von Frankreich und die Verleihung des Dienstgrades Reichsmarschall an Hermann Göring durch Adolf Hitler (aus Prestigegründen, nicht aufgrund militärischer Leistungen). Weitere Beispiele sind der folgenden Liste zu entnehmen.

Im Laufe der Zeit hat sich der Titel des Marschalls von einem höfischen Amtstitel hin zu einem militärischen und Ehrentitel entwickelt, dessen Vergabe kaum mehr mit dessen ursprünglichen Inhalt zu tun hat. Die folgende Übersicht gibt einen Überblick darüber, wie dieser Titel in verschiedenen Ländern der Welt verwendet wird und wurde.

In Brasilien war Marschall Manuel Deodoro da Fonseca 1889 zum Gründer der Republik geworden, sein Nachfolger wurde Marschall Floriano Vieira Peixoto (* 1839, † 1895). Weitere Marschälle waren Verteidigungsminister Odílio Denys, die Präsidenten Eurico Gaspar Dutra und Humberto Castelo Branco (1964–67) und der Oberbefehlshaber des Brasilianischen Expeditionskorps in Europa João Baptista Mascarenhas de Morais.

Der Field Marshal war seit 1736 ein Ehrenrang für die verdientesten Generale der britischen und der Commonwealth-Armee, gelegentlich wurde er an ausländische Monarchen sowie an Mitglieder der königlichen Familie ehrenhalber verliehen. Von Anfang bis kurz vor Ende des 20. Jahrhunderts war er außerdem in Krieg und Frieden fest mit dem Amt Chef des Imperialen Generalstabes bzw. Chief of the Defence Staff verbunden. Nach Ausscheiden von Field Marshal Baron Peter Inge 1997 wurde der Titel seinem Nachfolger nicht mehr verliehen. Der Rang steht aber weiterhin zur Disposition. <br> Bekannte britische Feldmarschälle (Jahr der Ernennung) waren:


In Großbritannien, Australien und Neuseeland wird die Bezeichnung "Air Marshal" in den Luftstreitkräften anstelle der Bezeichnung "General" verwendet („Luftmarschall“). Die vier höchsten Dienstgrade der Royal Air Force lauten (wobei der erste und höchste in Friedenszeiten abgeschafft wurde):


Außerhalb des Militärs waren Earl Marshal von England und Earl Marischal von Schottland erbliche Hofämter mit zeremonieller Funktion am englischen und schottischen Königshof.

Rang als Generalfeldmarschall:


Weitere Einzelheiten siehe: Reichsmarschall und Generalfeldmarschall

Der Reichs-Erzmarschall "(Archimarescallus)" sowie der Reichs-Erbmarschall "(Vicemarescallus)", Erzamt seit dem 12. Jahrhundert, außerdem der Reichs-General-Feldmarschall seit dem 17. Jahrhundert

Der letzte Feldmarschall der Niederlande war von 1840 bis 1881 Prinz Frederik, der zweite Sohn von König Wilhelm I.

Im Kaiserreich Österreich (bis 1867) wurde die Bezeichnung k.k. Feldmarschall und in Österreich-Ungarn (bis 1918) k.u.k. Feldmarschall verwendet.

Weitere Informationen siehe: Generalfeldmarschall


Am 4. Juni 1942 wurde Carl Gustaf Emil Mannerheim der Ehrentitel des Marschalls von Finnland zu seinem 75. Geburtstag verliehen. Zuvor war er am 19. Mai 1933 zum Feldmarschall ernannt worden. Dies erfolgte im Zusammenhang mit dem 15. Erinnerungsjahr der Beendigung des Krieges für die Unabhängigkeit von Sowjetrussland im Jahre 1917.

Der Rang Marschall von Frankreich, frz. "," wurde um 1190 von Philipp II. für Albéric Clement geschaffen. Unter anderem trugen diesen Titel Ferdinand Foch und Henri Philippe Pétain.

Angesichts zahlreicher Militärputsche in der Geschichte des Irak seit den 1930er Jahren und rivalisierender Autoritätsansprüche erschien es Staatspräsident Aref bei seinem Machtantritt notwendig, sich durch einen übergeordneten Dienstgrad der Loyalität des Militärs zu versichern. Er selbst hatte zusammen mit General Abd al-Karim Qasim 1958 geputscht, war von diesem zum Oberst ernannt worden und hatte dann aber als solcher zunächst vergeblich gegen Qasim geputscht.

1963 gelang schließlich der Sturz Qasims mit Hilfe anderer Militärs, z. B. des baathistischen Majors Ahmad Hasan al-Bakr, der nun ebenfalls zum Obersten bzw. General aufrückte. Arefs Marschallsrang sollte den Vorrang vor beiden herausstellen und wurde in dieser Tradition auch von Bakr übernommen, als er 1968 Aref stürzte. Diese Erhöhungen wurden von der irakischen Armee anerkannt, da beide Amtsinhaber tatsächlich die Militärakademie besucht hatten. Die Selbsternennung von Bakrs Nachfolger und Cousin Saddam Hussein, der keine Offiziersausbildung besaß, jedoch stieß auf Unmut in der Armee. Saddam Hussein ernannte deshalb seinen Cousin, den Offizier bzw. General Ali Hasan al-Madschid ebenfalls zum Feldmarschall und somit zum zweithöchsten Militär des Irak nach sich selbst als Oberbefehlshaber. Seit dem Sturz Saddams 2003 gibt es keinen Marschall des Irak mehr.

Der Rang Marschall von Italien, ital. "," wurde im faschistischen Italien seit 1924 verwendet. Es gab auch den Rang eines ' (Luftmarschall) und den des ' (Großadmiral).

Diese höchsten Ränge wurden in der italienischen Armee 1946 abgeschafft. Auch der Dienstgrad Armeegeneral (OF-9) wurde gestrichen. Seine Funktionen übernahm bis 1997 der so genannte „Generalleutnant in besonderer Dienststellung“. Heute ist der höchste Dienstgrad in Italien der General bzw. Admiral (OF-9). Diese Dienstgradebene ist jedoch ausschließlich dem Generalstabschef der Streitkräfte vorbehalten.

Ein "Maresciallo" hingegen ist ein Dienstgrad im Range eines Unteroffiziers (OR-7).

Bekannte Träger des Ranges Marschalls waren:

Jugoslawien kannte nur einen einzigen „Marschall von Jugoslawien“: Josip Broz Tito. Am 29. November 1943 wurde Tito während der zweiten Versammlung des Antifaschistischen Rates der Volksbefreiung Jugoslawiens (AVNOJ - Antifasisticko Vijece Narodnog Oslobođenja Jugoslavije) in Jajce, die Marschallwürde verliehen. Damit wurde er der höchstrangige Offizier, der während des Zweiten Weltkrieges verwundet worden war, da er persönlich an Schlachten und Kämpfen teilgenommen hatte (siehe Schlacht von Neretva und Sutjeska). Die Alliierten erkannten schon 1944 seinen Rang an, den der neu gegründete jugoslawische Staat, dem Tito selbst vorstand, schließlich bestätigte. Nach seinem Tod 1980 wurde der Rang faktisch abgeschafft, Jugoslawien brach nach 1991 auseinander.

In Nordkorea gibt es zwei Kategorien des Marschallsranges: „Marschall der Demokratischen Volksrepublik Korea“ und „Marschall der Koreanischen Volksarmee“. Der erste Marschallstitel wurde bisher lediglich von den Oberbefehlshabern der nordkoreanischen Streitkräfte geführt und soll deren militärische Fähigkeiten hervorheben; letzterer Marschallstitel dient der Auszeichnung verdienter Militärführer. 

Der Titel „Marschall der Demokratischen Volksrepublik Korea“ wurde am 7. Februar 1953 Kim Il-sung, am 20. April 1992 Kim Jong-il und am 18. Juli 2012 Kim Jong-un verliehen. Der Titel „Marschall der Koreanischen Volksarmee“ wurde 1992 an O Jin-u und 1995 an Choe Kwang und Ri Ul-sol vergeben.

Der einzige noch lebenden Marschall ist Kim Jong-un (Stand: 27. Oktober 2017).


Diesen Titel trug Józef Klemens Piłsudski seit dem Jahre 1920 in Ausübung seines Amtes als Staatspräsident. Bereits Józef Antoni Poniatowski war 1812 Marschall, allerdings „Marschall von Frankreich“ unter Napoleon, geworden. Mit Ferdinand Foch wurde 1919 ein weiterer französischer Marschall auch Marschall von Polen. Der Rang existiert noch heute in den polnischen Streitkräften, ist aber zurzeit nicht besetzt und nur für den Kriegsfall vorgesehen. Auch während der Zeit der Nordischen Kriege gab es in Polen Heerführer mit dem Titel Marschall. 

Weitere polnische Marschälle:
Den Titel „Marschall von Polen“ trugen auch in den Zeiten des Kommunismus Marian Spychalski und Michał Rola-Żymierski, amtierend 1955 bis 1968.

Neben dem militärischen Titel besteht in Polen die Verwendung des Titels "Marschall" im politischen bzw. administrativen Bereich fort. So tragen die Präsidenten der Abgeordnetenkammer und des Senats die Bezeichnungen „Sejmmarschall“ und „Senatsmarschall“. Ebenso heißt der Präsident eines Bezirksparlaments „Woiwodschaftssejmikmarschall“ und der Vorsitzende einer Woiwodschaft „Woiwodschaftsmarschall“.

Armee und Luftwaffe Portugals kennen einen Marschallsrang bis heute, er entspricht dort aber mit vier Sternen nur dem Armee- bzw. Luftgeneral.

In Rumänien führte Ion Antonescu den Marschalltitel von 1940 bis 1944.

In Schweden wurde von 1561 bis 1824 der Feldmarschalltitel (fältmarskalk) als höchster militärischer Rang 77 Mal verliehen.

Der Titel Marschall wurde bereits im zaristischen Russland verwendet. In den dreißiger Jahren wurden fünf Generäle des Bürgerkrieges von Josef Stalin zum Marschall der Sowjetunion ernannt, von denen jedoch drei im Rahmen des Großen Terrors 1937/38 hingerichtet wurden. 1943 wurden die Dienstgrade Marschall und Hauptmarschall der Waffengattung, also z. B. Marschall der Panzertruppen, Hauptmarschall der Luftstreitkräfte, eingeführt.

Allgemeine Truppenkommandeure wie Armee- und Frontoberbefehlshaber, der Generalstabschef und seine Stellvertreter, der Verteidigungsminister und seine Stellvertreter wurden vom Generaloberst zum Armeegeneral und danach zum Marschall der Sowjetunion befördert. Kommandeure von Spezialarmeen, wie z. B. Panzer- und Luftarmeen, Chefs von Waffengattungen in Fronten (Militärbezirken bzw. Heeresgruppen) oder im Verteidigungsministerium wurden vom Generaloberst zum Marschall und Hauptmarschall der Waffengattung befördert. Der Marschall der Waffengattung war also dem Armeegeneral gleichgestellt.

Oberhalb des Marschalls der Sowjetunion wurde nach dem Zweiten Weltkrieg für Stalin als Obersten Befehlshaber der Rang des Generalissimus geschaffen.


Auch das Sultanat und Kalifat der osmanischen Türken kannte einen Dienstgrad oberhalb der Befehlshaber selbständiger Armeen (Armeegeneräle) für einen Oberfeldherrn mit dem höchsten militärischen Rang. Dieser wurde jedoch zunächst nicht Marschall, sondern ' (heutige türk. Schreibweise: ' oder auch '; aus dem Arabischen, etwa: Berater, Geheimrat) genannt und entsprach etwa dem Feldmarschall bzw. Feldmarschallleutnant (Vizemarschall) oder Waffenmarschall. Im Ersten Weltkrieg gab es mehrere Muschire an den verschiedenen Fronten. Neben dem Militär gab es den Rang auch in der Zivilverwaltung, wo er etwa einem Vizeminister bzw. Staatsminister bzw. einem Mitglied des Staatsrates entsprach. Heute wird meist das aus den europäischen Sprachen übernommene Wort ' verwendet. 

Im Zuge der Liman-von-Sanders-Krise wurde der deutsche General Otto Liman von Sanders zum osmanischen Marschall ernannt.

Die Türkei hat bisher zwei Generäle mit dem Marschallrang ausgezeichnet:

In den USA ist der "" eine Behörde des Justizministeriums. Der militärischen Marschallsfunktion entspricht dort der und (Fünfsternegeneral), nicht zu verwechseln mit dem Armeegeneral (Viersternegeneral).

Die Koninklijke Marechaussee ist eine militärische Polizei in den Niederlanden.

In Italien ist der Maresciallo in verschiedenen Abstufungen ein Dienstgrad der Streitkräfte (einschließlich der mit Polizeiaufgaben betrauten Carabinieri) und der Finanzpolizei (guardia di financia). Er entspricht einem Unteroffizier oder einem Beamten des gehobenen Dienstes, ist also im Gegensatz zum Marschall in anderen Staaten ein Dienstgrad der mittleren Ebene.



</doc>
<doc id="12710" url="https://de.wikipedia.org/wiki?curid=12710" title="Reichsmarschall">
Reichsmarschall

Der Reichsmarschall – des Reiches Marschall (Erzmarschall) war ursprünglich der militärische Stellvertreter des Kaisers und bekleidete eines der Erzämter im Heiligen Römischen Reich. Im 17. und 18. Jahrhundert wurde der Titel eines Reichsgeneralfeldmarschalls durch Kaiser und Reichstag verliehen. Im Jahr 1938 wurde er im faschistischen Italien und 1940 im Deutschen Reich unter dem Nationalsozialismus für eine sowohl militärische als auch politische Oberbefehlshaberposition neu eingeführt.

Reichsmarschall entspricht in etwa dem Titel eines Generalissimus. Heute ist der Rang eines Reichsmarschalls nur in Schweden für den Chef des königlichen Hofstaates gebräuchlich.
Das Amt des Marschalls war seit Otto I. eines der Erzämter und später mit der Kurwürde verbunden. In der Goldenen Bulle wird der Kurfürst von Sachsen als "Reichs-Erzmarschall" (Archimarescallus) benannt. Er war bei zeremoniellen Gelegenheiten der Träger des Reichsschwertes. Sein Amtszeichen waren zwei gekreuzte rote Schwerter, die er im Wappen führte (siehe Reichsrennfahne). Die Stellvertretung war erblich als "Reichserbmarschall (Vicemarescallus)" an die Grafen von Pappenheim gebunden. Waren beide Ämter im Hochmittelalter noch mit realer Funktion als Befehlshaber des gesamten Reichsaufgebots versehen, so hatten sie spätestens seit der Renaissance lediglich zeremonielle Aufgaben bei Kaiserkrönungen und Reichstagen zu erfüllen.

Als "Generalissimus" wurde im Dreißigjährigen Krieg Albrecht von Wallenstein als oberster General des Kaisers bezeichnet. In den Wahlkapitulationen der Kaiser des 17. Jahrhunderts sollte das Reichsheer, eine Kontingentarmee des Reiches neben den Armeen der Fürstentümer mit zum Teil eigenen Feldmarschallen, von einem "Reichs-General-Feldmarschall" befehligt werden. Die Entscheidung, wem diese Ernennung zustehen sollte, lag beim Kaiser und dem Reichstag. Es wurden stets zwei Reichsgeneralfeldmarschalle bestellt, je einer aus dem katholischen und einer aus dem evangelischen Lager. 1734 wurden auf Zeit vier Reichsgeneralfeldmarschalle ernannt.
In Dänemark war der Reichsmarschall (dän. "Rigsmarsk") bis 1660 das nach dem Reichskanzler zweitwichtigste Reichsamt im dänischen Reichsrat. Frederik III. ersetzte 1660/61 das bestehende Wahlkönigtum zugunsten einer Erbmonarchie und schaffte das Amt des Rigsmarsk ab. Letzter Träger des Amtes war Joachim von Gersdorff.

In Schweden hat sich die Bedeutung des Reichsmarschalles "(Riksmarsk" und "Riksmarskalk)" im Laufe der Zeit geändert. Ursprünglich Chef der schwedischen Kavallerie, wie die Marschälle anderer Länder, entwickelte sich der "Riksmarsk" ab der Mitte des 13. Jahrhunderts zum militärischen Oberbefehlshaber. Bedeutende Reichsmarschälle waren unter anderen Torgils Knutsson und Karl Knutsson.

Unter den Wasa-Königen wurde der Titel "Riksmarsk" als militärischer Titel vergeben. In der Verfassung von 1634 wurden zwei Ämter eingerichtet, das des "Riksmarsk," der Präsident des Kriegskollegiums war, und das des "Riksmarskalk," der als Chef des schwedischen Hofstaates im Reichsrat vertreten war. Nach dem Tod des Reichsmarschalls Carl Gustav Wrangel 1676 wurde das Amt des "Riksmarsk" nicht nachbesetzt. 1680 änderte man den Titel "Riksmarskalk" in "Överstemarskalk (Oberstmarschall"), aber wechselte 1722 wieder zurück zu Reichsmarschall.

Auch heute noch ist der Reichsmarschall der Chef des schwedischen Hofstaates. Als solcher nimmt er an der festlichen Eröffnung eines neuen Reichstages teil. Bis 1982 führte der Reichsmarschall das Prädikat Exzellenz.

Nach dem Sieg über Äthiopien führte Benito Mussolini als Oberbefehlshaber über die Italienischen Streitkräfte und "Capo del governo" am 30. März 1938 für sich die Würde eines "Ersten Reichsmarschalls (Primo maresciallo dell'Impero)" ein, den er notgedrungen auch dem König Viktor Emanuel III. als Staatsoberhaupt antragen musste. Der über den im Jahre 1924 eingeführten "Marschällen von Italien" (Maresciallo d’Italia) stehende Rang wurde mit diesen 1946 wieder abgeschafft.

Am 19. Juli 1940 wurde der Dienstgrad „Reichsmarschall des Großdeutschen Reiches“ in der Wehrmacht, oberhalb des Generalfeldmarschalls, geschaffen. Er diente ausschließlich der Hervorhebung der führenden Position Hermann Görings als ranghöchster deutscher Soldat (und zweiter Mann im Staate). Göring war der einzige, der den Titel erhielt.

Am gleichen Tag ernannte Hitler zwölf Generäle zum Generalfeldmarschall.

Der Rang des Reichsmarschalls war im Falle Görings nur formaler Natur. Eine Kommandobefugnis über die Wehrmacht im Gesamten war damit nicht verbunden. (Siehe auch: NS-Ranggefüge, Oberkommando der Wehrmacht)

Die Schulterstücke der Uniform unterschieden sich von denen der Generalfeldmarschälle mit gekreuzten Marschallstäben durch einen Reichsadler, der einen Kranz mit den Marschallstäben in den Fängen hielt.

Göring erhielt einen besonderen Marschallstab aus Elfenbein, Gold und Brillanten. Der Marschallstab weicht von denen des Heeres ab und war denen der Luftwaffe ähnlicher. Neben dem Eisernen Kreuz war auf diesem Stab das Balkenkreuz der Luftwaffe aufgelegt.

Außerdem erhielt Göring einen Interimsstab, dessen Spitze aus Elfenbein war.
Dem Reichsmarschall entspricht seit dem 11. Jahrhundert in Bedeutung und Aufgabe der Connétable von Frankreich. Der "comes stabuli" (Stallgefährte) ist die mittellateinische Entsprechung des deutschen "marahscalc" (Marschall – Pferdeknecht). Er war ab dem 14. Jahrhundert der Oberbefehlshaber der Armee (oberster Kronfeldherr), dem ab 1190 die Marschälle von Frankreich als Stellvertreter dienten. In der Folgezeit war es eine der mächtigsten Funktionen in Frankreich. Das Amt des "Connétable" wurde von Richelieu schließlich 1624/1627 abgeschafft, weil sich die Machtfülle nicht mit der Ideologie des Absolutismus vereinen ließ. Seine Befugnisse gingen zeitweilig auf das im späten 16. Jahrhundert neu geschaffene Amt des "General-Marschalls (maréchal général des camps et armées du roi)" über.

Fälschlich werden auch die von Napoleon zu "Marschällen des Kaiserreiches (Maréchaux d’Empire)" ernannten Marschälle von Frankreich oder Großmarschälle seines Palastes als Reichsmarschälle bezeichnet.

Der Dai-Gensui, der 1871 erstmals ernannt worden ist, entspricht dem Reichsmarschall.

Zur Zweihundertjahrfeier wurde am 11. Oktober 1976 als höchster Dienstgrad der General of the Armies of the United States (mit 6 Sternen) geschaffen, der über dem mit einem Feldmarschall vergleichbaren General of the Army (5 Sterne) steht. Postum wurde nur George Washington diese Würde „für die Vergangenheit und Gegenwart“ verliehen. Hierbei wurde bestimmt, dass kein US-Offizier jemals einen höheren Rang als Washington bekleiden kann.

Den Titel führte General John J. Pershing nach dem Ersten Weltkrieg in Anerkennung seiner Leistung als Oberbefehlshaber der American Expeditionary Forces in Europa. Als Rangabzeichen trug er vier Sterne, jedoch in Gold statt wie üblich in Silber. Allerdings entsprach der Rang damals wohl eher dem des Feldmarschalls, da es den 5-Sterne-General noch nicht gab.

Bisher ist keine weitere Persönlichkeit mit diesem Rang ausgezeichnet worden.




</doc>
<doc id="12712" url="https://de.wikipedia.org/wiki?curid=12712" title="Kompass">
Kompass

Der Kompass (von italienisch "compasso" „Zirkel, Magnetnadel“, abgeleitet von "compassare" „abschreiten“, Plural: "Kompasse") ist ein Instrument zur Bestimmung einer fest vorgegebenen Richtung, z. B. Himmelsrichtung, Navigations-Kurs, Peilrichtung. Ursprünglich ergänzte der Kompass in der Schifffahrt andere Methoden der Navigation, zum Beispiel anhand von Sonne, Sternen und Landmarken, Strömungen, Wellengang und Wassertiefe. Die älteste Ausführung des Kompasses ist die Kimme, die das Anpeilen des Polarsterns bei klarer Nacht erlaubt.

Das klassische Gerät ist der Magnetkompass, der anhand des Erdmagnetfelds die Bestimmung der magnetischen Nordrichtung und daraus aller anderen Himmelsrichtungen erlaubt. Andere Ausführungen sind elektronische Kompasse auf Basis von Hall-Sensoren oder Fluxgate-Magnetometern; mit Letzteren kann Betrag und Richtung des Erdmagnetfeldes auf ein 1/100.000 des Absolutwerts genau bestimmt werden.

Ganz ohne Ausnutzung des Erdmagnetfeldes arbeiten Kreiselkompasse, deren Wirkungsweise auf der Erdrotation beruht. Die Richtungsmessung erfolgt bezüglich der geografischen Nord-Süd-Richtung anstatt zu den Magnetpolen. Es gibt auch Kreiselinstrumente ohne Richtungsbezug (freie Kreisel wie den Kurskreisel), die allerdings periodisch nachgestellt werden müssen. Ebenfalls ohne Magnetfeld kommen Sonnenkompasse aus.

Ein Kompass mit Peilvorrichtung wird auch Bussole genannt. Meist wird dieser Begriff in der Vermessungstechnik für Präzisions-Peilkompasse verwendet, vor allem in Österreich und Italien wird aber auch der einfache Wander- oder Marschkompass so genannt.

Die Erkenntnis, dass sich Splitter von Magneteisenstein in die Nord-Süd-Richtung drehen, war in Europa seit der griechischen Antike und in China seit der Zeit der Streitenden Reiche, zwischen 475 v. Chr. und 221 v. Chr. bekannt.

Die seriösen Studien zum Ursprung des Kompasses von J. Klaproth und L. de Saussure führen zu dem Ergebnis, dass die chinesischen Navigatoren den nassen Kompass bereits um die Jahrtausendwende kannten. Die Chinesen benutzten seit dem 11. Jahrhundert eine schwimmende, nasse Kompassnadel, die Südweiser genannt wurde. Tatsächlich ist auf dem "chinesischen Kompass" die Südrichtung als Hauptrichtung markiert. Im Laufe der Zeit entwickelten sich daraus spezielle Kompassformen mit einer Einteilung in 24, 32, 48 oder 64 Striche bzw. Himmelsrichtungen (siehe Erdzweige). Ende des 11. Jahrhunderts empfahl Shen Kuo (1031–1095) in seinem Hauptwerk einen Kompass mit Einteilung in 24 Richtungen; kurz nach seinem Tod waren solche Kompasse tatsächlich in Gebrauch.

Die Matrosen des östlichen Mittelmeeres haben spätestens zur Zeit der Kreuzzüge vom nassen Kompass erfahren und ihn optimiert. Da er seinem Besitzer jedoch einerseits große Vorteile gegenüber der Konkurrenz brachte und andererseits quasi mit verbotenen magischen Kräften funktionierte, wurde dieses Wissen möglichst geheim gehalten. In Europa beschrieb der englische Gelehrte Alexander Neckam 1187 den nassen Kompass als eine magnetisierte schwimmende Nadel, die unter Seeleuten in Gebrauch war. Auch in einer kirchenkritischen Schrift des französischen Mönches Hugues de Bercy wurde die schwimmende Magnetnadel um 1190 (vielleicht auch bereits vor 1187) erwähnt.

Auf der Arabischen Halbinsel wurde der Kompass wahrscheinlich nicht erfunden, da die arabischen Seeleute um die Jahrtausendwende über gute astronomische Kenntnisse verfügten und dank der gleichmäßigen Winde in ihrer Weltregion gut navigieren konnten. Im arabischen Raum lässt sich der nasse Kompass etwa einhundert Jahre nach Alexander Neckams Erwähnung nachweisen.

1932 veröffentlichte Edmund Oskar von Lippmann eine Studie, in der er versuchte, die angebliche Überlegenheit der „nordischen Rasse“ zu beweisen, indem er Argumente für eine hypothetische, unabhängige Erfindung des Kompasses in Europa beizubringen suchte, ohne auf alle anderen früheren Untersuchungen einzugehen. Diese falsche Theorie wird heute teilweise immer noch vertreten.

Die erste schriftliche Erwähnung einer "trocken" auf einem Stift spielenden Magnetnadel findet sich im "Epistola de magnete" von 1269, geschrieben von Petrus Peregrinus de Maricourt, womit der noch heute benutzte "trockene Kompass" erfunden war. Ein Seefahrer namens Flavio Gioia, dessen Existenz nicht gesichert ist, wird am Hafen von Amalfi als angeblicher „Erfinder des Kompasses“ mit einem Denkmal geehrt. Die Legende um Flavio Gioia beruht wahrscheinlich auf einem Übersetzungsfehler.

Der trockene Kompass war sehr viel genauer als die instabile schwimmende Nadel und ermöglichte so eine präzisere Navigation. Im späten 13. Jahrhundert kombinierten die Seefahrer des Mittelmeers als erste die Magnetnadel mit der Windrose.

"Zu den vielfachen Bedeutungen oder Verwendungen von „Windrosen“ siehe Windrose (Begriffsklärung)".

Um das Jahr 1400 bauten europäische Seefahrer die trockene Kompassnadel und Windrose in ein festes Gehäuse ein, um es fest auf ihren Schiffen zu stationieren. Leonardo da Vinci schlug als erster vor, den Kompasskasten in einer kardanischen Aufhängung zu platzieren, um so die Genauigkeit weiter zu steigern. Ab 1534 wurde seine Idee verwirklicht und setzte sich während des 16. Jahrhunderts in ganz Europa durch, wodurch europäische Segelschiffe über die fortschrittlichste und exakteste Kompasstechnik ihrer Zeit verfügten. Nach China kam der trockene Kompass etwa um das Jahr 1600 über Japan, das ihn von Spaniern und Portugiesen übernommen hatte.

Der Kompass wurde auch im Bergbau von Markscheidern als Vermessungsinstrument eingesetzt. In der norditalienischen Bergstadt Massa sind Kompasse zur Bestimmung der Vortriebsrichtung und Vermeidung von Durchschlägen zwischen Grubenbetrieben bereits im 13. und 14. Jahrhundert belegt, und im Tiroler Bergbau war er in der zweiten Hälfte des 15. Jahrhunderts selbstverständlich. Das "Bergbüchlein" des deutschen Montanwissenschaftlers Ulrich Rülein von Calw (1505) kann als eine erste theoretische Abhandlung über den untertägigen Einsatz des Kompasses gelten.

Der Magnetkompass besteht aus einem drehbaren Zeiger aus ferromagnetischem Material und einem Gehäuse, in dem dieser Zeiger möglichst reibungsarm gelagert ist. Als Träger der Magnetnadel werden dazu z. B. abriebsichere Edelsteine wie Rubin oder Saphir verwendet. Am Gehäuse oder dem Zeiger ist in der Regel eine Winkelskala angebracht. Der Zeiger selbst kann die traditionelle Form einer Nadel haben. In einigen neueren Kompassen ist eine komplette Scheibe zu finden und in Schiffskompassen meist eine Kugel.

Der Zeiger richtet sich, wenn er nach allen Richtungen frei beweglich ist, in Richtung des Erdmagnetfelds aus. Dessen Feldlinien verlaufen in weiten Bereichen der Erde und insbesondere in Mitteleuropa ungefähr in geographischer Nord-Süd-Richtung. Da die Abweichung sehr genau bekannt ist und teilweise in topografischen Karten verzeichnet ist, kann aus der Richtung des Zeigers relativ präzise auf die geografische Nordrichtung geschlossen werden.

Kompasskapseln sind in der Regel mit einer Flüssigkeit gefüllt, um die Bewegung der Nadel zu dämpfen. Dadurch vibriert sie bei Erschütterungen weniger, was das Ablesen erleichtert und Ablesefehler verringert, ohne dass dadurch das rasche Einschwingen erschwert wird. Die Flüssigkeit besteht oft aus einem leichten Öl oder einem Lösungsmittel, das nicht zum Rosten der Nadel führt und unter extremen Bedingungen nicht stockt.

Trotz der Existenz von Positionsbestimmungssystem wie GPS oder GLONASS wird der Magnetkompass nach wie vor oft genutzt. Navigationssysteme können die Navigation anhand der Sterne mit Karte und Kompass sinnvoll ergänzen, jedoch nicht vollständig ersetzen. Neben der Abhängigkeit von einer Energieversorgung und Elektronik ist die Kurswinkelbestimmung mit einem Kompass einfacher und genauer durchzuführen als mit einem Navigationsgerät, sofern dieses über keinen elektronischen magnetischen Kompass verfügt. Darüber hinaus erzwingt ein Kompass die ständige Auseinandersetzung mit der realen Situation, während das Navigationsgerät leicht dazu verführen kann, sich blindlings auf die Satellitentechnik zu verlassen.

Unter Wasser ist der Kompass, kombiniert mit der Zeit- und Geschwindigkeitsmessung, oft die einzige Möglichkeit der geografischen Positionsbestimmung. U-Boote und Taucher können ab einer gewissen Tauchtiefe weder die Sonne oder Sterne beobachten noch ein Navigationssystem zur Navigation nutzen. Sowohl das Licht der Sonne oder Sterne als auch die Hochfrequenzsignale der Navigationssatelliten werden vom Wasser stark absorbiert. Das Erdmagnetfeld hingegen durchdringt auch das Wasser. Auf größeren U-Booten wird die Navigation mit dem Magnet-Kompass meist durch Kreiselkompasse ergänzt.

Für die Navigation mit Karte und Kompass wird heute meist ein "Plattenkompass", auch Kartenkompass genannt, verwendet, dessen Gehäuse sich in einer durchsichtigen Acrylglas-Platte befindet oder ein Kartenwinkelmesser mit Planzeiger. Diese Platte erleichtert die Kartenarbeit und macht es einfach, die Nord-Süd-Linien des Kompasses mit dem Gitternetz einer topografischen Landkarte in Übereinstimmung zu bringen. Ein Einnorden der Karte ist nicht notwendig.

Immer wieder führt die Frage zu Verwirrung, ob im Norden der Erde der magnetische Nord- oder der magnetische Südpol liege. Ein Blick in die Geschichte hilft, den Sachverhalt zu verstehen. Als die magnetische Eigenschaft der Magnetit-Nadel entdeckt wurde, nannte man das Ende der Nadel, das nach Norden zeigte, naheliegenderweise den Nordpol der Nadel. Erst sehr viel später erkannte man den Grund des Effekts und dass sich bei Magneten immer gegensätzliche Pole anziehen. Da war die Bezeichnung der Polarität aber bereits definiert. Die Erde hat im geographischen Norden also einen magnetischen Südpol.

Zur Vermeidung dieser sprachlichen Ambivalenz werden in jüngerer Zeit auch die Termini „arktischer Magnetpol“ und „antarktischer Magnetpol“ verwendet.

Grundsätzlich werden Kompassrosen in gleich große Kreissektoren geteilt. Bei Grad sind das 360, bei Neugrad 400 und bei Strich bzw. Mil 6400 Teilbereiche, wobei meist nur die Hunderter eingezeichnet werden (Marschzahl). Der nautische Strich teilt den Kreis in 32 Segmente, wird aber heute in der Navigation nur noch selten benutzt, etwa bei der Vierstrichpeilung.

Da die Verbindungslinie der magnetischen Pole gegenüber der Erdachse um etwa 11,5° geneigt ist, liegen die magnetischen Pole derzeit etwa 2000 km von den geographischen Polen entfernt. Die magnetischen Pole verändern ihre Lage im Verlauf der Zeit, weil der Erdmagnetismus auf veränderlichen Strömungen im metallischen Erdkern beruht. Zusätzlich wird der Verlauf der magnetischen Feldlinien von den örtlichen geologischen Gegebenheiten (z. B. eisenhaltigem Gestein) beeinflusst. Diese beiden Faktoren bewirken, dass die Abweichung der Kompassnadel von der geografischen Nordrichtung an jedem Ort der Erde unterschiedlich ist. Diese Abweichung wird "Ortsmissweisung" oder "Deklination" genannt. Es ist nicht sicher, wer diese zuerst erkannte. Jedoch gilt als gesichert, dass Georg von Peuerbach der erste war, der über die Missweisung schrieb. Der älteste erhaltene Kompass, bei dem die Missweisung eingezeichnet ist, stammt von Peuerbach. Ein Kreiselkompass zeigt keine Deklination, da er unabhängig vom Erdmagnetfeld arbeitet.

Als "Deviation" werden Abweichungen bezeichnet, die durch magnetische Felder in der Nähe des Kompasses hervorgerufen werden können. Solche Felder werden durch magnetische oder magnetisierbare Gegenstände und Geräte sowie durch fließenden Wechsel- oder Gleichstrom erzeugt. Eine mögliche Gegenmaßnahme besteht darin, einen Mutterkompass an einer besser geeigneten Stelle als auf der Brücke oder im Cockpit des Schiffes oder Flugzeuges unterzubringen. Ansonsten oder zusätzlich werden zur Kompensation größerer Abweichungen entweder Magnetnadeln an dafür vorgesehenen Stellen in das Kompassgehäuse eingesetzt (z. B. Ludolph-Kompass) oder beweglich gelagerte Magnete im Kompassgehäuse über Stellschrauben entsprechend justiert (z. B. Airpath-Kompass). Um diese Kompensierung zu erreichen, wird das Fahrzeug (Schiff) langsam um die durch den Kompass führende Hochachse gedreht, die optische Peilung einer in ihrer geografischen Ausrichtung bekannten Linie beobachtet und mit der Kompassanzeige verglichen und die Abweichung notiert. Verbleibende Rest- und Anzeigefehler, die unter 5° liegen sollen, werden in eine Deviationstabelle eingetragen, von der zu jedem Kompasskurs die dazugehörige Korrektur abgelesen werden kann. Die Kompensation wird nach Werftaufenthalten oder bei voller Stahlladung wiederholt. In der allgemeinen Luftfahrt muss die Deviation nach bestimmten Veränderungen am Flugzeug oder in festgelegten Zeitabständen überprüft und der Kompass erneut kompensiert werden. Entsprechende Vorgaben finden sich beispielsweise in den Flug- und Betriebshandbüchern.

Als "Inklination" wird der Winkel zwischen den Tangenten an die magnetischen Feldlinien und der horizontalen Tangentialebene an die Erdoberfläche bezeichnet. In Mitteleuropa beträgt die Inklination etwa 66,5°. Mit anderen Worten, die Vertikalkomponente des Magnetfelds ist rund doppelt so groß wie die Horizontalkomponente.

Da zur Bestimmung der Nordrichtung nur die horizontale Komponente der Magnetfeldlinien von Bedeutung ist, muss die Inklination bei der Konstruktion des Kompasses individuell kompensiert werden. So wird bei einfachen Wanderkompassen z. B. einfach die Südhälfte der Nadel mit einem sogenannten Reiter beschwert. Ein solcher Kompass kann auf der Südhalbkugel nicht verwendet werden, da die Nadel schief hängt oder schlimmstenfalls den Boden des Kompassgehäuses berühren würde. Um weltweit Kompasse verkaufen zu können, entwickelten die Hersteller die folgenden zwei Lösungen:

Beim magnetischen Schiffskompass ist die Skala mit der Gradeinteilung am Zeiger angebracht, der entweder kardanisch aufgehängt ist oder in einer Flüssigkeit schwimmt (Kugelkompass), sodass sie trotz der durch den Seegang verursachten Schiffsbewegungen immer waagerecht liegt. Die Kompassrose dreht sich dabei als Ganzes, die Richtung wird an einer fest mit dem Gehäuse verbundenen Markierung abgelesen. Im Gegensatz dazu ist bei Kompassen, die an Land verwendet werden, die Kompassrose am Gehäuse angebracht, und nur die Kompassnadel dreht sich. In Bezug auf das Erdmagnetfeld dreht sich in beiden Fällen das Kompassgehäuse, und der Zeiger (Kompassnadel/-rose) bleibt an den magnetischen Feldlinien ausgerichtet.

In der Luftfahrt zeigt der Magnetkompass aufgrund seiner Massenträgheit bei Beschleunigungen falsche Werte an. Dieser Effekt wird "Beschleunigungsfehler" genannt. Beim Kurvenflug tritt der "Drehfehler" auf. Beim Steig- oder Sinkflug gibt es solche Anzeigefehler hingegen nicht, entgegen der häufig gegenteiligen Ansicht.

Der Beschleunigungsfehler entsteht dadurch, dass auch bei den in der Luftfahrt üblichen Kompassen der Schwerpunkt des drehbaren Teils tiefer liegt als dessen Aufhängepunkt. Bei Beschleunigungen quer zur Magnetisierung (Ost-West-Richtung) kippt der Kompassmagnet derart, dass die vertikale Komponente des Erdmagnetfelds dessen Nordpol nach unten ziehen kann, was die Anzeige verfälscht.

Der Drehfehler beruht auf der Schräglage des Kompasses in Kurven und tritt in Nord-Süd-Richtung auf. Auch hier wird der Kompassmagnet durch die vertikale Komponente des Erdmagnetfeldes verstellt. Eine Kurve ist daher auf nördlichen Kursen früher, auf südlichen später zu beenden, als der Kompass anzeigt. Dies gilt auf der Nordhalbkugel, auf der Südhalbkugel ist der Effekt entgegengesetzt. Heute werden im Motorflug Kurven allgemein nach dem drehfehlerfreien Kurskreisel geflogen, sodass der Kompassdrehfehler hier kaum noch von praktischer Bedeutung ist.

Es gibt beschleunigungs- und drehfehlerfreie Kompasse, beispielsweise Modelle der Schweizer Firma Bohli-Magnete oder der deutschen Firma Schanz Feinwerktechnik. Bohli- und Schanz-Kompass sind speziell für Segelflugzeuge entwickelt worden und dort insbesondere für den Einsatz im Wolkenflug. Diese Kompasse haben den Nachteil, dass sie von Hand der Querlage des Flugzeugs angepasst werden müssen. Da der Wolkenflug heute im Segelflug-Wettbewerb verboten ist, sind sie heute auch in Segelflugzeugen nur noch selten anzutreffen. Außerhalb der Segelflugszene ist diese Variante des Kompasses kaum bekannt.

Der "Adrianowkompass" () ist ein Militärkompass, der bereits bei der zaristischen Armee verbreitet war. Eine erste Version wurde von dem Militärvermesser Wladimir Adrianow 1907 entwickelt. Entsprechende Armbandkompasse wurden bei der Roten Armee und der Sowjetarmee getragen. Bei dem ikonographischen Bild Auf dem Berliner Reichstag, 2. Mai 1945 fiel bei dem flaggenhissenden Soldaten auf, dass er zwei Armbanduhren zu tragen schien, eine davon wurde vor der Veröffentlichung wegretuschiert. Die weitverbreitete Deutung als Plünderer liegt zwar aus der Entfernung nahe, ist aufgrund der möglichen Verwechslung mit dem Armbandkompass nicht zwingend.






</doc>
<doc id="12713" url="https://de.wikipedia.org/wiki?curid=12713" title="Reliabilität">
Reliabilität

Die Reliabilität (dt.: Zuverlässigkeit) ist ein Maß für die formale Genauigkeit bzw. Verlässlichkeit wissenschaftlicher Messungen. Sie ist derjenige Anteil an der Varianz, der durch tatsächliche Unterschiede im zu messenden Merkmal und nicht durch Messfehler erklärt werden kann. Hochreliable Ergebnisse müssen weitgehend frei von Zufallsfehlern sein, d. h. bei Wiederholung der Messung unter gleichen Rahmenbedingungen würde das gleiche Messergebnis erzielt werden (Reproduzierbarkeit von Ergebnissen unter gleichen Bedingungen).

Die Reliabilität stellt neben der Validität und der Objektivität eines der drei wichtigsten Gütekriterien für empirische Untersuchungen dar. Hohe Reliabilität ist grundsätzlich eine Voraussetzung für hohe Validität, wobei eine zu hohe Reliabilität zu Lasten der Validität gehen kann (Reliabilitäts-Validitäts-Dilemma).

Reliabilität umfasst drei Aspekte:

In der psychologischen Diagnostik wird sie zu den Hauptgütekriterien von psychologischen Tests gerechnet. Sie ist einer von mehreren Anhaltspunkten, wie genau ein Persönlichkeits- oder Verhaltensmerkmal gemessen wird.

Die Reliabilität kann mit verschiedenen Methoden geschätzt werden. Je nach Methode wird von anderen Reliabilitäts-Typen gesprochen.









</doc>
<doc id="12716" url="https://de.wikipedia.org/wiki?curid=12716" title="325">
325






</doc>
<doc id="12717" url="https://de.wikipedia.org/wiki?curid=12717" title="Validität">
Validität

Validität ( „kräftig“ „wirksam“; ; auch: Gültigkeit) ist (neben der Reliabilität und der Objektivität) ein Gütekriterium für Modelle, Mess- oder Testverfahren. Validität bezeichnet die inhaltliche Übereinstimmung einer empirischen Messung mit einem logischen Messkonzept. Allgemein ist dies der Grad an Genauigkeit, mit der dasjenige Merkmal tatsächlich gemessen wird, das gemessen werden soll. 

Dabei unterscheidet man zwischen einem Repräsentationsschluss (wenn das Testverhalten repräsentativ für Gesamtverhalten ist) und einem Korrelationsschluss (wenn das Verhalten im Test mit dem Verhalten außerhalb der Testsituation korreliert). Je nachdem, welche Variable als Kriterium für das Verhalten außerhalb der Testsituation verwendet wird, unterscheidet man zwischen inhaltlicher, prädiktiver oder Konstruktvalidität.

Die Validität gehört zu den sogenannten Hauptgütekriterien für Messinstrumente. Sie ist ein Maß dafür, ob die bei der Messung erzeugten Daten wie beabsichtigt die zu messende Größe repräsentieren. Nur dann können die Daten sinnvoll interpretiert werden.

Neben der Validität gehören die Objektivität (Unabhängigkeit der Ergebnisse von den Messbedingungen) und die Reliabilität (Zuverlässigkeit, formale Genauigkeit der Messung) zu den drei Haupt-Gütekriterien. Sie bauen aufeinander auf: Ohne Objektivität keine Reliabilität, ohne Reliabilität keine Validität.

Es gibt verschiedene Aspekte der Validität und zugeordnete Mess- und Schätzverfahren.

Vor allem für psychologische Tests finden diese Gütekriterien als Bewertungskriterien der Qualität Anwendung. Ein Test muss so konstruiert sein, dass Durchführung, Auswertung und Interpretation unabhängig vom Testleiter oder den Untersuchungsbedingungen sind (Objektivität) sowie das Testergebnis sich auch mit dem gleichen oder einem vergleichbaren Test bestätigt (Reliabilität). Die Validität oder Gültigkeit ist darauf bezogen, dass z. B. durch einen Intelligenztest wirklich Aspekte der Intelligenz gemessen werden und diese Messung eine Vorhersage der Leistungsfähigkeit im realen Leben (z. B. Ausbildungserfolg oder Berufserfolg) erlaubt. Solche Vorhersagen sind messungsbedingt mit einem Fehler versehen und nur Wahrscheinlichkeitsaussagen – zugleich wird an einigen Inhalten auch Kritik geübt, vgl. z. B. Kritik am Intelligenzbegriff.

In ihren "Technical recommendations for psychological tests and diagnostic techniques" (1954) schlug die American Psychological Association vier Arten der Validität vor, diese sind "Inhaltsvalidität", "Konstruktvalidität" und "prognostische und diagnostische Kriteriumsvalidität", von denen „historisch und praktisch gesehen […] die kriteriumsbezogene Validität der bedeutsamste Aspekt“ ist. „Die Übereinkunft durch ein Rating ist wie alle Übereinkünfte nicht etwas Abgeschlossenes, sondern kann einem ständigen Wandel unterworfen sein. […] Es bleibt dabei jedem Testinterpreten überlassen, dieses Kriterium anzuerkennen oder zu verwerfen bzw. nach einem besseren zu suchen.“

Inhaltsvalidität (engl. "") wird angenommen, wenn ein Verfahren zur Messung eines bestimmten Konstrukts oder Merkmals die bestmögliche Operationalisierung dieses Konstrukts ist. Das ist zum Beispiel bei Interessen- und Kenntnistests der Fall: Eine Klassenarbeit oder Führerscheinprüfung repräsentieren direkt die zu messenden Fähigkeiten. Daher spricht man auch von "logischer" oder "trivialer Validität". Ob Inhaltsvalidität gegeben ist oder nicht, entscheiden Experten per Rating.

Unter dem Begriff Konstrukt werden theoretische Eigenschaftsdimensionen (latente Variablen) verstanden. Konstruktvalidität bezieht sich auf die Zulässigkeit von Aussagen aufgrund der Operationalisierung über das gesamte dahinter liegende Konstrukt. Dies ist in der Regel dann der Fall, wenn der Bedeutungsumfang des Konstruktes vollständig, präzise und nachvollziehbar abgebildet ist. Als empirische Indikatoren der Konstruktvalidität gelten die "konvergente" und "diskriminante" (oder auch: "divergente") "Validität":


Sowohl konvergente als auch diskriminante Validität müssen gegeben sein, um einen vollständigen Nachweis der Konstruktvalidität zu gewährleisten. Das empirische Vorgehen bei der konvergenten und diskriminanten Validität sind Spezialfälle der Kriteriumsvalidität.

Bei der "Multitrait-Multimethod-Analyse" werden die konvergente Validität und die diskriminante Validität anhand einer einzigen Stichprobe miteinander verglichen. Dabei wird verkürzt gesagt erwartet, dass die konvergente Validität größer ist als die diskriminante Validität.

Faktoren für eine verminderte Konstruktvalidität können sein:

Kriteriumsvalidität bezieht sich auf den Zusammenhang zwischen den Ergebnissen des Messinstruments und einem empirischen Kriterium (Schnell, Hill & Esser, 2005, S. 155). Zum Beispiel: Ein Forscher untersucht den Zusammenhang seines neuen Intelligenztests mit den Schulnoten der Probanden, um die Gültigkeit seines Tests zu prüfen. Von „innerer (Kriteriums)validität“ wird dabei dann gesprochen, wenn als Kriterium ein anderer, als valide anerkannter Test herangezogen wird. Sofern als Kriterium ein objektives Maß (zum Beispiel psychophysiologische Maße oder ökonomische Größen) oder ein Expertenrating herangezogen wird, wird von "äußerer (Kriteriums)validität" gesprochen. Auch lässt sich unterscheiden nach dem Zeitpunkt, zu dem Übereinstimmung mit dem Kriterium vorliegen soll:



Augenscheinvalidität, auch als " bezeichnet, hängt davon ab, ob ein Messinstrument auch Laien plausibel erscheint. Augenscheinvalidität sagt nichts über die tatsächliche Validität, also die Inhalts-, Kriteriums- und Konstruktvalidität aus, sondern bestimmt über die Akzeptanz für ein Messverfahren. Auch sehr wenig valide Messinstrumente (wie z. B. unstrukturierte Einstellungsinterviews) erfreuen sich hoher Augenscheinvalidität und werden in der Praxis deshalb häufig eingesetzt.

Aufbauend auf den auf einzelne Konstrukte bezogenen Operationalisierungen ziehen in den meisten empirischen Studien Forscher erst in der statistischen Auswertung und danach im Hinblick auf ihre Kausalhypothesen Schlussfolgerungen über Ursache-Wirkungs-Zusammenhänge. Die Begriffe der "statistischen", "internen" und "externen Validität" beziehen sich auf das Zustandekommen, die Gültigkeit und die Übertragbarkeit dieser (induktiven) Schlüsse. Der Validitätsgrad dieser Schlüsse lässt sich jeweils nur diskutieren und abschätzen, niemals beweisen, und es ist darum – wie gehabt – sinnvoller, eher vom Validitätsgrad zu sprechen als vom Vorhandensein (oder Nicht-Vorhandensein) dieser Validitätsformen.

Für Aussagen oder in empirischen Studien gezogene Schlussfolgerungen (in der Regel über Ursache-Wirkungs-Verhältnisse) wird ein hoher Grad an "statistischer Validität" angenommen, wenn die Reliabilität und Teststärke der Messinstrumente und gewählten statistischen Verfahren hoch ist und allgemein die Fehlervarianz begrenzt wurde, die mathematischen Annahmen der statistischen Methoden nicht verletzt wurden und nicht einzelne Signifikanzen (zum Beispiel aus einer Korrelationsmatrix) „herausgefischt“ wurden ("Fishing").

Für Aussagen oder in empirischen Studien gezogene Schlussfolgerungen wird ein hoher Grad an "interner Validität" angenommen, wenn Alternativerklärungen für das Vorliegen oder die Höhe der gefundenen Effekte weitestgehend ausgeschlossen werden können. Interne Validität (oder Ceteris-paribus-Validität) liegt vor, wenn die Veränderung der abhängigen Variable eindeutig auf die Variation der unabhängigen Variable zurückgeführt werden kann (keine Alternativerklärung). Um dies zu gewährleisten, müssen Störvariablen kontrolliert bzw. durch verschiedene Methoden wie Elimination, Konstanthaltung und Parallelisierung ausgeschaltet werden. Damit die Effekte nicht auf Merkmale der Probanden zurückgeführt werden können, müssen diese zufällig den Versuchsbedingungen zugeteilt werden.

Die interne Validität wird gefährdet durch:

Im Englischen existiert hierzu die Eselsbrücke THIS MESS. Dieses Akronym bezieht sich auf acht Faktoren, die Gefährdungen der internen Validität darstellen, nämlich "T"esting (vgl. Reaktivität), "H"istory (Geschichte), "I"nstrument change (Veränderung beim Messinstrument), "S"tatistical Regression toward the mean (Regression zur Mitte), "M"aturation (Reifung), "E"xperimental mortality (Ausfall), "S"election (Selektion durch mangelhafte Randomisierung) und "S"election Interaction (Wechselwirkung zwischen Selektion und einem anderen Faktor, z. B. Reifung nur in der Experimentalgruppe).

Die externe Validität – auch Allgemeingültigkeit, Verallgemeinerungsfähigkeit oder ökologische Validität (vgl. Ökologischer Fehlschluss) – bezeichnet die Übereinstimmung von tatsächlichem und intendiertem Untersuchungsgegenstand. Grundidee ist hier die Frage nach der Generalisierbarkeit (Induktion). Nach der klassischen Sicht haben Aussagen oder in empirischen Studien gezogene Schlussfolgerungen einen hohen Grad an "externer Validität", wenn sich (a) die Resultate auf die Grundgesamtheit verallgemeinern lassen, für die die Studie konzipiert wurde, und (b) über das konkrete Setting der Studie hinaus auf andere Designs, Instrumente, Orte, Zeiten und Situationen übertragen lassen, also allgemeingültig, verallgemeinerungsfähig sind. Die häufigste Gefährdung der personenbezogenen externen Qualität (a) liegt in praktischen Problemen bei der Rekrutierung der Informationsträger, also der Personen, die befragt werden, oder der für ein Experiment benötigten Versuchspersonen. Ist ihre Teilnahme erzwungen oder freiwillig? Wie haben sie von der Teilnahmemöglichkeit erfahren (durch Zeitungsanzeige, Aushang usw.)? Was motiviert sie zur Teilnahme (interessiert sie das Thema, brauchen sie das Geld usw.)? Dies sind Filter, die die Qualität der Stichprobe einschränken können. Die häufigste Gefährdung der situationsbezogenen externen Qualität (b) liegt in der Künstlichkeit von Laborexperimenten.

Die externe Validität erhöht sich mit jeder erfolgreichen Replikation der Befunde, denn durch die Wiederholung mit anderen Probanden (Altersgruppe, Geschlecht, Kultur usw.) oder Variationen der Versuchsbedingungen werden die Einschränkungen für die Gültigkeit der Befunde geringer. Beispiel: Solange Pawlow nur gezeigt hatte, dass Hunden beim Erklingen einer Glocke das Wasser im Munde zusammenläuft, wenn die Glocke zuvor oft genug gleichzeitig mit der Gabe von Futter erklang, hat er eben nur das gezeigt. Vom Phänomen der klassischen Konditionierung kann man erst sprechen, wenn viele Arten von Subjekten viele Arten von bedingten Reaktionen auf viele Arten von bedingten Reizen zeigen. Für die statistische Auswertung von Replikationsstudien steht die Methode der Metaanalyse zur Verfügung.

Aus dieser klassischen Sicht stehen interne und externe Validität im Widerstreit: Ein hohes Maß an interner Validität erreicht man am besten durch hochkontrollierte und deshalb recht künstliche (Labor-)Bedingungen. Besonders realitätsnahe Forschungsdesigns, wie sie für eine möglichst hohe externe Validität ratsam scheinen, bergen hingegen die Gefahr unkontrollierbarer oder übersehener Störeinflüsse. Aus einer deduktivistischen Perspektive ist dies jedoch nur ein scheinbarer Widerspruch. Da beide Kriterien aus einer induktivistischen Forschungslogik heraus entwickelt wurden, steht die Generalisierung empirischer Befunde (bspw. aus einem Experiment) im Vordergrund. Hier ist die Frage nach der Replizierbarkeit der Ergebnisse unter verschiedenen Bedingungen mit verschiedenen Stichproben eine sinnvolle Frage. Eine deduktivistische Forschungslogik verfolgt jedoch ein anderes Ziel. Hier wird versucht, eine (allgemeingültige) Theorie anhand einer speziellen Vorhersage zu falsifizieren, nicht, wie in einer empiristischen Forschungslogik, eine Theorie durch genügend Beobachtungen zu verifizieren. Widerspricht nach dieser Logik die Beobachtung der Theorie, gilt diese als falsifiziert. Hierbei ist es irrelevant, ob die Ergebnisse in irgendeiner Weise „repräsentativ“ sind. Bestätigt sich die Vorhersage einer Theorie in einem Experiment, gilt die Theorie als bewährt, muss aber weiteren Prüfungen unterzogen werden. Einwände, die die Gültigkeit der Ergebnisse des Experiments in Frage stellen, sind Einwände gegen die interne Validität des Experiments.

Das Forschungsdesign hat einen großen Einfluss auf die Zulässigkeit und Gültigkeit der Kausalschlüsse, darum werden die Validitäten bei experimentellen und quasi-experimentellen Forschungsdesigns immer kritisch hinterfragt.

Der Begriff „Validität“ bezieht sich in der biologischen Nomenklatur auf die formale Gültigkeit eines Taxons (eine systematische Einheit von Lebewesen). Die Gültigkeit ist gegeben, wenn die Erstbeschreibung des Taxons den entsprechenden formalen Ansprüchen genügt (in der Botanik als „gültige Publikation“ bezeichnet). In diesem Fall gilt auch der für das Taxon gewählte Name als „valide“ (gültig). Ist der vergebene Name des Taxons aufgrund formaler Mängel nicht valide, handelt es sich bei diesem Namen um ein Nomen nudum.




</doc>
<doc id="12723" url="https://de.wikipedia.org/wiki?curid=12723" title="Freiburg im Üechtland">
Freiburg im Üechtland

Freiburg, [] (, [], im regionalen Senslerdeutsch [] oder [], ), ist der Hauptort des gleichnamigen Kantons und des Saanebezirks. Zur Unterscheidung vom deutschen Freiburg im Breisgau kann der Zusatz "im Üechtland" (kurz "i. Ü." oder "i. Üe.") oder "(Schweiz)" verwendet werden.

Freiburg, beidseits der Saane im Schweizer Mittelland gelegen, ist ein wichtiges Wirtschafts-, Verwaltungs- und Bildungszentrum mit zweisprachiger Universität an der Kulturgrenze zwischen deutscher und französischer Schweiz. Die gut erhaltene Altstadt liegt auf einem schmalen Felssporn über dem Tal der Saane.

Die Altstadt von Freiburg liegt auf , 28 km südwestlich von Bern (Luftlinie). Die Stadt erstreckt sich auf dem Plateau beidseits der Saane () deren Flussbett hier tief in die Molassesandsteinschichten eingeschnitten ist, im Schweizer Mittelland. Die Altstadt befindet sich auf einem nur gut 100 Meter breiten Mäandersporn westlich der Saane, rund 40 m über dem Talboden des Flusses. Die meisten Stadtquartiere liegen auf dem Hochplateau auf durchschnittlich und auf den angrenzenden Hügeln, während der Talboden der Saane nur gerade im Bereich des Altstadtmäanders, der früher ärmlichen "Bolze" oder Unterstadt, bewohnt ist. Der tiefste Punkt der Stadt liegt auf im Gebiet Windig.

Die Fläche des mit 9,3 Quadratkilometer für eine Stadt relativ eingeschränkten Gemeindegebiets umfasst einen Abschnitt des Molasseplateaus im Freiburger Mittelland. Von Süden nach Norden wird das Gebiet vom stark gewundenen Lauf der Saane durchquert, die sich durch Erosion bis zu 100 Meter tief in das Plateau eingeschnitten hat. Der Talboden ist im Allgemeinen 200 bis maximal 500 Meter breit. Südlich der Stadt befindet sich der 1872 gestaute Pérolles-See mit der ältesten Gewichtsstaumauer Europas. Rund ein Kilometer nördlich der Altstadt beginnt bereits der Aufstau des Schiffenensees. Bei den Stauseen nimmt die Saane jeweils fast die ganze zur Verfügung stehende Breite des Talbodens ein.

Auf beiden Seiten wird der flache Talboden durch weitgehend bewaldete und teils mit Sandsteinfluhen durchzogene Steilhänge flankiert. Daran schliesst im Westen das Hochplateau von Freiburg an (610 bis ), das wiederum von den Molassehügeln von "Chamblioux" () und "Le Guintzet" () begrenzt wird. Östlich der Saane reicht der Gemeindeboden auf die Höhen von "Schönberg" (französisch "Schœnberg"), der mit den höchsten Punkt des Stadtgebietes darstellt, und "Bürglen" (französisch "Bourguillon;" bis ). Dazwischen befindet sich der ebenfalls in das Plateau eingetiefte Graben der Galtera (französisch "Gottéron"), die im Bereich des Altstadtmäanders in die Saane mündet. Von der Gemeindefläche entfielen 1997 61 % auf Siedlungen, 18 % auf Wald und Gehölze, 14 % auf Landwirtschaft und etwas weniger als 7 % war unproduktives Land.

Zur politischen Gemeinde Freiburg gehören der ehemalige Weiler "Bürglen" () auf dem Plateau südlich des Galterngrabens sowie ein Teil des Quartiers "Schönberg" (bis ) an der östlichen Stadtgrenze nördlich des Galterngrabens, der grössere Teil liegt bereits auf dem Gemeindegebiet von Tafers. Nachbargemeinden von Freiburg sind im Osten Düdingen und Tafers, im Südosten St. Ursen und Pierrafortscha, im Süden Marly, im Westen Villars-sur-Glâne und Givisiez und im Norden Granges-Paccot.

Mit Einwohnern (ständige Wohnbevölkerung am ) ist Freiburg die grösste Stadt des Kantons Freiburg. Rund 29 % davon sind Ausländer. Besonders zu Beginn des 20. Jahrhunderts sowie von 1930 bis 1970 stieg die Bevölkerungszahl von Freiburg markant an. Der Höchststand wurde 1974 mit rund 42'000 Einwohnern erreicht. Seitdem wurde ein Bevölkerungsrückgang von rund 14 % verzeichnet, der jedoch in den letzten Jahren gestoppt werden konnte.

Die Agglomeration wird durch das Bundesamt für Statistik (BFS) auf rund 100'000 Einwohner beziffert (2008). Der engere Grossraum/Wirtschaftsraum Freiburg zählt ungefähr 75'000 Einwohner (2015). Zu diesem gehören neben der Stadt Freiburg die Gemeinden Avry, Belfaux, Corminboeuf, Givisiez, Granges-Paccot, Marly, Matran und Villars-sur-Glâne.

Das Siedlungsgebiet von Freiburg ist heute lückenlos mit denjenigen von Villars-sur-Glâne, Givisiez und Granges-Paccot zusammengewachsen. Direkt am östlichen Stadtrand befinden sich der zu Tafers gehörende Ortsteil Klein-Schönberg (französisch: Petit-Schoenberg) und der zu Düdingen gehörende Weiler Uebewil (französisch: Villars-les-Joncs). Dieses geschlossene Siedlungsgebiet zählt rund 60'000 Einwohner (2015).

Von den Bewohnern im Jahr 2000 sprachen 63,6 % französisch und 21,2 % deutsch. Die restlichen 15,2 % verteilen sich auf weitere Sprachen, darunter Italienisch, Albanisch, Serbokroatisch, Spanisch und Portugiesisch. 

Die Stadt Freiburg gilt, anders als der offiziell zweisprachige Kanton Freiburg, politisch als französischsprachige Gemeinde mit bedeutender deutschsprachiger Minderheit. Seit vielen Jahren bemühen sich deutschsprachige Bewohner von Stadt und Kanton darum, dass die Gemeinde Freiburg offiziell zweisprachig wird. Begehren in diese Richtung wurden vom Gemeinderat bisher zurückgewiesen.

Im Kontakt mit Behörden kann jedoch sowohl auf Französisch als auch auf Deutsch kommuniziert werden. Ebenso können Schulen in beiden Sprachen besucht werden. Im Jahre 2008 wurde von einigen Stadtparlamentariern ein «Forum Sprachen» initiiert, das den Austausch und die Annäherung zwischen den Sprachen fördern soll. 2013 wurde der Bahnhof offiziell mit «Fribourg/Freiburg» beschriftet und wird so künftig in allen Fahrplänen und Tarifen erscheinen. Anders als Biel/Bienne, das offiziell als zweisprachig gilt, befindet sich Freiburg noch in einem Entwicklungsprozess, was die Sprachenfrage betrifft.

Freiburg lag stets an der Sprachgrenze, dem sogenannten «Röstigraben», doch war die deutsche Sprache zur Zeit der Stadtgründung im 12. Jahrhundert noch vorherrschend. Obwohl Deutsch bis vor 1800 die Amtssprache in der Stadt war, gewann das Französische allmählich an Einfluss. Mit der Industrialisierung wurde ein weiterer Anziehungspunkt für französischsprachige Arbeiter geschaffen. Seit den politischen Umwälzungen Ende des 18. Jahrhunderts und zu Beginn des 19. Jahrhunderts wurden die deutschsprachigen Bewohner in die Minderheit gedrängt und während einiger Zeit diskriminiert. In der Unterstadt (Basse-Ville), wo einst die arme Bevölkerung lebte, gab es früher eine deutsch-französische Mischsprache, das „Bolz“.

Genaue Zahlen über die Sprachenverhältnisse liegen erst seit 1888 vor. Damals gaben rund 37 % der Stadtbevölkerung Deutsch als Muttersprache an. Besonders seit 1950 ist der Anteil der Deutschsprechenden durch den Zuzug aus der französischsprachigen ländlichen Gegend westlich und südlich von Freiburg stark zurückgegangen. Die Stadt dehnte sich dabei vor allem gegen Westen hin aus. Trotzdem werden aber seit Mitte des 20. Jahrhunderts Anstrengungen unternommen, die Zweisprachigkeit zu erhalten.

Die Bevölkerung von Freiburg ist überwiegend katholisch. Im Jahr 2000 waren 69 % der Bewohner Katholiken, 9 % Protestanten, 14 % gehörten anderen Glaubensrichtungen an und 8 % waren konfessionslos. Die Stadt verblieb in der Reformationszeit beim katholischen Glauben und bildete bis ins 20. Jahrhundert hinein ein politisches und geistiges Zentrum des Schweizer Katholizismus. Die Stadt weist eine überdurchschnittlich hohe Dichte an Kirchen und Klöstern auf, und seit 1613 ist Freiburg Bischofssitz.

Nachdem es Juden jahrhundertelang verboten war, sich in Freiburg niederzulassen, wurde 1895 eine neue jüdische Gemeinde (Communauté israélite de Fribourg, CIF) gegründet, die bis heute besteht. 2006 hatte die Gemeinde 62 Mitglieder.

Gesetzgebende Behörde ist der von den Stimmberechtigten der Gemeinde Freiburg alle fünf Jahre gewählte Generalrat "(Conseil général)". Die 80 Abgeordneten werden im Proporzwahlverfahren gewählt. Die Aufgaben des Generalrates umfassen die Budget- und Rechnungsabnahme, die Festlegung der Gemeindereglemente und die Kontrolle der Exekutive.

Ausführende Behörde ist der Gemeinderat "(Conseil communal)". Er besteht aus fünf Mitgliedern und wird vom Volk im Proporz-Wahlverfahren gewählt. Die Zahl der Mitglieder wurde 2001 von neun auf fünf verkleinert. Die Amtsdauer beträgt fünf Jahre. Der Gemeinderat ist für die Vollstreckung der Beschlüsse des Generalrates, für die Ausführung der Gesetzgebung von Bund und Kanton sowie für die Repräsentation und Führung der Gemeinde zuständig. Der Stadtammann "(Syndic)" verfügt über erweiterte Kompetenzen. Er leitet die Sitzungen des Gemeinderates.

Die fünf amtierenden Gemeinderäte sind (Legislaturperiode 2016–2021):

Bei den Schweizer Parlamentswahlen 2015 betrugen die Wähleranteile in Freiburg: SP 36.3 %, CVP 22.5 %, SVP 13.8 %, Grüne 9.8 %, FDP 8.7 %, GLP 3.7 %, CSP 3.6 %.
In Freiburg entwickelten sich bereits im 13. und 14. Jahrhundert verschiedene Gewerbezweige. Die zu dieser Zeit vorgenommenen Stadterweiterungen am östlichen Ufer der Saane weisen auf einen starken wirtschaftlichen Aufschwung hin. Vor allem im Galterntal wurde die Wasserkraft für den Betrieb von Mühlen, Sägen, Hammerschmieden, Walken und Stampfwerke benutzt. Auch entlang der Saane entstanden mit den Ortsteilen Au, Neustadt und Matten Gewerbequartiere. Diese „Unterstadt“ blieb bis in die neueste Zeit, als pittoreske Altbauten chic wurden, ein Arbeiterquartier, bis in die 1950-Jahre hinein gar eine der ärmsten Regionen der Schweiz.

Zu einer eigentlichen wirtschaftlichen Blüte führten im 14. und 15. Jahrhundert die Gerberei und die Tuchmacherei, gestützt durch die damals in der Region weit verbreitete Schafzucht. Sie verhalfen Freiburg dank dem Warenhandel zur Bekanntheit in ganz Mitteleuropa. Der allmähliche Niedergang der Tuchherstellung begann in der zweiten Hälfte des 15. Jahrhunderts, als die Schafzucht immer mehr durch die Rinderzucht verdrängt wurde. Weitere Gründe für den Zusammenbruch der Tuchindustrie im 16. Jahrhundert liegen darin, dass die Zünfte den Übergang zu neuen Stoffen und Modeströmungen verweigerten und dass sich die Gesellschaftsstrukturen in der Stadt mit dem Aufkommen des Patriziats wandelten. Als weitere Ursache kam der Niedergang der Handelsmessen in Genf hinzu, so dass der Absatz der Waren behindert wurde.

In der Folgezeit war Freiburg durch das Kleingewerbe geprägt. Die Industrialisierung fasste erst nach der Anbindung an das schweizerische Eisenbahnnetz ab den 1870er Jahren Fuss. Nachdem der Pérolles-See 1872 aufgestaut worden war, konnte Energie auf das Pérolles-Plateau südlich der Stadt und westlich der Saane geliefert werden. So entstand auf diesem Plateau ein Industriegebiet, in dem zunächst eine Sägerei und eine Waggonfabrik dominierten. Ferner wurden zu dieser Zeit zwei Brauereien gegründet, die 1970 zur Sibra Holding AG fusionierten. 1901 wurde auf dem Boden von Villars-sur-Glâne eine Schokoladenfabrik gegründet. Diese kam durch eine Gebietsabtretung 1906 auf Freiburger Boden zu liegen.

Im Lauf des 20. Jahrhunderts entwickelte sich das Plateau von Pérolles, das auch einen Bahnanschluss hatte, zum eigentlichen Industriequartier der Stadt. Mit der Erschliessung neuer Industriezonen ausserhalb des Gemeindegebietes wurden ab den 1970er Jahren verschiedene Branchen an den Stadtrand auf den Boden von Givisiez, Granges-Paccot und Villars-sur-Glâne verlegt. Die Industrie konnte hier einen grösseren Platz beanspruchen und erhielt eine bessere Strassenanbindung (Nähe zur Autobahn), während die frei gewordenen Flächen in Zentrumsnähe zu Wohn- und Einkaufszonen umgenutzt werden konnten.

Heute bietet Freiburg rund 25'000 Arbeitsplätze an. Mit 0,6 % der Erwerbstätigen, die noch im primären Sektor beschäftigt sind, hat die Landwirtschaft nur noch einen minimalen Stellenwert in der Erwerbsstruktur der Bevölkerung. Sie konzentriert sich heute auf Milchwirtschaft, Viehzucht und etwas Ackerbau. Etwa 17 % der Erwerbstätigen sind im industriellen Sektor tätig, während der Dienstleistungssektor rund 82 % der Arbeitskräfte auf sich vereinigt (Stand 2001).

Freiburg weist damit einen starken Zupendlerüberschuss aus und gilt als regionaler Anziehungspunkt für Bewohner im weitgehend landwirtschaftlich dominierten Umland. Die in Freiburg ansässige Industrie hat sich heute auf die Branchen Nahrungs- und Genussmittel, Metallbau und Maschinenbau sowie auf Elektrotechnik, Elektronik und Computertechnologie spezialisiert. Hingegen gehört die Brauerei Cardinal unterdessen dem dänischen Konzern Carlsberg, der das Cardinal-Bier in Rheinfelden brauen lässt.

Die grösste Zahl der Arbeitnehmer ist im Dienstleistungsbereich tätig. Ein grosser Teil davon ist in der Verwaltung beschäftigt. Weitere wichtige Sektoren sind das Bildungswesen mit der Universität Freiburg, das Banken- und Versicherungswesen (Hauptsitz der Freiburger Kantonalbank), die Tourismus- und Gastronomiebranche (Villars Holding) sowie das Gesundheitswesen. Freiburg ist Sitz verschiedener administrativer Niederlassungen von international tätigen Firmen. Das Kantonsspital liegt gerade an der Gemeindegrenze, jedoch zum grössten Teil auf dem Gemeindeboden von Villars-sur-Glâne.

Neue Wohngebiete entstanden seit den 1950er Jahren vor allem im Westen der Stadt sowie in den Stadtteilen Bellevue und Schönberg östlich des Saanegrabens. Teils wurden ausgedehnte Wohnblockquartiere, teils auch Einfamilienhaus- und Villenquartiere wie um die Hügel von Chamblioux und Le Guintzet sowie am oberen Schönberg geschaffen.

Die Stadt Freiburg bildet einen Anziehungspunkt für Tagestouristen, welche die Sehenswürdigkeiten der Stadt besichtigen wollen. Als Touristenattraktionen gelten die historische Altstadt auf ihrer markanten Spornlage über dem Saanetal mit der gotischen Kathedrale Sankt Nikolaus mit den berühmten Glasfenstern von Józef Mehoffer und die Museen.

Das Naturhistorische Museum Freiburg wurde 1873 gegründet und befindet sich heute im Gebäude der naturwissenschaftlichen Fakultät der Universität Freiburg in Pérolles. Im Museum für Kunst und Geschichte (Musée d’art et d’histoire), das seit 1920 im Ratzéhof untergebracht ist, kann man bedeutende Sammlungen aus der Ur- und Frühgeschichte, Archäologie, Plastik und Malerei, traditionelle Zinnfiguren, Kunstgewerbe sowie Münz- und Graphische Sammlungen besichtigen.

In der Kathedrale ist seit 1992 eine Schatzkammer geöffnet. Der seit 1998 im ehemaligen Tramdepot eingerichtete "Espace Jean-Tinguely–Niki-de-Saint-Phalle" zeigt Werke des Künstlerehepaars. Zu den weiteren Museen gehören die Kunsthalle Fri-Art, das Schweizer Figurentheatermuseum (Musée Suisse de la Marionnette), das Schweizerische Nähmaschinenmuseum (Musée suisse de la machine à coudre), das Gutenberg-Museum der Schweizerischen grafischen Industrie und das Biermuseum Cardinal.

An kulturellen Veranstaltungen sind das zweijährlich stattfindende Internationale Festival Geistlicher Musik, das Internationale Folkloretreffen, die Jazz-Parade, das Internationale Filmfestival und "Cinéplus" (seit 1978) zu nennen. Daneben findet auch die Gegenwartskultur sowie die Elektro- und Rockmusik im "Fri-Son" ihren Platz.

Das im Jahr 1999 eröffnete Messe- und Kongresszentrum "Forum Fribourg" befindet sich auf dem Gemeindegebiet von Granges-Paccot am Nordrand der Stadt.

Seit Dezember 2011 gibt es in Freiburg das Kulturzentrum "Équilibre" für Theater, Konzerte, Oper und Tanz.

Jedes Jahr findet am ersten Samstag des Dezembers das traditionelle St.-Nikolaus-Fest statt, das bis zu 20'000 Menschen in die Strassen des Stadtkerns lockt. Anfang Dezember 2005 wurde die 100. Ausgabe gefeiert.

Freiburg hat sich seit der Gründung des Jesuitenkollegiums Sankt Michael im 16. Jahrhundert und der Gründung der zweisprachigen Universität Freiburg im Jahr 1889 den Ruf einer bedeutenden Bildungsstadt erworben. Sämtliche Schulstufen können in Freiburg auf Deutsch oder Französisch besucht werden. In Freiburg bietet sich – einzigartig in der Schweiz und in Europa – auch die Möglichkeit eines zweisprachigen Universitätsabschlusses. Auch Fachhochschul-Abschlüsse sind in beiden Sprachen möglich. Der Schwerpunkt der Bildungsarbeit verlagerte sich in der zweiten Hälfte des 20. Jahrhunderts vom Katholizismus zur Zweisprachigkeit.

Zu den weiterführenden Schulen, die ihren Sitz in Freiburg haben, zählen neben der Universität die Ecole de Multimédia et d’Art de Fribourg (EMAF), die Lehrwerkstätte Freiburg (Ecole des Métiers de Fribourg, EMF), die sich auf die Sparten Technik, Informatik, Elektronik, Automatik und Polymechanik konzentriert, die Ingenieur- und Architektenschule, die Hochschule für Wirtschaft und Verwaltung, die Hochschule für Gesundheit, die Pädagogische Hochschule sowie das Konservatorium.

Die Stadt besitzt drei Gymnasien, das Kollegium Sankt Michael, das Kollegium Heilig Kreuz und das Kollegium Gambach.

Freiburg ist der bedeutendste Verkehrsknotenpunkt des Kantons Freiburg. Die Stadt liegt an der Hauptstrasse 12, die von Bern nach Vevey führt. Weitere Hauptstrassenverbindungen bestehen mit Payerne, Murten und Thun. Der Anschluss an das schweizerische Autobahnnetz erfolgte im Jahr 1971 mit der Eröffnung der Autobahn A12 von Bern bis Matran. Seit 1981 ist die Autobahn durchgehend von Bern bis Vevey befahrbar. Freiburg lag danach während 20 Jahren bis zur Eröffnung der A1 an der Hauptachse des Strassenverkehrs von Bern in die Westschweiz. Die Autobahn umfährt die Stadt im Norden und Westen und tangiert das Gemeindegebiet nur in einem kurzen Abschnitt im Tälchen westlich der Höhe von Chamblioux. Die Anschlussstellen Fribourg-Sud und Fribourg-Nord sind jeweils rund 3 km vom Stadtkern entfernt. Die 2014 eröffnete Poyabrücke entlastet die Altstadt vom Strassenverkehr.

Die Anbindung an das Eisenbahnnetz vollzog sich in mehreren Schritten ab 1860. Zunächst wurde die Bahnstrecke Lausanne–Bern am 2. Juli 1860 in Betrieb genommen. Allerdings befand sich der damalige provisorische Endbahnhof beim Weiler Balliswil rund vier Kilometer nordnordöstlich der Stadt. Der Grandfey-Viadukt über den Saanegraben war damals noch nicht fertiggestellt. Gut zwei Jahre später, am 4. September 1862, wurde die gesamte Strecke von Balliswil via Freiburg nach Lausanne eröffnet. Auch der Freiburger Bahnhof war anfangs nur ein Provisorium, bis das eigentliche Gebäude 1873 errichtet wurde. Weitere Streckeneröffnungen erfolgten am 25. August 1876 (Freiburg–Payerne) und am 23. August 1898 (Freiburg–Murten). Die Verbindung vom Stadtteil Neuveville zur Oberstadt wird seit 1899 durch die mit Abwasser betriebene Standseilbahn Neuveville–Saint-Pierre hergestellt (siehe auch "Wasserballastbahn"). Von 1897 bis 1965 war in Freiburg die rund sechs Kilometer lange Strassenbahn in Betrieb. Sie musste jedoch ab 1951 dem 1949 eröffneten Trolleybus weichen. Es gab allerdings schon zwischen 1912 und 1932 eine Überland-Trolleybuslinie, die Gleislose Bahn Freiburg–Farvagny.

Für die Feinverteilung im öffentlichen Verkehr sorgt in der Stadt heute ein dichtes Liniennetz der Freiburgischen Verkehrsbetriebe. Es besteht aus drei Trolleybuslinien und ergänzend dazu vier Autobus-Linien. Zum Fahrplanwechsel vom Dezember 2012 wurden noch die ehemaligen Überlandbusse Nr. 542 (neu Nr. 8), Nr. 575 (neu Nr. 9) und Nr. 338 (neu Nr. 11) ins Stadtbusnetz aufgenommen. Im weiteren verkehren von der Stadt sternförmig in alle Richtungen des Kantons Regionalbuslinien, unter anderem nach Bulle, Avenches, Schmitten, Schwarzenburg und in die Tourismusregion Schwarzsee.

Seit 2010 gibt es ein öffentliches Netz von drei Veloverleih-Stationen. Es stehen 32 City- und Elektrovelos zur Verfügung. Die Stationen befinden sich am Bahnhof, beim St-Léonard und bei der Uni Pérolles. Tageskarten können beim Tourismusbüro oder am Schalter der TPF am Bahnhof bezogen werden.

Die Region von Freiburg war seit der Jungsteinzeit besiedelt, allerdings stammen aus dem heutigen Stadtgebiet nur spärliche Funde, beispielsweise einige Feuersteinfunde bei Bourguillon sowie Steinbeilklingen und Bronzegeräte. Während der Römerzeit befand sich bei Freiburg ein Übergang über die Saane. Die Hauptachse durch das Mittelland verlief damals jedoch weiter nördlich durch das Broyetal und über Aventicum (Avenches). Deshalb sind auch aus der römischen Epoche nur geringe Siedlungsspuren erhalten. Auf dem Plateau von Pérolles wurden einige Überreste von römischen Mauerfundamenten entdeckt.

Freiburg wurde im Jahr 1157 durch Herzog Berthold IV. von Zähringen an strategisch gut geschützter Lage auf einem Felsvorsprung über der Saane gegründet und mit grosszügigen Freiheiten ausgestattet. Die Zähringer konnten damit ihre Machtstellung im Schweizer Mittelland im Raum zwischen Aare und Saane festigen und ausbauen. Die ersten überlieferten Namen der Stadt sind "Friborc" (1157/80) und "Fribor" (1175). Als französischer Name ist "Fribourg en Nuithonie" überliefert. Der Name, der «freie Stadt» bedeutet, soll einerseits auf die Privilegien verweisen, welche der Stadtgründer den Bürgern verlieh, anderseits aber auch eine bewusste Nachahmung des wenig früher ebenfalls von den Zähringern gegründeten Freiburg im Breisgau sein.

Seit seinen Anfängen bildete Freiburg einen Stadtstaat, also eine Stadtherrschaft, zu der kaum Gebiet aus dem regionalen Umland gehörte. Als das Geschlecht der Zähringer 1218 erlosch, gelangte Freiburg durch einen Erbgang an die Grafen von Kyburg. Diese gewährten der Stadt ihre bisherigen Freiheiten und schrieben die Gemeindeverfassung im Jahr 1249 in der so genannten "Handfeste" nieder, in der die rechtliche, institutionelle und wirtschaftliche Organisation festgehalten wurde. In diese Zeit fiel auch der Abschluss von mehreren Bündnissen mit den Nachbarstädten, nämlich mit Avenches (1239), Bern (1243) und Murten (1245).

Durch Kauf kam die Stadt 1277 an das Haus Habsburg. Schon seit Mitte des 13. Jahrhunderts blühten in Freiburg Handel und Gewerbe auf. In der Frühzeit bestand Freiburg aus vier verschiedenen Quartieren: Burg, Neustadt, Au und Spital. Die Stadt entwickelte sich rasch und erfuhr die ersten Erweiterungen: Das Burgquartier dehnte sich bereits ab 1224 weiter nach Westen aus, 1254 wurde der Brückenkopf auf der Ostseite der Bernbrücke gegründet, ab 1280 wurden Erweiterungen im Bereich der Place Python vorgenommen. Diese Erweiterungen widerspiegeln den wirtschaftlichen Aufschwung Freiburgs.

Am 12. Februar 1378 verkaufte Jakob von Düdingen der Stadt Freiburg für 3000 Gulden seinen Anteil am Simmental. Bereits am 24. Februar verpflichtete sich auch Wilhelm von Düdingen, der Stadt Freiburg seine Burgen im Simmental (Blankenburg, Mannenberg und die Laubegg) offen zu halten.
Zur gleichen Zeit verpfändete Graf Rudolf von Kyburg der Stadt Freiburg für 5000 Gulden Burg, Stadt und Herrschaft Nidau.
Am 16. Mai 1382 konnte die Stadt Freiburg für 1050 Gulden den Inselgau (Seeland) kaufen. Dazu gehörten Worben, Jens, Merlingen, Bellmund, Wiler, Port und die Vogtei über die Petersinsel. All diese Erwerbungen neben der Alten Landschaft hätten einen soliden Grundstein für das Territorium des Stadtstaates Freiburg ergeben. Doch sie gingen nach dem Sempacherkrieg an Bern verloren und es entwickelte sich ein regelrechter Kleinkrieg zwischen Freiburg und Bern. Die Freiburger verloren nicht nur ihre Ansprüche auf Büren an der Aare und Nidau sowie auf das Simmental. Ihnen wurde nach langen, zähen Schiedsverhandlungen am 18. Februar 1398 auch der Inselgau abgesprochen.

Auch das 14. Jahrhundert stand ganz im Zeichen des Handels, der Tuchherstellung und der Lederverarbeitung, die der Stadt ab 1370 zur Bekanntheit in ganz Mitteleuropa verhalfen.

Der Burgrechtsvertrag mit Bern wurde im Jahr 1403 erneuert. Die Stadtherren verfolgten nun eine neue Territorialpolitik, indem sie allmählich Gebiete im engeren Umland erwarben und damit den Grundstein für die Freiburger Alte Landschaft legten. So hatte sich die Stadt bereits 1442 beidseits der Saane ein Umland von rund 20 km Durchmesser geschaffen. Es unterstand in der Folge direkt den Stadtherren und wurde nicht über die Zwischenstufe eines Vogtes verwaltet.

Die Zeit um die Mitte des 15. Jahrhunderts ist durch verschiedene kriegerische Auseinandersetzungen geprägt. Zunächst mussten grössere Verluste im Krieg gegen Savoyen beklagt werden. Das savoyische Element erreichte in Freiburg immer mehr Einfluss, und so geriet die Stadt 1452 von Habsburg unter die Oberhoheit Savoyen, in der sie bis 1477 nach den Burgunderkriegen verblieb. Als Bündnispartner von Bern nahm Freiburg an den Kriegen gegen Karl den Kühnen teil und konnte so weitere Gebiete für sich sichern.

Nach der Entlassung aus dem Einflussbereich Savoyens erhielt Freiburg 1478 den Status einer freien Reichsstadt, welche mit der Alten Landschaft über ein ansehnliches Hinterland verfügte. Seit 1481 ist Freiburg Mitglied der Schweizerischen Eidgenossenschaft und war lange für den schweizerischen und europäischen Katholizismus prägend (siehe dazu auch Geschichte des Kantons Freiburg). Weitere Gebietszuwächse konnte Freiburg im 16. Jahrhundert zunächst 1536 mit Bern bei der Eroberung des Waadtlandes und 1554 bei der Aufteilung der Grafschaft Greyerz verbuchen.

Aus dem Tuch- und Lederhandel gingen seit dem Ende des 14. Jahrhunderts verschiedene reiche Familien hervor, darunter Gottrau, Lanthen, Affry, Diesbach (ursprünglich aus Bern), Von der Weid, Fegeli und Weck. Zusammen mit dem Lokaladel (Familien Maggenberg, Düdingen/Velga, Montenach, Englisberg und Praroman) bildete sich ab dem 15. Jahrhundert das Patriziat, das in der Folge die Macht unter sich aufteilte. Genau dies war aber ein wichtiger Grund für den Niedergang der Tuchherstellung, denn die einst durch Handel und Gewerbe emporgekommenen Familien kümmerten sich nun vermehrt um die Stadtherrschaft und die Verwaltung des erworbenen Landbesitzes.

Ein wichtiger Meilenstein in der Stadtpolitik ist das Jahr 1627, in dem sich das damalige Patriziat mit einer neuen Verfassung für allein regimentsfähig erklärte und damit das aktive und passive Wahlrecht für sich in Anspruch nahm. Damit war die Oligarchie mit restriktiven Organisationsstrukturen, die sich bereits im Lauf des 15. Jahrhunderts abzeichneten, besiegelt.

Stets bildeten die Klöster von Freiburg ein Zentrum der geistlichen Kultur, zeichneten sich für Baukunst, Bildhauerei und Malerei verantwortlich und trugen wesentlich zur Blüte der Stadt bei. Das Kloster der Franziskaner-Konventualen wurde 1256 von Jakob von Riggisberg gestiftet. Es stand in seiner Anfangszeit in enger Verbindung mit dem Stadtrat, indem es bis 1433 das Stadtarchiv aufbewahrte und die Klosterkirche für Bürgerversammlungen zur Verfügung stellte. Von besonderer Bedeutung ist der Totentanz, den der Freiburger Maler Pierre Vuilleret zwischen 1606 und 1608 auf die Südwand des Klosterkreuzgangs gemalt hatte. Die ursprünglich 17 Wandbilder zeigten, wie der Tod jeweils mehreren Standespersonen entgegentritt, um sie mit sich zu nehmen. Auftraggeber war der Ritter Hans von Lanthen-Heid. Die noch verbliebenen Reste dieser Wandbilder wurden 1927 abgetragen, um den darunter liegenden spätgotischen Zyklus eines Marienlebens wieder sichtbar zu machen. Heute können die Szenen des Totentanzes noch nachempfunden werden, weil sich zwei Aquarelle von 1875 des Solothurner Malers Adolf Walser und 16 Gouachen von 1925/26 des Franziskaners Maurice Moullet erhalten haben.

Ebenfalls um die Mitte des 13. Jahrhunderts wurde das Augustinerkloster in der Au gegründet, das über lange Zeit die Unterstützung der Adelsfamilie Velga genoss. Auch das Frauenkloster Magerau (Maigrauge) existiert seit 1255 und gehört seit 1262 dem Zisterzienserorden an.

Eine wichtige Institution war das Mitte des 13. Jahrhunderts aus der Taufe gehobene Bürgerspital, das sich um die Pflege der Armen kümmerte. Unter den Johannitern wurde ab 1260 eine Kommende mit angeschlossenem Spital erbaut.

Während der Reformationszeit blieb Freiburg beim alten Glauben, obwohl sein Gebiet durch das nun reformierte Bern fast vollständig umgeben war. So kam es in den Grenzgebieten und in den gemeinsam mit Bern verwalteten Herrschaften immer wieder zu Streitigkeiten über die Glaubensrichtung. Die Stadt selbst wurde dadurch zu einer Hochburg der Gegenreformation. In der Zeit vom ausgehenden 16. Jahrhundert bis in die erste Hälfte des 17. Jahrhunderts wurden verschiedene neue Klöster gegründet, nämlich das Kapuzinerkloster (1608), das Kapuzinerinnenkloster auf dem Bisemberg (1621), das Ursulinenkloster (1634) und das Visitandinnenkloster (1635).

Der einflussreichste Orden aber waren die Jesuiten, die entscheidend zur Weiterentwicklung und Prosperität der Stadt beitrugen. Sie errichteten 1582 das Kollegium Sankt Michael, das mit seiner Theologischen Fakultät den Ursprung der Universität Freiburg darstellt. Auch die Entwicklung der obrigkeitlichen Druckerei geht auf die Initiative der Jesuiten zurück.

Von 1613 an wurde Freiburg Residenz des Bischofs von Lausanne, der sich nach der Reformation von Lausanne zunächst in Evian und nachher im burgundischen Exil aufhielt. Heute ist Freiburg Sitz des Bistums Lausanne, Genf und Freiburg.

Das strenge patrizische Regime (bestehend aus maximal 60 Familien) bekleidete während fast 200 Jahren sämtliche einflussreichen Posten in der Stadt und spielte die führende Rolle in politischer, wirtschaftlicher, kultureller und sozialer Hinsicht. Mehrfach schlossen sich die unterdrückten Bürger zusammen und probten den Volksaufstand, so auch 1781 unter der Führung von Pierre-Nicolas Chenaux. Mit der von Bern angeforderten Unterstützung konnte der Aufstand niedergeschlagen werden.

Mit dem Einmarsch der französischen Truppen in die Schweiz wurde 1798 das Ende des Ancien Régime eingeleitet. Freiburg kapitulierte am 2. März und musste seine Herrschaft über die Landschaft niederlegen. Damit war der Weg frei für die Wahl einer Gemeindebehörde, der als erster Stadtammann Jean de Montenach vorstand. Mit der Einführung der Mediationsakte unter Napoleon wurde 1803 die Trennung von Kanton und Gemeinde Freiburg endgültig vollzogen. Die Stadt war nun Hauptort des Bezirks und des Kantons Freiburg sowie zwischen 1803 und 1809 alternierend eine der Hauptstädte der Schweiz.

1814 kam die alte patrizische Herrschaft nochmals an die Macht und regierte während der Restaurationszeit bis 1830 die Stadt. Danach wurde das Regime durch eine liberalere Verfassung abgelöst. Freiburg, das sich als Stadt und Kanton am Sonderbund beteiligte, war einer der Schauplätze des Sonderbundskrieges und musste am 14. November 1847 kapitulieren. Ab 1848, mit der neuen Bundesverfassung und der Änderung der Kantonsverfassung, hatte in Freiburg jeder niedergelassene Schweizer Bürger das Recht, an Wahlen teilzunehmen. Das kantonale Frauenstimmrecht hat Freiburg am 7. Februar 1971 eingeführt, zum selben Datum seiner Einführung auf Bundesebene (wie auch die Kantone Aargau, Schaffhausen und Zug).

Im Lauf des 19. Jahrhunderts gab es einschneidende Veränderungen im Stadtbild. So wurden ab 1848 die Stadtmauern teilweise abgerissen und neue Brücken überspannten das Saane- und das Galterntal. Der Anschluss an das schweizerische Eisenbahnnetz ab 1862 führte zur Entstehung eines Bahnhofquartiers. Mit der verbesserten Verkehrsanbindung setzte sich auch die Industrialisierung durch. Der Schwerpunkt der Stadt verlagerte sich damit von der historischen Altstadt in das Bahnhofquartier. Umfangreiche Gebiete in den Quartieren Pérolles, Beauregard und Vignettaz wurden um 1900 mit Industrieanlagen und Wohnhäusern überbaut. Ein wichtiger Eckpunkt war auch die Eröffnung der Universität 1889. Weitere Impulse im wirtschaftlichen Aufschwung erhielt Freiburg 1971 mit der Eröffnung des ersten Abschnitts der Autobahn A12 von Bern nach Vevey.

Freiburg konnte seinen alten historischen Stadtkern erhalten. Er zählt heute zu den grössten geschlossenen mittelalterlichen Zentren Europas und liegt auf einem spektakulären Felsvorsprung, der auf drei Seiten von der Saane umflossen wird. Die Bausubstanz stammt zum grossen Teil aus der gotischen Zeit bis zum 16. Jahrhundert; die Häuser bestehen zumeist aus dem regionalen Molasse-Sandstein. Den Kern der Altstadt bildet das Burgquartier, aber noch im 12. Jahrhundert kamen das Auquartier (ebenfalls in der Saaneschlaufe, aber nur rund 10 m über dem Talboden) und zu Beginn des 13. Jahrhunderts der Brückenkopf östlich des Flusses hinzu. Dieser leicht abgewinkelte Stadtgrundriss weist eine Länge von rund einem Kilometer, aber eine Breite von nur etwa 100 bis 200 m auf.

Geschützt war die Stadt durch ein mindestens zwei Kilometer langes Ringmauersystem, das sich gut in die schwierige Topographie einfügte. Wichtige Zeugen dieser mittelalterlichen Militärarchitektur der Schweiz sind neben den erhaltenen Mauerresten auch 14 Türme und ein grosses Bollwerk aus dem 15. Jahrhundert. Das ehemalige Befestigungswerk ist insbesondere im Osten und Süden gut erhalten. Hierzu gehören der Berner Torturm, der Katzenturm, der Rote Turm (aus dem 13. Jahrhundert) und der Dürrenbühlturm. Etwas jünger sind das Murtentor (1410), der Vierpfundturm (Tour des Rasoirs, 1411), die Tour des Curtils novels und der halbrunde Thierryturm (Tour Henri, 1490) im nördlichen und westlichen Teil der Stadt.

Herausragendes Bauwerk in der Altstadt von Freiburg ist die Kathedrale Sankt Nikolaus. Sie wurde ab 1283 in mehreren Etappen bis 1490 an der Stelle eines romanischen Gotteshauses erbaut.

Im "Burgquartier" befinden sich zahlreiche weitere bedeutende Bauwerke. Das Rathaus (Hôtel de Ville) wurde von 1501 bis 1522 an der Stelle der ehemaligen, im 15. Jahrhundert zerstörten zähringischen Burg erbaut. Sein Uhrturm wurde im 16. und 17. Jahrhundert mehrfach verändert. Gleich nebenan steht das in einer Stilmischung von Barock und Klassizismus gehaltene Stadthaus von 1731. Auch die Staatskanzlei (1734–1737) zeigt dieselben Stilformen und ein skulptiertes heraldisches Motiv über dem Hauptportal. Der Rathausplatz mit dem Georgsbrunnen (Brunnenfigur von 1525) wird auch von der Gendarmerie, einem Gebäude im Stil Louis-seize von 1783 gesäumt. Das Postgebäude von 1756–1758 zeigt den Louis-XV-Stil. In der Hauptstrasse (Grand-Rue) wurde und wird immer noch der Markt abgehalten. Die Strasse wird von einer beeindruckenden Häusergruppe aus dem 16. bis 18. Jahrhundert gesäumt, darunter das Direktionsgebäude der städtischen Behörde mit Stilformen aus Gotik und Renaissance, das Haus Castella (1780) und das spätgotische Gebäude Les Tornalettes (1611–1613) mit Treppenturm und Eckerker. An der Zähringerstrasse stehen das Haus Techtermann, das im Kern aus dem 14. Jahrhundert stammt und damit das älteste Wohnhaus der Stadt ist, sowie das Hôtel Zaehringen aus dem 18. Jahrhundert.

Eine Reihe wichtiger kirchlicher Bauwerke findet man in der "Oberstadt", dem ehemaligen Spitalquartier. Die dreischiffige Liebfrauenkirche (Notre-Dame) geht im Kern auf das 12. Jahrhundert zurück, wurde aber von 1785 bis 1787 umfassend umgestaltet. Aus dieser Zeit stammt die barock-klassizistische Fassade, während der Glockenturm noch seine ursprüngliche Bausubstanz zeigt und im Unterbau eine romanisch-gotische Kapelle aus dem 13. Jahrhundert birgt. Auch die Franziskanerkirche (ursprünglich von 1281) mit ihrem dreischiffigen gotischen Chor ist sehenswert; das Schiff und die Aussenfassade wurde 1735–1746 erneuert. Zu der reichen Ausstattung gehören das hölzerne Chorgestühl von 1280, das zu den ältesten der Schweiz zählt, sowie ein Hochaltar aus dem 15. Jahrhundert. Im Kreuzgang sind Fresken aus der Zeit um 1440 erhalten. Etwas neueren Datums sind Kloster und Kirche der Visitandinnen, die 1653–1656 errichtet wurden. Bemerkenswert ist hier der Zentralbau der Konventskirche, der von einer oktogonalen Tambourkuppel gekrönt ist. Die Kirche Sankt Michael, die den Jesuiten gehörte, wurde 1604–1613 im spätgotischen Stil erbaut, während das Innere Mitte des 18. Jahrhunderts umgestaltet und mit Rokoko-Dekor versehen wurde. Die Kollegiumsbauten stammen aus der Epoche der Renaissance und wurden zumeist Ende des 16. Jahrhunderts errichtet. Die Ursulinerinnenkirche schliesslich wurde 1677–1679 erstellt.

Zu den bedeutenden profanen Bauten in der Oberstadt gehören der Ratzéhof (im Renaissancestil 1581–1585 erbaut, beherbergt heute das Museum für Kunst und Geschichte), das Schloss La Poya (eine Villa, die 1699–1701 für die Familie Lanthen-Heid errichtet wurde), das Haus Gottrau aus dem 18. Jahrhundert, das bischöfliche Palais (1842–1845) und das vom ausgehenden 17. Jahrhundert stammende ehemalige Bürgerspital.

Das "Auquartier" (französisch Quartier de l’Auge) bildet die südöstliche und auf einem tieferen Niveau liegende Fortsetzung des Burgquartiers. Hier befinden sich das Kloster und die Kirche der Augustiner. Die dreischiffige Kirche Sankt Mauritius mit Polygonalchor geht im Kern auf die Gründungszeit des Klosters im 13. Jahrhundert zurück, wurde aber im 16. und 18. Jahrhundert mehrfach verändert; sie besitzt eine reiche Innenausstattung, darunter ein Hochaltar mit geschnitztem Retabel (1602) und steinerne Priestersitze (1594). Die Konventsgebäude stammen zum grossen Teil aus dem 17. und 18. Jahrhundert und dienen als Sitz des kantonalen Dienstes für Denkmalpflege. Bis 2005 war hier das Staatsarchiv untergebracht, das sich heute im Pérollesquartier befindet. Das Auquartier zeichnet sich durch verschiedene gotische und spätgotische Häuser sowie durch mit Brunnen (Samariterbrunnen, Annabrunnen) geschmückte Plätze aus.

Im Talboden der Saane südlich des Burgquartiers befindet sich die "Neustadt" (Neuveville) mit der Kirche Mariahilf (1749–1762, barocke Innenausstattung) und zahlreichen spätgotischen Häusern.

Jenseits der Saane, im "Mattenquartier" (Quartier de la Planche), bildet die Komturei und Kirche Sankt Johannes den Mittelpunkt. Die 1264 geweihte Kirche wurde 1885 und 1951 stark verändert, während die Gebäude der ehemaligen Komturei aus dem 16. und 17. Jahrhundert stammen. Nahebei steht die Kaserne, ein 1708–1709 erbauter Speicher, der 1821 die Funktion einer Kaserne übernahm und dazu umgebaut wurde. Etwas abgeschieden und auf drei Seiten von der Saane umflossen liegt die Zisterzienserinnenabtei Magerau (Maigrauge), die 1255 erstmals erwähnt wurde. Die Kirche hat ihre ursprüngliche Form aus dem 13. Jahrhundert weitgehend bewahrt, die Konventsgebäude wurden nach einem Brand 1660–1666 neu erbaut. Auf dem östlich der Saane liegenden Vorsprung befinden sich das Kloster Montorge (1626 gegründet) mit einer schlichten einschiffigen Konventskirche von 1635, die Loretokapelle (1648 in Anlehnung an Santa Casa di Loreto erbaut) und das Bürglentor (Porte de Bourguillon), dessen Bausubstanz aus dem 14. bis 15. Jahrhundert stammt.

Ausserhalb der alten Stadt sind die Bauten der Universität, das Villenviertel mit Jugendstil-Bauten im Gambachquartier und der Betonbau der Christkönigskirche (1951–1953) in Pérolles zu erwähnen. Ebenfalls in Pérolles befinden sich das für die Familie Diesbach von 1508 bis 1522 erbaute Schloss und die Sankt Bartholomäuskapelle im gotischen Flamboyant-Stil, die eine Sammlung von Glasmalereien aus der Renaissance birgt. In Bourguillon (Bürglen) steht die einschiffige Kirche Notre-Dame, die 1464–1466 errichtet wurde.

Freiburg ist zudem für seine zahlreichen Brücken bekannt, die den Lauf der Saane überspannen. Die Bernbrücke, die das Auquartier mit dem Brückenkopf östlich der Saane verbindet, ist eine gedeckte Holzbrücke, die ihre Gestalt 1653 erhielt. Zum Mattenquartier führen vom Auquartier die Mittlere Brücke, eine steinerne Vierbogenbrücke von 1720, und von der Neustadt die Sankt Johannbrücke (1746, ebenfalls mit Tuffsteinquadern). Neben diesen Brücken im Tal besitzt Freiburg drei Hochbrücken. Die Zähringerbrücke verbindet das Burgquartier direkt mit dem Stadtteil Schönberg; sie wurde 1924 an der Stelle der Hängebrücke von 1834 erbaut, die bis 1849 die weltweit längste ihrer Art gewesen war. Die neue Galternbrücke ersetzte 1960 eine erste Hängebrücke von 1840, überspannt den Galterngraben und verbindet die Stadtteile Schönberg und Bürglen (Bourguillon). Die 1920 erstellte Pérollesbrücke gewährleistet schliesslich eine direkte Verbindung vom Stadtteil Pérolles nach Marly.

Das Stadtwappen von Freiburg zeigt in Blau einen Zinnenturm mit links angebauter zinnenbekrönter, in zwei Stufen abfallender Mauer mit einem unten hervorbrechenden halben Ring, alles in Silber. Obwohl bereits seit dem 13. Jahrhundert verwendet, wurde das Wappen nach verschiedenen Umgestaltungen erst 1803 zum offiziellen Wappen der Stadt erklärt. Die drei Türme des Stadtwappens verkörpern die Ehemaligen Regierenden Stadtpanner, Burgpanner, Neustadtpanner, Spitalpanner, und der Silberne Ring verkörpert den vierten Panner, das Aupanner das an der Saane liegt.

Seit Gründung der Stadt Freiburg im Jahr 1157 wurde dieses Wappen mehrmals verändert. Früher hatte das Wappen oberhalb der Türme noch den Zähringeradler, später wurde es in ein vierteiliges Wappen (zweimal Stadtwappen, zweimal Kantonswappen übers Kreuz) aufgeteilt, das zu den offiziellen Kantonsfarben Schwarz-Blau führte. Die heutigen Kantonsfarben Schwarz und Weiss wurden erst mit dem Zusammenbruch der ehemaligen Stadtpanner eingeführt; bis dahin wurden Stadt- und Kantonalwappen zusammen geführt.

Gut zu sehen sind die alten Kantonsfarben in der Tracht des Wappens des Sensebezirks sowie in den alten Regimentsfahnen (z. B. Regiment Oberlandrist – Oberer Schrot Düdingen), die alle schwarz-blau geflammten Hintergrund hinter dem eidgenössischen Kreuz hatten.

Das doppelt geführte Wappen kann bei dem alten Schild des Gasthofes «Aigle Noir (Alpenstrasse)» in der Freiburger Innenstadt noch besichtigt werden.

Der bekannteste Sportverein der Stadt ist der Eishockey-Club HC Fribourg-Gottéron, der in der National League A spielt und bisher viermal Schweizer Vizemeister wurde. Die Spiele werden in der BCF-Arena (Kapazität: 6800 Zuschauer) ausgetragen.

Der Basketballklub Benetton Fribourg Olympic ist ein weiterer Vorzeigeverein. In der Schweiz ist der Basketball im Vergleich zu Eishockey und Fussball eher ein Randsport – vor allem im deutschsprachigen Teil. Der Publikumsaufmarsch im «Heimstadion» (Sporthalle St. Leonhard, bis 2010 Turnhalle des Kollegiums Heilig Kreuz) von bis zu 3'500 Zuschauern wird national auf Klubebene nur noch in Fussball- und Eishockeystadien übertroffen. Auch sportlich (unter anderem 13-facher Meister: 1966, ’71, ’73, ’74, ’78, ’79, ’81, ’82, ’85, ’91, ’92, ’98, ’99 und 2007; sechsfacher Schweizer Cup-Sieger: 1967, ’76, ’78, ’97, ’98 und 2007 sowie 1. Liga-Cup-Sieger: 2007) ist der Verein nationale Spitze.

Der Fussball-Club FC Fribourg spielt in der 1. Liga, der höchsten Amateurklasse.

Seit 1933 findet am ersten Sonntag im Oktober der Murtenlauf (Course Morat-Fribourg) statt. Dieser zählt zu den bekanntesten und traditionsreichsten Volksläufen der Schweiz, mit jeweils Tausenden von Teilnehmern. Die Strecke ist rund 17 Kilometer lang, führt von Murten nach Freiburg und wird zum Gedenken an die Schlacht bei Murten gelaufen.





</doc>
<doc id="12724" url="https://de.wikipedia.org/wiki?curid=12724" title="Mammographie">
Mammographie

Mammographie bzw. -grafie ist eine Methode zur Früherkennung von Brustkrebs (Mammakarzinom), der in den meisten Ländern häufigsten Krebserkrankung der Frau. Die Mammographie ist in erster Linie ein Verfahren der Radiologie zur Diagnostik der weiblichen, gegebenenfalls aber auch der männlichen Brust, und wird als Synonym für die "Röntgenmammographie" verwendet. Alternative bildgebende Verfahren sind die Mammasonographie, die Tomosynthese sowie die Magnetresonanz-Mammographie.

Die weltweit erste klinische Mammographie fertigte der Chirurg Otto Kleinschmidt 1927 am Leipziger Uniklinikum an. Ein weiterer Pionier war Ende der 1950er Jahre Robert Egan am University of Texas M.D. Anderson Cancer Center, der 1964 ein Buch über Mammographie veröffentlichte. Die Breitenanwendung der Mammographie setzte aber erst nach einer umfangreichen klinischen Studie in New York ein, die 1966 veröffentlicht wurde (Leiter Philip Strax).

Die Untersuchung erfolgt an speziellen Röntgengeräten. Die angewendete Röntgenstrahlung ist eine weiche Strahlung mit einer Energie von ungefähr 25 bis 35 keV (Kiloelektronenvolt). In der Mammographie kommen praktisch kaum noch Film-Folien-Systeme, sondern hauptsächlich digitale Röntgengeräte zum Einsatz. Letztere teilen sich in Speicherfoliensysteme und Flachdetektorsysteme auf. Diese wiederum teilen sich auf in direkt digitale Detektoren, bei denen die Röntgenstrahlung direkt in ein elektrisches Signal umgewandelt wird und in indirekt digitale Detektoren, bei denen die Röntgenstrahlung erst in sichtbares Licht und dann in ein elektrisches Signal umgewandelt werden. Die Röntgenaufnahmen werden auf einer speziellen Mammographie-Befundstation betrachtet, welche sich im Wesentlichen durch zwei Graustufenmonitore mit jeweils 5 Megapixeln auszeichnet oder aber einen Graustufenmonitor mit mindestens 10 Megapixeln.

Bei der klassischen 2D-Mammographie besteht das Problem, dass durch die Überlagerung verschiedener Gewebestrukturen mitunter krankhafte Veränderungen im Gewebe verdeckt werden, wodurch sie übersehen werden können. Dies versucht man auszugleichen, indem man aus zwei Winkeln die Brust aufnimmt, und zwar „craniocaudal“ und „mediolateral oblique“, d. h. einmal senkrecht und einmal im 45-Grad-Winkel. Dies hilft, löst das Problem jedoch nicht immer. Bei der Tomosynthese wird die Brust aus unterschiedlichen Winkeln aufgenommen, und zwar je nach Hersteller über einen Winkel von 15 bis 50 Grad. Dabei werden zwischen 9 und 25 Aufnahmen mit niedriger Dosis und hoher Beschleunigungsspannung aufgenommen, sodass die Gesamtdosis in etwa der einer klassischen 2D-Aufnahme entspricht. Aus diesen Aufnahmen werden einzelne Schichten des Brustgewebes errechnet. Bei 1-mm-Schichten zum Beispiel werden für eine 5 cm dick komprimierte Brust 50 Schichten errechnet. Da nun die Schichten über und unter der jeweils zur Ansicht ausgewählten Schicht bei der Befundung ausgeblendet werden, sind Gewebeveränderungen leichter zu sehen. Da zur Befundung aber auch die konventionelle 2D-Aufnahme gebraucht wird, diese aber nicht zusätzlich zur Tomosynthese erstellt werden soll, da dann zusätzliche Röntgenstrahlung für die 2D-Aufnahme gebraucht wird (doppelte Dosis für einmal Tomosynthese und einmal 2D), wird bei einigen Herstellern inzwischen aus dem Datensatz der 3D-Tomosynhese auf ein 2D-Bild zurückgerechnet (synthetische 2D-Mammographie).

Die drei größten Studien zum Thema sind:


Jede Brust wird aus zwei, gegebenenfalls auch mehreren Richtungen aufgenommen. Die beiden am häufigsten und üblicherweise angewendeten Projektionen sind die cranio-caudale Abbildung (Röntgenbild der Brust von oben) und die MLO-Aufnahme (mediolateral oblique, Röntgenbild der Brust von der Mitte nach außen). Während der Aufnahme wird die Brust zwischen dem Objekttisch und einer Plexiglasplatte moderat komprimiert. Dies ist notwendig, um die Strahlendosis gering zu halten und die zu untersuchende Brustregion bestmöglich abzubilden. Die Untersuchung wird von einem Teil der Patientinnen als unangenehm empfunden.
Die weiche Strahlung führt zu kontrastreicheren Aufnahmen, als sie bei anderen Röntgenuntersuchungen mit "harter" Strahlung möglich wären. Mit der Untersuchung können kleine, nicht tastbare Gewebeformationen sowie Mikrokalk erkannt werden. Die Röntgenbilder werden durch den untersuchenden Arzt ausgewertet.

CAD-Systeme (Computer-assisted Detection) können den Radiologen bei der Auswertung von Röntgenaufnahmen unterstützen. Sie sind in den USA und den Niederlanden abrechnungsfähig. Studien zufolge verbessern die bislang verfügbaren Geräte die Erkennungsrate jedoch nicht. In den europäischen Screeningprogrammen wird daher die Doppelbefundung durch zwei Ärzte eingesetzt.

Die Mammographie kann zur weiteren Abklärung tumorverdächtiger Befunde eingesetzt werden, meist ein tastbarer Knoten oder eine Sekretion aus der Brustwarze. Unter Fachleuten ist dafür der Begriff "kurative Mammographie" gebräuchlich. Das Ergebnis der Untersuchung fließt in die Planung der weiteren Therapie ein.

Die Mammographie wird auch zur Früherkennung von Brustkrebs eingesetzt. Ziel dieser Screening-Maßnahme ist es, durch möglichst frühzeitiges Erkennen die Lebenserwartung an Brustkrebs erkrankter Frauen zu verlängern. Zu diesem Zweck führt man in einigen Staaten (Niederlande, Schweden, Finnland seit 1974; Großbritannien seit 1979) organisierte Reihenuntersuchungen an Frauen ohne Symptome durch. Damit konnte angeblich die Sterblichkeit um 25–30 % gesenkt werden.

Neuere Untersuchungen relativieren diesen Nutzen inzwischen aber. Eine Metaanalyse aus dem Jahre 2013 führt die ursprünglich gefundenen positiven Effekte auf die Sterblichkeit, auf mangelhafte Randomisierung der Patientenkollektive zurück. Die Studien, in welchen dies korrekt durchgeführt wurde, fanden hingegen keine signifikanten Unterschiede in der Krebssterblichkeit nach 10 Jahren und in der Gesamtsterblichkeit nach 13 Jahren.

Weiters wurde in einer kanadischen Studie über einen Zeitraum von 25 Jahren mit einer zufälligen Zuteilung von insgesamt fast 90 000 Frauen zu entweder einer Gruppe mit jährlicher Mammographie und klinischer Untersuchung oder einer Gruppe mit ausschließlich einer jährlichen klinischen Untersuchung, kein signifikanter Unterschied in der Brustkrebssterblichkeit zwischen den beiden Gruppen festgestellt.

Die Inzidenz von Brustkarzinomen steigt durch das Screening an, da es zu "Überdiagnosen" kommt, d. h. auch Karzinome entdeckt werden, an denen die Frau nicht gestorben wäre. Die Früherkennung kann eine Erkrankung an Brustkrebs nicht verhindern (wie z. B. Impfungen). Sie dient in erster Linie dazu, den Brustkrebs in einem frühen Stadium zu erkennen und weniger invasiv behandeln zu können.

In Deutschland wurde auf Beschluss des Bundestags seit 2005 ein "nationales Mammographie-Screeningprogramm" unter Beachtung der entsprechenden europäischen Leitlinie aufgebaut. Im Unterschied zur kurativen Mammographie erfolgt das Screening nur in wenigen hochspezialisierten Zentren und mit extrem aufwändigem Qualitätsmanagement. Nur speziell geschulte Radiologen bzw. Gynäkologen, Radiologieassistenten und Pathologen dürfen an dem Programm teilnehmen. Die Qualifikation muss durch jährliche Prüfungen erneut nachgewiesen werden. Die Erfolgsquoten werden durch spezielle übergeordnete Zentren, den sogenannten Referenzzentren, überprüft. 

Die gesamte Bevölkerung der 50- bis 69-jährigen Frauen wird über die Einwohnermelderegister angeschrieben. Im Dezember 2009 nahm die letzte der insgesamt 94 Einheiten den Betrieb auf. Bis Dezember 2009 wurden 9,2 Millionen Frauen zum Screening eingeladen, 54 % ließen sich untersuchen.
Um auch Frauen außerhalb von Ballungszentren zu erreichen, wird der ländliche Raum zum Teil durch Sattelauflieger mit eingebautem Mammographiegerät, so genannten "Mammobilen", erschlossen.

Durch radiologische und pathologische Doppelbefundung wird erreicht, dass die Rate kleiner Karzinome und präinvasiver Läsionen (z. B. DCIS) hoch ist und andererseits möglichst wenige Biopsien gutartiger Mammatumoren durchgeführt oder diese operativ entfernt werden. Die EUREF-Richtlinie verlangt mindestens 50 % bösartige Tumoren bei den Bioptaten; manche Programme erreichen bis zu 80 %.

Eine Qualitätssicherung hat die Senkung der Rate falsch-positiver oder übersehener Befunde zum Ziel. Speziell ausgebildete Radiologen, die in der Beurteilung an vielen Mammogrammen geübt sind, können sowohl die Spezifität als auch die Sensitivität stark verbessern.
Unerlässlicher Bestandteil des Mammographie-Screenings sind auch spezialisierte Pathologen, die entnommene Proben (z. B. Vakuumbiopsie) histologisch beurteilen. Zu der individuellen Diagnosefindung geben sie dem Radiologen auch eine Rückmeldung über die Richtigkeit seiner Befundinterpretation (Korrelation der Befunde) in einer sogenannten "Screening-Einheit". Das sind Zentren, die auf das Mammographie-Screening spezialisiert sind und eine gültige Zulassung (Zertifikat) hierfür besitzen.
Ärzte, die die strengen Kriterien der EuRef-Norm für Mammographieuntersuchungen erfüllen, bekommen dafür ein Zertifikat verliehen, das jährlich erneuert werden muss.
Frauen, die eine Mammographie durchführen lassen wollen oder müssen, sollten sich im Vorfeld erkundigen, ob der Röntgenarzt ein solches Zertifikat besitzt. Dies gibt ihnen weitgehende Sicherheit, sowohl was das Strahlenrisiko und die Bildqualität angeht, als auch bezüglich der Qualifikation des Arztes und seiner Mitarbeiterinnen.
Weiter dienen der Qualitätssicherung ein Brustkrebsregister und eine Qualitätssicherung der technischen Apparate.

Für Frauen unter 40 bis 50 Jahren scheint die Screening-Mammographie nach derzeitigem Stand des Wissens keinen Vorteil zu bringen, da der Anteil an Befunden umso höher wird, je jünger die Frau ist. Dies ist unter anderem mit der höheren Gewebedichte der Brüste jüngerer Frauen zu erklären, die die allgemeine Beurteilbarkeit der Röntgenaufnahme erschwert. Das deutsche Screeningprogramm lädt daher alle Einwohnerinnen von 50 bis 69 Jahren alle zwei Jahre zur Untersuchung ein.

Kritiker argumentieren, dass die " relative Risikoreduktion" oft missverstanden, bzw. der erwartete Nutzen für die Teilnehmerinnen überschätzt werde und schlussfolgern daraus unnötige Untersuchungen bei Berücksichtigung der Strahlenbelastung.
Ähnlich wie bei der unumstrittenen Vorsorgeuntersuchung gegen einen langsam wachsenden Gebärmutterhalskrebs (sog. „Abstriche“), ist es sehr wahrscheinlich, dass eine Frau, die regelmäßig zur Mammographie geht, niemals einen Krebs entwickelt und damit keinen Nutzen von der Untersuchung hat. Die Gesamtsterblichkeit von Frauen an Krebs (alle Arten) ändere sich durch Teilnahme an einem Mammographie-Screeningprogramm nicht. Es gibt keine effektive Möglichkeit zu ermitteln, welche Frau einen Nutzen von der Mammographie haben wird.
Ein wesentliches Problem des Brustkrebs und seiner Vorstufen ist, dass er in der Regel erst (relativ groß) erkannt wird, wenn ein fortgeschritteneres Tumorstadium vorliegt. Das Ziel des strahlenfreien Mammasonographie- und des Mammographie-Screenings ist jedoch nicht nur, die allgemeine Sterblichkeit an Brustkrebs zu verringern, sondern vor allem auch Tumoren in einem früheren Stadium zu entdecken und somit die Überlebenszeit und die Lebensqualität der betroffenen Frau zu verbessern.

Wie jeder Test liefert auch die Mammographie , also einen Krebsverdacht, obwohl kein Krebs vorhanden ist. Laut Christa Halbwachs von der "Austrian Breast Imaging Study Group" habe die Mammographie eine Sensitivität (Richtig-Positiv-Rate) von 83 % und eine Spezifität (Richtig-Negativ-Rate) von 97 %. Jeder krebsverdächtige Befund im Screening sollte standardmäßig abgeklärt werden, entweder durch eine Vakuum- oder Stanzbiopsie oder durch eine zeitnahe mammographische Kontrolluntersuchung (z. B. in sechs Monaten). Nur in Ausnahmefällen wird eine offene Biopsie (d. h. eine Operation) zur Diagnosesicherung vorgenommen. Diese falsch-positiven Befunde können für die betreffende Frau psychisch sehr belastend sein. Von vielen Frauen wird eine histologische Abklärung mit abschließender Entwarnung (also eine Sicherung der Diagnose) als Erleichterung empfunden, auch wenn die Untersuchung im Nachhinein unnötig war.
Im deutschen Screeningprogramm beträgt das Verhältnis von benignen zu malignen Biopsien 1:2,3, bei der Magnetresonanztomographie ist es viel ungünstiger.

Die Mammographie selbst kann, da es sich um ionisierende Strahlung handelt, zumindest statistisch Karzinome hervorrufen. Die Häufigkeit ist jedoch nicht direkt messbar, es existieren nur Daten historischer Untersuchungen, die das theoretische Risiko auf 0,01 % schätzen.

Der Brustkrebs ist keine einzelne Erkrankung, sondern besteht aus einer heterogenen Gruppe verschiedener Tumoren mit unterschiedlicher Prognose. Grundsätzlich ist die Überlebensrate von bestimmten Faktoren abhängig. Dazu zählen Tumorgröße, Absiedlungen in den Lymphknoten der Achselhöhle, Fernmetastasen (TNM-Klassifikation), histologischer Differenzierungsgrad und die Art der Tumortherapie.

Screening-Kritiker weisen darauf hin, dass unter anderem durch die mammographische Reihenuntersuchung auch Karzinome entdeckt werden, die – wären sie nicht in der Mammographie aufgefallen – einen nicht lebensbedrohlichen Verlauf nehmen könnten (indolenter Tumor). Dadurch, so die Kritik, würden unnötige Operationen und Krebstherapien durchgeführt, die die Lebensqualität der Patientin einschränken, auch wenn sie unbehandelt nicht an Brustkrebs gestorben wäre.
Da der individuelle Verlauf einer Krebserkrankung nicht mit ausreichender Sicherheit vorhergesagt werden kann, ist diese Haltung unter Experten stark umstritten.
Italienischen Forschern zufolge ist der Nutzen des Screenings größer als die Gefahr der Überdiagnostik.
Befürworter des Screening-Programmes verweisen auf die gesunkene Mortalität (Sterblichkeit) seit Einführung des Screenings. Die Sterblichkeit bei Frauen über 70 Jahren ist dabei jedoch nicht verringert, d. h. die Frauen profitierten aufgrund ihres Lebensalters nicht von der Untersuchung. Die obere Altersgrenze für die Teilnahme am Mammographie-Screening liegt deshalb bei 70 Jahren.

In Zusammenhang mit indolenten Tumoren wird häufig auch von unnötigen Operationen gesprochen, die zu einer Stigmatisierung oder Verstümmelung führen könnten.
Tatsächlich stellt die weibliche Brust besondere Anforderungen an den Operateur, um sowohl bei brusterhaltender Therapie als auch bei einer Mastektomie ein onkologisch gerechtes und gleichzeitig kosmetisch ansprechendes Ergebnis zu erzielen. Der Anspruch, ein möglichst gutes kosmetisches Ergebnis bei jeder Operation zu erzielen, ist auch in den S3-Leitlinien zur Behandlung des Mammakarzinoms verankert. Deshalb wird generell die Behandlung in einem zertifizierten Brustzentrum empfohlen.






</doc>
<doc id="12725" url="https://de.wikipedia.org/wiki?curid=12725" title="Limes">
Limes

Limes (lat. "limes" ‚Grenze, Grenzwall‘) bezeichnet

in der Mathematik:

LIMES steht als Abkürzung für:
Siehe auch:


</doc>
<doc id="12727" url="https://de.wikipedia.org/wiki?curid=12727" title="Nipkow-Scheibe">
Nipkow-Scheibe

Die Nipkow-Scheibe ist die Grundlage des von Paul Nipkow (1860–1940) erfundenen und so bezeichneten „Elektrischen Teleskops“, einer frühen Form des Fernsehens. Das Patent mit der Nummer 30105 wurde am 15. Januar 1885 vom kaiserlichen Patentamt publiziert und rückwirkend auf den 6. Januar 1884 datiert. Die von Paul Nipkow vorgeschlagene Spirallochscheibe hatte 24 Löcher, die ein Bild mit 24 Zeilen schreiben sollte. Mit ihrer Hilfe konnte es Bilder in Hell-Dunkel-Signale zerlegen und wieder zusammensetzen. Die rotierende Scheibe wandert dazu zeilenweise am Bild (bei der Zerlegung) bzw. der Projektionsfläche (bei der Zusammensetzung) vorbei.
Die Löcher der Nipkow-Scheibe sind entlang konzentrischer Kreise angebracht. Die einzelnen Löcher tasten somit von außen nach innen jeweils eine Lochbreite ab. Damit immer nur ein Loch innerhalb des interessanten Ausschnittes ist, darf sich in einem Kreissegment jeweils nur ein Loch befinden. Mit anderen Worten, die Scheibe ist in so viele Kreissegmente eingeteilt, wie sie Zeilen abtasten muss. Durch diese Vorgehensweise wird das Bild sequenziell abgetastet. Mit einer Nipkow-Scheibe erzeugte Bilder erkennt man infolgedessen (etwa auf alten Fotos) an den leicht bogenförmigen Zeilen.

1928 gelang es John Logie Baird in England das erste Mal, mit der Nipkowscheibe und RGB-Farbfiltern ein 30-zeiliges Farbbild zu übertragen.

Mit zunehmender Zeilenzahl erwies sich die Nipkow-Scheibe als immer unbrauchbarer, wie die folgende Überlegung zeigt:

Die mechanischen Schwierigkeiten waren zudem noch die geringeren. Das eigentliche Problem lag in der extrem geringen Lichtausbeute. Es ließ sich errechnen, dass selbst bei Studioaufnahmen mit einer Beleuchtung von 70.000 Lux nur eine Leuchtdichte erwartet werden durfte, die um vier Größenordnungen unter jener beim chemischen Film lag. 70.000 Lux, der im Freien maximal vorkommende Wert, war dabei für eine dauerhafte Studiobeleuchtung bei weitem nicht mehr praktikabel.

So erreichte die mechanische Abtastung schon im Jahre 1939 mit 441 Zeilen ihre technische Grenze, und dies mit einem bereits sehr großen Aufwand. Die höheren Zeilenzahlen in den 1940er Jahren konnten nur noch durch eine rein elektronische Abtastung erreicht werden.

Die niedrige Auflösung hatte jedoch den Vorteil, dass sie auch nur eine niedrige Bandbreite benötigte; Fernsehbilder konnten sogar über Mittel- oder Kurzwelle ausgestrahlt werden. Im Amateurfunk konnte daher mechanisches Fernsehen, überwiegend aufgrund der Einfachheit realisiert mit Nipkow-Scheiben, in der Form von Narrow Bandwidth Television als Nischenanwendung überleben.

Schon sehr früh versuchte man, die technischen Grenzen der Nipkowscheibe zu umgehen. John Logie Baird baute schon 1927 eine Nipkowscheibe, bei der die Löcher gegen deutlich größere Linsen getauscht wurden, um so eine größere Lichtausbeute zu erzielen. Da diese Bauform besonders beim Empfänger das Gerät stark verteuert, außerdem die Neonlampen besser und stärker wurden, war diese Bauform nur selten anzutreffen.

Versuche, statt einer Scheibe ein Endlosband zu verwenden, wurden aufgrund der hohen mechanischen Beanspruchung des Bandes schnell verworfen.

Für höher auflösendes Fernsehen (180 Zeilen und auch mehr) wurden auf der Nipkowscheibe mehrere Lochreihen angebracht, gleichzeitig rotierte eine Blende mit der Nipkowscheibe, so dass immer nur ein Loch gleichzeitig frei war. Auf diese Weise konnte die Scheibe wesentlich kleiner gebaut werden. Da entsprechend die Umdrehungszahl gesteigert werden musste, befand sich die Nipkowscheibe in einem Vakuumbehälter.

Ab 1938 wurden speziell zur Filmabtastung sogenannte Lochkränze oder auch Linsenkränze eingesetzt. Dies sind von der Funktionsweise auch Nipkowscheiben, aber mit dem Unterschied, dass der Abtaster trommelförmig war und die Löcher in die Seite eingestanzt wurden. Hierdurch entfällt die typische Krümmung der mit einer Nipkowscheibe erzeugten Bildzeilen. Gleichzeitig kam man mit sehr wenigen Löchern aus, die zudem nur auf einer Ebene angebracht waren. Der Kinofilm wurde nicht ruckartig wie in einem Kinoprojektor, sondern flüssig vor dem Lochkranz bewegt, so dass auf diese Weise das Bild zeilenweise abgetastet wurde. Auf diese Weise konnten sogar noch Filme in der damaligen Norm mit 441 Zeilen abgetastet werden.

Zwischen 1928 und 1935 gab es insbesondere in den USA und Großbritannien fertige Geräte und sogar Bausätze zu kaufen. Der populäre „Televisor“ mit einer Auflösung von 30 Zeilen von Baird kostete 1932 in Großbritannien beispielsweise 27 Pfund, in Deutschland wurde um 1930 ein Bausatz für rund 30 Reichsmark vertrieben. Bei mehr als 60 Zeilen waren damals jedoch die technischen Grenzen erreicht, wo Empfänger unverhältnismäßig teuer wurden. Die Bildqualität bei nur 30 Zeilen reicht jedoch nur für die Übertragung von Porträts, außerdem flimmerten die Bilder bei zunächst nur 12,5 Bildern pro Sekunde erheblich, so dass ein kommerzieller Erfolg versagt blieb. Bis zum Jahre 1938 wurden in den Fernsehstationen gelegentlich noch Nipkow-Scheiben verwendet, in der deutschen Fernsehnorm mit 180 Zeilen und in den Fernsehsprechzellen (einem Versuchsdienst von Fernsehtelefon) wurden sogar Kameras mit Nipkowscheiben eingesetzt, für die Filmabtastung wurden Nipkowscheiben sogar noch später eingesetzt, auch im hochauflösenden Fernsehen mit 441 Zeilen wurden Nipkowscheiben bis in die 1940er Jahre eingesetzt.

Mit schnellrotierenden Scheiben im Vakuum konnten Bilder bis zu 441 Zeilen abgetastet werden. Eingesetzt wurden sie von der Reichspost und vom Fernsehsender Paul Nipkow.

Neben der Nipkow-Scheibe gab es noch zahlreiche ähnlich arbeitende Verfahren: Das "Weillersche Spiegelrad" besaß Spiegel anstatt Löcher, es ermöglichte einen Lichtstrahl zeilenweise abzulenken. Hierfür ist ein dunkles Studio erforderlich, in dem eine Kamera ohne Ablenkeinheit arbeitet. Die Bildpunktzerlegung geschieht dadurch, dass der Lichtstrahl die Szene nacheinander abtastet und so für die Bildzerlegung sorgt. Umgekehrt funktioniert der "Linsenkranzabtaster", hier sind an dem Rad Linsen anstatt Spiegel angebracht, die für eine Bildzerlegung in der Kamera sorgen. Dieses Verfahren erfordert allerdings eine enorme Präzision.

Vollkommen anders funktioniert die 'Spiegelschraube', wo in einer Helix angeordnete Spiegel das Licht einer Lichtquelle reflektieren, und so beim Betrachter den Eindruck eines Bildes ergeben. Dieses System wurde in den Jahren 1930 bis 1935 von der Firma TeKaDe bis zu einer Auflösung von 180 Zeilen eingesetzt. Sehr viel einfacher war das ebenfalls von Baird entwickelte System der Spiegeltrommel. Auf einer schnell rotierenden Trommel wird für jede Zeile ein Spiegel angebracht, jeder Spiegel ist leicht versetzt, so dass das Bild zeilenmäßig abgetastet wird. Diese Abtastung ist zwar sehr viel lichtstärker, so dass auch größere Bilder realisierbar waren, allerdings ist die maximale Auflösung ebenfalls beschränkt auf niedrigzeiliges Fernsehen.

Verbessert wurde das System durch die Firma Scophony aus England, welche zwei Spiegeltrommeln gegeneinander laufen ließ, eine für die horizontale, eine für die vertikale Ablenkung. Mit diesem System wurden Auflösungen bis zu 405 Zeilen, angeblich im Versuchsbetrieb in den USA bis zu 525 Zeilen erreicht. Diese Geräte erreichten zwar eine um 1939 unerreichte Bildqualität, Helligkeit und Größe, allerdings waren die Empfänger um ein Vielfaches teurer als ein Fernseher mit Kathodenstrahlröhre. Im Krieg wurde die Entwicklung eingestellt, und nach Kriegsende nicht wieder aufgegriffen.

Bereits 1906 gelang es Max Dieckmann, ein Fernsehbild mit Hilfe einer Braunschen Röhre darzustellen, zur Bilderzeugung wurde allerdings eine Nipkow-Scheibe eingesetzt. Auf der Funkausstellung 1931 stellte Manfred von Ardenne ein vollelektronisch arbeitendes Fernsehsystem mit vor. Die Wiedergabe ermöglichte eine Hochvakuum-Kathodenstrahlröhre (Braunsche Röhre) mit 25 cm × 28 cm großem Bildschirm. Als Kamera diente eine zweite Kathodenstrahlröhre mit konstanter Helligkeit (Abtaströhre, wurde zusammen mit der Ulbrichtschen Kugel auch für Farbdia- bzw. -filmabtaster benutzt). Vor dieser befand sich eine Photozelle, deren Signal zur Modulation der Helligkeit der Empfängerröhre benutzt wurde. Ein Dia oder verschiedene Gegenstände wie eine Schere wurden zwischen „Kameraröhre“ und Photozelle gehalten. Dessen ausgezeichnete Qualität war von allen mechanischen Ablenkungssystemen unerreicht. So setzte sich dieses Verfahren schließlich 1932/33 durch: Auf der 12. Deutschen Rundfunkausstellung 1935 gab es 20 verschiedene Fernsehapparate zu sehen, von denen bereits 18 elektronisch arbeiteten, die übrigen beiden mit bewegten Spiegeln. 1938 verschwand schließlich die letzte Nipkowscheibe aus den deutschen Fernsehstudios, allerdings wurden zur Filmabtastung noch bis ungefähr 1941 mechanische Abtaster verwendet.

Eine besondere Form der Nipkowscheibe wird heute in der Raumfahrt verwendet: Zahlreiche Raumsonden verwenden anstelle einer Kamera nur eine einzelne Fotozelle. Durch die Drehung um die eigene Achse wird so eine Bildzeile abgetastet. Da sich die Raumsonde gleichzeitig vorwärts bewegt, kann so ein komplettes Bild übertragen werden. Es handelt sich allerdings hierbei nicht um Fernsehen im eigentlichen Sinne, da auf die beschriebene Weise ein „Endlosbild“ der Reisestrecke der Sonde übertragen wird, keine Folge bewegter Bilder.

Die Nipkow-Scheibe wurde neben der Abtastung für Bildübertragungen auch bei der Erfindung des konfokalen Mikroskops von Mojmir Petran verwendet. Gerade in den letzten Jahren werden in der konfokalen Mikroskopie vermehrt Mikroskope mit Nipkow-Scheiben verwendet.




</doc>
<doc id="12729" url="https://de.wikipedia.org/wiki?curid=12729" title="Mechanisches Fernsehen">
Mechanisches Fernsehen

Mechanisches Fernsehen ist eine zusammenfassende Bezeichnung, ein Gattungsbegriff für das erste bestehende Fernsehverfahren. Es umfasste die technischen Prozesse der Funktion von Studio- und Sendeanlagen bis hin zu den Empfängern. Das spätere System, welches dieses erste Fernsehen der 1920er bis 1940er Jahre ablöste, war das voll-elektronische Fernsehen, das bis in die Gegenwart besteht.

Der Begriff mechanisches Fernsehen ist bisher nicht aktualisiert worden, obwohl es ein Fernsehverfahren ist, das mit
Bauelementen arbeitet. Im Bereich des Amateurfernsehens wird teilweise das Prinzip dieses ursprünglichen Fernsehens heute noch angewendet.

Um ein Bild vom Studio bis zur heimischen „Mattscheibe“ zu übertragen, sind zahlreiche Schritte notwendig. Das mechanische Fernsehen umfasst die ersten Versuche der Bildübertragung. Die Grundlage dafür legten die Pioniere der Bildtelegraphie (historisch: "Kopiertelegraphie") mit ihren Arbeiten, um unbewegliche Bilder zu übertragen. So entwickelte bereits in der ersten Hälfte des 19. Jahrhunderts u. a. Frederick Collier Bakewell eine rotierende Trommel, ummantelt mit einer Folie aus Metall und einem elektrischen Kontakt, der ein darauf angebrachtes Bild spiralförmig abtastet. Die Weiterentwicklung dieses Trommel-Prinzips für bewegte Bilder war ein wichtiger Weg des mechanischen Fernsehens, das zu diesem Zeitpunkt „electrisches Fernsehen“ hieß.

Die erste brauchbare Realisierung des wichtigen Schrittes des Prozesses Fernsehen, der Bildzerlegung, erfand 1883 Paul Nipkow. Zuvor stellte er sich drei Aufgaben:

Die Ausgangslage beschreibt er so: „Apparate, die etwas derartiges leisteten, hatte man bereits in den Kopirtelegraphen; man konnte in der That mit Hülfe derselben ein unter gewissen Umständen gezeichnetes Bild in die Ferne übertragen.“ Auf dem Weg zu seiner eigenen Erfindung waren für ihn das auch auf einer Drehscheibe basierende (und später mehrfach weiterentwickelte) Telectroscope von 1877 sowie Constantin Selencq’s d’Ardres und Shelford Bidwell’s 1881er “Tele-Photography” mit einem Zylinder als drehendes Bauteil die wichtigsten Erfindungen.
Nipkows nach ihm benannte rotierende Scheibe ist das Herzstück seines „Elektrischen Teleskopes“. Allerdings ist dies schon strenggenommen kein mechanisches Bauteil mehr, sondern ein optisches. An ein wirkliches „Fern-Sehen“ war damit aber noch nicht zu denken, denn die sich drehende bildzerlegende Scheibe des Senders und die bildzusammensetzende Scheibe des Empfängers liegen noch auf ein und derselben Achse. Eine Synchronisierung beider Scheiben war zwar mit dieser Apparatur auf Entfernung noch nicht möglich, doch das Revolutionäre daran war, dass ein Bild, mit Hilfe der Technik in elektrischen Strom umgewandelt, durch einen Draht „floss“ um wieder als Bild zu erscheinen. Unter dem Namen „Elektrisches Teleskop“, meldete Paul Nipkow 1884 sein Patent beim Kaiserlichen Patentamt an, das dann am 15. Januar 1885 erteilt wurde. Aus Geldmangel jedoch verfiel das Patent schon ein Jahr später und konnte deshalb zahlreichen Fernsehpionieren als Grundlage für eigene Entwicklungen dienen.

Die Entwicklung des „electrischen Fernsehens“ beruht auf den Experimenten mehrerer Technikpioniere. Insbesondere ist die Arbeit Dénes von Mihálys und John Logie Bairds hervorzuheben. Der schottische Erfinder J. L. Baird setzte auf eine Bildzerlegungscheibe, während der ungarische Ingenieur D. von Mihály anfänglich ein vollkommen anderes Verfahren ohne Nipkow-Scheibe entwickelte, bei dem ein Spiegel zwischen einem Hufeisenmagneten schnell oszillierte. In einem verbesserten Verfahren wurde ein Spiegel auf Drahtsaiten befestigt, welche nach Stromdurchleitung in eine schnelle Schwingung versetzt wurden. Nach nur geringen Erfolgen verwendete auch Dénes von Mihály Nipkow-Scheiben; die ersten kommerziellen wurden um 1929 in Zusammenarbeit mit John Logie Baird gebaut. Ein wieder anderes Verfahren entwickelte August Karolus bei Telefunken, wo mit schnell rotierenden Spiegeln gearbeitet wurde. Durch Karolus wurden auch beide Systeme miteinander kombiniert; sprich das Spiegelrad für die horizontale Abtastung und der oszillierende Spiegel für die vertikale Abtastung.

John Logie Baird gebührt das Verdienst, den ersten voll funktionierenden Fernseher (mit passender Kamera), basierend auf einer Nipkow-Scheibe, im Jahre 1926 vorgestellt zu haben. Bairds „Television Development Company“ gelang außerdem die erste transatlantische Übertragung eines Fernsehbildes von London nach Hartsdale (N.Y.) 1928.

Weitere Pioniere des mechanischen Fernsehens waren für Frankreich René Barthélemy und für die USA Charles Francis Jenkins.

Sender nach dem Prinzip des mechanischen Fernsehens gab es hauptsächlich in Europa, Amerika und Australien. Versuchsprogramme wurden von 1927 an ausgestrahlt. Nach den ersten Übertragungen folgten 1929 regelmäßige Sendungen in mehreren US-amerikanischen Städten, im britischen London und im australischen Melbourne sowie 1931 in Moskau in der Sowjetunion. Ab 1932 gab es Ausstrahlungen unter anderem in Paris, Brüssel, Rom und an vier deutschen Standorten, Berlin-Witzleben, Döberitz und Königs Wusterhausen bei und in Berlin.

Bei der Kommerzialisierung des mechanischen Fernsehens setzte sich für die Bildzerlegung und für die Empfangsgeräte die Nipkow-Scheibe durch, welche auf der Senderseite bis in die frühen 1940er Jahre zur Bildzerlegung von Filmen Verwendung fand.

Auf Empfängerseite wurden bis ca. 1935 Fernseher mit Nipkow-Scheibe verkauft. Logie Baird vertrieb ab 1928 seinen "Model B Televisor". Da die Signale über die Frequenz- bzw. Wellenbereiche der Lang-, Mittel- oder Kurzwelle übertragen wurden, waren beachtliche Reichweiten möglich, die das vollelektronische Fernsehen bis heute nicht erreichen konnte.

Die Ausstattung der Empfänger war unterschiedlich. Es gab sowohl Gesamtempfänger wie auch Zusatzempfänger. Letztere waren beispielsweise die sowjetischen B-2-Geräte, die lediglich das Bild wiedergaben und an ein Kurz- oder Mittelwellenradio angeschlossen werden mussten, um über dieses das Bild vom Sender empfangen zu können. Der Empfang des Tones konnte mit einem zweiten Radio bewerkstelligt werden. Die Empfangsgeräte waren durch ihren schlichten Aufbau von Amateuren durchaus selbst herzustellen. Daher erfreuten sich bei Bastlern die Geräte großer Beliebtheit. Es wurden Bausätze und Bauanleitungen verkauft, praktisch ausschließlich auch nur mit Nipkow-Scheiben. Besonders erfolgreich waren diese Eigenbaugeräte neben Großbritannien auch in den USA, wo allerdings nach einem kurzen Boom aufgrund der zahlreichen verschiedenen Normen und der weiteren Entwicklung zu hochauflösendem Fernsehen das mechanische Fernsehen bis 1933 schnell wieder an Bedeutung verloren hatte.

Der Fernsehdienst der BBC sendete in London mit 30 Zeilen bis 1935. Ebenso in den Niederlanden, hier sogar in der britischen Norm bis September 1939. Praktisch alle Empfänger in beiden Ländern funktionierten mit dem mechanischen Verfahren. Alle verkaufsfähigen Empfangsgeräte erster Generation mit Nipkowscheiben, in Großbritannien, Frankreich, den Niederlanden, der UdSSR wie den USA hatten diese gleiche Bildzeilenzahl eines erzeugten Fernsehbildes von 30 Zeilen. Auch in Deutschland waren bis zur Einführung der späteren Norm mit 180 Zeilen (verwendet bis 1938) viele mechanische Empfänger im Einsatz, allerdings war das Fernsehen in Deutschland für den Privatmenschen kaum verbreitet. Zwar gab es auf der Senderseite in den 1940er Jahren verfeinernde Weiterentwicklungen der mechanischen Bildzerlegung, bei der auch wieder Spiegel-Trommelsysteme zum Einsatz kamen, womit bildverbessernde Zeilenzahlen über 400 möglich wurden. Doch zu einer äquivalenten empfängerseitigen Weiterentwicklung kam es in den 1940er Jahren nicht mehr.

Logie Baird vermarktete seine Fernseher unter dem Kunstwort „Televisor“, welches bis in die 1950er Jahre im Vereinigten Königreich ein gängiger Ausdruck für einen Fernsehempfänger selbst war. Dieses Wort fand auch Eingang in andere Sprachen, beispielsweise im Russischen ist wie im Spanischen "Televis"or ein Wort für Fernseher. Unterschieden wird hierbei nicht zwischen mechanischer oder elektronischer Bildzerlegung.

Während auch beim niedrigauflösenden Fernsehen mit mechanischen Bildzerlegern überwiegend ein Bildseitenverhältnis von 1:1 oder 4:3 (auch beim vollelektronischen Fernsehen hielt man weltweit über 60 Jahre am Format 4:3 (12:9) fest, durch alle Qualitätsstufen und spätere Farbfernseh-Systeme, bis ab Mitte der 1990er-Jahre das Verhältnis 16:9 eingeführt wurde) und horizontalen Zeilen eingesetzt wurden, verwendete Baird eine vertikale Abtastung mit Bildern in Portraitformaten 2:3 und 3:7, also hochkante Bilder im "Portraitformat". Seine Überlegung war, dass das Fernsehen ohnehin nicht die notwendige Leistung zur Übertragung von Filmen und Landschaftsaufnahmen bietet, aber bestens zur Übertragung von Nahaufnahmen, also in der Regel Menschen, geeignet sei. Die Tatsache, dass die Norm Bairds sich länger behaupten konnte als andere niedrigauflösende Systeme und auch heute noch in abgewandelter Form beim Narrow Bandwidth Television weit verbreitet ist, spricht für die Richtigkeit dieser Überlegung.

Der Begriff „Mechanisches Fernsehen“ ist zwar international in vielen Ländern üblich und hat sich etabliert, obwohl kaum mechanische Verfahren zur Anwendung kommen. Als einziges mechanisches Element erscheint nur die sichtbar rotierende Nipkow-Lochscheibe. Ob sich der Begriff aufgrund der optischen Wahrnehmung entwickelt hat, dass dieses sich drehende wesentliche Funktionselement als ein mechanisches Bauteil bewertet wurde, ist nicht belegbar. Nipkow selbst bezeichnete seinen Apparat in der Patentschrift als ein „Elektrisches Teleskop“.

Die Nipkowsche Vorrichtung besteht größtenteils aus elektrisch betriebenen Bauteilen, von denen mindestens eines – die Selenzelle – bereits ein frühes elektronisches Bauelement darstellt. Des Weiteren erfolgt die Übertragung vom Sender zum Empfänger über ein Kabel, elektrisch. Selbst die Geschwindigkeit des Motors, der die Nipkowscheibe antreibt, muss mit einer elektronischen Schaltung, zumeist mit einer Elektronenröhre synchronisiert werden.

Auf der Empfänger-Seite wird bei der Nipkowschen Ausführung ein polarisierter Lichtstrahl von dem Magnetfeld einer Spule in Abhängigkeit von dem Signal der Sender-Selenzelle soweit gedreht und durch nachfolgende Linsen gefiltert, dass für das Auge der gleiche Helligkeits-Eindruck hervorgerufen wird, wie er auf der Sender-Seite besteht.
Dieser polarisierte Lichtstrahl wurde mit Hilfe eines speziellen optischen Glases, dem sogenannten „Nicolschem Prisma“, erzeugt. Alternativ wurde zur Sichtbarmachung der Helligkeitsschwankungen auch eine mit Schwefelkohlenstoff oder Neon gefüllte Röhre (eine „Glimmlampe“) verwendet. Eine Bildröhre war somit nicht erforderlich. Mit der Einführung dieser und anderer optischer Fernsehverfahren auch teilweise bereits mit ersten optoelektronischen Bauelementen, war schon 1929 der Begriff als mechanisches Fernsehen nicht mehr ausreichend.

Damit wird sichtbar, dass der Begriff des „mechanischen Fernsehens“ zu kurz gefasst ist. Allenfalls korrekt wäre noch die Begriffsbildung „mechanische Bildzerlegung“. Der international übliche Begriff mechanisches Fernsehen ist also eher als eine historische abgrenzende Bezeichnung für alle anderen Verfahren des Fernsehens und seiner Technik
die "nicht" auf Techniken vom vollelektronischen Fernsehen basieren.

Der Begriff ist so – letztlich – nur als Gattungsbegriff zur Abgrenzung zu verstehen. Da der Begriff aber nicht eindeutig ist und bisher nicht aktualisiert wurde, ist als Konsequenz eine einwandfreie Zuordnung von seither entwickelten Technologien jedoch oft problematisch.

Grundsätzlich ist durchaus ein vollmechanisches Fernsehen denkbar. Die Zerlegung würde auf beschriebene Weise mittels Nipkowscheibe oder Spiegelsystem erfolgen, die Übertragung des Bildpunktes würde dabei aber nicht elektrisch, sondern mittels Glasfaserkabel erfolgen. Hierzu wäre kein einziges elektrisches Bauteil notwendig, für den Antrieb würden z. B. Federwerke oder Dampfmaschinen eingesetzt werden, als Lichtquelle dient Tageslicht, Kerzen- oder Gaslicht. Mehrere Mitglieder der Narrow Bandwidth Television Association arbeiten derzeit auch an einem solchen Projekt, welches beweisen soll, dass zumindest grundsätzlich schon im 19. Jahrhundert Fernsehen möglich gewesen wäre.

Auch heute ist das mechanische Fernsehen nicht bedeutungslos. Die Nipkow-Scheibe und Spiegelsysteme finden heute wieder Verwendung beim Bau von Konfokalmikroskopen oder Videoprojektoren. Eine besondere Anwendung des mechanischen Fernsehens gab es auf der amerikanischen Weltraumsonde Pioneer 11: Da die Sonde sich um die eigene Achse dreht, wurde an der Außenseite eine einzelne lichtempfindliche Zelle angebracht, welche durch die Drehung der Sonde eine Zeile abtastet. Da die Sonde sich selbst ebenfalls bewegt, können so komplette Bilder abgetastet werden. Vom Funktionsprinzip ist diese Art der Bildabtastung also durchaus mit der Nipkow-Scheibe vergleichbar. Der Virtual Boy von Nintendo verwendete lediglich eine Bildzeile, zusammengesetzt aus 240 LEDs; durch einen rotierenden Spiegel entsteht der Eindruck eines kompletten Bildes. Als Vorläufer moderner Werbetafeln kann ein von Peter Schmalenbach entwickeltes Gerät gelten. Vier rotierende Leuchtdiodenleisten mit jeweils 232 Leuchtdioden schreiben alle 625 Zeilen der heutigen Fernsehnorm in die Luft. Dadurch entsteht ein vollständiges Fernsehbild. Die Maschine zeigt also z. B. das eingespeiste laufende Fernsehprogramm oder Videos auf einer virtuellen Bildtrommel. Knapp 122.000 Bildpunkte bewirken eine bemerkenswerte Auflösung.

Projekte des Amateurfernsehens wie NBTV.org, eine aus heutiger Sicht die extrem schmalbandige Variante ("englisch" Narrow Bandwidth Television) des Fernsehens mit einer Bandbreite von 2,5 kHz, beschäftigten sich in der Gegenwart mit den Erfindungen beispielsweise Bairds, fertigen Repliken ebenso wie völlige Neukonstruktionen mit Nipkow-Bildzerlegungsscheiben. Dies führt dabei sogar, aufgrund der erheblichen Verbesserung und Miniaturisierung von Bauelementen seit den Zeiten Nipkows und Bairds, immer wieder zu erstaunlichen Erfolgen. Auch Farbbildübertragungen, an denen bereits J. L. Baird experimentierte, sind möglich.

In Großbritannien wird seit vielen Jahren ein Bausatz für einen Televisor nach historischem Vorbild verkauft.



</doc>
<doc id="12730" url="https://de.wikipedia.org/wiki?curid=12730" title="Elektronisches Fernsehen">
Elektronisches Fernsehen

Als elektronisches Fernsehen bezeichnet man Fernsehen, bei dem Bildzerlegung und -zusammensetzung im Gegensatz zum mechanischen Fernsehen elektrisch erfolgen.

Erste brauchbare Grundlagen des elektronischen Fernsehens sind die von Wladimir Sworykin 1923/24 erfundene Ikonoskop-Röhre als erster brauchbarer elektronischer Bildabtaster und die 1897 erfundene Kathodenstrahlröhre, die nach ihrem Erfinder Ferdinand Braun auch Braunsche Röhre genannt wird. Letztere ist in weiterentwickelter Form bis heute das üblichste Hilfsmittel zur Bilderzeugung in Fernsehgeräten.

Als Erfinder des elektronischen Fernsehens gilt der Naturwissenschaftler Manfred von Ardenne.



</doc>
<doc id="12732" url="https://de.wikipedia.org/wiki?curid=12732" title="Kanton Freiburg">
Kanton Freiburg

Freiburg ( FR; , , , , ) ist ein Kanton im Westen der Schweiz. Er gehört sowohl der frankophonen Romandie als auch der Deutschschweiz an. Etwa zwei Drittel der Bevölkerung sprechen Französisch und knapp ein Drittel Deutsch, womit Freiburg zusammen mit Bern und dem Wallis zu den offiziell zweisprachigen Kantonen gehört. Der Hauptort ist die gleichnamige Stadt Freiburg (Fribourg).

Der Kanton Freiburg ist mit 1670,8 Quadratkilometern der achtgrösste Kanton der Schweiz. Topografisch lässt sich Freiburg in zwei grössere Bereiche unterteilen: Der nördliche und westliche Teil gehört zum Schweizer Mittelland, der Süden zu den Schweizer Voralpen (siehe dazu den Hauptartikel "Freiburger Voralpen").

Die höchste Erhebung bildet der Vanil Noir mit . Er befindet sich im Süden des Kantons im Greyerzbezirk, an der Grenze zum Kanton Waadt.
Der tiefste Punkt liegt auf , am Neuenburgersee "(Lac de Neuchâtel)".

Die Nachbarkantone sind der Kanton Waadt im Westen und Süden, der Kanton Bern im Osten, und im Norden grenzt Freiburg durch den Neuenburgersee an den Kanton Neuenburg.
Als Kantonswappen wurde der schwarz-weisse Schild erst spät genutzt. Ende des 19. Jahrhunderts entstand das Wappen mit heutiger Form (zuvor getragen von einem oder zwei Löwen).

Schwarz und Weiss als Wappen wurde erstmals mit einem weissen Kreuz im schwarzen Feld verwendet. Die dadurch entstandenen vier Abtrennungen im schwarzen Feld symbolisierten die vier Stadtpanner (Au, Burg, Neustadt, Spital), welche die regierenden Häuser des Staates waren. Das Kreuz wurde im Laufe der Zeit aus dem Wappen entfernt, was zum heutigen Schwarz-Weiss führte.

Bis zum Zusammenbruch der Stadtpanner wurde das Stadtwappen in Zusammenhang mit dem Kantonalwappen verwendet, was zu den alten Kantonsfarben Schwarz und Blau führte.

Per betrug die Einwohnerzahl des Kantons Freiburg . Die Bevölkerungsdichte liegt mit /1670.8 round 0 Einwohnern pro Quadratkilometer unter dem Schweizer Durchschnitt (/41285 round 0 Einwohner pro Quadratkilometer). Der Ausländeranteil (gemeldete Einwohner ohne Schweizer Bürgerrecht) bezifferte sich am auf  Prozent, während landesweit  Prozent Ausländer registriert waren. Per betrug die Arbeitslosenquote  Prozent gegenüber  Prozent auf eidgenössischer Ebene.

Der Kanton Freiburg ist ein offiziell zweisprachiger Kanton, Amtssprachen sind Französisch und Deutsch. Die Mehrheit der Bevölkerung ist französischsprachig, Deutsch wird vor allem im Norden und Nordosten des Kantons gesprochen. Im Sensebezirk sowie in der Pfarrei Gurmels wird im Alltag Senslerdeutsch, im Seebezirk – einer ehemaligen Gemeinen Herrschaft von Bern und Freiburg – mehrheitlich Berndeutsch gesprochen; die Mundart von Jaun im Greyerzbezirk ähnelt dem Berner Oberländischen.

2011 wurden die Einwohner des Kantons zwecks ihrer Hauptsprache befragt, worauf sie mehrere Hauptsprachen angeben konnten. Dabei fanden bis zu drei Hauptsprachen je Person Berücksichtigung.

Auffällig beim obigen Resultat ist, dass Englisch (keine Amts- bzw. Landessprache eines Kantons bzw. der Schweiz) einen höheren Anteil als Italienisch aufweist.

Freiburg ist ein römisch-katholisch geprägter Kanton, obwohl er von den mehrheitlich protestantischen Kantonen Waadt und Bern umgeben ist. Nur im Nordwesten des Kantons, im Seebezirk, ist die Mehrheit der Bevölkerung traditionell protestantisch, da das Gebiet bis 1798 eine Gemeine Herrschaft von Bern und Freiburg war. Heute sind rund 70 Prozent der Kantonsbevölkerung römisch-katholisch und 15 Prozent protestantisch. Die Katholiken gehören landeskirchlich zur Katholischen Kirchlichen Körperschaft des Kantons Freiburg () und religiös zum Bistum Lausanne, Genf und Freiburg. Die Reformierten sind in der Evangelisch-reformierten Kirche des Kantons Freiburg zusammengeschlossen. 3 Prozent gehören einer anderen christlichen Religion an, die übrigen haben eine andere Religion, sind konfessionslos oder haben beim Zensus keine Angabe gemacht.

Im Jahr 2004 wurde eine neue Kantonsverfassung angenommen, die das aus dem Jahr 1856 stammende und seither vierzigmal überarbeitete Grundgesetz ersetzte.

Zwingend einer Volksabstimmung (obligatorisches Referendum) unterliegen die Teil- oder Totalrevision der Kantonsverfassung sowie Erlasse des Grossen Rates, die eine neue Nettoausgabe zur Folge haben, die 1 % der Gesamtausgaben der letzten vom Grossen Rat genehmigten Staatsrechnung übersteigt.

6000 stimm- und wahlberechtigte Bürger können mittels einer Volksinitiative die Teil- oder Totalrevision der Kantonsverfassung sowie den Erlass, die Änderung oder die Aufhebung eines Gesetzes beantragen. Volksinitiativen sind nach der Beratung im Grossen Rat der Volksabstimmung zu unterbreiten.

6000 stimm- und wahlberechtigte Bürger können verlangen, dass folgende Beschlüsse des Grossen Rates der Volksabstimmung zu unterbreiten sind (fakultatives Referendum): erstens ein vom Grossen Rat verabschiedetes Gesetz, zweitens ein Erlass des Grossen Rates, der eine neue Nettoausgabe zur Folge hat, die ¼ % der Gesamtausgaben der letzten vom Grossen Rat genehmigten Staatsrechnung übersteigt, und drittens ein Erlass des Grossen Rates, der Studienkredite von regionaler oder kantonaler Bedeutung betrifft.

300 Stimmberechtigte können eine Motion zuhanden des Grossen Rates einreichen (Volksmotion), die von diesem wie eine Motion eines seiner Mitglieder zu behandeln ist.

Das Kantonsparlament, der "Grosse Rat" oder "Grand Conseil," zählt seit Inkrafttreten der neuen Verfassung 110 Sitze. Eine Legislaturperiode dauert fünf Jahre. Die Wahlen werden im Proporzwahlverfahren durchgeführt.

Die letzten Grossratswahlen fanden am 6. November 2016 statt.

Die Kantonsregierung, der "Staatsrat" oder "Conseil d’Etat," besteht aus sieben Mitgliedern. Diese werden alle fünf Jahre vom Volk im Majorzwahlverfahren bestellt.
Der Staatsrat wird durch die Staatskanzlei SK (Chancellerie d’Etat CHA) unterstützt, die durch die Staatskanzlerin Danielle Gagnaux-Morel geleitet wird. Stellvertreter sind die Vize-Kanzlerin Sophie Perrier und der Vize-Kanzler Marc Valloton.

Erste gerichtliche Instanz sind die "Zivilgerichte" einerseits sowie die "Strafgerichte," das "Wirtschaftsstrafgericht" und die "Jugendstrafkammer" anderseits. Zweite gerichtliche Instanz ist das "Kantonsgericht".

Als Schlichtungsbehörde in zivilen Angelegenheiten sind der ersten Instanz die "Friedensgerichte" vorangestellt.

Die Verwaltungsgerichtsbarkeit wird durch das Kantonsgericht ausgeübt, soweit das Gesetz keine andere Behörde für zuständig erklärt.

Die Administrativ- und Disziplinaraufsicht über die Justiz wird vom "Justizrat" ausgeübt. In diesem Gremium haben je ein Vertreter des Staatsrates, des Grossen Rates, der Gerichte erster und zweiter Instanz, des Anwaltverbandes, der Universität und der Staatsanwaltschaft sowie zwei weitere Personen Einsitz.

Insgesamt umfasst der Kanton 136 politische Gemeinden (Stand: Januar 2017).

Nachfolgend aufgelistet sind die bevölkerungsreichsten politischen Gemeinden mit mehr als 5'000 Einwohnern per :

Der Kanton gliedert sich in die sieben Bezirke:

Die Verwaltung eines Bezirks heisst "Oberamt," deren Leiter "Oberamtmann". Nach französischem Vorbild trägt jeder Bezirk den Namen eines Gewässers («See» steht für den Murtensee).

Die Freiburger scheinen vom Singen derart begeistert zu sein, dass es im Kanton auf 35 Bewohner einen «organisierten» Sänger gibt. Kirchenchöre und weltliche Chöre, gemischt oder nicht; Chöre mit jungen und weniger jungen Mitgliedern, mit professionellen Ambitionen oder nicht: Die Freiburger Chorvereinigung umfasst rund 7'200 Sängerinnen und Sänger in 234 Gruppierungen; hinzu kommen kurzlebige Formationen für bestimmte Projekte und informelle Ensembles ausserhalb jeglicher Vereinsstrukturen.

Diese ungewöhnliche Dichte lässt sich durch eine uralte Tradition erklären, die fest in der regionalen Geschichte verankert ist. Obwohl sich die Chorbewegung in allen katholischen Kantonen entwickelte, fand sie in Freiburg – in einer durch den Klerus fest am Gängelband geführten ländlichen Gesellschaft – einen besonders fruchtbaren Boden. Das Chorwesen entfaltete sich aber auch ausserhalb des religiösen Umfelds, bisweilen als Gegenreaktion zu diesem. Abbé Joseph Bovet (1879–1951) gelang es jedoch, alle Stimmen zu vereinen, und sein Charisma bestimmte lange das Chorleben der ganzen Region. Im 20. Jahrhundert erweiterten sich das Repertoire wie die Einzugsgebiete, so dass die für eine Pfarreikultur typische Tradition, vor allem im und mit dem Dorf, im Schatten des Kirchturms und im Rhythmus des lokalen Leben zu singen, etwas in den Hintergrund geriet.




</doc>
<doc id="12733" url="https://de.wikipedia.org/wiki?curid=12733" title="Elektrizität">
Elektrizität

Elektrizität (von griechisch "ēlektron" „Bernstein“) ist der physikalische Oberbegriff für alle Phänomene, die ihre Ursache in ruhender oder bewegter elektrischer Ladung haben. Dies umfasst viele aus dem Alltag bekannte Phänomene wie Blitze oder die Kraftwirkung des Magnetismus. Der Begriff Elektrizität ist in der Naturwissenschaft nicht streng abgegrenzt, es werden aber bestimmte Eigenschaften zum Kernbereich der Elektrizität gezählt:


Vorgänge, bei denen keine zeitliche oder keine wesentliche zeitliche Änderung auftritt, werden der Elektrostatik zugeordnet. Vorgänge, bei denen die zeitliche Änderung einen wesentlichen Einfluss hat, werden zur Elektrodynamik gezählt. 

Phänomene der Elektrizität sind schon lange bekannt, das wohl bekannteste und spektakulärste Auftreten ist in der Meteorologie der Blitz. Das Auftreten des elektrischen Schocks, den bestimmte Fische wie der Zitterrochen oder Zitteraal zum Beutefang einsetzen, war im alten Ägypten um 2750 v. Chr. bekannt. In der Antike war den alten Griechen bereits die elektrostatische Aufladung des Bernsteins bekannt, der von ihnen als "elektron" bezeichnet wurde. Diese Erkenntnis wird dem Naturphilosophen Thales von Milet zugeschrieben, der sich um 600 v. Chr. mit elektrostatischen Aufladungen beschäftigte.

Im 1. Jahrhundert v. Chr. wurden parthische Tongefäße in der Nähe von Bagdad verwendet, die 1936 von Wilhelm König gefunden wurden und auch als Bagdad-Batterie bekannt sind. Diese Gefäße enthielten einen Eisenstab und einen Kupferzylinder, der mit Asphalt abgedichtet war. Versuche des Roemer- und Pelizaeus-Museums in Hildesheim zeigten, dass mit dieser Anordnung und Traubensaft als Elektrolyt eine Spannung von 0,5 V erreicht werden konnte. Die Verwendung als eine Art Batterie ist umstritten.

Eine gezielte und praktische Anwendung der Elektrizität erfolgte erst am Beginn der Neuzeit. Der Brite William Gilbert untersuchte systematisch die elektrische Aufladung an vielen Substanzen mit Hilfe des Versoriums und führte die Bezeichnung „Electrica“ ein (veröffentlicht 1600). Um 1663 entwickelte der Magdeburger Bürgermeister Otto von Guericke eine drehbare Schwefelkugel, die mit der Hand gerieben die kosmischen Wirkkräfte "(virtutes mundanae)" nachweisen sollte. Gezielt zum Erforschen elektrischer Wirkungen entwickelte Francis Hauksbee 1706 eine Reibungselektrisiermaschine, deren Kugel nicht mehr aus Schwefel, sondern aus Glas gebaut war. Diese und ähnliche Elektrisiermaschinen dienten in den Folgejahrzehnten vor allem der gesellschaftlichen Belustigung.

1733 stellte der französische Naturforscher Charles du Fay fest, dass es zwei entgegengesetzte Formen der elektrischen Ladung gibt, die er zunächst als Harz- und Glaselektrizität bezeichnete. Diese Feststellung war die Basis für die Bezeichnung als positive und negative elektrische Ladung.

Der niederländische Physiker Pieter van Musschenbroek erfand 1745 – unabhängig von, und ein Jahr nach Ewald Jürgen Georg von Kleist – die Leidener Flasche, die in Deutschland auch als „Kleistsche Flasche“ bekannt ist. Die Leidener Flasche gilt als der erste Kondensator zur Speicherung von elektrischen Spannungen.

Um 1752 fand der Amerikaner Benjamin Franklin nach der schon bekannten Reibungselektrizität Zusammenhänge mit der atmosphärischen Elektrizität. Er erfand den Blitzableiter und interpretierte das Phänomen als "Pluspol" und "Minuspol."
Der italienische Mediziner Luigi Galvani beobachtete um 1770 mit einer Elektrisiermaschine an Froschschenkeln Zuckungen eines toten Frosches. So wurde bekannt, dass Elektrizität auch Bewegungen auslösen kann. „Tierische“ Elektrizität an Froschschenkeln ist übertragene elektrochemische Energie und Grundlage der Elektrochemie.

1775 erfand der italienische Physiker Alessandro Volta das Elektrophor, ein Gerät zur Ladungstrennung mit Hilfe der Influenz. Fünf Jahre später entwickelte er die Voltasche Säule, die aus den Metallen Kupfer und Zink und einem Elektrolyt besteht. Diese Batterie ermöglichte erstmals die Stromerzeugung ohne Reibung, nur aus gespeicherter chemischer Energie. Die Voltasche Säule wurde für viele Jahre die wichtigste Apparatur zur Gleichstromerzeugung.

Am Ende des 18. Jahrhunderts fanden und beschrieben Charles Augustin de Coulomb, Joseph Priestley, Henry Cavendish und John Robison unabhängig voneinander das Gesetz zur Beschreibung der Kraft zwischen zwei elektrischen Ladungen, das als Coulomb-Gesetz bekannt ist. Der deutsche Physiker Georg Simon Ohm formulierte den grundlegenden Zusammenhang zwischen elektrischem Strom und elektrischer Spannung an linearen elektrischen Widerständen. Dieser Zusammenhang wird später als ohmsches Gesetz bekannt.

Um 1810 erzeugte der Chemiker Humphry Davy zwischen zwei Kohlestiften, die mit einer Batterie als Stromversorger verbunden waren, einen Lichtbogen und schuf damit die Grundlagen für die Kohlebogenlampe.

Um 1820 beobachtete Hans Christian Ørsted die Ablenkung einer Magnetnadel durch Stromeinfluss. André-Marie Ampère, ein französischer Physiker, deutete und beschrieb darauf aufbauend die Stärke des Magnetfeldausschlags in Abhängigkeit zur Stromstärke. Er erfand das „Amperemeter“, die Theorie des elektrischen Telegrafen, erstmals angewandt von Carl Friedrich Gauß und Wilhelm Eduard Weber, und den Elektromagneten. Er ist Begründer der Theorie vom Elektromagnetismus und Namensgeber der physikalischen Einheit des elektrischen Stromes Ampere.
Der britische Physiker Michael Faraday gilt als einer der Begründer der Elektrodynamik. Er formulierte erstmals das Induktionsgesetz und beschäftigte sich in weiteren Arbeiten mit den Gesetzen der Elektrolyse. Er schuf so die Grundlagen zur Erfindung der Telegraphie. Der schottische Physiker James Clerk Maxwell konzipierte 1864 die grundlegende Theorie der klassischen Elektrodynamik in Form der Maxwell-Gleichungen und verband damit Effekte ruhender und bewegter Ladungen sowie deren Felder zur Beschreibung elektromagnetischer Phänomene. Aus diesen Gesetzmäßigkeiten folgerte er die Existenz der elektromagnetischen Wellen. Er identifizierte das Licht als eine Erscheinungsform von elektromagnetischen Wellen.

Ab 1830, nachdem die notwendigen Gesetzmäßigkeiten bekannt waren, setzte eine breite Anwendung der Elektrizität ein. Diese technischen Anwendungen werden unter dem Begriff Elektrotechnik zusammengefasst. Seit 1831 experimentierten mehrere Forscher damit, den Elektromagnetismus für elektrische Antriebe und zur elektromechanischen Stromerzeugung zu nutzen. Diese gelang 1832 unabhängig voneinander dem Franzosen Hippolyte Pixii und dem Italiener Salvatore Dal Negro. 1834 meldete der US-Amerikaner Thomas Davenport ein Patent für eine elektrische Lokomotive an. Samuel F. B. Morse baute 1833 den ersten brauchbaren Schreibtelegrafen. Mit der von ihm eingerichteten Telegrafenlinie von Washington nach Baltimore und dem von einem seiner Mitarbeiter entwickelten Morsealphabet begann 1844 das Zeitalter der elektrischen Kommunikation – und damit der Gleichzeitigkeit. In den Jahren 1843/1844 gelang Henri Adolphe Archereau und Louis-Joseph Deleuil mit Kohlebogenlampen auf der Place de la Concorde in Paris erstmals die elektrische Beleuchtung eines öffentlichen Platzes.

Die Stromerzeugung (und damit ebenso die Möglichkeit des elektrischen Antriebs) mit einer elektrischen Maschine ohne Permanentmagnet gelang 1851 dem Slowaken Ányos Jedlik, 1854 dem Dänen Søren Hjorth, der dafür im selben Jahr das englische Patent Nr. 2198 erhielt, und 1866 Werner von Siemens, der sie zur Serienreife brachte.

1882 wurde erstmals eine elektrische Energieversorgung über eine große Entfernung installiert, mit der 57 km langen Gleichstromleitung Miesbach–München. 1886 begründete Nikola Tesla mit Hilfe seines Sponsors George Westinghouse die heute gebräuchliche elektrische Energieübertragung mittels Wechselstroms. 1891 gelang die erste Fernübertragung von heute in der Energietechnik üblichen Dreiphasenwechselstrom mit der Drehstromübertragung Lauffen–Frankfurt über 176 km.

In den Folgejahren kam es in der Elektrotechnik zu einer zunehmenden Spezialisierung. Während die nach wie vor eng mit dem Maschinenbau verbundene elektrische Energietechnik zu der Energieversorgung von Haushalten, Betrieben und Fabriken diente, verfolgte der Zweig der Nachrichtentechnik das Ziel, Information wie Nachrichten mit geringen zeitlichen Verzögerungen über weite Distanzen zu übertragen. Um 1895 führte Guglielmo Marconi in Bologna erste Funkversuche durch. Er baute hierbei auf den Entdeckungen von Heinrich Hertz, Alexander Stepanowitsch Popow und Édouard Branly auf. Am 24. März 1896 gelang Popow auf funktechnischem Wege die Übertragung der Worte „Heinrich Hertz“ während einer Demonstration vor der Russischen Physikalischen Gesellschaft.

Das 20. Jahrhundert ist durch eine starke Erweiterung des Theoriegebäudes gekennzeichnet. Die klassische Elektrodynamik von Maxwell wurde im Rahmen der speziellen Relativitätstheorie zur relativistischen Elektrodynamik erweitert. Mitte des 20. Jahrhunderts erfolgte, insbesondere durch Arbeiten des amerikanischen Physikers Richard Feynman, die Erweiterung zu der Quantenelektrodynamik.

In der technischen Anwendung wurde 1897 von Ferdinand Braun die Kathodenstrahlröhre entwickelt, die die Grundlage der ersten Fernsehapparate darstellt. 1911 beobachtete der Niederländer Heike Kamerlingh Onnes als erster Mensch den Effekt der Supraleitung. Mitte des 20. Jahrhunderts entwickelten Walter H. Brattain, John Bardeen und William Shockley den Transistor, der in den Folgejahren die Grundlage der ersten integrierten Schaltungen und der Computertechnik war.

Im heutigen Alltag ist Elektrizität im Sinne von elektrischer Energie unentbehrlich, was dem Menschen meistens erst durch Ausfälle von Versorgungsnetzen wieder bewusst wird. Allerdings haben 1,4 Mrd. Menschen weltweit keinen Zugang zu elektrischer Energie. Die Erzeugung dieses Energieträgers erfolgt fast immer in Kraftwerken, die Verteilung erfolgt flächendeckend durch Unternehmen der Stromnetze (siehe Elektrizität/Tabellen und Grafiken). Seit über einem Jahrhundert bestimmen Anwendungen von Elektrizität wie elektrisches Licht, Wärme und Kraft mehr und mehr das menschliche Leben. Eine wesentliche Bedeutung nimmt die Elektrizität im Bereich der Kommunikationstechnologie und Informationstechnologie ein.

Elektrizität in natürlicher Form tritt neben Blitzen auch weniger spektakulär in Form einer Informationsübertragung in Nervensystemen von Lebewesen auf. Dabei treten nur sehr kleine Spannungen auf. Elektrizität hat je nach Stärke auch unterschiedliche Auswirkungen auf den menschlichen Körper. Für Stromunfälle ist die Stärke und die Einwirkdauer des elektrischen Stromes auf den menschlichen Körper wesentlich. Elektrische Ströme ab zirka 30 mA können gefährlich sein und Atemlähmungen und Herzstillstand verursachen. Elektrischer Strom wird auch eingesetzt, um Menschen gezielt zu töten, wie dieses etwa mit dem elektrischen Stuhl geschieht.

Festkörper, die keine oder nur geringe elektrische Leitfähigkeit besitzen, werden nach ihrem Verhalten in elektrischen und magnetischen Feldern eingeordnet.
Ursache des unterschiedlichen Verhaltens der Materie in einem Feld ist das Vorhandensein bzw. Nichtvorhandensein von entsprechenden Dipolen und deren Verhalten. Dies wird durch die Verwendung der entsprechenden Vorsilbe für beide Felder äquivalent beschrieben (vgl. Magnetismus von Festkörpern). Es bedeutet dabei:

Mit den Vorsilben Ferro-, Ferri- und Antiferro- werden unterschiedliche Formen einer langreichweitigen Ordnung von Dipolen bezeichnet. Die Bereiche gleicher Ordnung werden Domänen genannt. Sie entsprechen den weissschen Bezirken im Magnetismus. Alle diese Materialien sind Festkörper, von denen viele eine paraelektrische Hochtemperaturphase besitzen.
Die Ausrichtung der Dipole kann durch ein ausreichend starkes äußeres Feld umgekehrt werden. Dies führt zu einer Hysterese. Die einzelnen Formen kann man durch die Art der Hysterese unterscheiden.
Die verschiedenen Phänomene der Elektrizität sind Betrachtungsgegenstände in Teilen der Physik und der Chemie:

Die Elektrotechnik bezeichnet denjenigen Bereich der Ingenieurwissenschaft und Technik, der sich mit allen Aspekten der Elektrizität befasst. Das Spektrum reicht von der elektrischen Energietechnik wie der Stromerzeugung, über die Regelungs-, Steuerungs- und Messtechnik bis zur Nachrichtentechnik und der Automatisierungstechnik als einem Studiengang der Elektrotechnik. Das theoretische Fundament bildet die theoretische Elektrotechnik.

Die große Bedeutung der elektrischen Energie liegt darin, dass sie in Hochspannungsleitungen transportiert werden kann und sich einfach in andere Energiearten wie mechanische Arbeit, Wärme, Licht oder Schall umwandeln lässt. Elektrische Energie kann allerdings nur schlecht gespeichert werden und muss dazu in andere Energieträger wie in Form chemischer Energie bei Akkumulatoren oder in potentielle Energie wie bei Pumpspeicherkraftwerken umgewandelt werden.



</doc>
<doc id="12735" url="https://de.wikipedia.org/wiki?curid=12735" title="AIX">
AIX

AIX ("Advanced Interactive eXecutive") ist ein Unix-Betriebssystem des Unternehmens IBM und für den Einsatz in Serversystemen beziehungsweise in Workstations ausgerichtet.

Die erste Version von AIX erschien im Jahr 1986. AIX wurde früher unter anderem für IBM PS/2, IBM RT, IBM RS/6000, IBM PC Power Series, Motorola PowerStack sowie Apple Network Server angeboten. Aktuelle AIX-Versionen unterstützen nur noch Power- und PowerPC-basierte Hardware von IBM (pSeries beziehungsweise RS/6000) und Bull (Escala).

Ab der Version 4.3 wird auch die Ausführung von GNU/Linux-Programmen wie GIMP, KDE, Gnome und GCC unterstützt, aber auch unter älteren Versionen wie AIX 3.x, 4.1 oder 4.2 war es im Allgemeinen möglich, damalige GNU-Programme zu übersetzen und zu verwenden. Mit der Version AIX5L wurde die Unterstützung auch durch das „L“ im Namen deutlich gemacht.

AIX (ab Version 5.2L) unterstützt das dynamische Repartitionieren, das heißt, dass bei Verwendung von LPAR-fähiger Hardware dem laufenden Betriebssystem Prozessoren, Speicher und I/O-Adapter hinzugefügt oder entzogen werden können. Ein Neustart ist nicht notwendig.

2004 wurde die Version 5.3 eingeführt, die auf IBM eServer p5 unterstützt wird und Virtualisierung und Micropartitionierung ermöglicht. Weitere Neuerungen betreffen SMT-Unterstützung, „workload management“ und ein neues „accounting system“. Seit dieser Version ist es auch möglich, AIX in einer Shell auf der IBM iSeries neben i5/OS und Linux zu betreiben.

AIX 6.1 kam im November 2007 zur Auslieferung. Für diese Version gab es erstmals in der Geschichte von AIX ein „open-beta“-Programm, an dem jedermann teilnehmen konnte. Neben der Unterstützung von neuester Hardware bringt AIX 6.1 vor allem zusätzliche Virtualisierungs-Features wie Workload Partitions, Application- und Partition-Mobility, sowie eine ganze Reihe von neuen Sicherheits-Funktionen mit (Enhanced RBAC, Encrypted File System, Trusted AIX und Trusted Execution etc.).
Das plakative „L“ für die Linux-Affinität wurde mit Version 6 wieder aus dem Produktnamen entfernt. Man will damit den großen Sprung von v5r3 auf v6r1 verdeutlichen (viele neue Features, den Bruch mit dem 32-Bit-Kernel), die Harmonisierung mit den Power6-Prozessoren darstellen und die Ängste der Kunden ob der Zukunft von AIX in Koexistenz mit Linux ausräumen.

AIX5L bietet keine Binärkompatibilität zu Linux (auch nicht zu PPC-Linux), sondern eine Quelltextkompatibilität. Durch native Unterstützung der Linux-Bibliotheken und Programmierschnittstellen können GNU-/Linux-Programme nach einmaligem Rekompilieren als native AIX-Programme ausgeführt werden. IBM selbst bietet die populärsten GNU-/Linux-Programme als vorkompilierte RPMs für AIX in der „AIX Toolbox for Linux Applications“ an.

AIX beinhaltet einen leistungsfähigen Logical Volume Manager, JFS- und JFS2-Dateisysteme, einen integrierten Workload Manager und viele andere Eigenschaften, die in anderen kommerziellen Unix-Systemen eher als kostenpflichtige Erweiterungen angeboten werden. Das Tool mksysb ermöglicht (ähnlich wie make_tape_recovery aus dem HP-UX-Paket Ignite-UX) eine komfortable und bootfähige Systemsicherung.

Seit AIX 3 kann das System über das "System Management Interface Tool" (smit) verwaltet werden. Statt direkt Kommandozeilen-Befehle einzugeben, wird über eine menügesteuerte Oberfläche gearbeitet, die die entsprechenden Kommandos mit Parametern generiert und ausführt. Es stehen zwei Versionen zur Verfügung: "smit" (X-Window-basierend) und "smitty" (auf der Kommandozeile, das Wort setzt sich zusammen aus „Smit“ und „tty“); beide verfügen über identische Möglichkeiten. Die Aktionen werden protokolliert (smit.log) und die ausgeführten Kommandos zusätzlich geschrieben (smit.script). Somit kann man die generierten Kommandos in eigene Skripte übernehmen.

Für den IBM PC/XT und PC/AT waren zuvor die Unix-Derivate PC/IX und Xenix verfügbar.

AIX/RT

AIX/6000




</doc>
<doc id="12738" url="https://de.wikipedia.org/wiki?curid=12738" title="Stapellauf">
Stapellauf

Der Stapellauf ist eine Art des Zu-Wasser-Lassens eines neuen Schiffs in der Werft.

Während des Baus ist der Schiffskörper auf feste Sockel gestützt; diese bezeichnet man als Pallen. Sie bestehen im unteren Bereich aus Beton und im oberen Bereich aus Holzteilen, die nach Lösen einer Schraube unter dem Gewicht des Schiffes auseinandergedrückt werden und zerfallen. Eine andere Variante sind die im üblichen Werftgebrauch als Patente bezeichneten Auflager. Diese Patente bestehen aus einem gleichschenkligen Keil, der in einem Unter- und einem Oberteil läuft. Durch Einschlagen des Keils kann die Höhe der Pallen geändert werden. Früher verwendete man Stapel aus Holzbohlen, wovon sich der Name "Stapellauf" ableitet.

Die Schiene, auf der das Schiff gebaut wird, wird Helling genannt. Beim Stapellauf wird das Gewicht des Schiffs auf einen Holzschlitten verlagert, der nach dem Lösen der Haltevorrichtungen eine schiefe Rampe ins Wasser hinunterrutscht. Das Fahrzeug liegt auf der Oberbahn und gleitet auf der Unterbahn ins Wasser. Beide Bahnen werden vorher mit Paraffin (Wachs) begossen, um eine gleitfähige Oberfläche zu erhalten. Danach werden die einzelnen Abschnitte mit Fett bestrichen, die Oberbahnteile an ihre Position gezogen und anschließend miteinander verbunden. Der gesamte Schlitten wird sodann von den Taklern an Deck gelascht. Am Bug wird in der Regel eine sogenannte Wippe aufgesetzt. Dabei handelt es sich um Weichholz, das den Zweck hat, die Druckkräfte, die durch das Aufschwimmen des Achterschiffes auftreten, in die Bahn abzuleiten. Die Wippe ist nur für einmaligen Gebrauch gedacht und wird danach entsorgt.

Das gesamte Ablaufgewicht ruht während des Einbringens der Ablaufbahn auf den Seitenpallen. Die Kielpallen müssen für die Vorbereitungen herausgenommen werden. Das Gewicht wird erst wenige Stunden vor dem Stapellauf durch das Lösen der Seitenpallen ins Fett abgelassen, damit sich die Oberbahn nicht im Fett festsaugt und das Schiff ggf. nicht ins Gleiten kommt. Je nach Werft wird auch ein Abdrücker verwendet, der das Schiff ggf. anschiebt. In der Regel fangen die Schiffe aber nach spätestens 8–10 Sekunden selber an zu laufen.

Dem Stapellauf kann die Schiffstaufe vorausgehen. Nach dem Stapellauf ist der Bau des Schiffs noch nicht abgeschlossen. Teilweise fehlen beim Stapellauf noch Aufbauten und Schiffskrane (sofern vorgesehen). Auch muss der Neubau noch ausgerüstet werden (Schiffsmaschine, Inneneinrichtung, bei Kriegsschiffen auch Bewaffnung, werden erst nach dem Stapellauf montiert).

Das Schiff kann dabei der Länge nach, meistens mit dem Heck voran (Längsstapellauf) oder seitlich (Querstapellauf) ins Wasser gelassen werden. Bei Hochseeschiffen kommt im Allgemeinen der Längsstapellauf zur Anwendung, bei Binnenschiffen, oft auch bei U-Booten, der Querstapellauf. Der Längsstapellauf wird seit der Antike angewandt, der Querstapellauf wurde erst im 19. Jahrhundert eingeführt.

Große Schiffe werden heute häufig in Trockendocks gebaut, die nach der Fertigstellung einfach geflutet werden, bis das Schiff aufschwimmt. Trotzdem hat sich auch hier der Name Stapellauf erhalten. Schiffe können auch auf Airbags rollen.

Der wenig gebräuchliche Begriff „Stapelhub“ beschreibt eine moderne Form des Stapellaufs für kleinere Schiffe, bei der das Schiff von seinem Bauplatz mit einem Kran erstmals ins Wasser gesetzt wird.

Der Stapellauf ist ein kritischer Moment für das Schiff. Beim Längsstapellauf wirken große Kräfte auf den Rumpf, der zuerst nach unten durchgebogen wird, sobald das Heck das Ende der Rampe (Schlagbrett) erreicht hat. Anschließend schwimmt das Heck auf, so dass der Rumpf nur auf Bug und Heck aufliegt und nach oben durchgebogen wird. Zudem hat das Schiff während dieser Zeit nur eine geringe Rollstabilität und kann kentern.
Dem wird unter anderem entgegengewirkt, indem freie Räume im Heck vor dem Stapellauf abgeschottet und geflutet werden oder solider Ballast eingebracht wird. Dadurch sinkt der Auftrieb des Hecks und die Länge, auf der der Kiel das Schiff frei tragen muss, verkürzt sich. Dies ist nur bei Heckformen und Schiffsformaten möglich, die ein Unterschneiden unwahrscheinlich machen.

Beim Querstapellauf wiederum gerät das Schiff nach dem Eintauchen durch die Bremswirkung des Wassers in starke Seitenlage. Oft steht der Werft nur ein begrenzter Platz zur Verfügung, so dass das Schiff schnell abgebremst werden muss, um eine Kollision zu vermeiden.

Es gab daher eine Reihe von misslungenen Stapelläufen, bei denen das Schiff auf der Rampe umkippte oder sofort im Wasser versank. Gleitmittel hatten bei zu hoher oder zu niedriger Temperatur nicht die nötige Gleitfähigkeit, so dass sich der Rumpf nicht von der Stelle bewegte. Ab der zweiten Hälfte des 20. Jahrhunderts haben moderne Gleitmittel diese Probleme weitgehend überwunden.

Alternativ kann ein Schiff mit weitgehend planer Rumpfunterseite auf abrollenden Airbags längs zu Wasser laufen. In den frühen 1990ern wurde diese Methode in China entwickelt und 1993 standardisiert. Das bisher größte so gerollte Schiff war 2008 ein 55.000 BRT großer Neubau in der Provinz Zhejiang.

Die Luftsäcke aus zugfestem Elastomer-Textil-Verbundmaterial (ähnlich dem von Schlauchbooten aus EPDM+PA6) sind schlank-zylindrisch und haben an den spitzen Enden axial Füllanschlüsse mit Manometer eingebaut. Sie liegen ziemlich dicht mehr als schiffsbreit quer unter dem Rumpf und bilden so ein Wälzlager auf einer flachen Rampe oder einem Strand. Diese Schläuche erreichen unbelastet gefüllt 0,8–1,8 m (Nenn-)Durchmesser. Kleinere Durchmesser und mehr Textillagen (üblich sind 3–6) erlauben höheren Arbeitsdruck (um 1–2 bar). Die Tragkraft entwickelt sich durch Eindellung. Ist ein Schlauch um 70 % seines Durchmessers gequetscht, erreicht er seine hohe typische Hebekraft pro Länge. Die Kraft kommt aus dem Innendruck mal flach an Rumpf (oder Untergrund) anliegender Fläche, die in diesem Fall so breit wie 110 % vom Nenndurchmesser ist.

Die Airbags werden unter dem Schiff gefüllt, heben mit wachsender Kraft aus steigendem Luftdruck den Rumpf von den Stapeln, zuletzt auch von den am höchsten gelegenen. Durch weitere Druckerhöhung in den höherliegenden Schläuchen rollt das Schiff dann los. Will sich das Schiff auf eine Seite neigen, werden dorthin alle Schläuche zunehmend etwas mehr eingedellt und entwickeln daher dort mehr Stützkraft. Zur anderen (Bord-)Seite hin ist es umgekehrt. Das Schiff liegt so kippstabil im Kräftegleichgewicht. In jedem Schlauch bleibt dabei der Innendruck erhalten.

Umgekehrt können die Schläuche auch unter ein fast schon auf Land gelaufenes Schiff durchgezogen und dann mit Druck gefüllt werden, das Schiff wird so unterstützt und kann dann so wälzgelagert flach landauf gezogen werden. Vor dem Schiff müssen dazu laufend weitere Luftwalzen bereitgelegt und passend gefüllt werden.



</doc>
<doc id="12739" url="https://de.wikipedia.org/wiki?curid=12739" title="Unix-Kommando">
Unix-Kommando

Unix-Systeme zeichnen sich durch eine Vielzahl von Kommandos (aus dem lateinischen "" für „befehlen“) aus, mit denen sich über eine Shell das Betriebssystem bedienen lässt. Die Syntax dieser Kommandos weicht unter den verschiedenen Systemen voneinander ab. Es existieren die beiden Hauptströmungen BSD und System V. Viele Kommandos gibt es in mehreren Varianten, auch die GNU-Tools als freie Software sind weit verbreitet. Die meisten Kommandos verfügen über ein- oder mehrbuchstabige Optionsschalter, die die Funktionalität verändern oder erweitern. Viele grundlegende Kommandos sind durch Standards wie POSIX vereinheitlicht, und daher auf fast allen Unix-Varianten zu finden.

Die grundlegende Philosophie der Unix-Kommandos ist das Baukasten-Prinzip: Statt großer, „alleskönnender“ Programme für ganze Aufgabengebiete werden kleine, spezialisierte Kommandos für elementare Aufgaben bereitgestellt, die miteinander verknüpft werden können, um komplexere Aktionen durchzuführen.

Im Folgenden wird in Klammern angegeben, bei welcher Unix-Variante das Kommando erstmals auftaucht beziehungsweise von der seine Verbreitung ausgeht und ob POSIX dieses Kommando spezifiziert:

Unix-Systeme werden traditionellerweise über die Kommandozeile in einem Terminal oder dessen Emulation bedient. Zwar gibt es auch Desktop-Umgebungen für Unix und unixartige Systeme, jedoch wird von vielen Nutzern weiterhin die Kommandozeile bevorzugt. Dies liegt mitunter daran, dass die Shells (Kommandozeileninterpreter) unter Unix wesentlich leistungsfähiger sind als zum Beispiel der DOS-Kommandozeileninterpreter. Außerdem ist eine Shell bei manchen Aufgaben einem grafischen Benutzerinterface überlegen, da durch die Shell-Syntax zum Beispiel Ablaufsteuerung und Programmrückmeldungen einbezogen werden können, was bei grafischen Anwendungen meist nicht möglich ist.

Neben der interaktiven Nutzung haben die Shells auch die Aufgabe, so genannte Shellskripte auszuführen. Diese sind grob vergleichbar mit DOS-Stapelverarbeitungsdateien, sind jedoch aufgrund der wesentlich besseren Ausstattung der Unix-Shells mit Kontrollstrukturen deutlich leistungsfähiger. Insbesondere erlauben es die Shells, mit einfacher Syntax die Kommando-„Bausteine“ zu verknüpfen, was bei DOS erst ab MS-DOS 2 möglich ist (neuere DOSe (z. B. FreeDOS, DCP) in der Regel von Anfang an).

Im Gegensatz zu DOS-Befehlen, die oft einfach englische Worte sind (z. B. TYPE, FORMAT) haben UNIX-Befehle oft stark abgekürzte Namen ("ed", "w") oder sogar Fantasienamen, die nichts mit ihrer Wirkung zu tun haben (z. B. "awk", "less") und erfordern oft eine schwierige Einarbeitung. 

Die Shells enthalten einige eingebaute Kommandos, hier einige Beispiele:


POSIX standardisiert keine Pfade. POSIX standardisiert nur, dass ein Programm sh existieren muss, das zu einer POSIX-Shell kompatibel ist und dass dieses Programm nach Eingabe von PATH=`getconf PATH` sh aufgefunden werden muss.


"(Weitere Shells siehe unter Unix-Shells)"

Ein Unix-Programm kennt drei Standardkommunikationswege:

Filter sind Kommandos, die diese Standardkommunikationswege benutzen, Eingabedaten über stdin bekommen, diese in irgendeiner Weise aufbereiten, und an stdout ausgeben. Das Programm codice_1 für "word count" (Wortanzahl) zum Beispiel zählt die Wörter die über seine Standardeingabe kommen und gibt das Ergebnis auf stdout aus.

Auf der Standardfehlerausgabe werden per Konvention Diagnose- und Fehlermeldungen oder Warnungen ausgegeben.

Man kann die Standardkommunikationswege „umbiegen“. In der Regel tut man das, um aus einer Datei zu lesen oder in eine Datei zu schreiben.

 wc -l < test.dat
 ls > dir.dat
 ls » dir.dat
 wc -l < test.dat 2> err.log

Das Prinzip der Ein-/Ausgabeumlenkung findet man auch im Betriebssystem MS-DOS.

Ein mächtiges Hilfsmittel ist die Verwendung einer "Pipe" zur Ein- und Ausgabeumlenkung

Man kann mehrere Filter miteinander verknüpfen (hintereinander hängen). In der Regel erwartet ein Filter die Eingabe von der Standardeingabe (Tastatur). Die Ausgabe erfolgt auf der Standardausgabe (Bildschirm). Beim Piping wird die Ausgabe eines Kommandos über eine im Betriebssystem-Kern implementierte Pipe in die Standardeingabe des folgenden Kommandos umgeleitet.

Beispiel:

In der Datei testdatei werden mittels grep alle Zeilen gesucht und ausgegeben, die das Wort „Fidibus“ enthalten, danach wandelt tr alle Kleinbuchstaben in Großbuchstaben um, und zum Schluss wird alles sortiert und auf dem Bildschirm ausgegeben. Diese Art der Pipes nennt man anonyme Pipes.

Diese Form der Ein- und Ausgabeumlenkung ist so alltäglich, dass es eine Vielzahl an Kommandos gibt, die erst durch diese Technik überhaupt sinnvoll sind – beispielsweise das oben genannte wc (word count), das ohne die Eingabeumleitung (weg von der Tastatur, hin auf eine Datei) kaum ein Einsatzgebiet hätte.

Zur dahinterliegenden Technik ist zu bemerken, dass die einzelnen Filter als jeweils eigene Prozesse gestartet werden und so quasi gleichzeitig ihre Arbeit verrichten, während die Pipes dafür sorgen, dass die Filter bei Bedarf so aufeinander warten, dass keine größeren Mengen an Zwischenergebnissen anfallen, die entsprechende Mengen an Speicherplatz in Anspruch nehmen würden.

Für komplexere Anwendungen lassen sich auch benannte Pipes (FIFOs) definieren, auf die dann wie auf eine Datei zugegriffen werden kann:
 mkfifo Rohr

Ein Einsatzgebiet ist etwa, wenn ein Filter Daten von verschiedenen Quellen mischen will – beispielsweise paste, das zwei Dateien nimmt und zeilenweise nebeneinander ausgibt.

siehe auch: GNU core utilities (Sammlung von grundlegenden Befehlszeilen-Programmen unter freier Lizenz)

Das Unix-Hilfesystem basiert wesentlich auf den so genannten Manpages (für "manual page", Handbuchseite). Idealerweise liegt für jedes Kommando, jeden Aufruf der Programmierschnittstelle (Systemfunktionen, C-Bibliothek) und jede wichtige Konfigurationsdatei eine eigene "man page" vor.

Die Handbuchseiten sind zudem in durchnummerierte Bereiche unterteilt, die unterschiedliche Schnittstellen abdecken (Benutzerkommandos, Administrations-Kommandos, C-Funktionen etc.).

Hilfeseiten sind im Format des Textsatzprogramms troff geschrieben, daher kann neben der ASCII-Textversion für die Online-Anzeige auch eine druckbare Version generiert werden.


Von GNU kommt ein alternatives Hilfssystem namens texinfo, das auf Hypertext beruht. Dieses ist in den Editor Emacs integriert, es gibt aber auch einen reinen info-Browser:
Die Tendenz geht anscheinend dahin, dass die GNU-Programmierer die manual pages etwas stiefmütterlich behandeln und das vollständigere und auch meistens korrektere Handbuch in Texinfo-Form liefern.

Zunehmend werden für GUI-Programme auch HTML-Seiten als Hilfedateien verwendet.

Folgendes ist eine unvollständige Zusammenstellung wichtiger Unix-Kommandos.

Der Umgang mit Dateien ist ein wesentlicher Teil der Arbeit am Computer. Dementsprechend gibt es auch unter Unix-Kommandos zum Umgang mit Dateien:

Dateien sind unter Unix (wie eigentlich auf allen modernen Betriebssystemen) in Verzeichnissen geordnet. Unter Unix gibt es jedoch, anders als etwa unter MS-DOS oder Windows, nur einen einzigen Verzeichnisbaum, in den der Inhalt anderer Datenträger bei Bedarf an einer bestimmten Stelle im Baum „eingehängt“ wird.

Zur Verzeichnisverwaltung stehen unter anderem folgende Befehle zur Verfügung:


Unix war von Anfang an ein Mehrbenutzerbetriebssystem (beispielsweise kann MS-Windows dies erst seit der NT-Generation). Das bedeutet, dass verschiedene Benutzer am Rechner arbeiten können und voreinander abgeschottet werden: Man kann anderen Benutzern erlauben oder verbieten, auf bestimmte Dateien zuzugreifen. Zusätzlich kann man Benutzer auch Gruppen zuordnen, denen kollektiv bestimmte Zugriffsrechte gewährt werden können. Eine besondere Rolle spielt der Benutzer root (Systemadministrator), der als einziger Benutzer vollen Zugriff auf das System hat.

Jeder Benutzer hat typischerweise sein eigenes Heimatverzeichnis, in welchem nur er (und root) Dateien anlegen und löschen kann.

Kommandos (Auswahl):

Eine der Stärken von Unix sind seine vielen Zubehörprogramme, insbesondere (aber nicht nur) zur Bearbeitung von Textdateien. Die Stärke beruht unter anderem darauf, dass die Zubehörprogramme leicht über "pipes" „zusammengeschaltet“ werden können, um komplexere Aufgaben zu erfüllen.







Unix wurde zusammen mit dem leistungsfähigen Textsatzsystem "troff" entwickelt, das unter anderem Tabellen- und Formelsatz ermöglicht.

Auch die Manualseiten können mit diesem System gesetzt werden.
"nroff" erlaubt die Ausgabe auf Textterminals und Zeilendrucker.
Das betriebssystemunabhängige TeX hat jedoch *roff weitgehend abgelöst. Auch das GNU-Hilfesystem "texinfo" arbeitet mit TeX für die Druckausgabe.






</doc>
<doc id="12740" url="https://de.wikipedia.org/wiki?curid=12740" title="Murten">
Murten

Murten ( im schweizerdeutschen Ortsdialekt []; frankoprovenzalisch ) ist eine politische Gemeinde und Hauptort des Seebezirks (frz. "District du Lac") im Schweizer Kanton Freiburg. Zu Murten gehören auch die früher selbständigen Gemeinden Altavilla (eingegliedert 1991), Burg (1975), Büchslen (2013), Courlevon, Jeuss, Lurtigen und Salvenach (alle 2016).

Murten bildet das regionale Kultur- und Wirtschaftszentrum des nördlichen Kantonsteils. Das mittelalterliche Zähringerstädtchen mit einer Ringmauer aus dem 13. bis 17. Jahrhundert und einer historischen Altstadt von nationaler Bedeutung liegt am nach ihm benannten Murtensee und war Schauplatz der Schlacht bei Murten.

Murten liegt auf , 14 km nördlich der Kantonshauptstadt Freiburg (Luftlinie). Die Stadt erstreckt sich auf einer rund 20 m hohen Anhöhe am Südostufer des Murtensees, östlich der Mündung des von Münchenwiler herkommenden Baches, im nördlichen Freiburger Mittelland.

Die Fläche des 12,0 km² grossen Gemeindegebiets umfasst einen Abschnitt am Südufer des Murtensees (rund 1,8 km Seeuferlänge) und der angrenzenden Molassehöhen. Der Gemeindeboden erstreckt sich vom Seeufer über einen flachen Uferrandstreifen und die Anhöhe von Murten in die südlich davon gelegene und vom Dorfbach von Münchenwiler durchflossene Geländemulde. Daran schliessen sich im Süden die vom eiszeitlichen Rhonegletscher überformten Molassehöhen mit verschiedenen Drumlins an, darunter die Höhen von "Bois Domingue" () und "Aderahubel" ().

Im Südwesten reicht das Gemeindegebiet über die Flächen von "Merlachfeld" und "Fin de Mossard" bis zum Wald "La Bourille", wobei der Gemeindebann von Meyriez auf der Landseite vollständig umschlossen wird. Nach Osten erstreckt sich das Gebiet auf das Hochplateau von Burg, in das der "Burggrabenbach" ein tiefes Erosionstal eingeschnitten hat, in den "Birchenwald" () und in die ausgedehnten Wälder östlich von Altavilla, nämlich "Trimbley" (bis ), "Bloster" () und "Murtenwald" (mit die höchste Erhebung von Murten).

Eine schmale, durchschnittlich rund 500 m breite, aber fast 4 km lange Exklave von Murten befindet sich in der landwirtschaftlich intensiv genutzten Ebene des Grossen Mooses. Sie reicht von der "Hanenmatt" bei Müntschemier südwärts über den "Grossen Kanal" und den "Biberenkanal" bis zum "Erlihof" bei Galmiz. Von der Gemeindefläche entfielen 1997 19 % auf Siedlungen, 27 % auf Wald und Gehölze und 54 % auf Landwirtschaft.

Zur Gemeinde Murten gehören die Dörfer Burg () und Altavilla () auf dem Hochplateau, die Weiler "Prehl" () südöstlich der Stadt, "Löwenberg" () am Nordfuss des Aderahubels und "Erli" () leicht erhöht am Südrand des Grossen Mooses, einige von der Stadt abgesonderte neue Wohnquartiere sowie zahlreiche Einzelhöfe. Nachbargemeinden von Murten sind Greng, Meyriez, Courgevaux, Galmiz, Muntelier, Ried bei Kerzers, Gempenach, Ulmiz, Gurmels, Cressier, Courtepin und Mont-Vully, das keiner Gemeinde zugehörige Gebiet Staatswald Galm im Kanton Freiburg sowie Münchenwiler und Müntschemier im Kanton Bern.

Mit Einwohnern (Stand ) gehört Murten zu den grösseren Gemeinden des Kantons Freiburg. Seine Bevölkerungszahl belief sich 1900 auf 2645 Einwohner. Danach nahm sie vorübergehend ab, um seither kontinuierlich anzusteigen. Die grössten Zuwachsraten wurden von 1950 bis 1970 sowie nach der Erschliessung neuer Wohnzonen in den 1990er Jahren verzeichnet. Das Siedlungsgebiet von Murten ist heute lückenlos mit dem seiner Nachbargemeinden Meyriez und Muntelier zusammengewachsen. Ein kleiner Teil des Murtner Gewerbegebiets gehört zur Gemeinde Courgevaux.

Von den Bewohnern sind 83 % deutschsprachig, 15 % französischsprachig und 0.5 % italienischsprachig (Stand Januar 2016). Die Amtssprache in Murten ist Deutsch, und städtische Reglemente werden allein in dieser Sprache publiziert. Aufgrund der Lage an der Sprachgrenze wird jedoch eine gewisse Zweisprachigkeit gepflegt, so führt die Gemeinde neben der deutschsprachigen Schule auch eine französischsprachige. Seit 2013, als die SBB den Stationsnamen «Murten/Morat» einführten, wird die Zweisprachigkeit auch nach aussen kommuniziert.

Noch im 15. Jahrhundert wurde in der Stadt hauptsächlich Französisch gesprochen. Nicht zuletzt im Zusammenhang mit der Reformation setzte sich jedoch immer mehr das Deutsche durch und gewann spätestens Ende des 17. Jahrhunderts die Oberhand. Im reformierten Murten wird, anders als im katholischen Sensebezirk, Berndeutsch und nicht freiburgisches Senslerdeutsch gesprochen.

Murten war stets ein agrarisch geprägtes Städtchen. Die landwirtschaftlichen Erzeugnisse des fruchtbaren Umlandes wurden hier verarbeitet und in den Handel gebracht. Auch die Fischerei im Murtensee spielte eine wichtige Rolle. Da die Stadt in der zweiten Hälfte des 19. Jahrhunderts abseits der Hauptverkehrsachsen zu liegen kam, hielt die Industrialisierung nur langsam Einzug. Ein wichtiger Arbeitgeber war zu dieser Zeit eine Uhrenfabrik. Der eigentliche wirtschaftliche Aufschwung setzte erst nach dem Zweiten Weltkrieg ein.

Heute bietet Murten rund 3400 Arbeitsplätze an. Mit 3 % der Erwerbstätigen, die noch im primären Sektor beschäftigt sind, hat die Landwirtschaft nur noch einen marginalen Stellenwert in der Erwerbsstruktur der Bevölkerung. Etwa 36 % der Erwerbstätigen sind im industriellen Sektor tätig, während der Dienstleistungssektor 61 % der Arbeitskräfte auf sich vereinigt (Stand 2001).

Auf den fruchtbaren Böden in der Umgebung von Murten und im Grossen Moos wird überwiegend Ackerbau und Gemüseanbau betrieben. Daneben sind auch die Milchwirtschaft und der Obstbau von Bedeutung.

Die Industrie- und Gewerbegebiete von Murten befinden sich in Bahnhofnähe und entlang der Ortsumfahrungsstrasse. Wichtige Unternehmen sind in den Bereichen Industrieelektronik (etwa Saia-Burgess), Produktion von Glaskeramik und Kochherden und in der Nahrungsmittelindustrie tätig. Daneben gibt es zahlreiche weitere kleinere und mittlere Betriebe des Bau- und Transportgewerbes, der Informationstechnologie, der Feinmechanik und der Textilindustrie.

Im tertiären Sektor vereinigen die Verwaltung, das Banken- und Versicherungswesen, das Bildungswesen und vor allem die Tourismus- und Gastronomiebranche zahlreiche Arbeitsplätze auf sich. Das Bezirksspital befindet sich jedoch in der Nachbargemeinde Meyriez.

Besonders seit dem Zweiten Weltkrieg haben sich südlich der Altstadt neue Wohnquartiere entwickelt. Heute findet man bevorzugte Wohnlagen in der Nähe des Seeufers sowie an aussichtsreicher Lage an den relativ sanft geneigten Hängen südlich und östlich der Stadt.

Sowohl für Kultur- als auch für Sportbegeisterte bietet Murten verschiedene Möglichkeiten. Erwähnenswert sind die Stadtbibliothek, die Ludothek, ein Freilichttheater und die Organisation von verschiedenen Konzerten, darunter das Murten Classics (Sommerfestspiele der Stadt Murten). Auch die jeweils Anfang März stattfindende Murtner Fastnacht ist von überregionaler Bedeutung. Ferner stehen Fussball- und Tennisplätze, eine Tennis- und Squashhalle und ein Hallenbad zur Verfügung. Alljährlich findet am 22. Juni die Solennität Murten statt, ein Umzug mit Musikformationen durch Murtens Altstadt. Das Jugendfest erinnert an die Schlacht bei Murten im Jahre 1476.

Seit 1933 findet am ersten Sonntag im Oktober der Murtenlauf statt. Dieser zählt zu den bekanntesten und traditionsreichsten Volksläufen der Schweiz, mit jeweils Tausenden von Teilnehmern. Die rund 17 Kilometer lange Strecke von Murten nach Freiburg wird ebenfalls zum Gedenken an die Schlacht bei Murten gelaufen.

Murten bildet ein wichtiges touristisches Zentrum der Dreiseenregion. Touristenattraktionen sind die gut erhaltene Altstadt mit Ringmauern und Türmen, das Historische Museum der Stadt (ausserhalb der Stadt in einer alten Mühle untergebracht) und die Seeuferanlagen. 

Einen wichtigen Aufschwung im Fremdenverkehr erlebte die Stadt im Jahr 2002 als Standort einer der fünf Arteplages der Schweizerischen Landesausstellung Expo.02. Die Arteplage von Murten stand unter dem Titel "Augenblick und Ewigkeit". Die Ausstellungen waren über einen Grossteil der historischen Altstadt verteilt. Wahrzeichen von Murten war der begehbare Monolith, ein nach Plänen von Architekt Jean Nouvel im Murtensee rund 200 m vor dem Hafen errichteter rostiger Stahlwürfel von 34 m Kantenlänge, in dem unter anderem ein Panorama der Schlacht von Murten zu sehen war. Mittlerweile ist die Konstruktion wieder zurückgebaut worden. Jedoch musste die «Expo-Gesellschaft» der Stadt Murten eine Entschädigung leisten, da Fundamentstücke aus Beton nicht vollständig aus dem See entfernt werden konnten. Schon 1964 war Murten einmal ein Ausstellungsort der Expo.

Die Gemeinde ist verkehrsmässig gut erschlossen. Sie liegt an der Hauptstrasse 1 von Bern via Payerne nach Lausanne. Die Altstadt und Meyriez werden durch eine lokale Ortsumfahrung vom Transitverkehr entlastet. Im Dezember 1997 wurde das Autobahnteilstück der A1 zwischen Löwenberg und Greng mit dem 2.2 km langen Tunnel Les Vignes und einem rund 1 km langen Tagbautunnel eröffnet. Vorher endete die A1 während rund 15 Jahren von Bern her kommend bei Löwenberg.

Die Anbindung an das schweizerische Eisenbahnnetz erfolgte am 12. Juni 1876 mit der Inbetriebnahme der Strecke von Murten nach Lyss. Nur wenig später, am 25. August 1876 wurde die Linie von Murten via Payerne nach Palézieux-Gare eingeweiht. Später kamen die Strecken von Murten nach Freiburg (am 23. August 1898 eröffnet) und von Murten nach Ins (am 1. Mai 1903 eröffnet) hinzu. Für die Feinverteilung im öffentlichen Verkehr sorgt die von Postauto betriebene Linie von Murten nach Gurmels-Düdingen, ebenfalls existieren drei Buslinien der Transports publics fribourgeois nach Gümmenen, Courtepin und Gempenach.

Ferner ist Murten durch das Schiffsverkehrsnetz auf dem Murtensee mit den anderen Seeanstössergemeinden sowie über den Broyekanal auch mit Neuenburg und Biel verbunden.

Gesetzgebende Behörde ist der von den Stimmberechtigten der Gemeinde Murten alle fünf Jahre gewählte Generalrat "(Conseil général)". Die fünfzig Abgeordneten werden im Proporzwahlverfahren gewählt, wobei 2016 die eingemeindeten Gemeinden eigene Wahlkreise bildeten. So stammen vierzig Abgeordnete aus Murten, je drei aus Jeuss und Salvenach, sowie je zwei aus Courlevon und Lurtigen. Die Aufgaben des Generalrates umfassen die Budget- und Rechnungsabnahme, die Festlegung der Gemeindereglemente und die Kontrolle der Exekutive. Die Grafik rechts zeigt die Zusammensetzung des Generalrats nach den Wahlen vom 8. November 2015.

Ausführende Behörde ist der Gemeinderat "(Conseil communal)". Er besteht aus sieben Mitgliedern und wird vom Volk im Proporzwahlverfahren gewählt. Die Amtsdauer beträgt fünf Jahre. Der Gemeinderat ist für die Vollstreckung der Beschlüsse des Generalrates, für die Ausführung der Gesetzgebung von Bund und Kanton sowie für die Repräsentation und Führung der Gemeinde zuständig. Stadtammann ist Christian Brechbühl (FDP, Stand 2016)

Nachgrabungen in einem Grabhügel aus der älteren Eisenzeit in der Nähe von Murten ergaben eine, möglicherweise auch weitere Nachbestattungen am Rande des Hügels. Gefunden wurden im Grab 2 an Grabbeigaben eine Eisen-Fibel vom Typ Marzabotto auf dem Oberkörper des Bestatteten, eine Bronze-Fibel auf der Schulter, ein Bronze-Hohlreif am Arm und ein daneben liegender Bronze-Torques mit Pufferenden. Diese Objekte sind typisch für die Frühlatènezeit. Im Grab 4 fand man einen Eisen-, einen Bronze- und einen Knöchelring, ebenfalls aus dieser Zeitepoche. Dies deutet auf latènezeitliche Nachbestattungen in einem Grabhügel aus der Hallstattzeit hin, wie es in dieser Übergangsperiode der älteren Eisenzeit häufig der Fall war.
Die erste urkundliche Erwähnung des Ortes erfolgte nach freilich umstrittener Datierung im Jahr 515 (oder 1017; Kopie Ende 12. Jahrhundert) als Hof "Muratum," der in einer Schenkungsurkunde dem Kloster Saint-Maurice überlassen wird. Die Deutung des Ortsnamens ist unsicher. Eine Herleitung von keltisch "mori(o)dūnum," das aus "mori" «See» und "dūnum" «Festung» zusammensetzt wäre, scheitert aus lautlichen Gründen. Nicht auszuschliessen ist hingegen eine Bildung zu lateinisch "mūrum" «Mauer» beziehungsweise suffixisch erweitertem "*mūráttu" «Mäuerchen, Gemäuer».

Murten gehörte bei seiner ersten Erwähnung zum (ersten) Königreich Burgund, das nach der Völkerwanderung auf den Trümmern des untergegangenen römischen Reiches entstand. Seit 534 stellten die fränkischen Merowinger die burgundischen Könige, seit 752 die Karolinger. Eine Überlieferung führt die Gründung des Schlosses auf das Jahr 814 und Ludwig den Frommen zurück.

888 entstand das zweite Königreich Burgund, dessen Zentrum wieder St. Maurice war. Murten war ein befestigter Ort dieses Königreichs, der um 1033 beziehungsweise 1034 von Kaiser Konrad II. erobert und praktisch dem Erdboden gleichgemacht wurde. In der Folgezeit versank Murten für mehr als 100 Jahre in der Bedeutungslosigkeit.

1127 hatten die Herzöge von Zähringen die Rechte im ehemaligen Königreich Burgund inne. Unter Berchtold IV. von Zähringen wurde die Stadt Murten mit dem typischen zähringischen Rechteck als Grundrissform in der Zeit zwischen 1157 und 1177 neu gegründet. Rasch erlebte die Stadt einen wirtschaftlichen Aufschwung und wurde 1218 nach dem Erlöschen des Geschlechts der Herzöge von Zähringen reichsfrei. Sie fristete aber ein unsicheres Dasein im Grenzgebiet zwischen den Besitztümern der Savoyer und der Kyburger resp. Habsburger. Ihre Ringmauer erhielt die Stadt ab 1238.

Nach den Wirren und Stürmen des dreizehnten Jahrhunderts hatte Murten definitiv an Savoyen einen festen Halt gefunden. Trotzdem hatte es auch nach anderen Seiten Verbindungen angeknüpft. Den ersten Bund, den Murten mit einer befreundeten Stadt eingegangen ist, datiert vom 24. Juni 1245 und wurde mit der Stadt Freiburg i.Uechtland geschlossen. In diesem Bund steht wörtlich geschrieben, dass die Bürger von Freiburg und Murten sich gegenseitige Hilfe in allen Notlagen und die Förderung des Friedens in der gemeinsamen Gegend mit einem Eid zusichern. Wörtlich ist niedergeschrieben: Damit aber nicht Jemandem im Verlaufe der Zeit Zweifel darüber entstehe, haben wir gegenwärtiges Privilegium unserer gegenseitigen Freundschaft zwischen Freiburg und Murten, im gemeinsamen Bundesbrief mit dem Siegel der Stadt Freiburg versehen.

Im Jahr 1255 geriet Murten zur Zeit Peters II. von Savoyen unter die Schutzherrschaft von Savoyen, in der sie mit wenigen Ausnahmen bis 1475 verblieb. Murten selbst wurde in der Folgezeit zum Mittelpunkt einer Herrschaft, welche das Gebiet südlich und östlich des Murtensees umfasste.

1318 trat Murten dem fünf Städte Bund bei. Am 25. September 1318 versammelten sich die fünf Städte im damals freiburgischen Gümmenen und schlossen einen Bund: «Im Namen Gottes, Amen. Wir die Schultheisse, Räte und Bürger der Städte Freiburg, Bern, Solothurn, Murten und Biel tun jedermann jetzt und später kund […] dass wir einen neuen Bund "(novam conspirationem)" getroffen haben.»

Durch eine Feuersbrunst wurden die damals noch weitgehend aus Holz gebauten Häuser 1416 schwer in Mitleidenschaft gezogen, während die Befestigungsanlagen nahezu unversehrt blieben. Murten konnte trotz der savoyischen Oberherrschaft eine gewisse Autonomie behaupten, auch weil es seit 1351 mit der benachbarten Reichsstadt Bern in einem Bündnis stand. Seit 1353 galt Murten über seine Verbindung mit Bern auch als zugewandter Ort der Eidgenossenschaft.

Ein neuer Abschnitt in der Stadtgeschichte wurde durch die Burgunderkriege zwischen der Eidgenossenschaft und Herzog Karl dem Kühnen von Burgund eingeleitet. Murten stand zu diesem Zeitpunkt unter Jakob von Savoyen, Graf von Romont und Grossmarschall von Burgund. Bei der Eröffnung der Feindseligkeiten zwischen Freiburg, Bern und Burgund zogen die beiden Städte auch gegen das mit Burgund verbündete Savoyen. Graf Jakob von Savoyen und zugleich Graf von Romont in Romont (Remund) besuchte 1475 Murten und inspizierte die Mauern, Türme und Festungswerk. Er ordnete an, das alles auszubessern, teilweise zu erneuern und mit Geschütz zu versehen sei. Weil den Savoyern die Annäherung Freiburgs an die Eidgenossen Sorgen bereitete, wollten sie Murten umso mehr verstärken. So blieb Murten nichts anderes, trotz der enormen Kosten die verlangten Verbesserungen vorzunehmen. Die Stadt Freiburg schickte ihre Maurer nach Murten und bezahlte diese selber, damit die Arbeit zügig voranging und die Kosten Murten nicht erdrückten.

Am 14. Oktober desselben Jahres willigte Freiburg auf das Drängen der Berner ein, zusammen mit ihnen nach Murten zu ziehen. Bereits am folgenden Tag standen die beiden vor Murten. Bern forderte von Murten, sich freiwillig zu ergeben und Berner zu werden, doch waren die Murtener ob dieser Forderung von einem Bündnispartner alles andere als angetan. Bern sagt ihnen zu, falls sie sich freiwillig ergäben, würde man sie nicht „schedigen“; andernfalls müssten «sy darumb liden dass inen an Lib und Gut übel keme». Auf das Versprechen der Freiburger, dass Murten selbständig bleiben dürfe, wenn es sich den beiden Städten Bern und Freiburg ergebe, ergab es sich freiwillig und schwur «zu beider Stetten Handen und thet man niemand, weder an Libe noch an Guot nüt». Bern und Freiburg einigten sich mit Zustimmung von Murten, dass eine Besatzung von Freiburgern unter dem Kommando von Wilhelm Perrotet stationiert wurde. Am Allerheiligentag 1475 erhielt Murten von beiden Städten den zugesicherten Freiheitsbrief.

Nachdem Karl der Kühne in der Schlacht bei Grandson eine Niederlage bezogen hatte, belagerte er am 9. Juni 1476 Murten, das von Adrian I. von Bubenberg und Wilhelm d’Affry verteidigt wurde. Am 22. Juni 1476 kam es zur Schlacht bei Murten, in der die Eidgenossen und ihre Verbündeten Karl dem Kühnen und seinen Truppen eine empfindliche Niederlage zufügten.

Im Frieden von Freiburg i. Ü. 1476 trat Savoyen unter anderem die Stadt und Herrschaft Murten an die Eidgenossenschaft ab. 1484 verzichteten die anderen eidgenössischen Orte gegen Geldentschädigung zulasten Berns und Freiburgs auf ihre Anteile an der Herrschaft über Murten, das diese von nun an bis zum Ende der Alten Eidgenossenschaft 1798 als gemeine Herrschaft verwalteten. Beide Stände stellten abwechslungsweise für fünf Jahre den Vogt, der im Schloss von Murten residierte. Die gemeine Herrschaft Murten umfasste den nördlichsten Teil des heutigen Kantons Freiburg mit den Zentren Murten, Kerzers und Vully. Die südliche Grenze verlief auf einer Linie von Courgevaux über Salvenach nach Ulmiz; diese Dörfer gehörten noch zur gemeinen Herrschaft wie auch die Exklave Wallenbuch.

1528 beschloss der Rat von Bern die Durchführung der Reformation im gesamten Machtbereich Berns und setzte diese auch in der gemeinen Herrschaft Murten durch. Dieses Vorgehen führte wiederum zu Auseinandersetzungen mit dem katholischen Freiburg, welches zur Lösung des Problems eine Befragung der Bevölkerung verlangte. Bern musste auf diese Verlangen eingehen, verzögerte aber die sofortige Abstimmung in Murten und setzte unter anderen den französischen Reformator Guillaume Farel als Reformprediger ein. In der 1530 erfolgten Abstimmung erlangten die Befürworter der Reformation eine kleine Mehrheit. Schliesslich übernahm Bern die Funktion der kirchlichen und schulischen Angelegenheiten, während Freiburg für die militärischen Angelegenheiten zuständig war. Bern gewann damit in friedlichen Zeiten mehr Einfluss auf das bürgerliche Leben, was allmählich zu einem Vordringen der deutschen Sprache in dem damals noch überwiegend französischsprachigen Städtchen führte.

Die vor den Toren von Murten gelegenen Orte Muntelier und Meyriez erhielten 1533 respektive 1536 das Gemeinderecht. Allerdings wurde je nur gerade ein Gebiet ausgeschieden, so weit die Häuser des Dorfes reichten, weshalb beide Gemeinden heute nur einen sehr kleinen Gemeindebann aufweisen. Nach dem Zusammenbruch des Ancien régime zur Zeit der Helvetik (1798) überliess die Berner Besatzung Murten den einmarschierten Franzosen.

Mit der Mediationsakte wurde die Stadt 1803 definitiv dem Kanton Freiburg zugeteilt. Murten wurde von Freiburg zum Hauptort des neu gebildeten Distrikts Murten bestimmt. Dieser wurde 1848 mit der neuen Kantonsverfassung aufgelöst und mit einer Reihe Gemeinden aus dem ehemaligen Deutschen Bezirk Freiburg im neu geschaffenen Seebezirk zusammengefasst, als dessen Hauptort weiterhin die Stadt Murten fungierte. Das 19. Jahrhundert war von einem immer wieder aufbrechenden Gegensatz zwischen der reformierten Bevölkerung von Stadt und Bezirk, die liberal bis radikal gestimmt war, und der konservativ-katholischen Mehrheit im Kanton Freiburg geprägt.

Am 29. Juni 1866 brach der Zirkuselefant des Wanderzirkus Bell & Myers aus und tötete seinen Wärter. Nach einer Verfolgungsjagd wurde das Tier in einer Gasse gestellt und mit einer Kanone erschossen.

Zwei kleinere Gebietsveränderungen erfolgten Ende des 20. Jahrhunderts, als zunächst das vorher selbständige Burg am 1. Januar 1975 sowie am 1. Januar 1991 die Gemeinde Altavilla nach Murten eingemeindet wurden. In den 10er-Jahren des neuen Jahrhunderts kam es zu zwei weiteren Veränderungen: Am 1. Januar 2013 fusionierte Büchslen mit der Stadt Murten und am 1. Januar 2016 stiessen Courlevon, Jeuss, Lurtigen und Salvenach zur Gemeinde Murten.

Murten besitzt eine malerische mittelalterliche Altstadt mit einem Ortsbild von nationaler Bedeutung. Sie hat die typische rechteckige Grundrissform der Zähringerstädte bewahrt und bedeckt eine Fläche von rund 300 m × 200 m. Das historische Städtchen ist durch drei Längsachsen und eine Quergasse untergliedert. Besonders die Hauptgasse zeichnet sich durch die charakteristischen Laubengänge aus. Die Bausubstanz der Häuser in der Altstadt stammt zum grössten Teil aus der Barockzeit des 17. und 18. Jahrhunderts.

Die Ringmauer von Murten gehört zu den am besten erhaltenen Befestigungsbauwerken der Schweiz. Sie wurde 1238 erstellt und später in mehreren Etappen ausgebaut, erhöht und verstärkt. Im 20. Jahrhundert wurde eine umfassende Restauration vorgenommen. Die ehemals vorhandenen Gräben wurden im Lauf des 16. Jahrhunderts zugeschüttet. Die fast vollständig erhaltene Ringmauer mit einer durchschnittlichen Höhe von 8.5 m besitzt einen Wehrgang aus dem 15. Jahrhundert, der im südlichen Abschnitt auf weite Strecken begehbar ist, sowie zwölf Türme in verschiedener Gestalt und Grösse. Die Stadt hatte zwei Haupteingänge, wovon das Berner Tor (erhielt seine heutige Form 1778) im Nordosten erhalten ist.

Am Südwestrand der Altstadt erhebt sich auf einem Vorsprung das Schloss, das ab Mitte des 13. Jahrhunderts unter Peter II. von Savoyen auf einem unregelmässigen fünfeckigen Grundriss erbaut wurde. Der älteste erhaltene Teil ist der massive viereckige Bergfried aus der Erbauungszeit. Die Aussenmauern des Schlosses sind in die Stadtbefestigung integriert und durch halbrunde Türme verstärkt. Die Wohngebäude wurden mehrfach umgebaut, vor allem während der Umwandlung des Schlosses in den Vogteisitz in der Zeit von 1476 bis 1540 und Ende des 18. Jahrhunderts, so dass heute verschiedene Stilrichtungen von der Spätgotik über die Spätrenaissance bis zum Barock miteinander vereinigt sind. Heute beherbergt das Schloss die Präfektur. Unterhalb des Schlosses befindet sich die alte Stadtmühle von 1578, in der das historische Museum untergebracht ist.

Die ursprüngliche Pfarrkirche von Murten befand sich ein Stück weit nordöstlich der Altstadt und kam bei der Abtrennung von Muntelier auf dessen Gemeindegebiet zu liegen. Die Kirche wurde 1762 abgerissen. Seit Mitte des 18. Jahrhunderts erfüllt die deutsch-reformierte Kirche die Funktion der Pfarrkirche für die deutsche Bevölkerungsmehrheit. Sie befindet sich an der Ostecke der Altstadt und wurde im Jahr 1399 erstmals als Kapelle Sainte-Marie erwähnt. Aus dieser Zeit stammen noch Teile des Chors, während die übrigen Partien bei Um- und Neubauten in späteren Jahren entstanden. Der Chorturm wurde 1683 in die Ringmauer integriert; das Kirchenschiff erhielt seine heutige Gestalt in der Zeit von 1710 bis 1713. Im Innern sind die reich geschnitzte Kanzel von 1484 und das Chorgestühl von 1494-98 sowie Gewölbemalereien von 1682 bis 1685 zu bewundern. Neben der Kirche steht das deutsch-reformierte Pfarrhaus im Berner Stil aus dem 18. Jahrhundert, in dem Jeremias Gotthelf geboren wurde.
Als reformierte französische Kirche dient die ursprüngliche Kapelle Sainte-Catherine, die von 1478 bis 1480 an der Nordecke der Altstadt erbaut wurde. Das Schiff stammt aus dem 18. Jahrhundert. Daneben befindet sich das 1732 erstellte französische Pfarrhaus.

Das Rathaus entstand 1474 durch Um- und Ausbau zweier ehemaliger Privathäuser. In mehreren Etappen folgten später weitere Umbauten. Die zum See hin zeigenden Arkaden stammen von 1589, die Hauptfassade von 1832. In der Altstadt sind zahlreiche Bürger- und Patrizierhäuser aus dem 16. bis 18. Jahrhundert erhalten. Erwähnenswert sind das heutige Hotel Murtenhof, dessen spätgotische Bausubstanz auf 1476 zurückgeht, das um 1740 für die Familie Schmid erbaute sogenannte Grosshaus, das bedeutendste Stadtpalais Murtens, und das Haus zum Rübenloch aus dem 16. Jahrhundert, ebenfalls mit einer spätgotischen Fassade und einem Berner Dach von 1672.

In Seeufernähe unterhalb der Altstadt stehen im Stadtteil Ryf verschiedene gotische Handwerkerhäuser. Ebenfalls ausserhalb der Umfassungsmauern befinden sich die katholische Kirche Sankt Mauritius, die 1885-87 im neugotischen Stil erbaut wurde, und das für die Familie Chaillet erbaute Herrenhaus Haldenhof von 1740. Das Schloss Löwenberg beim gleichnamigen Weiler nordöstlich der Stadt stammt im Wesentlichen aus der Zeit von 1666 bis 1700.

Die Murtner Solennität, ein Jugend- und Schulfest, gedenkt alljährlich am 22. Juni oder am Vortag, wenn der 22. Juni auf einen Sonntag fällt, der Murtenschlacht von 1476. Über den Tag verteilt werden auf dem «Kanonemätteli» 22 Böllerschüsse, jeweils zu den wichtigen Zeitpunkten, abgegeben – der erste zur Tagwache um 5 Uhr früh. Offizielle Feier, Blasmusikkonzert, Umzug von Schülern und Kadetten unter dem Klang von Trommlern und Blechbläsern, Ansprachen, Armbrustwettschiessen, Nationalhymne, Tanz und verschiedene Darbietungen stehen auf dem gegenwärtigen Programm dieses Feiertags, an dem viele ausgewanderte Murtner in ihre alte Heimat zurückkehren. Am morgendlichen Umzug durch das Stedtli nehmen weiss gekleidete oder uniformierte Schüler (Primarklassen und Oberstufe) sowie die Gemeinde- und Kantonsbehörden teil. Am Abend trifft sich die Bevölkerung gerne im «Soli-Pintli» auf dem Pausenplatz der Primarschule, um miteinander anzustossen oder den Hunger zu stillen.

An den Abstimmungssonntagen marschiert jeweils nach Eröffnung des Wahllokals eine Gruppe von Tambouren – deren Zahl kann von zwei bis acht gehen – durch die Altstadt von Murten und zieht zum Stimmlokal im alten Schulhaus vor der Stadt, um der Bevölkerung die anstehende Bürgerpflicht in Erinnerung zu rufen. Zu diesem Abstimmungs- und Wahlauftritt trifft sich die Gruppe beim Haus «Rübenloch» und zieht von dort trommelnd zum Wahllokal im Schulhaus. Das Wahltrommeln hat heute angesichts des überwiegenden Anteils der brieflichen Stimmabgabe vor allem einen symbolischen Charakter.

Die Tradition lässt sich bis in die Anfänge des Tambourenvereins zurückverfolgen, allerdings nur durch gelegentliche Erwähnungen in den Protokollen oder, ab 1934, in den (teilweise lückenhaften) Kassenbüchern, aus denen hervorgeht, dass die Stadt damals für das Wahltrommeln insgesamt 10 Franken an den Verein entrichtete. Im Jahresbericht 1936 wird das Wahltrommeln ausnahmsweise etwas detaillierter beschrieben: «[…] Am 3. Dezember fanden dann die Staats- u. Grossratswahlen statt, wo wir morgens 11 Uhr und nachmittags 1 Uhr durchs Städtchen denn Schall erklingen liessen und dann um 4 Uhr in der Wirtschaft Ringmauer bei einem Glas Wein und 1er Wurst den Abschluss fanden.»

Zum Gedenken an die Schlacht bei Murten wird seit 1930 jeweils am 22. Juni (wenn dieser auf einen Sonntag fällt) oder am auf den 22. Juni folgenden Sonntag das historische Murtenschiessen durchgeführt. Der Anlass findet auf dem «Bodemünzi» (Bois-Domingue) statt, einer Anhöhe bei Murten, wo sich im Juni 1476 das Lager des burgundischen Herzogs Karl des Kühnen und das Zentrum von dessen Belagerungsdispositiv befand. Die teilnehmenden Gruppen, jeweils aus zehn Schützen bestehend, treffen sich am Morgen beim Schulhaus vor dem Berntor und marschieren als Umzug, angeführt durch die Stadtmusik, durch die geschmückte Stadt und zum 1,5 km entfernten Bodemünzi, von dessen Kuppe aus eine Schusslinie gegen Süden, auf dem Boden der Gemeinde Münchenwiler angelegt ist.

Nach einem Gottesdienst und einer Ansprache durch einen Ehrengast – nicht selten nimmt seit der Jahrtausendwende der Chef des Departements für Verteidigung, Bevölkerungsschutz und Sport die Einladung an den Anlass an – beginnt der ungefähr zwei Stunden dauernde Schiessbetrieb. Zuvor findet aber eine Instruktion durch den Schützenmeister und eine Kontrolle der Waffen statt. Es wird mit Ordonanzwaffen auf eine Distanz von rund 180 bis 200 m geschossen.

Die Fastnacht in der Murtner Altstadt wird in der jetzigen Form ununterbrochen seit 1950 durchgeführt. In ihren Eckpunkten ist die Form seit damals gleich geblieben, es gab aber immer wieder punktuelle Anpassungen.

Die Fastnacht dauert drei Tage und Nächte, jeweils zwischen dem Samstagnachmittag und der Nacht von Montag auf Dienstag. Der Zeitpunkt der Fastnacht ist nicht an das Datum von Ostern gebunden, sondern ist auf eine Kalenderzeit, seit langem das erste Wochenende im März, festgelegt. Dementsprechend liegt die Murtner Fastnacht manchmal ziemlich nahe bei den anderen fasnächtlichen Anlässen im Kanton und in anderen Regionen – wenn Ostern spät ist –, es kann aber auch eine Verschiebung um mehrere Wochen geben. In Murten spricht man nicht von der „Fasnacht“, das Wort wird gemäss der Tradition mit einem t in der Mitte geschrieben: Fastnacht.

Den Kernpunkt der Fastnacht bildet der Umzug, jeweils am Sonntagnachmittag, bei dem die teilnehmenden Fastnachtsgruppen mit ihren Sujetwagen sowie die eingeladenen Guggenmusiken nach einer festgelegten Route durch die von Besuchern gesäumten Gassen der Altstadt ziehen. Dem Anlass sind durch die Enge der Gassen Grenzen gesetzt, da bei den grossen Cliquen der Wagenbau und der Betrieb zum gewählten Sujet recht viel Platz beanspruchen können. Bezüglich der Zuschauerzahl bieten die Gassen und Lauben Platz für höchstens etwa 12 000 Schaulustige (eine Zahl, die bei günstigen Witterungsverhältnissen immer wieder erreicht wurde). Weitere Fixpunkte der Fastnacht sind der Kinderumzug am Samstagnachmittag, die Proklamation der Fastnacht am Samstagabend sowie das Verbrennen des „Füdlibürgers“ am Montagabend.





</doc>
<doc id="12743" url="https://de.wikipedia.org/wiki?curid=12743" title="Doppler-Effekt">
Doppler-Effekt

Der Doppler-Effekt (selten Doppler-Fizeau-Effekt) ist die zeitliche Stauchung bzw. Dehnung eines Signals bei Veränderungen des Abstands zwischen Sender und Empfänger während der Dauer des Signals. Ursache ist die Veränderung der Laufzeit. Dieser rein kinematische Effekt tritt bei allen Signalen auf, die sich mit einer bestimmten Geschwindigkeit, meist Lichtgeschwindigkeit oder Schallgeschwindigkeit, ausbreiten. Breitet sich das Signal in einem Medium aus, so ist dessen Bewegungszustand zu berücksichtigen.

Bei periodischen Signalen erhöht bzw. vermindert sich die beobachtete Frequenz. Das betrifft sowohl Tonhöhen („iiiiuuuu“) als auch Modulationsfrequenzen, z. B. den Wechsel der Töne eines Martinhorns („tatü...taataa“). Bei geringen Geschwindigkeiten im Verhältnis zur Ausbreitungsgeschwindigkeit gibt dieses Verhältnis zugleich die relative Frequenzänderung formula_1 an. Bei reflektiertem Signal, wie beim Radar-Doppler und Ultraschall-Doppler, verdoppelt sich mit der Laufzeit auch die Doppler-Verschiebung formula_2.

Der Doppler-Effekt wurde bekannt durch Christian Doppler, der im Jahre 1842 Astronomen davon zu überzeugen versuchte, dass dieser Effekt die Ursache dafür sei, dass bei Doppelsternen zwischen den beiden Partnersternen Farbunterschiede erkennbar sind. Nach seiner Meinung kreisen diese Sterne so schnell umeinander, dass die Farbe des gerade vom Beobachter hinweg bewegten Sterns mit einer Rotverschiebung wahrgenommen wird, während die Farbe des zulaufenden Sterns in den blauen Bereich des Spektrums verschoben ist. Dieser Effekt konnte nach dem Tode Dopplers tatsächlich durch die Vermessung von Spektrallinien nachgewiesen werden. Er ist aber zu gering, um wahrnehmbare Farbunterschiede zu erklären. Die tatsächliche Ursache für mit dem Auge erkennbare Farbunterschiede zwischen Sternen sind deren Temperaturunterschiede.

Zur Erklärung des Effektes stellte Doppler ein Gedankenexperiment mit der Laufzeit von Wasserwellen an, die im Minutentakt von einem fahrenden Boot aus erzeugt werden. Daraus leitete er auch eine mathematische Beschreibung ab. Ein Verdienst von Doppler ist die Erkenntnis, dass die Endlichkeit der Lichtgeschwindigkeit auch eine Änderung der Wellenlänge des von bewegten Quellen eintreffenden Lichts bewirken muss. Im französischen Sprachraum wird dies oft Armand Fizeau (1848) zugesprochen.

Die Endlichkeit der Lichtausbreitung war bereits 180 Jahre zuvor von Ole Rømer gedeutet worden. Rømer interessierte sich für die Eignung der Jupitermonde als Zeitgeber zur Lösung des Längengradproblems. Die Verfinsterungen des Jupitermondes Io waren mit einer Frequenz von 1/1,8d bekannt, die gut als Zeitgeber geeignet wären. Allerdings stellte Rømer fest, dass sich diese Frequenz verringert, wenn sich die Erde auf ihrer Umlaufbahn um die Sonne gerade vom Jupiter wegbewegt. Mit formula_3 ist das formula_4 und verlängert die Zeit von Io-Finsternis zu Io-Finsternis gerade um , also ca. 1/4 Minute. Diese Verzögerung summierte sich nach 40 Umläufen von Io um Jupiter auf 10 Minuten, die Rømer für den 9. November 1676 vorhersagte. Auch wenn Rømer tatsächlich an der Frequenzänderung der Io-Finsternisse interessiert war: er interpretierte diese 10 Minuten viel einfacher als die Verzögerung, die das Licht für die entsprechend längere Wegstrecke benötigt hatte.

Für die Schallwellen hat der Naturforscher Christoph Buys Ballot im Jahre 1845 den Doppler-Effekt nachgewiesen. Er postierte dazu mehrere Trompeter sowohl auf einem fahrenden Eisenbahnzug als auch neben der Bahnstrecke. Im Vorbeifahren sollte jeweils einer von ihnen ein G spielen und die anderen die gehörte Tonhöhe bestimmen. Es ergab sich eine Verschiebung von einem Halbton, entsprechend einer Geschwindigkeit von 70 km/h.

Erst zwanzig Jahre später fand William Huggins die vorhergesagte spektroskopische Doppler-Verschiebung im Licht von Sternen. Er zeigte, dass Sirius sich stetig von uns entfernt.

Ein weiteres Jahrhundert später wurde durch Radar-Messungen zwischen Erde und Venus die Genauigkeit der Astronomischen Einheit von 10 (aus der Horizontalparallaxe von Eros) verbessert auf zunächst 10 anhand von Entfernungsmessungen in den unteren Konjunktionen der Jahre 1959 und 1961 (z. B. beim JPL durch Amplitudenmodulation mit bis zu 32 Hz), dann auf 10 durch Doppler-Messungen auf den Trägerfrequenzen über mehrere Monate vor und nach den unteren Konjunktionen der Jahre 1964 und 1966. Die Ergebnisse wurden wie 300 Jahre zuvor als Laufzeit angegeben, da der Wert der Lichtgeschwindigkeit damals erst auf sechs Stellen bekannt war.

Für den Nachweis der Periheldrehung des Merkur reichten Doppler-Messungen der Jahre 1964 bis 1966 – mit optischen Methoden waren anderthalb Jahrhunderte nötig.

Bei der Erklärung des akustischen Doppler-Effekts ist zu unterscheiden, ob sich die Schallquelle, der Beobachter, oder beide relativ zum Medium (der ruhenden Luft) bewegen.

Als Beispiel soll angenommen werden, dass das Martinshorn des Krankenwagens Schallwellen mit einer Frequenz von 1000 Hz aussendet. Dieses bedeutet, dass genau 1/1000 Sekunde nach dem ersten Wellenberg ein zweiter Wellenberg nachfolgt. Die Wellen breiten sich mit der Schallgeschwindigkeit formula_5 bei 20 °C aus.

Solange der Krankenwagen steht, ist die Wellenlänge formula_6 des Schalls, also der Abstand der Wellenberge:
Für einen Beobachter an der Straße kommen diese Wellenberge zwar je nach Entfernung etwas zeitverzögert an. Die Zeit zwischen zwei Wellenbergen ändert sich jedoch nicht. Die Grundfrequenz formula_8 des wahrgenommenen Tons ist für jeden Abstand von Beobachter und Krankenwagen gleich.

Die Situation ändert sich, wenn der Krankenwagen mit der Geschwindigkeit formula_9 auf den Beobachter zufährt. Da sich der Wagen in der Zeit zwischen den beiden Wellenbergen weiterbewegt, verkürzt sich der Abstand zwischen ihnen etwas. Er verkürzt sich um den Weg, den der Wagen in der Zeit von 1/1000 Sekunde zurücklegt:
Die Indizes formula_11 und formula_12 verweisen auf den Sender beziehungsweise Beobachter der Welle. Da sich beide Wellenberge mit derselben Schallgeschwindigkeit formula_13 zum Beobachter bewegen, bleibt der verkürzte Abstand zwischen ihnen erhalten, und der zweite Wellenberg kommt nicht erst 1/1000 Sekunde nach dem ersten an, sondern schon ein wenig früher.
Bezogen auf obiges Beispiel verkürzt sich die Wellenlänge bei einer Geschwindigkeit von formula_14 :

Dadurch erscheint dem Beobachter die Frequenz (also die Tonhöhe) des Martinshornes höher (formula_16).

Quantitativ erhält man die Frequenzänderung einfach durch Einsetzen der Beziehung formula_18 in obige Formel für formula_19. Für die vom Beobachter wahrgenommene Frequenz formula_20 ergibt sich somit:

Dabei bedeuten formula_8 die Frequenz der Schallquelle, formula_13 die Ausbreitungsgeschwindigkeit des Schalls und formula_9 die Geschwindigkeit der Schallquelle (also des Krankenwagens).

Wenn der Krankenwagen am Beobachter vorbeigefahren ist, verhält es sich sinngemäß umgekehrt: der Abstand zwischen den Wellenbergen (Wellenlänge) vergrößert sich, und der Beobachter hört einen tieferen Ton. Rechnerisch gilt obige Formel genauso, man muss nur für formula_9 eine negative Geschwindigkeit einsetzen. Bezogen auf das Beispiel:

Die beschriebenen Bewegungen der Signalquelle direkt auf den Beobachter zu oder direkt von ihm weg sind Spezialfälle. Bewegt sich die Signalquelle beliebig im Raum mit der Geschwindigkeit formula_27 so kann die Doppler-Verschiebung für einen ruhenden Empfänger zu

angegeben werden. formula_29 ist dabei der zeitabhängige Einheitsvektor, der die Richtung von der Signalquelle "S" zum Beobachter "B" beschreibt.

Auch bei ruhender Schallquelle formula_11 und bewegtem Beobachter formula_12 tritt ein Doppler-Effekt auf, allerdings ist hier die Ursache eine andere: Wenn der Wagen ruht, ändert sich auch nichts am Abstand zwischen den Wellenbergen, die Wellenlänge bleibt also gleich. Allerdings kommen die Wellenberge scheinbar schneller hintereinander bei dem Beobachter an, wenn sich dieser auf den Krankenwagen zubewegt:

Auch hier ergibt sich wieder der Fall eines sich entfernenden Beobachters durch Einsetzen einer negativen Geschwindigkeit.

Für eine beliebige Bewegung des Beobachters formula_12 mit dem Geschwindigkeitsvektor formula_27 ergibt sich bei ruhendem Sender formula_11 der Doppler-Effekt zu

wobei formula_29 wiederum der Einheitsvektor zur Beschreibung der Richtung von der Signalquelle formula_11 zum Beobachter formula_12 ist, der im allgemeinen Fall, genau wie der Geschwindigkeitsvektor formula_27, zeitabhängig sein kann.

Wie man sieht, sind die Gleichungen (1) und (2) nicht identisch (nur im Grenzfall formula_43 nähern sie sich einander an). Offensichtlich wird das im Extremfall: bewegt sich der Beobachter mit Schallgeschwindigkeit auf die Signalquelle zu, erreichen ihn die Wellenberge doppelt so schnell, und er hört einen Ton doppelter Frequenz. Bewegt sich hingegen die Signalquelle mit Schallgeschwindigkeit, wird der Abstand zwischen den Wellenbergen praktisch null, sie überlagern sich und es kommt zu einer extremen Verdichtung der Luft (siehe Schallmauerdurchbruch). Da so alle Wellenberge gleichzeitig beim Beobachter eintreffen, wäre das nach obiger Formel theoretisch eine unendliche Frequenz – praktisch hört man keinen Ton einer bestimmten Frequenz, sondern den Überschallknall.

Durch Kombination der Gleichungen (1) und (2) kann man eine Gleichung herleiten, welche die für den Beobachter wahrgenommene Frequenz formula_44 beschreibt, wenn der Sender und der Empfänger in Bewegung sind.

Sender und Empfänger bewegen sich aufeinander zu:

Sender und Empfänger bewegen sich voneinander weg:

Dabei ist formula_47 die Geschwindigkeit des Beobachters und formula_48 die Geschwindigkeit des Senders der Schallwellen relativ zum Medium.

Ebenfalls aus den oberen Gleichungen lässt sich die wahrgenommene Frequenz ableiten wenn die Welle eines ruhenden Senders S an einem mit der Geschwindigkeit formula_27 bewegten Objekt O gestreut wird und von einem ebenfalls ruhenden Beobachter B wahrgenommen wird.

formula_51 und formula_52 sind dabei jeweils die Einheitsvektoren vom stationären Sender zum bewegten Objekt und vom bewegten Objekt zum stationären Beobachter.

Anwendung findet diese Gleichung häufig in der akustischen oder optischen Messtechnik zur Messung von Bewegungen, z. B. Laser-Doppler-Anemometrie. Speziell in der Optik kann für formula_53 die Winkelabhängigkeit der gestreuten Frequenz zu

aus Beleuchtungsrichtung formula_51 und Beobachtungsrichtung formula_52 in sehr guter Näherung bestimmt werden.

Allgemein lässt sich der Frequenzunterschied schreiben als:

Dabei ist formula_47 die Geschwindigkeit des Beobachters und formula_59 die der Schallquelle, jeweils relativ zum Medium (z. B. der Luft). Das obere Operationszeichen gilt jeweils für Annäherung (Bewegung in Richtung des Senders bzw. Empfängers). D. h. beide Geschwindigkeiten werden positiv in Richtung des Beobachters bzw. Senders gemessen.
Mit formula_60 oder formula_61 ergeben sich die oben genannten Spezialfälle. Für formula_62 verschwindet der Effekt (es gibt also keine Tonhöhenänderung). Das tritt ein, wenn sich Sender und Empfänger in dieselbe Richtung mit derselben Geschwindigkeit relativ zum Medium bewegen; meist bewegt sich in solchen Fällen das Medium selbst, während Sender und Empfänger ruhen (Wind). Deswegen kommt es unabhängig von der Windstärke zu keinem Doppler-Effekt.

Die Formeln wurden unter der Annahme abgeleitet, dass sich Quelle und Beobachter direkt aufeinander zubewegen. In realen Fällen fährt z. B. der Krankenwagen in einem bestimmten Mindestabstand am Beobachter vorbei. Daher ändert sich der Abstand zwischen Quelle und Beobachter nicht gleichmäßig, und deswegen ist – besonders unmittelbar vor und nach dem Vorbeifahren – ein kontinuierlicher Übergang der Tonhöhe von höher zu tiefer zu hören.

Elektromagnetische Wellen breiten sich auch im Vakuum, also ohne Medium aus. Wenn sich der Sender der Wellen relativ zum Empfänger bewegt, tritt auch in diesem Fall eine Verschiebung der Frequenz auf. Dieser Relativistische Doppler-Effekt ist darauf zurückzuführen, dass die Wellen sich mit endlicher Geschwindigkeit, nämlich der Lichtgeschwindigkeit ausbreiten. Man kann ihn als geometrischen Effekt der Raumzeit auffassen.

Im Vakuum (Optischer Doppler-Effekt) hängt die beobachtete Frequenzänderung nur von der relativen Geschwindigkeit von Quelle und Beobachter ab; ob sich dabei die Quelle, der Beobachter oder beide bewegen, hat keinen Einfluss auf die Höhe der Frequenzänderung.

Aufgrund des Relativitätsprinzips darf sich jeder Beobachter als ruhend betrachten. Allerdings muss er dann bei der Berechnung des Doppler-Effekts zusätzlich zu obigen Betrachtungen auch noch die Zeitdilatation der relativ zum Beobachter bewegten Quelle berücksichtigen. Somit erhält man für den relativistischen Doppler-Effekt:
v > 0 bei Verringerung des Abstandes zwischen Quelle und Beobachter.

Bewegt sich ein Objekt zu einem gewissen Zeitpunkt quer zum Beobachter, so kann man die Änderung des Abstandes zu diesem Zeitpunkt vernachlässigen; dementsprechend würde man hier auch keinen Doppler-Effekt erwarten. Jedoch besagt die Relativitätstheorie, dass jedes Objekt aufgrund seiner Bewegung einer Zeitdilatation unterliegt, aufgrund der die Frequenz ebenfalls verringert wird. Diesen Effekt bezeichnet man als transversalen Doppler-Effekt. Die Formel hierfür lautet

wobei formula_13 hier die Vakuum-Lichtgeschwindigkeit und formula_9 die Geschwindigkeit der Signalquelle bezeichnet.

Der transversale Doppler-Effekt kann bei nicht-relativistischen Geschwindigkeiten (also Geschwindigkeiten weit unter der Lichtgeschwindigkeit) allerdings vernachlässigt werden.

Der Doppler-Effekt lässt sich ganz allgemein abhängig vom Winkel der Bewegungsrichtung zur Achse Quelle-Empfänger angeben. Die Frequenzänderung für beliebige Winkel α ergibt sich zu

Wenn man für den Winkel α nun 0° (Quelle bewegt sich direkt auf Empfänger zu), 90° (Quelle bewegt sich seitwärts), oder 180° (Quelle bewegt sich direkt vom Empfänger weg) einsetzt, dann erhält man die oben stehenden Gleichungen für longitudinalen und transversalen Doppler-Effekt. Man erkennt außerdem, dass der Winkel, unter dem der Doppler-Effekt verschwindet, von der Relativgeschwindigkeit abhängt, anders als beim Doppler-Effekt für Schall, wo er immer 90° beträgt.

Auch wenn die zu beobachtenden Auswirkungen von Doppler-Effekt und astronomischer Rotverschiebung identisch sind (Verminderung der beobachteten Frequenz der elektromagnetischen Strahlung eines Sterns oder einer Galaxie), so dürfen beide trotzdem nicht verwechselt werden, da sie gänzlich andere Ursachen haben.
Der relativistische Doppler-Effekt ist nur dann Hauptursache für die Frequenzänderung, wenn sich Sender und Empfänger wie oben beschrieben durch die Raumzeit bewegen und ihr Abstand so gering ist, dass die Ausdehnung des zwischen ihnen liegenden Raumes im Verhältnis gering ist. Ab einer bestimmten Entfernung überwiegt bei weitem jener Anteil, der durch die Ausdehnung der Raumzeit selbst hervorgerufen wird, so dass der Anteil des hier diskutierten Doppler-Effekts gänzlich vernachlässigt werden kann.

Radialgeschwindigkeiten sind durch den Doppler-Effekt messbar, wenn der Empfänger die Frequenz des Senders genügend genau kennt, insbesondere bei Echos von akustischen und elektromagnetischen Signalen.

Scharfe Spektrallinien erlauben eine entsprechend hohe Auflösung der Doppler-Verschiebung. Berühmt ist der Nachweis der Doppler-Verschiebung im Gravitationsfeld (Pound-Rebka-Experiment). Beispiele in der Astrophysik sind die Rotationskurven von Galaxien, spektroskopische Doppelsterne, die Helioseismologie und der Nachweis von Exoplaneten.

Beim Doppler-Radar berechnet man die Annäherungsgeschwindigkeit eines Objekts aus der gemessenen Frequenzänderung zwischen gesendetem und reflektiertem Signal. Die Besonderheit bei einem aktiven Radargerät ist jedoch, dass der Doppler-Effekt zweimal auftreten kann, auf dem Hin- und auf dem Rückweg. Ein Radarwarngerät, das die Signale des Hinwegs empfängt, misst eine Frequenz, die in Abhängigkeit von der Relativgeschwindigkeit variiert. Diese registrierte Frequenz wird von ihm reflektiert. Das Radargerät registriert die bereits Doppler-verschobenen Frequenzen wiederum in Abhängigkeit von der dann bestehenden Relativgeschwindigkeit. Im Fall eines unbeschleunigten Radargeräts tritt eine exakt zweifache Doppler-Verschiebung auf.


In der Medizin wird der akustische Doppler-Effekt bei Ultraschalluntersuchungen ausgenutzt, um die Blutstromgeschwindigkeit darzustellen und zu messen. Dabei hat er sich als außerordentlich hilfreich erwiesen.
Es gibt dabei einen:

Für die berührungslose Messung der Geschwindigkeitsverteilung von Fluiden (Flüssigkeiten und Gase) wird die Laser-Doppler-Anemometrie (LDA) angewandt. Sie beruht auf dem optischen Doppler-Effekt an streuenden Partikeln in der Strömung. In gleicher Weise dient ein Vibrometer der Messung der Schnelle vibrierender Oberflächen.


Während der Segmentierung von Wirbeltier-Embryonen laufen Wellen von Genexpression durch das paraxiale Mesoderm, das Gewebe, aus dem die Vorläufer der Wirbelkörper (Somiten) geformt werden. Mit jeder Welle, die das anteriore Ende des präsomitischen Mesoderms erreicht, wird ein neuer Somit gebildet. In Zebrabärblingen wurde gezeigt, dass die Verkürzung des paraxialen Mesoderms während der Segmentierung einen Doppler-Effekt verursacht, da sich das anteriore Ende des Gewebes in die Wellen hineinbewegt. Dieser Doppler-Effekt trägt zur Geschwindigkeit der Segmentierung bei.

Ein ruhender Beobachter hört eine Schallquelle, die sich genau auf ihn zubewegt, mit der Frequenz "f<nowiki>'</nowiki>"("v/c"), siehe Gleichung (1), wenn sie sich von ihm entfernt, mit der Frequenz "f<nowiki>'</nowiki>"("v/c"), siehe Gleichung (2). Bei Schallquellen spielt der relativistische transversale Doppler-Effekt keine Rolle. Je weiter der Beobachter von der linearen Flugbahn entfernt ist, desto langsamer ändert sich die radiale Geschwindigkeitskomponente bei Annäherung. Die Schnelligkeit der Frequenzänderung hängt ab von der kürzesten Entfernung zwischen Beobachter und Signalquelle. Das Diagramm rechts zeigt die Frequenzabhängigkeit relativ zu einem im Ursprung ruhenden Beobachter. Die rote Linie entspricht der Frequenz, die er hört, wenn ihn die Signalquelle in großem Abstand passiert, blau der bei geringem Abstand. Maximal- und Minimal-Frequenzen liegen nicht symmetrisch zur Eigenfrequenz, da die Geschwindigkeit "v" nicht sehr viel kleiner ist als die Schallgeschwindigkeit "c". Es gelten die Beziehungen (1) und (2).

Sind die Koordinaten der bewegten Signalquelle bekannt, kann man aus dem Frequenzverlauf den eigenen Standort ableiten (siehe z. B. Transit (Satellitensystem)).

Die Tonbeispiele geben die Tonhöhen, die ein ruhender Beobachter hört, wenn eine Signalquelle an ihm vorbeifliegt. Sie vernachlässigen den Effekt, dass die sich entfernende Quelle länger zu hören ist als die sich nähernde:
Erhöht sich die relative Geschwindigkeit, verschieben sich die Frequenzen:

Bei der Planung der Weltraummission Cassini-Huygens war nicht bedacht worden, dass der Funkverkehr zwischen den beiden Teilsystemen Cassini und Huygens durch den Doppler-Effekt einer Frequenzverschiebung unterliegt. Simulierende Tests wurden erst während der Reise durchgeführt, zu spät, um die Ursache, eine zu steif parametrisierte Phasenregelschleife, zu korrigieren. Diverse Maßnahmen im Umfeld des Fehlers konnten den erwarteten Datenverlust von 90 % auf 50 % senken. Zusätzlich wurde daher die Flugbahn der Mission verändert, um Datenverluste durch diesen Fehler ganz zu vermeiden.




</doc>
<doc id="12744" url="https://de.wikipedia.org/wiki?curid=12744" title="Cia">
Cia

Cia steht für:

Cia ist der Familienname folgender Personen:

Cia ist der Vorname folgender Personen:

cia steht für:

CIA steht für:

C. I. A. steht für:

Siehe auch:


</doc>
<doc id="12750" url="https://de.wikipedia.org/wiki?curid=12750" title="Marine">
Marine

Unter dem Begriff Marine versteht man die Gesamtheit der zur See fahrenden Flotte eines Staates. Diese besteht aus seiner "Handelsmarine" und seinen Seestreitkräften. 
Der Begriff Marine leitet sich vom lateinischen Wort "marinus", zum Meer gehörig, ab. 

In vielen Staaten lassen sich die militärischen Schiffe der Marine von den zivilen Schiffen der Marine und denen der Handelsmarine an der geführten Flagge unterscheiden. Kriegsschiffe führen eine Seekriegsflagge (), zivile Schiffe der Marine eine Dienstflagge () und Handelsschiffe die Handelsflagge ().

Die Seestreitkräfte eines Staates werden im allgemeinen Sprachgebrauch auch die Flotte genannt. Sie bestehen aus der eigentlichen Flotte, den dazugehörenden unterstützenden Einheiten und Einrichtungen an Land sowie deren Organisation und Administration.

Die ersten organisierten Marinen waren bei den Karthagern, Griechen und Römern anzutreffen.

Die Marine kann mit Kriegsschiffen, Hilfsschiffen und sonstigen zugeordneten Anteilen wie Marinefliegern, Marineinfanterie, Unterstützungs- und Ausbildungseinrichtungen ausgerüstet sein.

Innerhalb militärischer Marinen wird meist organisatorisch zwischen einer oder mehreren Flotten und den unterstützenden Elementen wie etwa Stützpunkten, Schulen usw. unterschieden. Die Einsatzkräfte zu Wasser, zu Lande und in der Luft unterstehen dem Flottenkommando. Große Marinen haben mehrere Flotten, wie zum Beispiel die US Navy mit ihrer 2., 3., 4., 5., 6. und 7. Flotte oder die russische Marine mit der Pazifik-, der Schwarzmeer-, der Nordmeer- und der Baltischen Flotte sowie der Kaspischen Flottille.

Die Beherrschung der See ist die Hauptaufgabe von Marinen im Krieg. Starke Seestreitkräfte sollen sicherstellen, dass ihr Land die Seewege als Transportwege für Güter und militärische Kräfte nutzen kann. Unterlegene Seestreitkräfte konzentrieren sich meist darauf, dem Gegner diese Nutzung zu verwehren, auch wenn man die See selbst nicht für eigene Zwecke nutzen kann. Typisch für diese Art der Auseinandersetzung auf See waren die beiden Weltkriege, in denen die zur See überlegenen Alliierten den Nordatlantik als Versorgungsroute benötigten, während die unterlegenen Deutschen diese Nutzung mit ihren U-Booten und Hilfskreuzern verhindern wollten, ohne je selbst die Chance zu haben, eigene Seetransporte über den Atlantik zu führen. Insofern sind Seekriege häufig Abnutzungskriege; die Seeschlacht ist die Ausnahme, auch wenn sie das Bild des Seekriegs im allgemeinen Bewusstsein geprägt hat.

Marinen entfalten Seemacht als militärische Wirkung nicht erst im Kriege, sondern bereits im Frieden und in Krisen oder regionalen Konflikten. So ist der Schutz der Seewege und des Seehandels gegen Piraterie für die meisten Marinen eine Daueraufgabe auch im Frieden. Dazu gehört außerdem die Beseitigung von Gefahren in den Gewässern wie beispielsweise durch Seeminen. Viele Marinen haben außerdem nationale Polizeiaufgaben, zum Beispiel bei der Überwachung und Durchsetzung des Meeresumweltschutzes. Bei Krisen und regionalen Konflikten können Seestreitkräfte anders als Landstreitkräfte auf der Hohen See ohne völkerrechtliche Hindernisse bewegt werden. Ihre Präsenz in einer Region unterstreicht den Machtanspruch der jeweiligen Nation.

In Deutschland meint man mit Marine den auf die militärischen Aufgaben auf bzw. von See spezialisierten Teil der Bundeswehr, in Abgrenzung zum Heer und zur Luftwaffe.

Die Dienstgrade in der Marine haben meist andere Bezeichnungen als die des Heeres oder der Luftwaffe. Dabei sind die militärischen Bezeichnungen teilweise an die der Handelsmarine angelehnt.

Neben dem heute gebräuchlichen Wort Handelsflotte wird auch der Begriff Handelsmarine für die Gesamtheit aller Handelsschiffe eines Landes und die dazugehörige Administration verwendet. In Deutschland regelt : "„Alle deutschen Kauffahrteischiffe bilden eine einheitliche Handelsflotte.“"


Verwandte Begriffe


</doc>
<doc id="12752" url="https://de.wikipedia.org/wiki?curid=12752" title="Wärmepumpe">
Wärmepumpe

Eine Wärmepumpe ist eine Maschine, die unter Aufwendung von technischer Arbeit thermische Energie aus einem Reservoir mit niedrigerer Temperatur (in der Regel ist das die Umgebung) aufnimmt und – zusammen mit der Antriebsenergie – als Nutzwärme auf ein zu beheizendes System mit höherer Temperatur (Raumheizung) überträgt. Der verwendete Prozess ist im Prinzip die Umkehrung eines Wärme-Kraft-Prozesses, bei dem Wärmeenergie mit hoher Temperatur aufgenommen und teilweise in mechanische Nutzarbeit umgewandelt und die Restenergie bei niedrigerer Temperatur als Abwärme abgeführt wird, meist an die Umgebung. Das Prinzip der Wärmepumpe verwendet man auch zum Kühlen (so beim Kühlschrank), während der Begriff „Wärmepumpe“ nur für das Heizaggregat verwendet wird. Beim Kühlprozess ist die Nutzenergie die aus dem zu kühlenden Raum aufgenommene Wärme, die zusammen mit der Antriebsenergie als Abwärme an die Umgebung abgeführt wird.

Wärmepumpen werden in der Regel mit Medien betrieben, die bei niedrigem Druck unter Wärmezufuhr verdampfen und nach der Verdichtung auf einen höheren Druck unter Wärmeabgabe wieder kondensieren. Der Druck wird so gewählt, dass die Temperaturen des Phasenübergangs einen für die Wärmeübertragung ausreichenden Abstand zu den Temperaturen von Wärmequelle und Wärmesenke haben. Je nach verwendetem Stoff liegt dieser Druck in unterschiedlichen Bereichen. Abbildung 1 zeigt das Schaltbild mit den vier für den Prozess erforderlichen Komponenten: Verdampfer, Verdichter (Kompressor), Kondensator und Drossel, Abbildung 2 den Prozess im T-s-Diagramm. Theoretisch wäre es möglich, die Arbeitsfähigkeit des Kondensates beim Entspannen auf den niedrigeren Druck durch eine Kraftmaschine, beispielsweise eine Turbine, zu nutzen. Doch dabei würde die Flüssigkeit teilweise verdampfen und so große technische Schwierigkeiten bei einem nur geringen Energiegewinn verursachen, so dass man der Einfachheit halber hier eine Drossel verwendet (Entspannung mit konstanter Totalenthalpie).

Bei der Wärmepumpe werden physikalische Effekte des Übergangs einer Flüssigkeit in die gasförmige Phase und umgekehrt ausgenutzt.
So zeigt Propan die Eigenschaft, abhängig vom Druck und seiner Temperatur einerseits entweder gasförmig oder flüssig zu sein und andererseits als Gas bei Kompression heiß zu werden und sich bei Entspannung abzukühlen: Propan bei normalem Luftdruck und kühler Außentemperatur (zum Beispiel 5 °C) ist gasförmig, komprimiert man es, wird es wärmer, bleibt aber gasförmig. Kühlt man es dann auf Zimmertemperatur ab, wird es flüssig (dabei sinkt der Druck wieder etwas). Wenn man das flüssige Propan entspannt, verdampft es (es wird wieder zu Gas) und wird dabei sehr kalt.

Diesen Effekt nutzt man bei der Wärmepumpe aus: Das Propangas wird im Verdichter durch einen Motor zusammengepresst und erhitzt sich dabei. Das heiße, komprimierte Gas kann dann im Wärmetauscher seine Wärme an das Wasser der Heizungsanlage abgeben. Dabei kühlt sich das komprimierte Gas ab und kondensiert zu flüssigem Propan (der Wärmetauscher einer Wärmepumpe wird deshalb Kondensator genannt). Beim anschließenden Durchgang durch die sogenannte Drossel (vereinfacht gesagt: einer extremen Engstelle im Rohr) wird das flüssige Propan entspannt, verdampft dabei und wird sehr kalt (deutlich kälter als 5 °C). Lässt man das kalte Gas dann durch einen zweiten Wärmetauscher (meist außerhalb des Hauses) strömen, der von außen - zum Beispiel durch Grundwasser oder die Außenluft - immer bei zum Beispiel 5 °C gehalten wird, erwärmt sich das sehr kalte Gas auf 5 °C und die Umgebung kühlt sich um 1 oder 2 °C ab. Auf diese Weise nimmt das Propan aus dem Grundwasser oder der Außenluft genauso viel Wärme auf, wie es vorher an das Heizungswasser abgegeben hat. Es wird dann wieder dem Verdichter zugeführt und der Prozess beginnt von neuem.

Die benötigte Energie zum Antrieb der Wärmepumpe verringert sich, das heißt der Betrieb wird umso sparsamer, je geringer die Temperaturdifferenz zwischen Erdtemperatur und Vorlauftemperatur der Heizungsanlage ist. Diese Bedingung erfüllen Niedertemperaturheizungen am besten, deshalb wird die Wärme im Wohnraum häufig durch eine Fußbodenheizung abgegeben.

Je nach Auslegung des Systems kann der Heizenergieaufwand um zirka 30 bis 50 % reduziert werden. Durch Kopplung mit Solarstrom, Haushaltsstrom oder Erdgas zum Antrieb der Wärmepumpe kann die Kohlendioxidemission im Vergleich zum Heizöl erheblich gesenkt werden.

Der Auswahl der richtigen Wärmequelle kommt eine besondere Bedeutung zu, denn von ihr hängt die Effizienz einer Wärmepumpe ab. Da die Wärmequelle oft eine Standzeit von mehr als 50 Jahren hat, ist ihr Bau eine Investition für Generationen.

Bei der direkten elektrischen Beheizung, zum Beispiel mit Heizstäben, entspricht die erzeugte Wärmeenergie genau der eingesetzten elektrischen Energie. Die elektrische Energie ist aber wesentlich hochwertiger als Wärmeenergie bei niedriger Temperatur, denn durch Einsatz einer Wärmekraftmaschine kann immer nur ein Teil der Wärmeleistung wieder in elektrische Leistung umgeformt werden.

Der Abluft, der Außenluft, dem Erdboden, dem Abwasser oder dem Grundwasser kann Wärme durch Einsatz einer Wärmepumpe entzogen werden. Ein Vielfaches der für die Wärmepumpe eingesetzten elektrischen Leistung kann der Wärmequelle (Luft, Erdboden) entzogen werden und auf ein höheres Temperaturniveau gepumpt werden. In der Leistungsbilanz wird der Wärmepumpe elektrische Leistung für den Verdichterantrieb und die der Umwelt entzogene Wärme zugeführt. Am Austritt der Wärmepumpe steht ein Teil der zugeführten Leistung als Wärme auf höherem Niveau zur Verfügung. In der Gesamtleistungsbilanz sind noch die Verluste des Prozesses zu berücksichtigen.

Das Verhältnis von der in den Heizkreis abgegebenen Wärmeleistung zur zugeführten elektrischen Verdichterleistung wird als Leistungszahl bezeichnet. Die Leistungszahl hat einen oberen Wert, der nicht überschritten und aus dem Carnot-Kreisprozess abgeleitet werden kann. Die Leistungszahl wird auf einem Prüfstand gemäß der Norm EN 14511 (früher EN 255) ermittelt und gilt nur unter den jeweiligen Prüfbedingungen. Gemäß EN 14511 wird die Leistungszahl auch COP genannt "(Coefficient Of Performance)". Der COP ist Gütekriterium für Wärmepumpen, erlaubt jedoch keine energetische Bewertung der Gesamtanlage.

Um eine möglichst hohe Leistungszahl und somit eine hohe Energieeffizienz zu erlangen, sollte die Temperaturdifferenz zwischen der Temperatur der Wärmequelle und der Nutztemperatur möglichst gering sein. Die Wärmeübertrager sollten für möglichst geringe Temperaturdifferenzen zwischen der Primär- und Sekundärseite ausgelegt sein.

Die Bezeichnung Wärmepumpe beruht darauf, dass Wärme aus der Umgebung auf ein höheres nutzbares Temperaturniveau angehoben ("gepumpt") wird. Die Wärmepumpe hat einen Verdichter, der elektrisch oder durch einen Verbrennungsmotor angetrieben wird. Der Verdichter komprimiert ein Kältemittel auf einen höheren Druck wobei es sich erwärmt. Die beim nachfolgenden Abkühlen und Verflüssigen des Kältemittels freigesetzte Energie wird in einem Wärmeübertrager auf das Wärmeträgermedium des Heizkreises, meistens Wasser oder Sole, übertragen. Das Kältemittel wird anschließend an einem Expansionsventil entspannt und es kühlt sich ab. Das kalte Kältemittel wird dem Verdampfer (Erdwärmesonden, Luftverdampfer) zugeführt und geht durch Aufnahme von Umgebungswärme (Anergie) in den gasförmigen Zustand über.

Ein Nachteil der Wärmepumpe ist der deutlich höhere apparative Aufwand. Besonders kostenintensiv sind wirkungsvolle Verdampfer (Erdwärmesonden, erdverlegte Flächenverdampfer). Die Investitionen gegenüber einem konventionellen Gas- oder Heizölbrenner sind deutlich höher. Dafür ist der regelmäßige Aufwand für Wartung und Instandhaltung deutlich geringer, zum Beispiel fallen keine Reinigungs- und Schornsteinfegerkosten an.

Der Wärmepumpenprozess, nach Rudolf Plank "Plank-Prozess" genannt, wird auch als Kraftwärmemaschine bezeichnet. Der Grenzfall einer reversibel arbeitenden Kraftwärmemaschine ist der linksläufige Carnotprozess.

Von 1930 bis zum Anfang der 1990er Jahre waren die Fluorchlorkohlenwasserstoffe (FCKW) die bevorzugten Kältemittel. Sie kondensieren bei Raumtemperatur unter leicht handhabbarem Druck. Sie sind nicht giftig, nicht brennbar und reagieren nicht mit den üblichen Werkstoffen. Wenn FCKW freigesetzt werden, schädigen sie jedoch die Ozonschicht der Atmosphäre und tragen zum Ozonloch bei. In Deutschland wurde daher der Einsatz von Fluorchlorkohlenwasserstoffen im Jahr 1995 verboten. Die als Ersatz verwendeten Fluorkohlenwasserstoffe (FKW) schädigen nicht die Ozonschicht, tragen jedoch zum Treibhauseffekt bei und sind im Kyoto-Protokoll als umweltgefährdend erfasst. Als natürliche Kältemittel gelten reine Kohlenwasserstoffe wie Propan oder Propylen, wobei deren Brennbarkeit besondere Sicherheitsmaßnahmen erforderlich macht. Anorganische, nicht brennbare Alternativen wie Ammoniak, Kohlendioxid oder Wasser wurden ebenfalls für Wärmepumpen eingesetzt. Aufgrund spezifischer Nachteile haben sich diese Kältemittel nicht im größeren technischen Maßstab durchsetzen können. Ammoniak (NH) und Kohlendioxid (CO) werden generell in industriellen Kühlanlagen wie Kühlhäusern und Brauereien eingesetzt. CO ist anstelle von Fluorkohlenwasserstoffen für die Klimatisierung von Fahrzeugen angedacht und wird bereits von ersten Herstellern eingesetzt (Stand 2017).

Die Leistungszahl ε, in der Literatur auch als Heizzahl bezeichnet, einer Wärmepumpe, englisch "Coefficient Of Performance" formula_1, ist der Quotient aus der Wärme, die in den Heizkreis abgegeben wird, und der eingesetzten Energie:

Bei typischen Leistungszahlen von 4 bis 5 steht das Vier- bis Fünffache der eingesetzten Leistung als nutzbare Wärmeleistung zur Verfügung, der Zugewinn stammt aus der entzogenen Umgebungswärme.

Die Leistungszahl hängt stark vom unteren und oberen Temperaturniveau ab. Die theoretisch maximal erreichbare Leistungszahl formula_3 einer Wärmepumpe ist entsprechend dem zweiten Hauptsatz der Thermodynamik begrenzt durch den Kehrwert des Carnot-Wirkungsgrads formula_4

Für die Temperaturen sind die absoluten Werte einzusetzen.

Der Gütegrad formula_6 einer Wärmepumpe ist die tatsächliche Leistungszahl bezogen auf die ideale Leistungszahl bei den verwendeten Temperaturniveaus. Er berechnet sich zu:

Praktisch werden Wärmepumpengütegrade formula_6 im Bereich 0,45 bis 0,55 erreicht.

Das untere Temperaturniveau einer Wärmepumpe liegt bei 10 °C (= 283,15 K), und die Nutzwärme wird bei 50 °C (= 323,15 K) übertragen. Bei einem idealen reversiblen Wärmepumpenprozess, der Umkehrung des Carnotprozesses, würde die Leistungszahl bei 8,1 liegen. Real erreichbar ist bei diesem Temperaturniveau eine Leistungszahl von 4,5. Mit einer Energieeinheit Exergie, die als technische Arbeit oder elektrische Leistung eingebracht wird, können 3,5 Einheiten Anergie aus der Umgebung auf das hohe Temperaturniveau gepumpt werden, so dass 4,5 Energieeinheiten als Wärme bei 50 °C Heizungs-Vorlauftemperatur genutzt werden können. ("1 Einheit Exergie + 3,5 Einheiten Anergie = 4,5 Einheiten Wärmeenergie").

In der Gesamtbetrachtung müssen aber der exergetische Kraftwerkwirkungsgrad und die Netzübertragungsverluste berücksichtigt werden, welche einen Gesamtwirkungsgrad von ca. 35 % erreichen. Die benötigte 1 kWh Exergie erfordert einen Primärenergieeinsatz von 100 / 35 × 1 kWh = 2,86 kWh. Wenn die Primärenergie nicht im Kraftwerk eingesetzt, sondern direkt vor Ort zur Beheizung genutzt wird, erhält man bei einem Feuerungswirkungsgrad von 95 % – demnach 2,86 kWh × 95 % = 2,71 kWh thermische Energie.

Mit Bezug auf das oben aufgeführte Beispiel kann im Idealfall (Leistungszahl = 4,5) mit einer Heizungswärmepumpe das 1,6-fache und bei einer konventionellen Heizung das 0,95-fache der eingesetzten Brennstoffenthalpie als Wärmeenergie umgesetzt werden. Unter sehr günstigen Randbedingungen kann so bei dem Umweg Kraftwerk → Strom → Wärmepumpe eine 1,65-fach höhere Wärmemenge gegenüber der direkten Verbrennung erreicht werden.

Am Prüfstand wird bei einer Grundwassertemperatur von 10 °C und einer Temperatur der Nutzwärme von 35 °C eine Leistungszahl von bis zu COP=6,8 erreicht. In der Praxis wird allerdings der tatsächlich über das Jahr erreichbare Leistungswert, die Jahresarbeitszahl (JAZ) incl. Verluste und Nebenantriebe, von nur 4,2 erzielt. Bei Luft/Wasser-Wärmepumpen liegen die Werte weit darunter, so dass die tatsächliche Reduzierung des Primärenergiebedarfs nicht so hoch ausfällt und unter ungünstigen Bedingungen (z. B. bei Strom aus fossilen Brennstoffen) sogar mehr Energie verbraucht wird als bei einer konventionellen Heizungsanlage. Dann wird eigentlich eine komplizierte Stromheizung betrieben, die weder im Hinblick auf den Klimaschutz noch volkswirtschaftlich sinnvoll ist. Eine Wärmepumpe mit einer JAZ > 3 gilt als energieeffizient. Allerdings werden laut einer Studie bereits bei dem Strommix aus dem Jahr 2008 bereits ab einer JAZ von 2 Kohlendioxidemissionen eingespart, mit weiterem Ausbau der Erneuerbaren Energien sowie dem Ersatz älterer Kraftwerke durch modernere und effizientere steigt das Einsparpotential, auch bestehender Wärmepumpen, weiter an.

In den Datenblättern zu den diversen Wärmepumpenerzeugnissen sind die Leistungsparameter jeweils auf Medium und Quell- und Zieltemperatur bezogen; zum Beispiel:

Nach mehreren gemessenen COP-Werte am WPT-Buchs. Angaben wie W10/W50 bezeichnen die Eingangs- und Ausgangstemperaturen der beiden Medien. W steht für Wasser, A für Luft (engl. air) und B für Sole (engl. brine), die Zahl dahinter für die Temperatur in °C. B0/W35 ist bspw. ein Betriebspunkt der Wärmepumpe mit einer Soleeintrittstemperatur von 0 °C und einer Wasseraustrittstemperatur von 35 °C.




Es gibt verschiedene physikalische Effekte, die in einer Wärmepumpe Verwendung finden können. Die wichtigsten sind:


Die elektrisch angetriebene Kompressions-Wärmepumpe stellt den Hauptanwendungsfall von Wärmepumpen dar. Das Kältemittel wird in einem geschlossenen Kreislauf geführt. Es wird von einem Verdichter angesaugt, verdichtet und dem Verflüssiger zugeführt. Der Verflüssiger ist ein Wärmeübertrager in dem die Verflüssigungswärme an ein Fluid – zum Beispiel an einen Warmwasserkreis oder an die Raumluft – abgegeben wird. Das verflüssigte Kältemittel wird dann zu einer Entspannungseinrichtung geführt (Kapillarrohr, thermisches oder elektronisches Expansionsventil). Durch die adiabate Entspannung wird das Kältemittel abgekühlt. Der Saugdruck wird durch die Entspannungseinrichtung in Kombination mit der Förderleistung des Verdichters in der Wärmepumpe so eingestellt, dass die Sattdampftemperatur des Kältemittels unterhalb der Umgebungstemperatur liegt. In dem Verdampfer wird somit Wärme von der Umgebung an das Kältemittel übertragen und führt zum Verdampfen des Kältemittels. Als Wärmequelle kann die Umgebungsluft oder ein Solekreis genutzt werden, der die Wärme aus dem Erdreich aufnimmt. Das verdampfte Kältemittel wird dann von dem Verdichter angesaugt. Aus dem oben beschriebenen Beispiel ist ersichtlich, dass durch Einsatz der elektrisch betriebenen Wärmepumpe bei dem vorausgesetzten Temperaturniveau kein wesentlich höherer thermischer Wirkungsgrad gegenüber der konventionellen Direktbeheizung möglich ist. Das Verhältnis verbessert sich zugunsten der elektrisch angetriebenen Wärmepumpe, wenn Abwärme auf hohem Temperaturniveau als untere Wärmequelle genutzt werden kann oder die Geothermie auf hohem Temperaturniveau unter Verwendung eines geeigneten Erdwärmekollektors genutzt werden kann.

Ein deutlich höherer thermischer Wirkungsgrad kann erreicht werden, wenn die Primärenergie als Gas oder Öl in einem Motor zur Erzeugung technischer Arbeit zum direkten Antrieb des Wärmepumpenverdichters genutzt werden kann. Bei einem exergetischen Wirkungsgrad des Motors von 35 % und einer Nutzung der Motorabwärme zu 90 % kann ein gesamtthermischer Wirkungsgrad von 1,8 erzielt werden. Allerdings muss der erhebliche Mehraufwand gegenüber der direkten Beheizung berücksichtigt werden der durch wesentlich höhere Investitionen und Wartungsaufwand begründet ist. Es gibt jedoch bereits Gaswärmepumpen am Markt (ab 20 kW Heiz-/Kühlleistung aufwärts), welche mit Service-Intervallen von 10.000 Stunden (übliche Wartungsarbeiten für Motor) und alle 30.000 Betriebsstunden für den Ölwechsel auskommen und so längere Wartungsvorschriften haben als Kesselanlagen. Zusätzlich ist zu bemerken, dass bestimmte Hersteller von motorgetriebenen Gaswärmepumpen diese in Serienproduktion herstellen, welche in Europa auf Lebensdauern von mehr als 80.000 Betriebsstunden kommen. Dies ist der Fall aufgrund des ausgeklügelten Motorenmanagements, der niedrigen Drehzahlen und der optimierten Geräteprozesse.






</doc>

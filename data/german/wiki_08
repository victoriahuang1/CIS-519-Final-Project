<doc id="889" url="https://de.wikipedia.org/wiki?curid=889" title="Charles Lindbergh">
Charles Lindbergh

Charles Augustus Lindbergh, Jr. (* 4. Februar 1902 in Detroit, Michigan; † 26. August 1974 in Kipahulu, Maui, Hawaii) war ein US-amerikanischer Pilot, Schriftsteller und Träger der Medal of Honor. Ihm gelang vom 20. bis 21. Mai 1927 der Nonstopflug von New York nach Paris, für den 1919 der Orteig-Preis von Raymond Orteig gestiftet worden war, und quasi nebenbei die erste Alleinüberquerung des Atlantiks, wodurch er zu einer der bekanntesten Personen der Luftfahrt wurde. Lindbergh schrieb mehrere Bücher über seinen Flug, darunter "The Spirit of St. Louis" (1953). Für dieses Werk erhielt er 1954 den Pulitzer-Preis.

Die erste Nonstop-Atlantiküberquerung von Amerika nach Europa mit einem Flugzeug war bereits im Juni 1919 John Alcock und Arthur Whitten Brown gelungen, und die erste Atlantiküberquerung mit Zwischenstopps wurde wiederum einige Wochen davor im Mai 1919 von Albert C. Read beendet.

Charles Augustus Lindbergh, jr., wurde in Detroit als Enkel eines schwedischen Einwanderers geboren, der seine Heimat aus politischen Gründen verlassen und bei seiner Emigration den Namen Lindbergh angenommen hatte. Sein Vater Charles August Lindbergh (1859–1924) war Rechtsanwalt und Kongressabgeordneter für Minnesota, seine Mutter Evangeline Lodge Land Chemielehrerin. Sein Großvater mütterlicherseits war der Zahnarzt Charles Henry Land (1847–1922), der die Jacketkrone (Mantelkrone) erfand und als „Vater der Porzellanzahnheilkunde“ gilt. Schon als Kind interessierte sich Lindbergh für Motoren und Maschinen.

1922 brach er nach knapp zwei Jahren wegen schlechter Leistungen ein Maschinenbaustudium ab und absolvierte eine Pilotenausbildung bei der "Nebraska Aircraft Corporation", die eine Mechanikerausbildung mit einschloss. Da aber kein festes Kursprogramm festgelegt worden war, kam er nur auf wenige Flugstunden. Den abschließenden Alleinflug durfte er nicht absolvieren, da er die 500 Dollar Sicherheitskaution für mögliche Beschädigungen des Flugzeugs nicht aufbringen konnte.

Einige Monate lang tat sich Lindbergh für Flugvorführungen mit einem anderen Piloten zusammen, wobei er jedoch nicht selbst flog, sondern nur Fallschirmsprünge unternahm. Danach kaufte er sich ein eigenes Flugzeug, eine Curtiss JN-4 „Jenny“, mit der er die noch fehlende Erfahrung erwarb und bis 1924 als Kunstflieger durchs Land zog.

In diesem Jahr trat er den US-amerikanischen Heeresfliegern (United States Army Air Service) bei, wo er eine gute Flugausbildung bekam. Nach anfänglichen Schwierigkeiten machte er im März 1925 seinen Abschluss als Jahrgangsbester. Da zu jener Zeit für Militärpiloten wenig Bedarf bestand, wurde Lindbergh Postflieger auf der Strecke St. Louis – Chicago.

Am 9. Juni 1926 wurde er in die Freimaurerloge "Keystone Lodge No. 243" in St. Louis aufgenommen, wo er am 20. Oktober zum Gesellen befördert und am 15. Dezember zum Meister erhoben wurde.

Ab 1926 beschäftigte er sich mit der Idee des Nonstopflugs von New York nach Paris. Im Mai 1919 hatte Raymond Orteig – ein in Frankreich geborener US-Amerikaner, der es vom Busschaffner zum wohlhabenden Hotelbesitzer gebracht hatte – einen Preis über 25.000 US-Dollar für den ersten Nonstopflug zwischen den beiden Städten, egal in welcher Richtung, ausgesetzt. Einige Piloten waren bereits an dieser Aufgabe gescheitert. Lindbergh kontaktierte den ziemlich unbekannten Flugzeughersteller Ryan Airlines in San Diego und fragte an, ob Ryan eine einmotorige Maschine für diese Strecke bauen könne. Ryan nahm die Herausforderung an, und bereits am 28. April 1927 war das Flugzeug nach nur zwei Monaten Entwicklungs- und Bauzeit fertig. Die Maschine wurde "Spirit of St. Louis" getauft.
Schon die Überführung des Flugzeugs von Küste zu Küste geschah in Rekordzeit. Am 20. Mai 1927 um 7:54 Uhr schließlich startete Lindbergh vom Roosevelt Field in New York zu seinem Alleinflug, dessen Strecke 5.808,5 km (3.610 Meilen) betrug. Als engagierter Freimaurer trug er während des Flugs das Freimaurersymbol auf seiner Jacke als Glücksbringer, auch das Flugzeug trug das Freimaurersymbol seiner Loge. Aus Gewichtsgründen hatte Lindbergh zugunsten maximaler Treibstoffzuladung auf Funkgerät und Sextant verzichtet und musste sich deshalb mit Armbanduhr, Karten und Kompass begnügen. Größte Probleme bereiteten ihm ein Schneesturm bei Neufundland, das er nach New York und Nova Scotia überflog, sowie die Überwindung der Müdigkeit auf seinem Weg über Südirland und Südengland auf den europäischen Kontinent. Die Navigation gelang ihm allerdings besonders gut, denn als er die Küste von Irland erreichte, war er nur 5 km vom Kurs abgewichen. Es war dann für ihn relativ leicht, an der Küste von Irland und England entlang zu fliegen und über den Ärmelkanal Frankreich zu erreichen. Paris schließlich fand Lindbergh durch die weithin sichtbare Beleuchtung des Eiffelturms mit dem Reklame-Schriftzug CITROEN.
In seiner Autobiografie schreibt Lindbergh, dass er mit dem Gedanken spielte, nach Rom weiterzufliegen, weil noch reichlich Treibstoff vorhanden war, er dann dort bei Tageslicht hätte landen können, und weil er sich nicht darüber klar war, wie sehr die Franzosen ihn erwarteten. Nach 33,5 Stunden landete er dann doch auf dem Flughafen Le Bourget in Paris unter dem Jubel einer begeisterten Menschenmenge und gewann damit das Preisgeld. Als „Flying Fool“ (Fliegender Narr) von der Presse tituliert, fand zu seinen Ehren sogar eine Konfettiparade in New York statt – Lindbergh war ein Nationalheld geworden.

Lindbergh war jedoch nicht, wie oft behauptet, der Erste überhaupt, der den Atlantik überflog. Tatsächlich war er schon der 67. Mensch, der dies vollbrachte, denn die erste Nonstop-Atlantiküberquerung per Flugzeug gelang bereits 1919 John Alcock und Arthur Whitten Brown. Wenige Tage später fuhr das englische Luftschiff R34 nonstop von England nach Mineola/New York und nach einer Landung nonstop zurück. Lindbergh gelang jedoch der erste Nonstopflug von New York nach Paris und die "erste Alleinüberquerung" des Atlantiks. Der erste Alleinflug über den Atlantischen Ozean von Ost nach West gelang am 18. August 1932 dem Schotten Jim Mollison.

1929 heiratete Lindbergh Anne Spencer Morrow, die Tochter des Geschäftsmanns und Politikers Dwight Morrow, der er ebenfalls das Fliegen beibrachte. Anne begleitete später ihren Mann auf seinen Flügen als Kopilotin und Funkerin. Aus der Ehe gingen sechs Kinder hervor, die zwischen 1930 und 1945 geboren wurden (zu den Kindern siehe Anne Morrow Lindbergh).

Am 22. Juni 1930 wurde der Sohn Charles III. geboren. Knapp zwei Jahre später, am 1. März 1932, wurde das Kind von Unbekannten entführt, die 50.000 Dollar Lösegeld verlangten. Am 12. Mai wurde das Kind tot aufgefunden. Aufgrund von Lindberghs Berühmtheit erregte der Fall großes Aufsehen. Für die Tat wurde Bruno Richard Hauptmann verurteilt und 1936 hingerichtet. Hauptmann bestritt stets die Tat, und bis heute gibt es Zweifel an seiner Schuld. Kritisiert wird dabei auch Lindberghs Aussage, die Stimme Hauptmanns zweifelsfrei als die des Lösegeldempfängers erkannt zu haben, obwohl er bei der Lösegeldübergabe 70 Meter entfernt in einem Auto saß. Die Tötung des Entführungsopfers trotz Lösegeldzahlung hat Agatha Christie zu ihrem im Januar 1934 erschienenen Roman "Mord im Orient-Express" inspiriert.

Im August 1932, wenige Monate nach dem Tod von Charles III., war der zweite Sohn Jon geboren worden. Charles Lindbergh fühlte sich durch die ständige Anteilnahme der Öffentlichkeit nach dem Tod seines ersten Sohnes zunehmend erschöpft und war um die Sicherheit des zweiten Kindes besorgt. Auf der Suche nach Ruhe und Sicherheit reiste das Ehepaar Lindbergh mit dem dreijährigen Kind Jon im Dezember 1935 heimlich von den USA nach England aus: Als einzige Passagiere an Bord eines Frachtschiffs, unter falschem Namen und ausgestattet mit Diplomatenpässen, fuhren sie von Manhattan nach Liverpool, wo sie am 31. Dezember 1935 ankamen. Die Familie besuchte zunächst Verwandte in Südwales und lebte dann in dem kleinen Dorf Sevenoaks Weald in der Grafschaft Kent. 1937 wurde der Sohn Land geboren. Im März 1938 kaufte Lindbergh für 16.000 US-Dollar eine kleine Insel vor der bretonischen Küste: die Île Illiec bei Penvénan. Auf der nur 1,6 Hektar großen Insel wohnte die Familie von Juni bis Anfang Dezember 1938.

Auf Ersuchen des US-Militärs und in seiner Funktion als Oberst des U.S. Army Air Corps reiste Lindbergh mehrmals nach Deutschland, um über die deutsche Luftrüstung zu berichten. Dabei traf er sich auch mit hochrangigen NS-Größen wie Hermann Göring, von dem er im Oktober 1938 das Großkreuz des Deutschen Adlerordens verliehen bekam. Im April 1939 kehrte das Ehepaar Lindbergh wieder in die Vereinigten Staaten zurück.

Nach der Gründung des America First Committee (AFC), einer isolationistischen Bewegung, die die Teilnahme der USA am Zweiten Weltkrieg zu verhindern suchte, wurde Lindbergh bald der bekannteste Sprecher dieser Organisation. In den Jahren 1940 und 1941 hielt er viel beachtete Radioansprachen und Reden vor Versammlungen mit tausenden von Zuhörern, in denen er – wie zum Beispiel in einer am 4. August 1940 ausgestrahlten Radioansprache – dafür eintrat, dass die USA sich aus dem europäischen Krieg heraushalten und sich mit den neuen Machtverhältnissen in Europa abfinden müssten:

Ferner erklärte er:
Lindbergh war, wie sein Biograf Scott Berg schreibt, überzeugt davon, dass die mächtigen USA, von blindem Idealismus geleitet, nicht erkennen könnten, dass die Vernichtung Hitlers Europa der Barbarei Stalins ausliefere und dadurch möglicherweise der westlichen Zivilisation eine tödliche Wunde geschlagen würde.
Nachdem Präsident Roosevelt am 25. April 1941 auf einer Pressekonferenz im Weißen Haus angekündigt hatte, dass Lindbergh wegen seiner politischen Ansichten nicht wieder zum Aktivdienst in den Streitkräften einberufen werde, legte dieser am 28. April „mit tiefstem Bedauern“ seinen Rang als Oberst der Luftwaffe nieder.

Am 11. September 1941 hielt Lindbergh auf einer AFC-Versammlung in Des Moines, Iowa, seine berüchtigte Rede "Who are the War Agitators?", in der er erklärte, die drei wichtigsten Gruppen, die die USA in den Krieg treiben wollten, seien „die Briten, die Juden und die Regierung Roosevelt“. Er sagte zwar, dass die Verfolgung der „jüdischen Rasse“ im Deutschen Reich von niemandem, „dem etwas an der Würde des Menschen liege“, gutgeheißen werden könne, richtete aber gleichzeitig eine deutliche Warnung an die Juden:

Schließlich wies er seine Zuhörer auch auf die vermeintlich von den Juden ausgehende „Gefahr“ für die Vereinigten Staaten hin: 

Die Des-Moines-Rede trug Lindbergh heftige Kritik von der Presse, jüdischen Organisationen, Politikern aller Parteien und sogar aus den Reihen des AFC ein. Lindbergh wurde als Sympathisant der Nationalsozialisten und als Antisemit kritisiert. Die Zeitung "Des Moines Register" schrieb beispielsweise, dass diese Rede „ihn [Lindbergh] für jeden Führungsanspruch in politischen Angelegenheiten in dieser Republik untauglich“ mache. Lindberghs öffentliches Ansehen schwand nach dieser Rede enorm, die ferner dazu führte, dass das FBI Nachforschungen über ihn und sein Privatleben anzustellen begann. Zeitungen und Rundfunk in Deutschland wurden hingegen von Goebbels angewiesen, Lindberghs Reden nicht zu positiv zu kommentieren, da er der Ansicht war, Lob aus Nazi-Deutschland könne für Lindbergh in den USA kontraproduktiv sein und die erhoffte Wirkung von Lindberghs öffentlichen Auftritten beeinträchtigen. Das endgültige Aus für seine Tätigkeit als AFC-Redner kam schließlich mit dem japanischen Angriff auf Pearl Harbor und dem Eintritt der USA in den Zweiten Weltkrieg, der die Selbstauflösung des AFC zur Folge hatte.

Nach Kriegsbeginn beantragte Lindbergh seine Wiederaufnahme in die US-Luftwaffe, stieß dabei aber auf Widerstand seitens wichtiger Mitglieder des Kabinetts Roosevelts und bei der Presse. Auch mehrere Versuche, eine Position in der US-Luftfahrtindustrie zu finden, scheiterten zunächst. Schließlich gelang es ihm, mit Billigung der US-Regierung eine Stelle als Berater des Ford-Bomberentwicklungsprogramms in Detroit zu bekommen. In den Folgejahren bildete er Piloten für das Kampfflugzeug "Corsair" aus und war selbst auch als Testpilot tätig. 1944 erhielt er die Genehmigung zum pazifischen Kriegsschauplatz zu reisen, um die "Corsair" im Einsatz zu beobachten. Dort beteiligte er sich auch an von Neuguinea ausgehenden Einsätzen gegen japanische Ziele, zunächst noch als Bordbeobachter, schließlich aber auch als Pilot mit einem eigenen Kampfflugzeug. Nach rund 50 Kampfeinsätzen und dem Abschuss eines japanischen Kampfflugzeugs im Juli 1944 kehrte er wieder in die USA zurück, wo er fortan für ein US-Luftfahrtunternehmen arbeitete.

1953 veröffentlichte Lindbergh den autobiografischen Bericht "The Spirit of St. Louis" über seinen Pionierflug und dessen Vorbereitung ab September 1926. Er schrieb darin auch, dass er über die NS-Konzentrationslager entsetzt gewesen sei. Für das Buch erhielt er 1954 den Pulitzer-Preis in der Kategorie Biographie und Autobiographie. Im gleichen Jahr wurde er auf Empfehlung von US-Präsident Dwight D. Eisenhower als Brigadegeneral in die Reserve der United States Air Force aufgenommen.

Von 1957 bis zu seinem Tode im Jahr 1974 hatte Lindbergh ein Verhältnis mit einer 24 Jahre jüngeren Frau, der Hutmacherin Brigitte Hesshaimer († 2001) aus München. Sie hatten drei gemeinsame Kinder: zwei Söhne und eine Tochter. Die Beziehung blieb aber bis zum Schluss geheim.
Die Kinder kannten die wahre Identität ihres Vaters nicht, der nur selten zu Besuch kam; für sie hieß er „Careu Kent“. Die Tochter Astrid Bouteuil fand später einen Zeitschriftenartikel über Lindbergh und entdeckte Fotografien und etwa 150 Briefe von ihm an ihre Mutter. Zwei Jahre nach deren Tod trat sie mit ihrem Wissen an die Öffentlichkeit (2003). Ein postumer Vaterschaftstest (DNA-Analyse an der Universität München) im November 2003 bestätigte die Richtigkeit der Vermutungen.

Darüber hinaus pflegte Lindbergh eine Beziehung zu Brigitte Hesshaimers Schwester Marietta (daraus gingen zwei Söhne hervor) und zu seiner Privatsekretärin Valeska, deren Nachname nicht bekannt ist (daraus gingen ein Sohn und eine Tochter hervor, ihre Namen sind nicht bekannt).

Die sieben Kinder aus diesen Beziehungen wurden in den Jahren 1958 bis 1967 geboren:

Am 26. August 1974 um 7:15 Uhr starb Lindbergh im Alter von 72 Jahren in seinem Haus auf der Hawaii-Insel Maui an Lymphdrüsenkrebs.

Sein Grab befindet sich in der Palapala Ho’omau Church in Kipahulu, Maui. Auf seinem Grabstein steht ein Vers aus Psalm 139 :
Gedanklich ist hier die Auflösung im nächsten Vers des Psalms zu ergänzen: „… auch dort würde deine Hand mich leiten und deine Rechte mich fassen.“

1927 wurde Lindbergh durch Beschluss des US-Kongresses mit der Medal of Honor, der höchsten militärischen Tapferkeitsauszeichnung der USA, ausgezeichnet.
1927 war er Mann des Jahres des Magazins "Time".

1938 erhielt er das von Adolf Hitler gestiftete Großkreuz des Deutschen Adlerordens.

1954 erhielt er für seine Autobiographie "The Spirit of St. Louis" den Pulitzer-Preis in der Kategorie Biographie und Autobiographie.

1976 wurde der Mondkrater Lindbergh nach ihm benannt.









</doc>
<doc id="890" url="https://de.wikipedia.org/wiki?curid=890" title="Chinesische Sprachen">
Chinesische Sprachen

Die chinesischen oder sinitischen Sprachen () bilden einen der beiden Primärzweige der sinotibetischen Sprachfamilie, der andere Primärzweig sind die tibetobirmanischen Sprachen. Chinesische Sprachen werden heute von ca. 1,3 Milliarden Menschen gesprochen, von denen die meisten in der Volksrepublik China und der Republik China (Taiwan) leben. In vielen Ländern, vor allem in Südostasien, gibt es größere chinesischsprachige Minderheiten. Die chinesische Sprache mit der größten Anzahl an Sprechern ist Mandarin. Auf diesem basiert das Hochchinesische, das auch einfach als „das Chinesische“ bzw. „Chinesisch“ bezeichnet wird.

In der Regel bezeichnet der Begriff „chinesische Sprache“ die Standardsprache Hochchinesisch ( in China, in Taiwan), die auf der größten sinitischen Sprache, dem Mandarin (), basiert und im Wesentlichen dem Mandarin-Dialekt von Peking () entspricht. Daneben gibt es neun weitere chinesische Sprachen, die ihrerseits in viele Einzeldialekte zerfallen. Diese Sprachen werden gemäß der traditionellen chinesischen Terminologie als Dialekte () bezeichnet, obwohl der Grad ihrer Abweichungen untereinander nach westlichem Maßstab eine Klassifikation als Sprache rechtfertigen würde. 

Selbst innerhalb einer großen sinitischen Sprache ist die Verständigung von Sprechern unterschiedlicher Dialekte nicht immer möglich, insbesondere der nordöstliche Dialekt (, ) und die südlichen Dialekte (, ) von Mandarin sind untereinander nicht verständlich. Für die Verständigung über Sprachgrenzen hinaus wird in China überwiegend das von den meisten Chinesen gesprochene Hochchinesisch angewendet; regional begrenzter dienen auch andere Sprachen wie das Kantonesische als Verständigungsmittel.

Die chinesische Schrift fungiert auch in eingeschränktem Maß als dialektübergreifendes Verständigungsmittel, da etymologisch verwandte Morpheme trotz unterschiedlicher Aussprache im Allgemeinen in allen Dialekten mit dem gleichen chinesischen Schriftzeichen geschrieben werden. Das folgende Beispiel möge dies illustrieren. Im Altchinesischen war das gewöhnliche Wort für „essen“ *"Ljɨk", das mit dem Zeichen geschrieben wurde. Die heutige Aussprache für das Wort „essen“ "shí" (Hochchinesisch), "sɪk˨˨" (Yue, kantonesischer Dialekt), "st˥" (Hakka, Meixian-Dialekt), "sit˦" (Südliches Min, Xiamen-Dialekt) stammen alle davon ab und werden daher ebenfalls geschrieben. 

Somit dient die logographische chinesische Schrift – jedes Zeichen steht im Prinzip für ein Wort – als einigendes Band, das die Sprecher der sehr unterschiedlichen chinesischen Sprachvarianten zu einer großen kulturellen Gemeinschaft mit einer Jahrtausende alten schriftlichen Tradition verbindet. Bei einer Alphabetschrift oder einer anderen Lautschrift wäre diese einigende Funktion nicht vorhanden. 

Das bedeutet jedoch nicht, dass sich die chinesischen Dialekte nur phonologisch unterscheiden. So wird für „essen“ im Hochchinesischen gewöhnlich nicht "shí", sondern "chī" benutzt, das nicht von *"Ljɨk" stammt und daher mit einem eigenen Zeichen, , geschrieben wird. Die Dialekte des Chinesischen verfügen, sofern sie geschrieben werden, für viele Wörter über eigene Zeichen, wie das Kantonesische , "mou˩˧", „nicht haben“. Daher, aber auch aufgrund grammatikalischer Abweichungen, sind auch geschriebene Texte dialektübergreifend nur eingeschränkt verständlich. 

Bis zum Beginn des 20. Jahrhunderts übte aber die Verwendung des klassischen Chinesisch, dessen schriftliche Form dialektunabhängig war und in ganz China und auch Japan, Korea und Vietnam verwendet wurde, auf der Ebene der Schriftsprache eine einigende Funktion aus.

Das ursprüngliche Verbreitungsgebiet der chinesischen Sprache ist schwer zu rekonstruieren, da die Sprachen der Nachbarn des antiken Chinas fast unbekannt sind und somit sich nicht entscheiden lässt, ob chinesische Sprachen außerhalb derjenigen chinesischen Staaten verbreitet waren, die Schriftzeugnisse hinterlassen haben; vor allem weite Teile Südchinas scheinen noch im 1. Jh. n. Chr. außerhalb des chinesischen Sprachgebiets gelegen zu haben. Bereits in der Zeit der Zhou-Dynastie (11. bis 3. Jh. v. Chr.) finden sich Hinweise auf eine dialektale Gliederung des Chinesischen, die sich in den folgenden Jahrhunderten wesentlich verstärkte. Heute werden meist acht chinesische Sprachen oder Dialektbündel unterschieden, die jeweils aus einer Vielzahl lokaler Einzeldialekte bestehen.

Die folgende Tabelle gibt die acht chinesischen Sprachen oder Dialektbündel mit ihren Sprecherzahlen und Hauptverbreitungsgebieten an. Die Sprecherzahlen stammen aus Ethnologue und anderen aktuellen Quellen. Eine detaillierte Auflistung lokaler Dialekte bietet der Artikel Liste der chinesischen Sprachen und Dialekte.

Die nordchinesischen Dialekte (, ), fachsprachlich auch Mandarin () genannt, sind die bei Weitem größte Dialektgruppe; sie umfasst das gesamte chinesische Sprachgebiet nördlich des Yangzi und in den Provinzen Guizhou, Yunnan, Hunan und Guangxi auch Gebiete südlich des Yangzi. Der Dialekt Pekings, die Grundlage des Hochchinesischen, gehört zu den Mandarin-Dialekten. Das Wu wird von etwa 80 Millionen Sprechern südlich der Mündung des Yangzi gesprochen, der Dialekt von Shanghai nimmt hier eine wichtige Stellung ein. Südwestlich daran grenzt das Gan vor allem in der Provinz Jiangxi mit 21 Millionen Sprechern und westlich davon, in Hunan, das Xiang mit 36 Millionen Sprechern. An der Küste, in der Provinz Fujian, im Osten Guangdongs sowie auf Taiwan und Hainan sowie in Singapur werden die Min-Dialekte gesprochen, zu denen insgesamt etwa 60 Millionen Sprecher gehören. In Guangxi, Guangdong und Hongkong wird von etwa 70 Millionen Menschen das Yue gesprochen, dessen wichtigster Dialekt das Kantonesische mit den Zentren Guangzhou und Hongkong ist.

Die übliche Klassifikation ist in erster Linie phonologisch motiviert, als wichtigstes Kriterium gilt die Entwicklung ursprünglich stimmhafter Konsonanten. Doch es finden sich auch deutliche lexikalische Unterschiede. So gelten das Pronomen der dritten Person "tā" (so die entsprechende hochchinesische Form), die Attributpartikel "de" und die Negation "bù" als typische Merkmale nördlicher Dialekte, besonders des Mandarin, teilweise aber auch von den Xiang-, Gan und Wu-Dialekten südlich des unteren Yangzi, die vom Mandarin beeinflusst sind. Typische Merkmale vor allem südlicher Dialekte sind dagegen die ausschließliche Verwendung von Negationen mit nasalem Anlaut (etwa kantonesisch m), Kognata des altchinesischen "qú" (kantonesisch "kʰɵy˩˧") oder "yī" (Shanghaiisch "ɦi˩˧") als Pronomen der dritten Person sowie einige Wörter, die sich weder in nördlichen Dialekten noch im Alt- oder Mittelchinesischen finden, wie „Schabe“, Xiamen "ka˥˥-tsuaʔ˥˥", Kantonesisch "kaːt˧˧-tsaːt˧˧", Hakka "tshat˦˦" und „vergiften“, Fuzhou "thau1˧", Yue "tou˧˧", Kejia "theu˦˨".

Die folgende Gegenüberstellung etymologisch zusammengehöriger Wörter aus Vertretern der große Dialektgruppen verdeutlicht genetische Zusammengehörigkeit, aber auch den Grad der Diversität der chinesischen Sprachen:

Im Chinesischen selbst ist eine Reihe unterschiedlicher Begriffe für die chinesische Sprache gebräuchlich. Zhōngwén () ist ein allgemeiner Begriff für die chinesische Sprache, der in erster Linie für die geschriebene Sprache verwendet wird. Da die geschriebene Sprache mehr oder weniger unabhängig vom Dialekt ist, umfasst dieser Begriff auch die meisten chinesischen Dialekte. Hànyǔ () wird dagegen vorrangig für die gesprochene Sprache verwendet, etwa im Satz „Ich spreche Chinesisch“. Da das Wort hàn für die Han-Nationalität steht, umfasst der Begriff ursprünglich alle Dialekte, die von Han-Chinesen gesprochen werden. Umgangssprachlich bezeichnet Hànyǔ allerdings das Hochchinesische, für das es einen eigenen Fachbegriff, das Pǔtōnghuà (), gibt. Huáyǔ () hingegen wird meist als Begriff von Auslandschinesen in der Diaspora außerhalb Chinas benutzt. Das Zeichen huá leitet sich vom historischen Begriff Huáxià () für das antike China ab. Während die Bezeichnung Tángwén () bzw. Tánghuà () für die chinesische Sprache sich vom Wort táng , das alte China der Tang-Dynastie, ableitet.

Dieser Abschnitt gibt einen kurzen Überblick über genetische Verwandtschaft des Chinesischen mit anderen Sprachen. Ausführlich wird dieses Thema im Artikel Sinotibetische Sprachen behandelt.

Das Chinesische wird heute allgemein als Primärzweig der sinotibetischen Sprachfamilie angesehen, die etwa 350 Sprachen mit 1,3 Mrd. Sprechern in China, dem Himalaya-Gebiet und Südostasien umfasst. Die meisten Klassifikationen des Sinotibetischen stellen das Chinesische dem Rest der tibetobirmanischen Sprachfamilie gegenüber, einige wenige Forscher betrachten das Sinitische als eine Untereinheit des Tibetobirmanischen, gleichrangig mit den vielen anderen Untergruppen dieser Einheit.

Das Chinesische hat zahllose Lexeme seines Grundwortschatzes mit anderen sinotibetischen Sprachen gemeinsam:

Außer dem gemeinsamen Basiswortschatz verbindet das Sinitische und Tibetobirmanische die ursprünglich gleiche Silbenstruktur (wie sie etwa im klassischen Tibetischen weitgehend erhalten ist und für das Altchinesische rekonstruiert werden kann) und eine weitverbreitete Derivationsmorphologie, die in gemeinsamen konsonantischen Präfixen und Suffixen mit bedeutungsändernder Funktion zum Ausdruck kommt. Eine relationale Morphologie (Veränderung der Nomina und Verben im Sinne einer Flexion) haben das Proto-Sinotibetische wie auch die modernen sinitischen Sprachen nicht ausgebildet, diese Form der Morphologie ist eine Innovation vieler tibetobirmanischer Sprachgruppen, die durch gebietsübergreifende Kontakte mit Nachbarsprachen und durch Überlagerung älterer Substratsprachen entstanden ist.

Genetische Verwandtschaft des Chinesischen mit Sprachen außerhalb des Tibetobirmanischen wird von der Linguistik nicht allgemein anerkannt, es existieren jedoch einige Versuche, das Chinesische in weit über die traditionellen Sprachfamilien hinausgehende Makrofamilien einzuordnen. Einige Forscher vertreten beispielsweise eine genetische Verwandtschaft mit den austronesischen Sprachen, den jenisseischen Sprachen oder sogar den kaukasischen oder den indogermanischen Sprachen, wofür Wortgleichungen wie chinesisch < *kwjəl „wer“ = lateinisch quis „wer“ herangezogen werden. Keiner dieser Versuche hat jedoch bisher die Zustimmung einer Mehrheit der Sprachwissenschaftler gewinnen können.

Aufgrund der jahrtausendelangen Koexistenz mit anderen, genetisch nicht verwandten Sprachen haben sich das Chinesische und verschiedene südost- und ostasiatische Sprachen gegenseitig stark beeinflusst. So finden sich in ihnen Hunderte von chinesischen Lehnwörtern, oft Bezeichnungen chinesischer Kulturgüter: > Koreanisch "čhäk", Bai "tsua˧˧". Diese Einflüsse haben sich in besonders hohem Maße auf Korea, Vietnam und Japan ausgewirkt, wo zudem auch die chinesische Schrift Anwendung findet und das klassische Chinesisch über Jahrhunderte als Schriftsprache benutzt wurde.

Auch das Chinesische selbst weist eine große Anzahl fremder Einflüsse auf. So sind einige wesentliche typologische Züge des modernen Chinesisch vermutlich auf Fremdeinfluss zurückzuführen, darunter die Ausbildung eines Tonsystems, die Aufgabe ererbter morphologischer Bildungsmittel und die obligatorische Anwendung von Zählwörtern. Fremdeinfluss zeigt sich auch in der Aufnahme nicht weniger Lehnwörter. Schon in sehr früher Zeit muss das Wort (altchinesisch *xlaʔ) aus den austroasiatischen Sprachen entlehnt worden sein, vgl. Mon klaʔ, Mundari "kula". Das Wort , das während der Han-Dynastie (206 v. Chr. bis 220 n. Chr.) das ältere verdrängte, wurde wohl während der Zeit der Zhou-Dynastie (um 1100–249 v. Chr.) aus dem Miao-Yao entlehnt. Auch aus nördlichen Nachbarsprachen wurden in vorgeschichtlicher Zeit Wörter übernommen, so beispielsweise , das sich in altaischen Sprachen wiederfindet: Mongolisch "tuɣul", Mandschurisch "tukšan". Besonders groß wurde die Zahl von Lehnwörtern im Chinesischen während der Han-Dynastie, als auch aus westlichen und nordwestlichen Nachbarsprachen Wörter übernommen wurden, beispielsweise aus einer iranischen Sprache, vgl. Persisch باده "bāda". Schwer nachweisbar sind Entlehnungen aus der Sprache der Xiongnu; hier ist mutmaßlich einzuordnen. Durch den starken Einfluss des Buddhismus während des 1. nachchristlichen Jahrtausends drang eine Vielzahl indischer Lehnwörter ins Chinesische ein: aus dem Sanskrit "candana", aus dem Sanskrit "śramaṇa". Nur wenige Lehnwörter hinterließ die mongolische Herrschaft der Yuan-Dynastie (1279–1368), beispielsweise aus dem Mongolischen "moku".

Im 16. Jahrhundert setzte ein starker europäischer Einfluss ein, der sich auch im chinesischen Wortschatz niederschlug. So wurden in dieser Zeit christliche Termini ins Chinesische entlehnt: aus dem spätlateinischen '. Seit dem 19. Jahrhundert wurden auch Bezeichnungen für Errungenschaften der europäischen Technik übernommen, wobei sich das Chinesische jedoch gegenüber Entlehnungen als wesentlich resistenter erwies als etwa das Japanische. Beispiele hierfür sind: aus dem Englischen ', aus dem Englischen '. In manchen Fällen fanden Lehnwörter über Dialekte den Weg ins Hochchinesische: z.B. aus dem Shanghaiischen "safa" vom Englischen '.

Eine besondere Erscheinung bildet eine Gruppe von Lehnwörtern insbesondere aus Japan, bei denen nicht die Aussprache, sondern die Schreibung entlehnt wird. Dies wird dadurch ermöglicht, dass das entlehnte Wort in der Ursprungssprache selbst mit chinesischen Schriftzeichen geschrieben wird:

Das Chinesische wird seit den frühesten bekannten Schriftzeugnissen aus dem 2. vorchristlichen Jahrtausend mit der chinesischen Schrift geschrieben. In der chinesischen Schrift wird – von Ausnahmen abgesehen – jedes Morphem mit einem eigenen Zeichen wiedergegeben. Da die chinesischen Morpheme einsilbig sind, lässt sich so jedem Zeichen ein einsilbiger Lautwert zuordnen. Entgegen einem weit verbreiteten Missverständnis werden synonyme, aber nicht homophone Wörter mit unterschiedlichen Zeichen geschrieben. So bedeuten sowohl als auch „Hund“, werden aber mit völlig anderen Zeichen geschrieben. Einige Zeichen gehen dabei auf piktographische Darstellungen des entsprechenden Wortes zurück, auch andere rein semantisch basierte Typen kommen vor. 

Etwa 85 % der heutigen Zeichen enthalten aber phonologische Information und sind aus zwei Komponenten zusammengesetzt, von denen eine die Bedeutung angibt und die andere ein Morphem mit ähnlicher Aussprache darstellt. So besteht das Zeichen aus „Frau“ als Bedeutungs- und als Aussprachekomponente. 

In einigen Fällen stellt ein Zeichen mehrere Morpheme dar, insbesondere etymologisch verwandte. Die Zahl aller chinesischen Zeichen ist aufgrund des morphemischen Prinzips verhältnismäßig hoch; bereits das Shuowen Jiezi () von 100 n. Chr. verzeichnet knapp 10.000 Zeichen; das Yitizi Zidian () von 2004 enthält 106.230 Zeichen, von denen sehr viele aber nicht mehr in Gebrauch sind oder lediglich seltene Schreibvarianten anderer Zeichen darstellen. Die durchschnittliche Zahl der Zeichen, die ein Chinese mit Universitätsabschluss beherrscht, beträgt aber weniger als 5000; etwa 2000 gelten für das Lesen einer hochchinesischen Zeitung als erforderlich.

Die chinesische Schrift ist nicht einheitlich. Seit der Schriftreform vom Jahre 1958 werden in der Volksrepublik China (und später auch in Singapur) offiziell die vereinfachten Zeichen (Kurzzeichen, ) verwendet, in Taiwan und in Hongkong dagegen werden weiterhin die traditionellen Zeichen, (Langzeichen, oder ) benutzt. Auch auf die Verschriftlichung anderer Sprachen, die chinesische Schriftzeichen nutzen, wie des Japanischen wurde die chinesische Schriftreform nicht angewendet; in Japan wurden aber bereits 1946 unabhängig vereinfachte Zeichenformen, auch "Shinjitai" genannt, eingeführt.

Neben der chinesischen Schrift waren in China auch einige andere Schriften in Gebrauch. Dazu zählt insbesondere die Nüshu, eine seit dem 15. Jahrhundert in der Provinz Hunan verwendete Frauenschrift. Unter der Yuan-Dynastie (1279–1368) wurde auch die phonetisch basierte Phagspa-Schrift für das Chinesische verwendet.

Neben der chinesischen Schrift gibt es zahlreiche auf dem lateinischen Alphabet basierende Transkriptionssysteme für Hochchinesisch und die einzelnen Dialekte beziehungsweise Sprachen. In der Volksrepublik China wird Hanyu Pinyin (kurz: Pinyin) als offizielle Romanisierung für das Hochchinesische verwendet; ein weiteres, besonders vor der Einführung von Pinyin sehr weit verbreitetes Transkriptionssystem ist das Wade-Giles-System. Für die verschiedenen Dialekte bzw. Sprachen existieren keine allgemein anerkannten Transkriptionssysteme, im Folgenden werden sie daher mit dem internationalen phonetischen Alphabet umschrieben. Frühere Formen des Chinesischen werden üblicherweise wie das Hochchinesische, folglich in Pinyin transkribiert, obwohl dies die Phonologie früherer Formen des Chinesischen nicht adäquat wiedergeben kann.

Muslimische Chinesen (vergleiche Religion in der Volksrepublik China) haben ihre Sprache auch in der arabisch-basierten Schrift Xiao’erjing geschrieben. Einige, die nach Zentralasien auswanderten, sind im 20. Jahrhundert zur kyrillischen Schrift übergegangen, siehe Dunganische Sprache.

Ursprünglich unterschieden sich die gesprochene und die geschriebene Sprache in China nicht wesentlich voneinander; die schriftliche Sprache folgte den Entwicklungen der gesprochenen Sprache. Seit der Qin-Dynastie (221–207 v. Chr.) wurden jedoch Texte aus der Spätzeit der Zhou-Dynastie für die geschriebene Sprache maßgeblich, sodass das klassische Chinesisch als Schriftsprache von der gesprochenen Sprache unabhängig wurde und in geschriebener Form allgemeines Verständigungsmedium über Dialektgrenzen hinaus bildete. Das klassische Chinesisch diente jedoch ausschließlich als geschriebene Sprache einer kleinen Elite, als gesprochene Sprache wurde spätestens seit der Qing-Dynastie (1644–1911) selbst von den hochgestellten Beamten der Dialekt der Hauptstadt benutzt. Beim Lesen von Texten in klassischem Chinesisch wurde der jeweilige lokale Dialekt angewendet, einige Dialekte besaßen dafür eigene phonologische Subsysteme, die sich von der gesprochenen Sprache unterschieden.

Vor allem im Zusammenhang mit der Ausbreitung des Buddhismus in China wurde volkstümliche Literatur zunehmend in der Volkssprache Baihua () abgefasst, die bei der schriftlichen Anwendung innerhalb Chinas bis zu einem gewissen Grad normiert war und sich mit wenigen Ausnahmen, wie dem im südlichen Min geschriebenen Lijingji aus dem 16. Jahrhundert, an frühen Formen der Mandarin-Dialekte orientierte. Möglicherweise kam es auch in der gesprochenen Sprache des 1. nachchristlichen Jahrtausends zu einer Standardisierung.

Erst gegen Ende des chinesischen Kaiserreiches, zu Beginn des 20. Jahrhunderts, schwand die Bedeutung des klassischen Chinesisch; als Amtssprache und als literarische Sprache wurde es bis zur Mitte des 20. Jahrhunderts vom Hochchinesischen abgelöst, das sich in Grammatik, Lexikon und insbesondere der Phonologie stark an den modernen Dialekt von Peking anlehnt. Auch für andere Dialektformen des Chinesischen wurden Verschriftlichungsversuche gemacht, jedoch verfügt nur das Kantonesische über eine etablierte Literatur in chinesischer Schrift. In einigen Dialekten wurde auch eine Verschriftlichung mittels lateinischer Schrift versucht.

Auch außerhalb der geschriebenen Sprache verdrängt das Hochchinesische zunehmend lokale Idiome, da das Hochchinesische landesweit an den Schulen gelehrt wird, wenngleich es die Dialekte als Umgangssprachen wohl nur stellenweise ersetzt.

Da die chinesische Schrift über 10.000 verschiedene Logogramme umfasst, das gesprochene Hochchinesisch aber weniger als 1.700 verschiedene Sprechsilben, hat das Chinesische wesentlich mehr homophone Morpheme, also unterschiedliche Bedeutung tragende Wortbestandteile mit gleicher Aussprache, als irgendeine europäische Sprache. Daher entsprechen weder gesprochene Sprache noch lateinische Umschriften exakt den in chinesischen Zeichen geschriebenen Texten. Vereinfachte Umschriften, die die Töne nicht markieren, lassen die Homophonie noch ausgeprägter erscheinen, als sie tatsächlich ist.

Zudem gibt es im Chinesischen auch Homonyme, also unterschiedliche Begriffe, die mit demselben Wort bezeichnet werden. Trotz der sehr vielen verschiedenen Logogramme existieren auch einige Homographen, das heißt Wörter, die mit denselben Zeichen geschrieben werden. Obwohl die meisten chinesischen Homographen gleich ausgesprochen werden, gibt es auch einige mit unterschiedlicher Aussprache.

Das Chinesische ist eine der wenigen noch gesprochenen Sprachen mit einer mehr als dreitausendjährigen schriftlichen Tradition. Die Sprachentwicklung lässt sich unter syntaktischen und phonologischen Gesichtspunkten in mehrere Phasen unterteilen.

Die älteste durch schriftliche Überlieferung fassbare Form des Chinesischen ist die Sprache der Orakelknocheninschriften aus der Spätzeit der Shang-Dynastie (16.–11. Jahrhundert v. Chr.). Sie bilden den Vorläufer der Sprache der Zhou-Dynastie (11.–3. Jahrhundert v. Chr.), die als Altchinesisch () bezeichnet wird und deren Spätform als Klassisches Chinesisch bis in die Neuzeit als Schriftsprache konserviert wurde. 

Nach der Zhou-Dynastie entfernte sich die gesprochene Sprache allmählich vom klassischen Chinesisch; erste grammatische Innovationen finden sich schon im 2. Jahrhundert v. Chr. Sie kennzeichnen das Mittelchinesische (), das vor allem die Sprache der volkstümlichen Literatur beeinflusste. 

Die Zeit seit dem 15. Jahrhundert umfasst das moderne Chinesisch () und das zeitgenössische Chinesisch (), das als Oberbegriff für die modernen chinesischen Sprachen dient.

In typologischer Hinsicht zeigt das moderne Chinesisch relativ wenige Übereinstimmungen mit den genetisch verwandten tibeto-birmanischen Sprachen, während sich wesentlich mehr Übereinstimmungen mit den über Jahrhunderte direkt benachbarten südostasiatischen Sprachen zeigen. Insbesondere ist das moderne Chinesische sehr stark isolierend und zeigt nur wenig Flexion; die syntaktischen Zusammenhänge werden demzufolge überwiegend durch die Satzstellung und freie Partikeln ausgedrückt. Jedoch darf nicht übersehen werden, dass auch das moderne Chinesisch morphologische Prozesse zur Wort- und Formenbildung kennt.

Das Phoneminventar der verschiedenen chinesischen Sprachen weist eine große Diversität auf; einige Merkmale haben sich jedoch weit verbreitet; beispielsweise das Vorhandensein aspirierter Plosive und Affrikaten sowie in einem großen Teil der Dialekte der Verlust der stimmhaften Konsonanten. Die Min-Dialekte im Süden Chinas sind aus historischer Sicht sehr untypisch, da sie sehr konservativ sind, aus typologischer Sicht jedoch geben sie einen guten Querschnitt durch das Konsonanteninventar des Chinesischen, weshalb im Folgenden das Konsonantensystem des Min-Dialektes von Fuzhou (Min Dong) dargestellt ist:
Diese Konsonanten finden sich in nahezu allen modernen chinesischen Sprachen; die meisten haben verschiedene zusätzliche Phoneme. So gibt es beispielsweise im Yue Labiovelare, in einigen Dialekten einen palatalen Nasal (ɲ) und im Mandarin und Wu palatale Frikative und Affrikaten. Das Hochchinesische hat folgende Konsonantenphoneme (in Klammern die Pinyin-Umschrift):

Traditionell wird die chinesische Silbe in einen konsonantischen Anlaut () und einen Auslaut () aufgeteilt. Der Auslaut besteht aus einem Vokal, bei dem es sich auch um einen Di- oder Triphthong handeln kann, sowie einem optionalen Endkonsonanten (). So lässt sich die Silbe "xiang" in den Anlaut "x" und den Auslaut "iang" zerlegen, dieser wiederum wird in den Diphthong "ia" und den Endkonsonanten "ng" analysiert. Der Anlaut besteht in allen modernen chinesischen Sprachen immer – abgesehen von Affrikaten – aus einem einzelnen Konsonanten (oder ∅); es wird jedoch davon ausgegangen, dass das Altchinesische auch Konsonantencluster im Anlaut besaß. Im Auslaut lassen die modernen chinesischen Sprachen nur wenige Konsonanten zu; im Hochchinesischen beispielsweise nur "n" und "ŋ"; auch hier war jedoch die Freiheit im Altchinesischen vermutlich wesentlich größer. Aufgrund dieser stark eingeschränkten Möglichkeiten zur Silbenbildung ist die Homonymie im modernen Chinesisch sehr stark ausgeprägt.

Das wohl offensichtlichste Merkmal der chinesischen Phonologie ist, dass die chinesischen Sprachen – wie viele genetisch nicht verwandte Nachbarsprachen – Tonsprachen sind. Die Anzahl der Töne, meist handelt es sich um Konturtöne, variiert in den verschiedenen Sprachen untereinander sehr stark. Um 800 n. Chr. besaß das Chinesische acht Töne, wobei jedoch nur drei Oppositionen tatsächlich phonemische Bedeutung hatten. In den verschiedenen modernen chinesischen Sprachen hat sich das antike Tonsystem stark verändert, das Hochchinesische etwa zeigt nur noch vier Töne, die aber alle phonemisch sind, wie die folgenden Beispiele zeigen (vergleiche den Artikel Töne des Hochchinesischen):

Der kantonesische Dialekt des Yue dagegen hat das antike System besser bewahrt und besitzt neun Töne, die in bestimmte Kategorien eingeteilt werden:
Es wird im Allgemeinen davon ausgegangen, dass das chinesische Tonsystem hauptsächlich unter dem Einfluss von erodierten Konsonanten am Silbenende entstanden ist; das Altchinesische war demzufolge nach der Meinung der Mehrzahl der Forscher noch keine Tonsprache.

Grundlage der chinesischen Morphologie ist das einsilbige Morphem, dem in der geschriebenen Form der Sprache ein Zeichen entspricht. Beispiele sind im Hochchinesischen die selbstständigen Lexeme , , und Affixe wie das Pluralsuffix -. Ausnahmen sind Gruppen zweier aufeinanderfolgender Morpheme, die eine einzelne Silbe bilden. In einigen Fällen ist dies auf phonologische Veränderungen beim Zusammentreffen zweier Morpheme (sogenanntes Sandhi) zurückzuführen, wie in Hochchinesisch nà-ér > nàr „dort“, klassisches Chinesisch yě-hū > , Kantonesisch 嘅呀 kɛː˧˧ aː˧˧ > 嘎 kaː˥˥. Da die Affixe der altchinesischen Wortbildungsmorphologie keine eigene Silbe bildeten, gehören auch die unten besprochenen Derivate zu diesen Ausnahmen. Ob das Altchinesische auch mehrsilbige Morpheme besaß, die nur mit einem Zeichen geschrieben wurden, lässt sich bislang nicht klären.

Im Altchinesischen entsprachen die Morphemgrenzen in der bei weitem überwiegenden Mehrzahl der Fälle den Wortgrenzen. Seit der Zeit der Han-Dynastie wurden durch Zusammensetzung einsilbiger Wörter neue, zweisilbige und bimorphemische Lexeme gebildet. Viele solcher Zusammensetzungen weisen syntaktische Strukturen auf, die sich ebenso in Phrasen und Sätzen finden, weshalb die Trennung von Syntax und Morphologie problematisch ist. So sind viele Substantive wie Nominalphrasen mit einem Attribut und folgendem Kern gebildet: wörtlich: „Deutschland – Mensch“ = „Deutscher“, wörtlich: „derjenige, der aufzeichnet“ = „Journalist“. Ebenso können Verben durch eine Kombination eines Verbs mit einem Objekt gebildet werden: aus und . Andere Zusammensetzungen sind schwieriger zu analysieren, beispielsweise aus und dem Synonym .

Ein weiteres Bildungsmittel zur Wortderivation des alten wie des modernen Chinesisch stellen Affixe dar. Das Altchinesische verfügte über eine Vielzahl an Prä-, In- und Suffixen, die jedoch vielfach nur schwer nachzuweisen sind, da sie in der Schrift keine oder nur unzureichende Spuren hinterlassen. Besonders häufig findet sich ein Suffix *-s, mit dem sowohl Substantive als auch Verben gebildet werden konnten ( (*trje) „wissen“ > / zhì (*trjes) „Weisheit“; (*wjang) „König“ > (*wjangs) „herrschen“). Auch verschiedene In- und Präfixe lassen sich rekonstruieren.

Auch das moderne Chinesisch verfügt über einige Suffixe zur Derivation (Beispiele aus dem Hochchinesischen):

In verschiedenen chinesischen Dialekten finden sich auch Präfixe, wie das im Hakka vertretene Präfix ʔa˧˧- zur Bildung von Verwandtschaftsbezeichnungen: ʔa˧˧ kɔ˧˧ „älterer Bruder“ = Hochchinesische . Derivation oder Flexion durch Tonwechsel spielt im modernen Chinesisch eine eher geringe Rolle, beispielsweise bei der Bildung des perfektiven Aspekts im Kantonesischen: "sek˧˥" „aß, hat gegessen“ zu "sek˨˨" „essen“.

Die Personalpronomina haben in verschiedenen Formen des Chinesischen die folgenden Formen:

Das frühe Altchinesisch unterschied bei den Personalpronomina die Numeri Singular und Plural sowie verschiedene syntaktische Funktionen; so diente in der 3. Person um 900 v. Chr. *kot (heute: jué) als Attribut, *tə (heute: zhī) als Objekt, und möglicherweise *gə (heute: qí) als Subjekt. Im Klassischen Chinesisch wurde die Unterscheidung der Numeri aufgegeben, seit der Han-Zeit verschwand auch die syntaktische Unterscheidung. Dafür entwickelten sich seit der Tang-Dynastie neue Plurale, die nun durch Affixe wie děng, cáo, bèi gebildet wurden. Dieses System ist in seinen Grundzügen bislang unverändert geblieben und findet sich in den modernen chinesischen Sprachen wieder.

Da die chinesischen Sprachen in großem Maße isolierend sind, werden Beziehungen der Wörter untereinander vorrangig durch die vergleichsweise feste Satzstellung zum Ausdruck gebracht. Kongruenz ist nicht vorhanden; von den Personalpronomina des Alt- und Mittelchinesischen abgesehen werden auch keine Kasus markiert. In allen historischen und modernen Formen des Chinesischen ist die Stellung Subjekt – Verb – Objekt (SVO) vorherrschend, nur dass bei Subjekten Pro-Drop auftritt:

In bestimmten Fällen wie Topikalisierung und in negierten Sätzen kann das Objekt auch präverbal stehen. Die Satzstellung SOV findet sich in verschiedenen Formen des chinesischen vor allem in negierten Sätzen. So standen im Altchinesischen pronominale Objekte oft vor negierten Verben:

Die Satzstellung SOV ist seit etwa dem 6. Jahrhundert auch in anderen Kontexten möglich, wenn das Objekt mit einer Partikel (, und andere) eingeleitet wird:

In den meisten historischen und den nördlichen modernen Varianten des Chinesischen steht das indirekte Objekt vor dem direkten; in einigen heutigen südlichen Sprachen steht hingegen das direkte voran:

Eine wichtige Rolle in der chinesischen Syntax nimmt das Phänomen der Topikalisierung ein, bei der eine pragmatisch hervorgehobene Nominalphrase aus ihrer kanonischen Position an den Satzanfang gestellt wird. Im Altchinesischen wurden bei der Extraktion von Objekten und Attributen Resumptiva verwendet; im modernen Chinesisch sind diese nicht mehr vorhanden. Typisch für das moderne Chinesisch sind auch Topics, die hinter dem Subjekt stehen sowie solche, die keinen direkten syntaktischen Bezug zum folgenden Satz haben:

Interrogativa stehen im Chinesischen in situ. Markierung von Fragen mit Interrogativa durch finale Fragepartikeln ist in einigen antiken und modernen Varianten des Chinesischen möglich:

Ja-nein-Fragen werden meist mit finalen Partikeln markiert; seit dem 1. Jahrtausend n. Chr. finden sich auch Fragen der Form „A – nicht – A“:

Aspekt, Tempus und Aktionsart können unmarkiert bleiben oder durch Partikeln oder Suffixe, manchmal auch durch Hilfsverben, zum Ausdruck gebracht werden. Im frühen Altchinesisch waren diese Morpheme ausschließlich präverbal; im späteren Altchinesisch waren die wichtigsten Aspektpartikeln dagegen das vermutlich stativisch-durativische yě und das perfektivische yǐ, die am Satzende standen:

Seit dem Ende des 1. Jahrtausends n. Chr. sind auch Aspektpartikeln belegt, die zwischen Verb und Objekt stehen; diese Stellung ist in allen modernen chinesischen Sprachen weit verbreitet. Auch am Satzende und, vor allem im Min, vor dem Verb können weiterhin bestimmte Aspektpartikeln stehen. Die folgende Tabelle illustriert die Konstruktionen, die das Hochchinesische besitzt, um Aktionsarten auszudrücken:

Wenngleich alle chinesischen Sprachen äußerlich ähnliche Systeme besitzen, weisen die benutzten Morpheme große Divergenzen auf. Das Hakka etwa benutzt die präverbalen Aspektpartikeln ∅ (Imperfektiv), ʔɛ˧˨ (Perfektiv), tɛn˧˨ (Kontinuativ), kuɔ˦˥ („Erfahrungs-Perfektiv“).

Während das Aktiv im Chinesischen unmarkiert ist, stehen zur Markierung des Passivs unterschiedliche Möglichkeiten zur Verfügung. Im Altchinesischen blieb es ursprünglich ebenfalls unmarkiert und konnte nur indirekt durch Angabe des Agens in einer Präpositionalphrase angedeutet werden. Seit dem Ende der Zeit der Zhou-Dynastie bildeten sich Konstruktionen mit verschiedenen Hilfsverben wie , , , und , die das unmarkierte Passiv aber nicht verdrängten.

Ein wichtiges und produktives Merkmal der Syntax der jüngeren chinesischen Sprachen ist die Verbserialisierung, die seit dem frühen 1. Jahrtausend n. Chr. belegt ist. In diesen Strukturen folgen zwei Verbalphrasen, die in einer bestimmten semantischen Relation stehen, ohne formale Trennung aufeinander. In vielen Fällen ist das Verhältnis der beiden Verbalphrasen resultativ, die zweite gibt also das Ergebnis der ersten an:

Ebenfalls häufig sind serialisierte Verben, bei denen das zweite Verb die Richtung der Handlung ausdrückt:

Eine ähnliche Konstruktion liegt bei den sogenannten Koverben vor. Hierbei handelt es sich um transitive Verben, die nicht nur als selbstständige Verben auftreten können, sondern auch die Funktion von Präpositionen übernehmen können und andere Verben modifizieren:
Eine besondere Rolle nehmen verschiedene Serialverbkonstruktionen mit dem Morphem oder dessen Entsprechungen in anderen Sprachen ein. In einer Konstruktion, die als "Komplement des Grades" bekannt ist, markiert "de" ein Adjektiv, das ein Verb modifiziert. Hat das Verb ein Objekt, wird das Verb hinter dem Objekt wiederholt, oder das Objekt wird topikalisiert:

In manchen Dialekten wie dem Kantonesischen kann das Objekt auch hinter "de" gestellt werden.

Außerdem können und die Negation oder deren dialektale Entsprechungen die Möglichkeit bzw. Unmöglichkeit markieren. Der Partikel folgt dabei ein Verb, das Resultat oder Richtung der Handlung angibt:

Im Chinesischen steht der Kopf einer Nominalphrase stets am Ende, Pronomina, Numeralia und Attribute stehen vor ihm und können von diesem durch eine Partikel getrennt werden. Diese Partikel hat in verschiedenen Dialekten unterschiedliche Formen; im Altchinesischen lautet sie beispielsweise , im Hochchinesischen . Bei dem Attribut kann es sich um eine eigene Nominalphrase handeln: klassisches Chinesisch „wessen – subordiniertes Partikel – Land“, modernes Chinesisch „hier – Attributpartikel – Menschen“, Moiyen (Hakka) ŋaɪ̯˩˩-"ɪ̯ɛ˥˥" su˧˧ „mein Buch“. 

Ist diese durch ein Attribut erweitert, können auch komplizierte Ketten von Attributen entstehen, die als für das Chinesische typisch gelten können. Häufig handelt es sich bei dem Attribut aber nicht um ein Substantiv, sondern um ein nominalisiertes Verb, optional auch mit Ergänzungen wie Subjekt, Objekt und adverbialen Bestimmungen. Derartige Attribute erfüllen ähnliche semantische Funktionen wie Relativsätze europäischer Sprachen. Im folgenden Beispiel aus dem Hochchinesischen ist der Kern der Nominalphrase koreferent mit dem Subjekt des nominalisierten Verbs:

Der Kopf der Nominalphrase kann aber auch mit anderen Ergänzungen des nominalisierten Verbs, wie seinem Objekt, koreferent sein. In den meisten Dialekten ist dies nicht formal markiert, teilweise finden sich aber Resumptiva:

Das Altchinesische konnte in Fällen, wo der Kopf nicht mit dem Subjekt des Verbs koreferent ist, die Morpheme (präklassisch), (klassisch) einsetzen: „was abgeschnitten wurde“.

Ein wesentliches typologisches Merkmal, welches das moderne Chinesische mit anderen südostasiatischen Sprachen teilt, ist die Anwendung von Zählwörtern. Während im Altchinesischen Zahlen und Demonstrativpronomina direkt vor Substantiven stehen können (; ), muss in den modernen chinesischen Sprachen zwischen beiden Wörtern ein Zählwort stehen, dessen Wahl vom Substantiv abhängt: Hochchinesisch , . In den Yue- und Xiang-Dialekten werden Zählwörter auch zur Determination eines Substantives sowie zur Markierung eines Attributs benutzt: Kantonesisch „"kʰɵy˨˧ "puːn˧˥" syː˥˥"“, "„tsiː˥˥ pɐt˥˥“". Die Wahl des Zählwortes wird durch die Semantik des Substantivs bedingt: steht im Hochchinesischen bei Substantiven, die ein Ding bezeichnen, das einen Griff besitzt; mit werden Substantive konstruiert, die ein Gebäude bezeichnen usw. Eine Übersicht über wichtige Zählwörter des Hochchinesischen bietet der Artikel Liste chinesischer Zählwörter.

Die ISO-Norm ISO 639 definiert Codes für die Auszeichnung von Sprachmaterialien. Die chinesischen Sprachen werden in der Norm unter dem Sprachcode zh (ISO 639-1) und zho/chi (ISO 639-2/T und /B) subsumiert. Die Norm ISO 639-3 führt den Sprachcode zho als sog. "Makrosprache" ein – ein Konstrukt, welches für eine Gruppe von Sprachen angewandt wird, wenn diese als Einheit behandelt werden kann. Im Falle der chinesischen Sprachen ist dieser Faktor durch die gemeinsame geschriebene Form gegeben. Die subsumierten Einzelsprachen sind im Einzelnen: gan (Gan), hak (Hakka), czh (Hui-Dialekt), cjy (Jin), cmn (Hochchinesisch), mnp (Min Bei), cdo (Min Dong), nan (Min Nan), czo (Min Zhong), cpx (Puxian), wuu (Wu), hsn (Xiang), yue (Kantonesisch).








</doc>
<doc id="891" url="https://de.wikipedia.org/wiki?curid=891" title="Carl Barks">
Carl Barks

Carl Barks (* 27. März 1901 in der Nähe von Merrill, Oregon; † 25. August 2000 in Grants Pass, Oregon) war ein US-amerikanischer Comicautor und -zeichner sowie Cartoonist und Maler, der als der bekannteste Disneyzeichner gilt und zahlreiche Comic-Figuren des Disney-Kosmos wie Dagobert Duck erschuf. Durch seine Comics gilt er auch als Kultur- und Politikkritiker, wenngleich er dies stets abstritt.

Barks war der bekannteste Zeichner und Autor der US-amerikanischen Disney-Comics, insbesondere der Geschichten um die Familie Duck. Die zum Teil noch recht eindimensionalen Charaktere aus den Trickfilmen und den Zeitungscomics von Al Taliaferro differenzierte er und fügte neue Figuren hinzu. Er ist der geistige Vater des reichsten Manns der Welt "Dagobert Duck" ("Scrooge McDuck"), des genialen Erfinders "Daniel Düsentrieb" ("Gyro Gearloose") und der "Panzerknacker" ("Beagle Boys"). Auch der amerikanische Name der Heimatstadt der Ducks, Entenhausen ("Duckburg"), stammt von Barks. Vor seinem Wirken war außer "Donald Duck" nur dessen Freundin "Daisy Duck" vorhanden. Donalds Neffen "Tick, Trick und Track" ("Huey, Dewey & Louie") hatten ihren ersten Auftritt in dem Disney-Kurztrickfilm "Donald's Nephews" von 1938, der in wesentlichen Teilen von Barks stammt, der vor seiner Comic-Karriere einige Jahre als Trickfilmzeichner und -schreiber in den Disney-Studios arbeitete.

Carl Barks hat dazu noch viele weitere Enten-Figuren erfunden. Einige dieser Figuren, die ursprünglich nur für einen bestimmten Comic entworfen worden waren, wurden so populär, dass sie später ihre eigene Comic-Serie erhielten, z. B. "Oma Duck".

In den 1950er Jahren waren seine Comics in den USA so beliebt, dass er von den Disney-Comiclesern „the good artist“ (dt. "Der gute Künstler") genannt wurde. Damals kannte noch niemand seinen Namen, weil alle Hefte der Disney-Verlage als Autorenvermerk nur die Marke „Walt Disney“ trugen. Erst Ende der 1960er Jahre gelang es hartnäckigen Fans, seinen Namen herauszufinden und den Meister, der längst in Rente war, zu kontaktieren und besuchen.

Mit Erika Fuchs fanden die Comics von Carl Barks eine kongeniale Übersetzerin ins Deutsche. Bekannte Sprüche wie „Wo man hinschaut, nichts als Gegend“ stammen aus ihrer Feder. Ihre Sprache war weitaus feiner differenziert als das US-amerikanische Original, in dem Barks auch viele Slang-Worte verwendete.

Carl Barks wurde am 27. März des Jahres 1901 als zweiter Sohn des Landwirtes William Barks und dessen Frau Arminta im US-Bundesstaat Oregon, unweit des Ortes Merrill auf einer Farm geboren. Bereits in jungen Jahren halfen Carl und sein zwei Jahre älterer Bruder Clyde nach der Schule ihrem Vater bei Farmarbeiten. 1911 verpachtete William Barks seine Farm und zog mit der Familie nach Santa Rosa in Kalifornien, um mit einer Pflaumenplantage sein Glück zu versuchen. Der Erfolg blieb jedoch aus, und als bei Carl Barks’ Mutter Krebs diagnostiziert wurde und sein Vater einen Nervenzusammenbruch erlitt, zog die Familie zurück in die bis dahin verpachtete Farm nach Merrill. Im Jahr 1916 verstarb seine Mutter im Alter von 56 Jahren. Carl Barks, der zu diesem Zeitpunkt gerade 15 Jahre alt war, brach daraufhin die Schule in der achten Klasse ab. Sein Gehör begann sich bereits in dieser Zeit deutlich zu verschlechtern. Im selben Jahr begann er einen Fernkurs an der "Landon School of Cartooning", den er jedoch nach nur vier Unterrichtsstunden wieder abbrach. Barks half vermehrt auf den Feldern mit, da aufgrund des Ersten Weltkrieges ein Arbeitskräftemangel herrschte und er sich mit den deshalb gezahlten, höheren Löhnen rasch seinen Auszug aus der elterlichen Farm finanzieren konnte. Nichtsdestotrotz habe er, nach eigener Aussage, aus dem kurzen Fernkurs und dem Studieren von Comicstrips in der täglichen Zeitung viel für seine spätere Karriere mitnehmen können. Im Dezember 1918 ging Carl Barks mit seinen Ersparnissen nach San Francisco wo er als Laufbursche für eine Druckerei arbeitete. Mit Zeichnungen, die er in seiner Freizeit anfertigte, bewarb er sich bei lokalen Zeitungen, die ihn jedoch allesamt ablehnten. Schließlich kehrte er 1920 nach 18 Monaten ohne nennenswerte Erfolge zu seinem Vater auf die Farm nach Oregon zurück.

Carl Barks ging nun, wie schon vor seiner Abreise nach San Francisco, seinem Vater auf der Farm zur Hand. 1921 heiratete Barks Pearl Turner, die Tochter eines Sägewerksbesitzers, mit der er in den darauffolgenden Jahren die Töchter Peggy (* 23. Januar 1923) und Dorothy (* 26. November 1924) bekam. Carl Barks arbeitete in dieser Zeit auch im Sägewerk seines Schwiegervaters, weil auf der Farm nicht immer genug Arbeit vorhanden war, um die junge Familie zu versorgen. Da dies jedoch nur im Sommer möglich war, suchte er weiterhin eine feste Anstellung mit dauerhaften Einkommen. Diese fand er schließlich als Hilfsarbeiter in einer Reparaturwerkstatt für Eisenbahnen der "Pacific Fruit Express" Gesellschaft in Roseville. Die Familie zog in die Stadt nahe Sacramento und blieb dort bis 1930. In dieser Zeit zog sich Barks in seiner Freizeit immer mehr an den Zeichentisch zurück, was seiner Frau zunehmend missfiel. Obwohl er mit dem Verkauf erster Zeichnungen etwas Geld dazuverdiente, trennten sie sich 1930. Carl Barks kam kurzfristig bei seinen Schwiegereltern in Oregon unter und verkaufte nun regelmäßiger Zeichnungen an das Männermagazin "Calgary Eye-Opener", sodass er bald ein kleines Haus in Medford mieten konnte. Lange blieb er dort nicht, denn im November 1931 nahm er eine Festanstellung beim "Calgary Eye-Opener" an und zog nach Minneapolis, um in der Redaktion zu arbeiten. Bis 1935 steuerte er nicht nur Zeichnungen und Karikaturen für das Magazin bei, sondern auch kurze Geschichten und Gedichte.

1935 bewarb sich Carl Barks bei den Disney-Studios in Los Angeles, die für ihren ersten spielfilmlangen Zeichentrickfilm Schneewittchen und die sieben Zwerge noch Zeichner suchten. Seine Bewerbung hatte Erfolg, und so reiste er mit Clara Balken, die er in Minneapolis kennengelernt hatte, nach Los Angeles, wo er nach einer einmonatigen Ausbildung als Zwischenphasenzeichner übernommen wurde. In den Disney Studios kam Barks das erste Mal mit Donald Duck in Kontakt, für den er eine Slapstick-Szene mit einem automatischen Friseurstuhl entwarf, um von den mühseligen Zwischenphasenzeichnungen loszukommen und sich trotz seiner Schwerhörigkeit im Studio zu bewähren. Walt Disney gefiel diese Szene, und er versetzte ihn daraufhin in die Abteilung für Geschichtenentwicklung. 1938 heiratete Carl Barks zum zweiten Mal und erwarb drei Jahre später mit seiner Frau Clara eine kleine Farm in San Jacinto. Am 9. November 1942 kündigte er seine Anstellung bei Disney, um sich mit einer Hühnerzucht und der Aussicht, ein hauptberuflicher Comiczeichner zu werden, selbstständig zu machen.

Carl Barks bewarb sich nach seiner Kündigung bei Disney Ende 1942 als Zeichner bei Western Publishing, jenem Verlag, der einige Monate zuvor bereits zwei Comics veröffentlicht hatte, an denen Barks mitgewirkt hatte. Die Antwort fiel positiv aus, und Barks erhielt das Skript zu "The Victory Garden" (dt. "Gesundheitsgemüse"), aus dem er einen zehnseitigen Comic mit Donald zeichnen sollte. Weitere Geschichten für Zehnseiter und auch längere Donald-Duck-Abenteuer schrieb Barks in den nächsten Jahren selber. Als der Verlag 1947 mit dem Wunsch nach einer Weihnachtsgeschichte an Barks herantrat, entwarf er einen reichen Onkel für Donald: Scrooge McDuck (dt. "Dagobert Duck") trat das erste Mal im Dezember 1947 in der Geschichte "Christmas On Bear Mountain" (dt. "Die Mutprobe") auf. Ihm folgten weitere, von Barks erfundene Figuren wie Gustav Gans, Daniel Düsentrieb und später die Panzerknacker sowie Gundel Gaukeley. Dagobert Duck bekam 1953 beim gleichen Verlag seine eigene Comicreihe mit dem Titel "Uncle Scrooge", für die hauptsächlich Carl Barks die Abenteuer lieferte.

Seine zweite Frau Clara verfiel zunehmend dem Alkohol und wurde dann aggressiv. Als ihr 1950 bei einer Krebsoperation ein Bein bis zum Knie abgenommen werden musste, versuchte Carl Barks sich als Pfleger. Doch auch dieser Einsatz konnte die Ehe nicht mehr retten, zumal sie weiterhin zur Flasche griff. Im Dezember 1951 wurden sie geschieden, und Barks musste mit 51 Jahren von vorne anfangen, denn ihm blieb nichts außer „zwei Decken, seiner Kleidung, dem Zeichenbrett und den National-Geographic-Ausgaben“. Doch Barks fühlte sich wie von einer Last befreit, fuhr durch das Land, sammelte Inspirationen und besuchte Ausstellungen. Auf einer solchen traf er 1952 auch Margaret Williams wieder, die sich bereits zehn Jahre zuvor bei ihm als Assistentin beworben hatte. Garé, wie Margaret von allen genannt wurde, hatte ebenfalls schon eine Scheidung hinter sich und war Landschaftsmalerin. Die beiden bezogen ein Haus im südkalifornischen Hemet und heirateten am 26. Juli 1954. Garé Barks unterstützte ihren Mann fortan bei seiner Arbeit, zeichnete Hintergründe, letterte und tuschte einige seiner Zeichnungen.

1959 begann Barks, vermehrt auch Auftragsarbeiten von Western Publishing zu zeichnen, wie etwa die "Daisy Duck’s Diary" oder "Grandma Duck’s Farm Friends" Geschichten, um sich selber keine neuen Handlungen mehr ausdenken zu müssen. Ab dem Jahre 1960 erreichten ihn die ersten Fanbriefe, die sein Verlag, der auch Barks’ Adresse geheim hielt, bis dahin nicht weitergeleitet hatte. Da seine Geschichten stets nur mit "Walt Disney" signiert waren und nicht mit "Carl Barks", wusste lange Zeit niemand, wer der Autor war, den die Fans mit dem Ehrennamen "The good artist" (dt. "Der gute Künstler") bedachten. Einige Ideen, die ihm seine Fans in Briefen schilderten, setzte er in den nächsten Jahren in seinen Comicgeschichten um. Am 30. Juni 1966 ging Carl Barks als Comiczeichner für Western Publishing offiziell in den Ruhestand, was ihn jedoch nicht davon abhielt, weitere Skripte an den Verlag zu schicken, die dann von anderen Zeichnern fertiggestellt wurden.

Carl Barks versuchte sich wie seine Frau Garé immer wieder an Landschaftsbildern mit Ölfarben. Kommerziell hatten seine Gemälde wenig Erfolg, doch Fans, die diese Bilder sahen, baten ihn, doch auch die Ducks in Öl zu verewigen. 1971 fragte Barks offiziell bei Disney an und war der erste Künstler, der eine Genehmigung dafür erhielt. Er malte seitdem für seine wachsende Fangemeinde auf Anfrage Covermotive oder Szenen aus seinen Comics als Ölbilder nach und verkaufte diese. Einige besonders gefragte Motive wurden in Varianten mehrfach von ihm gemalt. Als 1976 auf der Comicmesse in San Diego ein Händler illegale Nachdrucke eines seiner Gemälde verkaufte, entzog Disney Carl Barks die Lizenz, die Ducks in Öl malen zu dürfen. Barks beschränkte sich danach auf Ölgemälde ohne Disneyfiguren, die jedoch längst nicht soviel Anklang bei den Fans fanden.

Siehe: Liste der Gemälde von Carl Barks

In den folgenden Jahren war Carl Barks vor allem damit beschäftigt, seine Fanpost zu beantworten und jenen Verlegern mit Rat und Tat zur Seite zu stehen, die Sammelbände mit Nachdrucken seiner Comics oder Ölbilder herausgeben wollten. 1983 zog das Ehepaar Barks nach Grants Pass in Oregon, um etwas Abstand von den immer zudringlicheren Fans in Kalifornien zu bekommen. Garé Barks verstarb dort mit 75 Jahren am 10. März 1993. Im Sommer des darauffolgenden Jahres begab sich Barks zum 60. Geburtstag Donald Ducks auf Europareise und besuchte neben dem Egmont-Ehapa-Verlagshaus in Stuttgart auch Erika Fuchs in München, die alle seine Comics ins Deutsche übertragen hatte. Im Juli 1999 wurde bei ihm chronische lymphatische Leukämie diagnostiziert und Barks wurde – teilweise wegen der Chemotherapie – zunehmend schwächer. Fast ein Jahr später, im Juni 2000, entschloss er sich, seine medizinische Behandlung zu beenden. In der Nacht auf den 25. August 2000 starb Carl Barks im Alter von 99 Jahren in seinem Haus in Grants Pass im Schlaf.

Carl Barks war in seinem Leben dreimal verheiratet. Seine erste Frau Pearl, von der er sich 1930 scheiden ließ, verstarb 1987. Mit ihren gemeinsamen Töchtern Peggy und Dorothy hatte Barks nur wenig Kontakt. Peggy, die einen Sohn und zwei Töchter hatte, starb 1963 an Lungenkrebs. Dorothy hat einen Sohn und lebt im US-Bundesstaat Washington. Seine zweite Frau Clara verstarb 1964, 13 Jahre nach ihrer Scheidung von Barks, der ihr bis zu ihrem Tode Alimente zahlte. Sein Bruder Clyde lebte in Tulelake (Kalifornien), wo er ein Hotel betrieb. Er verstarb 1983 und hinterließ die Kinder William und Maxine sowie seine Frau Zena May Dillard, die 1986 starb.

Unüberschaubar groß ist Barks’ Lebenswerk ausgefallen, unvergessliche Comicgeschichten als Produkt einer jahrzehntelangen Schaffenszeit im Auftrag verschiedener Verlage zwischen 1942 und 1966. Der INDUCKS-Katalog listet über 850 Disney-Comics für Western Publishing, an denen Barks beteiligt war, Cover und Illustrationen nicht mitgezählt. Der Bogen spannt sich dabei von seinem Erstling "Piratengold" ("Pirate Gold", 1942) bis zu seinen letzten Zeichnungen in der Kurzgeschichte "Genau der richtige Job" ("The Dainty Daredevil", 1968). Höhepunkt sind die Jahre um 1950, also nach der Einführung der Figur des "Onkel Dagobert". Letzterer war zunächst als eher unsympathische Figur im Stil seines literarischen Vorbilds Ebenezer Scrooge von Charles Dickens angelegt, doch gestaltete Barks seinen Charakter später milder und gelegentlich mit weichem Herz unter rauer Schale. Barks selber nannte in Interviews häufig als seine Lieblingsgeschichten "Im Land der viereckigen Eier" ("Lost in the Andes!", 1948) und "Im alten Kalifornien" ("In Old California!", 1951). Von vielen Fans wird als ihre Lieblingsgeschichte oft "Weihnachten für Kummersdorf" ("A Christmas for Shacktown", 1952) genannt.

Unter den zahlreichen Nutzern der INDUCKS-Datenbank können Noten für sämtliche dort eingetragene Geschichten abgegeben werden. Hierbei belegt derzeit Barks Klassiker „Wiedersehen mit Klondike“ den 1. Platz von über 25.000 gelisteten Storys. Auch die Plätze 2–15 hält im Moment Barks, wobei sich die Geschichten immer wieder gegenseitig von den Spitzenpositionen verdrängen. Insgesamt stammen 75 von den Top-100-Geschichten von Barks.

Nicht in Entenhausen angesiedelt sind Carl Barks’ Comics "Barney Bear und Benny Burro" (in Deutschland erschienen ab Februar 1989).



Bereits in den ersten deutschsprachigen Micky-Maus-Heften (seit September 1951) sowie in den Micky-Maus-Sonderheften (Nr. 3, 8, 10, 16, 18, 21, 23, 24 und 31) ab 1952 fanden sich Barks-Geschichten, deren Nachdrucke zuerst ab Mai 1965 vor allem in den Heften der Reihe Die tollsten Geschichten von Donald Duck – Sonderheft erschienen (diese sind auch als Nachdruck in 2. Auflage erschienen). Die Zeitschrift "Goofy" brachte ab 1979 regelmäßig Barks’ "Ten Pagers" (zehnseitige Geschichten) in der Rubrik "Nostalgoofy".

Umfassend wurde sein Werk neu aufgelegt in "Die besten Geschichten mit Donald Duck" (58 Alben von 1984 bis 1999) und dem "Donald Duck Klassik Album" (6 gebundene 4er-Bände derselben Serie), und – erstmals systematisch – in der zwischen 1992 und 2004 erschienenen Barks Library, die 133 Alben in mehreren Teilserien umfasst und inzwischen überwiegend vergriffen ist. Die 51 Alben der Hauptserie erschienen auch in 17 gebundenen 3er-Bänden als "Barks Comics & Stories", seit Mai 2009 werden die 38 Alben der Barks-Library-Reihe "Carl Barks – Onkel Dagobert" in 13 gebundenen 3er-Bänden neu aufgelegt.

Als hochwertige 30-bändige Sammlerausgabe erschien seit Sommer 2005 im deutschsprachigem Raum und einigen anderen nordeuropäischen Ländern die auf 10 Schuber à drei gebundene Halbleinenbände großformatig angelegte "Carl Barks Collection", die sämtliche von Barks geschriebenen und gezeichneten Disney-Comics neben vielen kommentierenden Aufsätzen sowie weiteren Dokumenten über Barks enthält und im Dezember 2008 abgeschlossen wurde. Sie gilt zurzeit als "„die ultimative Edition“" von Carls Barks’ Œuvre im deutschsprachigen Raum.

Barks beobachtete die Entwicklung der Massenmedien in den USA mit großem Unbehagen. Wiederholt wies er in Interviews, die er seinen Anhängern und Journalisten gab, auf die Gefahren des Fernsehkonsums – besonders in Formen, wie er in den USA auftritt – hin.

Barks stritt zwar jegliche politische oder gesellschaftliche Intention seiner Werke ab, trotzdem fällt es bei manchen Geschichten, z. B. "Die Stadt der goldenen Dächer", schwer, die Kritik am (US-) Imperialismus zu übersehen.

Des Weiteren gibt er auch in manchen Werken die Berufe der Psychologen, Anwälte, Geheimdienstler usw. der Lächerlichkeit preis oder integriert Hitlers "Mein Kampf" in die Abbildung einer Müllkippe.

Auch den Vietnamkrieg behandelte Barks kritisch in seiner Geschichte "Der Schatz des Marco Polo". Die Geschichte ist einerseits durchgehend antikommunistisch, andererseits kommt aber auch Entenhausen (das die USA repräsentiert) nicht gut weg.

Diese Art von Kritik sorgte dafür, dass einige Werke von Barks stark zensiert oder lange Zeit gar nicht erst veröffentlicht wurden, weil sie für die "Walt Disney Studios" als politisch unerwünscht galten. Ein anderes Beispiel ist die Geschichte "Im Land der Zwergindianer", in der Barks auf Umweltprobleme und die Probleme indigener Völker aufmerksam macht.

Folgende Filme wurden unter Beteiligung von Carl Barks fertiggestellt:





</doc>
<doc id="892" url="https://de.wikipedia.org/wiki?curid=892" title="Charles de Gaulle">
Charles de Gaulle

Charles André Joseph Marie de Gaulle (* 22. November 1890 in Lille, Nord; † 9. November 1970 in Colombey-les-Deux-Églises, Haute-Marne) war ein französischer General und Staatsmann. Im Zweiten Weltkrieg führte er den Widerstand des "Freien Frankreich" gegen die deutsche Besatzung an. Danach war er von 1944 bis 1946 Präsident der Provisorischen Regierung. Im Zuge des Algerienkriegs wurde er 1958 mit der Bildung einer Regierung als Ministerpräsident beauftragt und setzte eine Verfassungsreform durch, mit der die Fünfte Republik begründet wurde, deren Präsident er von Januar 1959 bis April 1969 war. Die auf ihn zurückgehende politische Ideologie des Gaullismus beeinflusst die französische Politik bis heute.

De Gaulle wuchs in einer katholisch-konservativ geprägten und gleichzeitig sozial fortschrittlichen Intellektuellenfamilie in Lille auf: Sein Großvater war Historiker, seine Großmutter Schriftstellerin. Sein Vater, Henri Charles Alexandre de Gaulle (1848–1932) der an verschiedenen katholischen Privatschulen lehrte, bevor er seine eigene gründete, ließ ihn die Werke von Barrès, Bergson, Péguy und Maurras entdecken. Väterlicherseits hatte de Gaulle Vorfahren, die zum alten Landadel der Normandie und Burgunds gehörten. Seine Mutter, Jeanne Caroline Marie Maillot (1860–1940), stammte aus einer Familie reicher Unternehmer aus Lille mit französischen, irischen (MacCartan), schottischen (Fleming) und deutschen (Kolb) Vorfahren.

Während der Dreyfus-Affäre distanzierte sich die Familie von reaktionär-nationalistischen Kreisen und unterstützte den aus antisemitischen Gründen verurteilten Alfred Dreyfus. 1908 trat de Gaulle in die Militärschule Saint-Cyr ein, die er 1912 mit Diplom und Beförderung zum "Sous-lieutenant" (dt.: Leutnant) verließ. Dort lernte er auch Deutsch. Anschließend wurde er in die französische Armee übernommen. Er wurde dem 33 régiment d’infanterie (dt.: 33. Infanterieregiment) in Arras zugeteilt, dessen Kommandeur seit 1910 Colonel (dt.: Oberst) Philippe Pétain war.

Zu Beginn des Ersten Weltkriegs stieg er vom Lieutenant (dt.: Oberleutnant) zum Capitaine (dt.: Hauptmann) auf. Bereits im ersten Gefecht bei Dinant erlitt de Gaulle am 15. August 1914 eine Verwundung. Er kehrte dann als Chef der 7. Kompanie zum 33. Infanterieregiment an die Champagne-Front zurück. Am 10. März 1915 wurde er erneut im Gefecht verwundet. Er war entschlossen, weiterzukämpfen, und widersetzte sich seinen Vorgesetzten, indem er auf die feindlichen Gräben feuern ließ. Wegen dieses Akts des Ungehorsams enthob man ihn für acht Tage seiner Funktionen. Dennoch hatte sich de Gaulle als fähiger Offizier hervorgetan und der Kommandeur des 33. Infanterieregiments bot ihm an, dessen Adjutant zu werden.

Am 2. März 1916 wurde sein Regiment in der Schlacht um Verdun bei der Verteidigung des Dorfes Douaumont in der Flanke des Forts von Douaumont von den Deutschen attackiert. De Gaulles Kompanie war schließlich fast vollständig vernichtet, die Überlebenden in einer Ruine eingeschlossen. Laut offiziellem Bericht versuchte de Gaulle daraufhin einen Ausbruch, wurde durch einen Bajonettstich schwer verwundet und ohne Bewusstsein aufgefunden. Nach anderer Darstellung mehrerer Beteiligter ergab sich de Gaulle einer deutschen Einheit, ohne einen Ausbruchsversuch unternommen zu haben.
In deutscher Gefangenschaft erholte er sich von seiner Verwundung. Während der Internierung in Deutschland – zunächst in Osnabrück und Neisse – brachte man ihn nach zwei erfolglosen Fluchtversuchen von der Festung Rosenberg in Kronach in ein speziell für aufsässige Offiziere vorgesehenes Lager in der Festung Ingolstadt. In der Gefangenschaft lernte er Michail Tuchatschewski kennen. Er versuchte auch von dort zu fliehen. Einmal kam er bis in die Nähe von Ulm, ehe man ihn erneut fasste. 1918 kam de Gaulle schließlich auf die Wülzburg bei Weißenburg in Bayern. Ein „jämmerliches Exil“ („lamentable exil“), mit diesem Ausdruck beschrieb er seiner Mutter sein Schicksal eines Gefangenen.

Um die Langeweile zu ertragen, organisierte de Gaulle für seine Mitgefangenen umfangreiche Exposés über den Stand des laufenden Krieges. De Gaulles fünf Fluchtversuche scheiterten nicht zuletzt an seiner Körpergröße von 1,95 m, mit der er schnell auffiel, weil sie in der damaligen Zeit außergewöhnlich war. Darüber hinaus unterstützte er mehrere teilweise erfolgreiche Fluchtversuche anderer inhaftierter Kameraden. Nach dem Waffenstillstand im November 1918 wurde er von der Wülzburg entlassen. Von den zweieinhalb Jahren der Gefangenschaft behielt er eine bittere Erinnerung und schätzte sich selbst als „Heimkehrer“ und Soldat ein, der seinem Land nichts genützt hatte.

Während des Polnisch-Sowjetischen Krieges 1919/1920 meldete sich de Gaulle freiwillig für den Dienst in der französischen Militärmission in Polen und fungierte ab dem 17. April 1919 als Infanterieausbilder der neugeschaffenen polnischen Armee. Er wollte durch den Einsatz an diesem entlegenen Kriegsschauplatz seiner militärischen Karriere einen Schub geben, da er sich infolge der Kriegsgefangenschaft während des Ersten Weltkrieges kaum Verdienste erwerben konnte. Da ihm in Frankreich lediglich ein untergeordneter Posten als Referent beim Premierminister angeboten wurde, bei dem er Soldaten und Offiziere für Auszeichnungen vorschlagen sollte, verlängerte de Gaulle seinen Dienst in Polen und nahm im Mai 1920 an dem Angriff der polnischen Armee auf Kiew teil (polnisch-sowjetischer Krieg). Er wurde zum Stabschef General Henri Albert Niessels in Warschau befördert und erhielt die höchste polnische Militärauszeichnung Virtuti Militari. Einige Historiker nahmen fälschlich an, dass die Erfahrungen in Polen de Gaulles Ansichten in Bezug auf den Einsatz von Panzern und Flugzeugen und den Verzicht auf die traditionelle Kriegsführung mittels Schützengräben beeinflussten. Sein Biograph Eric Roussel (* 1951) weist demgegenüber darauf hin, dass das Konzept, Panzer für schnelle Vorstöße unabhängig von der Infanterie zu verwenden, erst 1927 durch den französischen General Aimé Doumenc entwickelt wurde.

Nach seiner Rückkehr aus Polen heiratete de Gaulle im April 1921 Yvonne Vendroux und nahm einen Posten als Lehrer an der renommierten Militärschule Saint-Cyr in Paris an, der Kaderschmiede der französischen Armee. De Gaulle war damit materiell gut abgesichert, geriet aber bald in Konflikt mit seinen Vorgesetzten aufgrund seines arroganten Verhaltens und seiner unkonventionellen Ansichten, die er in seinem Unterricht vertrat. Infolgedessen wurde er nicht befördert und wechselte 1925 in den persönlichen Stab des Marschalls Pétain. Gegenüber einem Freund soll er geäußert haben, dass er die Militärschule St.-Cyr nicht wieder betreten würde, außer als Direktor.

De Gaulles wichtigste Aufgabe bestand darin, zwei Bücher vorzubereiten, die unter dem Namen des berühmten Marschalls erscheinen sollen, jedoch kam es auch mit Pétain zu Auseinandersetzungen über deren Inhalt und zu einer deutlichen Abkühlung in dem ehemals freundschaftlichen Verhältnis. Dennoch förderte Pétain de Gaulles Karriere: Im September 1927 übernahm er als Bataillonschef ein aktives Kommando bei den französischen Besatzungstruppen in Trier. Ebenfalls setzte Pétain durch, dass de Gaulle im April 1927 eine Reihe von Vorträgen an der Militärschule St.-Cyr halten durfte, gegen den Willen des Schulleiters, General Pierre Héring. 1932 veröffentlicht de Gaulle den Inhalt dieser Vorträge unter dem Titel "Le fil de l'épée". Darin vertrat er sehr aggressiv die Ansicht, dass die französische Armee das Amt eines Oberkommandierenden schaffen müsse, der im Fall eines Krieges in alleiniger Verantwortung und mittels diktatorischer Vollmachten das Schicksal des Landes bestimmen müsse. Diese Auffassung ließ sich aber nicht durchsetzen aufgrund der Rivalität der Generäle im Generalstab und der traditionellen Feindschaft zwischen den einzelnen Waffengattungen der französischen Streitkräfte.

Von 1929 bis 1931 übernahm de Gaulle ein Kommando im französischen Mandatsgebiet Libanon. Dieser Posten, weit entfernt vom Hauptquartier in Paris, diente kaum seiner Karriere und widersprach zudem seinen persönlichen Ansichten, wonach die Kolonialarmeen bei der Verteidigung Frankreichs nur eine untergeordnete Rolle spielten. Wegen des Zerwürfnisses mit Pétain wurde ihm kein besseres Kommando angeboten. Von 1932 bis 1937 bekleidete de Gaulle eine untergeordnete Rolle im "Conseil supérieur de la défense nationale" (CSND), dem Nationalen Verteidigungsrat, dessen Aufgabe unter der Leitung von Marschall Pétain darin bestand, die französischen Streitkräfte auf einen möglichen Krieg vorzubereiten und über Kriegsstrategien, Bewaffnung und Aufstellung der Armee zu entscheiden. De Gaulles Rolle beschränkte sich darauf, Denkschriften für die Sitzungen des Verteidigungsrates vorzubereiten. Da er für eine offensive Kriegführung eintrat, die den Ansichten der meisten Generäle entgegenlief, wurden seine Entwürfe kaum beachtet.

1934 veröffentlichte de Gaulle sein bis dahin bedeutendstes Werk, eine Sammlung von Aufsätzen unter dem Titel "Vers l’Armée de Métier" („In Richtung auf eine Berufsarmee“) und forderte darin eine Reorganisation der französischen Armee, die von einer schlecht ausgebildeten Freiwilligenarmee in eine Berufsarmee umgewandelt werden sollte. Allein diese sei in der Lage, im Falle eines Krieges das Land ausreichend zu schützen und moderne Waffen wie Flugzeuge und Panzer wirkungsvoll einzusetzen. Diese Schrift forderte auch zum ersten Mal die Schaffung von Panzerverbänden, die in der Lage seien, mit schnellen, motorisierten Verbänden ins Territorium des Feindes einzudringen, statt hinter der Maginot-Linie defensiv auf dessen Angriff zu warten. Nur so könne Frankreich seine momentane qualitative Überlegenheit und seine quantitative Unterlegenheit gegenüber Deutschland kompensieren. Diese Forderungen verband de Gaulle erneut mit der Idee, im Falle eines Krieges sämtliche Streitkräfte dem Kommando eines einzelnen Oberbefehlshabers zu unterstellen. Für diesen Posten sah er einen Mann vor, der „stark genug sei, seine Rolle auszufüllen, geschickt darin, die Zustimmung der Menschen zu gewinnen, groß genug für eine große Aufgabe“ – einen Diktator, der die Macht im Land übernehmen würde. Nach Ansicht des Historikers Eric Roussel war das ein schwerer Fehler, denn dadurch wurde es sehr schwierig, für die Militärreformen eine Mehrheit im Parlament zu gewinnen: Der sozialistische Ministerpräsident Léon Blum etwa befürchtete 1936, durch die Formierung einer Berufsarmee würde die Basis für einen künftigen Staatsstreich geschaffen. Da de Gaulle kaum Unterstützung von Seiten des Generalstabs erwarten konnte, erschien sein Projekt unrealisierbar.

Die Militärs im Ausland, insbesondere Heinz Guderian im deutschen Generalstab, nahmen de Gaulles Ideen interessiert zur Kenntnis und sahen sich in ihren eigenen Bestrebungen bestärkt, eine moderne Panzerwaffe zu schaffen; de Gaulles Gegner im französischen Generalstab dagegen, besonders die Generäle Weygand, Gamelin (1872–1958) und Maurin, lehnten den Plan entschieden ab, woraufhin auch Marschall Pétain im März 1935 verlauten ließ, dass er die Reformpläne seines ehemaligen Schützlings nicht unterstützen würde. De Gaulle entfaltete daraufhin in den folgenden Jahren eine politische Kampagne in der Presse und im Parlament, die ihm den Spitznamen "Colonel Motors" einbrachte, und gewann in allen politischen Lagern genug Befürworter, sodass am 15. März 1935 zumindest Teile der Reform im französischen Abgeordnetenhaus beschlossen und sechs motorisierte Verbände aufgestellt wurden, deren Angehörige Berufssoldaten sein sollten. Am 25. Dezember 1936 übertrug man de Gaulle das Kommando über einen dieser neuen Panzerverbände, das 507. Panzerregiment in Metz. Im Generalstab wurde die Reform jedoch verwässert und bestimmt, dass diese Verbände ausschließlich der Defensive dienen und gemeinsam mit den sehr langsamen Infanterieverbänden zu operieren haben. Viele Militärhistoriker sehen darin eine wichtige Ursache für die Niederlage der französischen Armee im Mai 1940 gegenüber den schnellen deutschen Panzerarmeen. Obwohl de Gaulle mit seinem Reformkonzept letztlich scheiterte, hatte die politische Kampagne doch den Effekt, ihn bekannt zu machen; sie öffnete ihm den Weg in die Politik und damit auch in seine Rolle als Führer des französischen Widerstandes (siehe Forces françaises libres, Résistance).

Als der Zweite Weltkrieg ausbrach, war de Gaulle Colonel. Am 14. Mai 1940 erhielt er das Kommando über die neue "4 division blindée" (dt.: 4. Panzerdivision) mit 5000 Mann und 85 Panzern übertragen. Am 17. Mai führte er mit 200 Panzern ohne Luftunterstützung einen Gegenangriff auf Montcornet, nordöstlich von Laon. Am 28. Mai hatte er mehr Erfolg, als seine Panzerdivision die Wehrmacht bei Caumont zum Rückzug zwang. Er war in der Phase der deutschen Invasion in Frankreich der einzige französische befehlshabende Offizier, dem es gelang, die Deutschen zu einem Rückzug zu zwingen. Am 1. Juni hatte er den temporären Dienstgrad eines "Général de brigade" (dt.: Brigadegeneral).

Am 6. Juni ernannte Ministerpräsident Paul Reynaud ihn zum Staatssekretär des Kriegsstaates und zum Verantwortlichen für die Koordination mit Großbritannien. Als Kabinettsmitglied lehnte er den Waffenstillstand ab, verließ Frankreich am 15. Juni und setzte nach Großbritannien über. Dort vereinbarte er mit Winston Churchill am 16. Juni eine Fortsetzung der britisch-französischen Kooperation gegen Deutschland. Als er am Abend nach Bordeaux zurückkehrte, den provisorischen Sitz der französischen Regierung, schickte sich Marschall Philippe Pétain an, legal die Macht zu übernehmen. De Gaulle missbilligte die Politik Pétains, der den Waffenstillstand mit dem Deutschen Reich zu unterzeichnen bereit war, und lehnte Pétains Tun als illegitim ab. Mit 100.000 Goldfranc aus einem geheimen Fonds Paul Reynauds ausgestattet, flog er am Morgen des 17. Juni 1940 an Bord eines Flugzeugs von Bordeaux zurück nach London.

Während Philippe Pétain ankündigte, mit Deutschland einen Waffenstillstand zu vereinbaren, erlaubte Premierminister Winston Churchill de Gaulle, über die BBC zum französischen Volk zu sprechen. Er rief darin französische Offiziere und Soldaten, Ingenieure und Facharbeiter der Waffenindustrie im Vereinigten Königreich auf, ihm zu folgen und beschwor, dass die Niederlage nicht endgültig sei („Was auch immer geschehen mag, die Flamme des französischen Widerstandes darf nicht erlöschen und wird auch nicht erlöschen“). Er betonte die Bedeutung der Unterstützung durch Großbritannien und die Vereinigten Staaten. In Frankreich konnte man den Appell zuerst am 18. Juni 1940 um 19 Uhr hören. Er wurde in den Zeitungen des noch unbesetzten Südfrankreich abgedruckt und in den folgenden Tagen von der BBC wiederholt ausgestrahlt. Der Appell gilt als de Gaulles größte Rede, Régis Debray schreibt, auch wenn de Gaulles Appell „das Gesicht der Welt nicht verändert habe, so habe dank ihm immerhin Frankreich das seine gewahrt.“

Das britische Kabinett hatte im Vorfeld dem französischen Innenminister Georges Mandel vorgeschlagen, sich nach England zu begeben und selbst einen Appell an die Franzosen zu richten. Mandel hatte durch seine wiederholten Mahnungen über die Bedrohungen durch das Deutsche Reich – und im Gegensatz zu seinem Freund und ehemaligen Ministerpräsidenten Léon Blum – charakterlich wie ein Staatsmann gewirkt. Mandel weigerte sich jedoch, Frankreich zu verlassen, um sich nicht dem Vorwurf der Desertion auszusetzen (er war Jude ebenso wie Blum) und empfahl, die Aufgabe de Gaulle zu übertragen.

Am 25. Juni 1940 gründete de Gaulle in London das Komitee "Freies Frankreich" (France libre) und wurde Chef der "Freien Französischen Streitkräfte" (Forces françaises libres, FFL) und des "Nationalen Verteidigungskomitees". Daraufhin wurde de Gaulle vom Kriegsrat der Vichy-Regierung im August 1940 wegen Hochverrats in Abwesenheit zum Tode verurteilt.

Die meisten Staaten erkannten das Vichy-Regime Marschall Pétains als die legitime Regierung Frankreichs an. Churchill bemühte sich zwar anfangs diplomatisch um das Vichy-Regime, unterstützte dann aber de Gaulle und ließ die in Nordafrika in Mers-el-Kébir unter dem Kommando von Pétains Marineminister Admiral François Darlan vor Anker liegende französische Kriegsflotte am 3. Juli 1940 in der Operation Catapult zerstören.

Der Libanon wurde als eines der ersten französischen Protektorate im September 1941 durch alliierte Truppenverbände der Kontrolle des Vichy-Regimes entzogen. Bei der anschließenden Machtübernahme durch das "Freie Frankreich" kamen de Gaulle seine Kontakte aus seiner Dienstzeit in Beirut 1929–1931 zugute. General Fouad Chehab, der spätere Staatspräsident, bildete einen Freiwilligenverband von 20.000 Mann, der damals zu Beginn der Kampagne des Freien Frankreich einen erheblichen Teil des Truppenkontingents ausmachte.

Mehrere französische Kolonialbesitzungen, vornehmlich in Afrika, darunter Kamerun und Tschad, später ab 1942 Diego Suarez auf Madagaskar und Dakar in Französisch-Westafrika, unterstellten sich im Laufe des Krieges dem von de Gaulle organisierten Freien Frankreich, das von seinem "Comité National Français" regiert wurde. Er sorgte besonders dafür, dass Frankreich im Lager der Alliierten durch seine Freien Französischen Streitkräfte (FFL), die an verschiedenen Fronten den Kampf fortsetzten, stets präsent blieb. Unter anderem förderte er dank Colonel Passy, Pierre Brossolette und besonders Jean Moulin die Résistance. Mit der Transformation zum "France combattante" (kämpfendes Frankreich) strich er die politische Einheit des "France libre" mit der "Résistance intérieur" heraus.

Er stützte sich seit Juni 1940 auf das Freie Frankreich und verteidigte fortdauernd die Interessen Frankreichs im Krieg und für die Zeit danach, was in seinem Ausspruch gipfelte „Frankreich hat keine Freunde, es hat nur Interessen.“ Damit zitierte er einen damals bekannten Satz von William Ewart Gladstone (1809–1898). Dieser war ein viermaliger britischer Premierminister und einer der bedeutendsten britischen Politiker in der zweiten Hälfte des 19. Jahrhunderts.

De Gaulle konnte Churchill zur Unterzeichnung des "accord des Chequers" (7. August 1940) bewegen, demzufolge Großbritannien die Integrität aller französischen Besitzungen und die „integrale Restauration und Unabhängigkeit und die Größe Frankreichs“ erhalten sollte. Außerdem erbot sich die britische Regierung, die Ausgaben des Freien Frankreichs zu finanzieren; de Gaulle bestand aber darauf, dass die Summen rückzahlbare Vorschüsse und keine Spenden waren, die später einen Schatten auf ihn und die Unabhängigkeit seiner Organisation geworfen hätten. Die Vorschüsse wurden noch vor Ende des Krieges zurückgezahlt. 

Trotz der Verträge zwischen Churchill und de Gaulle waren die Beziehungen angespannt. Mit Blick auf die Nachkriegsordnung bezeichnete Churchill de Gaulle in Telegrammen als „größten einzelnen Feind für den Frieden in Europa“ und „schlimmsten Feind Frankreichs“. Churchill kritisierte, dass de Gaulle „sich als Retter Frankreichs aufspielen will, ohne einen einzigen Soldaten zur Operation beizusteuern“ und dass de Gaulles Verhalten und Persönlichkeit das größte Hindernis für die Beziehungen zwischen Frankreich und den Angloamerikanern seien. Über die Invasion in der Normandie informierte Churchill de Gaulle erst fünf Tage vor der Landung.

Auch die Beziehungen zu Franklin D. Roosevelt waren nicht ungetrübt; der amerikanische Präsident hatte kein Vertrauen zu de Gaulle. De Gaulle unterstellte den Amerikanern Arroganz und sagte: „Ich bin zu arm, um mich zu beugen.“ Roosevelt unterstellte de Gaulle diktatorische Absichten.

Trotz seines Ausschlusses von der anglo-amerikanischen Landung in Nordafrika (Operation Torch) durch Roosevelt und vor allem trotz dessen Unterstützung für Admiral François Darlan und General Henri Giraud, die nach der Landung in Nordafrika das Vichy-Regime mit US-amerikanischer Duldung in Algier fortzusetzen suchten, gelang es de Gaulle im Mai 1943, in Algier Fuß zu fassen. Er schuf von dort das französische Komitee für die nationale Befreiung (CFLN), um die politischen Richtungen des befreiten Frankreichs zu vereinigen, und stand alsbald an dessen Spitze. Das CFLN nahm im Juni 1944 den Namen ‚Gouvernement provisoire de la République Française‘ (GPRF) an und zog am 25. August 1944 in das befreite Paris ein, wo tags darauf auf der Avenue des Champs-Élysées ein von de Gaulle angeführter öffentlicher Triumphzug stattfand.

Es gelang de Gaulle, eine alliierte Militärregierung für die besetzten Gebiete in Frankreich zu verhindern und schnell den Forces françaises libres die Regierungsgewalt für die befreiten Gebiete zu übertragen. In weiten Teilen der Bevölkerung wurde er als Befreier gefeiert, obwohl er bei der Landung in der Normandie und dem folgenden Vormarsch der Alliierten keine militärische Rolle gespielt hatte.

Als de Gaulle sich nach dem Einmarsch in Paris "nicht" zuerst bei den Kämpfern der Forces françaises de l’intérieur (FFI) für ihre Unterstützung bedankte, sondern bei den Gendarmes (die erst am letzten Tag die Seiten gewechselt hatten), verstörte er damit viele Résistants. Auch damit wollte er jede Auseinandersetzung unter den bewaffneten Franzosen vermeiden, die den Alliierten einen Anlass für eine Besatzungsregierung geliefert hätte. Gleichzeitig erklärte er mit seiner Rückkehr in das Kriegsministerium die Kontinuität der Dritten Republik und die Illegitimität des Vichy-Regimes. Das Regime floh, als die Besatzungstruppen der Wehrmacht sich infolge der Operation Dragoon zurückziehen mussten, nach Sigmaringen.

De Gaulle wollte die Säuberungsaktion gegen französische Kollaborateure nicht den Siegermächten überlassen, sondern betrachtete dies als originäre Aufgabe der Franzosen.
Am 4. April 1944 nahm das CFLN zwei kommunistische Kommissare auf. Am 27. November 1944 amnestierte de Gaulle den bei Kriegsbeginn in die Sowjetunion desertierten Generalsekretär der KPF Maurice Thorez; im Februar 1945 erreichte er auf der Konferenz von Jalta die Anerkennung Frankreichs durch die drei großen Alliierten als eine der zukünftigen Besatzungsmächte Deutschlands. Anfang Dezember 1944 unterzeichnete de Gaulle einen auf 20 Jahre abgeschlossenen Hilfs- und Freundschaftsvertrag mit der UdSSR. Im Januar 1945 kam es zwischen de Gaulle und den USA zu Unstimmigkeiten bezüglich der Verteidigung Straßburgs während eines deutschen Gegenangriffes.

De Gaulle präsentierte seine Visionen der politischen Organisation eines demokratischen Staates am 16. Juni 1946 in Bayeux. Diese Reformen betrafen besonders ein modernes staatliches Sozialsicherungssystem und beinhalteten auch das Frauenwahlrecht.

Bereits am 16. Mai 1945 erreichte de Gaulle die Aufnahme Frankreichs in den Weltsicherheitsrat der UNO als ständiges Mitglied. Nach dem Krieg wurde er am 13. November 1945 zum Präsidenten der provisorischen Regierung ernannt, trat aber nach Meinungsverschiedenheiten mit den seit den Wahlen im Oktober das Parlament dominierenden Sozialdemokraten und Kommunisten am 20. Januar 1946 zurück, weil er die neu ausgearbeitete Verfassung der Vierten Republik missbilligte. Er verlangte eine stärkere Stellung des Präsidenten in der Verfassung, während die Mehrheit in der Nationalversammlung die Macht beim Parlament konzentrieren wollte. Möglicherweise ging de Gaulle davon aus, dass man ihn in das Amt zurückrufen würde, was seine Position gestärkt hätte. Als dies nicht geschah, gründete er 1947 eine politische Bewegung, das Rassemblement du Peuple Français (RPF), um auf diesem Weg eine neue Verfassung durchzusetzen. Als dies misslang, zog er sich 1953 nach Colombey-les-Deux-Églises zurück. 1947 hielt er zwei als bedeutend geltende Reden: am 7. April 1947 in Straßburg und am 27. Juli 1947 in Rennes.

Im Anschluss an den Misserfolg der Vierten Republik in Französisch-Indochina und im Zuge des Putsch d’Alger während des Algerienkrieges und der daraus folgenden konstitutionellen Krise ließ sich de Gaulle vom Präsidenten René Coty am 1. Juni 1958 zum Ministerpräsidenten nominieren, vom Parlament wählen und mit den von ihm geforderten weitreichenden Notstandsmachtbefugnissen für sechs Monate ausstatten. Er nutzte diese Gelegenheit, um eine neue Verfassung beschließen zu lassen. Im September nahm das Volk in einem Referendum die neue Verfassung mit 83 % an, wodurch die Fünfte Republik entstand. Alle Kolonien – Algerien wurde nicht als Kolonie, sondern Bestandteil der Republik betrachtet – konnten wählen, ob sie an der Abstimmung teilnehmen oder ihre sofortige Unabhängigkeit wählen wollten – unter Fortfall aller weiteren französischen Unterstützung. Mit Ausnahme Guineas nahmen alle Kolonien an dem Referendum teil. Im November gewann de Gaulle die Parlamentswahlen und erhielt eine komfortable Mehrheit. Am 21. Dezember wurde er in indirekter Wahl mit 78 % der Stimmen zum Präsidenten der Französischen Republik gewählt.

De Gaulle übernahm die Funktionen des Präsidenten der Republik am 8. Januar 1959. Er ergriff einschneidende Maßnahmen, um das Land zu revitalisieren, besonders die Einführung des neuen Franc (der 100 alten Francs entsprach). Er lehnte die Dominanz der USA und der Sowjetunion in der internationalen Szene ab und behauptete mit dem Aufbau der Force de frappe (erster Kernwaffentest am 13. Februar 1960) Frankreich als unabhängige Großmacht, welche mit einer eigenen Nuklearschlagkraft ausgestattet wurde, die letztlich die Großbritanniens übertraf.

Es ging ihm aber nicht nur um die "große" Politik. Um die Franzosen zu begeistern, auch den "Unpolitischen" unter ihnen die nationale Größe Frankreichs vorzuführen, ließ er z. B. den Spitzensport reorganisieren, setzte mit dem berühmten Bergsteiger Maurice Herzog ein nationales Symbol für erfolgreiche sportliche Leistung als Sportminister ein, zentralisierte die Talentauswahl und Spitzensportförderung, ließ Spitzensportler wie Staatsamateure finanzieren und ließ für die Übereinstimmung von gesellschaftlichem Anspruch und Spitzensportorganisation sorgen.

Als Gründungsmitglied der Europäischen Wirtschaftsgemeinschaft (EWG) legte de Gaulle zweimal – am 14. Januar 1963 und am 19. Dezember 1967 – sein Veto gegen den Beitritt Großbritanniens ein. Letztlich trat Großbritannien erst zum 1. Januar 1973 der EG bei und betreibt seit 2016 selbständig wieder den Austritt (BRexit) aus der Europäischen Union. 
Im April 1962 ersetzte de Gaulle den Premierminister Michel Debré durch Georges Pompidou. Im September 1962 schlug de Gaulle vor, die Verfassung dahingehend zu ändern, den Präsidenten der Republik durch eine Direktwahl zu wählen. Die Reform der Verfassung trat trotz des Widerstandes des Parlaments in Kraft. Im Oktober votierte die französische Nationalversammlung für einen Misstrauensantrag gegen die Regierung Pompidous, aber de Gaulle lehnte die ihm vom Premierminister angebotene Demission ab und entschied sich, die Nationalversammlung aufzulösen. Aus der Neuwahl im November 1962 ging die gaullistische Parlamentsmehrheit gestärkt hervor. Die direkten Präsidentschaftswahlen fanden am 5. und 19. Dezember 1965 statt; in der Stichwahl de Gaulle gegen François Mitterrand erhielt de Gaulle 55,2 % der Stimmen. Seine Gegner warfen ihm seinen Nationalismus und die abgeschwächte Wirtschaftskonjunktur in Frankreich vor.

De Gaulle sprach sich zunächst für eine Einheit des Mutterlandes und der Überseegebiete aus, auch die maßgeblich durch ihn geprägte Verfassung der Fünften Republik sah eine Unabhängigkeit nicht vor. Unter dem Eindruck des Algerienkriegs ermöglichte im September 1959 eine Verfassungsänderung den früheren Kolonien Unabhängigkeit unter fortbestehendem französischen Einfluss im Rahmen der Communauté française. Die Bürde Algeriens („boulet algérien“) reduzierte beträchtlich die französische Manövrierfähigkeit. Am 18. März 1962 unterzeichnete er in Évian-les-Bains die Verträge von Évian; diese sicherten Algerien das Recht auf eine Volksabstimmung über die Unabhängigkeit zu. Diese fand am 8. April 1962 statt. Die Politik der „nationalen Unabhängigkeit“ („l’indépendance nationale“) und der Lösung von „amerikanischer Bevormundung“ wurde ab dann verstärkt.

International förderte de Gaulle die Unabhängigkeit Frankreichs weiter: Er trat 1962 nachdrücklich für ein „Europa der Vaterländer“ (siehe auch Intergouvernementalismus, Souveränismus) unter der Führung Frankreichs ein, zu dem er neben den EWG-Staaten (ohne Großbritannien) Polen, die Tschechoslowakei, Ungarn, Rumänien, Bulgarien und Griechenland gewinnen wollte. Dafür nahm er den Rücktritt von Premierminister Michel Debré (1912–1996) in Kauf.

In seiner Deutschlandpolitik setzte er 1945 die Ruhrfrage, die 1948/1949 zur Einrichtung des Ruhrstatuts führte, auf die internationale politische Tagesordnung. Nachdem seine Regierung zunächst das Ziel verfolgt hatte, das Saarland sowie das Rheinland und Westfalen einschließlich des Ruhrgebiets von Deutschland zu lösen, nahm er zusammen mit den anderen Westalliierten anschließend großen Einfluss auf die Bildung einer in den Westen integrierten Bundesrepublik Deutschland. Am 9. September 1962 hielt er in Ludwigsburg auf Deutsch eine vielbeachtete "Rede an die deutsche Jugend." Sie gilt als ein Meilenstein in den deutsch-französischen Beziehungen und als ein entscheidender Schritt auf dem Weg zum deutsch-französischen Freundschaftsvertrag (Januar 1963).

De Gaulle verurteilte die Militärhilfe der USA an die Republik Vietnam gegen die vom Việt Minh geführte kommunistische Rebellion der Volksrepublik Vietnam und forderte die USA im Interesse eines dauerhaften Friedens zum Abzug ihrer Truppen auf. Er verurteilte 1967 den israelischen Gegenschlag gegen die ägyptische Blockade der Meerenge von Tiran während des Sechstagekriegs (Juni 1967) und die dauerhafte Besetzung des Gazastreifens und des Westjordanlands. Unter de Gaulle näherte sich der einst engste Verbündete Israels, Frankreich, der arabischen Welt, insbesondere Ägypten, aber auch Syrien und Libanon an, verhängte ein Waffenembargo gegen Israel, ließ die bereits bezahlten Mirage-Kampfflugzeuge nicht ausliefern und überließ es von da an den Amerikanern, Israel mit Waffen zu beliefern. Zur Haltung de Gaulles trugen auch die zunehmenden israelischen Operationen im bis dahin prowestlichen Libanon ab 1967 bei. De Gaulle hatte 1929–1931 (s. o.) im damals als Völkerbundsmandat französisch verwalteten Libanon gelebt und war persönlich eng mit zahlreichen Persönlichkeiten der seit Jahrhunderten frankophonen libanesischen Oberschicht verbunden, die ihn auch zum Teil bei der Kampagne des "Freien Frankreichs" 1941–1945 von Anfang an unterstützt hatten. Bis zur Präsidentschaft von Jacques Chirac (1995–2007) war die israelkritische, proarabische Orientierung französischer Außenpolitik eine gaullistische Konstante.

1958 lehnte de Gaulle die Unterstellung der französischen Mittelmeerflotte unter das NATO-Kommando ab. 1964 beendete de Gaulle das amerikanische Projekt einer multilateralen Atomstreitmacht (MLF), welche, unter internationaler Kontrolle stehend, zum Schutze Europas eingesetzt werden sollte. Zwei Jahre später forderte de Gaulle Strukturänderungen der NATO und drohte mit dem Austritt. Nach einem Ultimatum, in dem er den Abzug der NATO-Truppen bzw. ihre Unterstellung unter französisches Kommando forderte, zog sich Frankreich 1966 aus der integrierten militärischen Kommandostruktur der NATO zurück, blieb aber weiterhin NATO-Mitglied. Gleichzeitig wurde das europäische NATO-Hauptquartier SHAPE von Rocquencourt nach Mons (Hennegau, Belgien) verlegt.

Am 14. Dezember 1965 erklärte de Gaulle: „Selbstverständlich kann man auf den Stuhl wie ein Zicklein springen und rufen: ‚Europa, Europa, Europa!‘ Aber das führt zu gar nichts und bedeutet gar nichts.“ Dennoch war es Europa, das den Rahmen seiner Ambitionen festlegte, ein Europa, das selbst vom „Atlantik bis zum Ural“ geht, einen Strich durch den provisorischen Eisernen Vorhang ziehend.

In der Tat war die Hauptstütze der französischen Außenpolitik die Annäherung an den anderen Schwerpunkt des Kontinents, Deutschland, während man den „Angelsachsen“ den Rücken kehrte. Sein vertrauensvolles Verhältnis zu Konrad Adenauer und seine strategische Ausrichtung verhinderten eine Wiederholung der Politik Georges Clemenceaus, die das ohnehin schwierige Verhältnis Frankreichs zu Deutschland nach dem Ersten Weltkrieg vergiftet hatte. Gemeinsam betrieben de Gaulle und Adenauer die deutsch-französische Freundschaft, die mit einem deutsch-französischen Jugendwerk und zahlreichen Begegnungen gefördert wurde. Sie gipfelte im Élysée-Vertrag am 22. Januar 1963.

Den Beitritt des Vereinigten Königreichs zur Europäischen Wirtschaftsgemeinschaft versuchte de Gaulle systematisch zu verhindern. Neben der Befürchtung, die special relationship zu den USA könnte Großbritannien zu einem amerikanischen „trojanischen Pferd“ machen, sollen auch der mögliche Verlust der französischen Hegemonie in der europäischen Gemeinschaft und die Ablösung des Französischen als Arbeitssprache in Brüssel eine Rolle gespielt haben. Noch beim Begräbnis Adenauers musste de Gaulle vom Deutschen Bundespräsidenten zum Händedruck mit dem amerikanischen Präsidenten förmlich genötigt werden, nachdem sie sich zuvor demonstrativ aus dem Weg gegangen waren.
De Gaulle war antikommunistisch eingestellt. Allerdings ging er seit seiner Rückkehr an die Macht 1958 davon aus, dass keine Bedrohung durch eine russische Invasion bestünde. Er propagierte folglich die Normalisierung der Beziehungen mit diesen „vorübergehenden“ Regimes. Die Anerkennung des kommunistischen China ab dem 27. Januar 1964 ging in diese Richtung, wie auch seine Reise in die UdSSR im Juni 1966.

De Gaulle schuf mit der Communauté française (dt. Französische Gemeinschaft) ein Gegenstück zum britischen Commonwealth of Nations, wobei die Communauté Française die Außen-, Verteidigungs- und Währungspolitik bestimmte. Alle ehemaligen Kolonien führten Referenden durch, in denen die Gründung bestätigt wurde. Lediglich in Guinea entschied die Mehrheit anders. Mitglieder wurden Dahomey, Côte d’Ivoire, Gabun, Kongo, Madagaskar, Mauretanien, Niger, Obervolta, Tschad, Senegal, Mali, Togo und Kamerun. Dabei spielte auch die "Communauté Financière d’Afrique" des CFA-Franc eine große Rolle, bei der die französische Zentralbank die Parität des CFA zum FF jahrzehntelang stabil hielt. Durch Kooperationsabkommen sicherte sich de Gaulle starke französische Einflussmöglichkeiten. Ein Teil der Communauté Française schloss sich zur "Westafrikanischen Zollunion" (UDAO) zusammen. 1966 wurde sie zur "Zoll- und Wirtschaftsunion" (UDEAO) ausgebaut. Weitere Einflussmöglichkeiten schuf sich de Gaulle auch mit der Gründung der staatlichen Vorläufergesellschaft von Elf Aquitaine, ERAP, die unter dem Einfluss ihres langjährigen Chefs, des ehemaligen französischen Verteidigungsministers und Gründers des Auslandsgeheimdiensts DGSS, Pierre Guillaumat, dem französischen Nachrichtendienst eine hervorragende Tarnung und immense finanzielle Ressourcen für seine Aktivitäten in Afrika bot.

Hauptsächlich in der Außenpolitik kam das gaullistische Denken vom Wesen der Nation zum Ausdruck: „eine gewisse Idee Frankreichs“. De Gaulle schöpfte seine Stärke aus dem Wissen über die Geschichte Frankreichs. Nach ihm war das Gewicht dieser Geschichte der Art, dass sie Frankreich eine besondere Position inmitten des Konzerts der Nationen gab. Für ihn und für zahlreiche Franzosen waren England und die USA nur Sprösslinge Frankreichs. Gleichfalls bewertete er die Institution der UNO als lächerlich und nannte sie „das Ding“ („le machin“), was ihn jedoch nicht daran hinderte, den ständigen Sitz Frankreichs im Weltsicherheitsrat einzunehmen.

Jean-Marie Bastien-Thiry, ein von de Gaulle persönlich beförderter Oberstleutnant der französischen Armee, war mit dessen Algerien-Politik nicht länger einverstanden. Er beschloss daher mit Unterstützung der Organisation de l’armée secrète (OAS, "Organisation der geheimen Armee"), den Präsidenten zu entführen oder – falls sich eine Entführung als unmöglich herausstellen sollte – zu töten. Das Attentat von Petit-Clamart fand am 22. August 1962 auf einer Kreuzung in Petit-Clamart bei Paris statt. Sie existiert heute nicht mehr. Der Anschlag scheiterte, da die elf Attentäter das verabredete Signal in der Dunkelheit übersahen und das Feuer zu spät eröffneten. Das Präsidentenfahrzeug, eine Citroën DS, wurde von mehreren Kugeln getroffen. Eine Kugel verfehlte das Präsidentenpaar nur um einige Zentimeter. „Dies hätte ein schönes, sauberes Ende gemacht“, kommentierte de Gaulle, als er sich das Loch im Wagen ansah.

Die OAS setzte ihre Aktivitäten nach dem gescheiterten Attentat fort. Bis heute ist de Gaulles Algerien-Politik teilweise heftig umstritten. Bastien-Thiry wurde gefasst, nach kurzem Prozess zum Tode verurteilt und am 11. März 1963 hingerichtet. Seine gefassten Komplizen kamen mit zum Teil geringeren Strafen davon. De Gaulle hatte eine Begnadigung von Bastien-Thiry abgelehnt.

Das Attentat von Petit-Clamart diente Frederick Forsyth als Vorlage für seinen 1971 erschienenen Roman "Der Schakal". Der Stoff wurde 1973 verfilmt.

Bereits etwa ein Jahr zuvor, am 8. September 1961, war mit dem Attentat von Pont-sur-Seine ein Mordanschlag auf de Gaulle gescheitert. Die Attentäter hatten sich ebenfalls der OAS zugehörig erklärt.

Überzeugt von der strategischen Bedeutung der Atomwaffe, engagierte de Gaulle das Land unter Protest der Opposition für die kostspielige Entwicklung der force de frappe, von Spöttern, die sie nur als ein „Bömbchen“ („bombinette“) ansahen, als „farce de frappe“ bezeichnet. Die Antwort de Gaulles war: „In zehn Jahren werden wir etwas haben, womit wir 80 Millionen Russen töten können. Ich glaube nicht, dass man ein Volk angreift, welches die Fähigkeit hat, 80 Millionen Russen zu töten, selbst wenn man 800 Millionen Franzosen töten könnte, vorausgesetzt, es gäbe 800 Millionen Franzosen.“ Dafür veranlasste er 1960/61 in der algerischen Wüste vier oberirdische Kernwaffentests; dabei erlitten tausende Algerier Gesundheitsschäden. Von 1966 bis zum Ende seiner Amtszeit 1969 veranlasste er auf Atollen im Pazifik zehn weitere (acht davon auf dem Mururoa- und drei auf dem Fangataufa-Atoll).

John F. Kennedy hatte für die französische Unterstützung bei der Berlin- und der Kubakrise Hilfe in der Nuklearfrage versprochen, löste aber sein Versprechen bis zu seiner Ermordung nicht ein. Die Nuklearfrage belastete die franko-amerikanischen Beziehungen während der ganzen 1960er-Jahre. Erst mit Richard Nixon gab es ab 1969 erstmals einen amerikanischen Präsidenten, der klar profranzösisch war. Mit ihm teilte de Gaulle seine Geringschätzung für Ideologien, multilaterale Verträge und Institutionen. Nixon umschiffte zunächst die verpflichtende amerikanische Legislative in der Nuklearfrage, bevor er offiziell den Weg der nuklearen franko-amerikanischen Zusammenarbeit öffnete. Das Gros der Arbeit war schon geleistet. Am 24. August 1968 war es Frankreich ohne US-Hilfe gelungen, eine Wasserstoffbombe zur Detonation zu bringen ("Opération Canopus").

Die Briten, deren Nuklearstreitmacht eng mit der der Amerikaner verknüpft war, fassten es als Ohrfeige auf, als de Gaulle Frankreich zur dritten Atommacht des Westens erklärte. Die force de frappe bestand aus landgestützten Mittelstreckenraketen auf dem Plateau d’Albion (mittlerweile geschlossen), seegestützten Mittelstreckenraketen auf U-Booten und Atombomben, die von Flugzeugen abgeworfen werden konnten. Nicht zuletzt um auch auf diesem Gebiet von den beiden Supermächten unabhängig zu bleiben, forcierte er den Bau eigener französischer Kampf- (der Dassault Mirage III) und Zivilflugzeuge (der Caravelle) und unterzeichnete mit Deutschland den Airbusvertrag zur Entwicklung des Großraumflugzeugs A300. Auch die europäische Trägerraketentechnik, deren ziviler Zweig ELDO mit den Europa-Raketen war, wurde von de Gaulle in diesem Zusammenhang vorangetrieben.

Während François Mitterrand sich heftig gegen das Atomprogramm sperrte, übertrug de Gaulle die Aufsicht des Projekts dessen Bruder Jacques Mitterrand.

Auf Anregung des französischen Ökonomen Jacques Rueff (1896–1978) war die Währungspolitik unter de Gaulle stark auf Gold ausgerichtet. Im Februar 1965 kündigte de Gaulle an, Währungsreserven in US-Dollar im Rahmen des Bretton-Woods-Systems in Gold umzutauschen. Bis zum Sommer 1966 erhöhte Frankreich so den Goldanteil seiner Reserven auf 86 Prozent. Im Unterschied zu anderen Ländern, die im gleichen Zeitraum Dollar in Gold tauschten, darunter auch Deutschland, beließ Frankreich das Gold nicht in den Tresoren der Federal Reserve, sondern bestand darauf, die Goldbarren nach Frankreich zu verschiffen, damit sie nicht „dem Zugriff einer fremden Macht preisgegeben“ seien. Sein Ziel einer Rückkehr zum Goldstandard erreichte de Gaulle indes nicht.

De Gaulle wollte an der 100-Jahr-Feier der Nation in Kanada und der Weltausstellung 1967 teilnehmen, provozierte jedoch die Empörung der Föderalisten, als er in Montreal vor einer Menge von 100.000 Québécois ausrief: „Es lebe das freie Québec!“ („Vive le Québec libre!“), begleitet von allgemeinem, großem Beifall. Dies löste eine Regierungskrise in Kanada aus. In der Folge der Rede de Gaulles, in der er unter anderem sagte „ich werde euch ein kleines Geheimnis verraten, das Ihr niemandem weitererzählen werdet: auf meinem Weg habe ich eine Atmosphäre gesehen, die mich an die Befreiung erinnert hat“, erklärte der kanadische Premierminister Lester B. Pearson seine Worte für „inakzeptabel“. De Gaulle antwortete, dass das Wort „inakzeptabel“ selbst inakzeptabel sei, sagte die vorgesehene Visite in Ottawa ab und flog von Montréal zurück nach Frankreich. De Gaulle erklärte, mit seiner Rede den Frankokanadiern zu helfen, „sich selbst zu befreien“, da „nach einem Jahrhundert der Unterdrückung, das für sie nach der englischen Eroberung folgte, ihnen nunmehr auch das zweite Jahrhundert […] in ihrem eigenen Land weder Freiheit noch Gleichheit noch Brüderlichkeit brachte“. Die "New York Times" bewertete dies als „groben Akt gaullistischer Einmischung in die inneren Angelegenheiten Kanadas“ und als „bedeutende Eskalation des Streites, der während des Besuchs General de Gaulles in Kanada begann“, einer Umfrage des "L’Express" zufolge verurteilten 56 Prozent der befragten Einwohner von Paris das Auftreten de Gaulles.

Die Mai-Unruhen von 1968 waren eine weitere Herausforderung. Am 24. Mai, zwei Wochen nach Beginn der Unruhen, nahm de Gaulle erstmals im Rundfunk und Fernsehen Stellung zu den Forderungen der Demonstranten und versprach vage, ein Referendum zu Reformen auf den Weg zu bringen. Gleichzeitig forderten die Demonstranten den Rücktritt de Gaulles. Am 29. Mai reiste de Gaulle heimlich ins deutsche Baden-Baden, der Zweck dieser Reise ist unklar. Ein als mögliche Erklärung oft genanntes Treffen mit General Jacques Massu hält der Historiker Norbert Frei für unwahrscheinlich, er geht viel mehr davon aus, dass „die Staatskrise in diesem Moment in eine Nervenkrise übergegangen war“.

Nach seiner Rückkehr nach Colombey-les-Deux-Églises kündigte de Gaulle am 30. Mai 1968 in einer Rundfunkrede Neuwahlen an: „Als Inhaber der nationalen und republikanischen Legitimität habe ich seit 24 Stunden alle Eventualitäten, ohne Ausnahme, erwogen, die es mir ermöglichen würden, sie zu erhalten. Ich habe meine Entschlüsse gefasst. Unter den gegenwärtigen Umständen werde ich mich nicht zurückziehen. Ich werde nicht den Premierminister wechseln, der die Anerkennung von uns allen verdient. Ich löse heute die Nationalversammlung auf. Ich beauftrage die Präfekten, die Kommissare über das Volk geworden oder wieder geworden sind, die Subversion zu jeder Zeit und an jedem Ort zu verhindern. Was die Legislativwahlen angeht, so werden sie in den von der Verfassung vorgesehenen Fristen stattfinden, zumindest bis man hört, dass das ganze französische Volk mundtot gemacht wird, indem man es davon abhält, sich auszudrücken und gleichzeitig davon abhält, zu leben, durch dieselben Maßnahmen, durch die man versucht, die Studenten vom Studieren abzuhalten, die Lehrer vom Lehren, die Arbeiter vom Arbeiten. Diese Mittel sind Einschüchterung, Vergiftung und Tyrannei, ausgeübt seit langer Zeit in Folge durch organisierte Gruppen und eine Partei, die eine totalitäre Unternehmung ist, selbst wenn es schon Rivalen diesbezüglich gibt.“ Letzteres zielte auf die Kommunistische Partei Frankreichs.

Nach den vorangegangenen, enttäuschenden Reden schienen seine Anhänger den de Gaulle der großen Tage wiederzuentdecken: Eine Demonstration wurde für den 30. Mai 1968 organisiert, die nach Angabe der Organisatoren von einer Million Teilnehmern, nach Angaben des Polizeipräsidiums von 300.000 Teilnehmern besucht wurde. Die Wahlen vom Juni 1968 wurden ein großer Erfolg für die Gaullisten, die 358 von 487 Sitzen erhielten. Am 13. Juli 1968 wurde Georges Pompidou als Premierminister durch Maurice Couve de Murville abgelöst.

Im Februar 1969 kündigte de Gaulle an, noch im Frühjahr 1969 ein Referendum über die Reform der Regionalverwaltung und des Senats abhalten zu wollen. Wie schon 1962 sollte eine Verfassungsänderung ohne Beteiligung der Nationalversammlung durchgeführt werden. Im April kündigte de Gaulle an, dass er bei einer Ablehnung des Referendums sofort zurücktreten werde. Das Referendum erhielt somit den Charakter einer Abstimmung für oder gegen de Gaulle. In der Folge schloss sich Valéry Giscard d’Estaing mit seiner Partei der Républicains indépendants den Sozialisten an und forderte eine Ablehnung des Referendums. Obwohl das eigentliche Ziel einer Regionalreform in der Bevölkerung sehr populär war, wurde das Referendum mit 52,46 % der Stimmen abgelehnt und de Gaulle gab am 28. April 1969 kurz nach Mitternacht seinen Rücktritt vom Amt des Präsidenten der Republik bekannt.

Als Interimspräsident bis zur Neuwahl im Juni 1969 fungierte ordnungsgemäß der Präsident des Senats, Alain Poher. Am 20. Juni 1969 trat der Gaullist Georges Pompidou, der am 15. Juni die Stichwahl für das Präsidentenamt gegen den sozialen Christdemokraten Alain Poher gewonnen hatte, die Nachfolge von Charles de Gaulle an.

Nach seinem Rücktritt war de Gaulle einen Monat in Irland (von wo aus er per Brief wählte) und zog sich dann nach Colombey-les-Deux-Églises zurück, wo er an seinem (unvollendeten) Buch "Mémoires d’espoir" arbeitete. Nach einer Reise nach Spanien im Juni 1970 starb Charles de Gaulle am 9. November 1970 in Colombey-les-Deux-Églises an der Ruptur eines Aortenaneurysmas.

Sein Testament stammte aus der Zeit des Begräbnisses von General Jean de Lattre de Tassigny im Januar 1952. Dieser war nach seinem Tod vom offiziellen Frankreich und seinen Politikern in einer Art und Weise vereinnahmt worden, die de Gaulle abscheulich fand. Deshalb regelte er die Modalitäten seines Begräbnisses detailliert:

Am 12. November 1970 fand in der Kathedrale Notre-Dame de Paris ein großes Requiem für ausländische Staatschefs, Präsidenten und Könige statt. Anwesend waren US-Präsident Richard Nixon, der sowjetische Präsident Nikolai Podgorny, der britische Premierminister Edward Heath, Josip Broz Tito, Indira Gandhi, Fidel Castro, Olof Palme, Kaiser Haile Selassie, der Schah von Iran, König Bhumibol von Thailand, Juliana Königin der Niederlande, König Baudouin von Belgien, der britische Thronfolger Prinz Charles, der Fürst von Monaco und der Großherzog von Luxemburg. Neben dem deutschen Bundespräsidenten Gustav Heinemann nahmen auch die früheren Bundeskanzler Ludwig Erhard und Kurt Georg Kiesinger teil.

Zahlreiche öffentliche Straßen und Gebäude in Frankreich tragen seinen Namen. Im Besonderen die Place Charles-de-Gaulle in Paris und außerdem der Flughafen Paris-Roissy – Charles de Gaulle. Sein Name wurde auch dem gegenwärtig letzten französischen Flugzeugträger, der "Charles de Gaulle" gegeben. Sein Wohnhaus in Colombey, die "Boisserie," ist heute ein Museum, ebenso sein Geburtshaus in Lille.

In seinem Nachruf in der Wochenzeitung "Die Zeit" schrieb Theo Sommer, de Gaulle sei ein Mann des 17. oder 18. Jahrhunderts gewesen, der die Zukunft verfehlte, weil er „Vergangenheit restaurieren“ wollte. Innenpolitisch sei er den Problemen des Landes allein mit „altfränkischer Mythologie“ nicht beigekommen, seine Außenpolitik habe sich als eine unstete Folge leerer Gesten entpuppt, sein exzentrischer Auftritt in Quebec könne schließlich nur noch belächelt werden. Sommer schreibt: „Alles in allem hat Charles de Gaulle nicht viel Bleibendes bewirkt. Sein Anspruch war größer als seine Kraft, und es lag etwas Manisches in der Art, wie er diesen Anspruch verfocht. (…) Daß er voll verfehlter Ideen war, ist offenkundig. Niemand jedoch bestreitet, daß auch seine Fehler Format besaßen.“ Régis Debray bezeichnete De Gaulle als „super-scharfsichtig“, da viele seiner Vorhersagen (vom Fall des Kommunismus bis zur Wiedervereinigung Deutschlands) sich nach seinem Tod bewahrheiteten. Der Historiker Brian Crozier urteilte, „der Ruhm De Gaulles übersteige seine Leistungen“.

Nach Umfragen betrachten 70 Prozent der französischen Bevölkerung de Gaulle als die wichtigste Gestalt der gesamten französischen Geschichte. Als bleibende Leistungen de Gaulles werden vor allem der entschlossene Widerstand gegen das nationalsozialistische Deutschland und die Verfassung der Fünften Republik genannt.

Charles de Gaulle bezeichnete sich selbst als Monarchist: „Je suis un monarchiste, la République n’est pas le régime qu’il faut à la France.“ („Ich bin ein Monarchist, die Republik ist nicht die Regierungsform, die Frankreich braucht.“)

Charles hatte drei Brüder sowie eine Schwester:

Charles de Gaulle heiratete am 7. April 1921 Yvonne Vendroux (* 22. Mai 1900 in Calais; † 8. November 1979 in Paris). Der Ehe entstammen drei Kinder:

Ein Enkel (* 1948), Sohn von Philippe, trägt ebenfalls den Namen Charles de Gaulle und ist Mitglied des politischen Büros des Front National.

Als Präsident Frankreichs war Charles de Gaulle von Amts wegen Kofürst von Andorra.






</doc>
<doc id="893" url="https://de.wikipedia.org/wiki?curid=893" title="Chemnitz (Begriffsklärung)">
Chemnitz (Begriffsklärung)

Chemnitz steht für:
Chemnitz ist der Familienname folgender Personen:



Chemnitz ist der Name folgender Schiffe:
Siehe auch:



</doc>
<doc id="895" url="https://de.wikipedia.org/wiki?curid=895" title="Castel del Monte">
Castel del Monte

Das Castel del Monte (ursprünglich "castrum Sancta Maria de Monte") ist ein Bauwerk aus der Zeit des Stauferkaisers Friedrich II. in Apulien im Südosten Italiens. Das Schloss wurde von 1240 bis um 1250 errichtet, wahrscheinlich aber nie ganz vollendet. Insbesondere der Innenausbau ist anscheinend nicht beendet worden.

Das Castel del Monte liegt im Gemeindegebiet von Andria, einer Stadt in der Umgebung von Bari, sechzehn Kilometer vom Stadtkern entfernt. Sein Grundriss ist achteckig. An jeder der Ecken steht ein Turm mit ebenfalls achteckigem Grundriss. Das Hauptachteck ist 25 Meter hoch, die Türme 26 Meter. Die Länge der Seiten des Hauptachtecks beträgt 16,50 Meter, die der Türme je 3,10 Meter. Je sechs Seiten der Turm-Achtecke sind ausgeführt, zwei entfallen durch die Verbindung mit dem Hauptgebäude. Der Haupteingang ist nach Osten ausgerichtet. Der Durchmesser des Innenhofs beträgt, jeweils zwischen gegenüberliegenden Wänden gemessen 17,63 bis 17,86 Meter. Bemerkenswert sind die Abweichungen der Wandbreiten im Innenhof, diese liegen zwischen 6,96 und 7,92 Metern.

Über die Funktion der Burg ist gerätselt worden, wobei die achteckige Grundrissfigur phantastische Gedanken beflügelte. Die eher sachlichen Deutungen reichen von einem Jagdschloss bis hin zu einem Gebäude zur Aufbewahrung des Staatsschatzes. Besonders in den 1930er bis 1950er Jahren beliebt war die Deutung als "Steinerne Krone Apuliens" (Willemsen), als welche Castel del Monte angeblich die Macht Friedrichs II. symbolisieren sollte. Das Castel del Monte wird häufig auch als der Wehrbau und der Lieblingssitz Friedrichs II. bezeichnet. Ursache für die zahlreichen Vermutungen ist nicht zuletzt die spärliche Datenlage. Sicher ist, dass Friedrich II. mit einem Schreiben vom 28. Januar 1240 Richard von Montefuscolo, Justitiar der Capitanata, befahl, Vorbereitungen für den Bau eines "castrum" zu treffen. Der Bau sollte bei der Kirche Santa Maria del Monte erfolgen. Am Bau existieren jedoch keine Einrichtungen wie Gräben, Arsenale, Schießscharten oder Mannschaftsräume, die auf die Verwendung als Festung schließen lassen. Stattdessen wurde das Gebäude mit Schmuck am Hauptportal und den Fenstern, aufwändigen Sanitäreinrichtungen und Kaminen ausgestattet.

Unregelmäßigkeiten in den Abmessungen – zum Beispiel bei den Wandbreiten des Innenhofes – haben zu Überlegungen geführt, ob diese absichtlich so geplant wurden, um bestimmte Effekte hervorzurufen. So entspricht die Länge des Schattens am Tag der Herbsttag- und -nachtgleiche der Hofbreite, einen Monat später der Summe aus Hof- und Saalbreite, und einen weiteren Monat später erstreckt er sich bis zur Außenkante der Türme. 

Der Baustein des Mauermantels ist heller gelblicher oder grauweißer in der Umgebung gebrochener Kalkstein. Das Material des Eingangsportals und einiger ausgewählter Bauelemente ist "Breccia rossa" (= rote Brekzie), ein Konglomeratgestein, einige der Säulen in den Innenräumen sind aus grau-orangefarbenem Marmor. Der ursprüngliche Fußboden aus farbigem Mosaik ist nur noch in Spuren erhalten.

Die Räume sind in zwei Geschossen um einen achtseitigen Innenhof angeordnet. Die äußeren Ecken des Oktogons sind wiederum mit acht Türmen besetzt, die jeweils mit zwei Seiten in die Mauer eingebunden sind, sodass sechs Seiten freiliegen. Nur drei enthalten Wendeltreppen, in den anderen fünf sind Räume verschiedener Zweckbestimmung, u.a. Bäder und Toiletten, untergebracht. Jeder Raum im Erdgeschoss verfügt über ein einfaches Monoforium, im Obergeschoss über ein gotisch beeinflusstes Biforium. Ausnahme davon ist der Saal Richtung Andria, der über ein Triforium verfügt. Die Gewölbe sind durch kräftige Kreuzrippen betont, die auf Halbsäulen auslaufen und an deren Kreuzungspunkt sich in fast allen Räumen ein verzierter Schlussstein befindet.

Dem Eingangsportal hat der Bauherr besondere Aufmerksamkeit gewidmet, wie auch in seinen anderen Bauten. Hier wird dem Besucher der Herrschaftsanspruch des Kaisers schon am Eingang vermittelt: Die schon von weitem sichtbare plastische Betonung des Portals, also seine Pilasterrahmung, der Architrav dazwischen und der Flachgiebel zitieren die Antike, die Kapitelle erinnern an die Zisterziensergotik und die flache Rechteckumfassung des oberen Portalbereiches ist von islamischer Architektur beeinflusst. Auch der farbige Prunk des Portals mutet fast orientalisch an. Die Löwen auf den Säulen gehören zur apulischen Romanik. All das wird hier in der Portalarchitektur in eine neue Einheit gebracht, genauso wie Friedrich II. selbst von Anfang an das Imperium unter seiner Herrschaft in neuem Glanz vereinen wollte.

Es gibt einen schmalen Schlitz im Mauerwerk zwischen der äußeren Schale und der dadurch entstehenden inneren Portalzone. Hier konnte früher ein Fallgitter herabgelassen werden, um unerwünschten Besuch abzuhalten. Solche Sicherungsmaßnahmen gegen Feinde bestimmten – einer älteren Theorie zufolge – auch die Struktur des Gangsystems im Innern der Burg.

Castel del Monte wird von einem äußerst raffinierten Gangsystem durchzogen. Man konnte durchaus nicht von jedem Eingang aus in jeden Raum gelangen, zwei Räume sind nur durch einen einzigen Zugang zu erreichen. So gelangt man vom Eingangsraum, in dem heute der Eintritt zu bezahlen ist, ausschließlich in den rechts davon liegenden Raum (heute ein Souvenirladen), und von dort ausschließlich in den Innenhof. Letzteres Portal, wie alle anderen aus "Breccia rossa," ist auf der Innenseite verziert, außen, zum Innenhof hin, dagegen schmucklos. Bei den beiden anderen, unterschiedlich gestalteten, Portalen zum Innenhof ist es umgekehrt, dort befindet sich die verzierte Seite außen. Dadurch ist dem Besucher auf dem vorgegeben Weg immer die verzierte Seite zugewandt. Die Räume ohne Portal haben nur schmucklose Monoforien oder Okuli. Die beiden Räume, die nur einen Zugang haben, verfügen nicht nur über einen Kamin, sondern außerdem über Sanitäranlagen im angrenzenden Turm. In einem der beiden Räume sind geringe Reste eines Mosaikfußbodens erhalten.

Das Obergeschoss ist durch Spitzblendbögen und Pilaster gegliedert. Hier treten besonders drei Rundbogenfenster, ebenfalls aus "Breccia rossa," hervor, von denen sich nur eines über einem Portal des Erdgeschosses befindet. Da es im Obergeschoss einen umlaufenden Holzbalkon gab, dienten sie wohl auch als Zugänge zum Balkon. Die übrigen Räume haben als Fenster nur einfache Monoforien. Die Wände sind teilweise mit Opus reticulatum verziert, die Marmorverkleidungen wurden während der Plünderungen des 18. Jh. entfernt.

Früher glaubte man, dass der Kaiser Grund hatte, vor Attentaten auf der Hut zu sein, und dass er sich in seinem angeblichen „Jagdschloss“ dadurch sicherte, dass jeder, der ihn in seinen Räumen aufsuchen wollte, nicht anders konnte, als vorher nacheinander eine festgelegte Reihe von anderen Räumen zu passieren, in denen man ihn auf Waffen etc. untersuchen konnte. De Tommasi weist aber darauf hin, dass die verschiedenen Räume einfach verschiedene Funktionen hatten und der Gesichtspunkt der sozialen Rangfolge hier eine Rolle spielte und nicht die unbewiesene Hypothese eines Labyrinths zum Schutz des Kaisers.

In den Türmen gingen die Gänge in Wendeltreppen über, die sorgfältig aus Stein gebaut waren. In anderen Pfalzanlagen und auch in den Treppentürmen der Kirchen hat man in solchen Fällen meistens Holz verwendet. Oben war der Turm mit einem kleinen Kreuzrippengewölbe abgeschlossen, dessen Kragsteine die Form von Köpfen hatten. Es handelt sich also um ein in allen Einzelheiten genau durchdachtes und in mancher Hinsicht bis heute geheimnisvolles Bauwerk.

Von 1522 bis 1876 gehörte das Schloss der Familie Carafa, die 1552 zu Herzögen von Andria und Castel del Monte erhoben wurden. Nach Jahrhunderten in relativer Vergessenheit begann die kulturelle und baugeschichtliche Wiederentdeckung des Castel nach ersten Erwähnungen durch Pacichelli (1690) mit einer ersten systematischen Baubeschreibung durch Troyli (1749). Erst Henry Swinburne (1743–1803), der sich in seinen Reisebeschreibungen mit dem rätselhaften Bau beschäftigte, löste ein breiteres Interesse aus, das auch den Beginn der wissenschaftlichen Bearbeitungen und Auseinandersetzung mit diesem Bau europaweit markiert. Zwei jungen deutschen Architekten, Heinrich Wilhelm Schulz (1808–1855) aus Dresden und Anton Hallmann (1812–1845) aus Hannover, kommt das Verdienst zu, 1831 die erste architektonische Aufmessung und historische Dokumentation des Bauwerks vorgenommen zu haben. Ziel ihrer Arbeiten war dabei „die Bestandsaufnahme mit Vermessung und Baubeschreibung sowie die historische Verortung des Bauwerks auf dem Wege systematischer Urkundenstudien“ zu leisten, deren Ergebnisse erst posthum 1860 im Druck erschienen. Ihre Berichte, die Schulz und Hallmann noch im Winter 1835/36 in Rom vortrugen, bildeten die Grundlage und Hinweise für zwei Franzosen, den Historiker Huillard-Bréholles (1817–1871) und den Architekten Baltard (1805–1874), die auf den Spuren der beiden Deutschen ihre eigene Dokumentation des Castel mit detaillierten Plänen des Bauwerks erstellen und bereits 1844 in Paris veröffentlichen konnten. Herzog Honoré Théodoric d’Albert Duc de Luynes förderte dieses Projekt finanziell.

1876 wurde das Castel nach vielen Jahrzehnten des Leerstandes und der Plünderung vom italienischen Staat für 25.000 Lire erworben. Um 1900 begannen Restaurierungsarbeiten, die im damaligen Geschmack der Zeit ausgeführt wurden: Alle beschädigten Steine wurden durch Nachbildungen ersetzt, der ursprüngliche Bauzustand wurde mit modernen Materialien nachgebildet, die zwischenzeitliche Geschichte des Baus überdeckt und zugetüncht. Am Ende stand das Castel äußerlich wieder „wie neu“ da.

Zu Beginn dieser ersten Restaurierungsarbeiten (weitere folgten in den 1970er und 1980er Jahren) war – wie ältere Fotografien belegen – rund um das Castel noch ein Schuttkegel von über zwei Metern Höhe vorhanden. Um die Hauptzugangstreppe freizulegen, wurde dieser Schuttberg damals ohne weitere bautechnische Untersuchung abtransportiert. Dieser Schuttkegel enthielt – wie die damaligen Akten vermerken – viele skulpierte Elemente und zerbrochene Mauerquader.

Moderne Vermessungen unter Wulf Schirmer 1990–1996 haben die Basis für eine sachliche Beschäftigung mit dem Bauwerk "Castel del Monte" geschaffen.

Castel del Monte ist seit 1996 UNESCO-Welterbe und seit 2001 auf der Rückseite der italienischen 1-Cent-Münze abgebildet.

In dem Film "Der Name der Rose" ist nach dem Vorbild des Castel del Monte in noch gesteigerter Höhe das geheimnisvolle "Ädificium" gebaut worden, das die Bibliothek enthält, um die sich die Handlung des Romans von Umberto Eco dreht.




</doc>
<doc id="896" url="https://de.wikipedia.org/wiki?curid=896" title="ISO-Container">
ISO-Container

ISO-Container sind genormte Großraumbehälter (Seefracht-Container, engl. "freight containers") aus Stahl, die ein einfaches und schnelles Verladen, Befördern, Lagern und Entladen von Gütern ermöglichen.

Die einschlägigen Normen (z. B. Maße, Halterungen, Stapelbarkeit) wurden koordiniert von der Internationalen Seeschifffahrts-Organisation (IMO) beschlossen und sind in der ISO-Norm 668 festgelegt.

Container für Luftfracht sind nach den Standards der International Civil Aviation Organization (ICAO) genormt und unterliegen anderen Regeln.

Container für Seefracht können eine Transportkette über Land und Wasser durchlaufen, ohne dass einzelne Gebinde in Häfen und/oder Bahnhöfen umgeladen werden müssen.
Im Landverkehr auf der Straße sind diese Vorteile der Container gegenüber Trailern und Wechselbehältern nicht bedeutsam.

Man unterscheidet zwischen FCL-Verladung "(Full Container Load)", bei welcher der Versender den Container selbst belädt und der Empfänger selbst entlädt sowie LCL-Verladung "(Less than a Container Load)", bei welcher der Versender die Ware per Stückgut an den Transporteur sendet, der diese zusammen mit Stückgut-Sendungen anderer Versender in den Container verlädt, im Zielhafen wieder entlädt und per Stückgut an die Empfänger verteilt.

Die am weitesten verbreiteten ISO-Container haben eine Breite von 8 Fuß (2,4384 m) und sind entweder 20 Fuß (6,096 m) oder 40 Fuß (12,192 m) lang. Daraus ergeben sich die als Beladungs-Maßeinheiten verwendeten Abkürzungen „TEU“ "(Twenty-foot Equivalent Unit)" und „FEU“ "(Forty-foot Equivalent Unit)"; es wird beispielsweise zur Benennung der Ladefähigkeit von Containerschiffen, von Umschlagsmengen in Häfen oder von Güterbahnhöfen verwendet.

Vor allem im internationalen Warenverkehr ist es üblich, für ISO-Container die englischen Bezeichnungen zu verwenden. Die deutschen Bezeichnungen sind daher teilweise nicht geläufig.

Container sind so stabil gebaut, dass sie in mehreren Lagen übereinander gestapelt werden können. „Nach ISO-Minimalanforderungen können sechs voll beladene Container übereinander gestapelt werden. Viele Container sind allerdings auf eine Stapelhöhe von neun und mehr vollen Behältern ausgelegt.“ Je nachdem ob im Laderaum oder an Deck des Containerschiffs gelagert wird, sind weitere Einflüsse wie Wind und Wellenschlag zu beachten, die zu den Schiffsbewegungen (Quer- und Längsbeschleunigung) hinzu kommen. An Deck werden die Container mit sogenannten Twistlocks und Laschstangen/Spannschrauben gesichert, im Laderaum meistens durch Zellgerüste bzw. Cellguides. Da zwei 20′-Container zusammen 76 mm kürzer sind als ein 40′-Container, die Cellguides aber vielfach für 40′-Container bemessen sind, kommen in solchen Fällen zwischen zwei 20′-Containern sogenannte Staustücke "(Twist Stacker)" zum Einsatz, um ein Verrutschen zu verhindern.

Es gibt verschiedene Spezialversionen der Container, so zum Beispiel Kühlcontainer für verderbliche Fracht, Tankcontainer für flüssige und gasförmige Substanzen, Auto-Container für den Pkw-Transport, Wohncontainer für provisorische Unterkünfte oder Container für die Beförderung lebender Tiere.

Jeder einzelne Container besitzt eine eigene Nummer. Sie besteht aus vier Großbuchstaben, dem sogenannten Präfix, die für den Eigentümer des Containers stehen, und sechs Ziffern plus eine Kontrollziffer. Durch sie können Weg und Aufenthaltsort jedes einzelnen Containers auf seiner Reise verfolgt werden.

Vollcontainerschiffe werden nach ihrer Transportkapazität in Generationen eingeteilt. 2013 hatten die größten Containerschiffe der Triple-E-Klasse eine Kapazität von rund 18.270 TEU. Die Klassifikationsgesellschaft Germanischer Lloyd hatte eine Studie erstellt, nach der Containerschiffe über 20.000 TEU fassen können.

Als Urheber der Maße des ISO-Containers gilt der US-Amerikaner Malcom P. McLean, der 1956 zum ersten Mal Großbehälter für den Transport auf Lkw und Schiffen einsetzte. Um das übliche Umladen im Hafen einsparen zu können, soll er als junger Fuhrunternehmer im Jahr 1937 die Idee gehabt haben, zuerst ganze Lastwagen auf Schiffe zu verladen, später nur die Anhänger bzw. Sattelauflieger mitsamt ihren geladenen Behältnissen und schließlich nur noch die Behältnisse selbst.

McLean gründete die Reederei "Sea-Land Corporation" und ließ alte Öltanker so umbauen, dass an Deck zusätzlich Container geladen werden konnten. Die erste Fahrt führte die so umgebaute "Ideal X" am 26. April 1956 mit 58 Containern von Newark (New Jersey) nach Houston (Texas). Den Durchbruch hatte der Unternehmer McLean mit der Frachtversorgung des US-Militärs während des Vietnamkriegs. Es dauerte jedoch noch zehn Jahre, bis am 2. Mai 1966 ein Schiff mit Containern, die "Fairland", in einem europäischen Hafen (Rotterdam) anlegte; vier Tage später erreichte das Schiff Bremen.

Container wurden damals noch ausschließlich nach amerikanischen Normen gebaut. Da deren Maße aber nicht auf europäische Straßenverhältnisse anwendbar waren, wurden nach langen Verhandlungen die bis heute genutzten ISO-Normcontainer eingeführt.

Das erste deutsche Containerschiff, die "Bell Vanguard", lief 1966 bei der Hamburger Werft J. J. Sietas vom Stapel. 1981 war die "Frankfurt Express" der Hapag-Lloyd mit einer Stellplatzkapazität von 3420 TEU das bis dahin größte Containerschiff der Welt.

Container bestehen zum Großteil aus Stahl (meist dem widerstandsfähigen COR-TEN-Stahl). Die Herstellung eines Standardcontainers erfolgt in mehreren Schritten:
Zuerst wird die so genannte "superstructure", das Grundgerüst des Containers aus besonders stabilen Stahlteilen, montiert. An deren Ecken befinden sich die Stahlguss-Containerecken, im Fachjargon auch "corner-castings" oder schlicht "corners" genannt. Anschließend werden am Boden in Längsrichtung Streben eingezogen. Auf diesen Streben wird der Containerboden montiert, der aus mehreren Lagen von mit Schutzmitteln behandeltem Holz besteht. Da der Boden sehr tragfähig und widerstandsfähig sein muss, bestehen die verwendeten Sperrholzplatten meist aus tropischen Harthölzern. Inzwischen wird auch Material aus Bambus für die Containerböden benutzt, deren Pflanzen zehnmal schneller nachwachsen als tropische Hartholzbäume. Auch die Benutzung von Kompositwerkstoffen mit recyceltem Kunststoff werden untersucht, "Wood Plastic Composit Floorboards" für Maersk Container Industry (MCI).

Die Wände des Containers bestehen aus Trapez-Stahlblech "(Corrugation)" oder seltener glattem Stahlblech. Anschließend werden das Containerdach und die Türen montiert. Danach wird der Container mit einer schützenden Lackierung versehen und erhält seine Containernummer.

Für die Reederei Hapag-Lloyd wurde 2015 ein neuer Container-Typ mit Stahlboden statt des Holzbodens konstruiert. Durch die spezielle Konstruktion der Sicken im Stahl ist dieser "Steel Floor Container" bis zu 150 kg leichter als ältere Container.

Zur Qualitätskontrolle werden mehrere Container jeder Baureihe stichprobenartig von einer Klassifikationsgesellschaft geprüft. Entsprechen die Container den Anforderungen, erhält die Baureihe die CSC-Zulassung. Die meisten Container werden heute in China produziert. Der Preis für Seecontainer schwankt aufgrund der volatilen Stahlpreise und Dollarkurse. In der Regel bewegt sich der Preis zwischen 1950 und 2300 US-Dollar.

Optional können an Containern Zusatzelemente angebracht werden, darunter


Für den Transport von in Faltschachteln oder Kisten bzw. auf Paletten gepackte Güter mit gewöhnlichen Abmessungen werden Standardcontainer in den Größen 20 ft, 40 ft oder 45 ft High-Cube eingesetzt.

Auf dem nordamerikanischen Markt werden zunehmend High-Cube-Container (HC) mit 45′, 48′ und sogar 53′ (16,15 m) Länge eingesetzt. Für schwere Güter (z. B. schwere Maschinenteile) stehen 20′-"heavy-tested"-Container (HT) zur Verfügung, die dasselbe maximale Gesamtgewicht aufweisen wie normale 40′- und 45′-Container, d. h. 30 US-Tonnen (27,21554 ISO-Tonnen [t]).

Für den europäischen Markt gibt es Container mit etwas breiterem Innenraum, die es erlauben, zwei Europaletten quer nebeneinander einzustellen; diese werden als Binnencontainer oder „Pallet Wide“ (PW) bezeichnet. Außerdem erlaubt auch der im nordamerikanischen Binnenverkehr sehr gebräuchliche 53′-Container dank seiner Innenbreite von 2,515 m den Transport von zwei Europaletten quer nebeneinander.

Die in der Tabelle angegebenen Werte für Abmessungen und Gewichte beziehen sich auf Normwerte. In der Praxis können die Daten bedingt durch verschiedene Baureihen geringfügig abweichen.

Standardcontainer sind 8 Fuß und 6 Zoll hoch (2,59 m). Weiterhin gibt es für den Großteil der Containerarten auch die Ausführung „High-Cube“ ("HC", auch als HQ „High-Quantity“ bezeichnet). Diese Container haben eine Höhe von 9 Fuß und 6 Zoll (2,90 m). Die Abmessungen sind immer so gewählt, dass Container auch mit Lkw, Eisenbahn oder Binnenschiff befördert werden können.

Das Leergewicht des Standardcontainers liegt bei 2300 Kilogramm (kg) für einen 20-Fuß-Container und 3900 kg für einen 40-Fuß-Container. Die Zuladung beträgt bei 20-Fuß-Containern rund 21,7 Tonnen (t) bei 33 Kubikmeter (m³) Volumen. Ein 40-Fuß-Container fasst 26,5 t bei 67,6 m³ Volumen. Dies sind Standard-Angaben. Jedoch sollte bei der Beladung von Containern beachtet werden, dass in vielen Ländern für den Straßentransport ein Maximalgewicht inklusive Fahrzeug gilt. Ein 40-Fuß-Container, der mit 26,5 t Ladungsgewicht gepackt ist, kann in Deutschland beispielsweise auf der Straße befördert werden, weil im kombinierten Verkehr (d. h. Schiene – Straße – Wasserweg) 44 t Gesamtgewicht zulässig sind. Ein Container, sowohl 40-Fuß- als auch 45-Fuß-HC, darf ein Bruttogewicht von 30.480 kg haben.

Für den Transport von ISO-Containern mit Seeschiffen ist ab Sommer 2016 eine Verpflichtung vorgesehen, das Gewicht der einzelnen Container vor der Verladung festzustellen (durch Wiegen) und festzuhalten (als Bestandteil der Schiffspapiere). Hintergrund ist das „Internationale Übereinkommen zur Sicherheit auf See“ (SOLAS).

Wie auch im ISO-System gibt es eine Vielzahl an Sondermaßen:


Kühlcontainer (engl. "reefer container") werden in zwei Kategorien eingeteilt: Container, die mit Kaltluft aus der schiffsfesten Ladungskühlanlage gekühlt werden (Conair-Container, Porthole-Container), und Container mit integrierter Kälteanlage (Integralcontainer, Integral-Reefer).

Conair-Container sind doppelwandige, mit einer Wärmedämmung versehene Container, die auf einer Stirnseite zwei übereinanderliegende kreisrunde Öffnungen (Portholes) besitzen, die von Federverschlüssen geschützt werden. Diese Öffnungen dienen der Zu- und Abfuhr von Frischluft. Wird der Conair-Container in ein mit Conair-Kühlanlage ausgerüstetes Schiff geladen, öffnen sich die Verschlüsse, und Kühlluft, die von der zentralen Kühlanlage erzeugt wird, kann im Container zirkulieren. Diese Container wurden inzwischen von Integralcontainern abgelöst, da sie aufgrund der fehlenden Eigenständigkeit nur schwer im Inland oder auf nicht präparierten Schiffen genutzt werden konnten (Clip-On-Unit notwendig, siehe unten).

Integralcontainer verfügen über eine eigene Kühleinheit, die in der der Tür gegenüberliegenden Stirnwand eingebaut ist und mit elektrischem Strom betrieben wird. Jeder Container kann separat auf eine Kühl- oder Heiztemperatur eingestellt werden, die von der eingebauten Elektronik laufend überwacht und aufgezeichnet wird. Beim Inlandstransport benötigt der Container keine Clip-On-Unit (siehe unten), sondern kann mittels eines am Lkw-Chassis montierten Gensets (Generator) mit Strom versorgt werden. Die Gesamtzahl aller Kühlcontainer betrug Ende 2012 rund 2,2 Mio. TEU.

Um das zusätzliche Gewicht der Kühlanlage zu kompensieren, werden bei Integral-Reefern häufig Wände aus Aluminium verbaut.

Bei den Kühlcontainern unterscheidet sich bedingt durch die Isolation des Weiteren die Innenbreite/-länge/-höhe von der eines normalen ISO-Containers.

Bei Tankcontainern (englisch: tanktainer) handelt es sich um einen Tank für flüssige oder gasförmige Stoffe, der in einen Rahmen eingebettet ist, der der Superstructure einer TEU oder FEU entspricht. Je nach Transportgut können Kühl-, Heiz- oder Rühraggregate eingebaut sein. Insbesondere bei Stoffen mit hoher Dichte muss das Gesamtgewicht für die Ladeposition im Schiff bzw. das Transportmittel berücksichtigt werden. Tankcontainer erhöhen massiv die Umschlagsgeschwindigkeit gegenüber Tankwagen.


Auf der Grundlage der ISO-Container haben sich weitere Containertypen entwickelt.

Die wichtigsten sind:


Die unausgeglichenen Handelsströme zwischen Osten und Westen machen es erforderlich, leere Container zu repositionieren, also an einen Ort zu bringen, an dem diese wieder Ladung aufnehmen können. Weltweit werden ca. 30 % aller Seecontainer ohne Ladung umgeschlagen. Leere Container verursachen hohe Kosten für den Transport, Lagerung und Verladung.
Der Containerverkehr wächst weltweit um ca. 7 % pro Jahr. Damit steigt der Leertransport und Lagerflächenbedarf erheblich, was zu nicht unerheblichen Kosten führt.

Ein Ansatz, um das Leercontainer-Problem zu lösen, ist, die Container zusammenzuklappen. So können mehrere leere Container auf einem Stellplatz (= Slot) transportiert und gelagert werden. In der Vergangenheit hat es eine Reihe von Versuchen gegeben, die Logistikkette mit klappbaren Seecontainern zu verbessern. Diese Systeme sind jedoch alle gescheitert.

Gründe hierfür:

Klappcontainer-Projekte

Das OpenSeaContainer-Projekt greift den Ansatz der Firma Leanbox auf, die einen Container entwickelt hat, den man mit der Hilfe einer speziellen Maschine zerlegen und remontieren kann. Die Rechte an diesem See-Container sind an die „Peer Engineering Plattform“ PeerToProduct.com übergegangen. PeerToProduct hat die Konstruktionsdaten und Testresultate unter einer speziellen GNU General Public License für physische Produkte veröffentlicht.


Die großen, offen sichtbaren und visuell/optisch lesbaren Kennzeichen gemäß ISO 6346 für Container in Bauformen gemäß ISO 668 dienen der Transportabwicklung. Diese Kennzeichen dienen allenfalls mittelbar der Transportsicherheit oder dem Schutz der Ladung oder des Transportfahrzeugs.
Die Container tragen verschiedene Kennzeichen als
Für alle Zwecke wird bisher ein Kennzeichen des Herstellers auf dem Typenschild und eine Klarschriftkennzeichnung auf fünf der Oberflächen (Unterseite ohne Kennzeichen) verwendet. Optisch besser lesbare Codes und/oder elektronisch lesbare Kennzeichen wurden bisher nicht standardisiert.

Die Kennzeichnung von Containern in Klarschrift ist nach ISO 6346 international einheitlich genormt. Diese Norm beschreibt lediglich optisch lesbare Kennzeichen in Klarschrift. Gemäß ISO 15459-2 ist die herausgebende Stelle (issuing agency) für diese Kennzeichen das "Internationale Containerbüro" Bureau International des Containers et du Transport Intermodal (BIC) mit Sitz in Paris.

Jeder Container erhält hier bei der Registrierung seine weltweit eindeutige Containernummer, die an beiden Stirnfronten deutlich sichtbar angebracht wird. Sie besteht aus vier Standardbuchstaben (jeweils A–Z, an vierter Stelle bisher jedoch nur U), sechs Ziffern sowie einer aus allen 10 Zeichen und Stellen errechneten Prüfziffer, die eine fehlerhafte Erfassung durch Zahlendreher nahezu ausschließt. Eine Online-Überprüfung ist in einer Eingabemaske auf der Website der BIC möglich.

Die Containernummer des abgebildeten Containers ist an der dritten Stelle ungenau lesbar und bietet daher ein gutes Beispiel: weder ein Q noch ein G führen hier zum Ziel (Prüfziffer jeweils 3), erst die korrekte Eingabe „LSCU 107737“ gibt die 9 zurück. Das Beispiel zeigt jedoch auch, dass Nummern mit den ähnlichen Buchstaben G und Q verwechselt werden können (oder auch andere Buchstaben mit 10 Positionen Abstand im Alphabet, wie zum Beispiel H und R).

Die Standardisierung der Container und ihrer Kennzeichen wird in der ISO-Kommission "JTC1", einer gemeinsamen Kommission der TC104 und TC122, betrieben, die von Reedern und Verladern dominiert wird.

Weitere, aber nur in einzelnen Relationen verbreitete Kennzeichen nach dem Stand der Technik sind solche mit RTLS-Tags nach ISO/IEC 18000 und mit optischen Codes, auch mit Data Matrix Codes nach ISO/IEC 16022. Die Standardisierung dieser Kennzeichen entwickelt sich allmählich weiter.

Im Zuge der Verbreitung der pallet-wide-Container in Europa wurde die Intermodal Loading Unit (ILU) Initiative der EU gestartet. Diese zeigte Vorteile, wenn der Transport per Container und Wechselbehälter vereinheitlicht wird. Dies führte zur Einführung des "ILU-Codes" per Standard EN 13044, der das gleiche Format wie der bei ISO-Containern verwendete BIC-Code hat – das Internationale Containerbüro BIC verpflichtete sich, für ISO-Container nur Eigentümercodes zu vergeben, die an vierter Stelle ein U, J oder Z haben. Die neugeschaffene Vergabestelle der UIRR ("Internationale Vereinigung der Gesellschaften für den Kombinierten Verkehr Schiene-Straße") wird für Wechselbehälter nur Eigentümercodes vergeben, die an vierter Stelle ein A, B, C, D oder K enthalten – Inhaber eines BIC-U können die Ausgabe eines ILU-K mit gleicher voranstehender Ziffernfolge beantragen. Seit Juli 2011 begann die Vergabe der ILU-Codes, seit Juli 2014 werden im intermodalen Verkehr nur noch Wechselbehälter mit ILU-Code akzeptiert und ab Juli 2019 müssen alle Behälter ein standardkonformes Schild tragen.

Die Transportsicherheit der Container wird im Grundsatz nach denselben Kriterien organisiert wie vor deren Einführung (ca. seit 1968) bereits im europäischen und US-amerikanischen Eisenbahnverkehr üblich. Neu eingeführt und ebenfalls international einheitlich genormt sind gegenüber anderen Transportformen die genormten Eckbeschläge für die Handhabung. Neu gegenüber den seinerzeit üblichen Standards im Eisenbahnverkehr (um 1968) haben die Container

Anders als im europäischen Eisenbahnverkehr für Waggons üblich tragen die Container

Weitere technisch aufwändige Sicherheitseinrichtungen sind in einzelnen Fällen in Gebrauch, wie:

Der Container wird durch den Versender gepackt. Dieser trägt auch die Gewähr für die ordnungsgemäße Deklaration des Inhalts (nur in den Begleitpapieren) und dessen sichere Befestigung (zur Vermeidung von Schwerpunktsänderungen). In der weiteren Handhabung durch den Frachtführer (Spediteur, Reeder) und durch den Verladebetrieb gibt es weltweit keinerlei Gebrauch von Einrichtungen, die feststellen können
Daher werden alle erweiterten Maßnahmen zur Containersicherheit allein auf die Unversehrtheit des Verschlusszustandes abgestellt. Ein erkennbar geöffneter Container bleibt daher so lange stehen, bis die Unbedenklichkeit des Verschlusszustandes erneut geprüft und bescheinigt wurde.

Die Transportsicherheit wird jeweils vorlaufend zum physischen Transport dokumentiert und wiederholt durch zertifizierte Transportunternehmen geprüft. Spätestens 24 Stunden vor dem Verladen wird eine verlässliche Sicherheitsinformation durch den Zoll festgestellt oder der Container bleibt stehen, bis diese Information mit demselben Zeitabstand zur Verladung verfügbar ist. Einzelheiten zum Kontrollverfahren werden fortlaufend den Risikoanalysen der Sicherheitsbehörden und des Zolls angepasst.

Für Luftfrachtcontainer gibt es keinen vergleichbaren Verschlusszustand.

Investments in ISO-Container galten für lange Zeit als lukrativ, erwiesen sich gleichwohl ab Mitte der 2010er-Jahre nach Insolvenzen der Hamburger Magellan-Gruppe sowie Subunternehmen der P&R-Gruppe tendenziell als Fehlinvestition.





</doc>
<doc id="898" url="https://de.wikipedia.org/wiki?curid=898" title="Carotine">
Carotine

Carotine (von lateinisch "carota": „Karotte“) sind zu den Terpenen gehörige Naturfarbstoffe mit der Summenformel CH, die in vielen Pflanzen vorkommen, besonders in den farbigen Früchten, Wurzeln und Blättern. Sie zählen zu den sekundären Pflanzenstoffen. Chemisch handelt es sich dabei um Tetraterpene, bei denen ein bis zwei Jonon-Ringe durch eine Kohlenstoffkette mit neun Doppelbindungen verbunden sind. Deutlich abgegrenzt werden sie von den Xanthophyllen, die neben Kohlenstoff und Wasserstoff auch Sauerstoff enthalten.

Die Carotine sind unpolar und deswegen fettlöslich, d. h. weiterführend auch, sie können im menschlichen Organismus nur zusammen mit zumindest einer geringen Menge Fett verwertet werden. Carotine treten in vielen Varianten auf – über 600 sind bis heute bekannt. Allen gemeinsam ist eine ähnliche Grundstruktur bei unterschiedlichen Endgruppen. Das bekannteste Carotin ist β-Carotin. Von ihm leitet sich der Name der gesamten Gruppe der Carotine ab. Es ist die wichtigste Vorstufe von Vitamin A in Lebensmitteln und wird deswegen auch als Provitamin A bezeichnet. Neben β-Carotin können auch α- und γ-Carotin und β-Cryptoxanthin in Vitamin A umgewandelt werden. Aber die einzelnen Ausprägungen, wie etwa β-Carotin, haben auch von Vitamin A unabhängige Wirkungen.
In Pflanzen haben Carotine eine Funktion bei der Photosynthese und schützen sie vor schädlichen Auswirkungen der UV-Strahlen. In den Wurzeln von Pflanzen gebildet, übernehmen sie dort den Schutz vor Infektionen.

Der Mensch nimmt mit seiner Nahrung in größeren Mengen α- und β-Carotin, α- und β-Cryptoxanthin und Lycopin auf. Die Funktionen und Wirkungen der Carotine im menschlichen Körper werden mehr und mehr bekannt, sind aber auch leicht umstritten. So lassen etwa neuere Studien Zweifel an der krebshemmenden Wirkung aufkommen. Eine generell zellschützende Wirkung als Antioxidantien kann ihnen aber mit Sicherheit zugeschrieben werden.

Die IUPAC empfiehlt eine abweichende Nomenklatur der Carotine. So wird das Carotin entsprechend den Endgruppen benannt und mit β, ε (enthalten Jononringe) und ψ (offenkettig) gekennzeichnet. α-Carotin ist somit β,ε-Carotin, β-Carotin ist β,β-Carotin und γ-Carotin ist nach IUPAC-konformer Nomenklatur β,ψ-Carotin.

α-Carotin (Alpha-Carotin) ist mit β-Carotin der Farbstoff der Mohrrübe oder Karotte und mit Lycopin das Rot der Tomate. Auch die gelben bis roten Farbstoffe in Spinat, Salat, Orangen, Bohnen, Broccoli und Paprika sind Carotine. 
β-Carotin (Beta-Carotin, INN: Betacaroten) ist die wichtigste Vorstufe von Retinol (Vitamin A) und wird deshalb auch als Provitamin A bezeichnet.
Die besten Quellen von Beta-Carotin sind Grünkohl, tiefgelbe bis orange Früchte und Gemüse, aber auch dunkelgrüne Gemüsesorten. Grünkohl hat mit 8,68 mg β-Carotin/100 g den höchsten Gehalt an Betacarotin von allen Lebensmitteln.
Beispiele:

Aus Pflanzen extrahiertes oder synthetisch hergestelltes Beta-Carotin wird als Lebensmittelfarbe ("E 160" beziehungsweise "E 160 a", siehe Lebensmittelzusatzstoff) sowie als Beigabe zu Vitaminpräparaten verwendet. Beta-Carotin wird vielen Lebensmitteln wie zum Beispiel Butter, Margarinen, Süßwaren, Molkereiprodukten und Limonaden in teilweise sehr hohen Mengen zugesetzt, um dem Verbraucher das von ihm erwartete Bild der Ware (Farbe) zu bieten. Ansonsten wären beispielsweise Margarinen mehr oder weniger weiß bis hellgrau.

Die Aufnahme von β-Carotin ist schlechter als von Vitamin A. Es muss etwa sechsmal so viel β-Carotin aufgenommen werden, um dem Körper die gleiche Menge Vitamin A zur Verfügung zu stellen. Die beiden Stoffe sind frei kombinierbar. Die Deutsche Gesellschaft für Ernährung (DGE) empfiehlt für gesunde Erwachsene eine tägliche Zufuhr von 0,8 bis 1,1 mg Vitamin A.

Anders als bei Vitamin A kann es beim Menschen nach übermäßiger Zufuhr von Carotinen mit Provitamin-A-Aktivität nicht zu einer Hypervitaminose kommen. Dies liegt an der geringeren Resorptionsrate für Carotine (20–35 % für β-Carotin) sowie an der begrenzten Kapazität zur Umwandlung in Vitamin A. Die enzymatisch gesteuerte Konversion zu Vitamin A hängt von der Höhe der β-Carotin- und Proteinzufuhr, der Vitamin-E-Versorgung und der gleichzeitigen Fettzufuhr ab. Auch der Vitamin-A-Status spielt eine große Rolle: Je besser die Vitamin-A-Versorgung, desto geringer die Enzymaktivität.

Der Mensch transportiert einen Großteil aufgenommener Carotinoide in unveränderter Form, während die Ratte Carotinoide nahezu vollständig konvertiert. Carotinoide finden sich in verschiedenen Organen des Menschen. Die höchsten Konzentrationen lassen sich in der Leber, der Nebenniere, den Hoden und dem Gelbkörper feststellen. Niere, Lunge, Muskeln, Herz, Gehirn oder Haut weisen dagegen geringere Carotinoidspiegel auf.

Ein Überschuss an Carotinoiden als Folge der langandauernden, übermäßigen Zufuhr macht sich beim Menschen optisch als Gelbfärbung der Haut bemerkbar („Carotinodermie“, „Karottenikterus“). Betroffen sind zunächst der Bereich der Nasolabialfalten, die Palmarseite der Hände und die Fußsohlen. Die Gelbfärbung geht zurück, wenn die Überversorgung eingestellt wird.

Teratogene Effekte des β-Carotin sind nicht bekannt. Sogar hohe supplementierte Tagesdosen (20–30 mg) oder eine besonders carotinreiche Ernährung über lange Zeiträume sind nicht mit einer Toxizität verbunden.

Allerdings steht die mehrjährige Supplementierung von β-Carotin im Verdacht, bei Rauchern und Trinkern das Inzidenzrisiko für Lungenkrebs und Darmkrebs zu erhöhen. So wurde in einer australischen Studie im Journal of the National Cancer Institute vom 21. Mai 2003, die den Effekt als Sonnenschutzmittel untersuchen wollte, bei Rauchern und Personen, die regelmäßig mehr als ein alkoholisches Getränk pro Tag zu sich nahmen, eine doppelte Anzahl von Adenomen des Dickdarms – den Vorstufen von Darmkrebs – gefunden. Bei Nichtrauchern und Nichttrinkern reduzierte sich stattdessen deren Auftreten um 44 %. Die American Cancer Society verlangt Warnschilder auf β-Carotin-haltigen Waren, um Raucher auf ein möglicherweise gesteigertes Lungenkrebsrisiko hinzuweisen.

Verordnungen des Bundesinstituts für Arzneimittel, die seit Mai 2006 in Kraft sind, berücksichtigen nur zum Teil die Ergebnisse und naheliegenden Schlussfolgerungen aus obiger Studie. Seit diesem Zeitpunkt müssen alle Medikamente, die β-Carotin enthalten, eine Warnung aufweisen, dass sie ein erhöhtes Risiko für Raucher beinhalten, an Lungenkrebs zu erkranken. Auch dürfen Medikamente mit mehr als 20 mg β-Carotin nicht mehr an Raucher verschrieben werden. Durch die Studien kann keine Aussage darüber getroffen werden, ob durch den Konsum von naturbelassenen Lebensmitteln mit natürlichem Carotingehalt eine Gefahr bestehen könnte; allerdings ist der Anteil von β-Carotin in naturbelassenen Lebensmitteln und Säften oftmals merklich geringer als in künstlich damit angereicherten.

Im Rahmen der sogenannten CARET-Studie wurde in den 1990er Jahren der Effekt einer Supplementierung von täglich 30 mg β-Carotin in Kombination mit 25.000 I.E. Retinylpalmitat auf das Inzidenzrisiko für verschiedene Krebserkrankungen sowie die Mortalität untersucht. Es nahmen 18314 Personen teil, deren Inzidenzrisiko für Lungenkrebs erhöht war, weil sie entweder eine langjährige Raucherkarriere hinter sich hatten oder Asbeststaub ausgesetzt waren. Die Forscher fanden als mutmaßliche Supplementierungsfolge neben der weiteren Risikoerhöhung für Lungenkrebs auch eine erhöhte Sterblichkeit wegen Herz-Kreislauf-Erkrankung.

Carotin wurde zuerst von Heinrich Wilhelm Ferdinand Wackenroder entdeckt, der es aus Karotten isolierte. William Christopher Zeise erkannte, dass es sich um einen Kohlenwasserstoff handelt. Paul Karrer, der 1937 für seine Untersuchungen an Carotinoiden, Flavinen und Vitaminen den Nobelpreis für Chemie erhielt, hatte 1930 die korrekte Konstitutionsformel des beta-Carotins aufgestellt. Zur Strukturaufklärung nutzte er Abbaureaktionen mit Ozon, Kaliumpermanganat oder Chromsäure. Die genaue Anordnung der Atome von β-Carotin wurde mit Hilfe der Röntgenstrukturanalyse bestimmt und 1964 bekanntgegeben.


</doc>
<doc id="899" url="https://de.wikipedia.org/wiki?curid=899" title="Chemisches Element">
Chemisches Element

Ein Chemisches Element ist die Sammelbezeichnung für alle Reinstoffe, welche mit chemischen Methoden nicht mehr in andere Stoffe zerlegt werden können. Die Elemente sind in der Chemie daher die Grundstoffe der chemischen Reaktionen und die Atome ihre kleinsten nicht teilbaren Einheiten. Alle Atome eines chemischen Elements haben dieselbe Anzahl an Protonen im Atomkern, zeigen daher gleichen Aufbau der Elektronenhülle und verhalten sich folglich auch chemisch gleich. Die Atome eines Elements sind also Nuklide mit derselben Ordnungszahl.

Ein Element wird durch ein Elementsymbol bezeichnet, eine Abkürzung, die meist vom lateinischen Namen des Elements (beispielsweise Pb von "plumbum", Fe von "ferrum") abgeleitet ist. Die Elemente werden im Periodensystem nach steigender Kernladungszahl angeordnet. Insgesamt sind bis heute (2015) 118 Elemente nachgewiesen worden. Davon kommen die Elemente mit Ordnungszahl von 1 bis 94 auf der Erde natürlich vor, allerdings oft in Form von chemischen Verbindungen und zum Teil nur in äußerst geringen Spuren, z. B. als kurzlebige Zwischenprodukte im radioaktiven Zerfall.

Der Begriff "chemisches Element" entstand ab dem 17. Jahrhundert, als zunehmend erkannt wurde, dass der "Elementbegriff" der Alchemie untauglich für eine wissenschaftliche Aufklärung der vielfältigen Eigenschaften von Stoffen und ihren Reaktionen miteinander ist. Einen maßgeblichen Schritt tat Etienne de Clave, der 1641 die Definition gab, "Elemente" seien „einfache Stoffe, aus denen die gemischten Stoffe zusammengesetzt sind und in welche die gemischten Stoffe letztlich wieder zerlegt werden können“. Robert Boyle veröffentlichte 1661 unter dem Titel "The Sceptical Chymist" eine einflussreiche Kritik an den Unzulänglichkeiten der Alchemie. Darin führte er aus, dass man unter "chemischen Elementen" diejenigen primitiven Stoffe verstehen sollte, „die weder aus anderen Substanzen noch auseinander entstanden sind, sondern die Bestandteile bilden, aus denen gemischte Stoffe bestehen“.

Beide Forscher stellten sich damit einerseits in Gegensatz zur herrschenden Vier-Elemente-Lehre der Alchemisten, die alle Stoffe durch unterschiedliche Mischungen von "Feuer", "Wasser", "Luft" und "Erde" zu erklären suchte, und machten den Begriff "Element" überhaupt der näheren experimentellen Erforschung zugänglich. Andererseits blieben sie der Alchemie verhaftet, indem sie annahmen, einzeln könnten diese "Elemente" nicht in der Wirklichkeit vorkommen, vielmehr sei jeder reale Stoff eine Mischung "sämtlicher Elemente" gleichzeitig. Boyle bezweifelte, dass es solche "Elemente" überhaupt gibt. Ganz im Geist der damals aufkommenden Mechanik nahm er an, die einheitlich erscheinenden Stoffe bestünden aus einheitlichen "kleinen Teilchen", die ihrerseits in jeweils wohlbestimmter Weise aus kleinsten "Korpuskeln" zusammengesetzt sind. Die Vielfalt der Stoffe und ihrer Reaktionen erklärte er durch die die unzähligen möglichen Arten, in denen sich die "Korpuskeln" zu diesen, für jeden Stoff charakteristischen Teilchen verbinden können. Als Folge einer Umlagerung der "Korpuskel" sah er auch die in der Alchemie gesuchte Transmutation als möglich an, d. h. die "Umwandlung" eines "Elements" (z. B. Blei) in ein anderes (z. B. Gold).

Doch war Boyle damit der Wegbereiter für Antoine Laurent de Lavoisier, der zwar die "Korpuskeln" als metaphysische Spekulation abtat, aber 1789 die "chemischen Elemente" dadurch charakterisierte, dass sie nicht in andere Substanzen zerlegt werden konnten. Genauer: Alle Stoffe sollten als "elementar", d. h. nicht zusammengesetzt, gelten, solange keine Methoden zur weiteren Abtrennung einzelner Bestandteile gefunden wären.

Auf diese Definition gestützt, eröffneten Lavoisiers außerordentlich genaue Beobachtungen an chemischen und physikalischen Stoffumwandlungen den Weg zur modernen Chemie. Insbesondere entdeckte er die "Erhaltung der Gesamtmasse bei allen Stoffumwandlungen" und bestimmte die genauen "Massenverhältnisse", in denen "reine Elemente" miteinander reagieren. So wurde John Dalton auf das Gesetz der multiplen Proportionen geführt, das er 1803 durch die Annahme der Existenz "unveränderlicher" und "unzerstörbarer" kleinster Materieteilchen, der Atome, wissenschaftlich begründen konnte. Nach Dalton wird ein "Element" durch eine Sorte einheitlicher Atome definiert, die sich nach festen Regeln mit anderen Atomen verbinden können. Das unterschiedliche Verhalten der "Elemente" wird dadurch erklärt, dass ihre "Atomsorten" sich in "Masse", "Größe" und "Bindungsmöglichkeiten" zu anderen Atomen unterscheiden. Daraus entsteht u. a. die Möglichkeit, die relativen Atommassen der verschiedenen "Elemente" untereinander zu bestimmen, wodurch die Atome erstmals zum Gegenstand der experimentellen Naturwissenschaft wurden.

Daltons Ansatz erwies sich in der Interpretation der "chemischen Reaktionen" und "Verbindungen" als außerordentlich erfolgreich. Seine Definitionen von "Element" und "Atom" wurden daher beibehalten, auch als die Annahmen der "Unveränderlichkeit der Atome" (insbesondere ihrer Unteilbarkeit) und der "Gleichheit aller Atome desselben Elements" durch Beobachtungen an den 1896 entdeckten radioaktiven Elementen endgültig widerlegt wurden: 1902 erklärte Ernest Rutherford in seiner "Transmutationstheorie" die radioaktiven Zerfallsreihen als Folge von Teilungen der Atome und weiteren Elementumwandlungen. 1910 entdeckte Frederick Soddy, dass Atome desselben radioaktiven Elements in verschiedenen Zerfallsreihen mit "verschiedener Masse" auftreten können (Isotopie). Ab 1920 wurden diese Erscheinungen dann bei allen Elementen gefunden.

In der ersten Hälfte des 20. Jahrhunderts wurde der "Atombau" dahingehend geklärt, dass das chemische Verhalten weitestgehend von der "negativ" geladenen "Elektronenhülle" des Atoms bestimmt wird, die ihrerseits durch die "positive" Ladung des Atomkerns bestimmt ist. Daher geht der heutige Begriff des chemischen Elements von der elektrischen Ladung des Atomkerns aus. Sie ist durch die Anzahl der im Kern vorhandenen Protonen gegeben, die daher als chemische Ordnungszahl des Atoms bzw. des Elements bezeichnet wird.

Rückblickend auf die ursprünglichen Definitionen für den Begriff "Element" von Clave, Boyle und Lavoisier (s. o.) und auch auf die Boyleschen "Korpuskeln" scheint es, dass die besten Realisierungen dieser seinerzeit hypothetischen Vorstellungen nicht durch die heutigen chemischen Elemente und Atome, sondern durch die "Atombausteine" Proton, Neutron, Elektron gegeben sind.

In der Antike und bis weit ins Mittelalter war man der Auffassung, dass die Welt aus den vier Elementen Erde, Wasser, Luft und Feuer aufgebaut ist.

Von den Elementen im heutigen Sinne waren in der Antike nur wenige in Reinform bekannt, die entweder gediegen vorkamen oder aus Erz geschmolzen werden konnten: Kohlenstoff, Schwefel, Eisen, Kupfer, Zink, Silber, Zinn, Gold, Quecksilber und Blei. Im Laufe der mittelalterlichen Bergbaugeschichte wurden dann, vor allem im Erzgebirge, in "Erzen" geringe Mengen an Beimengungen unbekannter Metalle entdeckt und nach Berggeistern benannt (Cobalt, Nickel, Wolfram). Die Entdeckung des Phosphors 1669 durch Hennig Brand läutete schließlich das Zeitalter der Entdeckung der meisten Elemente ein, einschließlich des Urans aus Pechblende durch Martin Heinrich Klaproth 1789.

Vor dem Jahre 1751 waren folgende Nebengruppenelemente bekannt: Eisen, Cobalt, Nickel, Kupfer, Zink, Silber, Platin, Gold sowie Quecksilber, ferner die Hauptgruppenelemente Kohlenstoff, Phosphor, Schwefel, Arsen, Zinn, Antimon, Blei und Bismut.

Vom Jahre 1751 bis zum Jahre 1800 kamen noch Wasserstoff, Titan, Chrom, Mangan, Yttrium, Zirconium, Molybdän, Wolfram, Uran hinzu, ferner Stickstoff, Sauerstoff, Chlor und Tellur.

In der Zeit vom Jahre 1800 bis zum Jahre 1830 wurden insgesamt 22 neue Elemente entdeckt: die Nebengruppenelemente Vanadium, Tantal, Rhodium, Palladium, Cadmium, Osmium, Iridium und die seltene Erde Thorium, ferner die Hauptgruppenelemente Lithium, Beryllium, Natrium, Magnesium, Kalium, Calcium, Strontium, Barium, Bor, Aluminium, Silicium, Selen, Iod und Brom.

Elf weitere Elemente traten zwischen dem Jahre 1830 bis 1869 hinzu. Sie waren auch ein Marker für den technisch-wissenschaftlichen Entwicklungszustand, denn es wurden auch schwer auffindbare und seltene Elemente entdeckt und beschrieben. Es waren Helium, Rubidium, Caesium, Indium, Thallium, Niob, Ruthenium, Lanthan, Cer, Terbium, Erbium.

Im Laufe des 19. Jahrhunderts wurden die Metalle der Seltenen Erden entdeckt, womit fast alle natürlich vorkommenden Elemente bekannt waren. In dieser Zeit wurden auch viele hypothetische Elemente postuliert, die später wieder verworfen wurden, so etwa das Nebulium. Im 20. und dem begonnenen 21. Jahrhundert wurden viele in der Natur nicht vorkommende Elemente – die Transurane – künstlich erzeugt, teils in Kernreaktoren, teils in Teilchenbeschleunigern. Allen diesen Elementen ist gemeinsam, dass sie instabil sind, d. h. dass sie sich unterschiedlich schnell in andere Elemente umwandeln. Mit der Entdeckung weiterer solcher kurzlebiger Elemente ist zu rechnen; sie entstehen jeweils in nur äußerst geringen Mengen. Ihren Namen erhielten die Elemente jeweils von ihrem Entdecker, was im 20. Jahrhundert zu einer Elementnamensgebungskontroverse führte. Elemente, die noch nicht erzeugt oder benannt wurden, tragen Systematische Elementnamen.

Die Elemente ordnet man nach ihrer Kernladungszahl (Ordnungszahl) und der Elektronenkonfiguration ihrer Atome im "Periodensystem der Elemente" (PSE) in Gruppen und Perioden an. Dieses System wurde vom russischen Gelehrten Dmitri Iwanowitsch Mendelejew zeitgleich mit dem deutschen Arzt und Chemiker Lothar Meyer 1869 begründet.

Viele Grundeigenschaften chemischer Elemente lassen sich aus dem Aufbau ihrer Atome ableiten. Diverse, historisch gewachsene Atommodelle, wie das erfolgreiche Bohr’sche Schalenmodell liefern dazu die theoretischen Grundlagen.

Alle Atome eines Elements haben im elektrisch ungeladenen Zustand die der Protonenzahl entsprechende gleiche Anzahl Elektronen in den Elektronenhüllen. Bei der Anordnung der Elemente gemäß wachsender Protonenzahl beziehungsweise Ordnungszahl im sogenannten "Periodensystem" sind die Elemente zudem nach verwandten oder periodisch wiederkehrenden Eigenschaften (Schalenabschluss) gruppiert.

Bei chemischen Reaktionen werden nur die Elektronen auf den Außenschalen der Reaktionspartner umgeordnet, der Atomkern bleibt hingegen unverändert. Das Bestreben, "Schalen" durch "Abgabe" oder "Aufnahme" von Elektronen abzuschließen und damit relativ zu stabilisieren, "dominiert" über den elektrischen Ladungszustand eines Atoms. Beschrieben wird dieses Bestreben durch die Elektronegativität. "Schalenabschlusszustand" und "Ladungszustand" sind damit direkt mit dem "chemischen Reaktionsvermögen" eines Elements gekoppelt. Edelgase, Elemente mit abgeschlossener Schale im neutralen Zustand sind reaktionsarm, sie bilden nur unter drastischen Bedingungen Verbindungen. Atome suchen "primär" die sogenannte "Edelgaskonfiguration" (Schalenstabilität) zu erreichen, auch wenn das zu Lasten der "elektrischen Neutralität" geht, und streben "sekundär" nach "Ladungsausgleich" der Gesamtkonfiguration. Ein noch feineres Unterscheidungsschema zur eindeutigen Identifizierung der Elektronen eines Elements liefert das "Quantenzahlenquartett": Hauptquantenzahl, Nebenquantenzahl, Magnetquantenzahl, Spinquantenzahl, also quantentheoretische Elementeigenschaften.

Weitere Eigenschaften der Elemente ergeben sich durch die Beachtung der "Kernkonfigurationen" eines Elementatoms. Kerne ein und desselben Elements können mit einer unterschiedlichen Anzahl an Neutronen bestückt sein. Diese nach der Anzahl der Neutronen verschiedenen Atome eines Elements heißen Isotope, abgeleitet von gr.: "isos topos", was sinngemäß "gleicher Platz" (im Periodensystem) bedeutet. Isotope unterscheiden sich in der Atommasse und zeigen bei Kernreaktionen unterschiedliches Verhalten.

Identifiziert werden chemische Elemente über Nachweisreaktionen der Analytischen Chemie.

Die Atommassen der Isotope sind "annähernd", aber nicht genau ganzzahlige Vielfache der Masse des Wasserstoffatoms. Die Ursache für diese unter 0,9 Prozent liegenden Abweichungen sind:
Die letzten beiden Effekte kompensieren einander nur teilweise.

Chemische Elemente, die in ihren natürlichen Vorkommen nur eine Sorte von Atomen aufweisen, heißen Reinelemente; wenn sie dagegen aus zwei oder mehr Isotopen bestehen, heißen sie Mischelemente. Die meisten Elemente sind Mischelemente. Es existieren 19 "stabile" und drei "langlebige instabile" Reinelemente (Bismut, Thorium und Plutonium), insgesamt also 22 Reinelemente.

Vom Wasserstoff existieren beispielsweise drei Isotope: Protium (keine Neutronen), Deuterium (1 Neutron), Tritium (2 Neutronen). Der Kern des am häufigsten vorkommenden "Wasserstoffnuklids" (99,9851 % Protium) besteht aus einem einzelnen Proton. Wasserstoff mit einem Proton und einem Neutron im Atomkern (Deuterium) tritt in natürlichem Wasserstoff nur mit einem Anteil von 0,0149 % auf, Tritium mit < 10 %.

Der Heliumatomkern besteht aus zwei Protonen und zwei Neutronen. Es gibt aber auch das Isotop Helium-3, dessen Kern nur ein Neutron enthält. Es ist in natürlichem Helium nur mit einem Anteil von 0,000137 % vorhanden.

Chlor (17 Protonen) besteht aus einer Mischung aus Isotopen mit 18 Neutronen (75,8 %) und 20 Neutronen (24,2 %).

Im Periodensystem steht für Mischelemente die durchschnittliche Atommasse gemäß relativer Häufigkeit der Isotope, die darüber hinaus um den Massendefekt korrigiert wird. Das natürliche Mischverhältnis ist bei einem Element meist konstant; bei einigen Elementen kann die Isotopenzusammensetzung aber lokal schwanken. Blei zum Beispiel kann unterschiedliche durchschnittliche Atommassen aufweisen, je nachdem, aus welcher Lagerstätte es stammt. 2010 beschloss deshalb die IUPAC, dass zukünftig für die Elemente Wasserstoff, Bor, Lithium, Kohlenstoff, Stickstoff, Sauerstoff, Silicium, Schwefel, Chlor und Thallium ein "Massenbereich im Periodensystem" anzugeben ist.

Die Begriffe "Reinstoff" und "Reinelement", sowie "Stoffgemisch" und "Mischelement" sind strikt zu unterscheiden.

Chemische Elemente können, bis auf einige Edelgase, chemische Verbindungen eingehen. Dabei sind mehrere der elementaren Atome zu Molekülen oder Ionenkristallen zusammengeschlossen.

Elemente können eine Verbindung mit anderen Elementen oder auch mit sich selbst eingehen:
Bei vielen Gasen wie "Chlor" Cl oder "Fluor" F verbinden sich zwei Atome desselben Elements untereinander zu einem Molekül, hierbei Cl und F. Sauerstoff bildet neben O auch weniger stabile dreiatomige O-Moleküle aus, Schwefel bildet ringförmige aus "sechs bis acht" Atomen.
Gewöhnliches Wasser (Summenformel: HO) ist hingegen eine Verbindung aus den Elementen Wasserstoff H (2 Atome pro Molekül) und Sauerstoff (1 Atom pro Molekül).

Grundsätzlich gibt es drei Arten von chemischen Verbindungen zwischen den Atomen der Elemente:

Bereits beim Urknall entstanden die "leichten" Elemente Wasserstoff (ca. 75 %) und Helium (ca. 25 %), zusammen mit geringen Mengen Lithium und Beryllium.
Am Anfang der Kosmochemie steht daher der Wasserstoff mit einer relativen Atommasse von ca. 1,0 u (ein Proton). "Schwerere" Elemente entstehen im Universum durch Kernreaktionen in den Sternen. In Hauptreihen-Sternen, wie unserer Sonne, verschmelzen unter hoher Temperatur (mehrere Millionen Grad Celsius) und hohem Druck beispielsweise vier Wasserstoffatomkerne über mehrere Zwischenstufen zu einem Heliumatomkern (relative Atommasse ca. 4,0 u). Dieser ist ein wenig leichter als die vier Protonen zusammen, die Massendifferenz wird als Energie frei.

Diese Fusion (Atome mit geringerer Protonenzahl verschmelzen zu höheren) geht in den meisten Sternen bis zur Entstehung von Kohlenstoff, in "massereichen" bis zur Bildung von Eisen weiter, dem am "dichtesten" gepackten Atomkern. Dies erfolgt immer unter "Abgabe" von Energie, wobei die Energieausbeute mit zunehmender Ordnungszahl der gebildeten Elemente bis zum Eisen immer geringer wird. Die Fusionsreaktionen zu "schwereren" Kernen würden eine "Zufuhr" von Energie erfordern.

Schwerere Elemente als Eisen entstehen deshalb nicht durch "Kernfusion", sondern durch Neutroneneinfang bestehender Atome, die dabei in Elemente höherer Ordnungszahl umgewandelt werden. Dies geschieht bei massearmen Sternen im sogenannten s-Prozess, bei "massereichen" am Ende der Lebenszeit von Sternen während einer Supernova im r-Prozess.

Die entstandenen Elemente gelangen (kontinuierlich durch Sonnenwind oder explosiv in einer Supernova) in das interstellare Medium und stehen für die Bildung der nächsten "Sterngeneration" oder anderen astronomischen Objekten zur Verfügung. Jüngere "Sternensysteme" enthalten daher bereits von Anfang an geringe Mengen "schwererer Elemente", die Planeten wie in unserem Sonnensystem bilden können.

Von den 118 bekannten Elementen (Stand 2015) sind 80 stabil. Alle stabilen Elemente kommen auf der Erde natürlich vor, ebenso 14 radioaktive (siehe Elementhäufigkeit). Weitere radioaktive Elemente wurden "künstlich" hergestellt, ihre Zahl wird vermutlich weiter steigen.

Die Elemente lassen sich nach verschiedenen Kriterien unterteilen. Am häufigsten ist die Unterteilung in solche Elemente, die Metalle bilden und den Großteil der Elemente ausmachen, sowie in Nichtmetalle und die Zwischenstufe Halbmetalle.

Zur Gruppe der Nichtmetalle gehören nur 17 aller Elemente, diese bilden bei Standardbedingungen keine Metalle. Davon liegen die sechs Edelgase einatomig vor, weil deren Atome keine Moleküle bilden, d. h. nicht miteinander reagieren. Dagegen verbinden sich andere mit Atomen des gleichen Elements zu Molekülen. Dazu zählen die weiteren fünf unter Normalbedingungen gasförmigen Elemente: Wasserstoff (H), Stickstoff (N), Sauerstoff (O), Fluor (F) und Chlor (Cl) sowie das flüssige Brom (Br).

Die Häufigkeit der chemischen Elemente unterscheidet sich je nach dem betrachteten Bereich.

Im Universum ist sie eng verknüpft mit den Entstehungsprozessen im kosmologischen Zeitrahmen (Nukleosynthese). Dort ist das weitaus häufigste Element der Wasserstoff, gefolgt von seinem einfachsten "Fusionsprodukt" Helium, die beide schon bald nach dem "Urknall" entstanden. Die nächsthäufigsten Elemente sind "Kohlenstoff" und "Sauerstoff". Lithium, Beryllium und Bor entstanden ebenfalls beim Urknall, jedoch in wesentlich geringeren Mengen.

Helium, Kohlenstoff und Sauerstoff sowie alle anderen Atomsorten wurden durch "Kernfusion" in Sternen oder durch andere astrophysikalische Vorgänge gebildet. Dabei entstanden häufiger Atome mit "gerader Protonenzahl", wie Sauerstoff, Neon, Eisen oder Schwefel, während Elemente mit "ungerader Protonenzahl" seltener sind. Diese Regel wird nach dem US-amerikanischen Chemiker William Draper Harkins (1873–1951) "Harkinssche Regel" genannt. Markant ist die besondere Häufigkeit des Eisens als Endpunkt der möglichen "Kernfusion" in Sternen.

Die Verteilung auf der Erde unterscheidet sich von derjenigen, die im gesamten Universum vorherrscht. Insbesondere sind auf der Erde vergleichsweise geringe Mengen Wasserstoff und Helium vorhanden, weil diese Gase vom Schwerefeld der Erde nicht festgehalten werden können; im Sonnensystem befinden sie sich vor allem in den Gasplaneten wie Jupiter und Neptun. Auf Gesteinsplaneten wie der Erde überwiegen die schwereren Elemente, vor allem Sauerstoff, Silicium, Aluminium und Eisen.

Organismen bestehen hauptsächlich aus "Wasserstoff", "Sauerstoff", "Kohlenstoff" und "Stickstoff".

In dem jeweils betrachteten Bereich sehr häufig vorkommende Elemente bezeichnet man als Mengenelemente, sehr seltene als Spurenelemente.





</doc>
<doc id="902" url="https://de.wikipedia.org/wiki?curid=902" title="Chemiker">
Chemiker

Ein Chemiker ist ein Naturwissenschaftler, der sich mit Themen aus der Chemie befasst. Die Tätigkeitsbezeichnung Chemiker ist nicht geschützt. Hingegen ist der akademische Grad Diplom-Chemiker (Dipl.-Chem.) staatlich geschützt und setzt ein Hochschulstudium mit erfolgreich bestandenem Diplom voraus. Mit der Abschaffung der Diplomstudiengänge im Zuge des Bologna-Prozesses ersetzt der Bachelor- bzw. der Master-Grad den Diplom-Grad bei neu erworbenen Abschlüssen als Berufsbezeichnung zunehmend.

In der Bundesrepublik Deutschland ist an etwa 50 Hochschulen das Studium der Chemie möglich. Diplomstudiengänge beginnen mit einem viersemestrigen Grundstudium, das mit der nicht berufsqualifizierenden Vordiplom-Prüfung abgeschlossen wird. An das Grundstudium schließt das Hauptstudium an. Es folgen die meist mündlichen Diplomprüfungen und die sechs- bis neunmonatige Diplomarbeit. Das Studium besteht aus Vorlesungen, Seminaren und Übungen, Klausuren und mündlichen Prüfungen sowie den regelmäßigen lehrveranstaltungsbegleitenden Praktika an der Universität. In den Praktika werden handwerkliche Fähigkeiten und das wissenschaftliche, systematische Arbeiten erlernt. Die Leistungsnachweise (Scheine) werden vor allem durch Klausuren, die mündliche Prüfungen und die Testate für bestandene Praktika erbracht.

Im Zuge des Bologna-Prozesses werden bestehende Diplomstudiengänge nach und nach in sechssemestrige Bachelor-Studiengänge mit anschließendem, optionalem viersemestrigem Master-Studium übergehen. Dieser Prozess soll bis 2018 abgeschlossen sein. Daneben haben sich mehrere Ingenieurstudiengänge mit Schwerpunkt Chemie etabliert.

Nach dem Universitätsabschluss kann nach einer meist mehrjährigen Doktorarbeit die Promotion zum Doktor der Naturwissenschaften (Dr. rer. nat.) erfolgen. Bei technischer Ausrichtung des Promotionsthemas und einer entsprechend absolvierten Universitätsausbildung ist auch der Doktor der Ingenieurwissenschaften (Dr.-Ing.) möglich. Die Promotion wird von der Mehrheit der in einem Absolventenjahrgang in Deutschland diplomierten Chemiker begonnen.

Die Dauer richtet sich vor allem danach, ob der Doktorand während seiner Tätigkeit nur seine Promotionsziele verfolgen kann oder durch zusätzliche Verpflichtungen eingebunden wird, wie z. B. die Einbeziehung in das Schreiben von Drittmittelanträgen für neue Projekte, den Einsatz in der Lehre an der Universität oder die Übernahme von Verwaltungsaufgaben am Lehrstuhl des betreuenden Professors. Schwer vergleichbar wird die Promotionsdauer auch dadurch, dass ein Teil der Doktoranden die erfolgreichen Aufgabenstellungen von vorherigen Doktoranden nach deren Promotion weiterbearbeitet und dabei Konzept und Aufbau ihrer Vorgänger weiterbenutzt, während ein anderer Teil der Doktoranden absolut neue Themen erstmals zu bearbeiten versucht. Die Bezahlung des Doktoranden erfolgt in der Regel nach dem TVöD bei nicht voller wöchentlicher Arbeitszeit (meist 50 % oder 2/3) oder durch ein Stipendium.

Die abgeschlossene Promotion soll den Nachweis zur selbständigen Forschungstätigkeit, also der wissenschaftlichen Erarbeitung und Bearbeitung eines Themas erbringen. Das beinhaltet eine weitestgehend individuelle Versuchsplanung, den Versuchsaufbau und die Versuchsdurchführung einschließlich der Ergebnisauswertung bis zur Ergebnispublikation (Dissertationsschrift) mit Einordnung in den wissenschaftlichen Kontext.

Die Promotion ist erforderlich für Tätigkeiten in der Forschung an Universitäten, in der Industrie oder in Forschungsinstituten wie z. B. der Helmholtz-Gemeinschaft Deutscher Forschungszentren, der Max-Planck-Gesellschaft, der Fraunhofer-Gesellschaft oder auch der Leibniz-Gemeinschaft.

Promovierte Diplom-Chemiker mit dem Berufsziel des Hochschullehrers schließen in der Regel eine bis zu sechs Jahren andauernde Juniorprofessur oder eine Habilitation an die Promotion an. Eine weitere Möglichkeit zum Erlangen von zusätzlichen Erfahrungen und zur Erweiterung der Publikationsliste bieten Post-Doc-Stellen im Inland und im Ausland. Sie dienen vor allem dem Sammeln der von den Einstellenden der Industrie gewünschten „Auslandserfahrung“ und zum „Sprachkenntnis-“ und „Flexibilitätsnachweis“ oder schlicht zum Überbrücken der Arbeitslosigkeit nach der Promotion bei schlechter Stellenlage.

Die Gesellschaft Deutscher Chemiker (GDCh), die Fachorganisation der Chemiker in Deutschland, hat über 27.000 Mitglieder. Die Gesellschaft Österreichischer Chemiker (GÖCH) verfügt über etwa 1.900 Mitglieder. Die Schweizerische Chemische Gesellschaft (SCG) hatte Anfang 2016 2.700 Mitglieder.

Für diejenigen, die sich mit der "Chemie" beschäftigten, die früher auch "Chymie" genannt wurde, wurden verschiedene Begriffe nebeneinander und synonym verwendet: Im 17. Jahrhundert waren das die Bezeichnungen "Chymicus", "Chemicus", "Chemiker" und "Chemist". Auch im 18. Jahrhundert und in der ersten Hälfte des 19. Jahrhunderts wurden die Begriffe "Chymicus", "Chymiker", "Chymist", "Chemicus" und "Chemist" benutzt und in der Regel mit „Misch- und Scheidekünstler“ erklärt. Johann Wolfgang von Goethe nutzte die Begriffe "Chemist", "Chemiker" und – seltener – "Chemicus"; ebenso verwendete Johann Bartholomäus Trommsdorff abwechselnd "Chemiker" oder "Chemist"/"Chemisten". Im Laufe des 19. Jahrhunderts setzte sich die Bezeichnung „"Chemiker"“ durch. Das hängt auch damit zusammen, dass sich in vielen analogen Fällen (wie "Akademiker" und "Botaniker") die Bildungen mit der Endung -"iker" durchgesetzt haben und dass bei den -"ist"-Bildungen eine Endbetonung vorliegt, während die -"iker"-Form auf der vorletzten Silbe betont wird.

In Deutschland führte Johann Bartholomäus Trommsdorff nach 1800 einen systematischen Chemieunterricht durch, der sich aber vor allem an Pharmazeuten richtete. 1824 erhielt Justus Liebig eine Professur in Gießen und bildete dort systematisch Chemiker aus.

1877 erschien in Deutschland die erste Ausgabe der Chemiker-Zeitung, 1887 in Österreich die Österreichische Chemiker-Zeitung, was zeigt, dass der Beruf des Chemikers zu diesem Zeitpunkt etabliert war.



</doc>
<doc id="905" url="https://de.wikipedia.org/wiki?curid=905" title="Chinook">
Chinook

Chinook (Aussprache [] oder []; Chinook-Wort für „warmer Fallwind“ oder „starker Wind“) steht für:
Orte in den Vereinigten Staaten:
im NRHP gelistete Objekte:


</doc>
<doc id="906" url="https://de.wikipedia.org/wiki?curid=906" title="Carl Friedrich Gauß">
Carl Friedrich Gauß

Johann Carl Friedrich Gauß (latinisiert "Carolus Fridericus Gauss;" * 30. April 1777 in Braunschweig; † 23. Februar 1855 in Göttingen) war ein deutscher Mathematiker, Astronom, Geodät und Physiker.
Wegen seiner überragenden wissenschaftlichen Leistungen galt er bereits zu seinen Lebzeiten als "Princeps Mathematicorum" („Fürst der Mathematiker; Erster unter den Mathematikern“).

Mit 18 Jahren entwickelte Gauß die Grundlagen der modernen Ausgleichungsrechnung und der mathematischen Statistik (Methode der kleinsten Quadrate), mit der er 1801 die Wiederentdeckung des ersten Asteroiden Ceres ermöglichte. Auf Gauß gehen die nichteuklidische Geometrie, zahlreiche mathematische Funktionen, Integralsätze, die Normalverteilung, erste Lösungen für elliptische Integrale und die Gaußsche Krümmung zurück. 1807 wurde er zum Universitätsprofessor und Sternwartendirektor in Göttingen berufen und später mit der Landesvermessung des Königreichs Hannover betraut. Neben der Zahlen- und der Potentialtheorie erforschte er u. a. das Erdmagnetfeld.

Bereits 1856 ließ der König von Hannover Gedenkmedaillen mit dem Bild von Gauß und der Inschrift prägen. Da Gauß nur einen Bruchteil seiner Entdeckungen veröffentlichte, erschloss sich der Nachwelt die Tiefgründigkeit und Reichweite seines Werks in vollem Umfang erst, als 1898 sein Tagebuch entdeckt und ausgewertet wurde und als der Nachlass bekannt wurde.

Nach Gauß sind viele mathematisch-physikalische Phänomene und Lösungen benannt, mehrere Vermessungs- und Aussichtstürme, zahlreiche Schulen, außerdem Forschungszentren und wissenschaftliche Ehrungen wie die Carl-Friedrich-Gauß-Medaille der Braunschweigischen Akademie und die festliche Gauß-Vorlesung, die jedes Semester an einer deutschen Hochschule stattfindet.

Carl Friedrich war das einzige Kind der Eheleute Gebhard Dietrich Gauß (1744–1808) und Dorothea Gauß geborene Benze (1743–1839). Die Mutter Dorothea war die Tochter eines Steinmetzen aus Velpke, der früh starb, und wurde als klug, von heiterem Sinn und festem Charakter geschildert. Gauß hatte zeitlebens eine enge Beziehung zu seiner Mutter, die zuletzt bei ihm auf der Sternwarte in Göttingen wohnte. Sie arbeitete zunächst als Dienstmädchen, bevor sie die zweite Frau von Gebhard Dietrich Gauß wurde. Dieser hatte viele Berufe, er war unter anderem Gärtner, Schlachter, Maurer, Kaufmannsassistent und Schatzmeister einer kleinen Versicherungsgesellschaft. Einige Anekdoten besagen, dass bereits der dreijährige Carl Friedrich seinen Vater bei der Lohnabrechnung korrigierte. Später sagte Gauß von sich selbst, er habe das Rechnen vor dem Sprechen gelernt. Sein Leben lang behielt er die Gabe, selbst komplizierteste Rechnungen im Kopf durchzuführen.

Eine Anekdote, deren Ursprung auf die Erzählungen von Wolfgang Sartorius von Waltershausen zurückgeht, beschreibt das frühe mathematische Talent des kleinen Carl Friedrich:

Im Alter von sieben Jahren sei Gauß in die Volksschule gekommen. Als er neun Jahre alt war, habe sein Lehrer Büttner den Schülern zur längeren Beschäftigung die Aufgabe gestellt, die Zahlen von 1 bis 100 zu addieren. Gauß habe sie allerdings nach kürzester Zeit gelöst, indem er 50 Paare mit der Summe 101 gebildet (1 + 100, 2 + 99, …, 50 + 51) und 5050 als Ergebnis erhalten habe. Er legte die Antwort mit den Worten in Braunschweiger Plattdeutsch „Ligget se“ (svw: „Hier liegt sie“) dem Lehrer auf den Tisch.

Die daraus resultierende Formel wird gelegentlich als „der kleine Gauß“ bezeichnet. Gauß’ Lehrer Büttner erkannte und förderte seine außergewöhnliche mathematische Begabung, indem er (u. a.) ein besonderes Rechenbuch aus Hamburg für ihn beschaffte und, unterstützt von seinem Assistenten Martin Bartels, dafür sorgte, dass Gauß 1788 das Catharineum besuchen konnte.

Als der „Wunderknabe“ Gauß vierzehn Jahre alt war, wurde er dem Herzog Karl Wilhelm Ferdinand von Braunschweig bekanntgemacht. Dieser unterstützte ihn sodann finanziell. So konnte Gauß von 1792 bis 1795 am Collegium Carolinum studieren, das zwischen höherer Schule und Hochschule anzusiedeln ist und der Vorgänger der heutigen Technischen Universität in Braunschweig ist. Dort war es der Professor Eberhard August Wilhelm von Zimmermann, der sein mathematisches Talent erkannte, ihn förderte und ihm ein väterlicher Freund wurde.

Im Oktober 1795 wechselte Gauß an die Universität Göttingen. Dort hörte er bei Christian Gottlob Heyne Vorlesungen über klassische Philologie, die ihn damals genauso wie die Mathematik interessierte. Letztere wurde durch Abraham Gotthelf Kästner, der zugleich Dichter war, repräsentiert. Bei Georg Christoph Lichtenberg hörte er im Sommersemester 1796 Experimentalphysik und sehr wahrscheinlich im folgenden Wintersemester Astronomie. In Göttingen schloss er Freundschaft mit Wolfgang Bolyai.

Im Alter von 18 Jahren gelang es Gauß als Erstem, die Konstruierbarkeit des regelmäßigen Siebzehnecks zu beweisen, und zwar auf Basis einer rein algebraischen Überlegung – eine sensationelle Entdeckung, denn seit der Antike hatte es auf diesem Gebiet kaum noch Fortschritte gegeben. Danach konzentrierte er sich auf das Studium der Mathematik, das er 1799 mit seiner Doktorarbeit an der Academia Julia, der Universität in Helmstedt, abschloss. Die Mathematik war hier durch Johann Friedrich Pfaff – der sein Doktorvater wurde – vertreten, und nicht zuletzt legte Gauß’ Gönner, der Herzog von Braunschweig, Wert darauf, dass Gauß nicht an einer „ausländischen“ Universität promoviert werden sollte.

Im November 1804 verlobte er sich mit Johanna Elisabeth Rosina Osthoff (* 8. Mai 1780; † 11. Oktober 1809), der Tochter eines Weißgerbers aus Braunschweig, und heiratete sie am 9. Oktober 1805. Am 21. August 1806 wurde in Braunschweig ihr erstes Kind Joseph († 4. Juli 1873) geboren, benannt nach Giuseppe Piazzi, dem Entdecker des Zwergplaneten Ceres, dessen Wiederauffindung Gauß’ Bahnberechnung 1801 ermöglicht hatte.

Nachdem die Familie nach Göttingen gezogen war, wurden am 29. Februar 1808 die Tochter Wilhelmine, genannt Minna, und am 10. September 1809 der Sohn Louis geboren. Am 11. Oktober 1809 starb Johanna Gauß an den Folgen der Geburt, Louis selbst starb am 1. März 1810. Durch diese Ereignisse fiel Gauß eine Zeit lang in eine Depression, in der er eine 1927 von seinem Enkel Carl August Gauß (1849–1927) gefundene auf tränenbedecktem Papier geschriebene „Totenklage“ auf seine verstorbene Frau verfasste. Carl August Gauß war sein einziger in Deutschland geborener Enkel, Gutsbesitzer (Gut Lohnde bei Hannover) und Sohn von Joseph. Wilhelmine heiratete den Orientalisten Heinrich Ewald, der später als einer der Göttinger Sieben das Königreich Hannover verließ und Professor in Tübingen wurde.

Am 4. August 1810 heiratete der Witwer, der zwei kleine Kinder zu versorgen hatte, Friederica Wilhelmine Waldeck (genannt "Minna;" * 15. April 1788; † 12. September 1831), Tochter des Göttinger Rechtswissenschaftlers Johann Peter Waldeck, die die beste Freundin seiner verstorbenen Frau gewesen war. Mit ihr hatte er drei Kinder. Eugen, zerstritt sich als Student der Rechte mit seinem Vater, wanderte 1830 nach Amerika aus, wo er als Kaufmann lebte und die „First National Bank“ von St. Charles gründete. Wilhelm folgte Eugen 1837 nach Amerika nach und brachte es ebenfalls zu Wohlstand. Seine jüngste Tochter Therese führte ihrem Vater nach dem Tod der Mutter bis zu seinem Tod den Haushalt. Minna Gauß war nach 13-jähriger Leidenszeit an Tuberkulose verstorben.

Nach seiner Promotion lebte Gauß in Braunschweig von dem kleinen Gehalt, das ihm der Herzog zahlte, und arbeitete an seinem Werk "Disquisitiones Arithmeticae."

Einen Ruf an die Petersburger Akademie der Wissenschaften lehnte Gauß aus Dankbarkeit gegenüber seinem Gönner, dem Herzog von Braunschweig, und wohl in der Hoffnung, dass dieser ihm eine Sternwarte in Braunschweig bauen würde, ab. Nach dem plötzlichen Tod des Herzogs nach der Schlacht bei Jena und Auerstedt wurde Gauß im November 1807 Professor an der Georg-August-Universität Göttingen und Direktor der dortigen Sternwarte. Dort musste er Lehrveranstaltungen halten, gegen die er aber eine Abneigung entwickelte. Die praktische Astronomie wurde dort durch Karl Ludwig Harding vertreten, den mathematischen Lehrstuhl hatte Bernhard Friedrich Thibaut inne. Mehrere seiner Studenten wurden einflussreiche Mathematiker, darunter Richard Dedekind und Bernhard Riemann.

In fortgeschrittenem Alter beschäftigte er sich zunehmend mit Literatur und war ein eifriger Zeitungsleser. Seine Lieblingsschriftsteller waren Jean Paul und Walter Scott. Er sprach fließend Englisch und Französisch und las, neben seiner Vertrautheit mit den klassischen Sprachen der Antike aus seiner Jugendzeit, mehrere moderne europäische Sprachen (Spanisch, Italienisch, Dänisch, Schwedisch), wobei er zuletzt noch Russisch lernte und sich versuchsweise mit Sanskrit befasste, was ihm aber nicht zusagte.

1808 wurde er zum korrespondierenden und 1820 zum auswärtigen Mitglied der Bayerischen Akademie der Wissenschaften sowie 1822 in die American Academy of Arts and Sciences gewählt.
1838 erhielt er die Copley-Medaille der Royal Society. 1842 wurde er in die Friedensklasse des Ordens Pour le Mérite aufgenommen. Im selben Jahr lehnte er einen Ruf nach Wien ab. 1845 wurde er Geheimer Hofrat und 1846 zum dritten Mal Dekan der Philosophischen Fakultät. 1849 feierte er sein Goldenes Doktorjubiläum und wurde Ehrenbürger von Braunschweig und Göttingen. 1852 unternahm er seine letzte wissenschaftliche Arbeit, die Wiederholung des Foucaultschen Pendelversuchs zum Nachweis der Erdrotation.

Er sammelte numerische und statistische Daten aller Art und führte zum Beispiel Listen über die Lebenserwartung berühmter Männer (in Tagen gerechnet). So schrieb er am 7. Dezember 1853 an seinen Freund und Kanzler seines Ordens Alexander von Humboldt u. a.: „Es ist übermorgen der Tag, wo Sie, mein hochverehrter Freund, in ein Gebiet übergehen, in welches noch keiner der Koryphäen der exacten Wissenschaften eingedrungen ist, der Tag, wo Sie dasselbe Alter erreichen, in welchem Newton seine durch 30.766 Tage gemessene irdische Laufbahn geschlossen hat. Und Newtons Kräfte waren in diesem Stadium gänzlich erschöpft: Sie stehen zur höchsten Freude der ganzen wissenschaftlichen Welt noch im Vollgenuss Ihrer bewundernswürdigen Kraft da. Mögen Sie in diesem Genuss noch viele Jahre bleiben.“ Gauß interessierte sich für Musik, besuchte Konzerte und sang viel (ob er ein Instrument spielte, ist nicht bekannt). Er befasste sich mit Aktienspekulation und hinterließ so bei seinem Tod ein beträchtliches Vermögen von 170.000 Talern (bei einem Professoren-Grundgehalt von 1000 Talern jährlich) überwiegend in Wertpapieren, darunter vielfach von Eisenbahnen. Hierzu findet sich eine der wenigen Stellen im Briefwechsel, in denen er sich kritisch zur Politik und mit dieser kooperierenden Banken äußert, denn von ihm erworbene Eisenbahnaktien von Hessen-Darmstadt verloren drastisch an Wert, als bekannt wurde, dass die Eisenbahn jederzeit verstaatlicht werden konnte.

Er war noch gegen Ende seines Lebens wissenschaftlich aktiv und hielt 1850/51 Vorlesungen über die Methode der kleinsten Quadrate. Zwei seiner bedeutendsten Schüler, Bernhard Riemann (der bei Gauß 1851 promoviert wurde und Gauß 1854 mit seinem Habilitationsvortrag über die Grundlagen der Riemannschen Geometrie stark beeindruckte) und Richard Dedekind, hatte er erst gegen Ende seiner Laufbahn.

Gauß war sehr konservativ und monarchistisch eingestellt, die Revolution von 1848 hieß er nicht gut.

Gauß litt in seinen letzten Jahren an Herzinsuffizienz (diagnostiziert als "Wassersucht") und an Schlaflosigkeit. Im Juni 1854 reiste er mit seiner Tochter Therese zur Baustelle der Eisenbahnlinie von Hannover nach Göttingen, wobei die vorüberfahrende Eisenbahn die Pferde scheuen ließ und die Kutsche umwarf, der Kutscher wurde schwer verletzt, Gauß und seine Tochter blieben unverletzt. Gauß nahm noch an der Einweihung der Eisenbahnlinie am 31. Juli 1854 teil, danach war er durch Krankheit zunehmend auf sein Haus eingeschränkt. Er starb am 23. Februar 1855 morgens um 1 Uhr 05 in Göttingen in seinem Lehnstuhl. Er wurde dort auf dem Albani-Friedhof begraben.

Gauß misstraute bereits mit zwölf Jahren der Beweisführung in der elementaren Geometrie und ahnte mit sechzehn Jahren, dass es neben der euklidischen noch eine andere, nicht-euklidische Geometrie geben muss.

Diese Arbeiten vertiefte er in den 1820er Jahren: Unabhängig von János Bolyai und Nikolai Iwanowitsch Lobatschewski bemerkte er, dass das euklidische Parallelenaxiom nicht denknotwendig ist. Seine Gedanken zur nichteuklidischen Geometrie veröffentlichte er jedoch nicht, vermutlich aus Furcht vor dem Unverständnis der Zeitgenossen. Als ihm sein Studienfreund Wolfgang Bolyai, mit dem er korrespondierte, allerdings von den Arbeiten seines Sohnes János Bolyai berichtete, lobte er ihn zwar, konnte es aber nicht unterlassen zu erwähnen, dass er selbst schon sehr viel früher darauf gekommen war („[die Arbeit Deines Sohnes] loben hiesse mich selbst loben“). Er habe darüber nichts veröffentlicht, da er „das Geschrei der Böotier scheue“. Lobatschewskis Arbeiten fand Gauß so interessant, dass er noch in fortgeschrittenem Alter Russisch lernte, um sie zu studieren.

Mit 18 Jahren entdeckte er einige Eigenschaften der Primzahlverteilung und fand die "Methode der kleinsten Quadrate," bei der es darum geht, die Summe der Quadrate von Abweichungen zu minimieren, ohne zunächst etwas darüber zu publizieren. Nachdem Adrien-Marie Legendre 1805 seine „Méthode des moindres carrés“ in einer Abhandlung veröffentlicht hatte und Gauß seine Ergebnisse erst 1809 bekannt machte, entstand daraus ein Prioritätsstreit.

Nach dieser Methode lässt sich etwa das wahrscheinlichste Ergebnis für eine neue Messung aus einer genügend großen Zahl vorheriger Messungen ermitteln. Auf dieser Basis untersuchte er später Theorien zur Berechnung von Flächeninhalten unter Kurven (numerische Integration), die ihn zur gaußschen Glockenkurve gelangen ließen. Die zugehörige Funktion ist bekannt als die Dichte der Normalverteilung und wird bei vielen Aufgaben zur Wahrscheinlichkeitsrechnung angewandt, wo sie die (asymptotische, das heißt für genügend große Datenmengen gültige) Verteilungsfunktion von zufällig um einen Mittelwert streuenden Daten ist. Gauß selbst machte davon unter anderem in seiner erfolgreichen Verwaltung der Witwen- und Waisenkasse der Göttinger Universität Gebrauch. Er stellte über mehrere Jahre eine gründliche Analyse an, in der er zu dem Schluss kam, dass die Pensionen leicht erhöht werden konnten. Damit legte Gauß auch Grundlagen in der Versicherungsmathematik.

Als 19-Jähriger führte er 1796, bei Betrachtungen über die Bogenlänge auf einer Lemniskate in Abhängigkeit von der Entfernung des Kurvenpunktes zum Ursprung, mit den lemniskatischen Sinusfunktionen die historisch ersten, heute so genannten elliptischen Funktionen ein. Seine Notizen darüber hat er jedoch nie veröffentlicht. Diese Arbeiten stehen in Zusammenhang mit seiner Untersuchung des arithmetisch-geometrischen Mittels. Die eigentliche Entwicklung der Theorie der elliptischen Funktionen, den Umkehrfunktionen der schon länger bekannten elliptischen Integrale, erfolgte durch Niels Henrik Abel (1827) und Carl Gustav Jacobi.

Gauß erfasste früh den Nutzen komplexer Zahlen, so in seiner Doktorarbeit von 1799, die einen Beweis des Fundamentalsatzes der Algebra enthält. Dieser Satz besagt, dass jede algebraische Gleichung mit Grad größer als null mindestens eine reelle oder komplexe Lösung besitzt. Den älteren Beweis von Jean-Baptiste le Rond d’Alembert kritisierte Gauß als ungenügend, aber auch sein eigener Beweis erfüllt noch nicht die späteren Ansprüche an topologische Strenge. Gauß kam auf den Beweis des Fundamentalsatzes noch mehrfach zurück und gab neue Beweise 1815 und 1816.

Gauß kannte spätestens 1811 die geometrische Darstellung komplexer Zahlen in einer Zahlenebene (gaußsche Zahlenebene), die schon Jean-Robert Argand 1806 und Caspar Wessel 1797 gefunden hatten. In dem Brief an Bessel, in dem er dies mitteilt, wurde auch deutlich, dass er weitere wichtige Konzepte der Funktionentheorie wie das Kurvenintegral im Komplexen und den Cauchyschen Integralsatz kannte und erste Ansätze zu Perioden von Integralen. Er veröffentlichte darüber aber nichts bis 1831, als er in seinem Aufsatz zur Zahlentheorie "Theoria biquadratorum" den Namen "komplexe Zahl" einführte. In der Veröffentlichung der Begründung der komplexen Analysis war ihm inzwischen Augustin-Louis Cauchy (1821, 1825) zuvorgekommen. 1849 veröffentlicht er zu seinem Goldenen Doktorjubiläum eine verbesserte Version seiner Dissertation zum Fundamentalsatz der Algebra, in der er im Gegensatz zur ersten Version explizit komplexe Zahlen benutzt.

Am 30. März 1796, einen Monat vor seinem neunzehnten Geburtstag, bewies er die Konstruierbarkeit des regelmäßigen Siebzehnecks und lieferte damit die erste nennenswerte Ergänzung euklidischer Konstruktionen seit 2000 Jahren. Dies war aber nur ein Nebenergebnis bei der Arbeit für sein zahlentheoretisch viel weiterreichendes Werk "Disquisitiones Arithmeticae."

Eine erste Ankündigung dieses Werkes fand sich am 1. Juni 1796 im "Intelligenzblatt der allgemeinen Literatur-Zeitung" in Jena. Die 1801 erschienenen "Disquisitiones" wurden grundlegend für die weitere Entwicklung der Zahlentheorie, zu der einer seiner Hauptbeiträge der Beweis des quadratischen Reziprozitätsgesetzes war, das die Lösbarkeit von quadratischen Gleichungen „mod p“ beschreibt und für das er im Laufe seines Lebens fast ein Dutzend verschiedene Beweise fand. Neben dem Aufbau der elementaren Zahlentheorie auf modularer Arithmetik findet sich eine Diskussion von Kettenbrüchen und der Kreisteilung, mit einer berühmten Andeutung über ähnliche Sätze bei der Lemniskate und anderen elliptischen Funktionen, die später Niels Henrik Abel und andere anregten. Einen Großteil des Werks nimmt die Theorie der quadratischen Formen ein, deren Geschlechtertheorie er entwickelt.

Es finden sich aber noch viele weitere tiefliegende Resultate, oft nur kurz angedeutet, in diesem Buch, die die Arbeit späterer Generationen von Zahlentheoretikern in vielfältiger Weise befruchteten. Der Zahlentheoretiker Peter Gustav Lejeune Dirichlet berichtete, er habe die Disquisitiones sein Leben lang bei der Arbeit stets griffbereit gehabt. Das Gleiche gilt für die beiden Arbeiten über biquadratische Reziprozitätsgesetze von 1825 und 1831, in denen er die gaußschen Zahlen einführt (ganzzahliges Gitter in komplexer Zahlenebene). Die Arbeiten sind wahrscheinlich Teil einer geplanten Fortsetzung der "Disquisitiones", die aber nie erschien. Beweise für diese Gesetze gab dann Gotthold Eisenstein 1844.

André Weil regte die Lektüre dieser Arbeiten (und einiger Stellen im Tagebuch, wo es in versteckter Form um Lösung von Gleichungen über endlichen Körpern geht) nach seinen eigenen Angaben zu seinen Arbeiten über die Weil-Vermutungen an. Gauß kannte zwar den Primzahlsatz, veröffentlichte ihn aber nicht.

Gauß förderte auf diesem Gebiet eine der ersten Mathematikerinnen der Neuzeit, Sophie Germain. Gauß korrespondierte mit ihr ab 1804 über Zahlentheorie, wobei sie sich erst eines männlichen Pseudonyms bediente. Erst 1806 gab sie ihre weibliche Identität preis, als sie sich nach der Besetzung Braunschweigs bei dessen französischem Kommandanten für seine Sicherheit verwendete. Gauß lobte ihre Arbeit und ihr tiefes Verständnis der Zahlentheorie und bat sie, ihm für sein Preisgeld, das er mit dem Lalande-Preis erhielt, 1810 in Paris eine genaue Pendeluhr zu besorgen.

Nach der Fertigstellung der "Disquisitiones" wandte sich Gauß der Astronomie zu. Anlass hierfür war die Entdeckung des Zwergplaneten Ceres durch Giuseppe Piazzi am 1. Januar 1801, dessen Position am Himmel der Astronom kurz nach seiner Entdeckung wieder verloren hatte. Der 24-jährige Gauß schaffte es, die Bahn mit Hilfe einer neuen indirekten Methode der Bahnbestimmung und seiner Ausgleichsrechnungen auf Basis der "Methode der kleinsten Quadrate" so zu berechnen, dass Franz Xaver von Zach ihn am 7. Dezember 1801 und – bestätigt – am 31. Dezember 1801 wiederfinden konnte. Heinrich Wilhelm Olbers bestätigte dies unabhängig von Zach durch Beobachtung am 1. und 2. Januar 1802.

Das Problem der Wiederauffindung der Ceres als solches lag darin, dass durch die Beobachtungen weder der "Ort," ein Stück der "Bahn," noch die "Entfernung" bekannt sind, sondern nur die "Richtungen" der Beobachtung. Dies führt auf die Suche einer Ellipse und nicht nach einem Kreis, wie ihn Gauß’ Konkurrenten ansetzten. Einer der Brennpunkte der Ellipse ist bekannt (die Sonne selbst), und die Bögen der Bahn der Ceres zwischen den Richtungen der Beobachtung werden nach dem zweiten Keplerschen Gesetz durchlaufen, das heißt, die Zeiten verhalten sich wie die vom Leitstrahl überstrichenen Flächen. Außerdem ist für die rechnerische Lösung bekannt, dass die Beobachtungen selbst von einem Kegelschnitt im Raum ausgehen, der Erdbahn selbst.

Im Grundsatz führt das Problem auf eine Gleichung achten Grades, deren triviale Lösung die Erdbahn selbst ist. Durch umfangreiche Nebenbedingungen und die von Gauß entwickelte "Methode der kleinsten Quadrate" gelang es dem 24-Jährigen, für die Bahn der Ceres für den 25. November bis 31. Dezember 1801 den von ihm berechneten Ort anzugeben. Damit konnte Zach am letzten Tag der Vorhersage Ceres wiederfinden. Der Ort lag nicht weniger als 7° (d. h. 13,5 Vollmondbreiten) östlich der Stelle, wo die anderen Astronomen Ceres vermutet hatten, was nicht nur Zach, sondern auch Olbers gebührend würdigte.

Diese Arbeiten, die Gauß noch vor seiner Ernennung zum Sternwarten-Direktor in Göttingen unternahm, machten ihn mehr noch als seine Zahlentheorie in Europa mit einem Schlag bekannt und verschafften ihm unter anderem eine Einladung an die Akademie nach Sankt Petersburg, deren Mitglied er 1802 wurde.

Die in diesem Zusammenhang von Gauß gefundene iterative Methode wird noch heute angewandt, weil sie es einerseits ermöglicht, alle bekannten Kräfte ohne erheblichen Mehraufwand in das physikalisch-mathematische Modell einzubauen, und andererseits computertechnisch einfach handhabbar ist.

Gauß beschäftigte sich danach noch mit der Bahn des Asteroiden Pallas, auf dessen Berechnung die Pariser Akademie ein Preisgeld ausgesetzt hatte, konnte die Lösung jedoch nicht finden. Seine Erfahrungen mit der Bahnbestimmung von Himmelskörpern mündeten jedoch 1809 in seinem Werk "Theoria motus corporum coelestium in sectionibus conicis solem ambientium."

In der Potentialtheorie und Physik ist der gaußsche Integralsatz (1835, veröffentlicht erst 1867) grundlegend. Er identifiziert in einem Vektorfeld das Integral der Divergenz (Ableitungsvektor angewandt auf das Vektorfeld) über ein Volumen mit dem Integral des Vektorfeldes über die Oberfläche dieses Volumens.

Auf dem Gebiet der Geodäsie sammelte Gauß zwischen 1797 und 1801 die ersten Erfahrungen, als er dem französischen Generalquartiermeister Lecoq bei dessen Landesvermessung des Herzogtums Westfalen als Berater zur Seite stand. 1816 wurde sein ehemaliger Schüler Heinrich Christian Schumacher vom König von Dänemark mit der Durchführung einer Breiten- und Längengradmessung in dänischem Gebiet beauftragt. Im Anschluss daran erhielt Gauss von 1820 bis 1826 die Leitung der Landesvermessung des Königreichs Hannover („gaußsche Landesaufnahme“), wobei ihm zeitweise sein Sohn Joseph assistierte, der in der hannoverschen Armee als Artillerieoffizier tätig war. Diese Vermessung setzte die dänische auf hannoverschem Gebiet nach Süden fort, wobei Gauß die von Schumacher gemessene Braaker Basis mitbenutzte. Durch die von ihm erfundene Methode der kleinsten Quadrate und die systematische Lösung umfangreicher linearer Gleichungssysteme (gaußsches Eliminationsverfahren) gelang ihm eine erhebliche Steigerung der Genauigkeit. Auch für die praktische Durchführung interessierte er sich: Er erfand als Messinstrument das über Sonnenspiegel beleuchtete Heliotrop.

In diesen Jahren beschäftigte er sich – angeregt durch die Geodäsie und die Karten-Theorie – mit der Theorie der Differentialgeometrie der Flächen, führte unter anderem die gaußsche Krümmung ein und bewies sein "Theorema egregium." Dieses besagt, dass die gaußsche Krümmung, die durch die Hauptkrümmungen einer Fläche im Raum definiert ist, allein durch Maße der inneren Geometrie, d. h. durch Messungen innerhalb der Fläche, bestimmt werden kann. Daher ist die gaußsche Krümmung unabhängig von der Einbettung der Fläche in den dreidimensionalen Raum, sie ändert sich also bei längentreuen Abbildungen von Flächen aufeinander nicht.
Wolfgang Sartorius von Waltershausen berichtet, Gauß habe bei Gelegenheit der Hannoverschen Landesvermessung empirisch nach einer Abweichung der Winkelsumme besonders großer Dreiecke vom euklidischen Wert 180° gesucht – wie etwa bei dem von Gauß gemessenen planen Dreieck, das vom Brocken im Harz, dem Inselsberg im Thüringer Wald und dem Hohen Hagen bei Dransfeld gebildet wird (aufgrund der Größe der Erde beträgt der Winkelexzess in diesem Dreieck dennoch nur 0,25 Winkelminuten). Die oben erwähnte Vermutung zur Motivation ist Gegenstand von Spekulationen. Max Jammer schrieb über diese gaußsche Messung und ihr Ergebnis:
Zusammen mit Wilhelm Eduard Weber arbeitete er ab 1831 auf dem Gebiet des Magnetismus. Gauß erfand mit Weber das Magnetometer und verband so 1833 seine Sternwarte mit dem physikalischen Institut. Dabei tauschte er über elektromagnetisch beeinflusste Kompassnadeln Nachrichten mit Weber aus: die erste Telegrafenverbindung der Welt. Mit ihm zusammen entwickelte er das CGS-Einheitensystem, das 1881 auf einem internationalen Kongress in Paris zur Grundlage der elektrotechnischen Maßeinheiten bestimmt wurde. Er organisierte ein weltweites Netz von Beobachtungsstationen (Magnetischer Verein), um das erdmagnetische Feld zu vermessen.

Gauß fand bei seinen Experimenten zur Elektrizitätslehre 1833 vor Gustav Robert Kirchhoff (1845) die Kirchhoffschen Regeln für Stromkreise.

Von ihm stammt die Gaußsche Osterformel zur Berechnung des Osterdatums und er entwickelte auch eine Pessach-Formel.

Gauß arbeitete auf vielen Gebieten, veröffentlichte seine Ergebnisse jedoch erst, wenn eine Theorie seiner Meinung nach komplett war. Dies führte dazu, dass er Kollegen gelegentlich darauf hinwies, dieses oder jenes Resultat schon lange bewiesen zu haben, es wegen der Unvollständigkeit der zugrundeliegenden Theorie oder der ihm fehlenden, zum schnellen Arbeiten nötigen Unbekümmertheit nur noch nicht präsentiert zu haben.

Bezeichnenderweise besaß Gauß ein Petschaft, das einen von wenigen Früchten behangenen Baum und das Motto zeigte. Einer Anekdote zufolge lehnte er es Bekannten, die Gauß’ umfangreiche Arbeiten kannten oder ahnten, gegenüber ab, diesen Wahlspruch zu ersetzen, z. B. durch , da nach seinem eigenen Bekunden er lieber eine Entdeckung einem anderen überließ, als sie nicht vollständig ausgearbeitet unter seinem Namen zu veröffentlichen. Das ersparte ihm Zeit in den Bereichen, die Gauß eher als Randthemen betrachtete, so dass er diese Zeit auf seine originäre Arbeit verwenden konnte.

Der wissenschaftliche Nachlass von Gauß wird in den Spezialsammlungen der Niedersächsischen Staats- und Universitätsbibliothek Göttingen aufbewahrt.

Nach seinem Tod wurde das Gehirn von Gauß entnommen. Es wurde mehrfach, zuletzt 1998, mit verschiedenen Methoden untersucht, aber ohne einen besonderen Befund, der seine mathematischen Fähigkeiten erklären würde. Es befindet sich heute separat, in Formalin konserviert, in der "Abteilung für Ethik und Geschichte der Medizin" der Medizinischen Fakultät der Universität Göttingen.

Im Herbst 2013 wurde an der Universität Göttingen eine Verwechslung aufgedeckt: Die zu diesem Zeitpunkt über 150 Jahre alten Gehirnpräparate des Mathematikers Gauß und des Göttinger Mediziners Conrad Heinrich Fuchs sind – wahrscheinlich schon bald nach der Entnahme – vertauscht worden. Beide Präparate wurden in der Anatomischen Sammlung der Göttinger Universitätsklinik in Gläsern mit Formaldehyd aufbewahrt. Das Originalgehirn von Gauß befand sich im Glas mit der Aufschrift „C. H. Fuchs“, und das Fuchs-Gehirn war etikettiert mit „C. F. Gauss“. Damit sind die bisherigen Untersuchungsergebnisse über das Gehirn von Gauß obsolet. Die Wissenschaftlerin Renate Schweizer befasste sich wegen der vom vermeintlichen Gehirn von Gauß angefertigten MRT-Bilder, die eine seltene Zweiteilung der Zentralfurche zeigten, erneut mit den Präparaten und entdeckte, dass diese Auffälligkeit in Zeichnungen, die kurz nach Gauß’ Tod erstellt wurden, fehlte.

Von Gauß entwickelte Methoden oder Ideen, die seinen Namen tragen, sind:


Methoden und Ideen, die teilweise auf seinen Arbeiten beruhen, sind:

Zu seinen Ehren benannt sind:




In den Bänden 10 und 11 finden sich ausführliche Kommentare von Paul Bachmann (Zahlentheorie), Ludwig Schlesinger (Funktionentheorie), Alexander Ostrowski (Algebra), Paul Stäckel (Geometrie), Oskar Bolza (Variationsrechnung), Philipp Maennchen (Gauß als Rechner), Harald Geppert (Mechanik, Potentialtheorie), Andreas Galle (Geodäsie), Clemens Schaefer (Physik) und Martin Brendel (Astronomie). Herausgeber war zuerst Ernst Schering, dann Felix Klein.





Zu den zahlreichen auf Anleitung von Gauß aufgestellten Vermessungssteinen gehören:

Von Gauß gibt es relativ viele Bildnisse, unter anderem:

Belletristik:




</doc>
<doc id="908" url="https://de.wikipedia.org/wiki?curid=908" title="Containerschiff">
Containerschiff

Ein Containerschiff ist ein Schiffstyp, der für den Transport von ISO-Containern ausgelegt ist.

Die Ladungskapazität von Containerschiffen wird in TEU ("Twenty-foot Equivalent Units", vgl. Tonnage) angegeben und entspricht der Anzahl von 20-Fuß-Containern, die geladen werden können. Üblich sind auch 40-Fuß-Container (gemessen in FEU wie "Forty-foot Equivalent Unit"), seit Mitte der 1990er Jahre ebenso 45-, 48- und 53-Fuß-Container sowie die seltener anzutreffenden 30-Fuß-Container, die allerdings an Deck geladen werden müssen, da die "Cellguides" (Führungsschienen in der Vertikalen) nur für 40-Fuß-Container ausgelegt sind. Für sehr große bzw. schwere Stückgüter existieren auch so genannte "flat racks", Open-Top-Container oder "platforms", die im Verbund mit Standard-Containern geladen werden können.

Bis zu einer Ladungskapazität von 3400 TEU besitzen Containerschiffe teilweise eigenes Ladegeschirr, Schiffe mit höheren Frachtkapazitäten benötigen meist spezielle Containerbrücken an Containerterminals, die am jeweiligen Hafen zur Verfügung stehen müssen. Die Tendenz hin zu immer größeren Containerschiffen bewirkt eine steigende Konzentration der möglichen Anlaufpunkte für Containerschiffe auf relativ wenige, zentrale Containerhäfen, über die ein Großteil des Seehandels abläuft. Diese Häfen werden zu Hubs; von und nach dort fahren kleinere Containerschiffe, z. B. Feederschiffe (siehe auch Umladeproblem - ein Optimierungsproblem aus dem Bereich der Logistik).

Heutzutage werden rund 90 % der Stückgüter des Welthandels mit Containerschiffen transportiert. Anfang Juli 2014 waren weltweit 5072 Containerschiffe mit zusammengerechnet 17,6 Mio. TEU vorhanden, davon kamen 107 Schiffe erst 2014 in Fahrt. Die Nachfrage nach Transportleistungen schwankt während des Jahres und je nach Wirtschaftslage. z. B. werden im zweiten Halbjahr wegen des Weihnachtsgeschäfts deutlich mehr Güter transportiert als im ersten.

Seit der Finanzkrise 2008 macht anhaltender Preisdruck auf die Container-Frachtraten den Reedereien schwer zu schaffen. Anfang 2017 waren nach den Erkenntnissen der Analysten des Maritime Strategies International (MSI) weltweit 260 Containerschiffe mit zusammen 297.000 TEU ohne Beschäftigung, was 4,8 % der weltweiten Flotte entspricht (Auflieger). Noch im Oktober 2016 hatte die Zahl der beschäftigungslosen Containerschiffe die negative Rekordhöhe von 1,591 Mio. TEU erreicht. Damit schien der Höhepunkt der Krise erreicht. Seitdem wurde über eine Erholung des Marktes innerhalb der kommenden fünf Jahre bis 2022 spekuliert, allerdings nur für den Fall, dass wesentlich mehr Schiffe als bisher verschrottet werden und die Zahl der Neubau-Aufträge deutlich zurückgeht. Das ist bisher nicht absehbar. Im Gegenteil, Makler sprachen 2016 trotz neuer, kosteneffektiver Großallianzen unter den Reedereien von einer ruhigen Marktlage und einem „abflauenden Aufwind“. 2017 sollen 193 neue Schiffe mit insgesamt 1,32 Mio. TEU auf den Markt kommen. Demgegenüber wird 2017 voraussichtlich nur eine Kapazität von 700.000 TEU zum Abwracken verkauft und damit aus dem Markt ausscheiden. 2018 werden weitere 186 Schiffe mit 1,49 Mio. TEU neu in Betrieb genommen und voraussichtlich 500.000 TEU verschrottet. Damit wird 2017 die weltweite Flotte um 3,1 % und 2018 um weitere 4,7 % wachsen. Wie sich das auf die Frachtraten auswirkt, hängt auch vom Verhalten der Reedereien ab. So kündigte Hapag-Lloyd Ende 2016 an, weitere Charterschiffe zurückzugeben und damit sein Angebot zu reduzieren. 

Das Containerschiff entstand in den 1950er Jahren in den USA. Nach der 1955 in Dienst gestellten "Clifford J. Rogers" mit noch sehr kleinen Containern folgte 1956 der umgebaute Tanker "Ideal X" des Speditionsunternehmers Malcolm McLean. Dieser fing damit an, die Aufliegergehäuse von Sattelschleppern ohne Fahrgestell über größere Seestrecken mit dem Schiff zu befördern. 1960 gründete McLean die Reederei Sea-Land Corporation. Schon in der ersten Hälfte der 1960er Jahre entstanden als Semicontainerschiffe geplante Neubauten, wie die 1963 in Dienst gestellte Tobias Mærsk, 1964 wurde in Australien mit der "Kooringa" der erste als Vollcontainerschiff für ISO-Container geplante Neubau in Betrieb genommen.

Mitte der 1960er Jahre gab es in den Vereinigten Staaten bereits 171 (allerdings nahezu alles umgebaute) Containerschiffe. 1966 lief erstmals in Deutschland das Containerschiff "Fairland" der Reederei Sea-Land in Bremen ein. Schon am 31. Juli 1968 waren weltweit 102 Semi- oder Vollcontainerschiffe beauftragt oder in Bau. Ab 1968 begann die Umstellung der wichtigsten Liniendienste auf den Containerverkehr, zunächst im Nordatlantikverkehr (zwischen USA/Ostküste und Westeuropa), ab Oktober 1968 der Transpazifikdienst zwischen Japan–USA/Westküste. Hier wurde von der NYK Line die "Hakone Maru" eingesetzt. Ende 1968 wurden vom Bremer Vulkan mit der "Weser Express" für den Norddeutschen Lloyd sowie von Blohm + Voss, Hamburg, mit der "Elbe Express" für die HAPAG die ersten Containerschiffe (je 750 TEU) in Deutschland gebaut. Sie kamen mit den Schwesterschiffen "Rhein Express" und "Mosel Express" auf der Nordatlantik-Route in Betrieb. Ebenfalls 1968 setze die Hamburger Reederei August Bolten mit der "Bärbel Bolten" (140 TEU) ein weiteres Vollcontainerschiff unter deutscher Flagge ein.

Am 1. Juli 1970 betrug der weltweite Bestand an Semi- und Vollcontainerschiffen 201 Einheiten (davon 154 Vollcontainerschiffe), im Jahr darauf betrug der Bestand an Vollcontainerschiffen 231 Einheiten. 1969 erfolgte die Umstellung des Liniendienstes Europa–Australien/Neuseeland auf den Containerverkehr, Ende 1971 Europa–Fernost, im Mai 1977 Europa–Südafrika sowie Europa–Karibik/Golf von Mexiko. 1981 folgte die Route Südafrika–Fernost (Safari-Dienst). Damit war die Umstellung der wichtigsten Linienverbindungen auf den Containerverkehr abgeschlossen.

1984 bot die Reederei United States Lines erstmals einen in östliche Richtung laufenden "Round the World Service" an. Dieser mit zwölf Schiffen der American-New-York-Klasse betriebene Dienst endete nach sechs Monaten mit dem Konkurs der Reederei. Ein im gleichen Jahr von der Evergreen Marine aus Taiwan via Panamakanal und Sueskanal mit jeweils zwölf Schiffen in beiden Richtungen gestarteter Dienst wurde etwa 1999 wieder aufgegeben, da ein Linienverkehr von Punkt A nach B effizienter ist. In den 1990er Jahren ging auch die deutsche Senator Lines mit einem "Round the World Service" an den Start, stellte ihn jedoch zu Gunsten eines "Pendulum Service" ein. 

Als Containermaße haben sich 20 bzw. 40 ft. Länge, 8 ft. Breite und 8 ft. 6 in. Höhe international durchgesetzt. Das alte von Sea Land eingeführte Containermaß von 35 ft. ist weggefallen, vielmehr kommen heute vermehrt auch 40-ft.- und 45-ft.-"High-Cube"-Container zum Einsatz, innerhalb der USA auch 53-Fuß-Container, da dort längere Sattelzüge als in Europa zulässig sind. 

Containerschiffe werden in Generationen eingeteilt.

Die Größe der 1968 gebauten Containerschiffe war die Maßeinheit für ein Schiff der 1. Generation.

Anfang 1969 erschien mit der "Encounter Bay" das erste Schiff der 2. Generation, die fast alle eine maximale Schiffsbreite von 30,5 m aufweisen, das heißt, an Deck können maximal zwölf Container nebeneinander gestaut werden.

Lange Zeit lag die Obergrenze der Abmessungen von Containerschiffen bei 275 m Länge und 32,3 m Breite, damit sie den Panamakanal durchfahren konnten. Schiffe dieser Größe wurden früher als 3. Generation bezeichnet.

Seit etwa 1988 bezeichnet man als "Panamax" die Schiffe, die auch die maximale Länge von Panamakanalschleusen (294 Meter) nutzen (und nicht nur die Breite).

In der Anfangsphase (etwa 1972) waren maximal 3000 TEU die Obergrenze des technisch Umsetzbaren, 1988 waren es 4300 TEU. Spätere „Panamax“-Neubauten konnten bis zu 5060 TEU laden.

Für größere Schiffstypen mit mehr als 32,3 Meter Breite ist der Name "Post-Panamax" gebräuchlich. Schiffe mit über 7000 TEU werden (Stand 2011) als "Super-Post-Panamax-" oder "Post-Panamax-Plus"-Schiffe bezeichnet, die über 11.000 TEU als "New Panamax".

Die ersten Containerschiffe, die breiter als 32,3 m (Panamakanalschleusen) waren, sind die fünf Schiffe der President-Truman-Klasse der American President Lines (USA). Sie wurden 1988 von der Werft Bremer Vulkan (Vegesack) und HDW (Kiel) gebaut und nur im Transpazifikdienst der Reederei eingesetzt. Sie sind 275,0 m ü.a. lang und 38,5 m breit bei 61.296 BRZ, 53.613 tdw und können maximal 4400 TEU befördern. An Deck werden maximal 15 Container nebeneinander gestaut.

1991 wurde von der Daewoo Heavy Industries die "CGM Normandie" für die französische CGM (heutige CMA CGM) gebaut. Sie hatte eine Kapazität von 4410 TEU und war das erste Post-Panamax-Schiff im Europa–Fernost Dienst. Auch hier konnten bei einer Schiffsbreite von 38,0 m 15 Container nebeneinander an Deck platziert werden. 1992 folgte die "Bunga Pelangi" für die Reederei MISC (Malaysia International Shipping Corporation Berhad) mit ähnlichen Abmessungen.

1994/1995 folgten die "Nedlloyd Hongkong" und "Nedlloyd Honshu" als erste und einzige Post-Panamax-Open-Top-Schiffe für Royal Nedlloyd. Aus Japan folgten ab Dezember 1994 drei Schiffe der "NYK-Altair"-Klasse für die NYK Line, fünf baugleiche Schiffe für die Mitsui O.S.K. Lines und die "OOCL-California"-Klasse der Reederei OOCL (Hongkong), die erstmals mit 40 Metern Breite auf Deck 16 Container nebeneinander stauen konnten.

1995 wurden für American President Lines (APL) weitere sechs Post-Panamax-Schiffe (C11-Klasse) gebaut, jeweils drei bei HDW (Kiel) und Daewoo Heavy Industries. APL hatte zu dieser Zeit (bis Anfang 1999) mit insgesamt elf solchen Schiffen die größte Post-Panamax-Flotte.

Ab 1996 kam mit dem Regina-Mærsk-Typ die erste Baureihe der Very Large Container Ships (VLCS) in Fahrt. Es waren die ersten 42,8 Meter breiten (sie können 17 Containerreihen nebeneinander stauen) und ersten über 300 Meter langen Containerschiffe. Mit einer Stellplatzkapazität von 7000 TEU waren die Schiffe um mindestens 50 % größer als die bisherigen Rekordhalter – einen solchen Größensprung hatte es bis dahin noch nicht gegeben.

Projekte und fertige Konstruktionsentwürfe von Klassifizierungsgesellschaften oder/und Bauwerften für einen als Suezmax-Containerschiff bezeichneten Typ für bis zu 14.000 TEU gibt es seit etwa 1996. Seit dem Ausbau des Suezkanals können jedoch auch größere Schiffe wie die Emma-Mærsk-Klasse den Suezkanal passieren. Auch sind Entwürfe für ein Malaccamax-Containerschiff für 21.000 TEU bereits durchgerechnet. Hierbei blieben jedoch die Begrenzungen bei den Abfertigungskapazitäten und die Tiefgangsbeschränkungen in den Containerhäfen unberücksichtigt.

Die deutsche Hapag-Lloyd AG hielt sehr lange ausschließlich am Panamax-Schiffstyp fest und begann als letzte der größeren Container-Reedereien erst im Jahr 2001 mit dem Neubau eines ersten Post-Panamax-Schiffs, der "Hamburg Express".

2005 wurde mit der "MSC Pamela" das erste Containerschiff mit 45,6 Metern Breite in Dienst gestellt, hier können erstmals 18 Container nebeneinander gestaut werden.

Die Odense-Werft realisierte ab September 2006 mit der "Emma-Mærsk-Klasse" einen sehr großen Containerschiffstyp mit einer Tragfähigkeit von 14.770 TEU. Er kommt trotz seiner Größe mit einer Schiffsschraube aus; sie wird angetrieben von einem 14-Zylinder Wärtsilä RT-flex 96 C-B Zweitakt-Motor mit weit über 80 MW Leistung. Die acht Schiffe der "Emma-Mærsk"-Klasse, die einheitlich mit „E“ beginnende Mærsk-Namen tragen, können bei 56,4 m Breite 22 Container nebeneinander auf Deck laden. Sie sind 397 m lang und haben einen Maximaltiefgang von 16 m. Bis zu elf Containerlagen übereinander werden im Rumpf des Schiffes, darüber maximal neun Lagen an Deck gestapelt. Es gibt Anschlüsse für 1000 Kühlcontainer.

Im Jahr 2008 wurde die "MSC Daniela" in Dienst genommen. Sie ist im Dienst der Mediterranean Shipping Company. Mit einer Länge von 366 m und Breite von 51,2 m ist der vom Germanischen Lloyd klassifizierte „Megaboxer“ für den neuen Schleusenkanal des Panamakanals ausgelegt und kann 13.800 TEU transportieren. Das Typschiff der MSC-Daniela-Klasse leitete den Bau einer Serie von Schwesterschiffen ein, zu der auch die – allerdings anders motorisierte – CMA CGM Christophe Colomb gehört.

Maersk bestellte im Februar 2011 zehn 18.270-TEU-Schiffe der Triple-E-Klasse mit einer Option auf 20 weitere Schiffe. Im Juni 2011 wurde die Option auf den Bau von weiteren zehn Schiffen eingelöst, wodurch sich die Bauorder auf 20 Schiffe erhöhte. Sie sind mit zwei Hauptmaschinen ausgestattet. Ihre Maximalgeschwindigkeit beträgt 23 Knoten (die der Emma-Maersk-Klasse 25 Knoten). Auch sind die Schiffe darauf ausgelegt, bei niedrigerer Geschwindigkeit treibstoffsparend betrieben zu werden (Slow steaming).

"Triple E" steht für "economy of scale, energy efficiency and environmentally improved", also für wirtschaftlich durch Größe, energieeffizient und umweltfreundlicher. Ende 2012 fuhren weltweit 163 Containerschiffe mit einer Kapazität von mehr als 10.000 TEU, inklusive der 20 Triple-E's von Maersk waren 120 weitere bestellt. Das erste dieser Schiffe hatte am 23. Februar 2013 in Korea seinen "semi-launch".

Anfang März 2015 gab die japanische Reederei "Mitsui O.S.K. Lines" die ersten Einheiten mit über 20.000 Stellplätzen in Auftrag. Der MOL 20.000-TEU-Typ kam 2017 in Fahrt. Nach Auffassung von Schifffahrtsexperten ist damit das „Ende der Fahnenstange“ erreicht , andere Experten erwarten in wenigen Jahren Schiffe mit 22.000 TEU und mehr. Eine Studie der OECD ergab 2015, dass die Kostenersparnis in der Ultra-Large-Klasse sehr gering ist. Außerdem sei das wirtschaftliche Risiko enorm hoch, da sich die größten Schiffe nur rechnen, wenn sie wirklich voll beladen sind. Letztlich profitierten nur noch „koreanische Werften“ von „XXL-Schiffen“.

Anmerkungen

Die zur jeweiligen Zeit größten Containerschiffe der Welt. Bei den Schiffen einer baugleichen Baureihe wird nur das erste Schiff der Baureihe aufgelistet. Aktueller Rekordhalter sowie die aktuellen Höchstwerte in Fettdruck


Containerschiffe mit mehr als 50 Kühlcontainern werden häufig auch als Kühlcontainerschiffe bezeichnet. Der Markt der Kühlcontainer nimmt stark zu und stellt eine ernsthafte Konkurrenz zu den Kühlschiffen dar. 1972 begann United Fruit (heute Chiquita) mit dem Befördern von Bananen in Kühlcontainern. Einer der Gründe dafür ist, dass schnelleres Löschen des Schiffes im Hafen möglich wird. Dafür werden auch lukenlose Schiffe eingesetzt, um direkt mit einem Kran an die Container zu gelangen. Die ersten lukenlosen Kühlcontainerschiffe wurden im Jahr 1999 von HDW hergestellt und boten Stellplätze für 990 TEU Kühlcontainer sowie 33 TEU ungekühlte Container. Diese Schiffe sind für Dole im Einsatz und bedienen den Fruchttransport von Mittelamerika in die USA. Heute gehört Hamburg Süd zu einer der größten Reedereien, die sich auf den Transport von Kühlcontainern von und nach Südamerika spezialisiert haben.

Die Schiffe der „Monte“-Klasse mit 5500 TEU und „Rio“-Klasse mit 5905 TEU der Hamburg Süd sind die Containerschiffe mit der größten Kühlkapazität. Sie haben 1365 Anschlüsse für Kühlcontainer, das sind Stellplätze für rund 2500 TEU Kühlcontainer an und unter Deck.

Seit 1990 werden lukendeckellose Containerschiffe gebaut. Das sind Schiffe, deren Laderäume keine Lukendeckel (bzw. nur auf erstem und zweitem Laderaum hinter dem Wellenbrecher) haben, wodurch die Be- und Entladezeiten verringert werden sowie das Gewicht der Lukendeckel eingespart wird. Ein spezieller Bug gegen hohe Wellen und ein leistungsfähiges Pumpsystem sind dafür nötig. Das weltweit erste Schiff dieser Bauart war die Bell Pioneer. Die bisher einzigen großen Containerschiffe in dieser Bauweise betrieb Nedlloyd, später Royal P&O Nedlloyd NV, jetzt Mærsk mit den fünf 1991/1992 gebauten Panamax-Schiffen vom Ultimate-Container-Carrier-Typ, sowie den zwei weltweit ersten Postpanamax-Open-Top-Schiffen "Nedlloyd Hongkong" und "Nedlloyd Honshu", Baujahr 1994.

Aus Gründen der Schiffsfestigkeit (Torsion), der Schiffssicherheit und der Wirtschaftlichkeit ist man jedoch wieder davon abgekommen, Open-Top-Schiffe mit mehr als 1000 TEU zu bauen. Dadurch, dass die Lukendeckel fehlen, muss höherfester Stahl eingesetzt und zusätzlich die Gurtung versteift werden, was aus wirtschaftlicher Sicht zu teuer ist. Außerdem ist ein hoher Freibord nötig, um eindringendes Seewasser zu minimieren. (Siehe die Feederschiffe der Werft Sietas, die auch nur im mittleren Laderaum keine Deckel mehr haben, dafür aber hochgezogene Lukenkumminge.)

Die Containerschiffe der 1. und 2. Generation hatten Einschraubenantrieb (Dampfturbine oder Dieselmotor). Die Schiffe der 3. Generation (Baujahre 1971–1981) waren anfangs für 27–28 Knoten konzipiert. Dafür waren Zweischraubenantriebe (Turbine oder Diesel) bzw. sogar Dreischraubenantriebe mit drei Dieselmotoren notwendig. Ein weiteres Konzept, das in dieser Generation von Containerschiffen erstmals umgesetzt wurde, waren die Gasturbinenschiffe des Euroliner-Typs. Dieser Antrieb, bisher nur von Militärschiffen bekannt, stellte sich jedoch, insbesondere nach der Ölkrise Anfang der 1970er Jahre, schnell als unwirtschaftlich heraus. Erste große Panamaxschiffe mit Einschraubenantrieb wurden ab Ende 1980 in Dienst gestellt, als erstmals leistungsfähige Dieselmotoren mit 50.000 PS zur Verfügung standen und somit auf die teureren Mehrschraubenantriebe verzichtet werden konnte. Seit Ende der 1970er Jahre wurden wegen der hohen Rohölpreise und des Verzichts auf die sehr hohe Geschwindigkeit von 28 Knoten fast alle turbinenangetriebenen Containerschiffe auf Dieselmotorantrieb umgebaut, da diese erheblich weniger Brennstoff verbrauchen. Auch gab es Umbauten hinsichtlich des Antriebs von 2 Turbinen / 2 Propellern auf 1 Turbine / 1 Propeller, wie zum Beispiel die vier Hapag-Lloyd-Schiffe vom Typ „Hamburg Express“. Die gängige Dienstgeschwindigkeit fast aller großen Containerschiffe lag dann bei 24, Ende der 2000er Jahre bei etwa 25 Knoten.

Die im Mai 2006 von der Volkswerft Stralsund gelieferte "Mærsk Boston" als Typschiff von sieben sehr schnellen Panamax-Containerschiffen erreicht mit einem 12-Zylinder-Sulzer-Diesel eine Dienstgeschwindigkeit von 29,2 Knoten und ist das schnellste Containerschiff der Welt. Die "Mærsk Boston" besitzt folgende Daten: Länge 294,1 m, Breite 32,18 m, Tiefgang 13,5 m, DWT 52400 t, 48853 BRZ, 4170 TEU.

Die bis zum Jahr 2005 größten eingebauten Dieselmotoren sind 12-Zylinder-Zweitakt-Reihenmotoren der Typen MAN-B&W 12K98ME/MC mit 69,1 MW bei 94–104/min bzw. von Wärtsilä-Sulzer 12RT-flex96C Common Rail mit 68,7 MW bei 100/min.

Ein Problem bei der Verwirklichung der über 12.000 TEU Containerschiffe ist die Antriebsanlage. Die Reeder wollten bis zur Krise im Jahr 2008 nur Containerschiffe mit einer Dauergeschwindigkeit von 25 Knoten (plus Reserven) um die Schiffe (besonders wenn sie im Liniendienst fahren) in bestehende Umläufe integrieren zu können. Dafür wurde bei der Emma-Mærsk-Klasse statt eines bisher weit verbreiteten 12-Zylinder-Reihen-Dieselmotors mit 90.000 oder 93.000 PS ein 14-Zylinder-Dieselmotor des Typs Wärtsilä/Sulzer 14RT-flex96C mit 108.908 PS Leistung eingebaut. Der Propeller hat ca. 10,0 m Durchmesser und wiegt etwa 130 Tonnen, um die größere Motorkraft bei gleicher Drehzahl des Motors (94–104/min) in Vortrieb umzusetzen. 
Angesichts hoher Kraftstoffkosten und einer seit Mitte 2008 andauernden Schifffahrtskrise fahren die meisten Schiffe inzwischen deutlich langsamer als vor Beginn der Krise ("Slow steaming"). Bei den neu in Auftrag vergebenen größten Containerschiffen der Maersk Line mit einer Kapazität von 18.000 TEU wurde die Maximalgeschwindigkeit auf 23 Knoten herabgesetzt und ein Zweischraubenantrieb verbaut. Bei bereits in Betrieb befindlichen Schiffen konnte sich sogar rentieren, die Bugwulst umzubauen und durch eine Form zu ersetzen, die für eine geringere Geschwindigkeit optimiert ist.
Die neuen Nasen sollen 1–2 % Kraftstoff sparen. Das derzeit größte Containerschiff, die Barzan, ist für eine normale flexible Betriebsgeschwindigkeit (operation speed) von nur noch 12 bis 18 Knoten ausgelegt. 
Große Containerschiffe mit über 7000 TEU werden auf folgenden Werften gebaut:

Odense Staalskibsværft (Odense Steel Shipyard), Dänemark (2012 geschlossen)


In der Anfangsphase der Containerisierung von 1968 bis 1977 wurde eine beträchtliche Zahl an großen Containerschiffen der 1. bis 3. Generation von deutschen Werften gebaut, die damals auf diesem Sektor führend waren:


Die derzeit größten bei einer deutschen Werft gebauten Containerschiffe wurden ab Herbst 2005 bei der Volkswerft Stralsund, Stralsund gebaut. Das Typschiff, die "Mærsk Boston" wurde am 24. März 2006 getauft und im Mai 2006 abgeliefert. Sie sind vom Typ VWS4000 und maßen 294,1 m Länge ü.a. und 32,18 m Breite bei einer Kapazität von 4250 TEU. Der Antrieb besteht aus einem Sulzer 12 RTA 96C Diesel mit 93.400 PS. Dadurch wird eine Dienstgeschwindigkeit von 29,2 Knoten erreicht, es sind die schnellsten Containerschiffe der Welt.

Containerschiffe der Größe 2500/2700 TEU (Typ CV 2500/2700) bauten HDW in Kiel, SSW in Bremerhaven, Nordseewerke in Emden, Blohm + Voss in Hamburg, Volkswerft in Stralsund, Aker Werften in Wismar und Warnemünde.

Seit Jahren führend in Konstruktion und Bau von Containerschiffen bis 1200 TEU Größe (Feederschiffe), ist die Werft J. J. Sietas in Hamburg gewesen. Seit Anfang 2006 baute diese Werft auch größere Containerschiffe, das erste 1700-TEU-Schiff wurde mit der "Safmarine Mbashe" abgeliefert. Inzwischen wurde dort der Bau von Containerschiffen eingestellt. Im November 2011 musste das Unternehmen Insolvenz anmelden.

"Stand: August 2016"

Die 20 größten Containerschiffs-Reedereien beherrschen etwa 85 % des Marktes.

Die deutsche Reederei "Hapag-Lloyd AG", von 1976 bis 1983 noch größte Containerschiffsreederei der Welt, war seit vielen Jahren nicht mehr unter den „Top 10“ der Rangliste. Im August 2005 wurde die Übernahme von CP Ships durch Hapag-Lloyd bekanntgegeben. Hierzu bedurfte es einer Kapitalerhöhung von einer Milliarde Euro. Hapag-Lloyd rückte damit auf Platz 6 der großen Container-Reedereien.

Am 11. Mai 2005 wurde offiziell bekanntgegeben, dass Mærsk-Sealand für 2,96 Milliarden US-Dollar (entspr. 2,3 Mrd. Euro) P&O Nedlloyd übernehmen wollte. Die P&O-Nedlloyd-Aktie war am 10. Mai mit 41 US-Dollar bewertet, Mærsk-Sealand bot den Aktionären in einem bis zum 5. August 2005 terminierten Übernahmeangebot 57 US-Dollar pro Aktie. Mitte August 2005 war die Übernahme abgeschlossen. Die P&O Nedlloyd existiert seit Februar 2006 nicht mehr. Sie wurde vollständig in die „Mærsk Line“ integriert. Durch die Übernahme erhöhte der Marktführer Mærsk seinen Marktanteil von 12 auf 18 Prozent am Weltcontainerverkehr.

Nach Nationalität der Eigner hatten folgende Staaten Containerschiffs-Flotten mit mehr als 1000 BRZ je Schiff (Anzahl der Schiffe / Kapazität in tausend Standardcontainern)

Stand: 31. Dezember 2009
Der UNCTAD Bericht Review of maritime Transport 2012 (Seite 42 ff.) enthält die Angaben die den Anteil der Schiffe in deutscher Hand belegen. Man muss unterscheiden zwischen Schiffseignern (= Eigentümern) und Schiffsbetreibern (Reedereien). So ist z. B. die Reederei Maersk (Stand 2009) Eigentümerin etwa der halben von ihr betriebenen Flotte; die andere Hälfte hat sie gechartert. Der große Anteil von Schiffen in deutscher Hand lässt sich erklären durch die Steuerbegünstigungen von Schiffsbeteiligungen. Siehe auch Schifffahrtskrise seit 2008. Im Jahr 2014 verfügt Deutschland im Bereich Containerschifffahrt über rd. 26 Prozent der weltweiten Containerschifffahrtskapazitäten (rd. 4,8 Mio. TEU).


1969 erfolgte auf der Route "Europa–Australien/Neuseeland" die Umstellung des Liniendienstes auf Containerverkehr mit Schiffen der zweiten Generation ("ANZECS"-Dienst). Er wurde durch Hapag-Lloyd, Deutschland, der Overseas Containers Limited (OCL), Großbritannien (ein Zusammenschluss von fünf bedeutenden englischen Linienreedereien), der Associated Container Transportation (ACT), Großbritannien, der Nedlloyd, Niederlande, der Australian National Line und der New Zealand Shipping Company gegründet.

Die wichtigste und ladungsstärkste Schifffahrtsverbindung ist die "Europa–Fernost"-Route. 1968 wurde von den großen Linienreedereien die Umstellung auf den Containerverkehr beschlossen.

Hier gab es ab 1971 die erste große Allianz, den "TRIO-Dienst" (von Reedereien aus drei Ländern gegründet). Die Umstellung auf Containerverkehr bedeutete für die damalige Zeit ein solch großes Investitionsvolumen, dass keine Reederei es alleine finanzieren konnte oder wollte. Es wurden hier die damals größten und schnellsten Containerschiffe der 3. Generation in der Zeit von November 1971 bis Juli 1973 in Fahrt gesetzt. Der TRIO-Dienst wurde durch die Reedereien NYK Line/Japan (3, ab 1976 4 Schiffe), Mitsui O.S.K. Lines/Japan (2, ab 1977 3 Schiffe), Hapag-Lloyd/Deutschland (4, ab 1981 5 Schiffe), Overseas Container Line (OCL)/Großbritannien (5, ab 1989 7 Schiffe; wurde später von P&O übernommen) und Ben Line-Ellerman/Großbritannien (3 Schiffe) gebildet. Der "TRIO"-Dienst wurde Anfang 1991 aufgelöst, wobei jedoch die Hapag-Lloyd AG und die NYK Line weiterhin bis heute mit anderen Reedereien zusammenarbeiten.

Die zweite Gruppe war ab 1972 der "Scandutch Service" der Reedereien Wilh. Wilhelmsen/Norwegen, Det Østasiatiske Kompagni (EAC)/Dänemark, Broström/Schweden und Nedlloyd/Niederlande. CGM/Frankreich trat 1973 bei und "Malaysian Intern. Shipping Co." (MISC) 1977. Auch diese Allianz wurde 1991 beendet.

Die dritte Allianz wurde 1975 gegründet mit dem "ACE-Dienst" (Asian Container Europe) der Reedereien K-Line/Japan, Orient Overseas Container Line (OOCL)/Hongkong, Neptune Orient Lines (NOL)/Singapur und Compagnie Maritime Belge (CMB)/Belgien.

Von 1991 bis 1996 gab es eine Allianz zwischen Mærsk Line und P&O. Von 1996 an arbeitete Marktführer Mærsk Line aber mit der amerikanischen Sea Land Corp. global zusammen. Die Ergänzung war so günstig, dass Mærsk die US-Reederei 1999 fast komplett übernommen hat.

1991 begannen die drei skandinavischen Reedereien sowie Ben Line und Ellermann den BEN-EAC-Dienst. Der BEN-EAC-Dienst wurde 1993 komplett von Mærsk-Line übernommen.

Von 1996 bis 2001 gab es die "Global Alliance" der Reedereien Hapag-Lloyd AG, NYK Line, NOL und P&O. Royal Nedlloyd trat nach der Fusion mit P&O 1997 bei.

1977 begann der Containerverkehr auf der Route "Europa–Südafrika", genannt "SAECS"-Dienst, gegründet durch die Reedereien Deutsche Afrika Linien (Hamburg), "Compagnie Maritime Belge", "Royal Nedlloyd", "Overseas Container Line" (später P&O) und Safmarine, Südafrika. Hier wurden neun moderne 2400 TEU Zweischrauben-Containerschiffe eingesetzt. Dieser Dienst wird auch heute noch von Mærsk Line, Safmarine, CGM (nur bis Ende der 1990er Jahre), und Deutsche Afrika Linie bedient, seit Februar 2006 ist Mitsui-OSK Lines noch hinzugetreten. Inzwischen werden moderne 4500–5000-TEU-Schiffe (Sling 1) sowie 1800-TEU-Schiffe (Sling 2) eingesetzt.

Reederei-Allianzen müssen von der amerikanischen Federal Maritime Commission anerkannt werden, außerdem gibt es die Wettbewerbsbehörde in Singapur.

Die drei größten Allianzen waren Mitte 2017:

Die Zahlen in Klammern geben den Stellenwert der jeweiligen Reederei im weltweiten Ranking an.

2008 (in diesem Jahr endete eine Boomzeit und eine mehrjährige Schifffahrtskrise begann) wurde ein Containerschiff durchschnittlich nach 27 Jahren verschrottet. 2015 wurde ein Containerschiff durchschnittlich nach 22 Jahren verschrottet.





</doc>
<doc id="909" url="https://de.wikipedia.org/wiki?curid=909" title="C (Programmiersprache)">
C (Programmiersprache)

C ist eine imperative und prozedurale Programmiersprache, die der Informatiker Dennis Ritchie in den frühen 1970er Jahren an den Bell Laboratories entwickelte. Seitdem ist sie eine der am weitesten verbreiteten Programmiersprachen.

Die Anwendungsbereiche von C sind sehr verschieden. Sie wird zur System- und Anwendungsprogrammierung eingesetzt. Die grundlegenden Programme aller Unix-Systeme und die Systemkernel vieler Betriebssysteme sind in C programmiert. Zahlreiche Sprachen, wie C++, Objective-C, C#, D, Java, JavaScript, PHP, Vala oder Perl, orientieren sich an der Syntax und anderen Eigenschaften von C.

Eine einfache Version des Hallo-Welt-Programms in C ist diejenige, die Ritchie und Kernighan selbst in der zweiten Auflage ihres Buches "The C Programming Language" verwendet haben.
/*
int main(void)

C wurde 1969–1973 von Dennis Ritchie in den Bell Laboratories für die Programmierung des damals neuen Unix-Betriebssystems entwickelt. Er stützte sich dabei auf die Programmiersprache B, die Ken Thompson und Dennis Ritchie in den Jahren 1969/70 geschrieben hatten – der Name C entstand als Weiterentwicklung von B. B wiederum geht auf die von Martin Richards Mitte der 1960er-Jahre entwickelte Programmiersprache BCPL zurück. Ritchie schrieb auch den ersten Compiler für C. 1973 war die Sprache so weit ausgereift, dass man nun den Unix-Kernel für die PDP-11 neu in C schreiben konnte.

K&R C erweiterte die Sprache um neue Schlüsselwörter wie codice_1 oder codice_2 und führte die von Mike Lesk entwickelte I/O-Standardbibliothek und auf Empfehlung von Alan Snyder den Präprozessor ein.

C ist eine Programmiersprache, die auf fast allen Computersystemen zur Verfügung steht. Um den Wildwuchs zahlreicher Dialekte einzudämmen, wurde C mehrfach standardisiert (C89/C90, C99, C11). Abgesehen vom Mikrocontrollerbereich, wo eigene Dialekte existieren, sind die meisten aktuellen PC-/Server-Implementierungen eng an den Standard angelehnt; eine vollständige Implementierung aktueller Standards ist aber selten. In den meisten C-Systemen mit Laufzeitumgebung steht auch die genormte C-Standard-Bibliothek zur Verfügung. Dadurch können C-Programme, die keine sehr hardware-nahe Programmierung enthalten, in der Regel gut auf andere Zielsysteme portiert werden.
Bis ins Jahr 1989 gab es keinen offiziellen Standard der Sprache. Seit 1978 galt hingegen das Buch "The C Programming Language" als informeller De-facto-Standard, welches Brian W. Kernighan und Dennis Ritchie im selben Jahr veröffentlicht hatten. Bezeichnet wird diese Spezifikation als K&R C.

Da in den folgenden Jahren die Zahl an Erweiterungen der Sprache ständig wuchs, man sich nicht auf eine gemeinsame Standard-Bibliothek einigen konnte und nicht einmal die UNIX-Compiler K&R C vollständig implementierten, wurde beschlossen, einen offiziellen Standard festzulegen. Nachdem dieser schließlich im Jahr 1989 erschienen war, blieb K&R C zwar noch für einige Jahre De-facto-Standard vieler Programmierer, verlor dann aber rasch an Bedeutung.

Im Jahr 1983 setzte das American National Standards Institute (ANSI) ein Komitee namens X3J11 ein, das 1989 seine Arbeit abschloss und die Norm "ANSI X3.159-1989 Programming Language C" verabschiedete. Diese Version der Sprache C wird auch kurz als ANSI C, Standard C oder C89 bezeichnet.

Ein Jahr später übernahm die International Organization for Standardization (ISO) den bis dahin rein amerikanischen Standard auch als internationale Norm, die "ISO/IEC 9899:1990", kurz auch als C90 bezeichnet. Die Namen C89 und C90 beziehen sich also auf dieselbe Version von C.

Nach der ersten Entwicklung durch ANSI und ISO, wurde der Sprachstandard für einige Jahre kaum geändert. Erst 1995 erschien das "Normative Amendment 1" zu C90. Es hieß "ISO/IEC 9899/AMD1:1995" und wird auch kurz als C95 bezeichnet. Neben der Korrektur einiger Details wurden mit C95 internationale Schriftsätze besser unterstützt.

Nach einigen kleineren Revisionen erschien im Jahr 1999 der neue Standard "ISO/IEC 9899:1999", kurz C99. Er war größtenteils mit C90 kompatibel und führte einige neue, teilweise von C++ übernommene Features ein, von denen einige bereits zuvor von verschiedenen Compilern implementiert worden waren. C99 wurde im Lauf der Jahre durch drei "Technical Corrigendas" ergänzt.

Im Jahr 2007 begann die Entwicklung eines neuen Standards mit dem inoffiziellen Arbeitstitel "C1X". Er wurde im Dezember 2011 veröffentlicht und ist in der Kurzform als C11 bekannt. Neben einer besseren Kompatibilität mit C++ wurden der Sprache wiederum neue Features hinzugefügt.

Seit dem ersten internationalen Standard C90 wird C von der internationalen Arbeitsgruppe ISO/IEC JTC1/SC22/WG14 weiterentwickelt. Die nationalen Standardisierungsorganisationen übernehmen die Veröffentlichungen des internationalen Standards in an ihre Bedürfnisse angepasster Form.

Trotz des eher hohen Alters ist die Sprache C auch heute weit verbreitet und wird sowohl im Hochschulbereich, wie auch in der Industrie und im Open-Source-Bereich verwendet.

Das Haupteinsatzgebiet von C liegt in der Systemprogrammierung einschließlich der Erstellung von Betriebssystemen und der Programmierung von eingebetteten Systemen. Der Grund liegt in der Kombination von erwünschten Charakteristiken wie Portabilität und Effizienz mit der Möglichkeit, Hardware direkt anzusprechen und dabei niedrige Anforderungen an die Laufzeitumgebung zu haben.

Auch Anwendungssoftware wird oft in C erstellt. Viele Programmierschnittstellen für Anwendungsprogramme und Betriebssystem-APIs werden in Form von C-Schnittstellen implementiert, zum Beispiel Win32. Gemäß C-Standard existieren jedoch keine Funktionen zur positionierten Ausgabe auf Displays, d. h. text- oder grafisch orientierte Benutzeroberflächen sind in reinem C nicht realisierbar. Es existieren jedoch zahlreiche Bibliotheken, die für das jeweilige Zielsystem eine solche Ausgabe ermöglichen.

Wegen der hohen Ausführungsgeschwindigkeit und geringen Codegröße werden Compiler, Programmbibliotheken und Interpreter anderer höherer Programmiersprachen (wie z. B. die Java Virtual Machine) oft in C implementiert.

C wird als Zwischencode einiger Implementierungen höherer Programmiersprachen verwendet. Dabei wird diese zuerst in C-Code übersetzt, der dann kompiliert wird. Dieser Ansatz wird entweder verwendet, um die Portabilität zu erhöhen (C-Compiler existieren für nahezu jede Plattform), oder aus Bequemlichkeit, da kein maschinenspezifischer Codegenerator entwickelt werden muss.
Einige Compiler, die C auf diese Art benutzen, sind Chicken, EiffelStudio, Esterel, PyPy, Sather, Squeak und Vala.

C wurde allerdings als Programmiersprache und nicht als Zielsprache für Compiler entworfen. Als Zwischensprache ist es daher eher schlecht geeignet. Das führte zu C-basierten Zwischensprachen wie C--.

C wird oft für die Erstellung von "Anbindungen" (engl. bindings) genutzt (zum Beispiel Java Native Interface). Diese Anbindungen erlauben es Programmen, die in einer anderen Hochsprache geschrieben sind, Funktionen aufzurufen, die in C implementiert wurden. Der umgekehrte Weg ist oft ebenfalls möglich und kann verwendet werden, um in C geschriebene Programme mit einer anderen Sprache zu erweitern (z. B. mod perl).

C besitzt eine sehr kleine Menge an Schlüsselwörtern. Die Anzahl der Schlüsselwörter ist so gering, weil fast alle Aufgaben, welche in anderen Sprachen über eigene Schlüsselwörter realisiert werden, über Funktionen der C-Standard-Bibliothek realisiert werden (zum Beispiel die Ein- und Ausgabe über Konsole oder Dateien, dynamische Speicherverwaltung, usw.).

In C89 gibt es 32 Schlüsselwörter:
Mit C99 kamen fünf weitere dazu:
Mit C11 kamen sieben weitere hinzu:

Zum Speichern eines Zeichens (sowie von kleinen Zahlen) verwendet man in C üblicherweise den Datentyp Character, geschrieben als codice_3.
Vom Computer tatsächlich gespeichert wird nicht das Zeichen (wie zum Beispiel „A“), sondern eine gleichbedeutende mindestens acht Bit lange Binärzahl (z. B. 01000001). Diese Binärzahl steht im Speicher und kann anhand einer Tabelle jederzeit automatisch in den entsprechenden Buchstaben umgewandelt werden, wobei der aktuelle Zeichensatz bzw. die Codepage der Systemumgebung entscheidend ist. Zum Beispiel steht 01000001 gemäß der ASCII-Tabelle für das Zeichen „A“.
Um auch Zeichen aus Zeichensätzen aufnehmen zu können, die mehr Zeichen umfassen als der relativ kleine ASCII-Zeichensatz, wurde mit codice_4 bald ein zweiter für Zeichen konzipierter Datentyp eingeführt.
char zeichen = 'A'; /* gespeichert wird nicht das Zeichen „A“, sondern meist ein Byte („01000001“) */
printf("%c", 65); /* gibt das Zeichen mit der Ordnungszahl 65 der aktuellen Codepage aus (bei ASCII ein „A“) */
Zum Speichern einer Ganzzahl (wie zum Beispiel 3) verwendet man eine Variable vom Datentyp Integer, geschrieben als codice_5. Die Größe eines Integers beträgt heutzutage (je nach Prozessorarchitektur und Betriebssystem) meist 32 Bit, oft aber auch schon 64 und manchmal noch 16 Bit. In 16 Bit lassen sich 65536 verschiedene Werte speichern. Um die Verwendung von negativen Zahlen zu ermöglichen, reicht der Wertebereich bei 16 Bit gewöhnlich von -32768 bis 32767. Werden keine negativen Zahlen benötigt, kann der Programmierer mit codice_6 aber einen vorzeichenlosen Integer verwenden. Bei 16 Bit großen Integern ergibt das einen Wertebereich von 0 bis 65535.

Um den Wertebereich eines Integers zu verkleinern oder zu vergrößern, stellt man ihm einen der Qualifizierer codice_7, codice_1 oder codice_9 voran. Das Schlüsselwort codice_5 kann dann auch weggelassen werden, so ist codice_1 gleichbedeutend mit codice_12. Um zwischen vorzeichenbehafteten und vorzeichenlosen Ganzzahlen zu wechseln, gibt es die beiden Qualifizierer codice_13 und codice_2. Für einen vorzeichenbehafteten Integer kann der Qualifizierer aber auch weggelassen werden, so ist codice_15 gleichbedeutend mit codice_5. Die C-Standard-Bibliothek ergänzt diese Datentypen über die plattformunabhängige Header-Datei codice_17 in der ein Set von Ganzzahltypen mit fester Länge definiert ist.
char ganzzahl = 1; /* mindestens 8 Bit, also 256 mögliche Werte */
short ganzzahl = 2; /* mindestens 16 Bit, also 65536 mögliche Werte */
int ganzzahl = 3; /* mindestens 16 Bit, also 65536 mögliche Werte */
long ganzzahl = 4; /* mindestens 32 Bit, also 4294967296 mögliche Werte */
long long ganzzahl = 5; /* mindestens 64 Bit, also 18446744073709551616 mögliche Werte */
Zahlen mit Nachkommastellen werden in einem der drei Datentypen codice_18, codice_19 und codice_20 gespeichert. In den meisten C-Implementierungen entsprechen die Datentypen float und double dem international gültigen Standard für binäre Gleitpunktarithmetiken (IEC 559, im Jahr 1989 aus dem älteren amerikanischen Standard IEEE 754 hervorgegangen). Ein float implementiert das „einfach lange Format“, ein double das „doppelt lange Format“. Dabei umfasst ein float 32 Bit, ein double 64 Bit. doubles sind also genauer. Floats werden aufgrund dieses Umstands nur noch in speziellen Fällen verwendet. Die Größe von long doubles ist je nach Implementierung unterschiedlich, ein long double darf aber auf keinen Fall kleiner sein als ein double. Die genauen Eigenschaften und Wertebereiche auf der benutzten Architektur können über die Headerdatei codice_21 ermittelt werden.
float kommazahl = 0.000001f; /* Genauigkeit ist implementierungsabhängig */
double kommazahl = 0.000000000000002; /* Genauigkeit ist implementierungsabhängig */
long double kommazahl = 0.3l; /* Genauigkeit ist implementierungsabhängig */

Der Datentyp codice_22 wird im C-Standard als „unvollständiger Typ“ bezeichnet. Man kann keine Variablen von diesem Typ erzeugen. Verwendet wird codice_22 erstens, wenn eine Funktion keinen Wert zurückgeben soll, zweitens wenn explizit eine leere Parameterliste für eine Funktion verlangt wird und drittens, wenn ein Zeiger auf „Objekte beliebigen Typs“ zeigen soll.
void funktionsname(); /* Deklaration einer Funktion, die keinen Wert zurückgibt */
int funktionsname(void); /* Deklaration einer Funktion, die int zurückgibt und keine Parameter akzeptiert */
void* zeigername; /* Zeiger auf ein Objekt von beliebigem Typ */
Wie in anderen Programmiersprachen sind Zeiger in C Variablen, die statt eines direkt verwendbaren Wertes (wie das Zeichen „A“ oder die Zahl 5) eine Speicheradresse (wie etwa die Adresse 170234) speichern. Die Adressen im Speicher sind durchnummeriert. An der Speicheradresse 170234 könnte zum Beispiel der Wert 00000001 gespeichert sein (Binärwert der Dezimalzahl 1). Zeiger ermöglichen es, auf den Wert zuzugreifen, der an einer Speicheradresse liegt. Dieser Wert kann wiederum eine Adresse sein, die auf eine weitere Speicheradresse zeigt. Bei der Deklaration eines Zeigers wird zuerst der Datentyp des Objekts angegeben, auf das gezeigt wird, danach ein Asterisk, danach der gewünschte Name des Zeigers.
char *zeiger; /* kann die Adresse eines Characters speichern */
double *zeiger; /* kann die Adresse eines Doubles speichern */

Wie in anderen Programmiersprachen verwendet man Felder (Arrays) in C um mehrere Werte desselben Datentyps zu speichern. Die Werte eines Arrays haben aufeinanderfolgende Speicheradressen. Die Anzahl der verschiedenen Werte eines Arrays ist als Index des Feldes festgelegt. Da es in C keinen eigenen Datentyp für Strings gibt, werden Arrays auch verwendet, um Zeichenfolgen zu speichern.
int zahlen[] = { 17, 0, 3 }; /* Definition eines Arrays mit 3 ganzzahligen Werten */
char string[] = "hallo welt!\n"; /* Array, das zur Speicherung eines Strings verwendet wird */

Um verschiedenartige Daten in einer Variable zu speichern, verwendet man Structures, geschrieben als codice_24. Auf diese Weise können Variablen verschiedenen Datentyps zusammengefasst werden.
struct person

Wie in anderen Programmiersprachen dient ein Enum in C dazu, mehrere konstante Werte zu einem Typ zu kombinieren.
enum Temperatur { WARM, KALT, MITTEL };
enum Temperatur heutige_temperatur = WARM;
if (heutige_temperatur == KALT) {

Das Schlüsselwort typedef wird zur Erstellung eines Alias für einen Datentyp verwendet.
typedef int Ganzzahl; /* legt den Alias „Ganzzahl“ für den Datentyp „int“ an */
Ganzzahl a, b; /* ist jetzt gleichbedeutend zu „int a, b;“ */
Bis zum C99-Standard gab es keinen Datentyp zum Speichern eines Wahrheitswerts. Erst seit 1999 können Variablen als codice_25 deklariert werden und einen der beiden Werte 0 (falsch) oder 1 (wahr) aufnehmen.
_Bool a = 1; /* seit C99 */

Durch explizite Verwendung des Headers codice_26 ist die verbreitete Verwendung des logischen Datentyps codice_27 mit den zwei möglichen Ausprägungen codice_28 bzw. codice_29 möglich:

bool a = true; /* seit C99 */
Seit C99 gibt es drei Gleitkomma-Datentypen für komplexe Zahlen, welche aus den drei Gleitkommatypen abgeleitet sind: codice_30, codice_31 und codice_32. Ebenfalls in C99 eingeführt wurden Gleitkomma-Datentypen für rein imaginäre Zahlen: codice_33, codice_34 und codice_35.

Ein C-Programm besteht aus der codice_36-Funktion und optional aus weiteren Funktionen. Weitere Funktionen können entweder selbst definiert werden oder vorgefertigt aus der C-Standard-Bibliothek übernommen werden.

Jedes C-Programm muss eine Funktion mit dem Namen codice_36 haben, anderenfalls wird das Programm nicht kompiliert. Die codice_36-Funktion ist der Einsprungspunkt eines C-Programms, das heißt die Programmausführung beginnt immer mit dieser Funktion.
main(){return 0;} /* das kürzeste mögliche standardkonforme C89-Programm */
int main(){} /* das kürzeste mögliche standardkonforme C99-Programm */

Außer der codice_36-Funktion müssen in einem C-Programm keine weiteren Funktionen enthalten sein. Sollen andere Funktionen ausgeführt werden, müssen sie in der codice_36-Funktionen aufgerufen werden. Die codice_36-Funktion wird deshalb auch als Hauptprogramm bezeichnet, alle weiteren Funktionen als Unterprogramme.

In C lassen sich beliebig viele Funktionen selbst definieren. Eine Funktionsdefinition besteht erstens aus dem Datentyp des Rückgabewerts, zweitens dem Namen der Funktion, drittens einer eingeklammerten Liste von Parametern und viertens einem eingeklammerten Funktionsrumpf, in welchem ausprogrammiert wird, was die Funktion tun soll.
int summe (int x, int y) /* Datentyp des Rückgabewerts, Funktionsname und zwei Parameter */
int main (void)

Für die Definition einer Funktion, die nichts zurückgeben soll, verwendet man das Schlüsselwort codice_22. Ebenso falls der Funktion keine Parameter übergeben werden sollen.
void begruessung (void)

Die Funktionen der Standard-Bibliothek sind nicht Teil der Programmiersprache C. Sie werden bei jedem standardkonformen Compiler im hosted environment mitgeliefert und können verwendet werden, sobald man die jeweils entsprechende Header-Datei eingebunden hat. Beispielsweise dient die Funktion codice_43 zur Ausgabe von Text. Sie kann verwendet werden, nachdem man die Header-Datei codice_44 eingebunden hat.
int main() {

Eine Funktion besteht aus Anweisungen. Wie in den meisten Programmiersprachen sind die wichtigsten Anweisungen: Deklarationen und Definitionen, Zuweisungen, bedingte Anweisungen, Anweisungen die Schleifen umsetzen sowie Funktionsaufrufe. Im Folgenden eher sinnlosen
Programm finden sich Beispiele.
/* Unterprogramme */
void funktion_die_nichts_tut(void) { /* Definition */
int plus_eins_funktion(int argument) { /* Definition */

/* Hauptprogramm */
int main() { /* Definition */

Beim Benennen von eigenen Variablen, Konstanten, Funktionen und Datentypen muss man sich an einige Regeln zur Namensgebung halten. Erstens muss das erste Zeichen eines Bezeichners ein Buchstabe oder Unterstrich sein. Zweitens dürfen die folgenden Zeichen nur die Buchstaben A bis Z und a bis z, Ziffern und der Unterstrich sein. Und drittens darf der Name keines der Schlüsselwörter sein.

Seit C95 sind auch Zeichen aus dem Universal Character Set in Bezeichnern erlaubt, sofern die Implementierung es unterstützt. Die erlaubten Zeichen sind in Anhang D des ISO-C-Standards aufgelistet. Vereinfacht gesagt, sind es all jene Zeichen, die in irgendeiner Sprache als "Buchstabe" oder buchstabenähnliches Zeichen Verwendung finden.

Ab C99 lassen sich diese Zeichen plattformunabhängig über eine Escape-Sequenz wie folgt ersetzen:

Bestimmte Bezeichner sind außerdem "für die Implementierung" reserviert:

Erweiterungen am Sprachkern, die neue Schlüsselwörter erfordern, verwenden dafür ebenfalls Namen aus diesem reservierten Bereich, um zu vermeiden, dass sie mit Bezeichnern in existierenden C-Programmen kollidieren, z. B. codice_47, codice_48, codice_49.

C ist case-sensitiv.

Die C-Standard-Bibliothek ist integraler Bestandteil einer "gehosteten" (engl. ') C-Implementierung. Sie enthält unter anderem Makros und Funktionen, die mittels der Standard-Header-Datei verfügbar gemacht werden. Auf "freistehenden" (engl. ') Implementationen dagegen kann der Umfang der Standardbibliothek eingeschränkt sein.

Die Standardbibliothek ist aufgeteilt in mehrere Standard-Header-Dateien, die hinzugelinkte Bibliothek ist jedoch oft eine einzige große Datei.


Eine Modularisierung in C erfolgt auf Dateiebene. Eine Datei bildet eine Übersetzungseinheit; intern benötigte Funktionen und Variablen können so vor anderen Dateien verborgen werden. Die Bekanntgabe der öffentlichen Funktionsschnittstellen erfolgt mit so genannten Header-Dateien. Damit verfügt C über ein schwach ausgeprägtes Modulkonzept.

Das globale Sprachdesign sieht vor, dass ein Programm aus mehreren Modulen bestehen kann.
Für jedes Modul existiert eine Quellcode-Datei (mit der Endung .c) und eine Header-Datei (mit der Endung .h). Die Quellcode-Datei enthält im Wesentlichen die Implementierung, die Header-Datei das Interface nach außen. Beide Dateien konsistent zu halten, ist bei C (wie auch bei C++) Aufgabe des Programmierers.

Module, die Funktionen aus anderen Modulen benutzen, inkludieren deren Header-Dateien und geben dem Compiler damit die notwendigen Informationen über die vorhandenen Funktionen, Aufrufkonventionen, Typen und Konstanten.

Jedes Modul kann für sich übersetzt werden und erzeugt eine Object-Datei.
Mehrere Object-Dateien können zu einer Bibliothek zusammengefasst oder einzeln verwendet werden.

Mehrere Object-Dateien sowie Bibliotheken (die auch nur eine Sammlung von Objekt-Dateien sind) können mittels "Linker" (deutsch: "Binder") zu einem ausführbaren Programm gebunden werden.

Am weitesten verbreitet ist der seit 1987 bestehende freie C-Compiler der GNU Compiler Collection. Unter Windows ist auch der seit 1993 entwickelte Compiler Visual C++ weit verbreitet. Neben diesen beiden stehen zahlreiche weitere Compiler zur Verfügung.

Da es in C vergleichsweise wenige Schlüsselwörter gibt, ergibt sich der Vorteil eines sehr einfachen, kleinen Compilers. Auf neuen Computersystemen ist C deshalb oft die erste verfügbare Programmiersprache (nach Maschinencode und Assembler).

Die Programmiersprache C wurde mit dem Ziel entwickelt, eine echte Sprachabstraktion zur Assemblersprache zu implementieren. Es sollte eine direkte Zuordnung zu wenigen Maschineninstruktionen geben, um die Abhängigkeit von einer Laufzeitumgebung zu minimieren. Als Resultat dieses Designs ist es möglich, C-Code auf einer sehr hardwarenahen Ebene zu schreiben, analog zu Assemblerbefehlen. Die Portierung eines C-Compilers auf eine neue Prozessorplattform ist, verglichen mit anderen Sprachen, wenig aufwändig. Bspw. ist der freie GNU-C-Compiler (gcc) für eine Vielzahl unterschiedlicher Prozessoren und Betriebssysteme verfügbar. Für den Entwickler bedeutet das, dass unabhängig von der Zielplattform fast immer auch ein C-Compiler existiert. C unterstützt damit wesentlich die Portierbarkeit von Programmen, sofern der Programmierer auf Assemblerteile im Quelltext und/oder hardwarespezifische C-Konstrukte verzichten kann. In der Mikrocontroller-Programmierung ist C die mit Abstand am häufigsten verwendete Hochsprache.

Konzeptionell ist C auf eine einfache Kompilierbarkeit der Quelltexte und für den schnellen Ablauf des Programmcodes ausgelegt. Die Compiler erzeugen in der Regel aber nur wenig Code zur Gewährleistung der Datensicherheit und Betriebssicherheit während der Laufzeit der Programme. Daher wird zunehmend versucht, diese Mängel durch formale Verifikation aufzudecken und zu korrigieren beziehungsweise durch zusätzliche vom Programmierer zu erstellende Quelltexte zu beheben.

C schränkt direkte Speicherzugriffe kaum ein. Dadurch kann der Compiler (anders als zum Beispiel in Pascal) nur sehr eingeschränkt bei der Fehlersuche helfen. Aus diesem Grund ist C für sicherheitskritische Anwendungen (Medizintechnik, Verkehrsleittechnik, Raumfahrt) weniger geeignet. Wenn in diesen Bereichen dennoch C eingesetzt wird, so wird in der Regel versucht, die Qualität der erstellten Programme durch zusätzliche Prüfungen wie Code-Coverage zu erhöhen.

C enthält einige sicherheitskritische Funktionen; so überschreibt zum Beispiel codice_50, eine Funktion der Standardbibliothek, fremde Speicherbereiche (Pufferüberlauf), wenn es auf eine unpassende (zu lange) Eingabe stößt. Der Fehler ist innerhalb von C weder bemerk- noch abfangbar. Um den großen Vorteil von C – die Existenz zahlreicher älterer Quellcodes – nicht zu verlieren, unterstützen auch aktuelle Implementierungen weiterhin diese und ähnliche Funktionen, warnen jedoch in der Regel, wenn sie beim Übersetzen im Quelltext benutzt werden.

C ist nicht typsicher, da verschiedene Datentypen zuweisungskompatibel gehandhabt werden können.

Einführungen

Fortgeschritten

Handbücher

K&R C

K&R2



</doc>
<doc id="910" url="https://de.wikipedia.org/wiki?curid=910" title="Claire Danes">
Claire Danes

Claire Catherine Danes (* 12. April 1979 in New York City) ist eine US-amerikanische Schauspielerin.

Als zwölfjährige Tochter eines New Yorker Künstlerpaares hätte Danes eine Rolle in Steven Spielbergs "Schindlers Liste" bekommen sollen. Als man ihr jedoch für die Zeit der Dreharbeiten in Polen keinen Privatlehrer zusagen konnte, lehnten ihre Eltern die Rolle ab. Zuvor spielte Danes 1992 bereits in einer Episodenrolle in der Erfolgsserie "Law & Order". Ihre Hauptrolle in der Fernsehserie "Willkommen im Leben" (1994–1995) brachte Danes einen Golden Globe ein und machte sie und ihren TV-Partner Jared Leto noch während ihrer Ausbildung an der "Performing Arts High School of New York" zu Filmstars.

Für den Film "Titanic" wurde Claire Danes die Hauptrolle der Rose angeboten. Sie lehnte jedoch ab, weil sie kurz zuvor in "Romeo & Julia" mitspielte. 1998 erhielt Danes einen "London Critics Circle Film Award" als "Beste Schauspielerin" in dem Film "William Shakespeares Romeo + Julia" der gleichzeitig schauspielerisch ihr Durchbruch war. Nach Beendigung der Dreharbeiten zu "Brokedown Palace" im September 1998 begann sie ein vierjähriges Psychologie-Studium an der Yale University (mit einem von Oliver Stone verfassten Empfehlungsschreiben). Seit 2002 arbeitet Danes an ihrer Filmkarriere und spielte unter anderem in dem Filmdrama "Igby" mit. 2010 erhielt sie für die Titelrolle der Autistin Temple Grandin im HBO-Fernsehfilm "Du gehst nicht allein" den Emmy sowie 2011 ihren zweiten Golden Globe Award. Seit 2011 spielt sie die Hauptrolle der "Carrie Mathison" in der Showtime-Serie "Homeland", für die sie 2012 und 2013 zwei Golden Globes sowie zwei Emmys erhielt.

Im September 2015 bekam sie den 2.559ten Hollywood Walk of Fame-Star.

Claire Danes hatte Beziehungen mit dem Schauspieler Matt Damon, dem australischen Musiker Ben Lee sowie mit dem Schauspieler Billy Crudup. Seit September 2009 ist Claire Danes mit ihrem Schauspielkollegen Hugh Dancy verheiratet, den sie bei den Dreharbeiten zum Film "Spuren eines Lebens" kennengelernt hat. Am 17. Dezember 2012 kam ihr gemeinsamer Sohn zur Welt.







</doc>
<doc id="911" url="https://de.wikipedia.org/wiki?curid=911" title="C">
C

C bzw. c (gesprochen: []) ist der dritte Buchstabe des klassischen und modernen lateinischen Alphabets. Er bezeichnete zunächst die velaren Verschlusslaute /k/ und /g/ (letzterer seit dem 3. Jh. v. Chr. durch das neugeschaffene G vertreten); infolge der seit dem Spätlateinischen bezeugten Assibilierung vor Vorderzungenvokal bezeichnet "c" in den meisten romanischen und noch vielen anderen Sprachen auch eine (post-)alveolare Affrikate (ital. , dt. tschech. ) oder einen dentalen oder alveolaren Reibelaut (franz. engl. , span. []). Der Buchstabe C hat in deutschsprachigen Texten eine durchschnittliche Häufigkeit von 3,06 %.
Das Fingeralphabet für Gehörlose bzw. Schwerhörige stellt den Buchstaben "C" dar, indem der Daumen und restliche Finger einen offenen Halbkreis bilden.

In den meisten romanischen Sprachen sowie verbreitet in der mittelalterlichen und neuzeitlichen Aussprache des Lateins und zahlreicher daraus entlehnter Wörter steht "c" vor Konsonanten und hinteren Vokalen (einschließlich /a/) für den stimmlosen velaren Plosiv /k/, vor ursprünglichen Vorderzungenvokalen "e", "i" (auch vor lat. "ae", "oe", "y") dagegen für einen Zischlaut (je nach Sprache und Sprachstufe eine Affrikate , oder ein reiner Frikativ /s/, , ; vgl. Romanische Palatalisierung). Die Verteilung dieser Allophone nach darauffolgendem Vokal wird gelegentlich in sprachspezifischer Lautschrift durch die Reihe ka – ce/ze – ci/zi – ko – ku (sogenannte Ka-ze-zi-ko-ku-Regel) wiedergegeben oder einen Merkspruch folgender Art zusammengefasst: „Vor "a, o, u" sprich "c" wie "k", vor "e" und "i" sprich "c" wie "z".“ Wo ein solcher Zischlaut vor einem hinteren Konsonanten wie /a/, /o/, /u/ (oder einem erst später daraus entstandenen Vorderzungenvokal, etwa frz. [y] < lat. /u/) steht, wird er oft durch "ç", "z", oder (im Italienischen) den Digraphen "ci" bezeichnet. Umgekehrt tritt für den Velaren vor vorderem Vokal "k", im Französischen regelmäßig "qu", im Italienischen "ch" ein. Darüber hinaus wird der Buchstabe "c" zum Teil auch allgemein durch "z" bzw. "k" ersetzt, z. B. bei im heutigen Deutsch bei lateinischen Lehnwörtern: "Zirkus" statt "Circus".

Außerhalb des Italienischen steht der Digraph "ch" in vielen romanischen Sprachen ebenfalls für einen Zischlaut, im Deutschen und im Gälischen für einen velaren oder palatalen Reibelaut. Die Kombination ck dient im Deutschen als Variante von "k" zur Kennzeichnung, dass der vorhergehende Vokal kurz ausgesprochen wird; der Trigraph "sch" stellt den Laut [] dar (wie in "Schule").

Die aus dem proto-semitischen Alphabet stammende Urform des Buchstaben stellt einen Fuß dar. Im phönizischen Alphabet wurde diese Bedeutung beibehalten. Der Buchstabe erhielt den Namen Gimel (Kamel) und hatte den Lautwert [g]. Die Griechen übernahmen den Buchstaben als Gamma. Zu Beginn wurde das Gamma in einer Form geschrieben, die wie ein Dach aussah (ähnlich dem späteren Lambda). Bis zur klassischen Zeit entwickelte sich das Gamma zu Γ weiter. Mit dafür verantwortlich war wahrscheinlich neben dem Wechsel der Schreibrichtung von rechts-nach-links auf links-nach-rechts auch der notwendige Wechsel der Schreibwerkzeuge zum Beschreiben von organischen Stoffen.

Als die Etrusker das frühgriechische Alphabet übernahmen, hatten sie keine Verwendung für das Gamma, da im Etruskischen stimmhafte Verschlusslaute wie [g] nicht vorkamen. Allerdings hatte die etruskische Sprache drei k-Laute. Die Etrusker veränderten daher den Lautwert des Buchstabens, um den stimmlosen Verschlusslaut [k] vor [e] oder [i] wiederzugeben.

Mit eben dem Lautwert wanderte das Zeichen C dann in das lateinische Alphabet und wurde von den Römern, die durchaus zwischen der Tenuis K und der Media G unterschieden, so ursprünglich für die Laute [g] und [k], genauer, für die Silben [ge]; [gi] und [ke]; [ki] gesetzt. Wenn auch in archaischer Zeit in der lateinischen Schriftpraxis drei von ihren nachfolgenden Lauten unterschiedlich gefärbte [k]-Laute zeichenmäßig noch nicht konsequent unterschieden wurden, so setzte doch eine Differenzierung ein, nämlich C vor [e], [i], K vor [a] und Liquiden, Q vor [o], [u], von denen der Erstere auch noch für unser heutiges G verantwortlich ist.

Bereits im 4. Jahrhundert v. Chr. kam dieser Prozess zum Abschluss, indem der Buchstabe Q nur noch vor das konsonantische [u] gestellt, während der Buchstabe K ab dem 3. Jahrhundert v. Chr. nur noch in formelhaften Abkürzungen, wie Kal. = Kalendae und dem Brandmal K. = Kalumniator vorkam. Beide Buchstaben wurden zugunsten des C verdrängt.

Nun hing aber auch noch der [g]-Laut am Zeichen C und nach Plutarch (Quest. Rom. 54) war es 230 v. Chr. der Schreibschulen-Betreiber Spurius Carvilius Ruga, der durch Hinzufügung eines Striches das G aus dem C herausholte, und an die Stelle verfrachtete, welche der [ts]-Laut, also das Griechischen Zeta, unser Zet, im Griechischen einnahm. Erhalten blieb das Zeichen C als [ge]-Laut nur in den Abkürzungen C. = Gaius und CN. = Gnaeus.

Interessant ist dabei, dass der Römer den neuen Buchstaben, also die Verknüpfung des Zeichens C mit Cauda (Schwanz) = G mit dem Laute [g], nicht ans Ende des Alphabetes setzte, wie später mit dem griechischen Y und Z geschah, sondern an die Stelle, die dem Zet nach dem griechischen Alphabete zukam. Nachdem der stimmhafte [z]-Laut, der an der Zeta-Stelle im Alphabet, also an der 7. Stelle stand und durch das Zeichen 'I' dargestellt wurde, zu R geworden war (fesiae → feriae), war das Zeichen nicht mehr nötig und der Buchstabe wurde durch den Censor Appius Claudius Caecus 312 v. Chr. (Marc. Capella: 1,3) getilgt. Zudem war Griechenland noch nicht erobert und die griechische Gelehrsamkeit in Rom noch nicht heimisch. Das könnte ein Hinweis darauf sein, dass dort eine Lücke empfunden wurde, weil auch in Rom die Buchstaben noch die alte phönizische Zahlbedeutung hatten.

Im Spätlatein ab dem 5. Jahrhundert n. Chr. wurde das [k] vor einem hellen Vokal zu [ts]. Diese Aussprache wurde zum Standard im mittelalterlichen Latein, sie ist der Grund dafür, dass das C heute unterschiedliche Lautwerte hat. In romanischen Sprachen wurde diese Entwicklung zum Teil noch weiter fortgeführt; das C hat dort auch die Lautwerte [tʃ], [s] oder [θ].

"Da wir, gleich den Griechen und Slaven, die tenuis des gutturallauts mit K ausdrücken, so ist dafür das aus dem lateinischen Alphabet entnommene C ganz überflüssig, fehlt darum auch der gothischen und altnordischen schrift, die Slaven verwenden es für S, die Polen und Böhmen für Z." (…) "unentbehrlich aber bleibt, solange wir für die kehlaspirata kein einfaches Zeichen, wie die Gothen das gr. X, annehmen, C in CH." (aus dem Grimmschen Wörterbuch)




</doc>
<doc id="912" url="https://de.wikipedia.org/wiki?curid=912" title="C4">
C4

C4, C 4 oder C-4 steht für:

Kraftfahrzeuge:
C.4 steht für:
Siehe auch:


</doc>
<doc id="913" url="https://de.wikipedia.org/wiki?curid=913" title="Chemische Waffe">
Chemische Waffe

Chemische Waffen (auch Chemiewaffen) sind toxisch wirkende feste, flüssige oder gasförmige Substanzen oder Gemische, die – in Verbindung mit der notwendigen Waffentechnik zur Ausbringung (Granaten, Sprühvorrichtungen) – ursprünglich hergestellt wurden, um Menschen in kriegerischen Auseinandersetzungen sowie bei Terror- und Sabotageakten zeitweilig kampf- bzw. handlungsunfähig zu machen oder zu töten. In der 1997 in Kraft getretenen Chemiewaffenkonvention wird die Verwendung auf jede Chemikalie in Waffen erweitert, deren toxische Eigenschaften Menschen oder Tieren zeitweiligen oder permanenten Schaden zufügen, und auch die zu ihrer Produktion verwendeten Vorgängerstoffe werden, sofern sie nicht für eine andere Form der Weiterverarbeitung vorgesehen sind, zu den chemischen Waffen gezählt. Im erweiterten Sinn werden auch Brand- (Napalm), Nebel- und Rauchstoffe sowie Entlaubungsmittel (Herbizide) und Nesselstoffe zu den chemischen Waffen gerechnet. Chemische Waffen gehören zu den Massenvernichtungswaffen (CBRN-Waffen).

Bereits im Peloponnesischen Krieg 431 bis 404 v. Chr. setzten die Spartaner Brandkörper ein, die hohe Luftkonzentrationen von Schwefeldioxid verursachten. In der Schlacht bei Liegnitz (1241) wurden die christlichen Ritter von den Mongolen durch „dampfausstoßende Kriegsmaschinen“ in Schrecken versetzt.

Die ersten modernen chemischen Waffen sind im Ersten Weltkrieg eingesetzt worden und basierten zunächst auf Substanzen, die bereits in der chemischen Industrie verwendet wurden, also in ausreichend großen Mengen vorhanden waren; das waren Gase wie Chlor, Phosgen, Cyanwasserstoff (Blausäure) oder Arsin. Diese hatten jedoch zwei große Nachteile: Zum einen waren sie durch wechselnde Windrichtungen unberechenbar (so konnte eine Gaswolke auf die eigene Stellung zurückgeweht werden), und andererseits verflüchtigte sich das Gas relativ schnell. Daher sind die meisten späteren chemischen Kampfstoffe Flüssigkeiten, die als Aerosole versprüht werden. Das hat zur Folge, dass die Substanzen an Boden, Kleidung, Haut und Gasmasken klebenbleiben und auch in die Filter eindringen können. Deshalb ist die Verweildauer viel länger als bei Gas.

Das Hauptziel der neueren Kampfstoffe ist nicht allein die Lunge, sondern auch die Haut. Ein solcher Kampfstoff diffundiert durch die Haut hindurch in die Blutbahn und wird so schnell im ganzen Organismus verteilt. Daher stellen nur Ganzkörperschutzanzüge einen ausreichenden Schutz gegen Kampfstoffe dar. Ein bekannter und wichtiger Kampfstoff dieser Gruppe ist Schwefellost, auch bekannt unter dem Namen Senfgas.

Dass bereits 21 Jahre vor dem Ersten Weltkrieg die Entwicklung von Chemiewaffen politisch relevant war, zeigt ein Artikel der "Times" von 1893, in dem das "War Office Explosives Committee" die Unmöglichkeit thematisierte, Tests der neuen Waffen geheim zu halten:

Im Ersten Weltkrieg kam es zum ersten Einsatz von chemischen Kampfstoffen im August 1914 durch französische Truppen, die Xylylbromid – ein für die Pariser Polizei entwickeltes Tränengas – gegen deutsche Truppen einsetzten. Erste Versuche beider Seiten mit Stoffen wie Bromessigsäureethylester (durch Frankreich im März 1915) und o-Dianisidinchlorsulfonat, einem feinkristallinen Pulver, das Schleimhäute der Augen und Nase reizte (durch Deutschland am 27. Oktober 1914 bei Neuve-Chapelle), verliefen nicht zufriedenstellend, da die Stoffe sich beim Abschuss durch die entstehende Hitze zersetzten.

In großem Umfang setzte zuerst das deutsche Heer Kampfgase ein, als Ende Januar 1915 an der Ostfront bei Bolimów in Polen bei einer Offensive der 9. Armee mit Xylylbromid gefüllte Geschosse gegen russische Truppen abgefeuert wurden. 18.000 Gasgranaten waren bereitgestellt worden, deren Wirkung aber durch Kälte und Schnee nahezu aufgehoben wurde. Ungleich bekannter wurde jedoch der erste wirkungsvolle Einsatz von chemischen Waffen an der Westfront vom 22. April 1915 in der Zweiten Flandernschlacht bei Ypern. Das deutsche XV. Armee-Korps unter General der Infanterie von Deimling ließ 150 Tonnen Chlorgas nach dem so genannten Haberschen Blasverfahren aus Flaschen entweichen. Eingeatmetes Chlorgas führt zu einem lebensbedrohlichen toxischen Lungenödem. Da Chlor schwerer als Luft ist, sank das Gas in die französischen Schützengräben und forderte dort angeblich rund 5.000 Tote und 10.000 Verletzte; heute geht man von 1.200 Toten und 3.000 Verwundeten aus.

Frankreich setzte als erste der kriegführenden Nationen am 22. Februar 1916 Phosgen (COCl) in Reinform ein, nachdem deutsche Gastruppen eine Mischung aus Chlorgas mit einem etwa fünfprozentigen Zusatz von Phosgen bereits Ende Mai 1915 an der Ostfront in Bolimów an der Bzura gegen russische Truppen sowie an der Westfront am 31. Mai 1915 bei Ypern gegen französische Truppen verwendet hatten. Phosgen wird der größte Anteil an allen Gasverletzten zugeschrieben. Später wurden die Kampfstoffe mittels Giftgasgranaten verschossen, bei denen durch farbige Kreuze (Blaukreuz, Gelbkreuz, Grünkreuz und Weißkreuz) erkennbar war, welche Art von Kampfstoff sie enthielten.
An der Westfront wurde verstärkt „Gelbkreuz“ eingesetzt, das für Hautkampfstoffe stand.

Während des Ersten Weltkrieges wurden Kampfstoffe in der Spätphase häufig kombiniert eingesetzt. Stark reizend wirkende Kampfstoffe in Aerosol- oder Pulverform wie Blaukreuz konnten die Filter der Gasmasken durchdringen und zwangen die Träger, die Gasmaske abzunehmen. Gleichzeitig mit diesen "Maskenbrechern" wurden lungenschädigende Kampfstoffe wie Grünkreuz eingesetzt. Der kombinierte Einsatz verschiedener Kampfstoffe zu diesem Zweck wurde als „Buntschießen“ oder „Buntkreuz“ bezeichnet.

Bei der Offensive deutscher und österreichisch-ungarischer Verbände im Raum Flitsch-Tolmein (Schlacht von Karfreit oder auch Zwölfte Isonzoschlacht) am 24. Oktober 1917 wurde der Angriff durch „Buntschießen“ von Gasbatterien vorbereitet. Die italienischen Soldaten verfügten nur über ungenügende oder gar keine Schutzbekleidung – in diesem Abschnitt starben durch den Gasangriff über 5.000 Italiener. Die angreifenden Verbände hatten es dadurch erheblich leichter, den Durchbruch durch die italienische Front zu erreichen. Auch die psychische Wirkung auf die Italiener war verheerend. Sehr viele Soldaten ergaben sich den Angreifern, die Kampfmoral sank drastisch. Die italienische Front musste bis an den Piave zurückgenommen werden; zur Verstärkung wurden französische und britische Verbände an diese Front verlegt. Die Italiener konnten die Lage nach einer Reorganisation später selbst wieder stabilisieren. Im Juni 1918 versuchte Österreich-Ungarn in einer letzten Offensive, den Piave zu überschreiten. Der Angriff war jedoch nicht erfolgreich, da zum einen die Italiener besser gegen Gasangriffe gerüstet waren und zum anderen ein Teil der chemischen Waffen zu lange gelagert worden war und damit seine Wirksamkeit verloren hatte.

Ein weiterer militärisch erfolgreicher Fall von Buntschießen, wie von Oberst Georg Bruchmüller erfunden, erfolgte bei der Deutschen Frühjahrsoffensive vom 21. März bis 17. Juli 1918 an der Westfront in Nordfrankreich. Dabei lag das Augenmerk nicht auf einer langen Artillerievorbereitung und einem schwerfälligen Angriff auf breiter Front, sondern auf einem kurzen, aber zusätzlich durch gemischten Einsatz von Gasgranaten effektiven Artillerieschlag. Danach sollten die sogenannten Sturmbataillone nachrücken und verbliebene Widerstandsnester ausräumen. Der gemischte Gaseinsatz lähmte dabei die Widerstandskraft des Gegners entscheidend.

Chemische Kampfstoffe werden heute allgemein als die schrecklichsten Waffen des Ersten Weltkrieges angesehen. Sie verursachten kurzzeitig große Ausfälle, wobei allerdings im Vergleich zu anderen damaligen Waffen die Todesraten sehr gering waren. Trotz der teilweise qualvollen Verletzungen waren die Heilungschancen besser als im Vergleich zu Verwundungen durch Schussverletzungen oder Artillerie; abgesehen von den Spätfolgen wie zum Beispiel Hautkrebs im Falle von S-Lost, die zum Teil erst nach Jahrzehnten eintraten.

Chemische Waffen verursachten im Ersten Weltkrieg auf beiden Seiten insgesamt etwa 90.000 Tote und 1,2 Millionen Verwundete. Aufgrund mangelhafter Ausstattung mit Schutzausrüstung hatte Russland mehr als die Hälfte der Toten zu beklagen. An der Westfront hatten die Alliierten etwa doppelt so hohe Verluste wie die Deutschen. Deutschland und Österreich-Ungarn rüsteten ihre Soldaten mit wirksameren Gasmasken aus und konnten so höhere Verluste bei Gasangriffen vermeiden.

Aufgrund der verhältnismäßig niedrigen Todesrate (manche Historiker nehmen an, dass insgesamt nur 18.000 Mann an der Westfront durch Gasangriffe starben) und der teilweise unkalkulierbaren Wirkung infolge von nicht vorhersehbaren Faktoren wie bspw. wechselnde Windrichtungen gilt Giftgas im Ersten Weltkrieg als eine wenig effektive Waffe.

Im Ersten Weltkrieg hatte die Flugzeugtechnik deutliche Fortschritte gemacht: Reichweite, Zuverlässigkeit, Geschwindigkeit und maximale Zuladung hatten stark zugenommen. Auch hatten alle Seiten die Nützlichkeit von Aufklärungsflugzeugen erkannt.

Ab 1919 wurde das Konzept der kolonialen Herrschaft und Kontrolle aus der Luft von Winston Churchill erstmals umgesetzt. Die Royal Air Force sollte dabei die Kontrolle über die Kolonien im Nahen Osten übernehmen und ausführen. Neben konventionellen Waffen wurden dabei auch Giftgaseinsätze aus der Luft erwogen und von Churchill ausdrücklich gefordert. Aufgrund von technischen Problemen wurde Giftgas nur mit den bereits im Ersten Weltkrieg erprobten Methoden gegen die arabische Bevölkerung im Irak angewandt. Dabei kam es auch zu Giftgaseinsätzen gegen die Kurden in Sulaimaniyya im heutigen Irak.

Vorbehalte britischer Militärs wies Churchill zurück und erklärte: „"Ich verstehe die Zimperlichkeit bezüglich des Einsatzes von Gas nicht. Ich bin sehr dafür, Giftgas gegen unzivilisierte Stämme einzusetzen"“, ließ er verlauten. Das eingesetzte Gas müsse ja nicht tödlich sein, sondern nur „"große Schmerzen hervorrufen und einen umfassenden Terror verbreiten“".

Ein Verband der sowjetischen Armee, zusammengesetzt vorwiegend aus Einheiten der Tscheka, setzte während des Bauernaufstands von Tambow 1920/21 chemische Kampfstoffe zur Bekämpfung der aufständischen Bauern ein.

Im Rifkrieg in Nordmarokko setzte Spanien ab 1924 chemische Waffen gegen die aufständischen Rifkabylen, einen Berber-Stamm, ein. Dabei wurde Spanien von Frankreich und in einem Geheimvertrag von der deutschen Reichswehr unterstützt.
Ein weiteres Mal wurde Giftgas vom faschistischen Italien im Zweiten Italienisch-Libyschen Krieg sowie im Italienisch-Äthiopischen Krieg (1935–1936) verwendet. Italien setzte Giftgasbomben in Äthiopien ein, nachdem die äthiopische Weihnachtsoffensive erfolgreich italienische Truppen zurückgedrängt und Versorgungslinien unterbrochen hatte. Die äthiopischen Truppen waren sehr schlecht ausgerüstet und viele Krieger kämpften noch mit Speeren. Die Krieger trugen traditionelle Kleidung und verfügten über keine Schutzausrüstung, so dass besonders das hautschädigende Senfgas zu hohen Verlusten führte. Laut sowjetischen Schätzungen kamen durch den Einsatz von Giftgas 15.000 bis 50.000 Äthiopier ums Leben.

Der deutschen Reichswehr waren die Entwicklung und der Besitz von chemischen Waffen durch den Versailler Vertrag verboten. Um das Verbot zu umgehen, kooperierte Deutschland ab 1923 mit der Sowjetunion (siehe: Vertrag von Rapallo) und erprobte auf dem Testgelände Tomka chemische Waffen. Eine Zusammenarbeit fand auch mit Spanien statt.

In den USA wurden Chemiewaffen zwischen den Weltkriegen weiterentwickelt. Zuständig dafür war neben der American Chemical Society (Institut für Chemie an der Northwestern University) eine Militärbehörde, die "National Association for Chemical Defense". Deren Leiter H. Edmund Bullis empfahl 1928 sogar den Polizeibehörden den Einsatz dieser „höchst effektiven und zugleich humansten aller Waffen“, eben Chemiewaffen. In Cleveland und Chicago testeten Polizisten in dem Jahr „erfolgreich“ aus „genialen“ Füllfederhalter-großen oder aus normalen Pistolen abgefeuerte neuartige Gase, die „gezeigt haben, dass man drei oder vier Männer, die zusammen nicht weiter als fünf Meter entfernt stehen, mit einem einzigen Schuss nachhaltig ausschalten kann“. Auch Kneipen, die illegal Alkohol ausschenkten (Speakeasys), könne man mit Chemiewaffen „mindestens einen Monat lang unbewohnbar“ machen. Bullis setzte sich vehement gegen ein weltweites Verbot von chemischen Waffen im Krieg ein, mit der Begründung:

Er nannte als Beispiel den Austritt toxischen Phosgengases aus einem Kesselwagen bei der Chemischen Fabrik Stoltzenberg in Hamburg. Das Deutsche Reich durfte eigentlich solche Giftgase gar nicht herstellen und lagern.

Die englische Öffentlichkeit diskutierte nach dem Ersten Weltkrieg über eine stärkere Zusammenlegung von ziviler und militärischer Forschung, wozu auch die Entwicklung neuer Chemiewaffen gehörte. „Die ganze Zukunft der chemischen Kriegführung hängt von der Farbstoffindustrie ab“, schrieb 1920 der Kriegskorrespondent der Londoner Times.

Die Verwendung von vergiftenden Waffen war schon vor dem Ersten Weltkrieg durch die Haager Landkriegsordnung geächtet, deren Formulierung bot jedoch ausreichend Spielraum zu verschiedenen Auslegungen, so dass der Einsatz von Giftgas nicht eindeutig verboten war. Angesichts der Gräuel im Ersten Weltkrieg wurde 1925 im Genfer Protokoll die Anwendung von Giftgasen und bakteriologischen Mitteln ausdrücklich verboten.

Die Ratifizierung erfolgte zögerlich:
1926: Frankreich, 1928: Italien, Sowjetunion (Erklärung),
1929: Deutschland, 1930: Großbritannien, 1970: Japan, 1975: USA.

Viele der Unterzeichnerstaaten behielten sich bestimmte Handlungen vor, namentlich

Der Vertrag ist nur ein Verbot des "Erst"einsatzes von B- und C-Waffen.

Während des Zweiten Weltkrieges setzte das Kaiserreich Japan als einzige Nation chemische Waffen ein. Diese wurden zusammen mit biologischen Waffen in China gegen chinesische Truppen und auch zur gezielten Massentötung von Zivilisten eingesetzt.

Nach Erkenntnissen der Historiker Yoshiaki Yoshimi und Seiya Matsuno erhielt Okamura Yasuji vom Kaiser Hirohito die Erlaubnis, chemische Waffen während dieser Gefechte einzusetzen. Zum Beispiel ermächtigte der Kaiser den Einsatz von Giftgas während der Schlacht um Wuhan von August bis Oktober 1938 in 375 verschiedenen Einsätzen gegen die 1,1 Millionen chinesischen Soldaten, von denen 400.000 während der Schlacht starben. Artikel 23 der Haager Konvention (1899 und 1907) und Artikel V des Vertrags in Bezug auf die Nutzung von U-Booten und Schadgasen in der Kriegführung vom 6. Februar 1921 verurteilten jedoch bereits den Einsatz von Giftgas. Während der Schlacht um Changsha im Herbst 1939 setzte die Kaiserlich Japanische Armee ebenfalls große Mengen Giftgas gegen chinesische Positionen ein. Ein weiteres Beispiel ist die Schlacht von Yichang im Oktober 1941, in der das 19. Artillerieregiment die 13. Brigade der 11. Armee durch Beschuss der chinesischen Streitkräfte mit 1.000 gelben Gasgranaten und 1.500 roten Gasgranaten unterstützte. Das Gebiet war mit chinesischen Zivilisten, deren Evakuierung durch die japanische Armee untersagt wurde, überfüllt. Von den rund 3.000 chinesischen Soldaten in dem Gebiet waren 1.600 von der Wirkung des Gases erheblich betroffen.

Während der Schlacht um Changde im November und Dezember 1943 versuchten Truppen der Kaiserlich Japanischen Armee, darunter die Einheit 516, zusammen mit der Versprühung von biologischen Kampfstoffen von Flugzeugen aus, durch den massiven Einsatz von Giftgas, welches hauptsächlich mit Artilleriegranaten sowohl auf chinesische Stellungen im Umland als auch in die Stadt abgeschossen wurde, den Widerstand der Verteidiger zu brechen. Bei dem eingesetzten Gas handelte es sich neben anderen Arten zur Hauptsache höchstwahrscheinlich um Senfgas und Lewisit. Im Laufe der Schlacht starben 50.000 chinesische Soldaten und 300.000 Zivilisten. Wie viele davon durch die biologischen und chemischen Waffen gestorben sind, ist ungeklärt. Sowohl die Einsätze von biologischen als auch von chemischen Waffen durch die Kaiserlich Japanische Armee werden zu den japanischen Kriegsverbrechen gezählt.

Zu den zahllosen Menschenexperimenten der japanischen Armee, darunter der Einheit 731, gehörte auch das Testen von Giftgas an gefangenen chinesischen Zivilisten. Im Jahr 2004 entdeckten Yuki Tanaka und Yoshimi im australischen Nationalarchiv Dokumente, die belegen, dass Zyanidgas im November 1944 auf den Kai-Inseln (Indonesien) an australischen und niederländischen Kriegsgefangenen getestet wurde.

Das Verbot der Anwendung von vergiftenden, chemischen und biologischen Waffen wurde im Zweiten Weltkrieg zumindest auf dem europäischen Kriegsschauplatz weitgehend beachtet, obwohl nicht alle beteiligten Länder dem Protokoll beigetreten waren. Ein weiterer wichtiger Aspekt war auch die gegenseitige Abschreckung, vergleichbar mit der atomaren Abschreckung im Kalten Krieg: Hätte eine der kriegführenden Parteien Giftgas eingesetzt, wurde als Folge eine Bombardierung des eigenen Territoriums mit chemischen Waffen durch Gegner befürchtet. Für den Fall, dass Deutschland an der Ostfront Kampfstoffe einsetzen sollte, hatte der britische Premierminister Churchill bereits im Mai 1942 mit einem Großeinsatz von Kampfstoffen gedroht. Ein amerikanischer Plan vom April 1944 sah für den Fall des Kampfstoffeinsatzes durch Deutschland einen Vergeltungsangriff gegen 30 große deutsche Städte vor. Innerhalb von 14 Tagen sollten in diesem Fall die Städte mit einer Gesamtfläche von 217 km² angegriffen und über ihnen insgesamt 15.345 t Senfgas (Lost) und 21.176 t Phosgen abgeworfen werden. Wegen der extrem hohen Kampfstoffkonzentration in diesem Fall (168 Gramm je Quadratmeter) gingen Schätzungen von 5,6 Millionen unmittelbar durch den Einsatz Getöteten und weiteren 12 Millionen an den Folgen des Angriffs Gestorbenen und Verletzten aus.
Auch wäre der Einsatz meist unvorteilhaft gewesen, da die eigenen Soldaten in der Offensive verseuchtes Gelände eingenommen hätten und daher selbst Vergiftungen zu fürchten gehabt hätten.

An den europäischen Fronten sind während des ganzen Zweiten Weltkriegs nur vier Fälle bekannt geworden, in denen Menschen durch Kampfstoffe getötet oder verletzt wurden, dabei handelte es sich um einen gezielten Kampfstoffeinsatz sowie drei Unfälle:


Im nationalsozialistischen Deutschen Reich wurde im Dezember 1936 bei I.G. Farben im Werk Leverkusen durch den Chemiker Gerhard Schrader der Nervenkampfstoff Tabun entdeckt. Im Dezember 1939 synthetisierte er den in seiner Wirkung noch stärkeren Giftstoff Sarin. Ab Frühjahr 1942 produzierte I.G. Farben in ihrem Werk in Dyhernfurth in Schlesien Tabun. 1944 entdeckte der Nobelpreisträger Richard Kuhn mit seinem Mitarbeiter, Konrad Henkel, den Kampfstoff Soman in einer vom Heereswaffenamt unterhaltenen Abteilung des Kaiser-Wilhelm-Instituts für medizinische Forschung in Heidelberg. Diese Nervengifte wurden aufgrund der Furcht vor einem Gegenschlag nicht eingesetzt; da sie in gasförmiger Zubereitung – oft als Aerosol – eingesetzt werden sollten, werden dies Stoffe auch als "Nervengase" bezeichnet.

Deutschland hatte Ende der dreißiger Jahre als erste Nation die großtechnische (industrielle) Produktion von Nervenkampfstoffen entwickelt, war also als einzige Kriegspartei zur Herstellung von Nervenkampfstoffen im Kilogramm- und Tonnenbereich in der Lage. Dieser Umstand, gekoppelt mit der Verfügbarkeit modernster Trägersysteme wie der V-2, hätte die politische Führung in die Lage versetzt, einen strategischen Gaskrieg zu entfesseln, der unter Umständen von der Tragweite her ähnlich gravierend hätte sein können wie die Atombombenabwürfe auf Hiroshima und Nagasaki. Die verantwortliche Führung des deutschen Kampfgasentwicklungsprogramms verheimlichte Hitler gegenüber bewusst die tatsächlichen Möglichkeiten, denn eine Eskalation zum Gaskrieg wurde befürchtet, falls Hitler klar werden sollte, welche Wirkung beispielsweise ein mit Tabungefechtsköpfen bestückter V-2-Angriff auf London hätte haben können. Für den taktischen Einsatz waren bereits als Träger Werferwaffen (sog. Nebelwerfer) hergestellt und die entsprechenden Truppen (Nebeltruppe) geschult worden. Die oft geäußerte Vermutung, dass die Erfahrungen Hitlers im Ersten Weltkrieg ihn davon abgehalten haben sollen, chemische Kampfstoffe einsetzen zu lassen, entbehrt jeder Grundlage, da er selbst die Produktion dieser befahl und die Vorbereitungen für den Beginn eines Gaskrieges anordnete. Die Gründe dafür, dass die ab 1942 in großem Umfang produzierten Nervenkampfstoffe nicht zum Einsatz kamen, waren größtenteils logistischer (Rohstoffknappheit) und militärstrategischer Art. Ebenfalls von Bedeutung waren sowohl die deutsche Fehleinschätzung, die Alliierten würden ebenfalls über Nervenkampfstoffe verfügen, als auch die alliierte Androhung massiver Gegenschläge im Falle eines deutschen Ersteinsatzes chemischer Kampfstoffe.

In den Gaskammern der deutschen Vernichtungslager Auschwitz-Birkenau, Belzec, Sobibor, Mauthausen, Treblinka und Lublin-Majdanek wurden viele Opfer des Holocaust mit dem blausäurehaltigen Insektizid Zyklon B und mit Motorabgasen (Kohlenstoffmonoxid) ermordet.

Nach dem Zweiten Weltkrieg wurden umfangreiche deutsche Bestände – zwischen 30.000 und 40.000 Tonnen chemischer Waffen – in der Nord- und Ostsee in der von US-amerikanischen Streitkräften geleiteten Operation Davy Jones' Locker mitsamt ihren Transportschiffen versenkt, so vor der norwegischen Hafenstadt Arendal 1946. Die Versenkung der Schiffe erfolgte durch Sprengung oder Beschuss durch Bordwaffen begleitender britischer Kriegsschiffe. 1955/56 wurden Restbestände, die von der Royal Air Force gebunkert worden waren, in der Operation Sandcastle nordwestlich von Irland im Atlantik versenkt, so auch die SS Kotka. Von 1944 bis 1970 wurden von Seiten der United States Army in 26 so genannten Versenkungszonen ("dump zones") an der Ostküste der USA chemische Kampfstoffe versenkt, von denen aufgrund mangelnder oder unzureichender Dokumentation unklar ist, wo sie sich exakt befinden und welche Chemikalien in welcher Menge dort lagern.

Gesichert ist, dass Ägypten chemische Waffen im Jemen eingesetzt hat. Die Technologie dazu stammte aus der Sowjetunion, welche diese auch an andere mit ihr verbündete Staaten des Nahen Ostens – wie dem Irak – weitergegeben hatte.

Während anfangs von Frankreich und den USA noch konventionelle Brandbomben wie Napalm gegen die Nordvietnamesen und die FNL verwendet wurden, startete die Regierung Kennedy 1961 den systematischen Einsatz von Chemikalien gegen Nordvietnam. Die im Zuge der Operation Ranch Hand als Entlaubungsmittel eingesetzten Herbizide (vor allem Agent Orange) sollten dem Gegner die Deckung durch die Vegetation nehmen sowie seine Ernte vernichten. Agent Orange war mit 2,3,7,8-Tetrachlordibenzodioxin verunreinigt und verursachte dadurch schwere gesundheitliche Schäden unter der Bevölkerung und den Soldaten beider Seiten.

Erste Verhandlungen zu einem Chemiewaffenübereinkommen ("CWÜ", auch "Chemiewaffenkonvention" genannt) begannen 1968 mit der "Working Group on Chemical Weapons" bei der "Eighteen Nations Conference on Disarmament (ENCD)" der UN in Genf, die seit 1962 bestand. 1969 nahm eine "Conference of the Committee on Disarmament of the UN (CCD)" ihre Tätigkeit auf. Der angebliche Einsatz von Sarin gegen eigene Kräfte (Deserteure) in der "Operation Tailwind" im September 1970 in Laos entpuppte sich als politisch motivierte Falschmeldung. 1975 gab es 30 Teilnehmerstaaten für ein CWÜ; darunter waren auch die Bundesrepublik und die DDR. 1976 fanden bilaterale Verhandlungen von USA und UdSSR statt. Die Verhandlungen wurden im selben Jahr unterbrochen. Erst 1979 einigten sich die USA und UdSSR weitgehend über die Grundstruktur des Vertrags und weitgehend auch über Verifikationsmaßnahmen; ungelöst blieb aber die Frage von Ad-hoc-Verdachtskontrollen vor Ort. 1979 gab es ein "Committee on Disarmament of the United Nations" (CD); es hatte 40 Teilnehmerstaaten. 1980 bildete sich ein "Ad Hoc Committee on Chemical Weapons".

1981 beschuldigte der US-amerikanische Außenminister Alexander Haig die UdSSR und die von ihr unterstützte Vietnamesische Volksarmee, im Zweiten Laotischen Bürgerkrieg (1963–73) Mykotoxine eingesetzt zu haben, um Tausende von Hmong zu töten. Diese Vorwürfe konnten nicht bewiesen werden.

Ende der 1980er Jahre erkannte das US-Militär, dass die bisherigen, lange gelagerten Chemiewaffen bis spätestens 1990 zum Großteil zersetzt und damit militärisch unbrauchbar sein würden; daher unterschrieb Präsident Ronald Reagan 1987 ein Gesetz, um die alten chemischen Kampfstoffe zu zerstören und gegen neue, binäre Kampfstoffe zu ersetzen. Bei diesen wird nicht der endgültige und wirksame chemische Kampfstoff bereitgehalten, sondern verschiedene, stabilere und weniger korrosive Komponenten, die beim Einsatz der binären Waffen dann erst zum Wirkstoff reagieren.

Nach dem Ende des Kalten Krieges um 1990 änderte sich die geostrategische Lage deutlich. Es kam zu zahlreichen Abrüstungsverhandlungen zwischen westlichen Staaten und Nachfolgestaaten der Sowjetunion. Chemische Waffen (oft lagerten sie in inzwischen rostigen Tanks) galten vielen inzwischen als Altlast.

Am 3. September 1992 wurde das CWÜ von den Mitgliedstaaten der Genfer Abrüstungskonferenz (UNCD) verabschiedet.
Seit 13. Januar 1993 kann es unterzeichnet werden. Eine Unterzeichnung erfolgte durch etwa 150 Staaten, darunter USA und Russland.

Deutschland hat die Konvention 1994 ratifiziert, Österreich und die Schweiz 1995.

Am 29. April 1997 trat das Chemiewaffenübereinkommen in Kraft. 1997 erfolgte die Ratifizierung auch durch die USA und Russland. Die ratifizierenden Staaten haben sich durch das CWÜ unter anderem dazu verpflichtet, bis zum Jahr 2012 sämtliche Chemiewaffen unter internationaler Aufsicht zu vernichten.

Stand Dezember 2013 sind 190 Staaten der Konvention beigetreten.
Als jüngstes Ratifizierungsland ist Syrien der Konvention am 14. September 2013 beigetreten. Im Januar 1993 unterzeichnet, aber bis heute noch nicht ratifiziert wurde der Vertrag von Israel und Myanmar. Vier Staaten haben die Konvention bisher weder unterzeichnet noch ratifiziert: Ägypten, Angola, Nordkorea und Südsudan. Die Einhaltung des Abkommens wird durch die Organisation für das Verbot chemischer Waffen, OVCW (englisch Organisation for the Prohibition of Chemical Weapons, OPCW) überwacht. Die OVCW ist eine internationale Organisation mit Sitz in Den Haag.

Schon zu Beginn des Ersten Golfkriegs setzte die irakische Armee auf Weisung Saddam Husseins chemische Waffen gegen den Iran ein. So warf die irakische Luftwaffe bereits 1980 speziell dafür entwickelte Kanister mit chemischen Kampfstoffen über iranischen Stellungen ab. Bekanntheit erlangte der Giftgasangriff auf die Fernverkehrsstraße am 9. August 1983 Rawanduz–Piranschahr.

Insgesamt wurden etwa 100.000 iranische Soldaten Opfer von Gasangriffen. Viele davon wurden durch Senfgas, das von einer mit deutscher Unterstützung gebauten Insektizid-Fabrik in Samarra in größerem Maße ab 1983 hergestellt wurde, verwundet. Etwa 20.000 davon wurden während des Einsatzes sofort hauptsächlich durch die Nervenkampfstoffe Tabun und VX getötet. Diese Zahlen schließen allerdings keine Zivilisten ein. Da Giftgas während der Kämpfe auch auf Stellungen und Posten abgeworfen wurde, die sich in oder um Dörfer befanden und deren Einwohner keine Möglichkeit hatten, sich gegen die Gase zu schützen, gab es auch unter der Zivilbevölkerung sehr viele Opfer. Außerdem wurden durch den Einsatz verschiedener Gase Gebiete mit gefährlichen chemischen Schadstoffen kontaminiert.

Der Irak setzte chemische Waffen auch gezielt ein, um Zivilisten zu töten. Tausende wurden bei Giftgasangriffen auf Dörfer, Städte und Frontkrankenhäuser getötet, so auch beim Giftgasangriff auf Sardasht vom 28. Juni 1987. Bekanntestes Beispiel ist der Giftgasangriff auf Halabdscha am 16. März 1988, bei dem etwa 5.000 irakische Kurden getötet und 7.000 bis 10.000 so schwer verletzt wurden, dass viele von ihnen später starben. Die irakischen Streitkräfte setzten mehrere verschiedene Gase gleichzeitig ein. Dazu gehören Nervenkampfstoffe wie Tabun, Sarin und möglicherweise VX, aber auch Senfgas und ein Cyanidkampfstoff.

Im Rahmen der Vorbereitung auf den Ersten und Zweiten Irakkrieg kam es zu Auseinandersetzungen zwischen den USA und Deutschland über die Herkunft der irakischen Chemiewaffentechnologie.

1995 kam es beim Terror-Anschlag der japanischen Aum-Sekte zur Freisetzung des Nervengifts Sarin in der U-Bahn von Tokyo. Es gab 13 Tote und 6.252 Verletzte. Ein früherer Anschlag der Sekte mit 7 Toten und 500 Verletzten wurde erst im Nachhinein bekannt.

Im Oktober 2002 verwendeten russische Sicherheitskräfte in Moskau vermutlich das Opioid Carfentanyl und das Anästhetikum Halothan in Form eines Aerosol-Gas-Gemischs, um Terroristen kampfunfähig zu machen, die in einem Musical-Theater 800 Geiseln festhielten. Alle Geiselnehmer und über 129 Geiseln kamen ums Leben, die meisten aufgrund des Gases. Viele erlagen im Krankenhaus ihren Vergiftungen, wozu möglicherweise auch die fehlende Zusammenarbeit der Sicherheitskräfte mit den Ärzten beigetragen hat. Der Einsatz von Carfentanyl wurde offiziell nie bestätigt, möglicherweise im Hinblick auf die von Russland ratifizierte Chemiewaffenkonvention.

Während des Irakkrieges setzte eine Terrororganisation, bei der es sich Berichten zufolge um die al-Qaida handelte, chemische Waffen hauptsächlich gegen Zivilisten ein, aber auch gegen US-Soldaten und irakische Soldaten und Polizisten. Bei dem eingesetzten Gas handelte es sich um Chlorgas. Da die Anschläge alle unter freiem Himmel durchgeführt wurden, war die Zahl der Todesopfer meistens gering, die Zahl der Verletzten betrug jedoch oft mehrere hundert. Zu den am meisten wahrgenommenen Giftgasanschlägen im Irak zählen der Anschlag auf eine Polizeiwache am 6. April 2007 mit 27 Toten und der Anschlag auf einen Dorfmarkt in Abu Sayda am 15. Mai 2007 mit 45 Toten.

Im Umland von Damaskus sind laut Chemiewaffeninspektoren der UNO in mehreren Dörfern Kampfmittel mit Sarin zum Einsatz gekommen. Der mögliche Einsatz von chemischen Waffen in drei weiteren Orten (Chan al-Asal und Scheich Maksud in der Provinz Aleppo sowie Sarakib, einer Kleinstadt nahe der Provinzhauptstadt Idlib,) soll untersucht werden.

Ein Untersuchungsbericht der Vereinten Nationen meldete im August 2016, man könne "die Nutzung der weltweit geächteten Massenvernichtungswaffen in drei Fällen eindeutig belegen und zuordnen." In zwei Fällen habe die syrische Armee die Giftbomben abgeworfen, in einem Fall die Miliz Islamischer Staat (IS). Diese Fälle waren der Einsatz von Chlorgas und eventuell anderer "giftiger Substanzen", die am 21. April 2014 und am 16. März 2015 in zwei Dörfern in der nordwestlichen Provinz Idlib aus Hubschraubern der syrischen Luftwaffe abgeworfen wurden. Die IS-Miliz verwendete am 21. August 2015 im Ort Marea nahe Aleppo Senfgas.

Als chemische Kampf"mittel" bezeichnet man jede Art von Gegenständen (Munition, Schweltöpfe, aber auch im strengen Sinne z. B. einfache Flaschen), die es ermöglichen, einen chemischen Kampf"stoff" zu transportieren. Sie lassen sich nach ihrem Angriffsgebiet am menschlichen Körper beziehungsweise ihrer Wirkung einordnen. Eine Grenzziehung zwischen den einzelnen Gruppen ist dabei aber nicht immer eindeutig möglich. Auch ist bei manchen dieser Gruppen bereits die bloße Zuordnung zu den chemischen Kampfstoffen umstritten. Detaillierte Übersichtsarbeiten wurden von V. Pitschmann und von K. Ganesan u.a. vorgelegt.

Die chemischen Kampfmittel an sich werden in folgende Kategorien unterteilt:

Die chemischen Kampfstoffe im klassischen Sinn können erneut in verschiedene Kampfstoffklassen unterteilt werden, je nach Art und Ort ihrer Wirkung:


Viele chemische Kampfstoffe werden bevorzugt als Binärkampfstoffe eingesetzt, etwa die Nervenkampfstoffe Sarin, Soman und VX. Dabei werden zwei oder mehr im Vergleich zum Endstoff relativ ungefährliche Substanzen voneinander getrennt in einem Geschoss gelagert. Der eigentliche Kampfstoff entsteht erst nach dem Abschuss meist durch einfaches Vermischen der Komponenten, teilweise unter Zuhilfenahme eines geeigneten Reaktionsbeschleunigers. Vorteile sind die relativ gefahrlose Lagerung und Handhabung, da die verwendeten Chemikalien meist weniger giftig sowie besser lagerfähig als die Kampfstoffe selbst sind, das heißt, es tritt keine oder nur geringe Zersetzung der Chemikalien oder Korrosion der Geschosse auf.

Im Gegensatz zu den frühen Kampfstoffen, die gasförmig waren, werden heute überwiegend flüssige Kampfstoffe (selten auch Feststoffe) verwendet. Diese werden als Aerosol eingesetzt. Man unterscheidet hierbei nach der Tropfengröße zwischen zwei Einsatzarten: flüchtig und sesshaft.

Beim flüchtigen Einsatz werden sehr kleine Tropfen verwendet, die größtenteils augenblicklich verdampfen, so dass sehr schnell eine hohe Konzentration des Kampfstoffes wirksam werden kann (50 % als Dampf und 50 % als Feinaerosol). Die Belegungsdichte wird so gewählt, dass ein Atemzug in den meisten Fällen tödliche Mengen des Kampfstoffes enthält. Durch die rasche Verdampfung sollte das Gebiet nach maximal vier Stunden wieder ohne Schutz passierbar sein. Ziel des Angriffes ist es, den Gegner im angegriffenen Gebiet stark zu schwächen, um einen Durchbruch zu erleichtern, jedoch ohne die eigenen Truppen durch Schutzanzüge zu behindern. Am besten für einen flüchtigen Einsatz geeignet sind Sarin, Soman und Tabun (zusammengefasst unter dem Begriff G-Stoffe oder Trilone) oder Blausäure. Letztere stellt eine Ausnahme dar, da sie sehr leicht flüchtig ist und schon nach wenigen Minuten nicht mehr nachzuweisen ist (maximal 15 Minuten). Man spricht hier von einem superflüchtigen Kampfstoff. Wahrscheinlichste Einsatzmittel sind Mehrfachraketenwerfer und Fliegerbomben (eventuell mit Submunition), da diese eine sehr hohe Belegungsdichte ermöglichen.

Beim sesshaften Einsatz werden vergleichsweise große Tropfen (0,1 mm bis 1 mm Durchmesser) eingesetzt. Aufgrund der Größe fallen die Tropfen schneller, die Dampfkonzentration ist wesentlich kleiner (20 % Dampf, 80 % Tropfen) und ein Großteil des Kampfstoffes erreicht den Boden, wo er je nach Art des Kampfstoffes und der Witterung mehrere Wochen verbleiben kann. Ziel des Angriffes ist nicht die unmittelbare Vernichtung des Feindes, sondern die Einschränkung seiner Handlungsfreiheit. Schutz- und Dekontaminationsmaßnahmen kosten Zeit, kontaminiertes Gebiet ist nur mühsam zu durchqueren und die Moral der Truppe leidet erheblich. Des Weiteren müssen kontaminierte Truppenteile ersetzt und evakuiert werden, bevor die Schutzanzüge gesättigt sind (normalerweise nach spätestens 12 Stunden). Die wahrscheinlichsten Ziele sind gegnerische Flankenstellungen (um deren Gegenangriff zu erschweren oder zu verhindern), Artilleriestellungen (Ausschalten der Feuerunterstützung), Kommandostände und Nachschubwege. Am besten für diese Einsatzart geeignet sind Yperit (Senfgas/Lost) und V-Stoffe (namentlich VX). Die möglichen Einsatzmittel sind vielfältig, da nicht auf die Belegungsdichte geachtet werden muss (Artillerie, Bomben, Sprühflugzeuge, Raketen, Marschflugkörper etc.). Eine Sonderform des sesshaften Einsatzes ist der Einsatz verdickter Kampfstoffe. Dem Kampfstoff werden hierzu Verdickungsmittel beigemischt, um dessen Viskosität und damit die Tropfengröße weiter zu erhöhen. Dies führt wiederum zu einer geringeren Verdunstungsrate und damit größerer Sesshaftigkeit. Des Weiteren wird die Dekontamination stark erschwert. Hauptziele wären z. B. Flugplätze, um deren Benutzung langfristig zu verhindern.

Seit 1997 sind chemische Waffen durch die Chemiewaffenkonvention international offiziell geächtet; auch die Entwicklung, Herstellung und Lagerung sind verboten. Dennoch bleiben die USA nach wie vor größter Besitzer chemischer Kampfstoffe.

Mitte Juli 2007 wurde mitgeteilt, dass Albanien als weltweit erster Staat seine sämtlichen Bestände an Chemischen Waffen nachweislich vernichtet hat. Die Finanzierung des Projektes erfolgte mit insgesamt 48 Millionen US-Dollar. Die Vernichtung der Kampfstoffe Schwefellost, Lewisit, Adamsit und Chloracetophenon dauerte von Februar bis Juli 2007. Die Technologie und damit die Anlage für die Vernichtung der Kampfstoffe wurde von einem renommierten deutschen Anlagenbauer gestellt. Der Betrieb der Vernichtungsanlage erfolgte ebenfalls durch deutsches Personal.

In Deutschland wurden chemische Kampfstoffe im Zweiten Weltkrieg unter anderem bei der Firma ORGACID in Halle-Ammendorf und in beiden Weltkriegen in Munster hergestellt. Nach Ende des Krieges verblieben beträchtliche Mengen an Waffen in den Produktionsstätten. Sie wurden von den Alliierten beschlagnahmt und auf diverse Schiffe (z. B. SMS "Berlin") geladen, die dann im Skagerrak versenkt wurden. Aus heutiger Sicht wäre dies eine Umweltstraftat, war aber damals erlaubt.

Heute ist an den ehemaligen Produktionsstandorten nur noch verseuchter Boden übrig, der in zwei Entsorgungsanlagen der Gesellschaft zur Entsorgung von chemischen Kampfstoffen und Rüstungsaltlasten mbH (GEKA) kontrolliert vernichtet wird. In den Anlagen der bundeseigenen Gesellschaft wird kontaminierter Boden zuerst „gewaschen“, um die hochkontaminierten Bereiche abzutrennen. Diese werden mit Kalk vermischt und in einer Plasmaanlage bei 1.350 bis 1.550 °C im Lichtbogen geschmolzen. Es entsteht dabei nach dem Abkühlen glasartige Schlacke, in der nichtbrennbare Stoffe gebunden sind sowie Verbrennungsgase. Mit Chemikalien befüllte Munition wird vorher in einem so genannten Sprengofen gesprengt. In beiden Fällen werden die Gase ausgewaschen und anschließend die Salze ausgefällt.

Am 1. April 2006 wurde die zweite russische Anlage zur Vernichtung von Chemiewaffen in Kambarka, Republik Udmurtien in Betrieb genommen. In der Anlage, die mit deutscher Hilfe finanziert wurde, sollten 6.350 t arsenhaltiger Hautkampfstoff beseitigt werden, deren Vernichtungskosten über 270 Millionen Euro betragen. Deutschland trägt davon 90 Millionen Euro. Die erste C-Waffen-Vernichtungsanlage wurde im Dezember 2002 in der Kleinstadt Gorny im Gebiet Saratow am Mittellauf der Wolga gebaut. Außerhalb von Potschep, im Gebiet Brjansk, lagern abgefüllt in über 67.000 Fliegerbomben rund 7.500 t der Nervenkampfstoffe VX, Sarin und Soman. In einem ersten Schritt wurden die Kampfstoffe von russischer Seite waffenuntauglich gemacht und ab 2009 eine Anlage mit Hochturbulenzreaktoren zur thermischen Entsorgung der Kampfstoffe in Betrieb genommen. Russland übernahm von der ehemaligen Sowjetunion rund 40.000 Tonnen Chemiewaffen, die bis 2012 vernichtet werden sollen.

Die etwa 400 km östlich von Moskau gelegene Stadt Dserschinsk wurde 2006, 2007 und 2013 vom amerikanischen Blacksmith Institute zu einem der zehn am stärksten verseuchten Orte der Welt „nominiert“. Wasser und Böden sind hier hochgradig mit Chemikalien aus der Zeit der Chemiewaffenproduktion im Kalten Krieg verseucht, da neben Leckagen und anderen Unfällen in den Jahren 1930 bis 1998 etwa 300.000 Tonnen chemischer Abfälle unsachgemäß entsorgt wurden. Über laufende Sanierungsmaßnahmen ist bislang nichts bekannt.

Am 27. September 2017 wurde bekanntgegeben, dass der letzte chemische Sprengkopf in der Entsorgungsanlage Kisner in Udmurtien vernichtet wurde. Die Organisation für das Verbot chemischer Waffen bestätigte die Vernichtung aller russischen Chemiewaffen und gratulierte Russland, das somit früher als ursprünglich geplant chemiewaffenfrei ist. Der russische Präsident Wladimir Putin beobachtete den Vorgang per Videozuschaltung und forderte nun auch die USA auf, das Abkommen zu achten und die amerikanischen Chemiewaffen ebenfalls rasch zu vernichten.

Die USA nutzten ab Ende der 1980er Jahre bis Ende der 1990er Jahre eine Anlage für die Vernichtung von chemischen Kampfstoffen auf dem Johnston-Atoll im Pazifik.

Die Vernichtung von 90 % der C-Waffen der USA (31.000 Tonnen waren insgesamt deklariert worden) in den letzten zwei Jahrzehnten durch Verbrennung hat 35 Milliarden US-Dollar gekostet, nach anderen Angaben 28 Milliarden Dollar.

Die Reste des US-Chemiewaffenarsenals befinden sich in zwei Armeelagern in den Bundesstaaten Colorado und Kentucky. Die vollständige Vernichtung aller amerikanischen chemischen Kampfstoffe ist für 2023 geplant.

Russland schlug im September 2013 vor, Syrien möge seine Chemiewaffen unter westlicher Aufsicht zerstören. Die USA, die zuerst mit einem militärischen Schlag gedroht hatten, setzten dann auf eine diplomatische Lösung. Syrien hat nunmehr am 14. September 2013 den Beitritt zur OPCW ratifiziert, welcher 30 Tage später vertragsgemäß in Kraft trat. Alle Anlagen zur Produktion der Waffen und zum Abfüllen von Munition sollen nach Angaben der OPCW unmittelbar danach zerstört worden sein. 600 Tonnen Chemikalien wurden dabei auf dem US-Spezialschiff "MV Cape Ray (T-AKR-9679)" auf dem Mittelmeer neutralisiert. Die neutralisierten Chemikalien wurden in Deutschland und Finnland entsorgt. In Deutschland erfolgte die Verbrennung von 340 t Hydrolysats und 30 t sonstiger kontaminierter Abfälle ab September 2014 bei der GEKA.

Die britische Boulevard-Zeitung Daily Mail behauptete am 7. September 2013, dass von 2004 bis 2010 die britische Regierung fünfmal zwei britischen Firmen die Lieferung der Chemikalie Natriumfluorid bewilligt habe, die zur Synthese von fluorhaltigem Sarin verwendet werden kann.

Auf Anfrage der Fraktion Die Linke gab die deutsche Regierung am 18. September 2013 bekannt, dass zwischen 2002 und 2006 insgesamt 137 Tonnen Fluorwasserstoff, Ammoniumhydrogendifluorid, Natriumfluorid sowie Zubereitungen mit Kalium- und Natriumcyanid nach Syrien exportiert worden sind. Syrien hat eine geplante Verwendung dieser Dual-Use-Güter für zivile Zwecke plausibel dargestellt. Die Ausfuhrgenehmigung sei erst nach „sorgfältiger Prüfung aller eventueller Risiken, einschließlich von Missbrauchs- und Umleitungsgefahren im Hinblick auf mögliche Verwendungen in Zusammenhang mit Chemiewaffen“ erteilt worden, so das Wirtschaftsministerium.





</doc>
<doc id="915" url="https://de.wikipedia.org/wiki?curid=915" title="Clipboard">
Clipboard

Ein Clipboard ist der englischsprachige Begriff für



</doc>
<doc id="916" url="https://de.wikipedia.org/wiki?curid=916" title="Lake Champlain">
Lake Champlain

Der Lake Champlain (‚Champlainsee‘, frz. "Lac Champlain") ist das achtgrößte Binnengewässer in den Vereinigten Staaten.

Er liegt südlich von Montreal zwischen den Green Mountains und den Adirondack Mountains an der Grenze der US-Staaten Vermont und New York und hat im Norden noch geringen Anteil in der kanadischen Provinz Québec. Er ist nach Samuel de Champlain benannt, der ihn 1609 erforschte und ab dem 4. Juli des Jahres befuhr. Zu diesem Zeitpunkt galt er als Grenze zwischen den Stämmen der Algonkin im Westen und den Irokesen im Osten, was sich auch in den Namen ausdrückte, die die Urbewohner dem See gegeben hatten: "Pe-Tonbonque" (etwa: „Wasser, das zwischen ihnen [den Stämmen] liegt“) bei den Algonkin, "Caniaderi-Guarunte" (etwa: „See, der das Tor zum Land ist“), bei den Irokesen.

Im etwa 180 km langen und bis zu 19 km breiten Champlainsee liegen etwa 80 Inseln, von denen eine ein eigenes County im US-Staat Vermont bildet. Er ist nach den Großen Seen im Mittleren Westen, dem Iliamnasee in Alaska und dem Lake Okeechobee in Florida der größte See der Vereinigten Staaten.

Nach Entdeckung des Sees durch Samuel de Champlain 1609 gehörte dieser mit dem umliegenden Gebieten zu Neufrankreich, bis dieses im Verlaufe des Siebenjährigen Krieges (1754–1763) von den Briten erobert wurde. Die Franzosen erbauten mehrere Forts entlang des Sees von denen das bedeutendste Fort Carillon war, welches 1755 zu Beginn des Siebenjährigen Krieges am Südende des Sees errichtet wurde. Das Fort war mehrfach der Schauplatz militärischer Auseinandersetzungen zunächst zwischen Franzosen und Briten und dann zwischen Briten und Amerikanern.

Während des Amerikanischen Unabhängigkeitskrieges (1775–1783) kam es auf dem Lake Champlain am 11. Oktober 1776 zur Schlacht von Valcour, der ersten Seeschlacht der United States Navy. Sie endete mit einem Sieg der Briten.

Am 11. September 1814 erstritt die amerikanische Flotte unter Commodore Thomas Macdonough einen Sieg gegen die anrückenden Briten, als diese von Kanada aus eine Invasion des Staates New York versuchten (Schlacht bei Plattsburgh).

Im 19. Jahrhundert wurde der Champlainsee durch ein Kanalsystem mit dem Hudson River verbunden. Die Häfen Burlington, Port Henry und Plattsburgh sind nur noch von geringem kommerziellen Interesse.

Bekannt wurde das Gewässer durch das Gerücht eines Seeungeheuers, das nach seinem vermeintlichen Entdecker de Champlain "Champ" genannt wird. Tatsächlich notierte de Champlain in seinen Unterlagen lediglich die Sichtung eines ihm unbekannten Fisches, dessen Beschreibung auf einen im See lebenden Stör passt. Seit 1982 steht der entfernte Verwandte "Nessies" unter gesetzlichem Schutz, da er zu einer wichtigen Touristenattraktion geworden ist.

Der Champlainsee umfasst auch ungefähr 80 Inseln, einschließlich eines ganzen Countys in Vermont.



Die Lake Champlain Transportation Company bietet nördlich von Ticonderoga drei Fährverbindungen an:



</doc>
<doc id="918" url="https://de.wikipedia.org/wiki?curid=918" title="Cha-Cha-Cha">
Cha-Cha-Cha

Der Cha-Cha-Cha [] ist ein moderner, paarweise getanzter Gesellschaftstanz kubanischen Ursprungs.

Der Cha-Cha-Cha in seiner weltweit verbreiteten westlichen Variante gehört zu den lateinamerikanischen Tänzen des Tanzsports und wird als Bestandteil des Welttanzprogramms in Tanzschulen unterrichtet. Die ursprüngliche kubanische Form des Cha-Cha-Chas unterscheidet sich in Technik und Figurenrepertoire stark vom heutigen Turniertanz und ist in der spanischen Schreibweise "Cha-cha-chá" [] in der modernen Salsa-Szene anzutreffen.

Die Geschichte des Cha-Cha-Chas wurde nur mündlich überliefert und wird leicht unterschiedlich wiedergegeben. Der Rhythmus des Cha-Cha-Chas wurde zwischen 1948 und 1951 von Enrique Jorrín erfunden, einem kubanischen Komponisten und Violinisten, der damals in der kubanischen Charanga "Orquesta América" spielte. Jorrín variierte in seinen Kompositionen seit 1948 beständig den kubanischen Tanzrhythmus Danzón: Unter anderem reduzierte er die für die kubanische Musik typische Synkopierung und fügte dem ursprünglich rein instrumentalen Musikstil rhythmische Gesangseinlagen hinzu.

1951 führte Jorrín den Cha-Cha-Cha-Rhythmus unter dem von ihm gewählten Namen "neodanzón" (span. „neuer Danzón“) auf den kubanischen Tanzflächen ein. 1953 nahm die Orquesta América Jorríns Hits "La Engañadora" und "Silver Star" auf. Der neue Rhythmus kam beim Publikum sehr gut an und inspirierte die Tänzer zu einem Tanzschritt, der den Grundschritt des Mambo um einen schnellen Wechselschritt ergänzt. Dieser schnelle Wechselschritt verursachte laut Jorrín ein scharrendes Geräusch, das für ihn wie "cha cha chá" klang, und das er als rhythmische Gesangseinlage in einige seiner Lieder einbaute. Dieses Geräusch und die daraus resultierende rhythmische Zählweise "2 3 Cha-Cha-Cha" waren letztendlich namensgebend für den Tanz.

Der Cha-Cha-Cha verbreitete sich sehr schnell über die kubanische Grenze hinweg nach Mexiko und in die Vereinigten Staaten. In den Vereinigten Staaten avancierte der Cha-Cha-Cha über Nacht zum Modetanz des Jahres 1955, gestützt durch die legendären Mambo- und Cha-Cha-Cha-Orchester des Tanzsalons "Palladium" in New York City. Möglicherweise lag der große Erfolg des Cha-Cha-Cha im Entfernen der Synkopierung begründet, denn diese rhythmische Besonderheit erschwert westlichen Hörern das Tanzen und gilt als Mitursache für den schnellen Niedergang des Mambo.

Der Tanz erfuhr sehr früh technische Anpassungen an die Rumba. 1962 wurde er offiziell zu den Turniertänzen hinzugenommen und wies bereits damals die Grundform der heutigen Turniervariante auf. Einen großen Beitrag zur technischen Entwicklung lieferte Walter Laird, der mit seiner Tanzpartnerin Lorraine Reynolds in den Jahren 1962, 1963 und 1964 Latein-Weltmeister wurde und mehrere Tanzbücher verfasste. 1963 wurde der Cha-Cha-Cha als lateinamerikanischer Tanz in das Welttanzprogramm aufgenommen und gehört seither weltweit zum Grundstock allgemeiner Tanzschulen.

Cha-Cha-Cha-Musik ist heiter und unbeschwert. Der Tanz ist ein amüsanter und koketter Flirt zwischen den Tanzpartnern, die in frechen offenen und geschlossenen Figuren miteinander spielen. Er ist vorwitziger als die verträumt-erotische Rumba, aber weniger aufreizend als der überschäumende Samba. Die kubanische Variante ist insgesamt ruhiger und weicher.

Der Cha-Cha-Cha wird im -Takt notiert, hat die Hauptbetonung auf dem ersten Taktschlag und wird auf Turnieren in einem Tempo von 30 bis 32 Takten pro Minute gespielt und getanzt.

Cha-Cha-Cha ist in vielerlei Hinsicht ungewöhnlich für eine kubanische Musikform: Er wurde von Anfang an im -Takt notiert, während die meisten anderen kubanischen Musikformen ursprünglich im -Takt notiert und erst im Laufe der Zeit von Exilkubanern an die westliche -Notation angepasst wurden. Trotz seiner Wurzeln im "Danzón" hat er keinen Bezug zur Clave, dem aus Afrika stammenden Rhythmusschema, das fast alle kubanische Musik prägt. Schließlich weist er keine Synkopierung auf und steht damit unter anderem im Gegensatz zum Mambo, bei dem die Hauptbetonung vom ersten auf den vierten Taktschlag verschoben ist.

Seit Jorríns Tagen hat sich die Musik, auf die Cha-Cha-Cha getanzt wird, ständig verändert. Spielte das "Orquesta América" noch in der klassischen Charanga-Besetzung des "Danzóns" mit Perkussion, Klavier, Bass, Flöte und "Cuerdas", setzten sich bereits im "Palladium" fetzige Bläserbegleitungen durch. Zur nachhaltigen Beliebtheit des Cha-Cha-Cha trug auch bei, dass er problemlos mit der Metrik der westlichen Musik vereinbar ist. So wird Cha-Cha-Cha in Tanzschulen heute auf aktuelle Chart-Hits der Popmusik und des Latin Rock gelehrt.

Der Cha-Cha-Cha ist ein stationärer Tanz, wird also weitgehend am Platz getanzt. Wie die Rumba lebt auch der Cha-Cha-Cha von Hüftbewegungen. Die Schritte sollten eher klein ausfallen, da sonst die Hüftbewegung unnötig erschwert würde.

Charakteristisch für den Cha-Cha-Cha ist das Chassé auf „4 und 1“.

Der ursprüngliche kubanische Cha-Cha-Cha und die moderne Turniervariante unterscheiden sich in allen weiteren Punkten so sehr, dass sie im Folgenden getrennt dargestellt werden.

Die ursprüngliche Form des Cha-Cha-Cha ist dem Mambo sehr ähnlich und heute nur noch beim Freizeittanz in der Salsa-Szene anzutreffen. Der kubanische Cha-Cha-Cha wird klein und „erdig“ getanzt, das heißt die Schritte werden stets auf dem ganzen Fuß angesetzt und die Füße kaum vom Boden gehoben. Beim Chassé schleifen die Füße über das Parkett als wollte der Tänzer ein Blatt Papier über den Boden schieben. Die Hüftbewegung beim Chassé entsteht dadurch, dass die Schritte mit gebeugtem Knie angesetzt werden, wodurch sich die Hüfte wechselseitig absenkt. Der Grundschritt zeichnet sich durch eine leichte Vorwärts-Rückwärts-Bewegung aus:

Die lockere Tanzhaltung ist flexibel, was den Abstand der Partner zueinander betrifft, ausladende Armbewegungen gibt es keine. Das Figurenrepertoire ist durch den Platzwechsel Cross Body Lead und einfache Drehungen geprägt und etwa dem Basisrepertoire der modernen Salsa im geradlinigen Stil vergleichbar.

Die westliche Variante ist durch die Technik der lateinamerikanischen Tänze geprägt. Ähnlichkeiten zur Rumba sind in Grundschritt und Basic-Figuren zu finden, die Hüftbewegung hat jedoch durch das höhere Tempo und den schnellen Wechselschritt einen anderen Charakter, rotierende Anteile treten zurück. Die Füße sind in den Check- und Lockstep-Schritten wie in allen „echten“ lateinamerikanischen Tänzen (Salsa, Mambo, Rumba, eingeschränkt: Samba) leicht nach außen gedreht, die Schritte werden stets auf dem Fußballen angesetzt. Bei den langen Schritten auf den Taktschlägen 1, 2 und 3 wird das Bein ganz durchgestreckt und die Ferse flach aufgesetzt. Um die Musik besser rhythmisch akzentuieren zu können, erfolgen die Schrittansätze immer am Ende des ihnen jeweils zustehenden Zeitintervalls, dann aber sehr schnell ausgeführt. Die Betonungen auf „1“ und noch stärker „3“ sind deutlich zu machen; am Ende des Seitwärts-Chassés rotiert das gestreckte Spielbein aus, und zwar reaktiv durch das Senken in die Hüfte.

Die Schrittgröße und die Intensität der die Schritte einleitenden Hüftbewegungen korrelieren, was bei Betrachtung von Anfängern fälschlicherweise den Eindruck erwecken kann, man würde sich kaum bewegen.

Beim "Guapachá-Timing", einer rhythmischen Variante, belastet man nach einem Chassé den nächsten Vorwärts- bzw. Rückwärtsschritt erst auf der zweiten Hälfte des zweiten Taktschlages, so dass „4 und 1, "und" 3“ gezählt wird.

Cha-Cha-Cha-Folgen (oder Choreographien) zeigen häufig offene Figuren auf, d. h. die Partner tanzen ohne Berührung; ansonsten wird mehr in halboffener als in geschlossener Tanzhaltung getanzt. Da die Hüftbewegungen schnell sind, ist eine Kontrolle des Oberkörpers nötig, die nicht zu einer Versteifung führen darf. Die Arme unterstützen den Spannungsaufbau im Schultergürtel und dienen der Balance wie auch der optischen Vergrößerung der Figuren als Mittel der Präsentation.

Walter Lairds "The Technique of Latin Dancing" oder "Latin American Cha Cha Cha" (herausgegeben von der ISTD) fassen Grundschritte und Basic-Figuren (also keine Posen, Fall- und Hebefiguren) sowie weitere Hinweise zu Rhythmik und Ausführung zusammen, lassen aber weiten Raum für eine individuelle Interpretation jenseits der „bloßen“ Schritte. Die für Salsa und Mambo so typischen Figuren Cross Body Lead und alle darauf aufbauenden Figuren werden in diesen Werken nicht aufgeführt. Der Anfänger sollte die Figuren in etwa in der in den genannten Werken vorgesehenen Reihenfolge erlernen, wobei die Damendrehung, der New Yorker, Hip Twist, Fan, Hockeystick, Alemana, Opening Out, Three Chachas (Locksteps vor und zurück) als erster Einstieg und typische Figuren dienen können.

Die typische Musik für den Tanz, mit der der Tanz noch heute assoziiert wird, ist Tommy Dorseys Version von "Tea For Two Cha, Cha" vom September 1958.

Der Cha-Cha-Cha war Tanz des Jahres 2007. Zu diesem Anlass wurde von 323 deutschen und zwei österreichischen Tanzschulen am 3. November 2007 ein neuer Weltrekord im Cha-Cha-Cha-Massensimultantanzen mit 50.419 Teilnehmern aufgestellt. Der Tanz begann zeitgleich um 21:12 Uhr und dauerte ca. sechs Minuten.




</doc>
<doc id="919" url="https://de.wikipedia.org/wiki?curid=919" title="Computerprogramm">
Computerprogramm

Ein Computerprogramm oder kurz Programm ist eine den Regeln einer bestimmten Programmiersprache genügende Folge von Anweisungen (bestehend aus Deklarationen und Instruktionen), um bestimmte Funktionen bzw. Aufgaben oder Probleme mithilfe eines Computers zu bearbeiten oder zu lösen.

Ein Computerprogramm gehört zur Software eines Computers. Es liegt meist auf einem Datenträger als ausführbare Programmdatei, häufig im sogenannten Maschinencode vor, die zur Ausführung in den Arbeitsspeicher des Rechners geladen wird. Das Programm wird als Abfolge von Maschinen-, d. h. Prozessorbefehlen von dem oder den Prozessoren des Computers verarbeitet und damit ausgeführt. Unter Computerprogramm wird auch der Quelltext des Programms verstanden, aus dem im Verlauf der Softwareentwicklung der ausführbare Code entsteht.

Eine Programmdatei, die aus Maschinencode besteht, enthält Befehle aus dem Sprachschatz des Prozessors, d. h. Befehle, die für den Prozessor „verständlich“ und damit ausführbar sind. Die Erstellung eines solchen Programms bezeichnet man allgemein als Programmierung oder auch als Implementierung. In den Anfängen der Programmierung wurde – bis zur Entwicklung von Programmiersprachen – ausschließlich in Maschinencode programmiert. Der Programm- bzw. Quelltext, den der Programmierer in einer Programmiersprache abgefasst hat, besteht aus einer Abfolge von (zumeist der englischen Sprache entnommenen) Anweisungen, die für den Programmierer im Allgemeinen verständlicher sind (z. B. ADD, SUB, AND, OR) als der Maschinencode. Später ergänzten Schleifen, Abstraktion und modularen Aufbau die höheren Programmiersprachen.

Dateien, in denen der Programmcode gespeichert ist, sind meist durch eine Dateiendung gekennzeichnet. Quelltextdateien weisen damit auf die verwendete Hochsprache hin ("<programm>.c": ein in C formuliertes Programm). Sie kann im Allgemeinen mit einem einfachen Texteditor bearbeitet werden. Eine Datei, die dagegen Maschinencode enthält, besitzt keine oder eine betriebssystemspezifische Endung, die lediglich auf ihre Ausführbarkeit hinweist ("<programm>.exe" bei MS-DOS und Windows; "<programm>" bei unixoiden Systemen). Sie kann oft als Kommando in einem Terminal (Eingabeaufforderung) aufgerufen werden. Siehe auch Programmbibliothek.

Damit ein in einer Hochsprache geschriebenes Programm auf einem Prozessor ausgeführt werden kann, muss es in Maschinencode übersetzt werden. Eine Anweisung einer höheren Programmiersprache wird im Allgemeinen in mehrere Maschinenbefehle übersetzt. Den Übersetzungsvorgang nennt man Kompilierung. Um aus dem Quelltext den Maschinencode zu generieren, wird ein Assembler, Compiler oder Interpreter benötigt. Dieser übersetzt die Anweisungen der Programmiersprache, die für menschliche Benutzer verständlich und bearbeitbar sein sollen, in die semantisch entsprechenden Befehle der Maschinensprache des verwendeten Computers.

Die Anweisungen, die (als Teil von Programmen) einen konkreten Lösungsweg repräsentieren, werden als Algorithmus bezeichnet; Beispiel: Berechnen der Mehrwertsteuer.

Im Sprachgebrauch wird Computerprogramm meist zu "Programm" verkürzt oder der Begriff "Software" verwendet. Allerdings ist "Computerprogramm" kein Synonym zu "Software", da mit Software die komplette, für den Benutzer fertige Anwendung gemeint ist. Diese umfasst zusätzliche Ressourcen wie Betriebssystem, Datenbanken, Grafik- und Audiodateien, Schriftarten oder Hilfetexte.

Ein größeres Computerprogramm besteht meist aus mehreren Modulen – die entweder zum Programm selbst 'gehören' oder die als 'Bausteine' (Unterprogramme) aus bereits bestehenden Programmbibliotheken bei der Ausführung des Programms benutzt werden. Im umgekehrten Fall können Computerprogramme Teil eines übergeordneten, ein größeres Aufgabengebiet abdeckenden "Anwendungssystems" sein; Beispiel: Gehaltsabrechnung, Finanzbuchhaltung, Meldewesen.

Die Entwicklung von Computerprogrammen ist das Gebiet der Softwaretechnik. Je nach Komplexität der zu entwickelnden Computerprogramme geschieht dies im Rahmen von Projekten. Die Aktivitäten der Beteiligten werden dabei meist unter Anwendung von Vorgehensmodellen, speziellen Methoden und Werkzeugen zur Softwareentwicklung ausgeführt.

Neben den für Software im Allgemeinen geltenden Unterscheidungsmerkmalen lassen sich Computerprogramme (als Untervariante von Software) nach den folgenden, beispielhaft genannten Kriterien unterscheiden:

Als weltweit erstes Computerprogramm gilt eine Vorschrift für die Berechnung von Bernoulli-Zahlen, die Ada Lovelace in den Jahren 1842/1843 für die mechanische Analytical Engine von Charles Babbage erstellte. Das Programm konnte ihrerzeit nur von Hand ausgeführt werden, da es im 19. Jahrhundert noch keine funktionsfähige Maschine gab, die dazu in der Lage war.

In den Jahren 1936 bis 1941 entwarf Konrad Zuse die Rechner Z1 und Z3, die lange Befehlsfolgen auf einem Lochstreifen verarbeiteten, die ersten Computerprogramme, die auf realen Maschinen ausgeführt werden konnten. Die Rechner beherrschten die vier Grundrechenarten und Quadratwurzelberechnungen auf binären Gleitkommazahlen, der Lochstreifen enthielt jeweils eine Rechenoperation und eine Speicheradresse.

Auf Zuse geht auch die erste höhere Programmiersprache "Plankalkül" zurück. Damit lassen sich Probleme maschinenunabhängig formulieren und später in eine maschinenlesbare Form überführen.

Der EDVAC-Rechner, der auf einem Entwurf von John von Neumann aus dem Jahre 1945 basiert, hatte einen Quecksilber-Verzögerungsspeicher für 1024 Fest- oder Gleitkommazahlen mit jeweils 44 Bit. Jede Speicherzelle konnte statt einer Zahl auch einen Befehl aufnehmen. Bei diesem Rechnerkonzept war es möglich, die Befehle eines Computerprogramms vor der Ausführung zuerst in den Arbeitsspeicher zu übertragen. Das ist heute noch üblich. EDVAC wurde jedoch erst im Jahr 1951 teilweise fertiggestellt. Der Demonstrationsrechner Manchester SSE und der auf dem EDVAC aufbauende EDSAC-Rechner hatten schon vorher Programme aus dem Arbeitsspeicher ausgeführt.

Ende der 1950er-Jahre wurden Computer so leistungsfähig, dass spezielle Programme, Compiler genannt, Quelltexte in höheren Programmiersprachen automatisch in Maschinenbefehle, also ausführbare Programme, übersetzen konnten. Ausführbare Programme können dann, wie beim EDVAC, in den Speicher geladen und abgearbeitet werden.

Mit Fortran, COBOL, ALGOL und LISP entstanden in den späten 1950er-Jahren die ersten standardisierten höheren Programmiersprachen. Programme in diesen Sprachen laufen, durch einen entsprechenden Compiler übersetzt, auf unterschiedlichen Rechnern. Sie können teilweise auch noch auf modernen Computern eingesetzt werden.

Es soll ein Programm zur Bestimmung des größten gemeinsamen Teilers (ggT) zweier Zahlen erstellt werden. Zunächst muss ein geeigneter Algorithmus gefunden werden.

Der Euklidische Algorithmus, der bereits um 300 v. Chr. beschrieben wurde, ermittelt den größten gemeinsamen Teiler (ggT) zweier natürlicher Zahlen "A" und "B":

Sobald eine formale Beschreibung eines Algorithmus, also eine genau definierte Verarbeitungsvorschrift, vorliegt, kann der Algorithmus umgesetzt ("implementiert") werden. Dazu wird eine geeignete Programmiersprache ausgewählt.

Zur Umsetzung wird heute meist eine höhere Programmiersprache verwendet, die von einem Computer eventuell nicht direkt ausgeführt werden kann, sondern zuerst kompiliert oder interpretiert werden muss.

In Sprachen wie Pascal dienen Variablen, Ausdrücke, Vergleiche, Zuweisungen und Kontrollstrukturen zur Umsetzung des ggT-Algorithmus:
Der gleiche Algorithmus in der Programmiersprache Python:

Bei der Umsetzung wird mit der Prüfung von Schritt 3 begonnen. Der ursprüngliche Algorithmus berücksichtigt nicht den Fall, dass A und B bereits zu Beginn gleich sein können. Wäre es die Aufgabe, den größten Teiler von 103 und 103 zu finden, würde ein Mensch sofort das Ergebnis 103 nennen, er würde den Algorithmus gar nicht bemühen. Der originale Algorithmus würde aber null ergeben. Die Umsetzung auf einem Rechner muss auch alle Sonderfälle berücksichtigen. Durch das Vorziehen von Schritt 3 wird der Sonderfall hier korrekt behandelt.

Pascal und andere Programmiersprachen besitzen keine Operation zum Vertauschen von Zahlen. Dies muss daher in elementarere Schritte umgesetzt werden. Eine zusätzliche Variable H, eine sogenannte Hilfsvariable, erlaubt die Vertauschung mit Hilfe von drei Zuweisungen:
Auch dies ist ein kleiner Algorithmus.

Damit daraus ein korrektes Programm wird, muss der Algorithmus noch um Ein- bzw. Ausgabeanweisungen, oft jedoch auch um Variablen und eine Programmstruktur ergänzt werden. Diese sind nicht Teil des eigentlichen Algorithmus:
Ein solches Programm wird unter Verwendung eines Texteditors erstellt und als Quellcode in einer Datei oder Programmbibliothek (für Quellcode) gespeichert. Anschließend kann der Quellcode zu einer festen Ablaufanweisung für den Computer 'übersetzt’ werden. Hierzu ist ein Compiler erforderlich, der den Code aus der jeweiligen Programmiersprache in die Maschinensprache übersetzt und als Ergebnis ein "ausführbares Programm" erstellt, welches als Datei oder in einer Programmbibliothek (für ausführbare Programme) abgelegt wird. Dieses Programm kann dann über ein Betriebssystem zur Ausführung gestartet werden, und zwar beliebig oft (ohne neue Übersetzung).

Einige Programmiersprachen verwenden keinen Compiler, sondern einen Interpreter, der Programme erst zur Laufzeit in Maschinensprache übersetzt.

Eine weitere Möglichkeit besteht in der Verwendung von Zwischencode ("Bytecode"), der vom Compiler an Stelle des Maschinencodes generiert wird. Ein Beispiel dafür ist Java: Der Java-Compiler erzeugt Bytecode, welcher dann auf der sogenannten virtuellen Maschine ausgeführt wird. Die virtuelle Maschine interpretiert oder übersetzt dann den Bytecode für das darunterliegende Betriebssystem.

Ebenso muss in manchen Rechnerumgebungen, in der Regel bei Großrechnern, der vom Compiler erstellte Maschinencode noch mit einem Systemprogramm ('Linkage Editor' o. ä.) nachbearbeitet werden, wobei ggf. weitere Unterprogramme und Systemroutinen 'eingebunden' werden können. Erst so ist das entstandene Programm ausführbar.

Mittels spezieller Programme, sogenannter Decompiler, ist es in begrenztem Maße möglich, aus dem Maschinencode wieder einen in Hochsprache lesbaren Quelltext zu erzeugen.

Programme haben mindestens zwei klar getrennte Lebensphasen: Der Zeitraum bis zum Zeitpunkt der Kompilierung (inklusive) wird "Compilezeit" genannt, welche im Gegensatz zur "Laufzeit" steht. In der Compilezeit-Phase hat das Programm statische Eigenschaften, gegeben nur durch den festen Quellcode. Nach der Kompilierung und mit der Ausführung besitzt das binäre Programm dynamische Eigenschaften und Verhalten in zusätzlicher Abhängigkeit der jeweiligen Laufzeitumgebung (variierende Hardware, User-Interaktion etc.).

In detaillierterem Sinn lassen sich die Lebensphasen von Programmen auch als Software-Lebenszyklus verstehen. Demnach gehören zur inhaltlich präzisen Festlegung des Programm-Inhalts die Projektphasen "Problemstellung, Analyse und Entwurf", anschließend folgt die technische "Implementierung", in der das Programm in Form von Quelltext entsteht. Danach befindet es sich in der Phase "Einführung". Nach diesen Entstehungsphasen von Programmen folgt deren "produktive Nutzung", bei Bedarf werden Anpassungen und Erweiterungen ("Wartungs-/Pflegephase") vorgenommen.

Aus betriebswirtschaftlicher Sicht lassen sich auch Computerprogramme nach dem allgemeinen Produktlebenszyklus klassifizieren.

Ein Computerprogramm wird urheberrechtlich geschützt, wenn es individuelles Ergebnis einer eigenen geistigen Schöpfung ihres Urhebers ist ( Abs. 3 UrhG). Mit Umsetzung der Urheberrechtsrichtlinie aus dem Jahre 2001 wurde die Schutzschwelle für Computerprogramme in den EG-Mitgliedsstaaten harmonisiert. Es genügt ein Minimum an Individualität für den Schutz (Kleine Münze). Es wird vermutet, dass sich die Individualität des Urhebers im Programm niedergeschlagen hat, wenn Spielraum dazu bestand. Geistiger Gehalt wird vermutet, wenn das Programm von einem menschlichen Urheber geschaffen wurde.




</doc>
<doc id="920" url="https://de.wikipedia.org/wiki?curid=920" title="Chemische Energie">
Chemische Energie

Als chemische Energie wird die Energieform bezeichnet, die in Form einer chemischen Verbindung in einem Energieträger gespeichert ist und bei chemischen Reaktionen freigesetzt werden kann. Der Begriff geht auf Wilhelm Ostwald zurück, der ihn 1893 in seinem Lehrbuch „Chemische Energie“ (im Begriffspaar „Chemische und innere Energie“) neben andere Energieformen („Mechanische Energie“, „Wärme“, „Elektrische und magnetische Energie“ sowie „strahlende Energie“) stellte.

Der Ausdruck Chemische Energie beschreibt makroskopisch die Energie, die mit elektrischen Kräften in Atomen und Molekülen verbunden ist und sich in chemischen Reaktionen auswirkt. Sie kann unterteilt werden in kinetische Energie der Elektronen und potentielle Energie der elektromagnetischen Wechselwirkung von Elektronen und Atomkernen. Sie ist eine Innere Energie, wie die thermische Energie und die Kernenergie.

In der Fachwissenschaft Chemie wird der Ausdruck „chemische Energie“ nicht verwendet. Er ist nur unter Angabe der Umgebungsbedingungen eindeutig definiert – dafür existiert dann jeweils eine andere etablierter Bezeichnung.

Oft ist mit chemischer Energie die Energie gemeint, die durch eine Verbrennung eines Stoffes (bei konstantem Druck) freigesetzt wird, also die Verbrennungsenthalpie. Der Satz von Hess ermöglicht die Berechnung der Energien bei Stoffumwandlungen aus den exakt definierten Bildungsenthalpien der beteiligten Verbindungen. Ähnliche Begriffe sind Heizwert und Brennwert, die jeweils auf die bei einer Verbrennung maximal nutzbare Wärmemenge abzielen.

Die chemische Energie darf nicht mit der chemischen Bindungsenergie verwechselt werden. Die chemische Bindungsenergie beschreibt die Festigkeit einer bestimmten chemischen Bindung, gibt also an, wie viel Energie dem Molekül zur Auflösung der Bindung "zugeführt" werden muss.

In anderen Naturwissenschaften, den Ingenieurwissenschaften usw. wird der Begriff der chemischen Energie in seiner teilweise unscharfen Form verbreitet verwendet. Obwohl manche Physikdidaktiker die Verwendung des Begriffs kritisieren („Der Begriff ist nützlich, wenn es um eine grobe Orientierung geht, erweist sich aber als widerspenstig, wenn man ihn streng zu fassen sucht. Im physikalischen Jargon ist er brauchbar, im physikalischen Kalkül überflüssig, zum physikalischen Verständnis hinderlich.“) wird er in praktisch allen Didaktik-Veröffentlichungen und Schulbüchern verwendet.

Aus "technischer Sicht" ist in Treibstoffen chemische Energie gespeichert, die durch deren Verbrennung, etwa beim Antrieb von Fahrzeugen, in mechanische Energie umgewandelt wird. Brennstoffzellen erlauben den Wandel von chemischer Reaktionsenergie einer Verbrennung direkt in elektrische Energie. Bei Nutzung von Batterien wird über elektrochemische Redoxreaktionen chemische Energie direkt in elektrische Energie gewandelt. Ein Akkumulator verhält sich bei der Nutzung der Energie ähnlich wie eine Batterie, kann aber auch umgekehrt elektrische Energie in chemische wandeln und so speichern.

Aus "biologischer Sicht" ist in organischer Nahrung chemische Energie gespeichert, die in ATP-Moleküle als Energieträger umgewandelt wird. Grüne Pflanzen beziehen ihre chemische Energie nicht aus organischer Nahrung, sondern aus dem Energiegehalt der Sonnenstrahlung, manche Bakterien aus Oxidationsenergie reduzierter Verbindungen (z. B. Fe oder CH). Die ATP-Moleküle innerhalb der biologischen Zellen erlauben, chemische, osmotische und mechanische Arbeit zu leisten.



</doc>
<doc id="923" url="https://de.wikipedia.org/wiki?curid=923" title="Cosimo de’ Medici">
Cosimo de’ Medici

Cosimo de’ Medici (genannt "il Vecchio" ‚der Alte‘; * 10. April 1389 in Florenz; † 1. August 1464 in Careggi bei Florenz) war ein Staatsmann, Bankier und Mäzen, der jahrzehntelang die Politik seiner Heimatstadt Florenz lenkte und einen wesentlichen Beitrag zu ihrem kulturellen Aufschwung leistete. Wegen seiner Zugehörigkeit zur Familie der Medici (deutsch auch „Mediceer“) wird er „de’ Medici“ genannt; es handelt sich nicht um ein Adelsprädikat, die Familie war bürgerlich.

Als Erbe der von seinem Vater gegründeten, stark expandierenden Medici-Bank gehörte Cosimo von Haus aus zur städtischen Führungsschicht. Der geschäftliche Erfolg machte ihn zum reichsten Bürger von Florenz. Den Rahmen für seine politische Betätigung bot die republikanische Verfassung der Stadt, die er im Prinzip respektierte, aber mit Hilfe seiner großen Anhängerschaft umgestaltete. Dabei setzte er sich gegen heftige Opposition einiger bisher tonangebender Familien durch. Sein maßgeblicher Einfluss auf die Politik beruhte nicht auf den Ämtern, in die er gewählt wurde, sondern auf dem geschickten Einsatz seiner finanziellen Ressourcen und einem ausgedehnten Netzwerk persönlicher Beziehungen im In- und Ausland. Es gelang ihm, ein dauerhaftes Bündnis mit Mailand, einer zuvor feindlichen Stadt, zuwege zu bringen und damit außenpolitische Stabilität zu schaffen, die nach seinem Tode anhielt.

Cosimos politische Erfolge, seine umfangreiche Förderung von Kunst und Bildungswesen und seine imposante Bautätigkeit verschafften ihm eine einzigartige Autorität. Dennoch konnte er Entscheidungen in heiklen Fragen nicht eigenmächtig treffen, sondern blieb stets auf Konsensbildung in der Führungsschicht angewiesen. Er achtete darauf, nicht wie ein Herrscher aufzutreten, sondern wie ein Bürger unter Bürgern.

Das außerordentliche Ansehen, das Cosimo genoss, spiegelte sich in der posthumen Verleihung des Titels "Pater patriae" („Vater des Vaterlandes“) wider. Mit seinem Vermögen ging die informelle Machtstellung, die er errungen hatte, auf seine Nachkommen über, die seine mäzenatische Tätigkeit in großem Stil fortsetzten. Bis 1494 spielten die Medici in der florentinischen Politik und im kulturellen Leben eine dominierende Rolle.

In der modernen Forschung werden Cosimos Leistungen überwiegend positiv beurteilt. Seine staatsmännische Mäßigung und Weitsicht, seine unternehmerische Kompetenz und sein kulturelles Engagement finden viel Anerkennung. Andererseits wird auch auf das große Konfliktpotenzial hingewiesen, das sich aus der massiven, andauernden Dominanz einer übermächtigen Familie in einem republikanischen, traditionell antiautokratischen Staat ergab. Längerfristig erwies sich Cosimos Konzept der indirekten Staatslenkung mittels eines Privatvermögens als nicht tragfähig; im letzten Jahrzehnt des 15. Jahrhunderts brach das von ihm etablierte System zusammen.

Nach dem Zusammenbruch des staufischen Kaisertums im 13. Jahrhundert war in Nord- und Mittelitalien, dem sogenannten Reichsitalien, ein Machtvakuum entstanden, das niemand auszufüllen vermochte. Wenngleich die römisch-deutschen Könige im 14. und 15. Jahrhundert weiterhin Italienzüge unternahmen (wie Heinrich VII., Ludwig IV. und Friedrich III.), gelang es ihnen nicht, die Reichsgewalt in Reichsitalien dauerhaft durchzusetzen. Die traditionelle Tendenz zur Zersplitterung der politischen Landschaft setzte sich im Spätmittelalter allgemein durch. Es bildete sich eine Vielzahl von lokalen und regionalen Machtzentren heraus, die einander fortwährend in wechselnden Konstellationen bekämpften. Die wichtigsten unter ihnen waren die großen Städte, die keine übergeordnete Gewalt akzeptierten und nach der Bildung größerer von ihnen kontrollierter Territorien strebten. Nördlich des Kirchenstaats waren die Hauptakteure das autokratisch regierte Mailand, die bürgerliche Republik Florenz und die Adelsrepublik Venedig, die nicht zu Reichsitalien gehörte. Die Politik war in erster Linie von den scharfen Gegensätzen zwischen benachbarten Städten geprägt. Oft bestand zwischen ihnen eine Erbfeindschaft; die größeren versuchten, die kleineren niederzuhalten oder völlig zu unterwerfen, und stießen dabei auf erbitterten Widerstand. Die Kosten der immer wieder aufflackernden militärischen Konflikte führten häufig zu einer gravierenden wirtschaftlichen Schwächung der beteiligten Kommunen, was jedoch die Kriegslust kaum dämpfte. Überdies wurden in den Städten heftige Machtkämpfe zwischen einzelnen Sippen und politischen Gruppierungen ausgetragen, die gewöhnlich zur Hinrichtung oder Verbannung der Anführer und namhaften Parteigänger der unterlegenen Seite führten. Ein Hauptziel der meisten politischen Akteure war die Wahrung und Vermehrung der Macht und des Ansehens der eigenen Familie.

Manche Kommunen wurden von Alleinherrschern regiert, die eine Gewaltherrschaft errichtet oder geerbt hatten. Diese von Republikanern als Tyrannis gebrandmarkte Regierungsform wird in der Fachliteratur als Signorie bezeichnet (nicht zu verwechseln mit "signoria" als Bezeichnung für einen Stadtrat). Sie war gewöhnlich mit Dynastiebildung verbunden. Andere Stadtstaaten hatten eine republikanische Verfassung, die einer relativ breiten Führungsschicht direkte Machtbeteiligung ermöglichte.

In Florenz, der Heimat der Medici, bestand traditionell eine republikanische Staatsordnung, die fest verankert war und von einem breiten Konsens getragen wurde. Es herrschte das in Gilden und Zünften organisierte, überwiegend kommerziell oder gewerblich tätige Bürgertum. Man hatte ein ausgeklügeltes System der Gewaltenteilung ersonnen, das gefährlicher Machtzusammenballung vorbeugen sollte. Das wichtigste Regierungsorgan war die neunköpfige Signoria, eine Ratsversammlung, deren Mitglieder sechsmal im Jahr neu bestimmt wurden. Die Kürze der zweimonatigen Amtszeit sollte tyrannischen Bestrebungen den Boden entziehen. Die Stadt, die 1427 etwa 40.000 Einwohner hatte, war in vier Bezirke geteilt, von denen jeder zwei "priori" (Mitglieder der Signoria) stellte. Zu den acht "priori" kam als neuntes Mitglied der "gonfaloniere di giustizia" (Bannerträger der Gerechtigkeit) hinzu. Er war der Vorsitzende des Gremiums und genoss daher unter allen städtischen Amtsträgern das höchste Ansehen, hatte aber nicht mehr Macht als seine Kollegen. Zur Regierung gehörten noch zwei weitere Organe: der Rat der "dodici buonomini", der „zwölf guten Männer“, und die sechzehn "gonfalonieri" (Bannerträger), vier für jeden Bezirk. Diese beiden Gremien, in denen die Mittelschicht stark vertreten war, nahmen zu politischen Fragen Stellung und konnten Gesetzesentwürfe blockieren. Zusammen mit der Signoria bildeten sie die Gruppe der "tre maggiori", der drei führenden Institutionen, die den Staat lenkten. Die "tre maggiori" schlugen neue Gesetze vor, doch konnten diese erst in Kraft treten, wenn sie von zwei größeren Gremien, dem dreihundertköpfigen Volksrat "(consiglio del popolo)" und dem zweihundert Mitglieder zählenden Gemeinderat "(consiglio del comune)", mit Zweidrittelmehrheit gebilligt worden waren. In diesen beiden Räten betrug die Amtszeit vier Monate.

Ferner gab es Kommissionen, die für besondere Aufgaben zuständig waren und der Signoria unterstanden. Die bedeutendsten von ihnen waren der achtköpfige Sicherheitsausschuss "(otto di guardia)", der für die innere Staatssicherheit zu sorgen hatte und die Geheimdienstaktivitäten lenkte, und die "dieci di balìa" („zehn Bevollmächtigte“), ein Gremium mit sechsmonatiger Amtszeit, das sich mit Außen- und Sicherheitspolitik befasste und im Kriegsfall die militärischen Aktionen plante und überwachte. Die "dieci di balìa" hatten die Fäden der Diplomatie weitgehend in den Händen. Daher wurden sie für die Medici, als diese die Staatslenkung übernahmen, ein zentrales Instrument bei der Steuerung der Außenpolitik.

Das in Florenz herrschende tiefe Misstrauen gegen übermächtige Personen und Gruppen war der Grund dafür, dass die meisten Amtsträger, vor allem die Mitglieder der "tre maggiori", weder durch Mehrheitsbeschluss gewählt noch aufgrund einer Qualifikation ernannt wurden. Sie wurden vielmehr aus der Menge aller als amtstauglich anerkannten Bürger – etwa zweitausend Personen – durch das Los ermittelt. Man legte die Zettel mit den Namen in Losbeutel "(borse)", aus denen dann die Zettel der künftigen Amtsträger blind gezogen wurden. Für die Signoria galt ein Verbot aufeinanderfolgender Amtszeiten. Man durfte nur einmal in drei Jahren amtieren, und es durfte niemand aus derselben Familie im vorigen Jahr dem Gremium angehört haben.

Die Berechtigung zur Teilnahme an den Auslosungen musste in bestimmten Zeitabständen – theoretisch alle fünf Jahre, faktisch etwas unregelmäßiger – überprüft werden. Diesem Zweck diente das "squittinio", ein Verfahren, mit dem festgestellt wurde, wer die Anforderungen der Amtstauglichkeit erfüllte. Zu diesen zählten Freiheit von Steuerschulden und Zugehörigkeit zu mindestens einer der Zünfte. Es gab „größere“ (das heißt angesehenere und mächtigere) und „kleinere“ Zünfte, und sechs der acht Priorensitze in der Signoria waren den größeren vorbehalten. Das Ergebnis des "squittinio" war jeweils eine neue Liste der politisch vollberechtigten Bürger. Wer einer der größeren Zünfte "(arti maggiori)" angehörte und im "squittinio" für tauglich befunden worden war, konnte sich zum Patriziat der Stadt zählen. Da das "squittinio" Manipulationsmöglichkeiten bot und über den sozialen Rang der am politischen Leben beteiligten Bürger entschied, war seine Durchführung politisch heikel.

Das System der Ämterbesetzung durch Losentscheid hatte den Vorteil, dass zahlreiche Angehörige der städtischen Führungsschicht Gelegenheit erhielten, ehrenvolle Ämter zu bekleiden und so ihren Ehrgeiz zu befriedigen. Jedes Jahr wurden die Hauptorgane der Stadtverwaltung mit 1650 neuen Leuten besetzt. Ein Nachteil des häufigen Führungswechsels war die Unberechenbarkeit; eine neue Signoria konnte einen ganz anderen Kurs steuern als ihre Vorgängerin, wenn sich die Mehrheitsverhältnisse durch den Zufall des Losentscheids geändert hatten.

Für besondere Krisensituationen war der Zusammentritt eines "parlamento" vorgesehen. Das war eine Versammlung aller männlichen Bürger, die über 14 Jahre alt waren, mit Ausnahme der Kleriker. Das "parlamento" konnte eine Kommission für Notfälle, eine "balìa", wählen und mit Sondervollmachten zur Bewältigung der Krise ausstatten.

Cosimo wurde am 10. April 1389 in Florenz geboren. Sein Vater war Giovanni di Bicci de’ Medici (1360–1429), seine Mutter Piccarda de’ Bueri. Es war damals üblich, zwecks Unterscheidung von gleichnamigen Personen den Namen des Vaters anzugeben; daher nannte man Giovanni „di Bicci“ (Sohn des Bicci) und seinen Sohn Cosimo „di Giovanni“. Cosimo hatte einen Zwillingsbruder namens Damiano, der bald nach der Geburt starb. Die Brüder erhielten ihre Namen nach Cosmas und Damian, zwei antiken Märtyrern, die ebenfalls Zwillinge waren und als Heilige verehrt wurden. Daher feierte Cosimo später seinen Geburtstag nicht am 10. April, sondern am 27. September, der damals der Festtag des heiligen Brüderpaars war.

Cosimos Vater war bürgerlicher Herkunft. Er gehörte der weitverzweigten Sippe der Medici an. Schon im späten 13. Jahrhundert waren in Florenz Medici im Bankgewerbe tätig, doch in den 1360er und 1370er Jahren war die Sippe größtenteils noch nicht reich; die meisten ihrer Haushalte waren sogar relativ minderbemittelt. Dennoch spielten die Medici in der Politik bereits eine wichtige Rolle; im 14. Jahrhundert waren sie in der Signoria häufig vertreten. In ihrem Kampf um Ansehen und Einfluss erlitten sie jedoch einen schweren Rückschlag, als ihr Wortführer Salvestro de’ Medici 1378 beim Ciompi-Aufstand ungeschickt taktierte: Er ergriff zunächst für die Aufständischen Partei, änderte aber später seine Haltung. Dies brachte ihm den Ruf der Wankelmütigkeit ein. Dann wurde er des Strebens nach Tyrannenherrschaft verdächtigt, schließlich musste er 1382 ins Exil gehen. In der Folgezeit galten die Medici als unzuverlässig. Um 1400 waren sie so diskreditiert, dass ihnen die Bekleidung öffentlicher Ämter untersagt war; allerdings waren zwei Zweige der Sippe von dem Verbot ausgenommen. Cosimos Großvater Bicci de’ Medici gehörte zwar einem der beiden nicht betroffenen Zweige an, doch war die Erfahrung der Jahre 1378–1382 für die ganze Sippe ein einschneidendes Erlebnis, das zur Vorsicht mahnte.

Um 1380 betätigte sich Giovanni als kleiner Geldverleiher. Dieses Gewerbe wurde damals verachtet; im Gegensatz zum großen Bankgeschäft war es der Öffentlichkeit suspekt, da die Geldverleiher auf offensichtliche Weise das kirchliche Zinsverbot missachteten, während die Bankiers besser in der Lage waren, die Verzinsung ihrer Darlehen zu vertuschen. Später trat Giovanni in den Dienst des Bankiers Vieri di Cambio, des damals reichsten Angehörigen der Medici-Sippe. Ab 1385 leitete er die römische Filiale von Vieris Bank. Er konnte sich dort als Juniorpartner etablieren, vermutlich dank der Mitgift seiner Frau. Nachdem Vieris Bank 1391/1392 aufgelöst und in drei eigenständige Unternehmen aufgespalten worden war, machte sich Giovanni selbständig und übernahm die römische Filiale. Mit diesem Schritt gründete er die Medici-Bank.
Obwohl Rom der weitaus attraktivste Standort in ganz Italien war, verlegte Giovanni 1397 den Hauptsitz seines Unternehmens nach Florenz. Ausschlaggebend war dabei sein Wunsch, in seine Heimatstadt zurückzukehren. Dort schuf er in der Folgezeit zielstrebig ein Netzwerk von Verbindungen, von denen manche vor allem geschäftlich vorteilhaft waren, andere in erster Linie dazu dienten, sein Ansehen und seinen politischen Einfluss zu vergrößern. Seine beiden Söhne, Cosimo und der sechs Jahre jüngere Lorenzo, erhielten ihre Ausbildung in der väterlichen Bank und wurden dann an der Gestaltung der Geschäftspolitik beteiligt. Zu den Allianzen, die Giovanni di Bicci einging, gehörte seine Verbindung mit dem traditionsreichen adligen Geschlecht der Bardi. Die Bardi hatten in der ersten Hälfte des 14. Jahrhunderts zu den bedeutendsten Bankiers Europas gezählt. Ihre Bank war zwar 1345 spektakulär zusammengebrochen, doch betätigten sie sich später wieder mit Erfolg im Finanzbereich. Um 1413/1415 wurde das Bündnis der beiden Familien durch eine Heirat bekräftigt: Cosimo schloss die Ehe mit Contessina de’ Bardi di Vernio. Solche Heiraten waren ein wesentlicher Bestandteil der politischen und geschäftlichen Netzwerkbildung. Sie hatten gewichtige Auswirkungen auf den sozialen Status und den Einfluss einer Familie und wurden daher reiflich überlegt. Verschwägerung schuf Loyalitäten. Allerdings war nur ein Teil der Bardi-Sippe an dem Bündnis beteiligt, manche ihrer Zweige zählten zu den Gegnern der Medici.

Die ersten Jahrzehnte des 15. Jahrhunderts waren für die Medici-Bank eine Phase zielstrebig vorangetriebener Expansion. Sie hatte Zweigstellen in Rom, Venedig und Genf, zeitweilig auch in Neapel. Im Zeitraum von 1397 bis 1420 wurde ein Reingewinn von 151.820 Florin "(fiorini)" erwirtschaftet. Davon blieben nach Abzug des Anteils, der einem Partner zustand, für die Medici 113.865 Florin übrig. Mehr als die Hälfte des Gewinns stammte aus Rom, wo die wichtigsten Geschäfte getätigt wurden, nur ein Sechstel aus Florenz. Seinen größten Erfolg errang Giovanni 1413, als ihn der in Rom residierende Gegenpapst Johannes XXIII., mit dem er befreundet war, zu seinem Hauptbankier machte. Zugleich wurde sein Zweigstellenleiter in Rom päpstlicher Generaldepositar "(depositario generale)", das heißt, er übernahm die Verwaltung des größten Teils der Kircheneinkünfte gegen eine Provision. Als sich Johannes XXIII. im Herbst 1414 nach Konstanz begab, um an dem dorthin einberufenen Konzil teilzunehmen, gehörte Cosimo angeblich zu seinem Gefolge. Doch im folgenden Jahr erlitten die Medici einen herben Rückschlag, als das Konzil Johannes XXIII. absetzte. Damit verlor die Medici-Bank ihre fast monopolartige Stellung im Geschäft mit der Kurie; in den folgenden Jahren musste sie mit anderen Banken konkurrieren. Den Vorrang konnte sie sich erst wieder sichern, nachdem 1420 ein Hauptkonkurrent, die Spini-Bank, in die Insolvenz gegangen war.

Als sich Giovanni di Bicci 1420 aus der Leitung der Bank zurückzog, übernahmen seine Söhne Cosimo und Lorenzo gemeinsam die Führung des Unternehmens. Im Jahr 1429 starb Giovanni. Nach seinem Tod wurde das Familienvermögen nicht aufgeteilt; Cosimo und Lorenzo traten zusammen das Erbe an, wobei Cosimo als dem älteren die Entscheidungsgewalt zufiel. Das Vermögen bestand aus etwa 186.000 Florin, von denen zwei Drittel in Rom, jedoch nur ein Zehntel in Florenz erwirtschaftet worden waren – selbst die Zweigstelle in Venedig erwirtschaftete mehr. Neben der Bank gehörte der Familie umfangreicher Grundbesitz im Umland von Florenz, vor allem im Mugello, der Gegend, aus der die Familie ursprünglich stammte. Fortan erhielten die beiden Brüder zwei Drittel des Profits der Bank, der Rest ging an ihre Partner.

Angeblich hat Giovanni auf dem Totenbett seinen Söhnen geraten, diskret zu agieren. Sie sollten in der Öffentlichkeit zurückhaltend auftreten, um möglichst wenig Neid und Missgunst zu erregen. Beteiligung am politischen Prozess war für einen Bankier existenznotwendig, da er sonst damit rechnen musste, von Feinden und Rivalen ausmanövriert zu werden. Wegen der Heftigkeit und Unberechenbarkeit der politischen Auseinandersetzungen in der Stadt war aber eine zu starke Profilierung sehr gefährlich, wie der Ciompi-Aufstand gezeigt hatte. Konflikte waren daher möglichst zu vermeiden.

Mit dem wirtschaftlichen Erfolg und sozialen Aufstieg der Medici wuchs ihr Anspruch auf politischen Einfluss. Damit stießen sie trotz ihres zurückhaltenden Auftretens bei einigen traditionell tonangebenden Sippen, die sich zurückgedrängt sahen, auf Widerstand. So kam es zur Bildung zweier großer Gruppierungen, die einander lauernd gegenüberstanden. Auf der einen Seite standen die Medici mit ihren Verbündeten und der breiten Klientel derer, die von ihren Geschäften, ihren Aufträgen und ihrem Einfluss direkt oder indirekt profitierten. Im gegnerischen Lager versammelten sich die Sippen, die ihre herkömmliche Machtstellung behalten und die Aufsteiger in die Schranken weisen wollten. Unter ihnen war die Familie Albizzi die bedeutendste; deren Oberhaupt Rinaldo degli Albizzi wurde zum Wortführer der Medici-Gegner. In dieser Spaltung der Bürgerschaft spiegelten sich nicht nur persönliche Gegensätze zwischen führenden Politikern, sondern auch unterschiedliche Mentalitäten und Grundeinstellungen. Bei der Albizzi-Gruppe handelte es sich um die konservativen Kreise, deren Dominanz 1378 durch den Ciompi-Aufstand, eine von benachteiligten Arbeitern getragene Erhebung der unteren Volksschichten "(popolo minuto)", bedroht worden war. Seit dieser schockierenden Erfahrung bemühten sie sich, ihren Status abzusichern, indem sie das Eindringen von suspekten Cliquen in die maßgeblichen Gremien zu hemmen trachteten. Aufruhr, Umsturz und diktatorische Gelüste sollten im Keim erstickt werden. Die zeitweilige Unterstützung der Medici für die aufständischen Arbeiter war nicht vergessen. Die Albizzi-Gruppe war aber keine Partei mit einer einheitlichen Führung und einem gemeinsamen Kurs, sondern ein lockerer, informeller Zusammenschluss einiger etwa gleichrangiger Clans. Außer der Gegnerschaft zu potentiell gefährlichen Außenseitern verband die Mitglieder dieser Allianz wenig. Ihre Grundhaltung war defensiv. Die Medici-Gruppe hingegen war vertikal strukturiert. Cosimo war ihr unangefochtener Anführer, der die wesentlichen Entscheidungen traf und die finanziellen Ressourcen, die den gegnerischen weit überlegen waren, zielbewusst einsetzte. Aufsteigerfamilien "(gente nuova)" zählten zu den natürlichen Verbündeten der Medici, doch beschränkte sich deren Anhängerschaft nicht auf Kräfte, die von erhöhter sozialer Mobilität profitieren konnten. Die Medici-Gruppe umfasste auch angesehene Patriziergeschlechter, die sich in ihr Netzwerk hatten eingliedern lassen, unter anderem durch Verschwägerung. Offenbar hatten die Albizzi in der Oberschicht stärkeren Rückhalt, während die Medici beim Mittelstand – den Handwerkern und Ladenbesitzern – größere Sympathien genossen. Die Zugehörigkeit eines großen Teils von Cosimos Parteigängerschaft zur traditionellen Elite zeigt aber, dass die früher gelegentlich vertretene Deutung des Konflikts als Kampf zwischen Klassen oder Ständen verfehlt ist.

Die Verhärtung des Gegensatzes ließ einen offenen Machtkampf als unvermeidlich erscheinen, doch musste dieser in Anbetracht der vorherrschenden Loyalität zur verfassungsmäßigen Ordnung im Rahmen der Legalität ausgetragen werden. Ab 1426 spitzte sich der Konflikt zu. Die Propaganda beider Seiten zielte auf die Verfestigung von Feindbildern ab. Für die Medici-Anhänger war Rinaldo degli Albizzi der arrogante Wortführer volksferner, oligarchischer Kräfte, der vom Ruhm seines Vaters zehrte und infolge seiner Unbesonnenheit Führungsqualitäten vermissen ließ. Die Albizzi-Gruppe stellte Cosimo als potentiellen Tyrannen dar, der seinen Reichtum nutze, um die Verfassung auszuhebeln und sich durch Bestechung und Korruption den Weg zur Alleinherrschaft zu bahnen. Indizien deuten darauf, dass die Vorwürfe beider Seiten einen beträchtlichen wahren Kern enthielten: Rinaldo stieß durch seine Schroffheit einflussreiche Sympathisanten wie die Familie Strozzi vor den Kopf und zerstritt sich sogar mit seinem Bruder Luca so sehr, dass dieser die Familienloyalität aufkündigte und zur Gegenseite überlief, was für damalige Verhältnisse ein ungewöhnlicher Schritt war. Auch die Polemik gegen die Medici fußte, wenngleich sie wohl überzogen war, auf Tatsachen: Die Medici-Gruppe infiltrierte die Verwaltung, verschaffte sich dadurch geheime Informationen, schreckte vor Dokumentenfälschung nicht zurück und manipulierte das "squittinio" in ihrem Sinn.

Anlass zu Polemik bot die Einführung des "catasto", eines umfassenden Verzeichnisses aller steuerpflichtigen Güter und Einkommen, im Mai 1427. Das Verzeichnis bildete die Grundlage der Erhebung einer neu eingeführten Vermögenssteuer, die zur Reduzierung der dramatisch gestiegenen Staatsschulden benötigt wurde. Dieser Schritt bewirkte eine gewisse Verlagerung der Steuerlast von der indirekt besteuerten Mittelschicht zu den wohlhabenden Patriziern. Die besonders zahlungskräftigen Medici konnten die neue Last besser verkraften als manche ihrer weniger vermögenden Gegner, für die der "catasto" einen harten Schlag bedeutete. Zwar hatte Giovanni di Bicci die Einführung der Vermögenssteuer anfangs abgelehnt und später nur zögerlich unterstützt, doch gelang es den Medici, sich als Befürworter der in der Bevölkerung populären Maßnahme darzustellen. Sie konnten sich damit als Patrioten profilieren, die zu ihrem eigenen Nachteil für die Sanierung des Staatshaushalts eintraten und selbst einen gewichtigen Beitrag dazu leisteten.

Weiter angeheizt wurde der Konflikt durch den Krieg gegen Lucca, den Florenz Ende 1429 begann. Die militärischen Auseinandersetzungen endeten im April 1433 mit einem Friedensschluss, ohne dass die Angreifer ihr Kriegsziel erreicht hatten. Die beiden verfeindeten Cliquen in Florenz hatten den Krieg einhellig befürwortet, nutzten dann aber seinen ungünstigen Verlauf als Waffe in ihrem Machtkampf. Rinaldo hatte als Kriegskommissar am Feldzug teilgenommen, daher konnte er für dessen Misserfolg mitverantwortlich gemacht werden. Er seinerseits gab die Schuld dem für die Koordinierung der Kriegführung zuständigen Zehnerausschuss, in dem Anhänger der Medici stark vertreten waren; der Ausschuss habe seine Bemühungen sabotiert. Cosimo konnte sich bei dieser Gelegenheit in ein günstiges Licht rücken: Er hatte dem Staat 155 887 Florin geliehen, einen Betrag, der mehr als ein Viertel des kriegsbedingten Sonder-Finanzbedarfs ausmachte. Damit konnte der Mediceer seinen Patriotismus und seine einzigartige Bedeutung für das Schicksal der Republik propagandawirksam demonstrieren. Insgesamt stärkte der Kriegsverlauf somit die Stellung der Medici-Gruppe in der öffentlichen Meinung.

Die Strategie der Albizzi-Gruppe zielte darauf ab, die Gegner – vor allem Cosimo persönlich – verfassungsfeindlicher Umtriebe anzuklagen und sie so mit strafrechtlichen Mitteln außer Gefecht zu setzen. Eine Handhabe bot den Feinden der Medici ein von ihnen im Dezember 1429 durchgebrachtes Gesetz, das staatsschädliche Protektion unterbinden und den inneren Frieden sichern sollte. Es richtete sich gegen Aufsteiger, die sich durch ihre Beziehungen zu Mitgliedern der Signoria unerlaubte Vorteile verschafften, und gegen Große, die Unruhe stifteten. Diese Gesetzgebung zielte somit auf Cosimo und seine sozial und politisch mobile Klientel. Ab 1431 wurde den führenden Köpfen der Medici-Gruppe zunehmend mit Aberkennung der Bürgerrechte und Verbannung gedroht. Zu diesem Zweck sollte eine Sonderkommission gebildet und zu entsprechenden Maßnahmen bevollmächtigt werden. Nach dem Ende des Krieges gegen Lucca wurde die Gefahr für Cosimo akut, da er nun nicht mehr als Kreditgeber des Staats benötigt wurde. Daraufhin leitete er im Frühjahr 1433 den Transfer seines Kapitals ins Ausland ein. Einen großen Teil ließ er nach Venedig und Rom schaffen, einiges Geld versteckte er in Florenz in Klöstern. Damit sicherte er das Bankvermögen gegen das Risiko einer Enteignung, die im Fall einer Verurteilung wegen Hochverrats zu befürchten war.

Die Auslosung der Posten in der Signoria für die Amtszeit September und Oktober 1433 ergab eine Zweidrittelmehrheit der Medici-Gegner. Diese Gelegenheit ließen sie sich nicht entgehen. Cosimo, der sich außerhalb der Stadt aufhielt, wurde von der Signoria zu einer Beratung eingeladen. Bei seinem Eintreffen im Stadtpalast am 5. September wurde er sofort festgenommen. Mit der Mehrheit von sechs zu drei beschloss die Signoria seine Verbannung und eine Sonderkommission bestätigte das Urteil, da er ein Zerstörer des Staats und Verursacher von Skandalen sei. Fast alle Angehörigen der Medici-Sippe wurden für zehn Jahre von den Ämtern der Republik ausgeschlossen. Cosimo wurde nach Padua, sein Bruder Lorenzo nach Venedig verbannt; dort sollten sie zehn Jahre bleiben. Falls sie die ihnen zugewiesenen Aufenthaltsorte vorzeitig verließen, drohte ihnen ein weiteres Urteil, das die Heimkehr für immer ausschloss. Die lange Dauer der angeordneten Abwesenheit sollte das Netzwerk der Medici dauerhaft lahmlegen und zerreißen. Cosimo musste als Garantie für sein künftiges Wohlverhalten eine Kaution von 20.000 Florin hinterlegen. Er akzeptierte das Urteil, wobei er seine Loyalität zur Republik hervorhob, und ging Anfang Oktober 1433 ins Exil.

Bald zeigte sich, dass das Netzwerk der Medici nicht nur in Florenz intakt blieb, sondern sogar im fernen Ausland effizient funktionierte. Cosimos Abschied und seine Reise nach Padua wurden zu einer triumphalen Demonstration seines Einflusses im In- und Ausland. Schon unterwegs erhielt er eine Vielzahl von Sympathiekundgebungen, Treuebezeugungen und Hilfsangeboten prominenter Persönlichkeiten und ganzer Städte. In Venedig, zu dessen Territorium der Verbannungsort Padua damals gehörte, war die Unterstützung besonders stark, was mit dem Umstand zusammenhing, dass die Medici-Bank dort seit Jahrzehnten eine Filiale unterhielt. Als Cosimos Bruder Lorenzo in Venedig eintraf, wurde er vom Dogen Francesco Foscari persönlich sowie vielen Adligen empfangen. Die Republik Venedig ergriff klar für die Verfolgten Partei und schickte einen Gesandten nach Florenz, der sich um die Aufhebung des Urteils bemühen sollte. Dieser erreichte immerhin, dass Cosimo gestattet wurde, sich in Venedig anzusiedeln. Kaiser Sigismund, den die Venezianer informiert hatten, äußerte seine Missbilligung der Verbannung, die er für eine Dummheit der Florentiner hielt. Sigismund hatte auf seinem Italienzug, von dem er im Oktober 1433 heimkehrte, unter anderem eine Regelung seines Verhältnisses zur Republik Florenz angestrebt, aber keinen Verhandlungserfolg erzielen können.

Den Umschwung brachte schließlich ein neuer Geldbedarf der Republik Florenz. Da die Lage der Staatsfinanzen prekär war und die Medici-Bank als Kreditgeber nicht mehr zur Verfügung stand, zeichnete sich eine Steuererhöhung ab. Dies führte zu solcher Unzufriedenheit, dass im Lauf des Frühlings und Sommers 1434 die Stimmung in der Führungsschicht kippte. Anhänger der Medici und Befürworter einer Versöhnung bekamen zunehmend Oberwasser. Die neue Stimmungslage spiegelte sich in der für die Amtszeit September und Oktober 1434 ausgelosten Signoria, die teils dezidiert medicifreundlich, teils versöhnungsbereit war. Der neue "gonfaloniere di giustizia" war ein entschlossener Gefolgsmann Cosimos. Er setzte am 20. September die Aufhebung des Verbannungsurteils durch. Nun drohte den Anführern der Albizzi-Gruppe das Schicksal, das sie im Vorjahr ihren Feinden bereitet hatten. Um dem zuvorzukommen, planten sie für den 26. September einen Staatsstreich und zogen Bewaffnete zusammen. Da aber die Gegenseite rechtzeitig ihre Kräfte mobilisiert hatte, wagten sie den Angriff nicht, denn ohne das Überraschungsmoment hätte er einen Bürgerkrieg mit geringen Erfolgschancen bedeutet. Schließlich griff Papst Eugen IV. als Vermittler ein. Der Papst war von einem Volksaufstand aus Rom vertrieben worden und lebte seit einigen Monaten in Florenz im Exil. Als Venezianer war Eugen tendenziell medicifreundlich gesinnt, und vor allem konnte er auf künftige Darlehen der Medici-Bank hoffen. Es gelang ihm, Rinaldo zur Aufgabe zu bewegen.

Am 29. September brach Cosimo zur Heimkehr auf, die sich ebenso wie seine Abreise triumphal gestaltete. Am 2. Oktober wurde die Verbannung Rinaldos und einiger seiner Weggefährten verfügt. Damit hatte die Medici-Gruppe den Machtkampf endgültig zu ihren Gunsten entschieden. Als Sieger gab sich Cosimo versöhnlich und agierte wie gewohnt vorsichtig. Allerdings hielt er es zur Sicherung seiner Stellung für erforderlich, 73 feindliche Bürger ins Exil zu schicken. Viele von ihnen durften später zurückkehren und sich sogar wieder für die Signoria qualifizieren.

Die Ursachen für den Ausgang des Machtkampfs wurden im frühen 16. Jahrhundert von Niccolò Machiavelli analysiert. Er zog daraus allgemeine Lehren, darunter seine berühmte Forderung, dass ein Eroberer der Macht unmittelbar nach der Inbesitznahme des Staates alle unvermeidlichen Grausamkeiten auf einen Schlag begehen müsse. Machiavellis Einschätzung, wonach der Albizzi-Gruppe ihre Unentschlossenheit und Halbherzigkeit zum Verhängnis wurde, wird von der modernen Forschung geteilt. Weitere Faktoren, die den Medici-Gegnern schadeten, waren das Fehlen innerer Geschlossenheit und einer über Autorität verfügenden Führung. Hinzu kam ihr Mangel an Rückhalt im Ausland, wo Cosimo mächtige Verbündete hatte.

Nach seiner triumphalen Heimkehr wurde Cosimo faktisch der Lenker des florentinischen Staates und blieb bis zu seinem Tod in dieser informellen Stellung. Dabei respektierte er äußerlich die Institutionen der republikanische Verfassung, ein Amt mit Sondervollmachten strebte er für sich nicht an. Er agierte aus dem Hintergrund mittels seines weitgespannten in- und ausländischen Netzwerks.

Cosimo und seinen Zeitgenossen stand stets die Tatsache vor Augen, dass die Grundlage seiner politischen Machtentfaltung sein kommerzieller Erfolg war. Der Zusammenhalt seines Netzwerks hing in erster Linie von den Geldflüssen ab, die nicht versiegen durften. In Nord- und Mittelitalien florierte das Bankgeschäft, und niemand war darin erfolgreicher als er. Auch in der Kunst des Einsatzes finanzieller Ressourcen für politische Ziele war er zu seiner Zeit unübertroffen. Unter seiner Leitung expandierte die Medici-Bank weiter; neue Zweigstellen wurden in Pisa, Mailand, Brügge, London und Avignon eröffnet, die Genfer Filiale wurde nach Lyon verlegt.

Eine Haupteinnahmequelle der großen, überregional operierenden Banken, insbesondere der Medici-Bank, war die Kreditvergabe an Machthaber und geistliche Würdenträger. Besonders groß war der Kreditbedarf der Päpste, die zwar über gewaltige Einkünfte aus der gesamten katholischen Welt verfügten, aber wegen kostspieliger militärischer Unternehmungen immer wieder in Engpässe gerieten. Darlehen an Machthaber waren lukrativ, aber mit beträchtlichen Risiken verbunden. Es musste mit der Möglichkeit gerechnet werden, dass solche Schuldner die Rückzahlung verweigerten oder nach einem verlustreichen Krieg, den sie mit Fremdkapital finanziert hatten, zumindest zeitweilig nicht mehr zahlungsfähig waren. Ein weiteres Risiko bestand im gewaltsamen Tod des Schuldners durch einen Mordanschlag oder auf einem Feldzug. Durch solche Ereignisse bedingte Zahlungsausfälle konnten auch bei großen Banken zur Insolvenz führen. Die Einschätzung der Chancen und Risiken derartiger Geschäfte gehörte zu den wichtigsten Aufgaben Cosimos.

Ein Bankier des 15. Jahrhunderts benötigte politische Begabung und großes diplomatisches Geschick, denn Geschäft und Politik waren verschmolzen und mit vielfältigen familiären Interessen verknüpft. Darlehensgewährung war häufig auch faktische Parteinahme in den erbitterten Konflikten zwischen Machthabern, Städten oder auch Parteien innerhalb einer Bürgerschaft. Entscheidungen über die Vergabe, Begrenzung oder Verweigerung von Krediten oder Unterstützungsgeldern hatten weitreichende politische Konsequenzen; sie schufen und bewahrten Bündnisse und Netzwerke oder erzeugten gefährliche Feindschaften. Auch militärisch wirkten sie sich aus, denn die zahlreichen Kriege unter den nord- und mittelitalienischen Städten wurden mit dem kostspieligen Einsatz von Söldnerführern (Condottieri) ausgetragen. Diese standen mit ihren Truppen nur zur Verfügung, solange der Auftraggeber zahlungskräftig war; wenn dies nicht mehr der Fall war, ließen sie sich vom Feind abwerben oder plünderten auf eigene Rechnung. Die Entscheidungen, die Cosimo als Bankier traf, waren zum Teil nur politisch, nicht kommerziell sinnvoll. Manche seiner Zahlungen waren politisch unumgänglich, aber ökonomisch reine Verlustgeschäfte. Sie dienten der Pflege seines Ansehens oder der Sicherung der Loyalität von Verbündeten. Dazu zählten die Belohnungen für geleistete politische Dienste und die Erfüllung von Aufgaben, die als patriotische Pflichten galten.
In Florenz waren die Haupteinnahmequellen der Medici-Bank der Geldwechsel und die Kreditvergabe an Angehörige der Oberschicht, die in finanzielle Bedrängnis geraten waren. Insbesondere zur Bezahlung von Steuerschulden wurden Darlehen benötigt, denn säumige Steuerschuldner durften keine Ämter ausüben. Weitaus bedeutender war jedoch das Kreditgeschäft mit auswärtigen Machthabern. Der wichtigste Geschäftspartner der Bank war der Papst, als dessen Hauptbankier Cosimo fungierte. Vor allem dank der Verbindung mit der Kurie waren die römischen Geschäfte der Bank die lukrativsten. Die dortigen Zinseinnahmen und die Provisionen auf die getätigten Transaktionen boten eine hohe Gewinnspanne und die Geschäfte waren wegen des ständigen Geldbedarfs der Kurie sehr umfangreich. Daher erwirtschaftete die Filiale in Rom den größten Teil der Gewinne. Außerdem wirkte sich die enge Beziehung zur Kurie auch politisch vorteilhaft aus. Wenn der Papst Rom verließ, folgte ihm die römische Filiale; sie war stets dort zu finden, wo sein Hof sich aufhielt.

Neben der politischen und ökonomischen Kompetenz war der wichtigste Faktor, von dem der Erfolg eines Bankiers abhing, seine Menschenkenntnis. Er musste in der Lage sein, die Kreditwürdigkeit seiner Kunden und die Zuverlässigkeit seiner auswärtigen Zweigstellenleiter, die viele Gelegenheiten zum Betrug hatten, richtig einzuschätzen. Cosimo verfügte ebenso wie sein Vater in hohem Maße über diese Fähigkeiten. Seine Verschwiegenheit, Nüchternheit und Voraussicht und sein geschickter Umgang mit Geschäftspartnern verschafften ihm Respekt. Auch die moderne Forschung würdigt diese Qualitäten des Mediceers, die maßgeblich zu seinem kommerziellen und politischen Erfolg beitrugen.

Aus Cosimos Korrespondenz mit dem Leiter der Filiale der Medici-Bank in Venedig geht hervor, dass die Bank systematisch Steuern hinterzog und dass Cosimo persönlich Anweisungen zur Bilanzfälschung erteilte. Der Zweigstellenleiter, Alessandro Martelli, versicherte ihm, dass auf die Verschwiegenheit des Personals Verlass sei.

Der entscheidende Schritt, der nach dem Sieg von 1434 die Stellung Cosimos dauerhaft absicherte, war eine Änderung des Auslosungsverfahrens zur Bestimmung der Mitglieder der Signoria. Die Gesamtmenge der Namen auf den Loszetteln, die in die Beutel gelegt wurden, wurde von rund zweitausend auf eine Mindestzahl von 74 reduziert, für den Beutel des "gonfaloniere di giustizia" wurde eine Mindestzahl von vier festgelegt. Damit wurde die Anzahl der Kandidaten überschaubar und die Rolle des Zufalls im Auslosungsprozess stark vermindert. Mit der Füllung der Losbeutel waren traditionell von der Signoria ernannte Männer betraut, die "accoppiatori" genannt wurden. Sie sorgten fortan dafür, dass nur noch Namen von Bewerbern, die Cosimo genehm waren, in die Beutel kamen. So blieb es zwar beim Prinzip des Losentscheids, doch war nun ein wirksamer Filter eingebaut, der überraschende Änderungen der Machtverhältnisse verhinderte. Dieses Verfahren wurde "imborsazione a mano" („Handverlesung“) genannt. Es konnte zwar von Cosimo durchgesetzt werden, war aber in der Bürgerschaft tendenziell unbeliebt, da es offensichtlich manipulativ war und für viele den Zugang zu den prestigereichen Ämtern erschwerte oder verunmöglichte. Immer wieder wurde die Forderung nach Rückkehr zum offenen Losverfahren erhoben. Mit diesem Anliegen konnte man auf harmlose Art Unzufriedenheit mit der Machtfülle des Mediceers ausdrücken. Das Ausmaß des Widerstands gegen die Handverlesung wurde zum Gradmesser für die Unbeliebtheit des Herrschaftssystems. Dies hatte für Cosimo auch Vorteile: Er erhielt dadurch die Möglichkeit, flexibel zu reagieren, wenn sich in der Bürgerschaft Ärger aufstaute oder wenn er den Eindruck hatte, dass eine relativ entspannte Lage ihm Konzessionen gestattete. Je nach der Entwicklung der innen- und außenpolitischen Verhältnisse setzte er reine Handverlesung durch oder ließ freie Auslosung zu. Zeitweilig wurde ein Mischverfahren praktiziert, bei dem die Namen des "gonfaloniere di giustizia" und dreier weiterer Ratsmitglieder aus handverlesenen Beuteln gezogen und die übrigen fünf Mitglieder der Signoria frei ausgelost wurden.

Den vielen Bürgern, die keine Gelegenheit erhielten, Mitglied der Signoria zu werden, bot Cosimos System Gelegenheit, ihren Ehrgeiz dennoch teilweise zu befriedigen. Ansehen verschaffte nicht nur die Ausübung eines Regierungsamts, sondern schon die Anerkennung der Tatsache, dass man als ehrbarer Bürger die persönlichen Voraussetzungen dafür erfüllte. In die Beutel legte man daher auch Loszettel von Personen, gegen die keine persönlichen Einwände bestanden, die aber aus einem äußerlichen Grund nicht in Betracht kamen, etwa weil sie mit einem Amtsinhaber zu nahe verwandt waren oder infolge des Quotensystems ausscheiden mussten, da sie zur falschen Zunft gehörten oder im falschen Bezirk wohnten. Wenn dann ein solcher Zettel gezogen wurde, wurde festgestellt, dass der Betreffende als Ausgeloster „gesehen“ wurde "(veduto)", aber wegen eines formalen gesetzlichen Hindernisses seinen Sitz im Stadtrat nicht einnehmen konnte. Ein "veduto" konnte aus dem Umstand, dass ihm die theoretische Amtsfähigkeit bescheinigt wurde, Prestige ziehen.

Im Lauf der Zeit wurden immer wieder temporäre Gremien mit legislativen und finanzpolitischen Sondervollmachten geschaffen. Die Einrichtung von Kommissionen zur Erledigung besonderer Aufgaben, auch in Notstandslagen, war an sich keine Neuerung und stand mit der republikanischen Verfassung in Einklang. Ein Unterschied zu den früheren Verhältnissen bestand aber darin, dass solche Gremien früher nach einigen Tagen oder wenigen Wochen wieder aufgelöst wurden, während nun ihre Vollmachten für längere Zeiträume erteilt wurden. Damit nahm ihr politisches Gewicht zu, was Cosimos Absicht entsprach; für ihn waren die Kommissionen wichtige Machtinstrumente. Durch diese Entwicklung kam es jedoch zu Reibungen mit den fortbestehenden alten Institutionen, dem Volksrat und dem Gemeinderat. Diese verteidigten ihre herkömmlichen Rechte, waren aber im Machtkampf dadurch benachteiligt, dass ihre Amtsperiode nur vier Monate betrug. Die Abgrenzung der Zuständigkeiten zwischen den ständigen und den temporären Gremien war kompliziert und umkämpft, es ergaben sich Überschneidungen und Kompetenzstreitigkeiten. Dabei war die Steuergesetzgebung ein besonders heikles Feld. Hier war Cosimo darauf angewiesen, den Konsens mit der Führungsschicht der Bürgerschaft zu suchen. Da er keine diktatorische Macht besaß, waren die Gremien keineswegs gleichgeschaltet. Sowohl der Volks- und der Gemeinderat als auch die Kommissionen fassten Beschlüsse gemäß den Interessen und Überzeugungen ihrer Mitglieder, die nicht immer mit Cosimos Wünschen übereinstimmten. Die Räte waren in der Lage, seinen Absichten hinhaltenden Widerstand zu leisten. Die Abstimmungen in den Gremien waren frei, wie die manchmal knappen Mehrheiten zeigen.

Nur einmal geriet Cosimos Regierungssystem in eine ernste Krise. Dies geschah erst im letzten der drei Jahrzehnte, in denen er die Herrschaft ausübte. Als die italienischen Mächte im Februar 1455 einen allgemeinen Frieden schlossen, kam es zu einer außenpolitischen Entspannung, die so weitgehend war, dass das unpopuläre System der Handverlesung nicht mehr mit einem äußeren Notstand begründet werden konnte. In der Öffentlichkeit wurde die Forderung nach Wiedereinführung des offenen Losverfahrens lauter denn je. Cosimo gab nach: Die alte Ordnung trat wieder in Kraft, die Handverlesung wurde verboten, der Volksrat und der Gemeinderat erhielten den früheren Umfang ihrer legislativen und finanzpolitischen Entscheidungsgewalt zurück. Damit wurde die Medici-Herrschaft wieder von Zufällen und von der Gunst der öffentlichen Meinung abhängig. In dieser labilen Lage verschärfte sich ein Problem, das für das Regierungssystem eine ernste Bedrohung darstellte: Die öffentlichen Finanzen waren wegen langjähriger hoher Rüstungsaufwendungen und wiederholter Epidemien so zerrüttet, dass die Erhöhung der direkten Steuer, die von der wohlhabenden Oberschicht zu entrichten war, unumgänglich schien. Dieses Vorhaben stieß aber auf anhaltenden Widerstand, neue Steuergesetze wurden in den Räten blockiert. Im September 1457 entlud sich der Unmut in einer Verschwörung, die auf einen Umsturz abzielte. Das Komplott wurde entdeckt und sein Anführer Piero de’ Ricci hingerichtet.

Die Spannungen nahmen weiter zu, als die Räte schließlich im Januar 1458 ein neues, von Cosimo befürwortetes Steuergesetz billigten, das sich auf die gesamte wohlhabende Schicht auswirkte. Das Gesetz entlastete die Minderbemittelten und erhöhte den Steuerdruck auf die Reichen. Der seit Jahrzehnten unverändert gültige "catasto", das Verzeichnis der steuerpflichtigen Vermögen und Einkommen, sollte auf den aktuellen Stand gebracht werden. Das wurde von denjenigen, deren Besitz seit der letzten Veranlagung stark zugenommen hatte, als harter Schlag empfunden. Infolgedessen schwand im Patriziat die Zustimmung zum herrschenden System. Im April 1458 wurde ein Gesetz eingeführt, das die Schaffung bevollmächtigter Kommissionen stark erschwerte und ihnen die Durchführung eines "squittinio" verbot. Da Kommissionen für Cosimo ein wichtiges Instrument waren, mit dem er seinen Einfluss auf den "squittinio" und damit auf die Kandidaturen ausübte, richtete sich diese Maßnahme gegen ein Hauptelement seines Herrschaftssystems. Das neue Gesetz wurde im Volksrat und im Gemeinderat mit überwältigenden Mehrheiten gebilligt. Cosimos Schwächung war unübersehbar.

Die Lockerung der Medici-Herrschaft seit der Verfassungsreform von 1455 und die allgemeine Verunsicherung angesichts der sozialen Spannungen und fiskalischen Probleme führten zu einer grundsätzlichen Debatte über die Verfassungsordnung. Über das Ausmaß und die Ursachen der Übelstände sowie mögliche Abhilfen wurde offen und kontrovers diskutiert. Eine zentrale Frage war, wie der Personenkreis, der für wichtige Ämter in Betracht kam, festgelegt werden sollte. Cosimo wünschte einen kleinen Kreis potentieller Amtsträger, er erstrebte die Rückkehr zur Handverlesung. Auf der Gegenseite standen Geschlechter, die für Auslosung aus einem großen Kandidatenkreis eintraten, weil sie der Dominanz Cosimos überdrüssig waren und sein Regierungssystem beseitigen wollten. Die Signoria neigte einige Zeit zu einer Kompromisslösung, doch gewannen die Befürworter der Handverlesung zunehmend an Boden. Außerdem plädierten Anhänger der Medici-Herrschaft für die Einführung eines neuen ständigen Gremiums mit sechsmonatiger Amtszeit, das weitreichende Vollmachten erhalten sollte. Begründet wurde dies mit der Notwendigkeit der Effizienzverbesserung. Dieser Vorschlag war jedoch, wie seine Befürworter einräumten, im Volksrat und im Gemeinderat chancenlos. Daher wurde nicht einmal versucht, ihn dort durchzubringen.

Im Sommer 1458 kam es zu einer Verfassungskrise. In der Signoria, die im Juli und August amtierte, dominierte Cosimos Gefolgschaft, die entschlossen war, diese Gelegenheit zur Rückeroberung der Macht zu nutzen. Der Volksrat, in dem Gegner der Medici die Oberhand hatten, wies jedoch die Vorschläge der Signoria hartnäckig zurück. Die Medici-Gruppe versuchte, eine offene Abstimmung im Volksrat durchzusetzen, um Druck auf einzelne Ratsmitglieder ausüben zu können. Damit stieß sie aber auf den energischen Widerstand des Erzbischofs von Florenz, Antonino Pierozzi, der die geheime Abstimmung als Gebot der „natürlichen Vernunft“ bezeichnete und eine andere Verfahrensweise mit Androhung der Exkommunikation verbot.

Da unklar war, welche Seite ab September in der Signoria die Mehrheit haben würde, geriet die Medici-Gruppe unter Zeitdruck. Schließlich berief die Signoria, wie es die Verfassung für schwere Krisen vorsah, eine Volksversammlung "(parlamento)" ein. Eine solche Versammlung konnte verbindliche Beschlüsse fassen und eine Kommission mit Sondervollmachten zur Lösung der Krise einsetzen. Zuletzt war dies 1434 bei Cosimos Rückkehr geschehen, zuvor bei seiner Verbannung. Das "parlamento" von Florenz war in der Theorie als demokratisches Verfassungselement konzipiert; es sollte das Organ sein, das den Volkswillen zum Ausdruck brachte und in Notstandslagen eine Entscheidung herbeiführte, wenn der reguläre Gesetzgebungsprozess blockiert war. In der Praxis pflegte aber die Patriziergruppe, die das "parlamento" einberufen ließ, durch Einschüchterung dafür zu sorgen, dass die Beschlussfassung im gewünschten Sinn erfolgte. So war es auch diesmal. Cosimo, der sich nach außen hin zurückhielt, hatte am 1. August erstmals mit dem mailändischen Gesandten über militärische Unterstützung von außen verhandelt. Er war sich seiner Sache sicher; spätestens am 5. August fiel die Entscheidung, die Volksversammlung für den 11. August einzuberufen, obwohl noch keine Hilfszusage aus Mailand vorlag. Am 10. August ordnete die Signoria das "parlamento" für den folgenden Tag an. Als die Bürger zum Versammlungsort strömten, fanden sie ihn von einheimischen Bewaffneten und mailändischen Söldnern bewacht. Nach einem Augenzeugenbericht verlas ein Notar den Text, der zu genehmigen war, so leise, dass nur wenige in der Menge ihn verstanden und ihre Zustimmung äußerten. Dies wurde aber als ausreichend betrachtet. Die Versammlung billigte alle Vorschläge der Signoria und löste sich dann auf. Damit war die Krise beendet. Der Weg zur Verwirklichung einer Verfassungsreform, die Cosimos Herrschaft zementierte, war frei.

Die Sieger ergriffen die Maßnahmen, die ihnen zur Sicherung der Macht erforderlich schienen. Mehr als 1500 politisch unzuverlässigen Bürgern wurde die Qualifikation zur Kandidatur für Führungsämter aberkannt. Viele von ihnen verließen die Stadt, in der sie keine Zukunft mehr für sich sahen. Eine Reihe von Verbannungsurteilen sollte der erneuten Entstehung einer organisierten Opposition vorbeugen. Die Befugnisse des Geheimdienstes, der "otto di guardia", wurden vergrößert. Die Beschlüsse zum Umbau der Verfassung wurden teils schon von der Volksversammlung gefasst, teils von der neuen Sonderkommission, die zu diesem Zweck eingesetzt wurde. Der wichtigste Schritt neben der Rückkehr zur Handverlesung war die Schaffung eines ständigen Gremiums, das der Medici-Gruppe als dauerhaftes Herrschaftsinstrument dienen und die temporären Kommissionen der Zeit vor 1455 ablösen sollte. Dies war der „Rat der Hundert“, dessen Amtszeit auf sechs Monate festgelegt wurde. Ihm wurde die Aufgabe übertragen, als erster Rat über die Gesetze, welche die Ämterbesetzung, das Steuerrecht und die Einstellung von Söldnern betrafen, zu beraten und sie dann an den Volksrat und den Gemeinderat weiterzuleiten. Außerdem erhielt er ein Vetorecht bei allen nicht von ihm selbst ausgehenden Gesetzesinitiativen. Somit wurde für jedes neue legislative Vorhaben die Zustimmung aller drei Räte erforderlich, denn die alten Räte behielten das Recht, jede Gesetzgebung zu blockieren. Die Schonung der beiden alten Räte, die Hochburgen der Opposition gewesen waren, lässt erkennen, dass Cosimo beim Ausbau seiner Machtstellung vorsichtig vorging. Damit nahm er auf die Bedürfnisse des republikanisch gesinnten Patriziats Rücksicht. Für die Bestimmung der Mitglieder des Rats der Hundert wurde ein gemischtes Wahl- und Losverfahren mit komplizierten Regeln festgelegt. Qualifiziert sollten nur Bürger sein, deren Namen schon früher bei der Auslosung für die herkömmlichen Führungsämter "(tre maggiori)" gezogen worden waren. Diese Bestimmung sollte gewährleisten, dass nur bewährte Patrizier, deren Haltung bereits hinreichend bekannt war, in das neue Gremium gelangten.

Die Handverlesung für die Signoria wurde 1458 nur als Provisorium für fünf Jahre eingeführt. 1460 wurde das Provisorium nach der Aufdeckung einer Verschwörung um weitere fünf Jahre verlängert. Das lässt erkennen, dass dieses Verfahren weiterhin unbeliebt war und dem Patriziat nur aus besonderem Anlass und mit Befristung akzeptabel schien.

Unzufriedenheit machte sich in Florenz auch in den letzten Lebensjahren Cosimos noch bemerkbar, ernsthaft gefährdet wurde seine Stellung nach 1458 jedoch nicht mehr. In seinen letzten Jahren hielt er sich seltener im Palast der Signoria auf, er lenkte die Politik nun meist von seinem eigenen Palast in der Via Larga aus. Dorthin verlagerte sich das Machtzentrum.

Die Außenpolitik der Republik Florenz war zu Cosimos Zeit von einer Konstellation geprägt, in der neben Florenz die bedeutenden Regionalmächte Mailand, Venedig, Neapel und der Kirchenstaat die Hauptrollen spielten. Von diesen fünf Vormächten der italienischen Staatenwelt, die in der Forschung auch als "Pentarchie" bezeichnet werden, war Florenz die politisch und militärisch schwächste, aber durch das Bankwesen und den Fernhandel wirtschaftlich bedeutend. Zwischen Mailand und Florenz bestand eine traditionelle Feindschaft, die zu den bestimmenden Faktoren des Staatensystems im späten 14. Jahrhundert und in der ersten Hälfte des 15. Jahrhunderts gehörte. Die Florentiner sahen sich vom Expansionsdrang der Mailänder Herzöge aus dem Geschlecht der Visconti bedroht. Die Auseinandersetzung mit den Visconti betrachteten sie nicht als bloßen Konflikt zwischen zwei Staaten, sondern auch als Kampf zwischen ihrer republikanischen Freiheit und tyrannischer Gewaltherrschaft. Im Zeitraum 1390–1402 führte Florenz drei Abwehrkriege gegen Herzog Giangaleazzo Visconti, der Mailand zur Hegemonialmacht Italiens machen wollte und seinen Machtbereich nach Mittelitalien ausdehnte. Mailand war nicht nur militärisch überlegen, sondern hatte auch die Unterstützung der kleineren Städte der Toskana, die sich gegen die Unterwerfung unter florentinische Herrschaft wehrten. Florenz war auf sehr kostspielige Söldnertruppen angewiesen und litt daher unter den hohen Kriegskosten. Der dritte Krieg gegen Giangaleazzo verlief für die Florentiner ungünstig; am Ende standen sie 1402 ohne Verbündete da und mussten mit einer Belagerung rechnen. Nur der plötzliche Tod des Herzogs im Sommer 1402 rettete sie vor der existenziellen Gefahr.

Im Jahr 1424 führte die Expansionspolitik des Herzogs Filippo Maria Visconti zu einem neuen Krieg zwischen den beiden Städten, der bis 1428 dauerte. In diesem Kampf gegen Mailand war Florenz mit Venedig verbündet. Danach versuchten die Florentiner von Dezember 1429 bis April 1433 vergeblich, die toskanische Stadt Lucca militärisch zu unterwerfen. Lucca war theoretisch mit Florenz verbündet, stand aber faktisch auf der Seite Mailands. Cosimo, der die Aussichten auf einen Sieg über Lucca schon 1430 skeptisch beurteilt hatte, war im April 1433 maßgeblich an den Friedensverhandlungen beteiligt, die zur Beendigung der Feindseligkeiten führten.

Der Krieg gegen Lucca war für die Republik Florenz ein finanzielles Desaster, während die Medici-Bank als Kreditgeber des Staates davon profitierte. Daher gehörte zu den Anschuldigungen, die nach Cosimos Verhaftung 1433 gegen ihn erhoben wurden, auch die Behauptung, er habe den Krieg angezettelt und dann durch politische Intrigen unnötig verlängert, um daraus den größtmöglichen Profit zu schlagen. Die Glaubwürdigkeit der detaillierten Vorwürfe ist aus heutiger Sicht schwer zu beurteilen; auf jeden Fall ist mit polemischer Verzerrung zu rechnen. Unzweifelhaft ist, dass Cosimos Rivale Rinaldo degli Albizzi zu den profiliertesten Befürwortern des Krieges zählte. Nach dem Fehlschlag spielte die Schuldfrage in den innenpolitischen Machtkämpfen der Florentiner Patriziergeschlechter offenbar eine wichtige Rolle.

Das politische Gewicht der Medici zeigte sich in den Verhandlungen, die 1438 über die Verlagerung des in Ferrara tagenden Konzils nach Florenz geführt wurden. Cosimo hielt sich damals als Gesandter der Republik Florenz monatelang in Ferrara auf und verhandelte mit Papst Eugen IV. und dessen Mitarbeitern. Auch sein Bruder Lorenzo gehörte zu den maßgeblichen Akteuren. Die Florentiner erhofften sich von den guten Beziehungen der Medici zur Kurie eine wirksame Unterstützung ihres Anliegens. Tatsächlich kam eine Vereinbarung über den Umzug nach Florenz zustande, die einen bedeutenden Erfolg der florentinischen Diplomatie darstellte.

Auch nachdem Cosimo den innenpolitischen Machtkampf 1434 gewonnen hatte, blieb die Auseinandersetzung mit Filippo Maria Visconti eine zentrale Herausforderung für die auswärtige Politik der Republik Florenz. Der Konflikt wurde wiederum militärisch ausgetragen. Verbannte Florentiner Gegner der Medici, darunter Rinaldo degli Albizzi, hatten sich nach Mailand begeben; sie hofften, Filippo Maria werde ihnen die Heimkehr mit Waffengewalt ermöglichen. Florenz war mit Papst Eugen IV. und Venedig verbündet. In der Schlacht von Anghiari besiegten im Jahr 1440 Truppen dieser Koalition das mailändische Heer. Damit war der Versuch der exilierten Feinde Cosimos, ihn mit ausländischer Hilfe zu stürzen, endgültig gescheitert. Im folgenden Jahr wurde ein für Florenz vorteilhafter Friedensvertrag geschlossen, der zur Festigung von Cosimos Herrschaft beitrug. Die Feindschaft zwischen Mailand und Florenz dauerte aber an, bis Filippo Maria 1447 ohne männlichen Erben starb und damit die Dynastie der Visconti erlosch.

Cosimo betrachtete das Bündnis mit Venedig und den Kampf gegen Mailand nicht als naturgegebene, zwangsläufige Konstellation, sondern nur als Folge der unvermeidlichen Konfrontation mit dem Geschlecht der Visconti. Sein langfristiges Ziel war eine Allianz mit Mailand, die der bedrohlichen Ausweitung des venezianischen Machtbereichs auf dem Festland entgegentreten sollte. Dies setzte einen Dynastiewechsel in Mailand voraus. Nach dem Tod Filippo Marias drohte dort ein Machtvakuum. Als Folge war aus Cosimos Sicht die Auflösung des Herrschaftsbereichs der erloschenen Familie Visconti und damit eine Hegemonie Venedigs in Norditalien zu befürchten. Daher war es ein zentrales Anliegen des Florentiner Staatsmanns, dass in Mailand ein neues, ihm freundlich gesinntes Geschlecht von Herzögen an die Macht kam. Sein Kandidat war der Condottiere Francesco Sforza, der mit Filippo Marias unehelicher Tochter und Erbin Bianca Maria verheiratet war. Sforzas Ehrgeiz, die Nachfolge des letzten Visconti anzutreten, war seit langem bekannt.
Diese Konstellation hatte eine bewegte Vorgeschichte. Ab 1425 stand Sforza im Dienst Filippo Marias, der ihn zu seinem Schwiegersohn machen wollte, um ihn an sich zu binden. Im Jahr 1430 trug er zur Rettung Luccas vor einem Angriff der Florentiner bei. Im März 1434 ließ er sich aber von Eugen IV. für die Gegenseite, die Allianz der Visconti-Gegner, anwerben. Darauf belagerte er 1437 Lucca, das die Florentiner weiterhin unterwerfen wollten. Dies hinderte ihn aber nicht daran, erneut mit Filippo Maria über die geplante Ehe mit dessen Erbin zu verhandeln. Schließlich kam im März 1438 eine Einigung zustande: Die Heirat wurde beschlossen und die Mitgift festgelegt. Sforza durfte im Dienst der Florentiner bleiben, verpflichtete sich aber, nicht gegen Mailand zu kämpfen. Florenz und Mailand schlossen einen Waffenstillstand. Doch schon im Februar 1439 vollzog Sforza einen neuen Wechsel: Er nahm den Vorschlag der Florentiner und Venezianer an, das Kommando der Truppen der antimailändischen Liga zu übernehmen. Als Filippo Maria nach verlustreichen Kämpfen in eine schwierige Lage geraten war, sah er sich 1441 gezwungen, der Heirat endgültig zuzustimmen. Sforza musste dieses Zugeständnis des Herzogs, das ihn zu dessen präsumptivem Nachfolger machte, nicht mit einem neuen Allianzwechsel erkaufen; er blieb auch nach der Hochzeit Befehlshaber der Streitkräfte der Liga. Sein Verhältnis zu seinem Schwiegervater schwankte in der Folgezeit weiterhin zwischen Bündnis und militärischer Konfrontation.

In dieser Zeit schnell wechselnder Verbindungen entstand zwischen Francesco Sforza und Cosimo de’ Medici eine dauerhafte Freundschaft. Die beiden Männer schlossen ein persönliches Bündnis als Grundlage für eine künftige florentinisch-mailändische Allianz nach dem geplanten Machtwechsel in Mailand. Die Medici-Bank half dem Condottiere mit umfangreicher Kreditgewährung; als er 1466 starb, schuldete er ihr mehr als 115.000 Dukaten. Überdies stellte ihm die Republik Florenz auf Veranlassung Cosimos beträchtliche finanzielle Mittel zur Verfügung. Dieser Kurs war allerdings bei den Florentiner Patriziern – auch in Cosimos Anhängerschaft – umstritten. Es gab beträchtliche Vorbehalte gegen Sforza, die von der republikanischen Abneigung gegen Alleinherrscher genährt wurden. Außerdem entfremdete Cosimos Strategie ihm den Papst, der mit Sforza in einem Territorialstreit lag und sich daher gegen den Condottiere mit Filippo Maria verbündete. Eugen IV. wurde zu einem Gegner Cosimos, mit dem er zuvor erfolgreich zusammengearbeitet hatte. Ab 1443 residierte er nicht mehr in Florenz, wohin er 1434 geflohen war, sondern wieder in Rom. Seine neue Haltung zeigte sich sogleich darin, dass er dem Leiter der römischen Filiale der Medici-Bank das einträgliche Amt des päpstlichen Generaldepositars entzog. Als der Erzbischof von Florenz starb, ernannte Eugen den Dominikaner Antonino Pierozzi, der Cosimo sehr distanziert gegenüberstand, zum Nachfolger. Der Mediceer seinerseits unterstützte offen einen erfolglosen Versuch Sforzas, sich Roms zu bemächtigen. Nach dem Tod Eugens, der 1447 starb, gelang es Cosimo jedoch, zum Nachfolger Nikolaus V. ein gutes Verhältnis aufzubauen. Sein Vertrauensmann in Rom, Roberto Martelli, wurde wieder Generaldepositar.
In Mailand setzten sich nach Filippo Marias Tod zunächst republikanische Kräfte durch, doch gelang es Sforza im Jahr 1450, dort die Macht zu übernehmen. Nun konnte das von Cosimo gewünschte mailändisch-florentinische Bündnis verwirklicht werden, das eine tiefgreifende Änderung der politischen Verhältnisse bewirkte. Es wurde zu einer „Hauptachse der italienischen Politik“ und erwies sich damit als bedeutender außenpolitischer Erfolg des Florentiner Staatsmanns. Allerdings führte es zum Bruch der traditionellen Allianz der Republiken Florenz und Venedig. Die Venezianer, die gehofft hatten, vom Untergang der Visconti zu profitieren, waren die Verlierer der neuen Konstellation. Im Juni 1451 verbannte Venedig die florentinischen Kaufleute aus seinem Territorium. Im folgenden Jahr begann der Krieg zwischen Venedig und Mailand, Florenz blieb diesmal verschont. Die Feindseligkeiten endeten im April 1454 mit dem Frieden von Lodi, in dem Venedig Sforza als Herzog von Mailand anerkannte.

Es folgte die Gründung der Lega italica, eines Pakts, dem alle fünf Regionalmächte beitraten. Diese Vereinbarung garantierte den Besitzstand der Staaten und schuf ein stabiles Gleichgewicht der Mächte. Außerdem richtete sie sich implizit gegen Frankreich; die Vertragsmächte wollten einem militärischen Eingreifen der Franzosen auf italienischem Boden vorbeugen. Dieses insbesondere von Sforza angestrebte Ziel akzeptierte Cosimo nur zögernd. Er wollte zwar auch französische Truppen von Italien fernhalten, glaubte aber, dass Venedig für Florenz die größere Gefahr sei und daher die Option eines Bündnisses mit Frankreich erhalten bleiben solle. Schließlich schloss er sich aber Sforzas Auffassung an. Dank der Stabilität, die von der Lega italica ausging, wurde Cosimos letztes Lebensjahrzehnt zu einer Friedenszeit. Als sein Sohn Piero 1461 das Amt des "gonfaloniere di giustizia" antrat, konnte er erklären, der Staat befinde sich in einem Zustand des Friedens und des Glücks, „dessen weder die Bürger von heute noch ihre Vorfahren Zeugen waren oder sich erinnern konnten“.

Als Staatsmann und Bürger begnügte sich Cosimo bewusst mit einem niedrigen Profil und kultivierte seine Bescheidenheit, um möglichst wenig Neid und Verdacht zu erregen. Er vermied ein prunkvolles, herrscherähnliches Auftreten und achtete darauf, mit seinem Lebensstil die anderen angesehenen Bürger nicht zu übertreffen. Als Mäzen hingegen stellte er sich gezielt in den Vordergrund. Er nutzte seine Bautätigkeit und seine Stellung als Auftraggeber von Künstlern, um sich in Szene zu setzen und sein Ansehen und den Ruhm seiner Familie zu mehren.

Seine Spenden für den Bau und die Ausstattung sakraler Gebäude betrachtete Cosimo als Investitionen, die ihm Gottes Gnade verschaffen sollten. Er fasste sein Verhältnis zu Gott als Abhängigkeitsbeziehung im Sinne des Klientelismus auf: Ein Klient empfängt von seinem Patron Wohltaten und zeigt sich dafür durch Loyalität und tätige Dankbarkeit erkenntlich. Gegenüber seiner Anhängerschaft trat Cosimo als gütiger Patron auf, gegenüber Gott sah er sich als Klient. Wie sein Biograph Vespasiano da Bisticci berichtet, antwortete er auf die Frage nach dem Grund seiner großen Freigebigkeit und Fürsorge gegenüber Mönchen, er habe von Gott so viel Gnade empfangen, dass er nun sein Schuldner sei. Niemals habe er Gott einen Grosso (eine Silbermünze) gegeben, ohne von ihm dafür bei diesem „Tauschhandel“ "(iscambio)" einen Florin (eine Goldmünze) zu bekommen. Außerdem war Cosimo der Meinung, er habe mit seinem Geschäftsgebaren gegen ein göttliches Gebot verstoßen. Er befürchtete, Gott werde ihm zur Strafe seine Besitztümer wegnehmen. Um dieser Gefahr vorzubeugen und sich weiterhin das göttliche Wohlwollen zu sichern, bat er Papst Eugen IV. um Rat. Der Papst befand, eine Spende von 10.000 Florin für einen Klosterbau sei zur Bereinigung der Angelegenheit ausreichend. So wurde dann verfahren. Als der Bau vollendet war, bestätigte der Papst mit einer Bulle den Ablass, der dem Bankier für die Spende gewährt wurde.

Cosimo lebte in der Blütezeit des Renaissance-Humanismus, dessen bedeutendstes Zentrum seine Heimatstadt Florenz war. Das Ziel des humanistischen Bildungsprogramms, die Befähigung des Menschen zu einer optimalen Lebensführung und staatsbürgerlichen Pflichterfüllung durch Verbindung von Wissen und Tugend, fand damals im Florentiner Patriziat viel Anklang. Den Weg zur Verwirklichung des humanistischen Tüchtigkeitsideals sah man in der Aneignung antiker Bildungsgüter, die zur Nachahmung klassischer Vorbilder anspornen sollte. Cosimos Vater hatte sich dieser Auffassung angeschlossen; er ließ seinem Sohn eine humanistische Erziehung zukommen. Wie viele seiner gebildeten Mitbürger öffnete sich Cosimo der Gedankenwelt und den Wertvorstellungen der Humanisten. Er schätzte den Umgang mit ihnen, erwies ihnen Wohltaten und erhielt dafür seinerseits viel Anerkennung. Sein Leben lang zeigte er großes Interesse an Philosophie – insbesondere Ethik – und literarischen Werken. Dank seiner guten Schulbildung konnte er lateinische Texte lesen; seine eigenhändigen Vermerke in seinen Codices bezeugen, dass er Bücher nicht nur sammelte, sondern auch las. Wahrscheinlich war er aber nicht imstande, sich in gutem Latein auszudrücken.

Cosimos Wertschätzung für die Humanisten hing auch mit dem Umstand zusammen, dass sein gesellschaftlicher Status als erfolgreicher Bankier, Mäzen und republikanischer Staatsmann mit ihren moralischen Werten sehr gut vereinbar war. Bei seinen humanistischen Freunden konnte er mit vorbehaltloser Anerkennung rechnen, denn sie hatten ein unbefangenes Verhältnis zum Reichtum und verherrlichten seine Freigebigkeit. Großzügigkeit galt im humanistischen Milieu als eine der wertvollsten Tugenden. Dabei konnte man sich auf Aristoteles berufen, der in seiner "Nikomachischen Ethik" die Großzügigkeit oder Hochherzigkeit gepriesen und Reichtum als deren Voraussetzung bezeichnet hatte. Diese humanistische Haltung stand im Gegensatz zur Gesinnung konservativer Kreise, die das Bankwesen verdammten und Reichtum für moralisch suspekt hielten, wobei sie sich auf traditionelle christliche Wertvorstellungen beriefen. Außerdem widersprach die egalitäre Tendenz des Renaissance-Humanismus der mittelalterlichen Neigung, politische Führungspositionen denen vorzubehalten, die sich durch vornehme Abstammung auszeichneten. An die Stelle der herkömmlichen starren sozialen Ordnung, die Cosimos politische Gegner in der Albizzi-Gruppe bevorzugten, trat bei den Humanisten ein Konzept, das soziale Mobilität förderte; humanistische Bildung und persönliche Tüchtigkeit sollten als Qualifikationskriterien für die Staatslenkung ausreichen. Diese Einstellung kam Cosimo, dessen Familie zu den Aufsteigern "(gente nuova)" zählte und manchen alteingesessenen Geschlechtern suspekt war, zugute.
Besonders großzügig förderte Cosimo den humanistischen Philosophen Marsilio Ficino, dessen Vater Diotifeci d’Agnolo di Giusto sein Leibarzt war. Als väterlicher Freund verschaffte er Ficino die materielle Basis für ein ganz der Wissenschaft gewidmetes Leben. Er schenkte ihm ein Haus in Florenz und ein Landhaus in Careggi, wo er selbst eine prächtige Villa besaß. Ficino war ein begeisterter Platoniker und Bewunderer seines Gönners. Er schrieb in einem Brief an dessen Enkel Lorenzo, Platon habe ihm die platonische Idee der Tugenden einmal vor Augen gestellt, Cosimo habe sie jeden Tag in die Tat umgesetzt; daher habe er seinem Wohltäter nicht weniger zu verdanken als dem antiken Denker. Mehr als zwölf Jahre habe er glücklich mit ihm philosophiert. Im Auftrag Cosimos fertigte Ficino die erste lateinische Gesamtübersetzung der Werke Platons an, womit er maßgeblich zur Verbreitung platonischen Gedankenguts beitrug. Daraus lässt sich allerdings nicht folgern, dass Cosimo ebenso wie Ficino den Platonismus anderen philosophischen Schulrichtungen vorzog. Das Ausmaß seiner Hinwendung zum Platonismus wurde früher überschätzt; er scheint eher zum Aristotelismus geneigt zu haben. Bis gegen Ende des 20. Jahrhunderts glaubte man, Cosimo habe eine Platonische Akademie gegründet und deren Leitung Ficino übertragen. Diese Annahme ist jedoch von der neueren Forschung als falsch erwiesen worden. Es handelte sich nicht um eine Institution, sondern nur um einen informellen Kreis von Schülern Ficinos.

Auch zwei weitere namhafte Humanisten, Poggio Bracciolini und Johannes Argyropulos, beschenkte Cosimo mit Häusern. Hilfreich waren für seine humanistischen Freunde nicht nur seine Zuwendungen aus eigenen Mitteln; sie profitierten auch von seinem großen Einfluss im In- und Ausland, den er nutzte, um ihnen Gehör und Anstellungen zu verschaffen. Er sorgte dafür, dass zwei Humanisten, die er schätzte, Carlo Marsuppini und Poggio Bracciolini, das prestigereiche Amt des Kanzlers der Republik Florenz erhielten. Eng befreundet war Cosimo mit dem Geschichtsschreiber und späteren Kanzler Bartolomeo Scala und mit dem humanistisch gesinnten Mönch Ambrogio Traversari, einem angesehenen Altertumswissenschaftler. Ihn bewog er dazu, das Werk des antiken Philosophiehistorikers Diogenes Laertios über Leben und Lehren der Philosophen aus dem Griechischen ins Lateinische zu übersetzen und damit einer breiteren Öffentlichkeit zugänglich zu machen. Traversaris Kloster Santa Maria degli Angeli war der Treffpunkt einer Gruppe von Gelehrten, in deren Kreis Cosimo verkehrte. Unter ihnen war Niccolò Niccoli, ein eifriger Sammler von Handschriften antiker Werke, dem Cosimo Bücher und Geld schenkte. Poggio Bracciolini und Niccolò Niccoli betätigten sich im Konflikt mit der Albizzi-Gruppe als eifrige Anhänger des Mediceers.

Zeitweilig problematisch war Cosimos Verhältnis zu Leonardo Bruni, einem einflussreichen humanistischen Politiker und Staatstheoretiker, der sich als maßgeblicher Wortführer des florentinischen Republikanismus profilierte. Cosimo verschaffte Bruni, der aus Arezzo stammte und in Florenz eine neue Heimat gefunden hatte, 1416 das florentinische Bürgerrecht, und 1427 wurde der Humanist mit Billigung der Medici-Gruppe Staatskanzler. Dennoch pflegte Bruni auch Beziehungen zur Albizzi-Gruppe und vermied es, im Machtkampf von 1433–1434 für Cosimo Partei zu ergreifen. Trotz dieses Mangels an Loyalität zu den Medici durfte er nach 1434 das Amt des Kanzlers bis zu seinem Tod behalten und wichtigen Gremien angehören. Offenbar hielt es Cosimo für unzweckmäßig, diesen namhaften Theoretiker des republikanischen Staatskonzepts zu verärgern.

Die hohen Erwartungen, die Cosimos Wohlwollen bei den Humanisten weckte, zeigen sich darin, dass sie ihm mehr als vierzig Schriften widmeten. Dabei handelte es sich teils um Werke, die sie selbst verfasst hatten, teils um Übersetzungen. Die weite Verbreitung humanistischer Schriften, deren Widmungstexte Cosimo lobten, trug seinen Ruhm in alle Bildungsstätten West- und Mitteleuropas. Seine Bewunderer idealisierten und verherrlichten ihn auch in zahlreichen Gedichten, Briefen und Reden; sie verglichen ihn mit berühmten Staatsmännern der Antike. Erkennbar ist dabei – verstärkt in seinen letzten Lebensjahren – die Bemühung dieser Autoren, der Familie Medici dynastische Züge zu verleihen. Schon nach Cosimos Rückkehr aus dem Exil im Jahr 1434 feierten ihn seine Anhänger als "Pater patriae" („Vater des Vaterlandes“).

Einhellig war das Lob, das Cosimo zu seinen Lebzeiten bei den Humanisten fand, allerdings nicht. Einen erbitterten Gegner hatte er in dem namhaften humanistischen Gelehrten Francesco Filelfo. Dieser war 1429 mit Billigung Cosimos als Universitätslehrer nach Florenz geholt worden, überwarf sich dann aber mit dem Mediceer und nahm dezidiert für die Albizzi-Gruppe Partei. Die Medici-Gruppe versuchte seine Entlassung zu erwirken, konnte ihn aber nur vorübergehend aus der Universität vertreiben. Als 1433 ein Anschlag auf ihn verübt wurde, bei dem er eine Verletzung erlitt, verdächtigte er Cosimo, hinter dem Attentat zu stecken. Während Cosimos Exil 1433–1434 schrieb Filelfo eine heftige Satire gegen die Medici. Nach dem Umsturz von 1434, der zu Cosimos Rückkehr führte, verließ er Florenz, um der drohenden Rache der Sieger zu entgehen. In der Folgezeit bekämpfte er die Medici aus der Ferne. Im Herbst 1436 schloss er sich einer Gruppe an, die vergeblich versuchte, Cosimo von einem gedungenen Mörder umbringen zu lassen. Auf Filelfos literarische Angriffe reagierten Cosimos humanistische Verteidiger mit Entgegnungen.
Ein wichtiges Betätigungsfeld für Cosimos Mäzenatentum auf dem Gebiet der Bildungsförderung war das Bibliothekswesen. Er gründete mehrere klösterliche Bibliotheken. Die bedeutendste von ihnen befand sich im Florentiner Dominikanerkonvent San Marco. Sie war – anders als früher üblich – der Öffentlichkeit zugänglich.

Noch stärker als im literarischen Bereich engagierte sich Cosimo auf dem Gebiet der bildenden Kunst. Er ließ auf eigene Kosten Kirchen und Klöster erbauen und künstlerisch ausstatten. Damit betätigte er sich, obwohl er formal nur ein einfacher Bürger war, auf einem Gebiet, das traditionell weltlichen und geistlichen Machthabern vorbehalten war. Im 14. und frühen 15. Jahrhundert wäre eine ganz aus privater Initiative entfaltete Bautätigkeit solchen Umfangs in Florenz noch undenkbar gewesen. Erst der gesellschaftliche Wandel, der mit der fortschreitenden Entfaltung des Humanismus zusammenhing, machte derartige Vorhaben möglich. Eine humanistisch geprägte Mentalität zeigte sich auch im Willen zur Selbstdarstellung. Cosimo legte Wert darauf, dass seine Funktion als Auftraggeber sichtbaren Ausdruck fand. So ließ er an einer Kirche in Jerusalem, die mit seinen Mitteln restauriert wurde, sein Wappen anbringen, das fortan den Pilgern, die ins Heilige Land zogen und die Kirche aufsuchten, ins Auge fiel. Auch in Florenz weisen die von ihm gestifteten Bauten überall das Familienwappen der Medici auf. Nicht nur an Fassaden und Portalen, sondern auch an Kapitellen, Konsolen, Schluss-Steinen und Friesen ließ er es anbringen. Zwar waren Familienwappen in Kirchen damals in Florenz üblich, doch die Häufigkeit, mit der Cosimo das seinige überall ins Blickfeld der Öffentlichkeit rückte, war einzigartig und fiel auf.
Auch Wandgemälde mit biblischen Szenen, die im Auftrag der Medici gemalt wurden, dienten Cosimos Selbstdarstellung. Auf einem Fresko im Kloster San Marco erhielt einer der Heiligen Drei Könige die idealisierten Gesichtszüge des Mediceers. Er trägt Instrumente zur Erforschung der Gestirne. Auch auf einem um 1459 entstandenen Fresko der Heiligen Drei Könige an der Ostwand der Kapelle des Medici-Palastes befindet sich ein Porträt Cosimos. Dort ist er mit seinen Söhnen Piero und Giovanni und den Enkeln Lorenzo – später als Lorenzo il Magnifico bekannt – und Giuliano abgebildet. Im Grünen Kreuzgang von Santa Maria Novella ist Cosimo auf einer Lünette mit einer Szene aus der Sintfluterzählung zu sehen; anscheinend erscheint er dort als Personifikation der Weisheit. Für dieses Werk von Paolo Uccello war er wohl nicht selbst der Auftraggeber.
Ab 1437 entstand der Neubau des Klosters San Marco, das der Papst 1436 den Dominikaner-Observanten, einem Zweig des Dominikanerordens, übertragen hatte. Die bisherigen Klostergebäude wurden durch Neubauten ersetzt, von der Kirche wurde nur der Chor erneuert. Die Weihe der Kirche fand 1443 in Anwesenheit des Papstes statt, die Konventsgebäude wurden erst 1452 komplett fertiggestellt. Ursprünglich hatte Cosimo dafür mit Kosten von 10.000 Florin gerechnet, schließlich musste er insgesamt über 40.000 ausgeben. Für den Neubau der Basilica di San Lorenzo, einer bedeutenden Kirche, stellte er über 40.000 Florin bereit. An der Finanzierung dieses Großprojekts hatte sich schon sein Vater beteiligt. Im Mugello nördlich von Florenz, der Gegend, aus der die Medici ursprünglich stammten, förderte er den Bau des Franziskanerklosters San Francesco al Bosco (Bosco ai Frati). Bei der Franziskanerkirche Santa Croce ließ er einen Trakt für die Novizen bauen. Unter den weiteren kirchlichen Bauvorhaben, die er finanzierte, war das bedeutendste die Badia di Fiesole, das Kloster der Augustiner-Eremiten unterhalb von Fiesole. Dort ließ Cosimo ab 1456 das gesamte Klostergebäude einschließlich der Kirche neu errichten und mit einer Bibliothek ausstatten. Die Bauarbeiten waren bei seinem Tod noch nicht abgeschlossen.
Außer den Sakralbauten ließ Cosimo auch ein imposantes privates Gebäude errichten, den neuen Medici-Palast. Zuvor wohnte er in einem vergleichsweise bescheidenen älteren Palast, der Casa Vecchia. Erst 1445/1446, nachdem er bereits mit Kirchen- und Klosterbauten seine Großzügigkeit im Dienst des Gemeinwesens unter Beweis gestellt hatte, begann er mit dem aufwendigen Neubau des Familienpalastes an der damaligen Via Larga, der heutigen Via Cavour. In erster Linie ging es ihm dabei nicht um seinen eigenen Wohnkomfort, sondern um das Ansehen der Familie. Damit folgte er einer damals herrschenden sozialen Norm; die Wahrung und Mehrung des Ruhms der Familie war generell für Angehörige der Oberschicht eine zentrale Aufgabe. Der neue "palazzo" der Medici übertraf alle älteren Familienpaläste in Florenz an Größe und Ausstattung. Seine außergewöhnliche architektonische Qualität setzte einen neuen Maßstab für den Palastbau der Renaissance. Die Kapelle wurde von Benozzo Gozzoli mit Fresken geschmückt. An der Ausstattung des Palastes mit kostbaren Bildern waren auch die damals sehr geschätzten Maler Fra Angelico, Domenico Veneziano und Filippo Lippi beteiligt. Es wurde eine Umgebung geschaffen, in der prominente auswärtige Gäste repräsentativ empfangen werden konnten. Papst Pius II. meinte, dieses Bauwerk sei eines Königs würdig. Nach seinem Eindruck verfügte Cosimo über einen Reichtum, der vielleicht den des sprichwörtlichen Königs Krösus übertraf. Die Schätzungen der Baukosten schwanken zwischen 60.000 und 100.000 Florin.
Das Staunen der Zeitgenossen spiegelt sich in den Worten des Architekten und Architekturtheoretikers Filarete, der sich in seinem 1464 vollendeten "Trattato di architettura" äußerte. Filarete hob besonders die Würde "(dignitade)" der neuen Gebäude hervor. Er verglich Cosimo mit bedeutenden antiken Bauherren wie Marcus Vipsanius Agrippa und Lucius Licinius Lucullus. Diese seien allerdings keine bloßen Privatleute gewesen, sondern hätten große Provinzen regiert und seien dadurch zu ihrem Reichtum gekommen. Cosimo hingegen sei ein einfacher Bürger, der sein Vermögen durch seine unternehmerische Tatkraft erworben habe. Daher sei seine Leistung als Bauherr einzigartig.

Cosimos Neubauten veränderten das zuvor ganz vom Mittelalter geprägte Stadtbild. Sie trugen maßgeblich zur Einführung eines neuen Architekturtyps bei, mit dem Florenz zu einem Muster für ganz Italien wurde. Der neue Stil verband Zweckmäßigkeit mit antiker Proportionalität und antikisierendem Schmuck. Diese Stilrichtung hatte schon Filippo Brunelleschi, ein führender Architekt der Frührenaissance, eingeführt. Er hatte 1420 den Neubau von San Lorenzo begonnen und wurde dann 1442 von Cosimo beauftragt, das Werk zu vollenden. Ansonsten zog der Mediceer aber einen anderen Architekten, Michelozzo, vor, dessen Entwürfe weniger grandios waren als die Brunelleschis. Ob der Medici-Palast von Brunelleschi oder von Michelozzo entworfen wurde, ist in der Forschung umstritten; vermutlich waren beide beteiligt. In den lobenden Beschreibungen von Zeitgenossen wurden an Cosimos Bauten vor allem die Ordnung, die Würde, die Weite, die Schönheit der Proportionen und des architektonischen Schmucks und die Helligkeit hervorgehoben. Anerkennung fand ferner die leichte Begehbarkeit der Treppen. Sie stellte eine Neuerung dar, denn mittelalterliche Treppen waren gewöhnlich eng und steil. Die breiten Treppen mit niedrigen Stufen wurden sehr geschätzt, da sie ein bequemes und zugleich würdevolles Treppensteigen ermöglichten.
Die aufwendige Bautätigkeit des Mediceers, die an Umfang diejenige jedes anderen Privatmanns im 15. Jahrhundert übertraf, wurde von den Bürgern nicht nur wohlwollend und dankbar aufgenommen. Es wurde auch Kritik an der damit verbundenen Selbstdarstellung des reichsten Bürgers der Stadt laut. Die unterschiedlichen Ansichten und Bewertungen der Zeitgenossen sind aus einer Verteidigungsschrift ersichtlich, die der Theologe und Humanist Timoteo Maffei kurz vor 1456 zur Rechtfertigung des angegriffenen Mäzens verfasste. Maffei wählte für seine Darstellung die Form eines Dialogs, in dem er als Fürsprecher Cosimos einen Kritiker "(detractor)" widerlegt und schließlich überzeugt. Auf den Vorwurf, der Medici-Palast sei zu luxuriös, erwidert er, Cosimo habe sich dabei nicht nach dem gerichtet, was für ihn persönlich angemessen sei, sondern nach dem, was für eine so bedeutende Stadt wie Florenz passend sei. Da er von der Stadt weit größere Wohltaten empfangen habe als die anderen Bürger, habe er sich genötigt gesehen, sie entsprechend üppiger zu schmücken als jeder andere, um sich nicht als undankbar zu erweisen. Zur Entkräftung der Kritik an dem überall angebrachten Medici-Wappen bringt Maffei vor, der Zweck des Wappenzeichens bestehe darin, auf ein Vorbild aufmerksam zu machen, das zur Nachahmung anspornen solle.

Auch der Bildhauer Donatello arbeitete für Cosimo oder vielleicht für dessen Sohn Piero. Im Auftrag der Medici schuf er zwei berühmte Bronzeskulpturen, den David und die Judith. Beide Werke hatten einen politischen Hintergrund; die dargestellten biblischen Gestalten versinnbildlichten den Sieg über einen scheinbar übermächtigen Feind. Es ging um Ermutigung zur Verteidigung der Freiheit des Vaterlandes und der republikanischen Verfassung gegen Gefährdungen von außen.

Als Privatmann war Cosimo für seine Bescheidenheit und seinen Grundsatz des Maßhaltens bekannt. Seinen Palast und seine Villen gestaltete er zwar repräsentativ, doch achtete er darauf, in seiner Lebensführung unnötigen Aufwand, der Anstoß erregen konnte, zu vermeiden. So begnügte er sich mit einfachen Speisen und trug keine prächtige Kleidung. Dazu passte seine Betätigung in der Landwirtschaft, in der er sich gut auskannte. Auf seinen Besitzungen außerhalb der Stadt leistete er Landarbeit, er pfropfte Bäume und beschnitt Weinstöcke. Im Umgang mit den Bauern demonstrierte er Volksnähe; er fragte sie gern, wenn sie nach Florenz auf den Markt kamen, nach ihren Früchten und deren Herkunft.
Der Buchhändler Vespasiano da Bisticci verfasste eine verherrlichende Biographie Cosimos, mit dem er befreundet war. Darin trug er unter anderem Anekdoten aus dem Privatleben zusammen, für deren Authentizität er sich verbürgte. Er schilderte seinen Freund als Menschen von ernsthafter Wesensart, der sich mit gelehrten, würdevollen Männern umgeben habe. Er habe über ein vorzügliches Gedächtnis verfügt, sei ein geduldiger Zuhörer gewesen und habe niemals schlecht über jemanden geredet. Dank seiner umfassenden Kenntnis unterschiedlicher Wissensgebiete habe er mit jedem ein Thema gefunden. Er sei überaus freundlich und bescheiden gewesen, habe darauf geachtet, niemanden zu beleidigen, und nur wenige hätten ihn je erregt gesehen. Alle seine Antworten seien „mit Salz gewürzt“ gewesen.

Cosimo war für seine humorvollen und geistreichen, teils rätselhaften Bemerkungen bekannt, die im 15. und 16. Jahrhundert in einer Reihe von Anekdoten verbreitet wurden.

Cosimo litt an der Gicht. Die Anfälligkeit für diese Krankheit war in seiner Familie erblich. Ab 1455 scheint das Leiden ihn erheblich behindert zu haben. Er starb am 1. August 1464 in seiner Villa in Careggi und wurde am folgenden Tag in San Lorenzo beigesetzt. Pompöse Begräbnisfeierlichkeiten hatte er untersagt. Ein Testament hinterließ er nicht. Für die Gestaltung des Grabmals setzte die Signoria eigens eine zehnköpfige Kommission ein. Andrea del Verrocchio gestaltete die Grabplatte, für die ein zentraler Ort innerhalb der Kirche gewählt wurde, wie es bei Stiftergräbern üblich war. Dort wurde auf Beschluss der Stadt die Inschrift "Pater patriae" („Vater des Vaterlandes“) eingemeißelt, die an eine antike Ehrung außergewöhnlich verdienter Bürger anknüpfte. Nach der Fertigstellung des Grabmals wurden die Gebeine am 22. Oktober 1467 an ihre endgültige Stätte in der Krypta gebracht.

Mit seiner Gattin hatte Cosimo zwei Söhne, Piero (1416–1469) und Giovanni (1421–1463). Hinzu kam ein unehelicher Sohn namens Carlo, dessen Mutter eine tscherkessische Sklavin war. Carlo wurde zusammen mit seinen Halbbrüdern erzogen und schlug später eine kirchliche Karriere ein. Giovanni starb schon am 1. November 1463, neun Monate vor Cosimo, und hinterließ keine Kinder. Piero fiel das ganze väterliche Erbe zu, sowohl das Vermögen und die Führung der Bank als auch die Stellung des leitenden Staatsmanns von Florenz. Dank der Autorität seines verstorbenen Vaters konnte Piero dessen Rolle im Staat problemlos übernehmen. Er litt aber schwer an der Gicht, die seine Aktivitäten stark behinderte, und starb schon fünf Jahre nach Cosimo.

Pieros Nachfolge als informeller Machthaber trat im Dezember 1469 sein Sohn Lorenzo il Magnifico an. Wiederum verlief der Übergang ohne Komplikationen. Das neue Oberhaupt der Familie setzte die Tradition der großzügigen Kulturförderung fort und mehrte damit den Ruhm der Medici. Die von seiner Führung geprägten 22 Jahre der Geschichte von Florenz waren eine kulturell außerordentlich glanzvolle Epoche. Lorenzo verfügte aber nicht über das geschäftliche Talent seines Großvaters Cosimo. Es gelang ihm nicht, die finanzielle Basis der politischen Macht und des Mäzenatentums der Medici zu bewahren. Die Bank erlebte einen dramatischen Niedergang, der sie an den Rand des Zusammenbruchs brachte.

Ein scharfer Kritiker Cosimos war der zeitgenössische Geschichtsschreiber Giovanni Cavalcanti. Er gehörte einem alteingesessenen Patriziergeschlecht an und missbilligte den Aufstieg einer Schicht von Emporkömmlingen, für den er Cosimo verantwortlich machte. Vor allem verübelte er dem Mediceer das rigorose Vorgehen gegen die Steuerschuldner, zu denen er selbst zählte. Allerdings äußerte er sich stellenweise positiv über die Medici und hielt die Aufhebung von Cosimos Verbannung für gerecht.

Zeitgenössische medicifreundliche Autoren priesen Cosimo rückblickend als Retter der Unabhängigkeit der Republik Florenz. So befand der Humanist Benedetto Accolti der Ältere in seinem "Dialogus de praestantia virorum sui aevi", einem in Cosimos letzten Lebensjahren verfassten und ihm gewidmeten Werk, die Machtverhältnisse seien nach dem Tod von Filippo Maria Visconti für Venedig so günstig gewesen, dass die Venezianer ganz Italien hätten unterwerfen können, wenn Cosimo dies nicht durch das Bündnis mit Mailand verhindert hätte. Er allein sei der Urheber des Allianzwechsels, den er gegen starken Widerstand in Florenz durchgesetzt habe. In diesem Sinn äußerte sich auch der Geschichtsschreiber Benedetto Dei. Er verfasste in den 1470er Jahren ein gegen Venedig gerichtetes Pamphlet, in dem er Cosimos Außenpolitik rückblickend als weitsichtig und erfolgreich darstellte. Nach seiner Einschätzung hätte Venedig in Italien eine beherrschende Stellung errungen, wenn Cosimo nicht die Allianz mit Francesco Sforza zuwege gebracht hätte.

Im Zeitraum 1469–1475 schuf Sandro Botticelli im Auftrag des Bankiers G(u)aspar(r)e di Zanobi del Lama ein Gemälde, das die Anbetung der Heiligen Drei Könige zeigt. Der älteste der Könige trägt die Gesichtszüge Cosimos, auch weitere Angehörige der Familie Medici sind abgebildet. Somit soll das Werk der Familie huldigen, Cosimo erscheint als „Heiliger“.

Der Humanist Bartolomeo Platina schrieb den Dialog "De optimo cive" "(Über den besten Bürger)", den er 1474 Cosimos Enkel Lorenzo il Magnifico widmete. Mit dem „besten Bürger“ ist der leitende republikanische Staatsmann gemeint. Der Ort der Handlung ist die Villa der Medici in Careggi, den Inhalt bildet ein fiktives Gespräch zwischen dem bereits alten und gebrechlichen Cosimo als Hauptperson, Platina und dem Knaben Lorenzo. Der Vorrede zufolge wollte der Autor mit seiner Darstellung von Cosimos politischen Maximen den patriotischen Eifer der Leser anfeuern. Platina präsentierte ein Regierungsprogramm, das er dem alten Staatsmann in den Mund legte. Seine Dialogfigur Cosimo tritt für „Freiheit“ – die traditionelle republikanische Lebensform – ein, warnt vor Hochmut, Anmaßung und Luxus, kritisiert Übelstände und fordert Einschreiten gegen Männer, die nach der Tyrannis streben. Sie sollen verbannt werden; hinzurichten sind sie nur, wenn sie der Beteiligung an einer Verschwörung überführt worden sind.

Neben der humanistischen Verherrlichung Cosimos in lateinischer Sprache, die sich an Gebildete richtete, gab es auch eine volkstümliche in italienischen Gedichten. In dieser für eine breitere Öffentlichkeit bestimmten Dichtung erscheint er als gütige Vaterfigur, Förderer des religiösen Lebens und des Wohlstands und heldenhafter Verteidiger der Freiheit gegen Angriffe von außen.

Im letzten Jahrzehnt des 15. Jahrhunderts zerbrach der Konsens, der die informelle Herrschaft der Medici in der Republik Florenz ermöglicht hatte. Die Familie wurde im November 1494 aus der Stadt verjagt. Dies führte zu einer Neubewertung von Cosimos Rolle. Der Mönch Girolamo Savonarola, der für die Florentiner damals die maßgebliche Autorität war, verdammte die Medici-Herrschaft als monströs und äußerte zu der Cosimo zugeschriebenen Bemerkung, der Staat werde nicht mit Vaterunser-Beten regiert, dies sei ein Tyrannenwort. Am 22. November 1495 beschloss die Signoria, die Inschrift „Vater des Vaterlandes“ am Grabmal zu tilgen. Doch 1512 brachte ein spanisches Heer die Medici zurück nach Florenz und wieder an die Macht. Daraufhin wurde die Inschrift wiederhergestellt. Im Jahr 1527 mussten die Medici aber ein weiteres Mal dem Volkszorn weichen. Nach der erneuten Vertreibung der Familie beschlossen die nunmehr regierenden Republikaner 1528 wiederum die Beseitigung der Inschrift. Diesen Schritt begründeten sie damit, dass Cosimo nicht Vater des Vaterlandes gewesen sei, sondern Tyrann des Vaterlandes. Die medicilose Republik erwies sich jedoch als kurzlebig; im August 1530 wurde die Stadt von Truppen Kaiser Karls V. gestürmt, worauf die Medici wieder an die Macht kamen. Aus der Republik wurde eine Monarchie, deren Herrscher ihre Legitimation aus der Rolle ihrer Ahnen im 15. Jahrhundert bezogen.

Der Historiker Francesco Guicciardini behandelte die Zeit bis 1464 im ersten Kapitel seiner 1508/1509 verfassten "Storie fiorentine". Er befand, Cosimo und sein berühmter Enkel Lorenzo il Magnifico seien vielleicht die beiden angesehensten Privatleute seit dem Untergang des Römischen Reichs gewesen. Der Großvater sei dem Enkel an Beharrlichkeit und Urteilskraft sowie im Umgang mit Geld überlegen gewesen. Wenn man alle Aspekte bedenke, komme man zum Ergebnis, dass Cosimo der tüchtigere der beiden großen Mediceer gewesen sei. Insbesondere lobte Guicciardini das Bündnis mit Mailand, in dem er eine bedeutende historische Leistung Cosimos sah. Die Mehrheit der Florentiner sei für die Fortsetzung der alten Allianz mit Venedig gewesen, doch habe Cosimo seine Mitbürger überzeugen können, sich mit Francesco Sforza zu verbünden. Damit habe er die Freiheit nicht nur der Republik Florenz, sondern ganz Italiens gerettet. Nach Guicciardinis Meinung hätten die Venezianer erst Mailand und dann alle übrigen italienischen Staaten unterworfen, wenn Cosimo dies nicht verhindert hätte.

Niccolò Machiavelli urteilte in seinen 1520–1525 verfassten "Istorie fiorentine", Cosimo habe alle seine Zeitgenossen nicht nur an Autorität und Reichtum, sondern auch an Freigebigkeit und Klugheit übertroffen. Niemand sei ihm zu seiner Zeit in der Staatskunst ebenbürtig gewesen. Er habe in Florenz eine fürstliche Stellung eingenommen und sei dennoch so klug gewesen, die Grenzen bürgerlicher Mäßigung nie zu überschreiten. Alle seine Werke und Taten seien königlich gewesen. Entstehende Übel habe er frühzeitig erkannt; daher habe er genug Zeit gehabt, sie nicht wachsen zu lassen oder sich gegen sie zu wappnen. Er habe nicht nur den Ehrgeiz seiner bürgerlichen Rivalen zu Hause bezwungen, sondern auch den vieler Fürsten. Das Regierungssystem Cosimos missbilligte Machiavelli allerdings. Er hielt die Verbindung einer zentralisierten, quasi monarchischen Entscheidungsstruktur mit der Notwendigkeit, dennoch weiterhin wie in der vormediceischen Republik einen breiten Konsens zu finden, für verfehlt. In der Instabilität eines solchen Konstrukts sah er eine fundamentale Schwäche.
Im Jahr 1537 erlangte der Mediceer Cosimo I. die Würde eines Herzogs der Toskana. Der Herzog, der bis 1574 regierte (ab 1569 als Großherzog), war ein Nachkomme Lorenzos, des jüngeren Bruders von Cosimo il Vecchio. Er ließ im Palazzo della Signoria (Palazzo Vecchio) einen „Saal von Cosimo il Vecchio“ zu Ehren des Begründers von Ruhm und Macht der Medici einrichten. Die "Sala di Cosimo il Vecchio" wurde von Giorgio Vasari und seinen Gehilfen ausgemalt. Dabei wurde das Kirchenbauprogramm des berühmten Mäzens besonders hervorgehoben. Eines der Gemälde stellt seine Rückkehr aus dem venezianischen Exil als Triumph dar.

Im Zeitalter der Aufklärung wurde Cosimo wegen seiner Förderung des Humanismus geschätzt. Voltaire äußerte sich in seinem 1756 veröffentlichten "Essai sur les mœurs et l’esprit des nations" mit Begeisterung. Er urteilte, die frühen Medici hätten ihre Macht durch Wohltaten und Tugenden erlangt, daher sei sie legitimer als die jedes Herrschergeschlechts. Cosimo habe seinen Reichtum dazu genutzt, den Armen zu helfen, sein Vaterland mit Bauten zu schmücken und die aus Konstantinopel vertriebenen griechischen Gelehrten nach Florenz zu holen. Mit seinen Wohltaten habe er sich die Autorität verschafft, die bewirkt habe, dass seine Empfehlungen drei Jahrzehnte lang wie Gesetze befolgt worden seien. Edward Gibbon rühmte Cosimo im 1788 erschienenen sechsten Band seiner "History of the Decline and Fall of the Roman Empire" mit den Worten, er habe seine Reichtümer in den Dienst der Menschheit gestellt; der Name Medici sei fast gleichbedeutend mit der Wiederherstellung der Bildung.

Johann Wolfgang von Goethe würdigte Cosimo im Anhang zu seiner 1803 veröffentlichten Übersetzung der Autobiographie des Benvenuto Cellini. Dort beschrieb er die Patronage des Mediceers als „allgemeine Spende, die an Bestechung gränzt“. Als „großer Handelsmann“, der „das Zaubermittel zu allen Zwecken in Händen trägt“, sei er „an und für sich ein Staatsmann“ gewesen. Zu Cosimos kulturellen Aktivitäten bemerkte Goethe: „Selbst vieles, was er für Literatur und Kunst gethan, scheint in dem großen Sinne des Handelsmanns geschehen zu sein, der köstliche Waaren in Umlauf zu bringen und das Beste davon selbst zu besitzen sich zur Ehre rechnet.“

Georg Voigt veröffentlichte 1859 seine für die Erforschung des Frühhumanismus wegweisende Schrift "Die Wiederbelebung des classischen Alterthums". In diesem Werk, das 1893 in dritter Auflage erschien, konstatierte Voigt, die Literatur- und Kunstgeschichte habe Cosimo „mit einer Art von Heiligenschein umkleidet“. Er sei „der leibhaftigste Typus des florentinischen Edelmanns als großartiger Kaufherr, als kluger und überschauender Staatsmann, als Repräsentant der feinen Modebildung, als mäcenatischer Geist im fürstlichen Sinne“ gewesen. Seinen Blick habe er „auf das Weite und Allgemeine gerichtet“, seine Macht habe er in einer „kalt berechneten und geräuschlosen Weise“ gefestigt. Jedes wissenschaftliche Verdienst habe er nach Gebühr anerkannt, die Talente herangezogen, ihnen Stellung und Besoldung angewiesen.

Jacob Burckhardt zeichnete in der 1869 erschienenen zweiten Auflage seiner einflussreichen Schrift "Die Kultur der Renaissance in Italien" ein heute teilweise überholtes Bild von Cosimo. Er betonte die „Führerschaft auf dem Gebiete der damaligen Bildung“, die dem Mediceer zugekommen sei. Dieser besitze den „speziellen Ruhm, in der platonischen Philosophie die schönste Blüte der antiken Gedankenwelt erkannt“ und seine Umgebung mit dieser Erkenntnis erfüllt zu haben. So habe er „innerhalb des Humanismus eine zweite und höhere Neugeburt des Altertums ans Licht gefördert“.

In kulturgeschichtlichen Darstellungen dominierte bis gegen Ende des 20. Jahrhunderts Burckhardts Sichtweise: Cosimo wurde vielfach als Gründer einer platonischen Akademie gewürdigt. So schrieb beispielsweise Agnes Heller 1982, die Gründung der Akademie in Florenz sei epochemachend gewesen. Es handle sich um die erste philosophische Schule, die „von den alten kirchlichen und universitären Rahmenbedingungen unabhängig und insofern vollkommen weltlich und ‚offen‘“ gewesen sei. Patron dieser Akademie sei „der im traditionellen Sinn (aus dem Blickwinkel der offiziellen Bildung der Zeit) unstudierte Cosimo“ gewesen. Ähnlich schilderte noch 1995 Manfred Lentzen die Rolle des Mediceers. Erst die Forschungen von James Hankins entzogen in den 1990er Jahren dem Bild von Cosimo als Akademiegründer die Grundlage.

Im verfassungsgeschichtlichen Diskurs wird die Frage erörtert, inwiefern Cosimos dominante Rolle den Rahmen der republikanischen Verfassung sprengte und seine Bezeichnung als Herrscher von Florenz daher berechtigt ist. Zur Unterscheidung von einer offenen Alleinherrschaft wird Cosimos System als „Kryptosignorie“ (versteckte Herrschaft) bezeichnet. Damit ist eine Regierungsform gemeint, die sich erst später allmählich zu einer unverhüllten Signorie, der Staatslenkung durch einen einzelnen Machthaber mit erblicher Stellung, entwickelt hat. Anthony Molho bringt die Zwiespältigkeit des Systems auf die griffige Formel „Cosimo de’ Medici – Pater patriae (Vater des Vaterlandes) oder Padrino (Pate)?“ Damit wird angedeutet, dass der Patron des Klientelsystems eine „politische Maschine“ geschaffen habe und vielleicht sogar in die Nähe von Mafia-Paten zu rücken sei. Letzteres entspricht der Auffassung von Lauro Martines und Jacques Heers. Martines sieht in der „Palette unverblümter und umfassender Kontrollmaßnahmen der mediceischen Republik“ das Instrumentarium, mit dem Cosimo die Verfassung unterminiert und die Herrschaft der „Medici-Oligarchie“, der „an der Regierung befindlichen Clique“, gesichert habe. Allerdings habe sich die republikanische Verfassung nicht so stark beugen lassen, dass sie den Medici totale Macht garantiert hätte. Die Oligarchie sei ein Team gewesen, „keine Ein-Mann-Show“, und habe ihre wichtigen Entscheidungen kollektiv gefällt. Jacques Heers zeichnet das Bild einer finsteren, brutalen Tyrannei, die Cosimo errichtet habe. Werner Goez urteilt, Florenz habe sich unter Cosimo zweifellos auf dem Weg zu fürstlicher Alleinherrschaft befunden, auch wenn alles getan worden sei, diesen Tatbestand zu verschleiern. Volker Reinhardt befindet, ab 1434 sei es zu einer „eigentümlichen Vermischung“ von Signorie und Republik gekommen; rein republikanisch sei nur noch die Fassade gewesen. Michele Luzzati hält die Entwicklung für unausweichlich; es sei Cosimos wahre und große Einsicht gewesen, dass politische Stabilität in Florenz nur noch mit einem System erreichbar gewesen sei, das unter Wahrung der freiheitlichen Tradition auf dem Vorrang eines Mannes und einer Familie beruhte. Dieser Ansicht ist auch Ferdinand Schevill. Nach seiner Einschätzung führten die Verfassungsbestimmungen, die sehr kurze Amtszeiten und Auswahl der höchsten Amtsträger durch das Los aus einer großen Menge von Kandidaten vorschrieben, zu unhaltbaren Zuständen, denn sie bewirkten, dass ein hoher Prozentsatz von offenkundig Inkompetenten in Führungsstellungen gelangte und eine durchdachte, beständige Politik unmöglich war. Schevill meint, dieses System habe die elementarsten Forderungen der Vernunft missachtet; daher sei seine Umgehung und Umgestaltung unumgänglich gewesen.

Das verbreitete Bild von Cosimo als faktisch unumschränktem Herrscher wird allerdings von manchen Historikern für irreführend gehalten. Spezialuntersuchungen haben gezeigt, dass er seinen Willen keineswegs mühelos durchsetzen konnte und auch nach der Jahrhundertmitte weiterhin auf beträchtlichen, offenen Widerstand stieß. Nicolai Rubinsteins Analyse der Krise von 1455–1458 lässt das Ausmaß der zeitweiligen innenpolitischen Schwächung des Mediceers erkennen. Rubinstein kommt zum Ergebnis, dass Cosimo keineswegs Gehorsam als selbstverständlich voraussetzen konnte, auch nicht in seiner eigenen Anhängerschaft und nicht einmal bei der machtpolitisch zentralen Regelung der Ämterbesetzung. Es blieb ihm nicht erspart, Überzeugungsarbeit zu leisten. Rubinstein meint, auswärtige Zeitgenossen hätten Cosimos Macht wahrscheinlich überschätzt, sie werde wohl in Quellen wie den mailändischen Gesandtschaftsberichten teils übertrieben dargestellt. Dies führt er unter anderem darauf zurück, dass in despotisch regierten Staaten das nötige Verständnis der republikanischen Mentalität gefehlt habe; daher habe man dort die Bedeutung von Beratung und Konsens in einer Republik wie Florenz nicht angemessen berücksichtigt. Dale Kent schließt sich aufgrund eigener Forschungsergebnisse Rubinsteins Auffassung an. Auch Paolo Margaroli weist auf die Grenzen von Cosimos Macht hin. Als Beispiel nennt er die Friedensverhandlungen in Rom, bei denen 1453 die florentinischen Unterhändler so agierten, dass sie es nach Cosimos Ansicht, wie er dem Herzog von Mailand schrieb, nicht schlechter hätten machen können. Diese Gesandtschaft war in Florenz von oppositionellen Kräften vorbereitet worden. Michele Luzzati betont das Gewicht der seit Generationen kritischen öffentlichen Meinung, die Cosimo nicht habe missachten können. Nach der Darstellung von Daniel Höchli waren die meisten Patrizier nicht bereit, sich den Medici zu unterwerfen. Sie konnten dank eigener Patronage-Netzwerke ihre politische Unabhängigkeit bis zu einem gewissen Grad bewahren. Die Führungsrolle der Medici akzeptierten sie nur, solange sie ihre eigenen Interessen gewahrt sahen.

Mit der Debatte über die Natur der Kryptosignorie hängt die Frage zusammen, inwieweit das dezidiert republikanische, antiautokratische Gedankengut des Florentiner „Bürgerhumanismus“ – ein von Hans Baron geprägter Begriff – mit Cosimos Stellung im Staat vereinbar war. Die ältere Forschung – vor allem Hans Baron und Eugenio Garin – ging von einem fundamentalen Spannungsverhältnis aus. Man nahm an, der manipulative Charakter der Medici-Herrschaft habe das Grundprinzip des Bürgerhumanismus, die Ermutigung der Bürger zu einer aktiven und verantwortungsvollen Beteiligung am politischen Leben, unterminiert. Die Verbreitung eines unpolitischen Neuplatonismus nach der Jahrhundertmitte sei als Ausdruck der Abkehr der Humanisten von einer echt republikanischen Gesinnung zu deuten. Diese Sichtweise ist von der neueren Forschung, insbesondere unter dem Eindruck der Ergebnisse von James Hankins, aufgegeben worden. Es wird u. a. darauf hingewiesen, dass Leonardo Bruni als profilierter Theoretiker und Wortführer des Bürgerhumanismus keinen Gegensatz zwischen seiner Überzeugung und seiner Zusammenarbeit mit Cosimo sah. Nach der neueren Interpretation ist das Verhältnis zwischen Bürgerhumanismus und Medici-Herrschaft eher als Symbiose auf der Basis bedeutender Gemeinsamkeiten zu verstehen.

Als Ursache für Cosimos Erfolge wird in der Forschung insbesondere seine geschickte Finanzpolitik hervorgehoben, die ihm in den innenpolitischen Kämpfen bedeutende Vorteile verschafft habe. So konstatieren Werner Goez, Lauro Martines und Jacques Heers, Cosimo habe seine politische Macht vor allem dazu eingesetzt, die mit den Medici rivalisierenden Clans und Banken niederzuhalten. Mittels der Steuergesetzgebung habe er die Vermögen seiner Rivalen und missliebiger Personen belastet, um sich ihrer zu entledigen. Es gibt aber keinen Beleg dafür, dass er versuchte, politische Gegner durch direkte kommerzielle Angriffe auf ihre Unternehmen zu schädigen. Jacques Heers bestreitet, dass Cosimo durch seinen Reichtum an die Macht gelangte. Vielmehr sei es umgekehrt der Machtbesitz gewesen, den er zur Anhäufung der Reichtümer genutzt habe.

Als zentraler Faktor, der die Macht des Mediceers in Florenz befestigte, gilt in der Forschung sein Ansehen im Ausland und insbesondere sein Einfluss an der Kurie. Große Bedeutung wird auch seinem propagandistischen Geschick beigemessen. Dale Kent charakterisiert Cosimo als Meister der Selbstdarstellung, der sein Image sorgfältig kultiviert habe. Nach Kents Einschätzung ist sein einzigartiger Erfolg darauf zurückzuführen, dass er das war oder zumindest zu sein schien, was den Wünschen seiner Mitbürger entsprach: ein Wortführer, der ihre Wertvorstellungen artikulierte, und zugleich ein scharfsichtiger, abwägender Staatsmann, der nach außen als Stimme der Republik auftreten konnte und durch seine Führungsrolle den in der Verfassung angelegten Mangel an politischer Konsistenz kompensierte.

Als bedeutende außenpolitische Leistung Cosimos wird das Bündnis mit Mailand gegen Venedig beurteilt. Für Hans Baron handelt es sich um einen meisterhaften Schachzug. Nicolai Rubinstein meint, dieser Erfolg habe wohl mehr als jedes andere Ereignis nach 1434 das Ansehen des Mediceers im In- und Ausland gefestigt. Volker Reinhardt befindet, Cosimo habe „vorausschauend wie immer“ in die Karriere Sforzas viel Geld investiert, das sich dann als politische Rendite amortisiert habe. Die von ihm herbeigeführte Allianz zwischen Florenz und Mailand habe sich „als tragfähige Achse der italienischen Politik insgesamt“ erwiesen. Vincent Ilardi teilt diese Einschätzung der Allianz, merkt aber kritisch an, Cosimo habe die von Frankreich ausgehende Gefahr unterschätzt. Seine Neigung zu einem Bündnis mit Frankreich gegen Venedig sei ein Fehler gewesen. Sforza habe diesbezüglich mehr staatsmännische Voraussicht gezeigt.

Die Quellen zu Cosimos Leben, seiner Rolle als Staatsmann und Mäzen und zur Rezeptionsgeschichte sind sehr reichhaltig. Aus seiner Zeit sind etwa dreißigtausend von den Medici verfasste oder an sie gerichtete Briefe erhalten. Eine Fülle von einschlägigen Briefen und Dokumenten befindet sich im Staatsarchiv von Florenz in der Sammlung „Medici avanti il Principato“ (MAP), deren Grundstock Cosimos Privatarchiv bildet, sowie im Mailänder Staatsarchiv und anderen Archiven und Bibliotheken. Diese Archivalien geben sowohl über politische und geschäftliche Angelegenheiten als auch über Privates Aufschluss. Informativ sind auch die ausführlichen Steuerunterlagen, die im Staatsarchiv von Florenz aufbewahrt werden, sowie Unterlagen der Medici-Bank in verschiedenen Archiven. Hinzu kommen Aufzeichnungen über Sitzungen und Debatten, an denen die Medici und ihre Freunde teilnahmen und das Wort ergriffen. Gut dokumentiert sind die diplomatischen Aktivitäten; Gesandtschaftsberichte und Instruktionen, die den Gesandten erteilt wurden, erhellen Cosimos Rolle in der italienischen Politik. Hohen Quellenwert hat sein Briefwechsel mit Francesco Sforza. Zahlreiche erzählende Quellen in lateinischer und italienischer Sprache beleuchten das Bild Cosimos bei seinen Zeitgenossen und in den Jahrzehnten nach seinem Tod. Zu den wichtigsten edierten Quellen zählen:


Übersichtsdarstellungen und Einführungen
Aufsatzsammlung
Innenpolitik
Bankwesen
Außenpolitik
Kulturelle Bedeutung und Privatleben
Rezeption



</doc>
<doc id="925" url="https://de.wikipedia.org/wiki?curid=925" title="Christiaan Huygens">
Christiaan Huygens

Christiaan Huygens [] () (* 14. April 1629 in Den Haag; † 8. Juli 1695 ebenda), auch Christianus Hugenius, war ein niederländischer Astronom, Mathematiker und Physiker. Huygens gilt, obwohl er sich niemals der noch zu seinen Lebzeiten entwickelten Infinitesimalrechnung bediente, als einer der führenden Mathematiker und Physiker des 17. Jahrhunderts. Er ist der Begründer der Wellentheorie des Lichts, formulierte in seinen Untersuchungen zum elastischen Stoß ein Relativitätsprinzip und konstruierte die ersten Pendeluhren. Mit von ihm verbesserten Teleskopen gelangen ihm wichtige astronomische Entdeckungen.

Huygens wurde als Sohn von Constantijn Huygens geboren, der Sprachgelehrter, Diplomat, Komponist und der damals führende Dichter Hollands war. Durch seinen Vater kam Christiaan schon früh mit bedeutenden Persönlichkeiten in Kontakt, unter anderem mit Rembrandt, Peter Paul Rubens und René Descartes. Christiaan wurde als Kind von seinem Vater unterrichtet. Später studierte er an der Universität Leiden zunächst Rechtswissenschaften, wechselte dann aber bald zu Mathematik und Naturwissenschaften.

Seine erste veröffentlichte Arbeit (1651) befasste sich mit der Quadratur von Kegeln und zeigte einen Fehler in einem angeblichen Beweis der Quadratur des Kreises. Weiter beschäftigte er sich mit der Kreiszahl π (pi), Logarithmen und leistete wichtige Vorarbeiten für die Infinitesimalrechnung, auf denen dann Leibniz und Newton aufbauen konnten.
1657 veröffentlichte er die erste Abhandlung über die Theorie des Würfelspiels ("De ludo aleae"), wodurch er heute als einer der Begründer der Wahrscheinlichkeitsrechnung gilt. Vorausgegangen waren Briefwechsel zwischen Blaise Pascal und Pierre de Fermat, über dessen Inhalt Huygens, wie er behauptete, jedoch nichts bekannt war. Analysiert man die Lösungen der fünf am Ende seiner Abhandlung aufgeführten Probleme, muss man vermuten, dass er Pascals Vorstellungen wohl gekannt hat, nicht aber die kombinatorischen Wege von Fermat.

Zunehmend interessierte sich Huygens auch für die damals modernen Bereiche der Naturwissenschaften, Optik und die Astronomie mit Teleskopen. Er hatte Kontakt zu Antoni van Leeuwenhoek, dem damals führenden Linsenschleifer und Konstrukteur von Mikroskopen. Kurzzeitig untersuchte auch Huygens kleine Objekte unter dem Mikroskop.
Er begann aber bald selbst, Linsen für Teleskope zu schleifen und konstruierte zusammen mit seinem Bruder Constantijn Huygens Junior sein erstes Fernrohr. Huygens entwickelte die Wellentheorie des Lichts, die es ihm ermöglichte, Linsen mit geringeren Abbildungsfehlern (Aberration) zu schleifen und so bessere Teleskope zu bauen; seine Entdeckungen bewirkten auch eine Steigerung der Bildschärfe bei der Camera obscura und der Laterna magica. Er formulierte als erster das nach ihm benannte Huygenssche Prinzip, das als Grundlage der Wellenoptik gilt. Wie manch anderer Physiker seiner Zeit entwickelte auch Huygens eine eigene Theorie zu einem Äther für Licht und Gravitation.

Huygens entdeckte mit seinem selbstgebauten Teleskop 1655 erstmals den Saturnmond Titan. Damit war der Saturn der zweite Planet nach dem Jupiter (von der Erde abgesehen), bei dem ein Mond nachgewiesen werden konnte (Galileo Galilei hatte schon 1610 die vier größten Jupitermonde entdeckt). Außerdem konnte er durch die bessere Auflösung seines Teleskops erkennen, dass das, was Galilei als Ohren des Saturns bezeichnet hatte, in Wirklichkeit die Saturnringe waren.

Er fand auch heraus, dass diese Ringe keine Verbindung zum Planeten hatten und ihr geheimnisvolles Verschwinden alle 14 Jahre dadurch zustande kam, dass man sie dann genau von der Seite sah, sie aber zu dünn waren, um von der Erde aus noch wahrgenommen werden zu können.

Weitere astronomische Leistungen Huygens’ waren die Entdeckung der Rotationsbewegung des Mars und die Berechnung der Rotationsperiode (Marstag) mit ungefähr 24 Stunden sowie die Auflösung des Trapezes im Zentrum des Orion-Nebels in vier einzelne Sterne. Ihm zu Ehren wird die hellste Region des Orion-Nebels auch "Huygenssche Region" genannt. Er entdeckte ferner weitere Nebel und Doppelsternsysteme und äußerte die Vermutung, dass die Venus von einer dichten Wolkenhülle verhangen sei.

Außer für die Astronomie interessierte sich Huygens auch für die Mechanik. Er formulierte die Stoßgesetze und befasste sich mit dem Trägheitsprinzip und Fliehkräften. Seine Untersuchungen von Schwingungen und Pendelbewegungen konnte er zum Bau von Pendeluhren nutzen. Schon Galilei hatte eine solche entworfen, aber nicht gebaut. Huygens konnte seine Uhr hingegen zum Patent anmelden. Die in seinem Auftrag von Salomon Coster gebauten Uhren wiesen eine Ganggenauigkeit von zehn Sekunden pro Tag auf, eine Präzision, die erst hundert Jahre danach überboten werden konnte. Später konstruierte er auch Taschenuhren mit Spiralfedern und Unruh.

Christiaan Huygens veröffentlichte 1673 in seiner Abhandlung "Horologium Oscillatorium" eine ganggenaue Pendeluhr mit einem Zykloidenpendel, bei dem er sich die Tatsache zunutze machte, dass die Evolute der Zykloide selber wieder eine Zykloide ist. Der Vorteil in der Ganggenauigkeit wird jedoch durch den Nachteil der erhöhten Reibung wieder ausgeglichen. 

Von ihm stammt auch die früheste bekannte Uhr zur Bestimmung des Längengrades, die mehrere revolutionäre Techniken aufwies, und als deren Urheber er erst vor einiger Zeit wieder erkannt wurde.

In seiner letzten wissenschaftlichen Abhandlung 1690 formulierte Huygens den Gedanken, dass es noch viele andere Sonnen und Planeten im Universum geben könnte, und spekulierte bereits über außerirdisches Leben.

Von Huygens stammt auch die korrekte Ableitung der Gesetze des Elastischen Stoßes, wobei er von einem Relativitätsprinzip Gebrauch macht (siehe Galilei-Transformation). Er veröffentlichte seine Ergebnisse, die aus den 1650er Jahren stammten und die falsche Behandlung bei René Descartes korrigierten, 1669 (Philosophical Transactions of the Royal Society, Journal des Savants) und in seinem postum erschienenen Buch "De Motu Corporum" von 1703.

Christiaan Huygens und Samuel Sorbière (1617–1670) waren die ersten beiden ausländischen Wissenschaftler, die im Juni 1663 in die Royal Society aufgenommen wurden. 1666 wurde Huygens der erste Direktor der in diesem Jahr gegründeten französischen Akademie der Wissenschaften. Newton bezeichnete ihn als den elegantesten Mathematiker seiner Zeit.

Huygens entdeckte die Beziehungen zwischen Schallgeschwindigkeit, Länge und Tonhöhe einer Pfeife. Er beschäftigte sich intensiv mit der mitteltönigen Stimmung und berechnete 1691 die Teilung der Oktave in 31 gleiche Stufen, um den Fehler des pythagoreischen Kommas im Tonsystem der Musik zu beheben.

In den 1680er Jahren verschlechterte sich Huygens’ Gesundheitszustand, so dass er sein Haus nicht mehr häufig verließ. In den letzten Jahren seines Lebens beschäftigte sich der Wissenschaftler mit der Musiktheorie. 1695 starb Christiaan Huygens in Den Haag unverheiratet und kinderlos.



"Oeuvres complètes", 22 Bände. Den Haag 1888 bis 1950. Herausgeber: D. Bierens de Haan, Johannes Bosscha, Diederik Johannes Korteweg, Albertus Antonie Nijland, J. A. Vollgraf.





</doc>
<doc id="927" url="https://de.wikipedia.org/wiki?curid=927" title="Claus Schenk Graf von Stauffenberg">
Claus Schenk Graf von Stauffenberg

Claus Philipp Maria Schenk Graf von Stauffenberg (* 15. November 1907 in Jettingen, Königreich Bayern; † 20. oder 21. Juli 1944 in Berlin) war ein Offizier der deutschen Wehrmacht und während des Zweiten Weltkriegs eine der zentralen Persönlichkeiten des militärischen Widerstandes gegen den Nationalsozialismus im Deutschen Reich.

Oberst von Stauffenberg war Hauptakteur bei dem misslungenen Attentat vom 20. Juli 1944 auf Adolf Hitler und als Stabschef beim Befehlshaber des Ersatzheeres entscheidend an der anschließenden „Operation Walküre“ beteiligt, dem Versuch eines Staatsstreiches. Nach dessen Scheitern wurde er auf Befehl von Generaloberst Friedrich Fromm am 20. oder 21. Juli 1944 kurz vor oder nach Mitternacht im Hof des Berliner Bendlerblocks standrechtlich erschossen.

Er war „ein glühender Patriot, ein leidenschaftlicher deutscher Nationalist“ und sympathisierte zunächst mit den nationalistischen und revisionistischen Aspekten des Nationalsozialismus, bevor er den verbrecherischen Charakter des nationalsozialistischen Regimes erkannte und auch wegen der Aussichtslosigkeit der militärischen Gesamtlage des Deutschen Reiches zum aktiven Widerstand fand.

Claus von Stauffenberg wurde auf Schloss Jettingen im bayerischen Schwaben bei Burgau zwischen Augsburg und Ulm als dritter Sohn in die süddeutsche katholische Adelsfamilie Stauffenberg geboren. Seine Eltern waren Alfred Schenk Graf von Stauffenberg (1860–1936), Oberhofmarschall von Wilhelm II., dem letzten König von Württemberg, und Caroline, geb. Gräfin von Üxküll-Gyllenband (1875–1957). Über seine Mutter hatte er auch preußische Vorfahren wie den preußischen Heeresreformer Graf von Gneisenau. Prägend für seine Beteiligung am Widerstand war unter anderem sein Onkel Nikolaus Graf von Üxküll-Gyllenband.

Seine Kindheit verbrachte er vor allem in der Landeshauptstadt Stuttgart und im Stauffenberg-Schloss (heute Stauffenberg-Gedenkstätte), dem Sommersitz der Familie im heutigen Albstadter Stadtteil Lautlingen, zusammen mit den zwei Jahre älteren Zwillingsbrüdern Berthold und Alexander. Auch Claus hatte einen Zwillingsbruder, Konrad Maria, der aber am Tag nach der Geburt verstarb.

Während des Besuchs des Eberhard-Ludwigs-Gymnasiums in Stuttgart schloss sich von Stauffenberg den „Neupfadfindern“, einer Gruppierung der Bündischen Jugend an, wo man romantische Reichsvorstellungen (Reichsmystizismus) pflegte und mit der Verehrung Stefan Georges verband. Ihm sandte der 15-jährige Claus seine Gedichtversuche. Ein Jahr später wurde er mit seinen älteren Zwillingsbrüdern Berthold und Alexander in den elitären Dichterkreis um Stefan George aufgenommen. Von Stauffenbergs älterem Bruder Berthold widmete George zwei Gedichte in seinem letzten Lyrikband "Das Neue Reich" (1928) mit dem bereits 1922 entstandenen Poem "Geheimes Deutschland". Der musisch vielseitige von Stauffenberg galt im George-Kreis als Tat-Charakter, und er entschied sich früh für eine militärische Karriere.

Von Stauffenberg trat nach dem am 5. März 1926 bestandenen Abitur in die Reichswehr ein. Seinen Dienst begann er im traditionsreichen 17. Bayerischen Reiter-Regiment in Bamberg, in das er als Fahnenjunker aufgenommen wurde. Hier hatte er ein Jahr zu dienen, ehe er 1927 zur Infanterieschule in die Dresdener Albertstadt kommandiert wurde. Alle Offizieranwärter mussten hier ein Jahr der Ausbildung verbringen. Anfang August 1928 erhielt er dort seine Beförderung zum Fähnrich. Ende des Jahres 1928 wurde er an die Kavallerieschule in Hannover versetzt. Danach ging er zu seinem Regiment nach Bamberg zurück, wo er am 1. Januar 1930 zum Leutnant (mit Ehrensäbel) befördert wurde. Die Offiziersprüfung schloss er als Jahrgangsbester ab.

Gegen Ende der Weimarer Republik stand von Stauffenberg ebenso wie sein Bruder Berthold politisch den Kreisen um die Konservative Revolution nahe. Wie diese hatten sie für den aufkommenden Nationalsozialismus hauptsächlich Verachtung übrig, dennoch gab es im politischen Denken zahlreiche Berührungspunkte:
Bei der Reichspräsidentenwahl im April 1932 sprach sich von Stauffenberg daher gegen den konservativ-monarchistischen Amtsinhaber Paul von Hindenburg und für Adolf Hitler aus, dessen Ernennung zum Reichskanzler am 30. Januar 1933 er ausdrücklich begrüßte. Von Stauffenberg war an der militärischen Ausbildung der Mitglieder der Sturmabteilung (SA) beteiligt und organisierte die Übergabe von Waffendepots an die Reichswehr. Am 1. Mai 1933 folgte die Beförderung zum Oberleutnant.

Am 26. September 1933 heiratete er in Bamberg Nina Freiin von Lerchenfeld. Mit ihr hatte er fünf Kinder: Berthold, Heimeran, Franz-Ludwig, Valerie und Konstanze. Zuletzt lebte Nina in der Nähe von Bamberg und engagierte sich sehr für das alte Bamberg. Sie verstarb am 2. April 2006 im Alter von 92 Jahren in Kirchlauter bei Bamberg.

1934 wurde von Stauffenberg als "Bereiter-Offizier" an die Kavallerieschule Hannover versetzt. In Hannover qualifizierte er sich durch seine Studien über moderne Waffen wie Panzer und Fallschirmjäger. Später wandte er sich allerdings der Rolle des Pferdes in der militärischen Verwendung zu. In einer im Jahre 1938 verfassten Studie "(Heereskavallerie. Eine Studie)" hob er die Wichtigkeit einer operativen Beweglichkeit der Kampfführung hervor. Hierbei sei die Verwendung einer "Pferde"-Kavallerie neben der Verwendung von Panzerverbänden ausschlaggebend:
Am 1. Oktober 1936 wurde von Stauffenberg zur Generalstabsausbildung an die Kriegsakademie in Berlin-Moabit kommandiert. Am 1. Januar 1937 wurde er zum Rittmeister befördert. Im Juli 1938 wurde er als Zweiter Generalstabsoffizier (Ib) zum Divisionsstab der 1. leichten Division nach Wuppertal unter Generalleutnant Erich Hoepner kommandiert, mit der er im selben Jahr an der Besetzung des Sudetenlandes teilnahm.

Mit Beginn des Zweiten Weltkrieges, den der Berufssoldat von Stauffenberg als „Erlösung“ empfand, wurde er in der 1. leichten Division (später 6. Panzer-Division) im Polenfeldzug 1939 eingesetzt. Von hier schrieb er an seine Frau Nina:

Der Historiker Heinrich August Winkler führt das Briefzitat als Beleg dafür an, dass von Stauffenberg zu dieser Zeit die Rassenpolitik der Nationalsozialisten grundsätzlich bejahte, wenn er sie auch für überspitzt hielt. Auch der israelische Historiker Saul Friedländer nimmt an, dass sich von Stauffenbergs Haltung gegenüber dem Judentum nur graduell, aber nicht prinzipiell vom Antisemitismus der Nationalsozialisten unterschieden habe. Der Stauffenberg-Biograf Peter Hoffmann lehnt den Begriff „Antisemit“ für von Stauffenberg dagegen ab und will den Feldpostbrief im Zusammenhang interpretiert haben.

Peter Graf Yorck von Wartenburg, ein weitläufig Verwandter, und Ulrich Wilhelm Graf Schwerin von Schwanenfeld baten von Stauffenberg, sich zum Adjutanten Walther von Brauchitschs, des Oberbefehlshabers des Heeres, ernennen zu lassen, um an einem Umsturzversuch teilnehmen zu können. Von Stauffenberg lehnte ab. Im Januar 1940 wurde von Stauffenberg zum Hauptmann i. G. ernannt und nahm als Generalstabsoffizier der 6. Panzerdivision an der Westoffensive gegen Frankreich teil. Hierbei wurde er schließlich am 31. Mai 1940 mit dem Eisernen Kreuz 1. Klasse ausgezeichnet. Danach wurde er in die Organisationsabteilung des Oberkommandos des Heeres versetzt. Im Dezember 1941 hieß von Stauffenberg die Vereinheitlichung der Befehlsgewalt des Oberbefehlshabers des Heeres und des Obersten Befehlshabers der Wehrmacht in Hitlers Händen gut. Seine Beförderung zum Major i. G. erging im April 1941.

Als Gruppenleiter der Gruppe II der Organisationsabteilung im Oberkommando des Heeres gehörte er zu den maßgebenden Offizieren, die bewusst auf einen Wandel der Politik in den besetzten Gebieten hinarbeiteten. Besonders im Zusammenhang mit der Kampfführung der in den Kaukasus vordringenden Heeresgruppe A hatte er sich den Fragen der Freiwilligen in den sogenannten Ostlegionen zugewandt. Es ging um die Gewinnung von entlassenen Kriegsgefangenen und Überläufern für den Kampf auf deutscher Seite. Hierzu gab seine Abteilung am 2. Juni 1942 Richtlinien für die Behandlung turkestanischer und kaukasischer Soldaten heraus und steuerte im August 1942 die Organisation wie auch den Einsatz der Ostlegionen.
Bis Mitte November 1942 war die 10. Panzer-Division noch an der Besetzung der bis dahin unbesetzten Zone Frankreichs beteiligt. Unmittelbar danach wurde die Division nach Tunis verlegt. Von Stauffenberg war zwischenzeitlich im Generalstab des Heeres verwendet und war am 1. Januar 1943 zum Oberstleutnant i. G. befördert worden. Im März 1943 wurde er als Ia (Erster Generalstabsoffizier der Führungsgruppe) zur 10. Panzer-Division versetzt, die den Rückzug von Generalfeldmarschall Erwin Rommels Armee gegen die in Nordafrika gelandeten Alliierten decken sollte. Bei einem Tieffliegerangriff der Engländer in Tunesien am 7. April 1943 wurde er schwer verwundet. Im Feldlazarett 200 bei Sfax wurden sein linkes Auge, die zerschossene rechte Hand sowie Ring- und Kleinfinger der linken Hand amputiert. Er wurde zunächst ins Kriegslazarett 950 bei Carthago überführt und gelangte von dort ins Reservelazarett München 1. Darüber hinaus verbrachte er mehrere Genesungsurlaube in Lautlingen. Er war Patient des berühmten Chirurgen Ferdinand Sauerbruch. Für seine Verwundung wurde ihm am 14. April 1943 das Goldene Verwundetenabzeichen verliehen. Dieses wurde ihm von General Kurt Zeitzler, dem Chef des Generalstabes des Heeres, persönlich überreicht (laut Zeitzler hätte er eine derartige Verleihung auch bei jedem anderen schwer verwundeten Generalstabsoffizier vorgenommen). Am 8. Mai 1943 wurde von Stauffenberg mit dem Deutschen Kreuz in Gold ausgezeichnet.
Mitte Juni 1944 wurde von Stauffenberg Chef des Stabes bei Generaloberst Friedrich Fromm; am 1. Juli 1944 wurde er zum Oberst i. G. befördert.

Während der Stabsoffizier Henning von Tresckow sich bereits im Herbst 1941 der Berliner Widerstandsgruppe um Ludwig Beck, Carl Friedrich Goerdeler und Hans Oster angeschlossen hatte, fühlte sich von Stauffenberg wie viele andere Militärs zunächst weiter durch seinen Treueid an Hitler gebunden. Erst im Herbst 1943 ließ er sich nach Berlin versetzen und suchte dort bewusst Kontakt zu den Hitlergegnern um General der Infanterie Friedrich Olbricht, dem Leiter des Allgemeinen Heeresamtes, und von Tresckow. Er war sich bewusst, dass nur die Wehrmacht als einzige von der Geheimen Staatspolizei (Gestapo) und vom Sicherheitsdienst (SD) kaum infiltrierte Organisation über die nötigen Machtmittel zum Umsturz verfügte. Gemeinsam mit seinem Bruder Berthold und mit den Mitgliedern des Kreisauer Kreises war er an den Entwürfen zu Regierungserklärungen für die Zeit nach dem Umsturz beteiligt. Die Verschwörer legten ihre Ziele auf die Beendigung des Krieges und der Judenverfolgung und auf die Wiederherstellung des Rechtsstaates fest, wie er bis 1933 bestanden hatte. Auf eine angestrebte Staatsform konnten sie sich nicht einigen. Ein Großteil der aus den konservativen Kreisen von Bürgertum, Adel und Militär stammenden Verschwörer lehnte die parlamentarische Demokratie ab, so auch von Stauffenberg. Andererseits forderte er die Aufnahme von Sozialdemokraten wie Julius Leber in die neu zu bildende Regierung. Durch Vermittlung seines Cousins Peter Graf Yorck von Wartenburg lernte er Leber kennen, und es entstand ein enges Vertrauensverhältnis. Nach der Verhaftung Lebers Anfang Juli 1944 brach er gegenüber Adam von Trott zu Solz immer wieder in die Worte aus: „Ich hole ihn heraus“; für Lebers Rettung schien kein Preis zu hoch zu sein. Schließlich vertrat er die Ansicht, das Wichtigste sei die Beseitigung des NS-Regimes, alles andere werde sich dann finden.

Laut dem Mitverschwörer Hans Bernd Gisevius erstrebte der engere Kreis um von Stauffenberg ab 1944 ein Bündnis mit den Kommunisten. Von Stauffenbergs Vertrauter Julius Leber war aufgrund eines Treffens mit der Saefkow-Jacob-Bästlein-Organisation von der Gestapo festgenommen worden. Innerlich stand er Fritz-Dietlof Graf von der Schulenburg sehr nahe. Im Juli 1944 traf der engere Verschwörerkreis in Berlin-Wannsee im Haus Bertholds zusammen. Sie legten einen von Rudolf Fahrner und Berthold entworfenen Eid ab, indem sie sich auf ein gemeinsames Handeln nach dem Staatsstreich, selbst im Falle der Besetzung Deutschlands, verpflichteten.

Wegen dieser elitären, als „antidemokratisch“ und „nationalistisch“ interpretierten Haltung, die bis in die Formulierungen hinein dem Denken des George-Kreises verpflichtet war, glaubt der britische Historiker Richard J. Evans, dass von Stauffenberg an zukunftsweisendem politischen Gedankengut „nichts zu bieten“ hatte. „Als Vorbild für künftige Generationen“ sei er „schlecht geeignet“.

Spätestens mit der Invasion der Alliierten in der Normandie Anfang Juni 1944 war deutlich geworden, dass eine militärische Niederlage und damit ein „Zusammenbruch“ des Deutschen Reichs wohl nicht mehr abwendbar war. Von Stauffenberg fühlte sich aus ähnlichen Gründen wie von Tresckow dennoch verpflichtet, die Vorbereitungen zum Staatsstreich durch eine gewaltsame Beseitigung der nationalsozialistischen Führung voranzutreiben:
Gemeinsam mit General Friedrich Olbricht, Oberst Albrecht Ritter Mertz von Quirnheim und Henning von Tresckow arbeitete von Stauffenberg den Operationsplan "Walküre" aus. Offiziell diente der Plan der Niederwerfung möglicher innerer Unruhen, etwa bei einem Aufstand der zahlreichen Fremdarbeiter. Von Stauffenberg und Tresckow fügten dem Plan einige weitere Befehle hinzu und machten so aus "Walküre" einen Operationsplan für den Staatsstreich. Er sah vor, die Ermordung Hitlers zunächst einer Gruppe „frontfremder Parteifunktionäre“ anzulasten, um damit einen Grund für die Verhaftung der Angehörigen von NSDAP, SS, Sicherheitsdienst und Gestapo zu haben. Die Befehlshaber der Wehrkreiskommandos im gesamten Großdeutschen Reich sollten sofort nach der Auslösung von "Walküre" entsprechende Befehle erhalten. Das Militär sollte die ausführende Gewalt übernehmen. Für von Stauffenberg sahen die Umsturzpläne den Rang eines Staatssekretärs im Reichskriegsministerium vor.

Von Stauffenberg wurde zum Stabschef des Allgemeinen Heeresamtes im Berliner Bendlerblock ernannt, wodurch er Zugang zu den Lagebesprechungen in den Führerhauptquartieren erhielt. Er unterstand Olbricht und baute mit dessen Förderung ein militärisch-oppositionelles Netz auf. Er koordinierte die Attentatspläne mit Carl Friedrich Goerdeler und Generaloberst Ludwig Beck und hielt Verbindung zum zivilen Widerstand um Julius Leber, Wilhelm Leuschner sowie zu den Mitgliedern des Kreisauer Kreises, zu dem auch sein Cousin Peter Graf Yorck von Wartenburg gehörte. Nach der Verhaftung Helmuth James Graf von Moltkes im Januar 1944 fanden keine Treffen des Kreisauer Kreises mehr statt. Die Mehrheit der Mitglieder stellte sich von Stauffenberg – trotz Moltkes Vorbehalten gegen eine Tötung Hitlers – zur Verfügung.
Am 1. Juli 1944 wurde er Chef des Stabes beim Befehlshaber des Ersatzheeres (BdE) Generaloberst Fromm. Damit saß er nun gemeinsam mit Olbricht und Mertz von Quirnheim in der Schaltzentrale für die geplante Operation "Walküre". Ein heikler Punkt des Plans war, dass von Stauffenberg sowohl das Attentat ausführen, als auch von Berlin aus den Staatsstreichversuch leiten musste. Bereits am 11. Juli auf dem Berghof und am 15. Juli im Führerhauptquartier Wolfsschanze versuchte von Stauffenberg, Adolf Hitler zu töten. Beide Versuche brach er vorzeitig ab, weil entweder Heinrich Himmler und/oder Hermann Göring nicht anwesend waren. Ein drittes Mal sollte der Anschlag unter keinen Umständen verschoben werden.

Die nächste Gelegenheit ergab sich rein zufällig am 18. Juli, als von Stauffenberg für den übernächsten Tag ins Führerhauptquartier bestellt wurde, um dort über geplante Neuaufstellungen von Truppen zu berichten. Die Widerstandsgruppe hatte bereits die Mitglieder einer Nachfolgeregierung bestimmt. Es musste nur noch Hitler „beseitigt“ werden. Von Stauffenberg flog am 20. Juli um 7:00 Uhr mit seinem Adjutanten, Oberleutnant Werner von Haeften, vom Flugplatz Rangsdorf bei Berlin zur Wolfsschanze bei Rastenburg in Ostpreußen.

Da die Besprechung wegen eines geplanten Besuchs von Benito Mussolini unerwartet um eine halbe Stunde vorverlegt wurde, gelang es ihm nur noch, mit einer speziell für ihn angepassten Zange (er besaß nur noch drei Finger an seiner linken Hand), eines der beiden Sprengstoffpäckchen mit einem aktivierten britischen Bleistiftzünder (chemisch-mechanischen Zeitzünder) zu versehen. Das zweite Sprengstoffpäckchen, das die Sprengwirkung zweifellos erhöht hätte, steckte er nicht mit in seine Aktentasche. Dazu kam, dass die Besprechung nicht wie üblich in einem Betonbunker, sondern in einer leichten Holzbaracke stattfand und die Sprengladung so nicht die erhoffte Wirkung entfalten konnte. Von Stauffenberg stellte sie etwa zwei Meter entfernt von Hitler neben einem massiven Tischblock (der wohl die Wirkung weiter abschwächte) ab und verließ die Baracke unter dem Vorwand, telefonieren zu müssen. Die Sprengladung detonierte um 12:42 Uhr in der mit 24 Personen gefüllten Holzbaracke. Hitler und weitere 19 Anwesende überlebten die Detonation.
Von Stauffenberg und Haeften konnten in der allgemeinen Verwirrung nach dem Anschlag die Wolfsschanze rechtzeitig verlassen, warfen die verbleibende Sprengladung auf der Fahrt zum Flugplatz Rastenburg aus dem offenen Wagen und flogen nach Berlin zurück, im festen Glauben, Hitler sei tot. Bereits wenige Minuten nach der Explosion gelangte aber die Nachricht, dass Hitler überlebt hatte, nach Berlin: Propagandaminister Joseph Goebbels erhielt bereits gegen 13 Uhr in Berlin telefonisch Kenntnis vom misslungenen Attentat. Kurz darauf bestätigte der Mitverschwörer Oberst Hahn dem General Thiele im Bendlerblock in einem weiteren Telefonat aus der Wolfsschanze ausdrücklich, dass Hitler das Attentat überlebt habe. Thiele benachrichtigte die Generäle Friedrich Olbricht und Hoepner von den Ferngesprächen, sie einigten sich darauf, Walküre zunächst noch nicht auszulösen. Noch während von Stauffenberg auf dem Rückflug nach Berlin war, bekam Heinrich Müller, Chef der Geheimen Staatspolizei (Gestapo), den Auftrag, von Stauffenberg zu verhaften. Gegen 15:45 Uhr landete von Stauffenberg in Berlin, beteuerte in einem Telefonat mit Olbricht wahrheitswidrig, dass er mit eigenen Augen gesehen habe, dass Hitler tot sei, und begab sich zu Olbricht in den Bendlerblock. Erst gegen 16:30 Uhr, fast vier Stunden nach dem Attentat, wurde "Walküre" ausgelöst. Es zeigten sich jetzt aber schwere Mängel in Vorbereitung und Durchführung des Umsturzversuchs. So zog sich das Aussenden der Fernschreiben aus dem Bendlerblock in die Wehrkreise über Stunden hin und kreuzte sich bereits ab etwa 16 Uhr mit Fernschreiben aus der Wolfsschanze, dass Befehle aus dem Bendlerblock ungültig seien. Die meisten Offiziere außerhalb des Bendlerblocks verhielten sich wegen dieser widersprüchlichen Lage abwartend. Die Fernschreiben der Verschwörer mit den "Walküre"-Befehlen wurden weitgehend nicht befolgt.

Zwar hielten sich Georg und Philipp Freiherr von Boeselager bereit, um mit ihren Regimentern auf das „führerlose“ Berlin zu marschieren, und von Stauffenberg, Olbricht, Mertz von Quirnheim und Haeften ließen Generaloberst Fromm verhaften, der sie bis dahin gedeckt hatte, aber angesichts der unsicheren Nachrichtenlage von einer Beteiligung an dem Umsturzversuch nichts mehr wissen wollte. Der Einmarsch der Truppen unterblieb aber, und am späten Abend meldete sich Hitler selbst in einer Rundfunkansprache zu Wort.

Gegen 22:30 Uhr verhaftete eine Gruppe regimetreuer Offiziere, unter ihnen Otto Ernst Remer, von Stauffenberg und die Mitverschwörer. Generaloberst Fromm gab unter Berufung auf ein Standgericht, das angeblich stattgefunden habe, noch am Abend des 20. Juli den Befehl, Claus Schenk Graf von Stauffenberg gemeinsam mit Werner von Haeften, Albrecht Ritter Mertz von Quirnheim und Friedrich Olbricht zu erschießen; „[d]ie Erschießungen können kurz vor oder kurz nach Mitternacht erfolgt sein“. Die Exekution fand im Hof des Bendlerblocks statt, von Stauffenbergs letzte Worte sollen der Ausruf „Es lebe das heilige Deutschland!“ gewesen sein, nach anderen Quellen rief er in Anspielung auf die Ideenwelt Stefan Georges „Es lebe das ‚Geheime Deutschland‘!“. Am folgenden Tag wurden die Leichen der Erschossenen mit ihren Uniformen und Ehrenzeichen auf dem Alten St.-Matthäus-Kirchhof Berlin bestattet. Himmler ließ sie ausgraben und ordnete ihre Verbrennung an. Ihre Asche wurde über die Rieselfelder von Berlin verstreut.

Himmler plante, die Familien der Verschwörer zu ermorden und die Familiennamen auszulöschen. Die zunächst ins Auge gefasste Blutrache wurde wieder verworfen und stattdessen eine umfangreiche Sippenhaft befohlen. Von Stauffenbergs schwangere Ehefrau Nina Schenk Gräfin von Stauffenberg wurde in das Konzentrationslager Ravensbrück deportiert. Aufgrund der anstehenden Geburt wurde sie in ein NS-Frauenentbindungsheim in Frankfurt (Oder) verlegt, wo das fünfte Kind der Familie, Konstanze, am 27. Januar 1945 zur Welt kam. Die Kinder wurden in ein Kinderheim bei Bad Sachsa verbracht. Es gab Pläne, sie nationalsozialistischen Familien zur Adoption zu übergeben. Sie erhielten andere Nachnamen (die Stauffenberg-Kinder hießen ab sofort „Meister“) und verblieben dort bis zum Kriegsende.

Im Zusammenhang mit dem Attentat kam es zu zahlreichen postumen Ehrungen: Gedenktafeln befinden sich unter anderem in der Gedenkstätte Deutscher Widerstand im Bendlerblock in Berlin (seit 1960), in der Lönsstraße in Wuppertal (seit 1984) und im Bamberger Dom. In mehreren deutschen Städten gibt es nach Graf von Stauffenberg benannte Straßen oder Plätze. Am 20. Juli 1955 wurde die bisherige Bendlerstraße am Bendlerblock in Stauffenbergstraße umbenannt.

Die Kaserne der Bundeswehr in Sigmaringen trägt seit dem 20. Juli 1961 den Namen Graf-Stauffenberg-Kaserne. 1964 wurde auf ihrem Gelände ein Gedenkstein zur Erinnerung an von Stauffenberg enthüllt. Um trotz Schließung der Kaserne in Sigmaringen den Namen zu erhalten, wurde die Albertstadt-Kaserne in Dresden 2013 in Graf-Stauffenberg-Kaserne umbenannt.

Die Deutsche Bundespost widmete 1964 von Stauffenberg zum 20. Jahrestag des Attentats eine von E. und Gerd Aretz gestaltete Briefmarke aus einem Block. Die Briefmarke zum 100. Geburtstag von von Stauffenberg und Helmuth James Graf von Moltke aus dem Jahre 2007 wurde von Irmgard Hesse entworfen.

Seit 1967 trägt die 1965 als "4. Jungengymnasium" in Osnabrück gegründete Schule den Namen Graf-Stauffenberg-Gymnasium.

Seit dem 9. Februar 1979 trägt die Städtische Realschule in Bamberg den Namen "Graf-Stauffenberg-Realschule." Auch die städtische Wirtschaftsschule hat den Widerstandskämpfer seit 1979 als Namenspatron. Ein früher in demselben Gebäude untergebrachtes, aber mittlerweile in ein anderes Gymnasium integriertes Wirtschaftsgymnasium trug ebenfalls den Namen "Graf-Stauffenberg-Gymnasium". In Flörsheim am Main existiert ebenfalls ein "Graf-Stauffenberg-Gymnasium".

Am 3. April 2000 wurde eine Büste von Stauffenbergs in der Bayerischen Ruhmeshalle enthüllt. Im Stuttgarter „Alten Schloss“ wurde 2006 eine Erinnerungsstätte des Landes Baden-Württemberg eröffnet.

Zum 100. Geburtstag von Stauffenbergs, der unter anderem mit einem Großen Zapfenstreich einer Bundeswehrdivision begangen wurde, wurde am 15. November 2007 im Stauffenberg-Schloss in Lautlingen eine neue Gedenkstätte eröffnet; sie wurde gefördert von der Landesstiftung Baden-Württemberg und Sponsoren aus der Wirtschaft.

Die Stadt Dresden benannte im Stadtteil Albertstadt eine Straße „Stauffenbergallee“. Die an diese Straße grenzende Offizierschule des Heeres, an der er selbst ausgebildet worden war, benannte den großen Traditionslehrsaal „Stauffenbergsaal“. Von Stauffenberg zu Ehren tragen die Offizierlehrgänge des 71. OAJ (Offizieranwärterjahrgang) des Deutschen Heeres seinen Namen.

Alljährlich finden am 20. Juli Feierstunden der Bundesregierung und öffentliche Gelöbnisse der Bundeswehr in Erinnerung an das gescheiterte Attentat auf Hitler statt. Seit 2008 wird das Feierliche Gelöbnis im Wechsel am Berliner Dienstsitz des BMVg, Bendlerblock und vor dem Reichstagsgebäude abgehalten.

Von Stauffenberg wurde im Film unter anderem von folgenden Schauspielern dargestellt:







</doc>
<doc id="929" url="https://de.wikipedia.org/wiki?curid=929" title="C++">
C++

C++ ist eine von der ISO genormte Programmiersprache. Sie wurde ab 1979 von Bjarne Stroustrup bei AT&T als Erweiterung der Programmiersprache C entwickelt. C++ ermöglicht sowohl die effiziente und maschinennahe Programmierung als auch eine Programmierung auf hohem Abstraktionsniveau. Der Standard definiert auch eine Standardbibliothek, zu der verschiedene Implementierungen existieren.

C++ wird sowohl in der Systemprogrammierung als auch in der Anwendungsprogrammierung eingesetzt und gehört in beiden Bereichen zu den verbreitetsten Programmiersprachen.

Typische Anwendungsfelder in der Systemprogrammierung sind Betriebssysteme, eingebettete Systeme, virtuelle Maschinen, Treiber und Signalprozessoren. C++ nimmt hier oft den Platz ein, der früher ausschließlich Assemblersprachen und der Programmiersprache C vorbehalten war.

Bei der Anwendungsprogrammierung kommt C++ vor allem dort zum Einsatz, wo hohe Forderungen an die Effizienz gestellt werden, um durch technische Rahmenbedingungen vorgegebene Leistungsgrenzen möglichst gut auszunutzen. Ab dem Jahr 2000 wurde C++ aus der Domäne der Anwendungsprogrammierung von den Sprachen Java und C# zurückgedrängt.

Die Sprache C++ verwendet nur etwa 60 Schlüsselwörter („Sprachkern“), manche werden in verschiedenen Kontexten (codice_1, codice_2) mehrfach verwendet. Ihre eigentliche Funktionalität erhält sie, ähnlich wie auch die Sprache C, durch die C++-Standardbibliothek, die der Sprache fehlende wichtige Funktionalitäten beibringt (Arrays, Vektoren, Listen, ...) wie auch die Verbindung zum Betriebssystem herstellt (iostream, fopen, exit, ...). Je nach Einsatzgebiet kommen weitere Bibliotheken und Frameworks dazu. C++ legt einen Schwerpunkt auf die Sprachmittel zur Entwicklung von Bibliotheken. Dadurch favorisiert es verallgemeinerte Mechanismen für typische Problemstellungen und besitzt kaum in die Sprache integrierte Einzellösungen.

Eine der Stärken von C++ ist die Kombinierbarkeit von effizienter, maschinennaher Programmierung mit mächtigen Sprachmitteln, die einfache bis komplexe Implementierungsdetails zusammenfassen und weitgehend hinter abstrakten Befehlsfolgen verbergen. Dabei kommt vor allem die Template-Metaprogrammierung zum Zuge, eine Technik, die eine nahezu kompromisslose Verbindung von Effizienz und Abstraktion erlaubt.

Einige Design-Entscheidungen werden allerdings auch häufig kritisiert:

C++ hat keine Garbage Collection, allerdings gibt es Bestrebungen Garbage-Collection durch Bibliotheken oder durch Aufnahme in den Sprachstandard zu ermöglichen. Siehe auch Boehm-Speicherbereinigung.

Es ist jedoch möglich, Speicher im Programm zu verwalten; zur Implementierung von Low-Level-Bibliotheken wie der C++-Standardbibliothek ist es notwendig. In High-Level-Code wird hiervon jedoch dringend abgeraten.

In C++ gehören private Eigenschaften (Variablen und Methoden) normalerweise mit zur Schnittstelle, die in der Header-Datei veröffentlicht ist. Dadurch entstehen zur Compilezeit und zur Laufzeit Abhängigkeiten der Objekte zu den Stellen, die sie verwenden.

Diese Abhängigkeiten können durch bestimmte Konstruktionen, wie dem "pimpl-Idiom" (""), vermieden werden. Dabei werden die privaten Felder der Klasse ("example_class") in eine private, vorwärts-deklarierte Hilfsklasse verschoben, und ein Zeiger auf ein Objekt dieser Hilfsklasse ("example_class::impl * impl_ptr") bleibt in der eigentlichen Klasse. Die Definition der implementierenden Klasse findet bei der Implementierung der öffentlichen Klasse statt und ist damit für den Verwender der Klasse (der nur die Header-Datei kennt) unsichtbar. Dadurch, dass die Hilfsklasse nur durch einen Zeiger referenziert wird, bleiben alle Quelltextänderungen an privaten Feldern transparent und die Binärkompatibilität wird erhalten.

In C++ sind die Speicherbereiche der einzelnen Objekte zur Laufzeit nicht vor (absichtlichen oder versehentlichen) gegenseitigen Änderungen geschützt.

Das Verhalten von vielen Sprachkonstrukten ist nicht definiert. Dies bedeutet, dass der Standard weder vorgibt noch empfiehlt, was in einem solchen Fall passiert. Die Auswirkungen reichen von Implementierungsabhängigkeit (d. h. je nach Zielrechner und Compiler kann sich das Konstrukt unterschiedlich verhalten) über unsinnige Ergebnisse oder Programmabstürze bis hin zu gefährlichen Sicherheitslücken. Einige dieser Freiheiten des Compilers lassen zusätzliche Optimierungen des Codes zu, einige haben allerdings keinen praktischen Nutzen und sind unnötigerweise undefiniert. Eine dritte Gruppe stellt Code dar, den man ohne Performanceverluste als Code mit definiertem Verhalten schreiben kann.

Durch undefiniertes Verhalten kommt es zu unterschiedlichem Verhalten bei
Quellcode mit Codepassagen mit undefiniertem Verhalten kann nach der Kompilierung unerwartetes und absurd erscheinenden Verhalten zeigen. So werden zu spät durchgeführte Überprüfungen wegoptimiert oder Schleifen, die auf einen ungültigen Index eines Arrays zugreifen, durch leere Endlosschleifen ersetzt.

Wichtig für das Verständnis von undefiniertem Verhalten ist insbesondere, dass niemals nur eine einzelne Operation ungültig ist, sondern das "gesamte Programm" ungültig wird und kein wohlgeformtes C++ mehr darstellt. Der Grund ist, dass manche Arten von „undefiniertem Verhalten“ Auswirkungen auf ganz andere, auch in sich korrekte, Programmteile haben und deren Verhalten beeinflussen können, beispielsweise bei Pufferüberläufen oder unbeabsichtigte Änderung von Prozessor-Flags, die durch eine ungültige arithmetische Operation verursacht wurde und die nachfolgende Berechnungen beeinflussen können.

Beispiele für "undefiniertes Verhalten":

Einerseits ist das hieraus resultierende nichtdeterministische Laufzeitverhalten, insbesondere bei kleinen Änderungen der Plattform, mindestens als Risiko, in der Praxis oft aber als klarer Nachteil einzustufen. Andererseits werden hierdurch schnellere Programme ermöglicht, da Gültigkeitsüberprüfungen weggelassen werden können und der Compiler zudem oft Programmteile stärker optimieren kann, indem er Randfälle als per Definition ausgeschlossen ignoriert.

Ein oft nicht wahrgenommener Vorteil ist darüber hinaus, dass dadurch, dass undefiniertes Verhalten praktisch nur in äußerst fragwürdigen Konstrukten auftritt, die aber nicht zwingend während des Kompilierens feststellbar sind, unsemantischer oder anderweitig suboptimaler Code gewissermaßen verboten wird.

Beispielsweise besteht eine illegale Art zu prüfen, ob die Summe zweier positiver Ganzzahlen formula_1 und formula_2 vom Typ ‚int‘ verlustfrei wieder in einem ‚int‘ abgebildet werden kann, daraus zu schauen, ob ihre Summe größer 0 ist (bei Überlauf entsteht auf den meisten Computern durch die Zweierkomplement-Arithmetik eine negative Zahl). Eine derartige Überprüfung ist allerdings aus mathematischer Sicht nicht besonders sinnvoll. Eine bessere (semantischere) Herangehensweise ist hier, die eigentliche Frage, ob formula_3, wobei formula_4 die größte in einem ‚int‘ darstellbare Zahl ist, nach der mathematisch validen Umformung zu formula_5 zu verwenden.

Um an die Verbreitung der Programmiersprache C anzuknüpfen, wurde C++ als Erweiterung von C gemäß dem damaligen Stand von 1990 (, auch kurz C90 genannt) entworfen.

Die Kompatibilität mit C zwingt C++ zur Fortführung einiger dadurch übernommener Nachteile. Dazu zählt die teilweise schwer verständliche C-Syntax, der als überholt geltende Präprozessor sowie verschiedene von der jeweiligen Plattform abhängige Details der Sprache, die die Portierung von C++-Programmen zwischen unterschiedlichen Rechnertypen, Betriebssystemen und Compilern erschweren.

Einige C-Sprachkonstrukte haben in C++ eine leicht abgewandelte Bedeutung oder Syntax, so dass manche C-Programme erst angepasst werden müssen, um sich als C++-Programm übersetzen zu lassen. Weitere Änderungen an C fanden in den Jahren 1999 (, aka C99) und 2011 (, aka C11) also nach der ersten Normung von C++ statt, so dass dort eingeflossene Änderungen nicht in C++98 berücksichtigt werden konnten. In die C++-Revision von 2011 wurde ein Teil der Neuerungen von C99 übernommen; auf der anderen Seite wurden dem C-Standard neue Features hinzugefügt, die auch mit C++11 nicht kompatibel sind.

C++ basiert auf der Programmiersprache C wie in ISO/IEC 9899:1990 beschrieben. Zusätzlich zu den in C vorhandenen Möglichkeiten bietet C++ weitere Datentypen sowie neuartige Typumwandlungsmöglichkeiten, Klassen mit Mehrfachvererbung und virtuellen Funktionen, Ausnahmebehandlung, Templates (Schablonen), Namensräumen, Inline-Funktionen, Überladen von Operatoren und Funktionsnamen, Referenzen, Operatoren zur Verwaltung des dynamischen Speichers und mit der C++-Standardbibliothek eine erweiterte Bibliothek.
Der folgende Quelltext ist ein einfaches C++-Programm, das den Text „"Hallo Welt!"“ in den Standardausgabestrom, üblicherweise das Terminal, schreibt:

int main()

Der Präprozessorbefehl oder auch Präprozessordirektive genannt codice_7 bindet Header-Dateien ein, die typischerweise Deklarationen von Variablen, Typen und Funktionen enthalten. Im Gegensatz zu C besitzen Header der C++-Standardbibliothek keine Dateiendung.

Der Header codice_8 ist Teil der C++-Standardbibliothek und deklariert unter anderem den Standardeingabestrom codice_9 und die Standardausgabeströme codice_10 und codice_11 für die aus der C-Standardbibliothek bekannten Objekte codice_12, codice_13 und codice_14.

Bei codice_15 handelt es sich um die Funktion, die den Einsprungspunkt jedes C++-Programms darstellt. Das Programm wird ausgeführt, indem die Funktion codice_15 aufgerufen wird, wobei diese ihrerseits andere Funktionen aufrufen kann. Die Funktion codice_15 selbst darf allerdings in einem C++-Programm nicht rekursiv aufgerufen werden.

Der Standard verlangt von Implementierungen, zwei Signaturen für die Funktion codice_15 zu unterstützen: Eine ohne Funktionsparameter wie im Beispiel, und eine, die einen Integer und einen Zeiger auf Zeiger auf codice_19 entgegennimmt, um auf Kommandozeilenparameter zugreifen zu können (was nicht in allen Programmen vonnöten ist): codice_20. Implementierungen dürfen darüber hinaus weitere Signaturen für codice_15 unterstützen, alle müssen jedoch den Rückgabetyp codice_4 (Integer) besitzen, also eine Ganzzahl zurückgeben. Gibt codice_15 jedoch wie im Beispiel keinen Wert zurück, so schreibt der C++-Standard der Implementierung vor, codice_24 anzunehmen. codice_15 gibt also 0 zurück, wenn kein anderslautendes codice_5-Statement in ihr vorhanden ist.

codice_10 ist eine Instanz der Klasse codice_28, die sich wie die gesamte C++-Standardbibliothek im Namensraum codice_29 befindet. Bezeichner in Namensräumen werden mit dem Bereichsoperator (codice_30) angesprochen.

Die Ausgabe des Zeichenkettenliterals codice_31 übernimmt der Operator codice_32. Zeichenkettenliterale sind in C++ vom Typ "Array aus N konstanten chars" (codice_33), wobei "N" gleich der Länge der Zeichenkette + 1 für die abschließende Nullterminierung ist. Da die Standardtypumwandlungen von C++ die als "pointer-to-array decay" bekannte implizite Umwandlung eines Arrays codice_34 in einen Pointer codice_35 vorsehen, und damit codice_33 in einen codice_37 zerfällt, passt der überladene Operator codice_38 aus codice_39 und wird entsprechend aufgerufen (codice_40) und gibt die Zeichenkette aus.

Die Implementierung eines C++-Compilers gilt als aufwendig. Nach der Fertigstellung der Sprachnorm 1998 dauerte es mehrere Jahre, bis die Sprache von C++-Compilern weitestgehend unterstützt wurde.

Zu den verbreitetsten C++-Compilern gehören:







C++ war nicht der einzige Ansatz, die Programmiersprache C um Eigenschaften zu erweitern, die das objektorientierte Programmieren vereinfachen. In den 1980er Jahren entstand die Programmiersprache Objective-C, die sich aber im Gegensatz zu C++ syntaktisch wie von ihrem Funktionsprinzip an Smalltalk und nicht an Simula orientierte. Die Syntax von Objective-C (C beeinflusst durch Smalltalk) unterscheidet sich erheblich von C++ (C beeinflusst von Simula mit ganz eigenen syntaktischen Erweiterungen). Ende der 1980er Jahre wurde Objective-C erstmals kommerziell in NeXTSTEP verwendet, in dem es ein zentrales Bestandteil darstellt. Heutzutage findet es in der Programmierschnittstelle OpenStep sowie in unter den Betriebssystemen Apple iOS und macOS ein wichtigstes Einsatzgebiet.

Die Programmiersprachen Java und C# verfügen über eine ähnliche, ebenfalls an C angelehnte Syntax wie C++, sind auch objektorientiert und unterstützen seit einiger Zeit Typparameter. Trotz äußerlicher Ähnlichkeiten unterscheiden sie sich aber konzeptionell von C++ zum Teil beträchtlich.

Generische Techniken ergänzen die objektorientierte Programmierung um Typparameter und erhöhen so die Wiederverwertbarkeit einmal kodierter Algorithmen. Die generischen Java-Erweiterungen sind jedoch lediglich auf Klassen, nicht aber auf primitive Typen oder Datenkonstanten anwendbar. Demgegenüber beziehen die generischen Spracherweiterungen von C# auch die primitiven Typen mit ein. Dabei handelt es sich allerdings um eine Erweiterung für Generik zur Laufzeit, die die auf Kompilationszeit zugeschnittenen C++-Templates zwar sinnvoll ergänzen, nicht aber ersetzen können.

Gerade die generische Programmierung macht C++ zu einem mächtigen Programmierwerkzeug. Während die objektorientierte Programmierung in Java und C# nach wie vor den zentralen Abstraktionsmechanismus darstellt, ist diese Art der Programmierung in C++ rückläufig. So werden tiefe Klassenhierarchien vermieden, und zu Gunsten der Effizienz und der Minimierung des Ressourcenverbrauchs verzichtet man in vielen Fällen auf Polymorphie, einen der fundamentalen Bestandteile der objektorientierten Programmierung.

Auf die Idee für eine neue Programmiersprache kam Stroustrup durch Erfahrungen mit der Programmiersprache Simula während seiner Doktorarbeit an der "Cambridge University". Simula erschien zwar geeignet für den Einsatz in großen Software-Projekten, die Struktur der Sprache erschwerte aber die Erstellung hocheffizienter Programme. Demgegenüber ließen sich effiziente Programme zwar mit der Sprache BCPL schreiben, für große Projekte war BCPL aber wiederum ungeeignet.

Mit den Erfahrungen aus seiner Doktorarbeit erweiterte Stroustrup in den AT&T Bell Laboratories im Rahmen von Untersuchungen des Unix-Betriebssystemkerns in Bezug auf verteiltes Rechnen ab 1979 die Programmiersprache C. Die Wahl fiel auf die Programmiersprache C, da C eine Mehrzwecksprache war, die schnellen Code produzierte und einfach auf andere Plattformen zu portieren war. Als dem Betriebssystem Unix beiliegende Sprache hatte C außerdem eine erhebliche Verbreitung.

Eine der ersten Erweiterungen war ein Klassenkonzept mit Datenkapselung, für das die Sprache "Simula-67" das primäre Vorbild war. Danach kamen abgeleitete Klassen hinzu, ein strengeres Typsystem, Inline-Funktionen und Standard-Argumente.

Während Stroustrup „"C with Classes"“ („"C mit Klassen"“) entwickelte (woraus später C++ wurde), schrieb er auch "cfront", einen Compiler, der aus "C with Classes" zunächst C-Code als Zwischenresultat erzeugte. Die erste kommerzielle Version von "cfront" erschien im Oktober 1985.

1983 wurde "C with Classes" in "C++" umbenannt. Erweiterungen darin waren: Überladen von Funktionsnamen und Operatoren, virtuelle Funktionen, Referenzen, Konstanten, eine änderbare Freispeicherverwaltung und eine verbesserte Typüberprüfung. Die Möglichkeit von Kommentaren, die an das Zeilenende gebunden sind, wurde aus BCPL übernommen (codice_41).

1985 erschien die erste Version von C++, die eine wichtige Referenzversion darstellte, da die Sprache damals noch nicht standardisiert war. 1989 erschien die Version 2.0 von C++. Neu darin waren Mehrfachvererbung, abstrakte Klassen, statische Elementfunktionen, konstante Elementfunktionen und die Erweiterung des Zugriffsmodells um codice_42. 1990 erschien das Buch "The Annotated C++ Reference Manual", das als Grundlage für den darauffolgenden Standardisierungsprozess diente.

Relativ spät wurden der Sprache "Templates", "Ausnahmebehandlung", "Namensräume", neuartige "Typumwandlungen" und "boolesche Typen" hinzugefügt.

Im Zuge der Weiterentwicklung der Sprache C++ entstand auch eine gegenüber C erweiterte Standardbibliothek. Erste Ergänzung war die "Stream-I/O-Bibliothek", die Ersatz für traditionelle C-Funktionen wie zum Beispiel codice_43 und codice_44 bietet. Eine der wesentlichen Erweiterungen der Standardbibliothek kam später durch die Integration großer Teile der bei Hewlett-Packard entwickelten Standard Template Library ("STL") hinzu.

Nach jahrelanger Arbeit wurde schließlich 1998 die endgültige Fassung der Sprache C++ (ISO/IEC 14882:1998) genormt. Diese Version wurde im Nachhinein, als weitere Versionen der Sprache erschienen, auch "C++98" genannt. Im Jahr 2003 wurde ISO/IEC 14882:2003 verabschiedet, eine Nachbesserung der Norm von 1998, in der einige Missverständnisse beseitigt wurden und einige Details klarer formuliert wurde. Diese Version wird umgangssprachlich auch "C++03" genannt.

Um mit den aktuellen Entwicklungen der sich schnell verändernden Computer-Technik Schritt zu halten, aber auch zur Ausbesserung bekannter Schwächen, erarbeitete das C++-Standardisierungskomitee die nächste größere Revision von C++, die inoffiziell mit C++0x abgekürzt wurde, worin die Ziffernfolge eine grobe Einschätzung des möglichen Erscheinungstermins andeuten sollte. Später, als ein Erscheinungstermin bis Ende 2009 nicht mehr zu halten war, änderte sich der inoffizielle Name zu "C++1x".

Die vorrangigen Ziele für die Weiterentwicklung von C++ waren Verbesserungen im Hinblick auf die Systemprogrammierung sowie zur Erstellung von Programmbibliotheken. Außerdem sollte die Erlernbarkeit der Sprache für Anfänger verbessert werden.

Im November 2006 wurde der Zieltermin für die Fertigstellung auf das Jahr 2009 festgelegt. Im Juli 2009 wurde dieser Termin auf frühestens 2010 geändert. Im August 2011 wurde die Revision einstimmig von der ISO angenommen und am 11. Oktober 2011 als ISO/IEC 14882:2011 offiziell veröffentlicht. Inoffiziell heißt die Version C++11.

C++98 deckte einige typische Problemfelder der Programmierung noch nicht ausreichend ab, zum Beispiel die Unterstützung von Nebenläufigkeit (Threads), deren Integration in C++, insbesondere für die Verwendung in Mehrprozessorumgebungen, eine Überarbeitung der Sprache unumgänglich machte. Durch die Einführung eines Speichermodells wurden Garantien der Sprache für den nebenläufigen Betrieb festgelegt, um Mehrdeutigkeiten in der Abarbeitungsreihenfolge sowohl aufzulösen als auch in bestimmten Fällen aufrechtzuerhalten und dadurch Spielraum für Optimierungen zu schaffen.

Zu den weitreichenderen Spracherweiterungen gehörte ferner die automatische Typableitung zur Ableitung von Ergebnistypen aus Ausdrücken und die sogenannten "R-Wert-Referenzen", mit deren Hilfe sich als Ergänzung zu dem bereits vorhandenen "Kopieren" von Objekten dann auch ein "Verschieben" realisieren lässt, außerdem bereichsbasierte For-Schleifen (foreach) über Container und eingebaute Felder.

Im April 2006 gab das C++-Standardisierungskomitee den sogenannten ersten Technischen Report (TR1) heraus, eine nicht normative Ergänzung zur aktuell gültigen, 1998 definierten Bibliothek, mit der Erweiterungsvorschläge vor einer möglichen Übernahme in die C++-Standardbibliothek auf ihre Praxistauglichkeit hin untersucht werden sollen. Viele Compiler-Hersteller lieferten den TR1 mit ihren Produkten aus.

Im TR1 waren u. a. reguläre Ausdrücke, verschiedene intelligente Zeiger, ungeordnete assoziative Container, eine Zufallszahlenbibliothek, Hilfsmittel für die C++-Metaprogrammierung, Tupel sowie numerische und mathematische Bibliotheken enthalten. Die meisten dieser Erweiterungen stammten aus der Boost-Bibliothek, woraus sie mit minimalen Änderungen übernommen wurden. Außerdem waren sämtliche Bibliothekserweiterungen der 1999 überarbeiteten Programmiersprache C (C99) in einer an C++ angepassten Form enthalten.

Mit Ausnahme der numerischen und mathematischen Bibliotheken wurden alle TR1-Erweiterungen in die Sprachnorm C++11 übernommen. Ebenfalls wurde eine eigene Bibliothek zur Unterstützung von Threads eingeführt.

Mit der Norm ISO/IEC 14882:2011, auch bekannt als "C++11", wurden viele weitreichende Neuerungen in C++ eingeführt, wie auszugsweise:
template <typename ObjectCreatorType>
auto createObject (const ObjectCreatorType& creator) -> decltype( creator.makeObject() )

void printNames(const std::vector<std::string>& names)


Themen der Sprache C++, die Rechenzeit und Speicherplatz betreffen, wurden im sogenannten "technical report" ISO/IEC TR 18015:2006 behandelt.

Zum Zeitpunkt der Einführung des Standards und auch noch vergleichsweise lange darüber hinaus unterstützten viele gängige Compiler diesen nicht vollständig bzw. mit Bezug auf einige Erweiterungen mitunter fehlerhaft. Besonders starke Einschränkungen zeigte diesbezüglich zum Beispiel Microsoft mit "Visual C++ 2012". Mit "Visual C++ 2015" sind mittlerweile jedoch nahezu alle wichtigen größeren Spracherweiterungen berücksichtigt worden.

Die aktuelle Fassung von C++ (Januar 2015) ist ISO/IEC 14882:2014, auch bekannt als "C++14". C++14 erweitert die Einsatzmöglichkeiten von codice_45 und codice_46, schwächt die Voraussetzungen für codice_57 ab, erlaubt Variablen-Templates zu definieren (beispielsweise um mehrere Versionen von π zu definieren), führt Binärliterale ein (0b...), führt Hochkommata als Trennzeichen in Zahlen ein, erlaubt generische Lambdas, erweitert Lambda capture expressions und führt das Attribut codice_58 ein.

Außerdem wurde die Standardbibliothek um ein paar Funktionen ergänzt, die bei C++11 „vergessen“ bzw. „übersehen“ wurden (z. B. codice_59) und etliche Funktionsdeklarationen nun als codice_57 umdeklariert, was dem Compiler aggressivere Optimierungen gestattet.

Während der Entwicklungsphase wurde C++14 auch "C++1y" genannt, um anzudeuten, dass es die Nachfolgeversion der vormals als C++0x genannten Version sein wird.

Im März 2017 hat das ISO-C++-Komitee den Sprachstandard C++17 technisch abgeschlossen. Für die neue Fassung wurde unter anderem die Aufnahme des Typen codice_61 beschlossen. Dieser ist explizit für den Byte-weisen Zugriff auf den Speicher bestimmt. Bis zur offiziellen Verabschiedung wurde die Fassung auch als "C++1z" bezeichnet.

Nach dem Sommer-Meeting Mitte Juli verriet der C++-Experte Herb Sutter, der für die Einberufung des Komitees verantwortlich ist, in seinem Blog bereits erste Pläne für C++20.

Eine beträchtliche Anzahl an Features, die wahrscheinlich mit C++20 kommen wird, steht schon fest.

Der Name C++ ist eine Wortschöpfung von Rick Mascitti, einem Mitarbeiter Stroustrups, und wurde zum ersten Mal im Dezember 1983 benutzt. Der Name kommt von der Verbindung der Vorgängersprache C und dem Inkrement-Operator „++“, der den Wert einer Variablen inkrementiert (um eins erhöht). Der Erfinder von C++, Bjarne Stroustrup, nannte C++ zunächst „C mit Klassen“ ("").

Oft geäußerte Kritik an der Sprache umfasst beispielsweise:







</doc>
<doc id="930" url="https://de.wikipedia.org/wiki?curid=930" title="Sprachelemente von C-Sharp">
Sprachelemente von C-Sharp

Dieser Artikel bietet eine Übersicht einiger Sprachelemente von C#.

Der sogenannte codice_1-Block hat zur Folge, dass die zwischen den geschweiften Klammern liegenden Anweisungen nur dann in Kraft treten, wenn die angegebene Bedingung erfüllt ist, d. h. ihr Wert codice_2 ist. Der codice_3-Block, der nur in Verbindung mit einem codice_1-Block stehen kann, wird nur dann ausgeführt, wenn die Anweisungen des if-Blocks "nicht" ausgeführt wurden.
if (Bedingung)
else if (Bedingung)
else

Die codice_5-Anweisung ist die Darstellung eines Falles, in dem, je nach Wert eines Ausdrucks, andere Anweisungen ausgeführt werden müssen. Es wird immer bei dem codice_6-Label fortgefahren, dessen Wert mit dem Ausdruck in der codice_5-Anweisung übereinstimmt. Wenn keiner der Fälle zutrifft, wird zum optionalen codice_8-Zweig gesprungen.
switch (Ausdruck)

Im Gegensatz zu C und C++ hat die C#-codice_5-Anweisung keine "fall-through"-Semantik.
Nicht-leere Zweige müssen mit einem codice_10, codice_11, codice_12, codice_13 oder codice_14 enden; leere Zweige können jedoch von weiteren case-Zweigen fortgesetzt werden, um mehrere Fälle mit einem Satz von Anweisungen zu behandeln:
switch (Ausdruck)

In C# sind auch Strings als Prüfausdruck erlaubt:
string cmd;
switch (cmd)

Wird eine codice_15-Schleife ausgeführt, wird zuerst der Startausdruck gültig. Ist dieser vollständig bearbeitet, werden die Anweisungen im Schleifenrumpf und anschließend der Inkrementierungsausdruck solange wiederholt abgearbeitet, bis die Gültigkeitsbedingung codice_16 ergibt, d. h., ungültig wird.
for (Startausdruck; Gültigkeitsbedingung; Inkrementierungsausdruck)

Die codice_17-Schleife ist dagegen recht primitiv: sie wiederholt die Anweisungen, solange die Bedingung codice_2 zurückgibt. Die Bedingung der codice_17-Schleife wird immer "vor" dem Anweisungsblock ausgewertet. Wenn die Bedingung von Anfang an nicht erfüllt ist, wird der Schleifenrumpf nicht durchlaufen.
while (Bedingung)

Die Bedingung der Do-While-Schleife wird immer nach dem Anweisungsblock ausgeführt. Die Schleife wird daher mindestens ein Mal durchlaufen.
do
} while (Bedingung);
Mit der codice_20-Schleife wird durch alle Mitglieder einer Sequenz iteriert. In der Schleife besteht nur lesender Zugriff auf die Schleifenvariable.
foreach (Typ Variablename in Sequenz)

for (int i = 0; i < 10; ++i)

Die Anweisung codice_11 veranlasst den nächsten Durchlauf einer Schleife. (Dabei wird der restliche Code im Schleifenkörper nicht abgearbeitet). Im Beispiel wird der Text nur zweimal ausgegeben.
for (int i = 0; i < 100; ++i)

Die Anweisung codice_10 veranlasst das Programm, die nächste umschließende Schleife (oder das umschließende "switch") zu verlassen. In diesem Beispiel werden nur die Zahlen 0, 1, 2, 3 und 4 ausgegeben.
int a = 1;
Top:
a++;
if (a <= 5)
Console.WriteLine("a sollte jetzt 6 sein.");
Mit codice_12 springt das Programm an das angegebene Sprungziel.

Die Benutzung von codice_12 sollte jedoch möglichst vermieden werden, da dadurch der Quellcode in der Regel unleserlicher wird. Es gilt jedoch als akzeptiertes Sprachmittel, um tief verschachtelte Schleifen zu verlassen, da in diesen Fällen der Code mit codice_12 lesbarer ist als durch mehrfache Verwendung von codice_10 oder anderen Sprachmitteln. Siehe auch Spaghetticode.

Innerhalb einer codice_5-Anweisung kann mittels codice_28 bzw. codice_29 zu einem der Fälle gesprungen werden.
int result = ImQuadrat(2);

static int ImQuadrat(int x)

Mit codice_14 wird die aktuelle Methode verlassen und der im Kopf der Methode vereinbarte Referenz- bzw. Wertetyp als Rückgabewert zurückgeliefert. Methoden ohne Rückgabewert werden mit dem Schlüsselwort codice_31 gekennzeichnet.

Eine Besonderheit bildet der Ausdruck codice_32. Zweck ist es, in verkürzter Schreibweise eine Rückgabesequenz für eine Methode oder eine Eigenschaft zu erzeugen. Der Compiler nutzt hierzu einen eigenen Typ, der von "System.Collections.Generic.IEnumerable<T>" abgeleitet ist und somit mit einem codice_20-Block durchlaufen werden kann.
Jeder Aufruf von codice_32 fügt bis zum Verlassen der Methode/Eigenschaft ein neues Element der Sequenz hinzu. Wird das Konstrukt nicht einmal aufgerufen, ist die Sequenz leer:
private int[] zahlen = new int[] { 5980, 23980 };

public IEnumerable<int> ZahlenMinusEins

public IEnumerable<double> ToDouble(IEnumerable<int> intZahlen)
Hierdurch muss keine temporäre Liste erzeugt werden, die die umgewandelten Zahlen erst zwischenspeichert und dann zurückgibt:
private int[] zahlen = new int[] { 5980, 23980 };

// bspw. aus einer Eigenschaft heraus
public IEnumerable<int> ZahlenMinusEins

Jedes Element, das zurückgegeben wird, muss implizit in den Typ der Rückgabesequenzelemente konvertierbar sein.

Zum Abbrechen des Vorgangs kann die Anweisung codice_35 verwendet werden:
// bspw. aus einer Methode heraus
public IEnumerable<double> ToDouble(IEnumerable<int> intZahlen)

Die Anweisungen codice_14 und codice_32 können nicht gemeinsam verwendet werden.

Bei der Verwendung muss beachtet werden, dass die zurückgegebene Sequenz die ihr zugrundeliegende Logik "verzögert" ausführt, was bedeutet, dass der erste Durchlauf ein und derselben Sequenz andere Werte liefern kann, als beim zweiten Mal:

// interner Zähler
private int i = 0;

public IEnumerable<DateTime> GetNow() {

public IEnumerable<int> GetZahl() {

public void TestZahl() {

public void TestZeit() {

Die codice_38-Anweisung definiert einen Geltungsbereich, an dessen Ende der Speicher
von nicht mehr benötigten Objekten automatisch freigegeben wird. Bei begrenzten Ressourcen wie Dateihandler stellt die codice_38-Anweisung sicher, dass diese immer ausnahmesicher bereinigt werden.
using (Font myFont = new Font("Arial", 10.0f))

Hier wird ein codice_40-Objekt erzeugt, das am Ende des Blocks automatisch durch Aufruf seiner codice_41-Methode wieder freigegeben wird. Dies geschieht selbst dann, wenn in dem Block eine Ausnahme ausgelöst wird. Der Vorteil liegt in der vereinfachten Schreibweise, denn intern wird daraus folgendes Konstrukt, das ansonsten manuell so formuliert werden müsste:
Font myFont = new Font("Arial", 10.0f);

try
finally
Klassen müssen die codice_42-Schnittstelle implementieren, damit die codice_38-Anweisung auf diese Weise eingesetzt werden kann.

Wenn man von einem Objekt spricht, handelt es sich dabei in der Umgangssprache normalerweise um ein reales Objekt oder einen Gegenstand des täglichen Lebens. Beispielsweise kann das ein Tier, ein Fahrzeug, ein Konto oder Ähnliches sein.

Jedes Objekt kann durch verschiedene Attribute beschrieben werden und verschiedene Zustände annehmen und diese auch auslösen.

Übertragen auf die objektorientierte Programmierung und C# ist ein Objekt ein Exemplar (siehe Schlüsselwort codice_44) einer Klasse. Eine Klasse kann man dabei als Bauplan oder Gerüst eines Objektes ansehen.

Eine Klasse besitzt Eigenschaften (Variablen), Methoden (die Tätigkeiten darstellen) und Ereignisse, die die Folge von Zuständen sind bzw. diese auslösen.
Beispiel für den Bauplan eines Autos:

class Auto

Die Klasse codice_45 (ein Alias für System.Object) ist ein Referenztyp, von dem jede Klasse abgeleitet wird. So kann durch implizite Typumwandlung jeder Objekttyp in codice_45 umgewandelt werden.

.NET (und damit auch C#) unterscheidet zwischen Werttypen und Referenztypen. Werttypen sind jedoch auch (über den Zwischenschritt ValueType) von codice_45 abgeleitet. Deshalb kann (mittels eines boxing/unboxing genannten Verfahrens) auch eine Variable vom Typ codice_45 auf einen Werttyp, z. B. codice_49 verweisen.

Strukturen sind bereits als Sprachmittel aus Sprachen wie C++ oder Delphi (Records) bekannt. Sie dienen primär zur Erstellung eigener komplexer Datenstrukturen oder eigener Datentypen. So kann zum Beispiel eine Raumkoordinate, bestehend aus einer X-, einer Y- und einer Z-Position, durch eine Datenstruktur Koordinate abgebildet werden, die sich aus drei Gleitkommazahlen einfacher oder doppelter Genauigkeit zusammensetzt.
public struct Koordinate

C# fasst eine über das Schlüsselwort codice_50 definierte Struktur als einen Wertetyp auf. Strukturen in C# können außerdem Methoden, Eigenschaften, Konstruktoren und andere Elemente von Klassen aufweisen; sie können aber nicht beerbt werden.

Der Unterschied einer Strukturinstanz im Vergleich zu einem Objekt besteht in ihrer Repräsentation im Speicher. Da keine zusätzlichen Speicherreferenzen benötigt werden, wie es bei einem Objekt erforderlich ist (Referenztyp), können Strukturinstanzen wesentlich ressourcenschonender eingesetzt werden. So basieren beispielsweise alle primitiven Datentypen in C# auf Strukturen.

Aufzählungen dienen zur automatischen Nummerierung der in der Aufzählung enthaltenen Elemente. Die Syntax für die Definition von Aufzählungen verwendet das Schlüsselwort codice_51 (Abkürzung für Enumeration).

Der in C# verwendete Aufzählungstyp ähnelt dem in C, mit der Ausnahme, dass ein optionaler ganzzahliger Datentyp für die Nummerierung der Elemente angegeben werden kann. Ohne Angabe eines Datentyps wird codice_49 verwendet.
public enum Wochentag

Die Elementnummerierung in C# beginnt bei 0. Es ist aber auch möglich, wie in C, jedem Element – oder nur dem ersten Element – einen eigenen Startwert zuzuweisen (wie im obigen Beispiel). Dabei können sich die Anfangswerte wiederholen. Die Zählung beginnt dann jeweils von neuem bei dem definierten Startwert und Element.

In C# ist es auch möglich, ein bestimmtes Element einer Enumeration über seine Ordinalzahl anzusprechen. Hierzu ist aber eine explizite Typumwandlung notwendig.
Wochentag Tag = Wochentag.Mittwoch;
System.Console.WriteLine(Tag); // Ausgabe: Mittwoch
System.Console.WriteLine((int)Tag); // Ausgabe: 3
System.Console.WriteLine((Wochentag)3); // Ausgabe: Mittwoch

Neben der beschriebenen Variante des enum-Aufzählungstyps existiert noch eine spezielle Variante von Enumeration. Flags definieren Enumerationen auf Bitebene und werden durch die Metainformation <nowiki>[Flags]</nowiki> vor der Enum-Deklaration definiert. Flag-Elemente können auf Bitebene verknüpft werden, sodass mehrere Elemente zu einem neuen kombiniert werden können. Hierzu müssen den zu kombinierenden Elementen Zweierpotenzen als Werte zugewiesen werden, damit eine Kombination ermöglicht wird.
[Flags]
public enum AccessMode

Zugriffsmodifikatoren regeln den Zugriff auf Klassen und deren Mitglieder (Methoden, Eigenschaften, Variablen, Felder und Ereignisse) in C#. Die folgende Tabelle führt die von C# unterstützten Zugriffsmodifikatoren auf und beschreibt deren Wirkung und den Sichtbarkeitskontext.
Hinweise:

C# kennt zwei Arten von Datentypen: Wertetypen und Referenztypen.
Referenztypen dürfen dabei nicht mit Zeigern gleichgesetzt werden, wie sie u. a. aus der Sprache C++ bekannt sind. Diese werden von C# auch unterstützt, aber nur im „unsicheren Modus“ (engl. "unsafe mode").

Wertetypen enthalten die Daten direkt, wobei Referenztypen im Gegensatz dazu nur Verweise auf die eigentlichen Daten, oder besser, Objekte darstellen. Beim Lesen und Schreiben von Wertetypen werden die Daten dagegen über einen Automatismus, Autoboxing genannt, in einer Instanz der jeweiligen Hüllenklasse (engl. "wrapper") gespeichert oder aus ihr geladen.

Die Zuweisung eines Wertes bzw. einer Referenz kann während der Deklaration erfolgen oder später, sofern nicht eine Konstante deklariert wurde. Die Deklaration erfolgt durch Angabe eines Datentyps gefolgt von einem Variablennamen:
// Datentyp Variable;
int i;
System.Collections.IList liste;
Es können auch mehrere Variablen des gleichen Typs zeitgleich deklariert werden:
// Datentyp Variable1, Variable2, ...;
int i, j, k;
System.Collections.IList liste1, liste2;
Ferner besteht die Möglichkeit, der Variablen bei der Deklaration auch gleich einen Wert oder eine Referenz zuzuweisen (Initialwert):
// Datentyp Variable=Wert/Referenz;
int i = 5;
int j = 2, k = 3;
System.Collections.IList liste = new System.Collections.ArrayList();
Auch die Mehrfachzuweisung eines Wertes an verschiedene Variablen ist möglich:
int i, j, k;
i = j = k = 123;
Einen Sonderfall der Zuweisung stellt die Deklaration von Feldern (Arrays) dar. Näheres hierzu im entsprechenden Abschnitt.

Datentypen sind in C# nicht elementar, sondern objektbasiert. Jeder der in der Tabelle aufgeführten Datentypen stellt einen Alias auf eine Klasse des Namensraumes "System" dar. Beispielsweise wird der Datentyp codice_59 durch die Klasse codice_60 abgebildet.
Durch die Objektbasiertheit ist es möglich, Methoden auf Datentypen anzuwenden:

codice_61

Vergleichbar mit C++, und anders als bei Java, gibt es unter C# vorzeichenbehaftete und vorzeichenlose Datentypen. Diese werden durch Voranstellen des Buchstabens "s" (für "signed", englisch für "vorzeichenbehaftet") und durch Voranstellen des Buchstabens "u" (für "unsigned", englisch für "vorzeichenlos") gekennzeichnet (codice_62 mit Ausnahme von codice_63). Die Gleitkomma-Datentypen (codice_64) können neben einfacher auch doppelte Genauigkeit aufweisen und haben einen variierenden Speicherbedarf.
Dadurch ändert sich die Genauigkeit, was in der Anzahl der möglichen Nachkommastellen zum Ausdruck kommt.

Einem mit codice_65 deklarierten „Objekt“ kann nach der Deklaration und Initialisierung kein neuer Inhalt zugewiesen werden. Das „Objekt“ wird dadurch zu einer Konstanten.

Es muss dabei vom Compiler festgestellt werden können, dass der Wert, der einer Konstante zugewiesen wird, unveränderlich ist. Es ist also auch möglich, eine Konstante von einem Referenztypen zu definieren, allerdings darf dieser nur null zugewiesen werden.

Grund dafür ist, dass der Compiler alle Verwendungen von Konstanten bereits zum Zeitpunkt des Kompilierens ersetzt. Strukturen können nicht konstant sein, da sie in einem Konstruktor einen Zufallsgenerator benutzen könnten.

Fehlerhafte Zuweisungen einer Konstanten werden mit dem Kompilierfehler CS0133 vom C-Sharp-Kompilierer moniert.
using System;
using System.IO;

public class ConstBeispiel

Konstanten gibt es auch in anderen Sprachen (z. B. C++, Java). In Java werden Konstanten durch das Schlüsselwort codice_66 gekennzeichnet, in Fortran durch codice_67.

Operatoren führen verschiedene Operationen an Werten durch und erzeugen dabei einen neuen Wert. Je nach Anzahl der Operanden wird zwischen unäre, binäre und ternäre Operatoren unterschieden. Die Reihenfolge der Auswertung wird durch die Priorität und Assoziativität bestimmt und kann durch Klammerausdrücken geändert werden.
C# bietet die Möglichkeit Operatoren für benutzerdefinierte Datentypen zu implementieren. Als benutzerdefinierter Datentyp gilt eine selbst geschriebene Klasse oder Struktur. Die Implementierung geschieht mit öffentlichen statischen Methoden. Statt eines Namens tragen sie das Schlüsselwort codice_68 gefolgt von dem Operator-Zeichen welches überladen werden soll. Eine implizite oder explizite Typkonvertierung geschieht mittels codice_69 oder codice_70 als Methoden-Namen. Der Compiler ersetzt je nach Typ der Operanden den Quelltext in einen Aufruf der entsprechenden Methode:

struct SpecialDouble

Einschränkungen:

Um den Operator codice_80 zu simulieren, können Indexer verwendet werden. Die Operatoren codice_81 und codice_82 können nicht direkt überladen werden. Sie werden über die speziellen überladbaren Operatoren codice_2 und codice_16 ausgewertet.

Die Operation codice_85 wird als codice_86 ausgewertet, wobei codice_87 ein Aufruf von dem in codice_88 deklarierten Operator codice_16 ist und codice_90 ein Aufruf des ausgewählten Operator codice_91.

Die Operation codice_92 wird als codice_93 ausgewertet, wobei codice_94 ein Aufruf von dem in codice_88 deklarierten Operator codice_2 ist und codice_97 ein Aufruf des ausgewählten Operator codice_98.

Eine "Eigenschaft" ("property") ist eine Sicht auf eine öffentliche Variable einer Klasse. Die Variable selbst wird durch einen Zugriffsmodifikator wie codice_53 oder codice_55 (bei Variablen, die in abgeleiteten Klassen überschrieben werden sollen) für den Zugriff von außen gesperrt und über eine Eigenschaft zugänglich gemacht. Über die Eigenschaft kann dann bestimmt werden, ob ein lesender oder schreibender Zugriff auf die referenzierte Variable erfolgen darf. Beide Möglichkeiten sind auch miteinander kombinierbar.

Eine Eigenschaft wird durch Zuweisung eines Datentyps (der dem Datentyp der Variable entsprechen muss) zu einem Eigenschaftsnamen angelegt und hat eine ähnliche Struktur wie die Syntax einer Methode. Die Eigenschaft ist dabei wie eine Variable ansprechbar und ihr kann auch ein Zugriffsmodifikator zugewiesen werden. Eine Eigenschaft enthält selbst keine Daten, sondern bildet diese auf die referenzierte Variable ab (vergleichbar mit einem Zeiger).

Zur Abfrage einer Eigenschaft existiert in C# das Schlüsselwort codice_101 und zum Setzen eines Wertes das Schlüsselwort codice_102. Von außen stellt sich die Eigenschaft dann wie eine Variable dar und der Zugriff kann entsprechend erfolgen (vgl. VisualBasic).

Die Programmiersprache Java verfolgt mit den Set- und Get-Methoden (Bean-Pattern, Introspection) das gleiche Ziel – alle Zugriffe erfolgen nie direkt über eine Variable, sondern über die entsprechende Methode.

Beispiel einer Eigenschaftsdefinition codice_103 für eine private Variable (codice_104):
public class EigenschaftBeispiel

Durch das „Weglassen“ des Schlüsselwortes codice_102 oder des Schlüsselwortes codice_101 kann gesteuert werden, ob die Eigenschaft nur gelesen oder nur geschrieben werden darf. Das Schlüsselwort codice_107 ist dabei ein Platzhalter für den der Eigenschaft zugewiesenen Wert, der gesetzt werden soll. Er kann nur in Verbindung mit dem Schlüsselwort codice_102 im entsprechenden Block verwendet werden (und entspricht in etwa einer temporären lokalen Variable).

Beispiel für den Zugriff auf die oben definierte Eigenschaft codice_103:
EigenschaftBeispiel instanz = new EigenschaftBeispiel();
instanz.Wohnort = "Musterstadt";
Console.WriteLine(instanz.Wohnort);
// Ausgabe: "12345 Musterstadt"
Würde bei der obigen Definition der Eigenschaft ‚Wohnort‘ der codice_101-Block weggelassen, so würde der lesende Zugriff zu einem Zugriffsfehler führen (im Beispiel in der Zeile, in der die Ausgabe erfolgt).

Neben dem einfachen Setzen oder Lesen einer Eigenschaft, können im codice_102-Block bzw. codice_101-Block auch Operationen ausgeführt werden, beispielsweise die Potenzierung eines bei codice_102 übergebenen Wertes (codice_107 mal Exponent), bevor er der Variablen zugewiesen wird. Das Gleiche gilt für das Schlüsselwort codice_101.
Theoretisch kann somit ein Zugriff für den Benutzer einer Klasse ganz unerwartete Ergebnisse bringen. Deshalb sollten alle Operationen, die Veränderungen auf einen Wert durchführen über normale Methoden abgebildet werden. Ausgenommen sind natürlich Wertprüfungen bei codice_102.

Das Beispiel konkateniert den der Eigenschaft übergebenen Wert (hier: Musterstadt) zur Zeichenkette „12345 “. Diese Aktion ist syntaktisch und semantisch richtig, sie sollte dennoch in einer Methode ausgeführt werden.

Ab C# 3.0 ist es möglich, Eigenschaften automatisch zu implementieren. Dies ist eine verkürzte Schreibweise für Eigenschaften bei denen es sich um den Zugriff auf eine Variable handelt, die innerhalb der Klasse bzw. Struktur als Feld deklariert wurde.

Beispiel anhand der Struktur Point:

struct Point

Das gleiche Beispiel mit Hilfe automatisch implementierter Eigenschaften:

struct Point

Mit Hilfe des Objektinitialisierer (ab .NET 3.5) ist ein Konstruktor überflüssig:

Point p = new Point { X = 1.2, Y = -3.75 }; // Objektinitialisierer
Console.WriteLine(p.X); // Ausgabe: 1,2
Console.WriteLine(p.Y); // Ausgabe: −3,75
Der Indexer ist die Standardeigenschaft von einem Objekt. Der Aufruf geschieht wie bei einem Array mit eckigen Klammern.

Beispiel: Zugriff auf die 32 Bits einer codice_49 Variable mit Hilfe eines Indexers.
using System;

namespace DemoIndexers

Wie bei Eigenschaften kann der Zugriff auf nur lesend oder nur schreibend beschränkt werden, indem man den get-Accessor bzw. set-Accessor weglässt.

Unterschiede zu Arrays, Methoden und Eigenschaften:
codice_118

Ein auf das Zeichen „\“ (umgekehrter Schrägstrich, engl. "backslash") folgendes Zeichen wird anders interpretiert als sonst. Dabei handelt es sich meistens um nicht darstellbare Zeichen. Soll der umgekehrte Schrägstrich selbst dargestellt werden, so muss er doppelt angegeben werden („\\“).

Hexadezimale Zeichenfolge als Platzhalter für ein einzelnes Unicode-Zeichen: Das Zeichen wird dabei aus dem Steuerzeichen \x gefolgt von dem hexadezimalen Wert des Zeichens gebildet.

Zeichenfolge für "Unicode"-Zeichen in Zeichenliteralen: Das Zeichen wird dabei aus dem Steuerzeichen \u gefolgt von dem hexadezimalen Wert des Zeichens gebildet, z. B. „\u20ac“ für „€“ Der Wert muss zwischen U+0000 und U+FFFF liegen.

Zeichenfolge für Unicode-Zeichen in Zeichenkettenliteralen: Das Zeichen wird dabei aus dem Steuerzeichen \U gefolgt von dem hexadezimalen Wert des Zeichens gebildet. Der Wert muss zwischen U+10000 und U+10FFFF liegen. Hinweis: Unicode-Zeichen im Wertbereich zwischen U+10000 und U+10FFFF sind nur für Zeichenfolgen-Literale zulässig und werden als zwei Unicode-„Ersatzzeichen“ kodiert bzw. interpretiert (s. a. "UTF-16").

Mehrfachvererbung wird in C# nur in Form von Schnittstellen (engl. "interfaces") unterstützt.

Schnittstellen dienen in C# zur Definition von Methoden, ihrer Parameter, ihrer Rückgabewerte sowie von möglichen Ausnahmen.

An dieser Stelle ein Anwendungsbeispiel für die Mehrfachvererbung:
public class MyInt : IComparable, IDisposable

Schnittstellen in C# ähneln den Schnittstellen der Programmiersprache Java. Anders als in Java, dürfen Schnittstellen in C# keine Konstanten enthalten und auch keine Zugriffsmodifikator bei der Definition einer Methode vereinbaren.
public interface A
public interface B
public class Klasse : A, B

Eine Klasse, die ein oder mehrere Schnittstellen einbindet, muss jede in der Schnittstelle definierte (virtuelle) Methode implementieren. Werden mehrere Schnittstellen eingebunden, die Methoden mit dem gleichen Namen und der gleichen Struktur besitzen (d. h. gleiche Parametertypen, Rückgabewerte usw.), so muss die jeweilige Methode in der implementierenden Klasse durch das Voranstellen des Namens der Schnittstelle gekennzeichnet werden. Dabei wird die jeweilige Funktion nur dann aufgerufen, wenn der Zeiger auf das Objekt vom entsprechenden Typ ist:
public static void Main()

Auch Schnittstellen ohne Methodendefinition sind möglich. Sie dienen dann als sogenannte Markierungsschnittstellen (engl. "marker interface"). Auf die Verwendung von "marker interfaces" sollte zu Gunsten von Attributen verzichtet werden. Schnittstellen können jedoch keine statischen Methoden definieren.

Das Einbinden einer Schnittstelle erfolgt analog zur Beerbung einer Klasse. Schnittstellen werden per Konvention mit einem führenden „I“ (für Interface) benannt.

Das Überschreiben einer Methode durch eine abgeleitete Klasse kann mit codice_122 verhindert werden:
interface IMessage {

public class MyClass : IMessage {

Ein Interface kann auch durch eine Basisklasse implementiert werden:

interface IMessage {

public class Messenger {

public class MyClass : Messenger, IMessage {

Das Schlüsselwort wird im Zusammenhang von Vererbung genutzt. Es ist, vereinfacht gesagt, das für die Basisklasse, was codice_123 für die aktuelle Klasse ist. Java hingegen sieht hierfür das Schlüsselwort codice_124 vor.

Nun folgt ein Beispiel, das die Verwendung von codice_125 zeigt:
public class Example : Basisklasse
In diesem Beispiel wurde die Verwendung nur anhand des Basisklassenkonstruktors gezeigt. Wie in der Einleitung beschrieben, kann base auch für den Zugriff auf die Mitglieder der Basisklasse benutzt werden. Die Verwendung erfolgt äquivalent zur Verwendung von this bei der aktuellen Klasse.

"Versiegelte" Klassen sind Klassen, von denen keine Ableitung möglich ist und die folglich nicht als Basisklassen benutzt werden können. Bekanntester Vertreter dieser Art von Klassen ist die Klasse codice_126 aus dem Namensraum codice_127. Der Modifizierer codice_122 kennzeichnet Klassen als versiegelt. Es ist jedoch möglich versiegelte Klassen mit Erweiterungsmethoden zu erweitern.

Analog zu Visual Basic .NET Modulen, können in C# Klassen definiert werden, die ausschließlich aus statischen Elementen bestehen:
static class MeineStatischeKlasse

Ab der Version 3.0 können Datentypen "erweitert" werden. Hierzu wird eine statische Klasse definiert. Erweiterungsmethoden ("engl." extensions) beinhalten jeweils einen ersten Parameter, der mit dem Schlüsselwort "this" beginnt, gefolgt von der gewöhnlichen Definition des Parameters:
using System;

namespace MeinNamespace

Sofern die Klasse "ExtensionKlasse" für eine andere Klasse sichtbar ist, werden nun alle Zahlen vom Typ "int" mit der Methode "MalDrei" erweitert ohne aber den Typ "int" wirklich zu ändern. Der Compiler macht hierbei intern nichts anderes als die Methode "MalDrei" der Klasse "ExtensionKlasse" aufzurufen und den Wert 1993 als ersten Parameter zu übergeben.

Anonyme Methoden werden u. a. verwendet, um Code für ein Event zu hinterlegen, ohne in einer Klasse eine Methode mit einem eindeutigen Namen definieren zu müssen. Anstelle des Methodennamens steht das Schlüsselwort "delegate":
Button btn = new Button()

btn.Click += delegate(object sender, EventArgs e)

Ab der Version 3.0 besteht die Möglichkeit, anonyme Methoden in kürzerer Form zu definieren. Dies geschieht mit dem Operator Lambda codice_79 (ausgesprochen: "„wechselt zu“"). Auf der linken Seite des Lambda-Operators werden die Eingabeparameter angegeben, auf der rechten Seite befindet sich der Anweisungsblock bzw. ein Ausdruck. Handelt es sich um einen Anweisungsblock, spricht man von einem "Anweisungslambda". Ein "Ausdruckslambda", wie zum Beispiel codice_130, ist hingegen ein Delegate dessen einzige Anweisung ein codice_14 ist. Der Typ der Eingabeparameter kann weggelassen werden, wenn der Compiler diese ableiten kann. Lambda-Ausdrücke können sich auf äußere Variablen beziehen, die im Bereich der einschließenden Methode oder des Typs liegen, in dem der Lambda-Ausdruck definiert wurde.
Button btn = new Button()

// 'sender' wird implizit als System.Object betrachtet
// 'e' wird implizit als System.EventArgs betrachtet
btn.Click += (sender, e) => {

LINQ definiert drei Dinge:
Implementiert wird die Funktionalität durch sogenannte LINQ-Provider, die die namentlich definierten Methoden zur Verfügung stellen. Einer davon ist zum Beispiel LINQ-to-Objects.
// nimm die Liste der Mitarbeiter und
// schreibe jedes Element nach 'm'
var liste = from m in this.MitarbeiterListe

// lies das Property 'Nachname' und nimm nur die
// Elemente, die gleich 'Mustermann' sind
where m.Nachname == "Mustermann"

// sortiere zuerst ABsteigend nach dem Property 'Vorname'
// dann AUFsteigend nach dem Property 'MitarbeiterNummer'
orderby m.Vorname descending, m.MitarbeiterNummer

// wähle nun zum Schluss als Element für die Liste namens 'liste'
// den Wert aus dem Property 'Vorname' jedes Elements aus
select m.Vorname;
In MySQL könnte der obere Ausdruck bspw. folgendermaßen aussehen:
SELECT m.Vorname FROM MitarbeiterListe AS m
WHERE m.Nachname = "Mustermann"
ORDER BY m.Vorname DESC, m.MitarbeiterNummer ASC

In C# ist jeder Variablen ein Datentyp zugeordnet. Manchmal ist es nötig, Typen von Variablen ineinander umzuwandeln. Zu diesem Zweck gibt es Typumwandlungsoperationen. Dabei gibt es implizite und explizite Typumwandlungen.

Eine implizite Typumwandlung erscheint nicht im Quelltext. Sie wird vom Compiler automatisch in den erzeugten Maschinen-Code eingefügt. Voraussetzung dafür ist, dass zwischen Ursprungs- und Zieltyp eine implizierte Typumwandlungsoperation existiert.

Für explizite Typumwandlungen sind in C# zwei Konstrukte vorgesehen:

codice_132

codice_133

Während erstere Umwandlung im Fall einer ungültigen Typumwandlung eine Ausnahme auslöst, ist letztere nur möglich, wenn der Zieldatentyp ein Referenzdatentyp ist. Bei einer ungültigen Typumwandlung wird hier dem Ziel der Nullzeiger zugewiesen.
using System.Collections;
public class CastBeispiel

Die codice_134-Anweisung bzw. Anweisungsblock dient dazu den Überlauf einer Ganzzahl zu ignorieren. Mit codice_135 hingegen wird bei einem Überlauf ein OverflowException ausgelöst.

Zu den Typumwandlungen gehört auch das sogenannte "„Boxing“". Es bezeichnet die Umwandlung zwischen Wert- und Referenztypen. Der Zieltyp wird wie bei der expliziten Konvertierung in Klammern vor den umzuwandelnden Typ geschrieben. Erfolgt dies implizit, so spricht man von "„Autoboxing“".

C# erlaubt die Definition von benutzerdefinierten Typumwandlungen. Diese können als explizit oder implizit markiert werden. Implizite benutzerdefinierte Typumwandlung kommen u. a. bei der Überladungsauflösung zum tragen, während explizite dann verwendet werden, wenn oben genannte explizite Typumwandlungssyntax benutzt wird.

"Siehe auch:" Operatoren überladen, Explizite Typumwandlung

Bei der Definition von Interfaces, Klassen, Strukturen, Delegate-Typen und Methoden können Typparameter angegeben und gegebenenfalls mit Einschränkungen versehen werden. Werden Typparameter angegeben, so erhält man "generische" Interfaces, Klassen, und so weiter. Bei der späteren Benutzung solcher Generics füllt man die Typparameter mit konkreten Typargumenten.
Ab C#4 ist es möglich, Typparametern von Interfaces und Delegate-Typen eine Varianzannotation mitzugeben. Diese beeinflusst die Subtyprelation. Angenommen, der generische Typ heiße codice_136 und habe einen einzigen Typparameter codice_88. Möglich sind dann:

Neben der Fortsetzung von Untertypbeziehungen von Typargumenten auf Instanzen von Generics beeinflusst die Varianzannotation auch, an welchen Stellen ein Typparameter benutzt werden darf. So sind beispielsweise die Benutzung von kovarianten Typparametern als Typen von Methodenargumenten und die Benutzung von kontravarianten Typparametern als Rückgabetypen nicht erlaubt.

Siehe .NET Assemblies

Attribute geben die Möglichkeit Metadaten für Assemblies, Funktionen, Parameter, Klassen, Strukturen, Enumerationen oder Felder anzugeben. Diese können Informationen für den Compiler enthalten, für die Laufzeitumgebung oder über Reflexion während der Laufzeit ausgelesen werden.
In C# werden diese mit eckigen Klammern über dem Ziel angegeben. Das STAThread-Attribut wird z. B. benötigt, wenn ein Programm COM Interoperabilität unterstützen soll – Die Fehlermeldung lautet sonst: „Es wurde eine steuernde ‚IUnknown‘ ungleich NULL angegeben, aber entweder war die angeforderte Schnittstelle nicht 'IUnknown', oder der Provider unterstützt keine COM Aggregation.“

[STAThread()]
public static void Main(string[] argumente)

Attribute selbst sind wiederum Klassen, die von der Klasse "Attribute" abgeleitet sind, und können beliebig selbst definiert werden.

Beispiel:
[AttributeUsage(AttributeTargets.All)]
public class Autor : System.Attribute

Verwendung:
[Autor("Name des Autors")]
[Autor("Name des 2. Autors",Age=20)]
[Version(1,0,0,0)]
public class IrgendeineKlasse()

Abfrage von Attributen:
Autor[] a = typeof(IrgendeineKlasse).GetCustomAttributes(typeof(Autor), false) as Autor[];

Schema:
try {
catch (ExceptionTypException exception_data) {
catch (Exception ex){
finally {

Es ist nicht zwingend erforderlich, dass immer alle Blöcke (codice_151, codice_152) angegeben werden. Den Umständen entsprechend kann auf einen codice_153-Block auch direkt ein codice_152-Block folgen, wenn beispielsweise keine Behandlung einer Ausnahme erwünscht ist. Zudem ist es nicht zwingend erforderlich, dass auf ein codice_153-codice_151-Konstrukt ein codice_152-Block folgen muss. Es ist jedoch nicht möglich, nur einen codice_153-Block zu definieren. Ein codice_153-Block muss mindestens von einem weiteren Block gefolgt werden.

Zudem ist es möglich, mehrere codice_151-Blöcke zu definieren. Wird im codice_153-Bereich eine Ausnahme ausgelöst, so werden alle vorhandenen codice_151-Blöcke der Reihe nach durchgegangen, um zu sehen, welcher Block sich um die Ausnahme kümmert. Somit ist es möglich, gezielt verschiedene Reaktionen auf Ausnahmen zu programmieren.

Wird eine Ausnahme von keinem codice_151-Block abgefangen, so wird diese an die nächsthöhere Ebene weitergegeben.

Beispiele vordefinierter Ausnahme-Klassen:

Bei der Implementierung eigener, nicht-kritischer Ausnahmen, ist darauf zu achten, nicht von der Klasse codice_164 abzuleiten, sondern von codice_165.

Mittels codice_13 ist es möglich eine, in einem codice_151-Block, aufgefangene Ausnahme eine Ebene höher zu „werfen“ (weitergeben). Somit ist es möglich, dass auch nachfolgender Code von der aufgetretenen Ausnahme informiert wird und somit seinerseits Aktionen unternehmen kann, um auf die Ausnahme zu reagieren. Der Aufruf-Stack bleibt erhalten.

Durch codice_13 kann auch eine neue Ausnahme ausgelöst werden, beispielsweise eine programmspezifische Ausnahme, die nicht durch bereits vorhandene C#-Ausnahmen abgedeckt wird.

Durch die Codeverifizierung und .NET-Zugriffsverifizierung werden bei C# Speicherzugriffsfehler verhindert. Bei Verwendung von Zeigern werden diese Sicherheitsmechanismen umgangen. Dies ist nur in der Betriebsart „Unsafe Code“ möglich.

Beispiel:
using System;
class us_code {

Zudem können Klassen und Methoden als "unsafe" deklariert werden.

In C# sind, wie in C, C++ und Java, sowohl Zeilen- als auch Blockkommentare möglich.

Zeilenkommentare beginnen dabei mit zwei aufeinanderfolgenden Schrägstrichen (//) und enden in der gleichen Zeile mit dem Zeilenumbruch. Alles, was nach den Schrägstrichen folgt, wird bis zum Zeilenende als Kommentar angesehen und vom Compiler übergangen.

Blockkommentare, die sich über mehrere Zeilen erstrecken können, beginnen mit der Zeichenkombination /* und enden mit */.

Sowohl Zeilen- als auch Blockkommentare können zu Beginn oder auch mitten in einer Zeile beginnen. Blockkommentare können in derselben Zeile enden und es kann ihnen Quelltext folgen, der vom Compiler ausgewertet wird. Alles was innerhalb des Blockkommentars steht, wird vom Compiler übergangen.
// Dies ist ein Zeilenkommentar, der mit dem Zeilenumbruch endet
System.Console.WriteLine("Ein Befehl"); // Ein Zeilenkommentar am Zeilenende
/* Dies ist ein Blockkommentar, der in der gleichen Zeile endet */
System.Console.WriteLine("Ein weiterer Befehl"); /* Ein mehrzeiliger
Blockkommentar */System.Console.WriteLine("Noch ein Befehl");
System.Console.WriteLine("Befehl 1"); /* Kommentar */ System.Console.WriteLine("Befehl 2");
System.Console/* Kommentar */.WriteLine( /* Kommentar */ "..." )
Hinweis: Es sind auch Kommentare innerhalb einer Anweisung bzw. Deklaration möglich, z. B. zur Kommentierung einzelner Methodenparameter. Diese Art von Kommentaren sollte aus Gründen der Lesbarkeit und Wartbarkeit vermieden werden.

Zur Dokumentation von Methoden stellt C# in Form von Metadaten (Attribute) einen Mechanismus bereit, der es ermöglicht, eine XML-basierte Dokumentation erzeugen zu lassen.

Zur Dokumentation von Typen (das heißt, Klassen und deren Elemente wie Attribute oder Methoden) steht eine spezielle Form von Zeilenkommentaren bereit. Hierzu beginnt der Zeilenkommentar mit einem weiteren, dritten Schrägstrich ("///") und befindet sich direkt über dem zu dokumentierenden Typ (z. B. einer Methode). Es folgen nun XML-Tags, die jeweils eine bestimmte Funktion bei der Dokumentation übernehmen, beispielsweise eine Zusammenfassung durch einen "Summary-Tag".

/// <summary>
/// Diese Funktion gibt den größeren Betrag zurück
/// </summary>
/// <param name="a">Der erste Übergabeparameter</param>
/// <param name="b">Der zweite Übergabeparameter</param>
/// <returns>Die Zahl mit dem größeren Betrag</returns>
/// <remarks>Diese Funktion gibt den größeren Betrag der beiden Übergegeben <see cref="Int32"/>zurück.
/// Sind die Beträge gleich groß ist dies ein Fehler</remarks>
/// <exception cref="ArgumentException">Der Betrag der beiden Zahlen ist gleich groß</exception>
public int GetGreaterAbs(int a, int b)

Alternativ kann auch eine externe Ressource referenziert werden, die die Dokumentation enthält:
/// <include file='xml_include_tag.doc' path='MyDocs/MyMembers[@name="test"]/*' />
Dokumentation im XMLDoc-API-Dokumentationsformat wird vom Compiler als ein normaler Zeilenkommentar angesehen. Erst durch Angabe der Compiler-Option „/doc“ werden Kommentare, die mit drei Schrägstrichen beginnen, als XMLDoc erkannt und aus dem Quellcode in eine Datei extrahiert. Diese kann dann weiterverarbeitet werden, z. B. ähnlich wie bei Javadoc zur Erstellung einer HTML-Hilfe verwendet werden. (z. B mit NDoc)

"Siehe auch:" Kommentar (Programmierung)

Die folgenden Bezeichner sind "reservierte Schlüsselwörter" und dürfen nicht für eigene Bezeichner verwendet werden, sofern ihnen nicht ein codice_169-Zeichen vorangestellt wird (zum Beispiel codice_170).

codice_171

Die folgenden Bezeichner sind "Kontextschlüsselwörter", das heißt innerhalb eines gültigen Kontextes – und nur dort – haben sie eine besondere Bedeutung. Sie stellen aber keine reservierten Wörter dar, das heißt außerhalb ihres Kontextes sind sie für eigene Bezeichner erlaubt.

codice_172




</doc>
<doc id="931" url="https://de.wikipedia.org/wiki?curid=931" title="Cholesterin">
Cholesterin

Das Cholesterin, auch genauer Cholesterol (, und ), ist ein in allen tierischen Zellen vorkommender kristalliner, fettartiger Naturstoff. Der 1824 von dem Chemiker Michael Eugène Chevreul geprägte Name leitet sich davon ab, dass Cholesterin bereits im 18. Jahrhundert in Gallensteinen gefunden wurde. Abgesehen von Gallenflüssigkeit ist das in der Leber produzierte Cholesterin auch reichlich in Gehirn, Nerven und Blut enthalten. Cholesterin spielt eine Rolle bei der Stabilisierung von Zellmembranen und für die Nervenfunktion, ist für die Produktion von Sexualhormonen und andere Prozesse wichtig.

Cholesterin ist ein lebenswichtiges Sterol und ein wichtiger Bestandteil der Zellmembran. Es erhöht die Stabilität der Membran und trägt gemeinsam mit Proteinen dazu bei, Signalstoffe in die Zellmembran einzuschleusen und wieder hinauszubefördern. Der menschliche Körper enthält etwa 140 g Cholesterin, über 95 % des Cholesterins befindet sich innerhalb der Zellen und Zellmembranen. Um die Zellen mit Cholesterin, welches lipophil (fettlöslich) sowie hydrophob (in Wasser unlöslich) ist, über das Blut versorgen zu können, wird es für den Transport an Lipoproteine gebunden. Diese können von unterschiedlicher Dichte sein und werden nach ihrem Verhalten beim Zentrifugieren bzw. in der Elektrophorese unterteilt in Chylomikronen, VLDL, IDL, LDL, HDL und Lipoprotein a.
Cholesterin dient im Körper unter anderem als Vorstufe für Steroidhormone und Gallensäuren. Für die Bildung von Hormonen wandelt das Cholesterin-Seitenkettentrennungsenzym Cholesterin zu Pregnenolon um. Dieses ist die Ausgangsverbindung, aus der der Körper die Geschlechtshormone Testosteron, Östradiol und Progesteron und Nebennierenhormone (Corticoide) wie Cortisol und Aldosteron aufbaut. Auch Gallensäuren wie Cholsäure und Glykocholsäure basieren auf der Ausgangssubstanz Cholesterin.

Ein Zwischenprodukt der Cholesterinbiosynthese, das 7-Dehydrocholesterin, ist das Provitamin zur Bildung von Vitamin D durch UV-Licht.

Neue Forschungen zeigen zudem, dass der Körper Cholesterin zur Biosynthese herzwirksamer Glykoside nutzt. Welche Bedeutung diese endogen synthetisierten Glykoside haben, ist noch weitgehend unbekannt.

Aufgrund von Sedimentfunden mit chemischen Cholesterin-Verwandten (Sterolen) wird von einigen Forschern angenommen, dass das Cholesterinmolekül, sofern es nie anders als in belebter Materie auftrat, evolutionsgeschichtlich sehr alt sein müsse. Die Biosynthese des Moleküls könne allerdings erst funktionieren, seitdem Sauerstoff in der Atmosphäre vorhanden sei. In Bakterien und den Membranen von Mitochondrien findet sich aus diesem Grund kaum Cholesterin; Pflanzen und Pilze enthalten ebenfalls kein Cholesterin, dafür aber andere, strukturell ähnliche Sterole.

Cholesterin ist ein polycyclischer Alkohol. Herkömmlich wird es als zur Gruppe der Sterine (Sterole) gehörendes Steroid zu den Lipiden gerechnet. Entgegen einer verbreiteten Verwechslung ist es jedoch kein Fett. Die Steroide gehören zu den Isoprenoiden, die im Gegensatz zu den Fetten keine Ester aus Fettsäure und Alkohol sind, sondern hydrophile Pole als diverse Muster in ihrer hydrophoben Grundstruktur aufweisen können.

Cholesterin ist, wie viele Substanzen, sensibel gegenüber Oxidantien. Autoxidationsprozesse können zu vielen Reaktionsprodukten führen. Bisher sind bereits über achtzig solcher Substanzen bekannt, die häufig beachtliche physiologische Wirkungen haben. Die Isolierung und Reindarstellung der Oxidationsprodukte gelingt durch chromatographische Verfahren. Ihre sichere Identifizierung erfolgt durch spektroskopische Methoden wie z. B. der Massenspektrometrie. Eine umfassende Darstellung dieser Cholesterinoxidationsproduke gibt das Werk von Leland L. Smith: "Cholesterol Autoxidation".

Cholesterin ist ein für Menschen und Tiere lebenswichtiges Zoosterin. Beim Menschen wird Cholesterin zum Großteil (90 %) im Körper selbst hergestellt (synthetisiert), beim Erwachsenen in einer Menge von 1 bis 2 g pro Tag, und nur zu einem kleinen Teil mit der Nahrung (insbesondere von Tieren stammende wie Fleisch von Innereien und Eidotter oder etwa in Fleisch, Kokosnüssn und Palmkernöl vorkommenden gesättigten Fettsäuren) aufgenommen. Die Cholesterinresorption liegt im Durchschnitt bei 0,1 bis 0,3 g pro Tag und kann höchstens auf 0,5 g pro Tag gesteigert werden. Das entspricht 30 bis 60 % des in der Nahrung enthaltenen Cholesterins.

Alle Tiere synthetisieren Cholesterin. Ausgehend von „aktivierter Essigsäure“, dem Acetyl-CoA, wird über Mevalonsäure in vier Schritten Isopentenyldiphosphat erzeugt. Weitere drei Reaktionsschritte führen zum Squalen. Nach dem Ringschluss zum Lanosterin folgen etwa ein Dutzend enzymatischer Reaktionen, die auch parallel verlaufen können, bis schließlich Cholesterin entstanden ist. Dieser letzte Abschnitt ist nicht in allen Einzelheiten bekannt, die beteiligten Enzyme sind jedoch identifiziert.

Cholesterin wird über die Leber ausgeschieden, indem es in Form von Gallensäuren über die Gallenwege in den Darm sezerniert wird (etwa 500 mg pro Tag). Gallensäuren sind für die Resorption wasserunlöslicher Nahrungsbestandteile, also auch von Cholesterin, erforderlich. Cholesterin wird durch Gallensäuren emulgiert und im Dünndarm resorbiert. Da etwa 90 % der Gallensäuren wieder aufgenommen werden, ist die Ausscheidung von Cholesterin entsprechend ineffektiv. Durch Medikamente wie Colestyramin, die Gallensäuren binden und damit ihre Wiederaufnahme erschweren, kann die Cholesterinausscheidung gesteigert werden. Allerdings wird dann die Senkung des Cholesterinspiegels durch Zunahme der LDL-Rezeptordichte auf Leberzellen und die damit gesteigerte Cholesterinaufnahme aus dem Blut in die Leber, teilweise auch durch eine vermehrte Neusynthese, ausgeglichen.

Die Biosynthese des Cholesterins, die insbesondere durch Arbeiten von Konrad Bloch, Feodor Lynen, George Joseph Popják und John W. Cornforth aufgeklärt wurde, geht von den Endprodukten des Mevalonatbiosyntheseweges, von Dimethylallylpyrophosphat und von Isopentenylpyrophosphat aus und benötigt 13 weitere Reaktionen. Beim Menschen sind die Leber und die Darmschleimhaut die Hauptorte der Cholesterinsynthese.

Das Gleichgewicht zwischen benötigtem, selbst produziertem und über die Nahrung aufgenommenem Cholesterin wird über vielfältige Mechanismen aufrechterhalten. Als wichtig kann dabei die Hemmung der HMG-CoA-Reduktase, des wichtigsten Enzyms der Cholesterinbiosynthese, durch Cholesterin gelten (noch stärker wird die HMG-CoA-Reduktase durch Lanosterol, eine Vorstufe von Cholesterin, gehemmt). Damit hemmen Produkte dieses Stoffwechselwegs (Cholesterinsynthese) „ihr“ Enzym; dies ist ein typisches Beispiel negativer Rückkopplung. Außerdem verkürzt sich die Halbwertszeit der HMG-CoA-Reduktase bei erhöhtem Lanosterolspiegel stark, da sie dann vermehrt an Insigs (insulininduzierte Gene) bindet, was schließlich zu ihrem Abbau im Proteasom führt. Es gibt noch viele andere, weniger direkte Regulationsmechanismen, die auf transkriptioneller Ebene ablaufen. Hier sind die Proteine SCAP, Insig-1 und -2 wichtig, die in Anwesenheit von Cholesterin, für das sie eine Bindungsstelle besitzen, über die proteolytische Aktivierung von SREBPs die Aktivität einer größeren Anzahl Gene regulieren. Auch Insulin spielt hier eine Rolle, da es u. a. die Transkription von SREBP1c steigert.

Die HMG-CoA-Reduktase, das Schlüsselenzym der Cholesterinbiosynthese, kann spezifisch und effektiv durch verschiedene Substanzen gehemmt werden (beispielsweise Statine, die als HMG-CoA-Reduktase-Hemmer eine bestimmte Klasse von Medikamenten darstellen). Über den LDL-Rezeptor wird die Aufnahme in die Zelle aktiviert.

Die Höhe des Cholesterinspiegels hängt vor allem von der körpereigenen Produktion ab und erst in zweiter Linie von der Zufuhr über die Nahrung. Daneben gibt es eine Vielzahl genetisch bedingter Hypercholesterinämien. Auch als Folge anderer Erkrankungen kann der Cholesterinspiegel erhöht sein (beispielsweise durch Hypothyreose, Niereninsuffizienz oder metabolisches Syndrom).

Da Cholesterin in Wasser unlöslich ist, erfolgt der Transport im Blutplasma zusammen mit anderen lipophilen Substanzen wie Phospholipiden, Triglyceriden oder Fettsäuren, mit Hilfe von Transportvesikeln, den Lipoproteinen.

Das über die Nahrung zugeführte Cholesterin sowie Triglyceride werden nach der Resorption aus dem Darm von den Chylomikronen aufgenommen und von dort in die Leber transportiert. Lipoproteine verschiedener Dichte (VLDL, IDL und LDL) transportieren selbst hergestelltes und aufgenommenes Cholesterin von der Leber zu den Geweben. HDL nehmen Cholesterin aus den Geweben auf und bringen es zur Leber zurück (reverser Cholesterintransport). Das Cholesterin in den Lipoproteinen ist überwiegend mit Fettsäuren verestert. Das Spektrum dieser Fettsäuren ist in starkem Maße durch die mit der Nahrung aufgenommenen Triglyceride zu beeinflussen. Dies zeigen insbesondere Studien an Bevölkerungsgruppen mit speziellen Ernährungsformen wie z. B. Vegetarier und Veganer.

Für den Abbau des LDL-Cholesterins im Blut gibt es im menschlichen Körper zwei voneinander unabhängige Wege, den LDL-Rezeptorweg und den sogenannten "Scavenger-Pathway". Der größte Teil, ca. 65 % des LDL-Cholesterins im Plasma, wird über LDL-Rezeptoren verstoffwechselt. LDL-Rezeptoren findet man in allen Zelltypen der Arterien und in Hepatozyten (Leberzellen). Neben dem LDL-Rezeptorweg werden circa 15 % des LDL-Cholesterins im Plasma über den Scavenger-Pathway in den Blutgefäßen abgebaut. Als Scavenger-Zellen werden die Makrophagen bezeichnet. Sie besitzen sogenannte Scavenger-Rezeptoren, über die chemisch modifizierte (oxidierte) LDL ungehemmt und konzentrationsunabhängig aufgenommen und gespeichert werden können.

Zusammenfassend lassen sich drei verschiedene Wege beschreiben, die das Cholesterin (unabhängig ob über die Nahrung oder selbst synthetisiert) im Organismus nimmt:

Der durchschnittliche Gesamtcholesterinspiegel wie auch die LDL- und HDL-Spiegel der gesunden Normalbevölkerung sind von Land zu Land verschieden und darüber hinaus alters- und geschlechtsabhängig. Es besteht eine positive Korrelation zwischen den Blutcholesterin-Werten und dem Body-Mass-Index.

Generell nimmt der Gesamtcholesterinspiegel mit dem Alter deutlich zu. In der Regel ist er bei jungen Frauen etwas niedriger als bei jungen Männern. Mit zunehmendem Alter gleicht sich dieser Unterschied jedoch aus, und ältere Frauen haben schließlich im Mittel einen höheren Cholesterinspiegel als ältere Männer. Einen Sonderfall stellt die Schwangerschaft dar, in der der Gesamtcholesterinspiegel im Normalfall deutlich erhöht ist.

Der durchschnittliche Gesamtcholesterinspiegel der Altersgruppe zwischen 35 und 65 Jahren in Deutschland liegt bei etwa 236 mg/dl (entspricht 6,1 mmol/l), die Standardabweichung bei ±46 mg/dl. Das bedeutet näherungsweise, dass etwa zwei Drittel der deutschen Bevölkerung in dieser Altersgruppe einen Gesamtcholesterinwert im Bereich zwischen 190 mg/dl und 282 mg/dl aufweisen und je ein Sechstel der Deutschen in dieser Altersgruppe Werte oberhalb beziehungsweise unterhalb dieses Bereichs. In manchen Teilen Chinas liegt der durchschnittliche Cholesterinwert bei 94 mg/dl mit Normwerten zwischen 70 mg/dl und 170 mg/dl, wobei die geringeren Cholesterinwerte mit einer geringeren Wahrscheinlichkeit an Herz- und Krebserkrankungen korrelieren.

Der LDL-Cholesterinspiegel unterliegt einer ähnlichen alters- und geschlechtsabhängigen Verteilung. Auch hier ist der altersabhängige Anstieg bei den Frauen deutlich stärker ausgeprägt als bei den Männern. Der Mittelwert der Altersgruppe zwischen 35 und 65 Jahren liegt dabei bei den deutschen Frauen bei 164 mg/dl (Standardabweichung ±44 mg/dl), bei den Männern bei 168 mg/dl (±43 mg/dl).

Der durchschnittliche HDL-Spiegel unterscheidet sich stärker zwischen den beiden Geschlechtern, wobei Frauen im mittleren Alter einen höheren HDL-Spiegel aufweisen als Männer. Die Altersabhängigkeit zeigt sich hier bei beiden Geschlechtern in einem Absinken ab einem Alter von etwa 55 Jahren. Der durchschnittliche HDL-Spiegel bei den deutschen Frauen in der Altersgruppe zwischen 35 und 65 Jahren liegt bei 45 mg/dl (±12 mg/dl), bei den Männern bei 37 mg/dl (±11 mg/dl).

Auf Grundlage der vorgenannten Parameter werden gelegentlich Quotienten aus diesen Werten bestimmt. Der Mittelwert des Quotienten aus LDL- und HDL-Spiegel liegt für die deutschen Frauen zwischen 35 und 65 Jahren bei 3,9 (±1,6), bei den Männern bei 4,9 (±1,9). Die entsprechenden Durchschnittswerte für den Quotienten aus dem Gesamtcholesterin- und dem HDL-Spiegel liegen für die Frauen bei 5,7 (±2,1), für die Männer bei 7,0 (±2,3).

Die Bestimmung der Konzentration von Cholesterin im Blut in medizinischen Routinelabors gehört heute zu den Bestimmungsmethoden, die in Deutschland ringversuchspflichtig sind. Ein Ringversuch ist die externe Qualitätskontrolle von Laborparametern, die von der Bundesärztekammer kontrolliert und zertifiziert wird. An die so genannten „Richtlinien der Bundesärztekammer“ (RiLiBÄK) muss sich jedes Labor in Deutschland halten. Der Referenzbereich (oftmals irreführend als „Normalwert“ bezeichnet) ist vom Messgerät und der Methode abhängig. Für die Bestimmung von Cholesterin werden in Deutschland in den meisten Labors Geräte von Roche Diagnostics (früher Boehringer Mannheim) verwendet. Auf dem Modell "Hitachi" wird als Referenzwert für das Gesamtcholesterin 110–230 mg/dl angegeben. Beim neueren Gerät "Modular" wird als Referenzbereich <240 mg/dl angegeben. Die Referenzbereiche wurden in den letzten Jahren mehrfach nach oben korrigiert. Um eine Verfälschung der Ergebnisse auszuschließen, wird die Bestimmung häufig erst 12 bis 16 Stunden nach der letzten Mahlzeit durchgeführt.

Lange Zeit wurde im Labor nur das Gesamtcholesterin bestimmt, da die direkte Messung der verschiedenen Lipoproteine nicht möglich bzw. sehr aufwendig war. Das hat sich mittlerweile geändert. Das LDL-Cholesterin wird nicht direkt bestimmt, sondern aus den direkt gemessenen Werten für Gesamtcholesterin, Triglyceride und HDL nach Friedewald et al. abgeschätzt als Gesamtcholesterin minus HDL-Cholesterin minus ein Fünftel des Triglyceridwertes (alle Angaben in mg/dl). Diese Methode kann nicht angewendet werden für Triglyzeridwerte über 400 mg/dl oder bei Vorliegen einer Chylomikronämie. Verschiedene Korrekturfaktoren sind vorgeschlagen worden, um die Präzision dieser Abschätzung zu erhöhen, jedoch sind sie bisher nicht in die klinische Praxis eingegangen. Der Referenzbereich für den LDL-Cholesterinspiegel wird für Frauen und Männer zwischen 70 und 180 mg/dl angegeben.

In Westdeutschland wird für die Angabe der Konzentration von Cholesterin im Blut häufig die Einheit „mg/dl“ (Milligramm pro Deziliter) verwendet. In Ostdeutschland wird dagegen – wie im angelsächsischen Sprachraum – überwiegend die Einheit „mmol/l“ (Millimol pro Liter, vergleiche Milli und Mol) benutzt. Für Cholesterin (nicht jedoch für Triglyceride oder andere Stoffe) gilt der folgende Zusammenhang zwischen diesen Maßeinheiten:

Beispiel:

Für Triglyceride gelten die folgenden Umrechnungsformeln:

Zu den bekannten Erkrankungen im Zusammenhang mit Cholesterin gehören die familiäre Hypercholesterinämie und Gallensteine (Gallenkonkrement).

Es gibt erbliche Störungen des Cholesterinstoffwechsels (familiäre Hypercholesterinämie), die unabhängig von der Nahrungsaufnahme zu stark erhöhten Cholesterinwerten im Blut führen. Bei einer der bekannten Formen der Hypercholesterinämie sind die LDL-Rezeptoren nur unvollständig ausgebildet oder fehlen ganz.

Heterozygote Träger dieser Erbfaktoren sind überdurchschnittlich häufig schon in jüngeren Jahren von Herzinfarkten und anderen Gefäßkrankheiten betroffen. Gemäß einer Untersuchung aus dem Jahre 1991 gilt dies nicht mehr für ältere Personen. Hier geht die Mortalität sogar deutlich zurück und liegt nur bei 44 % gegenüber dem Standard.

Die Prävalenz der häufigsten monogenetischen Hypercholesterinämie, der sogenannten autosomal dominanten familiären Hypercholesterinämie, liegt bei ca. 1:500. Allerdings scheint es im Verlauf der letzten 200 Jahre eine bedeutende Variabilität in der Häufigkeit von Symptomen bei Betroffenen gegeben zu haben, was auf eine Interaktion einer veränderten Umwelt (beispielsweise Ernährung, Lebensstil) mit dem Genotyp hindeutet. Bei schwereren Ausprägungen der Hypercholesterinämie (wie der familiären Hypercholesterinämie) werden medikamentöse Therapien mit Statinen, die LDL-Apherese und teilweise auch chirurgische Therapieformen eingesetzt.

Cholesterin wird mit der Gallensäure im Darm vom Körper aufgenommen. Dabei wird Cholesterin emulgiert und im Dünndarm resorbiert. Die Löslichkeit von Cholesterin in der Gesamtgalle liegt bei 0,26 %. Bei einer Veränderung der Zusammensetzung der Galle kommt es zur Bildung von Cholesterinsteinen. 80 % der Gallensteine sind cholesterinreich und 50 % reine Cholesterinsteine. Die Bildung von Gallensteinen erfolgt nicht nur in der Gallenblase, sondern auch in der Leber.

Weniger bekannte Erkrankungen sind zum Beispiel die Cholesterinspeicherkrankheit (Xanthomatose oder "Hand-Schüller-Christian-Syndrom"), bei der Cholesterin krankhaft unter anderem in der Haut gespeichert wird.

Mit einer Häufigkeit von ca. 1:60.000 kommt in Europa das Smith-Lemli-Opitz-Syndrom (SLO) vor. Grund für die Erkrankung mit SLO-Syndrom ist ein Defekt des letzten Enzyms des Cholesterin-Biosynthesewegs, der 7-Dehydrocholesterin-Reduktase. Das klinische Bild ist gekennzeichnet durch geistige Retardierung, Wachstumsprobleme, Entwicklungsstörungen und Gesichtsveränderungen.

Weiterhin ist eine Hypocholesterinämie bekannt, bei der der Cholesterinspiegel unter 130 mg/dl im Blut vorliegt. Dies tritt vor allem bei Leberschädigung wie einer Leberzirrhose, der genetisch bedingten Tangier-Krankheit und bei Mangan­mangel auf. Dabei kann unter anderem das Vitamin E nicht mehr an seine entsprechenden Zielorte transportiert werden.

Herz-Kreislauf-Erkrankungen, dabei insbesondere die koronare Herzkrankheit (KHK), lösten mit steigendem Lebensstandard im 20. Jahrhundert in den westlichen Industrienationen die Infektionskrankheiten als häufigste Todesursache ab. In den 1950er-Jahren fand die Hypothese des amerikanischen Ernährungsforschers Ancel Keys große Beachtung, diese Entwicklung sei zusätzlich dadurch begünstigt, dass der steigende Wohlstand mit einer zu fetthaltigen Ernährung einhergehe. Insbesondere führe eine cholesterinreiche Ernährung (in erster Linie Fleisch, Hühnerei, Milch, Butter und andere Milchprodukte) zu einem erhöhten Cholesterinspiegel. Der erhöhte Cholesterinspiegel wiederum führe zu Arteriosklerose, welche die häufigste Ursache von Herzinfarkten ist. Die Aufnahme von cholesterinhaltiger Nahrung sei somit eine von vielen Ursachen für einen Herzinfarkt. Diese These ist umstritten. "Siehe auch:" Kritik.

Die Hypothese, cholesterinreiche Ernährung und ein hoher Blut-Cholesterinspiegel spielten eine ursächliche Rolle bei der Entstehung von Herzinfarkten, hat in den vergangenen Jahrzehnten im wissenschaftlichen Umfeld wie in der öffentlichen Wahrnehmung große Verbreitung gefunden. Sie bildet heute in der medizinischen Praxis ein wesentliches Element der Vorbeugung von Herzinfarkten. Sie führte insbesondere in den USA, aber auch in Europa zur Verbreitung künstlich cholesterinreduzierter oder cholesterinfreier Lebensmittel (beispielsweise Margarine) und darüber hinaus zu einer routinemäßigen Verschreibung von Medikamenten zur Senkung des Cholesterinspiegels.

Cholesterinsenker stellen heute das weltweit umsatzstärkste Segment des Pharmamarktes dar. Im Jahre 2004 wurden mit Cholesterinsenkern weltweit Umsätze von 27 Milliarden US-Dollar erzielt, bei einer Wachstumsrate von 10,9 %. Umsatzstärkstes Medikament ist Atorvastatin (Lipitor, Sortis) des US-Herstellers Pfizer, welches 2005 einen Umsatz von weltweit 12,2 Milliarden US-Dollar erzielte. Dieses Medikament spielt allerdings auf dem deutschen Markt heute keine wesentliche Rolle mehr, seit die Krankenkassen eine Festbetragsregelung für Statine eingeführt haben.

Weltweit nehmen etwa 25 Millionen Menschen regelmäßig cholesterinsenkende Präparate ein.

Die Cholesterin-Hypothese stützt sich ausschließlich auf empirisch gewonnene Hinweise. Es konnte jedoch bisher kein biologischer Mechanismus nachgewiesen werden, der über das Cholesterin bzw. einen erhöhten Cholesterinspiegel zur Plaquebildung führt.


Die ursprüngliche Hypothese, ein erhöhter Cholesterinspiegel sei kausal verantwortlich für die koronare Herzerkrankung, wird in jüngerer Zeit meist in etwas modifizierter Form vertreten. Unterschieden wird nun zwischen HDL- und LDL-Cholesterin, wobei ein hoher HDL-Cholesterinspiegel als günstig, ein hoher LDL-Spiegel dagegen als weniger günstig angesehen wird. Entsprechend dieser Vorstellung wird HDL populärwissenschaftlich als „gutes“ Cholesterin bezeichnet, LDL als „schlechtes“ oder „böses“ Cholesterin. Diese Vorstellung stützt sich im Wesentlichen auf folgende Beobachtungen:


Die Hypothese, Cholesterin sei kausal verantwortlich für Herzinfarkte, führte bereits in den 1960er Jahren zu einer breit angelegten öffentlichen Informationskampagne in den USA, um die Bevölkerung vor den möglichen Gefahren eines hohen Cholesterinspiegels zu warnen. Im Jahre 1984 warnte das amerikanische Nachrichtenmagazin Time in einer Titelgeschichte vor dem Verzehr von Eiern und Wurst. Im Jahre 1985 wurde zur Ausweitung dieser Kampagne durch die American Heart Association (AHA, Amerikanischer Kardiologenverband) das "National Cholesterol Education Program" (NCEP, Nationales Cholesterin-Erziehungsprogramm) ins Leben gerufen. Das NCEP gibt seit seiner Gründung regelmäßig Empfehlungen heraus, an denen sich die Behandlung von Patienten mit hohem Cholesterinspiegel orientieren soll. In Deutschland ist die "Deutsche Gesellschaft für Kardiologie" (DGK) die entsprechende Fachgesellschaft, die eigene Zielwerte herausgibt, die aber in der Regel den amerikanischen Werten sehr ähnlich sind. Eine vergleichbare Rolle wie das NCEP übernimmt in Deutschland die industrienahe Lipid-Liga.

Die grundlegenden Richtlinien der NCEP III, denen sich die europäischen und deutschen Gesellschaften angeschlossen haben, unterscheiden drei gestaffelte Risikogruppen. Zur Gruppe 1 zählen alle Patienten, die bereits eine KHK entwickelt haben oder ein vergleichbares Risiko aufweisen (dazu zählt z. B. auch eine Diabeteserkrankung). Diese Patienten haben ein 10-Jahres-Risiko für ein kardiales Ereignis von >20 %. Zur Gruppe 2 zählen die Patienten, die mindestens zwei Risikofaktoren aufweisen, zur Gruppe 3 die Patienten, die weniger als zwei Risikofaktoren aufweisen.

Patienten der Gruppe 1 sollten bei LDL-Werten über 100 mg/dl Lebensstiländerungen vornehmen (Ernährung etc.), bei Werten über 130 mg/dl eine medikamentöse Therapie beginnen. Ziel sollte für sie sein, LDL-Werte unter 100 mg/dl zu erreichen.

Patienten der Gruppe 2 sollten bei LDL-Werten über 130 mg/dl Lebensstiländerungen vornehmen, bei Werten über 130 mg/dl oder 160 mg/dl (abhängig von der spezifischen Risikoberechnung) eine medikamentöse Therapie beginnen. Ziel sollte sein, LDL-Werte unter 130 mg/dl zu erreichen.

Patienten der Gruppe 3 sollten bei LDL-Werten über 160 mg/dl eine Lebensstiländerung vornehmen und eine medikamentöse Therapie erwägen, ab 190 mg/dl wird eine medikamentöse Therapie dringend empfohlen.

Als Risikofaktoren gelten:

Als Lebensstiländerungen werden empfohlen:

Die Anwendung dieser Zielwerte wird von den deutschen Fachgesellschaften der Kardiologen und Internisten unterstützt und befürwortet.

Die Forderung, ein (LDL-)Cholesterinspiegel oberhalb der publizierten Zielwerte müsse gegebenenfalls durch Ernährungsumstellung und/oder eine medikamentöse Therapie abgesenkt werden, war und ist umstritten. Im Folgenden werden die wichtigsten Kritikpunkte aufgeführt:

Die auf der Cholesterinhypothese beruhenden Empfehlungen führen häufig dazu, dass sich gesunde Menschen prophylaktisch einer risikobehafteten medikamentösen Therapie unterziehen oder ihre Ernährung umstellen. Auf Basis der umfangreichen Studienlage zu dieser Fragestellung wird zunehmend angezweifelt, dass ein auslösender, kausaler Zusammenhang zwischen dem Cholesterinspiegel und der Koronaren Herzkrankheit (KHK) besteht.




Der hohe Grad der Finanzierung durch Mittel der Pharmaindustrie betrifft einen Großteil der gesamten medizinischen Forschung und Entwicklung. Auch universitäre Institute finanzieren ihre Forschungsarbeit häufig durch Drittmittel. Einfluss auf die öffentliche Meinung und die Verordnungspraxis von Medikamenten versucht die Pharmaindustrie auch durch sogenannte „Meinungsbildner“ zu gewinnen, die Beratungs- und Vortragshonorare erhalten. Nach einer Untersuchung aus dem Jahr 2001 werden etwa 3 % des Marketingbudgets der Pharmaindustrie – im Fall von cholesterinsenkenden Präparaten entspräche dieser Anteil jährlich einem dreistelligen Euro-Millionenbetrag – in Form von substanziellen Zuwendungen an eine relativ kleine Gruppe von meist international, national oder regional bekannten Professoren ausgeschüttet. Es besteht in Deutschland keine Verpflichtung, diese finanziellen Verflechtungen transparent zu machen. Seit Januar 2006 fordert das „Deutsche Ärzteblatt“ allerdings seine Autoren auf, solche Abhängigkeiten bekanntzugeben und zu veröffentlichen, entsprechend den Gepflogenheiten in internationalen Fachpublikationen. In einer im Jahre 2005 veröffentlichten Studie kritisiert die Anti-Korruptions-Organisation Transparency International Deutschland sowohl die Abhängigkeit der medizinischen Forschung von der Pharmaindustrie als auch die nach seiner Ansicht „alltägliche Praxis der Pharmaindustrie“, sich medizinische Meinungsbildner zu „kaufen“, und spricht in diesem Zusammenhang von einer „strukturellen Korruption“.

Ein hoher Cholesterinspiegel wird häufig als Risikofaktor für Schlaganfälle dargestellt. Die industrienahe Lipid-Liga bezeichnet Cholesterin als einen der wichtigsten Risikofaktoren für Schlaganfälle, und gelegentlich wird Cholesterin sogar als „der wichtigste Risikofaktor“ für Schlaganfälle dargestellt, und eine Senkung des Cholesterinspiegels wird als Vorbeugemaßnahme empfohlen.

Tatsächlich existiert nach Studienlage kein Zusammenhang zwischen dem Cholesterinspiegel und dem Schlaganfallrisiko, der diese Behauptung rechtfertigen würde, und eine mögliche Rolle des Serum-Cholesterins bei der Entstehung von Schlaganfällen und ein Nutzen von cholesterinsenkenden Medikamenten sind umstritten.

In der Framingham-Studie, der größten zu dieser Fragestellung vorliegenden Kohortenstudie, findet sich keinerlei Korrelation zwischen dem Cholesterinspiegel und dem Schlaganfallrisiko.

Auch eine Metaanalyse von 45 Kohortenstudien mit insgesamt 450.000 beobachteten Individuen und über 13.000 beobachteten Schlaganfällen ergab keinerlei Korrelation zwischen dem Cholesterinspiegel und dem Schlaganfallrisiko. Allenfalls bei unter 45-jährigen Patienten besteht möglicherweise eine leichte positive Korrelation.

Andere Studien zeigen bei jungen Frauen ebenfalls eine positive Korrelation zwischen dem (in diesem Alter allerdings absolut gesehen sehr geringen) Schlaganfallrisiko und dem Cholesterinspiegel, während bei älteren Frauen ab dem 50. Lebensjahr das Schlaganfallrisiko mit steigendem Cholesterinspiegel sogar sinkt.

Unterscheidet man zwischen den unterschiedlichen Arten von Schlaganfällen, so sind niedrige Cholesterinspiegel mit einem leicht erhöhten Risiko für hämorrhagische Schlaganfälle verbunden, während hohe Cholesterinspiegel mit einem leicht erhöhten Risiko für ischämische Schlaganfälle einhergehen.

Bis heute liegt keine randomisierte Studie vor, die dafür angelegt war, den Einfluss von Cholesterinsenkung auf das Schlaganfallrisiko zu untersuchen. Allerdings können die vorliegenden Cholesterinsenkungs-Studien zur KHK-Prävention, meist mit KHK-Hochrisikopatienten als Probanden (in der Regel mit vorangegangenem Herzinfarkt oder Diabetes), als Grundlage für entsprechende Auswertungen herangezogen werden.

In einer im Jahr 2003 veröffentlichten Meta-Analyse von 38 zufällig ausgewählten Cholesterinsenkungs-Studien mit unterschiedlichen Präparaten zeigte sich in der Behandlungsgruppe eine zwar geringe, aber statistisch signifikante relative Reduzierung der Schlaganfall-Häufigkeit um 17 Prozent, gleichzeitig jedoch eine nicht signifikante Zunahme der tödlichen Schlaganfälle um 9 Prozent. Statine sind dabei die einzige Wirkstoffgruppe, die zu einer statistisch signifikanten Reduzierung des Schlaganfallsrisikos führt. Möglicherweise ist hierfür allerdings die geringe, aber statistisch signifikant nachgewiesene blutdrucksenkende Wirkung von Statinen verantwortlich; Bluthochdruck gilt als wichtiger Schlaganfall-Risikofaktor.

In der einzigen vorliegenden randomisierten Cholesterinsenkungsstudie mit älteren Patienten (PROSPER) zeigte sich ebenfalls ein Rückgang nichttödlicher ischämischer Schlaganfälle bei einer gleichzeitigen Zunahme tödlicher Schlaganfälle. In der im Jahre 2005 veröffentlichten 4D-Studie mit unter Typ2-Diabetes leidenden Dialysepatienten kam es zu einer statistisch signifikanten Verdopplung der tödlichen Schlaganfälle in der mit Statinen behandelten Gruppe (siehe Abschnitt „Studien“).

Bei Krebserkrankung ist der Cholesterinspiegel zum Beispiel bei an Brustkrebs erkrankten Frauen im Vergleich zu Gesunden erhöht. Ursache dafür könnte sein, dass ein Abbauprodukt von Cholesterin, das Oxysterol, dem Östrogen sehr ähnlich ist und auch eine wachstumsfördernde Wirkung hat.<ref name="DOI10.1126/science.1241908">E. R. Nelson, S. E. Wardell, J. S. Jasper, S. Park, S. Suchindran, M. K. Howe, N. J. Carver, R. V. Pillai, P. M. Sullivan, V. Sondhi, M. Umetani, J. Geradts, D. P. McDonnell: "27-Hydroxycholesterol Links Hypercholesterolemia and Breast Cancer Pathophysiology." In: "Science." 342, 2013, S. 1094–1098, .</ref>

Bei fortschreitendem Leberkrebs wird die Cholesterinbildung eingeschränkt und als Folge sinkt auch der Serum-Cholesterinspiegel.

Von besonderer Bedeutung ist darüber hinaus die Fragestellung, ob eine Cholesterinsenkung eine präventive Wirkung gegenüber bestimmten Krebserkrankungen hat oder ob diese die Entstehung von Krebserkrankungen sogar begünstigt.

Eine im Juli 2007 veröffentlichte Metaanalyse von prospektiven Cholesterinsenkungsstudien ergab eine signifikante Korrelation des Krebsrisikos mit der Einnahme von Statinen. Je niedriger die erzielten LDL-Cholesterinwerte, desto höher war der Anteil der Patienten, die an Krebs erkrankten. Innerhalb einer Beobachtungsdauer zwischen einem und fünf Jahren wurde in der Gruppe der Patienten mit den niedrigsten erzielten LDL-Cholesterinspiegeln etwa eine zusätzliche Krebserkrankung auf 1000 Patienten beobachtet.

In der 1996 veröffentlichten CARE-Studie hatte sich ein hochsignifikanter Anstieg der Brustkrebsfälle in der mit Pravastatin behandelten Gruppe gezeigt (von 1 auf 12). In der 2002 veröffentlichten PROSPER-Studie mit einem im Vergleich zu anderen Statin-Studien vergleichsweise hohen mittleren Alter (und damit Krebsrisiko) der Probanden fand sich ein statistisch signifikanter Anstieg von Krebserkrankungen in der mit Pravastatin behandelten Gruppe. Auch in der 4S- und HPS-Studie zeigte sich jeweils ein (nicht signifikanter) Anstieg von Krebserkrankungen in der mit Simvastatin behandelten Gruppe.

In den letzten Jahren fand auf der Grundlage verschiedener Fall-Kontroll-Studien die gegenteilige Hypothese große Beachtung, Statine hätten möglicherweise gegen verschiedene Krebserkrankungen (u. a. Prostata-Karzinom, Kolorektales Karzinom, Brustkrebs, Nierenkrebs) sogar eine vorbeugende Wirkung. Grundlage für die zum Teil euphorische Medienberichterstattung war folgende Beobachtung: Unter denjenigen Patienten, die die jeweilige Krebserkrankung entwickelt hatten, war der Anteil der Patienten, die Cholesterinsenker eingenommen hatten, niedriger als in einer Vergleichsgruppe ohne Krebserkrankung.

Solche nicht randomisierten Fall-Kontroll-Studien sind allerdings statistisch nur begrenzt aussagekräftig und erlauben insbesondere keinerlei Aussage über Ursache-Wirkungsbeziehungen (siehe auch Fall-Kontroll-Studie). Der hier beobachtete Effekt kann beispielsweise auch darauf beruhen, dass Patienten mit hohem Cholesterinspiegel, die bekanntermaßen eine niedrigere Krebsrate haben, häufiger Cholesterinsenker verschrieben bekommen. Bei dieser Verschreibungspraxis würde sich auch bei einem völlig wirkungsfreien Medikament ergeben, dass diejenigen Patienten, die das Medikament einnehmen, eine niedrigere Krebsrate aufweisen.

Die Fragestellung, ob Statine eine präventive Wirkung gegen das Kolorektal-Karzinom haben, wurde in einer 2006 veröffentlichten Analyse einer großen Kohortenstudie geprüft. Es fand sich jedoch eine nicht signifikante Erhöhung des Krebsrisikos bei der Patientengruppe, die mit cholesterinsenkenden Mitteln behandelt worden war.

Eine im selben Jahr erschienene Meta-Analyse der zahlreichen Statin-Studien kommt gleichfalls zu dem Schluss, dass eine Cholesterinsenkung mit Statinen eindeutig keine präventive Wirkung gegenüber Krebserkrankungen hat, weder auf die Gesamtheit aller Krebserkrankungen noch auf einzelne Krebsarten, die Entstehung von Krebs jedoch auch nicht statistisch signifikant begünstigt.

Die eindeutig negativen Ergebnisse der beiden letztgenannten Studien lassen weitere Studien zu der erhofften krebspräventiven Wirkung von Cholesterinsenkungspräparaten nach Einschätzung von Experten nicht sinnvoll erscheinen.

Nach einer Diagnose eines hohen Cholesterinspiegels wird in der Regel als erste Maßnahme eine fettmodifizierte und cholesterinarme Ernährung empfohlen. Diese Empfehlung ist allerdings umstritten. Eine umfassende Darstellung zu dieser Frage wurde 1981 vom Institut für Sozialmedizin und Epidemiologie des Bundesgesundheitsamts veröffentlicht.

Gemäß den Empfehlungen der DGFF (Lipid-Liga) sollten hierbei folgende Punkte bei der Nahrungsaufnahme bedacht werden:


Kritiker halten dagegen, dass der Einfluss einer kurzfristigen Nahrungsumstellung auf den Cholesterinspiegel nur gering ist, da die Zusammensetzung der Nahrung nur einen geringen Anteil bei der Bildung von Cholesterin hat.

In dem Entwurf der Ernährungsratschläge des US-Landwirtschaftsministeriums von 2015 wird zur Verringerung des Risikos für Herzinfarkte und Schlaganfälle nicht mehr von Produkten mit hohem Cholesteringehalt abgeraten, sondern von dem übermäßigen Konsum gesättigter Fettsäuren.

Eine prospektive Studie, die Verbundstudie Ernährungserhebung und Risikofaktoren Analytik (VERA, von 1985 bis 1988 mit 25.000 Teilnehmern), ergab, dass auch bei verschiedenen Mengen von gesättigten, aber auch ungesättigten Fettsäuren sowohl die HDL- als auch die LDL-Werte sich, wenn überhaupt, nur minimal änderten. Dagegen wird wiederum eingewendet, die Zusammensetzung des LDLs werde ignoriert: Es gebe Hinweise, dass es einen Unterschied machen würde, mit welchen Fettsäuren (ungesättigte oder gesättigte) das Lipoprotein LDL „bestückt“ sei.

Selbst wenn sich diese Zusammenhänge bestätigen sollten, bleibt unbewiesen, dass der Anteil ungesättigter Fette an der Nahrung seinerseits Einfluss auf die Zusammensetzung des LDLs hat und welche ungesättigten Fettsäuren dabei welche Wirkung haben.

Unbestritten ist, dass ein hoher Spiegel an Lipoproteinen niedriger Dichte (LDL-Cholesterin) im Blut die Entstehung einer koronaren Herzkrankheit begünstigt. LDL transportieren Cholesterin aus der Leber zu den Körperzellen. Wird LDL-Cholesterin oxidiert, lagert es sich bevorzugt an den Wänden der Blutgefäße ab. In diesem Fall können lösliche Ballaststoffe wie Beta-Glucan aus Gerste aus der Nahrung entgegen wirken. Diese binden überschüssiges Cholesterin und Gallensäuren im Verdauungstrakt, so dass diese im Dünndarm nicht mehr ins Blut rückresorbiert werden. Eine beta-glucanreiche Kost erhöht deren Ausscheidung über den Stuhl. Zur ernährungstherapeutischen Behandlung der Hypercholesterinämie wird Beta-Glucan aus Gerste bevorzugt eingesetzt. Beta-Glucan-Lieferanten können beispielsweise Gerstenbrote, Gerstenbackwaren und Beta-Glucan-Gerste als Sättigungsbeilage sowie Gerstenflocken mit einem hohen Beta-Glucan Anteil sein. Bereits 3 g Beta-Glucan aus Gerste am Tag als Teil einer abwechslungsreichen und ausgewogenen Ernährung, senken bei ausreichend Bewegung nachweislich den Cholesterinspiegel und reduzieren damit einen wesentlichen Risikofaktor für die koronare Herzerkrankung. Die tägliche Zufuhr muss über einen Zeitraum von mindestens zwei bis drei Wochen erfolgen, um eine signifikante Senkung des LDL-Cholesterinwerts im Blut zu erzielen.

Bei Übergewicht treten Fettstoffwechselstörungen wie erhöhtes LDL-Cholesterin und hohe Plasma-Triglyzeride (→Hypertriglyceridämie) auf. Nach Gewichtsreduktion kann innerhalb kurzer Zeit eine Senkung des Gesamtcholesterin und des LDL-Cholesterin beobachtet werden. Im Vergleich zu anderen Lipiden am deutlichsten verringern sich die Triglyzeride unter einer Gewichtsreduktion. Es zeigte sich, dass eine Gewichtsreduktion von 7 bis 10 kg eine eindeutige Verbesserung in den Lipidparametern mit sich bringt. Zwischen dem HDL-Cholesterin und dem Körpergewicht besteht eine negative Korrelation. Bei einem Body-Mass-Index >30 werden unabhängig von Alter und Geschlecht des Betroffenen niedrigere Werte für das HDL-Cholesterin im Plasma gemessen. Beim Fasten oder einer Ernährung mit weniger als 1000 kcal pro Tag fällt der HDL-Cholesterinspiegel zunächst ab. Erst bei Gewichtskonstanz oder einem geringeren Energiedefizit steigt das HDL-Cholesterin an. Unter körperlichem Training bleibt es allerdings auch bei rascher Gewichtsreduktion konstant. Durch eine langfristige Veränderung der Fettzusammensetzung – insbesondere den teilweisen Ersatz gesättigter durch ungesättigte Fette – kann bei Männern das Risiko für Herz-Kreislauf-Erkrankungen gesenkt werden. Allerdings ist ungeklärt, welches ungesättigte Fett der ideale Ersatz ist.

Bei Vegetariern und Veganern werden verringerte Cholesterinspiegel beobachtet, deren Ursache allerdings nicht geklärt ist, da dies auch ein Nebeneffekt einer anderen, insgesamt gesundheitsbewussteren Lebensweise sein könnte. Auch eine ursprünglichere Lebensweise oder genetische Dispositionen könnten für geringere Cholesterinspiegel verantwortlich sein. Dafür spricht, dass Gruppen wie die Massai oder Samburu-Männer, die sich fast ausschließlich von Milch und Fleisch ernähren, verglichen mit US-Amerikanern deutlich niedrigere Cholesterinwerte haben. Japaner, die in ihrer Heimat weniger von Arteriosklerose betroffen sind, erlitten nach ihrer Migration in die USA genauso häufig Herzinfarkte wie US-Amerikaner. Dies wird häufig als Beleg für die Richtigkeit der Cholesterinhypothese angeführt. Allerdings zeigte eine genauere Auswertung, dass dies unabhängig von der Nahrungszusammensetzung und dem ermittelten Cholesterinspiegel war. Vielmehr wirkte sich ein Beibehalten des japanischen Lebensstils, unabhängig von der Ernährung, positiv aus: Japaner, die sich so fett ernährten wie Amerikaner, jedoch ansonsten weitgehend ihre traditionelle Lebensweise beibehielten, litten seltener an Arteriosklerose – sogar seltener als Japaner, die sich weiterhin fettarm/japanisch ernährten und sich aber an den amerikanischen Lebensstil gewöhnt hatten.

Beta-Glucane sind in Wasser lösliche Ballaststoffe, die in der Haferkleie und im Korninneren bestimmter Gerstensorten, so genannter Beta-Glucan-Gersten, konzentriert vorkommen. Werden regelmäßig beta-glucanreiche Nahrungsmittel aufgenommen, sinkt ein erhöhter Cholesterinspiegel bereits nach wenigen Wochen signifikant. Bei einem hohen Cholesteringehalt im Blut setzt sich das Nahrungsfett vermehrt an den Gefäßwänden ab, so dass wichtige Blutgefäße verstopfen können. Deshalb ist ein erhöhter Cholesterinspiegel ein wichtiger Risikofaktor für die koronare Herzerkrankung. Beta-Glucan aus Gerste bindet überschüssiges Cholesterin und Gallensäuren im Verdauungstrakt und transportiert beides aus dem Körper. Dadurch wird die Leber angeregt, neue Gallensäuren aus Cholesterin zu bilden und der LDL-Cholesterinspiegel sinkt. Bereits 3 g Beta-Glucan am Tag aus beta-glucanreichen Lebensmitteln wie Gerstenbackwaren, Gerstenflocken, Gersten-Salate oder Haferkleie als Teil einer abwechslungsreichen und ausgewogenen Ernährung senken nachweislich den Cholesterinspiegel und reduzieren damit einen wesentlichen Risikofaktor von Herz- und Gefäßerkrankungen.

Neben der umstrittenen These des Einflusses einer direkten Cholesterinaufnahme durch die Nahrung besteht auch ein Einfluss der Ernährung auf die Cholesterinsynthese durch die Beeinflussung der Synthese von Prostaglandinen. Prostaglandine sind Gewebshormone, die unter anderem die Synthese von Cholesterin steuern, wobei ein Prostaglandin in die eine Richtung wirkt (etwa das cholesterinsenkende Serie-1 PGE1) und ein anderes gegensätzlich (hier Serie-2 PGE2).
Die Bildung von Serie-1 oder Serie-2 Prostaglandinen wiederum wird durch das Verhältnis von mehrfach ungesättigten Fettsäuren (omega-3 zu omega-6) in der Nahrung beeinflusst.
Prostaglandine steuern außer der Cholesterinsynthese auch andere Faktoren der Entstehung von Arteriosklerose, so z. B. Lipoprotein(A) und Entzündungsparameter.
Die Ernährungsempfehlungen, um die Prostaglandine günstig zur Cholesterinsenkung zu beeinflussen, wären etwa:

Niedrige Cholesterinspiegel der werdenden Mutter (Gesamtcholesterin unter 160 mg/dl) sind ein Risikofaktor für Frühgeburt und niedriges Geburtsgewicht.

Muttermilch enthält einen sehr hohen Anteil an Cholesterin (ca. 25 mg / 100 g, Kuhmilch enthält nur ca. 12 mg / 100 g). Es wird vermutet, dass der höhere Cholesterinanteil der Muttermilch dafür verantwortlich sein könnte, dass gestillte Kinder später im Mittel einen höheren IQ entwickeln, auch weil bekannt ist, dass Cholesterin beim Aufbau des Gehirns und Nervensystems eine wesentliche Rolle spielt. Babynahrungshersteller verzichten auf die Anreicherung von Muttermilch-Ersatz mit Cholesterin, vermutlich weil sie wegen negativer Assoziationen der Verbraucher mit diesem Stoff mit Absatzproblemen rechnen müssten.

In einer im Jahre 2005 veröffentlichten Studie zeigte sich ein statistisch signifikanter Zusammenhang zwischen einem niedrigen Gesamtcholesterinspiegel bei Kindern und Schulverweisen. Kinder und Jugendliche mit einem Gesamtcholesterinspiegel unterhalb des formula_1-Perzentils (<145 mg/dl) hatten eine fast dreifach erhöhte Wahrscheinlichkeit, in ihrer Schullaufbahn von der Schule verwiesen worden zu sein. Dies wird von den Autoren als weiterer Hinweis dafür gewertet, dass niedrige Cholesterinspiegel mit einer erhöhten Aggressivität im Zusammenhang stehen.

Niedrige Cholesterinspiegel haben sich in verschiedenen Studien als Risikofaktor für das Auftreten von Depressionen herausgestellt. So zeigte sich beispielsweise bei jungen, gesunden Frauen mit einem Gesamtcholesterinspiegel unterhalb von 4,14 mmol/l (160 mg/dl) ein etwa doppelt so hohes Risiko für das Auftreten von Depressionen wie bei Frauen mit mittlerem bis hohem Cholesterinspiegel.

Auch die Einnahme von cholesterinsenkenden Medikamenten begünstigt offenbar die Entstehung von Depressionen. So zeigte sich in einer Studie an 234 älteren, depressiven Patienten, dass diejenigen Patienten, die Cholesterinsenker einnahmen, ein statistisch signifikant um fast 80 % erhöhtes relatives Risiko für das Auftreten eines Rückfalls hatten als Patienten ohne diese Medikation. In einer placebokontrollierten Studie an über 70-jährigen Patienten zeigte sich, dass die Stimmungslage der Patienten in der mit einem Cholesterinsenker behandelten Patientengruppe statistisch signifikant negativ beeinträchtigt war.

Von möglichen positiven Auswirkungen der Statin-Einnahme auf die Psyche berichtet eine Kohortenstudie, in der Patienten, die über den Zeitraum von vier Jahren ununterbrochen Statine eingenommen hatten, mit solchen Patienten verglichen wurden, die gar nicht oder nur mit Unterbrechungen Statine eingenommen hatten. In der ersten Gruppe zeigte sich eine reduzierte Prävalenz von Depressionen, die jedoch nicht mit dem Maß der Cholesterinsenkung in Zusammenhang stand. Die Aussagekraft dieser Studie ist jedoch dadurch beeinträchtigt, dass Patienten, die zum Beispiel wegen möglicher Nebenwirkungen der Medikamenteneinnahme aus der Studie ausschieden, in der Auswertung nicht berücksichtigt werden konnten.

In verschiedenen Studien wurde der Einfluss einer Cholesterinsenkung auf die Gedächtnisleistung untersucht. In einer im Jahr 2000 veröffentlichten Studie an 192 gesunden Erwachsenen zeigte sich, dass sowohl die Gedächtnisleistung als auch die Aufmerksamkeit der Probanden in der mit dem Cholesterinsenker Lovastatin behandelten Gruppe signifikant schlechter ausfiel als in der Kontrollgruppe. Der Leistungsunterschied war signifikant verknüpft mit den absoluten LDL-Cholesterinwerten nach der Behandlung, d. h., niedrigere Cholesterinwerte gingen mit einer schlechteren Gedächtnisleistung einher. Auch in einer an 326 Frauen mittleren Alters durchgeführten und 2003 veröffentlichten Studie zeigte sich eine lineare Korrelation der Gedächtnisleistung mit dem LDL-Cholesterinspiegel.

In einem im Jahr 2003 veröffentlichten Übersichtsartikel werden 60 Fälle von totalem Gedächtnisverlust im Zusammenhang mit einer Statin-Behandlung beschrieben. Nach Absetzen der Statin-Behandlung verschwanden in etwas weniger als der Hälfte der dokumentierten Fälle die Gedächtnisstörungen ganz oder teilweise.

Niedrige Serum-Cholesterinspiegel stehen offenbar mit dem Auftreten von nächtlichen Albträumen in Verbindung. Darüber hinaus gibt es einzelne Fallberichte, in denen ein direkter Zusammenhang zwischen der Einnahme von Cholesterinsenkern und dem Auftreten von Albträumen beschrieben wurde.

In einer Studie aus dem Jahr 2005 wurde ein Zusammenhang zwischen erhöhtem psychischem Stress und einer Erhöhung des Cholesterinspiegels nachgewiesen. Dieser Zusammenhang zeigte sich sowohl kurzfristig als auch innerhalb eines Zeitraums von drei Jahren. Allerdings war die Ausprägung dieses Phänomens für verschiedene Probanden stark unterschiedlich. Die Probanden, die auch kurzfristig unter Stresseinfluss einen relativ hohen Cholesterinanstieg zeigten, hatten auch besonders hohe Anstiege über den längeren Zeitraum.

Die ersten Mittel zur Senkung des Cholesterinspiegels waren Gallensäureaustauscherharze "(Cholestipol)". Später kamen dann Fibrate sowie Nikotinsäurepräparate und deren Derivate auf den Markt. Heute werden in diesem Indikationsbereich fast nur noch Statine und Cholesterinwiederaufnahmehemmer eingesetzt, in Einzelfällen noch Fibrate.

Derzeit sind die Wirkstoffe Bezafibrat, Fenofibrat und Gemfibrozil im Einsatz. Fibrate zeichnen sich durch eine gute Triglyceridsenkung aus und werden heute deshalb vor allem bei Diabetikern eingesetzt.

Als die zur Zeit wirksamsten Medikamente zur Senkung des Cholesterinspiegels gelten Statine. Sie gehören zur Gruppe der HMG-CoA-Reduktase-Hemmer (CSE-Hemmer), da sie das Schlüsselenzym der Cholesterinsynthese in der Zelle, die β-Hydroxy-β-methylglutaryl-Coenzym-A-Reduktase hemmen. Als Folge stellt die Zelle benötigtes Cholesterin nicht mehr selbst her, sondern nimmt Cholesterin aus dem Blut, über LDL-Rezeptoren, auf.

Der Wirkstoff Ezetimib ist ein im Darm wirkender, selektiver Cholesterinwiederaufnahmehemmer, der gezielt das Niemann-Pick C1-Like 1 (NPC1L1)-Protein blockiert. NPC1-L1 sitzt in der Membran von Enterozyten der Dünndarmwand und ist für die Aufnahme von Cholesterin und Phytosterolen aus dem Darm zuständig.

Das Enzym Proproteinkonvertase Subtilisin/Kexin Typ 9 (PCSK9) ist ein wichtiger intrinsischer Determinator des LDL-Spiegels. Es bindet den LDL-Rezeptor irreversibel und vermindert daher die Resorptionsrate von LDL aus dem Blut mit entsprechend höherem LDL-Spiegel. Bei einer seltenen Gen-Variante mit verminderter PCSK9-Aktivität zeigte sich ein deutlich geringerer LDL-Spiegel mit geringerer Rate koronarer Herzerkrankungen. Dies führte zur Entwicklung spezifischer gegen PCSK9 gerichteter monoklonaler Antikörper (PCSK9-Hemmer).

2015 wurden in der Europäischen Union die folgenden PCSK9-Hemmer als Arzneistoffe zugelassen: Alirocumab als Praluent der Firma Sanofi und Evolocumab als Repatha der Firma Amgen.

Für Alirocumab und Evolocumab konnte in Phase-III-Studien an fast 7.000 Patienten mit trotz Statin-Therapie erhöhtem LDL-Spiegel eine sehr gute Wirksamkeit der beiden monoklonalen Antikörper gezeigt werden. Im Mittel sank der LDL-Spiegel um 61-62 %. Bei "post hoc"-Analysen zeigte sich trotz des relativ kurzen Untersuchungszeitraums von 12 – 18 Monaten eine etwa 50%ige Reduktion schwerer kardiovaskulärer Ereignisse. Die Antikörper müssen zweiwöchentlich oder monatlich subkutan gespritzt werden.

Schwerwiegende Nebenwirkungen wurden bisher nicht beschrieben. Als häufigste Nebenwirkungen sind allgemeine allergische Reaktionen und Reizungen im Bereich der Injektionsstelle zu nennen (bei je unter 10 % der Patienten).

Mit Bococizumab befindet sich ein weiterer PCSK9-Antikörper in der Entwicklung.

In zahlreichen Studien wurde die Auswirkung des Cholesterinspiegels auf die Inzidenz von Herz-Kreislauf-Erkrankungen untersucht, aber auch andere Fragen im Zusammenhang mit dem Cholesterinspiegel. Die Vielzahl der Studien macht das Heranziehen einzelner Studien zur Begründung eines Effekts grundsätzlich problematisch, da die Durchführung mehrerer Studien zur Beantwortung der gleichen Frage die vermeintliche statistische Signifikanz einer einzelnen Studie außer Kraft setzen kann. So genügen im Mittel zwanzig für sich betrachtet methodisch korrekt angelegte Studien, um einen nicht vorhandenen Effekt einmal statistisch signifikant „nachzuweisen“. Metastudien gewinnen daher im Zusammenhang mit dem Cholesterin-Thema ein besonderes Gewicht. Auch diese sind jedoch durch den sogenannten „Publikationsbias“ beeinflusst.

Eine der wegweisenden Studien auf dem Gebiet der Untersuchung von KHK-Risikofaktoren war die Framingham-Studie, die heute als die wichtigste epidemiologische Studie der USA gilt. Sie untersuchte 6000 Personen zweier Generationen in Framingham/Massachusetts. Über die Framingham-Studie wurden bis zum heutigen Tag über 1000 wissenschaftliche Publikationen erstellt. Im Rahmen dieser Studie wurde unter anderem nachgewiesen, dass Rauchen und Übergewicht wichtige KHK-Risikofaktoren sind. Es ergab sich darüber hinaus, dass bei Männern im Alter von 30 bis 59 Jahren das Auftreten von KHK entsprechend dem Cholesteringehalt im Blut erhöht ist. Bei Männern in den Dreißigern wiesen die Personen mit dem höchsten Gesamtcholesteringehalt im Blut ein viermal höheres Risiko auf als diejenigen mit dem geringsten Cholesterin. Für Frauen und für Personen über 50 Jahre zeigte sich kein solcher Zusammenhang. Eine Prüfung der Framingham-Studie im Jahre 1987 zeigte, dass bei Personen über 50 eine Absenkung des Cholesterinspiegels um 1 mg/dl zu einer Steigerung der Gesamttodesrate von 11 % und zu einer Steigerung der Todesrate durch Herzkrankheiten um 14 % führte.

Das American National Heart, Lung and Blood-Institute führte Metastudien zum gesundheitlichen Nutzen der Cholesterinsenkung durch. 19 Studien wurden analysiert. Untersucht wurden 650.000 Menschen und 70.000 Todesfälle: Geringe Cholesterinspiegel gehen nicht mit einer allgemeinen Erhöhung der Lebenserwartung einher, sondern beziehen sich nur auf Herz-Kreislauf-Erkrankungen, sie erhöhen das Risiko von Schlaganfällen und das Krebsrisiko. Allerdings ist immer noch umstritten, wo hier Ursache und Wirkung liegen; zum Zeitpunkt der Messung könnten niedrige wie auch hohe Cholesterinspiegel auch durch (noch nicht diagnostizierte) Krankheiten im Anfangsstadium verursacht sein. Als gesichert gilt, dass sehr hohe, sehr niedrige und fallende Cholesterinspiegel mit einer erhöhten Mortalität verbunden sind, wobei unklar bleibt, ob das Cholesterin Ursache oder eben nur Indiz eines verschlechterten Gesundheitszustandes ist.

Die CARE-Studie ("Cholesterol And Recurrent Event Study") mit Patienten mit 3 bis 20 Monaten zurückliegenden Herzinfarkt zeigte als Folge einer LDL-Cholesterinsenkung zwischen 115 und 174 mg/dl eine statistisch nicht signifikante Reduktion von Reinfarktraten und der Frequenz des Koronartods (von 5,7 % in der Kontrollgruppe auf 4,6 % in der Behandlungsgruppe nach fünf Jahren). Der Rückgang der KHK-Toten wurde allerdings durch eine Zunahme anderer Todesursachen in der Behandlungsgruppe ausgeglichen. Bei den nicht-tödlichen Herzinfarkten und bei der Zahl der Schlaganfälle zeigten sich Vorteile in der Behandlungsgruppe.

Die erste Statin-Studie begann 1990 unter dem Namen "Expanded Clinical Evaluation of Lovastatin (EXCEL)". An der Studie nahmen 8245 Personen mit „moderat erhöhtem“ Cholesterinspiegel teil. Die drei Behandlungsgruppen erhielten Lovastatin in unterschiedlichen Dosierungen, die Kontrollgruppe von 1.650 Patienten erhielt ein Placebo. In einer 1991 veröffentlichten ersten Auswertung der Studie zeigte sich ein Anstieg der Sterblichkeit von 0,2 % in der Kontrollgruppe auf 0,5 % im Mittel der drei Behandlungsgruppen, der grenzwertig statistisch signifikant war. Über den weiteren Verlauf der Mortalität in dieser Studie wurden vom Lovastatin-Hersteller MSD Sharp & Dohme keine Zahlen veröffentlicht.

Die "Scandinavian Simvastatin Survival Study" wird kurz als 4S-Studie bezeichnet. Innerhalb der ersten fünf Behandlungsjahre wurden unter den beteiligten 4444 Patienten mit mindestens sechs Monate zurückliegendem Herzinfarkt oder stabiler Angina Pectoris in der Vorgeschichte die LDL-Cholesterinspiegel um durchschnittlich 35 % gesenkt und die HDL-Cholesterinspiegel um durchschnittlich 8 % gesteigert. Im gleichen Zeitraum wurde die KHK-Mortalität von 8,5 % auf 5,0 % gesenkt, die Rate definitiver Herzinfarkte reduzierte sich von 12,1 % auf 7,4 %.

An dieser Studie gibt es erhebliche methodische Kritik, z. B. vom anzeigenfreien Arznei-Telegramm zur Bewertung von Medikamenten. Die Altersverteilung von Simvastatin- und Placebogruppe war aus den veröffentlichten Daten nicht entnehmbar, gleichzeitig traten typische altersabhängige Krankheiten in der Placebogruppe deutlich häufiger auf. Eine Standardisierung für andere gleichzeitig eingenommene Medikamente, wie zum Beispiel Aspirin, wurde nicht vorgenommen. Darüber hinaus war das KHK-Risikoprofil der Kontrollgruppe deutlich ungünstiger. Und schließlich erschien es verdächtig, dass in einer angeblichen Doppelblindstudie bei den Patienten der Simvastatin-Gruppe die Dosis nach einem halben Jahr verdoppelt wurde, bei denen der Cholesterinspiegel nicht gesunken war. Die Autoren mussten später einräumen, dass das Patientenkollektiv nicht vollständig randomisiert war.

Dennoch bestätigt das Arzneitelegramm im Jahr 2004, dass die 4S-Studie erstmals den Nachweis erbrachte, dass männliche Patienten mit Herzinfarkt oder stabiler Angina Pectoris in der Vorgeschichte von einer medikamentösen Cholesterinsenkung im Sinne einer Lebensverlängerung profitieren könnten. Dieses Ergebnis sei inzwischen durch zwei weitere Studien (HPS und LIPID) bestätigt worden. Das Fachblatt rät jedoch vom Einsatz von Statinen bei Frauen und bei über 70-Jährigen ohne arteriosklerotische Erkrankung ab.

Die PROCAM-Studie ("Prospective Cardiovascular Münster Study") begann 1979 in Münster und untersuchte fast 50.000 Angehörige von Firmen und Mitarbeitern des öffentlichen Dienstes. Sie ergab Hinweise, dass nicht nur die Höhe des Gesamtcholesterins, sondern auch das Verhältnis der verschiedenen Cholesterinfraktionen (LDL, HDL, Triglyceride) für die KHK-Risikobetrachtung ausschlaggebend sein könnte. Auf Basis der Studie wurden Risikorechner für Herzinfarkt und Schlaganfall entwickelt.

Die LIPID-Studie ("Long-Term Intervention with Pravastatin in Ischaemic Disease-Study") zeigte an fast 10.000 Probanden mit mindestens 3 bis 36 Monate zurückliegendem Herzinfarkt oder Krankenhausentlassung nach instabiler Angina Pectoris mit Gesamtcholesterinwerten ab 155 mg/dl und durchschnittlichen LDL-Cholesterinwerten von 150 mg/dl, dass das LDL um durchschnittlich 25 % stärker als unter Placebo gesenkt und das HDL um 5 % angehoben wurde. Dabei wurde die Gesamtsterblichkeit von 14 % auf 11 % gesenkt, die KHK-Sterblichkeit von 8,3 % auf 6,2 %. Die Wirkung hing dabei nicht vom anfänglichen Gesamt- oder LDL-Cholesterinspiegel ab. Auch andere Todesursachen, wie Krebs und Selbstmord, nahmen in der Behandlungsgruppe ab. Damit wird von Kritikern die Vergleichbarkeit der beiden Gruppen in Frage gestellt. Anders als üblich sind entsprechende Durchschnittswerte der Studie nicht zu entnehmen.

In der englisch-skandinavischen Heart Protection Study ließ sich an 20.536 Patienten mit koronarer Herzkrankheit oder anderen atherosklerotischen Erkrankungen oder Hypertonie etc. eine zwar geringe, aber signifikante Senkung der Gesamtsterblichkeit von 14,7 % in der Placebogruppe auf ca. 12,9 % in der mit dem Cholesterinsenker Simvastatin behandelten Gruppe nachweisen; d. h., rund 50 Personen müssen fünf Jahre behandelt werden, um einen Todesfall zu verhindern (NNT=56). Die zur Errechnung dieser Schlussfolgerung der Studie verwendeten statistischen Methoden sind nicht unumstritten. Auch die Kosteneffizienz wurde, u. a. wegen der hohen Preise für Simvastatin, in Frage gestellt. Allerdings hat sich die Grundlage dieser Berechnungen mit Einführung der Simvastatin-Generika 2003 verändert. Ob der in der Studie erkennbare positive Effekt auf die cholesterinsenkende Wirkung oder auf andere Wirkmechanismen der Statine zurückzuführen ist, ist umstritten und Gegenstand aktueller Forschungsarbeit.

In der 4D-Studie („Die Deutsche Diabetes Dialyse“-Studie) wurde erstmals die Wirkung von Atorvastatin (Lipitor/Sortis) bei der Behandlung von Dialysepatienten mit Typ-2-Diabetes untersucht, die ein sehr hohes kardiovaskuläres Risiko haben. Die placebokontrollierte Studie umfasste 1.255 Patienten, die über vier Jahre beobachtet wurden. In der Behandlungsgruppe wurde der LDL-Cholesterinspiegel im Mittel um 42 % reduziert. Es zeigte sich jedoch kein Vorteil bei den KHK-Todesfällen oder bei der Gesamtsterblichkeit. Stattdessen kam es in der Behandlungsgruppe zu einer statistisch signifikanten Verdopplung der Zahl der tödlichen Schlaganfälle.

Hu u. a. untersuchten 1999 in ihrer Studie: „A Prospective Study of Egg Consumption and Risk of Cardiovascular Disease in Men and Women“ mit über 117.000 Probanden den vermuteten Zusammenhang zwischen Eiverzehr und KHK oder Schlaganfall. Ein erhöhtes Risiko bei erhöhtem Eikonsum konnte dabei nur für Diabetiker festgestellt werden. Für die Gesamtgruppe gab es keinen signifikanten Zusammenhang.

Weingärtner u. a. untersuchten 583 Angestellte des Universitätsklinikums des Saarlandes ohne kardiovaskuläre Erkrankungen, bzw. Lipid-senkende Medikamente bezüglich Cholesterinhomöostase (Cholesterinsynthese/Cholesterinresorption) und früher Atherosklerose. Daten dieser Studie zeigen, dass nicht nur Gesamtcholesterin und der Framingham-Risiko-Score, sondern auch Unterschiede in der Cholesterinhomöostase direkt mit Carotis Intima-Media Dicke assoziiert sind.

Petturson u. a. werteten 2012 prospektive Daten über Todesfälle unter 52.087 norwegischen Teilnehmern der HUNT-2-Studie (Nord-Trøndelag Health Study) aus, die zum Studienbeginn keine kardiovaskulären Vorerkrankungen aufwiesen. Die Autoren untersuchten den Zusammenhang zwischen Gesamtcholesterin und verschiedenen Mortalitätsraten. Sie stellten dabei einen leicht signifikanten, jedoch invers verlaufenden Zusammenhang zwischen dem Gesamtcholesterinspiegel und der kardiovaskulären und Gesamtmortalität bei Frauen fest. Frauen mit einem Cholesterinspiegel von mehr als 7 mmol/l (270 mg/dl) wiesen das niedrigste Mortalitätsrisiko auf, jene mit einem Cholesterinspiegel von weniger als 5 mmol/l (193 mg/dl) das höchste. Bei Männern wurde das niedrigste Mortalitätsrisiko bei einem Cholesterinspiegel zwischen 5 und 5,9 mmol/l (193–228 mg/dl) ermittelt. Die Ergebnisse würden demnach gegen den Einsatz von cholesterinsenkenden Medikamenten bei Frauen sprechen. Ebenfalls ein Ergebnis der Studie war, dass die Sterblichkeit unter Rauchern zu Nichtrauchern unabhängig vom Cholesterinspiegel teils um den Faktor 3 erhöht war. Lediglich bei den rauchenden Frauen gab es bei gleichzeitig erhöhtem Cholesterinspiegel eine nochmals signifikant erhöhte Sterblichkeit. Insgesamt sprach man Cholesterin aufgrund der geringen Ausprägung der Ergebnisse eine vernachlässigbare Rolle bei der Sterblichkeit sowohl von Frauen als auch von Männern zu.








</doc>
<doc id="932" url="https://de.wikipedia.org/wiki?curid=932" title="Commodore 64">
Commodore 64

Der Commodore 64 (kurz C64, umgangssprachlich 64er oder „Brotkasten“) ist ein 8-Bit-Heimcomputer mit 64 KB Arbeitsspeicher.

Seit seiner Vorstellung im Januar 1982 auf der "Winter Consumer Electronics Show" war der von Commodore gebaute C64 Mitte bis Ende der 1980er Jahre sowohl als Spielcomputer als auch zur Softwareentwicklung äußerst populär. Er gilt als der meistverkaufte Heimcomputer weltweit – Schätzungen der Verkaufszahlen bewegen sich zwischen 12,5 Mio. und 30 Mio. Exemplaren. Der C64 ermöglichte mit seiner umfangreichen Hardwareausstattung zu einem – nach einer teureren Einführungsphase – erschwinglichen Preis in den 1980er-Jahren erstmals Zugang zu einem für die damalige Zeit leistungsstarken Computer.

Im Gegensatz zu modernen PCs verfügte der C64, wie es zu dieser Zeit bei Heimcomputern üblich war, über keinerlei interne Massenspeichergeräte. Alle Programme mussten von externen Laufwerken, wie dem Kassettenlaufwerk Datasette oder dem 5¼″-Diskettenlaufwerk VC1541, oder von einem Steckmodul "(Cartridge)" geladen werden. Lediglich Grundfunktionen wie der Kernal, der BASIC-Interpreter und zwei Bildschirmzeichensätze waren in drei ROM-Chips mit Speicherkapazitäten von zweimal acht und einmal vier KB gespeichert.

Im Januar 1981 begann die frühere MOS Technology, jetzt als Commodore Semiconductor Group eine Tochter von Commodore International, mit der Entwicklung eines neuen Chipsatzes für Grafik und Audio für eine Spielkonsole der nächsten Generation. Die Arbeit an den beiden Chips VIC II (Grafik) und SID (Audio) war im November 1981 erfolgreich abgeschlossen. Im Anschluss entwickelte der japanische Ingenieur Yashi Terakura von Commodore Japan auf Basis der beiden neuen Chips den Rechner Commodore Max (in Deutschland als VC 10 angekündigt). Die Produktion wurde jedoch bereits kurz nachdem die ersten Commodore MAX in Japan ausgeliefert worden waren wieder eingestellt.

Mitte 1981 machten Robert Russell (System-Programmierer und Entwickler des VC 20) und Robert „Bob“ Yannes (Entwickler des SID) mit der Unterstützung von Al Charpentier (Entwickler des VIC-II) und Charles Winterble (Manager von MOS Technology) dem CEO von Commodore International, Jack Tramiel, den Vorschlag, aus den entwickelten Chips einen wirklichen Low-Cost-Rechner zu bauen, der der Nachfolger des VC 20 werden sollte. Tramiel war einverstanden und erklärte, dass der Rechner einen vergrößerten Speicher von 64 KB RAM, den vollen Adressraum von 16 Bit nutzend, haben solle. Auch wenn zu diesem Zeitpunkt 64 KB RAM noch über 100 US-Dollar kosteten, nahm er an, dass die RAM-Preise bis zur vollen Markteinführung des C64 auf einen akzeptablen Preis fallen würden. Tramiel setzte gleichzeitig das Fristende für die Präsentation des Rechners auf den Beginn der Consumer Electronics Show (CES) im Januar 1982 in Las Vegas. Die Besprechung fand im November 1981 statt, so dass den Entwicklern lediglich zwei Monate blieben, um entsprechende Prototypen des Rechners zu bauen.

Das Projekt hatte zunächst den Codenamen VC-40, der in Anlehnung an das Vorgängermodell VC-20 gewählt worden war. Das Team, welches das Gerät entwickelte, bestand aus Robert Russell, Robert „Bob“ Yannes und David A. Ziembicki. Das Design des C64, Prototypen und einige Beispielsoftware wurden gerade rechtzeitig vor der CES in Las Vegas fertig, nachdem das Team die gesamte Weihnachtszeit (auch an den Wochenenden) durchgearbeitet hatte. Die 40 im Namen sollte die Textauflösung von 40 Zeichen pro Zeile kennzeichnen. Commodore legte diese Auflösung unter anderem deswegen so fest, um unter der Leistungsfähigkeit der für den professionellen Gebrauch vorgesehenen eigenen Rechner der CBM-8000-Serie zu bleiben, die zu der Zeit mit gleicher Prozessorgeschwindigkeit, kleinerer oder gleicher Speicherausstattung, nur monochromen oder deutlich eingeschränkten Farbmöglichkeiten und einem nur wenig leistungsfähigeren BASIC 4.0 angeboten wurden. Ein kennzeichnender Faktor für die professionelle Anwendbarkeit war damals die Möglichkeit, Textzeilen für die Druckausgabe in voller Breite darstellen zu können, wofür 80 Zeichen notwendig waren.

In der Produktionsperiode des C64 änderte man immer wieder optische und technische Details, um moderne Fertigungsmöglichkeiten auszunutzen und Produktionskosten zu senken. Obwohl sich das Innenleben der ersten C64 deutlich von dem der letzten Version unterscheidet, war es den Entwicklern gelungen, alle Versionen von Seiten der Software beinahe hundertprozentig kompatibel zueinander zu halten – was bedeutete, dass die Leistungsdaten des Rechners während des Produktionszyklus nicht gesteigert wurden. Beispielsweise wurde das Hauptplatinenlayout mehrfach geändert sowie CPU, Grafikchip, Soundchip und andere Bauteile überarbeitet. Auch die zur Verschaltung innerhalb des Rechners notwendigen Logikchips fasste man zusammen und integrierte sie in einem Custom-Chip.

Vom C64 gab es im Gegensatz zu anderen damaligen Heimcomputern keine Nachbauten aus Ostblock-Ländern, Lateinamerika oder Fernost. Das ist vor allem in der hochintegrierten Bauweise mit Custom-Chips und in der vertikalen Integration der Firma Commodore begründet – von der Chipfertigung über Chipdesign und Systemdesign bis zum Gehäusedesign war alles in einer Hand, wodurch diese Chips für Nachbauer nicht erhältlich waren.

"Commodore Business Machines" (CBM) hatte vor dem C64 bereits erfolgreich den Bürorechner PET 2001 und seine Nachfolger, aber auch schon den Heimcomputer VC 20 eingeführt. Firmengründer Jack Tramiel prägte die Formel "„We need to build computer for the masses, not the classes!“", was ihm mit dem C64 letztlich auch gelang.

Um die Neuentwicklung in das vorhandene Produktangebot einbinden zu können, entschied sich die Marketingabteilung für den Namen „C64“, was für „Consumer“ und die Größe des verwendeten Speichers in KB stehen sollte. Für den amerikanischen Markt waren bereits nach gleichem Schema benannte Modelle, der B(usiness)256 bzw. der P(ersonal Computer)128, geplant. Letzterer gehörte in die in Europa als Commodore CBM 500 veröffentlichte Reihe und ist nicht identisch mit dem später erschienenen C128.

Im September 1982 kam der C64 für 595 US$ auf den amerikanischen und Anfang 1983 zum Startpreis von 1495 DM (in heutiger Kaufkraft €) auf den deutschen Markt und war in Deutschland, wie in allen wichtigen Märkten der Welt (mit Ausnahme von Japan), sehr erfolgreich. Schon 1983 sank der Preis auf 698 DM.

Hauptkonkurrent war der in den USA stark vertretene "Atari 800 XL". Viele Spiele waren gleichzeitig auf einer 5¼-Zoll-Diskette für beide Systeme erhältlich, wie etwa das Computer-Rollenspiel "Alternate Reality" (Vorderseite C64, Rückseite Atari), was als Hinweis auf die Dominanz der beiden Marken angesehen werden kann. Trotz der Konkurrenz durch Atari und vieler anderer Heimcomputer in dieser Zeit (TI-99/4A, Apple II, Sinclair ZX81, ZX Spectrum, Dragon 32) beurteilten viele Konsumenten das Preis-Leistungs-Verhältnis des C64 zum Beginn seiner Auslieferung günstig. In Kombination mit der rasch ansteigenden Zahl an Softwaretiteln für den C64 entwickelte sich der Rechner zum Erfolg. Auch trug die Tatsache, dass der Computer nicht nur in Fachgeschäften, sondern auch in Kaufhausketten, Versandhäusern (z. B. Quelle), Supermarktketten (z. B. allkauf) und Computer-Versandhäusern (z. B. Vobis) zum Verkauf stand, dazu bei, dass das Gerät in kurzer Zeit ein voller Erfolg wurde. Mit dem Aufstieg des C64 als Heimcomputer kam auch zugleich der endgültige Fall der bis dato am weitesten verbreiteten Konsole, dem Atari VCS 2600.

Commodore produzierte den C64 etwa elf Jahre lang; über 22 Millionen Stück wurden verkauft (andere Quellen geben 17 Millionen an). Damit ist der C64 der meistverkaufte Computer der Welt.

Der Prozessor ist ein 6510 (8500 beim C64C/II), eine Variante des 6502 von MOS Technology. Commodore hatte diese Firma Mitte der 1970er-Jahre aufgekauft, um über ein eigenes Halbleiterwerk zu verfügen. Der 6510 besitzt im Gegensatz zum 6502 einen 6 Bit breiten bidirektionalen I/O-Port, der sich über die Speicheradressen 0 und 1 ansprechen lässt und beim C64 unter anderem dazu genutzt wird, um in einzelnen Speicherbereichen zwischen RAM, ROM und dem I/O-Bereich durch Bank Switching umzuschalten.

Der Prozessor arbeitet mit einer Taktfrequenz von 0,985249 MHz in der PAL-Version und 1,022727 MHz in der NTSC-Version. Der Unterschied ergibt sich daraus, dass im C64 aus der Schwingungsfrequenz nur eines Quarz-Oszillators alle benötigten Frequenzen einfach abgeleitet werden und dass die Farbträgerfrequenzen der beiden Farbübertragungssysteme unterschiedliche Werte haben, die eingehalten werden müssen. In der NTSC-Version stehen so mehr Taktzyklen pro Rasterzeile in der Grafikausgabe zur Verfügung, und auch insgesamt ist die CPU etwas schneller. Dafür hat NTSC weniger Zeilen pro (Halb-)Bild, nur 262 im Vergleich zu 312 bei PAL. Daher müssen Programme, die den Rasterzeileninterrupt (s. u.) zur bildsynchronen Ablaufsteuerung verwenden, austauschbare Codeteile für beide C64-Versionen besitzen oder gleich in zwei verschiedenen Versionen vorliegen.

Der C64 verfügt über 64 KB RAM. Davon sind 38911 Bytes für BASIC-Programme nutzbar. Die Größe des Speichers war für die damalige Zeit üppig (der zwei Jahre ältere Vorgänger VC 20 hat nur 5 KB Arbeitsspeicher, wovon für die Programmiersprache BASIC lediglich 3584 Byte nutzbar sind). Zwei Bytes (0 und 1) sind nicht für das RAM nutzbar, hier befindet sich der Prozessorport des 6510.

Der C64 verfügt über 20 KB ROM. Etwa 9 KB davon enthalten in nahezu unveränderter Form den BASIC-V2-Interpreter des älteren Commodore VC 20 (erschienen 1980), der ursprünglich von der Firma Microsoft stammt. In weiteren knapp 7 KB ist ein Betriebssystem, der sogenannte Kernal, untergebracht, welcher die Tastatur, den Bildschirm, die Kassettenschnittstelle, die RS-232-Schnittstelle sowie eine serielle IEC-Schnittstelle (den CBM-Bus) zur Ansteuerung von Druckern, Diskettenlaufwerken usw. verwaltet. Auch dieses stammt ursprünglich von älteren Commodore-Maschinen und wurde an die veränderte Hardware des C64 angepasst. Die restlichen 4 KB enthalten zwei Zeichensätze à 256 Zeichen in 8×8-Matrixdarstellung für den Bildschirm. Die Zeichensätze entsprechen dem Commodore-eigenen PETSCII-Standard und enthalten deshalb keine deutschen Umlaute.

Um über verschiedene Versionen hinweg auf Maschinensprachenebene kompatibel zu bleiben, war ganz am Ende des ROM-Bereichs (also kurz vor codice_1) eine Sprungtabelle angelegt, über die man die wichtigsten Betriebssystemroutinen aufrufen konnte. Commodore behielt diese Sprungtabelle vom PET 2001 bis über den C64 hinaus bei. Die Kompatibilität von Anwendungssoftware hat sich dadurch nicht besonders gesteigert, weil viele Programmierer diese kompatible Methode des Aufrufs schlichtweg ignoriert haben und sie ohnehin nur für rein textbasierte Programme brauchbar war. – Beispiel: Der Aufruf codice_2 gibt auf jedem Commodore-8-Bit-Rechner den Inhalt des Akkumulators als Zeichen auf den Bildschirm aus.
Der Grafikchip des C64 ist ein MOS 6569/8565 (PAL) bzw. MOS 6567 (NTSC) und wird VIC (Video Interface Controller) genannt. Er bietet:

Da der VIC nur 14 Adressleitungen besitzt, kann er nur 16 KB des zur Verfügung stehenden Speichers auf einmal ansprechen. Die zwei fehlenden Adressbits steuert der zweite im C64 verbaute CIA6526-Chip bei. Diese vier Speicherseiten zu 16 KB verhalten sich nicht gleich – im Speicherbereich codice_3 bis codice_4 (bzw. codice_5 bis codice_6) wird vom VIC stets das Zeichengenerator-ROM ausgelesen. In diesen Bereichen können daher auch kein Bildschirmspeicher (Text oder Bitmap) und keine Spritedaten abgelegt werden. Umgekehrt muss in den beiden anderen Speicherseiten im Textmodus ein Zeichengenerator im RAM abgelegt werden.

Das Farb-RAM, das aus Sicht des Hauptprozessors an den Adressen codice_7 bis codice_8 eingeblendet werden kann, ist aus Timinggründen ein einzelner 1024×4-Bit-SRAM-Chip (µPD2114), der vier eigene Dateneingänge in den VIC besitzt. Das Farb-RAM muss daher nicht in den „normalen“ VIC-Adressraum eingeblendet werden. Genau genommen besitzt der C64 damit 66048 Byte RAM. Da die letzten 24 Adressen nicht für die Farbdarstellung gebraucht werden, kann man die dahinterliegenden Speicherzellen für Sonderzwecke nutzen.

Der VIC sorgt ebenfalls, wie damals für die Grafikhardware üblich, durch das regelmäßige Auslesen aller Speicherseiten für den nötigen Refresh der DRAM-Chips des C64.

Der C64 ist dank der Rasterzeileninterrupts und des Grafikchipdesigns recht flexibel im Bildaufbau.
Viele der hardwaretechnischen Einschränkungen können durch kreative Programmierung und Ausnutzung von vom Hersteller nicht explizit implementierten Nebeneffekten umgangen werden. So lassen sich beispielsweise verschiedene Darstellungsmodi mischen (z. B. obere Bildschirmhälfte Textdarstellung mit Scrolling, untere Bildschirmhälfte Grafik) und auch die acht Sprites mehrfach in verschiedenen Bildbereichen verwenden, so dass viele Spiele weitaus mehr als acht Sprites darstellen können. Durch Ausnutzung von undokumentierten Videochip-Eigenschaften ist auch die Verwendung von zusätzlichen Videomodi möglich, die die Beschränkungen in der Farbwahl und Auflösung teilweise aufheben. Auch der Bildschirmrahmen kann mit einigen Tricks zur Darstellung von Grafik benutzt werden.

Der Basic-Interpreter stellt keine Befehle zur Programmierung der hochauflösenden Grafik bereit, so dass deren Nutzung dem normalen Anwender verschlossen bleibt. Abhilfe schaffen kommerzielle Basic-Erweiterungen wie Simons’ Basic, s. u.

Klänge werden über den dreistimmig polyphonen Soundchip MOS Technology SID 6581 (buskompatibel mit der Prozessorfamilie 65xx) erzeugt, welcher dem C64 damals revolutionäre, weit über andere Heimcomputer hinausgehende Möglichkeiten zur Klangerzeugung verlieh. Spätere C64-Varianten enthielten den 8580.

Der SID besitzt drei universell einsetzbare monophone Stimmen mit einer in 65536 Stufen einstellbaren Grundfrequenz von 0 bis 4000 Hz und 48 dB Aussteuerung, die "gleichzeitig" in subtraktiver Synthese vier Schwingungsformen (Dreieck, Sägezahn, Rechteck in 4096 Stufen einstellbarer Pulsbreite, sowie Rauschen) erzeugen können. Die Lautstärke jeder Stimme kann einzeln mittels dreier programmierbarer ADSR-Hüllkurvengeneratoren mit exponentiellem Kurvenverlauf eingestellt werden. Weiterhin ist eine Synchronisierung von zwei oder allen drei Oszillatoren möglich. Ein Ringmodulator ergibt weitere Effekte. Eine der Stimmen kann außerdem wahlweise ausschließlich zur Modulation der anderen Stimmen verwendet werden.

Weiterhin besitzt der SID ein subtraktives Multimode-Filter (Tiefpass, Hochpass, Bandpass oder Notch Filter), durch das die internen Stimmen sowie eine über die Monitorbuchse des C64 zumischbare externe Quelle geleitet werden können.

Da die Lautstärke der Tonwiedergabe in 16 Stufen eingestellt werden konnte, benutzten schon bald einige Programme den Lautstärkesteller als D/A-Wandler, um Samples, zum Beispiel Sprache oder Schlagzeug, wiederzugeben. Bekannte Beispiele dafür sind das Spiel zum Film „Ghostbusters“ und das Musikspiel „To Be on Top“. Die Tonqualität war dabei allerdings nicht besonders gut, außerdem gab es eine Inkompatibilität zwischen den ursprünglichen und den späteren C64-Versionen: Der später verbaute SID II (MOS 8580) schaltete seinen Ausgang nur durch, wenn auf mindestens einer Stimme ein Ton abgespielt wurde. Dadurch verringerte sich zwar das Grundrauschen bei fehlender Tonwiedergabe, reine Samples ohne Hintergrundmusik wurden nur noch sehr leise abgespielt. Neuere Programme berücksichtigten diese Tatsache, Anpassungen für ältere Software gab es in der Regel nicht.

Durch geschicktes Mischen unterschiedlicher Samples war auf Softwareebene außerdem die Wiedergabe mehrerer Samples möglich; dies bedingte jedoch zwangsläufig eine Einschränkung der Wiedergabegenauigkeit (resolution) bzw. der Abspielrate (sample/playback rate), das heißt, die so erzeugten Töne waren weniger gut aufgelöst und „ungenauer“. Eine Reihe von bekannten Spielemusikprogrammierern bediente sich dieser Technik.

Neben der Audiowiedergabe besaß der SID noch zwei Analogeingänge mit niedriger Abtastrate, die im C64 zum Anschluss von Paddles oder einer speziellen Maus mit Analogausgang genutzt wurden.

Zum Ende der C64-Ära wurden in Bastlerkreisen Methoden entwickelt, um den C64 stereofähig zu machen. Dazu wurde ein zweiter SID eingebaut und zur Ansteuerung die Tatsache ausgenutzt, dass der Adressbereich des SID mehrfach gespiegelt ist. Durch geeignete Adress-Selektion konnten so beide SIDs unabhängig voneinander angesteuert werden. Diese Lösung wurde als Bauanleitung in der "64'er" beschrieben, kam jedoch nie kommerziell auf den Markt.

Der C64 bietet mehrere Schnittstellen und war daher bei Hardware-Bastlern beliebt (von links nach rechts, von der Rückseite aus gesehen):

Für den C64 stand eine große Auswahl an Peripheriegeräten zur Verfügung.

Das Datasette (auch Datassette) genannte Bandlaufwerk war die günstigste Lösung für Datenspeicherung am C64. Es benutzt normale Kompaktkassetten. Meist war Software auf Kassetten günstiger als entsprechende Diskettenversionen. Anders als in Deutschland, wo das Diskettenlaufwerk (trotz höherer Anschaffungskosten) sehr verbreitet war, war die Datassette in Großbritannien das dominierende Datengerät. Von Commodore gab es das Datassetten-Laufwerk VC-1530, welches mit dem C64 kompatibel war. Auch andere Hersteller boten Datasetten-Laufwerke für den C64 und den C128 an. Lade- und Speichervorgänge geschehen sehr langsam und sind durch notwendige Spulvorgänge umständlich. Schnelllader wie Turbo Tape verringern die Ladezeiten etwa um den Faktor 10.

Dieses Laufwerk vom Typ VC1541 war das Standardlaufwerk für den C64 und wurde vom Großteil der Benutzer verwendet. Es benutzt die damals sehr weit verbreiteten 5¼-Zoll-Disketten mit doppelter Aufzeichnungsdichte "(Double Density)." Das Laufwerk arbeitet einseitig und bietet etwa 165 kB Speicherkapazität pro Diskettenseite, angegeben werden jedoch die zur Verfügung stehenden „Blöcke“, von denen es standardmäßig 664 gibt. Um die Rückseite beschreiben zu können, muss die Diskette dem Laufwerk entnommen und gewendet werden. Dafür gab es beidseitig beschreibbare Disketten mit Aussparungen für die Schreibfreigabe auf beiden Seiten. Jedoch konnte man auch die preisgünstigeren, offiziell nur einseitig beschreibbaren Disketten auf der Rückseite beschreiben. Dazu musste jedoch vorher seitlich eine zweite Kerbe ausgestanzt werden, beispielsweise mittels eines Diskettenlochers oder mit einem Teppichmesser. Die Daten werden von den Laufwerken als schreibgeschützt erkannt, wenn diese Kerbe überklebt wird. Entsprechende Aufkleber lagen den Disketten bei.

Ältere Versionen der VC1541 hatten keine Möglichkeit zu erkennen, wann der Schreib–Lese-Kopf am unteren Ende („Spur 0“) angekommen ist, und hatten deshalb eine mechanische Sperre. Das führte zu dem bekannten mechanischen „Rattern“ des Laufwerks bei der Formatierung einer Diskette, da der Schreib-Lese-Kopf so bis zu fünfmal an den Anschlag fuhr – dadurch konnte er verstellt werden. Neuere Versionen hatten eine Lichtschranke, um das Problem zu lösen; da jedoch das ROM des Laufwerks geändert wurde, führte das teilweise zu Inkompatibilitäten mit Schnellladeprogrammen und Kopierschutzmechanismen.

Das Laufwerk war ein eigenständiger Computer mit eigenem Prozessor und Speicher. Anders als praktisch alle anderen Firmen hatte Commodore das DOS als ROM im Laufwerk selbst realisiert, anstatt es in den Speicher des Computers zu laden. Es gab Programme, die Teile der Rechenarbeit auf das Laufwerk auslagerten und somit eine Art Parallelprogrammierung ermöglichten; wegen des kleinen Speichers des Laufwerks war das nur sehr eingeschränkt nützlich. Ebenfalls gab es Jux-Programme, die durch kreative Programmierung des für die Schreib-Lese-Kopfbewegung zuständigen Schrittmotors sogar Musik mit dem Laufwerk erzeugten.

Von dem Laufwerk wurden drei Haupt- und viele Untervarianten hergestellt. Fremdhersteller boten Klone an, die zwar preisgünstiger, aber wegen des aus Urheberrechtsgründen abweichenden ROMs meist nicht vollständig kompatibel waren.

Die Geschwindigkeit der Diskettenoperationen war aufgrund des geringen Speicherausbaus der Laufwerke, der seriellen Schnittstelle sowie umständlicher Programmierung der DOS-Funktionen – das 1541-DOS wurde aus dem der Doppelprozessor-Doppelfloppies CBM 8050 abgeleitet – sehr langsam, so dass viele verschiedene Turbolader als Software- oder als Hardwarebeschleuniger entwickelt wurden.

Diese Beschleuniger schrieben als erstes eigene in Assembler entwickelte Routinen in den Speicher des Laufwerks, die anschließend zusammen mit im Computer ablaufenden Routinen den Datentransfer realisierten.

Das Laufwerk vom Typ VC1581 fristete im Zusammenhang mit dem C64 aufgrund seiner Inkompatibilität zur VC1541 nur ein Schattendasein – trotz seines gegenüber der VC1541 erheblich gesteigerten Speichervermögens von 800 kB auf 3½-Zoll-DD-Disketten. Wegen Kopierschutzmaßnahmen erforderten sehr viele Programme das VC1541-Laufwerk, so dass dem Modell 1581 kein Erfolg beschert war. Wie die VC1541 war auch dieses Laufwerk technisch gesehen ein eigenständiger Computer.

Mäuse spielten als Eingabegeräte beim C64 eine eher untergeordnete Rolle, da sie sich erst Jahre nach ihrer Einführung etablierten. Es gab nur wenige Programme, die sie unterstützten bzw. für Mausbenutzung (anstatt Joystick) ausgelegt waren, so z. B. das grafikorientierte Betriebssystem GEOS, Hi-Eddi und Printfox.

Neben der Tastatur sind Joysticks die wichtigsten Eingabegeräte am C64, denn fast alle Spiele und viele Anwendungen lassen sich nur mit ihnen steuern. Beim C64 wird der damals recht verbreitete Atari-Standard für Joysticks unterstützt, so dass die gleichen Joysticks wie an sehr vielen anderen Rechnern verwendet werden konnten. Zwar stellte Commodore eigene Joysticks her, beliebter und verbreiteter waren jedoch Spectravideos QuickShot-Joysticks, Joysticks von QuickJoy sowie – aufgrund seiner Robustheit – der Competition Pro.

Grafiktablett für den C64, das für das Grafikprogramm KoalaPainter entwickelt worden war, aber auch von einigen anderen Programmen genutzt wurde.

Lichtgriffel sind „Stifte“, die zum Zeichnen direkt auf dem Monitor verwendet werden. Wie auch Paddles hatten sie auf dem C64 kaum eine Bedeutung.

Eine Lightgun ist von der Funktionsweise ähnlich wie die Lichtgriffel, jedoch meist in der Form einer Pistole und für Spiele gedacht. Auch dieses Eingabegerät war beim C64 kaum von Bedeutung.

Paddles sind Eingabegeräte, die vor allem in den 1970er-Jahren bei vielen Videospielen verbreitet waren und so auch ihren Weg zum C64 fanden. Bis auf wenige der frühen C64-Spiele und einige spätere Ausnahmen wie Arkanoid hatten Paddles aber kaum eine Bedeutung auf dem C64.

Von der Firma Scanntronik waren ein Schwarz-Weiß-Scanner erhältlich, der auf den Druckkopf geeigneter Nadeldrucker aufgesteckt wurde und das zu scannende Bild zeilenweise abtastete, während es von der Druckerwalze transportiert wurde, sowie ein Handscanner.

Commodore verkaufte seine eigenen Druckermodelle MPS 801, -802, -803 und -1230 (hauptsächlich Seikosha-OEM, z. B. der baugleiche GP 500 VC). Diese Matrixdrucker können im Textmodus aufgrund technischer Limitierungen (Unihammer-Technik beim MPS 801/803) bzw. der Tatsache, dass nur 8 der 9 verfügbaren Nadeln angesteuert wurden (MPS 802), keine echten Unterlängen drucken. Für dieses Problem gab es aber einige Softwarelösungen im Angebot. Fremdhersteller produzierten einige spezielle Drucker für Commodore-Rechner, die wie ein Diskettenlaufwerk am seriellen Bus des C64 angeschlossen werden, z. B. der sehr beliebte Star LC10. Weit verbreitet waren zwei weitere Lösungen: Man konnte gängige Drucker mit Centronics-Schnittstelle über einen speziellen Konverter an den seriellen IEC-Bus des C64 anschließen und dann wie Commodore-Drucker ansteuern, oder man konnte sie mittels eines einfachen Kabels mit dem Userport verbinden, brauchte dann aber eine Software, die eine spezielle Unterstützung für diesen Anschlussmodus bot. In einigen Floppy-Schnellladern (z. B. SpeedDOS) waren solche Routinen bereits integriert. Es gab elektrische Schreibmaschinen, die von diesen Schnittstellen angesteuert und als Drucker verwendet werden konnten. Auf Treiberebene existieren zwei Standards, der MPS-801/803-Modus sowie der Epson FX-80-Modus (ESC-P) für Neun-Nadel-Drucker. Der Standard NEC-P6 wurde nur selten unterstützt, da die meisten NEC-P6-kompatiblen Drucker auch FX-80-kompatibel sind, wenngleich dann die Ausgabe lediglich mit neun Nadeln erfolgte. Die überwiegende Mehrheit der damaligen Drucker waren Nadeldrucker mit 7, 8, 9 oder 24 Nadeln, wobei 24-Nadel-Drucker aufgrund ihres hohen Preises eher selten waren und nur mit Spezialsoftware eingesetzt werden konnten. Im untersten Preissegment fanden sich einige Thermodrucker, die aber wegen ihres schlechten Druckbildes, dem notwendigen teuren Thermopapier und der schlechten Haltbarkeit des Drucks keine sehr weite Verbreitung fanden. Tintenstrahldrucker, Thermotransferdrucker und Laserdrucker waren zu dieser Zeit noch sehr teuer und somit selten bei Heimcomputeranwendern zu finden.
Mit Hilfe des HF-Ausgangs konnte der C64 über die Antennenbuchse an jeden Fernseher angeschlossen werden, so dass kein zusätzlicher Monitor für den Betrieb des Rechners nötig war. Die Bildqualität war aufgrund der Umsetzung naturgemäß bescheiden.Für den C64 und andere damalige 8-Bit-Heimcomputer gab es eine recht große Auswahl an Video-Monitoren. Hier sind vor allem der Commodore 1701 und der Philips CM8833, mit Auflösungen von 300×300 Pixeln, sowie die kompatiblen Monitore der Amiga-Baureihe zu nennen, die aufgrund ihrer feineren Lochmaske ein schärferes Bild lieferten.

Weniger verbreitet war der Plotter Commodore VC-1520, ein einfacher Stiftplotter für Endlos-Rollenpapier. Die Papierrolle war ca. 10 cm breit. Das Gerät bot die Möglichkeit der einfachen Textausgabe in Rot, Grün, Blau und Schwarz. Außerdem konnten Zeichnungen in den gleichen Farben ausgegeben werden.

Damals war der Betrieb von nicht durch die Deutsche Bundespost zertifizierten – und das waren die meisten – Modems am deutschen Telefonnetz illegal, so dass man anstelle dieser Modems sogenannte Akustikkoppler verwenden musste. Die Übertragung war damit allerdings sehr langsam, typischerweise 300 bis 1200 Bit/s und zudem sehr fehleranfällig, da Nebengeräusche oft zu Übertragungsfehlern führten.

Es gab spezielle C64-Modems, die an den Userport des C64 angeschlossen wurden, sowie andere, die mit Hilfe einer (gegebenenfalls am Expansionsport anzuschließenden) RS-232-Schnittstelle am C64 betrieben werden konnten.

2003 kam von "individual Computers" ein Netzwerkadapter für den C64 unter der Bezeichnung "RR-Net" auf den Markt. Für den Betrieb benötigt man allerdings das "Retro Replay Cartridge" oder das "MMC64", welches ebenfalls von individual Computers herausgebracht wurde.

Ein auf Flashspeicher basierendes Modul, das viele ROM-basierende Module ersetzen kann. Die Grundidee war dabei, die „großen“ Ocean-Spielmodule ersetzbar zu machen, indem der Inhalt eines solchen Moduls in den Flashspeicher geschrieben wird und sich das EasyFlash dann wie ein Original-„Ocean-Modul“ verhält. In der Entwicklungsphase wurden dann noch weitere Modulformate implementiert, so dass ein EasyFlash fast alle Spielmodultypen korrekt emulieren kann. Das EasyFlash verfügt über einen 1 MB großen Flashspeicher, der mittels des C64 und Diskettenlaufwerk oder größere Massenspeicher beschrieben wird. Daraufhin wurde Software (EasyLoader) entwickelt, die es ermöglicht, beliebige Programme oder auch Modulkopien für den C64 auf den Flashspeicher zu schreiben und über ein Startmenü auszuwählen. In Betrachtung dieser Möglichkeiten wurden mittlerweile viele Spieletitel auf das EasyFlash umgesetzt, so dass die Diskettenladezeiten entfallen und sogar die Möglichkeit gegeben ist, die Spielstände auf dem EasyFlash zu speichern. Die Umsetzung von „Prince of Persia“ für den C64 basiert auf dem Easyflash.
Diese Karten erlaubten den direkten Zugriff auf ein oder mehrere EPROMs zum Aufruf fest gespeicherter Programme und waren meist elektronisch umschaltbar.
In den 1990er-Jahren entwickelte die Firma CMD neue Diskettenlaufwerke mit einer Speicherkapazität von bis zu 2880 kB. In den späten 1990er-Jahren entwickelten technisch versierte Bastler eine IDE-Schnittstelle. Sowohl an der IDE-Schnittstelle als auch an der SCSI-Festplatte lassen sich weitere Geräte wie CD-ROM-Laufwerke oder Compact-Flash-Karten betreiben. Die beiden Laufwerke CBM D9060 und CBM D9090 waren die einzigen IEEE-488 Festplatten, welche von Commodore für die PET und CBM 8-Bit Computer hergestellt wurden.

Das MMC64 ist ein Steckmodul für den C64, das es ermöglicht, mit dem C64 MMC- und SD-Speicherkarten zu lesen und zu beschreiben. Programme können direkt von der Speicherkarte geladen und ausgeführt werden. Jedoch können Programme nicht vom Speicher des C64 auf die SD-Karte (oder MM-Karte) geschrieben werden. Damit kann für Selbstprogrammierer die MMC64 die Commodore Floppy 1541 als Speichermedium nicht ersetzen. Laden und danach Speichern funktioniert nur mit einer alten 1541. Das MMC64 ist daher eher für die Ausführung (Wiedergabe) von fertigen Spielen (oder auch eigenen Programmen) gedacht. Das Laden eines solchen Spiels wird in wenigen (Milli-)Sekunden bewerkstelligt. Darüber hinaus existieren zahlreiche Plugins, die es beispielsweise ermöglichen, sogenannte Diskettenimages von Disketten zu erzeugen oder diese auf Diskette zu schreiben (immer nur als ganze Image-Dateien).

Das MP3@64 ist ein MP3-Modul für das MMC64.

Sie waren sehr verbreitet. Das hat vor allem mit der geringen Ladegeschwindigkeit der 1541 zu tun, die sich per Software auf die 10- bis 20-fache Geschwindigkeit steigern ließ. Den Anfang machten einfache Schnelllader-Cartridges, schnell kamen weitere Funktionen dazu, so dass am Ende Cartridges wie "The Final Cartridge 3," "Hypra Load II" oder "Action Replay" mit einer großen Anzahl von Funktionen aufwarteten. Neben dem Schnelllader sind meist noch diverse BASIC-Erweiterungen, Funktionstastenbelegungen, Freezerfunktionen, Druckfunktionen, Maschinensprachemonitor und einiges mehr vorhanden. Auch heute wird noch ein solches Cartridge hergestellt und verkauft: das "MMC Replay." Es ist wie sein (mittlerweile eingestellter) Vorgänger Retro Replay Cartridge weitgehend "Action-Replay"-kompatibel und um Fehler bereinigt. Das Modul verwendet höher integrierte und modernere Bauteile und bietet mehr Speicher, mehr Funktionen und hat die Möglichkeit des ROM-Updates, zusätzlich wurde die Funktionalität des (ebenfalls eingestellten) MMC64 integriert.

Um den C64 zum Steuern von elektronischer Hardware zu benutzen, existierten diverse Relais-Karten. Diese wurden meist an den Userport angeschlossen und erlaubten so die Ansteuerung von acht Relais.

Ein Steckmodul, das es ermöglichte, durch Drücken eines Tastenschalters den C64 zu resetten. Die überwiegende Anzahl der in Maschinencode geschriebenen Programme konnte nur verlassen werden, indem der Computer aus- und wieder eingeschaltet wurde. Ebenso musste bei Abstürzen vorgegangen werden, nur selten funktionierte das eigentlich hierfür vorgesehene gleichzeitige Drücken von und . Die übermäßige Benutzung des Ein- und Ausschalters war nicht nur lästig, sondern konnte auch zu Defekten führen.

Es gab eine Steckkarte für den C64, "The Final Chesscard," die einen vollständigen Computer mit einer Schachspielsoftware (Schachcomputer) enthielt, der C64 übernahm dabei die Darstellung des Spiels und die Eingabe der Züge.

Es kam vor dass für bestimmte Anwendungen die 64 KB Hauptspeicher des C64 nicht ausreichend waren, so dass zahlreiche Speichererweiterungen hergestellt wurden, die meistens an den Expansionsport angeschlossen wurden. Von Commodore selbst vertrieben wurde die REU (RAM Expansion Unit CBM1700, CBM1764 und CBM1750). Alle Speichererweiterungen für den C64 konnten nur von Software ausgenutzt werden, die speziell darauf ausgelegt war; das schloss vor allem die meisten Spiele aus. Weitere, meist nur von GEOS oder wenigen Spezialanwendungen (z. B. "Pagefox") unterstützte Speichererweiterungen spielten nur eine untergeordnete Rolle.

Als Bausatz wurden sogenannte Teleclubdecoder vertrieben. Damit konnte die recht einfache Verschlüsselung des Pay-TV-Senders Teleclub aufgehoben werden.

Es gab einige wenige Versuche, dem C64 mit Hilfe eines schnelleren Prozessors zu mehr Leistung zu verhelfen. Als erstes kam die Erweiterung "Turbo Process" von "Roßmöller" auf den Markt, die einen 65C02-Prozessor mit 4 MHz hatte. Der direkte Nachfolger dieser Karte war die "Flash 8," mit einer 8 MHz schnellen 65816-CPU. Beide Karten sind teilweise inkompatibel zu existierender Software und überdies im Betrieb sehr instabil, so dass sie lediglich ein Nischendasein fristeten. Erst der SuperCPU, einer Beschleunigerkarte basierend auf einem mit 20 MHz getakteten 65816-Prozessor, war ein gewisser Erfolg beschieden. Eine Prozessorkarte mit einem Z80, die den C64 zu einem CP/M-Computer werden ließ, wurde bei der Markteinführung des C64 stark propagiert, erreichte aber wegen der sehr geringen CPU-Geschwindigkeit und der schlechten Kompatibilität zu anderen CP/M-Rechnern keine große Verbreitung. Insbesondere verlangten fast alle kommerziellen CP/M-Programme eine Zeilenlänge von 80 Zeichen, was der C64 von Haus aus nicht bieten konnte.

Es gab Erweiterungen, mit deren Hilfe der C64 die Videotexttafeln der Fernsehsender auslesen konnte.

Intern gab es 16 verschiedene Versionen des C64-Mainboards.

Der C64 wurde anfangs in einer beigefarbenen „Brotkasten“-Gehäuseform, zunächst mit orangefarbenen, dann mit dunkelbraunen Funktionstasten produziert. Urversionen mit den orangen Funktionstasten und dem silbernen Commodore-Typenschild gehören zu den Raritäten. Ein Großteil der deutschen Produktion wurde im Commodore-Werk in Braunschweig montiert.

Der Educator 64 ist eine spezielle Version des C64 im PET-Gehäuse, er ist vor allem für Schulen gedacht. Das Modell ist auch als „4064“ oder „PET 64“ bekannt. Diese Version konnte sehr preisgünstig angeboten werden, da instand gesetzte Hardware von reklamierten C64 verwendet wurde.

Der SX-64/DX-64 ist eine tragbare Version des C64 mit einem (SX-64) oder zwei (DX-64) eingebauten 1541-kompatiblen Diskettenlaufwerken und eingebautem 5-Zoll-Farbmonitor. Der Rechner war nicht hundertprozentig kompatibel, man konnte aber C64-ROMs anstelle der leicht geänderten SX-64-ROMs einsetzen. Aufgrund niedriger Absatzzahlen wurden jedoch nur wenige Geräte hergestellt: Vom SX-64 etwa 9000 Stück, vom DX-64 noch weniger.

Die „Gold Edition“ des C64 besaß ein goldfarbenes Brotkasten-Gehäuse und war auf einer Acryl-Platte mit einem Emblem montiert. Anlass war der einmillionste verkaufte C64 in Deutschland. Produziert wurde die Kleinserie 1986 in sehr geringer Stückzahl von etwa 400 Stück, andere Quellen geben 1000 Stück an. Bei einer Feier am 5. Dezember 1986 im Münchner BMW-Museum wurde dieser C64 an wichtige Personen innerhalb des Unternehmens sowie Journalisten und Händler vergeben, die maßgeblich zum Erfolg des C64 beigetragen hatten. Die speziell in Braunschweig gefertigte „Gold Edition“ wurde damals von Hand ab Nummer 1000000 beschriftet. Dieses Gerät ist sehr selten und ein begehrtes Sammlerstück.

Das Modell C64C hat ein neues, flacheres Gehäuse und trägt die Aufschrift „Personal Computer“. Zudem ist es mit leicht überarbeiteter, kostenreduzierter Hardware (kleinere Hauptplatine) ausgestattet. In Deutschland wird der C64C oft als „C64-II“ bezeichnet.

Der C64G besitzt wieder die alte Gehäuseform („Brotkasten“), diesmal grau/beige mit heller Tastatur und kostenreduzierter Hauptplatine. Die Grafikzeichen der Tastatur sind auf der Oberseite statt auf der Stirnseite der Tasten abgebildet. Das G in der Bezeichnung steht für Germany, da in Deutschland die Brotkastenform sehr beliebt war und man dem Wunsch der Kunden mit diesem Modell nachkommen wollte.

Der Aldi-C64 ist dem C64G ähnlich. Er war nur in Deutschland erhältlich und der Vertrieb erfolgte über Discounter (zum Beispiel die Aldi-Gruppe). Durch das Wegfallen des 12-V-Spannungsreglers bei den 250469-Boards wurde in der Zeitschrift "64er" fälschlicherweise geschrieben, der neue SID 8580 würde ausschließlich 5 Volt Gleichspannung benötigen. Die 9 Volt Wechselspannung würde daher nicht mehr benötigt und am Userport fehlen. Diese Angaben waren falsch. Der neue SID 8580 benötigte zusätzlich zu den 5 Volt Gleichspannung auch 9 Volt Gleichspannung, die aus den 9 Volt Wechselspannung erzeugt wurde. Auch für das Taktsignal (50 Hz) der beiden CIA-Echtzeituhren und für die Motoransteuerung der Datasette wurde die 9 Volt Wechselspannung benötigt. Die neuen flacheren Gehäuse und hochintegrierten Platinen waren bei Bastlern unbeliebt, da sie mit internen Erweiterungen von Fremdherstellern nicht mehr kompatibel waren.

Der Commodore 64 GS (GS = Games System) ist ein 1987 als Spielekonsole herausgebrachter C64. Es war der Versuch die Marke Commodore auch auf den Konsolen-Markt zu etablieren. Es besaß keine Tastatur und kein Anschluss für Datasette und Diskettenlaufwerke. Spiele konnten nur über Module geladen werden. Der Modulschacht befand sich auf der Oberseite des Gerätes. Der C64 GS war genauso teuer wie ein vollwertiger C64, weswegen der C64 GS floppte. Offiziell wurde das Modell C64GS nur in England vertrieben.

Der Vorgänger des C64 war der 1981 zur Marktreife gebrachte, farbfähige VC 20, von dem erstmals in der Geschichte der Mikrocomputer über eine Million Exemplare verkauft wurden. Als offizieller Nachfolger des C64 wurde 1985 der Commodore 128 auf den Markt gebracht, welcher neben dem eigenen C128-Modus über einen C64- sowie einen CP/M-Modus verfügte. Die Produktion des Nachfolgemodells wurde jedoch wegen nicht zufriedenstellender Verkaufszahlen und hoher Produktionskosten schon 1989 und damit fünf Jahre vor dem Produktionsende des C64 eingestellt.

Die ab 1984 gefertigten Modelle der Commodore-264-Serie, der C16, C116 und Plus/4, konnten sich aufgrund ihrer Inkompatibilität zum beliebten C64 sowie bestimmter technischer Defizite auf dem Markt ebenfalls nicht durchsetzen. Noch im gleichen Jahr erfolgte die Produktionseinstellung und verbliebene Geräte wurden für Schleuderpreise verkauft. Als späten Nachfolger des C64 entwickelte Commodore den Commodore 65, der jedoch nie in Serie produziert wurde, da man dem sehr erfolgreichen Amiga 500 mit dem C65 keine Konkurrenz machen wollte.

Die als Tastaturcomputer ausgeführten Einsteigermodelle der von Commodore hergestellten Amiga-Reihe, insbesondere der Amiga 500, erfreuten sich Ende der 1980er-Jahre einer ähnlichen Beliebtheit als leistungsfähige Spielecomputer wie der C64, die jedoch den C64 nie vom Markt verdrängen konnten. Technisch war der Amiga dem C64 überlegen, er besaß allerdings auch eine vollkommen abweichende und modernere Hardware.

Commodore International musste am 29. April 1994 Insolvenz anmelden. Mit dem Hersteller Commodore verschwand gleichzeitig auch der letzte Heimcomputer C64 vom Markt, dessen Produktionseinstellung eigentlich erst für 1995 vorgesehen war.

Während der 8-Bit-Ära gab es vom C64, anders als bei vielen Konkurrenzmodellen, keine legalen oder illegalen Nachbauten durch andere Firmen. Die vielen speziellen Chips im C64, die nur von Commodore selbst beziehungsweise von deren Tochter MOS Technology hergestellt und die nicht an potenzielle Nachbauer verkauft wurden, verhinderten dies.

Im Jahre 1998 erschien von der belgischen Firma "Web Computers International" der "Web.it", ein PC-kompatibler Rechner mit Microsoft Windows 3.1 und vorinstalliertem C64-Emulator. Herz war ein AMD-Mikroprozessor auf 486-Basis (66 MHz), dazu kamen 32 MiByte RAM, 32 MiByte ROM. Der Web.it war zudem mit einem Webbrowser (Netscape Navigator), einem Textverarbeitungsprogramm (Lotus AmiPro) und einer Tabellenkalkulation (Lotus 1-2-3) ausgestattet. Wie beim Original-C64 befand sich der gesamte Rechner im selben Gehäuse wie die Tastatur. Die Produktion des erfolglosen Modells wurde relativ schnell wieder eingestellt. Das mag unter anderem damit zusammengehangen haben, dass das Gerät nicht annähernd die notwendige Prozessorgeschwindigkeit aufwies, um einen C64 in Echtzeit zu emulieren.

Jeri Ellsworth und Individual Computers entwickelten den C-One oder auch "Commodore One" als Nachbau des C64 und bildeten die Hardware mittels FPGAs nach. Erste Platinen wurden 2003 ausgeliefert.

Ende 2004 brachte die englische Firma "The Toy:Lobster Company" den C64 Stick – auch als C64 DTV ("Direct To TV") bekannt – heraus, der auch in Deutschland erschien. Der Entwurf stammt ebenfalls von Jeri Ellsworth, es handelt sich im Wesentlichen um einen auf das Notwendigste reduzierten C-One. Es ist ein C64-Nachbau in Form des Joysticks Competition Pro mit 30 eingebauten Spielen (darunter unter anderem Summer Games, California Games sowie Pitstop, Super Cycle und Uridium). Der Anschluss erfolgt direkt an das Fernsehgerät. Begabte im Löten und technisch Bewanderte können den Joystick um weitere Joystickports sowie um PS/2-Port für Tastatur, IEC-Port für Drucker und Diskettenlaufwerke sowie Buchse für Stromanschluss erweitern. Es existieren NTSC- (seit 12/2004) und PAL-Versionen (seit 8/2005).

Im August 2010 veröffentlichte "Commodore USA" die Nachricht, die weltweiten Lizenzrechte für bisherige Commodore-Marken erworben zu haben, insbesondere für den C64 und den Amiga-Computer. Im Dezember 2010 wurde ein "Commodore 64" genanntes PC-System im originalgetreuen Retro-Gehäuse angekündigt. Basis ist ein Mainboard mit einem Intel Atom D525 DualCore-Chip, nVidia ION2 Grafik, USB-Ports, Kartenleser sowie optional DVD- oder BluRay-Laufwerk. Der Rechner wurde mit dem Betriebssystem Ubuntu Version 10.10 ausgeliefert. Später erhielt es mit Commodore OS ein eigenes Betriebssystem und einen integrierten C64-Emulator.

Der Chameleon 64 ist ein von "Individual Computers" entwickeltes Modul, das 2013 erschien. Es enthält unter anderem einen VGA-Port, PS/2-Anschlüsse für Maus und Tastatur und einen Slot für SD-Karten. Grundsätzlich stehen zwei Betriebsmodi zur Verfügung:

Wird das Modul am C64 betrieben, so bietet es eine VGA-Ausgabe, VC-1541-kompatible Floppy-Emulation von zwei Diskettenlaufwerken, die Emulation von REU, GeoRAM und diverser Anwendungs- und Spielmodule. Im autarken Betrieb stehen zusätzlich die Funktionen eines mittels FPGA umgesetzten C64 zur Verfügung. Das Modul bietet zusätzlich einen Uhrenport zum Anschluss einer Netzwerkkarte vom Typ RR-Net Mk2 oder Mk3, die im Modulgehäuse untergebracht werden kann.

Erstes Modell

Am 1. April 2014 kündigte individual Computers an, neue C64-Mainboards unter dem Namen "C64 reloaded" zu produzieren. Beim Platinenentwurf wurde sich zum größten Teil an den Originalschaltplan mit der Commodore-Nummer 250466 gehalten. Jedoch gibt es auch Abweichungen vom Schaltplan. So wurden Nullkraftsockel verbaut und eine 12V DC-DC Wandlertechnik hielt Einzug. Statt eines TV-Modulators wurde ein S-Video-Ausgang und eine 3,5mm Audio-Klinkenbuchse verbaut. Der C64 reloaded kann ohne Lötarbeiten von PAL- auf NTSC-Videonorm umgeschaltet werden. Der C64 Reloaded benötigt zum Betrieb zusätzlich originale Chips, die aus defekten C64-Computern entnommen werden können. Dieses Modell wurde noch ohne den Markennamen "Commodore" vertrieben. Der Verkaufsstart des C64 reloaded war am 20. Mai 2015. Diese Boards waren schnell ausverkauft.

Zweites Modell

Das "C64 reloaded MK2" ist das erste Board der Reihe welches unter dem Markennamen Commodore vertrieben wird. Im Gegensatz zum ersten Modell erkennt das MK2 die installierten Chipversionen automatisch und konfiguriert sich von selbst entsprechend. Der Verkaufsstart ist für den 21. November 2017 anvisiert.

2017 wurde bekannt dass die britische "Retro Games Ltd." und die österreichische Koch Media eine voll lizenzierte Mini-Version des C64 unter den Namen "TheC64 Mini" Anfang 2018 auf den Markt bringen wird. Das Gerät selbst lehnt sich an das Design des C64 an, ist aber nur halb so groß. Die Tastatur des "TheC64 Mini" ist eine Attrappe. Es verfügt über einen HDMI-Port für moderne Fernsehgeräte und Monitore. Der mitgelieferte Joystick wird über einen USB-Port mit dem Gerät verbunden. Auch eine USB-Tastatur lässt sich an dem Gerät anschließen, somit ist es möglich eigene Basic-Programme auf dem "TheC64 mini" zu schreiben. Das Gerät wird mit 64 vorinstallierten Spielen ausgeliefert.

Im August 2016 wurde bekannt, dass "individual Computers" (icomp) neben der Lizenz für den Markennamen Commodore auch die originalen Gussformen des C64C-Gehäuses erworben hat und mit diesen neue Gehäuse produzieren will. Am 22. August 2017 wurde das Gehäuse auf der Gamescom der Öffentlichkeit präsentiert und bereits verkauft; der reguläre Verkauf startete am 1. September 2017. Die neuen Gehäuse sind dabei in den vier Farbgebungen Original Beige, Classic Bread Bin, SX-64 Style und Black Edition verfügbar. Da fast alle C64-Platinen vom Aufbau identisch sind und es nur kleinere Abweichungen von der C64C Platine gibt, lassen sich auch andere C64-Versionen in dem Gehäuse einbauen. Dies trifft auch auf die Modelle des "C64 reloaded" zu.

Obwohl der C64 oft als „Spielcomputer“ und „Daddelkiste“ bezeichnet wurde, da der überwiegende Teil der Software Spiele waren, wurden für das Gerät – auch wegen seiner für die damalige Zeit gehobenen Hardware-Eigenschaften – auch viele „ernsthafte“ Programme produziert. Neben Office-Programmen wie der Textverarbeitung Vizawrite oder Textomat und den Tabellenkalkulationen Microsoft Multiplan und SuperCalc gab es für alle erdenklichen Anwendungen eine Vielzahl von Programmen, von denen hier stellvertretend nur einige aus dem deutschen Raum genannt seien: Für grafische Anwendungen waren Programme wie "Hi-Eddi" (für HiRes-Grafik) von Hans Haberl, "Amica Paint" von Oliver Stiller für Multicolor-Grafiken und "GIGA-CAD" von Stefan Vilsmeier für 3D-Modelle konzipiert. Ebenfalls von Hans Haberl und veröffentlicht von Scanntronik waren die Desktop-Publishing-Programme Printfox und Pagefox. Letzteres wurde als Steckmodul entwickelt und enthielt eine zusätzliche Speichererweiterung, um Zeichensätze, Grafiken und Text für eine ganze DIN-A4-Seite im Speicher halten zu können. Dabei standen alle üblichen Layoutfunktionen zur Verfügung, inklusive Spezialfunktionen wie Kerning.

Auch etliche Lernprogramme wurden für den C64 produziert, wenngleich er kein typischer Rechner war, der im Schulunterricht zum Einsatz kam. Hier waren besonders der Apple II und seine Klone stark verbreitet.

Neben Lernprogrammen wie Vokabeltrainern, Mathekursen und Programmen zum Erlernen des Chemielernstoffes wurden auch Hardware-Erweiterungen angeboten, mit denen Schüler zum Beispiel mit der Fischertechnik-Schnittstelle 30562 für den C64/VC20 die Grundzüge der Robotik erlernen konnten. Der C64 konnte auch für Lern- und Forschungszwecke genutzt werden. So trat das Gerät in den 1980er-Jahren bei vielen Beiträgen der Jugend-forscht-Wettbewerbe als Bestandteil der Versuchsanordnungen in Erscheinung.

Auch in der Fliegerei wurden Programme für den C64 eingesetzt. US-Piloten konnten beispielsweise Flüge nach Instrumentenflugregeln (IFR) mit dem "Flight-Simulator II" von Bruce Artwick machen, die für die Verlängerung der Pilotenlizenz angerechnet wurden. Das deutsche Pendant dazu war der "Flight-Teacher" von Uwe Schwesig, der eine Einführung in die Fliegerei bot.

Im Jahr 1986 wurde das Betriebssystem GEOS (Graphic Environment Operating System) mit grafischer Oberfläche (GUI) für den C64 angeboten. Es wurde in mehreren Versionen veröffentlicht und enthielt sehr viele Anwendungsprogramme. Diese grafische Oberfläche erweiterte den C64 in seiner Anwendungsbreite stark. Das war notwendig geworden, weil ab Mitte der 1980er-Jahre grafische Oberflächen immer häufiger als Serienausstattung bei Heimcomputern zum Einsatz kamen, so zum Beispiel beim Commodore Amiga, dem Apple Macintosh oder beim Atari ST. GEOS wird auf unterschiedlichen Plattformen bis heute (Stand 2005) gepflegt und erweitert. Allerdings ist es sehr ressourcenaufwendig, so dass sich im Besitz des Anwenderkreises von GEOS auch am häufigsten moderne Hardware wie große Speichererweiterungen, Super-CPUs oder Festplatten befinden.

Darüber hinaus wurde für den C64 ein Unix-Derivat namens "LUnix" entwickelt. Aktuell weiterentwickelt wird das ebenfalls Unix-orientierte "Wings"-Betriebssystem für den C64.

Neue C64-Software und C64-Hardware wird auch heute noch von verschiedenen Firmen (zum Beispiel "Protovision") vertrieben und entwickelt.

Die wichtigsten Programmiersprachen für den C64 waren das eingebaute BASIC und Assembler. Daneben gab es eine Vielfalt an Programmiersprachen und -Dialekten für den C64:

Das eingebaute Commodore BASIC V2 bietet keinerlei Befehle, um die Grafik- und Soundmöglichkeiten des C64 anzusprechen, da diese beim VC20, von dem der Code übernommen worden war, noch nicht vorhanden waren. Das bereits vorhandene und bessere BASIC 4.0 der neueren PETs wurde beim C64 nicht verwendet, da man den PETs keine interne Konkurrenz machen wollte. Über die BASIC-Befehle codice_9 und codice_10 kann direkt auf die Hardware zugegriffen werden, weiterhin ist über den codice_11-Befehl das direkte Anspringen von Systemroutinen möglich: Beispielsweise bewirkt codice_12 einen Reset des C64. Sound und Grafik lassen sich nur in Assembler oder erweiterten BASIC-Varianten wie etwa Simons’ BASIC effektiv programmieren, die jedoch nicht Bestandteil des Lieferumfangs waren. Spiele für den C64 sind daher nahezu ausschließlich in Assembler programmiert. Bei späteren BASIC-Versionen, beispielsweise dem BASIC 3.5 des C16 und Plus4, ist der Befehlsvorrat wesentlich umfangreicher.

Neben dem eingebauten Commodore BASIC V2 gab es noch diverse Dialekte und Compiler. Eine Auswahl:

Austrospeed ist ein 2-Pass-Compiler (3-Pass-Compiler im Overlay-Modus), der BASIC V2.0-Code in einen kompakten, schnell interpretierbaren Zwischencode (ähnlich P-Code) übersetzt. Derart kompilierte Programme laufen drei- bis fünfmal schneller ab als unkompilierte. Zum Austrospeed gab es auch einen dazugehörigen Decompiler.

Basic-Boss ist ein BASIC-Compiler aus dem Hause Markt & Technik Verlag, der im Jahre 1988 erschien und sehr stabile Programmergebnisse aus reinen BASIC-Programmen erzeugt. Reine BASIC-Programmierer können mit Hilfe des Compilers schnelle Programme erhalten, ohne auf Assembler ausweichen zu müssen. Der Benutzer muss dafür bestimmte „Definitionen“ in sein BASIC-Programm einbauen, die dann nach dem Kompilieren diese hohen Geschwindigkeiten ermöglichen. In sehr günstigen Fällen laufen die Programme 50- bis 100-fach schneller ab.

Für den C64 gab es Bascoder für den BASIC-Dialekt BASICODE. Dabei handelte es sich um einen rechnerübergreifenden BASIC-Standard.

Exbasic Level II ist ein erweitertes und verbessertes BASIC für den C64, das von Diskette geladen oder per Cartridge installiert wurde. Im Gegensatz zu Simons’ Basic war Exbasic Level II ursprünglich nicht für den C64 geschrieben worden, so dass nicht alle Möglichkeiten der Hardware dieses Rechners durch diese BASIC-Erweiterung ausgenutzt wurden.

G-Basic stellte umfangreiche Programmierfunktionen zur Verfügung, über die das Standard-BASIC des C64 nicht verfügte. Es wurde als Hardwareerweiterung geliefert, die in Form und Größe an eine Zigarettenschachtel erinnerte. Diese besaß einen eigenen Resettaster, da der C64 ab Werk nicht über einen solchen verfügte.

Geo-Basic ist ein BASIC unter der grafischen Oberfläche GEOS. Es enthielt allerdings viele Fehler und lief langsam, weshalb es sich nicht durchsetzen konnte. Auch war der für die Anwendungsprogramme zur Verfügung stehende Arbeitsspeicher nur sehr klein.

Petspeed ist ein Compiler für das eingebaute BASIC V2.0 von Commodore; für längere Programme benötigte der Compiler ein – selten vorhandenes – Doppelfloppylaufwerk.

Simons’ Basic ist ein stark erweitertes BASIC mit grafischen Funktionen (Kreis, Ellipse) sowie teilweise strukturierter Programmierung. Vertrieben auf Diskette oder Cartridge.

Assembler ist die wichtigste und – zusammen mit dem eingebauten BASIC – die am häufigsten genutzte Programmiersprache für den C64. Nur mit Assembler konnten die Fähigkeiten des Gerätes optimal genutzt werden. Es gab verschiedene Assembler-Entwicklungsumgebungen, die bekanntesten hießen TurboAss, Hypra-Ass und Giga-Ass. Für große Projekte wurden Cross-Assembler-Systeme eingesetzt. Diese bestanden aus zwei Computern, die mit einem Datenkabel verbunden waren: Einem C64, auf welchem das neu entwickelte Programm getestet wurde, und einem zweiten Computer, zum Beispiel ein weiterer C64, ein Amiga oder PC, auf welchem der Quelltext geschrieben und von einem Cross-Assembler übersetzt wurde. Das machte die Programmierung weitaus komfortabler, da auf dem Test-C64 der komplette Speicher bis auf die wenigen Bytes für die Übertragungsroutine zur Verfügung stand und im Fall eines Absturzes Quelltext und Assembler nicht gelöscht wurden. Jedoch reichte schon ein einfacher Maschinensprachemonitor aus, um Software für den C64 zu entwickeln: Das wohl bekannteste Exemplar eines solchen Programmes war der Smon. Auch brachten viele Erweiterungsmodule, wie das Action Replay oder die Final Cartridge, eigene Maschinensprachemonitore mit.

Mit Oxford Pascal gab es eine Pascal-Implementierung, die in der Lage war, eigenständige Programme auf Diskette zu schreiben oder im Speicher zu halten. Sie war durchaus standard-konform. Auch von UCSD Pascal gab es eine Portierung auf den C64; sie war jedoch so umständlich und langsam, dass sie in der Praxis keine Rolle spielte.

Zusätzlich zu den genannten Sprachen gibt es weitere Programmiersprachen, die aber eher exotisch sind. So gibt es einen C-Compiler (der allerdings nur eine Teilmenge von C implementiert), Forth und COMAL sind ebenfalls vertreten; es wurde sogar eine COBOL-Implementierung produziert. Auch Logo gibt es für den C64.

Weiterhin existiert das Betriebssystem Contiki, das eine Internet- und Ethernetanbindung über den C64 erlaubt.

Heute existiert mit cc65 ein leistungsfähiger Cross-Compiler für die Sprache C, der bis auf Gleitkommazahlen fast den ganzen ANSI-Standard abdeckt. Der Compiler selbst läuft auf den meisten modernen Plattformen.

Die Spiele für den C64 waren eines der besten Verkaufsargumente für den Rechner: Fast jedes bekannte Computerspiel in den 1980er- und teilweise in den 1990er-Jahren wurde für den C64 umgesetzt, darunter viele Arcade-Spiele, so auch Donkey Kong und Pac-Man. Schätzungen gehen von etwa 17.000 kommerziellen Spieletiteln für dieses Gerät aus, nicht mitgezählt die zahllosen Spiele, die C64-Besitzer selbst programmierten. Über 95 Prozent aller Spiele haben eine Auflösung von 160 × 200 doppelbreiten Pixeln.

Im Laufe der Jahre wurden insbesondere die Spiele immer komplexer und grafisch anspruchsvoller. Einige grafische Höhepunkte für den C64 sind unter anderem das Strategiespiel Defender of the Crown oder Manfred Trenz’ Actionspiel "Turrican II: The Final Fight", deren Grafiken teilweise an Amiga-Qualität heranreichen. Andere herausragende Beispiele sind Wizball (Rahmensprites), Stunt Car Racer (3D-Grafik mit ausgefüllten Polygonen) oder die "Last-Ninja"-Trilogie. Auch die Präsentation und Animation der beliebten Sportspiele der Firma "Epyx/U.S.Gold" ("Summer Games 1+2," "Winter Games", "California Games" und so weiter) konnten überzeugen. Das von Nintendos Mario-Serie inspirierte Great Giana Sisters erfreute sich ebenfalls großer Popularität.
Schon in den 1980er-Jahren erprobten politische Gruppierungen die Möglichkeit, Computerspiele für ihre Zwecke zu nutzen. Diese technisch primitiven Spiele, die als Kopien auf Schulhöfen getauscht wurden, basieren meist auf der Technik des C64, etwa das von einem 17-Jährigen programmierte rechtsextremistische Spiel „Anti-Türken-Test“, in dem rassistische Fragen über die Tastatur zu beantworten sind oder das Spiel „KZ-Manager“, in dem ein Konzentrationslager möglichst effektiv geführt werden muss. Viele dieser Programme wurden in den 1980er- und frühen 1990er-Jahren durch die Bundesprüfstelle für jugendgefährdende Medien (damals noch Bundesprüfstelle für jugendgefährdende Schriften, kurz BPjS) indiziert und später durch Gerichtsbeschlüsse bundesweit beschlagnahmt.

Der C64 trug besonders zur Entwicklung einer vielfältigen Subkultur bei, in der talentierte Programmierer Tricks entwickelten (zum Beispiel die Ausnutzung undokumentierter Hardwarefunktionen, darunter sehr viele Tricks für den Grafikchip), um die augenscheinlichen Limitierungen des Computers zu umgehen. Teile dieser Szene leben heute noch fort (siehe auch Demoszene) oder entwickelten sich weiter zu anderen Computersystemen wie Amiga oder PC. Die Demoszene entstand in den 1980er-Jahren aus der damaligen Crackerszene. Die Intros, die ursprünglich als Vorspann zwecks Präsentation der Fähigkeiten und Wiedererkennung vor gecrackte Spiele gesetzt wurden, nahmen stetig an Komplexität zu und wurden schließlich als Einzelwerke ("Demos") ohne dazugehörige gecrackte Software veröffentlicht.

Einem Außenstehenden erschließen sich die Schwierigkeiten dieser Programmierung häufig nicht, da er die Komplexität oder die laut Spezifikation eigentliche Unmöglichkeit des Effekts nicht einschätzen kann. Einige der grundlegenden Mechanismen betrafen die Nutzung des im Grafikchip integrierten sogenannten Rasterzeileninterrupts (Interrupt-Auslösung bei einer bestimmten Bildzeile) zur Synchronisierung von Code-Sequenzen, das ruckfreie Scrollen der Bildschirmfläche in beiden Achsen oder die Wiederverwendung von Sprites innerhalb eines Bildes. Typische Kennzeichen waren vor allem rasante, tanzende Rollschriften, mit 16 Farben vorgetäuschte, waagerechte Zylinderformen, sowie fast immer ein üppiges akustisches Beiwerk.

Die Demoszene lotete die Möglichkeiten des C64 am weitesten aus. Höhepunkte setzten Demos wie "Deus Ex Machina" der Gruppen Crest und Oxyron, "Tower Power" der Gruppe Camelot, "+H2K" der Gruppe Plush oder "Dutch Breeze" der Gruppe Blackmail sowie "Double Density" von Mr.Cursor aka Ivo Herzeg, der in der Entwicklung mitverantwortlich für bekannte PC-Spiele wie Far Cry ist.

Die Website der Demogruppe Alpha Flight 1970 enthält einige Flashversionen von szenetypischen Produktionen. Ein riesiges Repertoire an Informationen zu alten wie neuen Produktionen sind in der "Commodore 64 Scene Database" (CSDb) verzeichnet.

Mit dem rasanten Aufstieg des Heimcomputers in den 1980er-Jahren im Allgemeinen und des C64 im Speziellen entstand auch ein Tauschmarkt für Raubkopien von Software für diesen Rechner. Auch Anwendersoftware, aber im überwiegenden Maße Spiele wurden zwischen den C64-Besitzern getauscht. Das war mit den ersten kommerziellen Programmen noch sehr einfach machbar. Doch schon bald versuchte die Softwareindustrie, durch verschiedene Kopierschutzmaßnahmen (mittels Datenträger, durch Papier-basierte Abfragen oder auch durch Laufzeitmaßnahmen) der Situation Herr zu werden. Das gelang kaum, da quasi gleichzeitig die Szene dafür sorgte, dass die Software einerseits mit ihren eigenen Programmen wieder kopierbar wurde, andererseits erzeugte man durch Decodieren und gezieltes Modifizieren der Originale ungeschützte Versionen, die sich mit jedem beliebigen Kopierprogramm duplizieren ließen. Es entstand eine Art „Hase-und-Igel“-Wettlauf zwischen der Softwareindustrie und den C64-Besitzern, in dem immer neue Kopierschutzmaßnahmen die illegale Verbreitung von Software verhindern sollten. Letztlich war jedoch fast jedes Programm für den C64 früher oder später auch als „freie“ Raubkopie in Umlauf.

Eine erste Abmahnwelle veranlasste Ende 1992 der Rechtsanwalt Freiherr von Gravenreuth, als er über Testbesteller auf verdächtig erscheinende Kleinanzeigen in Computerzeitschriften, in denen überwiegend Privatleute inserierten, die sogenannten „Tanja-Briefe“ (unter dem Pseudonym „Tanja Nolte-Berndel“ und einigen weiteren weiblichen Pseudonymen) versandte. Falls ein so Angeschriebener auf die Bitte um Softwaretausch des angeblichen Teenagers einging, wurde dieser bei entsprechender Beantwortung wegen Verstoßes gegen das Urheberrecht abgemahnt, gegebenenfalls auch angezeigt. Auch führten einige Fälle zu Hausdurchsuchungen.

Mit der Zeit wurde es Brauch bei den Crackern, vor die von ihnen „geknackten“ Programme einen eigenen, mehr oder weniger aufwändigen Vorspann (ein sogenanntes „Cracktro“) zu setzen. Typischerweise wurde dort in Laufschriften die eigene Coolness gepriesen, es wurden befreundete Crackergruppen gegrüßt, und zunehmend stellte man auch optisch und akustisch die eigene Programmierkunst zur Schau. Die oben beschriebene Demoszene ging zuerst aus der Verselbstständigung dieser Cracker-Vorspänne zu eigenständigen Programmen hervor, auch wenn später eine klare Abgrenzung der Demo- von der Crackerszene stattfand.

Der Soundchip des C64 war zum Verkaufsstart des C64 eine Sensation, da es bis dahin keinen vergleichbaren Heimcomputer gab, der eine solche Vielfalt an Klangvariationen bot. Durch diese technischen Möglichkeiten machten sich unzählige Programmierer daran, den C64 als Musikcomputer zu nutzen und entsprechende Musik auf ihm zu programmieren.

Für den deutschen Sprachraum besonders zu erwähnen ist das komplett auf dem C64 programmierte Stück „Shades“ von Chris Hülsbeck, der mit diesem Song im Jahre 1986 den Musikwettbewerb der Fachzeitschrift 64’er gewann und damit den Grundstock für seine Karriere im Bereich der Spielevertonung legte. Weitere bekannte C64-Komponisten waren Rob Hubbard, Martin Galway, Ben Daglish, David Dunn, Markus Schneider, Stefan Hartwig, Reyn Ouwehand, Jonathan Dunn, Matt Gray, Jeroen Tel, Jens-Christian Huus (JCH) und Charles Deenen (Maniacs of Noise).

Auch die professionelle Musikszene nutzte den C64 als Musikinstrument. So experimentierte der Musiker und Musikproduzent Michael Cretu in den 1980er-Jahren mit den Klängen des C64 und auch die Band von Inga Rumpf setzte den C64 ein. Viele Musiker geben noch heute an, durch den C64 den ersten Zugang zu einem Synthesizer bekommen zu haben, der eine Grundlage ihrer späteren Entwicklung war, so z. B. Rick J. Jordan von der Band Scooter. In der E-Musik wurde der C64 etwa von Yehoshua Lakner eingesetzt und bewusst als „historisches Musikinstrument“ mit eingeschränkten, aber produktiven Möglichkeiten gesehen.

Mitte der 1980er-Jahre kamen MIDI-Sequenzer-Softwares u. a. von der Hamburger Firma Steinberg (heute mit dem Produkt "Cubase" marktbeherrschend) auf den Markt, die den C64 als Steuerzentrum für MIDI-Synthesizer und MIDI-Sampler nutzten. Mit der Software "Pro 16" von Steinberg konnte man professionelle Popmusik-Produktionen erstellen. Der C64 konnte über eine grafische Darstellung und mit manipulierbaren Zahlenwerten 16 verschiedene Instrumente (Piano, Drums, Bass usw.) gleichzeitig ansteuern. Die Taktrate und der Speicher des C64 reichten voll und ganz aus, MIDI-Instrumente nach Belieben zu steuern. Der SID des C64 wurde dabei nicht benutzt, da die Sounds nur von den Peripheriegeräten kamen. Auch in der Filmmusikszene fasste der C64 (wenn auch nur kurzzeitig) Fuß. So wurde beispielsweise der 80-minütige Dokumentarfilm über die berüchtigten Meuterer von der Bounty „Pitcairn – Endstation der Bounty“ (Regie: Reinhard Stegen) vollständig mit Musik untermalt, die auf einem C64 komponiert worden war. Damit stellte der C64 seine Praxistauglichkeit auch im Profibereich vollends unter Beweis. Der kurz darauf, Ende der 1980er-Jahre aufkommende Atari ST übernahm in fast allen deutschen Musikstudios das Kommando in Sachen MIDI-Sequencing und löste den C64 im Profibereich ab.

In Deutschland kamen ab Anfang der 1980er-Jahre verschiedene Computermagazine speziell für den C64 auf den Markt. Am bekanntesten war die „64’er“ vom Verlag „Markt & Technik“, der Heise-Verlag gab mit der „Input 64“ ein Magazin auf einem Datenträger (Kassette und Diskette) heraus. Auch bekannt und verbreitet waren die Diskettenmagazine „Magic Disk 64“ und sein Ableger „Game On“ sowie die „RUN“. Als inoffizieller Nachfolger der 64'er erschien von 1997 bis 2006 die „Go64!“ (CSW-Verlag, Winnenden), die in der „Retro“ aufging, welche seit 2006 vierteljährlich erscheint. Des Weiteren existieren gegenwärtig noch zwei deutschsprachige Amateur-Printmagazine, die „Lotek64“ (auch als kostenlose PDF-Version im World Wide Web erhältlich) und die „Return“. In England waren „Commodore Force“ und „Commodore Format“ beliebt. Heute gibt es noch das englischsprachige Fanmagazin „Commodore Free“, das ebenfalls kostenlos als PDF erhältlich ist. Zudem erscheinen in mehr oder weniger regelmäßiger Reihenfolge Magazine auf Diskette, wie etwa die "Digital Talk", die "Mail Madness" oder das australische Diskmag "Vandalism News". Diese enthalten neben am Bildschirm zu lesenden Artikeln auch aktuelle Software, Musik und Bilder.

Auch einige der damaligen Magazine, die viele verschiedene Rechnerplattformen abdeckten (wie „Happy Computer“, „Power Play“ und „ASM“) waren aufgrund des Markterfolges des C64 zunächst sehr auf diesen fixiert, was Besitzer anderer Rechner oftmals bemängelten.

Inhalte aller dieser Magazine war nicht nur die Berichterstattung über neue Hard- und Software für die jeweiligen Geräte, sondern auch der seitenweise Abdruck von Listings, also von Programmtexten, die der Leser dann per Hand in den Computer abtippen konnte. Diese Art des Vertriebs von Software für den C64 war für den Besitzer, neben dem Erwerb von Kaufsoftware oder Schwarzkopien, oft der einzige Weg, an Programme zu gelangen, da es den Download über das Internet noch nicht gab.

Ab Ende 1985 wurde der C64 im Intershop gegen „Westgeld“ oder Forumschecks verkauft, weitere Geräte fanden als Geschenk ihren Weg in die DDR. Gelegentlich konnte der C64 dank asiatischer Gastarbeiter im staatlich organisierten Gebrauchtwarenhandel („A & V“) für 8000 Mark als Neuware erstanden werden. Daneben gab es einen Privathandel über Kleinanzeigen. Die Gebrauchtpreise lagen bei 3000 bis 6000 Mark für den C64 und bis zu 5000 Mark für ein Diskettenlaufwerk. Bespielte Disketten und Kassetten unterlagen als Datenträger jedoch strengsten Importkontrollen, durften auch nicht als Geschenk aus dem Westen geschickt werden und waren daher ohne Beziehungen so gut wie nicht erhältlich.

Heute gibt es etliche Commodore-64-Emulatoren, wie den VICE, den M.E.S.S., Power 64 (für Mac OS X und Mac OS 9), Frodo (u. a. für Symbian-Handys, sowie Apple iOS und Android) und den ccs64. Diese erlauben es, C64-Software auf moderneren Rechnern wie etwa einem Windows-PC auszuführen. Mit den Emulatoren kann neben Disk-Images auch Original-C64-Zubehör wie z. B. Disketten- und Datasettenlaufwerke angesteuert werden. Für die Verwendung der Datasette oder der Original-Diskettenlaufwerke sind jedoch Bastelarbeiten für Kabel notwendig, um die Geräte mit den heutigen Ports anzusteuern. Für Nutzer, die die langen Ladezeiten des C64 nicht mögen, bieten die Emulatoren einen virtuellen Lademodus.

Die meiste C64-Software, die in den 1980er-Jahren veröffentlicht wurde, kann auf heutigen Systemen (PC, Mac) mit Hilfe dieser Emulatoren genutzt werden. Seit dem 28. März 2008 stehen ausgewählte C64-Spiele im Download-Katalog der Wii-Konsole zur Verfügung.
















</doc>
<doc id="933" url="https://de.wikipedia.org/wiki?curid=933" title="Culpa in contrahendo">
Culpa in contrahendo

Culpa in contrahendo (lateinisch: "Verschulden bei Vertragsschluss"), oft auch "c.i.c." abgekürzt, bezeichnet die schuldhafte Verletzung von Pflichten aus einem vorvertraglichen (gesetzlichen) Schuldverhältnis. Die c.i.c. gehört zu den vertragsähnlichen Ansprüchen.

Gesetzlich geregelt wurden die Folgen eines Verschuldens beim Vertragsschluss erstmals im Preußischen ALR von 1794: "„Was wegen des bey Erfüllung des Vertrages zu vertretenden Grades der Schuld Rechtens ist, gilt auch auf den Fall, wenn einer der Contrahenten bey Abschließung des Vertrags die ihm obliegenden Pflichten vernachläßigt hat.“"

Als Entwickler des Grundsatzes der "culpa in contrahendo" gilt allerdings Rudolf von Jhering, der 1861 eine Abhandlung darüber verfasste. Diese befasste sich in erster Linie mit den Themenkomplexen der Haftung des Irrenden nach Anfechtung, der Haftung des Vertreters ohne Vertretungsmacht beziehungsweise des Verkäufers einer nicht existierenden Sache. Unter Einbezug eines Urteils des damaligen Landgerichts Köln, das den sogenannten „Telegraphen-Fall“ zu entscheiden hatte, ging Jhering beispielsweise davon aus, dass schon im Veranlassen eines Missverständnisses ein Verschulden des Erklärenden erblickt werden könne. Für abgehandelten Fallgestaltungen wurde im BGB die von Jhering vorgeschlagene Rechtsfolge der Haftung auf das negative Interesse verankert. 

Gleichwohl gab es zunächst keine Norm, die vorvertragliche Pflichtverletzungen allgemein regelte. Da darin eine Gesetzeslücke im BGB erkannt wurde, füllte die Rechtsprechung diese durch die (Weiter-)Entwicklung des Rechtsinstituts der „culpa in contrahendo“.

Mit der gesetzlichen Schuldrechtsmodernisierung im Jahr 2002 ist das Rechtsinstitut letztlich gesetzlich ( Abs. 2 in Verbindung mit Abs. 1 in Verbindung mit Abs. 2 BGB) geregelt worden.

Das „Gegenstück“ zur vorvertraglichen „culpa in contrahendo“ bildet die culpa post contractum finitum. Sie erfasst Verletzungen nachwirkender Pflichten, die erst nach der Abwicklung des Vertrags auftreten.

Es geht um den Ersatz eines außervertraglichen (vertragsähnlichen) Vertrauensschadens. Der Anspruch ergibt sich in besonderen Fällen eines vertrauensbildenden (Geschäfts-)Kontaktes aus der Konstruktion eines gesetzlichen Schuldverhältnisses, das sich nicht bereits aus einem Vertrag oder einer sonstigen gesetzlichen Regelungen ergibt. Dieser Kontakt kann durch die Aufnahme von Vertragsverhandlungen entstehen, unabhängig davon, ob es letztendlich zu einem Vertragsschluss kommt oder nicht. Rechtsdogmatische Begründung der c.i.c. ist, dass bereits im vorvertraglichen Bereich dem Gegenüber eine erhöhte Einwirkungsmöglichkeit auf Rechtsgüter Dritter ermöglicht wird. Deshalb wird davon ausgegangen, dass gesteigerte Schutz- und Verkehrssicherungspflichten bestehen, deren Verletzung schadensersatzpflichtig machen. Werden beispielsweise einem Unternehmensberater von einem potenziellen Mandantenunternehmen während der Akquisephase Geschäftsgeheimnisse anvertraut, kommt im Anschluss aber kein Vertrag zustande, und der Unternehmensberater veröffentlicht daraufhin die Geschäftsgeheimnisse dieses Interessenten, so liegt ein Fall der "culpa in contrahendo" vor.

In einer Grundsatzentscheidung hatte der BGH betont, dass Haftungsansprüche Dritter grundsätzlich nur die unmittelbar am beabsichtigten Vertrag beteiligten Parteien seien und nicht deren Vertreter oder Verhandlungsgehilfen. Soweit die Rechtsprechung dazu Ausnahmen zuließ, waren diese auf an Vertragsverhandlungen beteiligte Dritte beschränkt, die neben der verhandelnden Partei „besonders an einem Vertragsschluss interessiert“ waren oder „besonderes Vertrauen für sich beansprucht“ hatten. Der BGH erkannte in diesen beiden deutlich auseinanderliegenden Kriterien für die Schaffung von Ausnahmen eine Widersprüchlichkeit, die er so auflöste, dass heute nur das letztere Kriterium die Haftung eines hinter den an der Vertragsanbahnung Beteiligten, auslösen kann. 

Aber auch in alltäglicheren Situationen erlangt dieses Institut Bedeutung: Verletzt man sich z. B. beim Bummeln im Kaufhaus, weil die Reinigungskräfte ihren Aufgaben nicht ordentlich nachgekommen sind (Salatblattfall) oder weil das Verkaufspersonal Ware unsachgemäß in einem Hochregal gelagert hat (sogenannter ), so ist auch hier eine vertragliche Haftung des Kaufhausbetreibers eröffnet. Nachrangig greift zwar daneben die deliktische Haftung. Im Deliktsrecht kann sich der Geschäftsherr jedoch (anders als im Bereich des vertraglichen Schadensersatzes) gegebenenfalls von der Verantwortung für das Fehlverhalten der Angestellten exkulpieren ( BGB). 

Dieser Umstand kann bedeutsam sein, wenn der verantwortliche Angestellte nicht konkret ermittelt werden kann oder selbst gar nicht die finanziellen Mittel besitzt, um für den Schaden aufzukommen. Der "culpa in contrahendo" kommt besondere Bedeutung dort zu, wo die vertragliche Haftung gegenüber anderen Haftungsinstituten, insbesondere gegenüber dem Deliktsrecht, einen weiterreichenden Schutz bietet. Der Vorteil besteht vor allem im Bereich der Verschuldenszurechnung (keine Exkulpationsmöglichkeiten des Haftenden, vgl. zweites Beispiel), sowie einer Vermutung dieses Verschuldens ( Abs. 1 S. 2 BGB), welche dann die Gegenseite widerlegen muss (Beweislastumkehr). Auch sind über die weitergehenden Vertragspflichten reine Vermögensschäden erfasst. (vgl. im ersten Beispiel oben den besonderen Vertrauensschutz).

Ausnahmsweise können auch Dritte vom Schutz der "culpa in contrahendo" erfasst werden. Dies geschieht nach den Regeln des Vertrages mit Schutzwirkung zugunsten Dritter.

Wer schuldhaft eine vorvertragliche Pflicht verletzt, ist, ebenso wie der Verletzer einer vertraglichen Pflicht, zum Schadensersatz nach den §§ 249 ff. BGB verpflichtet. Unabhängig davon, ob später ein Vertrag geschlossen wurde, kann der Geschädigte verlangen, so gestellt zu werden, wie er ohne die Pflichtverletzung während der Vertragsverhandlungen stünde. Wäre es bei pflichtgemäßem Verhalten des anderen Teils überhaupt nicht zu einem Vertragsabschluss gekommen, kann der Geschädigte die Rückabwicklung des Vertrages verlangen.

Die culpa in contrahendo ist im Schweizer Recht die schuldhafte Verletzung von vorvertraglichen Pflichten. Ihre Voraussetzungen sind dabei Vertragsverhandlungen, das Vorliegen eines schutzwürdigen Vertrauens, eine Pflichtverletzung sowie ein Schaden, Kausalzusammenhang und Verschulden. Die Pflichtverletzung im Besonderen leitet sich aus dem Grundsatz von Treu und Glauben ab und umfasst u. a. die Pflicht zu ernsthaften Verhandlungen.

Ihrer Natur nach handelt es sich um eine eigenständige Haftungsgrundlage, welche zwischen Vertrag und Delikt angesiedelt ist. In der Schweiz hat die c.i.c. bisher jedoch noch keinen Niederschlag im Gesetz gefunden. Gemäß schweizerischer Doktrin ist die c.i.c. eine Sonderform der Vertrauenshaftung. Aus dogmatischer Sicht ist die c.i.c. im Schweizer Recht den quasivertraglichen Ansprüchen zuzuweisen, was zur Folge hat, dass das positive Vertrauensinteresse zu ersetzen ist, dem Schädiger jedoch die Herabsetzungsgründe nach Art. 44 OR sowie Art. 99 Abs. 3 OR zur Verfügung stehen. Während das Bundesgericht von einer Verjährungsfrist von 1 Jahr ausgeht (Delikt), verlangt die Lehre eine 10-jährige Verjährungsfrist (Vertrag).

Das Vorliegen eines c.i.c. führt ebenfalls nicht zur Aufhebung des Vertrags, sondern nur zu den Schadenersatzfolgen. Wer als Geschädigter den Vertrag aufheben will, muss sich daher auf Übervorteilung (Art. 21 OR), Irrtum (Art. 23 OR) oder absichtliche Täuschung (Art. 28 OR) berufen.




</doc>
<doc id="935" url="https://de.wikipedia.org/wiki?curid=935" title="Chile">
Chile

Chile (Aussprache: [], dt. auch [], []), amtlich (), ist ein Staat im Südwesten Südamerikas, der den westlichen Rand des so genannten Südkegels "(Cono Sur)" des Kontinents bildet. Chile erstreckt sich in Nord-Süd-Richtung zwischen den Breitengraden 17° 30′ S und 56° 0′ S; somit beträgt die Nord-Süd-Ausdehnung rund 4200 Kilometer. In west-östlicher Richtung liegt Chile zwischen dem 76. und dem 64. westlichen Längengrad und besitzt eine Ausdehnung von durchschnittlich weniger als 200 Kilometern. Wegen dieser – durch seine Lage am Westhang der Andenkordillere bedingten – ungewöhnlichen Form wird Chile schon seit seiner Entdeckung häufig „das langgestreckte Land“ genannt. Das Land grenzt im Westen und Süden an den Pazifischen Ozean, im Norden an Peru (auf einer Länge von 160 Kilometern), im Nordosten an Bolivien (861 km) und im Osten an Argentinien (5308 km). Die Gesamtlänge der Landgrenzen beträgt 6329 Kilometer. Daneben zählen die im Pazifik gelegene Osterinsel (Rapa Nui), die Insel Salas y Gómez, die Juan-Fernández-Inseln (einschließlich der "Robinson-Crusoe-Insel"), die Desventuradas-Inseln sowie im Süden die Ildefonso-Inseln und die Diego-Ramírez-Inseln zum Staatsgebiet Chiles. Ferner beansprucht Chile einen Teil der Antarktis. Über die vollständig zu Chile gehörende Magellanstraße hat das Land Zugang zum Atlantischen Ozean.

Chile belegt im Index der menschlichen Entwicklung den 38. Platz und liegt somit innerhalb Lateinamerikas an erster Stelle. Chile hat eines der höchsten Pro-Kopf-Einkommen in Lateinamerika und eine im lateinamerikanischen Vergleich hohe durchschnittliche Lebenserwartung. Nach Einschätzung der Weltbank ist Chile ein Schwellenland mit einem Nettonationaleinkommen im oberen Mittelfeld.

Die Herkunft des Wortes "Chile" ist nicht eindeutig nachgewiesen. Die verbreitetste Erklärung ist, dass sich das Wort aus der Sprache der Aymara herleitet. Dort bedeutet das Wort "chilli" „Land, wo die Welt zu Ende ist“. Dies würde durch die Tatsache unterstützt, dass die ersten Spanier, die nach Chile kamen, von den Siedlungsgebieten der Aymara aus aufbrachen. Die Spanier bezeichneten seit Anbeginn der Kolonisation Südamerikas das Land südlich der Atacamawüste mit dem Namen "Chile". In den chilenischen Schulen wird außerdem noch die Variante gelehrt, dass Chile die lautmalerische Bezeichnung eines Vogels namens "Trile" sein könnte.

Eine weitere, wenig verbreitete Theorie nennt die Inka-Sprache Quechua als Ursprung. Die maximale Ausdehnung des Inkareichs reichte bis zum Gebiet des heutigen Santiago, woraufhin die Inka das Land südlich des Río Aconcagua in Anlehnung an das relativ kalte Klima und die schneebedeckten Anden "tchili" nannten, was Schnee bedeutet.

Als gesichert hingegen gilt, dass die Landesbezeichnung "Chile" nicht auf die (spanisch gleichnamige) Chilischote zurückzuführen ist. Dieses Wort stammt aus der mittelamerikanischen Aztekensprache Nahuatl. Die Chili (und die daraus gemachte Salsa) heißt im chilenischen Spanisch "ají" (siehe auch: Beispiele für Unterschiede im Wortschatz).

Chile erstreckt sich auf dem südamerikanischen Kontinent über 4275 Kilometer in Nord-Süd-Richtung entlang der Anden und des Pazifischen Ozeans (zählt man den antarktischen Teil hinzu, circa 8000 Kilometer), ist aber durchschnittlich nur circa 180 Kilometer breit. Die engste Stelle im kontinentalen Chile (ohne Antarktis) beträgt 90 Kilometer, die breiteste Stelle etwa 440 Kilometer. Die Längenausdehnung Chiles entspricht auf Europa und Afrika übertragen in etwa der Entfernung zwischen der Mitte Dänemarks und der Sahara.

Aufgrund der langen Nord-Süd-Ausdehnung über mehr als 39 Breitengrade, aber auch der beträchtlichen Höhenunterschiede in West-Ost-Richtung weist Chile eine große Vielfalt an Klima- und Vegetationszonen auf.

Chile liegt an der Grenze mehrerer Lithosphärenplatten: Unter die Südamerikanische Platte wird bis zum Golf von Penas die Nazca-Platte subduziert, südlich davon bis zur Magellanstrasse mit geringerer Geschwindigkeit die Antarktische Platte. Durch die Magellanstrasse verläuft in ost-westlicher Richtung die Grenze zwischen der Südamerikanischen und der Scotia-Platte.

Dies ist die Ursache des ausgeprägten Vulkanismus in Chile und der regelmäßig auftretenden, zum Teil massiven Erdbeben. Das erste dokumentierte Beben war das große Erdbeben von Concepción im Jahre 1570. Das Erdbeben von Valdivia 1960, dessen Tsunami im gesamten zirkumpazifischen Raum schwere Schäden verursachte, war das Beben mit der weltweit größten jemals aufgezeichneten Magnitude. Am 27. Februar 2010 erschütterte ein massives Erdbeben der Stärke 8,8 M auf der Momenten-Magnituden-Skala den Süden Chiles und zerstörte große Teile der chilenischen Infrastruktur. Auch Zentral-Chile war stark betroffen. In der Region VI und VII trafen nach etwa 20 Minuten hohe Tsunami-Wellen ein und zerstörten ganze Küstenstädte und Gebiete. Auch noch Wochen nach dem Erdbeben wurde das Land von vielen Nachbeben erschüttert. Insgesamt waren in Chile die Regionen III bis IX betroffen.

→ "Siehe auch: Liste von Erdbeben in Chile"

Stark vereinfacht besteht Mittel- und Südchile aus zwei parallelen Gebirgszügen mit Nord-Süd-Verlauf: den Anden im Osten und dem niedrigeren Küstenbergzug (Küstenkordillere, "Cordillera de la Costa") im Westen. Dazwischen liegt das Zentraltal ("Valle Central" oder "Valle Longitudinal") mit dem Hauptteil der Bevölkerung, des Ackerlands und des Weinbaus. Die Höhe von Kordillere, Zentraltal und Anden nimmt im Mittel von Norden nach Süden ab, so dass das Zentraltal südlich der Stadt Puerto Montt, die etwa 1.000 Kilometer südlich von Santiago liegt, unter den Meeresspiegel abtaucht. Die Küstenkordillere, von der nur noch die Bergspitzen aus dem Wasser ragen, wird gleichzeitig zur Inselkette. In dieser Region lässt sich deswegen eine einzigartige Fjord- und Insellandschaft entdecken. Im Norden Chiles dagegen gibt es kein ausgeprägtes Zentraltal, das heißt, die Landschaft steigt von der Küste kommend zunächst steil an und bildet dann mit der Pampa del Tamarugal ein etwa 1000 bis 1500 Meter hohes Plateau bis zum Fuße der Anden.

Die chilenischen Anden, die nur an wenigen Stellen die 2000-Meter-Höhenlinie unterschreiten, unterteilen sich hinsichtlich ihrer geologisch-tektonischen Struktur von Nord nach Süd in vier größere Blöcke.

Der Übergangsbereich zwischen Küstenkordillere und den Anden lässt sich in zwei Bereiche untergliedern: die "Pampa del Tamarugal" im Norden und das "Valle Longitudinal" im zentralen und südlichen Bereich. Beide sind ausgeprägte Graben-Systeme. Die "Pampa del Tamarugal" erstreckt sich direkt entlang der nördlichen Vulkankette, während das etwas tiefer gelegene "Valle Longitudinal" der südlichen Vulkankette folgt und bei Puerto Montt (41° 30′ S) ins Meer abtaucht.

Die Küstenkordillere erstreckt sich mit einer kurzen Unterbrechung südlich der Insel Chiloé über die gesamte Westseite des Landes. Sie steigt im Norden des Landes zwischen Arica und Chañaral (26. Breitengrad) als Steilküste unmittelbar auf 1000 m ü. M. (stellenweise über 2000 m) an. Da die wenigen Flüsse in diesem Raum aufgrund des extrem ariden Klimas nicht die Kraft zum Durchbruch haben, wird sie hier nur von wenigen Tälern durchschnitten. Die Talsysteme häufen sich erst südwärts von Chañaral. Das Küstengebirge flacht nach Süden hin ab und erreicht im Kleinen Süden schließlich nur noch an wenigen Stellen Höhen über 1000 m. Die Küstenkordillere setzt sich ab dem 44. Breitengrad (Chonos-Archipel) als Inselkette fort.

Die chilenischen Anden bilden einen der höchsten Gebirgszüge der Welt und weisen eine Vielzahl von Gipfeln über 6000 m auf. Unter ihnen befindet sich der höchste Berg Chiles, der Ojos del Salado (6893 m), welcher zugleich der höchste Vulkan der Welt ist.

Im Folgenden sind die bekanntesten Berge Chiles aufgelistet (vom Norden nach Süden):

Aufgrund der besonderen Struktur des Landes gibt es in Chile keine längeren Flüsse. Der mit 443 Kilometern längste ist der Río Loa im Norden inmitten der Atacamawüste. Die Flüsse, die dauerhaft Wasser führen, werden meist aus der Schnee- und Eisschmelze der Anden genährt. Gemäß den zunehmenden Niederschlägen nimmt nach Süden hin das mitgeführte Wasservolumen zu. Die Flüsse werden für die Bewässerung in der Landwirtschaft, zur Energiegewinnung und zu kleineren Teilen auch für den Tourismus genutzt. Einige Flüsse von Nord nach Süd sind folgende:

Zu den chilenischen Seen zählen im Norden die Salzseen, deren größter und bekanntester der Salar de Atacama (3000 Quadratkilometer) ist. Ganz im Norden liegt der 21,5 Quadratkilometer große Lago Chungará auf rund 4500 Meter Höhe, einer der höchstgelegenen Seen der Welt.

Die großen und landschaftlich schönsten Seen Chiles erstrecken sich südöstlich der Stadt Temuco bis nach Puerto Montt in folgender Reihenfolge:

Viele Seen sind schiffbar.

Chile liegt auf der Südhalbkugel, weshalb die Jahreszeiten um ein halbes Jahr im Vergleich zur Nordhalbkugel verschoben sind. Das Land lässt sich klimatisch in drei Zonen einteilen: Nord-, Mittel- und Südchile.

Nordchile (genannt „großer Norden“) besitzt viele Berge, die über hoch sind. Zwischen der Küste und der westlichen Anden-Hauptkette erstreckt sich die Atacamawüste. Diese Wüste ist eines der trockensten Gebiete der Erde; oft fällt jahrelang kein Regen. Die Wüste war in der Vergangenheit für ihre großen Salpetervorkommen bekannt, während dort heute vor allem Kupfer gefördert wird. Die größte und wichtigste Stadt dieser Region ist die Hafenstadt Antofagasta (310.000 Einwohner).

In Mittelchile herrscht ein dem Mittelmeerraum vergleichbares Klima. Diese Region ist sehr fruchtbar und dicht besiedelt. Hier befindet sich die Hauptstadt Santiago de Chile mit rund 5,5 Millionen Einwohnern. Daneben sind Valparaíso (Seehafen und Parlamentssitz, 280.000 Einwohner), Viña del Mar (beliebter Urlaubsort, 320.000 Einwohner) und Concepción (Zentrum der Landwirtschaft und Industrie, 216.000 Einwohner) von Bedeutung. Der Raum nördlich von Santiago wird „kleiner Norden“, der südlich von Santiago „kleiner Süden“ genannt.

Das sehr dünn besiedelte Südchile (genannt „großer Süden“) ist eine äußerst niederschlagsreiche Region. Die Küste ist durch eine Vielzahl vorgelagerter Inseln stark zerklüftet. Südlich des Festlands befindet sich die Insel Feuerland, die sich Chile mit dem Nachbarland Argentinien teilt. Auf der Feuerland vorgelagerten Insel Isla Hornos befindet sich Kap Hoorn, der südlichste Punkt Chiles und Südamerikas.

Insgesamt wird das Klima Chiles stark durch den Humboldt-Meeresstrom entlang der Küste beeinflusst. Dieser fließt von Süden nach Norden und transportiert kaltes Meerwasser aus der Antarktis. Während zum Vergleich Nordeuropa vom warmen Golfstrom profitiert, liegen die Wassertemperaturen in Chile bei analogem Breitengrad (Nord-/Südkoordinate) deutlich niedriger. In Punta Arenas (Südchile) – das etwa gleich weit vom Äquator entfernt liegt wie Hamburg – beträgt die mittlere Tagestemperatur im Sommer 12 Grad Celsius.

Eine Besonderheit des chilenischen Klimas ist der El-Niño-Effekt, auch "Südliche Oszillation" genannt. Dieses Klimaphänomen betrifft zwar hauptsächlich Länder wie Peru oder Indonesien, aber auch in Chile ist es etwa alle sieben Jahre wirksam und führt hier zu vermehrten Niederschlägen im Vergleich zu Normaljahren.

Aufgrund der riesigen Ausdehnung von über 4.000 Kilometern Länge gibt es in Chile sehr viele Vegetationszonen. Im Bereich der Atacamawüste wächst wenig. Bewuchs gibt es nur in Küstennähe oder im Bereich der Anden. Hier wachsen sehr viele verschiedene Kakteenarten, Sukkulenten und Zwergsträucher. Allerdings kommt es im Zusammenhang mit dem Klimaphänomen El Niño regelmäßig zum Phänomen der blühenden Atacamawüste, bei dem nach Regenfällen in der Wüste große Wüstenflächen nur für wenige Tage von Millionen von Blumen überzogen werden.

Südlich der Wüste folgt die Steppe mit trockenem Grasland und in den Anden wächst die steinharte Yareta ("Azorella yareta"), auch „Andenpolster“ genannt. In den trockenen Gebieten wächst der „Boldo-Strauch“ ("Peumus boldus"). An den Küstengebirgen und in den Anden gibt es Nebelwälder („hydrophile Wälder“), wo zum Beispiel ein Baumfarn (spanisch "Helecho arborescente") wächst.

Die Weinanbaugebiete beginnen im Bereich des Flusses Río Elqui, außerhalb des Flusstals gibt es dagegen nur Dornensträucher und Kakteen.

In Zentralchile wächst die Honigpalme ("Jubaea chilensis"). Die Araukarie ("Araucaria araucana") ist der heilige Baum der Mapuche, ihre großen Samen dienten ihnen zur Ernährung. In Chile gibt es auch zahlreiche große Eukalyptus-Plantagen.

In Südchile gibt es große Wälder, die dem gemäßigten Regenwald zugeordnet werden. Sie setzen sich vorwiegend aus Zypressen, Kiefern und Lärchen zusammen, ebenso sind Antarktische Scheinbuchen ("Nothofagus antarctica") und Pappeln weit verbreitet.

In der XI. Region (Aisén) gibt es Wälder mit beispielsweise folgenden Baumarten: Lenga-Südbuche ("Nothofagus pumilio"), Coihue-Südbuche ("Nothofagus dombeyi"), "Luma apiculata", "Aextoxicon punctatum" (der unter anderem in Chile Olivillo heißt), "Embothrium coccineum", Chilenische Scheinulme ("Eucryphia cordifolia"), Kerzenbaum ("Maytenus boaria").

Die „Nationalblume“ Chiles ist die rote Chilenische Wachsglocke ("Lapageria rosea"), sie heißt in Chile "Copihue" und ist eine Kletterpflanze.

Patagonien besteht aus weiten Steppen und Halbwüsten, an der Südwestküste findet sich die sogenannte Magellan-Tundra. Große Teile der Region Aisén und der Region Magallanes sind bereits vergletschert, so dass hier keine Vegetation mehr anzutreffen ist.

Feuerland ist von großen Mooren durchzogen. Hier halten sich nur noch wenige Baumarten, wie die Lenga-Südbuche, die Magellan-Südbuche ("Nothofagus betuloides") oder die Coihue-Südbuche ("Nothofagus dombeyi").

In den Steppengebieten sind Guanakos, die zur Familie der Kamele gehören, weit verbreitet. In den Andenregionen leben Vikunjas und der Huemul, der als "Nationaltier" Chiles zusammen mit dem Andenkondor im Staatswappen dargestellt ist.

Der Chinchilla, ein Nagetier, sowie der Puma leben ebenfalls in gebirgigen Steppenlandschaften. Die Wälder bieten Platz für Hirsche, Chilenische Waldkatzen, Füchse und für Kolibris.

Der Humboldt-Pinguin, Pelikane und Mähnenrobben leben selbst an den kalten Küsten Nordchiles, Mähnenrobben und Magellan-Pinguine im eisreichen Süden.

Über fast den ganzen Bereich Chiles ist der majestätische Andenkondor verbreitet, einer der größten Vögel der Welt. Die großen Salzseen beherbergen tausende Flamingos.

Im kargen Süden Feuerlands leben Eulen, Magellan-Füchse und Darwin-Nandus. Sehr häufig anzutreffen sind Strauchratten (Degus), kleine, ausschließlich in Chile heimische und vom Aussehen her rattenähnliche Nagetiere aus der Familie der Trugratten, die mit drei Arten fast das ganze Land bewohnen. Sie leben in Erdhöhlen in Kolonien und nehmen im Ökosystem die Nische ein, die in Deutschland die Wildkaninchen innehaben.

Das staatliche chilenische Statistikamt INE schätzt, dass das Land in der Mitte des Jahres 2015 18.006.407 Einwohner hatte. Davon waren 8.911.940 Männer und 9.094.467 Frauen. Die Bevölkerungszählung von 2002 hatte noch 15.116.435 Einwohner (7.447.695 Männer und 7.668.740 Frauen) ergeben.

Die Bevölkerung hat sich seit Beginn des 20. Jahrhunderts verfünffacht. Bei der Volkszählung des Jahres 1895 wurden 2.695.625 Einwohner ermittelt. Die Einwohnerzahl stieg auf 5.023.539 bei der Zählung 1940 und 13.348.401 im Jahr 1992. Seitdem hat sich das Bevölkerungswachstum verlangsamt, von 1,24 % pro Jahr zwischen 1992 und 2002 auf 0,99 % zwischen 2002 und 2012.

Das Bevölkerungswachstum ist nicht zuletzt auf die stark gestiegene Lebenserwartung zurückzuführen. Im Jahr 2017 hatten die Chilenen die höchste Lebenserwartung aller Südamerikaner. Sie lag im Zeitraum von 2010 bis 2015 bei 78,9 Jahren: 76,2 für Männer und 81,3 für Frauen. Die Lebenserwartung war damit vergleichbar mit der in den USA. Seit den Sechzigern hat Chile eine streng monoton steigende Bevölkerungsentwicklung. 2015 lag die Geburtenrate bei 13,4 ‰ und die Sterberate bei 6,1 ‰, was ein natürliches Bevölkerungswachstum von 0,85 % ergab. Die Kindersterblichkeit ist seit 2006 stetig gesunken und lag in 2016 bei 8,3 ‰. Diese Entwicklungen führen zu einer Alterung der Gesellschaft, so dass im Jahr 2020 die Mehrheit der Bevölkerung über 35 Jahre alt sein wird, während momentan die Unter-35-Jährigen die Mehrheit stellen. Für 2025 wird die für den Demografischen Übergang charakteristische Tropfenform der Bevölkerungspyramide erwartet.

Der Großteil der Bevölkerung lebt in den Regionen V bis X. Am dichtesten besiedelt ist der Großraum Región Metropolitana de Santiago, wo etwa die Hälfte der chilenischen Einwohner lebt. Die Stadt selbst hat etwa 5,5 Millionen Einwohner; sie beherbergt also in etwa ein Drittel aller Einwohner Chiles. Nördlich und vor allem südlich davon erstrecken sich landwirtschaftlich genutzte und dicht besiedelte Gebiete in der Ebene zwischen den Hauptketten der Anden. Nur 100 Kilometer westlich von Santiago liegt der Großraum um die Hafenstadt Valparaíso mit etwa einer Million Einwohnern.

Nach Norden und Süden verringert sich die Bevölkerungsdichte immer stärker. Die Atacamawüste im äußersten Norden und die rauen, stürmischen Gebiete im Süden sind aufgrund der ungünstigen klimatischen Bedingungen nur sehr dünn besiedelt.

Die Migrationsrate Chiles lag im Jahr 2012 bei 0,35 Migranten je 1.000 Einwohner und Jahr und war damit eine der niedrigsten in ganz Lateinamerika.

Im Jahre 1848 begann die deutsche Kolonisierung, die von der chilenischen Regierung gefördert wurde, um den Süden des Landes zu bevölkern. Die Einwanderung aus deutschen Staaten (Deutschland, deutscher Sprachraum) beeinflusste die Kultur eines großen Gebietes in Südchile, besonders in den Provinzen Valdivia, Osorno und Llanquihue. Einwanderer aus anderen europäischen und nahöstlichen Staaten kamen im 19. und 20. Jahrhundert vor allem in Valparaíso und im äußersten Norden und Süden an. Darunter befanden sich Österreicher, Briten und Iren, Kroaten, Spanier, Franzosen, Griechen, Italiener, Niederländer, Polen, Russen, Schweizer, Juden und Palästinenser. Im Jahre 1953 gründete Präsident Carlos Ibáñez del Campo die Einwanderungsbehörde "Departamento de Inmigración" und ließ Regeln über die Einwanderung aufstellen.

In den letzten Jahrzehnten hat die Einwanderung aus den benachbarten Staaten stark an Bedeutung gewonnen. Zwischen 2004 und 2010 stieg sie um 50 % auf geschätzt 365 459 Personen. Die Volkszählung des Jahres 2012 ergab, dass 339 536 im Ausland geborene Menschen in Chile wohnhaft waren. Sie stammten vor allem aus Peru (103 624), Argentinien (57 019), Kolumbien (27 411), Bolivien (25 151) und Ecuador (16 357).

Obwohl die Auswanderung aus Chile im vergangenen Jahrzehnt zurückgegangen ist, lebten im Jahr 2005 487.174 Chilenen außerhalb Chiles. Dies entspricht 3,01 % der Bevölkerung des gleichen Jahres (16 165 316 Menschen). Die meisten ausgewanderten Chilenen leben heute in Argentinien (43,33 %), des Weiteren 16,58 % in den USA, 5,61 % in Schweden, 5,21 % in Kanada und 4,80 % in Australien.

Die interne Migration von den ländlichen Gebieten in die Großstädte hat sich in den letzten Jahrzehnten verstärkt. So wurden etwa 80 % der Bevölkerung der mittleren und südlichen Regionen Chiles in der Region selbst geboren. In Biobío erreicht dieser Wert mit 86,11 % den Höchststand. Nur 71 % der Bewohner der Hauptstadtregion wurden auch in der Region geboren und gar nur 55 % der Region Magallanes y Antártica Chilena stammen von dort.

Die Bevölkerung Chiles ist im internationalen Vergleich sehr ungleich verteilt. Die Volkszählung des Jahres 2002 ergab, dass 13.090.113 Chilenen, oder 86,59 % der Gesamtbevölkerung, in Städten leben. Die Regionen in klimatisch extremen Zonen weisen den höchsten Urbanisierungsgrad auf — 97,68 % der Bevölkerung der Region Antofagasta, 94,06 % von Tarapacá und 92,6 % von Magallanes y Antártica Chilena leben in Städten. Auch die Industriestandorte in Mittelchile sind stark urbanisiert — 96,93 % der Menschen in der Hauptstadtregion und 91,56 % in der Region Valparaíso sind Stadtbewohner. Die 2 026 322 Menschen oder 13,41 % der Gesamtbevölkerung, die auf dem Land leben, arbeiten größtenteils in Landwirtschaft und Viehzucht. Sie konzentrieren sich auf Mittel- und Südchile; 33,59 % der Bevölkerung von Maule, 32,33 % von La Araucanía und 31,56 % von Los Lagos leben auf dem Land.
In den 1920er Jahren begann eine starke Abwanderung von Landbewohnern, die auf der Suche nach besseren Lebensbedingungen in die Städte zogen. Die Städte wuchsen dadurch schnell und bildeten große Ballungsräume. Der Großraum Santiago ist der größte derartige Ballungsraum. Im Jahr 2002 wohnten hier 5 428 590 Menschen oder mehr als ein Drittel aller Bewohner von Chile, während er 1907 383 587 und 1920 noch 549 292 Einwohner hatte, was 16 % der Bevölkerung Chiles entsprach. Das Städtewachstum führte auch dazu, dass vormals ländliche Gebiete Teile der Städte wurden, wie Puente Alto oder Maipú, die heute zu den bevölkerungsreichsten Gemeinden Chiles zählen. Im Januar 2015 nahm Santiago hinter São Paulo, Ciudad de México, Buenos Aires, Rio de Janeiro, Lima und Bogotá den siebenten Rang unter den lateinamerikanischen Städten mit den größten Bevölkerungszahlen ein; weltweit rangiert es auf 54. Stelle.

Analog zur Hauptstadt sind auch Valparaíso und Viña del Mar von starkem Bevölkerungswachstum betroffen gewesen. Sie sind dadurch mit Concón, Quilpué und Villa Alemana zum Großraum Gran Valparaíso zusammengewachsen. Auch Concepción, Talcahuano, Hualpén, Chiguayante, San Pedro de la Paz, Penco, Coronel, Lota, Hualqui und Tomé bilden ein Ballungsgebiet namens Gran Concepción. Beide Ballungsräume hatten im Jahr 2002 mehr als 660 000 Einwohner.

Weitere wichtige Städte und Ballungsräume sind die Agglomeration La Serena-Coquimbo (296 253 Einwohner im Jahre 2002), Antofagasta (285 255), Temuco-Padre Las Casas (260 878), Rancagua (236 363), Iquique-Alto Hospicio (214 586), Talca (191 154), Arica (175 441), Chillán-Chillán Viejo (165 528), Puerto Montt (153 118), Los Ángeles (138 856), Calama (136 600), Copiapó (134 531), Osorno (132 245), Quillota (128 874), Valdivia (127 750), Punta Arenas (116 005), San Antonio (106 101) und Curicó (104 124). Die Mehrzahl dieser Städte liegt entlang der Pazifikküste oder im Zentraltal in der Mitte Chiles zwischen Santiago und Puerto Montt.

Die chilenische Bevölkerung ist durch einen hohen Grad an Homogenität gekennzeichnet. Die Chilenen mit europäischen Vorfahren und Mestizen bilden rund 88,92 Prozent der Bevölkerung. 11,08 Prozent werden durch die indigene Bevölkerung gebildet. Davon sind 82 Prozent Mapuche, 6 Prozent Aymara, 2,5 Prozent Diaguita und 0,5 Prozent Rapanui. Chile hat bisher die Convention 169 der ILO welche die Rechte indigener Völker schützt nicht unterzeichnet .

Das Volk der Mapuche lebt überwiegend in der Region zwischen den Flüssen Bío-Bío und Toltén und besitzt dort einen Bevölkerungsanteil von 23 Prozent. Die Mapuche, früher zusammen mit anderen Völkern der Region auch unter der (von den Mapuche selbst abgelehnten) Sammelbezeichnung Araukaner bekannt, lassen sich in Picunche, Pewenche und Huilliche unterteilen. Ihre Sprache, das Mapudungun, wird seit wenigen Jahren als Ergänzungsfach in der Schule gelehrt und für eine tägliche Nachrichtensendung im lokalen Fernsehen auf "Canal 13 Temuco" verwendet. Trotz dieser Errungenschaften bleibt die traditionelle Lebensweise der Mapuche durch den Verlust ihres Landes und die liberale Wirtschaftsordnung gefährdet. Mapuche müssen oft in die Großstädte abwandern, um bezahlte Arbeit zu suchen.

Im nördlichen Teil Chiles leben kleinere Gruppen von Quechua, Aymara, Chango, Atacameño, Diaguita und Kolla. Im Süden Chiles lebten bis zum Anfang des 20. Jahrhunderts auch kleine Gruppen von Selk’nam, Kawéskar, Yámana, Caucahue sowie Tehuelche, deren Nachfahren in der heutigen Mehrheitsbevölkerung aufgegangen sind oder sich heute als Mapuche verstehen. Von den rund 5.000 Einwohners der Osterinsel sind ca. 40 Prozent, also etwa 2.000 Menschen, Polynesier (Rapanui).

Während der Kolonialzeit wurde Chile durch spanische Einwanderer besiedelt, die großteils aus den kastilischen Regionen Spaniens kamen. Im 19. Jahrhundert wanderten besonders viele englische und irische sowie deutsche Siedler nach Chile ein. Nennenswerte Einwandererkontingente kamen außerdem aus Frankreich, Italien, Kroatien und in jüngerer Zeit aus Palästina bzw. dem Nahen Osten. Die ersten Deutschen trafen 1843 ein und siedelten sich später vor allem im Gebiet um den Llanquihue-See und in Valdivia, Osorno sowie Puerto Montt an. Im Jahr 1913 nannte das Handbuch des Deutschtums im Ausland 30.000 in Chile lebende „Auslandsdeutsche“; 1916 betrug die Zahl der im Lande ansässigen Deutschchilenen und Chiledeutschen einer Zählung des Deutsch-Chilenischen Bunds zufolge etwa 25.500. Zuletzt gab es während und nach dem Zweiten Weltkrieg eine Zuwanderungswelle aus dem deutschsprachigen Raum. Noch heute wird die deutsche Sprache von bis zu 35.000 Einwohnern verwendet, deren Zahl allerdings stetig abnimmt.

Die Einfuhr schwarzer Sklaven nach Chile war zu allen Zeiten sehr gering. Die Mehrheit von ihnen konzentrierte sich auf die Städte Santiago de Chile, Quillota und Valparaíso. Im Laufe der Jahrhunderte vermischten sich die Schwarzen mit den Weißen und Mestizen, so dass heute das afrikanische Element in Chile fast völlig verschwunden ist. Eine Ausnahme bildet die Stadt Arica in der Provinz Tarapacá. Arica wurde 1570 gegründet und gehörte bis 1883 zu Peru. Die Stadt zählte zu den peruanischen Einfuhrzentren für afrikanische Sklaven. Von hier aus wurde auch ein großer Teil der bolivianischen Handelsgüter auf europäische Schiffe verladen. Arica lag mitten in der Wüste und bildete – dank der hervorragenden Anbaumöglichkeiten für Zuckerrohr und Baumwolle im Azapatal – eine Oase. Erdbeben, Piratenüberfälle und der Ausbruch von Malariaepidemien führten dazu, dass viele Weiße die Stadt verließen. So entwickelte sich mit der Zeit eine mehr oder weniger isolierte afro-chilenische Enklave. Chile erklärte sich 1811 als erster Staat in Südamerika gegen die Sklaverei und schaffte sie 1823 endgültig ab.

In den vergangenen Jahrzehnten suchten vermehrt Arbeiter aus Peru und Bolivien in Chile ihr Glück. In den 1980er Jahren gewann hierdurch bedingt die peruanischen Küche einen gewissen Einfluss in Chile. 2007 beschloss die Regierung eine Amnestie für diejenigen Ausländer, meistens aus Peru, die ohne Aufenthaltserlaubnis im Land arbeiteten. Die Wirtschaftskrise in Argentinien zwang seit der Jahrtausendwende vermehrt auch Argentinier zur Arbeitssuche im Nachbarland. Eine kleine Gruppe von Einwanderern kommt aus Asien, vor allem aus Korea, und lebt im Großraum Santiago.

Die bekannteste indigene Sprache ist das Mapudungun, die in Südchile gesprochene Sprache der Mapuche (ca. 250.000 Sprecher); daneben sind in Nordchile Aymara (ca. 20.000 Sprecher) und auf der Osterinsel Rapanui (ca. 1.000 Sprecher) verbreitet. Insgesamt werden in Chile neun verschiedene Sprachen und Idiome benutzt, darunter mindestens vier aussterbende Sprachen, für die nur noch einige wenige Sprecher bekannt sind. So wird für die Sprache der Yámana im Jahr 2013 noch ein einziger, 85-jähriger Sprecher genannt. Das in Peru und Bolivien verbreitete Quechua wird nur in den (früher peruanischen bzw. bolivianischen) chilenischen Nordprovinzen von einer nennenswerten Sprecherzahl verwendet. Vor allem in den südchilenischen Regionen IX und X leben zahlreiche Deutschchilenen, die teilweise noch Deutsch sprechen, sodass Deutsch die drittverbreitetste Sprache des Landes nach Spanisch und Mapudungun ist (genannt werden ca. 35.000 Sprecher).

Die Amtssprache und bei weitem überwiegende Alltagssprache Chiles ist Spanisch (in Chile "Castellano" genannt), wobei das in Chile gesprochene Spanische verschiedene Eigentümlichkeiten aufweist, welche Vokabular und Aussprache, die charakteristische Sprechmelodie und einzelne grammatikalische Besonderheiten betreffen. Zahlreiche in Chile verwendete Ausdrücke wurden aus einheimischen Sprachen (größtenteils dem Quechua und Aymara, nur selten aus dem Mapudungun) oder aus den Sprachen der Einwanderer übernommen (zum Beispiel "cachar" – von engl. "to catch" – oder "kuchen"). Von 1844 bis 1927 galt in Chile die an den Vorschlägen von Andrés Bello orientierte, von den Regelungen der Real Academia Española stark abweichende „amerikanische“ spanische Rechtschreibung.

In Chile herrscht wie in Lateinamerika generell der Seseo. Wie überall im lateinamerikanischen Spanischen fehlt auch eine grammatikalische zweite Person im Plural völlig; auch das dazugehörige Pronomen "vosotros/-as" („ihr“ als Plural-Anrede) ist unbekannt und die Anrede einer Mehrzahl von Personen erfolgt ausschließlich mit den Verbformen der dritten Person und dem Anredepronomen "ustedes". Auch im Singular wird die Anrede standardsprachlich in der dritten Person mit der Höflichkeitsform "Usted" gewählt. Die vertraute Anrede mit "tú" („Du“) ist auf den Kreis engster Freunde, Lebenspartner und gleichaltriger oder jüngerer Angehöriger beschränkt. Umgangssprachlich ist ein unvollständiger Voseo gebräuchlich, bei dem zur Anrede des Gegenübers in der zweiten Person Singular spezifische chilenische Voseo-Verbformen (zum Beispiel: „estái“ statt "estás", „querís“ statt "quieres", „venís“ statt "vienes", „vai“ statt "vas") eingesetzt werden, deren Bildungsweise an die Formen der zweiten Person Plural nach kontinentalspanischem Standard ("estáis", "queréis", "venís", "vais") erinnert. In deutlichem Kontrast zum Nachbarland Argentinien wird das entsprechende Personalpronomen "Vos" („Ihr“ als Singular-Anrede) in Chile jedoch gemieden und zumeist als vulgär oder despektierlich empfunden.

Das Land galt lange als sehr katholisch geprägt, auch wenn Staat und Kirche seit 1925 offiziell getrennt sind. Der kirchliche Einfluss auf das gesellschaftliche Leben, das Rechtswesen (besonders das Familienrecht) und die Kultur- und Medienwelt ist noch immer recht stark. So gehörte bis 2010 der zweitgrößte Privatsender des Landes, Canal 13, allein der römisch-katholischen Kirche. Jedoch sind eheliche und uneheliche Kinder seit 1998 rechtlich gleichbehandelt, das chilenische Eherecht sieht seit November 2004 eine Möglichkeit der Scheidung vor und 2015 wurde für gleich- und verschiedengeschlechtliche Paare die eingetragene Partnerschaft "(acuerdo de unión civil)" eingeführt. Abtreibungen sind seit 1990 verboten; eine Lockerung des absoluten Abtreibungsverbots in bestimmten medizinisch und ethisch indizierten Fällen wurde aber seit Jahren kontrovers diskutiert und 2017 verwirklicht.

Rund 70 Prozent der Bevölkerung (7.853.000 Befragte) rechneten sich bei der Volkszählung 2002 zur römisch-katholischen Kirche, die die zahlenmäßig stärkste Religionsgemeinschaft des Landes bildet. Die kirchliche Verwaltungsstruktur besteht aus fünf Kirchenprovinzen mit 26 Bistümern und 920 Pfarreien. Im Oktober 1999 wurde ein Gesetz zur Gleichstellung der Religionsgemeinschaften verabschiedet, das allerdings die Privilegien der römisch-katholischen Kirche unangetastet ließ und nur den Status der anderen Kirchen und Glaubensgemeinschaften verbesserte. Rund 15 Prozent der Chilenen gehörten 2002 protestantischen Glaubensgemeinschaften an; durch den weit verbreiteten pfingstlerischen Einfluss ist der Anteil der evangelischen Einwohner wie in ganz Lateinamerika vor allem in den letzten Jahrzehnten angestiegen (er lag in Chile 1930 bei 1,5 Prozent, 1992 bereits bei rund 13 Prozent). An weiteren Weltanschauungen wurden 8,3 Prozent Agnostiker und Atheisten genannt und 4,4 Prozent „Andere“, wozu auch die indigenen Religionen zählen (etwa die Religion der Mapuche). Kleinere Glaubensrichtungen bilden die Zeugen Jehovas (1,06 Prozent), Mormonen (0,92 Prozent), Juden (0,13 Prozent) und andere.

Neuere Befragungen ergaben, dass Chile zusammen mit Uruguay das am stärksten säkularisierte Land in Lateinamerika ist. Demnach entfielen auf die katholische Kirche 2013 noch 57 Prozent, auf die evangelischen Kirchen (hauptsächlich Evangelikale) 13 Prozent, während Religionsfreie seit 2011 mit 25 Prozent zu Buche schlagen. Auffällig ist der um 2010 abrupt einsetzende Rückgang der praktizierten Religiosität in Chile: Nur noch 27 Prozent der gläubigen Chilenen bezeichneten sich 2013 als praktizierend (2010: 41 %; 2011: 38 %), das ist das niedrigste Ergebnis in ganz Lateinamerika. Zugleich gehört Chile zu den lateinamerikanischen Ländern, in denen evangelikale Gruppierungen aktuell ein vergleichsweise geringes Wachstum verzeichnen. Obwohl der Untersuchungszeitraum der Studie bereits 2013 endete, halten die Autoren einen Vertrauenszuwachs der katholischen Kirche infolge des Amtsantritts von Papst Franziskus in allen Ländern einschließlich Chiles für erkennbar, ohne jedoch dessen Nachhaltigkeit einschätzen zu können. Die Anfang 2018 veröffentlichte Folgeuntersuchung desselben Instituts belegt eine ungebrochene Fortsetzung des Rückgangs auch nach 2013: Demnach bezeichnen sich heute nur noch 45 Prozent der Chilenen als Katholiken, sodass Chile nach Uruguay als zweites lateinamerikanisches Land keine mehrheitlich katholische Bevölkerung mehr besitzt. Bei der amtlichen Volkszählung 2017 bezeichneten sich allerdings noch 59 Prozent der Chilenen als katholisch.

Etwa 13.000 Jahre v. Chr. siedelten die ersten Menschen im heutigen Staatsgebiet Chiles (siehe Monte Verde). Später gehörte der Norden Chiles bis zu seiner Eroberung durch die Spanier kurzzeitig zum Inkareich. Im Jahr 1520 entdeckte der Portugiese Ferdinand Magellan während seines Versuches, die Erde zu umsegeln, die nach ihm benannte Magellanstraße, die an der heutigen Südspitze Chiles liegt. Die nächsten Europäer, die das heutige Chile erreichten, waren Diego de Almagro und seine Gefolgschaft, die 1535, von Peru kommend, nach Gold suchten, aber von der lokalen Bevölkerung zurückgetrieben wurden. Die erste permanente Siedlung der Europäer war das 1541 durch Pedro de Valdivia gegründete Santiago. Seit 1542 war Chile Bestandteil des spanischen Vizekönigreiches Peru.

Da die Spanier wenig Gold und Silber fanden, war Chile aufgrund seiner abgeschiedenen Lage eine eher wenig beachtete Kolonie der spanischen Krone. Die große Atacamawüste behinderte den direkten Weg nach Peru. Erst später wurde Chile durch landwirtschaftliche Produkte für die anderen spanischen Besitzungen ein wichtiger Versorgungspartner.

Chile beherbergte verschiedene Volksgruppen, die lange Zeit fälschlicherweise unter dem Begriff Araukaner zusammengefasst wurden. Im Süden leisteten die Mapuche in zahlreichen Kriegen erbitterten Widerstand. Der Konflikt, der als Arauco-Krieg ("Guerra de Arauco") bezeichnet wird, verhinderte eine spanische Besiedlung der südlichen Hälfte Chiles nachhaltig. Die meisten Städte, Ansiedlungen und Forts wurden kurz nach ihrer Errichtung von den Kampfverbänden der Ureinwohner überrannt und wieder zerstört. Ab 1602 bildete der Fluss Bío Bío faktisch die Grenze zum Mapuchegebiet. Der andauernde Widerstand der Ureinwohner zwang die Spanier 1641 zur Anerkennung einer unabhängigen Mapuche-Nation im Vertrag von Quillín. Darin wurde der Bío-Bío-Fluss als Grenze festgeschrieben und dem Volk der Mapuche Souveränität zugebilligt, ein in der Geschichte indigener Bevölkerungen in Südamerika einzigartiger Vorgang. Zwar kam es auch danach immer wieder zu kriegerischen Auseinandersetzungen und glücklosen Eroberungsversuchen, doch hatte die Grenzziehung im Wesentlichen bis zum Ende der Kolonialzeit Bestand. Erst im Rahmen der 1861 von Präsident José Joaquín Pérez ausgerufenen sogenannten „Befriedung Araukaniens“ wurde die Mapuche mit Hilfe chilenischer Truppen gewaltsam unterworfen und im Jahre 1883 an Chile angegliedert.

Neben den indianischen Angriffen behinderten schwere Erdbeben, Tsunamis und Vulkanausbrüche die Entwicklung des Landes. Viele Städte wurden komplett zerstört, wie beispielsweise Concepción 1570 und Valdivia 1575. Die chilenischen Küstenstädte waren im 16. und 17. Jahrhundert häufigen Angriffen englischer Piraten ausgesetzt.

1609 wurde das Generalkapitanat Chile gegründet, dieses war jedoch abhängig vom Vizekönigreich Peru. 1778 wurde Chile zum eigenständigen "Generalkapitanat" mit Handelsfreiheit innerhalb des spanischen Königreiches.

Der Drang nach Unabhängigkeit kam auf, als 1808 Spanien von Napoleons Bruder Joseph regiert wurde. Am 18. September 1810 wurde eine Junta ins Leben gerufen, die die Treue Chiles zum abgesetzten König Ferdinand VII. erklärte, und zwar als eine autonome Provinz innerhalb des spanischen Königreichs. Dieses Datum feiert man in Chile als den Beginn der Unabhängigkeit. Wenig später erklärte Chile seine Loslösung von Spanien und der Monarchie.

1814, nach dem Ende des Spanischen Unabhängigkeitskrieges und der Niederlage der Patrioten in der Schlacht von Rancagua, übernahm Spanien wieder die Macht in Chile. Die Spanier wurden aber in der Schlacht von Chacabuco durch ein chilenisch-argentinisches Heer unter General José de San Martín geschlagen. In der Schlacht von Maipú 1818 brach die spanische Kolonialherrschaft endgültig zusammen. San Martín verzichtete zugunsten von Bernardo O’Higgins auf das Präsidentenamt.

O’Higgins selbst wurde gestürzt und ging 1823 ins Exil nach Peru. Sein Nachfolger Ramón Freire y Serrano konnte seine politische Macht nicht richtig festigen und wurde von Francisco Antonio Pinto Díaz 1828 gestürzt. Dieser führte eine liberale Verfassung ein, was den Zorn der Konservativen hervorrief. Am 17. April 1830 stürzte Diego Portales Palazuelos in der Schlacht von Lircay die Regierung. Portales regierte (indirekt, denn er wurde nie Präsident) bis August 1831 mit diktatorischen Mitteln. Im Jahre 1833 entstand mit Hilfe Portales eine streng präsidiale Verfassung. Diese stark zentralistische Verfassung gewährte Chile eine lange Zeit der Stabilität, bis zum Bürgerkrieg von 1891.

Von 1836 bis 1839 kam es zum Konföderationskrieg gegen Bolivien und Peru, den die Chilenen gewannen.

Am 17. September 1865 erklärte Chile Spanien den Krieg (Spanisch-Südamerikanischer Krieg), nachdem Spanien versucht hatte mit militärischen Mitteln in Peru Einfluss zu gewinnen. Es kam daraufhin zu den Seegefechten bei Papudo sowie bei Abtao vor der Insel Chiloé. Am 5. Dezember 1865 verbündete sich auch Peru mit Chile, um den gemeinsamen Gegner zu bekämpfen. Die Spanier beschossen am 31. März 1866 die Stadt Valparaíso massiv. Der Konflikt mit Spanien konnte aber erst in Verträgen von 1871 und 1883 endgültig gelöst werden.

Im Verlauf des 19. Jahrhunderts wanderten verstärkt auch nicht-spanische Europäer nach Chile ein, darunter Deutsche, deren Spuren noch heute vor allem im südlichen Mittelteil des Landes zu sehen sind (Valdivia, Osorno, Puerto Montt, Puerto Varas, Frutillar, Puerto Natales).

Im Salpeterkrieg von 1879 bis 1884 besetzte Chile die bis dahin zu den Nachbarländern Peru und Bolivien gehörende Atacamawüste, Lima und Teile der Pazifikküste von Peru. Im Friedensvertrag von 1904 zwischen Chile und Bolivien übergab Bolivien an Chile seinen freien Zugang zum Pazifik. In den eroberten Gebieten wurden später große Kupfervorkommen gefunden: Chuquicamata, der größte Kupfertagebau der Welt, befindet sich in diesem Gebiet.

Peru übergab an Chile im Vertrag von Ancón die heutigen Regionen von Arica, Parinacota und Tarapacá als Reparationen.

1891 widersetzten sich Parlament und Marine dem Präsidenten José Manuel Balmaceda; es kam zum Bürgerkrieg. In diesem Konflikt starben rund 6000 Menschen. Balmaceda verlor zwei größere Schlachten und beging am 18. September 1891 Selbstmord. Das bis dahin präsidial geprägte Regierungssystem wurde nach dem Sieg der Kongressanhänger durch ein parlamentarisches System ersetzt, bis 1925 wiederum ein präsidentielles Regierungssystem eingeführt wurde. Während der Unruhen kam es zum Baltimore-Zwischenfall, der zu einem diplomatischen Konflikt zwischen der neuen chilenischen Regierung und den USA führte.

Trotz des Grenzvertrags mit Argentinien (1881) verschärften sich ab 1893 die Grenzstreitigkeiten mit Argentinien, weil der Vertrag die Andenkordillere als Grenze bestimmte: Die Grenzlinie verlaufe „über die höchsten Berge, die die Wasserscheide bilden“. Auf manchen Abschnitten führte diese Definition zu strittigen Ergebnissen. Im Norden tauschte Bolivien einen Teil der "Puna" gegen Tarija mit Argentinien, nachdem Chile die Puna-Region im Salpeterkrieg besetzt hatte. Zwischen Chile und Argentinien kam es zu einem Wettrüsten. Erst durch ein Schiedsgerichtsverfahren konnte der Grenzstreit 1902 beigelegt werden. Patagonien und Feuerland wurden neu aufgeteilt, dabei fielen 54.000 Quadratkilometer an Chile und 40.000 Quadratkilometer an Argentinien. Der Grenzverlauf mit Bolivien wurde 1904 mit einem im gegenseitigen Einvernehmen geschlossenen Friedensvertrag festgelegt. Doch bald keimte in Bolivien ein Revisionismus auf, der bis heute eine schwierige und oft sehr angespannte politische Situation zwischen den beiden Ländern verursacht. In den 1970er Jahren, als beide Länder durch Militärdiktaturen regiert wurden, wurde von chilenischer Seite angeboten, einen zirka 10 km breiten Gebietsstreifen entlang der Grenze mit Peru an Bolivien abzutreten, um endgültig Frieden zu schaffen. Der Vorschlag wurde nicht umgesetzt, weil Bolivien keine Kompensation dafür geben wollte. Bolivien versucht zur Zeit einen Anspruch auf einen souveränen Zugang zum Meer mit einer Klage vor dem Internationalen Gerichtshof in Den Haag durchzusetzen.

Chile blieb im Ersten Weltkrieg neutral, die innenpolitische Lage war aber weiterhin instabil. Präsident Arturo Alessandri Palma, der in Chile ein Sozialversicherungssystem eingeführt hatte, wurde 1924 durch einen Militärputsch abgesetzt, kam aber, nach der Einführung einer neuen Verfassung im Jahre 1925, im März 1926 wieder an die Macht. Bis 1932 regierte (am längsten) Carlos Ibáñez del Campo das Land mit diktatorischen Mitteln. 1932 wurde die verfassungsmäßige Ordnung wiederhergestellt und die Radikalen erwiesen sich in den folgenden 20 Jahren als führende Partei.

Die Weltwirtschaftskrise um 1930 traf Chile besonders hart. Die Preise für die wichtigsten Exportgüter Kupfer und Salpeter verfielen dramatisch. Ab den 1930er Jahren folgte eine langsame Erholung des Landes, die 1938 durch einen Putschversuch der Nationalsozialistischen Bewegung Chiles und das darauffolgende Massaker unterbrochen wurde.

1934 kam es zu einer letzten großen Bauernrebellion in Ranquil, die durch Polizeikräfte niedergeschlagen wurde.

Nachdem Chile lange Zeit – auch aus Rücksicht auf die zahlreichen deutschstämmigen Chilenen – im Zweiten Weltkrieg neutral geblieben war, beschloss 1944 Präsident Juan Antonio Ríos Morales, an der Seite der Alliierten in den Krieg einzutreten. Der Einfluss Chiles auf den Kriegsausgang blieb jedoch unbedeutend.

1945 gehörte das Land zu den Gründungsmitgliedern der Vereinten Nationen und trat 1948 der OAS bei. Das Frauenwahlrecht wurde 1949 eingeführt.

Am 2. August 1947 ernannte Präsident González Videla ein Kabinett aus Militärs und Unabhängigen. Finanzminister dieses Kabinetts war Jorge Alessandri, der 1958 unter Mithilfe der Konservativen, der Liberalen und der Radikalen Partei Präsident Chiles wurde. Er gewann die Präsidentschaftswahl gegen Salvador Allende, den Kandidaten der Vereinigten Linken.

Großer Gegenspieler der Konservativen wurden die Christdemokraten, die zwar strikt antikommunistisch, nach europäischen Maßstäben aber in Fragen der Sozialpolitik gemäßigt links eingestellt waren.

Am 22. Mai 1960 erschütterte das bisher stärkste gemessene Erdbeben der Welt mit anschließendem Tsunami die Küsten Chiles und verwüstete besonders die Hafenstadt Valdivia. Das Beben hatte die Stärke 9,5 auf der Richterskala. Mehr als 2000 Menschen starben, was in Folge Ablenkung von innenpolitischen Problemen schaffte. Weite Teile des Landes waren immer noch in den Händen einiger weniger vermögender Familien.

1964 gewann Eduardo Frei Montalva als Kandidat der Christdemokratischen Partei die Wahl zum Präsidenten, auch mit Wahlhilfe aus den USA. Er versuchte unter dem Motto "„Revolution in Freiheit“", Sozialreformen mit dem Erhalt der demokratischen Ordnung zu verbinden und den Spagat zwischen den radikalen Forderungen der Linken und der rigorosen Abwehr von Reformen durch die Rechten zu schaffen. Eine Landreform verteilte über drei Millionen Hektar Großgrundbesitz an Bauerngenossenschaften. Frei scheiterte letztlich mit seinen wichtigsten Reformen, darunter der teilweisen Verstaatlichung der Kupferindustrie. 1969 trat Chile als Gründungsstaat der Andengemeinschaft bei, allerdings 1976 wieder aus.

Wie schon 1958 hieß auch 1970 der Gegner von Jorge Alessandri im Wahlkampf um das Präsidentenamt Salvador Allende, dem die Gewerkschaften und Sozialisten zur Seite standen. Allende gewann die Wahl und wurde Präsident.

Die Kräfte der Linken bildeten 1969 die Unidad Popular (UP), ein Wahlbündnis, dem neben der Kommunistischen und der Sozialistischen Partei kleine humanistische, linkschristliche und marxistische Parteien angehörten. Die UP vertrat eine sozialistische Linie, warb für die Verstaatlichung der Industrie und die Enteignung der Großgrundbesitzer. Dieses Bündnis stellte 1970 als Präsidentschaftskandidaten Salvador Allende auf, der schon zum vierten Mal kandidierte.

Aus den Wahlen von 1970 ging das linke Wahlbündnis Unidad Popular mit 37 % der Stimmen als stärkste Kraft hervor und Salvador Allende wurde zum Präsidenten gewählt. Sein konservativer Gegner, Jorge Alessandri, kam auf 35,3 %, und der Christdemokrat Radomiro Tomic erzielte 28,1 %. Stichwahlen waren in der damaligen Verfassung nicht vorgesehen. Allende wurde im Parlament mit den Stimmen der Christdemokraten (um Tomic) unter der Voraussetzung, er werde sich streng an die Verfassung und Rechtsstaatlichkeit halten, als Präsident gewählt. Er verstaatlichte in der Folge die wichtigsten Wirtschaftszweige (Bankwesen, Landwirtschaft, Kupferminen, Industrie, Kommunikation) und geriet dadurch in wachsende Konflikte mit der Opposition – obwohl die Verstaatlichungen von der Verfassung gedeckt waren. Zudem stieß der Wahlsieg Allendes in den USA auf heftigen Widerstand.

Mit dem Sieg der „Volksfrontregierung“ unter marxistischem Einfluss in Chile war nach Kuba der zweite amerikanische Staat sozialistisch regiert. Dies schien die 1954 von US-Präsident Eisenhower postulierte Domino-Theorie zu bestätigen, wonach die Länder Südamerikas nach und nach wie Dominosteine dem Kommunismus anheimfallen würden. US-Außenminister Henry Kissinger ließ, als der Sieg der linken Kräfte absehbar war, verlauten: „Ich sehe nicht ein, weshalb wir zulassen sollen, dass ein Land marxistisch wird, nur weil die Bevölkerung unzurechnungsfähig ist.“ Allende betrachtete sich nicht als Marxist und lehnte sowohl die Diktatur des Proletariats als auch ein Einparteiensystem entschieden ab.

Bei seinem Amtsantritt hatte Allende also mit Sanktionen und Gegenmaßnahmen der USA zu rechnen. So kam es bereits 1970 zu einem tödlichen Attentat auf General René Schneider, bei dem die CIA und Außenminister Kissinger massiv beteiligt waren (siehe US-Intervention in Chile). Schneider war für die US-Regierung ein Hindernis, da er gegen einen Militärputsch war.

Durch den Boykott der USA, der westeuropäischen Staaten und der internationalen Konzerne wurde das politische System derart labil, dass von Teilen des Militärs ein Putsch geplant wurde. Ein erster Putsch des 2. Panzerregiments scheiterte im Juni 1973.

Am 11. September 1973 kam es schließlich zu einem blutigen Militärputsch gegen die Regierung. Präsident Allende beging in der Moneda Selbstmord. Hunderte seiner Anhänger kamen in diesen Tagen ums Leben, Tausende wurden inhaftiert. Sämtliche staatlichen Institutionen in ganz Chile wurden binnen Stunden vom Militär besetzt. Die Macht als Präsident einer Junta übernahm General Augusto Pinochet.

Überall im Lande errichtete das Militär in der Folgezeit Geheimgefängnisse, wo Oppositionelle und deren Sympathisanten nicht selten zu Tode gefoltert wurden. Tausende Chilenen gingen wegen der fortgesetzten Menschenrechtsverletzungen ins Exil (→ Folter in Chile).

Kurz nach der Machtübernahme Pinochets begannen die USA und die westeuropäischen Staaten, Chile wieder intensiv mit Wirtschaftshilfe zu unterstützen. Die Militärregierung machte die Verstaatlichungen Allendes mit Ausnahme der Kupferminen rückgängig, führte radikale Wirtschaftsreformen durch und schaffte die Gewerkschaftsrechte ab.

Im Jahr 1976 ernannte die Militärregierung den Papierfabrikanten und früheren Präsidenten Chiles, Jorge Alessandri, zum Präsidenten des neu gegründeten Staatsrates ("Consejo de Estado"), dessen Aufgabe es war, eine neue Verfassung zu schreiben, um die Militärdiktatur zu legitimieren.

In Deutschland erhielt die Regierung Pinochets lange Zeit Unterstützung aus den Reihen der Unionsparteien, vor allem der CSU. So lobte Franz Josef Strauß 1977 bei seinem Besuch den Umsturz als „gewaltigen Schlag gegen den internationalen Kommunismus“. Es sei „Unsinn, davon zu reden, daß in Chile gemordet und gefoltert würde“. Die Auseinandersetzung um die Bezeichnung der Militär-Junta als „Mörderbande“ durch den der SPD angehörigen Forschungsminister Hans Matthöfer anlässlich eines Streits um Wirtschaftshilfe im Jahr 1975 steht exemplarisch für die Gespaltenheit der deutschen Politik in dieser Frage. In den achtziger Jahren wurde auch in der CDU die Kritik an den Menschenrechtsverletzungen des Regimes deutlicher. In diese Zeit fällt auch der Chilebesuch von Norbert Blüm, bei dem dieser Pinochet im direkten Gespräch damit konfrontierte.

Insbesondere in der Colonia Dignidad, einer streng bewachten Siedlung von Auslandsdeutschen unter Führung von Paul Schäfer, wurde gefoltert. Die Sekte beziehungsweise totalitäre Religionsgemeinschaft war etwa zehn Jahre vor der Machtübernahme Pinochets gegründet worden und diente während der Militärherrschaft als Folterzentrum für die chilenischen Geheimdienste. Darüber entwickelt sich die Colonia zu einem florierenden Konzern, der unter anderem Titan nach Deutschland exportierte. Trotz Hinweisen, gerichtlichen Anklagen und Fluchtversuchen deutscher Bürger übte die deutsche Botschaft in Chile „äußerste Zurückhaltung“ und blieb untätig, mehr noch, sie ließ Handwerker der Siedlung die Botschafterresidenz renovieren.

Im Dezember 1978 verschärfte sich der Beagle-Konflikt mit Argentinien und es kam zu kriegerischen Drohungen gegen Chile. Die unbewohnten Inseln Lennox, Picton und Nueva im Beagle-Kanal wurden zum Streitpunkt, vor allem weil in der Gegend größere Ölreserven vermutet wurden. Der Streit erreichte seinen gefährlichsten Höhepunkt am 22. Dezember 1978, als Argentinien die Operation Soberanía startete, um die Inseln militärisch zu besetzen und in Kontinental-Chile einzumarschieren. Der Einmarsch wurde gestoppt, als die Junta in Buenos Aires einer päpstlichen Vermittlung zustimmte. Diese Mediation führte nach der Niederlage Argentiniens im Falklandkrieg zu dem Freundschafts- und Friedensvertrag von 1984 zwischen Chile und Argentinien, bei dem alle drei Inseln Chile zugesprochen wurden. Die fast abschließende Grenzziehung mit Argentinien am Fitz-Roy-Massiv wurde am 16. Dezember 1998 vereinbart. Es bleibt bis heute nur noch ein kleiner undefinierter Abschnitt im Bereich des Campo de Hielo Sur („Südliches Eisfeld“) übrig. Dieser Bereich beherbergt das größte Süßwasserreservoir Südamerikas.

Infolge des früheren Konflikts mit Argentinien unterstützte Chile während des Falklandkrieges 1982 Großbritannien. So landete ein beschädigter britischer Hubschrauber in Chile. Bisher ist der Grund seines Aufenthaltes in dieser Region allerdings unbekannt. Des Weiteren half Chile Großbritannien mit Radar- und Spionagetätigkeit. Der chilenische Ex-Luftwaffenchef Fernando Matthei bestätigte später die geheime Kooperation.

1988 wurde eine Volksabstimmung abgehalten, bei der sich eine Mehrheit (55 %) gegen eine weitere Amtszeit Pinochets aussprach. 1989 fanden die ersten freien Wahlen nach 15-jähriger Diktatur statt, Präsident wurde der Christdemokrat Patricio Aylwin. Bereits wenige Monate nach der Rückkehr zur Demokratie setzte der neugewählte Präsident Mitte 1990 eine Wahrheits- und Versöhnungskommission ein. Sie sollte die zwischen 1973 und 1989 begangenen politischen Morde und den Verbleib von Verschwundenen (Desaparecidos) aufklären. Neu und für spätere Wahrheitskommissionen in postdiktatorischen Demokratien des Ostblocks und Afrikas während der neunziger Jahre prägend war dabei, dass die chilenische Kommission „Wahrheit“ über Verbrechen während der Zeit der Diktatur als ihr Ziel definierte. Mithilfe einer „offiziellen Wahrheit“ sollte die Spaltung der chilenischen Gesellschaft in zwei Lager mit jeweils unterschiedlichen Deutungen der Geschichte überwunden werden.

Aylwin setzte die neoliberale Wirtschaftspolitik Pinochets fort und bemühte sich, die verfeindeten politischen Lager zu versöhnen, um ein demokratisches Zusammenleben zu ermöglichen. Behutsam („Gerechtigkeit soweit es geht“) begann er mit der Aufarbeitung der Verbrechen der Militärdiktatur: Im November 1993 standen erstmals Offiziere wegen Menschenrechtsverletzungen vor Gericht. Viele Exilanten kehrten zurück in ihre Heimat. Von 1994 bis 2000 regierte der Christdemokrat Eduardo Frei Ruiz-Tagle.

Pinochet trat 1998 als Heereschef ab, blieb aber Senator auf Lebenszeit und genoss daher Immunität. Im gleichen Jahr wurde er in Großbritannien aufgrund eines Haftbefehls des spanischen Richters Baltasar Garzón verhaftet, konnte aber 1999 aus gesundheitlichen Gründen nach Chile zurückkehren. 1998 wurde er von Gladys Marín vor dem chilenischen Richter Juan Guzmán Tapia angeklagt, 2002 jedoch wegen leichter Demenz als verhandlungsunfähig erklärt, worauf Pinochet auf sein Amt als Senator verzichtete. Weitere Versuche, ihn gerichtlich zu belangen, scheiterten. Er starb am 10. Dezember 2006, ohne je verurteilt worden zu sein.

Im Jahr 2000 wurde der Sozialist Ricardo Lagos neuer chilenischer Präsident. Er bezwang in einer Stichwahl seinen konservativen Gegner Joaquín Lavín nur knapp. Mit Lagos zog nach Allende der zweite sozialistische Präsident in die Moneda ein. Lagos machte die Bekämpfung der Arbeitslosigkeit zum Ziel seiner Regierung. Sein Programm sah außerdem die Wiedereinführung der Tarifautonomie und die Einbindung des Armee-Budgets in den staatlichen Haushalt vor. Lagos verließ im Jahr 2006 das Amt mit einer rückwirkend wirtschaftlich und politisch positiven Bilanz. Als Nachfolgerin wurde die Sozialistin Michelle Bachelet zur ersten Präsidentin in der Geschichte des Landes gewählt.

2010 gewann Sebastián Piñera nach einer Stichwahl gegen seinen Konkurrenten Frei die Präsidentschaftswahl. Am 11. März 2011 war sein Amtsantritt. Piñera war der erste rechtsgerichtete Präsident nach fast 20 Jahren.

Am 15. Dezember 2013 wurde in einem zweiten Wahlgang wieder die Sozialistin Michelle Bachelet zur Präsidentin gewählt. Bachelet setzte sich mit rund 62,2 Prozent der Stimmen gegen die konservative Herausforderin Evelyn Matthei durch.

Chile ist eine Präsidialrepublik. Die Verfassung wurde am 16. August 2005 vom chilenischen Parlament geändert.

Der Präsident, nach US-amerikanischem Vorbild zugleich Regierungschef, wird für eine vier Jahre andauernde Amtszeit gewählt. Der Präsident kann zwar mehrere Amtszeiten absolvieren, jedoch nicht direkt hintereinander. Er ernennt die Minister (2005: 18 Minister) und Subsekretäre (Vergleichbar mit Staatssekretären; 2005: 30) sowie die Regional-Intendanten (einen für die Hauptstadtregion und je einen für die Regionen) und Provinzgouverneure (je Provinz einer). Er kann innerhalb eines durch die Verfassung festgelegten Rahmens Dekrete erlassen, die Gesetzeskraft haben. Zudem kann er die obersten Befehlshaber der Teilstreitkräfte ernennen.

Die Legislative (Congreso Nacional) besteht aus zwei Kammern. Der erste chilenische Kongress wurde am 4. Juli 1811 durch Beschluss (1810) der Regierungs-Junta gebildet.

Die Abgeordnetenkammer ("Cámara de Diputados") besteht aus 120 durch direkte Wahl ermittelten Abgeordneten. Das ganze Land wird in 60 Wahlkreise eingeteilt, in denen alle vier Jahre jeweils zwei Abgeordnete gewählt werden. Das erstplatzierte Parteibündnis stellt jedoch beide Abgeordnete, wenn es doppelt so viele Stimmen wie das oppositionelle Wahlbündnis erreicht. Dieses binomiale Wahlsystem verhindert, dass kleinere Parteien ins Parlament gewählt werden.

Der Senat ("Senado") umfasst 38 Mitglieder. Aufgrund der Verfassungsreform, die am 16. August 2005 beschlossen wurde, werden seit dem 11. März 2006 alle Senatoren direkt von den Wahlbürgern gewählt. Die gewählten Senatoren stammen aus 19 Wahlbezirken. Jede der zwölf Regionen und die Hauptstadtregion besitzen mindestens einen Wahlbezirk. Die V., VII., VIII., IX. und X. Region sowie die Hauptstadtregion werden in jeweils zwei Wahlbezirke aufgeteilt. Alle vier Jahre wird jeweils die Hälfte der Senatoren für eine Amtszeit von acht Jahren gewählt.

Der Oberste Gerichtshof ("Corte Suprema de Justicia") ist ein Kollegialgericht mit 21 Richtern. Es ist die höchste richterliche Gewalt in Chile. Die Richter werden von den Richtern des Obersten Gerichts vorgeschlagen und vom Präsidenten auf Lebenszeit ernannt. Unter dem Obersten Gerichtshof ist das Appellationsgericht angesiedelt. Zusätzlich gibt es 17 Berufungsgerichte in Chile.

Durch eine Justizreform wurden die Aufgaben des Anklägers (Staatsanwalt) und des Richters getrennt. Im Zuge dieser Reform werden Gerichtsverfahren nun öffentlich und mündlich geführt, statt wie zuvor üblich schriftlich. Angeklagte mit geringem Einkommen können einen staatlichen Pflichtverteidiger in Anspruch nehmen. Für dieses neue Justizsystem mussten 300 neue Gerichtsgebäude in zahlreichen chilenischen Städten gebaut werden.

Das Verfassungsgericht "(Tribunal Constitucional)" ist zuständig für die Kontrolle der vom Parlament erlassenen Gesetze auf Verfassungswidrigkeit.

Chile besitzt trotz präsidialer Verfassung eine für Lateinamerika ungewöhnlich starke parteiendemokratische Tradition. Parteien wurden bereits in der Endphase der Militärdiktatur ab 1987 wieder zugelassen. Das gegenwärtige Wahlrecht hat dazu geführt, dass sich alle Parteien zu Parteibündnissen zusammengeschlossen haben. Im Demokratieindex 2016 belegt Chile Platz 32 von 167 Ländern, womit es als eine „unvollständige Demokratie“ gilt. Nach Beendigung der Diktatur (1973–1990) erreicht das Land inzwischen eine der besten Platzierung in Lateinamerika.

Die „Alianza por Chile“ war ein konservatives Bündnis, das aus der „Nationalen Erneuerungspartei“ (Renovación Nacional, RN) und der Rechtspartei „Unabhängige Demokratische Union“ (Unión Demócrata Independiente, UDI) bestand, die in der Transitionszeit beide für eine Verlängerung der von Augusto Pinochet begründeten autoritären Regierungsform eintraten. Zwischenzeitlich gehörten dem Bündnis auch einige andere rechtsliberale und konservative Parteien an. Das Bündnis ging siegreich aus den Präsidentschaftswahlen 2009 hervor und stellte mit Sebastián Piñera den ersten konservativen Präsidenten Chiles nach dem Ende der Militärdiktatur. Unter seiner Präsidentschaft wurde es umbenannt in "Coalición por el Cambio", später "Alianza por el Cambio", und 2015 schließlich durch das neue Rechtsbündnis Chile Vamos ersetzt.

Die „Concertación de Partidos por la Democracia“ war ein Bündnis aus vier Mitte-Links-Parteien, die sich aktiv am Sturz der Militärdiktatur beteiligt hatten. Mitglieder waren die Parteien „Christlich-Demokratische Partei“ (Partido Demócrata Cristiano, PDC), „Radikale und Sozialdemokratische Partei“ (Partido Radical Social Demócrata, PRSD), „Sozialdemokratische Partei“ (Partido por la Democracia, PPD) sowie „Sozialistische Partei“ (Partido Socialista, PS). Das Bündnis, das nach dem Rückzug der Militärs bis zum Amtsantritt Sebastián Piñeras im Frühjahr 2010 ununterbrochen die Regierung stellte, wurde nach der erneuten Wahl der Sozialistin Michelle Bachelet 2013 aufgelöst und durch das neue Linksbündnis Nueva Mayoría ersetzt.

Das Linksbündnis "Juntos Podemos Más" („Gemeinsam können wir mehr“) umfasste die Christliche Linke, die Humanistische Partei, die Kommunistische Partei sowie einige andere linke und linksliberale Splitterparteien. 2010 konnte "Juntos Podemos Más" zwei kommunistische Abgeordnete ins Parlament senden. Die Mehrzahl der Mitgliedsparteien bildete anschließend zusammen mit den Parteien der "Concertación" das neue Bündnis der "Nueva Mayoría".

Die Streitkräfte Chiles () bestehen aus den Teilstreitkräften Heer, Marine, Luftwaffe und der nationalen Polizei (Carabineros de Chile). Im Jahre 2008 umfassten die Streitkräfte der Republik Chile insgesamt 77.300 Soldaten.

Chile ist Mitglied der APEC und versucht momentan mit möglichst vielen Staaten Freihandelsabkommen zu schließen (zum Beispiel gibt es Abkommen mit den USA, der EU, Südkorea und China).

Chile ist seit 1945 Mitglied der UN und seit 1948 Mitglied der OAS. In der UN spielt Chile seit 2004 eine wichtigere Rolle, da es sich zur Teilnahme an Friedensmissionen entschlossen hat. Heute stehen chilenische UN-Einheiten zum Beispiel in Haiti.

Im Mai 2007 lud die OECD Chile zu Beitrittsgesprächen ein und der Beitritt wurde am 7. Mai 2010 vollzogen.

In Südamerika ist Chile assoziiertes Mitglied des Mercosur; diese Nichtvollmitgliedschaft erlaubt Chile, eigene Handelsabkommen zu schließen. Die Konzentration Chiles auf die großen Handelspartner USA, EU und Asien wird von den anderen Andenstaaten kritisch gesehen. Man befürchtet eine Vernachlässigung des lateinamerikanischen Marktes. Insbesondere die vielen Freihandelsabkommen Chiles, aber auch die niedrigen Einfuhrzölle Chiles machen eine engere Bindung an Mercosur schwer.

Chile hat seit 1988 eine Reihe von Konfliktherden mit Argentinien und Peru abgebaut. Dies betrifft den Beagle-Kanal und die Grenzziehung am Fitz-Roy-Massiv. Seitdem der peruanische Kongress im Oktober 2005 maritime Gebiete Chiles in Frage stellt, sind allerdings wieder starke Spannungen im Verhältnis beider Länder vorhanden.

Die Beziehungen zu Bolivien sind weiterhin stark gestört, da der Wunsch Boliviens nach einem Meerzugang bisher ungelöst ist sowie ein Konflikt um Wasserrechte am Río Lauca besteht. Eine geplante Erdgas-Pipeline von Bolivien zu chilenischen Häfen, um Flüssigerdgas in die Vereinigten Staaten zu exportieren, wurde durch den starken Widerstand in der bolivianischen Bevölkerung nicht gebaut.

Im Zuge der geglückten Rettung von Bergleuten nach dem Grubenunglück von San José trafen sich im Oktober 2010 die Präsidenten beider Länder am Unglücksort. Boliviens Präsident Morales dankte den Chilenen für die Rettung eines Bolivianers, der zu den verschütteten Kumpeln gehörte. Die Präsidenten vereinbarten erstmals seit Jahrzehnten den gegenseitigen Austausch von Botschaftern.

Chile war bis zur Fertigstellung der LNG-Terminal von Quintero stark von Erdgaslieferungen aus Argentinien abhängig. Die Drosselung der Lieferungen von Seiten Argentiniens zwang Chile zum verstärkten Nachdenken über alternative Energien. Das Erdgas-Lieferabkommen zwischen Bolivien und Argentinien verbietet den Argentiniern den Export von bolivianischem Erdgas nach Chile.

Die Militärdiktatur unter Augusto Pinochet hatte stets enge Beziehungen zu den USA, die den Sturz der demokratisch gewählten Linksregierung unter Salvador Allende 1973 aktiv gefördert hatten. Auch nach dem Ende der Diktatur bestehen gute Beziehungen zwischen beiden Ländern. Allerdings lehnte die chilenische Regierung ein Eingreifen im Irakkrieg ab. Chile konnte am Ende langer Verhandlungen mit der Stimme der USA den Posten des Generalsekretärs der OAS mit José Miguel Insulza einnehmen.

Die USA sind der wichtigste Handelspartner für Chile, beide Länder haben 2004 ein Freihandelsabkommen geschlossen. Allerdings sinkt der Anteil des US-Handels zugunsten der EU und Asiens. Chile hat 2002 moderne Kampfflugzeuge von Typ F-16 in den USA bestellt.

Die Europäische Union ist neben den USA ein sehr wichtiger Handelspartner. 2005 trat ein Assoziierungsabkommen zwischen der EU und Chile sowie ein Abkommen über die technisch-wissenschaftliche Zusammenarbeit in Kraft.

Es bestehen enge Verbindungen zwischen beiden Ländern, die Schwerpunkte liegen in der Politik, Wirtschaft und besonders in der Wissenschaft. Viele Chilenen pflegen einen besonderen Bezug zu Deutschland, was zum Teil auf historische Bindungen unter Deutschchilenen, zum Teil auf Exilerfahrungen während der Diktatur zurückzuführen ist. Zur Zeit der Herrschaft von Augusto Pinochet flohen Regimegegner häufig in die DDR ins Exil. Auch die gegenwärtige Präsidentin des Landes, Michelle Bachelet, verbrachte einige Jahre im ostdeutschen Exil, lernte Deutsch an der Universität Leipzig und studierte Medizin an der Humboldt-Universität Berlin. Auch in der Vorgängerregierung unter Präsident Sebastián Piñera gab es mehrere Minister, die deutsche Schulen besucht oder in Deutschland studiert haben. Hinzu kommt, dass in der Zeit zwischen dem Salpeterkrieg und dem Ersten Weltkrieg deutsche Ausbilder die Streitkräfte Chiles formten, die bis heute stark von preußisch-deutschen Traditionen geprägt sind, was sich etwa in der Uniformierung und im militärischen Liedgut äußert.

Seit 2002, als der damalige Präsident Ricardo Lagos eine Reform des Ausbildungssystems auf den Weg brachte, gibt es eine Schulpflicht. Diese ist auf zwölf Jahre begrenzt. Die Schulen unterstehen dem Erziehungsministerium. Es herrscht Lehrmittelfreiheit. Die Alphabetisierung liegt bei 97,3 %  Prozent, dies ist für Südamerika sehr hoch. Im Jahr 2006 hat Chile erstmals an der PISA-Studie der OECD teilgenommen.

Chile führte in den 1990er-Jahren das Programm „PRADJAL“ ("Programa Regional de Acciones para el Desarrollo de la Juventud en América Latina") sowie das Ausbildungsprogramm „Chile Joven“ ein. Ziel der Programme ist die Senkung der Jugendarbeitslosigkeit durch eine staatlich finanzierte Berufsausbildung mit anschließendem Betriebspraktikum. Außerdem werden Kurse für jugendliche Unternehmensgründer angeboten. Damit soll auch die Jugendkriminalität und der Drogenkonsum indirekt bekämpft werden.

Die wichtigsten Universitäten wie die Pontificia Universidad Católica de Chile liegen in Santiago de Chile, Concepción und Valparaíso. Allerdings ist der Zugang zu den Universitäten aufgrund hoher Studiengebühren trotz Stipendienprogrammen für die ärmeren Schichten nur schwer möglich.

Das Niveau der Universitäten streut durch viele private Einrichtungen stark, da sich auch die Berufsakademien Universität nennen dürfen. In letzter Zeit formieren sich in Chile Studentenproteste, die u. a. von Camila Vallejo angeführt worden sind. Unter anderem werden bessere Kreditbedingungen verlangt, damit auch Jugendliche aus unteren, ärmeren Schichten Zugang zu Bildung haben.

Aufgrund ihrer klimatischen Eigenschaften und ihrer guten infrastrukturellen Erschließung sind die Wüsten Chiles beliebte Orte für Teleskope. Allein die Europäische Südsternwarte (ESO) hat drei Standorte in Chile mit Großteleskopen: La Silla, Paranal und Alma. Daneben gibt es weitere Observatorien in Chile, beispielsweise Cerro Pachon mit dem 8,1-Meter-Gemini-Süd-Teleskop oder das Las Campanas mit den zwei 6,5-Meter-Magallan-Teleskopen.

Seit den frühen 1990er-Jahren arbeitet in Chile das Paranal-Observatorium. Das Observatorium befindet sich in der Atacamawüste im Norden des Landes auf dem Berg Cerro Paranal. Dieser liegt etwa 120 Kilometer südlich von Antofagasta und 12 Kilometer von der Pazifikküste entfernt. Das Observatorium wird von der ESO betrieben und ist Standort des Very Large Telescope (VLT) und des Very Large Telescope Interferometer (VLTI). Zusätzlich werden die Surveyteleskope VISTA und VST gebaut. Die Atmosphäre über dem Gipfel zeichnet sich durch trockene und außergewöhnlich ruhige Luftströmung aus, was den Berg zu einem sehr attraktiven Standort für ein astronomisches Observatorium macht. Der Gipfel wurde in den frühen 1990ern von seiner ursprünglichen Höhe von 2660 Metern auf 2635 Meter heruntergesprengt, um ein Plateau für das VLT zu schaffen.

Amnesty International weist darauf hin, dass es im Zusammenhang mit Landstreitigkeiten immer wieder zu Menschenrechtsverletzungen an Angehörigen der indigenen Gruppe der Mapuche kommt. Diese geraten nach Informationen von Amnesty International oft in Konflikte bei der Verteidigung ihrer wirtschaftlichen, sozialen und kulturellen Rechte gegenüber Forst- und Energieunternehmen, die in ihren traditionellen Siedlungsgebieten tätig sind. Im Jahr 2006 kam es zu Ausschreitungen der Polizei gegen Angehörige der Mapuche. Festgenommene gaben damals an, gefoltert worden zu sein.

Chile erfüllt bis heute nicht alle Verpflichtungen, die es mit Unterzeichnung der "UN-Antifolter-Konvention" eingegangen ist. Besonders gravierend ist die Situation in den oft überfüllten Gefängnissen, die nicht internationalen Standards gerecht wird. Nach einer Reihe von Ereignissen kam es am 8. Dezember 2010 in der Haftanstalt San Miguel bei Santiago zu einem Aufstand und ein Feuer brach aus. In San Miguel waren zu diesem Zeitpunkt 1900 Menschen inhaftiert – ausgelegt ist das Gefängnis für 1000. In der Haftanstalt Santiago Sur mussten 400 Häftlinge die für 76 Insassen vorgesehenen Räumlichkeiten teilen. Die medizinische Versorgung und sanitäre Grundausstattung in den Gefängnissen sind nicht ausreichend. Eine Trennung minderjähriger Gefangener von Erwachsenen ist nicht sichergestellt.

Mit Inkrafttreten des ersten Chilenischen Strafgesetzbuches im Jahr 1875, das in diesem Punkt dem Vorbild des spanischen Strafgesetzbuches von 1870 folgte, waren Abtreibungen in Chile strafbar. Strafmilderungen waren möglich, wenn die Betroffene die Abtreibung zum Zwecke der Verheimlichung eines Ehebruchs, mithin zum Schutz der Ehre (ihrer eigenen Person oder des Mannes) vorgenommen hatte. Unter dem autoritären Militärregime von Oberst Carlos Ibáñez, der bürgerlich-liberalen Ehrbegriffen kritisch gegenüberstand und gesellschaftspolitisch eine linkspopulistische Linie verfolgte, wurde der Schwangerschaftsabbruch 1931 in Fällen der kriminologischen, medizinischen und eugenischen Indikation (d. h. nach Vergewaltigung, bei Gefahr für das Leben der Mutter und wenn der Fötus nicht überlebensfähig ist) erlaubt. Diese Indikationsregelung wird in der chilenischen Rechtsterminologie und politischen Debatte „therapeutischer Abort“ genannt. 1968 wurde die Anwendung der „therapeutischen“ Indikationen unter dem christdemokratischen Präsidenten Eduardo Frei Montalva nochmals erleichtert. Die Indikationsregelung bestand formell auch während der Militärdiktatur Augusto Pinochets fort, dessen Regierung sich erfolglos bemühte, ein absolutes Abtreibungsverbot in der Chilenischen Verfassung von 1980 zu verankern. Stattdessen wurde der Schutz des ungeborenen Lebens nur mit einer unbestimmten Formulierung in die von den Militärmachthabern erarbeitete Verfassung aufgenommen. Gesetzlich umgesetzt wurde das absolute Abtreibungsverbot erst im Zuge der politischen Neuordnung und schrittweise Entmachtung Pinochets gegen Ende der 1980er Jahre mit Unterstützung der römisch katholischen Kirche und scheidender Mitglieder der ehemaligen Militärjunta. Mit dem 1990 in Kraft getretenen Abtreibungsgesetz vom 15. September 1989, das auf Druck des damaligen Juntachefs Admiral José Toribio Merino und unter Mitwirkung des damaligen Bischofs von Rancagua und späteren Kardinals Jorge Medina zustande kam, wurde Chile zu einem der wenigen Länder der Welt, in denen Schwangerschaftsabbrüche auch in allen denkbaren Ausnahmekonstellationen und sogar in Abwägung gegen das Leben der Mutter komplett verboten waren. Die kontroverse Diskussion über eine Lockerung des absoluten Abtreibungsverbots zumindest in medizinisch und ethisch indizierten Ausnahmefällen begann schon unmittelbar nach Inkrafttreten des Verbots und dauert bis heute an. In der zweiten Amtszeit von Michelle Bachelet, zu deren Wahlprogramm die Aufhebung des Abtreibungsverbots gehörte, begann ein Gesetzgebungsprozess, der 2016 zum Abschluss gelangte. Am 21. August 2017 hob der chilenische Oberste Gerichtshof das generelle Abtreibungsverbot in Chile auf und machte den Weg für das von Staatspräsidentin Bachelet und der Mehrheit der Bevölkerung unterstützte und vom Parlament bereits verabschiedete Gesetz frei, das die Abtreibung in den drei „therapeutischen“ Ausnahmetatbeständen (nach Vergewaltigung, bei Gefahr für das Leben der Mutter und wenn der Fötus nicht überlebensfähig ist) wieder erlaubt. Laut UN-Schätzungen werden in Chile jährlich zwischen 60.000 und 70.000 illegale Abtreibungen vorgenommen.

Das Gesundheitssystem wurde in den 1970er und 1980er Jahren stark privatisiert. Chilenische Arbeitnehmer müssen sich privat krankenversichern. Den ärmeren Bevölkerungsschichten steht für bestimmte Krankheiten eine kostenlose Behandlung in staatlichen Gesundheitszentren zu. Rund 80 Prozent der Bevölkerung nutzen das staatliche Gesundheitssystem und 20 Prozent lassen sich privat behandeln. Michelle Bachelet, die bereits unter Ricardo Lagos für das Ressort Gesundheit zuständig gewesen war, brachte nach ihrem Amtsantritt als Präsidentin einige Reformen auf den Weg, die unter anderem eine kostenlose Gesundheitsversorgung für ältere Menschen vorsehen. Außerdem wurden Anstrengungen unternommen, die medizinische Infrastruktur, in der teils deutliche Unterschiede bestehen, weiter zu verbessern.

Die durchschnittliche Lebenserwartung der Chileninnen beträgt 79 Jahre und der Chilenen 72 Jahre (Stand: 2003).

1981 wurde das Rentenversicherungssystem unter José Piñera, einem Minister im Kabinett des Diktators Augusto Pinochet, vom Umlageverfahren auf das Kapitaldeckungsverfahren umgestellt. Die Arbeitnehmer müssen seitdem 13 Prozent ihres Gehalts als Beiträge für private Rentenfonds abführen. Außerdem werden 7 Prozent ihres Einkommens für eine private Krankenversicherung abgezogen, sodass die Sozialbeiträge im neuen System ein Fünftel eines Gehalts ausmachen. Nur das chilenische Militär wurde von Piñeras Reform ausgenommen; Soldaten haben bis heute Anspruch auf großzügige Pensionsleistungen vom Staat. Das neue Rentensystem wurde später von der Weltbank trotz interner Kritik als vorbildhaft angepriesen und verbreitete sich seit den frühen 1990er-Jahren in verschiedenen Varianten in Lateinamerika, wobei die meisten Staaten wie etwa Argentinien ein gemischtes Modell mit teils privatem, teils staatlichem Gesundheitssystem einführten. Besonders das argentinische Modell wurde später von weiteren Ländern übernommen, besonders in Osteuropa. Unter der Regierung Bachelet wurde das Rentensystem im Jahr 2008, auch auf Empfehlung der Weltbank, erneut reformiert.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 55,74 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 49,52 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 2,5 % des BIP.

Die Staatsverschuldung betrug 2016 21,1 % des BIP.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

Chile ist in 15 Regionen (spanisch "Región") aufgeteilt, die mit römischen Zahlen durchnummeriert sind. Die Nummer 13 gibt es nicht, die Hauptstadtregion wird mit RM ("Región Metropolitana") abgekürzt. Die Regionen spielen jedoch nur eine geringe politische Rolle, da Chile als ausgeprägter Zentralstaat gilt. Die Regionen sind in 54 Provinzen unterteilt.

Unterhalb der Provinzebene befinden sich die 346 Gemeinden ("municipalidad" oder "comuna"). Diese sind gemäß Artikel 61 der Verfassung die Organe der lokalen Selbstverwaltung. Sie werden von einem Bürgermeister ("alcalde") und einem Stadtrat geleitet.

"Siehe auch: Nationalstraßen in Chile"

Der Straßenverkehr hat sich in Chile zum wichtigsten Verkehrsträger entwickelt. Im Jahr 2005 besaß das Land ein Straßen- und Wegenetz von insgesamt 80.651 Kilometern, davon waren 16.967 Kilometer asphaltiert. Bedingt durch die gerade in den letzten Jahren intensiv betriebenen Ausbaumaßnahmen sind diese Zahlen allerdings heute obsolet.

Die wichtigste und mittlerweile von La Serena bis Puerto Montt als Autobahn ausgebaute Transportachse ist die etwa 3.000 Kilometer lange "Ruta 5" – ein Bestandteil der Panamericana. Sie verläuft in Nord-Süd-Richtung von der Grenzstadt Arica bis nach Quellón im Süden.

Im Jahre 1976 begann der Bau der Carretera Austral, ein ehrgeiziges Straßenbau-Projekt unter Diktator Augusto Pinochet, um die Regionen von Puerto Montt bis Feuerland zu verbinden. Die Straße ist bis heute im Bau – eine durchgehende Straßenverbindung zwischen Puerto Montt und Coyhaique bzw. zwischen Coyhaique und Punta Arenas existiert nach wie vor nur über das Nachbarland Argentinien.

Gut ausgebaut sind insbesondere die Panamericana und andere mautpflichtige Straßen vor allem im Großraum Santiago, in dem in den letzten Jahren viele neue Stadtautobahnen wie zum Beispiel die "Américo Vespucio" oder "Costanera Norte" entstanden sind; ferner eine Jahr für Jahr zunehmende Zahl von Hauptverbindungsstrecken. Viele Nebenstrecken, insbesondere in den abgelegeneren Teilen des Landes, bestehen jedoch nach wie vor lediglich aus unbefestigten Schotter- oder Erdpisten. Das Landstraßennetz wurde besonders in den mittleren Landesteilen seit dem Jahr 2005 sehr stark modernisiert und verbessert, wobei kontinuierlich bislang einfach ausgebaute Strecken zu mehrspurigen Fernstraßen ausgebaut werden. Straßenkarten sind im Buchhandel, bei Mietwagenstationen, Hotels oder an Tankstellen nicht zu bekommen und müssen im Voraus bei der Reiseplanung besorgt werden.

Im Großraum Santiago wird der öffentliche Personennahverkehr (Transantiago) von privaten Firmen betrieben, jedoch unter starker staatlicher Kontrolle. Mit dem vor einigen Jahren umgestellten System gibt es nun feste Haltestellen, an denen die Busfahrer verpflichtet sind, zu halten. Im Gegensatz zum deutschen System gibt es allerdings keine genauen zeitlichen Fahrpläne, sondern Taktzahlen, die von den Verkehrsverhältnissen abhängig sind. In Santiago wird in Bussen wegen der Diebstahlsgefahr vielfach mit Prepaid-Karten anstelle von Bargeld bezahlt.

Das Nahverkehrsbusnetz in den Provinzen ist vollkommen in privater Hand und recht unübersichtlich. Bei den städtischen Omnibussen ("Micro") sollte vorher der Streckenverlauf bekannt sein, denn an dem Großteil der Bushaltestellen gibt es keine Informationen über die Buslinien. Man erhält solche unter Umständen am lokalen Busbahnhof, wo auch die Fernbusse abfahren, oder in den Touristeninformationszentren an zentralen Plätzen. Auch Passanten und Fahrer sind häufig hilfsbereit und auskunftsfreudig, oft aber auch wenig informiert. Mitfahrinteressenten signalisieren dem ankommenden Busfahrer ihren Zusteigewunsch durch Handzeichen am Fahrbahnrand. Für aussteigewillige Fahrgäste gibt es einen Haltewunschknopf oder man meldet sich bei den Schaffnern oder Schaffnerinnen, die durch den Wagen gehen und die Fahrkarten verkaufen und kontrollieren. Die Busse sind wie Reisebusse bestuhlt (Pullmanbänke), Stadtbusse europäischen Typs gibt es kaum. Oft werden Kleinreisebusse für 18 oder 36 Personen als "Micro" genutzt.

Eine wichtige Rolle für den lokalen Personennahverkehr spielen Sammeltaxen ("Colectivos"), die auf festen Routen verkehren und Fahrgäste an beliebigen Stellen auf der Strecke aufnehmen und absetzen. Im Ortszentrum gibt es normalerweise einen oder mehrere Knotenpunkte, an denen Sammeltaxen aller Linien ständig vorfahren und wo man die Fahrtrouten erfragen und in den passenden Wagen einsteigen kann. Bei den Sammeltaxen handelt es sich um gewöhnliche Personenkraftwagen (nicht wie in anderen Ländern um Kleinbusse), die sich farblich nicht von normalen Taxen unterscheiden, sondern nur durch Kennzeichen (etwa die Liniennummer auf dem Dach) erkennbar sind. Der Fahrpreis (in der Regel ein Pauschalpreis unabhängig von Mitfahrstrecke oder Fahrziel) wird beim Einsteigen oder kurz nach Fahrtbeginn an den Fahrer gezahlt, der so lange weitere Personen zusteigen lässt, bis der Wagen voll besetzt ist. Gepäck (etwa Einkaufstaschen) kann man wie bei einem Taxi auch im Kofferraum ablegen, soweit dort noch Platz ist. Die Endpunkte der Linien, an denen der Fahrer wendet und wieder zurück ins Stadtzentrum fährt, liegen meist an markanten Punkten in den Vororten oder Wohngebieten.

Auch der gewöhnliche Taxiverkehr ist von Bedeutung, das gilt besonders für die Hauptstadt und größere Metropolen. Die Fahrpreise sind relativ erschwinglich. Traditionell sind die Taxen durch ihre Farbgebung im Straßenbild erkennbar und können überall angehalten oder angerufen werden. Besonders in der Hauptstadt gibt es aber auch immer mehr Funktaxen, die sich nach außen hin nur durch ihre Nummernschilder von privaten Pkw unterscheiden und ihre Fahrgäste nur auf telefonische Bestellung abholen. Da auch illegale Taxiunternehmer mit solchen Privatfahrten Geld zu verdienen suchen und teils mit kriminellen Netzwerken kooperieren, wird vor allem in Santiago strikt davon abgeraten, ohne telefonische Vorbestellung auf freier Strecke in solche nicht klar gekennzeichneten Taxen einzusteigen, auch wenn der Fahrer dazu einlädt und mit besonders günstigen Preisen wirbt. Ansonsten lassen sich die öffentlichen Verkehrsmittel in Chile bei Beachtung gängiger Sicherheitsempfehlungen in der Regel gefahrlos nutzen. Uber-Fahrer arbeiten in Chile in einer rechtlichen Grauzone und werden sowohl von den Behörden als auch von konkurrierenden Taxiunternehmern bekämpft.

Die gängigste Art, um mit öffentlichen Verkehrsmitteln in andere Städte und Regionen zu gelangen, ist die Tag- und Nachtreise mit dem Überlandbus. Es gibt verschiedene Anbieter, teils mit regionalen Schwerpunkten, und verschiedene Preis- und Komfortklassen. Diese Klassen gehen von Standardsitzen bis hin zu Liegebussen ("Bus Cama"), die für Übernachtreisen ausgelegt und mit höhenverstellbaren Sitzliegen in der Art eines Business-Class-Flugzeuges ausgestattet sind. Die Fahrzeugflotten vieler Unternehmen sind hochmodern, ältere Fahrzeuge finden sich nur noch bei Kleinunternehmen oder Billiganbietern. Die meisten größeren Gesellschaften bieten ein landesweites Routennetz an oder kooperieren mit Partnerfirmen in den von ihnen selbst nicht versorgten Regionen. Seit 2008 sind die Busunternehmer dazu übergegangen, alle Busse mit LED-Geschwindigkeitsanzeigern zu versehen, die von außen und von innen ständig sichtbar sind, um die Einhaltung der Geschwindigkeitsbegrenzungen nachvollziehbar zu dokumentieren. Die Fahrer sind immer in Doppelbesetzung unterwegs, oft fährt zusätzlich ein uniformierter Gepäckbursche mit, der auch die Fahrscheine kontrolliert. Alle Städte, auch Kleinstädte, verfügen über einen Busbahnhof ("terminal de buses") mit Schalterhalle, Ladenstraße, Gastronomiebetrieben, marktähnlichen Vorhallen und zum Teil überdachten Bussteigen. Die Reisen dauern oft sehr lange, Pausen werden in der Regel bis auf die fahrplanmäßigen Zwischenhalte nicht gemacht. Getränke und Speisen kann man an Bord mitbringen oder erwerben, häufig steigen streckenweise lokale Händler zu oder bieten ihre Waren an den Stationen feil. Der Ticketverkauf an den Schaltern und über das Internet funktioniert reibungslos; Fahrpläne werden in aller Regel zuverlässig eingehalten, bei Verspätungen werden Ersatzbusse eingesetzt.

Die älteste Eisenbahn Chiles, die auch als erste des südamerikanischen Festlandes angesehen wird, ist die im Mai 1850 begonnene und am 2. Januar 1852 fertiggestellte Bahn vom Hafen Caldera nach Copiapó. Die erste Strecke der Staatsbahnen von Valparaíso nach Santiago de Chile wurde am 15. September 1865 in Betrieb genommen.

Mit dem Salpeterboom wurde das Schienennetz in Nordchile, damals noch Teil Perus und Boliviens, ab 1871 stark ausgebaut. Eine der ersten Strecken führte von La Noria nach Iquique. Die Strecken wurden direkt an den Salpeterminen verlegt, so dass Ende des 19. Jahrhunderts große Teile der Regionen Tarapacá, Antofagasta und Atacama eisenbahntechnisch erschlossen waren. Parallel dazu erfolgte der Ausbau der Strecken zu den großen Hafenstädten in Zentralchile, wie San Antonio und Talcahuano.

Das Eisenbahnnetz von Chile hatte 1909 einen Umfang von 5675 km, davon 2618 km Staatsbahnen und 3057 km Privatbahnen, im Bau befindlich waren weitere 1393 km Staatsbahnen. Auch mehrere Privatbahnen waren im Bau und in Vorbereitung.

Heute wird das chilenische Schienennetz von der staatlichen Eisenbahngesellschaft EFE betrieben. Zudem betreibt eine Tochtergesellschaft der staatlichen Eisenbahngesellschaft das S-Bahn-Netz in Valparaíso.

Der Güter- und Personenverkehr, das Immobilienvermögen und der Personalbestand werden von verschiedenen Tochtergesellschaften verwaltet. Das Eisenbahnnetz kann aufgrund unterschiedlicher Spurweiten nicht durchgehend betrieben werden und besteht deshalb aus zwei verschiedenen Teilnetzen:

Der Eisenbahnpersonenverkehr ist seit vielen Jahrzehnten im Rückgang begriffen und wird kaum genutzt, was durch die starke Konkurrenz der Busunternehmen, die schlechte Qualität der Waggons und die wenigen bedienten Strecken begründet ist. Es gibt Bestrebungen, wieder mehr Personen per Schiene zu befördern. So wurde in modernere Waggons und Triebwagen investiert, Bahnhöfe renoviert oder neu errichtet (Puerto Montt hat 2006 einen neuen Hauptbahnhof am Stadtrand erhalten) sowie die Strecke zwischen Temuco und Puerto Montt wieder hergerichtet. Allerdings konnte die Eisenbahngesellschaft EFE ihre hochgesteckten Ziele nicht erfüllen und hat mit erheblichen technischen und organisatorischen Problemen zu kämpfen. Ebenso sind die Pläne auf Eis gelegt, Valdivia wieder an das Streckennetz anzubinden.

In der Hauptstadt Santiago de Chile existiert ein über 45 km langes U-Bahn-Netz ("Metro de Santiago"), dessen erste Teilstrecke 1975 eröffnet wurde. Zurzeit wird das U-Bahn-Netz stark ausgebaut und soll dann 83 km lang sein.

In die Agglomeration von Valparaiso ist seit 2005 eine Stadtbahn in Betrieb, und im Großraum Concepción verkehrt seit 1999 der so genannte Biotrén.

Ein Großteil des Im-/Exports Chiles wird über große Seehäfen abgewickelt. Die Hauptausfuhrgüter sind Kupfer, Eisen, Zellstoff, sowie landwirtschaftliche Erzeugnisse. Viele Häfen verfügen über moderne Containerterminals.

Wichtige Häfen gibt es in Arica, Iquique, Antofagasta, Chañaral, Coquimbo, Valparaíso, San Antonio, Talcahuano, Puerto Montt und Punta Arenas.

Besonders in Südchile spielen Fährverbindungen eine wichtige Rolle, da hier die Straßenverbindungen aufgrund von vielen Fjorden und Inseln schlecht realisierbar sind.

Wichtigste Stützpunkte der chilenischen Marine sind Valparaíso und Talcahuano.

Vor dem Bau des Panama-Kanals waren Valparaíso und Punta Arenas die wichtigsten Häfen sowohl für den Pazifikhandel als auch über die Magellanstraße und den dadurch ermöglichten direkten Zugang zum Atlantik nach Europa. 1840 errichtete die Pacific Steam Navigation Company die erste Dampfschifffahrtslinie in Südamerika von Valparaíso nach Callao in Peru.

Aufgrund der großen Entfernungen spielt der Flugverkehr eine wichtige Rolle. Die größten Flughäfen besitzen Santiago de Chile, Puerto Montt, Concepción, Temuco, Iquique, Antofagasta und Punta Arenas. Der größte Flughafen ist der "Comodoro Arturo Merino Benítez Airport" (5.650.000 Passagiere) in Santiago, der derzeit um ein großes internationales Terminal erweitert wird und bis 2020 eine Passagierkapazität von mehr als 30 Mio. Personen jährlich erreichen soll. Über das ganze Land sind zusätzlich viele kleine Regionalflughäfen verteilt, die untereinander durch die zwei wichtigsten chilenischen Luftverkehrsgesellschaften LATAM Airlines und Sky Airline verbunden werden. Von einigen Regionalflughäfen werden auch direkte Verbindungen in die Nachbarländer angeboten. Die zu Chile gehörende Osterinsel wird von Santiago de Chile und Lima in Peru angeflogen.

Der erste Motorflug in Chile fand am 21. August 1910 statt, der Pilot hieß César Copetta Brosio. Bekannt wurden insbesondere Dagoberto Godoy, der 1918 als erster die Anden überflog und José Luis Sánchez Besa, ein chilenischer Flugbootpionier. Der Beginn der Flugpostbeförderung wurde ab 1. Januar 1919 von Santiago de Chile nach Valparaíso vom Piloten Clodomiro Figueroa mit einer Morane-Saulnier "MS-35" durchgeführt.

1929 gründete Kommodore Arturo Merino Benítez mehrere lokale Fluggesellschaften, diese formierten sich ab 1932 zur Línea Aérea Nacional (LAN), die als nationale Airline Chiles fungierte. Diese wurde 1989 privatisiert und ging 2012 in der chilenisch-brasilianischen Fluggesellschaft LATAM Airlines auf. LAN beziehungsweise heute LATAM gehört infolge der expansiven Geschäftspolitik zu den drei größten Airlines Lateinamerikas, mit Filialen in Peru (LATAM Airlines Perú), Argentinien (LATAM Airlines Argentina) und Ecuador (LATAM Airlines Ecuador). Die zweite chilenische Fluggesellschaft Ladeco wurde Anfang der 1990er-Jahre von LAN aufgekauft.

Das Postwesen wurde bereits 1748 von den Spaniern eingeführt, ab 1853 gab es chilenische Briefmarken ("Siehe": Chilenische Postgeschichte).

Im Jahre 1851 erhielt der Engländer William Wheelwright von der Pacific Steam Navigation Company den Auftrag eine Telegrafenlinie zu errichten. Nach ersten Tests konnte am 21. Juni 1852 die erste Nachricht von Valparaíso nach Santiago geschickt werden. Im April 1853 begann der reguläre Betrieb. Danach begann man mit dem Ausbau der Telegrafenstrecken entlang der Eisenbahnlinien. Die ersten Strecken von Santiago führten nach Valparaíso und Talca und wurden 1857 komplett fertig gestellt. Bis 1892 konnte das ganze Land mit Telegraphen erreicht werden. Feuerland war über ein Seekabel angebunden worden.

Die ersten Telefone wurden 1880 in Valparaíso eingeführt. 1930 bildete sich die Telefongesellschaft "Compañía de Teléfonos de Chile CTC", die später von der spanischen Telefónica übernommen wurde. Der Radiobetrieb begann mit "Radio Chileña" in Santiago 1922.

Der zweite große Telekommunikationskonzern in Chile ist ENTEL "(Empresa Nacional de Telecomunicaciones SA de Chile)", der größte Anbieter von Internet- und Mobilfunkdiensten in Chile. ENTEL, Telefonica und weitere zum Teil lokale Anbieter betreiben heute ein nahezu flächendeckendes Netz für die Mobiltelefonie.

2016 nutzten 77,8 Prozent der Bevölkerung das Internet. Die digitale Infrastruktur gilt als die beste in Lateinamerika.

Als Gegenpol zum sozialistischen Konzept von Salvador Allende wurde die chilenische Volkswirtschaft unter Augusto Pinochet nach der Maxime der Chicago Boys konsequent nach marktwirtschaftlich-wirtschaftsliberalen Aspekten umgebaut. Staatliche Unternehmen wurden sowohl zu Zeiten Pinochets als auch danach größtenteils privatisiert, allerdings sind die von Allende verstaatlichten Kupferminen, die seit Pinochet unter direkter Kontrolle des Militärs standen, immer noch in Staatsbesitz. Auch wenn die nach Pinochet regierenden Mitte-links-Regierungen bemüht waren, soziale Härten abzufedern, gilt Chile heute nach wie vor als eines der Länder mit den größten sozialen Ungleichgewichten.

Die chilenische Volkswirtschaft wies zwischen 1988 und 1998 überdurchschnittliche Wachstumsraten auf. Die Asien- und Brasilienkrise 1997/98 führten zwar zu einer Rezession, seit 2000 wächst die Wirtschaft jedoch wieder mit Wachstumsarten zwischen 2,5 Prozent und 6 Prozent.

Im jährlichen Korruptionsindex von Transparency International belegt Chile permanent einen der vorderen Plätze, gilt also als relativ korruptionsfrei und lässt somit auch europäische Länder wie Belgien, Frankreich und Spanien hinter sich. Im Jahr 2010 war Chile auf Platz 21 zu finden. Im lateinamerikanischen Vergleich kommt nur Uruguay mit Platz 24 in die Nähe, weiter abgeschlagen liegen Brasilien und Kuba auf Platz 69, Kolumbien und Peru 78, Mexiko 98, Argentinien 105, Bolivien 110, Ecuador 127, Paraguay 146 und Venezuela auf Platz 164.

In einem Ranking der unternehmerfreundlichsten Länder der Welt, welches von der Weltbank-Tochter International Finance Corporation erstellt wurde, landete Chile 2005 auf dem 25. Platz als bestes lateinamerikanisches Land. Deutschland besetzt laut dieser Studie Platz 19, Kolumbien als zweitbestes südamerikanisches Land Platz 66.

Im jährlichen Globalisationsindex der Beratungsfirma A.T. Kearney und der Zeitschrift Foreign Policy lag Chile 2007 auf Rang 43 der insgesamt 72 bewerteten Länder. Im Vergleich zu 2006 verlor Chile die führende Position unter den lateinamerikanischen Ländern an Panama, den letzten Platz belegte Venezuela. Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegt Chile Platz 33 von 137 Ländern (Stand 2017–2018).

Die Arbeitslosenrate lag im Jahre 2004 bei 7,8 Prozent. Die Armutsrate lag 2003 bei 18,8 Prozent und die Rate extremer Armut bei 4,7 Prozent der Bevölkerung. Die Armutsrate hat sich seit 1987 mehr als halbiert, die extreme Armut hat nur noch etwa 30 Prozent des Wertes von 1987. Die neue demokratische Regierung Chiles führte das Programm „Chile Solidario“ ein, damit werden die 250.000 ärmsten Familien im Land von staatlichen Helfern betreut und finanziell unterstützt.

Die Inflationsrate liegt im Schnitt zwischen 2 Prozent und 4 Prozent. Seit 1998 hat sie die Fünf-Prozent-Marke nicht mehr überschritten. Im Jahre 2004 lag sie bei 2,4 Prozent und wurde 2006 mit einem Wert von 2,6 Prozent angegeben.

Das Bruttosozialprodukt stieg im Jahr 2016 um 1,6 Prozent auf 247,0 Milliarden US-Dollar, dies entspricht in etwa 7146 US-Dollar je Einwohner. Chile hat das höchste Pro-Kopf-Einkommen und das höchste Exportvolumen je Einwohner unter den südamerikanischen Staaten.

Den größten Anteil an der Wertschöpfung hat der Dienstleistungssektor mit 57 %, gefolgt vom Produktionssektor und der Landwirtschaft mit 34 Prozent beziehungsweise 9 Prozent Anteil (Stand: 2001).

Chile gehört zu den führenden Wirtschaftsnationen Lateinamerikas sowie zu den größten Rohstoffproduzenten. Es verfügt über die größten bekannten Kupfervorkommen der Welt (etwa 40 Prozent). In Chile liegen die größten Kupferminen der Welt, Chuquicamata (Übertage) und El Teniente (Untertage), die vom staatlichen Konzern Codelco ausgebeutet werden. Die von der Produktionsmenge größte Kupfermine ist Escondida (Übertage), die von der privaten Gesellschaft Minera Escondida betrieben wird. Verschiedene Edelmetalle und vor allem Salpeter führten Chile schon im 19. Jahrhundert zum Reichtum. Momentan wird mit dem Pascua-Lama-Projekt eine der größten Goldminen der Welt geplant, bei dem jedoch große Umweltschäden befürchtet werden. Darüber hinaus ist Chile auch der größte Lithium-Produzent der Welt.

Daneben werden heute Forst-, Fischerei- und Landwirtschaft betrieben. Nur etwa 7 Prozent der Landfläche werden für die Landwirtschaft genutzt. Diese Flächen befinden sich hauptsächlich im Zentraltal. Im wüstenhaften Norden Chiles beschränkt sich die Landwirtschaft weitgehend auf Oasen. Die Viehzucht ist hauptsächlich in Zentralchile und im nördlichen Teil von Südchile angesiedelt. Chile ist das einzige Land Südamerikas, in dem Zuckerrüben angebaut werden. Besondere Erwähnung verdient der Weinbau, der Chile zum Weinexporteur Nummer eins in Südamerika gemacht hat.

Chiles Wirtschaft hängt stark vom Export ab. 2004 betrug der Exportanteil 34 Prozent des Bruttosozialprodukts, was ziemlich genau dem von Deutschland entspricht. Besonders wichtig für die chilenische Wirtschaft ist der Kupferexport. Momentan steigen andere Exportgüter stärker als Kupfer und Mineralien. 1975 lagen diese noch bei 30 Prozent, heute sind es etwa 60 Prozent. Zu diesen Exportgruppen gehören Forst- und Holzprodukte, frische Früchte, Wein und Nahrungsmittel, Lachs, verarbeitete Nahrungsmittel, Fischmehl und Meeresfrüchte. 2004 stieg die Handelsbilanz um neun Milliarden US-Dollar, viel stärker als 2003. Mit dem starken Anstieg der Rohstoffpreise explodierten die Exporte geradezu von 20,4 (2003) auf 32,1 im Jahr 2004 und 39,4 Milliarden US-Dollar im Jahr 2005. Chile verdrängte 2005 Norwegen als weltweit größten Lachsproduzent.

Die VR China ist mit 28 Prozent der größte chilenische Einzel-Exportmarkt, auf Platz 2 stehen die USA. Weitere Haupthandelspartner des Landes sind Brasilien und Argentinien, mit denen Chile über den Mercosur assoziiert ist. Bis heute ist Chile dem Mercosur jedoch nicht als vollständiges Mitglied beigetreten, da dies dem Land die Möglichkeit nehmen würde, eigenständige Handelsabkommen mit anderen Ländern abzuschließen und es auch befürchtet wird, dass das Land im Falle eines vollständigen Beitritts sich der Gefahr aussetzen würde, durch wirtschaftliche Schwankungen seiner Nachbarn stärker getroffen zu werden. Durch den Kompromiss der Assoziierung bestand für Chile die Möglichkeit, eigene Freihandelsabkommen mit Japan, der EU und der NAFTA abzuschließen. Chile hat 2005 auch ein Freihandelsabkommen mit der Volksrepublik China und 2006 eines mit Brunei, Neuseeland und Singapur (P4 Agreement) abgeschlossen. Aufgrund dessen gilt die chilenische Volkswirtschaft heute als eine der offensten der Welt.

Die wichtigen Wirtschaftskennzahlen Bruttoinlandsprodukt, Inflation, Haushaltssaldo und Außenhandel entwickelten sich in den letzten Jahren folgendermaßen:

Im Jahre 2012 lag Chile bzgl. der jährlichen Erzeugung mit 66,89 Mrd. kWh an Stelle 41 und bzgl. der installierten Leistung mit 18.600 MW an Stelle 42 in der Welt. Eine Studie der IEA sagt bis 2020 einen Anstieg des Verbrauchs auf 100 Mrd. kWh voraus.

Gegenwärtig (Stand Juni 2016) gibt es in Chile kein landesweites Verbundnetz, sondern zwei voneinander unabhängige Verbundsysteme, das "Sistema Interconectado Central" (SIC) für das Zentrum sowie das "Sistema Interconectado del Norte Grande" (SING) für den Norden des Landes. Die beiden Netze sollen bis 2017 miteinander synchronisiert werden. Darüber hinaus gibt es im Süden des Landes noch verschiedene Inselnetze. Es besteht eine Verbindung zwischen dem SING und dem "Sistema Argentino de Interconexión" in Argentinien: eine 345-kV-Leitung verbindet Mejillones in Chile mit Cobos in der argentinischen Provinz Salta.

Die Stromerzeugung in Chile beruht traditionell zu einem erheblichen Teil auf Wasserkraft. Die Wasserkraftwerke liegen praktisch ausschließlich im Bereich des SIC. Eine ausgeprägte Trockenheit, verursacht durch El Niño führte von November 1998 bis April 1999 zu Stromabschaltungen in der Hauptstadt Santiago de Chile. Daraufhin beschloss die Regierung, die Abhängigkeit von Wasserkraft zu verringern und die Stromerzeugung durch GuD-Kraftwerke zu diversifizieren. Das Erdgas wurde durch Pipelines aus Argentinien geliefert. 2004 verringerte die Regierung von Néstor Kirchner jedoch die Gaslieferungen unerwartet, wodurch es in Chile erneut zu einer Krise kam. Aufgrund dieser Erfahrungen setzt Chile seither auch wieder verstärkt auf Kohle zur Stromerzeugung. Außerdem wurden Gasterminals für den Import von LNG errichtet, eines in Mejillones für die Versorgung der Gaskraftwerke im SING und eines in Quintero für das SIC. Ein weiterer Ausbau der Wasserkraft ist aus ökologischen Gründen umstritten (siehe HidroAysén).

Aufgrund der großen Länge des Landes verfügt Chile über unterschiedlichste Landschaften. Im Norden dominiert die Atacamawüste. Der Osten ist von den Anden geprägt. Zentralchile ist von mediterranem Klima beeinflusst. Der "Kleine Süden" ist geprägt von Wäldern und herrlichen Landschaften, die oft auch als "Chilenische Schweiz" bezeichnet werden. Ab der Region XI. gibt es bereits große Gletschergebiete. Der größte Gletscher Südamerikas ist das Campo de Hielo Sur. Hier beginnt die karge Landschaft Patagoniens. Das Klima ist rau und regenreich.

Eine ganz andere Welt bieten die ozeanischen Inseln, wie die Osterinsel und die Juan-Fernández-Inseln. Die Osterinsel ist besonders aus archäologischer Sicht sehr interessant.

Feuerland dient oft als Ausgangspunkt zur Chilenischen Antarktis.

Chile verfügt über eine sehr große Anzahl von Nationalparks und nationalen Reservaten, diese werden von der chilenischen Forstbehörde CONAF verwaltet.

Die bekanntesten Nationalparks sind der Nationalpark Conguillio, der Nationalpark Torres del Paine, der Nationalpark Lauca, der Nationalpark Bernardo O’Higgins und der Nationalpark Rapa Nui auf der Osterinsel.

In der Provinz Palena bei Chaitén liegt der mit privaten Mitteln errichtete, über 3000 Quadratkilometer große Parque Pumalín. Er wurde vom US-Amerikaner Douglas Tompkins durch große Landkäufe ab Mitte der 1990er-Jahre errichtet. Das Land wurde später der Non-Profit-Organisation "Fundación Pumalin" übergeben. Der Park ist insbesondere für den Öko-Tourismus interessant.

Die UNESCO erklärte insgesamt 8 Gebiete in Chile zu Biosphärenreservaten.

Die UNESCO erklärte bisher drei Plätze in Chile zum Weltkulturerbe: im Jahr 2000 den Nationalpark Chiloé sowie einen Teil der dort befindlichen Holzkirchen, 2003 das historische Viertel der Hafenstadt Valparaíso sowie 2005 die Humberstone- und Santa-Laura-Salpeterwerke in der Atacamawüste im Norden Chiles.

Zum Weltnaturerbe wurden 1978 der Nationalpark Torres del Paine und 1995 der Nationalpark Rapa Nui auf der Osterinsel ernannt.

Santiago de Chile, Concepción (Chile) und Valparaíso bieten die größte Vielfalt an Museen und historischen Plätzen. Über das Land verteilt gibt es viele Monumente, die lange vor der spanischen Besiedlung entstanden sind.

Zwischen der Kultur in den Städten und auf dem Land gibt es starke Unterschiede. Auf dem Lande spielt die Folklore mit traditionellen Tänzen, wie dem Nationaltanz Cueca, eine wichtige Rolle. Die volkstümliche Kultur ist stark spanisch und araukanisch geprägt. "Payadores" sind Volkssänger, deren Lieder meist von Liebe und Träumen handeln. Politische Lieder waren ihnen während der Pinochet-Diktatur verboten. Das Kunsthandwerk auf dem Lande ist von indianischen Einflüssen gekennzeichnet. Hergestellt werden vor allem Web- und Töpferarbeiten sowie Schnitzereien. Eine wichtige Rolle auf dem Lande spielen die Huasos, eine Art chilenischer Cowboys oder Gauchos. Sie sind auf fast allen Folklorefesten und speziell beim chilenischen Rodeo dabei. Die Stadtkultur ist kosmopolitischer geprägt.

Fast 50 Prozent der Chilenen gaben in einer 2008 durchgeführten repräsentativen Umfrage an, nie oder fast nie zu lesen. Bücher sind in Chile sehr teuer, da die Auflagen sehr gering sind. Der Buchmarkt hat sich nach der kulturellen Lähmung unter der Militärdiktatur nur langsam erholt.

Isabel Allende (* 1942) ist wohl die bekannteste zeitgenössische Schriftstellerin Chiles. Ihre Romane wie "Das Geisterhaus", "Fortunas Tochter" oder "Der unendliche Plan" sind weltweit verlegt worden. Viele ihrer Bücher sind stark autobiografisch geprägt. Sie ist die Nichte des früheren Präsidenten Salvador Allende.

Roberto Bolaño (1953–2003), Verfasser surrealistischer Lyrik und Prosa, ging nach dem Militärputsch 1973 ins Exil. Er ist Träger vieler Literaturpreise und starb in Barcelona.

Jorge Edwards (* 1931) ist Träger des Cervantespreises. In Deutschland ist sein Werk kaum bekannt. Erst neun Jahre nach seiner Veröffentlichung erschien sein Roman "Der Ursprung der Welt" 2005 in deutscher Übersetzung.

Alberto Blest Gana (1830–1920) schrieb den ersten chilenischen Roman ("Martín Rivas", 1862), eine realistische und sozialkritische Familiengeschichte. Er wurde seit 1925 fünfmal verfilmt und auch für das Theater und als Musical adaptiert. Die Werke des Autors sind in Chile heute noch wichtige Bestandteile der Schullektüre.

Gabriela Mistral (1889–1957), Dichterin und Nobelpreisträgerin 1945, schrieb in ihren Gedichten über Liebe, Tod und Hoffnung, nachdem ihr Geliebter Romelio Ureta Selbstmord begangen hatte. Später arbeitete sie im diplomatischen Dienst Chiles.

Der avantgardistische Lyriker Vicente Huidobro (1893–1947) begründete mit seinem Gedichtband "Ecos de Alma" (1911) die modernistische Bewegung in Chile. Er kämpfte im spanischen Bürgerkrieg auf republikanischer Seite.

Pablo Neruda (1904–1973) war ein weltbekannter Dichter, Schriftsteller und Nobelpreisträger 1971. Er verfasste viel soziale und politische Lyrik und arbeitete als Botschafter in Frankreich für die Regierung von Salvador Allende. Er starb kurz nach dem Militärputsch 1973 an Krebs. Sein Begräbnis wurde zur ersten öffentlichen Demonstration gegen das Militärregime.

Luis Sepúlveda (* 1949) wurde unter Pinochet mehrfach verhaftet und musste ins Exil gehen. Zu seinen in Deutschland bekannten Werken gehört das „Tagebuch eines sentimentalen Killers“.

Auch Antonio Skármeta (* 1940), Schriftsteller und Anhänger von Salvador Allende, verließ nach dem Militärputsch 1973 das Land. Er verfasste Romane und Erzählungen, die sich oft mit der Militärdiktatur befassten. Von 2000 bis 2003 war er chilenischer Botschafter in Berlin, wo er auch während seines Exils gelebt hatte.

1941 wurde das "Orquesta Sinfónica de Chile", 1955 das "Orquesta Filarmónica de Santiago" (das kommunale und Opernorchester von Santiago) gegründet.

Von der chilenischen Volksmusik beeinflusst sind die Werke des Komponisten Carlos Isamitt (1887–1974).

Claudio Arrau (1903–1991), geboren in Chillán, war der bedeutendste chilenische Pianist und eine der wichtigsten Musikerpersönlichkeiten der Nachkriegszeit. Seine Interpretation der Werke Beethovens, Schumanns und vieler anderer Komponisten des klassischen Repertoires setzen bis heute Maßstäbe.

Violeta Parra (1917–1967) begründete die „Nueva canción Chilena“. Von Chile ausgehend erreichte diese gesellschaftskritische künstlerische Bewegung („Neues Lied“) bis in die 1980er Jahre weite Verbreitung in Lateinamerika, Portugal und Spanien. Die Sängerin wuchs in Armut auf, komponierte schon früh eigene Folklorelieder und begann in den 1950er Jahren, traditionelles Liedgut zu sammeln und zu dokumentieren. Ihre davon beeinflussten eigenen Werke hatten einen stärker politisch-gesellschaftskritischen Charakter. Neben der Musik dichtete sie, malte, webte und schuf Skulpturen. Viele chilenische und internationale Künstler wie Mercedes Sosa und Joan Baez haben ihre Werke interpretiert; ihr bekanntestes Lied ist "Gracias a la vida".

Víctor Jara (1932–1973) war ein politischer Sänger und zählt ebenfalls zu den großen Vertretern der „Nueva canción“. Er unterstützte Salvador Allende und wurde während des Militärputsches 1973 gefoltert und getötet.

Die Gruppen Illapu, Inti Illimani und Quilapayún machten die Musik der „Nueva Canción Chilena“ weltbekannt. Sie mussten nach dem Militärputsch lange Jahre im Exil verbringen und haben ihr musikalisches Spektrum beständig erweitert.

Der moderne chilenische Film beschäftigt sich oft mit der Zeit der Militärdiktatur von 1973 bis 1989. Zu den berühmtesten Regisseuren gehören Andrés Wood und Miguel Littín.

Carmen Castillo (* 1945) ist eine chilenische Dokumentarfilmerin. Sie schrieb 1979 "Santiago de Chile, ein Tag im Oktober", ein Buch über ihr Leben im Untergrund nach dem Militärputsch.
Orlando Lübbert (* 1945) und Andrés Wood (* 1965) befassen sich mit humorvollen Sozialdramen, Cristián Galaz (* 1958) zeigt den Beziehungalltag der Chilenen. Alejandro Jodorowsky ist Schauspieler, Autor und Regisseur einer Reihe surrealistischer Filme, darunter El Topo und Montana Sacra – Der heilige Berg.

Juan Francisco González (1853–1933) und Pablo Burchard Eggeling (1875–1964), der vom Stil Pierre Bonnards beeinflusst war, begründeten den chilenischen Impressionismus. Ramón Subercaseaux (1854–1936) zeigte sich von Cézanne beeinflusst. Roberto Matta (1911–2002) war ein weltweit bekannter surrealistischer Maler des 20. Jahrhunderts. Zeitweise lebte er in Paris und war mit Salvador Dalí und Federico Garcia Lorca befreundet. Zu den Expressionisten zählen Israel Roa (1909–2002). Ximena Cristi (* 1920) zeigt sich von Matisse und Bonnard beeinflusst. Beide zählten zur "Generation der 40er". Den abstrakten Expressionismus der 1960er vertrat Guillermo Núñez (* 1930).

Der Maler und Grafiker Nemesio Antúnez (1918–1993) begründete die „Werkstatt 99“ ("taller 99"), die der modernen Druckgrafik den Weg bereitete. 1973 bis 1984 lebte er im Exil. Der 1913 auf Kuba geborene Maler Mario Carreño Morales malte wuchernde Formen in warmen Farben; er gilt als einer der wichtigsten lateinamerikanischen Maler und starb 1999 in Santiago de Chile.

Vielfach international ausgezeichnet wurden Arbeiten des in Katalonien geborenen, auch politisch sehr aktiven Vertreter der Informel José Balmes (* 1927), der 1939 nach Chile und 1973 nach Frankreich ins Exil ging, um 1986 nach Chile zurückzukehren. Im gleichen Jahr wurde Gracia Barrios geboren, eine Vertreterin des "realismo informal", die Alltagsszenen malt. Mario Toral (* 1934) knüpft in seinem Werk an präkolumbianische Formen an.

In der neueren chilenische Malerei dominieren figürliche Darstellungen, wobei bemerkenswert viele Frauen aktiv sind. Einen neorealistischen Stil vertreten die in Spanien geborene Roser Brus (* 1923), die feministische Malerin Carmen Aldunate (* 1940) und Natalia Barbarovic (* 1966).

Ein Vertreter des Neoexpressionismus und Mitbegründer der "Schule der 80er" ist Samy Benmayor (* 1956).

Die Fußball-Weltmeisterschaft 1962 fand in Chile statt. Die chilenische Fußballnationalmannschaft erreichte dabei einen achtbaren dritten Platz; das ist das beste Ergebnis bei einer Weltmeisterschaft. Chile hat sich bisher achtmal für die WM qualifiziert und liegt nach diesem Kriterium in Südamerika auf dem vierten Platz hinter Brasilien, Argentinien und Uruguay. Darüber hinaus gewann die Nationalmannschaft auch die Copa América 2015, die im eigenen Land ausgetragen wurde sowie die Copa América Centenario 2016 in den USA. Zu den nationalen Fußballlegenden zählen Ivan Zamorano, Marcelo Salas und unangefochten an der Spitze Elías Figueroa, erster (und neben Zico einziger) Spieler Amerikas, der dreimal den Titel des besten Spielers des Kontinents erringen konnte. Figueroa gilt heutzutage als einer der besten Abwehrspieler des letzten Jahrhunderts. Zu erwähnen sind auch Matías Fernández, Südamerikas Fußballer des Jahres 2006, und David Arellano, der als Erfinder des Fallrückziehers (auf Spanisch "la chilena") gilt. Die bekanntesten chilenischen Fußballspieler sind aktuell Alexis Sánchez und Arturo Vidal.

Neben Fußball spielen insbesondere Tennis, der Reitsport (hier vor allem auch das chilenische Rodeo) und das Segeln eine bedeutende Rolle. Im Tennisdoppel gewann Nicolás Massú mit seinem Partner Fernando González bei den Olympischen Sommerspielen 2004 das erste Olympiagold für Chile überhaupt. Einen Tag später krönte Massú seine Olympiateilnahme mit dem Sieg im Herreneinzel, González gewann die Bronzemedaille. Eine der großen Sportlegenden in Chile ist Marcelo Ríos, der als erster spanischsprechender Tennisspieler die Spitze der Weltrangliste erreichte und hierbei zeitweise Pete Sampras ablöste. Im Segelsport, Kategorie Breitling, besetzten chilenische Teams in prestigeträchtigen Rennen wie der Copa del Rey regelmäßig die ersten drei Plätze.

Am 3. Mai 2008 besiegte die chilenische Polo-Nationalmannschaft im WM-Finale in Mexiko-Stadt den amtierenden Weltmeister Brasilien und wurde somit erstmals Weltmeister in dieser Disziplin.

Im Rudern (Zweier ohne Steuermann) wurden Christian Yantani und Miguel Angel Cerda im Jahre 2002 in Sevilla Weltmeister. 2005 wurde Cerda in Japan mit Felipe Leal Vizeweltmeister und in der Schweiz Weltmeister.

Carlo de Gavardo ist zweifacher Motorrad-Rallye-Weltmeister.

Wichtigste Informationsquelle der chilenischen Bevölkerung ist das Fernsehen.
Die wichtigsten Fernsehsender sind das staatliche TVN-Programm, der Sender "Canal 13" der katholischen Universität "Universidad Católica" und private Sender wie "Megavisión" oder "Chilevisión". Das Programm der Fernsehsender ist hauptsächlich auf Unterhaltung, das heißt auf Shows, US-amerikanische Filme und Fernsehserien, die beliebten „Teleseries“ (Telenovelas, zumeist aus eigener Produktion) sowie auf Sportberichterstattung ausgerichtet. Politische Sendungen, Naturdokumentationen und Kulturprogramme sind dagegen eher dünn gesät, dann aber oft von guter Qualität. Die Nachrichten beginnen erst um 21 Uhr und dauern etwa eine Stunde.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte Chile Platz 33 von 180 Ländern. Nach Uruguay belegte Chile damit den zweitbesten Rang in Südamerika. Die Presselandschaft wird weitgehend von zwei Konzernen dominiert, dem "Mercurio"- und dem "COPESA"-Konzern, nachdem sich eine Reihe von Publikationen aus dem politischen Mitte-links-Spektrum nach dem Rückgang der Politikbegeisterung zur Zeit der Redemokratisierung nicht auf Dauer im Markt halten konnten. Die beiden jeweiligen „Flaggschiffe“ der Pressekonzerne sind "El Mercurio", eine Zeitung, die in Qualität und politischer Ausrichtung etwa mit der Frankfurter Allgemeinen Zeitung zu vergleichen ist, sowie "La Tercera". Zu den selten gewordenen "bunten Vögeln" in der Presselandschaft gehört das Hausblatt der kommunistischen Partei, "El Siglo", sowie die ebenfalls linksorientierte, aber parteiungebundene Zeitschrift "Punto Final".

Wichtige Wochenzeitschriften sind "Ercilla" und "Qué Pasa". Darüber hinaus gibt es die deutschsprachige Wochenzeitung "Cóndor".

Die chilenische Küche ist keineswegs ein Ableger der spanischen Küche, wie viele vermuten. Vielmehr gibt es eine Vielzahl von Einflüssen – vielfach auch von deutschen Einwanderern. So finden sich etwa deutsche Bezeichnungen wie „Kuchen“ ("kuchen", Aussprache wie im Deutschen) oder „Apfelstrudel“ ("estrudel") auch im Wortschatz der chilenischen Konfiserie. Berliner (zumeist mit einer Puddingfüllung) sind unter der Bezeichnung "Berlines" verbreitet. Auch der Christstollen als Weihnachtsgebäck ist bekannt (unter der Bezeichnung "pan de pascua") und gilt in Südamerika als chilenische Spezialität; ebenso die Schweinskopfsülze ("queso de cabeza"), Tatar ("tártaro de carne") oder die einer Bouillabaisse ähnelnde chilenische Fischsuppe "Paila marina". Ebenfalls auf mitteleuropäische Einflüsse zurückzuführen ist das typisch chilenische Sauerkraut (genannt "Chucrú", abgeleitet vom französischen Choucroute), die Vorliebe für quarkähnliche Frischkäsezubereitungen und die vor allem im Süden sehr starke Brautradition. Viele Biere werden nach dem deutschen Reinheitsgebot gebraut und oft wird aus deutschen Anbaugebieten importierter Hopfen verwendet.

Aufgrund der sonnigen Bedingungen in Mittel- und Nordchile und der vulkanischen Böden eignet sich das Land sehr gut zum Anbau von Feldfrüchten und Obstsorten, die in großer Vielfalt auf Chiles Märkten feilgeboten werden. In Chile als einem der Ursprungsländer der Kartoffel finden sich auch sehr viele unterschiedliche Speisekartoffelsorten. Der mindestens einmalige wöchentliche Marktbesuch und die Verwendung frischer Gemüse und anderer Zutaten in der Küche spielt für die Mehrzahl der chilenischen Hausfrauen und die in wohlhabenden Haushalten häufig anzutreffenden Küchenmamsells noch immer eine große Rolle.

Neben einem reichen Angebot an Fisch und Meeresfrüchten wird in Chile sehr gerne Huhn gegessen. Gegrilltes Fleisch, ein so genannter "Asado", gehört wie im Nachbarland Argentinien zu den traditionellen Speisen bei geselligen Anlässen. Neben Rind- und Schweinefleisch werden dabei vor allem würzige Paprikawürste ("Longanizas") verwendet. Das Fleisch wird vor dem Grillen gern einige Stunden in Bier eingelegt, um seine Zartheit zu erhöhen.

Zu den Nationalgerichten zählt die chilenische Empanada, das sind unterschiedlich (bspw. mit Rindfleisch, Hühnchen, Meeresfrüchten oder Käse) gefüllte Teigtaschen, die entweder im Ofen gebacken oder in Bratfett frittiert zubereitet werden können. Die "Cazuela" ist ein kräftiges Eintopfgericht, für das Hühnchen oder auch Rindfleisch, Maiskolben ("Choclos"), Kürbis und weitere Gemüse verwendet werden. Als "Humitas" wird ein Maisbrei bezeichnet, der in Maisblättern gekocht und süß oder salzig gegessen wird. "Pebre" ist eine aus scharfem Paprika ("Ají"), fein gehackten Zwiebeln und Kräutern zubereitete Öl-Zitronen-Sauce, die vor allem zu Fleisch, aber auch zu sonstigen Gerichten als Würze gereicht wird. Beliebt sind auch mit getrocknetem Kelp, "Cochayuyo" genannt (es handelt sich um Braunalgen der Art "Durvillaea antarctica"), zubereitete Beilagen. Die relativ geschmacklose Alge wird dazu klein geschnitten und mit Zwiebeln, unterschiedlichen Gewürzen und Kräutern und unter Umständen Hülsenfrüchten oder anderem Gemüse vermengt gegart. Typisch ist auch das so genannte „geröstete Mehl“ ("harina tostada"), gewonnen aus erhitztem und anschließend zermahlenen Weizen, der mit Wasser und Zucker, eventuell auch Melonensaft oder Wein, zu einer zähflüssigen Mischung verarbeitet werden kann, dem "ulpo", der als stärkendes Erfrischungsgetränk zu sich genommen wird.

Der klassische chilenische Schnellimbiss ist der in den 1950er Jahren entstandene "Completo", eine Art Hot Dog, der mit reichlich Avocadomus ("Palta") und Sauerkraut oder Krautsalat ("Chucrú") gereicht und mit Chilipaste ("salsa de ají chileno") und dem mild-süßen chilenischen Senf gegessen wird. Ebenfalls als typisch chilenisch gelten die oft mit gebratenem Fleisch oder anderen Zutaten reich belegten Sandwiches ("sánguches"), die an Garküchen, Imbissständen oder in Gaststätten fast überall in den Städten auch zum Mitnehmen zu bekommen sind.

Zu den Besonderheiten der Mahlzeitenfolge in Chile gehört, dass neben Frühstück ("desayuno") und Mittagessen ("almuerzo") auch am frühen Abend ein Imbiss gereicht wird, zu dem stets Tee getrunken wird und der das in der Regel erst sehr spät eingenommene Abendessen ("comida") mitunter ersetzen kann. Diese Zwischenmahlzeit wird "tomar once" (wörtlich „Elf einnehmen“) genannt. Diese für Chile eigentümliche Tradition wird oft auf englische Gebräuche (etwa den „Fünf-Uhr-Tee“ oder den in England „Elevenses“ genannten Vormittagstee) zurückgeführt. Einer volkstümlich-humoristischen Erklärung zufolge soll der Ausdruck dagegen auf die Tatsache zurückgehen, dass das spanische Wort "aguardiente" (Schnaps) genau elf ("once") Buchstaben hat. Zu Zeiten eines Alkoholverbots in Chile hätten die Leute deshalb "Once" bestellt und Schnaps in einer Tasse serviert bekommen. Auch wenn sich der genaue Ursprung der Bezeichnung nicht mehr mit letzter Sicherheit aufklären lässt, liegt besonders eine Ableitung aus der nahe, die in vom Katholizismus geprägten Ländern wie Spanien, Italien oder Chile bis weit ins 19. Jahrhundert üblich war: Die elfte Stunde der kirchlichen Tageseinteilung entspricht exakt der traditionellen „Tea Time“ 17 Uhr. Dessen ungeachtet wird die „Once“ in Chile allerdings besonders im Sommer oft bis 19.30 Uhr hinausgeschoben.

Der Wein in Chile ist von sehr guter Qualität und wird seit vielen Jahren mit großem Erfolg auf den Weltmarkt exportiert. Rebsorten wie Merlot und Cabernet Sauvignon sind im Rotweinsegment weit verbreitet und werden auf verschiedenen Qualitätsstufen produziert. Eine exklusive Rebsorte ist die Carménère, eine besonders empfindliche Sorte von außergewöhnlicher Qualität, die heute (da in Frankreich durch Reblausbefall ausgestorben) praktisch nur noch in Chile angebaut wird.





</doc>
<doc id="941" url="https://de.wikipedia.org/wiki?curid=941" title="Rechnergestützte Entwicklung">
Rechnergestützte Entwicklung

Die rechnergestützte Entwicklung (englisch "" und kurz "CAE" genannt) umfasst alle Varianten der Rechner-Unterstützung von Arbeitsprozessen in der Technik.

Die folgenden Teilgebiete sind bekannt:


</doc>
<doc id="945" url="https://de.wikipedia.org/wiki?curid=945" title="CERN">
CERN

Das CERN, die Europäische Organisation für Kernforschung, ist eine Großforschungseinrichtung bei Meyrin im Kanton Genf in der Schweiz. Am CERN wird physikalische Grundlagenforschung betrieben, insbesondere wird mit Hilfe großer Teilchenbeschleuniger der Aufbau der Materie erforscht.
Der derzeit bedeutendste ist der Large Hadron Collider, der 2008 in Betrieb genommen wurde.

Das Akronym CERN leitet sich vom französischen Namen des Rates ab, der mit der Gründung der Organisation beauftragt war, dem "Conseil européen pour la recherche nucléaire". Die offiziellen Namen des CERN sind "European Organization for Nuclear Research" im Englischen beziehungsweise "Organisation européenne pour la recherche nucléaire" im Französischen.

Derzeit hat das CERN 22 Mitgliedstaaten. Mit etwa 3.200 Mitarbeitern (Stand: 31. Dezember 2015) ist das CERN das weltweit größte Forschungszentrum auf dem Gebiet der Teilchenphysik. Über 10.000 Gastwissenschaftler aus 85 Nationen arbeiten an CERN-Experimenten. Das Jahresbudget des CERN belief sich 2014 auf ungefähr 1,11 Milliarden Schweizer Franken (ca. 1 Milliarde Euro).

Nach zwei UNESCO-Konferenzen in Florenz und Paris unterzeichneten elf europäische Regierungen die Vereinbarung zu einem provisorischen CERN. Im Mai 1952 traf sich der provisorische Rat zum ersten Mal in Paris. Am 29. Juni 1953, auf der 6. Konferenz des provisorischen CERN in Paris, unterzeichneten Vertreter der zwölf europäischen Staaten die Gründungsurkunde. Im Oktober 1953 wurde auf einer Konferenz in Amsterdam der Sitz des CERN und dessen Laboratoriums in der Nähe von Genf bestimmt. Am 24. Februar 1954 erfolgte die 1. Konferenz des CERN-Rates nach der Gründung in Genf. Am 29. September 1954 ratifizierten sieben der zwölf Mitgliedstaaten den Staatsvertrag zur Gründung. Am 10. Juni 1955 erfolgte die Grundsteinlegung des CERN-Laboratoriums durch Felix Bloch, den ersten regulären Generaldirektor des CERN.

Ursprünglich war das CERN vor allem für die Forschung im Bereich der Kernenergie vorgesehen, schon bald entstanden aber die ersten Teilchenbeschleuniger. 1957 wurde das Synchro-Zyklotron (SC), das Protonen auf bis zu 600 MeV beschleunigte, in Betrieb genommen, das erst nach über 33 Jahren Betrieb 1990 abgeschaltet werden sollte. Am 24. November 1959 folgte das Protonen-Synchrotron (PS) mit einer (damals weltweit höchsten) Protonenergie von 28 GeV, es arbeitet heute noch als Vorbeschleuniger. 1965 erfolgte eine Vereinbarung mit Frankreich, die geplanten Protonen-Speicherringe, Intersecting Storage Rings (ISR) genannt, auch auf französischen Boden auszubauen. 1968 erfand Georges Charpak einen Teilchendetektor, der in einer gasgefüllten Kammer eine große Anzahl parallel angeordneter Drähte zur besseren Orts- und Energieauflösung enthielt. Er revolutionierte mit dieser Drahtkammer den Teilchennachweis und erhielt 1992 den Nobelpreis für Physik. 1970 belief sich das Budget des CERN auf 370 Millionen Schweizer Franken. Die Kosten wurden 1970 zu 23 Prozent durch die Bundesrepublik Deutschland, zu 22 Prozent durch das Vereinigte Königreich und zu 20 Prozent von Frankreich getragen.

1970/71 gingen die großen Blasenkammern Gargamelle und BEBC zur Untersuchung von Neutrino-Reaktionen in Betrieb. 1971 wurde auch der ISR fertiggestellt. 1973 gelang mit Gargamelle die Entdeckung der neutralen Ströme der Z0-Teilchen durch André Lagarrigue. 1976 folgte als neuer Beschleuniger das Super-Protonen-Synchrotron (SPS), das auf einem Bahnumfang von 7 km Protonen mit 400 GeV liefert. 1981 wurde es zum Proton-Antiproton-Collider ausgebaut; dabei wurde die Technik der stochastischen Kühlung von Simon van der Meer genutzt. Im Mai 1983 wurden am CERN die W- und Z-Bosonen entdeckt, Carlo Rubbia und Simon van der Meer erhielten dafür 1984 den Nobelpreis.

Die im Laufe der über 60-jährigen Geschichte verwendeten und inzwischen abgebauten oder außer Betrieb gesetzten Beschleuniger sind:

Im August 1989 ging der Large Electron-Positron Collider (LEP) in Betrieb, einer der größten jemals gebauten Beschleuniger. In einem Tunnel von 27 km Länge kollidierten hier an ausgewählten Orten Elektronen und ihre Antiteilchen, die Positronen, mit Energien von 100 GeV. 1996 wurden am LEAR-Speicherring (Low Energy Antiproton Ring) erstmals Antiwasserstoffatome produziert, es gab dabei erste Hinweise auf geringfügige Unterschiede zwischen Materie und Antimaterie (CP-Verletzung), was 2001 durch ein weiteres Experiment bestätigt wurde.

Die vier Detektoren am LEP wurden für den Test des Standardmodells entwickelt. Sie wurden nach erfolgreichem Betrieb abgebaut, um Platz für die LHC-Detektoren zu schaffen. Es handelte sich um die folgenden LEP-Detektoren:

Im Jahre 1999 begannen die Bauarbeiten für den LHC in dem Tunnel des Large Electron-Positron Colliders. Im Jahre 2000 wurde der LEP endgültig abgeschaltet.

Am CERN werden der Aufbau der Materie und die fundamentalen Wechselwirkungen zwischen den Elementarteilchen erforscht, also die grundlegende Frage, woraus das Universum besteht und wie es funktioniert. Mit großen Teilchenbeschleunigern werden Teilchen auf nahezu Lichtgeschwindigkeit beschleunigt und zur Kollision gebracht. Mit einer Vielzahl unterschiedlicher Teilchendetektoren werden sodann die Flugbahnen der bei den Kollisionen entstandenen Teilchen rekonstruiert, woraus sich wiederum Rückschlüsse auf die Eigenschaften der kollidierten sowie der neu entstandenen Teilchen ziehen lassen. Dies ist mit einem enormen technischen Aufwand für die Herstellung und den Betrieb der Anlagen sowie mit extremen Anforderungen an die Rechnerleistung zwecks Datenauswertung verbunden. Auch aus diesem Grund wird CERN international betrieben und finanziert.

Am Anfang der Experimente stehen Beschleuniger, welche den Teilchen die für die Untersuchungen notwendige kinetische Energie verleihen. Hervorzuheben sind das Super Proton Synchrotron (SPS) für die Vorbeschleunigung und der Large Hadron Collider (LHC), der Große Hadronen-Speicherring, der bei weitem größte und aufwendigste Beschleuniger, der am Anfang vieler Experimente steht. Weitere Anlagen sind:

Der Large Hadron Collider (LHC) ist der größte Teilchenbeschleuniger der Welt. Der Beschleunigerring hat einen Umfang von 26.659 m und enthält 9.300 Magnete. Zur Durchführung der Experimente muss der Speicherring in zwei Schritten auf die Betriebstemperatur heruntergekühlt werden. Im ersten Schritt werden die Magnete mit Hilfe von flüssigem Stickstoff auf 80 K (−193,2 °C), in einem zweiten Schritt mittels flüssigen Heliums auf 1,9 K (−271,25 °C) heruntergekühlt. Anschließend wird die Anlage kontrolliert hochgefahren. Die Teilchen werden in mehreren Umläufen auf nahezu Lichtgeschwindigkeit beschleunigt und mit extrem hoher kinetischer Energie zur Kollision gebracht.

Am 8. August 2008 wurden die ersten Protonen in den LHC geschossen, am 10. September 2008 folgte der erste Rundumlauf von Protonen. Noch vor dem 21. Oktober 2008 sollte es zu den ersten Protonen-Kollisionen kommen; dieser Termin konnte jedoch auf Grund der erzwungenen Abschaltung nach einem Problem nicht eingehalten werden. Am 23. Oktober 2009 wurden erneut Protonen in den Tunnel injiziert. Am 30. März 2010 gelang es erstmals, Protonen mit einer Rekordenergie von jeweils 3,5 TeV (also insgesamt 7 TeV) aufeinandertreffen zu lassen. 2012 wurde die Gesamtenergie auf 8 TeV erhöht, 2015 auf 13 TeV. Später sind 14 TeV geplant.

Es wurden auch erfolgreich Blei-Ionen zur Kollision gebracht, sowie Kollisionen von Bleiionen mit Protonen herbeigeführt. Nach einer mehrjährigen Pause (LS1, Long Shutdown 1), die für Reparaturen und Verbesserungen genutzt wurde, ist der LHC seit dem 5. April 2015 wieder in Betrieb.

Die bei den Kollisionen entstehenden Teilchen werden im Rahmen verschiedener Experimente mit Hilfe von Detektoren registriert und anschließend von internationalen Wissenschaftler-Teams mittels spezieller Computerprogramme analysiert. Die Experimente bzw. Detektoren am LHC sind:

Neben den Experimenten am LHC werden mit den anderen Beschleunigern und Detektoren weitere Experimente durchgeführt zur Erforschung von Hadronstruktur und -produktion, Neutrinooszillation und Dunkler Materie:

Daneben werden eine Vielzahl kleinerer Experimente durchgeführt, so unter anderem:

Um die ungeheuren Datenmengen, die seit November 2009 an den vier großen Experimenten ALICE, ATLAS, CMS und LHCb des LHC anfallen, verarbeiten zu können, wurde das LHC Computing Grid, ein System für verteiltes Rechnen, entwickelt.

Auch das World Wide Web hat seine Ursprünge am CERN. Um Forschungsergebnisse auf einfache Art und Weise unter den Wissenschaftlern austauschen zu können, wurde das Konzept bereits 1989 quasi als Nebenprodukt der eigentlichen Forschungsarbeit von Tim Berners-Lee entwickelt.

Viele fundamentale Erkenntnisse über den Aufbau der Materie und die Grundkräfte der Physik wurden am CERN gewonnen. Die Entdeckung der W- und Z-Bosonen gelang 1983 Carlo Rubbia und Simon van der Meer, für die sie 1984 den Nobelpreis erhielten. Auch der erste Hinweis auf die Entstehung eines Quark-Gluon-Plasmas bei extrem hohen Temperaturen wurde 1999 am Relativistic Heavy Ion Collider (RHIC) gefunden. Folgeexperimente laufen am LHC mit dem ALICE-Detektor. Im Jahre 2002 gelang die Produktion und Speicherung von mehreren tausend „kalten“ Antiwasserstoff-Atomen durch die ATHENA-Kollaboration, ebenso begann die Datenaufnahme im COMPASS-Experiment.

Ein weiteres Forschungsfeld ist die Erforschung des Higgs-Bosons, eines wichtigen Teils des Standardmodells. Nach jahrzehntelanger Suche wurde 2012 ein Teilchen gefunden, das in allen gemessenen Eigenschaften mit dem gesuchten Higgs-Boson übereinstimmt. Die Erhöhung der Energie am Large Hadron Collider von 7 auf 13 TeV ermöglicht es, dessen Eigenschaften genauer zu vermessen. Dies ist auch für die Suche nach schweren Teilchen notwendig sowie für die genauere Untersuchung des Quark-Gluon-Plasmas.

Das Hauptgelände des CERN liegt bei Meyrin (nahe Genf) in der Schweiz, nahe der Grenze zu Frankreich; große Teile der Beschleunigerringe und auch einige unterirdische Experimentierplätze befinden sich auf französischem Staatsgebiet. Das CERN hat damit, wie auch das Europäische Laboratorium für Molekularbiologie, als internationales Forschungszentrum eine besondere Stellung. Das oberste Entscheidungsgremium der Organisation ist der "Rat des CERN", in welchen alle Mitgliedsstaaten jeweils zwei Delegierte entsenden: einen Repräsentanten der Regierung und einen Wissenschaftler. Die offiziellen Arbeitssprachen des CERN sind Englisch und Französisch.

Seit Dezember 2012 verfügt das CERN über einen Beobachterstatus bei der Generalversammlung der Vereinten Nationen. Dieser besondere Status verleiht dem CERN das Recht, bei Konferenzen der Generalversammlung zu sprechen, bei formellen Abstimmungen zu votieren und UN-Resolutionen zu unterstützen und unterzeichnen, nicht jedoch über sie mit abzustimmen.
Der Status wurde verliehen, nachdem die Schweiz und Frankreich unter Befürwortung aller weiteren 18 Mitgliedsstaaten sowie diverser weiterer Nicht-Mitgliedsstaaten einen entsprechenden Antrag gestellt hatten.
Begründet wurde die Entscheidung mit der wichtigen Rolle des CERN in Wissenschaft und Entwicklung und dem Aspekt der außerordentlichen internationalen Zusammenarbeit.

Der rechtliche Status des CERN beruht auf einem Abkommen zwischen der Schweiz und der Europäischen Organisation für Kernphysikalische Forschung vom 11. Juni 1955. Im Abkommen werden die internationale Rechtspersönlichkeit und die Rechtsfähigkeit der Organisation festgelegt. Demnach genießt das CERN die bei internationalen Organisationen üblichen Immunitäten und Vorrechte, soweit sie zur Erfüllung ihrer Aufgaben notwendig sind. Auch die natürlichen Personen, die das CERN nach außen hin vertreten, genießen in der Schweiz Immunität. Das CERN unterliegt weder der Schweizer Gerichtsbarkeit noch dem Schweizer Steuerregime.

Die Gründungsmitglieder 1954 waren die Schweiz, Belgien, Dänemark, Bundesrepublik Deutschland, Frankreich, Griechenland, Vereinigtes Königreich, Italien, Jugoslawien (bis 1961), Niederlande, Norwegen und Schweden.

Es folgten weitere Staaten: Österreich (1959), Spanien (1961–1968 und ab 1983), Portugal (1986), Finnland (1991), Polen (1991), Ungarn (1992), Tschechien (1993), Slowakei (1993), Bulgarien (1999), Israel (2013) und Rumänien (2016).

Die Anteile der Finanzierung haben dabei keinen beziehungsweise nur geringen Einfluss auf die Vertretung der einzelnen Nationalitäten. Dies spiegelt sich sowohl bei den offiziellen Arbeitssprachen Englisch und Französisch, als auch bei der Herkunft der beschäftigten Mitarbeiter () und Gastwissenschaftler () wider. Deutschland ist hier mit 1194 Gastwissenschaftlern (Stand: 2015) im Vergleich zu seinem Finanzierungsanteil deutlich unterrepräsentiert. Auch auf die Anzahl der in den "Rat des CERN" entsandten Vertreter haben die Anteile an der Finanzierung keinen Einfluss.

Serbien, Slowenien und die Republik Zypern sind assoziierte Mitglieder im Vorstadium einer Vollmitgliedschaft.
Pakistan, Indien, die Ukraine, die Türkei und Litauen sind assoziierte Mitglieder.

Beobachterstatus haben gegenwärtig die Staaten Japan, Russland und die Vereinigten Staaten sowie die internationalen Organisationen Europäische Kommission, JINR und UNESCO.

CERN hat zudem Kooperationsvereinbarungen mit mehr als 40 weiteren Staaten abgeschlossen, darunter Australien, Brasilien, Volksrepublik China, Iran, Kanada und Südkorea.





</doc>
<doc id="964" url="https://de.wikipedia.org/wiki?curid=964" title="Charles Sanders Peirce">
Charles Sanders Peirce

Charles Santiago Sanders Peirce (ausgesprochen: /'pɜrs/ wie: "pörs") (* 10. September 1839 in Cambridge, Massachusetts; † 19. April 1914 in Milford, Pennsylvania) war ein US-amerikanischer Mathematiker, Philosoph, Logiker und Semiotiker.

Peirce gehört neben William James und John Dewey zu den maßgeblichen Denkern des Pragmatismus, wobei er sich später deutlich von den Entwicklungen der pragmatischen Philosophie distanzierte (insbesondere wendete er sich gegen die relativistische Nützlichkeitsphilosophie, die von vielen Pragmatisten als Grundprinzip der Wahrheit mit dem Pragmatismus gelehrt wurde) und sein philosophisches Konzept fortan "Pragmatizismus" nannte, um sich von James, Dewey, Schiller und Royce abzugrenzen; außerdem gilt er als Begründer der modernen Semiotik. Bertrand Russell und Karl-Otto Apel bezeichneten ihn als den „größten amerikanischen Denker“, Karl Popper betrachtete ihn sogar als „einen der größten Philosophen aller Zeiten“.

Peirce leistete wichtige Beiträge zur modernen Logik, unter anderem:

Peirce beschäftigte sich auch mit logischen Schlussfolgerungsweisen und führte neben der bekannten Induktion und Deduktion die Abduktion (Hypothese) als dritte Schlussfolgerungsweise in die Logik ein. Aus der Abfolge von Abduktion, Deduktion und Induktion entwickelte er einen erkenntnis- und wissenschaftstheoretischen Ansatz.

Peirce wurde in Cambridge, Massachusetts, als zweites von fünf Kindern von Sarah und Benjamin Peirce (1809–1880) geboren. Sein Vater war Professor für Astronomie und Mathematik an der Harvard-Universität und nachweislich der erste ernsthaft forschende Mathematiker in den USA. Sein Lebensumfeld war das eines gut situierten Bildungsbürgertums. Schon als Junge erhielt Peirce von einem Onkel die Einrichtung eines Chemielabors. Sein Vater erkannte seine Begabung und bemühte sich, ihm eine umfassende Bildung zu vermitteln. Mit 16 Jahren begann er die "Kritik der reinen Vernunft" zu lesen. Er benötigte für das Studium des Werkes, mit dem er sich täglich mehrere Stunden auseinandersetzte, drei Jahre, nach denen er nach eigener Aussage das Buch fast auswendig konnte. Peirce studierte in Harvard und an der Lawrence Scientific School. Er bestand 1862 den Master of Arts und war einer der ersten (1863), die den Bachelor of Science im Fach Chemie ablegten – mit Summa cum laude. Noch während seines Chemiestudiums heiratete er Harriett Melusina Fay, die aus einer prominenten Pfarrersfamilie stammte. Sie veröffentlichte Bücher zu allgemein politischen Themen und war in der Frauenrechtsbewegung aktiv.

Von 1859 bis 1891 war er mit Unterbrechungen bei der United States Coast and Geodetic Survey tätig. Ab 1861 hatte er eine reguläre Planstelle, so dass er nicht am amerikanischen Sezessionskrieg teilnehmen musste. Er erhielt diese Stelle auf Vermittlung seines Vaters, der als einer der Gründer dieser Behörde dort als Aufsichtsrat fungierte. Peirce' Aufgabenfeld lag im Bereich Geodäsie und Gravimetrie in der Weiterentwicklung der Anwendung von Pendeln zur Bestimmung von lokalen Abweichungen in der Erdgravitation. In Harvard hielt Peirce zwischen 1864 und 1870 nebenberuflich Vorlesungen über Wissenschaftsgeschichte und Wissenschaftstheorie. Schon zu diesem Zeitpunkt findet man in den Manuskripten der Vorlesungen fast alle Grundthemen der Philosophie, die ihn sein Leben lang beschäftigten. Zu Beginn war er sehr stark von Kant geprägt, setzte sich aber intensiv mit Fragen der Logik auseinander und entwickelte zunächst seine eigene Kategorienlehre. Die logischen Arbeiten standen in den ersten Jahren im Vordergrund. So befasste er sich 1865 mit der neuen Logik von George Boole und 1866 erhielt er einen Sonderdruck von Augustus De Morgans Logik der Relative, die seiner Denkentwicklung einen wesentlichen Impuls gab. 1867 wurde er in die American Academy of Arts and Sciences gewählt, 1877 in die National Academy of Sciences. 1868 veröffentlichte Peirce eine Artikelserie in den Proceedings der American Academy of Arts and Sciences (PAAAS, Band 7, 1868).
Kurz darauf publizierte er im Journal of Speculative Philosophy die zweite Artikelserie
Ab 1869 schrieb Peirce in unregelmäßigen Abständen eine Vielzahl von Rezensionen und kleineren Beiträgen in "The Nation", der Sonntagsausgabe der New York Evening Post. Zum Jahreswechsel 1869/70 hielt Peirce erneut Vorlesungen über die Geschichte der Logik mit Schwerpunkt "British Logicians" an der Harvard-Universität.

In den 1860er Jahren begleitete Peirce interessiert die astronomischen Forschungen des gleichaltrigen George Mary Searle, der in dieser Zeit ebenfalls für die Coast Survey und am Harvard Observatory tätig war. Von 1869 bis 1872 arbeitete Peirce dann selbst im astronomischen Observatorium von Harvard als Assistent über Fragen der Photometrie zur Bestimmung der Helligkeit von Sternen und der Struktur der Milchstraße. 1870 erschien eine kleine, für Peirce und Logiker aber wichtige Schrift über die Logik der Verwandten (Begriffe), die auch als Vortrag vor der American Academy of Arts and Sciences veröffentlicht wurde unter dem Titel "Description of a Notation for the Logic of Relatives, Resulting from the Amplification of Boole's Calculus of Logic" (CP 3.45–148). Wichtig für Peirce und auch für James war ein Zirkel junger Wissenschaftler verschiedener Disziplinen Anfang der 1870er Jahre, der als "metaphysischer Club" bezeichnet wurde. Hier lernte Peirce die Philosophie von Alexander Bain kennen, von dem er das Prinzip des Zweifels und der Überzeugungen, die das Handeln der Menschen bestimmen, übernahm. Peirce trug seine Grundgedanken zum Pragmatismus vor und stellte sie zur Debatte, woraus später seine wichtige Aufsatzreihe von 1877/78 entstand. Diese Veröffentlichung in Popular Science wird gewöhnlich als die Geburtsstunde des Pragmatismus bezeichnet. Die Aufsatzreihe umfasst die Titel

Zwischen 1871 und 1888 konnte Peirce im Rahmen seiner geodätischen Aufgabenstellung insgesamt fünf jeweils mehrmonatige Forschungsreisen nach Europa unternehmen, wo er eine Reihe prominenter Wissenschaftler traf. In einem Bericht an den Coast Survey legte Peirce 1879 eine neue Methode der Kartenprojektion vor, die er "Quincunx" oder auch "quincunial projection" nannte. Diese Art der Projektion wurde (in erweiterter Fassung) noch im Zweiten Weltkrieg von der Coast Survey als besonders geeignet zur Aufzeichnung internationaler Flugrouten vorgeschlagen. 1879 wurde Peirce "half-time lecturer of logic" an der Johns-Hopkins-Universität in Baltimore, seine einzige akademische Festanstellung. Dort waren unter anderem John Dewey und Josiah Royce seine Hörer. In dieser Zeit kam es zur Veröffentlichung von "A Brief Description of the Algebra of Relatives" (1882, Privatdruck) und der Herausgabe der "Studies in Logic by Members of the Johns Hopkins University" (1883).

Peirce hatte außer dieser Anstellung niemals wieder eine feste akademische Stelle. Von seinen Biographen wird als Ursache seine schwierige Persönlichkeit gesehen. Es gibt Vermutungen, dass er manisch-depressiv gewesen sei (Brent). Seine erste Frau verließ ihn 1876 während eines Europaaufenthaltes, von dem sie allein zurückkehrte. Über den Grund haben sich beide nie geäußert. Schon bald darauf ging er ein Verhältnis mit Juliette Froissy (Geburtsname nicht gesichert) ein, mit der er bis zu seiner Scheidung von Fay 1883 unverheiratet zusammenlebte. Schon zwei Tage nach der Scheidung heiratete er Juliette. Vermutlich aufgrund des damit verbundenen Skandals verlor er 1884 seinen Posten an der Johns-Hopkins-Universität.
1887 nutzte Peirce die Erbschaft seiner Eltern, um sich eine Farm bei Milford, Pennsylvania, zu kaufen, wo er – mit Ausnahme einiger Reisen, vor allem zu Vorträgen – den Rest seines Lebens verbrachte, unablässig schreibend. Ende der 1880er Jahre leistete Peirce einen wesentlichen Beitrag zu "The Century Dictionary and Cyclopedia", einer von James Mark Baldwin herausgegebenen, 450.000 Begriffe und Namen umfassenden Enzyklopädie in den Bereichen Mechanik, Mathematik, Astronomie, Astrologie und Philosophie. Nachdem er einen umfangreichen wissenschaftlichen Bericht über seine Pendelversuche an die US Coast Survey geliefert hatte, dieser aber von dem erst seit kurzem amtierenden Superintendenten Thomas C. Mendenhall abgelehnt worden war, gab Peirce seine Stellung bei dieser Behörde nach über 30 Jahren Ende 1891 auf. Damit hatte er seine gesicherte wirtschaftliche Lebensgrundlage verloren und musste nun sein Geld ausschließlich durch Unterricht, Übersetzungen, Vorträge und Veröffentlichungen verdienen. Eine wesentliche Basis waren Lexikonbeiträge sowie die Rezensionen in der Zeitschrift "The Nation", mit deren Herausgeber Wendell Phillips Garrison Peirce freundschaftlich verbunden war. Durch eine weitere Freundschaft mit einem Richter fand er auch ab ca. 1890 Zugang zu Paul Carus, dem Herausgeber der Zeitschrift "The Monist", in der er eine Vielzahl von Aufsätzen veröffentlichte. Erst spät baute Peirce seine metaphysischen Gedanken aus, insbesondere den des Kontinuums und die Integration der Evolution in seine Philosophie. Diesen Themenkreis behandelte Peirce in seiner ersten Aufsatzserie in "The Monist" (1891–1893):
Den Schwerpunkt Logik und Methoden logischen Schließens hatte eine Artikelserie aus dem Jahr 1892 in "The Open Court", einer ebenfalls von Carus herausgegebenen Zeitschrift:
Der formale und mathematische Anspruch dieser Artikelreihe war so hoch, dass zwei weitere Artikel, deren Manuskripte bereits fertiggestellt waren, nicht mehr zur Veröffentlichung kamen:
Peirce Verhältnis zur Religion ergibt sich unter anderem aus drei Artikeln in "The Open Court" aus dem Jahr 1893, in denen er sich einerseits für eine klare Trennung von Wissenschaft und Religion aussprach, andererseits Verkrustungen und Zersplitterungen der verfassten Kirchen kritisierte. Liebe ist das Prinzip für das Leben und die einzige Grundlage einer Universalreligion. Die Titel der Aufsätze lauten:

In den Folgejahren begann er eine Reihe von Buchprojekten, die sich jedoch nicht realisieren ließen, obwohl die Manuskripte zum Teil schon weit gediehen waren. Im Winter 1892/93 konnte Peirce am Lowell Institut 12 Vorlesungen über Wissenschaftsgeschichte halten. Im Lauf der Zeit geriet er in immer größere finanzielle Schwierigkeiten, die ihn bis an sein Lebensende begleiteten. Oft genug fehlte das Geld, um auch nur Nahrungsmittel oder Brennmaterial für die Heizung zu beschaffen. Auf Vermittlung von William James, mit dem er seit der Zeit seines Chemiestudiums befreundet war, konnte Peirce im Jahr 1898 eine Vorlesungsreihe in Cambridge mit dem Generalthema "Reasoning and the Logic of Things" halten. 1903 konnte James nochmals helfen, so dass Peirce die Möglichkeit einer Vorlesungsreihe in Harvard über "Pragmatism as a Principle and Method of Right Thinking" erhielt. Ebenfalls 1903 konnte Peirce acht Vorlesungen am Lowell Institut über "Some Topics of Logic Bearing on Questions Now Vexed" halten. Die drei Vorlesungsreihen sind für die Rezeption wichtig, da Peirce sich auf Drängen von James bemüht hatte, seine Vorlesungen nicht zu schwierig zu gestalten, sondern auf ein allgemeines Publikum auszurichten. So hat Peirce in einem relativ reifen Stadium seines Denkens wesentliche Eckpunkte seiner Philosophie in einem geschlossenen Zusammenhang dargestellt, allerdings nicht veröffentlicht. Einen anderen Überblick über das Denken von Peirce gibt eine Bewerbung aus dem Jahr 1902 auf ein Stipendium der Carnegie Institution, in der er in einem umfangreichen Exposé darlegt, wie er seine Philosophie in einem geschlossenen Werk darstellen könnte. Seine Bewerbung wurde jedoch abgelehnt. Ebenfalls in das Jahr 1903 fällt die Rezension des Buches "What is Meaning" von Victoria Lady Welby. Diese hatte für die Klärung des Begriffs Bedeutung einen semiotischen Ansatz mit drei Graden der Signifikation gefunden. Hieraus entspann sich ein langjähriger Briefwechsel, aus dem sich umfangreiche Darlegungen zur Semiotik ergaben. In den Jahren 1905 bis 1907 distanzierte Peirce sich immer mehr von den anderen Pragmatisten und nannte schließlich seine Philosophie Pragmatizismus. Ab 1906 wurde er von einer Stiftung unterstützt, die James ins Leben gerufen hatte. Peirce blieb ohne Kinder und starb 1914 an Krebs.

Peirce hat seine Gedanken zur Mathematik, Logik und Philosophie niemals in einer geschlossenen Arbeit publiziert. Während seiner Tätigkeit an der Johns-Hopkins-Universität gab er die "Studies in Logic" (1883) heraus, die einige Kapitel von ihm selbst sowie weitere seiner Doktoranden enthielten. Sein Ruf ist ursprünglich fast ausschließlich begründet durch Aufsätze in Fachzeitschriften.

Nach seinem Tod erwarb die Harvard-Universität auf Veranlassung von Josiah Royce die Papiere aus seinem Nachlass. Da Royce bereits 1916 verstarb, kam es nicht zur geplanten Aufarbeitung des Materials. Es wurde ein kleiner unvollständiger Katalog verfasst. Die Unterlagen wurden in 83 Kisten verpackt und verschwanden zunächst in den Archiven der Universität. Dass Peirce überhaupt weiter rezipiert wurde, verdankt sich Morris Raphael Cohen, der 1923 einen Sammelband unter dem Titel "Chance, Love and Logic" mit einigen wichtigen Aufsätzen von Peirce herausgab. Beigefügt ist auch ein Aufsatz von Dewey aus dem Jahr 1916, den dieser im Rückblick auf Peirce verfasst hatte.

Das Vorhaben der Herausgabe der Werke von Peirce wurde erst in den 1930er-Jahren in Harvard von Charles Hartshorne und Paul Weiss aufgegriffen. Aus der Fülle des gesamten Materials wurden die meisten Veröffentlichungen sowie umfangreiches unveröffentlichtes Material thematisch zusammengestellt und zwischen 1931 und 1935 in sechs Bänden als "Collected Papers" publiziert. Die Themen der Bände lauten:
Zwei weitere Bände wurden mit Förderung der Rockefeller Foundation erst nach dem Zweiten Weltkrieg ergänzt und von Arthur W. Burks herausgegeben:

Erst mit der Herausgabe der "Collected Papers" begann man sich überhaupt mit den Arbeiten von Peirce intensiver auseinanderzusetzen. Durch die systematische Zusammenstellung der "Collected Papers" ist allerdings der innere Zusammenhang des Werkes teilweise verloren gegangen. So wurden Aufsatzreihen und Vorlesungen teilweise auf die verschiedenen Bände verteilt und Arbeiten aus verschiedenen Entwicklungsphasen nebeneinandergestellt, obwohl bei Peirce eine deutliche Entwicklung des Denkens zu konstatieren ist. Zum Teil wurden sogar zum Zweck der systematischen Darstellung Fragmente verschiedener, zeitlich nicht zusammengehörender Texte zusammengestellt.

Erst nach Veröffentlichung der "Collected Papers" begann man mit einer systematischen Katalogisierung und Mikroverfilmung. Die Mikroverfilmung war erst 1966 (vorläufig) abgeschlossen. Immer wieder wurden in den Archiven Ergänzungen gefunden, zuletzt 1969, so dass die Mikrofilmdateien und die Kataloge jeweils nachgepflegt werden mussten. Die aktuelle Katalogisierung basiert auf dem Jahr 1971. Erst dann wurde klar, dass Peirce neben den 12.000 Druckseiten seines Werkes ungefähr 1650 unveröffentlichte Manuskripte mit ca. 80.000 handschriftlichen Seiten hinterlassen hatte, von denen der größte Teil auch heute noch nicht veröffentlicht ist. Ein Teil der Unterlagen, der nicht nach Harvard gegangen war, ging verloren, weil er nach dem Tode von Peirce' Frau Juliette verbrannt wurde. Da die "Collected Papers" unvollständig sind und auch nicht allen wissenschaftlichen Ansprüchen genügen, wurde in den 1970er Jahren im Rahmen des sogenannten "Peirce Edition Projects" mit einer kritischen, chronologisch organisierten Edition begonnen, in der bis 2004 sechs von geplanten ca. 30 Bänden erschienen sind, die die Zeit bis 1890 abdecken. Eine wesentliche Ergänzung der gedruckten Werke ist die Ausgabe vorwiegend mathematisch-naturwissenschaftlicher Schriften in vier Bänden (fünf Teilbänden) unter dem Titel "The New Elements of Mathematics" aus dem Jahr 1976 durch Carolyn Eisele. Die Rezensionen und Beiträge für die Zeitschrift "The Nation" sind in der vierbändigen Ausgabe "C.S. Peirce: Contributions to the Nation" von Ketner/Cook aus dem Jahr 1975–1979 erfasst. Eine weitere wichtige Quelle ist "Semiotic and Significs. The Correspondence between CHARLES S. PEIRCE and VICTORIA LADY WELBY", herausgegeben von Charles S. Hardwick (1977).

Peirce' Schriften umfassen ein weites Feld an Disziplinen: von der Astronomie über Meteorologie, Geodäsie, Mathematik, Logik, Philosophie, Geschichte und Philosophie der Wissenschaften, Sprachwissenschaft, Ökonomie bis zur Psychologie. Sein Werk zu diesen Themen fand nun in jüngerer Zeit erneute Aufmerksamkeit und Zustimmung. Diese Wiederbelebung ist nicht nur angeregt durch die Vorwegnahme aktueller wissenschaftlicher Entwicklungen durch Peirce, sondern vor allem auch dadurch, dass seine triadische Philosophie und Semiotik sowohl in der modernen Logik als auch in vielen Wissenschaftsbereichen von der Linguistik über die Rechts- und Religionswissenschaften bis hin zur Informatik einen Schlüssel zur methodischen Strukturierung des Stoffs für die praktische Arbeit bietet.

Peirce bewies 1881 unabhängig von Ferdinand Georg Frobenius den Satz von Frobenius zur Klassifizierung der endlich-dimensionalen reellen assoziativen Divisionsalgebren.

Als Grundlage aller weiteren Betrachtungen entwickelte Peirce eine Kategorienlehre, die sich nicht wie bei Kant mit den Arten der Erkenntnis, sondern mit Erscheinungsweisen des Seins befasst und die Grundlage seiner Zeichenlehre bildet. Die Kategorien von Peirce können nicht mit Logik beschrieben, sondern nur phänomenologisch untersucht werden. Sie sind in jedem Phänomen enthalten und daher universal. Begrifflich unterschied Peirce rein formal "Erstheit", "Zweitheit" und "Drittheit" als Formen, in denen alles, was ist, sich widerspiegelt:

In einer kritischen Analyse der Kategorien Kants zeigte Peirce, dass diese auf die Funktion der Modalität zurückführbar sind und eine Entsprechung mit seinen eigenen Kategorien haben, indem man Möglichkeit = Erstheit, Aktualität = Zweitheit und Notwendigkeit = Drittheit setzt. Ähnlich verhält es sich mit den Relationen Qualität (1), Tatsache (2) und Verhalten bzw. Gesetz (3) sowie mit den Begriffen Gegenstand (1), Relation (2) und Repräsentation (3). Die Triade war für Peirce eine grundlegende Perspektive auf alle Phänomene, und er sah sie sogar in der christlichen Dreifaltigkeit bestätigt. Die Kategorien sind zwar gedanklich unterscheidbar, aber sie sind nicht separierbar. Sie sind jeweils alle in jedem Gedanken enthalten und nur in einem langen Prozess der Aneignung mit Klarheit zu erfassen. Dementsprechend gibt es von Peirce immer wieder Texte verschiedener Annäherung an die Kategorien.

Peirce' Auffassung vom Bewusstsein knüpft eng an die Kategorienlehre an. Er versuchte dabei, die bisherige Unterscheidung des Geistes in der Philosophie (auch bei Kant) in die drei Teile Gefühl (Lust und Schmerz), Willen (Willenskraft und Verlangen) sowie Wissen (Erkenntnis) auf eine wissenschaftlichere, auch für die Psychologie geeignete Grundlage zu stellen. Das noch unreflektierte Bewusstsein als ein Bündel von Repräsentationen ist Erstheit. Im Bewusstsein sind wiederum die Kategorien identifizierbar. Die Erscheinung von Erstheit im Bewusstsein ist das reine Gefühl oder die Gefühls-Qualität, das Gefühl des unmittelbaren Bewusstseins ohne Bezug auf etwas anderes. Man kann es als die unanalysierte Erscheinung aller Qualitäten in einem Moment beschreiben:

Die Erscheinung der Zweitheit im Bewusstsein, die Peirce „Altersense“ nannte, ist die Konfrontation mit dem Anderen, ist das Bewusstsein des Hier und Jetzt. Zur Zweitheit im Bewusstsein zählen Sinnesempfindungen als lebendige Erfahrungen. Ebenso gehört hierzu der Wille oder Wunsch als Empfindung ohne die Reflexion auf das Gewünschte. Die Zweitheit ist die Erfahrung der Dualität. Ebenso wie die Erstheit wird hier noch vom Denken abstrahiert. Weder das reine Gefühl auf der Ebene der Erstheit noch die Empfindung des Gegenübers, des Anderen auf der Ebene der Zweitheit lassen sich konkret in Begriffe fassen. Sobald dies geschieht, bewegt man sich in der Ebene der Drittheit, die die Ebene der Gedanken ist. Zweitheit kann vorwiegend aktiv sein, dann dominiert die Empfindung des Willens. Ist sie hingegen vorwiegend passiv, dann dominieren die Sinnesempfindungen.

Die Erscheinung der Drittheit im Bewusstsein bezeichnete Peirce als „Medisense“, in der die Beziehung zu einem Objekt repräsentiert wird. Hierzu zählen das Denken, das Lernen, das Gewahrsein von etwas Drittem. Dieser Modus des Gewahrseins führt bei genügender Wiederholung zu Verhaltensgewohnheiten.

Die so beschriebene psychologische Struktur des Bewusstseins verband Peirce mit einer physiologischen Sicht, in der die psychischen Prozesse jeweils physische Entsprechungen im Gehirn haben. Er vertrat eine monistische Position:

Selbstbewusstsein entsteht dadurch, dass die Repräsentationen als Zeichen im Bewusstsein sich selbst zum Gegenstand werden. Diese Reflexion ist für Peirce überwiegend dem Bereich des Altersense (der Zweitheit) zuzuordnen, da das Selbstbewusstsein so etwas wie das Wahrnehmen des Selbst ist. (CP 5.225, 5.266) Im Selbstbewusstsein treten sich die Empfindung des Ego, das wir kontrollieren können, und des unkontrollierbaren Non-Ego gegenüber.

Die Drittheit im Bewusstsein führt zu einer erneuten Reflexion, nun auf das Selbstbewusstsein. Hieraus entsteht die Selbstkontrolle, die Selbstüberprüfung und Selbstkorrektur beinhaltet. Peirce begründete mit der Vorstellung der Selbstkontrolle die Fähigkeit, sich zu entscheiden und damit die eigenen Verhaltensgewohnheiten zu beeinflussen. Eine unmittelbar kausale Wirkung aus der Selbstkontrolle sah er nicht. Die kognitive Fähigkeit der Selbstkontrolle hat aber Einfluss auf Einstellungen, die für künftiges Handeln maßgeblich sind. Durch den Vergleich mit Standards ist Selbstkontrolle zugleich Grundlage von moralischen Einstellungen und ethischem Verhalten.

"Wir haben kein Vermögen, ohne Zeichen zu denken." (CP 5.265). Diese Grundannahme der gesamten Philosophie von Peirce ist so auch Ausgangspunkt für seine Theorie der Wahrnehmung.

Wahrnehmung findet durch eine Umwandlung von Sinneseindrücken statt und ist deshalb niemals unmittelbar. Klassisches Beispiel dafür, dass Wahrnehmungen falsch gedeutet werden können, sind die Sinnestäuschungen. Peirce verwendet das Beispiel des blinden Flecks auf der Netzhaut. Trotz dieser Eigenschaft erscheinen Gegenstände als vollständige Bilder.
Peirce unterscheidet das Wahrgenommene (Perzept) und das Wahrnehmungsurteil.

Dabei muss ein Wahrnehmungsurteil nicht in Form von Sprache erfolgen, sondern kann z. B. auch diagrammatisch sein (z. B. die Vorstellung eines Dreiecks).

Das Wahrgenommene ist das Zeichen, das zwischen dem Objekt und dem Wahrnehmungsurteil steht. Der Zugang zu den Objekten erfolgt immer durch die Abbildung des Perzeptes als Zeichen. Das Zeichen hat die Form eines sinnlichen Eindrucks, also eines Bildes, eines Klangs etc.

Das Perzept wird als etwas interpretiert. Ein Ton kann eine Stimme sein, das Klingeln eines Telefons oder der Klang eines Radios.

Das Perzept als Zeichen ist ein so genanntes indexikalisches Zeichen (vgl. unten), das heißt es ist bestimmt zu seiner Relation zum Objekt, wie z. B. der Rauch zum Feuer. Das Wahrnehmungsurteil selbst, also der Rauch als Begriff, ist der Interpretant der Wahrnehmung (des Perzepts). Die Form des Schlusses bei einem Wahrnehmungsurteil nannte Peirce Abduktion: "Abduktion ist der Vorgang, in dem eine erklärende Hypothese gebildet wird." (CP 5.171). Indem wir eine Wahrnehmung haben, nehmen wir an, dass es sich um einen bestimmten Gegenstand handelt. "Die Form der Folgerung ist dann folgende: Die überraschende Tatsache C wird beobachtet; aber wenn A wahr wäre, würde C eine Selbstverständlichkeit sein; folglich besteht Grund zur Vermutung, dass A wahr ist." (CP 5.189). Wenn man einen grauen Schleier in der Luft sieht, kann es sich um Nebel handeln, aber auch um Rauch. Indem man diesen grauen Schleier sieht und schließt, dass es sich um Rauch handelt (z. B. aufgrund der Form oder weil rund herum die Sonne scheint), fällt man ein Wahrnehmungsurteil. Wahrnehmungsurteile sind eine extreme Form der Abduktion, weil sie in aller Regel unbewusst und weitgehend unkontrolliert ablaufen und weil man sie aufgrund der immer aktiven Sinne nicht verneinen kann.

Je öfter sich wiederholende Wahrnehmungsurteile bestätigen, umso mehr werden sie als wahr verinnerlicht und dann zu Denk- und Verhaltensgewohnheiten.

Neben Ferdinand de Saussure ist Peirce einer der Begründer der Semiotik, wobei sein bevorzugter Begriff hierfür „semeiotic“ lautete, während Saussure seinen eigenen Ansatz als „sémiologie“ (Semiologie) bezeichnete. Im Gegensatz zu Saussures Zeichenbegriff, der sich ausschließlich und formal auf Sprache bezieht, so dass hieraus wesentliche Impulse in der Linguistik entstanden, ist Peirce' Zeichenbegriff ganzheitlich. Er enthält neben der Repräsentationsfunktion ebenso eine Erkenntnisfunktion der Zeichen. Gleichfalls darf man die Semiotik von Peirce nicht mit der Unterteilung von Charles W. Morris (Syntax, Semantik und Pragmatik) vermischen (obwohl sich Morris auf Peirce bezieht).

Peirce definierte "semiosis" (siehe auch Semiose) als

Peirce unterteilte Semiotik in spekulative Grammatik, logische Kritik und spekulative Rhetorik. Das Wort „spekulativ“ war dabei für ihn gleichbedeutend mit „theoretisch“.


In der spekulativen Grammatik arbeitete Peirce ein System möglicher Zeichenrelationen aus, in denen die Welt sich dem Menschen vermittelt. Ausgehend von der Triade Objekt – Zeichen – Interpretant unterschied er dabei drei Trichotomien:

Die Zeicheneigenschaft

Ein Quali-Zeichen ist eine Qualität, die als Zeichen wirkt, z. B. die Stille eines Raumes. Quali-Zeichen sind immer Ausdruck von Erstheit. Sin-Zeichen sind Gegenstände oder Sachverhalte, die existieren, ohne dass sie schon mit einem Begriff oder einer Bedeutung belegt sind. Legi-Zeichen sind Regeln, die als Zeichen wirken. So bedeutet die Zahl sechs die Idee einer Anzahl von sechs Gegenständen, z. B. Gläsern oder Stühlen. Die Ausprägung des Legi-Zeichens ist wieder ein Sin–Zeichen. Ob man nun das deutsche Wort „sechs“, die Ziffer ‚6‘ oder das englische Wort „six“ verwendet, sie alle verkörpern die Idee der Zahl sechs.

Die Objekt-Beziehung

Ikone sind Zeichen, die durch eine strukturelle Ähnlichkeit unmittelbar eine Relation zu einem Objekt herstellen. Hierzu zählen Bilder, Piktogramme oder Graphiken. Ein Ikon ist grundsätzlich erstheitlich. Der Index ist insofern ein zweitheitliches Zeichen, als er ohne Beschreibung auf ein Objekt hinweist, also eine dyadische Beziehung zwischen Zeichen und Objekt besteht – das Klingeln verweist darauf, dass jemand vor der Tür steht. Symbole haben hingegen eine Bedeutung. Sie sind nur Zeichen, weil ein Interpret versteht, wofür das Zeichen benutzt wird. Dass ein Tisch mit dem Wort „Tisch“ bezeichnet wird, beruht auf einer Konvention. Verstanden wird das Wort „Tisch“, weil seine Bedeutung zur Gewohnheit geworden ist.

Die Interpretanten-Beziehung

Rhema ist ein Begriff, mit dem ein Gegenstand bezeichnet wird. Es kann auch ein Diagramm oder ein Ton sein. In einer Aussage (Dicent) wird zumindest eine zweistellige Relation hergestellt, also die Eigenschaft eines Gegenstandes oder ein Sachverhalt beschrieben. Das Argument drückt eine gesetzmäßige Beziehung zwischen Aussagen aus, z. B. in Form von Naturgesetzen.

Der Interpretant als die "eigentliche bedeutungstragende Wirkung eines Zeichens" muss nun wieder differenziert werden nach seinem emotionalen, energetischen und logischen Gehalt oder nach seiner unmittelbaren, dynamischen und finalen Wirkung. Er ist unmittelbar, wenn er nur eine Gefühlsqualität ist, z. B. das Empfinden der Stille (Erstheit). Er ist dynamisch, wenn er eine effektive Wirkung auslöst (ein Gefühl oder eine Handlung). Ein Interpretant ist final, wenn er mit einer beabsichtigten Wirkung verbunden ist, z. B. einer Veränderung einer Gewohnheit.

Die eigentliche semiotische Bestimmung eines Zeichens entsteht aus den logisch möglichen Kombinationen der Zeicheneigenschaft mit der Objekt- und der Interpretanten-Beziehung (Quali-Zeichen sind weder als Index noch als Symbol denkbar; Argumente können entsprechend kein Index oder Ikon sein). Bei der Bestimmung von Zeichenrelationen besteht das grundsätzliche Problem, dass einerseits Objekte durch mehrere, auch ihrer Art nach höchst unterschiedliche Zeichen repräsentiert werden können. Andererseits können die jeweiligen Zeichen situationsabhängig verschieden interpretiert werden. Zeichenbeziehungen sind daher immer perspektivisch. Wir wissen immer, dass das Objekt, wie wir es in der Kommunikation oder in der Wahrnehmung erfassen (das unmittelbare Objekt), durch Zeichen vermittelt ist. Als Folge wissen wir auch stets, dass wir uns über die Vermittlung täuschen können und demgemäß unsere Interpretation über das eigentliche Objekt (das dynamische Objekt) gegebenenfalls anpassen müssen.

Im Laufe der Zeit entwickelte Peirce seine Auffassung fort und kam aufgrund der Komplexität der möglichen Vermittlungsweisen von Zeichen zwischen Subjekt und Objekt schließlich zu einem System aus 59.049 (3 hoch 10) möglichen Elementen und Relationen. Ein Grund für diese hohe Anzahl liegt darin, dass er bei jedem Interpretanten die Möglichkeit, selbst Zeichen zu sein, zuließ, wodurch jeweils eine neue kennzeichnende Relation entsteht.

Wie bei anderen Themen schrieb Peirce niemals eine genaue Bestimmung seiner Semiotik. Vielmehr befasste er sich mit dem Thema immer wieder während seines Lebens, wobei er oft seine Auffassung über die Definition von Schlüsselbegriffen veränderte. Bei Liszka (1996) findet sich ein verdienstvoller Versuch einer kohärenten Darlegung. Gerhard Schönrich verweist darauf, dass man Parallelen zu Kant in der Theorie der des Bewusstseins ziehen kann, wenn man die kantischen Begriffe in die von Peirce übersetzt, so etwa die Synthesis bei Kant als Semiose, den Gegenstand als Zeichenobjekt, Begriff oder Regel (im Schematismus) als Interpretant und Vorstellung als Zeichen(mittel).

In seiner Erkenntnistheorie brach Peirce mit der Vorstellung, dass das Subjekt der Maßstab für Erkenntnis ist, wie es seit Descartes und bis hin zu Kant gegolten hatte.

Der zweite grundlegende Aspekt in Peirce’ Erkenntnistheorie ist die evolutionstheoretische Vorstellung, wie er sie in seiner Metaphysik entwickelte (vgl. unten). Der Mensch und sein Denken ist Bestandteil eines Entwicklungsprozesses. Zweck des Denkens ist eine Orientierung in der Welt, indem Zweifel untersucht und durch Forschen feste Überzeugungen gewonnen werden, die geeignet sind, als Grundlage des Handelns zu dienen. Hierin liegt die Vermittlung von Theorie und Praxis.

Das dritte Element der Peirce’schen Erkenntnistheorie ist das Denken in Zeichen.

Denken findet aber nicht in einzelnen, isolierbaren Zeichen statt, sondern als ein Strom von Gedanken im Bewusstsein, als ein kontinuierlicher Prozess.

Diese Ebene der Empfindungen im Bewusstseinsstrom ist die Erstheitlichkeit des Denkens.

Der Prozess der Wahrnehmung (siehe oben) führt die Ebene der Zweitheit in den Erkenntnisprozess ein. Die Bedeutung von Zeichen (Ebene der Drittheit) ergibt sich aber nicht allein aus den Sinnesdaten.

Peirce formulierte seine Überlegungen als Pragmatische Maxime:

Die Bedeutung eines Gedankens liegt also darin, welche Verhaltensweise er erzeugt. Verhaltensweise ist dabei nicht als tatsächlicher Handlungsablauf zu verstehen, sondern als Disposition zu einer möglichen Handlung. "Die Elemente eines jeden Begriffs treten durch die Pforte der Wahrnehmung in das logische Denken ein und verlassen es durch die Pforte des zweckbestimmten Handelns; und was immer seinen Ausweis an diesen beiden Pforten nicht vorweisen kann, muss, als durch die Vernunft nicht genehmigt, eingesperrt werden." (CP 5.212)

Mit diesem Konzept wich Peirce von der klassischen Fragestellung der Erkenntnistheorie ab, für die das Ziel der Erkenntnissuche die Wahrheit ist. Doch der klassische Begriff der Wahrheit als Korrespondenz von Gedanken und Tatsachen (Realität) war für Peirce nicht fassbar, weil er auf dem noch unschärferen Begriff der Realität beruht. Peirce definierte stattdessen Wahrheit pragmatistisch:

In dieser Definition steckt die Vorstellung, dass am Ende aller Tage es möglich sein wird, die Realität vollständig zu erkennen. Dieser Zustand ist aber nur ein Grenzwert, an den die Menschheit sich als Ganzes in einem Prozess des Erkenntnisfortschritts annähert. Wahrheit ist dabei objektiv, insofern sie intersubjektiv ist, d. h. nicht mit einzelnen, individuellen Vorstellungen, sondern in der Kommunikation aller (Forscher) bestimmt wird. Bis zu diesem Zeitpunkt, der in der Lebenspraxis des Menschen nicht erreicht werden kann, besteht aber immer und zu jeder Zeit die Möglichkeit, dass die bisher gewonnenen Überzeugungen falsch sein können und revidiert werden müssen. Peirce nannte diese Grundannahme Fallibilismus, die später dann von Popper neu aufgegriffen wurde. Peirce hat auch nicht ausgeschlossen, dass schon gegenwärtige Überzeugungen in vollem Umfang der Realität entsprechen. Je besser solche Hypothesen überprüft sind und sich bewährt haben, umso größer ist die Wahrscheinlichkeit hierfür. Nur sicher sein kann man sich hierüber nicht.

"Hauptartikel": Abduktion

Erkenntniserweiterung erfolgt nach Peirce ausschließlich durch Abduktion. Sie tritt auf in der Wahrnehmung sowie im schließenden Interpretieren vorhandenen Wissens. Der Mensch gewinnt in der Wahrnehmung bestimmte Überzeugungen, die sich in Gewohnheiten umsetzen, die seine Handlungen und Unterlassungen bestimmen. Entstehen durch die Wahrnehmung unerklärbare Sachverhalte, die keiner Gewohnheit entsprechen, gerät der Mensch in Zweifel und sucht nach einer neuen Orientierung. Er stellt über die zweifelhaften Phänomene Hypothesen auf und überprüft diese solange, bis er hierüber eine neue feste Überzeugung gewinnt ("doubt-belief"-Schema).

Die rationale Umsetzung dieses Schemas der Gewinnung von Überzeugungen erfolgte für Peirce im logischen Denken. Je nach Stadium des "doubt-belief"-Schemas ist die Schlussweise unterschiedlich. Liegen zunächst ein oder wenige Tatbestände vor, erfolgt das Aufstellen der Hypothese, das Peirce „Retroduktion“ oder „Abduktion“ nannte. Liegen genügend Informationen zur Hypothese vor, kann diese als Gesetzmäßigkeit formuliert werden. Die entsprechende Schlussweise ist die Induktion. Die Deduktion schließlich ist die Anwendung der Gesetzmäßigkeit, die allein analytisch ist, also einer strengen Wahrheit unterliegt. Abduktion beruht im Prinzip auf einer instinktiven Grundfähigkeit des Menschen zur Kreativität. Induktion ist durch Erfahrung bestimmt und nur Deduktion ist streng logisch. Zur Verdeutlichung hat Peirce die verschiedenen Schlussweisen, die er als einen ineinander greifenden Interpretationsprozess ansah, im Schema des Syllogismus dargestellt:
Während in der Deduktion von der Regel über den Fall auf das Ergebnis geschlossen wird, sind die Resultate der Schlussfolgerungen der Abduktion und der Induktion nicht notwendig. Sie haben ihre Berechtigung nur als hypothetisch-pragmatische Verfahren im Rahmen des Prozesses zur Absicherung einer Überzeugung und unterliegen den Gesetzen der Wahrscheinlichkeit, wobei der Abduktion aufgrund des spontanen Charakters zumeist eine erheblich geringere Wahrscheinlichkeit zukommt.

Peirce untersuchte in seiner Logik das natürliche Schließen aus Hypothesen und entwickelte hierzu eine eigenständige Logik der Relationen, die er „Logik der Relative“ nannte. Ihm gelangen grundlegende Entdeckungen in der formalen Logik:

Er zeigte, dass die Boolesche Algebra durch eine einfache binäre Operation ausgedrückt werden kann als NAND oder dual als NOR (siehe auch DeMorgan's Gesetz). Weiterhin ergänzte er die Boolesche Algebra um Multiplikation und Exponentiation (Allquantor) und versuchte sie in die allgemeine Algebra zu integrieren.

Ein wenig später, aber unabhängig von Freges "Begriffsschrift", entwickelte er gemeinsam mit seinem Studenten O.H. Mitchell die vollständige Syntax für eine Quantorenlogik, die sich nur in wenigen Zeichen von der späteren Russell-Whitehead-Syntax (1910) unterschied. Ernst Schröder, Leopold Löwenheim von der polnischen Schule und der junge Kurt Gödel verwendeten Peirce' Notation.

Die Unterscheidung zwischen der Quantifizierung erster und zweiter Ebene war der erste Entwurf einer einfachen axiomatischen Satz-Theorie. Die von Peirce konzipierte Theorie reflexiver und transitiver Relationen wurde von Ernst Schröder in dessen "Algebra der Logik" weiterentwickelt.

Zur Anwendung der algebraischen Zeichen in der Logik führte Peirce die logischen Terme "absolute Relative" (monadisch = singuläres Objekt), "einfache Relative" (dyadisch = Anderssein) und "konjugative Relative" (triadisch = Drittheit) ein. Alle mehrstelligen Relationen sind auf triadische Relationen zurückführbar. Diese Reduktionsthese von Peirce, die ihm für den Nachweis seiner Kategorien wichtig war, konnte mittlerweile bewiesen werden. Insbesondere konnte gezeigt werden, dass die triadische Reduktion von Peirce nicht im Widerspruch zur dualistischen Reduktion von Quine steht.

Er erfand die existentiellen Graphen (engl. "existential graphs"), eine graphische Schreibweise für die Aussagenlogik (Alphagraphen), Prädikatenlogik erster Stufe (Betagraphen) und für die Prädikatenlogik höherer Stufe sowie für Modallogik (Gammagraphen). Zusammen mit den Schlussregeln, die er dazu formulierte, bilden die existenziellen Graphen einen Aussagen- bzw. Prädikatenkalkül. Die Graphen sind die Grundlage für die Begriffsgraphen von John F. Sowa und für die diagrammatische Begründung bei Sun Joo-Shin.

In einem Brief an seinen früheren Studenten Allan Marquand von 1886, der erst nach 1950 entdeckt wurde, zeigte er bereits die Anwendungsmöglichkeit der Booleschen Logik auf elektrische Schaltungen, mehr als 50 Jahre vor Claude Shannon. Einer solchen Schaltung, die er auch in zwei verschiedenen Graphiken skizzierte, schrieb er folgende Eigenschaften zu: 1. Entwicklung von Ausdrücken (a, b, c etc.) aus Zeichenketten und Syntaxregeln, 2. Vereinfachung der Ausdrücke, 3. Multiplikation mit Polynomen, 4. Addition. (NEM IV, 632) Es kann gezeigt werden dass eine Verknüpfung der Existential Graphs mit einer solchen mechanischen Lösung möglich ist.

Bemerkenswert ist auch seine Ausarbeitung zu den verschiedenen Zahlensystemen und sein Verweis darauf, dass das Binärsystem besonders geeignet für die maschinelle Verarbeitung sei.

Auch wenn Peirce kein explizites System entwickelte, so kann er doch als systematischer Philosoph im traditionellen Sinne betrachtet werden. Sein Werk befasst sich mit den wissenschaftlichen und logischen Fragen nach Wahrheit und Wissen, die er mit seiner Erfahrung als Logiker und experimenteller Wissenschaftler verband. Peirce war der Überzeugung, dass Wahrheit etwas Vorläufiges ist und bei jeder Aussage ein Faktor an Unsicherheit mit enthalten ist. Für Peirce war der "Fallibilismus" ein Gegenpol gegen den Skeptizismus, der für seine Philosophie keine geringere Bedeutung hatte als der Pragmatismus, den er wiederum als Gegenpol gegen den Positivismus sah.

Peirce leistete wesentliche Beiträge zur deduktiven Logik, war aber vor allem interessiert an der Logik der Wissenschaft und vor allem an der Abduktion, die sich nicht nur im Bereich der wissenschaftlichen Forschung, sondern in allen praktischen Lebensbereichen findet. Sein Pragmatismus kann auch verstanden werden als eine Methode zur Klärung begrifflicher Verwirrung durch die Verknüpfung der Bedeutung von Begriffen mit ihren praktischen Konsequenzen.

Peirce' Pragmatismus hat allerdings nichts zu tun mit dem allgemein üblichen Begriff des pragmatischen Handelns, der oftmals irreführend Rücksichtslosigkeit, Übervorteilung und Vorteilsnahme zumindest indirekt impliziert. Stattdessen suchte Peirce eine objektive, verifizierbare Methode, um die Wahrheit von Wissen zu überprüfen, und zwar in Konkurrenz zu den klassischen Ansätzen von

Das von ihm als wissenschaftliche Methode entwickelte Konzept beschreibt den Wissenschaftsprozess als einen stufenweisen Vorgang, der mit Abduktion aufgrund ungeklärter Phänomene beginnt, bei genügender Sicherheit induktiv Gesetze formuliert, die anhand von Deduktion praktisch geprüft werden. Zum rationalen Wissenschaftsprozess gehörte für ihn ausdrücklich die Wirtschaftlichkeit der Forschung, da Verschwendung angesichts der unendlichen zu lösenden Fragen irrational ist.

Sein Ansatz wurde oft auch als eine neue Form des Fundamentalismus betrachtet, aber durch
beinhaltet er eher eine rationale Basis als eine induktive Verallgemeinerung, die sich rein auf Phänomene beruft. Peirce' Pragmatismus wurde so als erstes wissenschaftliches Verfahren zur Anwendung auf Fragen der Erkenntnistheorie angesehen.

Eine Theorie, die nachweislich erfolgreicher in der Vorhersage und der Nachvollziehbarkeit gegenüber der Lebenswelt ist als ihre Konkurrenten, kann man als näher an der Wahrheit bezeichnen. Dies ist eine in der Wissenschaft angewendete operationale Kennzeichnung von Wahrheit. Anders als andere Pragmatisten hat Peirce niemals explizit eine Theorie der Wahrheit formuliert. Aber seine verstreuten Anmerkungen zur Wahrheit haben eine Reihe erkenntnistheoretischer Wahrheitstheoretiker beeinflusst und waren eine hilfreiche Grundlage für deflationäre und korrespondenztheoretische Theorien der Wahrheit.

Ähnlich wie Kant hat Peirce den spekulativen Charakter der traditionellen Metaphysik vielfach heftig kritisiert. Andererseits hat er immer danach gestrebt, eine mit den Naturwissenschaften verträgliche Idee für eine Erklärung der Grundprinzipien der Lebenswelt zu entwickeln. Ausgangspunkt war für ihn wie in vielen anderen Dingen die Logik und hier insbesondere die von Mill zur Induktion entwickelte Theorie, dass diese ihre Gültigkeit aus der Gleichförmigkeit der Natur herleitet. Peirce kritisierte hieran, dass die Annahme der Gleichförmigkeit als Voraussetzung dann nicht zugleich über die Induktion die Gleichförmigkeit als Ergebnis liefern könne.

Als breit bewanderter und erfahrener Naturwissenschaftler führte Peirce eine Reihe von Argumenten gegen den Determinismus an, für den es aus seiner Sicht keine wissenschaftliche Begründung gibt. Insbesondere betonte er, dass die praktischen Messwerte der angewandten Wissenschaften theoretische Konzepte niemals bestätigen, weil sie in aller Regel aufgrund von Versuchsanordnungen zu ungenau sein müssen. Messergebnisse haben immer eine Verteilung, die durch Regression oder ähnliche Verfahren approximiert werden muss. Alle natürlichen Erscheinungen beinhalten Unregelmäßigkeiten.

Gegen den Determinismus setzte Peirce die Hypothese, dass die Welt eine Zufalls-Welt (Chance-world, CP 6.399) ist. Geht man davon aus, dass es einen Urzustand des völligen (nicht beschreibbaren) Zufalls für das Universum gibt, so ist bereits der erste Entwicklungsschritt eine Wahl aus einer unbegrenzten Anzahl an Möglichkeiten. Jeder weitere Schritt führt wieder zu einer Auswahl bis zum Heute. Das Erklärungsprinzip ist die Evolution als Eigenschaft unserer Welt, die sich aus einer unendlichen Zahl möglicher Welten entwickelt hat und in diesem Entwicklungsprozess fortschreitet.

Dieses Konzept der Welterklärung durch einen fortschreitenden Prozess, der zufällige Ereignisse systematisch beinhaltet, nannte Peirce „Tychismus“. Mit diesem verbunden sind eine umfassende Idee der Evolution, für die die Theorie Darwins nur einen Teil der Erklärungen liefert, sowie die Vorstellung der Selbstorganisation der Materie. Gegen den Determinismus sah Peirce sich dadurch bestätigt, dass das Prinzip des Wachstums und des Lebens unumkehrbare Vorgänge sind, die einem Determinismus widersprechen. Spontaneität (die Popper mit Emergenz verband) war für ihn ein objektiver Tatbestand der Natur und eine wesentliche Grundlage seines Fallibilismus.

Seine Auffassung sah Peirce auch gestützt durch die evolutionäre Denkweise, wie sie für ihn Hegel in Bezug auf Geschichte, Charles Lyell in Bezug auf Geologie und Charles Darwin in der Biologie vertraten. Evolution war für Peirce eines der grundlegenden Prinzipien der Welt.

Peirce ging aber noch einen Schritt weiter. Seine Frage lautete nicht, wie Erkenntnis möglich ist, sondern wie sind überhaupt physikalische Gesetze möglich? Er bezog sich dabei unter anderem auf den 2. Hauptsatz der Thermodynamik und das Phänomen der Entropie, wie auch auf die Inexaktheit der Molekularbewegungen (MS 875). Die Tendenz zur Heterogenität und die Unumkehrbarkeit der Prozesse waren für ihn Zeichen, dass der Evolutionsprozess auch in der physikalischen Welt gilt und eine innewohnende Tendenz hat, stabile Zustände („habits“ = Verhaltensgewohnheiten) anzunehmen.

Peirce hätte sich durch die Ergebnisse der Quantenphysik mit dem Übergang zu wahrscheinlichkeitstheoretischen Erklärungsmodellen und die Heisenbergsche Unschärferelation bestätigt gefunden.

Ausgehend von der Idee des Zufalls und der Evolution entwickelte Peirce seine Weltsicht weiter zu einem umfassenden Konzept. Basis ist das Thema des Kontinuums, das ihn über die gesamte Zeit seines Arbeitens beschäftigte. Den ersten Schritt machte Peirce erneut in der mathematischen Logik, wo er sich mit der Frage der infinitesimalen Teilbarkeit befasste. Ein Infinitesimal ist eine Größe, die kleiner ist als jede endliche Größe, aber größer ist als Null. Das klassische Beispiel eines Kontinuums ist eine Linie. Das Kontinuum ist nicht metrisch, so dass Punkte auf der Linie nur potentielle Punkte sind, die wieder ein beliebig teilbares infinitesimales Intervall sind. Ein Kontinuum kann durch keine Menge von Einzelbestimmungen ausgeschöpft werden (CP 6.170). In diesem Zusammenhang hat Peirce mathematische Vorstellungen entwickelt, die heute in der Nicht-Standard-Analysis diskutiert werden und ist davon ausgegangen, dass der Raum nicht-euklidisch ist.

Phänomene wie Energie, zu der auch die Gravitation gehört, oder die Zeit sind Kontinua, die dem Prozess der Evolution innewohnen. Der Mensch kann sie selbst nicht beobachten, sondern nur ihre Auswirkungen. So ist die Zeit zunächst nur ein reines vages Gefühl der Möglichkeit (Erstheit). Die Veränderung oder Wechselwirkung ist die Erfahrung des Gegensatzes (Zweitheit). Das Fortbestehen der Vorstellungen in der Zeit ist geistige Kontinuität (Drittheit).

Für Peirce war der Urgrund aller Wirklichkeit der Geist, der nichts ist als Empfindung und Qualität, reine Möglichkeit ohne Zusammenhang und Regelmäßigkeit. Dieser Geist schaffte durch ein erstes Ereignis (einen ersten dyadischen Schritt) Zeit, Raum, die Existenz der Materie und die Naturgesetze, die als relativ konstante Regelmäßigkeiten die kontinuierliche Entwicklung der Evolution in Gang setzten. In der Evolution ist das Fortschreiten eines Wachstums zu einer sich immer weiter entwickelnden Heterogenität enthalten, an deren sehr fernem Ende die vollständige Gesetzmäßigkeit steht. "Zufall ist das Erste, Gesetzmäßigkeit das Zweite und die Neigung, Gewohnheiten auszubilden, das Dritte." (CP 6.27). Den Anfang und das Ende der Evolution bilden (theoretische) Grenzsituationen. Für Peirce war damit die Wirklichkeit eine Wirklichkeit des Geistes, die auch die Wirklichkeit ihrer Objekte bestimmt. Folgerichtig vertrat er einen uneingeschränkten Universalienrealismus. Mit dieser Position eines objektiven (logischen) Idealismus sah er sich in einer Linie mit Schelling:

Der Mensch ist Teil des evolutionären Prozesses, der sich in seinem Strom des Bewusstseins ebenso wie in dem Prozess des Denkens in Zeichen widerspiegelt. Das Denken in Zeichen funktioniert aber nur im Miteinander der Menschen; denn ohne den Anderen und die Kommunikation mit ihm ist menschliche Existenz nicht möglich. Aus diesem Horizont leitete Peirce die Grundthese ab, dass nur das Prinzip der Liebe (Agape), die Überwindung der Selbstsucht und des Egoismus zu Harmonie und Fortschritt führt. Wie das teleologische Streben nach Heterogenität in der Natur kommt der Fortschritt des Menschen nur aus dem Gedanken, dass der Einzelne seine Individualität im Mitgefühl zu seinen Mitmenschen aufgehen lässt.

Peirce hat nur wenig zur praktischen Ethik geäußert. Es gibt immerhin eine kleine Schrift, in der er eine grundsätzlich veränderte Sicht auf das Strafrecht forderte. Der Mensch hat zwar das Recht, sich vor der Kriminalität zu schützen. Aber aus dem naturgegebenen Zweck der Solidarität hat er kein Recht auf Rache. Daraus folgte für Peirce die Forderung, Verbrecher zu resozialisieren und für sie Bedingungen zu schaffen, die ihnen eine Rückkehr in die Gemeinschaft ermöglichen.

Peirce wurde zu seiner Zeit als professioneller Philosoph kaum wahrgenommen, weil er keine grundlegenden Schriften zu seinem Gegenstand veröffentlichte. Bekannt war er hingegen als Naturwissenschaftler, Mathematiker und Logiker. So stand er in direktem Kontakt zu Augustus de Morgan. Ernst Schröder basierte seine Logik der Algebra auf Peirce. Über Schröder wirkte Peirce auch auf Peano und die „Principia Mathematica“ von Russell und Whitehead. Da er aber seit 1884 im Prinzip vom akademischen Leben ausgeschlossen war, entstanden Bezüge zu seinem Werk vor allem durch die Personen, die mit ihm persönlich bekannt waren. Dies waren vor allen Dingen William James und der Hegelianer Josiah Royce. Nach William James sind zwei Schriften aus den 1870er Jahren zu nennen, die die Quelle des Pragmatismus ausmachen. James widmete auch seine Schrift „The Will to Believe“ Peirce. Anders als James und spätere Pragmatisten, insbesondere John Dewey, verstand Peirce allerdings seinen Pragmatismus vor allem als Methode zur Klärung der Bedeutung von Gedanken durch Anwendung wissenschaftlicher Methodik auf die Philosophie. Den Pragmatismus von James, der sich als Physiologe vorrangig mit psychologischen Themen befasste und seinen Pragmatismus mit lebensphilosophischen Fragestellungen verband (Theorie der Emotion, Philosophie der Religion), hielt Peirce für einen individualistischen Subjektivismus, den er selbst ablehnte. Und von Deweys Logik sagte Peirce, dass sie eher eine „Naturgeschichte des Denkens“ als Logik im traditionellen Sinne einer Lehre von den normativen Prinzipien und Regeln des Denkens und Schließens sei (The Nation 1904, 220). An beiden kritisierte er das nominalistische Denken. Zur Abgrenzung gegen vereinfachende Formen des Pragmatismus (auch gegen James und Dewey) nannte Peirce seine Form des semiotischen Pragmatismus ab zirka 1905 "Pragmatizismus".

Peirce' Leistungen wurden nur allmählich wahrgenommen. An der Universität wirkte er lediglich fünf Jahre im Bereich Logik. Sein einziges Buch ist eine kurze Schrift über Astronomie ("Photometrische Untersuchungen" von 1878), das wenig Beachtung fand. Seine Zeitgenossen William James und Josiah Royce würdigten ihn zwar, aber nur zu einem gewissen Grad. Als er starb, waren Cassius Keyser von der Columbia-Universität und Morris Raphael Cohen aus New York vielleicht seine einzigen Anhänger. Zwei Jahre nach Peirce Tod erschien im Jahr 1916 ein Sonderheft des Journal of Philosophy, das Peirce gewidmet war, mit Beiträgen von Royce, Dewey, Christine Ladd-Franklin, Joseph Jastrow und Morris R. Cohen. Als erste Arbeit über Peirce gilt „A Survey of Symbolic Logic“ von Clarence Irving Lewis, der einen konzeptualistischen Pragmatismus vertrat. Ausdrücklich auf die Anthologie „Chance, Love and Logic“ bezog sich Frank Plumpton Ramsey in seiner Arbeit „Truth and Probability“ aus dem Jahr 1926. Ramsey seinerseits führte kritische Diskussionen mit Wittgenstein über den Tractatus. Eine Ähnlichkeit des späten Wittgenstein zu Peirce ergibt sich, wenn man sein Verständnis von Bedeutung eines Begriffs als dessen Gebrauch mit der pragmatischen Maxime von Peirce vergleicht. Ähnlich wie für Peirce das Zeichen unhintergehbar und nur in den Kategorien phänomenologisch fassbar war, war auch Sprache für Wittgenstein nur in der Anwendung beschreibbar. Eine andere frühe Rezeptionslinie ergibt sich aus dem Werk „The Meaning of Meaning“ von Charles Kay Ogden und Ivor Armstrong Richards aus dem Jahr 1923, das im Anhang einige Passagen von Peirce enthält. Die semiotische Interpretation von Peirce fand dann ihren Fortgang bei Charles William Morris (Foundations of a Theory of Signs, Chicago 1938), der allerdings einen behavioristischen Ansatz verfolgte.

Auch die Veröffentlichung seiner "Collected Papers" (1931–1935) führte nicht zu einem unmittelbaren Aufschwung in der Sekundärliteratur. Die Herausgeber, Charles Hartshorne und Paul Weiss waren keine Peirce-Spezialisten. Eine nachweisliche Rezeption begann erst mit den Arbeiten von James Feibleman (1946) und Thomas Goudge (1950), der zweiten Auflage der Collected Papers – herausgegeben von Philip Wiener und Frederick Young – sowie der umfangreichen Arbeit von Max Fisch, des Begründers des Peirce-Edition-Projekts an der Indiana University in Indianapolis. Die „Charles Sanders Peirce Society“ wurde 1946 gegründet. Seit 1965 gibt es die Zeitschrift "Transactions of the Peirce Society", die auf Peirceiana spezialisiert ist.

Die erste systematische Auseinandersetzung mit Peirce in Deutschland lieferte Jürgen von Kempski 1952, jedoch noch ohne große Wirkung. Seit den 1960er Jahren haben Max Bense und Elisabeth Walther ihre Semiotik der Stuttgarter Schule ausgehend von einer intensiven Peirce-Rezeption entwickelt. In etwa zeitgleich begründete der Linguist Roman Jacobson seine Zeichentheorie ausgehend von Peirce, wie auch Umberto Ecos strukturalistische Semiotik an Peirce anknüpft.

Aber erst mit der Veröffentlichung eines Textbandes durch Karl-Otto Apel im Jahr 1967, gefolgt von einem zweiten Band 1970 (siehe Schriften), setzte auch in Deutschland eine breitere Rezeptionswelle ein. Peirce lieferte für Apels Intention einer Transformation der Transzendentalphilosophie einen grundlegenden Ansatz: „Ich möchte indessen die kritische Pointe des neuen kommunikationstheoretischen Ansatzes noch zusätzlich mit Hilfe der Wittgensteinschen Konzeption des 'Sprachspiels' erläutern. Mit Hilfe dieser Konzeption lässt sich m.E. zeigen, dass die von Peirce eingeleitete – und in unserem Jahrhundert allenthalben bestätigte – semiotische oder sprachanalytische Transformation der Erkenntniskritik und Wissenschaftstheorie auf eine radikale Überwindung des 'methodischen Solipsismus' hinausläuft, der die philosophische Erkenntnistheorie von Descartes bis Husserl beherrscht hat.“ Nur ein Jahr nach Apel setzte sich auch Jürgen Habermas intensiv mit Peirce auseinander. Habermas sah im Gegensatz zu Apel Peirce nicht in der Tradition der Transzendentalphilosophie, sondern als Wissenschaftstheoretiker: „Peirce begreift Wissenschaft aus dem Horizont methodischer Forschung, und Forschung versteht er als einen Lebensprozeß. Die logische Analyse der Forschung richtet sich deshalb nicht auf die Leistungen eines transzendentalen Bewusstseins, sondern auf die Leistungen eines Subjektes, das den Forschungsprozeß im ganzen trägt, auf das Kollektiv der Forscher.“ Als Kontrapunkt zur vorherrschenden nominalistischen und empiristischen Philosophie ist die Rezeption von Peirce seit den 1990er Jahren auf ein breites Spektrum von Anwendungsbereichen gewachsen. Dieses reicht von der Informatik über die Linguistik, die Semiotik, die Sozialwissenschaften, die Literaturtheorie, die Philosophie der Mathematik, die Naturphilosophie bis hin zur Religionsphilosophie. So bewertet Ilya Prigogine sein Werk: „Peirce wagte es, das Universum der klassischen Mechanik zugunsten eines evolutionären Universums zu einer Zeit zu verwerfen, als keinerlei experimentelle Ergebnisse vorlagen, die diese These hätten stützen können.“

Wenn man den Umfang von Peirce' Themen betrachtet, muss man ihn als Universalgelehrten bezeichnen, mit dem man nur wenige aus der Geschichte vergleichen kann. Besondere Ähnlichkeit findet man zu Gottfried Wilhelm Leibniz, der sich wie er mit Mathematik, Logik, Naturwissenschaften, Geschichte, Philosophie des Geistes und der Sprache und Metaphysik befasste. Beide waren metaphysische Realisten und der scholastischen Philosophie zumindest teilweise zugeneigt. So bewunderte Peirce Duns Scotus. Die Gedanken beider wurden in der Nachfolge zunächst nur wenig geschätzt und von ersten Interpreten stark vereinfacht dargestellt. Leibniz unterschied sich von Peirce vor allem in seiner finanziellen Lage, seinem Glauben und einer Korrespondenz von ca. 15.000 Briefen. Beide publizierten wenige Bücher, aber viele Aufsätze und hinterließen einen umfangreichen Nachlass. Die Werke beider Autoren sind noch bei weitem nicht vollständig ediert.

"siehe auch das (unvollständige) Verzeichnis der Einzelschriften: Schriften von Charles Sanders Peirce"


Die im Text enthaltenen Zitate stammen aus den deutschen Ausgaben oder der genannten Literatur.








</doc>
<doc id="968" url="https://de.wikipedia.org/wiki?curid=968" title="Compiler">
Compiler

Ein Compiler (auch "Kompiler"; von für "zusammentragen" bzw. ‚aufhäufen‘) ist ein Computerprogramm, das Quellcodes einer bestimmten Programmiersprache in eine Form übersetzt, die von einem Computer (direkter) ausgeführt werden kann.

Teils wird zwischen den Begriffen "Übersetzer" und Compiler unterschieden. Ein Übersetzer übersetzt ein Programm aus einer formalen Quellsprache in ein semantisches Äquivalent in einer formalen Zielsprache. Compiler sind spezielle Übersetzer, die Programmcode aus problemorientierten Programmiersprachen, sogenannten Hochsprachen, in ausführbaren Maschinencode einer bestimmten Architektur oder einen Zwischencode (Bytecode, p-Code oder .NET-Code) überführen. Diese Trennung zwischen den Begriffen Übersetzer und Compiler wird nicht in allen Fällen vorgenommen.

Der "Vorgang" der Übersetzung wird auch als "Kompilierung" oder Umwandlung (bzw. mit dem entsprechenden Verb) bezeichnet. Das Gegenteil, also die Rückübersetzung von Maschinensprache in Quelltext einer bestimmten Programmiersprache, wird "Dekompilierung" und entsprechende Programme "Decompiler" genannt.

Ein Übersetzer ist ein Programm, das als Eingabe ein in einer Quellsprache formuliertes Programm akzeptiert und es in ein semantisch äquivalentes Programm in einer Zielsprache übersetzt. Es wird also insbesondere gefordert, dass das erzeugte Programm die gleichen Ergebnisse wie das gegebene Programm liefert. Als Ausnahme wird oft die Quell-Sprache Assembler angesehen - ihr Übersetzer (in Maschinencode) heißt „Assembler“ und wird i. A. nicht als „Compiler“ bezeichnet. Die Aufgabe des Übersetzers umfasst ein großes Spektrum an Teilaufgaben, von der Syntaxanalyse bis zur Zielcodeerzeugung. Eine wichtige Aufgabe besteht auch darin, Fehler im Quellprogramm zu erkennen und zu melden.

Das Wort „Compiler“ stammt vom Englischen „to compile“ (dt. zusammentragen, zusammenstellen) ab und heißt im eigentlichen Wortsinn also „Zusammentrager“. In den 1950er-Jahren war der Begriff noch nicht fest in der Computerwelt verankert. Ursprünglich bezeichnete Compiler ein Hilfsprogramm, das ein Gesamtprogramm aus einzelnen Unterprogrammen oder Formelauswertungen zusammentrug, um spezielle Aufgaben auszuführen. (Diese Aufgabe erfüllt heute der Linker, der jedoch auch im Compiler integriert sein kann.) Die einzelnen Unterprogramme wurden noch „von Hand“ in Maschinensprache geschrieben. Ab 1954 kam der Begriff „algebraic compiler“ für ein Programm auf, das die Umsetzung von Formeln in Maschinencode selbständig übernahm. Das „algebraic“ fiel im Laufe der Zeit weg.

Ende der 1950er-Jahre wurde der Begriff des Compilers im englischsprachigen Raum noch kontrovers diskutiert. So hielt das Fortran-Entwicklerteam noch jahrelang am Begriff „translator“ (deutsch „Übersetzer“) fest, um den Compiler zu bezeichnen. Diese Bezeichnung ist sogar im Namen der Programmiersprache Fortran selbst enthalten: Fortran ist zusammengesetzt aus Formula und Translation, heißt also in etwa Formel-Übersetzung. Erst 1964 setzte sich der Begriff Compiler auch im Zusammenhang mit Fortran gegenüber dem Begriff Translator durch. Nach Carsten Busch liegt eine „besondere Ironie der Geschichte darin“, dass der Begriff Compiler im Deutschen mit „Übersetzer“ übersetzt wird. Einige deutsche Publikationen verwenden jedoch auch den englischen Fachbegriff Compiler an Stelle von Übersetzer.

In einem engeren Sinne verwenden einige deutschsprachige Publikationen den Fachbegriff Compiler jedoch nur, wenn die Quellsprache eine höhere Programmiersprache ist als die Zielsprache. Typische Anwendungsfälle sind die Übersetzung einer höheren Programmiersprache in die Maschinensprache eines Computers, sowie die Übersetzung in Bytecode einer virtuellen Maschine. Zielsprache von Compilern (in diesem Sinne) kann auch eine Assemblersprache sein. Ein Übersetzer zur Übertragung von Assembler-Quellprogrammen in Maschinensprache wird als Assembler oder Assemblierer bezeichnet.

Bereits für die erste entworfene höhere Programmiersprache, den Plankalkül von Konrad Zuse, plante dieser - nach heutiger Terminologie - einen Compiler. Zuse bezeichnete ein einzelnes Programm als "Rechenplan" und hatte schon 1944 die Idee für ein sogenanntes "Planfertigungsgerät", welches automatisch aus einem mathematisch formulierten Rechenplan einen gestanzten Lochstreifen mit entsprechendem Maschinenplan für den Zuse-Z4-Computer erzeugen sollte.

Konkreter als die Idee von Zuse eines Planfertigungsgeräts war ein Konzept von Heinz Rutishauser zur automatischen Rechenplanfertigung. In einem Vortrag vor der "Gesellschaft für Angewandte Mathematik und Mechanik (GAMM)" wie auch 1951 in seiner Habilitationsschrift an der ETH Zürich beschrieb er, welche zusätzlichen Programmierbefehle (Instruktionen) und Hardware-Ergänzungen an der damals an der ETHZ genutzten Z4 nötig seien, um den Rechner ebenfalls als Hilfsmittel zur automatischen Programmerstellung einzusetzen.

Ein früher Compiler wurde 1949 von der Mathematikerin Grace Hopper konzipiert. Bis zu diesem Zeitpunkt mussten Programmierer direkt Maschinencode erstellen. (Der erste Assembler wurde zwischen 1948 und 1950 von Nathaniel Rochester für eine IBM 701 geschrieben.) Um diesen Prozess zu vereinfachen, entwickelte Grace Hopper eine Methode, die es ermöglichte, Programme und ihre Unterprogramme in einer mehr an der menschlichen als der maschinellen Sprache orientierten Weise auszudrücken. Am 3. Mai 1952 stellte Hopper den ersten Compiler A-0 vor, der Algorithmen aus einem Katalog abrief, Code umschrieb, in passender Reihenfolge zusammenstellte, Speicherplatz reservierte und die Zuteilung von Speicheradressen organisierte. Anfang 1955 präsentierte Hopper bereits einen Prototyp des Compilers B-0, der nach englischen, französischen oder deutschen Anweisungen Programme erzeugte. Hopper nannte ihren Vortrag zum ersten Compiler „The Education of a Computer“ („Die Erziehung eines Computers“).

Die Geschichte des Compilerbaus wurde von den jeweils aktuellen Programmiersprachen (vgl. Zeittafel der Programmiersprachen) und Hardwarearchitekturen geprägt. Weitere frühe Meilensteine sind 1957 der erste Fortran-Compiler und 1960 der erste COBOL-Compiler. Viele Architekturmerkmale heutiger Compiler wurden aber erst in den 1960er Jahren entwickelt.

Früher wurden teilweise auch Programme als Compiler bezeichnet, die Unterprogramme zusammenfügen. Dies geht an der heutigen Kernaufgabe eines Compilers vorbei, weil Unterprogramme heutzutage mit anderen Mitteln eingefügt werden können: Entweder im "Quelltext" selbst, beispielsweise von einem Präprozessor (siehe auch Precompiler) oder bei "übersetzten Komponenten" von einem eigenständigen Linker.

Die prinzipiellen Schritte bei der Übersetzung eines Quellcodes in einen Zielcode lauten:




Der Compilerbau, also die Programmierung eines Compilers, ist eine eigenständige Disziplin innerhalb der Informatik.

Moderne Compiler werden in verschiedene Phasen gegliedert, die jeweils verschiedene Teilaufgaben des Compilers übernehmen. Einige dieser Phasen können als eigenständige Programme realisiert werden (s. Precompiler, Präprozessor). Sie werden sequentiell ausgeführt. Im Wesentlichen lassen sich zwei Phasen unterscheiden: das Frontend (auch "Analysephase"), das den Quelltext analysiert und daraus einen attributierten Syntaxbaum erzeugt, sowie das Backend (auch "Synthesephase"), das daraus das Zielprogramm erzeugt.

Im Frontend wird der Code analysiert, strukturiert und auf Fehler geprüft. Es ist selbst wiederum in Phasen gegliedert.
Sprachen wie modernes C++ erlauben auf Grund von Mehrdeutigkeiten in ihrer Grammatik keine Aufteilung der Syntaxanalyse in lexikalische Analyse, syntaktische Analyse und semantische Analyse. Ihre Compiler sind entsprechend komplex.

Die lexikalische Analyse zerteilt den eingelesenen Quelltext in lexikalische Einheiten ("Tokens") verschiedener Typen, zum Beispiel Schlüsselwörter, Bezeichner, Zahlen, Zeichenketten oder Operatoren. Dieser Teil des Compilers heißt Scanner oder Lexer.

Ein Scanner benutzt gelegentlich einen separaten Screener, um Whitespace (Leerraum, also Leerzeichen, Tabulatorzeichen, Zeilenenden usw.) und Kommentare zu überspringen.

Eine weitere Funktion der lexikalischen Analyse ist es, erkannte Tokens mit ihrer Position (z. B. Zeilennummer) im Quelltext zu assoziieren. Werden in der weiteren Analysephase, deren Grundlage die Tokens sind, Fehler im Quelltext gefunden (z. B. syntaktischer oder semantische Art), können die erzeugten Fehlermeldungen mit einem Hinweis auf den Ort des Fehlers versehen werden.

Lexikalische Fehler sind Zeichen oder Zeichenfolgen, die keinem Token zugeordnet werden können. Zum Beispiel erlauben die meisten Programmiersprachen keine Bezeichner, die mit Ziffern beginnen (z. B. „3foo“).

Die syntaktische Analyse überprüft, ob der eingelesene Quellcode in einer korrekten Struktur der zu übersetzenden Quellsprache vorliegt, das heißt der kontextfreien Syntax (Grammatik) der Quellsprache entspricht. Dabei wird die Eingabe in einen Syntaxbaum umgewandelt. Der syntaktische Analysierer wird auch als Parser bezeichnet. Falls der Quellcode nicht zur Grammatik der Quellsprache passt, gibt der Parser einen Syntaxfehler aus.
Der so erzeugte Syntaxbaum ist für die nächste Phase (semantische Analyse) mit den "Inhalten" der Knoten annotiert; d. h. z. B., Variablenbezeichner und Zahlen werden, neben der Information, dass es sich um solche handelt, weitergegeben.

Die semantische Analyse überprüft die statische Semantik, also über die syntaktische Analyse hinausgehende Bedingungen an das Programm. Zum Beispiel muss eine Variable in der Regel deklariert worden sein, bevor sie verwendet wird, und Zuweisungen müssen mit kompatiblen (verträglichen) Datentypen erfolgen. Dies kann mit Hilfe von Attributgrammatiken realisiert werden. Dabei werden die Knoten des vom Parser generierten Syntaxbaums mit Attributen versehen, die Informationen enthalten. So kann zum Beispiel eine Liste aller deklarierten Variablen erstellt werden. Die Ausgabe der semantischen Analyse nennt man dann dekorierten oder attributierten Syntaxbaum.

Das Backend erzeugt aus dem vom Frontend erstellten attributierten Syntaxbaum den Programmcode der Zielsprache.

Viele moderne Compiler erzeugen aus dem Syntaxbaum einen Zwischencode, der schon relativ maschinennah sein kann und führen auf diesem Zwischencode zum Beispiel Programmoptimierungen durch. Das bietet sich besonders bei Compilern an, die mehrere Quellsprachen oder verschiedene Zielplattformen unterstützen. Hier kann der Zwischencode auch ein Austauschformat sein.

Der Zwischencode ist Basis vieler Programmoptimierungen. Siehe Programmoptimierung.

Bei der Codegenerierung wird der Programmcode der Zielsprache entweder direkt aus dem Syntaxbaum oder aus dem Zwischencode erzeugt. Falls die Zielsprache eine Maschinensprache ist, kann das Ergebnis direkt ein ausführbares Programm sein oder eine sogenannte Objektdatei, die durch das Linken mit der Laufzeitbibliothek und evtl. weiteren Objektdateien zu einer Bibliothek oder einem ausführbaren Programm führt. Dies alles wird vom Codegenerator ausgeführt, der Teil des Compilersystems ist, manchmal als Programmteil des Compilers, manchmal als eigenständiges Modul.




Viele Optimierungen, die früher Aufgabe des Compilers waren, werden mittlerweile innerhalb der CPU während der Codeabarbeitung vorgenommen.
Maschinencode ist gut, wenn er kurze kritische Pfade und wenig Überraschungen durch falsch vorhergesagte Sprünge aufweist, Daten rechtzeitig aus dem Speicher anfordert und alle Ausführungseinheiten der CPU gleichmäßig auslastet.
Zur Steuerung des Übersetzens kann der Quelltext neben den Anweisungen der Programmiersprache zusätzliche spezielle Compiler-Anweisungen enthalten.

Üblicherweise bietet ein Compiler Optionen für verschiedene Optimierungen mit dem Ziel, die Laufzeit des Zielprogramms zu verbessern oder dessen Speicherplatzbedarf zu minimieren. Die Optimierungen erfolgen teilweise in Abhängigkeit von den Eigenschaften der Hardware, zum Beispiel wie viele und welche Register der Prozessor des Computers zur Verfügung stellt. Es ist möglich, dass ein Programm nach einer Optimierung langsamer ausgeführt wird, als das ohne die Optimierung der Fall gewesen wäre. Dies kann zum Beispiel eintreten, wenn eine Optimierung für ein Programmkonstrukt längeren Code erzeugt, der zwar an sich schneller ausgeführt werden würde, aber mehr Zeit benötigt, um erst einmal in den Cache geladen zu werden. Er ist damit erst bei häufigerer Benutzung vorteilhaft.

Einige Optimierungen führen dazu, dass der Compiler Zielsprachenkonstrukte erzeugt, für die es gar keine direkten Entsprechungen in der Quellsprache gibt. Ein Nachteil solcher Optimierungen ist, dass es dann kaum noch möglich ist, den Programmablauf mit einem interaktiven Debugger in der Quellsprache zu verfolgen.

Optimierungen können sehr aufwendig sein. Vielfach muss vor allem in modernen JIT-Compilern daher abgewogen werden, ob es sich lohnt, einen Programmteil zu optimieren. Bei Ahead-of-time-Compilern werden bei der abschließenden Übersetzung alle sinnvollen Optimierungen verwendet, häufig jedoch nicht während der Software-Entwicklung (reduziert den Kompilier-Zeitbedarf). Für nichtautomatische Optimierungen seitens des Programmierers können Tests und Anwendungsszenarien durchgespielt werden (s. Profiler), um herauszufinden, wo sich komplexe Optimierungen lohnen.

Im Folgenden werden einige Optimierungsmöglichkeiten eines Compilers betrachtet. Das größte Optimierungspotenzial besteht allerdings oft in der Veränderung des Quellprogramms selbst, zum Beispiel darin, einen Algorithmus durch einen effizienteren zu ersetzen. Dieser Vorgang kann meistens nicht automatisiert werden, sondern muss durch den Programmierer erfolgen. Einfachere Optimierungen können dagegen an den Compiler delegiert werden, um den Quelltext lesbar zu halten.

In vielen höheren Programmiersprachen benötigt man beispielsweise eine Hilfsvariable, um den Inhalt zweier Variablen zu vertauschen:

Mit der Optimierung werden statt 6 nur noch 4 Assemblerbefehle benötigt, außerdem wird der Speicherplatz für die Hilfsvariable hilf nicht gebraucht. D. h., diese Vertauschung wird schneller ausgeführt und benötigt weniger Hauptspeicher. Dies gilt jedoch nur, wenn ausreichend Register im Prozessor zur Verfügung stehen. Die Speicherung von Daten in Registern statt im Hauptspeicher ist eine häufig angewendete Möglichkeit der Optimierung.

Die oben als optimiert gezeigte Befehlsfolge hat noch eine weitere Eigenschaft, die bei modernen CPUs mit mehreren Verarbeitungs-Pipelines einen Vorteil bedeuten kann: Die beiden Lesebefehle und die beiden Schreibbefehle können problemlos parallel verarbeitet werden, sie sind nicht vom Resultat des jeweils anderen abhängig. Lediglich der erste Schreibbefehl muss auf jeden Fall abwarten, bis der letzte Lesebefehl ausgeführt wurde. Tiefer gehende Optimierungsverfahren fügen deshalb unter Umständen zwischen "b → Register 2" und "Register 2 → a" noch Maschinenbefehle ein, die zu einer ganz anderen hochsprachlichen Befehlszeile gehören.

Die Berechnung des Kreisumfangs mittels

pi = 3.14159
u = 2 * pi * r

kann ein Compiler bereits zum Übersetzungszeitpunkt zu u = 6.28318 * r auswerten. Diese Formelauswertung spart die Multiplikation 2 * pi zur Laufzeit des erzeugten Programms. Diese Vorgehensweise wird als Konstantenfaltung (englisch „constant folding“) bezeichnet.

Wenn der Compiler erkennen kann, dass ein Teil des Programmes niemals durchlaufen wird, dann kann er diesen Teil bei der Übersetzung weglassen.

"Beispiel:"

100 goto 900
200 k=3
900 i=7

Wenn in diesem Programm niemals ein codice_1 auf die Sprungmarke codice_2 erfolgt, kann auf die Anweisung codice_3 verzichtet werden. Der Sprungbefehl codice_4 ist dann ebenfalls überflüssig.

Wird eine Variable nicht benötigt, so muss dafür kein Speicherplatz reserviert und kein Zielcode erzeugt werden.

"Beispiel:"

subroutine test (a,b)

Hier wird die Variable c nicht benötigt: Sie steht nicht in der Parameterliste, wird in späteren Berechnungen nicht verwendet und wird auch nicht ausgegeben. Deshalb kann die Anweisung c = 3.14 * b entfallen.

Insbesondere Schleifen versucht man zu optimieren, indem man zum Beispiel

Manche dieser Optimierungen sind bei aktuellen Prozessoren ohne Nutzen oder sogar kontraproduktiv.

Bei kleinen Unterprogrammen fällt der Aufwand zum Aufruf des Unterprogrammes verglichen mit der vom Unterprogramm geleisteten Arbeit stärker ins Gewicht. Daher versuchen Compiler, den Maschinencode kleinerer Unterprogramme direkt einzufügen – ähnlich wie manche Compiler/Assembler/Präcompiler Makro-Anweisungen in Quellcode auflösen. Diese Technik wird auch als Inlining bezeichnet. In manchen Programmiersprachen ist es möglich, durch "inline"-Schlüsselwörter den Compiler darauf hinzuweisen, dass das Einfügen von bestimmten Unterprogrammen gewünscht ist. Das Einfügen von Unterprogrammen eröffnet oft, abhängig von den Parametern, weitere Möglichkeiten für Optimierungen.

Anstatt mehrfach auf dieselbe Variable im Speicher, beispielsweise in einer Datenstruktur, zuzugreifen, kann der Wert nur einmal gelesen und für weitere Verarbeitungen in Registern oder im Stack zwischengespeichert werden. In C, C++ und Java muss dieses Verhalten ggf. mit dem Schlüsselwort "volatile" abgeschaltet werden: Eine als "volatile" bezeichnete Variable wird bei jeder Benutzung wiederholt vom originalen Speicherplatz gelesen, da ihr Wert sich unterdessen geändert haben könnte. Das kann beispielsweise der Fall sein, wenn es sich um einen Hardware-Port handelt oder ein parallel laufender Thread den Wert geändert haben könnte.

"Beispiel:"

int a = array[25]->element.x;
int b = 3 * array[25]->element.x;
Im Maschinenprogramm wird nur einmal auf codice_5 zugegriffen, der Wert wird zwischengespeichert und zweimal verwendet. Ist x volatile, dann wird zweimal zugegriffen.

Es gibt außer "volatile" noch einen anderen Grund, der eine Zwischenspeicherung in Registern unmöglich macht: Wenn der Wert der Variablen v durch Verwendung des Zeigers z im Speicher verändert werden könnte, kann eine Zwischenspeicherung von v in einem Register zu fehlerhaftem Programmverhalten führen. Da die in der Programmiersprache C oft verwendeten Zeiger nicht auf ein Array beschränkt sind (sie könnten irgendwohin im Hauptspeicher zeigen), hat der Optimizer oft nicht genügend Informationen, um eine Veränderung einer Variablen durch einen Zeiger auszuschließen.

Statt einer Multiplikation oder Division von Ganzzahlen mit einer Zweierpotenz kann ein Schiebebefehl verwendet werden. Es gibt Fälle, in denen nicht nur Zweierpotenzen, sondern auch andere Zahlen (einfache Summen von Zweierpotenzen) für diese Optimierung herangezogen werden. So kann zum Beispiel (n « 1) + (n « 2) schneller sein als n * 6. Statt einer Division durch eine Konstante kann eine Multiplikation mit dem Reziprokwert der Konstante erfolgen. Selbstverständlich sollte man solch spezielle Optimierungen auf jeden Fall dem Compiler überlassen.

Programmiersprachen wie Java fordern Laufzeitüberprüfungen beim Zugriff auf Felder oder Variablen. Wenn der Compiler ermittelt, dass ein bestimmter Zugriff immer im erlaubten Bereich sein wird (zum Beispiel ein Zeiger, von dem bekannt ist, dass er an dieser Stelle nicht NULL ist), kann der Code für diese Laufzeitüberprüfungen weggelassen werden.

Eng zusammenhängende Codebereiche, zum Beispiel ein Schleifenrumpf, sollte zur Laufzeit möglichst auf der gleichen oder in möglichst wenigen Speicherseiten („Page“, zusammenhängend vom Betriebssystem verwalteter Speicherblock) im Hauptspeicher liegen. Diese Optimierung ist Aufgabe des (optimierenden) Linkers. Dies kann zum Beispiel dadurch erreicht werden, dass dem Zielcode an geeigneter Stelle Leeranweisungen („NOPs“ – "N"o "OP"eration) hinzugefügt werden. Dadurch wird der erzeugte Code zwar größer, aber wegen der reduzierten Anzahl notwendiger TLB-Cache-Einträge und notwendiger Pagewalks wird das Programm schneller ausgeführt.

Durch das Vorziehen von Speicherlesezugriffen und das Verzögern von Schreibzugriffen lässt sich die Fähigkeit moderner Prozessoren zur Parallelarbeit verschiedener Funktionseinheiten ausnutzen. So kann beispielsweise bei den Befehlen: a = b * c; d = e * f; der Operand e bereits geladen werden, während ein anderer Teil des Prozessors noch mit der ersten Multiplikation beschäftigt ist.

Folgendes in ANTLR erstelltes Beispiel soll die Zusammenarbeit zwischen Parser und Lexer erklären. Der Übersetzer soll Ausdrücke der Grundrechenarten beherrschen und vergleichen können. Die Parsergrammatik wandelt einen Dateiinhalt in einen abstrakten Syntaxbaum (AST) um.

Die Baumgrammatik ist in der Lage, die im AST gespeicherten Lexeme zu evaluieren. Der Operator der Rechenfunktionen steht in der AST-Schreibweise vor den Operanden als Präfixnotation. Daher kann die Grammatik ohne Sprünge Berechnungen anhand des Operators durchführen und dennoch Klammerausdrücke und Operationen verschiedener Priorität korrekt berechnen.

tree grammar Eval;
options {
@header {
import java.lang.Math;
start : line+; //Eine Datei besteht aus mehreren Zeilen

compare returns [double value]

Ist eines der oben als "compare" bezeichnete Ausdrücke noch kein Lexem, so wird es von der folgenden Lexer-Grammatik in einzelne Lexeme aufgeteilt. Dabei bedient sich der Lexer der Technik des rekursiven Abstiegs. Ausdrücke werden so immer weiter zerlegt, bis es sich nur noch um Token vom Typ "number" oder Operatoren handeln kann.

grammar Expression;
options {
tokens {
start : (line {System.out.println($line.tree==null?"null":$line.tree.toStringTree());})+;
line : compare NEWLINE -> ^(compare); //Eine Zeile besteht aus einem Ausdruck und einem
compare : sum ('='^ sum)?; //Summen sind mit Summen vergleichbar
sum : product ('+'^ product|'-'^ product)*; //Summen bestehen aus Produkten (Operatorrangfolge)
product : pow ('*'^ pow|'/'^ pow|'%'^ pow)*; //Produkte (Modulo-Operation gehört hier dazu) können
pow : term ('^'^ pow)?; //Potenzen werden auf Terme angewendet
term : number //Terme bestehen aus Nummern, Subtermen oder Summen
number : INT; //Nummern bestehen nur aus Zahlen
INT : '0'..'9'+;
NEWLINE : '\r'? '\n';
WS : (' '|'\t'|'\n'|'\r')+ {skip();}; //Whitespace wird ignoriert

Die Ausgabe hinter dem Token "start" zeigt außerdem den gerade evaluierten Ausdruck.

Eingabe:
Ausgabe (in den ersten Zeilen wird nur der Ausdruck der Eingabe in der AST-Darstellung ausgegeben):
Der erste Ausdruck wird also als wahr (1) evaluiert, bei den anderen Ausdrücken wird das Ergebnis der Rechnung ausgegeben.



</doc>
<doc id="971" url="https://de.wikipedia.org/wiki?curid=971" title="Columban von Luxeuil">
Columban von Luxeuil

Columban von Luxeuil, auch Kolumban geschrieben (* 540 in Nobber bei Navan (West Leinster), Irland; † 23. November 615 in Bobbio (Provinz Piacenza), Italien) war ein irischer Wandermönch und Missionar. Er wird von Katholiken und orthodoxen Christen als Heiliger verehrt. Auch in der Evangelischen Kirche in Deutschland gilt er als denkwürdiger Glaubenszeuge.

Im Unterschied zum heiligen Kolumban, der Schottland missionierte, wird er als Columban von Luxeuil, Columban von Bobbio (ital. "Colombano") oder Columban der Jüngere bezeichnet.

Sein liturgischer Gedenktag ist der 23. November.

Nachdem er sich von der Welt losgesagt hatte, begann er eine klösterliche Erziehung bei Abt "Sinell" in Cluaninis bei Lough Erne. Später zog es ihn in die Abtei Bangor, die damals vom heiligen Comgall geleitet wurde.

Um das Jahr 591 brach Columban mit einer Reihe von Brüdern, traditionell ist in Anlehnung an die Jüngerzahl Jesu von zwölfen die Rede, vom Kloster Bangor zur Küste auf. Unter seinen Gefährten werden der heilige Gallus, Domoal, Comininus, Eunocus und Equonanus genannt. Die Gefährten schifften sich ein und erreichten zunächst die Küste Britanniens (der Text lässt offen, ob es sich um die Küste der Bretagne oder die des heutigen Großbritanniens handelt). Hier, so berichtet Jona, ruhten sie sich aus und berieten ihre Pläne, bevor sie nach Gallien gingen.

Childebert II. (Jonas nennt hier Sigebert) lud Columban kurz nach dessen Eintreffen in Gallien nach Austrasien ein. Hier gründete Columban mit seinen Gefährten zunächst das Kloster Annegray. Vor allem fränkische Adlige und Beamte sandten ihre Söhne als Oblaten in das Kloster, um sie dort ausbilden zu lassen. Schon bald gründeten die irischen Mönche die Klöster Luxeuil und Fontaines. In diesem Zusammenhang entsteht die Regula Monachorum des hl. Columban.

Der Erfolg der irischen Mönche unter Columban musste den Neid der Bischöfe wecken, denn er entzog sich ihrer Jurisdiktion, da er unter dem Schutz Childeberts II. und später von dessen Nachfolger Theuderich II. stand. Da er weiterhin dem irischen Festkalender folgte, feierte er das Osterfest zu einem anderen Termin als der Rest der römischen Kirche. Dies suchten die fränkischen Bischöfe zu einer Klage auszunutzen. Der kam Columban jedoch zuvor, als er sich um das Jahr 600 in einem Brief an Papst Gregor wandte. Im zweiten noch erhaltenen Brief, der sich vermutlich an die Synode der fränkischen Bischöfe von Chalon des Jahres 603 wandte, bittet er darum, in Frieden in seiner neuen Heimat bleiben zu dürfen. Noch stand Columban unter dem Schutz Theuderichs, doch als er von Theuderich gebeten wurde, dessen vier illegitime Kinder zu segnen, weigerte sich Columban und drohte später mit der Exkommunikation. Nun sandte ihn Theuderich unter Bewachung nach Besançon. Nach dem Bericht des Jonas kam es hier zu einigen Vorfällen, die die Bewacher veranlassten, Columban gehen zu lassen. So kehrte er nach Luxeuil zurück. Doch im Jahre 610 wurde er erneut unter Bewachung gestellt und mit einigen seiner irischen Gefährten nach Nantes gebracht. Theuderich wollte wohl, da nun Theudebert II. ins Elsass eingefallen war, dieses unsichere Element loswerden. Nach zeitgenössischen Berichten war Columban schon unterwegs nach Irland, als ein Sturm ihn dazu zwang, auf den Kontinent zurückzukehren.

Der Rhein war durch den Kleinen Laufen bei Laufenburg für Boote nicht passierbar, ebenso bei Ettikon, man konnte hier nur zu Fuß oder Pferd weiter. Der einfachste Weg an den Zürichsee führte damals über die Aare, die man vom Hochrhein aus auf der Höhe ab Waldshut oder Koblenz mit Booten, meist Weidlingen befuhr, dann kam man auf der Limmat in den Zürichsee.

Columban und seine Gefährten kamen zunächst nach Tuggen an den oberen Zürichsee, wo sie mit der Missionierung begannen. Ein Teil der Einwohner nahm den neuen Glauben an, andere blieben aber skeptisch. Zum Beweis, dass ihre alten Götter nichtig seien, nahm Gallus eine Statue und warf sie in den See. Das von den Heiden erwartete Strafgericht ihrer Götter trat nicht ein, und einige mehr ließen sich vom neuen Glauben überzeugen und taufen. Dennoch mussten die Glaubensboten weiterziehen, weil ihnen die verbliebenen Heiden nach dem Leben trachteten. 

Es verschlug Columban an den Bodensee, wo er in Bregenz Christen vorfand, die heidnische Bräuche wieder aufgenommen hatten. Mit der Hilfe von Gallus brachte er die kirchliche Zucht in Ordnung, und die Verehrung der heiligen Aurelia von Straßburg, einer Gefährtin der heiligen Ursula, lebte wieder auf. Weil er und seine Gefährten in ihrem missionarischen Eifer unter den Einheimischen Streit auslösten, forderte der Herzog von Überlingen den Missionar auf, um des Friedens willen die Gegend zu verlassen. Gallus aber blieb in der Gegend, vorgeblich weil er aufgrund einer Krankheit nicht weiterziehen konnte. Weil ihm Columban nicht glaubte, verbot er ihm, die Messe zu lesen, bis zum Tag seines eigenen Todes. Gallus aber wurde zum Gründer der Stadt St. Gallen und zu ihrem Schutzpatron.

612 zog Columban nach Mailand und mischte sich in den Streit um den Nestorianismus ein. Ein ihm zugesprochener Brief an Papst Bonifatius IV. ist ein großes Zeugnis der Papstverbundenheit des irischen Missionars. Der langobardische König Agilulf vermachte ihm ein Gebiet namens Bobbio (Provinz Piacenza) am Fluss Trebbia, wo er ein Kloster gründete und die Zeit bis zu seinem Lebensende verbrachte – trotz einer Einladung der Franken, nach Luxeuil zurückzukehren. Er starb am 23. November 615 in Bobbio in Norditalien. Der Legende nach soll Gallus an diesem Tag im Gedenken an seinen Meister erstmals wieder die heilige Messe gelesen haben – die gesicherte Nachricht über dessen Tod erreichte ihn erst Wochen später.

Columban hat mit seiner Peregrinatio eine umfassende Missionsbewegung auf dem europäischen Festland angestoßen und gilt als einer der bekanntesten iroschottischen Wandermissionare. Außerdem war er auch auf dem Gebiet der Liturgie tätig. Es werden ihm einige Hymnen, Briefe, Predigten sowie ein theologisches Traktat über die Buße zugeschrieben. 

Einen prägenden Einfluss hatte er auf die Christianisierung des bis dahin noch heidnisch, das heißt gallo-römisch geprägten ländlichen Raums auf der Alpennordseite. Bemerkenswert dabei ist, dass sich sein Erfolg wesentlich auf die von seiner irischen Heimat geprägte iroschottische Form des Christentums stützte. Sie war im Gegensatz zum römischen Kirchenmodell deutlich weniger hierarchisch und legte großen Wert auf die persönliche Beziehung.

Columban selbst hatte nur drei Klöster gegründet, Luxeuil (das zusammen mit Annegray und Fontaines unter einer Verwaltung stand), das nach einem Jahr untergegangene Bregenz und Bobbio. Es entstand aber infolge seiner Tätigkeit die erfolgreiche iroschottische Mission auf dem europäischen Festland. Sie wurde insbesondere durch seine Schüler Eustasius († 629) und Gallus († 645) und deren Schüler Kilian von Würzburg († 689) fortgeführt. Dabei kam es vor allem zu einer großen Klostergründungsbewegung auf dem Land. Unter römischer Vorherrschaft war das Christentum nur in den Städten verbreitet gewesen und hatte es in gut fünf Jahrhunderten nicht geschafft, die gallo-römische Landbevölkerung zu erreichen. Dies änderte sich mit Columbans Klostergründungswelle, in deren Folge sich eine - vom fränkischen Adel getragene - Bewegung entwickelte, die im 7. Jahrhundert circa 300 neue Klöster gründete. Mit diesen geistlichen Zentren hielt erstmals das Christentum im ländlichen Raum Einzug. Durch ihr landwirtschaftliches Wissen und ihre wirtschaftlichen Aktivitäten waren die Klöster darüber hinaus prägend für das Entstehen der europäischen Kulturlandschaft.

Columban gab dem Mönchtum durch eine von ihm verfasste Ordensregel wesentliche Impulse. 
Sein Nachfolger Abt Eustachius von Luxeuil wurde wegen der Liturgie und Ordensregel Columbans angeklagt. Der König ließ daraufhin 627 die Synode von Mâcon einberufen, auf der allerdings die Regel Columbans bestätigt wurde. Dennoch ist davon auszugehen, dass nach dem Tode des Abts schon bald Teile der Benediktinerregel im Kloster Luxeuil praktiziert wurden. Durch die Klostergründungen der von Columban initiierten Bewegung breitete sich diese Mischregel stark aus. Um das Jahr 670 beschloss das Konzil von Autun, dass die Klöster künftig nach der Regel Benedikts geführt werden sollten. Dennoch wurden in der Folge vielfach Mischregeln, vor allem mit Teilen Columbans und Benedikts, verwendet. Die Regel Columbans konnte sich dauerhaft zwar nicht gegen die des Benedikt von Nursia durchsetzen, dennoch beachteten einige Klöster, besonders die im Reich der Franken, noch einige Jahrhunderte zumindest Teile seiner Regel. Erst durch die umfangreichen Reformen von Benedikt von Aniane wurde die Benediktinerregel mit Unterstützung Ludwigs des Frommen über das Frankenreich im gesamten Abendland die verbindliche Mönchsregel. Insofern hatte Columban einen prägenden Einfluss auf die Christianisierung und das klösterliche Leben in Europa.

Der Gedenktag des Heiligen ist der 21. November (orthodox) und 23. November (evangelisch und römisch-katholisch), in Irland und bei den Benediktinern wird er hingegen am 24. November verehrt; in den Bistümern Chur, St. Gallen und Feldkirch am 27. November. Er gilt als Patron der Motorradfahrer und Helfer bei Überschwemmungen. Dargestellt wird er als bärtiger Mönch umgeben von einem Wolfsrudel, eine Versinnbildlichung der widrigen Umstände, unter denen der Heilige oft wirkte.
Die Pfarrkirche Bregenz-St. Kolumban ist ihm gewidmet, zusätzlich ist die "Kolumbanstraße" in Bregenz nach ihm benannt.





</doc>
<doc id="973" url="https://de.wikipedia.org/wiki?curid=973" title="C. S. Lewis">
C. S. Lewis

C. S. Lewis ("Clive Staples Lewis," privat auch "Jack" genannt; * 29. November 1898 in Belfast; † 22. November 1963 in Oxford) war ein irischer Schriftsteller und Literaturwissenschaftler. Er lehrte am Magdalen College der University of Oxford und hatte den Lehrstuhl für Englische Literatur des Mittelalters und der Renaissance an der University of Cambridge inne. Vor allem im angloamerikanischen Raum ist er bekannt für seine Kinderbuchserie "Die Chroniken von Narnia."

Clive Staples („Jack“) Lewis wurde 1898 im nordirischen Belfast geboren und wuchs dort zusammen mit seinem drei Jahre älteren Bruder Warren („Warnie“) auf. Sein Vater Albert James Lewis hatte als erster seiner Familie einen akademischen Beruf ergriffen, er war Anwalt. Seine Vorfahren stammten aus Wales und waren in der Landwirtschaft tätig. Die Mutter Florence Augusta Hamilton Lewis („Flora“) war Pfarrerstochter und hatte an der Queens-Universität Mathematik und Logik studiert. Die Eltern waren sehr belesen und besaßen eine große Hausbibliothek, die den jungen Jack faszinierte und ihm einen Zugang zur Literatur ermöglichte.

Ein tiefer und traumatischer Einschnitt in Jacks Leben war die Krebserkrankung seiner Mutter. Nach anfänglicher Besserung verschlechterte sich ihr Zustand. Sie starb im August 1908, als Jack neun Jahre alt war. Im gleichen Jahr starben auch ein Großvater und ein Onkel. Sein Vater schickte ihn darauf in britische Internate:
Wynyard School in Watford (September 1908 – Juni 1910), Campbell College in Belfast (September – Dezember 1910), Cherbourg School in Malvern (Januar 1911 – Juni 1913) und Malvern College (September 1913 – Juni 1914). 1914–1917 genoss er Privatunterricht bei William Thompson Kirkpatrick, von dessen Klarheit im Denken Lewis sehr profitierte.

Wegen des Ersten Weltkriegs musste Lewis sein Studium am University College in Oxford, das er im April 1917 angefangen hatte, abbrechen und Soldat der britischen Armee werden. In Oxford absolvierte er seine Offiziersausbildung und wurde zum Offizier in das dritte Bataillon der Somerset Light Infantry befördert. An seinem 19. Geburtstag (29. November) kam er an die Front nach Frankreich. Am 15. April 1918 wurde Lewis in Mont-Bernanchon bei Lillers von einer fehlgeleiteten englischen Granate verwundet und zur Genesung zurück nach England geschickt.

1919 nahm er seine Studien am University College in Oxford wieder auf, legte 1920 die erste öffentliche Universitätsprüfung mit Ehren in Griechisch und Latein, 1922 die Abschlussprüfung in Philosophie und antiker Geschichte und 1923 die erste öffentliche Universitätsprüfung in Englisch ab. Bis Mai 1925 war Lewis Philosophielehrer am University College und erhielt ein "Fellowship" für englische Sprache am Magdalen College in Oxford, wo er auch J. R. R. Tolkien kennenlernte.

In der Jugend war Lewis ein überzeugter Atheist gewesen, hatte sich dann aber während seines Studiums der pantheistischen Philosophie des Englischen Hegelianismus zugewandt. 1929 akzeptierte er jedoch einen personalen Gott und den Gedanken der Schöpfung: er wurde also Theist. Nach einer langen nächtlichen Diskussion mit Tolkien und Hugo Dyson im September 1931 bekehrte er sich schließlich zum Christentum. Zum Leidwesen des überzeugten Katholiken Tolkien blieb er allerdings in der anglikanischen Kirche, zu der auch seine Vorfahren gehört hatten, er vertrat aber (bis auf seine Ablehnung des päpstlichen Primates) überwiegend auch von Katholiken akzeptierbare Ansichten.

Ab dem Herbstsemester 1933 bis 1947 traf sich Lewis mit seinem Freundeskreis, den „Inklings“, zu dem auch J. R. R. Tolkien, sein Bruder Warnie, Owen Barfield, Nevill Coghill, Hugo Dyson, Charles Williams und andere gehörten. In den nächsten Jahren wurde Lewis durch seine Vorlesungen, Veröffentlichungen in christlichen Zeitschriften und durch seine Radioansprachen, die von der BBC ausgestrahlt wurden, bekannt. Im Jahr 1948 wurde er zum Mitglied der "Royal Society of Literature" gewählt und erhielt in den nächsten Jahren drei Ehrendoktorwürden für Theologie und Literatur. Ab 1954 hatte er den Lehrstuhl für Literatur des Mittelalters und der Renaissance in Cambridge inne. 1958 wurde er Ehrenmitglied des University College Oxfords.

Am 23. April 1956 heiratete Lewis die amerikanische Schriftstellerin Helen Joy Davidman standesamtlich in Oxford, um sie und ihre beiden Söhne vor einer Ausweisung aus England zu bewahren. Erst als er von ihrer Krebserkrankung erfuhr, begann er sie bewusst zu lieben. Am 21. März 1957 wurde an ihrem Bett im Wingfield Krankenhaus eine kirchliche Trauung nach anglikanischem Ritus durch den Geistlichen Peter Bide durchgeführt, obwohl Joy Davidman geschieden war. In den nächsten Jahren erfuhr sie eine außergewöhnliche Genesung von ihrem letzten Krebsanfall.

Im Juli 1958 fuhr Lewis mit seiner Frau 10 Tage nach Irland in Urlaub, worauf er zehn Vorträge über "The Four Loves" in London hielt. Joy starb am 13. Juli 1960 im Alter von 45 Jahren, was Lewis in eine tiefe Sinn-, Glaubens- und Lebenskrise stürzte. 1961 verarbeitete er diese Krise auch literarisch in "A Grief Observed" (deutsch: "Über die menschliche Trauer"), die er zuerst unter dem Pseudonym N. W. Clerk veröffentlichte.

C. S. Lewis starb an den Folgen eines Nierenversagens eine Woche vor seinem 65. Geburtstag am 22. November 1963. In der Presse wurde sein Tod kaum beachtet, weil knapp eine Stunde später das Attentat auf John F. Kennedy stattgefunden hatte und zudem Aldous Huxley am selben Tag starb.

Das Grab befindet sich im Garten der Holy Trinity Church in Headington Quarry in Oxford. Lewis’ Bruder Warren starb am 9. April 1973. Ihre Namen stehen auf einem gemeinsamen Stein mit der Inschrift, die Warnie ausgewählt hatte: "Men must endure their going hence" („Dulden muss der Mensch sein Scheiden aus der Welt“).

C. S. Lewis verfasste neben seinen literaturkritischen Werken bekannte christliche apologetische Schriften sowie die ebenfalls christliche Symbolik verwendende Kinderbuchserie über das Land Narnia (Genre Fantasy) und die Perelandra-Trilogie (Genre Science-Fiction und Fantasy).
1955 erhielt Lewis die Carnegie Medaille in Anerkennung des letzten Buches der Narnia-Chroniken "The Last Battle" (dt. "Der letzte Kampf").

Er war Mittelpunkt des christlich geprägten Literaturkreises der "Inklings" und lange Zeit eng mit J. R. R. Tolkien befreundet. Die beiden Autoren beeinflussten sich gegenseitig wesentlich. Später jedoch kühlte diese Freundschaft wegen Tolkiens Kritik an den „Narnia-Chroniken“ ab, in einigen späten Briefen Tolkiens finden sich ausgesprochen negative persönliche Angriffe auf Lewis. Ein weiterer enger Freund von Lewis war der britische Schriftsteller Charles Williams (1886–1945), der theologische Bücher, Romane, Gedichte und literaturwissenschaftliche Texte verfasste.

Er bezeichnete den schottischen Pfarrer und Schriftsteller George MacDonald (1824–1905) vielfach als sein Vorbild und als seinen „Meister“. Im Buch "Die große Scheidung" (engl. "The Great Divorce") ließ er MacDonald sogar als Charakter auftreten.
In seinem Erzählwerk beleuchtete Lewis Fragen christlicher Ethik und Glaubenslehre. Sein Roman "Du selbst bist die Antwort" (englischer Originaltitel: "Till We Have Faces: A Myth Retold") ist eine Version von "Amor und Psyche" aus der Sicht von Psyches Schwester und gilt bei einigen Literaturkritikern als sein bestes Werk.

Im Februar 1943 hielt Lewis an der Universität von Durham die "Riddell Memorial Lectures," eine dreiteilige Vorlesungsreihe, die später als "The Abolition of Man" (dt. "Die Abschaffung des Menschen") herausgegeben wurde und sein wohl bekanntestes fachliterarisches Werk darstellt. Er kritisierte das angeblich oberflächliche Schulmaterial, das die Schüler zu einer Weltanschauung ohne jegliche objektive Werte führen würde. Die gesammelten Vorträge von Lewis, die die BBC ausstrahlte, wurden unter dem Titel "Mere Christianity" (dt. "Christentum schlechthin" bzw. "Pardon, ich bin Christ") veröffentlicht.

Lewis hielt es für wichtig, die Existenz einer Hölle konkret ins Auge zu fassen. In "Über den Schmerz" schrieb er:
Lewis war bereits zu Lebzeiten durch seine Bücher und durch seine populären Radioansprachen eine bekannte Persönlichkeit in Großbritannien. Nach seinem Tod 1963 geriet er vorerst eher etwas in Vergessenheit, weil die junge Generation den Wandel und somit die Distanz zur Kultur der Eltern suchte. Lewis verkörperte für sie die Werte und Haltungen der Vergangenheit, zudem war er bei Gelehrten in Oxford wegen seiner populären Werke nicht sonderlich angesehen. Er hatte vorerst nur wenige Fürsprecher wie die amerikanischen Episkopalisten Chad Walsh und Walter Hooper, der kurz vor Lewis' Tod sein Privatsekretär geworden war.

Anfang der 1970er-Jahre erwarb der britische Verlag "William Collins & Sons" die Rechte an Lewis' Werken und verbreitete sie. Ihm kam dabei zugute, dass die Tolkien-Welle in den USA um 1970 ihn und seine Narnia-Kinderbücher erneut publik machte. Vor allem in den USA entstanden literarische Gesellschaften, die das Vermächtnis von Lewis pflegten. Vorreiter war 1969 die "New York C.S. Lewis Society", weitere folgten später weltweit. 1974 wurde am "Wheaton College" bei Chicago das "Marion E. Wade Center" zur Erforschung des Lebens und Werks von Lewis gegründet. Ab 1974 erschienen auch fundiertere Biografien. Ab dem Jahr 2000 gab Walter Hooper zudem die umfangreiche Korrespondenz (3.500 Seiten) von Lewis heraus.

1993 wurde die Freundschaft und Liebe zwischen C. S. Lewis und Joy Davidman Gresham in Sir Richard Attenboroughs Film "Shadowlands" mit Anthony Hopkins und Debra Winger verfilmt.

2001 wurde der Asteroid (7644) Cslewis nach ihm benannt.








2017: postum in die Science Fiction Hall of Fame aufgenommen




C.S. Lewis-Gesellschaften:


</doc>
<doc id="976" url="https://de.wikipedia.org/wiki?curid=976" title="Chemotherapie">
Chemotherapie

Die Chemotherapie ist eine medikamentöse Therapie von Krebserkrankungen (antineoplastische Chemotherapie) oder Infektionen (antiinfektiöse bzw. antimikrobielle Chemotherapie). Umgangssprachlich (auch als Chemo bezeichnet) ist meistens die zytostatische Behandlung von Krebs gemeint.
Eine Chemotherapie kann unter kurativen, adjuvanten oder palliativen Gesichtspunkten durchgeführt werden.

Die Chemotherapie verwendet Stoffe, die ihre schädigende Wirkung möglichst gezielt auf bestimmte krankheitsverursachende Zellen beziehungsweise Mikroorganismen ausüben und diese abtöten oder in ihrem Wachstum hemmen. In der Krebstherapie heißen diese Substanzen Zytostatika; in der Behandlung von Infektionskrankheiten Antibiotika, Chemotherapeutika, Virustatika, Antimykotika und Anthelminthika. Bei der Behandlung bösartiger Tumorerkrankungen nutzen die meisten dieser Substanzen die schnelle Teilungsfähigkeit der Tumorzellen, da diese empfindlicher als gesunde Zellen auf Störungen der Zellteilung reagieren; auf gesunde Zellen mit ähnlich guter Teilungsfähigkeit üben sie allerdings eine ähnliche Wirkung aus, wodurch sich Nebenwirkungen wie Haarausfall oder Durchfall einstellen können. Bei der Behandlung von bakteriellen Infektionskrankheiten macht man sich den unterschiedlichen Aufbau von eukaryotischen (Mensch) und prokaryotischen Lebewesen (Bakterien) zunutze.

Bei der Krebstherapie mit monoklonalen Antikörpern und Zytokinen, wie beispielsweise Interleukinen und Interferonen, handelt es sich nicht um eine Chemotherapie, sondern um eine Krebsimmuntherapie.

Der im 18. Jahrhundert erstmals aufgetauchte Begriff wurde 1906 von Paul Ehrlich neu definiert und geprägt. Er beschrieb damit die Behandlung von Infektionskrankheiten mit Methoden, die direkt gegen den Krankheitserreger vorgehen. Als erstes wirksames Chemotherapeutikum hatte er 1904 Trypanrot erkannt, mit dem er an der Schlafkrankheit erkrankte Mäuse heilte. Ehrlich begann am 31. August 1909 in Frankfurt am Main weitere Versuche, indem er Erreger der Syphilis in Ratten injizierte und anschließend mit Hilfe chemotherapeutischer Verfahren heilte. Diese Versuche hatten eine so überzeugende Wirkung, dass man hierin die neue „Waffe“ der Medizin gegen Infektionskrankheiten sah. Die verwendeten Medikamente werden entweder künstlich hergestellt oder sind Abkömmlinge von in der Natur vorkommenden Stoffen.

Das Ansprechen einer Chemotherapie hängt von verschiedenen Faktoren ab. Erstens wird ein Chemotherapeutikum unterschiedlich schnell im Menschen abgebaut, und je kürzer das Medikament im Körper wirksam beziehungsweise präsent ist, desto kürzer kann es auch nur wirken. Zweitens ist die Erreichbarkeit der krankheitsverursachenden Zellen oder Mikroorganismen ein wichtiger Faktor. So kann ein Tumor sehr kompakt geformt sein und über wenig Blutversorgung verfügen. Daraus resultiert, dass das Medikament den eigentlichen Wirkort nicht oder nur schlecht erreichen kann. Ein dritter Faktor bestimmt das Ansprechverhalten von Chemotherapeutika. Zum Beispiel können auch bei guter Erreichbarkeit des Tumors durch das Zytostatikum die Krebszellen resistent gegen das Medikament sein. Diese Eigenschaften werden als Chemosensitivität und Chemoresistenz bezeichnet.

Es ist möglich, die Wirksamkeit von Chemotherapeutika auf Bakterien im Rahmen eines Antibiogramms zu testen. Ebenso kann bei Krebszellen die Chemosensitivität "in vitro" getestet werden (Chemosensitivitätstest).

Wegen der höheren Bioverfügbarkeit wird in der Regel eine intravenöse Verabreichung gewählt. Einige Therapien sind aber auch oral möglich.

Eine bestimmte Zytostatikadosis kann immer nur einen bestimmten Anteil, z. B. 90 % der Zielzellen abtöten. Mit fortschreitender Behandlung bleibt dieser Anteil gleich, d. h. zwei Dosen erreichen 99 % der Zellen, drei Dosen 99,9 % usw. Dieser Mechanismus erklärt, warum eine Chemotherapie im Laufe der Behandlung nicht vermindert werden darf, auch wenn der sichtbare Tumor bereits verschwunden ist ("Log cell kill", Howard E. Skipper 1964). Im Gegenteil: Es muss damit gerechnet werden, dass durch eine schwache Behandlung gerade die widerstandsfähigsten Tumorzellklone selektiert werden, d. h. übrig bleiben. Moderne Protokolle versuchen daher, „so früh und so hart wie möglich zuzuschlagen“. Die Chemotherapie wird in schneller Abfolge appliziert, und fast immer werden zwei oder mehr Zytostatika kombiniert, um die Wirksamkeit zu erhöhen. Mangelnde Therapieerfolge bei einigen Tumorarten und neuere theoretische und tierexperimentelle Daten lassen jedoch Zweifel an der generellen Richtigkeit dieses Konzeptes aufkommen.

"Adjuvant" nennt man eine Chemotherapie, die zur Erfolgssicherung nach einer vollständigen operativen Beseitigung des Tumors dienen soll. "Neoadjuvant" ist eine Chemotherapie "vor" der Operation. Sehr häufig wird die adjuvante, neoadjuvante oder alleinige Chemotherapie mit Strahlentherapie kombiniert ("Radiochemotherapie").

Bei der Behandlung von alten Menschen muss berücksichtigt werden, dass diese oft eine verminderte Leber- und Nierenfunktion und eine verminderte Knochenmarksreserve haben und ihre Empfindlichkeit gegenüber den Substanzen daher erhöht ist. Wenn die Dosis nach dem Körpergewicht oder der Körperoberfläche abgeschätzt wird, ist der erhöhte Anteil an Körperfett im Alter einzurechnen.

Resistenzen der Tumorzellen gegen einzelne oder mehrere der eingesetzten Zytostatika sind nicht selten. Außerdem sollte man während einer Chemotherapie nicht rauchen, denn bei einigen Standard-Chemotherapeutika wurde nachgewiesen, dass ihre Wirkung durch Nikotin abgeschwächt wird.
Resistenzen können viele Ursachen haben, beispielsweise verminderten Transport der Substanz in das Zellinnere oder erhöhten Transport aus der Zelle (Multiple Drug Resistance). Auch kann die Zelle inaktivierende Enzyme besitzen. Gute Durchblutung des Tumors (Angiogenese) führt wegen hoher Nährstoffversorgung zu schnellem Wachstum, aber auch zu besserem Ansprechen auf die Chemotherapie, da der Anteil der sich teilenden Zellen höher ist. Viele der durch die Zytostatika in den Zellen erzeugten Schäden setzen voraus, dass vorhandene Kontrollsysteme (beispielsweise p53) in den Tumorzellen noch aktiv sind und diese Fehler bemerken. Reparaturmechanismen (beispielsweise Exzisionsreparatur) dürfen hingegen nicht aktiviert sein, stattdessen muss ein kontrolliertes Absterben der Zelle eingeleitet werden. Resistenzen müssen frühzeitig erkannt werden, um Änderungen des Therapieregimes rechtzeitig wirksam werden zu lassen, sonst häufen sich Mutationen im Tumor an, die ihn schwerer kontrollierbar machen. Auch das Auffinden der für den speziellen Tumor optimalen Kombinationstherapie durch Labortests wird diskutiert
und wurde erfolgreich eingesetzt.

Prinzipiell können bei der Chemotherapie zwei unterschiedliche Wege zur Bekämpfung der Krebszellen eingeschlagen werden. Mit Zytotoxinen soll die Apoptose, das heißt der programmierte Zelltod der malignen Zellen, herbeigeführt werden. Dies ist der in den meisten Fällen angestrebte Weg, den Tumor zu eradizieren, das heißt vollständig aus dem Körper des Erkrankten zu beseitigen.
Zytostatika (griechisch "cyto"=Zelle und "statik"=anhalten) sind dagegen definitionsgemäß Substanzen, die Krebszellen nicht abtöten, sondern deren Zellwachstum und die Zellteilung (Proliferation) unterbinden. Konventionelle klassische Chemotherapeutika wirken im Wesentlichen zytotoxisch, während zielgerichtete neuere Therapien aus dem Bereich der Krebsimmuntherapie, wie beispielsweise monoklonale Antikörper, zytostatische Eigenschaften haben.

In der Literatur wird allerdings in vielen Fällen nicht zwischen Zytostatika und Zytotoxinen unterschieden. Die meisten derzeit angewandten Chemotherapeutika wirken zudem sowohl zytotoxisch als auch zytostatisch.


Eine antineoplastische Chemotherapie sollte nicht begonnen werden, wenn

Beispiele für Krebserkrankungen, bei denen eine Chemotherapie zu einer dauerhaften Heilung führen kann:

Die Wahl des Chemotherapeutikums richtet sich nicht nur nach dem Organ der Krebserkrankung (z. B. Brust-, Lungen-, Darmkrebs), sondern auch nach individuellen Kriterien, die bei verschiedenen Patienten mit "derselben" Krebserkrankung unterschiedlich sein können.
Solche Kriterien können beispielsweise sein:

Trotz dieser individuellen Gesichtspunkte können für maligne Erkrankungen typische Chemotherapeutika genannt werden, die bei diesen regelhaft zum Einsatz kommen.

Heutzutage werden bei der Chemotherapie fast immer (abgesehen von möglicherweise nebenwirkungsärmeren Monotherapien bei der palliativen zytostatischen Chemotherapie) mehrere Wirkstoffe kombiniert. Dazu wurden Schemata entwickelt, in denen festgelegt ist, welche Wirkstoffe in welcher Abfolge und mit welchem Zeitabstand anzuwenden sind, um eine optimale Wirkung zu erzielen. Aus den Namen der beteiligten Wirkstoffe wird der Name des Schemas (meist als Akronym) abgeleitet:


Die Nebenwirkungen einer Chemotherapie sind abhängig von der Art der Therapie und der individuellen Verträglichkeit. Die einzelnen Nebenwirkungen treten unabhängig voneinander auf und können ganz ausbleiben oder in verschiedener Stärke (von mild bis tödlich) auftreten.

Diese Nebenwirkungen sind Übelkeit und Erbrechen, Erschöpfung, Haarausfall, Schleimhautentzündungen und Blutbildveränderungen. Sie werden nach den Common Toxicity Criteria eingeteilt.

Viele Zytostatika sind selbst karzinogen, etwa Busulfan, Chlorambucil, Cyclophosphamid oder Semustin. Insgesamt ist die Rate an Zytostatika induzierten Leukämien zwar rückläufig, aber bei einigen Tumorarten steigt die Zahl der Erkrankungen immer noch an. Darunter fällt das Multiple Myelom, das Non-Hodgkin-Lymphom, Ösophaguskarzinom, Analkarzinom, Zervixkarzinom und Prostatakarzinom. Während bei den ersten beiden auch nach einem Jahrzehnt nach der Chemotherapie eine therapiebedingte akute myeloische Leukämie (tAML) auftreten kann, ist sie bei den übrigen Karzinomen auf die ersten zehn Jahre nach der Behandlung beschränkt.

Bis zu drei Viertel der Tumorpatienten mit einer Chemotherapie erkranken an einer chemotherapieassoziierten Anämie. Ein hohes Risiko besteht vor allem bei Tumorentitäten wie dem Lymphom, multiplem Myelom, Bronchialkarzinom sowie bei gynäkologischen und urogenitalen Tumoren. Häufigkeit und Schweregrad der Anämie sind auch vom Tumorstadium abhängig.
Während die nach den Common Toxicity Criteria aufgelisteten Nebenwirkungen meist mit dem Absetzen der Chemotherapie verschwinden, kann es unter Umständen zu einer irreversiblen Herzmuskelschädigung sowie zu einer temporären oder endgültigen Unfruchtbarkeit kommen. Die Gabe von Anthracyclinen führt bei etwa zehn Prozent der Patienten zu einer bleibenden Schädigung der Herzmuskelzellen, welche Herzrhythmusstörungen und/oder eine Herzinsuffizienz (Herzschwäche) auslösen kann. Seit 2007 sind sogenannte Kardioprotektiva zugelassen, welche Herzschäden durch die Gabe der Anthracycline Doxorubicin oder Epirubicin verhindern können.

Wegen einer etwaigen durch die Chemotherapie bedingten Unfruchtbarkeit wird vor der Behandlung bei Männern, falls vom Patienten gewünscht, eine Aufbewahrung des Samens (ähnlich wie es bei Samenspendern praktiziert wird) vorgenommen. Durch die fachgerechte Lagerung wird dann die Chance auf eigene Kinder erhalten. Fertilitätserhaltende Maßnahmen bei Frauen sind möglich, tragen jedoch zum Teil noch experimentellen Charakter. Das Netzwerk Fertiprotekt bemüht sich im deutschsprachigen Raum, über Maßnahmen bei Männern und Frauen zu informieren und sie anzubieten.

Manche Patienten erleben nach einer Chemotherapie eine meist vorübergehende Beeinträchtigung des Denk-, Merk- und Stressbewältigungsvermögens, die als Post-chemotherapy Cognitive Impairment (PCCI) (auch Chemotherapy-induced Cognitive Dysfunction oder „Chemo Brain“) bezeichnet wird. Die Ursache dieses Phänomens wird derzeit erforscht. Sie kann nach gegenwärtigem Forschungsstand entweder in der psychisch belastenden, traumaähnlichen Situation der Diagnose und Krankheit selbst, in den direkten physischen Auswirkungen der Chemotherapie oder in beiden Faktoren liegen.

Zur Vorbeugung einer ausgeprägten Mukositis können mehrere Lokalanästhesien mit Vasokonstriktor im Mund-/Kieferbereich verabreicht werden, wodurch eine Anflutung des Chemotherapeutikums in die Schleimhaut vermindert wird. Zusätzlich kann eine Kältetherapie mittels Lutschen von Eiswürfeln die lokale Vasokonstriktion bei der Strahlentherapie verstärken. Die dadurch erreichte Sauerstoffunterversorgung des Gewebes vermindert die zelluläre Strahlenempfindlichkeit.

Die Wirksamkeit einer Chemotherapie hängt sehr stark von der Art des Tumors und seinem Stadium ab. Während es sehr viele Studien zu der Wirkung spezifischer Zytostatika auf entsprechende Tumorarten gibt, existiert bisher lediglich eine einzige Metastudie, welche den Nutzen der Chemotherapie bei allen Krebsfällen untersucht. Laut ihr wird der Gesamtbeitrag adjuvant angewandter zytotoxischer Chemotherapien zur Fünf-Jahres-Überlebensrate bei Erwachsenen auf 2,3 Prozent in Australien und 2,1 Prozent in den USA geschätzt. Die Studie bestätigt jedoch auch, dass bei bestimmten Krebsarten wie z. B. Hodenkrebs, Hodgkin-Lymphomen oder Zervixkarzinomen eine adjuvant angewandte Chemotherapie eine um 10 bis 40 Prozent bessere Prognose bringt.

Die Studie wurde von australischen Onkologen stark kritisiert. Die Autoren hätten die verschiedenen Krebsarten nicht gewichtet (die Fallgruppe der Krebsarten, bei welchen die Chemotherapie schlecht wirkt und somit oft auch nicht angewendet wird, ist am größten) und es gebe methodische Mängel. Bei Anwendung sauberer Methodik würde aus dem gleichen Datenmaterial die Effektivität auf 6 Prozent über alle Fälle steigen. Außerdem wurden einige Krebsarten, welche hauptsächlich durch Chemotherapie behandelt werden (z. B. Leukämie) und wo diese Therapie sehr effektiv ist, nicht betrachtet. Überdies stammten die Daten aus den 1990er Jahren und seien folglich veraltet. Da die Wirkung einer Chemotherapie von der Art des Tumors abhängt, ist ein solcher Zusammenwurf aller Tumorarten nicht zielführend, denn er sage nichts über den Einzelfall aus.

Tatsache ist, dass hochwirksame Zytostatika dazu beigetragen haben, die relative Fünf-Jahres-Überlebensrate bei bestimmten Krebsarten in den letzten 20 Jahren signifikant – mit verbesserten Prognosen im zweistelligen Prozentbereich – zu erhöhen. Dies gilt einerseits bei der adjuvanten Anwendung beispielsweise bei Brustkrebs, Hodenkrebs und Lungenkrebs sowie andererseits bei der primären Anwendung der Chemotherapie als Mittel der ersten Wahl, wie beispielsweise bei Hodgkin-Lymphomen und Leukämie.




</doc>
<doc id="977" url="https://de.wikipedia.org/wiki?curid=977" title="Cantor-Diagonalisierung">
Cantor-Diagonalisierung

Als Cantor-Diagonalisierung werden zwei von Georg Cantor entwickelte Diagonalisierungsbeweisverfahren bezeichnet:

Im Jahr 1874 fand bzw. veröffentlichte Georg Cantor einen Beweis zur Abzählbarkeit der rationalen Zahlen und der algebraischen Zahlen durch Anwendung des „Ersten Cantorschen Diagonalverfahrens“. Gleichzeitig veröffentlichte er einen Beweis zur Überabzählbarkeit der reellen Zahlen inkl. Folgerung der Existenz nicht-algebraischer reeller Zahlen. In den Jahren 1890 und1891 fand bzw. veröffentlichte er den Beweis, dass die Potenzmenge einer beliebigen Menge mächtiger ist als diese und dass insbesondere die Potenzmenge der natürlichen Zahlen überabzählbar ist. Dieser Beweis wird als „Zweites Cantorsches Diagonalverfahren“ bezeichnet und war Auslöser der Begründung der transfiniten Mengenlehre durch Georg Cantor in den Jahren 1895 bis 1897. Die Überabzählbarkeitsbeweise beweisen auch die Überabzählbarkeit des Kontinuums.


</doc>
<doc id="978" url="https://de.wikipedia.org/wiki?curid=978" title="Comic">
Comic

Comic [] ist der gängige Begriff für die Darstellung eines Vorgangs oder einer Geschichte in einer Folge von Bildern. In der Regel sind die Bilder gezeichnet und werden mit Text kombiniert. Das Medium Comic vereint Aspekte von Literatur und bildender Kunst, wobei der Comic eine eigenständige Kunstform und ein entsprechendes Forschungsfeld bildet. Gemeinsamkeiten gibt es auch mit dem Film. Als genre-neutraler Begriff wird auch „sequenzielle Kunst“ verwendet, während regionale Ausprägungen des Comics teils mit eigenen Begriffen wie Manga oder Manhwa bezeichnet werden.

Comic-typische Merkmale und Techniken, die aber nicht zwangsläufig verwendet sein müssen, sind Sprechblasen und Denkblasen, Panels und Onomatopoesien. Diese finden auch in anderen Medien Verwendung, insbesondere dann, wenn Text und die Abfolge von Bildern kombiniert sind wie in Bilderbuch und illustrierter Geschichte, in Karikaturen oder Cartoons. Die Abgrenzung zu diesen eng verwandten Künsten ist unscharf.

In den 1990er Jahren etablierte sich eine Definition von Comic als eigenständiger Kommunikationsform unabhängig von Inhalt, Zielgruppe und Umsetzung. 1993 definierte Scott McCloud Comics als „zu räumlichen Sequenzen angeordnete, bildliche oder andere Zeichen, die Informationen vermitteln und/oder eine ästhetische Wirkung beim Betrachter erzeugen“. Er nimmt damit Will Eisners Definition auf, der Comics als sequenzielle Kunst bezeichnet. Im deutschsprachigen Raum wird das von McCloud definierte Medium auch allgemein als „Bildgeschichte“ bezeichnet und der Comic als dessen moderne Form seit dem 19. Jahrhundert. So spricht Dietrich Grünewald von einem übergeordneten „Prinzip Bildgeschichte“, als dessen moderne Form der Comic mit seinen um 1900 entwickelten Gestaltungsmitteln gilt. Andreas Platthaus nennt den Comic die "„avancierteste Form“" der Bildgeschichte. Wie auch bei McCloud wird der Comic bzw. die Bildgeschichte als eigenständiges Medium definiert, das durch Bildfolgen erzählt. Eckart Sackmann definiert den Comic in direktem Bezug auf McCloud als „Erzählung in mindestens zwei stehenden Bildern“. Jedoch ist bei einigen Definitionen offen, ob auch einzelne, narrativ angelegte Bilder, die ein Geschehen darstellen, ohne das Davor und Danach zu zeigen, zum Comic zählen. Auch eine Darstellung, die formal nur aus einem Bild besteht, kann mehrere Sequenzen enthalten – so bei mehreren Sprechblasen oder mehr als einer Handlung in einem Bild, die nicht zeitgleich stattfinden können.

Frühere Definitionen des Comics bezogen sich unter anderem auf formale Aspekte wie Fortsetzung als kurze Bilderstreifen oder Erscheinen in Heftform, eine gerahmte Bildreihung und der Gebrauch von Sprechblasen. Daneben wurden inhaltliche Kriterien herangezogen, so ein gleichbleibendes und nicht alterndes Personeninventar oder die Ausrichtung auf eine junge Zielgruppe, oder die Gestaltung in Stil und Technik. Diese Definitionen wie auch das Verständnis von Comics als ausschließliches Massenmedium oder Massenzeichenware wurden spätestens in den 1990er Jahren zugunsten der heutigen Definition verworfen.

Illustrationen, Karikaturen oder Cartoons können auch Comics oder Teil eines solchen sein. Die Abgrenzung, insbesondere bei Einzelbildern, bleibt unscharf. Beim Bilderbuch und illustrierten Geschichten dagegen haben, anders als beim Comic, die Bilder nur eine unterstützende Rolle in der Vermittlung des Handlungsgeschehens. Der Übergang ist jedoch auch hier fließend.

Der Begriff "Comic" stammt aus dem Englischen, wo es als Adjektiv allgemein „komisch“, „lustig“, „drollig“ bedeutet. Im 18. Jahrhundert bezeichnete „Comic Print“ Witzzeichnungen und trat damit erstmals im Bereich der heutigen Bedeutung auf. Im 19. Jahrhundert wurde das Adjektiv als Namensbestandteil für Zeitschriften gebräuchlich, die Bildwitze, Bildergeschichte und Texte beinhalteten. Mit dem 20. Jahrhundert kam der Begriff „Comic-Strip“ für die in Zeitungen erscheinenden, kurzen Bildgeschichten auf, die in Streifen (engl. „strip“) angeordneten Bildern erzählen. In den folgenden Jahrzehnten dehnte sich die Bedeutung des Wortes auch auf die neu entstandenen Formen des Comics aus und löste sich vollständig von der Bedeutung des Adjektivs „comic“. Nach dem Zweiten Weltkrieg kam der Begriff auch nach Europa und trat in Deutschland zunächst in Konkurrenz zu „Bildgeschichte“, welche qualitativ höherwertige deutsche Comic-Werke von lizenzierten ausländischen Comics abgrenzen sollte. Schließlich setzte sich „Comic“ auch im deutschen Sprachraum durch.

Comicstrips prägten durch ihre Form auch den französischen Begriff „Bande dessineé“ und den chinesischen „Lien-Huan Hua“ (Ketten-Bilder). Das häufig verwendete Mittel der Sprechblase führte im Italienischen zur Bezeichnung „Fumetti“ („Rauchwölkchen“) für Comics. In Japan wird „Manga“ (, „spontanes Bild“) verwendet, das ursprünglich skizzenhafte Holzschnitte bezeichnete.

Die Ursprünge des Comics liegen in der Antike. So finden sich im Grab des Menna von vor 3400 Jahren Malereien, die in einer Bildfolge Ernte und Verarbeitung von Getreide darstellen. Speziell diese Bildfolge liest sich im Zickzack von unten nach oben. In der Szene vom Wägen des Herzens im Papyrus des Hunefer (ca. 1300 v. Chr.) werden die Bildfolgen mit Dialogtext ergänzt. Ägyptische Hieroglyphen selbst stellen jedoch keine Vorform des Comics dar, da diese, trotz ihrer Bildlichkeit für Laute, nicht für Gegenstände stehen. Andere Beispiele früher Formen von Bildergeschichten stellen die Trajanssäule und japanische Tuschemalereien dar.

In Amerika wurden ebenso früh Erzählungen in sequenziellen Bildfolgen wiedergegeben. Ein Beispiel dieser Kunst wurde 1519 von Hernán Cortés entdeckt und erzählt vom Leben eines präkolumbianischen Herrschers des Jahres 1049. Dabei werden die Bilder um erklärende Schriftzeichen ergänzt. In Europa entstand im Hochmittelalter in Frankreich der Teppich von Bayeux, der die Eroberung Englands durch die Normannen im Jahr 1066 schildert. Auch hier werden Text und Bild kombiniert. Viele Darstellungen in Kirchen dieser Zeit, wie Altarbilder oder Fenster, haben einen comicartigen Charakter. Sie vermittelten damals besonders analphabetischen Gesellschaftsschichten Erzählungen. Auch die "Wiener Genesis", ein byzantinisches Manuskript aus dem 6. Jahrhundert, gehört zu derartigen Werken. In vielen Fällen wird dabei schon das Mittel der Sprechblase in Form von Spruchbändern vorweggenommen. In Japan zeichneten seit dem 12. Jahrhundert Mönche Bildfolgen auf Papierrollen, häufig mit shintoistischen Motiven. Bis ins 19. Jahrhundert fanden Hefte mit komischen oder volkstümlichen Erzählungen Verbreitung. Zugleich wurde in Japan der Begriff "Manga" geprägt, der heute für Comics steht. Aus dieser Zeit am bekanntesten ist das Werk des Holzschnittkünstlers Katsushika Hokusai.

Nach der Erfindung des Buchdrucks in Europa fanden Drucke von Märtyrergeschichten in der Bevölkerung weite Verbreitung. Später wurden die Zeichnungen feiner und der Text wurde, wie bei den verbreiteten Drucken, wieder weggelassen. So bei William Hogarth, der unter anderem "A Harlot’s Progress" schuf. Diese Geschichten bestanden aus wenigen Bildern, die in Galerien in einer Reihe aufgehängt waren und später gemeinsam als Kupferstich verkauft wurden. Die Bilder waren detailreich und die Inhalte der Geschichten sozialkritisch. Auch Friedrich Schiller schuf mit "Avanturen des neuen Telemachs" eine Bildgeschichte, die auch wieder Text gebrauchte und diesen wie im Mittelalter in Schriftrollen integrierte.
Besonders in britischen Witz- und Karikaturblättern wie dem "Punch" fanden sich ab Ende des 18. Jahrhunderts viele Formen des Comics, meist kurz und auf Humor ausgerichtet. Aus dieser Zeit stammt auch der Begriff "Comic". Als Vater des modernen Comics bezeichnet McCloud Rodolphe Töpffer. Er verwendete Mitte des 19. Jahrhunderts erstmals Panelrahmen und stilisierte, cartoonhafte Zeichnungen und kombinierte Text und Bild. Die Geschichten hatten einen heiteren, satirischen Charakter und wurden auch von Johann Wolfgang Goethe bemerkt mit den Worten "Wenn er künftig einen weniger frivolen Gegenstand wählte und sich noch ein bisschen mehr zusammennähme, so würde er Dinge machen, die über alle Begriffe wären". Auch die im 19. Jahrhundert populären Bilderbögen enthielten oft Comics, darunter die Bildgeschichten Wilhelm Buschs.

In den USA wurden im späten 19. Jahrhundert kurze Comicstrips in Zeitungen veröffentlicht, die meist eine halbe Seite einnahmen und bereits "Comics" genannt wurden. "Yellow Kid" von Richard Felton Outcault aus dem Jahr 1896 wird teilweise als erster moderner Comic betrachtet, weist jedoch noch kein erzählendes Moment auf. Ein solches brachte Rudolph Dirks mit seiner von Wilhelm Busch inspirierten Serie "The Katzenjammer Kids" 1897 ein. Ein weiterer bedeutender Comic jener Zeit war "Ally Sloper’s Half Holiday" von Charles H. Ross Andreas Platthaus sieht in George Herrimans ab 1913 erscheinenden Comicstrip "Krazy Kat" eine größere Revolution als in den vorhergehenden Werken, denn Herriman erschafft das Comic-eigene Genre Funny Animal und entwickelt neue Stilmittel. Auch in Europa gab es zu Beginn des 20. Jahrhunderts Karikaturenzeitschriften, jedoch kaum sequenzielle Comics. Auch in Japan etablierten sich Karikaturmagazine und das Stilmittel der Sprechblasen wurde aus Amerika übernommen. Kitazawa Rakuten und Okamoto Ippei gelten als die ersten professionellen japanischen Zeichner, die in Japan Comicstrips anstatt der bis dahin bereits verbreiteten Karikaturen schufen.

In Europa entwickelte sich in Frankreich und Belgien eine andere Form von Comics, das Comicheft, in dem längere Geschichten in Fortsetzung abgedruckt wurden. Ein bedeutender Vertreter war Hergé, der 1929 "Tim und Struppi" schuf und den Stil der Ligne claire begründete. Auch in Amerika wurden bald längere Geschichten in Beilagen der Sonntagszeitungen veröffentlicht. Hal Fosters "Tarzan" machte diese Veröffentlichungsart populär. 1937 folgte "Prinz Eisenherz", bei dem erstmals seit langem wieder auf die Integration von Texten und Sprechblasen verzichtet wurde. Ähnlich entwickelten sich unter anderem die Figuren Walt Disneys von Gagstrips zu längeren Abenteuergeschichten. Dies geschah bei Micky Maus in den 1930er Jahren durch Floyd Gottfredson, bei Donald Duck in den 1940er Jahren durch Carl Barks. Nach der Erfindung von Superman durch Jerry Siegel und Joe Shuster 1938 brach in den USA ein Superhelden­boom aus. Dieser konzentrierte sich auf die Zielgruppe von Kindern und Jugendlichen und verhalf dem Comicheft zum Durchbruch.

Durch den Zweiten Weltkrieg kam es besonders in Amerika und Japan zu einer Ideologisierung der Comics. Mit dem Aufschwung der Superheldencomics in den USA kam es vermehrt dazu, dass die Arbeit des Autors und des Zeichners getrennt wurden. Das geschah vor allem, um die Arbeit an den Heften rationell zu gestalten. In Amerika gehörten der Zeichner Jack Kirby und der Autor Stan Lee zu den Künstlern, die das "Golden Age" der Superhelden in den Vierziger Jahren und das "Silver Age" in den 1960er Jahren prägten. In den 1950ern kam es wegen des Comics Codes zur Schließung vieler kleiner Verlage und Dominanz der Superheldencomics in den USA. Auch in Europa wurde die Arbeitsteilung häufiger.

In der DDR galt der Begriff "Comic" als zu westlich. So entstand in der DDR die Idee, in der Tradition von Wilhelm Busch und Heinrich Zille etwas Eigenes zu schaffen, das man dem „Schund“ aus dem Westen entgegensetzen könnte. 1955 erschienen mit "Atze" und "Mosaik" die ersten Comic-Hefte in der DDR. "Mosaik" wurde das Aushängeschild des DDR-Comics.

Während der 1980er Jahre kam es kurzzeitig zu einer Rückkehr der Generalisten, die die Geschichten schrieben und zeichneten. In den 1990er Jahren kehrte man in den USA und Frankreich wieder zu der Aufteilung zurück. Diese Entwicklung führte dazu, dass die Autoren mehr Aufmerksamkeit genießen und die Zeichner, besonders in Amerika, von diesen Autoren ausgewählt werden. Zugleich entwickelte sich in Amerika seit den Sechziger Jahren der Undergroundcomic um Künstler wie Robert Crumb und Gilbert Shelton, der sich dem Medium als politischem Forum widmete. Einer der bedeutendsten Vertreter ist Art Spiegelman, der in den 1980er Jahren "Maus – Die Geschichte eines Überlebenden" schuf.

In Japan entwickelte sich der Comic nach dem Zweiten Weltkrieg neu. Der Künstler Osamu Tezuka, der unter anderem "Astro Boy" schuf, hatte großen Einfluss auf die Weiterentwicklung des Mangas in der Nachkriegszeit. Der Comic fand in Japan weite Verbreitung in allen Gesellschaftsschichten und erreichte ab den 1960er und 1970er Jahren auch viele weibliche Leser. Auch gab es vermehrt weibliche Zeichner, darunter die Gruppe der 24er. Ab den 1980er Jahren, besonders in den Neunzigern, wurden Mangas auch außerhalb Japans populär, darunter bekannte Reihen wie "Sailor Moon" und "Dragonball".

Ab den 1990er Jahren gewannen Graphic Novels an Bedeutung, so autobiografische Werke wie Marjane Satrapis "Persepolis", Joe Saccos Reportagen "Palästina" oder die Reiseberichte Guy Delisles. Seit 1995 mit "Argon Zark" von Charley Parker der erste Webcomic erschien, wird auch das Internet von zahlreichen Comicproduzenten zur Veröffentlichung und Bewerbung ihrer Werke genutzt und dient Comiclesern und Comicschaffenden zum Gedankenaustausch.

Der Comicstrip (vom englischen "comic strip", strip = Streifen) umfasst als Begriff sowohl die "daily strips" („Tagesstrips“) als auch die "Sunday pages" („Sonntags-Strips“ oder Sonntagsseiten). Der Ursprung von Comicstrips liegt in den amerikanischen Sonntagszeitungen, wo sie zunächst eine ganze Seite füllten. Er ging hervor aus den im Laufe des 19. Jahrhunderts in Zeitungen verbreiteten Bilderfolgen und Karikaturen sowie aus dem älteren Bilderbogen. Als erster Comicstrip gilt "Hogan’s Alley", später bekannt als "The Yellow Kid," von Richard Felton Outcault, der 1894 entstand. Ab der Jahrhundertwende fanden Comicstrips auch in Zeitungen anderer englischsprachiger Länder Verbreitung, in Kontinentaleuropa erst in den 1920er Jahren. Eine Verbreitung wie in den USA fanden sie hier nie.

1903 erschien der erste werktägliche "daily strip" auf den Sportseiten der "Chicago American", ab 1912 wurde zum ersten Mal eine fortlaufende Serie abgedruckt. Der Tagesstrip, der von Anfang an nur auf schwarz-weiß beschränkt war, sollte auch von seinem Platz her sparsam sein. Da er nur eine Leiste umfassen sollte, wurde die Länge auf drei oder vier Bilder beschränkt, die in der Regel mit einer Pointe endeten. Bis heute hat sich erhalten, dass der Comicstrip eine feststehende Länge besitzt, die über eine Längsseite gehen sollte. Häufig werden bestimmte Motive variiert und ihnen dadurch neue Perspektiven abgewonnen. Nur in absoluten Ausnahmefällen ergeben sich längerfristige Veränderungen, meist handelt es sich um die Einführung neuer Nebenfiguren. In der Serie "Gasoline Alley" altern die Figuren sogar. Erscheinen die Geschichten täglich, werden sie häufig eingesetzt, um im Laufe einer Woche eine Art Handlungsbogen zu bestimmen, der in der nächsten Woche von einem neuen abgelöst wird.

Deshalb setzte sich vermehrt die Praxis durch, dass die "Sunday pages" unabhängig von dem Handlungsbogen funktionieren mussten, da es einerseits einen Leserstamm ausschließlich für die Sonntagszeitungen gab, der die vorhergehenden Geschichten nicht kannte und außerdem die Sonntagsstrips zum Teil separat vertrieben wurden.

Aufgrund der wirtschaftlichen Zwänge beim Druck der Strips gab es während des Zweiten Weltkriegs immer stärkere Einschränkungen der formalen Möglichkeiten. Zudem verloren die Zeitungsstrips wegen der zunehmenden Konkurrenz durch andere Medien an Beliebtheit und Bedeutung. So wurde der Comicstrip seit den 1940er Jahren formal und inhaltlich nur noch wenig verändert. Bedeutende Ausnahmen sind Walt Kellys "Pogo", "Die Peanuts" von Charles M. Schulz oder Bill Wattersons "Calvin und Hobbes". Der, wie "Pogo", politische Comicstrip "Doonesbury" wurde 1975 mit dem Pulitzer-Preis ausgezeichnet. Nach einer inhaltlichen Erweiterung hin zu gesellschaftskritischen Themen und formalen Experimenten in den 1960er Jahren bewegten sich die nachfolgenden Künstler innerhalb der bestehenden Konventionen.

Seit Anfang des 20. Jahrhunderts werden Zeitungsstrips auch gesammelt in Heft- oder Buchausgaben veröffentlicht. Bis 1909 erschienen bereits 70 solcher Nachdrucke. Auch heute erscheinen viele aktuelle oder historische Comicstrips nachgedruckt in anderen Formaten.

In den 1930er-Jahren etablierte sich der Vertrieb von Comics in den Vereinigten Staaten in Heftform. 1933 veröffentlichte die Eastern Color Printing Company erstmals ein Comicheft in noch heute gebräuchlicher Form, das aus einem Druckbogen auf 16 Seiten gefalzt und gebunden wurde. Die Seitenzahl beträgt entsprechend in der Regel 32, 48 oder 64. Zunächst wurden die Hefte als Werbegeschenk von Firmen für ihre Kunden verbreitet, gefüllt noch mit Sammlungen von Comicstrips. Bald wurden die Hefte als regelmäßige Publikationen von Verlagen auch direkt vertrieben und mit eigenen Produktionen gefüllt. Die Hefte "Detective Comics" (1937) und "Action Comics" (1938) vom Verlag Detective Comics waren die ersten bedeutenden Vertreter, mit dem Start von "Action Comics" war auch der erste Auftritt von "Superman" verbunden. Aufgrund des Formates wurden sie in den USA "Comic Books" genannt und stellen seit Ende der 1940er Jahre die gängige Vertriebsform in vielen Ländern dar.

Nach dem Zweiten Weltkrieg, teilweise schon in den 1930er Jahren, kam das Heftformat nach Europa und fand in Form von Comic-Magazinen wie dem "Micky-Maus"-Magazin Verbreitung. Das Magazin vereint verschiedene Beiträge unterschiedlicher Autoren und Zeichner, die es häufiger als Fortsetzungen übernimmt, und ergänzt diese unter Umständen um redaktionelle Beiträge. Zu unterscheiden sind Magazine wie das an Jugendliche gerichtete "Yps", in dem importierte Reihen wie "Lucky Luke" und "Asterix und Obelix" neben deutschen Beiträgen zu finden sind und deren Aufmachung Heftcharakter besitzt, von den an Erwachsene gerichteten Sammlungen wie "Schwermetall" oder "U-Comix". Zu den bedeutendsten Magazinen des Frankobelgischen Comics zählen "Spirou" (seit 1938), "Tintin" (1946–1988) und "Pilote" (1959–1989).

"Fix und Foxi" von Rolf Kauka, eine der erfolgreichsten Comic-Serien aus deutscher Produktion, erschien ab 1953 als Comic-Magazin. Sie besitzt inzwischen allerdings keine große wirtschaftliche Relevanz mehr. Im Osten Deutschlands wurden die eigenen Comiczeitschriften, zur Unterscheidung von westlichen Comics, als "Bilderzeitschriften" bzw. "Bildergeschichten" bezeichnet. Besonders prägte das "Mosaik" mit seinen lustigen unpolitischen Abenteuergeschichten die dortige Comiclandschaft. Das "Mosaik" von Hannes Hegen mit Digedags wurde 1955 in Ost-Berlin gegründet. Später wurde die Comiczeitschrift mit den Abrafaxen fortgeführt. Das "Mosaik" erscheint noch immer als monatliches Heft mit einer Auflage von etwa 100.000 Exemplaren im Jahr 2009, wie sie keine andere Zeitschrift mit deutschen Comics erreicht. Mittlerweile existieren kaum noch erfolgreiche Magazine in Deutschland und Comics werden vornehmlich in Buch- und Albenformaten veröffentlicht.

In Japan erschien 1947 mit "Manga Shōnen" das erste reine Comic-Magazin, dem bald weitere folgten. Dabei entwickelten sich eigene und insbesondere im Vergleich zum europäischen Magazin deutlich umfangreichere Formate mit bis zu 1000 Seiten. Auf dem Höhepunkt der Verkäufe im Jahr 1996 gab es 265 Magazine und fast 1,6 Mrd. Exemplaren Auflage pro Jahr. Das bedeutendste Magazin, "Shōnen Jump", hatte eine Auflage von 6 Mio. pro Woche. Seit Mitte der 1990er Jahre sind die Verkaufszahlen rückläufig.

Neben den Comic-Heften setzten sich auch das Album und das Taschenbuch durch. Comicalben erschienen in Frankreich und Belgien ab den 1930er Jahren. In ihnen werden die in Magazinen veröffentlichten Comics gesammelt und als abgeschlossene Geschichte abgedruckt. Ihr Umfang beträgt, bedingt durch die Verwendung 16-fach bedruckter Bögen, in der Regel 48 oder 64 Seiten. Im Gegensatz zu Heften sind sie wie Bücher gebunden, von diesen heben sie sich durch ihr Format, meist DIN A4 oder größer, ab. Sie sind insbesondere in Europa verbreitet. Seit es weniger Comic-Magazine gibt, erscheinen Comics in Europa meist ohne Vorabdruck direkt als Album. Bekannte in Albenform erschienene Comics sind "Tim und Struppi" oder "Yakari". In den 1950er- und 1960er-Jahren brachte der Walter Lehning Verlag das aus Italien stammende Piccolo-Format nach Deutschland. Die mit 20 Pfennig günstigen Hefte wurden mit den Comics Hansrudi Wäschers erfolgreich verkauft und prägten den damaligen deutschen Comic.
Comic-Publikationen in Buchformaten entstanden in den 1960er Jahren und kamen mit den Veröffentlichungen des Verlags Eric Losfeld auch nach Deutschland. Die 1967 gestarteten "Lustigen Taschenbücher" erscheinen noch heute. Ab den 1970er Jahren wurden bei den Verlagen Ehapa und Condor auch Superhelden im Taschenbuchformat etabliert, darunter "Superman" und "Spider-Man". Dazu kamen in diesem Format humoristische Serien, wie etwa "Hägar". In Japan etablierte sich, als Gegenstück zum europäischen Album, das Buch für zusammenfassende Veröffentlichung von Serien. Die entstandenen Tankōbon-Formate setzten sich in den 1990er Jahren auch im Westen für die Veröffentlichung von Mangas durch. Mit Hugo Pratt in Europa sowie Will Eisner in den USA entstanden ab den 1970ern erstmals Geschichten als Graphic Novel, die unabhängig von festen Formaten, in ähnlicher Weise wie Romane veröffentlicht wurden. Der Begriff „Graphic Novel“ selbst wurde aber zunächst nur von Eisner verwendet und setzte sich erst deutlich später durch. Die zunehmende Zahl von Graphic Novels wird üblicherweise in Hard- oder Softcover-Buchausgaben herausgebracht. Auch ursprünglich in Einzelheften erschienene Comicserien, wie "From Hell" oder "Watchmen", werden, in Buchform gesammelt, als Graphic Novels bezeichnet.

Die meisten Comics wurden und werden mit Techniken der Grafik geschaffen, insbesondere als Zeichnung mit Bleistift oder Tusche. Üblich ist auch, dass zunächst Vorzeichnungen mit Bleistift oder anderen leicht entfernbaren Stiften gezeichnet werden und danach eine Reinzeichnung mit Tusche erfolgt. Als Ergänzung dazu ist teilweise der Einsatz von Rasterfolie oder vorgefertigten, mit Bildmotiven bedruckten Folien verbreitet. Neben der Zeichnung mit Stift und Tusche sind auch alle anderen Techniken der Grafik und Malerei sowie die Fotografie zur Produktion von Comics möglich und finden Anwendung, beispielsweise in Fotoromanen. Bis zum 19. Jahrhundert, in dem sich mit dem modernen Comic auch die heute üblichen Techniken durchsetzten, gab es bereits eine große Bandbreite an künstlerischen Verfahren für Bildgeschichten. So das Malen in Öl und Drucken mit Stichen, Fresken, Stickerei oder aus farbigem Glas gesetzte Fenster. Auch mit Relief und Vollplastik wurden Comics geschaffen. Seit den 1990er Jahren hat die im Ergebnis dem traditionellen Zeichnen optisch oft ähnliche Fertigung mit elektronischen Mitteln wie dem Zeichenbrett größere Verbreitung erfahren. Darüber hinaus entstand mit dem ausschließlich elektronischen Zeichnen auch neue Stile und Techniken. Eine Sonderform bilden die 3D-Comics.

Bestimmend für die Wahl der Technik war oft, dass die Bilder mit Druckverfahren vervielfältigt werden. Daher dominieren Werke mit Grafiken, die aus festen Linien bestehen. Für farbige Bilder werden in der Regel im Druck Flächenfarben oder Rasterfarben des Vierfarbdrucks ergänzt. Durch die Verbreitung von Scanner und Computer zur Vervielfältigung sowie dem Internet als Verbreitungsweg sind die Möglichkeiten der Zeichner, andere Mittel und Techniken zu nutzen und zu entwickeln, deutlich gewachsen.

In Amerika und Europa traten in der Comicbranche lange Zeit fast ausschließlich weiße, heterosexuelle Männer in Erscheinung. Jedoch war in der ersten Hälfte des 20. Jahrhunderts über die meisten Künstler nur wenig bekannt. Angehörige von Minderheiten konnten so Vorurteilen entgehen. Frauen und gesellschaftliche Minderheiten traten erst ab den 1970er Jahren vermehrt als Autoren und Zeichner in Erscheinung. Dies ging häufig einher mit der Gründung von eigenen Organisationen, wie der "Wimmen’s Comicx Collective" oder dem Verlag "Afrocentric" in den Vereinigten Staaten.

Bis ins 19. Jahrhundert wurden Comics und Bildergeschichten fast ausschließlich von einzelnen Künstlern allein angefertigt. Durch die Veröffentlichung der Comics in Zeitungen und zuvor bereits in ähnlichen Massenprintmedien waren die Künstler im 19. immer öfter für einen Verlag tätig. Ihr Produkt war dennoch individuell und Serien wurden eingestellt, wenn der Künstler sie nicht selbst fortsetzte. Mit Beginn des 20. Jahrhunderts kam es häufiger zu Kooperationen von Zeichnern und Autoren, die gemeinsam im Auftrag eines Verlags an einer Serie arbeiteten. Zunehmend wurden Serien auch mit anderen Künstlern fortgesetzt. In großen Verlagen wie "Marvel Comics" oder unter den Herausgebern der Disney-Comics haben sich so Stilvorgaben durchgesetzt, die ein einheitliches Erscheinungsbild von Serien ermöglichen sollen, auch wenn die Beteiligten ausgewechselt werden. Dennoch gibt es auch in diesem Umfeld Künstler, die mit ihrem Stil auffallen und prägen. Im Gegensatz dazu entwickelten sich auch Comic-Studios, die selbstständiger von Verlagen sind. Teilweise werden diese von einem einzelnen Künstler dominiert oder bestehen schlicht zur Unterstützung des Schaffens eines Künstlers. Eine solche Konstellation findet sich beispielsweise bei Hergé und ist in Japan weit verbreitet. In Anlehnung an den von den Regisseuren der Nouvelle Vague geprägten Begriff des Autorenfilms entstand auch der Begriff des "Autorencomic", der im Gegensatz zu den arbeitsteilig entstehenden konventionellen Mainstream-Comics nicht als Auftragsarbeit, sondern als Ausdruck einer persönlichen künstlerischen und literarischen Handschrift, die sich kontinuierlich durch das gesamte Werk eines Autors zieht, entsteht. Je nach Arbeitsweise – allein, im Team oder direkt für einen Verlag – verfügt der einzelne Mitwirkende über mehr oder weniger Spielraum, was sich auch auf die Qualität des Werkes auswirkt.

Sowohl bei Verlagen als auch bei Studios einzelner Künstler ist die Arbeit in der Regel auf mehrere Personen verteilt. So kann das Schreiben des Szenarios, das Anfertigen von Seitenlayouts, das Vorzeichnen der Seiten, das Tuschen von Bleistiftzeichnungen und das Setzen von Text von verschiedenen Personen ausgeführt werden. Auch die Anfertigung von Teilen des Bildes wie Zeichnen von Figuren und Hintergrund, Setzen von Schraffuren und Rasterfolie und das Kolorieren kann auf mehrere Mitwirkende verteilt sein.

Populäre Lithographien, frühe Comics und Bildgeschichten, wurden in Deutschland von Lumpensammlern verkauft, die diese mit sich trugen. Später wurden Comics in Nordamerika und Europa bis in die 1930er Jahre fast ausschließlich über Zeitungen verbreitet. Mit den Comicheften kam in den USA ein Remittendensystem auf, in dem die Comics über Zeitungskioske vertrieben wurde. Nicht verkaufte Exemplare gingen dabei zum Verlag zurück oder wurden auf dessen Kosten vernichtet. Ab den 1960er Jahren konnten sich reine Comicläden etablieren und mit ihnen der „Direct Market“, in dem der Verlag die Bücher direkt an den Laden verkauft. Auch neu entstandene Formate wie das Comicalbum oder Comicbook wurden über diesen Weg an ihren Kunden gebracht.

Durch die Entwicklung des Elektronischen Handels ab den 1990er Jahren nahm der Direktvertrieb vom Verlag oder direkt vom Künstler zum Leser zu, darunter der Vertrieb von digitalen statt gedruckten Comics. Dieser bietet den Vorteil geringerer Produktionskosten, was zusammen mit der für alle Verkäufer großen Reichweite und Marketing über soziale Netzwerke zu größeren Chancen auch für kleinere Anbieter, wie Selbstverleger und Kleinverlage, führt.

Der Umgang mit den Urheber- und Nutzungsrechten an Comics war in der Geschichte des Mediums immer wieder umstritten. So führte der Erfolg von William Hogarths Bildergeschichten dazu, dass diese von anderen kopiert wurden. Zum Schutz des Urhebers verabschiedete das englische Parlament daher 1734 den "Engraver’s Act". Künstler, die ihre Werke selbst und allein schaffen, verfügen über die Rechte an diesen Werken und können über deren Veröffentlichung bestimmen. Im 19. und beginnenden 20. Jahrhundert traten neue Konflikte auf, da zunehmend mehr Menschen an einem einzelnen Comic beteiligt waren, so der Redakteur oder verschiedene Zeichner und Autoren. Dies führte unter anderem dazu, dass die Rechte einer Serie zwischen einem Verlag und dem Künstler aufgeteilt wurden oder dass die Urheber im Vergleich zum Erlös des Verlags nur eine geringe Bezahlung erhielten. Im Laufe des 20. Jahrhunderts etablierten sich Verträge zwischen allen Beteiligten, die zu einer klaren Rechtslage führen.

Neben vielfältigen Techniken hat sich im Comic eine eigene Formensprache entwickelt, da das Medium besonders auf bildhafte Symbole angewiesen ist. Diese dienen zur Verdeutlichung von Gemütszuständen oder der Sichtbarmachung nicht gegenständlicher Elemente der dargestellten Ereignisse. Dabei finden übertrieben dargestellte, aber tatsächlich auftretende „Symptome“ wie Schweißtropfen oder Tränen, oder gänzlich metaphorische Symbole Verwendung. Besonders verbreitet ist die Sprechblase als symbolische Darstellung der nicht sichtbaren Sprache und zugleich Mittel zur Integration von Text. Zur symbolhaften Darstellung von Bewegung finden vor allem „Speedlines“, die den Weg des Bewegten nachzeichnen, oder eine schemenhafte Darstellung mehrerer Bewegungsphasen Anwendung. Insbesondere beim Einsatz verschiedener Strich-, Linien- und Schraffurformen als expressionistisches Mittel zur Vermittlung von Emotionen hat sich im Comic eine große Bandbreite entwickelt. Sehr ähnlich wie der Strich wird die Schriftart und -größe von Text eingesetzt. Der Einsatz von Farben, wenn überhaupt, wird sehr verschieden gehandhabt. Da die meist eingesetzten flächigen Kolorierungen die Konturen betonen und damit das Bild statisch erscheinen lassen und die Identifizierung des Lesers erschweren können, ist die Farbkomposition auch für das Erzählen der Geschichte und die Wirkung der Figuren von großer Bedeutung.

Neben dem Einsatz der eigentlichen Symbole werden oft auch die handelnden Figuren sowie die dargestellte Szenerie vereinfacht, stilisiert oder überzeichnet dargestellt. Verschiedene Ebenen des Bildes, wie Figuren und Hintergründe, aber auch unterschiedliche Figuren, können dabei verschieden stark abstrahiert werden. Es existiert ein breites Spektrum an inhaltlicher oder formaler Abstraktion, von fotografischen oder fotorealistischen Darstellungen bis zu weitgehend abstrakten Formen oder reinen Bildsymbolen. Gerade die stilisierte, cartoonhafte Darstellung der handelnden Figuren ist bedeutend, da sie der leichten Identifikation des Lesers mit diesen Figuren dient. Durch verschiedene Maße der Stilisierung kann auf diese Weise auch die Identifikation und Sympathie des Lesers beeinflusst werden. So ist es laut Scott McCloud in vielen Stilen, wie der Ligne claire oder Manga, die Kombination von stark stilisierten Figuren und einem eher realistischen Hintergrund üblich, um den Leser "„hinter der Maske einer Figur gefahrlos in eine Welt sinnlicher Reize“" eintreten zu lassen. Er nennt dies den „Maskierungseffekt“. Dieser kann auch flexibel eingesetzt werden, sodass die Veränderung der Darstellungsart einer Figur oder eines Gegenstandes auch zu einer anderen Wahrnehmung dieser führt. Die Stilisierung und Übertreibung von Merkmalen der Figuren dient auch ihrer Charakterisierung und Unterscheidbarkeit für den Leser. Durch die Verwendung von physischen Stereotypen werden Erwartungen des Lesers geweckt oder auch bewusst gebrochen.

Für das Erzählen mit Comics zentral ist die Art, wie die Inhalte der Geschichte in Bilder aufgeteilt werden, welche Ausschnitte und Perspektiven der Autor wählt und wie die Panels angeordnet werden. Die drei Prinzipien der Erzählung im Comic nennt Eckart Sackmann das kontinuierende, integrierende und separierende, und nimmt dabei Bezug auf den Kunsthistoriker Franz Wickhoff. In Erstem reihen sich die Ereignisse ohne Trennung aneinander (zum Beispiel Trajanssäule), das integrierende Prinzip vereint die zeitlich versetzten Szenen in einem großen Bild (zum Beispiel Bilderbogen oder Wiener Genesis). Das separierende Prinzip, das im modernen Comic vorherrscht, trennt die Vorgänge in nacheinander folgende Bilder. Aus den inhaltlichen Unterschieden zwischen aufeinanderfolgenden Panels schließt der Leser durch Induktion auf die Geschehnisse, auch ohne dass jeder Moment dargestellt wird. Je nach inhaltlicher Nähe beziehungsweise Ferne der Bilder wird dem Leser verschieden großer Interpretationsspielraum gewährt. Scott McCloud ordnet die Panelübergänge in sechs Kategorien: Von Augenblick zu Augenblick, von Handlung zu Handlung (bei gleich bleibendem betrachteten Gegenstand), von Gegenstand zu Gegenstand, von Szene zu Szene, von Aspekt zu Aspekt und schließlich der Bildwechsel ohne logischen Bezug. Er stellt fest, dass für Erzählungen besonders häufig die Kategorien 2 bis 4 verwendet werden, während die letzte Kategorie für Narration gänzlich ungeeignet ist. Der Umfang, in dem bestimmte Kategorien verwendet werden, unterscheidet sich stark je nach Erzählstil. Ein bedeutender Unterschied besteht zwischen westlichen Comics bis zu den 1990er Jahren und Mangas, in denen die Kategorien 1 und 5 deutlich stärkere Verwendung finden. Dietrich Grünewald definiert dagegen nur zwei Arten von Anordnungen: die „enge“ und die „weite Bildfolge“. Während die erste Aktionen und Prozesse abbilde und seit dem ausgehenden 19. Jahrhundert den Comic vorrangig präge, beschränke sich die zweite auf deutlich weiter auseinanderliegende, ausgewählte Stationen eines Geschehens. Diese miteinander zu verbinden verlange eine aufmerksamere Betrachtung des einzelnen Bildes; bis zum modernen Comic sei dies die vorherrschende Erzählweise gewesen. Die Panelübergänge beeinflussen sowohl die Wahrnehmung von Bewegung und welche Aspekte der Handlung oder des Dargestellten vom Leser besonders wahrgenommen werden, als auch die vom Leser gefühlte Zeit und den Lesefluss.

Für die Wahrnehmung von Zeit und Bewegung ist darüber hinaus das Layout der Seiten von Bedeutung. Bewegung, und mit ihr auch Zeit, wird außerdem durch Symbole dargestellt. Auch die Verwendung von Text, insbesondere der von Figuren gesprochener Sprache, wirkt sich auf den Eindruck von erzählter Zeit aus. Ebenso dient der Einsatz verschiedener Panelformen und -funktionen dem Erzählen. Verbreitet ist die Verwendung von „Establishing Shots“ bzw. eines „Splash Panel“, die in eine Szene bzw. einen neuen Ort der Handlung einführen. Diese sind auch ein Anwendungsfall teilweise oder ganz randloser Panels. Die Auswahl des Bildausschnitts und dargestellten Moments einer Bewegung beeinflusst des Lesefluss insofern, dass die Wahl des „fruchtbaren Moments“, also der geeigneten Pose, die Illusion einer Bewegung und damit die Induktion unterstützt.

Die Integration von Text geschieht im Comic sowohl über Sprechblasen, als auch die Platzierung von Wörtern, insbesondere Onomatopoesien und Inflektive, direkt im Bild oder unter dem Bild. Text und Bild können auf verschiedene Weise zusammen wirken: sich inhaltlich ergänzen oder verstärken, beide den gleichen Inhalt transportieren oder ohne direkten Bezug sein. Ebenso kann ein Bild bloße Illustration des Textes oder dieser nur eine Ergänzung des Bildes sein.

Der Leser des Comics nimmt zum einen das Gesamtbild einer Seite, eines Comicstrips oder eines einzeln präsentierten Panels als Einheit wahr. Es folgt die Betrachtung der einzelnen Panels, der Teilinhalte der Bilder und der Texte, in der Regel geführt durch Seitenlayout und Bildaufbau. Dabei findet sowohl aufeinander folgende als auch simultane, abstrakte und anschauliche Wahrnehmung statt. Die oft symbolischen Darstellungen werden vom Leser interpretiert und in einen, soweit ihm bekannten, Kontext gesetzt und das dargestellte Geschehen und seine Bedeutung daraus aktiv konstruiert. Dietrich Grünewald nennt, auf Grundlage der Arbeit Erwin Panowskys, vier inhaltliche Ebenen der Bildgeschichte. Die erste Ebene, „vorikonografische“ ist die der dargestellten Formen, Grünewald nennt dies auch die „Inszenierung“, also die Auswahl und Anordnung der Formen sowie der Bilder und Panel auf der Seite. Die zweite, „ikonografische“ Ebene umfasst die Bedeutung der Formen als Symbole, Allegorien u. A. Die „ikonologische“ dritte Ebene ist die der eigentlichen Bedeutung und Inhalt des Werks, wie sie sich auch aus dem Kontext ihrer Zeit und des künstlerischen Werks des Schöpfers ergibt. Eine vierte Ebene sieht er in der Bildgeschichte als Spiegel der Zeit, in der sie entstanden ist, und in dem, was sie über ihren Künstler, ihr Genre oder ihren gesellschaftlichen Kontext aussagt.

Der Comic als Kunstform und Medium ist an kein Genre gebunden. Dennoch sind bestimmte Genres innerhalb des Comics besonders weit verbreitet oder haben in ihm ihren Ursprung. So entstand durch Serien wie "Krazy Kat" bereits zu Beginn des 20. Jahrhunderts mit dem „Funny Animal“-Comicstrip ein dem Medium eigenes Genre, das später auch im Trickfilm Verwendung fand. Die seit dieser Zeit entstandenen humorvollen Comics werden allgemein als Funnys bezeichnet. Daneben waren zunächst vor allem Geschichten aus dem Alltag der Leser oder über realistische oder phantastische Reisen verbreitet. Die amerikanischen Abenteuercomics der 1930er Jahre prägten gemeinsam mit dem damaligen Film Kriminal- und Piratengeschichten, Western und Science-Fiction. Zugleich entstand als eine Zwischenform von Funny und Abenteuercomic der Semifunny. Mit dem Superhelden entstand in den USA Ende der 1930er Jahre erneut ein Comic-eigenes Genre, das sich später insbesondere auch in Film und Fernsehen fand. Das kurzzeitig umfangreiche Aufkommen von Comics mit Horror- und besonders gewaltorientierten Krimi-Themen, vor allem publiziert vom Verlag EC Comics, wurde durch den Comics Code zu Beginn der 1950er Jahre beendet.

Im europäischen Comic hat sich neben humoristischen Zeitungsstrips eine Tradition etwas längerer Abenteuergeschichten gebildet, dessen bedeutendste frühe Vertreter Hergé und Jijé sind. In Japan entstand mit der Entwicklung des modernen Mangas eine große Anzahl an Genres, die dem Medium eigen sind und sich später auch im Anime etablierten. Einige bedeutende Genres, wie Shōnen und Shōjo, kategorisieren dabei nicht nach Thema des Werks, sondern nach Zielgruppe, in diesem Falle Jungen und Mädchen. Dabei wurde, in zuvor durch das Medium Comic nicht erreichtem Umfang, auch eine weibliche Leserschaft angesprochen.

Nachdem Comics mit romantischen Geschichten, die sich traditionell an Mädchen richteten, im westlichen Comic fast völlig verschwunden waren, konnten sich weibliche Zeichner und Comics für ein weibliches Publikum ab den 1970er Jahren nur langsam durchsetzen. In der gleichen Zeit wurden Underground Comix mit Zeichnern wie Robert Crumb und Art Spiegelman zum Ausdruck der Gegenkultur in den USA. Wie auch in Japan wurden zunehmend Werke mit politischen und historischen Themen, später auch biografische Werke und Reportagen, veröffentlicht und es entwickelte sich die Graphic Novel bzw. Gekiga als Sammelbegriff für solche Comics.

Bis zum 19. Jahrhundert griffen Comics vor allem den Alltag ihres Publikums komisch oder satirisch auf, vermittelten historische Begebenheiten oder religiöse Themen. Mit dem modernen Comic kamen zu den Werken mit Unterhaltungsfunktion oder politischer Intention auch wissensvermittelnde Sachcomics und Comic-Journalismus.

Ein ebenfalls bedeutendes Genre des Comic ist der erotische Comic. Dabei ist die ganze Breite der erotischen Darstellungen vertreten; von romantisch, verklärten Geschichten über sinnlich anregende Werke bis hin zu pornografischen Inhalten mit den Darstellungen der verschiedensten Sexualpraktiken. Bedeutende Vertreter des Genres sind Eric Stanton, Milo Manara und Erich von Götha, aber auch der deutsche Zeichner Toni Greis.

Der Leser eines Comics fügt die Inhalte der einzelnen Panels zu einem Geschehen zusammen. Damit dies möglichst gut gelingt, werden auch Techniken verwendet, wie sie in der Filmkunst ähnlich vorkommen. Die einzelnen Panels zeigen Einstellungsgrößen wie "Totale" oder "Halbnahe", es wird zwischen verschiedenen Perspektiven gewechselt. Fast alle Techniken der Filmkunst haben ihr Pendant im Comic, wobei im Comic durch den variablen Panelrahmen die Veränderung des Ausschnitts noch leichter fällt als im Film. So entspricht dem genannten Establishing Shot in vielen Comics ein „Eröffnungs-Panel“ bzw. ein Splash Panel, das die Szenerie zeigt.

Die enge Verwandtschaft zeigt sich auch in der Erstellung von Storyboards während der Produktionsphase eines Films, die den Verlauf des Films und insbesondere die Kameraeinstellung in einem Comic skizzieren und dem Regisseur und Kameramann als Anregung oder Vorlage dienen. Der textliche Entwurf eines Comics, geschrieben vom Autor, wird „Skript“ genannt und dient dem Zeichner als Grundlage für seine Arbeit. Während die durch die Gutter-Struktur vorgegebenen „Informationslücken“ im (skizzenhaften) Film-Storyboard vernachlässigt und im späteren Produkt durch filmische Mittel geschlossen werden können, erfordern sie von Comic-Autoren eine erhöhte Aufmerksamkeit, damit beim endgültigen Produkt ein flüssiges Leseverstehen seitens der Leserschaft gewährleistet ist.

Im Unterschied zum Film erfordert der Comic jedoch das Ausfüllen der Lücken zwischen den Panels. Denn anders als im Film, wo sowohl eine Änderung der Perspektive durch Kameraschwenk und/oder Zoom als auch Bewegungsabläufe von Personen und Objekten innerhalb einer Einstellung vermittelt werden können, kann dies im Comic innerhalb eines Panels allenfalls durch "Bewegungslinien", einander in "Bewegungsschemata" überlagernde Bilder oder "Panel im Panel" angedeutet werden. Zwischen den Panels ergibt sich so zwangsläufig eine Informationslücke, die im Allgemeinen größer ist, als die zwischen Einstellung und Einstellung. Der Comic-Leser ist also im Vergleich zum Film-Zuschauer stärker gefordert, durch selbsttätiges Denken – „Induktion“; vgl. Induktion (Film) – einen dynamischen Ablauf aus statischen Bildern zu konstruieren. Auf diese Weise und auch wegen der in der Regel geringeren Zahl an Beteiligten an einem Werk ist die Beziehung zwischen Autor und Konsument im Comic intimer und aktiver als im Film. Ein weiterer Unterschied ist die Lese- bzw. Sehgeschwindigkeit sowie die Reihenfolge, in der die Bilder erfasst werden. Im Film ist dies vorgegeben, der Comicleser dagegen bestimmt diese frei, kann dabei aber vom Künstler geleitet werden. Ähnliches gilt für den Inhalt der Einzelbilder, dessen Wahrnehmung beim Film durch die Tiefenschärfe gelenkt wird und auf eine gleichzeitig laufende Handlung eingeschränkt. Im Comic dagegen sind in der Regel alle Teile des Bildes scharf dargestellt und es gibt die Möglichkeit, zwei parallele Handlungen, zum Beispiel Kommentare von Figuren im Hintergrund, in einem Bild darzustellen.

Die stärkste Verwandtschaft der Medien Film und Comic zeigt sich im Fotocomic, da für diesen die einzelnen Bilder der Comicseite nicht gezeichnet, sondern wie beim Film mit einer Kamera produziert werden.

Ähnlich wie bei der Vorstellung der Handlung in rein wortbasierten Literaturformen ist im Comic die aktive Mitwirkung des Lesers erforderlich. Im Unterschied zur reinen Textliteratur ist das Kopfkino beim Comic-Lesen in der Regel stärker visuell ausgeprägt, der Gebrauch bildlicher Mittel ist der bedeutendste Unterschied zwischen Comic und Textliteratur. Durch Gebrauch von Bildsymbolen wirkt der Comic unmittelbarer auf den Leser als die Erzählstimme der Prosa. Auch kann der Autor nicht nur durch die Wahl der Worte, sondern auch in den Bildern einen persönlichen Stil zeigen. Die Notwendigkeit, Textkohäsion durch grafische Mittel herzustellen, führt Scott McCloud als wichtiges Kriterium von Comics an. Aufgrund dieses Kriteriums sind Comics aus literaturwissenschaftlicher Perspektive eine Form von Literatur, obgleich sie dessen unbeschadet aus kunstwissenschaftlicher Sicht eine eigenständige Kunstform darstellen.

In der Bedeutung von markanten Posen, Symbolen und stilisierten Figuren weist der Comic Gemeinsamkeiten mit dem Theater auf, insbesondere mit dem Papiertheater. In beiden Medien soll der Rezipient die Figuren durch hervorgehobene Eigenschaften, in Gesicht oder dem Kostüm, wiedererkennen um dem Geschehen folgen zu können. Dabei werden durch Stereotypen bekannte Muster und Vorurteile angesprochen, die das Verständnis der Geschichte erleichtern oder erzählerischen Kniffen dienen. Auch die Darstellung des Handlungsortes durch einen einfachen aber prägnanten Hintergrund bzw. ein Bühnenbild ist in beiden Medien wichtig. Einige Techniken des Theaters zur Vermittlung von Raumtiefe und Dreidimensionalität, so die Überlagerung von Figuren aus dem Papiertheater, die Fluchtperspektiven des Theaters der Renaissance oder das Ansteigen des Bühnenbodens nach hinten, wurden vom Comic adaptiert. Während im Theater jedoch, eingeschränkt auch im Papiertheater, Bewegung direkt dargestellt werden kann, ist der Comic auf die Verwendung von Symbolen und die Abbildung von mehreren Bewegungsphasen angewiesen. Ähnlich verhält es sich mit Geräuschen und Sprache. Im Comic fällt es dagegen leichter, parallele Handlungen, Ort- und Zeitsprünge abzubilden.

Da der Comic sich der Mittel der bildenden Kunst zur Darstellung des Handlungsablaufs bedient, gibt es einige Schnittmengen zwischen beiden Kunstformen. So ist in beiden die Wahl von Bildausschnitt, Perspektive und dargestelltem Moment bzw. Pose bedeutsam. Der richtig gewählte „fruchtbare Moment“ lässt ein Bild lebendiger, überzeugender wirken und unterstützt im Comic den Lesefluss. Methoden zur Darstellung von Bewegung, die Künstler des Futurismus erkundet haben, fanden später Anwendung im Comic.

In der Anfangszeit des modernen Comic wurde das Medium als Unterhaltung für die ganze Familie verstanden. Auch ernsthafte Künstler wie Lyonel Feininger beschäftigten sich mit dem Comic und Pablo Picasso war begeistert vom Strip "Krazy Kat". Erst mit der von den Vertrieben vorgeschriebenen Beschränkung der Strips auf simple Gags und der Etablierung des Fernsehens als vorherrschendes Familienunterhaltungmedium wandelt sich die Wahrnehmung der Comics in den USA.

Zunehmend wurde Comics der Vorwurf gemacht, sie übten auf jugendliche Leser einen verrohenden Einfluss aus, der zu einer oberflächlichen, klischeehaften Wahrnehmung ihrer Umwelt führe. Ein Artikel von Sterling North, in dem erstmals auf die vermeintliche Gefahr durch Comics aufmerksam gemacht wurde, leitete 1940 in den USA landesweit eine erste Kampagne gegen Comics ein. Höhepunkt waren die Bemühungen im Amerika der 1950er Jahre, Horror- und Crime-Comics wie "Geschichten aus der Gruft" vom Verlag EC Comics zu verbieten. 1954 veröffentlichte der Psychiater Fredric Wertham sein einflussreiches Buch "Seduction of the Innocent", in dem er die schädliche Wirkung der Crime- und Horrorcomics auf Kinder und Jugendliche nachzuweisen suchte. Einer Studie von 2012 gemäß sind zahlreiche der Forschungsergebnisse in Werthams Buch durch den Autor bewusst manipuliert oder sogar erfunden worden; in seiner Zeit wurde es jedoch breit rezipiert und wirkte sich nachhaltig auf die Produktion und das Verständnis von Comics aus. Es folgten Senatsanhörungen zum Problem der Comics, was zwar nicht zum generellen Comicverbot, aber zur Einführung des Comics Code führte, einer Selbstzensur der Comicindustrie. Die hier festgelegten Verpflichtungen wie das Verbot, Verbrecher in irgendeiner Weise sympathisch und ihre Handlungen nachvollziehbar erscheinen zu lassen, führten zu einer erzählerischen Verflachung der Comics. Die Wahrnehmung der Comics beschränkte sich danach im englischen Sprachraum lange Zeit auf Genres wie den Superhelden-Comic oder Funny Animal. In Deutschland kam es in den 1950er Jahren zu einer ähnlich gearteten, sogenannten „Schmutz-und-Schund“-Kampagne. In dieser wurden Comics pauschal als Ursache für Unbildung und Verdummung der Jugend, als „Gift“, „süchtig machendes Opium“ und „Volksseuche“ bezeichnet. Auf dem Höhepunkt der Kampagne wurden Comics öffentlichkeitswirksam verbrannt und vergraben. Die Forderungen der Kritiker waren ähnlich wie in den Vereinigten Staaten und gingen bis zu einem generellen Verbot von Comics. Dies wurde jedoch nicht erfüllt, der Bundesgerichtshof forderte eine konkrete Prüfung der einzelnen Darstellung. Die dafür neu gegründete Bundesprüfstelle für jugendgefährdende Schriften indizierte schließlich deutlich weniger Werke, als von den Kritikern gewünscht. Ebenso wie in den USA wurde in Deutschland eine Freiwillige Selbstkontrolle (FSS) gegründet, die Comics auf sittliche Verstöße und Gewalt prüfte und mit einem Prüfsiegel versah. Ähnliche Initiativen und Entwicklungen gab es auch in anderen europäischen Ländern. In der Folge galten Comics, insbesondere in Deutschland, seit den frühen 1950er Jahren als Inbegriff der Schundliteratur. Langfristige Folge war, so urteilt Bernd Dolle-Weinkauff 1990, nicht die Verdrängung der Comics, sondern die Abschreckung von Autoren, Zeichnern und Verlagen mit qualitativem Anspruch, sodass „die Produktion von Schund […] kräftig gefördert“ wurde.

Die Wahrnehmung von Comics wurde im Nachgang der „Schmutz-und-Schund“-Kampagne geteilt – Bilderfolgen von Dürer bis Masereel wurden als Hochkultur anerkannt, ebenso einige Werke des frühen modernen Comics, darunter Wilhelm Busch. Die als Massenmedien verbreiteten Werke des 20. Jahrhunderts wurde als Unterhaltende, minderwertige Kunst gesehen. Seit den 1970er Jahren schwächte sich dies ab, da zum einen Populärkultur allgemein immer weniger pauschal abgewertet wird und Einfluss auf anerkannte Hochkunst nahm, zum anderen haben Werke wie Art Spiegelmans "Maus – Die Geschichte eines Überlebenden" die öffentliche Sicht auf Comics verändert. Seitdem findet beispielsweise auch in Feuilletons die Rezension von Comics statt. Das Schweizerische Jugendschriftenwerk titelte in der Ausgabe 4/1987 einem Artikel "Vom Schund zum Schulmittel" von Claudia Scherrer. Mit den Worten "„Das Medium Comic ist so salonfähig geworden, daß selbst das Schweizerische Jugendschriftenwerk SJW Bildgeschichten ins Programm aufgenommen hat – dies, obwohl“" [sic] "„das SJW zum Schutz der Jugend gegen Schundliteratur gegründet worden war“" empfahl es auch Werke anderer Verlage. Auch in Deutschland und Österreich sind Comics seit den 1970er Jahren unterrichtsrelevant, sowohl als Thema im Deutsch-, Kunst- oder Sozialkundeunterricht als auch als Unterrichtsmittel in anderen Fächern.

Kritik am Inhalt von Comics seit den 1960er Jahren bezieht sich oft auf wiederholende, nur wenig variierte Motive, wie sie insbesondere in den Abenteuer-Genres üblich sind (Western, Science-Fiction, Fantasy). Dem Leser werde eine einfache Welt geboten, in der er sich mit dem Guten identifiziere und mit diesem einen (Teil-)Sieg erringe. Dem wird entgegnet, dass der Reiz für den Leser gerade darin liege, dass er in Geschichten mit solchen Motiven aus seiner komplexen aber erlebnisarmen Alltagswelt ausbrechen könne. Einen vergleichbaren Zugang und Reiz wie die Abenteuer-Genres böten die älteren Märchen. Wiederholende Themen und Strukturen böten einen einfachen Einstieg in die Unterhaltungslektüre. Schließlich bevorzuge der Leser dabei Geschichten, die nicht zu weit von seinen Erwartungen abweichen, was Künstler und Verlage, die eine breite Leserschaft erreichen wollen, zu einer gewissen Konformität zwingt. Dies wirkt aber bei anderen Medien der Popkultur, wie Film und Fernsehen, ähnlich. Dennoch entwickeln sich die Motive bei gesellschaftlichen Änderungen weiter und nehmen an diesen Teil. Beispielsweise zeigt sich das an der Entwicklung der Superheldencomics, in denen mit der Zeit auch Themen wie Gleichberechtigung und soziales Engagement Einzug hielten. Inhaltliche Kritik gab es außerdem an Comics, in den 1970er Jahren vor allem Disney-Geschichten, in denen die Vermittlung imperialistischer, kapitalistischer oder anderer Ideologie vermutet wurde. Jedoch gab es auch widersprechende Interpretationen, so kann die Figur des Dagobert Duck als Verniedlichung des Kapitalismus, aber auch als Satire mit dem Mitteln der Übertreibung gelesen werden. Sowohl bei der Flucht des Lesers aus dem Alltag in eine Fantasiewelt, bei der negative Auswirkungen auf Leben und Wahrnehmung des Lesers unterstellt werden, als auch der Furcht vor Ideologie, hängt die Sicht auf die jeweiligen Comic-Werke vor allem davon ab, welche Fähigkeit zur Distanzierung und Interpretation dem Leser zugetraut werden. Darüber hinaus gibt es viele Comics, die zwar oft keine große Bekanntheit erreichen, sich inhaltlich aber jenseits der kritisierten Motive und Klischees bewegen.

Besonders in den USA kam es immer wieder zu Auseinandersetzungen und Prozessen um Comics, die pornografisch waren oder so angesehen wurden, da Comics als reine Kinderliteratur wahrgenommen wurden. In Deutschland blieben juristische Maßnahmen wie Beschlagnahmen von Comics die Ausnahme, Gerichte räumten der Kunstfreiheit in der Regel auch bei Comics einen höheren Rang ein als dem Jugendschutz. In lexikalischen Werken wurden Comics meist abschätzig beurteilt. So befand noch die Brockhaus Enzyklopädie in ihrer 19. Auflage, Bd. 4 (1987), die meisten der Serien seien als triviale Massenzeichenware zu charakterisieren, als „auf Konsum angelegte Unterhaltung, die von Wiederholungen, von Klischees bestimmt wird und ihren Lesern kurzfristig Ablenkung von ihren Alltagsproblemen bietet“. Daneben gebe es aber auch ein Comic-Angebot, das sich künstlerischer Qualität verpflichtet fühle.

Wissenschaftliche Schriften zu Comics erschienen ab den 1940er Jahren, standen dem Medium jedoch oft einseitig und undifferenziert kritisch gegenüber und setzten sich nicht mit den Funktionsweisen und Aspekten des Comics auseinander. In den USA erschien mit Martin Sheridans "Comics and Their Creators" 1942 das erste Buch, das sich dem Comic widmete. Es folgten zunächst Beschäftigungen mit der Geschichte der Comics und erste Schriften, die den Umfang an erschienenen Werken systematisch erschließen sollten. Comics wurden in Deutschland zunächst wegen der „Schmutz-und-Schund“-Kampagne der 1950er Jahre nur wenig wissenschaftliche Beachtung. In bestimmten Kreisen der Literaturwissenschaft wurde dem Comic der Vorwurf der Sprachverarmung gemacht, was durch den häufigeren Gebrauch von unvollständigen Sätzen und umgangssprachlichen Ausdrücken in Comics gegenüber der Jugendliteratur nachgewiesen werden sollte. Dabei wurde missverstanden, dass der Text in den meisten Comics fast ausschließlich aus Dialogen besteht, und eine eher dem Kino und dem Theater als der Literatur vergleichbare Funktion besitzt. Die Kritik der Sprachverarmung kann auch aus dem Grunde als veraltet und ahistorisch bezeichnet werden, als die Verwendung von Umgangs- und Vulgärsprache in der Literatur schon lange kein Qualitätskriterium mehr darstellt.

Eine ernsthaftere, kulturwissenschaftliche Beschäftigung begann in den 1960er Jahren zunächst in Frankreich, begonnen mit der Gründung des "Centre d’Études des Littératures d’Expression Graphique" (CELEG) 1962. Der Umfang der Veröffentlichungen nahm zu, neben der Geschichte wurden auch Genres, Gattungen, einzelne Werke und Künstler untersucht. Es erschienen lexikalische Werke über Comics, erste Ausstellungen fanden statt und Museen wurden gegründet. Bald geschah dies auch in anderen europäischen Ländern, Nordamerika und Japan. Mit der Zeit entstanden auch erste Studiengänge zu Comics. Die umfangreichere wissenschaftliche Auseinandersetzung mit Comics begann in den Vereinigten Staaten in den 1970er Jahren und beschränkte sich zunächst auf soziologische Gesichtspunkte. Unter dem Einfluss der 68er-Bewegung wurde der Comic dann zunächst unter dem Aspekt des Massenmediums oder der Massenzeichenware betrachtet und als solche definiert. Soziologisch und medienkritisch orientierte Betrachtungen waren daher zunächst vorherrschend, später kamen auch psychologische dazu, wie die Untersuchung der Auswirkung von Gewaltdarstellungen auf Kinder. Auch nachdem diese stärker eingeschränkte Definition bis spätestens in den 1990er Jahren zugunsten der heutigen verworfen wurde, bleibt die Betrachtung dieser Aspekte ein wichtiger Teil der Comicforschung- und theorie. Untersuchungen des Erzählens mit Comics fand zunächst mit Methoden statt, die für die Textliteratur entstanden und für den Comic angepasst wurden. In Berlin gründete sich mit der Interessengemeinschaft Comic Strip (INCOS) ein erster deutscher Verband zur Förderung der Comicforschung. 1981 folgte ihm der Interessenverband Comic, Cartoon, Illustration und Trickfilm (ICOM), der in seinen Anfangsjahren Veranstaltungen organisierte, darunter 1984 mit dem Comic-Salon Erlangen die bedeutendste deutsche Veranstaltung zu Comics, sowie Comicforschung unterstützt. So enthält das seit 2000 als Nachfolger des verbandseigenen Fachmagazins erscheinende COMIC!-Jahrbuch neben Interviews auch immer wieder Artikel zur Struktur und Entwicklung des Mediums. 2007 gründete sich die Gesellschaft für Comicforschung. Seit den 1970er Jahren erscheinen auch im deutschsprachigen Raum Fachmagazine und Fanzines zu Comics, darunter die "Comixene", "Comic Forum" und "RRAAH!". Auch Museen zeigten seitdem Ausstellungen zu Comics und die systematische Erfassung deutscher Werke begann. In der DDR fand dagegen nur wenig wissenschaftliche Beschäftigung mit Comics statt, diese war zudem auf die Abgrenzung von „kapitalistischem Comic“ und „sozialistischer Bildgeschichte“, das heißt die Produktionen der sozialistischen Länder, fokussiert.

Gemeinsam mit der ersten Comicforschung begann in den 1970er Jahren die Diskussion, ob Comics eine eigene Kunstform darstellen. In den 1990er Jahren wurden Comics zunehmend als Kunst anerkannt und es erfolgte die Auseinandersetzung mit den Formen und der Semiotik des Comics, zu der auch Erzähltheorien des Comics entwickelt wurden. Auch empirische Untersuchungen des Leseverhaltens finden seitdem statt, jedoch oft motiviert durch die Verlage und mit Methoden, die in Zweifel gezogen werden. In der universitären Forschung etablierte sich die 1992 gegründete Arbeitsstelle für Graphische Literatur (ArGL) an der Universität Hamburg, darüber hinaus finden sporadisch Symposien und Tagungen statt.





</doc>
<doc id="981" url="https://de.wikipedia.org/wiki?curid=981" title="Cosinus (Begriffsklärung)">
Cosinus (Begriffsklärung)

Cosinus steht für


</doc>
<doc id="983" url="https://de.wikipedia.org/wiki?curid=983" title="Compact Disc">
Compact Disc

Die (kurz CD, für "kompakte Scheibe") ist ein optischer Speicher, der Anfang der 1980er Jahre zur digitalen Speicherung von Musik von Philips/PolyGram und Sony eingeführt wurde und die Schallplatte ablösen sollte. Zuanfangs war die einzige Nutzung der Compact Disc die für digitale Audiodaten, daher wird sie auch oft als Synonym für die Compact Disc Digital Audio (CD-DA) bezeichnet. Später kamen jedoch weitere Nutzungen hinzu, so etwa zur Speicherung von Daten als CD-ROM, für Videos als VCD oder für interaktive Daten als CD-i. Als beschreibbare CD-R löste sie Ende der 1990er Jahre die Compact Cassette als bevorzugtes Audio-Aufnahmemedium im Privatbereich ab, wurde aber in den 2000er Jahren durch die MP3-Technologie weitgehend verdrängt.

Bei Einführung der CD-ROM Anfang der 1990er Jahre stand „CD“ noch für Audio-CD und „CD-ROM“ für die Daten-CD, spätestens eine Dekade später kann „Compact Disc“ alias „CD“ jedoch als Synonym für alle Arten von CDs verstanden werden – die Unterscheidung ergibt sich durch die genaue Bezeichnung des Mediums (CD-ROM, CD-R, CD-RW) und die darauf enthaltenen Daten (Audio, Daten, Video, Interactive).

CDs bestehen aus Polycarbonat, sowie einer dünnen Metallschicht (z. B. Aluminiumbedampfung) mit Schutzlack und Druckfarben. Sie werden – im Gegensatz zu Schallplatten – nicht gepresst, sondern in Spritzgussmaschinen in Form auf die Vater-Matrize gespritzt. Die Anlagen zur Herstellung optischer Datenträger werden dennoch Presswerk genannt.

Die Informationen der CD, das sogenannte „Programm“, sind auf einer spiralförmig nach außen verlaufenden Spur angeordnet; sie belegen maximal 85 % der CD-Gesamtfläche. Der Programmbereich reflektiert Licht mit deutlichen Farberscheinungen wegen seiner Mikrostruktur, den Pits. Länge und Abstand dieser kleinen Vertiefungen bilden einen seriellen digitalen Code, der die gespeicherte Information repräsentiert. Auf einer Audio-CD können maximal 99 Musiktitel gespeichert werden; dazu hat jede Scheibe ein Inhaltsverzeichnis (TOC, table of contents) und einen der Information eingelagerten Zeitcode. Texteinblendungen und weitergehende Informationen können optional aufgebracht werden. Die Abtastung der CD erfolgt kontaktlos über einen der Spur nachgeführten Laser-Interferenzdetektor von der spiegelnden Unterseite her. Die Geschwindigkeit, mit der die Daten eingelesen werden, hängt von der Drehzahl der CD ab; diese wird traditionellerweise so geregelt, dass eine vorgegebene Datenrate eingehalten wird. Die Datenspur hat eine konstante Bahngeschwindigkeit (engl. "constant linear velocity, CLV"). Dadurch ergibt sich bei nach außen fahrendem Abtastsystem eine Drosselung der Drehzahl. Bei Verfahren ähnlich der Analogschallplatte spricht man hingegen von konstanter Winkelgeschwindigkeit (CAV).

Bei einer CD werden Daten mit Hilfe einer von innen nach außen laufenden Spiralspur gespeichert (also umgekehrt wie bei der Schallplatte). Die Spiralspur besteht aus "Pits" (Gruben) und "Lands" (Flächen), die auf dem Polycarbonat aufgebracht sind. Die Pits haben eine Länge von 0,833 bis 3,054 µm und eine Breite von 0,5 µm. Die Spiralspur hat etwa eine Länge von sechs Kilometern. Je nachdem, wie die CD erstellt wird, entstehen die Pits. Bei der industriellen Herstellung werden zunächst auf photochemischem Wege ein Glas-Master und darauf dann auf galvanischem Wege ein oder auch mehrere Stamper (Negativ) gefertigt. Anschließend wird damit in Presswerken per Spritzverfahren eine Polycarbonatscheibe geprägt und die Reflexions- und Schutzschicht angefügt.

Eine CD besteht demnach zum größten Teil aus Polycarbonat. Die Reflexionsschicht darüber besteht aus einer im Vakuum aufgedampften Aluminiumschicht.

Zwischen dem Aufdruck (Grafik und Text) und der Aluminiumschicht (Dicke der Reflexionsschicht: 50 bis 100 nm) befindet sich noch eine Schutzlackschicht, um das Aluminium vor äußeren Einflüssen zu schützen. Der Abschluss ist der Aufdruck, der mit dem Siebdruckverfahren (bis zu sechs Farben) aufgebracht wird. Alternativ kann hier auch das Offsetdruckverfahren eingesetzt werden.

Der Datenstrom einer Audio-CD hat, wenn er dekodiert ist, eine Daten­übertragungs­rate von 176,4 kB/s. Bei üblichen Daten-CDs ist durch eine weitere Fehlerkorrektur-Ebene die Blockgröße geringer als bei Audio-CDs (2048 statt 2352 Bytes); daraus folgt bei gleicher Blockrate (75 pro Sekunde) eine Daten­übertragungs­rate von 153,6 kB/s. Diese Daten­übertragungs­rate wird als "einfache Geschwindigkeit" bezeichnet. Die Geschwindigkeits­angaben bei CD-ROM-Laufwerken sind Vielfache dieser Daten­übertragungs­rate; siehe dazu auch nebenstehende Tabelle "Daten­übertragungs­raten von CD-Laufwerken."

Daten-CDs können aufgrund der zusätzlichen Fehlerkorrektur-Ebene je nach verwendetem Laufwerk mit höheren Daten­übertragungs­raten gelesen werden, so dass viele Laufwerke ihre angegebene Geschwindigkeit nur bei Daten-CDs erreichen, mit Audio-CDs dagegen langsamer arbeiten.

Ein CD-RW-Medium besitzt im Prinzip die gleichen Schichten wie ein CD-R-Medium. Die reflektierende Schicht ist jedoch eine Silber-Indium-Antimon-Tellur-Legierung, die im ursprünglichen Zustand eine polykristalline Struktur und reflektierende Eigenschaften besitzt. Beim Schreiben benutzt der Schreibstrahl seine maximale Leistung und erhitzt das Material punktuell auf 500 bis 700 °C. Das führt zu einer Verflüssigung des Materials. In diesem Zustand verliert die Legierung ihre polykristalline Struktur, nimmt einen amorphen Zustand an und verliert ihre Reflexionskraft. Der polykristalline Zustand des Datenträgers bildet die Gräben, der amorphe die Erhebungen. Das Abtastsignal beim Auslesen entsteht also nicht durch Auslöschung oder Verstärkung des Laser-Lichtes durch Überlagerung des reflektierten Lichtes mit dem ausgesendeten wie bei gepressten CDs (Interferenz), sondern wie bei beschreibbaren CDs durch gegebene oder nicht gegebene (bzw. schwächere) Reflexion des Laserstrahls. Zum Löschen des Datenträgers erhitzt der Schreibstrahl die – nur metastabilen – amorphen Bereiche mit niedriger Leistung auf etwa 200 °C. Die Legierung wird nicht verflüssigt, kehrt aber in den polykristallinen Zustand zurück und wird damit wieder reflexionsfähig.

Das Abtasten einer CD erfolgt mittels einer Laserdiode (Wellenlänge 780 nm), wobei die CD von unten gelesen wird. Der Laserstrahl wird an der CD reflektiert und mit einem halbdurchlässigen Spiegel in eine Anordnung mehrerer Fotodioden gebündelt. Die Fotodioden registrieren Schwankungen in der Helligkeit. Die Helligkeitsschwankungen entstehen teilweise aufgrund von destruktiver Interferenz des Laserstrahls mit sich selbst: Der Fokus des Laserstrahls ist etwa zwei- bis dreimal so groß wie die Breite eines Pits. Wird gerade ein "Pit" ausgelesen, dann wird der Laserstrahl teilweise vom "Pit" und teilweise vom umliegenden "Land" reflektiert. Dann kommen zwei Teilwellen zurück, die einen leicht unterschiedlichen Laufweg haben. Der Höhenunterschied zwischen "Pit" und "Land" ist so gewählt, dass der Laufzeitunterschied etwa eine halbe Wellenlänge beträgt (siehe auch Abschnitt „Funktionsweise“), so dass wegen destruktiver Interferenz die Intensität des reflektierten Lichts abnimmt. Zusätzlich wird bei den "Pits" ein Teil des Lichtes an dessen Kanten weggestreut. Die Fotodioden registrieren also auf den "Pits" eine reduzierte Helligkeit. Da die CDs von der Oberseite gepresst werden, sind die Pits (Vertiefungen) von der Unterseite her als Hügel zu erkennen.

Durch eine spezielle Lichtführung auf die Fotodioden, beispielsweise durch einen Astigmaten auf eine quadratische Anordnung von vier Fotodioden, können durch Differenzbildung der Signale unterschiedlicher Fotodioden neben dem Nutzsignal (Summe aller Signale) auch Regelgrößen für die Spurführung und den Fokus (richtigen Abstand zwischen CD und Leseoptik) ermittelt werden.

Die Optik mit dem Laser bewegt sich beim Abspielen vom ersten zum letzten Track – im Gegensatz zur Schallplatte – von innen nach außen. Außerdem hat die CD keine feste Winkelgeschwindigkeit "(Umdrehungszahl);" diese wird der momentanen Position des Lesekopfs angepasst, so dass die Bahngeschwindigkeit (CLV) und nicht, wie bei der Schallplatte, die Winkelgeschwindigkeit (CAV) konstant ist. Wenn der Lesekopf weiter außen auf der CD liest, wird die CD also langsamer gedreht. Auf diese Weise kann überall auf der CD mit voller Aufzeichnungsdichte gearbeitet werden, und es ist ein konstanter Datenstrom gewährleistet, wie er bei Audio-CDs benötigt wird. Im Red Book sind zwei verschiedene Geschwindigkeiten festgelegt, 1,2 m/s und 1,4 m/s. Somit sind entsprechend Spielzeiten von 74:41 Min. bzw. 64:01 Min., unter maximaler Ausnutzung aller Toleranzen 80:29 Min. möglich. Das entspricht einer Umdrehungsgeschwindigkeit von über 500 min am Anfang der CD (innere Spuren bei 1,4 m/s) bis unter 200 min am Außenrand der CD bei 1,2 m/s. Die Umdrehungsgeschwindigkeit wird durch einen Regelkreis anhand des Füllstandes eines FIFO-Puffers geregelt. Daher muss keine Umschaltung (weder manuell noch automatisch) je nach benutzter Linear-Geschwindigkeit erfolgen. Durch den genannten Puffer wirken sich Schwankungen der Drehzahl nicht auf die Wiedergabegeschwindigkeit aus.

Viele moderne CD-ROM-Laufwerke, ab etwa 32-facher Lesegeschwindigkeit, lesen Daten-CDs hingegen mit konstanter Umdrehungsgeschwindigkeit, um das zeitraubende Beschleunigen und Abbremsen der CD beim Hin- und Herspringen der Leseposition (aufgrund des notwendigen wahlfreien Zugriffs) zu vermeiden. Dadurch hängt bei Daten-CDs die Datenrate von der Position des Lesekopfes, also letztlich der Position auf der CD, ab. Die auf der Verpackung angegebene Geschwindigkeit ist üblicherweise die maximale, nicht die durchschnittliche.

Durch die mechanische Festigkeit der CD sind der Steigerung der Lesegeschwindigkeit durch Erhöhung der Umdrehungsgeschwindigkeit Grenzen gesetzt. Sogenannte „52-fach“-Laufwerke drehen die CD mit bis zu 10.000 min. Bei diesen Drehzahlen führen selbst kleinste Unwuchten der CD zu starken Vibrationen, die einerseits deutlich hörbar sind und zum anderen auf Dauer sowohl Laufwerk als auch Medium beschädigen können.

Zur Aufzeichnung der Nutzdaten auf der CD müssen diese mit einer passenden Kanalkodierung (genauer: Leitungskodierung) kodiert werden, die den Eigenheiten des Speichermediums (hier also der optischen Abtastung und der Form und Größe der Pits) Rechnung tragen muss. Bei der CD ist das die sogenannte "Eight-to-Fourteen-Modulation" (EFM). Wenn sich die CD mit der richtigen Geschwindigkeit dreht, kommen die Daten vom optischen Abtaster mit einer Datenrate von gut 4,3 Mbit/s, entsprechend einer Bitdauer von ca. 231 ns. Die EFM stellt sicher, dass sich alle drei bis elf Bitdauern die Polarität des ausgelesenen Signals ändert, dass also nach einer 1 zwei bis zehn 0 folgen. Das geschieht, wenn der Laser in der Spur einen Übergang von einer Vertiefung "(pit)" zu einem Abschnitt ohne Vertiefung "(land)" passiert oder umgekehrt.
Der Hintergrund ist folgender: Die Abschnitte mit Vertiefungen bzw. ohne Vertiefungen müssen lang genug sein, damit der Laser die Veränderung erkennen kann. Würde man ein Bitmuster direkt auf den Datenträger schreiben, würden bei einem alternierenden Signal (1010101010101010…) falsche Werte ausgelesen, da der Laser den Übergang von 1 nach 0 beziehungsweise von 0 nach 1 nicht verlässlich auslesen könnte bzw. diese Übergänge gar nicht erst in der notwendigen Feinheit in Kunststoff ‚gepresst‘ werden könnten. Somit ermöglicht die EFM die hohe Datenrate. Das klingt zunächst widersprüchlich, da sie das Signal von acht auf 14 Bit aufbläht, also rechnerisch die Datenmenge erhöht. Hinzu kommen noch weitere drei Füllbits, die so gewählt werden, dass die oben erwähnte Forderung, dass sich alle drei bis elf Bitdauern die Polarität ändert, auch zwischen den 14-Bit-Symbolen erfüllt wird. Aber durch diese Modulation kann die Datenrate so hoch gewählt werden, dass unmodulierte Daten gar nicht mehr in "pits" und "land" aufgelöst werden könnten; ein "pit" kann nicht kürzer sein als seine Breite (600 nm), trotzdem kann die Länge auch in Bruchteilen der eigenen Breite (ca. 278 nm bei 1,2 m/s) variieren – diese Tatsache wird durch die Kodierung ausgenutzt. Sie ist mithin eine Designentscheidung, die (unter anderen) für die Spieldauer verantwortlich ist. Weiterhin wird durch die Kodierung dafür gesorgt, dass das Signal der Fotodioden keinen Gleichanteil enthält; dadurch wird die Signalverarbeitung wesentlich vereinfacht.

Damit sich Kratzer und Produktionsfehler nicht negativ auf die Lesbarkeit der Daten auswirken, sind die Daten mittels Reed-Solomon-Fehlerkorrektur gesichert, so dass Bitfehler erkannt und korrigiert werden können. Weiterhin sind aufeinanderfolgende Datenbytes per Interleaving auf eine größere Länge verteilt. Der Cross Interleaved Reed-Solomon-Code (CIRC) ist dadurch in der Lage, einen Fehler von bis zu 3500 Bit (das entspricht einer Spurlänge von etwa 2,4 mm) zu korrigieren und Fehler von bis zu 12000 Bit (etwa 8,5 mm Spurlänge) bei der Audio-CD zu verdecken. Bei der Verdeckung wird der Fehler nicht korrigiert, sondern es wird versucht, ihn unhörbar zu machen, zum Beispiel über eine Interpolation. Falls der Datenträger von der Unterseite sehr stark verkratzt ist, ist er nur eingeschränkt oder nicht mehr lesbar.
Man unterscheidet zwischen C1- und C2-Fehlern. C1-Fehler geben singuläre Einzelfehler an (beispielsweise kleine Kratzer), C2 größere Blockfehler, welche von der ersten Korrekturstufe nicht mehr korrigiert werden konnten. Die Fehler vom Typ C1 können nur von wenigen Laufwerken gemeldet werden, zum Beispiel von auf Plextor oder Lite-On basierenden mit spezieller Software "(Cdrtools, Plextools, k-probe)." C2-Fehler können von den meisten Laufwerken bestimmt werden, und es gibt Software für sogenannte C2-Scans, zum Beispiel "readcd", "Nero CD-Speed" oder "CD-Doctor".

Informationen, die sich aus C1- oder C2-Fehlern ableiten lassen, geben Auskunft über den Zustand der optischen Datenträger (beeinflusst durch Alterung, Kratzer etc.), über die prinzipielle Lese- oder Brennqualität eines optischen Laufwerks in Abhängigkeit von der Geschwindigkeit (beispielsweise über eine C2-Statistik vieler Medien) und über die Qualität von frisch gebrannten Medien für eine langfristige Datenspeicherung (große C1-/C2-Werte nach dem Brennen weisen auf eine nur begrenzte langfristige Datensicherheit hin). Ein Problem ist, dass sich die Fehlerursachen nur schwer oder gar nicht trennen lassen. So kann beispielsweise ein Rohlingstyp, der mit einem spezifischen Brenner schlechte Werte erzielt, mit einem anderen Brenner-Typ trotzdem gute Ergebnisse erreichen. Außerdem lassen sich die C2-Informationen verwenden, um beim Übertragen von Audiomaterial auf einen Computer auf die Güte zu schließen, mit der Audiodaten von CD ausgelesen wurden. Dadurch können kritische Stellen ggf. erneut gelesen werden bzw. andersherum das erneute Lesen auf die kritischen Stellen eingeschränkt werden.

Bei der Interpretation für Medien gilt, dass neue CDs maximal 250 C1-Fehler pro Sekunde und keine C2-Fehler aufweisen sollten. Ein häufiges Auftreten von C2-Fehlern kann ein Indikator für eine fortschreitende Alterung des Mediums darstellen. Zur Datensicherung empfiehlt sich ein Umkopieren auf ein neues Medium. Eine solche Datensicherung sollte auch sofort nach dem Kauf von Medien erfolgen, die mit einem ‚Kopierschutz‘-Mechanismus ausgestattet sind („Un-CDs“), da diese meist absichtlich mit weit über 250 C1-Fehlern pro Sekunde produziert werden und daher schon eine geringe Menge sonst harmloser Kratzer solche Medien unlesbar machen kann. Ferner ist die Verwendung eines Reparatursprays oder von Schleif- und Poliergeräten möglich, um eine beschädigte CD oder DVD zu retten. Zufriedenstellende Ergebnisse können jedoch nicht in jedem Fall garantiert werden.

CDs gibt es in zwei verschiedenen Größen, am weitesten verbreitet ist die Version mit einem Durchmesser von 120 mm und 15 Gramm Gewicht, seltener die Mini-CD mit einem Durchmesser von 80 mm und 30 % der Speicherkapazität bei einem Gewicht von 6,7 Gramm.

Daneben gibt es auch CDs, die eine andere Form als eine runde Scheibe haben. Diese sogenannten Shape-CDs fanden aber aufgrund von Abspielproblemen (Unwucht, kein Einzug in Slot-Laufwerke) nur eine geringe Verbreitung.

Die Format-Spezifikationen der Audio-CD (kurz "CD-DA"), bekannt als „Red Book“-Standard, wurde von dem niederländischen Elektronikunternehmen Philips entworfen. Philips besitzt auch das Recht der Lizenzierung des „Compact Disc Digital Audio“-Logos. Die Musikinformationen werden in 16-Bit-Stereo (Quantisierung mit 2 = 65.536 Stufen) und einer Abtastrate von 44,1 Kilohertz gespeichert.

Die Spezifikationen der "CD-ROM" sind im „Yellow Book“-Standard festgelegt. Ein plattformübergreifendes Dateisystem der CD-ROM wurde von der ISO im Standard ISO 9660 festgeschrieben. Sein Nachfolger lautet UDF.

Es gibt verschiedene Möglichkeiten, Audio-CD-Inhalte und CD-ROM-Inhalte auf einer Scheibe zu kombinieren. Die einfachste Möglichkeit ist, einen Datentrack mit dem CD-ROM-Inhalt als ersten Track auf die CD zu bringen ("Mixed Mode CD", von einigen Herstellern auch "Enhanced CD"genannt). Dem inzwischen praktisch nichtigen Vorteil, dass der CD-ROM-Teil auch in ausschließlich Single-Session-fähigen CD-ROM-Laufwerken gelesen werden kann, steht der vergleichsweise große Nachteil der Sichtbarkeit dieses Datentracks für normale Audio-CD-Spieler entgegen, insbesondere da manche ältere CD-Spieler die CD-ROM-Daten fälschlich als Audio-Daten interpretieren. Die unbeabsichtigte Wiedergabe der Nicht-Audio-Daten führt im Ergebnis je nach Lautstärke zu ohrenbetäubendem und die Lautsprecher gefährdenden Krach.

Als Weiterentwicklung wurde der Datentrack mit einer Index-Position von 0 versehen, wodurch dieser nicht ohne Weiteres vom CD-Spieler angefahren wird ("i-Trax"). Das Audiomaterial beginnt, wie bei einfachen Audio-CDs, an Index-Position 1 von Track 1. (Problematisch für die Abspielkompatibilität könnte die Tatsache sein, dass innerhalb des Tracks der Modus von "CD-ROM Mode 1" auf "Audio" wechselt.)

Inzwischen werden zu diesem Zwecke praktisch ausschließlich Multisession-CDs benutzt – die Audio-Daten liegen in der ersten Session, während die CD-ROM-Daten in einer zweiten Session enthalten sind, die nicht von Audio-CD-Spielern gelesen wird ("CD-Extra", "CD-Plus"). Natürlich wird für den CD-ROM-Teil ein multisessionfähiges CD-ROM-Laufwerk benötigt.

Eine Mischform ist die CD+G (CD+Graphics). Diese CD stellt zeitgleich zur Musik grafische Daten, wie beispielsweise den Liedtext, auf einem Bildschirm dar. Häufigste Anwendung dieses Formats ist Karaoke. In einem normalen CD-Spieler ist die CD+G als normale Audio-CD abspielbar. Auf speziellen Geräten (in jüngerer Zeit auch auf einigen DVD-Spielern) ist zur Musik auch die Grafik auf dem Bildschirm sichtbar. Die zusätzlichen Daten sind im Subcode der CD gespeichert, d. h., sie sind im Gegensatz zum Inhalt von Datentracks nicht ohne weiteres für ein Betriebssystem sichtbar.

Deutlich häufiger anzutreffen sind dagegen CDs mit "CD-Text". Dabei werden im Subcode der CD (meistens im Lead-in) zusätzliche Informationen gespeichert, beispielsweise Titel und Künstler. Diese Informationen werden dann von geeigneten Geräten während des Abspielens der CD angezeigt.

Weitere CD-Formate sind:


Daneben gibt es noch sogenannte "HDCD"-CDs. Diese sind mit echten 20-Bit-Musik-Information kodiert (anstatt mit 16) und sollen in Verbindung mit entsprechenden CD-Playern besser klingen. HDCD-CDs sind vollständig kompatibel mit „normalen“ CD-Spielern.

Weiterentwicklungen der CD sind die "DVD-Audio" und die "Super Audio Compact Disc" (SACD). DVD-Medien bieten wesentlich größere Speicherkapazitäten von 4,7 (eine Schicht) bis 8,5 Gigabyte (zwei Schichten). Der Hauptvorteil ist dabei nicht eine längere Spielzeit, sondern dass die Audiodaten im "5.1-Soundformat" vorliegen. Während die Super-Audio-CD und DVD Audio ausschließlich für Audiodaten verwendet werden, sind bei der DVD verschiedene Datenarten möglich (DVD Data, DVD-Video, DVD-Audio, DVD-ROM, DVD+/-R(W)). Allerdings hat sich die DVD im Audiobereich nicht durchgesetzt.

Eine von Sony weiterentwickelte Variante der CD war die "Double Density Compact Disc" (DDCD). Die Speicherkapazität beträgt das Doppelte der Speicherkapazität der 640-MB-CD. Sie war in zwei Varianten erhältlich (eine beschreibbare DDCD-R und eine wiederbeschreibbare DDCD-RW), konnte sich jedoch nicht gegen die DVD durchsetzen.

Eine weitere von Sony entwickelte Variante ist die Blu-spec CD, die 2008 im Markt eingeführt wurde.

Das "CD-Videoformat" ist (im Gegensatz zur Video-CD) keine Compact Disc, sondern eine LV/LD (Bildplatte) mit analogen Videodaten und digitalen Audio-Daten.

Die CD-Standards sehen keinen Kopierschutz vor, da zur Zeit ihrer Festlegung Anfang der 1980er Jahre noch nicht absehbar war, dass in näherer Zukunft beschreibbare digitale Speichermedien mit der nötigen Datenkapazität für den Endverbraucher erschwinglich sein würden – das Kopieren wurde also einfach dadurch verhindert, dass es nichts gab, wohin die Daten realistischerweise kopiert werden konnten. Es blieb nur die qualitativ schlechtere und nicht beliebig wiederholbare Analogkopie auf Audiokassetten, die ebenso wie bei Schallplatten in Kauf genommen wurde. Das Aufkommen von CD-Brennern, hochkapazitiven Festplatten, Kompressionsverfahren wie etwa MP3 und Internet-Tauschbörsen in den 1990er Jahren hat diese Situation entscheidend geändert.

Seit dem Jahr 2001 werden in Deutschland auch Medien verkauft, die einen Kopierschutz enthalten, der das digitale Auslesen der Audiodaten (und damit das Kopieren der Daten) verhindern soll. Sie wurden zwar teils ebenfalls als "Audio-CD" bezeichnet, entsprechen aber nicht den Bestimmungen des Red Book und sind daher in diesem Sinne keine echten Audio-CDs. Diese CDs werden daher auch als „Un-CDs“ "(Nicht-CDs)" bezeichnet.

Der Kopierschutz wird realisiert, indem Fehler oder eine zweite fehlerhafte Session eingebracht werden. Auch Abweichungen vom Red-Book-Standard sind möglich, aber eher selten. Der „Abspielschutz“ ergibt sich daraus, dass die Fehler bewirken sollen, dass sich die Scheiben nicht mehr in dem CD-Laufwerk eines PC abspielen lassen. So soll das Kopieren verhindert werden. Manche CD-Laufwerke und die meisten DVD-Laufwerke lassen sich davon aber nicht beeinflussen und können die Daten trotzdem lesen, wodurch diese Idee des „Kopierschutzes“ letztendlich nutzlos wird.

Stattdessen verursachen die Fehler auf der „kopiergeschützten“ CD Probleme auf zahlreichen normalen Audio-CD-Spielern und vielen Autoradios mit integrierter CD-Einheit. Diese können diese Medien entweder gar nicht oder nur teilweise abspielen, teilweise entstehen sogar ernsthafte Hardware-Defekte, etwa wenn die Firmware des CD-Spielers abstürzt und sich das Medium nicht mehr auswerfen lässt. Außerdem leiden oft die Tonqualität und die Lebensdauer des Abspielgerätes unter dem Kopierschutz.

Seit dem 1. November 2003 sind die Hersteller in Deutschland durch § 95d UrhG gesetzlich verpflichtet, kopiergeschützte Medien als solche zu kennzeichnen. Solchen Kennzeichnungen ist jedoch kaum zu entnehmen, welche Probleme im Einzelfall mit Autoradios, MP3-CD-Spielern, DVD-Spielern und anderen Geräten auftreten können.

Da der Kopierschutz in der Praxis kaum wirksam ist, immer wieder zu Problemen beim Abspielen führt und auch eine teilweise Kaufzurückhaltung zur Folge hatte, haben ab ca. 2009 immer mehr Labels das Konzept „kopiergeschützte CD“ wieder aufgegeben. Zunehmend werden wieder gewöhnliche, ungeschützte CDs nach dem Red Book veröffentlicht, zumal sich so außerdem Lizenzgebühren für den Kopierschutz einsparen lassen.

Die meisten CDs sind auf dem Innenring der Abtastseite mit Angaben zum Hersteller, dem Produktionsland (zum Beispiel "Made in Germany by EDC", "Made in France by PDO" oder "Mastered by DADC Austria") und weiteren Kennungen (zum Beispiel Katalog-Nr., IFPI-Kennung, "Source Identification Code (SID)") versehen. Diese Identifizierungsmerkmale befinden sich i. d. R. auf einem etwa 5 mm breiten Kreis (dem Spiegel) und sind mit dem bloßen Auge nur schwer zu erkennen.

Gerade für Sammler von CDs sind solche Hinweise teilweise sehr wichtig, da man daran zum einen die legal hergestellte CD von einer Schwarzkopie unterscheiden und zum anderen „Sonderpressungen“ erkennen kann. Oft werden CDs eines Interpreten mit gleichem Inhalt in verschiedenen Ländern produziert. Die Auflagen können unterschiedlich hoch und dementsprechend wertvoll für Sammler und Fans sein.

Beschreibbare CDs gibt es in einer einmal beschreibbaren Variante (CD-R: CD recordable) und in einer mehrfach wiederbeschreibbaren Variante (CD-RW: CD rewritable). Während die Reflexionseigenschaften einer CD-R denen einer normalen CD nahezu gleichen und diese somit auch in älteren CD-Laufwerken gelesen werden können sollte, ist das Lesekopf-Ausgangssignal einer CD-RW weitaus schwächer, so dass diese Medien nur von entsprechend ausgestatteten (neueren) Laufwerken bzw. Spielern gelesen werden können.

Zum Beschreiben einer CD kann kein gewöhnlicher CD-Spieler benutzt werden. Es ist ein sogenannter ‚CD-Brenner‘ (bzw. ein CD-Rekorder) notwendig. CD-Brenner können CDs nicht nur beschreiben, sondern auch lesen. Daher sind reine CD-ROM-Lesegeräte für Computer inzwischen praktisch vom Markt verschwunden.

Das ISO-9660-Dateiformat einer CD-ROM gestattet keine nachträglichen Änderungen. Außerdem können beschreibbare CDs – im Gegensatz zu Festplatten – nicht blockweise beschrieben werden. Deshalb muss erst ein Speicherabbild angelegt werden, das eine exakte Kopie der auf die CD zu brennenden Daten enthält. Dieses Abbild kann dann (als eine Spur) in einem Durchgang auf die CD „gebrannt“ werden. Dafür sind spezielle CD-Brennprogramme nötig. Aktuelle Brennprogramme beherrschen das Erstellen des Abbildes „on-the-fly“, das heißt, das ISO-Abbild wird während des Schreibens erzeugt.

Allerdings kann man, solange die CD nicht abgeschlossen („finalisiert“) wurde, mit einem weiteren Schreibvorgang nachträglich in einem weiteren Track (das heißt normalerweise in einer weiteren Session) der CD ein neues Dateisystem erzeugen. Die Verzeichnisse dieses neuen Dateisystems können auch auf Dateien in den älteren Tracks referenzieren. Da beim normalen Betrieb immer das Dateisystem des letzten Tracks benutzt wird, ist es so möglich, Dateien hinzuzufügen, umzubenennen, zu „löschen“ und zu „überschreiben“. Natürlich kann der belegte Platz nicht erneut benutzt werden. Mit spezieller Software (zum Beispiel IsoBuster unter Windows oder ISO Master unter Linux) kann auch auf die älteren Dateisysteme zugegriffen werden, das heißt, die „gelöschten“ Dateien bzw. die älteren Versionen ‚überschriebener‘ Dateien sind damit noch erreichbar (Multisession-CD).

Alternativ können die Dateisysteme in den Tracks einer CD (analog zu Partitionen einer Festplatte) als unterschiedliche virtuelle Laufwerke betrachtet werden (Multivolume-CD). Dieses Verfahren wurde zum Beispiel beim klassischen Mac OS in den Versionen 8 und 9 eingesetzt, ist jedoch sonst kaum verbreitet.

CD-RWs können theoretisch blockweise beschrieben werden. Das muss auch vom CD-Brenner unterstützt werden. Da das auf CD-ROMs verwendete ISO-9660-Dateiformat keine nachträglichen Änderungen an Dateien unterstützt, wurde dafür ein eigenes Dateisystem namens UDF eingeführt, das auch auf DVDs verwendet wird. Dieses Format erlaubt es, wie zum Beispiel bei einer Diskette, direkt Dateien auf der CD zu speichern.

Für den Labelaufdruck bei der CD stehen, ebenso wie bei der DVD, verschiedene Drucktechniken zur Auswahl:

Die Compact Disc besteht hauptsächlich aus dem wertvollen Kunststoff Polycarbonat. Ein sortenreines Recycling lohnt sich zwar nicht für die Herstellung neuer Compact Discs, jedoch kann der sehr hochwertige Rohstoff in der Medizin, der PC- und der Autoindustrie verwendet werden. Verschiedene Firmen bieten Sammelsysteme an. Dabei werden Sammelbehälter kostenlos bereitgestellt. Sammelstellen (zum Beispiel Betriebe oder Kommunen) haben somit keinerlei Risiko, sondern müssen nur eine entsprechende Fläche für den Sammelbehälter vorhalten. Die Deutsche Telekom nimmt eigene CDs in ihren Shops zurück, AOL-CDs können unfrei an AOL gesendet werden.

Da eine CD auch vertrauliche Daten enthalten kann, muss es sichere Verfahren geben, um diese Daten vor der Entsorgung unleserlich zu machen, sei es, weil die Daten nicht mehr benötigt werden oder weil sie gelöscht werden müssen.



Die oben beschriebenen mechanischen und thermischen Verfahren sind unter gesundheitlichen Gesichtspunkten nur eingeschränkt empfehlenswert, da durch eine derartige Behandlung der Datenträger Chemikalien der Beschichtung zum Teil als sehr kleine Partikel in die Umgebungsluft abgegeben werden. Unter diesem Gesichtspunkt ist "Smart-Erase" das einzige unbedenkliche Verfahren.





</doc>
<doc id="987" url="https://de.wikipedia.org/wiki?curid=987" title="CT">
CT

CT steht als Abkürzung für:


Kfz-Kennzeichen:

Ortsbezeichnungen: 
ČT steht für:
Ct steht für:
C steht für:
ct steht für:
c. t. steht für:
c’t steht für:
Siehe auch:


</doc>
<doc id="991" url="https://de.wikipedia.org/wiki?curid=991" title="Carl Amery">
Carl Amery

Carl Amery (Pseudonym von "Christian Anton Mayer"; * 9. April 1922 in München; † 24. Mai 2005 ebenda) war ein deutscher Schriftsteller und Umweltaktivist. 

Er war Mitglied der Gruppe 47, 1976/77 Vorsitzender im Verband deutscher Schriftsteller (VS) und von 1989 bis 1991 Präsident des deutschen PEN-Zentrums. Von 1967 bis 1974 war Amery Mitglied der SPD, nachdem er zuvor der GVP angehört hatte. Später war Amery Gründungsmitglied der Partei Die Grünen beim Bundeskongress der Grünen in Karlsruhe am 13. Januar 1980 und Schirmherr der Wasserallianz München. Als Initiator und Mitbegründer war Amery von 1980 bis 1995 Präsident der E-F-Schumacher-Gesellschaft für Politische Ökologie (München).

Seine Kindheit verbrachte Carl Amery vorwiegend in Passau und Freising als Schüler des Humanistischen Gymnasiums Passau bzw. des Dom-Gymnasiums – beide Städte sollten ihre Spuren in seinem Werk hinterlassen (z. B. Passau in "Der Wettbewerb" und "Der Untergang der Stadt Passau" und Freising in "Das Geheimnis der Krypta"). Anschließend war Amery Stipendiat des Maximilianeums und studierte Neuphilologie an der Ludwig-Maximilians-Universität München, später dasselbe Fach, dazu Literaturtheorie und -kritik, an der Catholic University of America in Washington, D.C.

Im Zweiten Weltkrieg geriet er mit 21 Jahren in amerikanische Kriegsgefangenschaft und kehrte 1946 nach München zurück, wo er sein unterbrochenes Studium (Sprachen und Literaturwissenschaft) wieder aufnahm. Er begann zu schreiben, zunächst unter seinem leicht amerikanisierten Namen als "Chris Mayer", dann wählte er sich als Pseudonym "Carl Amery", wobei "Amery" ein Anagramm von "Mayer" ist.

1954 erschien sein erster Roman ("Der Wettbewerb"). Inzwischen Mitglied der Gruppe 47, begründete er 1958 durch den Roman "Die große Deutsche Tour" seinen Ruf als Satiriker. Dieser Ruf blieb lange haften.

Eine andere Seite seines Arbeitens zeigte er in der 1963 erschienenen kirchenkritischen Schrift "Die Kapitulation oder Deutscher Katholizismus heute," gefolgt von "Das Ende der Vorsehung. Die gnadenlosen Folgen des Christentums", in dem er dem Christentum Mitschuld an der globalen Umweltzerstörung zuwies. Dies prädestinierte Amery für die Rolle des Vordenkers der politischen Ökologie, die er mit weiteren Schriften wie insbesondere "Die ökologische Chance" ausfüllte und durch sein persönliches Engagement unterstrich – zunächst in der Anfangsphase bei der Partei Die Grünen, dann 1980 durch die Gründung der unabhängigen E.-F.-Schumacher-Gesellschaft, deren Vorsitzender er bis 1995 war. 

Von 1967 bis 1971 war Amery Direktor der Städtischen Bibliotheken in München – mit einer entsprechend langen Publikationspause.

1974 schließlich wandte er sich einem dritten Genre zu, dem Science-Fiction, was damals für einen der Hochliteratur zugerechneten Autor ein etwas überraschender Schritt war. Beeinflusst war er dabei nicht zuletzt durch Gilbert Keith Chesterton, dessen Science-Fiction-Romane er später in überarbeiteten deutschen Fassungen herausgab. Vor allem drei Romane Amerys gehören in diesen Bereich, nämlich 




Zwei weitere mit Fantasy-Elementen durchsetzte Romane Amerys thematisieren bayrische Spiritualität: In "Die Wallfahrer" (1986) bewegen sich die Protagonisten auf den alten Gnadenort Tuntenhausen zu; und im "Geheimnis der Krypta" bildet die Krypta im Freisinger Dom (und dort die sogenannte Bestiensäule) das Zentrum der sich über drei Generationen erstreckenden Handlung, wobei die aus den "Feuern der Leyermark" (aber auch aus dem "Königsprojekt") her bekannte Fragestellung zu einer Wissenschaft der „Sphagistik“ erhoben wird, wie die Geschichte aufgrund geringfügig veränderter Einzelbedingungen hätte ganz anders verlaufen können. 

Ab 1985 erschienen Amerys gesammelte Werke in Einzelausgaben im Münchner Paul List-Verlag. 2001 erklärte Amery in einem Interview, dass er aus gesundheitlichen Gründen keine weiteren Romane verfassen werde.

Carl Amerys Gesundheitszustand verschlechterte sich in den letzten Lebensjahren aufgrund eines Lungenemphysems bis zu seinem Tod zunehmend. Er wurde am 30. Mai 2005 am Münchner Ostfriedhof im engsten Familienkreis beigesetzt. Die Rechte an seinen Publikationen liegen bei seiner Witwe Marijane Mayer.

2007 erschien posthum der Aufsatzband "Arbeit an der Zukunft", der das Fragment einer Streitschrift gegen die US-amerikanische Religiöse Rechte enthält, an der Amery bis kurz vor seinem Tod arbeitete.


Im Jahr 2007 stiftete der "Verband deutscher Schriftsteller in Bayern" zu seinem Gedächtnis den Carl-Amery-Literaturpreis.








</doc>

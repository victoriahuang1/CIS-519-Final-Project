<doc id="3719" url="https://de.wikipedia.org/wiki?curid=3719" title="Ottawa">
Ottawa

Ottawa ( [], []) ist die Bundeshauptstadt Kanadas. Sie liegt im östlichen Teil der Provinz Ontario am Fluss Ottawa, unmittelbar an der Grenze zur Provinz Québec. Ottawa bedeutet „Händler“ in der Sprache der Algonkin, einem Volk, das zur Zeit der Besiedlung am Fluss Handel trieb. Am anderen Ufer des Flusses liegt die Zwillingsstadt Gatineau. Ottawa selbst zählt 883.391 Einwohner und ist damit die sechstgrößte Stadt Kanadas, der Großraum Ottawa-Gatineau ist mit 1.236.324 Einwohnern (Volkszählung 2011) der viertgrößte Ballungsraum Kanadas.

Die Bevölkerung ist zu 63 % englisch- und zu 15 % französischsprachig. Ottawa ist in der Region die einzige offiziell zweisprachige Stadt. In der Stadt selbst überwiegt die englische Sprache, im Gegensatz zu dem auf der anderen Seite des Ottawa-Flusses gelegenen Gatineau, in dem die französische Sprache überwiegt. Durch eine große Zahl von Einwanderern sind auch zahlreiche weitere Sprachen geläufig.

Die Wirtschaft der Hauptstadt wird hauptsächlich von zwei Sektoren getragen: zum einen durch die Arbeitsplätze der Bundesbehörden und der Bundesregierung, zum anderen von denen der Hochtechnologieindustrie. Ottawa belegt beim Bruttoinlandsprodukt und dem Nettoeinkommen der Angestellten vordere Plätze im landesweiten Vergleich und belegt den ersten Platz bei der Pro-Kopf-Zahl von Einwohnern mit akademischem Grad. (→ Bildung)

Die Stadt Ottawa liegt am südlichen Ufer des Ottawa-Flusses, an der Mündung von Rideau-Kanal und Rideau River. Von Norden her, über das Stadtgebiet Gatineaus, fließt der Rivière Gatineau in den Ottawa-Fluss. Die Stadt Ottawa befindet sich in der östlichen Ecke der Provinz Ontario und liegt etwa auf demselben Breitengrad wie Bordeaux und Venedig.
Die ältesten Teile der Stadt werden als "Lower Town" bezeichnet und befinden sich im Bereich zwischen Rideau-Kanal und Rideau River. Am gegenüberliegenden Ufer des Kanals ist "Centretown" (auch Downtown genannt), das finanzielle und kommerzielle Zentrum der Stadt. Zwischen dem Stadtzentrum und dem Ottawa-Fluss liegt Parliament Hill, das Regierungsviertel. Im Ottawa-Fluss liegen mehrere kleinere Inseln. Vor einer stauen sich die rund 60 m breiten Chaudière-Fälle mit 15 m Falltiefe. Die natürlichen Wasserfälle werden heute zusätzlich künstlich gestaut und zur Stromgewinnung genutzt. Östlich der Innenstadt stürzen am Rideau River die Rideau-Fälle über mehrere Kaskaden in den Ottawa-Fluss und bilden damit die Mündung. Das gesamte Stadtgebiet ist vergleichsweise weitläufig und umfasst mit 2779 km² einen Bereich, der größer als die Fläche des Saarlands ist.

Ottawa grenzt an folgende Bezirke "(counties)" in Ontario: im Osten an die Prescott and Russell United Counties, im Südosten an die Stormont, Dundas and Glengarry United Counties, im Süden an die Leeds and Grenville United Counties, im Südwesten an Lanark County und im Westen an Renfrew County. Im Norden grenzt es an die kreisfreie Stadt Gatineau und an die regionale Grafschaftsgemeinde Les Collines-de-l’Outaouais in Québec. Das Stadtgebiet wird südlich von einem Grüngürtel mit einer Gesamtfläche von 203,5 km² umgeben. Dieser wurde 1950 vom Stadtplaner Jacques Gréber im Rahmen eines Masterplans für die Stadt vorgeschlagen und ab 1956 von der Regierung umgesetzt.

2001 wurde Ottawa (das damals rund 350.000 Einwohner zählte) mit den zuvor selbstständigen Städten Nepean, Kanata, Gloucester, Rockcliffe Park, Vanier und Cumberland sowie den ländlichen Gemeinden West Carleton, Osgoode, Rideau und Goulbourn verschmolzen. Das gesamte heutige Stadtgebiet gehörte von 1969 bis 2001 zur Regionalgemeinde Ottawa-Carleton, die einzelne gemeindeübergreifende Infrastrukturaufgaben wahrnahm. Seit der Verschmelzung ist das Stadtgebiet Ottawas auf ein Vielfaches gewachsen, was zur Folge hat, dass 80 % der Gesamtfläche ländlich geprägt sind. Die Stadt ist in 23 Verwaltungseinheiten (engl. "wards") unterteilt. Diese wiederum werden informell in Viertel ("Neighborhoods") unterteilt.

Obwohl Ottawa und seine Zwillingsstadt Gatineau verwaltungstechnisch getrennt sind und sogar in unterschiedlichen Provinzen liegen, bilden sie zusammen mit weiteren kleineren Gemeinden einen gemeinsamen Ballungsraum mit über 1,4 Millionen Einwohnern. Die offizielle Bezeichnung dieses seit 1959 bestehenden und 4715 km² großen Ballungsraums lautet National Capital Region (frz. "Région de la capitale nationale"). Im Gegensatz zu anderen Flächenstaaten gibt es in Kanada keinen eigentlichen Bundesregierungsbezirk, der Status Ottawas entspricht jenem anderer kreisfreier Städte in der Provinz Ontario. Die Bundesregierung übt jedoch über die National Capital Commission, die die Gebäude und weitläufigen Grundstücke im Besitz des Bundes verwaltet, indirekt Einfluss auf die städtebauliche Entwicklung in der National Capital Region aus.

Die meisten tektonischen Bewegungen sind im Westen Kanadas auszumachen. Dennoch sind auch im östlichen Teil und damit in der Region um Ottawa leichte bis mittlere Erdbeben zu verzeichnen. Kanada liegt auf der vergleichsweise stabilen Nordamerikanischen Platte und damit hat der Osten – verglichen mit anderen Teilen der Erde – relativ geringe seismische Aktivitäten. Jedes Jahr werden ungefähr 450 Erdbeben in Ostkanada registriert, davon übertreffen etwa vier die Magnitude 4 auf der Richterskala.

Ostkanada wird in weitere Erdbebenzonen unterteilt. Ottawa selbst befindet sich in der ausgedehnten Zone "Western Quebec Seismic Zone", die das gesamte Ottawatal von Montreal bis Temiscaming, eingeschlossen die Stadtregionen Montreal, Ottawa-Hull und Cornwall, umfasst. Die Muster der bisherigen Beben zeigen eine Konzentration der Aktivitäten vor allem auf den beiden Gebieten entlang des Ottawa-Flusses und auf der Achse Montreal-Maniwaki.

Am 16. September 1732 ereignete sich in Montreal ein Beben mit einer geschätzten Stärke von 5,8 auf der Richterskala. Dieses richtete beträchtliche Schäden an. Am 1. November 1935 gab es mit einer Stärke von 6,2 das bis heute stärkste Beben, dessen Epizentrum bei Timiskaming in der Provinz Quebec nordwestlich von Ottawa lag. Am 5. September 1944 richtete ein Beben der Stärke 5,6 mit Epizentrum bei Cornwall, einer Kleinstadt zwischen Ottawa und Montreal, etwa 2 Millionen Dollar Schaden an und ließ über 2000 Schornsteine zusammenfallen. Zwischen 1980 und 2000 erreichten in Ottawa 16 Beben Stärken über der Magnitude 4.

Das Klima in Ottawa ist ein feuchtes Kontinentalklima mit einer starken Bandbreite und Rekordtemperaturen (Effektive Klimaklassifikation: Dfb). Am 4. Juli 1913 wurde eine Höchsttemperatur von 37,8 °C gemessen, dagegen am 29. Dezember 1933 mit –38,9 °C die tiefste je gemessene Temperatur. Bezogen auf diesen Tiefsttemperaturrekord gehört Ottawa nach Ulaanbaatar, Astana und Moskau zu den vier kältesten Hauptstädten der Welt. Diese enormen Temperaturunterschiede erlauben es der Stadt, eine Vielzahl an jährlich wiederkehrenden Veranstaltungen abzuhalten, wie beispielsweise das "Winterlude Festival" auf dem zugefrorenen Rideau-Kanal. Durch die relativ warmen Sommer gehört Ottawa allerdings im Jahresdurchschnitt betrachtet weltweit nur zu den sieben kältesten Hauptstädten.
Während der Wintermonate dominieren Schnee und Eis. Jedes Jahr fällt in der Summe etwa 235 cm Schnee. Die größte je gemessene Schneemenge an einem Tag war 73 cm am 2. März 1947. Die durchschnittliche Temperatur im Januar beträgt –10,8 °C mit starken Schwankungen zwischen den Tag- und Nachttemperaturen. Während die Tagestemperaturen leicht über der Null-Grad-Grenze liegen können, fallen die Nachttemperaturen hin und wieder auf unter –30 °C. In einem durchschnittlichen Winter liegt die Hauptstadt von Mitte Dezember bis Anfang April unter einer geschlossenen Schneedecke, obwohl es auch schneefreie Tage um die Weihnachtszeit geben kann. Der Winter 2007/08 war dabei mit insgesamt 432,7 cm Schnee besonders ergiebig, die Schneehöhe lag damit nur knapp unter der des Rekordjahres 1970/71 von 444,1 cm. Hohe Windchills sind dabei ebenso üblich wie überfrierender Regen. Einer dieser Eisstürme verursachte im Januar 1998 sogar Stromausfälle und beeinträchtigte die lokale Wirtschaft erheblich.

Die Sommer sind verhältnismäßig feucht und warm, dauern allerdings nur relativ kurz. Die durchschnittliche Maximaltemperatur im Juli beträgt 26 °C mit gelegentlichen Einfällen von kalter Luft mit fallender Luftfeuchtigkeit aus dem Norden des Landes. Frühling und Herbst fallen aufgrund der Extreme dementsprechend wechselhaft aus. Heiße Tage mit über 30 °C können bereits Anfang März vorkommen, aber auch noch Ende Oktober. Der durchschnittliche Niederschlag beträgt rund 914 mm. Die größte Regenmenge innerhalb eines Tages wurde am 9. September 2004 mit 136 mm gemessen. In Ottawa scheint die Sonne etwa 2060 Stunden im Jahr, was 47 % der möglichen Sonnenstunden entspricht. Im Sommer kann es auch zu Tornados kommen.

Die Erforschung der Frühgeschichte im Gebiet der kanadischen Hauptstadt begann erst sehr spät, obwohl sich bereits 1843 ein unbekannter Autor mit einer Begräbnisstätte von 20 Indianern in Ottawa befasste, 1853 Edward Van Cortlandt mit einer (anderen?) Begräbnisstätte. Doch kam die Forschung, sieht man von den Arbeiten Thomas Walter Edwin Sowters um 1900 ab, mehr als ein Jahrhundert lang zum Erliegen. Auslöser für knappe Erörterungen war der Fund einer Portage und eines Indianerlagers genau dort, wo sich heute das Kanadas Nationalmuseum für Geschichte und Gesellschaft befindet. Erst 2002, anlässlich des Baues des neuen Kriegsmuseums, kam es zur intensiven Erforschung der durch die Bauarbeiten gefährdeten Stätten.

Die ältesten menschlichen Spuren reichen rund 6500 Jahre zurück und finden sich am Leamy Lake, vor allem aber im Tal des Ottawa. Die Ottawa, denen die Hauptstadt ihren Namen verdankt, kamen im 14. Jahrhundert von Osten zu den Großen Seen. Sie siedelten jedoch nur bis 1651 in der Region.

Étienne Brûlé befuhr 1610 als erster Europäer den Fluss Ottawa, Samuel de Champlain traf 1613 mit dem Ottawa-Häuptling Tessouat nahe der späteren Stadt Ottawa zusammen. Er nannte die Bewohner „Oudaouais“. Sie siedelten im Winter in Gruppen von zwei oder drei Familien und fanden sich im Sommer zu großen Jagdverbänden zusammen. Von ihnen übernahmen die Franzosen die Schneeschuhe. An den Chaudière-Fällen opferten sie Tabak, wie Champlain berichtet. 1620 schickte er Jean Nicolet zu den Kichesipirini, die den Fluss „Kichesippi“ (Großer Fluss) nannten. Um diese Zeit war es den Ottawa gelungen, ein Handelsmonopol entlang des Flusses zu errichten. Mit ihren Kanus transportierten sie Pelze zu den Dörfern der Wyandot oder Huronen, wo sie von den Franzosen in Empfang genommen wurden. In der Gegenrichtung transportierten sie französische Handelswaren zu den weiter entfernten Stämmen. Um 1630 begann mit den Biberkriegen ein langwieriger Kampf um den Pelzhandel mit den Irokesen, der weiträumige Völkerwanderungen in Bewegung setzte. 1636 versuchten die Kichesipirini vergeblich, eine Koalition mit Huronen, Algonkin und Nipissing gegen die Irokesen zusammenzubringen, die um 1650 die Huronen und später weitere Stämme vernichteten. Erst um 1700 kehrten einige der Gruppen zurück, doch blieben die Ottawa überwiegend südlich der Großen Seen und damit auf dem Gebiet der späteren USA ansässig. Dennoch hatten sich die französischen Pelzhändler angewöhnt, jeden Indianer aus der Region der späteren Hauptstadt als „Ottawa“ zu bezeichnen, auch wenn es ein Algonkin oder Ojibway war. So wurde aus dem „Grande Rivière des Algoumequins“ genannten Fluss bald fälschlicherweise die „Grande Riviere des Outaouais“.
Um Ottawa, etwa auf der Isle-aux-Alumettes, siedelten, abgesehen von den rund zwei Jahrzehnten, in denen die Ottawa ihr Handelsmonopol aufbauten, Algonkin-Gruppen. Champlain nannte eine dieser Gruppen „Algoumequins“. Deren Sprache war eine weit verbreitete Händlersprache, so dass diese Bezeichnung bald auf alle Stämme dieser Sprachfamilie übertragen wurde. Nördlich von Ottawa (bei Maniwaki) leben heute die Kitigan Zibi Anishinabeg, westlich, am Golden Lake, siedelt die Algonquins of Pikwàkanagàn First Nation.

Im Jahre 1759 kam das Gebiet unter britische Herrschaft. 1800 gelangte mit Philemon Wright aus Massachusetts eine erste Siedlergruppe von fünf Familien und 33 Arbeitern an die Chaudière-Fälle, die Wright „Columbia Falls“ nannte. Aus der Siedlung "Wright’s Town" entstand das heutige Gatineau. 1806 fuhr ein erstes Floß aus 700 Holzstämmen den Fluss abwärts bis Québec, doch erst die Kontinentalsperre Napoleons mit ihren hohen Preisen machte aus diesen Fahrten ein lohnendes Geschäft. Auch lieferte Wright, der den Ort dominierte, ab 1812 Weizen an die USA. Bis 1830 wurde Wright’s Town, insbesondere sein 1814 gegründetes Unternehmen "P. Wright & Sons", zur wichtigsten Holzunternehmung in Kanada, doch zugleich verhinderte Wright drei Jahrzehnte lang aus Furcht vor Konkurrenz jede Industrieansiedlung.

Den Pelzhandel dominierte, nachdem er lange von unabhängigen Jägern und Händlern betrieben worden war, die in Montreal ansässige North West Company. Sie wurde allerdings 1821 mit der Hudson’s Bay Company zwangsweise vereinigt, jedoch war die Pelztierjagd im Ottawatal inzwischen nur noch von geringer Bedeutung.

Der Name "Bytown" wurde 1827 zum ersten Mal verwendet, um die Siedlung rund um die Baustelle des Rideau-Kanals zu benennen. Dessen Bezeichnung geht auf Colonel John By zurück, der von 1826 bis 1832 den Bau des Verbindungskanals zwischen dem Ottawa und dem Rideau River leitete.

James Johnston gründete 1836 die erste Zeitung des Ortes, "Bytown Independent". 1839 zählte die Siedlung 2073 Einwohner. Nach einigen Kontroversen erlangte Bytown 1850 das Stadtrecht. Den Namen behielt die Stadt bis 1854; ab 1. Januar 1855 hieß die Stadt offiziell Ottawa. Die erste bedeutende Industrie war die Holzindustrie, die den Ottawa zum Transport riesiger Flöße nutzte. An den Chaudière- und Rideau-Fällen entstanden Sägewerke, J. R. Booth war der erfolgreichste dieser "lumber barons" (Holzbarone). 1855 hatte Ottawa bereits rund 10.000 Einwohner. Der Rideau-Kanal brachte Holz nach Kingston und über den Eriesee nach Oswego, eine Öffnung der kanadischen Wälder Richtung USA, die durch die Eisenbahnbauten noch ausgebaut wurde. Ähnlich wie im Pelzhandel verdrängten Großunternehmen bald die Familienbetriebe.

Am 31. Dezember 1857 wurde Königin Victoria aufgefordert, eine Hauptstadt für die Provinz Kanada auszuwählen. (→ Geschichte Kanadas) Sie entschied sich für Ottawa. Es ranken sich mehrere Legenden um die Auswahl. So soll die Königin ihre Hutnadel auf einer Landkarte etwa zur Hälfte zwischen die Städte Toronto und Montreal gesteckt haben; der nächste Ort sei Ottawa gewesen. Tatsächlich dürfte die Wahl auf die Stadt gefallen sein, weil Ottawa zum einen an der Sprachgrenze lag und damit für beide europäischen Bevölkerungsteile (in Kanada spricht man heute von Euro-Canadians) akzeptabel schien, andererseits, im Gegensatz zum nahe an der Grenze zu den USA gelegenen Toronto, das vom Ontariosee aus – im erneuten Kriegsfall mit den Vereinigten Staaten – womöglich leicht angreifbar gewesen wäre, lag Ottawa weit im Hinterland. Der Barracks Hill, wo By seine Wachmannschaften untergebracht hatte, sollte zum Sitz des Parlaments werden. Der Bau des Gebäudes begann 1860, er sollte bis 1866 rund 4,5 Millionen Dollar verschlingen. Allerdings brachte er Bauarbeiter und Architekten, dann zahlreiche Verwaltungsbeamte und Angehörige des Parlaments nebst ihren Familien nach Ottawa. Beim Baustil bevorzugte man die damals in Großbritannien beliebte Neugotik.

Das Verfassungsgesetz von 1867 erhob Ottawa zur Hauptstadt des neuen kanadischen Bundesstaates. 1877 wurde der Öffentlichkeit hier das erste Telefon präsentiert. 1899 entstand die "Ottawa Improvement Commission", die für eine Verschönerung der Stadt sorgen sollte. Am 26. April 1900 zerstörte jedoch ein Feuer rund 2000 Gebäude. Ein defekter Schornstein in Hull, dem alten Stadtkern von Gatineau, fing Feuer und breitete sich durch die Witterungsverhältnisse bis nach Ottawa aus. Sieben Menschen kamen damals unmittelbar durchs Feuer um, weitere Tote gab es durch die nachfolgenden Seuchen, 15.000 wurden obdachlos. Am 3. Februar 1916 zerstörte ein weiterer großer Brand das Parlaments- und Senatsgebäude. Das neue Parlamentsgebäude wurde 1922 fertiggestellt.

Im Jahre 1927 ersetzte die "Federal District Commission" die 1899 eingesetzte Kommission, die zudem für den Erhalt der ausgedehnten Wälder im Gatineau-Park sorgte.

Kurz nach Ende des Zweiten Weltkrieges rückte die Stadt in die weltweite öffentliche Wahrnehmung, als am 5. September 1945 der sowjetische Kryptograph Igor Gouzenko aus der Botschaft in Ottawa zu den Westmächten überlief. Er stahl dabei 109 Geheimakten über die Entwicklung von Nuklearwaffen.
Auf dem Höhepunkt des Kalten Krieges entstand zum Schutz von Regierung, Verwaltung und der Archivalien der Diefenbunker in Carp, einem Dorf westlich von Ottawa. Er kann heute besichtigt werden. Der Bunker sollte im Falle eines Atombombenangriffs, selbst bei völliger Zerstörung der Stadt, das weitere Funktionieren der Regierungstätigkeiten sicherstellen.

Da Ottawa städtebaulich vergleichsweise schlecht entwickelt war, wurden nach der Gründung der National Capital Commission im Jahr 1959 unter Leitung von Jacques Gréber Industrie- und Eisenbahnanlagen aus dem Innenstadtbereich verbannt, Grünanlagen geschaffen und das Kulturleben gefördert. Dazu gehörte auch die Zentralisierung zahlreicher Artefakte der kanadischen Geschichte in einem zentralen Museum. Durch den "National Capital Act" war bereits 1958 ein Hauptstadtbezirk von 4800 km² Fläche entstanden, zu dem 27 Orte (municipalities) gehörten, vor allem aber Ottawa und Hull.

Die Stadtregierung (engl.: "Ottawa City Council", franz.: "Conseil municipal d’Ottawa") ist das Verwaltungsgremium der Stadt und setzt sich aus 23 Stadträten und einem Bürgermeister zusammen. Die Verwaltungsstruktur ist einstufig und trägt für sämtliche Bereiche der städtischen Dienstleistungen die Verantwortung. Der Bürgermeister vertritt die Interessen und Belange der gesamten Stadt, während die Stadträte Repräsentanten ("City Councillors") der 23 Verwaltungseinheiten ("wards") sind. Sie werden für eine vierjährige Legislaturperiode gewählt. Die letzten Wahlen fanden am 13. November 2006 statt. Bürgermeister war seit dem 1. Dezember 2006 Larry O’Brien (Konservative Partei Kanadas), dem am 1. Dezember 2010 Jim Watson folgte. Er wurde mit 47,08 % der Stimmen gewählt und löste damit Bob Chiarelli ab. Sitz der Stadtregierung ist das Rathaus, welches sich südlich des Confederation Park befindet.

Die Stadtregierung trifft sich regelmäßig zweimal monatlich im Rathaus, jeweils am zweiten und vierten Mittwoch im Monat. Im Bedarfsfall werden auch Sondersitzungen eingelegt. Dem Stadtrat ist ein ständiges Komitee ("Standing Committee") beigeordnet. Diesem Komitee können Bürger ihre Anliegen vortragen, das wiederum das Anliegen prüft und in kurzen Vorträgen dem Stadtrat präsentiert. Außerdem gibt es 16 Beratungskomitees, welche zu verschiedenen Themenbereichen fachlich dem Stadtrat zur Seite stehen und Empfehlungen aussprechen. Einige Mitglieder des Komitees bestehen aus ehrenamtlichen Mitarbeitern.

Das Wappen der Stadt Ottawa wurde am 20. Oktober 1954 durch Vincent Massey vorgestellt. Es zeigt einen weißen Schild, der kreuzförmig durch zwei wellenförmige, blaue Linien unterbrochen wird. Sie stellen die beiden Flüsse Rideau River und Ottawa River dar, die durch die Stadt fließen. An der heraldisch rechten oberen Ecke ist die Königskrone von Königin Victoria abgebildet, die Ottawa zur Hauptstadt erhoben hat. An der heraldisch linken unteren Ecke befindet sich ein rotes Ahornblatt. Der obere Teil des Schildes zeigt auf rotem Grund von rechts nach links Pfeile, ein Astrolabium und Schaufeln. Die Pfeile symbolisieren die ersten Einwohner, während das Astrolabium für die Entdeckung durch Samuel de Champlain steht. Die Schaufeln stehen für John By, der den Rideau-Kanal erbaute. Rechts und links vom Schild stehen ein Holzfäller und ein Offizier. Beide stehen auf einem grünen Untergrund aus Gras und Kiefernzapfen. Darunter befindet sich ein Banner mit dem Stadtmotto „Advance – Ottawa – en avant“. Oberhalb des Schildes befindet sich ein silberner Helm. Darauf thront eine Kiefer, die historisch betrachtet die wirtschaftliche Basis für das Ottawatal und später die Stadt Ottawa bildete.

Die Flagge stellt ein stilisiertes weißes O dar, den Anfangsbuchstaben der Stadt. Die dynamischen Linienführungen symbolisieren außerdem den "Maple Leaf", das Nationalsymbol Kanadas und den Peace Tower des Parlamentsgebäudes. Die Hintergrundfarben sind blau und ein helles grün. Blau steht dabei für die Wasserwege durch Ottawa, die grüne Farbe für die Wälder, Bäume und Parklandschaften. Die Flagge besteht seit dem 1. Januar 2000 mit der Verschmelzung der Stadt mit ihren Nachbargemeinden.

Von 1987 bis 2000 war die Stadtflagge eine dreifarbige Flagge mit vertikalen Balken und dem Stadtwappen in seiner Mitte. Die Trikolore repräsentierte die Monarchie (purpur), die Liberalen (rot) und die Konservativen (blau). Von 1901 bis 1987 verwendete man die gleiche Flagge ohne Wappen.

Städtepartnerschaften bestehen zu

Seit dem 10. Januar 1997 besteht außerdem zwischen Ottawa und Seoul, Südkorea ein Memorandum of Understanding über zukünftige Zusammenarbeit.

Die Bevölkerung der Stadt wuchs in den vergangenen Jahrzehnten stetig an. Die nebenstehende Tabelle stellt die Einwohnerentwicklung innerhalb der jeweils gültigen Stadtgrenzen dar. Vergrößerungen der Stadtgrenzen fanden seit Gründung von Bytown insgesamt zwölfmal statt, vor allem in den 1940er und 1950er Jahren. Im Jahr 2001 erfolgte die größte und bisher letzte Erweiterung des Stadtgebietes. Die Metropolregion Ottawa-Gatineau verzeichnete in der Volkszählung 2011 1.236.324 Einwohner. Innerhalb der ursprünglichen Grenzen vor der Verschmelzung mit anderen Stadtteilen hatte Ottawa lediglich 337.031 Einwohner, ihre Zahl ging 2006 sogar auf 328.105 zurück. Nach einer Prognose soll die Stadtbevölkerung weiter wachsen und etwa im Jahr 2021 die Millionengrenze überschreiten. Im Jahr 2011 hatte Ottawa 883.391 Einwohner.

Im Jahr 2001 betrug der Frauenanteil rund 51,23 %, Jugendliche unter 14 Jahren waren mit 19,30 % an der Gesamtbevölkerung vertreten, der Bevölkerungsanteil der über 65-Jährigen betrug 10,81 %. Das Durchschnittsalter lag bei 36,6 Jahren. Der Ausländeranteil betrug 2006 22,28 %. Der Anteil der „sichtbaren Minderheiten“ lag bei 20,2 %, während der Anteil der Ureinwohner 1,5 % von der Gesamtbevölkerung ausmacht. Die größte sichtbare Minderheit wird von den Schwarzen mit 4,9 % gestellt, gefolgt von den Chinesen mit 3,8 %, Südasiaten mit 3,3 % und Arabern mit 3,0 %.

Ottawa ist offiziell englisch- und französischsprachig. Die gesamte Verwaltung ist auf die Zweisprachigkeit ausgerichtet. Die Bevölkerung ist überwiegend englischsprachig (62,6 %); etwa 14,9 % sind französischsprachig (→ Franko-Ontarier). Nur ein geringer Bevölkerungsanteil (etwa 0,85 %) ist mit zwei Muttersprachen aufgewachsen. Ein Anteil von 59,87 % hat nur Kenntnisse der englischen Sprache, 1,62 % beherrschen nur Französisch. Die französischsprachigen Einwohner leben vor allem in den nordöstlichen Bezirken entlang des Ottawa River. Etwa 37 % der Bevölkerung sprechen beide Sprachen, rund 1,3 % beherrschen keine der beiden Sprachen. Durch die Einwanderer sind viele andere Sprachen wie Arabisch, Chinesisch, Italienisch, Deutsch, Spanisch oder Persisch vertreten.

Nach der Volkszählung des Jahres 2001 gehörten 79,34 % der Bevölkerung einer christlichen Konfession an. Davon gehört mit 54,16 % die größte Gruppe der römisch-katholischen Kirche an, 21,85 % sind Protestanten, 1,68 % gehören der orthodoxen Kirche an. Die verbleibenden sind Angehörige der freien christlichen Kirchen, der Zeugen Jehovas oder der Kirche Jesu Christi der Heiligen der Letzten Tage. Die größte nichtchristliche Gruppe bilden die Angehörigen des Islam mit 3,97 %. Danach folgen Angehörige des Judentums (1,09 %) und des Buddhismus (0,95 %). Der Anteil der Konfessionslosen beträgt 13,29 %.

In Ottawa befinden sich die wichtigsten politischen Institutionen des Landes. Das kanadische Parlament mit Senat und Unterhaus sowie der Generalgouverneur von Kanada haben hier ebenso ihren Sitz wie der Oberste Gerichtshof Kanadas. An der Adresse 24 Sussex Drive befindet sich seit 1949 der offizielle Amtssitz des kanadischen Premierministers. Das eher bescheidene Haus wurde zwischen 1866 und 1868 von Joseph Merrill Currier (1820–1884) erbaut. Der Landsitz des Premierministers befindet sich außerhalb der Stadt am Harrington-See im Gatineau-Park, rund 33 km nordwestlich des Regierungsviertels. Die Residenz des Staatsoberhauptes (der Monarch bzw. der Generalgouverneur als Stellvertretung) residiert seit dem 1. Juli 1867 in der Rideau Hall (1 Sussex Drive). Das repräsentative Gebäude wird vor allem für Staatsempfänge genutzt. Lediglich 500 m² der insgesamt 9500 m² sind als Wohnung konzipiert. Auch der Oppositionsführer des Unterhauses und dessen Vorsitzender ("speaker") haben offizielle Residenzen im Hauptstadtbezirk. Das offizielle Gästehaus für Staatsbesuche ist ein viktorianisches Gebäude in der Nähe der Rideau Hall.

Außerdem haben 126 Auslandsvertretungen und Hochkommissare ihren Sitz in der Hauptstadt. Die meisten Botschaften und Residenzen befinden sich im Diplomatenviertel Rockcliffe nordöstlich des Regierungsviertels, eine weitere Ansammlung von diplomatischen Missionen befindet sich in der südlichen Innenstadt. Die Deutsche Botschaft in Ottawa besteht, nach Unterbrechung durch den Zweiten Weltkrieg, wieder seit dem 13. Februar 1951.

Die Zentralbank "Bank of Canada" wurde 1935 in Ottawa gegründet und befindet sich an der Ecke Wellington Street zu Bank Street in der Downtown.

In Ottawa befindet sich seit 1971 die nationale Statistikbehörde Statistics Canada (franz.: "Statistique Canada"). Diese Organisation ging aus dem 1918 gegründeten "Dominion Bureau of Statistics" hervor. Die Behörde mit rund 1700 Mitarbeitern hat ihren Sitz westlich der Downtown.

Außerdem befinden sich in der Stadt folgende Ministerien und Bundesbehörden: Agriculture and Agri-Food Canada, Citizenship and Immigration Canada, Department of Finance Canada, Fisheries and Oceans Canada, Global Affairs Canada, Health Canada, Industry Canada, Department of Justice, Department of National Defence, Natural Resources Canada, Public Safety Canada, Transport Canada, Veterans Affairs Canada sowie Canada Border Services Agency, Nav Canada, Royal Canadian Mounted Police, Communications Security Establishment Canada und der kanadische Kronrat. 

In Gatineau sind u.a. folgende Ministerien und Bundesbehörden untergebracht: Employment and Social Development Canada, Environment and Climate Change Canada, Public Services and Procurement Canada, Indigenous and Northern Affairs Canada, Transportation Safety Board of Canada sowie Canadian International Development Agency.

Die Angestellten arbeiten in der Stadt vornehmlich entweder für die kanadischen Bundesbehörden und die Bundesregierung oder in der Hochtechnologieindustrie. In Ottawa sind unter anderem die Unternehmen 3M, General Dynamics Canada, Adobe Systems, Bell Canada, IBM Canada, MacDonald Dettwiler, Telesat Canada, Neptec, Corel Corporation und Hewlett-Packard niedergelassen. Die Konzentration an Unternehmen dieser Branche brachte der Stadt auch den Spitznamen „Silicon Valley des Nordens“ ein. Die Firma Royal Canadian Mint prägt hier an ihrem denkmalgeschützten Hauptsitz exklusiv die Anlage- und Gedenkmünzen des Kanadischen Dollars.

Nach einer Erhebung aus dem Jahr 2006 gibt es in Ottawa insgesamt 522.000 Arbeitsstellen. Zwischen 2001 und 2006 entstanden rund 40.000 neue Arbeitsstellen. Trotz der Steigerung ist das durchschnittliche Wachstum auf einen Fünf-Jahres-Zeitraum bezogen langsamer als Ende der 1990er Jahre. Während die Zahl der Beschäftigten im Bereich der Bundespolitik stagnierte, wuchs sie in der Branche der Hochtechnologie mit 2,4 % stärker an. Das gesamte Wachstum der Arbeitsplätze in Ottawa-Gatineau betrug 1,3 % im Vergleich zum Vorjahr und lag damit auf Platz 6 hinter Edmonton (6,7 %), Calgary (3,9 %), Vancouver (3,0 %), Montreal (2,5 %) und Toronto (2,3 %). Die Arbeitslosenquote in Ottawa-Gatineau betrug 5,2 % (nur Ottawa: 5,1 %) und lag damit unter dem nationalen Durchschnitt von 6,0 %.

Die Zunahme des Bruttoinlandsprodukt betrug in Kanada 2007 2,4 %; Ottawa war mit 2,7 % auf Platz 4 der großen Städte. Die Region Ottawa-Gatineau hat die dritthöchsten Einkommen aller großen Städte Kanadas. Das durchschnittliche Bruttoeinkommen in der Region lag bei 40.078 Dollar (nach Calgary mit 52.927 Dollar und Edmonton mit 42.866 Dollar) und stieg damit um 4,9 % im Vergleich zum Vorjahr. Das Nettoeinkommen lag bei 30.347 Dollar (Zuwachs um 4,4 % zum Vorjahr) und bedeutet für die Region ebenfalls Platz 3. Der Lebenshaltungskostenindex wurde 2007 mit 110,7 gemessen, was einer Jahresteuerungsrate von 1,9 % entsprach.

Die Weltwirtschaftskrise bewirkte einen Anstieg der Arbeitslosenquote zwischen April 2008 und April 2009 von 4,7 auf 6,3 %. In der Provinz stieg diese Quote allerdings im gleichen Zeitraum von 6,4 auf 9,1 %.

In der kanadischen Hauptstadt erscheinen eine Reihe von Tageszeitungen. Der 1845 gegründete "Ottawa Citizen" ist eine englischsprachige, konservative Tageszeitung mit einer Auflage von rund 140.000 Exemplaren. Die Zeitung führt den Peace Tower in ihrem Logo. Wie in auch in anderen kanadischen Großstädten erscheint die kostenlose Pendlerzeitung 24 Hours. Die "Ottawa Sun" gilt als konservativ-populistisch und ist die Schwesterzeitung der "Toronto Sun". Mit rund 35.000 Exemplaren erscheint täglich die französischsprachige "Le Droit". Die 1913 gegründete Zeitung ist gleichzeitig die einzige frankophone Tageszeitung in der Provinz Ontario, wird aber auch im angrenzenden Gatineau gelesen.

Obwohl Ottawa die Hauptstadt Kanadas ist, sind die meisten – auch örtlichen – Fernsehsender im Süden Ontarios, vor allem in Toronto beheimatet. Wegen der Zweisprachigkeit der Stadt hat sie terrestrische Empfangsmöglichkeiten für die meisten englisch- und französischsprachigen Sender des Landes. In der Hauptstadt sind unter anderen das 1947 gegründete Talkradio "580 CFRA", der Sportradiosender "The Team 1200", Oldie-Radiosender "Oldies 1310", Hot 89.9FM und Live 88.5FM beheimatet.

Ottawa ist durch zwei Bahnhöfe an das Eisenbahnnetz von VIA Rail angeschlossen. Der Bahnhof Ottawa ist Ausgangs- und Endpunkt der Linien nach Toronto und Montreal. Der rund zehn Kilometer südlich des Zentrums gelegene Ottawa Macdonald-Cartier International Airport wird von einer Vielzahl von Fluggesellschaften angeflogen. In der Region der Stadt befinden sich noch zehn weitere kleinere Flugplätze. Ottawa ist mittels Langstreckenbusverbindungen mehrerer Gesellschaften wie zum Beispiel Greyhound Canada mit anderen kanadischen Städten verbunden.
In Ottawa werden Buslinien von OC Transpo unterhalten. Bemerkenswert ist der Transitway. Es handelt sich dabei um ein Busstraßensystem, das weitgehend kreuzungsfrei geführt wird und Haltestellenanlagen besitzt, die an U-Bahnstationen erinnern. In der Innenstadt verlassen die Busse den Transitway und verkehren auf dem normalen Straßennetz. Außerdem existiert als Pilotprojekt der O-Train, eine dieselbetriebene Stadtbahn.

Ottawa ist sternförmig durch ein Netzwerk mehrerer Autobahnen erschlossen. Der Highway 417 ("The Queensway") verläuft als Teil des Trans-Canada Highway von Westen nach Osten in Richtung Montreal. Nach Süden führt der Highway 416 ("Veterans Memorial Highway") bis nach Prescott und bindet die Hauptstadt damit an die restlichen 400er-Autobahnen und das Goldene Hufeisen an.

Der Highway 7 führt über 537 km von London im Südwesten Ontarios über Kitchener und Guelph nach Ottawa. Mit ihrer Schwesterstadt Gatineau ist die Hauptstadt über fünf Straßenbrücken und eine Eisenbahnbrücke über dem Ottawa-Fluss verbunden.

Eine große Anzahl von Straßen werden als sogenannte "Scenic Parkways" von der National Capital Commission verwaltet, sie sollen hauptsächlich dem Freizeitverkehr dienen und sind daher für LKW gesperrt. An den Sommerwochenenden werden einige Scenic Parkways für den motorisierten Verkehr gesperrt und dienen Fußgängern, Rollschuhfahrern sowie dem Radverkehr.

Ottawa ist von einem großen Netz an Fußgänger- und Radwegen durchzogen. Die Wege meiden verkehrsreiche Autostraßen, sind mit weiten Bordsteinen ausgestaltet und erstrecken sich auch entlang der Flüsse. Sie bieten damit gute Möglichkeiten für Radwanderungen oder Spaziergänge.

Nach statistischen Erhebungen nutzten im Jahr 2005 – gemessen wurde an einem verkehrsreichen Morgen – rund 62 % der Bürger das Automobil, 21 % die öffentlichen Verkehrsmittel, 9 % liefen zu Fuß, 2 % nutzten das Fahrrad und 6 % nutzten andere Fortbewegungsmittel. Für die Zukunft soll aufgrund der wachsenden Bevölkerungszahl der Schwerpunkt auf den öffentlichen Nahverkehr gelegt und eine Auslastung von 30 % angestrebt werden. Der Pendlerverkehr von Gatineau nach Ottawa betrug 2005 insgesamt 43.200 was 31 % des Gesamtverkehrs entspricht. Umgekehrt gab es von Ottawa nach Gatineau nur 17.200 Pendler, was einer Quote von 4 % entspricht. Der Pendleraustausch zwischen dem ländlichen Stadtgebiet und der Downtown fällt im Verhältnis von 67 % stadteinwärts zu 2 % stadtauswärts noch drastischer aus.

Die Stadt verfügt über zwei staatliche Universitäten, die "Universität Ottawa" und die "Carleton University". Darüber hinaus gibt es mehrere Hochschulen. Das 1967 gegründete "Algonquin College" ist eine Hochschule für angewandte Kunst und Technologie. Das "Dominican University College" bietet mehrere Fakultäten der Philosophie und Theologie. Die kleine Hochschule hat rund 160 Studenten und wurde im Jahr 1900 gegründet. Die 1848 gegründete "Saint Paul University" ist eine päpstlich-katholische Universität mit etwa 1000 Studenten. Sie beherbergt Fakultäten für Kirchenrechts, Humanwissenschaften, Philosophie und Theologie. Die meisten Hochschulen und Universitäten lehren englisch und französisch. Das "La Cité collégiale" mit etwa 5.000 Studenten ist eine französischsprachige Hochschule, die als Ableger des Algonquin College 1990 gegründet wurde.

Das Ottawa-Carleton District School Board (OCDSB), "Conseil des écoles publiques de l'Est de l'Ontario", Ottawa Catholic School Board und "Conseil des écoles catholiques du Centre-Est" betreiben zahlreiche allgemeinbildenden Schulen, die in englischer oder französischer Sprache unterrichten. Daneben gibt es mehrere privat geführte Schulen, z. B. das Internat Ashbury College und eine Montessorischule.

Ottawa hat landesweit die höchste Pro-Kopf-Konzentration von Ingenieuren, Wissenschaftlern und Einwohnern mit akademischem Grad.

Das nationale Archiv und die Nationalbibliothek Kanadas "Library and Archives Canada" wurde in einem neu errichteten Bau 1967 von Premierminister Lester B. Pearson eröffnet.

Im Gegensatz zu Toronto und Montreal dominieren nur wenige hohe Gebäude das Stadtbild Ottawas. Mit 112 m ist das höchste Gebäude "Place de Ville II" nur unwesentlich höher als der historische "Peace Tower" des Parlamentsgebäudes. Vor allem in der Innenstadt sind die Straßen rechtwinkelig angelegt, wobei die großen Ausfallstraßen teilweise ihren ursprünglichen Verlauf behalten haben und dem Muster nur teilweise folgen.

Als Hauptsehenswürdigkeit gilt das Regierungsviertel, dessen Gebäude in Anlehnung an die Regierungsbauten in London entworfen wurden und im Stil der britischen Neugotik gehalten sind. Die Bauwerke befinden sich auf dem Parlamentshügel ("Parliament Hill") zwischen dem Rideau-Kanal und dem Ottawa River. Das Parlamentsgebäude ist ein Komplex, der aus drei Teilen ("East Block", "West Block" und "Centre Block") und dem markanten 92 m hohen Peace Tower besteht. An der Nordseite des Centre Block befindet sich die Parlamentsbibliothek, die aufgrund ihres Kuppeldachs und seinen Stützpfeilern an einen gotischen Sakralbau erinnert. Auf dem Vorplatz des Parlaments brennt die "Centennial Flame", die in der Neujahrsnacht 1966 an die ersten hundert Jahre der Kanadischen Konföderation erinnern soll. Einige hundert Meter westlich des Parlaments befindet sich der Oberste Gerichtshof Kanadas. Das höchste kanadische Gericht tagt in einem 1939 erbauten Gebäude mit grünlichem Dach. Im Regierungsviertel finden sich zahlreiche Statuen berühmter kanadischer Politiker. Östlich des Parlamentsgebäudes befindet sich eine Statue aus dem Jahr 1977, die Königin Elisabeth II. auf einem Pferd darstellt.

Die 1846 geweihte römisch-katholische Kathedralbasilika Notre Dame östlich des Rideau-Kanals am Sussex Drive ist die älteste Kirche Ottawas und Sitz des Erzbischofs. Die neogotische Kirche wurde von 1841 bis 1865 erbaut, der prachtvolle Innenraum zwischen 1876 und 1885 gestaltet.

Westlich der Notre Dame befindet sich die Kanadische Nationalgalerie ("National Gallery of Canada"). Südlich des Museums befinden sich die Zufahrt zur Alexandra Bridge nach Gatineau und ein kleiner Park mit dem Denkmal Samuel de Champlains ("Nepean Point").

Zwischen den Parlamentsgebäuden und dem Nobelhotel Château Laurier fließt der Rideau-Kanal, der an der Mündung zum Ottawa River mehrere Schleusenstufen aufweist und einen Höhenunterschied von 24,1 Meter mit Hilfe von acht von Hand zu bedienenden Toren überbrückt. In den Wintermonaten wird der Kanal zur längsten Schlittschuhlaufbahn der Welt (etwa sieben Kilometer).

Am Rideau-Kanal befindet sich der Confederation Square, eine dreieckige Parkanlage, in der sich ein monumentales Kriegsdenkmal befindet. Südlich des Parks steht das "National Arts Centre", ein Konzert-, Opern und Theaterhaus. Nördlich des Confederation Square befindet sich das "Château Laurier". Das an eine mittelalterliche Burg erinnernde Hotelgebäude wurde 1912 als Eisenbahnhotel errichtet und zählt zu den besten Hotels der Stadt.

Die Sammlung der "Kanadischen Nationalgalerie" dokumentiert die Entwicklung der kanadischen Kunst, stellt aber auch asiatische und europäische Kunst aus. Das Museumsgebäude mit seinem auffälligen Glaskuppelbau wurde 1988 von Moshe Safdie entworfen. Das Museum besitzt Werke von Künstlern wie Lucas Cranach d. Ä., El Greco, Gustav Klimt, Pablo Picasso oder Andy Warhol. Die kanadische Kunst ist z. B. mit Werken von Paul Kane und der Group of Seven vertreten. Außerdem zeigt das Museum in einer eigenen Abteilung Videokunst. Auf dem Vorplatz befindet sich die 1999 aufgestellte, zehn Meter hohe Spinnenskulptur Maman aus Bronze, die die französische Bildhauerin Louise Bourgeois schuf.

Neben dem Château Laurier befindet sich das der Nationalgalerie angegliederte "Museum of Contemporary Photography". In einem ehemaligen Eisenbahntunnel untergebracht, beherbergt es über 150.000 Fotografien von zumeist kanadischen Künstlern.

Seit 2005 befindet sich das "Kanadische Kriegsmuseum" in einem neuen Gebäude in den Lebreton Flats nahe dem Ottawa-Fluss. Das Gebäude wurde von den Architekten Raymond Miriyama und Alex Rankin entworfen. In fünf verschiedenen Sektionen wird chronologisch die Kriegsgeschichte Kanadas beleuchtet, im eigenen Land wie auch in anderen Erdteilen. Eine Attraktion ist ein originales Mercedes-Benz-770-Cabriolet von Adolf Hitler. Bei diesem Exponat, das ursprünglich der Wagen von Hermann Göring hätte sein sollen, handelt es sich um eine Schenkung eines Québecer Geschäftsmannes aus dem Jahr 1970. Zudem rekonstruierte das Museum einige bekannte Kriegsschauplätze, beispielsweise einen Schützengraben aus dem Ersten Weltkrieg.

Im benachbarten Gatineau befindet sich mit "Kanadas Nationalmuseum für Geschichte und Gesellschaft", das meistbesuchte Museum Kanadas. Es widmet sich der Kultur- und Besiedlungsgeschichte Kanadas, wobei es sich in Form von Dauerausstellungen mit dem Thema der Ureinwohner (vor allem der First Nations, aber auch der Métis und Inuit), der Besiedlungsgeschichte durch europäische und asiatische Einwanderer und herausragenden Persönlichkeiten der kanadischen Geschichte befasst. Im Museum ist außerdem das Kanadische Postmuseum ("Canadian Postal Museum"), das u. a. eine Sammlung sämtlicher in Kanada erschienenen Briefmarken besitzt.

Weitere Museen Ottawas sind das "Canadian Museum of Nature" am Südrand der Downtown, das "Canada Science and Technology Museum" am südöstlichen Stadtrand, das "Canada Aviation and Space Museum" auf dem Gelände des Flugplatzes Ottawa-Rockcliffe nordöstlich der Innenstadt und das "Museum für Landwirtschaft und Ernährung" auf dem Gelände der Central Experimental Farm/Ferme expérimentale centrale im Südwesten der Innenstadt.

Im Gebäude der Bank of Canada ist das Münzmuseum ("Currency Museum") untergebracht. Es zeigt Münzen aus China, dem antiken Griechenland, Rom, Byzanz und Münzen vom Mittelalter bis zur Renaissance sowie die Entwicklung des Geldwesens in Nordamerika.

Zwischen der Elgin Street und dem Rideau-Kanal befindet sich das National Arts Centre, ein Gebäudekomplex, der mehrere Hallen für Konzert- und Theateraufführungen bietet. Es beherbergt die zwei Orchester "National Arts Centre Orchestra" und das "Ottawa Symphony Orchestra" sowie die Operngruppe "Opera Lyra Ottawa". Diese treten vor allem in der über 2300 Plätze fassenden Southam Hall auf. Das Theater mit knapp 900 Sitzen ist der Stammsitz einer englisch- und einer französischsprachigen Theatergruppe. Das dunkelbraune Gebäude, das von den Architekten Dimitri Dimakopoulos und Fred Lebensold entworfen wurde, ist länglich und besteht aus mehreren hexagonalen Formen. Die Eröffnung des Kunstzentrums fand am 2. Juni 1969 statt.

In der Nähe des Algonquin College befindet sich das "Centrepoint Theater". Es bietet für kleinere Musik- und Theateraufführungen Platz für knapp 1000 Zuschauer. Die Veranstaltungshalle wurde am 3. Mai 1988 eröffnet.

Über 60 Festivals und Veranstaltungen finden jedes Jahr im regelmäßigen Turnus in Ottawa statt.

Jährlich im Mai findet die Tulpenparade statt, die an die Geburt der niederländischen Prinzessin Margriet während des Exils der Königsfamilie im Zweiten Weltkrieg in Ottawa erinnert. Königin Juliana konnte während ihres Exils ihre Tochter in einem kurzfristig extraterritorial erklärten Krankenhauszimmer zur Welt bringen, was dieser den Anspruch auf die Thronfolge sicherte. Aus Dankbarkeit dafür schickt das niederländische Königshaus bis heute jedes Jahr 20.000 Tulpenzwiebeln in die kanadische Hauptstadt. Teil dieses Festes ist eine große Bootsparade auf dem Rideau-Kanal.

Das im Sommer stattfindende "Ottawa International Chamber Music Festival" gehört weltweit zu den größten Kammermusikereignissen. Seit 1981 wird ebenfalls im Sommer das "Ottawa International Jazz Festival " abgehalten. Im Jahr 2008 kamen über 180.000 Besucher zu dieser Veranstaltung. Kanadas größtes Bluesfestival findet ebenfalls in Ottawa statt. Seit 1994 werden an elf Tagen im Juli auf verschiedenen Bühnen Blueskonzerte aufgeführt. 2007 besuchten mehr als 300.000 Menschen das Festival.

Der Kanadische Nationalfeiertag "Canada Day" am 1. Juli wird grundsätzlich im ganzen Land gefeiert. In Ottawa konzentrieren sich allerdings viele Veranstaltungen wie Paraden, große Konzerte und die Präsenz von Vertretern der kanadischen Politik. In den Jahren 1967, 1990, 1992 und 1997 nahm auch Königin Elisabeth II. an den Feierlichkeiten teil.

Seit 1979 wird in Gatineau und Ottawa im Februar das Freiluftfestival "Winterlude" (franz.: "Bal de neige") ausgetragen. Da in den vergleichsweise strengen Wintermonaten der Rideau-Kanal oft vollständig zugefroren ist, finden auf dem Kanal Musikkonzerte und eissportliche Aktivitäten statt. Einer der Höhepunkte ist ein Skulpturenwettbewerb, bei dem Skulpturen aus Schnee und Eis gefertigt und nachts angeleuchtet werden. 2007 lockte das Festival rund 1,6 Millionen Besucher an.

Mit den Ottawa Senators in der National Hockey League (NHL) verfügt Ottawa über eine Major-League-Mannschaft. Ihre Heimspiele tragen die seit 1992 am Spielbetrieb teilnehmenden Senators im 19.153 Zuschauer fassenden Canadian Tire Centre aus, das 1996 eröffnet und 2005 erweitert wurde. Die Ottawa 67’s sind eine Mannschaft in der Jugendliga Ontario Hockey League (OHL). In den Jahren 1978 und 1984 fanden die Eiskunstlauf-Weltmeisterschaftenen in Ottawa statt.

Mannschaften, welche die weiteren kanadischen Nationalsportarten Lacrosse ("Ottawa Rebel") und Canadian Football ("Ottawa Renegades") ausübten, existierten nur kurzzeitig. Das höchstklassige Fußballteam der Stadt sind Ottawa Fury in der Profiliga United Soccer League. Mit 26.559 Plätzen ist das TD Place Stadium die größte Sportstätte Ottawas. Seit 2014 wird durch die Ottawa RedBlacks auch wieder Canadian Football gespielt. Innerhalb Ottawas befinden sich ungefähr 170 Kilometer asphaltierte Wege, die speziell für Inlineskater geeignet sind. Jährlich am letzten Maiwochenende finden Laufwettbewerbe ("Ottawa Race Weekend") statt, zu dessen Höhepunkt der Ottawa-Marathon zählt.





</doc>
<doc id="3720" url="https://de.wikipedia.org/wiki?curid=3720" title="Oktaeder">
Oktaeder

Das (auch, v. a. österr.: der) Oktaeder [] (von griech. "oktáedron" ‚Achtflächner‘) ist einer der fünf platonischen Körper, genauer ein regelmäßiges Polyeder ("Vielflächner") mit

Das Oktaeder ist sowohl eine gleichseitige vierseitige Doppelpyramide (mit quadratischer Grundfläche) als auch ein gleichseitiges Antiprisma (mit einem gleichseitigen Dreieck als Grundfläche).

Wegen seiner hohen Symmetrie – alle Ecken, Kanten und Flächen sind untereinander gleichartig – ist das Oktaeder ein reguläres Polyeder. Es hat:
und ist
Insgesamt hat die Symmetriegruppe des Oktaeders – die Oktaeder- oder Würfelgruppe – 48 Elemente.

Das Oktaeder ist das zum Hexaeder (Würfel) duale Polyeder (und umgekehrt).

Setzt man auf die Seiten des Oktaeders Tetraeder auf, entsteht das Sterntetraeder.

Mithilfe von Oktaeder und Würfel können zahlreiche Körper konstruiert werden, die ebenfalls die Würfelgruppe als Symmetriegruppe haben. So erhält man zum Beispiel
als Durchschnitte eines Oktaeders mit einem Würfel (siehe archimedische Körper)
und
als konvexe Hülle einer Vereinigung eines Oktaeders mit einem Würfel.

Die Analoga des Oktaeders in beliebiger Dimension "n" werden als ("n-dimensionale") Kreuzpolytope bezeichnet und sind ebenfalls reguläre Polytope. Das "n"-dimensionale Kreuzpolytop hat formula_1 Ecken und wird von formula_2 "(n−1)"-dimensionalen Simplexen (als "Facetten") begrenzt. Das vierdimensionale Kreuzpolytop hat 8 Ecken, 24 gleich lange Kanten, 32 gleichseitige Dreiecke als Seitenflächen und 16 Tetraeder als Facetten.
(Das eindimensionale Kreuzpolytop ist eine Strecke, das zweidimensionale Kreuzpolytop ist
das Quadrat.)

Ein Modell für das "n"-dimensionale Kreuzpolytop ist die Einheitskugel bezüglich der Summennorm
im Vektorraum formula_5. Und zwar ist das (abgeschlossene) Kreuzpolytop daher

Das Volumen des n-dimensionalen Kreuzpolytops beträgt formula_12, wobei formula_13 der Radius der Kugel um den Ursprung bezüglich der Summennorm ist. Die Beziehung lässt sich mittels Rekursion und dem Satz von Fubini beweisen.

In der Chemie können sich bei der Vorhersage von Molekülgeometrien nach dem VSEPR-Modell oktaedrische Moleküle ergeben. Auch in Kristallstrukturen, wie der kubisch flächenzentrierten Natriumchlorid-Struktur (Koordinationszahl 6), taucht das Oktaeder in der Elementarzelle auf; genauso in der Komplexchemie, falls sich 6 Liganden um ein Zentralatom lagern.

Einige in der Natur vorkommende Minerale, z. B. das Alaun, kristallisieren in oktaedrischer Form aus.

In Rollenspielen werden oktaedrische Spielewürfel verwendet und dort als „W8“, also als Würfel mit 8 Flächen, bezeichnet.



</doc>
<doc id="3721" url="https://de.wikipedia.org/wiki?curid=3721" title="Oswald Spengler">
Oswald Spengler

Oswald Arnold Gottfried Spengler (* 29. Mai 1880 in Blankenburg am Harz; † 8. Mai 1936 in München) war ein deutscher Geschichtsphilosoph, Kulturhistoriker und antidemokratischer politischer Schriftsteller.

In seinem Hauptwerk "Der Untergang des Abendlandes" richtet sich Spengler gegen eine lineare Geschichtsschreibung, die die Geschichte „der Menschheit“ als Geschichte des Fortschritts erzählt. Stattdessen vertritt er eine Zyklentheorie, nach der immer wieder neue Kulturen entstehen, eine Blütezeit erleben und nach dieser Vollendung untergehen. Er fasst Kulturen als eindeutig abgrenzbare, quasi-organische Gebilde mit einer Lebensdauer von etwa 1000 Jahren auf, die jeweils ganz charakteristische, das Denken und Handeln der Individuen prägende Eigenschaften aufweisen. Schon der Titel des Werkes enthält die These, die im Buch dargestellt und begründet werden soll, dass in der Gegenwart Spenglers die „Kultur des Abendlandes“ im Untergang begriffen sei.

Spengler wird zur nationalistischen und antidemokratischen „Konservativen Revolution“ gerechnet, lehnte aber den Nationalsozialismus und namentlich dessen Rassenideologie ab. Sein Ideal sah er eher in Benito Mussolini verwirklicht, dem Diktator des faschistischen Italien.

Spengler hat zwar im Urteil einiger Zeitgenossen Entwicklungen seiner Zeit richtig vorausgesagt und andere Geschichtswissenschaftler stark beeinflusst, darunter Franz Borkenau und vor allem Arnold J. Toynbee, aber sein Werk wird von der heutigen Geschichtswissenschaft nicht als grundlegend erachtet.

Spengler wurde am 29. Mai 1880 als zweites von fünf Kindern des Postbeamten Bernhard Spengler und seiner Frau Pauline Spengler, geb. Grantzow, in Blankenburg am Harz geboren. Im Jahr 1891 zog die Familie nach Halle an der Saale, wo Spengler die Latina der pietistisch orientierten Franckeschen Stiftungen besuchte. Seine Kindheit war geprägt von Nervenkrisen und Panikanfällen, außerdem neigte er zum Somnambulismus.

Später erinnerte er sich an seine Jugend als eine durch „Kopfschmerzen“ und „Lebensangst“ geprägte Zeit. Bereits früh richtete er seine bemerkenswerte Phantasie auf historische Themen: Als 15-Jähriger füllte er ganze Hefte mit detaillierten Angaben zu Geschichte, Geografie und Verwaltung zweier fiktiver Reiche. Spengler bildete sich neben der als eng empfundenen Schulwelt autodidaktisch weiter.

Nachdem er 1899 das Abitur bestanden hatte und wegen eines schweren Herzfehlers vom Militärdienst befreit worden war, studierte er in Halle, München und Berlin Mathematik, Naturwissenschaften und Philosophie. Seine Dissertation schrieb er bei dem Philosophen Alois Riehl zum Thema "Der metaphysische Grundgedanke der Heraklitischen Philosophie". Die Prüfungskommission lehnte nach der mündlichen Verteidigung die Dissertation mit der Begründung ab, es sei zu wenig Fachliteratur zitiert worden. Spengler wiederholte die Prüfung und wurde am 6. April 1904 an der Universität Halle zum Dr. phil. promoviert.

Im Dezember 1904 bestand er die Prüfung für das Höhere Lehramt in den Fächern Zoologie, Botanik, Physik, Chemie und Mathematik. Das Thema der Staatsexamensarbeit lautete: "Die Entwicklung des Sehorgans bei den Hauptstufen des Tierreiches". Darin manifestierte sich, wie Koktanek hervorhob, ein Leitmotiv des Spenglerschen Denkens, das später sowohl in der Schrift "Der Mensch und die Technik" (1931) als auch in den posthum veröffentlichten "Urfragen" zum Tragen kam.

Spengler lernte in seinem Studium einerseits die Naturwissenschaften kennen, andererseits die Philosophie. Prägend wirkten auf ihn Ernst Haeckel, die fiktionale Philosophie Hans Vaihingers ("Philosophie des Als Ob"), in besonderem Ausmaß aber die Kulturkritik Friedrich Nietzsches, besonders seine Konzepte von Dekadenz und dem Willen zur Macht. Außerdem verehrte er lebenslang Goethe als einen Gipfel der abendländischen Kultur.

Das Seminarjahr trat Spengler 1905 in Lüneburg an. Der Schuldienst sagte ihm jedoch nicht zu. Er erlitt einen Nervenzusammenbruch, woraufhin er wieder abreiste. Auf Wunsch seiner Mutter nahm er 1906 eine Stelle in Saarbrücken an und absolvierte sein Probejahr in Düsseldorf. Nachdem er die Lehrbefugnis für das Fach Mathematik erhalten hatte, trat er 1908 eine feste Anstellung als Ordinarius an einem Hamburger Gymnasium an. Bei seinen Schülern war er beliebt, besonders wegen seiner improvisierten Vorträge.

Eine kleine Erbschaft nach dem Tod seiner Mutter eröffnete Spengler die Möglichkeit, seine Unterrichtstätigkeit aufzugeben und als freier Schriftsteller seinen literarischen Ambitionen nachzugehen. Schließlich zog er im März 1911 nach München-Schwabing. In München war er zunächst für verschiedene Zeitungen als Kulturreferent tätig. Für die „unvergleichliche Atmosphäre vieldeutiger Modernität“ der Münchner Szene voller Künstler und Revolutionäre verschiedener Couleur empfand Spengler nur Ekel und Verachtung. Daher teilte er den in diesen Kreisen offenen Antisemitismus nicht, dessen Sprache er als vulgär ablehnte. Gleichwohl sind von ihm aus dieser Zeit Äußerungen überliefert, die judenfeindliche Ressentiments zeigen, wenn er etwa „Schmutz und Gemeinheit“ der deutschen Literaturszene seiner Gegenwart mit „einem russischen Ghetto“ verglich.

Die Zweite Marokkokrise 1911 nahm er als Demütigung des Deutschen Reichs wahr, dessen Außenpolitik ihm schwächlich erschien. Dies stellte er später als den Anlass dar, mit der Arbeit an seinem Hauptwerk "Der Untergang des Abendlandes" („Umrisse einer Morphologie der Weltgeschichte“) zu beginnen. Im April 1917 schloss er den ersten Band ab, der im September 1918 erschien, wenige Wochen vor Ende des Ersten Weltkriegs, an dem Spengler wegen seiner Gesundheitsprobleme nicht hatte teilnehmen können. Die Koinzidenz zwischen dem unheilverkündenden Titel und der deutschen Niederlage trug zum fulminanten publizistischen Erfolg des Buches bei. Spengler wurde schlagartig berühmt und in literarischen, wissenschaftlichen und politischen Kreisen zum Gegenstand heftiger Debatten und Kontroversen. Der zweite Band erschien 1922. Während seiner rund zehnjährigen Arbeit an seinem Hauptwerk lebte er isoliert, litt unter psychischen Problemen und später unter materiellen Schwierigkeiten. Während seiner Münchner Zeit litt Spengler stark unter seiner sozialen und intellektuellen Isolierung. „Insgeheim vergleicht er sich mit Deutschland, das ebenfalls allein ist.“ Er war erschöpft und fühlte sich müde. Dennoch ging er davon aus, dass sein Werk „epochemachend“ sein würde.

Zwischen 1914 und 1917 verfasste Spengler zwei undatierte Denkschriften, die nur in Fragmenten überliefert sind. Die eine richtete er an Kaiser Wilhelm II., die andere an den Adel. In seiner Denkschrift an den Kaiser fordert Spengler, dass die „Monarchie den republikanischen Herausforderungen mit der Bereitschaft der Selbsterneuerung begegnen“ müsse. Vom Adel forderte er, dass er auf seine politischen Privilegien verzichtet. Mit seiner antiaufklärerischen Kritik forderte Spengler eine demokratische Elitenbildung, damit „mit großer Wahrscheinlichkeit so starke Begabungen tatsächlich an der geeigneten Stelle und unter hinreichender Schulung vorhanden sind, wie das System stillschweigend voraussetzt“. Spenglers Überzeugung war, dass ein leistungsfähiger Adel in einem monarchischen Staat, der Aufstiegsmöglichkeiten für Nichtadelige bietet, grundsätzlich besser sei als eine reine Demokratie.

Die deutsche Niederlage im Ersten Weltkrieg akzeptierte er nicht. Bereits im Dezember 1918 schrieb er in einem Brief, dass der Friede nur provisorisch sein könne: Der Weltkrieg trete „erst jetzt in sein zweites Stadium“ ein.

Als politischer Schriftsteller brachte Spengler seine antidemokratische Gesinnung in kleineren Schriften zum Ausdruck. Er hoffte, dass der Weimarer Republik durch einen Diktator ein Ende gesetzt werde, der imstande sei, die großen innen- und vor allem außenpolitischen Herausforderungen in einem Zeitalter der „Vernichtungskriege“, das er in seinem "Untergang des Abendlandes" prophezeit hatte, erfolgreich zu bewältigen. 

Anfang der zwanziger Jahre versuchte er auch selbst auf die Politik Einfluss zu nehmen. Mit dem Geld schwerindustrieller Freunde und Bekannten, darunter Albert Vögler und Paul Reusch wollte er 1922 ein geheimes Büro zur zentralen Lenkung der Presse aufbauen, in dem neben ihm selbst auch der rechtskatholische Publizist Martin Spahn und der Journalist Paul Nikolaus Cossmann von den "Münchner Neuesten Nachrichten" tätig werden würden. Die Zeitungen sollten über eine Kontrolle der Anzeigen, die eine für sie wichtige Einnahmequelle darstellten, auf eine nationalistische Linie gebracht werden. Die Presse des Hugenberg-Konzerns und die von Hugo Stinnes kontrollierten Blätter hoffte er durch Vermittlung Georg Escherichs einbinden zu können, des Führers der illegalen Einwohnerwehren. Der Plan scheiterte an der Rivalität Alfred Hugenbergs gegen Spenglers Förderer Reusch und Karl Haniel. Nach dem Historiker Paul Hoser war Spenglers Idee einer geheimen Presselenkung „nichts als ein phantastischer Kleinbürgertraum“.

In der zweiten Jahreshälfte 1923 beteiligte Spengler sich an Planungen rechtsgerichteter Kreise um den Chef der Heeresleitung der Reichswehr, Hans von Seeckt, die Reichsregierung unter Gustav Stresemann durch ein autoritär regierendes „Direktorium“ zu ersetzen, in dem er selbst Bildungsminister werden sollte. Ein persönliches Treffen mit Seeckt verlief für beide Seiten enttäuschend, denn dieser äußerte anschließend, er wünschte, Spengler „wäre mit dem Abendland untergegangen – ein politischer Narr!“ Der bezeichnete seinerseits Seeckt in der Folge als „Opportunisten“. Nun wünschte sich Spengler Escherich oder den bayrischen Generalstaatskommissar Gustav von Kahr als Diktator. Dazu orchestrierte er eine Pressekampagne gegen Stresemann, die erfolglos blieb – nach Spenglers Ansicht auch, weil man zu seinem Bedauern darauf verzichtet hatte, Informationen über Stresemanns Privatleben auszuschlachten. Nach dem gescheiterten Hitlerputsch zog er sich aus der aktiven politischen Tätigkeit zurück und arbeitete nur noch publizistisch. In den 1920er Jahren stand er dem Nietzsche-Archiv nahe.

Den Nationalsozialismus lehnte Spengler ebenso ab wie die Weimarer Republik. Ein Angebot Gregor Straßers, des NSDAP-Gauleiters von Niederbayern, an den "Nationalsozialistischen Monatsheften" mitzuwirken, schlug er 1925 aus, weil ihm die „primitive Lösung eines Antisemitismus“ zuwider war. Aus dem gleichen Grund lehnte er 1927 eine Mitarbeit an der völkischen Zeitschrift "Deutsches Volkstum" ab.

Nach der Machtergreifung änderte sich seine ablehnende Haltung nicht, das Kabinett Hitler bezeichnete er in einem Brief als „Faschingsministerium“. Wie er in seinem 1933 erschienenen Werk "Jahre der Entscheidung" deutlich machte, sah er seine antidemokratischen und antiparlamentarischen Ideale vielmehr in Benito Mussolini als Diktator des faschistischen Italien erreicht. 

Der Bitte von Reichspropagandaleiter Joseph Goebbels, anlässlich des Tages von Potsdam am 21. März 1933 eine Rede zur Versöhnung von „Preußentum und Sozialismus“ zu halten, kam er nicht nach. Am 14. Juni 1933 erhielt Spengler einen Ruf an die Universität Leipzig, lehnte aber auch diesen ab, nachdem er bereits im Jahr 1919 einem Ruf an die Universität Göttingen nicht gefolgt war. Am 25. Juli 1933 fand in Bayreuth eine Unterredung zwischen Spengler und Adolf Hitler statt, bei der aber nur deren gegenseitige Abneigung deutlich wurde.

In seinem Buch "Jahre der Entscheidung", das am 18. August 1933 erschien, distanzierte sich Spengler öffentlich von Hitler und dem Nationalsozialismus, dagegen feierte er Mussolini enthusiastisch. Trotz seiner oppositionellen Tendenz wurde das Buch vom NS-Regime nicht verboten. Es lief zwar eine Kampagne gegen das Buch, doch Reichspropagandaminister Goebbels bemühte sich weiterhin, Spengler auf seine Seite zu ziehen. Erst nachdem dieser ein Angebot von Goebbels ausgeschlagen hatte, einen Aufsatz über den Austritt Deutschlands aus dem Völkerbund (26. Oktober 1933) zu schreiben, gab der Minister auf und erteilte die Anweisung, Spengler in Zukunft zu ignorieren. 

Die als „Röhm-Putsch“ bekannt gewordene politische Säuberungswelle der Nationalsozialisten vom 30. Juni bedeutete für Spengler den endgültigen Bruch mit dem Nationalsozialismus. Unter den Ermordeten befand sich mit Gregor Straßer einer seiner früheren politischen Ansprechpartner. Besonders betroffen war er aber vom Tod des Münchner Musikkritikers Willi Schmid, der das Opfer einer mutmasslichen Verwechslung mit dem ebenfalls bei den "Münchner Neusten Nachrichten" publizierenden Paul Schmitt wurde. 

Obwohl er wesentliche Gedanken wie die Rassenideologie nicht mittrug und sich vom Nationalsozialismus unter Hitler distanzierte, gilt er als Wegbereiter des Nationalsozialismus. Als so genannter „Meisterdenker der Konservativen Revolution“ trug er wie Ernst Jünger und andere Protagonisten dieser Bewegung wesentlich dazu bei, das verhasste „System“ der Weimarer Republik zu delegitimieren und zu unterminieren. Die Nationalsozialisten selbst sahen ihn als einen ihrer „geistigen Väter“ an.

In seinen letzten Jahren widmete sich Spengler wieder verstärkt wissenschaftlichen Fragen, die im Horizont einer Weltgeschichte von Anfang an standen, in die die Geschichte der Hochkulturen eingebunden werden sollte. Parallel dazu legte Spengler unter dem Stichwort „DiG“ (Deutschland in Gefahr) Notizzettel für den zweiten Band der "Jahre der Entscheidung" an. Darin rechnete er mit dem Nationalsozialismus ab und stellte ihn auf eine Stufe mit dem Bolschewismus, den er bisher als das größte aller Übel auf der Ebene der Politik bezeichnet hatte. Hingegen behielt er in diesen Notizen seine Bewunderung für Mussolini bei.

Im Oktober 1935 trat Spengler aus dem Vorstand des Nietzsche-Archivs aus, weil er sich mit der Neudeutung Friedrich Nietzsches im Nationalsozialismus nicht abfinden wollte.

Spengler starb in der Nacht vom 7. auf den 8. Mai 1936 in seiner Münchner Wohnung an Herzversagen; sein unerwarteter Tod gab Anlass für „Gerüchte, er sei von NS-Männern ermordet worden“. Spengler wurde auf dem Münchner Nordfriedhof beigesetzt (Sektion 125, Grabanlage 2).

Hauptthema aller seiner dichterischen und philosophischen Arbeiten ist seine morphologische Sicht der Geschichte. Sein Hauptwerk "Der Untergang des Abendlandes. Umrisse einer Morphologie der Weltgeschichte" stellt dieses Thema in aller Ausführlichkeit dar. Hauptthese seiner geschichtsphilosophischen Sicht ist die kulturpessimistische Aussage, seine Zeit sei unfähig, kreativ zu wirken. Daraus folge die Verpflichtung, die von früheren Generationen geschaffene Kultur zu bewahren und sich angesichts der politischen Herausforderungen in Zeiten des Verfalls zu bewähren. Dabei soll der „Blick über die Kulturen hin“ den Weg weisen. Erkenntnistheoretisch berief er sich dabei auf Goethe.

Spengler entwirft ein Panorama von acht Kulturen und schildert ihre spezifischen Besonderheiten und ihre „Lebensphasen“: Frühzeit, Reifung, Alterung und unaufhaltsamen Verfall: Das Alte Ägypten, Babylonien, Indien, China, die „apollinische“ Antike, das „magische“ Arabien, zu dem er auch das Judentum rechnet, die Kultur der Azteken und das „faustische“ Abendland. 

Dabei werden die Phasen unterschiedlicher Kulturen als einander entsprechend (Spengler schreibt: „gleichzeitig“) in Parallele gesetzt: Der Frühzeit der Antike, der Dorik, entspreche im Abendland die Gotik, der antiken Alterungskrise, der Sophistik, entspreche die abendländische Aufklärung usw. Interkulturelle Lernprozesse, Rezeptionen und Renaissancen hält er für nicht möglich: Jede Kultur durchlaufe in den ihr zugewiesenen rund eintausend Jahren Lebensdauer mit Notwendigkeit die genannten Phasen. Eine Beeinflussung einer jüngeren Kultur durch eine reifere sah er als schädlich an: Eine solche „Pseudomorphose“ hemme die Selbstbewusstwerdung, wofür die spätantike Beeinflussung der arabischen Kultur oder die westlichen Einflüsse auf Russland seit Peter dem Großen Beispiele seien. Um den Untergang des Abendlandes gegenüber der als nächstes heraufziehenden russischen Kultur wenn schon nicht zu verhindern, so doch zu verlangsamen, empfiehlt Spengler Technokratie, Imperialismus und Sozialismus.

Das Werk ist von einem deutlichen Biologismus gekennzeichnet. Werden und Vergehen der Völker, Staaten und Kulturen werden in Begriffen wie Geburt, Reifung, Blüte, Verwesung beschrieben, die eigentlich für die Beschreibung einzelner Pflanzen oder Tiere geprägt wurden. Diese Begriffe werden von einigen Forschern als metaphorisch gemeint interpretiert. Der Historiker Alexander Bein glaubt dagegen, dass Spengler sie „real-naturalistisch“ für adäquate Begriffe zur Beschreibung politisch-gesellschaftlicher Prozesse gehalten habe. Dadurch habe er seine Kulturschau in den Bereich des Mythos gerückt.

"Der Untergang des Abendlandes" war eines der erfolgreichsten und umstrittensten Werke, die nach 1918 erschienen. Der Titel wurde zum geflügelten Wort. Bei zeitgenössischen Intellektuellen lässt sich fast immer voraussetzen, dass sie es gelesen haben. Der Erfolg des Werkes hatte vor allem zwei Gründe: Erstens erschien "Der Untergang des Abendlandes" zu einem Zeitpunkt, als der im 19. Jahrhundert entwickelte Fortschrittsoptimismus durch den Ersten Weltkrieg zutiefst erschüttert und durch ein umfassendes gesellschaftliches Krisenbewusstsein verdrängt wurde. Das Werk wurde als ausgesprochen aktuell wahrgenommen. Zweitens hatte es den Vorzug, eine immense Fülle an Daten aus den unterschiedlichsten Wissenschaftsdisziplinen zu einer Gesamtschau zu verarbeiten. Das Resultat war eine universalgeschichtliche, d. h. Vergangenheit, Gegenwart und Zukunft umfassende Darstellung der Entwicklung des Abendlandes, die viele Leser zu faszinieren vermochte. Umstritten waren insbesondere Spenglers Methode der „historischen Morphologie“, also seine Herleitung geschichtlicher Analogien, die von der Fachwissenschaft als unseriös betrachtet wurden, sowie die politischen Implikationen, die Spengler mit seiner Vorstellung vom Zyklus der Hochkulturen verband. Mit den acht Kulturmonaden konnte er auf positivistisch arbeitende Historiker kaum Eindruck machen. In der Geschichtswissenschaft wird "Der Untergang des Abendlandes" zumeist vernichtend kritisiert. Als Darstellung der Geschichte gilt es als unwissenschaftlich, außerdem wurde Spengler Dilettantismus vorgeworfen.

Spengler selbst bezeichnete sein Hauptwerk als „Metaphysik“. Das hinderte den britischen Historiker Arnold J. Toynbee nicht, ihn zeitlebens zu bewundern. Noch bei Franz Borkenau findet sich eine Spengler sehr ernst nehmende grundsätzliche Auseinandersetzung. Auch in weiten Teilen der Bildungsschicht, besonders in Deutschland und Österreich (Egon Friedell, Gottfried Benn u. a.), wurde sein Blick auf die Weltgeschichte ernst genommen. Der Dichter Gottfried Benn war zeitlebens angetan von Spenglers Morphologie und wurde „Poet des Spenglerschen Lebensgefühls“.

Robert Musil äußerte am Ende einer vernichtenden Kritik, andere hätten nur deshalb nicht so viele Fehler gemacht, weil sie nicht die beide Ufer berührende Spannweite besäßen, um so viele (Fehler) darauf unterzubringen. Er schrieb: „Es gibt zitronengelbe Falter, es gibt zitronengelbe Chinesen. In gewisser Weise kann man also sagen, der Falter ist der geflügelte mitteleuropäische Zwergchinese. Falter und Chinese sind bekannt als Sinnbilder der Wollust. Zum ersten Mal wird hier der Gedanke an die noch nie beachtete Übereinstimmung des großen Alters der lepidopteren Fauna und der chinesischen Kultur gefasst. Dass der Falter Flügel hat und der Chinese keine, ist nur ein Oberflächenphänomen!“

Thomas Mann lobte das Werk zunächst emphatisch und schlug es der Jury des Nietzsche-Preises zur Auszeichnung vor. Es sei ein „Buch voller Schicksalsliebe und Tapferkeit der Erkenntnis, worin man die großen Gesichtspunkte findet, die man heute gerade als deutscher Mensch braucht.“
Schon 1922, als er sich mit der Weimarer Republik zu versöhnen begann, distanzierte er sich von Spengler. In seinem ersten Brief aus Deutschland würdigte er zwar den literarischen Glanz des Werkes, sprach dem Verfasser aber den humanistischen Pessimismus eines Schopenhauer oder den „tragisch-heroischen“ Charakter Nietzsches ab. Das Werk sei vielmehr fatalistisch und zukunftsfeindlich. „Solche Anmaßung aber und solche Nichtachtung des Menschlichen sind Spenglers Teil … Er tut nicht wohl daran, Goethe, Schopenhauer und Nietzsche zu Vorläufern seines hyänenhaften Prophetentums zu ernennen.“

Erheblichen Einfluss übte das Denken Spenglers auf die anthropologische und ethnologische Forschung aus – so etwa im Frühwerk von Claude Lévi-Strauss ("Tristes tropiques").

Karl Popper polemisierte 1957 in seiner Schrift "Das Elend des Historizismus" – gegen Spengler (und Marx) geschrieben – gegen die Annahme, es gebe unabänderliche historische Gesetzmäßigkeiten. 

Der marxistische Literaturwissenschaftler Georg Lukács kritisierte das Werk als eine Position auf der Linie „von Nietzsche zu Hitler“. Nach dem deutschen Historiker Alexander Bein trug Spengler mit seinen antisemitischen Geschichtsspekulationen wesentlich dazu bei, Stereotype über „die Juden“ auch in Kreisen plausibel zu machen, die sich von plumpen judenfeindlichen Geschichtsklitterungen fernhielten.

Theodor W. Adorno verteidigte Spenglers Geschichtsphilosophie. Die in der Nachkriegszeit kurrente Kritik an diesem extrem reaktionären Theoretiker sei zu einfach und affirmativ, da seine Kritik am Liberalismus einer progressiven Kritik überlegen sei. Seine Voraussicht auf den Faschismus als Cäsarismus sei wertvoll und enthalte Wahrheiten über Massenkultur und Parteienorganisation. Im größeren Teil des Aufsatzes leistet Adorno aber eine grundlegende Kritik an Spenglers Einverständnis mit dem blutigen Lauf der Geschichte und kritisiert ihn als „beflissenen Agenten des Weltgeistes“, der dessen Bewegungsrichtung entgegen seinem eigenen Anspruch nicht habe vorhersehen können.

Im Jahr 1936 verfasste Jorge Luis Borges ein Biogramm Spenglers in der Zeitschrift "El Hogar", in dem er konstatierte, dass die deutschen Philosophen, so auch Spengler, – im Gegensatz zu ihren französischen und englischen Kollegen – dazu neigten, die Welt nicht so zu beschreiben, wie sie sei, sondern dass ihr Hauptaugenmerk auf der Schönheit und Symmetrie ihrer erdachten Systeme liege. Man könne zwar über Spenglers biologischen Geschichtsbegriff streiten, nicht jedoch über seinen glänzenden Stil.

In jüngerer Zeit zeigte sich Morris Berman in seiner Kritik der amerikanischen Zivilisation von Spenglers Werk beeinflusst. Auch der belgische Althistoriker David Engels, der sich auch sonst intensiv mit Spengler auseinandergesetzt hat, beruft sich bei seinem Versuch, die Krise der Europäischen Union mit dem Untergang der römischen Republik zu vergleichen, explizit auf Spenglers Geschichtsmorphologie.

Die Streitschrift "Preußentum und Sozialismus" plante Spengler am Tag nach der Ermordung des bayerischen Ministerpräsidenten Kurt Eisner, sie erschien im November 1919 als Reaktion auf den Versailler Vertrag und die Weimarer Verfassung. Die Schrift ist vor allem werkgeschichtlich von Bedeutung und entspricht nach Spenglers Aussage weitgehend dem Keim seines Hauptwerks. Spengler plädiert darin für einen autoritären Staat unter einem caesaristischen Diktator, der auf den Traditionen des alten Preußen basieren sollte. Preußen stehe für Tugenden wie Pflicht, Ordnung und Gerechtigkeit, die Ideale einer „deutschen Kultur“ – im Gegensatz zu Freiheit, Gleichheit, Brüderlichkeit, den Idealen der westlichen Zivilisation. Diesem Begriff, den er mit Dekadenz gleichsetzt, stellt er den positiv besetzten Begriff "Kultur" (vertreten durch Goethe) gegenüber. Der nationale Sozialismus, den Spengler skizziert, ist nicht als Ansatz zu einer Änderung der Wirtschaftsverfassung oder einer Umverteilung des gesellschaftlichen Reichtums zu verstehen. Mit den Worten des Historikers Hans Mommsen handelt es sich um einen „Sozialismus der Gesinnung, nicht um eine ökonomische Theorie“, der keinen Gegensatz zu Spenglers elitärer Verachtung der Masse darstellt. Mit ihm will Spengler sowohl den marxistischen Sozialismus der Arbeiterbewegung bekämpfen, als auch den liberalen Parlamentarismus, den er als plutokratisch denunziert. Spenglers Sozialismus-Vorstellung ist explizit gegen den Westen und dessen Betonung individueller Freiheitsrechte gerichtet:

Zur Überwindung des gehassten westlichen Liberalismus und des Versailler Vertrags strebte Spengler vor allem ein Bündnis mit Russland bzw. der Sowjetunion an. 

1924 erschienen seine ergänzenden Schriften "Politische Pflichten der deutschen Jugend" und "Neubau des Deutschen Reiches", in denen er zur Überwindung des „nationalen Sumpfes“ aufrief, in den das Deutsche Reich geraten sei: Die Weimarer Republik tut er ab als „eine fünfjährige Orgie von Unfähigkeit, Feigheit und Gemeinheit“. Sie müsse überwunden werden, um für das seines Erachtens bevorstehende Ringen um die deutsche Weltgeltung gewappnet zu sein. Dabei verglich er das Deutschland seiner Gegenwart mit Frankreich zur Zeit des Direktoriums, eines Regimes, das 1799 von Napoleon Bonaparte in einem Putsch beseitigt worden war. Der darauf folgende Neubau des Reiches könne aber auf „Rassegefühle“, so „tief und natürlich“ sie auch sein mochten, keine Rücksicht nehmen, wie die Beispiele des Italieners Napoleon, des Juden Benjamin Disraeli und der Deutschen Katharina II. lehren würden.

Spenglers Schrift "Jahre der Entscheidung. Deutschland und die weltgeschichtliche Entwicklung" erschien 1933. Ursprünglich hatte sie den Titel "Deutschland in Gefahr" tragen sollen, worauf er aus Angst nach der Machtergreifung verzichtete. Gleich zu Beginn begrüßte er zwar die Machtergreifung der Nationalsozialisten:

Im weiteren Verlauf unterstellt er den Nationalsozialisten Realitätsferne: Sie würden „glauben ohne und gegen die Welt fertig zu werden und ihre Luftschlösser bauen zu können, ohne eine mindestens schweigende aber sehr fühlbare Gegenwirkung von außen her“. Seine antidemokratischen und antiparlamentarischen Ideale sieht Spengler vielmehr in dem Diktator des faschistischen Italiens, Benito Mussolini, erreicht. Spenglers Schrift "Jahre der Entscheidung" (1933), die ursprünglich den Titel "Deutschland in Gefahr" tragen sollte, worauf er aus Angst nach der Machtergreifung verzichtete, soll laut einem Vorwort der Nichte Spenglers Hildegard Kornhardt seinerzeit von NS-Kritikern und NS-Sympathisanten gleichermaßen als Angriff auf die NS-Ideologie verstanden worden sein. Spengler unterscheidet darin das Ideal des pietistisch-idealistischen preußischen Dienst- und Leistungsethos', wohin seiner Meinung nach eine nationale Revolution zurückführen sollte, von der biologistischen Rassenlehre des Nationalsozialismus. Das preußische Ethos sei ein „Daseinstakt“, der in generationenlanger Einübung und Verfestigung kulturprägender führender Familien entwickelt und sodann gesellschaftsprägend geworden sei. Ein derartiges gesellschaftliches Ethos sei nicht durch den geistigen Druck eines Parteiprogramms ersetzbar.

Spengler sieht das Europa seiner Gegenwart zweierlei Bedrohungen ausgesetzt: Der „weißen Weltrevolution“, worunter er die Sowjetunion versteht, und der „farbigen Weltrevolution“. Damit meinte er die Bedrohung der „weißen Rasse“ durch die außereuropäischen Völker. Ihnen während des Kolonialismus die technischen Errungenschaften Europas nicht vorenthalten zu haben, könne sich als verhängnisvoller Fehler erweisen. Als „größte Bedrohung“ malte er eine Verbindung beider „Weltrevolutionen“ aus: „Wie, wenn sich eines Tages Klassenkampf und Rassenkampf zusammenschließen, um mit der weißen Welt ein Ende zu machen?“ Als Heilmittel appellierte an den Selbstbehauptungswillen der Europäer und insbesondere der germanischen Rasse, der er noch große Möglichkeiten attestierte.

Laut Aussage seines Biographen Detlef Felken darf Spenglers Distanz zu den Nationalsozialisten, die sich in diesem Werk zeigt, „nicht über das antidemokratische Potential der "Jahre der Entscheidung" hinwegtäuschen“.







</doc>
<doc id="3722" url="https://de.wikipedia.org/wiki?curid=3722" title="Ozon">
Ozon

Ozon ( "ozein" „riechen“) ist ein aus drei Sauerstoffatomen bestehendes Molekül (O). Ozonmoleküle in der Luft zerfallen unter Normalbedingungen innerhalb einiger Tage, im Dunkeln jedoch sehr schnell zu biatomarem Sauerstoff. Einerseits ist Ozon ein starkes Oxidationsmittel, das bei Menschen und Tieren zu Reizungen der Atemwege und der Augen führen kann, andererseits schützt die Ozonschicht in der Stratosphäre die Lebewesen auf der Erde vor Schädigungen durch energiereiche mutagene ultraviolette Strahlung der Sonne.

Im Jahre 1839 beschrieb Christian Friedrich Schönbein zum ersten Mal die in der Chemie einzigartige Erscheinung, dass ein Element in Gasform in zwei verschiedenen molekularen Formen nebeneinander beständig ist – Ozon und Disauerstoff. Zunächst jedoch erschien diese Tatsache zu eigenartig, als dass die einfache Deutung Schönbeins, eine Allotropie im Gaszustand, allgemeine Anerkennung gefunden hätte.

Die Abbaureaktionen von Ozon durch Stickoxide wurden 1970 erstmals von Paul Josef Crutzen (Nobelpreis für Chemie 1995) beschrieben.

Die Menge an Ozon in der Atmosphäre wird in Dobson-Einheiten (also pro Erdoberfläche) oder in ppm (also pro Stoffmenge Luft) angegeben. Die höchste Konzentration mit einigen ppm weist Ozon in der Stratosphäre auf. Für seine Entstehung ist dort der Ozon-Sauerstoff-Zyklus verantwortlich. Ozon ist in der Stratosphäre unschädlich und absorbiert teilweise die Ultraviolettstrahlung der Sonne. In der Atemluft ist es jedoch bereits in weit geringeren Konzentrationen gesundheitsschädlich, insbesondere bewirkt die lokal sehr unterschiedliche Ozonbelastung Reizungen der Atemwege.

Diese sehr unterschiedliche Gefahreneinschätzung in den verschiedenen atmosphärischen Schichtungen führt sehr häufig zu Verwechselungen und Unterschätzung der Gefahren. Das gesundheitliche Risiko des Ozons in den bodennahen Luftschichten ist durch seine Reaktivität begründet, Ozon ist eines der stärksten Oxidationsmittel.

In Reinluftgebieten ist die Ozon-Konzentration im Sommer oft höher als in Städten. Dies ist darauf zurückzuführen, dass durch die vielen Autoabgase in den Städten die Konzentration des Stickoxides NO sehr hoch sein kann. NO wirkt jedoch der Ozonbildung entgegen. Im Einzelnen laufen folgende Reaktionen ab:

Ozon entsteht wie folgt:

Gleichzeitig wird Ozon durch NO wieder abgebaut:

Wären also nicht noch weitere Stoffe, sogenannte flüchtige Kohlenwasserstoffe oder auch CO, in der unteren Luftschicht vorhanden, würde sich kein weiteres Ozon bilden, sondern abhängig von der Sonneneinstrahlung stellt sich dann ein Gleichgewicht zwischen O, NO und NO ein. Je stärker die Sonne scheint, desto mehr Ozon und weniger NO ist vorhanden, da letzteres durch die UV-Strahlung gespalten wird (Reaktion 1).

In der (verschmutzten) planetaren Grenzschicht der Atmosphäre finden sich auch Kohlenwasserstoffe, die sowohl vom Menschen (anthropogen) als auch von der Vegetation (biogen) emittiert werden. Sie werden von OH-Radikalen oxidiert, wobei Peroxid-Radikale R-O-O entstehen. Diese wiederum sorgen dafür, dass NO zu NO oxidiert wird, ohne dass dabei ein O verbraucht wird, wie in Reaktion 3, also:

Wenn danach wieder Reaktion 1 und 2 stattfinden, wird netto neues Ozon gebildet.

Da NO durch Autos und Industrie ausgestoßen wird, wird Ozon in der Stadt schneller wieder abgebaut (nach Reaktion 3) als in ländlichen Gegenden. Außerdem finden sich in ländlichen Gebieten häufig Kohlenwasserstoffe, die leichter von OH-Radikalen angegriffen werden können, wodurch Reaktion 4 schneller abläuft. Ein bekanntes Beispiel für so einen leicht abbaubaren biogenen Kohlenwasserstoff ist Isopren.
Die genaue Reaktionskette ist im Artikel Sommersmog beschrieben.

Die im Zusammenhang mit der Ozonschicht häufig erwähnten FCKW (Fluorchlorkohlenwasserstoffe) werden durch UV-Strahlung gespalten, wodurch freie Chlorradikale entstehen, die wiederum viele Ozon-Moleküle "„zerstören“" können.

Ozon entsteht aus gewöhnlichem Sauerstoff gemäß der Grundgleichung

Ozon bildet sich in der Atmosphäre vor allem auf drei Arten:

Beim Betrieb von Raumluftreinigungsgeräten kann es gezielt oder ungewollt zur Bildung von Ozon kommen. So bilden einige Ionisatoren Ozon, um geruchlich wahrgenommene Moleküle der Umgebungsluft zu spalten und zu eliminieren. Allerdings bergen die Abbauprodukte von Nikotin und Zigarettenrauch, neben dem Ozon selbst, hohe gesundheitliche Risiken, so dass z. B. die Deutsche Lungenstiftung davor warnt, den schlechten Geruch verrauchter Räume mit Ozon generierenden Luftreinigern zu beseitigen. Die Richtlinie VDI 6022 Blatt 5 „Raumlufttechnik, Raumluftqualität - Vermeidung allergener Belastungen - Anforderung an die Prüfung und Bewertung von technischen Geräten und Komponenten mit Einfluss auf die Atemluft“ empfiehlt daher, beim Einsatz von Ionisatoren gegebenenfalls die Ozon-Emissionsrate zu bestimmen.

Ozon kann auch beim Betrieb von elektrostatischen Abscheidern (Elektrofiltern), die zur Raumluftreinigung eingesetzt werden, entstehen. Dies ist insbesondere dann der Fall, wenn durch die negative Polung der Sprühelektrode eine negative Koronaentladung erzielt wird. Deshalb wird in der Regel bei raumlufttechnischen Anlagen von dieser Konstellation abgesehen.

Ozon kann ebenfalls beim Betrieb von Raumluftreinigungsgeräten entstehen, die gezielt nichtthermisches Plasma erzeugen. Die Menge des erzeugten Ozons hängt dabei von der Bauart und der Leistungsaufnahme des eingesetzten Geräts ab.

Bei älteren Fotokopierern sowie Laserdruckern kann man einen typischen „Ozongeruch“ wahrnehmen. Dieser Geruch rührt nur indirekt vom durch die Ionisation der Luft im Gerät gebildeten Ozon her; er kommt vielmehr durch Spuren nitroser Gase (NO) zustande, die durch Reaktion des Ozons mit dem Luftstickstoff gebildet werden. Das Funktionsprinzip der Geräte erfordert eine Ionisierung der Luft bei Spannungen von 5–15 kV. Meist besitzen die Geräte Ozonfilter, die das produzierte Ozon in Kohlendioxid umwandeln. Dennoch sollten diese Geräte möglichst nicht in unbelüfteten Räumen verwendet werden. Moderne Drucker und Fotokopierer arbeiten mit einer Transferrollentechnik, welche die Ozonbildung verhindert und die ältere Coronadrahttechnologie weitestgehend ersetzt hat.

Ozon kann aus der Reaktion von Kaliumpermanganat mit konzentrierter Schwefelsäure gewonnen werden. Das als Zwischenprodukt entstehende instabile Dimanganheptoxid MnO zerfällt bei Raumtemperatur zu Mangandioxid und Sauerstoff, der reich an Ozon ist.

Bei der Elektrolyse verdünnter Schwefelsäure (ca. 20 %) entwickelt sich an einer Gold- oder Platinanode besonders bei hohen Stromdichten Ozon. Bei guter Kühlung lassen sich so 4–5 % Ozongehalt im entstehenden Sauerstoff erreichen, eine Konzentration, die ausreicht, um alle Reaktionen des Ozons im präparativen Maßstab ausführen zu können. Über ausgefeilte Apparaturen (z. B. feine Platindrahtwendeln) und Kühlung auf −14 °C lassen sich noch erheblich höhere Ozonkonzentrationen erreichen.

Ozon lässt sich weiterhin aus Luftsauerstoff unter Einwirkung von Ultraviolettstrahlung oder stillen elektrischen Entladungen herstellen. Entsprechende, als Ozonisatoren bezeichnete Geräte gibt es im Handel.

Aufgrund seiner Instabilität kann Ozon nicht über längere Zeit gelagert oder wie andere industriell verwendete Gase in Druckflaschen gekauft werden. Vor seiner Anwendung (chemische Synthese, Wasseraufbereitung, als Bleichmittel etc.) muss es an Ort und Stelle erzeugt werden.

Zur Herstellung wird meistens getrocknete Luft oder Sauerstoff (Taupunkt mind. −65 °C) als Trägergas eingesetzt. In selteneren Fällen wird Sauerstoff mit Argon, Kohlenstoffdioxid u. ä. gemischt. Im Ozonerzeuger (Ozonisator) werden die Sauerstoffmoleküle durch stille elektrische Entladung zu Sauerstoffatomen dissoziiert, wonach noch im Plasma der Entladungsfilamente die Ozonsynthese und Ozonanreicherung stattfindet. In Luft bewegen sich typische Endkonzentrationen zwischen einem und fünf Prozent Massenanteil, in Sauerstoff zwischen sechs und dreizehn Prozent Massenanteil.

Aus reinem, trockenem Sauerstoff können bis zu 90 g·m, aus Luft (bei Kühlung) bis zu 40 g·m Ozon gewonnen werden. Für 1 kg Ozon aus Sauerstoff (im Bereich von 1–6 Gew-%) werden 7–14 kWh Strom und 1,8 m/h Kühlwasser verbraucht.

Die in der Praxis eingesetzten technischen Vorrichtungen können auf folgenden Elektrodenkonfigurationen basieren:
Bei Anlagen mit mehr als 20 kg Ozon pro Stunde werden üblicherweise nur Röhrenozonisatoren eingesetzt.

In erster Näherung ist die Ozonanreicherung eine Funktion des elektrischen Energieeintrags pro Gasvolumen. Zur Optimierung des Wirkungsgrades können folgende Parameter variiert werden:

Auch durch Überlagerung eines inhomogenen elektrischen Feldes während des Energieeintrags (Dielektrophorese) kann das chemische Gleichgewicht, welches sich aus Synthese und Zersetzung bei wenigen Gewichtsprozenten einstellt, zugunsten des Ozons verschoben werden.

Obwohl die Ozonbildung aus Sauerstoff unter
Wärmeabsorption erfolgt, sind Ozonerzeugerkessel in industriellen Anwendungen wassergekühlt, da fast 90 Prozent der eingetragenen Energie infolge der hohen Zersetzungsrate wieder abgeführt werden müssen. Für den Wirkungsgrad der Ozonsynthese ist die Gastemperatur ein weiterer dominierender Faktor.

Wegen der hohen Reaktivität von Ozon sind nur wenige Materialien gegen Ozon beständig. Dazu gehören Edelstahl (z. B. 316L), Glas, Polytetrafluorethylen (PTFE), Perfluoralkoxy-Polymere (PFA), Polyvinylidenfluorid (PVDF) und Perfluorkautschuk. Bedingt beständig ist Viton, das unter Ozon keiner mechanischen Wechselbelastung ausgesetzt werden darf.

Flüssiges Ozon lässt sich in Form einer 30 bis 75 %igen Lösung in flüssigem Sauerstoff bei −183 °C in Gegenwart von Stabilisatoren wie CClF, OF, SF oder andere ohne Explosionsgefahr lagern. Gasförmiges Ozon lässt sich im reinen Zustand (keine Verunreinigungen durch organische Verbindungen, Schwefel oder bestimmte Metalle) bei −112 bis −50 °C bei leichtem Überdruck recht gut lagern.

Ozon ist bei Standardbedingungen gasförmig. Aufgrund seiner oxidierenden Wirkung reizt es bei Menschen und Tieren die Atemwege. Es vermag sogar Silber bei Raumtemperatur zu oxidieren. Ozonaufnahme kann beim Menschen häufig zu heftigen Schläfenkopfschmerzen führen. In hohen Konzentrationen riecht das Gas aufgrund der oxidierenden Wirkung auf die Nasenschleimhaut charakteristisch stechend-scharf bis chlorähnlich, während es in geringen Konzentrationen geruchlos ist. Die Geruchsschwelle liegt bei 40 µg/m, allerdings gewöhnt man sich schnell an den Geruch und nimmt ihn dann nicht mehr wahr.
Reines O ist eine allotrope Form von Disauerstoff O. Bei Zimmertemperatur liegt es als instabiles, farbloses bis bläuliches, diamagnetisches Gas vor, das bei −110,5 °C zu einer tiefblauen Flüssigkeit kondensiert und bei −192,5 °C (80 K) zu einem schwarzvioletten Feststoff erstarrt.

Das gewinkelte polare Molekül mit einem Dipolmoment von 0,5337 D (entspricht 1,780 · 10 C · m) bleibt im Festkörper erhalten. Der O-O-Abstand beträgt 128 pm, der Winkel zwischen den drei Sauerstoffatomen beträgt 117°.

Ozon unterhält die Verbrennung sehr viel stärker als Disauerstoff: Etliche Materialien flammen schon bei Raumtemperatur bei Kontakt mit reinem Ozon auf. Gemische aus reinem Sauerstoff und Ozon ab einem Volumenanteil von 11,5 % können sich unter Atmosphärendruck bei entsprechend hoher Zündenergie explosionsartig zersetzen. Durch Beimischung von 1 % Methan oder NO wird die Zündgrenze auf ca. 5 % Ozon herabgesetzt.

Bei der Wasseraufbereitung dient Ozon unter anderem zur umweltfreundlichen Oxidation von Eisen, Mangan, organischer Substanz und zur Entkeimung.
So gehört eine Ozonierung in vielen Trinkwasserwerken zu den zentralen Aufbereitungsstufen (siehe Weblinks).

Oberflächenwasser kann in den wärmeren Jahreszeiten höhere Gehalte an Algen enthalten. Wird ein derartiges Wasser zu Brauchwasser für die Verwendung in der Industrie aufbereitet, so kann durch eine "Hochozonisierung" die Reinigungswirkung der Filteranlagen deutlich verbessert werden. Ozon tötet durch sein hohes Oxidationspotential sowohl Keime wie auch Algen weitgehend ab und verbessert die Abfiltrierbarkeit dieser feindispersen Verunreinigungen und damit die Reinigungswirkung.

Auch in der Behandlung von kommunalen und industriellen Abwässern kommt Ozon zum Einsatz (Kläranlage). Die Ozonierung wird dabei der üblichen Abwasserreinigung durch Mikroorganismen zusätzlich nachgeschaltet. Allerdings handelt es sich bei Kläranlagen mit Ozonanlagen meist um Pilotprojekte (wie zum Beispiel in Regensdorf-Watt in der Schweiz), denn die Produktion von Ozon in solch großen Maßstäben ist teuer, energieaufwändig und die Schutzmaßnahmen gegen den giftigen und ätzenden Stoff sind erheblich. Zurzeit wird diskutiert, ob die Abwasserreinigung durch die ungiftige Aktivkohle nicht ungefährlicher, billiger und umweltfreundlicher ist.

Ziele einer weitergehenden Ozonbehandlung des konventionell gereinigten Abwassers sind:
(b) oxidative Elimination/Transformation von nicht oder nur schlecht abbaubaren organischen Spurenstoffen (insbesondere Medikamentenrückstände).

Ein Nachteil der Ozonierung ist die Entstehung von unbekannten und möglicherweise giftigen Produkten, wenn Ozon mit Schadstoffen im Wasser reagiert. So wird die Bildung von krebserregenden Nitrosaminen vermutet. Des Weiteren werden einige Schadstoffe, zum Beispiel iodhaltige Röntgenkontrastmittel, von Ozon praktisch nicht abgebaut. Sie gelangen deshalb weiterhin in die Umwelt.

Ozon kann sehr gut in Verfahrenskombinationen mit nachfolgenden biologischen Systemen (Biofilter) eingesetzt werden, so beispielsweise bei der Oxidation des chemischen Sauerstoffbedarfs (CSB) zum biologischen Sauerstoffbedarf (BSB), der dann im Biofilter weiterverarbeitet wird. Ebenso findet Ozon in Fischkreisläufen in der Aquakultur oder Aquariensystemen Anwendung.

Bei den meisten „chlorfrei“ benannten Produkten oder Verfahren wird Ozon eingesetzt, so zum Beispiel beim Bleichen von Papier. In diesem Zusammenhang ist oft von „aktivem Sauerstoff“ die Rede.

Bei der oxidierenden Gaswäsche wird Ozon als Oxidationsmittel in Gaswäschern eingesetzt, um in der Waschflüssigkeit gelöste Substanzen chemisch umzusetzen und so das treibende Gefälle zu erhöhen zwischen zu reinigendem Gas und Waschflüssigkeit zu erhöhen. Dieses Verfahren findet bei reaktionsträgen organischen Stoffen und bei heterogenen Gasgemischen mit häufig geruchsintensiven Stoffen Anwendung. Alternativ besteht die Möglichkeit, schwer wasserlösliche Verunreinigungen mittels Ozon, das in den Abgasstrom geleitet wird, in höhere Oxidationsstufen überzuführen, die dann mit einem Gaswäscher entfernt werden können.

Eine sogenannte Ozonbehandlung wird in der professionellen Fahrzeugaufbereitung vorgenommen. Insbesondere bei Gebrauchtwagen mit Geruchsbelastung im Innenraum (z. B. ehemalige Raucherfahrzeuge) kann diese so beseitigt werden. Durch die oxidierende Wirkung des Ozons werden Geruchsstoffe in geruchsneutrale Stoffe umgewandelt. Ebenso werden Keime und geruchverursachende Bakterien dabei – auch an sonst unzugänglichen Stellen – abgetötet. Als Ergebnis ist das Fahrzeug nach dieser Behandlung desinfiziert und in der Regel geruchsfrei.

Einige moderne Waschmaschinen haben ein Ozonprogramm, welches Umgebungsluft nutzt und mittels eines Ozongenerators die Wäsche desinfiziert und Gerüche eliminiert. Auch Wäschereien nutzen diese Technik.

Die EU hat schon seit längerer Zeit Richtwerte für die Ozonkonzentration festgelegt. Keine Gefahr für die Gesundheit besteht laut EU-Richtlinie durch Ozon unter einem Gehalt von 110 µg/m. Ab einem Ein-Stunden-Mittelwert von 180 µg/m erfolgt die Unterrichtung der Bevölkerung, da bei dieser Konzentration die Leistungsfähigkeit empfindlicher Menschen bereits beeinträchtigt werden kann. Ab ungefähr 200 µg/m Ozon können Symptome wie Tränenreiz, Schleimhautreizungen in Rachen, Hals und Bronchien, Kopfschmerzen, verstärkter Hustenreiz, Verschlechterung der Lungenfunktion auftreten. Ab einem Ein-Stunden-Mittelwert von 360 µg/m werden Warnungen ausgesprochen, da ab dieser Konzentration Gefahr für die menschliche Gesundheit bestehen kann.

In der Schweiz liegt die Grenze des Ein-Stunden-Mittelwertes bei 120 µg/m (ca. 60 ppb). Dieser Wert wird jedoch sehr oft überschritten.

Eine langanhaltende Erhöhung der Ozonkonzentration in der Atemluft führt zu einem erhöhten Risiko, an Atemwegserkrankungen zu sterben. Eine 2018 veröffentlichte Studie zeigt einen Zusammenhang zwischen der Exposition mit Ozon sowie Feinstaub und der Alzheimer-Krankheit.<ref name="DOI10.1016/j.envres.2018.03.023">Lilian Calderón-Garcidueñas, Angélica Gónzalez-Maciel u. a.: "Hallmarks of Alzheimer disease are evolving relentlessly in Metropolitan Mexico City infants, children and young adults. APOE4 carriers have higher suicide risk and higher odds of reaching NFT stage V at ≤ 40 years of age." In: "Environmental Research." 164, 2018, S. 475, .</ref>

Erhöhte Immissionswerte treten vor allem im Einflussbereich von Industriegroßräumen und Autobahnen auf. Dabei wirken sich meteorologische Effekte stark auf die lokale Bildung und den Transport des Ozons aus, so dass räumliche Abhängigkeiten über mehrere Hundert Kilometer entstehen können.

Bei Hitzewellen nimmt die Konzentration zu, da Pflanzen weniger Ozon absorbieren können. Es wird geschätzt, dass dieser Effekt beispielsweise in Großbritannien während des Hitzesommers 2006 für 450 zusätzliche Tote verantwortlich war.

Ozon hat adverse Effekte auf Pflanzen und deren Wachstum. So sinken die Konzentrationen von Chlorophyll, Carotinoiden und Kohlenhydraten, während bei der Aminocyclopropancarbonsäure eine Erhöhung eintritt und vermehrt Ethen gebildet wird. In Zitruspflanzen konnte gezeigt werden, dass eine erhöhte Exposition mit Ozon Schutzmaßnahmen gegen oxidativen Stress auslöste. Über längere Zeit anhaltende hohe Ozonbelastungen können besonders Laubbäume, Sträucher und Kulturpflanzen schädigen und deren Wachstum vermindert, so dass es zu Ertragseinbußen kommen kann.


Ozon-Konzentrationen wurden früher und werden in den USA weiterhin überwiegend in ppb (also Milliardstel Volums-, Teilchen- oder Partialdruck-Anteilen) und werden SI-konform in µg/m angegeben. 1 ppb Ozon entspricht 2,15 µg/m (unter Normalbedingungen).

Ozon in der Außenluft kann photometrisch erfasst werden. Dazu wird die kontinuierlich angesaugte Probenluft durch eine Messküvette geleitet, die mit monochromatischer Strahlung einer bestimmten Wellenlänge beaufschlagt wird. Die durchtretende und somit nicht absorbierte Strahlung wird mittels Photodiode oder Photomultiplier gemessen und gibt damit Auskunft über die Ozonkonzentration in der Luft. Dieses Messverfahren beruht auf dem Lambert-Beerschen Gesetz.

Ein anderes Verfahren zur messtechnischen Erfassung von Ozon in der Außenluft ist das Kaliumiodid-Verfahren: In wässriger Lösung reagiert Ozon mit Kaliumiodid unter Freisetzung von Iod und Sauerstoff. Die Extinktion der Iodlösung ist ein Maß für die Ozonkontration der Probenluft, die durch die Kaliumiodidlösung geleitet wurde. Das Verfahren ist nicht selektiv bezüglich Ozon. Als Absorptionsgefäße sind Muenke-Waschflaschen zu verwenden.

Auch die differenzielle optische Absorptionsspektroskopie DOAS wird zur Ozonmessung eingesetzt. Untersuchungen zur Qualitätssicherung unterschiedlicher Messmethoden liegen ebenfalls vor.

Problematisch bei der Immissionsmessung von Ozon ist, dass keine haltbaren Prüfgase hergestellt werden können. Zudem ist darauf zu achten, dass die eingesetzten Werkstoffe nicht mit dem Ozon reagieren können.

Wirkungen von Ozon können mit Tabakpflanzen systematisch untersucht werden. Zur Bioindikation werden die makroskopisch erkennbaren Blattschäden an der Pflanze als Wirkungsmessgröße herangezogen.


Als Luftschadstoff:

Messung und Vorhersage:


</doc>
<doc id="3723" url="https://de.wikipedia.org/wiki?curid=3723" title="Onlinespiel">
Onlinespiel

Onlinespiele (häufig auch bekannt als Internetspiele) sind Computerspiele, die online über ein Wide Area Network, heute üblicherweise das Internet, gespielt werden.

In der Anfangszeit in den 1970er und 1980er Jahren bestanden die Onlinespiele vor allem aus Text-Adventures, MUDs und bekannten Brettspielen (Schach, Go usw.), frühe Spiele liefen noch über Mailboxen oder andere nicht-Internet Verbindungen.

Mit "Ultima Online" etablierte sich 1997 erstmals ein grafisches MMORPG, bei dem mehrere tausend Spieler gleichzeitig online sein können.

Inzwischen gibt es Spiele, in denen zehntausende von Spielern – meist über mehrere Server oder Cluster verteilt – interagieren.

Onlinespiele lassen sich technisch in zwei Kategorien unterteilen: Zum einen gibt es Browser-basierte Onlinespiele (Browserspiele, Social Network Games), Single- oder Multiplayer, die entweder auf reinem HTML-Code basieren oder zusätzliche Browser-Plug-Ins (z. B. Flash oder Java) benötigen.

Zum Anderen gibt es Client-basierte Multiplayer-Onlinespiele. Diese setzen die Installation einer Client-Software voraus. 
Die Client-Software verbindet sich dann entweder mit anderen Clients – diese Peer-to-Peer-Architektur ist besonders bei Strategie- und Actionspiele für kleine Spielerzahlen verbreitet – oder sie stellt eine Verbindung zu einem Spielserver her. So funktionieren die meisten Online-Ego-Shooter und sämtliche MMOGs.

Es sind zahlreiche weitere Unterteilungen möglich, z. B. nach Kosten und Genre. Inzwischen verfügen viele Spiele aller Arten über – teils optionale – Online-Elemente.

Onlinespiele verwenden auf Serverseite häufig eine Datenbank, um die Spielinformationen zu speichern und so Persistenz zu gewährleisten. Die vom Client zum Server übertragenen Daten sind meist verschlüsselt, und es wird in den Nutzervereinbarungen verboten sie zu dekodieren, um das Spiel vor Hacks zu schützen.

Sehr viele Benutzer gleichzeitig mit einem Server zu verbinden stellt immer noch eine technische Herausforderung dar. Daher werden in MMOs normalerweise die Spieler auf mehrere Server verteilt. Der Übergang zwischen den einzelnen Servern ist teils möglich, teils nicht.





</doc>
<doc id="3724" url="https://de.wikipedia.org/wiki?curid=3724" title="Organisation für wirtschaftliche Zusammenarbeit und Entwicklung">
Organisation für wirtschaftliche Zusammenarbeit und Entwicklung

Die Organisation für wirtschaftliche Zusammenarbeit und Entwicklung (, "OECD"; , "OCDE") ist eine internationale Organisation mit 35 Mitgliedstaaten, die sich der Demokratie und Marktwirtschaft verpflichtet fühlen. Die meisten OECD-Mitglieder gehören zu den Ländern mit hohem Pro-Kopf-Einkommen und gelten als entwickelte Länder. Sitz der Organisation und ihrer Vorgängerorganisation, der Organisation für europäische wirtschaftliche Zusammenarbeit (englisch "Organisation for European Economic Co-operation" "OEEC") ist seit 1949 Schloss La Muette in Paris.

Die OECD wurde 1961 als Nachfolgeorganisation der Organisation für europäische wirtschaftliche Zusammenarbeit (englisch "Organisation for European Economic Co-operation" "OEEC") und des Marshallplans zum Wiederaufbau Europas gegründet, die ab dem 16. April 1948 agierten. Das Ziel der OEEC war, ein gemeinsames Konzept zum wirtschaftlichen Wiederaufbau und zur Zusammenarbeit in Europa zu erarbeiten und umzusetzen. Insbesondere sollten die europäischen Länder in den Entscheidungsprozess über die Verwendung der Gelder aus dem Marshallplan eingebunden werden. Nach Abwicklung der Marshallplanhilfe wurde weiterer Bedarf für einen Austausch über wirtschaftspolitische Fragen gesehen und die OEEC im September 1961 in die OECD überführt.

In den ersten Jahren ihres Bestehens zählte die OEEC 20 Mitglieder (18 europäische Staaten sowie die USA und Kanada). In den 1960er Jahren traten in die nun OECD genannte Organisation Italien (1962), Japan (1964) und Finnland (1969) bei, es folgten Australien (1971) und Neuseeland (1973), in den 1990er Jahren kamen Mexiko (1994), Tschechien (1995), Ungarn (1996), Südkorea (1996), Polen (1996) und die Slowakei (2000) hinzu und 2010 traten Chile, Slowenien, Israel sowie Estland bei, 2016 folgte Lettland.

Heute versteht sich die OECD als Forum, in dem Regierungen ihre Erfahrungen austauschen, "best practice" identifizieren und Lösungen für gemeinsame Probleme erarbeiten. In der Regel ist Gruppenzwang der wichtigste Anreiz für die Umsetzung der erarbeiteten Empfehlungen. Häufig werden im Rahmen der OECD auch Standards und Richtlinien erarbeitet, gelegentlich auch rechtlich verbindliche Verträge.

Laut OECD-Konvention sind die Ziele der Organisation

Die Analysen und Empfehlungen der OECD zur Wirtschaftspolitik der Mitgliedstaaten orientieren sich an einer liberalen, marktwirtschaftlichen und effizienten Wirtschaftsordnung. Für die Arbeits- wie für die Produktmärkte spricht sich die Organisation für den Abbau von Schranken und für mehr Wettbewerb aus. In den vergangenen Jahren haben Bildungspolitik und Sozialpolitik an Gewicht gewonnen. So hat sich die OECD mit den PISA-Studien zu einem Fürsprecher von Chancengleichheit im Bildungssystem gemacht. 2016 hat die Organisation in einer Studie auf eine Zunahme von Armut und Ungleichheit (Einkommensschere) in ihren Mitgliedstaaten hingewiesen.

In den internationalen Wirtschaftsbeziehungen sind der freie Waren- und Kapitalverkehr Kernziele der Organisation. Gleichzeitig wurden und werden im Rahmen der OECD Standards erarbeitet, um negativen Seiten der Globalisierung entgegenzutreten. Dazu gehören die OECD-Leitsätze für multinationale Unternehmen als Standards für Direktinvestition und die Zusammenarbeit mit Zulieferern, die OECD-Konvention gegen Bestechung ausländischer Amtsträger sowie Standards zur Verhinderung von Geldwäsche und Steuerflucht.

Die OECD ist keine supranationale (überstaatliche) Organisation, sondern hat eher den Charakter einer permanent tagenden Konferenz. Die Organisation ist strikt intergouvernemental (zwischenstaatlich) verfasst, ihre Beschlüsse sind völkerrechtlich bindend, in den Mitgliedstaaten aber nicht unmittelbar anwendbar.

Der obere Rat ist das oberste Entscheidungsorgan und setzt sich zusammen aus je einem Vertreter der Mitgliedstaaten und der Europäischen Kommission. Er tagt regelmäßig auf Botschafterebene. Mindestens einmal jährlich findet ein Treffen auf Ministerebene statt, um das Arbeitsprogramm der Organisation festzulegen. Beschlüsse werden im Konsens gefasst. Länder können sich aber enthalten. Macht ein Land von dieser Möglichkeit Gebrauch, muss es die betreffende Empfehlung nicht anwenden.

Der Generalsekretär führt den Vorsitz im Rat, wenn dieser auf Botschafterebene tagt. Gleichzeitig untersteht ihm das Sekretariat. Er wird für fünf Jahre von den Mitgliedstaaten im Einvernehmen ernannt. Amtsinhaber ist seit Juni 2006 der ehemalige mexikanische Finanz- und Außenminister José Ángel Gurría. Derzeit wird der Generalsekretär von vier stellvertretenden Generalsekretären unterstützt.

Generalsekretäre der OEEC/OECD:

Das Sekretariat setzt die Beschlüsse des Rates um, unterstützt die Ausschüsse und Arbeitsgruppen in ihrer Arbeit und erarbeitet Vorschläge für neue Aktivitäten. Von den rund 2500 Mitarbeitern sind etwa 1600 Experten, zumeist Ökonomen, Juristen, Natur- oder Sozialwissenschaftler. Das Sekretariat ist in zwölf inhaltliche Direktionen und sechs Zentralabteilungen gegliedert. Die meisten Bediensteten arbeiten am Hauptsitz in Paris. Verbindungsbüros unterhält die OECD in Berlin, Mexiko-Stadt, Tokio und Washington, D.C.

In den rund 200 Ausschüssen und Arbeitsgruppen findet die Facharbeit der Organisation statt. Delegierte aus den Ministerien und Behörden der Mitgliedstaaten tauschen sich hier aus, diskutieren die Arbeit des Sekretariats oder liefern eigene Beträge. Etwa 40.000 Vertreter aus nationalen Verwaltungen nehmen jährlich an solchen OECD-Arbeitstreffen teil. An vielen dieser Gremien nehmen auch Vertreter von Nicht-Mitgliedern als Beobachter teil.

Die Organisation finanziert sich aus Beiträgen der Mitgliedstaaten, also letztlich aus Steuermitteln. Das Zentralbudget (2016: 370 Mio. €) wird nach einem von der Wirtschaftskraft abhängigen Beitragsschlüssel von den Mitgliedern getragen. Mit 20,93 Prozent waren die USA 2016 der größte Beitragszahler, gefolgt von Japan (10,79 Prozent) und Deutschland (7,52 Prozent). Die Schweiz trägt 2,0 Prozent und Österreich 1,42 Prozent zum Zentralbudget bei. Darüber hinaus können die Mitgliedstaaten über freiwillige Beiträge zusätzlich Projekte finanzieren. In diesem Rahmen werden auch die Kosten für die PISA-Studie getragen.

Über besondere Beratungsgremien gibt es einen institutionalisierten Austausch mit Vertretern der Wirtschaft (BIAC) und der Arbeitnehmer (TUAC). Darüber hinaus finden zu einzelnen Vorhaben öffentliche Anhörungen statt, und Nicht-Regierungsorganisationen werden von verschiedenen Ausschüssen und Arbeitsgruppen in die Arbeit einbezogen. Das jährlich rund um das Ministerratstreffen stattfindende OECD Forum soll darüber hinaus einem regelmäßigen Austausch mit der Zivilgesellschaft dienen.

Zur OECD gehören eine Reihe von Sonder- und Tochterorganisationen mit eigenem Mitgliederkreis und eigenen Aufsichts- und Steuerungsgremien:

Derzeit gibt es 35 Mitglieder


Im Unterschied zu vielen anderen internationalen Organisationen steht die Mitgliedschaft in der OECD nicht automatisch allen Ländern offen. Nach Beitrittsverhandlungen entscheiden die OECD-Mitglieder, ob und unter welchen Bedingungen ein Land aufgenommen wird.

Am 16. Mai 2007 hat die OECD Chile, Estland, Israel, Russland und Slowenien zu Beitrittsgesprächen eingeladen. Außerdem wurde mit den großen Schwellenländern Brasilien, Volksrepublik China, Indien, Indonesien und Südafrika eine „verstärkte Zusammenarbeit mit Blick auf eine mögliche Mitgliedschaft“ vereinbart. Diese sowie 70 weitere Staaten arbeiten schon heute in den verschiedenen Ausschüssen und Arbeitsgruppen der OECD mit.

Seit Mitte 2007 findet bei der OECD ein Dialog zwischen den G8-Staaten und den großen Schwellenländern Brasilien, China, Indien, Mexiko und Südafrika statt. Dieser Heiligendamm-Prozess wurde auf dem G8-Gipfel in Heiligendamm vereinbart und soll zu den Themen Investitionen, Energieeffizienz und Klimaschutz, Schutz geistigen Eigentums und Entwicklungspolitik zu einer Verständigung zwischen den großen Industrie- und Schwellenländern beitragen.

Am 7. Mai 2010 vollzog Chile als erster Staat Südamerikas den Beitritt zur OECD und am 27. Mai 2010 wurden Estland, Israel und Slowenien eingeladen, der Organisation beizutreten. Am 21. Juli 2010 vollzog Slowenien, am 7. September 2010 Israel und am 9. Dezember 2010 Estland den Beitritt.

Derzeit (2018) führt die OECD Beitrittsgespräche mit drei weiteren Ländern: seit Mai 2013 mit Kolumbien und seit April 2015 mit Costa Rica und Litauen. Die Beitrittsgespräche mit der Russischen Föderation wurden im März 2014 vorläufig ausgesetzt.. Des Weiteren haben bereits Malaysia und Peru Interesse an einer Mitgliedschaft bekundet.

Am 30. Mai 2017 hat Brasilien den Antrag auf Beitritt zur OECD gestellt.

Die Arbeit der OECD ist sehr breit gefächert und berührt abgesehen von der Verteidigungspolitik fast alle Bereiche des staatlichen Handelns. Die Organisation selbst teilt ihre Tätigkeit in die sieben Kategorien Wirtschaft, Gesellschaft, Innovation, Finanzen, Governance, Nachhaltigkeit sowie Entwicklung. Diese Kategorien sind in insgesamt 26 Unterthemen gegliedert.

Die Organisation analysiert und vergleicht die Alterssicherungssysteme der Mitgliedstaaten. Von zentraler Bedeutung sind dabei die alle zwei Jahre erscheinenden Modellrechnungen zur Altersrente im Verhältnis zum Einkommen während der Erwerbsphase. Auf dieser Basis und angesichts einer Zunahme prekärer Erwerbsverhältnisse und unterbrochener Erwerbsbiografien hat die Organisation wiederholt vor der Gefahr von Altersarmut in Deutschland gewarnt.

Die Analysen konzentrieren sich auf effektive Gestaltung der Arbeitsmarktpolitik. Grundlage dafür sind unter anderem Statistiken zur Erwerbsbeteiligung und Indikatoren zum Verhältnis von Arbeitslohn und Lohnersatzleistungen. Der jährlich erscheinende OECD-Beschäftigungsausblick gibt einen Überblick über die Entwicklung der Beschäftigung und fasst aktuelle Studien der Organisation zur Arbeitsmarktpolitik zusammen. Insgesamt hat die Organisation in den vergangenen Jahren einen deutlichen Kurswechsel in der Arbeitsmarktpolitik vollzogen. So wurde Mitte der 1990er Jahre noch eine Liberalisierung der Arbeitsmärkte mit Abbau von Kündigungsschutz, Einschränkung von Gewerkschaftsmacht und Kürzung von Arbeitslosenunterstützung nach angelsächsischem Modell propagiert. Mit der revidierten „Job Strategy“ von 2006 erkennt nun neben dem angelsächsischen auch das skandinavische Modell der Arbeitsmarktpolitik mit geringem Kündigungsschutz aber gute Absicherung bei Arbeitslosigkeit und aktiver Vermittlung in den Arbeitsmarkt als erfolgversprechend an.

Der ökonomische Nutzen von Bildung für den Einzelnen und die Gesellschaft sowie Chancengleichheit im Bildungssystem stehen in der bildungspolitischen Arbeit im Vordergrund. In der jährlich erscheinenden Publikation Bildung auf einen Blick veröffentlicht die OECD vergleichende Statistiken und Indikatoren zum Ressourceneinsatz in Form von Finanzmitteln oder Personalausstattung in nationalen Bildungssystemen und analysiert, wie sich Bildung auf Innovationskraft und Arbeitsmarkt auswirken. Mit der PISA-Studie hat die Organisation sich international einen Namen bei der Messung von nach bestimmten Kriterien entwickelten Leistungsdaten 15-Jähriger gemacht. Die PISA-Studie ist "keine" Untersuchung der Leistungsfähigkeit von Schulsystemen, wenngleich dies in der Öffentlichkeit so wahrgenommen wurde. Ähnliche Studien zur Untersuchung des Kompetenzstandes von Erwachsenen und Hochschulabsolventen sind in Arbeit beziehungsweise in Vorbereitung. Darüber hinaus erforscht die Organisation, wie das Management in Schule und Hochschule verbessert werden kann.

Zentraler Bestandteil der Arbeit in diesem Bereich sind Statistiken und Berichte über die Entwicklungshilfezahlungen der OECD-Länder im Ausschuss für Entwicklungshilfe (DAC). In jährlichen Berichten wird überprüft ob die öffentliche Entwicklungshilfe (ODA) den gemachten Zusagen entspricht. In den vergangenen Jahren haben mit Arbeiten rund um die Paris Declaration on Aid Effectivness Analysen zu einem effizienteren Einsatz von Entwicklungshilfe zugenommen. Daneben berichtet die Organisation regelmäßig über die wirtschaftliche Entwicklung in Afrika und Lateinamerika.

Im Kampf gegen Korruption ist die OECD einer der zentralen internationalen Akteure. Sie gründete 1989 eine ad hoc working group, die rechtzeitig zur Gründung des EG-Binnenmarktes (Maastricht 1993) eine „Recommendation on Combating Bribery in International Business Transactions“ vorlegte und die 1994 vom OECD-Rat auf Ministerialebene verabschiedet wurde. Daraus resultierte die 1997 von allen OECD-Staaten (plus fünf weiterer) unterschriebene und 1999 in Kraft getretene OECD-Konvention gegen Bestechung ausländischer Amtsträger, wodurch nach US-amerikanischem Beispiel (FCPA 1977) die Bestechung ausländischer Amtsträger auch im Herkunftsland unter Strafe gestellt und verfolgt wird. Außerdem wurde die steuerliche Absetzbarkeit für Bestechungszahlungen abgeschafft, die bis dahin vor allem auch in Deutschland gegolten hatte, wo zudem die Namen der Empfänger, vor der Strafverfolgung geschützt, vertraulich blieben. Gleichzeitig unterstützt die Organisation Mitgliedstaaten und Nichtmitglieder im Rahmen von regionalen Initiativen die Anfälligkeit gegen Korruption zu verringern.

Migration wird aus der Sicht der Ziel- wie der Herkunftsländer analysiert. Aus Sicht der Zielländer steht die Integration von Migranten in den Arbeitsmarkt und die Sozialstruktur im Vordergrund. Aus Sicht der Herkunftsländer werden die wirtschaftlichen Folgen von Migration etwa durch Rücküberweisungen oder den Verlust an Fachkräften analysiert.

Die Arbeiten im Umweltschutz sollen helfen, eine effiziente und effektive Politik zur Bewältigung von Umweltproblemen und zur nachhaltigen Bewirtschaftung von Naturressourcen zu konzipieren und umzusetzen. In Länderberichten erarbeitet die Organisation konkrete Empfehlungen zur Verbesserung der Umweltpolitik. Im Jahr 2008 hat die OECD eine umfassende Analyse zu den großen Herausforderungen in der Umweltpolitik vorgelegt.

Anlässlich des Anfang Oktober 2013 erschienenen OECD-Berichts „Climate and carbon: Aligning prices and policies“ sprach sich Generalsekretär Gurria dafür aus, die CO-Bepreisung (z. B. durch eine CO-Steuer oder Emissionsrechtehandel) zum Eckpfeiler der internationalen Klimapolitik zu machen. Um das Zwei-Grad-Ziel zu erreichen, müssten die CO-Emissionenen durch die fossile Energiegewinnung bis zur zweiten Hälfte dieses Jahrhunderts auf null reduziert werden.

Im Bereich Steuern und Steuerpolitik hilft die OECD den Mitgliedstaaten, ihre Steuersysteme an die Bedingungen der globalisierten Wirtschaft anzupassen. Die Organisation veröffentlicht unter anderem Statistiken zum Steueraufkommen in den OECD-Ländern sowie Indikatoren zur Steuer- und Abgabenlast auf Arbeitseinkommen. Diese sind die Grundlage für Analysen und Empfehlungen für eine wachstumsfördernde Steuer- und Fiskalpolitik. Zur Koordinierung der grenzüberschreitenden Besteuerung erarbeitet die OECD Referenzwerke wie das OECD-Musterabkommen und die Richtlinien für Verrechnungspreise. Standards zum internationalen Informationsaustausch in Steuersachen sollen helfen, grenzüberschreitende Steuerhinterziehung einzudämmen. Zudem erarbeiten die Mitgliedsstaaten in Zusammenarbeit mit den G20 im so genannten BEPS-Projekt ("Base Erosion and Profit Shifting") internationale Standards gegen Gewinnverkürzung und -verlagerung durch multinationale Unternehmen.

Mit einer Reihe von Standards versucht die Organisation eine verantwortungsvolle Unternehmensführung zu etablieren. Die OECD-Leitsätze zur Unternehmensführung stellen den wichtigsten internationalen Standard zum Aktien- und Unternehmensrecht dar. Die OECD-Leitsätze für multinationale Unternehmen setzen Standards bei Auslandsinvestitionen und in den Beziehungen zu Zulieferern.

Die Analysen zur Wirtschaftspolitik sind in Konjunktur- und Strukturpolitik gegliedert. Zweimal jährlich veröffentlicht die OECD in ihrem Wirtschaftsausblick eine Konjunkturprognose für alle OECD-Länder und große Schwellenländer. Diese Prognose wird durch eine Zwischenbewertung für die großen Wirtschaftsräume sowie die G7-Staaten ergänzt. Darüber hinaus werden alle eineinhalb Jahre für jedes OECD-Land und einige Nicht-Mitglieder umfassende Wirtschaftsberichte mit konkreten wirtschaftspolitischen Empfehlungen erarbeitet. Diese Berichte sind Teil der in der OECD üblichen Peer Reviews, denn die Empfehlungen spiegeln den Konsens der Mitgliedstaaten wider.

Die Länderberichte werden von den Gewerkschaften der betreffenden Länder regelmäßig zurückgewiesen, insbesondere was die Arbeitsmarktpolitik angeht, weil sie zu unspezifisch nicht die besonderen, historisch gewachsenen Gegebenheiten berücksichtigten und politisch einseitig von einem neoliberalen Bewertungsschema ausgehen.

Weitere Themen der Organisation sind Biotechnologie, Bürokratieabbau, Energie, Gesundheit, Handel, Innovation, Investitionen, Landwirtschaft, Öffentliche Verwaltung, Räumliche Entwicklung und Wettbewerbspolitik.

Ein wesentlicher Teil der Arbeit der OECD besteht in der Sammlung und Aufbereitung von Statistiken und Indikatoren sowie in der Erarbeitung von Studien. Rund 300 Titel veröffentlicht die Organisation pro Jahr. Alle Datenbanken und Studien werden in der Onlinebibliothek OECD iLibrary zur Verfügung gestellt.

Ein Überblick über wichtige Strukturdaten findet sich im jährlich erscheinenden OECD-Factbook. Die meisten Daten sind mittlerweile über die (kostenpflichtige) Plattform OECD.Stat erhältlich. Eine kostenfreie Auswahl steht mit OECD-Stat Extracts zur Verfügung.





</doc>
<doc id="3725" url="https://de.wikipedia.org/wiki?curid=3725" title="Organisation für Sicherheit und Zusammenarbeit in Europa">
Organisation für Sicherheit und Zusammenarbeit in Europa

Die Organisation für Sicherheit und Zusammenarbeit in Europa (OSZE; , OSCE) ist eine verstetigte Staatenkonferenz zur Friedenssicherung. Am 1. Januar 1995 ging sie aus der Konferenz über Sicherheit und Zusammenarbeit in Europa (KSZE) hervor, welche am 1. August 1975 mit der Schlussakte von Helsinki gegründet worden war. Die OSZE besteht aus folgenden 57 Teilnehmerstaaten:
Der Sitz des Generalsekretariats und der wichtigsten Gremien ist Wien mit der Hofburg sowie seit 2007 auch dem Palais Pálffy an der Wallnerstraße (Hauptsitz).

Die Ziele der OSZE sind die Sicherung des Friedens und der Wiederaufbau nach Konflikten. Sie sieht sich selbst als stabilisierenden Faktor in Europa. Als regionale Abmachung nach Kapitel VIII der Charta der Vereinten Nationen soll die OSZE nach dem Subsidiaritätsprinzip als erster internationaler Ansprechpartner bei Konflikten innerhalb ihres Wirkungsbereiches dienen. Sie wird als System kollektiver Sicherheit angesehen und steht damit durchaus in Konkurrenz zur NATO, die allerdings deutlich militärischer ausgerichtet ist. Nach dem Prinzip „OSZE zuerst“ arbeitet sie auch mit Internationalen Organisationen zusammen. Bedingt durch das ergebnislose Gipfeltreffen 2010 blieb die Frage einer künftigen Zielsetzung der OSZE offen.

Die Aktivitäten der OSZE gliedern sich in drei Themenbereiche („Dimensionen“), die auf die drei Körbe der Schlussakte von Helsinki zurückgehen. Diese sind die Politisch-Militärische Dimension, die Wirtschafts- und Umweltdimension und die Humanitäre (Menschenrechts-) Dimension.


Die Generalsekretäre der OSZE:

Der Generalsekretär der OSZE hat die folgenden Aufgaben:



Das Büro für demokratische Institutionen und Menschenrechte (BDIMR, mit der englischen Abkürzung "ODIHR" bezeichnet) in Warschau ist die „Hauptinstitution der Menschlichen Dimension“ (Korb III) der OSZE. Ursprünglich war das "Büro für Freie Wahlen" (eine Institution für internationale Wahlbeobachtung) die Komponente der Menschlichen Dimension im Institutionenpaket, über das auf dem Pariser Gipfel der KSZE 1990 verhandelt werden sollte.

Seine erste Aufgabe bestand darin, die Wahlen in den ehemaligen Ostblock-Staaten Mittel- und Osteuropas sowie in den zentralasiatischen Republiken der ehemaligen Sowjetunion zu beobachten. Mit dem Helsinki-Dokument von 1992 wird das ODIHR weiter gestärkt, Norwegen ließ den Begriff der Demokratisierung und Menschenrechte in den Institutionentitel aufnehmen.

In der Folge organisiert das ODIHR alle zwei Jahre ein Implementierungstreffen in Warschau, das die Einhaltung der OSZE-Verpflichtungen aus Korb III überwacht und an dem neben den OSZE-Teilnehmerstaaten auch andere zwischenstaatliche Organisationen und Nichtregierungsorganisationen teilnehmen. Darüber hinaus organisiert es Seminare, unterstützt die Missionen der OSZE und den Aufbau demokratischer Strukturen durch vielfältige andere Maßnahmen, sammelt Informationen und stellt sie zur Verfügung und publiziert Anleitungen. Weiterhin macht die Wahlbeobachtung einen großen Teil der Aktivitäten aus.

Die Wirtschafts- und Umweltdimension geht auf den zweiten sog. „Korb“ von Helsinki (Zusammenarbeit in den Bereichen Technologie, Wissenschaft, Wirtschaft und Umwelt) zurück. In der Wirtschafts- und Umweltdimension kümmert sich die Organisation unter anderem um die Bekämpfung von Korruption, Geldwäsche, Finanzierung des Terrorismus, organisierter Kriminalität, sowie Internetkriminalität. Außerdem fördert die OSZE Zusammenarbeit im Umweltbereich, der Wasserverwaltung, Migrationsfragen und Energie.

Der Posten des Hohen Kommissars für nationale Minderheiten (HKNM) wurde auf dem Gipfel 1992 in Helsinki geschaffen. Das Büro des HKNM befindet sich in Den Haag und beschäftigt etwa 10 Mitarbeiter.

Geprägt wurde dieses Amt der stillen Diplomatie seit 1992 durch den Niederländer Max van der Stoel, der 2001 von dem Schweden Rolf Ekéus abgelöst wurde. Von 2007 bis 2013 hatte der ehemalige norwegische Außenminister Knut Vollebaek das Amt des HKNM inne. Am 20. August 2013 hat die ehemalige Europaparlamentarierin und finnische Ministerin für Migration und europäische Angelegenheiten Astrid Thors ihre Amtsperiode als HKNM angetreten. Das Amt soll Spannungen, die den Frieden, die Stabilität oder die guten Beziehungen zwischen den OSZE-Teilnehmerstaaten gefährden könnten und sich aus ethnischen Spannungen entwickeln, erkennen und lösen. Sein Mandat erlaubt dem Hohen Kommissar (High Commissioner on National Minorities, HCNM) das frühe Eingreifen, also die Präventivdiplomatie.

Das Mandat des HKNM ist im Vergleich zu den bisherigen Instrumenten der Konfliktbekämpfung innovativ, da es die zwischenstaatliche Ebene verlässt und so ein direktes Ansetzen im betroffenen Staat ermöglicht. Der HKNM dient der Frühwarnung bei Spannungen in Bezug auf nationale Minderheiten, und er kann im Zuge seines Engagements zum Ergreifen von Frühmaßnahmen vom Hohen Rat ermächtigt werden.

Schließlich wird mit der Entscheidung 193 auf der Sitzung des Ständigen Rats am 5. November 1997 als jüngste dieser drei unabhängigen Institutionen das Amt des Beauftragten für Medienfreiheit (Representative on Freedom of the Media, RFOM) mit Sitz in Wien eingerichtet.

Die Schaffung der Institution des OSZE-Beauftragten für Medienfreiheit geht auf eine deutsche Initiative zurück. Sie beruht auf der Anerkennung der besonderen Bedeutung von OSZE-Verpflichtungen hinsichtlich der Freiheit der Meinungsäußerung und der Rolle freier und pluralistischer Medien. Der Auftrag für die Schaffung der neuen Institution erging durch den OSZE-Gipfel, der im Jahr 1996 in Lissabon stattfand. Das Mandat wurde durch den Ministerrat in Kopenhagen (Dezember 1997) verabschiedet, durch den auch die Ernennung von MdB a. D. Freimut Duve zum ersten OSZE-Beauftragten für Medienfreiheit erfolgte. Sein Nachfolger war von März 2004 bis März 2010 (ebenfalls für die zulässige Dauer von zwei Amtszeiten) der Ungar Miklós Haraszti. Im Juni 2017 wurde Harlem Désir aus Frankreich zum OSZE-Medienbeauftragten ernannt.

Der Medienbeauftragte hat vergleichbar dem Hohen Kommissar für nationale Minderheiten der OSZE eine Frühwarnfunktion. Er wird tätig bei Einschränkungen der Medienfreiheit, die in der Regel Anzeichen einer konfliktträchtigen politischen Entwicklung sind. Bei Verdacht auf ernste Verstöße gegen OSZE-Prinzipien hat der Medienbeauftragte die Möglichkeit, direkte Kontakte mit dem Teilnehmerstaat und anderen Parteien aufzunehmen und den Sachverhalt zu beurteilen sowie dem Teilnehmerstaat Hilfestellung zu leisten und zur Lösung des Problems beizutragen.

Nicht unmittelbar zur OSZE gehörig, jedoch an die Organisation in Wien angebunden, ist die OSCC, die für die Umsetzung des Vertrags über den Offenen Himmel (Open Skies) verantwortlich ist.

Vorläufer der OSZE war die Konferenz über Sicherheit und Zusammenarbeit in Europa (KSZE), die auf eine Initiative des Warschauer Paktes hin zustande kam. Ab den 1950er Jahren hatte die Sowjetunion eine derartige Konferenz gefordert, aber die Westmächte, allen voran Westdeutschland, hatten dies abgelehnt. Bonn befürchtete, aus solchen Gesprächen könne auch eine internationale Akzeptanz der deutschen Teilung entstehen. Erst die neue Ostpolitik der sozial-liberalen Koalition unter Bundeskanzler Willy Brandt (SPD) Anfang der 1970er brachte den Gedanken einer KSZE im Westen auf die Tagesordnung. Unter Brandts Motto „Wandel durch Annäherung“ wurde die eisige Stimmung des Kalten Krieges aufgelockert und die KSZE ermöglicht. Die erste dieser multinationalen Konferenzen fand von 1973 bis 1975 in Helsinki statt. Teilnehmer der blockübergreifenden Konferenz waren alle europäischen Staaten (mit Ausnahme von Albanien), die Sowjetunion sowie die USA und Kanada.

Die Konferenz war von einem Tauschgeschäft geprägt: Für den Ostblock brachte sie die Anerkennung der Grenzen der Nachkriegsordnung und einen stärkeren wirtschaftlichen Austausch mit dem Westen. Im Gegenzug machte der Osten Zugeständnisse bei den Menschenrechten. In den Folgejahren entstanden in mehreren sozialistischen Ländern Bürgerrechtsbewegungen, die sich auf die Schlussakte von Helsinki beriefen und zum Zusammenbruch des Ostblocks beitrugen, so dass die KSZE entscheidend zum Ende des Ost-West-Konflikts beitrug.

Die ursprünglich als einmalige Veranstaltung geplante Konferenz wurde unter anderem mit den KSZE-Folgekonferenzen in Belgrad (1977–1978), Madrid (1980–1983), Wien (1986–1989) und wiederum Helsinki (1992) fortgeführt.

Beim KSZE-Gipfeltreffen am 5. und 6. Dezember 1994 in Budapest wurde beschlossen, die KSZE zu institutionalisieren und mit Wirkung vom 1. Januar 1995 in "Organisation für Sicherheit und Zusammenarbeit in Europa (OSZE)" umzubenennen.

Nach einer elfjährigen Pause fand das erste OSZE-Gipfeltreffen vom 1. bis 2. Dezember 2010 in Astana statt. Den Vorsitz der Konferenz führte der kasachische Präsident Nursultan Nasarbajew. Auffassungsunterschiede zwischen westlichen und östlichen Mitgliedsländern bezüglich einer künftigen inhaltlichen und strategischen Ausrichtung der OSZE führten zu einem weiteren ergebnislosen Abschluss der Konferenz. Die geplante Verabschiedung eines Aktionsplanes zur Lösung internationaler Konflikte und zur Reform der OSZE scheiterte.

Zum Abschluss des Treffens am 5. Dezember 2014 in Basel gab es Deklarationen, aber keine Abschlusserklärung. Der Vorsitzende Didier Burkhalter erklärte, dass sich die Sicherheitslage in Europa im Jahr 2014 aufgrund der Ukraine-Krise verschlechtert habe.

Am 3./4. Dezember 2015 tagte der OSZE-Ministerrat in Belgrad (Serbien). In der Zeit vom 8. Dezember 2016 bis zum 9. Dezember 2016 tagte der OSZE-Ministerrat in Hamburg. Das Treffen fand auf dem Gelände der Hamburg Messe statt. Die Außenministerinnen und Außenminister der Teilnehmerstaaten versammelten sich darüber hinaus am 8. Dezember im Großen Festsaal des Hamburger Rathauses, zu einem gemeinsamen Arbeitsessen. Das Treffen fand in Hamburg statt, weil Deutschland am 1. Januar 2016, zum zweiten Mal nach 1991, den Vorsitz der Organisation für Sicherheit und Zusammenarbeit in Europa übernommen hatte.

Der Vorsitz wechselt jährlich. Als Vorsitzender agiert jeweils ein Außenminister. 

Trotz ihrer Bezeichnung ist es fraglich, ob die OSZE den Charakter einer internationalen Organisation hat, da Artikel 22 der Budapester Erklärung die Hinterlegung beim Generalsekretariat der Vereinten Nationen (gemäß Artikel 102 der Charta der Vereinten Nationen) explizit nicht vorsieht. Der Generalsekretär der Vereinten Nationen hat daher wiederholt auf eine Klärung der Rechtsnatur der OSZE gedrängt. Eine internationale Expertenkommission hat die OSZE aufgrund ihrer Tätigkeiten als internationale Organisation eingestuft; die herrschende Meinung in der Lehre sowie auch die weit überwiegende Staatenpraxis behandelt die OSZE jedoch nicht als Internationale Organisation.





</doc>
<doc id="3726" url="https://de.wikipedia.org/wiki?curid=3726" title="Ostsee">
Ostsee

Die Ostsee (auch Baltisches Meer, von lat. "Mare Balticum", oder auch "Baltische See" genannt) ist ein 412.500 km² großes und bis zu 459 m tiefes Binnenmeer in Europa und gilt als das größte Brackwassermeer der Erde, auch wenn in der westlichen Ostsee aufgrund des Wasseraustausches mit der Nordsee zumeist ein höherer Salz- und Sauerstoffgehalt beobachtet werden kann. Der Rauminhalt des Meeres beträgt rund 20.000 km³. Im Ostseeraum leben, je nachdem, wie weit man diese Region eingrenzt, zwischen 50 und 85 Millionen Menschen.

Die Ostsee trennt die Skandinavische Halbinsel von den zusammenhängenden Festländern Nord-, Nordost- und Mitteleuropas.

Die westlichste Stelle der Ostsee liegt am Westende der Flensburger Förde bei der Stadt Flensburg, der nördlichste Punkt befindet sich an der schwedisch-finnischen Landesgrenze am Bottnischen Meerbusen, ihre östlichste Stelle beim russischen Sankt Petersburg. Ihren südlichsten Punkt stellt das Südende des Stettiner Haffs bei Stettin dar.

Die Anrainerstaaten der Ostsee sind Deutschland, Dänemark, Schweden, Finnland, Russland, Estland, Lettland, Litauen und Polen.

Sowohl historisch als auch in den modernen Wissenschaften wurde und wird die westliche Abgrenzung der Ostsee unterschiedlich definiert.

In Politik und Bürokratie:

Die Helsinki-Konvention von 1992 bezeichnet das Kattegat nur als Eingang zur Ostsee: „For the purposes of this Convention the „Baltic Sea Area“ shall be the Baltic Sea and the Entrance to the Baltic Sea, bounded by the parallel of the Skaw (d. h. Breitengrad von Skagen) in the Skagerrak at 57°44.43'N.“ Aufgrund dieser Vereinbarung wird das Kattegat aber vielfach mit in die Ostsee einbezogen.

Handel und Geschichte:

Historisch verlief die Grenze durch die Beltsee, denn an den Einfahrten zur Ostsee erhob das Königreich Dänemark den Sundzoll. Die Mautstelle im Öresund war Schloss Kronborg bei Helsingør. Im Großen Belt wurde er bei Nyborg kassiert. Für den Kleinen Belt wurde der Sundzoll "Stromzoll" ("strømtold") oder "Beltzoll" ("bælttold)" genannt und seit der Gründung der Festung Fredericia 1650 dort erhoben, vorher andernorts. Die engste Stelle (Middelfartsund) liegt allerdings bei Middelfart.

Moderne Naturwissenschaften:

Das Leibniz-Institut für Ostseeforschung schreibt: „Aus physikalischer Sicht gibt es Argumente, die Trennung zwischen Nord- und Ostsee im Großen Belt bei Langeland und im Öresund auf die Drogdenschwelle zu legen.“

Auch das Sveriges Meteorologiska och Hydrologiska Institut (SMHI) zieht die Grenze zwischen Ostsee und Kattegat durch die Drogdenschwelle am Südende des Öresunds, im Großen Belt zwischen Korsør und Nyborg und im Kleinen Belt bei Middelfart. Die maximal 7 m tiefe Drogdenschwelle erstreckt sich nördlich der Køgebucht zwischen Dragør im Süden Kopenhagens und Malmö. Hier wurde die Öresundquerung mit dem "Drogdentunnel" gebaut. Demnach würden also die dänischen Ostseeinseln in der Beltsee die ungefähre Grenze der Ostsee markieren.

Eine Untersuchung der schwedischen Chemikalienaufsicht Kemikalieinspektionen nimmt hingegen westlich der Drogdenschwelle die 18 m tiefe Darßer Schwelle als Begrenzung. Die begrenzt den Zufluss von Salzwasser aus dem Kattegat und der Beltsee, da es unterhalb des salzarmen Ostseewassers strömt.

Beim Vergleich der verschiedenen Definitionen dürfen deren Konsequenzen nicht übersehen werden: Da Kattegat, Beltsee und Öresund sauerstoff- und artenreich sind, ist die Ökologie der Ostsee statistisch betrachtet gesünder, wenn man sie einbezieht, dagegen schwer bedroht, wenn man sie nicht zur Ostsee zählt.

Der mittlere Wasserstand der Ostsee (Mittelwasser) liegt etwa bei Normalnull. Bei Kiel liegt er beispielsweise 1 cm unter NN, das mittlere Hochwasser (MHW) und mittlere Niedrigwasser liegen etwa 1,22 m darüber bzw. darunter. Mit einer Wahrscheinlichkeit von 80 % bleibt der Pegel im Laufe eines Jahres unter 1,45 m über Mittelwasser, er wird im Mittel alle fünf Jahre überschritten. Mit einer Wahrscheinlichkeit von 99 % bleibt der Pegel unter 2,26 m über Mittelwasser, er wird im Mittel alle 100 Jahre überschritten.

Vor allem die westliche Ostsee unterliegt dem Einfluss einer regelmäßigen, aber schwach ausgeprägten Tide mit einer Periode von 12,4 Stunden und Amplituden von ca. 15 cm bei Wismar und 10 cm in Warnemünde. In der mittleren Ostsee liegt die Amplitude unter 5 cm.

Als Sturmhochwasser wird gemeinhin ein Wasserstand von mehr als einem Meter über Normalmittelwasser bezeichnet. In Warnemünde ereigneten sich von 1950 bis 2000 etwa 110 Sturmfluten, durchschnittlich etwas mehr als zwei pro Jahr.

Historische Hochwasserereignisse sind die Allerheiligenflut 1304 und weitere in den Jahren 1320, 1449, 1625, 1694, 1784 und 1825. Über ihre Ausmaße ist wenig bekannt. Seit 1872 werden an der Ostsee regelmäßig verlässlich Pegel genommen. Der höchste war das katastrophale Ostseesturmhochwasser 1872 mit 2,43 m mittlerer Wasserhöhe über NN in Warnemünde und einem Maximum von 2,83 m über NN in Warnemünde und 3,12 m bei Holnis. Die letzten sehr schweren Hochwasserereignisse waren 1904 mit 1,88 m, 1913 mit 1,89 m, im Januar 1954 mit 1,73 m, am 2.–4. November 1995 mit 1,68 m und am 21. Februar 2002 mit 1,65 m mittlerer Wasserhöhe über NN.



Der Salzgehalt (die "Salinität") der Ostsee nimmt von West nach Ost ab. Er liegt in der Beltsee im Westen bei mindestens 19 PSU (1,9 %), im nordöstlichen Teil (Bottenwiek und Finnischer Meerbusen) hingegen nur noch zwischen 5 und 3 PSU (0,5 % bis 0,3 %). (Im Vergleich dazu erreicht der Salzgehalt des Atlantiks und der nördlichen Nordsee 3,5 %.) Der Rückgang des Salzgehalts verläuft dabei nicht kontinuierlich, sondern stufenweise. Dies wird darauf zurückgeführt, dass das Bodenprofil der Ostsee eiszeitbedingt aus Becken und Schwellen besteht. Das größte Gefälle der Salzkonzentration ist im Bereich der Darßer Schwelle nördlich von Rostock zu finden, die die Grenze zwischen Beltsee und Arkona-Becken bildet. Westlich davon beträgt die Salinität etwa 1,7 %, östlich nur 0,8 %. Östlich der Darßer Schwelle ist die Ostsee daher ein reines Brackwassermeer.

Wegen des hohen Süßwassereintrags und der geringen Verdunstung der Ostsee ist ihr Salzgehalt größtenteils auf den Wasseraustausch mit dem Weltmeer zurückzuführen. Da Salzwasser schwerer ist als Süßwasser, findet zudem eine Schichtung des Seewassers statt. Besonders viel Salz findet sich im tiefen Wasser unterhalb von 60–70 Metern. In den Belten und Sunden gibt es eine Oberflächenströmung mit geringem Salzgehalt von der Ostsee zum Kattegat und eine Tiefenströmung salzreichen Wassers aus dem Kattegat in die Ostsee. Über drei Viertel des Wasseraustausches finden durch den Großen Belt statt und etwa 9 % durch den Kleinen Belt. Durch die Schwellen- und Beckenstruktur des Ostseebodens bleibt aber ein beachtlicher Teil des Salzwassers im Becken der Beltsee zurück und dringt nicht weiter nach Osten vor.

Zeitliche Schwankungen der Salinität kommen durch stürmische Perioden zustande, die den Wasseraustausch durch die Meerengen beschleunigen, und durch große Niederschlagsmengen, die den Süßwassereintrag (im Mittel 500 km³/Jahr) vermehren. So kommt es bei starkem Südwest-Wind dazu, dass viel Wasser in die nordöstliche Ostsee gedrückt wird und der Wasserstand in der westlichen Ostsee sinkt; gleichzeitig entsteht im Skagerrak ein Sturmhochwasser, und Nordseewasser läuft über die Beltsee in die Ostsee. Damit gelangen sowohl Salz als auch Sauerstoff in das Tiefenwasser der Ostsee. Gibt es über längere Zeit keinen neuen Zustrom, wird der Sauerstoff von den Organismen aufgebraucht. Es bildet sich giftiger Schwefelwasserstoff, der zum Beispiel Fischeier oder Larven abtötet.

Der Sauerstoffvorrat eines zusätzlichen Nordseewassereinflusses während stürmischer Perioden hält etwa ein bis drei Jahre vor. Der vorletzte solche Einbruch war 2003, der davor 1993. Noch in den 1970er Jahren fanden solche Ereignisse viel häufiger statt als heute. Im Winter 2014/15 wurde der drittumfangreichste Salzwassereinbruch seit 1880 beobachtet, als rund 4 Gigatonnen Salz in die westliche Ostsee gelangten, die größte Menge seit sechs Jahrzehnten.

Für die Größe der Flüsse, die in die Ostsee münden, ergibt sich ein unterschiedliches Bild, ob man sie nach dem mittleren Abfluss betrachtet, nach der nominellen Länge oder der hydrologischen Länge, also der Gewässerlänge von der entferntesten Quelle zur Mündung:

Eine 2004 von Schweden veröffentlichte Gliederung orientiert sich stark am Bodenrelief des Meeres.
Die Ostsee besteht aus verschiedenen Becken und Meeresbuchten, die durch Meerengen miteinander verbunden und durch Inseln voneinander getrennt sind. Dazu kommen spezielle Küstengewässer wie die Förden, die Fjärde, die Haffe und die Bodden. In Übergangsbereichen am Eingang und zwischen den größeren Meeresteilen gibt es Überlappungen:

Das 22.000 km² große Kattegat wird von den Anrainern zumeist als eigenständiges Meeresgewässer betrachtet. Im Gegensatz zur Ostsee ist es ozeanografisch, biologisch, verkehrstechnisch und historisch kein Binnenmeer. Weil es der "Zugang zur Ostsee" ist, wird es in manchen Zusammenhängen zusammen mit der Ostsee behandelt. In anderen Zusammenhängen ist es eines der Randgewässer der Nordsee.

Der Süden des Kattegat wird auch der Beltsee zugerechnet.

Die etwa 8.000 km² große Beltsee, auch "Westliche Ostsee" genannt, umfasst die Meeresgewässer westlich von Seeland, Falster und der sich zwischen dieser Insel und der deutschen Küste bei Rostock erstreckenden Darßer Schwelle. Hier liegen die engsten Stellen des Eingangsbereichs der Ostsee. Das Meer ist hier durch Inseln in ein Netz von Meerengen und Buchten geteilt, die mit der übrigen Ostsee kaum enger verbunden sind als mit dem Kattegat. Die mittlere Salzkonzentration im Wasser der Beltsee ist mehr als doppelt so hoch wie in den östlich angrenzenden Becken. Zwischen den nördlicher und südlicher gelegenen Teilen dieses Übergangsbereichs zwischen Kattegat und Ostsee gibt es ebenfalls erhebliche Unterschiede. Daher werden die nördlichen Teile gleichzeitig dem Kattegat zugerechnet, die südlichen gleichzeitig der „eigentlichen“ Ostsee.
Die eigentliche Ostsee reicht im weiteren Sinne von der deutschen Ostseeküste, im engeren Sinne von der Linie Falster–Darß im Westen (Darßer Schwelle) bis etwa zur Linie Stockholm–Åland–nordwestliches Estland im Nordosten. Zu den nicht eingeschlossenen Meeresbuchten im Osten gehört außer dem Bottnischen und dem Finnischen auch der Rigaer Meerbusen. Nach dem Relief des Meeresgrundes wird die eigentliche Ostsee in mehrere Becken unterteilt. Deren westlichste gehören gleichermaßen auch zur Beltsee: Südlicher Kleiner Belt, Kieler Bucht, Südlicher Großer Belt und Mecklenburger Bucht. Östlich der Darßer Schwelle erstreckt sich rund um Rügen bis zur Linie Sandhammaren (Schonen) – Bornholm – Wolin die Arkonasee, die mithin auch den Westteil der Pommerschen Bucht umfasst. Zwischen Bornholm im Westen, der Stolper Schwelle im Osten und der Küste Blekinges im Norden erstreckt sich das Bornholmbecken. Rund um Gotland unterscheidet man das Nordgotlandbecken nördlich der kleinen Insel Gotska Sandö, das Westgotlandbecken mit dem Landsorttief zwischen Gotska Sandö, Gotland und der schwedischen Küste, sowie das Ostgotlandbecken mit dem Gotlandtief zwischen der großen Insel, Saaremaa (Ösel), Kurland und Hinterpommern. Südöstlich daran schließt sich die Danziger Bucht an; als Meeresbecken wesentlich größer definiert als anhand der Küstenverläufe, reicht sie bis an die Linie zwischen Rozewie (Rixhöft) zum Süden der lettischen Küste.

Der Rigaer Meerbusen oder "Rigaischer Meerbusen" zwischen Kurland und dem estnischen Väinameri-Archipel kann auch als "Östliche Ostsee" bezeichnet werden.

Der Finnischen Meerbusen zwischen Estland, Finnland und Russland kann auch als "Nordöstliche Ostsee" bezeichnet werden.

Die "Nördliche Ostsee" von Åland nordwärts zwischen Finnland und Schweden wird auch als Bottnischer Meerbusen bezeichnet.

Auf der 19. Konferenz der BSHC (Baltic Sea Hydrographic Commission) am 10.–12. Juni 2014 wurde eine neue Einteilung der Ostsee vereinbart, die das Kattegat einbezieht, aber wiederum als Eingangsbereich. Nach Auskunft der deutschen Teilnehmer der Konferenz orientiert sich die Vereinbarung allein an nautischen Belangen, nicht an ökologischen. Daher wurde das Bodenrelief bei dieser Einteilung kaum berücksichtigt. Gar keine Rolle spielte, welcher Süßwassereintrag aus dem Binnenland in welche Seegewässer gelangt.

Unter dem Gesamtgebiet der Ostsee werden weitere Stufen unterschieden:

Systematik:

Der Begriff „Ostsee“ ist sinngemäß primär in den germanischen Sprachen (außer Englisch) verbreitet: Dänisch "Østersøen", Isländisch/Färöer "Eystrasalt", Niederländisch "Oostzee", Norwegisch "Østersjøen", Schwedisch "Östersjön". Hierfür mag die geografische Sicht aus der Lage dieser Länder wesentlich sein. In Finnland beruht der Begriff (Finnisch "Itämeri", Schwedisch "Östersjön") dagegen eher auf der Oberhoheit Schwedens vom 12. bis ins 18. Jahrhundert über das heutige Finnland.

Im Englischen sowie den meisten anderen Sprachen wird das nordeuropäische Binnenmeer sinngemäß „Baltische See“ bzw. „Baltisches Meer“ genannt.

In römischen Quellen wurde die Ostsee in der Regel nach den an seiner Südküste lebenden Sueben als "Mare Suebicum" bezeichnet.
Eine andere Benennung ist "Aestenmeer" – benannt nach dem Volk der Aesti, vermutlich ein von den Germanen verwendetes Exonym („Ostleute“) für die Balten, die Tacitus in der Germania als die am weitesten östlich am "Mare Suebicum" lebenden Menschen beschrieb.

Dagegen besteht bei der Deutung von "Balt-" keine Einigkeit. Verschiedene Herkünfte werden für möglich gehalten.

Die Ostsee entstand am Ende der letzten Eiszeit, der Weichsel-Kaltzeit, vor etwa 12.000 Jahren nach dem Abschmelzen der riesigen Gletschermassen. Ihre heutige Gestalt und Eigenart bildete sich über mehrere Etappen durch ein Zusammenspiel von Landhebung und Meeresspiegelanstieg:
Etwa 10.000 bis 8.200 v. Chr. tauten infolge des damaligen Klimaumschwunges die Gletscher in Richtung Skandinavien zurück. Als sich der Eisrand nach Abschmelzen der Inlandeismassen auf der Höhe der heutigen Åland-Inseln, nordöstlich von Stockholm, befand, bildete sich in seinem Vorland der "Baltische Eisstausee".

Etwa 8.200 bis 6900 v. Chr. stieg der Meeresspiegel so stark, dass sich zumindest im Bereich der heutigen mittelschwedischen Seenplatte, nach anderen Quellen auch zum Weißen Meer, eine Verbindung zum Weltmeer bildete. Durch den dadurch bedingten Süßwasserausstrom und Salzwassereinstrom bildete sich das sogenannte (salzige) "Yoldiameer".

Etwa 6900 bis 5000 v. Chr. tauten die skandinavischen Gletscher weiter zurück, der Druck auf die skandinavische Landmasse nahm ab, so dass sie sich zu heben begann und dadurch die Meeresverbindungen blockierte. Es entstand der/die (süße) "Ancylussee".

Etwa 5000 v. Chr. bis etwa zum Jahre 1 stieg der Meeresspiegel durch die sogenannte Littorina-Transgression so, dass die Festlandbrücke zwischen Südschweden und Dänemark überflutet wurde und der Osten Dänemarks sich in die heutigen Inseln aufteilte. Weiter öffnete sich der Zugang in der Nähe der Darßer Schwelle vor der deutschen Küste, und auch im südlichen Bereich der Ostsee bildeten sich die Grobformen der heutigen Küsten aus.

Die Gletscher waren nun fast vollständig verschwunden. Das Festland von Skandinavien hob sich weiter, so dass sich die Küstenlinie weiter veränderte. Der südliche Bereich der Ostsee senkte sich, das vorrückende Meer überflutete die jungglaziale Landschaft und formte sie dabei um. Als Ergebnis findet man drei Küstenformen im südlichen Bereich wieder: Fördenküste (Beispiel: Kieler Förde), Buchtenküste (Beispiel: Lübecker Bucht) und die Bodden- bzw. Boddenausgleichsküste (Beispiel: Halbinsel Fischland-Darß-Zingst) z. T. mit der Bildung von Haffen (Beispiel: Stettiner Haff).

Tabellarische Übersicht der Entwicklungsstufen
Der Südteil der Ostsee befindet sich in der gemäßigten Klimazone, die bei Dänemark noch ausgesprochen maritime Züge trägt, nach Osten hin jedoch im Bereich des Kontinentalklimas liegt. Der nördliche Teil, insbesondere der Bottnische Meerbusen, ist geprägt durch das kalte Klima der borealen Nadelwälder. Die reichen in Finnland bis etwa 200 km nördlich des Polarkreises. Weil die Ostsee vom klimabeeinflussenden Golfstrom abgekoppelt und ihre Fläche recht klein ist, aufgrund geringer Verdunstung und reicher Süßwasserzuführung der Salzgehalt außerdem sehr niedrig liegt, kann sie nur sehr geringfügig zum klimatischen Ausgleich beitragen; sie entwickelt kein eigenes maritimes Klima. Daher vereist sie jeden Winter teilweise, hin und wieder sogar vollständig. Dadurch wirkt sie nach harten Wintern als Kältespeicher. Hafenstädte wie Oulu in Finnland zählen bis zu sechs vereiste Monate pro Jahr. Eisschichten können in kalten Wintern auch an der deutschen Küste solche Mächtigkeiten erreichen, dass Personen darauf gehen können. Nur einige Inseln wie Bornholm profitieren von einem ungewöhnlich milden Mikroklima.

Die Ostseeregion erwärmt sich überdurchschnittlich: Im letzten Jahrhundert wurde es an der Ostsee um 0,85 Grad wärmer, weltweit dagegen nur um 0,75 Grad. Seit 1990 ist die Luft durchschnittlich knapp ein Grad wärmer als in den Jahrzehnten zuvor. Bis Ende des Jahrhunderts könnte sich die Luft in der Ostsee-Region um weitere drei bis sechs Grad erwärmen.

Ungefähr 20 Prozent der Böden der Kern-Ostsee – zwischen Dänemark und den Åland-Inseln – gehören inzwischen zu den sogenannten „Todeszonen“, in denen aufgrund Sauerstoffmangels kein Leben außer anaeroben Organismen existiert. Dies ergaben Messungen des schwedischen Meteorologischen Instituts im Jahr 2008. Andere Berichte bezeichneten ein Sechstel (70.000 km² der rund 412.500 km²) großen Ostsee als lebensfeindliche Gebiete. Im Jahr 2008 waren es erst 42.000 km².

Ursache ist, dass aus der Landwirtschaft Phosphor- und Stickstoff-Verbindungen in die Ostsee gelangen. Phosphor und Stickstoff sind Düngerstoffe.
Sie fördern das Algenwachstum; die Zersetzung toter Algen lässt den Sauerstoffgehalt sinken. Wasser mit höherem Salzgehalt und dadurch bedingt höherem spezifischem Gewicht bleibt auf dem Meeresgrund, isoliert vom Oberflächenwasser und der Atmosphäre. 
In den Todeszonen leben nur anaerobe Bakterien; sie zersetzen organische Substanz und setzen dabei Schwefelwasserstoff frei. Eine Anreicherung mit Sauerstoff findet überwiegend durch Herbst- und Winter-Stürme aus westlichen Richtungen statt, die salziges und sauerstoffreicheres Wasser aus der Nordsee in die Ostsee transportieren.

Der Ostseerat beschäftigt sich mit dem Thema.

Auch wurden bei Untersuchungen in den Jahren 1980/81 im Bereich der westlichen und mittleren Ostsee auf zwei Fahrten Wasserproben aus verschiedenen Tiefen zur Untersuchung auf ihren Gehalt an künstlichen Radionukliden entnommen. Die Ergebnisse zeigen, das der Gehalt an Radionuklid Cs im Tiefenwasser bzw. in der westlichen Ostsee durchweg höher war (zwischen 19 und 107 mBq/l), als die Konzentration im Oberflächenwasser bzw. in der mittleren Ostsee (15-60 mBq/l). Das Isotop ist in dieser Konzentration vollkommen unbedenklich, eignet sich aber um das Strömungsverhalten der Ostsee genauer zu untersuchen.

siehe auch weiter unten: Munitionsentsorgung in der Ostsee

Die Fischbestände leiden außer unter dem Sauerstoffmangel und den Schadstoffeinträgen auch unter Überfischung. Die Situation des Herings ist in der Ostsee deutlich schlechter als in der Nordsee. Darum hat sich die EU auf eine Herabsetzung der Fangquoten geeinigt. Der Dorsch laicht beispielsweise in etwa 60 Metern Tiefe, wo die Salzkonzentration optimal für die Fischeier ist. Dort wird allerdings zunehmend eine signifikant wachsende Sauerstoffarmut registriert, die zur Folge hat, dass die Fischeier absterben. Der Bestand an Dorsch (Kabeljau) hat allerdings in den letzten Jahren wieder leicht zugenommen, bedingt durch einen Kaltwasserschub und ein besseres Einhalten der Fangquoten insbesondere durch polnische Fischer.

Die Ostsee ist reich an Inseln, Inselgruppen und -ketten sowie bewohnten und unbewohnten Eilanden. Eine exakte Zahl wird nicht genannt, weil die Definitionen auseinandergehen, wonach eine Insel und ein Eiland unterschieden werden. Im Folgenden werden die den Anrainerstaaten zugordneten Inseln kurz dargestellt (ausführlich siehe das jeweilige Lemma).
Die großen dänischen Inseln Seeland und Fünen trennen die Ostsee vom Kattegat. Seeland trägt die dänische Hauptstadt Kopenhagen (København), ist die größte Insel des Königreichs und inzwischen durch die Öresundbrücke und den Drogdentunnel mit dem südschwedischen Schonen (das bis 1658 zu Dänemark gehörte) und durch die Großer-Belt-Querung mit der drittgrößten dänischen Insel Fünen verbunden. Etwa 150 km südöstlich von Kopenhagen liegt die dänische Insel Bornholm.

Die dänischen Inseln Seeland und Fünen sind wesentlich dichter besiedelt als die Halbinsel Jütland, welche die Beltsee und Kattegat nach Westen begrenzt. Die meisten Inseln liegen im Segelrevier der dänischen Südsee. Dort befinden sich größere Inseln wie Lolland, Falster, Møn, Langeland, Ærø und Alsen. Nordöstlich von Bornholm besitzt das Land mit Christiansø seinen östlichsten Außenposten.

Zu den kleinsten bewohnten dänischen Ostseeinseln gehören die Ochseninseln in der Flensburger Förde. Sie liegen unmittelbar an der deutsch-dänischen Grenze.

Zu Deutschland gehören die großen Ostseeinseln Fehmarn, Rügen und Usedom, welche auch zu einem kleinen Teil zu Polen gehört.

Fehmarn liegt vor der Halbinsel Wagrien an der Lübecker Bucht und ist mit dem Festland über die Fehmarnsundbrücke als Teil der Vogelfluglinie verbunden. Zurzeit (2010) ist eine weitere feste Fehmarnbeltquerung als Alternative zur Jütlandlinie in Planung, so dass das Brücken- und Tunnelnetz auf dem Weg von Mitteleuropa nach Skandinavien komplettiert wäre.

Rügen, die größte deutsche Insel, ist über den Rügendamm und die Rügenbrücke (zweite Strelasundquerung) bei Stralsund mit dem Festland verbunden. Rügen hat einige vorgelagerte Inseln, von denen Hiddensee die bekannteste ist.

Usedom, dessen Ostteil zu Polen gehört, besitzt wie Rügen eine reiche Gliederung in Halbinseln, außerdem existieren dort viele Seen.

Estlands größte Insel, und gleichzeitig die größte Ostseeinsel des Baltikums ist Saaremaa ("Ösel"). Zweitgrößte estnische Insel ist Hiiumaa ("Dagö"). Daneben gibt es noch die Inseln Vormsi, Muhu, Naissaar, Vilsandi, Kihnu, Ruhnu und ca. 1500 weitere, kleinere Inseln.

Die Zahl der finnischen Ostseeinseln und Eilande wird mit etwa 80.000 angegeben. In dieser Zahl sind die ca. 6500 Inseln von Åland ebenso enthalten wie dessen Schären. Der Rest sind zumeist Schären, die nicht zu Åland gehören. Finnland besitzt also eine bedeutende Inselwelt in der Ostsee.

Die Festung Suomenlinna liegt auf den Inseln vor Helsinki. Mit dem Kvarken und dieser Festung hat Finnland zwei insulare Weltkulturerbe in der Ostsee.

Zwischen Schweden und Finnland liegt die zu Finnland gehörende, aber mit Autonomierechten ausgestattete schwedischsprachige Inselgruppe Åland, die aus über 6.500 Inseln besteht. 65 dieser Inseln sind bewohnt und beherbergen 26.530 Einwohner (Ende 2004).

Rund fünf Kilometer vor Kap Kolka befindet sich mit einer künstlichen Leuchtturm-Insel die einzige Insel der lettischen Ostsee.

Litauen hat keine Inseln in der offenen Ostsee, dafür aber im Kurischen Haff: Kiaulės Nugara (dt.: „Schweinerücken“) bei Klaipėda sowie Rusnė und einige andere im Memeldelta. Der litauische Teil der Kurischen Nehrung hat keine direkte Landverbindung zum übrigen Staatsgebiet. Von Klaipėda muss man mit der Fähre übersetzen. Pläne für eine Brücke stehen im Konflikt zum Status der Landzunge als Nationalpark und Weltkulturerbe und wurden daher bisher verworfen. Auf dem Landweg ist die Nehrung nur von der russischen Oblast Kaliningrad aus zu erreichen.

Polen teilt sich Usedom mit Deutschland. Ganz zu Polen gehört die Nachbarinsel Wollin. Daneben gibt es eine Reihe kleinerer Inseln im Stettiner Haff.

Russland besitzt mit Kotlin vor Sankt Petersburg eine historisch wichtige Insel. Sie ist besser bekannt unter dem Namen Kronstadt der gleichnamigen Stadt und Festung.

Die zweitgrößte Ostseeinsel ist das schwedische Gotland. Hier wird ein recht eigenständiger Dialekt, das Gotländische, gesprochen. Wichtig ist auch die zweitgrößte schwedische Insel Öland. An Schwedens Küste liegen tausende kleiner Schären, die teilweise bewohnt sind.

Die Hauptstadt von Gotland, Visby, ist ebenso Weltkulturerbe wie die südliche Landschaft Ölands.

Die Küstenformen der Ostsee sind ein Resultat eiszeitlicher Gletscherbewegungen und nacheiszeitlicher Geländehebung im nördlichen und -absenkung im südlichen Bereich der Ostsee, die bis heute andauern. Beeinflusst werden die Küsten außerdem durch die Lage in der Westwindzone, wodurch von Westen her beständig Sedimente angeschwemmt werden. Unterschieden werden folgende Erscheinungsformen:

Die schwedisch-finnische Küste in der "Zentralen", "Nördlichen" und "Östlichen Ostsee" ist fast ausschließlich eine Schärenküste; ab und zu findet man noch vereinzelte Fjorde (Fjord-Schären-Küste). Schären sind der Küste vorgelagerte, kleine und kleinste felsige Inseln, die durch den Abschleifeffekt der Gletscher eine charakteristische Kuppenform aufweisen. Weil die Ostsee nur geringe Gezeiten aufweist, sind sie über die letzten Jahrtausende praktisch unverändert geblieben. Das flach abfallende Gelände wurde beim Abschmelzen des Eispanzers überflutet und die Kuppen ragten fortan als Inseln heraus; durch die Geländehebung sind mit der Zeit weitere, vorgelagerte Schären entstanden.

An einigen Stellen, zum Beispiel auf Gotland, Bornholm, Møn und Rügen, haben sich Kliffküsten gebildet. Diese ragen steil und schroff hervor und markieren Geländebrüche im geologischen Untergrund. Kliffkanten finden sich auch unterhalb des Meeresspiegels. Auch die Nordküste Estlands zum Finnischen Meerbusen hin ist durch solch eine Bruchlinie geprägt. Von West nach Ost rückt dieses Kliff immer näher an die aktuelle Küstenlinie heran und erreicht bei Sillamäe immerhin knapp 60 m Höhe.

Eine bekannte Steilküste befindet sich auf der Insel Rügen. Der weiße Kreidefelsen des Königsstuhls im dortigen Nationalpark Jasmund kann als totes Kliff bezeichnet werden, da er nicht ständig von der Brandung erreicht wird. Dagegen sind die benachbarten Wissower Klinken im Jahre 2005 ein Opfer der Meeresbrandung geworden.

Die Ostküste Schleswig-Holsteins und Jütlands ist durch Förden gekennzeichnet, die allerdings in Dänemark Fjord genannt werden. Diese schmalen langen Buchten sind bei der Entstehung der Ostsee durch den Anstieg des Meeresspiegels vollgelaufene ehemalige Gletscherzungenbecken. Der Unterschied zu Fjorden besteht darin, dass die Gletscher sich nicht vom Land zur See bewegten, sondern umgekehrt der Eispanzer über der heutigen Ostsee Gletscher vorantrieb, die nach dem Abschmelzen eine Rinne übrig ließen, die sich mit Seewasser füllte. Die schleswig-holsteinischen Förden werden von den Landschaften Angeln, Schwansen und Dänischer Wohld getrennt. Zwischen der Kieler Förde und der ihr vorgelagerten Kieler Bucht einerseits und der Lübecker Bucht als Teil der Mecklenburger Bucht andererseits liegt die Probstei und die Halbinsel Wagrien mit der Insel Fehmarn. Der Hemmelsdorfer See bei Timmendorfer Strand ist ebenfalls eine alte Förde. Er ist wesentlich tiefer als die durch eine eiszeitliche Landbarriere abgeschnittene, davorliegende Lübecker Bucht.

Die Landschaft der Fördenküste wird durch den Baltischen Landrücken geprägt, der sich entlang oder parallel der westlichen, südlichen und südöstlichen Ostseeküste von Jütland bis ins Baltikum erstreckt.

Die vorpommersche Küste ist durch Boddenlandschaften geprägt. Bodden sind dadurch entstanden, dass vormalige Inseln durch stetige Zuführung von Material, hauptsächlich Sand, durch schmale Brücken miteinander verbunden worden sind. Die rückwärtigen Gewässer, die Bodden, sind dadurch größtenteils von der Ostsee abgetrennt worden und mit ihr nur noch durch Rinnen verbunden.

Die Ausgleichsküste bestimmt vor allem die Küstenlinie Polens von Stettin bis kurz vor Danzig und die lettische Küste. Hier sind die typischen reich gegliederten glazialen Küstenformen durch die Anströmung und den Sedimenttransport von Westen her ausgeglichen worden, so dass der Verlauf fast gerade ist. Dies ist möglich geworden, weil die zumeist von Westwind geprägte Brandung auf eine Küstenlinie trifft, die von Südwest nach Nordost verläuft und dadurch Transportmaterial anlagert. Auch an der Küste Vorpommerns sind durch solche Ausgleichsprozesse Landzungen und Nehrungen entstanden, wie z. B. die Halbinsel Fischland-Darß-Zingst und die Schaabe, die Schmale Heide und der Bug auf Rügen.

Die Haff- oder Nehrungsküste ist im Küstenabschnitt zwischen Danzig und Klaipėda entstanden. Außerdem wird das Stettiner Haff ebenfalls hinzugezählt. Haffe entstehen vor Flussmündungen als Brackwasserreservoire, die durch schmale Landzungen, die Nehrungen, von der übrigen Ostsee größtenteils abgetrennt wurden. Durch die ständige Zufuhr von Flusswasser schließen sich die Nehrungen nicht, sondern bleiben als langgestreckte Halbinseln bestehen, die eine Rinne zum Meer offen lassen.

Die bekanntesten Haffs sind das Kurische und das Frische Haff. Eine (unvollständige) Nehrung bildet auch die Halbinsel Hel bei Zoppot.

Die Ostsee wird vor fast 2000 Jahren in der Germania des Tacitus als "Mare Suebicum" erwähnt, das er als Teil des die Erde umgebenden Ozeans ansah (siehe hierzu Namensgebung und -deutung).

Schon aus damaliger Zeit sind weitverzweigte Handelswege belegt, über die der begehrte Bernstein, der an der Ostseeküste häufig gefunden wurde, in alle Teile des Römischen Reichs gelangte. Exportwaren waren weiterhin Felle und Pelze. Umgekehrt gelangten römische Erzeugnisse wie Keramikwaren, Wein und Öl nach Norden.

Im Hochmittelalter spielte die Ostsee eine immense Rolle als Verkehrs- und Handelsweg in Europa. Die in Nachbarschaft der Ostsee liegenden Städte schlossen sich zum Bund der Hanse zusammen und brachten es dabei zu großem Reichtum. Wichtigste Hansestädte an der Ostsee und in deren Einzugsgebiet waren Lübeck, Wismar, Rostock, Stralsund, Greifswald, Stettin, Danzig, Königsberg, Memel, Riga, Reval und Nowgorod.

Im Dreißigjährigen Krieg versuchte Schweden, über die Ostsee hinweg Großmachtpläne zu verwirklichen. Infolgedessen gehörten auch später noch viele südlich der Ostsee gelegene Landstriche lange zu Schweden (siehe auch Schwedisch-Pommern).
In den Nordischen Kriegen gelang es Russland, von Osten her Anschluss an die Ostsee zu bekommen. Zar Peter der Große ließ im Mündungsdelta der Newa die neue Reichshauptstadt Sankt Petersburg erbauen, die für das Land ein „Tor zur Welt“ sein sollte.

Im Jahre 1872 kam es zur größten Sturmkatastrophe in der gesamten Ostsee. Am 11. November 1872 sollen in der Ostsee angeblich insgesamt 654 Schiffe gesunken sein.

Im 20. Jahrhundert war die Ostsee während der Weltkriege Schauplatz zahlreicher bewegender Vorfälle. Die Ostseehäfen waren gegen Ende des Ersten Weltkrieges Orte, in denen Geschichte geschrieben wurde: Die Sankt Petersburg vorgelagerte Festungsinsel Kronstadt war der Schauplatz eines Matrosenaufstandes gegen die russische Revolutionsregierung. Die Revolte wurde unter Einsatz von Kriegsschiffen blutig beendet. In den allerletzten Kriegstagen meuterten die deutschen Marineeinheiten in den Häfen von Kiel und Flensburg gegen einen sinnlosen Befehl der Obersten Heeresleitung, die Flotte zu einer militärisch nicht mehr entscheidenden Schlacht ausrücken zu lassen. Der Kieler Matrosenaufstand von 1918 weitete sich zu einer Revolution in ganz Deutschland aus und führte zum Sturz der Monarchie.

Im Zweiten Weltkrieg wurden in der Ostsee einige Kämpfe zwischen deutschen und sowjetischen Flotten- und U-Boot-Verbänden ausgefochten. Zu Kriegsende war fast die gesamte schiffbare Fläche vermint, so dass die Personenschifffahrt eingestellt wurde. 1945 wurde gleichwohl versucht, die in Kurland, Ostpreußen und Hinterpommern eingeschlossenen deutschen Truppen, aber auch die flüchtende Zivilbevölkerung, über die Ostsee zu evakuieren. Besonders tragisch war die Versenkung des ehemaligen KdF-Schiffes Wilhelm Gustloff, das fast ausschließlich Zivilisten an Bord hatte. Das Schiff sank nach mehreren Treffern sowjetischer Geschosse und riss schätzungsweise 9000 Menschen in den Tod, die entweder ertranken oder im eiskalten Wasser bald erfroren. Es war – gemessen an Menschenleben – die größte Schiffskatastrophe aller Zeiten.

Am 2. Mai 1945, fünf Tage vor der deutschen Kapitulation, griffen britische Flugzeuge den in der Lübecker Bucht liegenden ehemaligen Luxusdampfer "Cap Arcona" und die "Thielbek" an. Über 7.500 Menschen kamen dabei ums Leben; es waren überwiegend Gefangene deutscher Konzentrationslager.

Auch der Kalte Krieg forderte Opfer in der Ostsee: Rund 5000 DDR-Bürger versuchten, über die Ostsee in den Westen zu flüchten. Nur etwa 600 Flüchtende erreichten ihr Ziel, einige sogar auf Surfbrettern. Die meisten Fluchtversuche scheiterten und endeten oft genug tödlich. Der Leuchtturm in Dahmeshöved (Ostseeheilbad Dahme) diente vielen Flüchtlingen an der mecklenburgischen Küste als realistisches Ziel einer erfolgreichen Flucht.

Zu einem der schwersten Schiffsunglücke der europäischen Nachkriegsgeschichte kam es am 28. September 1994, als die Ostseefähre "Estonia" auf ihrem Weg von Tallinn nach Stockholm sank und 852 Passagiere dabei den Tod fanden.

Nach dem Zweiten Weltkrieg wurden in der Ostsee große Mengen Munition, darunter auch Giftgasmunition, entsorgt. Vor allem von phosphorhaltiger Munition geht nach wie vor eine große Gefahr aus. Bernsteinfarbene Phosphorklumpen entzünden sich nach dem Trocknen schon bei 34 °C, brennen dann mit einer Temperatur von 1300 °C und sind nur noch mit Sand zu löschen. Seit Ende des Zweiten Weltkriegs sind laut offiziellen Aufzeichnungen 168 Menschen durch Munitionsreste in der Ostsee zu Tode gekommen, 250 trugen Verletzungen davon. Dänemark veröffentlichte eine Studie mit weitaus höheren Verletzungszahlen. So sollen jährlich 20 Menschen Unfälle mit Munitionsresten erleiden, die meisten von ihnen sind Fischer. Der schwedische Sender Sveriges Television veröffentlichte 2009 Berichte über die Verklappung von chemischen Kampfstoffen und radioaktiven Abfällen der sowjetischen Marine vor Gotland in den Jahren 1989 bis 1992. Diese stammten von der Marinebasis Karosta im heutigen Lettland.

Der Ostseeraum ist ein Wirtschafts- und Wachstumsraum. Während im nördlichen und westlichen Teil der Ostsee schon etablierte Volkswirtschaften mit einem hohen BIP/Kopf und einer hohen Produktivität vorherrschen (Beispiel Deutschland oder Schweden), befinden sich im östlichen Teil der Ostsee noch relativ wirtschaftsschwache Länder, die aber ein überdurchschnittlich hohes BIP-Wachstum aufweisen. Infolge der Finanzkrise ab 2007 brach dieses Wachstum wieder ein. Besonders die baltischen Staaten mussten hier Wachstumseinbußen von über 10 % einstecken. Lediglich Polen machte nur geringe Einbußen und hielt im gesamten Ostseeraum seine Wirtschaft am stabilsten.

Das Wachstum des Ostseeraums beruht auf guten Standortfaktoren. Besonders hervorzuheben sind hierbei die vorteilhafte Lage der Ostsee innerhalb der Welt und die Mobilität innerhalb des Ostseeraumes. Zur Ansiedlung von neuen Unternehmen bietet der Ostseeraum zum einen hoch entwickelte Wirtschaftsregionen. Diese bilden umfangreiche Cluster und investieren sehr stark in Forschung und Technik. Die guten weichen Standortfaktoren sind auch ausschlaggebend. Herausragend entwickelte sich die Öresundregion, die laut der Zeitschrift The Economist im Jahr 2007 die wirtschaftsfreundlichsten Bedingungen der Welt aufweist. Zum anderen ist die Wirtschaftslage der baltischen Staaten sehr lukrativ für die Wirtschaft. Hier herrscht ein vergleichsweise liberales Geschäftsumfeld. Zugute kommen auch eine geschäftsfreundliche Steuerpolitik und eine umfangreiche Telekommunikationsstruktur.

Nach dem Seerechtsübereinkommen steht den Anrainerstaaten der Ostsee eine Ausschließliche Wirtschaftszone zu. Am 1. Januar 1995 hat die Bundesrepublik Deutschland eine solche auch für ihr Küstenmeer an der Ostsee erklärt.

Wichtige Häfen sind Kopenhagen, Malmö, Stockholm, Turku, Helsinki, Sankt Petersburg, Tallinn, Riga, Liepāja, Klaipėda (ehem. "Memel"), Kaliningrad (ehem. Königsberg), Danzig, Gdynia, Stettin, Świnoujście (ehem. Swinemünde), Trelleborg, Sassnitz, Rostock, Wismar, Lübeck, Kiel und Flensburg (vgl. Flensburger Hafen).

In der Mitte der südlichen Ostsee verläuft eine der wichtigsten Seeschifffahrtsrouten weltweit, die "Kadetrinne". Sie ist dicht befahren und war in der Vergangenheit gelegentlich im Zusammenhang mit Havarien in den Schlagzeilen.

Eine besondere Rolle für den Verkehr auf der Ostsee spielen die vielen Fährverbindungen sowie die großen Brücken, die in Skandinavien zum Teil größere Meerengen überspannen.

Die meistbefahrene künstliche Seeschifffahrtsstraße der Erde ist der Nord-Ostsee-Kanal, welcher die Ostsee mit der Nordsee verbindet, und so den Seeweg über Kattegat (Ostsee) und Skagerrak (Nordsee) abkürzt. Er führt in Schleswig-Holstein von Kiel nach Brunsbüttel zur Elbe.

Die Fläche der Ostsee innerhalb des deutschen Hoheitsgebietes ist als Seewasserstraße eine Bundeswasserstraße.

Mit dem wachsenden Schiffsverkehr von Frachtschiffen und Kreuzfahrtschiffen auf der Ostsee wächst auch die Emission von Kohlendioxid, Stickoxiden und Schwefeldioxid. Dabei werden während der Hafenliegezeiten die Häfen und ihre Anwohner, während der Fahrt das offene Meer belastet. Verschärft wird dieses Problem dadurch, dass Schiffe Schweröl mit einem sonst nicht zulässigen hohen Schwefelgehalt von 1,5 % (= 15.000 ppm) als Treibstoff verwenden. Straßendiesel enthält nur 10 ppm Schwefel. Ab 2010 sollen die Schwefel-Grenzwerte EU-weit auf 0,1 Prozent sinken, das wären dann 1000 ppm.

Die Ostseeanrainerstaaten haben diesbezüglich zahlreiche Initiativen begonnen, den Umweltschutz in der Seeschifffahrt voranzubringen. So gibt es, um die Emissionen während der Hafenliegezeiten zu senken, erste Versuche, Kreuzfahrtschiffe im Hafen verpflichtend an die Stromversorgung des Hafens anzuschließen (Beispiel Hamburg).

Im Osten ist die Ostsee über die Newa und verschiedene Wasserstraßen mit der Wolga, dem Weißen, Schwarzen und dem Kaspischen Meer verbunden.

Auf Binnenschifffahrt ausgerichtet sind der Wasserweg Weichsel – Bug – Dnepr-Bug-Kanal – Dnepr und der wesentlich ältere Ossolinskikanal von der Memel an den Dnepr.

Der Nord-Ostsee-Kanal verkürzt zwar nur den Umweg um die Kimbrische Halbinsel, hat aber wegen oft schwieriger Witterungsverhältnisse im Skagerrak historische Vorläufer, den Eider-Kanal und die mittelalterliche Passage über Schlei, Rheider Au und Treene, bei der seetüchtige Boote zumindest zeitweise auf Rollen über Land gezogen wurden.

Der Göta-Kanal von der Ostsee zum Kattegat in Schweden wurde im 18. Jahrhundert angelegt, um mit damaligen – kleineren – Seeschiffen den dänischen Sundzoll zu umfahren.

Die zahlreichen Meerengen der Beltsee werden seit dem frühen zwanzigsten Jahrhundert von einer zunehmenden Zahl fester Straßen- und Schienenverbindungen gekreuzt.
Die wichtigsten Verkehrsachsen des Ostseeraums bilden zum einen die Vogelfluglinie mit der damit verbundenen festen Fehmarnbelt-Querung.
Zum anderen sind Verkehrsachsen wie die Via Hanseatica – ein Teil davon bildet die Bundesautobahn 20 – und die Via Baltica wichtige Stützpfeiler für den nördlichen und östlichen Teil Europas. Es ist geplant, einen großen Teil Südschwedens mit Hochgeschwindigkeitszügen zu erschließen. Der Europakorridor bildet den Rahmen für die Durchführung dieses Projekts.

Seit 2011 transportiert die auch „Ostseepipeline“ genannte Nord Stream Pipeline russisches Erdgas durch die Ostsee nach Deutschland.

Die Küsten und Inseln des Ostseeraumes sind stark vom Tourismus geprägt, der neben der Werftindustrie und dem Handel der wichtigste Wirtschaftssektor ist. Ein wichtiger Bereich des Fremdenverkehrs ist der Badeurlaub in Seebädern. Er ist von einer für den Ostseebereich typisch-starken Saisonalität gekennzeichnet, welche die Monate Juli und August als Schwerpunkt haben. Andere Angebotsformen wie Wellness, Fahrrad- oder Kulturtourismus entwickeln sich und schwächen die Saisonalität etwas ab.

Weitere Faktoren im Ostsee-Tourismus sind Kreuzfahrtschiffe, die beispielsweise in Kiel, Rostock-Warnemünde, Kopenhagen, Tallinn, Riga, Danzig, Helsinki, St. Petersburg, Mariehamn und Stockholm anlegen, sowie maritime Großveranstaltungen wie die Kieler Woche oder die Hanse Sail, die jeweils Millionen von Besuchern anziehen.




</doc>
<doc id="3727" url="https://de.wikipedia.org/wiki?curid=3727" title="Orlando (Roman)">
Orlando (Roman)

Orlando – eine Biographie ist ein Roman von Virginia Woolf (erschienen 1928).

Die Handlung beginnt im England des 16. Jahrhunderts. Die Hauptperson Orlando ist ein junger Adliger, der bei Königin Elisabeth I. in hohem Ansehen steht und von ihr einen Landsitz erhält. Während des Frostjahrmarkts auf der im 16. Jahrhundert aufgrund der Kleinen Eiszeit regelmäßig zugefrorenen Themse lernt Orlando eine geheimnisvolle russische Gräfin namens Sasha kennen, die ihn jedoch nach einer leidenschaftlichen Affäre verlässt und nach Russland zurückkehrt. In der Folge zieht sich Orlando auf seinen Landsitz zurück, beschäftigt sich mit der Verwaltung seines Guts und versucht sich als Schriftsteller. Als ihn eine hartnäckige Verehrerin, die Erzherzogin Harriet, nicht in Frieden lässt, lässt sich Orlando als Botschafter nach Konstantinopel versetzen.

In Konstantinopel fällt Orlando nach einer von sozialen Unruhen geprägten Nacht in einen mehrtägigen Schlaf, aus dem er als Frau erwacht. Die Ursachen der Verwandlung bleiben im Dunkeln, allerdings findet sich unter Orlandos Papieren ein Dokument, das seine Eheschließung mit einer Tänzerin festhält. Orlando verlässt die Stadt heimlich mit Hilfe eines Zigeunerclans, bei dem er (bzw. nun sie) in den Bergen eine Weile leben kann, bis die unterschiedlichen kulturellen Hintergründe dann doch zu Konflikten führen. Orlando kehrt auf einem Schiff namens "Enamoured Lady" nach England zurück und macht sich während der Überfahrt, auf der sie erstmals europäische Frauenkleider trägt, nach und nach die Konsequenzen ihres Frauseins bewusst.

In England nimmt Orlando ihre Güter wieder in Besitz, muss jedoch zunächst abwarten, ob sie nach ihrer Verwandlung noch als rechtmäßige Inhaberin ihres Adelstitels und ihrer Ländereien anerkannt wird. Die entsprechenden Gerichtsverfahren ziehen sich lange hin, werden jedoch schließlich zu Orlandos Gunsten entschieden, ihre Ehe mit der Tänzerin wird annulliert. Orlando interessiert sich nun für das gesellschaftliche Leben in London, langweilt sich aber schon nach einer kurzen Zeit in den Salons der adeligen Damen und bemüht sich darum, mit den wichtigsten Schriftstellern ihrer Zeit (nun des 18. Jahrhunderts) bekannt zu werden. Tatsächlich bewirtet sie schon bald regelmäßig Alexander Pope, Joseph Addison und Jonathan Swift mit Tee, dem ihr verhassten neuen Modegetränk der Epoche, muss aber erkennen, dass diese ihre Ansichten nicht ernst nehmen. Dank ihrer Angewohnheit, zuweilen nachts in Männerkleidern auszugehen, macht Orlando bald die Bekanntschaft verschiedener Halbweltdamen Londons, mit denen sie sich anfreundet, nachdem sie sich ihnen als Frau zu erkennen gegeben hat.

Erst mit dem Beginn der Viktorianischen Epoche, deren Familiensinn und biedere Moral Orlando zu schaffen machen, stellt Orlando ihr ungebundenes Leben in Frage. Die bedrückende gesellschaftliche Atmosphäre der Zeit wird auch durch einen klimatischen Umschwung symbolisiert: Das englische Wetter wird nun dauerhaft feucht und klamm, die Sonne ist kaum noch zu sehen. Glücklicherweise begegnet ihr schon bald der Kapitän Marmaduke Bonthrop Shelmerdine, den Orlando umgehend heiratet, wenngleich sich aufgrund seiner häufigen Abwesenheit ihr Leben dadurch kaum verändert. Der Kritiker Nicholas Greene, den Orlando schon zu Zeiten Königin Elisabeths kennengelernt hat, verhilft Orlando nun zur Publikation ihres seit Jahrhunderten fortgeschriebenen Gedichts "The Oak Tree," das immerhin in sieben Auflagen gedruckt wird und einen Literaturpreis gewinnt. Das Buch endet im Jahr seiner Publikation (1928), in dem die nun ca. 300-jährige Orlando als Frau von 36 Jahren ein Auto besitzt und im Warenhaus einkauft.

In jeder Epoche Britanniens werden die Änderungen des Klimas, der Umgangsformen und der Literatur beschrieben und kritisiert. Ein zentrales Ereignis ist die Verwandlung Orlandos zur Frau. Virginia Woolf hinterfragt hiermit die Rollen von Mann und Frau, die Stellung der Frau in der Gesellschaft und ihren Zugang zur Literatur. Geschrieben hat Virginia Woolf dieses Buch für ihre damalige Liebe Vita Sackville-West, es stellt eine Art fiktive Biografie der Schriftstellerin Vita selbst dar und enthält Schilderungen über ihr Geburtshaus Knole House in Kent. Woolf bezeichnet den Zeitraum, in dem sie Orlando verfasst hat, als einmalig glücklichen Herbst, und dass sie nie ein Buch schneller verfasst habe. Das Buch selbst beschreibt sie als heiter und schnell lesbar; es zu schreiben sei für sie als Schriftstellerin Urlaub gewesen.


Der Roman diente 1981 als Vorlage für Ulrike Ottingers Film "Freak Orlando" mit Magdalena Montezuma als Orlando. 1992 wurde das Buch von Sally Potter mit Tilda Swinton in der Titelrolle als "Orlando" verfilmt. Die Rolle der Königin Elizabeth I. wird von dem Cross-Dresser Quentin Crisp bekleidet.



</doc>
<doc id="3728" url="https://de.wikipedia.org/wiki?curid=3728" title="Obiter dictum">
Obiter dictum

Ein (lat. „nebenbei Gesagtes“) ist eine in einer Entscheidung eines Gerichtes geäußerte Rechtsansicht, die nicht die gefällte Entscheidung trägt, sondern nur geäußert wurde, weil sich die Gelegenheit dazu bot. Den Gegensatz zum ' bildet die '.

Letztinstanzliche Gerichte fügen ihren Urteilen gelegentlich ' bei, weil sich sonst für die entscheidenden Richter häufig auf lange Zeit keine Möglichkeit mehr bietet, ihre Rechtsauffassung zu ähnlich gelagerten Fällen oder einen Grundsatz, der für den Fall keine Rolle spielt, kundzutun. Dies liegt daran, dass in manchen Bereichen seit langem eine gefestigte Rechtsprechung besteht, so dass keine Klagen mehr eingereicht werden, weil diese nur dann erfolgreich wären, wenn eine geänderte Rechtsauffassung gelten würde. Diesen Kreis zwischen dem Fehlen einer geänderten Rechtsauffassung und dem gegenseitig daraus resultierenden Fehlen von Urteilen sollen ' durchbrechen.

Nach Auffassung des damaligen BAG-Vizepräsidenten Hans-Jürgen Dörner haben " „die Schwäche, zur konkreten Rechtsfindung des Einzelfalls nichts beizutragen, die Leser regelmäßig zu verwirren und häufig späteren Erkenntnissen im Wege zu stehen“. Im günstigsten Fall trägt ein zur Rechtsfortbildung bei. Abgesehen von dem von Dörner beschriebenen „Rechtsverwirrungsmoment“ besteht überdies die Gefahr der Missachtung des Prinzips der Gewaltenteilung. Das Gericht hat nur den jeweiligen Einzelfall, also betreffend den vorliegenden Streitgegenstand, zu entscheiden. Widerspricht das Gericht per obiter dictum geltendem Recht, greift es über seinen Entscheidungsauftrag hinaus der Gesetzgebungskompetenz der Legislative vor. Das gilt auch für das deutsche Bundesverfassungsgericht: Ultima ratio seiner Kompetenz wäre die Nichtigkeitserklärung gemäß Abs. 2 S. 2 BVerfGG.

Andererseits können auch zukünftige eventuelle Änderungen der Rechtsprechung ankündigen und so das Vertrauen in eine gefestigte Rechtsprechung lockern, womit dem rechtsstaatlich gebotenen Prinzip der Rechtssicherheit genüge getan wird.

Ein bekanntes " wurde 1993 vom Zweiten Senat des Bundesverfassungsgerichts im Urteil zur Neuregelung des Schwangerschaftsabbruchs ausgesprochen. Der Senat stellte fest, dass eine rechtliche Qualifikation des Daseins eines Kindes als Schadensquelle von Verfassung wegen nicht in Betracht komme. Deshalb verbiete es sich, die Unterhaltspflicht für ein Kind als Schaden zu begreifen. Mit dieser Feststellung, die für das eigentliche Normenkontrollverfahren ohne jede Bedeutung war, wandte sich der Zweite Senat gegen die Rechtsprechung des Bundesgerichtshofs zur Haftung von Ärzten für ungewollte Schwangerschaften. Das Oberlandesgericht Düsseldorf bemerkte daraufhin, eine „beiläufige und nicht bindende Bemerkung“ des Bundesverfassungsgerichts führe nicht dazu, dass weder Schadensersatz noch Schmerzensgeld zu gewähren seien. Nachdem der BGH von seiner ständigen Rechtsprechung jedoch auch nach der Entscheidung des Zweiten Senats des BVerfG nicht abrückte, da er in den Ausführungen des Zweiten Senats des BVerfG nur ein unverbindliches obiter dictum erblickt, wurde kurz darauf anhand einer Verfassungsbeschwerde der – für Zivilsachen zuständige – Erste Senat des BVerfG angerufen. Dieser bestätigte die Ansicht des BGH.

Im anglo-amerikanischen ' sind ' anders als die ' eines Urteils in einem Präzedenzfall kein ', also nicht für die unteren Gerichte bindend. Dennoch werden sie oft in Entscheidungen mit einbezogen.



</doc>
<doc id="3729" url="https://de.wikipedia.org/wiki?curid=3729" title="Ohne Arbeit kein Lohn">
Ohne Arbeit kein Lohn

Ohne Arbeit kein Lohn ist ein Grundsatz im deutschen Arbeitsvertragsrecht, der sich mittelbar aus der Fälligkeitsregel des Satz 1 BGB ergibt. Danach ist die Vergütung "nach" Leistung der Dienste zu entrichten.

Die grundsätzliche Überlegung ist, dass die Arbeitsleistung eine Fixschuld ist, die zu erfüllen mit Zeitablauf in der Regel unmöglich wird. Hierüber besteht weitgehend Einigkeit. Wie das im Einzelfall zu begründen ist, ist streitig. Manche nehmen generell eine absolute Fixschuld an und meinen, die Arbeit könne nie nachgeholt werden. Andere (auch das Bundesarbeitsgericht) wollen von einer relativen Fixschuld im Sinne von Abs. 2 Nr. 2 BGB unter Berücksichtigung der konkreten Situation (Arbeitszeitkonten, Gleitarbeitszeit etc.) ausgehen. Im Normalfall gilt aber, dass ein Nachholen der Arbeit bereits deshalb unmöglich ist, weil der Arbeitnehmer an den folgenden Tagen bereits eine neue Arbeitsleistung schuldet.

Da im Arbeitsrecht grundsätzlich das allgemeine Leistungsstörungsrecht gilt, ergibt sich daraus für die Hauptleistungspflicht Arbeit eine Unmöglichkeit nach BGB. Daraus folgt dann gemäß Abs. 1 Satz 1 BGB, dass der Anspruch auf die Gegenleistung entfällt ("keine Leistung ohne Gegenleistung").

Von diesem Grundsatz gibt es aber eine Reihe von wichtigen Ausnahmen:



</doc>
<doc id="3730" url="https://de.wikipedia.org/wiki?curid=3730" title="Otho">
Otho

Marcus Salvius Otho (* 28. April 32 in Ferentium; † 16. April 69 in Brixellum) war vom 15. Januar 69 bis zu seinem Tod drei Monate später römischer Kaiser. Er war einer der vier Kaiser des Vierkaiserjahres.

Die Familie Othos stammte aus dem südetruskischen Ferentium. Sein Großvater Marcus Salvius Otho gehörte ursprünglich dem Ritterstand an und wurde als erster der Familie in den Senat aufgenommen. Durch Förderung der Livia wurde er 8 v. Chr. Münzmeister und später Prätor. Dessen Sohn, Othos Vater Lucius Salvius Otho, wurde im Jahr 33 Suffektkonsul, unter Caligula und Claudius Statthalter der Provinzen Africa und Dalmatia. Er gehörte den Arvalbrüdern an und wurde von Claudius in den Patrizierstand erhoben. Verheiratet war er mit Albia Terentia, die aus einer ritterlichen Familie stammte.

Während seiner Jugend pflegte Otho zum Missfallen seines Vaters einen verschwenderischen und leichtfertigen Lebensstil. Durch Neros zeitweilige Geliebte Acte gelangte er Mitte der 50er Jahre in den Kreis um den jungen Kaiser. 58 heiratete Otho die schöne Poppaea Sabina, angeblich auf Betreiben Neros, der sie als Geliebte gewinnen wollte, vielleicht aber auch aus Liebe. Nero empfand ihn jedenfalls bald als Rivalen um Poppaeas Gunst und schickte ihn, obwohl Otho erst die Quästur bekleidet hatte, im Jahr 59 als Statthalter nach Lusitanien, um Poppaea selbst heiraten zu können.

Seine Provinz, in der er zehn Jahre blieb, soll Otho gut verwaltet haben. Als sich Servius Sulpicius Galba, der Statthalter der benachbarten Provinz Hispania Tarraconensis, gegen Nero erhob, unterstützte Otho ihn und ging mit Galba nach Rom. Er machte sich Hoffnungen darauf, vom neuen Kaiser adoptiert und dadurch zum Nachfolger bestimmt zu werden. Galba entschied sich jedoch für Lucius Calpurnius Piso Frugi Licinianus als Nachfolger. Der zurückgesetzte Otho stiftete am 15. Januar 69 die Prätorianergarde an, Galba und Piso zu töten und ihn selbst zum Kaiser auszurufen. Widerwillig erkannte der Senat Otho als Kaiser an.

Zur Überraschung aller regierte Otho während seiner kurzen Amtszeit mit erstaunlichem Geschick. Seine Stellung war jedoch von Anfang an nicht unangefochten, denn ungefähr gleichzeitig mit Othos Staatsstreich erhob sich auch der niedergermanische Statthalter Aulus Vitellius zum Kaiser, gestützt auf die Legionen in Germania inferior und Superior und Britannien. Die Provinzen im Osten legten jedoch einen Eid auf Otho als Kaiser ab.

Otho versuchte, an die Herrschaft Neros anzuknüpfen (er führte zeitweilig sogar dessen Namen und hob die "damnatio memoriae" auf). Er stützte sich vor allem auf die Prätorianer und ehemalige Anhänger Neros, die wieder in Ämter eingesetzt wurden, bis auf den früheren Prätorianerpräfekten Tigellinus, den Otho für den Verrat an Nero hinrichten ließ.

Versuche Othos, die Erhebung des Vitellius durch Verhandlungen zu beenden, scheiterten. Vitellius gab sich nicht mit der angebotenen Rolle eines Mitregenten zufrieden und setzte seine Truppen nach Italien in Marsch. Otho musste zur Verteidigung auf eine inhomogene Streitmacht zurückgreifen, die aus den Prätorianern, 2.000 Gladiatoren und einer aus Flottensoldaten aufgestellten Legion (I Adiutrix) bestand, zu denen eilig herbeigerufene Legionen aus den Donauprovinzen kamen. Sie stellte sich den Truppen des Vitellius in Oberitalien entgegen, wo Otho in Brixellum sein Hauptquartier aufschlug. Seine Truppen waren zunächst in drei kleineren Schlachten erfolgreich, am Fuß der Alpen, bei Placentia und in der Nähe von Cremona bei einem Ort namens "ad Castores".

Am 14. April 69 unterlag Othos Armee in der (ersten) Schlacht von Bedriacum (bei Cremona). Als Otho in Brixellum davon tags darauf erfuhr, erdolchte er sich am nächsten Morgen in seinem Zelt, in der Hoffnung, weiteres Blutvergießen zu verhindern, obwohl seine Umgebung die Niederlage nicht für kriegsentscheidend hielt und weiterkämpfen wollte. Sein Leichnam wurde verbrannt und seine Asche in einem einfachen Grabmal beigesetzt. Aulus Vitellius wurde vom Senat offiziell als Nachfolger anerkannt.

Tacitus lässt anklingen, dass Otho sich mit Beginn der Herrschaft nur verstellt habe:
Besonders die Umstände von Othos Tod brachten ihm bei der Nachwelt aber auch sehr große Bewunderung ein. Der Biograph Sueton, dessen Vater sich als Militärtribun in der Umgebung Othos aufhielt, berichtet, dass Otho immer wieder seine Abscheu vor dem Bürgerkrieg betont habe. Martial verfasste ein Epigramm auf den Selbstmörder, das mit höchstem Lob nicht geizte und Otho sogar über Cato Uticensis, den Heros der ausgehenden Republik, stellte:
Die wichtigsten Quellen für Leben und Herrschaft Othos sind:




</doc>
<doc id="3731" url="https://de.wikipedia.org/wiki?curid=3731" title="Oslo">
Oslo

Die Kommune Oslo hat 669.060 Einwohner. Sie bildet eine eigenständige Provinz (Fylke) und ist zudem Verwaltungssitz für die benachbarte Provinz Akershus.

Mit 975.744 Einwohnern ist Oslo der mit Abstand größte Ballungsraum des Landes. In der Groß-Oslo-Region leben rund 1,5 Millionen Menschen, also fast ein Drittel der gesamten Bevölkerung Norwegens von rund 5,3 Millionen.

Beide "o" werden – wie in den meisten Wörtern – im Norwegischen wie deutsch "u" ausgesprochen, also [] oder []. Die verbreitetste Erklärung leitet den Namen Oslo von dem Fluss "Alna" her, der in früheren Zeiten "Lo(en)" genannt wurde. Norwegisch "os" ist die Flussmündung. Gegenüber den zahlreichen anderen Mündungsorten wie Nidaros und Namsos sind hier allerdings die Wortbestandteile Flussname und Mündung vertauscht, wodurch das Zusammentreffen zweier gleicher Vokale vermieden wurde.

Nach einem großen Stadtbrand 1624 wurde die Stadt unter dem dänischen König Christian IV. etwa einen Kilometer nordwestwärts verlegt und in "Christiania" umbenannt. 1877 änderte sich unter dem schwedisch-norwegischen König Oskar II. die offizielle Schreibweise in der Matrikel und im Staatskalender in "Kristiania", während die Stadtverwaltung bis 1897 die ursprüngliche Schreibweise beibehielt. Erst 1924, zwanzig Jahre, nachdem Norwegen seine Eigenständigkeit errungen hatte, wurde beschlossen, der Stadt nach rund 300 Jahren zum 1. Januar 1925 wieder den ursprünglichen Namen "Oslo" zu geben. Das monumentale Rathaus von Oslo, dessen Bau 35 Jahre von 1915 (erster Architektenwettbewerb) bis 1950 dauerte, kann als Symbolbauwerk der neu gewonnenen Unabhängigkeit gelten.

Ein Kosename der Stadt ist "Tigerstaden" (Tigerstadt) nach einem Gedicht von Bjørnstjerne Bjørnson („Sidste Sang“, 1870). In diesem Gedicht wird Oslo als gefährliche und unbarmherzige Stadt beschrieben. Vor dem Rathaus und vor dem Bahnhof erinnern Tigerskulpturen an diesen Namen, der seinen negativen Klang inzwischen verloren hat.

In der Heimskringla, einer Geschichte der norwegischen Könige, behauptet der isländische Gelehrte Snorri Sturluson, dass Oslo im Jahr 1048 von König Harald III. gegründet worden sei. Ausgrabungen der jüngeren Zeit haben indes christliche Gräber aus der Zeit um 1000 zum Vorschein gebracht. Aus diesem Grund beging die Stadt im Jahr 2000 ihr tausendjähriges Jubiläum, während 1950 erst der 900. Geburtstag begangen worden war.

Die mittelalterliche Stadt hatte zwei Burgen, den Königshof und die Bischofsburg. Innerhalb der Stadtmauern gab es neun Kirchen, darunter die St. Clemens und die Hallvardskathedrale, ein Hospital und etwa 400 Stadthäuser von Kaufleuten und Handwerkern. Die Wohnhäuser bestanden aus Holz.

König Håkon V. machte Oslo 1299 zur Hauptstadt Norwegens, veranlasste den Bau der Festung Akershus und sorgte für eine Reihe weiterer Bauaktivitäten wie der Errichtung der St. Marienkirche. Im Spätmittelalter entwickelte sich Oslo zu einer wichtigen Kaufmanns- und Residenzstadt. Die Einwohnerzahl verdoppelte sich auf 3.500.

1308 wurde Oslo von Herzog Erik av Södermanland geplündert und niedergebrannt. Die noch unfertige Festung Akershus hingegen überstand die Belagerung.

Die Stadt wurde wiederholt von Stadtbränden heimgesucht, jedoch immer wieder aufgebaut. Nach der Reformation verfielen das Kloster und die meisten der zahlreichen Kirchen. Nach den Bränden wurden diese Gebäude abgetragen und die Steine für andere Bauzwecke verwendet. Während der schwedischen Belagerung 1537 brannte die Stadt abermals.

Nach dem großen Brand von 1624 wurde die Stadt jedoch nicht wieder aufgebaut, sondern auf Befehl des dänischen Königs Christian IV. – Norwegen war zu dieser Zeit Provinz Dänemarks – näher an die Festung Akershus verlegt.

Die neu erbaute Stadt wurde nach dem Idealbild der Renaissance mit rechteckigen Quartieren und breiten Straßen errichtet und erhielt eine Festungsanlage mit Bastionen. Die Häuser baute man nun in Stein oder aus gemauertem Fachwerk, um das Ausbreiten von Bränden zu verhindern.
Gleichzeitig erhielt die Stadt den Namen Christiania, nach dem König Christian IV.

Das alte Oslo lag nun außerhalb der Stadtmauern von Christiania. Trotz königlichen Verbots wurde es wieder besiedelt, hauptsächlich von Armen und Landlosen, die sich das teure Leben im modernen Christiania nicht leisten konnten.

Die Johannes Kirke wurde 1875 erbaut und 1928 abgerissen.

Innerhalb der Kernstadt herrscht ein für Europa ungewöhnlich hohes Preisniveau. In entsprechenden Rankings, basierend auf standardisierten Warenkörben, belegt die Stadt regelmäßig Spitzenplätze. Laut "The Economist" hat Oslo seit 2006 Tokio als die weltweit teuerste Stadt abgelöst. Bis dahin hatte die japanische Hauptstadt 14 Jahre lang Platz eins belegt.

Die Universität Oslo ist mit etwa 30.000 Studenten die größte des Landes und wurde 1811 nach Vorbild der Berliner Friedrich-Wilhelms-Universität gegründet. 1952 fanden die Olympischen Winterspiele in Oslo statt, unter anderem am Holmenkollen.

Anfang der 1990er Jahre fanden die ersten geheimen Verhandlungen der Streitparteien PLO und Israel unter norwegischer Vermittlung in Oslo statt. 

Am 22. Juli 2011 tötete der Attentäter Anders Behring Breivik bei einem Bombenanschlag im Regierungsviertel in Oslo acht Menschen. Anschließend erschoss der Attentäter auf der Insel Utøya weitere 69 Menschen.
Oslo liegt in innerer Fjordlage und ist von Wald und Fjord umgeben. Der Akerselva entspringt dem Maridalsvannet, durchfließt die Stadt von Nord nach Süd und mündet in den Oslofjord. Im Süden liegt der Østensjøvannet-See.

Entsprechend der Lage am Oslofjord hat Oslo ein stark maritim geprägtes feucht-kontinentales Klima (Köppen: "Dfb"), das sich durch milde Winter und einen auf die zweite Jahreshälfte verschobenen, regenreichen Spätsommer auszeichnet. Die Sommer sind angenehm und sonnig. In den höher gelegenen Randgebieten ist es oft deutlich kühler als im Stadtzentrum. Die Temperaturunterschiede können oft mehr als 10 °C betragen, besonders im März/April, wenn in den am Stadtrand angrenzenden Wäldern noch eine geschlossene Schneedecke vorhanden ist.

Etwa 30 Prozent der Einwohner sind Ausländer.
Zum 1. Januar 2004 wurden die Stadtteile neu so eingeteilt, dass nunmehr 15 Stadtteile bestehen. Sentrum (Stadtmitte) und Marka (Wald und landwirtschaftlich benutztes Land) sind keine Stadtteile in politischem Sinne, da sie zentral verwaltet werden. Am 16. Januar 2011 überschritt Oslo Kommune die Einwohnerzahl von 600.000 Einwohnern.

Die Stadtversammlung ("bystyre") von Oslo besteht aus 59 gewählten Stadtverordneten. Sie erfüllt auch die Aufgaben als Fylkesting. 1986 wurde das Proporzsystem durch das parlamentarische Mehrheitsprinzip ersetzt: Die politische Mehrheit in der Stadtversammlung wählt den Bürgermeister und den Stadtrat ("byråd" oder "byregjering"), bestehend aus dem Bürgervorsteher ("byrådsleder") und bis zu sieben Stadträten ("byråd").

Traditionell haben die Mitte-rechts-Parteien in der Hauptstadt das Übergewicht. Seit 2015 regiert jedoch eine linke Mehrheit aus Arbeiterpartei, Grünen und Linkspartei unter Führung von Bürgervorsteher Raymond Johansen (Ap).

Die jüngste Kommunalwahl fand am 14. September 2015 statt. In der folgenden Übersicht sind die Stadtregierungsparteien grau unterlegt, ein Sternchen markiert feste Partner bei der Verabschiedung des jährlichen Haushaltsplans.
Das Amt eines Bürgermeisters ("ordfører") von Oslo/Christiania existiert seit 1837.

Das Stadtwappen ist als Rundschild ausgebildet.

Blasonierung: „Innerhalb eines zirkularen, golden gefassten, silbernen Bordes, darin in goldenen Majuskeln die lateinische Devise „UNANIMITER•ET•CONSTANTER•OSLO“ (EINMÜTIG UND BESTÄNDIG OSLO) in Blau auf silbernem Winkelschildfuß, der Schildkrümmung folgend eine liegende unbekleidete Frau mit langem goldenen Haar, ein in ein kurzärmliges langes rotes Gewand, rote Schuhe und silbernen Umhang gekleideter, silbern behelmter, golden nimbierter Mann, in den ausgestreckten Händen rechts einen silbernen Mühlstein haltend, in der Linken drei gebündelte silberne Pfeile, beidseitig hinter ihm hervorragend zwei liegende goldene Löwen, begleitet neben Haupt und Löwen von vier goldenen fünfstrahligen Sternen. Auf dem oberen Schildrand eine fünfzinnige goldener Mauerkrone.“

Wappenerklärung: Der Heilige stellt St. Hallvard dar, der beim Versuch, eine Frau vor Gewalttätern zu retten, von ihnen mit Pfeilen verwundet und, mit einem Mühlstein beschwert, ertränkt wurde. 

Zum besonderen Flair Oslos in der inneren Fjordlage tragen viele Sehenswürdigkeiten bei. Die bedeutendste ist die so genannte Gamlebyen (deutsch "Altstadt") mit den freigelegten Grundmauern des mittelalterlichen Oslo sowie das Schloss und die Burg Festung Akershus ("Akershus slott og festning").

Entlang der zentralen Einkaufsstraße, der Karl Johans gate, liegen sehenswerte Regierungsgebäude wie das Storting sowie "Slottet", das Königliche Schloss. Auf Nr. 31 – im Gebäude des 1874 vom Restaurateur Julius Fritzner eröffneten Grand Hotel – befindet sich heute noch das Grand Café, in dem einst Henrik Ibsen Stammgast war.

Ebenfalls im Stadtzentrum liegen das markante Rathaus, in dem alljährlich der Friedensnobelpreis verliehen wird, der Osloer Dom (Oslo Domkirke) sowie das Nationaltheater.

Das Neue Opernhaus der Norwegischen Oper, geplant vom norwegischen Architekturbüro Snøhetta, wurde 2008 eröffnet.

Auch die Museen der Stadt bieten zahlreiche Sehenswürdigkeiten. Dazu zählen vor allem das Frammuseum und Kon-Tiki-Museum auf Bygdøy, die Nationalgalerie, das Munch-Museum mit dem Nachlass des Malers Edvard Munch, das Norsk Folkemuseum, ein Freilichtmuseum mit wiedererrichteten Gebäuden aus ganz Norwegen, und das Vikingskipshuset mit archäologischen Wikingerschiffsfunden und das Kulturhistorisk Museum, das auch den Runenstein von Tune und den Gjermundbu-Helm präsentiert. 1993 wurde das private "Astrup Fearnley Museet for Moderne Kunst" eröffnet, das über eine umfangreiche Sammlung von Werken norwegischer und internationaler Gegenwartskunst verfügt, darunter seit 2002 die monumentale Porzellanskulptur "Michael Jackson and Bubbles" von Jeff Koons. Die deutsche Besatzungszeit wird im Holocaustmuseum, in der Villa Grande sowie dem Widerstandsmuseum "Norges Hjemmefrontmuseum" im Komplex der Festung Akershus aufgearbeitet. Die wechselvolle Geschichte der Stadt wird im Oslo Bymuseum gezeigt, dem Stadtmuseum auf Gut Frogner.

Bei gutem Wetter laden die Skisprunganlage Holmenkollbakken oberhalb der Stadt mit dem "Skimuseum am Holmenkollen" sowie die Vigeland-Anlage im Frognerpark mit Skulpturen Gustav Vigelands zum Verweilen ein. Eine weitere Möglichkeit zur Gestaltung der Freizeit ist ein Besuch des größten Vergnügungsparks in Norwegen, dem TusenFryd. Der Park liegt etwa 20 min südlich von Oslo.

Oslo ist die größte Studentenstadt Norwegens mit zahlreichen Einrichtungen. Zu den wichtigsten gehören:


Oslo ist Sitz diverser norwegischer Unternehmen, darunter Medinor, Qt Software und Opera Software. Hier befindet sich auch die Deutsch-Norwegische Handelskammer. 

Um der Luftverschmutzung entgegenzuwirken, gibt es Bestrebungen, den Individualverkehr mit Kraftfahrzeugen stark einzuschränken. So gibt es Programme, bis 2019 ein Fahrverbot für Privatfahrzeuge im Stadtzentrum einzuführen. Schon seit 1990 gibt es für die Stadt eine Innenstadtmaut, bei der die Maut an Mautstationen der Einfallstraßen zu bezahlen ist.

Oslo ist über zwei internationale Flughäfen zu erreichen: Flughafen Oslo-Gardermoen und Flughafen Torp. Der frühere Flughafen Oslo-Fornebu wird seit der Inbetriebnahme von Gardermoen nicht mehr bedient.

Gardermoen ist Norwegens Hauptflughafen. Er liegt ca. 50 km nordöstlich der Stadt und ist über eine Eisenbahnverbindung (Hochgeschwindigkeitszug Flytoget und NSB-Regionalzüge) und über mehrere Buslinien (SAS Flybussen und Flybussekspressen) angebunden.

Torp liegt ca. 120 km südwestlich von Oslo bei Sandefjord. Der Billigflieger Ryanair verwendet die Bezeichnung „Oslo-Torp“. Nach Oslo gibt es Bahn- (NSB) und Busverbindungen (Torpekspressen).

Oslo besitzt ein Netz von U-Bahnen (T-bane), Straßenbahnen und Buslinien. Die U-Bahn verkehrt durch die Stadtmitte unterirdisch, in den Randgebieten oberirdisch. Im Jahre 2006 wurde das U-Bahn-Netz durch eine Ringbahnstrecke ergänzt. Seit 2009 gibt es ein neues Fahrkartensystem, das beim Betreten und Verlassen der Bahnstationen die Karten kontrolliert. Jedoch können auch weiterhin Fahrausweise aus Papier gekauft werden.

Oslo verfügt über ein System öffentlicher Fahrräder, die während des Sommers an verschiedenen Standorten im Stadtgebiet ausgeliehen werden können und dort auch wieder abgegeben werden müssen.

Die Fjordinseln werden von Personenfähren bedient. Diese sind im Preis für ein Ticket des öffentlichen Nahverkehrs in Oslo inbegriffen.

Fernverbindungen mit den Tag- oder Nachtzügen der Norges Statsbaner verbinden Oslo mit Kristiansand, Stavanger, Bergen und Trondheim. Internationale Verbindungen bestehen nach Göteborg und Stockholm. Regionalzüge verkehren stündlich bis Lillehammer, Halden und Larvik. Weitere Orte sind durch Lokalzüge angebunden.

Es gibt Fährverbindungen nach Kiel mit Color Line, Frederikshavn mit Stena Line und Kopenhagen mit DFDS Seaways.



</doc>
<doc id="3732" url="https://de.wikipedia.org/wiki?curid=3732" title="One Time Programmable">
One Time Programmable

Unter einem OTP-Baustein versteht man ein programmierbares elektronisches Bauelement, das einen nicht flüchtigen Speicher (PROM) enthält. Dieser Speicher lässt sich jedoch nur einmal beschreiben. Je nach Bauart des Bauelements kann ein Programmiergerät zusätzlich erforderlich sein. Mittlerweile gibt es eine Vielzahl von Bauelementen, die zusätzlich auch in der OTP-Technologie hergestellt werden.

Die OTP-Technologie kann grundsätzlich als Fuse-Technologie oder als Antifuse-Technologie realisiert werden. Bei der Herstellung der OTP-Bauelemente durch den Halbleiterhersteller sind beispielsweise bei der Antifuse-Technologie alle Verbindungsstellen offen, erst durch die Programmierung werden die relevanten Verbindungsstellen verbunden.

Es existieren jedoch auch „unechte“ OTP-Speicher, welche als sog. „OTP-Area“ in bestimmten Flash-Speichern implementiert sind. Diese Speicherbereiche bestehen in der Regel aus normalen Flash-Zellen, welche durch eine Zusatzlogik gegen Veränderung gesichert werden.

Die OTP-Technologie kann beispielsweise bei den nachfolgenden Bauelementen eingesetzt werden.

Einzelne OTP-Bereiche existieren auch in bestimmten Bauelementen, wie beispielsweise Flash-Speichern, Smartcards und Mikrocontrollern, wobei diese dazu dienen, Seriennummern oder andere Identifikationsdaten unabänderlich zu speichern. Ein Beispiel dazu ist die IMEI der meisten modernen Handys.

Der Vorteil der OTP-Technologie liegt darin, dass die Schaltungsfunktion der programmierten Bauelemente nicht (also auch nicht unbeabsichtigt) geändert werden kann. Gegenüber Bauelementen mit Maskenprogrammierung kann bei der Baugruppenproduktion durch die Verwendung von OTP-Bauelementen kurzfristig auf eine Änderung des Dateninhalts reagiert werden.

Die OTPs haben beim praktischen Schaltungsaufbau aber entscheidende Nachteile, die bei der Schaltungsentwicklung berücksichtigt werden müssen. Einmal programmiert und einmal auf einer Baugruppe verbaut, kann die Logikfunktion dieses Bauelements nur noch durch einen Tausch des Bauelements (=Auslöten des alten Bauelements von der Leiterplatte und Einlöten des neuen Bauelements auf die Leiterplatte) geändert werden. Eine Funktionserweiterung, wie sie heute beispielsweise durch ein Software-Update oder bei einer Schaltungsänderung bei der Logikfunktion bei anderen programmierbaren Logikbauelementen üblich ist, ist bei den OTP-Bauelementen generell nicht möglich. 

Weiterhin darf die Tragweite eines Fehlers in der Funktion oder bei der Programmierung der OTP-Bauelemente nicht unterschätzt werden. Je umfangreicher die Schaltungsfunktion ist, desto größer ist das Risiko, dass in der Funktion ein Fehler enthalten ist, der auch mit den Entwicklungstests nicht gefunden wird. Im Extremfall kann eine Fehlfunktion einen Ausbau des Geräts beim Endkunden und eine Änderung durch den Hersteller zur Folge haben.


</doc>
<doc id="3733" url="https://de.wikipedia.org/wiki?curid=3733" title="Olympische Spiele">
Olympische Spiele

Olympische Spiele (von "ta Olýmpia" „die Olympischen Spiele“ "olymbiakí agónes" „olympische Wettkämpfe“) ist die Sammelbezeichnung für regelmäßig ausgetragene Sportwettkampfveranstaltungen, die „Olympischen Spiele“ und „Olympischen Winterspiele“. Bei diesen treten Athleten und Mannschaften in verschiedenen Sportarten gegeneinander an. Organisiert werden sie vom Internationalen Olympischen Komitee (IOC). Der Zeitraum zwischen den Spielen wird als "olymbiada" „Olympiade“ bezeichnet.

Die Einführung der Olympischen Spiele der Neuzeit wurde 1894 als Wiederbegründung der antiken Festspiele in Olympia auf Anregung von Pierre de Coubertin beschlossen. Als „Treffen der Jugend der Welt“ sollten sie dem sportlichen Vergleich und der Völkerverständigung dienen. Seit 1896 finden alle vier Jahre Olympische Spiele und seit 1924 Olympische Winterspiele statt. Seit 1994 alternieren Winter- und Sommerspiele im zweijährigen Rhythmus.

Das IOC übernimmt auch die Schirmherrschaft für die Paralympics als Wettkämpfe behinderter Sportler, der Deaflympics, Special Olympics und der World Games für nichtolympische Sportarten. Darüber hinaus gibt es seit 2010 die Olympischen Jugendspiele, die für Jugendliche im Alter von 14 bis 18 Jahren bestimmt sind.

Die Olympischen Spiele sind in ihrem Umfang stetig gewachsen, so dass mittlerweile fast jedes Land der Welt mit Sportlern vertreten ist. Neben den Fußball-Weltmeisterschaften gelten sie gegenwärtig als das größte Sportereignis der Welt.

Der Ursprung der Olympischen Spiele der Antike liegt vermutlich im 2. Jahrtausend v. Chr. Die Siegerlisten reichen bis ins Jahr 776 v. Chr. zurück und wurden im 4. Jahrhundert v. Chr. rekonstruiert. Die Zählung nach Olympiaden war ein Zeitmaß im gesamten antiken Griechenland. „Olympiade“ ist somit – entgegen einem heute weit verbreiteten Irrtum – nicht synonym mit „Olympische Spiele“, sondern bezeichnet den Zeitraum von vier Jahren, der mit den Spielen beginnt. Die Olympischen Spiele, benannt nach ihrem Austragungsort Olympia im Nordwesten der Halbinsel Peloponnes, waren Teil eines Zyklus, der drei weitere Panhellenische Spiele umfasste: Die Pythischen Spiele in Delphi, die Nemeischen Spiele in Nemea und die Isthmischen Spiele auf dem Isthmus von Korinth.

In der Anfangszeit gab es nur einen Wettlauf über die Distanz des Stadions (192,24 Meter). Die Spiele erhielten mit der Zeit eine immer größere Bedeutung. Sie waren aber keine „Sportveranstaltung“ in unserem heutigen Sinne, sondern ein religiöses Fest zu Ehren des Göttervaters Zeus und des göttlichen Helden Pelops. In ihrer Blütezeit dauerten die Spiele fünf Tage – der erste Tag war bestimmt von kultischen Zeremonien wie Weihehandlungen und dem Einzug der Athleten, Betreuer, Schiedsrichter und Zuschauer in den heiligen Hain von Olympia. Neben den Wettkämpfen – zuletzt waren es 18 in den Sportarten Leichtathletik, Schwerathletik, Pentathlon und Reiten – waren musische Wettbewerbe ebenso wichtig. Nicht der Sport als solcher stand im Mittelpunkt, sondern die religiöse Komponente.

Die eigentlichen Spiele begannen mit dem Umzug aller Beteiligten zum Tempel des Zeus. Hier schworen die Athleten, sich an die Regeln der Spiele zu halten. Die Sieger erhielten einen Siegeskranz aus Olivenzweigen sowie ein Stirnband. Man sah sie als „von den Göttern begünstigt“ an und verewigte sie mit Gedichten und Statuen. Jede Niederlage, sogar schon ein zweiter oder dritter Platz, galt als untilgbare Schmach. Die Verlierer kehrten auf Schleichwegen in ihre Heimat zurück, um dem Spott zu entgehen, der sie erwartete. Als berühmtester Olympionike der Antike gilt der Ringer Milon von Kroton, der erste namentlich bekannte ist Koroibos.

Die antiken Spiele waren aus heutiger Sicht außerordentlich brutal, jeder Teilnehmer in den klassischen Kampfsportarten (Boxen, Ringen, Stockfechten, Pankration) musste auch mit dem Tod rechnen und teilweise wurden Kämpfer für ihr Durchhalten zum Sieger erklärt, nachdem ihr Tod im Kampf festgestellt wurde.

Als die Römer im Jahr 148 v. Chr. Griechenland eroberten, verloren die Olympischen Spiele ihren panhellenischen Charakter. Von nun an war es auch nichtgriechischen Athleten gestattet, teilzunehmen. Vermutlich zum letzten Mal fanden die Spiele im Jahr 393 statt, bevor der römische Kaiser Theodosius I. alle heidnischen Zeremonien verbot. Fest steht, dass die Spiele nicht nach 426 n. Chr. ausgetragen werden konnten, weil damals Theodosius II. alle griechischen Tempel zerstören ließ. Überschwemmungen, Erdrutsche und Erdbeben verschütteten die übrigen Anlagen.

Die olympische Idee ging nicht ganz verloren. So fanden im Westen Englands zu Beginn des 17. Jahrhunderts erstmals die "Cotswold Olympick Games" statt. Ein weiterer Versuch, die Olympischen Spiele wiederzubeleben, waren die "Olympiades de la République", die von 1796 bis 1798 jährlich im revolutionären Frankreich ausgetragen wurden. Auf diese Veranstaltung geht auch die Verwendung des metrischen Systems im Sport zurück. 1850 führte die landwirtschaftliche Lesegesellschaft von Much Wenlock in der englischen Grafschaft Shropshire eine „olympische Klasse“ ein. Daraus entwickelten sich zehn Jahre später die "Wenlock Olympian Games", die bis heute unter der Bezeichnung "Wenlock Olympian Society Annual Games" fortgeführt werden. 1866 organisierte William Penny Brookes, der Vorsitzende der Wenlock Olympian Society, nationale Olympische Spiele im Londoner Crystal Palace.

Das griechische Interesse an der Wiedereinführung der Olympischen Spiele erwachte nach der Griechischen Revolution gegen die Herrschaft des Osmanischen Reiches. Der Dichter und Verleger Panagiotis Soutsos machte den ersten entsprechenden Vorschlag in seinem 1833 veröffentlichten Gedicht „Dialog der Toten“. Als wichtigster Vorläufer der modernen Olympischen Spiele gelten die Olympien, die ihrerseits das Münchner Oktoberfest zum Vorbild hatten. Sie wurden vom wohlhabenden griechischen Kaufmann Evangelos Zappas ins Leben gerufen und durch eine königliche Verfügung von Otto I. als eine nationale Aufgabe von hohem Rang angesehen, die auch internationale Beachtung erfuhr. Die erste Ausgabe fand 1859 im Stadtzentrum Athens statt. Zappas ließ das Panathinaiko-Stadion instand setzen, das bis 1889 Austragungsort weiterer Olympien war.

Nachdem 1766 die Sport- und Tempelanlagen in Olympia wiederentdeckt worden waren, begannen 1875 groß angelegte archäologische Ausgrabungen unter der Leitung des Deutschen Ernst Curtius. Um diese Zeit kam in Europa die romantisch-idealistische Antiken-Rezeption immer mehr in Mode; der Wunsch nach einer Wiedererweckung des olympischen Gedankens verbreitete sich. So sagte Baron Pierre de Coubertin damals: "„Deutschland hatte das ausgegraben, was vom alten Olympia noch vorhanden war. Warum sollte Frankreich nicht die alte Herrlichkeit wiederherstellen?“" Nach de Coubertins Meinung war die mangelnde körperliche Ertüchtigung der Soldaten eine der Hauptursachen für die Niederlage Frankreichs im Deutsch-Französischen Krieg von 1870/71 gewesen. Er strebte danach, diesen Zustand durch die verbindliche Einführung von Sportunterricht an den Schulen zu verbessern. Gleichzeitig wollte er nationale Egoismen überwinden und zum Frieden und zur internationalen Verständigung beitragen. Die „Jugend der Welt“ sollte sich bei sportlichen Wettkämpfen messen und sich nicht auf den Schlachtfeldern bekämpfen. Die Wiederbelebung der Olympischen Spiele schien in seinen Augen die beste Lösung zu sein, um diese Ziele zu erreichen.
Die Wenlock Olympian Games, die de Coubertin 1890 besuchte, bestärkten ihn in der Ansicht, dass eine Wiedereinführung der Olympischen Spiele im großen Rahmen möglich sei. Er griff Brookes und Zappas’ Ideen auf und fügte selbst das Prinzip der Rotation zwischen verschiedenen Austragungsländern hinzu. De Coubertin präsentierte einer internationalen Zuhörerschaft seine Vorstellungen auf einem Kongress, der vom 16. bis 23. Juni 1894 in der Sorbonne-Universität in Paris stattfand und als erster Olympischer Kongress in die Geschichte einging. Am letzten Tag des Kongresses beschlossen die Teilnehmer, dass die ersten Olympischen Spiele der Neuzeit 1896 in Athen stattfinden sollten, also im Ursprungsland. Um die Spiele zu organisieren, wurde das Internationale Olympische Komitee (IOC) gegründet. Erster Präsident wurde der Grieche Dimitrios Vikelas, während de Coubertin zunächst als Generalsekretär amtierte.

Die ersten Spiele der Neuzeit erwiesen sich als großer Erfolg. Obwohl nur rund 250 Athleten teilnahmen, waren sie ein großes sportliches Ereignis. Die griechischen Offiziellen waren vom Erfolg derart begeistert, dass sie den Vorschlag machten, die Spiele zukünftig immer in Griechenland stattfinden zu lassen. Doch das IOC hielt am Rotationsprinzip zwischen verschiedenen Ländern fest.

Nach dem Anfangserfolg geriet die olympische Bewegung in eine Krise. Die Spiele von 1900 in Paris und 1904 in St. Louis waren in die parallel stattfindenden Weltausstellungen eingebettet. Die Wettkämpfe zogen sich über mehrere Monate hin, waren schlecht organisiert und wurden kaum beachtet, zudem nahmen in St. Louis nur wenige Ausländer teil. Bei den Olympischen Zwischenspielen 1906 in Athen standen die sportlichen Wettkämpfe wieder im Vordergrund. Das IOC stimmte der Austragung zwar widerstrebend zu, erkannte die Resultate jedoch nie offiziell an. Von manchen Sporthistorikern werden diese Spiele als Rettung der olympischen Idee angesehen, da sie das Absinken in die Bedeutungslosigkeit verhinderten.

Die Wintersportart Eiskunstlauf stand 1908 und 1920 auf dem Programm von Sommerspielen, Eishockey 1920. Das IOC wollte diese Liste erweitern, um andere winterliche Aktivitäten abzudecken. Am Olympischen Kongress 1921 in Lausanne fiel der Beschluss, dass die Organisatoren der Sommerspiele 1924 zusätzlich eine „internationale Wintersportwoche“ unter der Schirmherrschaft des IOC veranstalten sollten. Diese „Woche“ (eigentlich waren es elf Tage) in Chamonix erwies sich als großer Erfolg, weshalb das IOC 1925 beschloss, sie rückwirkend als I. Olympische Winterspiele anzuerkennen und weitere Veranstaltungen dieser Art zukünftig im selben Jahr wie die Sommerspiele auszurichten.

1986 beschloss das IOC, beginnend mit 1994 einen separaten Zyklus zu eröffnen und die Winterspiele „im zweiten Kalenderjahr, das jenem folgt, in dem die Spiele der Olympiade abgehalten werden“ auszutragen.
Ludwig Guttmann strebte danach, die Rehabilitierung körperlich behinderter Soldaten des Zweiten Weltkriegs zu fördern und sie so in die Gesellschaft zu integrieren. Er organisierte 1948 einen mehrere Sportarten umfassenden Wettstreit zwischen verschiedenen Spitälern. Diese "Stoke Mandeville Games" entwickelten sich zu einem jährlich ausgetragenen Sportereignis. Guttmann und andere verstärkten ihre Öffentlichkeitsarbeit, bis schließlich 1960 die ersten Paralympics stattfanden. Diese werden seither alle vier Jahre ausgetragen (seit 1976 auch im Winter). Seit 1988 sind die Austragungsorte der Paralympics und der Olympischen Spiele identisch. Ebenfalls vom IOC anerkannt sind die seit 1968 durchgeführten Special Olympics für Menschen mit geistiger Behinderung, die 1924 eingeführten Deaflympics für Gehörlose und die seit 1981 stattfindenden World Games für nichtolympische Sportarten mit hoher weltweiter Verbreitung.

Die Olympischen Jugendspiele für jugendliche Sportler im Alter von 14 bis 18 Jahren gehen auf eine Idee von IOC-Präsident Jacques Rogge zurück. 2007 fiel der Beschluss zur Einführung, 2010 fanden in Singapur erstmals Olympische Jugend-Sommerspiele statt, die Olympischen Jugend-Winterspiele wurden erstmals 2012 in Innsbruck ausgetragen.

Von 1912 bis 1948 fanden zusätzlich olympische Kunstwettbewerbe statt. In den Jahren 1924, 1932 und 1936 wurde mit dem Prix olympique d’alpinisme auch ein Preis für herausragende Leistungen im Bergsteigen vergeben.

An den ersten Olympischen Spielen der Neuzeit hatten 1896 rund 250 Athleten aus 14 Ländern teilgenommen. Im Laufe der Jahre stiegen die Teilnehmerzahlen ständig. Beispielsweise nahmen an den Sommerspielen 2008 in Peking über 11.000 Athleten aus 204 Ländern an 302 Wettbewerben teil. Die Anzahl der Teilnehmer bei Winterspielen ist im Vergleich dazu bedeutend geringer, bei den Winterspielen 2006 in Turin waren etwas mehr als 2.500 Athleten aus 80 Ländern gemeldet, die in 84 Wettbewerben an den Start gingen.

Die Zahl der Mitgliedsländer des IOC beträgt 205 (vgl. Liste im Artikel Nationales Olympisches Komitee). Sie ist höher als jene der Länder, die von den Vereinten Nationen anerkannt werden (momentan 193). Das bedeutet, dass es 13 weitere IOC-Mitglieder gibt. Der Grund dafür ist, dass auch Nationen zugelassen sind, die nicht die strikten Anforderungen für politische Souveränität erfüllen, wie dies von den meisten anderen internationalen Organisationen verlangt wird. Als Folge davon besitzen mehrere Kolonien bzw. abhängige Gebiete eigene Delegationen, die getrennt von ihren Mutterländern teilnehmen.

Eine Vielzahl nationaler und internationaler Sportorganisationen und -verbände, anerkannte Medienpartner sowie Athleten, Betreuer, Schiedsrichter und jede andere Person oder Organisation, die sich zur Einhaltung der Olympischen Charta verpflichtet hat, bilden zusammen die so genannte olympische Bewegung. Ihre Dachorganisation ist das Internationale Olympische Komitee (IOC) mit Sitz in Lausanne, das seit 2013 von Thomas Bach präsidiert wird. Das IOC hält die Schirmherrschaft über die olympische Bewegung und beansprucht alle Rechte an den olympischen Symbolen sowie den Spielen selbst. Seine Hauptverantwortung liegt in der Betreuung und Mitorganisation der Olympischen Spiele und der Paralympics, der Auswahl der Austragungsorte und der Sportarten sowie der Vermarktung der Übertragungsrechte.

Die olympische Bewegung besteht aus drei Hauptkomponenten:

Englisch und Französisch sind die offiziellen Sprachen der olympischen Bewegung. Hinzu kommt bei jeder Austragung die Amtssprache des jeweiligen Austragungslandes. Jede Proklamation geschieht in diesen drei Sprachen oder in den zwei Hauptsprachen, falls die Amtssprache eines Landes Englisch oder Französisch ist.

In den Delegationen einiger Nationen reist geistlicher Beistand mit. Für die deutsche Mannschaft waren das 2004 in Athen, 2008 in Sydney und 2012 in London die Priester Hans-Gerd Schütt und Thomas Weber.

Die Gastgeberstadt von Olympischen Spielen wird sieben Jahre vor der Austragung bestimmt. Der Auswahlprozess umfasst zwei Phasen, die sich über zwei Jahre erstrecken. Eine Stadt bewirbt sich zunächst beim NOK ihres Landes. Falls mehr als eine Stadt im selben Land eine Kandidatur einreicht, führt das NOK eine interne Selektion durch, da dem IOC nur eine Stadt pro Land präsentiert werden darf. Nach Ablauf der Vorschlagsfrist beginnt die erste Phase. Die Organisationskomitees der Städte werden aufgefordert, einen detaillierten Fragebogen zu verschiedenen Schlüsselkriterien in Bezug auf die Organisation von Olympischen Spielen auszufüllen. Die Bewerberstädte müssen versichern, dass sie die olympische Charta und andere vom Exekutivkomitee des IOC aufgestellte Vorschriften einhalten werden. Ein spezialisierter Ausschuss prüft anhand der Fragebögen die Projekte aller Bewerber und deren Potenzial, die Spiele auszurichten. Basierend auf dieser Evaluation bestimmt das IOC-Exekutivkomitee jene Bewerber, die in die zweite Bewerbungsphase vorrücken.

In der zweiten Bewerbungsphase müssen die Städte dem IOC eine umfangreichere und detailliertere Projektpräsentation vorlegen. Jede Stadt wird von der Evaluationskommission eingehend analysiert. Die Kommissionsmitglieder besuchen die Kandidatenstädte, wo sie Vertreter lokaler Behörden befragen und die Standorte der vorgesehenen Sportanlagen inspizieren. Einen Monat vor der endgültigen Entscheidung des IOC veröffentlicht die Kommission einen Bericht mit ihren Beurteilungen. Während der zweiten Phase müssen die Städte auch finanzielle Garantien abgeben. Nach Vorliegen des Evaluationsberichts stellt das IOC-Exekutivkomitee die endgültige Liste der Kandidaten zusammen. Die Vergabe der Spiele findet bei der Generalversammlung der IOC-Mitglieder statt; diese treffen sich in einer Stadt, die nicht in einem Land mit einer Kandidatur liegt. In geheimer Abstimmung wird schließlich der Austragungsort bestimmt. Nach der Wahl unterzeichnet das erfolgreiche Organisationskomitee (zusammen mit dem NOK des entsprechenden Landes) einen Vertrag "(Host City Contract)" mit dem IOC.

Das IOC wehrte sich ursprünglich gegen die Finanzierung durch Sponsoren. Erst nach dem Rücktritt des als sehr prinzipientreu geltenden Avery Brundage im Jahr 1972 begann das IOC, das Potenzial des Mediums Fernsehen und den damit verbundenen lukrativen Werbemarkt auszuloten. Unter der Präsidentschaft von Juan Antonio Samaranch passte sich das IOC immer mehr den Bedürfnissen internationaler Sponsoren an, die ihre Produkte mit den olympischen Namen- und Markenzeichen bewerben wollten.

Die Vermarktung der olympischen Markenzeichen ist umstritten. Hauptkritikpunkt ist, dass die Olympischen Spiele nicht mehr von anderen kommerzialisierten Sportspektakeln unterschieden werden können. Das IOC wurde kritisiert, dass insbesondere während der Sommerspiele 1996 und 2000 eine Marktsättigung eingetreten sei und die Gastgeberstädte von Unternehmen und Händlern überflutet worden seien, die ihre Olympia-Produkte verkaufen wollten. Das IOC versprach, in Zukunft der Übervermarktung entgegenzuwirken. Eine weitere Kritik spricht die Tatsache an, dass Olympische Spiele von den Gastgeberstädten und den Regierungen der entsprechenden Staaten finanziert werden. Das IOC kommt nicht für die Kosten auf, kontrolliert aber alle Rechte, profitiert von den olympischen Symbolen und beansprucht einen Anteil an allen Sponsoren- und Medieneinnahmen. Städte bewerben sich aber weiterhin um das Recht, Olympische Spiele auszutragen, obschon sie keine Gewissheit haben, dass ihre Kosten gedeckt sein werden. Wichtig ist ihnen vor allem die weltweite Ausstrahlungskraft.

In der ersten Hälfte des 20. Jahrhunderts verfügte das IOC nur über ein kleines Budget. Avery Brundage lehnte jegliche Versuche ab, die Olympischen Spiele mit kommerziellen Interessen zu verbinden. Er war davon überzeugt, die Interessen der Unternehmen würden unannehmbare Auswirkungen auf die Entscheidungen des IOC haben. Brundages Ablehnung dieser Einnahmequelle bedeutete, dass die Organisationskomitees einzelner Spiele selbst Sponsorenverträge aushandelten. Als er 1972 zurücktrat, hatte das IOC ein Vermögen von 2 Millionen USD. Acht Jahre später war diese Zahl auf 45 Millionen USD angewachsen, da das IOC gegenüber Sponsoring und dem Verkauf der Übertragungsrechte mittlerweile eine weitaus liberalere Haltung einnahm. Als Juan Antonio Samaranch 1980 das Präsidentenamt übernahm, war er fest entschlossen, das IOC finanziell unabhängig zu machen.

Die Sommerspiele 1984 in Los Angeles markierten einen Wendepunkt. Dem von Peter Ueberroth angeführten Organisationskomitee LAOOC gelang es, durch den Verkauf exklusiver Vermarktungsrechte einen zuvor unvorstellbaren Überschuss von 225 Millionen USD zu erwirtschaften. Das IOC strebte danach, diese Sponsoreneinnahmen für sich selbst zu sichern. Samaranch schuf 1985 das exklusive Sponsorenprogramm "The Olympic Program" (TOP). Die Teilnehmer an TOP erhalten für ihre Produktekategorie weltweite Vermarktungsrechte und können die olympischen Symbole in ihrer Werbung verwenden.

Für die Gastgeberstädte und -länder bieten die Olympischen Spiele eine prestigeträchtige Gelegenheit sich der Welt zu präsentieren und für sich zu werben. Die Sommerspiele 1936 in Berlin waren die ersten, die im Fernsehen übertragen wurden, die Reichweite über den Fernsehsender Paul Nipkow war jedoch gering. Als erste erreichten die Winterspiele 1956 in Cortina d’Ampezzo ein internationales Publikum und 1960 bezahlten Fernsehsender erstmals für die Übertragungsrechte. In den folgenden Jahrzehnten entwickelten sich die Olympischen Spiele zu einer ideologischen Front im Kalten Krieg. Durch die Konkurrenz der politischen Systeme auf sportlicher Ebene stieg das Medieninteresse, wovon das IOC wiederum profitierte. Der Verkauf von Übertragungsrechten ermöglichte es ihm, die Olympischen Spiele bekannter zu machen und dadurch noch mehr Interesse zu generieren. Dies wiederum war attraktiv für Unternehmen, die Werbezeit im Fernsehen kauften. Durch diesen Kreislauf konnte das IOC immer höhere Gebühren für diese Rechte verlangen.

Von den 1960er Jahren bis Ende des Jahrhunderts stieg die Zuschauerzahl exponentiell an. Für die Sommerspiele 1968 in Mexiko-Stadt werden 600 Millionen Fernsehzuschauer geschätzt. Bis 1984 in Los Angeles stieg diese Zahl auf 900 Millionen an, 1992 in Barcelona betrug sie bereits 3,5 Milliarden. Bei den Sommerspielen 2000 in Sydney verzeichnete NBC jedoch die tiefsten Einschaltquoten seit 1968. Dies war auf zwei Faktoren zurückzuführen: Einerseits die größere Konkurrenz durch Kabelsender, andererseits das Internet, das Bilder und Resultate in Echtzeit liefern konnte. Insbesondere amerikanische Fernsehsender setzten noch immer auf zeitverschobene Übertragungen, im Informationszeitalter ein rasch veraltendes Konzept. Angesichts der hohen Kosten der Übertragungsrechte und der Konkurrenz durch neue Medien forderte die Fernsehlobby Konzessionen ein. Das IOC reagierte mit diversen Änderungen am Wettkampfprogramm. Beispielsweise wurden die beliebten Schwimm- und Turnwettbewerbe auf mehr Tage verteilt. Schließlich konnte die amerikanische Fernsehlobby in einzelnen Fällen auch diktieren, zu welcher Zeit bestimmte Wettbewerbe stattfanden, so dass sie live während der Prime Time in den USA gezeigt werden konnten.

Die olympische Bewegung verwendet mehrere weltweit (in Deutschland durch das Olympiaschutzgesetz) geschützte Symbole, die durch die Olympische Charta festgelegt werden. Das bekannteste ist die olympische Flagge mit den fünf verschiedenfarbigen, verschlungenen Ringen auf weißem Feld. Die sechs Farben Weiß, Rot, Blau, Grün, Gelb und Schwarz wurden deshalb gewählt, weil die Flagge jedes Landes der Welt mindestens eine dieser Farben aufweist. Weiterhin steht die Anzahl der Ringe für die fünf Erdteile (klassische Zählweise). Die Flagge wurde 1914 entworfen und wird seit den Sommerspielen 1920 in Antwerpen gehisst.

Das offizielle Motto der olympischen Bewegung lautet "citius, altius, fortius" (Latein für „schneller, höher, stärker“). De Coubertins Ideale spiegeln sich am besten im olympischen Credo wider: „Das Wichtigste an den Olympischen Spielen ist nicht der Sieg, sondern die Teilnahme, wie auch das Wichtigste im Leben nicht der Sieg, sondern das Streben nach einem Ziel ist. Das Wichtigste ist nicht, erobert zu haben, sondern gut gekämpft zu haben.“
Einige Monate vor den Spielen wird an historischer Stätte in Olympia in einer an antike Rituale angelehnten Zeremonie die olympische Fackel entzündet. Eine als Priesterin verkleidete Schauspielerin entfacht die Fackel mittels eines Parabolspiegels und übergibt sie dem ersten Läufer des anschließenden Staffellaufs. Dieser Lauf führt von Olympia bis zum Hauptstadion der jeweiligen Gastgeberstadt, wo die Flamme während der Dauer der Veranstaltung brennt. Das erste Mal wurde bei den Sommerspielen 1928 in Amsterdam ein olympisches Feuer entzündet. Es gab jedoch damals weder einen Fackellauf vor der Eröffnungsfeier, noch wurde das Feuer von einer bestimmten Person entzündet. Nach einer Idee von Carl Diem fand der erste Fackellauf vor den Sommerspielen 1936 in Berlin statt, 1952 in Oslo der erste Fackellauf anlässlich von Winterspielen.

Die Übergabe einer eigenen Olympiafahne an den nächsten Ausrichter der Spiele ist seit 1924 in Paris üblich und fester Bestandteil der Olympischen Spiele. Zunächst wurde die sogenannte Antwerpen-Fahne innerhalb der Schlussfeier an den Ausrichter der gegenwärtigen Spiele übergeben. Bei den ersten Spielen nach dem Zweiten Weltkrieg, 1948 in London, übergab zunächst ein Offizier der schottischen Garde die Fahne an den damaligen Präsidenten Edström, der sie an den Bürgermeister von London weiterreichte. Dieses Zeremoniell wurde 1960 in die Eröffnungsfeier verschoben. Durch die Weigerung der damaligen Sowjetunion an den Spielen 1984 teilzunehmen, erhielt der Bürgermeister der Stadt Los Angeles die Antwerpen-Fahne aus den Händen des damaligen IOC-Präsidenten Samaranch. Bei der Schlussfeier wurde die Fahne an die Delegation von Seoul übergeben. Aufgrund der zunehmenden Beanspruchung der historischen Fahne wurde in Seoul eine neue Fahne in Auftrag gegeben, die seitdem weitergereicht wird.

Seit den Winterspielen 1968 in Grenoble gibt es zu Promotionszwecken ein offizielles olympisches Maskottchen, üblicherweise eine heimische Tierart der Austragungsregion, seltener auch eine menschliche Figur, die das kulturelle Erbe repräsentiert.

Die Eröffnungsfeiern der Olympischen Spiele umfassen eine Reihe traditioneller Elemente, die in der Olympischen Charta festgelegt sind. Die Feier beginnt üblicherweise mit dem Hissen der Flagge und dem Abspielen der Nationalhymne des Gastgeberlandes. Es folgen verschiedene künstlerische Darbietungen (Musik, Gesang, Tanz, Theater), die die Kultur des Gastgeberlandes repräsentieren. Deren Größe und Komplexität sind mit den Jahren stetig gewachsen, da jedes Gastgeberland danach strebt, die früheren Feiern zu übertreffen und einen bleibenden Eindruck zu hinterlassen. So betrugen die Kosten der Eröffnungsfeier der Sommerspiele 2008 in Peking über 100 Millionen Dollar.

Anschließend beginnt der Einmarsch der teilnehmenden Athleten ins Stadion, jeweils ein Athlet geht einige Schritte vor dem Rest seiner Mannschaft und trägt dabei die Flagge seines Landes. Seit 1928 marschiert stets die Mannschaft Griechenlands als erste ins Stadion, um an die antike Tradition zu erinnern. Danach folgen die weiteren teilnehmenden Nationen in alphabetischer Reihenfolge der Hauptsprache des Gastgeberlandes. Falls die Sprache des Gastgeberlandes kein Alphabet mit fester Reihenfolge kennt, verläuft der Einmarsch gemäß der englischen oder französischen Sprache. 2008 in Peking war die Zahl der Striche des chinesischen Schriftzeichens für den Ländernamen maßgeblich. Den Abschluss des Einmarschs bildet die Mannschaft des Gastgeberlandes.

Sind alle Athleten eingetroffen, hält der Vorsitzende des Organisationskomitees eine kurze Rede. Auf diesen folgt der Präsident des IOC, der am Ende seiner Rede das Staatsoberhaupt des Gastgeberlandes vorstellt. Dieses wiederum eröffnet formell die Spiele. Als Nächstes wird die olympische Hymne gespielt, während die olympische Flagge ins Stadion getragen wird (seit 1960). Danach versammeln sich die Flaggenträger aller teilnehmenden Länder um ein Podium. Auf diesem sprechen ein Athlet (seit 1920) und ein Schiedsrichter (seit 1972) den olympischen Eid, mit dem sie das Einhalten der Regeln versprechen.

Zuletzt trägt der vorletzte Läufer des Staffellaufs die olympische Fackel ins Stadion und übergibt sie an den letzten Läufer. Dieser, oftmals ein sehr bekannter und erfolgreicher Sportler des Gastgeberlandes, entzündet dann mit der Fackel das Feuer in einer großen Schale. Ab 1920 wurden auch Friedenstauben freigelassen; man strich diesen Programmpunkt jedoch wieder, nachdem 1988 in Seoul einige Tauben im olympischen Feuer verbrannt waren.

Athleten (oder Mannschaften), die sich in einem olympischen Wettbewerb an erster, zweiter oder dritter Stelle klassieren, erhalten Medaillen als Auszeichnung überreicht. Bei der Verleihung stehen die Sportler auf einem Podest und die Nationalhymne des Siegerlandes wird gespielt.

Der Sieger erhält eine Goldmedaille. Dabei handelt es sich um Silbermedaillen mit einem goldenen Überzug; das IOC schreibt vor, dass die Medaille zu mindestens 92,5 % aus Silber bestehen und sechs Gramm Gold enthalten sein müssen. Der Zweitplatzierte erhält eine Medaille aus mindestens 92,5 % Silber, der Drittplatzierte eine aus Bronze. In einigen Wettbewerben, die im K.-o.-System ausgetragen werden (beispielsweise Boxen), werden beiden Halbfinalverlierern Bronzemedaillen überreicht.

1896 und 1900 wurden nur die zwei Besten mit Medaillen ausgezeichnet (Silber für den Ersten und Bronze für den Zweiten). 1904 erhielt erstmals der Sieger eine Goldmedaille, die anderen Medaillenfarben versetzte man um einen Platz nach unten. Seit 1948 erhalten die Athleten auf den Plätzen 4 bis 6 olympische Diplome (seit 1976 auch die drei Medaillengewinner). Seit 1984 erhalten auch die Siebt- und Achtplatzierten Diplome. Damit sollten nicht nur alle Teilnehmer eines Viertelfinales gewürdigt werden, es entfiel auch die Notwendigkeit, in Wettkämpfen mit K.-o.-System Platzierungskämpfe um die Plätze 5 bis 8 auszutragen.

Die Schlussfeier findet statt, wenn alle sportlichen Wettkämpfe abgeschlossen sind. Seit 1956 sind die Schlussfeiern weit weniger formell und strukturiert als die Eröffnungsfeiern. Erneut marschieren die Athleten ins Stadion ein, diesmal jedoch nicht nach Ländern geordnet, sondern bunt gemischt. Damit wird die Verbundenheit der Athleten nach Ende der Wettkämpfe symbolisiert. Der IOC-Präsident hält eine Rede, in der er den Erfolg der Spiele betont. Danach übergibt er Mitgliedern des Organisationskomitees den Olympischen Orden und erklärt die Spiele für beendet; gleichzeitig ruft er „die Jugend der Welt“ auf, sich in vier Jahren erneut zu versammeln. Traditionell werden drei Flaggen gehisst, jene Griechenlands, des aktuellen und des nächsten Gastgeberlandes. Darüber hinaus wird seit 1984 in Los Angeles dem Bürgermeister der nächsten Olympiastadt die olympische Flagge übergeben. Zuletzt werden die olympische Hymne gespielt und das olympische Feuer gelöscht. Anschließend stellt sich der Gastgeber der nächsten Olympischen Spiele mit einer kurzen kulturellen Darbietung vor. Ende des 20. Jahrhunderts hat es sich eingebürgert, dass im Anschluss ein Rock- und Popkonzert folgt, das aber nicht mehr zum offiziellen Teil gehört.

Das aktuelle Programm der Olympischen Spiele umfasst insgesamt 35 Sportarten, davon 28 im Sommer und sieben im Winter. Bei dieser Zählweise des IOC werden die Sportarten nach Sportverbänden zusammengefasst. Werden diese wie üblich aufgeteilt, ergeben sich 41 Sommer-Sportarten und 15 Winter-Sportarten (siehe olympische Sportarten). Im Programm sämtlicher Sommerspiele enthalten waren Leichtathletik, Schwimmen, Fechten und Kunstturnen. Bei sämtlichen Winterspielen wurden Wettkämpfe im nordischen Skisport, Eisschnelllauf, Eiskunstlauf und Eishockey ausgetragen, die beiden letztgenannten vor 1924 auch bei Sommerspielen. Bis 1992 wurden oft auch Wettkämpfe in so genannten Demonstrationssportarten durchgeführt. Absicht war es, diese Sportarten einem größeren Publikum vorzustellen. Die Gewinner dieser Wettbewerbe gelten nicht als offizielle Olympiasieger. Manche Sportarten waren nur in den jeweiligen Gastgeberländern populär, andere hingegen werden weltweit betrieben. Einige dieser Demonstrationssportarten wie Curling und Taekwondo wurden schließlich ins offizielle Programm aufgenommen.

Reglementiert werden die olympische Sportarten von internationalen Sportverbänden, die das IOC als globale Aufsichtsbehörden anerkennt. Zurzeit sind 35 Sportverbände im IOC vertreten. Darüber hinaus erkennt das IOC aufgrund weltweiter Verbreitung und Einhaltung bestimmter Standards diverse Sportverbände an, die nicht im offiziellen Wettkampfprogramm mit Wettbewerben vertreten sind (siehe Liste der vom IOC anerkannten internationalen Verbände). Im Rahmen einer Programmrevision anlässlich einer IOC-Session können solche Sportarten mit einer Zweidrittelmehrheit der IOC-Mitglieder ins offizielle Programm aufgenommen oder auch ausgeschlossen werden.

2004 bildete das IOC eine Kommission "(Olympic Programme Commission)", die mit der Beurteilung des olympischen Programms und aller nichtolympischen Sportarten der anerkannten Verbände beauftragt wurde. Ziel war es, für die Planung des Programms zukünftiger Olympischer Spiele ein systematisches Vorgehen festzulegen. Die Kommission legte sieben Kriterien fest, an denen eine aufzunehmende Sportart gemessen wird: Geschichte und Tradition der Sportart, Verbreitung, Beliebtheit, Gesundheit der Athleten, Entwicklung des zuständigen Sportverbandes und Kosten der Ausrichtung. Erstmals kam dieses Verfahren 2005 zur Anwendung, als das IOC-Exekutivkomitee anlässlich der Session in Singapur fünf Sportarten empfahl. Squash und Karate kamen in die engere Auswahl, erhielten jedoch nicht die notwendige Zweidrittelmehrheit, um ins offizielle Programm aufgenommen zu werden. Erfolgreich waren vier Jahre später am Olympischen Kongress 2009 in Kopenhagen die Sportarten Golf und 7er-Rugby, die seit 2016 Teil des Programms sind.

An seiner Session in Mexiko-Stadt im Jahr 2002 beschloss das IOC, das Programm der Olympischen Sommerspiele auf 28 Sportarten, 301 Wettbewerbe und 10.500 Athleten zu begrenzen. Drei Jahre später wurde in Singapur die erste umfassende Programmrevision vorgenommen. Dabei fiel der Beschluss, Baseball und Softball aus dem Programm der Sommerspiele 2012 zu streichen. Da sich die IOC-Mitglieder nicht über die Aufnahme zweier anderer Sportarten als Ersatz einigen konnten, standen 2012 nur 26 Sportarten auf dem Programm. Mit der Aufnahme von Golf und Rugby sind es seit 2016 wieder 28.

Pierre de Coubertin war maßgeblich vom Ethos der Aristokratie beeinflusst, das an englischen Privatschulen vorgelebt wurde. Ihrer Ansicht nach bildete Sport einen wichtigen Teil der Erziehung; eine Haltung, die in der Redewendung "mens sana in corpore sano" (lat.: „ein gesunder Geist in einem gesunden Körper“) zum Ausdruck kommt. Gemäß diesem Ethos war ein Gentleman eine Person, die vieles gut kann, nicht jedoch der beste auf einem bestimmten Gebiet. Vorherrschend war auch das Konzept der Fairness, das Üben oder Training mit Betrug gleichsetzte. Profisportler hatten somit den Ruf, sich gegenüber Amateuren einen unfairen Vorteil zu verschaffen.
Der Ausschluss von Profis von der Teilnahme an Olympischen Spielen hatte zur Folge, dass es immer wieder zu Kontroversen und aufsehenerregenden Konflikten um die Ausgrenzung oder Zulassung von Sportlern kam. Beispielsweise wurde Jim Thorpe, der Olympiasieger von 1912 im Fünfkampf und im Zehnkampf, disqualifiziert, nachdem bekannt geworden war, dass er zuvor halbprofessionell Baseball gespielt hatte; erst 1983 rehabilitierte ihn das IOC. Skiläufer aus der Schweiz und Österreich blieben den Winterspielen 1936 fern, um damit ihre Solidarität mit den Skilehrern zu bekunden, die gemäß Weisung des IOC als Profisportler nicht teilnahmeberechtigt waren. IOC-Präsident Avery Brundage schloss den österreichischen Skiläufer Karl Schranz kurz vor den Winterspielen 1972 in Sapporo wegen eines Verstoßes gegen den Amateurstatus aus. Als Schranz nach Wien zurückkehrte, bereiteten ihm mehrere Zehntausend Menschen einen heroischen Empfang.

Die aristokratisch geprägten Amateurregeln wurden immer offensichtlicher von der Entwicklung des Sports überholt und galten zunehmend als Heuchelei. Insbesondere waren Athleten aus kommunistisch regierten Ländern eigentlich Staatsangestellte („Staatsamateure“), die effektiv die Möglichkeit erhielten, sich vollständig dem Sport zu widmen und deshalb nur dem Namen nach Amateure waren. Weiterhin hatten Sportler in westlichen Ländern die Möglichkeit, sich als Sportsoldaten ausschließlich auf das Training zu konzentrieren. Auch Sportler aus finanziell abgesicherten sozialen Schichten waren in der Lage, sich ohne berufliche Tätigkeit der Wettkampfvorbereitung zu widmen. Dennoch hielt das IOC lange unbeirrt am Amateurstatus fest.

Ab Ende der 1970er Jahre wurden die Amateurregeln gelockert und in den 1990er Jahren schließlich ganz aufgehoben. Das sichtbarste Zeichen für diesen Sinneswandel war die Zulassung des „Dream Team“, das gänzlich aus gutbezahlten NBA-Stars zusammengesetzt war und 1992 überlegen die Basketball-Goldmedaille gewann. Von 2004 bis 2016 war Boxen die einzige Sportart, in der keine Profis zugelassen waren, wobei selbst hier der Amateurstatus sich auf die Kampfregeln bezog und nicht auf die Bezahlung. Seit 2016 dürfen auch Profiboxer antreten. Im Fußballturnier der Männer (jedoch nicht in jenem der Frauen) ist die Anzahl der über 23-jährigen Spieler auf drei pro Mannschaft begrenzt.

Das stetige Wachstum und die zunehmende internationale Bedeutung der Olympischen Spiele führte auch zu zahlreichen Problemen. So geriet in der Vergangenheit das IOC verstärkt unter Druck. Es wurde als unbewegliche, unflexible, kommerzielle und intransparente Organisation kritisiert. Besonders kontrovers waren die Präsidentschaften von Avery Brundage und Juan Antonio Samaranch. Brundage musste sich die Kritik gefallen lassen, er sei rassistisch und antisemitisch. Unter Samaranch galt das IOC als autokratisch und korrupt. Auch seine engen Beziehungen zum Franco-Regime und seine lange Amtszeit von 21 Jahren (er trat erst im Alter von 81 Jahren zurück) gaben Anlass zur Kritik. Ebenfalls Anlass zu Kritik gab die Tatsache, dass zahlreiche IOC-Mitglieder in sehr fortgeschrittenem Alter waren und teilweise bis zu ihrem Tod im Amt blieben.

1998 wurde bekannt, dass mehrere IOC-Mitglieder bestochen worden waren, damit sie bei der Wahl des Austragungsortes der Winterspiele 2002 ihre Stimme der Stadt Salt Lake City gaben. Das IOC führte eine Untersuchung durch, in deren Folge vier Mitglieder zurücktraten und sechs weitere ausgeschlossen wurden. Die Aufarbeitung des Skandals zog Reformen nach sich. Unter anderem wurde das Auswahlverfahren geändert, um weitere Bestechungen zu vermeiden. Das IOC ernannte zahlreiche aktive und ehemalige Athleten zu Mitgliedern und beschränkte die Amtszeit.

Im August 2004 strahlte der britische Fernsehsender BBC eine Dokumentation mit dem Titel "Buying the Games" („Wie die Spiele gekauft werden“) aus. Er untersuchte dabei Korruptionsvorwürfe im Zusammenhang mit der Vergabe der Sommerspiele 2012 und wies nach, dass es noch immer möglich sei, IOC-Mitglieder zu bestechen, damit sie sich für eine bestimmte Stadt entscheiden.

Entgegen Pierre de Coubertins Hoffnungen verhinderten die Olympischen Spiele nicht den Ausbruch von Kriegen. Tatsächlich konnten mehrere Veranstaltungen nicht ausgetragen werden: Die Sommerspiele 1916 entfielen wegen des Ersten Weltkriegs, die Sommer- und Winterspiele von 1940 und 1944 wegen des Zweiten Weltkriegs.
Die Nationalsozialisten benutzten erfolgreich die Winterspiele 1936 in Garmisch-Partenkirchen und die Sommerspiele 1936 in Berlin als Propagandaforum, um das Ansehen Deutschlands im Ausland zu verbessern und um guten Willen und Friedensbereitschaft vorzutäuschen. Auch sollte die angebliche Überlegenheit der „arischen Rasse“ demonstriert werden, was angesichts der Erfolge von Jesse Owens jedoch nicht gelang. Antisemitische Parolen wurden vorübergehend entfernt und das Hetzblatt "Der Stürmer" durfte für die Dauer der Spiele nicht öffentlich in Kiosken ausliegen.

Die Sowjetunion nahm bis 1952 nicht an Olympischen Spielen teil. Hingegen organisierte sie ab 1928 Spartakiaden. Während der Zwischenkriegszeit fanden mehrmals Arbeiterolympiaden statt. Diese Veranstaltungen waren Alternativen zu den Olympischen Spielen, die als kapitalistisch und aristokratisch galten. Mehrere kürzlich unabhängig gewordene (meist sozialistische) Staaten veranstalteten in den 1960er Jahren vom IOC nie anerkannte Gegenveranstaltungen. Sie trugen den Namen GANEFO ("Games of the New Emerging Forces", dt: „Spiele der neu aufstrebenden Kräfte“) und fanden 1963 in Jakarta sowie 1966 in Phnom Penh statt. Die chinesische Kulturrevolution verhinderte die dritte Austragung 1969.

Zehn Tage vor der Eröffnung der Sommerspiele 1968 in Mexiko-Stadt kam es zum Massaker von Tlatelolco, als bei der brutalen Niederschlagung von Studentenprotesten zwischen 300 und 500 Studenten getötet wurden. Ein demgegenüber vergleichsweise kleinerer politischer Zwischenfall ereignete sich bei diesen Spielen, als die zwei US-amerikanischen Leichtathleten Tommie Smith und John Carlos während der Siegerehrung des 200-Meter-Laufs ihre Fäuste mit schwarzen Handschuhen in die Höhe streckten. Es handelte sich dabei um das Symbol der Bewegung Black Power, die sich gegen die Diskriminierung der afroamerikanischen Bevölkerung in den USA richtete. Das IOC stellte das Olympische Komitee der USA (USOC) vor die Wahl, entweder die beiden Athleten nach Hause zu schicken oder die ganze Leichtathletik-Mannschaft zurückzuziehen. Das USOC entschied sich für ersteres.
Während der Sommerspiele 1972 in München nahm die palästinensische Terrororganisation "Schwarzer September" elf Mitglieder der israelischen Mannschaft gefangen. Eine missglückte Befreiungsaktion auf dem Flugplatz Fürstenfeldbruck führte zum Tod aller Geiseln sowie von fünf Terroristen und einem Polizeibeamten. Die Geiselnahme von München blieb weltweit als „München-Massaker“ in Erinnerung. IOC-Präsident Avery Brundage setzte sich für die Fortführung der Spiele ein, berühmt geworden ist sein Ausspruch "„The games must go on“" („Die Spiele müssen weitergehen“). Das tragische Ereignis wurde mehrmals verfilmt, beispielsweise durch Kevin Macdonald ("Ein Tag im September", 1999) und Steven Spielberg ("München", 2006).

Die Sowjetunion versuchte die Sommerspiele 1984 in Los Angeles zu sabotieren. Sie schickte den Nationalen Olympischen Komitees von elf asiatischen und afrikanischen Nationen Drohbriefe, die angeblich vom Ku-Klux-Klan stammen sollten und den Athleten, besonders den Dunkelhäutigen, mit Erschießungen und Lynchmord drohten. Dass die Briefe gefälscht waren, konnte jedoch schnell nachgewiesen werden.

Im Centennial Olympic Park von Atlanta explodierte während der Sommerspiele 1996 eine Bombe. Dabei starben zwei Menschen und 111 wurden verletzt. Die Bombe war von Eric Rudolph gelegt worden, der der rassistischen Christian-Identity-Bewegung nahesteht. Nach einer fast siebenjährigen Flucht konnte er 2003 verhaftet werden. Zunächst war der Wachmann Richard Jewell beschuldigt und in einer beispiellosen Medienkampagne vorverurteilt worden.

Der Kaukasuskrieg zwischen Georgien und Russland brach am Eröffnungstag der Sommerspiele 2008 in Peking aus. Beim Luftpistolenschießen der Frauen gewann die Russin Natalja Paderina die Silber- und die Georgierin Nino Salukwadse die Bronzemedaille. Beide Frauen umarmten und küssten sich demonstrativ auf dem Siegerpodest und setzten so ein viel beachtetes Zeichen gegen den Krieg.

Mit dem Schlagwort Olympiaboykott bezeichnet man die Entscheidung einzelner Länder oder Ländergruppen, nicht an Olympischen Spielen teilzunehmen. Die Olympischen Spiele der Neuzeit wurden mehrmals aus meist politischen Gründen von einem oder mehreren Staaten boykottiert.

Den ersten Versuch eines Olympiaboykotts gab es bereits im Vorfeld der Spiele von 1896. Unter dem Motto „Olympiateilnahme ist Vaterlandsverrat“ versuchten nationalistische Kreise eine deutsche Olympiateilnahme zu verhindern, was jedoch scheiterte. Durch die deutsch-französische Erbfeindschaft ideologisch geprägt, störten sie sich an der Person Pierre de Coubertins und an der damals noch ungewohnten Idee internationaler Sportveranstaltungen. Aufgrund der Machtübernahme der Nationalsozialisten gab es Bestrebungen in den verschiedensten Ländern die Olympischen Spiele 1936 zu boykottieren. Am intensivsten war die Diskussion in den USA, wo am Ende der amerikanische Sportbund AAU nur mit drei Stimmen Mehrheit die Teilnahme beschloss. Ohne Unterschrift des Sportverbandes wäre die Teilnahme kaum möglich gewesen, da nur der Verband die Amateureigenschaft des Sportlers bestätigen konnte. Die Niederlande, Spanien und die Schweiz boykottierten die Sommerspiele 1956 in Melbourne aus Protest gegen die Niederschlagung des ungarischen Volksaufstands durch die Sowjetunion. Wegen der Sueskrise im selben Jahr blieben auch Ägypten, der Irak, Kambodscha und der Libanon dieser Veranstaltung fern.

1972 und 1976 drohte eine große Anzahl afrikanischer Staaten mit einem Boykott, falls das IOC sich weigern sollte, Südafrika und Rhodesien von den Spielen auszuschließen. Das IOC gab in beiden Fällen nach, um damit ein Zeichen gegen die Rassendiskriminierung zu setzen. 1976 forderten die Afrikaner auch den Ausschluss Neuseelands von den Spielen. Die neuseeländische Rugby-Union-Nationalmannschaft hatte in Südafrika gespielt und damit den Sportbann gegen das Apartheid-Regime gebrochen. Weil jedoch Rugby Union damals keine olympische Sportart war, lehnte das IOC den Ausschluss aller neuseeländischen Sportler ab. 28 afrikanische Staaten zogen daraufhin ihre Mannschaften aus Montreal zurück (einige Athleten waren bereits im Einsatz gewesen). Lediglich der Irak und Guyana solidarisierten sich mit den Afrikanern. Auf Druck der Volksrepublik China teilte die kanadische Regierung der Mannschaft der Republik China mit, dass sie nicht unter diesem Namen antreten dürfe. Der Kompromissvorschlag "Taiwan" stieß auf Ablehnung und die Republik China verzichtete auf eine Teilnahme. Erst seit 1984 nimmt sie unter der Bezeichnung "Chinese Taipei" wieder teil, mit einer vom IOC eigens für diesen Zweck gestalteten Flagge.
1980 und 1984 boykottierten die Supermächte des Kalten Kriegs gegenseitig die Spiele im Land des Gegners. Die USA weigerten sich, an den Sommerspielen 1980 in Moskau teilzunehmen; Grund war die sowjetische Invasion in Afghanistan ein Jahr zuvor. Mit der Bundesrepublik Deutschland, Kanada, Norwegen und der Türkei folgten vier der 15 verbündeten NATO-Staaten dem Aufruf der US-Amerikaner, ebenso wie 37 weitere NOK hauptsächlich von Dritte Welt- bzw. islamisch geprägten Ländern. Entgegen entschieden sich der Großteil der westlichen Staaten wie Großbritannien, Italien, Frankreich, Spanien oder Österreich gegen einen Boykott und für eine differenzierte Form des Protests, beispielsweise der Nichtteilnahme an Eröffnungs- oder Abschlussveranstaltung oder der Verwendung der olympischen Flagge statt ihrer Nationalflagge. Weitere 24 NOK verzichteten aus finanziellen oder sportlichen Gründen auf eine Teilnahme oder ließen die Einladung unbeantwortet, so dass am Ende 66 Staaten den Spielen von Moskau fernblieben.

Die Sowjetunion wiederum nahm nicht an den Sommerspielen 1984 in Los Angeles teil. Sie begründete dies mit angeblich mangelnder Sicherheit ihrer Athleten angesichts der feindseligen Stimmung und der antisowjetischen Hysterie in den USA. Tatsächlich gab es dort spätestens nach dem Abschuss der südkoreanischen Passagiermaschine durch die sowjetische Luftwaffe am 1. September 1983 vermehrt Aktionen antikommunistischer Gruppierungen, die schließlich im Zusammenschluss der Koalition „Ban the Soviets“ gipfelte. Darüber hinaus wurde im kalifornischen Kongress sowie im kalifornischen Senat eine Resolution gegen die „sowjetische Aggression“ einstimmig gebilligt, die unter anderem einen Ausschluss der sowjetischen Athleten von den kommenden Olympischen Spielen anstrebte. Trotz weiterer Konfrontationen, so wurde dem sowjetischen Olympia-Attaché die Akkreditierung wegen angeblicher KGB-Mitgliedschaft verweigert, gab es seitens zweier US-Präsidenten die Garantie, dass alle vom IOC akzeptierten Sportler ungehindert einreisen konnten. Letzten Endes behielt die sowjetische Führung ihren Kurs bei, der jedoch auch hier unter den Verbündeten alles andere als unumstritten war. So sicherte Rumänien als Ostblock-Land dem IOC seine Teilnahme zu, auch die DDR versuchte bis zuletzt eine Umgehung des sowjetischen Beschlusses, beugte sich aber schließlich, um die zu diesem Zeitpunkt angespannten Beziehungen zu Moskau nicht weiter zu strapazieren. Am Ende schlossen sich 19 NOK dem Boykott an, der 1982 vom Iran eingeleitet worden war. Die boykottierenden Staaten trugen 1984 die Wettkämpfe der Freundschaft als Gegenveranstaltung aus.

Nach dem mittlerweile dritten großen Boykott der Olympischen Spiele verabschiedete das IOC in einer außerordentlichen Versammlung Anfang Dezember 1984 eine Resolution, in der es als „prinzipielle Pflicht eines Nationalen Olympischen Komitees“ bezeichnet wurde, die Teilnahme der Athleten seines Landes bei Olympischen Spielen zu sichern. Ein bereits 1976 vorgelegter Vorschlag Griechenlands, die Olympischen Spiele künftig ständig auf einem neutralen Territorium auf Griechenlands Staatsgebiet auszutragen, um künftigen politischen Einmischungen vorzubeugen, wurde hingegen abgelehnt. Trotzdem konnte nicht verhindert werden, dass Nordkorea die kommenden Sommerspiele 1988 in der südkoreanischen Hauptstadt Seoul boykottierte, weil das Land entgegen früheren Zusagen nicht als Co-Gastgeber berücksichtigt worden war. Verhandlungen über die Austragung einzelner Wettbewerbe in Nordkorea hatten sich über drei Jahre hingezogen und waren schließlich ergebnislos gescheitert. Äthiopien, Kuba und Nicaragua blieben aus Solidarität mit Nordkorea ebenso fern. Vor den Sommerspielen 2008 in Peking gab es in verschiedenen Ländern Boykottaufrufe wegen der gewaltsamen Tibet-Politik der Volksrepublik China und der dortigen Unterdrückung der Menschenrechte, letztlich aber ergebnislos.


Eines der Hauptprobleme bei Olympischen Spielen (und im Sport im Allgemeinen) ist die unerlaubte Leistungssteigerung durch Doping. Zu Beginn des 20. Jahrhunderts begannen zahlreiche Athleten Drogen zu sich zu nehmen; so war die Verwendung von Kokain weit verbreitet. Thomas Hicks, Gewinner des Marathonlaufs der Sommerspiele 1904, erhielt von seinem Trainer während des Rennens Brandy, der mit Strychnin angereichert war. Als Athleten und Betreuer zu immer extremeren Mitteln griffen, wurde den Verantwortlichen allmählich bewusst, dass diese Methoden nicht mehr mit dem Ideal von „Gesundheit durch Sport“ zu vereinbaren waren. Der erste (und bisher einzige bekannte) durch Doping verursachte Todesfall bei Olympischen Spielen ereignete sich 1960 in Rom, als der dänische Radsportler Knud Enemark Jensen von seinem Fahrrad fiel und später starb. Eine Autopsie ergab, dass er mit Amphetaminen gedopt gewesen war.

Nicht zuletzt aufgrund dieses Vorfalls begannen mehrere Sportverbände Mitte der 1960er Jahre mit Dopingtests, das IOC folgte 1967 diesem Beispiel. Der erste positiv auf verbotene Substanzen getestete Athlet war 1968 der Schwede Hans-Gunnar Liljenwall, der seine Bronzemedaille im modernen Fünfkampf wegen der Einnahme von Alkohol zurückgeben musste. Seither wurden Dutzende Athleten überführt, darunter mehrere Medaillengewinner. Für den größten Skandal sorgte der Kanadier Ben Johnson: Er war 1988 mit neuem Weltrekord Olympiasieger im 100-Meter-Lauf geworden, wurde dann aber positiv auf Stanozolol getestet. Trotz der Tests verwendeten viele Athleten Doping, ohne je überführt worden zu sein. Im Jahr 1990 aufgetauchte Dokumente zeigten, dass zahlreiche Athleten aus der DDR auf Anweisung der Regierung gezielt von ihren Betreuern mit anabolen Steroiden und anderen Mitteln gedopt worden waren.

Ende der 1990er Jahre begann das IOC, den Kampf gegen das Doping besser zu organisieren; die Welt-Anti-Doping-Agentur (WADA) nahm 1999 ihre Arbeit auf. Die strengeren Kontrollen durch die WADA führten ab 2000 dazu, dass deutlich mehr Sportler überführt werden konnten, insbesondere im Gewichtheben und im Skilanglauf. Die vom IOC vorgegebenen Standards in der Dopingbekämpfung dienen mittlerweile weltweit als Vorbild für weitere Sportverbände und finden auch in Anti-Doping-Gesetzen verschiedener Staaten Einzug.

Nach den Olympischen Winterspielen 2014 deckten Journalisten in Russland ein System des Staatsdopings auf. Daraufhin beauftragte die WADA den unabhängigen Ermittler Richard McLaren, einen Untersuchungsbericht anzufertigen. Der McLaren-Report wurde am 18. Juli 2016 veröffentlicht und bestätigte russisches Staatsdoping. Daraufhin empfahl die WADA den kollektiven Ausschluss Russlands von den Olympischen Sommerspielen 2016. Trotzdem erteilte das IOC 271 der 389 russischen Athletinnen und Athleten eine Starterlaubnis, lediglich in der Leichtathletik und im Gewichtheben durften keine russischen Athletinnen und Athleten antreten. Die russische Whistleblowerin Julija Igorewna Stepanowa erhielt hingegen keine Starterlaubnis, da sie laut IOC nicht die ethischen Anforderungen an eine olympische Athletin erfüllte. Die Entscheidung des IOC, russische Athletinnen und Athleten, die in der Vergangenheit bereits wegen Dopings gesperrt gewesen waren, von den Spielen auszuschließen, wurde vom Internationalen Sportgerichtshof (CAS) gekippt. Die russische Schwimmerin Julija Jefimowa, die bereits zweimal wegen Dopings gesperrt gewesen war, wurde daraufhin zum Ziel des internationalen Protests, nachdem sie über 100 m Brust die Silbermedaille gewonnen hatte. Während der Spiele beschuldigten sich mehrere Schwimmerinnen und Schwimmer gegenseitig des Dopings.

Die Olympischen Spiele bieten zuvor weniger bekannten Athleten die Möglichkeit, national und international zu viel beachteten Sportlern aufzusteigen. Weil die Olympischen Spiele nur alle vier Jahre ausgetragen werden, genießen sie bei Zuschauern und Athleten ein höheres Prestige als Weltmeisterschaften, die oft im jährlichen oder zweijährigen Rhythmus stattfinden. Viele Athleten wurden nach einem Olympiasieg zu Prominenten in ihren jeweiligen Ländern, manche sogar weltweit. Ein Vergleich der Leistungen von Athleten in verschiedenen Sportarten und zu verschiedenen Zeiten ist von begrenzter Aussagekraft. Legt man jedoch die Anzahl der Goldmedaillen zugrunde, so können die folgenden Athleten als die erfolgreichsten angesehen werden (die Olympischen Zwischenspiele 1906 werden dabei nicht mitberücksichtigt):






</doc>
<doc id="3734" url="https://de.wikipedia.org/wiki?curid=3734" title="Ostern">
Ostern

Ostern (, von "pessach") ist das Fest der Auferstehung Jesu Christi.

Da Leiden, Sterben und Auferstehung Christi laut den Aussagen des Neuen Testamentes in eine Pessachwoche fielen, bestimmt der Termin dieses beweglichen jüdischen Hauptfestes auch das Osterdatum. Es wird über einen Lunisolarkalender bestimmt und fällt in der Westkirche immer auf den Sonntag nach dem ersten Frühlingsvollmond, im gregorianischen Kalender also frühestens auf den 22. März und spätestens auf den 25. April. Danach richten sich auch die Daten der beweglichen Festtage des Osterfestkreises.

In der Alten Kirche wurde Ostern als Einheit von Leidensgedächtnis und Auferstehungsfeier in der Osternacht begangen („Vollpascha“). Ab dem 4. Jahrhundert wurde das höchste Fest im Kirchenjahr als Dreitagefeier ("Triduum Sacrum" oder "Triduum paschale)" historisierend entfaltet. Die Gottesdienste erstrecken sich seitdem in den meisten Liturgien von der Feier des letzten Abendmahls am Gründonnerstagabend – dem Vorabend des Karfreitags – über den Karsamstag, den "Tag der Grabesruhe des Herrn", bis zum Anbruch der neuen Woche am Ostersonntag.

Mit dem Ostersonntag (liturgisch "Dominica Resurrectionis", „Sonntag der Auferstehung“ [des Herrn]) beginnt die österliche Freudenzeit (Osterzeit), die fünfzig Tage bis einschließlich Pfingsten dauert. Im Mittelalter entwickelte sich aus dem ursprünglichen Triduum ein separates Ostertriduum, das die ersten drei Tage der Osteroktav von der restlichen Feierwoche abhob. Später wurde dieser arbeitsfreie Zeitraum verkürzt, bis nur noch der Ostermontag als gesetzlicher Feiertag erhalten blieb.

Viele Sprachen bezeichnen das Osterfest mit einer Wortableitung vom aramäischen "pas-cha", angelehnt an das hebräische Wort "Pessach", unter anderem:
Im nordwestdeutschen Raum hat sich die Bezeichnung "Paasken" für Ostern im Plattdeutschen bis heute erhalten. Diese Sprachtradition weist auf die wesentliche Beziehung von Tod und Auferstehung Jesu zum Auszug der Israeliten aus der Sklaverei hin und betont die bleibende Verwurzelung des Christentums im Judentum.

Die meisten slawischen Sprachen nennen das Osterfest „Große Nacht (Große Nächte)“, auf Polnisch "Wielkanoc", Tschechisch "Velikonoce" und Slowenisch "Velika noč". Hingegen verwenden das Bulgarische "Великден" und das Ukrainische "Великдень" mit der Bedeutung „Großer Tag“ (Große Tage), in gleicher Weise die baltischen Sprachen Lettisch "Lieldienas" und Litauisch "Velykos".

In den beiden sorbischen Standardsprachen lautet das Wort für Ostern "Jutry" bzw. "Jatšy" und ist nach einer Deutung aus dem althochdeutschen "ōst(a)rūn" entlehnt; nach einer anderen leitet sich vom slawischen "jutro" („der Morgen“) ab. Das ungarische "húsvét" bedeutet wörtlich „Fleisch zu sich nehmen“, ebenso das estnische "lihavõte". Der georgische Name „აღდგომა“ (aghdgoma) heißt auf Deutsch „Auferstehung“ oder „Aufstehen“ im Allgemeinen, ebenso wie das kroatische, bosnische und serbische "Uskrs" oder "Vaskrs".

Die Einführung und Kultivierung des Begriffs Ostern in Deutschland hängt eng mit der Strukturierung der fränkisch-deutschen Kirchenprovinzen zusammen. Diese waren sprachlich und klerikal unterschiedlich geprägt. Im Erzbistum Köln, der kölnischen Kirchenprovinz, die fränkisch geprägt war, herrschte der Begriff "pāsche" vor und wurde vor allem in den heute erhaltenen Dokumenten so auch geschrieben. Bonifatius hatte als Bischofssitz Mainz, und aus der angelsächsischen Tradition wurde dort in den Dokumenten "ôstarun" in angelsächsischer Anlehnung als typisches Missionswort verwendet.

Das neuhochdeutsche "Ostern" und das englische "Easter" haben die gleiche sprachliche Wurzel, zu deren Etymologie es verschiedene Lösungsansätze gibt. Das Herkunftswörterbuch des Duden leitet das Wort vom altgermanischen "Austrō > Ausro" „Morgenröte“ ab, das eventuell ein germanisches Frühlingsfest bezeichnete und sich im Altenglischen zu "Ēostre, Ēastre", im Althochdeutschen zu "ōst(a)ra", Plural "ōstarun" fortbildete. Der Wortstamm ist mit dem altgriechischen Namen der vergöttlichten Morgenröte "Ēōs" und dem lateinischen "aurora" „Morgenröte“ verwandt, die ihrerseits weitere Sprachen beeinflusst haben. Die zugrunde liegende indogermanische Wurzel ist das Substantiv *"h₂au̯s-os" „Morgenröte“, abgeleitet von einer indogermanischen Verbalwurzel *"h₂u̯es-" „(morgens) hell werden“ oder *"h₂au̯s-" „(aus dem Wasser) schöpfen, Feuer holen“.

"Ēostra" ist erstmals 738 bei Beda Venerabilis ("de temporum ratione" 15) belegt. Auf ihn geht die Vermutung zurück, das Wort habe eine angelsächsische Lichtgöttin bezeichnet, nach der der Monat April auf angelsächsisch "Ēosturmanoth" benannt war. Das Deutsche Wörterbuch der Brüder Grimm zitiert ihn mit dem Vorbehalt, er könne diese Göttin – als deren späteren Namen sie "Ostara" vermuten – erfunden haben. Die hypothetische Gottheit "Ostara" wird heute skeptischer gesehen.
Wahrscheinlicher ist, dass Beda Volkstraditionen aufgriff, die im Rahmen frühjährlicher Vegetationsriten gepflegt wurden und mit den Matronen- und Disenkulten in Verbindung standen und darüber hinaus im damaligen paganen germanischen Raum üblich waren und teilweise heute noch tradiert werden.

Wegen der Entdeckung des leeren Grabes Jesu „früh am Morgen, als eben die Sonne aufging“ ist die Morgenröte im Christentum Symbol der Auferstehung. Die "Canones Hippolyti" (um 350) gaben daher für die Osternacht die Weisung: „Alle sollen daher bis zur Morgenröthe wachen, dann ihren Leib mit Wasser waschen, bevor sie Pascha feiern, und das ganze Volk sei im Lichte“. Dies knüpfte auch an die biblische Exodustradition der Israeliten in der Nacht des „Vorübergehens“ (hebräisch "pessach", engl. ): „Eine Nacht des Wachens war es für den Herrn, als er sie aus Ägypten herausführte. Als eine Nacht des Wachens zur Ehre des Herrn gilt sie den Israeliten in allen Generationen“ .

Honorius Augustodunensis (12. Jh.) leitete "Ostern" von Osten (vgl. engl. und ) ab, der Himmelsrichtung des Sonnenaufgangs. Viele neue Christen ließen sich damals „bei Sonnenaufgang“ am Ostermorgen – althochdeutsch "zu den ostarun" – taufen. Hier knüpft auch der Namenforscher Jürgen Udolph an, der das Wort mit Bezugnahme auf den österlichen Tauftermin aus der nordgermanischen Wortfamilie "ausa" („gießen“) und "austr" („begießen“) erklärt. So wurde ein vorchristlicher Wasserritus als "vatni ausa" („mit Wasser begießen“) bezeichnet.

Eine weitere Deutung geht von der lateinischen Bezeichnung "hebdomada in albis" („weiße Woche“) für die Osteroktav aus. Da "alba" im französischen die Bedeutung „weiß“ verliert und die spezielle Bedeutung „Morgenlicht“ bzw. „Morgenröte“ annimmt, kann dies durch das entsprechende germanische Wort wiedergegeben worden sein.

Die vielfältige neutestamentliche Osterüberlieferung wird im Kern auf die Jerusalemer Urgemeinde zurückgeführt. Aus ihr stammt der emphatische Jubelruf, der bis heute viele Ostergottesdienste weltweit eröffnet :

Nach Auskunft aller Evangelien ist Jesu Auferweckung exklusive Tat Gottes und wurde von keinem Menschen beobachtet. Erst ihre Folgen werden für seine ersten Nachfolger als wahrnehmbar beschrieben: Frauen aus seiner Heimat, die sein Sterben und seine Grablegung mitangesehen hatten, entdecken, dass sein Grab leer ist. Dabei teilen Engel ihnen die Botschaft von der Auferweckung mit und senden sie zu Petrus und den übrigen verbliebenen Jüngern.

Laut der ältesten überlieferten Version im Markusevangelium kündigt der Engel ein Wiedersehen mit Jesus in Galiläa an. Die Frauen erzählen jedoch niemandem von dieser Begegnung, da sie sich fürchten. Damit endet das Evangelium wohl ursprünglich; die weiteren Abschnitte () kamen als Zusammenfassung anderer Überlieferungen erst später hinzu.

Auch in Matthäus schickt der Engel die Jünger nach Galiläa. Lukas und Johannes siedeln die übrigen Ereignisse in Jerusalem und Umgebung an, wo Jesu eigenes Reden und Handeln seine verzweifelten Jünger zum Glauben an sein neues, unzerstörbares Leben führt (; ). Jesu Begegnung mit den versammelten Erstberufenen am Abend des Ostertages ist der Durchbruch: Jesus bringt seine Jünger zum Glauben an ihn, stellt die zerbrochene Gemeinschaft mit ihm wieder her und beauftragt sie zur weltweiten Mission (; ; ).

Die Jerusalemer Urchristen hielten die Namen der ersten Osterzeugen als besonders bedeutsam für ihren Glauben fest . Paulus von Tarsus, der sich als letzter in diese Reihe stellte, erzählt, dass er dem Auferstandenen als Christenverfolger persönlich begegnet sei und von ihm zum Völkerapostel beauftragt worden sei. Er, Paulus, habe die Jerusalemer Urchristen erst Jahre danach kennengelernt .

Nach dem wohl frühesten christlichen Glaubensbekenntnis wurde Jesus am „dritten Tag gemäß der Schrift“ von den Toten erweckt . Die Angabe bezieht sich auf die Entdeckung des leeren Grabes am „ersten Tag der Woche“ (; ; ; ) und auf die Jesuserscheinung vor einigen seiner Jünger am Abend desselben Tages . Dieser Auferstehungstag folgte nach den Evangelien auf den Schabbat nach Jesu Kreuzigung, die nachmittags an einem Rüsttag zum Schabbat stattfand. Die christliche Chronologie zählt somit den Ostertag als „dritten Tag“ beginnend mit dem Kreuzigungstag als erstem Tag. Damit entspricht sie der zeitgenössischen jüdischen Praxis, bei der Angabe einer Frist auch nur teilweise betroffene Zeitabschnitte als ganze Einheit mitzurechnen.

Zudem bringt diese geprägte Formel Jesu Auferstehung mit vorgegebener Tradition in Verbindung. So ist der „dritte Tag“ im Tanach häufig der Zeitpunkt besonderer Ereigniszuspitzung , Tag einer Rettung aus Todesnot und ultimativen Wende zum Heil durch Gottes Eingreifen in die Geschichte : Mit Bezug auf die Auferstehung besonders deutlich in . Dies reflektieren auch Jesu Leidens- und Auferstehungsankündigungen, die in den synoptischen Evangelien seine Passionsgeschichte einleiten und gliedern. Das Markusevangelium bevorzugt dabei den Ausdruck „nach drei Tagen“ (μετὰ τρεῖς ἡμέρας: ; ; ), der jedoch eine Binnenfrist, keine Ablauffrist angibt, wie die Aussage „innerhalb von drei Tagen“ (; ) bestätigt. Im Matthäusevangelium dominiert die Ordinalzahl mit bestimmtem Artikel (; ; ; nicht ). Diese findet sich auch im Lukasevangelium (; ; ; nicht in ).

Während diese Ankündigungen häufig als nachträgliche Redaktion von Urchristen gelten, enthalten auch einige mögliche echte Leidens- und Todesankündigungen Jesu eine Dreitagesangabe: so das Rätselwort vom „Zeichen des Jona“ (), dessen Angabe „nach drei Tagen und drei Nächten“ dem Osterdatum jedoch widerspricht, und das Wort vom Tempelabriss und -neubau „in drei Tagen“, das die Urchristen auf Jesu Tod und Auferstehung bezogen ().

Jesu Kreuzigung fand nach den Synoptikern am Hauptfesttag des Pessach, dem 15. Nisan, statt. Nach dem Johannesevangelium dagegen starb er am 14. Nisan zur selben Zeit, als die Pessachlämmer im Jerusalemer Tempel geschlachtet wurden.

Jesu Tod wird somit im Urchristentum in die Leidensgeschichte, andererseits die Befreiungshoffnung Israels eingezeichnet. Seine Auferstehung wird als Bekräftigung dieser Hoffnung verstanden und ihre Ausweitung auf alle Völker erwartet.

Die christliche Eucharistie geht zurück auf das in den Evangelien dargestellte Abendmahl Jesu, das bei den Synoptikern ein Pessachmahl ist . Hinzu kommt aus dem Johannesevangelium und vor allem bei Paulus aus das Symbol des Agnus Dei, das an die bis 70 n. Chr. im Tempel geschlachteten Pessachtiere erinnert.

Ostern gehört zu den beweglichen Festen, deren Kalenderdatum jedes Jahr variiert. Der Ostersonntag hängt vom Frühlingsvollmond ab, wobei der Frühlingsanfang festgelegt ist auf den 21. März und anders berechnet wird als im jüdischen Kalender.

Nachdem auf dem Ersten Konzil von Nicäa im Jahre 325 eine erste allgemeinverbindliche Regelung beschlossen worden war, kam es durch die Einführung des gregorianischen Kalenders erneut zu einem unterschiedlichen Osterdatum. Die Ostkirchen (mit Ausnahme der Finnisch-Orthodoxen Kirche und der Ostsyrischen Kirche) nahmen den gregorianischen Kalender zur Berechnung der beweglichen Feste nicht an, so dass der Ostertermin der westlichen Christenheit von dem der orthodoxen und altorientalischen Kirchen um bis zu fünf Wochen voneinander abweichen kann.

Alle übrigen beweglichen christlichen Feste werden vom Ostersonntag aus berechnet.

Der österliche Festkreis beginnt in den westlichen Kirchen seit dem Jahr 1091 mit dem Aschermittwoch, dem eine 40-tägige Fastenzeit folgt. Diese erinnert an die 40 Jahre der Israeliten in der Wüste sowie an die 40 Tage, die Jesus in der Wüste fastete und betete. Die Fastenzeit, auch österliche Bußzeit genannt, endet mit dem 40. Tag am Karsamstag. Das östliche Christentum rechnet die Sonntage zur Fastenzeit mit hinzu, zählt aber andererseits die Woche vor dem Ostersonntag nicht mit zu den 40 Fastentagen, sondern als eigene Zeitperiode.

Diese letzte Woche vor Ostersonntag, die Karwoche, beginnt mit dem Palmsonntag, an dem die Christen den Einzug Jesu in Jerusalem feiern. In der Karwoche ist es in einigen Gemeinden üblich einen Frühjahrsputz durchzuführen, damit die Kirche zum höchsten Fest der Christen in einem neuen Glanz erstrahlt. Am Gründonnerstag feiert das Christentum das letzte Abendmahl Jesu mit seinen Jüngern.

Am folgenden Karfreitag wird des Todes Jesu am Kreuz gedacht, am Karsamstag ist Grabesruhe, und am dritten Tag, dem Ostersonntag, wird schließlich die Auferweckung Jesu von den Toten gefeiert.

Die westkirchliche Theologie versteht das Gedenken an Leiden und Kreuzestod Christi, seine Auferstehung von den Toten und seine Himmelfahrt und Erhöhung als Einheit, das in der Liturgie gegenwärtig gesetzt wird. Die römisch- wie die altkatholische Theologie beschreiben es als „Pascha-Mysterium“, in dem Jesus Christus „durch seinen Tod […] unseren Tod vernichtet und durch seine Auferstehung das Leben neu geschaffen“ hat.

In den evangelischen Kirchen wird der Zusammenhang – ohne auf den Mysteriumsbegriff zurückzugreifen – in einer eigenen Präfation zum Osterfest gleichfalls ausgedrückt: „Geopfert ist unser Osterlamm, Christus. Durch ihn hast du hinweg genommen die Sünde der Welt, sein Sterben lässt du für uns zum Sieg werden über den Tod, in seiner Auferstehung schenkst du uns wieder das Leben. Darum jubelt [heute] der ganze Erdkreis in österlicher Freude“.

Ostern war in den ersten christlichen Jahrhunderten der einzige ordentliche Tauftermin. Seit karolingischer Zeit erfüllten die Osterspiele für die zumeist ungebildeten Gläubigen eine bedeutende katechetische Rolle, da die liturgische Auferstehungsfeier in der Westkirche zur Klerikerliturgie verkümmerte, die bereits am Karsamstagmorgen vorgefeiert wurde.

Die Ostkirchen haben demgegenüber bis heute an der Feier als Nachtwache vom Abend bis zum Morgen festgehalten, während in den meisten Kirchen des Abendlandes bis zur Wiederherstellung der Osternachtliturgie die „Messe am Tag“ (Hochamt) den Höhepunkt des Osterfestes bildete.

Da die österliche Freudenzeit nach dem Zeugnis des Neuen Testaments am frühen Morgen des ersten Tages der Woche mit der Entdeckung des leeren Grabes Jesu begann, endet die Osternachtliturgie zum Sonnenaufgang mit der Feier der Eucharistie. Die Morgenröte, das Erscheinen des Lichts nach finsterer Nacht, ist in vielen Kirchenliedern, literarischen Werken und künstlerischen Darstellungen wiederkehrendes Symbol für die Auferstehung Christi und die kommende Auferstehung aller Menschen.

So heißt es in dem Ambrosius von Mailand zugeschriebenen und aus dem 4. oder 5. Jahrhundert stammenden Hymnus "Aurora lucis rutilat", der zu den Laudes des Ostersonntags gesungen wird:

In deutschsprachigen Ländern und den Niederlanden suchen die Kinder bunt bemalte versteckte Hühnereier und Süßigkeiten, die vom „Osterhasen“ versteckt wurden. Es gibt auch den Brauch, Zweige in Vasen oder auf Bäumen im Garten mit bunt bemalten Ostereiern zu schmücken. Als Ostergebäck gibt es einen Kuchen in Hasen- oder Lammform. Bräuche zum Osterei sind das Ostereiertitschen, Ostereierschieben, Ostereierwerfen und Eierschibbeln.

In katholischen Gemeinden werden die Kirchenglocken zwischen Karfreitag und der Osternacht nicht geläutet. In einigen Gemeinden, vorwiegend im süddeutschen Raum, aber auch in Luxemburg, ziehen stattdessen Kinder und Jugendliche mit speziellen Ratschen oder Klappern durch das Dorf, um zu den Gottesdiensten und zum Angelusgebet zu rufen.

In Frankreich, Österreich, aber auch in überwiegend katholischen Regionen Deutschlands erzählt man den Kindern, dass die Glocken am Karfreitag nach Rom fliegen und am Ostersonntag zurückkommen, um zu erklären, wieso sie nicht läuten. Die Glocken würden auf dem Rückweg aus Rom Süßigkeiten für die Kinder verstecken. Die Suche nach den versteckten Süßigkeiten findet in Frankreich – im Gegensatz zu den deutschsprachigen Ländern – erst am Ostermontag statt.

In einigen Gegenden ist auch die Speisensegnung (in Teilen Österreichs Fleischweihe genannt) am Gründonnerstag oder am Karsamstag gebräuchlich, wobei traditionelle Osterspeisen (Osterschinken, Würste, Zunge, Meerrettich, Eier) gesegnet werden. Bei den Kindern ist das „Eierpecken“ sehr beliebt: Jeder Teilnehmer erhält ein Ei und stößt es mit jenem von einem anderen Teilnehmer zusammen. Derjenige, dessen Ei bis zum Schluss ganz bleibt, hat gewonnen.

In Polen werden am Karsamstag Speisen für das Frühstück am Ostersonntag gesegnet (siehe Święconka). Am Ostermontag besprengt man sich gegenseitig mit Wasser (siehe Śmigus-dyngus).

In Bulgarien, Griechenland, Russland, Serbien und Schweden werden hartgekochte Eier rot bemalt oder teils nach altem Brauch mit Hilfe von Zwiebelschalen rot gefärbt als Symbol für das neue Leben, das durch das Opfer Christi erworben wurde. In Russland ist es außerdem üblich, neben Ostereiern traditionelle Osterspeisen (Kulitsch, Pascha) am Karsamstag weihen zu lassen.

In Griechenland wird nach der Auferstehungsliturgie die "Majiritsa", eine Suppe aus den Innereien des Lamms gegessen, das dann im Laufe des Ostersonntags am Spieß gegrillt wird und am Abend werden in vielen griechischen Gemeinden Feuerwerke und Knallkörper gezündet. Während der Ostertage begrüßt man sich – wie auch in allen anderen orthodoxen Ländern – mit dem Ostergruß: Χριστὸς ἀνέστη! (‚Christus ist auferstanden!‘) Der so Gegrüßte antwortet: Ἀληθῶς ἀνέστη! (‚Er ist wahrhaftig auferstanden!‘).

In Tschechien, der Slowakei, Ungarn und Rumänien wird am Ostermontag ein Brauch ausgeübt, bei dem die Männer Frauen mit Wasser, in Ungarn mit Parfüm, besprengen und mit einer Art handgemachten Rute – pomlázka (Tschechien) – korbáč (Slowakei) – die mit bunten Bändern geschmückt ist, „symbolisch“ (d. h. ohne weh zu tun) schlagen. Der Überlieferung nach soll dies die Gesundheit und Schönheit der betroffenen Frauen im kommenden Jahr erhalten. Frauen, die dabei übersehen werden, können sich unter Umständen beleidigt fühlen. Im Gegenzug schenkt die Frau dem Mann ein bunt bemaltes Ei oder auch einen geringen Geldbetrag. In manchen Gegenden kann sich die Frau dann am Nachmittag oder am darauf folgenden Tag revanchieren, indem sie Männer mit einem Eimer kalten Wassers übergießt.
In der sorbisch-katholischen Oberlausitz um Bautzen ziehen beim Osterreiten am Ostersonntag mehrere Prozessionen von einer Pfarrgemeinde in die Nachbargemeinde, um die Botschaft der Auferstehung singend zu verkünden. An den neun sorbischen Prozessionen nehmen jährlich etwa 1.500 Reiter teil. Auch in Ostritz an der Neiße wird dieser Brauch gepflegt, hier jedoch auf Deutsch. Die Prozessionen werden jedes Jahr von Tausenden Besuchern verfolgt.

Die Ukraine, Tschechien, die Slowakei und Polen sowie die sorbischsprachigen Gebiete in Deutschland (Brandenburg, Sachsen) sind wohl die Länder mit der kunstvollsten Eierbemal-Tradition. Auf den "Pisanki" (pl.) bzw. "Писанки" (ukr.) und "velikonoční kraslice" (cz.) (Bemalungen auf den Eiern) werden mit flüssigem Wachs Ornamente aufgetragen, die Eier in einer Farbstofflösung gekocht und in einem mit Gras oder ähnlichem Material ausgelegten Korb verschenkt. Für das sorbische Osterei gibt es vier verschiedene Techniken, die sich regionalgeografisch unterscheiden.

In Italien gibt es die „Torta di Pasquetta“: eine Art Gugelhupf mit gekochten Eiern, Spinat und der sogenannten „Ostertaube“. Am Karfreitag findet in vielen Orten eine Prozession statt, bei der das Kreuz schweigend durch die Straßen getragen wird. Die Auferstehung wird traditionell am zweiten Feiertag mit der Familie und Freunden mit Picknick gefeiert.

In Finnland schlagen Freunde und Bekannte einander leicht mit einer Birkenrute, um an die Palmzweige, mit denen Jesus in Jerusalem empfangen wurde, zu erinnern. Am Ostersonntag ziehen Kinder mit Trommeln und Tröten durch die Straßen zur Beendigung der Trauerzeit. In Finnland ist Ostern auch das Fest der Kerzen.

In Mexiko feiert man für etwa zwei Wochen eine Art Volksfest mit Musik und Tanz. Die Straßen sind mit Girlanden geschmückt. Am Karfreitag ist es ruhig, und es finden Prozessionen statt.

In Schweden gehen Frauen nachts heimlich und schweigend an eine Quelle, um das Osterwasser zu holen. Schaffen sie es, dabei nicht gesehen zu werden und mit dem Wasser ihren Liebsten zu benetzen, dann erobern sie damit seine Liebe. Ostern wird mit Feuerwerk und Lärm gefeiert. Die „Osterhexen“ werden symbolisch am Osterfeuer verjagt. Am Gründonnerstag verkleiden sich die schwedischen Kinder als „Osterweiber“ (Påskkärring). Sie laufen mit langen Röcken und Kopftüchern durch die Straßen und betteln an den Türen um Süßigkeiten, als „Bezahlung“ überreichen sie selbstgemalte Osterbilder.

In England lässt man die bunten Eier an abschüssigen Straßen etc. hinunterrollen, bis die Schale ganz kaputt ist.

In den USA gibt es die traditionelle „Easter Parade“ auf der 5th Avenue in New York City. Man verkleidet sich und fährt mit bunt geschmückten Wagen durch die Straßen. Am Weißen Haus in Washington findet das Eierrollen („The White House Easter Eggs Roll“) statt, wobei jeder Teilnehmer ein vom Präsidenten und seiner Gattin signiertes Holzei erhält.

Auf den Philippinen pflegt man auch den Brauch mit Hasen und bunten Ostereiern. Wenn die Osterglocken läuten, fassen die Eltern die kleinen Kinder beim Kopf und heben sie hoch. Sie glauben, dass die Kinder so größer werden.

In Kroatien wird eine Art Kasseler Rippenspeer in der Kirche gesegnet und anschließend mit Meerrettich und hart gekochten Eiern als Osteressen serviert.

Auf der Südhalbkugel fällt Ostern in den Herbst, weshalb der Charakter des Brauchtums sich dort teilweise unterscheidet. In Südamerika südlich des Äquators wird vielerorts mit Blumenschmuck in den Straßen ein frühlingshaftes Ambiente imitiert.

In Australien schöpfen verlobte Paare Ostern fließendes Wasser aus einem Bach und bewahren es bis zu ihrem Hochzeitstag auf. Bevor sie zur Kirche gehen, besprengen sie sich gegenseitig damit. Dies soll Glück bringen.

Weitere österliche Bräuche und Symbole verschiedenster Herkunft sind:

Der Ostermorgen hat traditionell auch viele Künstler angeregt, zum Beispiel:







</doc>
<doc id="3735" url="https://de.wikipedia.org/wiki?curid=3735" title="Otto Spamer">
Otto Spamer

Johann Christian Gottlieb Franz Otto Spamer (* 29. August 1820 in Darmstadt; † 27. November 1886 in Leipzig) war Buchhändler und Verleger für Bücher und Zeitschriften in Leipzig.

Nach einer Lehre bei Eduard Heil in Darmstadt und einigen Jahren, in denen er in Aschaffenburg und in Leipzig bei Johann Jacob Weber Berufserfahrung sammelte, machte er sich 1847 in Leipzig als Buchhändler selbständig. Die Revolution von 1848 verschlug ihn über Wien bis in die Türkei. Nach seiner Rückkehr nach Leipzig brauchte es einige Zeit, bis seine Buchhandlung wieder Fuß fassen konnte.

Sein verlegerischer Schwerpunkt lag im Bereich der Familien- und Volksschriften sowie der Kinder- und Jugendliteratur, wobei es sich in der Regel um reichlich illustrierte Ausgaben handelte.





</doc>
<doc id="3736" url="https://de.wikipedia.org/wiki?curid=3736" title="Oktober">
Oktober

Der Oktober ist der zehnte Monat des Jahres im gregorianischen Kalender. Er hat 31 Tage. Im Mittelalter galt der Oktober als heiliger Monat, in dem man bevorzugt heiratete; auch Könige heiraten meistens im Oktober. In den Ländern, in denen im Oktober die Zeit von Sommerzeit auf Normalzeit umgestellt wird, ist der Oktober der längste Monat des Jahres. Der Oktober beginnt außer in Schaltjahren mit demselben Wochentag wie der Januar.

Die Römer nannten ihren achten Monat des Jahres "mensis october" (lat. "octo" = acht). Obwohl der Monat nach der julianischen Kalenderreform 46 v. Chr. an die zehnte Stelle verschoben wurde, blieb es bei seinem römischen Namen. Dies wird manchmal bei der Übertragung früher verwendeter lateinischer Datumsangaben („10ber“ & „8ber“) übersehen. Zur Regierungszeit Kaiser Tiberius schlug der Senat vor, den Oktober nach seiner Mutter Livia Drusilla in "Livius" umzubenennen, dies lehnte der Kaiser allerdings ab, um nicht eine Gleichrangigkeit mit seiner Mutter akzeptieren zu müssen. Zur Regierungszeit Kaiser Domitian wurde der Monat dann in "Domitianus" umbenannt, das sich aber im Gegensatz zu Juli und August nicht durchsetzte. Zum Beginn der Regierungszeit des Antoninus Pius schlug der Senat wiederum vor, den Oktober nach dem Namen seiner Frau Annia Galeria Faustina in "Faustinus" umzubenennen, der Kaiser lehnte allerdings ab. Unter Kaiser Commodus hieß der Monat dann "Hercule(u)s", nach dem griechischen Halbgott Herakles, auch diese Umbenennung wurde nach dem Tod des Kaisers wieder rückgängig gemacht.

Andere urdeutsche Namen sind "Weinmonat" - dieser Name soll bereits von Karl dem Großen im 8. Jahrhundert eingeführt worden sein und weist auf den Beginn der Weinlese und der weiteren Weinverarbeitung hin - oder der altdeutsche Name "Gilbhart", der sich aus gilb für die Gelbfärbung des Laubes und hart für viel zusammensetzt. Allgemein wird er wegen des Beginns der Verfärbung der Laubblätter häufig als Goldener Oktober bezeichnet. In der Jägersprache wird der Oktober auch Dachsmond genannt.

Im Oktober liegen die Sternzeichen Waage (24. September bis 23. Oktober) und Skorpion (24. Oktober bis 22. November).



</doc>
<doc id="3737" url="https://de.wikipedia.org/wiki?curid=3737" title="One-Time-Pad">
One-Time-Pad

Das One-Time-Pad (Abkürzung: OTP, deutsch: Einmalverschlüsselung oder Einmalschlüssel-Verfahren, wörtlich "Einmal-Block", nicht zu verwechseln mit dem "Einmal-Passwort-Verfahren") ist ein symmetrisches Verschlüsselungsverfahren zur geheimen Nachrichtenübermittlung. Kennzeichnend ist, dass ein Schlüssel verwendet wird, der (mindestens) so lang ist wie die Nachricht selbst. Das OTP ist informationstheoretisch sicher und kann nachweislich nicht gebrochen werden – vorausgesetzt, es wird bestimmungsgemäß verwendet.

Das Verfahren wurde zum ersten Mal durch den amerikanischen Kryptologen Frank Miller (1842–1925) bereits im Jahr 1882 vorgeschlagen, bevor es – 35 Jahre später – durch seinen Landsmann Gilbert Vernam (1890–1960) zum Patent angemeldet wurde. Der Amerikaner Joseph O. Mauborgne (1881–1971) setzte diese Idee um und nannte das Verfahren "One-Time Pad" (deutsch: "Einmal-Block"). Kurz darauf arbeiteten auch die Deutschen Werner Kunze, Rudolf Schauffler und Erich Langlotz an dieser Methode. Sie schlugen im Jahr 1921 vor, Blöcke, die mit zufällig erstellten Ziffern bedruckt waren, zur Überschlüsselung der damaligen diplomatischen Codes zu verwenden, und bezeichneten diese als "i-Wurm" (individueller Wurm). Diese Methode wurde vom diplomatischen Dienst der Weimarer Republik auch tatsächlich eingesetzt.

Seit dieser Zeit bis zum heutigen Tag, speziell auch während der Zeit des Kalten Krieges, wird dieses Verfahren verwendet. Beispielsweise war der „Heiße Draht“ (auch als das „Rote Telefon“ bekannt), also die hochsichere direkte Fernschreibverbindung zwischen dem amerikanischen Präsidenten und dem sowjetischen Generalsekretär, durch ein Einmalschlüssel-Verfahren geschützt.

Das One-Time-Pad gehört zu den polyalphabetischen Substitutionsverfahren, bei denen die einzelnen Buchstaben (oder Zeichen) in jeweils andere Buchstaben (oder Zeichen) umgewandelt (verschlüsselt) werden. Kennzeichnendes Merkmal der Einmalverschlüsselung ist die "einmalige" Verwendung eines "zufälligen" Schlüssels, der (mindestens) die gleiche Länge wie die zu verschlüsselnde Nachricht aufweist.

Sowohl der Klartext als auch der Schlüssel und das Chiffrat sind beim One-Time-Pad Zeichenfolgen derselben Länge. Während moderne Implementierungen des OTP auf Bits und Bytes basieren, lässt sich das OTP auch „klassisch“ beispielsweise mithilfe der üblichen 26 Großbuchstaben des lateinischen Alphabets benutzen. Dies kann bei Bedarf leicht durch Kleinbuchstaben, Zahlzeichen oder Sonderzeichen erweitert werden. Zur Verschlüsselung wird der Schlüssel zeichenweise mit dem Klartext kombiniert. Die Art der Kombination ist beliebig und muss nicht geheimgehalten werden. Bei Verwendung von Bits ist eine Exklusiv-Oder-Verknüpfung (XOR) von Klartext- und Schlüsselbits gebräuchlich, weil diese besonders einfach durchzuführen ist. Alternativ kann aber auch eine andere Verknüpfung, beispielsweise eine zeichenweise Addition (ohne Übertrag) verwendet werden. Benutzt man Handmethoden und nur Großbuchstaben, empfiehlt sich die Addition. Hierzu werden die Buchstaben zumeist entsprechend ihrer Position im Alphabet durchnummeriert, wobei häufig eine Nummerierung von 0 bis 25 (A=0, B=1, … Z=25) oder von 1 bis 26 (A=1, B=2, … Z=26) erfolgt. Man kann sich auch für eine beliebige andere Zuordnung entscheiden. Dies hat keinen prinzipiellen Einfluss auf die Methode oder deren Sicherheit.

Grundlegende Voraussetzungen für die Sicherheit des Einmalschlüssel-Verfahrens sind: Der Einmalschlüssel muss
Damit erfüllt das Einmalschlüsselverfahren Kerckhoffs’ Prinzip, nach dem die Sicherheit eines Kryptosystems nicht von der Geheimhaltung des Algorithmus abhängen darf, sondern nur von der Geheimhaltung des Schlüssels, in idealer Weise. Die vier Bedingungen der Schlüsselwahl stellen zusammen mit der Beschreibung des Verfahrens sicher, dass die folgenden Voraussetzungen erfüllt sind:
Unter diesen Bedingungen ist das Verfahren informationstheoretisch sicher (auch "perfekte Sicherheit" genannt) und kann auch mit beliebig hohem Rechenaufwand nicht gebrochen werden. Allein mit Kenntnis des Schlüssels kann der Geheimtext entschlüsselt werden. Diesen zu erraten oder anderweitig zu erschließen ist unmöglich, da jeder mögliche Schlüssel mit der gleichen Wahrscheinlichkeit auftreten kann. Zudem gibt es keine Möglichkeit, herauszufinden, ob ein Schlüssel richtig erraten wurde oder nicht. Würde man den Schlüssel nicht, wie oben vorausgesetzt, zufällig wählen, sondern beispielsweise Textpassagen verwenden, dann wäre diese Eigenschaft nicht gegeben.

Verwandt mit dem One-Time-Pad ist die Stromverschlüsselung, bei welcher der Schlüssel pseudozufällig aus einem kürzeren Schlüssel, einer PIN oder einem Pass- oder Kennwort erzeugt wird. Andere historische und auch aktuelle kryptographische Verfahren verwenden Schlüssel, die in der Regel deutlich kürzer sind als die Länge des zu verschlüsselnden Klartextes. Diese Verfahren weisen – im Gegensatz zum OTP – sämtlich keine perfekte Sicherheit auf.

Das Einmalschlüsselverfahren ist gut geeignet für eine maschinelle Realisierung. Im Gegensatz zu vielen modernen kryptographischen Methoden (wie DES, PGP oder AES), die wegen ihrer Komplexität auf Computer angewiesen sind, eignet sich das One-Time-Pad jedoch ebenso gut zur manuellen Durchführung der Ver- und Entschlüsselung.

Eine einfache Handmethode zur Verschlüsselung ist beispielsweise die buchstabenweise Addition von Klartext und Schlüssel. Hierzu ersetzt man zunächst mithilfe einer beliebigen Substitutionstabelle die Buchstaben des Klartextalphabets durch Zahlen. Im einfachsten Fall ordnet man den 26 Großbuchstaben des lateinischen Alphabets Zahlen zu, die ihrer Position im Alphabet entsprechen. Mit anderen Worten, man nummeriert das Alphabet wie folgt durch:

Jetzt ist eine buchstabenweise Addition leicht möglich. Beispielsweise ergibt die Addition von A und F den Buchstaben G, entsprechend ihren Platznummern 1 + 6 = 7. Falls die Summe den Wert 26 überschreiten sollte, so zieht man einfach 26 ab (Modulo-Operation) und erhält so wieder einen der 26 Alphabetbuchstaben. Beispielsweise X plus U ist numerisch 24 + 21 = 45, nach abziehen von 26 ergibt sich 19 und damit der Buchstabe S, also X + U = S.

Die Zusammenhänge bei der Addition von Buchstaben lassen sich an der folgenden Tabelle, die Ähnlichkeit mit einer klassischen "Tabula recta" hat, übersichtlich darstellen:
In der kursiv gedruckten oberen Zeile und in der ersten Spalte am linken Rand sind die beiden zu addierenden Summanden angegeben. Im Kreuzungspunkt innerhalb der Tabelle lässt sich die Summe ablesen, also das Ergebnis der Summation von Klartextbuchstaben und Schlüsselbuchstaben. Dies ist der entsprechende Buchstabe des Geheimtextes.

Zur Verschlüsselung wird man einen zufälligen Schlüssel benutzen, der in diesem Beispielfall passenderweise ebenfalls aus den 26 Großbuchstaben zusammengesetzt ist und dessen Länge (mindestens) der Länge des zu verschlüsselnden Klartextes entspricht. Entscheidend für die Sicherheit der Verschlüsselung ist, dass die einzelnen Buchstaben des Schlüssels wirklich zufällig verteilt sind, unvorhersagbar sind und in keinerlei Zusammenhang untereinander stehen. Als Beispiel für einen zufälligen Schlüssel dient die folgende Buchstabenfolge:

Der Schlüssel S ist in diesem Beispiel recht kurz, er umfasst nur 21 Buchstaben und ist bei bestimmungsgemäßer Verwendung sehr schnell „verbraucht“, nämlich bereits nach Verschlüsselung eines Textes aus 21 Buchstaben.

Beispielsweise soll der folgende Klartext K verschlüsselt werden:

Zur Verschlüsselung werden Klartext K und Schlüssel S, wie oben erläutert, buchstabenweise addiert. Als „Summe“ (K + S = G) erhält man nach der so durchgeführten Einmalverschlüsselung den Geheimtext G:

Der im Ergebnis erhaltene Geheimtext G ist von einem Zufallstext nicht zu unterscheiden und kann prinzipiell mit keiner noch so gearteten kryptoanalytischen Angriffsmethode (weder jetzt noch in Zukunft) entziffert werden. Allein die Kenntnis des Schlüssels S erlaubt es, aus dem Geheimtext G durch Subtraktion des Schlüssels wieder den Klartext K zu gewinnen. Ohne den Schlüssel kann man prinzipiell alle denkbaren und mehr oder weniger sinnvollen Buchstabenkombinationen aus 21 Buchstaben „konstruieren“. Theoretisch könnte ein Angreifer dies probieren. Er würde so eine Unmenge an Sätzen erhalten, die in beliebigen Sprachen beliebige Informationen verkünden würden, beispielsweise

mit dem dazu „passenden“ Schlüssel, der der Differenz zwischen Geheimtext G und dem konstruierten Pseudo-Klartext K' entspricht (S' = G - K'):

Dieser Schlüssel S' erfüllt die Bedingung, dass die buchstabenweise Summe von ihm mit dem oben erzeugten (falschen) Klartext K' genau den gleichen Geheimtext ergibt wie die Summe aus dem echten, aber dem Angreifer unbekannten Schlüssel S mit dem echten, aber dem Angreifer ebenso unbekannten Klartext K. So kann der Angreifer eine unübersehbare Fülle von denkbaren Klartext-Schlüsselpaaren konstruieren, die in (buchstabenweiser) Summe alle den gleichen echten Geheimtext ergeben. Er hat jedoch keine Möglichkeit, daraus auf den echten Klartext zurückzuschließen. Auch Brute Force, also das erschöpfende Durchprobieren aller möglichen Schlüssel, führt nicht zum Erfolg. Der Angreifer kann zwar so – sogar ohne überhaupt den Geheimtext zu kennen – alle denkbaren Texte mit 21 Buchstaben erzeugen, unter denen natürlich auch der ursprüngliche sein wird. Es gibt jedoch keinerlei Anhaltspunkte, zu entscheiden, welcher aus der Unmenge der durchaus sinnvollen Texte der tatsächliche Klartext ist. Solange ihm der Schlüssel nicht in die Hände fällt, bleibt der Klartext auf ewig ein Geheimnis.

Um diese Sicherheit nicht zu gefährden, sollte der Einmalschlüssel nach bestimmungsgemäßem Gebrauch unwiederbringlich vernichtet werden.

Bei korrekter Anwendung des Einmalschlüsselverfahrens ist OTP, wie der amerikanische Wissenschaftler Claude Shannon in den 1940er-Jahren zeigte, nachweislich sicher und kann nicht gebrochen werden. In der praktischen Anwendung können jedoch Fehler passieren, die einen Einbruch möglich machen. Ursache dafür ist die Verletzung von grundlegenden Sicherheitsmaßnahmen.
Ein möglicher Fehler ist, den Einmalschlüssel nicht geheim zwischen Sender und Empfänger auszutauschen, sodass er durch Dritte ausgespäht werden kann. Auch eine sichere Aufbewahrung des Schlüssels ist essentiell. Im Gegensatz zu Passwörtern oder Kennwörtern lässt sich ein Einmalschlüssel kaum merken. Er muss auf irgendeinem Medium, sei es Papier, Datendiskette, CD-ROM oder USB-Stick gespeichert werden. Solch ein Medium – und mit ihm der Schlüssel – kann leicht kopiert werden. Ebenso darf nicht vergessen werden, den Einmalschlüssel nach Gebrauch sicher und unwiederbringlich zu vernichten. Gelingt es dem Angreifer, den Schlüssel nachträglich zu finden oder zu restaurieren, so ist die Kommunikation nicht mehr geheim.

Ein Kardinalfehler ist, als Einmalschlüssel keine zufällige Buchstabenfolge, sondern eine Textpassage zu benutzen. Selbst wenn der verwendete Text einmalig ist und niemandem (außer den beiden Kommunikationspartnern) bekannt ist, weisen Buchstabenfolgen, die aus einem „sinnvollen“ Text stammen, im Gegensatz zu zufälligen Buchstabenfolgen (Zufallstexten), statistisch auswertbare Abhängigkeiten auf, die eine Entschlüsselung möglich machen können.

Damit aus dem verschlüsselten Text ohne Schlüssel nicht wieder das Original rekonstruiert werden kann, muss der Schlüssel kryptologisch sicher sein, das heißt, er darf von einer echt zufälligen Zahlen- beziehungsweise Buchstabenfolge nicht zu unterscheiden sein. Ist er dies nicht, so kann ein Angreifer unter Umständen durch Kryptoanalyse, auch ohne Kenntnis des Schlüssels, den Text entschlüsseln.

Um einen kryptologisch sicheren Schlüssel zu erzeugen, wird im Idealfall ein physikalischer Zufallszahlengenerator verwendet, da nur solche echt zufällige, das heißt nicht-deterministische, Werte liefern.

Grundsätzlich kann auch ein Generator für Pseudozufallszahlen verwendet werden. Dann darf der Schlüssel jedoch nicht so lang sein, dass man aus ihm auf den internen Zustand des Generators schließen könnte, denn sonst könnten alle darauffolgenden Werte vorhergesagt werden. Das ist bei einfachen Generatoren (wie zum Beispiel Kongruenzgeneratoren) bereits nach wenigen Werten der Fall. Bei kryptographisch sicheren Generatoren können dies erheblich längere Folgen sein, sie sind aber ebenfalls deterministisch und können theoretisch dennoch geknackt werden.

Ein in der praktischen Anwendung (Beispiele: Lorenz-Schlüsselmaschine und Venona) schon häufig gemachter Fehler ist, mehr als nur die beiden, allein für Sender und Empfänger bestimmten Kopien des Einmalschlüssels herzustellen und zu verteilen oder den Schlüssel mehr als einmal zur Verschlüsselung zu verwenden. Schon eine zweimalige Verwendung eines Einmalschlüssels genügt, um die Kommunikation erfolgversprechend angreifen zu können.

Der Angreifer geht dabei von folgender Überlegung aus: Angenommen, der Absender hat für beide verschlüsselten Nachrichten (versehentlich) denselben Schlüssel verwendet, dann kann der Angreifer die Differenz der beiden verschlüsselten Texte analysieren. Klartexte und folglich auch Differenzen von Klartexten zeigen nämlich im Gegensatz zu Zufallstexten eine Reihe von Auffälligkeiten, die statistisch ausgewertet und zur Entzifferung ausgenutzt werden können. So zeigt die Buchstabenhäufigkeit von Differenztexten ebenso charakteristische Auffälligkeiten wie bspw. die von Klartexten oder von monoalphabetisch verschlüsselten Texten.
Die folgende Tabelle zeigt als Ergebnis einer Auszählung der Differenz von zwei deutschen Texten aus jeweils einer Million Buchstaben die relativen Häufigkeiten der Buchstaben in ‰ (Promille). Bei einer Gleichverteilung würde man jeden der 26 Buchstaben mit einer Häufigkeit von 1/26, also mit 38,5 ‰ erwarten.

Bei Differenztexten – falls sie von zwei Klartexten oder von zwei nach dem Einmalschlüssel-Verfahren mit identischem Schlüssel verschlüsselten Geheimtexten stammen – dominiert der Buchstabe Z mit etwa 77 ‰, der durch Koinzidenzen von identischen Buchstaben in beiden Klartexten und damit auch (bei zweifacher Verwendung desselben Einmalschlüssels) in beiden Geheimtexten auftritt. Ein Nebenmaximum tritt beim Buchstaben M mit 50 ‰ auf. Ursache dafür sind Koinzidenzen der in deutschsprachigen Texten häufigen Buchstaben E und R beziehungsweise R und E, die beide die gleiche Differenz, nämlich 13, aufweisen. Falls der Angreifer diese Auffälligkeiten am Differenztext entdeckt, bestätigt dies seinen Verdacht der Mehrfachverwendung eines Einmalschlüssels (siehe auch: Koinzidenzindex). Basierend auf weiteren statistischen Abhängigkeiten und mithilfe von speziellen Differenztextbasen sowie passender Trigramm-, Tetragramm- und Pentagramm-Tabellen (siehe "N-Gramm") und rechnergestützter Untersuchung der verschiedenen Fälle können die Geheimtexte nun erfolgversprechend angegriffen werden.

Auf diese Weise kann das theoretisch unknackbare Einmalschlüssel-Verfahren plötzlich dennoch gebrochen werden, falls bei der praktischen Anwendung Fehler passieren.

Variiert die Länge der verschlüsselten Nachrichten oder die Rate des Nachrichtenaustauschs, so lassen sich dadurch, unabhängig vom Verschlüsselungsverfahren, Rückschlüsse auf den Inhalt ziehen. Ein probates Gegenmittel hierzu sind „Füllsprüche“, die dem Gegner Funkaktivität vortäuschen, auch dort, wo keine Nachrichten ausgetauscht werden.

Das One-Time-Pad benötigt einen Schlüssel, der genauso lang ist wie die Nachricht selbst. Um beispielsweise die gesamten Daten eines Festplattenlaufwerks zu verschlüsseln, ist ein zweites Festplattenlaufwerk (mit mindestens gleicher Größe) zur Speicherung des Schlüssels nötig.

Zur Wahl des Schlüssels ist ein physikalischer Zufallszahlengenerator nötig, der in der Regel in spezieller Hardware realisiert wird. Die Sicherheit des One-Time-Pads hängt von der Güte des Zufallszahlengenerators ab. Eine weniger aufwendige Alternative zum One-Time-Pad ist eine Stromchiffre, die mit einem Pseudozufallsgenerator auskommt, aber dafür keine informationstheoretische Sicherheit besitzt. 

Will man Nachrichten mit dem OTP-Verfahren verschlüsselt übertragen, muss der Schlüssel über einen anderen (sicheren) Kanal übertragen werden als die Nachricht. Beispielsweise kann eine CD mit Schlüsseln durch einen Boten überbracht werden, während die damit verschlüsselten Nachrichten über das Internet übertragen werden.
Aufgrund des hohen logistischen Aufwands konnte sich das One-Time-Pad für die Verschlüsselung in größeren Kommunikationsnetzen nicht durchsetzen.

Das One-Time-Pad ist informationstheoretisch gegen Kryptanalyse sicher und kann nicht entziffert werden, wenn der Schlüssel genauso lang ist wie die Nachricht und aus Zeichen besteht, die zufällig und unabhängig sind, und wenn er nur einmal zur Verschlüsselung verwendet wird. Unter diesen Voraussetzungen kann der Geheimtext nur mit Kenntnis des Schlüssels entschlüsselt werden.

Andere Verschlüsselungsverfahren (wie AES) erreichen ihre Sicherheit durch den immensen Berechnungsaufwand der theoretisch denkbaren Entzifferung, der auf absehbare Zeit praktisch nicht realisierbar ist. Mit anderen Worten, einem potenziellen Angreifer fehlt es an notwendigen Ressourcen (zum Beispiel Rechenleistung oder Zeit), um seinen Entzifferungsversuch erfolgreich durchführen zu können. Die Sicherheit des One-Time-Pad dagegen beruht auf der einmaligen Verwendung des Schlüssels sowie der zufälligen Wahl des verwendeten Schlüssels. Richtig angewendet, bietet es für die zweiseitige geheime Kommunikation unübertreffliche Sicherheit und kann auch mit beliebig hoher Rechenleistung nicht gebrochen werden.





</doc>
<doc id="3738" url="https://de.wikipedia.org/wiki?curid=3738" title="Onomasiologie">
Onomasiologie

Die Onomasiologie (von ‚benennen‘ zu ‚Name‘) oder Bezeichnungslehre ist ein Teilgebiet der Semantik und untersucht, mit welchen sprachlichen Ausdrücken eine bestimmte Sache bezeichnet wird.

Damit geht sie von einem "Gegenstand" aus und fragt nach der Benennung – im Unterschied zur Semasiologie, die von einer "Bezeichnung" ausgehend nach der Bedeutung fragt, z. B. welche unterschiedlichen Gegenstände so benannt werden. Durch das Aufstellen von Wortfeldern werden Bezeichnungen systematisiert und graduelle Bedeutungsänderungen dargestellt.

Nicht in allen, aber doch in vielen Fällen wird die Onomasiologie eher historisch verstanden, das heißt als Lehre vom Bezeichnungswandel.

Jeder Sprecher hat die Möglichkeit, bei der Benennung einer Sache auf eine schon vorhandene Bezeichnung zurückzugreifen oder – manchmal unbewusst – eine neue Bezeichnung zu schaffen. Die Schaffung einer neuen Bezeichnung kann auf unterschiedliche Gründe, Motive und Auslöser zurückgeführt werden: dies können rein sprachlich-kommunikative, aber auch psychische, gesellschaftliche und durch Veränderung in der Welt begründete Aspekte sein. Wer eine neue Bezeichnung schaffen will, hat prinzipiell drei Möglichkeiten:

Obschon onomasiologische Arbeiten bis Jakob Grimm zurückreichen, ist der Beginn der eigentlichen Onomasiologie doch erst verbunden mit den romanistischen Studien von Friedrich Diez (1875), Ernst Tappolet (1895), Adolf Zauner (1902), welcher der Disziplin ihren Namen gab, und Clemente Merlo (1904) sowie der Arbeit des Indogermanisten Berthold Delbrück (1889). Gerade in der Romanistik und in der Germanistik, aber auch in der Indogermanistik sind in der Folge zahlreiche onomasiologische Arbeiten, oftmals Dissertationen, veröffentlicht worden. Als Begründer einer anglistischen Onomasiologie darf der ehemalige Heidelberger Professor Johannes Hoops angesehen werden.

In der ersten Hälfte des 20. Jahrhunderts lassen sich im Wesentlichen drei Strömungen nachzeichnen. Wir können dabei von einer frühen Onomasiologie sprechen, in der die Etymologien von Namen für genau definierbare Konkreta untersucht wurden. 

In einer zweiten Phase wird die Methode „Wörter und Sachen“ bzw. „Sachen und Wörter“ entwickelt, die mit den beiden widerstreitenden Grazer Namensgebern Rudolf Meringer und Hugo Schuchardt verbunden ist. Meringer hat 1909 auch eine gleichnamige Zeitschrift gegründet, die allerdings während der Zeit des Nationalsozialismus unter dem Herausgeber Walther Wüst zu sehr auf der Linie des Regimes war und daher nach dem Zweiten Weltkrieg nicht weiter gedruckt wurde. 

Die dritte Phase ist die Wortfeldforschung, die mit dem Namen Jost Trier (in den Jahren um 1930) verbunden ist, wenngleich Ansätze einer Feldforschung schon bei Michel Bréal (1883) und Ferdinand de Saussure (1916) zu finden sind. Parallel hat sich seit Jules Gilliérons Arbeiten auch die Sprachgeographie immer weiter verfeinert: während im "Atlas linguistique de la France" (ALF) (1901–1910) nur neutrale Termini für die wichtigsten Konzepte verzeichnet sind, so sind im "Sprach- und Sachatlas Italiens und der Südschweiz" (AIS) (1928–1940, von Karl Jaberg und Jakob Jud) mitunter schon Anmerkungen zur besonderen (situativen) Verwendungsweise eines Ausdrucks gegeben. Zwei vorübergehend letzte onomasiologische Höhepunkte erscheinen im Jahr 1949 mit dem indogermanischen historischen Wörterbuch von Carl Darling Buck (1866–1955), an welchem Buck im Bewusstsein aller Probleme über 20 Jahre seines Lebens intensivst arbeitete und das rund 1.500 Konzepte betrachtet, und der jahrelang eher wenig beachteten sprachfamilienübergreifenden Studie von Carlo Tagliavini zu den Bezeichnungen für die Pupille.

Zwar ist auch in der zweiten Hälfte des 20. Jahrhunderts eine Vielzahl an onomasiologischen Abhandlungen entstanden, wie die „Bibliography of Onomasiological Works“ der Zeitschrift "Onomasiology Online" zeigt (ohne dass dabei schon von einer völligen Erfassung aller onomasiologischen Arbeiten ausgegangen werden kann, da viele Artikel in wenig verbreiteten Zeitschriften veröffentlicht worden sind). Theoretische Abhandlungen zur Historischen Onomasiologie sind aber nach dem Zweiten Weltkrieg zumindest in Europa ausgeblieben; lediglich in der amerikanischen Anthropologie sind nennenswerte (meist sprachübergreifende) Arbeiten hervorgebracht worden, besonders verbunden mit den Namen Cecil H. Brown, zum Teil in Kooperation mit Stanley Witkowski und Brent Berlin. Erst um 1990 ist langsam wieder eine theoretische Befassung mit Onomasiologie in der Linguistik im engeren Sinne zu verzeichnen. Aus lexikographischer Sicht ist Henri Vernays Dictionnaire onomasiologique des langues romanes (DOLR) zu erwähnen und das an der Universität Tübingen unter Leitung von Peter Koch entstehende DÉCOLAR (Dictionnaire étymologique et cognitif des langues romanes). 

Von Andreas Blank und Peter Koch wird vor allem eine „kognitive Onomasiologie“ propagiert, d. h. dass neben der Ebene des Konzepts oder des Designats auch die einzelsprachliche Ebene der (strukturierten) Bedeutung berücksichtigt werden müsse und von einer anthropozentrischen Wahrnehmung der Welt ausgegangen werde. Neuere Überlegungen zur theoretischen Onomasiologie stammen aus der Feder des schon erwähnten Pavol Štekauer selbst sowie von Dirk Geeraerts, Peter Koch und Joachim Grzega.

Onomasiologische Arbeitsinstrumente sind Sprachatlanten und Wörterbücher, insbesondere Dialektwörterbücher, etymologische Wörterbücher und historische Wörterbücher, bei denen das historische Wort Zielwort und nicht Ausgangslemma ist. Listen onomasiologischer Quellen des Englischen bietet die „Bibliography of Onomasiological Sources“ der Internetzeitschrift "Onomasiology Online".

Wenn ein Sprecher eine bestimmte konkrete Sache in einer bestimmten konkreten Situation zu benennen hat, so sucht er diese als erstes einem Designat (= einer Kategorie) zuzuordnen. Kann er den Referenten einem Designat oder Signifikat, "signifié" zuordnen, so kann er – unter Berücksichtigung einer kommunikationsbezogenen, sprachökonomischen Kosten-Nutzen-Berechnung – auf ein schon vorhandenes Wort zurückgreifen oder sich mehr oder minder bewusst entscheiden, eine neue Bezeichnung zu bilden. 

Die Bildung einer neuen Bezeichnung kann auf verschiedene, auch gleichzeitig wirkende Faktoren zurückgehen. Der Gesamtkatalog umfasst folgende Faktoren: 
In der bisherigen Forschungsliteratur tauchen auch auf: Abnahme an Salienz, Fehlleistungen beim Lesen, Bequemlichkeit, übermäßige Kürze, schwierige Lautverbindungen, unklare Betonungsmuster, misslungene Bildungen/Kakophonie. Neuere empirische Forschungen (vgl. Joachim Grzega (2004)) bezweifeln jedoch, dass diese Faktoren Bezeichnungswandel auslösen.

Genauer betrachtet laufen Bezeichnungswandel folgendermaßen ab: Bei der beabsichtigten, bewussten Bezeichnungsinnovation muss der Sprecher gegebenenfalls mehrere Ebenen des Wortfindungsprozesses passieren. Dies sind (1) die Analyse der spezifischen Merkmale des Konzeptes, (2) die Auswahl des Benennungsmotivs, (3) die Auswahl der Formen zum Ausdruck des Benennungsmotivs. 
Wird nicht ein schon vorhandenes Wort gekürzt, sondern ein gänzlich neues gebildet, so stehen dem Sprecher verschiedene Formen der Zusammensetzung (einschließlich Blending und Phraseologismen), Rückableitung, Übernahme eines schon vorhandenen Wortes, syntaktische Rekategorisierung, verschiedene Formen der Alternanz, Wortspiel und Wurzelneuschöpfung zur Verfügung (wobei er einem Vorbild seines eigenen Idioms oder dem eines anderen Idioms oder keinem Vorbild folgen kann – letzteres allerdings nur im Falle der Wurzelneuschöpfungen). Wir erhalten somit folgenden Gesamtkatalog formaler Bezeichnungsverfahren (nicht alle Verfahren kommen im Deutschen vor):
Das Verfahren schließt mit Ebene (4), der tatsächlichen Aussprache, ab. 

Um jedoch eine Bezeichnung zu kreieren, die nicht einfach auf der Kürzung eines schon vorhandenen Wortes beruht, müssen erst ein bis zwei physisch und/oder psychisch saliente Bezeichnungsmotive (Ikoneme) erwählt werden. Die Wahl wird dabei von einer oder mehreren potentiellen kognitiv-assoziativen Relationen zwischen dem zu bezeichnenden Konzept und dem ausgewählten Bezeichnungsmotiv respektive -motiven geleitet. Wichtige Phänomene sind dabei: 
Folgende Relationen können also wirksam werden: 

Die konkreten Assoziationen können dabei ohne Vorbild zustande kommen, auf einem eigensprachlichen Vorbild oder auf einem fremdsprachlichen Vorbild beruhen.

chronologisch



</doc>
<doc id="3740" url="https://de.wikipedia.org/wiki?curid=3740" title="Offener Sternhaufen">
Offener Sternhaufen

Als offene Sternhaufen werden Ansammlungen von etwa zwanzig bis zu einigen tausend Sternen bezeichnet, die sich aus derselben Riesen-Molekülwolke (engl. GMC) gebildet haben. Ihre Konzentration im Haufenzentrum ist relativ gering. Dennoch heben sie sich deutlich vom Sternhintergrund ab. Von den dicht gepackten Kugelsternhaufen unterscheiden sie sich durch Größe, Lokalisation, Alter und Entstehung, vor allem aber durch die geringere Sterndichte.

Offene Sternhaufen findet man nur in Spiral- oder irregulären Galaxien, in denen noch Sternbildung stattfindet (wofür z. B. elliptische Galaxien zu alt sind). Die Haufen sind selten älter als ein paar hundert Millionen Jahre, weil sie durch die Eigenbewegung der Sterne, deren innere Vorgänge oder durch gegenseitige Bahnstörungen Mitglieder verlieren. Manchmal werden sie auch durch Zusammenstöße mit anderen Sternhaufen oder Gaswolken zerstört.

Junge offene Sternhaufen können sich immer noch in jener Molekülwolke befinden, aus der sie entstanden sind. Diese wird dadurch aufgehellt, und es entsteht ein ionisiertes H-II-Gebiet. Jedoch führt der Strahlungsdruck der jungen Sterne dazu, dass die Molekülwolke allmählich zerstreut wird. Für gewöhnlich werden 10 % der Masse der Gaswolke für die Sternentstehung benutzt, bevor der Strahlungsdruck den Rest fortbläst.

Für die Untersuchung der Sternentstehung sind offene Sternhaufen sehr wichtige Objekte. Der Grund dafür ist, dass alle Haufensterne ungefähr das gleiche Alter und dieselbe chemische Zusammensetzung haben. So fallen kleine Unterschiede der Eigenschaften viel schneller auf, als wenn man nur isolierte Sterne beobachtet. Auch lässt sich ihre gemeinsame Bewegungsrichtung (Sternstromparallaxe) zur Entfernungsbestimmung nützen.

Die bekanntesten offenen Sternhaufen wie die Plejaden werden seit dem Altertum als Gruppe von Sternen aufgefasst. Andere wurden als Lichtflecken beobachtet, konnten aber erst mit der Erfindung des Teleskops als Sternhaufen identifiziert werden. Nach weiteren Beobachtungen wurden die Sternhaufen in zwei Klassen unterteilt. Die einen bestanden aus tausenden von Sternen in einer regelmäßigen, kugelförmigen Gestalt und sind überall am Himmel zu finden. Die andere Gruppe hatte weniger Sterne, eine unregelmäßigere Form und man findet sie fast ausschließlich in der galaktischen Ebene der Milchstraße. Der ersten Gruppe gab man den Namen Kugelsternhaufen und die zweite bezeichnete man als offene Sternhaufen oder galaktische Haufen.

Es wurde festgestellt, dass die Sterne in einem offenen Sternhaufen ähnliche Eigenschaften haben. Der Geistliche John Michel berechnete 1767 die Wahrscheinlichkeit, dass eine Sternengruppe wie die Plejaden lediglich eine zufällige Anordnung am Sternenhimmel sei, auf 1 zu 496.000. Als die Astrometrie genauer wurde, fand man heraus, dass sich die Sterne im Haufen mit der gleichen Eigenbewegung durch den Nachthimmel bewegen. Durch spektroskopische Beobachtungen ermittelte man auch die gleiche Radialgeschwindigkeit. Daraus wurde geschlussfolgert, dass die Sterne zur selben Zeit entstanden sind und als Gruppe miteinander verbunden sind.

Obwohl Kugelsternhaufen und offene Sternhaufen klar voneinander getrennte Gruppen bilden, können die Unterschiede zwischen spärlichen Kugelsternhaufen und sehr reichen offenen Sternhaufen gering sein. Einige Astronomen glauben, dass beiden Typen von Sternhaufen die gleichen Mechanismen zu Grunde liegen mit dem Unterschied, dass die Ursachen, die zur Bildung von großen Kugelsternhaufen führen, in unserer Galaxie nicht mehr gegeben sind.

Alle Sterne entstehen aus Mehrfachsternensystemen, denn nur eine Gaswolke mit einer vielfachen Sonnenmasse ist schwer genug, um unter ihrer eigenen Schwerkraft zu kollabieren, jedoch kann so eine schwere Wolke nicht zu einem einzelnen Stern kollabieren.

Die Entstehung eines offenen Sternhaufens beginnt mit dem Kollaps eines Teils einer Riesenmolekülwolke, eine Gaswolke mit dem Gewicht von mehreren tausend Sonnenmassen. Viele Faktoren können der Auslöser dafür sein. Sobald die Riesenmolekülwolke anfängt zu kollabieren, beginnt die Sternentstehung durch die Bildung immer kleinerer Fragmente, aus denen am Ende vielleicht mehrere tausend Sterne werden. In unserer Galaxie bilden sich offene Sternhaufen alle paar tausend Jahre.

Sobald die ersten Sterne entstanden sind, stoßen die größten und heißesten Sterne eine enorme Menge ultravioletter Strahlung aus. Diese Strahlung ionisiert das umliegende Gas der Riesenmolekülwolke, wodurch sich ein H-II-Gebiet bildet. Sternenwinde der schweren Sterne und der Strahlungsdruck verdrängen das umliegende Gas. Nach ein paar Millionen Jahren kommt es zur ersten Supernova eines Sternes, wodurch weiteres Gas aus dem System hinausgeschleudert wird. Nach einigen Zehnmillionen Jahren ist nur noch so viel Gas übrig geblieben, dass es nicht mehr zu einer Sternentstehung kommen kann. Meistens werden vom anfänglich vorhandenen Gas nur 10 % zur Sternenbildung genutzt. Der Rest wird weggeblasen.

In der Regel bilden sich aus einer Molekülwolke zwei oder mehrere offene Sternhaufen. In der großen Magellanschen Wolke sind sowohl Hodge 301 als auch R136 aus Gasen des Tarantelnebels hervorgegangen. Ein Beispiel aus unserer Galaxie wären Hyaden und Praesepe. Durch Zurückverfolgung ihrer Bewegung nimmt man an, dass sie sich aus derselben Wolke vor 600 Millionen Jahren gebildet haben.

Manchmal formen sich zwei Sternhaufen, die in der gleichen Zeit entstanden sind und bilden sogenannte Doppelsternhaufen. Das bekannteste Beispiel in der Milchstraße ist der Doppelsternhaufen h Persei und Chi Persei, man kennt jedoch noch zehn weitere. Man hat viele in der kleinen und großen Magellanschen Wolke gefunden. Sie sind in anderen Galaxien einfacher aufzuspüren, da Projektionseffekte in der Milchstraße dazu führen können, dass nicht zusammengehörige Sterne so wirken, als würden sie sich dicht nebeneinander befinden.

Die Anzahl der Sterne in einem offenen Sternhaufen variiert zwischen ein paar zehn Sternen bis hin zu großen Ansammlungen von einigen tausend Sternen. Sie enthalten meist einen dichteren Kern, der von einer weitläufigen Korona aus weiteren Sternen umgeben ist. Der Kern hat meist einen Durchmesser von 3 bis 4 Lichtjahren, während sich die Korona bis in eine Entfernung von ungefähr 20 Lichtjahren vom Zentrum erstreckt. Im Kern befinden sich rund 1,5 Sterne pro Kubiklichtjahr (die Sternendichte in dem Gebiet um unsere Sonne beträgt ca. 0,0035 Sterne pro Kubiklichtjahr).

Offene Sternhaufen werden meist nach einem von Robert Trumpler entwickelten Schema von 1930 klassifiziert. Dazu sind drei Angaben nötig. Die römischen Zahlen von I bis IV geben die Konzentration und Loslösung vom umliegenden Sternenfeld an (von stark bis schwach konzentriert). Die arabischen Ziffern von 1 bis 3 geben an, wie stark sich die einzelnen Sterne in ihren Helligkeiten unterscheiden (von gering zu stark). Die Buchstaben p, m, oder r geben an, ob das Kluster wenig (poor), durchschnittlich (medium) oder viele (rich) Sterne hat. Ergänzend (optional) können noch drei weitere Kriterien angegeben werden: n (nebulosity – nebelig) = Im Haufen sind interstellare, leuchtende Materiewolken eingebettet; e (elongated – länglich) = Der Sternhaufen erscheint in einer Richtung auseinandergezogen; u (unsymmetrical – unsymmetrisch) = Die Sterne im Haufen sind in verschiedenen Richtungen gestreut. Nach diesem Schema sind die Plejaden beispielsweise als I3rn klassifiziert (stark konzentriert mit reicher Population mit Materiewolken), die Hyaden sind klassifiziert als II3m (mehr zerstreut und weniger Sterne).

Es sind über 1.000 offene Sternhaufen in unserer Galaxie bekannt, aber die wirkliche Anzahl dürfte bis zu zehn Mal höher sein. In Spiralgalaxien findet man sie fast ausschließlich in den Spiralarmen. Der Grund ist, dass hier wegen der höheren Gasdichte die meisten Sterne entstehen und die Sternhaufen wieder vergehen, bevor sie jenseits der Spiralarme gelangen können. Sie sind in unserer Galaxie in der galaktischen Ebene konzentriert mit einer Ausdehnung der Höhe von rund 180 Lichtjahren (verglichen mit dem Radius der Milchstraße von rund 100.000 Lichtjahren)

In Irregulären Galaxien kann man offene Sternhaufen überall in der Galaxie finden. Ihre Konzentration ist dort am größten, wo auch die Gaskonzentration am höchsten ist. Man findet sie jedoch nicht in elliptischen Galaxien, da hier der Sternentstehungsprozess vor vielen Jahren aufgehört hat, so dass sich alle offenen Sternhaufen bereits aufgelöst haben.

In unserer Galaxie hängt die Verteilung vom Alter ab. Ältere Sternhaufen werden meist in größeren Entfernungen vom galaktischen Zentrum gefunden. Die Gezeitenkräfte sind in der Nähe des Zentrums unserer Galaxie stärker, so dass die Sternhaufen viel leichter zerstört werden. Weiterhin sind die Riesenmolekülwolken, die die offenen Sternhaufen ebenfalls zerstören können, eher in den inneren Regionen der Galaxie konzentriert. Also vergehen die meisten Sternhaufen in den inneren Regionen der Galaxie viel früher als die in den äußeren Regionen.

Weil sich offene Sternhaufen zerstreuen, bevor die meisten ihrer Sterne sterben, kommt das meiste Licht von jungen, heißen blauen Sternen. Diese Sterne sind die schwersten und haben die kürzeste Lebenserwartung von ein paar zehn Millionen Jahren. Ältere offene Sternhaufen haben dagegen mehr gelbe Sterne.

Einige offene Sternhaufen enthalten heiße blaue Sterne, die jünger zu sein scheinen als ihre restlichen Sterne. Diese blauen Nachzügler werden auch in Kugelsternhaufen beobachtet. Es wird angenommen, dass sie entstehen, wenn Sterne kollidieren und verschmelzen, und dabei einen wesentlich heißeren und schwereren Stern bilden. Auf jeden Fall ist die Sternendichte viel geringer als in Kugelsternhaufen, so dass Sternenkollisionen nicht die Anzahl an Nachzüglern erklären kann. Es wird eher angenommen, dass die meisten ihren Ursprung in einem Doppelsternsystem haben. Wechselwirkungen des Doppelsternsystems mit anderen Sternen führen dann zur Verschmelzung beider Sterne zu einem Stern.

Sobald ein Stern seinen Wasserstoffvorrat aufgebraucht hat und damit die Kernfusion nicht mehr stattfinden kann, stößt er seine äußeren Schichten ab und bildet einen Planetarischen Nebel mit einem Weißen Zwerg im Inneren. Die meisten offenen Sternhaufen werden jedoch zerstreut, bevor viele ihrer Sterne das Stadium eines weißen Zwerges erreichen. Jedoch ist die Anzahl weißer Zwerge in offenen Sternhaufen nochmals wesentlich geringer als erwartet. Eine mögliche Erklärung ist die folgende: Wenn ein Roter Riese seine äußeren Schichten abstößt und einen planetarischen Nebel bildet, reicht eine kleine Asymmetrie des abgestoßenen Materials aus, um dem übrig gebliebenen Stern einen Stoß von ein paar Kilometern pro Sekunde zu geben. Dieser ist stark genug, um ihn aus dem Haufen entkommen zu lassen.

Die Zeitspanne, die ein Sternhaufen Bestand hat, hängt hauptsächlich von seiner Anfangsmasse ab. Viele offene Sternhaufen sind seit ihrer Entstehung instabil. Ihre Gesamtmasse ist so gering, dass die Fluchtgeschwindigkeit aus diesem System geringer ist als die durchschnittliche Geschwindigkeit ihrer Sterne. Diese Sternhaufen lösen sich innerhalb von ein paar Millionen Jahren auf. Da das umliegende Gas durch den Strahlungsdruck der jungen heißen Sterne weggeblasen wird, reduziert sich die Masse, so dass eine schnelle Zerstreuung möglich ist.

Sternhaufen mit einer ausreichend großen Masse, um die Sterne durch die Gravitation dauerhaft zu binden, können mehrere zehn Millionen Jahre existieren, jedoch führen auch hier interne und externe Prozesse dazu, dass sie allmählich zerstreut werden. Kommen sich im Inneren Sterne zu nah, führt das oft dazu, dass die Geschwindigkeit des einen Sterns stark erhöht wird, die Fluchtgeschwindigkeit des Sternhaufens überschreitet und er ihm dadurch entkommen kann. Das führt zur langsamen Auflösung des Sternhaufens. Die Zeitspanne bis zum Verlust der Hälfte der Sterne reicht von 150 bis 800 Millionen Jahre, je nach Anfangsdichte.

Im Schnitt wird alle halbe Million Jahre ein offener Sternhaufen durch einen äußeren Faktor, wie zum Beispiel der Zusammenstoß mit einer Molekülwolke, zerstört. Die durch die Gravitation hervorgerufenen Gezeitenkräfte führen dann zur Zerstörung der Struktur des Haufens. Schließlich wird aus dem Sternhaufen ein Band aus Sternen, die zwar nicht eng genug zusammen liegen, um als Haufen bezeichnet zu werden, aber alle miteinander verbunden sind und sich in die gleiche Richtung bewegen.

Nachdem die Gravitation so schwach geworden ist, dass sie nicht mehr ausreicht, um die Sterne zu binden, bewegen sich die meisten der Sterne immer noch in die gleiche Richtung. So eine Sternassoziation wird dann auch Bewegungshaufen oder Bewegungssternhaufen genannt. Viele der hellsten Sterne in dem 'Pflug' von Ursa Major waren früher ein offener Sternhaufen, die nun eine lose Verbindung, die Ursa-Major-Gruppe darstellen.

Wenn man die Sterne eines offenen Sternhaufens im Hertzsprung-Russell-Diagramm einträgt, dann liegen sie meist auf der Hauptreihe. Die schwersten Sterne liegen etwas abseits der Hauptreihe und werden Rote Riesen. Die Position dieser Sterne kann benutzt werden, um das Alter des Sternhaufens zu bestimmen.

Da alle Sterne in einem offenen Sternhaufen ungefähr die gleiche Entfernung zur Erde haben und ungefähr zur gleichen Zeit aus dem gleichen Rohmaterial entstanden sind, hängen die Helligkeitsdifferenzen nur von den unterschiedlichen Massen der Sterne ab. Dadurch sind offene Sternhaufen sehr nützlich, wenn man die Sternentwicklung untersuchen will. Denn wenn man zwei Sterne eines Sternhaufens vergleichen will, fallen die meisten Parameter raus.

Die Untersuchung von Lithium- und Berylliumvorkommen in offenen Sternhaufen sind wichtige Anhaltspunkte für die Evolution der Sterne und ihrer inneren Strukturen. Während Wasserstoffkerne unter einer Temperatur von 10 Millionen K nicht zu Helium fusionieren können, werden Lithium und Beryllium bereits bei einer Temperatur von 2,5 Millionen K und 3,5 Millionen K zerstört. Das bedeutet, dass ihr Vorkommen stark davon abhängt, was im Sterneninneren geschieht. Aus den Daten kann man auf das Alter und die chemische Zusammensetzung schließen.

Um ein astronomisches Objekt zu verstehen, ist es zwingend erforderlich, dessen Entfernung zu kennen. Die näher gelegenen Sternhaufen können mit zwei verschiedenen direkten Methoden gemessen werden. Zum einen kann man die Parallaxe bestimmen, also die scheinbare Verschiebung des Objekts gegenüber sehr weit entfernten Objekten, die eigentlich aus der Bewegung der Erde um die Sonne resultiert. Die zweite Methode ist die so genannte Bewegungssternhaufenmethode (Sternstromparallaxe, siehe Parallaxe). Ihr liegt die Tatsache zu Grunde, dass sich die Sterne in einem Sternhaufen zusammen auf einen gemeinsamen Fluchtpunkt (Vertex) zubewegen. Man bestimmt nun aus den Sternspektren mit Hilfe von Dopplereffektmessungen die Radialgeschwindigkeit. Sobald man die Radialgeschwindigkeiten, die Eigenbewegung und den beobachteten Winkel vom Sternhaufen zum Fluchtpunkt kennt, kann man mit einfacher Trigonometrie die Entfernung berechnen. Die Hyaden sind das bekannteste Beispiel, bei der diese Methode angewendet wurde. Ihre Entfernung beträgt 46,3 Parsec.

Sobald die Entfernung von nahe liegenden Sternhaufen bekannt ist, können für größere Entfernungen Techniken benutzt werden, die auf die gewonnenen Daten bei nahen Sternhaufen aufbauen. Von den nahen Sternhaufen weiß man, dass sich ihre Sterne bei einer bekannten Entfernung in der Hauptreihe des Hertzsprung-Russell-Diagramms einordnen, und so kann man leicht die Entfernung von Sternhaufen bestimmen, die sich viel weiter von der Erde weg befinden.

Der der Erde am nächsten gelegene offene Sternhaufen sind die Hyaden. Sie sind jedoch eher ein Bewegungssternhaufen als ein offener Sternhaufen. Der am weitesten entfernte offene Sternhaufen in der Milchstraße ist der Berkeley 29 mit einer Entfernung von rund 15,000 Parsec. Offene Sternhaufen findet man in vielen Galaxien der Lokalen Gruppe.

Die genaue Entfernung von offenen Sternhaufen ist wichtig, um die Perioden-Leuchtkraft-Beziehung bestimmter Größen veränderlicher Sterne (Cepheiden und RR-Lyrae-Sterne) zu eichen. Diese Sterne sind sehr hell und können noch in sehr großer Entfernung ausgemacht werden. Sie werden deshalb als Standardkerze verwendet, um die Entfernung zu nahen Galaxien in der Lokalen Gruppe zu berechnen.




</doc>
<doc id="3741" url="https://de.wikipedia.org/wiki?curid=3741" title="Oboe">
Oboe

Die Oboe // (veraltet auch "Hoboe"; beides Eindeutschungen von frz. "hautbois") ist ein Holzblasinstrument mit Doppelrohrblatt. Sie hat ihren Ursprung in der französischen Barockmusik des 17. Jahrhunderts und stellt eine Fortentwicklung der mittelalterlichen Schalmei dar.

"Hautbois", der französische Name des Instruments, bedeutet so viel wie „hoch klingendes Holzinstrument“ (Kompositum aus "haut", „hoch“ und "bois", „Holz“) und ist als Bezeichnung für eine Art Schalmei bereits im 15. Jahrhundert belegt. Im deutschen Schrifttum der Barockzeit erscheint das Wort (nun als Bezeichnung der Barockoboe) zunächst in unveränderter Schreibung, also als Fremdwort (erstmals 1619 bei Michael Praetorius), ab 1750 dann vermehrt in der eingedeutschten Form "Hoboe". Diese wurde ihrerseits im Lauf des 19. Jahrhunderts vom heute allgemein gebräuchlichen Namen "Oboe" verdrängt, der sich wohl durch den Einfluss des Italienischen erklärt (vgl. it. "oboe", veraltet "oboè", gleichfalls um 1700 aus dem Frz. entlehnt).

Das etwa 65 Zentimeter lange Instrument hat eine konische Bohrung und überbläst in die Oktave. Der Korpus der Oboe ist dreiteilig und setzt sich aus "Oberstück", "Mittelstück" und "Becher" (oder "Fußstück") zusammen. Oberstück und Mittelstück haben an ihrem unteren Ende einen korkummantelten Zapfen, der in eine entsprechende Metallhülse am oberen Ende von Mittelstück bzw. Becher gesteckt wird. Zuletzt wird das Mundstück, von Oboisten meist schlicht "Rohr" genannt, oben in das Oberstück gesteckt. Sowohl für Korpus als auch für die Rohre gibt es eigene Etuis, in denen sie aufbewahrt und transportiert werden.

Oboen werden aus Grenadill-, Buchsbaum- oder Ebenholz gebaut, seltener sind Instrumente aus Rosenholz, Palisander, Cocobolo oder anderen exotischen Hartholzarten. Am oberen Ende des Mittelstückes ist auf der Rückseite der Daumenhalter angebracht, mit dessen Hilfe das Instrument gehalten wird.
Aufgrund der Klappenmechanik, die im Laufe ihrer Geschichte (um den steigenden Ansprüchen an Klang und Intonation gerecht zu werden) mit zunehmender Kompliziertheit dazu führte, dass auf immer engerem Raum – speziell am Oberstück – immer mehr Bohrungen und Metalleinsätze angebracht wurden, war das Holz deshalb immer größeren Belastungen ausgesetzt. Dies führte dazu, dass nach und nach auf immer härtere Holzarten zurückgegriffen wurde, die dieser Belastung standhalten können. Inzwischen gibt es auch recht erfolgreiche Versuche mit Kunststoff bzw. mit Kompositmaterialien (Holzabfälle und kohlenstofffaserverstärkter Kunststoff). Auch Oboen aus transparentem Acrylglas werden hergestellt. Die Ebonit- und Acrylglasoboen sind besonders gefragt für den Einsatz unter extremen Klimabedingungen, da dort das Holz leicht Gefahr läuft zu reißen.

Die Oboe gilt als eines der im Aufbau kompliziertesten Blasinstrumente. Die Klappen und Böcke werden aus Neusilber oder ähnlichen leichtschwingenden Materialien geschmiedet und anschließend mit diversen Silber- und/oder Goldlegierungen überzogen. Die Anzahl der Klappen variiert von Modell zu Modell. Die Tonlöcher moderner Oboen werden durch Klappen verschlossen. Jede Klappe ist mit einem „Klappenpolster“ versehen, das das Tonloch abdeckt. Diese Polster bestehen entweder aus Fischhaut mit einer Füllung darin oder aus Kork und müssen vom Instrumentenbauer exakt eingepasst werden, damit sie luftdicht schließen. Auf der Unterseite jeder Klappe ist eine Stahlfeder eingehakt, die dafür sorgt, dass die Klappe von allein in die richtige Position zurückkehrt, sobald man die Klappe loslässt.
Die Klappen werden entweder direkt mit den Fingern oder mittels ausgeklügelter Hebelmechanik bedient. Direkt mit den Fingern verschlossene Klappen weisen dabei oft Löcher auf, die ein teilweises Verschließen des Tonloches erlauben. Bei der obersten dieser Klappen ist dies über eine spezielle Form der Klappe vorgesehen, dieses "Halbloch" wird bei einigen Tönen zur Oktavierung eingesetzt. Andere solcher Löcher werden mechanisch bei Druck anderer Klappen teilweise verschlossen.
Ringklappenoboen verfügen über Ringklappen, welche durch teilweises Verschließen ihrer großen Löcher mit dem Finger ein einfacheres Spiel von Glissandi und Mikrotönen erlauben (ohne Ringklappen lassen sie sich über den Ansatz, Mikrotöne auch über spezielle Griffe erreichen). Es existieren auch Oboen mit nur einzelnen Ringklappen.
Die Klappen- und Hebelmechanik ist recht kompliziert; es existiert eine Vielzahl an Querverbindungen zwischen den einzelnen Klappen, welche mithilfe kleiner Schrauben justiert und eingestellt werden.

Die französische Oboe

Es gibt sogenannte "voll"- und "halbautomatische" Oboen. Bei einer halbautomatischen ist für die erste und zweite Oktavklappe je ein Hebel zum Öffnen der Klappe vorhanden. Bei einer vollautomatischen Oboe existiert für beide Oktavklappen nur ein Hebel, der Wechsel geschieht hier zwischen den Tönen <nowiki>gis"</nowiki> und <nowiki>a"</nowiki> automatisch. Die vollautomatische Mechanik ist in Deutschland, Polen und in den Niederlanden besonders verbreitet, die halbautomatische in den USA und Frankreich.
Beide Bauweisen haben ihre Vor- und Nachteile. Mit halbautomatischen Oboen lassen sich vor allem im oberen Tonbereich von <nowiki>c</nowiki> aufwärts mehr alternative Griffe für die einzelnen Töne finden, die mehr verschiedene Klangfarben und ein differenzierteres Spiel ermöglichen. Auch bei halbautomatischen Oboen lassen sich die beiden Oktavklappen jedoch nicht unabhängig voneinander bedienen, der Hebel für die zweite Oktavklappe verschließt automatisch die erste. Die vollautomatische Oboe ist einfacher zu bedienen, da eine Klappe wegfällt, ist dafür aber reparaturanfälliger und kann bei modernen experimentellen Werken mitunter nicht oder nur erschwert eingesetzt werden. Unabhängig davon ist eine dritte Oktavklappe verbreitet, die sich durch einen weiteren Hebel bedienen lässt.

Je nach Modell existieren verschiedene Trillerklappen, welche eingesetzt werden, wenn in schnellen Tonverbindungen, insbesondere bei Trillern, ein hinreichend schneller Wechsel zwischen den zwei Griffen nicht möglich ist, etwa für die Verbindungen <nowiki>c"</nowiki>-<nowiki>d"</nowiki>, <nowiki>c"</nowiki>-<nowiki>cis"</nowiki> und <nowiki>as'</nowiki>-<nowiki>b'</nowiki>. Klang und Höhe unterscheiden sich jedoch teils beträchtlich gegenüber den Standardgriffen. Davon unabhängig existieren für viele Töne alternative Griffweisen, die teilweise durch die Mechanik zum Verschluss derselben Klappen führen, andernfalls aber ebenfalls klangliche Unterschiede implizieren.
Die Wiener Oboe

Neben der auf der ganzen Welt verbreiteten Bauform der "französischen Oboe" existiert auch die "Wiener Oboe", die fast ausschließlich in Wien gespielt wird, beispielsweise im Orchester der Wiener Philharmoniker. Sie ist etwas anders mensuriert, hat in der Tiefe einen etwas weicheren, in der oberen Lage engeren und spitzeren, obertonreicheren Klang. Sie reicht in der Tiefe in der Standardform bis zum kleinen h, mit einem besonderen Fußstück ist jedoch auch das kleine b spielbar. Die Wiener Oboe ist dem Barock-Instrument und der klassischen Oboe baulich, klanglich und in der Spieltechnik ähnlicher als die französische Oboe, da diese durch Innovationen französischer Instrumentenbauer wie Henri Brod oder Guillaume Triébert stärker verändert wurde. So verschwanden in französischen Modellen die Holzpflöcke der Klappenlager zugunsten solcher aus Metall und es wurden viele Klappen zur Erweiterung des Tonumfangs und alternativer Griffkombinationen hinzugefügt. Die Wiener Oboe wurde weniger stark verändert, das Oktavieren ist jedoch durch eine Oktavklappe wesentlich erleichtert worden. Die Klangfarbe der Wiener Oboe ändert sich zwischen piano und forte weniger stark. Die Wiener Schule der Oboenausbildung unterscheidet sich auch im Interpretationsstil (weniger Vibrato-Einsatz, deutlichere Phrasierung, kürzere Noten, weniger sanglich).

Die Tonerzeugung geschieht mit einem Doppelrohrblatt, das zwischen die nach innen gewölbten Lippen genommen und durch das mit hohem Druck hindurchgeblasen wird. Im Korpus der Oboe wird der Ton nach dem Prinzip der stehenden Welle in einem Instrumentenrohr erzeugt. Es bildet sich eine schwingende Luftsäule. Mit dem Öffnen und Schließen der Klappen wird die Länge der schwingenden Luftsäule und somit deren Wellenlänge verändert: der Ton wird höher oder tiefer. Da die Oboe – schematisch dargestellt – ein (konisch gebohrtes) beidseitig geöffnetes Rohr ist (Entsprechungen lassen sich auch bei Orgelpfeifen finden), bildet sich für das tiefe Register (z. B. Grundton c) als Luftsäule eine halbe Welle aus, in der alle geraden und ungeraden Obertöne enthalten sind. Beim (ersten) Überblasen bildet sich in der Mitte ein zusätzlicher Druckknoten aus und die Oboe überbläst in die Oktave (doppelte Frequenz). (Beispiel: üblicher Kammerton a = 443 Hz und das um eine Oktave darüber liegende a = 886 Hz).
Die physikalischen Eigenschaften der Oboe sind äußerst kompliziert und gegenwärtig noch nicht vollständig geklärt, da eine Vielzahl an Faktoren die Töne bzw. deren Qualität beeinflussen. Viele Klappen sind mit Hilfe von Stellschräubchen verstellbar und stehen zum Teil mit der Klangqualität und/oder Intonation anderer Töne im engen Zusammenhang. Beispielsweise kann die geringste Abweichung durch unsachgemäßes Einstellen der Klappe des c' dazu führen, dass Töne im höchsten Register zu rauschen beginnen oder sogar unspielbar werden. Neuerungen im Instrumentenbau zur Stabilisierung der Intonation oder komfortableren Spielbarkeit und Ansprache (was bedeutet, wie leicht der jeweilige Ton in Schwingung zu bringen ist) beruhen bis heute immer noch auf Versuchen, die je nach Ergebnis weiterentwickelt oder eben wieder verworfen werden.

In der Barockzeit hatte die Oboe einen Tonumfang von zwei Oktaven chromatischer Intervalle, vom <nowiki>c'</nowiki> bis zum <nowiki>c</nowiki>. Durch die noch fehlende Oktavklappe war eine besondere Überblastechnik für die zweite Oktave und von der unteren Oktave differierende Griffe notwendig, um eine korrekte Intonation zu erhalten.
Der Tonumfang der modernen Oboe beginnt meistens beim kleinen b, je nach Modell auch beim kleinen a oder beim kleinen h. Ab dem <nowiki>e</nowiki> variieren die verwendeten Griffe recht stark, Angaben über die übliche Obergrenze schwanken zwischen <nowiki>f</nowiki> und <nowiki>b</nowiki>, höhere Töne sind jedoch möglich. Mit einer speziellen Ansatztechnik, der sogenannten Beißtechnik, bei welcher der Oboist die oberen und unteren Zähne auf die Grundlinie der Schabung des Mundstücks auflegt und somit einen viel kürzeren Teil des Rohres zum Schwingen bringt, sind noch höhere Töne, eventuell sogar bis zum <nowiki>a'</nowiki> spielbar, wie sie manchmal in zeitgenössischen Kompositionen gefordert werden.

Der Klang der Oboe ist ausdrucksstark und klingt je nach Bläserschule und regionaler Tradition von nasal-hell bis dunkel-samtig. Vom äußerst weichen Klangcharakter der Barockoboe entwickelte sich der Ton immer weiter zu dem genaueren Ton der modernen Oboe, die ein differenzierteres Spiel zulässt, da sie über mehr dynamische Möglichkeiten verfügt (besonders im leisen Bereich) und auch schnelles Staccato vereinfacht. Die Spielweise und somit der Klang der Oboe ist zwischen den einzelnen Schulen sehr unterschiedlich; so wird von manchen Oboisten wie zum Beispiel Albrecht Mayer oder François Leleux ein sehr samtig-weicher Ton gepflegt, während andere Oboisten wie zum Beispiel Heinz Holliger, Pierre Pierlot oder Burkhard Glaetzner die Oboe eher heller und nasaler spielen. Dabei ist die frühere, eher national begrenzte Aufteilung in einen voluminös-runden „deutschen“ Klang und einen engeren, dafür flexibleren „französischen“ Klang in den Hintergrund getreten.

Weil der Oboenton sehr ausgeprägte Obertöne hat (speziell den 3., 4. und 5.), ist sein Klang besonders deutlich hörbar. Daher hat es sich seit dem 19. Jahrhundert eingebürgert, dass einer der Oboisten vor Proben und Aufführungen den anderen Musikern den Ton a' zum Einstimmen angibt. Heute verwenden Oboisten zur genauen Kontrolle der Frequenz gerne ein elektronisches Stimmgerät. Die Oboe ist in Deutschland und Österreich in der Regel auf eine Stimmtonhöhe von a' = 442 bis 444 Hz gestimmt, die Wiener Oboe von 443 Hz bis 446 Hz. In anderen Ländern sind auch andere Stimmhöhen zwischen 440 Hz und 444 Hz üblich (siehe auch Kammerton).

Das Mundstück der Oboe, kurz „Rohr“ genannt, wird vom Oboisten aus den Internodien des Pfahlrohrs "(Arundo donax)" gefertigt. Das Holz stammt aus der Region um Avignon (Südfrankreich) oder aus Kalifornien, wo es auf eigens für diesen Zweck betriebenen Plantagen angebaut wird. Die französischen Lagen bei Frejus und Avignon haben besondere klimatische Bedingungen, die sich nirgendwo anders finden. Zum Beispiel scheinen die warme, trockene Luft der Sahara, die durch Südfrankreich fegt, sowie der Mistral-Wind dafür mitverantwortlich zu sein. Daher sind viele Versuche, das Holz anderswo anzubauen, gescheitert. Oboenrohre sind empfindlich gegenüber mechanischen Einwirkungen. Vor Gebrauch weicht der Oboist das Mundstück in Wasser ein, damit es biegsam und damit spielbar wird.

Die Klangqualität und Ansprache des Oboentons und damit das spielerische Niveau des Oboisten hängen in starker Weise von der Qualität des verwendeten Rohrholzes sowie der sorgfältigen Fertigung des Oboenrohrs ab. Oboisten verwenden daher viel Zeit und Sorgfalt auf den Bau ihrer eigens auf ihre persönliche Konstitution zugeschnittenen Rohre.

Auch die Leichtigkeit, mit der das Instrument spielbar ist, hängt weitgehend vom Mundstück ab. Da das Oboenspiel durch den immer aufrechtzuhaltenden Lippendruck sehr anstrengend ist, können je nach Bedarf verschieden leichte Mundstücke angefertigt werden; sehr leicht spielbare Mundstücke haben jedoch, da sie sehr dünn sind, einen scharfen und nasalen Klang – das Bauen von Mundstücken ist also eine Gratwanderung zwischen Klangfülle und Spielbarkeit.
Das Mundstück besteht aus einer Hülse (ein am unteren Ende korkummanteltes konisches Metallröhrchen) und dem Holz, das auf diese Hülse aufgebunden wird.
Es gibt verschiedene Schulen und dementsprechend Bauweisen zur Herstellung von Oboenmundstücken:

Die deutsche und europäische Bauweise:
Das Mundstück wird bei Bedarf mit Goldschlägerhaut („Fischhaut“), Teflonband oder Bienenwachs abgedichtet, und es wird bei Bedarf eine Drahtzwinge zur Stabilisierung um das Rohr gedreht. Der Oboist schabt mit einem Schabemesser unter Zuhilfenahme einer Schabezunge den oberen Teil des Holzes, um den von ihm gewünschten Klang zu erhalten. Den geschabten Teil nennt man „Bahn“, und die obersten (und dünnsten) Millimeter des Mundstücks nennt man „Ansprache“. Die Länge der Schabung variiert zwischen 9 mm (Deutschland) bis 14 mm (Niederlande).

Eine amerikanische Variante der Bauweise von Oboenmundstücken wurde durch Marcel Tabuteau, John De Lancie und seine Schüler entwickelt. Es wird auf Draht dringend verzichtet. Ein gut geformtes Rohr braucht noch keine Abdichtung, da diese durch Versatz von selbst abdichten, anderenfalls geschieht dies mittels Paraffin, Bienenwachs, Zigarettenpapier oder auch Goldschlägerhaut.

Wichtiger Unterschied ist die aufwendige Form des abgeschabten Blattes. Es wird ein „Herz“ mit einem „Halbmond“ nach vorne geformt. Hinter dem „Herz“ befindet sich in der Mitte die „Wirbelsäule“, die gerade unter der Schale liegt und zu beiden Seiten die „Lungen“ für die Basstöne. Die Lungen werden manchmal asymmetrisch angelegt. Am Rand werden die „Rippen“ aus vollständiger Schale überlassen und abklingend geformt. Die Klangschönheit ist eine Kombination der Qualität des Holzes und vor allem durch das Herz bestimmt; die Intonation in mittlerer und besonders höherer Lage wird bestimmt durch die absolute und relative Länge der Spitze in Verhältnis zu anderen Bereichen, während die Stützkraft und tiefe Lage besonders den Lungen und einem Versatz in der Schabung zugeordnet werden können. Diese Rohre sind auf Lorée- und andere französische Oboen abgestimmt.

Bei der Atemtechnik nimmt die Oboe unter allen Blasinstrumenten eine Sonderstellung ein. Mit keinem anderen Blasinstrument lassen sich mit einem einzigen Atemzug dermaßen lange Soli spielen wie mit der Oboe. Der Grund liegt in der Beschaffenheit des Mundstückes. Um das kleine Doppelrohrblatt zum Schwingen zu bringen, benötigt es großen Druck. Gleichzeitig ist die Distanz der beiden gegeneinander schlagenden Blätter winzig, sie liegen nur etwa einen Millimeter auseinander, deshalb verbraucht man kaum Luft und braucht eine präzise Atemtechnik. Die Lungen werden beim Spiel kaum geleert, sodass das Volumen beim Spiel meist durchgängig oberhalb des am Ende des normalen Ausatmens erreichten Volumens verbleibt. Vor dem Einatmen muss in der Regel ein Ausatmen erfolgen, etwa in derselben Atempause oder kurz zuvor, um den Kohlenstoffdioxidgehalt niedrig zu halten.

Es ist eine weit verbreitete Vorstellung, dass die Oboe besonders schwer zu spielen sei. Etwa führte das Guinness-Buch der Rekorde von 1989 die Oboe als neben dem Horn schwierigstes Instrument auf. Dies hat verschiedene Gründe:


Es existiert auch ein verbreitetes Gerücht, dass das Spielen der Oboe durch den „Druck im Kopf“ „verrückt“ oder dumm mache. Es gibt hierfür keine wissenschaftlichen Belege. Allerdings haben Oboisten insbesondere als Anfänger mitunter durchaus mit der Atemtechnik zu kämpfen. Eine Untersuchung von 1999 zeigte in der Tat, dass der in der Spitze erreichte Druck im Mund bei Oboisten wesentlich höher als bei Klarinettisten, Saxophonisten oder Fagottisten ist. Ebenso geht das Gerücht, Oboe-Spielen sei mit einem erhöhten Risiko für einen Schlaganfall verbunden.

Die früheste Abbildung eines Oboenvorläufers stammt aus dem Jahre 3000 v. Chr.
Schon während der Antike gab es oboenähnliche Instrumente wie den griechischen Aulos oder die römische Tibia.
Die Bibel erwähnt ein offenbar oboenartiges Instrument namens "Chalil". Dieses wurde im Tempel eingesetzt und den Überlieferungen nach in ganz Jerusalem gehört. Die Psalmen fordern auf, Gott mit dem Chalil zu loben.

Im Mittelalter gab es verschiedene Formen von konischen Doppelrohrblattinstrumenten wie den Pommer oder die Schalmei. Aus letzterer entstand im 17. Jahrhundert durch den Instrumentenbauer Jean de Hotteterre (im Auftrag von Jean-Baptiste Lully) die (Barock-)Oboe. Die Barockoboe hatte zunächst sieben Grifflöcher und zwei Klappen. Im Laufe der Zeit wurde sie von Holzblasinstrumentenbauern weiterentwickelt, enger mensuriert (Französische Bohrung) und mit einer ausgefeilten Mechanik versehen. Im 18. Jahrhundert gab es die beiden Hauptformen der Oboe piccola (die heute gebräuchliche Form) und der Oboe bassa (Grand Hautbois), die etwas größer und eine Terz tiefer (in A) stehend war.
Die ersten Oboen entstanden um 1660 zu Zeiten von Jean-Baptiste Lully und Jean de Hotteterre. Die erste verzeichnete Verwendung der Oboe ist in der Oper "Pomone" (1671) von Robert Cambert zu finden. Diese Oboen wurden vor allem im 19. Jahrhundert durch französische Instrumentenbauer zu den heutigen Modellen umgebaut. 

2017 erklärten die Landesmusikräte Schleswig-Holstein, Baden-Württemberg, Berlin, Brandenburg und Bremen die Oboe zum Instrument des Jahres.

Es gibt verschiedene Arten von Oboen. In Europa sind insbesondere bekannt:


Die Oboe d’amore, in a stehend, klingt eine kleine Terz tiefer als die Oboe. Das Englischhorn steht in f und klingt eine Quinte tiefer. Vorgängerinstrument des Englischhorns in gleicher Stimmlage war die Oboe da caccia. Noch tiefer (eine Oktave unterhalb der Oboe) klingen Heckelphon und Baritonoboe (auch "Bassoboe"), beide sind in c gestimmt, haben jedoch unterschiedliche Mensuren. Die Musette (in f) ist eine Quarte höher als die Oboe gestimmt.
Für den Bedingungen der Barockzeit möglichst ähnliche Aufführungen werden vermehrt auch Instrumente des damaligen Entwicklungsstandes, sogenannte Barockoboen, nachgebaut. Diese besitzen nur eine bis drei Klappen und haben durch das ein wenig breitere Mundstück und durch die engere Mensur einen dunkleren aber leiseren Klang als die modernen, klassischen Oboen.

Als Doppelrohrblattinstrument in Bass-Lage ist im Sinfonieorchester das Fagott etabliert, in noch tieferer Lage das Kontrafagott, die nicht zu den Oboeninstrumenten gezählt werden.

Seit der Barockzeit ist die Oboe ein beliebtes Soloinstrument, viele Komponisten schätzten sie in der Ausdruckskraft als der menschlichen Stimme am ähnlichsten. Johann Sebastian Bach setzte sie in seinen Kantaten und Passionen regelmäßig als Begleitinstrument zur Darstellung unterschiedlicher Affekte (oftmals Leid oder Trauer, aber es finden sich auch genügend Beispiele für pastorale oder freudige Empfindungen) ein. Zudem führt Bach in seinem Werkverzeichnis vier Oboenkonzerte auf. 
Ein bedeutender Komponist für Oboe im 18. Jahrhundert war Georg Philipp Telemann, von dem allein neun Oboenkonzerte erhalten blieben, hinzu kommen drei Konzerte für Oboe d’amore. Eines der ersten Werke, die er in seinem Verlag publizierte, war die "Kleine Cammer-Music", sechs Partiten „besonders […] vor die Hautbois“ von 1716. Diese Partiten sind außerdem Oboisten gewidmet.

So war in der Barockzeit auch die Sonate für Oboe und Generalbass eine beliebte Form, und später trat die Oboe als kammermusikalisches Soloinstrument unter anderem in den "Drei Romanzen" von Robert Schumann und in den Sonaten für Oboe und Klavier von Camille Saint-Saëns oder Paul Hindemith auf. Erwähnenswert sind auch die Sonate für Oboe und Klavier von Francis Poulenc sowie die Werke für Oboe und Klavier von Benjamin Britten.

Zu den bekannten Oboenkonzerten zählen:
Weitere Komponisten, die Beiträge zu dieser Gattung geleistet haben, sind Georg Friedrich Händel und Antonio Vivaldi.

Von Ludwig van Beethovens Konzert für Oboe und Orchester F-Dur Hess 12 sind die Satzanfänge ("Incipits") aus dem Werkverzeichnis von Beethovens Privatsekretär Schindler in einer Abschrift von Anton Diabelli von 1840 erhalten. Vom zweiten Satz wurde 1960 eine Skizze aus Beethovens Skizzenbuch mit Oboenstimme und Teilen der Begleitung gefunden, er wurde von Cees Nieuwenhuizen und Jos van der Zanden rekonstruiert und 2003 erstmals aufgeführt.

Das von Frigyes Hidas als Diplomarbeit geschriebene Oboenkonzert stieß von Anfang an auf Begeisterung und ist heute das meistgespielte ungarische Oboenkonzert.

Der amerikanische Komponist John Corigliano weist in seinem Oboenkonzert auf einige ungewöhnliche, aber typische Eigenarten der Oboe hin: so beginnt der erste Satz, "Tuning Game", mit einem auskomponierten Einstimmen des Orchesters durch die Solooboe, die diese Stimmung dann verändert. Im letzten Satz, "Rheita Dance", imitiert der Oboist eine arabische Oboe (Rhaita), indem er das Rohrblatt weiter in den Mund nimmt, wodurch ein schärferer Klang entsteht.

Ab dem 20. Jahrhundert entstanden viele Werke für Oboe ohne Begleitung. Erwähnenswert sind die Sechs Metamorphosen nach Ovid von Benjamin Britten, die Sonatina von Ernst Krenek, "Monodies" von Charles Koechlin, die Elegie von Dietrich Erdmann, Piri von Isang Yun, Solo für Oboe von Aribert Reimann, "Sequenza VII" von Luciano Berio, sowie zahlreiche Studien Heinz Holligers.

Aus der Barockzeit sind zahlreiche Triosonaten für zwei Oboen und Basso continuo erhalten.
In der Holzbläser-Kammermusik spielt die Oboe im Bläserquintett und in der Harmoniemusik (Bläseroktett, meist je zwei Oboen, Klarinetten, Fagotte und Hörner) eine wichtige Rolle. In der Mozart-Zeit wurden zahlreiche Opern und andere Werke auf 'Harmonie gesetzt'. Weniger bekannt sind Oboentrio (3 Oboen oder 2 Oboen und Englischhorn) oder Rohrblatttrio ("Trio d’Anches", mit Oboe, Klarinette und Fagott). Weitere wichtige Stücke in anderen Besetzungen gibt es von Francis Poulenc, Jean Françaix, Heitor Villa-Lobos, Bohuslav Martinů oder André Jolivet.

Das Oboenquartett (mit Streichtrio) KV 370 von Mozart ist das bekannteste Kammermusikwerk für Oboe mit Streichern, in seiner Tradition stehen einige andere Werke dieser Besetzung. Ein weiteres schönes Beispiel für gemischte Kammermusik mit Oboe ist das "Nonett" von Louis Spohr.

Seit der Barockzeit besitzt die Oboe einen festen Platz im Orchester und ist somit neben Flöte und Fagott die erste Vertreterin der Holzblasinstrumente. In den sehr variablen Besetzungen der Barockzeit findet man in Deutschland (zum Beispiel Bachs Orchestersuiten) meist zwei Oboen, im französischen Stil oft drei, die häufig mehrfach besetzt wurden (Am französischen Hof entstanden zeitgleich mit den „violons du Roi“ auch die ebenso privilegierte Gruppe der „hautbois du Roi“). Seit der Mannheimer Orchesternorm gibt es zwei Oboenstimmen (1. und 2. Oboe), besonders in der Romantik jedoch auch drei und vier (vgl. Gustav Mahler, Richard Strauss) und/oder eine Englischhornstimme. Gelegentlich (selten) werden Oboenstimmen verdoppelt.

Große Oboensoli in der Orchesterliteratur findet man bei allen Komponisten, meistens für lyrische, getragenere Melodien. Erwähnenswert sind neben den erwähnten Werken von Bach zum Beispiel der Trauermarsch in Beethovens 3. Sinfonie, das Thema im langsamen Satz der großen C-Dur-Sinfonie von Schubert, das Thema im langsamen Satz im Violinkonzert von Brahms oder das "Andante" aus der "4. Sinfonie" von Pjotr Iljitsch Tschaikowski. In schnellen Passagen, vor allem im Staccato kann die Oboe auch einen komischen Effekt erzeugen, wie bei vielen Stellen in Wagner-Opern, Alban Bergs "Wozzeck" oder auch gemeinsam mit Flöte und Piccoloflöte im "Kükenballett" von Modest Mussorgskis "Bilder einer Ausstellung" (Ravel-Orchestrierung).

Auch außerhalb ihres klassischen Einsatzbereiches wird die Oboe als Instrument verwendet. Zu erwähnen ist hier sicherlich der französische Oboist Jean-Luc Fillon, welcher der stärker improvisierten Jazz-Musik durch die Verwendung von Oboe und Englischhorn in seinen Stücken neue Impulse gegeben und unbekannte Klanghorizonte eröffnet hat. Der Saxophonist Yusef Lateef verwendet auch öfter die Oboe, die er gerne nach Art der arabischen Rhaita, also mit dem Rohr weiter im Mund spielt, was einen scharfen, schalmeiartigen Ton erzeugt.
Ein weiterer bekannter Oboist der Jazzszene ist Paul McCandless von der Gruppe "Oregon". McCandless spielt eine durch Tabuteau-Technik verfeinerte Lorée, sowie auch Englischhorn und Heckelphon.

Auch in der Rockmusik wurde die Oboe als Instrument gelegentlich eingesetzt. Beispielsweise verwendete Peter Gabriel auf verschiedenen Platten von Genesis ("Nursery Cryme" 1971, "Foxtrot" 1972, "Selling England By The Pound" 1973, "The Lamb Lies Down on Broadway" 1974) die Oboe als markant klingendes Holzblasinstrument zur klanglichen Ergänzung des mitunter filigranen und sehr nuancenreichen Musikstils der Gruppe. Auch Roxy Music hat die Oboe seit den Anfängen regelmäßig eingesetzt. In der Popmusik ist die Oboe u. a. bei Art Garfunkel (im Lied "Bright Eyes", 1979, Komp. Mike Batt) und bei Tanita Tikaram (im Lied "Twist in My Sobriety", 1988) zu hören. Die französische Metal-Band Penumbra verwendet ebenfalls eine Oboe als charakteristisches Merkmal, wie auch die Pagan Metal Band Finsterforst in ihrem Album "Weltenkraft" (2007).

Die Oboe war lange Zeit führendes Instrument der Militärmusik. Daraus hat sich bis Anfang des 20. Jahrhunderts der Unteroffiziersrang "Hautboist" bzw. "Stabsoboist" als Leiter eines Musikkorps erhalten.

Insbesondere in der neuen Musik finden einige Extended techniques auf der Oboe Verwendung, dazu zählen:

Bis in die 1970er Jahre wurde Kindern mit noch nicht ausgereiften Lungen abgeraten, Oboe zu erlernen. Durch die Wiederentdeckung der Barockoboe mit ihren leichter anzublasenden Rohren hat sich dies geändert. So können heute Kinder bereits im Alter von sieben bis zehn Jahren mit dem Oboenunterricht beginnen. Hierzu stehen Oboen speziell für Kinder (mit vereinfachter Mechanik oder ohne Klappen, zum Teil auch in hoch f) zur Verfügung. Unterrichtet wird das Instrument an den meisten Jugendmusikschulen sowie bei Privatmusiklehrern. Besonders förderlich und motivierend ist das frühe Ensemblespiel, zum Beispiel in kleinen Kammermusikgruppen, im Blasorchester oder klassischen Symphonieorchester.

Oboenschulen schrieben u. a. Apollon Barret, Joseph Sellner, Francois Joseph Garnier, Gustav Adolf Hinke.

Wichtige Oboenhersteller sind Marigaux und Rigoutat. Ihre Oboen unterscheiden sich vor allem in der Klangfarbe; die Oboen von Marigaux (gespielt von François Leleux und Lajos Lencses) klingen allgemein weicher und samtiger, während eine Rigoutat (gespielt von Heinz Holliger) direkter und nasaler klingt, wodurch sie sich vor allem für Neue Musik eignet. Weitere wichtige Oboenhersteller sind Lorée, Buffet Crampon, Mönnig (gespielt von Albrecht Mayer) und Dupin (gespielt von Christoph Hartmann).


Bekannte Oboisten des Barock waren vor allem Giuseppe Sammartini und Nicolas Chédeville, die auch beide Kompositionen für das Instrument verfassten. Zur Zeit der Klassik lebten die berühmten Oboisten Ludwig August Lebrun, der auch als Komponist tätig war und einige Konzerte für sein Instrument verfasst hat, und Giuseppe Ferlendis, dem das Oboenkonzert von Wolfgang Amadeus Mozart gewidmet ist. Berühmtester Oboist der Romantik war sicherlich Antonio Pasculli, ein sizilianischer Oboenvirtuose, der virtuose Oboenkonzerte über bekannte Opernthemen schrieb und dadurch technisch neue Maßstäbe des Oboenspiels setzte.

Bekannte Oboisten der ersten Hälfte des 20. Jahrhunderts waren Pierre Pierlot und vor allem Léon Goossens, dem die Oboenkonzerte von Ralph Vaughan Williams, Cyril Scott und Eugène Goossens gewidmet sind.

Bekannte zeitgenössische Oboisten sind Albrecht Mayer und Hansjörg Schellenberger (beide waren bzw. sind Mitglieder der Berliner Philharmoniker), François Leleux, Thomas Indermühle, Emanuel Abbühl, Burkhard Glaetzner, Lajos Lencses, Ingo Goritzki und Heinz Holliger, der sich neben der Wiederentdeckung von Komponisten wie zum Beispiel Jan Dismas Zelenka und Johann Gottlieb Graun besonders für die Avantgarde einsetzt und dem Werke vieler bedeutender zeitgenössischer Komponisten wie Luciano Berio und Isang Yun gewidmet sind.




</doc>
<doc id="3745" url="https://de.wikipedia.org/wiki?curid=3745" title="Organisation erdölexportierender Länder">
Organisation erdölexportierender Länder

Die Organisation erdölexportierender Länder (kurz OPEC, von ) ist eine 1960 gegründete internationale Organisation mit Sitz in Wien. Derzeit gehören dem Kartell vierzehn Staaten an: Algerien, Angola, Ecuador, Äquatorialguinea, Gabun, Iran, Irak, Kuwait, Libyen, Nigeria, Katar, Saudi-Arabien, die Vereinigten Arabischen Emirate und Venezuela. Indonesien ist Ende 2016 ausgetreten.

Fünf OPEC-Mitglieder (Saudi-Arabien, Iran, Kuwait, Venezuela, Vereinigte Arabischen Emirate) gehören zu den zehn größten Erdölförderern der Welt. Insgesamt fördern die OPEC-Mitgliedstaaten ungefähr 40 Prozent der weltweiten Erdölproduktion und verfügen über drei Viertel der weltweiten Erdölreserven. Nachdem alle Nicht-OPEC-Staaten ihr Ölfördermaximum überschritten haben, wird erwartet, dass der Einfluss der OPEC steigt. Andererseits stellen manche Experten (z. B. Matthew Simmons) die Angaben zu den Reserven in Frage, etwa die Saudi-Arabiens.

Das Ziel ist ein monopolisierter Ölmarkt, der sich gegen die Preisbildung auf dem Weltmarkt durch die Festlegung von Förderquoten für die einzelnen OPEC-Mitglieder und die Regelung der Erdölproduktion absichern kann. Durch die künstliche Verknappung oder Steigerung der Ölförderung soll der Preis für Erdöl weltweit nach Absprache aller OPEC-Mitgliedsländer so gedrückt, stabilisiert oder angehoben werden, dass er innerhalb eines festgelegten Zielpreiskorridors liegt. Dieser Zielpreiskorridor ist jeweils auch variabel, aber gilt als Richtwert über einen längeren Zeitraum.

Allerdings kommt es auch vor, dass sich einzelne Mitglieder nicht an die festgesetzten Förderquoten halten, sondern ihre eigenen wirtschaftlichen und politischen Ziele verfolgen. So kündigte beispielsweise Indonesien 2008 seinen Austritt an, da die OPEC-Preisvorstellungen für den größeren inländischen Markt eine höhere Belastung darstellen, als man durch teure Exporte wieder ausgleichen könnte. (Indonesien wurde 2008 durch abnehmende Fördermengen zum Netto-Importeur, verlor also seine Fähigkeit, mehr Öl zu fördern, als es für den Eigenbedarf benötigt.)

Als OPEC-Hardliner gelten vor allem Algerien, Iran, Libyen und Venezuela. So sprach der OPEC-Vorsitzende Chakib Khelil offen im französischen Sender France 24 von möglichen Preiserhöhungen auf bis zu 400 Dollar pro Fass, sofern darüber Einigkeit herrsche.

Andere OPEC-Länder geben den Forderungen der Industriestaaten nach weniger gedrosselten Förderquoten gegen die Gefahr einer Rezession teils aber auch nach. Als Gegenmaßnahme operieren einige OPEC-Staaten hingegen wieder verstärkt mit zahlungskräftigen Finanzinvestoren in den Märkten. Dies funktioniert so, dass staatliche Indexinvestoren sich über die eigenen Staatsfonds vermehrt an den gehandelten Rohstoffindizes beteiligen als die in den Medien oftmals nur vermeintlichen Spekulanten.

Die obersten Organe der OPEC sind:


Darüber hinaus bestehen noch folgende Organe:

Weiters wurden ein Rechtsbeirat () sowie ein Interner Rechnungsprüfer eingerichtet.

Der sogenannte Präsident der OPEC ist tatsächlich lediglich der Präsident der Ministerkonferenz und als solcher lediglich zum Vorsitz der Sitzung berufen, auf der er gewählt wurde. Er behält das Amt bis zur nächsten Sitzung der Ministerkonferenz (die statutengemäß halbjährlich stattfinden).

Die Organisation funktioniert folgendermaßen:
Dreimal jährlich treffen sich die für Energie und Erdöl zuständigen Minister der OPEC-Mitgliedsländer zur Ministerkonferenz, um den Stand des Erdölmarktes zu beurteilen und entsprechende Maßnahmen einzuleiten, die dazu dienen sollen, einen stabilen Ölmarkt sicherzustellen und gleichzeitig ihre eigenen Erdöl-Gewinne zu sichern. Diese Konferenz gibt anschließend die neuen Richtlinien preis. Wesentliche Richtlinien sind die Festlegung der Erdölförderquoten, die seit 1985 an die vorhandenen Reserven gekoppelt sind. Bei Überschreitung der festgelegten Quoten kann die Ministerkonferenz Sanktionen einleiten. (Dies ist jedoch bislang nicht eingetreten, weil die betreffenden Staaten ihre Ölvorkommen in der Vergangenheit deutlich – und anscheinend auch künstlich – nach oben korrigiert haben.)

Die Öffentlichkeitsarbeit führt das Sekretariat. Ihm unterliegen zudem die Aufgaben der Forschung im Bereich Energie und Finanzen, weiterhin werden Statistiken erstellt und veröffentlicht. Auch Vorträge und Seminare sind Aufgabenbereich des Sekretariats. Es verfügt auch über eine große Bibliothek, die den Vertretern der Mitgliedstaaten sowie Forschern und Studenten offensteht. Finanziert wird das Sekretariat durch Beiträge der Mitgliedstaaten.
Der Repräsentant der OPEC ist der Generalsekretär, der auch Leiter des Sekretariats ist. Dieser wird entweder für drei Jahre gewählt oder alphabetisch durch das Rotationsprinzip für zwei Jahre ernannt.
Abteilungen des Sekretariats sind: Forschung, Energiestudien, Wirtschaft und Finanzen, Datenservice, Personal und Verwaltung, OPECNA (OPEC News Agency), Büro des Generalsekretärs und Recht.

Während der 1950er Jahre sank der Ölpreis wegen der Erschließung immer neuer Quellen und des damit verbundenen Überangebots auf dem Weltmarkt kontinuierlich ab, was zu schweren Verlusten in den Staatskassen der Ölförderländer führte. Um 1960 befanden sich mehrere von ihnen deshalb in ernsten Haushaltskrisen. In dieser Situation regte Saudi-Arabien die Gründung eines Förderkartells an. Es sollte nicht nur die Fördermenge kontrollieren, sondern auch ein Gegengewicht zu den großen Ölkonzernen bilden, die auf der Basis von Verträgen aus der Kolonialzeit ihre Gewinne weitgehend ohne Beteiligung der Staaten erwirtschafteten, auf deren Gebiet die Ölquellen lagen.

Am 14. September 1960 wurde die Organisation in Bagdad gegründet. Die Gründungsmitglieder waren Irak, Iran, Kuwait, Saudi-Arabien und Venezuela. Bis 1975 schlossen sich acht weitere Staaten an: Katar (1961), Indonesien und Libyen (1962), die Vereinigten Arabischen Emirate (1967), Algerien (1969), Nigeria (1971), Ecuador (1973) und Gabun (1975). Ecuador und Gabun traten 1992 bzw. 1995 wieder aus. 2007 trat Angola bei, und am 17. November 2007 kehrte Ecuador nach 15-jähriger Pause in die Organisation zurück. Am 1. Juli 2016 trat auch Gabun der Organisation wieder bei. Indonesien trat im Januar 2009 aus, wurde im Dezember 2015 wieder aufgenommen und trat im Dezember 2016 erneut aus.

Als erste Maßnahmen wurden eine weitgehende Verstaatlichung der Ölquellen, das zukünftige Absprechen der Fördermengen und eine erhöhte Besteuerung der Ölfirmen vereinbart. Letzteres sollte eine von der Fördermenge unabhängige Geldquelle eröffnen. Zunächst blieb die OPEC weitgehend wirkungslos und wurde auch weltweit als wenig schlagkräftig eingeschätzt, zumal sie nur einen kleinen Teil der Förderländer umfasste.






</doc>
<doc id="3746" url="https://de.wikipedia.org/wiki?curid=3746" title="Octavia E. Butler">
Octavia E. Butler

Octavia Estelle Butler (* 22. Juni 1947 in Pasadena, Kalifornien; † 24. Februar 2006 in Lake Forest Park, Seattle) war eine amerikanische Science-Fiction-Autorin. Sie war eine der wenigen schwarzen Schriftstellerinnen des Science-Fiction-Genres und die erste, die größere Bekanntheit erreichte. Ihr Werk behandelt häufig feministische und rassenproblematische Themen, so sind ihre Protagonisten oft Teil einer Minderheit.

Octavia E. Butler war die Tochter von Laurice und Octavia M. (Guy) Butler und wuchs in ärmlichen Verhältnissen auf. Ihr Vater war ein Schuhputzer und starb, als sie noch sehr jung war; ihre Mutter war ein schlechtbezahltes Dienstmädchen. Während ihrer Schulzeit wurde bei ihr eine Leseschwäche diagnostiziert. Sie besuchte dennoch von 1965 bis 1968 das Pasadena City College und ab 1969 die California State University in Los Angeles und die University of California in Los Angeles. Dort besuchte sie weiterführende Kurse, bei denen einer von Theodore Sturgeon geleitet wurde. Als sie beim Open-Door-Programm der Writers Guild of America einige Sitcom-Manuskripte einreichte, riet ihr der Seminarleiter Harlan Ellison, es stattdessen ernsthaft mit Kurzgeschichten und Romanen zu versuchen. Das tat sie. Ellison überredete sie auch, am "Clarion Science Fiction Writers’ Workshop" teilzunehmen, wo sie Samuel R. Delany traf und ihre ersten Geschichten schrieb, die verkauft und veröffentlicht wurden. Danach schlug sich Octavia Butler mit Gelegenheitsjobs durch, schrieb weiter und schaffte es schließlich mit Ellisons Hilfe, ihren ersten Roman bei Doubleday unterzubringen. 

1976 wurde sie der breiten Öffentlichkeit bekannt, als sie den Roman "Patternmaster" veröffentlichte, Auftakt des Patternmaster-Zyklus'. Er schildert die Entstehung telepathisch begabter Menschen und behandelt dabei auch Rassen- und Geschlechterfragen. Es folgten vier weitere Romane aus diesem Zyklus, ehe Octavia E. Butler mit "Kindred" 1979 ein Buch veröffentlichte, das kaum noch etwas mit SF zu tun hatte und sehr erfolgreich war. Ohne die Zeitreise irgendwie technisch oder wissenschaftlich zu erklären, beschreibt sie darin, wie eine junge schwarze Frau immer wieder in die Vergangenheit ihrer Vorfahren hineingerissen wird, einschließlich eines weißen Sklavenhändlers. Dieser Kunstgriff, Gegenwart und Geschichte in einer Person zu konzentrieren, brachte dem Buch ein großes Publikum und Octavia E. Butler den ökonomischen Erfolg, um fortan vom Schreiben leben zu können.

In den 1980er Jahren gewann sie mit der Erzählung "Bloodchild" 1985 die vier wichtigsten SF-Preise zugleich, nachdem sie bereits ein Jahr zuvor den Hugo Award erhalten hatte. Danach veröffentlichte sie ihre "Xenogenesis"-Trilogie (später unter dem Titel "Lilith's Brood" zusammengefasst). Darin geht es um die Rettung der Menschheit nach einem verheerenden Krieg; die Überlebenden treffen auf eine Alien-Rasse, die ein drittes Geschlecht besitzt, welches nicht nur die anderen beiden Geschlechter mental miteinander verknüpfen, sondern sie auch genetisch verändern kann.

Die 1990er Jahre über arbeitete Octavia E. Butler an ihrer "Parable"-Trilogie, von der sie nur die ersten beiden Bände fertigstellen und veröffentlichen konnte. Sie schildert darin eine Dystopie, in der eine neue Religion namens "Earthseed" die stete Veränderung aller Dinge predigt. Gesundheitliche Probleme und eine Schreibblockade hinderten sie daran, den dritten Band zu vollenden. Der zweite Band "Parable of the Talents" gewann 1999 den Nebula Award.

1995 wurde sie mit dem "Genius Award" des MacArthur Fellows Program ausgezeichnet, der mit 295.000 Dollar dotiert war. Erst 2005 veröffentlichte sie ihren nächsten und letzten Roman "Fledgling", einen Vampir-Roman mit einigen SF-Elementen. 

Bis 1999 lebte sie im Süden Kaliforniens, zog aber nach dem Tod ihrer Mutter nach Seattle. In ihren letzten Lebensjahren war sie gesundheitlich angeschlagen: sie litt unter Bluthochdruck und hatte Herzprobleme. Am 24. Februar 2006 stürzte sie auf dem Kopfsteinpflaster vor ihrem Haus und zog sich eine Kopfverletzung zu. Sie starb noch am selben Tag.

Die Zuordnung der Romane Butlers zu diesem Zyklus gibt es in den unterschiedlichsten Varianten – die Angaben schwanken dabei zwischen 3 und 6 Romanen. 2007 ist allerdings ein Sammelband erschienen, der die vier Romane "Patternmaster", "Mind of My Mind", "Wild Seed" und "Clay's Ark" enthält. Die thematisch weniger zu diesen vier passenden Romane "Survivor" und "Kindred" werden deshalb weiter unten als Einzelwerke aufgeführt.






"Survivor", 1978
"Alanna", Bastei-Lübbe, 1984, ISBN 3-404-24052-9

Dieses Buch ist Teil der Patternist-Saga und reiht sich zeitlich ein nach "Clay's Ark" und vor "Patternmaster". Dieses Buch ist nicht Teil des Sammelbandes, da Octavia Butler die weitere Veröffentlichung nach wenigen Auflagen einstellte.























</doc>
<doc id="3747" url="https://de.wikipedia.org/wiki?curid=3747" title="Ohr">
Ohr

Das Ohr ist ein Sinnesorgan, mit dem Schall, also Töne, Klänge oder Geräusche aufgenommen werden. Zum Ohr als Organ gehört auch das Gleichgewichtsorgan.

Zum Hörsystem, das die auditive Wahrnehmung ermöglicht, gehören außer Außen-, Mittel- und Innenohr auch der Hörnerv und die Umschalt- und Verarbeitungsstationen im zentralen Nervensystem, bei Säugetieren also einige Areale im Hirnstamm und Zwischenhirn, bis hinauf zur auditiven Hirnrinde.

Das gemeingerm. Wort mhd. "ōre", ahd. "ōra" beruht auf idg. "*ōus-" „Ohr“ (vgl. ; "us", Genitiv ὠτός "otós").

Der Hörsinn ist gegen den Vibrationssinn abzugrenzen. Letzterer nimmt Substratschall auf, etwa wenn der Untergrund vibriert. Hören, d. h. die Wahrnehmung rhythmischer Druckwellen in Luft oder Wasser, ist in der Evolution nur bei relativ wenigen Tiergruppen entstanden. Fast alle Tetrapoden, viele Fische und etliche Insektenarten können demnach hören, ebenso einige Kopffüßer. Die meisten Wirbellosen leben jedoch in einer stummen Welt. Bei den Wirbeltieren hat die Natur das Hören wahrscheinlich 2- bis 3-mal unabhängig voneinander erfunden. Die ersten Hörorgane entstanden im Devon vor etwa 380 Millionen Jahren. Ein wesentlicher Schritt zum Erwerb eines guten Hörvermögens war danach die Entwicklung eines Mittel- und Innenohrs, inklusive eines Trommelfelles. Bei den Insekten entstand das Hörvermögen sogar mindestens 20-mal unabhängig voneinander.

Aufbau und Platzierung der Hörorgane sind bei den verschiedenen Arten sehr unterschiedlich. Bei Heuschrecken sitzen die Ohren am Hinterleib oder den Beinen, bei Zikaden an den Beinen und bei Mücken und Fliegen an den Fühlern. Einige Eidechsen- und Salamanderarten hören mit Brustkorb und Lunge. Äußere Ohren sind bei den meisten Säugetierarten und Vogelarten vorhanden, Ausnahmen finden sich bei einigen Delfinarten. Reptilien, Amphibien und Fische haben keine äußeren Ohren. Bei Reptilien und Amphibien sitzt dadurch das Trommelfell direkt an der Außenseite des Kopfes.

Der Hörbereich des menschlichen Ohrs reicht in jungen Jahren von etwa 16 Hertz bis maximal 20.000 Hertz. Unter anderem können Elefanten noch tiefere Frequenzen wahrnehmen, den Infraschall, während eine Reihe von Tieren, zum Beispiel Mäuse, Hunde, Delfine und Fledermäuse, noch wesentlich höhere Frequenzen, den Ultraschall, hören können.

Eine Aufgabe des Hörens ist die Orientierung im Raum, also Schallquellen zu lokalisieren, das heißt, deren Richtung und Entfernung zu bestimmen. Seitlich einfallender Schall erreicht das zugewandte Ohr eher als das abgewandte und ist dort lauter, da das abgewandte Ohr durch den Kopf abgeschattet wird. Diese Laufzeitdifferenzen und Pegeldifferenzen zwischen beiden Ohren werden vom Gehirn ausgewertet und zur Richtungsbestimmung genutzt. Darüber hinaus erzeugt die Ohrmuschel je nach Richtung spezifische Veränderungen des Frequenzgangs, die ebenfalls ausgewertet und zur Richtungsbestimmung benutzt werden.

Viele Lebewesen, auch der Mensch, können vorhandene Schallquellen lokalisieren, die Orientierung im Raum erfolgt aber vor allem mit Hilfe des Gleichgewichtssinns und des Gesichtssinns. Delfine und Fledermäuse haben in der Evolution den Gehörsinn zu einem besonders hochstehenden Orientierungssystem entwickelt. Beide stoßen hochfrequente Signale im Ultraschallbereich aus (bis 200 kHz) und orientieren sich anhand des Echos. Dieses aktive Verfahren zur Orientierung nennt man Ortung. Bei den Fledermäusen hat das Gehör die Augen weitgehend ersetzt, die in der Dunkelheit von keinem großen Nutzen sind.

Beim Menschen und anderen Säugetieren wird das Ohr in drei Bereiche eingeteilt:


Die Wahrnehmung von akustischen Signalen wird wesentlich davon mitbestimmt, wie Schallschwingungen auf ihrem Weg vom Außenohr über das Mittelohr hin zu den Nervenzellen des Innenohrs jeweils umgeformt und verarbeitet werden. Das menschliche Gehör kann akustische Ereignisse nur innerhalb eines bestimmten Frequenz- und Schalldruckpegelbereichs wahrnehmen. Zwischen der Hörschwelle und der Schmerzschwelle liegt die Hörfläche.

Der leiseste wahrnehmbare Schalldruck bei normalhörenden Menschen ist bei einem Ton von 2.000 Hz etwa 20 Mikro-Pascal (20 µPa = 2·10 Pa), das entspricht "L" = 0 dB Schalldruckpegel. Diese Schalldruckveränderungen "Δ p" werden über das Trommelfell und die Mittelohrknöchelchen ins Innenohr übertragen, und im Ohr-Gehirnsystem entsteht dann der Höreindruck. Weil das Trommelfell als Sensor mit dem Ohrsystem die Eigenschaften eines Schalldruckempfängers hat, beschreibt der Schalldruckpegel als Schallfeldgröße die Stärke des Höreindrucks am besten. Die Schallintensität "J" in W/m² ist als Schallenergiegröße hingegen nicht geeignet, den Höreindruck zu beschreiben; aufgrund der komplexen Impedanz des Außen- und Mittelohres bei gleichem Schalldruckpegel. Gleiches gilt sinngemäß für die Schallschnelle.

Das menschliche Gehör vermag bereits eine äußerst geringe Schallleistung aufzunehmen. Der leiseste wahrnehmbare Schall erzeugt eine Leistung von weniger als 10 W im Innenohr. Innerhalb einer zehntel Sekunde, die das Ohr braucht, um dieses Signal in Nervenimpulse umzusetzen, wird durch eine Energie von etwa 10 Joule schon ein Sinneseindruck erzeugt. Daran wird deutlich, wie empfindlich dieses Sinnesorgan eigentlich ist.

Die Schmerzgrenze liegt bei über 130 dB, das ist mehr als der dreimillionenfache Schalldruck des kleinsten hörbaren (63,246:0,00002 = 3.162.300). Vor allem das Innenohr und hier die Haarzellen und deren Stereozilien, nehmen bei hohem Schalldruck Schaden.

Beim Richtungshören und bei der Kopfhörer-Stereofonie spielen Laufzeitunterschiede und Pegelunterschiede zwischen beiden Ohren und somit auch der individuelle Ohrabstand eine gewisse Rolle, sowie spektrale Eigenschaften der Ohrsignale.

Die Techniken zur Untersuchung der Hörfähigkeit werden unter dem Begriff Audiometrie zusammengefasst. Ein Ergebnis eines Hörtests, der das Hörvermögen bei verschiedenen Frequenzen untersucht, nennt sich Audiogramm. Aus diesem lässt sich meistens die Hörschwelle ablesen.

Außerhalb des eigentlichen Ohres liegen jedoch die Nervenbahnen, die zum Hörzentrum des Hirns führen, sowie das Hörzentrum selbst. Sind diese beeinträchtigt, so kann auch bei einem funktionsfähigen Ohr die Schallwahrnehmung beeinträchtigt sein.

Der Weg des Schalls: Ohrmuschel → Gehörgang → Trommelfell → Gehörknöchelchen → Hörschnecke → Hörnerv

Das menschliche Ohr kann auf verschiedenartige Weisen erkranken, die jeweils für den betroffenen Teil des Ohres spezifisch sind.

Zur Diagnostik von Erkrankungen des Ohres stehen, insbesondere der Hals-Nasen-Ohren-Heilkunde, neben den allgemein üblichen Methoden der Medizin wie Röntgenuntersuchungen, serologischen und visuellen Untersuchungen auch Hörtests zur Verfügung.

Der Abdruck der Ohren kann zur Identifizierung einer Person dienen. Dabei hat der Ohrabdruck einen ähnlich hohen Beweiswert wie ein Fingerabdruck. Die Kriminalistik kann auf Basis der hinterlassenen Ohrabdrücke, z. B. beim Lauschen an Fenstern oder Haustüren, durchaus Straftäter überführen. Vorteil gegenüber dem Fingerabdruck ist, dass ein Ohrabdruck meist nicht zufällig entsteht. Fingerabdrücke sind meist von vielen Personen am Tatort zu finden.

Das menschliche Außenohr wächst nach der Adoleszenz langsam weiter, mit durchschnittlich etwa 0,2 mm pro Jahr.




</doc>
<doc id="3748" url="https://de.wikipedia.org/wiki?curid=3748" title="Optik">
Optik

Die Optik (von altgriechisch "optikós" ‚zum Sehen gehörend‘), auch "Lehre vom Licht" genannt, ist ein Gebiet der Physik und beschäftigt sich mit der Ausbreitung von Licht sowie dessen Wechselwirkung mit Materie, insbesondere im Zusammenhang mit optischen Abbildungen.

Die „technische Optik“ (s. u.) ist dagegen kein Wissenschaftsgebiet der Physik, sondern eine technische Disziplin bezogen auf die Fertigung optischer Systeme. Umgangssprachlich wird die Summe aller optischen Bauteile eines Gerätes gelegentlich auch "Optik" genannt. Optik im Sinne von "Aussehen" findet sich ebenfalls in der Umgangssprache.

Unter Licht wird in der Regel der sichtbare Teil des elektromagnetischen Spektrums im Bereich 380 nm und 780 nm (790 THz bis 385 THz) verstanden. In der Physik wird der Begriff Licht mitunter auch auf unsichtbare Bereiche von elektromagnetischer Strahlung ausgeweitet und umfasst im allgemeinen Sprachgebrauch dann auch das Infrarotlicht oder das ultraviolette Licht. Viele Gesetzmäßigkeiten und Methoden der klassischen Optik gelten allerdings auch außerhalb des Bereichs sichtbaren Lichts. Dies erlaubt eine Übertragung der Erkenntnisse der Optik auf andere Spektralbereiche, zum Beispiel die Röntgenstrahlung (siehe Röntgenoptik) sowie Mikro- und Funkwellen.

Auch Strahlen geladener Teilchen bewegen sich in elektrischen oder magnetischen Feldern oft nach den Gesetzen der Optik (siehe Elektronenoptik).

Es werden zwei klassische Zugänge zur Lichtausbreitung unterschieden: Die Wellenoptik und die geometrische Optik. Grundlage der Wellenoptik ist die Wellennatur des Lichts. Die Gesetzmäßigkeiten der geometrischen Optik gelten für den Fall, dass die Abmessungen des optischen Systems sehr groß sind gegenüber der Wellenlänge des Lichts. Bei geringen Abmessungen der Komponenten gegenüber der Wellenlänge spricht man von der Mikrooptik.

Eine wichtige Teildisziplin der Optik ist die Quantenoptik, die sich mit den Wechselwirkungen von Licht und Materie beschäftigt. Dabei spielt besonders der gequantelte Charakter des Lichts eine bedeutende Rolle.

Daneben sind die nichtlineare Optik (bei der das Licht im Gegensatz zur "linearen Optik" das umgebende Medium beeinflusst und dadurch zusätzliche Effekte bewirkt) und die Fourieroptik von theoretischem und technischem Interesse. Ein interdisziplinärer Teilbereich ist die atmosphärische Optik, in der Leuchterscheinungen in der Erdatmosphäre untersucht werden.

In der geometrischen Optik wird Licht durch idealisierte Strahlen angenähert. Der Weg des Lichtes, etwa durch ein optisches Instrument, wird durch Verfolgen des Strahlenverlaufs konstruiert. Das snelliussche Brechungsgesetz beschreibt die Brechung des Lichtes an Grenzflächen zwischen transparenten Medien mit verschiedenem Brechungsindex (an Oberflächen von Linsen oder Prismen). Bei Reflexion an Spiegeln und bei der Totalreflexion gilt die Regel, dass der Einfallswinkel dem Reflexionswinkel gleich ist. Mittels dieser Methode lassen sich Abbildungen, beispielsweise durch Linsen oder Linsensysteme (Mikroskop, Teleskop, Objektiv) und die dabei auftretenden Abbildungsfehler behandeln. Eine wichtige Näherung ist die paraxiale Optik, welche aus einer Linearisierung des Snelliusschen Brechungsgesetzes abgeleitet werden kann, und wichtige Begriffe wie Brennweite und Abbildungsmaßstab definiert.

Als Wellenoptik wird der Bereich der Optik bezeichnet, der von der Wellennatur des Lichts handelt. Sie erklärt Phänomene, die durch die geometrische Optik nicht erklärt werden können, da bei ihnen die Welleneigenschaft des Lichtes relevant sind. Beispielsweise ist in der geometrischen Optik im Prinzip eine ideale Abbildung möglich, wohingegen die Wellenoptik zeigt, dass durch Beugungseffekte der Auflösung eine prinzipielle Grenze gesetzt ist; dies ist unter anderem bei fotolithografischen Prozessschritten bei der Herstellung moderner integrierter Schaltungen zu beachten. Wichtige Elemente der Wellenoptik sind:


Die Wellenoptik kann auch Effekte beschreiben, die von der Wellenlänge des Lichts abhängen; man spricht dabei allgemein von Dispersion. (Beispiel: „Warum ist der Himmel blau?“) Je nach oben genanntem Mechanismus müssen sehr verschiedene Modelle zur Beschreibung genutzt werden, die zu sehr unterschiedlichen Wellenlängenabhängigkeiten führen.

Auf der Wellenoptik bauen die Kristalloptik und die Magnetooptik auf.

Die Wechselwirkung von Licht mit wirklichen (d. h. nicht idealisierten) Oberflächen ist für die optische Wahrnehmung des Menschen bedeutsam, ist aber bislang nur unvollständig verstanden. Bedeutsam ist die Remission, also die Absorption eines Teil des Lichts sowie die Reflexion, Transmission beziehungsweise Streuung des restlichen Spektralanteils. Reflexion und Transmission lassen sich durch die Brechung des Lichts an den Grenzflächen beschreiben. Wiederum ist eine Wellenlängenabhängigkeit der meisten Mechanismen zu beachten, also deren Dispersion.

Manche Oberflächen, wie etwa die menschliche Haut, sind in den obersten Hautschichten teilweise transparent, so dass optisch keine reflektierende Fläche, sondern eine reflektierende Schicht vorliegt. Eine abstrakte Beschreibung der optischen Vorgänge an derartigen Oberflächen ist kompliziert, und einer der Gründe, dass computergenerierte Bilder künstlich wirken können.

Das Auge ist das optische Sinnesorgan des Menschen, es wertet den Reiz von Licht unterschiedlicher Wellenlänge an den Photorezeptoren zu Aktionspotentialfolgen der Ganglienzellen der Netzhaut aus. Die physiologische Optik befasst sich mit der Optik und dem Aufbau des Auges. In der Medizin spricht man bei der das Auge betreffenden Medizin von der Augenoptik oder Optometrie als der Messung der Sehweite. Der Vorgang des Sehens lässt sich mithilfe der Optik nur teilweise erklären. Das Gehirn spielt dabei eine große Rolle, denn es verarbeitet Informationen erst zu dem, was wir als Sehen bezeichnen. Dieser Teil fällt aber der Biologie zu. Alle durchsichtigen Teile des Auges wirken zusammen wie eine einzige Sammellinse und entwerfen ein stark verkleinertes, verkehrtes, wirkliches Bild.

Mit der Entstehungsgeschichte von Sehhilfen sind die auf Gotland gefundenen Visby-Linsen aus dem 11. Jahrhundert verbunden. Einige dieser Linsen können sich in der Abbildungsqualität mit heutigen Linsen messen.
Die Herkunft der Visby-Linsen ist trotz genauer Analyse unklar, die Verarbeitung von Bergkristall war aber bereits im 11. Jahrhundert weit verbreitet.

Der arabische Gelehrte Ibn Al-Haitham (996–1038) schrieb über das Sehen, die Refraktion (Augenoptik) und die Reflexion in seinem Buch „Schatz der Optik“. Um 1240 wurde das Buch ins Lateinische übersetzt. Genial war seine Überlegung, das Auge mit geschliffenen Linsen zu unterstützen. Europäische Mönche griffen diesen Gedanken auf und fertigten später als im Orient halbkugelige Plankonvexlinsen für Sehhilfen (Lesesteine).

Das Design, die Auslegung und die Fertigung optischer Systeme wird als technische Optik bezeichnet und zählt im Unterschied zur physikalischen Optik zu den Ingenieurwissenschaften, da hier die konkrete Konstruktion und Herstellung optischer Geräte sowie die Konzeption spezifischer Strahlengänge im Vordergrund stehen. Bedeutende Vertreter dieser Fachrichtung waren unter anderen Johannes Kepler, Eustachio Divini (1610–1685), Joseph von Fraunhofer und Ernst Abbe. Die heutige gewerbliche Berufsbezeichnung ist "Feinoptiker".

Sie stellt eine inhaltliche Verknüpfung der Teilgebiete Optische Messtechnik, Lasertechnik und theoretische Optik (einschließlich Mikrooptik, Lichttechnik oder Faseroptik) dar. Anwendung findet die technische Optik unter anderem in der Projektionstechnik, Holografie und Fotografie sowie in der Spektroskopie.

Im Folgenden sind die wichtigsten Bauelemente, Komponenten und Geräte aufgelistet.

Optische Bauelemente

Optische Komponenten

Optische Geräte

Bekannte Optiker waren Ernst Abbe, Alhazen, Laurent Cassegrain, John Dollond, Peter Dollond, Benjamin Franklin, Joseph von Fraunhofer, Hans-Joachim Haase, Hans Lipperhey, Zacharias Janssen, Christiaan Huygens, Johannes Kepler, Antoni van Leeuwenhoek, Johann Nathanael Lieberkühn, Dmitri Dmitrijewitsch Maksutow, Isaac Newton, Josef Maximilian Petzval, Hermann Pistor, Carl Pulfrich, Christoph Scheiner, Bernhard Schmidt, Ludwig Seidel und August Sonnefeld.




</doc>
<doc id="3749" url="https://de.wikipedia.org/wiki?curid=3749" title="Ordnung (Biologie)">
Ordnung (Biologie)

Die Ordnung () ist eine Rangstufe der biologischen Systematik. Sie dient zur Einteilung und Benennung der Lebewesen (Taxonomie). 

Bezüglich der Hauptstufen steht die Ordnung zwischen Klasse und Familie. Zusätzlich kann unmittelbar oberhalb der Ordnung eine Überordnung ("superordo") und unmittelbar unterhalb eine Unterordnung ("subordo") sowie Teilordnung ("infraordo") vorhanden sein.

In der Botanik leiten sich die wissenschaftlichen Ordnungsnamen von der Gattung der Typusart ab und enden auf "-ales" (zum Beispiel Asterales, abgeleitet von Aster). Die Endung führte John Lindley 1833 ein, durch George Bentham und William Jackson Hooker wurde sie 1862 etabliert.

Noch bei Carl von Linné ("ordines naturales") und Antoine-Laurent de Jussieu bezeichnete die Ordnung eine Rangstufe oberhalb der Gattung und entsprach eher der heutigen Familie. Nachdem sich der Gebrauch der Familie eingebürgert hatte, wurde die Ordnung als nächsthöhere Rangstufe übernommen. 

Zeitweise gab es alternative Rangstufenbezeichner wie "nixus", "cohors" oder "alliance". Von besonderer Bedeutung war der bis 1959 insbesondere in der deutschsprachigen Botanik verwendete und von Adolf Engler eingeführte Begriff „Reihe“. Diese Begriffe wurden zugunsten der Ordnung verworfen, entsprechend beschriebene Taxa gelten jedoch als gültige Ordnungen.

Bei den Pilzen ist die Ordnung oft die wesentlichste Darstellungsebene.



</doc>
<doc id="3750" url="https://de.wikipedia.org/wiki?curid=3750" title="Olympiade">
Olympiade

Eine Olympiade (vom Wortstamm "Olympiád-" des griechischen Substantivs "Olympiás" „olympisch, Olympische Spiele, Sieg bei den Olympischen Spielen, Zeitraum zwischen zwei Olympischen Spielen“; der Wortstamm ist im Nominativ nicht erkennbar, aber z. B. im Genitiv "Olympiádos") bezeichnet gemeinhin den vierjährigen Zeitraum zwischen zwei Olympischen Spielen. Dem Ursprung nach beginnt sie mit der Eröffnung Olympischer Spiele und endet mit dem Moment, in dem die Folgeolympiade einsetzt. Nach neuzeitlicher Definition beginnt eine Olympiade dagegen bereits mit dem Beginn des Jahres, in dem Olympische Sommerspiele turnusgemäß abgehalten werden, womit sie dann exakt vier Jahre umfasst. Schon in der griechischen Archaik des 6. Jahrhunderts v. Chr., wie teilweise auch heute, wird die Bezeichnung auch für die Olympischen Spiele selbst verwendet.

Die ersten Olympischen Spiele in Olympia wurden der Überlieferung zufolge im antiken Griechenland im Jahr 776 v. Chr. abgehalten. Mit diesem Jahr beginnen die Siegerlisten. Es ist der Ausgangspunkt der klassischen griechischen Zeitrechnung. Die Zählung der Olympiaden wurde aber erst vom Geschichtsschreiber Timaios von Tauromenion eingeführt, der im 4. und 3. Jahrhundert v. Chr. lebte. Erst danach begann die Datierung von Ereignissen nach Olympiaden, und die Olympiadenrechnung wurde zur Grundlage der griechischen Chronologie.

1896 begann die erste Olympiade der Neuzeit in Athen. Das Internationale Olympische Komitee definiert die Olympiade als den Zeitraum von vier Jahren, der jeweils am 1. Januar des Jahres der Sommerspiele beginnt. Die erste Olympiade der Neuzeit begann demnach am 1. Januar 1896, die XXXI. Olympiade der Neuzeit am 1. Januar 2016, im Jahr der Olympischen Sommerspiele 2016, die deshalb offiziell die "Spiele (zur Feier) der XXXI. Olympiade" heißen. Die Olympiaden werden mit römischen Zahlen nummeriert. Sommerspiele finden in jenen Jahren statt, die durch vier teilbar sind. 

Die Zeitrechnung in Olympiaden ist unabhängig davon, ob die Spiele stattfinden oder nicht. Die Olympischen Sommerspiele 1936 waren die "Spiele der XI. Olympiade". Nachdem wegen des Zweiten Weltkrieges 1940 und 1944 keine Spiele ausgetragen wurden, wurden die Sportveranstaltungen 1948 mit den "Spielen der XIV. Olympiade" wieder aufgenommen.

Die Winterspiele werden im Gegensatz zu den Sommerspielen fortlaufend nummeriert. Die "IV. Olympischen Winterspiele" wurden 1936 ausgetragen, die "V. Olympischen Winterspiele" aber erst 1948. Seit 1994 finden die Olympischen Winterspiele zudem in jenen geraden Jahren statt, in denen keine Sommerspiele stattfinden. Sie finden seither in der Mitte jeder Olympiade statt.

Schon Pindar verwandte zu seiner Zeit (im 6. Jahrhundert v. Chr.) den Begriff nicht nur für den Vierjahreszeitraum, sondern ebenfalls für die Veranstaltung der "Olympischen Spiele". Dies gilt ebenso für viele Sprachen heute. In deutschen Wörterbüchern und Lexika ist die synonyme Verwendung für die Spiele seit den 1930er Jahren belegt. Im Englischen unterscheidet man hingegen zwischen dem Zeitraum der "Olympiad" und der Veranstaltung "Olympics".

Im Deutschen und in weiteren Sprachen erhalten andere Wettbewerbe den Namen, wie Schacholympiade oder Deutsche Mathematik-Olympiade.

Die Begriffe „Olympiade“, „Olympia“ und „olympisch“ sind in Deutschland durch das Olympiaschutzgesetz geschützt. Eine Verwendung außerhalb der vom IOC veranstalteten oder genehmigten Anlässe kann zu Schadenersatzforderungen führen. Allerdings wurde die Verfassungsmäßigkeit des Gesetzes durch das Landgericht Darmstadt in einem zwischenzeitlich rechtskräftigen Urteil (Aktenzeichen 14 O 744/04) angezweifelt, wodurch eine tatsächliche Rechtswirkung fraglich bleibt.




</doc>
<doc id="3751" url="https://de.wikipedia.org/wiki?curid=3751" title="Odenwaldkreis">
Odenwaldkreis

Der Odenwaldkreis ist ein Landkreis im Regierungsbezirk Darmstadt in Hessen. Mit etwas weniger als 100.000 Einwohnern ist er der bevölkerungsärmste Landkreis Hessens.

Der Odenwaldkreis ist der einzige Landkreis, der mit dem gesamten Kreisgebiet im Odenwald liegt, somit dessen Kerngebiet umfasst. Zentraler Landschaftsteil des Odenwaldkreises ist das in Süd-Nord-Richtung verlaufende Mümlingtal und die dieses östlich und westlich begleitenden Höhenzüge. Entlang der Mümling sind die größten Städte des Kreises aufgereiht: Beerfelden, Erbach, Michelstadt, Bad König und Höchst. Im Nordwesten reicht das Kreisgebiet in einen Teil der Gersprenzniederung hinein und im Süden gibt es jenseits der Hauptwasserscheide von Main und Neckar einige Täler, die nach Süden dem Neckar zustreben: der Finkenbach, der Gammelsbach, der Sensbach und die Itter.

Die höchsten Erhebungen des Odenwaldkreises sind der Kohlwald () nordwestlich von Bullau, die Sensbacher Höhe () und der Krähberg (), beide in der Gemarkung Ober-Sensbach. Würzberg und Bullau sind mit 515 Meter die beiden höchstgelegenen Ortschaften des Kreises. Die tiefstgelegene ist Hainstadt an der Mümling kurz vor der bayerischen Grenze mit .

Der Odenwaldkreis grenzt im Uhrzeigersinn im Norden beginnend an die Landkreise Darmstadt-Dieburg (in Hessen), Miltenberg (in Bayern), Neckar-Odenwald-Kreis und Rhein-Neckar-Kreis (beide in Baden-Württemberg) sowie Bergstraße (wiederum in Hessen).

Bis zur Mediatisierung 1806 war das Gebiet des Odenwaldkreises, in ähnlichen Grenzen wie seit der Kreisreform, ein reichsunmittelbares Territorium der Grafen von Erbach, gehörte als Exklave zum Fränkischen Reichskreis und kam dann zum Großherzogtum Hessen. Nach Verkündung der Verfassung des Großherzogtums am 17. Dezember 1820 folgte am 14. Juli 1821 eine umfassende Verwaltungsreform. Statt der Ämter wurden nun Landratsbezirke eingesetzt. Diese waren die Vorläufer der Kreise.
In Erbach wurde 1822 für die Gräflich Erbachischen und Erbach-Fürstenauischen Ämter der Landrats- und Gerichtsbezirk Erbach gegründet. Ebenfalls 1822 entstand ein Landrats- und Gerichtsbezirk Breuberg mit Sitz in Neustadt (ab 1847) bzw. Höchst im Odenwald. Mit Gesetz vom 31. Juli 1848 wurden die Verwaltungseinheiten ein weiteres Mal vergrößert. An die Stelle der Kreise und Landratsbezirke traten Regierungsbezirke, wobei die bisherigen Landratsbezirke Breuberg und Erbach zum Regierungsbezirk Erbach vereinigt wurden. Hinzu kam der Landratsbezirk Wimpfen, eine Exklave in Baden. Bereits vier Jahre später kehrte man aber zur Einteilung in Kreise zurück, wodurch neben dem Kreis Erbach, der Kreis Neustadt und der Kreis Wimpfen gegründet wurden. Am 1. Juli 1874 wurde der Kreis Neustadt wieder aufgelöst und der größte Teil der Gemeinden wurden in den Kreis Erbach integriert. Ebenso wurde der Kreis Lindenfels aufgelöst und gab 16 Gemeinden an den Kreis Erbach ab.

Im Zuge der 1874 im Großherzogtum Hessen nach preußischem Vorbild vorgenommenen Reform der Kreisverfassung kam es auch zu einer neuen Kreiseinteilung. Die damals geschaffene Gliederung des Großherzogtums in sieben die Provinz Starkenburg bildende Kreise (Bensheim, Darmstadt, Dieburg, Erbach, Groß-Gerau, Heppenheim, Offenbach) hatte mehr als sechs Jahrzehnte Bestand.

Nach der 1936 erfolgten Auflösung der Provinzial- und Kreistage im nunmehrigen Volksstaat Hessen (ab 1918) und der 1937 durchgeführten Aufhebung der drei Provinzen Starkenburg, Oberhessen und Rheinhessen brachte das Jahr 1938 eine Überprüfung der Kreisgrenzen. Am 1. November 1938 wurde in Hessen eine einschneidende Gebietsreform durchgeführt. In jeder der drei hessischen Provinzen Starkenburg, Rheinhessen und Oberhessen wurde jeweils ein Kreis aufgelöst. In Starkenburg war davon der Kreis Bensheim betroffen. Dieser wurde zum größten Teil dem Kreis Heppenheim zugeschlagen, der auch zum Rechtsnachfolger des Kreises Bensheim wurde. Die neue Verwaltungseinheit wurde in Landkreis Bergstraße umbenannt. Gleichzeitig wurden die Städte Darmstadt, Gießen, Mainz, Offenbach und Worms als Stadtkreise verselbständigt. Diese so geschaffene Kreiseinteilung des Volksstaates hatte zunächst bis zum Kriegsende 1945 Bestand.

Im Rahmen der Gebietsreform in Hessen kam am 1. Juli 1971 die Gemeinde Laudenau aus dem Kreis Bergstraße nach einer Bürgerbefragung zum Landkreis Erbach, als sie sich der Gemeinde Reichelsheim (Odenwald) anschloss. Mit Inkrafttreten des "Gesetzes zur Neugliederung des Landkreises Erbach" vergrößerte sich das Kreisgebiet am 1. August 1972 um die Gemeinden Fränkisch-Crumbach und Brensbach (mit Wersau) aus dem Landkreis Dieburg. Zugleich erhielt der Kreis den Namen "Odenwaldkreis".

Am 1. Januar 2018 schlossen sich die Gemeinden Beerfelden, Hesseneck, Rothenberg und Sensbachtal zur neuen Stadt Oberzent zusammen.

Die Kommunalwahl am 6. März 2016 lieferte folgendes Ergebnis, in Vergleich gesetzt zu früheren Kommunalwahlen:


Der Odenwaldkreis führt ein Wappen sowie eine Hiss- und Bannerflagge.

Das Gebäude des Landratsamtes wurde 1902–1904 vom Wormser Stadtbaumeister und späteren Professor der Technischen Hochschule Darmstadt Karl Christian Hofmann (1856–1933) im Renaissancestil entworfen und vom Bauunternehmer und Steinmetz Adam Hild aus Hetzbach errichtet. Erweiterungsbauten wurden 1960 und 1989 notwendig.

An dieser Stelle befand sich ehemals die Gerichtsstätte der Erbacher Zent und danach eine Ziegelhütte.

Der Odenwaldkreis unterhält eine Partnerschaft mit der Region Falkirk, die im Zentrum Schottlands zwischen Edinburgh und Glasgow liegt.


Am 1. Juli 1956 wurde dem Vorgänger des Odenwaldkreises, dem Landkreis Erbach, das Unterscheidungszeichen "ERB" zugewiesen. Dieses wurde auch nach der am 1. August 1972 erfolgten Umbenennung in Odenwaldkreis weiter benutzt und wird durchgängig bis heute ausgegeben.




</doc>
<doc id="3752" url="https://de.wikipedia.org/wiki?curid=3752" title="Off-Screen">
Off-Screen

Off-Screen bezeichnet



</doc>
<doc id="3753" url="https://de.wikipedia.org/wiki?curid=3753" title="Off camera">
Off camera

Off camera (kurz "Off", „außerhalb [des Blickfelds einer Kamera]“) ist ein Fachausdruck bei der Bildregie in audiovisuellen Medien und bei der Filmvertonung. Im Englischen wird er auch allgemein für alle Aspekte einer Filmproduktion verwendet, die nicht vor der Kamera stattfinden, analog zu Backstage im Theater („hinter den Kulissen“).

Im Off befinden sich akustische Ereignisse, deren Quelle nicht von der Kamera gezeigt werden, zum Beispiel Stimmen von Dialogpartnern, die in einer Kameraeinstellung nicht zu sehen sind ("Off-Stimme"). Eine grundsätzliche Unterscheidung dabei ist, ob dies zur Handlung gehört oder nicht (siehe Diegese): Die Stimme einer Figur aus dem Off, die in der Szene anwesend, aber im Moment gerade nicht sichtbar ist, hat eine andere Funktion als eine Erzählstimme aus dem Off, die das Geschehen aus zeitlicher und örtlicher Distanz kommentiert (Voice-over). 

Zur Handlung gehört es, wenn zum Beispiel jemand durch ein offenes Fenster in die Szene ruft, nicht zur Handlung gehört es dagegen, wenn über eine Filmsequenz später eine Übersetzerstimme gelegt wird. Auch Dialogpartner im Spielfilm, die nicht sichtbar sind, werden möglichst von Schauspielern dargestellt, die bei der Aufnahme anwesend sind, weil das Spiel der sichtbaren Figuren dann lebendiger wirkt, auch wenn die ganze Szene nachvertont wird.

Auch Annahmen des Zuschauers, zum Beispiel über Beziehungen zwischen Protagonisten, die sich aus der tatsächlich dargebotenen Handlung ableiten lassen (Implikation), können als "Off camera" bezeichnet werden (siehe Fokalisierung). Manchmal werden solche Vermutungen durch Filmmusik, Kameraeinstellungen und andere Kommentare von der Beobachterperspektive des Films aus suggeriert.

Die Off-Stimme als Voice-over dient dazu, einen Film mit Erklärungen oder Erzählungen zu ergänzen. Off-Sprecher sind meist gut ausgebildet. Statt neutraler Perfektion können je nach Einsatzgebiet auch besonders charakteristische Stimmen mit sprachlichen Unvollkommenheiten, wie etwa rauchige, besonders alt oder jung klingende Stimmen mit dialektalen Färbungen gefragt sein, oder es werden bekannte Stimmen eingesetzt, die die (Werbe-)Wirksamkeit erhöhen. 

Eine Synchronstimme dagegen ist keine Off-Stimme, weil sie scheinbar zur Figur im Bild gehört.




</doc>
<doc id="3754" url="https://de.wikipedia.org/wiki?curid=3754" title="Orchideen">
Orchideen

Die Orchideen oder Orchideengewächse (Orchidaceae) sind eine weltweit verbreitete Pflanzenfamilie. Die zwei hodenförmigen Wurzelknollen der Knabenkräuter (v. griech. ὄρχις "orchis" ‚Hoden‘) haben der gesamten Pflanzenfamilie ihren Namen gegeben. Nach den Korbblütlern (Asteraceae) stellen die Orchideen die zweitgrößte Familie unter den bedecktsamigen Blütenpflanzen dar. Sie werden als besonders schön angesehen, und vielen gilt die Orchidee als "Königin der Blumen". Sie gehören innerhalb der Klasse der Bedecktsamer zu den Einkeimblättrigen Pflanzen (Monokotyledonen). Etwa 1000 Gattungen mit 15.000 bis 30.000 Arten werden von den Botanikern anerkannt.

Die Pflanzentaxa der Familie Orchideen unterscheiden sich nur durch einige wenige eindeutige Merkmale von anderen verwandten Pflanzenfamilien der Einkeimblättrigen Pflanzen. Dabei gibt es trotz der vielfachen Merkmale, die bei den meisten Orchideenarten zu finden sind, nur sehr wenige, die bei allen vorkommen.
Die Orchideen weisen folgende spezifische Merkmale auf:

Orchideen sind in der Regel ausdauernde Pflanzen, könnten theoretisch je nach Wuchsform unbegrenzt lange weiterwachsen (jedes Jahr ein oder mehrere Neutriebe oder permanentes Weiterwachsen eines Sprosses). Tatsächlich ist aber nur sehr wenig darüber bekannt, welches Alter Orchideen erreichen können.

Orchideen können auf verschiedene Art und Weise wachsen. Man unterscheidet dabei folgende Formen:

Mehr als die Hälfte aller tropischen Arten wachsen als Epiphyten auf Bäumen. Sie besitzen spezielle morphologische (Velamen radicum, Pseudobulben) und physiologische (CAM-Mechanismus) Besonderheiten, um mit den teilweise widrigen Bedingungen wie Trockenheit und Nährstoffmangel im Kronenraum zurechtzukommen.

Man unterscheidet monopodial wachsende Orchideen, die eine an der Spitze weiterwachsende einheitliche Sprossachse besitzen (teilweise auch mit Verzweigungen) und sympodial wachsende Orchideen, die durch Verzweigung nacheinanderfolgende Sprossglieder mit begrenztem Spitzenwachstum ausbilden. Bei den monopodial wachsenden Orchideen dienen Blätter und/oder Wurzeln als Speicherorgane, während die sympodial wachsenden Orchideen dazu mehr oder weniger dicke ein- oder mehrgliedrige Pseudobulben ausbilden. Einige Orchideengattungen bilden auch unterirdische Speicherorgane (Kormus). Neben den beiden angeführten Wachstumsformen gibt es aber auch seltene Abwandlungen, die nicht dem normalen Schema monopodialen vs. sympodialen Wachstums entsprechen. So bilden etwa viele Arten der Pleurothallidinae (Bsp. "Pleurothallis", "Lepanthes") trotz sympodialen Wuchses keine Pseudobulben aus, sondern haben stattdessen fleischige Blätter.
Orchideen bilden keine Primärwurzel (Pfahlwurzel) aus, sondern nur sekundäre Wurzeln, die dem Spross entspringen. In ihrer Dicke unterscheiden sie sich teilweise ziemlich deutlich. Beim überwiegenden Teil der Orchideen weisen die Wurzel ein Velamen auf. Neben ihrer Funktion als Aufnahmeorgan für Wasser und Nährstoffe dienen sie oft auch als Haft- und Halteorgan. Dies ist besonders bei epiphytisch wachsenden Arten von Bedeutung. Die Form der Wurzeln hängt im Wesentlichen davon ab, wo sie wachsen. Während die frei in der Luft hängenden Wurzeln der Epiphyten bzw. die Wurzeln, die völlig in den Boden wachsen, meist zylindrisch sind, weisen die Haft- und Haltewurzeln, die auf den Oberflächen wachsen, eine eher abgeflachte Form auf. Bei einigen Arten sind die Wurzeln chlorophylltragend, um auch während klimatisch bedingtem Blattabwurf weiterhin Nährstoffe verarbeiten zu können. Die Wurzeln der Orchideen verzweigen eher selten. Sie haben eine Lebensdauer, die von verschiedenen Umweltfaktoren abhängt und kürzer ist als die des Sprosses. Die Neubildung von Wurzeln erfolgt in der Regel mit dem Wachstum des neuen Sprosses zum Ende der Vegetationsperioden oder auch während der Wachstumsphase. Bei vielen terrestrischen Orchideenarten bilden sich an den Wurzeln Speicherorgane oder knollenähnliche Gebilde.
Bei einigen Gattungen ist es möglich, dass sich an den Wurzeln Adventivknospen bilden, aus denen neue Sprosse entstehen.

Neben der Mykorrhiza, die für die embryonale Entwicklung aus einem Samen notwendig ist, gibt es auch in den Wurzeln Mykorrhiza. Dabei wachsen die Pilzfäden in die äußeren oder unteren Zellschichten der Wurzeln oder Rhizomen. Die Orchideen nehmen auch in diesem Fall durch Verdauung von Pilzteilen oder -ausscheidungen Nährstoffe auf. Da der Pilz, der das Protokorm (Keimknöllchen) befällt, in der Regel nicht mit den neuen Wurzeln nach außen wächst, muss die Mykorrhiza jedes Jahr von neuem (mit der Bildung neuer Wurzeln) ausgebildet werden. Bei ausreichendem Angebot von Licht und Nährstoffen sind Orchideen in der Regel nicht auf diese Mykorrhiza angewiesen. Ausnahmen sind die myko-heterotroph lebenden Orchideen.

Der überwiegende Teil der Orchideen besitzt parallelnervige Blätter mit kaum sichtbaren Querverbindungen. Sie sitzen in der Regel zweireihig, abwechselnd an den entgegengesetzten Seiten des Sprosses. Viele Orchideen bilden nur ein einziges richtiges Blatt aus, die Anlagen der Blätter sind jedoch ebenfalls zweireihig. Die Form der Blätter und Blattspitzen, die Festigkeit, die Färbung und der Blattaufbau variieren sehr stark.


Viele Arten verlieren klimatisch bedingt ihre Blätter, um sie zu Beginn des nächsten Vegetationszyklus neu auszubilden. Während bei dem überwiegenden Teil dieser Arten die Blätter tatsächlich nur einjährig sind, gibt es ebenso Arten, die ihre Blätter nur unter widrigen Standortbedingungen abwerfen bzw. unter günstigen Bedingungen behalten. Es gibt aber auch Arten, die völlig blattlos wachsen ("Dendrophylax lindenii"). Dafür besitzen sie chlorophylltragende Wurzeln.

Die Blütenstände der Orchideen sind in der Regel traubenförmig, an denen sich je nach Art bis zu hundert und mehr Blüten ausbilden können. Wachsen verzweigte Blütenstände (rispenförmig), so ist die Traubenform jeweils an den äußersten Zweigen zu finden. Neben den trauben- oder rispenförmigen Blütentrieben gibt es aber auch eine Vielzahl von Orchideen, die nur einblütig sind. Bei einigen Arten bilden sich nacheinander mehrere Blüten an demselben Blütentrieb, wobei jedoch nie mehr als eine Blüte geöffnet ist (z. B. "Psychopsis papilio"). Die Blütenstände können an jeder Stelle des Sprosses der Orchidee entspringen. Dabei wird zwischen endständigen (terminal (an der Triebspitze), apikal (zentral am Triebansatz)) und seitenständigen (lateral) Blütenständen unterschieden. Meist entspringen die Blütentriebe einer Blattachsel. Aufgrund der Wuchsrichtung sind die Blütenstände der monopodialen Orchideen immer seitenständig. Die einzelnen Blüten werden stets von einer Braktee (Tragblatt) gestützt, die meist unauffällig ist.

Keine andere Pflanzenfamilie hat ein solches Spektrum, was Formen und Farben der Blüten anbelangt, wie die Familie der Orchideen. Die Größe der Blüten variiert von einigen Millimetern (Beispiel "Lepanthes calodictyon") bis zu 20 Zentimetern und mehr pro Blüte (Beispiel "Paphiopedilum hangianum"). Das Farbspektrum reicht dabei von zartem Weiß über Grün- und Blautöne bis zu kräftigen Rot- und Gelbtönen. Viele der Orchideenblüten sind mehrfarbig.

Außer bei einigen Gattungen (zum Beispiel "Catasetum") sind die dreizähligen Blüten der Orchideen zwittrig. Die Blütenhülle (Perianth) besteht aus zwei Kreisen. Es gibt einen äußeren Hüllblattkreis, der aus drei Kelchblättern (Sepalen) besteht und einen inneren Hüllblattkreis, der aus drei Kronblättern (Petalen) besteht. Die Blütenblätter können frei oder zu einem gewissen Grad miteinander verwachsen sein. Bei einigen Orchideengattungen, so etwa in der Unterfamilie Cypripedioideae oder bei "Acriopsis", sind die unteren beiden Sepalen komplett verwachsen. Das in der Symmetrieachse gelegene Blütenhüllblatt des inneren Hüllkreises ist in der Regel deutlich abweichend was Größe, Farbe und Form betrifft. Es bildet die Lippe (Labellum) der Orchideenblüte. Bei vielen Orchideen ist die Lippe auf der Rückseite zu einem schlauchigen bis sackigen Gebilde verlängert, dem so genannten Sporn (Beispiele "Aeranthes", "Aerangis"). In ihm befindet sich entweder Nektar oder er ist leer. Andere Arten bilden aus der Lippe einen „Schuh“ (zum Beispiel die Gattungen der Unterfamilien Cypripedioideae). Außerdem sind Säule (Gynostemium) und der Fruchtknoten wesentliche Bestandteile der Blüten. Im Grundaufbau unterscheidet man monandrische (ein fertiles Staubblatt, Beispiele "Cattleya", "Phalaenopsis") und diandrische (zwei fertile Staubblätter, Beispiel "Paphiopedilum", "Cypripedium") Orchideen. Der Fruchtknoten ist bei Orchideen unterständig. Die anderen Blütenteile (Sepalen und Petalen, Säule, Lippe) sind mit diesem vollständig verwachsen und stehen über ihm. In der Regel ist der Fruchtknoten nur sehr schmal und schwillt erst nach der Bestäubung an (Ausbildung der Samenkapsel).
Die Blüten der Orchideen sind mit Ausnahme einiger Gattungen (Beispiel "Cycnoches", "Mormodes") bilateral-symmetrisch (zygomorph). Das heißt, dass man durch die Mitte der Blüte eine Spiegelachse legen kann, und zwar nur eine einzige (monosymmetrisch).

Eine zentrale Rolle in der Fortpflanzung von Orchideen spielen die besonderen Pollenanhäufungen. Die von den Staubblättern gebildeten Pollen sind zu zwei lockeren oder festen Bündeln verklebt (Pollinien). Diese beiden Klumpen hängen auf einem mehr oder weniger langen Schaft mit einer Klebscheibe (Viscidium), sie haftet an dem Bestäuber durch eine Flüssigkeit aus der Klebdrüse (Rostellum).

Fast alle Orchideenfrüchte sind Kapseln. Sie unterscheiden sich in Größe, Form und Farbe deutlich. Epiphyten besitzen eher dickere Früchte mit fleischigen Wänden, terrestrische Arten oft dünnwandige trockene Früchte. Es gibt dreieckige, rundliche mit einer mehr (bis 9) oder weniger (bis 3) großen Anzahl von Rippen oder auch geschnäbelte Früchte. Manche sind behaart oder stachelig oder besitzen eine warzige Oberfläche. Die Früchte entwickeln sich aus dem bereits im Knospenstadium am Boden der Blüte vorgebildeten Fruchtknoten, welcher aus drei Fruchtblättern besteht. Bei eintretender Reife platzen die meisten Orchideenfrüchte der Länge nach auf, ohne sich an der Spitze vollständig zu trennen. Dabei bilden sich in der Regel drei oder sechs Längsspalten, bei manchen auch nur eine oder zwei. Fast immer werden die Samen dabei trocken verstreut.

Orchideen können auf unterschiedliche Weise vermehrt werden. Es gibt die Vermehrung durch Samen als auch die vegetative Vermehrung. Unter künstlichen Bedingungen ist auch die Vermehrung durch Meristeme möglich.

Fast alle Orchideen haben winzige Samen. Jede Pflanze produziert Hunderttausende bis Millionen von Samen in einer Samenkapsel. Durch ihre geringe Größe sind die Samen von Orchideen nur noch auf eine Hülle und den in ihr liegenden Embryo reduziert. Im Gegensatz zu anderen Samen fehlt ihnen das Nährgewebe oder Endosperm, das für eine erfolgreiche Keimung nötig ist. Nur bei wenigen Gattungen ist dieses noch vorhanden (z. B. Bletilla). Orchideen sind deshalb auf eine Symbiose mit Pilzen angewiesen. Bei diesem als Mykorrhiza bezeichneten Vorgang wird der mit der Keimung beginnende Embryo durch das Eindringen von Pilzfäden in den Samen infiziert. Der Embryo bezieht über diese Verbindung Nährstoffe (Mykotrophie), indem er Teile des Pilzkörpers oder Ausscheidungen des Pilzes verdaut. Sobald der Sämling zur Photosynthese fähig ist, übernimmt diese die Versorgung der Pflanze mit Nährstoffen und die Mykotrophie ist zur weiteren Entwicklung nicht mehr notwendig. Es gibt aber einige Orchideenarten, die aufgrund des fehlenden oder nur in unzureichenden Mengen vorhandenen Chlorophylls zeitlebens auf die Mykotrophie angewiesen sind (Bsp. Korallenwurz). Dies betrifft alle vollkommen myko-heterotroph lebenden Arten.

Während der überwiegende Teil der Orchideen trockene Samen verstreut, gibt es einige Gattungen (Bsp. "Vanilla"), bei denen die Samen von einer feuchten Masse umgeben sind.

Die Bestäubung der Orchideen erfolgt in der Natur hauptsächlich durch Insekten (z. B. Ameisen, Käfer, Fliegen, Bienen, Schmetterlinge), aber auch durch Vögel (z. B. Kolibris), Fledermäuse oder Frösche. Dabei haben sich teilweise Art-Art-Bindungen (z. B. "Drakea glyptodon" und "Zapilothynus trilobatus" oder die einheimische "Orchis papilionacea" und "Eucera tuberculata") oder Gattungs-Gattungs-Bindungen (z. B. wird die Orchideengattung "Chloraea" von Bienen der Gattung "Colletes" bestäubt) herausgebildet. Diese Spezialisierung ist in der Regel nur einseitig, da keine Insektenart auf die Bestäubung einer einzigen Orchideenart beschränkt ist. Innerhalb der Familie gibt es aber auch einige Gattungen, bei denen sich einige oder alle Arten auf asexuellem Weg durch Selbstbestäubung fortpflanzen. Dazu zählen unter anderem die Gattungen "Apostasia", "Wullschlaegelia", "Epipogium" und "Aphyllorchis". Von der Art "Microtis parviflora" ist bekannt, dass sie sich ebenfalls selbstbestäuben kann, wenn die Bestäubung durch Ameisen ausbleibt. Die Bestäuber sind bei einer Vielzahl von Orchideengattungen jedoch unbekannt oder nur wenig erforscht.

Orchideen sind in der Regel nicht selbststeril.

In der Natur entstehen teilweise durch die Bestäuber Hybriden zwischen zwei verwandten Arten (seltener über Gattungsgrenzen hinweg), diese werden "Naturhybriden" genannt.

Im Vergleich zu anderen Blütenpflanzen fällt auf, dass beispielsweise nicht-tropische Orchideen häufig keine Belohnung in Form von Nahrung anbieten, sondern ihr Ziel durch Mimikry oder Täuschung erreichen. Werden Belohnungen angeboten, bestehen diese oft nicht aus Nahrung, sondern aus Duftstoffen (zum Beispiel Sexuallockstoffe für Insekten wie es bei manchen Wespenarten der Fall ist) oder Wachs.

Durch die evolutionäre Entwicklung verschiedener Blütenformen ergab sich eine zunehmende Spezialisierung auf bestimmte Bestäubergruppen und somit auch auf die Art und Weise, wie die Blüten bestäubt werden. Im Folgenden werden einige Bestäubungssysteme und -mechanismen erläutert.


Die Pollen sind bei Orchideen zu Pollinien mit angehefteten Viscidien (Viscidium = Klebscheibe, Klebkörper) zusammengeballt (eine Ausnahme bilden dabei beispielsweise die Cypripedioideae). Dies ermöglicht es, die Pollenpakete exakt zu positionieren, so dass es möglich ist, dass an einem Bestäuber die Pollinien verschiedener Arten befestigt werden können, ohne dass es zu falschen Bestäubungen kommt. An verschiedenen Bienenarten (Euglossinae) konnten bis zu 13 Anheftungsstellen festgestellt werden. Im Gegensatz zu anderen Blütenpflanzen dient der Orchideenpollen nicht als Nahrung.

Eine ungewöhnliche Bestäubungstechnik wendet die epiphytisch lebende chinesische Orchideenart "Holcoglossum amesianum" an: die Antherenkappe öffnet sich und die männlichen Staubfaden drehen sich aktiv und ohne jedes Hilfsmittel um fast 360 Grad in Richtung der weiblichen Narbe. Die an dem biegsamen Staubfaden befestigten Pollenkörner werden anschließend bei Berührung der Narbe freigegeben, so dass eine Selbstbefruchtung erfolgen kann. Es wird vermutet, dass es sich bei dieser Technik um eine Anpassung der Orchidee an ihren trockenen und insektenarmen Lebensraum handelt, die womöglich bei Pflanzen vergleichbarer Biotope gar nicht so selten ist (Quelle). Die bereits bekannte Selbstbestäubung der Bienen-Ragwurz ("Ophrys apifera") folgt einem ähnlichen Schema.

Orchideen und Prachtbienen: Die bestuntersuchten Blumendüfte sind die von "Stanhopea" und "Catasetum", die durchdringend nach Ananas, Vanille, Zimt, Kümmel oder Menthol riechen und Prachtbienenmännchen anziehen, wobei diese die Blüten weder bestäuben noch angreifen, sondern lediglich das von der Pflanze produzierte Öl einsammeln und für ihre Balz benutzen wollen. Es gibt sowohl unzählige Prachtbienen- als auch jeweils dazugehörige Orchideen-Arten.

Verschiedene Arten haben die Möglichkeit, sich durch die Bildung von Stolonen (Bsp. "Mexipedium xerophyticum"), Knollen (Bsp. "Pleionen") oder Kindeln (Adventiv-Pflanzen; Bsp. "Phalaenopsis lueddemanniana") auf vegetativem Weg fortzupflanzen. Die entstehenden Pflanzen sind genetisch identisch.

Die Vermehrung über Meristeme erfolgt vor allem im Erwerbsgartenbau zur Erzeugung großer Mengen von Orchideen für den Schnitt als auch zum Verkauf als Topfpflanze, welche man häufig in Pflanzencentern oder Baumärkten erwerben kann. Große Produzenten findet man vor allem in den Niederlanden oder in Thailand. Außerdem ist es die einzige Möglichkeit, von bestimmten Klonen, beispielsweise prämierten Pflanzen, identische Nachkommen in großen Mengen zu erzeugen, die auch den gleichen Kultivarnamen tragen dürfen. Im Erwerbsgartenbau geht man bei der Massenvermehrung aber immer mehr dazu über, mittels In-vitro-Aussaat von Orchideensamen und Clusterbildung durch Hormongaben den Bedarf zu decken.

Orchideen wachsen mit Ausnahme der Antarktis auf jedem Kontinent.
Aufgrund ihrer enormen Vielfalt gibt es Orchideen fast in jeder Ökozone (nicht in Wüsten). Selbst oberhalb des nördlichen Polarkreises oder in Patagonien und den dem ewigen Eis des Südpols vorgelagerten Inseln, z. B. Macquarie Island gibt es Orchideen. Der Großteil der Arten wächst allerdings in den Tropen und Subtropen, hauptsächlich in Südamerika und Asien. In Europa gibt es etwa 250 Arten.

Einen groben Überblick über die Häufigkeit auf den einzelnen Kontinenten bietet die folgende Auflistung:

In den Anfängen der botanischen Systematik finden sich bei Linné 1753 acht Gattungen, die zu den Orchideen gehören. Jussieu fasste sie 1789 erstmals als Familie "Orchidaceae" zusammen. In der Folge wurden rasch sehr viele tropische Arten bekannt; so unterschied Swartz im Jahr 1800 schon 25 Gattungen, von denen er selbst zehn neu aufstellte. Swartz publizierte im selben Jahr eine Monographie der Familie und gilt als einer der ersten Spezialisten für die Systematik der Orchideen.

Im 19. Jahrhundert erschienen, bedingt durch die Kenntnis immer neuer tropischer Orchideen, weitere wichtige Arbeiten. Lindley veröffentlichte von 1830 bis 1840 "The Genera and Species of Orchidaceaous Plants" mit fast 2000 Arten und einer wegweisenden Einteilung in Unterfamilien und Triben. In England erschien 1881 Benthams Systematik, die auch in seinem zusammen mit Hooker herausgegebenen Werk "Genera Plantarum" verwendet wurde. Am Heidelberger botanischen Garten entstand 1887 Pfitzers "Entwurf einer natürlichen Anordnung der Orchideen". 1926 erschien posthum Schlechters Arbeit "Das System der Orchidaceen" mit 610 Gattungen; es wurde für die nächsten Jahrzehnte das Standardwerk.

Im 20. Jahrhundert waren die Publikationen Dresslers einflussreich, vor allem "The Orchids. Natural History and Classification" von 1981. Die weitere Entwicklung verläuft über die kladistische Analyse äußerer Merkmale zur Auswertung genetischer Untersuchungen, die zahlreich etwa von Mark W. Chase publiziert wurden.

Aus phylogenetischer Sicht existieren die fünf primären monophyletische Linien Apostasioideae, Cypripedioideae, Vanilloideae, Orchidoideae und Epidendroideae, deren Verwandtschaftsverhältnisse in einem Kladogramm wie folgt dargestellt werden können:
Danach gibt es keine genetischen Anhaltspunkte für die Existenz der Unterfamilien Vandoideae oder Spiranthoideae. Die Unterfamilie Vandoideae ist nach diesen Untersuchungen ein Bestandteil innerhalb der Epidendroideae, die Spiranthoideae ein Bestandteil der Orchidoideae. Die separate Unterfamilie Vanilloideae war „klassisch“ Bestandteil der Epidendroideae.

Innerhalb der einkeimblättrigen Pflanzen werden die Orchideen in die Ordnung der Spargelartigen (Asparagales) gestellt. Auf Basis äußerer Merkmale ließ sich die Frage nach den nächsten Verwandten der Orchideen nur unsicher beantworten, Alstroemeriaceae, Philesiaceae oder Convallariaceae wurden vermutet, auch eine Einordnung in die Ordnung der Lilienartigen (Liliales) schien möglich. Genetische Untersuchungen bestätigten die Zuordnung zu den Spargelartigen und sehen die Orchideen als Schwestergruppe zu allen anderen Spargelartigen, das heißt, sie haben sich schon früh von den anderen Pflanzen dieser Ordnung entfernt.

Die Orchideen wurden häufig als besonders junge Familie angesehen. Anhand eines fossilen Polliniums von "Meliorchis caribea" wurde das Mindestalter des letzten gemeinsamen Vorfahren aller Orchideen auf 76 bis 84 Millionen Jahre bestimmt. Bis zum Ende der Kreidezeit vor 65 Millionen Jahren spalteten sich schon die fünf Unterfamilien auf. Im Tertiär fand eine große Zunahme der Artenvielfalt der Orchideen statt. Nach der Methode der „molekularen Uhr“ datiert der Ursprung der Orchideen noch früher, vor mindestens 100, wenn nicht sogar 122 Millionen Jahren. Es wird angenommen, dass sie sich in einem tropischen Gebiet als erstes entwickelten. Die Verbreitung verschiedener primitiver Orchideen (Bsp. "Vanilla", "Corymborkis") und das Vorkommen der primitiven Gattungen (Bsp. "Cypripedium", "Epistephium") in nahezu allen tropischen Gebieten ist ein Indiz dafür, dass die Entwicklung der Orchideen in einer Zeit begonnen haben muss, in der Afrika und Südamerika enger beieinanderlagen (Kontinentaldrift). Der Hauptteil der Evolution der Orchideen hat allerdings erst begonnen, als sich die wichtigsten tropischen Regionen schon weiter voneinander entfernt hatten.

Die epiphytische Lebensweise vieler Orchideen, vor allem der tropisch und subtropischen Arten, ist das Resultat einer evolutionären Anpassung an verschiedene Bedingungen. Periodisch trockenes Klima oder gut entwässerte Standorte, die bereits zur Entstehung der Orchideen vorhandene Neigung zur Insektenbestäubung sowie der zumindest kurzzeitige Zyklus einer myko-heterotrophen Lebensweise und der damit einhergehenden Entwicklung von kleinen Samen scheinen wesentliche Faktoren gewesen zu sein, dass Orchideen Bäume besiedelten. Andererseits scheint auch die Ausbildung von fleischigen Wurzeln mit Velamen oder von fleischigen Blättern als Anpassung an die periodisch trockenen Standortbedingungen eine Voraussetzung oder eine Möglichkeit gewesen zu sein, von Felsen oder anderen gut entwässerten Standorten auf Bäume überzusiedeln. Ob dabei der Weg über Humusepiphyten und anschließende Besiedlung der ökologischen Nischen in den Baumkronen oder die direkte Besiedlung der Bäume erfolgte, konnte bis heute nicht geklärt werden.

Bei der Wuchsform der Orchideen geht man davon aus, dass sich die Vielfalt der heutigen Orchideen aus einer sehr primitiven Form entwickelt hat, die man noch ansatzweise in fast allen Unterfamilien findet. So werden die ersten Orchideen einen sympodialen Wuchs mit schmalen Rhizomen, fleischigen Wurzeln (keine Speicherorgane), gefaltete Blätter und endständige Blütenstände besessen haben. Aufgrund der fehlenden Fossilien lässt sich nur schwer ableiten, auf welchem Weg sich die verschiedenen Wuchsformen herausgebildet haben und welches die Hauptrichtungen der Wuchsevolution sind. Ähnlich verhält es sich bei der evolutionären Entwicklung der verschiedenen Blütenformen. Es wird davon ausgegangen, dass die Entwicklung und Anpassung der Blüten vor allem mit den bestäubenden Insekten in Verbindung zu bringen ist. Am Anfang stand sicherlich eine lilienähnliche Blüte, die nach und nach ihre ventralen Staubbeutel verloren hat. Dies hängt wahrscheinlich mit der Art zusammen, wie die Bestäuber in die röhrenförmige Blüte eingedrungen sind. Dabei konnten wohl nur die dorsalen Staubbeutel ihre Pollen an eine für die Bestäubung sinnvolle Position heften. Die Ausbildung der Lippe resultierte ziemlich wahrscheinlich daraus, dass die Insekten immer wieder auf die gleiche Art und Weise auf den Blüten „gelandet“ sind. Vermutlich konnten Pflanzen mit lippenförmigem unterem Petalum (medianes Blütenhüllblatt des inneren Blütenhüllblattkreises) die jeweiligen Bestäuber besser unterstützen, was ein entscheidender Vorteil in ihrer Evolution gewesen sein dürfte.

Siehe auch: Liste der Orchideengattungen

Die Schätzungen über die Artenzahl der Orchideen reichen von 15.000 bis 35.000. Govaerts, der für die Kew Gardens eine Checkliste führt, stellte 2005 einen Stand von 25.158 Arten in 859 Gattungen fest. Im Zeitraum von 1990 bis 2000 wurden pro Jahr 200 bis 500 neue Arten beschrieben. Die artenreichsten Gattungen besitzen eine hauptsächlich tropische Verbreitung, dies sind:

In der gemäßigten Zone ist die Artenvielfalt geringer, je etwa 250 Arten sind in Europa, Ostasien und Nordamerika verbreitet. Gattungen der gemäßigten Zone sind unter anderen:

Nur für die wenigsten Gattungen liegen gesicherte Informationen über die Stärke der Populationen vor. Trotzdem muss davon ausgegangen werden, dass die Bestände vieler Arten in der Natur stark gefährdet sind. Dies gilt für die Habitate in allen Regionen der Welt. Vor allem die Abholzung der Regenwälder oder die landwirtschaftliche Nutzung von Gebieten mit Orchideenhabitaten reduzieren die Bestände stetig. Zusätzlich werden sie durch das unkontrollierte Sammeln gefährdet.
Zum Schutz der Pflanzen wurden Vorschriften erlassen, die den Handel und den Umgang mit ihnen regeln. Alle Orchideenarten stehen mindestens im Anhang II des Washingtoner Artenschutz-Übereinkommens (WA). Folgende Gattungen und Arten stehen aufgrund besonders umfangreicher Aufsammlungen in der Vergangenheit und/oder der Gegenwart auf dem Anhang I und unterliegen somit noch strengeren Auflagen:

Der Rückgang vieler europäischer Arten ist auch auf eine veränderte ländliche Bewirtschaftung zurückzuführen. Durch den enormen Rückgang der Beweidung (Schafe usw.), vor allem in Mitteleuropa, gehen die durch menschlichen Eingriff entstandenen Habitate (Trockenrasen) in ihren ursprünglichen, wäldlichen Zustand zurück. Orchideenarten, die auf Trockenrasen wachsen, treten in diesen Wäldern kaum noch auf. Es gibt Anlass zur Hoffnung, dass mit steigendem Naturschutz-Bewusstsein auch seltenen Orchideen neue Chancen eingeräumt werden. Im Bereich des Lechs wird aktuell versucht Restheiden mit neuen Heiden aus zweiter Hand zu verbinden. So bieten die Lechtalheiden mit ihren speziellen Böden ein wachsendes Rückzugsgebiet für unsere heimischen Orchideenarten.

Siehe auch: Liste bedeutender Orchideenforscher

Orchideen faszinieren und beschäftigen die Menschen schon mehr als 2500 Jahre. Sie wurden als Heilmittel, Dekoration und Aphrodisiakum verwendet oder sie spielten im Aberglauben eine große Rolle. 
Die ältesten Überlieferungen über Orchideen stammen aus dem Kaiserreich China und beziehen sich auf die Kultur von Orchideen aus der Zeit um 500 v. Chr. (Tsui Tsze Kang: "Orchideenkultur im Kum Cheong" (erschienen in der Song-Dynastie 1128–1283)). Der chinesische Philosoph Konfuzius (551–478 v. Chr.) berichtete über ihren Duft und verwendete sie als Schriftzeichen »lán« (), was so viel wie Anmut, Liebe, Reinheit, Eleganz und Schönheit bedeutet. Allgemein gilt die Orchidee in der chinesischen Gartenkunst als Symbol für Liebe und Schönheit oder auch für ein junges Mädchen. Orchideen in der Vase stehen dort für Eintracht. 

Die ersten monographischen Abhandlungen über Orchideen entstanden in China bereits während der Song-Dynastie (Tsui Tsze Kang: "Orchideenkultur im Kum Cheong", Wong Kwei Kok "Die Orchideenkultur des Herrn Wong"). Anhand der Schilderungen in diesen Werken kann man ablesen, dass sich die Orchideenkultur in China damals bereits auf einer hohen Stufe befand.

Auch in Amerika (Mexiko) werden Orchideen schon lange kultiviert. Noch bevor die Spanier das Land eroberten, wurden vor allem die Früchte von „Tlilxochitl“ ("Vanilla planifolia") als Gewürz geschätzt. Die Azteken verehrten »Coatzontecomaxochitl« ("Stanhopea"-Arten) als heilige Blumen und kultivierten diese in den Gärten ihrer Heiligtümer. 

Die ältesten europäischen Überlieferungen, in denen Orchideen erwähnt werden, stammen aus der griechischen Spätklassik von Theophrastus von Lesbos (etwa 372–289 v. Chr.). In seinem Werk "Historia plantarum" beschrieb er eine Pflanze mit zwei unterirdischen Knollen und bezeichnete sie als "orchis", was dem griechischen Wort "ὄρχις" „Hoden“ entspricht. Vermutlich handelte es sich dabei um die Art "Orchis morio". Die älteste erhalten gebliebene Schrift über Orchideen stammt von Pedanios Dioscurides (1. Jh.). Er beschrieb vier Arten (Orchis, anderes Orchis, Satyrion und rotes Satyrion), die nach seinen Angaben botanisch nur schwer zu bestimmen sind. Medizinisch sollten sie wundheilend, stuhlstopfend und gegen Atemnot wirken. Unter Anwendung der Signaturenlehre unterschied Dioscurides bei den rundknolligen Orchideen zwei Arten von Wurzelknollen: die großen diesjährigen und die kleinen letztjährigen. Die großen, von Männern verzehrt, sollten die Geburt von Knaben bewirken, die kleinen, von Frauen genossen, die Geburt von Mädchen. Allgemein sollten die Orchideenwurzeln als Aphrodisiakum wirken. Die Orchideenbeschreibungen bei Plinius (1. Jh.) und Galen (2. Jh.) sowie bei späteren Autoren weichen nur unwesentlich von denen des Dioscurides ab.

Seit dem Spätmittelalter wurden flache, handförmige Orchideenwurzeln von runden Wurzeln unterschieden. Die flachen, handförmigen Wurzeln wurden „palma christi“, „manus christi“, „stendel wurcz das wyblin“ oder „hendel wurcz“ genannt.

Ab der ersten Hälfte des 16. Jh. setzte man sich auch in Europa stärker mit den Orchideen auseinander. So in den Werken der Väter der Botanik, die die bisher bekannten Pflanzen ordneten, indem sie verwandte Arten zusammenstellten, Wuchsformen, Blüten und Wurzelknollen beschrieben.
Mit dem Erscheinen von "Species plantarum" von Carl von Linné (1753) erhielten auch verschiedene Orchideenarten erstmals Namen nach der binären Nomenklatur. Jussieu begründete 1789 mit der Herausgabe des Werkes "Genera Plantarum" die Grundlagen der botanischen Klassifikation und somit auch die Schaffung der "Orchidaceae" als Pflanzenfamilie. Der schwedische Botaniker O. Swartz gliederte 1800 als erster die Orchideenfamilie in zwei verschiedene Gruppen (ein oder zwei fruchtbare Staubblätter). Mit seinem Werk "The Genera and Species of Orchidaceous Plants" (London, 1830 bis 1840) und unzähligen Einzelbearbeitungen wurde J. Lindley zum eigentlichen Begründer der Orchideenkunde. Sein Hauptwerk lag in der Gliederung und Beschreibung von Arten. Seine Arbeiten wurden später durch H. G. Reichenbach (Rchb. f.), J. D. Hooker, R. Schlechter und andere ergänzt, erweitert und zum Teil wesentlich überarbeitet.

Bevor man in Europa begann, aus Übersee tropische Orchideen einzuführen, kultivierte man schon lange Zeit heimische Orchideen in den Gärten. Die erste tropische Orchidee in Europa erblühte 1615 in Holland ("Brassavola nodosa"). 1688 wurde "Disa uniflora" aus Südafrika nach Europa eingeführt. Vor allem durch seine weltweite Vormachtstellung als Kolonialmacht und die daraus resultierenden Verbindungen gelangten viele Arten nach England, wo im 19. Jahrhundert zahlreiche Sammlungen entstanden. Vor allem C. Loddiges war ausgesprochen erfolgreicher Kultivateur. Als 1818 bei W. Cattley die erste "Cattleya labiata" (später als "Cattleya labiata var. autumnalis" bezeichnet) erblühte, war die große lavendelblaue Blüte eine Sensation in Europa und führte zu einem immer stärkeren Bedarf an weiteren tropischen Orchideen. Es wurden immer mehr Sammler und Forschungsreisende (darunter John Gibson, William und Thomas Lobb, D. Burke, J. H. Veitch) in alle Welt geschickt, um neue unbekannte Arten zu finden und diese Pflanzen in die Sammlungen der zahlenden Gärtnereien (zum Beispiel C. Loddiges, J. Linden, F. Sander, L. van Houtte, Veitch and Sons) und Privatpersonen (zum Beispiel W. Cattley, A.L. Keferstein, Senator Jenisch) einzugliedern. Die Anzahl der Importe verringerte sich erst wieder, als die Orchideenzüchtung immer mehr an Bedeutung gewann (Anfang 20. Jahrhundert). Mit dem Beginn der stärkeren wissenschaftlichen Untersuchung der Familie Orchidaceae – unter anderem zur Klärung offener Verwandtschaftsverhältnisse – und dem wachsenden Interesse von Amateuren stieg der Bedarf und das Interesse an den Naturformen wieder. Auch heute noch sind Gärtnereien in aller Welt daran interessiert, Wildformen in ihre Bestände einzugliedern, um durch Einkreuzungen vorhandenes Pflanzenmaterial aufzufrischen. Auch heute werden bisher unbekannte Arten neu entdeckt.
In den letzten Jahrzehnten wurde die Orchideenkultur immer populärer, das Angebot und die Verfügbarkeit von Kulturhybriden wurde größer und so versuchten sich immer mehr Amateure daran, in den heimischen Zimmern, Vitrinen und Gewächshäusern Orchideen zu kultivieren. Heute ist die Kultur dieser Pflanzen nichts Ungewöhnliches mehr. Vor allem der Massenproduktion von Orchideen in Taiwan, Thailand und den Niederlanden ist es zu verdanken, dass die Preise der Pflanzen so gesunken sind, dass eine blühende Orchidee im Topf (zum Beispiel in Deutschland) zum Teil preiswerter ist als ein durchschnittlicher Blumenstrauß. Diese Popularität hat aber auch dazu geführt, dass die Jagd nach dem Besonderen, dem Einzigartigen, dem Besitz besonders hochwertiger Pflanzen wieder aktueller denn je ist. Die Folge ist zum einen, dass für besonders rare Exemplare oder prämierte Pflanzen exorbitante Preise in Japan oder den USA gezahlt werden, und zum anderen, dass aus Geldgier besonders bei neuentdeckten Arten häufig die natürlichen Bestände geplündert werden, nur um die Nachfrage sogenannter „Sammler“ zu befriedigen. So führte die Entdeckung von "Phragmipedium kovachii" neben einem Streit um die Erstbeschreibung auch dazu, dass die bekanntgewordenen Habitate in Peru stark dezimiert wurden.
Orchideen standen zudem im Mittelpunkt des künstlerischen Schaffens der amerikanischen Malerin Georgia O’Keeffe, die Blumenmotive mit der Sexualität weiblicher Körper assoziierte. Zu nennen wären etwa die Bilder „"An Orchid“" oder „"Narcissa's Last Orchid“, "jeweils aus dem Jahr 1941.

Trotz ihrer großen Vielfalt werden nur wenige Orchideenarten als kultivierte Nutzpflanze verwendet. Dazu zählt die Gewürzvanille ("Vanilla planifolia") zur Gewürzproduktion. Einige Arten werden auch zur Aromatisierung/Bereitung von Tee (Bsp. "Jumellea fragrans") oder auch als Parfümierungsmittel für Parfüm und Tabak (Bsp. "Vanilla pompona") genutzt.
Wo nationale Naturschutzgesetze dies nicht unterbinden, werden verschiedene Arten der Gattungen Orchis und Ophrys (Bsp. "Orchis morio") durch Naturentnahmen zur Gewinnung von Gallerte aus „Salep“ genutzt. Die ausgegrabenen Wurzelknollen werden in der Türkei zur Aromatisierung von Speiseeis verwendet.

Große wirtschaftliche Bedeutung erlangen die Orchideen als Zierpflanzen oder Schnittblumen. Viele können in weitem Umfang, auch über Gattungsgrenzen hinweg, zur Kreuzung verwendet werden. So entstanden im Lauf der letzten etwa 150 Jahre ungefähr 100.000 Hybriden. Von diesen werden wiederum einige Tausende als Zierpflanzen kommerziell vermehrt und verkauft. Den größten Anteil daran haben im Zierpflanzenbereich die Züchtungen von Hybriden der Gattungen "Phalaenopsis", "Cattleya", "Dendrobium", "Paphiopedilum" und "Cymbidium". Außer als getopfte Pflanzen werden die Blütentriebe der Gattungen "Phalaenopsis", "Dendrobium" und "Cymbidium" häufig auch als Schnittblumen vermarktet.

Im südostasiatischen Raum erwirtschaftet Thailand mit dem Export von Orchideen jährlich ca. 2 Milliarden Baht (etwa 40 Mio. Euro), wobei die Hauptmärkte in den USA, Japan, Europa, Hongkong, Taiwan und Südkorea liegen. Dies sorgte 2002 für den Export von über 3,1 Mio. Orchideenpflanzen. Da laut thailändischer Landwirtschaftsbehörde ein Trend mit großem Umsatzpotenzial erkannt wurde, wird versucht, die Qualität und Attraktivität der thailändischen Orchideen mit Zertifikaten weiter zu steigern.
In Europa werden große Mengen von Orchideenhybriden vor allem in den Niederlanden für den Massenmarkt (Baumärkte, Pflanzen- und Blumencenter) produziert. So gab es 2003 dort etwa 216 ha überglaste Anbaufläche alleine für die Produktion von Orchideen für den Schnittblumenverkauf. In den USA betrug der Umsatz durch getopfte Orchideen etwa 121 Millionen US-$ (2003).

Der Massenmarkt wird vorwiegend mit in vitro erzeugten Pflanzen bedient. Die Bedeutung dieses Geschäftszweiges lässt sich anhand der Entwicklung der Produktionsmengen belegen. Innerhalb von 10 Jahren (1991–2000) hat sich die Menge der in Deutschland in vitro produzierten Orchideen fast verfünffacht (1991: ca. 2,5 Millionen Pflanzen, 2000: über 12 Millionen Pflanzen). Den größten Anteil hatten daran Pflanzen (größtenteils Hybriden) der Gattungen "Phalaenopsis" (2000: über 9 Millionen Pflanzen).

Darwins Entdeckungen
Schon Charles Darwin war fasziniert von einer madagassischen Orchideen-Blüte "Angraecum sesquipedale" mit einem bis zu 35 cm langen Sporn. Auch diese Blüte muss irgendwie bestäubt werden, und irgendein Tier muss in diesen Sporn hineinkommen. Tatsächlich fand man 1903 das zu der Pflanze passende Insekt, den Schwärmer "Xanthopan morgani praedicta".

Orchideen als psychoaktive Pflanze
Die "Trichocentrum cebolleta" ist eine Orchideenart mit gelb-braun getupften Blüten, die im tropisch-subtropischen Amerika und in der Karibik wächst. In Europa wird sie schon seit langem als Zierpflanze kultiviert. Die Blätter enthalten als wirksame Inhaltsstoffe verschiedene Phenanthrene. Diese wirken halluzinogen und werden von den Tarahumara (einem mexikanischen Indianerstamm) als Ersatz für den Peyotekaktus "Lophophora williamsii" gebraucht (Hauptwirkstoff Meskalin).

Orchidee als Metapher in der Sprache
Die besondere Stellung der Orchidee unter den Blumen macht das Wort Orchidee zu einer beliebten Metapher in der Sprache. Die Orchidee gilt als ausnehmend schön und als selten zu finden. Daher steht einerseits „Orchidee“ oft für etwas besonders "Schönes". In Verbindung mit der sexuellen Konnotation wird daher oft eine äußerst hübsche Frau als Orchidee bezeichnet, so im Film Wilde Orchidee. Andererseits steht „Orchidee“ für etwas besonders "Seltenes". Diese zweite Metapher kann auch spöttisch sein; so wird eine selten studierte Studienrichtung mit außergewöhnlichen Inhalten als Orchideenfach bezeichnet.


Allgemein

Vereine und Gesellschaften


</doc>
<doc id="3757" url="https://de.wikipedia.org/wiki?curid=3757" title="Organ">
Organ

Organ (von altgriechisch "organon" ‚Werkzeug‘) steht:

Organ heißen:
Siehe auch:


</doc>
<doc id="3758" url="https://de.wikipedia.org/wiki?curid=3758" title="Oberon (Programmiersprache)">
Oberon (Programmiersprache)

Oberon ist eine von Niklaus Wirth und Jürg Gutknecht entwickelte, objektorientierte, streng strukturierte Programmiersprache. Sie ist den ebenfalls von Wirth entworfenen Vorgängern Pascal und Modula-2 recht ähnlich, allerdings strukturierter als Pascal und mächtiger, gleichzeitig aber erheblich weniger umfangreich als Modula-2. Das ETH Oberon System ist ein eigenständiges Betriebssystem der ETH Zürich, das in der Sprache Oberon implementiert ist, als Entwicklungsgrundlage für die Sprache diente und ebenso wie der Compiler kostenlos erhältlich ist.

Oberon wurde – wie sein Vorgänger Modula-2 – parallel zu einer Workstation (Ceres) entwickelt.

Oberon fand nach seiner Veröffentlichung recht schnell unter anderem zu Bildungszwecken in Schulen und Universitäten Verwendung. Es existieren inzwischen allerdings auch auf Oberon basierende, ebenfalls kostenlos verfügbare Werkzeuge, die auch kommerziell eingesetzt werden, wie zum Beispiel die Programmiersprache Component Pascal und die Entwicklungsumgebung BlackBox Component Builder.

Die Vorteile von Oberon liegen besonders im modularen Aufbau, der großen Sicherheit und in der Einfachheit der Sprache, die eindeutig und vergleichsweise kurz definiert werden kann "(siehe auch EBNF)". Mit Oberon ist es besonders leicht und sicher, das Programmieren auf verschiedene Personen aufzuteilen und die Arbeit später zusammenzufügen.

Hanspeter Mössenböck hat Oberon mit wenigen Änderungen zur Programmiersprache Oberon-2 weiterentwickelt, wobei zusätzlich im Wesentlichen explizit typgebundene Prozeduren erlaubt wurden, so dass die entsprechenden Objekte nicht mehr implizit in der formalen Parameterliste der Methoden aufgeführt werden müssen. Ferner wurde die Exportmarke "-" (als Alternative zu "*") zur Unterdrückung von Schreibrechten auf Objekte oder deren Komponenten eingeführt.

Die Quelltexte der Compiler sind in der Regel frei verfügbar. Es gibt verschiedene Programm-Entwicklungsumgebungen, genannt werden kann zum Beispiel auch POW!. Neben dem Einsatz als Programmiersprache ist auch die Nutzung als Betriebssystem (Native Oberon) möglich.

Anders als bei anderen vollwertigen, objektorientierten Programmiersprachen wird der Quelltext nicht interpretiert (zum Beispiel Ruby) oder in Bytecode übersetzt (zum Beispiel Java), sondern in der Regel in einem einzigen Compilerdurchlauf sehr schnell in Maschinensprache übersetzt. Der kompilierte Code ist typsicher, und Speicherbereichsprüfungen sind obligatorisch. Die Verwendung von Programmanweisungen zur Deallokation von Zeigervariablen ist obsolet.

Es ist möglich, Haltepunkte zu setzen (Anweisung HALT) und auch alle lokalen Variablen nach dem Abbruch des Programms zu analysieren. Globale Variablen können im Laufzeitsystem jederzeit analysiert werden. Die Entwicklungszeiten mit Oberon sind daher sehr kurz, und der Maschinencode ist dennoch sehr effizient und robust. Auch Echtzeitanwendungen können mit Oberon implementiert werden.

Die Programmiersprache Oberon zeichnet sich dadurch aus, dass sie die objektorientierte Architektur im Gegensatz zum Beispiel zu C++ unter anderem mit einem integrierten Laufzeitsystem Oberon System und einer Automatischen Speicherbereinigung "(garbage collection)" vollständig unterstützt. Auf Mehrfachvererbung wurde bewusst verzichtet, um den Compiler von komplexen Verwaltungsaufgaben zu entlasten und den Programmierer vor unerwarteten Ergebnissen im Zusammenhang mit dem Diamond-Problem zu bewahren.

Hello World im Ulmer OBERON-System:

Die objektorientierte Programmierung wird mit erweiterten Datenverbünden vom Datentyp "RECORD" erreicht. Die Definition von Methoden wird durch typengebundene Prozeduren, und die Definition von Sichtbarkeiten wird durch Exportmarken ("*" für Schreibzugriff und "-" für Lesezugriff) erwirkt. Beispiel in Oberon-2:

Attribute, die nur einen Lesezugriff haben, können durch typengebundene Prozeduren (Methoden) verändert werden. Beispiel:

Ausgehend von Oberon und Oberon-2 sind die Programmiersprachen Component Pascal, Active Oberon und Zonnon entstanden.





</doc>
<doc id="3760" url="https://de.wikipedia.org/wiki?curid=3760" title="Oberfranken">
Oberfranken

Oberfranken liegt im Norden des Freistaats Bayern, im fränkischen Teil, und grenzt an die Bundesländer Sachsen und Thüringen sowie die bayerischen Regierungsbezirke Unterfranken, Mittelfranken und Oberpfalz. Eine Außengrenze existiert zum Verwaltungsbezirk Karlsbad (Karlovarský kraj) der Tschechischen Republik.

Oberfranken ist sowohl ein Regierungsbezirk als auch ein Bezirk als Selbstverwaltungskörperschaft. Letzteres bedeutet das Recht, eigene Wappen und Fahnen zu führen. Verwaltungssitz des Bezirks und zugleich Sitz des Regierungspräsidenten und der Bezirksregierung ist Bayreuth. 

Der Name Oberfranken bezieht sich auf die Lage zum Main. Oberfranken liegt an dessen Oberlauf und Unterfranken am Unterlauf. Diese Benennung geht zurück auf die Bildung des Mainkreises im Zuge der von Graf Montgelas 1808 verfassten Konstitution des Königreichs Bayern. Die Einteilung der Territorien wurde dem französischen Vorbild angeglichen und orientierte sich primär an Flussnamen.

Seit der Wiedervereinigung gilt Oberfranken wieder als ein Herzstück in der Mitte Europas. Die Tourismus- und Industrieregion Oberfranken hat eine landschaftliche und kulinarische Vielfalt und eine Vielzahl an Unternehmen.

Blasonierung: Über rotem Schildfuß, darin drei silberne Spitzen, zweimal gespalten: vorne in Gold ein mit einer silbernen Schrägleiste überdeckter, linksgewendeter, rotbewehrter schwarzer Löwe; Mitte geviert von Silber und Schwarz; hinten fünfmal geteilt von Schwarz und Gold, belegt mit einem schräggestellten und geschwungenen grünen Rautenkranz.

Das Wappen erinnert im oberen Teil an die drei maßgeblichen historischen Territorien in Oberfranken: Der schwarze Löwe auf goldenem Grund am linken Rand steht für das Hochstift Bamberg, das Geviert von Silber und Schwarz versinnbildlicht das hohenzollersche Markgraftum Brandenburg-Bayreuth, während der rechte Teil in Gold, Schwarz und Grün das Herzogtum Sachsen-Coburg darstellt. Die Symbole der drei ehemaligen Gebiete stehen auf dem fränkischen Rechen im Schildfuß.

Die Flagge Oberfrankens stellt eine auf den Kopf gestellte weiß-rote Frankenfahne mit mittig angeordnetem Bezirkswappen dar.



Bis nach dem Zweiten Weltkrieg wurden die Regierungsbezirke Mittelfranken und Oberfranken gemeinsam verwaltet.

Der Regierungsbezirk Oberfranken umfasst vier kreisfreie Städte und neun Landkreise:





Johann Friederich Esper beschrieb bereits zwischen 1774 und 1790 einige Höhlen der Fränkischen Alb und J. B. Fischer grub 1788 die Grabhügel von Mistelgau im Landkreis Bayreuth aus. Die älteste Anwesenheit von Menschen ist durch Werkzeuge aus Lydit aus dem Riß-Würm-Interglazial (120000–80000 v. Chr.) belegt, die die Neandertaler fertigten. Die nächsten Artefakte sind wenig jünger und stammen aus dem Präsolutréen von Kösten, einem Stadtteil von Lichtenfels. Das frühe und mittlere Spätpaläolithikum ist in Oberfranken bisher nicht, das Jungpaläolithikum undeutlich vertreten. Der Nachweis von neolithischen Siedlungen der Bandkeramiker, die ab 5500 v. Chr. auftraten, ist besonders im Bereich des Altneolithikums im Maintal möglich. Hier sind u. a. mehr als 50 vor- und frühgeschichtliche Erdwerke oder Ringwälle bekannt, deren Größe zwischen 3 und 50 Hektar schwankt. Die größten liegen in Hetzles, Rödlas und Wiesenthau-Schlaifhausen im Landkreis Forchheim und auf dem Staffelberg in Bad Staffelstein-Romansthal im Landkreis Lichtenfels. Grabfunde liegen aber überhaupt nicht vor. Die bedeutendsten Fundplätze sind zwei Höhlen, die Jungfernhöhle von Tiefenellern und der Hohle Stein bei Schwabthal. Siedlungen aus dieser Zeit, die untersucht wurden, sind Altenbanz und Zilgendorf. Auch die darauffolgenden Kulturen sind dort bis in die frühe Bronzezeit nicht sonderlich stark repräsentiert. Siedlungen fehlen sogar noch aus der mittleren Bronzezeit. Die Hortfunde von Forchheim und Hollfeld belegen jedoch die relativ dünne Besiedlung in der Frühzeit. Die Anwesenheit von typischen Artefakten zeigt eine Orientierung nach Hessen und Thüringen. In der Urnenfelderzeit (1300–750 v. Chr.) werden die Spuren deutlicher und die Zahl der Depots nimmt zu. Gräber wie das so genannte Adelsgrab von Eggolsheim, Landkreis Forchheim, gewähren Einblicke in die Sepulkralkultur. Die darauffolgende Hallstattzeit ist durch Gräber und Grabhügel stark vertreten, so dass von einer dichteren Besiedlung auszugehen ist. In der La-Tène-Zeit (500–100 v. Chr.) war Oberfranken ein Kernbereich der tönernen Pferdeplastiken. Auch zahlreiche Funde römischer Herkunft wurden gemacht.

In der Völkerwanderungszeit dehnten zuerst die Thüringer ihren Einflussbereich nach Oberfranken aus. Nach Chlodwigs Sieg 496 n. Chr. über die Alemannen in der Schlacht von Zülpich geriet zunächst das westliche Maingebiet unter fränkischen Einfluss. Als im Jahre 531 (Schlacht bei Burgscheidungen) auch die Thüringer geschlagen wurden, geriet das ganze Maintal unter fränkische Herrschaft. Es war jedoch auch slawische Zuwanderung zu beobachten (Bavaria Slavica).

Das Gebiet des heutigen Oberfrankens bestand später im Wesentlichen aus den zwei historischen Territorien des Hochstifts Bamberg und des hohenzollernschen (seit 1791/1792 preußischen) Fürstentums Bayreuth (auch: Markgraftum "Brandenburg-Bayreuth" bzw. früher "Brandenburg-Kulmbach"). Zudem ist Oberfranken, wie Franken überhaupt, durch viele kleinräumige Herrschaften geprägt, deren Träger sich vorwiegend aus dem Ritterstand rekrutierten. Aufgrund der großen Zahl der Herrschaftsträger neben den größeren Territorialherren kann man den fränkischen Raum als "Adelslandschaft" bezeichnen, die in dieser Form im Alten Reich einmalig war. Die Ritter in Franken waren in der Regel reichsunmittelbar, also nur dem Kaiser untertan. Der Fränkische Ritterkreis untergliederte sich in die sechs Kantone: Altmühl, Baunach, Gebirg, Odenwald, Rhön-Werra und Steigerwald. Die meisten Rittersitze im heutigen Oberfranken gehörten zum Kanton Gebirg.

Nach der Abdankung des letzten Markgrafen von Ansbach-Bayreuth, Karl Alexander, gingen die beiden Hohenzollern-Markgraftümer an die preußische Linie in Berlin über. Das Königreich Preußen unter dem leitenden Minister Karl August von Hardenberg versuchte nun, Vorstellungen moderner Staatlichkeit auch in den neu erworbenen fränksichen Provinzen durchzusetzen, z.B. durch Mediatisierungen.

Die Vertreter der Fränkischen Ritterschaft sowie weiterer betroffener Territorialherren wandten sich rechtsuchend an den Wiener Hof, um sich gegen die Mediatisierungsversuche Preußens zu wehren. Aufgrund der Belastung durch die Koalitionskriege gegen das revolutionäre Frankreich waren die Einflussmöglichkeiten des Kaisers jedoch gering.

Im Jahre 1795 schloss Preußen einen Separatfrieden mit Frankreich (Frieden von Basel). Nun konnte Preußen noch ungehinderter seine Mediatiserungspolitik in Franken durchsetzen, die seit den späten 1790er Jahren praktisch abgeschlossen war. Die Markgraftümer Ansbach-Bayreuth waren nun geschlossene Territorien, in denen einheitlich das Allgemeine Landrecht für die Preußischen Staaten galt.

Nach der militärischen Besetzung Bambergs durch das Königreich Bayern entstand am 29. November 1802 zunächst die "Bayerische Provinz Bamberg," die am 1. Oktober 1808 in "Mainkreis" umbenannt wurde. Das Königreich Bayern kaufte das von 1806 bis 1810 als "pays reservé" (Napoleons Privatbesitz) unter französischer Herrschaft stehende Markgraftum Bayreuth für 15 Millionen Francs von den Franzosen und übernahm es am 30. Juni 1810. So entstand der "Obermainkreis" mit Bayreuth als Hauptstadt. Den Namen "Oberfranken" trägt der Bezirk seit dem 1. Januar 1838 in Anlehnung an das Herzogtum Franken, in dessen ehemaligem Ostteil er liegt. Seine Abrundung erhielt der Bezirk, als zum 1. Juli 1920 der Freistaat Coburg nach Bayern eingegliedert wurde. Kleinere Veränderungen des Gebiets von Oberfranken brachte schließlich die bayerische Gebietsreform von 1972. Der Großteil des Landkreises Höchstadt an der Aisch ging an Mittelfranken, hingegen kamen Gemeinden aus Unterfranken und der Oberpfalz zu Oberfranken.

Teilgebiete Oberfrankens und der Oberpfalz, besser gesagt ein Korridor von Hof bis nach Weiden, zu dem auch Bayreuth gehört, waren von Mitte des 19. Jahrhunderts bis Mitte des 20. Jahrhunderts nach dem Ruhrgebiet der am dichtesten industrialisierte Raum innerhalb Deutschlands. Hierbei bestanden enge Handels- und Wirtschaftsbeziehungen mit Thüringen und Sachsen. Nach dem Ende des Zweiten Weltkrieges, der Gründung zweier deutschen Staaten und schließlich dem Mauerbau am 13. August 1961, endete eine Ära der wirtschaftlichen Blüte dieser Region endgültig. Um die Region wieder wirtschaftlich voran zu bringen, griff man auf strukturpolitische Maßnahmen zurück. Dazu gehörten eine Auflockerung der Branchenstruktur und eine Anhebung des Qualitätsniveaus, um so eine Wettbewerbsfähigkeit zu garantieren. Durch diese Maßnahmen konnte die ständige Bevölkerungsabnahme der Region fast gänzlich gestoppt werden. Um junge Menschen, dabei vor allem junge Absolventen des Gymnasiums, in der Umgebung von Bayreuth und der nördlichen Oberpfalz zu halten und diesen eine Zukunftsperspektive zu geben, entscheid man sich 1971 zur Errichtung der Universität Bayreuth. Ähnlich wie an der Universität Regensburg, die 1962 gegründet wurde, führte auch in Bayreuth die Gründung einer Universität zu einem stetigen Bevölkerungsanstieg. Ihren Forschungsbetrieb nahm die Universität Bayreuth im Wintersemester 1975/1976 auf, deren Grundstein im März 1974 südlich der Stadtteile Kreuzstein und Birken gelegt wurde. Neben diesem Bauplatz waren in Bayreuth noch ein Areal am Roten Hügel und in Wendelhöfen im Gespräch. 

Die Region Oberfranken hat die zweithöchste Industriedichte Europas. Am Wissenschafts- und Industriestandort sind viermal mehr Hidden Champions als im Bundesdurchschnitt angesiedelt. Die Innovationskraft der Region zeigt sich auch, verglichen mit dem Bundesdurchschnitt, in der doppelt so hohen Anzahl an Patentanmeldungen. Gemessen am Bruttoinlandsprodukt gehört Oberfranken zu den wohlhabenderen Regionen der EU mit einem Index von 113 (EU27: 100, Deutschland: 116) (2008). Über ein Viertel aller Betriebe sind Automobilzulieferer.
Wichtigste Industriebranchen nach Beschäftigtenzahl sind (Stand: September 2005):

Oberfranken hat innerhalb Europas eine sehr hohe Industriedichte. Der Raum Hof-Bayreuth-Kulmbach zählt zu den wichtigsten Textilzentren Deutschlands, der Landkreis Wunsiedel ist das Zentrum der deutschen Keramikindustrie (Haushaltsporzellan, Hotelporzellan und technische Keramiken), der Raum Lichtenfels-Coburg ist Zentrum der deutschen Polstermöbelindustrie.

Die wirtschaftlichen Interessenvertretungen Oberfrankens sind die Industrie- und Handelskammer für Oberfranken Bayreuth, die Industrie- und Handelskammer zu Coburg und die Handwerkskammer für Oberfranken mit Sitz in Bayreuth.

Von 1970 bis 1990 wurde jährlich, danach etwa alle zwei Jahre bis 2011 von der IHK Bayreuth der Kulturpreis der oberfränkischen Wirtschaft an Personen vergeben, die sich um das kulturelle Leben in Oberfranken verdient gemacht haben.

In Bayreuth, der zweitgrößten Stadt Oberfrankens, finden jährlich im Sommer die Richard-Wagner-Festspiele statt, auch bekannt unter dem Namen Bayreuther Festspiele.

Im Museum für Bäuerliche Arbeitsgeräte in der Altstadt in Bayreuth ist die KulturServiceStelle des Bezirks Oberfranken untergebracht. Im Jahr 2015 kürte sie erstmals ein "Oberfränkisches Wort des Jahres", wobei die Wahl auf „Wischkästla“ (für Smartphone) fiel, daneben gab es aber auch weitere Vorschläge wie „Herrgottsmuggerla“ (für "Marienkäfer") und „etzerla“ (für "jetzt aber"). Im Jahr 2016 wurde der Satz „A weng weng“ (auf Hochdeutsch „Ein bisschen wenig“) gewählt, 2017 folgte der Ausdruck „Urigeln“ (das Kribbeln, wenn kalte Hände und Füße wieder warm werden).

Im Regierungsbezirk gibt es 95 Naturschutzgebiete, 70 Landschaftsschutzgebiete, 113 FFH-Gebiete, neun EU-Vogelschutzgebiete und mindestens 565 ausgewiesene Geotope. Das größte Naturschutzgebiet im Bezirk ist das Muschelkalkgebiet am Oschenberg.

Siehe auch:



Der einzige ICE-Systemhalt in Oberfranken ist Bamberg, der annähernd stündlich von Zügen der Linie Hamburg–Berlin–München bedient wird. Einzelne Zugpaare dieser Verbindung halten auch in Coburg. Des Weiteren verkehrt eine ICE-Verbindung werktags von Lichtenfels aus nach München. Zusätzlich verkehrt ein IC-Zugpaar Karlsruhe–Leipzig täglich über Bamberg, Lichtenfels und Kronach.

Die elektrifizierten Strecken im Westen Oberfrankens, die Forchheim, Bamberg, Lichtenfels, Coburg und Kronach miteinander verbinden, werden im Regionalverkehr durch den Franken-Thüringen-Express erschlossen. Zudem verläuft die Linie S1 der S-Bahn Nürnberg auf ihrem nördlichsten Abschnitt durch Oberfranken. Die nicht elektrifzierten Strecken im Osten Oberfrankens, die Bayreuth, Kulmbach, Hof und Marktredwitz an das Eisenbahnnetz anbinden, werden von Dieselzügen bedient. Von Hof aus besteht eine elektrifizierte Verbindung nach Leipzig. 2011 übernahm die agilis Verkehrsgesellschaft den Betrieb auf den nicht-elektrifizierten Nahverkehrsstrecken in Oberfranken.

Elektrifizierte Strecken, die durch Oberfranken verlaufen, sind:

Darüber hinaus werden folgende nicht elektrifizierte Haupt- und Nebenstrecken betrieben:


Die Strecken Strullendorf–Schlüsselfeld und Bamberg–Bamberg Hafen werden nur im Güterverkehr, die Strecken Ebermannstadt–Behringersmühle und Steinwiesen–Nordhalben nur im Museumsbetrieb befahren.




Der Bezirk Oberfranken bildet gemeinsam mit den anderen bayerischen Bezirken die dritte kommunale Ebene des Bundeslandes. Die Kernaufgaben des Bezirks liegen im sozialen und kulturellen Bereich. Die Organe des Bezirks sind der Bezirkstag, der Bezirksausschuss und der Bezirkstagspräsident ().

Nach Einrichtung der Kreise als höhere Kommunalverbände erhielten sie auch eine Legislative. Diese hieß zunächst Landrat, deren Mitglieder Landräte, die Vorsitzenden des Gremiums hießen Präsidenten des Landrats (1829−1919), Präsidenten des Kreistages (1919−1933), Präsidenten des Bezirksverbandstages. Seit 1954 werden sie als Präsidenten des Bezirkstages bezeichnet.

Der Regierungsbezirk Oberfranken ist gebietsmäßig identisch mit dem Bezirk Oberfranken. Er ist der Zuständigkeitsbereich der staatlichen Mittelbehörde "Regierung von Oberfranken".
Anmerkung: 1933 bis 1948 gemeinsamer Regierungspräsident mit Mittelfranken




</doc>
<doc id="3761" url="https://de.wikipedia.org/wiki?curid=3761" title="OSI-Modell">
OSI-Modell

Das OSI-Modell () ist ein Referenzmodell für Netzwerkprotokolle als Schichtenarchitektur. Es wird seit 1983 von der International Telecommunication Union (ITU) und seit 1984 auch von der International Organization for Standardization (ISO) als Standard veröffentlicht. Seine Entwicklung begann im Jahr 1977.

Zweck des OSI-Modells ist, Kommunikation über unterschiedlichste technische Systeme hinweg zu ermöglichen und die Weiterentwicklung zu begünstigen. Dazu definiert dieses Modell sieben aufeinander folgende Schichten (engl. ) mit jeweils eng begrenzten Aufgaben. In der gleichen Schicht mit klaren Schnittstellen definierte Netzwerkprotokolle sind einfach untereinander austauschbar, selbst wenn sie wie das Internet Protocol eine zentrale Funktion haben.

In einem Computernetz werden den verschiedenen Clients Dienste unterschiedlichster Art durch andere Hosts bereitgestellt. Dabei gestaltet sich die dafür erforderliche Kommunikation komplizierter, als sie zu Beginn erscheinen mag, da eine Vielzahl von Aufgaben bewältigt und Anforderungen bezüglich Zuverlässigkeit, Sicherheit, Effizienz usw. erfüllt werden müssen. Die zu lösenden Probleme reichen von Fragen der elektronischen Übertragung der Signale über eine geregelte Reihenfolge in der Kommunikation bis hin zu abstrakteren Aufgaben, die sich innerhalb der kommunizierenden Anwendungen ergeben.

Aufgrund dieser Vielzahl von Aufgaben wurde das OSI-Modell eingeführt, bei dem die Kommunikationsabläufe in sieben Ebenen (auch Schichten genannt) aufgeteilt werden. Dabei werden auf jeder einzelnen Schicht die Anforderungen separat umgesetzt.

Die verwendeten Instanzen müssen sowohl auf der Sender- als auch auf der Empfängerseite nach festgelegten Regeln arbeiten, um die Verarbeitung von Daten zu ermöglichen. Die Festlegung dieser Regeln wird in einem Protokoll beschrieben und bildet eine logische, "horizontale" Verbindung zwischen zwei Instanzen derselben Schicht.

Jede Instanz stellt "Dienste" zur Verfügung, die eine direkt darüberliegende Instanz nutzen kann. Zur Erbringung der Dienstleistung bedient sich eine Instanz selbst der Dienste der unmittelbar darunterliegenden Instanz. Der reale Datenfluss erfolgt daher "vertikal". Die Instanzen einer Schicht sind genau dann austauschbar, wenn sie sowohl beim Sender als auch beim Empfänger ausgetauscht werden können.

Der Abstraktionsgrad der Funktionalität nimmt von Schicht 1 bis zur Schicht 7 zu.

Das OSI-Modell im Überblick (siehe im Vergleich dazu das TCP/IP-Referenzmodell):
Die Bitübertragungsschicht (engl. "Physical Layer") ist die unterste Schicht. Diese Schicht stellt mechanische, elektrische und weitere funktionale Hilfsmittel zur Verfügung, um physische Verbindungen zu aktivieren bzw. zu deaktivieren, sie aufrechtzuerhalten und Bits darüber zu übertragen. Das können zum Beispiel elektrische Signale, optische Signale (Lichtleiter, Laser), elektromagnetische Wellen (drahtlose Netze) oder Schall sein. Die dabei verwendeten Verfahren bezeichnet man als übertragungstechnische Verfahren. Geräte und Netzkomponenten, die der Bitübertragungsschicht zugeordnet werden, sind zum Beispiel die Antenne und der Verstärker, Stecker und Buchse für das Netzwerkkabel, der Repeater, der Hub, der Transceiver, das T-Stück und der Abschlusswiderstand (Terminator).

Auf der Bitübertragungsschicht wird die digitale Bitübertragung auf einer leitungsgebundenen oder leitungslosen Übertragungsstrecke bewerkstelligt. Die gemeinsame Nutzung eines Übertragungsmediums kann auf dieser Schicht durch statisches Multiplexen oder dynamisches Multiplexen erfolgen. Dies erfordert neben den Spezifikationen bestimmter Übertragungsmedien (zum Beispiel Kupferkabel, Lichtwellenleiter, Stromnetz) und der Definition von Steckverbindungen noch weitere Elemente. Darüber hinaus muss auf dieser Ebene gelöst werden, auf welche Art und Weise ein einzelnes Bit übertragen werden soll.

Damit ist folgendes gemeint: In Rechnernetzen werden heute Informationen zumeist in Form von Bit- oder Symbolfolgen übertragen. Im Kupferkabel und bei Funkübertragung dagegen sind modulierte hochfrequente elektromagnetische Wellen die Informationsträger, im Lichtwellenleiter Lichtwellen von bestimmter oder unterschiedlicher Wellenlänge. Die Informationsträger kennen keine Bitfolgen, sondern können weitaus mehr unterschiedliche Zustände annehmen als nur 0 oder 1. Für jede Übertragungsart muss daher eine Codierung festgelegt werden. Das geschieht mit Hilfe der Spezifikation der Bitübertragungsschicht eines Netzes.

Hardware auf dieser Schicht: Repeater, Hubs, Leitungen, Stecker, u. a.

Protokolle und Normen: V.24, V.28, X.21, RS 232, RS 422, RS 423, RS 499

Aufgabe der Sicherungsschicht (engl. "Data Link Layer"; auch "Abschnittssicherungsschicht", "Datensicherungsschicht", "Verbindungssicherungsschicht", "Verbindungsebene", "Prozedurebene") ist es, eine zuverlässige, das heißt weitgehend fehlerfreie Übertragung zu gewährleisten und den Zugriff auf das Übertragungsmedium zu regeln. Dazu dient das Aufteilen des Bitdatenstromes in Blöcke – auch als "Frames" oder "Rahmen" bezeichnet – und das Hinzufügen von Prüfsummen im Rahmen der Kanalkodierung. So können fehlerhafte Blöcke vom Empfänger erkannt und entweder verworfen oder sogar korrigiert werden; ein erneutes Anfordern verworfener Blöcke sieht diese Schicht aber nicht vor.

Eine „Datenflusskontrolle“ ermöglicht es, dass ein Empfänger dynamisch steuert, mit welcher Geschwindigkeit die Gegenseite Blöcke senden darf. Die internationale Ingenieursorganisation IEEE sah die Notwendigkeit, für lokale Netze auch den konkurrierenden Zugriff auf ein Übertragungsmedium zu regeln, was im OSI-Modell nicht vorgesehen ist.

Nach IEEE ist Schicht 2 in zwei Unter-Schichten "(sub layers)" unterteilt: LLC (Logical Link Control, Schicht 2b) und MAC (Media Access Control, Schicht 2a).

Hardware auf dieser Schicht: Bridge, Switch (Multiport-Bridge)

Das Ethernet-Protokoll beschreibt sowohl Schicht 1 als auch Schicht 2, wobei auf dieser als Zugriffskontrolle CSMA/CD zum Einsatz kommt.

Protokolle und Normen, die auf anderen Schicht-2-Protokollen und -Normen aufsetzen: HDLC, SDLC, DDCMP, IEEE 802.2 (LLC), RLC, PDCP, ARP, RARP, STP, Shortest Path Bridging

Protokolle und Normen, die direkt auf Schicht 1 aufsetzen: IEEE 802.11 (WLAN), IEEE 802.4 (Token Bus), IEEE 802.5 (Token Ring), FDDI

Die Vermittlungsschicht (engl. "Network Layer"; auch "Paketebene oder Netzwerkschicht") sorgt bei leitungsorientierten Diensten für das Schalten von Verbindungen und bei paketorientierten Diensten für die Weitervermittlung von Datenpaketen. Die Datenübertragung geht in beiden Fällen jeweils über das gesamte Kommunikationsnetz hinweg und schließt die Wegsuche (Routing) zwischen den Netzwerkknoten ein. Da nicht immer eine direkte Kommunikation zwischen Absender und Ziel möglich ist, müssen Pakete von Knoten, die auf dem Weg liegen, weitergeleitet werden. Weitervermittelte Pakete gelangen nicht in die höheren Schichten, sondern werden mit einem neuen Zwischenziel versehen und an den nächsten Knoten gesendet.

Zu den wichtigsten Aufgaben der Vermittlungsschicht zählt das Bereitstellen netzwerkübergreifender Adressen, das Routing bzw. der Aufbau und die Aktualisierung von Routingtabellen und die Fragmentierung von Datenpaketen. Aber auch die Aushandlung und Sicherstellung einer gewissen Dienstgüte fällt in den Aufgabenbereich der Vermittlungsschicht.

Neben dem Internet Protocol zählen auch die NSAP-Adressen zu dieser Schicht. Da ein Kommunikationsnetz aus mehreren Teilnetzen unterschiedlicher Übertragungsmedien und -protokolle bestehen kann, sind in dieser Schicht auch die Umsetzungsfunktionen angesiedelt, die für eine Weiterleitung zwischen den Teilnetzen notwendig sind.

Hardware auf dieser Schicht: Router, Layer-3-Switch (BRouter)

Protokolle und Normen: X.25, ISO 8208, ISO 8473 (CLNP), ISO 9542 (ESIS), IP, IPsec, ICMP

Zu den Aufgaben der Transportschicht (engl. ; auch "Ende-zu-Ende-Kontrolle", "Transport-Kontrolle") zählen die Segmentierung des Datenstroms und die Stauvermeidung (engl. ).

Ein Datensegment ist dabei eine Service Data Unit, die zur Datenkapselung auf der vierten Schicht (Transportschicht) verwendet wird. Es besteht aus Protokollelementen, die Schicht-4-Steuerungsinformationen enthalten. Als Adressierung wird dem Datensegment eine Schicht-4-Adresse vergeben, also ein Port. Das Datensegment wird in der Schicht 3 in ein Datenpaket gekapselt.

Die Transportschicht bietet den anwendungsorientierten Schichten 5 bis 7 einen einheitlichen Zugriff, so dass diese die Eigenschaften des Kommunikationsnetzes nicht zu berücksichtigen brauchen.

Fünf verschiedene Dienstklassen unterschiedlicher Güte sind in Schicht 4 definiert und können von den oberen Schichten benutzt werden, vom einfachsten bis zum komfortabelsten Dienst mit Multiplexmechanismen, Fehlersicherungs- und Fehlerbehebungsverfahren.

Protokolle und Normen: ISO 8073/X.224, ISO 8602, TCP, UDP, SCTP, DCCP.

Die Schicht 5 (Steuerung logischer Verbindungen; engl. "Session Layer"; auch "Sitzungsschicht", Kommunikationsschicht, Kommunikationssteuerungsschicht) sorgt für die Prozesskommunikation zwischen zwei Systemen. Hier findet sich unter anderem das Protokoll RPC (Remote Procedure Call).
Um Zusammenbrüche der Sitzung und ähnliche Probleme zu beheben, stellt die Sitzungsschicht Dienste für einen organisierten und synchronisierten Datenaustausch zur Verfügung. Zu diesem Zweck werden Wiederaufsetzpunkte, so genannte Fixpunkte (Check Points) eingeführt, an denen die Sitzung nach einem Ausfall einer Transportverbindung wieder synchronisiert werden kann, ohne dass die Übertragung wieder von vorne beginnen muss.

Protokolle und Normen: ISO 8326 / X.215 (Session Service), ISO 8327 / X.225 (Connection-Oriented Session Protocol), ISO 9548 (Connectionless Session Protocol)

Die Darstellungsschicht (engl. "Presentation Layer"; auch "Datendarstellungsschicht", "Datenbereitstellungsebene") setzt die systemabhängige Darstellung der Daten (zum Beispiel ASCII, EBCDIC) in eine unabhängige Form um und ermöglicht somit den syntaktisch korrekten Datenaustausch zwischen unterschiedlichen Systemen. Auch Aufgaben wie die Datenkompression und die Verschlüsselung gehören zur Schicht 6. Die Darstellungsschicht gewährleistet, dass Daten, die von der Anwendungsschicht eines Systems gesendet werden, von der Anwendungsschicht eines anderen Systems gelesen werden können. Falls erforderlich, agiert die Darstellungsschicht als Übersetzer zwischen verschiedenen Datenformaten, indem sie ein für beide Systeme verständliches Datenformat, die ASN.1 (Abstract Syntax Notation One), verwendet.

Protokolle und Normen: ISO 8822 / X.216 (Presentation Service), ISO 8823 / X.226 (Connection-Oriented Presentation Protocol), ISO 9576 (Connectionless Presentation Protocol)

Dienste, Anwendungen und Netzmanagement.
Die Anwendungsschicht stellt Funktionen für die Anwendungen zur Verfügung. Diese Schicht stellt die Verbindung zu den unteren Schichten her. Auf dieser Ebene findet auch die Dateneingabe und -ausgabe statt. Die Anwendungen selbst gehören nicht zur Schicht.

Anwendungen: Webbrowser, E-Mail-Programm, Instant Messaging

7. Schicht / Anwendung: Funktionen für Anwendungen, sowie die Dateneingabe und -ausgabe.

6. Schicht / Darstellung: Umwandlung der systemabhängigen Daten in ein unabhängiges Format.

5. Schicht / Sitzung: Steuerung der Verbindungen und des Datenaustauschs.

4. Schicht / Transport: Zuordnung der Datenpakete zu einer Anwendung.

3. Schicht / Vermittlung: Routing der Datenpakete zum nächsten Knoten.

2. Schicht / Sicherung: Segmentierung der Pakete in Frames und Hinzufügen von Prüfsummen.

1. Schicht / Bitübertragung: Umwandlung der Bits in ein zum Medium passendes Signal und physikalische Übertragung.

Das OSI-Referenzmodell wird oft herangezogen, wenn es um das Design von Netzprotokollen und das Verständnis ihrer Funktionen geht. Auf der Basis dieses Modells sind auch Netzprotokolle entwickelt worden, die jedoch fast nur in der öffentlichen Kommunikationstechnik verwendet werden, also von großen Netzbetreibern wie der Deutschen Telekom. Im privaten und kommerziellen Bereich wird hauptsächlich die TCP/IP-Protokoll-Familie eingesetzt. Das TCP/IP-Referenzmodell ist sehr speziell auf den Zusammenschluss von Netzen "(internetworking)" zugeschnitten.

Die nach dem OSI-Referenzmodell entwickelten Netzprotokolle haben mit der TCP/IP-Protokollfamilie gemeinsam, dass es sich um hierarchische Modelle handelt. Es gibt aber wesentliche konzeptionelle Unterschiede: OSI legt die Dienste genau fest, die jede Schicht für die nächsthöhere zu erbringen hat. TCP/IP hat kein derartig strenges Schichtenkonzept wie OSI. Weder sind die Funktionen der Schichten genau festgelegt noch die Dienste. Es ist erlaubt, dass eine untere Schicht unter Umgehung zwischenliegender Schichten direkt von einer höheren Schicht benutzt wird. TCP/IP ist damit erheblich effizienter als die OSI-Protokolle. Nachteil bei TCP/IP ist, dass es für viele kleine und kleinste Dienste jeweils ein eigenes Netzprotokoll gibt. OSI hat dagegen für seine Protokolle jeweils einen großen Leistungsumfang festgelegt, der sehr viele Optionen hat. Nicht jede kommerziell erhältliche OSI-Software hat den vollen Leistungsumfang implementiert. Daher wurden OSI-Profile definiert, die jeweils nur einen bestimmten Satz von Optionen beinhalten. OSI-Software unterschiedlicher Hersteller arbeitet zusammen, wenn dieselben Profile implementiert sind.

Zur Einordnung von Kommunikationsprotokollen in das OSI-Modell siehe auch:

Das Konzept des OSI-Modells stammt aus der Datenwelt, die immer Nutzdaten (in Form von Datenpaketen) transportiert. Um die Telekommunikationswelt auf dieses Modell abzubilden, waren Zusätze erforderlich. Diese Zusätze berücksichtigen, dass in der Telekommunikation eine von den Datenströmen getrennte Zeichengabe für den Verbindungsauf- und -abbau vorhanden ist, und dass in der Telekommunikation die Geräte und Einrichtungen mit Hilfe eines Management-Protokolls von Ferne konfiguriert, überwacht und entstört werden. ITU-T hat für diese Zusätze das OSI-Modell um zwei weitere Protokoll-Stacks erweitert und ein generisches Referenzmodell standardisiert (ITU-T I.322). Die drei Protokoll-Stacks werden bezeichnet als
Jede dieser „Planes“ ist wiederum nach OSI in sieben Schichten strukturiert.

Das genormte Referenzmodell wird in der ISO weiterentwickelt. Der aktuelle Stand ist in der Norm ISO/IEC 7498-1:1994 nachzulesen. Das technische Komitee „Information Processing Systems“ hatte sich das Ziel gesetzt, informationsverarbeitende Systeme verschiedener Hersteller zur Zusammenarbeit zu befähigen. Daher kommt die Bezeichnung „Open Systems Interconnection“.

An der Arbeit im Rahmen der ISO nahm auch der Ausschuss "Offene Kommunikationssysteme" des DIN teil, der dann den ISO-Standard auch als deutsche Industrienorm in der englischen Originalfassung des Textes übernahm. Auch ITU-T übernahm ihn: In einer Serie von Standards X.200, X.207, … sind nicht nur das Referenzmodell, sondern auch die Services und Protokolle der einzelnen Schichten spezifiziert.

Weitere Bezeichnungen für das Modell sind "ISO/OSI-Modell", "OSI-Referenzmodell", "OSI-Schichtenmodell" oder "7-Schichten-Modell"

Standardisierungsdokumente:

Das OSI-Modell lässt sich durch folgende Analogie aus dem Geschäftsleben beschreiben:

Ein Firmenmitarbeiter möchte seinem Geschäftspartner eine Nachricht senden. Der Mitarbeiter ist mit dem Anwendungsprozess, der die Kommunikation anstößt, gleichzusetzen. Er spricht die Nachricht auf ein Diktiergerät. Sein Assistent bringt die Nachricht auf Papier. Der Assistent wirkt somit als Darstellungsschicht. Danach gibt er die Nachricht an die Sekretärin, die den Versand der Nachricht verwaltungstechnisch abwickelt und damit die Sitzungsschicht repräsentiert. Der Hauspostmitarbeiter (gleich Transportschicht) bringt den Brief auf den Weg. Dazu klärt er mit der Vermittlungsschicht (gleich Briefpost), welche Übertragungswege bestehen, und wählt den geeigneten aus. Der Postmitarbeiter bringt die nötigen Vermerke auf den Briefumschlag an und gibt ihn weiter an die Verteilstelle, die der Sicherungsschicht entspricht. Von dort gelangt der Brief zusammen mit anderen in ein Transportmittel wie LKW oder Flugzeug und nach eventuell mehreren Zwischenschritten zur Verteilstelle, die für den Empfänger zuständig ist.

Auf der Seite des Empfängers wird dieser Vorgang in umgekehrter Reihenfolge durchlaufen, bis der Geschäftspartner die Nachricht auf ein Diktiergerät gesprochen vorfindet.

Diese Analogie zeigt nicht auf, welche Möglichkeiten der Fehlerüberprüfung und -behebung das OSI-Modell vorsieht, da diese beim Briefversand nicht bestehen.

Es gibt einige Eselsbrücken/Informatik-Merksprüche zu den Namen der einzelnen OSI-Schichten, die gerne zum einfacheren Merken verwendet werden. Wohl mitunter einer der populärsten Sprüche lautet "“Please Do Not Throw Salami Pizza Away”" ("Physical Layer", "Data Link Layer" usw.). Eine deutsche Variante ist "„Alle deutschen Studenten trinken verschiedene Sorten Bier“" (Anwendungsschicht, Darstellungsschicht, …). Weitere sehr eingängige Eselsbrücken für die englische Variante lauten "„Alle Priester Saufen Tequila Nach Der Predigt“" und "„All People Seem to Need Data Processing“".

Unter IT-Fachleuten wird das OSI-Modell gelegentlich auf eine nicht existierende achte Schicht erweitert. Da im Modell die letzte, siebte Schicht dem Benutzer am nächsten liegt, spricht man dann von einem Problem auf dem OSI-Layer 8, wenn dessen Ursache beim Benutzer selbst vermutet wird. Selten werden finanzielle, politische und religiöse Vorbehalte in den Schichten acht bis zehn behandelt.





</doc>
<doc id="3764" url="https://de.wikipedia.org/wiki?curid=3764" title="Oberer See">
Oberer See

Der Obere See (; ) ist der größte der fünf Großen Seen Nordamerikas sowie das nach dem Kaspischen Meer flächenmäßig zweitgrößte Binnengewässer der Erde und damit der flächenmäßig größte Süßwassersee (tiefster und vom Volumen her größter Süßwassersee ist der Baikalsee in Sibirien).

Durch den Oberen See verläuft die Grenze zwischen Kanada und den Vereinigten Staaten. Sein Wasserspiegel liegt auf bei einer Gesamtfläche von 82.103 km²(entspricht etwa der Größe Österreichs). Seine größte Tiefe beträgt 406 m, vom nördlichsten Punkt bis zum südlichsten beträgt die Entfernung 290,2 km, die größte Ost-West-Ausdehnung beträgt 599,6 km. Der "Obere See" grenzt im Norden an die Provinz Ontario in Kanada (kanadischer Seeanteil 29.847 km², amerikanischer Seeanteil 52.256 km²) und den US-Bundesstaat Minnesota, im Süden an die US-Bundesstaaten Wisconsin und Michigan. Die größte Insel im See ist die Isle Royale, von Süden ragt die Keweenaw-Halbinsel weit in den See hinein.

Der "Obere See" ist der "Große See" mit der besten Wasserqualität, da im Gegensatz zu den übrigen Seen an seinem Ufer nur wenige Industrieanlagen angesiedelt sind und er nicht von den anderen Seen gespeist wird.

Der See hat über 200 Zuflüsse. Die größten sind der Nipigon River, der Saint Louis River, der Pigeon River, der Pic River, der White River, der Michipicoten River und der Kaministiquia River.

Der Obere See fließt über den Saint Marys River in den Huronsee ab und ist damit Teil des Wasserweges der Großen Seen. Die Stromschnellen auf diesem Fluss erfordern Schleusen, die Soo Locks bei Sault Ste. Marie, damit Schiffe die acht Meter Höhenunterschied zum Huronsee überwinden können.

In der Sprache der Anishinabe (auch Ojibwa oder Chippewa-Indianer) wird der See „Gichigami“ („großes Wasser“) genannt. Weiterhin ist er bekannt als der „Gitche Gumee“, so wie z. B. auch in Henry Wadsworth Longfellows 1855 Epos, The Song of Hiawatha und Gordon Lightfoots Ballade „The Wreck of the Edmund Fitzgerald“, welche vom Untergang der Edmund Fitzgerald auf dem Oberen See im November 1975 handelt. Der See wurde im 17. Jahrhundert von französischen Forschern „le lac supérieur“ (Oberer See) genannt, weil er sich oberhalb des Huronsees befindet.

Der Obere See war schon für die indianische Urbevölkerung ein wichtiger Verkehrsweg, am Nordwest-Ufer münden mit dem Pigeon River und dem Kaministiquia River zwei Flüsse, die als Routen ins Innere des heutigen Kanadas dienten. Die ersten Weißen waren französische Pelzhändler, die von den Indianern diese Verbindungen gezeigt bekamen. Die sogenannte "Grand Portage" am Pigeon River wurde zum Handelsstützpunkt ausgebaut, die 1783 von französischstämmigen Händlern gegründete North West Company legte dort ein befestigtes Lager an, in das sie im Sommerhalbjahr ihr Hauptquartier verlegten. Der rekonstruierte Stützpunkt ist heute als Grand Portage National Monument ausgewiesen.

Im 19. Jahrhundert entstanden Bergbausiedlungen rund um den See, insbesondere Eisenerz wurde abgebaut und per Schiff über den See transportiert. Duluth ist heute der umsatzstärkste Binnenhafen der Vereinigten Staaten, Steinkohle und Eisenerz sind die wichtigsten Güter.

Heute spielt der Tourismus eine wichtige Rolle, das milde Klima macht das Gebiet im Sommer und Winter zu einem beliebten Ziel. Fischen, Bootfahren, Wandern, im Winter Skilanglauf und Schneemobilfahren sind die häufigsten Aktivitäten. Die Wasserqualität macht den See trotz der geringen Temperaturen zu einem attraktiven Tauchgebiet.

Seit 2007 steht ein erheblicher Teil der kanadischen Küste des Sees unter Schutz. Dort befindet sich die Lake Superior National Marine Conservation Area.

Der Obere See ist eine wichtige Transportroute für Eisenerz und andere Bergbauprodukte. Große Frachtschiffe, sogenannte Laker und kleinere seetüchtige Schiffe der Seawaymax-Klasse transportieren solche Güter auf dem Sankt-Lorenz-Seeweg.

Das Südufer des Sees zwischen Grand Marais und Whitefish Point gilt als Schiffsfriedhof, da in diesem Gebiet mehr Schiffe untergegangen sind als in jedem anderen Teil des Sees. Diese Schiffswracks sind durch die "Whitefish Point Underwater Preserve" geschützt. Das letzte größere Schiff, das auf dem großen See in einem Sturm gesunken ist, war die SS Edmund Fitzgerald. Dieses Schiff war am 10. November 1975 rund 27 km von Whitefish Point entfernt mit 29 Besatzungsmitgliedern an Bord untergegangen. In der Geschichte der Schifffahrt auf dem Oberen See haben wiederholt Stürme mehrere Schiffe zum Untergang gebracht, etwa der Mataafa-Sturm vom 28. November 1905 oder der Great Lakes Storm of 1913.

Im August 2007 etwa wurde das Wrack der "Cyprus" gefunden. Dieser 128 m lange Eisenerzfrachter sank in einem Sturm am 11. Oktober 1907 auf seiner zweiten Fahrt von Superior, Wisconsin nach Buffalo, New York. Das in Lorain, Ohio gebaute Schiff war erst am 17. August 1907 in Dienst gestellt worden.


</doc>
<doc id="3765" url="https://de.wikipedia.org/wiki?curid=3765" title="Ontariosee">
Ontariosee

Der Ontariosee (; ) ist der flächenmäßig kleinste der fünf Großen Seen Nordamerikas, die miteinander durch Flussläufe verbunden sind.

Nach Tiefe und Volumen ist er jedoch nach dem Eriesee der zweitkleinste.

Durch den Ontariosee verläuft die Grenze der Vereinigten Staaten zu Kanada. Er liegt auf Höhe und seine Fläche beträgt 19.011 km². Der kanadische Seeanteil beträgt 9969 km² und der US-amerikanische Seeanteil beträgt 9042 km². Zum Vergleich: Seine Ausdehnung entspricht ungefähr der Größe des deutschen Bundeslandes Rheinland-Pfalz; die Fläche des größten Sees Europas, des Ladogasees, übertrifft er um rund 1000 km².

Die Wassertiefe beträgt bis zu 244 Meter. Der Hauptzufluss erfolgt über den Niagara River aus dem Eriesee. Weitere Zuflüsse sind der Trent River, der Genesee River, der Oswego River und der Salmon River. Der Ontariosee entwässert über den Sankt-Lorenz-Strom. Durch den arktischen Einfluss ist der Uferrand durchschnittlich drei Monate im Jahr zugefroren, dient aber im Sommer als Wärmespeicher, so dass er im Gebiet um die Niagarafälle u. a. den Weinanbau ermöglicht.

Das Wasser aus dem See wird auch für die Trinkwasserversorgung und zur Kühlung von Bürogebäuden in der Stadt Toronto verwendet (siehe DLWC).

Wie bei den anderen Großen Seen Nordamerikas, so sind auch beim Ontariosee die Gezeiten – anders als bei vielen kleineren Binnenseen – bemerkbar. Der Ontariosee hat rund 1500 Inseln, die größte, Wolfe Island, liegt am östlichen Abfluss des Sees im Sankt-Lorenz-Strom.

Der Name des Sees leitet sich von der Sprache der Wyandot ab, wo das Wort "ontarío" „Großer See“ bedeutet. Die kanadische Provinz wurde später nach dem Ontariosee benannt. In frühen Karten wurde der See teilweise anders genannt. In den "Relation des Jésuites" trug er den Namen „Lac Ontario ou des Iroquois“ oder „Ondiara“. Eine französische Karte aus dem Jahr 1712 bezeichnete ihn als „Lac Frontenac“. In der irokesischen Sprache hieß der See „Skanadario“.

Wegen der Ähnlichkeiten in Form und Fläche diente der Lake Ontario als Namensgeber für den Ontario Lacus auf dem Saturnmond Titan.



</doc>
<doc id="3766" url="https://de.wikipedia.org/wiki?curid=3766" title="Oxidation">
Oxidation

Die Oxidation ist eine chemische Reaktion, bei der ein Atom, Ion oder Molekül Elektronen abgibt. Seine Oxidationszahl wird dabei erhöht. Ein anderer Stoff nimmt die Elektronen auf und wird reduziert. Beide Reaktionen zusammen werden als Teilreaktionen einer Redoxreaktion betrachtet.

Der Begriff "Oxidation" wurde ursprünglich von dem Chemiker Antoine Laurent de Lavoisier geprägt, der damit die Vereinigung von Elementen und chemischen Verbindungen mit dem Element Sauerstoff (Oxygenium, franz: oxygène), also die Bildung von Oxiden, beschreiben wollte.

Später erfolgte eine Erweiterung des Begriffes, indem man Reaktionen mit einbezog, bei denen einer Verbindung Wasserstoffatome entzogen wurden (Dehydrierung). So werden z. B. bei vielen biochemischen Vorgängen organischen Verbindungen Wasserstoffatome durch bestimmte Coenzyme (NAD, NADP, FAD) „entrissen“.

Auf Grundlage der Ionentheorie und des Bohrschen Atommodells konnte die Oxidation schließlich unter elektronentheoretischen Gesichtspunkten interpretiert und verallgemeinert werden. Das Charakteristische an diesem Vorgang wird nun in der Elektronenabgabe eines chemischen Stoffes gesehen.

Als Oxidation im ursprünglichen Sinn bezeichnete man früher die chemische Reaktion eines Stoffes mit Sauerstoff. Oxidationen unter Flammenerscheinung werden als Verbrennung oder Feuer bezeichnet. Dazu zählt auch das Feuerwerk.

Klassische Beispiele für die Oxidation durch Sauerstoff sind alle Arten der Verbrennung von kohlenstoffhaltigen Stoffen unter Luftsauerstoff, beispielsweise Verbrennung von Kohle, Holz, Erdgas, Flüssiggas, Benzin im Motor, Kerzen usw. Ausgehend von Kohle (reiner Kohlenstoff) gibt jedes Kohlenstoff-Atom vier Elektronen an zwei Sauerstoff-Atome zur Ausbildung von zwei Doppelbindungen ab. Es entsteht Kohlenstoffdioxid (CO).

Nahrung wird im Körper in den vielen Schritten des biochemischen Stoffwechsels unter anderem zu körpereigenen Stoffen, Kohlenstoffdioxid (CO) und Wasser oxidiert. Ein Beispiel ist die β-Oxidation von Fettsäuren. Auch die enzymatische Bräunung beruht auf der Oxidation. Nicht nur "in vivo", auch "in vitro" können organische Stoffe auf vielfältige Weise mit Sauerstoff reagieren: Ein primärer Alkohol (Alkanol) wird sanft oxidiert. Dabei entsteht zunächst ein Aldehyd (Alkanal), bei nochmaliger Oxidation eine Carbonsäure (Alkansäure). Bei energischerer Oxidation kann der Schritt zum Aldehyd übersprungen werden, so dass die Oxidation des primären Alkohols direkt zur Isolation einer Carbonsäure führt. Wird ein sekundärer Alkohol oxidiert, so bildet sich dabei ein Keton (Alkanon). Tertiäre Alkohole können auf Grund ihrer bereits vorhandenen drei C-Bindungen nicht zu einer Carbonylverbindung oxidiert werden.

Eisen rostet (korrodiert) unter dem Einfluss von Sauerstoff und bildet verschiedene Eisenoxide (Rost, FeO, FeO, FeO). Aluminium überzieht sich durch Luftoxidation mit einer Schutzschicht aus Aluminiumoxid.

Bei der Reaktion von Wasserstoff mit Sauerstoff (siehe "Knallgas") entsteht Wasser ("Wasserstoffoxid", HO):

Auch heute noch assoziiert man mit dem Begriff Oxidation vielfach die Umsetzung mit (Luft-)Sauerstoff und die Bildung von Oxiden. Jedoch ist im Rahmen der allgemeineren Definition diese Reaktion nur eine von vielen, die sich mit Hilfe der Valenzelektronentheorie erklären lässt.

Reagiert z. B. ein Metallatom mit einem Sauerstoff-Atom, so kann man die Oxidation des Metalls und somit die Metalloxidbildung anhand folgender Reaktionsgleichungen nachvollziehen:

Sauerstoff hat in diesem Fall die Tendenz, durch Aufnahme von zwei Elektronen eine stabile Valenzelektronenschale mit insgesamt acht Elektronen aufzubauen (Oktettregel). Das Metall wiederum kann durch Abgabe der Elektronen teilbesetzte Schalen auflösen und so die nächstniedrigere stabile Elektronenkonfiguration erreichen.

Der erweiterte Begriff der Oxidation wird heute auf Reaktionen angewandt, die nach dem gleichen chemischen Prinzip ablaufen, auch wenn kein Sauerstoff daran beteiligt ist. In diesem weiteren Sinne bedeutet Oxidation das Abgeben von Elektronen. Zum Beispiel gibt bei der Reaktion von Natrium und Chlor zu Natriumchlorid das Natriumatom ein Elektron an das Chloratom ab, Natrium wird also oxidiert. Im Gegenzug wird Chlor dabei reduziert.




Da Chlor nur molekular als Cl in die Reaktion eingeht, schreibt man stöchiometrisch richtig:


</doc>
<doc id="3767" url="https://de.wikipedia.org/wiki?curid=3767" title="Liste der Städte und Gemeinden in Mecklenburg-Vorpommern">
Liste der Städte und Gemeinden in Mecklenburg-Vorpommern

Das deutsche Bundesland Mecklenburg-Vorpommern besteht aus insgesamt

Diese verteilen sich wie folgt:

<nowiki>*</nowiki> "die amtsfreie Stadt Grevesmühlen bildet mit dem Amt Grevesmühlen-Land eine Verwaltungsgemeinschaft"

54 Städte und 656 Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte zu 76 Ämtern zusammengeschlossen. In 29 dieser Ämter übernimmt ein hauptamtlicher Bürgermeister einer Gemeinde (meist der Bürgermeister einer amtsangehörigen Stadt) als Leitender Verwaltungsbeamter (LVB) die Verwaltung der anderen zum Amt gehörenden Gemeinden.

(sind in der nachfolgenden alphabetischen Liste aller Gemeinden Mecklenburg-Vorpommerns ebenfalls enthalten):


(sind in der nachfolgenden alphabetischen Liste aller Gemeinden Mecklenburg-Vorpommerns ebenfalls enthalten):

Alle politisch selbständigen Städte und Gemeinden in Mecklenburg-Vorpommern (Städte sind fett dargestellt):

Ein hauptamtlicher Bürgermeister einer amtsangehörigen Stadt übernimmt hier als Leitender Verwaltungsbeamter (LVB) die Verwaltung der anderen zum Amt gehörenden Gemeinden.



</doc>
<doc id="3768" url="https://de.wikipedia.org/wiki?curid=3768" title="Liste der Städte und Gemeinden in Baden-Württemberg">
Liste der Städte und Gemeinden in Baden-Württemberg

Das deutsche Bundesland Baden-Württemberg besteht aus insgesamt

Diese verteilen sich wie folgt:

440 Städte und Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte in 114 Gemeindeverwaltungsverbände zusammengeschlossen und 471 Städte und Gemeinden haben sich zu 156 Vereinbarten Verwaltungsgemeinschaften zusammengeschlossen.

Des Weiteren bestehen in Baden-Württemberg zwei unbewohnte Gemeindefreie Gebiete, der „Gutsbezirk Münsingen“ im Landkreis Reutlingen sowie der „Gemeindefreie Grundbesitz“ Rheinau (Ortenaukreis).

Bei den Stadtkreisen handelt es sich, mit Ausnahme von Baden-Baden, um Großstädte. Eine weitere Großstadt ist Reutlingen (siehe bei „Große Kreisstädte“).

Alle politisch selbständigen Städte und Gemeinden in Baden-Württemberg(Städte sind fett dargestellt):


(keine politisch selbständigen Gemeinden) insgesamt 2690 Orte mit zugehöriger Gemeinde:


</doc>
<doc id="3769" url="https://de.wikipedia.org/wiki?curid=3769" title="Liste der Städte und Gemeinden in Bayern">
Liste der Städte und Gemeinden in Bayern

Der Freistaat Bayern besteht aus insgesamt

Diese verteilen sich wie folgt:

982 Städte, Märkte und Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte in 311 Verwaltungsgemeinschaften zusammengeschlossen.

Des Weiteren bestehen in Bayern 179 gemeindefreie Gebiete (überwiegend Forste und Seen), siehe Liste der gemeindefreien Gebiete in Bayern.

25 kreisfreie Städte: (sind in der nachfolgenden alphabetischen Liste aller Gemeinden Bayerns ebenfalls enthalten):
29 Große Kreisstädte (sind in der nachfolgenden alphabetischen Liste aller Gemeinden Bayerns ebenfalls enthalten):


Die von den ausgeschriebenen Ortsnamen abweichenden amtlichen Schreibweisen von Städten und Gemeinden in Bayern sind nachfolgend aufgeführt:
Umfassend verzeichnet sind Dörfer bzw. Ortsteile bei:
Ortsdatenbank der Bayerischen Landesbibliothek Online.de.


</doc>
<doc id="3770" url="https://de.wikipedia.org/wiki?curid=3770" title="Liste der Bezirke und Ortsteile Berlins">
Liste der Bezirke und Ortsteile Berlins

Berlin ist ein Stadtstaat, der aus mehreren Bezirken besteht. Diese Liste der Bezirke und Ortsteile Berlins gibt eine Übersicht über die zwölf Bezirke und 96 Ortsteile der deutschen Hauptstadt.

Jeder Bezirk Berlins besitzt eine parlamentsähnlich ausgestattete Bezirksverordnetenversammlung, die allerdings Teil der Verwaltung ist. Jeder Bezirk verfügt über ein Bezirksamt, dem die Stadträte mit den ihnen nachgeordneten Verwaltungsteilen angehören, außerdem wird in jedem Bezirk ein Bürgermeister gewählt.

Die Bezirke bestehen jeweils aus mehreren Ortsteilen, die meist historischen Ursprungs sind, ehemals selbstständige Städte, Dörfer oder Landgemeinden und als solche die Identifikationszellen der Stadt, die daraus am 1. Oktober 1920 zu Groß-Berlin vereinigt und in damals 20 Bezirken unterteilt wurde. Da Berlin seitdem eine Einheitsgemeinde ist, besitzen die einzelnen Bezirke keine Rechtspersönlichkeit im Sinne einer Gemeinde, sie besitzen aber eigene Bezirksverwaltungen als Teil der zweistufigen Berliner Verwaltung. Bezirke und Ortsteile sind administrativ vierstellig durch Ziffern gekennzeichnet, zwei für den Bezirk und zwei weitere zugehörig für die Ortsteile.

Neben der Einteilung in Ortsteile besteht eine kleinräumige Gliederung der Berliner Bezirke in 195 "Statistische Gebiete," die mit dreistelligen Ziffern gekennzeichnet sind. Ein solches Statistisches Gebiet ist zum Beispiel das "Bayerische Viertel" in Schöneberg, in dem die Straßen vor allem nach Städten in Bayern benannt sind. Die Statistischen Gebiete entsprechen annähernd den Ortslagen oder den Wohngebieten (Kiez). Sie können über Ortsteilgrenzen hinwegreichen; in solchen Fällen ist eine eindeutige Zuordnung eines Statistischen Gebiets zu einem bestimmten Ortsteil nicht möglich. So liegt zum Beispiel das "Statistische Gebiet Rudow" teilweise im Ortsteil Rudow und teilweise im Ortsteil Gropiusstadt.

Für spezielle Zwecke gibt es weitere Unterteilungen der Statistischen Gebiete. So werden sie beispielsweise in der Verkehrsplanung jeweils in benannte Verkehrszellen unterteilt und diese wiederum in unbenannte Teilverkehrszellen. Ein Beispiel wären die beiden Teilverkehrszellen 00521 und 00522 der Verkehrszelle 0052 (Potsdamer Brücke) im Statistischen Gebiet 005 (Lützowplatz), das zum Ortsteil 0104 (Tiergarten) im Bezirk 01 (Mitte) gehört.

Im gemeinen Sprachgebrauch gehen die Begriffe zur Verwaltungsteilung häufig durcheinander. Sowohl die amtlichen Bezirke als auch die amtlichen Ortsteile werden mitunter als „Stadtteile“ oder „Bezirke“ bezeichnet. Die Ortslagen werden oft als „unselbstständige Ortsteile“ oder auch fälschlich als „Ortsteile“ bezeichnet. Dies rührt oft daher, dass bis zur Verwaltungsreform im Jahr 2002 viele der heutigen amtlichen Ortsteile selbst Bezirke waren oder den Namen ehemaliger Bezirke tragen, die in nur einem weiteren Schritt in übliche Ortslagen differenziert wurden, die man bis dahin üblich Ortsteil nannte.

Bis zum Jahr 2000 gab es in Berlin 23 Bezirke. Im Zuge der letzten Verwaltungsreform "(Bezirksgebietsreform)" im Jahre 2001 wurde die Zahl durch Fusion von jeweils zwei oder drei Bezirken auf zwölf reduziert. Von der Bezirksfusion ausgenommen waren Spandau, Reinickendorf und Neukölln, die bereits über 200.000 Einwohner zählten.

Die Bezirke sind amtlich in 96 Ortsteile gegliedert, deren Namen auch im Stadtbild auftauchen, sowohl auf Wegweisern als auch auf Schildern an den Ortszufahrten (auf rechteckigen grünen Schildern mit gelber Schrift, die teilweise noch die früheren, heute nicht mehr gültigen Bezirks- oder Ortsteilnamen anzeigen).



</doc>
<doc id="3771" url="https://de.wikipedia.org/wiki?curid=3771" title="Liste der Städte und Gemeinden in Brandenburg">
Liste der Städte und Gemeinden in Brandenburg

Das deutsche Bundesland Brandenburg besteht aus insgesamt

Diese verteilen sich wie folgt:

31 Städte und 239 Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte zu 52 Ämtern zusammengeschlossen.

In Brandenburg erhielten kreisangehörige Städte ab 45.000 Einwohnern bzw. 25.000 Einwohner gemäß § 2 Abs. 3 Gemeindeordnung für das Land Brandenburg a.F. den Status einer Großen bzw. Mittleren kreisangehörigen Stadt. Mit Einführung der Brandenburgischen Kommunalverfassung (BbgKVerf) wurde der Status der Mittleren kreisangehörigen Stadt abgeschafft, allerdings wurde in § 141 Abs. 2 BbgKVerf eine Bestandsschutzregelung aufgenommen. Zudem genügen nunmehr gemäß § 1 Abs. 3 BbgKVerf für das Erreichen des Status einer Großen kreisangehörigen Stadt 35.000 Einwohner.

Bei offiziell zweisprachigen Städten und Gemeinden ist neben dem deutschen auch der niedersorbische Name angegeben.

(in der nachfolgenden alphabetischen Liste sind alle Städte und Gemeinden Brandenburgs aufgeführt):




</doc>
<doc id="3772" url="https://de.wikipedia.org/wiki?curid=3772" title="Bezirke in Hamburg">
Bezirke in Hamburg

Die Freie und Hansestadt Hamburg ist als Land und Einheitsgemeinde seit 1951 in sieben Bezirke gegliedert, in denen Bezirksämtern die selbständige Erledigung übertragener Aufgaben obliegt. Die Bezirksämter sind für eine Reihe dezentral wahrzunehmender Verwaltungsaufgaben zuständig, insbesondere im Sozial-, Gesundheits-, Bau-, Melde-, Wohnungs- und Liegenschaftswesen sowie im Bereich der Wirtschaftsüberwachung. Jeder Bezirk hat zur Beratung und Mitwirkung der Bürger an Verwaltung und bezirkspolitischen Themen eine Bezirksversammlung.

Die sieben Bezirke Hamburgs sind in insgesamt 104 Stadtteile untergliedert (seit 1. Januar 2011), die aus einem oder mehreren Ortsteilen bestehen. Stadt- und Ortsteile sind geografische und statistische, aber keine politischen Einheiten. Der Zuschnitt der Bezirke wurde so gewählt, dass die meisten Bezirke sowohl zentral gelegene Stadtteile als auch weiter außerhalb liegende Gebiete an der Landesgrenze umfasst.

Die Bezirke bestehen jeweils aus mehreren "Stadtteilen", deren Namen und Grenzen oftmals historisch gewachsen, zum Teil auch jüngeren Datums sind. Für statistische Zwecke sind viele Stadtteile nochmals in mehrere "Ortsteile" unterteilt. Die 181 Ortsteile Hamburgs (seit 1. Januar 2011) werden mit einer dreistelligen Nummer bezeichnet, deren erste Stelle jeweils für den Bezirk steht (Beispiel: Der Stadtteil Neustadt im Bezirk Hamburg-Mitte besteht aus den vier Ortsteilen 105 bis 108. Der relativ kleine Stadtteil Cranz im Bezirk Harburg lediglich aus einem Ortsteil 718, bei identischer Stadt- und Ortsteilgrenze).

Der flächengrößte Stadtteil ist Wilhelmsburg (35,3 km²), der kleinste der 2008 neu gebildete Stadtteil Sternschanze (0,47 km²; zuvor Hoheluft-Ost mit 0,581 km²). Der bevölkerungsreichste Stadtteil ist Rahlstedt mit 86.962 Einwohnern (Stand 2011), während für Altenwerder nur drei Einwohner verzeichnet sind. 

Neben diesen offiziellen Untergliederungen bestehen weitere historische oder in jüngerer Zeit entstandene Bezeichnungen für Ortsteile, Viertel, Quartiere, oder für landschaftlich größere Gebiete und weitere geografische Einheiten (beispielsweise Flur- und Inselnamen).

Hamburger Bezirk mit 19 Stadtteilen:
Hamburg-Altstadt,
Billbrook,
Billstedt,
Borgfelde,
Finkenwerder,
HafenCity,
Hamm,
Hammerbrook,
Horn,
Kleiner Grasbrook,
Neustadt,
Neuwerk (Exklave),
Rothenburgsort,
St. Georg,
St. Pauli,
Steinwerder,
Veddel,
Waltershof,
Wilhelmsburg.

Hamburger Bezirk mit 14 Stadtteilen:
Altona-Altstadt,
Altona-Nord,
Bahrenfeld,
Blankenese,
Groß Flottbek,
Iserbrook,
Lurup,
Nienstedten,
Osdorf,
Othmarschen,
Ottensen,
Rissen,
Sternschanze,
Sülldorf.

Hamburger Bezirk mit 9 Stadtteilen:
Eidelstedt,
Eimsbüttel,
Harvestehude,
Hoheluft-West,
Lokstedt,
Niendorf,
Rotherbaum,
Schnelsen,
Stellingen.

Hamburger Bezirk mit 13 Stadtteilen:
Alsterdorf,
Barmbek-Nord,
Barmbek-Süd,
Dulsberg,
Eppendorf,
Fuhlsbüttel,
Groß Borstel,
Hohenfelde,
Hoheluft-Ost,
Langenhorn,
Ohlsdorf,
Uhlenhorst,
Winterhude.

Hamburger Bezirk mit 18 Stadtteilen:
Bergstedt,
Bramfeld,
Duvenstedt,
Eilbek,
Farmsen-Berne,
Hummelsbüttel,
Jenfeld,
Lemsahl-Mellingstedt,
Marienthal,
Poppenbüttel,
Rahlstedt,
Sasel,
Steilshoop,
Tonndorf,
Volksdorf,
Wandsbek,
Wellingsbüttel,
Wohldorf-Ohlstedt.

Die drei Karten zeigen noch nicht den neuen (ab 1. Januar 2011) Stadtteil Neuallermöhe.

Hamburger Bezirk mit 14 Stadtteilen:
Allermöhe,
Altengamme,
Bergedorf,
Billwerder,
Curslack,
Kirchwerder,
Lohbrügge,
Moorfleet,
Neuallermöhe,
Neuengamme,
Ochsenwerder,
Reitbrook,
Spadenland,
Tatenberg.

Hamburger Bezirk mit 17 Stadtteilen:
Altenwerder,
Cranz,
Eißendorf,
Francop,
Gut Moor,
Harburg,
Hausbruch,
Heimfeld,
Langenbek,
Marmstorf,
Moorburg,
Neuenfelde,
Neugraben-Fischbek,
Neuland,
Rönneburg,
Sinstorf,
Wilstorf.

Wie auch andere Freie Reichsstädte konnte Hamburg im Laufe der Geschichte zahlreiche umliegende Dörfer und Städte für sich gewinnen und unter seine Hoheit stellen bzw. gemeinsam mit anderen Staaten verwalten. Das Staatsgebiet der Freien Reichsstadt Hamburg bestand daher aus dem eigentlichen Stadtgebiet (innerhalb der Stadtmauern und -wälle) und dem so genannten Landgebiet, also einer Vielzahl von Landgemeinden, darunter die äußeren Stadtteile der heutigen Bezirke Hamburg-Mitte, Hamburg-Nord und Eimsbüttel sowie die so genannten Walddörfer. Die meisten dieser Landgemeinden bildeten mit der Stadt Hamburg ein geschlossenes Staatsgebiet, jedoch lagen einige auch als Exklaven vollständig außerhalb (z. B. Amt Ritzebüttel an der Elbmündung). Ebenso bestanden kleine Enklaven, wie in den Vierlanden, die wie die Stadt Bergedorf gemeinschaftlich mit der Freien und Hansestadt Lübeck verwaltet wurden, bevor die Hoheitsrechte 1868 gänzlich an Hamburg fielen.

Das Landgebiet wurde seit dem 15. Jahrhundert in Landherrenschaften eingeteilt, deren Zuschnitt sich im Laufe der Zeit mehrfach änderte. Bei der Reichsgründung 1871 war das Staatsgebiet Hamburgs in folgende Bereiche eingeteilt:


Mit der Einführung der Hamburgischen Städteordnung am 2. Januar 1924 wurden Bergedorf, Cuxhaven und Geesthacht eigenständige Städte im Hamburger Staatsgebiet und schieden aus den Landherrenschaften aus. 

Nicht zum hamburgischen Staatsgebiet gehörten damals die selbständigen Städte Altona, Wandsbek (beide seit 1866 zur preußischen Provinz Schleswig-Holstein gehörig) sowie Harburg und Wilhelmsburg (Provinz Hannover). Diese vier kamen erst durch das Groß-Hamburg-Gesetz von 1937 zu Hamburg.

Nach dem Inkrafttreten des Groß-Hamburg-Gesetzes am 1. April 1937 bestand das nunmehr deutlich vergrößerte Staatsgebiet vorübergehend aus fünf Städten (Hamburg, Altona, Harburg-Wilhelmsburg, Wandsbek und Bergedorf), dem bisherigen Hamburger Landgebiet sowie dem neuen "Landkreis Hamburg", der aus 27 ehemals preußischen Gemeinden gebildet wurde.

Erst durch die Einführung der Deutschen Gemeindeordnung am 1. April 1938 wurde Hamburg formell zu einer Gemeinde zusammengeschlossen und zugleich die Verwaltung in eine "staatliche" und eine "kommunale" Ebene getrennt. Auf der staatlichen Ebene wurden die städtisch besiedelten Gebiete zu einem "Stadtbezirk" und die ländlichen Vororte zu einem "Landbezirk" nach dem Vorbild der bisherigen Landherrenschaft zusammengefasst. Letzterer unterstand Polizeisenator Alfred Richter als „Landbürgermeister“ und umfasste neben der Hauptverwaltung auch 20 dezentrale Dienststellen. 

Zusätzlich wurde das Hamburger Stadtgebiet wiederum ein Jahr später, am 1. April 1939, in enger Anlehnung an die Parteistruktur der NSDAP in zehn "Kreise" eingeteilt, denen 110 „Bezirke“ (entsprechend in etwa den heutigen Stadtteilen) und 178 Ortsteile nachgeordnet waren. Diese Kreiseinteilung nahm in vielerlei Hinsicht die spätere Bezirksgliederung vorweg:
Nach den verheerenden Luftangriffen im Juli 1943 und der darauffolgenden Evakuierung großer Teile der Bevölkerung wurden die am schwersten betroffenen Kreise 2, 3, 5 und 6 auf ihre Nachbarkreise aufgeteilt. Da auch der öffentliche Nahverkehr weitgehend zusammengebrochen war, wurden ab 15. November 1943 zusätzlich 23 dezentrale Ortsämter eingerichtet, die den jeweiligen Kreisen zugeordnet waren und auch nach Kriegsende weiter fortbestanden:

Nach 1945 wurde zunächst die Trennung in Staats- und Gemeindeverwaltung wieder aufgehoben, ebenso die Kreisverwaltungen mit Ausnahme von Harburg und Bergedorf. 

Am 21. September 1949 beschloss die Hamburgische Bürgerschaft das "Gesetz über die Bezirksverwaltung in der Freien und Hansestadt Hamburg", das am 11. Mai 1951 in Kraft trat und die heutigen sieben Bezirke schuf. Grundlage für die Beratungen war ein dem Senat 1948 vorgelegtes Gutachten "Die Neuordnung der kommunalen Verwaltung der Hansestadt Hamburg" von Oskar Mulert, dem ehemaligen geschäftsführenden Präsidenten des Deutschen Städtetages. Dieses sah ein „sektorales“ Gliederungsprinzip vor, demzufolge jeder Bezirk sowohl Bereiche der verdichteten inneren Stadt als auch weniger verdichtete Außengebiete aufweisen sollte. 

Den neu geschaffenen Bezirksämtern wurden auch die bestehenden "Ortsämter" und Ortsdienststellen nachgeordnet. Zur Beratung kommunalpolitischer Themen und zur Mitwirkung der Bürger an der Verwaltung wurden in allen Bezirken "Bezirksausschüsse" geschaffen, die 1961 in Bezirksversammlungen umbenannt wurden.

Bis zum 29. Februar 2008 gliederte sich jeder Bezirk in ein Kerngebiet und ein bis vier Ortsamtsgebiete. Im Zuge der Bezirksverwaltungsreform wurden die Ortsämter und die Ortsausschüsse aufgelöst. Bereits zum 1. Februar 2007 wurden die Bezirksämter einheitlich neu organisiert und die bis dahin vorhandenen Ortsämter (mit den vorstehenden Ortsamtsleitern) und Ortsdienststellen aufgelöst. Die Aufgaben der ehemaligen Ortsämter werden nunmehr durch die Bezirksämter und regionale Kundenzentren wahrgenommen und die ehemaligen Ortsausschüsse durch Regionalausschüsse ersetzt.
Durch das am 18. Juli 2006 durch die Hamburgische Bürgerschaft beschlossene "Gesetz über die räumliche Gliederung der Freien und Hansestadt Hamburg" traten zudem zum 1. März 2008 die folgenden Gebietsänderungen in Kraft:

Die bis 1937 hamburgischen Inseln Neuwerk und Scharhörn mit einem Teil des Wattenmeeres (und der später entstandenen Insel Nigehörn) gingen 1969 wegen eines geplanten, aber nie gebauten Tiefwasserhafens vom Land Niedersachsen wieder an Hamburg. Das Gebiet in der Elbmündung wurde als Stadtteil Neuwerk dem Bezirk Hamburg-Mitte zugeordnet. Mitte der 1970er Jahre gab es zudem im Bezirk Bergedorf (im Stadtteil Altengamme) geringfügige Änderungen durch Korrekturen der Landesgrenzen von Hamburg, Niedersachsen und Schleswig-Holstein im Bereich der Staustufe Geesthacht.

Abgesehen von den zeitgleich mit der Bezirksverwaltungsreform 2008 vorgenommenen Änderungen der Stadtteile wurde 1961 der Stadtteil Farmsen in Farmsen-Berne umbenannt und 1970 im Bezirk Hamburg-Mitte der Stadtteil Billwerder Ausschlag aufgehoben und dem Gebiet von Rothenburgsort zugeordnet. Zum 1. Januar 2011 wurden im Bezirk Hamburg-Mitte die Stadtteile Hamm-Nord, Hamm-Mitte und Hamm-Süd zum Stadtteil Hamm zusammengelegt (Ortsteilgliederung 121-127 blieb) und im Bezirk Bergedorf der Stadtteil Neuallermöhe neu geschaffen. Dieser setzt sich aus den Wohngebieten Neuallermöhe-Ost (vorher Gebietsteil des Stadtteils Bergedorf) und Neuallermöhe-West (zuvor Teil von Allermöhe) zusammen. Gleichzeitig wurde die Siedlung Alt-Nettelnburg aus dem Stadtteil Allermöhe nun dem Stadtteil Bergedorf zugeordnet und ein kleines Gelände an der Bahnstrecke vom Stadtteil Billwerder ebenfalls an Bergedorf übertragen.

In den Bezirken besteht jeweils ein Bezirksamt, das mit seinen verschiedenen Dezernaten, Fachämtern und Dienstleistungszentren dezentrale und ortsnahe Verwaltungsaufgaben wahrnimmt. Für Bürger wurden im Zuge der Bezirksverwaltungsreform "Kundenzentren" bei den Bezirksämtern (bzw. an Stelle der ehemaligen Ortsämter) für Aufgaben des Einwohnermeldeamtes eingerichtet. Zum Teil noch im Aufbau sind die "Sozialen Dienstleistungszentren" für staatliche Transfer- und Unterstützungsleistungen und die "Zentren für Wirtschaftsförderung, Bauen und Umwelt".

Daneben üben die dem Senat unterstehenden Fachbehörden (Ministerien in Flächenländern) und Ämter auch kommunale Aufgaben im gesamten Gebiet der Freien und Hansestadt Hamburg aus. Die Dienstaufsicht (Bezirksaufsichtsbehörde) obliegt dem auch für weitere Angelegenheiten der Bezirksverwaltung zuständigen Amt 6 der Finanzbehörde, das aus dem Senatsamt für Bezirksangelegenheiten hervorgegangen ist.

Die Bezirke verfügen jeweils über eine "Bezirksversammlung", die seit 2014 alle fünf Jahre parallel zur Europawahl in direkter Wahl gewählt wird. Die Mitglieder der Bezirksversammlungen werden mitunter auch als „Bezirksabgeordnete“ bezeichnet, obgleich sie keine Abgeordneten sind. An der Spitze der Verwaltung (des "Bezirksamtes") steht der "Bezirksamtsleiter", der von der Bezirksversammlung gewählt wird, jedoch zur Amtsübernahme der Bestätigung durch den Senat bedarf.

Die Selbstverwaltungsrechte der Bezirke entsprechen jedoch nicht denen von Gemeinden in anderen Bundesländern. Dies kommt unter anderem darin zum Ausdruck, dass der Bezirksamtsleiter zum Amtsantritt der Bestätigung durch den Senat bedarf und dass Entscheidungen der bezirklichen Instanzen vom Senat außer Kraft gesetzt werden können, indem er die betreffende Angelegenheit per Evokation an sich zieht oder fachlich zuständige Senatoren bindende "Einzelweisungen" erlassen.



</doc>
<doc id="3773" url="https://de.wikipedia.org/wiki?curid=3773" title="Liste der Städte und Gemeinden in Hessen">
Liste der Städte und Gemeinden in Hessen

Das deutsche Bundesland Hessen besteht aus insgesamt

Diese verteilen sich wie folgt:

5 kreisfreie Städte:

7 kreisangehörige Städte mit Sonderstatus:

Alle politisch selbständigen Städte und Gemeinden Hessens (Städte sind fett dargestellt):





</doc>
<doc id="3774" url="https://de.wikipedia.org/wiki?curid=3774" title="Liste der Ortsteile in Niedersachsen">
Liste der Ortsteile in Niedersachsen


</doc>
<doc id="3775" url="https://de.wikipedia.org/wiki?curid=3775" title="Liste der Städte und Gemeinden in Nordrhein-Westfalen">
Liste der Städte und Gemeinden in Nordrhein-Westfalen

Das Land Nordrhein-Westfalen besteht aus insgesamt
Diese verteilen sich wie folgt:

Die 30 einwohnerreichsten Städte sind in folgender Tabelle dargestellt, davon sind 29 Großstädte:

Die kleinste Gemeinde des Bundeslandes und auch kleinste Verwaltungseinheit in Nordrhein ist Dahlem ( Einwohner) im Kreis Euskirchen, die kleinste Stadt ist Heimbach im Kreis Düren, sie zählt Einwohner. Die kleinste Verwaltungseinheit im westfälischen Landesteil ist die nur unwesentlich größere Stadt Hallenberg ( Einwohner) im Hochsauerlandkreis.

Für einen Überblick über die Gebietsreform in Nordrhein-Westfalen auf Gemeindeebene siehe Liste aller Gemeinden Nordrhein-Westfalens A–E, F–K, L–R und S–Z

In Nordrhein-Westfalen gibt es die folgenden 22 kreisfreien Städte. Rechnet man den Sonderfall Aachen dazu, sind es 23. (Stand: ):

Das Land Nordrhein-Westfalen gliedert sich in 22/23 kreisfreie Städte (s. o.) und 374/373 kreisangehörige Gemeinden mit den Funktionsbezeichnungen

Große und Mittlere kreisangehörige Städte werden nach der Gemeindeordnung durch Rechtsverordnung bestimmt und nehmen zusätzliche Aufgaben wahr. Die Zugehörigkeit orientiert sich an der Einwohnerzahl, die an mehreren Stichtagen bestimmte Werte über- oder unterschreiten muss. Städte mit mehr 60.000 Einwohnern werden von Amts wegen zur Großen kreisangehörigen Stadt bestimmt, Städte mit mehr als 50.000 Einwohnern können dies beantragen. Sie verlieren diesen Status von Amts wegen, wenn sie weniger als 45.000 Einwohner haben. Bei mehr als 25.000 Einwohnern wird eine Gemeinde von Amts wegen eine Mittlere kreisangehörige Stadt, bei mehr als 20.000 Einwohnern kann sie dies beantragen. Sinkt die Einwohnerzahl unter 15.000, wird sie aus der Liste der Mittleren kreisangehörigen Städte gestrichen.

Die Bezeichnung „Stadt“ führen die Gemeinden, denen dies nach bisherigen Recht zusteht, sowie alle Großen und Mittleren kreisangehörigen Städte. Diese Bezeichnung ist unabhängig von der künftigen Einwohnerentwicklung.

"(i.d.R. mehr als 60.000 Einwohner; Anzahl: 35; 6 davon sind Großstädte)"
(*) weniger als 60.000 Einwohner

"(i.d.R. 25.000 bis 60.000 Einwohner)"
Anzahl: 129
(*) weniger als 25.000 Einwohner
(**) weniger als 20.000 Einwohner

"(unter 25.000 Einwohner, Bezeichnung „Stadt“ nach bisherigem Recht)"
Anzahl: 84

"(unter 25.000 Einwohner)"
Anzahl: 125


</doc>
<doc id="3776" url="https://de.wikipedia.org/wiki?curid=3776" title="Liste der Städte und Gemeinden in Rheinland-Pfalz">
Liste der Städte und Gemeinden in Rheinland-Pfalz

Das Land Rheinland-Pfalz besteht aus insgesamt

Diese verteilen sich wie folgt:

95 Städte und 2167 Ortsgemeinden gehören 143 Verbandsgemeinden an, die für die angehörenden Gemeinden die Verwaltungsgeschäfte übernehmen.

Die Kommunen gehörten bis Ende 1999 einem der drei Regierungsbezirken an (Koblenz, Trier und Rheinhessen-Pfalz).



</doc>
<doc id="3777" url="https://de.wikipedia.org/wiki?curid=3777" title="Liste der Ortsteile im Saarland">
Liste der Ortsteile im Saarland

Nicht selbständige Stadt- und Gemeindeteile des deutschen Bundeslandes Saarland

Die Liste ist systematisch überprüft und dürfte vollständig sein








</doc>
<doc id="3778" url="https://de.wikipedia.org/wiki?curid=3778" title="Liste der Städte und Gemeinden in Sachsen">
Liste der Städte und Gemeinden in Sachsen

Das deutsche Bundesland Freistaat Sachsen besteht aus insgesamt 421 politisch selbständigen Städten und Gemeinden (Stand: 1. Januar 2018).

Diese verteilen sich wie folgt:

49 Städte und 113 Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte in 66 Verwaltungsgemeinschaften und 21 Gemeinden in 6 Verwaltungsverbänden eingebunden.

Am 3. Oktober 1990 gab es noch 1.626 Gemeinden. Seitdem ist die Zahl um Dreiviertel gesunken.


Den Status Große Kreisstadt können Städte mit mehr als 17.500 Einwohnern oder Städte, die mit den Kreisreformen 1994, 1996 und 2008 ihre Funktion als Kreisstadt verloren haben, besitzen.

Alle politisch selbständigen Städte und Gemeinden Sachsens mit Angaben zu Einwohnerzahl, Fläche, Bevölkerungsdichte und dem Landkreis (Städte sind fett dargestellt, Daten mit Stand vom ):



</doc>
<doc id="3779" url="https://de.wikipedia.org/wiki?curid=3779" title="Liste der Städte und Gemeinden in Sachsen-Anhalt">
Liste der Städte und Gemeinden in Sachsen-Anhalt

Das deutsche Land Sachsen-Anhalt besteht aus insgesamt

Diese verteilen sich wie folgt:

(sind in der nachfolgenden alphabetischen Liste aller Gemeinden Sachsen-Anhalts ebenfalls enthalten):

Alle politisch selbständigen Städte und Gemeinden in Sachsen-Anhalt mit ihrer Einwohnerzahl vom (Städte sind fett dargestellt):



</doc>
<doc id="3780" url="https://de.wikipedia.org/wiki?curid=3780" title="Liste der Städte und Gemeinden in Schleswig-Holstein">
Liste der Städte und Gemeinden in Schleswig-Holstein

Das Land Schleswig-Holstein besteht aus insgesamt

Diese verteilen sich wie folgt:

12 Städte und 1010 Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte in 84 Ämtern zusammengeschlossen. Zwei Ämter sind davon kreisübergreifend: Amt Großer Plöner See (seit dem 1. Januar 2007) und Amt Itzstedt (seit dem 1. Januar 2008).

Des Weiteren bestehen in Schleswig-Holstein zwei unbewohnte Forstgutsbezirke (gemeindefreie Gebiete): Buchholz und Sachsenwald.

(sind in der nachfolgenden alphabetischen Liste aller Gemeinden Schleswig-Holsteins ebenfalls enthalten):

(sind in der nachfolgenden alphabetischen Liste aller Gemeinden Schleswig-Holsteins ebenfalls enthalten):

Alle politisch selbständigen Städte und Gemeinden in Schleswig-Holstein (Städte sind fett dargestellt):



</doc>
<doc id="3781" url="https://de.wikipedia.org/wiki?curid=3781" title="Liste der Städte und Gemeinden in Thüringen">
Liste der Städte und Gemeinden in Thüringen

Das deutsche Bundesland Thüringen besteht aus insgesamt

Diese verteilen sich wie folgt:


601 Städte und Gemeinden haben sich zur Erledigung ihrer Verwaltungsgeschäfte in 69 Verwaltungsgemeinschaften zusammengeschlossen.

Eine Besonderheit in Thüringen sind erfüllende Gemeinden. 39 Städte und Gemeinden, die keiner Verwaltungsgemeinschaft angehören, sind erfüllende Gemeinden für 98 weitere Gemeinden.

13 der Städte und Gemeinden werden als Landgemeinden bezeichnet. Sie sind allerdings gleichzeitig Städte bzw. Gemeinden.

Alle politisch selbständigen Städte und Gemeinden Thüringens (Städte sind fett und Landgemeinden "kursiv" dargestellt):



</doc>
<doc id="3782" url="https://de.wikipedia.org/wiki?curid=3782" title="Liste der Verwaltungsebenen in Bremerhaven">
Liste der Verwaltungsebenen in Bremerhaven

Die Liste der Verwaltungsebenen in Bremerhaven gibt die aktuelle Einteilung der Verwaltung in der Stadtgemeinde Bremerhaven wieder mit

"Die Sortierung der Stadtbezirke erfolgt gemäß ihren Schlüsselnummern."


"Die Sortierung der Stadtteile erfolgt gemäß ihren Schlüsselnummern. Diese sind offiziell vierstellig, aber die vierte Stelle ist tatsächlich immer die Null. Die Schlüsselnummern sind hierarchisch aufgebaut: Die erste Stelle bezeichnet den Stadtbezirk, die zweite Stelle den Stadtteil und die dritte Stelle den Ortsteil."





</doc>
<doc id="3785" url="https://de.wikipedia.org/wiki?curid=3785" title="OHG">
OHG

Die Abkürzung OHG steht für:

Siehe auch:


</doc>
<doc id="3787" url="https://de.wikipedia.org/wiki?curid=3787" title="Objektdatenbank">
Objektdatenbank

Eine Objektdatenbank oder objektorientierte Datenbank ist eine Datenbank, die auf dem Objektdatenbankmodell basiert. Im Unterschied zur relationalen Datenbank werden Daten hier als Objekte im Sinne der Objektorientierung verwaltet. Das zugehörige Datenbankmanagementsystem wird als das objektorientierte Datenbankmanagementsystem bezeichnet. Objektdatenbank und Objektdatenbankmanagementsystem bilden gemeinsam das Objektdatenbanksystem.

Ein Objekt modelliert normalerweise einen Gegenstand oder Begriff und enthält insbesondere dazugehörige Attribute; so gehört zum Beispiel die Farbe und das Gewicht eines Autos zu dem Objekt Auto. Attribute beschreiben ein Objekt näher. Daten und Methoden (die Funktionen zum Zugriff auf die Daten) werden in den Objekten zusammen abgelegt.

Das Datenbankmanagementsystem (DBMS) ist die Software, die zur Verwaltung einer Datenbank benötigt wird. Im Falle einer Objektdatenbank wird diese als das Objektdatenbankmanagementsystem (ODBMS) bezeichnet. Das ODBMS ist für die Speicherung und den zuverlässigen Zugriff auf die Daten zuständig. Neben den klassischen Eigenschaften eines DBMS hat es folgende zusätzliche Anforderungen zu erfüllen, um als vollwertiges ODBMS verwendbar zu sein:


Neben diesen Eigenschaften gibt es eine Reihe optionaler Anforderungen, die hier nicht im Einzelnen dargestellt sind. Sie wurden auf der Konferenz DOOD’98 festgelegt.

Als Abfragesprache wurde von der ODMG die Sprache Object Query Language (OQL) standardisiert. Als Datenmanipulationssprache wird Object Definition Language (ODL) verwendet.

Objektdatenbanksysteme schließen eine Lücke, die bei der Programmierung moderner Datenbankanwendungen entsteht, wenn die Anwendung in einer objektorientierten Programmiersprache entwickelt wurde, die Datenbank jedoch ein klassisches relationales Datenbanksystem ist. Beide Konzepte widersprechen sich in einigen wichtigen Punkten. Dieses Problem wird allgemein als der „object-relational impedance mismatch“ bezeichnet. Als Lösung für das Problem werden sogenannte objektrelationale Abbildungen verwendet. Dies sind Softwarekomponenten, die zwischen einer relationalen Datenbank und einer objektorientierten Software vermitteln. Durch die Verwendung eines Objektdatenbanksystems wird diese Vermittlung überflüssig. Die Anwendung kann direkt mit der Datenbank kommunizieren.

Das Zusammensetzen komplexer Datenobjekte mittels Joins über mehrere Datenbanktabellen entfällt. Objekte können einfach über die in der Datenbank gespeicherten Beziehungen abgefragt werden.

Weiterhin hilft ein ODBMS beim Zugriff auf Daten. Da Objekte eine komplexe Struktur haben können, sind semantische Zusammenhänge zwischen Objekten dem Datenbanksystem bekannt. Das Datenbanksystem hat also ein Verständnis davon, welche Daten zusammengehören. Dieses Wissen kann bei der Abfrage der Daten mittels einer Abfragesprache wie OQL verwendet werden. Im Gegensatz zu relationalen Datenbanksystemen ist das Ergebnis einer Anfrage nicht eine Menge von Datensätzen. OQL erlaubt die Abfrage einzelner Objekte.

Außerdem wird das Problem der Objektidentität gelöst. Während bei relationalen Datenbanken der Datenbankentwickler oft einen künstlich erzeugten Schlüssel (Surrogate Key) zu seinen Daten hinzufügen muss, wird dies von einem ODBMS automatisch in Form eines OIDs gemacht. Die Verwaltung dieser IDs wird dabei vollständig vom System übernommen.

Objektdatenbanken haben bis heute nur eine geringe Verbreitung. Entsprechend sind viele Schnittstellen und Tools wie JDBC/ODBC, ETL oder OLAP für den Einsatz mit einem ODBMS nicht vorbereitet. 

Bei bestimmten Anfragen sind Objektdatenbanken noch immer im Nachteil gegenüber relationalen Datenbanken. Dies ist beispielsweise durch Zugriffspfade zu Objekten über mehrere Pfadarten (bspw. Vererbung und Assoziation) verursacht. Dies führt bei Schreiboperationen in der Sperrverwaltung zu einer exponentiellen Komplexität und somit zu Performanceproblemen. Die Leistungsprobleme wurden in den objektrelationalen Datenbanken aufgegriffen, in denen nur die Konstrukte aus objektorientierten Datenbanken mit niedrigerer Komplexität (bspw. formula_1) übernommen wurden.

Objektdatenbanken wurden Ende der 1980er Jahre entwickelt. Somit gehören sie zu den vergleichsweise neuen Datenbankkonzepten. Bis heute spielen sie auf dem Datenbankmarkt, der von den relationalen Datenbanksystemen dominiert wird, eine eher geringe Rolle. Dennoch sind seit 2004 mehrere Objektdatenbanksysteme wie zum Beispiel db4o entwickelt worden, die zum Teil als Open Source verfügbar sind.





</doc>
<doc id="3788" url="https://de.wikipedia.org/wiki?curid=3788" title="Onkologie">
Onkologie

Als Onkologie ( "onkos" ‚Anschwellung‘ und -logie) bezeichnet man die Wissenschaft, die sich mit Krebs befasst.

Im engeren Sinne widmet sich die Onkologie der Prävention, Diagnostik, Therapie und Nachsorge von malignen Erkrankungen.

Die dabei involvierten medizinischen Disziplinen sind die Tumore operierenden chirurgischen Fächer (z. B. Chirurgie, Gynäkologie, HNO, Neurochirurgie, Dermatologie, Urologie..), die Radioonkologie und Innere Medizin mit Zusatzausbildung in internistischer Onkologie / Hämatoonkologie.

Die moderne Onkologie ist von der interdisziplinären Zusammenarbeit der je nach Tumorerkrankung involvierten medizinischen Fachrichtungen geprägt. So sollten in jedem Tumorboard immer Vertreter folgender Fachrichtungen verpflichtend anwesend sein: Radioonkologie, internistische Onkologie, diagnostische Radiologie (zur Beurteilung der Tumorausbreitung), Pathologie (zur Beurteilung der Art der Tumorerkrankung) und das jeweilig involvierte chirurgische Fach.

Viele Bemühungen in der Onkologie richten sich darauf, Krebs zu verhindern (Krebsprävention) oder seine Ausbreitung im Körper des Patienten zu unterdrücken. Von zentraler Bedeutung ist es dabei, Risikofaktoren zu erkennen. Dabei arbeiten Onkologen mit Epidemiologen zusammen und werten zum Beispiel Krankengeschichten statistisch aus. Die Kenntnis um Risikofaktoren wird in zweierlei Weise genutzt:

Wesentlicher Teil jeglicher Prävention ist die Forschung über die Krebsentstehung. Daraus können sich neue Wege in der Krebsprävention, Diagnostik und Therapie ergeben. Das internationale Netzwerk baut in Deutschland auf das Deutsche Krebsforschungszentrum (DKFZ) in Heidelberg.

Am Anfang der Krebsdiagnostik steht die Anamnese. Dabei erfragt der Arzt Symptome und Risikofaktoren. Auf dieser Basis werden dann Screeningtests oder spezifischere Untersuchungen empfohlen, und zwar hauptsächlich:

Ergibt oder erhärtet sich der Krebsverdacht, versucht man meist, eine definitive Diagnose anhand der histologischen oder zytologischen Untersuchung einer Gewebeprobe aus dem verdächtigen Bereich zu erzielen. Gleichzeitig wird mittels weiterer Diagnosemethoden das Stadium der Erkrankung bestimmt. Wegen der oft schlechten Prognose bösartiger Erkrankungen einerseits und der Risiken und Nebenwirkungen der Behandlung andererseits ist dieser Schritt besonders wichtig und rechtfertigt viel Aufwand, bis hin zu explorativen Operationen einschließlich Probeexzision.

Die wichtigsten Behandlungsmethoden der Onkologie sind:

Die Therapien der Onkologie zielen entweder auf die Entfernung oder Zerstörung des gesamten Tumorgewebes (kurative Therapie) oder, wenn dies nicht mehr möglich ist, auf die Verkleinerung des Tumorgewebes mit dem Ziel, die Lebenszeit zu verlängern und tumorbedingte Beschwerden zu reduzieren (Palliation).

Für verschiedene Tumoren haben sich spezielle Therapieschemata etabliert, die in großen internationalen Untersuchungen laufend optimiert werden (Therapieoptimierungsstudien). Ausgehend vom festgestellten Stadium werden mit dem Patienten mögliche Therapieoptionen erörtert. Hierbei spielen der körperliche Allgemeinzustand und die Begleiterkrankungen eine wesentliche Rolle. Die nach aktuellem Stand der Wissenschaft erfolgversprechende Therapieform wird dem Patienten vorgeschlagen. Möglichkeiten sind die einmalige oder mehrmalige Chemotherapie oder Bestrahlung oder eine Operation zur Entfernung des Tumorgewebes. Verschiedene Chemotherapeutika können kombiniert werden. Die Kombination aller drei Methoden ist ebenfalls möglich.

Bösartige Tumoren stellen insbesondere bei fortgeschrittenen Erkrankungen die heutige Medizin immer noch vor erhebliche Probleme.

Zunehmend sanftere Methoden wurden und werden entwickelt, um den Patienten zu schonen. Dazu gehören unter anderem:

Während man sich in der Onkologie lange Zeit mit der Verbesserung der Überlebensraten beschäftigte, konnten hier nun so große Fortschritte gemacht werden, dass nun vor allem auch die Langzeitfolgen der onkologischen Therapien untersucht werden. Sowohl die Chemotherapie als auch die Strahlentherapie hinterlässt Spuren im Körper, die noch nach vielen Jahren oder Jahrzehnten zu Folgeerkrankungen führen können.

Eine Studie beschäftigte sich zum Beispiel mit Erkrankungen von Erwachsenen, die sich in ihrer Kindheit einer onkologischen Therapie unterzogen.<ref name="DOI10.1001/jama.2013.6296">Melissa M. Hudson: "Clinical Ascertainment of Health Outcomes Among Adults Treated for Childhood Cancer." In: "JAMA." 309, 2013, S. 2371, .</ref> Hier zeigte sich, dass fast alle betroffenen mindestens an einer chronischen Krankheit leiden. Im Alter von 50 Jahren standen Kardiomyopathie, Herzklappenfehler, Lungenprobleme, Funktionsstörungen der Hypophyse und Schwerhörigkeit bzw. Taubheit im Vordergrund. Bei einem Drittel besteht Unfruchtbarkeit. Trotz verbesserter Therapien sollte auch heute noch auf eine Langzeitbetreuung ehemaliger onkologischer Patienten geachtet werden um Erkrankungen früh erkennen und therapieren zu können.

Die Fortschritte in der Krebsforschung haben dazu beigetragen, neue wirkungsvollere Therapien gegen Krebs zu entwickeln und Behandlungsansätze zu optimieren. So konnten Überlebenschancen und Lebensqualität Krebskranker in den vergangenen Jahren stetig verbessert werden. Die Forschungsförderung durch private Organisationen hat dabei wachsende Bedeutung. So fördert die Deutsche Krebshilfe seit über 40 Jahren ihres Bestehens onkologische Forschungsprojekte aus Spendengeldern der Bürger.




</doc>
<doc id="3789" url="https://de.wikipedia.org/wiki?curid=3789" title="Organische Chemie">
Organische Chemie

Die organische Chemie (kurz OC oder häufig auch Organik) ist ein Teilgebiet der Chemie. Darin werden die chemischen Verbindungen behandelt, die auf Kohlenstoff basieren, abgesehen von einigen Ausnahmen wie manchen anorganischen Kohlenstoffverbindungen und dem elementaren (reinen) Kohlenstoff.

Die große Bindungsfähigkeit des Kohlenstoffatoms ermöglicht eine Vielzahl von unterschiedlichen Bindungen zu anderen Atomen. Während viele anorganische Stoffe durch Temperatureinfluss und katalytische Reagenzien nicht verändert werden, finden organische Reaktionen oft bei Raumtemperatur oder leicht erhöhter Temperatur mit katalytischen Mengen an Reagenzien statt. Auch die Entstehung der Vielzahl der Naturstoffe (pflanzliche, tierische Farbstoffe, Zucker, Fette, Proteine, Nukleinsäuren) und letztlich der bekannten Lebewesen basiert auf dieser Bindungsfähigkeit.

Organische Moleküle enthalten als Elemente neben Kohlenstoff häufig Wasserstoff, Sauerstoff, Stickstoff, Schwefel, Halogene; die chemische Struktur und die funktionellen Gruppen sind die Grundlage für die Verschiedenartigkeit der Einzelmoleküle.

In der organischen Analytik erfolgt zunächst aus einem Gemisch von Stoffen eine physikalische Trennung und Charakterisierung (Schmelzpunkt, Siedepunkt, Brechungsindex) von Einzelstoffen, dann werden die elementare Zusammensetzung (Elementaranalyse), Molekülmasse und funktionellen Gruppen (mit Hilfe von chemischen Reagenzien, NMR-, IR- und UV-Spektroskopie) bestimmt.

Außerdem untersuchen organische Chemiker die Einwirkung von Reagenzien (Säuren, Basen, anorganischen und organischen Stoffen) auf organische Stoffe, um Gesetzmäßigkeiten von chemischen Reagenzien auf bestimmte funktionelle Gruppen und Stoffgruppen zu bestimmen.

Aus der Kenntnis der Vielzahl von Gesetzmäßigkeiten kann ein organischer Chemiker eigenständig Synthesen von organischen Naturstoffen (z. B. Zucker, Peptide, Naturfarbstoffe, Alkaloide, Vitamine) planen oder in der Natur unbekannte organische Stoffe (Kunststoffe, Ionenaustauscher, Arzneistoffe, Pflanzenschutzmittel, Kunstfasern für Kleidungsstücke) synthetisieren. 

Dadurch wird der Wohlstand einer Gesellschaft erheblich beeinflusst: Die Entwicklungen der organischen Chemie hatten in den letzten 150 Jahren einen bedeutenden (positiven) Einfluss auf die menschliche Gesundheit, die Ernährung, die Kleidung und auf die Zahl verfügbarer Konsumgüter.

Mit wenigen Ausnahmen umfasst die Organik die Chemie aller Verbindungen, die der Kohlenstoff mit sich selbst und anderen Elementen eingeht. Dazu gehören auch alle Bausteine des derzeit bekannten Lebens. Im Jahre 2012 waren etwa 40 Millionen organische Verbindungen bekannt.

Ausnahmen sind formal zunächst die elementaren Formen des Kohlenstoffs (Graphit, Diamant) und systematisch alle zur anorganischen Chemie zählenden wasserstofffreien Chalkogenide des Kohlenstoffs (Kohlenstoffmonoxid, Kohlenstoffdioxid, Schwefelkohlenstoff), die Kohlensäure und Carbonate, die Carbide sowie die ionischen Cyanide, Cyanate und Thiocyanate (siehe Kohlenstoff-Verbindungen).

Die Blausäure gehört zum Grenzgebiet der anorganischen und organischen Chemie. Obwohl man sie traditionell zur anorganischen Chemie zählen würde, wird sie als Nitril (organische Stoffgruppe) der Ameisensäure aufgefasst. Die Cyanide werden in der Anorganik behandelt, wobei hier nur die Salze der Blausäure gemeint sind, wohingegen die unter selbigem Namen bekannten Ester als Nitrile zur Organik gehören. Auch die Cyansauerstoffsäuren, Thiocyansäuren und deren Ester gelten als Grenzfälle. Weiter ist die metallorganische Chemie (Metallorganyle) nicht konkret der organischen oder anorganischen Chemie zuzuordnen.

Auch völlig unnatürlich wirkende Stoffe, wie Kunststoffe und Erdöl, zählen zu den organischen Verbindungen, da sie wie die Substanzen von Lebensformen aus Kohlenstoffverbindungen bestehen. Erdöl, Erdgas und Kohle, die Ausgangsstoffe für viele synthetische Produkte, sind letztlich organischen Ursprungs.

Alle Lebewesen enthalten organische Stoffe: Aminosäuren, Proteine, Kohlenhydrate und die DNA. Das Teilgebiet der organischen Chemie, das sich mit den Stoffen und Stoffwechselprozessen in Lebewesen befasst, ist die Biochemie (oder auch Molekularbiologie).

Die Sonderstellung des Kohlenstoffs beruht darauf, dass das Kohlenstoffatom vier Bindungselektronen hat, wodurch es unpolare Bindungen mit ein bis vier weiteren Kohlenstoffatomen eingehen kann. Dadurch können lineare oder verzweigte Kohlenstoffketten sowie Kohlenstoffringe entstehen, die an den nicht mit Kohlenstoff besetzten Bindungselektronen mit Wasserstoff und anderen Elementen (vorwiegend Sauerstoff, Stickstoff, Schwefel, Phosphor) verbunden sind, was zu großen und sehr großen Molekülen (z. B. Homo- und Heteropolymere) führen kann und die riesige Vielfalt an organischen Molekülen erklärt. Von dem ebenfalls vierbindigen Silicium gibt es auch eine große Anzahl Verbindungen, aber bei Weitem keine solche Vielfalt.

Die Eigenschaften organischer Substanzen werden sehr stark von ihrer jeweiligen Molekülstruktur bestimmt. Selbst die Eigenschaften von einfachen organischen Salzen wie den Acetaten werden deutlich von der Molekülform des organischen Teils geprägt. Es gibt auch viele Isomere, das sind Verbindungen mit der gleichen Gesamtzusammensetzung (Summenformel), aber unterschiedlicher Struktur (Strukturformel).

Dagegen bestehen die Moleküle in der anorganischen Chemie meist nur aus einigen wenigen Atomen, bei denen die allgemeinen Eigenschaften von Festkörpern, Kristallen und/oder Ionen zum Tragen kommen. Es gibt aber auch Polymere, die keinen Kohlenstoff enthalten (oder nur in Nebengruppen), z. B. die Silane.

Organische Synthesestrategien unterscheiden sich von Synthesen in der anorganischen Chemie, da organische Moleküle meist Stück für Stück aufgebaut werden können. Etwa 60 % der Chemiker in Deutschland und den USA haben als Schwerpunktfach die organische Chemie gewählt.

Viele organische Naturstoffe wurden schon in der Frühzeit der menschlichen Entwicklung genutzt (die Farbstoffe Indigo, Alizarin, die ätherischen Öle, Weingeist). Eine künstliche Darstellung von organischen Stoffen durch Menschenhand ist jedoch in sehr früher Zeit nicht beschrieben worden.

Johann Rudolph Glauber beschrieb in seinen Werken eine Vielzahl von selbst dargestellten organischen Verbindungen, da jedoch die Elementaranalyse noch nicht entwickelt war, kann nur vermutet werden, welche Stoffe er damals erhalten hatte. Weingeist und Essig reinigte Glauber über eine fraktionierte Destillation, Ethylchlorid erhielt er aus Weingeist, Essigsäure aus der Holzdestillation, Aceton aus der Erhitzung von Zinkazetat, Acrolein entstand bei der Destillation von Rüb-, Nuss- und Hanföl, Benzol aus Steinkohle, Alkaloide fand er durch eine Salpetersäure-Trennung.

Lemery schrieb 1675 das Buch "Cours de Chymie". In diesem Werk wurden die Stoffe in drei Gebiete eingeteilt: Mineralreich (Metalle, Wasser, Luft, Kochsalz, Gips), Pflanzenreich (Zucker, Stärke, Harze, Wachs, Pflanzenfarbstoffe), Tierreich (Fette, Eiweiße, Hornsubstanzen).
Lemery unterschied auch die Stoffe des Pflanzen- und Tierreiches als organische Stoffe im Gegensatz zu den Stoffen der unbelebten Natur des Mineralreiches.

Bereits im 18. Jahrhundert war eine beträchtliche Zahl von "organischen Substanzen" als Reinstoff isoliert worden.

Beispiele sind der Harnstoff (1773 von Hilaire Rouelle) und viele Säuren, wie die von Ameisen erhaltene Ameisensäure (1749 von Andreas Sigismund Marggraf), die Äpfelsäure aus Äpfeln, und die aus dem Weinstein gewonnene Weinsäure (1769), die Citronensäure (1784), das Glycerin (1783), die Oxalsäure, die Harnsäure (von Carl Wilhelm Scheele).

Antoine Laurent de Lavoisier bestimmte erstmals qualitativ die in organischen Stoffen enthaltenen chemischen Elemente: Kohlenstoff, Wasserstoff, Sauerstoff, Stickstoff. Joseph Louis Gay-Lussac und Louis Jacques Thenard führten erste Elementaranalysen zur Ermittlung der quantitativen Zusammensetzung von Elementen in organischen Stoffen aus. Die Elementaranalyse wurde 1831 von Justus von Liebig verbessert. Nun konnte die elementare Zusammensetzung von organischen Stoffen schnell bestimmt werden.

Jöns Jakob Berzelius stellte die These auf, dass organische Stoffe nur durch eine besondere "Lebenskraft" im pflanzlichen, tierischen oder menschlichen Organismus geschaffen werden kann. Berzelius wendete auch das Gesetz der multiplen Proportionen – mit dem er im Bereich der anorganischen Verbindungen Atomgewichte und Zusammensetzung, d. h. deren chemische Formeln, bestimmen konnte auch auf organische Verbindungen an.

Die Struktur und Zusammensetzung von organischen Verbindungen war um 1820 noch sehr ungeklärt. Gay-Lussac glaubte, dass das Ethanol eine Verbindung aus einem Teil Ethen und einem Teil Wasser sei.

Weiterhin glaubten die Chemiker damals, dass bei gleicher qualitativer und quantitativer Zusammensetzung (Summenformel) der Elemente einer Verbindung (Elementaranalyse) die Stoffe auch identisch sein müssen. Erste Zweifel traten im Jahr 1823 auf als Justus von Liebig und Friedrich Wöhler das knallsaure Silber sowie das cyansaure Silber untersuchten. Sie fanden bei gleicher chemischer Zusammensetzung sehr unterschiedliche Stoffe.

Im Jahr 1828 erhitzte Friedrich Wöhler Ammoniumcyanat und erhielt einen ganz andersartigen Stoff, den Harnstoff. Ausgangsprodukt und Endprodukt haben die gleiche chemische Summenformel (Isomerie), sie besitzen jedoch sehr unterschiedliche Eigenschaften: das Ammoniumcyanat ist eine anorganische Verbindung, der Harnstoff ist eine organische Verbindung. Damit war die Hypothese von Berzelius, dass organische Verbindungen nur durch eine besondere "Lebenskraft" entstehen können, widerlegt.

Hermann Kolbe formulierte 1859 die These, dass alle organischen Stoffe Abkömmlinge der anorganischen Stoffe – insbesondere des Kohlenstoffdioxids – sind. So ergibt der Ersatz einer Hydroxygruppe durch Alkylreste oder Wasserstoff Carbonsäuren, der Ersatz zweier Hydroxygruppen durch Alkylgruppen oder Wasserstoff die Aldehyde, Ketone. Kolbe gebrauchte auch das Wort Synthese im Zusammenhang mit der künstlichen Darstellung von organischen Naturstoffen. Chemiker konnten bald durch eigene Forschungen neue organische Moleküle synthetisieren.

In Analogie zu positiv und negativ geladenen Ionen in der anorganischen Chemie vermutete Berzelius sogenannte Radikale in der organischen Chemie; darauf basierte seine Radikaltheorie. Ein Radikalteil des organischen Moleküls sollte eine positive, der andere Teil eine negative Ladung besitzen. Einige Jahre später untersuchten Jean Baptiste Dumas, Auguste Laurent, Charles Gerhardt und Justus von Liebig die Substitution bei organischen Verbindungen. Die Wasserstoffatome in organischen Verbindungen wurden durch Halogenatome ersetzt. Die alte Radikaltheorie von Berzelius, nach der sich positiv und negativ geladene Radikalteile in organischen Molekülen zusammenlagern, musste verworfen werden. In der Folge wurde von August Wilhelm von Hofmann, Hermann Kolbe, Edward Frankland, Stanislao Cannizzaro weitere Grundlagen über die Zusammensetzung von organischen Stoffen gefunden. 1857 veröffentlichte Friedrich August Kekulé seine Arbeit "„Über die s. g. gepaarten Verbindungen und die Theorie der mehratomigen Radikale“" in "Liebigs Annalen der Chemie," die als Ausgangspunkt der organischen Strukturchemie gesehen wird. In dieser Arbeit wird der Kohlenstoff erstmals als vierwertig beschrieben.

Adolf von Baeyer, Emil Fischer, August Wilhelm von Hofmann erforschten Synthesen von Farbstoffen, Zuckern, Peptiden und Alkaloiden.

Ein Großteil der Arbeitszeit der früheren Chemiker lag in der Isolierung eines Reinstoffes.

Der Prüfung der Stoffidentität von organischen Stoffen erfolgte über Siedepunkt, Schmelzpunkt, Löslichkeit, Dichte, Geruch, Farbe, Brechungsindex.

Besonders wichtig wurde der Rohstoff Kohle für die organische Chemie. Ihren Aufschwung nahm die organische Chemie mit der Untersuchung der bei der Leuchtgaserzeugung entstehenden Abfallprodukte, als der deutsche Chemiker Friedlieb Ferdinand Runge (1795–1867) im Steinkohlenteer die Stoffe Phenol und Anilin entdeckt hatte. William Henry Perkin – ein Schüler August Wilhelm von Hofmann – entdeckte im Jahr 1856 den ersten synthetischen Farbstoff – das Mauvein. Von Hofmann und Emanuel Verguin führten das Fuchsin in die Färberei ein. Johann Peter Grieß entdeckte die Diazofarbstoffe. Die organische Chemie gewann nun zunehmende wirtschaftliche Bedeutung.

In den 1960er Jahren gelang die Herstellung von Valenzisomeren des Benzols durch aufwändige organische Synthesen. Bereits früher wurde mit dem 2-Norbornylkation ein nicht klassisches Carbokation gefunden, das fünf statt drei Bindungen zu anderen Atomen eingeht. 1973 wurde dann erstmals das pentagonal-pyramidale Hexamethylbenzol-Dikation mit sechsfach koordiniertem Kohlenstoff synthetisiert, dessen Struktur 2016 kristallographisch nachgewiesen werden konnte.<ref name="DOI10.1002/ange.201608795">Moritz Malischewski, K. Seppelt: "Die Molekülstruktur des pentagonal-pyramidalen Hexamethylbenzol-Dikations im Kristall." In: "Angewandte Chemie," 129, 2017, S. 374, .</ref>

Die organische Chemie ist ein Teilbereich der Wissenschaft (Lehrbücher, Studium), deren Grundlagen im 19. Jahrhundert nur für eine kleine Schicht der Bevölkerung zugänglich war. Durch die Bildungsreformen im 20. Jahrhundert erhalten fast alle Schüler eine Wissensgrundlage in organischer Chemie. Der Chemieunterricht ermöglicht dem Schüler die Teilhabe an kultureller Bildung, fördert das Verständnis für die Einordnung und Zusammenhänge bei Fragen, die chemisch relevant sind. Politiker, Juristen, Betriebswirte, Informatiker, Maschinenbauer benötigen in unserer Kultur Basiskenntnisse in organischer Chemie, um Zusammenhänge besser einordnen zu können.

Das geistige Vermögen der forschenden Chemiker und der Chemiker in der chemischen Industrie sorgten in den Industrieländern für ein großes Vermögen mit dauerhaft hohen Zinserträgen. Durch Auslandsinvestitionen und der Vernetzung mit Rohstoffanbietern konnte das gesellschaftliche Vermögen noch gesteigert werden.

Als Grundlage in der Naturwissenschaft dient das Experiment. Ein Experiment muss jedoch vor seiner Ausführung gut durchdacht sein. Der Chemiker prüft in der chemischen Fachliteratur, ob ein ähnliches Experiment von anderen Chemikern bereits unternommen wurde, damit überflüssige Arbeit vermieden wird. Für ein Experiment sollten in der Regel sehr reine Stoffe verwendet werden, um eindeutige Ergebnisse bei einem Experiment zu erhalten. Bei einer Stoffumsetzung kann es auch sehr entscheidend sein, ob Lösungsmittel, die sich bei einer Stoffumsetzung chemisch nicht verändern, zugesetzt werden oder nicht. Der Zusatz von anderen Lösungsmitteln kann zu geänderten Stoffumsetzungen führen.

In früherer Zeit untersuchten die organischen Chemiker beispielsweise den Einfluss von konzentrierten Säuren (Schwefelsäure, Salpetersäure, Salzsäure) auf organische Stoffe wie Ethanol, Baumwolle, Benzol.

Bei der Einwirkung von konzentrierter Schwefelsäure auf Ethanol entsteht ein neuer Stoff, der Diethylether, der ganz andere Eigenschaften als das Ethanol hatte und als Narkosemittel und als neues Lösungsmittel Anwendung gefunden hat. Bei der Einwirkung von Salpetersäure und Schwefelsäure auf Baumwolle entsteht die Schießbaumwolle, die als Explosivstoff, als Weichmacher und Lösemittel von Lacken, als Faser Verwendung fand.

Aus Benzol entsteht durch Einwirkung von konzentrierter Schwefelsäure und Salpetersäure das Nitrobenzol. Dieser Stoff ließ sich mit Reduktionsmitteln wie Eisenpulver und Salzsäure zu Anilin umwandeln. Anilin war das Ausgangsprodukt für viele neue Farbstoffe, die den Wohlstand unseres Gemeinwesens erhöhten.

Die Einwirkung von konzentrierter Schwefelsäure auf Baumwolle oder Holz ergibt Zuckermoleküle. Ähnlich wie in der anorganischen Chemie benutzten auch organische Chemiker bestimmte Nachweisreagenzien. Für organische Chemiker sind jedoch die funktionellen Gruppen im Molekül von großer Wichtigkeit. Mit Fehlingscher Lösung lassen sich Aldehydgruppen nachweisen. Funktionelle Gruppen können dazu genutzt werden, zwei organische Moleküle mit unterschiedlichen funktionellen Gruppen zu verknüpfen, so dass ein größeres Molekül entsteht. Durch Kenntnis der organischen Reaktionsmechanismen, der Wahl der Reagenzien und dem Einsatz von Schutzgruppen kann ein organischer Chemiker sehr komplexe organische Stoffe herstellen. Heutzutage können Peptide oder Proteine mit mehr als 100 Aminosäuren (mit einer molekularen Masse größer als 10.000) oder Kohlenhydrate sowie Pflanzeninhaltsstoffe (Terpene) synthetisiert werden. Kaum eine organische Reaktion verläuft mit 100 % Ausbeute, häufig ergeben sich auch unerwartete Nebenreaktionen, so dass komplexe Stoffe auf synthetischer Basis nur in geringer Menge (wenigen Milligramm bis mehreren Kilogramm) anfallen.

Viele organische Grundstoffe werden in der Industrie bei der Herstellung von Kunststoffen, Farbstoffen, Lösungsmitteln in sehr großen Mengen (1.000 bis 1.000.000 t) hergestellt. Spezialisierte Firmen verwenden die Industrieprodukte, um Feinchemikalien für Schule und Hochschule herzustellen. Der Organiker wünscht sich bei seinen Synthesen möglichst selektive Reagenzien, die nur eine bestimmte funktionelle Gruppe oxidieren, reduzieren oder mit einer anderen Gruppe verknüpfen.

Manchmal sind Stoffumsetzungen nur bei einer gesteigerten Temperatur möglich. Hohe Temperaturen werden in der organischen Chemie jedoch nur selten angewendet, da viele organische Stoffe durch eine erhöhte Temperatur zerstört werden. Die Reaktionstemperaturen in der organischen Chemie liegen daher meist zwischen Raumtemperatur und 150 °C. Die Wahl des Lösungsmittels und dessen Siedepunkt sind entscheidend für die Einstellung der Reaktionstemperatur. Eine Temperaturerhöhung um 10 °C verdoppelt in der Regel die Reaktionsgeschwindigkeit (RGT-Regel).

Beispiele für organische Reaktionen bei hoher Temperatur sind die Bildung von Aceton aus Calciumacetat und die Darstellung von 2,3-Dimethyl-butadien aus Pinakol.

Aus Calciumcarbonat und Essigsäure lässt sich das organische Salz Calciumacetat darstellen. Erhitzt man das Calciumacetat auf ca. 400 °C, so erhält man Aceton. Aceton und etwas Magnesium bilden den organischen Stoff Pinakol. Erhitzt man diesen Stoff bei 450 °C mit Aluminiumoxid, so bildet sich 2,3-Dimethyl-1,3-butadien. Stoffe mit Doppelbindungen lassen sich unter dem Einfluss einer Säure oder von Radikalbildnern polymerisieren, so dass ein Kunststoff mit ganz anderen Eigenschaften als das Monomer entsteht. Das polymerisierte 2,3-Dimethyl-1,3-butadien spielte eine wichtige Rolle als Ersatzstoff des früher sehr teuren Kautschuks. Fritz Hofmann konnte aus dem 2,3-Dimethyl-1,3-butadien den ersten synthetischen Methylkautschuk herstellen, der im Jahr 1913 in den Handel kam, als der Preis für natürlichen Kautschuk im Handel Höchstwerte erreichte.

Nach einer chemischen Umsetzung muss der organische Chemiker zunächst die stark reaktiven, ätzenden, brennbaren Stoffe wie konzentrierte Schwefelsäure, Natrium, Natriumhydrid, Lithiumaluminiumhydrid mit geeigneten Stoffen in harmlose Verbindungen überführen.
Darauf folgt eine Abtrennung der anorganischen Salze durch Ausschütteln im Scheidetrichter – unter Zusatz von weiterem organischen Lösungsmittel und einer wässrigen Lösung. Die organische Phase wird über wasserfreien Salzen wie Natriumsulfat getrocknet, dabei werden die letzten Reste von Wasser aus der organischen Phase entfernt. Das organische Lösungsmittel wird durch Destillation – häufig am Rotationsverdampfer – entfernt. Der eingedampfte Rückstand enthält das Reaktionsprodukt. Sehr selten kommt es vor, dass bei einer organischen Reaktion nur ein chemisches Produkt entsteht, vielfach entstehen Stoffmischungen aus unterschiedlichen organischen Stoffen. Durch eine fraktionierte Destillation im Vakuum oder durch eine Säulenchromatographie lassen sich die einzelnen Stoffe isolieren.

Grundlage der Stoffkenntnis ist die chemische Strukturformel. Dies ist der Bauplan eines organischen Moleküls. Die Strukturformel eines Stoffes muss immer gedanklich aus Ergebnissen der Stoffanalyse abgeleitet werden. Die Stoffanalyse umfasst mindestens den korrekten Kohlenstoff-, Wasserstoff-, Sauerstoff- und Stickstoffgehalt eines Moleküls (Elementaranalyse), die Art der funktionellen Gruppen und die Bestimmung der molaren Masse. 

Durch den kommerziellen Verkauf von Kernspinresonanzspektroskopen (NMR-Spektroskopie) und Massenspektrometern seit Anfang der sechziger Jahre an Hochschulen verkürzte sich die Zeit bis zur Strukturaufklärung von neuen komplizierten organischen Stoffen erheblich.
Aus der Veränderung der Strukturformel vor und nach einer organischen Reaktion kann der Chemiker den Reaktionsmechanismus einer chemischen Reaktion ableiten. Alle organischen Moleküle, die einen ähnlichen Aufbau besitzen, können unter den gleichen Reaktionsbedingungen analoge Reaktionen eingehen. Der Chemiker kann durch die Kenntnis der Reaktionsmechanismen den Aufbau von neuen organischen Stoffen systematisch planen.

Eine sehr wichtige Reaktionsklasse bezieht sich auf den Ersatz eines Wasserstoffatoms im Molekül durch ein Halogen, eine Nitrogruppe, eine Sulfongruppe, man bezeichnet diese Reaktion als Substitution. Zu Beginn dieses Abschnittes wurden einige Beispiele aus dieser Reaktionsklasse genannt. Eine weitere wichtige Reaktionsklasse ist die Eliminierung. Die Abspaltung von Hydroxygruppen und Halogenen und der Ausbildung von Doppelbindungen im Molekül bezeichnet man als Eliminierung. Die Wasserabspaltung bei Pinakol zu 2,3-Dimethyl-1,3-butadien ist eine Eliminierung. Andere sehr wichtige Umsetzungen sind die Oxidation und die Reduktion von organischen Molekülen. Die Reduktion von Nitrobenzol zu Anilin durch Zink oder Eisenspäne in Anwesenheit einer Säure oder die Oxidation von Ethanol zu Acetaldehyd oder Essigsäure mittels Kaliumpermanganat sind Beispiele für diese Reaktionsklassen.

In fast allen Gütern unseres täglichen Gebrauchs sind Stoffe der organischen Chemie vorhanden. Die Farbstoffe in Bildbänden, Zeitschriften, Verpackungsaufdrucken, die Kunststoffe im Großteil unserer Gebrauchsgüter in fast jedem Spielzeug, im Computergehäuse, in Rohrleitungen, Kabeln, Tragetaschen usw., die organischen Kunstfasern im großen Teil unserer Kleidung, die Lacke für Hausfassaden, Autos, den Wohnbereich, die Reinigungsmittel von einfachen Seifen bis komplexen Tensiden für Spezialanwendungen, die Arzneimittel, die Aroma- und Duftstoffe in Lebensmitteln und Blumen, die Lebensmittelkonservierungsstoffe, die Ionenaustauscher in Entsalzungsanlagen. Auch Holz und Baumwolle sind organische Stoffe, sie können durch ein reiches Vorkommen aus der Natur gewonnen werden. Die Mehrzahl der organischen Stoffe muss jedoch auf synthetischer Basis – hauptsächlich aus Erdöl – von der chemischen Industrie erzeugt werden. Bei einer weltweiten Verknappung von Erdöl könnte man gegenwärtig nur bedingt andere fossile Rohstoffe wie Kohle oder Erdgas nutzen, um die organischen Stoffe des täglichen Bedarfs herzustellen. Ein hoher Preis für Erdöl führt zu Anstrengungen, Substitutionsverfahren auf Basis von Kohle und Erdgas zu entwickeln. Die Verfahren werden jedoch weniger rentabel als auf Basis von Erdöl sein. Bei sehr hohen Preisen für Erdöl könnte es zu Verknappungen im Bereich der Konsumgüter kommen.

Basis für alle wichtigen synthetischen Stoffe sind die Grundchemikalien. Sie werden in großen Chemieanlagen aus Erdöl, Erdgas oder Kohle hergestellt.

Bis zum Zweiten Weltkrieg war die Kohle die Basis für die Grundchemikalien der organischen Chemie. Aus der Kohle konnte Benzol, Toluol, Xylol – Bausteine für organische Farbstoffe – gewonnen werden. Mit einem elektrischen Lichtbogen kann aus Kohle und Kalk das Kalziumcarbid (großtechnisch seit 1915) gewonnen werden. Kalziumcarbid lässt sich in Acetylen umwandeln und bildete damals nach Verfahren von Walter Reppe (Reppe Chemie) das Ausgangsprodukt für Acetaldehyd, Essigsäure, Aceton, Butylenglyckol, Butadien, Acrylsäure, Acrylnitril.
Aus Kohle ließ sich auch Methanol (Synthese nach Pier) und Dieselöl (nach Bergius) gewinnen. Auch nach dem Zweiten Weltkrieg wurden viele Grundchemikalien noch aus Kohle hergestellt. Zwischen 1960 und 1970 wurden die Verfahren in den westlichen Industriestaaten durch modernere Verfahren auf Basis von Erdöl ersetzt.

Die Investitionskosten für derartige Anlagen sind beträchtlich, hauptsächlich sind in diesem Geschäftsbereich Firmen der Mineralölindustrie involviert. Früher wurden die chemischen Rohstoffe in die Industrieländer transportiert und dort chemisch zu Grundchemikalien umgewandelt. Noch in den achtziger Jahren waren die USA, Japan und die Bundesrepublik Deutschland die wichtigsten Chemieländer mit mehr als 50 % der Weltproduktion der organischen Grundstoffe. Im Zuge der weltweiten Verflechtungen und aus ökonomischen Gründen werden viele Anlagen in den Rohstoffländern (von Erdöl und Erdgas) errichtet.

Sehr wichtige Grundchemikalien sind Ethylen (19,5 Millionen Tonnen in EU-27, 2011), Propen (14,3 Mio. t, EU-27, 2011), Butadien (2,8 Mio. t, EU-27, 2011), Methan, Benzol (7,4 Mio. t, EU-27, 2011), Toluol (1,5 Mio. t., EU-27, 2011), Xylol. Aus diesen Grundchemikalien können weitere wichtige organische Grundstoffe hergestellt werden. Seit 2005 schwanken die Verkaufspreise in der EU für die organischen Grundstoffe erheblich, im Jahr 2010 stiegen die Verkaufspreise in der EU deutlich an.

Aus Ethylen gewinnt die Industrie Polyethylen, Vinylacetat (nachfolgend Polyvinylacetat, Polyvinylalkohol, Polyvinylacetal), Acetaldehyd, Essigsäure, Dichlorethan (nachfolgend Polyvinylchlorid), Ethylenoxid, Ethanol (nachfolgend Diethylether).

Aus Propylen gewinnen Unternehmen Polypropylen, Isopropanol (nachfolgend Aceton, Keten, Essigsäureanhydrid, Diketen, Essigsäureester, Acetylcellulose), Propylenoxid (nachfolgend Polyetherpolyole, Polyurethan), Allylchlorid (nachfolgend Epichlorhydrin, Glycerin, Allylalkohol), Acrylnitril (nachfolgend Polyacrylnitril, Acrylamid), Acrylsäure (nachfolgend Polyacrylate), Butanol.

Aus Methan gewinnt man Methanol (nachfolgend Formaldehyd und Ethylenglycol), Acetylen, Methylchlorid, Methylenchlorid, Chloroform (nachfolgend Tetrafluorethylen, Teflon), Tetrachlorkohlenstoff.

Aus Benzol wird Ethylbenzol (nachfolgend Styrol), Dihydroxybenzol (Resorcin, Hydrochinon und Brenzcatechin), Cumol (nachfolgend Phenol), Nitrobenzol (nachfolgend Anilin, Farbstoffe), Cyclohexan (nachfolgend Cyclohexanon, Adipinsäure, Nylon) synthetisiert. Aus Xylol kann Terephthalsäure, Phthalsäureanhydrid hergestellt werden.

Industrieprodukte sind überwiegend Mischungen von organischen Substanzen, die für eine anwendungstechnische Herstellung zubereitet worden sind. Industrieprodukte werden in sehr großen Mengen (bis mehrere Mio. Tonnen) von der chemischen Industrie hergestellt, bei diesen Produkten sind die Rohstoffkosten sehr entscheidend für den Verkaufspreis.

Wichtige organische Industrieprodukte sind: Chemiefasern, Kunststoffe, Farbmittel, Kautschuk, Lösemittel, Tenside.
Seit 2009 ist der Umsatz für Kunststoffe deutlich zurückgegangen.

Spezialprodukte sind organische Stoffe, die im Vergleich zu Industrieprodukten in deutlich geringerer Menge produziert werden. Der Verkaufspreis ist in geringerer Weise von Rohstoffkosten abhängig. Zu dieser Gruppe gehören beispielsweise Arzneimittel, Aromen und Duftstoffe, Enzyme, Lacke, Desinfektionsmittel, Diagnostika, Ionenaustauscherharze, Klebstoffe, Herbizide, Pflanzenschutzmittel, Waschmittel.

Es ergeben sich zwei Möglichkeiten für eine systematische Einteilung der einzelnen Substanzen der organischen Chemie in Stoffgruppen:



Siehe Reaktionsmechanismus

Die Reaktionen in der organischen Chemie lassen sich größtenteils in die folgenden Grundtypen einordnen:


Darüber hinaus sind viele Reaktionen unter dem Namen ihres Entdeckers bekannt (siehe Liste von Namensreaktionen).

Eine Einteilung nach dem entstehenden Bindungstyp bzw. Baustein findet sich in der Liste von Reaktionen in der organischen Chemie.

Die organische analytische Chemie beschäftigt sich mit der Untersuchung von organischen Stoffen.
Dabei kann es darum gehen,

Wichtige Methoden zum Nachweis und zur Reinheitsbestimmung (qualitative Analyse) sind klassische nasschemische Farb- und Niederschlagsreaktionen, biochemische Immunassay-Methoden und eine Vielfalt von chromatographischen Methoden.

Mengenverhältnisse in Gemischen (quantitative Analyse) festzustellen ist möglich durch nasschemische Titrationen mit unterschiedlicher Endpunktsanzeige, durch biochemische Immunassayverfahren und durch eine Vielzahl von chromatographischen Verfahren so wie durch spektroskopische Methoden, von denen viele auch zur Strukturaufklärung herangezogen werden, wie Infrarotspektroskopie (IR), Kernspinresonanzspektroskopie (NMR), Ramanspektroskopie, UV-Spektroskopie. Zur Strukturaufklärung werden neben charakteristischen chemischen Reaktionen weiterhin die Röntgenbeugungsanalyse und die Massenspektrometrie (MS) verwendet.




</doc>
<doc id="3790" url="https://de.wikipedia.org/wiki?curid=3790" title="Oper">
Oper

Als Oper (von ital. "opera in musica", „musikalisches Werk“) bezeichnet man seit 1639 eine musikalische Gattung des Theaters. Ferner werden auch das Opernhaus (die Aufführungsstätte oder produzierende Institution) oder die aufführende Kompagnie als "Oper" bezeichnet.

Eine Oper besteht aus der Vertonung einer dramatischen Dichtung, die von einem Sängerensemble, einem begleitenden Orchester sowie manchmal von einem Chor und einem Ballettensemble ausgeführt wird. Neben dem Gesang führen die Darsteller Schauspiel und Tanz auf einer Theaterbühne aus, die mit den Mitteln von Malerei, Architektur, Requisite, Beleuchtung und Bühnentechnik gestaltet ist. Die Rollen der Darsteller werden durch Maske und Kostüme optisch verdeutlicht. Als künstlerische Leitung betätigen sich der Dirigent für das Musikalische, der Regisseur für die Personenführung und der Bühnenbildner für die Ausstattung. Im Hintergrund unterstützt sie die Dramaturgie.

Die Oper wird mit einigen anderen Formen unter dem Begriff Musiktheater zusammengefasst. Die Grenzen zu verwandten Kunstwerken sind fließend und definieren sich in jeder Epoche, meist auch im Hinblick auf bestimmte nationale Vorlieben, immer wieder neu. Auf diese Art bleibt die Oper als Gattung lebendig und erhält immer wieder neue Anregungen aus den verschiedensten Bereichen des Theaters.

Schauspiele in dem strengen Sinne, dass auf der Bühne nur gesprochen würde, sind in der Theatergeschichte selten. Mischformen aus Musik, Rezitation und Tanz waren die Regel, auch wenn sich zu manchen Zeiten Literaten und Theaterleute um eine Rettung oder Reform des Schauspiels bemüht haben. Seit dem 18. Jahrhundert sind Mischformen zwischen Schauspiel und Oper aus den verschiedenen Spielarten der Opéra-comique hervorgegangen, wie Ballad Opera, Singspiel oder Posse mit Gesang. Die Singspiele Mozarts werden der Oper zugerechnet, diejenigen Nestroys gelten als Schauspiele. Auf der Grenze bewegen sich z. B. auch die Werke von Brecht/Weill, deren "Dreigroschenoper" dem Schauspiel näher steht, während "Aufstieg und Fall der Stadt Mahagonny" eine Oper ist. Sich dem Schauspiel völlig unterordnende Musik bezeichnet man als Schauspielmusik.

Eine verbreitete, dem Schauspiel verwandte Theaterform seit dem Beginn des 19. Jahrhunderts war das Melodram, das heute nur noch im populären Film gegenwärtig ist. Es hatte mit seinen Abenteuerstoffen großen Einfluss auf die Oper in jener Zeit. Stellenweise enthielt es Hintergrundmusik als Untermalung der Bühnenhandlung (weniger des gesprochenen Texts). Darauf bezieht sich der heute noch bekannte Begriff Melodram. Eine solche Untermalung findet sich zum Beispiel in Mozarts "Idomeneo", Ludwig van Beethovens "Fidelio", in Webers "Der Freischütz" (in der Wolfsschluchtszene) und in Humperdincks "Königskinder".

In französischer Tradition war der Tanz seit dem Barock in die Oper integriert. Das klassische Ballett löste sich im 19. Jahrhundert mühevoll aus dieser Verbindung, aber in neoklassizistischen Werken des 20. Jahrhunderts, beispielsweise bei Igor Strawinsky oder Bohuslav Martinů, bestätigt sich die Verwandtschaft von Oper und Ballett erneut. Auch die italienische Oper war nicht frei von Tanz, wenn auch der Tanz nicht im gleichen Maß dominierte. Heute werden die Ballette und Divertissements der Repertoirewerke meist aus den Partituren gestrichen, sodass der Eindruck einer Spartentrennung entsteht.

Das Genre der Operette und verwandter Formen wie der Zarzuela grenzt sich als Weiterentwicklung aus dem Singspiel durch die gesprochenen Dialoge, aber auch durch dessen vorherrschenden Unterhaltungsanspruch und das vorrangige Bemühen um Popularität oder kommerziellen Erfolg von der ab der Mitte des 19. Jahrhunderts zunehmend durchkomponierten Oper ab. Diese Abgrenzung entstand erst im ausgehenden 19. Jahrhundert: Als die „komische Oper“ vom „niederen“ zum „hohen“ Genre geworden war, bildete sich die Operette als neues „niederes“ Genre. Ähnliches gilt für das Musical, die Weiterentwicklung des populären Musiktheaters in den Vereinigten Staaten. Operette und Musical sind gleichwohl in nicht geringerem Maße Kunstformen als die Oper.

Bereits im Theater der griechischen Antike verband man szenische Aktion mit Musik. Die Oper der Neuzeit berief sich immer wieder auf dieses Vorbild und konnte es, weil von der Aufführungspraxis wenig überliefert ist, auf unterschiedlichste Weise deuten. Ein Chor, der sang und tanzte, hatte eine tragende Rolle, indem er das Drama in Episoden gliederte oder auch die Aufgabe hatte, die Handlung zu kommentieren. Die Römer pflegten eher die Komödie als die Tragödie. Mimus und später Pantomimus hatten einen hohen Musikanteil. Durch die Zerstörung der römischen Theater im 6. Jahrhundert und die Bücherverluste in der Spätantike sind viele Quellen darüber verloren gegangen.

Jedoch werden seit Beginn des 20. Jahrhunderts zahlreiche antike Bauten, insbesondere Amphitheater und Theaterbauten, für Opernaufführungen genutzt. Die bekanntesten sind das Théâtre Antique in Orange (mit Unterbrechungen seit 1869), die Arena di Verona (seit 1913), das Odeon des Herodes Atticus in Athen (seit den 1930er Jahren), die Thermen des Caracalla in Rom (seit 1937) und der Römersteinbruch St. Margarethen (seit 1996).

Im Hochmittelalter entstand ausgehend vom Gottesdienst der Ostermesse eine neue Tradition gesungener Handlung. Das geistliche Spiel fand zunächst in der Kirche, im 13. Jahrhundert dann als Passionsspiel oder Prozessionsspiel außerhalb der Kirche statt. Beliebte Themen waren das biblische Oster- und Weihnachtsgeschehen, auch mit komödiantischen Einlagen. Die Melodien sind oft überliefert, der Einsatz von Musikinstrumenten ist wahrscheinlich, aber selten belegbar. Im höfischen Bereich gab es weltliche Stücke wie Adam de la Halles melodienreiches "Jeu de Robin et de Marion" (1280).

Die Zeit des Karnevals, die später zur traditionellen Opernsaison wurde, bot seit dem 15. Jahrhundert Gelegenheit zu musikalisch-theatralischen Aktionen, die von den damals größten europäischen Städten in Italien ausgingen: Intermedien, Tanzspiele, Masken- und Triumphaufzüge gehören zur städtischen Repräsentation in der italienischen Renaissance. Das Madrigal war die wichtigste Gattung der Vokalmusik und verband sich oft mit Tänzen.

Der Königshof in Frankreich gewann im 16. Jahrhundert gegenüber Italien an Bedeutung. Das "Ballet comique de la reine" 1581 war eine getanzte und gesungene Handlung und gilt als bedeutender Vorläufer der Oper.

Ein früher Versuch in Deutschland, eine dramatische Handlung mit singenden Protagonisten in einem Bühnenbild aufzuführen, ist die Aufführung von "Orpheus und Amphion" auf einer Simultanbühne anlässlich der "Jülichschen Hochzeit" von Johann Wilhelm von Jülich-Kleve-Berg mit Markgräfin Jakobe von Baden in Düsseldorf 1585. Als möglicher Komponist der nicht überlieferten Musik wird Andrea Gabrieli genannt. Die Musik sei so schön gewesen, „daß es denselben / so dazumahl nit zugegen gewesen / und solchen Musicum concentum & Symphoniam gehört haben / onmüglich zu glauben.“ Die Handlung war freilich primär eine Allegorese im Sinne eines Fürstenspiegels.

Die Oper im heutigen Sinn entstand Ende des 16. Jahrhunderts in Florenz. Eine wichtige Rolle in der Entstehungsgeschichte spielte die Florentiner Camerata, ein akademischer Gesprächskreis, in dem sich Dichter (z. B. Ottavio Rinuccini), Musiker, Philosophen, Adelige und ein Kunstmäzen – zunächst übernahm Graf Bardi diese Rolle, später Graf Corsi – zusammenfanden. Diese Humanisten versuchten, das antike Drama wiederzubeleben, an dem ihrer Meinung nach Gesangssolisten, Chor und Orchester beteiligt waren. Nach den Pastoraldramen des 16. Jahrhunderts wurde das Libretto gestaltet und mit den musikalischen Mitteln der Zeit in Musik gesetzt.
Vincenzo Galilei gehörte dieser Gruppe an. Er entdeckte Hymnen des Mesomedes, die heute verloren sind, und schrieb ein Traktat gegen die niederländische Polyphonie. Dies war ein deutlicher Beweis für den gewünschten musikalischen Stil, den damals neuen Sologesang mit Instrumentalbegleitung.

Textverständlichkeit der Vokalmusik war für die Florentiner Camerata das Wichtigste. Eine klare, einfache Gesangslinie wurde zum Ideal erklärt, der sich die sparsame Generalbass-Begleitung mit wenigen und leisen Instrumenten wie Laute oder Cembalo unterzuordnen hatte. Großartig ausgearbeitete melodische Einfälle waren unerwünscht, um den Inhalt der Worte nicht durch den Gesang zu verschleiern. Man sprach sogar von einer „nobile sprezzatura del canto“ (Giulio Caccini: "Le nuove musiche", 1601), einer „noblen Verachtung des Gesangs“. Diese Art des Singens nannte man "recitar cantando", rezitierenden Gesang. Die Schlichtheit und Beschränkung des "recitar cantando" steht im Gegensatz zur vorherrschenden Polyphonie mit ihren komplexen Ton- und Textschichtungen. Mit der Monodie, wie man diesen neuen Stil in Anlehnung an die Antike nannte, sollte das Wort wieder zu seinem vollen Recht kommen. Es entwickelte sich eine Theorie der Affekte, die durch den gesungenen Text transportiert werden konnten. Zur Monodie der einzelnen Gesangsstimme gesellten sich Chöre in Madrigalform oder als Motette. Das Orchester spielte dazwischen Ritornelle und Tänze.

Als erstes Werk der Gattung Oper gilt "La Dafne" von Jacopo Peri (1597) mit einem Text von Ottavio Rinuccini, von der bis auf ein paar Fragmente nichts erhalten geblieben ist. Weitere wichtige Werke aus der Anfangszeit sind Peris "Euridice" (1600) als älteste erhaltene Oper, sowie "Euridice" (1602) und "Il Rapimento di Cefalo" von Giulio Caccini. Stoffe dieser frühen Opern entnahm man der Schäferdichtung und vor allem der griechischen Mythologie. Wunder, Zauber und Überraschungen, dargestellt durch aufwendige Bühnenmaschinerie, wurden zu beliebten Bestandteilen.

Besondere Beachtung fand Claudio Monteverdis erste Oper "L’Orfeo" (1607). Sie wurde anlässlich des Geburtstags von Francesco IV. Gonzaga am 24. Februar 1607 in Mantua uraufgeführt. Hier sind im Vergleich zu seinen Vorgängern erstmals ein reicheres Instrumentarium (wenngleich es in der Partitur meist nur angedeutet ist), ausgebaute Harmonik, tonmalerisch-psychologische und bildhafte Ausdeutung von Worten und Figuren sowie eine die Personen charakterisierende Instrumentation zu hören. Posaunen werden zum Beispiel für die Unterwelt- und Todesszenen eingesetzt, Streicher bei Schlafszenen, für die Hauptfigur Orfeo kommt eine Holzorgel zum Einsatz.

Monteverdi erweitert die Gesangslinie des "recitar cantando" zu einem mehr arienhaften Stil, und gibt den Chören größeres Gewicht. Seine Spätwerke "Il ritorno d’Ulisse in patria" (1640) und "L’incoronazione di Poppea" (1643) sind in Hinblick auf ihre Dramatik Höhepunkte der Operngeschichte. Noch in dieser letzten Oper Monteverdis, "L’incoronazione di Poppea", findet man den Prolog durch drei allegorische Figuren dargestellt, in der Fortuna die "Virtù" (Tugend) verspottet. Die übrige Handlung spielt in der irdischen Welt um den römischen Kaiser Nero, dessen ungeliebte Gattin Ottavia und Poppea, die Gattin des Prätors Ottone. Diese wird Neros Gattin und Kaiserin. Neros brutaler Charakter wird von einem Kastraten und entsprechend virtuoser Musik dargestellt, Ottone wirkt dagegen weich, und Neros würdiger Lehrer und Berater Seneca bekommt die Bassstimme zugewiesen. Belcanto-Gesang und Koloraturreichtum werden für den Adel und für Göttergestalten eingesetzt, für die übrigen Personen schlichtere Ariosi und Lieder.

Das Teatro San Cassiano in Venedig als erstes öffentliches Opernhaus wurde 1637 eröffnet. In schneller Folge entstanden neue Spielstätten, und Venedig wurde mit seiner „venezianischen Oper“ zum Opernzentrum Norditaliens. Historische Darstellungen verdrängten bald die mythischen Stoffe, wie in der Oper "L’incoronazione di Poppea" (1642), die noch den Namen Claudio Monteverdis trägt, wobei die Forschung seit Alan Curtis darüber diskutiert, ob es sich vielmehr um ein Pasticcio handle, das sich den berühmten Namen zu Nutze machte.

Das Publikum dieser Opern setzte sich vornehmlich aus Angehörigen der nichtadeligen Stände zusammen. Den Spielplan bestimmte der geldgebende Adel auf Grund des Publikumsgeschmacks. Die aus den Akademien hervorgegangene Oper wurde in diesem Zusammenhang kommerzialisiert und vereinfacht, das Orchester reduziert. Die Da-capo-Arie mit vorangestelltem Rezitativ prägte für lange Zeit den Sologesang, Chöre und Ensembles wurden gekürzt. Verwechslungen und Intrigen bildeten das Grundgerüst der Handlungen, die mit komischen Szenen der beliebten Nebenfiguren angereichert wurden. Francesco Cavalli und Antonio Cesti waren die bekanntesten venezianischen Opernkomponisten in der auf Monteverdi folgenden Generation. Die Schriftsteller Giovanni Francesco Busenello und Giovanni Faustini galten als stilbildend und wurden häufig nachgeahmt.

Zum zweiten, stärker vom Geschmack der Aristokratie geprägten Opernzentrum Italiens wurde seit den 1650er Jahren die Großstadt Neapel. Als Begründer der neapolitanischen Oper gilt der Komponist Francesco Provenzale. In der folgenden Generation wurde Alessandro Scarlatti zum Vorreiter der neapolitanischen Schule.

Die Librettisten erhielten ihr Geld durch den Verkauf von Textbüchern, die zusammen mit Wachskerzen zum Mitlesen vor der Vorstellung verteilt wurden. Lange Zeit blieb die Literatur des Renaissance-Humanismus Vorbild der italienischen Operntexte.

Opern wurden nur zu bestimmten Spielzeiten (ital.: "stagione") gegeben: während des Karnevals, von Ostern bis zur Sommerpause sowie vom Herbst bis zum Advent. In der Passions- und Adventszeit wurden stattdessen Oratorien gespielt. In Rom erhielten nicht nur Maschineneffekte und Chöre ein größeres Gewicht, sondern auch geistliche Stoffe.

In Paris entwickelte Jean-Baptiste Lully zusammen mit seinem Librettisten Philippe Quinault eine französische Variante der Oper, deren herausragendstes Merkmal neben den Chören das Ballett ist. Lully verfasste eine französische Version von Cavallis "L’ercole amante" (1662), in die er Ballette einfügte, die größeren Beifall fanden als die Oper. "Cadmus et Hermione" (1673) wird als erste "Tragédie lyrique" angesehen und blieb modellhaft für die nachfolgenden französischen Opern.

Die aus Italien importierte Oper wurde von der Tragédie lyrique zurückgedrängt. Dennoch versuchten Lullys Nachfolger Marc-Antoine Charpentier und André Campra, französische und italienische Stilmittel zu verbinden.

Ausgehend von italienischen Vorbildern, entwickelte sich bereits gegen Mitte des 17. Jahrhunderts eine eigenständige Operntradition innerhalb des deutschen Sprachgebietes, welche auch die Verwendung deutschsprachiger Libretti mit einschließt.

Die erste Oper eines „deutschen“ Komponisten war 1627 die (verschollene) "Dafne" von Heinrich Schütz, der die Musikform der Oper bei seinem Studienaufenthalt 1609–1613 in Italien kennengelernt hatte. Nur wenige Jahre später entstand 1644 die erste erhaltene deutschsprachige Oper von Sigmund Theophil Staden nach einem Libretto von Georg Philipp Harsdörffer "Das geistlich Waldgedicht oder Freudenspiel, genannt Seelewig", ein pastorales Lehrstück in starker Nähe zum moralisierenden Schuldrama der Renaissance.

Kurz nach dem 30-jährigen Krieg etablierten sich auch im deutschsprachigen Raum Opernhäuser zunehmend als zentraler Versammlungs- und Repräsentationsort der führenden Gesellschaftsschichten. Eine zentrale Rolle spielten dabei die führenden Fürsten- und Königshäuser, welche sich zunehmend eigene Hoftheater samt der zugehörigen Künstler leisteten, die in der Regel auch für die (wohlhabende) Öffentlichkeit zugänglich waren. So erhielt München sein erstes Opernhaus 1657, Dresden 1667.

Bürgerliche, d. h. durch Städte und/oder private bürgerliche Akteure finanzierte „öffentliche und populäre“ Opernhäuser wie in Venedig existierten hingegen lediglich in Hamburg (1678), Hannover (1689) und Leipzig (1693). Im bewussten Gegensatz zum durch italienischsprachige Opern dominierten Betrieb an den „adligen“ Häusern, setzte insbesondere die Hamburger Oper am Gänsemarkt als ältestes bürgerliches Opernhaus Deutschlands bewusst auf deutschsprachige Werke und Autoren. So Händel, Keiser, Mattheson und Telemann. Jene etablierten bereits ab Beginn des 18. Jahrhunderts unter Verwendung deutschsprachiger Libretti von Dichtern wie Elmenhorst, Feind, Hunold und Postel eine eigenständige deutschsprachige Opern- und Singspieltradition. Die Bedeutung Hamburgs für die Entwicklung einer eigenständigen deutschsprachigen Operntradition unterstreichen auch die beiden zeitgenössischen Schriften zur Theorie der Oper: Heinrich Elmenhorsts "Dramatologia" (1688) und Barthold Feinds "Gedancken von der Opera" (1708).

In England verbreitete sich die Oper erst relativ spät. Die vorherrschende musikalische Theaterform in der Zeit des Elisabethanischen Theaters war die Masque, eine Kombination aus Tanz, Pantomime, Sprechtheater und musikalischen Einlagen, bei denen der vertonte Text meist nicht in unmittelbarem Zusammenhang mit der Handlung stand. Im Anschluss an das puritanische Verbot von Musik- und Theateraufführungen von 1642 begründete erst die Stuart-Restauration ab 1660 wiederum ein Theaterleben, in das die Oper integriert wurde.

Ein in jeder Hinsicht singuläres Werk ist Henry Purcells knapp einstündige Oper "Dido and Aeneas" (Uraufführung vermutlich 1689, Libretto: Nahum Tate). Der Komponist greift darin Elemente der französischen und der italienischen Oper auf, entwickelt jedoch eine eigene Tonsprache, die sich vor allem dadurch auszeichnet, dass sie sehr eng am Text bleibt. Chorpassagen und tänzerische Abschnitte stehen den ariosen Passagen der Hauptfiguren gegenüber, die fast ohne arienartige Formen auskommen. Die wechselnden Stimmungen und Situationen werden mit musikalischen Mitteln genau wiedergegeben; die Schlussszene, wenn die karthagische Königin Dido aus unglücklicher Liebe zu dem trojanischen Helden Aeneas an gebrochenem Herzen stirbt, gehört zum Bewegendsten der Opernliteratur.

Im Laufe des 18. Jahrhunderts bilden sich zwei Operntypen heraus: Neben der etablierten Opera seria als vorwiegend vom Repräsentations- und Legitimationsbedürfnis des Adels getragene Form, die vorwiegend auf mythologischen oder historischen Stoffen basiert und deren Personal aus Göttern, Halbgöttern, Heroen, Fürsten sowie deren Geliebten und ihrer Dienerschaft besteht, entwickelt sich ab ca. 1720 die Opera buffa mit zunächst grobschlächtig komischen Handlungen, die sich zu bürgerlich-sentimentalen entwickeln.

Eine Konkurrenz zu den italienischen Opern bilden in Frankreich einerseits die höfische Tragédie lyrique, mit ihrem im Vergleich zu älteren italienischen Opern volleren Instrumentarium, und andererseits die Opéra-comique, die vom Pariser Jahrmarktstheater herstammt. Diese Gattungen regen auch außerhalb Frankreichs Opernaufführungen in der eigenen Landessprache an, als einheimisches Gegengewicht zu den allgegenwärtigen italienischen Gesangsvirtuosen.

Stilprägend wurde die im zweiten Viertel des 18. Jahrhunderts von Italien ausgehende Tendenz, aus dem ursprünglichen Dramma per musica ein Arienkonzert und daraus eine Nummernoper mit festgelegtem Inhalt und Musik zu machen.

Eine weitere zentrale Entwicklung während der ersten Hälfte des 18. Jahrhunderts ist die Einteilung der auf fünf Teile angewachsenen Da-capo-Arien mit der Abfolge AA'–B–AA' in spezifische Untergruppen:


Der Star des Abends konnte zudem eine virtuose "Aria baule" („Koffer-Arie“) einschieben, die mit der Handlung nichts zu tun hatte. Solche Arien konnten leicht vertauscht oder mehrfach eingesetzt werden. Der Belcanto-Gesang wurde zu einer Präsentation virtuoser Gesangstechniken, die extreme Spitzentöne, geschmeidige Triller und weite Sprünge umfassten.

Aufgrund des im 18. Jahrhundert noch nicht etablierten Gedankens der Werktreue und dem stetigen Wunsch der Auftraggeber und des Publikums nach neuen, noch nie gehörten Opern, sowie den lokal häufig nur begrenzt zur Verfügung stehenden Ressourcen an Instrumentalisten und Sängern, bestand eine weitverbreitete Aufführungspraxis des 18. Jahrhunderts darin, Arien und Ensembles aus den verschiedensten Werken mit Rücksicht auf die zur Verfügung stehende Besetzung möglichst wirkungsvoll zusammenzustellen und das so entstandene Gebilde mit neuen Texten und einer neuen Handlung zu unterlegen. Diese Art von Opern nennt man Pasticcio; ein Opernpasticcio konnte sowohl aus der Feder eines einzigen Komponisten stammen, der dabei vorhandene Nummern aus früheren Werken wiederverwendete, als auch aus Werken verschiedener Komponisten zusammengesetzt sein. Diese Praxis führte dazu, dass Handlung und Stimmung einer Opernaufführung bis Ende des 18. Jahrhunderts – an einigen Aufführungsorten auch bis in die 1830er Jahre hinein – nicht bindend festgelegt waren und ständigen Anpassungen, Wandlungen und Veränderungen unterlagen. Die Praxis des Pasticcio bedeutete daher in der Praxis, dass bis zum Beginn des 19. Jahrhunderts kaum eine Aufführung des gleichen Werks einer vorhergehenden musikalisch wie inhaltlich glich.

Das daraus folgende Handlungschaos – entstanden aus dem Bedürfnis, vielen Geschmäckern und Bedürfnissen zugleich gerecht zu werden – schreckte die italienischen Librettisten Apostolo Zeno und Pietro Metastasio ab. Als Gegenmaßnahme verzichteten sie ab den späten 1730er Jahren zunehmend auf überflüssige Seitenhandlungen, mythische Allegorien und Nebenfiguren und bevorzugten stattdessen eine klare, nachvollziehbare Handlung und Sprache. Damit schufen sie die Grundlage für einen „ernsteren“ Operntypus jenseits der bis dahin üblichen Aufführungspraxis der Opera seria. Das zu diesem Zweck entwickelte Handlungsschema verwickelt die Hauptfiguren nach und nach in ein scheinbar unlösbares Dilemma, das sich zum Schluss durch einen unverhofften Einfall zum Guten wendet "(lieto fine)". Auch dichterisch leiteten beide Autoren eine Erneuerung der Oper ein. Als weitere Maßnahme gegen die Praxis des Pasticcio führten sie eine Nummerierung der musikalischen Teile ein, wodurch ein Austausch dieser Elemente erschwert wurde. So trugen sie wesentlich zur Herausbildung der Nummernoper mit ihrer festgelegten Abfolge bei. Als fortan in sich geschlossenes Werk mit stringenter Handlung konnte sich die Oper nunmehr auch gegenüber dem Schauspiel behaupten.

Die Gattung der Opera buffa entstand zeitgleich in Neapel und Venedig als zumeist heiterer und lebensnaher Operntypus. Einerseits gab es selbstständige musikalische Komödien, andererseits die komischen Intermezzi zur Opera seria anfangs der 1730er Jahre, aus der Apostolo Zeno und Pietro Metastasio die komischen Elemente ausgeschlossen hatten, sodass sie auf Einlagen zwischen den Akten beschränkt werden mussten. Als stilprägende Werke gelten die Oper "Lo frate ’nnamorato" von Giovanni Battista Pergolesi, uraufgeführt am 28. September 1732 im Teatro dei Fiorentini in Neapel, und die ab Mitte der 1740er Jahre in Venedig uraufgeführten Werke Baldassare Galuppis, die in enger Zusammenarbeit mit dem venezianischen Komödiendichter und Librettisten Carlo Goldoni entstanden.

Inhaltlich schöpfte die Opera buffa aus dem reichen Fundus der Commedia dell’arte. Die Handlungen waren oft Verwechslungskomödien, deren Personal aus einem adligen Liebespaar und zwei Untergebenen, oft Magd und Diener, bestand. Letztere können im Unterschied zur Opera seria als Hauptakteure auftreten, womit sich ein bürgerliches und subbürgerliches Publikum identifizieren konnte. Die Opera buffa wurde aber auch von der Aristokratie geschätzt, die ihre Provokationen kaum ernst nahm.

Seit Mitte des 18. Jahrhunderts begann eine Verlagerung der Komik in der Opera buffa auf alltagsweltliche und gegenwartsbezogene Handlungen, in denen Adlige nicht mehr unangreifbar waren. Mozarts "Don Giovanni" (1787) wurde zunächst als Opera buffa angesehen und erst im 19. Jahrhundert uminterpretiert, als das Schicksal der bürgerlichen Verführten ernst genommen und der adlige Verführer als Schurke betrachtet werden konnten.

Ausdruck dieser Veränderungen ist die Weiterentwicklung der Opera buffa zum Typus der Opera semiseria Ende des 18. Jahrhunderts, weil ein bürgerliches Publikum sich auf der Bühne nicht mehr verlacht sehen wollte. Die Alltagsnähe der Opera buffa und ihres französischen Gegenstücks, der Opéra-comique, besaß in der zweiten Hälfte des 18. Jahrhunderts soziale Sprengkraft. Damit im Zusammenhang stand der von 1752 bis 1754 in Frankreich ausgetragene Buffonistenstreit. Jean-Jacques Rousseau schätzte den bürgerlich geprägten „heiteren“ Operntypus mehr als die Tragédie lyrique der Hocharistokratie. Seine Verurteilung der französischen Oper zu Gunsten der italienischen führte zu wütenden Reaktionen.

Im Englischen Sprachraum wurde der zuvor in Halle und Hamburg tätige Georg Friedrich Händel (engl. George Frederic Haendel) zu einem der produktivsten Opernkomponisten (mehr als 45 Opern-Werke). Sein Wirken in London zog die Zuhörer aufgrund mehrerer unglücklicher Umstände nicht wie gewünscht in seinen Bann, u. a. wegen der starken Konkurrenz des berühmten Kastraten Farinelli, der in der rivalisierenden Operntruppe sang. Die Qualität seiner Werke bleibt davon jedoch unberührt. Vor allem "Alcina", "Giulio Cesare" und "Serse" sind zu beliebten Händel-Opern geworden. In den letzten Jahrzehnten sind jedoch auch viele andere Händel-Opern wieder vermehrt aufgeführt worden (u. a. "Ariodante", "Rodelinda", "Giustino" etc.); nachdem im Zuge der Alte-Musik-Bewegung die historische Aufführungspraxis immer besser erforscht worden war, entstanden auch an den großen Opernhäusern stilbildende Produktionen unter Mitwirkung der Barock-Spezialisten in z. T. von der Pop Art inspirierter Inszenierungsästhetik, die einen regelrechten Boom auslösten (z. B. seit 1994 an der Bayerischen Staatsoper).

Frankreichs Pendant zur in Paris umstrittenen Opera buffa wurde die "Opéra-comique". Die Rezitative wurden durch gesprochene Dialoge ersetzt. Auch dieses Modell fand im Ausland Erfolg. Die neue Einfachheit und Lebensnähe schlägt sich auch in kleineren Arietten und "nouveaux airs", die im Unterschied zu den allseits bekannten Vaudevilles neu komponiert wurden, nieder.

1752 erlebte Frankreich eine neue Konfrontation zwischen der französischen und der italienischen Oper, die unter dem Namen "Buffonistenstreit" in die Geschichte einging. Giovanni Battista Pergolesis Oper "La serva padrona" (deutsch: "Die Magd als Herrin") war der Anlass dafür. Gegen die Künstlichkeit und Stilisierung der herkömmlichen französischen Adelsoper waren vor allem Jean-Jacques Rousseau und Denis Diderot, die sich gegen die Kunst und Stilisierung Rameaus zur Wehr setzten. Rousseau verfasste neben der bewusst einfach gestalteten Oper "Le devin du village" (deutsch: "Der Dorfwahrsager") auch ein preisgekröntes Traktat mit dem Titel "Discours sur les sciences et les arts" (1750), in dem er ein von Wissenschaft und Kultur unverdorbenes Leben zum Ideal erklärt. Weitere Musikartikel schrieb er für die berühmte umfassende Encyclopédie der französischen Aufklärung. Der Buffonistenstreit ging schließlich zu Ungunsten der italienischen Operntruppe aus, die aus der Stadt vertrieben wurde. Somit war der Streit zwar vorläufig beendet, an Beliebtheit stand die Grand opéra aber immer noch hinter der Opéra comique zurück.

Die Schließung der Oper am Gänsemarkt im Jahr 1738 führte zu einer weiteren Stärkung des zu diesem Zeitpunkt bereits dominanten italienischsprachigen Opernbetriebs im deutschen Sprachraum. Dennoch etablierte sich – ausgehend vom Hamburger Vorbild – ab Mitte des 18. Jahrhunderts zunehmend die Praxis bei Aufführungen französischer und italienischer Opern die Rezitative ins Deutsche zu übersetzen und – aus vorwiegend musikalischen Gründen – lediglich bei den Arien die Originalsprache beizubehalten. Auch wurden ab Mitte des 18. Jahrhunderts der Verkauf oder die Verteilung gedruckter Erläuterungen und Übersetzungen nicht-deutschsprachiger Werke in deutscher Sprache an das Publikum mehr und mehr üblich.

Um 1780 setzt mit dem Werk Wolfgang Amadeus Mozarts schließlich eine bis weit ins 19. Jahrhundert reichende Entwicklung ein, die zur zunehmenden Verdrängung des bis dahin dominierenden Italienischen zugunsten deutschsprachiger Werke und Aufführungen in deutscher Übersetzung führte.

Dabei fand Mozart seinen ganz eigenen Weg, mit der Tradition der italienischen Oper umzugehen. Er reüssierte bereits in jugendlichen Jahren mehrfach in Italien (u. a. mit "Lucio Silla" und "Mitridate, re di Ponto") und komponierte mit "Idomeneo" (1781), einer ebenfalls auf italienisch geschriebenen "Opera seria", für München sein erstes Meisterwerk. Auf diese Form sollte er mit "La clemenza di Tito" (1791) kurz vor seinem Tod nochmals zurückkommen. Nach den Singspielen "Bastien und Bastienne", "Zaide" (Fragment) und "Die Entführung aus dem Serail" (mit dieser 1782 uraufgeführten Oper gelang es ihm, sich in Wien als freier Komponist zu etablieren) schaffte er es in seinem "Figaro" (1786) und mehr noch im "Don Giovanni" (1787), Opera seria und Opera buffa einander wieder anzunähern. Neben den zuletzt Genannten entstand 1790 als drittes Werk in kongenialer Zusammenarbeit mit dem Librettisten Lorenzo Da Ponte "Così fan tutte." In der "Zauberflöte" (1791) verband Mozart Elemente der Oper mit jenen des Singspiels und des lokal vorherrschenden Alt-Wiener Zaubertheaters, das seine Wirkung besonders aus spektakulären Bühneneffekten und einer märchenhaften Handlung bezog. Dazu kamen Ideen und Symbole aus der Freimaurerei (Mozart war selbst Logenmitglied). Mozart-Opern (und insbesondere die "Zauberflöte") gehören bis heute zum Standardrepertoire eines jeden Opernhauses. Er selbst bezeichnete die Oper als „Große Oper in 2 Akten“.

Der ebenfalls sowohl in Italien wie auch in Wien tätige Christoph Willibald Gluck leitete mit seinen Opern "Orfeo ed Euridice" (1762) und "Alceste" (1767) in denen er Elemente der ernsten Oper aus Italien und Frankreich mit der realistischeren Handlungsebene der Opera buffa kombinierte eine umfassende Opernreform ein. Der konsequent klar und logisch aufgebaute Handlungsablauf, gestaltet von Ranieri de’ Calzabigi, kommt dabei ohne komplexe Intrigen oder Verwechslungsdramen aus. Die Zahl der Protagonisten schrumpft. Oberstes Ziel ist eine größere Einfachheit und Nachvollziehbarkeit der Handlung.

Dabei ordnet sich Glucks Musik vollständig Dramaturgie und Text unter, charakterisierte Situationen und Personen und stand nicht für den belcanto-Gesang an sich. Durchkomponierte oder strophisch gestaltete Lieder ersetzten die Da-capo-Arie. Dadurch wurde eine neue Natürlichkeit und Einfachheit erreicht, die hohlem Pathos und Sängermanierismen entgegenwirkte. Der Chor schaltete sich getreu dem antiken Vorbild aktiv in die Handlung ein. Die Ouvertüre bezieht sich auf die Handlung und steht nicht mehr als abgelöstes Instrumentalstück vor der Oper. Italienisches Arioso, französisches Ballett und Pantomime, englisches und deutsches Lied sowie Vaudeville wurden in die Oper integriert, nicht als nebeneinanderstehende Einzelstücke, sondern als neuer klassischer Stil. Glucks ästhetische Ideen wurden von seinem Schüler Antonio Salieri im späten 18. Jahrhundert zu einer neuen Blüte gebracht. Besonders bedeutend sind die Opern Les Danaïdes, Tarare und Axur, re d’Ormus.

Weiterer Ausdruck der größeren Alltagsnähe der Opera buffa und der durch Christoph Willibald Gluck angeregten Neuerungen der Opernreform ist die in der 2. Hälfte des 18. Jahrhunderts einsetzende Praxis auf hohe Kastratenpartien für Männerpartien zugunsten realistischerer Stimmlagen zu verzichten. Neben der bewussten Abgrenzung von der stark durch das Virtuosentum der Kastraten geprägten Opernkultur der Opera seria des Adels, spielten hierfür nicht zuletzt Kostengründe eine entscheidende Rolle. Da Impresarios mit der Opera buffa auf ein weniger zahlungskräftiges bürgerliches und sub-bürgerliches Publikum zielten, waren die horrenden Kosten für die Gage eines bekannten Kastraten kaum zu erwirtschaften. Die hieraus folgende Identifikation der Virtuosenkultur der Kastratenpartien mit der durch den Adel geprägten kostspieligen Tradition der Opera seria erklärt auch das Verschwinden der Kastraten aus dem Opernbetrieb nach dem Ende des Ancien Régime und dem hierdurch bedingten Aufstieg der durch die „natürlichere“ Stimmbesetzung der Opera buffa und Opera semiseria geprägten bürgerlichen Schichten zur auch in Sachen Oper führenden Gesellschaftsschicht des 19. Jahrhunderts.

Im ersten Viertel des 19. Jahrhunderts verschwinden zunehmend die durch den Generalbass begleiteten Rezitative zugunsten einer ausnotierten Orchesterfassung. Neben die bis dahin noch führende italienische Oper und die französischen Operntypen treten nach und nach andere Nationalopern, insbesondere in Deutschland. Die Französische Revolution und der Aufstieg Napoleons zeigten ihre Auswirkungen auf die Oper am deutlichsten bei Ludwig van Beethovens einziger Oper "Fidelio" bzw. "Leonore." Dramaturgie und musikalische Sprache orientierten sich deutlich an Luigi Cherubinis "Médée" ("Medea," 1797). Die Handlung beruht auf einer wahren Begebenheit innerhalb der Revolution, und der Freiheitsdrang der ursprünglichen Volksbewegung zeigt sich deutlich als Ideal der Oper. "Fidelio" kann zum Typus der „Rettungsoper“ gezählt werden, in der die dramatische Errettung eines Menschen aus großer Gefahr der Gegenstand ist. Formal ist das Werk uneinheitlich: der erste Teil ist singspielhaft, der zweite mit dem groß angelegten Chorfinale erreicht symphonische Durchschlagskraft und nähert sich dem Oratorium. Nach der "Zauberflöte" und dem "Fidelio" brauchte die deutsche Kulturlandschaft mehrere Anläufe, um schließlich in der Romantik eine eigene Opernsprache zu entwickeln. Eine der wichtigsten Vorstufen hierzu lieferten Louis Spohr mit seiner Vertonung des "Faust" und E. T. A. Hoffmann mit seiner romantischen Oper "Undine."

Carl Maria von Weber war es schließlich, der aus der Tradition des Singspiels mit viel dramatischem Farbenreichtum im Orchester die deutsche Oper in Gestalt des "Freischütz" im Jahr 1821 gebührend aufleben ließ. Sein wegen des schlechten Textbuches kaum gespieltes Werk "Oberon" maß dem Orchester so viel Bedeutung zu, dass sich später namhafte Komponisten wie Gustav Mahler, Claude Debussy und Igor Strawinsky auf ihn beriefen.

Weitere Komponisten der deutschen Romantik waren die als Opernkomponisten kaum bekannten Hochromantiker Franz Schubert ("Fierrabras"), dessen Freunde ihm keine kongeniale Textvorlage liefern konnten, und Robert Schumann, der mit der Vertonung des unter Romantikern beliebten "Genoveva"-Stoffs nur eine Oper vorlegte. Ferner zu nennen sind Heinrich Marschner, der mit seinen Opern um übernatürliche Ereignisse und Naturschilderungen "(Hans Heiling)" großen Einfluss auf Richard Wagner ausübte, Albert Lortzing mit seinen Spielopern (u. a. "Zar und Zimmermann" sowie "Der Wildschütz"), Friedrich von Flotow mit seiner komischen Oper "Martha" und schließlich Otto Nicolai, der mit den "Lustigen Weibern von Windsor" etwas „italianità“ in die deutsche Oper trug.
Richard Wagner schließlich formte die Oper so grundlegend nach seinen Ideen um, dass die oben genannten deutschen Komponisten neben ihm schlagartig verblassten. Mit "Rienzi" erlebte der bis dahin eher glücklose Wagner seinen ersten Erfolg in Dresden: er wurde später von "Der fliegende Holländer" noch übertroffen. Wegen seiner Verwicklung in die Märzrevolution von 1848 in Dresden musste Wagner für viele Jahre ins Exil in die Schweiz. Sein späterer Schwiegervater Franz Liszt trug durch die Uraufführung des "Lohengrin" in Weimar dazu bei, dass Wagner trotzdem weiterhin in Deutschland präsent war. Mit der Unterstützung des jungen bayerischen Königs Ludwig II. konnte Wagner schließlich den lang gehegten Plan des "Ring des Nibelungen" verwirklichen, für den er eigens das Bayreuther Festspielhaus erbauen ließ, in dem bis heute nur seine Werke gespielt werden.

Eine der spezifischen Neuerungen für die Gattung Oper durch Wagner bestand darin, dass er eine vollständige Auflösung der Nummernoper vornahm. Tendenzen zur durchkomponierten Oper zeigten sich schon in Webers "Freischütz" oder in Robert Schumanns selten gespielter Oper "Genoveva"; konsequent vollendet wurde diese Entwicklung durch Wagner. Eine weitere kann man darin sehen, dass Singstimme und Orchesterpart grundsätzlich gleichberechtigt behandelt werden, das Orchester also nicht mehr den Sänger begleitet, sondern als „mystischer Abgrund“ in vielfältige Beziehung zum Gesungenen tritt. Die Länge seiner Opern verlangte Sängern und Zuhörern viel Konzentration und Ausdauer ab.
Spezielle Wagner-Themen seiner fast durchweg ernsten Opern (mit Ausnahme der "Meistersinger"), deren Libretto er sämtlich selbst verfasste, sind Erlösung durch Liebe, Entsagung oder Tod. In "Tristan und Isolde" verlegte er das innere Drama der Hauptfiguren in die Musik – die äußere Handlung der Oper ist erstaunlich ereignisarm. Der Beginn dieser Oper setzt die bis dahin gültigen harmonischen Regeln außer Kraft. Er wurde so berühmt, dass er als sogenannter „Tristan-Akkord“ in die Musikgeschichte einging. Musikalisch zeichnen sich Wagners Opern sowohl durch seine geniale Behandlung des Orchestersatzes, die auch auf die symphonische Musik der Zeit bis hin zu Gustav Mahler starken Einfluss ausübte, aus, als auch durch den Einsatz wiederkehrender Motive (sog. Leitmotive), die sich mit Personen, Situationen, einzelnen Texten oder auch mit bestimmten Ideengehalten verbinden.
Mit dem "Ring des Nibelungen," auch kurz "Ring" genannt, dem wohl bekanntesten Opernzyklus in vier Teilen (daher auch „Tetralogie“ genannt) mit etwa 16 Stunden Aufführungszeit insgesamt, schuf Wagner die Erfüllung seines Lebenswerkes. "Parsifal" war die letzte seiner Opern, die die Musikwelt in zwei Lager spalteten und sowohl Nachahmer (Engelbert Humperdinck, Richard Strauss vor seiner "Salome") als auch Skeptiker – insbesondere in Frankreich – hervorriefen.

In Frankreich herrschte zunächst die in der zweiten Hälfte des 18. Jahrhunderts entwickelte Form der "Opéra-comique" vor. Daniel-François-Esprit Auber gelang mit seiner Oper "La muette de Portici," deren Titelheldin von einer stummen Ballerina dargestellt wurde, der Anschluss an die Grand opéra (= „Große Oper“). Der Librettist Eugène Scribe war zu dieser Zeit einer der gefragtesten Dichter. In der Grand opéra traten neben den Verwicklungen von Liebesgeschichten und einer Vielzahl von Personen, vor allem historisch-politische Motive in den Vordergrund, wie es deutlich in Rossinis letzter Oper "Guillaume Tell" (1829) vorgeprägt ist. Der erfolgreichste Vertreter der Grand Opéra war Giacomo Meyerbeer, mit seinen Werken "Robert le diable" (1831), "Les Huguenots" (1836) und "Le prophète" (1849), die jahrzehntelang, noch bis ins beginnende 20. Jahrhundert hinein, im internationalen Repertoire gespielt wurden. Andere bedeutende Beispiele sind "La Juive" (Die Jüdin; 1835) von Halévy, Donizettis "Dom Sébastien" (1843), oder Verdis "Don Carlos" (1867).

Etwa ab 1850 vermischten sich Opéra comique und Grand opéra zu einer neuen Opernform ohne Dialoge. Georges Bizet schrieb 1875 sein bekanntestes Bühnenwerk "Carmen" noch als Opéra comique; die posthum von Ernest Guiraud zugefügten Rezitative rücken "Carmen" allerdings in die Nähe der Grand opéra, zu der jedoch die „realistische“ Handlung und der Ton nicht passen; im Widerspruch zur Opéra comique steht wiederum das tragische Ende, das bei der Uraufführung zunächst für einen Misserfolg sorgte. Weitere Beispiele für die Vermischung von Opéra comique und Grand opéra sind Charles Gounods "Faust" von 1859 – hier wird zum ersten Mal der Begriff "Drame lyrique" verwendet – und Jacques Offenbachs "Les contes d’Hoffmann" ("Hoffmanns Erzählungen", 1871–1880).

Schließlich trat auch Russland mit seinen ersten Nationalopern auf den Plan, genährt durch den Import anderer Erfolge aus dem Westen. Michail Glinka komponierte 1836 mit "Iwan Sussanin" (oder auch: "Ein Leben für den Zaren") das erste Werk mit russischen Sujets, war aber musikalisch noch stark westlichen Einflüssen verhaftet. Seine bekannteste Oper "Ruslan und Ljudmila" übte großen Einfluss auf die folgenden Generationen russischer Komponisten aus. Modest Mussorgski löste sich mit "Boris Godunow" nach einem Drama von Alexander Puschkin endgültig von westlichen Einflüssen. Auch Borodins "Fürst Igor" führte Glinkas Erbe weiter. Pjotr Tschaikowski stand zwischen den russischen Traditionen und denen der westlichen Welt und entwarf mit "Eugen Onegin" und "Pique Dame" Liebesdramen mit bürgerlichem Personal, die beide ebenfalls auf einer Vorlage von Puschkin beruhen.

In Böhmen waren Bedřich Smetana und Antonín Dvořák die meistgespielten Komponisten der Prager Nationaloper, die mit Smetanas "Libusa" im neuen Nationaltheater in Prag ihren Anfang nahm. "Die verkaufte Braut" desselben Komponisten wurde zum Exportschlager. Dvořaks Oper "Rusalka" verknüpfte volkstümliche Sagen und deutsche Märchenquellen zu einer lyrischen Märchenoper. Bohuslav Martinů und Leoš Janáček führten ihre Bestrebungen weiter. Letztgenannter Komponist ist in seiner Modernität in den letzten Jahrzehnten wiederentdeckt worden und hat vermehrt die Spielpläne erobert. Während "Das schlaue Füchslein" noch immer meist in der deutschen Übersetzung von Max Brod gegeben wird, werden andere Werke wie "Jenůfa", "Katja Kabanowa" oder "Die Sache Makropulos" immer häufiger in der tschechischsprachigen Originalversion aufgeführt; das ist insofern wichtig, da Janáčeks Tonsprache sich eng an die Phonetik und Prosodie seiner Muttersprache anlehnt.

Italien verfiel ab dem Jahr 1813, wo seine Opern "Tancredi" und "L’italiana in Algeri" aufgeführt wurden, dem jungen und überaus produktiven Belcanto-Komponisten Gioachino Rossini. "Il barbiere di Siviglia" (1816), "La gazza ladra" (deutsch: "Die diebische Elster") und "La Cenerentola" nach dem Aschenputtel-Märchen von Charles Perrault sind bis heute im Standardrepertoire der Opernhäuser zu finden. Federnder Rhythmus und eine geistreich-brillante Orchestrierung, sowie eine virtuose Behandlung der Singstimme ließen Rossini zu einem der beliebtesten und verehrtesten Komponisten Europas werden. Die bis dato noch üblichen improvisierten Verzierungen der Sänger schrieb Rossini dezidiert in seine Partien hinein und unterband damit ausufernde Improvisationen. Eine neue formale Idee verwirklichte er mit seiner "scene ed arie", die den starren Wechsel Rezitativ-Arie auflockerte und doch das Prinzip der Nummernoper aufrechterhielt.

Rossini hat auch eine ganze Reihe von "Opere serie" geschrieben (z. B. seinen "Otello" oder "Semiramide"). Er ging 1824 nach Paris und schrieb wichtige Werke für die Opéra. Eine politische "Grand opéra" verfasste er über Wilhelm Tell (franz.: "Guillaume Tell"; später auch ital.: "Guglielmo Tell"), die in Österreich verboten und an verschiedenen europäischen Orten in entschärfter Fassung mit anderen Haupthelden aufgeführt wurde.

Rossinis jüngere Zeitgenossen und Nachfolger kopierten zunächst seinen koloraturenreichen Stil, bis vor allem Vincenzo Bellini und Gaetano Donizetti es schafften, sich mit einem eigenen, etwas schlichteren, ausdrucksvollen und romantischeren Stil von dem übermächtigen Vorbild zu emanzipieren. Bellini war berühmt für die ausdrucksvolle und ausgefeilte Deklamation seiner Rezitative und die „unendlich“ langen und ausdruckvollen Melodien seiner Opern, wie "Il pirata" (1827), "I Capuleti e i Montecchi" (1830), "I puritani" (1835), "La sonnambula" (1831), und vor allem "Norma" (1831). Die Titelpartie dieser Oper mit der berühmten Arie „Casta diva“ schrieb Bellini, genau wie die Amina in "La sonnambula", der großen Sängerin Giuditta Pasta auf den Leib. Die Norma ist so anspruchsvoll, dass sie nur von ganz wenigen großen Sängerinnen gesungen und interpretiert werden kann, sie wurde durch die historische Interpretation von Maria Callas wieder der Vergessenheit entrissen.

Der wenige Jahre ältere Donizetti war ein ungemein fleißiger Komponist, der neben Bellini und vor allem nach dessen frühzeitigem Tode (1835) zum erfolgreichsten italienischen Opernkomponisten aufstieg. Seinen ersten großen Durchbruch hatte er mit "Anna Bolena" (1830), deren Titelpartie ebenfalls von der Pasta kreiert und von der Callas wiederentdeckt wurde. Dagegen ist "Lucia di Lammermoor" (1835) mit der berühmten koloraturreichen Wahnsinnsszene nie ganz aus dem Repertoire verschwunden, und hält sich neben den heiteren Opern "L’elisir d’amore" (1832), "Don Pasquale" (1843), und "La fille du régiment" (1840) konsequent auf den Spielplänen der Opernhäuser.

Die weit gespannten Melodiebögen Bellinis machten starken Eindruck auf den jungen Giuseppe Verdi. Seit seiner dritten Oper "Nabucco" galt er als Nationalkomponist für das immer noch von den Habsburgern beherrschte Italien. Der Chor „Va, pensiero, sull’ ali dorate“ entwickelte sich zur heimlichen Nationalhymne des Landes. Musikalisch zeichnet Verdis Musik eine stark betonte, deutliche Rhythmik aus, über der sich einfache, oft extrem ausdrucksstarke Melodien entwickeln. In seinen Opern, bei denen Verdi mit untrüglichem Theaterinstinkt auch oft selbst am Textbuch mitwirkte, nehmen Chorszenen zunächst eine wichtige Stellung ein. Verdi verließ zunehmend die traditionelle Nummernoper; ständige emotionale Spannung verlangte nach einer abwechslungsreichen Durchmischung der einzelnen Szenen und Arien. Mit "Macbeth" wandte sich Verdi endgültig von der Nummernoper ab und ging seinen Weg der intimen Charakterschilderung von Individuen weiter. Mit "La traviata" (1853, nach dem 1848 erschienenen Roman "Die Kameliendame" von Alexandre Dumas d. J., der um die authentische Figur der Kurtisane Marie Duplessis kreist) brachte er erstmals einen Gegenwartsstoff auf die Opernbühne, wurde von der Zensur jedoch gezwungen, die Handlung aus der Jetztzeit zu verlegen. Verdi vertonte häufig literarische Vorlagen, etwa von Friedrich Schiller (z. B. "Luisa Miller" nach "Kabale und Liebe" oder "I masnadieri" nach "Die Räuber"), Shakespeare oder Victor Hugo ("Rigoletto"). Mit seinen für Paris geschriebenen Beiträgen zur "Grande Opéra" (z. B. "Don Carlos", 1867) erneuerte er auch diese Form und nahm mit dem späten "Otello" Elemente von Richard Wagners Musikdrama auf, bis er mit der überraschenden Komödie "Falstaff" (1893; Dichtung in beiden Fällen von Arrigo Boito) im Alter von 80 Jahren seine letzte von fast 30 Opern komponierte. Wahrscheinlich seine populärste Oper ist "Aida," geschrieben 1871.

Nach dem Abtreten Verdis eroberten die jungen Veristen (ital. "vero" = wahr) in Italien die Szene. Ungeschönter Naturalismus war eines ihrer höchsten ästhetischen Ideale – dementsprechend wurde von säuberlich verfassten Versen Abstand genommen. Pietro Mascagni ("Cavalleria rusticana," 1890) und Ruggero Leoncavallo ("Pagliacci," 1892) waren die typischsten Komponisten aus dieser Zeit. Giacomo Puccini wuchs hingegen an Ruhm weit über sie hinaus und ist bis heute einer der meistgespielten Opernkomponisten überhaupt. "La Bohème" (1896), ein Sittengemälde aus dem Paris der Jahrhundertwende, der „Politkrimi“ "Tosca" (1900, nach dem gleichnamigen Drama von Victorien Sardou) und die fernöstliche "Madama Butterfly" (1904), mit der unvollendeten "Turandot" (Uraufführung posthum 1926) noch um ein weiteres an Exotismus gesteigert, sind vor allem wegen ihrer Melodien zu Schlagern geworden. Puccini war ein eminenter Theatraliker und wusste genau für die Stimme zu schreiben; die Instrumentierung seiner meist für großes Orchester gesetzten Partituren ist sehr differenziert und meisterhaft. Zurzeit wird der damals sehr populäre jüdische Komponist Alberto Franchetti, trotz dreier Welterfolge ("Asrael", "Christoforo Colombo" und "Germania") zwischendurch fast vergessen, zaghaft wiederentdeckt.

Claude Debussy gelang es schließlich, sich vom Einfluss des Deutschen zu befreien, und schuf mit "Pelléas et Mélisande" 1902 eines der nuanciertesten Beispiele für die von Wagner übernommene Leitmotivtechnik. Maurice Maeterlincks Textvorlage bot viel an mehrdeutigen Symbolismen an, die Debussy in die Orchestersprache übernahm. Die Gesangspartien wurden fast durchweg rezitativisch gestaltet und boten der „unendlichen Melodie“ Wagners mit dem „unendlichen Rezitativ“ ein Gegenbeispiel. Eine der raren Ausnahmen, die dem Hörer eine gesangliche Linie darbieten, ist das schlichte Lied der Mélisande, das wegen seiner Kürze und Schmucklosigkeit kaum als echte Arie angesehen werden kann.

Nach Richard Strauss, der mit "Salome" und "Elektra" zunächst zum spätromantischen Expressionisten wurde, sich dann allerdings mit "Der Rosenkavalier" wieder früheren Kompositionsstilen zuwendete und mit einer Reihe von Werken bis heute viel gespielt wird (z. B. "Ariadne auf Naxos", "Arabella", "Die Frau ohne Schatten" und "Die schweigsame Frau"), schafften es nur noch wenige Komponisten, einen festen Platz im Repertoire der Opernhäuser zu finden. Stattdessen wurden (und werden) eher die Werke der Vergangenheit gepflegt. Die Aufnahme eines zeitgenössischen Werkes in das Standardrepertoire bleibt die Ausnahme.

Alban Berg gelang dies dennoch mit seinen Opern "Wozzeck," der freitonal angelegt wurde, und "Lulu," die sich ganz der Zwölftonmusik bedient. Die zuerst Fragment gebliebene "Lulu" wurde von Friedrich Cerha für die Pariser Aufführung unter Pierre Boulez und Patrice Chéreau in ihrer dreiaktigen Gestalt vollendet. Von beiden Opern hat insbesondere "Wozzeck," bei dem Gehalt des Stücks und musikalische Vision zu einer Einheit finden, inzwischen weltweit in unzähligen Inszenierungen an großen wie kleineren Bühnen Eingang in das vertraute Opernrepertoire gefunden und eine unbestrittene Stellung erobert. Durchaus ähnlich verhält es sich mit "Lulu," die jedoch wegen ihres im Werk angelegten Aufwands oft nur von größeren Bühnen bewältigt werden kann. Sie inspiriert allerdings regelmäßig wichtige Interpretinnen wie Anja Silja, Evelyn Lear, Teresa Stratas oder Julia Migenes.

Von Arnold Schönberg werden regelmäßig das Monodram "Erwartung" – die erste Oper für eine einzige Sängerin – sowie das vom Komponisten bewusst unvollendet hinterlassene, höchste Ansprüche an den Chor stellende Werk "Moses und Aron" aufgeführt. "Erwartung," bereits 1909 entstanden, doch erst 1924 in Prag mit Marie Gutheil-Schoder unter der Leitung von Alexander von Zemlinsky uraufgeführt, bewies in den dem Zweiten Weltkrieg folgenden Jahren eine spezifische Faszination gleichermaßen für Sängerinnen (besonders Anja Silja und Jessye Norman) wie für Regisseure (z. B. Klaus Michael Grüber mit Silja 1974 in Frankfurt; Robert Wilson mit Norman 1995 bei den Salzburger Festspielen). 1930 begann Schönberg die Arbeit an "Moses und Aron," die er 1937 abbrach; nach der szenischen Uraufführung in Zürich 1957 hat diese Oper international zumal seit den 1970er Jahren in zahlreichen Aufführungen seine besondere Bühnentauglichkeit bewiesen. Interessant ist ferner, dass Moses sich über die gesamte Oper hinweg eines Sprechgesangs bedient, dessen Tonhöhe vorgezeichnet ist, Aron dagegen singt.

Ansonsten hinterließ die Wiener Schule keine weiteren Spuren im Standardrepertoire. Musikalisch musste sich allerdings jeder moderne Komponist mit der Zwölftonmusik auseinandersetzen und entscheiden, ob er auf ihrer Grundlage weiter arbeitete oder eher in tonalen Bahnen dachte.

Hans Pfitzner gehörte zu den bedeutendsten Komponisten der ersten Jahrhunderthälfte, die bewusst an den tonalen Traditionen festhielten. Sein Opernschaffen zeigt gleichermaßen Einflüsse Richard Wagners und frühromantischer Komponisten, wie Weber und Marschner. Pfitzners Musik wird zum großen Teil von linear-polyfonem Denken bestimmt, die Harmonik bewegt sich zwischen schlichter Diatonik und bis an die Grenzen der Tonalität gehender Chromatik. Von Pfitzners Opern ist die 1917 uraufgeführte Musikalische Legende "Palestrina" am bekanntesten geworden. Er schrieb außerdem: "Der arme Heinrich", "Die Rose vom Liebesgarten", "Das Christ-Elflein" und "Das Herz."

Franz Schreker schuf 1912 mit "Der ferne Klang" einen der großen Opernerfolge vor dem Zweiten Weltkrieg, geriet jedoch später in Vergessenheit, als der Nationalsozialismus seine Werke aus den Spielplänen verdrängte. Nach vielen früheren Versuchen begann erst in den 1980er Jahren die wirklich tief greifende Wiederentdeckung dieses Komponisten, die neben Neuinszenierungen von "Der ferne Klang" (Teatro La Fenice 1984, Wiener Staatsoper 1991) auch Aufführungen von "Die Gezeichneten", "Der Schatzgräber" oder "Irrelohe" zeitigte. Eine wesentliche Rolle in Schrekers Musik spielen stark ausdifferenzierte Klangfarben. Die chromatische Harmonik Wagners erfährt bei Schreker eine nochmalige Intensivierung, die nicht selten die tonalen Bindungen bis zur Unkenntlichkeit verwischt.

Ähnlich wie Schreker erging es dem Wiener Alexander von Zemlinsky und dem Brünner Erich Wolfgang Korngold, deren Werke es nach 1945 ebenfalls schwer hatte. Seit den 1980er Jahren gelang es beiden Komponisten, wieder einen Platz im internationalen Repertoire zu erlangen, Zemlinski mit "Kleider machen Leute," besonders aber "Eine florentinische Tragödie", "Der Zwerg" und "Der König Kandaules," Korngold mit "Die tote Stadt."

Auch das Schaffen von Walter Braunfels wurde von den Nationalsozialisten verboten und erfährt erst seit Ende des 20. Jahrhunderts wieder verstärkte Aufmerksamkeit. Mit seiner Oper "Die Vögel" war Braunfels in den 1920er Jahren einer der meist gespielten Komponisten auf deutschen Opernbühnen. An seinen Werken fällt ihre stilistische Vielseitigkeit auf: Bietet "Prinzessin Brambilla" einen auf die Commedia dell’arte zurückgreifenden Gegenentwurf zum Musikdrama der Wagnernachfolge, zeigen "Die Vögel" den Einfluss Pfitzners. Mit den späteren Opern "Verkündigung", "Der Traum ein Leben" und "Scenen aus dem Leben der heiligen Johanna" nähert Braunfels sich der Tonsprache des späteren Hindemith an.

Zu den in den 1920ern erfolgreichsten Komponisten der jungen Generation zählte Ernst Krenek, ein Schüler Schrekers, der zunächst mit in freier Atonalität gehaltenen, expressionistischen Werken für Aufsehen sorgte. Ein Skandalerfolg wurde 1927 seine Oper "Jonny spielt auf", die Elemente des Jazz aufgreift. Sie ist ein typisches Beispiel für die damals entstandene Gattung der „Zeitoper“, die ihre Handlungen dem stark vom Wechsel unterschiedlicher Moden bestimmten Alltag der damaligen Zeit entnahm. Kreneks Musik wurde von den Nationalsozialisten später als „entartet“ abgelehnt und verboten. Der Komponist emigrierte in die USA und brachte es bis 1973 auf über 20 Opern, in denen sich die wechselvolle Entwicklung der Musik des 20. Jahrhunderts exemplarisch widerspiegelt.

Der Zweite Weltkrieg bezeichnete einen großen Einschnitt in der Geschichte Europas und Amerikas, der sich auch auf die musikalische Welt auswirkte. In Deutschland wurden kaum noch Opern mit modernen Klängen gespielt und gerieten immer mehr ins Abseits. Ein bezeichnendes Beispiel hierfür bildet Paul Hindemith, der in den 1920ern mit Werken wie der Oper "Cardillac" als musikalischer „Bürgerschreck“ galt, nach 1930 aber schließlich zu einem gemäßigt modernen Stil neoklassizistischer Prägung gefunden hatte, dem u. a. "Mathis der Maler" (aus Teilen dieser Oper stellte der Komponist eine viel gespielte Sinfonie zusammen) zuzurechnen ist. Trotz des Stilwandels bekam Hindemith die Ablehnung deutlich zu spüren, da Adolf Hitler persönlichen Anstoß an seiner 1929 vollendeten Oper Neues vom Tage genommen hatte. Schließlich wurden auch Hindemiths Werke mit dem Etikett „entartet“ versehen und ihre Aufführung verboten. Hindemith ging, wie andere Künstler und Komponisten vor und nach ihm, 1938 ins Exil.

Die Zeit nach 1945 ist durch eine deutliche Internationalisierung und Individualisierung des Opernbetriebes gekennzeichnet, welche die bis dahin übliche Unterteilung in Nationale Traditionen kaum mehr sinnvoll erscheinen lässt.

Die Oper wurde immer stärker von individuellen Einflüssen der Komponisten abhängig, als von allgemeinen Strömungen. Die ständige Präsenz der „Klassiker“ des Opernrepertoires ließ die Ansprüche an moderne Opern steigen, und jeder Komponist musste seinen eigenen Weg finden, um mit der Vergangenheit umzugehen, sie fortzuführen, zu verfremden oder mit ihr zu brechen.
Im Folgenden entstanden immer wieder Opern, die die Grenzen der Gattung sprengten und zu überwinden trachteten. Auf musikalischer wie textlicher Ebene verließen die Komponisten zunehmend bekanntes Terrain und bezogen die Bühne und die szenische Aktion in den – oft genug abstrakten – musikalischen Ablauf mit ein. Kennzeichen für die Erweiterung der visuellen Mittel im 20. Jahrhundert sind die zunächst handlungsbegleitenden, später selbstständigeren Videoprojektionen.
In der zunehmenden Individualisierung der Musiksprache lassen sich in der Oper der zweiten Hälfte des 20. Jahrhunderts dennoch Strömungen erkennen: zum einen Literaturopern, deren Dramaturgie sich zu großen Teilen an der Tradition ausrichtet. Dazu werden aber mehr und mehr aktuelle Stoffe und Libretti verwendet. Dennoch sind zwei wegweisende Werke dieser Zeit ausgerechnet Opern, die Klassiker der Literatur als Grundlage verwenden, nämlich Bernd Alois Zimmermanns Oper "Die Soldaten" nach Jakob Michael Reinhold Lenz und Aribert Reimanns "Lear" nach William Shakespeare. Weitere Beispiele für die Literaturoper wären Reimanns "Das Schloss" (nach Kafka) und "Bernarda Albas Haus" (nach Lorca). Zunehmend werden auch politische Stoffe vertont, beginnend mit Luigi Nono und Hans Werner Henze; ein jüngeres Beispiel ist Gerhard Rosenfelds Oper "Kniefall in Warschau" über Willy Brandt, deren Uraufführung 1997 in Dortmund allerdings bei Publikum wie Presse gleichermaßen wenig Wirkung zeigte und keine Folgeproduktionen zeitigte.

Können schon Luigi Nonos Werke aufgrund ihrer experimentellen Musiksprache nicht mehr als Literaturoper kategorisiert werden, so wird auch die Dramaturgie der Opernvorlage auf ihre experimentellen Möglichkeiten hin ausgelotet. Der Begriff "Oper" erfährt daher eine Wandlung in der zweiten Hälfte des 20. Jahrhunderts, viele Komponisten ersetzen ihn durch "Musiktheater" oder "musikalische Szenen" und verwenden den Begriff Oper nur für explizit mit der Tradition verbundene Werke. In den Werken experimenteller Komponisten ist nicht nur ein kreativer Umgang mit Text und Dramaturgie zu entdecken, auch die Bühne, die Orchesterbesetzung und nicht zuletzt die Musik selbst überwindet konservative Muster, das Genre ist hier nicht mehr klar eingrenzbar. Zudem werden neue Medien wie Video, Elektronik eingesetzt, aber auch das Schauspiel, Tanz und Performance halten Einzug in die Oper.

Eine ganz eigene Stimme im zeitgenössischen Musiktheater verkörpert ein anderer italienischer Komponist: Salvatore Sciarrino. Er schafft mit seinem Interesse an Klangfarben oder auch der Stille in der Musik, z. T. im Rückgriff auf Kompositionstechniken der Renaissance (z. B. in seiner Oper "Luci mie traditrici" von 1998 über das Leben des Madrigal-Komponisten Carlo Gesualdo) unverwechselbare Werke.

Benjamin Britten ließ das moderne England auf den internationalen Opernbühnen Einzug halten. Von seinen überwiegend tonalen Opern sind "A Midsummer Night’s Dream", basierend auf dem Schauspiel William Shakespeares, "Albert Herring", "Billy Budd" und "Peter Grimes" am bekanntesten. Immer wieder zeigte sich Brittens Vorliebe und Talent zur Klangmalerei insbesondere in der Darstellung des Meeres.

Die "Dialogues des Carmélites" "(Gespräche der Karmelitinnen", Uraufführung 1957) von Francis Poulenc gelten als eines der bedeutendsten Werke des modernen Musiktheaters. Grundlage bildet der historische Stoff der Märtyrinnen von Compiègne, die 1794 unter den Augen des Revolutionstribunals singend zum Schafott schritten, nachdem sie sich geweigert hatten, ihre Ordensgelübde zu brechen. Auf Poulenc geht auch die zweite bekannt gewordene Oper für eine einzige Sängerin zurück: In "La voix humaine" zerbricht die schlicht als „Frau“ bezeichnete Person an der Untreue ihres Geliebten, der ihr per Telefon den Laufpass gibt. Luciano Berio verwendete in "Passaggio" zu der weiblichen Hauptfigur „Sie“ auch einen kommentierenden Chor.

Der Komponist Philip Glass, der Minimal Music verhaftet, verwendete für "Einstein on the Beach" keine zusammenhängenden Sätze mehr, sondern Zahlen, Solfège-Silben, Nonsens-Worte. Entscheidend war die Darstellung der Geschehnisse auf der Bühne. 1976 entstand "Einstein on the Beach", der erste Teil einer Trilogie, in der auch "Satyagraha" und "Akhnaten" vertreten sind – Hommagen an Persönlichkeiten, die die Weltgeschichte veränderten: Albert Einstein, Mahatma Gandhi und den ägyptischen Pharao Echnaton. Glass’ Arbeiten haben besonders in Verbindung mit den als kongenial empfundenen Inszenierungen von Robert Wilson oder Achim Freyer große Publikumswirksamkeit bewiesen.

Mauricio Kagels Bühnenwerke sind ebenso oft Werke über Musik oder Theater an sich, die am ehesten als „Szenisch-musikalische Aktion“ zu klassifizieren ist – die Musik ist kaum festgelegt, da Kagel sich der freien Improvisation seiner Interpreten überlässt, die auf Nicht-Instrumenten (Reißverschlüssen, Babyflaschen etc.) spielen oder sie ungewöhnlich benutzen, bedeutungslose Silben singen oder Handlung und/oder Musik per Zufall oder durch improvisierte Lesart entstehen lassen. Mit Witz übte Kagel dabei hintersinnige Kritik an Staat und Theater, Militär, Kunstbetrieb usw. Skandale erregte sein berühmtestes Werk "Staatstheater", in dem die verborgenen Mechanismen desselben an die Oberfläche gekehrt werden.

Luigi Nono verwendete seine Musik dagegen, um politische und soziale Missstände anzuklagen. Besonders deutlich wird dies in "Intolleranza 1960", wo ein Mann auf einer Reise zu seiner Heimat Demonstrationen, Proteste, Folterungen, Konzentrationslager, Gefängnishaft und Missbrauch bis hin zu einer Überschwemmung erlebt und schließlich feststellt, dass seine Heimat dort ist, wo er gebraucht wird.

Ein sehr produktiver Komponist war der 2003 mit dem Premium Imperiale der Japan Art Foundation (sog. "Nobelpreis der Kunst") ausgezeichnete Hans Werner Henze. Er stand von Anfang an im Konflikt mit den teilweise dogmatisch ausgerichteten herrschenden Strömungen der zeitgenössischen Musik in Deutschland (Stichwort "Darmstadt" bzw. "Donaueschingen", s. o.), griff serielle Techniken auf, wandte jedoch auch ganz andere Kompositionstechniken bis hin zur Aleatorik an. Am Beginn seines Opernschaffens stand seine Zusammenarbeit mit der Dichterin Ingeborg Bachmann ("Der junge Lord", 1952, und die Kleist-Adaption "Der Prinz von Homburg", 1961). Die "Elegie für junge Liebende" (1961) entstand mit W. H. Auden und Chester Kallman, den Librettisten von Strawinskys Oper "The Rake’s Progress". Später vertonte er Libretti von Edward Bond ("The Bassarides", 1966, und "The English Cat", 1980). Sein Werk "L’Upupa und der Triumph der Sohnesliebe" wurde 2003 bei den Salzburger Festspielen uraufgeführt. Henze, der seit vielen Jahrzehnten in Italien lebte, hat viele jüngere Komponisten nachhaltig gefördert und beeinflusst. Seit 1988 gibt es in München die von ihm gegründete "Biennale für Neues Musiktheater".
Karlheinz Stockhausen vollendete 2005 seine 1978 begonnene Heptalogie "LICHT". Mit seinem Hauptwerk hinterlässt er ein religiöse Themen behandelndes, monumentales Opus, bestehend aus sieben Opern, die jeweils für einen Wochentag stehen. Die ersten Opern erlebten in Mailand ihre Uraufführung ("Donnerstag", "Samstag", "Montag"), in Leipzig wurden "Dienstag" und "Freitag" zum ersten Mal gespielt. In seiner Gesamtheit wurde das insgesamt 29 Stunden Musik umfassende komplexe Werk nicht zuletzt wegen der immensen organisatorischen Schwierigkeiten noch nicht aufgeführt.

Aufmerksamkeit erregte in Deutschland 1996 die Oper "Das Mädchen mit den Schwefelhölzern" von Helmut Lachenmann. Sie basiert auf der bekannten Weihnachtsgeschichte von Hans Christian Andersen. Auf eigenwillige Weise und mit teilweise neuartigen Instrumentaltechniken setzt Lachenmann hier das Gefühl der Kälte in Klang um.

Nach der Statistik sind die meistaufgeführten lebenden Opernkomponisten (in den letzten fünf Spielzeiten von 2011 bis 2016, an den ersten fünf Stellen) die Amerikaner Philip Glass, Jake Heggie und Erik Lund, der Engländer Jonathan Dove und der Amerikaner John Adams. Als erfolgreichste (meistaufgeführte) deutsche Komponisten nennt Operabase Marius Felix Lange nach Aufführungszahlen an 7., Wolfgang Rihm an 9., Detlev Glanert an 16. und Vollmer an 18. Stelle. 

Seit Humperdincks Märchenoper Hänsel und Gretel (Oper) haben Opernkomponisten immer wieder Kinderopern geschrieben, wie z. B. Henze ("Pollicino", 1980), Oliver Knussen ("Wo die wilden Kerle wohnen", 1980 und 1984) und Wilfried Hiller ("Tranquilla Trampeltreu", "Norbert Nackendick", "Der Rattenfänger", "Eduard auf dem Seil", "Wolkenstein" und "Der Goggolori").

Opern sind von einer Formenvielfalt geprägt, die durch konventionelle Kompositionsstile ebenso wie durch individuelle Lösungen der Komponisten bestimmt wird. Deshalb gibt es keine allgemeingültige Formel für ihre Struktur. Grob gesehen, kann man jedoch eine Entwicklung von der Nummernoper über viele verschiedene Mischformen bis hin zur durchkomponierten Oper gegen 1900 feststellen.

Von der Barockzeit bis in die Romantik hinein ist die Oper eine Aneinanderreihung in sich geschlossener Musikstücke („Nummern“), die durch Rezitative oder (im Singspiel) gesprochene Dialoge miteinander verbunden werden und eine durchgängige Handlung darstellen. Wie auch das Schauspiel kann eine Oper in Akte, in Bilder, Szenen bzw. gegliedert sein. Die musikalischen Bestandteile der Oper sind vielfältig:



Die Trennung der Nummern und die Abgrenzung zwischen Rezitativ und Arie wurden im 19. Jahrhundert in Frage gestellt. Ab 1825 verschwand allmählich das Secco-Rezitativ, an seine Stelle trat in der italienischen Literatur das Prinzip von Scena ed Aria, das bei Giuseppe Verdi die Akte zu einem größeren musikalischen Ganzen formt. Richard Wagner propagierte ab der Mitte des Jahrhunderts den Verzicht auf die Nummernstruktur zugunsten eines durchkomponierten, auf der Grundlage von Leitmotiven geformten Ganzen. Für Wagners Opern hat sich der Begriff Musikdrama durchgesetzt, das Stichwort „Unendliche Melodie“ steht für ein kontinuierliches Fortschreiten der musikalischen und emotionalen Entwicklung, das sich nach seiner Auffassung gegen musikalische Tanzformen durchsetzen sollte. Seine Oper "Tristan und Isolde" (1865) bezeichnete Wagner als „Handlung in Musik“, was an die ursprünglichen Opernbegriffe „favola in musica“ oder „dramma per musica“ erinnern sollte.

Die durchkomponierte Form wurde im späten 19. Jahrhundert allgemein bevorzugt, auch bei Jules Massenet oder Giacomo Puccini, und blieb das vorherrschende Modell der frühen Moderne bis zum Neoklassizismus, der mit brüchigen Strukturen und mit Rückbezügen auf Formen der frühen Operngeschichte experimentierte. Auch in sich abgeschlossene Teile aus durchkomponierten Opern werden in Konzerten aufgeführt, wie etwa viele Arien aus Puccini-Opern. Als Meister der durchkomponierten Großform gilt Richard Strauss, der dies insbesondere in den Einaktern "Salome", "Elektra" und "Ariadne auf Naxos" unter Beweis stellte.

Im 20. Jahrhundert griffen viele Komponisten wieder auf das Nummernprinzip zurück, zum Beispiel Zoltán Kodály, Igor Strawinski oder Kurt Weill. Die Nummernoper besteht außerdem in Operette und Musical weiter.

In der Geschichte der Oper gab es zumeist einen „hohen“ und einen „niederen“ Stil, frei nach der antiken Unterscheidung zwischen Tragödie und Komödie. Nicht immer bedeutet dies jedoch eine Grenze zwischen ernst und lustig. Der „hohe“ Stil kann sich über den „niederen“ auch einfach durch antike Stoffe erheben oder durch adlige Figuren oder durch eine „literarisch“ ernst zu nehmende Vorlage oder durch „schwierige“ (bzw. bloß durchkomponierte) Musik. All diese Anhaltspunkte für das Wertvollere wurden im Lauf der Geschichte angegriffen. Dabei gab es Gattungen, die den Gegensatz abzuschwächen versuchten wie die Opera semiseria.

Solange die Oper noch im Stadium des Experiments war, wie bis zu Beginn des 17. Jahrhunderts, war eine Trennung noch nicht nötig. Sie ergab sich erst, als Opernaufführungen zur Gewohnheit wurden, und zwar aus sozialen Gründen: Die ernste Oper enthielt aristokratisches Personal und „hohe“ politische Symbolik, die komische hatte bürgerliche Figuren und „unwesentliche“ alltägliche Handlungen zum Thema. Allmählich trennten sich Opera seria und Tragédie lyrique von ihren komischen Intermezzi, aus denen Opera buffa und Opéra-comique hervorgingen. Diese Trennung wurde erst am Ende des 18. Jahrhunderts aufgebrochen: Weil die Bürger in der für sie bestimmten „niederen“ Operngattung nicht mehr komisch (also lächerlich) dargestellt werden wollten, wurde das Komische oft ins Sentimentale abgebogen und aufgewertet. Daher sind „komische Opern“ oft nicht lustig. Nach der Französischen Revolution löst sich die Ständeklausel auf, und auch bürgerliche Opern durften „ernst“ sein. Somit ergaben sich im 19. Jahrhundert andere Abgrenzungen zwischen Tragödie und Komödie als im 18. Jahrhundert.

Ein Sammelbegriff sowohl für tragische als auch für komische Werke ist das italienische Dramma per musica, wie die Oper in ihrer Anfangszeit betitelt wurde. Ein Beispiel für eine frühe ernste Oper ist "Il ritorno d’Ulisse in patria" von Claudio Monteverdi. Der seriöse Anspruch resultiert aus dem Rückgriff auf antike Theaterstoffe – insbesondere Tragödien – und epische Heldendichtungen. Sie wurden seit dem späteren 18. Jahrhundert von jüngeren historischen Sujets verdrängt. Im Italien des 19. Jahrhunderts wurde der Begriff Dramma in der Zusammensetzung Melodramma verwendet und nicht mehr auf das antike Drama bezogen. Sowohl Bellinis tragische Oper "Norma" als auch die komödiantische Oper "L’elisir d’amore" von Gaetano Donizetti wurden so genannt.

Als fester Begriff etablierte sich die Opera seria erst im 18. Jahrhundert. Mischformen oder tragikomische Inhalte waren mit dieser Titelbezeichnung ausgeschlossen. Händels Oper "Radamisto" ist ein typisches Werk. Als Antipode zu Italien verlieh Frankreich seiner eigenen Form der Opera seria den Titel Tragédie lyrique, wesentlich geprägt durch Jean-Baptiste Lully und das Ballett am Hofe Louis’ XIV., später durch Jean-Philippe Rameau. Nach der Französischen Revolution etablierte sich allmählich die Grand opéra als bürgerliche ernste Oper. Dazu zählen "Les Huguenots" von Giacomo Meyerbeer, auch weniger erfolgreiche Werke wie "Les Troyens" von Hector Berlioz.

Das durchkomponierte Musikdrama des reiferen Richard Wagner "(Der Ring des Nibelungen)" hatte großen internationalen Einfluss. Französische Komponisten jener Zeit wie Massenet setzten dagegen eher auf einen durchsichtigen und gesanglichen Opernstil, für den die Bezeichnung Drame lyrique verwendet wurde. Noch Debussy verwendete diesen Begriff für seine Oper "Pelléas et Mélisande".

Schon immer konnten Opernstoffe von Romanen, Novellen oder Bühnenwerken herstammen. Die italienische Oper des 18. Jahrhunderts verstand sich als in Musik gekleidete Literatur. Seitdem die Musik die absolute Vorherrschaft erlangt hat, also seit dem späten 19. Jahrhundert, nennt man ausgesprochen literarische Opern Literaturoper. "Death in Venice" von Benjamin Britten nach der Vorlage von Thomas Mann ist eine recht getreue Umsetzung des literarischen Stoffes in Musik.

Die Opera buffa ist die Urform der heiteren Oper. Pergolesis "La serva padrona" galt um die Mitte des 18. Jahrhunderts als das maßgebliche Beispiel. Ein spätes Beispiel ist "Il barbiere di Siviglia" von Gioachino Rossini. Die ausnehmend heiteren Opern waren oft geringer angesehen als die sentimentalen. Ihre Stoffe stammen aus dem Volkstheater und von der Posse, stark beeinflusst durch die italienische Commedia dell’arte.

Aus der frühen Opera buffa geht die französische Opéra-comique (Werkgattung) hervor, die vor der Revolution zur Oper eines zunehmend selbstbewussten Bürgertums wird. Zunächst verstand man hierunter eher ein Liederspiel (Vaudeville). Doch der musikalische Anteil wurde immer größer und begann zu überwiegen. Aus der Opéra-comique ist das deutschsprachige Singspiel entstanden. Das Singspiel trägt oft volkstümlich-bürgerlichen Charakter, ist geprägt von einfachen Lied- bzw. Rondo-Formen und verwendet statt Rezitativen gesprochene Dialoge, gelegentlich auch Melodramen zwischen den musikalischen Nummern.

Der Hof sprach Französisch. Das Problem der deutschen Oper war im 18. und zum Teil noch im 19. Jahrhundert, dass sie als volkssprachliche Oper zur „niederen“ Gattung gehörte und sich behaupten und emanzipieren musste. "Die Entführung aus dem Serail" von Wolfgang Amadeus Mozart ist eines der bekanntesten Singspiele mit dieser Zielsetzung. Mozart bedient sich für die Arien auch komplexerer musikalischer Formen. Das im Auftrag von Kaiser Joseph II. zur Etablierung eines "Nationalsingspiels" geschaffene, 1782 am Wiener Burgtheater uraufgeführte Werk war für die Entwicklung der deutschen Oper von entscheidender Bedeutung.

Paris war im 19. Jahrhundert führend für die Operngeschichte, und auch die Italiener wie Rossini und Verdi kamen hierher. Die Opéra-comique, die im Haus der Opéra-Comique aufgeführt wurde, blieb auch gegenüber der neu entstandenen, durchkomponierten Grand opéra, die in der Opéra zur Aufführung kam, zweitrangig – weniger von ihrer musikalischen als von ihrer sozialen Bedeutung her. Aus den erwähnten Gründen musste sie nicht unbedingt einen heiteren Inhalt haben. Ein auch im deutschen Sprachgebiet bekanntes Beispiel einer komisch-rührseligen Opéra-comique ist "Der Postillon von Lonjumeau" von Adolphe Adam. Eine Gruppe von formal noch als Opéra-comique zu bezeichnenden Werken nach 1860 verstärkte den sentimentalen Grundcharakter (etwa "Mignon" von Ambroise Thomas). Ein sentimentaler Einschlag findet sich auch in einigen komischen Opern von Rossini ("La Cenerentola").

Eine Erneuerung der Opéra-comique gelang mit "Carmen" von Georges Bizet, deren Dramatik in die Richtung der Verismo-Oper weist. Bei ihr war – abgesehen von den proletarischen Figuren – das Reißerische ein Merkmal des „niederen“ Stils.

Auch die „Größe“ kann ein Zeichen für hohen oder niederen Stil sein. Zuweilen findet sich der Begriff „Große Oper“ als Untertitel eines Werkes. Damit wird zum Beispiel gesagt, dass das Orchester und der Chor in großer Besetzung spielen und singen sollten, oder dass die Oper ein abendfüllendes Werk mit integriertem Ballett ist. Dies sind Opern, die nur in einem größeren Theater zur Aufführung kommen und sich vom Repertoire der fahrenden Truppen unterscheiden konnten. Als Beispiel für eine „Große Oper“ ist "Manon" von Jules Massenet zu nennen.

Der Begriff Kammeroper bezieht sich dagegen auf ein mit geringem Personal realisierbares Werk. Die Anzahl der Sänger ist in der Regel nicht mehr als fünf, das Orchester wird auf ein Kammerorchester begrenzt. Dies konnte aus der Not materielle Armut hervorgehen und damit auf das „niedere“ Genre verweisen oder im Gegenteil die größere Exklusivität und Konzentration eines „höheren“ Genres bedeuten. Auch die Bühne ist oftmals kleiner, was zu einer intimeren Atmosphäre beitragen kann, die für die Wirkung des Werkes von Vorteil ist. Beispiele dafür wären "Albert Herring" von Benjamin Britten oder „Les Larmes de couteau“ von Bohuslav Martinů.

Manche Opernkomponisten wehrten sich auch gegen die Einordnung in Gattungstraditionen oder bezeichneten ihre Werke in bewusster Relation zu diesen mit bestimmten Untertiteln. Wagners "Tristan und Isolde" trägt zum Beispiel die Bezeichnung „Handlung in Musik“, Luciano Berio verwendete für sein Werk "Passaggio" etwa den Begriff "Messa in scena" („Inszenierung“). George Gershwin beschrieb sein Werk "Porgy and Bess" als „An American Folk Opera“. Um sich von klischeehaften Vorstellungen abzugrenzen, bevorzugen moderne Komponisten oft alternative Bezeichnungen wie etwa „azione scenica“ ("Al gran sole carico d’amore" von Luigi Nono) oder „azione musicale“ (musikalische Handlung) ("Un re in ascolto" von Luciano Berio). Auch ein bekanntester Beispiel Eugen Onegin wurde vom Komponisten „Lyrische Szenen“ genannt.

Richard Geppert schrieb 2016 die deutsche Rockoper "Freiheit" mit den musikalischen Ausdrucksmitteln und Instrumenten der Rockmusik.

Aufgrund der nicht immer leichten Abgrenzbarkeit der Gattung Oper von anderen musikalischen Gattungen und Genres und der Praxis des Pasticcios ist eine Aussage zum Gesamtumfang des Opern-Repertoires mit zahlreichen Schwierigkeiten behaftet. Aktuelle Auflistungen gehen von ca. 5800 bis 6000 bekannten Werken aus. Rechnet man die nicht unerhebliche Anzahl verschollener und verlorener Werke, insbesondere des 18. und frühen 19. Jahrhunderts mit ein, dürfte eine Gesamtzahl von ca. 60.000 Opern realistisch sein.
Die große Menge an Werken macht es Theatern und Opernhäusern nicht einfach, eine Auswahl zu treffen, die einem hohen Anspruch genügt und auch genügend Publikum findet. Abhängig von der Größe des Theaters und dem vorhandenen Budget wird von Intendant und Dramaturgie für jede Sparte des Theaters (Schauspiel, Musiktheater, Ballett, Kinder- und Jugendtheater, Puppentheater etc.) ein Spielplan erarbeitet, der dem Haus und seinen Mitarbeitern angepasst ist. Der Spielplan geht auf die regionalen Eigenheiten und Aufführungstraditionen des Ortes ein – zum Beispiel durch open air-Festspiele, Weihnachts- oder Neujahrskonzerte – weist aber auch auf aktuelle Strömungen des Musiktheaters hin, indem auch zeitgenössische Werke aufgeführt werden. Je nach Größe des Hauses werden verschiedene Opern in einer Spielzeit neu inszeniert. Die erste öffentliche Darbietung einer neuen Oper nennt man Uraufführung, die erste öffentliche Darbietung einer Oper in einer neuen Inszenierung Premiere.

Nach und nach hat sich ein praxiserprobter, mehr oder weniger enger Kanon an Opern herausgebildet, die regelmäßig auf dem Spielplan stehen. Etwa 150 Opern bilden diesen nicht festgeschriebenen Kanon im Kern. Entsprechend hat sich das Interesse vor allem des Feuilletons von den vielfach bereits bekannten Werken hin zu deren Interpretation verlagert, wobei vor allem die Inszenierung in den Vordergrund rückt. Das Publikum verbindet seine Lieblingsopern oft mit bestimmten Traditionen, die zum Teil auch in Konventionen erstarrt sind, und reagiert auf radikale Deutungsansätze (Regietheater) kontrovers.

Bis zur Mitte der 1960er Jahre wurden Opern zumeist in der jeweiligen Landessprache des Aufführungsortes aufgeführt. So wurden Verdi-Opern in Deutschland in deutscher Sprache und Wagner-Opern in Italien in italienischer Sprache gesungen, wie auch Radio- und Fernsehaufzeichnungen belegen. Bereits zuvor gab es jedoch Theater, die Opern in der jeweiligen Originalsprache aufführten, etwa die Metropolitan Opera in New York. Auch die Salzburger Festspiele zeigten Opern stets ausschließlich in der Originalsprache. Aufgrund eines Vertrages mit der Mailänder Scala, bei dem sich italienische Sänger verpflichteten, auch an der Wiener Staatsoper zu singen, führte Herbert von Karajan 1956 an der Wiener Staatsoper das Prinzip ein, Opern in der Originalsprache aufzuführen. Mit seiner Begründung, die Einheit von Wort und Musik gehe bei Übersetzungen in eine andere Sprache verloren, wurden Opern allmählich immer mehr in ihrer ursprünglichen Form aufgeführt. Auch der Schallplatten und Sänger-Markt, der sich zunehmend internationalisierte, trug entscheidend zu dieser Entwicklung bei. In der DDR gab es hingegen weiterhin eine große Tradition von Übersetzungen, jedoch wurde mit neuen Übertragungen (z. B. Walter Felsenstein, Siegfried Schoenbohm) versucht, den Inhalt des Originals genauer, sprachlich gelungener und vor allem musikalisch passender umzusetzen. Heute werden in fast allen großen Opernhäusern Opern in der Originalsprache aufgeführt und dazu simultan Übertitel eingeblendet.

An vielen kleineren Theatern, vor allem im Osten Deutschlands, gibt es noch Aufführungen in deutscher Sprache. Auch gibt es in einigen Städten (z. B. Berlin, München, Wien) mehrere Opernhäuser, von denen eines Opern in Übersetzungen aufführt, wie etwa die Volksoper Wien, die Komische Oper Berlin, das Staatstheater am Gärtnerplatz in München, oder in London die English National Opera. Hin und wieder gibt es auch eine autorisierte Übersetzung (wie im Falle der Opern Leoš Janáčeks, deren deutscher Text von Janáčeks Freund Max Brod stammt, so dass auch der deutsche Text als original gelten darf). Schwierig gestaltet sich die Aufführung in Originalsprache auch immer dann, wenn Dialoge in dem Werk vorkommen. Hier gibt es auch Mischformen, das heißt, gesprochene Texte werden übersetzt, gesungene erklingen jedoch in Originalsprache. Im Bereich Singspiel, Operette, Musical ist daher die übersetzte Musiktheateraufführung weit verbreitet. Für die exakte Übersetzung aus einer Fremdsprache ist am Theater die Dramaturgie zuständig. Wenn die Sprachkenntnisse der Korrepetitoren vertieft werden sollen, werden auch spezialisierte Coaches für eine Fremdsprache hinzugezogen.






</doc>
<doc id="3794" url="https://de.wikipedia.org/wiki?curid=3794" title="Ordensgemeinschaft">
Ordensgemeinschaft

Eine Ordensgemeinschaft (auch "Orden", von ‚Ordnung‘, ‚Stand‘) ist eine durch eine Ordensregel verfasste Lebensgemeinschaft von Männern oder Frauen, die sich durch die Profess an ihre Lebensform binden und ein geistliches Leben in Gemeinschaft führen, in der Regel in einem Kloster.

Von Ordensleuten zu unterscheiden sind Eremiten, die zwar auch Mönche oder Nonnen sein können, aber als Einzelne ein Leben in Abgeschiedenheit führen, und andere Formen geweihten Lebens (etwa gottgeweihte Jungfrauen, Mitglieder von Säkularinstituten, evangelische Diakonissen und Diakonengemeinschaften). Der im Deutschen außerhalb des kirchlichen Sprachgebrauchs allerdings wenig gebräuchliche Oberbegriff für alle, die in einer der durch Gelübde oder bindende Versprechen begründeten Formen des geweihten Lebens (lat. "Vita consecrata") leben, lautet "Religiosen" oder "gottgeweihte Personen".

Innerhalb der Ordensgemeinschaften der (lateinischen) Kirche unterscheidet man

Zu den alten Orden, die feierliche Gelübde ablegen, gehören länger als 700 Jahre bestehende Gemeinschaften, darunter monastische Orden, deren Mitglieder Mönche oder Nonnen sind, geistliche Ritterorden, Bettelorden und Regularkanoniker. Ordensgemeinschaften neueren Ursprungs werden meist als Kongregationen bezeichnet. Der Codex Iuris Canonici (CIC) von 1983 kennt die Unterscheidung zwischen Orden und Kongregationen nicht mehr. Über das Eigenrecht der einzelnen päpstlich oder bischöflich approbierten Gemeinschaften sind die teilweise sehr alten Regelungen aber weiterhin Bestandteil des katholischen Kirchenrechts.

Der CIC von 1983 unterscheidet drei Formen von Ordensgemeinschaften und neueren verwandten, nach dem Zweiten Vatikanischen Konzil entstandenen Lebensformen:
Ordensinstitute werden zusammen mit den Säkularinstituten als Institute des geweihten Lebens (Cann. 573–606) bezeichnet.

Es gibt neben den Ordensgemeinschaften in der römisch-katholischen Kirche auch anglikanische sowie evangelische Gemeinschaften und Kommunitäten. Kaum Ordensgemeinschaften gibt es dagegen in den orthodoxen Kirchen und den in deren kirchlicher Tradition stehenden katholischen Ostkirchen. Das orthodoxe Mönchtum wird vielmehr größtenteils in selbständigen Klöstern und Klosterverbänden (z. B. die Mönchsrepublik vom Berg Athos) praktiziert. In einem allgemeinen, weiteren Verständnis fasst man auch orthodoxe Mönche und Nonnen unter den Oberbegriff des Ordenslebens.

Die aus dem Kirchenrecht stammende Bezeichnung "Orden" wurde später auch von bestimmten weltlichen Gemeinschaften verwendet. So stifteten europäische Monarchien seit dem 14. Jahrhundert eine Reihe von höfischen Ritterorden, aus denen später meist wichtige Verdienstorden hervorgingen (z. B. Hosenbandorden).

Außer im Christentum gibt es Orden oder ordensähnliche Gemeinschaften auch in anderen Religionen, beispielsweise im Buddhismus, Hinduismus und Islam. Spiritualität und Lebensformen sind in den verschiedenen Religionen sehr unterschiedlich.

Während der Zeit der Christenverfolgung war die große Anziehungskraft des christlichen Glaubens unter anderem darin begründet, dass Menschen mit Unbedingtheit und Unbeirrbarkeit ihren Glauben vertraten – das Neue Testament nennt dies „Zeugnis ablegen“ –, selbst wenn sie dafür ihr Leben als Märtyrer oder Blutzeugen verloren. Dies beruhte auf der Naherwartung der Wiederkunft Christi. Man glaubte, dass das Jüngste Gericht innerhalb der ersten oder zweiten Generation nach Jesu Tod kommen werde und dass man sich dafür am besten durch kompromisslose Hingabe an das Gottesreich würdig erweisen konnte.

Durch Kontakt mit der Gnosis und der griechischen Philosophie entwickelte die frühe Christenheit eine von einem Hang zur Askese und einer gewissen Leibfeindlichkeit gekennzeichnete Spiritualität, bei der persönliche Hingabe an die Stelle der Naherwartung trat. Die Anhänger dieser Strömung suchten eine tiefere Gottesbegegnung und ihr persönliches Heil durch Enthaltsamkeit, Bußübungen, ständiges Gebet und Schweigen zu erlangen. Dabei kam ein sehr radikales Vollkommenheitsideal zum Tragen, das innerhalb einer weltlich orientierten Umgebung nur schwer zu verwirklichen war.

Bald führte das Bedürfnis, eine tiefere Gottverbundenheit und Spiritualität zu verwirklichen, zur Entwicklung des christlichen Eremitentums, dessen theologische Basis die alttestamentliche „Wüstentheologie“ ist; so bedeutet das Wort „Eremit“ wörtlich „Wüstenbewohner“. Der Begriff nimmt Bezug auf die innere Einkehr in der Wüste, die als Bild nicht nur für Stille und Zurückgezogenheit, sondern auch für den Gehorsam und die Anerkennung Gottes als Herrn steht, wie sie in der 40-jährigen Wanderung der Israeliten in der Wüste nach ihrem Auszug aus Ägypten sowie in den Berufungsgeschichten des Mose und vieler biblischer Propheten zum Ausdruck kommt. So berichten die Evangelien von einem 40-tägigen Aufenthalt Jesu in der Wüste als einem einschneidenden Moment der Entscheidung und Begegnung mit Gott. Das christlich-eremitische Leben entwickelte sich etwa zeitgleich in Syrien und Ägypten. Als erster christlicher Eremit in Ägypten gilt Paulus von Theben; sein Schüler Antonius der Große wurde zu einem der großen Wüstenväter.

Im Verlauf des 3. Jahrhunderts führten die Erfahrungen der Eremiten, die sich oft zu Einsiedlerkolonien zusammenschlossen, zu dem Bedürfnis vieler, ein auf Gebet und Askese konzentriertes, zurückgezogenes Leben auch in einer Gemeinschaft führen zu können.

Mönche und Nonnen – deren Lebensform sich aus Zusammenschlüssen von geweihten Jungfrauen entwickelt hatte –, die sich dem kontemplativen Leben in Gemeinschaft widmen, nennt man im Unterschied zu den Eremiten (Anachoreten) „Koinobiten“. Um 320 gründete Pachomios (um 292–346) in Oberägypten das erste Kloster. Basilius von Caesarea verfasste um 350 in Anlehnung an Pachomios’ "Engelsregel" eine Mönchsregel, die heute noch für die Mehrzahl der Klöster der orthodoxen Kirchen gilt und auch Grundlage für die von Benedikt von Nursia um 540 verfasste Regula Benedicti war. Die Regeln der frühen Mönchsgemeinschaften zielten in der praktischen Verwirklichung des Evangeliums auf ein Gleichgewicht zwischen Gebet und tätiger Arbeit ab und schrieben ein anspruchsloses, brüderliches gemeinsames Leben vor. Schon früh wurden die evangelischen Räte (Armut, Ehelosigkeit und Gehorsam) als Synthese und Richtschnur dieser Lebensweise angesehen und entwickelten sich zum unterscheidenden Merkmal des Mönchtums und des Ordensstandes überhaupt. Sie sollten es den Religiosen ermöglichen, in einer Nachahmung der Lebensweise Jesu "(Imitatio Christi)" zu leben und damit sowohl ihre Gottesbeziehung zu vertiefen als auch für das Seelenheil der Menschen zu beten.
Im frühen Mittelalter spielte die iroschottische Kirche in Europa eine zentrale Rolle bei der Verbreitung des christlichen Glaubens und des Ordenswesens. Als herausragende Mönche der frühen Zeit sind die hll. Patrick und Columban von Iona zu nennen.

Auf dem europäischen Festland konnten sich in der Spätantike und im frühen Mittelalter die kirchlichen Strukturen, durch die Wirren der Völkerwanderung, nur langsam entwickeln. Das gilt insbesondere für die zuvor römisch besetzten Gebiete. Dort brachen die Verwaltungsstrukturen in den Zeiten des Umbruchs völlig zusammen, bis die germanischen Stämme ihre Gebiete klar abgesteckt hatten. Das Leben auch klösterlicher Gemeinschaften war somit ständig bedroht. Anders war das in Irland, einem Gebiet das niemals römisch besetzt war und nicht von der Völkerwanderung tangiert wurde. Die Ordensgemeinschaften Irlands hatten entsprechend auch während der Völkerwanderung stabile und gefestigte Strukturen. Mönche hatten eine hohe gesellschaftliche Stellung inne, da sie von Adel und Bevölkerung als Vertreter des neuen Glaubens und zugleich als legitime Nachfolger der keltischen Druiden anerkannt wurden. So erblühten die Klöster Irlands in der Spätantike und im Frühmittelalter. Die Klöster waren kulturelle und religiöse Zentren und legten großen Wert auf das Studium der Schriften. Irland hatte über Jahrhunderte hinweg den Ruf als „Insel der Heiligen und Gelehrten“, weshalb fränkische Herrscher wie Karl der Große irische Gelehrte an ihren Hof holten. Die iroschottische Kirche wurde jahrhundertelang zentral von ihren Klöstern geprägt. Anders als in den römisch besetzten Gebieten Englands und des Festlands gab es in Irland keine Diözesen. Die Äbte ernannten vielmehr in ihrem Kirchensprengel den Bischof und waren ihm vorgesetzt, bisweilen übten sie auch beide Ämter aus.

Vor diesem Hintergrund war es möglich. eine weitreichende Missionstätigkeit zu entwickeln, die immer eng an das Mönchtum geknüpft blieb und bereits im 7. Jahrhundert sehr erfolgreich war. Die Mönche missionierten zunächst in Schottland, wobei das Jahr 563 als Beginn gilt, als Columban auf der Insel Iona ein Kloster gründete. Sie weiteten die iroschottische Mission danach auf das europäische Festland aus. 590 brach erstmals ein Mönch, Columban der Jüngere (†615), von den Inseln auf, um auf dem europäischen Festland zu missionieren. Er begann seine Missionstätigkeit auf fränkischem Gebiet, missionierte in Frankreich, Italien und der Schweiz und gründete drei Klöster: Luxeuil, Bregenz und Bobbio. Nachfolger Kolumbans war sein Schüler Eustasius (†629), der 615 Abt im Kloster Luxeuil wurde. In Süddeutschland, Österreich und der Schweiz wirkten Gallus (†645) und Kilian (†689). Infolge von Columbans Festlandsmission kam es im 7. Jahrhundert zu rund 300 Klosterneugründungen. Den iroschottischen Mönchen gelang es mit Hilfe des fränkischen Adels auch, auf dem Lande erfolgreich zu missionieren und in ländlichen Gebieten Klöster zu gründen, während das Christentum zuvor vor allem in den Städten verbreitet war. Das iroschottische Mönchtum und die Regel Columbans verbreiteten sich auf diese Weise in Europa. Durch Beschluss des Konzils von Autun (um 670) versuchte man zwar, die Regula Benedicti für alle Orden verbindlich einzuführen, tatsächlich blieben aber beide Regeln bis 817 (Reform Benedikts von Aniane), vorwiegend in Mischform, verbreitet. Als letzter wichtiger Vertreter der iroschottischen Mission gilt Virgilius, der um 750 Bischof in Salzburg wurde. Iroschottische Klöster prägten somit durch ihre frühe Missionstätigkeit die Verbreitung des christlichen Glaubens und des Mönchtums in Europa nachhaltig.

In ihrem Bemühen, ihr geistliches Ideal mit einer nutzbringenden Arbeit zu verbinden und diese Aufgabe mit Sorgfalt zu erfüllen, hatten die Orden, vor allem die benediktinischen, großen Anteil an der Kultivierung Europas. Das in den Klöstern angesammelte Wissen ermöglichte es, die Kultur in den Bereichen Landwirtschaft, Gartenbau, Medizin, Literatur, Musik, Kunst und Philosophie auf einen annähernd so hohen Stand zu bringen, wie er im römischen Reich vor der Völkerwanderung bestanden hatte.

Schenkungen, Erbschaften und erfolgreiches Wirtschaften führten in den Klöstern wie in der gesamten Kirche zu einem Anwachsen des Vermögens und der wirtschaftlichen und gesellschaftlich-politischen Macht. Im Lauf der Zeit kamen immer wieder Reformbewegungen auf, die zu den Ursprüngen des Mönchtums zurückkehren und die Klostergemeinschaft vor allem durch stärkere Askese und Disziplin gegen Verwässerung der geistlichenen Ideale und Verfall der Sitten schützen wollten. Dadurch kam es häufig zu Abspaltungen und Neugründungen. Im Zuge der Kirchenreformen des 11. Jahrhunderts gewann das so erneuerte Mönchtum (speziell Cluny und seine Tochtergründungen) entscheidenden kirchenpolitischen Einfluss und stellte eine Reihe von Päpsten. Später war es die von dem Cluniazenser und Prediger Bernhard von Clairvaux inspirierte Reformbewegung der Zisterzienser, die die benediktinische Lebensweise wieder zu ihrer alten Strenge zurückführen wollte. Durch vielfache Klostergründungen und Rodungen in bis dahin wenig besiedelten oder unzugänglichen Waldgebieten wurden besonders die Zisterzienser im 12. Jahrhundert zu einem Motor der siedlungsgeschichtlichen Dynamik in vielen Gebieten Europas.

Zur Betreuung der Pilger aller Religionen in Jerusalem entstand dort im 11. Jahrhundert ein Hospitalsorden, dessen Aktivitäten 1099 erstmals dokumentiert sind und der 1113 als Johanniterorden von der Kirche anerkannt wurde. Ihm folgten mehrere geistliche Ritterorden, zur Pflege der Pilger als Hauptaufgabe trat deren Schutz und die Verteidigung der Pilgerstätten im Heiligen Land.

Als Reaktion auf die sozialen Spannungen in der hochmittelalterlichen Gesellschaft, die von einer wachsenden Bedeutung der Städte und den Umbrüchen der entstehenden Geldwirtschaft geprägt war, kamen im 13. Jahrhundert die Bettelorden oder "Mendikanten" auf. Diese neuen Gemeinschaften stellten die Armut und Bedürfnislosigkeit Jesu Christi in den Mittelpunkt ihres Lebens, das sich vor allem für die Ordensmänner nicht mehr vorwiegend in der Abgeschiedenheit der Klöster abspielte, sondern in den Städten und mitten unter der Bevölkerung. Die Predigt war eine Hauptaufgabe der Brüder- Während sich die Dominikaner besonders der Erneuerung der Priesterausbildung, der theologischen Wissenschaft und der Katechese widmeten, stand bei den Franziskanern die Seelsorge und die unbedingte Beachtung des Armutsideals im Vordergrund. Beide Gemeinschaften sind als kirchliche Antworten auf die damaligen akuten Gefährdungen der Kirche durch Zeitströmungen zu begreifen. Sie nahmen daher auch in der Ketzerverfolgung und Inquisition wichtige Funktionen ein. Auch die Karmeliten (eigentlich ein Eremitenorden) und die Augustiner-Eremiten (Mitte des 13. Jahrhunderts aus norditalienischen Mendikantengruppen entstanden) gehören zu den Bettelorden.

Martin Luther, der zunächst selbst dem Orden der Augustiner-Eremiten angehörte, lehnte in seinen reformatorischen Lehren den Zölibat der Priester und die Ordensgelübde ab (einer freiwilligen Ehelosigkeit stand er zumindest anfänglich jedoch nicht ablehnend gegenüber). Die Verstrickung mancher Orden in die Ausbeutung der unteren Bevölkerungsschichten (Unfreiheit der Bauern, Fürstäbte) führte dazu, dass in den Bauernkriegen viele Abteien geplündert wurden. Nonnen und Mönche, die sich der Reformation anschlossen, verließen ihre Ordensgemeinschaften. Häufig wurden die Frauenklöster aber in weltliche Stifte umgewandelt, in denen die Stiftsdamen keine Gelübde ablegten. Klöster in den evangelischen Fürstentümern und Städten wurden geschlossen. Das Vermögen und die Gebäude der Orden und Klöster wurden dabei manchmal von den Fürsten beschlagnahmt, meist allerdings für die Bezahlung der neuen evangelischen Pfarrer oder die Einrichtung von Schulen und Spitälern verwendet. Im 16. Jahrhundert bildete der neugegründete Orden der Jesuiten ein wichtiges ausführendes Organ der einsetzenden Gegenreformation.

Die Eroberung Amerikas und die Ausbreitung der Europäer über die gesamte Welt brachte eine völlig neue Perspektive im Hinblick auf die Mission. In der Folge vermischten sich redliche Bemühungen, die indigene Bevölkerung mit dem christlichen Glauben bekannt zu machen, und die Ausbeutung der Menschen zu einer Missionierung mit Feuer und Schwert. Die Ordenspriester der Franziskaner, die Jesuiten und die Dominikaner waren die ersten, die in Amerika missionierten, wobei es Priester gab, die Sklaverei und Zwangstaufen als Mittel zur Bekehrung und Zivilisierung der Bevölkerung ansahen. Manche Orden gaben sich hier als ausführende Organe der erobernden Fürsten her. Es gab aber auch kritische Stimmen (z. B. der Dominikaner Bartolomé de Las Casas), die sich dieser Barbarei entgegenstellten.

Im 18. Jahrhundert führte das Zeitalter der Aufklärung dazu, dass viele Fürsten, auch Kardinäle, dem Ordensleben kritisch gegenüberstanden, sofern es nicht mit einer humanistischen oder sozialen Komponente verbunden war. In der Folge wurden kontemplative Gemeinschaften aufgefordert, sich an der Schulbildung der Bevölkerung zu beteiligen. Ende des 18. Jahrhunderts in Frankreich und zu Beginn des 19. Jahrhunderts in Gebieten unter französischer Herrschaft führte die Säkularisation zur Enteignung und Aufhebung vieler Klöster. Die Ländereien und Vermögen der Ordensgemeinschaften flossen dem französischen Staat oder den Fürsten zu. Viele Konvente starben aus, weil sie keine Novizen mehr aufnehmen durften und das Ablegen der Profess verboten war.

Nach der Säkularisation dagegen fand in der katholischen Kirche ein Neuaufbruch auch des Ordenslebens statt. Soziale Missstände wie mangelnde Krankenpflege, Volksbildung und Kinderfürsorge wurden aufgegriffen, indem Weltpriester oder Laien an vielen Orten Gemeinschaften gründeten, die häufig die Drittordensregel des hl. Franz von Assisi oder die Regel der Vinzentinerinnen annahmen. Die evangelische Kirche griff dieses Anliegen in den mehrheitlich reformierten Gebieten unter anderem durch die Diakonissen und die von Bodelschwinghschen Anstalten in Bethel auf.

Seit den 1960er Jahren geriet das Ordensleben in Westeuropa insgesamt zunehmend in eine personelle und damit verbundene strukturelle Krise. In der säkularisierten Welt ist die Attraktivität des Ordenslebens gesunken, das Leben als Ordensfrau oder Ordensmann verliert an gesellschaftlichem Prestige, und das Verständnis für eine solche Lebensform ist vielfach geschwunden. Neueintritte, vor allem bei den aktiven Orden, gehen in den westlichen Ländern dadurch zurück. Viele Kongregationen mussten ihr Tätigkeitsfeld in den vergangenen Jahrzehnten stark reduzieren, ändern oder Niederlassungen schließen. Das entstandene unausgewogene Altersverhältnis führte mancherorts zu Spannungen. Dabei lassen sich in manchen Bereichen Unterschiede zwischen aktiv und kontemplativ lebenden Gemeinschaften erkennen, wobei Letztere ebenso wie einige neugegründete Ordensgemeinschaften (etwa die Gemeinschaften von Jerusalem oder die Gemeinschaft vom Lamm) eine dem allgemeinen Trend gegenläufige gute Nachwuchssituation haben. Auch stellt sich die Lage des Ordenslebens in anderen Kontinenten (etwa in Afrika und bis vor einiger Zeit in Lateinamerika) anders dar, wo einige Gemeinschaften eine hohe Zahl an Neueintritten haben und viele Neugründungen vornehmen.

In der Westkirche unterscheidet man heute sechs Grundformen des Ordenslebens, denen sich die einzelnen Verbände zuordnen lassen (siehe auch Liste der Ordensgemeinschaften):

Nicht in dieser Systematik erfasst sind die mit dem Ordensleben verwandte, relativ junge Lebensform der Säkularinstitute sowie die Gesellschaften apostolischen Lebens (etwa die Pallottiner oder die Vinzentinerinnen), die formalrechtlich nicht als Ordensinstitute betrachtet werden.

Maßgebend sind zurzeit die Bestimmungen über die Institute des geweihten Lebens im Codex des Kanonischen Rechtes in der Fassung von 1983.

Ein römisch-katholischer Orden im engeren Sinne ist eine Gemeinschaft von Mönchen, Ordensbrüdern und Ordenspriestern oder Regularkanonikern beziehungsweise Nonnen oder Ordensschwestern, die sich in feierlichen Gelübden zum Leben nach den Evangelischen Räten unter einem Oberen und nach ihrer jeweiligen Ordensregel verpflichtet haben.

Zum Lebensstil der Orden gehören unbedingt die Lebensgemeinschaft in einem Konvent oder Kloster, der Gehorsam gegenüber einem Oberen (je nach Tradition "Abt", "Prior", "Superior", "Guardian" oder "Minister" genannt, bei Frauenorden "Äbtissin, Priorin" oder "Oberin"), die (teilweise öffentliche) Feier des Stundengebets, das Leben nach einer Ordensregel und die enge Verbindung von Gebet und Arbeit, sowie in der Regel eine Ordenstracht.

Viele Orden sind schon im frühen bis hohen Mittelalter entstanden, wie beispielsweise die Benediktiner, die Prämonstratenser, der Deutsche Orden oder die Augustiner-Chorherren.

Regularkleriker sind ursprünglich Gemeinschaften von Priestern, die sich unter einer ordensähnlichen Regel oder Verfassung zusammengefunden haben, um einem bestimmten Charisma zu folgen. Viele von ihnen entstanden im 16. und 17. Jahrhundert. Ihre Lebensform kann in bestimmten Aspekten abweichend von jener der traditionellen Orden geregelt sein. So fehlt bei einigen Gemeinschaften die Armutsverpflichtung oder die unbedingte Pflicht zum Gemeinschaftsleben, manche (etwa die Jesuiten oder die Marianen) kennen zusätzliche Gelübde.

Kongregationen sind in der Regel jüngeren Datums. Viele von ihnen entstanden im 18. und 19. Jahrhundert. Sie haben sich prinzipiell einer ursprünglichen Ordensregel angeschlossen, jedoch eine eigene Ausprägung mit eigenen Satzungen (meist "Konstitutionen" genannt) entwickelt. Hierunter fallen Gemeinschaften wie die Borromäerinnen, die Spiritaner, Gemeinschaften der Regulierten Dritten Orden der Franziskaner oder Dominikaner und sehr viele andere Frauen- und Männergemeinschaften. Die Unterscheidung zwischen Orden und Kongregationen besaß im früheren Kirchenrecht große Bedeutung, spielt aber im heutigen Kodex nur eine untergeordnete Rolle. Allerdings spiegeln sich die traditionellen Unterschiede oft im Eigenrecht der betreffenden Gemeinschaften wider. Ihre Mitglieder legen keine feierlichen, sondern so genannte einfache Gelübde ab, was jedoch über die kirchenrechtliche Bezeichnung hinaus kaum praktische Bedeutung besitzt.

Die Gesellschaften apostolischen Lebens unterscheiden sich in ihrer Lebensweise kaum von einer Kongregation. Sie legen jedoch keine Gelübde ab, sondern ein Versprechen, was den Gelübden inhaltlich gleichkommt, kirchenrechtlich aber nicht die gleiche Bindung bewirkt. Die Mitglieder dieser Gemeinschaften legen nach einigen Jahren die endgültigen zeitlichen Versprechen ab. Typische Gesellschaften des Apostolischen Lebens sind die Vinzentinerinnen und die Pallottiner.

Die Mitglieder einer Ordensgemeinschaft leben in Gemeinschaft, entweder in mehr oder weniger strenger Klausur, das heißt, abgeschieden von der Welt und im beständigen Wechsel von Gebet und Arbeit in der Stille. Man spricht dann von kontemplativen Gemeinschaften. Oder sie üben ein Apostolat aus, das bedeutet, sie sind im Sinne praktizierter Nächstenliebe in den unterschiedlichsten Berufen aktiv in der Welt und in der Kirche tätig, was sowohl unmittelbar kirchliche Aufgabenbereiche (etwa Verkündigung oder Mission) als auch allgemeine soziale oder gesellschaftliche Aufgabenfelder umfassen kann (etwa Krankenpflege, Erziehung oder Wissenschaft). Diese Gemeinschaften nennt man apostolische Ordensgemeinschaften. Ein Beispiel für eine apostolische Ordensgemeinschaft sind die Salvatorianer. Es gibt auch gemischt kontemplativ-apostolische Ordensgemeinschaften.

Streng klausurierte Orden sind zum Beispiel die Trappisten, die Kartäuser, die Klarissen und Karmelitinnen. Die Bezeichnung "Nonne" (weibliche Form von griechisch und lateinisch "nonnus", "Mönch") umfasst kirchenrechtlich nur die in der so genannten "päpstlicher Klausur" lebenden Schwestern monastischer Orden; Mitglieder, die nicht klausuriert lebenden und nicht-monastischen Gemeinschaften angehören, heißen dagegen allgemein "Ordensschwestern".

Ordensgemeinschaften haben ihre eigenen Bestimmungen für die Mitglieder, die sich nach langer Erfahrung mit dem Leben in Gemeinschaft zum Eremitentum berufen fühlen, sodass sie − mit Genehmigung − diesen Schritt unternehmen können, ohne ihre Zugehörigkeit zur Ordensgemeinschaft aufgeben zu müssen. Nonnen und Ordensschwestern, in deren Orden dies nach altem Brauch üblich ist, können bei oder kurz nach der feierlichen Profess auch die Jungfrauenweihe empfangen. Für beide Fälle sind Can. 603 beziehungsweise Can. 604 des CIC nicht anwendbar, da diese kirchenrechtlichen Regelungen lediglich solche Eremiten und geweihte Jungfrauen betreffen, die in das geweihte Leben eintreten, ohne Mitglieder einer Ordensgemeinschaft zu sein.

Viele Institute ermöglichen es ihnen verbundenen Gläubigen, die ihr weltliches Leben weiterführen möchten, einer mit der Ordensgemeinschaft verbundenen Laienorganisation beizutreten oder sich der Gemeinschaft als Oblaten (von lat. oblatum, dargebracht) oder Drittordensmitglieder anzuschließen. Auf diese Weise können Männer und Frauen an der Spiritualität einer Ordensgemeinschaft teilhaben, deren geistliche Impulse in die Welt hinaustragen und an der Erfüllung ihrer Aufgaben mitwirken, ohne als Vollmitglieder in den Verband einzutreten. Oblaten, die nach einer Probezeit ihre Profess ablegen können, sind besonders bei den Orden der benediktinischen Tradition verbreitet. Andere Orden – etwa die Franziskaner, Karmeliten und Dominikaner – verfügen traditionell über einen eigenen weltlichen Zweig, einen so genannten Dritten Orden (der neben dem männlichen und weiblichen Zweig des Hauptordens besteht). In apostolischen Ordensgemeinschaften neueren Ursprungs nehmen diese Stellung oft so genannte „Mitarbeiter“ ("Cooperatores") ein, die rechtlich meist in Form eines kirchlichen Vereins ("Öffentliche Vereinigung von Gläubigen") verfasst sind. So gibt es zum Beispiel innerhalb der Don-Bosco-Familie die Salesianischen Mitarbeiter Don Boscos.

Zu den weiteren Formen des gemeinschaftlichen religiösen Lebens in der römisch-katholischen Kirche gehören die Säkularinstitute. In Säkularinstituten lebt jedes Mitglied allein und unauffällig in der Gesellschaft. Entsprechend diesem Grundsatz tragen die Mitglieder der Säkularinstitute auch keine äußeren Erkennungszeichen. Es handelt sich hierbei um eine Form der "Vita consecrata", die nach dem zweiten Vatikanum entstand.

Traditionell bilden Ordensleute zusammen mit anderen Religiosen wie Eremiten und gottgeweihten Jungfrauen einen eigenen geistlichen Stand, der weder klerikalen noch laikalen Charakter besitzt. Kirchenrechtlich gesehen sind sie in der lateinischen Kirche heute aber je nachdem, ob sie das Weihesakrament empfangen haben oder nicht, entweder den Klerikern oder den Laien zuzurechnen.

Die Reformatoren waren dem Ordensleben gegenüber überwiegend ablehnend eingestellt, so dass es durch die Reformation zum Erliegen des Ordenslebens in den evangelischen Konfessionen kam.

In den evangelischen Kirchen gibt es heute nur sehr wenige ordensähnliche Gemeinschaften: Eine Besonderheit sind die Orden vom Hl. Johannes (Johanniter), die evangelischen Zweige der heutigen katholischen Malteser, die noch immer in der Tradition des vorreformatorischen Jerusalemer Ordens stehen. Nach der Reformation haben verschiedene evangelische Stifte die Tradition ihrer Klöster und Konvente in erneuerter Form fortgeführt. Ordensgemeinschaften im eigentlichen Sinne waren sie aber nicht. Hier sind beispielsweise in Deutschland die Lüneklöster (Lüne, Wennigsen u. a.) zu nennen, die von der Klosterkammer Hannover verwaltet werden. Bis heute leben im 1529 reformierten Kloster Ebstorf evangelische Frauen unter der Leitung einer evangelischen Äbtissin. Eine Sonderstellung nimmt das Kloster Loccum ein, das 1585 evangelisch wurde und seitdem keinen residierenden Konvent, aber nach wie vor einen Abt und Konventualen hat.

Die Diakonissenhäuser ermöglichen Frauen einen Zusammenhalt und eine religiöse Lebensgemeinschaft, wie sie auch aus katholischen Ordensgemeinschaften mit stark karitativer und diakonischer Ausrichtung bekannt ist. Solche Gemeinschaften entstanden vornehmlich im 19. Jahrhundert.

Neugründungen, zumeist im 20. Jahrhundert, wie die Communität Casteller Ring und die Communität Christusbruderschaft Selbitz, führen die christliche Tradition des Ordenslebens heute auch in der evangelischen Kirche weiter. Andere Gemeinschaften wie etwa die Michaelsbruderschaft haben sich zwar ordensähnliche Regeln gegeben, leben aber im Alltag nicht zusammen.

All diese Entwicklungen sind zumeist in den lutherisch geprägten Kirchen aufgekommen. Die reformierte Kirche kennt hingegen keine Ordensgemeinschaften und lehnt diese Lebensform weiterhin insgesamt ab. Auch pietistische und freikirchliche Gemeinschaften wie die Herrnhuter Brüdergemeine stehen ihrem Selbstbild zufolge grundsätzlich nicht in der Tradition der Ordensgemeinschaften.

In der anglikanischen Kirche gibt es inzwischen wieder zahlreiche Orden und Ordensgemeinschaften. Viele sind von den Franziskanern oder den Benediktinern inspiriert.

Das Leben der Religiosen spielt sich in den orthodoxen Kirchen in größtenteils selbstständigen Mönchs- und Nonnenklöstern ab. Grundlage des orthodoxen Mönchtums bildet in den meisten Fällen die Ordensregel des Basilius von Caesarea oder die des Theodor Studites. Für Theologie und Spiritualität des orthodoxen Christentums sind die Klöster von überragender Bedeutung. Da die orthodoxe Kirche den Pflichtzölibat nur für Bischöfe, nicht aber für Priester kennt, kommen für das Bischofsamt praktisch ausschließlich Mönche (in seltenen Fällen auch unverheiratete Weltpriester) in Frage, so dass deren Gewicht innerhalb der Kirche auch institutionell sehr hoch ist.

Eine ökumenische Gemeinschaft stellt die Communauté de Taizé dar, deren Gründer Roger Schutz selbst evangelisch war, jedoch Mitglieder unabhängig von ihrer Konfession aufnahm. In der Liturgie und Spiritualität der Gemeinschaft sind katholische, evangelische und orthodoxe Elemente enthalten und weiterentwickelt worden.

In Werningshausen gibt es seit 1973 ein ökumenisches Benediktinerkloster Priorat Sankt Wigberti.

Orden in einem dem christlichen Ordensleben vergleichbaren Sinn kennt der Islam nicht, es gibt jedoch in manchen islamischen Strömungen Bruderschaften, die Formen eines religiös motivierten Gemeinschaftslebens praktizieren oder Gebetsriten und Gottesdienste in geregelter Form gemeinschaftlich zelebrieren. Seit dem 12. Jahrhundert entstanden besonders innerhalb der islamischen Mystik (Sufismus) sehr viele derartige Bruderschaften, so genannte Tariqas (siehe auch: Derwisch, Liste der Sufi-Orden).

Ordensähnliche Gemeinschaften gibt es im Hinduismus seit dem 8. Jahrhundert unserer Zeitrechnung. Der älteste bekannte Orden wurde von dem Philosophen Shankara gegründet.







</doc>
<doc id="3803" url="https://de.wikipedia.org/wiki?curid=3803" title="Oberrheinische Tiefebene">
Oberrheinische Tiefebene

Die Oberrheinische Tiefebene, vor allem naturräumlich auch "Oberrheinisches Tiefland" genannt, ist ein etwa 300 km langes und bis zu 40 km breites Tiefland am oberen Mittellauf des Rheins, das sich zwischen den Städten Basel (Schweiz) im Süden und Frankfurt am Main (Deutschland) im Norden erstreckt.

Die Ebene entstand durch einen tief in die Erdkruste reichenden und mit Sedimenten verfüllten Grabenbruch, der als Oberrheingraben bezeichnet wird.

Die Oberrheinische Tiefebene wird vom Rhein – und zwar von seinem etwa 350 km langen Abschnitt Oberrhein, nach dem sie benannt ist – durchflossen. Der südlichste Teil der Ebene liegt in der Nordwestschweiz um die Stadt Basel, der südwestliche Abschnitt in den nordostfranzösischen Départements Haut-Rhin und Bas-Rhin (Elsass), der nordwestliche Teil und das Gebiet östlich des Rheins gehören zu Deutschland. Die Ebene ist der morphologische Ausdruck des Oberrheingrabens, einer der größten geologischen Strukturen in Mitteleuropa (siehe Abschnitt Grabenbruch).

Naturräumlich umfasst das sogenannte "Oberrheinische Tiefland" auch das Rhein-Main-Tiefland, das nach Nordosten dem Unterlauf des Mains und der Wetter (nordwärts durch die Wetterau) flussaufwärts folgt.

Die deutschen Anteile des Oberrheinischen Tieflands gliedern sich wie folgt:

Alle größeren Fließgewässer in der Oberrheinischen Tiefebene besitzen ihre Quellen in den umgebenden oder in weiter entfernten Mittelgebirgsregionen und münden sämtlich in den Rhein. Die mehr als 200 km langen Nebenflüsse sind rechtsrheinisch Neckar und Main, linksrheinisch Ill und Nahe. Natürliche Seen gibt es nicht; die heutigen Stillgewässer sind sogenannte Baggerseen und resultieren aus industriellem Sand- und Kiesabbau. Polder, die entlang des Oberrheins zum Hochwasserschutz angelegt wurden, können bei Bedarf geflutet werden.

Der Oberrheingraben und seine Randzonen zu den Gebirgen hin gelten als die wärmste Region Deutschlands. Sie hat die mildesten Winter und die wärmsten Sommer bei geringen bis mäßigen Niederschlägen. Die Jahresdurchschnittstemperaturen erreichen teilweise um 11 °C; im wärmsten Monat Juli liegen die Durchschnittswerte um oder sogar knapp über 20 °C, was in Deutschland mit Ausnahme klimatisch begünstigter Ballungsräume (Rhein-Main-Zentren, Berlin-Mitte) nirgendwo erreicht wird. Ursache dafür sind häufige Südwest-Wetterlagen mit Luftmassen aus dem westlichen Mittelmeerraum; Föhn-Effekte durch absinkende Luft an der westlichen Grabenbruchkante können zusätzliche Temperaturerhöhungen bewirken. Die Niederschlagsmengen nehmen nach Osten hin zu, weil es an der östlichen Bruchkante zu Steigungsregen kommt.


Der Oberrheingraben ist eines der zentralen Segmente einer Grabenbruchzone, die sich von der Nordsee bis in das westliche Mittelmeer erstreckt (Mittelmeer-Mjösen-Zone). Die früher vertretene These, dass für die Entstehung eine subkrustale Wärmequelle (Plume) verantwortlich sei ("Aktives Rifting"), ist nach neueren Befunden aus der Geophysik und Geodynamik nicht haltbar. Ursache für die Entstehung der Grabenzone waren vielmehr Zugspannungen in Erdkruste und Erdmantel, die zum sogenannten "Passiven Rifting" führten, einer Dehnung der Erdkruste, die auch ihre Ausdünnung zur Folge hatte. Deswegen senkte sich die Erdoberfläche in der Grabenzone ab. Dagegen wölbte sich die Kruste-Mantel-Grenze (Moho) unter dem Graben auf.

Im Bereich des Oberrheingrabens wurden zeitgleich die Gebiete westlich und östlich zu den Grabenschultern von Vogesen/Pfälzerwald bzw. Schwarzwald/Odenwald emporgehoben. Ein Teil des entstandenen Reliefs wurde durch Sedimentation, die in den abgesunkenen Graben hinein erfolgte, sowie Erosion der gehobenen Schultern ausgeglichen.

Die Entstehung des Oberrheingrabens begann vor über 50 Millionen Jahren. Sie verlief im Wesentlichen in zwei Phasen:

In Phase I vor 50 bis 20 Millionen Jahren herrschte in Mitteleuropa ein Dehnungsregime. Die Dehnung wurde im Oberrheingrabengebiet an bereits vorhandenen Verwerfungen lokalisiert. Es kam über die gesamte Länge des Grabens zwischen Frankfurt und Basel zu einer Absenkung der Erdoberfläche und Ablagerung von Sedimenten. Die randlichen Gebiete hoben sich zu Grabenschultern heraus.

Mit dem Übergang in Phase II wurde die Dehnung durch ein Blattverschiebungsregime abgelöst. Die Gebiete westlich des Oberrheingrabens (Elsass, Pfalz, Rheinhessen) verschoben sich relativ zu den rechtsrheinischen Gebieten nach Südwesten. Die weitere Absenkung im Graben beschränkte sich auf das Grabensegment nördlich der Stadt Karlsruhe. Dagegen unterlagen die anderen Grabenabschnitte samt den randlichen Schultern der Hebung und Erosion. Das Blattverschiebungsregime ist heute weiterhin aktiv. Allerdings hat sich in jüngerer geologischer Vergangenheit die Größe und Ausrichtung der Spannungen in der Erde geringfügig geändert, so dass wieder Sedimentation im gesamten Grabenbereich stattfindet.

Der Oberrheingraben ist ein Gebiet erhöhter Seismizität. Die Erdbeben sind im Allgemeinen von geringer Stärke und Intensität (gemäß der MSK-Skala). Es kommt durchschnittlich alle paar Monate zu einem Erdbeben der Stärke 3, das von Menschen in der unmittelbaren Umgebung des Epizentrums gespürt werden kann. Ungefähr alle zehn Jahre sind überregional wahrnehmbare seismische Erschütterungen mit Stärken größer als 5 und leichten Schäden zu erwarten.

Eine Ausnahme stellt die Region um Basel und den angrenzenden Schweizer Jura dar. Dort traten in Mittelalter und Neuzeit Beben auf, die – wie etwa das Basler Erdbeben von 1356 – beträchtliche Zerstörungen bewirkten. Es wird vermutet, dass diese Erdbeben mit der fortdauernden Überschiebung des Schweizer Juras auf den südlichen Oberrheingraben in Verbindung stehen.

Erdbeben werden in weiten Bereichen des Oberrheingrabens bis in Tiefen von etwa 15 km hinunter ausgelöst. In noch größeren Tiefen verformen sich die Gesteine aufgrund der hohen Temperaturen durch raumgreifendes Kriechen. Ein Versatz von Gesteinsschichten entlang von Verwerfungen, der eine Voraussetzung für das Auftreten von Erdbeben wäre, findet im Oberrheingrabengebiet nicht mehr statt.

In Südwestdeutschland mit dem Oberrheingrabengebiet sind Überreste einstiger Vulkane weit verbreitet (z. B. Kaiserstuhl, Hegau, Schwäbischer Vulkan, Steinsberg, Katzenbuckel, Pechsteinkopf). Die meisten Vulkanite sind um die 40 Millionen Jahre alt, ein zweiter vulkanischer Höhepunkt war vor 18 bis 14 Millionen Jahren. Die Magmen stammen fast ausschließlich aus einem bis zu 2 % aufgeschmolzenen Teilbereich des Erdmantels (Asthenosphäre). Er befindet sich unter Südwestdeutschland in Tiefen von über 70 km. Die Magmen stiegen aus diesen Tiefen nahezu unverändert bis an die Erdoberfläche auf und erstarrten vorwiegend als Nephelinite und Melilithite. Nur lokal entwickelten sich beim Aufstieg andere Magmenzusammensetzungen (z. B. am Kaiserstuhl).

Eine Grabenbildung kann durch die Ausdünnung der Erdkruste zur Entstehung thermischer Anomalien im Erdmantel führen. Die Anomalien rufen die Produktion magmatischer Schmelzen und Vulkanismus an der Erdoberfläche hervor. Im Oberrheingrabengebiet entstand jedoch keine solche thermische Anomalie, weil der Erdmantel wegen der langsam erfolgten Dehnung bei seinem Aufstieg abkühlte. Es wird eher ein Zusammenhang zwischen dem Vulkanismus und der Entstehung der Alpen vermutet, weil bedeutende geologische Ereignisse im Alpenraum zeitlich mit den Höhepunkten vulkanischer Aktivität in Südwestdeutschland zusammenfielen.

Die Oberrheinische Tiefebene ist Teil der sogenannten „Blauen Europa-Banane“, einer europäischen Wirtschafts- und Entwicklungszone, die von der Irischen See bis zum Mittelmeer reicht. In der dicht besiedelten Rheinebene zählen dazu folgende bedeutende Wirtschaftsregionen: die Trinationale Metropolregion Oberrhein mit den Städten Straßburg, Mülhausen und Colmar (F), Karlsruhe und Freiburg (D) sowie Basel (CH), in Deutschland die Metropolregion Rhein-Neckar mit Mannheim, Ludwigshafen und Heidelberg sowie das Rhein-Main-Gebiet mit Frankfurt am Main, Offenbach, Darmstadt, Mainz und Wiesbaden.

Der Oberrheingraben ist mit sehr jungen Sedimenten bedeckt. Der Sand und der Kies, die den Grundwasserleiter aufbauen, stammen aus dem Eiszeitalter des Pleistozäns sowie aus der Jetztzeit, dem Holozän. Im Raum zwischen Basel und Frankfurt deckt das örtliche Grundwasser mehr als drei Viertel des Trinkwasserbedarfs der Bevölkerung (im Elsass, in Baden-Württemberg, Rheinland-Pfalz und Hessen) sowie mehr als die Hälfte des von der lokalen Industrie benötigten Wassers. Auch die Bewässerung der zahlreichen landwirtschaftlich genutzten Flächen erfolgt fast vollständig aus dem Grundwasser mit Ausnahme großer Flächen in der Vorderpfalz, die mit Rheinwasser beregnet werden. Wie in einen Trichter fließt der Niederschlag aus den Hochebenen und den Randgebirgen unterirdisch und über zahlreiche Bäche und kleine Flüsse in die Rheinebene und speist ein riesiges Grundwasserreservoir.

Der Grundwasserleiter ist mehrschichtig aus verschiedenen Sand- und Kieslagen unterschiedlicher Korngrößen mit dazwischen liegenden, oft meterdicken stauenden Ton- und Schluffschichten, aufgebaut. Seine Basis (Sohle) liegt im Raum Karlsruhe zwischen 70 m und 260 m Teufe, erreicht im Raum Mannheim/Heidelberg (Heidelberger Loch) eine Teufe von ca. 500 m und steigt nach Norden wieder an. Trinkwasserbrunnen werden bis zu Teufen von 300 m und sogar bis 400 m Teufe ausgebaut. Das Problem dabei ist die zunehmende Temperatur des Wassers (geothermische Tiefenstufe), die im Oberrheingraben höher ist (tektonische Schwachzone). Das Grundwasser aus den tieferen Wechsellagen (> 100/150 m) ist gut geschützt, von Umweltbelastungen nahezu frei, von hervorragender Qualität (ausgenommen regionale geogene Einflüsse z. B Arsen, Methan, Brackwasser) und hat je nach Entnahmeteufe letztmals vor 5.000 bis 20.000 Jahren am natürlichen Kreislauf teilgenommen.

Der Oberrhein-Aquifer ist mit einer geschätzten Größe von 45 Milliarden m einer der größten Grundwasserleiter Mitteleuropas. Sein Wasserspiegel ist meist bereits wenige Meter unter der Erdoberfläche zu finden, in Flussauen, Auftriebsquellen und Seen auch oberirdisch. Der so genannte Flurabstand ist dabei sehr unterschiedlich und im Süden des Gebietes größer.

Im Hessischen Ried trägt der Wasserbeschaffungsverband Riedgruppe Ost in Zusammenarbeit mit Hessenwasser wesentlich zur Trinkwasserversorgung des Rhein-Main-Gebietes bei. Zugleich entnimmt der Wasserverband Hessisches Ried im Wasserwerk von Biebesheim dem Rhein zum Zweck der Grundwasseranreicherung bis zu 5400 m Wasser pro Stunde. Dies entspricht an diesem Stromabschnitt einem Tausendstel der mittleren Abflussmenge.

Vor allem in tieferen Schichten, teilweise aber auch oberflächennah, fand und findet sich Erdöl. Die Vorkommen in Merkwiller-Pechelbronn (Elsass) sind seit 1498 belegt und gehören weltweit zu den ersten, die ausgebeutet wurden. Der Name "Pechelbronn" bedeutet „Pechbrunnen“ und weist auf das aus der Erde hervortretende Öl hin. Nach dieser ältesten Erdölquelle werden die Vorkommen im Oberrheingraben als Pechelbronner Schichten bezeichnet. Zwischen 1952 und 1994 wurde im Hessischen Ried Erdöl gefördert, bis 1979 auch Erdgas. Noch heute wird in der Nähe von Landau in der Pfalz in geringen Mengen Öl gefördert, seit 2008 auch bei Speyer.

Angesichts steigender Rohstoffpreise und zu Ende gehender Erdölressourcen wird auch in der Rheinebene wieder nach Erdöl gesucht, so seit 2011 durch die Unternehmen Geopetrol und Millennium Geoventure bei Soufflenheim im Elsass. Ab Dezember 2011 betrieb das polnische Unternehmen Geofizyka Toruń im Auftrag von Rhein Petroleum aus Heidelberg im Rahmen des Projekts „Erdölsuche am nördlichen Oberrhein“ dreidimensionale seismische Vermessungen, um Erdölreserven nachzuweisen.

In neuerer Zeit begann die Nutzung von Erdwärme: Im Bereich des Oberrhein-Aquifers sind oder waren mehrere Pilotprojekte in Niederenthalpie-Lagerstätten im so genannten Hot-Dry-Rock-Verfahren (HDR) in der Erprobung. So ging z. Bsp. in Soultz-sous-Forêts im Elsass (Frankreich) das Geothermiekraftwerk 2008 ans Netz; ein weiteres wird in Rittershofen nördlich von Straßburg gebaut, außerdem eines in Landau (Rheinland-Pfalz).

An der Vorbergzone zum Schwarzwald ergaben sich die Hebungsrisse in Staufen im Breisgau. Ein Bohrprojekt in Kleinhüningen bei Basel (Deep Heat Mining Basel) wurde nach dabei erzeugten Erdbeben gestoppt.

Entlang des Rheins wurde und wird in größeren Mengen Kies und Sand abgebaut, um als Baustoff Verwendung zu finden. Aus den Rheinkiesen wird auch etwas Gold gewonnen. Ton, früher für die Herstellung von keramischem Geschirr gebraucht, wird in mittlerweile sehr begrenztem Umfang für die Ziegelherstellung abgebaut.

Der Oberrheingraben verfügt dank des gemäßigten Klimas mit einer hohen Sonnenscheindauer und des mehr als ausreichenden Wasserdargebots durch den Oberrhein-Aquifer über sehr gute Voraussetzungen für die Erzeugung von Nahrungs- und Genussmitteln. Die fruchtbaren Böden haben seit früher Zeit Ackerbau möglich gemacht; das Gebiet wird bis zur Hälfte seiner Fläche landwirtschaftlich genutzt.

An Sonderkulturen werden vor allem Wein, Spargel, Zwetschgen, Süß- und Sauerkirschen, Erdbeeren, verschiedene Gemüse, Hopfen sowie Tabak angebaut. Mit Rheinhessen, der Pfalz und Baden liegen die drei in dieser Reihenfolge flächenmäßig größten deutschen Weinanbaugebiete nahezu vollständig im Oberrheingraben. Die günstigen klimatischen Bedingungen lassen neben Weinreben auch Mandelbäume, Feigen sowie Esskastanien im Freiland gedeihen und Früchte tragen. Der Austrieb der Spargelstangen wird zunehmend durch Abdeckung der Felder mit Kunststofffolie, welche die Bodenerwärmung fördert, zeitlich nach vorne verlagert.




</doc>
<doc id="3805" url="https://de.wikipedia.org/wiki?curid=3805" title="Oberflächenspannung">
Oberflächenspannung

Die Oberflächenspannung ist die infolge von Molekularkräften auftretende Erscheinung bei Flüssigkeiten, ihre Oberfläche klein zu halten. Die Oberfläche einer Flüssigkeit verhält sich ähnlich einer gespannten, elastischen Folie. Dieser Effekt ist zum Beispiel die Ursache dafür, dass Wasser Tropfen bildet, und trägt dazu bei, dass einige Insekten über das Wasser laufen können oder eine Rasierklinge auf Wasser „schwimmt“.
Als Oberflächenspannung (Formelsymbol: "σ", "γ") bezeichnet man gelegentlich auch die Grenzflächenspannung. Gemessen wird sie in den SI-Einheiten "kg/s²", gleichbedeutend mit "N/m".

Die Oberflächenspannung ist eine ziehende Kraft, die an der Oberfläche einer Flüssigkeit lokalisiert ist und ihre Wirkungsrichtung ist parallel zur Flüssigkeitsoberfläche. Demnach steht eine Flüssigkeitsoberfläche stets unter Spannung. Eine Flüssigkeitsoberfläche kann somit mit einer leicht gespannten dünnen Folie verglichen werden, bloß dass die Spannung nicht von der Dehnung abhängt.

Die Oberflächenspannung verleiht einer Flüssigkeitsoberfläche spezielle Eigenschaften. So können nichtbenetzte Objekte auf einer Wasseroberfläche schwimmen, solange ihr Gewicht nicht ausreicht, um die Oberflächenspannung zu überwinden. Anschaulich wird dies, wenn man beispielsweise eine Büroklammer – aus fettigem Eisendraht – auf eine Wasseroberfläche legt. Sie wird nicht oder nur teilweise benetzt, sinkt etwas unter den Wasserspiegel, nimmt dabei aber die Oberfläche mit, dellt sie ein. Die Oberflächenspannung greift mit vertikalen Kraftkomponenten an der Büroklammer an und trägt diese. Dieser Effekt wird auch von Lebewesen wie dem Wasserläufer ausgenutzt, um auf einer Wasseroberfläche laufen zu können.

Die Oberflächenspannung ist die Ursache dafür, dass Flüssigkeiten kugelförmige Gestalt annehmen, wenn keine anderen Kräfte auf sie wirken. Ein extremes Beispiel dafür sind Flüssigkeitstropfen im Weltraum, die nahezu keinen äußeren Kräften ausgesetzt sind. Nach Quecksilber als Spitzenreiter unter den Reinstoffen hat Wasser eine besonders hohe Oberflächenspannung. Diese sinkt mit steigender Temperatur von Wasser deutlich und kann durch Hinzufügen schon geringer Mengen oberflächenaktiver Stoffe (Detergentien) radikal reduziert werden.

Ein praktisches Beispiel sind besonders kleine Wassertröpfchen. Zur Erklärung denke man sich eine Flüssigkeit, deren Gestalt nicht kugelförmig ist. Die Oberflächenspannung greift parallel zur Flüssigkeitsoberfläche an und gleicht lokal abweichende Krümmungen aus.

Wenn andere Kräfte auf einen Flüssigkeitstropfen wirken, so weicht dessen Gestalt von der kugelförmigen ab. Ein Beispiel dafür sind Regentropfen von mehr als 1 mm Durchmesser und Flüssigkeitstropfen auf einer Festkörperoberfläche, wo zusätzlich anziehende Kräfte zwischen Festkörper und Flüssigkeit wirken (Adhäsion). Die Gestalt des Tropfens weicht umso mehr von der kugelförmigen ab und benetzt die Festkörperoberfläche, umso höher die Adhäsion zwischen Festkörper und Flüssigkeit ist.

Ein anderes Beispiel für die Wirkung der Oberflächenspannung ist die sechseckige Form von Wabenzellen der Honigbienen. Die Zellen werden zuerst rund aus Bienenwachs gebaut. Das Material gibt aber durch die im Bienenstock herrschenden Temperaturen nach (fließt) und bildet dabei plane Grenzflächen (Minimalflächen) zwischen den einzelnen Zellen.

Es gibt zwei Definitionen der Oberflächenspannung, die konsistent sind. Einerseits die mechanische Definition, nach der die Oberflächenspannung eine Kraft pro Länge ist, und die thermodynamische, wonach die Oberflächenspannung eine Energie pro Fläche ist.
Die mechanische Definition lässt sich anhand eines Bügels mit der Breite formula_1 erklären, in dem ein Flüssigkeitsfilm eingespannt ist. Wenn der Flüssigkeitsfilm durch eine Kraft formula_2 parallel zur Oberfläche und senkrecht zu formula_1 um formula_4 auseinandergezogen wird, so wird am Film die Arbeit formula_5 verrichtet und die Oberfläche wächst um formula_6 (Faktor 2 wegen Vorder- und Rückseite des Films). Die Oberflächenspannung ist das Verhältnis formula_7. Demnach handelt es sich bei der Oberflächenspannung um eine Kraft pro Länge, die parallel zur Flüssigkeitsoberfläche gerichtet ist.

Die Richtigkeit der Vorstellung der Oberflächenspannung als Kraft parallel zur Oberfläche zeigt sich in zahlreichen Messmethoden und Effekten wie der Bügelmethode, der Kapillarität oder dem Kontaktwinkel.

Die thermodynamische Vorstellung der Oberflächenspannung als Energie pro Fläche rührt von dem Bild her, dass an der Flüssigkeitsoberfläche die Symmetrie der Flüssigkeitsmoleküle gestört ist. Das Fehlen von Flüssigkeitsmolekülen vertikal zur Flüssigkeitsoberfläche und die somit „fehlende“ Bindungsenergie muss durch eine positive Energie formula_8 kompensiert werden. Um die Oberfläche einer Flüssigkeit zu vergrößern benötigt man Energie, wobei die Oberflächenspannung formula_9 definiert ist als Energie die man benötigt um die Flüssigkeitsoberfläche um eine Einheitsfläche zu vergrößern. Somit folgt formula_10, womit die Analogie der Vorstellung „fehlender Bindungsenergie“ zur mechanischen Definition gezeigt ist.

Diese anschauliche Interpretation ist jedoch noch nicht ausreichend um die Oberflächenspannung thermodynamisch zu definieren. Um dies zu tun geht man von der Änderung der freien Enthalpie formula_11 bei konstanter Temperatur formula_12 und konstantem Druck formula_13 aus, welche durch Gleichung (1) beschrieben wird, wobei formula_14 die Enthalpie, formula_12 die Temperatur und formula_16 die Entropie kennzeichnet.

Man kann diese Gleichung umschreiben indem man die Definition der Enthalpie formula_18 einsetzt und berücksichtigt, dass formula_19 gilt.

Für die Änderung der inneren Energie wird formula_21 eingesetzt, wobei formula_22 für die verrichtete Arbeit steht. Für die Wärmemenge gilt formula_23. Daraus folgt:

Der Ausdruck für die Arbeit kann in einen Term für die Volumenarbeit formula_25 und nicht expansive Arbeit formula_26 zerlegt werden.

Bei konstanter Temperatur und konstantem Druck entspricht die Änderung der freien Enthalpie der nicht expansiven Arbeit. Dieser Ausdruck kann nun in Verbindung mit der Oberflächenspannung formula_9 gebracht werden. Sofern nur Arbeit aufgewendet wird um die Oberfläche einer Flüssigkeit zu vergrößern so entspricht diese dem Ausdruck formula_29. Da nun die Oberflächenspannung als Arbeit pro Einheitsfläche definiert ist muss noch die Oberfläche formula_30 der Flüssigkeit berücksichtigt werden. Somit folgt:

Die Oberflächenspannung ist somit thermodynamisch als partielle Ableitung der freien Enthalpie nach der Oberfläche bei konstanter Temperatur und konstantem Druck definiert.

Die Vorstellung der fehlenden Flüssigkeitsmoleküle an der Oberfläche verleitet intuitiv zu der Annahme, dass die Oberflächenspannung eine Kraft vertikal zur Flüssigkeitsoberfläche sei. Dies stimmt jedoch nicht mit der mechanischen Definition der Oberflächenspannung überein. Um die mechanische Definition hierbei in Einklang mit der thermodynamischen zu bringen muss man berücksichtigen, dass innerhalb einer Flüssigkeit sowohl anziehende als auch abstoßende Kräfte auf ein Molekül wirken. Während in einem Festkörper lokal entweder anziehende oder abstoßende Kräfte wirken, da sich die Teilchen an fixierten Plätzen befinden, so sind in einer Flüssigkeit die Moleküle beweglich. Die Abstände zwischen den Flüssigkeitsmolekülen können sich verändern und somit können auf ein Flüssigkeitsteilchen abstoßende und auch anziehende Kräfte wirken. Dieser Sachverhalt kann auch in einem Lennard-Jones-Potential veranschaulicht werden. Dieses beschreibt allgemein das Potential zwischen zwei ungeladenen Teilchen in Abhängigkeit von deren Distanz. Geraten die Teilchen bei kurzen Distanzen in Kontakt, so stoßen sie sich ab, während sie sich bei größeren Distanzen anziehen. Während in einem Festkörper der Abstand zwischen zwei Teilchen fixiert ist, kann sich dieser in einer Flüssigkeit aufgrund der thermischen Bewegung ändern, was anziehende und auch abstoßende Kräfte auf ein Flüssigkeitsmolekül ermöglicht. Im Bild rechts ist eine schematische Darstellung eines Lennard-Jones-Potentials abgebildet, das die Kräfte zwischen Flüssigkeitsmolekülen erklärt. Haben die Flüssigkeitsmoleküle Kontakt, so stoßen sie sich ab (oranger Bereich), während sie sich bei großen Distanzen anziehen (blauer Bereich). In einer Flüssigkeit ändern sich die Abstände zwischen den Teilchen ständig aufgrund der Wärmebewegung, was durch den schwarzen Doppelpfeil in der Abbildung dargestellt ist. Somit können auf ein Flüssigkeitsmolekül sowohl anziehende als auch abstoßende Kräfte wirken.

Man kann die abstoßenden Kräfte als Kontaktkräfte interpretieren. Aufgrund dessen kann deren Wirkung im Raum als richtungsunabhängig, also isotrop angesehen werden. Die anziehenden Kräfte innerhalb einer Flüssigkeit wirken bei weiteren Entfernungen, sind bedingt durch die Struktur der Moleküle und können als richtungsabhängig im Raum, also anisotrop angesehen werden.
An der Phasengrenzfläche zwischen Flüssigkeit und Gasphase ändert sich die Dichte der Flüssigkeit sprunghaft im Bereich weniger Moleküllängen, bis sie konstant auf dem Wert des Flüssigkeitsinneren bleibt. Dies bewirkt, dass auch die abstoßenden Kräfte in der Flüssigkeit sprunghaft an der Oberfläche größer werden bis sie den konstanten Wert des Flüssigkeitsinneren erreichen, wobei dieser Anstieg in alle Raumrichtungen gleich groß ist aufgrund der isotropen Natur der abstoßenden Kräfte.
Zur weiteren Erklärung dient das Bild rechts, in dem die Kräfte auf ein Flüssigkeitsmolekül an der Oberfläche und im inneren veranschaulicht sind. An der Flüssigkeitsoberfläche ist die Symmetrie gestört, das heißt, die Moleküle dort haben in vertikaler Richtung keine benachbarten Moleküle. Somit wirken in vertikaler Richtung nur von unten abstoßende Kräfte (grauer Pfeil) auf die Moleküle. Um das Kräftegleichgewicht zu wahren, werden die abstoßenden Kräfte in vertikaler Richtung durch anziehende Kräfte (oranger Pfeil) ausgeglichen. In horizontaler Richtung, also parallel zur Oberfläche ist dies nicht notwendig, da die Symmetrie nicht gestört ist. Das heißt, dass in horizontaler Richtung von allen Seiten abstoßende Kräfte auf die Flüssigkeitsmoleküle an der Oberfläche wirken. Zusätzlich zu den abstoßenden Kräften wirken auch anziehende Kräfte in horizontaler Richtung. Diese sind jedoch nicht notwendig um das Kräftegleichgewicht zu wahren und können daher und aufgrund ihrer anisotropen Natur in ihrem Betrag größer sein als die abstoßenden Kräfte. Das bedeutet, dass an der Flüssigkeitsoberfläche in horizontaler Richtung die anziehenden Kräfte auf die Flüssigkeitsmoleküle größer sind als die abstoßenden Kräfte. Im Flüssigkeitsinneren sind die anziehenden und abstoßenden Kräfte auf ein Molekül gleich groß.

Um die Oberflächenspannung nun als Kraft parallel zur Oberfläche weiter zu verstehen, ist es anschaulich, die Flüssigkeit in zwei Hälften zu teilen, wie es im Bild rechts abgebildet ist. Dort sieht man eine gepunktete und eine nicht gepunktete Hälfte, wobei diese lediglich zur Markierung der beiden Teile dienen. Man betrachtet die Kräfte, die von dem nicht gepunkteten Teil auf den gepunkteten Teil der Flüssigkeit ausgeübt werden.
a.) Erst legt man die Trennlinie zwischen den Flüssigkeitshälften parallel zur Flüssigkeitsoberfläche. In Richtung des Flüssigkeitsinneren nimmt die Dichte zu, daher werden auch die abstoßenden Kräfte (grau) auf den gepunkteten Teil größer. Diese werden durch anziehende Kräfte (orange) ausgeglichen.
b.) Legt man nun die Trennlinie zwischen den Hälften in vertikaler Richtung, so kann man wiederum die abstoßenden Kräfte, die auf den gepunkteten Teil wirken, einzeichnen. Diese sind aufgrund ihrer isotropen Natur in ihrem Betrag gleich groß wie in vertikaler Richtung. Die anziehenden Kräfte auf den gepunkteten Teil sind jedoch nicht isotroper Natur und können in ihrem Betrag größer sein als die abstoßenden Kräfte. Man erkennt auch, dass sich dieser Unterschied verkleinert je weiter man ins Flüssigkeitsinnere fortschreitet. Bereits nach ein paar Moleküllängen gleichen sich anziehende und abstoßende Kräfte in horizontaler Richtung aus, da die Dichte in Richtung des Flüssigkeitsinneren zunimmt.
c.) Der nicht gepunktete Teil der Flüssigkeit übt eine anziehende Kraft auf den gepunkteten Teil aus, die in Richtung des Flüssigkeitsinneren abnimmt.

Zusammenfassend kann gesagt werden, dass sich im Bereich weniger Moleküllängen die Dichte an der Flüssigkeitsoberfläche (rote Kurve im Bild rechts) ändert, bis sie den konstanten Wert des Flüssigkeitsinneren erreicht. Dies hat zur Folge, dass an der Flüssigkeitsoberfläche eine ziehende Kraft in horizontaler Richtung wirkt. Die blaue Kurve im Bild rechts beschreibt die Differenz zwischen anziehender und abstoßender Kraft, die von dem nicht gepunkteten Teil der Flüssigkeit auf den gepunkteten Teil in horizontaler Richtung ausgeübt wird. Sie entspricht der Oberflächenspannung und ist im Bereich weniger Moleküldurchmesser an der Oberfläche lokalisiert.



Wasser hat also eine vergleichsweise hohe Oberflächenspannung (siehe auch in WikiBooks).

Man kann die Oberflächenspannung zum Beispiel mit Hilfe der Ring- (von Lecomte De Noüy), Platten- (von Wilhelmy) oder Bügel-Methode (von Lenard), mit einem Tensiometer oder durch den Kapillareffekt messen.

Auch kann man über eine optische Auswertung den liegenden oder hängenden Tropfen vermessen und so die Oberflächenspannung der Flüssigkeit ermitteln.

Bei der Bügelmethode (auch als Abreißmethode bekannt) wird ein Bügel mit einem darin eingelöteten extrem dünnen Draht (meist aus Platin) in die Flüssigkeit gehängt, sodass dieser gerade in die Flüssigkeit eintaucht und von dieser benetzt wird. Mit einer Präzisionsfederwaage wird dann die Zugkraft am Bügel nach und nach erhöht. Der Draht wird dann aus der Flüssigkeit gezogen und zieht einen Flüssigkeitsfilm mit. An einem bestimmten Punkt reißt dieser Film ab.

Durch das Ziehen am Bügel wird Arbeit gegen die Oberflächenspannung verrichtet. Aus der maximal möglichen Zugkraft am Bügel, bevor der Flüssigkeitsfilm abreißt, den Abmessungen des Bügels und der Dichte der Flüssigkeit kann dann die Oberflächenspannung berechnet werden.

Bei Flüssigkeiten wie Ethanol und Drahtlängen von 2–3 cm bei einem Radius von 0,1 mm liegt der Erwartungswert für die Masse im zwei- bis dreistelligen Milligramm-Bereich. Es sind also sehr präzise Waagen nötig. Bei einer Messunsicherheit der Waage von 5 mg und einer Vermessung des Drahtes auf 1 µm genau beträgt der größte Fehler des Endergebnisses bereits 8 bis 12 %.

Bei dieser Messmethode macht man sich den Kapillareffekt zunutze, also, dass Flüssigkeiten in dünnen Röhren nach oben steigen. Man benötigt ein Gefäß (etwa eine Küvette) und eine möglichst dünne Kapillare. Diese wird dann einfach in die Flüssigkeit gestellt und die Steighöhe wird gemessen.

Da die Flüssigkeit theoretisch unendlich lange braucht, um ihren Endstand zu erreichen, zieht man die Flüssigkeit zunächst in der Kapillare (etwa mit einer Spritze) nach oben und lässt sie anschließend wieder absinken. Die Oberflächenspannung kann dann direkt aus der Steighöhe abgelesen werden, wenn die Dichte der Flüssigkeit und der Kapillarradius bekannt sind. Da dessen Messung recht schwierig ist, nimmt man Einmalmikropipetten und misst deren Länge. Da das Volumen bekannt ist, lässt sich so der Innenradius berechnen.

Wasser erreicht in Kapillaren mit einem Radius von 0,2 mm Steighöhen von bis zu 7 cm. Für die möglichst exakte Messung der Steighöhe eignet sich beispielsweise ein Kathetometer. Ist die Dichte der Flüssigkeit genau bekannt und kann man die Steighöhe auf 0,1 mm genau ablesen, liegt der Fehler im unteren einstelligen Prozentbereich.


Der Begriff der Oberflächenspannung wurde erstmals 1629 von Niccolò Cabeo verwendet und 1751 von Johann Andreas von Segner klarer gefasst. Zur Theorie wurde 1805 von Thomas Young, 1806 von Pierre-Simon Laplace, 1830 von Siméon Denis Poisson (siehe auch Young-Laplace-Gleichung, Youngsche Gleichung) und 1842 bis 1868 von Joseph Plateau Wertvolles beigetragen.





</doc>
<doc id="3806" url="https://de.wikipedia.org/wiki?curid=3806" title="Obst">
Obst

Obst ist ein Sammelbegriff der für den Menschen roh genießbaren meist wasserhaltigen Früchte oder Teilen davon (beispielsweise Samen), die von Bäumen, Sträuchern und mehrjährigen Stauden stammen.

In Deutschland liegt der durchschnittliche tägliche Verzehr von Obst und Obsterzeugnissen (ohne Obstsäfte) laut einer Studie von 2008 bei Männern bei 230 Gramm und bei Frauen bei 278 Gramm.

Der Begriff Obst (, , frühneuhochdeutsch "obs") setzt sich aus dem Verhältniswort "ob" sowie einem mit "essen" verwandten Verbalnomen mit einer Ausgangsbedeutung ‚Zuspeise‘ (zur Grundnahrung), ursprünglich wohl Hülsenfrüchte, zusammen. Die unorganische -t-Endung tritt seit dem 16. Jahrhundert auf. Die Bezeichnung ist gemeinwestgermanisch, siehe mittelniederdeutsch "ōvet, āvet, ōves, ōvest", mittelniederländisch und sowie .

Die Unterscheidung zwischen Obst und Gemüse ist unscharf. In der Regel stammt Obst von mehrjährigen und Gemüse von einjährigen Pflanzen (laut Lebensmitteldefinition). Der Zuckergehalt beim Obst ist meist höher. Botanisch gesehen entsteht Obst aus der befruchteten Blüte, Gemüse entsteht aus anderen Pflanzenteilen. Paprika, Tomaten, Kürbisse, Zucchini, Auberginen und Gurken sind zwar Früchte und gehören laut der obigen (botanischen) Definition zu Obst (da sie aus befruchteten Blüten entstehen), werden aber als einjährige Pflanzen (Lebensmitteldefinition: Gemüse) und gemeinhin wegen der fehlenden Süße beziehungsweise Säure als Fruchtgemüse bezeichnet. Rhabarber hingegen ist ein Blattstiel, wird aber auch als Obst verwendet.

Die unten beschriebene Einteilung von Obst in Obstartengruppen (Kernobst, Steinobst, exotische Früchte usw.) ist die heute im Handel übliche. In der Botanik dagegen fasst man unter dem Sammelbegriff Obst 

Verschiedene Pflanzengruppen der Nutzpflanzen und deren Arten geben Früchte, die als Obst bezeichnet werden. Die Einteilung von Obst erfolgt in Gartenbau und Handel nicht streng botanisch. Die typischen Artengruppen sind Kernobst, Steinobst, Beerenobst, Schalenobst, klassische Südfrüchte und weitere exotische Früchte.

Daneben gilt auch eine Einteilung nach „heimischer“ und importierter Ware verschiedener Art (etwa als "Flugobst") aus Sicht der Herkunft und des Transports sowie seit einiger Zeit "aus biologischem Anbau" im Sinne einer Qualitätsangabe. Außerdem gibt es noch kaum gewerbsmäßig genutztes Wildobst.

Innerhalb einer obsttragenden Art gibt es zahlreiche züchterische Sorten, also Varietäten mit verschiedenen Eigenschaften, was Aussehen, Gehalt und Eigenschaften bezüglich Reife, Lagerung und Verwendung betrifft. Geschützte Handelssorten unterliegen dem Sortenschutz; daneben gibt es zahlreiche traditionelle Sorten, "Alte Sorten" genannt, die sich in der regionalen Landwirtschaftsgeschichte entwickelt haben. Hier spricht man dann etwa von "Edelsorte" und "Bauernobst".

Zu den Sorten einzelner Arten siehe:

Je nach Obstnutzung wird "Tafel-" und "Wirtschaftsobst" unterschieden, ersteres sind Sorten von bester Qualität für den Einzelhandel und direkten Verzehr, zweiteres für die Weiterverarbeitung, also Saftproduktion, Einmachobst und ähnliches sowie "Industrieobst" als Rohstoff für spezielle Produkte wie Geliermittel oder Lebensmittelzusatzstoffe und Farbstoffe.

Typische Nutzungssorten sind etwa "Lagerobst", das erst nachreift und nicht gekühlt oder sofort verzehrt werden muss, oder spezielle "Kochobstsorten" mit sich erst durch Erwärmung entwickelndem Geschmack.

Die einzelnen Früchte werden nach Größe und Qualität in verschiedene Handelsklassen einsortiert. Exemplare, welche die Anforderungen an die Handelsklassen nicht erfüllen, werden aussortiert und anderweitig verwertet. Die drei Qualitätsklassen sind Extra, I und II, sie sind als EU-Qualitätsnorm festgelegt.





</doc>
<doc id="3807" url="https://de.wikipedia.org/wiki?curid=3807" title="Apache OpenOffice">
Apache OpenOffice

Apache OpenOffice (vormals OpenOffice.org) ist ein freies Office-Paket, das aus einer Kombination verschiedener Programme zur Textverarbeitung, Tabellenkalkulation, Präsentation und zum Zeichnen besteht. Ein Datenbankprogramm und ein Formeleditor sind ebenfalls enthalten. Es ist für alle wichtigen Betriebssysteme verfügbar. Das quelloffene Projekt war bis zur Abspaltung von LibreOffice eines der international führenden Office-Pakete, inzwischen hat LibreOffice diese Rolle aufgrund der größeren Entwicklerzahl und häufigerer Sicherheitsupdates übernommen.

Der Zugang zu Funktionen und Daten wird durch offengelegte Schnittstellen und ein XML-basiertes Dateiformat ermöglicht. OpenOffice.org wurde unter der LGPL verbreitet. Apache OpenOffice wird unter der Apache-Lizenz Version 2 herausgegeben. Da diese keine Verbreitung zu gleichen Lizenzbedingungen vorschreibt, kann der Code weiterhin in das Projekt LibreOffice fließen.

OpenOffice.org entstand im Jahr 2000 aus den offengelegten Quelltexten des damaligen StarOffice und wurde seither maßgeblich von Sun Microsystems, das später von Oracle aufgekauft wurde, entwickelt. Heute wird es von der Apache Software Foundation weiterentwickelt. Nachdem im September 2010 aus Unzufriedenheit mit Oracles Lizenz- und Entwicklerpolitik durch die von vielen ehemaligen OpenOffice.org-Entwicklern gegründete The Document Foundation die Abspaltung LibreOffice entstand, teilte Oracle im Juni 2011 mit, seine Rechte an "OpenOffice.org" auf die Apache Software Foundation (ASF) übertragen zu wollen.

Das Apache OpenOffice Projekt verließ innerhalb von rund einem Jahr den „Incubator“-Projektstatus und wurde 2012 zu einem „Top-Level-Projekt“ hochgestuft, war jedoch ab Sommer 2014 weitestgehend inaktiv.

Das Office-Paket enthält die folgenden Module, die in den anschließenden Abschnitten genauer beschrieben werden:
Apache OpenOffice ist für die Betriebssysteme Windows, Apple macOS (bis zur Version 2.x als X11-Version und als Nebenprojekt NeoOffice verfügbar; ab Version 3.0 ist Apache OpenOffice eine native Aqua-Anwendung), IBM OS/2, eComStation, Linux, Solaris (SPARC- und x86-Prozessorarchitektur), FreeBSD und andere Unix-Varianten erhältlich. ReactOS wird, je nach Version, durch die MS-Windows-Version unterstützt.

Mit „OpenOffice.org Portable“, auch „Portable OpenOffice.org“ genannt (siehe auch PortableApps), stehen seit 2.0.4 ausgewählte Versionen für Windows zur Verfügung, die zum Beispiel von einem USB-Stick lauffähig sind, ohne notwendigerweise Datenrückstände auf dem genutzten Rechner zurückzulassen (siehe auch Portable Software). Weiterhin gibt es eine U3-Version, die von einem USB-Stick ausführbar ist und abgespeicherte Daten verschlüsselt sowie mit einem Passwort schützt. Die aktuelle portable Version ist 4.1.5.

Apache OpenOffice kann die Daten vieler anderer Programme sowie die verbreiteten Dateiformate von Microsoft Word (*.doc/*.docx), Microsoft Excel (*.xls/*.xlsx) und Microsoft PowerPoint (*.ppt/*.pptx) zumeist ohne Probleme importieren und exportieren (Export nur *.doc/*.xls/*.ppt), doch es können (wie auch zwischen unterschiedlichen MS Office-Versionen) Formatierungsprobleme auftreten, zum Beispiel verrutschte Absätze. Auch lassen sich diverse „Legacy-Formate“ (ältere Dateiformate) anderer Anbieter importieren. Alle Formate lassen sich ohne Umwege ins Portable Document Format (PDF) exportieren.

Apache OpenOffice ist modular aufgebaut, aber als Gesamtpaket konzipiert. Identische Utensilien werden in der gesamten Suite genutzt. Die Werkzeuge, die es etwa im "Writer" zum Arbeiten mit Grafiken gibt, finden sich auch in "Impress" und "Draw" wieder. Alle Module teilen sich zudem dieselbe Rechtschreibprüfung etc. Das komplette Office-Paket kann in einem einzigen Vorgang installiert werden.

Mit "Writer" können sowohl kurze Texte wie Briefe, Serienbriefe, Memos, Etiketten, Visitenkarten als auch umfangreiche Schriften wie Bücher oder mehrteilige Dokumente mit Tabellen sowie Inhalts- und Literaturverzeichnissen geschrieben und gestaltet werden. Die Textverarbeitung bietet gängige Funktionen wie Textbausteine, Teamfunktionen, Rechtschreibprüfung, Silbentrennung, Thesaurus, Autokorrektur, mehrstufiges Undo sowie verschiedene Dokumentvorlagen. Mit Hilfe eines Assistenten werden eigene Dokumentvorlagen, Briefe, Faxe und Tagesordnungen erstellt. Neben dem Zugriff auf die Systemschriftarten enthält das Paket einen Satz freier Schriften. Versionsverwaltung von Dokumenten ist möglich. Das Paket ist voll Unicode-tauglich, es beherrscht CJK-Unterstützung und neben Rechtslauf auch Linkslauf.

Formatvorlagen für einzelne Zeichen, Absätze, Rahmen und Seiten können mit dem "Stylist" (ab Version 2.0 Fenster "Formatvorlagen") erstellt und zugewiesen werden. Der "Navigator" erlaubt es, sich schnell im Dokument zu bewegen, es in einer Gliederungsansicht zu betrachten und den Überblick über darin eingefügte Objekte zu behalten. Innerhalb der Texte können verschiedene Verzeichnisse (Inhalt, Literatur, Stichworte, Abbildungen u. a.) erzeugt und angepasst werden. Querverweise können gesetzt werden, und mit Hyperlinks kann man über Textmarken direkt zu Textstellen springen.

Texte können mehrspaltig formatiert und mit Textrahmen, Tabellen, Grafiken und anderen Elementen versehen werden. Mit Hilfe der Zeichenwerkzeuge werden innerhalb des Dokuments Zeichnungen, Legenden und andere Zeichenobjekte erstellt. Grafiken unterschiedlicher Formate können eingebunden werden, zum Beispiel Grafiken in den Formaten GIF oder PNG. Es lassen sich die gängigen Bildformate im Textverarbeitungsdokument mit dem mitgelieferten Bildbearbeitungswerkzeugen bearbeiten. Clipartsammlungen, Animationen und Klänge werden in der "Gallery" verwaltet und nach Themen geordnet.

Textdokumente verfügen über eine integrierte Rechenfunktion, mit der Rechenoperationen oder logische Verknüpfungen ausgeführt werden. Die für die Berechnung benötigte Tabelle lässt sich in einem Textdokument erstellen.

Der in "Writer" enthaltene HTML-Editor ist ein WYSIWYG-Editor zum Erstellen von HTML-Webseiten. Ein umfassendes Hilfesystem steht zur Verfügung, das Anweisungen für einfache und komplexe Vorgänge abdeckt.

In "Calc" werden Daten in Tabellen bearbeitet, analysiert, verwaltet und verdeutlicht. Daten können angeordnet, gespeichert und gefiltert werden. Die Tabellenkalkulation bietet über 450 Berechnungsfunktionen etwa aus den Bereichen elementare Mathematik, Finanzen, Statistik, Datum und Zeit. Es steht ein Funktionsassistent zum Erstellen von Formeln und komplexen Berechnungen einschließlich Vektoralgebra (Matrizen-Rechnen) zur Verfügung. Mit externen Add-ins lassen sich weitere Funktionen nachrüsten.

Tabellen können durch Ziehen und Ablegen aus Datenbanken übernommen und Tabellendokumente diverser Formate (OOo-intern, aber auch etwa CSV) als Datenquelle eingesetzt werden. Auch das Einbetten von Webinhalten (Tabellen aus HTML-Dokumenten) ist möglich. Bestimmte Datenbereiche können ein- oder ausgeblendet werden. Es gibt einen "Datenpilot" für die Analyse von Zahlenmaterial und zur Erstellung von Pivottabellen. Es besteht die Möglichkeit, in Berechnungen, die aus mehreren Faktoren bestehen, die Auswirkungen von Änderungen einzelner Faktoren zu beobachten. Zur Verwaltung umfangreicher Tabellen stehen verschiedene vordefinierte Szenarien zur Verfügung; Teil- oder Gesamtergebnisse können berechnet werden.

"Calc" ermöglicht die Darstellung von Tabellendaten in dynamischen Diagrammen, die bei Änderung der Daten automatisch aktualisiert werden. Ein Assistent für Diagramme ist vorhanden.

Mit "Impress" können Vortragsfolien mit Animationen und verschiedenen Hintergründen erstellt werden. Präsentationen können mit Diagrammen, Zeichenobjekten, Multimedia- und vielen anderen Elementen versehen werden. Einzelnen Folien können unterschiedliche Übergangseffekte zugeordnet werden.

Ein Assistent für das Erstellen von Präsentationen ist ebenso enthalten wie verschiedene Vorlagen. Beim Erstellen einer Präsentation stehen mehrere Ansichten zur Verfügung. Die Folienansicht zeigt zum Beispiel die Folien im Überblick, während die Handzettelansicht zusätzlich zur Präsentation begleitenden Text enthält. Die Folien können auf dem Bildschirm automatisch vorgeführt oder manuell gesteuert werden. Der zeitliche Ablauf der Präsentation kann angepasst werden. Die Präsentationen können als Handzettel verteilt oder als HTML-Dokumente gespeichert werden.

Mit dem vektorbasierten "Draw" ist es möglich, verlustfrei skalierbare 2D-Vektorgrafiken inklusive dreidimensionaler Effekte zu erstellen. "Draw" verarbeitet die üblichen geometrischen Grundelemente, Splines und Bézierkurven und beherrscht grundlegendes Shading und Manipulation der Lichtquelle. Es sind Vorlagen und eine Auswahl an anpassbaren Ausgangsformen für Zeichnungselemente enthalten. Raster und Fanglinien sind optische Hilfen, die die Anordnung von Objekten in Zeichnungen erleichtern. Textmanipulation ist mit dem Modul "FontWork" möglich.

In "Draw" lässt sich die Beziehung zwischen verschiedenen Objekten mit speziellen Linien, den sogenannten "Verbindern", darstellen. Die Verbinder werden an die Klebepunkte der Zeichenobjekte angefügt und lösen sich auch nicht, wenn die miteinander verbundenen Objekte verschoben werden. Daher ist adaptive Bemaßung möglich, zum Beispiel für technische Zeichnungen. Mit Draw können lineare Größen anhand von Bemaßungslinien berechnet und angezeigt werden. Außerdem sind Flow-Charts, Concept-Maps und Ähnliches damit umsetzbar.

Tabellen (aus "Calc"), Diagramme, Formeln (aus dem Modul "Math") und andere in Apache OpenOffice erzeugte Elemente können in Zeichnungen eingefügt und umgekehrt die Grafiken in OOo-Dokumente anderen Typs eingebettet werden.

Zeichnungen können in unterschiedlichen Formaten gespeichert werden – darunter SVG, EPS, Windows WMF und MacPict, Adobe PDF und Shockwave SWF (nicht aber DWG/DXF). Auch Rastergrafikkonvertierung nach BMP, GIF, PNG, TIFF und JPG ist möglich.

Das Importieren dieser Dateien (außer PDF und SVG, für die externe Erweiterungen vorhanden sind, sowie SWF) ist ebenfalls möglich, zusätzlich ist ein Werkzeug zur Vektorisierung (Umwandlung von Raster- in Vektorgrafik) implementiert. Der Importfilter für PDF-Dateien befindet sich zwar nicht mehr im Beta-Stadium, arbeitet jedoch noch nicht fehlerfrei.

Das Datenbankmanagementsystem (DBMS) "Base" kann große Datenmengen speichern und für Abfragen und Berichte bereitstellen. Es verwaltet Relationale Datenbanken, in denen die Daten in Tabellenform abgelegt sind. Externe Datenbanksysteme, wie beispielsweise MySQL, HSQL, SQLite oder PostgreSQL, können mittels ODBC oder JDBC angebunden werden und stehen somit ebenfalls als Datenquelle etwa für Serienbriefe zur Verfügung. Für OpenOffice.org 3.1 in Verbindung mit MySQL ab 5.1 gibt es einen nativen Datenbanktreiber, der Umweg über ODBC/JDBC entfällt somit. "Base" unterstützt einige Datenbankformate, wie zum Beispiel das dBASE-Format.

"Math" dient zum Verfassen von mathematischen Formeln. Formeln werden in "Math" nicht ausgewertet, es ist also kein „Rechenprogramm“ (das ist das Modul "Calc") oder gar ein Computeralgebraprogramm, sondern ein Editor für Formelsatz, der auf einem TeX-Dialekt aufgebaut ist. Formeln werden als Objekte innerhalb eines anderen Dokuments erstellt, lassen sich also wie Bilder in den Textfluss einpassen.

Beim Einfügen einer Formel in ein anderes Dokument wird "Math" automatisch gestartet. Die Befehle zum Aufbau der Formeln sind in einem Auswahlfenster zu finden und können dort mit der Maus angeklickt werden, um sie hinzuzufügen. Vordefinierte Symbole, Sonderzeichen und eine Basisauswahl an Funktionen stehen zur Verfügung. Es können eigene Symbole erstellt und Zeichen aus fremden Zeichensätzen übernommen werden. Eine Formel kann entweder direkt eingegeben oder in einem Befehlsfenster bearbeitet werden – die Eingaben im Befehlsfenster werden gleichzeitig im Textfenster angezeigt (WYSIWYG-Editor), einschließlich Fehlererkennung.

Der "Navigator" und das "Globaldokument" sind die zwei grundlegenden Werkzeuge, die die Funktion als Gesamtpaket der Text- und weiteren Datenverarbeitung als „Office“ zusammenstellen.


Die Benutzeroberfläche kann konfiguriert werden. Symbole und Menüs lassen sich anpassen. Auch Tastaturkürzel können festgelegt werden. Bestimmte Programmfenster und die meisten Symbolleisten sind als schwebende Fenster frei platzierbar oder können am Rand des Arbeitsbereichs angedockt werden.

Assistenten zur Konvertierung von Dokumenten sind enthalten, beispielsweise können alle Microsoft-Word-Dokumente, die sich in einem Verzeichnis befinden, mit einem einzigen Vorgang umgewandelt werden. Es lassen sich auch Dateien einlesen, die von anderen Officepaketen erstellt wurden, wie etwa Lotus Notes, Corel WordPerfect. Weiterhin können andere Einzelformate, wie auch alle Vorgängerformate seit StarOffice 3.0 gelesen und meist auch geschrieben werden. Auch Konvertierer für Wiki-Syntax sowie DocBook können installiert werden. Mit der Reparaturfunktion können beschädigte Dateien oft wiederhergestellt werden, weiterhin gibt es einen automatischen regelmäßigen Abspeichermechanismus.

In der StarOffice-Basic-IDE können Makros erstellt werden. Zur Erweiterung der Programmfunktionen steht eine Vielzahl von Vorlagen, "Add-ons", "Add-ins" und Makros in den Sprachen StarOffice Basic, Python, Java und JavaScript zur Verfügung, eine Entwicklungsumgebung dafür kann installiert werden. Java-Applets können in die Dokumente eingebunden werden. Auch Plug-ins für die Websuche können ergänzt werden.

Für einige Assistenten, die eingebaute HSQL-Datenbank und einige Exportfilter wird ein Java Runtime Environment (JRE) benötigt. Von diesen Funktionen abgesehen ist Apache OpenOffice auch ohne JRE lauffähig. Das kostenlose Java Runtime Environment wird bei einigen OOo-Installationspaketen mitgeliefert, lässt sich aber auch nachträglich installieren.

Erweiterungen in Libreoffice und Openoffice teilen sich die Dateinamenserweiterung ".oxt".

Marco Börries gründete 1984 im Alter von 16 Jahren in Lüneburg die Firma Star Division, deren Hauptprodukt das Office-Paket StarOffice wurde. Nachdem StarOffice mehr als 25 Millionen mal verkauft worden war, erwarb Sun Microsystems 1999 die inzwischen in Hamburg ansässige Firma Star Division für einen zweistelligen Millionenbetrag. Sun bot StarOffice zunächst als kostengünstiges Konkurrenzprodukt zu Microsoft Office an. Ab Version 5.1a und später 5.2 wurde das Programm inklusive der fremdlizenzierten Bestandteile (wie z. B. Rechtschreibprüfung) als kostenlose Version zum Herunterladen und auf CDs angeboten, die Computerzeitschriften beigelegt waren. Parallel dazu wurde weiterhin eine kommerzielle Version inklusive der Datenbankanwendung Adabas vertrieben. Am 19. Juli 2000 wurde das OpenOffice.org-Projekt von Sun Microsystems öffentlich bekanntgegeben und am 13. Oktober 2000 ging die Website OpenOffice.org online, über die der Quelltext einer Vorversion von StarOffice 6.0 bezogen und von der Community bearbeitet und verbessert werden konnte. Der Quelltext war zu diesem Zeitpunkt etwa 400 MB groß und enthielt über 35.000 Dateien mit insgesamt rund 7,5 Millionen Zeilen C++-Code. Von Drittanbietern lizenzierte Komponenten waren zuvor daraus entfernt worden.

Build 638c – die erste funktionierende, frei verfügbare OpenOffice-Version – wurde im Oktober 2001 veröffentlicht. OpenOffice.org 1.0 wurde am 1. Mai 2002 und OpenOffice.org 1.1 im September 2003 herausgegeben. Im Oktober 2005 erfolgte der Schritt auf Version 2.0, im Oktober 2008 die Version 3.0 veröffentlicht.

Die letzten Versionen von StarOffice, seit 2010 als Oracle Open Office bezeichnet, basieren auf OpenOffice.org, werden aber von Sun Microsystems/Oracle um die aus dem OpenOffice.org-Quellcode entfernten Komponenten (darunter Rechtschreibkorrektur, Thesaurus, Datenbankmodul Adabas D und Cliparts) erweitert. Aufgrund der Lizenzierung kann der OpenOffice.org-Code für das nicht quelloffene Oracle Open Office verwendet werden. Sun Microsystems hatte beim Projektstart OOo unter die GNU Lesser General Public License (LGPL) und unter die Sun Industry Standards Source License (SISSL) gestellt. Seit September 2005 steht OpenOffice.org nur noch unter der LGPL, nachdem Sun bekanntgegeben hatte, die SISSL in Zukunft nicht mehr zu nutzen.

Seit Herbst 2007 gibt es mehr als 80 Sprachversionen. Allein die Version 3 von OpenOffice.org wurde bereits über 100 Millionen Mal heruntergeladen.

Nach der am 27. Januar 2010 erfolgten Sun-Übernahme durch die Oracle Corporation wird OpenOffice.org in einer eigenen Abteilung weitergeführt. Die freie Community-Version existiert weiterhin.

Am 28. September 2010 gab die neu gegründete „The Document Foundation“ bekannt, dass sie das Projekt unter dem Namen LibreOffice weiterführen wolle und sich damit von Sun/Oracle komplett löst. Auf der Website der Stiftung wurde erklärt, dass man darauf hoffte, dass Oracle auch die Rechte am Namen OpenOffice.org an die Stiftung übergeben werde. Dazu ist es jedoch nicht gekommen.

In der Folgezeit verließen viele Entwickler OpenOffice und arbeiteten stattdessen an LibreOffice weiter.

2015 kristallisierte sich heraus, dass OpenOffice den Wettlauf gegen LibreOffice verloren hatte. Zu viele Entwickler waren zum Konkurrenten LibreOffice abgewandert, es gab kaum neue OpenOffice-Versionen und im Vergleich zu LibreOffice nur wenig neue Features. Zudem war die Entwicklung von OpenOffice geprägt von Streitigkeiten zwischen den Entwicklergruppen der beiden Programme, vor allem wegen der unterschiedlichen Lizenzierung der beiden Softwarepakete.

2016 hat Dennis E. Hamilton, der Vorsitzende des OpenOffice Project Management Committees, öffentlich das Aus des Projekts diskutiert. An der Entwicklung beteiligten sich zu diesem Zeitpunkt nur noch sechs Entwickler, die kaum die Zeit aufbringen konnten, auch nur die Sicherheitslücken zu beheben, geschweige denn neue Features zu entwickeln oder regelmäßig neue Versionen zu veröffentlichen.

OpenOffice.org 1.0 wurde am 1. Mai 2002 veröffentlicht. Augenfällige Änderung gegenüber StarOffice 5.2 war das Weglassen des integrierten Desktops. Auch die im StarOffice 5.2 enthaltenen Anwendungen Mail-Client, Organizer und die Datenbank Adabas D fielen weg. Es kamen drei Aktualisierungen heraus, wobei die letzte im April 2003 unter der Versionsnummer 1.0.3.1 erschien. Version 1.0.3.1 war offiziell die letzte Version für Windows 95, Version 1.1.5 die letzte Version für Windows NT 4.0.

Im Oktober 2003 wurde die Version 1.1 freigegeben. Auch bei dieser Version kamen in unregelmäßigen Abständen fehlerkorrigierte Versionen heraus. Wichtige Änderungen in der Version 1.1 waren:

Am 14. September 2005 erschien OpenOffice.org 1.1.5. Diese letzte Aktualisierung unter der Versionsnummer 1 enthält neben zahlreichen Fehlerkorrekturen als hauptsächliche Neuerung Importfilter für die OASIS OpenDocument-Formate, die ab OpenOffice.org 2.0 als Standardformat genutzt werden. Am 4. Juli 2006 erschien das Sicherheitspatch "1.1.5secpatch", welches das unaufgeforderte Ausführen von Makro-Befehlen (BASIC) in manipulierten OpenOffice.org-Dateien unterbindet.

Die Entwicklung an Version 2 von OpenOffice.org begann bereits im Juli 2003. Es wurden zwei Beta-Versionen und mehrere Snapshots unter der Versionsnummer 1.9 veröffentlicht. Die endgültige Version wurde am 20. Oktober 2005 freigegeben. Wichtigste Neuerungen waren die eigene Datenbankanwendung (Base), das neue Dateiformat OpenDocument und eine sich den Desktop-Einstellungen anpassende Oberfläche. Außerdem wurde die Benutzerführung optimiert, um Benutzern von Microsoft Office einen möglichst einfachen Umstieg auf OpenOffice.org zu ermöglichen. Wichtige Änderungen in der Version 2.0 sind:

Impress wurde von Grund auf neu programmiert und bietet jetzt unter anderem mehr Diashow-Übergänge und Animationseffekte. Der PDF-Export wurde erweitert: Hyperlinks sind jetzt möglich, das Format für Formularübermittlung ist auswählbar, Notizen können exportiert werden, Vorschaubilder und mehr Stufen für die Komprimierung von Bildern. Mit der neuen Wortzählfunktion können jetzt markierte Textabschnitte gezählt werden. Am 15. Dezember 2005 wurde die Version 2.0.1 veröffentlicht. Diese erste Aktualisierung für OpenOffice.org 2.0 behob eine Reihe von Fehlern. Außerdem wurde eine Serien-Mail-Funktion integriert. OpenOffice.org 2.0.2 erschien am 8. März 2006. Diese Version ersetzt das bisher für die Rechtschreibprüfung verwendete MySpell durch Hunspell. Weitere Neuerungen sind Icons für KDE und Gnome sowie ein Importfilter für das MS-Word 2/5-Textformat. Die englische Version von OpenOffice.org 2.0.3 wurde am 29. Juni veröffentlicht. Die deutsche Version erschien am 3. Juli 2006. Neben der Beseitigung von Fehlern wurden auch neue Funktionen implementiert, unter anderem Unterstützung von x86-64-Plattformen, eine Aktualisierungsfunktion und die optionale Unterstützung der Grafikbibliothek Cairo. Letztere verspricht unter anderem Antialiasing in Präsentationen. Auffälligster Fehler war der Export in PDF-Dateien, der mitunter (oft) ungültige („Unbekannter Token“) und zum Teil nicht lesbare PDF-Dateien erzeugte, was aber erst beim Öffnen mit einem PDF-Reader sichtbar wurde. Am 13. Oktober 2006 wurde die Version 2.0.4 veröffentlicht. Diese Version erhielt einen Exportfilter für LaTeX und PDF-Verschlüsselung. Sie bildet den Abschluss der 2.0-Produktreihe.

Version 2.1 von OpenOffice.org erschien am 12. Dezember 2006. In ihr wurden neu implementiert: ein Aktualisierungssystem, der überarbeitete Schnellstarter sowie die Verbesserung der Exportfunktion von HTML-Dateien aus Calc. Die Version enthielt einen auffälligen Fehler, durch den in Textdokumenten nach Seitenumbrüchen das Inhaltsverzeichnis versetzt und umformatiert wurde. Kapitel 7.2 des offiziellen Installationshandbuches beschreibt zudem die Installation (geht bis Version 2.1) unter Windows NT 4.0. Mit einer aktualisierten Systemdatei kann OOo 2.0.2 unter Windows 95 gestartet werden. Unter diesen Windowsversionen gilt die OOo-Unterstützung als experimentell und wird nicht vom Support abgedeckt.

Am 28. März 2007 erschien die Version 2.2 mit erweiterten Vista-Funktionen, erweitertem Umgang mit Extensions und ebenfalls erweitertem PDF-Export. Mit dieser Version ändert OpenOffice.org die Zeitabstände zwischen Aktualisierungen mit neuen Funktionen von drei auf sechs Monate. Auffälligster Fehler war in CALC unsichtbarer Text beim Bearbeiten von Notizen von Zellen. Dieser Fehler wurde mit der Version 2.2.1 behoben. OpenOffice.org 2.2.1 erschien am 12. Juni 2007 und brachte einige Fehlerkorrekturen – neue Funktionen wurden nicht integriert.

Version 2.3 wurde am 17. September 2007 veröffentlicht. Die Installationsdatei ist mit minimal 100 MB wesentlich größer als die der Vorgängerversionen. Neben Fehlerkorrekturen sind auch neue Funktionen integriert. In dieser Version ist das Diagrammmodul Chart neu programmiert worden. Verbesserungen gibt es bei der Geschwindigkeit der Darstellung, bei Regressionsdarstellungen in Diagrammen und der 3D-Funktionalität; auch der Diagrammassistent ist überarbeitet. Für das Datenbankmodul ist ein neuer Reportgenerator verfügbar. Writer beherrscht nun teilweise das MediaWiki-Format (Tabellen, Zeichenformatierung, Weblinks) als Exportoption. Weiterhin wurden Calc, die Rechtschreibprüfung und der HTML-Export bei Präsentationen verbessert. OpenOffice.org 2.3.1 erschien am 4. Dezember 2007 und brachte einige Fehlerkorrekturen, neue Funktionen gab es nicht.
Die Version 2.4.0 erschien am 27. März 2008. Verbessert wurden in Calc die Formulareingabe und das Sortieren von Spalten im Datenpilot mittels Drag and Drop. Der Datenpilot erlaubt jetzt einen Drilldown aus Ergebniszellen. Mit der Komponente Chart sind Diagramm-Beschriftungen besser positionierbar. In Impress können Hintergrundgrafiken per Kontextmenü eingebunden werden. Folientitel werden beim PDF-Export als Lesezeichen abgespeichert. Die Statusleiste in Writer zeigt die Sprachversion des Absatzes an. Blockmarkierungen in Textdokumenten sind ab sofort möglich. Base unterstützt unter Windows zusätzlich Datenbanken im Access-2007-Format. Die Sicherheitsfunktionen wurden um ein Master-Passwort für Internet-Verbindungen erweitert. Der Zugriff auf WebDAV-Server über HTTPS ist ebenfalls möglich geworden. Der PDF-Export bietet PDF/A-1 (ISO 19005-1) zur Langzeitarchivierung. Beim automatischen Suchen von Programmaktualisierungen wird auch geprüft, ob neue Versionen installierter Erweiterungen vorliegen. Die Hilfefunktion wurde ebenfalls erweitert.

Die Version 2.4.1 erschien am 10. Juni 2008. Es handelt sich dabei um eine reine Fehlerkorrektur-Version ohne neue Funktionen; insbesondere wurde damit ein Sicherheitsproblem behoben. Ende Oktober 2008 erschien mit Version 2.4.2, und im September 2009 mit Version 2.4.3 die letzte Aktualisierung der Version 2.4.x, welche ebenfalls reine Fehlerkorrektur-Versionen waren. Es war die letzte Version für Windows 98, Windows 98 SE und Windows Me.

Seit Dezember 2009 wird der 2.x-Versionszweig von der Entwickler-Community nicht mehr kostenfrei weiterentwickelt.

Version 3.0 wurde am 13. Oktober 2008 veröffentlicht. Diese Version enthält gegenüber den Versionen 2.x zahlreiche neue Funktionen und ist erstmals auch nativ, also ohne X11, für macOS mit Aqua-Unterstützung erhältlich. OpenOffice 3 enthält eine Überarbeitung von Calc sowie erweiterte Kommentarfunktionen. Ab Version 3.0.1 wurde der "Extension Manager" für freie Programmerweiterungen mit StarOffice 9 PU 1 harmonisiert, um dieselben Erweiterungen verwenden zu können. Zusätzlich sind die Importfunktionen um die Dateiformate aus Microsoft Office 2007 für Text- und Tabellendokumente, das „Office Open XML“, ergänzt worden.

Ab der Version 3.1.1 werden Grafikobjekte mit Hilfe von Antialiasing in höherer Qualität dargestellt. Die Version 3.2 brachte eine erhöhte Stabilität und Geschwindigkeit. Außerdem unterstützt OpenOffice.org ab Version 3.2 Graphite- und OpenType-Schriften, Kommentarfunktionen für Impress und Draw und neue Blasendiagramme für Calc. Zusätzlich wurde die Startgeschwindigkeit (Kalt- und Warmstart) erheblich erhöht.

Die ebenfalls vorgesehene Erweiterung um einen Personal Information Manager (PIM) wurde auf eine spätere Version verschoben. Sun Microsystems betätigt sich zu diesem Zweck im Lightning-Projekt der Mozilla-Foundation. Ziel ist die Integration des Mail-Clients Mozilla Thunderbird mit Adressbuch und Kalender als Groupware-Client in OpenOffice.org.

Die Version 3.4.0 ist am 8. Mai 2012 veröffentlicht worden. Der Name wurde von "OpenOffice.org" auf "Apache OpenOffice" (ohne „.org“) geändert. Sie ist die erste Version nach der Übernahme durch die Apache Software Foundation. Die Version 3.4.1 ist am 23. August 2012 veröffentlicht worden.

Version 4.0.0 wurde am 23. Juli 2013 veröffentlicht. Die auffälligste Neuerung dieser Version stellt eine Seitenleiste dar. Diese soll die Arbeit an Breitbildmonitoren erleichtern, weil dadurch die sonst oben liegenden Bedienelemente an die Seite verlagert werden. Darüber hinaus wurde die Unterstützung für drei weitere Sprachen geschaffen, 500 Programmfehler bereinigt, die Kompatibilität mit Microsoft Office erhöht, die Arbeit mit Grafiken verbessert und vieles mehr. Die Unterstützung für Windows 2000 entfiel.

Version 4.0.1 wurde am 1. Oktober 2013 veröffentlicht. Einige Programmfehler wurden beseitigt, die Geschwindigkeit der Speicherung von .XLS-Dateien erhöht und zusätzliche Sprachen werden unterstützt.

Version 4.1.0 wurde am 29. April 2014 veröffentlicht. Es wurden neue Funktionen hinzugefügt und Verschiedenes geändert: Unterstützt werden nun Kommentare/Anmerkungen zu Textbereichen, iAccessible2, In-place-Bearbeitung von Eingabefeldern sowie eine interaktive Funktion zum Zuschneiden. Sie setzt macOS 10.7 oder höher voraus, die Version OS 10.6 (Snow Leopard) wird damit nicht mehr unterstützt.

Version 4.1.1 wurde am 21. August 2014 veröffentlicht. Es handelt sich um eine Aktualisierung, die einige Fehler bereinigt und verbesserte Übersetzungen enthält. Neu hinzugekommen ist Katalanisch.

Version 4.1.2 wurde am 28. Oktober 2015 veröffentlicht. Das WebDAV- Management zur Bereitstellung von Dateien im Internet wurde überarbeitet und die Integration mit SharePoint bereitgestellt. Der PDF-Export wurde überarbeitet und Fehlerbereinigungen in Writer, Calc, Impress/Draw und Base durchgeführt.

Version 4.1.3 wurde am 12. Oktober 2016 veröffentlicht. Es ist ein Release zur Fehlerbeseitigung, welches Sicherheitsprobleme beseitigt, Wörterbücher aktualisiert und einige sonstige bekannte Fehler korrigiert.

Version 4.1.4 wurde am 19. Oktober 2017 veröffentlicht. Es ist ein Wartungs-Release mit einigen wenigen wichtigen Bugfixes, Sicherheitsfixes, aktualisierten Wörterbüchern und Buildfixes.

Version 4.1.5 wurde am 30. Dezember 2017 veröffentlicht. Es ist ein Wartungsrelease, das darauf abzielt, eine Reihe sogenannter Regressionsbugs (3) zu korrigieren und das aktuellste Sprachwörterbuch für Englisch zu liefern.

Apache OpenOffice lässt sich unter aktuellen Windows-, macOS-, Linux-, Unix- und eComStation-Systemen nutzen und ist damit weitgehend plattformunabhängig.

Die aktuelle Version 4.1.5 von OpenOffice lässt sich unter Windows XP, Windows Vista, Windows 7, Windows 8 und Windows 10 installieren.

Die Mac-OS‑X-Portierung von OpenOffice.org erfordert mindestens Mac OS X Tiger (10.4, 2005) und bis OpenOffice.org Version 2.4.3 auch die X11-Bibliotheken. Ab Version 3.0 wurde sowohl die PowerPC- als auch die Intel-x86-Architektur nativ unterstützt (separate Versionen, nicht als Universal Binary); Version 3.2.1, gleichzeitig die letzte PowerPC-Version des Office-Paketes, ist als „OpenOffice.org X11“ wie zuvor als X11-Version und als „OpenOffice.org Aqua“ nativ für die grafische Benutzeroberfläche von Mac OS X verfügbar; letztere setzt Mac OS X Tiger 10.4.11 (November 2007) voraus.

Die letzte unter Mac OS X Snow Leopard (10.6, 2009) lauffähige OpenOffice-Version ist 4.0.1.; ab Version 4.1 wird Mac OS X Lion (10.7, 2011) oder neuer vorausgesetzt.

Bei den verbreiteten Linux-Distributionen (z. B. openSUSE, Fedora, Ubuntu) wurde OpenOffice durch LibreOffice ersetzt. OpenOffice kann aber von der Webseite des Apache-Projektes heruntergeladen werden, wird dann aber nicht automatisch mit Updates versorgt.

Für Solaris wird mindestens Solaris 8 auf der SPARC- oder x86-Prozessorplattform vorausgesetzt.

Für OS/2 und eComStation gab es bis zur Version GA 3.2 spezielle Versionen von OpenOffice.org von Serenity Systems International und Mensys BV im Rahmen kostenpflichtiger Supportverträge. Apache OpenOffice 4.1.3 wird für diese Betriebssysteme über die Firma "bww bitwise works" distribuiert.

Apache weist darauf hin, dass es Portierungen von Drittanbietern gibt. Insbesondere ist eine Portierung auf Android ab Version 2.3 erhältlich, sowie verschiedene Anpassungen an bestimmte Linux-Varianten, die nicht offiziell unterstützt werden. Bedingt durch die Quelloffenheit des Programmcodes und der freien Lizenzen hängt es von den Bedürfnissen und Kompetenzen externer Entwickler ab, ob und welche weiteren Betriebssysteme mit entsprechenden Anpassungen unterstützt werden.

Inzwischen gibt es einige Projekte, die OpenOffice.org an besondere Bedürfnisse oder Verwendungszwecke angepasst haben:


Im Gegensatz zu den vorgenannten Projekten ist "LibreOffice" eine Abspaltung von OpenOffice.org. Sie wird koordiniert durch die "Document Foundation" und seit September 2010 entwickelt. Ziel ist es, im Rahmen einer unabhängigen Stiftung die Arbeit aus der vorangegangenen zehnjährigen Entwicklung von OpenOffice.org weiterzuführen und neue Beiträge zentral durch die Stiftung zu verwalten. Die Stiftung wurde von OpenOffice.org-Projektmitgliedern aus Unzufriedenheit darüber gegründet, dass die Unterstützung durch die Entwicklungsabteilung von Oracle (vormals Sun Microsystems) für OpenOffice.org immer spärlicher ausgefallen war und die Markenrechte unklar waren. Nachdem Oracle eine Beteiligung an der Document Foundation abgelehnt und OpenOffice.org wie bisher weiterführen wollte, war die Übertragung der Namensrechte an die Document Foundation und eine Zusammenführung von OpenOffice.org und LibreOffice zunächst unwahrscheinlich geworden. 2011 übergab Oracle das OpenOffice.org-Projekt samt Namens- und Logorechten an die Apache Software Foundation , was neue Chancen für einen Codeaustausch zwischen beiden Projekten brachte.
Das Dateiformat von OpenOffice.org wurde von der Organization for the Advancement of Structured Information Standards (OASIS) als Basis für das neue offene Austauschformat OpenDocument verwendet, welches das Standardformat von OpenOffice.org ab Version 2.0 ist. Die XML-Dateien sind gepackt und belegen deshalb sehr wenig Speicherplatz. Die Dokumentinhalte werden im Java-Archive-Format gespeichert, einer ZIP-Datei mit speziellen Einträgen. Die Dateiendung eines Java-Archivs ist normalerweise „.jar“, jedoch werden für OpenDocument-Dateien Dateiendungen des Musters „.od?“ verwendet, wobei an der Stelle des ‚?‘ je nach Dokumententyp ein spezifischer Buchstabe steht, z. B. ‚t‘ für Writer-Dokumente: „.odt“.

Es kann mit jedem üblichen Packprogramm entpackt werden. Die eigentliche Textinformation (Datei "content.xml") kann danach mit jedem Texteditor angesehen und verändert werden. Zum Beispiel kann man Programme schreiben, die Formulare automatisch mit Inhalten einer Datenbank ausfüllen. Außerdem ist sichergestellt, dass auch in vielen Jahren noch uneingeschränkt auf diese Dateien zugegriffen werden kann; das ist gerade im kommerziellen und behördlichen Einsatz wegen der langen Aufbewahrungsfristen für Unterlagen ein nicht zu unterschätzender Vorteil. Die Europäische Union plant, das OASIS-Dateiformat als einheitliches Standarddatenformat für ihre Dokumente einzusetzen.

Im Mai 2006 wurde „OASIS-OpenDocument 1.0“ zum ISO-Standard (ISO 26300) erklärt.

In OpenOffice.org 1.0 und 1.1 wurden Dokumente standardmäßig im eigenen XML-basierten Dateiformat mit der Dateiendung „.sx?“ gespeichert. Dieses Dateiformat ist nicht identisch mit dem OpenDocument-Format, das in diesen OOo-Versionen noch nicht unterstützt wurde. Erst mit OpenOffice.org 1.1.5 konnten OpenDocument-Dateien zumindest geöffnet und bearbeitet werden – das Speichern musste im alten Dateiformat geschehen. OpenOffice.org 2.0 kann alle Dateiformate früherer Versionen verlustfrei lesen und schreiben, dazu zählen auch die alten StarOffice-Dateiformate mit der Dateiendung „.sd?“.

Ab OpenOffice.org 2.0.3 und dem darin neu eingeführten Rechtschreibprogramm Hunspell wird mit der deutschsprachigen Version von OpenOffice.org auch die Hunspell-Variante des deutschen Wörterbuchs igerman98 mitgeliefert. In früheren Versionen musste dieses teilweise noch aufgrund nicht kompatibler Lizenzen nachträglich hinzugefügt werden. Das Rechtschreibprogramm Hunspell, das ein direkter Nachfolger des vorher verwendeten Myspell ist, ermöglicht eine deutlich bessere Unterstützung von Sprachen, die Kompositabildung (Wortzusammensetzungen) erlauben. Die Wörterbücher für eine Kontrolle in weiteren Sprachen können ab Version 3.0 als „Extensions“ aus dem Internet bezogen werden und wie alle Programmerweiterungen automatisch auf Aktualisierungen überprüft werden.

Eine Grammatikprüfung wird bislang nicht angeboten. Mit der freien Erweiterung "LanguageTool" lässt sie sich für einige Sprachen, darunter Deutsch, nachrüsten.

Die Brockhaus-Tochter Brockhaus Duden Neue Medien (BDNM) bot bis Ende 2013 die proprietäre Erweiterung „Duden-Korrektor“ für OpenOffice.org an, die eine Rechtschreib-, Stil-, und Grammatikprüfung sowie eine automatische Silbentrennung und einen Thesaurus umfasste. Das Produkt wurde jedoch im Zuge der Aufgabe des Geschäftsbereichs Sprachtechnologie eingestellt und ist mit aktuellen Versionen von Apache OpenOffice und LibreOffice nicht mehr kompatibel.

Mit dem "Apache OpenOffice Software Development Kit" (SDK) können Entwickler das Office-Paket um weitere Funktionen erweitern oder externe Programme einbetten. Im SDK sind alle notwendigen Tools und Anleitungen enthalten. Die englischsprachige Dokumentation beschreibt die Konzepte der API und Komponententechnik UNO "(Universal Network Objects)", seit 2.0 auch Common Language Infrastructure (CLI). Es kann in den Sprachen StarOffice Basic, C, C++, Python und Java programmiert werden, in der Standardinstallation von Apache OpenOffice sind jedoch nur StarBasic und Python als portable Laufzeitumgebungen vorhanden. Das SDK steht unter der Apache-Lizenz Version 2 und kann für Windows, macOS und Linux von den Projektseiten kostenlos geladen werden.

Auch ein API-Plugin für NetBeans ist verfügbar.

OpenOffice.org wurde nur selten auf neuen Rechnern vorinstalliert. Im Sommer 2007 hat das amerikanische Unternehmen Everex Computer mit OOo 2.2 ausgestattet und in Nordamerika über den Einzelhandel vertrieben. Im Frühjahr 2008 wurden einige Modelle des Eee PC (mit Linux) in Deutschland und Österreich mit OOo ausgeliefert.

Studien aus den Jahren 2003 bis 2010 kamen auf Zahlen zwischen 3 und 15 Prozent für den internationalen Marktanteil und 5 Prozent für den deutschen Marktanteil im professionellen Umfeld. Im Oktober 2005 wurde eine strategische Partnerschaft von Google Inc. und Sun Microsystems geschlossen. Sie sollte unter anderem die Verbreitung von OpenOffice.org fördern. In einer Studie eines Web-Analytics-Dienstes im Januar 2010 wurde der OpenOffice-Marktanteil in Deutschland auf 21,5 Prozent bestimmt. Dazu wurden die installierten Office-Programme von über einer Million Internetnutzern in Deutschland bestimmt. Damit liegt Deutschland bei der OpenOffice-Marktdurchdringung im internationalen Vergleich im vorderen Bereich.

In einigen Firmen und öffentlichen Verwaltungen, wie etwa in München (LiMux-Projekt) und Wien (Wienux-Projekt), wurde zeitweilig OpenOffice.org eingesetzt. Seit 2009 wurde das Projekt in Wien nicht weiter verfolgt; 2017 beschloss auch der Stadtrat der Stadt München, zukünftig anstatt der Open-Source-Lösung einen Windows-Basis-Client mit Produkten aus dem Hause Microsoft zu nutzen. Die Open-Source-Lösungen hinkten den kommerziellen Produkten bisweilen im Funktionsumfang hinterher und wiesen nicht diejenige Kompatibilität mit anderen Produkten auf, die gewünscht sei. Ein großer Anwender war beispielsweise auch die französische Gendarmerie, die im Jahre 2005 etwa 70.000 Desktoprechner von Microsoft Office zu OpenOffice.org migriert hat.

In den wichtigsten Linux-Distributionen wurde OpenOffice mittlerweile durch LibreOffice ersetzt.

Die Arbeit am Quelltext wird vorrangig von den Entwicklern von Oracle (ehemals Sun Microsystems) übernommen. Weitere Unternehmen, die Entwickler stellen, sind beispielsweise IBM, Novell, Intel, Red Hat und Red Flag.

Die vielen Unterprojekte sind in drei Kategorien aufgeteilt:
Im Gesamtprojekt bildet der "Community Council" das oberste Organ. Er legt unter anderem die Ziele des Projektes fest.

Das Programm wird oft auch kurz "OpenOffice" genannt; dieser Begriff ist bzw. war jedoch in einigen Ländern markenrechtlich durch Dritte für andere Produkte geschützt. Das Projekt und das Programm nannten sich deshalb "OpenOffice.org" (Abkürzung "OOo") und seit 2012 (ab Version 3.4.0) "Apache OpenOffice" (Abkürzung "AOO"), um dieses Problem zu umgehen. Zwischenzeitlich war die Notwendigkeit hinzugekommen, "Apache OpenOffice" von dem damit nicht identischen, inzwischen eingestellten "StarOffice" (zwischenzeitlich "Oracle Open Office") zu unterscheiden.

Sun Microsystems hält die Urheberverwertungsrechte an Apache OpenOffice. Entwickler unterschreiben eine "Sun Microsystems Inc. Contributor Agreement" (SCA) genannte Vereinbarung (Nachfolger des früher verwendeten "Joint Copyright Assignment"), womit Sun ein gemeinsames Verwertungsrecht an Beiträgen erhält, die die Entwickler an Apache OpenOffice leisten. Die Übertragung der Nutzungsrechte wird von einigen Entwicklern, zum Beispiel vom Novell-Mitarbeiter Michael Meeks, als problematisch angesehen. Durch das SCA wird Sun Microsystems unter anderem in die Lage versetzt, mögliche Urheberrechts- bzw. Lizenzverletzungen rechtlich verfolgen zu lassen und die Lizenz festzulegen. So wurde für die Version 1 von OpenOffice.org eine duale Lizenzierung aus GNU Lesser General Public License (LGPL) und der Sun Industry Standards Source License (SISSL) verwendet, für die Version 2 wurde nur noch die LGPL (Version 2) genutzt und die Version 3 des Programms ist unter den Bedingungen der LGPL Version 3 veröffentlicht worden.

Die Rechte an der Wort- und Wort-Bild-Marke „OpenOffice.org“ hält in den USA und in der EU Oracle America, Inc. In manchen Ländern werden aber von Dritten Rechte am Markennamen „OpenOffice.org“ oder ähnlich klingenden geltend gemacht, so dass OpenOffice.org dort nicht unter seinem eigentlichen Namen in den Markt eintreten kann. Deshalb heißt zum Beispiel in Brasilien die Software „BrOffice.org“.




</doc>
<doc id="3808" url="https://de.wikipedia.org/wiki?curid=3808" title="Ohm (Begriffsklärung)">
Ohm (Begriffsklärung)

Ohm steht für:

Ohm ist der Familienname folgender Personen:

Siehe auch:


</doc>
<doc id="3809" url="https://de.wikipedia.org/wiki?curid=3809" title="Ordnung">
Ordnung

Ordnung oder Einordnung steht für:


mathematisch:

sonstiges:
Ordnung ist der Familienname folgender Personen:

Siehe auch:



</doc>
<doc id="3812" url="https://de.wikipedia.org/wiki?curid=3812" title="Oortsche Wolke">
Oortsche Wolke

Die Oortsche Wolke (andere Schreibweise: "Oort’sche" Wolke), auch als zirkumsolare Kometenwolke oder Öpik-Oort-Wolke bezeichnet, ist eine hypothetische und bisher nicht nachgewiesene Ansammlung astronomischer Objekte im äußersten Bereich des Sonnensystems.

Die Wolke wurde 1950 vom niederländischen Astronomen Jan Hendrik Oort als Ursprungsort der langperiodischen Kometen postuliert. Oort griff damit einen Vorschlag des estnischen Astronomen Ernst Öpik von 1932 auf.

Oort gründete seine Hypothese auf der Untersuchung von Kometenbahnen und auf der Überlegung, dass die Kometen nicht aus den bekannten Regionen des Sonnensystems stammen könnten, wie bis dahin angenommen wurde. Kometen werden im Verlauf von mehreren Passagen des Bereiches der Planeten durch den stärkeren Sonnenwind und die Ausbildung eines Kometenschweifs zerstört; nach den alten Voraussetzungen dürften sie daher heute nicht mehr vorkommen.

Der Theorie nach umschließt die von Oort angenommene „Wolke“ die übrigen Zonen des Sonnensystems kugelschalenförmig in einem Abstand zur Sonne bis 100.000 Astronomischen Einheiten (AE), was rund 1,6 Lichtjahren entspricht. Im Vergleich dazu ist der sonnenfernste Planet Neptun nur ca. 4,2 Lichtstunden (30 AE), der nächste Stern Proxima Centauri dagegen bereits 4,2 Lichtjahre von der Sonne entfernt. Die vom Sonnenwind maßgeblich durchströmte Heliosphäre hat einen geschätzten Radius von etwa 110 bis 150 AE. Schätzungen der Anzahl der Objekte liegen zwischen einhundert Milliarden und einer Billion. Vermutlich geht die Oortsche Wolke kontinuierlich in den Kuipergürtel (30 bis 50 AE) über, dessen Objekte allerdings gegen die Ekliptik konzentriert sind.

Die Oortsche Wolke besteht nach heutiger Auffassung aus Gesteins-, Staub- und Eiskörpern unterschiedlicher Größe, die bei der Entstehung des Sonnensystems und dem Zusammenschluss zu Planeten übriggeblieben sind. Diese sogenannten Planetesimale wurden von Jupiter und den anderen großen Planeten in die äußeren Bereiche des Sonnensystems geschleudert. Durch den gravitativen Einfluss benachbarter Sterne wurden die Bahnen der Objekte mit der Zeit so gestört, dass sie heute nahezu isotrop in einer Schale um die Sonne herum verteilt sind. Wegen der weit größeren Entfernung zu den Nachbarsternen sind die Objekte der Oortschen Wolke trotz ihres relativ großen Abstandes zur Sonne gravitativ an diese gebunden, also feste Bestandteile des Sonnensystems.

Durch den Einfluss der Gravitationsfelder der benachbarten Sterne sowie der galaktischen Gezeiten werden die Umlaufbahnen der Objekte der Oortschen Wolke gestört und einige von ihnen geraten ins innere Sonnensystem. Dort erscheinen sie dann als langperiodische Kometen, mit einer Periode von mehreren tausend Jahren. Kurzperiodische Kometen können sich nicht aus Kometen der Oortschen Wolke bilden, da eine hierfür benötigte Störung durch die großen Gasplaneten zu gering ist.

Die Oortsche Wolke ist nicht der einzige Ursprungsort von Kometen: Bei einer mittleren Periodenlänge können diese auch aus dem Kuipergürtel stammen.

Ein direkter Nachweis der Oortschen Wolke durch Beobachtung ist auch in naher Zukunft nicht zu erwarten, aber es gibt genügend indirekte Anzeichen, so dass ihre Existenz als nahezu sicher gilt.
Die Oortsche Wolke wird unterteilt in eine innere und eine äußere Oortsche Wolke, wobei die Grenze bei einer großen Halbachse der Umlaufbahn von 10.000 bis 20.000 AE angenommen wird. Grund für diese Unterteilung ist die Jupiter-Barriere, die verhindert, dass Kometen mit einer großen Halbachse kleiner als etwa 10.000 AE in das innere Sonnensystem gelangen können. Objekte der inneren Oortschen Wolke gelten daher als nicht beobachtbar. Langperiodische Kometen mit einer großen Halbachse kleiner als etwa 10.000 AE, die im inneren Sonnensystem beobachtet werden, stammen aus der äußeren Oortschen Wolke und haben wahrscheinlich schon mehrere Bahnumläufe durch das innere Sonnensystem erfahren, wobei ihre große Halbachse durch Bahnstörungen durch Planeten schrittweise reduziert wurde. Diese Kometen werden daher als „dynamisch alt“ bezeichnet, im Gegensatz zu „dynamisch neuen“ Kometen, die direkt aus der Oortschen Wolke stammen und das innere Sonnensystem zum ersten Mal erreichen.

Von den Entdeckern des extrem weit außen umlaufenden Planetoiden Sedna – mit einem Aphel bei etwa 926 AE – wurde eine Zugehörigkeit dieses Objekts zur inneren Oortschen Wolke vorgeschlagen, was aber bisher nicht allgemein akzeptiert ist.




</doc>
<doc id="3813" url="https://de.wikipedia.org/wiki?curid=3813" title="Oxide">
Oxide

Oxide (von griech. ὀξύς, oxýs = "scharf, spitz, sauer") sind Sauerstoff-Verbindungen eines Elements, in denen dieser die Oxidationszahl −II hat. Die meisten Oxide entstehen, wenn brennbare Stoffe mit Sauerstoff reagieren (Ursprung des Wortes Oxidation): Bei ihrer Oxidation geben sie Elektronen an das Oxidationsmittel Sauerstoff ab, so dass Oxide gebildet werden.

Grundsätzlich gilt, dass jede Verbindung eines Elementes mit Sauerstoff als Oxid bezeichnet wird. Eine Ausnahme bilden die Sauerstoff-Fluor-Verbindungen. Da der Sauerstoff in diesen eine positive Oxidationszahl besitzt und das Fluor die negative Oxidationszahl, heißen diese Verbindungen nicht "Fluoroxide", sondern "Sauerstofffluoride".

Je nach Bindungspartner unterscheidet man in der Chemie zwei Stoffgruppen von Oxiden:

Entsprechend ihrer stöchiometrischen Zusammensetzung unterscheidet man Monoxide, Dioxide, Trioxide, Tetroxide, Pentoxide, so bei Kohlenmonoxid, Chlordioxid und Schwefeltrioxid. Der überwiegende Teil der Erdkruste und des Erdmantels besteht aus Oxiden (vor allem aus Siliciumdioxid (Quarz) und hiervon abgeleiteten Salzen, den Silikaten sowie Aluminiumoxid). Auch Wasser gehört zur Stoffgruppe der Oxide. Ethylenoxid ist ein Beispiel für ein organisches Oxid.

Oxide werden hergestellt durch:

Das oben abgebildete schwarze Kupfer(II)-oxid kann also z. B. durch folgende Reaktionen synthetisiert werden:

Ferner ließe sich das rote Kupfer(I)-oxid durch Sauerstoff in schwarzes Kupfer(II)-oxid umwandeln. Auch beim Rösten sulfidischer Kupfererze wird Kupfer(II)-oxid hergestellt, indem man Kupfer(II)-sulfid an Luft oder im Sauerstoffstrom glüht (Nebenprodukt Schwefeldioxid).

Wie leicht ein Metall ein Oxid bildet, hängt von der Elektronegativität und Sauerstoffaffinität des Elementes ab. Je unedler ein Metall, desto heftiger kann es im Allgemeinen mit Sauerstoff reagieren und Oxide bilden. Daneben hängt die Reaktivität auch von der Passivierung eines Elementes ab, da bei vielen Elementen eine dicht haftende Oxidschicht die weitere Reaktion verhindert. Nur wenn diese sauerstoffdurchlässig ist oder entfernt wird, kann das Metall weiterreagieren.

Es gibt – eingeteilt nach ihrer Reaktion mit Wasser – saure, basische, amphotere und indifferente Oxide

Oxide edlerer Metalle werden zwecks Reaktion mit Wasser daher oft über einen Umweg als Salze in Hydroxide verwandelt: Kupfer(II)-oxid kann z. B. in konz. Salzsäure zu Kupfer(II)-chlorid gelöst werden. Dieses bildet mit Natronlauge Kupfer(II)-hydroxid, welches wie oben angegeben durch Erhitzen in Kupfer(II)-oxid umgewandelt werden kann. 

Hydroxide sind flockige Niederschläge, die oft charakteristische Färbungen aufweisen (Kupfer(II)-hydroxid hellblau, Nickel(II)-hydroxid apfelgrün, Chrom(III)-hydroxid graugrün, Mangan(II)-hydroxid rosa und an Luft infolge von Oxidation braun werdend, Cobalt(II)-hydroxid blau oder rosa, Eisen(III)-hydroxid rostbraun, Eisen(II)-hydroxid graugrün).

Das den Metalloxiden zugrunde liegende O-Ion entsteht bei der Redoxreaktion des Oxidationsmittels Sauerstoff mit einem Metall. Es ist nur in Schmelzen und in Kombination mit Kationen (in Form von Salzen) existent, nicht jedoch als freies Ion, denn es ist eine extrem starke Base und wird somit in wässriger Lösung quantitativ zum Hydroxid-Ion protoniert (Säure-Base-Reaktion). Metall-Hydroxide enthalten das OH-Ion und werden meistens aus Salzlösungen und Laugen gewonnen.

In Nichtmetalloxiden liegt in der Regel kein Oxid-Anion vor, da Nichtmetalle untereinander eine kovalente Bindung eingehen. Das dem Oxidion ähnelnde Peroxidion weist eine Oxidationszahl von −I statt −II auf, da hier zwei Sauerstoffatome miteinander verbunden sind. Nichtmetalloxide reagieren mit Wasser zu Säuren (mit Oxo-Anionen wie Sulfat, Carbonat usw.). Sie sind somit als Hydroxide von saurem Charakter anzusehen.

Sauerstoff ist ein starkes Oxidationsmittel und bildet mit fast allen Elementen isolierbare Oxide, mit Ausnahme der Edelgase Helium, Neon, Argon, Krypton und des Halogens Fluor (Fluor nimmt hierbei eine Sonderstellung ein, weil zwar die Sauerstoffverbindungen OF, OF und OF darstellbar sind, diese Stoffe aber wegen der höheren Elektronegativität des Fluors nicht als Fluoroxide, sondern als Sauerstofffluoride bezeichnet werden).

Sauerstoff bildet neben Oxiden auch Oxo-Anionen: Hier haben sich mehrere Sauerstoffatome an ein Atom gebunden, welches zumeist die höchstmögliche Oxidationszahl aufweist (Beispiele: Phosphat, Sulfat, Chromat, Permanganat, Nitrat, Carbonat). Sie entstehen in der Regel, wenn Nichtmetall- und Nebengruppenmetall-Oxide mit sehr hoher Oxidationszahl mit Wasser zu Säuren reagieren.

Zudem existieren Sauerstoff-Sauerstoff-Verbindungen wie z. B. im Bleichmittel Wasserstoffperoxid (s. o.). Anorganische Peroxide sind stark ätzend und oxidierend, organische Peroxide in der Regel explosiv.

Natürliche Metalloxide dienen als Erze zur Metallgewinnung. Ihnen wird durch Verhüttung – beispielsweise mittels Kohlenstoff (Hochofenprozess) – der Sauerstoff entzogen und so das reine Metall gewonnen.

Metalloxide wurden schon in der Steinzeit als Pigmente benutzt und auch Erdpigmente genannt.

Eine weitere Anwendung in der neueren Zeit ist die Verwendung als Isolator in der Informationstechnik.


Sauerstoffverbindungen mit Sauerstoff in anderen Oxidationsstufen sind:



</doc>
<doc id="3814" url="https://de.wikipedia.org/wiki?curid=3814" title="Oberpfalz">
Oberpfalz

Die Oberpfalz ist ein Regierungsbezirk und auch ein flächengleicher Bezirk im Nordosten des Bundeslandes Bayern und grenzt an Tschechien und an die bayerischen Regierungsbezirke Oberbayern, Niederbayern, Mittelfranken und Oberfranken.
Verwaltungssitz des Bezirks Oberpfalz und Sitz der Regierung der Oberpfalz ist Regensburg. Bis zum Jahr 1954 wurden die Regierungsbezirke Niederbayern und Oberpfalz gemeinsam verwaltet.

Der Regierungsbezirk Oberpfalz umfasst drei kreisfreie Städte und sieben Landkreise:

Kreisfreie Städte

Landkreise

Von 1939 bis 1945 gehörten zu Niederbayern-Oberpfalz auch Gebiete, die nach dem Münchner Abkommen von 1938 mit dem Sudetenland von der Tschechoslowakei abgetrennt worden waren. Es waren die drei Landkreise:
Außerdem wurden dem Landkreis Waldmünchen 1940/45 11 Gemeinden aus dem früheren tschechoslowakischen Staatsgebiet zugeteilt.

Vor der Landkreisreform am 1. Juli 1972 hatte der Regierungsbezirk Oberpfalz fünf kreisfreie Städte und neunzehn Landkreise.

Kreisfreie Städte:

Landkreise:

Die Statistik spiegelt ein gleichmäßiges Wachstum wider, das sollte aber nicht überbewertet werden. Lokal gab es teilweise gravierende Unterschiede. Momentan wachsen Stadt und Landkreis Regensburg dynamisch, während im Norden der Oberpfalz die Bevölkerungszahlen seit 20 Jahren rückläufig sind. Der Ausländeranteil liegt aktuell (2016) bei 7,52 % und damit deutlich unter dem bayerischen Durchschnitt. Einem Sterbeüberschuss von 2960 Personen stand 2015 ein Wanderungszugewinn von 12.138 Personen gegenüber.

Die gesamte Oberpfalz, ausgenommen die fränkische Sprach-Enklave Neustadt am Kulm, gehört zum bairischen Sprachraum. 

Im Norden und in der Mitte der Oberpfalz wird Nordbairisch gesprochen, im Süden der Oberpfalz, ab Waldmünchen und Burglengenfeld, beginnt ein breiter sprachlicher Mischraum zwischen Nord- und Mittelbairisch. Im Chamer Becken tendieren die Mundarten zum Niederbayerischen und im Regensburger Raum eher nach Oberbayern. Es gibt einen deutlichen Unterschied zwischen Sprechern aus Regensburg und Straubing, obwohl auch Straubing noch zu dieser Mischzone gehört.

Das Nordbairische ist eine urtümliche Variante des Bairischen, die noch viele Archaismen bewahrt, die im zentralen mittelbairischen Sprachraum schon ausgestorben sind. Es hat viele lautliche Eigenheiten, die es teilweise mit den benachbarten ostfränkischen Dialekten teilt. Das Nordbairische zeichnet sich besonders durch die „gestürzten Diphthonge“ (voraus lagen mhd. "uo, ië" und "üe") und die diphthongierten mittelhochdeutschen Langvokale "â, ô, ê" und "œ" aus; beispielsweise entsprechen den standarddeutschen Wörtern "Bruder, Brief" und "müde" (monophthongierte Vokale) hier "Brouda, Brejf" und "mejd" (zuerst Monophthongierung, danach erneute Diphthongierung) anstatt "Bruada, Briaf" und "miad" (erhaltene Diphthonge) wie im Mittelbairischen südlich der Donau. Weiterhin entspricht beispielsweise dem standarddeutschen "Schaf" hier "Schòuf" (mittelbair. "Schòòf"), "rot" hier "ròut/rout" (mittelbair. "rot/rout"), "Schnee" hier "Schnèj" (mittelbair. "Schnèè"), oder "böse" hier "bèjs" (mittelbair. "bèès"). 

Bei den Dialekten im Westen und im Nordwesten des nordbairischen Sprachraums ist charakteristisch auch eine Hebung der Vokale "e" (und "ö" nach Entrundung) und "o" zu "i" und "u" zu verzeichnen, beispielsweise "Vuugl" und "Viigl," im Gegensatz zu den südlicheren Formen "Voogl" und "Veegl" für standardsprachlich "Vogel" und "Vögel." Diese Hebung gilt auch als charakteristisches (ost-)fränkisches Merkmal. 

Im Nordosten des Sprachraums werden diese Laute zu den Diphthongen "ua" und "ia," also "Vuagl" und "Viagl." Verkleinerungs- und Koseformen enden in der Mehrzahl meist auf "-(a)la," in der Einzahl auf "-(a)l," beispielsweise "Moidl" = "Mädchen, d’ Moi(d)la" = "die Mädchen." Die Endung "-en" nach "k," "ch" und "f" ist in den nördlicheren nordbairischen Dialekten als Konsonant erhalten geblieben, beispielsweise "hockn, stechn, hoffn, Soifn" (= "Seife"). 

In den südlicheren nordbairischen Dialekten ist sie wie in den mittelbairischen weiter im Süden zu "-a" geworden, also "hocka, stecha, hoffa, Soifa." Kennzeichnend ist auch die Form "niad" für mittelbairisch "net" und die vielfältigen Formen des Personalpronomens für die 2. Person Plural: "enk, enks, ees, èts, deets, diits, diats" u. a.

Nach Einwohnerzahl gemessen sind die größten Städte in der Oberpfalz:

Die Oberpfalz ist eine Landschaft mit Mittelgebirgen und in den flacheren Regionen mit zahlreichen Weihern und Seen. Sie hat im Vergleich zu anderen Regionen in Deutschland eher ländlichen Charakter, ist dünner besiedelt und grenzt (im Uhrzeigersinn von Norden aus) an Oberfranken, Tschechien, Niederbayern, Oberbayern und Mittelfranken.

Bemerkenswerte Landschaften sind:

Im Regierungsbezirk gibt es 62 Naturschutzgebiete, 86 Landschaftsschutzgebiete, 95 FFH-Gebiete, 14 EU-Vogelschutzgebiete und mindestens 570 Geotope (Stand März 2017). Das größte Naturschutzgebiet im Bezirk ist die Regentalaue zwischen Cham und Pösing.

Siehe auch:

Historisch ist die Oberpfalz mit dem bayerischen Nordgau des 7. bis 14. Jahrhunderts identisch. Der Name des Regierungsbezirkes Oberpfalz steht im direkten Zusammenhang mit der Pfalz bzw. dem davon abgeleiteten Namen der Kurpfalz.

Nach dem Tod Ludwigs II. des Strengen teilte sich das Haus Wittelsbach 1329 (Hausvertrag von Pavia) in die ältere, pfälzische und die jüngere, bayerische Linie, wobei die Pfälzer Linie einen Teil der Gebiete in Nordbayern erhielt, die später als "Obere Pfalz gen Bayern" genannt wurden. Aus dieser Bezeichnung entstand Anfang des 19. Jahrhunderts der Name "Oberpfalz" im Zuge der Neuordnung des Königreiches Bayern. Die Oberpfalz wurde seit 1329 von Heidelberg aus regiert und wurde im 16. Jahrhundert protestantisch. Nach der Niederlage des Kurfürsten Friedrichs V. in der Schlacht am Weißen Berg bei Prag am 8. November 1620 wurde die Oberpfalz 1621 von Bayern besetzt, rekatholisiert und 1628 annektiert. Die Verlagerung der Handelsstraßen nach Prag und Nürnberg, die Verwüstungen des Dreißigjährigen Krieges sowie die Vertreibung der Protestanten, die nicht zur katholischen Kirche übertreten wollten, verursachten einen wirtschaftlichen Niedergang der Oberpfalz.

Von 1806 bis 1808 wurde das Königreich Bayern in 15 Kreise eingeteilt, deren Namen sich nach Flüssen richteten. Der Regenkreis umfasste zunächst 13 Landgerichte und seit 1809 die kreisunmittelbare Stadt Straubing. 1810 wurde er erheblich vergrößert, unter anderem durch das Fürstentum Regensburg. Danach wurde Regensburg Sitz des Generalkreiskommissariats. Der Regenkreis gab aber auch Gebiete an den Unterdonaukreis ab.

Bei der von König Ludwig I. veranlassten Gebietsreform vom 29. November 1837, bei der man sich auf die historischen Landesbezeichnungen besann, erfolgte die Umbenennung in "Kreis Oberpfalz und Regensburg" und die Erweiterung um Teile des Obermainkreises. Zum 1. April 1932 wurden die Regierungsbezirke "Niederbayern" und "Oberpfalz und Regensburg" zum Regierungsbezirk Niederbayern und Oberpfalz mit dem Sitz der Regierung in Regensburg zusammengelegt. 1939 wurde der Regierungsbezirk Niederbayern-Oberpfalz um bis zum Münchner Abkommen 1938 zur Tschechoslowakei gehörendes Gebiet, die Landkreise Bergreichenstein, Markt Eisenstein und Prachatitz, erweitert, das 1945 wieder abgetrennt wurde. Mit Inkrafttreten der Bayerischen Verfassung von 1946 (BV) wurden die Regierungsbezirke (Kreise) gemäß Art. 185 BV in der Form von vor 1932/33 wiederhergestellt. Der Zusatz "und Regensburg" für die Oberpfalz entfiel.

Der Regierungsbezirk Oberpfalz wird von folgenden Eisenbahnlinien durchquert:

Darüber hinaus gibt es eine Reihe von untergeordneten Strecken wie z. B. die Donautalbahn Regensburg – Ingolstadt

Über das Gebiet der Oberpfalz verlaufen folgende Fernstraßen:


Die Wirtschaft der Oberpfalz hat in den Jahren 1994 bis 2004 einen Wandel durchlebt. So nahmen die Erwerbstätigen in diesem Zeitraum im primären Sektor (Land- und Forstwirtschaft / Fischerei) ab, die Zahl der Erwerbstätigen im produzierenden Gewerbe sank ebenfalls. Die Erwerbstätigen im Dienstleistungssektor nahmen jedoch um 18,8 % zu. Da dies mit ca. 64 % Anteil an der Bruttowertschöpfung der bestimmende Sektor ist, nahmen die Erwerbstätigen damit insgesamt um 5,6 % zu. Heutzutage hat sich aufgrund der positiven wirtschaftlichen Entwicklung die Arbeitslosenquote auf 4,6 % zurückgebildet. Gemessen am BIP gehört die Oberpfalz zu den wohlhabenderen Regionen der EU mit einem Index von 130 (EU27: 100, Deutschland: 123) (2011). Die Wirtschaftskraft des Regierungsbezirkes variiert relativ stark zwischen der nördlichen und der südlichen Oberpfalz.

Die Wirtschaft ist insgesamt geprägt von klein- und mittelständischen Unternehmen, von denen einige zu den Führenden ihrer Branche zählen. Ebenso ist der Tourismus ein großer wirtschaftlicher Faktor. Die Landwirtschaft und die Teichwirtschaft, die vor allem in den nördlichen Regionen der Oberpfalz auftritt, haben gesamtwirtschaftlich gesehen eher eine kleine Rolle inne.
Industrielle Strukturen sind am stärksten im Großraum Regensburg vertreten, der in den vergangenen 25 Jahren eine beachtliche wirtschaftliche Dynamik entwickelt hat. Neben BMW, der Krones AG, Infineon und Continental produzieren eine Reihe weiterer Unternehmen in und um Regensburg. Regensburg ist nach München der zweitstärkste Biotechnologiestandort Bayerns (bundesweit Rang 5).

Auf Landkreis-Ebene ist der Kreis Schwandorf in der Oberpfalz an erster Stelle beim Steueraufkommen. Es gibt 300 Industriebetriebe mit ca. 16.000 Arbeitsplätzen. Die namhaftesten Unternehmen sind unter anderem die MEILLERGHP GmbH, die Benteler Automobiltechnik GmbH und die Nabaltec AG in Schwandorf, der Innovationspark Wackersdorf (BMW), die Läpple AG mit Werk in Teublitz, die F.EE GmbH in Neunburg v.W. und die Heidelberg Cement AG in Burglengenfeld.

Der intensive Bergbau in der Oberpfalz bewirkte einen starken wirtschaftlichen Aufschwung und machte die Region zu einem Zentrum der Eisenerzgewinnung und -verhüttung in Europa. Auch später noch bis in die 1980er Jahre wurden die Erzvorkommen der Region wirtschaftlich ausgebeutet. Bis etwa 1990 bzw. 2002 war die Eisen- und Stahlindustrie (Maxhütte mit den Standorten Sulzbach-Rosenberg und Maxhütte-Haidhof; ein verbliebener Schwerindustrierest ist die Luitpoldhütte in Amberg) in Verbindung mit bedeutenden Vorkommen von Eisenerz (in Auerbach in der Oberpfalz) und Braunkohle (in Wackersdorf) ein bestimmender Wirtschaftsfaktor.

Aufgrund ihrer Randlage am Eisernen Vorhang wurde die Oberpfalz seit den 1950er Jahren zu einem Stationierungsschwerpunkt der US-Armee und der neugegründeten Bundeswehr. Die Militärpräsenz ist seitdem ein wichtiger Wirtschaftsfaktor in dem strukturschwachen Raum, auch wenn das Ende des Kalten Krieges seit Anfang der 1990er Jahre eine deutliche Truppenreduzierung mit sich brachte.

Touristische Anziehungspunkte sind der Oberpfälzer Wald, das Stiftland und der Steinwald im Norden, die aus dem Tagebau hervorgegangene Seenlandschaft bei Schwandorf in der Mitte, die Jurahöhen im Westen sowie das untere Naabtal und die Bezirkshauptstadt Regensburg im Süden. Bei Nabburg betreibt der Bezirk das Oberpfälzer Freilandmuseum.

Im nationalen und internationalen Tourismus besser bekannt ist der Bayerische Wald, der sowohl in der Oberpfalz als auch im benachbarten Niederbayern gelegen ist. Dort ist eine alte Tradition der Glasbläserkunst vorhanden, die in Zwiesel, Bayerisch Eisenstein und angrenzenden Orten auch touristisch genutzt wird. Eine wichtige Rolle in Ostbayern spielen der Naturpark Oberpfälzer Wald, der Naturpark Oberer Bayerischer Wald und der Naturpark Bayerischer Wald. Die Region Oberpfalz ist als eine der preiswertesten deutschen Ferienregionen bekannt; die Preise von Gaststätten und für Beherbergungen sind auf vergleichsweise günstigem Niveau.

Die Marktgemeinde Plößberg zeichnet die hohe Dichte an industriellen Niederlassungen aus. Darunter zählen die Betriebe Ziegler Holzindustries KG, Erdenwerk Gregor Ziegler, Kartonagenwerk Liebenstein und Horn Glas Industries AG. Der Betrieb Ziegler Holzindustries KG, das Erdenwerk Ziegler und die Firma Horn Glas Industries AG wurden von 2004 bis 2010 durch den bayerischen Staatsminister Otto Wiesheu bzw. Martin Zeil zu den Bayerns best 50 Betrieben ausgezeichnet.



Der Bezirk Oberpfalz bildet gemeinsam mit den anderen bayerischen Bezirken die dritte kommunale Ebene des Bundeslandes. Die Kernaufgaben des Bezirks liegen im sozialen und kulturellen Bereich. Die Organe des Bezirks sind der Bezirkstag, der Bezirksausschuss und der Bezirkstagspräsident ().

Im amtierenden Bezirkstag (seit 2013) bilden Grüne, ÖDP und BP eine Ausschussgemeinschaft.

Seit 2008 ist Franz Löffler (CSU) Bezirkstagspräsident. Seine Stellvertreter sind Lothar Höher (CSU) und Norbert Hartl (SPD).

Blasonierung: „Gespalten durch eine aufsteigende und eingeschweifte rote Spitze, darin zwei schräg gekreuzte silberne Schlüssel, vorne in schwarz ein linksgewendeter rot bewehrter und rot gekrönter goldener Löwe, hinten silbern-blaue Rauten.“ Das Wappen gibt es seit 1960 (Musterentwurf vom 25. August 1960)

Wappenerklärung: Der steigende goldene Löwe ist der Pfälzer Löwe, das silbern-blaue (weiß-blaue) Rautenmuster die bayerischen Rauten. Die schräg gekreuzten silbernen Schlüssel in der roten eingeschweiften Spitze repräsentieren das Wappen der Stadt Regensburg. Es ersetzt den aufgelegten Reichsapfel als Zeichen der Kurwürde des Landesfürsten (als Erztruchsess) im ursprünglichen Siegel der Oberpfälzer Landstände des 16. Jahrhunderts, dem das Oberpfälzer Wappen entstammt. Damit wird auch an die ursprünglichen Bezeichnung des Regierungsbezirkes "Oberpfalz und Regensburg" sowie an die 1810 erfolgte Eingliederung der einstmaligen Reichsstadt Regensburg in das Königreich Bayern erinnert. Das Wappen symbolisiert damit auch die bis ins Mittelalter zurückreichende geschichtliche Tradition des aus dem kurpfälzischen Territorium in Bayern entstandenen kurbayerischen Fürstentums der „Oberen Pfalz“, denn nach der Belehnung des bayerischen Herzogs Ludwig I. des Kelheimers aus dem Hause Wittelsbach 1214 mit der Pfalzgrafschaft diente es über Jahrhunderte als gemeinsames heraldisches Symbol der altbayerischen und pfälzischen Wittelsbacher.


Der Regierungsbezirk Oberpfalz ist gebietsmäßig identisch mit dem Bezirk Oberpfalz. Er ist der Zuständigkeitsbereich der staatlichen Mittelbehörde "Regierung der Oberpfalz".






</doc>
<doc id="3815" url="https://de.wikipedia.org/wiki?curid=3815" title="Orang-Utans">
Orang-Utans

Die Orang-Utans ("Pongo") sind eine Primatengattung aus der Familie der Menschenaffen (Hominidae). Von den anderen Menschenaffen unterscheiden sie sich durch ihr rotbraunes Fell und durch ihren stärker an eine baumbewohnende Lebensweise angepassten Körperbau. Sie leben auf den südostasiatischen Inseln Sumatra und Borneo; die Bestände beider Inseln werden heute als drei getrennte Arten geführt: Borneo-Orang-Utan ("Pongo pygmaeus"), Sumatra-Orang-Utan ("Pongo abelii") und der neu entdeckte Tapanuli-Orang-Utan ("Pongo tapanuliensis").

Orang-Utans erreichen eine Kopf-Rumpf-Länge von 1,25 bis 1,5 Metern. Hinsichtlich des Gewichtes herrscht ein deutlicher Geschlechtsdimorphismus: Männchen sind mit 50 bis 90 Kilogramm nahezu doppelt so schwer wie Weibchen, die 30 bis 50 Kilogramm auf die Waage bringen. Tiere in Gefangenschaft neigen hingegen dazu, deutlich schwerer zu werden, Männchen können dabei ein Gewicht von nahezu 200 Kilogramm erreichen. Die Sumatra-Orang-Utans sind im Allgemeinen etwas leichter und zierlicher als ihre Verwandten auf Borneo. Das eher dünne und zottelig wirkende Fell der Orang-Utans ist dunkelrot oder rötlich braun gefärbt – bei den Tieren aus Sumatra meist etwas heller.

Die Gliedmaßen dieser Tiere zeigen starke Spezialisierungen an eine baumbewohnende Lebensweise. Die Arme sind sehr lang und kräftig und können eine Spannweite von 2,25 Metern erreichen. Die Hände sind hakenförmig und langgestreckt, der Daumen hingegen sehr kurz und nahe an der Handwurzel lokalisiert. Die vergleichsweise kurzen Beine sind sehr beweglich und nach innen einbiegbar, was dem senkrechten Klettern an Baumstämmen dient. Die Großzehe ist analog zum Daumen verkürzt und relativ nah an der Fußwurzel angebracht, die übrigen Zehen sind hingegen verlängert und gebogen. Insgesamt erwecken die Füße dadurch einen handähnlichen Eindruck.

Der Kopf der Orang-Utans ist durch den hohen, gerundeten Schädel und die vorspringende, gewölbte Schnauze charakterisiert. Im Gegensatz zu den afrikanischen Menschenaffen sind die Überaugenwülste nur schwach ausgeprägt und die Augen sind klein und stehen eng beisammen. Die Schädel der Männchen sind allerdings wie die der Gorillas mit Sagittal- und Nuchalkämmen (Wülsten an der Oberseite des Kopfes und am Nacken) ausgestattet, die als Muskelansatzstellen dienen. Beide Geschlechter tragen einen Bart, wobei der der sumatranischen Art länger ist. Männliche Tiere sind überdies mit einem Kehlsack ausgestattet, der bei der borneanischen Art besonders groß ist. Erwachsene Männchen haben auffällige Wangenwülste, diese wachsen das ganze Leben und sind bei alten Tieren am deutlichsten ausgeprägt. Bei Borneo-Orang-Utans wachsen diese Wülste nach außen und sind nahezu unbehaart, bei Sumatra-Orang-Utans liegen sie flach am Kopf und sind mit weißen Haaren bedeckt.

Wie alle Altweltaffen haben Orang-Utans 32 Zähne, die Zahnformel lautet I2-C1-P2-M3. Die mittleren Schneidezähne sind groß, die äußeren hingegen stiftförmig und klein. Die Eckzähne der Männchen sind deutlich größer als die der Weibchen; die Backenzähne sind durch niedrige Zahnhöcker und eine stark gekräuselte Kaufläche charakterisiert, was eine Anpassung an die oft hartschalige Nahrung darstellt.

Noch vor einer Million Jahren kamen die Orang-Utans in ausgedehnten Gebieten Südostasiens vor. Ihr ursprüngliches Verbreitungsgebiet reichte vom südlichen China über Thailand, Vietnam bis nach Java, was durch Fossilienfunde in Südchina, Vietnam und der Insel Java belegt ist. In Teilen dieses Gebietes dürften sie zumindest bis vor wenigen tausend Jahren überlebt haben.

Orang-Utans kommen heute nur mehr auf den Inseln Borneo und Sumatra vor. Auf Sumatra bewohnen sie die nordwestlichen Regionen und Teile der Westküste, auf Borneo sind sie vorwiegend in den südlichen und östlichen Regionen anzutreffen.

Lebensraum der Orang-Utans sind tropische Regenwälder vom Meeresniveau bis in 1500 Metern über dem Meeresspiegel. Sie sind oft in Sumpfgebieten oder in der Nähe von Flüssen zu finden, einen weiteren bedeutenden Lebensraum stellen Dipterocarpaceen-Wälder dar.

Wie alle Menschenaffen sind Orang-Utans tagaktiv. Sie haben zwei Aktivitätshöhepunkte, einmal am Vormittag und einmal am späten Nachmittag, in der Mittagszeit halten sie Rast. Zur Nachtruhe errichten sie sich ein Nest aus Ästen und Blättern. In der Regel bauen sie jede Nacht ein neues Nest, gelegentlich wird dasselbe zweimal verwendet.

Sie sind vorwiegend Baumbewohner. Dort bewegen sie sich hauptsächlich fort, indem sie langsam mit allen vier Gliedmaßen klettern oder auf den Ästen gehen – ihre Bewegungen sind aber gemächlicher als beispielsweise die der Gibbons. Insbesondere wenn sie es eilig haben, schwingen sie an den langen Armen (Brachiation). Um von einem Baum auf den anderen zu gelangen, können sie, um die Distanz zu verringern, diese in heftige Schaukelbewegungen versetzen.

Orang-Utans kommen selten auf den Boden. Oft geschieht dies nur, um von einem Baum zum nächsten zu kommen, wobei ihre Bewegungen vorsichtig und scheu sind. Erwachsene Männchen können hingegen manchmal sogar Streifzüge auf dem Boden unternehmen. Dieses Verhalten ist bei der borneanischen Art häufiger, vermutlich weil es dort im Gegensatz zu Sumatra keine Tiger gibt. Ihre Fortbewegung auf der Erdoberfläche ist ein vierfüßiges Gehen; im Gegensatz zu den afrikanischen Menschenaffen (Schimpansen und Gorillas) bewegen sie sich nicht im Knöchelgang fort, sondern stützen sich entweder auf die Fäuste oder auf die Innenkanten der Hände.

Orang-Utans haben mehrere Revierstrategien und können als „ansässige Tiere“, „Pendler“ und „Wanderer“ bezeichnet werden.

„Ansässige Tiere“ bewohnen feste Territorien. Bei Weibchen umfassen diese rund 70 bis 900 Hektar und können sich mit den Revieren anderer Weibchen überschneiden. Die Territorien der Männchen sind mit 4000 bis 5000 Hektar deutlich größer und überlappen sich meist mit denen mehrerer Weibchen. Die Länge der täglichen Streifzüge hängt mit der Reviergröße zusammen; sie dienen aber nicht nur der Nahrungsaufnahme, sondern bei Männchen auch dem Kontakt mit den Weibchen oder der Suche nach etwaigen männlichen Konkurrenten.

Die Mehrzahl der männlichen Orang-Utans etabliert jedoch kein festes Territorium, sondern zieht als „Pendler“ oder „Wanderer“ umher. „Pendler“ halten sich nur für einige Wochen oder Monate in einem Gebiet auf und wechseln mehrmals im Jahr ihren Aufenthaltsort. Oft sind sie im darauffolgenden Jahr wieder in den gleichen Gebieten anzutreffen. Die Aufenthaltsorte der „Pendler“ können mehrere Kilometer auseinander liegen, die Streifzüge dieser Tiere sind dementsprechend deutlich länger als die der ansässigen Orang-Utans. Junge erwachsene Männchen sind meist „Wanderer“, sie sind nie lange in einem Gebiet anzutreffen, sondern ziehen beständig umher. Wenn sie älter werden, können sie manchmal ein festes Revier etablieren oder aber zeitlebens Wanderer bleiben.

Orang-Utans sind in der Regel einzeln anzutreffen, dauerhafte Bindungen gibt es nur zwischen Weibchen und Jungtieren. Dennoch interagieren diese Tiere mit Artgenossen und führen keine strikt einzelgängerische Lebensweise, die Details dieser sozialen Beziehungen sind aber noch nicht restlos bekannt.

Begegnungen zwischen Männchen verlaufen meist feindselig. Durch Rufe machen sie auf sich aufmerksam, bei direkten Begegnungen kann es auch zu Handgreiflichkeiten kommen. Weibchen reagieren hingegen friedlicher aufeinander, manchmal gehen sie zu zweit für mehrere Tage gemeinsam auf Nahrungssuche. Generell sind Sumatra-Orang-Utans sozialer als Borneo-Orang-Utans, es gibt für diese Art Beobachtungen von größeren Gruppenbildungen und auch zeitweiligen Zusammenschlüssen eines Männchens mit einem Weibchen und deren Jungtieren.

Fix ansässige Tiere dürften einen höheren sozialen Rang als herumziehende haben, was unter anderem an den unterschiedlichen Fortpflanzungsstrategien deutlich wird. Niederrangigere, umherziehende Männchen erzwingen oft die Fortpflanzungen mit Weibchen. Opfer dieser erzwungenen Kopulationen, manchmal anthropomorph als „Vergewaltigung“ bezeichnet, werden ihrerseits zumeist junge oder rangniedrige Weibchen. Ansässige Männchen hingegen überwachen bei ihren Streifzügen die in ihrem Revier lebenden Weibchen, um sie vor erzwungenen Kopulationen zu schützen. Aufgrund des höheren Ranges dieser Männchen dürften die Weibchen der Paarung zustimmen.

Orang-Utans sind ruhiger als andere Menschenaffen. Auffälligster Laut sind die lauten Schreie der Männchen. Diese dienen dazu, andere Männchen auf ihr Revier hinzuweisen und den Kontakt zu Weibchen herzustellen. Bedingt durch den größeren Kehlsack sind die Schreie der Borneo-Orang-Utans lauter und langgezogener als die der Sumatra-Orang-Utans. Über andere lautliche Äußerungen oder Kommunikationen mittels Mimik und Körperhaltungen ist wenig bekannt.

Der Gebrauch von Werkzeugen kommt bei Orang-Utans in freier Wildbahn seltener vor als beispielsweise bei Schimpansen. Man hat aber Tiere dabei beobachtet, wie sie Holzstöcke dazu verwendet haben, um damit zu graben, zu kämpfen oder sich zu kratzen. Um an die schmackhaften Samen von Neesia-Früchten zu gelangen, die in eine Fruchtschale mit stechenden Haaren eingebettet sind, stellen Orang-Utans aus dünnen Zweigen passende Holzstäbchen her. Vor Regen und praller Sonne schützen sie sich mit großen Blättern, die sie über ihren Kopf halten.

Der vergleichsweise geringe Werkzeuggebrauch könnte auch an der eher einzelgängerischen Lebensweise dieser Tiere liegen, was die Bedingungen für die Weitergabe erworbenen Verhaltens erschwert. Das stimmt auch mit Beobachtungen überein, wonach der Werkzeuggebrauch bei den sozialeren Sumatra-Orang-Utans weitaus häufiger ist als bei den Borneo-Orang-Utans.

Der bedeutendste natürliche Feind der Sumatra-Orang-Utans ist der Sumatra-Tiger. Der auf Sumatra und Borneo lebende Nebelparder wird heranwachsenden Tieren und Weibchen gefährlich, kann aber ausgewachsene Männchen in der Regel nicht erlegen. Weitere Bedrohungen stellen manchmal Krokodile und verwilderte Haushunde dar.

Orang-Utans sind hauptsächlich Pflanzenfresser. Mit rund 60 % stellen Früchte den größten Bestandteil ihrer Nahrung dar, dabei nehmen sie oft Früchte mit harten Schalen oder Samen zu sich. Außerdem verzehren sie auch Blätter, junge Triebe und Rinde. Tierische Nahrung spielt nur eine untergeordnete Rolle. Gelegentlich verzehren sie aber Insekten, Vogeleier und kleine Wirbeltiere. Die sumatrische Art scheint einen etwas größeren tierischen Anteil in ihrer Nahrung zu haben als die Borneos. Durch die Verbreitung der Samen der gegessenen Früchte spielen sie eine Rolle für die Vermehrung mancher Pflanzen.

Bei der Nahrungsaufnahme sitzen sie entweder oder hängen an den Ästen, wobei ihr Körpergewicht diese nach unten biegt, was es ihnen erleichtert, an Früchte oder Blätter zu gelangen. Ihre kräftigen Arme erlauben es ihnen, auch dickere fruchttragende Äste umzubiegen oder manchmal sogar abzubrechen.

Während der alle 2 bis 10 Jahre vorkommenden Mastjahre in Dipterocarpaceen-Wäldern können sie weit mehr Nahrung als üblich zu sich nehmen. Diese wird als Fettvorrat für Zeiten des Nahrungsmangels angelegt. Diese Veranlagung dürfte ein Grund dafür sein, warum Orang-Utans in Gefangenschaft zur Verfettung neigen.

Orang-Utans haben keine feste Paarungszeit, die Fortpflanzung kann das ganze Jahr über erfolgen. Allerdings kann sie vom Nahrungsangebot abhängen, sodass mehrere Weibchen einer Region ihre Jungen nahezu gleichzeitig zur Welt bringen. Die Länge des Sexualzyklus beträgt rund 28 Tage, der Östrus dauert rund 5 bis 6 Tage, die Weibchen zeigen keine Regelschwellung.

Wie oben erwähnt, gibt es zwei Fortpflanzungsstrategien, die erzwungene Kopulation durch umherziehende Männchen und die freiwillige Paarung mit ansässigen Männchen. In einer Untersuchung sorgte jede der beiden Strategien für rund die Hälfte des Nachwuchses. Nach einer rund acht- bis neunmonatigen Tragzeit (durchschnittlich 245 Tage) bringt das Weibchen in der Regel ein einzelnes Jungtier zur Welt, Zwillinge sind selten. Neugeborene wiegen rund 1,5 bis 2 Kilogramm. Das Geburtsintervall beträgt vier bis acht Jahre und ist somit das längste aller Menschenaffen.

Die Aufzucht der Jungtiere obliegt allein dem Weibchen, das Männchen beteiligt sich nicht daran. In den ersten Lebensmonaten klammert sich das Neugeborene am Bauch der Mutter fest und bis zum Alter von zwei Jahren wird es bei den Streifzügen getragen, von ihr mit Nahrung versorgt und schläft im gleichen Nest. Im Altersabschnitt von zwei bis fünf Jahren beginnt das Jungtier seine Kletterfähigkeiten zu entwickeln, es beginnt seine Umgebung zu erkunden, ohne allerdings den Sichtkontakt zu verlieren, und es lernt den Nestbau. Im gleichen Zeitraum – mit rund 3,5 bis 4 Jahren – wird es entwöhnt.

Im Alter von fünf bis acht Jahren setzt die zunehmende Trennung von der Mutter ein. Zwar haben sie noch häufigen Kontakt mit ihr, suchen aber in dieser Phase häufig den Kontakt zu Gleichaltrigen und bilden mit ihnen Zusammenschlüsse. In dieser Zeit kann es dazu kommen, dass ein Weibchen zwei Kinder um sich hat, ein heranwachsendes und ein neugeborenes.

Weibchen erreichen die Geschlechtsreife mit rund sieben Jahren, bei Männchen dürfte das variabler sein und mit acht bis 15 Jahren eintreten. Nach der endgültigen Trennung von der Mutter versuchen die Weibchen, ein eigenes Territorium zu etablieren, oft nahe dem Revier der Mutter. Es dauert allerdings noch einige Jahre, bevor sie sich das erste Mal fortpflanzen, meist erst ab dem 12. Lebensjahr.

Männchen durchleben nach dem Eintreten der Geschlechtsreife meist eine längere Periode als „Wanderer“. In dieser Zeit sind sie zwar zeugungsfähig (und erzwingen Kopulationen), unterscheiden sich aber äußerlich noch kaum von Weibchen. Die typischen sekundären Geschlechtsmerkmale wie Wangenwülste und Kehlsäcke erscheinen erst viel später, ungefähr zwischen dem 15. und 20. Lebensjahr. Das Auftreten dieser Merkmale hängt oft mit der Etablierung eines eigenen Reviers oder mit der Abwesenheit anderer Männchen zusammen. Gelingt es ihnen, ein eigenes Territorium zu errichten, bilden sich diese Merkmale schnell, oft innerhalb weniger Monate.

Weibchen bringen aufgrund ihrer langsamen Fortpflanzungsrate oft nur zwei bis drei Jungtiere in ihrem Leben zur Welt, die Menopause bei Tieren in Gefangenschaft tritt mit rund 48 Jahren ein. Die Lebenserwartung in freier Wildbahn wird auf bis zu 50 Jahre geschätzt. Tiere in menschlicher Obhut werden älter und können ein Alter von 60 Jahren erreichen.

Die Bezeichnung „Orang-Utan“ stammt von den malaiischen Wörtern „orang“ (Mensch) und „utan“ oder „hutan“ (Wald) und bedeutet demzufolge „Waldmensch“. In europäischen Sprachen erschien dieser Name erstmals 1631. Laut Brehms Tierleben behaupten „die Javaner […], dass die Affen wohl reden könnten, wenn sie nur wollten, es jedoch nicht täten weil sie fürchteten, arbeiten zu müssen.“ In lokalen Sprachen der Region sind auch die Bezeichnungen "maias" oder "mawas" gebräuchlich.

Die Orang-Utans Sumatras wurden im 19. Jahrhundert zunächst als eigene Art beschrieben, später setzte sich die bis zum Ende des 20. Jahrhunderts gültige Systematik durch, die die Populationen beider Inseln als Unterarten einer gemeinsamen Art betrachtete. Die Unterschiede im Körperbau und im Verhalten, kombiniert mit molekularen Studien, führten dazu, dass sie heute als zwei getrennte Arten geführt werden.

Die wissenschaftliche Bezeichnung der Gattung "Pongo" geht auf den englischen Seefahrer Andrew Battell (um 1565–1614) zurück. In seinem „Bericht über Angola und die angrenzenden Regionen“ (1613) beschrieb er zwei „Monster“ (vermutlich Gorilla und Schimpanse), deren größeres die Einheimischen „Pongo“ und deren kleineres sie „Engeco“ nannten.

Battells Reisebeschreibung wurde häufig zitiert und vielfach nachgedruckt, mit der Folge, dass "Pongo" (vergl.: M'Pungu) Jahrzehnte lang als übergeordnete Bezeichnung für alle damals von Europäern „entdeckten“ großen Menschenaffen verwendet wurde. So heißt es beispielsweise noch 1802 bei Immanuel Kant in dessen "Physischer Geographie":

Ähnlich art-übergreifend formulierte es 1819 Arthur Schopenhauer, als er sich über die Intelligenz der Menschenaffen Gedanken macht:

Andererseits bezeichnete Jean-Baptiste de Lamarck 1809 umgekehrt die afrikanischen Schimpansen und die asiatischen großen Menschaffen als "Orangs":

Die nomenklatorische Verwirrung innerhalb der Menschenaffen wurde erst im Laufe des 19. Jahrhunderts bereinigt; sie ist vor allem darauf zurückzuführen, dass zuvor kaum einer der frühen europäischen Naturforscher einen lebenden Menschenaffen zu Gesicht bekommen hatte und kein einziger Vergleiche an lebenden Exemplaren von Orang-Utans, Gorillas und Schimpansen hatte vornehmen können.

Der Orang-Utan war Pate bei der Vergabe des Tivialsnamens der mit orange-braunroten „Haaren“ versehenen Krabbe "Achaeus japonicus", die in mehreren europäischen Sprachen als Orang-Utan-Krabbe bezeichnet wird.

Erst ab der zweiten Hälfte des 20. Jahrhunderts wurde mit Feldstudien begonnen, um das Verhalten dieser Tiere in freier Wildbahn zu untersuchen. Bekannteste Forscherin in diesem Kontext ist Birutė Galdikas.

Wie bei den anderen Menschenaffen werden auch bei Orang-Utans Laboruntersuchungen durchgeführt, um die Intelligenz und die Kommunikationsfähigkeit dieser Tiere zu erforschen. Bekannt wurde hier der 20 Jahre alte Orang-Utan Chantek im Zoo von Atlanta (US-Staat Georgia). Im Gegensatz zur freien Natur, wo nur selten Werkzeuggebrauch vorkommt, lässt sich bei Tieren in Gefangenschaft häufig die Verwendung von Werkzeugen beobachten. Sie schaffen es auch, knifflige Problemstellungen zu lösen, etwa eine mit Schnallen verschlossene Schachtel zu öffnen, in der sich eine reife Frucht befindet. Ihre Intelligenz, Geschicklichkeit und Kraft befähigt sie außerdem dazu, nachlässig konstruierte Sicherheitsmechanismen in Zoos zu überwinden und aus Gehegen auszubrechen.

Im Rahmen der Erforschung der Kommunikationsfähigkeit wurde Orang-Utans beigebracht, mit Hilfe einer Symbolsprache zu kommunizieren.

Das Verbreitungsgebiet der Orang-Utans ist seit dem Pleistozän stark zurückgegangen. Heute sind beide Arten stark bedroht. Die Gründe dafür liegen in erster Linie in der Zerstörung ihres Lebensraumes, daneben in der Bejagung und im Handel – insbesondere mit Jungtieren. Verschärft werden diese Faktoren durch die langsame Reproduktionsrate der Tiere.

Hauptbedrohung stellt heute die Zerstörung ihres Lebensraumes dar. In großem Ausmaß werden Wälder gerodet, einerseits zur Holzgewinnung, andererseits zur Errichtung landwirtschaftlich genutzter Flächen. Neuerdings gefährdet die starke Nachfrage nach Palmöl zunehmend die Habitate der Orang-Utans. Malaysia und Indonesien, die beiden Länder, in denen Orang-Utans leben, zählen zu den Hauptproduzenten dieses Produktes.

Die Bejagung stellt einen weiteren Faktor dar. In manchen Gegenden – etwa im Inneren Borneos – wird ihr Fleisch gegessen. Darüber hinaus werden sie mancherorts gezielt verfolgt, wenn sie auf der Nahrungssuche in Obstplantagen eindringen. Ihre Größe und ihre eher gemächlichen Bewegungen machen sie zu einem leichten Ziel für Jäger. Hinzu kommt, dass Jungtiere gefangen und als Haustiere verkauft werden, was meist mit der Tötung der Mutter einhergeht. In den 1990er Jahren wurden Orang-Utans nach Taiwan geschmuggelt, vielleicht unter Einfluss einer Fernsehshow, in der ein Orang-Utan als „ideales Haustier“ auftrat. Einer Schätzung aus dem Jahr 2002 zufolge werden wöchentlich zwei Tiere aus Borneo herausgeschmuggelt. Da Orang-Utans im Washingtoner Artenschutzübereinkommen (CITES) gelistet sind, sind solche Praktiken illegal.

Darüber hinaus sind diese Tiere durch die Übertragung von Krankheiten gefährdet. Durch ihre enge Verwandtschaft mit dem Menschen können sie etwa an Hepatitis, Cholera, Malaria und Tuberkulose erkranken, die beispielsweise durch die zahlreichen Kontakte in Nationalparks mit Wildhütern und Touristen übertragen werden.

Sowohl auf Sumatra als auch im malaysischen und indonesischen Teil Borneos gibt es Schutzgebiete und Nationalparks für die bedrohte Fauna der Region. Es wurden auch einige Auswilderungsstationen gegründet, in denen beschlagnahmte Jungtiere wieder auf ein Leben in freier Wildbahn vorbereitet werden sollen.

Auf Sumatra kommen wildlebende Orang-Utans nur noch in den Wäldern der beiden nördlichen Provinzen Aceh und Nordsumatra vor, viele davon im Nationalpark Gunung Leuser. Orang-Utans stehen seit mehr als 60 Jahren unter Schutz. Das indonesische Gesetz verbietet es, sie zu töten, zu fangen, zu halten oder mit ihnen zu handeln. Trotzdem landen jedes Jahr zahlreiche Tiere auf dem Schwarzmarkt und in Privathaushalten.

Die Stiftung PanEco setzt sich mit dem Sumatra Orang-Utan Schutzprogramm SOCP seit 1999 für das Überleben der Orang-Utans auf Sumatra ein. Zusammen mit der indonesischen Naturschutzbehörde PHKA, der Stiftung Yayasan Ekosistem Lestari YEL (Stiftung für ein nachhaltiges Ökosystem) und der Zoologischen Gesellschaft Frankfurt werden illegal in Gefangenschaft gehaltene Orang-Utans konfisziert und wieder in ihren natürlichen Lebensraum ausgewildert. Eine Auswilderungsstation befindet sich im Nationalpark Bukit Tigapuluh in der Provinz Jambi. Hier gibt es seit dem 19. Jahrhundert keine Orang-Utans mehr, obwohl der Wald als Lebensraum für diese Tiere geeignet ist. Die Wiederansiedlung ist hier möglich, denn laut IUCN-Richtlinien dürfen keine Tiere in Gebieten freigelassen werden, in denen noch wildlebende Populationen vorkommen. Aus diesem Grunde musste das in den 1970er Jahren gegründete Rehabilitationszentrum Bohorok in Nordsumatra geschlossen werden, welches drei Jahrzehnte lang konfiszierte Orang-Utans in den Gunung Leuser Nationalpark auswilderte. Anfang 2011 konnte die Stiftung PanEco eine zweite Auswilderungsstation im Naturreservat Jantho in der nördlichsten Provinz Aceh eröffnen. Hier werden alle in Aceh konfiszierten Tiere wieder ausgewildert. Neben der Wiederansiedlung von Orang-Utans gehören auch das Monitoring von Orang-Utan Populationen, die Erforschung des Verhaltens wildlebender Orang-Utans, Umweltbildung und Öffentlichkeitsarbeit zu den Kernaufgaben der Stiftung PanEco auf Sumatra. Ihr Hauptziel ist, den tropischen Regenwald zu schützen und zu erhalten.

Auf Borneo unterhält die Borneo Orangutan Survival Foundation (BOS) zwei Rehabilitationsprojekte, beide im indonesischen Teil der Insel: Nyaru Menteng sowie Samboja Lestari. Neben anderen werden diese Projekte in Zusammenarbeit mit der indonesischen BOS Foundation auch von Borneo Orangutan Survival Deutschland e.V. gefördert.

Weitere Schutzgebiete befinden sich unter anderem im Nationalpark Gunung Palung, im Nationalpark Tanjung Puting und im Nationalpark Kutai. Der größtenteils zu Malaysia gehörige Norden Borneos führt zwei Aufzuchts- und Auswilderungszentren: das Sepilok Orangutan Rehabilitation Centre bei Sandakan im Bundesstaat Sabah sowie das kleinere, außerhalb Kuchings gelegene Semenggoh-Reservat in Sarawak. Ein weiteres wichtiges Schutzgebiet für Borneo-Orang-Utans in Malaysia ist die Danum-Valley-Conservation-Area, die ebenfalls in Sabah liegt. In den 1990er-Jahren wurden von der Borneo Orangutan Survival Foundation 350 Orang-Utans in das Schutzgebiet des Meratus ausgewildert. Dort betreibt die ALT Foundation mit Unterstützung der Borneo Orang-Utan-Hilfe ein gemeinsames Schutzprojekt.

Ein Austausch findet zudem zwischen Sarawak und der malaysischen Halbinsel statt: auf der Aufzuchts- und Auswilderungsinsel Pulau Orang Utan, Teil der bei Taiping (Perak) zu findenden Erholungsanlage Bukit Merah.

Schätzungen über die Gesamtpopulation der Orang-Utans sind schwierig. Transekt-Untersuchungen auf Sumatra im Jahre 2015 ergaben als Hochrechnung eine Population von 14.613 Sumatra-Orang-Utans, deutlich mehr als in alten Schätzungen. Bei den Borneo-Orang-Utans wird die Population auf 54.000 geschätzt. Eine genauere Hochrechnung der Zahl der Borneo-Orang-Utans wird für das Jahr 2016 erwartet.

Die IUCN listet die Art auf Sumatra als „vom Aussterben bedroht“ ("critically endangered") und die Art auf Borneo als stark gefährdet ("endangered").

Orang-Utans bilden zusammen mit den Gorillas, den Schimpansen (Gemeiner Schimpanse und Bonobo) sowie dem Menschen die Familie der Menschenaffen (Hominidae). Sie bilden dabei die Schwestergruppe der übrigen Arten und werden in einer eigenen Unterfamilie, Ponginae, geführt, die den Homininae gegenübersteht. Das wird auch geographisch deutlich, da die übrigen Menschenaffen in Afrika leben beziehungsweise von dort stammen. Das kommt im Kladogramm (rechts) zum Ausdruck.

Es gibt einige ausgestorbene Primaten, die heute in den Formenkreis des Tribus Pongini gestellt und somit als Verwandte der Orang-Utan-Vorfahren interpretiert werden. Hierzu gehören unter anderem "Sivapithecus" / "Ramapithecus", "Khoratpithecus", "Ankarapithecus", "Lufengpithecus" und vermutlich auch "Gigantopithecus".

Traditionell wurden beide auf getrennten Inseln lebende Populationen als Unterarten einer gemeinsamen Art betrachtet. Genetische Untersuchungen Ende des 20. Jahrhunderts sprachen aber für eine Aufteilung in zwei Arten, was in der Abspaltung des Sumatra-Orang-Utan ("Pongo abelii") vom Borneo-Orang-Utan ("Pongo pygmaeus") im Jahr 2001 resultierte. Beide sind heute als eigenständige Arten anerkannt. Diese Aufteilung kann heute neben den genetischen Daten auch mit Unterschieden im Körperbau und der Lebensweise begründet werden. Die borneanische Art wird in zwei oder drei Unterarten, "Pongo pygmaeus pygmaeus", "Pongo pygmaeus wurmbii" und manchmal zusätzlich "Pongo pygmaeus morio" unterteilt, die sich hinsichtlich des Schädelbaus unterscheiden. Weitere genetische Untersuchungen an den Orang-Utans von Sumatra konnten diese aber nicht als monophyletische Gruppe darstellen, da eine Population südlich des Tobasees näher mit dem Borneo-Orang-Utan verwandt ist. Dies führte im Jahr 2017 zur Beschreibung des Tapanuli-Orang-Utans ("Pongo tapanuliensis").

Die ausgestorbenen Orang-Utans aus verschiedenen Regionen Südostasiens, die zum Teil deutlich größer waren als die heutigen Tiere, wurden als verschiedene Arten beschrieben, etwa "Pongo palaeosumatrensis" (Sumatra), "Pongo weidenreichi" (südliches China und Vietnam) und "Pongo hooijeri" (ebenfalls Vietnam). Ihr taxonomischer Status und ihr Verhältnis zu den heutigen Arten ist umstritten.

Im Januar 2011 gab ein Team von Wissenschaftlern bekannt, dass das komplette Genom des Orang-Utans sequenziert worden sei.

Die Entwicklung der Gattung Orang-Utan ("Pongo") erfolgte in Asien (während sich "Homo" primär in Afrika entwickelte). Nach molekulargenetischen Untersuchungen spaltete sich zuerst der Sumatra-Orang-Utan ("Pongo abelii") von der Linie der ursprünglichen Orang-Utans ab, dies erfolgte im Pliozän vor rund 3,4 Millionen Jahren. Er ist heute nördlich des Tobasees auf Sumatra verbreitet. Die zweite Linie umfasst den Tapanuli-Orang-Utan ("Pongo tapanuliensis"), der südlich des Tobasees vorkommt, und den Borneo-Orang-Utan ("Pongo pygmaeus") von der Insel Borneo. Beide trennten sich erst im Mittelpleistozän vor 674.000 Jahren voneinander ab.

Aufgrund fortschreitender Trockenheit, wiederkehrender Vergletscherungen und Veränderung der Monsunaktivität verschob sich der Lebensraum des tropischen Regenwaldes auf dem asiatischen Kontinent seit dem späten Miozän und im Pleistozän in Richtung Äquator. Im Pleistozän war die Gattung noch vor etwa 60.000 Jahren auch in Malaysia, vor 40.000 Jahren noch im Raum Südchina bis Java verbreitet. Die Folgen des klimabedingten Rückzuges sind noch heute an den genetischen Verteilungsmustern des Borneo-Orang-Utan ("Pongo pygmaeus") auf dem pleistozänen Refugium Sumatra-Borneo ablesbar. Allerdings wird die Auffassung vom Refugialraum Borneo in Frage gestellt.




</doc>
<doc id="3817" url="https://de.wikipedia.org/wiki?curid=3817" title="Oviparie">
Oviparie

Als ovipar (lateinisch "oviparus" ‚eigeboren‘) bezeichnet man Tiere, die Eier legen. Der Oviparie steht die Viviparie gegenüber. Die Vertreter beider Fortpflanzungsformen stellen keine taxonomischen Gruppen (Taxon) dar, sondern werden lediglich über das Merkmal definiert.

Bei der echten Oviparie handelt es sich um eine Fortpflanzungsform, bei der befruchtete Eier abgelegt werden. Damit diese zustande kommen, ist eine innere Befruchtung durch Begattung oder durch die Aufnahme einer Spermatophore nötig. Der Embryo wird während seiner gesamten Embryogenese ("Embryonalentwicklung") vom im Ei gespeicherten Dotter ernährt. Hat das Jungtier nach der Eiablage eine bestimmte Größe und damit ein bestimmtes Entwicklungsstadium erreicht, schlüpft es aus.

Ovipar sind die Vögel, die meisten Reptilien, inklusive der Dino- und anderer Saurier, sowie der überwiegende Teil der Schwanzlurche, der Gliederfüßer und der Würmer. Die einzigen oviparen Säugetiere sind die Kloakentiere, zu denen das Schnabeltier und die vier Arten der Ameisenigel gehören.

Der Begriff der Ovipaire ist eng mit dem Taxon der Amniota verknüpft, deren Eigenschaft es ist, sich ohne freies Larvalstadium unabhängig von Gewässern fortpflanzen zu können.

Ovuliparie (von Ovulation – als Entstehung unbefruchteter Eizellen) liegt vor, wenn unbefruchtete Eier abgelegt werden, die erst außerhalb des Körpers der Mutter befruchtet werden, also durch äußere Befruchtung. Ovulipar sind die meisten Knochenfische und der größte Teil der Froschlurche.

Abzugrenzen ist die Oviparie von der Viviparie. Tiere, bei denen der Embryo im Mutterleib heranwächst und die in der Regel nicht außerhalb des Mutterkörpers „schlüpfen“, sind vivipar oder lebendgebärend. Echte Viviparie besteht nur, wenn die Versorgung des Embryos über den Stoffwechsel der Mutter erfolgt, häufig über eine Plazenta. Hier wird dann von plazentaler Viviparie gesprochen.

Eine Spezialform der Oviparie beziehungsweise eine Übergangsform zwischen Oviparie und Viviparie ist die Ovoviviparie. Dabei verbleibt das Ei im Mutterleib. Dort wird der Embryo durch den im Ei enthaltenen Dotter versorgt. Die Jungtiere können dann entweder bereits im Mutterleib schlüpfen oder auch kurz nach der Eiablage. Ovovivipare Tiere brüten ihre Eier also im Körperinneren aus. Häufig werden auch diese etwas ungenau als "lebendgebärend" bezeichnet. Zu ihnen gehören zum Beispiel die meisten Seeschlangen, viele Haie und andere Knorpelfische, wenige Knochenfische, einige Spinnen und die Blattläuse.



</doc>
<doc id="3818" url="https://de.wikipedia.org/wiki?curid=3818" title="Politik">
Politik

Politik bezeichnet die Regelung der Angelegenheiten eines Gemeinwesens durch verbindliche Entscheidungen. Sehr allgemein kann jegliche Einflussnahme, Gestaltung und Durchsetzung von Forderungen und Zielen in privaten oder öffentlichen Bereichen als Politik bezeichnet werden. Zumeist bezieht sich der Begriff nicht auf das Private, sondern auf die Öffentlichkeit und das Gemeinwesen im Ganzen. Dann können das öffentliche Leben der Bürger, Handlungen und Bestrebungen zur Führung des Gemeinwesens nach innen und außen sowie Willensbildung und Entscheidungsfindung über Angelegenheiten des Gemeinwesens als Politik beschrieben werden. Im engeren Sinne bezeichnet Politik die Strukturen ("Polity"), Prozesse ("Politics") und Inhalte ("Policy") zur Steuerung politischer Einheiten, zumeist Staaten, nach innen und ihrer Beziehungen zueinander.

In der Politikwissenschaft hat sich allgemein die Überzeugung durchgesetzt, dass Politik „die Gesamtheit aller Interaktionen definiert, die auf die autoritative [durch eine anerkannte Gewalt allgemein verbindliche] Verteilung von Werten [materiellen wie Geld oder nicht-materiellen wie Demokratie] abzielen“.

Politisches Handeln kann durch folgenden Merksatz charakterisiert werden: „Soziales Handeln, das auf Entscheidungen und Steuerungsmechanismen ausgerichtet ist, die allgemein verbindlich sind und das Zusammenleben von Menschen regeln“.

Der Ausdruck Politik wurde, mit Umwegen über das Lateinische ("politica", "politicus"), nach ("politiká") gebildet. Dieses Wort bezeichnete in den Stadtstaaten des antiken Griechenlands alle diejenigen Tätigkeiten, Gegenstände und Fragestellungen, die das Gemeinwesen – und das hieß zu dieser Zeit: die Polis – betrafen. Entsprechend ist die wörtliche Übersetzung von "politiká" anzugeben als „Dinge, die die Stadt betreffen“ bzw. die „politischen Dinge“. In dieser Bedeutung ist „Politik“ vergleichbar mit dem römischen Begriff der "res publica", aus dem der moderne Terminus der „Republik“ hervorgegangen ist. Eine begriffsgeschichtlich besonders prominente Verwendung fand das Wort als Titel eines Hauptwerks des antiken Philosophen Aristoteles, der "Politik".

Die kontroversen Politikbegriffe und -definitionen können in drei Dimensionen sortiert werden, ohne dass diese sich untereinander ausschlössen.

Zu den regierungszentrierten oder gouvernementalen Politikbegriffen gehören die Konzepte "Macht, Herrschaft" und "Führung." Im 19. Jahrhundert galt der Staat und seine Macht (Gewaltmonopol) als das Hauptwesen der Politik. Alle Machtphänomene wurden versucht dem Staat zuzuordnen. In den internationalen Beziehungen ist Macht bis heute einer der Grundpfeiler der Theoriebildung (vgl. zum Beispiel Politischer Neorealismus). Kurt Sontheimer (1962) weist auf die Gefahr hin, dass Politikwissenschaft bei diesem Politikverständnis leicht zum Handlanger der Macht und der Mächtigen werden kann.

Emanzipatorische Politikauffassungen konzentrieren sich dagegen auf Machtbeschränkungen durch Partizipation, Gleichheit und Demokratisierung als Gegengewicht zu einer ordnenden Macht. Dazu gehört auch die kritische Analyse der vorherrschenden Herrschaftsstrukturen und Gesellschaftskritik.

Zu den normativen Politikbegriffen lassen sich die Konzepte "rechte Ordnung", "Frieden", "Freiheit" und "Demokratie" zählen und insbesondere auch alle emanzipatorischen Politikdefinitionen. Dabei geht es nicht um die reine Beschreibung politischer Phänomene, sondern es wird ein wertender Soll- oder Zielwert als Hauptkategorie eingesetzt. Das Konzept Freiheit kann zum Beispiel als ein Gegenbegriff zum Konzept "Macht" oder "Herrschaft" verstanden werden. Meist werden harmonische Gemeinwohlvorstellungen angeboten, die sich nur schwer mit den heutigen pluralistischen Gesellschaftsbedingungen vereinbaren lassen. Ein spezielles Problem der Kategorie ‚Frieden‘ ist, dass sie nicht bloß die Abwesenheit von Gewalt, sondern auch den Abbau von Ungleichheiten meinen kann.

Die rein deskriptiven, also beschreibenden, Politikvorstellungen lehnen Sollwerte als Wesen der Politik ab. Zu ihnen zählen die in der Einleitung gegebene Politikdefinition, diejenige von Lehmbruch und die von David Easton ("authoritative allocation of values"; Systemtheorie David Eastons). Ebenso wie die regierungszentrierten, Macht betonenden Politikbegriffe stehen diese in Gefahr, den Status quo zu stabilisieren und den gerade Herrschenden zu nutzen.

Konfliktorientierte Politikbegriffe gehen von der Existenz von Konflikten als unabänderlichen und notwendigen Erscheinungen des politisch-sozialen Lebens aus. Diese Konflikte müssten durch die politischen Prozesse geregelt werden. Die Voraussetzung für die Verwendung der Kategorie Konflikt ist, dass eine hinreichend flexible und stabile Gesellschaftsstruktur vorhanden ist, die die friedliche Konfliktaustragung zwischen den verschiedenen sozialen Gruppen mit ihren divergierenden Interessen ermöglicht. Zu den konfliktorientierten Politikbegriffen gehören neben dem deskriptiven systemtheoretischen Politikverständnis auch die Konflikttheorien von Ralf Dahrendorf und Lewis Coser, die Konflikte als die Triebkräfte jedes sozialen Wandels begreifen. Auch der marxistische Politikbegriff fußt auf Konflikt als Grundkategorie, nämlich dem Kampf der Klassen und ihrer Parteien um die Durchsetzung ihrer primär sozialökonomisch bedingten Interessen.

Im Gegensatz dazu ist bei konsensbezogenen Politikbegriffen das gesellschaftliche Gemeinwohl nur durch Konsens herstellbar. Zu diesen Politikbegriffen zählt neben dem klassischen emanzipatorischen Politikverständnis Jean-Jacques Rousseaus auch der Politikbegriff von Thomas Meyer.

Auch ohne Entscheidung über "die" Hauptkategorie von Politik kann man drei Dimensionen unterscheiden, die uns eine begriffliche Klärung und Unterscheidung der komplexen Wirklichkeit der in verschiedener Gestalt auftretenden Politik ermöglichen. Dafür haben sich im deutschsprachigen Raum die englischen Bezeichnungen "Polity", "Policy" und "Politics" eingebürgert.

Unterschiedliche normative Vorstellungen (wie etwas sein sollte) über den Inhalt, also Aufgaben und Ziele, von Politik, führen aufgrund begrenzter Mittel (Ressourcenknappheit) dazu, dass nicht alle Wünsche befriedigt werden können. Es kommt zu Interessenkonflikten innerhalb der unterschiedlichsten Politikbereiche, wie Sicherheitspolitik, Wirtschaftspolitik, Sozialpolitik und viele weiteren. Diese Konflikte müssen im Sinne der Stabilität des politischen Systems durch Kompromisse und folgende allgemeinverbindliche Entscheidungen vermittelt werden.

"Policy" steht also für die inhaltliche Dimension der Politik. Bezüglich der Politik einer Partei oder Regierung umfasst der Begriff, was diese zu tun beabsichtigt bzw. auch tut. Dazu gehören neben den von einer Regierung vergebenen und bewilligten materiellen Gütern auch immaterielle Aspekte. Da aber die allermeisten Maßnahmen der Politik eine materiell-ökonomische Seite besitzen, können die öffentlichen Haushalte oder die eingebrachten Haushaltsentwürfe einen Eindruck geben welche "policy" ein Land bzw. eine Regierung umsetzt.

Wenn im Alltag von „guter“ und „schlechter Politik“ gesprochen wird, dann ist damit in der Regel die "policy" der Regierung gemeint. Insofern als die Bevölkerung damit beurteilt, was bei einer bestimmten Politik für wen dabei herauskommt, ist dies die Sicht der von politischen Entscheidungen Betroffenen. Die Beurteilungskriterien sind dabei in den pluralistischen Gesellschaften allerdings in der Regel sehr verschieden, abhängig von den jeweiligen Wert- und Gerechtigkeitsvorstellungen, abhängig davon, mit welchen gesellschaftlichen Gebilden (einer bestimmten gesellschaftlichen Gruppe oder Klasse, der Nation oder einem über die Landesgrenzen hinausreichenden gesellschaftlichen Kollektiv) sich identifiziert wird.

Da es in der "policy" stets um gesellschaftliche Inhalte, Werte und Interessen geht, geht es nie nur um die Antwort auf die Frage nach der besten Politik. Vielmehr stehen auch die am politischen Entscheidungsprozessen Beteiligten und die Konsequenzen der Entscheidung für den Einzelnen im Fokus der Analyse. Folglich ist ebenfalls die Frage nach den Begünstigten und den Belasteten relevant.

"Kategorien:" Politisches Problem; Programme, Ziele, Lösungen; Ergebnisse der Politik; Bewertung der Politik

nach der räumlichen Abgrenzung: Mikropolitik, Kommunalpolitik, Metropolenpolitik, Landespolitik, Bundespolitik, Europapolitik, Weltpolitik

nach Sachgebieten: Arbeitsmarktpolitik, Außenpolitik, Auswärtige Kulturpolitik, Baupolitik, Behindertenpolitik, Bildungspolitik, Drogenpolitik, Energiepolitik, Entwicklungspolitik, Familienpolitik, Finanzpolitik, Forschungspolitik, Frauenpolitik, Gleichstellungspolitik, Gesundheitspolitik, Innenpolitik, Internationale Politik, Jugendpolitik, Landwirtschaftspolitik, Kulturpolitik, Lohnpolitik, Medienpolitik, Minderheitenpolitik, Rechtspolitik, Schulpolitik, Sozialpolitik, Sportpolitik, Sprachpolitik, Steuerpolitik, Technologiepolitik, Umweltpolitik, Verbraucherschutzpolitik, Verkehrspolitik, Verteidigungspolitik, Wirtschaftspolitik, Wissenschaftspolitik

Die ablaufenden politischen Willensbildungs- und Interessenvermittlungsprozesse prägen die möglichen Ergebnisse der "policy" maßgeblich.
Besonders Macht und ihre Durchsetzung im Rahmen der formellen und informellen Regeln bestimmen diese "politics"-Prozesse (Regierungskunst im weitesten Sinne) zusätzlich. In liberal-demokratischen Systemen (moderne Demokratie, mit Rechtsstaat und Marktwirtschaft) wird die Akzeptanz der Kompromissbildung dadurch erhöht, dass frühzeitig neben den Parteien auch gesellschaftliche Interessengruppen (Lobbyverbände wie Gewerkschaften und Unternehmensverbände) und Einzelpersonen in den Prozess der Entscheidungsfindung eingebunden werden.

Bei der Entwicklung und Beeinflussung der "policy" zeigt sich die Politik von ihrer konflikthaften Seite, dem Kampf um Macht und Einfluss der verschiedenen Gruppen und Personen. Damit inhaltliche Handlungsprogramme umgesetzt werden können, bedarf es neben der Erringung, dem Erhalt und dem Ausbau von Machtpositionen, auch der geschickten Auswahl des politischen Führungspersonals, der Formulierung der Wünsche und Interessen der gesellschaftlichen Gruppen, der Abstimmung mit anderen Forderungen und Interessen, um so ein umfassendes Handlungsprogramm anbieten zu können und wählbar zu sein. Dies erfordert die ständige Berücksichtigung anderer Menschen (Wähler, Parteikollegen usw.), deren mögliche Reaktionen bei der Erstellung und Durchführung der "policy" von vornherein mit einkalkuliert, antizipiert, werden müssen. Gerade in demokratischen Systemen geht es also auch immer um das Sammeln von Zustimmung und Einwilligung zu den Handlungsprogrammen.

Für die Politiker selbst ist aber daher auch der Aspekt des Kampfes um Entscheidungsbefugnis, welches mehr umfasst als die Erlangung der staatlichen Machtpositionen, entscheidend. Denn im Gegensatz zu typischen Verwaltungsbeamten, deren Kompetenzbereich klar über das Amt geregelt ist, muss sich der Politiker diesen Bereich erst erarbeiten und dann behaupten. Daher ist es für ihn zu wenig, nur die rein sachlichen Gesichtspunkte bei seiner Entscheidungsfindung zu berücksichtigen. Die Aspekte des Machterwerbs und des Machterhalts sind gerade in demokratischen, eben responsiven, Systemen besonders wichtig; insofern ist gerade die Demokratie eine hochpolitische Regierungsform.

"Politics" spielt aber auch in autoritären Systemen eine Rolle, in denen die Führer weniger Rücksicht auf die Bevölkerung nehmen müssen. Solange die Handelnden unter einem gewissen Zwang zur Rücksichtnahme auf andere Akteure stehen und versuchen müssen, Zustimmungsbereitschaft zu erzeugen, mit welchen Mitteln auch immer, kann von "politics" gesprochen werden. Auf welche Art die Zustimmung geschaffen wird (Interessenberücksichtigung, Kompromiss, Überzeugung, Zwang usw.) kann dann durchaus für eine Beurteilung von Politik als „gut“ oder „schlecht“ dienen. „Unter einem ‚klugen und geschickten Politiker’ verstehen wir offensichtlich nicht einfach einen ‚guten Fachmann‘, der viel von der Sache versteht – wenn er auch das tut, umso besser –, sondern eine Person, die die Fähigkeit hat, Menschen dazu zu bringen, bestimmten Handlungsprogrammen zuzustimmen und Folge zu leisten.“

Dabei kann zwischen "policy" und "politics" nicht immer streng getrennt werden. Es gibt nicht erst ein inhaltliches Programm und dann das Bemühen um Zustimmung zu diesem. Die politische Gruppenbildung (Interessenkoalitionen) findet in Wechselwirkung mit der Programmentwicklung statt. So wird eine die Regierungsmacht anstrebende politische Partei, die gewisse gesellschaftliche Reformen beabsichtigt (oder verhindern möchte), in der Regel auch weitere Programmpunkte vertreten, die ihr zwar weniger wichtig sind, aber für die Chance auf Gewinn der Regierungsmehrheit als notwendig erachtet werden. Dies ist von der „Regierungskunst“ nicht zu trennen. Die gedankliche Unterscheidung von "policy" und "politics" rechtfertigt sich dadurch, dass es uns erlaubt, „Ordnung in unser Nachdenken über das Politische zu bringen.“

"Kategorien:" politische Akteure, Beteiligte und Betroffene; Partizipation; Konflikte; Kampf um Machtanteile und um Entscheidungsbefugnis; Interessenvermittlung, -artikulation, -auswahl, -bündelung, -durchsetzung; Legitimationsbeschaffung durch Verhandlungen, Kompromisssuche, Konsensfindung

Die Verfassung, die geltende Rechtsordnung und Traditionen bestimmen die in einem politischen System vorhandenen Institutionen wie zum Beispiel Parlamente und Schulen. Dadurch wird die Art und Weise der politischen Willensbildung geprägt und der Handlungsspielraum der anderen Dimensionen beeinflusst. Politik im Sinne von "policy" und "politics" vollzieht sich stets innerhalb dieses Handlungsrahmens. Dieser ist nicht unveränderbar, aber doch so stabil, dass er nicht beliebig und jederzeit zur Disposition steht.
In (modernen) Staaten drückt sich dieser zunächst einmal durch die Verfassung aus, welche hier allgemein als grundlegende Organisationsform, die das Verhältnis der Staatsorgane untereinander regelt, verstanden wird, und nicht die schon inhaltlich bestimmte Vorstellung des „Verfassungsstaats“ meint, welcher schon mit konkreten Ordnungsvorstellungen wie Rechtsstaatlichkeit, Gewaltenteilung und Garantie von Freiheits- und Bürgerrechten verbunden ist. Ferner geht die "polity" als Organisationsform auch über den Inhalt der geschriebenen Verfassung im engeren Sinn hinaus und umfasst auch weitere grundlegende Gesetze wie beispielsweise in der Bundesrepublik Deutschland das Bundeswahlgesetz oder die Bestimmungen, die das Verhältnis von Parlament und Regierung, Regierung und Verwaltung, Bund und Ländern regeln.

Zur "polity" gehören auch die Grenzen, die dem politischen Handeln gesetzt sind (beispielsweise durch die Bürgerrechte, die Bürgerdefinition oder die Staatsgrenzen). Eine solche staatliche „Verfassung“ beruht also auch auf einer Einheit (Volk oder Bürgerbevölkerung), die durch diese „verfasst“ wird. Somit gehört zur "polity" auch der Aspekt der Abgrenzung.

Neben die offiziellen, geschriebenen Regelwerke (Verfassung, Gesetze) tritt auch die jeweilige Politische Kultur eines Landes; man sprach auch schon von einer „doppelten politischen Verfassung“. So kann die geschriebene Verfassung eine parlamentarische Demokratie vorsehen, aber das Desinteresse der Bevölkerung oder der Missbrauch durch die Regierenden die tatsächliche Verfasstheit des Staates als autoritär begründen. Gerade die nach 1945 versuchte, allzu einfache Übertragung von westlichen Verfassungsvorstellungen auf Länder der Dritten Welt hat dies durch ihr teilweises grandioses Scheitern gezeigt. Rechtliche Regelungen und politische Institutionen allein, egal wie ausgeklügelt das politische Institutionensystem auch sein mag, genügen nicht zur Stabilisierung eines politischen Systems und zur Erklärung der tatsächlichen Funktionsweise. Gesellschaftliche Normen und Sitten, zum Beispiel den politischen Gegner nicht unter die Gürtellinie zu schlagen, sind meist wichtiger für das Fortbestehen guter politischer Umgangsformen und damit für die Stabilität des politischen Systems als die Möglichkeiten, gegen politische Verleumdungen gerichtlich, also im Rahmen der geschriebenen Verfassung, vorgehen zu können. Zur politischen Kultur einer Gesellschaft gehören die typischen politischen Orientierungs- und Verhaltensmuster der Menschen.

"Kategorien:" Internationale Abkommen und Regelungen; Grundgesetz; Zentrale Verfassungsprinzipien; politische Institutionen; Gesetze und Rechtsnormen; Politische Kultur

"Politische Fragen" tauchen zwar meist im Zusammenhang mit Sachfragen auf, aber sie können nicht von Fachleuten rein wissenschaftlich, technokratisch entschieden werden. Zur Beantwortung sind immer normative Grundentscheidungen und Abwägungen von prinzipiell gleichberechtigten Ansprüchen nötig, bei denen es kein Richtig oder Falsch im Sinne absoluter Wahrheit gibt. Bei politischen Fragen geht es immer auch um Fragen des menschlichen Zusammenlebens. Daher spielen bei der Beantwortung neben subjektiven Meinungen und Überzeugungen über unsere Interessen und Rechte auch der Wille, diese durchzusetzen, eine Rolle. Als der beste Agent unserer eigenen Interessen sieht die liberale Demokratietheorie dabei uns selbst an, daher die Notwendigkeit von Grundrechten der politischen Mitwirkung. Politische Fragen sind also normative Fragen, die nicht wissenschaftlich entscheidbar sind (siehe Politische Theorie und Wissenschaftstheorie).

Doch nicht alle zwischenmenschlichen Probleme sind auch politische Probleme. Als menschliches Handeln definiert man allgemein ein Verhalten, mit dem der Handelnde einen subjektiven Sinn verbindet, und soziales Handeln als Handeln, dessen gemeinter Sinn auf das Verhalten anderer bezogen ist (Max Weber). Dazu benötigen Menschen Empathie, die Fähigkeit, sich in den Interaktionspartner hineinzuversetzen und die Situation „mit seinen Augen“ zu sehen.

Dieses Soziale wird nun politisch, sobald das Zusammenleben der Menschen als solches zum Problem wird (konfliktorientierter Politikbegriff). In allen sozialen Beziehungen (Freundeskreis, Kollegen etc.) "kann" ein spezifisches Vorgehen nötig werden, um Konflikte zu regeln. Alle Anstrengungen, die zu einer Vermittlung und Regelung führen (sollen), kann man als "Politik im weiteren Sinne" bezeichnen. Diese Art Politik ist aber nicht der eigentliche Zweck dieser informellen Gruppen und sozialen Organisationen (zum Beispiel Sportverein).

Erst auf der Ebene der nicht mehr auf persönlicher Bekanntschaft aufbauenden, anonymen Gesellschaft wird Politik auch zum eigentlichen Zweck, weil das Zusammenleben der vielen sozialen Gruppen, Interessen und Weltanschauungen "stets" konfliktanfällig ist und der Regelung bedarf. Alles soziale Handeln, das gesamtgesellschaftlich verbindliche Regelungen bezweckt, wird als "Politik im engeren Sinne" bezeichnet.

siehe auch: Politische Ideengeschichte und Staatstheorie

Früh befassten sich Gelehrte damit, wie Politik auszusehen hat; dabei standen die Fragen „Was ist eine gute und gerechte Staatsordnung?“ und „Wie erlangt man wirklich Macht im Staat?“ im Mittelpunkt der Diskussion. Schon im Altertum verglich beispielsweise Aristoteles (384 bis 322 v. Chr.) alle ihm bekannten Verfassungen (Politische Systeme) und entwickelte eine auch heute viel zitierte Typologie in seinem Werk "Politik". Neben der Anzahl der an der Macht Beteiligten (einer, wenige, alle) unterschied er zwischen einer guten gemeinnützigen Ordnung (Monarchie, Aristokratie, Politie) und einer schlechten eigennützigen Staatsordnung (Tyrannis, Oligarchie, Demokratie). Erste geschriebene Gesetze belegen, dass Politik sich nicht nur mit den Herrschenden, sondern auch früh schon mit sozialen Regeln befasste, die bis heute überliefert wurden. Der Codex Hammurapi (Babylon, etwa 1700 v. Chr.) oder das Zwölftafelgesetz (Rom, etwa 450 v. Chr.) sind Beispiele verbindlicher Regeln, die sicher als Ergebnis von Politik gewertet werden können. Befasst man sich mit den Politikern der Römischen Republik und dem Römischen Kaiserreich, erkennt man viele Elemente damaliger Politik auch heute noch. Es wurde mit Kreide Wahlwerbung an die Hauswände geschrieben (etwa in Pompeji). Es gab einen komplexen Regierungsapparat und hitzige Rivalität zwischen den Amtsträgern. Korruption war ein Thema der Gesetzgebung und römischer Gerichtsverhandlungen. Briefe Ciceros an einen Verwandten belegen, wie gezielt die Wahl in ein Staatsamt auch taktisch vorbereitet wurde.

Mit dem Verfall des Römischen Reiches verlor Politik in Europa wieder an Komplexität und die Gemeinwesen wurden wieder überschaubarer, Konflikte kleinräumiger. In der Zeit der Völkerwanderung und des frühen Mittelalters war Politik mehr kriegerische Machtpolitik und weniger durch Institutionen und allgemein akzeptierte Regeln geprägt. Je stärker der Fernhandel, Geld und Städte wieder an Bedeutung gewannen, desto mehr wurden wieder feste Machtzentren gebraucht und desto wichtiger wurden Institutionen. Beispielsweise bildete sich die Hanse als Interessen- und Machtverbund einflussreicher sich selbst regierender Städte. Wichtiges relativ konstantes Machtzentrum war die katholische Kirche. Aus sozialen Gemeinschaften, die bestimmten Führern die Treue schworen (Personenverband), wurden langsam Erbmonarchien mit festen Grenzen.

In Frankreich entwickelte sich der Urtypus des absolutistischen Herrschers, in England entstand die an Recht und Gesetz gebundene konstitutionelle Monarchie. Dort waren bald auch die wohlhabenden Bürger offiziell an der Politik beteiligt. Mit der Zeit wurde dann das Zensuswahlrecht auf größere Teile der Bevölkerung ausgeweitet. In der Zeit der Aufklärung erdachten Gelehrte neue Modelle der Staatskunst. Statt Niccolò Machiavellis Modell der absoluten Macht, das er in seinem Buch "Der Fürst" ("Il Principe") darstellte, definierte John Locke das Modell der Gewaltenteilung. Die Bürgerlichen Freiheiten wurden von verschiedenen politischen Philosophen gefordert. Mit Thomas Jeffersons Menschenrechtserklärung und der US-amerikanischen Verfassung begann die Zeit der modernen Verfassungsstaaten. Die Französische Revolution und die Feldzüge Napoleons wälzten Europa um. Mit dem Code civil in Frankreich wurde das erste Gesetzbuch auf Basis der Menschenrechte eingeführt. Überall fielen allmählich die Standesschranken. Politik wurde zu einer Angelegenheit des ganzen Volkes. Es entstanden Parteien, die zuerst von außen eine Opposition organisierten, um später selbst die Regierung zu stellen. Einige Parteien wie die SPD oder später die Grünen entstanden aus sozialen Bewegungen wie der Arbeiterbewegung oder der Anti-Atom- und Friedensbewegung, andere formierten sich vor einem religiösen Hintergrund (Zentrum).

Im 20. Jahrhundert kam es schließlich zur Herausbildung internationaler Organisationen mit zunehmendem Einfluss auf die Politik. Der erste Versuch, im sogenannten Völkerbund eine Völkergemeinschaft zu bilden, scheiterte mit dem Zweiten Weltkrieg. Heute existiert neben den Vereinten Nationen eine Vielzahl weiterer internationaler Organisationen. Eine Besonderheit stellt die Europäische Union dar, die ein höheres Integrationsniveau als eine klassische internationale Organisation aufweist, aber trotzdem kein föderaler Staat ist.


Anarchismus - Autoritarismus - Christdemokratie - Demokratie - Diktatur - Faschismus - Institutionalismus - Kapitalismus - Kommunismus - Kommunitarismus - Konservatismus - Kontextualismus - Politischer Liberalismus - Neoliberalismus - Marxismus - Nationalismus - Nationalsozialismus - Parlamentarismus - Sozialdemokratie - Sozialismus - Totalitarismus

Platon - Aristoteles - Niccolò Machiavelli - Baruch de Spinoza - Jean Bodin - Hugo Grotius - Charles de Montesquieu - Jean-Jacques Rousseau - Thomas Hobbes - John Locke - John Stuart Mill - Karl Marx - Michail Bakunin - Max Weber - John Rawls - Hannah Arendt




</doc>
<doc id="3819" url="https://de.wikipedia.org/wiki?curid=3819" title="Personal Computer">
Personal Computer

Ein Personal Computer (engl. zu dt. „persönlicher“ bzw. „privater Rechner“, kurz PC) ist ein Mehrzweckcomputer, dessen Größe und Fähigkeiten ihn für den individuellen persönlichen Gebrauch im Alltag nutzbar machen; im Unterschied zu vorherigen Computermodellen beschränkt sich die Nutzung nicht mehr auf Computerexperten, Techniker oder Wissenschaftler. Das Konzept geht zurück auf eine Idee aus den 1970er Jahren, begründet von Hackern. Die leichte Bedienbarkeit und ein für Privathaushalte erschwinglicher Preis waren wichtige Voraussetzungen für das Konzept, das seit 1976 technisch umgesetzt wird. Erst Geräte dieser Art lösten das aus, was der Journalist Steven Levy als Computerrevolution bezeichnet. Demgegenüber werden Geräte aus einer früheren Zeit vereinzelt bereits Personal Computer genannt, obgleich sie nicht in das Konzept passen.

Ein PC ist ein Mikrocomputer, in Abgrenzung zu einem Minirechner oder Großrechner. Er tritt beispielsweise als Desktop-, Notebook- oder Tablet-Computer in Erscheinung und kann unter einem beliebigen Betriebssystem laufen, wie beispielsweise Windows, iOS oder Unix. Das Spektrum reicht vom Bereich des Heimcomputers bis hin zum typischen Arbeitsplatzrechner. Überdurchschnittlich leistungsfähige Arbeitsplatzrechner für rechen- und speicherintensive Anwendungen werden als Workstation bezeichnet; ihr Preis kann ein Vielfaches eines PCs betragen.

Obwohl bereits in den 1970er-Jahren üblich, wurde der Begriff „Personal Computer“, vor allem dessen Kurzform „PC“, ab 1981 im Sprachgebrauch zunehmend und exklusiv mit dem IBM Personal Computer und dessen IBM-kompatiblen PC-Nachbauten verknüpft. Das war dem Marketing von IBM mit seiner erfolgreichen Werbung geschuldet. Die Verknüpfung bezog sich auf die darin verbaute x86-Prozessor-Familie und der darauf laufenden Betriebssysteme DOS und Windows. Darüber hinaus wird der Begriff vereinzelt mit der Bauart eines x86er-Desktop-PCs assoziiert, was jedoch im Widerspruch steht zur Bauart und den Bezeichnungen alternativer x86er-PC-Geräte, wie beispielsweise dem Microsoft Tablet-PC.

Ein (aktueller) PC verfügt normalerweise über die folgenden Komponenten:
Außerdem (nicht im Bild):

Streng genommen zählen Peripheriegeräte wie Monitor, Tastatur, Maus und Drucker nicht zwangsläufig zu den Komponenten des Personal Computers.

Der Journalist Steven Levy veröffentlichte 1984 das weltweit erste Buch, das sich unter anderem mit der frühen Geschichte des Personal Computers auseinandersetzt und dabei die Entwickler und deren Motivation in den Mittelpunkt stellt. Es trägt den Titel "„Hackers – Heroes of the Computer Revolution“". Darin beschreibt er eine Gruppierung von Hackern – eine Art stark ausgeprägter Technikenthusiasten – die sich in den 1970er Jahren für die Idee eines persönlichen Computers begeistern konnten. Ihnen ging es darum, Computer im Alltagsleben zu integrieren, sie für jedermann öffentlich zugänglich zu machen, bis hin zu dem damals visionären Ziel, einer breiten Masse die Nutzung universell einsetzbarer persönlicher Computer zu ermöglichen.

Um dieses Ziel zu erreichen, musste der persönliche Computer einige Voraussetzungen erfüllen: So war eine praktikable Größe wichtig, die es einer durchschnittlichen Person erlaubt, ihn transportieren und beispielsweise auf einem Schreibtisch installieren zu können. Er musste für Privathaushalte verfügbar, erschwinglich und universell einsetzbar sein. Entscheidend war eine Handhabung, die für die breite Masse geeignet ist. Das machte eine intuitive und universelle Datenein- und -ausgabe erforderlich, die weit hinaus ging über die Kippschalter und Leuchtdioden der bislang üblichen Computer des unteren Preissegments. Damit die Nutzung nicht nur Elektronik-Fachleuten vorbehalten blieb, mussten Bausätze auch über fertig verlötete Komponenten verfügbar sein, die der Computerhändler oder sogar der Benutzer leicht zusammenfügen kann. Über die jeweilige Bedienungsanleitung hinaus sollte es keine spezielle Schulung erfordern, sowohl den Computer als auch eine darauf installierte Anwendungssoftware zu betreiben. Darüber hinaus sollte der Endbenutzer die Möglichkeit erhalten, seinen persönlichen Computer frei programmieren zu können. Erst Geräte dieser Art lösten das aus, was Levy in seinem oben genannten Buch als Computerrevolution bezeichnet.

Diese Idee wurde von der damals vorherrschenden Industrie als absurd abgetan. So soll Thomas J. Watson, der frühe Chef von IBM, 1943 erklärt haben: „Ich glaube, es gibt einen Weltmarkt für vielleicht 5 Computer“. Wenn auch nicht in diesem Ausmaß, folgten in den 1970er Jahren Unternehmen wie Texas Instruments, Fairchild, IBM und DEC im Grunde noch immer diesem Dekret. Von einem Mitarbeiter auf die Entwicklung eines persönlichen Computers angesprochen, wies DEC-Chef Ken Olsen 1977 diesen Vorschlag mit der Begründung von sich, dass er sich keine Privatperson vorstellen könne, die einen solchen Computer haben wolle.

Der Idee eines öffentlich zugänglichen Computers, der Vorstufe auf dem Weg zum persönlichen Computer, widmete Ted Nelson 1974 ein Buch mit dem Titel „Computer Lib“, welches zum Standardwerk unter den damaligen Verfechtern dieser Idee wurde. Lee Felsenstein gründete bereits im selben Jahr das „Community Memory“-Projekt, welches über öffentliche Terminals in Plattenläden und Bibliotheken den Zugriff auf einen Computer ermöglichte. Das Projekt war für die damalige Zeit wegweisend und hatte den praktischen Nutzen eines schwarzen Bretts, auf dem man per ADD einen beliebigen Beitrag einfügen und mit FIND finden konnte.

Aufgrund der Größe und Kosten der Computer der 1950er und 1960er Jahre, die meist ganze Räume füllten oder als "Minicomputer" etwa schrankgroß waren, konnten diese kaum einem einzelnen Menschen persönlich zugewiesen werden. Das änderte sich erst allmählich, als seit 1961 TTL-Chips und seit 1971 die Mikroprozessoren auf den Markt kamen und die bis dahin vorherrschenden Kernspeicher durch Halbleiterspeicher ersetzt wurden. Für Privathaushalte und damit für den Bau eines PCs erschwinglich wurden solche Komponenten seit Mitte der 1970er Jahre.

Rund um den von Fred Moore und Gordon French im März 1975 gegründeten Homebrew Computer Club in der Region von San Francisco, der Westküste der Vereinigten Staaten, trafen sich technikbegeisterte Menschen, Hacker, wie Lavy schreibt. Angefangen von praktischen Projekten und Entwicklungen, bis hin zur Geburt einer vollkommen neuen Industrie im Silicon Valley, haben sie die Entwicklung des persönlichen Computers entscheidend vorangetrieben. Sie machten bezüglich des PCs immer wieder mit Konzepten und praktischen Entwicklungen auf sich aufmerksam. Viele Computer-Pioniere gingen aus ihren Reihen hervor; Mitglieder dieses Vereins gründeten zahlreiche Computerunternehmen. Der Homebrew Computer Club wird daher als „Schmelztiegel für eine ganze Branche“ bezeichnet.

Levy setzt in seinem oben genannten Buch die Grenze zum Personal Computer dort, wo er das damals visionäre Ziel erreicht sieht, den die von ihm interviewten Entwickler im Personal Computer sahen. Aus dieser Sichtweise heraus gilt der im April 1976 veröffentlichte Apple I als erster Personal Computer der Welt, 1977 gefolgt vom Commodore PET, dem Tandy TRS-80 Model 1 und dem Apple II.

Die oben genannten Voraussetzungen für einen PC sind jedoch nicht festgeschrieben; der Begriff "Personal Computer" hat keine feste Definition. Die Benennung des Apple I als ersten Personal Computer ist daher nicht unumstritten.

Es gibt je nach Gewichtung tatsächlich mehrere Computer, die jeweils als erster Personal Computer der Welt bezeichnet werden. Vernachlässigt man beispielsweise den Punkt der einfachen für die breite Masse geeigneten Handhabung und setzt auch die universelle Einsatzmöglichkeit des Computers nicht zwingend voraus, so ist der für Privathaushalte erstmals erschwingliche Simon von 1949 der erste Personal Computer; ein auf Relais basierender Lerncomputer, der ausschließlich als Selbstbausatz zu erwerben war. Soll es ein rein elektronischer Computer mit integrierten Schaltkreisen sein, der als komplett montiertes Gerät ausgeliefert wurde, so gilt der Kenbak-1 von 1971 als der erste PC. Wird ein in Serie produzierter Mikrocomputer vorausgesetzt, der also als zentrale Recheneinheit (CPU) einen Mikroprozessor nutzt, dann ist es der Micral N von 1973. Einigen gilt auch der Altair 8800 von 1975 als erster Personal Computer der Welt, auch wenn er sich nicht viel unterschied vom Micral N. Allen bis hier hin genannten Computern gemein ist die Voraussetzung, das ein Personal Computer für Privathaushalte erschwinglich sein muss. Doch selbst dieser Punkt erhebt keinen Anspruch auf Allgemeingültigkeit; Menschen die der einfachen Handhabung eine besonders hohe Gewichtung geben, aber weder dem Preis noch der Verfügbarkeit eine Bedeutung beimessen, bezeichnen den Xerox Alto von 1973 als den ersten PC der Welt. 

Die Bedeutung des Wortes liegt in seinem Gebrauch. Für Levys Abgrenzung spricht, dass der PC umgangssprachlich nicht mit einem Gerät assoziiert wird, das man per Kippschalter und Lämpchen bedient, sondern per Tastatur und Monitor. Der für 666 US-Dollar erhältliche Apple I war unbestritten der erste für Privathaushalte erschwingliche Personal Computer, der ab Werk mit einem Betriebssystem und allen benötigten Anschlüssen ausgestattet war, um ihn auf moderne Weise per Tastatur und Monitor zu betreiben. Nicht im Lieferumfang enthalten waren einige für den Betrieb wichtige Komponenten; die Tastatur, das Gehäuse und Netzteil mussten separat erworben werden und ohne ein heimisches Fernsehgerät als Monitorersatz und einen Kassettenrecorder als Datenspeicher war er nicht arbeitsfähig. Im Januar 1977 wurde der weltweit erste PC dieser Art mit einer kompletten betriebsbereiten Ausstattung vorgestellt: der für 795 US-Dollar erhältliche Commodore PET 2001. Der vier Jahre ältere Xerox Alto wurde zwar auch per Tastatur und Monitor betrieben und verfügte sogar über eine Maussteuerung, jedoch wird er im Unterschied zu diesen beiden Geräten als Workstation klassifiziert aufgrund seines hohen Preises. Eine Workstation kann ein vielfaches eines Personal Computers kosten. Seine Herstellungskosten lagen bei 12.000 US-Dollar (was auf die heutige Kaufkraft bezogen einem Wert von US-Dollar entspricht); der führende Entwickler Charles P. Thacker schätzt, dass der Verkaufspreis im Jahr 1973 sogar bei 40.000 US-Dollar gelegen hätte (damals wurde der Xerox Alto nicht offiziell zum Verkauf angeboten). Bereits vor dem Xerox Alto existierten andere grafische Workstations mit Tastatur und Monitor, wie beispielsweise der IMLAC PDS-1 von 1970 und der IBM 2250 aus dem Jahr 1964. 

Ihre einfache Handhabung und der geringe Preis machten Personal Computer seit 1976 für durchschnittliche Privatanwender weitgehend tauglich und attraktiv. Erst der überragende Verkaufserfolg solcher Geräte durch andere (zum Teil branchenfremde, zum Teil neu gegründete) Unternehmen sollte die vorherrschende Computerindustrie dazu veranlassen, sich der Idee des persönlichen Computers anzunehmen, eigene Produkte zu entwickeln und seit 1981 auf den Markt zu bringen. Dabei war das Marketing von IBM mit der Werbung für ihren IBM Personal Computer, kurz IBM-PC, derart erfolgreich, dass der Begriff „Personal Computer“ häufig mit dieser Marke in Verbindung gebracht wurde, obwohl gerade dieser PC preislich grenzwertig zum ursprünglichen PC-Konzept war. Dafür setzten sich die wesentlich preiswerteren Nachbauten, die "„IBM-PC-kompatiblen Computer“", als eine der erfolgreichsten Plattformen für den persönlichen Computer durch; die heute marktüblichen PCs mit Windows-Betriebssystem und x86-Prozessoren beruhen auf der stetigen Weiterentwicklung des damaligen Entwurfs von IBM. Der erfolgreichen Werbung von IBM ist es zu verdanken, dass häufig der IBM-PC 5150 von 1981 als erster Personal Computer der Welt bezeichnet wird. Dabei war IBM nicht einmal das erste Unternehmen, das ihr Produkt als "Personal Computer" bewarb; in der Werbung erstmals als Personal Computer bezeichnet wurde der Tischrechner HP-9100A von 1968, gefolgt vom HP-9830 von 1972 und dem Altair 8800 von 1975.

1949 stellte Edmund C. Berkeley mit "Simon" den ersten Computer für den Heimgebrauch vor. Er bestand aus 50 Relais und wurde für 300 US-Dollar in Gestalt von Bauplänen vertrieben, von denen in den ersten zehn Jahren über 400 Exemplare verkauft wurden. Er ist ein für damalige Verhältnisse kompakter digitaler programmierbarer und weitgehend automatisierter Computer, der für Privathaushalte erschwinglich ist. Rechnet man die Bauteile hinzu, konnte er damals für rund 500 US-Dollar gebaut werden (was auf die Kaufkraft des Jahres bezogen einem Wert von US-Dollar entspricht). Für seinen Betrieb benötigt man über die Bedienungsanleitung hinaus keine spezielle Schulung. Damit erfüllt er bereits viele Voraussetzungen für einen persönlichen Computer und gilt daher manchen Menschen als erster PC der Welt. Mit seinen fünf Bedientasten, dem Lochstreifen als Programmablaufspeicher und den fünf Lämpchen als Ausgabeeinheit, die Zahlen von 0 bis 4 darstellen konnten, entspricht dieses Gerät jedoch technisch nicht dem, was man heute unter einem Personal Computer versteht. Der "Simon" ist speziell als Lerncomputer entwickelt worden, der dem Anwender die grundlegende Funktionsweise eines Computers näher bringen sollte. Ähnlich verhält es sich mit dem auf Drehscheiben basierenden GENIAC von 1955, dem analogen, auf Röhren basierenden Heathkit EC-1 von 1959 und dem Relais-Computer Minivac 601 aus dem Jahr 1961.

Der erste frei programmierbare Tischrechner der Welt, der "Programma 101" von der Firma Olivetti, erschien 1965 für einen Preis von 3.200 US-Dollar. Drei Jahre später brachte die Hewlett-Packard Company mit dem "HP-9100A" ein programmierbares Rechengerät auf den Markt, das im Vergleich zum Programma 101 bereits mehr Möglichkeiten der Anzeige und Programmierung bot, aber mit 4.900 UD-Dollar rund das Doppelte eines damaligen durchschnittlichen Bruttojahresgehaltes kostete. Dieser Rechner wurde in einer Werbeanzeige erstmals in der Literatur als Personal Computer bezeichnet, obgleich er weder preislich noch technisch dem heutigen Verständnis eines PCs entspricht. Bemerkenswert ist, dass die Leistung beider Tischrechner ohne die Verwendung von integrierten Schaltkreisen erbracht wurde.

1967 erschien ein Buch mit dem Titel „How To Build a Working Digital Computer“ von den Autoren Edward Alcosser, James P. Phillips und Allen M. Wolk. Das Buch beschreibt, wie man einen einfachen Computer aus Alltagsgegenständen bauen kann, wie beispielsweise aus Büroklammern für Schalter und einer Konservendose für den Trommelspeicher. Für 1.000 US-Dollar vertrieb das Unternehmen COMSPACE 1969 eine professionell zusammengebaute Version dieses Lerncomputers unter dem Namen "Arkay CT-650".

Mit dem "IMLAC PDS-1" erschien 1970 eine vernetzte Grafik-Workstation des Herstellers Imlac Corporation of Needham, einem kleinen Unternehmen aus Massachusetts, USA. Bemerkenswert war, dass dieses Unternehmen eine sehr effiziente Konstruktion entwickelte, die es erlaubte, dass sie ihren Computer bereits für 8.300 US-Dollar zum Kauf anbieten konnten (zum Vergleich kostete die technisch in etwa vergleichbare "IBM 2250" aus dem Jahr 1964 noch 280.000 US-Dollar). Die PDS-1 war ein Vorreiter auf dem Weg zu einem grafischen Personal Computer, in Teilen vergleichbar mit dem weitaus teureren Xerox Alto aus dem Jahr 1973.

Im September 1971 erschien der von John Blankenbaker entwickelte "Kenbak-1" für 750 US-Dollar. Obwohl die ersten Mikroprozessoren seit 1971 verfügbar waren, verwendete sein Computer keinen Mikroprozessor; Blankenbaker konstruierte die Maschine auf einer einzigen Platine mit TTL-Chips. Der Kenbak-1 wird mitunter als erster Personal Computer der Welt bezeichnet. Da er kein Betriebssystem erhielt, mussten sämtliche Aktionen in einem reinen Maschinencode programmiert werden über eine Reihe von Tasten und Schaltern, die auf der Frontseite untergebracht waren. Die Ausgabe bestand aus einer Reihe von Lichtern auf der Rückseite. Geräte wie dieses waren für durchschnittliche Privatanwender weitgehend untauglich und auch kaum attraktiv; der Kenbak-1 passt daher weder in das Konzept des persönlichen Computers noch entspricht er technisch dem, was man umgangssprachlich unter einem Personal Computer versteht.

Der 1972 veröffentlichte "HP-9830" war der erste Tischrechner mit einem im ROM integriertem BASIC-Interpreter. Im Unterschied zum HP-9100A und dem Programma 101 verfügt er über eine vollständige alphanumerische Tastatur und ein alphanumerisches Display, wodurch er eine Brücke schlug zwischen einem üblichen Tischrechner und einem All-in-One-Desktop-Computer. Zwar besitzt dieses Gerät nur eine einzige Bildschirmzeile mit lediglich 32 Zeichen. Zudem ist er mit 5.975 US-Dollar (was auf das Jahr bezogen einem Wert von US-Dollar entspricht) für Privathaushalte kaum erschwinglich. Dennoch kommt er dem heutigen Verständnis zum Begriff Personal Computer schon recht nahe. Daher gilt er manchen Menschen als erster PC der Welt.

Der "Micral N" war ein weiterer Vorläufer des Personal Computers; der erste in Serie hergestellte Computer seiner Art mit einem Mikroprozessor, in diesem Fall ein Intel 8008. Er wurde in Frankreich von André Truong Trong Thi und François Gernelle entwickelt und dort seit 1973 für 8.500 FF verkauft (umgerechnet 1.750 US-Dollar, was nach heutiger Kaufkraft einem Wert von US-Dollar entspricht). Erfolgte die Datenein- und -ausgabe zunächst per Kippschalter und Lämpchen, wurde er seit 1974 gegen Aufpreis mit einer Tastatur und einem Bildschirm ausgeliefert; Festplatten waren ab 1975 erhältlich. Bedientechnisch entsprach dieser Computer seither dem modernen Verständnis eines PCs, jedoch preislich nicht. Ebenfalls im Jahr 1973 erschien der "Scelbi-8H", ein weiterer Mikrocomputer mit einem Intel 8008.

Mit dem "HP-65" kam 1973 für 795 US-Dollar der erste vollständig programmierbare Taschenrechner der Welt auf den Markt. Da er jedoch über kein alphanumerisches Display verfügt, kann er als anspruchsvoller programmierbarer Rechner betrachtet werden, der technisch weiter von einem PC entfernt ist, als beispielsweise der ein Jahr ältere HP-9830. 

Das Unternehmen Xerox PARC stellte 1973 ihren "Xerox Alto" der Weltöffentlichkeit vor, ein etwa kühlschrankgroßes Gerät. Mit einer schreibmaschinenähnlichen Tastatur, einer 3-Tasten-Maus, einer zusätzlichen kleinen 5-Tasten-Akkordtastatur für besondere Befehle, einem objektorientierten Betriebssystem, einem Bildschirm mit grafischer Benutzeroberfläche (engl. "graphical user interface", kurz "GUI") und einer Ethernet-Schnittstelle war er wegweisend für den künftigen Personal Computer. Diese Workstation war jedoch als wissenschaftliches Gerät gedacht; sie war weder für den privaten Gebrauch erschwinglich, noch in dieser Zeit für den Handel verfügbar und wurde erst ab 1978 zu einem Preis von 32.000 US-Dollar zum Kauf angeboten (nach heutiger Kaufkraft wären das US-Dollar).

Der "Mark-8" erschien 1974 und war ein weiterer Mikrocomputer, betrieben von einem Intel 8008. Vom Mark-8 wurde ausschließlich der Bauplan und die Platine verkauft; er war somit lediglich als Selbstbausatz erhältlich.

Mit dem "Altair 8800" des Anbieters MITS kam 1975 ein in Serie produziertes Gerät auf den Markt, das ebenfalls als "Personal Computer" bezeichnet wird und als Bausatz für 397 US-Dollar, als Komplettgerät für 695 US-Dollar zu erwerben war. Innerhalb der frühen Szene rund um den "Homebrew Computer Club" erfreute sich der Altair 8800 großer Beliebtheit und diente den Mitgliedern des Clubs als Kernstück für eigene Erweiterungen. Zukunftsweisend war die Ausstattung mit einem Bus-Stecksystem für Erweiterungskarten nach dem S-100-Bus-Standard. Mit seinen Kippschaltern als Eingabeeinheit und Leuchtdioden als Ausgabeeinheit entspricht jedoch auch dieses Gerät technisch nicht dem, was man heute unter einem Personal Computer versteht; zur Benutzung von Altair BASIC oder CP/M als Kommandozeilen-Betriebssystem musste ein Text-Terminal über die serielle Schnittstelle (das RS-232-Interface) angeschlossen werden. Ähnlich war es mit dem im selben Jahr erschienenen "KIM-1" des Unternehmens MOS Technology, der immerhin schon eine 24-Tasten-Eingabeeinheit im Taschenrechnerformat zur direkten Eingabe von HEX-Code besaß sowie über eine 6-stellige 7-Segment-LED-Anzeige als Ausgabeeinheit verfügte.

Der 1975 veröffentlichte "IBM 5100" wartete mit seiner schreibmaschinenähnlichen Tastatur, einem integrierten Monitor und einem Kassettenlaufwerk für wechselbare Datenspeicher auf. Rein technisch konnte er alles vorweisen, was man heute unter einem Personal Computer versteht. Jedoch war der Preis von damals 9.000 bis 20.000 US-Dollar ( bis US-Dollar auf das Jahr bezogen) für Privathaushalte deutlich zu hoch, weshalb er ebenfalls nicht in das Konzept des Personal Computers passte.

Steve Wozniak (in der Szene bekannt als "The Woz") war ein prominentes Mitglied des Homebrew Computer Clubs. Im April 1976 stellte er der Öffentlichkeit seinen Computer vor, der dem Altair 8800 technisch weit überlegen war. Als erstes Gerät der Welt war er mit 666 US-Dollar für Privathaushalte erschwinglich und entsprach zugleich den modernen bedientechnischen Vorstellungen eines persönlichen Computers: Sein Computer verwendete eine schreibmaschinenähnliche Tastatur als Eingabeeinheit und einen Bildschirm (zunächst in Form eines umfunktionierten Fernsehgerätes) als Ausgabeeinheit. Als einziges Peripheriegerät gab es ein Kassetten-Interface, mit dem sich in Kombination mit einem herkömmlichen Kassettenrecorder Programme auf Audiokassetten speichern und von diesen wieder laden ließen. Apple war eines der Unternehmen, die aus dem Homebrew Computer Club hervorgingen, wobei Steve Wozniak neben Steve Jobs und Ronald Wayne einer der Gründer ist. Sein Computer wurde zwar vor der Unternehmensgründung entwickelt, aber dann dort in Serie produziert und unter dem Namen "Apple I" verkauft. Als Einplatinencomputer wurde er in Form einer komplett bestückten Platine ausgeliefert und vom Händler oder Endbenutzer um ein Netzteil, Gehäuse und eine Tastatur ergänzt, ehe er am heimischen Fernseher betrieben werden konnte.

Das Nachfolgemodell, der Apple II, wurde nun auch in kompletter Ausführung ausgeliefert mit einem Gehäuse, Netzteil, Tastatur und Monitor, später sogar mit einer Maus. Gleichzeitig war er der letzte industriell hergestellte PC, der vollständig von einer einzelnen Person, Steve Wozniak, entworfen wurde. Er wurde im April 1977 in den USA vorgestellt und für einen Preis von 1.298 US-Dollar angeboten (das entspricht einem Wert von US-Dollar auf das Jahr bezogen). Bei seiner Markteinführung hatte er acht freie Steckplätze des 8-Bit-Apple-Bus-Systems, mit denen er durch Einsetzen der entsprechenden Erweiterungskarte für unterschiedliche Anwendungen (z. B. Textverarbeitung, Spiele, Steuerungstechnik) genutzt werden konnte. Diese Eigenschaft eines Computers, der also durch Steckplätze individuell an die Wünsche des Konsumenten angepasst werden kann, gilt heute als Grundeigenschaft eines PCs. Außerdem konnten mit diesem Computer bereits Farben dargestellt und Töne wiedergegeben werden. Die Apple-II-Baureihe war ein offenes System, das heißt, alle wesentlichen Konstruktionsdetails wurden veröffentlicht.

Der weltweit erste industriell hergestellte PC in kompletter Ausführung (inklusive Gehäuse, Netzteil, Tastatur, Monitor und Massenspeicher in Form einer Datasette) wurde im Januar 1977 vorgestellt: der Commodore PET 2001, der für 795 US-Dollar über den Ladentisch ging. Im August desselben Jahres folgte der Tandy TRS-80 Model 1 für 599 US-Dollar. Von den Leistungsdaten her waren beide Geräte dem Apple II ähnlich, hatten aber keine Steckplätze für Erweiterungskarten, keine Farbdarstellung und keine Tonausgabe. Der PET verfügte über den in der professionellen Messtechnik verbreiteten (parallelen) IEC-Bus, was zur Folge hatte, dass er in Forschung und Industrie Verbreitung fand.

Am 12. August 1981 wurde der erste IBM-PC 5150 vorgestellt. Er bewegte sich preislich an der Obergrenze der handelsüblichen PCs. In der Grundausstattung konnte er für 1.565 US-Dollar erworben werden (ohne Diskettenlaufwerke und Monitor, dafür mit TV-Anschluss) oder für 3.005 US-Dollar in kompletter Ausführung (dies wären heute US-Dollar). In der maximalen Ausbaustufe mit mehr Speicher und Farbgrafik wurde er für 6.000 US-Dollar angeboten. IBM nutzte ihre damalige Marktführung für (Großrechner-)Datenverarbeitungsanlagen und schaffte es, dass ihr IBM-PC als Arbeitsplatzcomputer in zahlreichen Unternehmen eingesetzt wurde.

Das Gerät war mit dem Intel-8088-Prozessor ausgestattet und verfügte über ein 8-Bit-ISA-Bussystem. Auch die folgenden Modelle wurden mit Prozessoren von Intel ausgerüstet. Der bereits ein Jahr vor dem 8088-Prozessor (4,77–9,5 MHz Takt; interne CPU-Wortbreite 16 Bit; System-Datenbus 8 Bit) von Intel vorgestellte 8086-Prozessor (6–12 MHz Takt; CPU-Wortbreite 16 Bit; System-Bus 16 Bit) sorgte dafür, dass sich für die Serie die Abkürzung „x86-Architektur“ etablierte.

Der IBM-PC wurde von 1981 bis 1995 ausschließlich mit dem Betriebssystem von IBM, PC DOS, vertrieben, das von Microsoft an IBM lizenziert worden war. Die 1981 begonnene Zusammenarbeit endete 1985. Beide Unternehmen entwickelten danach das Betriebssystem getrennt weiter, achteten jedoch auf gegenseitige Kompatibilität. Das Betriebssystem MS-DOS von Microsoft gab es seitdem nur auf Computern, die in der Bauweise jenen von IBM entsprechen.

Das Unternehmen IBM legte die Grundkonstruktion seines PC offen und schuf einen informellen Industriestandard; es definierte damit die bis heute aktuelle Geräteklasse der "„IBM-PC-kompatiblen Computer“". Zahlreiche preiswerte Nachbauten und Fortführungen der IBM PCs durch andere Unternehmen machten die Plattform sowohl am Arbeitsplatz als auch im Heimbereich sehr erfolgreich.

Im Februar 1984 wurde der IBM Portable Personal Computer vorgestellt, eine frühe Vorstufe der Laptops, später Notebooks genannt (als Klasse der tragbaren Personal Computer).

Mit TV-Ausgang und Tonausgabe kamen ab den 1980er-Jahren weitere Geräte als Heimcomputer auf den Markt. Die meistverkauften Modelle waren der Commodore C64 und die Geräte der Amiga-Reihe, wie auch verschiedene Ausführungen des Atari ST.

Im deutschen Sprachraum wurde in den 1980er-Jahren das englische Wort "personal" (persönlich) mitunter inkorrekt mit dem deutschen "Personal" (Arbeiter, Angestellte) assoziiert. Eine Ableitung von "Personal Computer" hin zu einer professionellen Nutzung entsprechender Geräte wurde hierzulande daher gebräuchlich. So wurden in den Medien Geräte mittlerer Leistung manchmal als "„reicht an die Leistung eines Personal Computers [nicht] heran“" klassifiziert, obgleich es sich bei solchen Geräten tatsächlich auch um Personal Computer handelte. Da die Amiga-Reihe und der Atari ST zu Heimcomputerpreisen die Leistung der IBM PC XT und AT übertrafen und teilweise die Gehäuseform der professionellen Geräte verwendeten, verschwand die irrtümliche Unterscheidung zum Ende der 1980er-Jahre.
Da IBM kein Monopol auf die verwendeten Komponenten hatte (mit Ausnahme des BIOS), konnte Compaq 1983 den ersten zum IBM-PC kompatiblen Computer auf den Markt bringen. Vor allem in Ostasien schufen Unternehmen eine Reihe von Nachbauten, in Deutschland waren es Unternehmen wie Commodore und später Schneider. Der sich so entwickelnde Markt führte durch den Konkurrenzkampf zu sinkenden Preisen und verstärkter Innovation.

Die Stückzahlen waren zu Anfang noch bei weitem nicht mit den heutigen vergleichbar. Die Marktsituation Ende 1983 laut für professionelle Mikrorechner (ohne Heimcomputer):

Marktpositionen der sechs wichtigsten Anbieter von professionellen Mikros per Ende 1983:

Im amerikanischen Weihnachtsgeschäft 1984 spielten Personal Computer erstmals eine signifikante Rolle. Jedoch hatten sowohl IBM als auch Apple zu viele Geräte produziert und klagten im Frühjahr 1985 über ein enttäuschendes Ergebnis. Viele Händler blieben auf den PCs sitzen, und Kunden klagten, „sie könnten nicht viel mit den Maschinen anfangen.“ Ein Apple-Händler schenkte sogar jedem, der einen Rechner kaufte, ein italienisches Fahrrad dazu.

Auch Apple-Computer wurden teils nachgebaut, aber das Unternehmen konnte sich (mit deutlich geschrumpftem Marktanteil) behaupten. Die Apple-II-Linie wurde Anfang der 1990er-Jahre eingestellt. Heute wird nur noch die Macintosh-Reihe hergestellt. Apple und Sun (Unix) sind die beiden einzigen Hersteller, die Hardware und Software (Betriebssystem und Anwenderprogramme) selbst entwickeln und auch zusammen vermarkten.

Die meisten anderen Hersteller, wie etwa Commodore und Schneider, verschwanden Anfang der 1990er-Jahre weitgehend vom Markt oder wandten sich wieder anderen Geschäftsfeldern zu (Atari). Die aktuelleren PC-Modelle von IBM, wie der PC 300GL, blieben weitgehend unbekannt und gingen auf dem Markt neben den Produkten anderer Hersteller unter. Ähnlich erging es dem Versuch von IBM, den Markt mit der Personal-System/2-Reihe und dem Betriebssystem OS/2 zurückzuerobern.

Im Privatbereich wurden Heimcomputer und PC zunächst zum Experimentieren, Lernen und Spielen benutzt. Zunehmend wurden sie auch in Bereichen wie Textverarbeitung, Datenbanken und Tabellenkalkulation eingesetzt und fanden so Eingang in den betrieblichen Alltag.

Die Leistungsfähigkeit von Personal Computern nahm seit ihrer Entstehung stetig zu (Moore’sches Gesetz). Neben den Aufgaben der Textverarbeitung und Tabellenkalkulation wurde der Multimedia-Bereich zu einem der Hauptanwendungsgebiete. Um auch den Anforderungen neuester PC-Spiele gerecht zu werden, gibt es sogenannte Gaming-PCs, die mit hoher Rechenleistung und sehr leistungsfähigen Grafikkarten ausgestattet sind.

Bei modernen PCs kommt seit 2006, unabhängig vom eingesetzten Betriebssystem, praktisch durchweg Hardware auf Basis der x86-Architektur zum Einsatz, die historisch auf den IBM Personal Computer von 1981 bzw. dessen sogenannte IBM-kompatible Weiterentwicklungen zurückgeht. Von den anderen Computerarchitekturen für Einzelplatzrechner waren bis Anfang 2006 die PowerPC-Modelle von Apple erhältlich, bevor auch Apple diese durch x86-Modelle ersetzte. PowerPC-Rechner von Apple werden vom Betriebssystem seit Mac OS X Snow Leopard nicht mehr unterstützt.

Als Betriebssysteme werden neben dem marktführenden Windows hauptsächlich unixoide Betriebssysteme eingesetzt, vor allem Linux und BSD. Auch das Apple-Betriebssystem ist seit Einführung von Mac OS X ein Unix-Derivat, das im Gegensatz zu den verschiedenen Linux-Distributionen und freien BSD-Betriebssystemen ab der Version 10.5 als UNIX zertifiziert ist ("siehe auch" Liste von Betriebssystemen).

Entsprechend der technischen Entwicklung wandelten sich auch die Bauformen mit der Zeit.

Der erste IBM PC war wortwörtlich ein Desktop-Computer, er und seine Zeitgenossen von anderen Herstellern hatten Gehäuse im Querformat und standen auf dem Arbeitstisch. Auf ihnen stand wiederum der Monitor mit einer Bildschirmdiagonalen von damals nur 10 bis 13 Zoll. Als diese Desktop-PCs mit der Zeit noch etwas größer wurden und nicht nur auf dem Schreibtisch immer mehr im Weg waren, sondern die auch langsam größer werdenden Monitormodelle auf dem Computer ergonomisch immer ungünstiger standen, ging man zu "neben" dem Monitor stehenden PC-Gehäusen im Hochformat über, sogenannten "Tower"-Modellen. Letztere differenzierten sich im Anschluss in "Big Towers", "Midi-Towers" und weitere Abstufungen. Je nach Höhe des Towers und Vorlieben des Benutzers stehen viele heutige Personal Computer auch unter oder neben dem Tisch.

Schon seit Anfang der 1980er-Jahre bemühte man sich parallel dazu, "tragbare Computer" zu entwickeln. Damit wird üblicherweise ein Gerät mit der Technik und den Ausmaßen eines Desktop-Computers bezeichnet, dessen zumeist kofferförmiges Gehäuse jedoch zum regelmäßigen Transport ausgelegt ist. Die ersten Geräte dieser Art waren 1981 der Osborne-1 und der Kaypro, sowie 1983 der SX64. Sie benötigten für den Betrieb zwingend einen Stromnetzanschluss; an Batteriebetrieb war noch nicht zu denken, vor allem weil diese Modelle noch mit integrierten Bildröhren ausgestattet waren, die viel Energie benötigten.

Die Geräteklasse der "mobilen Computer" (mit Akkubetrieb) wurden seit der Verfügbarkeit preisgünstiger LCD-Anzeigen entwickelt. 1981 erschien der GRiD Compass 1100 und eröffnete die Klasse der noch recht schweren Schoßrechner (Laptop genannt); der erste kommerziell erfolgreiche Laptop erschien 1986 mit dem IBM PC Convertible. Die Bezeichnung Notebook wird tendenziell für die mittelkleinen und leichteren Ausführungen der mobilen Computer benutzt, während der Begriff Netbook für ein deutlich kleineres Gerät ohne optisches Laufwerk verwendet wird, dessen Tasten auch zu klein für die Verwendung im Zehnfingersystem sein können.

Eine spezielle Bauform eines Personal Computers, die zu den "Handheld-Geräten" zählt, ist der Tablet-PC. Auch wenn schon vorher Tablets existiert haben, erlangten sie 2002 mit Microsofts Windows XP Tablet PC Edition größere Aufmerksamkeit; der Durchbruch für diese Geräteklasse erfolgte jedoch erst 2010 mit der Veröffentlichung von Apples iPad.

Die Zahl der weltweit verkauften PCs ist im Jahr 2013 weiter zurückgegangen, insgesamt wurden ca. 316 Millionen Stück verkauft, davon knapp 26 Millionen in Europa (genauer: EMEA – die Wirtschaftsregion, die Europa, den Mittleren Osten und Afrika umfasst). Insgesamt sank der Verkauf gegenüber 2012 um ca. 10 %. Dieser Rückgang ist u. a. auf die weitere Verbreitung von Tabletcomputern und Smartphones zurückzuführen.

In der Vergangenheit wurde die Mehrzahl der verkauften PCs als Arbeitsplatzrechner in Wirtschaft und Verwaltung eingesetzt, aber auch viele Privathaushalte verfügten über PCs.

Besonders in aufstrebenden Ländern („Emerging Markets“) haben die Menschen heute anstelle eines PCs mit Internetzugang als erste Geräte eher ein Smartphone für die Kommunikation und einen Tabletcomputer als Computer.

Spätestens seit ca. 2005 sind durch PCs verursachte Umweltauswirkungen anerkannt und werden erforscht.
Die Umweltauswirkungen sind durch die hohen Absatzzahlen und vielfältige Schadstoffe in der Produktion erheblich, sie belasten die Umwelt insbesondere rund um Produktionsanlagen und durch den Material- und Energieverbrauch. Das Gebiet in der Informatik, das sich mit Umweltaspekten von PCs und Computerhardware im Allgemeinen beschäftigt, ist die Green IT.

Einer Studie aus dem Jahr 2003 zufolge braucht man für die Herstellung eines Computers samt 17-Zoll-Röhrenmonitor 240 Liter fossile Brennstoffe. Geht man bei einem Gesamtgewicht des Systems – inklusive Röhrenmonitor – von rund 24 Kilogramm aus, entspricht das dem Zehnfachen seines Eigengewichts. Zusätzlich werden rund 22 kg Chemikalien und 1.500 kg Wasser benötigt.

Um gegenwärtig (Stand 2013) seinen PC möglichst sparsam betreiben zu können, empfiehlt sich die Beachtung gewisser Normen der Industrie. Für Netzteile ist dies heute die „80-PLUS“-Zertifizierung in Bronze, Silber, Gold oder Platinum nach der ENERGY-STAR-Richtlinie der US-Umweltbehörde EPA.

Ein einzelner PC in "Desktop-Ausführung" brauchte über lange Zeit weitgehend konstant um die 50 W an elektrischer Leistung. Dieser Wert hielt sich etwa bis zur Einführung des Intel-Pentium-III-Prozessors Ende der 1990er-Jahre. In der Folgezeit stiegen diese Werte rapide auf weit über 100 W alleine für den Prozessor und teilweise über 200 W für den kompletten Rechner an. Eine Trendwende gab es 2004, als der Prozessorhersteller AMD für seinen AMD Athlon 64 erstmals bisher nur bei Notebooks eingesetzte Funktionen zur dynamischen Änderung des Prozessortaktes einsetzte. Durch diese heute in sämtlichen Prozessoren verfügbare Funktion ist der Stromverbrauch zumindest ohne eine dedizierte Grafikkarte und ohne aufwändige Berechnungen wieder gefallen.

Deutliche Abweichungen davon ergeben sich, wenn der Prozessor tatsächlich ausgelastet wird, und noch wesentlich mehr bei der Verwendung einer dedizierten Grafikkarte, die – auch wenn nur ein normaler Desktop darzustellen ist – bereits zwischen 10 und 80 W benötigt.

Laptops und Notebooks, die mobil sein sollen und auf Akkubetrieb ausgelegt sind, versuchen, möglichst sparsam mit der elektrischen Energie umzugehen, um möglichst lange Akkulaufzeiten zu erreichen. Hier werden je nach Geschwindigkeitsanforderung und Auslastung zwischen ca. 10 W und (z. B. für mobile 3D-Grafik) deutlich über 60 W erreicht. Die Werte sind über die Zeit weitgehend konstant; Verbesserungen bei der Akkutechnik werden hauptsächlich in eine Verkleinerung der Gehäuse und nur zu kleinen Teilen in eine Verlängerung der Laufzeit gesteckt. Auch Industrie-PCs verwenden oft Laptop-Technik, das jedoch weniger aufgrund des Stromverbrauchs, sondern um auf bewegliche Teile in Gestalt von Lüftern verzichten zu können und so die mechanische Robustheit zu erhöhen.

Die noch kleineren Einplatinencomputer, UMPCs oder Netbooks benötigen mit teilweise unter 10 W noch weniger elektrische Leistung, wobei hier jedoch meist Zugeständnisse bei der Rechenleistung gemacht werden müssen.

Aufgrund des hohen Ressourcenaufwandes bei der Herstellung ist es nicht sinnvoll, allein mit Hinblick auf eine Energieeinsparung ein sparsames Neugerät zu kaufen, da im Vergleich zum Energieverbrauch bei Herstellung und Entsorgung der Energieverbrauch beim Gebrauch vergleichsweise gering ist. Der durch die Neuproduktion anfallende zusätzliche Energieverbrauch kann – wenn das überhaupt bei normalem privaten Gebrauch möglich ist – nur nach etlichen Jahren durch die geringere Leistungsaufnahme kompensiert werden.

Aufwändige Berechnungen wie 3D-Bilder in Computerspielen, Bildberechnungen von Grafikprogrammen oder Videobearbeitung erhöhen den Energiebedarf auf 300 W. Leistungsstarke PCs mit sehr schnellen Prozessoren kommen auf Werte bis zu 425 W. Hochleistungsgrafikkarten benötigen jeweils weitere bis zu 275 W, so dass bei zwei Grafikkarten unter Volllast des Systems Leistungsaufnahmen von knapp 1.000 W möglich sind.

Personal Computer bestehen aus den unterschiedlichsten Komponenten, hauptsächlich Elektronik und Metall. Sie werden in Deutschland nach der Elektronikschrottverordnung von den Herstellern über Erfassungsstrukturen zurückgenommen. Besitzer sind verpflichtet, die Geräte getrennt vom Restmüll den Erfassungsstellen zuzuführen. Die Rücknahme ist in Deutschland kostenfrei. Im Zuge der Verschrottung werden heute viele Elektronikkomponenten der Wiederverwendung zugeführt, um seltene Erden zu retten.

Veraltete, noch funktionsfähige PCs oder Bauteile können auch verkauft oder an Bastler oder Bedürftige weitergegeben werden – z. B. im Rahmen des Projektes linux4afrika. Oft werden alte Geräte auch illegal in Drittweltländer verfrachtet, wo, oft unter Vernachlässigung von Arbeits- und Umweltschutzmaßnahmen, die wertvollen Metalle extrahiert werden und der Rest auf Deponien abgelagert wird (z. B. Guiyu in China oder Agbogbloshie in Ghana).

Allgemein ist ein Personal Computer ein US-Produkt, da der größte Teil beziehungsweise der größte Kostenfaktor aus Importprodukten von Herstellern aus den Vereinigten Staaten stammt, gefolgt von Taiwan. So ist in den meisten PCs ein Intel- oder AMD-Prozessor verbaut. Auch die gebräuchlichsten Grafikkarten stammen von US-Unternehmen wie Nvidia, Intel oder AMD.

Bei den PC-Mainboards hingegen führt die Republik China (Taiwan) die Produktion an, mit Produkten von Asus, Gigabyte Technology und Micro-Star International inklusive der intern meistverbauten Soundchips des Anbieters Realtek. Bei der eigentlichen Herstellung der Boards ist jedoch die ebenfalls taiwanesische Firma Foxconn führend.

Marktführer der externen Soundlösungen ist hingegen Creative Technology (Singapur) mit der Soundblaster-Serie.

Bei den Festplatten (HDDs) führen hingegen US-Anbieter wie Seagate Technology und Western Digital den Markt an.

Bei den Netzteilen führen die Hersteller Seasonic, Thermaltake und Enermax aus Taiwan die Produktion an. Wobei das allgemeine Qualitätskriterium für Netzteile heute, nämlich die 80-PLUS-Zertifizierung in Bronze, Silber, Gold, Platinum und Titanium von der US-amerikanischen Umweltbehörde EPA stammt und sich als Marktstandard durchgesetzt hat. Netzteile ohne 80+-EPA-Prüfsiegel sind heute praktisch unverkäuflich.

Bei den Speichermodulen führen US-Hersteller wie Corsair Memory, Mushkin, Micron Technology und Kingston Technology den Markt an gefolgt von G.Skill und TeamGroup aus Taiwan. Die Speicherchips werden jedoch hauptsächlich von den koreanischen Herstellern Samsung und Hynix sowie von der amerikanischen Firma Micron Technology hergestellt.

Ferner haben auch die drei größten FPGA-Hersteller Xilinx, Altera und Atmel ihren Sitz in den USA.

An der Fertigung aktueller Personal Computer haben europäische Hersteller damit nur einen verschwindend geringen Anteil und sind hier stark auf Importe angewiesen. Auch der US-Marktführer Microsoft der häufigst eingesetzten Betriebssystemsoftware Windows trägt hier einen großen Anteil zur Wertschöpfung in den USA bei.

Für das Vereinigte Königreich ist hier jedoch noch die ARM Limited vorteilhaft, welche die ARM-Architektur weltweit lizenziert, sowie Raspberry Pi für die Marktführerschaft unter den Einplatinencomputern mit dem Raspberry Pi. Die Eurozone selber profitiert hiervon jedoch erst mal nicht.

Die Entwicklung europäischer Alternativen kam über das Prozessordesign kaum hinaus, da bereits in den 1980er-Jahren alle bedeutenden Heimcomputerhersteller wie Commodore und Atari ihren Sitz in den USA hatten, sowie auch die Prozessorhersteller MOS Technology, Motorola und Zilog.

Am europäischsten war in diesem Zusammenhang noch der Acorn Archimedes des britischen Unternehmens Acorn mit zudem auch der eigenen ARM-Architektur, sowie heute auch noch das niederländische Unternehmen ASML das die EUV-Lithografie-Belichtungsmaschinen, die Schlüsseltechnologie für die Intel- und AMD-Prozessorfertigung, herstellt.

Die Entwicklung in Deutschland hatte bis in die 60er Jahre mit den Computern der Zuse KG des Computerpioniers Konrad Zuse eine gewisse Bedeutung. Der Z1 bis zum 5-Kanal-Lochstreifen gesteuerten Zeichentisch Zuse Z64 Graphomat waren bedeutende Eigenentwicklungen mit internationaler Anerkennung.

Die Siemens AG übernahm 1969 die Zuse KG komplett und lagerte die Computersparte nach München in das neue Unternehmen Siemens Nixdorf aus, das später zu Fujitsu Siemens Computers fusionierte. Der Schwerpunkt lag in Kassensystemen und Computerhandel.
Der 1975 erschienene Nixdorf Quattro 8870 Großrechner mit dem Betriebssystem Business BASIC und der Anwendungssoftware COMET hatte jedoch noch bis in die 80er Jahre einen gewissen Erfolg in Unternehmen. Die CPU (ein nicht näher definierter 1585.01, vermutlich ein Plagiat) stammte bereits von der Firma Digital Computer Controls, Inc. aus den USA.

Die CPU Entwicklung in Russland stützt sich vor allem auf den russischen Elbrus-2000-Mikroprozessor, hierfür existiert sogar ein eigener 130-Nanometer-Fertigungsprozess nach der von AMD übernommenen Ausrüstung aus der Fab 30.

Im Mai 2015 machte ein aktueller Elbrus Heim-PC basierend auf dem Elbrus-4C Chip Schlagzeilen durch die flüssige Darstellung des Spiels Doom BFG von 2004.

In der Vergangenheit begnügte sich Russland mit dem Kopieren des Zilog Z80, dessen Derivate wie der MME U880 auch die Grundlage für Computer in der DDR stellte.

1982 stellte NEC den PC-98 vor, der mit einem Intel-8086-Prozessor und 128 kB RAM bestückt war. Er wurde aus dem PC-88 entwickelt, der noch einen Zilog Z80 nutzte. Obwohl der PC-98 dem IBM-PC sehr ähnlich war, nutzte er den 16-Bit breiten C-Bus, der schon im PC-88 vorhanden war, während beim IBM-PC der ISA-Bus verwendet wurde. Der PC-98 war in Japan über eine Dekade lang so erfolgreich, dass man vom „IBM-PC Japans“ sprechen kann. Die Leistung der verbauten Komponenten (CPU, RAM, Speicherkapazitäten, etc.) wurde während dieser Zeit ständig angepasst. Erst mit dem Aufstieg von Windows wurde der PC-98 zunehmend aus dem Markt verdrängt. Da es Windows 3.1 und Windows 95 auch in Versionen für den PC-98 gab, griffen Kunden zunehmend zu billigeren PCs, die ebenfalls Windows-fähig waren. Innerhalb von 5 Jahren schwand der Marktanteil in Japan von 60 auf 33 %. 1997 wurde die Produktion des PC-98 eingestellt.

Als der japanische Heimcomputer galt allgemein in den 80er-Jahren der MSX-Computer, der sich dort auch als Alternative zum C64 etabliert hatte, wobei auch der MSX-1 im Wesentlichen noch auf US-Herstellern aufbaute, so auch hier wieder der Z80-Prozessor des Herstellers Zilog, der Grafikchip von Texas Instruments und der Soundchip von General Instrument. Japanische Eigenmarken stellten erst die Nachfolger der MSX-2 und MSX turbo R mit dem Yamaha v9958 Grafikprozessor und Yamaha YM2149/YM2413 Soundchip dar. Die 7,16 MHz schnelle R800 CPU für den MSX turbo R war dabei zwar Zilog kompatibel, tatsächlich aber auch eine Eigenentwicklung der japanischen ASCII Corporation und wurde hergestellt von der Mitsui Bussan. 1987 folgten der X68000-Heimcomputer von Sharp mit einer von Hitachi produzierten HD68HC000-CPU (später wurden 68000er von Motorola verbaut) und 1989 der FM Towns mit erstmals serienmäßig eingebauten CD-ROM-Laufwerk, der jedoch auch schon auf der Intel-8086-Architektur basierte.

Afrika spielt für den weltweiten PC-Handel eine Rolle, da hier der Großteil des Computer- und Elektroschrotts landet. So recyceln z. B. Kinder in Agbogbloshie auf der giftigsten Müllhalde der Welt in einem Slum am Rande der Hauptstadt Accra in Ghana, viele Altgeräte durch das Herauslösen von Aluminium aus Monitorrahmen und Kupfer aus den Kabeln.

In den arabischen Ländern fand wie in Afrika praktisch keine Entwicklung statt. Am ehesten entwickelt noch Israel mit dem Rüstungshersteller Rafael Advanced Defense Systems einen Teil der Computer-Technologie. Bedeutende Eigenentwicklungen waren hier bereits das Raketenabfangsystem Iron Dome wie auch das Trophy (APS) System zur Verteidigung von Panzern.





</doc>
<doc id="3822" url="https://de.wikipedia.org/wiki?curid=3822" title="Popmusik">
Popmusik

Popmusik (aus engl. "popular music") bezeichnet Musik, die vorwiegend seit den 1950er Jahren aus dem Rock ’n’ Roll, der Beatmusik, Folk aber auch dem Jazz entstand, von den Beatles fortgeführt und von der schwedischen Band ABBA seit Beginn der 1970er Jahre geprägt wurde. Sie gilt seit den 1960er Jahren als europäisierte, international etablierte Variante angloamerikanischer Musik, die im Kontext jugendlicher Subkulturen entstand und inzwischen zumeist auf Grundlage der Erkenntnisse aus der Elektronischen Musik aufbereitet und massenmedial verbreitet wird. Im weiteren Sinne zählt jede durch Massenmedien verbreitete Art von Unterhaltungsmusik wie Schlager, Filmmusik, Operette, Musical, Tanzmusik sowie populäre Adaptionen aus der klassischen Musik und Folklore zur Popmusik. Zwischen Populärer Musik und Popmusik wird unterschieden. Entgegen dem Stigma des "Populären" enthält die "Popmusik" den Beigeschmack des „spritzigen“ und frischen Musikereignisses. Der Terminus "Populäre Musik" transportiert dagegen ein Gefühl der wissenschaftlichen Distanz zum Gegenstand.

Popmusik ist musikwissenschaftlich nicht gleichzusetzen mit „populärer Musik“, obwohl der Begriff insbesondere in der soziologischen Literatur auch in diesem Sinne verwendet wird. Ähnlich wie beim Kunstgenre Pop-Art, deren Popularität um 1962 zur Verwendung des Begriffs "Pop" führte, sei die Ableitung von „Pop“ als Abkürzung von "populär" unzureichend. Popmusik stelle nach Peter Wicke „technisch rekontextualisierte Musik und somit prinzipiell jede Musikform dar, die einen ökonomisch rentablen Verbreitungsgrad erreichen kann“. Daher sei es kaum möglich, sie auf bestimmte musikalische Charakteristika einzuschränken.

Auf der anderen Seite wird der Begriff in der Literatur mit unterschiedlichen Bedeutungsinhalten und Wertungen angefüllt. So beschrieb der Musikwissenschaftler Tibor Kneif "„Der Begriff Pop-Rock bezeichnet heute eine ins kitschige, billig Sentimentale abgleitende Richtung innerhalb der Rockmusik“". Synonym mit Pop werden auch die Bezeichnungen Beat- und Rockmusik gebraucht. In Musikzeitschriften wie Spex oder Rolling Stone wurden die Begrifflichkeiten "Pop" und manchmal auch "Rock ’n’ Roll" verwendet, um die über das Musikalische herausgehende Dimensionen der Popmusik zu betonen, während der Begriff "Rock" eher musikalische Parameter in den Mittelpunkt stelle.

Der Begriff „populär“ lässt sich in der Musikgeschichte schon wesentlich früher als in den 1960er Jahren nachweisen, in denen der Begriff „Popmusik“ im Rahmen der Entwicklung der Popkultur geprägt wurde. Der deutsche Begriff „Volkslied“, als Übersetzung der englischen Bezeichnung „popular song“, stammt aus einer 1773 erschienenen Rezension von Johann Gottfried Herder über eine 1765 in England erschienene Sammlung von englischen und schottischen Balladen.

Die Entwicklung der Popmusik, die 1965 begann, erreichte in den 1980er Jahren ihren Endpunkt als rein jugendkulturelles Phänomen. Popmusik wurde weitgehend zum gesellschaftlich akzeptierten Phänomen und Bestandteil der Alltagskultur. Zahlreiche Popmusiker machten nun speziell Musik auch für ein erwachsenes Publikum.

Die nun für den „Mainstream“ produzierte Popmusik bezieht sich nicht nur auf ihre eigene ursprüngliche Tradition aus dem Vaudeville, dem Volkslied und dem Kunstlied, sondern inkorporiert verschiedene aktuelle Musikstile. Dabei nimmt sie den ursprünglichen Musikformen meist die Komplexität, entfernt für die gängigen Hörgewohnheiten Ungewohntes und Irritierendes, um sie für eine breite Masse zugänglicher und konsumierbarer zu machen. Das trifft insbesondere auf modifizierte, „gezähmte“ Anleihen bei ursprünglichen afro-amerikanischen Musikstilen wie Jazz, aber auch den Rap zu. Der Erfolg der kommerziell ausgerichteten Popmusik misst sich in den Hitparaden. Popmusik ist heute der kommerziell lukrativste Zweig der Musikindustrie.

Häufig wird Popmusik im Gegensatz etwa zur Kunstmusik mit Attributen wie „Einfachheit“ oder „Trivialität“ belegt: Im Einzelnen etwa durch eine als angenehm empfundene einfache Harmonik, leicht einzuprägende und nachsingbare Melodiefolgen, die oft auf der Diatonik beruhen, wenig komplexe, durchgehende Rhythmen, einem klassischen Liedaufbau aus Strophe und Refrain sowie einen sanften, melodiebetonten Gesang. Allerdings sind dies keine allgemeinen Merkmale, die für jede Form von populärer Musik gelten. Die Charakterisierung von Popmusik als „einfach“ folgt zumeist einer bewussten oder unbewussten Gegenüberstellung mit klassischer Musik, die in der Regel rhythmisch, harmonisch und melodisch ungleich vielschichtiger ist.

In Zeiten feudaler Herrschaftssysteme hatte die Musik zwei Hauptaufgaben: für die Regenten bzw. Adligen war die bei Hofe gespielte, als anspruchsvoll und vollendet geltende Musik nicht nur Unterhaltung, sondern auch Statussymbol, da zumindest im Mittelalter nur wohlhabende Adelige bzw. Fürsten sich professionelle Musiker und teure Instrumente leisten konnten, während bei der noch zumeist eher separiert über die Dörfer verteilt lebenden Bevölkerung die Musik überhaupt seltener und nur zu bestimmten Anlässen wie Hochzeiten oder Erntedankfesten von gering qualifizierten Gelegenheitsmusikanten gespielt wurde und nur die Funktion erfüllen musste, „tanzbar“ zu sein. Dabei war die Trennung zwischen den (musikalischen) Schichten durchaus durchlässig, denn um die strenge Etikette bei Hof umgehen zu können, erfand der Adel die Maskenbälle, bei denen sich die vornehme Gesellschaft als einfache Leute verkleidete und ungeniert deren ausgelassene Feste einschließlich der Musik imitierte.

Die schnelle Verbreitung von Musikstücken oder gar die Entwicklung von neuen Musikrichtungen wurde jedoch durch mehrere Faktoren behindert: die Menschen waren durch die dörfliche Siedlungsstruktur relativ isoliert, Austausch fand eher selten statt; fahrende Musikanten kamen selten und blieben nicht lang genug, um von ihnen neue Stücke auswendig zu lernen; die meisten Menschen konnten nicht lesen und schreiben und damit nicht ihre eigenen Lieder aufzeichnen, die Fähigkeit, Töne in Form von Noten aufzuzeichnen, war noch viel seltener. In Österreich und Deutschland kam noch erschwerend hinzu, dass es bis vor wenigen Hundert Jahren z. T. enorme Sprachbarrieren gab, da Personen, die von weit her kamen, manchen regionalen Dialekt kaum verstehen konnten.

Populäre Musik bestand zur Hauptsache aus dem Lied, das fast nur in religiösen Zusammenhängen aufgezeichnet wurde. Ausnahmen wie die "Carmina Burana" (1230) gestatten einen Einblick in die Praxis des mittelalterlichen Singens. Melodien sind allerdings nur teilweise erhalten, und wie sie gesungen wurden, kann man nicht mehr sagen. Die Tradition der Kontrafaktur zeigt, dass weltliche Melodien auch auf die religiöse Musik Einfluss hatten. Aufzeichnungen waren für die Musikpraxis nicht nötig. Berühmte Melodien wie die „timbres“ der Vaudevilles seit dem 15. Jahrhundert verbreiteten sich trotz der beschwerlichen Verkehrswege ungeheuer schnell.

Die Verbreitung der Musik an sich war bis zum Ende des 18. Jahrhunderts auf das Abschreiben von Notenblättern beschränkt, da das Notensystem zu kompliziert zum Letter-Druck war; es konnten (auch in Ermangelung von zum Noten lesen fähiger Schreiber) lediglich im mechanischen Verfahren des Notenstichs einzelne Kopien von Notenblättern hergestellt werden. Dies änderte sich erst, als Alois Senefelder das Verfahren des Steinplattendrucks (die Lithografie) erfand. Dadurch gelang es zum ersten Mal im Jahre 1796 in München eine auf Spezialpapier geschriebene Partitur als Negativ aufzutragen und originalgetreu abzudrucken. Zwar waren mit diesem Verfahren zuerst nur wenige Kopien möglich, doch die Erfindung der Dampfmaschine und die Verbesserung der Technik ermöglichten schon Anfang des 19. Jahrhunderts größere Auflagen von Notenblättern. Durch die Industrialisierung des Notenblattdruckes ergaben sich bedeutende Konsequenzen für die Entwicklung der populären Musik: die Musik des „einfachen“ Volkes konnte nun erstmals günstig in Massen reproduziert werden, was eine Vereinheitlichung der Versionen klassischer Volkslieder in Bezug auf Text und Tonfolge zur Folge hatte, ähnlich der „Standardisierung“ der deutschen Märchen durch die Zusammenstellung der Brüder Grimm; bei der klassischen Konzert- und Opernmusik setzte außerdem ein Trend ein, der dem heutigen „covern“ von Popsongs nicht unähnlich ist, so wurden z. B. ganze Opern für wenige oder nur ein Instrument umarrangiert – mit teilweise kuriosen Ergebnissen wie Mozarts "Zauberflöte" als reines Flötensolo. Damit entstand auch ein ganz neuer Berufszweig: der „Arrangeur“, der quasi aus altbekanntem Material klassischer Komponisten „frische“ Versionen bastelte.

Die bedeutenden gesellschaftlichen Umwälzungen des 19. Jahrhunderts hatten natürlich auch Auswirkungen auf die populäre Musik. Die Urbanisierung z. B. führte zu einer Anpassung der Dialekte innerhalb der großen Stadtgemeinschaften, die Städte übernahmen die Funktion kultureller „Schmelztiegel“, in denen sich Menschen aus verschiedenen Regionen auch musikalisch austauschten. Die kleinen bäuerlichen Gemeinden verwandelten sich im Urbanen in die große Berufsklasse der Industriearbeiter, die nach wie vor viel arbeiten mussten, ihre gelegentlichen Feste nun aber im großen Rahmen feierten (das Oktoberfest in München dürfte ein bekanntes Beispiel sein). Aber auch das an Einfluss und Wohlhaben immer mehr gewinnende Bürgertum fand in Salons und Varietés einen gemeinsamen Ort und Stil von Musik. Die zunehmende Geschwindigkeit, mit der Personen und Waren sich durch ganz Europa bewegen konnten, erleichterte auch die Verbreitung neuer Stile, im Bereich der Tanzmusik sei hier der Walzer als Beispiel angeführt.

Dass die USA in der Musikgeschichte bis in die erste Hälfte des 19. Jahrhunderts praktisch kaum eine Rolle spielte, hat im Wesentlichen zwei Gründe: Erstens waren die gesellschaftlichen Verhältnisse anders als in Europa, vor der Gründung der USA existierten neben den Indianerkulturen nur einzelne Kolonien europäischer Nationen, die keine eigenständige kulturelle Identität besaßen, sondern einfach das Kulturgut und die Musik der Mutternationen importierten; zweitens musste erst der Prozess der Erschließung des Westens vollendet sein, damit sich eine stabile Gesellschaftsstruktur und eine eigenständige kulturelle Identität entwickeln konnte.

Besonders bedeutend für die Entwicklung der populären Musik war jedoch der Unterschied zwischen den als „Rassen“ klassifizierten Menschengruppen: Während die europäischstämmige Bevölkerung trotz eines amerikanischen Selbstbewusstseins in kultureller Hinsicht weitestgehend ihren zumeist europäischen Wurzeln verhaftet blieb, waren die Afroamerikaner als Sklaven aus Afrika verschleppt und in den USA oft absichtlich von Menschen ihrer eigenen ethnischen Gruppe getrennt worden. Da die Siedlungsstruktur in Afrika dezentral war und einige Stämme auch nomadisch lebten, standen die Verschleppten in den USA nicht nur vor einer Sprachbarriere (fast jeder sprach eine andere Sprache oder Dialekt), sondern auch vor einem kulturellen Problem, da es kein „nationales“ Liedgut gab, das allen bekannt war. Zudem war ihnen die Ausübung ihrer kulturellen Traditionen, so auch der Musik, verboten. So mussten die Sklaven nicht nur die Sprache ihrer „Besitzer“ lernen (das Sprechen oder Singen in der Heimatsprache stand auf den Baumwollplantagen oft unter Strafe), sondern sich auch auf gemeinsame Inhalte verständigen, die zumeist auch noch von christlichen Missionaren beeinflusst wurden. Andererseits entwickelte sich durch diese Unterdrückung und gewaltsame Abtrennung von der Heimatkultur unter den Afroamerikanern als ersten US-Amerikanern so etwas wie eine gemeinsame neue Kultur, die auf übernommenen Elementen der europäischen Kultur in Verbindung mit afrikanischen Traditionen beruhte. Diese spielte in der ersten Hälfte des 19. Jahrhunderts aufgrund ihres Status und ihrer sozialen Situation erst einmal keine besondere Rolle.

Nach dem Sezessionskrieg, der den Sklaven zumindest formal die Freiheit der Berufswahl brachte, strömten viele der ehemaligen Sklaven von den Plantagen im Süden in die Industriezentren im Norden, um dort ihr Geld zu verdienen, ein nicht unbedeutender Teil aber ergriff auch andere „einfache“ Berufe, die bei den Weißen nicht auf besonderes Interesse stießen, dazu zählte z. B. auch der Beruf des Salonmusikers, der zumeist verschiedene populäre musikalische Stile beherrschte. So mischten sich auch immer mehr Schwarze unter die zuvor rein weißen Minstrels. Einige ehemalige Plantagenarbeiter gründeten aber auch gleich nach dem Bürgerkrieg eigene kleine Bands und kauften u. a. die ausgemusterten Instrumentenbestände der recht zahlreichen Militärkapellen auf. Daraus entwickelte sich in den ersten 20 Jahren nach dem Bürgerkrieg eine fortschreitende Dominanz von Schwarzen im Berufsmusikertum, während die weißen Musiker vornehmlich Bereiche wie die „vornehmen“ klassischen Orchester besetzten. Von den Zentren wie New Orleans, das sich schon im 19. Jahrhundert aufgrund vergleichsweise größerer Freiheiten für Afroamerikaner zu einem musikalischen Zentrum entwickelt hatte, und Chicago aus gewannen die schwarzen Musiker so einen bedeutenden Einfluss auf die Entwicklung der populären Musik in den USA. Zu erkennen ist dies z. B. an den zunehmenden Imitationen von „schwarzen“ Kompositionen durch weiße Komponisten in der zweiten Hälfte des 19. Jahrhunderts. Schließlich entwickelte sich gegen Ende des 19. Jahrhunderts der erste von Schwarzen geprägte Musikstil, der quasi zum nationalen „Trend“ wurde: der "Ragtime". Die entstehende Jazz-Musik gilt als erste eigenständige US-amerikanische Form der populären Musik.

Der Ragtime (zu deutsch etwa „Fetzentakt“) entstand in den 1890er Jahren aus, der europäischen Kulturtradition entlehnten, auf die eigene Art interpretierten, Tänzen der Afroamerikaner wie dem Cakewalk, dem Jig oder dem Strut und war ursprünglich eher als Tanzmusik konzipiert, viele frühe Ragtimes tragen auch die Taktbezeichnung „march time“, sind also auch verwandt mit dem aus Europa stammenden Marsch - nicht zuletzt deshalb entstanden bereits um 1885 herum erste Ragtimes weißer Komponisten. Als der bedeutendste Komponist des Ragtime gilt Scott Joplin, dessen erste Stücke 1895 erschienen. Ihm gelang es, aus einer Musik der Bordelle und Kneipen einen allgemein anerkannten, konzertfähigen Stil zu machen, nicht zuletzt durch seine mit dem Genie von Mozart, Chopin und Brahms verglichenen Fähigkeiten am Klavier, dem Instrument des Ragtime und überhaupt dieser Zeit. Ein besonderer Meilenstein in der Musikgeschichte gelang ihm 1899, als er seinen "Maple Leaf Rag" veröffentlichte, dessen „sheet of music“ (engl. für „Notenblatt“ - man nannte die populäre Musik dieser Zeit daher auch „sheet music“) sich innerhalb kürzester Zeit eine Million Mal verkaufte – ein bis dahin nie gesehener Erfolg eines kurzen Unterhaltungsmusikstückes. Der Ragtime ging schließlich ab ca. 1916 im Blues und Jazz auf.

Das bedeutendste Live-Medium für populäre Musik war bis zum ersten Drittel des 20. Jahrhunderts das Theater, vom Boulevardtheater bis zur Music Hall. Reine Konzertmusik gab es kaum. Gedruckte Musiknoten waren zur Hauptsache Erinnerungen an Theatererlebnisse. Auch Tonaufzeichnungen in der Frühgeschichte des Grammophons erfüllten noch diese Funktion. Mit dem Aufkommen des Rundfunks und vor allem des Fernsehens verlor das Theater diese Vorherrschaft.

In den ersten beiden Jahrzehnten des 20. Jahrhunderts hatte sich die Technik zur Aufzeichnung und Reproduktion von Tonaufnahmen soweit entwickelt, dass sie voll kommerziell nutzbar war; abgesehen von der mangelnden Tonqualität war der 1877 entwickelte Phonograph bis dahin noch so teuer, dass sich nur reichere US-Amerikaner ein solches Gerät leisten konnten. 1902 wurden die Caruso-Arien zum ersten weltweiten Schallplatten-„Hit“.

Das Geschäft mit der Musik und den Notenblättern wurde von der sogenannten „Tin Pan Alley“ in New York aus gesteuert, wo die meisten großen Musikverlage dieser Zeit ansässig waren. Deren Aufstieg begann mit der zunehmenden Nachfrage nach den "song sheets" (Notenblätter) und "song books" (Liederbücher) ab den 1890er Jahren, speziell durch die beliebten „Rags“, den Schlagern der Ragtime-Zeit, die jedoch musikalisch nur wenig mit dem instrumentalen Ragtime zu tun hatten. Sie gingen von Revuen wie den Ziegfeld Follies, Vaudevilles, Minstrel Shows oder der Musical Comedy aus.

Die Tin Pan Alley trug entscheidend zur verstärkten Kommerzialisierung der populären Musik in den USA bei: Hier wurde nur das herausgebracht, was mit großer Sicherheit den Massengeschmack eines möglich großen Marktes treffen würde. Wollten Komponisten eher klassische Stücke veröffentlichen, wurden sie hier meist abgewiesen und mussten den Umweg über Europa nehmen oder im Eigenverlag veröffentlichen. Neben den Faktoren der zunehmenden Verbreitung von Phonographen und der wachsenden Beliebtheit der Broadway-Musicals in den 1920er Jahren war die Einführung des Tonfilms in der zweiten Hälfte des Jahrzehnts ein besonders wichtiger Wendepunkt, da nun die Film- und die Musikindustrie zu verschmelzen begannen (z. B. wurde das Filmstudio Warner Brothers als Musikverlag aktiv).

Als die „Erfolgsproduzenten“ der Tin Pan Alley gelten trotz prominenter Konkurrenz wie Irving Berlin und George Gershwin die Produzenten und Komponisten Richard Rodgers und Oscar Hammerstein, die nicht nur mit dem kommerziellen Erfolg ihres Musicals "Oklahoma!", das noch 50 Jahre nach seiner Uraufführung erfolgreich war und als LP zum ersten Mal die Millionen-Absatzmarke übersprang (sowie als Partitur in wenigen Jahren weltweit zwei Millionen mal verkauft wurde), einen Meilenstein setzten, sondern auch als erste Künstler ein selbst beim inflationsbereinigten Vergleich mit heutigen Gagen nur als exorbitant zu bezeichnendes Einkommen von 15 bis 20 Millionen Dollar jährlich erreichten.

Ab etwa 1920 ließen sich auch für die einfache Bevölkerung erschwingliche in der Tonqualität für die damalige Zeit akzeptable Schallplatten und die entsprechenden Abspielgeräte herstellen. Diese Schallplatten wurden in Drogerien und Gemischtwarenläden für einige Cent verkauft, die Abspielgeräte gab es beim Möbelhändler. Besonders interessant für die nicht an das Stromnetz angeschlossene Landbevölkerung waren Kurbelplattenspieler, die in dieser Zeit populär wurden.

Um den neuen Markt zu erschließen, wurden auch Aufnahmen von Minderheitenmusik wie der bald so bezeichnete „Race Music“ der Afroamerikaner und der Old-Time Music der weißen, südstaatlichen Landbevölkerung gemacht. Im Februar 1920 erschien die erste Blues-Schallplatte, aufgenommen von Mamie Smith. Sie verkaufte sich so gut, dass die Musikindustrie plötzlich ein großes Interesse an den schwarzen Blues-Sängerinnen bekam, die bisher nur in den sogenannten Vaudeville-Theatern zu hören waren. Wie nah auch der Blues noch an den Wurzeln aus der Zeit der Sklaverei war, zeigt der neben dem „klassischen“ Blues in dieser Zeit ebenfalls sehr populäre Country Blues, der textlich und musikalisch deutlich den Worksongs und „Field Hollers“ der Plantagenarbeiter ähnelte. Um den Bedarf an Blues-Schallplatten zu decken, wurden spezielle Labels von den Plattenfirmen gegründet, die zu Anfang ausschließlich schwarze Sängerinnen unter Vertrag nahmen – die bekannteste dürfte Bessie Smith sein, bei den männlichen Interpreten hat John Lee Hooker wohl den größten Ruf.

Auch die erst später so bezeichnete Country-Musik, die sich aus verschiedenen volksmusikalischen Stilen der europäischen Einwanderer, besonders der irischen und englischen, entwickelt hatte, wurde ab ca. 1923 als Absatzmarkt entdeckt. Die Geschäftsleute Polk Brookmann und besonders erfolgreich Ralph Peer entdeckten das kommerzielle Potenzial der Musik der abgelegenen Bergregionen der Appalachen. Aufgrund seiner Popularität wurde die Musik zum Teil auch gefördert, um der landesweiten Begeisterung auch vieler Weißer für Ragtime Einhalt zu gebieten. Während der Großen Depression in den 1930er Jahren wurde Country-Musik, die auch bei den Schwarzen der Südstaaten beliebt war, als vereinigende US-amerikanische Musik von staatlicher Seite popularisiert.

Der Swing, der seine Blütezeit (den sogenannten „Swing Craze“) etwa zwischen 1935 und 1945 hatte, war der erste Stil der populären Musik, der die gesamte amerikanische Gesellschaft ohne Unterschiede zwischen schwarz und weiß oder arm und reich erreichte. Dies lag nicht zuletzt an dem auf Tanzbarkeit statt auf „Aussage“ ausgerichteten Charakter dieses Stils. Im gewissen Sinn ist diese Musik außerdem ein Bekenntnis der US-Amerikaner zu Größe und Aufwand, manifestiert durch die Big Bands, die aus doppelt oder dreimal so vielen Musikern bestehen wie übliche Jazz-Formationen. Bei Big Bands mit 14 oder mehr Mitgliedern war die Jazz-typische Kollektivimprovisation praktisch ausgeschlossen, an ihre Stelle traten Soli einzelner Musiker, meist von bekannten „Star-Solisten“. Der Swing enthält gut hörbar Elemente des Jazz, aber auch von „weißen“ Musikstilen, wobei der Anteil der schwarzen Musik am Swing oft unterschätzt wird, da viele der bekannten Big Bands auch aufgrund rassistischer Beschränkungen stark weiß besetzt waren. Der bekannteste schwarze Band-Leader dürfte Duke Ellington sein, mehr bekannte Namen finden sich bei den Weißen wie z. B. Benny Goodman, Jimmy und Tommy Dorsey, Les Brown und natürlich Glenn Miller. Nicht zuletzt bedingt durch den Zweiten Weltkrieg blieb der Swing ein fast ausschließlich US-amerikanisches Phänomen, das lediglich in Großbritannien noch als „Import“ gewisse Verbreitung fand. Die verbreitete Ansicht, dass die 1930er bis 1950er Jahre den künstlerischen Höhepunkt des (gesungenen) populären Musikschaffens in den USA gebracht hätten, kommt im Begriff Great American Songbook zum Ausdruck, mit dem eine nicht genau festgelegte Anzahl herausragender Songs der amerikanischen Unterhaltungsmusik dieser Zeit bezeichnet wird.

Der Rock ’n’ Roll als Musikstil ist eine Synthese aus verschiedenen, unabhängig voneinander entstandenen (regionalen) Stilen, die wichtigsten sind der Rhythm and Blues (kurz R&B) und die Country-Unterstile Western Swing und der Honky Tonk. Der R&B ist im Prinzip ein Blues-Stil, der aber auch Elemente aus speziellen Jazz- und Swing-Stilen enthält und von „Vocal Groups“ mit nur geringfügiger instrumentaler Begleitung (meist nur Gitarre) geprägt wurde. Die bedeutendsten regionalen Formen waren der R&B aus Chicago, der z. B. Chuck Berry beeinflusste, und die New-Orleans-Variante, deren bekanntester Vertreter Fats Domino wurde.

Der Western Swing ist eine Spielart der von der weißen Landbevölkerung der US-Südstaaten geprägten Country-Musik mit Elementen des Swing, die Ende der 1930er Jahre bekannt wurde, besonderen Auftrieb aber erst durch den ASCAP-Streit 1944 bekam. Er beeinflusste bekannte Interpreten wie Bing Crosby oder Bill Haley.

Nicht zuletzt durch den bis dahin in seinem Ausmaß ungekannten Starkult um den „King“ Elvis Presley wurde der Rock ’n’ Roll zum weltweiten Trend, der auch das mittlerweile vom Zweiten Weltkrieg etwas erholte Europa und damit Österreich sowie Deutschland ergriff, wo man sich wieder nach Unterhaltung und („unschuldigen“) Vorbildern sehnte. Außerdem prägte er den Jugendkult in der Popmusik entscheidend mit, da beim Rock ’n’ Roll die Interpreten von den Plattenfirmen erstmals hauptsächlich nach dem Kriterium der Altersnähe zum Zielpublikum ausgesucht wurden und oft nur Amateurmusiker waren. Die Entstehung des Rock ’n’ Roll steht in engem Zusammenhang mit den massiven gesellschaftlichen Umbrüchen dieser Zeit und markiert gemeinsam mit den Beats auch die beginnende Entwicklung der Popkultur.

Zum Rock ’n’ Roll zählte als kurzlebiger Trend der „Twist“ Anfang der 1960er Jahre, der von der Beatmusik der Beatles und Anderer abgelöst wurde, die dieses Jahrzehnt bestimmte. Schließlich folgten die unter der Begriffsabspaltung „Rock“ zusammengefassten Musikstile. Rock ’n’ Roll und nachfolgende Stile haben seitdem vorangehende Stile wie Entertainer und Schlager in der Popularität stark zurückgedrängt und bestimmen die Popmusik-Hitparaden. Seit der Kommerzialisierung des Rock ’n’ Roll (und somit auch der Abschwächung seines widerständigen Potentials) in den späten 1950er Jahren wurden Trends der Popmusik in Europa, aber auch der restlichen Welt nicht nur aus ästhetischen, sondern auch ökonomischen Gründen von den USA aus geprägt.

Seit den 1960er Jahren entwickelten sich unzählige neue Stile und Unterstile der Popmusik. Die Funktion der Stile besteht vor allem im "Unterhaltungsaspekt". Trotzdem gelang es aber auch Musikern wie Bob Dylan, Anfang der 1960er Jahre, eine populäre Synthese aus Musik und politischen Inhalten zu schaffen und wie zum Beispiel John Lennon in seinem Lied" Imagine" eine philosophische Idee zu verbreiten. Mit dem Lied "Houses Burning Down" verarbeitete Jimi Hendrix 1968 aktuelles politisches und gesellschaftliches Zeitgeschehen. Das Lied handelt vom Watts-Aufruhr.

Popmusik war und ist auch stets ein Ausdrucksmittel einer Generation oder eines Milieus und dient zur Vermittlung eines gemeinschaftlichen Lebensgefühls und einer gemeinsamen Ästhetik, die sich z. B. in der Form der Musik und in der Kleidung ausdrückt.

Als Beispiel für einen Milieu-Stil sei hier der Rap genannt, der ursprünglich nur die Musik der schwarzen Jugendlichen in den amerikanischen Großstadt-Ghettos war und dessen Wurzeln bis zum Rhythm & Blues zurückreichen. Erst seit etwa 1990 wurde der Rap von den stets nach neuen Trends suchenden Medienkonzernen zum global populären Musikstil „hochpromotet“, wobei diese Entwicklung nicht nur auf die Musik beschränkt blieb, denn auch der Kleidungsstil der Hip-Hop-Bewegung wurde in den 1990ern zum allgemeinen Modetrend und ist heute fast schon fester Bestandteil unseres Modebewusstseins. Als Beispiele für Generationen erfassende Stile seien hier die Flower-Power-Bewegung und die Disco-Musik der 1970er genannt. Allerdings unterstützen die Musikproduzenten mittlerweile nicht nur Massenbewegungen, sondern auch zwar global verbreitete, aber im Gegensatz zur Musik einer Britney Spears oder Madonna nur von einer kleineren Zielgruppe in kulturellen Nischen nachgefragte Musikstile wie z. B. den Gothic Rock. Dies resultiert aus dem enorm harten Wettbewerb unter den Produzenten, der diese geradezu dazu zwingt, jeden irgendwie Absatz versprechenden Trend auszunutzen oder gar selbst neue, „unverbrauchte“ Trends zu schaffen.

Im Gegensatz dazu ist bei einer anderen Funktion, die die populäre Musik seit je her abdeckt, der individuelle Zuschnitt überhaupt nicht gefragt, sondern hauptsächlich der Rhythmus und die Genussbefriedigung der breiten Masse, Mainstream. Ziel ist hier kein differenzierter ästhetischer Anspruch, sondern die Anregung und Begleitung zum Tanzen. Die bekanntesten Nachfolger des Swing als Tanzmusik dürften, neben dem Twist der frühen 1960er, vor allem die Disco-Musik, die seit Mitte der 1970er zum großen Teil ihren Underground-Charakter verloren hatte und Teile des später populären Techno sein.

Anfang der 1980er Jahre erlebt die deutschsprachige Popmusik im Zuge der Neuen Deutschen Welle einen großen Aufschwung in Deutschland. In Österreich existierte der Austropop, der sich großer Beliebtheit erfreute.

Im Zusammenhang mit der Disco-Musik wird außerdem der Aspekt der Wechselbeziehung zwischen Film und Musik noch einmal interessant, da in der Rock ’n’ Roll-Ära die Musikfilme noch eher die Folge des bereits populären Stils waren, während die Disco-Musik ihren weltweiten Siegeszug infolge des Films "Saturday Night Fever" 1978 antrat. Nicht zuletzt durch diesen Film und seine Nachfolger wurde der Trend verstärkt, zur Musik auch Bilder zu liefern (Musik und dazugehörige Bilder gab es bereits seit den Nickelodeons). Das bedeutendste Datum in diesem Zusammenhang ist der 1. August 1981, als in den USA der erste Spartenfernsehkanal nur für Musikvideos auf Sendung ging: MTV. Ab sofort wurde kaum ein Popmusiker zum Star, zu dessen Songs es nicht ein Video gab. Entsprechend bedingen sich der größte Popstar und der größte Videokünstler der 1980er jeweils gegenseitig – Michael Jackson. Dabei sind die Videos keineswegs notwendige Bestandteile der Popmusik – sie kommt auch ohne sie aus –, sondern vielmehr so etwas wie Werbespots, die mit möglichst intensiven, ungewöhnlichen oder spektakulären Bildern auf den Künstler und sein Produkt aufmerksam machen sollen.

Auch wenn man den Eindruck gewinnen könnte, die Popmusik stagniere langsam aber sicher in ihrer Entwicklung, so bahnen sich doch im Zusammenhang mit dem Fortschritt der Kommunikations- und Computertechnik bedeutende Veränderungen für die Zukunft an. So wie die Entwicklung der elektronischen Verstärkung und Nachbearbeitung, des Synthesizers, der digitalen Aufnahme usw. den Klang der Musik veränderte, so werden sicherlich auch die neuen Verbreitungsmöglichkeiten durch das Internet die gegenwärtige Form der Musikproduktion entscheidend verändern. Die seit um 1900 bekannten Singles und die später hinzugekommenen Alben könnten bald Geschichte sein, da weniger Kunden Geld für einen Datenträger ausgeben, auf dem nur die Musik einzelner Interpreten gespeichert ist, wenn einzelne interessante Stücke einfach kostenlos über Filesharing wie Peer-to-Peer-Netzwerke oder gegen Entgelt bei kommerziellen Anbietern aus dem Internet heruntergeladen werden können. Angesichts der schier unglaublichen Fülle an Musiktiteln im Netz gewinnt auch die Möglichkeit, Musikstücke zu konsumieren, ohne sie dauerhaft auf dem eigenen Rechner zu speichern (z. B. über Musikstreaming-Angebote), zunehmend an Bedeutung.

Politische Inhalte haben in der Popmusik eine lange Tradition. Neben den Themen Liebe, Sex und Partnerschaft waren auch soziale und politische Motive immer schon, wenn auch mit unterschiedlicher Intensität, unterschiedlichem künstlerischem Gehalt sowie aus unterschiedlichsten Gründen in der Populärmusik vertreten. Allgemein wird aber der Wirkungsgrad politisch motivierter Musik von Seiten der Medien und der Wissenschaft gerne unterschätzt.

Tatsächlich aber zählt die politische Popmusik zu einem der bedeutendsten Bereiche im Rahmen der Unterhaltungsmusik, von dem eine klare ästhetische Wirkung ausgeht. Dabei darf man nicht den Begriff der politischen Ästhetik mit der ästhetischen Wirkung politisch orientierter Popmusik verwechseln, wenngleich in beiden Bereichen oft mit ähnlichen Mitteln operiert wird. Sowohl in der politischen Ästhetik wie auch im Bereich der Ästhetik politisch orientierter Popmusik wird mit den Mitteln der Reklametechnik und der Suggestion gearbeitet. Beide ästhetische Formen sind auf Massenwirkung bedacht, mit dem Ziel, eine emotionale Identifikation der angesprochenen Massen mit den Inhalten der Politiker und Musiker aufzubauen. Freilich unterscheiden sich die Ziele gewaltig. Während die Politiker trachten, mit den Mitteln der Ästhetik einerseits die Ehrfurcht der Bürger zu erwecken und zu festigen, und andererseits eine Bereitschaft zur Unterstützung einer Partei oder einer Person hervorrufen wollen, steht bei der politisch motivierten Popmusik häufig die Selbstdarstellung der Künstler im Vordergrund.

Die Musiker verkünden zwar eine Botschaft, deren Inhalte aber zumeist sekundär sind; entscheidend ist der von ihr ausgehende Mythos. Die Popmusik folgt dabei exakt den Strukturen der Mythen des Alltags, indem sie immer auf schon Vorhandenes, Bekanntes zurückgreift. Sie ist damit niemals authentisch, sondern plagiatorisch. Die Popmusik hat daher immer Anleihen bei anderen Musikstilen gemacht.

„Die Popmusik benutzt die Aura authentischer Musikformen, um mit ihnen eine ganz andere Aussage zu verschlüsseln. Das Gleiche gilt für die Texte. Sie sind latent poetisch, aber selten authentisch poetisch, sie sind latent ideologisch, aber selten offen ideologisch, sie sind latent politisch, aber nie wirklich politisch, sie sind latent gesellschaftskritisch, jedoch niemals der Sache auf den Grund gehend.“

Genau dieser Umstand aber wird von Musikwissenschaftlern und Journalisten heftig kritisiert. Dabei werden offensichtlich die Voraussetzungen für die Entstehung von Popmusik vergessen. Die Musiker trachten, mit ihrer Musik ein möglichst großes Publikumsinteresse zu wecken. Dies kann allerdings nur gelingen, indem das Publikum nicht vor vollendete Tatsachen gestellt wird. Die Musik soll die Fantasie der Rezipienten anregen, Aussagen mit uneingeschränktem Wahrheitscharakter würden das Publikum bald langweilen.

Diese Vorgangsweise bedeutet nicht, dass politische Inhalte dabei auf der Strecke bleiben müssen, sondern lediglich, dass die Musiker trachten, dem Publikum einen Spielraum zur Interpretation der ausgehenden Botschaften einzuräumen. Die Vertreter der Popmusik können sehr wohl systemkritisch agieren, würden sie jedoch systemzerstörend agieren, dann würden sie sich ihre eigene (Erwerbs-)Grundlage entziehen.

Die Grenzen der Popmusik in diesem Bereich sind also klar abgesteckt; auch wenn die Musiker sich noch so respektlos dem Establishment gegenüber verhalten, so wissen sie doch, dass sie Teil desselben sind. Obwohl dieser Umstand die künstlerische Freiheit einzuengen scheint, agieren politische Popgruppen sehr erfolgreich in der internationalen Musikszene.

Im Idealfall gelingt den Vertretern der Popmusik nämlich die Erziehung des Publikums zu politischem Bewusstsein, ohne gleichzeitig Lösungen von musikalisch behandelten politischen Themen präsentieren zu müssen. Würden die Musiker tatsächlich immer Lösungen präsentieren, wären sie schon bald keine Musiker mehr, sondern vielmehr im Bereich der (Partei-)Politik wiederzufinden.

Bono, Sänger der irischen Popgruppe U2, tätigte in diesem Zusammenhang einen Ausspruch, der wie kein anderer die heillose Überforderung der Musiker an der übertriebenen Erwartungshaltung des Publikums wie der Medien aufzeigt: „Es ist verdammt gefährlich, pausenlos als Sprachrohr einer Generation hingestellt zu werden, wenn man nichts anderes zu sagen hat als 'Hilfe!' Aber mehr sagen wir mit unserer Musik nicht. Fragt uns nicht pausenlos nach Antworten, wir können sie euch nicht geben. Alles was wir tun können, ist, die richtigen Fragen zu stellen!“

Die Frage ist demnach nicht wie politisch die Populärmusik ist, sondern ob sie politisch ist, und ob sie überhaupt ein geeignetes Medium zur Verbreitung politischer Inhalte darstellt. Letzteres kann wohl eindeutig bejaht werden, liegt doch die Stärke der Popsongs gerade in ihrer Kommunikabilität. „Songs can communicate even to those who can't read and write and, at their best, can inspire, console, dodge round censors and frontiers, summarize a political mood, or as that most political star of the salsa world, RUBEN BLADES, put it, 'tell people they are not alone'.“

Die Musiker erkannten schon recht bald, dass sie dadurch zu einer privilegierten Bevölkerungsschicht gehörten; mit ihrer Arbeit konnten sie relativ einfach eine große Zahl von Menschen erreichen. Es war ihnen aufgrund ihrer Popularität daher möglich, einen großen Einfluss auf politische wie gesellschaftliche Entscheidungen auszuüben.

Es ist nicht überraschend, dass sich die Musiker in den Anfängen der Popmusik hauptsächlich mit Problemen ihres näheren sozialen Umfelds beschäftigten. Da die Popmusik häufig von jungen Künstlern gemacht wurde, ging die Entwicklung der politischen Popmusik auch mit der Entwicklung der politischen Jugendbewegungen einher.

Ihren Ursprung hatte die politische Popmusik daher auch in den 1960er Jahren mit dem Aufkommen der Flowerpower-Bewegung. Doch schon zuvor waren Sänger wie Woody Guthrie oder Pete Seeger durch ihre kritische Hinterfragung der modernen Gesellschaftsform maßgeblich an der Entstehung des Political Pop beteiligt. Ihr bekanntester Protagonist aber war Bob Dylan, der Erneuerer des Protestsongs.

Mit den Erfolgen von Sängern wie Woody Guthrie, Bob Dylan und eben Pete Seeger, die sich neben dem Thema Krieg auch mit Fragen der Menschenrechte, der Umweltpolitik, der Kernenergie und der Rassentrennung auseinandersetzten, etablierten sich politische Anliegen in der Popmusik zusehends. Interessant an dieser Entwicklung ist, dass die politisch orientierten Musiker und Bands und ihre Botschaften bis heute allgemein dem linken politischen Lager zugerechnet werden.

Tatsächlich haben die politischen Anliegen der Pioniere des Polit-Pop, wie etwa die Bekämpfung der Arbeitslosigkeit und Aufhebung der Klassenschranken, bis heute nicht ihre Bedeutung verloren. Gerade in Großbritannien stoßen diese Themen beim Publikum noch immer auf große Gegenliebe. Bands wie Simple Minds oder U2 sowie Musiker wie Sting und Peter Gabriel haben nicht zuletzt aus diesem Grund großen Anklang beim Publikum gefunden.

Trotz der wachsenden Zahl politisch interessierter Musiker wurde der Wirkungsgrad, den die politische Popmusik beim Publikum erreichen konnte, konsequent unterschätzt. In Politikerkreisen wollte man noch bis in die 1980er Jahre den engagierten Arbeiten der Musiker keine realpolitischen Auswirkungen zugestehen. Dabei gab es schon damals wahrlich genug Beispiele für politische Änderungen, die durch Popmusiker hervorgerufen wurden.

1982 etwa legte Stevie Wonder anlässlich einer politischen Demonstration vor über 50.000 Menschen in Washington mit seiner Ansprache den Grundstein für die Einrichtung des sogenannten Martin-Luther-King-Feiertages in Amerika, als er meinte: „… we need a day to celebrate our work on an unfinished symphony, a day for a dress rehearsal for our solidarity.“ Obwohl dieser Feiertag, der erste in den USA überhaupt, der einem Schwarzen gewidmet war, nicht seine Idee war, war es die Popularität Stevie Wonders, der Politiker wie Reverend Jesse Jackson und John Conyers die Durchsetzung dieses Anliegens zu verdanken hatten.

Gerade die schwarzen US-Musiker gaben dem Begriff des politischen Pop eine neue Bedeutung. „The tradition had grown up in the days of slavery, when the blues and gospel provided one uncontrollable outlet for black expression. No one could stop the music in the fields or in the churches, and black America's preachers haven't forgotten what a powerfull medium a song can be; …. 'Songs', said Reverend Cecil Franklin, 'have the advantage of being packaged and wrepped in universal appeal. Songs are not limited by natural or human boundaries.'“ Diese alte Tradition politischer Inhalte in der Musik schwarzer Amerikaner findet heute in der Stilrichtung Rap seine Fortsetzung. Rap-Musik stellt heute eine der wichtigsten Formen politischer Artikulation der schwarzen Bevölkerung in den USA dar.

Ein anderes Beispiel, welch ungeheure Auswirkungen die Arbeiten von Popmusikern auf politischer Ebene haben können, ist zweifellos das Lebenswerk von Bob Marley. Nicht nur, dass es mit der Person Bob Marley zum ersten Mal ein Musiker aus einem Land der Dritten Welt schaffte, sich in der Liga der westlichen Superstars zu etablieren, viele seiner Songs wurden auch zum Symbol einer zukunftsträchtigen politischen Vision.

In Großbritannien inspirierte Bob Marley mit seiner Reggae-Musik die Rock-Against-Racism (RAR)- Bewegung, die bis heute als Forum für politische Anliegen musikinteressierter Jugendlicher sehr erfolgreich agiert. Die Stilrichtung der Reggae-Musik fand überhaupt, nicht zuletzt aufgrund ihres bekanntesten Vertreters Bob Marley, in England großen Anklang. Bands wie etwa UB40 oder The Police beschäftigten sich in vielen ihrer Songs mit dieser Stilrichtung. Wenngleich auch der Reggae in der weißen Popmusik immer mehr von der Stilrichtung zum Stilmittel wurde, besteht durch die Verbreitung dieses Musikstils durch weiße Musiker dennoch die Hoffnung, dass der ursprüngliche politische Charakter dieser Musik nicht zu schnell in Vergessenheit gerät.

Am 13. Juli 1985 wurde das bis dahin größte Popfestival der Geschichte veranstaltet. Live Aid war ein Benefizkonzert, das vom Musiker Bob Geldof organisiert wurde, nachdem er einen Fernsehfilm über die hungernde Bevölkerung Äthiopiens gesehen hatte. Nachdem Geldof den Sänger der Band Ultravox, Midge Ure, für die Idee gewinnen konnte, bekundeten immer mehr Musikstars ihr Interesse an diesem Ereignis. Letztlich waren beinahe alle relevanten Popmusiker dieser Zeit bei Live Aid vertreten. Zugunsten der Hilfe für die Menschen verzichteten alle Beteiligten auf ein Honorar. Bis Mitte 1987 hatte das Ereignis etwa 60 Millionen englische Pfund eingebracht.

1988 kam es mit dem „A Tribute To Nelson Mandela“-Konzert zu einem der bedeutendsten politischen Manifestationen in der Geschichte der Popmusik. Songs wie "Mandela Day" von den Simple Minds, "Brothers in Arms" von den Dire Straits, Tracy Chapmans "Talkin' 'Bout a Revolution" oder Peter Gabriels "Biko" wurden zum Symbol des neuen politischen Verständnisses unter den jugendlichen Hörern. 20 Monate nach dem Konzert wurde Nelson Mandela aus der Haft entlassen.

Nach den Erfolgen von Live Aid und A Tribute To Nelson Mandela kam es in der Folge zu einer beinahe inflationären Zahl ähnlicher Ereignisse, sodass sich bald eine große Zahl bekannter Stars gegen eine Überstrapazierung dieser Idee starkmachte, um zu vermeiden, dass die politische Botschaft solcher Veranstaltungen zugunsten individueller Promotioninteressen einiger aufstrebender Talente in den Hintergrund geraten könnte. Nicht zuletzt hatte auch die Industrie die Werbewirksamkeit dieser Großveranstaltungen entdeckt, was Ende der 1980er Jahre viele bekannte Musikgrößen abschreckte, weiter bedenkenlos an jedem dieser Benefizkonzerte teilzunehmen. Als Zeichen politischen Widerstands behielt das Popkonzert jedoch seine ernstzunehmende Bedeutung. Atomkraftwerkspolitik, Umweltzerstörung sowie Drogenproblematik sind die neueren Themen des politischen Pops.

Im Laufe der Entwicklung des politischen Pop kam es auch immer wieder zu Missverständnissen und Vereinnahmungen der Musiker von Seiten einiger politischer Repräsentanten. Ein Beispiel dafür ist die irische Band U2, die oft mit der Irisch Republikanischen Armee (IRA) in Verbindung gebracht wurde. Dies nicht zuletzt aufgrund der Tatsache, dass Repräsentanten der IRA immer wieder auf die geistige Nähe einiger U2-Texte zu den Ideen der terroristischen Organisation hinwiesen. Konkreter Anlass für solche Spekulationen war der Song "Sunday Bloody Sunday", dessen Text die IRA-Mitglieder als Aufruf zum Widerstand gegen die englischen Besatzer in Nordirland missverstanden. Die Mitglieder der Band bestritten dies mehrmals öffentlich, als die Gerüchte aber nicht verstummen wollten, hielt Bono während eines Konzertes in den USA quasi als Einleitung zu dem Song fest, dass sie nicht für den bewaffneten Widerstand, sondern vielmehr gegen jede gewalttätige Konfliktlösung sind.

Auch Bruce Springsteen blieb von einer solchen Vereinnahmung nicht verschont. Sein Hit "Born in the U.S.A.", der eigentlich eine kritische Position bezüglich der Rolle Amerikas im Vietnamkrieg einnehmen sollte, wurde zu einer amerikanischen Hymne hochstilisiert. Springsteen fühlte sich völlig missverstanden, als sein Song zum musikalischen Symbol der Reagan-Ära gemacht wurde, erachtete er doch selbst den Text als eindeutig. Nach den allgemeinen Gründen für das politische Engagement von Popmusikern befragt, meinte Jim Kerr, Sänger der schottischen Band Simple Minds: „Wir Musiker haben durch unsere Popularität eine Menge Spielraum für die Artikulation unserer Gedanken, es wäre eine Schande, eine solche Gelegenheit nicht zu nutzen.“

Aus folgenden Genres gibt es Musikstücke, die der Popular- und Popmusik zuzurechnen sind:


Musik, die sich aus rein kommerziellen Gesichtspunkten heraus aus den Genres der populären Musik eklektisch bedient (vor allem Blasmusik, populäre Klassik und Musical) und dies mit Volkstümlichen Schlagern anreichert, nennt man volkstümliche Musik.





</doc>
<doc id="3824" url="https://de.wikipedia.org/wiki?curid=3824" title="Die Pyramide (Fernsehsendung)">
Die Pyramide (Fernsehsendung)

Die Pyramide ist eine Quizsendung, die vom 16. März 1979 bis zum Oktober 1994 im ZDF ausgestrahlt wurde. Der Moderator war Dieter Thomas Heck, und die Titelmusik "(Die Pyramide)" der deutschen Show stammt von Gershon Kingsley. 2012 wurde im ZDF eine Neuauflage der Sendung ausgestrahlt.

Die deutsche Version basiert auf der US-amerikanischen Spielshow "The $10,000 Pyramid", die von Bob Stewart entwickelt wurde, der auch das Konzept von "Der Preis ist heiß" und "Sag die Wahrheit" entwickelte. Die Sendung wurde 1973 zunächst von CBS ausgestrahlt, aber erst bei ABC zum Quotenerfolg. Auf verschiedenen Sendern wird die Quizshow, die im Laufe der Jahre als "The $25,000 Pyramid", "The $50,000 Pyramid" und "The $100,000 Pyramid" Erfolg hatte, in den USA bis heute ausgestrahlt.

Von 1996 bis 1999 lief bei dem deutschen Privatsender Sat.1 die Spielshow "Hast Du Worte" nach dem gleichen Spielprinzip. Die Sendung wurde anfangs von Jörg Pilawa und später von Thomas Koschwitz moderiert.

Im August 2012 startete die Neuauflage von "Die Pyramide", zunächst im Vorabendprogramm von ZDFneo, später auch im Nachmittagsprogramm des ZDF.

Jede Staffel wurde meist einige Monate vor der Ausstrahlung im Fernsehstudio München (FSM) in Unterföhring aufgezeichnet. So wurde z. B. mit den prominenten Kandidaten Heidi Brühl und Reinhard Mey (auch Musikauftritt) eine 45-minütige Pilotfolge produziert, die aber erstmals am 3. April 2012 auf ZDFkultur ausgestrahlt wurde. Die Sendung wurde in unregelmäßigen Abständen, manchmal aber auch im wöchentlichen Rhythmus, an verschiedenen Wochentagen ausgestrahlt.

Ziels des Spiels, das über mehrere Runden geht, ist, dass sich zwei Kandidaten in dreißig Sekunden sieben Begriffe auf verschiedene Art und Weise (Pantomime, Begriffsumschreibungen) erklären müssen, ohne Teile des Begriffs selbst zu verwenden.

In der ZDF-Ausgabe wurden zwei Teams aus jeweils einem Prominenten und einem Zuschauerkandidaten gebildet. Gespielt wurden pro Sendung insgesamt drei Pyramiden-Runden. Es wurden zunächst zwischen beiden Kandidatenpaaren jeweils drei Vorrunden abwechselnd gespielt, bei denen die Hände für Gestiken benutzt werden durften. Es standen allerdings auch nur 30 Sekunden Zeit zur Verfügung, um sieben Begriffe zu erraten. Wurde bei einem der Begriffe gegen die Vorschrift, keine Bestandteile des Lösungswortes zu verraten, verstoßen, schied der Begriff aus; die Runde lief jedoch weiter bis zum zeitlichen Ende, wenn noch zu ratende Begriffe vorhanden waren. Schaffte es das Kandidatenpaar, alle sieben Begriffe einer Vorrunde korrekt zu erraten, gab es 200 DM extra.

Das Kandidatenpaar, welches die meisten Begriffe in den drei Vorrunden erriet, konnte schließlich um die entsprechende Pyramide spielen. Bei Gleichstand wurde eine Stichfrage gestellt. Schließlich wurde die Spielhälfte der siegreichen Kandidaten der Vorrunde in die Mitte des Feldes geschoben, eine Pyramidenanimation erschien und innerhalb 60 Sekunden mussten sechs Begriffe erraten werden.

Die Pyramiden, um die gespielt wurde, waren dabei unterschiedlich dotiert: die erste Pyramide mit 500 DM, die zweite mit 1000 DM und die dritte mit 1500 DM. Die zu erratenden Begriffe waren dabei auch nicht mit stets gleichen, sondern mit steigenden DM-Beträgen gewertet: 50 DM für die ersten drei, 100 DM für die nächsten zwei, 150 DM für den letzten Begriff (insgesamt 500 DM) in der ersten Pyramide; analog dreimal 100 DM, zweimal 200 DM, einmal 300 DM (insgesamt 1000 DM) in der zweiten Pyramide; und dito dreimal 150 DM, zweimal 300 DM, einmal 450 DM (insgesamt 1500 DM) in der dritten Pyramide.

In der Pyramidenrunde waren Gestiken jedoch fast gar nicht möglich, da der Hinweisgeber seine Hände in zwei Schlaufen stecken musste. Fatal bei dieser Runde: wenn gegen die bereits erwähnte Regel bei Umschreibungen verstoßen wurde, war die Runde sofort zu Ende und nicht erst nach 60 Sekunden.

In strittigen Situationen (meist einmal pro Sendung) schaltete sich der Oberschiedsrichter und Münchner Rechtsanwalt Josef Heindl über Telefon ein. In der „Halbzeit“ kam ein Musiktitel, welcher hauptsächlich grob in die Kategorie Schlager einzuordnen war und zumeist auf Deutsch gesungen wurde.

Die Gewinne der Prominenten wurden drei Sendungen lang gesammelt und dann für einen guten Zweck gespendet (der jeweilige Fall wurde von Heck kurz geschildert, jedoch die betroffene Person namentlich nicht erwähnt). Die Zuschauerkandidaten durften ihr Geld behalten.

Das Spiel wurde 1987 auch einmal innerhalb der ebenfalls von Dieter Thomas Heck moderierten Ausgabe der "Super-Hitparade" zum Jubiläum "10 Jahre „Ein Herz für Kinder“" von Kindern gespielt und bei der einmaligen Ausgabe von "Guten Abend, Deutschland", einer Gemeinschaftsproduktion von ZDF und DFF am 6. Mai 1990 aus dem Friedrichstadt-Palast in Berlin, lief das Quiz sogar live. Es nahmen jedoch nur Prominente teil; ein Team bestand aus zwei Vertretern vom DFF und das andere entsprechend vom ZDF. Da für einen guten Zweck gespielt wurde und man eine möglichst hohe Summe erreichen wollte, schienen die Kandidaten bereits die Antworten zu kennen:

Umschreibung der ZDF-Prominenten: „Das gab’s bei uns nicht, aber das gab’s in der DDR und gibt’s jetzt auch nicht mehr …“ – (richtige) Antwort: „Staatssicherheit“.

Hier wurde dann auch wieder scheinbar Herr Heindl ins Spiel gebracht, im Gegensatz zu sonstigen Ausgaben der „Pyramide“ war seine Stimme nicht zu vernehmen. Dieter Thomas Heck begrüßte ihn diesmal mit den Worten "„Herr Heindl, wie haben Sie es denn geschafft, hier durchzukommen?“" – als Anspielung auf das seinerzeit marode Telefonnetz der DDR.

Das ständige Wortduell der beiden vermeintlichen Streithähne wurde zum festen Bestandteil der Show. Da jedoch niemand Herrn Heindl sehen konnte, da dieser ja in einem Büro im Studio saß und immer nur über das rote Telefon zu hören war, wurde ein Preisausschreiben zu wohltätigen Zwecken veranstaltet, bei dem Kinder versuchen sollten, Herrn Heindl so zu malen, wie sie ihn sich vorstellen. Hierbei kam es zu tausenden von Einsendungen.

In der folgenden Show kam dann Herr Heindl auf die Bühne und verkündete selbst den Gewinner.

Heute gibt es einige Gesellschaftsspiele (Brett- und Kartenspiele), die diesem Vorbild folgen, beispielsweise Activity.

Quelle: u. a. Archiv des Hamburger Abendblatts, Online-Archiv der Wiener Arbeiter-Zeitung, ZDF-Programmservice

Eine Neuauflage der Show, die seit 2012 produziert wurde, moderierte Micky Beisenherz. Joachim Llambi bekleidete die Position des Schiedsrichters, der, im Gegensatz zum Original, in der Neuauflage öffentlich zu sehen war. Es wurden nur noch zwei Vorrunden und zwei Pyramiden gespielt, wobei die erste mit 5.000 Euro und die zweite mit 10.000 Euro dotiert war. Die Sendung lief zwecks Resonanz- und Quotentest bereits seit dem 6. August 2012 wochentags um 18:45 Uhr als werbefreie Vorabausstrahlung auf dem digitalen Spartenkanal zdf neo. Seit dem 27. August 2012 wurde sie regulär um 16:15 Uhr im ZDF ausgestrahlt und dort einmal durch einen kurzen Werbeblock und Programmhinweise unterbrochen. Wegen zu schwacher Quoten wurde die Sendung ab dem 17. September 2012 ins Nachtprogramm verlegt und inzwischen eingestellt.

Quelle:



</doc>
<doc id="3827" url="https://de.wikipedia.org/wiki?curid=3827" title="Gemeinfreiheit">
Gemeinfreiheit

Der Gemeinfreiheit unterliegen alle geistigen Schöpfungen, an denen keine Immaterialgüterrechte, insbesondere kein Urheberrecht, bestehen. Die im anglo-amerikanischen Raum anzutreffende Public Domain (PD) ist ähnlich, aber nicht identisch mit der europäischen Gemeinfreiheit. Nach dem Schutzlandprinzip bestimmt sich die Gemeinfreiheit immer nach der jeweiligen nationalen Rechtsordnung, in der eine Nutzung vorgenommen wird.

Gemeinfreie Güter können von jedermann ohne eine Genehmigung oder Zahlungsverpflichtung zu jedem beliebigen Zweck verwendet werden. Wer Immaterialgüterrechte geltend macht (Schutzrechtsberühmung), obwohl das Gut in Wahrheit gemeinfrei ist, kann Gegenansprüche des zu Unrecht in Anspruch Genommenen auslösen.

Der Begriff der Gemeinfreiheit wird vor allem in Bezug auf Urheberrechte benutzt, in Bezug auf andere Immaterialgüterrechte sind Begriffe wie "Freihaltebedürfnis" im Markenrecht oder "Freier Stand der Technik" und "naheliegende Weiterentwicklung" im Patentrecht üblich. Im gewerblichen Feld wird auch von "Wettbewerbsfreiheit" gesprochen. Sie fallen alle unter die Gemeinfreiheit im weiteren Sinne.

Die Gemeinfreiheit ist die Grundnorm allen Wissens und aller geistigen Schöpfungen. Von der Nutzung gemeinfreier Güter kann niemand ausgeschlossen werden, die Nutzung durch eine Person verhindert nicht, dass andere dasselbe gemeinfreie Gut nutzen: Sie ist nicht exklusiv und nicht rivalisierend.

Verschiedene Bereiche wirken in der Gemeinfreiheit zusammen: Ökonomisch sind gemeinfreie Güter nicht knapp und da die Nutzung nicht-rivalisierend ist, ergeben sich auch bei intensivem Zugriff auf gemeinfreie Güter positive Externalitäten. Demokratische, rechtsstaatliche Funktionen zeigen sich bei amtlichen Werken. Diese müssen gemeinfrei sein und eine möglichst weite Verbreitung anstreben, da ihre Kenntnis Voraussetzung für das Funktionieren der Gesellschaft und des Staates ist. Kulturell ist Gemeinfreiheit im Bereich Bildung und Wissenschaft angelegt, Ideen und Wissen können nicht geschützt und damit monopolisiert werden. Eine Weiterentwicklung der Wissenschaft setzt den Zugang zum aktuellen Stand voraus. In der Kunst ist der kulturelle Grundbestand der nicht mehr geschützten Werke das gemeinschaftliche kulturelle Erbe der Menschheit. Daraus, aber auch aus Reflexionen und Kritik ergibt sich die Inspiration für neue Werke.

Die Gemeinfreiheit, als Abwesenheit von Immaterialgüterrechten, ist ein Feld des offenen Wettbewerbs. Reto M. Hilty stellt fest, dass dieser Kreativität und Wachstum fördert. Der Eingriff in den Wettbewerb mit einem Monopolrecht muss daher immer begründet werden und kann keinesfalls Selbstzweck sein. Die plakative These „Mehr Schutz = mehr Kreativität“ weist er ausdrücklich zurück. Gemeinfreiheit ist Ausdruck der allgemeinen Handlungsfreiheit und kann nur durch gesetzliche Regelungen beschränkt werden. Die Immaterialgüterrechte sind solche gesetzlichen Regelungen.

Die herrschende Meinung sieht einen Gleichrang von Gemeinfreiheit und Immaterialgüterrechten und strebt daher ein ausgewogenes Verhältnis zwischen beiden an. Rechtsdogmatisch wird dagegen das Regel-Ausnahme-Verhältnis vorgebracht, nach dem die Gemeinfreiheit Vorrang genießt, „die erstmalige Gewährung von Immaterialgüterrechten ist rechtfertigungbedürftig.“

Auf dieser Grundlage kann Gemeinfreiheit in verschiedenen Formen begründet sein:


In konkreten Anwendungsbereichen können auch Schranken des Urheberrechts die Wirkung der Gemeinfreiheit entfalten.

Das Urheberrecht und andere Immaterialgüterrechte schützt nur Werke, nicht jedoch jede geistige Schöpfung. Voraussetzungen sind zum einen, dass die Schöpfung in einer konkreten Form verkörpert ist, also über eine Idee hinausgeht, und auch nur diese Form geschützt ist, und zum anderen ist eine gewisse Schwelle an Individualität oder Originalität erforderlich, da ein Sockel aus Basiswissen, Gestaltungsprinzipien und einfachen Leistungen für jedermann zur Verfügung stehen muss. Auch kleine, naheliegende Innovationen sind als routinemäßige Weiterentwicklungen nicht schutzfähig. Derartige Schöpfungen und Leistungen unterliegen direkt der Gemeinfreiheit.

Alle Immaterialgüterrechte, die als Schutz von Innovationen angelegt sind, haben nur eine begrenzte Laufzeit. Die Dauer des Schutzes unterscheidet sich nach den verschiedenen Schutzarten und richtet sich nach deren Regelungen. Eine Leistung wird nach der Regelschutzfrist mit Ablauf des Schutzes gemeinfrei.

Dabei ist jedoch an Urheberpersönlichkeitsrechte zu denken, die etwa im französischen Urheberrecht als ewiges "droit moral" dauerhaft fortbestehen.

Eine Ausnahme sind Marken, die unbegrenzt verlängert werden können, solange sie im Markt benutzt werden.

Auf die Mehrzahl der Immaterialgüterrechte kann nach Belieben des Schöpfers verzichtet werden. Patente müssen ausdrücklich angemeldet werden, Designs eingetragen. Bei Leistungen, die in einem Arbeitsverhältnis erbracht werden, sind jedoch gegebenenfalls die Regelungen des Arbeitnehmererfindungsgesetzes zu prüfen.

Nach deutschem und österreichischem Recht ist umstritten, ob ein Totalverzicht auf das Urheberrecht zugunsten der Allgemeinheit möglich ist. Die wohl herrschende Meinung schließt dies unter Berufung auf UrhG-D bzw. UrhG-Ö aus. Daher gibt es dort keine Gemeinfreiheit durch Rechteverzicht wie in den USA, wo auf alle Rechte verzichtet werden kann und das Public-Domain-Werk den gleichen Status besitzt wie ein noch nie oder nicht mehr geschütztes Werk. Problematisch ist diese Position insbesondere mit Blick auf verwaiste Werke, die urheberrechtlich geschützt bleiben, aber für eine legale, lizenzierte Verwendung unzugänglich bleiben. Nach einer anderen Ansicht dient das Verbot des Verzichts auf das Urheberrecht nur dem Schutz des Urhebers vor Ausbeutung bei einer Übertragung von Urheber- und Nutzungsrechten auf einen Dritten. Bei Aufgabe zugunsten der Allgemeinheit gibt es keinen einzelnen Begünstigten und daher auch keine Ausbeutung. Diese Auslegung hält die Entlassung eines Werkes in die Gemeinfreiheit auch nach deutschem Urheberrecht für zulässig und argumentiert unter anderem mit der Gesetzesbegründung bei der Einführung der Linux-Klausel.

In jedem Fall ist es möglich, das Werk unter einem solchen Nutzungsrecht zur Verfügung zu stellen, dass es von jedermann frei veränderbar ist - durch eine freie Lizenz. Zur Kennzeichnung der Freigabe weitest möglicher Nutzungsrechte unter Verzicht auf eine Vergütung wurde von der Organisation Creative Commons die CC Zero-Lizenz erstellt.

In den USA wurde Mitte der 2000er Jahre das Public Domain Enhancement Act diskutiert. Nach diesem Vorschlag würde jedes urheberrechtlich geschützte Werk, für welches nach Ablauf von 50 Jahren keine symbolische Gebühr bezahlt wird, unwiderruflich in die Gemeinfreiheit fallen. Dies würde nicht nur das Problem verwaister Werke lösen, sondern auch die Gemeinfreiheit stärken.

Die Schranken der Immaterialgüterrechte erlauben die freie Benutzung von ansonsten geschützten Leistungen in einem bestimmten Kontext. Innerhalb dieser Grenzen kann die Leistung genutzt werden, als wäre sie gemeinfrei.

So sind amtliche Werke nach deutschem Recht gemeinfrei; in den Vereinigten Staaten geht diese Regel noch weiter: alle Leistungen von Angehörigen der Bundesregierung, die diese in Ausübung ihres Dienstes erbringen, sind unmittelbar in der "Public Domain".

Zu Zwecken der Rechtspflege und öffentlichen Sicherheit können alle urheberrechtlich geschützten Werke in Deutschland verwendet werden.

Die freie Benutzung noch geschützter Werke ist zulässig, wenn die persönlichen Züge des Originalwerkes verblassen und die des neuen Urhebers in den Vordergrund treten.

Der Rechtsbegriff "Public Domain" steht im angelsächsischen Common Law für „frei von Urheberrechten“. Die Bedeutung englischer Begriffe wie „Copyright“ und „Public Domain“ kann nicht ohne weiteres auf die deutschen Begriffe „Urheberrecht“ und „Gemeinfreiheit“ übertragen werden.

So kennt das angelsächsische Copyright kein ausdrückliches Urheberpersönlichkeitsrecht, das in kontinentaleuropäischen Rechtsordnungen dazu führen kann, dass trotz Gemeinfreiheit einer Schöpfung bestimmte Nutzungsformen im Einzelfall als Verletzung von Persönlichkeitsrechten des Urhebers unzulässig sein können; in Frankreich sogar mit ewiger Dauer. Aus demselben Grund ist eine Aufgabe des Copyrights und die Entlassung eines Werkes in die Public Domain unproblematisch, während sie in Kontinentaleuropa umstritten und nach der herrschenden Meinung unzulässig ist.

Das rechtliche Prinzip des Copylefts ist nicht vereinbar mit dem der Gemeinfreiheit, da Copyleft auf dem Urheberrecht aufbaut, anstatt wie die Gemeinfreiheit darauf zu verzichten. Die Motivation hinter Copyleft-Lizenzen ist jedoch ähnlich der von gemeinfreien Inhalten, nämlich den Nutzern Freiheiten bezüglich der Weiterverwendung der Werke zu geben, also Kopien und modifizierte Versionen zu gestatten (siehe auch freie Inhalte). Bei gemeinfreien Werken kann eine dritte Person urheberrechtlich geschütztes Material zu dem gemeinfreien Werk hinzufügen, so dass das Gesamtwerk urheberrechtlich geschützt ist und Einschränkungen der Kopien und Bearbeitungen enthalten kann. Die Freiheit der Benutzer, die Inhalte zu modifizieren, kann also durch Änderungen Dritter verlorengehen. Um dies zu verhindern, nutzt Copyleft die Befugnisse des Autors, das Urheberrecht (Copyright), um alle weiteren Autoren eines Werkes dazu zu zwingen, das Werk mit all seinen Änderungen wieder unter die ursprüngliche Lizenz zu stellen.

Copyleft hat also aus der Sicht der Verbraucher den Vorteil, dass auch langfristig die Freiheit sichergestellt ist, während die Gemeinfreiheit den Vorteil bietet, auch ohne komplizierte Lizenz-Bedingungen Kopien und modifizierte Versionen zu erlauben.

Copyleft-Lizenzen sind zum Beispiel die GNU General Public License, die GNU Free Documentation License oder Creative-Commons-Lizenzen, die den Baustein "Share Alike" (Englisch, "Weitergabe unter gleichen Bedingungen") enthalten.

Die Creative Commons schlugen 2010 das "Public Domain Mark" (PDM) als Symbol zur Anzeige von Schöpfungen vor, die frei von Copyright-Ansprüchen und damit in der "Public domain" sind. Es ist das Analogon zum Copyrightzeichen, welches als „Copyright Mark“ agiert. Die Europeana Datenbank nutzt diese Zeichen, und auf den Wikimedia Commons sind im Februar 2016 2,9 Millionen Arbeiten (~10 % aller) in die Kategorie "PDM" eingeordnet.




</doc>
<doc id="3828" url="https://de.wikipedia.org/wiki?curid=3828" title="Paul Verhoeven (Regisseur, 1938)">
Paul Verhoeven (Regisseur, 1938)

Paul Verhoeven [] (* 18. Juli 1938 in Amsterdam, Niederlande) ist ein niederländischer Filmregisseur, Drehbuchautor und Filmproduzent.

Von 1960 bis 1963 drehte Verhoeven vier Kurzfilme. Er studierte an der Universität Leiden und graduierte 1964 in Mathematik und Physik. Während seiner Militärzeit konnte er in der Königlich Niederländischen Marine (Filmdienst der Marineinfanterie) erste Erfahrungen mit Massen- und Schlachtenszenen sammeln.

1968 war die Fernsehserie "Floris von Rosemund" der Anfang seiner erfolgreichen Zusammenarbeit mit Rutger Hauer, die 1985 in unversöhnlichem Streit endete. 1970 begann seine langjährige Zusammenarbeit mit Kameramann Jan de Bont. 1985 zog Verhoeven – enttäuscht von der kritischen Haltung der Öffentlichkeit und der Filmförderung in seiner Heimat – nach Hollywood. In dem Dokumentarfilm "Paul Verhoeven - Meister der Provokation" aus dem Jahre 2016 erklärt Verhoeven: "Ich konnte nicht amerikanisches Format erreichen, wenn ich gemütlich in der holländischen Provinz hocken blieb. Der Chef von Orion sagte mir, Paul, du musst herkommen und hier Filme drehen, dann funktioniert das auch. Die durch und durch amerikanische Umgebung wird dich vor deinen schlechten europäischen Angewohnheiten abschirmen [...] Ich hatte Angst, im Grunde fand ich es schrecklich, aber schließlich hab ich den Schritt doch gewagt." 

Seit Mitte der 1970er Jahre hat er auch mehrmals mit dem deutschen Kameramann Jost Vacano zusammengearbeitet, den er in Hollywood wiedertraf und mit dem er "RoboCop" drehte. Ursprünglich wollte Verhoeven abwechselnd mit den beiden Kameramännern arbeiten, dazu kam es allerdings nur einmal, weil de Bont ins Regiefach wechselte. Vacano drehte noch vier weitere Filme mit Verhoeven, bis er sich zur Ruhe setzte. 1996 wurde er für seinen Film "Showgirls" mit der Goldenen Himbeere als schlechtester Regisseur ausgezeichnet. Er holte diesen Schmähpreis persönlich ab, hielt eine Rede, in der er sagte: „Meine Filme werden hier kritisiert, weil sie als dekadent, pervers und schmierig gelten. Das bedeutet sicher, dass ich Teil dieser großartigen amerikanischen Gesellschaft bin. Danke!“ 

2005 kehrte Verhoeven nach Europa zurück, um im Studio Babelsberg den Zweite-Weltkrieg-Thriller "Black Book" mit Carice van Houten und Sebastian Koch zu drehen. Er begründete die Rückkehr und rein europäische Finanzierung der 17-Millionen-Euro-Produktion unter anderem damit, dass er sich so die größtmögliche Entscheidungsfreiheit bewahre.
Sein erster französischsprachiger Spielfilm "Elle", ein Thriller mit Isabelle Huppert in der Hauptrolle, kam am 25. Mai 2016 in die französischen Kinos. Für diesen erhielt er seine zweite Einladung in den Wettbewerb der 66. Internationalen Filmfestspiele von Cannes. 

Im Februar 2017 leitete Verhoeven als erster niederländischer Jury-Präsident die Wettbewerbsjury der 67. Internationalen Filmfestspiele Berlin. Als nächstes Filmprojekt ist eine Filmbiografie über die lesbische Nonne Benedetta Carlini (1591–1661) angekündigt. Virginie Efira soll die Hauptrolle übernehmen.

Paul Verhoeven ist seit dem 7. April 1967 mit Martine Tours verheiratet und hat zwei Kinder.

Weil viele von Verhoevens Filmen Gewalt und Sexualität thematisieren, sind sie Gegenstand heftiger Auseinandersetzungen unter Filmkritikern und in der Öffentlichkeit. Gewalt wird in Verhoevens Filmen in einer stark überzogenen (RoboCop, Total Recall, Starship Troopers) oder in einer extrem „realistischen“ Weise (Flesh and Blood) dargestellt. Einige seiner Filme lassen sich damit auch als eine Art von Karikatur auf herkömmliche Arten der Gewaltdarstellung verstehen.
Sexualität in Verhoevens Filmen wird von Kritikern oft in die Nähe von Pornographie und Obszönität gerückt. Auch lösten seine Filme bei den unterschiedlichsten gesellschaftlichen Gruppen (Frauenverbände, Homosexuelle) heftigste Proteste aus (Türkische Früchte, "Spetters", Basic Instinct). In dem Dokumentarfilm "Paul Verhoeven - Meister der Provokation" von 2016 schildert Verhoeven: "Solche Szenen werden eigentlich immer ausgespart, sowohl im Film als auch in der Literatur. Oder es wird irgendwie verharmlosend dargestellt. Keiner zeigt, wie es wirklich ist. So ist es doch in Wahrheit, da legen sich Menschen aufeinander und das Ding wird reingeschoben [...] Ich zeige nur, wie es in Wahrheit ist." 








</doc>
<doc id="3831" url="https://de.wikipedia.org/wiki?curid=3831" title="Paradoxon">
Paradoxon

Ein Paradoxon (Plural "Paradoxa"; auch "Paradox" oder "Paradoxie", Plural "Paradoxe" bzw. "Paradoxien"; vom altgriechischen Adjektiv "parádoxos" „wider Erwarten, wider die gewöhnliche Meinung, unerwartet, unglaublich“) ist ein Befund, eine Aussage oder Erscheinung, die dem allgemein Erwarteten, der herrschenden Meinung oder Ähnlichem auf unerwartete Weise zuwiderläuft oder beim üblichen Verständnis der betroffenen Gegenstände bzw. Begriffe zu einem Widerspruch führt. Die Analyse von Paradoxien kann zu einem tieferen Verständnis der betreffenden Gegenstände bzw. Begriffe oder Situationen führen, was den Widerspruch im besten Fall auflöst.

Es existieren verschiedene spezielle Formen des Paradoxons:


Gemeinsam ist allen Paradoxa der Widerspruch zwischen dem Behaupteten einerseits und den Erwartungen und Beurteilungen andererseits, die sich aus vertrauten Denkheuristiken, Vorurteilen, Gemeinplätzen, Mehrdeutigkeiten oder begrenzten Perspektiven als alltägliche Meinung ("doxa") ergeben. Auch scheinbare Widersprüche, die sich durch genauere Analyse vollständig auflösen lassen, wirken daher im ersten Moment paradox oder galten im Laufe der Geistesgeschichte als unlösbare Paradoxa oder Aporien. Auflösbare Paradoxien sind wahre Aussagen, deren Untersuchung – beispielsweise im Rahmen eines Gedankenexperiments – zu wichtigen Erkenntnisfortschritten in Wissenschaft, Philosophie und Mathematik führen kann, die für das Alltagsverständnis aber unerwartet oder überraschend sind. Der Widerspruch besteht hier oft nur zwischen der erwarteten und der tatsächlichen Lösung. Ein Beispiel aus der Mathematik ist das Ziegenproblem, das logisch und mathematisch exakt lösbar ist, aber der Erwartung vieler Menschen widerspricht.

Die Aufzählung der Paradoxien in den verschiedenen Wissenschaften belegt, dass das Erkennen und Lösen von Paradoxien ein bedeutendes Motiv wissenschaftlicher Arbeit sein kann. Der Mathematiker Roger Penrose drückte es einmal so aus:

Gesellschaftliche Ideologien enthalten in der Praxis oft paradoxe Elemente, vor allem wenn sie mit absolut gesetzten Werten wie Freiheit oder Gleichheit operieren. Beispiele: So werden, um eine „freiheitliche“ Ordnung aufrechtzuerhalten, Maßnahmen eingesetzt, die die Freiheit einschränken (z. B. McCarthy-Ära in den USA oder auch die aktuellen Debatten um die Einschränkung von Bürgerrechten im Anti-Terror-Kampf). Umgekehrt wurden in kommunistischen Ideologien, um das Ideal der „Gleichheit“ zu erhalten, Systeme etabliert, in denen einige deutlich „gleicher“ waren als andere. Praktisch alle politischen Ideologien, in denen „der Zweck die Mittel heiligt“, beinhalten diese Paradoxie: In der Durchsetzung bestimmter Werte für die Zukunft werden die gleichen Werte in der Gegenwart geopfert.

Wie bei vielen Paradoxien entsteht der Widerspruch auch hier durch die Anwendung eines Prinzips (Freiheit, Gleichheit) auf sich selbst und auf die Bedingungen, die dieses Prinzip ermöglichen sollen.

Zu den psychologischen Paradoxien gehören Fälle, in denen Menschen sich genau entgegen der „Logik“ verhalten. Dazu gehört die sogenannte „Sei-spontan-Paradoxie“, wie es häufig in Beziehungen zum Ausdruck kommt: Die Erwartung, dass mein Gegenüber seine Entscheidungen gefälligst frei und selbständig treffen soll – und genau damit seine Unselbständigkeit unter Beweis stellen würde. Der Wunsch „Sag mir doch öfter mal spontan, dass du mich liebst!“ ist, sobald ausgesprochen, nicht mehr erfüllbar. 
In den sogenannten paradoxen Interventionen werden psychologische Paradoxien wiederum gezielt eingesetzt, insbesondere dann, wenn das Gegenüber (ein Kind zum Beispiel) ein trotziges Verhalten zeigt und auf Aufforderungen bewusst mit dem Gegenteil reagiert. Entsprechend wird in der paradoxen Intervention eine Erwartung geäußert, deren Gegenteil eigentlich erreicht werden soll.

Ein weiteres Beispiel für psychologische Paradoxien sind die sogenannten „gemischten Botschaften“, wenn zwischen dem, "was" gesagt wird, und der Art, "wie" es gesagt wird, ein Widerspruch besteht, zum Beispiel wenn eine Person, auf Avancen, die ihr gemacht werden, mit einem „Nein“ reagiert, dabei aber freundlich lächelt. In langdauernden Beziehungen können so die von Gregory Bateson beschriebenen sogenannten Double-Bind-Kommunikationsstrukturen entstehen, wenn also zum Beispiel einer der Partner (insbesondere in Eltern-Kind-Beziehungen) dem anderen seine Zuneigung immer mit unbewegter Mimik, emotionsloser Stimme und ohne Körperkontakt versichert.

Großvater-Paradoxon: Ein Zeitreisender, der in der Vergangenheit seinen Großvater umbringt, würde nicht geboren werden, und könnte daher nie seinen Großvater umgebracht haben. 

In "Das Leben des Brian" von Monty Python findet sich in der „Balkonszene“ folgendes paradoxes Geschehen: Brian wird gegen seinen Willen für den Messias gehalten und fordert seine Anhänger auf, Individuen zu sein: 




</doc>
<doc id="3832" url="https://de.wikipedia.org/wiki?curid=3832" title="Pflicht">
Pflicht

Pflicht (von "pflegen" im Sinne von "wie etwas zu sein pflegt"), auch "Sollen" oder "Müssen" ist eine Aufgabe, die jemandem aus prinzipiellen, persönlichen, situativen oder sozialen Gründen erwächst und deren Erfüllung er sich nicht entziehen kann. Daneben wird als Pflicht auch das bezeichnet, was von einer äußeren Autorität von jemandem gefordert wird und Verbindlichkeit beansprucht, insbesondere per Gesetz. Die Pflicht ist einer der Grundbegriffe der Ethik, die Achtung von Pflichten gilt im Allgemeinen als tugendhaft. Pflichten können in religiösen Vorschriften kodifiziert sein, bestimmte Pflichten sind auch im Recht, einer politischen Verfassung oder allgemein durch eine Satzung im soziologischen Sinne vorgegeben.

Die philosophische Lehre von den Pflichten heißt Deontologie, zusammengesetzt aus dem griechischen "to deon", „das Erforderliche, die Pflicht“, und "logos", „Lehre“, also „Pflichtenlehre“.

Das Grundprinzip ist die Berufung auf die Motivation der Handlung. Es folgt die Prüfung, ob Motivation und Handlung mit einem Wertmaßstab, den jeder vernünftige Mensch sofort einsieht, vereinbar sind oder nicht. Das Begründungsverfahren lässt nur die Attribute „gut“ oder „schlecht“ zu.

In Abgrenzung zum Zwang unterscheidet sich die Pflicht dadurch, dass sie auf einem gesellschaftlichen, rationalen oder ethischen Diskurs einschließlich Findung eines Konsenses beruht. Erforderlich ist demnach, dass ein Pflichtausübender die Notwendigkeit der Ausübung selbst erkennt und einsieht. Sie führt folglich zur Übernahme von Verantwortung und endet mit Erfolg oder Misserfolg, wodurch sich für den Handelnden sowohl positive als auch negative Konsequenzen in Bezug auf die eigene Erwartungshaltung ergeben können. Daraus resultiert, dass Pflichtausübung stets einer Gewissensprüfung und einer sorgfältigen Risikoabschätzung bedarf. Beim Zwang hingegen wird etwas unbedingt abverlangt auch ohne Einverständnis oder Einsicht. Das Erzwungene kann nach dem Konzept von einem freien Willen angenommen, abgewiesen oder erduldet werden.

Moral und Sitte im Sinne praktischer Wertvorgaben begründen ähnlich dem Recht bestimmte Handlungspflichten und -verbote. Die Moral wendet sich jedoch an die Gesinnung eines Menschen, während das Recht sein äußeres Verhalten regelt. Ein Verstoß gegen moralische Wertvorstellungen zieht nur die gesellschaftliche Missbilligung nach sich, es gibt keine allgemein verbindlichen moralischen Sanktionsnormen.

Moralische Pflicht steht in Relation zum moralischen Recht, das eine Handlung ermöglicht und nicht fordert. Der Unterschied besteht somit zwischen der Aufforderung und der Erlaubnis zu einer Handlung.

Pflicht spielt auch in den Religionen eine wichtige Rolle und bedeutet zuerst die Pflicht des Menschen gegenüber dem Gesetz Gottes.


Pflicht () oder Rechtspflicht () sind im deutschen und internationalen Recht die einem Rechtssubjekt durch Rechtsnormen oder Vertrag auferlegten Verhaltensregeln.

Pflicht und Rechtspflicht sind Rechtsbegriffe, die im deutschen Recht sehr häufig vorkommen, alleine als Pflicht oder Wortbestandteil im BGB 945 Mal, im EStG 701 Mal oder im HGB 292 Mal. Ob Pflicht und Rechtspflicht im Rechtssinne inhaltlich übereinstimmen, ist in der Fachliteratur umstritten. Rechtsnormen jedenfalls differenzieren bei beiden Rechtsbegriffen nicht.

Pflichten spielen in der Rechtswissenschaft eine große Rolle. Rechtsnormen, deren Summe man als objektives Recht bezeichnet, sind abstrakt-generelle Sollensnormen. Sie ordnen bestimmten Lebenssachverhalten bestimmte Rechtsfolgen zu und sehen für bestimmte Pflichtverstöße Sanktionen vor. Sie verpflichten ein Rechtssubjekt zu einem bestimmten Handeln, Dulden (Gebote) oder Unterlassen (Verbot). Der die Pflicht Übernehmende hat sein Verhalten so einzurichten, wie es ihm vorgeschrieben wird. Sie ist ein von der Rechtsordnung an Personen gerichteter und von diesen zu befolgende Instruktion. Rechtspflichten gelten sowohl im Verhältnis des Staates zum Bürger, so im Öffentlichen Recht und im Strafrecht, regeln im Zivilrecht aber auch die Rechte und Pflichten der einzelnen Bürger untereinander (Verpflichtetsein). Das Recht, von einem anderen ein Tun oder Unterlassen zu verlangen, wird als Anspruch bezeichnet ( Abs. 1 BGB). Kraft eines subjektiv-öffentlichen Rechts steht dem Bürger gegenüber dem Staat ein bestimmter Anspruch zu.

Die gesamte rechtsstaatliche Staatsgewalt ist der verfassungsmäßigen Ordnung verpflichtet ( Abs. 3 GG). Auch den Bürgern sind bestimmte Pflichten auferlegt, etwa die Schulpflicht, herkömmliche öffentliche Dienstleistungspflichten wie die Räum- und Streupflicht oder die elterliche Fürsorge- und Erziehungspflicht. Die Wehrpflicht wurde in Deutschland 2011 ausgesetzt. Eigentum verpflichtet. Sein Gebrauch soll zugleich dem Wohle der Allgemeinheit dienen ( Abs. 2 GG).

Nur im Privatrecht gibt es einzelne gesetzliche Bestimmungen, die nicht in jedem Fall verpflichtend sind, sondern von den Parteien abbedungen werden können.

Im Strafrecht kann eine Pflichtenkollision einen Rechtfertigungsgrund darstellen. Es ist beispielsweise einem Rettungsschwimmer oder einem Feuerwehrmann nicht in jedem Fall zuzumuten, seiner Pflicht nachzukommen, wenn die Gefahr für sein eigenes Leben zu hoch scheint.

Die sittliche Pflicht ist eine juristisch feststellbare Verpflichtung für Leistungen an eine natürliche Person.


Bei vielen Sportdisziplinen wie Voltigieren und Turnen gibt es im Wettkampfbereich einen Pflicht- und einen Kürteil. Unter der Pflicht versteht man in diesem Zusammenhang eine vorgegebene Reihenfolge bestimmter Bewegungselemente und Übungen. Die Kampfrichter können bei der Pflicht – anders als bei der freier gestalteten Kür – die Leistungen der Wettkämpfer direkt miteinander vergleichen.




</doc>
<doc id="3833" url="https://de.wikipedia.org/wiki?curid=3833" title="Pipeline (Begriffsklärung)">
Pipeline (Begriffsklärung)

Pipeline steht für:


Informatik:
Siehe auch:


</doc>
<doc id="3835" url="https://de.wikipedia.org/wiki?curid=3835" title="Puffer">
Puffer

Puffer, teils synonym Buffer (engl., dt. unter anderem "Zwischenspeicher, Reserve oder Dämpfer"), teils auch Pufferspeicher, steht für:

Puffer bezeichnet folgende Speisen:

Puffer ist der Familienname folgender Personen:

Buffer ist der Familienname folgender Personen:

Buffer ist der Name eines Unternehmens im Bereich Sozialer Netzwerke 
Siehe auch:


</doc>
<doc id="3836" url="https://de.wikipedia.org/wiki?curid=3836" title="Programmiersprache">
Programmiersprache

Eine Programmiersprache ist eine formale Sprache zur Formulierung von Datenstrukturen und Algorithmen, d. h. von Rechenvorschriften, die von einem Computer ausgeführt werden können. Sie setzen sich aus Anweisungen nach einem vorgegebenen Muster zusammen, der sogenannten Syntax.

Während die ersten Programmiersprachen noch unmittelbar an den Eigenschaften der jeweiligen Rechner orientiert waren, verwendet man heute meist problemorientierte Sprachen, sogenannte "höhere Programmiersprachen", die eine abstraktere und für den Menschen leichter verständliche Ausdrucksweise erlauben. In diesen Sprachen geschriebene Programme können automatisiert in Maschinensprache übersetzt werden, die von einem Prozessor ausgeführt wird. Zunehmend kommen auch visuelle Programmiersprachen zum Einsatz, die den Zugang zu Programmiersprachen erleichtern.

Die in einer bestimmten Programmiersprache, häufig mittels einfacher Texteditoren erzeugten Anweisungen nennt man Quelltext (oder auch Quellcode). Um auf einem Computer ausgeführt zu werden, muss der Quelltext in die Maschinensprache dieses Computer(typ)s übersetzt werden. Diese ist im Gegensatz zu höheren Programmiersprachen und zur Assemblersprache ein für Menschen schwer lesbarer Binärcode. Wird von Programmierung in Maschinensprache gesprochen, so ist heute meist die Assemblersprache gemeint.

Die Übersetzung in Maschinensprache kann entweder vor der Ausführung durch einen Compiler oder – zur Laufzeit – durch einen Interpreter oder JIT-Compiler geschehen. Oft wird eine Kombination aus beiden Varianten gewählt, bei der zuerst der Quelltext der angewendeten Programmiersprache in einen Zwischencode übersetzt wird, welcher dann zur Laufzeit innerhalb einer Laufzeitumgebung in Maschinencode überführt wird. Dieses Prinzip hat den Vorteil, dass derselbe Zwischencode auf verschiedenen Plattformen ausführbar ist. Beispiele für einen solchen Zwischencode sind der Java-Bytecode sowie die Common Intermediate Language.

Programmiersprachen bieten meist mindestens
Meist ist es möglich, aus diesen Grundfunktionen höhere Funktionen zu erstellen und diese als Bibliothek wiederverwendbar zu kapseln. Von dort zu einer höheren oder problemorientierten Sprache zu gelangen, ist kein großer Schritt mehr. So gab es schon bald eine große Zahl an Spezialsprachen für die verschiedensten Anwendungsgebiete. Damit steigt die Effizienz der Programmierer und die Portabilität der Programme, meist nimmt dafür die Verarbeitungsgeschwindigkeit der erzeugten Programme ab, und die Mächtigkeit der Sprache nimmt ab: Je höher und komfortabler die Sprache, desto mehr ist der Programmierer daran gebunden, die in ihr vorgesehenen Wege zu beschreiten.

Sprachen sind verschieden erfolgreich - manche „wachsen“ und finden zunehmend breitere Anwendung; immer wieder sind auch Sprachen mit dem Anspruch entworfen worden, Mehrzweck- und Breitbandsprachen zu sein, oft mit bescheidenem Erfolg (PL/1, Ada, Algol 68).

Die Bedeutung von Programmiersprachen für die Informatik drückt sich auch in der Vielfalt der Ausprägungen und der Breite der Anwendungen aus.


Umgangssprachlich wird auch in anderen Bereichen von Programmiersprachen gesprochen. Nachfolgende Sprachen sind jedoch "nicht" für die Beschreibung von Algorithmen und allgemeine Datenverarbeitung entworfen, also keine "General Purpose Languages":
Derartige Sprachen fallen unter die "domänenspezifischen Sprachen."

Die Anweisungen von Programmiersprachen (Beispiele siehe hier) lassen sich nach folgenden Gruppen klassifizieren:

Um ein in einer bestimmten Programmiersprache erstelltes Programm ausführen zu können, muss dessen Quellcode in eine äquivalente Folge von Maschinenbefehlen übersetzt werden. Das ist notwendig, da der Quellcode aus Zeichenfolgen besteht (z. B. „A = B + 100 * C“), die der Prozessor nicht „versteht“.

Die in der Geschichte der Computertechnik und der Softwaretechnologie eingetretenen Entwicklungssprünge brachten auch unterschiedliche Werkzeuge zur Erzeugung von Maschinencode, ggf. über mehrere Stufen, mit sich. Diese werden beispielsweise als Compiler, Interpreter, Precompiler, Linker etc. bezeichnet.

In Bezug auf die Art und den Zeitpunkt der Übersetzung können zwei Ablaufprinzipien unterschieden werden:
Daneben existieren verschiedene "Mischvarianten":

Zur Steuerung des Übersetzens kann der Quelltext neben den Anweisungen der Programmiersprache zusätzliche spezielle Compiler-Anweisungen enthalten. Komplexe Übersetzungsvorgänge werden bei Anwendung bestimmter Programmiersprachen / Entwicklungsumgebungen durch einen Projekterstellungsprozess und die darin gesetzten Parameter gesteuert.

Zur Vorgeschichte der Programmiersprachen kann man von praktischer Seite die zahlreichen Notationen zählen, die sowohl in der Fernmeldetechnik (Morsezeichen) als auch zur Steuerung von Maschinen (Jacquardwebstuhl ) entwickelt worden waren; dann die Assemblersprachen der ersten Rechner, die doch nur deren Weiterentwicklung waren. Von theoretischer Seite zählen dazu die vielen Präzisierungen des Algorithmusbegriffs, von denen der λ-Kalkül die bei Weitem bedeutendste ist. Auch Zuses Plankalkül gehört hierhin, denn er ist dem "minimalistischen" Ansatz der Theoretiker verpflichtet (Bit als Grundbaustein).

In einer ersten Phase wurden ab Mitte der 1950er Jahre unzählige Sprachen entwickelt, die praktisch an gegebenen Aufgaben und Mitteln orientiert waren. Seit der Entwicklung von Algol 60 (1958–1963) ist die Aufgabe des Übersetzerbaus in der praktischen Informatik etabliert und wird zunächst mit Schwerpunkt Syntax (-erkennung, Parser) intensiv bearbeitet. Auf der praktischen Seite wurden erweiterte Datentypen wie Verbunde, Zeichenketten und Zeiger eingeführt (konsequent z. B. in Algol 68).

In den 1950er Jahren wurden in den USA die ersten drei weiter verbreiteten, praktisch eingesetzten höheren Programmiersprachen entwickelt. Dabei verfolgten diese sowohl imperative als auch deklarativ-funktionale Ansätze.

Die Entwicklung von Algol 60 läutete eine fruchtbare Phase vieler neuer Konzepte, wie das der prozeduralen Programmierung ein. Der Bedarf an neuen Programmiersprachen wurde durch den schnellen Fortschritt der Computertechnik gesteigert. In dieser Phase entstanden die bis heute populärsten Programmiersprachen: BASIC und C.

In der Nachfolgezeit ab 1980 konnten sich die neu entwickelten logischen Programmiersprachen nicht gegen die Weiterentwicklung traditioneller Konzepte in Form des objektorientierten Programmierens durchsetzen. Das in den 1990er Jahren immer schneller wachsende Internet forderte seinen Tribut beispielsweise in Form von neuen Skriptsprachen für die Entwicklung von Webserver-Anwendungen.

Derzeit schreitet die Integration der Konzepte der letzten Jahrzehnte voran. Größere Beachtung findet so beispielsweise der Aspekt der Codesicherheit in Form von virtuellen Maschinen. Neuere integrierte, visuelle Entwicklungsumgebungen erfordern deutlich weniger Aufwand an Zeit und Kosten. Bedienoberflächen lassen sich meist visuell gestalten, Codefragmente sind per Klick direkt erreichbar. Dokumentation zu anderen Programmteilen und Bibliotheken ist direkt einsehbar, meist gibt es sogar "lookup"-Funktionalität, die noch während des Schreibens herausfindet, welche Symbole an dieser Stelle erlaubt sind und entsprechende Vorschläge macht (Autovervollständigen).

Neben der mittlerweile etablierten objektorientierten Programmierung ist die modellgetriebene Architektur ein weiterer Ansatz zur Verbesserung der Software-Entwicklung, in der Programme aus syntaktisch und semantisch formal spezifizierten Modellen generiert werden. Diese Techniken markieren gleichzeitig den Übergang von einer eher handwerklichen, individuellen Kunst zu einem industriell organisierten Prozess.


Man hat die Maschinen-, Assembler- und höheren Programmiersprachen auch als Sprachen der "ersten bis dritten Generation" bezeichnet; auch in Analogie zu den gleichzeitigen Hardwaregenerationen. Als "vierte Generation" wurden verschiedenste Systeme beworben, die mit Programmgeneratoren und Hilfsprogrammen z. B. zur Gestaltung von Bildschirmmasken ("screen painter") ausgestattet waren. "Die" Sprache der fünften Generation schließlich sollte in den 1980er Jahren im Sinne des "Fifth Generation Computing" Concurrent Prolog sein.

Die Programmiersprachen lassen sich in Kategorien einteilen, die sich im evolutionären Verlauf der Programmiersprachen-Entwicklung als sog. Programmierparadigmen gebildet haben. Grundlegend sind die Paradigmen der strukturierten, der imperativen und der deklarativen Programmierung - mit jeweils weiteren Unterteilungen. Eine Programmiersprache kann jedoch auch mehreren Paradigmen gehorchen, das heißt die begriffsbestimmenden Merkmale mehrerer Paradigmen unterstützen.

Strukturierte Programmierung ist Anfang der 1970er Jahre auch aufgrund der Softwarekrise populär geworden. Es beinhaltet die Zerlegung eines Programms in Unterprogramme (prozedurale Programmierung) und die Beschränkung auf die drei elementaren Kontrollstrukturen Anweisungs-Reihenfolge, Verzweigung und Wiederholung.

Ein in einer imperativen Programmiersprache geschriebenes Programm besteht aus Anweisungen (latein "imperare" = befehlen), die beschreiben, "wie" das Programm seine Ergebnisse erzeugt (zum Beispiel Wenn-dann-Folgen, Schleifen, Multiplikationen etc.).

Den genau umgekehrten Ansatz verfolgen die deklarativen Programmiersprachen. Dabei beschreibt der Programmierer, welche Bedingungen die Ausgabe des Programms (das "Was") erfüllen muss. Wie die Ergebnisse konkret erzeugt werden, wird bei der Übersetzung, zum Beispiel durch einen Interpreter festgelegt. Ein Beispiel ist die Datenbankabfragesprache SQL.

Ein Programm muss nicht unbedingt eine Liste von Anweisungen enthalten. Stattdessen können grafische Programmieransätze, zum Beispiel wie bei der in der Automatisierung verwendeten Plattform STEP 7, benutzt werden.

Die Art der formulierten Bedingungen unterteilen die deklarativen Programmiersprachen in logische Programmiersprachen, die mathematische Logik benutzen, und funktionale Programmiersprachen, die dafür mathematische Funktionen einsetzen.

Hier werden Daten und Befehle, die auf diese Daten angewendet werden können, in Objekten zusammengefasst. Objektorientierung wird im Rahmen der Objektorientierten Programmierung verwendet, um die Komplexität der entstehenden Programme zu verringern.

Die Bausteine, aus denen ein objektorientiertes Programm besteht, werden als Objekte bezeichnet. Die Konzeption dieser Objekte erfolgt dabei in der Regel auf Basis der folgenden Paradigmen:

Variablen sind mit einem Namen versehene Orte im Speicher, die einen Wert aufnehmen können. Um die Art des abgelegten Wertes festzulegen, muss in vielen Programmiersprachen der Variablen ein Datentyp zugewiesen werden. Häufige Datentypen sind Ganz- und Gleitkommazahlen oder auch Zeichenketten.

Es wird zwischen typisierten und typenlosen Sprachen unterschieden. In typisierten Sprachen (zum Beispiel C++ oder Java) wird der Inhalt der Variable durch einen Datentyp festgelegt. So gibt es für Ganz- und Gleitkommazahlen verschiedene Datentypen, die sich durch ihren Wertebereich unterscheiden. Sie können vorzeichenlos oder vorzeichenbehaftet sein. Nach aufsteigendem Wertebereich sind dies zum Beispiel: Short, Integer oder Long. Datentypen für Gleitkommazahlen sind zum Beispiel Float oder Double. Einzelne Zeichen können im Datentyp Char gespeichert werden. Für Zeichenketten steht der Datentyp String zur Verfügung.

Die typisierten Sprachen können anhand des Zeitpunkts der Typüberprüfung unterschieden werden. Findet die Typüberprüfung bereits bei der Übersetzung des Programms statt, spricht man von statischer Typisierung. Findet die Typprüfung zur Laufzeit statt, spricht man von dynamischer Typisierung. Erkennt eine Programmiersprache Typfehler spätestens zur Laufzeit, wird sie als typsicher bezeichnet.

Bei statischer Typprüfung ist der Programmierer versucht, diese zu umgehen, oder sie wird nicht vollständig durchgesetzt (zum jetzigen Stand der Technik muss es in jeder statischen Sprache eine Möglichkeit geben, typlose Daten zu erzeugen oder zwischen Typen zu wechseln – etwa wenn Daten vom Massenspeicher gelesen werden). In Sprachen mit dynamischer Typprüfung werden manche Typfehler erst gefunden, wenn es zu spät ist. Soll der Datentyp einer Variablen geändert werden, ist ein expliziter Befehl zur Umwandlung nötig.

Die typenlosen Sprachen (zum Beispiel JavaScript oder Prolog) verfügen, im Gegensatz zu den typisierten Sprachen, über keine differenzierten Datentypen. Der Datentyp einer Variablen wird erst zur Laufzeit festgelegt. Wird einer Variablen ein Wert eines anderen Typs zugewiesen, findet eine Umwandlung der Variablen in den neuen Typ statt. Die typenlosen Sprachen behandeln oftmals alle Einheiten als Zeichenketten und kennen für zusammengesetzte Daten eine allgemeine Liste.

Durch die Festlegung des Datentyps werden vor allem zwei Zwecke verfolgt:


Das sichere Typsystem der Programmiersprache ML bildet die Grundlage für die Korrektheit der in ihr programmierten Beweissysteme (LCF, HOL, Isabelle); in ähnlicher Weise versucht man jetzt auch die Sicherheit von Betriebssystemen zu gewährleisten. Schließlich ermöglichen erst unterschiedliche Typangaben das populäre Überladen von Bezeichnern. Nach Strachey sollte das Typsystem im Mittelpunkt der Definition einer Programmiersprache stehen.

Die Definition von Daten erfolgt im Allgemeinen durch die Angabe einer konkreten Spezifikation zur Datenhaltung und der dazu nötigen Operationen. Diese konkrete Spezifikation legt das allgemeine Verhalten der Operationen fest und abstrahiert damit von der konkreten Implementierung der Datenstruktur (s. a. Deklaration).

Oft kann an den "Bürgern erster Klasse" ("First class Citizens" – FCCs) einer Programmiersprache – also den Formen von Daten, die direkt verwendet werden können, erkannt werden, welchem Paradigma die Sprache gehorcht. In Java z. B. sind Objekte FCCs, in Lisp ist jedes Stück Programm FCCs, in Perl sind es Zeichenketten, Arrays und Hashes. Auch der Aufbau der Daten folgt syntaktischen Regeln. Mit Variablen kann man bequem auf die Daten zugreifen und den dualen Charakter von Referenz und Datum einer Variablen ausnutzen. Um die Zeichenketten der Daten mit ihrer (semantischen) Bedeutung nutzen zu können, muss man diese Bedeutung durch die Angabe eines Datentyps angeben. Zumeist besteht im Rahmen des Typsystems auch die Möglichkeit, neue Typen zu vereinbaren. LISP verwendet als konzeptionelle Hauptstruktur Listen. Auch das Programm ist eine Liste von Befehlen, die andere Listen verändern. Forth verwendet als konzeptionelle Hauptstruktur Stacks und Stack-Operationen sowie ein zur Laufzeit erweiterbares Wörterbuch von Definitionen und führt in den meisten Implementationen überhaupt keine Typprüfungen durch.

Ein beliebter Einstieg in eine Programmiersprache ist es, mit ihr den Text "Hello World" (oder deutsch „Hallo Welt“) auf den Bildschirm oder einem anderen Ausgabegerät auszugeben (siehe Hallo-Welt-Programm). Entsprechend gibt es Listen von Hallo-Welt-Programmen und eigene Webseiten, die Lösungen in verschiedenen Programmiersprachen gegenüberstellen.





</doc>
<doc id="3840" url="https://de.wikipedia.org/wiki?curid=3840" title="Periodensystem">
Periodensystem

Das Periodensystem (Langfassung Periodensystem der Elemente, abgekürzt PSE) stellt alle chemischen Elemente mit steigender Kernladung (Ordnungszahl) und entsprechend ihren chemischen Eigenschaften eingeteilt in Perioden sowie Haupt- und Nebengruppen dar. Es wurde 1869 unabhängig voneinander und fast identisch von zwei Chemikern aufgestellt, zunächst von dem Russen Dmitri Mendelejew (1834–1907) und wenige Monate später von dem Deutschen Lothar Meyer (1830–1895). Historisch war das Periodensystem für die Vorhersage der Entdeckung neuer Elemente und deren Eigenschaften von besonderer Bedeutung. Heute dient es vor allem der Übersicht.

Nachstehend ist das Periodensystem in seiner heute bekanntesten Form als Langperiodensystem wiedergegeben:

Ein über die Ordnungszahl 118 hinausgehendes Periodensystem befindet sich unter Erweitertes Periodensystem.

Die Anordnung der Atome im Periodensystem ist vollständig durch die Elektronenkonfiguration erklärbar.

Aufbau der Atome:

Neutronen:

Kernmasse/Atommasse:

Die innerste Schale, die "K-Schale", kann nur von zwei Elektronen besetzt werden. Damit gibt es auch nur zwei chemische Elemente, deren Atome ausschließlich diese innerste Schale nutzen. Das sind Wasserstoff (Ordnungszahl 1) und Helium (Ordnungszahl 2). Sie bilden in der Darstellung des Periodensystems die oberste „Zeile“ "(1. Periode)".

Die Atomhülle des nächstfolgenden Elements, von Lithium (Ordnungszahl 3) hat drei Elektronen. Das dritte Elektron befindet sich auf einer weiter außen liegenden Elektronenschale, der "L-Schale". Diese zweite Schale kann maximal von acht Elektronen besetzt werden. Entsprechend enthält die "2. Periode" neben dem Lithium sieben weitere Elemente (mit vier bis zehn Elektronen), dargestellt in der zweiten „Zeile“. Die Atome des Elementes mit der Ordnungszahl 11 (Natrium) besitzen jeweils eine weitere Elektronenschale, die die L-Schale umgibt und mit einem Elektron besetzt ist. Diese dritte Schale, die M-Schale, kann wiederum von maximal acht Elektronen besetzt werden. Somit bilden nach Natrium weitere sieben Elemente bis zur Ordnungszahl 18 (Argon) die dritte „Zeile“ im Periodensystem "(3. Periode)".

Die Elektronen der jeweils äußersten Schale nennt man Außenelektronen, oder besser: Valenzelektronen. Sie spielen eine Rolle für die Bildung von chemischen Verbindungen aus den Atomen der Elemente. Die Anzahl der Valenzelektronen nimmt bei den Elementen einer „Zeile“ (Periode) in den ersten drei Perioden immer von links nach rechts zu. Bei den Atomen des Wasserstoffs und Heliums sind das eine bzw. die beiden Elektronen der Atomhülle zugleich Außenelektronen. Bei den Atomen der Elemente der 2. und 3. Periode befinden sich die Außenelektronen in der L- bzw. M-Schale, sodass Lithium und Natrium jeweils ein, Neon und Argon jeweils acht Außenelektronen haben.

Vergleicht man die Stoffeigenschaften von Elementen, deren Atome dieselbe Anzahl Valenzelektronen besitzen, finden sich viele Übereinstimmungen. Diese Gemeinsamkeiten kommen auch durch die Anordnung der Elemente im Periodensystem zum Ausdruck. Die Elemente mit nur einem von möglichen acht Valenzelektronen in der äußersten Schale stehen jeweils an erster Stelle in ihrer Periode. Die sich daraus ergebende „Spalte“ im Periodensystem wird "1. Hauptgruppe" genannt und die darin enthaltenen Elemente werden unter der Bezeichnung Alkalimetalle zusammengefasst. Die Elemente mit sieben Außenelektronen in der äußersten Schale stehen an jeweils vorletzter Stelle in ihrer Periode. Die sich daraus ergebende „Spalte“ im Periodensystem wird "7. Hauptgruppe" genannt und die darin enthaltenen Elemente werden unter der Bezeichnung Halogene zusammengefasst. Die Elemente mit acht Elektronen in der äußersten Schale, das heißt mit einer voll aufgefüllten äußersten Schale, stehen an jeweils letzter Stelle in ihrer Periode in der "8. Hauptgruppe" und werden unter der Bezeichnung Edelgase zusammengefasst. Auch für die Elemente anderer Hauptgruppen gibt es Überbegriffe, bspw. Erdalkalimetalle für die der 2. Hauptgruppe und Chalkogene für die der 6. Hauptgruppe.

Diese Anordnung der Elemente in Hauptgruppen wird ab der "4. Periode" allerdings unterbrochen. In der 4. und "5. Periode" befinden sich zwar auch die Valenzelektronen der Atome der jeweils ersten beiden Elemente (Ordnungszahl 19 Kalium und 20 Calcium bzw. 37 Rubidium und 38 Strontium) nur in der äußersten Schale, der "N-" bzw. "O-Schale", bei den gemäß ihrer Ordnungszahl jeweils folgenden 10 Elementen (Ordnungszahl 21 bis 30 bzw. 39 bis 48) jedoch nicht. Diese besitzen in der zweitäußersten Schale (M- bzw. N-Schale) zusätzliche Kapazitäten für maximal 10 Elektronen, von denen wenigstens eines als Valenzelektron fungieren kann, während sich in der N- bzw. O-Schale höchstens zwei Elektronen befinden. Die aus diesen Elementen gebildeten „Spalten“ des Periodensystems, die sich auch auf die 6. und 7. Periode erstrecken, werden "Nebengruppen" genannt. Wegen Besonderheiten in der Aufteilung der Elektronen auf die beiden äußeren Schalen beginnt der Block der Nebengruppenelemente nicht mit der 1., sondern mit der 3. Nebengruppe, und die 1. und 2. Nebengruppe folgt auf die 8. Nebengruppe, die jeweils 3 Elemente pro Periode beinhaltet. Bei den Nebengruppenelementen handelt es sich ausschließlich um Metalle, die sogenannten Übergangsmetalle. Bei allen auf das letzte Nebengruppenelement der 4. und 5. Periode folgenden Hauptgruppenelementen ist die M- bzw. N-Schale bereits mit 18 Elektronen gefüllt. Stattdessen wird bei diesen Elementen mit steigender Ordnungszahl die äußerste Schale auf maximal 8 Elektronen aufgefüllt.

In den "Perioden 6" und "7" folgen auf die nach ihrer Ordnungszahl ersten Elemente des Nebengruppenblocks (57 Lanthan bzw. 89 Actinium) jeweils 14 Elemente (Ordnungszahl 58 bis 71 bzw. 90 bis 103), bei denen in der drittäußersten Elektronenschale, der N- bzw. O-Schale, weitere Kapazitäten für maximal 14 Elektronen frei sind, während sich in der zweitäußersten (O- bzw. P-Schale) meistens acht, und in der äußersten (P- bzw. Q-Schale) höchstens zwei Elektronen befinden. Da sich bei diesen 28 Elementen also die Unterschiede im Bau der Atomhülle im Wesentlichen auf die drittäußerste Schale beschränken, sind sie in ihren Eigenschaften einander sehr ähnlich. Deshalb stehen sie alle in derselben, nämlich der 3. Nebengruppe. Sie werden nach dem gemäß der Ordnungszahl jeweils ersten Nebengruppenelement ihrer Periode als Lanthanoide (6. Periode) und Actinoide (7. Periode) bezeichnet. Bei allen auf das letzte Actinoid bzw. Lanthanoid folgenden Neben- und Hauptgruppenelementen besitzt die N- bzw. O-Schale bereits 32 Elektronen. Stattdessen wird bei den Nebengruppenlementen mit steigender Ordnungszahl die zweitäußerste Schale auf maximal 18 Elektronen und bei den sich anschließenden Hauptgruppenelementen endlich auch die äußerste Schale auf maximal 8 Elektronen aufgefüllt.
Einige Eigenschaften der Elemente lassen sich in bestimmten Positionen und Bereichen des Periodensystems finden oder mit ihm voraussagen:

Als weitere Informationen, die aber mit der Elektronenkonfiguration und daher mit der Stellung im PSE nichts zu tun haben, sind die radioaktiven Elemente gekennzeichnet:
Das Element 82 (Blei) ist das letzte Element, von dem stabile, also nicht radioaktive Isotope existieren. Alle nachfolgenden (Ordnungszahl 83 und höher) sind ausnahmslos radioaktiv und somit instabil. Dabei ist 83 (Bismut) ein Sonderfall oder Grenzfall mit einer extrem langen Halbwertszeit. Auch innerhalb der Elemente 1 bis 82 sind zwei Stoffe enthalten, die radioaktiv, also instabil sind: 43 (Technetium) und 61 (Promethium).

So bleiben tatsächlich nur 80 stabile Elemente übrig, die in der Natur vorkommen – alle anderen sind radioaktive Elemente. Von den radioaktiven Elementen sind nur Bismut, Thorium und Uran in größeren Mengen in der Natur vorhanden, da diese Elemente Halbwertszeiten in der Größenordnung des Alters der Erde oder länger haben. Alle anderen radioaktiven Elemente sind bis auf ein Isotop des Plutoniums entweder wie das Radium intermediäre Zerfallsprodukte einer der drei natürlichen radioaktiven Zerfallsreihen oder entstehen bei seltenen natürlichen Kernreaktionen oder durch Spontanspaltung von Uran und Thorium. Elemente mit Ordnungszahlen über 94 können nur künstlich hergestellt werden; obwohl sie ebenfalls bei der Elementsynthese in einer Supernova entstehen, wurden aufgrund ihrer kurzen Halbwertszeiten bis heute noch keine Spuren von ihnen in der Natur gefunden. Das letzte bislang nachgewiesene Element ist Oganesson mit der Ordnungszahl 118, dieses hat allerdings nur eine Halbwertszeit von 0,89 ms.

Die Datierung der Entdeckung solcher chemischen Elemente, die bereits seit der Frühzeit oder Antike bekannt sind, ist nur ungenau und kann je nach Literaturquelle um mehrere Jahrhunderte schwanken. Sicherere Datierungen sind erst ab dem 18. Jahrhundert möglich. Bis dahin waren erst 15 Elemente als solche bekannt und beschrieben: 12 Metalle (Eisen, Kupfer, Blei, Bismut, Arsen, Zink, Zinn, Antimon, Platin, Silber, Quecksilber und Gold) und drei Nichtmetalle (Kohlenstoff, Schwefel und Phosphor).

Die meisten Elemente wurden im 19. Jahrhundert entdeckt und wissenschaftlich beschrieben. Zu Beginn des 20. Jahrhunderts waren nur noch zehn der natürlichen Elemente unbekannt. Seither wurden vor allem schwer zugängliche, oftmals radioaktive Elemente dargestellt. Viele dieser Elemente kommen nicht in der Natur vor und sind das Produkt von künstlichen Kernverschmelzungsprozessen. Erst im Dezember 1994 wurden die beiden künstlichen Elemente Darmstadtium (Eka-Platin) und Roentgenium (Eka-Gold) hergestellt.

Anfang des 19. Jahrhunderts stellte Johann Wolfgang Döbereiner erstmals einen Zusammenhang zwischen der Atommasse und den chemischen Eigenschaften einzelner Elemente fest. Alexandre-Emile Béguyer de Chancourtois entwickelte 1862 eine dreidimensionale Darstellung, wobei er die Elemente nach steigenden Atomgewichten schraubenförmig auf einem Zylinder anordnete. 1863 stellte John Alexander Reina Newlands eine nach Atommassen geordnete Tabelle der Elemente in Achtergruppen (Gesetz der Oktaven) auf.

Das gültige Periodensystem selbst wurde 1869 nahezu gleichzeitig und unabhängig voneinander zuerst von Dmitri Iwanowitsch Mendelejew (1834–1907) und darauf von Lothar Meyer (1830–1895) aufgestellt. Dabei ordneten sie ebenfalls die chemischen Elemente nach steigenden Atommassen, wobei sie Elemente mit ähnlichen Eigenschaften (Anzahl der Valenzelektronen) untereinander anordneten. Daneben wurden von Heinrich Adolph Baumhauer und Julius Quaglio Versuche unternommen, das System spiralförmig darzustellen. Im 20. Jahrhundert wurde der Aufbau der Atome entdeckt, die Periodizität wurde durch den Aufbau der Elektronenhülle erklärt.

Dieses Periodensystem gibt einen Überblick über die Entdecker bzw. Erzeuger der einzelnen Elemente durch Anklicken der Elementenkennung. Für die Elemente, für die kein Entdecker/Erzeuger bekannt ist, wird der aktuelle historische Wissensstand unter dem Übersichtsplan kurz wiedergegeben.

Die Form des Periodensystems von Dmitri Mendelejew hat sich durchgesetzt. Dennoch gab (und gibt) es weitere Vorschläge für alternative Ordnungen der Elemente nach ihren Eigenschaften.

Kein alternatives Periodensystem, aber dennoch eine deutlich anders aussehende Darstellung ist das Kurzperiodensystem, bei dem Haupt- und Nebengruppen ineinander verschachtelt sind.





</doc>
<doc id="3841" url="https://de.wikipedia.org/wiki?curid=3841" title="Zeittafel der Programmiersprachen">
Zeittafel der Programmiersprachen



</doc>
<doc id="3844" url="https://de.wikipedia.org/wiki?curid=3844" title="Physikalische Konstante">
Physikalische Konstante

Eine physikalische Konstante oder Naturkonstante (gelegentlich auch Elementarkonstante) ist eine physikalische Größe, deren Wert sich nicht beeinflussen lässt und sich weder räumlich noch zeitlich verändert.

Als fundamentale Naturkonstante werden die Konstanten bezeichnet, die sich auf allgemeine Eigenschaften von Raum, Zeit und physikalischen Vorgängen beziehen, die für jede Art Teilchen und Wechselwirkung gleichermaßen gelten. Diese sind die Lichtgeschwindigkeit, das plancksche Wirkungsquantum und die Gravitationskonstante (siehe auch Natürliche Einheiten).

Weitere elementare (oder grundlegende) Naturkonstanten beziehen sich auf die einzelnen Teilchenarten und Wechselwirkungen, z. B. ihre Massen und Ladungen. Abgeleitete Naturkonstanten lassen sich aus den fundamentalen und elementaren Konstanten berechnen. Beispielsweise ist der bohrsche Radius, eine für die Atomphysik maßgebliche Konstante, aus dem planckschen Wirkungsquantum, der Lichtgeschwindigkeit, der Elementarladung und der Masse des Elektrons zu berechnen.

Teilweise werden auch Parameter oder Koeffizienten, die nur in einer bestimmten Anordnung oder Konstellation konstant sind, als "Konstante" bezeichnet, so etwa die Kepler-Konstante, die Zerfallskonstante oder die Federkonstante etc. Streng genommen sind es aber keine Konstanten, sondern Parameter der untersuchten Anordnung.

Einige Naturwissenschaften fassen wichtige Konstanten zu Gruppen von "Fundamentalkonstanten" zusammen, z. B. in der Astronomie und Geodäsie sind dies die genauen Referenzwerte von Erd- und Sonnenmasse, der Erdradius, die astronomische Einheit oder die Gravitationskonstante.

In der Praxis gebräuchliche Referenzwerte, wie etwa die Dauer eines Jahres, der Druck der Standardatmosphäre oder die Erdbeschleunigung, sind keine Naturkonstanten. Sie sind dem Menschen in seiner irdischen Umgebung nützlich, haben aber in der Regel keine darüber hinausgehende Bedeutung grundlegender Art und erweisen sich bei zunehmender Messgenauigkeit auch nicht als wirklich konstant. Allerdings dienten sie zur ersten Festlegung von Maßeinheiten (auch z. B. für Sekunde, Meter, Kilogramm). Daher gehen aktuell die Bemühungen dahin, die Maßeinheiten möglichst durch direkten Bezug zu (fundamentalen oder elementaren) Naturkonstanten zu definieren. Die dafür ausgewählten Naturkonstanten erhalten dadurch einen fest definierten, unveränderlichen Zahlenwert. Siehe dazu auch Internationales Einheitensystem.

Die Ziffern in Klammern hinter einem Zahlenwert bezeichnen die Unsicherheit in den letzten Stellen des Wertes. (Beispiel: Die sog. Kurzschreibweise 6,674 08(31) ist gleichbedeutend mit 6,674 08 ± 0,000 31.) Die Unsicherheit ist als geschätzte Standardabweichung des angegebenen Zahlenwertes vom tatsächlichen Wert angegeben. Die Zahlenwerte beruhen auf CODATA 2014.
Ob die Naturkonstanten auch über astronomische Zeiträume hinweg wirklich "konstant" sind, ist Gegenstand aktueller Forschung. So schienen Messungen der Spektrallinien von Quasaren mit dem Keck-Teleskop auf Hawaii auf eine leichte Abnahme der Feinstrukturkonstante um etwa ein hundertstel Promille im Verlauf von zehn Milliarden Jahren hinzudeuten. Dieses Resultat war von Anfang an umstritten; zum einen wiesen Forscher auf die unsichere Fehlerabschätzung der Datenauswertung hin, zum anderen gibt es Daten aus der Oklo-Mine in Westafrika, wo vor etwa 2 Milliarden Jahren Uran so stark angehäuft war und einen so hohen Gehalt des Isotops U-235 hatte, dass eine Kernspaltungs-Kettenreaktion stattfand. Nach diesen Daten hatte die Feinstrukturkonstante damals denselben Zahlenwert wie heute. Neuere Messungen der Spektrallinien von Quasaren mit dem Very Large Telescope der Europäischen Südsternwarte in Chile widersprechen den früheren Resultaten am Keck-Teleskop und weisen auf die Konstanz der Feinstrukturkonstante hin.

Inzwischen sind Präzisionsmessungen möglich, die etwaige stetige Schwankungen in der Größenordnung, wie sie die Beobachtungen mit dem Keck-Teleskop nahelegen, auch im Labor in kurzen Zeiträumen überprüfen können. Untersuchungen von Theodor Hänsch und seiner Arbeitsgruppe am Max-Planck-Institut für Quantenoptik belegen die Konstanz der Feinstrukturkonstante mit einer Genauigkeit von 15 Nachkommastellen über einen Zeitraum von vier Jahren.

Wie sich die Angaben der Naturkonstanten durch immer genauere Messungen ändern, hält das "Committee on Data for Science and Technology", kurz CODATA, in Dokumenten fest. Das eng mit CODATA zusammenarbeitende "National Institute of Standards and Technology" (NIST) in den USA veröffentlicht bereits seit einiger Zeit online PDF-Dokumente mit aktuellen Abschätzungen der Werte der physikalischen Konstanten, darunter auch ältere Dokumente, mit denen sich z. B. alle Veränderungen im Zeitraum von 1986 bis 2014 erfassen lassen.





</doc>
<doc id="3845" url="https://de.wikipedia.org/wiki?curid=3845" title="Point-of-View-Shot">
Point-of-View-Shot

Ein Point-of-View-Shot (engl., etwa: "Einstellung mit einem bestimmten Standpunkt", Abkürzung "POV-Shot") ist in der Filmtheorie eine Einstellung, die den Zuschauern einen Blick durch die Augen einer Figur der dargestellten Handlung ermöglicht. 

Der entsprechende deutsche Begriff lautet "subjektive Kamera" oder "subjektive Einstellung", kurz "Subjektive". 

Eine subjektive Einstellung ist häufig eine von zwei direkt aufeinanderfolgenden Einstellungen: Die eine Einstellung zeigt eine Figur, die irgendwo hin blickt, meist auf einen Punkt außerhalb des Bildes. Die andere Einstellung (der eigentliche POV-Shot) zeigt das, was die Figur betrachtet, von der Position der Figur aus gefilmt.

In welcher Reihenfolge diese Einstellungen geschnitten werden, ist nicht zwingend. Das heißt, es ist einerseits möglich, zuerst den POV-Shot zu zeigen und ihn erst anschließend als solchen kenntlich zu machen, wenn nämlich in der folgenden Einstellung die blickende Person gezeigt wird. Oder aber man zeigt erst die blickende Person und dann den POV-Shot.

POV-Shots sind illusionierend, d. h., der Zuschauer fühlt sich in die Handlung hineinversetzt. Oft sind POV-Shots technisch verfremdet: Unschärfe signalisiert etwa den Blick eines Brillenträgers ohne Brille. Die Subjektivierung des POV-Shots wird oft widersprüchlich kombiniert mit einer Objektivierung durch technische Geräte, etwa dem Blick durch Fernrohre oder Nachtsichtgeräte. Beides erhöht den Eindruck der Authentizität.

Manchmal werden POV-Shots durch auffällig bewegte, scheinbar unprofessionelle Kameraführung (Handkamera, Steadicam) deutlich gemacht.

Der Film "Der Florentiner Hut" von Wolfgang Liebeneiner aus dem Jahr 1939 enthält einige technische Raffinessen. In einer langen Sequenz am Anfang erlebt der Zuschauer die Szene aus der Sicht der Figur Theo Farina, ohne ihn selbst zu sehen. Erst später erkennt man Heinz Rühmann in dieser Rolle. Dazu war eine bewegliche Kamera nötig, die es bei der ersten Verfilmung dieses Stoffes von Eugène Labiche noch nicht gab.

Der Film "Die Dame im See" (orig. "The Lady in the Lake") aus dem Jahr 1947 ist der erste Film, der den POV-Shot den ganzen Film hindurch nutzt. Der Regisseur Robert Montgomery präsentiert die Handlung ausschließlich aus der Sicht der Hauptfigur Philip Marlowe. Der Kameramann war Paul Vogel. Den Hauptdarsteller (ebenfalls Montgomery) erkennt man nur, wenn er vor einen Spiegel (oder ähnliche reflektierende Oberflächen) tritt. Die Produktionsfirma MGM warb damit, den revolutionärsten Film seit Einführung des Tonfilms zu zeigen, Zuschauer und Hauptfigur würden das Geheimnis des Verbrechens gemeinsam lösen.

"Smack My Bitch Up" ist das Video von Jonas Åkerlund zur 13. Singleveröffentlichung der britischen Big-Beat-Gruppe The Prodigy. Das mit zwei MTV Video Music Awards ausgezeichnete Musikvideo zeigt in Verwendung einer subjektiven Kameraperspektive einen exzessiven Nachttrip einer zunächst unerkannt bleibenden Person. Zu Beginn befindet sich diese im Badezimmer und zieht sich danach an, während sie bereits alkoholische Getränke sowie Kokain zu sich nimmt. Der Protagonist begibt sich in der Folge ins Londoner Nachtleben, wo er durch aggressives Verhalten gegenüber Frauen und DJs auffällig wird. Zwischenzeitlich übergibt er sich mehrfach und nimmt Heroin zu sich. Durch den Einsatz visueller Verzerrungen wird die fortschreitende Wirkung der eingenommenen Drogen sichtbar. Hierauf begibt er sich in einen Stripclub, in welchem er sich einer der dort engagierten Tänzerinnen annähert und diese mit in ein soeben geklautes Auto nimmt. Er fährt mit ihr in sein Appartement, in dem er Sex mit ihr hat. Zum Schluss des Videos zeigt die subjektive Kamera einen Blick in einen Spiegel, aus dem hervorgeht, dass es sich bei der von vornherein als männlich wahrgenommenen Person um eine Frau handelt.

"Enter the Void" ist ein französischer Spielfilm aus dem Jahr 2009, der komplett aus der Perspektive des Protagonisten Oscar erzählt wird. Die Point-of-View-Einstellung zieht sich dabei konsequent durch den gesamten Film und wird nur durch einige sehr eindrucksvolle Drogentrips unterbrochen, die durch Animated Visuals dargestellt werden. Die Idee zum Film kam dem Regisseur nach eigenen Aussagen, als er sich den Film "Die Dame im See" unter Einfluss von halluzinogenen Pilzen ansah.

"Hardcore" ist ein russisch-amerikanischer Action-Spielfilm aus dem Jahre 2015, der komplett aus der Sicht des Hauptdarstellers gefilmt wurde. Der Regisseur Ilja Naischuller hatte bereits 2013 in seinem Musikvideo "Bad Motherfucker" seiner Band Biting Elbows eine vergleichbare Technik angewandt.

In einigen Filmen, vor allem im Horrorfilm, wird ein POV-Shot als durchgehende Erzählperspektive eingesetzt. Der Zuschauer erfährt durch die Handlung, aus wessen Perspektive der Shot erfolgt (meistens aus der des Mörders oder der des Monsters), aber ein Überblick wird nicht gegeben, um ein Gefühl der Unsicherheit und des Ausgeliefertseins zu erzeugen.

Häufig wird der POV-Shot aus Sicht eines männlichen Akteurs in Pornofilmen verwendet; teils ist die Regie ganzer Szenen und Filme von diesem Stilmittel bestimmt. Der Pornodarsteller Peter North vertreibt eine eigene Filmreihe unter dem Titel „P.O.V.“. In der japanischen Pornografie heißt dieses Genre "Hamedori" (jap. ハメ撮り).

Unter anderem nutzt die Fernsehserie "" das Stilmittel in verschiedenen Episoden, beispielsweise in "I've Got a Pain in My Sawdust!" (Staffel 8, Folge 1).



</doc>
<doc id="3846" url="https://de.wikipedia.org/wiki?curid=3846" title="Plansequenz">
Plansequenz

Eine Plansequenz (frz. ', etwa: ‚fortlaufende Sequenz‘) ist eine Sequenz innerhalb eines Films, die nur aus einer einzigen, meist vergleichsweise langen Einstellung besteht und eine abgeschlossene Handlung ohne Schnitte zeigt. Die Szenenänderungen, die die klassischen Schnitte ersetzen, werden entweder durch passend inszenierte Auftritte der Darsteller erreicht (das kann sogar mit einer starren, also unbewegten, Kameraposition sein), durch Ortsveränderungen innerhalb des Filmsets oder beides in Kombination. Hierbei wird das Geschehen meist durch den Einsatz einer Kamerafahrt unterstützt, was auch in der früher gebräuchlichen Bezeichnung Horizontalmontage deutlich wird.

Die Plansequenz als solche legt noch keine bestimmte Dramaturgie fest, unterstützt aber die Inszenierung einer Szene. So kann sie z. B. durch den "Nur-wenig-geschieht-Effekt", gerade in Verbindung mit einer starren Kameraposition, für den Zuschauer eine melancholische Stimmung hervorrufen "(Lichter der Großstadt)" wie auch durch den "Dauernd-passiert-irgendwas-Effekt" "(Mein Onkel)" eine eher komödiantische Empfindung auslösen. Königsdisziplin der Plansequenz ist der "Lange-geschieht-nichts-Effekt"; hier wird eine ungeheuere Spannung aufgebaut "(Der unsichtbare Dritte)".

Eine besondere Herausforderung für Regisseure sind Szenen, die im fertigen Film wie Plansequenzen aussehen sollen, aus bestimmten Gründen am Filmset aber nicht durchgeführt werden können. Dazu zählt "Cocktail für eine Leiche". Wegen der begrenzten Länge einer Filmrolle (35 mm Kameranegative sind auf 300 m konfektioniert, was etwa 10 Minuten entspricht) gestaltete Hitchcock bestimmte Übergänge optisch so (Kamerazufahrt aus der Szene auf ein Bild, Kamerastopp und nach dem Schnitt Kamerarückfahrt vom Bild in die Szene zurück, also de facto ein unsichtbarer Stoptrick), dass der tatsächliche harte Schnitt nicht als solcher zu erkennen ist. Wesentlich ausgeklügelter ist die Alarmszene in "Das Boot"; hier lag die Beschränkung nicht in der Rollenlänge des Films, sondern darin, dass der Kameramann wegen der Enge der originalgetreuen Kulisse des Innenraums des U-Bootes und der Einteilung in Schotte, nicht mit der Mannschaft durchstürmen konnte. So wurde die Szene in Takes gedreht und schnelle optische Übergangshilfen für die harten Schnitte eingesetzt (Vorhang, Overall für wenige Frames bildfüllend über Schwarz, dichter Rauch), die im fertigen Film den Eindruck erwecken, es handele sich um eine echte Plansequenz.

Eine Plansequenz wird beispielsweise eingesetzt, um den Schauspielern – ähnlich dem Theater – mehr Raum zum Spielen zu geben. Ihr Spiel kann sich dadurch in einem Fluss entfalten. Die Szene wird dabei nicht 'klassisch' in einzelne Shots zerlegt, die jeweils nur ein kleines Stück der Szene repräsentieren und erst am Schneidetisch zur Szene verbunden werden.

Bei der Produktion von Musikvideos wird auch von "Oneshot" gesprochen, wenn keine einzelnen Takes gedreht werden. Das Video kann dabei auch turbulente Szenenwechsel enthalten. Überraschende Kamerabewegungen, Tanzauftritte, Lichteffekte manchmal in Verbindung mit Bühnennebel oder Trockeneis, Pyrotechnik oder auch Effekte der Postproduktion können dabei Schnitte ersetzen. Trotzdem besteht ein Oneshot aus einer einzigen durchgängig gedrehten, also auch später ungeschnittenen Einstellung und ist damit auch eine Plansequenz. Ein Video, das komplett aus einem Oneshot besteht, wird One-Cut-Video genannt.

1948 brachte Alfred Hitchcock den 80-minütigen Spielfilm "Cocktail für eine Leiche" ins Kino, der im Wesentlichen aus nur fünf langen De-facto-Plansequenzen bestand. Die wegen des nur für jeweils zehn Minuten reichenden Filmvorrats in der Kamera notwendigen (technischen) Schnitte wurden dadurch kaschiert, dass am Ende einer Filmrolle die Kamera jeweils auf einen Gegenstand oder Darsteller nahe heranfuhr und sich – nach dem Wechsel der Filmrolle – wieder entfernte. Über dieses Experiment hinaus setzte Hitchcock oft Plansequenzen in seinen Filmen ein: u. a. die Vorstellung der Nachbarn und die dialoglose Einführung des Protagonisten in "Das Fenster zum Hof" und die langsame Fahrt vom Ort des Verbrechens auf die belebte Straße in "Frenzy."

Eines der berühmtesten Beispiele für eine Plansequenz ist der Anfang von Orson Welles’ "Im Zeichen des Bösen" (1958).

Als großer Meister der Plansequenz gilt der Nouvelle-Vague-Regisseur Jean-Luc Godard. Meisterhafte Beispiele von endlos langen Plansequenzen finden sich in seinem Film "Die Verachtung" (1963, mit Brigitte Bardot und Michel Piccoli). Auch Michelangelo Antonioni nutzte dieses filmische Mittel 1975 eindrucksvoll in "."

In vielen Filmen des sowjetischen Regisseurs Andrei Tarkowski, z.B. "Nostalghia" (1983) und "Opfer" (1986), spielen Plansequenzen sowohl für die Handlung als auch für die Wirkung eine zentrale Rolle.

Von Tarkowski inspiriert, setzte der ungarische Regisseur Béla Tarr ab 1982 seine Filme mit langen Einstellungen um, die nicht selten eine ganze 35-mm-Rolle dauerten.

In dem Film Good Fellas (1990) wurde mit der Szene "A Night at the Club" die bekannteste und bis dahin längste zusammenhängende Sequenz mittels Steadicam aufgezeichnet.

Ironisiert wird das Prinzip der Plansequenz in Robert Altmans "The Player" (1992). Der Film eröffnet mit einer siebenminütigen Einstellung, in der er das rastlose Treiben auf einem Hollywood-Studiogelände etabliert und einen der Protagonisten gebetsmühlenartig Orson Welles’ Plansequenz in "Im Zeichen des Bösen" als Fanal gegen die moderne Unsitte des schnellen Schnitts hochhalten lässt.

Mit "Russian Ark" drehte der russische Regisseur Alexander Sokurow 2002 den ersten abendfüllenden Spielfilm in einer einzigen Einstellung. Durch die Fortschritte in der Videotechnik war es möglich, den kompletten Film auf Festplatte aufzuzeichnen.

Gaspar Noés rückwärts erzählter Skandalfilm "Irreversibel" (2002) besitzt eine scheinbare Schnittlosigkeit, die unter Zuhilfenahme moderner Tricktechnik vorgenommen wurde. Ähnlich trickreich entstanden zum Teil über 6 Minuten lange Takes in Alfonso Cuaróns Children of Men (2006), die dem Film einen dokumentarischen Touch verleihen. Ein weiterer Film, der komplett aus einem einzigen Take zu bestehen scheint, ist "Birdman oder (Die unverhoffte Macht der Ahnungslosigkeit)" (2014) des Regisseurs Alejandro González Iñárritu. Tatsächlich befinden sich in "Birdman" jedoch mehrere Schnitte, die durch komplettes Schwarzbild, Zeitraffersequenzen oder andere technische Effekte kaschiert wurden.

2015 schuf Sebastian Schipper mit "Victoria" einen fast 140 Minuten langen Spielfilm, der komplett in einer einzigen Plansequenz realisiert wurde. Dabei wurden viele Dialoge und Szenen improvisiert. Nach diversen Proben wurde dreimal der komplette Film am Stück gedreht, die finale Fassung wurde in einem Stück gelassen. Der 2013 in Venedig uraufgeführte 134-minütige Film Fish & Cat wurde zwar in einer einzigen Plansequenz gedreht, erzählt die Ereignisse aber dennoch nicht in chronologischer Reihenfolge.

Ein klassischer Vertreter des aus einer Plansequenz bestehenden Musikvideos ist das Video zu dem Song "Unfinished Sympathy" (1991) der britischen Band Massive Attack von Baillie Walsh.

Zu den europäischen Regisseuren, die Plansequenzen als erzählerisches Mittel einsetzten, gehörte der griechische Regisseur Theo Angelopoulos. In seinem Film "Der Blick des Odysseus" von 1995 dient eine Plansequenz dazu, zu zeigen, wie sich an einem Ort Geschichte und Gegenwart verbinden. Der Protagonist des Films (gespielt von Harvey Keitel) kehrt auf seiner Odyssee über den Balkan Anfang der 1990er Jahre in seine Geburtsstadt (Constanța) zurück und betritt das Haus seiner Kindheit, womit die Plansequenz beginnt. Hier wird er herzlich von seiner versammelten Familie (Mutter, Großeltern usw.) empfangen, befindet sich aber jetzt nicht mehr in den 1990er Jahren, sondern im Jahr 1944, kurz vor dem Ende des Zweiten Weltkriegs. Innerhalb der weiteren, insgesamt etwa zehnminütigen Sequenz betreten und verlassen Schauspieler (mit Ausnahme des Protagonisten) fortwährend die Szene, die teils von der sich bewegenden, hauptsächlich aber stehenden Kamera aufgenommen wird. Die Szene endet 1950 mit dem Exodus der Griechen aus Constanța. Die Familie versammelt sich für ein letztes Foto, für das dann ein Kind, das aus der Richtung der Kamera die Szene betritt, den Platz des Protagonisten einnimmt, der erst kurz zuvor die Szene in Richtung Kamera verlassen hat. Mit einer langsamen Fahrt auf das in der Mitte der Gruppe zwischen seinen Eltern stehende, in die Kamera blickende Kind endet die Sequenz.



</doc>
<doc id="3847" url="https://de.wikipedia.org/wiki?curid=3847" title="Politiker">
Politiker

Als Politiker wird eine Person bezeichnet, die ein politisches Amt oder Mandat innehat oder in sonstiger Weise dauerhaft politisch wirkt. Politiker sind meist Mitglied einer Partei.

Politiker agieren auf allen Ebenen eines Staates oder einer Partei. Manchmal werden sie entsprechend benannt (Bundespolitiker, Landespolitiker, Kommunalpolitiker). Politische Ämter können Regierungsämter (z. B. Minister) oder ein Amt in einer Partei (z. B. Parteivorsitzender, dort ohne Volkswahl) sein. Politische Mandate werden in den Gremien der Legislative und in einigen Positionen der Exekutive ausgeübt. Verschiedene Denkrichtungen sehen eine Trennung von Amt und Mandat als wünschenswert an.

Politiker haben das Ziel, durch ihr Denken Probleme der Gesellschaft zu lösen und durch ihr Handeln Einfluss auf politische Entscheidungen zu nehmen. Hierzu können sie zum einen ihre durch politische Ämter gesicherten Rechte nutzen (z. B. bei Abstimmungen im Parlament). Außerdem können sie durch Meinungsäußerung Einfluss nehmen.

Als Mitglied einer Partei vertritt ein Politiker deren Interessen. Es gibt jedoch auch Politiker, die sich keiner Partei anschließen (Parteilose) oder deren Aufgabe nicht die Interessenvertretung ihrer Partei ist (z. B. Präsidenten eines Staates). Neben dem Berufspolitiker, der z. B. als Abgeordneter, Staatssekretär, Minister oder Vizeminister oder als bezahlter Parteifunktionär arbeitet, gibt es noch den ehrenamtlich arbeitenden Politiker, der die Politik nur neben seinem Beruf ausübt, beispielsweise im politischen System der Schweiz.

Kommunalpolitiker arbeiten grundsätzlich ebenfalls ehrenamtlich als Mitglied des Gemeinderats, des Kreistags oder seiner Ausschüsse. Überwiegend werden auch die hauptamtlichen kommunalen Wahlbeamten nicht nur als Leiter der Kommunalverwaltung, sondern auch als Kommunalpolitiker angesehen. Grundsätzlich kann festgestellt werden, dass in kleineren Kommunen parteipolitische Motive eine geringere Rolle spielen, als in größeren.

Das Handeln von Politikern ist Gegenstand der Politikwissenschaft. Sie erklärt das Handeln der Politiker und den politischen Wettbewerb.

Grundsätzliche Bedeutung für die politische Motivation Einzelner hat der Wunsch, gute politische Entscheidungen herbeizuführen, um beispielsweise der eigenen Region oder dem ganzen Land zu helfen. Dies führt zu einem Einsatz zum Wohle aller Bürger, wie es beispielsweise die Verantwortungsethik postuliert.

Oft werden diese langfristigen Ziele jedoch von den Wählern nicht als beste Wahl wahrgenommen, weswegen der politische Erfolg solcher Positionierungen begrenzt ist. Ferner gehen die Auffassungen darüber, was langfristig das „Wohl aller Bürger“ bzw. „Wohl des Staatsvolkes“ darstellt sowie auf welchem Weg dieses erreicht werden soll, auseinander. Auch dies trägt dazu bei, dass sich im politischen Wettbewerb nicht zwangsläufig das „beste“ Modell durchsetzt. Zweifelsohne lässt sich auch für Politiker, deren Handeln an ihren Zielen orientiert ist, ein karrierebezogenes Politikerbild erklären: Die Überzeugung, selbst die richtigen Entscheidungen zu treffen, führt zu einem Streben nach Macht und Einfluss.

Einen weniger positiven Ansatz zur Erklärung des Handelns von Politikern mit wirtschaftlichen Grundsätzen liefert die Neue Politische Ökonomie (NPÖ). Sie erklärt Strukturen und Verhalten überwiegend auf Basis der neoklassischen Theorie. Grundsätzliche Annahme ist dabei, dass sich Politiker als rationale Nutzenmaximierer verhalten. Dies bedeutet im Wesentlichen, dass Politiker eine starke "Wiederwahlorientierung" haben und deswegen eine Politik betreiben, die bei den nächsten Wahlen zu einer "Stimmenmaximierung" führt.

Hierzu lassen sich zwei wichtige Stränge unterscheiden:

Ein gemäß der NPÖ nutzenmaximierender Politiker wird bei seinen Entscheidungen berücksichtigen, dass der Wähler eher die Erreichung kurzfristiger Ziele als das Anstreben langfristiger Ziele honoriert, da der Wähler selbst eine starke Gegenwartspräferenz aufweist, was wiederum daran liegt, dass langfristig ausgerichtete Konzepte dem politisch und ökonomisch weniger gebildeten Wähler wegen der hohen Komplexität nicht vermittelbar sind. Auf Wiederwahl bedacht wird der Politiker daher vor kurzfristig schmerzhaften Maßnahmen zurückschrecken, auch wenn diese ökonomisch oder politisch unbedingt nötig sind.

Beispiele für eine solche Politik sind die dauerhaft zu beobachtende Neuverschuldung reicher Industrienationen, fehlende Rücklagen im gesetzlichen Rentensystem, zyklische statt antizyklischer Wirtschaftspolitik oder fehlender Mut zu schmerzhaften, aber notwendigen Reformen.

Ein wichtiger Ansatz in diesem Zusammenhang ist das Medianwählermodell: Geht man bei Politikern vom Ziel der Stimmenmaximierung aus, so führt ein Politiker bzw. eine Partei genau diejenige Politik aus, die der Medianwähler wünscht. Dadurch werden von den großen Parteien politische Ränder und Problembereiche vernachlässigt.

Zudem können für den Bürger sichtbare, ökonomisch aber nicht zwangsläufig vernünftige Maßnahmen unterstellt werden, während möglicherweise wichtigeren Zielen, die jedoch nicht vom Wähler als solche erkannt werden, nicht nachgegangen wird. Vielmehr können dann individuell spürbare Maßnahmen bei wenig spürbaren Belastungen für den Wähler unterstellt werden.





</doc>
<doc id="3848" url="https://de.wikipedia.org/wiki?curid=3848" title="Pfarrer">
Pfarrer

Pfarrer ist ein in christlichen Gemeinden verwendeter Begriff für eine Person, die mit der Leitung von Gottesdiensten, der seelsorglichen Betreuung und in der Regel auch mit der Leitung einer Kirchengemeinde betraut ist. In der römisch-katholischen Kirche kann nur ein Priester Pfarrer einer Gemeinde sein. Anstelle der Leitung einer Gemeinde kann ein Pfarrer jedoch auch einen besonderen Dienst übernehmen. Die römisch-katholische Kirche, die Evangelische Kirche in Deutschland und die Selbständige Evangelisch-Lutherische Kirche regeln die Rechte und Pflichten der Pfarrer durch Kirchengesetz (Pfarrerdienstrecht), das sich weitgehend am staatlichen Beamtenrecht und an den Laufbahnen von Studienräten orientiert.

Das Wort Pfarrer geht auf das ältere Pfarre „Amtsbezirk eines Pfarrers“ zurück, das vom griechischen " παροικία" („Nachbarschaft“) entlehnt wurde. „Pfarrin“ bezeichnete im 19. Jahrhundert die Ehefrau des Pfarrers.

Ein Pfarrer bekleidet in der Regel ein Pfarramt. In Mittel- und Norddeutschland wird der Pfarrer auch Pastor genannt. Die Stellung und Aufgaben eines Pfarrers sind in den christlichen Konfessionen unterschiedlich: In den evangelischen Kirchen und der Altkatholischen Kirche ist der Pfarrer aus dem Kreise der bei der jeweiligen Kirche verbeamteten Geistlichen entweder von der Gemeinde gewählt oder von der übergeordneten Kirchenleitung ernannt. Im zweiten Fall muss auf die Ernennung die Vokation durch den Kirchenvorstand erfolgen. In seltenen Fällen wird die Pfarrstelle aber auch nach Präsentation durch einen Patron besetzt.

In den evangelischen Kirchen wird man durch die Ordination zum Pfarrer. Ein Bewerber wird dadurch mit der öffentlichen Verkündigung des Wortes Gottes und der Verwaltung der Sakramente (vor allem Taufe und Abendmahl) beauftragt. In der Ordination verspricht der Pfarrer seelsorgerliche Verschwiegenheit und die Wahrung des Beichtgeheimnisses. Ein Pfarrer kann dann eine Gemeinde leiten oder auch in einem anderen Bereich geistliche Aufgaben erfüllen. Die meisten evangelischen Kirchen ordinieren Frauen und Männer ausschließlich ins Gemeindepfarramt und setzen für Sonderpfarrämter eine Praxiszeit im Gemeindepfarramt voraus.

In der römisch-katholischen Kirche wird ein geweihter Priester vom Diözesanbischof zum Pfarrer ernannt, nachdem er die entsprechenden Examina abgelegt hat. Neben der Spendung der Sakramente und der Verkündigung des Wortes Gottes in Gottesdienst und Seelsorge gehört zum klassischen Gemeindepfarramt immer auch die Verwaltung einer Gemeinde, etwa das Pflegen der Kirchenbücher und die dienstrechtliche Aufsicht über die Mitarbeitenden.

Um Pfarrer werden zu können, muss ein Bewerber Theologie studiert und Examina nach den Kirchenordnungen abgelegt haben. In der Regel gibt es eine akademische und eine praktische Vorbereitungsphase für den Pfarrdienst. In der katholischen Kirche stehen vor der Priesterweihe die akademischen und nach einer Praxiszeit als Kaplan in einer Pfarrgemeinde die praktischen Prüfungen (Pfarrexamen). Ein Pfarramt wird verliehen, nachdem sich ein Priester einige Jahre im geistlichen Dienst bewährt hat.

Die Einführung eines Pfarrers in sein Amt bzw. in seine Gemeinde geschieht im Rahmen einer feierlich gestalteten Heiligen Messe. Dies soll bei der Gemeinde und beim Bewerber den Glauben stärken, dass Gott ihn in seinen Dienst beruft.

Neben dem Wirkungsbereich innerhalb einer Ortsgemeinde (Pfarrei; in Österreich: Pfarre) arbeiten Pfarrer auch in anderen Institutionen: Schulpfarrer, Krankenhauspfarrer, Gefängnispfarrer, Leiter karitativer Einrichtungen, Studenten-/Hochschulpfarrer, Studienleiter an katholischen Akademien oder evangelischen Akademien, Wirtschafts- und Sozialpfarrer, Betriebsseelsorger, Militärpfarrer, Polizeipfarrer, Fernsehpfarrer, Medienpfarrer, Zirkuspfarrer oder Schaustellerpfarrer. Katholische Geistliche in solchen Funktionen erhalten den Titel "Pfarrer" durch bischöfliches Dekret.

Pfarrer waren in früheren Zeiten gelegentlich auch in ganz anderen Bereichen tätig, etwa als Mathematiker, Techniker, und Erfinder, zum Beispiel René-Just Haüy, Jacob Christian Schäffer, Michael Stifel, Friedrich Christoph Oetinger, Franz Senn und Philipp Matthäus Hahn. Politisch aktive Pfarrer waren bzw. sind Carl Sonnenschein, Peter Hintze und Joachim Gauck.

Katholischen Priestern ist seit 1983 durch das Kirchenrecht untersagt, öffentliche Ämter anzunehmen, die eine Teilhabe an der Ausübung weltlicher Gewalt mit sich bringen.

In der römisch-katholischen Kirche amtiert der Pfarrer zwar unter der Autorität und im Namen des Bischofs, hat der Pfarrer jedoch erst einmal von seinem Amt Besitz ergriffen (Investitur), so kann ihn der Bischof ohne seine Einwilligung, besondere Gründe oder seinen Amtsverzicht nicht versetzen. Bis auf die Vollmacht, die hl. Weihen (Ordination) zu erteilen und die Firmung zu spenden, steht ihm die volle Jurisdiktion in seiner Pfarrei zu. Es ist dem Bischof jedoch möglich, den Pfarrer des Amtes zu entheben. Die Amtsgeschäfte in seinen Pfarreien führt der Pfarrer bis zu einem gewissen Grade frei, ist aber an die Vorgaben der Diözese gebunden und auf die Zusammenarbeit mit pastoralen Mitarbeitern und Ehrenamtlichen in Räten (z. B. Kirchengemeinderat, Kirchenvorstand, Pfarrverwaltungsrat, Pfarrgemeinderat und Ausschüssen) angewiesen.

Eine besondere Stellung in der römisch-katholischen Kirche nimmt der Moderator ein. Er ist zwar geweihter Priester und kann mit der Leitung einer Pfarrei betraut werden; er kann aber aus bestimmten Gründen den Titel "Pfarrer" nicht tragen, etwa weil er als Ordenspriester nicht direkt der Diözese, sondern der Leitung seines Ordensinstitutes untersteht oder weil er nicht die Staatsangehörigkeit des Landes besitzt, in welchem er tätig ist. Der Moderator kann auch jederzeit ohne seine Einwilligung und ohne Amtsverzicht versetzt werden. Ansonsten leitet er seine Pfarrei mit den gleichen Rechten und Pflichten wie ein Pfarrer.

In den lutherischen Landeskirchen bestimmt die Kirchengemeindeordnung, dass die Kirchengemeinden vom Kirchenvorstand (bzw. Kirchengemeinderat) und Pfarrer gemeinsam geleitet werden. Die Mitglieder des Kirchenvorstandes sind hiernach verpflichtet, nach dem Maß ihrer Gaben und Kräfte zusammenzuarbeiten. Der Kirchenvorstand besteht aus gewählten Mitgliedern und Mitgliedern kraft Amtes (d. h. dem Pfarrer bzw. den Pfarrern). Vorsitz und stellvertretender Vorsitz werden von einem gewählten Mitglied und einem Mitglied kraft Amtes wahrgenommen. In der Württembergischen Evangelischen Landeskirche wird der Pfarrer in Verwaltungsangelegenheiten durch den Kirchenpfleger entlastet, der kraft Amtes auch Mitglied des Kirchengemeinderats ist und als Mitarbeiter der Kirchengemeinde vielfältige Verwaltungsaufgaben zu bearbeiten hat (Abwicklung der Einnahmen und Ausgaben, Buchungen, Haushaltsplan, Bauaufgaben, Personalangelegenheiten). Pädagogische Aufgaben oder solche der Sozialarbeit können in den evangelischen Kirchen einem Diakon übertragen sein. Auf diese Weise wird dem Pfarrer mehr Freiraum für seine Kernaufgaben in der geistlichen Gemeindearbeit verschafft.

Pfarrergesetze regeln kirchenintern die rechtlichen Rahmenbedingungen und die Beziehung zwischen einzelnen Pfarrern und der Kirchenleitung.





</doc>
<doc id="3849" url="https://de.wikipedia.org/wiki?curid=3849" title="Psychologe">
Psychologe

Psychologe ist die Berufsbezeichnung von Personen, die das Studium der Psychologie an einer Hochschule (Universität oder Fachhochschule) erfolgreich absolviert und als Diplom-Psychologe (Dipl.-Psych., BRD) bzw. (M.Sc. Psychologie), Master of Arts (M.A. Psychologie) oder als diplomierter Psychologe (Dipl.-Psych. FH, Schweiz) abgeschlossen haben. Damit verbunden ist die Fachkunde zur Beschreibung, Erklärung, Modifikation und Vorhersage menschlichen Erlebens und Verhaltens. Der Beruf des Psychologen ist dem Gesetz nach ein Freier Beruf. Für die Tätigkeit als Psychologischer Psychotherapeut ist nach dem Abschluss des universitären Diplom-/Masterstudiums eine zusätzliche mehrjährige Weiterbildung notwendig.

Die Erlaubnis der Verwendung als Berufsbezeichnung ist in verschiedenen Ländern teilweise unterschiedlich gesetzlich geregelt, setzt jedoch ein Hochschulstudium voraus. Psychologen sind in sehr vielen verschiedenen Anwendungsfeldern (Gesundheitswesen, Bildungswesen, Wirtschaft, Forschung und Entwicklung, Rechtswesen, Verkehrswesen, Verwaltung etc.) tätig.

Die Ausbildung in Psychologie erfolgt orientiert am heutigen Psychologieverständnis als empirische Wissenschaft, die sich mit dem Erleben und Verhalten des Menschen beschäftigt und enthält einen großen Anteil an Wissenschaftsmethodik und Statistik.

Psychologische Arbeitsfelder sind breit gefächert: im Zentrum steht die Entwicklung, Durchführung und Evaluation von Diagnostik- und Interventionsverfahren, v. a. psychologischer Beratung und Trainings, sowohl in klinischen als auch in anderen Bereichen der angewandten Psychologie, sowie der wissenschaftlichen Grundlagenforschung.

Grundlage der psychologischen Tätigkeiten sind wissenschaftlich begründete Erkenntnisse und eine ethisch einwandfreie, vertrauenswürdige Arbeitsgestaltung und Behandlung der Klienten. Alle Arbeitsbereiche werden sowohl von Selbständigen als auch von Angestellten angeboten.

Das Berufsfeld umfasst zahlreiche Spezialisierungen, die ggf. auch spezielle Aus- und Weiterbildungen voraussetzen, um dort tätig sein zu dürfen (z. B. im Bereich der Psychotherapie oder Verkehrspsychologie).

Typische Arbeitsbereiche sind:

Zur Grundlagen- und Anwendungsforschung (Forschung und Entwicklung) gehören etwa:

Typische gutachterliche Tätigkeiten sind etwa:
Die beiden erstgenannten Tätigkeiten erfordern nicht notwendiger- aber sinnvollerweise eine Zusatzausbildung zum Rechtspsychologen.

Kanning (2014) berichtet, dass über das Berufsbild des Psychologen in der Öffentlichkeit ein verzerrtes Bild bestehe. Psychologen werden regelhaft unzulässig mit Psychotherapeuten bzw. generell mit Beratern und Helfern im Gesundheitswesen und der Erziehung gleichgesetzt. Sie werden also in Bezug auf Qualifikationen und Tätigkeiten mit anderen Berufsgruppen verwechselt (z. B. Ärzten, Psychotherapeuten). 

Im Gegensatz zum häufig verzerrten Bild in der Öffentlichkeit beschreibt der Begriff des Psychologen zwar eine Person mit einem abgeschlossenen Hochschulstudium aber keinen Heilberuf. Aus diesem Grund darf ein ausgebildeter Psychologe auch nicht selbstständig heilkundlich (z. B. psychotherapeutisch) tätig werden (siehe auch Unterschiede Psychotherapeut – Psychiater – Psychologe). Die Art der tatsächlichen Qualifikation des Psychologen und der Tatsache, dass die Diagnostik und Therapie von Erkrankungen unter einem Approbationsvorbehalt stehen und somit approbierten Berufsgruppen wie z. B. Ärzten vorbehalten ist, ist weitgehend unbekannt. Psychologen üben heute vorwiegend Tätigkeiten in Forschung, Entwicklung, Evaluation, Assessment, Ausbildung und wirtschaftsnahen wissenschaftlichen Dienstleistungen aus.

An den verschiedenen Universitäten ist das "Psychologiestudium" fakultär den Geisteswissenschaften oder den Naturwissenschaften zugeordnet und zum Teil recht pragmatisch mit anderen Fächern institutionell verbunden.

Für einige berufliche Tätigkeiten ist neben dem Universitätsstudium eine spezialisierende "Aus- oder Weiterbildung" notwendig (z. B. Psychologischer Psychotherapeut oder Verkehrspsychologe). Solche meist postgradualen Weiterbildungen können zum Führen von Fachtiteln berechtigen, welche die besondere Qualifikation ausdrücken. Regelmäßige berufsbegleitende "Fortbildung" kann als Forderung von entsprechenden Berufsverbänden als Voraussetzung für das Weiterführen von Fachtiteln bestehen und von diesen kontrolliert werden.

Klärungsbedarf besteht, welche beruflichen Tätigkeiten von Personen ausgeübt werden können, die einen BA-Abschluss erworben haben, aber keinen Masterabschluss. Einerseits wird daran festgehalten, dass ein vollwertiger Abschluss in Psychologie nur mit einem abgeschlossenen Masterstudium erreicht werden kann und dieser Voraussetzung für den Zugang zu Spezialisierungen und Weiterbildungen bleibt. Andererseits muss die Psychologie sich der Forderung stellen, dass der BA auch ein berufsqualifizierender Abschluss sein soll.

Auf europäischer Ebene wird im Rahmen des Bolognaprozess und der Niederlassungsfreiheit eine Harmonisierung des Berufsprofils vorbereitet ("EuroPsy"). Vorgesehen ist eine Mindestqualifikation in Form eines abgeschlossenen wissenschaftlichen Universitätsstudiums von mindestens fünf Jahren Dauer, in dem ein naturwissenschaftlich orientiertes Mindestcurriculum absolviert wurde. Da der Beruf hier zu den naturwissenschaftlichen Berufen gehört, wird ein "Bachelor of Science" (B.Sc.) bzw. "Master of Science" (M.Sc.) für den Beruf des Psychologen vorausgesetzt.

Für über die Bologna-Deklaration hinausgehende Spezialisierungen muss nach Abschluss des M.Sc. zusätzlich ein in Vollzeit absolviertes, einjähriges, von einem Psychologen supervidiertes und positiv evaluiertes Praxisjahr (Internship) abgeleistet werden.

Psychologen üben ihre berufliche Tätigkeit als abhängig beschäftigte Arbeitnehmer oder als Selbständige aus. Die Tätigkeit von selbständigen Psychologen ist in Deutschland seit 1995, unabhängig vom konkreten Tätigkeitsbereich, als freier Beruf (Katalogberuf gem. EStG bzw. PartGG) anerkannt.

Berufspsychologen mit staatlich anerkannter wissenschaftlicher Abschlussprüfung sind nach StGB zur Verschwiegenheit verpflichtet, welche sich nicht nur auf den klinischen Bereich bezieht.

Aus wettbewerbsrechtlichen Gründen ( UWG) ist es unzulässig, die Bezeichnung "Psychologe" im Zusammenhang mit einer geschäftlichen Handlung zu führen, ohne über die entsprechende akademische Qualifikation zu verfügen, denn dies wird als Irreführung des Verbrauchers angesehen.

Ein Bachelorabschluss soll nach Auffassung des Berufsverbands Deutscher Psychologinnen und Psychologen (BDP) nicht zur Führung der Berufsbezeichnung Psychologe qualifizieren, da die in den Rechtskommentaren geforderte Mindestqualifikation nicht erreicht werde. Außerdem vertritt der BDP die Ansicht, die Führung der Berufsbezeichnung "Psychologe" ohne über eine entsprechende akademische Ausbildung zu verfügen, sei nach Abs. 2 StGB strafbar.

Gesetzliche Regelung der universitären Ausbildung in Deutschland

Das Studium der Psychologie mit dem Abschluss Diplom ist in Deutschland seit 1941 gesetzlich geregelt und wurde seit Neugründung der universitären Lehre nach dem Ende des Nationalsozialismus regelmäßig durch die Deutsche Gesellschaft für Psychologie (DGPs) und die Kultusministerkonferenz (KMK) überarbeitet. Ziel der Ausbildungsorganisation war die Professionalisierung der Absolventen, also die Standardisierung und qualitative Sicherung der berufsmäßigen Ausübung der Psychologie. Für die neuen Studiengänge mit Abschlüssen als Bachelor und Master gibt es nur noch Empfehlungen der DGPs, welche sich am bisherigen Diplom-Studiengang orientieren. Eine gesetzliche Regelung und damit Bindung von Hochschulen an bestimmte wissenschaftliche Standards und (bestimmte) Inhalte eines Studiums mit der Bezeichnung Psychologie existieren für die neuen Studiengänge nicht mehr.

Durch eine mindestens dreijährige, selbst zu finanzierende Vollzeit-Zusatzausbildung/Aufbaustudium kann ein akademisch ausgebildeter Diplom-Psychologe den gesetzlich geschützten Titel des "Psychotherapeuten" erwerben, indem er nach bestandener Staatsprüfung die Approbation als "Psychologischer Psychotherapeut" erhält, mit der er dann im Rahmen der bedarfsabhängigen gesetzlichen Bestimmungen eine Kassenzulassung beantragen kann.

Änderung der Ausbildung: Umstellung auf Bachelor- und Masterstudiengänge

Ursprünglich stellte die DGPs die Forderung, dass eigentlich nicht der Bachelor, sondern der Master der Regelabschluss sein müsste. „Psychologie ist ein komplexes Studium, so komplex wie sein Gegenstand, das menschliche Verhalten und Erleben. Die Wissens- und Kompetenzvermittlung nimmt mehr als drei Jahre in Anspruch“, sagt Hannelore Weber, seit 1994 Professorin an der Uni Greifswald und von 2004 bis 2006 Präsidentin der Deutschen Gesellschaft für Psychologie. Diese Forderung ließ sich aber gegen die bildungspolitischen Forderungen der Politik nicht umsetzen.

Auch bei der Umstellung des Studiums vom Diplom auf Bachelor und Master stoßen die Fachbereiche auf bildungs- und hochschulpolitische Hürden bei der Umsetzung der DGPs-Empfehlungen. Zum einen wird zu wenig Zeit für das Studium eingeräumt, was in ersten Erfahrungen nur durch Reduktion der Anforderungen (Niveau) und der zu erbringenden Prüfungsleistungen kompensiert werden konnte (mitgeteilt auf dem Symposium "Neue Studiengänge" auf dem DGPs-Kongress 2006 in Nürnberg). Zum anderen wurde der Curricularnormwert von 4,0 (Diplom-Psychologie) auf Werte zwischen 2,2 und 3,4 (Bachelor of Science) bzw. zwischen 1,1 und 1,7 (Master of Science) deutlich reduziert (ebd.). Verluste sollen durch Einrichtung von postgradualen Studiengängen (Graduiertenkollegs) kompensiert werden. Damit scheint der Weg, der auch bei vergleichbaren Studiensystemen im Ausland beschritten wird, auch in Deutschland zur Notwendigkeit zu werden.

Ziele der Ausbildung zum Diplom-Psychologen

Die richtlinienorientierte universitäre Diplom-Ausbildung des Psychologen in Deutschland benötigt real, unabhängig von der Regelstudienzeit, sechs bis sieben Jahre. Ziel ist, dass ein berufspraktisch arbeitender Psychologe bei praktischen Entscheidungen auf wissenschaftliche Methodologie und empirische Befunde zurückgreift, im Rahmen seiner Tätigkeit nur wissenschaftlich valide Methoden, Instrumente und Techniken einsetzt, dass er, soweit möglich, seine Kunden, Klienten, Patienten, sowie Mitglieder anderer Berufsgruppen über empirische Befunde und wissenschaftlich begründete Analyse-, Klärungs- und Lösungsmöglichkeiten ihrer Probleme und Fragestellungen informiert und Fragen selbst mittels angewandter Forschung und Entwicklung bearbeitet und die Befunde für sich und andere anwendungsorientiert umsetzt. Weitere Ziele sind der Gebrauch des Fachwissens zu Aufbau und Aufrechterhaltung von effektiver Zusammenarbeit und Teamarbeit mit Angehörigen anderer Berufsgruppen und zur Verfügung stellen des durch das Training in wissenschaftlichen Methoden und in der Durchführung von empirischer Forschung erworbenen Know-Hows für andere Berufsgruppen, die nicht über eine solche Ausbildung verfügen, um z. B. Team- und andere Entscheidungen wissenschaftlich abzusichern, die Qualität der Arbeit zu verbessern usw. Weiterhin soll ein Psychologe eigenes Handeln für andere "transparent" gestalten, es selbst kritisch reflektieren und vor allem wissenschaftlich evaluieren, sowie sich kontinuierlich fortbilden.

Berufschancen in Deutschland

Der Berufs-Chancen-Check gibt 206 Berufe an, die wissenschaftlich ausgebildete Psychologen ausüben können. Es zeigt sich eine Fortsetzung des Trends, dass sowohl in der Ausbildung im Bachelor-Master-System wie auch in der Berufspraxis psychosoziale und klinische Bereiche und Tätigkeiten stark rückläufig sind, zugunsten wirtschaftsnaher und auch neuer Arbeitsfelder, in denen allerdings stärker die wissenschaftlich-methodischen Kompetenzen von Psychologen nachgefragt werden. Dabei gibt es weiterhin einen starken Trend weg vom klassischen Angestelltenverhältnis hin zur Selbständigkeit.

Rechtsgrundlagen: Das Psychologengesetz
In Österreich sind Ausbildung, Zugang, Berufsbezeichnung und -ausübung durch das "Bundesgesetz über die Führung der Berufsbezeichnung „Psychologe“ oder „Psychologin“ und über die Ausübung des psychologischen Berufes im Bereich des Gesundheitswesens (Psychologengesetz)" geregelt.

Dabei ist ausdrücklich festgelegt:

"Diplompsychologe" ist die alte Bezeichnung, anstelle des Magister treten "Bachelor/Master of Science".

Berufsbereich Psychologie: Berufe und Ausbildungen

Der Beruf umfasst folgende Felder:

Für beide Berufe ist umfangreiche Weiterbildung notwendig, sie umfasst den Erwerb theoretischer und praktischer fachlicher Kompetenz, erstere im Ausmaß einer Gesamtdauer von zumindest 160 Stunden (§ 5 Abs. 1) in Lehrveranstaltungen anerkannter privat- oder öffentlich-rechtlicher Einrichtungen einschließlich der Universitätsinstitute und Universitätskliniken (§ 5 Abs. 1), zweitere von 1480 Stunden, davon zumindest 150 Stunden innerhalb eines Jahres in einer facheinschlägigen Einrichtung des Gesundheitswesens (§ 5 Abs. 2), sowie eine begleitende Supervision in der Gesamtdauer von zumindest 120 Stunden (§ 5 Abs. 2 Z. 2), mit Bestätigung (§ 9).
Nur dann ist der Psychologe berechtigt, im Gesundheitswesen tätig zu sein, dann aber auch als vollwertiger Gesundheitsberuf. Die Ausübung des Berufes umfasst dann insbesondere (§ 3 Abs. 2):
Nach Gesetz unterliegen Klinische und Gesundheitspsychologen dann auch medizinischen Grundverpflichtungen wie Ausüben des Berufs "nach bestem Wissen und Gewissen", Weiterbildung unter Beachtung der Entwicklung der Erkenntnisse der Wissenschaft, Verschwiegenheit, Auskunft über die Behandlung, und Ähnlichem ("Berufspflichten" §§ 13,14). Sie werden auch in eine am Bundeskanzleramt geführte Berufsliste eingetragen ("Liste der klinischen Psychologen und Gesundheitspsychologen" §§ 16,17)

Weitere Spezialausbildungen sind etwa "Psychosozialer Gesundheitstrainer", "Mehrfachtherapie-Konduktor für Cerebralparetiker und Mehrfachbehinderte"

Verwandte Berufe – das heißt, dass die Ausbildung teilweise angerechnet wird – sind
"Arzt, Bildungs- und Berufsberater, Ehe- und Familienberater, Kognitionswissenschafter, Lebens- und Sozialberater, Pädagoge, psychiatrische Gesundheits- und Krankenschwester/Pfleger, Psychotherapeut, Sozialarbeiter, Soziologe".



Zugeordnet wird der Beruf Psychologe dem Berufsbereich "Gesundheit und Medizin" nach AMS für die gesetzlich besonders geregelten Formen, sonst den Bereichen "Soziales, Erziehung und Bildung" und "Wissenschaft, Forschung und Entwicklung", oder der Berufsgruppe "Gesundheit/Medizin/Pflege" (Arbeitsfeld "Arbeitsplatz Krankenhaus") nach BIC

Ausbildung in Österreich
Das Universitätsstudium Psychologie (Bachelor- und Diplomstudium, 6 bzw. 10 Semester) wird in Österreich derzeit (Stand 2011/12) angeboten:

Das Studium hat großen Andrang, an den meisten österreichischen Universitäten wurden Zugangsbeschränkungen eingerichtet.

Für die berufliche Weiterbildung gibt es, neben Krankenhäusern und Universitäten auch:

Die Niederlassungsfreiheit ist über das "Bundesgesetz über die Niederlassung und die Ausübung des freien Dienstleistungsverkehrs von klinischen Psychologen und Gesundheitspsychologen aus dem Europäischen Wirtschaftsraum (EWR-Psychologengesetz)" geregelt, es gilt für die EWR-Vertragsstaaten und die Schweizerische Eidgenossenschaft, sowie dann für Drittländer, wenn deren Ausbildung in ersteren anerkannt ist. Im Ausland ausgebildete Nicht-Österreicher werden damit – eine vorausgehende Prüfung der Ausbildung durch den Gesundheitsminister vorausgesetzt – Österreichern (mit in- und ausländischer Qualifikation) gleichgestellt und allenfalls ebenfalls in der "Liste der klinischen Psychologen und Gesundheitspsychologen" zertifiziert.

Organisationen und Institutionen

Dem "Psychologenbeirat", dem Beratungsorgan am Bundeskanzleramt (laut § 19 "Psychologengesetz") gehören neben Vertretern von BÖP, ÖGP und GkPP und "Psychotherapiebeirat" beim Bundeskanzleramt auch Vertreter folgender Organisationen der Sozialpartnerschaft an: Österreichische Ärztekammer, Bundeskammer der Gewerblichen Wirtschaft, Hauptverband der Österreichischen Sozialversicherungsträger, Österreichischer Arbeiterkammertag, Österreichischer Gewerkschaftsbund, Präsidentenkonferenz der Landwirtschaftskammern Österreichs

Weitere spezialisierte anerkannte Organisationen und Fachgesellschaften sind:

2013 ist das Psychologieberufegesetz (PsyG) in Kraft getreten. Es regelt den Titelschutz und die Berufspflichten der Psychologen. Zwecke sind der Gesundheitsschutz und der Schutz vor Täuschung und Irreführung von Personen, die Leistungen auf dem Gebiet der Psychologie in Anspruch nehmen. Zu diesem Zweck werden anerkannte inländische Hochschulabschlüsse in Psychologie, Anforderungen an die Weiterbildung, Voraussetzungen für die Erlangung eines eidgenössischen Weiterbildungstitels, periodische Akkreditierung der Weiterbildungsgänge, Anerkennung ausländischer Ausbildungsabschlüsse und Weiterbildungstitel, Anforderungen an die privatwirtschaftliche Berufsausübung der Psychotherapie in eigener fachlicher Verantwortung sowie die Voraussetzungen für die Verwendung geschützter Berufsbezeichnungen und eidgenössischer Weiterbildungstitel festgelegt. Die zugehörige Verordnung regelt u. a. Weiterbildung, Anerkennung ausländischer Abschlüsse, Akkreditierung von Weiterbildungsgängen, Titelverwendung in der Berufsbezeichnung Danach dürfen sich nur die Personen „Psychologe“ oder „Psychologin“ nennen, die einen nach Gesetz anerkannten schweizerischen Master-, Lizenziats- oder Diplomabschluss in Psychologie an einer schweizerischen Universität oder Fachhochschule erworben haben. Vergleichbare ausländische Abschlüsse werden dann anerkannt, wenn die für ausländische Abschlüsse zuständige Psychologieberufekommission diese nach Abwägung anerkennt oder ein Staatsvertrag zur gegenseitigen Anerkennung entsprechender Diplome vorliegt. Bachelors dürfen sich gewerblich nicht schriftlich als Psychologe/Psychologin bezeichnen. Der Bachelortitel (Bachelor of Science in Psychologie) ist aber ebenfalls geschützt.

In der Schweiz wird ein Psychologiestudium – früher mit dem Abschluss eines Lizenziats (lic.phil), heute EU-kompatibel als BA/BSc- oder MA/MSc-Studium – an folgenden Universitäten angeboten: Basel, Bern, Freiburg, Genf, Lausanne, Neuenburg und Zürich.
Die Studienpläne entsprechen dabei weitgehend etwa denen in Deutschland, Schweizer Wissenschaftler haben an der Erarbeitung der Empfehlungen der Deutschen Gesellschaft für Psychologie mitgearbeitet.

Daneben wird ein Fachhochschulstudium an der Zürcher Hochschule für Angewandte Wissenschaften – Departement Psychologie (ZHAW-P) und der Fachhochschule Nordwestschweiz (FHNW) – Hochschule für Angewandte Psychologie, mit einer Ausbildung zum Psychologen angeboten, welches gesetzlich anerkannt ist (BSc. MSc.). Der Ausbildungsschwerpunkt liegt dabei vor allem auf dem Aspekt der angewandten Psychologie. Die Vorgängereinrichtung bot bis 2003 Ausbildungsabschlüsse mit dem Titel "Psych. IAP" (Institut für angewandte Psychologie) danach wurde das Institut in eine Hochschule für angewandte Psychologie HAP umgewandelt und es wurde bis 2009 mit dem Title "dipl. Psych. FH" abgeschlossen. Der dipl. Psychologe FH ist ebenfalls gesetzlich geschützt.

Die Föderation der Schweizer Psychologinnen und Psychologen (FSP) als Dachverband mit ihren kantonalen und thematischen Gliedverbänden ist der größte Berufsverband (ca. 6500 Mitglieder) der Psychologen in der Schweiz (Gründung 1987). Daneben besteht noch der ältere Schweizerische Berufsverband für Angewandte Psychologie (Gründung 1952), in welchen sich mehrheitlich die FH-Psychologen organisiert haben (ca. 900). Die Schweizerische Gesellschaft für Psychologie (SGP, Gründung 1943) als älteste Psychologenvereinigung der Schweiz ist die Wissenschaftliche Gesellschaft der Psychologie in der Schweiz.

In einigen Staaten, wie z. B. in den USA oder in Australien, ist Psychologe ebenfalls eine geschützte Berufsbezeichnung, die aber darüber hinaus einer besonderen staatlichen Zulassung zusätzlich zu einem absolvierten Studium bedarf. Da staatliche Regelungen für die Ausbildung von Psychologen oft fehlen, wird jeder Antragsteller in Bezug auf seine erworbenen Kompetenzen überprüft, ob und inwieweit er über eine ausreichende Qualifikation verfügt, als Psychologe (unabhängig vom Berufsfeld) verantwortungsvoll tätig sein zu können; dieses Vorgehen dient als Vorbild für die europäische Regelung. Darauf aufbauend gibt es auch weitere staatliche Zulassungen, wie z. B. für den Bereich der Psychotherapie.




</doc>

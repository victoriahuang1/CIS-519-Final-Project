<doc id="1278" url="https://de.wikipedia.org/wiki?curid=1278" title="Ernst Thälmann">
Ernst Thälmann

Ernst Johannes Fritz Thälmann (* 16. April 1886 in Hamburg; † 18. August 1944 im KZ Buchenwald) war ein deutscher Politiker in der Weimarer Republik. Er war von 1925 bis zu seiner Verhaftung im Jahr 1933 Vorsitzender der Kommunistischen Partei Deutschlands (KPD), die er von 1924 bis 1933 im Reichstag vertrat und für die er in den Reichspräsidentenwahlen von 1925 und 1932 kandidierte. Thälmann führte von 1925 bis zum Verbot 1929 den Roten Frontkämpferbund (RFB) an, der als paramilitärische "Schutz- und Wehrorganisation" der KPD vor allem in Straßenkämpfen mit politischen Gegnern und der Polizei in Erscheinung trat.

Er schloss die in den Statuten der Kommunistischen Internationale vorgesehene Umstrukturierung der KPD als Partei neuen Typus ab. Aufbauend auf die sowjetische Sozialfaschismusthese bekämpfte die KPD, die sich unter seiner Führung zunehmend stalinisierte, die SPD als politischen Hauptfeind innerhalb der Weimarer Republik.

Seine Verhaftung erfolgte am 3. März 1933, zwei Tage vor der Reichstagswahl März 1933 und einige Tage nach dem Reichstagsbrand. Thälmann wurde im August 1944, nach über elf Jahren Einzelhaft, vermutlich auf direkten Befehl Adolf Hitlers, erschossen.

Von 1893 bis 1900 besuchte Thälmann die Volksschule. Rückblickend beschrieb er später Geschichte, Naturgeschichte, Volkskunde, Rechnen, Turnen und Sport als seine Lieblingsfächer. Religion hingegen mochte er nicht. 1895 eröffneten seine Eltern ein kleines Gemüse-, Steinkohlen- und Fuhrwerksgeschäft in Hamburg-Eilbek. In diesem Geschäft musste er nach der Schule aushelfen. Seine Schularbeiten erledigte er am frühen Morgen vor dem Unterrichtsbeginn. Seine Erfahrungen im elterlichen Geschäft beschrieb er später so:

Trotz dieser Belastung war Thälmann ein guter Schüler, dem das Lernen viel Freude bereitete. Sein Wunsch, Lehrer zu werden oder ein Handwerk zu erlernen, erfüllte sich nicht, da seine Eltern ihm die Finanzierung verweigerten. Er musste daher weiter im Kleinbetrieb seines Vaters arbeiten, was ihm, nach eigenen Aussagen, großen Kummer bereitete. Durch das frühzeitige „Schuften“ im elterlichen Betrieb kam es zu vielen Auseinandersetzungen mit seinen Eltern. Thälmann wollte für seine Arbeit einen richtigen Lohn und nicht nur ein Taschengeld. Darum suchte er sich eine Arbeit als „Ungelernter“ im Hafen. Hier kam Thälmann bereits als Zehnjähriger mit den Hafenarbeitern in Kontakt, als sie vom November 1896 bis Februar 1897 im Hamburger Hafenarbeiterstreik die Arbeit niederlegten. Der Arbeitskampf wurde von allen Beteiligten erbittert geführt. Er selbst schrieb 1936 aus dem Gefängnis an seine Tochter, dass „der große Hafenarbeiterstreik in Hamburg vor dem Kriege, […] der erste sozialpolitische Kampf“ gewesen sei, „der sich für immer in […] (sein) Herz“ eingeprägt habe. Der (sozial)politische Inhalt der Gespräche der Hafenarbeiter soll ihn sehr geprägt haben.

Anfang 1902 verließ er im Streit das Elternhaus und kam zunächst in einem Obdachlosenasyl unter, später in einer Kellerwohnung. Ab 1904 fuhr er als Heizer auf dem Frachter "Amerika" zur See, unter anderem in die USA. Hier war er 1910 in der Nähe von New York für kurze Zeit als Landarbeiter tätig. In den Jahren bis zum Ersten Weltkrieg betätigte sich Thälmann als konsequenter Streiter für die Interessen der Hamburger Hafenarbeiter. Von 1913 bis 1914 arbeitete er als Kutscher für eine Wäscherei.

Anfang 1915 wurde er zum Kriegsdienst bei der Artillerie eingezogen und kam an die Westfront, an der er bis zum Kriegsende als Kanonier kämpfte. Zweimal kam er nach Verwundungen in Lazarette in Köln und Bayreuth. Er selbst gab an, an folgenden Schlachten und Gefechten teilgenommen zu haben: Schlacht in der Champagne (1915–1916), Schlacht an der Somme (1916), Schlacht an der Aisne, Schlacht von Soissons, Schlacht von Cambrai (1917) und Schlacht bei Arras.

Thälmann erhielt im Krieg mehrere Auszeichnungen:


Thälmanns Vater, Johannes Thälmann ("Jan" genannt; * 11. April 1857; † 31. Oktober 1933), wurde in Weddern in Holstein geboren und arbeitete dort als Knecht. Die Mutter Thälmanns, Maria-Magdalene (geb. "Kohpeiss"; * 8. November 1857; † 9. März 1927), kam im vierländischen Kirchwerder als Tochter eines Zimmermanns zur Welt. Die Hochzeit fand 1884 in Hamburg statt. Dort verdiente sich Johannes Thälmann sein Geld zunächst als Speditionskutscher.

Die Eltern waren parteilos; im Unterschied zum Vater war die Mutter tief religiös. Nach der Geburt ihres Sohnes Ernst übernahmen die Eltern eine Kellerwirtschaft am Alten Wall 68 in der Hamburger Altstadt, zwischen Hafen und Rathaus. Am 4. April 1887 wurde Frieda, die Schwester von Ernst Thälmann geboren († 8. Juli 1967 in Hamburg). Im März 1892 wurden die Eltern Thälmanns wegen Hehlerei zu zwei Jahren Zuchthaus verurteilt, weil sie entwendete Waren gekauft oder für Schulden in Zahlung genommen hatten. Thälmann und seine jüngere Schwester Frieda wurden getrennt und in unterschiedliche Familien zur Pflege gegeben. Die Eltern wurden jedoch vorzeitig aus der Haft entlassen (die Mutter im Mai und der Vater im Oktober 1893). Die Straftat seiner Eltern wurde noch 36 Jahre später im Wahlkampf gegen Ernst Thälmann verwendet. Den politischen Gegnern kam es gelegen, dass schon der Vater ein „Zuchthäusler“ gewesen war.

Wenige Tage vor Beginn seines Kriegsdienstes heiratete er am 13. Januar 1915 Rosa Koch. Aus dieser Ehe ging die Tochter Irma Thälmann hervor.

In einem neueren Buch findet sich der Hinweis, Irma sei „nicht die einzige Nachkommin ihres Vaters“. Weitere Angaben werden dort aber nicht gemacht.

Thälmann wurde am 15. Mai 1903 Mitglied der SPD. Am 1. Februar 1904 trat er dem "Zentralverband der Handels-, Transport- und Verkehrsarbeiter Deutschlands" bei, in dem er zum Vorsitzenden der "Abteilung Fuhrleute" aufstieg. 1913 unterstützte er eine Forderung von Rosa Luxemburg nach einem Massenstreik als Aktionsmittel der SPD zur Durchsetzung politischer Forderungen.
Im Oktober 1918 desertierte Thälmann gemeinsam mit vier befreundeten Soldaten, indem er aus dem Heimaturlaub nicht mehr an die Front zurückkehrte, und trat Ende 1918 der USPD bei.

In Hamburg beteiligte er sich am Aufbau des Hamburger Arbeiter- und Soldatenrates. Ab März 1919 war er Vorsitzender der USPD in Hamburg und Mitglied der Hamburger Bürgerschaft. Gleichzeitig arbeitete er als Notstandsarbeiter im Hamburger Stadtpark, dann fand er eine gut bezahlte Stelle beim Arbeitsamt. Hier stieg er bis zum Inspektor auf. Ende November 1920 schloss sich der mitgliederstarke linke Flügel der USPD der Kommunistischen Internationale (Komintern) an und vereinigte sich damit mit deren deutscher Sektion, der KPD. Diese firmierte daraufhin für die folgenden zwei Jahre auch unter dem Alternativnamen Vereinigte Kommunistische Partei Deutschlands (VKPD). Thälmann war der wichtigste Befürworter dieser Vereinigung in Hamburg. Auf sein Betreiben hin traten 98 Prozent der Mitglieder der Hamburger USPD der KPD bei.

Im Dezember wurde er in den Zentralausschuss der KPD gewählt. Thälmann stand ab 1921 dem Linken Flügel der Partei nahe und wurde zur Führungsfigur der Hamburger Parteilinken. Am 29. März 1921 wurde er wegen seiner politischen Tätigkeit vom Dienst im Arbeitsamt fristlos entlassen, nachdem er unerlaubt seinem Arbeitsplatz ferngeblieben war. Er war einem Aufruf der KPD gefolgt, sich der März-Aktion anzuschließen. Im Sommer des Jahres 1921 fuhr Thälmann als KPD-Vertreter zum III. Kongress der Komintern nach Moskau und lernte dort Lenin kennen. Am 17. Juni 1922 wurde ein rechtsradikales Attentat auf seine Wohnung verübt, um Thälmann zu ermorden. Angehörige der nationalistischen Organisation Consul warfen eine Handgranate in seine Parterrewohnung in der Hamburger Siemssenstraße 4. Seine Frau und seine Tochter blieben unverletzt. Thälmann selbst kam erst später heim.

Thälmann war Teilnehmer und einer der Organisatoren des Hamburger Aufstandes vom 23. bis 25. Oktober 1923. Der Aufstand scheiterte, und Thälmann musste für eine Weile untertauchen. Später urteilte er in der Berliner Ausgabe des Parteiorgans "Die Rote Fahne":

Das Scheitern des Aufstandes wurde vor allem den ehemaligen KPD-Vorsitzenden und „Rechtsabweichlern“ Heinrich Brandler und August Thalheimer vorgeworfen. Die fehlende Bolschewisierung sei schuld an der Niederlage gewesen. Zu einem ähnlichen Schluss kam Georgi Dimitrow nach dem gescheiterten „Antifaschistischen Septemberaufstand“ 1923 in Bulgarien.

Ab Februar 1924 war er stellvertretender Vorsitzender und ab Mai Reichstagsabgeordneter der KPD. Unter seiner Führung lehnte die Partei die Kritik Rosa Luxemburgs am Leninismus als "Luxemburgismus" ab, was sich in der unkritischen Solidarität Stalins bemerkbar machte. Die Entwicklung der bolschewistischen Partei in der Sowjetunion, die sich mehr auf Stalin und seine gesonderte Interpretation des Kommunismus konzentrierte, machte sich auch unter ihm in der KPD bemerkbar. Den Posten im Reichstag hatte Thälmann bis zum Ende der Weimarer Republik inne. Im Sommer 1924 wurde er auf dem V. Kongress der Komintern in ihr Exekutivkomitee und kurze Zeit später ins Präsidium gewählt. Am 1. Februar 1925 wurde er Vorsitzender des Roten Frontkämpferbundes und am 1. September des Jahres Vorsitzender der KPD, als Nachfolger von Ruth Fischer, die kurze Zeit später als „ultralinke Abweichlerin“ aus der KPD ausgeschlossen wurde. Thälmann kandidierte bei der Reichspräsidentenwahl 1925 auch für das Amt des Reichspräsidenten. Obwohl er im ersten Wahlgang nur sieben Prozent der Stimmen bekommen hatte, hielt er seine Kandidatur auch für den zweiten Wahlgang aufrecht. In diesem Zusammenhang wurde Thälmann vorgeworfen, dass sein Wahlergebnis von 6,4 Prozent dem Kandidaten der bürgerlichen Partei, Wilhelm Marx (45,3 Prozent), fehlten und den Sieg des Monarchisten Paul von Hindenburg mit 48,3 Prozent ermöglichten. Im Oktober 1926 unterstützte Thälmann in Hamburg den dortigen Hafenarbeiterstreik. Er sah dies als Ausdruck der Solidarität mit einem englischen Bergarbeiterstreik, der seit dem 1. Mai anhielt und sich (positiv) auf die Konjunktur der Unternehmen im Hamburger Hafen auswirkte. Thälmanns Absicht war, dieses „Streikbrechergeschäft“ von Hamburg aus zu unterbinden. Am 22. März 1927 beteiligte sich Ernst Thälmann an einer Demonstration in Berlin, wo er durch einen streifenden Säbelhieb über dem rechten Auge verletzt wurde. 1928 fuhr Thälmann nach dem VI. Kongress der Komintern in Moskau nach Leningrad, wo er zum Ehrenmitglied der Besatzung des Kreuzers "Aurora" ernannt wurde.

Bei der Rückkehr vom VI. Weltkongress berichtete Thälmann Wilhelm Florin über die Veruntreuung von Parteigeldern in Höhe von mindestens 1.500 Mark seitens des Politischen Sekretärs des KPD-Bezirks Wasserkante, John Wittorf. Bei dieser Gelegenheit gab er zu, bereits seit Mai von der Unterschlagung gewusst zu haben, sie jedoch verschwiegen zu haben, um Schaden von der Partei im Rahmen der Reichstagswahl 1928 abzuwenden. Wittorf hatte seit 1927 seine Funktion im Bezirk Wasserkante inne, war seit dem 11. Parteitag Mitglied im ZK und seit Mai 1928 neu gewählter Reichstagsabgeordneter. Willy Presche, Ludwig Ries und John Schehr waren auf die Unterschlagung aufmerksam geworden und baten um ein Gespräch Thälmanns mit Wittorf. Er konnte Wittorf davon überzeugen, Schuldscheine auszustellen und über diese das Geld der Partei zurückzuzahlen.

Am Abend des 26. September 1928 schloss das Zentralkomitee der Partei den Hamburger Wittorf aus der Partei aus und entfernte ihn von allen politischen Ämtern. Die Parteirechte – die "Versöhnler" – forderte sogar den Ausschluss Thälmanns aus der Partei. Dieser beantragte – den Statuten entsprechend – eine Diskussion im Exekutivkomitee der Kommunistischen Internationale (EKKI) und bekannte sich zu seinen Fehlern in der Wittorf-Affäre. Am Ende der ZK-Sitzung fokussierte man eine öffentliche Parteidiskussion, indem man einen Text über Thälmanns Verfehlungen im Partei-Organ "Die Rote Fahne" platzierte. In der Druckerei zog man in Erwägung, den Text nicht abzudrucken. Thälmann wurde all seiner Ämter enthoben. Weitergehende Anträge waren der Ausschluss von Presche, Ries und Schehr, die Thälmann auf die Unterschlagung Wittorfs hinwiesen, die Einberufung eines Sonderparteitags, die Umformung des Zentralkomitees mit der Einbeziehung von Heinrich Brandler und August Thalheimer in die Parteiarbeit und die Einstellung der "Hetze" gegen die "Rechten" in der Partei. Es wurde versucht, die Affäre auszunutzen, um einen Putsch innerhalb der Partei durchzuführen und die Beschlüsse des 11. Parteitags in Essen zu revidieren. Es kam zu Protesten innerhalb der Partei und der "Roten Fahne".

Das EKKI setzte Thälmann am 6. Oktober nach einer Intervention Stalins wieder in seine Parteifunktionen ein. Stalin verurteilte die Fraktionsbildung innerhalb der KPD, die Lenin schon in seinem Werk "Was tun?" kritisiert hatte und die bei den Mitgliedsparteien der KI verboten war, obgleich die Broschüre sich auf die besondere Rolle der Parteien im damaligen zaristischen System konzentrierte, da eine legale Parteiarbeit unmöglich erschien. Des Weiteren wurden die Beschlüsse des Essener Parteitags durch das EKKI bestätigt, der Ausschluss Wittorfs und die fehlerhafte Haltung Thälmanns bestätigt. Dem EKKI-Präsidium lag ein Telegramm vor, das am 5. Oktober Otto Kuusinen zugegangen war. Darin distanzierten sich 25 Mitglieder des deutschen ZK von dessen am 26. September gefassten Beschluss gegen Thälmann. Philipp Dengel, der als Sekretär des ZK den Vorsitz mit Thälmann innehatte und ebenfalls für dessen Absetzung stimmte, wurde auf dem nachfolgenden Parteitag der KPD nicht wieder bestätigt und war nur einfaches Mitglied im ZK. Thälmann besaß nun den alleinigen Vorsitz der Partei. Daraufhin kehrte Heinrich Brandler, der bis dahin im "Moskauer Ehrenexil" weilte, zusammen mit Thalheimer zurück und gründete als kommunistische Gegenspielerin die KPD-O, woraufhin alle Beteiligten aus der KPD ausgeschlossen wurden.

In den nachfolgenden Wochen wurde in den KPD-Bezirken in Sitzungen der Bezirksleitungen und Parteiarbeiterkonferenzen die Resolution der EKKI diskutiert und zur Abstimmung gestellt. Die parteiinterne Abstimmung ergab eine "dominierende Majorität in der Partei". Die Affäre samt ihrem Widerhall in der Öffentlichkeit schadete der KPD in ihrer Kampagne für einen Volksentscheid gegen den angestrebten Panzerschiffsbau der SPD-Regierung, den sie in der Opposition zuvor bekämpft hatte.

Auf dem 12. Parteitag der KPD vom 9. bis 15. Juni 1929 in Berlin-Wedding ging Thälmann angesichts der Ereignisse des Blutmai, der sich dort zuvor zugetragen hatte, auf deutlichen Konfrontationskurs zur SPD. Neben innenpolitischem Engagement setzte er sich auch für außenpolitische und nationale Belange ein, insbesondere kritisierte er die Nationalsozialisten, die nicht für die Anträge der KPD stimmten, die einen Austritt aus dem Völkerbund und eine Beseitigung der Reparationslasten forderten. So schrieb er in einem Brief in der "Neuen Deutschen Bauernzeitung Nr. 4 von 1931": „Die nationalsozialistischen und deutschnationalen Betrüger versprachen euch Kampf zur Zerreißung des Youngplanes, Beseitigung der Reparationslasten, Austritt aus dem Völkerbund, aber sie wagten nicht einmal, im Reichstag für den kommunistischen Antrag auf Einstellung der Reparationszahlungen, Austritt aus dem Völkerbund zu stimmen.“ In dem Brief betont er auch seine nationalen Absichten mit „Vorwärts zur nationalen und sozialen Befreiung!“ Am 13. März 1932 kandidierte er neben Adolf Hitler und Theodor Duesterberg (welcher aber nach dem ersten Wahldurchgang seine Kandidatur zurückzog) für das Amt des Reichspräsidenten gegen Hindenburg. Wahlspruch der KPD war: „Wer Hindenburg wählt, wählt Hitler, wer Hitler wählt, wählt den Krieg.“ Gegen den stärker werdenden Nationalsozialismus propagierte er kurze Zeit später eine „Antifaschistische Aktion“ als „Einheitsfront von unten“, also unter Ausschluss der SPD-Führung. Dieses Vorgehen entsprach der Sozialfaschismusthese der Komintern. Die Zerschlagung der SPD blieb ein zentrales Ziel der KPD. Die Antifaschistische Aktion diente auch dazu, deren Führer als Verräter der Arbeiterklasse zu „entlarven“. Nach der Reichstagswahl im November 1932, bei der die NSDAP eine empfindliche Stimmeneinbuße verzeichnete, schienen die Nationalsozialisten auf einem absteigenden Ast. Thälmann verschärfte den Kampf der KPD gegen die Sozialdemokratie im Gegenzug abermals.

Als der NSDAP am 30. Januar 1933 die Macht übertragen wurde, schlug Thälmann der SPD einen Generalstreik vor, um Hitler zu stürzen, doch dazu kam es nicht mehr. Am 7. Februar des Jahres fand im Sporthaus Ziegenhals bei Königs Wusterhausen eine vom ZK einberufene Tagung der politischen Sekretäre, ZK-Instrukteure und Abteilungsleiter der KPD statt. Auf dem von Herbert Wehner vorbereiteten Treffen sprach Thälmann zum letzten Mal vor leitenden KPD-Funktionären zu der am 5. März 1933 bevorstehenden Reichstagswahl und bekräftigte die Notwendigkeit eines gewaltsamen Sturzes Hitlers durch das Zusammengehen aller linken und liberalen Parteien zu einer Volksfront.

Am Nachmittag des 3. März 1933 wurde Thälmann zusammen mit seinem persönlichen Sekretär Werner Hirsch in der Wohnung der Eheleute Hans und Martha Kluczynski in Berlin-Charlottenburg (Lützower Straße 9, heute Alt-Lietzow 11) durch acht Beamte des Polizeireviers 121 festgenommen. Dem war eine gezielte Denunziation durch Hermann Hilliges, Gartennachbar der Kluczynskis in Gatow, vorausgegangen. In den Tagen zuvor hatten allerdings noch mindestens vier weitere Personen ihr Wissen über die Verbindung Kluczynski-Thälmann an die Polizei weitergegeben. Die Unterkunftsmöglichkeit in der Lützower Straße hatte Thälmann schon seit einigen Jahren gelegentlich und nun wieder seit Januar 1933 genutzt; sie zählte zwar nicht zu den sechs illegalen Quartieren, die der M-Apparat für Thälmann vorbereitet hatte, galt aber nicht als polizeibekannt. Thälmann hatte am 27. Februar eine Sitzung des Politbüros in einem Lokal in der Lichtenberger Gudrunstraße geleitet und war bei seiner Rückkehr über den Brand des Reichstages und die schlagartig einsetzenden Massenverhaftungen kommunistischer Funktionäre informiert worden. In den nächsten Tagen verließ er die Wohnung nicht mehr und stand nur noch über Mittelsmänner mit der restlichen Parteiführung in Verbindung. Die in der älteren Literatur gelegentlich anzutreffende Angabe, Thälmann habe auf Drängen führender Genossen eingewilligt, am 5. März ins Exil zu gehen, wird von der neueren Forschung angezweifelt. Für den 3. März plante Thälmann den Wechsel in eines der vorbereiteten illegalen Quartiere, ein Forsthaus bei Wendisch Buchholz. Beim Packen der Koffer wurde er von der Polizei überrascht. Thälmanns Festnahme war rechtswidrig, da seine nach Artikel 40a der Reichsverfassung als Mitglied des Ausschusses zur Wahrung der Rechte der Volksvertretung gewährleistete Immunität auch durch die Reichstagsbrandverordnung nicht aufgehoben worden war. Erst am 6. März stellte ein Berliner Staatsanwalt „im Interesse der öffentlichen Sicherheit“ einen – formell ebenfalls rechtswidrigen – Haftbefehl aus, der dann einfach rückdatiert wurde.

Einige Ungereimtheiten im Zusammenhang mit der die KPD stark verunsichernden Festnahme Thälmanns waren nach 1933 bereits Gegenstand von parteiinternen Untersuchungen. Zu diesen Auffälligkeiten zählte etwa, dass Thälmann trotz der offenen Verfolgung der Partei wochenlang ein- und dieselbe, für eine derartige Situation nicht vorgesehene Wohnung genutzt hatte, vor allem aber der erstaunliche Umstand, dass weder das Gebäude noch die Wohnung selbst von Angehörigen des Parteiselbstschutzes gesichert worden war. Dadurch liefen nach einigen Stunden auch noch Erich Birkenhauer, Thälmanns politischer Sekretär, und Alfred Kattner, der persönliche Kurier des Parteichefs, in die Arme der Polizei. Bei den KPD-Ermittlungen geriet insbesondere Hans Kippenberger ins Zwielicht, der als Leiter des M-Apparats die Verantwortung für die Sicherheit des Parteichefs trug und mit Blick auf die Ereignisse des 3. März auch ausdrücklich übernahm („eine Katastrophe und eine Schande vor der ganzen Internationale“ so Kippenberger). In den folgenden Jahren kam es dennoch wiederholt zu Vertuschungsversuchen und gegenseitigen Verdächtigungen der mittel- und unmittelbar beteiligten Personen, die noch durch gezielte Desinformationsmaßnahmen und vor allem durch weitere Verhaftungserfolge der Gestapo angeheizt wurden. Dieser war es gelungen, Kattner in der Haft „umzudrehen“ und mit dessen Hilfe am 9. November 1933 den Thälmann-Nachfolger John Schehr sowie am 18. Dezember auch Hermann Dünow, der Kippenberger abgelöst hatte, festzunehmen. Kattner, dem von der Gestapo obendrein eine tragende Rolle im geplanten Prozess gegen Thälmann zugedacht worden war, wurde am 1. Februar 1934 in Nowawes von Hans Schwarz, einem Mitarbeiter des M-Apparats, erschossen. Birkenhauer, dem Thälmann die Schuld an der Verzögerung seines Quartierwechsels und damit an seiner Festnahme gegeben hatte, und Kippenberger wurden im sowjetischen Exil hingerichtet, Hirsch kam in sowjetischer Haft ums Leben.

Die nationalsozialistische Justiz plante zunächst, Thälmann einen Hochverrats-Prozess zu machen. Hierfür sammelte sie intensiv belastendes Material, das die behauptete „Putschabsicht“ der KPD beweisen sollte. Ende Mai 1933 wurde Thälmanns „Schutzhaft“ aufgehoben und eine formelle Untersuchungshaft angeordnet. In diesem Zusammenhang wurde er vom Polizeipräsidium am Alexanderplatz in die Untersuchungshaftanstalt Moabit verlegt. Dieser Ortswechsel durchkreuzte den ersten einer Reihe von unterschiedlich konkreten Plänen, Thälmann zu befreien.

Thälmann wurde 1933 und 1934 mehrfach von der Gestapo in deren Zentrale in der Prinz-Albrecht-Straße verhört und dabei auch misshandelt. Bei einem Verhör am 8. Januar schlug man ihm vier Zähne aus, anschließend traktierte ihn ein Vernehmer mit einer Nilpferdpeitsche. Am 19. Januar suchte Hermann Göring den zerschundenen Thälmann auf und ordnete seine Rückverlegung in das Untersuchungsgefängnis Moabit an. Die in dieser Phase entstandenen Verhörprotokolle wurden bis heute nicht aufgefunden und gelten als verloren. Thälmann blieb unterdessen lange ohne Rechtsbeistand; der jüdische Anwalt Friedrich Roetter, der sich seiner angenommen hatte, wurde nach kurzer Zeit aus der Anwaltschaft ausgeschlossen und selbst in Haft genommen. 1934 übernahmen die Rechtsanwälte Fritz Ludwig (ein NSDAP-Mitglied) und Helmut R. Külz die Verteidigung Thälmanns. Vor allem Ludwig, der für ihn Kassiber aus der Zelle bzw. Zeitungen und Bücher in die Zelle schmuggelte sowie die als Geheime Reichssache deklarierte Anklageschrift an Unterstützer im Ausland weiterleitete, vertraute Thälmann sehr. Über die Anwälte – daneben auch über Rosa Thälmann – lief ein Großteil der verdeckten Kommunikation zwischen Thälmann und der KPD-Führung. Mit Rücksicht auf das Ausland, vor allem aber, weil die Beweisabsicht der Staatsanwaltschaft erkennbar wenig gerichtsfest war und ein mit dem Reichstagsbrandprozess vergleichbares Desaster vermieden werden sollte, einigten sich die beteiligten Behörden im Laufe des Jahres 1935, von einer „justizmäßigen Erledigung“ Thälmanns Abstand zu nehmen. Am 1. November 1935 hob der II. Senat des Volksgerichtshofes die Untersuchungshaft auf (ohne das Verfahren als solches einzustellen) und überstellte Thälmann gleichzeitig als „Schutzhäftling“ an die Gestapo.

1935/36 erreichte die internationale Protestbewegung gegen die Inhaftierung Thälmanns einen Höhepunkt. Zu seinem 50. Geburtstag am 16. April 1936 bekam er Glückwünsche aus der ganzen Welt, darunter von Maxim Gorki, Heinrich Mann, Martin Andersen Nexø und Romain Rolland. Im selben Jahr begann der Spanische Bürgerkrieg. Die XI. Internationale Brigade und ein ihr untergliedertes Bataillon benannten sich nach Ernst Thälmann.

1937 wurde Thälmann von Berlin in das Gerichtsgefängnis Hannover als „Schutzhäftling“ überführt. Thälmann bekam später eine größere Zelle, in der er jetzt Besuch empfangen konnte. Dies war ein Vorwand, um Thälmann in der Zelle abzuhören. Allerdings wurde ihm die Information über das heimliche Abhören zugespielt. Um sich dennoch frei „unterhalten“ zu können, nutzten er und seine Besucher kleine Schreibtafeln und Kreide.

Als Deutschland und die Sowjetunion 1939 ihre Beziehungen verbessert hatten (Hitler-Stalin-Pakt), setzte Stalin sich offenbar nicht für Thälmanns Freilassung ein. Nach der Befreiung seiner Familie durch die Rote Armee erfuhren die Angehörigen sogar, dass Thälmanns Rivale Walter Ulbricht alle ihre Bitten ignoriert und nicht für die Befreiung von Thälmann Position bezogen hatte.

Anfang 1944 schrieb Ernst Thälmann in Bautzen seine heute noch erhaltene "Antwort auf die Briefe eines Kerkergenossen."

Die genauen Umstände von Thälmanns Tod sind unklar und in der Forschung bis heute umstritten.

Ein möglicher Hergang ist, dass Thälmann am 17. August 1944 durch zwei Gestapo-Beamte aus dem Zuchthaus Bautzen ins KZ Buchenwald gebracht wurde, wo er ohne Gerichtsverfahren auf Befehl Adolf Hitlers erschossen wurde.
Dies könnte am frühen Morgen des 18. August in einem Heizungskeller nahe dem Krematorium geschehen und seine Leiche im Anschluss sofort verbrannt worden sein. So berichten Zeugen, dass am Nachmittag des 17. August auf Befehl sofort ein Verbrennungsofen anzuheizen war und die Asche nach der Verbrennung dunkel gewesen sei, was auf eine Verbrennung mit Kleidung zurückzuführen wäre.

Eine andere Version schilderte der Buchenwald-Gefangene Walter Hummelsheim 1945: Thälmann sei erst vier oder fünf Tage nach der Bombardierung des Lagers am 24. August 1944, zusammen mit neun anderen Kommunisten, in der Stallanlage des Lagers erschossen worden. Die dort Ermordeten seien nie in die offiziellen Lagerlisten aufgenommen worden. Der polnische Häftling Marian Zgoda soll die Tat sogar – versteckt hinter einem Schlackehaufen – direkt beobachtet haben. Zgoda sagte vor dem Landgericht Krefeld aus, er habe gehört, einer der Schützen habe die Frage eines anderen bejaht, ob es sich bei dem Erschossenen um Thälmann handele. Bei einem der mutmaßlichen Täter sollte es sich dieser Aussage nach um den SS-Stabsscharführer Wolfgang Otto gehandelt haben. Nach einem mehrjährigen Verfahren wurde Otto im Jahre 1988 in der Bundesrepublik freigesprochen. Auch der SS-Oberscharführer Werner Berger und der SS-Obersturmführer Erich Gust werden mit der Ermordung Thälmanns in Verbindung gebracht.

Nach neueren Forschungen ist es ebenso möglich, dass er von dem Buchenwalder Berufsverbrecher und Kapo Müller getötet oder sogar noch in Bautzen ermordet wurde. Eine weitere Version besagt, dass der Mordbefehl absolute Geheimhaltung forderte, weshalb der Lagerkommandant in Buchenwald kein SS-Exekutionskommando bestellte, sondern dem Transportkommando, das Thälmann gebracht hatte, befahl, ihn an Ort und Stelle zu erschießen.

Gesichert ist, dass am 16. September vom Parteiorgan der NSDAP, dem "Völkischen Beobachter", wahrheitswidrig die Meldung verbreitet wurde, Thälmann sei zusammen mit dem ehemaligen Vorsitzenden der SPD-Reichstagsfraktion Rudolf Breitscheid bei einem alliierten Bombenangriff auf Buchenwald am 24. August ums Leben gekommen, wobei der Völkische Beobachter vom 28. August schreibt:

Seit 1992 erinnert im Berliner Ortsteil Tiergarten an der Ecke Scheidemannstraße / Platz der Republik eine der 96 Gedenktafeln zur Erinnerung an von den Nationalsozialisten ermordete Reichstagsabgeordnete an Thälmann.

Neben der Benennung von Einheiten der Internationalen Brigaden (siehe Thälmann-Bataillon) nach Ernst Thälmann noch zu seinen Lebzeiten wurde 1948 in der SBZ die „Pionierorganisation Ernst Thälmann“ gegründet, der dieser Name 1952 offiziell verliehen wurde. Pioniere der älteren Jahrgänge (etwa zehn bis 14 Jahre) wurden „Thälmann-Pioniere“ genannt.

Viele Arbeitskollektive, Schulen, Straßen (siehe Ernst-Thälmann-Straße), Plätze, Orte bzw. Siedlungen, Sportstadien und Betriebe in der DDR, wie als eines der bekanntesten Beispiele der VEB SKET (Schwermaschinenbaukombinat Ernst Thälmann) oder die Offiziershochschule der Landstreitkräfte der NVA, trugen ebenfalls seinen Namen. Auch wurde die Ernst-Thälmann-Insel in der kubanischen Schweinebucht nach ihm benannt. Am 30. November 1949 wurde der Berliner Wilhelmplatz feierlich in Thälmannplatz umbenannt. Auch die angrenzende U-Bahn-Station bekam den Namen Thälmannplatz. In den 1980er Jahren wurde in Berlin im Prenzlauer Berg der Ernst-Thälmann-Park angelegt, dazu wurde ein großes Ernst-Thälmann-Denkmal des sowjetischen Bildhauers Lew Kerbel errichtet. Daneben gibt es weitere Ernst-Thälmann-Denkmäler.

Hamburg benannte eine Straße nach Thälmann. Im Jahr 1956 erhielt die Straße als Zeichen gegen die blutige Niederschlagung des Volksaufstands in Ungarn durch die Sowjetunion den Namen Budapester Straße. Seit 1969 gibt es in Hamburg die private „Gedenkstätte Ernst Thälmann“ in seinem Wohnhaus am 1985 nach ihm benannten Ernst-Thälmann-Platz in Hamburg-Eppendorf.

Außerdem gab es noch die inzwischen abgerissene Ernst-Thälmann-Gedenkstätte Sporthaus Ziegenhals bei Berlin. Dort bot er 1933 in seiner "Ziegenhals-Rede" der SPD die „Antifaschistische Aktion“ als Einheitsfront gegen den deutschen Faschismus an. Eine weitere Thälmann-Gedenkstätte befindet sich im Kleistpark Frankfurt (Oder).

In Dresden gibt es eine Ernst-Thälmann-Gedenkstätte im Stadtteil Strehlen.

In der Gedenkstätte der Sozialisten auf dem Berliner Zentralfriedhof Friedrichsfelde ist eine Inschrift für Thälmann im zentralen Rondell angebracht, mit der er symbolisch geehrt wird. Seine Grabstätte wird durch diese nicht gekennzeichnet.

Seit dem 24. Juli 2009 erinnert vor seinem letzten Wohnhaus in der Tarpenbekstraße in Hamburg-Eppendorf ein Stolperstein an Ernst Thälmann.

Am 8. Juni 2012 wurden vor dem Hamburger Rathaus Stolpersteine für die ermordeten Mitglieder der Hamburgischen Bürgerschaft verlegt, darunter auch ein weiterer für Ernst Thälmann.

Am 20. Februar 2018 wurden in Singen Stolpersteine für Ernst, Rosa und Irma Thälmann verlegt. Irma Thälmann hatte im Juni 1940 ihrem Jugendfreund Heinrich Vester geheiratet. Mit ihm wohnte sie ab Dezember 1941 in der Rielasinger Straße 180, wo sie am 15. April 1944 verhaftet und ins KZ Ravensbrück gebracht wurde. Auch Rosa Thälmann wohnte einige Monate bei ihrer Tochter in Singen. 

Seinen Namen trugen drei Ausbildungsschiffe der GST-Marineschule „August Lütgens“ Greifswald-Wieck: Die mit 150 m² Segelfläche größte GST-Yacht, der Seekreuzer „Ernst Thälmann“, späterer Name „Ernst Schneller“, das ehemalige MLR vom Typ „Habicht“ und Rettungsschiff „R-11“ der Volksmarine, das nach Umbau anschließend von 1968 bis 1977 als GST-Motorschulschiff „Ernst Thälmann“ (I) im Einsatz war, und der moderne Nachfolger des Schiffes, das MSR „Anklam“ der Volksmarine, das als MSS „Ernst Thälmann“ (II) von 1977 bis 1989 als Schulschiff fuhr und 1990 nach Dänemark verkauft wurde. Zudem wurde das ehemalige dänische Inspektionsschiff Hvidbjørnen, das sich 1945 zur Reparatur in Rostock befand, von der Volkspolizei See 1952 als Flagg- und Schulschiff "Ernst Thälmann" übernommen. Nach der Umwandlung der VP-See in die Volksmarine 1956, fuhr es dort bis 1961 und wurde kurz vor seiner Außerdienststellung noch in "Albin Köbis" umbenannt. Es endete als Zielübungsschiff 1965, wobei es in der Ostsee versenkt wurde.

Während zu DDR-Zeiten tausende Deutsche Ernst Thälmanns gedachten, kamen an seinem 125. Geburtstag im Jahr 2011 in Hamburg nur noch knapp 100 Gäste zusammen. Egon Krenz als Ehrengast würdigte die Leistung Thälmanns mit den Worten „Er blieb ein Kämpfer, sich und seiner Sache treu, bis in den Tod.“ und beklagte gleichzeitig, dass die Verdienste Thälmanns nicht mehr gewürdigt werden. Aus Moskau kamen an diesem Tag „solidarisch kämpferische Grüße“ vom Ukrainischen Bund der sowjetischen Offiziere.

Nach Thälmann benannte Ortschaften:

Darüber hinaus ist Thälmann Namensgeber für die Thälmannberge in der Antarktis.

Schon zu Lebzeiten wurde Thälmann auch von der Linken zum Teil scharf kritisiert. Die damalige KPD-Führung stand dem unter seiner Führung stehenden Hamburger Aufstand kritisch gegenüber. In seiner Zeit als Chef der KPD unterwarf Thälmann die deutschen Kommunisten der Hegemonie der Kommunistischen Partei der Sowjetunion. Anhänger eines unabhängigen Kurses wurden aus der Partei gedrängt.

Clara Zetkin, die im April 1925 mit ihrer Polemik gegen Thälmanns Amtsvorgängerin Ruth Fischer vor dem Exekutivkomitee der Komintern mithalf, diesen an die Spitze der Partei zu bringen, charakterisierte die KPD unter Thälmann im September 1927 als „schwach und unfähig“, geprägt durch „Herausbildung kleiner Kliquen, persönliches Intrigieren, Gegeneinanderarbeiten“. Einem scheinbar hilflosen Thälmann attestiert sie, dass er „… kenntnislos und theoretisch ungeschult ist, in kritiklose Selbsttäuschung und Selbstverblendung hineingesteigert wurde, die an Größenwahnsinn grenzt und der Selbstbeherrschung mangelt …“

Die Strategie der KPD während der Weimarer Republik, in der SPD einen Hauptfeind zu sehen (These vom Sozialfaschismus), wird oft als Schwächung der antifaschistischen Kräfte gesehen. Auch ein maßgeblicher Kommunismus-Forscher wie Hermann Weber urteilt kritisch: „Thälmann muss bei allem Respekt für seine Standhaftigkeit in Hitlers Kerker nachgesagt werden, dass er nur ein Provinzpolitiker mit demagogischem Talent war.“ Klaus Schroeder, der Leiter des "Forschungsverbundes SED-Staat an der Freien Universität Berlin", stellt in dem Artikel "Warum wir Thälmann nicht ehren sollten" fest, dass der „KPD-Führer ein Gegner der Demokratie“ war.



Postum erschienen:





</doc>
<doc id="1280" url="https://de.wikipedia.org/wiki?curid=1280" title="El Niño">
El Niño

El Niño (span. für „der Junge, das Kind“, hier konkret: „das Christuskind“) nennt man das Auftreten ungewöhnlicher, nicht zyklischer, veränderter Strömungen im ozeanographisch-meteorologischen System ("El Niño-Southern Oscillation", ENSO) des äquatorialen Pazifiks. Das Phänomen tritt in unregelmäßigen Abständen von durchschnittlich vier Jahren auf. Der Name ist vom Zeitpunkt des Auftretens abgeleitet, nämlich zur Weihnachtszeit. Er stammt von peruanischen Fischern, die den Effekt aufgrund der dadurch ausbleibenden Fischschwärme wirtschaftlich zu spüren bekommen.

Zur Weihnachtszeit beträgt die normale Wassertemperatur im Pazifik vor Indonesien 28 °C, die vor der Küste Perus dagegen nur 24 °C. Durch die Passatwinde kommt es vor Peru zum Auftrieb von kühlem Wasser aus den Tiefen des Ozeans. Dieser Auftrieb ist Teil des Humboldtstroms vor der Küste Südamerikas.
Bei El Niño kommt es zu einem geringeren Auftrieb durch die schwächeren Passatwinde und somit wird der kalte Humboldtstrom allmählich schwächer und kommt zum Erliegen. Das Oberflächenwasser vor der Küste Perus erwärmt sich so sehr, dass die obere Wasserschicht nicht mehr mit dem kühlen und nährstoffreichen Tiefenwasser durchmischt wird. Deshalb kommt es zum Absterben des Planktons, das zum Zusammenbruch ganzer Nahrungsketten führt. Außerdem führen die größeren Mengen verdunstenden Wassers vor der südamerikanischen Pazifikküste zu sehr starken Regenfällen an der Westseite der Anden, die zu Hangrutschungen und Überschwemmungen der Abflussgewässer führen; dadurch werden auch die Siedlungen der Bewohner betroffen.
Der Ostpazifik vor Südamerika erwärmt sich, während vor Australien und Indonesien die Wassertemperatur absinkt. Aufgrund der im Normalfall erhöhten Temperatur im Westpazifik kommt es zu einer Luftdruckabnahme und im kälteren Ostpazifik zur Bildung eines Hochdruckgebiets. Dadurch entstehen bodennahe Ostwinde, die warmes Oberflächenwasser aus dem Pazifik vor Südamerika in Richtung Westen nach Indonesien schieben. Während eines El Niños wird diese Luftzirkulation, genannt „Walker-Zirkulation“, umgekehrt. Dabei strömt innerhalb von ca. drei Monaten die Warmwasserschicht von Südostasien nach Südamerika. Die Ostwinde begünstigen die Kelvinwellen, die durch den schwachen Passat nun Wasser in östliche Richtung treiben und dort den Wasserstand um ca. 30 cm erhöhen. 
El Niño ist ein natürliches Klimaphänomen; in den letzten Jahren stoppt die warme Meeresschicht weiter vor der Küste. Ob dies im Zusammenhang mit dem anthropogenen Treibhauseffekt oder mit längerfristigen natürlichen Schwankungen des Pazifiks steht, der bei El Niño von einer warmen in eine kalte Phase umschwenkt, ist bisher nicht geklärt.

Auf drei Vierteln der Erde werden die Wettermuster beeinflusst. Auf den Galápagos-Inseln und an der südamerikanischen Küste kommt es zu starken Regenfällen. Diese führen zu Überschwemmungen entlang der westlichen Küste Südamerikas. Selbst an der nordamerikanischen Westküste kommt es zu Überschwemmungen.

Der Regenwald im Amazonasgebiet leidet dagegen unter Trockenheit. Vor Mexiko können gewaltige Wirbelstürme entstehen, die enorme Schäden anrichten. In Südostasien und Australien kommt es durch den fehlenden Regen zu Buschfeuern und riesigen Waldbränden. Während es in Ostafrika in Ländern wie Kenia und Tansania mehr Regen gibt, ist es in Sambia, Simbabwe, Mosambik und Botswana (südliches Afrika) deutlich trockener.

Es kommt zu einem Massensterben von Meerestieren, Seevögeln und Korallen. Durch die Erwärmung des Meereswassers kommt es zum Absterben des Planktons vor der peruanischen Küste. Hier gab es in normalen Jahren bis zu zehnmal so viel Fisch wie an anderen Küsten. Bei El Niño finden die Fische nichts mehr zu fressen und wandern ab. Die Robbenkolonien finden keine Nahrung mehr und viele Tiere verhungern. Der wirtschaftliche Schaden für die Menschen ist kaum zu beziffern.

Durch die hohen Temperaturen tritt auch in den Gebieten die Korallenbleiche in den Riffen auf, die bisher davon verschont blieben.

Europa blieb bis auf wenige Ausnahmen, wie etwa dem in Europa ungewöhnlich kalten Winter 1941/1942, von den Fernwirkungen El Niños verschont. Allerdings wurde eine Auswirkung auf den kalten und schneereichen Winter 2009/2010 in Europa und Nordamerika diskutiert. Außerdem führt das El-Niño-Phänomen zu Auswirkungen auf den Indischen Monsun – in El-Niño-Jahren ist der Niederschlag stark erhöht, wohingegen der Monsun in La-Niña-Jahren geringeren Niederschlag mit sich bringt.

Bedingungen für das Auftreten von El Niño stellten sich innerhalb der letzten 300 Jahre in Zeitabschnitten von zwei bis sieben (oder acht) Jahren ein. Jedoch sind die meisten Niños eher schwach ausgeprägt. Es gibt Hinweise auf sehr starke El-Niño-Ereignisse zu Beginn des Holozäns vor etwa 11.700 Jahren.

Größere El-Niño-Ereignisse wurden für die Jahre 1790–1793, 1828, 1876–1878, 1891, 1925/1926, 1972/1973 und 1982/1983 notiert. In der jüngsten Vergangenheit kam es in den Jahren 1982/1983 und 1997/1998 zu größeren Ereignissen, während das Ereignis 2015/2016 das drittstärkste seit 65 Jahren sein dürfte.

El Niño beeinträchtigte die vorkolumbianischen Inka und mag sogar zum Untergang der Moche und anderer kolumbianischer und peruanischer Kulturen beigetragen haben. Die erste echte Aufzeichnung stammt aus dem Jahr 1726.
Eine weitere frühe Aufzeichnung erwähnt sogar den Ausdruck "El Niño" in Bezug auf Klimaereignisse im Jahr 1892. Sie stammt von Captain Camilo Carrillo aus seinem Bericht auf dem Kongress der geographischen Gesellschaft in Lima, in dem er sagte, dass peruanische Seeleute diese warme nördliche Strömung "El Niño" nannten, da sie in der Zeit um Weihnachten auftrete.

Das Phänomen war von langfristigem Interesse, da es sich auf die Guanoindustrie auswirkte und auch auf andere Industriezweige, die biotische Produkte des Meeres nutzten.

Charles Todd beobachtete im Jahr 1893, dass Trockenzeiten in Indien und Australien gleichzeitig mit dem Phänomen eintraten. Dasselbe hielt auch Norman Lockyer im Jahr 1904 fest. Eine Verbindung mit Überflutungen wurde 1895 von Pezet und Eguiguren ins Feld geführt. 1924 prägte Gilbert Walker (Namensgeber für die Walkerzirkulation) den Begriff "Südliche Oszillation".

In den meisten Jahren ist es unwahrscheinlich, dass das Phänomen Auswirkungen bis nach Europa hat. Jedoch gibt es Jahre, in denen das Klima Europas mit einem ENSO-Ereignis zu korrelieren scheint. So sehen manche Studien eine Beziehung zwischen dem besonders harten Winter 1941/1942 beim deutschen Russlandfeldzug und El Niño. Hierbei sind möglicherweise eher langskalige Zyklen wie die PDO zu berücksichtigen als El Niño selbst.

Das große El-Niño-Ereignis von 1982/1983 führte zu einer starken Belebung des Interesses durch die wissenschaftlichen Kreise. Die Zeit von 1990 bis 1994 war sehr auffällig, da El Niño in diesen Jahren in ungewöhnlich schneller Folge auftrat.

Über den Jahreswechsel 1982/1983 und im Jahr 1997/1998 war El Niño ungewöhnlich stark ausgeprägt. Die Wassertemperatur lag sieben Grad Celsius über der normalen Durchschnittstemperatur, so dass Wärmeenergie in die Erdatmosphäre abgegeben wurde. Bei diesem Ereignis wurde die Luft zeitweilig um bis zu 1,5 °C erwärmt, viel im Vergleich zur üblichen Erwärmung von 0,25 °C im Umfeld eines El Niño. 1997/1998 kam es darüber hinaus zu einem geschätzten Absterben von einem Sechstel der weltweiten Riffsysteme. Seit dieser Zeit ist der Effekt der Korallenbleiche (Coral bleaching) weltweit bekannt geworden; in allen Regionen wurden Bleichstellen gefunden.

Sang-Wook Yeh und Mitarbeiter äußerten 2010 die These, El Niño trete nicht mehr zungenförmig, sondern hufeisenförmig auf. 
Dieser Trend könnte durch den Klimawandel und/oder durch natürlich wiederkehrende Zyklen des Pazifiks möglicherweise in den kommenden Jahrzehnten stärker werden. 
Am 5. März 2015 prognostizierte die NOAA die Ankunft eines neuen El-Niño-Ereignisses in den nächsten Monaten. Die Prognose erwies sich als richtig. Die Auswirkungen gelten im südlichen Afrika und Ostafrika als die stärksten seit mehreren Jahrzehnten und führten zu Viehsterben, Nahrungsmittelknappheit und politischer Instabilität. Allein in Äthiopien sind 10 bis 20 Millionen Menschen von Hunger und akuter Wasserknappheit bedroht, weltweit wird die Zahl der Betroffenen auf über 60 Millionen geschätzt. Insgesamt erwies sich der El-Niño als einer der drei stärksten, die jemals beobachtet wurden (neben 1982/1983 und 1997/1998), und gilt neben der Globalen Erwärmung als Hauptursache für den 2015 aufgetretenen weltweiten Hitzerekord.

Einigen Studien zufolge könnten El-Niño-Ereignisse genauer als bisher angenommen voraussagbar sein<ref name="DOI10.1038/428709a">
</ref><ref name="DOI10.1073/pnas.1323058111"></ref> (siehe hierzu auch Witterungsprognose).

Ein Vorhersageverfahren beruht auf der Auswertung charakteristischer Luftdruckanomalien im südpazifischen Raum. Grundlage sind Luftdruckmessungen aus Tahiti und Darwin (Australien). Ergebnis dieser Auswertung ist der Southern Oscillation Index (SOI).

Ein verwandtes Phänomen im Atlantik ist etwa die dekadische Nordatlantische Oszillation (NAO), die durch Telekonnektion über den Nordpol (Arktische Oszillation  AO) und die amerikanischen Landmassen auch zeitverzögert mit El-Niño-/La-Niña-Phasen ursächlich zusammenhängen könnte.

Im Gegensatz zu El Niño ist La Niña eine außergewöhnlich kalte Strömung im äquatorialen Pazifik, also sozusagen ein Anti-El-Niño, worauf auch die Namensgebung (spanisch: „kleines Mädchen“) beruht.
Durch diese kalte Strömung entwickelt sich über Indonesien ein besonders starkes Tiefdruckgebiet. Die Passatwinde wehen stark und lang anhaltend. Dadurch kühlt sich der östliche Pazifik weiter ab und es gibt in Indonesien besonders viel Regen. Dagegen ist es in Peru sehr trocken und es fällt kaum Niederschlag. Durch die globale Erwärmung sind jedoch mittlerweile auch außergewöhnlich kalte La-Niña-Jahre wärmer als der langjährige Durchschnitt des 20. Jahrhunderts.




</doc>
<doc id="1282" url="https://de.wikipedia.org/wiki?curid=1282" title="Eindhoven">
Eindhoven

Eindhoven ([], ) ist eine Stadt in der Provinz Nordbrabant im Süden der Niederlande. Sie liegt auf einer Höhe von .

Eindhoven besteht aus sieben Stadtteilen:

Eindhoven wurde auf einem etwas erhöhten Gebiet am Zusammenfluss der Flüsse Dommel und Gender an einem Handelsweg von Holland nach Lüttich gegründet und erhielt bereits 1232 Stadt- und Marktrechte durch den Herzogen Heinrich I. von Brabant, blieb jedoch in den ersten Jahrhunderten recht unbedeutend.

Um 1388 wurden die Verteidigungsanlagen der Stadt weiter ausgebaut und zwischen 1413 und 1420 wurde ein neues Schloss innerhalb der Stadtmauern errichtet. 1486 wurde Eindhoven geplündert und niedergebrannt. Der Wiederaufbau und der Bau eines neuen Schlosses dauerten bis 1502. Doch bereits 1543 fiel Eindhoven erneut: Die Verteidigungsanlagen waren wegen der herrschenden Armut nicht instand gehalten worden.

Ein großes Feuer zerstörte 1554 rund drei Viertel der Häuser. Diese wurden jedoch mit Unterstützung von Wilhelm von Oranien bereits 1560 wieder aufgebaut.

Die heutige Stadt Eindhoven entstand durch das Zusammenwachsen der Kirchspiele Eindhoven, Woensel, Strijp, Tongelre, Gestel und Stratum als Folge der industriellen Entwicklung um 1900, als die Glühlampenfabrik Philips immer mehr Arbeitnehmer anzog. Später trug auch DAF (Automobile) zur Expansion der Stadt bei.
Während des Zweiten Weltkrieges war die Stadt ein wichtiges Ziel während der Operation Market Garden. Auch wegen der Philips-Röhrenwerke wurde die Stadt angegriffen. 
Schwere Bombardierungen der Westalliierten zerstörten große Teile der Stadt. Beim Wiederaufbau wurden nur sehr wenige historische Gebäude erhalten. Ein Beispiel für die moderne Architektur Eindhovens ist das 1966 als Museum errichtete Evoluon, jetzt Konferenzgebäude und nicht mehr zu besichtigen.

Heute ist Eindhoven mit etwas mehr als 200.000 Einwohnern die fünftgrößte Stadt der Niederlande und gilt als Technologiezentrum im Süden des Landes. Durch die Studentinnen und Studenten der Technischen Universität Eindhoven und durch einige höhere Schulen hat Eindhoven einen relativ niedrigen Altersdurchschnitt.



Eindhoven ist der wichtigste Produktions- und Forschungsstandort der Firma Philips. Das ehemalige Tochterunternehmen NXP Semiconductors hat hier seinen Hauptsitz. UPS betreibt ein großes Logistikzentrum. Zu den größten in Eindhoven ansässigen Arbeitgebern gehört auch die Fachhochschule Fontys. OTB Solar B.V. fertigt hier Produktionsanlagen für Solarzellen. Weiterhin ist der LKW-Hersteller DAF in Eindhoven ansässig. Die Firma ASML ist im Vorort Veldhoven ansässig.

Seit Mai 2001 befindet sich in der Stadt die Europa-Niederlassung des japanischen Unternehmens Kanefusa, Asiens führendem Hersteller von Werkzeugen für die Holzbearbeitung.


Die Stadt Eindhoven ist ein zentraler Eisenbahnknotenpunkt in den Niederlanden. Hier bündeln sich die Strecken aus dem Norden und Süden des Landes. Am Bahnhof Eindhoven verkehrt nationaler Regional- und Fernverkehr. Städte wie Den Haag, Rotterdam, Amsterdam, Utrecht und Maastricht sind mit dem Zug von Eindhoven aus direkt erreichbar.

Insgesamt führen fünf Autobahnen durch Eindhoven. Auch im niederländischen Autobahnnetz bündeln sich hier Strecken aus sowohl dem Norden und Süden als auch aus dem Westen des Landes. Weiterhin liegt Eindhoven direkt an der wichtigen E34.

Den städtischen Busverkehr betreibt das Busunternehmen Hermes.

Eindhoven verfügt über einen Flughafen, den Eindhoven Airport (IATA-CODE: EIN). Er ist sowohl Transport- als auch Militärflugplatz der niederländischen Luftwaffe und der größte Regionalflughafen der Niederlande.





</doc>
<doc id="1283" url="https://de.wikipedia.org/wiki?curid=1283" title="Edmund Stoiber">
Edmund Stoiber

Edmund Rüdiger Rudi Stoiber (* 28. September 1941 in Oberaudorf) ist ein deutscher Jurist und Politiker (CSU). Von Mai 1993 bis September 2007 war er Ministerpräsident des Freistaates Bayern und von 1999 bis 2007 Vorsitzender der CSU. Seit 2007 ist er Ehrenvorsitzender der CSU.

Stoiber war bei der Bundestagswahl im September 2002 Kanzlerkandidat der Union. SPD und Grüne erhielten zusammen eine Mehrheit von 306 von 603 Bundestagssitzen, Gerhard Schröder blieb Bundeskanzler, die Union konnte ihr Wahlergebnis um 3,4 Prozentpunkte verbessern und erhielt 248 Sitze.
Bei der bayerischen Landtagswahl 2003 erreichte die CSU mit Stoiber als Spitzenkandidaten das nach Sitzverteilung beste Ergebnis, das je bei einer Landtagswahl in der Bundesrepublik erzielt wurde (60,7 % der Stimmen → 124 von 180 Sitzen entspricht also zweidrittel-Mehrheit).
Von November 2007 bis Oktober 2014 war Stoiber in Brüssel ehrenamtlicher Leiter einer EU-Arbeitsgruppe zum Bürokratieabbau.

Edmund Stoiber wuchs im Oberaudorf (Oberbayern, Landkreis Rosenheim) als Sohn des aus Schwarzenfeld in der Oberpfalz stammenden Bürokaufmanns Edmund Georg Stoiber und der gebürtigen Rheinländerin Elisabeth Stoiber (geb. Zimmermann) aus Dormagen auf. Stoibers Großeltern mütterlicherseits stammen ebenfalls aus der Oberpfalz: sie waren aus Nabburg – acht Kilometer nördlich von Schwarzenfeld – ins Rheinland abgewandert.

Von 1951 bis 1961 besuchte Stoiber das Ignaz-Günther-Gymnasium in Rosenheim, musste dabei die siebte Klasse wiederholen und legte dort das Abitur ab. Danach leistete er von 1961 bis 1962 als Reserveoffizieranwärter seinen Wehrdienst beim Gebirgsjägerbataillon 231 in Bad Reichenhall und beim Gebirgsjägerbataillon 233 in Mittenwald. Da er sich während der Ausbildung eine schwere Knieverletzung zuzog, wurde er vorzeitig entlassen. Er ist Mitglied im Kameradenkreis der Gebirgstruppe.

Nach dem Wehrdienst begann Stoiber im Herbst 1962 ein Studium der politischen Wissenschaften an der Hochschule für Politik München und der Rechtswissenschaft an der Ludwig-Maximilians-Universität München, welches er 1967 mit dem ersten juristischen Staatsexamen beendete. Danach war er wissenschaftlicher Mitarbeiter am Lehrstuhl für Strafrecht und Ostrecht an der Universität Regensburg. 1968 ging er ins Referendariat und wurde 1971 bei Friedrich-Christian Schroeder an der Universität Regensburg mit der Dissertation „"Der Hausfriedensbruch im Lichte aktueller Probleme"“ zum Dr. iur. promoviert. Im selben Jahr bestand er das zweite juristische Staatsexamen mit Prädikat.

Noch im selben Jahr trat er in das Bayerische Staatsministerium für Landesentwicklung und Umweltfragen ein. Hier war er von 1972 bis 1974 persönlicher Referent des Staatsministers Max Streibl und zuletzt auch Leiter des Ministerbüros. Seit 1978 ist Stoiber als Rechtsanwalt zugelassen. Von 1978 bis 1982 war er außerdem als Syndikus für die Lotto-Toto-Vertriebsgemeinschaft Bayern tätig.

Edmund Stoiber trat im Dezember 1971 der Jungen Union und der CSU bei. Von 1974 bis 2008 war er Mitglied des Bayerischen Landtages. Von 1978 bis 1983 war er unter dem Parteivorsitzenden Franz Josef Strauß Generalsekretär der CSU. In diesem Amt erwarb er sich beim politischen Gegner einen Ruf als "„blondes Fallbeil“". Als Generalsekretär war er außerdem verantwortlich für den Bundestagswahlkampf 1980, bei der der Kanzlerkandidat von CDU und CSU, Franz Josef Strauß, dem amtierenden Bundeskanzler Helmut Schmidt (SPD) unterlag.

Seit 1989 war Stoiber Stellvertretender Vorsitzender der CSU. Außerdem war er 1989 bis 1993 Vorsitzender der Grundsatzkommission der CSU. Nach der für die Union verlorenen Bundestagswahl 1998 wurde er als Nachfolger des ehemaligen Bundesfinanzministers Theodor Waigel am 16. Januar 1999 mit 93,4 Prozent zum Parteivorsitzenden der CSU gewählt. Zuvor war es seit Stoibers Wahl zum bayerischen Ministerpräsidenten im Jahr 1993 mehrfach zu Konflikten mit Waigel gekommen. Am 4. September 1999 entließ Ministerpräsident Stoiber den Justizminister Alfred Sauter telefonisch, weil er ihn für die sogenannte LWS-Affäre verantwortlich machte. Die halbstaatliche Wohnungsbaugesellschaft LWS hatte zu diesem Zeitpunkt 367 Millionen Mark Verlust angehäuft. Sauter bezeichnete Stoibers Anschuldigungen als „Schafsscheiß“ und rechnete eine Woche später im Landtag öffentlich mit ihm ab.
Seine Wiederwahl zum Parteivorsitzenden erfolgt am 9. Oktober 1999 mit 90 Prozent, am 13. Oktober 2001 mit 96,59 Prozent, am 19. Juli 2003 mit 96,97 Prozent, sowie am 3. September 2005 mit 93,1 Prozent.

Innerparteilich wurde nach Stoibers Rückkehr aus Berlin namentlich durch die Fürther Landrätin Gabriele Pauli die Meinung vertreten, dass er nicht zur Wiederwahl als Ministerpräsident antreten solle. Am 18. Januar 2007 gab Stoiber aufgrund schwindenden Rückhaltes in seiner Partei sowie sinkender Umfragewerte bekannt, dass er sein Amt als Regierungschef in Bayern am 30. September 2007 abgeben und im Herbst auf dem Parteitag der CSU nicht mehr für den Vorsitz kandidieren werde.

Bei seiner Abschiedsrede als Parteichef und Ministerpräsident hatte er auf dem Parteitag der CSU am 18. September 2007 von seinen Nachfolgern einen eigenständigen Kurs und ein klares konservatives Profil gefordert. Bayerns Wirtschaftsminister Erwin Huber wurde in einer Kampfabstimmung gegen CSU-Vizechef Horst Seehofer und die Parteirebellin Gabriele Pauli Nachfolger als Vorsitzender der CSU. Neuer bayerischer Ministerpräsident wurde am 9. Oktober der bisherige Innenminister Günther Beckstein. Zu seinem Abschied als Ministerpräsident Bayerns gab die Bundeswehr am 2. Oktober 2007 eine Serenade im Münchner Hofgarten. Gäste waren unter anderem Verteidigungsminister Franz Josef Jung, Generalinspekteur Schneiderhan sowie CSU-Funktionäre. Zur Landtagswahl in Bayern 2008 stand Stoiber nicht mehr zur Wahl.

1982 wurde Stoiber als Staatssekretär und Leiter der Bayerischen Staatskanzlei in die von Ministerpräsident Franz Josef Strauß geführte Bayerische Staatsregierung berufen. 1986 wurde er in gleicher Funktion zum Staatsminister ernannt. Nach dem Tod von Franz Josef Strauß wurde Stoiber dann 1988 im Kabinett von Max Streibl zum bayerischen Staatsminister des Innern ernannt.

Nachdem Max Streibl im Mai 1993 als bayerischer Ministerpräsident wegen der "Amigo-Affäre" zurückgetreten war, wurde Stoiber am 28. Mai 1993 zu seinem Nachfolger gewählt. In dieser Funktion war er vom 1. November 1995 bis zum 31. Oktober 1996 auch Präsident des Bundesrates. Bei den Landtagswahlen 1994 und 1998 konnte er als Spitzenkandidat der CSU deren absolute Mehrheit mit 52,8 % bzw. 52,9 % der abgegebenen Stimmen verteidigen und bei der Landtagswahl 2003 bei geringer Wahlbeteiligung (57,3 %) auf 60,7 % ausbauen. Mit diesem zweitbesten Ergebnis ihrer Geschichte erhielt die CSU über zwei Drittel (124 von 180) der Sitze im Bayerischen Landtag (was allein allerdings keine Verfassungsänderungen ermöglicht, hierzu ist ein Volksentscheid notwendig).

Im Januar 2002 konnte sich Stoiber gegenüber Angela Merkel als gemeinsamer Kanzlerkandidat von CDU und CSU für die Bundestagswahl 2002 durchsetzen. Nach dem „Wolfratshauser Frühstück“ mit Merkel wurde er von der Unionsspitze als Kanzlerkandidat nominiert – als erster CSU-Politiker nach Franz Josef Strauß. Schwerpunktthemen seines Wahlkampfs waren die Wirtschafts- und Sozialpolitik, dabei besonders die Bekämpfung der Arbeitslosigkeit, die Innere Sicherheit und die wirtschaftliche Entwicklung des strukturschwachen Nordostens Deutschlands. Wahlkampfleiter im damals parallel zum gemeinsamen Team der CDU/CSU agierenden Stoiber-Team war Michael Spreng, der ehemalige Chefredakteur der „Bild am Sonntag“. Trotz Zugewinnen gelang es Stoiber nicht, einer Koalition aus CDU/CSU und FDP die absolute Mehrheit zu sichern. CDU und CSU erhielten 3,4 Prozentpunkte mehr; damit endete ein 16 Jahre andauernder Abwärtstrend der Unionsparteien bei Bundestagswahlen. Allerdings waren die erreichten 38,5 Prozent immer noch ein unterdurchschnittliches Ergebnis für die Unionsparteien. Die SPD erhielt ebenfalls 38,5 Prozent (minus 2,4 Prozentpunkte), aber insgesamt 6.027 Stimmen (= 0,01 %) mehr als die Union. Die SPD konnte aufgrund von Überhangmandaten die stärkste Bundestagsfraktion stellen. Insgesamt verfehlte Stoiber sein Ziel, eine Schwarz-Gelbe Koalition zu bilden, um etwa 570.000 Stimmen.

Im Februar 2004 wurde Stoiber von Jacques Chirac mit Zustimmung von Bundeskanzler Gerhard Schröder das Amt des Präsidenten der Europäischen Kommission angetragen, was er jedoch ablehnte. Er wurde außerdem zu Beginn des Jahres 2004 noch vor Bekanntgabe der Köhler-Kandidatur als möglicher Kandidat für das Amt des Bundespräsidenten gehandelt, verzichtete jedoch auch in diesem Fall.

Vom 18. Oktober bis zum 8. November 2005 war Stoiber Mitglied des 16. Deutschen Bundestages. Im selben Jahr war er designierter Wirtschaftsminister der Bundesrepublik Deutschland im Kabinett Merkel, entschied sich allerdings am 1. November gegen diesen Posten. Den Rückzug begründete er mit dem damals ebenso angekündigten Rücktritt von Franz Müntefering als Parteivorsitzender der Sozialdemokraten. Seine Pläne für ein "Superministerium" konnte er im Streit mit der designierten Forschungsministerin Annette Schavan (CDU) nicht voll umsetzen.

Wegen seiner zögerlichen und unentschlossenen Haltung bezüglich seines Eintritts in eine Bundesregierung unter Angela Merkel geriet er – auch in der CSU – in die Kritik. Diese forderte, sollte Stoiber in München einen Neuanfang wagen, einen Politikwechsel, damit der Ministerpräsident sich wieder das Vertrauen der Wähler sichern könne. Am 18. Januar 2007 gab Stoiber bekannt, dass er zum 30. September 2007 sein Amt als bayerischer Ministerpräsident abgeben und auf dem Parteitag 2007 nicht erneut für das Amt des CSU-Vorsitzenden kandidieren werde, Details "siehe oben bei Parteilaufbahn". Das vierte Kabinett Stoiber blieb bis zum 16. Oktober 2007 kommissarisch im Amt.

Als ehemaliger Ministerpräsident unterhält er in München ein Büro, das 2008 mit 496.800 Euro aus dem bayerischen Landeshaushalt finanziert wurde.

Im Nachhinein wird der Rückzug aus Berlin als Anfang vom Ende Stoibers politischer Karriere angesehen. In der Zeit zwischen Stoibers Rückzug aus Berlin und seinem Rücktritt vom Amt des bayerischen Ministerpräsidenten fanden seine diversen Versprecher ein zunehmendes Medienecho. Ein typisches Beispiel ist die als "Transrapid-Rede" bekannt gewordene Passage seiner Rede beim Neujahrsempfang 2002 der Münchener CSU, die 2006 wiederentdeckt und zu einer populären Persiflage wurde.

Stoiber wurde nach seinem Rücktritt vom Amt des bayerischen Regierungschefs von José Manuel Barroso angeboten, eine Arbeitsgruppe der Europäischen Kommission zum Abbau der Bürokratie in der EU ehrenamtlich zu leiten. Von November 2007 bis Oktober 2014 war Stoiber in Brüssel Leiter dieser EU-Arbeitsgruppe, die Industriekommissar Günter Verheugen unterstellt war. Der Abschlussbericht dieser Arbeitsgruppe wurde im Oktober 2014 veröffentlicht. Nach eigenen Angaben soll die Arbeitsgruppe dazu beigetragen haben, dass die Unternehmen in Europa jedes Jahr 33 Milliarden Euro an Kosten einsparen.

Als Ministerpräsident war Stoiber Kurator der Initiative Neue Soziale Marktwirtschaft. Zusammen mit Franz Müntefering war er Vorsitzender der Bundesstaatskommission, einer gemeinsamen Kommission von Bundesrat und Bundestag zur Modernisierung der bundesstaatlichen Ordnung Deutschlands. Weiterhin engagierte er sich in zahlreichen anderen Funktionen:


Stoiber ist seit 1968 mit Karin Stoiber verheiratet. Das Ehepaar hat drei Kinder, Dominic, Veronica (Vroni) und Constanze, sowie fünf Enkelkinder und wohnt in Wolfratshausen. 2017 wurde Stoiber von der TU München zum Ehrensenator erhoben. Die TU München würdigt mit der höchsten Auszeichnung das Wirken des ehemaligen Ministerpräsidenten um die TU und die akademische Welt.

Stoiber hat sich in ausgewählten Politikfeldern eindeutig positioniert: Seine christlich-konservative Weltsicht, insbesondere zu den Themen Ehe, Homosexualität und Einwanderung, sowie seine scharfe Rhetorik wirken oft polarisierend. Er folgt damit der schon von seinen Vorgängern verfolgten Linie, konservative Kreise in die Partei zu integrieren. Im Umfeld seines Ausscheidens als Parteichef und Ministerpräsident erklärte er in einem Interview im September 2007, Franz Josef Strauß, sein politischer Mentor, habe Grundsätze formuliert und geprägt, die heute noch genauso gültig seien wie damals. Konservativ sein heiße, an der Spitze des Fortschritts zu marschieren. Rechts von der CSU dürfe es keine demokratisch legitimierte Partei geben. Den Maßstab für die CSU als Volkspartei bei Wahlen sei 50 Prozent plus x. Auch bei sehr guten Wahlergebnissen dürfe man sich nicht zurücklehnen, denn dies seien Momentaufnahmen und keine Bankguthaben.

In der Wirtschaftspolitik sprach sich Stoiber gegen eine schuldenfinanzierte Politik aus – im Interesse der zukünftigen Generationen und auch wegen der Stabilitätskriterien zur Euro-Einführung. Die Stärkung des Wirtschaftsstandortes Bayern (das seit 1995 seine führende Position in Deutschland trotz stagnierenden Wachstums halten konnte) u. a. durch die so genannte „High-Tech-Offensive Bayern“ hat zwar für ihn Priorität, doch gibt es „"im Zeitalter der Globalisierung keinen Weg zurück zu einem antiquierten Wirtschaftsnationalismus“".

Stoiber war im Rahmen der Föderalismusreform federführend an der 2001 beschlossenen Neuordnung des Länderfinanzausgleichs beteiligt. Er erklärte in einer Regierungserklärung, Bayern könne „höchst zufrieden“ sein.

Er erklärte wiederholt, die hohen Transferzahlungen zum Ausgleich räumlicher Disparitäten – z. B. von Nord- nach Süditalien oder von West- nach Ostdeutschland – sollten auslaufen. Der Übertrag nationaler Souveränität in der Geldpolitik auf die Europäische Zentralbank müsse durch eine föderalistische Wirtschafts-, Finanz- und Haushaltspolitik ergänzt werden.

Stoiber betrieb in Bayern seit der gewonnenen Landtagswahl 2003 eine rigide Sparpolitik, damit ab 2006 der Haushalt ausgeglichen sei. Ziel dessen war, den Rahmen für einen langfristig stabilen Wohlstand des Landes zu bilden. Politische Beobachter unterstellten damals, Stoiber wollte sich damit auch eine Empfehlung für ein Amt als Wirtschafts- und Finanzminister auf Bundesebene nach der nächstanstehenden Bundestagswahl erwerben.

2001 erklärte Stoiber, dass die Türkei nicht in die EU gehört, denn im Fall einer Aufnahme würde die Europäische Union eine Grenze mit Staaten wie Iran, Syrien und Irak haben. Europa würde sich so „nicht mehr zu bewältigende Schwierigkeiten“ aufladen. Stoiber sprach sich wiederholt strikt gegen einen Beitritt der Türkei zur Europäischen Union aus. Er erklärte Verhandlungen über einen EU-Beitritt der Türkei kämen nicht in Betracht, die Kernfrage sei ob die Türkei zu Europa gehöre und wo dessen Grenzen sind. Jenseits der Tatsache, dass nach wie vor massive Zweifel an der Rechtsstaatlichkeit der Türkei bestünden, müsse über die erwähnten Kernfragen eine „europaweite Diskussion“ stattfinden. Die EU und speziell Deutschland seien Freunde der Türkei. Trotzdem aber stehe nicht ein Beitritt auf dem Programm, sondern die Entwicklung „sehr spezieller, sehr enger Beziehungen“ zwischen der EU und der Türkei. Die Reformansätze in der Türkei müssten erst noch weiter entwickelt und umgesetzt werden. Im Jahr 2006 verlangte Stoiber von der Bundesregierung und der Europäischen Kommission einen schärferen Kurs gegenüber der Türkei. Er verlangte die EU-Beitrittsgespräche müssten „eingefroren“ werden und die „anhaltenden massiven rechtsstaatlichen Defizite“ in der Türkei und der „eklatante Vertragsbruch gegenüber dem EU-Mitglied Zypern“ müssten konkrete Konsequenzen haben.

Stoiber forderte wiederholt Wiedergutmachung – zum Beispiel von Seiten der Tschechischen Regierung, Beneš-Dekrete – für die Verluste und Leiden der im Zweiten Weltkrieg Vertriebenen. Bei einigen stieß auf Widerspruch, dass er dabei die Frage von Entschädigungszahlungen und Aufhebung der Dekrete mit dem EU-Beitritt von Tschechien im Rahmen der EU-Osterweiterung am 1. Mai 2004 verknüpfte. Bayern ist von der Thematik stark betroffen, weil sich hier ein Großteil der ab 1945 aus dem ehemaligen Sudetenland Vertriebenen niedergelassen hatte, darunter auch Stoibers Ehefrau.

In der Diskussion um die Wehrpflicht hat sich Stoiber mit seinem Eintreten für eine Sicherheitspolitische Dienstpflicht positioniert.

In der Debatte um eine mögliche deutsche Beteiligung an UN-Missionen im Libanon-Konflikt wies Stoiber darauf hin, dass es aufgrund der deutschen Vergangenheit schwer sei, gegenüber Israel eine neutrale, gegebenenfalls aber auch resolute Haltung zu bewahren und deshalb das Risiko, in Kampfeinsätze hineingezogen zu werden, vermieden werden sollte.

In den umstrittenen Fragen um Einwanderung und Zuwanderung nahm Stoiber eine Gegenposition zur rot-grünen Bundesregierung ein und forderte im Verhältnis zum vorgelegten Regierungsentwurf eines Einwanderungsgesetzes eine in Umfang, Ausmaß und Anforderungen enger umrissene Form der Zu- und Einwanderung. Laut Süddeutscher Zeitung vom 4. November 1988 soll Stoiber während eines Gesprächs mit Journalisten vor einer „durchmischten und durchrassten Gesellschaft“ gewarnt haben. Stoiber sprach damals von einem aus dem Zusammenhang gerissenen Zitat und entschuldigte sich später für die Äußerung. In der Debatte um die Flüchtlingskrise in Europa 2015 widersprach Stoiber Angela Merkel indem er erklärte, dass das deutsche Asylrecht zwar juristisch keine Obergrenze vorsehe, aber dies praktisch nicht leisten könne und dass wohl die Muslime, nicht aber der Islam zu Deutschland gehöre.

2006 strahlte der Sender MTV einen Werbespot für die kontrovers diskutierte Zeichentrick-Fernsehserie Popetown aus. Dieser zeigte unter dem Titel „Lachen statt Rumhängen“ einen vom Kreuz gestiegenen lachenden Christus beim Fernsehen. Die Ausstrahlung führte, auch in Zusammenhang mit den zuvor erschienenen Mohammed-Karikaturen und einem Auftritt des Schauspielers Mathieu Carrière, bei dem dieser in einem Jesuskostüm an einem Kreuz hängend für mehr Rechte von Trennungsvätern demonstrierte, zu einer öffentlichen Debatte um die Bedeutung der Gotteslästerung in Deutschland Stoiber forderte in Folge eine Verankerung von härteren Strafen wegen Blasphemie im Strafrecht. Er erklärte, es dürfe nicht alles mit Füßen getreten werden, was anderen heilig ist. Der Paragraf 166 des Strafgesetzbuches sei „völlig stumpf und wirkungslos, weil er eine Bestrafung nur dann vorsieht, wenn der öffentliche Frieden gefährdet ist und Aufruhr droht.“ Wer bewusst auf den religiösen Empfindungen anderer Menschen herumtrampele, müsse mit Konsequenzen rechnen – in schweren Fällen mit bis zu drei Jahren Gefängnis. Stoiber begründete seine Position weiterhin damit, der Streit um die Mohammed-Karikaturen zeige auf alarmierende Weise, wohin die Verletzung religiöser Gefühle führen könne. Seitens der Kirchen wurde die Initiative Stoibers nicht unterstützt. Während die Deutsche Bischofskonferenz erklärte, sie werde sich zu dem Vorschlag nicht äußern, lehnte die Evangelische Kirche in Deutschland (EKD) diesen ausdrücklich ab. Petra Bahr, Kulturbeauftragte der EKD, erklärte in diesem Zusammenhang: „Wir sehen keine Gründe für die Verschärfung des Strafrechts“. Die Rechtsprechung sei bislang sensibel mit blasphemischen Handlungen umgegangen.

Im folgenden Jahr legte Stoibers Justizministerin Beate Merk einen Gesetzesvorschlag zur Verschärfung des § 166 StGB vor. Dieser stellte die Grundlage für eine Bundesratsinitiative zur Änderung des Paragrafen dar. Nach der Vorlage sollte nicht erst eine Beschimpfung von Religion und Kirche, die den öffentlichen Frieden stören könnte, strafbar sein, sondern bereits die Verspottung oder Herabwürdigung. Die Vorlage sah vor, dass der öffentliche Friede zukünftig schon dann gestört werde, wenn der Spott „das Vertrauen der Betroffenen in die Achtung ihrer religiösen oder weltanschaulichen Überzeugung beeinträchtigen oder bei Dritten die Bereitschaft zu Intoleranz“ gegenüber Religion fördern könne. Stoibers Initiative blieb folgenlos. Die CSU hatte bereits 1986, 1995 und 1998 versucht, Blasphemie einfacher und härter unter Strafe zu stellen, die drei ersten Bundesratsinitiativen scheiterten ebenfalls.

Stoiber trat vehement gegen die Einführung der von Sozialdemokraten und Grünen befürworteten gleichgeschlechtlichen eingetragenen Lebenspartnerschaften in Deutschland ein. Dies beeinflusste seinen Bundestagswahlkampf im Jahre 2002 gegen Schröder. Das von Bayern zusammen mit den Bundesländern Sachsen und Baden-Württemberg angestrengte abstrakte Normenkontrollverfahren gegen die eingetragene Lebenspartnerschaft wurde vom Bundesverfassungsgericht in Karlsruhe abgewiesen. Gegen die im Jahr 2005 beschlossene Novelle zum Lebenspartnerschaftsgesetz, die von Sozialdemokraten, Grünen und der FDP getragen wurde und zum 1. Januar 2006 in Kraft trat, strebte Stoiber erneut ein gerichtliches Verfahren gegen die Einführung der Stiefkindadoption durch gleichgeschlechtliche Paare vor dem Bundesverfassungsgericht an, das diesmal allein vom CSU-regierten Bundesland Bayern betrieben wird.







</doc>
<doc id="1284" url="https://de.wikipedia.org/wiki?curid=1284" title="Elektronische Musik">
Elektronische Musik

Elektronische Musik bezeichnet Musik, die durch elektronische Klangerzeuger hergestellt und mit Hilfe von Lautsprechern wiedergegeben wird. Im deutschen Sprachgebrauch war es bis zum Ende der 1940er-Jahre üblich, alle Instrumente, an deren Klangentstehung bzw. -übertragung in irgendeiner Weise elektrischer Strom beteiligt war, als "elektrische Instrumente" zu bezeichnen. Konsequenterweise sprach man daher auch von "elektrischer Musik". Bis heute besteht eine Kontroverse in der Terminologie, da einerseits ein wissenschaftlicher Begriff der Akustik und gleichzeitig aber auch ein Oberbegriff über "neue Musikstile der Unterhaltungsmusik" gemeint ist. Andererseits kategorisiert man mit elektronischer Musik auch eine Gattung der Neuen Musik, wobei sich hier der Begriff der Elektroakustischen Musik etabliert hat.

In der Zeit um 1980 erlebte die elektronische Musik durch die zunehmende Verfügbarkeit und Etablierung synthetischer Klangerzeugungsmöglichkeiten einen rasanten Aufschwung. Insbesondere im Bereich der speziell für die Clubszene produzierten Musik nahmen synthetisch produzierte Songs ab etwa 1980 eine stetig wichtigere Stellung ein und lösten den in den 1970er-Jahren üblichen, vornehmlich akustisch produzierten Disco-Sound sehr schnell ab. Es begann die Phase der elektronischen Tanzmusik, die im Verlauf der 1980er zum Sound der Ära werden sollte und mit Musikstilen wie Synthpop, Euro Disco, House und schließlich Techno nicht nur den Sound der Dekade, sondern auch den der nachfolgenden Jahrzehnte entscheidend prägen sollte. Seit dieser Zeit sind synthetisch produzierte Musikstücke in höchstem Ausmaß populär und haben traditionell akustisch aufgenommene Songs, vor allem im Bereich der Clubmusik, aber auch im Bereich der Popmusik allmählich mehr oder weniger verdrängt.

In der elektronischen Musik begegnen sich zwei gegensätzliche Sphären menschlichen Schaffens: die künstlerisch-ästhetische der Musik und die naturwissenschaftliche der Physik und Elektrotechnik. Daher muss die Entwicklung ihrer Voraussetzungen aus einem ideengeschichtlichen und einem technischen Blickwinkel heraus betrachtet werden. Im Zuge der radikalen musikalischen Veränderungen, die das 20. Jahrhundert zum Jahrhundert der "Neuen Musik" haben werden lassen, spielt die elektronische Musik eine wichtige Rolle. Von grundlegender Bedeutung sind zunächst diejenigen Konzepte, die schon Möglichkeiten der elektronischen Musik voraussetzten, noch bevor diese tatsächlich (technisch) zur Verfügung standen:

Das erste Musikinstrument, das Elektrizität verwendete, war das Clavessin électrique von Jean-Baptiste Delaborde. Das oft genannte "Denis d’or" des tschechischen Erfinders Pater Prokop Diviš aus den frühen 1750er-Jahren war zwar in der Lage, dem Spieler aus Spaß kleine elektrische Schläge zu versetzen, benutzte aber wahrscheinlich keine Elektrizität bei der Klangerzeugung. 1867 konstruierte der Direktor der Telegraphenfabrik Neuchâtel Hipp ein elektromechanisches Klavier. Ein erstes Patent auf dem Gebiet elektronischer Klangerzeugung wurde am 12. März 1885 an E. Lorenz aus Frankfurt am Main erteilt.

Eine ungewöhnliche Erfindung des elektronischen Instrumentenbaus war das von Thaddeus Cahill 1897 entwickelte Teleharmonium oder Dynamophon. Es arbeitete nach dem Prinzip eines Zahnradgenerators, wog 200 Tonnen und war so groß wie ein Güterwaggon. Cahill benutzte für jeden Halbton einen riesigen dampfgetriebenen Mehrfachstromerzeuger, der ihm die sinusförmigen Ausgangsspannungen lieferte. In seinem 1907 erschienenen "Entwurf einer neuen Ästhetik der Tonkunst" entwickelte Ferruccio Busoni seine Theorie der Dritteltöne, wobei er für dessen klangliche Umsetzung das Dynamophon am geeignetsten hielt.

Leon Theremin konstruierte als Leiter des Laboratoriums für elektrische Schwingungen des staatlichen physikalisch-technischen Instituts in Leningrad von 1920 bis 1928 das aufsehenerregende Instrument "Ätherophon", das später nach ihm "Theremin" benannt wurde. Das Instrument war technisch gesehen eine Schwebungssummerkonstruktion, d. h. die Erzeugung eines hörbaren Tones erfolgte durch Überlagerung zweier hochfrequenter und nicht mehr hörbarer Töne. Diese Eigenschaft der Klangerzeugung inspirierte einige Komponisten zu Werken speziell für das Theremin. Vom Komponisten Anis Fuleihan wurde auf diese Weise ein Konzert für Theremin und Orchester geschaffen, das 1945 mit dem New York Symphony Orchestra unter Leopold Stokowski und der Solistin Clara Rockmore am Theremin uraufgeführt wurde.

Etwa zeitgleich beschäftigte sich der deutsche Volksschullehrer und Organist Jörg Mager mit der exakten Erzeugung von Mikrointervallen und stellte Erfindungen wie das "Elektrophon" (1921) und das "Sphärophon" (1928) vor. Mager war ein Anhänger des tschechischen Komponisten Alois Hába, der sich, durch Anregung von Ferruccio Busoni, bereits mit Mikrointervallen praktisch beschäftigte. Zudem leitete Mager sein Interesse an Mikrointervallen von der Beobachtung des Akustikers und Musikethnologen Erich Moritz von Hornbostel ab, dass die Melodie bei einer Veränderung der Tonhöhenlage, aber auch der Notenlänge, stets als ein und dieselbe Gestalt erscheint. So wurden später sein "Sphärophon II", sein "Kaleidosphon" und seine Elektrotonorgel fertiggestellt.

Beim "Ondes Martenot" handelte es sich ebenfalls um einen Tonfrequenz erzeugenden Schwebungssummer mit dem Unterschied, dass zusätzlich an einem Seil gezogen wurde, womit Tonhöhen verändert werden konnten. Olivier Messiaen verwendete dieses Instrument in seiner "Turangalîla-Symphonie", der schweizerische Komponist Arthur Honegger setzte es im Oratorium "Johanna auf dem Scheiterhaufen" ein. Bereits 1907 hatte Busoni in seiner visionären und einflussreichen Schrift "Entwurf einer neuen Ästhetik der Tonkunst" mögliche Entwicklungslinien aufgezeigt, die erst mit den Mitteln der elektronischen Musik ab den 1950er-Jahren realisiert werden konnten. Er griff darin unter anderem die Idee der "Klangfarbenmelodie", die Arnold Schönberg erstmals in seiner "Harmonielehre" (1911) vorstellte und in den folgenden Jahren wiederholt angesprochen hat, von Relevanz für das musikalische Konzept der frühen elektronischen Musik. Weiterhin kann die kompositorische Konzeption Edgar Varèses mit ihrer gleichermaßen von Busoni und den italienischen Futuristen beeinflussten Geräuschhaftigkeit als Vorwegnahme rein elektronischer Möglichkeiten der Klanggestaltung betrachtet werden.

Durch die Bedeutung des Rundfunks als Medium zunächst zur Durchsetzung politischer Ziele und später der Unterhaltung wurde der Weg für Übertragungen von Musik geebnet.

In dieselbe Zeit fällt die Entwicklung des Trautoniums durch Friedrich Trautwein im Jahre 1930, das später durch Oskar Sala weiterentwickelt wurde. Aus diesem Jahr stammen auch die ersten Trautoniumstücke von Paul Hindemith: Sieben Stücke für drei Trautonien mit dem Untertitel "Des kleinen Elektromusikers Lieblinge".

Im Jahr 1935 konkurrierten die "Hammond-Orgel" und die "Lichttonorgel", wobei Erstere die Oberhand gewann.

Die Geschichte der elektronischen Musik ist eng an die Geschichte der elektronischen Klangerzeugung (Instrumente, Apparate) gekoppelt. Im Allgemeinen spricht man bis ca. 1940 von der "elektrischen Musik" und von "elektrischen Musikinstrumenten". Ab Anfang der 1950er-Jahre wurde eine bestimmte, mit elektronischen Geräten realisierte Kompositionstechnik "elektronische Musik" genannt.

1943 rief der Ingenieur Pierre Schaeffer eine Forschungsstelle für radiophone Kunst in Paris, den "Club d’Essai", ins Leben, der bald Künstler wie Pierre Henry, Pierre Boulez, Jean Barraqué, Olivier Messiaen und Anfang der 1950er-Jahre dann Karlheinz Stockhausen anzog. Am 5. Oktober 1948 gingen beim Pariser Rundfunk Schaeffers "Cinq études de bruits" in einem als "Concert des Bruits" betitelten Radioprogramm über den Äther und markieren damit die Geburtsstunde der Musique concrète. Am 18. März 1950 fand dann das erste öffentliche Konzert konkreter Musik in der École Normale de Musique statt. Da in der Anfangszeit des „Club d’Essai“ außerhalb Deutschlands noch keine Tonbandmaschinen zur Verfügung standen, wurden die Geräusche auf Schallplatten festgehalten und in einem Arbeitsgang aus bis zu acht Schallplatten gleichzeitig abgemischt. Bei der Bearbeitung dieser Klänge, die einfache Alltagsgeräusche waren, handelte es sich um deren Transformation und collagenartige Kombination. Ästhetisch erweist sich die frühe Musique concrète damit als Vorstufe zum Hörspiel und der radiophonen Collage. Der Terminus „Konkrete Musik“, den Schaeffer 1949 vorschlug, trägt zum einen der Verwendung vorgefundener Geräusche – sogenannter „Klangobjekte“ – Rechnung, sollte aber auch als Abgrenzung gegenüber der komponierten und damit „abstrakten“ Musik (Serialismus) verstanden werden. Mit diesem radikalen (bruitistischen) Ansatz sorgte Schaeffer auch im eigenen Lager für einige Irritation. In den 1950er-Jahren erlaubte die Tonaufnahme auf Magnetband auch in Paris die Einführung weiterer Bearbeitungstechniken wie Schnittmöglichkeiten, Geschwindigkeitstransformationen und damit Tonhöhenveränderungen. Durch diese Möglichkeiten entstand zu dieser Zeit das "Phonogen", eine Art Mellotron mit Klangtransponiermöglichkeit, und das "Morphophon", vergleichbar einem Bandschleifen-Verzögerungsgerät.

Im Bewusstsein der interessierten Öffentlichkeit befand sich die Musique concrète damit in direkter Rivalität zur gleichzeitig in Erscheinung tretenden „elektronischen Musik“ aus Köln. Anfang der 1950er-Jahre wurde die Arbeit Schaeffers und seines Mitarbeiters Pierre Henry in eine Art ideologischen und zum Teil auch chauvinistisch motivierten Streit verwickelt. Eine debakulöse Aufführung ihrer Gemeinschaftskomposition "Orphée 53", die anlässlich der Donaueschinger Musiktage am 10. Oktober 1953 stattfand, besiegelte ihre „Niederlage“ und schadete dem internationalen Ansehen der Musique concrète auf Jahre hinaus. Die Komponisten, die Anfang der 1950er-Jahre der "Groupe de Recherches de Musique concrète" (die 1951 aus dem "Club d’Essai" hervorgegangen war) nahestanden, haben durchaus versucht, in die Musique concrète kompositorische Ordnungsprinzipien einzuführen, konnten sich aber zunächst nicht gegen die Geräuschkonzeption Schaeffers durchsetzen. 1954 realisierte Edgar Varèse als Gast die Tonbänder für seine Komposition "Déserts". Erst ab 1956/57 entstanden Arbeiten von Luc Ferrari, Iannis Xenakis, François Bayle und anderen, die in viel stärkerem Maße kompositorische Gesichtspunkte und später sogar serielle Prinzipien in den Vordergrund stellten. Folgerichtig gab Schaeffer den Begriff „Musique concrète“ nun zu Gunsten von „elektroakustischer Musik“ auf und benannte auch seine "Groupe de Recherches de Musique concrète" 1958 in "Groupe de Recherches Musicales" um.

Als Werner Meyer-Eppler für eine bestimmte Art des Komponierens mit technischen Hilfsmitteln den Terminus „elektronische Musik“ vorschlug, ging es ihm dabei vor allem um eine Abgrenzung gegenüber den bisherigen Entwicklungen der elektrischen Klangerzeugung, der elektrischen Musik, zu der er auch die "Musique concrète" und die "Music for Tape" (s. u.) zählte.

Der Physiker Werner Meyer-Eppler, der Tonmeister Robert Beyer, der Techniker Fritz Enkel und der Komponist Herbert Eimert gründeten 1951 mit Hilfe des NWDR das Kölner Studio für Elektronische Musik. Das erste öffentliche Konzert fand dann am 26. Mai 1953 auf dem Kölner „Neuen Musikfest 1953“ statt. Im Unterschied zur Musique concrète wurde hier versucht, elektronisch erzeugte Töne nach physikalischen Regeln wie der Fourier-Analyse wissenschaftlich zu erfassen. Die Klangfarbe, als Resultat der Überlagerung mehrerer Sinustöne, und die Parameter Frequenz, Amplitude und Dauer wurden dabei ausführlich analysiert.

Zunächst ging es Eimert und Beyer (nur) um die differenzierte Gestaltung von Klangfarben. Erst eine zweite Generation junger Komponisten, unter ihnen Henri Pousseur, Karel Goeyvaerts und Karlheinz Stockhausen, arbeitete dann ab 1953 vor allem an der konsequenten Durchführung serieller Kompositionsmethoden mit elektronischen Mitteln. Signifikant für diese frühe musikalische Konzeption des Kölner Studios ist die ausschließliche Verwendung „synthetisch“ hergestellter Klänge sowie deren direkte Verarbeitung und Speicherung auf Magnettonband und schließlich die Wiedergabe über Lautsprecher. Dadurch wurden (zumindest theoretisch) zwei musikhistorisch revolutionäre Dinge erreicht: zunächst die vollständige Kontrolle über den Parameter Klangfarbe, der bisher für die Komponisten immer unwägbar geblieben war und nun ebenfalls der seriellen Organisationsmethode unterworfen werden konnte. Zweitens wurde der Interpret als vermittelnde – und damit die kompositorische Absicht potentiell verfälschende – Instanz ausgeschaltet. Zum ersten Mal in der Geschichte der abendländischen Musik schien es den Komponisten mit Werken wie Stockhausens "Studie II" möglich, ihre Ideen „unvermittelt“ an den Hörer weiterzugeben. Die jahrhundertealten Versuche, die musikalische Absicht immer präziser durch Notenschrift zu fixieren, waren damit überholt.

Da die klanglichen Ergebnisse dieser frühen Arbeiten aber deutlich hinter den in sie gesetzten Erwartungen zurückblieben, beschritt man in der Technik der Klangsynthese neue Wege und verließ bereits 1954 das ursprüngliche Sinuston-Konzept wieder. Mit wachsender Komplexität des Herstellungsprozesses nahm nun einerseits die Klangqualität ab und andererseits entzogen sich die Klangkomponenten auch zunehmend der Kontrolle durch die Komponisten. Eine erste Konsequenz daraus zog Stockhausen in seiner Komposition "Gesang der Jünglinge" (1955/56), die konzeptuell zwischen elektronischen Klängen und Phonemen vermittelt und statistische Ordnungsprinzipien (Aleatorik) durch im Raum verteilte Lautsprechergruppen zur Anwendung brachte.

Die Idee der klanglichen Vermittlung zwischen heterogenen Ausgangsmaterialien führt dann konsequent zum Entwurf der Live-Elektronik und auch zur Transformation von Klängen beliebiger Herkunft, womit die Entwicklung der elektronischen Musik Kölner Ausprägung ihre größte Annäherung zum einstigen „Erbfeind Musique concète“ vollzogen hat. Das Kölner Studio war nicht der einzige Ort, an dem Techniker und Musiker an der Entstehung der Elektronischen Musik zusammenarbeiteten. Einflussreich waren das Siemens-Studio für elektronische Musik ab 1956 in München unter der künstlerischen Leitung von Orff-Schüler Josef Anton Riedl und das Columbia-Princeton Electronic Music Center in New York. Bereits ein Jahr zuvor, am 1. März 1955, wurde das Studio für Elektronische Komposition Darmstadt eingeweiht, mit dessen Leitung der Komponist Hermann Heiß beauftragt wurde. 1957 privatisierte Hermann Heiß das Studio unter dem Namen "Studio für Elektronische Komposition Hermann Heiß Darmstadt". 1977 kam das IRCAM im Pariser Centre Pompidou von Pierre Boulez hinzu. Das Elektronische Studio Basel und das Studio für Elektronische Musik in Dresden wurden erst in den 1980er-Jahren eingerichtet. Weitere Studios für elektronische Musik standen oder stehen in Mailand, Stockholm und Utrecht.

Im sogenannten "Tape Music Studio" der Columbia-Princeton Universität in New York unterrichteten Vladimir Ussachevski und Otto Luening Studenten in einer speziellen Art des Umgangs mit auf Tonband aufgezeichneten Klängen. Sie gingen davon aus, dass die große Bandbreite möglicher elektronischer Manipulation die Herkunft des Klanges mehr und mehr in den Hintergrund treten lässt. Erste bekannt gewordene Studien der "Music for Tape" stammen vom New Yorker Ehepaar Louis und Bebe Barron, die sich seit 1948 in ihrem eigenen professionellen Aufnahmestudio mit erweiterten Möglichkeiten des Tonbandes zur Musikproduktion beschäftigten. Im Studio der Barrons realisierte John Cage 1951 das "Project of Music for Magnetic Tape", gemeinsam mit den Komponisten Earle Brown, Morton Feldman, David Tudor, und Christian Wolff.

Bei der "Music for Tape" war vor allem die Vielseitigkeit bei Auswahl und Bearbeitung von Klangquellen für die musikalische Umsetzung von Bedeutung. In Amerika wurde die Unterscheidung in kontrollierbare (elektronische) und „unkontrollierbare“ (mechanische) Klänge als nicht sinnvoll betrachtet.

Ein weiterer bedeutender Vorreiter der elektronischen Musik in den USA war der unabhängig von den im Aufbau begriffenen Hochschulstudios wirkende Richard Maxfield.

Der kanadische Physiker Hugh Le Caine machte entscheidende Experimente in der Anschlagsdynamik eines Keyboards zwischen 1945 und 1948. Bei dem von ihm erfundenen "Sackbut" konnte der Spieler sogar durch wechselnden seitlichen Druck der Taste subtile Veränderungen der Tonhöhe, Lautstärke und Klangfarbe ermöglichen und zusätzlich expressive Merkmale wie Vibrato, Intensität und Einschwingvorgänge kontrollieren. 1955 erfand er den "Special Purpose Tape Recorder", bei dem es sich um eine Synthese aus Mehrkanal-Bandmaschine und Mellotron handelt, mit der sich bei der Arbeit mit konkreten Klängen ungeahnte Möglichkeiten ergaben.
Das 1955 von Le Caine komponierte Stück "Dripsody" ist nur etwas über eine Minute lang und besteht aus dem mit dem "Recorder" aufgenommenen Geräusch eines Wassertropfens, welches vielfach kopiert und mit unterschiedlichen Geschwindigkeiten in einer pentatonischen Skala angeordnet wurde, woraus sich unterschiedliche Tonhöhen ergaben. Beginnend mit dem Originaltropfen, steigern sich Intensität und Dichte durch weitere Bandschleifen zu einem Climax, bis hin zu zwölftönigen Arpeggien, die alle aus dem Klangmaterial des Tropfens abgeleitet sind.

Lejaren Hiller gründete in der University of Illinois at Urbana-Champaign 1958 das zweite amerikanische Studio für elektronische Musik, das "Experimental Music Studio". Er experimentierte dort neben anderen Forschern mit dem "ILLIAC-Rechner" und später dem "IBM 7090-Rechner".

Neben der Verwendung in studiotechnischen Geräten lassen sich heute drei große musikalische Anwendungsbereiche für Computer ausmachen, die mit den Stichworten Komposition (Partitursynthese), Klangerzeugung (durch Simulation) und Klangsteuerung umrissen werden.

Beim „Grand Price Of Ars Electronica“ wurde 1979 der von Kim Ryrie und Peter Vogel in Australien entwickelte Fairlight Musikcomputer erstmals einem größeren internationalen Publikum vorgeführt. Diese aufwändige (8-Bit-)Rechenmaschine brachte als wesentliche Neuerung die Sampling-Methode hervor: Sie ermöglichte es erstmals alle Klänge unserer Welt in einen Computer sowohl zu speichern als auch sie mittels der Tastatur jederzeit nicht nur einfach abrufen zu können, sondern sie auf jede gewünschte Tonhöhe bringen zu können und überdies formbar zu machen.

Dies öffnete Komponisten und Produzenten völlig neue musikalische und konzeptionelle Dimensionen. Im Januar 1982 beispielsweise erschien auf einem eigens für solche Art von Musik von Ulrich Rützel in Hamburg gegründeten Label und -Verlag das Album „Erdenklang Computerakustische Klangsinfonie“. Es war der erste verfügbare Tonträger mit dieser neuen Produktionstechnologie. In ihren Linernotes zu diesem Album vermerkte Wendy Carlos: „Erdenklang darf nicht mehr ausschließlich als technische, sondern muss weitgehend als musikalische Errungenschaft betrachtet werden. Etwas, worum die elektronische Musik, seit es sie gibt, kämpft.“

Hubert Bognermayr und Ulrich Rützel führten für diese Musik-Gattung den Begriff Computerakustische Musik ein. Die 1983 erschienene „Bergpredigt – Oratorium für Musikcomputer und Stimmen“ verfestigte diese musikhistorische Entwicklung und stellt bis heute einen Meilenstein in der Computermusik dar.

Bei dem am 25. April 1987 vom WDR veranstalteten Konzert „Million Bits In Concert“ mit den Elektronik-Musikern Hubert Bognermayr, Harald Zuschrader, Johannes Schmoelling, Kristian Schultze und Matthias Thurow kamen erstmals verschiedene Computersysteme (wie z. B. der Fairlight) auch in einem Livekonzert zum Einsatz. Mike Oldfield ließ sich von Bognermayr und Zuschrader in diese Technologie einführen und ging mit dem Fairlight und Harald Zuschrader auf Tournee.

Vereinzelt wird Computermusik inzwischen auch für technisch gesteuerte Theater- und Freiluft-Inszenierungen verwendet, z. B. als Schaltmusik für Feuerwerke.


In den 1970er-Jahren entstanden im Kontext von Rockmusik der Progressive Rock und Psychedelic Rock, die durch einen prägnanten Einsatz von elektronischen Tasteninstrumenten zum Teil Elemente elektronischer Musik verarbeiten. Durch den Einfluss von Instrumenten der Computermusik entstanden Synthesizer und Sequencer neben Soundmodulen. Besonders der Synthesizer wurde zum prägenden Instrument der Popmusik. Wendy „Walter“ Carlos, die an der Columbia-Universität Kompositionslehre studierte, war eine der ersten, die sich für den Moog-Synthesizer interessierten, und beriet seit 1964 Robert Moog bei seiner Herstellung. Keith Emerson verwendete den Moog-Synthesizer ebenfalls oft, der durch seine virtuose Spielart stilbildend auf jüngere Musiker wirkte. Die neue Möglichkeit, beliebig lang anhaltende Töne langsamen klanglichen Veränderungen zu unterwerfen, zeigte eine starke Affinität zur „zerfliessenden Formlosigkeit“ des Psychedelic Rock (The United States of America, Silver Apples und Fifty Foot Hose). In den 1970er-Jahren entstand in Deutschland die sogenannte Berliner Schule, die später den Krautrock beeinflusste.
Bis in die 1980er-Jahre hinein entstanden nebeneinander zahlreiche Musikgenres, die elektronisch erzeugte Musik als ästhetisches Mittel verwendeten; aus New Wave wurde Electro Wave, aus Funk wurde Electro Funk und später Hip-Hop, aus Disco wurde House.

Im Bereich der Synthesizer-orientierten Musik hatten großen Einfluss auf viele spätere Musiker die Gruppen wie Kraftwerk, Depeche Mode und Suicide, die für kommende Stile wie EBM, Elektropop, Hip-Hop und Techno eine Art Pionierarbeit leisteten.

Das Sampling im "Techno" wurde durch mehrere Genres (Funk, Electro Funk, New Wave, Electronic Body Music) Ende der 1980er-Jahre geschaffen. Ferner liegen Einflüsse in der Perkussionbetonung der "Afroamerikanischen und Afrikanischen Musik".

Der Schwerpunkt liegt im elektronisch erzeugten Schlagzeug-Rhythmus durch Drumcomputer. Durch Sampling werden Loops erzeugt, wodurch ein Repetitives Arrangement als charakteristisches Klangbild entsteht.

Ende der 1990er-Jahre wurden Elemente der elektronischen Musik in die bis dahin oft als konservativ angesehenen Genres des klassischen Rock und Folk übernommen. Bands wie Radiohead oder Tortoise, aber auch Stereolab verarbeiteten elektronische Elemente in Strukturen des klassischen Songwritings und trugen zu einer Neuetablierung elektronischer (Tanz-)Musik außerhalb der Techno-Szene bei.

Seit 2014 gibt es auch auf dem Electric Love Festival in Salzburg eine eigene Hardstyle-Stage.


Rockmusik wird im allgemeinen Sprachgebrauch nicht zur elektronischen Musik gerechnet, obwohl auch dort elektronische Instrumente und besonders elektronische Effektgeräte eingesetzt werden. Bei Elektrogitarren sind zwar die klangverändernden Wirkungen von Verstärker und Effektgeräten essenziell, trotzdem werden sie nicht zu den Elektrophonen gezählt. Im Psychedelic Rock (z. B. Led Zeppelin oder Deep Purple) kommen auch „echte“ Elektrophone (z. B. Theremin oder Hammond-Orgel) vor, aber auch er wird von der elektronischen Musik abgegrenzt. Im Metal spielen – je nach Substil – analoge Effektgeräte eine bedeutende Rolle, aber Musiker und Szenemitglieder haben oft klare Vorstellungen, welche Geräte verboten sind, um nicht zur elektronischen Musik zu gehören. Digitale Effektgeräte oder digitale Produktion gelten in der ganzen Musikrichtung als Tabu (wobei Keyboards mit analogem Ausgang toleriert werden), und es besteht bei Gitarrenverstärkern eine Ablehnung von Halbleitern.




</doc>
<doc id="1286" url="https://de.wikipedia.org/wiki?curid=1286" title="Einzugsbereich">
Einzugsbereich

Als Einzugsgebiet oder Einzugsbereich wird ein geographischer Raum bezeichnet, der einen infrastrukturellen Einflussbereich eines meist zentralen Objektes, einer Einrichtung oder einer Struktur darstellt. Dies erfordert geeignete Verbindungswege und deren natürliche Voraussetzungen. Als Beispiele seien genannt das Einzugsgebiet einer Stadt, einer Schule oder eines Unternehmens. 

Der Terminus wird hauptsächlich verwendet in der Wirtschaftsgeografie, Sozialgeographie, Verkehrsgeographie und in verwandten Fachdisziplinen sowie in der Raumplanung.

Das Einzugsgebiet einer Stadt und ihrer öffentlichen Einrichtungen wurde von Walter Christaller in seiner Theorie der zentralen Orte (1930er) auch als "Ergänzungsgebiet" bezeichnet. Im Ergänzungsgebiet wohnen die Nachfrager der Güter (insbesondere Dienstleistungen), die in dem zentralen Ort angeboten werden. Die Nachfrager im Ergänzungsgebiet sind in ihrem Kundenverhalten auf den zentralen Ort hin orientiert bzw. werden von ihm versorgt. Streng genommen versteht man in dieser Theorie unter einem "zentralen Ort" jedwede Form von Angebotsstandort, in der Praxis wird hierunter allerdings meist eine Gebietskörperschaft verstanden. Dies gilt insbesondere für das System der zentralen Orte in der Raumordnung. Weitere Namen solcher Gebiete von Siedlungen in ihrer funktionalen Vielfalt sind "Verflechtungsbereich", "Marktgebiet" und (auf die Infrastruktur bezogen) "Versorgungsgebiet". Vereinzelt haben oder hatten Institutionen wie niedergelassene Ärzte, Apotheken, oder Rauchfangkehrer und andere Infrastruktur von öffentlichem Interesse auch einen rechtlich garantierten Gebietsschutz.

In einem ähnlichen Sinne sind die Einzugsgebiete von beispielsweise Schulen und von Unternehmen (insbesondere Einzelhandels­betriebe und andere Anbieter haushaltsorientierter Dienstleistungen) zu verstehen, doch kann ihr geografischer Zusammenhang stärker gelockert sein. Denn Ausbildungs- oder Produktionsbetriebe haben zwar meist ihren regionalen Markt, sprechen jedoch auch weiter entfernte Kunden an. Die Planung der Einzugsbereiche von Schulen ergibt die Schulsprengel, in denen Pflicht zum Besuch einer gewissen öffentlichen Schule besteht. Je spezieller eine Schule oder eine Firma ausgerichtet ist, desto größer ist im Regelfall ihr Einzugsgebiet. Ähnliches gilt für Krankenhäuser, kulturelle Angebote (Theater, Museen, usf.), aber auch Freizeit-Einrichtungen (Vergnügungsparks, Tourismusregionen, usf.).

Einzugsgebiete sind Konstrukte; sie unterliegen großen räumlich-zeitlichen Variationen und können nur anhand von theoretischen Überlegungen exakt abgegrenzt und segmentiert werden (z. B. mit dem Huff-Modell),<ref name="Heinritz/Klein/Popp 2003"></ref> oder aber regulierenden politischen Eingriffen (Grenzziehungen) absichtlich hergestellt.

Die Planung solcher Versorgungs- und Interessensgebiete obliegt im öffentlichen Bereich – etwa für Schulen, Ämter und Apotheken – der Raumordnung. In der privaten Wirtschaft ist die Planung Sache der Marktforschung und der Unternehmens-Strategie. 


</doc>
<doc id="1287" url="https://de.wikipedia.org/wiki?curid=1287" title="Liste von Erfindern">
Liste von Erfindern

Dies ist eine Liste von Erfindern, die die Welt mit ihren Erfindungen bereichert haben.

Ein Erfinder ist jemand, der ein Problem erkannt hat, es gelöst und mindestens einmal damit Erfolg gehabt hat. Er muss nicht der erste gewesen sein; eine Erfindung kann auch mehrmals gemacht werden oder durch ständige Verbesserungen in mehreren Schritten entstanden sein.
Eine Erfindung ist meistens ein technisches Gerät oder eine Maschine, kann aber auch ein Verfahren, eine Methode oder eine Dienstleistung sein. Auch eine nicht materielle Idee, selbst wenn sie erst später von einem anderen ausgeführt wurde, kann als Erfindung zählen. Außerdem gibt es Erfindungen, die keinen Nutzen hatten oder die später wieder verworfen wurden.
































</doc>
<doc id="1289" url="https://de.wikipedia.org/wiki?curid=1289" title="Mehlbeeren">
Mehlbeeren

Die Mehlbeeren ("Sorbus"), auch Vogelbeeren, Ebereschen oder Elsbeeren genannt, sind eine Pflanzengattung der Kernobstgewächse (Pyrinae) innerhalb der Familie der Rosengewächse (Rosaceae). Die etwa 100 Arten sind in den gemäßigten Gebieten der Nordhalbkugel verbreitet. Alle Arten tragen im Herbst auffällige Früchte. Einige Arten wie etwa die Japan- oder die Kaschmir-Eberesche werden wegen ihrer auffälligen Herbstfärbung in Gartenanlagen und Parks gepflanzt. Zur Gattung gehören auch der Vogelbeerbaum, dessen Früchte zu Schnaps und Marmelade verarbeitet werden, sowie der Speierling, der in der Apfelweinherstellung eine Rolle spielt. Senikov & Kurto haben im Jahr 2017 die europäischen Mehlbeeren in mehrere verschiedene neue Gattungen aufgeteilt. Der Gattungsname "Sorbus" verbleibt danach nur noch bei der eigentlichen Vogelbeere ("Sorbus aucuparia" ).

"Sorbus"-Arten wachsen als meist sommergrüne Bäume und Sträucher. Die Bäume sind gelegentlich mehrstämmig und haben häufig eine weit ausladende Krone. Einige Arten erreichen eine Wuchshöhe zwischen 25 und 30 Metern. Zu den groß werdenden Arten zählt beispielsweise "Sorbus pohuashanensis", eine bis zu 20 Meter hoch werdende Art, die in den Bergregionen Nordchinas zu finden ist. Die meisten Arten bleiben deutlich niedriger. Die in Mitteleuropa heimische Zwerg-Mehlbeere erreicht eine Wuchshöhe von etwa 3 Metern. Die in Westchina beheimatete "Sorbus reducta" wird sogar nur 1,5 Meter hoch und bildet durch ihre zahlreichen Ausläufer dichte Gestrüppe.

Die meist relativ großen Winterknospen sind eiförmig, konisch oder spindelförmig und manchmal klebrig mit einigen sich dachziegelartig überdeckenden Knospenschuppen, die kahl oder flaumig behaart sind.

Die wechselständig an den Zweigen angeordneten Laubblätter sind in Blattstiel und Blattspreite gegliedert. Die Blattspreiten sind einfach oder gefiedert. Die Blattränder sind oft gesägt, selten fast ganzrandig. Die Blattflächen sind kahl oder flaumig behaart. Es liegt Fiedernervatur vor. Die Nebenblätter fallen meist früh ab.

Die Blütezeit liegt je nach Art im Frühjahr bis Sommer. Die endständigen, meist zusammengesetzten, selten einfachen schirmtraubigen oder rispigen Blütenstände enthalten meist viele Blüten.

Die zwittrigen Blüten sind radiärsymmetrisch und fünfzählig mit doppelter Blütenhülle. Der Blütenbecher (Hypanthium) ist glockenförmig, selten verkehrt-konisch oder krugförmig. Die fünf meist grünen Kelchblätter sind eiförmig oder dreieckig und kahl bis flaumig oder wollig behaart; manchmal befinden sich Drüsenhaare entlang der Ränder. Die fünf freien gelblich-weißen bis weiß-rosafarbenen Kronblätter können genagelt sein und sind kahl bis flaumig behaart. Die meist 15 bis 25, selten bis 44 Staubblätter stehen in zwei oder drei Kreisen und sind ungleich lang. Die Staubbeutel sind eiförmig oder fast kugelig. Die zwei bis fünf unterständigen bis halbunterständigen Fruchtblätter sind teilweise oder vollständig mit dem Blütenbecher verwachsen. Jedes Fruchtblatt enthält zwei oder drei, selten vier aufrechte, anatrope Samenanlagen, von denen meist eine verkümmert. Es sind zwei bis fünf freie oder teilweise verwachsene Griffel, die kahl oder flaumig behaart sind, vorhanden.

Die Sammelbalgfrüchte der Mehlbeeren sind Apfelfrüchte und erinnern an kleine Äpfel, was aufgrund der botanischen Einordnung in die Kernobstgewächse (Pyrinae) nicht überrascht. Ähnlich wie beim Apfel sitzen oben an der Frucht die fünf Kelchblätter, die haltbar sind oder abfallen und eine ringförmige Narbe hinterlassen. Die Apfelfrüchte färben sich je nach Art bei Reife weiß, gelb, rosafarben, braun oder orange bis rot. Die meist relativ kleinen Apfelfrüchte sind eiförmig oder kugelförmig bis ellipsoid oder länglich. Die Fruchtschale ist kahl oder flaumig behaart und es können kleine Lentizellen vorhanden sein. Das pergamentartige Kerngehäuse besteht aus meist zwei bis fünf, selten bis zu sieben Kammern, die jeweils ein oder zwei Samen enthalten.

In den Samen umgibt ein dünnes Perisperm und Endosperm den Embryo mit seinen zwei zusammengepreßten Keimblättern (Kotyledonen).

Sämlinge und junge Bäume der "Sorbus"-Arten werden von Hirschen, Rehen, Hasen, Kaninchen, Wühlmäusen und Mäusen geschädigt. Mäuse und Wühlmäuse benagen unter anderem die Rinde der Bäume und können dadurch junge Pflanzen zum Absterben bringen.

Die Larven des Gefurchten Dickmaulrüsslers fressen die Rinden der Wurzeln und können im Extremfall eine Pflanze so schädigen, dass sie abstirbt. Die Wahrscheinlichkeit, dass eine Pflanze von diesen Larven befallen wird, steigt mit dem Humusgehalt des Bodens. In normaler Erde ist der zu erwartende Schädlingsbefall gering.

Die auffälligen Früchte der Mehlbeeren werden von einer großen Anzahl von Vogelarten gefressen. Der Samen der Früchte passiert in der Regel unbeschädigt den Darmkanal der Vögel. Zu den fruchtfressenden Arten zählen Amseln, Drosseln wie etwa Rotdrossel, Rotkehlchen und Wacholderdrossel sowie Stare und Seidenschwänze.

Die Geschwindigkeit, mit der der Fruchtbehang eines Baumes von Vögeln geplündert wird, scheint abhängig vom umgebenden Bewuchs zu sein. In Vorstädten werden nach den Untersuchungen von McAllister reifende "Sorbus"-Bäume innerhalb von nur ein oder zwei Wochen ihrer Früchte beraubt. McAllister führt dies auf einen höheren Bestand an immergrünen Pflanzen zurück, die den Vögeln Deckung bieten. "Sorbus"-Bäume, die vereinzelt in einem Feld oder auf einer Rasenfläche stehen, behalten ihre Früchte dagegen über Monate. Auch die "Sorbus"-Art hat einen Einfluss auf die Geschwindigkeit, mit der Vögel die Früchte fressen. Von Vögeln besonders geschätzt sind die Arten "Sorbus decora", "Sorbus cascadensis", "Sorbus sitchensis" und "Sorbus matsumurana". Diese früh reifenden Arten werden von Vögeln bereits gefressen, bevor sie vollreif sind.

Die etwa 100 Arten der Gattung "Sorbus" besitzen ein weites natürliches Verbreitungsgebiet auf der Nordhalbkugel und kommen hauptsächlich in Eurasien und im nördlichen Nordamerika vor. Einige wenige Arten kommen auch in Nordafrika sowie auf Madeira vor. In China kommen 67 Arten vor, 43 davon nur dort.
Typisch für die meisten Arten dieser Gattung ist ihre Anpassungsfähigkeit an extreme Standorte. Bäume und Sträucher dieser Gattung findet man beispielsweise in exponierten Felsenwänden, wo sie in nur wenig Erde führenden Spalten wurzeln. Gelegentlich sieht man junge Ebereschen, die in Dachrinnen heranwachsen. Die Ansprüche der "Sorbus"-Arten an den Boden sind gering und sie sind in der Lage, auf nährstoffarmen und sauren Böden zu gedeihen. Die meisten Arten reagieren auf nährstoffreiche und humose Böden mit erhöhtem Wachstum. Zu den kalkfliehenden Arten zählen "Sorbus cracilis" und "Sorbus poteriifolia", die Chlorose entwickeln, wenn sie auf kalkhaltigen Böden stehen.

Viele "Sorbus"-Arten reagieren empfindlich auf Trockenheit und werfen nach längeren Trockenperioden frühzeitig im Jahr ihre Blätter ab. Als besonders trockenheitsempfindlich gelten Arten, die im Himalaya beheimatet sind. Trockene Standorte haben auch einen Einfluss auf das maximale Höhenwachstum eines Baumes. Speierlinge erreichen beispielsweise an optimalen Standorten in Bayern und Baden-Württemberg Wuchshöhen von über 30 Metern. An niederschlagsärmeren Standorten auf dem Balkan und Frankreich werden diese Bäume selten höher als 25 Meter. Im Gebirge kommen "Sorbus"-Arten regelmäßig bis zur Baumgrenze vor. "Sorbus"-Arten zählen auch zu den Baumarten, die in der Waldtundra gedeihen.

Reine Bestände mit "Sorbus"-Arten sind selten. "Sorbus"-Arten sind tendenziell lichthungrige Pionierpflanzen, die an Waldrändern und Lichtungen gedeihen, wo die Konkurrenzsituation mit anderen Baumarten geringer ist. Da der Samen von "Sorbus"-Arten regelmäßig durch Vögel verbreitet wird, findet man in Unterholz von Wäldern eine große Anzahl von "Sorbus"-Sämlingen. Fallen konkurrierende Bäume einem Feuer oder Sturm zu Opfer, werden die entstehenden Lücken schnell durch "Sorbus"-Arten geschlossen.

Es gibt eine Vielzahl von Bastarden zwischen den Arten innerhalb der Gattung "Sorbus". Meistens entstehen aus diesen Hybridisierungsvorgängen nur kleinräumig verbreitete Arten, die sich apomiktisch vermehren, indem sie Samen auf ungeschlechtlichem Weg mittels Agamospermie bilden. Nachkommen aus diesen Samen sind genetisch identisch mit dem Pflanzenexemplar, von der der Samen abstammt.

Beispielsweise umfasst der Komplex der Bastard-Mehlbeeren ("Sorbus latifolia" agg.) apomiktische Arten, die aus einer Hybridisierung der Elsbeere ("Sorbus torminalis") und Arten der "Sorbus aria"-Gruppe hervorgegangen sind. Ein weiterer Bastard-Komplex ist aus der Vogelbeere ("Sorbus aucuparia") und der Felsen-Mehlbeere ("Sorbus rupicola") entstanden. Hierzu gehören einige Endemiten aus England wie die Art "Sorbus leyana" oder aus Deutschland mit Sorbus lonetalensis, die mit nur noch 11 bis 16 Exemplaren zu den seltensten Bäumen der Welt zählen.

Die Gattung "Sorbus" wurde durch Carl von Linné aufgestellt. Synonyme für "Sorbus" sind: "Aria" , "Ariosorbus" , "Chamaemespilus" , "Hahnia" , "Micromeles" , "Torminalis" , "Cormus" .

Die Gattung "Sorbus" gehört zur Subtribus der Kernobstgewächse (Pyrinae) in der Unterfamilie Spiraeoideae innerhalb der Familie Rosaceae.

Nach Hugh McAllister 2005 wird die Gattung "Sorbus" in sieben Untergattungen und diese in Sektionen gegliedert:

Zu den etwa 100 Arten der Gattung "Sorbus" zählen beispielsweise:

Mehlbeeren haben längst nicht die ökonomische Bedeutung wie andere Kernobstgewächse. Eine ökonomische Bedeutung haben vor allem die Eberesche und der Speierling.

Die Früchte der Eberesche werden sowohl in der Wildkräuterküche wie in der Pflanzenheilkunde verwendet. Die Früchte enthalten Sorbinsäure und zwischen 0,02 und 0,30 Prozent Parasorbinsäure. Nach der Entbitterung werden sie meist zu Marmelade, Kompott oder Gelee gekocht. Ebenso werden aus ihnen Schnaps, Sirup und Tee hergestellt. Die Volksheilkunde setzt Vogelbeeren bei Rheuma, Verstopfung und bei Blutungen sowie Absude der Früchte als Gurgelmittel bei Heiserkeit ein. Früher wurden die Früchte auch zur Sorbitgewinnung verwendet. Sorbit, auch "Sorbitol" genannt, ist ein Zuckeralkohol, der als Zuckerzusatz in Diabetikerpräparaten Verwendung findet.

Die Früchte des Speierlings werden aufgrund ihres Tanningehalts als klärender Zusatz zu Apfelwein verwendet.

Das Holz der Elsbeere, teils auch das ähnliche Holz des Speierlings gilt als sehr wertvoll ("Schweizer Birnbaum") und wird v.  a. als Furnierholz und in der Möbelindustrie verwendet. Früher wurden aus dem formstabilen Holz auch Lineale und Rechenschieber gefertigt.

Alle Mehlbeeren, insbesondere Speierling, Elsbeere, Vogelbeere/Eberesche, Echte Mehlbeere sind als Kernobstgewächse stark durch Befall mit Feuerbrand gefährdet und zählen mit zu den Hauptwirtsgruppen.

Im Jahr 2011 wurde die Elsbeere ("Sorbus torminalis") in Deutschland zum Baum des Jahres 2011 ernannt.




</doc>
<doc id="1290" url="https://de.wikipedia.org/wiki?curid=1290" title="Ethik">
Ethik

Die Ethik ( ' „das sittliche (Verständnis)“, von "ēthos" „Charakter, Sinnesart“ (dagegen ἔθος: Gewohnheit, Sitte, Brauch), vergleiche lateinisch ') ist jener Teilbereich der Philosophie, der sich mit den Voraussetzungen und der Bewertung menschlichen Handelns befasst. Im Zentrum der Ethik steht das spezifisch moralische Handeln, insbesondere hinsichtlich seiner Begründbarkeit und Reflexion. Cicero übersetzte als erster "êthikê" in den seinerzeit neuen Begriff "philosophia moralis". In seiner Tradition wird die Ethik auch als Moralphilosophie bezeichnet.

Die Ethik und ihre benachbarten Disziplinen (z. B. Rechts-, Staats- und Sozialphilosophie) werden auch als „praktische Philosophie“ zusammengefasst, da sie sich mit dem menschlichen Handeln befasst. Im Gegensatz dazu steht die „theoretische Philosophie“, zu der als klassische Disziplinen die Logik, die Erkenntnistheorie und die Metaphysik gezählt werden.

Als Bezeichnung für eine philosophische Disziplin wurde der Begriff "Ethik" von Aristoteles eingeführt, der damit die wissenschaftliche Beschäftigung mit Gewohnheiten, Sitten und Gebräuchen ("ethos") meinte, wobei allerdings schon seit Sokrates die Ethik ins Zentrum des philosophischen Denkens gerückt war (Sokratische Wende). Hintergrund war dabei die bereits von den Sophisten vertretene Auffassung, dass es für ein Vernunftwesen wie den Menschen unangemessen sei, wenn dessen Handeln ausschließlich von Konventionen und Traditionen geleitet wird. Aristoteles war der Überzeugung, menschliche Praxis sei grundsätzlich einer vernünftigen und theoretisch fundierten Reflexion zugänglich. Ethik war somit für Aristoteles eine philosophische Disziplin, die den gesamten Bereich menschlichen Handelns zum Gegenstand hat und diesen Gegenstand mit philosophischen Mitteln einer normativen Beurteilung unterzieht und zur praktischen Umsetzung der auf diese Weise gewonnenen Erkenntnisse anleitet.

Die allgemeine Ethik – die im Folgenden einfach als Ethik bezeichnet wird – wird heute als eine philosophische Disziplin verstanden, deren Aufgabe es ist, Kriterien für gutes und schlechtes Handeln und die Bewertung seiner Motive und Folgen aufzustellen. Sie ist die Grundlagendisziplin der Angewandten Ethik, die sich als Individualethik und Sozialethik sowie in den Bereichsethiken mit den normativen Problemen des spezifischen Lebensbereiches befasst.

Die Ethik baut als philosophische Disziplin allein auf das Prinzip der Vernunft. Darin unterscheidet sie sich vom klassischen Selbstverständnis theologischer Ethik, die sittliche Prinzipien als in Gottes Willen begründet annimmt und insofern im Allgemeinen den Glauben an eine göttliche Offenbarung voraussetzt. Besonders im 20. Jahrhundert haben allerdings Autoren wie Alfons Auer theologische Ethik als weitgehend autonom zu konzipieren versucht.

Das Ziel der Ethik ist die Erarbeitung von allgemeingültigen Normen und Werten. Sie ist abzugrenzen von einer "deskriptiven Ethik", die keine moralischen Urteile fällt, sondern die tatsächliche, innerhalb einer Gesellschaft gelebte Moral mit empirischen Mitteln zu beschreiben versucht. Die Metaethik, die sich zu Beginn des 20. Jahrhunderts als eigenständige Disziplin entwickelte, reflektiert die allgemeinen logischen, semantischen und pragmatischen Strukturen moralischen und ethischen Sprechens und stellt insofern die Grundlage für die deskriptive und normative Ethik dar.
Die philosophische Disziplin "Ethik" (die auch als "Moralphilosophie" bezeichnet wird) beschäftigt sich überwiegend mit den folgenden drei Problemfeldern:

1. mit der Frage nach dem „höchsten Gut“;

2. mit der Frage nach dem richtigen Handeln in bestimmten Situationen – also: „Wie soll ich mich in dieser Situation verhalten?“ (Die einfachste und klassische Formulierung einer solchen Frage stammt von Immanuel Kant: ) und

3. mit der Frage nach der Freiheit des Willens.

Als Hauptgegenstand der Ethik gelten den meisten Philosophen die menschlichen Handlungen und die sie leitenden Handlungsregeln. Die Ergebnisse bestehen in anwendbaren ethischen (bzw. moralischen) Normen, die beinhalten, dass unter bestimmten Bedingungen bestimmte Handlungen geboten, verboten oder erlaubt sind.

Insofern als in der Ethik nach allgemeingültigen Antworten auf die Frage nach dem richtigen Handeln gesucht wird, stellt sich die Frage nach der Möglichkeit allgemeingültiger ethischer Normen und deren Begründung. Diese Diskussion über die Grundlagen der Ethik, ihre Kriterien und Methoden, ist ein wichtiger Teil der philosophischen Disziplin "Ethik", der auch als Metaethik bezeichnet wird.

Die Ethik ist von ihrer Zielsetzung her eine praktische Wissenschaft. Es geht ihr nicht um ein Wissen um seiner selbst willen "(theoria)," sondern um eine verantwortbare Praxis. Sie soll dem Menschen (in einer immer unüberschaubarer werdenden Welt) Hilfen für seine sittlichen Entscheidungen liefern. Dabei kann die Ethik allerdings nur allgemeine Prinzipien guten Handelns oder ethischen Urteilens überhaupt oder Wertvorzugsurteile für bestimmte Typen von Problemsituationen begründen. Die Anwendung dieser Prinzipien auf den einzelnen Fall ist im Allgemeinen nicht durch sie leistbar, sondern Aufgabe der praktischen Urteilskraft und des geschulten Gewissens. Aristoteles vergleicht dies mit der Kunst des Arztes und des Steuermanns. Diese verfügen über ein theoretisches Wissen, das aber situationsspezifisch angewendet werden muss. Entsprechend muss auch die praktische Urteilskraft allgemeine Prinzipien immer wieder auf neue Situationen und Lebenslagen anwenden. Dabei spielt für die richtige sittliche Entscheidung neben der Kenntnis allgemeiner Prinzipien die Schulung der Urteilskraft in praktischer Erfahrung eine wichtige Rolle.

Auch die Theorie der rationalen Entscheidung beantwortet die Frage: Wie soll ich handeln? Jedoch unterscheidet sie sich von ethischen Fragestellungen dadurch, dass Theorien rationalen Handelns nicht in jedem Falle auch Theorien des moralisch Guten sind. Von ethischen Theorien mit einem allgemeinverbindlichen Anspruch unterscheiden sich Theorien rationaler Entscheidung dadurch, dass nur die Ziele und Interessen eines bestimmten Individuums oder eines kollektiven Subjekts (z. B. eines wirtschaftlichen Unternehmens oder eines Staates) berücksichtigt werden. Zur Unterscheidung zwischen Ethik und Moral kann auf Hegel verwiesen werden und seinen Versuch einer Synthese aus dem klassischen Gemeinschafts- und dem modern-individualistischen Freiheitsdenken.

Auch die Rechtswissenschaft fragt danach, wie gehandelt werden soll. Im Unterschied zur Ethik bezieht sie sich jedoch i. Allg. auf eine bestimmte, faktisch geltende Rechtsordnung (positives Recht), deren Normen sie auslegt und anwendet. Wo die Rechtswissenschaft als Rechtsphilosophie, Rechtspolitik oder Gesetzgebungslehre auch die Begründung von Rechtsnormen behandelt, nähert sie sich der Ethik an.

Auch religiös motivierte Ethiken geben Antworten auf die Frage, wie gehandelt werden soll. Im Unterschied zu philosophisch begründeten Ethiken beanspruchen diese jedoch nicht in jedem Fall, dass ihre Antworten auf für jeden nachvollziehbare Argumente gegründet sind, sondern können sich etwa auf eine göttliche Offenbarung als Quelle von Handlungsnormen berufen (siehe etwa die Sollens-Aussagen der Zehn Gebote im Judentum).

Mit gesellschaftlichen Normen des Handelns befassen sich auch empirische Wissenschaften wie Soziologie, Ethnologie und Psychologie. Im Unterschied zur normativen Ethik im philosophischen Sinne geht es dort jedoch um die Beschreibung und Erklärung faktisch bestehender ethischer Überzeugungen, Einstellungen und Sanktionsmuster und nicht um deren Rechtfertigung oder Kritik.

Die Frage, ob man überhaupt moralisch sein soll, wird in Platons "Politeia" im ersten Kapitel aufgeworfen. In der Moderne wurde der Diskurs um die Frage von Bradley und Prichard eingeleitet.

Metaethische Kognitivisten behaupten, erkennen zu können, wie man moralisch handeln solle. Somit stellt sich ihnen die Frage, ob man das überhaupt tun soll, nicht mehr, da sie auch gleich mit erkennen, dass man dies tun soll.

Metaethische Nonkognitivisten hingegen müssen die Frage, ob man moralisch handeln soll, klären.
Die Diskussion wird in der Philosophie zumeist anhand der Frage „Warum soll man moralisch sein?“ geführt. Das Sollen innerhalb der Frage ist dabei kein moralisches Sollen, sondern verweist auf eine Akzeptanz besserer Gründe, z. B. anhand der Theorie der rationalen Entscheidung. Die Antwort auf die Frage hängt also ab vom jeweiligen Verständnis von Vernunft.

Die Frage, ob man moralisch sein soll oder nicht, wird beantwortet mit:

Die Situation des Menschen, der sich zwischen diesen Antworten entscheiden muss, hat ihre klassische Gestaltung in der so genannten Prodikos-Fabel von Herakles am Scheideweg gefunden, die auch von vielen christlichen Autoren rezipiert wurde.

Eine bekannte absolute Moralbegründung ist die der Letztbegründung von Apel. Angenommen jemand lehnt es ab, über Zwecke zu reden, dann sei diese Ablehnung bereits ein Reden über Zwecke. Insofern ist dies ein so genannter performativer Selbstwiderspruch.

Moralbegründung aus Sicht der Systemtheorie verzichtet darauf, zu begründen, warum Individuen moralisch handeln sollen. Stattdessen wird dargelegt, warum Moral als Regulierungsfunktion des Kommunikationssystems unentbehrlich ist (s. a. AGIL-Schema).

Viele Philosophen behaupten, dass man zwar nicht beweisen kann, dass Amoralismus logisch widersprüchlich ist, dass aber im wirklichen Leben Amoralisten viele Nachteile haben, so dass moralisches Verhalten größere Rentabilität im Sinne der Theorie der rationalen Entscheidung besitzt. Ethik wird mit dieser Form von Moralbegründung zu einer Spezialform von Zweckrationalität. Einer der wichtigsten Vertreter dieser Argumentationslinie ist David Gauthier.

Viele Philosophen dieser Richtung berufen sich auf den Grundsatz quid pro quo oder auf "Tit for Tat"-Strategien.

Andere meinen, Amoralisten seien auf Einsamkeit festgelegt, da man ihnen nicht vertrauen könne und auch sie niemandem vertrauen könnten. Daher könnten sie eines der wichtigsten Lebensgüter, soziale Gemeinschaft und Anerkennung, nie erreichen.

Nach R. M. Hare können Amoralisten keine moralischen Begriffe gebrauchen und daher nicht von ihren Mitmenschen fordern, sie fair zu behandeln. Die Möglichkeit entsprechender Lügen sah Hare nicht. Hare behauptete zudem, der Aufwand, den Amoralisten treiben müssten, um ihre Überzeugung zu verschleiern, wäre so groß, dass sie sozial immer im Nachteil seien.

Amoralisten kritisieren verschiedene Moralbegründungen, indem sie darauf verweisen, dass es in vielen Teilen der Welt relativ stabile Verhältnisse gibt, die üblichen moralischen Vorstellungen widersprechen, z. B. völkerrechtswidrige Kriege um Ressourcen, Sklaverei oder erfolgreiche Mafia-Organisationen.

Dezision (von latein decidere: entscheiden, fallen, abschneiden) bedeutet so viel wie Entscheidung.
Der Begriff des Dezisionismus wird oft in pejorativer Bedeutung gebraucht von Metaethischen Kognitivisten gegenüber Philosophen, die nur relative Begründungen der Moral anerkennen, z. B. Hare oder Popper und Hans Albert.

Dezisionisten sehen keine Alternative zu Prinzipienentscheidungen, die aus logischen oder pragmatischen Gründen ihrerseits nicht mehr weiter begründet werden können. So behauptete z. B. Henry Sidgwick, der Mensch müsse sich zwischen Utilitarismus und Egoismus entscheiden.

Dem Dezisionismus wird von seinen Kritikern ähnlich wie dem metaethischen Nonkognitivismus entgegengehalten, dass auch Entscheidungen wiederum einer Bewertung unterzogen werden könnten: Man entscheide sich nicht für bestimmte ethische Prinzipien, sondern diese würden umgekehrt die Grundlage von Entscheidungen darstellen.

Außerdem argumentieren Vertreter des Naturrechts dafür, dass sich die Objektivität der Ethik (also das Sollen) auf die Natur bzw. das Wesen des Seienden und letztlich auf das Sein selbst (z. B. Gott) zurückführen ließen.

Im Mittelpunkt deontologischer Ethiken steht der Begriff der Handlung. Sie wird in erster Annäherung definiert als „eine von einer Person verursachte Veränderung des Zustands der Welt“. Die Veränderung kann eine äußere, in Raum und Zeit beobachtbare oder eine innere, mentale Veränderung sein. Auch die Art und Weise, wie man von außen einwirkenden Ereignissen begegnet, kann im weiteren Sinne als Handlung bezeichnet werden.

Handlungen unterscheiden sich von Ereignissen dadurch, dass wir als ihre Ursache nicht auf ein weiteres Ereignis verweisen, sondern auf die Absicht des Handelnden. Die Absicht (intentio; nicht zu verwechseln mit dem juristischen Absichtsbegriff, dem dolus directus 1. Grades) ist ein von der Handlung selbst zu unterscheidender Akt. Geplanten Handlungen liegt eine zeitlich vorausgehende Absicht zugrunde. Wir führen die Handlung so aus, wie wir sie uns vorher schon vorgenommen hatten. Der Begriff der Absicht ist von dem der Freiwilligkeit zu unterscheiden. Die Freiwilligkeit ist eine Eigenschaft, die zur Handlung selbst gehört. Der Begriff der Freiwilligkeit ist weiter als der der Absicht; er umfasst auch die spontanen Handlungen, bei denen man nicht mehr von Absicht im engeren Sinne sprechen kann.

Eine Handlung ist dann "freiwillig", wenn sie mit Wissen und Willen durchgeführt wird.

Die Unwissenheit kann dabei allerdings nur dann die Freiwilligkeit einer Handlung aufheben, wenn die handelnde Person sich nach besten Kräften vorher informiert hat, und sie mit dem ihr fehlenden "Wissen" anders gehandelt hätte. War dem Handelnden eine Kenntnis der Norm oder der Folgen zuzumuten, ist er für ihre Übertretung verantwortlich ("ignorantia crassa" oder "supina"). Noch weniger entschuldigt jene Unkenntnis, die absichtlich zum Vermeiden eines Konflikts mit der Norm herbeigeführt wurde "(ignorantia affectata)," wenn also z. B. bewusst vermieden wird, sich über ein Gesetz zu informieren, um sagen zu können, man hätte von einem bestimmten Verbot nicht gewusst. Das Sprichwort sagt zu Recht: „Unwissenheit schützt vor Strafe nicht“. Auch im deutschen Strafrecht wird diesem Sachverhalt Rechnung getragen. So heißt es z. B. in § 17 StGB:

Für die sittliche Bewertung einer Handlung ist außerdem das effektive "Wollen" wesentlich, die Absicht ihrer Verwirklichung. Das setzt voraus, dass zumindest der Handelnde der Auffassung war, dass ihm eine Verwirklichung seiner Absicht möglich sei, d. h. dass das Ergebnis von seinem Handeln kausal herbeigeführt werden könne. Unterliegt der Handelnde einem äußeren Zwang, hebt dieser die Freiwilligkeit der Handlung im Allgemeinen auf.

Absichten finden ihren Ausdruck in praktischen Grundsätzen. Diese können zunächst einmal in inhaltliche und formale Grundsätze unterschieden werden. Inhaltliche Grundsätze legen konkrete inhaltliche Güter (Leben, Gesundheit, Besitz, Vergnügen, Umwelt etc.) als Bewertungskriterium für das Handeln zugrunde. Sie sind teilweise subjektiv und haben unter Umständen einen dezisionistischen Charakter. In diesen Fällen können sie ihre eigene Vorrangstellung nicht gegenüber anderen, konkurrierenden inhaltlichen Grundsätzen begründen.

Formale Grundsätze verzichten auf einen Bezug zu konkreten inhaltlichen Gütern. Das bekannteste Beispiel ist der Kategorische Imperativ Kants.

Es lassen sich grundsätzlich drei Ebenen der praktischen Sätze voneinander unterscheiden:

Die Ethik ist häufig nur in der Lage, Aussagen zu den ersten beiden Ebenen zu machen. Die Übertragung von praktischen Grundsätzen auf eine konkrete Situation, erfordert das Vermögen der praktischen Urteilskraft. Nur mit seiner Hilfe können eventuell auftretende Zielkonflikte gelöst und die voraussichtlichen Folgen von Entscheidungen abgeschätzt werden.

Wesentlich für die ethische Bewertung von Handlungen sind die mit ihnen verbundenen Folgen. Diese werden unterschieden in motivierende und in Kauf genommene Folgen. Motivierende Folgen sind solche, um derentwillen eine Handlung ausgeführt wird. Sie werden vom Handelnden unmittelbar angezielt („Voluntarium in se“).

In Kauf genommene Folgen („Voluntarium in causa“) werden zwar nicht unmittelbar angezielt, aber als Nebenwirkung der motivierenden Folgen vorausgesehen und bewusst zugelassen (Prinzip der Doppelwirkung). So unterliegt beispielsweise bewusste Fahrlässigkeit als bedingter Vorsatz (dolus eventualis) der ethischen und rechtlichen Verantwortung: Volltrunkenheit entschuldigt nicht bei einem Verkehrsunfall.

Bereits Thomas von Aquin unterscheidet eine zweifache Kausalität des Willens: die „direkte“ Einwirkung des Willens, in der durch den Willensakt ein bestimmtes Ereignis hervorgerufen wird, und die „indirekte“, in der ein Ereignis dadurch eintritt, dass der Wille untätig bleibt. Tun und Unterlassen unterscheiden sich hierbei nicht hinsichtlich ihrer Freiwilligkeit. Beim Unterlassen verzichtet jemand auf das Eingreifen in einen Prozess, obwohl er die Möglichkeit dazu hätte. Auch das Unterlassen kann daher als Handlung aufgefasst werden und strafbar sein.

Die strikte Unterscheidung zwischen diesen beiden Handlungsformen, die z. B. in der medizinischen Ethik eine große Rolle spielt (vgl. aktive und passive Sterbehilfe etc.), erscheint daher vom ethischen Standpunkt aus gesehen als teilweise fragwürdig.

Im Mittelpunkt teleologischer Ethiken steht die Frage, was ich mit meiner Handlung letztlich bezwecke, welches Ziel ich mit ihr verfolge. Der Begriff „Ziel“ (finis, telos;) ist hier insbesondere als „letztes Ziel“ oder „Endziel“ zu verstehen, von dem all mein Handeln bestimmt wird.

In der Tradition wird als letztes Ziel des Menschen häufig das Glück oder die Glückseligkeit (beatitudo) genannt. Der Ausdruck „Glück“ wird dabei in einem mehrdeutigen Sinne gebraucht:

Philosophiegeschichtlich konkurrieren die Bestimmungen von Glück als „Lebensglück“ und als subjektives Wohlbefinden miteinander. Für die Eudämonisten (Platon, Aristoteles) ist Glück die Folge der Verwirklichung einer Norm, die als Telos im Wesen des Menschen angelegt ist. Glücklich ist dieser Konzeption zufolge vor allem, wer auf vernünftige Weise tätig ist.

Für die Hedonisten (Sophisten, klassische Utilitaristen) gibt es kein zu verwirklichendes Telos des Menschen mehr; es steht keine objektive Norm zur Verfügung, um zu entscheiden, ob jemand glücklich ist. Dies führt zu einer Subjektivierung des Glücksbegriffs. Es obliege allein dem jeweiligen Individuum, zu bewerten, ob es glücklich ist. Glück wird hier mit dem Erreichen von Gütern wie Macht, Reichtum, Ruhm etc. gleichgesetzt.

Das Wort „Sinn“ bezeichnet grundsätzlich die Qualität von etwas, das dieses verstehbar macht. Wir verstehen etwas dadurch, indem wir erkennen, worauf es „hingeordnet“ ist, wozu es dient. Die Frage nach dem Sinn steht also in einem engen Zusammenhang mit der Frage nach dem Ziel oder Zweck von etwas. Auch der Sinn einer Handlung oder gar des ganzen Lebens kann nur beantwortet werden, wenn die Frage nach seinem Ziel geklärt ist. Eine menschliche Handlung bzw. ein gesamtes Leben ist dann sinnvoll, wenn es auf dieses Ziel hin ausgerichtet ist.

„Gut“ gehört wie der Begriff „seiend“ zu den ersten und daher nicht mehr definierbaren Begriffen. Es wird zwischen einem adjektivischen und einem substantivischen Gebrauch unterschieden.

Als Adjektiv bezeichnet das Wort „gut“ generell die Hinordnung eines „Gegenstandes“ auf eine bestimmte Funktion oder einen bestimmten Zweck. So spricht man z. B. von einem „guten Messer“, wenn es seine im Prädikator „Messer“ ausgedrückte Funktion erfüllen – also z. B. gut schneiden kann. Analog spricht man von einem „guten Arzt“, wenn er in der Lage ist, seine Patienten zu heilen und Krankheiten zu bekämpfen. Ein „guter Mensch“ ist demnach jemand, der in seinem Leben auf das hin ausgerichtet ist, was das Menschsein ausmacht, also dem menschlichen Wesen bzw. seiner Natur entspricht.

Als Substantiv bezeichnet das Wort „das Gut“ etwas, auf das hin wir unser Handeln ausrichten. Wir gebrauchen es normalerweise in dieser Weise, um „eine unter bestimmten Bedingungen vollzogene Wahl als richtig oder gerechtfertigt zu beurteilen“. So kann beispielsweise eine Aussage wie „Die Gesundheit ist ein Gut“ als Rechtfertigung für die Wahl einer bestimmten Lebens- und Ernährungsweise dienen. In der philosophischen Tradition war man der Auffassung, dass prinzipiell jedes Seiende – unter einer gewissen Rücksicht – Ziel des Strebens sein könne („omne ens est bonum“). Daher wurde die „Gutheit“ des Seienden zu den Transzendentalien gerechnet.

Gemäß der Analyse von Richard Mervyn Hare werden wertende Wörter wie „gut“ oder „schlecht“ dazu verwendet, in Entscheidungssituationen Handeln anzuleiten bzw. Empfehlungen zu geben. Die Wörter „gut“ oder „schlecht“ haben demnach keine beschreibende (deskriptive), sondern eine vorschreibende (präskriptive) Funktion.

Dies kann an einer außermoralischen Verwendung des Wortes „gut“ verdeutlicht werden. Wenn ein Verkäufer zum Kunden sagt: „Dies ist ein guter Wein“, dann empfiehlt er den Kauf dieses Weines, er beschreibt damit jedoch keine wahrnehmbare Eigenschaft des Weines. Insofern es jedoch sozial verbreitete Bewertungsstandards für Weine gibt (er darf nicht nach Essig schmecken, man darf davon keine Kopfschmerzen bekommen etc.), so bedeutet die Bewertung des Weines als „gut“, dass der Wein diese Standards erfüllt und dass er somit auch bestimmte empirische Eigenschaften besitzt.

Die Bewertungskriterien, die an eine Sache angelegt werden, können "je nach dem Verwendungszweck variieren". Ein herber Wein mag als Tafelwein gut, für sich selbst getrunken dagegen eher schlecht sein. Der Verwendungszweck einer Sache ist keine feststehende Eigenschaft der Sache selbst, sondern beruht auf menschlicher Setzung. Eine Sache ist „gut“ – immer bezogen auf bestimmte Kriterien. Wenn der Verkäufer sagt: „Dies ist ein sehr guter Tafelwein“ dann ist er so, wie er gemäß den üblichen Kriterien für Tafelwein sein soll.

Wenn das Wort „gut“ in moralischen Zusammenhängen gebraucht wird („Dies war eine gute Tat“), so empfiehlt man die Tat und drückt aus, dass sie so war, wie sie sein soll. Man beschreibt damit jedoch nicht die Tat. Wird auf allgemein anerkannte moralische Kriterien Bezug genommen, drückt man damit zugleich aus, dass die Tat bestimmte empirische Eigenschaften besitzt, z. B. eine Zurückstellung des Eigeninteresses zugunsten überwiegender Interessen von Mitmenschen.

Als das höchste Gut ("summum bonum") wird das bezeichnet, was nicht nur unter einer bestimmten Rücksicht (für den Menschen) gut ist, sondern schlechthin, da es dem Menschen als Menschen ohne Einschränkung entspricht. Es ist identisch mit dem „unbedingt Gesollten“. Seine inhaltliche Bestimmung hängt ab von der jeweiligen Sicht der Natur des Menschen. In der Tradition wurden dabei die unterschiedlichsten Lösungsvorschläge präsentiert:

Der Begriff „Wert“ stammte ursprünglich aus der Nationalökonomie, wo man unter anderem zwischen Gebrauchs- und Tauschwert unterschied. Er wurde erst in der zweiten Hälfte des 19. Jahrhunderts ein philosophischer Terminus, wo er im Rahmen der Wertphilosophie (Max Scheler u. a.) eine zentrale Bedeutung einnahm. Dort führte man ihn als Gegenbegriff zur Kantischen Pflichtethik ein, in der Annahme, dass Werten vor allen Vernunftüberlegungen eine „objektive Gültigkeit“ zukommen würde.

In der Alltagssprache taucht der Begriff auch in jüngster Zeit wieder verstärkt auf, gerade wenn von „Grundwerten“, einem „Wertewandel“ oder einer „neuen Wertedebatte“ die Rede ist.

Der Wertbegriff weist große Ähnlichkeiten mit dem Begriff des Guten auf. Er wird wie dieser grundsätzlich in einer subjektiven und einer objektiven Variante gebraucht:

Im Vergleich zum Begriff des Guten kommt dem Wertbegriff allerdings eine stärkere gesellschaftliche Bedingtheit zu. So spricht man von einem „Wertewandel“, wenn man ausdrücken will, dass sich bestimmte, in einer Gesellschaft allgemein akzeptierte Handlungsnormen im Verlauf der Geschichte verändert haben. Damit meint man aber in der Regel nicht, dass das, was früher für gut gehalten wurde, nun „tatsächlich“ nicht mehr gut sei, sondern nur, dass sich das allgemeine Urteil darüber geändert habe.

Die richtige Abwägung ethischer Güter und ihre Durchsetzung setzt Tugend voraus.
In ihrer klassischen Definition formuliert sie Aristoteles als "„jene feste Grundhaltung, von der aus [der Handelnde] tüchtig wird und die ihm eigentümliche Leistung in vollkommener Weise zustande bringt“" (NE 1106a).

Die Leistung der ethischen Tugenden besteht vor allem darin, im Menschen eine Einheit von sinnlichem Strebevermögen und sittlicher Erkenntnis zu bewirken. Wir bezeichnen einen Menschen erst dann als „gut“, wenn er zur inneren Einheit mit sich selbst gekommen ist und das als richtig Erkannte auch affektiv voll bejaht. Dies ist nach Aristoteles nur durch eine Integration der Gefühle durch die ethischen Tugenden möglich. Die ungeordneten Gefühle verfälschen das sittliche Urteil. Das Ziel der Einheit von Vernunft und Gefühl führt über eine bloße Ethik der richtigen Entscheidung hinaus. Es kommt nicht nur darauf an, was wir tun, sondern auch wer wir sind.

Tugend setzt neben Erkenntnis eine Gewöhnung voraus, die durch Erziehung und soziale Praxis erreicht wird. Wir werden gerecht, mutig etc., indem wir uns in Situationen begeben, wo wir uns entsprechend verhalten können. Die wichtigste Rolle kommt dabei der Tugend der Klugheit (phronesis) zu. Ihr obliegt es, die rechte „Mitte“ zwischen den Extremen zu finden und sich für die optimale Lösung in der konkreten Situation zu entscheiden.

Der Begriff „sollen“ ist ein Grundbegriff deontologischer Ethikansätze. Er bezieht sich – als Imperativ – auf eine Handlung, mit der ein bestimmtes Ziel erreicht werden soll. Dabei müssen folgende Bedingungen erfüllt sein:

Sprachanalytisch lässt sich das Sollen mit Hilfe der sogenannten deontischen Prädikatoren erklären. Diese beziehen sich auf die sittliche Verbindlichkeit von Handlungen. Folgende Varianten sind dabei zu unterscheiden:

Moralisch mögliche Handlungen sind sittlich erlaubt, d. h. man darf so handeln. Moralisch notwendige Handlungen sind sittlich geboten. Hier spricht man davon, dass wir etwas tun sollen bzw. die Pflicht haben, etwas zu tun. Moralisch unmögliche Handlungen sind sittlich verbotene Handlungen, die wir nicht ausführen dürfen; siehe auch Sünde.

Die Begriffe „gut“ und „gesollt“ sind zwar eng miteinander verwandt aber nicht deckungsgleich.
So können wir in Situationen stehen, in denen wir nur zwischen schlechten Alternativen wählen können. Hier ist es gesollt, dass wir uns für das „geringere Übel“ entscheiden. Umgekehrt ist nicht alles Gute auch gesollt. Das kann z. B. der Fall sein, wenn das Erreichen eines Gutes ein anderes Gut ausschließt. Hier muss eine Güterabwägung erfolgen, die zum Verzicht eines Gutes führt.

Der Begriff der Gerechtigkeit ist seit der intensiven Diskussion um die „Theorie der Gerechtigkeit“ von John Rawls und vor allem seit der aktuellen politischen Debatte um die Aufgaben des Sozialstaates (Betonung der Chancen- und Leistungsgerechtigkeit gegenüber der Verteilungsgerechtigkeit) wieder stark ins Blickfeld geraten.

„Gerecht“ wird – wie der Begriff „gut“ – in vielerlei Bedeutungen gebraucht. Es werden Handlungen, Haltungen, Personen, Verhältnisse, politische Institutionen und zuweilen auch Affekte (der „gerechte Zorn“) als gerecht bezeichnet. Grundsätzlich kann zwischen einem „subjektiven“ und einem „objektiven“ Gebrauch unterschieden werden, wobei beide Varianten aufeinander bezogen sind.

Die "subjektive" oder besser personale "Gerechtigkeit" bezieht sich auf das Verhalten oder die ethische Grundhaltung einer Einzelperson. Eine Person kann gerecht handeln ohne gerecht zu sein und umgekehrt. Damit im Zusammenhang steht die kantische Unterscheidung zwischen Legalität und Moralität. Legale Handlungen befinden sind nach außen hin betrachtet in Übereinstimmung mit dem Sittengesetz, geschehen aber nicht ausschließlich aufgrund moralischer Beweggründe, sondern z. B. auch aus Angst, Opportunismus etc. Bei moralischen Handlungen dagegen stimmen Handlung und Motiv miteinander überein. In diesem Sinne wird Gerechtigkeit als eine der vier "Kardinaltugenden" bezeichnet.

Die "objektive" oder institutionelle "Gerechtigkeit" bezieht sich auf die Bereiche Recht und Staat.
Hier geht es immer um Pflichten innerhalb einer Gemeinschaft, die das Gleichheitsprinzip berühren. Es ist grundsätzlich zu unterscheiden zwischen der "ausgleichenden Gerechtigkeit" (iustitita commutativa) und "Verteilungsgerechtigkeit" (iustitita distributiva). Bei der ausgleichenden Gerechtigkeit tritt der Wert einer Ware oder Leistung in den Vordergrund. Bei der Verteilungsgerechtigkeit geht es um den Wert der beteiligten Personen.

Die Gerechtigkeit der Einzelpersonen und der Institutionen sind in einem engen Zusammenhang zueinander zu sehen. Ohne gerechte Bürger werden keine gerechten Institutionen geschaffen oder aufrechterhalten werden können. Ungerechte Institutionen erschweren andererseits die Entfaltung der Individualtugend der Gerechtigkeit.

Das Anliegen der Ethik beschränkt sich nicht auf das Thema „Gerechtigkeit“. Zu den Tugenden gehören noch diejenigen, die man vor allem sich selbst gegenüber hat (Klugheit, Mäßigung, Tapferkeit). Zu den ethischen Pflichten gegenüber anderen zählt noch die Pflicht des "Wohltun"s (beneficientia), die über die Gerechtigkeit hinausgeht und ihre Wurzel letztlich in der Liebe hat. Während der Gerechtigkeit das Gleichheitsprinzip zugrunde liegt, ist dies beim Wohltun die Notlage oder Bedürftigkeit des anderen. Diese Unterscheidung entspricht der zwischen „iustitia“ und „caritas“ (Thomas von Aquin), Rechts- und Tugendpflichten (Kant) bzw. in der Gegenwart der zwischen „duties of justice“ und „duties of charity“ (Philippa Foot).

Klassen ethischer oder moralphilosophischer Theorien lassen sich danach unterscheiden, welche Kriterien sie für die Bestimmung des moralisch Guten zugrunde legen. Das moralisch Gute kann bestimmt werden durch:
Dabei werden unterschiedlichste Kombinationen und feinere moraltheoretische Bestimmungen vertreten.

Die verschiedenen Ethikansätze werden traditionell prinzipiell danach unterschieden, ob sie ihren Schwerpunkt auf die Handlung selbst (deontologische Ethikansätze) oder auf die Handlungsfolgen (teleologische Ethikansätze) legen. Die Unterscheidung geht zurück auf C. D. Broad
und wurde bekannt durch William K. Frankena. In dieselbe Richtung geht auch die Aufteilung Max Webers in Gesinnungs- und Verantwortungsethiken, wobei diese von ihm als Polemik gegenüber Gesinnungsethiken verstanden wurde.

Das griechische Wort „telos“ bedeutet so viel wie Vollendung, Erfüllung, Zweck oder Ziel. Unter teleologischen Ethiken versteht man daher solche Theorieansätze, die ihr Hauptaugenmerk auf bestimmte Zwecke oder Ziele richten. In ihnen wird die Forderung erhoben, Handlungen sollten ein Ziel anstreben, das in einem umfassenderen Verständnis gut ist. Der Inhalt dieses Zieles wird von den verschiedenen Richtungen auf recht unterschiedliche Art und Weise bestimmt.

Teleologische Ethiken geben valuativen Sätzen einen Vorrang gegenüber normativen Sätzen. Für sie stehen Güter und Werte im Vordergrund. Die menschlichen Handlungen sind insbesondere insofern von Interesse, als sie hinderlich oder förderlich zum Erreichen dieser Güter und Werte sein können. "„Eine Handlung ist dann auszuführen und nur dann, wenn sie oder die Regel, unter die sie fällt, ein größeres Überwiegen des Guten über das Schlechte herbeiführt, vermutlich herbeiführen wird oder herbeiführen sollte als jede erreichbare Alternative“" (Frankena).

Innerhalb teleologischer Ethikansätze wird wiederum zwischen „onto-teleologischen“ und „konsequentialistisch-teleologischen“ Ansätzen unterschieden.

In onto-teleologischen Ansätzen – klassisch vertreten durch Aristoteles – wird davon ausgegangen, dass das zu erstrebende Gut in gewisser Weise dem Menschen selbst als Teil seiner Natur innewohne. Es wird gefordert, dass der Mensch so handeln und leben solle, wie es seiner Wesensnatur entspricht, um so seine artspezifischen Anlagen auf bestmögliche Weise zu vervollkommnen.

In konsequentialistisch-teleologischen Ansätzen hingegen wird nicht mehr von einer letzten vorgegebenen Zweckhaftigkeit des menschlichen Daseins ausgegangen. Das zu erstrebende Ziel wird daher durch einen außerhalb des handelnden Subjekts liegenden Nutzen bestimmt. Dieser Ansatz wird bereits in der Antike (Epikur) und später in seiner typischen Form durch den Utilitarismus vertreten.

Das griechische Wort "„to deon“" bedeutet „das Schickliche, die Pflicht“. Deontologische Ethiken kann man daher mit Sollensethiken gleichsetzen. Sie sind dadurch gekennzeichnet, dass bei ihnen den Handlungsfolgen nicht dieselbe Bedeutung zukommt wie in teleologischen Ethiken. Innerhalb der deontologischen Ethiken wird häufig zwischen aktdeontologischen (z. B. Jean-Paul Sartre) und regeldeontologischen Konzeptionen (z. B. Immanuel Kant) unterschieden. Während die "Regeldeontologie" allgemeine Handlungstypen als verboten, erlaubt oder geboten ausweist (vgl. z. B. das Lügenverbot oder die Pflicht, Versprechen zu halten), bezieht sich den "aktdeontologischen Theorien" zufolge das deontologische Moralurteil unmittelbar auf spezifische Handlungsweisen in jeweils bestimmten Handlungssituationen.

In deontologischen Ethiken haben normative Sätze eine Vorrangstellung gegenüber valuativen Sätzen. Für sie bilden Gebote, Verbote und Erlaubnisse die Grundbegriffe. Es rücken die menschlichen Handlungen in den Vordergrund, da nur sie gegen eine Norm verstoßen können. Robert Spaemann charakterisiert sie als "„moralische Konzepte, […] für welche bestimmte Handlungstypen ohne Beachtung der weiteren Umstände immer verwerflich sind, also z. B. die absichtliche direkte Tötung eines unschuldigen Menschen, die Folter oder der außereheliche Beischlaf eines verheirateten Menschen“."

Die Unterscheidung zwischen teleologischen und deontologischen Ethiken wird von einigen Kritikern als fragwürdig bezeichnet. In der Praxis sind auch selten Ansätze zu finden, die eindeutig einer der beiden Richtungen zugeordnet werden könnten.

Einer strikten deontologischen Ethik müsste es gelingen, Handlungen aufzuzeigen, die „in sich“, völlig losgelöst von ihren Folgen, als unsittlich und „in sich schlecht“ zu bezeichnen wären. Diese wären dann „unter allen Umständen“ zu tun oder zu unterlassen gemäß dem Spruch "„Fiat iustitia et pereat mundus“" ("„Gerechtigkeit geschehe, und sollte die Welt darüber zugrunde gehen“", Ferdinand I. von Habsburg). Bekannte Beispiele solcher Handlungen sind die „Tötung Unschuldiger“ oder die nach Kant unzulässige Lüge. In den Augen der Kritiker liegt in diesen Fällen häufig eine „petitio principii“ vor. Wenn z. B. die Tötung Unschuldiger als Mord und dieser wiederum als unsittliche Handlung definiert wird, könne sie natürlich in jedem Fall als „in sich schlecht“ bezeichnet werden. Das gleiche gelte für die Lüge, wenn sie als unerlaubtes Verfälschen der Wahrheit bezeichnet wird.

Gerade in der Analyse ethischer Dilemmasituationen, in denen nur die Wahl zwischen mehreren Übeln möglich ist, zeige sich, dass es kaum möglich sein dürfte, bestimmte Handlungen unter allen Umständen als „sittlich schlecht“ zu bezeichnen. Nach einer strikten deontologischen Ethik wäre die „Wahl des kleineren Übels“ nicht möglich.

An strikt teleologisch argumentierenden Ethikansätzen wird kritisiert, dass sie das ethisch Gesollte von außerethischen Zwecken abhängig machen. Damit bleibe die Frage unbeantwortet, weshalb wir diese Zwecke verfolgen sollen. Eine Güterabwägung werde damit unmöglich gemacht, da die Frage, was ein oder das bessere „Gut“ ist, nur geklärt werden könne, wenn vorher allgemeine Handlungsprinzipien definiert wurden. In vielen teleologischen Ansätzen würden diese Handlungsprinzipien auch einfach stillschweigend vorausgesetzt, wie z. B. im klassischen Utilitarismus, für den Lustgewinnung und Unlustvermeidung die Leitprinzipien jeglicher Folgenabschätzung darstellen.

Ethische Positionen lassen sich auch danach unterscheiden, wie sich das Gesollte aus einem bestimmten Wollen ergibt.

Die aufgelisteten Positionen liegen auf unterschiedlichen logischen Ebenen und schließen sich deshalb auch nicht logisch aus. So ist z. B. die Verbindung einer religiösen Position mit einer intuitionistischen Position möglich. Denkbar ist auch eine Verbindung der konsenstheoretischen Position mit einer utilitaristischen Position, wenn man annimmt, dass sich ein Konsens über die richtige Norm nur dann herstellen lässt, wenn dabei der Nutzen (das Wohl) jedes Individuums in gleicher Weise berücksichtigt wird.

Außerdem ist zu beachten: Einige dieser Ansätze haben ausdrücklich nicht den Anspruch, umfassende ethische Konzepte zu sein, sondern z. B. nur Konzepte für die Beurteilung, ob eine Gesellschaft in politisch-ökonomischer Hinsicht gerecht eingerichtet ist; z. B. bei John Rawls, im Unterschied zu umfassenderen Ansätze, die auch Fragen privater, individueller Ethik betreffen – etwa, ob es eine moralische Pflicht gibt, zu lügen, wenn genau dies notwendig ist, um ein Menschenleben zu retten (und wenn ohne diese Lüge niemand sonst stattdessen gerettet würde). Auch z. B. Habermas beantwortet diese Frage nicht „inhaltlich“, aber sein Konzept beinhaltet den Bereich auch solcher Fragen, indem es „formal“ postuliert, richtig sei, was in dieser Frage alle, die an einem zwanglosen und zugleich vernünftigen Diskurs dazu teilnehmen würden, als verbindlich für alle dazu herausfinden und akzeptieren würden.

Wenn man fragt, warum Individuum A eine bestimmte Handlungsnorm N befolgen soll, so gibt es zwei Arten von Antworten.

Die eine Art von Antworten bezieht sich auf "eine Institution oder ein Verfahren, wodurch die Norm gesetzt wurde". Beispiele hierfür sind:

A soll N befolgen, weil …


Die andere Art von Antworten bezieht sich auf "die inhaltliche Beschaffenheit der Norm". Beispiele für diese Art von Antworten sind:

A soll N befolgen, weil …


Offensichtlich liegen diese Begründungen auf zwei verschiedenen Ebenen, denn man kann ohne logischen Widerspruch sagen: „Ich halte den Beschluss der Parlamentsmehrheit zwar für inhaltlich falsch, aber dennoch ist er für mich verbindlich. Als Demokrat respektiere ich die Beschlüsse der Mehrheit.“

Man kann die ethischen Theorien nun danach unterscheiden, wie sie mit dem Spannungsverhältnis zwischen der Ebene der verfahrensmäßigen Setzung von verbindlichen Normen und der Ebene der argumentativen Bestimmung von richtigen Normen umgehen.

Auf der einen Seite stehen ganz außen die "Dezisionisten". Für sie ist nur die verbindliche Setzung von Normen bedeutsam. Sie bestreiten, dass man in Bezug auf Normen überhaupt von inhaltlicher Richtigkeit und von einer Erkenntnis der richtigen Norm sprechen kann.

Das Hauptproblem der Dezisionisten ist, dass es für sie keine Berechtigung für einen Widerstand gegen die gesetzten Normen geben kann, denn „verbindlich ist verbindlich“. Außerdem können Dezisionisten nicht begründen, warum man das eine Normsetzungsverfahren irgendeinem anderen Verfahren vorziehen soll.

Auf der anderen Seite stehen ganz außen die ethischen "Kognitivisten". Für sie ist das Problem ethischen Handelns allein ein Erkenntnisproblem, das man durch die Gewinnung relevanter Informationen und deren Auswertung nach geeigneten Kriterien lösen kann. Eine Legitimation von Normen durch Verfahren ist für sie nicht möglich.

Das Hauptproblem der Kognitivisten ist, dass es auch beim wissenschaftlichen Meinungsstreit oft nicht zu definitiven Erkenntnissen kommt, die als Grundlage der sozialen Koordination dienen könnten. Es werden deshalb zusätzlich verbindliche und sanktionierte Normen benötigt, die für jedes Individuum das Handeln der anderen berechenbar macht.

Teleologische Ethiken sind in der Regel Güter-Ethiken; sie bezeichnen bestimmte Güter (z. B. „Glück“ oder „Lust“) als für den Menschen gut und damit erstrebenswert.

Schon David Hume hat den Einwand erhoben, dass der Übergang von Seins- zu Sollensaussagen nicht legitim sei („Humes Gesetz“). Unter dem Stichwort „Naturalistischer Fehlschluss“ hat George Edward Moore damit eng verwandte Fragen aufgeworfen, die aber genau genommen nicht dieselben sind.

Hume kritisiert an den ihm bekannten Moralsystemen,
Für Hume sind logische Schlussfolgerungen von dem, was ist, auf das, was sein soll, unzulässig, denn durch logische Umformungen könne aus Ist-Sätzen kein völlig neues Bedeutungselement wie das Sollen hergeleitet werden.

Wie später die Positivisten betont haben, muss erkenntnistheoretisch zwischen Ist-Sätzen und Soll-Sätzen wegen ihres unterschiedlichen Verhältnisses zur Sinneswahrnehmung differenziert werden. Während der Satz „Peter ist um 14 Uhr am Bahnhof gewesen“ durch intersubjektiv übereinstimmende Beobachtungen überprüfbar, also verifizierbar oder falsifizierbar ist, lässt sich der Satz „Peter soll um 14 Uhr am Bahnhof sein“ mit den Mitteln von Beobachtung und Logik allein nicht begründen oder widerlegen.

Die erkenntnistheoretische Unterscheidung zwischen Sein und Sollen liegt den modernen Erfahrungswissenschaften zugrunde. Wer diese Unterscheidung nicht akzeptiert, der muss entweder ein Sein postulieren, das nicht direkt oder indirekt wahrnehmbar ist, oder er muss das Gesollte für sinnlich wahrnehmbar halten. Beiden Positionen mangelt es bisher an einer intersubjektiven Nachprüfbarkeit.

Die vermeintliche Herleitung ethischer Normen aus Aussagen über das Seiende wird oft nur durch die unbemerkte Ausnutzung der normativ-empirischen Doppeldeutigkeit von Begriffen wie „Wesen“, „Natur“, „Bestimmung“, „Funktion“, „Zweck“, „Sinn“ oder „Ziel“ erreicht.

So bezeichnet das Wort „Ziel“ einmal das, was ein Mensch "tatsächlich anstrebt" („Sein Ziel ist das Diplom“). Das Wort kann jedoch auch das bezeichnen, was ein Mensch "anstreben sollte" („Wer nur am Materiellen ausgerichtet ist, der verfehlt das wahre Ziel des menschlichen Daseins“).

Die unbemerkte empirisch-normative Doppeldeutigkeit bestimmter Begriffe führt dann zu logischen Fehlschlüssen wie: „Das Wesen der Sexualität ist die Fortpflanzung. Also ist Empfängnisverhütung nicht erlaubt, denn sie entspricht nicht dem Wesen der Sexualität.“

Aus der logischen Unterscheidung von Sein und Sollen folgt jedoch keineswegs, dass damit eine auf Vernunft gegründete Ethik unmöglich ist, wie dies sowohl von Vertretern des logischen Empirismus als auch des Idealismus geäußert wird. Zwar lässt sich allein auf Empirie und Logik keine Ethik gründen, aber daraus folgt noch nicht, dass es nicht andere allgemein nachvollziehbare Kriterien für die Gültigkeit ethischer Normen gibt. Ein aussichtsreiches Beispiel für eine nachpositivistische Ethik ist die am Kriterium des zwangfreien Konsenses orientierte Diskursethik.

Mit der Feststellung, dass das Gesollte nicht aus dem Seienden logisch ableitbar ist, wird eine Begründung von Normen noch nicht aussichtslos. Denn neben den Seinsaussagen und den normativen Sätzen gibt es Willensäußerungen. Die Willensäußerung einer Person: „Ich will in der nächsten Stunde von niemandem gestört werden“ beinhaltet die Norm: „Niemand soll mich in der nächsten Stunde stören“. Die Aufgabe der Ethik ist es, allgemeingültige Willensinhalte bzw. Normen zu bestimmen und nachvollziehbar zu begründen.

Die logische Unterscheidung zwischen Ist-Sätzen und Soll-Sätzen wird vor allem von Vertretern idealistischer Positionen als eine unzulässige Trennung von Sein und Sollen angesehen und es wird eingewandt, dass ihr ein verkürzter Seinsbegriff zugrunde liege. So argumentiert Vittorio Hösle, das Sollen könne nur vom realen, empirischen Sein strikt abgegrenzt werden, „... ein ideales Sein, das nicht vom Menschen gesetzt ist, wird dem Sollen damit ebenso wenig abgesprochen wie eine mögliche Prinzipiierungsfunktion gegenüber dem empirischen Sein“. Es könne gerade als Aufgabe des Menschen angesehen werden, „damit fertig zu werden, dass das Sein nicht so ist, wie es sein soll“. Das Gesollte solle eben sein und sei als solches bereits Prinzip des Seins:

Die Möglichkeit einer teleologischen Ethik scheint mit der logischen Unterscheidung von Seins- und Sollens-Aussagen grundsätzlich in Frage gestellt. Aus Sicht der klassischen Position des Realismus bezüglich der Ethik, insbesondere des Naturrechts, ist es aber gerade das Sein, aus dem das Sollen abgeleitet werden muss, da es (außer dem Nichts) zum Sein keine Alternative gibt. Weil das Gute das Seinsgerechte, also das dem jeweiligen Seienden gerechte bzw. entsprechende ist, muss demnach das Wesen des Seins zunächst erkannt und aus ihm die Forderung des Sollens (ihm gegenüber) logisch abgeleitet werden.

Trotz der teilweise apokalyptischen geschichtlichen Ereignisse des 20. Jahrhunderts wird der Begriff „böse“ in der Umgangssprache nur noch selten gebraucht. Stattdessen werden meist die Begriffe „schlecht“ („ein schlechter Mensch“) oder „falsch“ („die Handlung war falsch“) verwendet. Das Wort „böse“ gilt im gegenwärtigen Bewusstsein generell als metaphysikverdächtig und aufgrund der allgemeinen Dominanz des naturwissenschaftlichen Denkens als überholt.

In der philosophischen Tradition wird das Böse als eine Form des Übels betrachtet. Klassisch geworden ist die Unterscheidung von Leibniz zwischen einem metaphysischen (malum metaphysicum), einem physischen (malum physicum) und einem moralischen Übel (malum morale). Das metaphysische Übel besteht in der Unvollkommenheit alles Seienden, das physische Übel in Schmerz und Leid. Diese Übel sind Widrigkeiten, die ihren Ursprung in der Natur haben. Sie sind nicht „böse“, da sie nicht das Ergebnis des (menschlichen oder allgemeiner gesagt geistigen) Willens sind. Das moralische Übel oder das Böse hingegen besteht in der Nicht-Übereinstimmung einer Handlung mit dem Sittengesetz bzw. Naturrecht. Es kann, wie Kant betont, nur „die Handlungsart, die Maxime des Willens und mithin die handelnde Person selbst“ böse sein. Das Böse ist also als Leistung oder besser Fehlleistung des Subjekts zu verstehen.

Die Verhaltensforschung führt das Böse auf die allgemeine „Tatsache“ der Aggression zurück. Diese sei einfachhin ein Bestandteil der menschlichen Natur und als solcher moralisch irrelevant. Daher spricht Konrad Lorenz auch vom „sogenannten Bösen“. Dieser Erklärung wird von Kritikern eine reduktionistische Betrachtungsweise vorgeworfen. Sie übersehe, dass dem Menschen auf der Grundlage der Freiheit die Möglichkeit gegeben ist, zu seiner eigenen Natur Stellung zu nehmen.

In der Philosophie stellte sich bereits Platon die Frage, wie das Böse überhaupt möglich sei. Das Böse werde nur getan, weil jemand im irrtümlichen Glauben annimmt, er (oder jemand) habe einen Nutzen davon. Somit wolle er aber den mit dem Bösen verbundenen Nutzen. Das Böse um seiner selbst willen könne niemand vernünftigerweise wollen:

Dieses in der Antike noch weit verbreitete Verständnis, das Böse ließe sich durch die Vernunft überwinden, wird allerdings durch die geschichtlichen Erfahrungen, insbesondere die des 20. Jahrhunderts in Frage gestellt. Diese lehren in den Augen vieler Philosophen der Gegenwart, dass der Mensch durchaus im Stande sei, das Böse auch um seiner selbst willen zu wollen.

Als Motiv für das Böse kann zunächst einmal der Egoismus ausgemacht werden. Er äußert sich in vielen Spielarten. In seiner harmlosen Variante zeigt er sich im Ideal einer selbstbezogenen Bedürfnisbefriedigung. In dieser Form stellt er letztlich auch die „Vertragsgrundlage“ des Utilitarismus dar, der nichts anderes als einen Interessensausgleich zwischen den Individuen schaffen möchte. Dieser Aspekt trifft – wie die geschichtliche Erfahrung zeigt – noch nicht den eigentlichen Kern des Bösen. Dieser wird erst dann sichtbar, wenn die eigene Bedürfnisbefriedigung nicht mehr im Vordergrund steht:

Die Ursache dieses „radikal Bösen“ ist nach Kant weder in der Sinnlichkeit noch in der Vernunft zu sehen, sondern in einer „Verkehrtheit des Herzens“, in der sich das Ich gegen sich selbst wendet:

Dieser Grundgedanke Kants von der Selbstwidersprüchlichkeit des Ichs als Ursache des Bösen wird vor allem in der Philosophie des Idealismus noch einmal vertieft. Schelling unterscheidet zwischen einem alle Bindung verneinenden „Eigenwillen“ und einem sich in Beziehungen gestaltenden „Universalwillen“. Die Möglichkeit zum Bösen bestehe darin, dass der Eigenwille sich seiner Integration in den Universalwillen widersetzt.

Das radikal Böse bewirke einen Umsturz der Ordnung in mir selbst und in Bezug zu anderen. Es erfolge um seiner selbst willen, denn „wie es einen Enthusiasmus zum Guten gibt, ebenso gibt es eine Begeisterung des Bösen“.

Nach der klassischen Lehre (Augustinus, Thomas von Aquin etc.) ist das Böse selbst letztlich substanzlos. Als privativer Gegensatz des Guten besteht es nur in einem Mangel (an Gutem). Im Gegensatz zum absolut Guten (Gott) gibt es demnach das absolut Böse nicht.

Das Durchsetzungsproblem der Ethik besteht darin, dass die Einsicht in die Richtigkeit ethischer Prinzipien zwar vorhanden sein kann, daraus aber nicht automatisch folgt, dass der Mensch auch im ethischen Sinne handelt. Die Einsicht in das richtige Handeln bedarf einer zusätzlichen Motivation oder eines Zwangs.

Das Problem erklärt sich daraus, dass die Ethik einerseits und das menschliche Eigeninteresse als Egoismus andererseits oft einen Gegensatz bilden. Das Durchsetzungsproblem gewinne zudem durch die weltweite Globalisierung eine neue Dimension, die zu einer "Ethik der Neomoderne" führe.

Die Tatsache, dass die Menschen im Land X Hunger leiden und ihnen geholfen werden sollte, ja es moralisch geboten erscheint ihnen zu helfen, wird niemand bestreiten. Die Einsicht es auch zu tun, einen Großteil seines Vermögens dafür herzugeben, wird es im nennenswerten Umfang erst geben, wenn eine zusätzliche Motivation auftaucht, etwa die Gefahr einer Migration wegen Hungers ins eigene Land unmittelbar bevorsteht.

Das Durchsetzungsproblem zeigt sich auf andere Weise auch in der Erziehung, etwa wenn fest verinnerlichte Verhaltensregeln später auf entwickelte ethische Prinzipien stoßen.

Erkenntnisse der Evolutionären Spieltheorie lassen Rückschlüsse darauf zu, dass das Durchsetzungsproblem durch Selbstdurchdringung gelöst werden kann. Diese Auffassung vertraten zuerst Vertreter der Neuen Institutionenökonomik. So wiesen Eirik Furubotn und Rudolf Richter darauf hin, dass der Aufbau einer Reputation eine dominate Spielstrategie sein kann.

Die angewandte Ethik ist ein Teilbereich der allgemeinen Ethik. Teilbereiche der angewandten Ethik (oder Bereichsethik) sind beispielsweise Medizinethik, Umweltethik und Wirtschaftsethik. Aufgabe der verschiedenen Bereichsethiken ist es, in Kommissionen, auf Instituten usw. Normen oder Handlungsempfehlungen für bestimmte Bereiche zu erarbeiten.

Im deutschsprachigen Raum gibt es zahlreiche Ethik-Institute, die sich mit den weiten Problemfeldern der angewandten Ethik beschäftigen:


In einer Reihe von Fachgebieten werden Ethikrate eingesetzt, welche begleitend zu oder auch "in" größeren Institutionen die Betrachtung ethischer Aspekte sicherstellen sollen. Diese sind teils gesetzlich fundiert wie beim Deutschen Ethikrat für die Medizin, teils nur per Proklamation aus einer Organisation selbst gebildet.





</doc>
<doc id="1295" url="https://de.wikipedia.org/wiki?curid=1295" title="Exponentialfunktion">
Exponentialfunktion

In der Mathematik bezeichnet man als Exponentialfunktion eine Funktion der Form formula_1 mit einer reellen Zahl formula_2 als Basis (Grundzahl). In der gebräuchlichsten Form sind dabei für den Exponenten formula_3 die reellen Zahlen zugelassen. Im Gegensatz zu den Potenzfunktionen, bei denen die Basis die unabhängige Größe (Variable) und der Exponent fest vorgegeben ist, ist bei Exponentialfunktionen der Exponent (auch Hochzahl) des Potenzausdrucks die Variable und die Basis fest vorgegeben. Darauf bezieht sich auch die Namensgebung. Exponentialfunktionen haben in den Naturwissenschaften, z. B. bei der mathematischen Beschreibung von Wachstumsvorgängen, eine herausragende Bedeutung (siehe exponentielles Wachstum).

Als "die" Exponentialfunktion im engeren Sinne (präziser eigentlich "natürliche Exponentialfunktion") bezeichnet man die e-Funktion, also die Exponentialfunktion formula_4 mit der eulerschen Zahl formula_5 als Basis; gebräuchlich hierfür ist auch die Schreibweise formula_6 . Diese Funktion hat gegenüber den anderen Exponentialfunktionen besondere Eigenschaften. Unter Verwendung des natürlichen Logarithmus lässt sich mit der Gleichung formula_7 jede Exponentialfunktion auf eine solche zur Basis formula_8 zurückführen. Deshalb befasst sich dieser Artikel im Wesentlichen mit der Exponentialfunktion zur Basis formula_8. 

Bisweilen unterscheidet man im Deutschen auch zwischen exponentiellen Funktionen (allgemein) und der Exponentialfunktion (zur Basis "e").

Die Exponentialfunktion zu der Basis formula_8 kann auf den reellen Zahlen auf verschiedene Weisen definiert werden. 

Eine Möglichkeit ist die Definition als Potenzreihe, die sogenannte Exponentialreihe
wobei formula_12 die Fakultät von formula_13 bezeichnet.

Eine weitere Möglichkeit ist die Definition als Grenzwert einer Folge mit formula_14:

Beide Arten sind auch zur Definition der komplexen Exponentialfunktion formula_16 auf den komplexen Zahlen geeignet (s. weiter unten).

Die reelle Exponentialfunktion formula_17 ist positiv, stetig, streng monoton wachsend und surjektiv. Dabei bezeichnet formula_18 die Menge der positiven reellen Zahlen.

Sie ist folglich bijektiv.
Deshalb existiert ihre Umkehrfunktion, der natürliche Logarithmus formula_19. 

Daraus erklärt sich auch die Bezeichnung Antilogarithmus für die Exponentialfunktion.

Die punktweise Konvergenz der für die Definition der Exponentialfunktion verwendeten Reihe 
lässt sich für alle reellen und komplexen formula_21 einfach mit dem Quotientenkriterium zeigen; daraus folgt sogar absolute Konvergenz. Der Konvergenzradius der Potenzreihe ist also unendlich. Da Potenzreihen an jedem inneren Punkt ihres Konvergenzbereiches analytisch sind, ist die Exponentialfunktion also in jedem reellen und komplexen Punkt trivialerweise auch stetig.

Da die Exponentialfunktion die Funktionalgleichung formula_22 erfüllt, kann man mit ihrer Hilfe das Potenzieren auf reelle und komplexe Exponenten verallgemeinern, indem man definiert:
für alle formula_24 und alle reellen oder komplexen formula_3.

Generell gilt diese Umformung von formula_26 auch für beliebige andere Werte formula_27 als neue Basis:

Solche Funktionen heißen exponentielle Funktionen und „verwandeln“ Multiplikation in Addition.
Genauer zeigen das die folgenden Gesetze:

Diese Gesetze gelten für alle positiven reellen formula_36 und formula_37 und alle reellen formula_3 und formula_39.
Ausdrücke mit Brüchen und Wurzeln können oft mit Hilfe der Exponentialfunktion vereinfacht werden:

Siehe auch Rechenregeln für Logarithmus.

Die große Bedeutung der e-Funktion, eben die Exponentialfunktion mit Basis formula_8, beruht auf der Tatsache, dass ihre Ableitung wieder die e-Funktion ergibt:

Wenn man zusätzlich

fordert, ist die e-Funktion sogar die einzige Funktion formula_45, die dies leistet. Somit kann man die e-Funktion auch als Lösung dieser Differentialgleichung mit dieser Anfangsbedingung definieren. 

Allgemeiner folgt für reelles formula_46 aus

und der Kettenregel die Ableitung beliebiger Exponentialfunktionen:

In dieser Formel kann der natürliche Logarithmus nicht durch einen Logarithmus zu einer anderen Basis ersetzt werden; die Zahl "e" kommt also in der Differentialrechnung auf „natürliche“ Weise ins Spiel.

Mit Hilfe der Reihendarstellung
lässt sich die Exponentialfunktion für komplexe Zahlen formula_50 definieren. Die Reihe konvergiert für alle formula_51 absolut. 

Die Exponentialfunktion behält für alle komplexen Zahlen formula_50, formula_53 folgende wichtige Eigenschaften:

Die Exponentialfunktion ist somit ein surjektiver, aber nicht injektiver Gruppenhomomorphismus von der abelschen Gruppe formula_58 auf die abelsche Gruppe formula_59, also von der additiven auf die multiplikative Gruppe des Körpers formula_60.

In formula_61 hat die Exponentialfunktion eine wesentliche Singularität, ansonsten ist sie holomorph, d. h., sie ist eine ganze Funktion. 
Die komplexe Exponentialfunktion ist periodisch mit der komplexen Periode 2πi, es gilt also
Beschränkt man ihren Definitionsbereich auf einen Streifen
mit formula_64, dann besitzt sie eine wohldefinierte Umkehrfunktion, den komplexen Logarithmus.

Die Exponentialfunktion kann zur Definition der trigonometrischen Funktionen für komplexe Zahlen verwendet werden:
Dies ist äquivalent zur eulerschen Formel

Daraus abgeleitet ergibt sich speziell die Gleichung 
der in Physik und Technik wichtigen komplexen Exponentialschwingung mit der Kreisfrequenz formula_69 und der Frequenz formula_70. 

Ebenso kann die Exponentialfunktion zur Definition der hyperbolischen Funktionen verwendet werden:

Man kann auch im Komplexen eine allgemeine Potenz definieren:
Die Werte der Potenzfunktion sind dabei abhängig von der Wahl des Einblättrigkeitsbereichs des Logarithmus, siehe auch Riemannsche Fläche. Dessen Mehrdeutigkeit wird ja durch die Periodizität seiner Umkehrfunktion, eben der Exponentialfunktion, verursacht. Deren grundlegende Gleichung

entspringt der Periodizität der Exponentialfunktion formula_77 mit reellem Argument formula_3. Deren Periodenlänge ist genau der Kreisumfang formula_79 des Einheitskreises, den die Sinus- und Kosinusfunktionen wegen der Eulerschen Formel beschreiben. Die Exponential-, die Sinus- und die Kosinusfunktion sind nämlich nur Teile derselben (auf komplexe Zahlen verallgemeinerten) Exponentialfunktion, was im Reellen nicht offensichtlich ist.

Die Exponentialfunktion lässt sich auf Banachalgebren, zum Beispiel Matrix-Algebren mit einer Operatornorm, verallgemeinern. Sie ist dort ebenfalls über die Reihe
definiert, die für alle beschränkten Argumente aus der jeweils betrachteten Banachalgebra absolut konvergiert.

Die wesentliche Eigenschaft der reellen (und komplexen) Exponentialfunktion
ist in dieser Allgemeinheit allerdings nur noch gültig für Werte formula_3 und formula_39, die kommutieren, also für Werte mit formula_84 (dies ist in den reellen oder komplexen Zahlen natürlich immer erfüllt, da die Multiplikation dort kommutativ ist).
Einige Rechenregeln dieser Art für die Exponentiale von linearen Operatoren auf einem Banachraum liefern die Baker-Campbell-Hausdorff-Formeln.

Eine wichtige Anwendung dieser verallgemeinerten Exponentialfunktion findet sich beim Lösen von linearen Differentialgleichungssystemen der Form formula_85 mit konstanten Koeffizienten. In diesem Fall ist die Banachalgebra die Menge der formula_86-Matrizen mit komplexen Einträgen. Mittels der jordanschen Normalform lässt sich eine Basis bzw. Ähnlichkeitstransformation finden, in welcher die Exponentialmatrix eine endliche Berechnungsvorschrift hat. Genauer gesagt, man findet eine reguläre Matrix formula_87, so dass formula_88, wobei formula_89 eine Diagonalmatrix und formula_90 eine nilpotente Matrix sind, welche miteinander kommutieren. Es gilt damit
Das Exponential einer Diagonalmatrix ist die Diagonalmatrix der Exponentiale, das Exponential der nilpotenten Matrix ist ein matrixwertiges Polynom mit einem Grad, der kleiner als die Dimension formula_13 der Matrix formula_93 ist.

Als fundamentale Funktion der Analysis wurde viel über Möglichkeiten zur effizienten Berechnung der Exponentialfunktion bis zu einer gewünschten Genauigkeit nachgedacht. Dabei wird stets die Berechnung auf die Auswertung der Exponentialfunktion in einer kleinen Umgebung der Null reduziert und mit dem Anfang der Potenzreihe gearbeitet. In der Analyse ist die durch die Reduktion notwendige Arbeitsgenauigkeit gegen die Anzahl der notwendigen Multiplikationen von Hochpräzisionsdaten abzuwägen.

Der Rest der formula_90-ten Partialsumme hat eine einfache Abschätzung gegen die geometrische Reihe, welche auf

Die einfachste Reduktion benutzt die Identität formula_99 , d. h. zu gegebenem formula_3 wird formula_101 bestimmt, wobei formula_102 nach den Genauigkeitsbetrachtungen gewählt wird. Damit wird nun, in einer gewissen Arbeitsgenauigkeit, formula_103 berechnet und formula_102-fach quadriert: formula_105. formula_106 wird nun auf die gewünschte Genauigkeit reduziert und als formula_107 zurückgegeben.

Effizientere Verfahren setzen voraus, dass formula_108, besser zusätzlich formula_109 und formula_110 (Arnold Schönhage) in beliebiger (nach Spezifikation auftretender) Arbeitsgenauigkeit verfügbar sind. Dann können die Identitäten
benutzt werden, um formula_3 auf ein formula_39 aus dem Intervall formula_115 oder einem wesentlich kleineren Intervall zu transformieren und damit das aufwändigere Quadrieren zu reduzieren oder ganz zu vermeiden.

Auf die Exponentialfunktion stößt man, wenn man versucht, das Potenzieren auf beliebige reelle Exponenten zu verallgemeinern. Man geht dabei von der Rechenregel formula_116 aus und sucht daher eine Lösung der Funktionalgleichung formula_117 mit formula_118. Nimmt man nun zunächst einmal an, dass eine Lösung tatsächlich existiert, und berechnet deren Ableitung, so stößt man auf den Ausdruck
definierte Zahl formula_8 (bzw. formula_121, formula_122 muss dann also der Logarithmus zur Basis formula_8 sein) nach der Kettenregel formal
formula_8 erfüllt dann vermutlich

Wie kann man diese Zahl formula_8 berechnen?
Setzt man rein formal formula_128 und löst die Gleichung
ist also zu vermuten, dass
gilt.

Für formula_133 erhält man mit formula_134 auch rein formal die Darstellung
also die eine Definition der Exponentialfunktion.

Alternativ kann man auch versuchen, die Funktion 


</doc>
<doc id="1296" url="https://de.wikipedia.org/wiki?curid=1296" title="Erasable Programmable Read-Only Memory">
Erasable Programmable Read-Only Memory

Ein EPROM (engl. Abk. für , wörtlich: "löschbarer programmierbarer Nur-Lese-Speicher") ist ein nichtflüchtiger elektronischer Speicherbaustein, der bis etwa in die Mitte der 1990er-Jahre vor allem in der Computertechnik eingesetzt wurde, inzwischen aber weitgehend durch EEPROMs abgelöst ist.

Dieser Bausteintyp ist mit Hilfe spezieller Programmiergeräte (genannt „EPROM-Brenner“) programmierbar. Er lässt sich mittels UV-Licht löschen und danach neu programmieren. Nach etwa 100 bis 200 Löschvorgängen hat das EPROM das Ende seiner Lebensdauer erreicht. Das zur Löschung nötige Quarzglas-Fenster (normales Glas ist nicht UV-durchlässig) macht das Gehäuse relativ teuer. Daher gibt es auch Bauformen ohne Fenster, die nominal nur einmal beschreibbar sind (, OTP), sich durch Röntgenstrahlung aber ebenfalls löschen lassen.

Das EPROM wurde 1970 bei Intel von Dov Frohman entwickelt.

Ein EPROM enthält eine Matrix aus Speicherzellen, in denen jeweils ein Transistor ein Bit repräsentiert. Eine Speicherzelle besteht aus einem MOSFET-Transistor mit einer zusätzlichen Gateelektrode zwischen Gate und Kanal, die jedoch keinen Anschluss besitzt. Es kann daher frei ein Potential annehmen und wird deshalb "Floating Gate" genannt. Es ist in einer sehr dünnen Siliciumdioxid-Schicht eingebettet. Bei normalen Betriebsverhältnissen können keine Elektronen hingelangen oder es verlassen. Zum Programmieren wird eine erhöhte Spannung an das Gate angelegt, sodass das "Floating Gate" geladen wird, indem energiereichere Elektronen durch die dünne Isolierschicht tunneln. Dadurch verschiebt sich die Ansteuerspannung, bei der der Transistor einschaltet (Schwellspannung oder "threshold"). Die Daten lassen sich nun beliebig oft auslesen, wobei die Lesespannung unterhalb der Programmierspannung liegt.

Zum Löschen wird kurzwellige Ultraviolettstrahlung verwendet, typischerweise 245 nm (4,9 eV) von Quecksilberdampflampen. Dadurch werden Fotoelektronen angeregt, die ausreichende Energie haben, die Isolierbarriere zu überwinden - die "Floating Gates" werden entladen. Das Bitmuster ist dadurch gelöscht und das EPROM in seinen ursprünglichen Zustand zurückversetzt. Es entstehen durch die harte UV Strahlung jedoch auch immer Defekte, sodass das Löschen nicht beliebig oft erfolgen kann. EPROMs sind zum Löschen mit einem UV-B-transparenten Fenster versehen. Es besteht meist aus Kieselglas, selten auch aus hochreiner transluzenter Aluminiumoxid-Keramik (DDR-Typen, z.B. U2732). Auch die fensterlosen nur einmal beschreibbaren Typen (OTP für "One Time Programmable") lassen sich mit Röntgenstrahlung löschen, da diese auch ohne Fenster durch das Gehäuse dringt und der Baustein selbst bis auf das Gehäuse der gleiche ist.

Ein konventioneller Löschvorgang dauert ca. 10 bis 30 Minuten. Da die Ionisation nach dem Ausschalten der Lichtquelle nicht sofort wieder abgeklungen ist und die Bausteine je nach Bauart des Löschgerätes auch über die für das Programmieren zulässige Temperatur hinaus erhitzt werden, kann das Programmieren erst nach einer weiteren Wartezeit erfolgen. Die Zeiten können durch den Einsatz von Löschgeräten mit Blitzlampen deutlich verkürzt werden. Statt einer kontinuierlichen Bestrahlung werden dabei Lichtblitze verwendet. Falls die Vorgaben des Bausteinherstellers für das Löschen nicht korrekt eingehalten werden, kann eine scheinbar richtige Programmierung mit verkürzter Datenlebensdauer die Folge sein.

Das Quarzglas-Fenster sollte nach dem Programmieren mit einem lichtundurchlässigen Aufkleber geschützt werden. Ein ungeschütztes EPROM kann nach ca. 90 Tagen direkter Sonneneinstrahlung gelöscht sein. Die Beleuchtung der Speicherzellen mit einem gewöhnlichen Fotoblitzgerät kann kurzzeitige Datenverfälschung und damit Computerausfälle verursachen. 

Übliche EPROMs hatten 8 Bit breite Datenpfade, und die Gesamtspeicherkapazität war in der Bezeichnung enthalten. So enthielt ein 2764 64 KiBit, die als 8 Ki × 8 organisiert waren.

Eine Weiterentwicklung des EPROM ist das elektrisch löschbare EEPROM () und das Flash-EEPROM. Flash-EEPROMs haben mittlerweile die EPROMs weitgehend vom Markt verdrängt.

Wie andere integrierte Schaltkreise sind die gängigen EPROMs durch die JEDEC in ihrer Pinbelegung standardisiert.

Da EPROMs nicht unbegrenzt wiederbeschreibbar sind, werden in der Entwicklungsphase von elektronischen Geräten Simulatoren verwendet. Diese gibt es in verschiedenen Varianten. Zum Beispiel gibt es Simulatoren mit USB-Anschluss, die EPROMs bis zu 4 MiBit Größe simulieren. Bei diesen Geräten wird der Programmcode über USB in den Simulator geladen und das simulierte EPROM in den Schaltungsaufbau eingefügt, beispielsweise über einen Steckadapter. Es kann sofort mit der Simulation begonnen werden. Die zu testende Schaltung verhält sich dabei genau so, als wenn ein echter EPROM-Baustein eingebaut wäre. Eine bei vorhandenem EPROM-Programmiergerät sehr kostengünstige Lösung bieten auch schon einfache Simulatoren aus batteriegepufferten RAM-Bausteinen mit Schreibschutzschalter, die am EPROM-Programmiergerät programmiert und danach mit aktiviertem Schreibschutz auf die Testschaltung gesteckt werden.



</doc>
<doc id="1298" url="https://de.wikipedia.org/wiki?curid=1298" title="Enigma (Maschine)">
Enigma (Maschine)

Die Enigma ( "aínigma" „Rätsel“, Eigenschreibweise auch: ENIGMA) ist eine Rotor-Schlüsselmaschine, die im Zweiten Weltkrieg zur Verschlüsselung des Nachrichtenverkehrs der Wehrmacht verwendet wurde. Auch Polizei, Geheimdienste, diplomatische Dienste, SD, SS, Reichspost und Reichsbahn setzten sie zur geheimen Kommunikation ein. Trotz mannigfaltiger vor und während des Krieges eingeführter Verbesserungen der Verschlüsselungsqualität gelang es den Alliierten mit hohem personellen und maschinellen Aufwand, die deutschen Funksprüche nahezu kontinuierlich zu entziffern.

Nach dem Ersten Weltkrieg suchten die deutschen Militärs nach einem Ersatz für die inzwischen veralteten, umständlichen und unsicheren manuellen Verschlüsselungsverfahren, wie ÜBCHI, ABC-Chiffre und ADFGX, die bis dahin verwendet wurden. Hierfür kamen maschinelle Verfahren in Betracht, weil sie eine einfachere Handhabung und eine verbesserte kryptographische Sicherheit versprachen. Mit der Einführung der elektrischen Schreibmaschine und des Fernschreibers zu Beginn des 20. Jahrhunderts kamen unabhängig voneinander und fast gleichzeitig mehrere Erfinder auf die Idee des Rotor-Prinzips zur Verschlüsselung von Texten. Die ersten waren 1915 in Batavia (damals Hauptstadt von Niederländisch-Ostindien, heute Jakarta, Hauptstadt von Indonesien) die beiden niederländischen Marineoffiziere Theo A. van Hengel und R.P.C. Spengler. Ihnen wurde jedoch nicht gestattet, ihre Erfindung zum Patent anzumelden. Der nächste war im Jahr 1917 der Amerikaner Edward Hugh Hebern (Patentanmeldung 1921). Im Jahr 1918 folgte der Deutsche Arthur Scherbius und schließlich 1919 der Niederländer Hugo Koch und der Schwede Arvid Gerhard Damm, die alle ihre Ideen zu Rotor-Chiffriermaschinen zum Patent anmeldeten.

Als Erfinder der Enigma gilt der promovierte deutsche Elektroingenieur Arthur Scherbius (1878–1929), dessen erstes Patent hierzu vom 23. Februar 1918 stammt (siehe auch: Enigma-Patente). Noch im selben Jahr am 15. April bot er seine neue Erfindung der Kaiserlichen Marine an, die aber den Einsatz einer maschinellen Verschlüsselung für nicht erforderlich erachtete und ihn zurückwies. Nach dem Krieg beschloss er, die Maschine für zivile Anwendungen zu vermarkten. Zur Fertigung wurde am 9. Juli 1923 die Chiffriermaschinen-Aktiengesellschaft (ChiMaAG) in Berlin (W 35, Steglitzerstr. 2, heute Pohlstraße, Berlin-Tiergarten) gegründet. Das erste Modell der Enigma, genannt Enigma-A , wurde kommerziell auf Messen zum Kauf angeboten, wie 1923 in Leipzig und Bern und 1924 auf dem internationalen Postkongress des Weltpostvereins in Stockholm.

Dies weckte das Interesse auch des deutschen Militärs, das inzwischen durch Veröffentlichungen, wie Winston Churchills „The World Crisis“ und Julian Corbetts „Naval Operations“, von den alliierten Entzifferungserfolgen erfahren hatte. Dazu gehörte die britische Entzifferung der deutschen Marinefunksprüche, was mithilfe des durch verbündete russische Taucher vom gestrandeten Kreuzer "Magdeburg" geborgenen deutschen Signalbuchs (Codebuch) gelang, die französische Entzifferung von ÜBCHI, einer frühen Handschlüsselmethode des Kaiserlichen Heers, sowie deren Nachfolgerinnen, der ABC- und ABCD-Chiffre, ferner die britische Entzifferung der Zimmermann-Depesche, worauf der Kriegseintritt der USA erfolgte, und die französische Entzifferung der ADFGX- sowie ADFGVX-Chiffre, was im "Radiogramme de la Victoire" () gipfelte.

Da die deutschen Militärs eine Wiederholung dieser kryptographischen Katastrophe des Ersten Weltkriegs unbedingt vermeiden wollten, erkannten sie die neue Art der maschinellen Verschlüsselung als sicherste Lösung. Im Jahr 1926 wurde die Enigma zunächst von der Reichsmarine unter dem Namen „Funkschlüssel C“, zwei Jahre später auch vom Heer versuchsweise eingesetzt und verschwand daraufhin vom zivilen Markt. Kurz nach Beginn der Serienfertigung verunglückte Scherbius im Jahr 1929 tödlich. Im Jahr 1934 übernahmen Rudolf Heimsoeth und Elsbeth Rinke die ChiMaAG. Unter der neuen Firma „Heimsoeth & Rinke“ (H&R) setzten sie Entwicklung und Produktion der Maschine in Berlin fort. Die Zeit des Nationalsozialismus hatte bereits begonnen. Im Zuge der Aufrüstung der Wehrmacht wurde ein zuverlässiges Verschlüsselungssystem benötigt, und so stand dem Erfolg der Enigma nichts mehr im Wege.

Man schätzt, dass etwas mehr als 40.000 Maschinen hergestellt wurden. Im Laufe der Zeit bis zum Kriegsende 1945 und noch darüber hinaus, so 1965 in , kamen viele verschiedene Modelle und Varianten der Enigma zum Einsatz. Die meistgebrauchte war die Enigma I (sprich: „Enigma eins“), die ab 1930 von der Reichswehr und später von der Wehrmacht eingesetzt wurde und die während des Zweiten Weltkriegs das auf deutscher Seite am häufigsten benutzte Maschinenschlüsselverfahren verkörperte.

Die Enigma I inklusive Holzgehäuse wiegt rund 12 kg und die äußeren Abmessungen (L×B×H) betragen etwa 340 mm × 280 mm × 150 mm (Daten ohne Gehäuse: 10,35 kg und 310 mm × 255 mm × 130 mm). Ihr Erfinder sagt: „Die Maschine ist ganz ähnlich einer Schreibmaschine gebaut und wird auch genau wie diese bedient.“ Sie besteht im Wesentlichen aus der Tastatur zur Buchstabeneingabe, einem Walzensatz von drei austauschbaren Walzen (Rotoren mit einem Durchmesser von etwa 100 mm) und einem Glühlampenfeld zur Anzeige. Der Walzensatz ist das Herzstück zur Verschlüsselung. Die drei Walzen sind nebeneinander unabhängig drehbar angeordnet. Jede von ihnen weist auf beiden Seiten 26 elektrische Kontakte auf. Jeder Kontakt ist einem der 26 Großbuchstaben des lateinischen Alphabets zugeordnet. Jeweils ein Kontakt auf der einen Seite einer Walze ist durch einen isolierten Draht im Inneren der Walze mit einem Kontakt auf der anderen Seite der Walze verbunden. Insgesamt sind so, für jede Walze unterschiedlich, alle 26 Kontakte auf der einen Seite einer Walze paarweise und unregelmäßig mit den 26 Kontakten auf der anderen Seite elektrisch verbunden (siehe auch: Verdrahtungstabelle im folgenden Kapitel).

Drückt man eine Buchstabentaste, so fließt elektrischer Strom von einer in der Enigma befindlichen 4,5-Volt-Batterie über die gedrückte Taste durch den Walzensatz und lässt eine Anzeigelampe aufleuchten. Der aufleuchtende Buchstabe entspricht der Verschlüsselung des gedrückten Buchstabens. Da sich bei jedem Tastendruck die Walzen ähnlich wie bei einem mechanischen Kilometerzähler weiterdrehen, ändert sich das geheime Schlüsselalphabet nach jedem Buchstaben.

Gibt man „OTTO“ ein, so leuchten nacheinander beispielsweise die Lampen „PQWS“ auf. Wichtig und kryptographisch stark ist, dass aufgrund der Rotation der Walzen jeder Buchstabe auf eine andere Weise verschlüsselt wird, im Beispiel das vordere O von OTTO zu P, das hintere aber zu S. Man spricht von vielen unterschiedlichen (Geheim-) „Alphabeten“, die zur Verschlüsselung benutzt werden und bezeichnet dies als polyalphabetische Substitution. Im Gegensatz dazu verwendet eine monoalphabetische Substitution nur ein einziges Geheimalphabet, und ein Klartextbuchstabe wird stets in denselben Geheimtextbuchstaben verwandelt („OTTO“ beispielsweise in „GLLG“). Würden sich die Walzen der Enigma nicht drehen, so bekäme man auch bei ihr nur eine einfache monoalphabetische Verschlüsselung.

Rechts der drei drehbaren Walzen (5) des Walzensatzes (siehe gelb hinterlegte Zahlen in der Prinzipskizze) befindet sich die Eintrittswalze (4) (Stator), die sich nicht dreht und deren Kontakte über 26 Drähte (hier sind nur vier davon gezeichnet) mit den Buchstabentasten (2) verbunden sind. Links des Walzensatzes befindet sich die Umkehrwalze (6) (UKW), die bei der Enigma I ebenfalls feststeht. Bei der Umkehrwalze (auch genannt: Reflektor), handelt es sich um eine Erfindung (patentiert am 21. März 1926) von Willi Korn, einem Mitarbeiter von Scherbius. Sie weist nur auf ihrer rechten Seite 26 Kontakte auf (in der Skizze sind wieder nur vier davon eingezeichnet), die paarweise miteinander verbunden sind. Die Umkehrwalze bewirkt, dass der Strom, der den Walzensatz zunächst von rechts nach links durchläuft, umgelenkt wird und ihn noch einmal durchfließt, nun von links nach rechts. Der Strom verlässt den Walzensatz, wie er gekommen ist, wieder über die Eintrittswalze.

Die Tabelle zeigt das als „Geheime Kommandosache!“ eingestufte damals streng geheime Verdrahtungsschema der bei der Enigma I verfügbaren fünf drehbaren Walzen I bis V und der Umkehrwalzen A (bis 1937 gebraucht), B (ab 1937 im Einsatz) und C (1940 und 1941 sporadisch verwendet):

An der Gerätefront ist ein Steckerbrett mit doppelpoligen Steckbuchsen für jeden der 26 Buchstaben angebracht. Der Strom von der Buchstabentaste (2) wird, bevor er die Eintrittswalze (4) erreicht, über dieses Steckerbrett (3) geführt. Nach Durchlaufen des Walzensatzes fließt er ein zweites Mal über das Steckerbrett (7, 8) und bringt schließlich eine der 26 Buchstabenlampen (9) zum Aufleuchten. Die Buchstabenlampen sowie die Tastatur und die Steckbuchsen sind ähnlich wie bei einer deutschen Schreibmaschinentastatur angeordnet:

Bei einer gedrückten Buchstabentaste, beispielsweise A, wird der Batteriestrom über die Taste A zur gleichnamigen Buchse im Steckerbrett durchgeschaltet. Ist dort die Buchse A mit einer anderen Buchse durch ein von außen angebrachtes Kabel verbunden („gesteckert“), so wird A mit einem anderen Buchstaben, beispielsweise J, vertauscht. Ist kein Kabel gesteckt („ungesteckert“), dann gelangt der Strom direkt zum Kontakt A der Eintrittswalze.

Bei der weiteren Beschreibung der Funktion wird auf das Bild „Stromfluss“ (zunächst nur obere Hälfte) Bezug genommen. Es dient nur zur Illustration und ist eine vereinfachte Darstellung des rotierenden Walzensatzes (mit linkem, mittlerem und rechtem Rotor) und der statischen Umkehrwalze (englisch: "Reflector"). Aus Übersichtlichkeitsgründen wurde in der Skizze die Anzahl der Buchstaben von 26 auf 8 (nur A bis H) verringert.

Angenommen der Buchstabe A sei ungesteckert, dann wird der Strom über die Eintrittswalze (sie ist in der Skizze nicht eingezeichnet) zum Eingangskontakt A der rechten Walze geleitet. Deren Verdrahtung bewirkt eine Substitution (Ersetzung) des Buchstabens durch einen anderen. Der Strom, der am Eingangskontakt A von rechts eintritt, verlässt hier die Walze auf deren linken Seite am Ausgangskontakt B. So wird durch die rechte Walze A durch B ersetzt.

Der Strom gelangt nun über den Kontakt B in die mittlere Walze. Da es bei der Verdrahtung einer Walze durchaus möglich ist, dass (wie im Bild) ein Eingangskontakt mit dem gleichnamigen Ausgangskontakt verbunden ist, bleibt B hier unverändert. Der Strom verlässt über Kontakt B die mittlere Walze und tritt in die linke Walze ein. Deren Verdrahtung sorgt dafür, dass der Strom vom Eingangskontakt B zum Ausgangskontakt D geleitet wird. Der Strom hat nun alle drei (drehbaren) Walzen einmal durchlaufen und die Umkehrwalze erreicht. Sie hat nur Kontakte auf der rechten Seite und verbindet die Buchstaben paarweise, beispielsweise D mit E.

Nun fließt der Strom ein zweites Mal durch den Walzensatz, jetzt aber von links nach rechts. Durch die Umkehrwalze gelangt er über den Kontakt E in die linke Walze. Hier ist beispielsweise E mit C verdrahtet. Folglich fließt der Strom weiter über Kontakt C in die mittlere Walze, verlässt sie wieder über den Kontakt F und fließt in die rechte Walze. Der Strom verlässt die rechte Walze schließlich am Kontakt G.

Der weitere Stromfluss geht aus der Skizze nicht hervor, ist aber leicht erklärt. Nach Austritt aus dem Walzensatz wird der Strom über die Eintrittswalze zurück zum Steckerbrett geleitet. Ist hier der Buchstabe G mit einem anderen Buchstaben gesteckert, dann findet eine letzte Permutation statt. Ist G ungesteckert, leuchtet die Lampe G auf. Sie leuchtet nur solange auf, wie die Taste A gedrückt gehalten wird, da nur bei gedrückter Taste der Umschaltkontakt auf die Batterie umgeschaltet ist. Lässt man sie los, erlischt die Lampe. Im geschilderten Beispiel wird somit der Buchstabe A, dessen Taste eingangs gedrückt wurde und noch immer gedrückt ist, als Buchstabe G verschlüsselt.

Falls der zu verschlüsselnde Text „AACHENISTGERETTET“ lautet, ist erneut ein A einzugeben. Also wird die Taste A losgelassen und zum zweiten Mal gedrückt. Wichtig ist, dass mit dem mechanischen Druck auf die Taste mithilfe eines Fortschaltmechanismus gleichzeitig die rechte Walze um eine Position rotiert wird. Die mittlere Walze rotiert erst nach 26 Schritten der rechten Walze. In der unteren Hälfte des Bildes „Stromfluss“ ist die Situation skizziert, nachdem die rechte Walze sich um eine Position (nach unten) weitergedreht hat.

Wie man an der Skizze erkennen kann, hat sich der Pfad für den erneut am Kontakt A der rechten Walze eintretenden Strom radikal geändert. Er nimmt jetzt auch bei der mittleren und linken Walze sowie der Umkehrwalze einen völlig anderen Weg als zuvor, obwohl sich diese Walzen nicht gedreht haben. Das Ergebnis ist eine andere Verschlüsselung des Buchstabens A, der nun in C umgewandelt wird.

Bei der Enigma I standen zunächst drei, ab 1939 fünf unterschiedliche Walzen zur Verfügung, die mit römischen Zahlen (I, II, III, IV und V) durchnummeriert waren. Der Benutzer wählte nach Vorgabe einer geheimen Schlüsseltabelle, die für jeden Tag wechselnde Einstellungen vorsah, drei der fünf Walzen aus und setzte diese nach der im Tagesschlüssel unter der Überschrift „Walzenlage“ vorgeschriebenen Anordnung ein.

Die „Schlüsseltafel“ stellte tabellarisch für einen kompletten Monat die jeweils gültigen Tagesschlüssel dar, die um Mitternacht gewechselt wurden (Ausnahmen: Bei der Luftwaffe geschah der Wechsel um 3 Uhr nachts. Für die Kriegsmarine siehe Enigma-M4). Unten sind beispielhaft nur drei Monatstage dargestellt, wobei, wie damals üblich, die Tage absteigend sortiert sind. Dies erlaubt es dem Verschlüssler, die verbrauchten Codes der vergangenen Tage abzuschneiden und zu vernichten.

Beispiel für den 29. des Monats: Walzenlage I IV III bedeutet, dass Walze I links (als langsamer Rotor), Walze IV in der Mitte und Walze III rechts (als schneller Rotor) einzusetzen ist. (Als Umkehrwalze wurde mit wenigen Ausnahmen stets die UKW B benutzt.) Die Ringe, die außen am Walzenkörper angebracht sind und den Versatz zwischen der internen Verdrahtung der Walzen und dem Buchstaben bestimmen, zu dem der Übertrag auf die nächste Walze erfolgt, sind auf den 16., 26. beziehungsweise 8. Buchstaben des Alphabets einzustellen, also auf P, Z und H.

Die Ringstellung wurde oft (wie hier) numerisch und nicht alphabetisch verzeichnet, vermutlich um Verwechslungen mit den anderen Teilschlüsseln vorzubeugen. Als Hilfe für den Bediener „zum Umsetzen der Zahlen in Buchstaben oder umgekehrt“ ist innen im Gehäusedeckel der Enigma als Teil der Hinweisplakette „Zur Beachtung!“ eine Umsetzungstabelle angebracht.

Schließlich sind die doppelpoligen Steckbuchsen an der Frontplatte mit entsprechenden doppelpoligen Kabeln zu beschalten. In der Regel wurden genau zehn Kabel eingesteckt. Die jeweils obere Buchse eines Buchsenpaars hat einen etwas größeren Durchmesser (4 mm) als die untere (3 mm), so dass die Stecker nur in einer Orientierung eingesteckt werden können. So wurde sicher die gewünschte elektrische Überkreuzung und damit die Vertauschung der beiden Buchstaben erreicht. Sechs Buchstaben blieben ungesteckert. (Diese feste Regel der "Six self-steckered letters" war für die Codeknacker eine Hilfe.)

Um die Gefahr des Erratens von Schlüsseln zu reduzieren, wurden von den deutschen Stellen einige Regeln für die Aufstellung der Schlüsseltabellen erfunden. So war es (zeitweise) verboten, dass eine Walzenlage, die an einem Monatstag bereits benutzt wurde, sich an einem anderen Monatstag wiederholte. (Die Briten erkannten dies und nannten es die "non-repeating rule".) Auch durfte sich eine Walze an zwei aufeinanderfolgenden Monatstagen nicht an derselben Stelle im Walzensatz befinden ("non-clashing rule"). Eine dritte Regel sollte das Erraten von naheliegenden Steckerkombinationen verhindern. So war es verboten, dass zwei im Alphabet aufeinanderfolgende Buchstaben miteinander gesteckert wurden. (Auch dies nutzten die britischen "Codebreakers" zu ihren Gunsten und nannten es "Consecutive Stecker Knock-Out CSKO".)

All diese Vorschriften bewirkten das Gegenteil, nämlich eine Schwächung der Verschlüsselung. Sie führten zu einer Arbeitserleichterung für die Codeknacker, die aufgrund der genannten Regeln insbesondere mit Fortschreiten eines Monats immer mehr Schlüsselkombinationen ausschließen konnten.

Nach Einlegen der drei Walzen und Einstellen der Ringe sowie Stecken der zehn Steckerverbindungen entsprechend der Schlüsseltafel schloss der Bediener die oberhalb des Walzensatzes angebrachte Klappe und die Frontklappe. Letzteres bewirkte ein festes Andrücken der Stecker und eine sichere Kontaktgabe sowie einen Schutz vor Ausspähen des Schlüssels. Damit war die Enigma zur Verschlüsselung oder auch Entschlüsselung bereit, vorausgesetzt der Benutzer drehte nun noch die drei (rotierenden) Walzen in die korrekte Anfangsstellung.

Um sicherzustellen, dass nicht alle Funksprüche eines Schlüsselnetzes mit identischen Schlüsseln verschlüsselt werden, was die Texte angreifbar machen würde, war vorgeschrieben, für jeden Spruch eine individuelle Anfangsstellung der drei Walzen einzustellen, „Spruchschlüssel“ genannt. Die Prozeduren hierzu änderten sich von Zeit zu Zeit und waren auch nicht bei allen Wehrmachtteilen gleichartig. Bei Heer und Luftwaffe galt ab dem 1. Mai 1940 (neun Tage vor Beginn des Westfeldzugs) das folgende in der „Schlüsselanleitung zur Schlüsselmaschine Enigma“ beschriebene Schema, wenn beispielsweise der folgende Klartext übermittelt werden soll:
Da die Enigma nur Großbuchstaben und keine Ziffern oder Satzzeichen verschlüsseln kann und auch kein Leerzeichen kennt, muss der oben dargestellte Klartext vor der Verschlüsselung zunächst entsprechend aufbereitet werden. Dabei werden Satzzeichen durch „X“ ersetzt, Eigennamen verdoppelt und in „X“ eingeschlossen und Zahlen ziffernweise ausgeschrieben. Ferner war es üblich, (außer bei Eigennamen) das „ch“ und das „ck“ durch „Q“ zu ersetzen und den Text anschließend in Fünfergruppen aufzuteilen. Man erhält somit den folgenden für die Verschlüsselung vorbereiteten Klartext:
Der Verschlüssler hat seine Enigma I, wie weiter oben beschrieben, nach dem Tagesschlüssel beispielsweise für den 29. des Monats eingestellt. (Walzenlage B I IV III, Ringstellung 16 26 08 und Steckerverbindungen AD CN ET FL GI JV KZ PU QY WX. Sowohl dieser als auch die im Folgenden beschriebenen Schritte können mithilfe frei erhältlicher Computersimulationen realitätsnah nachvollzogen werden, siehe auch: Simulationen unter Weblinks.) Der Bediener denkt sich nun eine zufällige Grundstellung aus, beispielsweise „QWE“, und stellt die drei Walzen so ein, dass genau diese drei Buchstaben in den Anzeigefenstern sichtbar werden. Nun lässt er sich einen zufälligen Spruchschlüssel ebenfalls mit drei Buchstaben einfallen, beispielsweise „RTZ“. Diesen verschlüsselt er mit seiner Enigma und beobachtet, wie nacheinander die Lampen „EWG“ aufleuchten. Den so verschlüsselten Spruchschlüssel teilt er dem Empfänger zusammen mit der zufällig gewählten Grundstellung als Indikator sowie der Uhrzeit und der Anzahl der Buchstaben des Textes als „Spruchkopf“ offen mit.

Laut damals geltender H.Dv.g.14 (= Heeres-Dienstvorschrift, geheim, Nr. 14) enthält der Spruchkopf die Uhrzeit als vierstellige Zahl, die Buchstabenanzahl des Spruchs einschließlich der fünf Buchstaben der Kenngruppe sowie die gewählte Grundstellung und den verschlüsselten Spruchschlüssel (Beispiel: 2220 – 204 – qweewg). Im Allgemeinen wurden alle Buchstaben handschriftlich klein geschrieben, da sie so schneller notiert werden konnten als bei Gebrauch von Großbuchstaben. Ein authentisches Spruchformular mit dem Spruchkopf „kr – 2300 – 182 – zzxprq –“, wobei „kr“ (Abkürzung für „kriegswichtig“ oder „Kriegsnotmeldung“ mit dem auffälligen Morsezeichen − · −   · − ·) als Symbol für „Dringend“ steht, ist unter Weblinks als „Spruch Nr. 233“ zu sehen. Es handelt sich um eine Anfrage nach Munition für die schwere Feldhaubitze (sFH).
Als Nächstes wählt der Bediener noch drei für diesen Tag gültige Kenngruppenbuchstaben anhand einer Kenngruppentabelle aus, beispielsweise „NOW“. Die Kenngruppe hat keine kryptologische Bedeutung, sie dient dem Empfänger der Nachricht nur dazu, zu erkennen, dass die Nachricht wirklich für ihn bestimmt ist und auch befugt entschlüsselt werden kann. Zur Verschleierung der Kenngruppe werden die drei Buchstaben vom Absender beliebig permutiert und um zwei für jeden Spruch zufällig zu wechselnde „Füllbuchstaben“, beispielsweise „XY“, ergänzt. Aus „NOW“ wird so zunächst etwa „OWN“ und schließlich „XYOWN“. Diese fünf Buchstaben werden unverschlüsselt als erste Fünfergruppe dem Geheimtext vorangestellt.

Der Verschlüssler stellt nun die drei Walzen seiner Enigma auf den von ihm gewählten Spruchschlüssel „RTZ“ ein und verschlüsselt den obigen Klartext, das heißt, er gibt jeden einzelnen Buchstaben des Klartextes über die Tastatur der Enigma ein und liest die jeweils aufleuchtende Lampe als Geheimtextbuchstaben ab und notiert ihn. Zusammen mit dem Spruchkopf und der getarnten Kenngruppe ergibt sich der folgende Funkspruch:

Kopf und Geheimtext werden als Morsezeichen gefunkt und vom Empfänger aufgenommen. Dieser prüft als erstes, ob die Anzahl der Buchstaben (hier: 204) korrekt ist und der Spruch unverstümmelt empfangen wurde. Dann betrachtet er die Kenngruppe, also die erste Fünfergruppe, ignoriert die ersten beiden Buchstaben und sieht „OWN“. Er sortiert die drei Buchstaben in alphabetischer Reihenfolge, erhält so „NOW“, schaut in seine Kenngruppentabelle, entdeckt dort diese Kenngruppenbuchstaben und kann nun sicher sein, dass der Spruch für ihn bestimmt ist und er ihn entschlüsseln kann. Seine Enigma ist bereits bezüglich Walzenlage, Ringstellung und Steckerverbindungen entsprechend dem auch ihm bekannten Tagesschlüssel identisch mit der des Absenders eingestellt. Es fehlt ihm noch der Spruchschlüssel, also die richtige Anfangsstellung der Walzen zur Entschlüsselung des Spruchs. Diese Information erhält er aus dem Indikator „QWE EWG“ im Spruchkopf, den er wie folgt interpretiert: Stelle die Walzen auf die Grundstellung „QWE“ ein und taste dann „EWG“. Nun kann er beobachten, wie nacheinander die Lampen „RTZ“ bei seiner Enigma aufleuchten. Dies ist der einzustellende Spruchschlüssel.

Er dreht nun die Walzen auf die Anfangsstellung „RTZ“ und beginnt, den Geheimtext, angefangen mit der zweiten Fünfergruppe „LJPQH“, in seine Enigma einzugeben. Nun leuchten nacheinander die Lampen auf, und der folgende Text erscheint:

Als die Enigma im Jahr 1918 durch Scherbius zum Patent angemeldet wurde, also noch während der Zeit des Ersten Weltkriegs, war sie eine kryptographisch äußerst starke Maschine und durfte zu Recht als „unknackbar“ bezeichnet werden. Innovativ war, im Gegensatz zu den damals noch gebräuchlichen manuellen Verschlüsselungsverfahren (beispielsweise ADFGVX), die Einführung einer maschinellen Verschlüsselung. Sie war durch die damals allein üblichen manuellen, hauptsächlich linguistisch gestützten, Entzifferungsmethoden unangreifbar und blieb es auch noch bis in die 1930er-Jahre, also mehr als zehn Jahre lang.

Die kryptographischen Stärken der Enigma sind im Wesentlichen durch den rotierenden Walzensatz gegeben. Durch die Drehung der Walzen wird erreicht, dass jeder Buchstabe des Textes mit einem neuen Alphabet verschlüsselt wird (polyalphabetische Verschlüsselung). Auf diese Weise wird das bei den monoalphabetischen Verfahren so verräterische Häufigkeitsgebirge bis zur Unkenntlichkeit abgeschliffen und klassische Angriffe zur Entzifferung des Geheimtextes, wie statistische Analysen, Doppler- oder Mustersuche, sind zum Scheitern verurteilt. Auch die Periodensuche mithilfe des Koinzidenzindexes, als übliche Angriffsmethode auf polyalphabetische Verschlüsselungen, wie beispielsweise der Vigenère-Chiffre, ist ebenso aussichtslos, denn im Vergleich zur Periodenlänge (von 16.900, siehe auch: Verbesserungspotenzial) der Enigma war eine vergleichsweise winzige Höchstlänge der Funksprüche von 250 Buchstaben vorgeschrieben.

Entscheidend für die Sicherheit der Verschlüsselung gegen unbefugte Entzifferung sind die Geheimhaltung der Walzenverdrahtung sowie die Anzahl der im Walzensatz verwendeten Walzen. Letzteres ist ein wichtiger Faktor, der die wesentlich stärkere Verschlüsselung der bei den deutschen U-Booten eingesetzten Vierwalzen-Enigma-M4 im Vergleich zur Enigma I (mit nur drei Walzen) erklärt. Es sind drei mit einer M4-Maschine verschlüsselte Funksprüche öffentlich bekannt, deren Inhalt bis zum Jahr 2006 nicht enträtselt werden konnte. Erst dann gelang es dem Hobby-Kryptologen Stefan Krah, zwei der Nachrichten, die vom U-Boot "U 264" beziehungsweise "U 623" im Jahr 1942 gefunkt wurden, durch verteiltes Rechnen ("distributed computing") und Zusammenschluss von mehreren tausend Computern im Internet innerhalb eines Monats zu entziffern. Der dritte Funkspruch schließlich widerstand weitere sieben Jahre und wurde erst im Januar 2013 entziffert. Dies zeigt eindrucksvoll, dass die Enigma, deren Patentierung sich am 23. Februar 2018 zum hundertsten Mal jährte, auch mit modernen kryptanalytischen Angriffsmethoden und der heutigen weit fortentwickelten Rechnertechnik nicht einfach zu knacken ist, sondern noch immer eine harte Nuss darstellt.

Die Ringe (Ringstellung) bestimmen den Versatz zwischen der inneren Verdrahtung der Walzen und dem Buchstaben, zu dem der Übertrag auf die nächste Walze erfolgt. Außerdem dienten sie zum Schutz vor Spionage. So wurde verhindert, dass durch Ablesen der von außen sichtbaren Walzenstellung auf die interne Drehposition der Walzen geschlossen werden konnte.

Mithilfe der „Doppelsteckerschnüre“, die von vorne in das Steckerbrett gesteckt werden können, lassen sich Buchstaben vor und nach Durchlaufen des Walzensatzes paarweise involutorisch vertauschen. Diese Maßnahme diente zur weiteren Stärkung der kryptographischen Sicherheit der Enigma. Tatsächlich wird hierdurch der Schlüsselraum beträchtlich erweitert.

Die Größe des Schlüsselraums der Enigma lässt sich aus den vier einzelnen Teilschlüsseln sowie der Anzahl der jeweils möglichen unterschiedlichen Schlüsseleinstellungen berechnen. Der gesamte Schlüsselraum der Enigma I (für M4 siehe Enigma-M4) ergibt sich aus den folgenden vier Faktoren:





Der gesamte Schlüsselraum einer Enigma I mit drei aus einem Vorrat von fünf ausgewählten Walzen und einer Umkehrwalze sowie bei Verwendung von zehn Steckern lässt sich aus dem Produkt der in den obigen Abschnitten a) bis d) ermittelten 60 Walzenlagen, 676 Ringstellungen, 16.900 Walzenstellungen und 150.738.274.937.250 Steckermöglichkeiten berechnen. Er beträgt:

60 · 676 · 16.900 · 150.738.274.937.250 = 103.325.660.891.587.134.000.000

Das sind etwa 10²³ Möglichkeiten und entspricht einer Schlüssellänge von ungefähr 76 bit. Die gelegentlich zu hörenden „150 Millionen Millionen Millionen“ Kombinationen, beispielsweise in den Spielfilmen „Enigma – Das Geheimnis“ und „The Imitation Game – Ein streng geheimes Leben“, basieren auf dem Weglassen der Ringstellungen. Die genaue Rechnung ergibt in diesem Fall 60 · 16.900 · 150.738.274.937.250 oder 152.848.610.786.371.500.000 unterschiedliche Fälle, wobei die Briten zumeist statt 16.900 alle 26³ oder 17.576 mögliche Walzenstellungen berücksichtigten und als Produkt dann 158.962.555.217.826.360.000 erhielten.

Der Schlüsselraum war für die damalige Zeit enorm groß und hält sogar einem Vergleich mit moderneren Verfahren stand. Beispielsweise verfügt das über mehrere Jahrzehnte gegen Ende des 20. Jahrhunderts zum Standard erhobene Verschlüsselungsverfahren DES ("Data Encryption Standard") über eine Schlüssellänge von genau 56 bit, also deutlich weniger als die Enigma. Eine Exhaustion (vollständiges Durchsuchen) des Schlüsselraums der Enigma ist selbst mit modernen Mitteln kaum möglich und war mit der damaligen Technologie vollkommen illusorisch.

Die Größe des Schlüsselraums ist jedoch nur eine notwendige, aber keine hinreichende Bedingung für die Sicherheit eines kryptographischen Verfahrens. Selbst eine so simple Methode wie die einfache monoalphabetische Substitution verfügt (bei Verwendung eines Alphabets aus 26 Buchstaben wie die Enigma) über 26! (Fakultät) mögliche Schlüssel. Das sind grob 4000·10²³ Schlüssel (ungefähr 88 bit) und ist verglichen mit der Zahl 10²³ der Enigma I sogar noch um etwa den Faktor 4000 größer. Dennoch ist eine monoalphabetische Substitution sehr unsicher und kann leicht gebrochen (entziffert) werden.

Auch bei der Enigma ähnelt die wesentlich zur Größe des Schlüsselraums beitragende konstruktive Komponente, nämlich das Steckerbrett, einer einfachen monoalphabetischen Substitution, denn die Steckerung bleibt schließlich während der gesamten Verschlüsselung unverändert. Das Steckerbrett kann folglich mithilfe einer intelligenten kryptanalytischen Angriffsmethode ("Turing-Bombe") überwunden und praktisch gänzlich eliminiert werden. Damit kann der Faktor 150.738.274.937.250 bei der Berechnung des Schlüsselraums effektiv wieder gestrichen werden.

Ebenso bewirken die Ringe nur eine geringe kryptographische Stärkung des Verfahrens. Bei falscher Ringstellung der rechten Walze und ansonsten korrektem Schlüssel sind periodisch (Periodenlänge = 26 Buchstaben) bereits Klartextpassagen lesbar, die jeweils nach einigen Buchstaben immer wieder abreißen. Noch weniger wirkt der Ring der mittleren Walze, wobei hier die Periodenlänge 650 Buchstaben (25·26) beträgt. Die mittlere Ringstellung trägt somit zumeist überhaupt nicht zur Größe des Schlüsselraums bei, immer dann nämlich, wenn während des Spruchs "kein" Übertrag auf die linke Walze erfolgt, der aufgrund der vorgeschriebenen Spruchlänge von höchstens 250 Buchstaben nur selten passierte. Die Ringstellung der linken Walze ist, wie schon erwähnt, kryptographisch völlig bedeutungslos. Für den Kryptoanalytiker stellt die Feinjustierung der Ringe keine größere Schwierigkeit mehr dar. Damit kann man bei der Berechnung der Größe des Schlüsselraums auch den Faktor 676 getrost wieder streichen.

Als kryptographisch wirksam übrig bleiben nur die 60 Walzenlagen und die (bei unbekannter Ringstellung) 17.576 zu berücksichtigenden Walzenstellungen. So schrumpft der vorher noch so gigantisch erscheinende Schlüsselraum auf vergleichsweise winzige 60·17.576 = 1.054.560 (gut eine Million) Möglichkeiten (etwa 20 bit), eine Zahl, die auch bereits zu Zeiten des Zweiten Weltkriegs mithilfe der damaligen elektromechanischen Technik exhaustiv (erschöpfend) abgearbeitet werden konnte.

Scherbius' Mitarbeiter Willi Korn erreichte durch die Umkehrwalze, dass das Schlüsselverfahren involutorisch wird, das heißt, wenn bei einer bestimmten Stellung der Walzen ein U in ein X verschlüsselt wird, dann wird bei dieser Stellung auch ein X in ein U verschlüsselt. So vereinfachte er Bedienung und Konstruktion der Maschine, denn man muss nicht mehr zwischen Verschlüsselung und Entschlüsselung unterscheiden. Darüber hinaus erhoffte er sich auch eine Steigerung der Sicherheit, denn der Strom durchfließt die Walzen ja nun zweimal:

Mit diesen Worten erläutert Korn die Vorteile seiner Umkehrwalze in der Patentschrift (DRP Nr. 452 194). Dies war jedoch ein Trugschluss mit weitreichenden Konsequenzen.

Zum einen bewirkt die Umkehrwalze, dass nun kein Buchstabe mehr in sich selbst verschlüsselt werden kann, denn der Strom kann ja in keinem Fall genau den Weg durch den Walzensatz wieder zurücknehmen, den er gekommen ist. Er wird stets auf einem anderen Weg zurückgeleitet, als er zur Umkehrwalze hingeflossen ist. Mathematisch spricht man hier von fixpunktfreien Permutationen. Diese Einschränkung mag als unwesentliche Kleinigkeit erscheinen, denn es bleiben ja noch 25 weitere Buchstaben des Alphabets zur Verschlüsselung, tatsächlich bedeutet dies jedoch eine drastische Reduzierung der zur Verschlüsselung verfügbaren Alphabete und darüber hinaus eine neue Angreifbarkeit des Geheimtextes. Zum anderen verursacht die Umkehrwalze dadurch, dass die Permutation und damit die Verschlüsselung involutorisch wird, eine weitere Verringerung der Alphabetanzahl.

Die durch die Umkehrwalze eingefügten kryptographischen Schwächen, insbesondere die Reduzierung der Anzahl der zur Verfügung stehenden Alphabete, lassen sich leicht klarmachen, wenn man statt von 26 Buchstaben vereinfacht von beispielsweise nur vier Buchstaben ausgeht. Mit vier Buchstaben lassen sich 4! = 24 unterschiedliche Alphabete (damit meint der Kryptograph unterschiedliche Anordnungen der Buchstaben) erzeugen, nämlich

Beschränkt man sich hier, statt auf alle 24 möglichen, nur auf die fixpunktfreien Permutationen, so fallen alle Alphabete weg, bei denen ein Buchstabe in sich selbst verschlüsselt wird, also auf seinem gewohnten alphabetischen Platz steht. Aus der obigen Liste sind damit die folgenden fünfzehn Alphabete zu streichen, da sie einen oder mehrere Fixpunkte aufweisen (unten rot und unterstrichen).

Übrig bleiben nur die folgenden neun fixpunktfreien Permutationen:

Berücksichtigt man jetzt noch, dass die Umkehrwalze nicht nur alle Permutationen mit Fixpunkten eliminiert, sondern auch alle nichtinvolutorischen Permutationen, so müssen aus der obigen Tabelle noch weitere sechs Fälle gestrichen werden, nämlich die, bei denen die zweifache Anwendung der Permutation nicht wieder zum ursprünglichen Buchstaben führt. Übrig bleiben von allen möglichen 24 Permutationen eines Alphabets aus vier Buchstaben lediglich die drei fixpunktfreien "und" involutorischen Fälle. Sie werden als „echt involutorische Permutationen“ bezeichnet.

Bei der Enigma mit ihren 26 Buchstaben bewirkt diese Beschränkung, dass statt der 26! (Fakultät), also ungefähr 4·10 insgesamt möglichen permutierten Alphabete lediglich die 25·23·21·19···7·5·3·1 = 25!! (Doppelfakultät), also etwa 8·10 echt involutorisch permutierten Alphabete genutzt werden können. Durch die Umkehrwalze verschenkt man so den "Faktor" von etwa 5·10 an Möglichkeiten – eine gigantische Schwächung der kombinatorischen Komplexität der Maschine. Übrig bleibt weniger als die Quadratwurzel der ursprünglich möglichen Permutationen.

Kryptographisch noch katastrophaler als diese drastische Reduktion der Alphabetanzahl ist jedoch, dass durch die Vermeidung von Fixpunkten Aussagen über den Text möglich sind wie „Nichts ist jemals es selbst“, die bei der Entzifferung eine ganz wesentliche Hilfe waren. Weiß der Angreifer, dass niemals ein Buchstabe die Verschlüsselung seiner selbst ist, dann eröffnet ihm diese Kenntnis Abkürzungen, und er muss nicht mehr mühsam jeden einzelnen Fall abarbeiten, wie an folgendem Beispiel illustriert wird.

Ein seit Jahrhunderten bekanntes und bewährtes Entzifferungsverfahren ist die „Methode des Wahrscheinlichen Worts“. Hierbei errät, vermutet oder weiß der Angreifer, dass im Text eine bestimmte Phrase (, ) auftritt, beispielsweise „OBERKOMMANDODERWEHRMACHT“. Liegt dem Angreifer zum Beispiel ein mit der Enigma verschlüsseltes Geheimtextfragment wie das folgende vor, so kann er ganz leicht ermitteln, an welcher Stelle im Text das vermutete Wahrscheinliche Wort sich "nicht" befinden kann, indem er für jede mögliche Lage prüft, ob ein Zeichen in sich selbst verschlüsselt würde, was, wie er von der Enigma weiß, unmöglich ist. Dazu schreibt er das Wahrscheinliche Wort in den verschiedenen Lagen unter den Geheimtext und prüft auf Kollisionen (), die im unteren Beispiel rot und unterstrichen hervorgehoben sind:

Die Anzahl der durch Kollisionen auszuschließenden Lagen lässt sich nach folgender Überlegung abschätzen: Bei einem Wahrscheinlichen Wort der Länge 1 (also nur ein einzelner wahrscheinlicher Buchstabe) ist die Wahrscheinlichkeit für eine Kollision 1/26. Folglich ist die Wahrscheinlichkeit für "keine" Kollision 1−1/26. Bei einem Wahrscheinlichen Wort wie oben mit der Länge 24 ist dann die Wahrscheinlichkeit für keine Kollision (1−1/26), das sind etwa 39 %. Das heißt, bei 27 untersuchten Lagen erwartet man im Mittel für 27·(1−1/26) der Fälle keine Kollisionen. Der Ausdruck ergibt etwa den Wert 10,5 und stimmt recht gut mit den im Beispiel beobachteten (und grün gekennzeichneten) acht kollisionsfreien Crib-Lagen überein.

Mithilfe dieser äußerst simplen kryptanalytischen Angriffsmethode lassen sich so von den 27 möglichen Lagen des Wahrscheinlichen Worts hier 19, also mehr als zwei Drittel, als unmöglich eliminieren – eine erhebliche Arbeitsvereinfachung für den Angreifer.

Die Betreiber der Schlüsselmaschine Enigma waren der Meinung, dass die durch sie maschinell verschlüsselten Texte im Gegensatz zu fast allem, was bis 1918 gebräuchlich war, mit manuellen Methoden nicht zu knacken sind. Übersehen wurde, dass einer maschinellen Verschlüsselung durch maschinelle Entzifferung begegnet werden kann.

Die Geschichte der Entzifferung der Enigma beginnt im Jahr 1932, als der für Frankreich unter dem Decknamen HE ("Asché") spionierende Deutsche Hans-Thilo Schmidt geheime Schlüsseltafeln für die Monate September und Oktober 1932 sowie die Gebrauchsanleitung (H.Dv.g.13) und die Schlüsselanleitung (H.Dv.g.14) an den französischen Geheimdienstmitarbeiter "Capitaine" (deutsch: Hauptmann) und späteren "Général" Gustave Bertrand gegen Geld verriet. Zu dieser Zeit waren erst drei Walzen (I bis III) im Einsatz und die Walzenlage wurde nur vierteljährlich und noch nicht, wie dann ab Oktober 1936, täglich gewechselt. Das "Deuxième Bureau" des französischen Geheimdienstes leitete die Unterlagen an britische und polnische Stellen weiter.

Während es Franzosen und Briten nicht gelang, in die Verschlüsselung einzubrechen und sie die Enigma als „unknackbar“ einstuften, glückte dem 27-jährigen polnischen Mathematiker Marian Rejewski bei seiner Arbeit in dem für Deutschland zuständigen Referat BS4 des "Biuro Szyfrów" (deutsch: „Chiffrenbüro“) bereits im Jahr 1932 der erste Einbruch in die Enigma. Dabei nutzte er eine legal gekaufte kommerzielle Maschine (vermutlich Modell C), bei der – anders als bei der ihm noch unbekannten militärischen Enigma I – die Tastatur mit der Eintrittswalze in der üblichen QWERTZ-Reihenfolge (Buchstabenreihenfolge einer deutschen Tastatur, beginnend oben links) verbunden war. Rejewski erriet die von den Deutschen für die militärische Variante gewählte Verdrahtungsreihenfolge, die den britischen "Codebreaker" Dillwyn „Dilly“ Knox selbst noch 1939 fast zur Verzweiflung brachte. Anschließend schaffte es Marian Rejewski mithilfe seiner exzellenten Kenntnisse der Permutationstheorie (siehe auch: Enigma-Gleichung), die Verdrahtung der drei Walzen (I bis III) sowie der Umkehrwalze (A) (siehe auch: Enigma-Walzen) zu erschließen – eine kryptanalytische Meisterleistung, die ihn mit den Worten des amerikanischen Historikers David Kahn „in das Pantheon der größten Kryptoanalytiker aller Zeiten erhebt“ (im Original: „[…] elevates him to the pantheon of the greatest cryptanalysts of all time“). Der englische Codeknacker Irving J. Good bezeichnete Rejewskis Leistung als „The theorem that won World War II“ (deutsch: „Das Theorem, das den Zweiten Weltkrieg gewann“).

Die nächste Aufgabe, die gelöst werden musste, war, jeweils die richtige Walzenlage und Walzenstellung zu erschließen. Dazu nutzte Rejewski zusammen mit seinen 1932 hinzugekommenen Kollegen Jerzy Różycki und Henryk Zygalski einen schwerwiegenden verfahrenstechnischen Fehler aus, der den Deutschen unterlief: Um eine sichere Übertragung zu gewährleisten, wurde zu dieser Zeit der Spruchschlüssel noch zweimal hintereinandergestellt und verschlüsselt an den Anfang einer Nachricht geschrieben („Spruchschlüsselverdopplung“). Somit war der erste und vierte, der zweite und fünfte sowie der dritte und sechste Geheimtextbuchstabe jeweils demselben Klartextbuchstaben zuzuordnen. Mithilfe zweier, durch ihren Kollegen Antoni Palluth von der Firma AVA speziell zu diesem Zweck gebauter Maschinen, genannt Zyklometer und "Bomba", die zwei beziehungsweise dreimal zwei hintereinandergeschaltete und um jeweils drei Drehpositionen versetzte Enigma-Maschinen verkörperten, konnten die polnischen Kryptoanalytiker für jede der sechs möglichen Walzenlagen feststellen, bei welchen Walzenstellungen die beobachtete Zuordnung der Buchstabenpaare möglich war und so den Suchraum erheblich einengen. Nach Analyse mehrerer Funksprüche war der korrekte Spruchschlüssel gefunden.

Nachdem die Deutschen, die von alledem nichts wussten, am 15. September 1938 ihre Verfahrenstechnik änderten und drei Monate später mit Einführung der Walzen IV und V die Anzahl der möglichen Walzenlagen von sechs (= 3·2·1) auf sechzig (= 5·4·3) erhöhten, konnten die Polen nicht mehr mithalten, und die Enigma war wieder sicher. Angesichts der drohenden Gefahr übergaben sie kurz vor dem deutschen Überfall auf Polen ihr gesamtes Wissen an ihre Verbündeten. Am 26. und 27. Juli 1939 kam es zum legendären Geheimtreffen französischer, britischer und polnischer Codeknacker im Kabaty-Wald von Pyry, knapp 20 km südlich von Warschau, bei dem sie den verblüfften Briten und Franzosen ihre Enigma-Nachbauten und ihre kryptanalytischen Maschinen präsentierten und ihre Methoden offenbarten. Die erste Frage, die Dilly Knox bei diesem Treffen (vermutlich auf Französisch) gestellt hat, war: "„Quel est le QWERTZU?“" (deutsch: „Was ist der QWERTZU?“; also sinngemäß: „Wie lautet die Verdrahtungsreihenfolge der Eintrittswalze?“). Dies hatte ihn schon lange gequält. Rejewskis Antwort war genial einfach: „ABCDEFG…“. Ein Gedanke, der Knox so abstrus erschien, dass er es nicht fassen konnte. Marian Rejewski hingegen kannte die „Tugend der Deutschen: den Ordnungssinn“, und dies hatte ihn bereits sieben Jahre zuvor die von den deutschen Kryptographen gewählte, denkbar simpelste aller Permutationen erkennen lassen – den Trivialfall der Identität. Daraus resultierte die für ihn leicht zu erratende gewöhnliche alphabetische Reihenfolge der Verdrahtung der Eintrittswalze.

Mit diesem Anschub, vor allem mit den nun endlich bekannten Walzenverdrahtungen, konnten die britischen Kryptoanalytiker mit Ausbruch des Krieges im etwa 70 km nordwestlich von London gelegenen Bletchley Park (B.P.) einen erneuten Angriff auf die Enigma starten. Das wichtigste Hilfsmittel dabei war – neben ihrer intellektuellen Leistungsfähigkeit und dem hohen Personaleinsatz von später zehntausend bis vierzehntausend Frauen und Männern – vor allem eine spezielle elektromechanische Maschine, genannt die "Turing-Bombe", die als Nachfolgerin auf der polnischen "Bomba" aufbaute und vom englischen Mathematiker Alan Turing ersonnen wurde. Turings Idee zur Schlüsselsuche bestand darin, durch ringförmige Verkettung von mehreren, meist zwölf, Enigma-Walzensätzen die Wirkung des Steckerbretts komplett abzustreifen. Dadurch gelang es ihm, die praktisch unüberschaubare Anzahl von mehr als 200 Trilliarden Verschlüsselungsmöglichkeiten, auf die die deutschen Kryptographen ihre Hoffnungen setzten, drastisch zu reduzieren.

Das Grundprinzip geht von der Enigma I aus, bei der drei Walzen aus einem Sortiment von fünf Walzen eingesetzt werden und nur die Umkehrwalze B zur Verfügung steht. Eine andere Umkehrwalze (UKW C), von den Briten lautmalerisch "Uncle Walter" genannt, tauchte kurzzeitig auf und verschwand schnell wieder. Für jede der 60 verschiedenen Walzenlagen gibt es 26³, also 17.576 Walzenstellungen. Wenn man bei der Schlüsselsuche von den Ringstellungen und vom Steckerbrett absehen kann, was mithilfe der durch die "Bombe" realisierten kryptanalytischen Angriffsmethode ermöglicht wurde, dann bleiben „nur“ noch 60·17.576, also 1.054.560 Möglichkeiten übrig. Diese etwa eine Million unterschiedlichen Fälle sind von Hand in vernünftiger Zeit praktisch nicht durchzuprobieren. Mithilfe der "Turing-Bombe" jedoch, die motorbetrieben mit 64 Umdrehungen pro Minute während jeder Umdrehung 26 Fälle abarbeiten konnte, brauchte man nur noch 1.054.560/(26·64) Minuten, also etwas mehr als zehn Stunden, um sämtliche Möglichkeiten durchzutesten. Hinzu kommt noch die Zeit zum Einstellen und Umrüsten der Maschine auf die sechzig verschiedenen Walzenlagen, wodurch die Zeit auf rund zwanzig Stunden verdoppelt wird. Leistet man sich den Aufwand, sechzig "Bombes" einzusetzen, jeweils eine für jede Walzenlage, dann schrumpft die Zeit für einen Durchlauf von etwas mehr als zehn Stunden auf gut zehn Minuten – eine durchaus erträgliche Zeit. Tatsächlich waren Anfang 1944 mehr als 330 "Bombes" im Vereinigten Königreich und den Vereinigten Staaten in Betrieb, nachdem sich die Briten noch Ende 1941 mit nur zwölf "Bombes" begnügen mussten.

Entscheidend wichtig für die Funktion der "Bombe" sind Wahrscheinliche Wörter ("Cribs"), deren Auftreten man im Text erwarten kann. Fehlen diese, dann scheitert die Entzifferung. Beispielsweise gelang den Briten der Einbruch in zwei Schlüsselkreise der Deutschen Reichsbahn nicht, die in Bletchley Park nach der frühen Dampflokomotive "Rocket" als "Rocket II" und "Rocket III" bezeichnet wurden. Grund war, wie sie nach dem Krieg zu ihrer Überraschung feststellten, nicht eine besonders sichere Enigma-Variante, sondern die ungewohnte Eisenbahnersprache und die Art der Transportmeldungen, die ihnen das Erraten von Wahrscheinlichen Wörtern nicht erlaubten. Militärische Meldungen hingegen waren häufig stereotyp abgefasst und enthielten viele leicht zu erratende "Cribs" wie OBERKOMMANDODERWEHRMACHT, die die britischen Codeknacker zur Entzifferung nutzen konnten.

Darüber hinaus profitierten sie von der deutschen Gründlichkeit bei der Abfassung von Routinemeldungen, wie Wetterberichte, die jeden Morgen pünktlich zur selben Zeit und vom selben Ort gesendet wurden. Zwar verbot die deutsche Dienstvorschrift „Allgemeine Schlüsselregeln für die Wehrmacht“ (H.Dv.g.7) ausdrücklich „Regelmäßigkeiten im Aufbau, gleichlautende Redewendungen und Wiederholungen im Text“ und warnte eindringlich „Es muß auf jeden Fall vermieden werden, daß durch flüchtig ausgebildetes Personal Schlüsselfehler gemacht werden, die […] der feindlichen Nachrichtenaufklärung die Entzifferung ermöglichen“, dennoch passierten genau diese Fehler, die die Codeknacker wahrnehmen und ausnutzen konnten. Aus britischer Sicht war eine täglich frisch verschlüsselte Enigma-Meldung, die stets mit den Worten „WETTERVORHERSAGEBEREICHSIEBEN“ begann, ähnlich wertvoll wie es eine direkte öffentliche Bekanntgabe des jeweils gültigen Tagesschlüssels gewesen wäre. So wurde beispielsweise der Enigma-Schlüssel vom "D-Day", also dem Tag der Landung der Alliierten in der Normandie ("Operation Overlord"), durch den "Crib" „WETTERVORHERSAGEBISKAYA“, den die britischen Kryptoanalytiker leicht erraten konnten und korrekt vermuteten, in weniger als zwei Stunden nach Mitternacht gebrochen.

Nicht selten provozierten die Briten sogar bewusst Vorfälle, nur um die darauf prompt zu erwartenden deutschen Funksprüche mit bekanntem Inhalt (und mit aktuellem Tagesschlüssel verschlüsselt) zu erhalten und nannten diese Technik "gardening" (deutsch: „Gärtnern“). Der britische "Codebreaker" Rolf Noskwith aus Baracke 8 beschrieb sie folgendermaßen: „Die RAF warf an bestimmten Stellen in der Nordsee Minen ab, so daß die Minenwarnung der Deutschen uns als Crib diente. Die Stellen waren sorgfältig ausgewählt, um bestimmte Ziffern, wie insbesondere 0 und 5, [als Koordinaten] zu vermeiden, für die die Deutschen unterschiedliche Buchstaben benutzten.“ Die Briten konnten sich so, unter Vermeidung der Fallunterscheidungen für „NULL“ und „NUL“ sowie „FUENF“ und „FUNF“, die Arbeit etwas erleichtern. Außer im Fall „ZWEI“ und „ZWO“ gab es für die übrigen Ziffern nur eine Schreibweise. Auch entzifferte Botschaften von kleineren Marineeinheiten, wie Hafenschiffen, die nicht über die Enigma verfügten und stattdessen Handschlüsselverfahren (Werftschlüssel oder Reservehandverfahren) benutzten, dienten den Briten als "Cribs" beim Bruch der Enigma. Die Deutschen versendeten nämlich viele Funksprüche, wie Minenwarnungen, wortgleich sowohl als Enigma-Geheimtexte als auch mit den Handverfahren verschlüsselt. Die Briten waren dankbar für diese „Geheimtext-Geheimtext-Kompromisse“ und nannten sie "Kisses".

So gelang es unter dem Decknamen „Ultra“, beginnend mit Januar 1940 zunächst die von der Luftwaffe und später auch die vom Heer mit der Enigma I verschlüsselten Nachrichten nahezu während des gesamten Zweiten Weltkriegs kontinuierlich zu brechen. Im Jahr 1943 beispielsweise wurden mehr als 80.000 Funksprüche pro Monat abgefangen und entziffert, also durchschnittlich mehr als 2500 jeden Tag, während des Krieges insgesamt waren es über zweieinhalb Millionen.

Hartnäckiger zeigten sich die Verschlüsselungsverfahren der deutschen Kriegsmarine, die eine Variante (Enigma-M3) mit drei aus acht Walzen (I bis VIII) sowie eine ausgeklügelte Spruchschlüsselvereinbarung nutzte. Hier gelang den Briten der Einbruch erst im Mai 1941 nach Kaperung des deutschen U-Boots "U 110" und Erbeutung einer intakten M3-Maschine und sämtlicher Geheimdokumente (Codebücher inklusive der entscheidend wichtigen „Doppelbuchstabentauschtafeln“) durch den britischen Zerstörer HMS "Bulldog" am 9. Mai 1941. Eine für die Briten schmerzliche Unterbrechung ("Black-out") gab es dann, als am 1. Februar 1942 die M3 (mit drei Walzen) exklusiv bei den U-Booten durch die M4 (mit vier Walzen) abgelöst wurde. Dieses von den Deutschen „Schlüsselnetz Triton“ und von den Briten "Shark" (deutsch: „Hai“) genannte Verfahren konnte zehn Monate lang nicht gebrochen werden, eine Zeit, von den U-Boot-Fahrern die „“ genannt, in der die deutsche U-Bootwaffe erneut große Erfolge verbuchen konnte. Der Einbruch in "Shark" gelang erst am 12. Dezember 1942, nachdem der britische Zerstörer HMS "Petard" am 30. Oktober 1942 im Mittelmeer das deutsche U-Boot "U 559" aufbrachte. Ein Prisenkommando, bestehend aus "Lieutenant" Tony Fasson (1913–1942), "Able Seaman" Colin Grazier (1920–1942) und dem jungen Tommy Brown (1926–1945), enterte das Boot und erbeutete wichtige streng geheime Schlüsselunterlagen, wie Kurzsignalheft und Wetterkurzschlüssel, mit deren Hilfe es die Kryptoanalytiker in Bletchley Park schafften, auch die Enigma-M4 zu überwinden.

Nun kamen auch die Amerikaner zu Hilfe, die unter Federführung von Joseph Desch in der "National Cash Register Company (NCR)" in Dayton, Ohio, ab April 1943 mehr als 120 Stück Hochgeschwindigkeitsvarianten der "Turing-Bombe" produzierten, die speziell gegen die M4 gerichtet waren. Danach waren die deutschen U-Boote nie mehr sicher (siehe auch: U-Boot-Krieg). Unmittelbare Folge der amerikanischen Entzifferungen war – beginnend mit "U 118" am 12. Juni 1943 – die Versenkung von neun der zehn deutschen U-Tanker („Milchkühe“) innerhalb weniger Wochen im Sommer 1943. Dies führte zu einer Schwächung aller Atlantik-U-Boote, die nun nicht mehr auf See versorgt werden konnten, sondern dazu die lange und gefährliche Heimreise durch die Biskaya zu den U-Boot-Stützpunkten an der französischen Westküste antreten mussten.

Es gilt als unbestritten, dass die Kompromittierung der Enigma von enormer strategischer Bedeutung für den Verlauf des Zweiten Weltkriegs war. Einige Geschichtswissenschaftler gehen davon aus, dass, falls die Enigma nicht hätte gebrochen werden können, sich am Ausgang des Krieges zwar nichts geändert hätte, er aber wesentlich länger gedauert hätte und noch weitaus blutiger verlaufen wäre. So äußerte sich der englische Historiker Sir Harry Hinsley, der in Bletchley Park mitgearbeitet hatte, zur Bedeutung von "Ultra" mit den Worten „shortened the war by not less than two years and probably by four years“ (). Die Vermutung erscheint gerechtfertigt, dass es den polnischen, britischen und amerikanischen Kryptoanalytikern und ihrer Leistung bei der Entzifferung der deutschen Maschine zu verdanken ist, dass der Zweite Weltkrieg erheblich verkürzt werden konnte und damit unzähligen Menschen auf allen Seiten das Leben gerettet wurde.

Es gibt aber auch Historiker, Politiker und Militärs, die die nachrichtendienstlichen Erkenntnisse als „entscheidend“ für den Sieg der Alliierten ansehen. So äußerte sich beispielsweise der amerikanische Historiker Harold Deutsch, der im Zweiten Weltkrieg Leiter der Analyse beim Amt für strategische Dienste im Kriegsministerium der Vereinigten Staaten, dem OSS war:

Experten, die Deutschs Ansicht teilen, berücksichtigen die Tatsache, dass die Entzifferungen nicht nur auf militärisch-taktischer Ebene (Heer, Luftwaffe und Marine) eine große Hilfe waren, sondern aufgrund der nahezu vollständigen Durchdringung des deutschen Nachrichtenverkehrs auf allen Ebenen (Polizei, Geheimdienste, diplomatische Dienste, SD, SS, Reichspost, Reichsbahn und Wehrmacht) auch einen äußerst genauen Einblick in die strategischen und wirtschaftlichen Planungen der deutschen Führung erlaubten. Speziell schätzten die Alliierten die Authentizität der aus Enigma-Funksprüchen gewonnenen Informationen, die aus anderen Quellen, wie Aufklärung, Spionage oder Verrat, nicht immer gegeben war. So konnten die Briten ihre zu Beginn des Krieges noch sehr begrenzten Ressourcen deutlich besser koordinieren und viel gezielter gegen die erkannten deutschen Schwächen einsetzen, als es ohne die Entzifferung der Enigma möglich gewesen wäre. Im späteren Verlauf des Krieges nutzten sie dann zusammen mit ihren amerikanischen Verbündeten die "Ultra"-Informationen, um die gemeinsame Überlegenheit noch besser auszuspielen.

Einer der führenden ehemaligen "Codebreaker" aus Bletchley Park, der britische Schachmeister Stuart Milner-Barry, schrieb: „Mit Ausnahme vielleicht der Antike wurde meines Wissens nie ein Krieg geführt, bei dem die eine Seite ständig die wichtigen Geheimmeldungen von Heer und Flotte des Gegners gelesen hat.“ Ein ähnliches Fazit zieht ein nach dem Krieg verfasster amerikanischer Untersuchungsbericht: „Ultra schuf in der Militärführung und an der politischen Spitze ein Bewusstsein, das die Art und Weise der Entscheidungsfindung veränderte. Das Gefühl, den Feind zu kennen, ist höchst beruhigend. Es verstärkt sich unmerklich im Laufe der Zeit, wenn man regelmäßig und aufs genaueste seine Gedanken und Gewohnheiten und Handlungsweisen beobachten kann. Wissen dieser Art befreit das eigene Planen von allzu großer Vorsicht und Angst, man wird sicherer, kühner und energischer.“

Der ehemalige Sicherheitsberater von US-Präsident Jimmy Carter, der polnisch-amerikanische Politikwissenschaftler Zbigniew Brzeziński zitierte den Oberbefehlshaber der alliierten Streitkräfte General Dwight D. Eisenhower, der "Ultra" als "decisive" () für den Sieg bezeichnete. Die polnischen Historiker Władysław Kozaczuk und Jerzy Straszak schrieben „it is widely believed that Ultra saved the world at least two years of war and possibly prevented Hitler from winning“. ().
Der renommierte britische Historiker und Kryptologe Ralph Erskine sagte einfach und klar: Der Einbruch in die Marine-Enigma „rettete Großbritannien vor der Niederlage im U-Boot-Krieg“. Auch Stuart Milner-Barry vertrat die Ansicht, dass „had we not at the most crucial times and for long periods read the U-boat ciphers, we should have lost the war“ (). In einer Ausstellung über den "Secret War" (), die im Jahr 2003 in einem der bedeutendsten Kriegsmuseen weltweit, dem "Imperial War Museum" () in London stattfand, wurde der ehemalige britische Premierminister Winston Churchill zitiert, der seinem König George VI. gesagt hatte: „It was thanks to Ultra that we won the war.“ ().

In seinem Buch "The Hut Six Story" beschreibt Gordon Welchman, der neben Alan Turing einer der führenden Köpfe der britischen "Codebreaker" in Bletchley Park war, die Gratwanderung, die die alliierten Kryptoanalytiker zu vollbringen hatten, um nicht den Anschluss an die von den Deutschen immer wieder neu eingeführten kryptographischen Komplikationen zu verlieren. Mehrfach stand die Entzifferungsfähigkeit auf des Messers Schneide, und immer wieder senkte sich die Waagschale zugunsten der Codeknacker, oft auch mit viel Glück, wie Welchman in seinem Buch einräumt: „We were lucky“ ().

Die Betrachtung alternativer Geschichtsverläufe ist zwangsläufig höchst spekulativ. Entscheidend ist natürlich auch der Zeitpunkt, zu dem die Enigma möglicherweise einbruchssicher gemacht worden wäre. Falls dies erst im Jahr 1945 geschehen wäre, hätte es vermutlich nur geringe Konsequenzen auf den Kriegsverlauf gehabt. Im Jahr 1944 dagegen wären die alliierten Invasionspläne der "Operation Overlord" („D-Day“) behindert worden. Wie man heute weiß, war aus entzifferten Enigma-Funksprüchen nicht nur die gesamte deutsche Gefechtsaufstellung in der Normandie detailliert bekannt, sondern die alliierten Befehlshaber wurden dank "Ultra" auch jeden Tag äußerst präzise über die deutschen Pläne und Gegenmaßnahmen auf dem Laufenden gehalten. In den Jahren ab 1941 wären die deutschen U-Boote nicht mehr so leicht zu finden gewesen, deren Positionen und Pläne die Alliierten aus entzifferten Funksprüchen genau verfolgen konnten.

Was aber wäre gewesen, wenn die Enigma von Anfang an unknackbar geblieben wäre? Im Jahr 1940 beispielsweise setzte die "Royal Air Force" ihre letzten Reserven ein, um schließlich die Luftschlacht um England ("Battle of Britain") zu gewinnen. Auch hierbei waren entzifferte Funksprüche, insbesondere über die Angriffspläne der deutschen Luftwaffe, eine große Hilfe. Ohne diese Hilfe wäre die Luftschlacht eventuell verloren worden und das Unternehmen Seelöwe, also die deutsche Invasion Englands, hätte stattfinden können. Wie es ausgegangen wäre, darüber lässt sich nur spekulieren: Denkbar wäre, dass nach einer deutschen Besetzung der britischen Inseln noch im Jahr 1940 der Krieg beendet gewesen wäre, denn zu diesem Zeitpunkt befanden sich weder die Sowjetunion noch die Vereinigten Staaten im Krieg. (Der deutsche Überfall auf die Sowjetunion begann am 22. Juni 1941. Der japanische Angriff auf Pearl Harbor fand am 7. Dezember 1941 statt und die Kriegserklärung Deutschlands an die USA erfolgte am 11. Dezember 1941.) Wie sich die Weltgeschichte in einem solchen Fall weiterentwickelt hätte, kann niemand sagen, denn die Geschichte verrät uns ihre Alternativen nicht. In einem Essay, das David Kahn als Kontrafaktische Geschichte unter der Annahme verfasste, den Alliierten sei es nicht gelungen, die Enigma zu knacken, führt es zu einem weiteren Siegeszug der Wehrmacht, der schließlich durch eine Atombombe abrupt beendet wird. Das alles sind Spekulationen – deutlich wird allerdings die enorme Bedeutung der Kryptographie und der Kryptanalyse der Schlüsselmaschine Enigma für den Verlauf der Geschichte.

Besonders bemerkenswert ist die Tatsache der perfekt funktionierenden Geheimhaltung der in Bletchley Park über entzifferte Enigma-Funksprüche gewonnenen "Ultra"-Informationen. Churchill selbst würdigte seine verschwiegenen "Codebreakers" mit den Worten „My geese that laid the golden eggs and never cackled“ (). Dieses „Enigma-Geheimnis“ wurde während des gesamten Krieges und selbst danach bis in die 1970er-Jahre gehütet ("Britain’s best kept secret", ). Die Deutschen hatten keinerlei Ahnung von "Ultra". In Bletchley Park gab es keinen Maulwurf – mit einer Ausnahme, John Cairncross, aber der spionierte für die Sowjetunion.

Aufgrund verschiedener verdächtiger Ereignisse wurden auf deutscher Seite zwar mehrfach Untersuchungen angestellt, ob die Enigma wirklich sicher sei, hier wurden jedoch die falschen Schlussfolgerungen gezogen und die Fachleute mit der richtigen Einschätzung setzten sich nicht durch. Auch die Zersplitterung der deutschen Dienste – im Gegensatz zu der in B.P. konzentrierten Kompetenz – ist sicherlich ein Grund für das Nichterkennen der Sicherheitslücken der Maschine. Nebeneinander existierten in Deutschland die teilweise rivalisierenden kryptologischen Dienststellen, wie die Chiffrierabteilung des Oberkommandos der Wehrmacht (OKW/Chi), der General der Nachrichtenaufklärung (OKH/GdNA) im Oberkommando des Heeres, der B-Dienst (Beobachtungsdienst) der Kriegsmarine, das Forschungsamt (FA) der Luftwaffe sowie das Amt IV E im Reichssicherheitshauptamt (RSHA).

Ein kurz nach dem Krieg verfasster Bericht der amerikanischen "Army Security Agency" erwähnt, dass der deutsche Befehlshaber der U-Boote (BdU) Admiral Karl Dönitz den wahren Grund für den noch vor Juli 1942 zum Greifen nahen Sieg und der nur wenige Monate darauf verlorenen Schlacht im Atlantik niemals verstanden hat:

Dabei wäre es für die Deutschen durchaus nicht schwierig gewesen, die Sicherheit ihrer Maschine zu überprüfen. So schlägt der britische Historiker Hugh Sebag-Montefiore als Test vor, eine mit der Enigma wie üblich verschlüsselte Nachricht zu versenden, in der als Täuschungsmanöver beispielsweise ein Treffen deutscher U-Boot-Tanker an einem entlegenen Ort auf See vereinbart wird, der normalerweise nicht von alliierten Schiffen aufgesucht wird. Falls nun zu dem im Funkspruch angegebenen Zeitpunkt plötzlich alliierte Kriegsschiffe am vereinbarten Treffpunkt erscheinen sollten, hätte es den Deutschen ziemlich schnell klar werden können, dass ihre Maschine tatsächlich kompromittiert war.

Nach dem Krieg wurden die in Stückzahlen von mehreren Hunderten, möglicherweise Tausenden, erbeutete und auch nachgebaute Enigma-Maschinen vor allem von den Westmächten an Verbündete oder befreundete Nationen verkauft oder verschenkt. So boten die Briten dem im Jahr 1948 neu gegründeten Staat Israel 30 Stück der zu dieser Zeit allgemein noch als „hochsicher“ und „unknackbar“ geltenden deutschen Verschlüsselungsmaschine an. Die Israelis waren hocherfreut über dieses wertvolle Geschenk und begannen, die deutschen Maschinen für ihre Zwecke zu modifizieren. Sie verbesserten die kryptographische Sicherheit und kombinatorische Komplexität der Enigma und ersetzten bei Tastatur, Lampenfeld, Steckerbrett und Walzensatz das lateinische Alphabet durch hebräische Buchstaben. Sie verzichteten jedoch schließlich auf den Einsatz dieser nun israelischen Enigma-Maschinen, nachdem sie durch den britisch-jüdischen Mathematiker Joseph Gillis (1911–1993), der in Bletchley Park mitgearbeitet hatte, einen subtilen Hinweis erhalten hatten. In Korea, in ehemaligen britischen Kolonien und Protektoraten sowie einigen afrikanischen Staaten wurden Enigmas teilweise noch bis 1975 genutzt, wodurch es den Westmächten gelang, deren Nachrichtenverkehr mitzulesen. Die wenigen heute noch existierenden intakten Exemplare – man schätzt, dass es noch rund 200 Exponate in Museen oder bei privaten Sammlern gibt – werden zu Liebhaberpreisen im fünf- und sogar sechsstelligen Bereich gehandelt.

Schon 1883 formulierte der niederländische Kryptologe Auguste Kerckhoffs unter der später (1946) explizit von Shannon angegebenen Annahme „the enemy knows the system being used“ () seine für seriöse Kryptographie bindende Maxime.

Die kryptographische Sicherheit der Enigma hing – im Widerspruch zu Kerckhoffs’ Maxime – wesentlich von der Geheimhaltung ihrer Walzenverdrahtung ab. Diese war vom Benutzer nicht veränderbar, somit ein Teil des Algorithmus und nicht des Schlüssels. Bemerkenswert ist, dass die Walzenverdrahtung seit den Anfängen in den 1920er-Jahren bis 1945 niemals verändert wurde. Unter den üblichen Einsatzbedingungen einer so weit verbreiteten Schlüsselmaschine wie der Enigma darf man nicht annehmen, dass deren algorithmische Bestandteile auf Dauer geheimgehalten werden können, auch wenn die Deutschen es versucht haben.

Eine erste Möglichkeit zur Verbesserung der Enigma wäre somit das beispielsweise jährliche vollständige Auswechseln des Walzensortiments, mit jeweils radikal geänderter Verdrahtung, gewesen, ähnlich wie es die Schweizer mit ihrem Modell K machten. Noch wesentlich wirkungsvoller wären Walzen, deren innere Verdrahtung schlüsselabhängig variabel gestaltet werden könnte. Interessanterweise gab es hierzu einen Ansatz, nämlich die Umkehrwalze D (britischer Spitzname: "Uncle Dick"), die genau diese Eigenschaft aufwies, jedoch erst spät (Jan. 1944) und nur vereinzelt zum Einsatz kam. Diese „stöpselbare Umkehrwalze Dora“, wie sie von deutscher Seite mithilfe des damals gebräuchlichen Buchstabieralphabets bezeichnet wurde, ermöglichte eine frei wählbare Verdrahtung zwischen den Kontaktstiften und somit eine variable Verbindung zwischen Buchstabenpaaren.

Wesentliche kryptographische Stärkungen der Enigma wären im Konstruktionsstadium leicht möglich gewesen. In erster Linie hätte man die Beschränkung auf fixpunktfreie Permutationen vermeiden müssen. Auch die Involutorik (Verschlüsseln = Entschlüsseln), zwar bequem für die Bedienung, schwächte die Maschine enorm. Beides wäre vermieden worden, hätte man auf die Umkehrwalze verzichtet.

Bereits eine frühe Vorläuferin der Enigma I, gemeint ist die aus dem Jahr 1929 stammende Enigma-H, die von der Reichswehr und später auch von der Wehrmacht als Enigma II bezeichnet wurde, verfügte über "acht" nebeneinander fest angeordnete (nicht austauschbare) Walzen und einen allein durch die Walzenstellung einstellbaren Schlüsselraum von mehr als 200 Milliarden, verglichen mit nur 17.576 Walzenstellungen der Enigma I. Zudem verfügte dieses frühe Enigma-Modell über keine Umkehrwalze, hatte also auch nicht deren Schwächen. Hätte man diese Grundkonstruktion mit acht (statt nur drei) Walzen auf die Enigma I übertragen und zusätzlich wie dort die Lage der Walzen austauschbar gestaltet, hätte dies bei acht Walzen 8! = 40.320 (statt nur 60) Walzenlagen und in Kombination mit den Walzenstellungen einen kryptographisch wirksamen Schlüsselraum von 8.419.907.243.704.320 (mehr als acht Billiarden oder knapp 53 bit) ergeben. Im Vergleich zu den nur gut eine Million (etwa 20 bit) kryptographisch wirksamen Möglichkeiten der tatsächlich realisierten Enigma wäre so eine deutlich stärkere Maschine entstanden, die trotz der vielen Fehler auf deutscher Seite und des gigantischen Aufwands auf britischer Seite vermutlich nicht hätte gebrochen werden können. Allerdings wäre eine solche Maschine mit acht Walzen natürlich weniger handlich gewesen als die Enigma mit nur drei Walzen. Andererseits darf Handlichkeit keine höhere Priorität als kryptographische Sicherheit erhalten, wenn man sich zum Ziel setzt, geheim miteinander zu kommunizieren, denn sonst könnte man sich auch mit nur einer Walze (oder gar überhaupt keiner Walze) begnügen. Entscheidend ist stets die Sicherheit der Verschlüsselung gegen unbefugte Entzifferung, und zwar nach Möglichkeit auch unter Beachtung von in der Praxis unvermeidlichen Bedienfehlern.

Scherbius hatte in seinem grundlegenden Patent vom 23. Februar 1918 sogar schon "zehn" Walzen und die (bereits ohne Austauschen) daraus resultierenden rund 100 Billionen Schlüssel angegeben, außerdem keine Umkehrwalze, sondern einen Umschalter zur Einstellung von Ver- und Entschlüsselung, sowie eine über Getriebe einstellbare "unregelmäßige" Weiterbewegung der Walzen vorgeschlagen – sämtlich gute Ideen und kryptographisch starke Konstruktionsmerkmale, die jedoch im Laufe der Zeit in Vergessenheit gerieten. Der Gründungspräsident des Bundesamt für Sicherheit in der Informationstechnik (BSI), der promovierte Mathematiker und Kryptologe Otto Leiberich meint, mit vier Walzen „und mit einem ungleichförmigen Antrieb wäre die Enigma nie entziffert worden.“

Ein Beispiel für die Stärke dieser Ideen ist die Schlüsselmaschine Sigaba. Dabei handelt es sich um eine amerikanische Rotor-Maschine ähnlich wie die Enigma und ebenso aus dem Zweiten Weltkrieg, die jedoch über keine Umkehrwalze, sondern fünf Chiffrierwalzen ("cipher rotor bank", ) verfügt und zusätzlich zweimal fünf weitere Walzen ("control rotor bank" und "index rotor bank", ) aufweist, die allein zur Erzeugung einer "unregelmäßigen" Fortschaltung der Chiffrierwalzen dienen. Die Sigaba erzeugt sowohl Fixpunkte als auch nichtinvolutorische Permutationen und konnte zu keinem Zeitpunkt, weder von deutschen noch von japanischen Kryptoanalytikern, noch von den Amerikanern selbst, die dies probeweise versuchten, gebrochen werden.

Eine sehr einfache Möglichkeit, die Enigma sicherer zu gestalten, ist die Verwendung von mehr als einer Übertragskerbe. Diese Kerben sind Bestandteil jeder Walze und bewirken den Übertrag auf die nächste, im Walzensatz weiter links liegende Walze und sorgen so für die Fortschaltung der Rotoren. Den Codeknackern kam es sehr gelegen, dass sie 26 Buchstaben lang davon ausgehen konnten, dass allein die rechte Walze rotierte und erst dann eine Fortschaltung auf den mittleren Rotor passierte. Für relativ lange Textpassagen besteht die Enigma somit aus Sicht des Kryptoanalytikers nur aus einer einzigen sich drehenden (rechten) Walze und einer, aus mittlerer und linker Walze sowie der Umkehrwalze bestehenden, sozusagen besonders dicken (feststehenden) Umkehrwalze. Erst der Übertrag auf die mittlere Walze stört dies. Hätten die Walzen der Enigma über mehr als nur eine einzige Übertragskerbe verfügt, beispielsweise neun, wie bei der britischen Schlüsselmaschine Typex, so hätte sich für den Anwender praktisch nichts geändert, die Kryptanalyse jedoch wäre durch das dann häufigere Weiterschalten der mittleren und der linken Walze stark gestört worden.

Peter Twinn, einer der Mitarbeiter Turings in Bletchley Park, kommentierte es mit den Worten „they certainly missed a trick in not combining multiple-turnover wheels with Steckerverbindungen“ (). Gordon Welchman unterstrich die Folgen dieses deutschen Fehlers: „We would have been in grave trouble if each wheel had had two or three turnover positions instead of one“ (). Die Typex erwies sich nicht zuletzt auch durch ihre im Vergleich zur Enigma größeren Anzahl an Übertragskerben für OKW/Chi, die Chiffrierabteilung des OKW, als unknackbar.

Vielleicht fürchteten die Entwickler der Enigma eine Reduzierung der Periode, das ist die Anzahl der Zeichen, nach der sich das zur Verschlüsselung verwendete Alphabet wiederholt. Die Periode beträgt bei der Enigma I 26·25·26 = 16.900, wobei der Faktor 25 bei der mittleren Walze durch die bereits erwähnte (unwichtige) Anomalie des Fortschaltmechanismus verursacht wird. Bei Verwendung einer geraden Anzahl oder von dreizehn Übertragskerben statt nur einer würde die Periode tatsächlich drastisch absinken, da diese Zahlen gemeinsame Teiler mit 26 aufweisen. Bei zum Beispiel drei, fünf, sieben, neun oder elf Kerben hingegen besteht diese Gefahr nicht, da diese Zahlen zu 26 teilerfremd sind. Interessanterweise wurden bei der Marine, in Ergänzung zu den von der Enigma I bekannten fünf Walzen, drei weitere Walzen eingesetzt (VI, VII und VIII), die mehr als eine, nämlich zwei Übertragskerben aufweisen. Die exklusiv von der Marine verwendeten drei Walzen vermieden außerdem einen weiteren Fehler der fünf Walzen der Enigma I, denn sie hatten ihre Übertragskerben alle bei identischen Buchstaben. Nicht so die Walzen I bis V, die sich durch den bei unterschiedlichen Buchstaben erfolgenden Übertrag verrieten. Die Codeknacker hatten sich dafür den (sprachlich unsinnigen) Merkspruch „Royal Flags Wave Kings Above“ gebildet, der für die Walzen I bis V in dieser Reihenfolge den jeweiligen Buchstaben nennt, der stets im Sichtfenster erscheint, nachdem ein Übertrag auf die nächste Walze erfolgt ist.

Eine bedeutende Innovation, die die kryptographische Sicherheit der Enigma erheblich verbessert hätte, die aber zu spät kam, um während des Krieges noch eingesetzt werden zu können, waren die sogenannten „Lückenfüllerwalzen“ (Foto siehe unter Weblinks). Diese neuartigen Rotoren erlaubten es „an jeder Walze Schaltlücken beliebig nach Art und Zahl einzustellen“. Die Einstellungen hätten schlüsselabhängig verändert werden können und so wesentlich zur kryptographischen Stärkung der Maschine beigetragen. Im Juli 1944 erhielt das Ertel-Werk in München einen Fertigungsauftrag über 8000 Stück Lückenfüllerwalzen, der kurz darauf auf 12.000 Stück erhöht wurde. Kriegsbedingt konnten jedoch nur wenige hergestellt und keine mehr ausgeliefert werden. Das amerikanische "Target Intelligence Committee" "(TICOM)" konfiszierte gegen Ende des Krieges sämtliche Informationen über die Lückenfüllerwalze und hielt sie für viele Jahre sorgsam unter Verschluss. Falls sie in ausreichender Stückzahl hätte gefertigt und eingesetzt werden können, so wären die britischen Codeknacker vermutlich aus dem Rennen gewesen, insbesondere, wenn es, wie geplant, gelungen wäre, die Lückenfüllerwalze in Kombination mit der Umkehrwalze D einzusetzen.

Die deutsche "Abwehr" (Geheimdienst) verwendete ein Enigma-Modell (G), das über einen exklusiven Walzensatz verfügte, bei dem die (drei) Walzen tatsächlich mehrere Übertragskerben aufwiesen, nämlich 11, 15 beziehungsweise 17 Kerben. Selbst die Umkehrwalze war – im Unterschied zu den anderen Enigma-Modellen – drehbar und rotierte mit. Dies stärkte die Verschlüsselung und sorgte sicher auch dafür, dass andere deutsche Stellen nicht mitlesen konnten. Allerdings verzichtete die "Abwehr" bei dieser besonders kompakten (äußere Abmessungen 270 mm × 250 mm × 165 mm) und handwerklich hervorragend gebauten Enigma auf ein Steckerbrett. Die Folge war, dass es den "Codebreakers" von Bletchley Park, an der Spitze „Dilly“ Knox und seine Mitarbeiterinnen Margaret Rock und Mavis Lever, in B.P. als „Dilly’s girls“ hochgeachtet, am 8. Dezember 1941 gelang, auch diese Verschlüsselung zu überwinden und so dazu beizutragen, dass deutsche Agenten bereits bei ihrer Einreise „in Empfang genommen“ werden konnten. Diese wurden anschließend nicht einfach nur eliminiert, sondern es gelang dem britischen Inlandsgeheimdienst "MI5", viele von ihnen „umzudrehen“ und im Rahmen des Systems "Double Cross" () als Doppelagenten einzusetzen. Zusammen mit den aus Enigma-G-Sprüchen entzifferten Informationen erhielt der "MI5" ein so detailliertes und zutreffendes Bild über die Pläne und den Wissensstand der "Abwehr", dass jeder einzelne noch in Großbritannien operierende deutsche Agent genau bekannt war und gezielt kontrolliert und manipuliert werden konnte. Dies wurde auch zur Desinformation der deutschen Führung genutzt (siehe auch: "Operation Fortitude").

Zusammenfassend können folgende Punkte zur kryptographischen Stärkung der Enigma festgehalten werden, deren Umsetzung noch vor oder während des Krieges möglich gewesen wäre und die leicht dazu hätten führen können, dass die Enigma sich plötzlich „außerhalb der Reichweite der bereits stark gedehnten anglo-amerikanischen kryptanalytischen Finger befunden hätte, was möglicherweise den Verlauf des Krieges verändert hätte“ ():
Eine verblüffend einfache und dabei durchschlagend wirksame Maßnahme, die laut Gordon Welchman zu jedem beliebigen Zeitpunkt ganz leicht hätte eingeführt werden können und die er während des Krieges am meisten befürchtet hatte, ist die Verwendung von einpoligen Steckerverbindungen anstelle der doppelpoligen involutorischen Kabel. Dann könnte man beispielsweise X mit U steckern und U nun aber nicht notwendigerweise mit X, sondern mit irgendeinem anderen beliebigen Buchstaben. So hätte schlagartig die Involutorik des Steckerbretts – wenn auch nicht der ganzen Maschine – beseitigt werden können. Dies hätte nach Welchman katastrophale Auswirkungen für die Codeknacker in Bletchley Park gehabt. Ein Großteil der dort erarbeiteten Methodik inklusive des von Welchman selbst erfundenen "diagonal board" () wäre nutzlos geworden. Er schreibt „the output of Hut 6 Ultra would have been reduced to at best a delayed dribble, as opposed to our up-to-date flood.“ ()

Eine grobe Übersicht der verwirrenden Modellvielfalt der Enigma zeigt die folgende Tabelle (siehe auch: Stammbaum der Enigma unter Weblinks). Neben dem Modellnamen ist das Jahr der Indienststellung, die Walzenanzahl sowie die daraus resultierende Anzahl der möglichen Walzenlagen angegeben. Ferner ist die Anzahl und die Art der Umkehrwalze (UKW) notiert, wobei zwischen fest eingebauten UKW sowie manuell einstellbaren, also „setzbaren“ UKW und rotierenden UKW unterschieden werden muss, also UKW, die während des Verschlüsselungsvorgangs weiterrotieren. Ein Beispiel dafür ist die (weiter oben) beschriebene Enigma-G der "Abwehr". Einige frühe Maschinen, wie die Enigma-A, verfügten über keine UKW. Ferner ist die Anzahl der Übertragskerben angegeben sowie eine Literaturstelle als Referenz und für weitere Informationen.

Neben den meistverwendeten Modellen Enigma I, Enigma-M3 und Enigma-M4 sowie deren Vorläuferinnen Enigma-A bis Enigma-D und den bereits genannten Enigma-G und Enigma-K ist noch die Enigma-T erwähnenswert, die speziell für den Nachrichtenverkehr der beiden Kriegsverbündeten Deutschland und Japan konzipiert war. Sie wurde nach dem deutschen Großadmiral der früheren Kaiserlichen Marine Alfred von Tirpitz (1849–1930) auch als „Tirpitz-Maschine“ bezeichnet und verfügte über kein Steckerbrett, aber über eine „setzbare“ (einstellbare, jedoch nicht rotierende) Umkehrwalze und insgesamt acht Walzen mit jeweils fünf Übertragskerben (siehe auch: Enigma-Walzen), von denen drei ausgewählt wurden. Die Enigma-T kam kaum zum Einsatz. Nicht verwechselt werden darf sie mit dem in Japan entwickelten Enigma-Nachbau, der "San-shiki Kaejiki".

Ein Kuriosum stellt die Enigma-Z dar, die dem spanischen Außenministerium im Jahr 1931 zum Kauf angeboten wurde. Bei ihr handelt es sich um eine Variante ähnlich der Enigma-D, die jedoch keinerlei Buchstabentasten, sondern allein zehn Zifferntasten („1“ bis „0“) und entsprechend (kleinere) Walzen mit nur zehn Kontakten und zehn Glühlampen für „1“ bis „0“ aufweist. Sie war also nicht zur Verschlüsselung von Texten, sondern nur von Zahlen gedacht, wie zur Überschlüsselung von diplomatischen Codes. So konnte beispielsweise die Ziffernfolge „25183 91467“ als „38760 15924“ verschlüsselt werden. Die Spanier verzichteten damals auf den Erwerb der Enigma-Z und entschieden sich stattdessen für die noch weniger sichere Kryha.

Der Fortschaltmechanismus der Walzen weist eine konstruktive Besonderheit auf, die zur Folge hat, dass sich die Walzen der Enigma nicht immer so weiterdrehen, wie es bei einem mechanischen Kilometerzähler der Fall wäre. Diese Besonderheit äußert sich so, dass, wenn die linke (langsame) Walze rotiert, sie die mittlere Walze „mitnimmt“. Dies lässt sich an einem Beispiel illustrieren.

Bei beispielsweise Walzenlage B I II III, Ringstellung 01 01 01 und der Walzenstellung ADU dreht sich der Walzensatz mit dem ersten Tastendruck auf ADV weiter. Das ist eine ganz normale Weiterdrehung nur der rechten Walze, ohne Weiterschaltung der mittleren oder der linken Walze. Nach der bekannten Merkregel „Royal Flags Wave Kings Above“ ist für Walze III mit dem nächsten Tastendruck, also wenn sie von V auf W weiterrotiert, mit einem Übertrag auf die mittlere Walze zu rechnen. Dann wird nicht nur die rechte Walze normal weiterrotieren, sondern gleichzeitig auch die mittlere Walze von D auf E umschalten. Die nächste Walzenstellung ist somit AEW.

Nun jedoch hat die mittlere Walze (hier: Walze II) den Buchstaben erreicht, nämlich E, der nach der Merkregel unmittelbar vor ihrem Umschaltbuchstaben F liegt. Damit ist jetzt der Moment gekommen, zu dem die mittlere Walze ihrerseits einen Übertrag auf die linke Walze bewirkt. Mit dem nächsten Tastendruck wird sich also die linke Walze von A auf B weiterdrehen. Aufgrund der erwähnten konstruktiven Besonderheit führt dieses Weiterdrehen jedoch dazu, dass sie die mittlere Walze mitnimmt und sich diese noch einmal weiterdreht, also von E auf F. Folglich werden mit dem nächsten Tastendruck alle drei Walzen gleichzeitig weitergeschaltet und nach der vorherigen Walzenstellung AEW sind nun unmittelbar die Buchstaben BFX in den Anzeigefenstern der Enigma zu sehen. Nach diesem etwas fremdartig erscheinenden Ereignis kehrt die Maschine wieder in den regulären Fortschaltmodus zurück, bis dann nach 650 Tastendrücken erneut die mittlere Walze den Buchstaben E erreicht.

Zusammenfassend noch einmal das Weiterschalten des Walzensatzes. Man erkennt hier die Anomalie beim dritten Tastendruck, die sich als „Doppelschritt“ der mittleren Walze äußert (hier: D → E → F).

In Summe führt dieser durch die Anomalie des Fortschaltmechanismus hervorgerufene Effekt des Doppelschritts der mittleren Walze dazu, dass von den theoretisch möglichen 26³ = 17.576 Walzenstellungen der Enigma I 26² = 676 ausgelassen werden und nur 26·25·26 = 16.900 übrig bleiben.

Mit der Enigma verschlüsselte Nachrichten wurden im Regelfall per Funk übermittelt, nur selten als Fernschreiben oder telefonisch als „Fernspruch“ oder per Signallampe als „Blinkspruch“. Der Absender füllte ein Formular mit dem Klartext aus, das vom Verschlüssler als Grundlage für den mithilfe der Enigma-Maschine erzeugten Geheimtext diente. Diesen übertrug er Buchstaben für Buchstaben in ein entsprechendes Funkspruchformular (siehe auch: Dokumente unter Weblinks), das wiederum dem Funker als Basis für den im Morsecode übermittelten Funkspruch diente. Verfasser, Verschlüssler und Funker der Nachricht konnten drei verschiedene Personen sein oder auch ein und dieselbe.

Eine wichtige Kennzeichnung des Funkspruchs, die im Spruchzettel mit einem Buntstift besonders hervorgehoben wurde, war die „Spruchnummer“. Durch farbige Angabe der Nummer unterschieden die Deutschen zwischen „abgegangenen“, also den zu sendenden oder bereits gesendeten Funksprüchen, bei denen die Spruchnummer mit einem blauen Farbstift in das Formular eingetragen wurde, und „angekommenen“, also empfangenen Funksprüchen, bei denen die Nummer in Rot geschrieben wurde. Nur wenige der unzählig vielen während der Zeit des Krieges ausgefüllten Spruchzettel sind erhalten geblieben. Die überwiegende Mehrzahl wurde nach Empfang und Entschlüsselung der Nachrichten vernichtet.

Die zweite sehr wichtige Quelle für authentische Enigma-Sprüche stellen die reichhaltigen Aufzeichnungen der Alliierten dar. Insbesondere die Archive des damals weltweit arbeitenden britischen "Y Service" (deutsch: „Y-Dienst“) sind prall gefüllt, bisher jedoch leider nur zu einem kleinen Teil öffentlich zugänglich. Das Bild zeigt eine der raren Ausnahmen aus dem Archiv des Dienstes in Bletchley Park. Das „Y“ steht im Englischen hier lautmalerisch für die Anfangssilbe des Wortes "wireless" (deutsch wörtlich: „drahtlos“, mit der Bedeutung: „Funk“). Eine sinngemäße Übersetzung von "Y Service" wäre somit „Funkabhördienst“.

Die unten wiedergegebenen Enigma-Funksprüche stammen aus freien Quellen und stellen kostbare Raritäten dar. Sie sind ein Schatz für alle, die sich etwas intensiver für die wahre Geschichte der Enigma interessieren und einmal selbst mit echter Kryptanalyse befassen möchten. Die Geheimtexte sind inzwischen mithilfe moderner kryptanalytischer Methoden und Rechnertechnik entziffert worden. Die Schlüssel und Klartexte sind öffentlich aber kaum bekannt, so dass diese Sprüche als faszinierende Übungsbeispiele für eigene Experimente und lehrreiche realitätsnahe Erfahrungen bei der Enträtselung der Enigma dienen können.

Zu beachten ist, dass dies keine fiktiven Funksprüche sind, wie sie der Wettbewerb "Enigma Cipher Challenge" (siehe auch: Entzifferungen unter Weblinks) bietet, sondern dass es sich um Originalfunksprüche handelt, die im Zweiten Weltkrieg wirklich so aufgezeichnet worden sind. Es ist deshalb durchaus möglich, und hier tatsächlich auch der Fall, dass Verstümmelungen auftreten. Das bedeutet, dass einige Zeichen nicht korrekt sind oder fehlen. Dies betrifft sowohl die Buchstaben als auch die Zahlen. Letzteres kann sehr leicht gesehen werden. Dazu ist nur die Geheimtextlänge zu zählen und mit der im Spruchkopf angegeben Zahl zu vergleichen. Gründe für in der Praxis kaum vermeidbare Verstümmelungen, die auch durch die kriegsbedingten besonderen Rahmenbedingungen erklärt werden können, sind Schreibfehler, Tastfehler, atmosphärische Störungen wie beispielsweise Gewitterblitze während der Funkübertragung, Hörfehler oder schlicht Flüchtigkeitsfehler. Ausgehende Funksprüche, die man auch durch den Eintrag im Feld „Befördert am…“ (und nicht „Aufgenommen am…“) identifizieren kann, sind naturgemäß weniger verstümmelt als die eingehenden und daher (bei gleicher Textlänge) zumeist einfacher zu knacken.

Die Enigma ist in einigen Spielfilmen zu sehen, die vor dem Hintergrund des U-Boot-Krieges spielen. Im deutschen Kinoklassiker „Das Boot“ nach dem gleichnamigen Roman wird sie zur Entschlüsselung empfangener Funksprüche benutzt. Man hört die Stimme von Herbert Grönemeyer sagen „Erst durch die Schlüsselmaschine ergibt sich aus wirren Buchstabenfolgen ganz langsam ein Sinn“, während in Großaufnahme die Enigma im Einsatz zu sehen und auch zu hören ist. Historisch nicht ganz korrekt ist hier die Verwendung einer M4, da sie erst am 1. Februar 1942 in Dienst gestellt wurde, während "das Boot" in Roman und Film seine Feindfahrt im Herbst und frühen Winter des Jahres 1941 durchführt. Somit hätte korrekterweise eine M3 gezeigt werden müssen.

Im amerikanischen Film „U-571“ wird eine Enigma durch amerikanische Seeleute von einem deutschen U-Boot erbeutet. Speziell von britischer Seite wurde kritisiert, dass, in Verkennung der geschichtlichen Realität, hier Amerikaner als Helden bei der Erbeutung einer Enigma dargestellt werden, während es in Wirklichkeit Briten waren, denen dies gelang.

Die britisch-amerikanische Gemeinschaftsproduktion „The Imitation Game – Ein streng geheimes Leben“ illustriert das Leben und die Beiträge von Alan Turing als Codeknacker in Bletchley Park. Auch hier spielt die Enigma eine zentrale Rolle. Auf Kosten der historischen Korrektheit werden im Film viele Fakten verdreht oder dramatisch überhöht dargestellt. Beispielsweise wird Turings Romanze mit seiner Kollegin Joan Clarke intensiver dargestellt als sie tatsächlich war. Turings Nichte Inagh Payne kritisierte das Drehbuch mit den Worten: „You want the film to show it as it was, not a lot of nonsense“ (deutsch: „Man will doch, dass der Film es so darstellt, wie es war, und nicht einen Haufen Unsinn“). Im Film findet Turing heraus, dass Cairncross ein Spion ist. Diesem gelingt es jedoch, Turing mit seiner damals strafbaren Homosexualität zu erpressen. So decken sie gegenseitig das Geheimnis des anderen. Diese Falschdarstellung wurde heftig kritisiert, denn so wird Turing im Film faktisch als „Landesverräter“ dargestellt. Tatsächlich stand er niemals unter diesem Verdacht. Bei aller Sympathie für Überhöhungen aus dramaturgischer Sicht wurde diese Darstellung als Herabwürdigung Turings energisch zurückgewiesen und der Film daher als untragbar eingestuft.

Im britischen Spielfilm "Enigma – Das Geheimnis", der auf dem Roman "Enigma" basiert, wird die Entzifferungsarbeit der britischen "Codebreaker" in Bletchley Park thematisiert. Bemerkenswert sind die vielen authentischen Requisiten im Film, bei denen es sich um Original-Schaustücke aus dem Bletchley-Park-Museum handelt. Die diversen Funksprüche sind speziell für den Film nach den Original-Vorschriften und Verfahren wirklichkeitsgetreu erzeugt und verschlüsselt worden. Gegen Ende des Films entpuppt sich ein polnischer Codeknacker als Verräter, der versucht, das „Enigma-Geheimnis“ an die Deutschen zu verraten. Dies entspricht in zweierlei Hinsicht nicht den historischen Tatsachen. Zum einen gab es – wie bereits dargelegt – keine Verräter in Bletchley Park, die für die Deutschen spioniert hätten. Zum anderen hat dort nicht ein einziger polnischer Kryptoanalytiker mitgearbeitet, denn aus Geheimhaltungsgründen verwehrten die Briten fast allen Ausländern, selbst Marian Rejewski, den Zutritt und erst recht die Mitarbeit. Somit ist die filmische Darstellung in diesem Punkt historisch verfehlt. Kritisiert wurde insbesondere, ausgerechnet einen Polen im Film als Verräter darzustellen, denn am allerwenigsten haben Polen das Enigma-Geheimnis verraten. Im Gegenteil, polnische Kryptoanalytiker wie Marian Rejewski, Jerzy Różycki und Henryk Zygalski haben bereits vor dem Krieg die entscheidenden Grundlagen für den Einbruch in das Rätsel der Enigma geschaffen, ohne die es den britischen Codeknackern vermutlich nicht gelungen wäre, deutsche Funksprüche zu entziffern und der Zweite Weltkrieg einen anderen Verlauf genommen hätte.

Im Folgenden sind einige wichtige Zeitpunkte zur Geschichte der Enigma aufgelistet
(spezielle Zeitpunkte zur Marine-Version siehe M4):

Im Zusammenhang mit der Arbeitsweise der Enigma und deren Kryptanalyse wird die folgende Fachterminologie verwendet:










</doc>
<doc id="1300" url="https://de.wikipedia.org/wiki?curid=1300" title="Eisenoxid">
Eisenoxid

Eisenoxid steht für: 


</doc>
<doc id="1301" url="https://de.wikipedia.org/wiki?curid=1301" title="Electrically Erasable Programmable Read-Only Memory">
Electrically Erasable Programmable Read-Only Memory

Ein EEPROM (engl. Abk. für , wörtlich: "elektrisch löschbarer programmierbarer Nur-Lese-Speicher", auch EPROM) ist ein nichtflüchtiger, elektronischer Speicherbaustein, dessen gespeicherte Information elektrisch gelöscht werden kann. Er ist verwandt mit anderen löschbaren Speichern, wie dem durch UV-Licht löschbaren EPROMs und dem ebenfalls elektrisch löschbaren Flash-Speicher. Er wird verwendet zur Speicherung kleinerer Datenmengen in elektrischen Geräten, bei denen die Information auch ohne anliegende Versorgungsspannung erhalten bleiben muss oder bei denen einzelne Speicherelemente bzw. Datenworte einfach zu ändern sein müssen. Ein typisches Beispiel ist der Rufnummernspeicher eines Telefons. Zur Speicherung größerer Datenmengen wie z. B. dem BIOS in PC-Systemen sind meist Flash-Speicher ökonomischer.

Der Ausdruck "EEPROM" beschreibt lediglich die Eigenschaften des Speichers, dass dieser nicht-flüchtig ist und allein mit elektrischer Energie gelöscht werden kann (im Gegensatz zu dem nur durch UV-Licht löschbaren EPROM). Er umfasst deshalb genau genommen die heute üblicherweise als EEPROM bezeichneten wort- oder byteweise löschbaren Speicher, als auch die neueren blockweise löschbaren Flashspeicher. Da bei letzteren die sonst pro Speicherzelle notwendigen Schreib-, Lese- und Löschtransistoren entfallen können, ist mit ihnen eine deutliche höhere Speicherdichte erreichbar.

Ein EEPROM besteht aus einer Matrix aus Feldeffekttransistoren mit isoliertem Steueranschluss (Floating Gate), in der jeder Transistor ein Bit repräsentiert. Beim Programmiervorgang wird auf das Floating Gate eine Ladung eingebracht, die nur durch den Löschvorgang wieder entfernt werden kann. Im Normalbetrieb bleibt die Ladung auf dem vollständig isolierten Gate erhalten.

Bei UV-löschbaren EPROMs wird die Ladung durch Injektion heißer Ladungsträger (engl. , HCI) auf das Gate gebracht und kann nur durch Bestrahlung mit UV-Licht wieder entfernt werden, während bei EEPROMs sowohl beim Schreiben als auch Löschen die Ladung durch Fowler-Nordheim-Tunneln auf das isolierte Gate aufgebracht bzw. entfernt wird. Beim Flash hingegen wird die Ladung beim Schreiben durch HCI auf das Gate aufgebracht und beim Löschen durch Fowler-Nordheim-Tunneln wieder entfernt.

Zum Programmieren des EEPROMs wird ein hoher Spannungspuls an das Control Gate gelegt, wobei ein Tunnelstrom von diesem durch das isolierende Dielektrikum auf das Floating Gate fließt. Diese hohe Spannung musste bei EPROMs von außen an den Speicherbaustein angelegt werden, während sie beim EEPROM, und auch bei den Flashspeichern, bausteinintern erzeugt wird.

Nach dem Schreiben des Speichers, d. h. dem selektiven Aufbringen von Ladung auf die Floating Gates, werden die geschriebenen Daten durch ein Bitmuster geladener/ungeladener Gates repräsentiert. Diese Daten lassen sich nun über die Drain-Source-Anschlüsse der Transistoren beliebig oft auslesen, wobei die normale Betriebsspannung beim Lesen weit unterhalb der Programmierspannung liegt. Die Anzahl der möglichen Schreibvorgänge der einzelnen Speicherzellen ist allerdings begrenzt, die Hersteller garantieren üblicherweise einige 10.000 bis über 1.000.000 Schreibzyklen. Dieses wird zum Teil durch redundante Speicherzellen erreicht.

EEPROMs können im Unterschied zu Flash-EEPROMs byteweise beschrieben und gelöscht werden. Im Vergleich zu Flash-EEPROMs, die zwischen 1 μs und 1 ms für einen Schreibzyklus benötigen, sind herkömmliche EEPROMs mit 1 ms bis 10 ms erheblich langsamer. EEPROMs verwendet man deshalb bevorzugt, wenn einzelne Datenbytes in größeren Zeitabständen verändert und netzausfallsicher gespeichert werden müssen, wie zum Beispiel bei Konfigurationsdaten oder Betriebsstundenzählern.

Als Ersatz für die früher als Programm- oder Tabellenspeicher dienenden ROMs oder EPROMs eignete sich das EEPROM aufgrund der deutlich höheren Herstellungskosten nicht, diese Rolle wurde später durch die Flash-Speicher übernommen. Die höheren Kosten der EEPROM-Technologie führten dazu, dass zunächst eigenständige EEPROM-Bausteine zumeist über ein serielles Interface an die Mikrocontroller angeschlossen wurden. Später wurden dann bei etlichen Mikrocontrollern auch On-Chip-EEPROMs angeboten. Da Mikrocontroller heute meist sowieso in robusten Flashtechnologien hergestellt werden, die ein häufigeres Löschen und Programmieren erlauben, kann meist auch ein Bereich des Flash-Speichers für veränderliche Daten verwendet werden. Dazu wird ein Teilbereich des Flashspeichers reserviert und z. T. mit speziellen Algorithmen beschrieben und gelesen. Dabei muss eine "Page" vor der Löschung, ebenso wie der gesamte reservierte Bereich, erst komplett ausgenutzt sein, bevor sie neu beschrieben wird. Dieses Verfahren macht in vielen Fällen das EEPROM in Mikrocontrollern überflüssig.

Allerdings lässt sich ein EEPROM nicht in allen Anwendungen durch Flash ersetzen. Momentan ist es noch nicht möglich, Flash über einen so weiten Temperaturbereich wie EEPROM zuverlässig zu beschreiben, allerdings macht hier die Prozesstechnik Fortschritte und Temperaturkompensation beim Schreiben verbessert das Verhalten. Zudem kann es in bestimmten Anwendungen problematisch sein, eine nicht deterministische Schreibzeit zu haben, die sich bei EEPROM-Emulation mittels Flash ergeben kann, wenn eine Page gelöscht werden muss.

Neben Bausteinen mit parallel herausgeführten Adress- und Datenbussen gibt es auch EEPROMs in Gehäusen mit z. B. nur 8 Anschlüssen, bei denen Adressen und Daten über einen seriellen Bus wie I²C ausgetauscht werden. Derartige EEPROMs werden z. B. auf SDRAM-Modulen vom Hersteller zur Speicherung von Produktparametern verwendet, die dann von der CPU ausgelesen werden können. Mit dieser Information im SPD-EEPROM können die Speichermodule im PC dann automatisch konfiguriert werden.

In EEPROMs gespeicherte Daten können von drei Arten von Ausfallerscheinungen betroffen sein: der begrenzten Lebensdauer bzw. Beschreibbarkeit ("byte endurance"), dem begrenzten Erhaltungsvermögen des Speicherzustands der einzelnen Speicherplätze im EEPROM ("retention") und dem Überschreiben unveränderter Speicherzellen wenn eine benachbarte Speicherzelle geändert wird (bezeichnet als "write disturb")

In der Oxidschicht des Gates der in EEPROMs eingesetzten Floating-Gate-Transistoren sammeln sich eingefangene Elektronen an. Das elektrische Feld der eingefangenen Elektronen summiert sich zu dem Feld des "Floating Gates" und schmälert so das Fenster zwischen den Schwellenspannungen, die für die Speicherzustände Eins bzw. Null stehen. Nach einer bestimmten Anzahl von Schreibvorgängen wird die Differenz zu klein, um unterscheidbar zu bleiben, und die Speicherstelle bleibt dauerhaft auf dem programmierten Wert stehen. Hersteller geben üblicherweise die maximale Anzahl von Schreibvorgängen mit 10 oder mehr an.

Die während der Speicherung in das "Floating Gate" eingebrachten Elektronen können durch die Isolierschicht lecken, dies vor allem bei erhöhten Temperaturen (z.B. 170...180 °C), dadurch einen Verlust des Ladungszustands verursachen und die Speicherstelle so in den gelöschten Zustand zurückversetzen. Hersteller gewährleisten üblicherweise die Beständigkeit gespeicherter Daten für einen Zeitraum von 10 Jahren.

Beim Beschreiben von Speicherzellen wird der Inhalt benachbarter Zellen dann verändert, wenn nach der letzten Änderung der Nachbarzelle insgesamt eine Anzahl von Schreibvorgängen auf dem Chip erfolgte (z.B. Zelle 0x0000 geändert, Zelle 0x0001 aktuell nicht geändert und nach der letzten Änderung von 0x0001 bisher an beliebiger Stelle insgesamt 1 Mio. Schreibvorgänge insgesamt => dann Gefahr von Datenverlust auf Zelle 0x0001). Somit kann die Angabe für "write disturb" zehnmal größer sein als die Angabe für "byte endurance". Kurz bevor "write disturb" erreicht wird, sollte ein Refresh des gesamten EEPROM erfolgen. Es wird jede Speicherzelle einzeln gelesen und neu beschrieben. Möglich ist auch, erst zu lesen dann zu löschen und danach neu zu schreiben.




</doc>
<doc id="1302" url="https://de.wikipedia.org/wiki?curid=1302" title="Elektrotechnik">
Elektrotechnik

Elektrotechnik ist diejenige Ingenieurwissenschaft, die sich mit der Forschung und der Entwicklung sowie der Produktionstechnik von Elektrogeräten befasst, die zumindest anteilig auf elektrischer Energie beruhen. Hierzu gehören der Bereich der Wandler, die elektrischen Maschinen und Bauelemente sowie Schaltungen für die Steuer-, Mess-, Regelungs-, Nachrichten-, Geräte- und Rechnertechnik bis hin zur technischen Informatik.

Die klassische Einteilung der Elektrotechnik war die Starkstromtechnik, die heute in der Energietechnik und der Antriebstechnik ihren Niederschlag findet, und die Schwachstromtechnik, die sich zur Nachrichtentechnik formierte. Als weitere Gebiete kamen die elektrische Messtechnik und die Automatisierungstechnik sowie die Elektronik hinzu. Die Grenzen zwischen den einzelnen Bereichen sind dabei vielfach fließend. Mit zunehmender Verbreitung der Anwendungen ergaben sich zahllose weitere Spezialisierungsgebiete. In unserer heutigen Zivilisation werden fast alle Abläufe und Einrichtungen elektrisch betrieben oder laufen unter wesentlicher Beteiligung elektrischer Geräte und Steuerungen.

Die elektrische Energietechnik (früher Starkstromtechnik) befasst sich mit der Gewinnung, Übertragung und Umformung elektrischer Energie und auch der Hochspannungstechnik. Elektrische Energie wird in den meisten Fällen durch Wandlung aus mechanisch-rotatorischer Energie mittels Generatoren gewonnen. Zur klassischen Starkstromtechnik gehören außerdem der Bereich der Verbraucher elektrischer Energie sowie die Antriebstechnik. Zu dem Bereich der Übertragung elektrischer Energie im Bereich der Niederspannung zählt auch der Themenbereich der Elektroinstallationen, wie sie unter anderem vielfältig im Haushalt zu finden sind.

Die Antriebstechnik, früher ebenfalls als „Starkstromtechnik“ betrachtet, setzt elektrische Energie mittels elektrischer Maschinen in mechanische Energie um. Klassische elektrische Maschinen sind Synchron-, Asynchron- und Gleichstrommaschinen, wobei vor allem im Bereich der Kleinantriebe viele weitere Typen bestehen. Aktueller ist die Entwicklung der Linearmotoren, die elektrische Energie ohne den „Umweg“ über die Rotation direkt in mechanisch-lineare Bewegung umsetzen. Die Antriebstechnik spielt eine große Rolle in der Automatisierungstechnik, da hier oft eine Vielzahl von Bewegungen mit elektrischen Antrieben zu realisieren sind. Für die Antriebstechnik wiederum spielt Elektronik eine große Rolle, zum einen für die Steuerung und Regelung der Antriebe, zum anderen werden Antriebe oft mittels Leistungselektronik mit elektrischer Energie versorgt. Auch hat sich der Bereich der Lastspitzenreduzierung und Energieoptimierung im Bereich der Elektrotechnik erheblich weiterentwickelt.

Mit Hilfe der Nachrichtentechnik, auch "Informations- und Kommunikationstechnik" (früher Schwachstromtechnik) genannt, werden Signale mit elektromagnetischen Wellen als Informationsträger von einer Informationsquelle (dem Sender) zu einem oder mehreren Empfängern (der Informationssenke) übertragen. Dabei kommt es darauf an, die Informationen so verlustarm zu übertragen, dass sie beim Empfänger erkannt werden können (siehe auch Hochfrequenztechnik, Amateurfunk). Wichtiger Aspekt der Nachrichtentechnik ist die Signalverarbeitung, zum Beispiel mittels Filterung, Kodierung oder Dekodierung.

Die Elektronik befasst sich mit der Entwicklung, Fertigung und Anwendung von elektronischen Bauelementen wie zum Beispiel Spulen oder Halbleiterbauelementen wie Dioden und Transistoren. Die Mikroelektronik beschäftigt sich mit der Entwicklung und Herstellung integrierter Schaltkreise. Die Entwicklung der Leistungshalbleiter (Leistungselektronik) spielt in der Antriebstechnik eine immer größer werdende Rolle, da Frequenzumrichter die elektrische Energie wesentlich flexibler bereitstellen können, als es beispielsweise mit Transformatoren möglich ist.

Die Digitaltechnik lässt sich insoweit der Elektronik zuordnen, als die klassische Logikschaltung aus Transistoren aufgebaut ist. Andererseits ist die Digitaltechnik auch Grundlage vieler Steuerungen und damit für die Automatisierungstechnik bedeutsam. Die Theorie ließe sich auch der theoretischen Elektrotechnik zuordnen.

In der Automatisierungstechnik werden mittels Methoden der Mess-, Steuerungs- und Regelungstechnik (zusammenfassend MSR-Technik genannt) einzelne Arbeitsschritte eines Prozesses automatisiert bzw. überwacht. Heute wird üblicherweise die MSR-Technik durch Digitaltechnik gestützt. Eines der Kerngebiete der Automatisierungstechnik ist die Regelungstechnik. Regelungen sind in vielen technischen Systemen enthalten. Beispiele sind die Regelung von Industrierobotern, Autopiloten in Flugzeugen und Schiffen, Drehzahlregelungen in Motoren, die Stabilitätskontrolle (ESP) in Automobilen, die Lageregelung von Raketen und die Prozessregelungen für Chemieanlagen. Einfache Beispiele des Alltags sind die Temperaturregelungen zusammen mit Steuerungen in vielen Konsumgütern wie Bügeleisen, Kühlschränken, Waschmaschinen und Kaffeeautomaten (siehe auch Sensortechnik).

Die elektronische Gerätetechnik befasst sich mit der Entwicklung und Herstellung elektronischer Baugruppen und Geräte. Sie beinhaltet damit den Entwurf und die anschließende konstruktive Gestaltung elektronischer Systeme (Verdrahtungsträger, Baugruppen, Geräte).

In Gebäuden sorgen Elektroinstallationen sowohl für die leitungsgebundene Verteilung elektrischer Energie als auch für die Nutzungsmöglichkeit von Kommunikationsmitteln (Klingeln, Sprechanlagen, Telefone, Fernsehgeräte, Satellitenempfangsanlagen und Netzwerkkomponenten). Neben der leitungsgebundenen Informationsverteilung kommt verstärkt Funkübertragung (DECT, WLAN) zum Einsatz.
Die Gebäudeautomation nutzt Komponenten der Mess-, Steuerungs- und Regelungstechnik in Gebäuden, um den Einsatz elektrischer und thermischer Energie zu optimieren. Im Rahmen der Gebäudeautomation finden zudem verschiedenste Systeme für Gebäudesicherheit Verwendung.

Die Basis der Theorie und Bindeglied zur Physik der Elektrotechnik sind die Erkenntnisse aus der Elektrizitätslehre. Die Theorie der Schaltungen befasst sich mit den Methoden der Analyse von Schaltungen aus passiven Bauelementen. Die theoretische Elektrotechnik, die Theorie der Felder und Wellen, baut auf den Maxwell-Gleichungen auf.

Das Phänomen, dass bestimmte Fischarten (z. B. Zitterrochen oder Zitteraal) elektrische Spannungen erzeugen können (mit Hilfe des Elektroplax), war im alten Ägypten um 2750 v. Chr. bekannt.

Die meteorologische Erscheinung der Gewitterblitze begleitet die Menschheit schon immer. Die Deutung, dass die Trennung elektrischer Ladungen innerhalb der Atmosphäre in Gewittern dieses Phänomen verursacht, erfolgte jedoch erst in der Neuzeit. Elektrostatische Phänomene waren allerdings schon im Altertum bekannt. Die erste Kenntnis über den Effekt der Reibungselektrizität wird dem Naturphilosophen Thales von Milet zugeschrieben. In trockener Umgebung kann Bernstein durch Reiben an textilem Gewebe (Baumwolle, Seide) oder Wolle elektrostatisch aufgeladen werden. Durch Aufnahme von Elektronen erhält Bernstein eine negative Ladung, das Reibmaterial durch Abgabe von Elektronen dagegen eine positive Ladung. Durch die Werke von Plinius dem Älteren wurde dieses Wissen bis ins Spätmittelalter überliefert.

Der englische Naturforscher William Gilbert unterschied im zweiten Kapitel des zweiten Buchs seines im Jahr 1600 erschienenen Werks "Über den Magneten" zwischen Magnetismus und Reibungselektrizität ("„Differentia inter magnerica & electrica“"). Gilbert verwendete somit als Erster den Begriff "Elektrizität", den er aus dem altgriechischen Wort für "Bernstein" (ἤλεκτρον; transkribiert: ḗlektron; übersetzt: Hellgold) abgeleitet hatte.

Im Jahre 1663 erfand Otto von Guericke die erste Elektrisiermaschine, eine Schwefelkugel mit einer Drehachse, die Elektrizität durch von Hand bewirkte Reibung erzeugte.

Um die Mitte des 18. Jahrhunderts wurde von Ewald Georg von Kleist und Pieter van Musschenbroek die Leidener Flasche erfunden, die älteste Bauform eines Kondensators. 1752 erfand Benjamin Franklin den Blitzableiter und veröffentlichte 1751 bis 1753 die Resultate seiner "Experiments and Observations on Electricity". 1792 unternahm Luigi Galvani sein legendäres Froschschenkel-Experiment, in dem eine elektrochemische Galvanische Zelle als Spannungsquelle diente.

Von den Experimenten Galvanis angeregt, baute Alessandro Volta um 1800 die so genannte Voltasche Säule, die erste funktionierende Batterie, mit der zum ersten Mal eine kontinuierliche Spannungsquelle für die elektrotechnische Forschung zur Verfügung stand. 1820 machte Hans Christian Ørsted Versuche zur Ablenkung einer Magnetnadel durch elektrischen Strom. André-Marie Ampère führte diese Experimente weiter und wies 1820 nach, dass zwei stromdurchflossene Leiter eine Kraft aufeinander ausüben. Ampère erklärte den Begriff der elektrischen Spannung und des elektrischen Stromes und legte die Stromrichtung fest. Der Physiker Georg Simon Ohm konnte 1826 nachweisen, dass in einem stromdurchflossenen metallischen Leiter die sich einstellende elektrische Stromstärke I dem Quotienten aus angelegter elektrischer Spannung U und dem jeweiligen elektrischen Widerstand R entspricht. Zu Ehren Ohms wird dieser physikalische Zusammenhang als ohmsches Gesetz bezeichnet.
Michael Faraday leistete einen großen Beitrag auf dem Gebiet der elektrischen und magnetischen Felder, von ihm stammt auch der Begriff der „Feldlinie“. Die Erkenntnisse Faradays waren die Grundlage für James Clerk Maxwells Arbeiten. Er vervollständigte die Theorie des Elektromagnetismus zur Elektrodynamik und deren mathematische Formulierung. Die Quintessenz seiner Arbeit, die 1864 eingereichten und 1865 veröffentlichten Maxwell-Gleichungen, sind eine der grundlegenden Theorien in der Elektrotechnik.

Philipp Reis erfand 1860 am Institut Garnier in Friedrichsdorf das Telefon und damit die elektrische Sprachübermittlung. Allerdings wurde seiner Erfindung keine große Beachtung geschenkt, so dass erst 1876 Alexander Graham Bell in den USA das erste wirtschaftlich verwendbare Telefon konstruierte und auch erfolgreich vermarktete.
Zu den Wegbereitern der „Starkstromtechnik“ gehörte Werner Siemens (ab 1888 von Siemens), der 1866 mittels des dynamoelektrischen Prinzips den ersten elektrischen Generator entwickelte. Elektrische Energie war damit erstmals in nennenswert nutzbarer Menge verfügbar. 1879 prägte Siemens das Wort "Elektrotechnik", als er Heinrich von Stephan die Gründung eines "Elektrotechnischen Vereins" vorschlug. Als dessen erster Präsident setzte er sich für die Errichtung von Lehrstühlen der Elektrotechnik an technischen Hochschulen in ganz Deutschland ein.
1879 erfand Thomas Alva Edison die Kohlefadenglühlampe und brachte damit das elektrische Licht zu den Menschen. In der Folge hielt Elektrizität Einzug in immer größere Bereiche des Lebens. Zur gleichen Zeit wirkten Nikola Tesla und Michail von Dolivo-Dobrowolsky, die Pioniere des Wechselstroms waren und durch ihre bahnbrechenden Erfindungen die Grundlagen der heutigen Energieversorgungssysteme schufen.

Erasmus Kittler begründete 1883 an der TH Darmstadt (heute TU Darmstadt) den weltweit ersten Studiengang für Elektrotechnik. Der Studiengang dauerte vier Jahre und schloss mit einer Prüfung zum Elektrotechnikingenieur ab. 1885 und 1886 folgten das University College London (GB) und die University of Missouri (USA), die weitere eigenständige Lehrstühle für Elektrotechnik einrichteten. Die so ausgebildeten Ingenieure waren erforderlich, um eine großflächige Elektrifizierung zu ermöglichen.
Heinrich Hertz gelang am 13. November 1886 der experimentelle Nachweis der Maxwell-Gleichungen. Die Berliner Akademie der Wissenschaften unterrichtete er am 13. Dezember 1888 in seinem Forschungsbericht „Über Strahlen elektrischer Kraft“ über die elektromagnetischen Wellen. Durch den Nachweis der Existenz elektromagnetischer Wellen wurde er zum Begründer der drahtlosen Informationsübertragung und damit auch der elektrischen Nachrichtentechnik. Im Jahr 1896 führte Alexander Popow eine drahtlose Signalübertragung über eine Entfernung von 250 m durch. Das Verdienst der ersten praktischen Nutzung der Funken-Telegrafie steht Guglielmo Marconi zu. Nachdem er im Juni 1896 seinen Funken-Telegrafen in Großbritannien zum Patent angemeldet hatte, übertrug Marconi im Mai 1897 ein Morsezeichen über eine Distanz von 5,3 Kilometer.
1897 entwickelte Ferdinand Braun die erste Kathodenstrahlröhre. Verbesserte Varianten kamen zunächst in Oszilloskopen und Jahrzehnte später als Bildröhren in Fernsehgeräten und Computermonitoren zum Einsatz.

John Ambrose Fleming erfand 1905 die erste Radioröhre, die Diode. 1906 entwickelten Robert von Lieben und Lee De Forest unabhängig voneinander die Verstärkerröhre, Triode genannt, die der Funktechnik einen wesentlichen Impuls gab.

John Logie Baird baute 1926 mit einfachsten Mitteln den ersten mechanischen Fernseher auf Grundlage der Nipkow-Scheibe. 1928 folgte der erste Farb-Fernseher. Im selben Jahr gelang ihm die erste transatlantische Fernsehübertragung von London nach New York. Bereits 1931 war seine Erfindung jedoch veraltet, Manfred von Ardenne führte damals die Kathodenstrahlröhre und damit das elektronische Fernsehen ein.
1941 stellte Konrad Zuse den weltweit ersten funktionsfähigen Computer, den Z3, fertig. Im Jahr
1946 folgt der ENIAC () von John Presper Eckert und John Mauchly. Die erste Phase des Computerzeitalters begann. Die so zur Verfügung stehende Rechenleistung ermöglichte es den Ingenieuren und der Gesellschaft, völlig neue Technologien zu entwickeln und Leistungen zu vollbringen. Ein frühes Beispiel ist die Mondlandung im Rahmen des Apollo-Programms der NASA.
Die Erfindung des Bipolartransistors 1947 in den Bell Laboratories (USA) durch William B. Shockley, John Bardeen und Walter Brattain und der gesamten Halbleitertechnologie erschloss der Elektrotechnik sehr weite Anwendungsgebiete, da nun viele Geräte sehr kompakt gebaut werden konnten. Ein weiterer wesentlicher Schritt in diese Richtung war die Entwicklung der Mikrointegration: Der 1958 von Jack Kilby erfundene integrierte Schaltkreis (IC) machte die heutigen Prozessorchips und damit die Entwicklung moderner Computer überhaupt erst möglich. Für den Feldeffekttransistor, der aber erst nach 1960 gefertigt werden konnte, hatte Julius E. Lilienfeld bereits 1928 ein Patent erhalten.

1958 erfanden und bauten George Devol und Joseph Engelberger in den USA den weltweit ersten Industrieroboter. Ein solcher Roboter wurde 1960 bei General Motors erstmals in der industriellen Produktion eingesetzt. Industrieroboter sind heute in verschiedensten Industrien, wie z. B. der Automobilindustrie, ein wichtiger Baustein der Automatisierungstechnik.

Gerhard Sessler und James E. West erfanden 1962 das Elektretmikrofon, das damals bis heute am häufigsten produzierte Mikrofon weltweit. Es ist z. B. Bestandteil von Handys und Kassettenrekordern.
Im Jahr 1968 erfand Marcian Edward Hoff, bekannt als "Ted Hoff", bei der Firma Intel den Mikroprozessor und läutete damit die Ära des Personal Computers (PC) ein. Zugrunde lag Hoffs Erfindung ein Auftrag einer japanischen Firma für einen Desktop-Rechner, den er möglichst preisgünstig realisieren wollte. Die erste Realisierung eines Mikroprozessors war 1969 der Intel 4004, ein 4 Bit Prozessor. Aber erst der Intel 8080, ein 8-Bit-Prozessor aus dem Jahr 1973, ermöglichte den Bau des ersten PCs, des Altair 8800.

Die Firma Philips erfand 1978 die Compact Disc (CD) zur Speicherung digitaler Informationen. 1982 resultierte dann aus einer Kooperation zwischen Philips und Sony die Audio-CD. 1985 folgte die CD-ROM.

Im Jahr 1996 präsentierte die Firma Honda den weltweit ersten funktionsfähigen humanoiden Roboter, den P2. Einen ersten prototypischen humanoiden Roboter, der aber noch nicht voll funktionsfähig war, entwickelte bereits 1976 die japanische Waseda-Universität. Aus dem P2 resultierte der zurzeit aktuelle Android, Hondas etwa 1,20 m großer Asimo. Neben vielen elektronischen und elektrotechnischen Komponenten bestehen humanoide Roboter auch wesentlich aus mechanischen Komponenten, deren Zusammenspiel man als Mechatronik bezeichnet.

Eine Fortbildung zum Elektromeister findet an einer Meisterschule statt und dauert 1 Jahr Vollzeit bzw. 2 Jahre berufsbegleitend.

Eine Fortbildung zum Elektrotechniker kann an einer Technikerschule in 4 Semestern Vollzeit bzw. 8 Semestern berufsbegleitend absolviert werden.

Elektrotechnik wird an vielen Universitäten, Fachhochschulen und Berufsakademien als Studiengang angeboten. An Universitäten wird während des Studiums die wissenschaftliche Arbeit betont, an Fachhochschulen und Berufsakademien steht die Anwendung physikalischer Kenntnisse im Vordergrund.

Die ersten Semester eines Elektrotechnik-Studiums sind durch die Lehrveranstaltungen "Grundlagen der Elektrotechnik", Physik und Höhere Mathematik geprägt. In den Lehrveranstaltungen "Grundlagen der Elektrotechnik" werden die physikalischen Grundlagen der Elektrotechnik vermittelt. Diese Elektrizitätslehre umfasst die Themen:
Aufgrund der Interdisziplinarität und der engen Verflechtung mit der Informatik ist auch Programmierung Teil eines Elektrotechnik-Studiums. Weitere Grundlagenfächer sind Elektrische Messtechnik, Digitaltechnik, Elektronik sowie Netzwerk- und Systemtheorie. Als Vertiefungsfächer finden sich beispielsweise Nachrichtentechnik, Regelungstechnik, Automatisierungstechnik, Elektrische Maschinen, Elektrische Energietechnik oder Modellbildung/Simulation.

Der jahrzehntelang von den Hochschulen verliehene akademische Grad "Diplom-Ingenieur" (Dipl.-Ing. bzw. Dipl.-Ing. (FH)) wurde aufgrund des Bologna-Prozesses durch ein zweistufiges System berufsqualifizierender Studienabschlüsse (typischerweise in der Form von Bachelor und Master) ersetzt.
Der Bachelor (Bachelor of Engineering oder Bachelor of Science) ist ein erster berufsqualifizierender akademischer Grad, der je nach Prüfungsordnung des jeweiligen Fachbereichs nach einer Studienzeit von 6 bzw. 7 Semestern erworben werden kann. Nach einer weiteren Studienzeit von 4 bzw. 3 Semestern kann der Master als zweiter akademischer Grad (Master of Engineering oder Master of Science) erlangt werden. Der „Doktor-Ingenieur“ ist der höchste akademische Grad, der im Anschluss an ein abgeschlossenes Masterstudium im Rahmen einer Assistenzpromotion oder in einer Graduate School erreicht werden kann.

An einigen Hochschulen kann der Bachelor-Studiengang "Elektro- und Informationstechnik" in sieben Semestern mit anschließendem dreisemestrigem Master-Studiengang "Master für Berufliche Bildung" studiert werden. Mit diesem Master-Abschluss und nach weiteren 1,5 Jahren Referendariatszeit besteht die Möglichkeit, eine berufliche Tätigkeit als Gewerbelehrer (höherer Dienst) an einer Berufsschule zu finden.

Der größte Berufsverband für Elektrotechnik weltweit ist das "Institute of Electrical & Electronics Engineers" (IEEE). Er zählt über 420.000 Mitglieder und publiziert Zeitschriften auf allen relevanten Fachgebieten in Englisch.
Seit 2008 gab es den "IEEE Global History Network" (IEEE GHN), wobei in verschiedenen Kategorien wichtige Meilensteine (beurteilt durch ein Fachgremium) und persönliche Erinnerungen von Ingenieuren () festgehalten werden können. Solche Erinnerungsberichte von Schweizer Elektroingenieuren können als Beispiele eingesehen werden. Seit Anfang 2015 hat sich der IEEE GHN einer erweiterten Organisation "Engineering and Technology History Wiki" angeschlossen, welche weitere Fachbereiche des Ingenieurwesens umfasst.

Der VDE Verband der Elektrotechnik Elektronik Informationstechnik e.V. ist ein technisch-wissenschaftlicher Verband in Deutschland. Mit ca. 35.000 Mitgliedern engagiert sich der VDE für ein besseres Innovationsklima, Sicherheitsstandards, für eine moderne Ingenieurausbildung und eine hohe Technikakzeptanz in der Bevölkerung.

Der Zentralverband der Deutschen Elektro- und Informationstechnischen Handwerke (ZVEH) vertritt die Interessen von Unternehmen aus den drei Handwerken Elektrotechnik, Informationstechnik und Elektromaschinenbau.
ZVEH-Mitglied waren im Jahr 2014 55.579 Unternehmen, die 473.304 Arbeitnehmer, davon rund 38.800 Auszubildende, beschäftigten. Dem ZVEH als Bundesinnungsverband gehören zwölf Fach- und Landesinnungsverbände mit insgesamt etwa 330 Innungen an.

Der Zentralverband Elektrotechnik- und Elektronikindustrie e.V. (ZVEI) setzt sich für die Interessen der Elektroindustrie in Deutschland und auf internationaler Ebene ein. ZVEI-Mitglied sind mehr als 1.600 Unternehmen, in denen im Jahr 2014 etwa 844.000 Beschäftigte in Deutschland tätig waren. Als ZVEI-Untergliederungen finden sich derzeit 22 Fachverbände.




</doc>
<doc id="1303" url="https://de.wikipedia.org/wiki?curid=1303" title="Elektronik">
Elektronik

Die Elektronik ist die Wissenschaft von der Steuerung des elektrischen Stromes durch elektronische Schaltungen, aber auch durch damit in Zusammenhang stehende elektrische Schaltungen. Elektronik ist außerdem die Bezeichnung für die Gesamtheit elektronischer Anwendungen. Elektronik verarbeitet elektrische Signale informationsmäßig oder erzeugt sie, oder verwandelt elektrische Energie hinsichtlich ihres Spannungs-Strom-Verhältnisses unter Zuhilfenahme von Verstärkern oder Gleichrichtern.

Elektronische Schaltungen werden zumeist auf Platinen aufgebaut und als Modul entweder zu elektronischen Geräten zusammengebaut, oder sie werden Teil elektrotechnischer Apparate.

Die Optoelektronik ist ein Teilgebiet der Elektronik und beschäftigt sich mit der Steuerung durch Licht.

Der Begriff Elektronik leitet sich von dem griechischen Wort "elektron" (ήλεκτρον) ab, das Bernstein bedeutet. Elektronik ist ein Kofferwort, das aus den Begriffen "Elektron" (dem Elementarteilchen) und "Technik" zusammengefügt wurde. Die Elektronik ist sozusagen die "Elektronen-Technik."

1873 entdeckte Willoughby Smith, dass Selen in der Lage ist, bei Licht zu leiten (Photoeffekt). Auf diese Erkenntnis hin entdeckte Karl Ferdinand Braun 1874 den Gleichrichtereffekt. Stoney und Helmholtz prägten den Begriff des Elektrons als Träger des elektrischen Stromes. 1883 erhielt Thomas Alva Edison ein Patent auf einen Gleichspannungsregler, der auf der Glühemission (dem Edison-Richardson-Effekt) beruhte, einer Voraussetzung für alle Vakuumröhren. 1897 begann die Entwicklung der Braunschen Röhre durch Karl Ferdinand Braun. Im Jahre 1899 begann daraufhin die Entwicklung der Spitzendiode. 1904 erlangte John Ambrose Fleming ein Patent auf eine Vakuumdiode.
Zu Beginn des 20. Jahrhunderts war die Entwicklung von Elektronenröhren bereits fortgeschritten. Die ersten Elektronenröhren wurden entwickelt und bereits in elektrischen Schaltungen genutzt. Mit der Triode stand zum ersten Mal ein brauchbares Bauelement zum Aufbau von Verstärkern zur Verfügung. Dadurch wurden Erfindungen wie Rundfunk, Fernsehen und Radar möglich.

Im Jahr 1948 wurde der erste Transistor vorgestellt. Transistoren können wie Röhren als Verstärker, elektronische Schalter oder als Oszillator eingesetzt werden. Jedoch lassen sich Transistoren im Gegensatz zu Vakuumröhren, die sehr viel Raum und elektrische Leistung brauchen, sehr klein fertigen, denn sie basieren auf Halbleitertechnologie, wodurch sehr viel höhere Stromdichten möglich sind.

In den 1960er Jahren gelang die Fertigung von kompletten, aus mehreren Transistoren und weiteren Bauelementen bestehenden Schaltungen auf einem einzigen Siliziumkristall. Die dadurch eingeleitete Technik der integrierten Schaltkreise (kurz IC von engl. ) hat seitdem zu einer stetigen Miniaturisierung geführt. Heute ist die Halbleiterelektronik der wichtigste Zweig der Elektronik.

Als Schlüsseltechnologie für die Zukunft wird zuweilen die Polytronik gesehen. Sie bezeichnet die Zusammenführung kunststoffbasierter Systemfunktionen zu der Vision „intelligentes Plastik“.

Die Analogtechnik beschäftigt sich vor allem mit der Verarbeitung von kontinuierlichen Signalen. Man nutzt dabei die physikalischen Gesetze aus, die das Verhalten der Bauelemente (Widerstände, Kondensatoren, Transistoren, Röhren usw.) beschreiben, oder man schafft durch Schaltungsprinzipien günstige Voraussetzungen. Typische Grundschaltungen sind Stromquellen, Stromspiegel, Differenzverstärker und Kaskaden, sowie Referenzelemente wie die Bandgap. Daraus lassen sich kompliziertere Schaltungen aufbauen, wie z. B. Verstärker, mit deren Hilfe sich weitere Funktionen aufbauen lassen (Oszillator, Filter etc.). Der Operationsverstärker ist ein Verstärker mit einem Differenzeingang (Differenzverstärker). Sein Name rührt daher, dass mit ihm mathematische Operationen (Subtraktion, Addition, Integration etc.) ausgeführt werden können. Operationsverstärker finden in der Analogelektronik breite Anwendung. Der Genauigkeit der Signalverarbeitung sind in der Analogelektronik durch die Herstellungstoleranzen der Bauelemente und deren Nichtidealitäten (z. B. Rauschen, Nichtlinearität, Hysterese) sowie durch weitere störende Effekte wie Übersprechen und Einkopplungen von Störsignalen Grenzen gesetzt. Es wurden sehr weit fortgeschrittene Verfahren entwickelt, die solche Fehler kompensieren oder minimieren und damit Genauigkeiten in der Präzisionselektronik im Bereich von wenigen ppm erlauben. Solche hohe Genauigkeit ist z. B. notwendig, um Analog-Digital-Umsetzer mit 20 Bit Auflösung zu realisieren. Die Analogtechnik bildet prinzipiell die Grundlage der Digitaltechnik.

Die Digitalelektronik oder Digitaltechnik beschäftigt sich mit der Verarbeitung von diskreten Signalen (ausgedrückt als Zahlen oder logische Werte). Die Diskretisierung betrifft dabei immer den Wertebereich und oft auch zusätzlich das zeitliche Verhalten. In der Praxis beschränkt man sich auf zweiwertige Systeme, d. h.: Spannungen oder Ströme sollen – abgesehen von Übergangsvorgängen – nur zwei Werte annehmen (an/aus, 1 oder 0, auch "high/low," kurz H/L). Die Änderung der Werte kann bei zeitdiskreten Systemen nur zu bestimmten, meist äquidistanten Zeitpunkten stattfinden, die ein Takt vorgibt. Bei der Digitalelektronik werden analoge Signale entweder vor der Verarbeitung mit Hilfe von Analog-Digital-Umsetzern digitalisiert (in Digitalsignale umgesetzt) oder existieren bereits von vornherein als diskrete Werte. Transistoren werden in der Digitaltechnik in der Regel als Schaltverstärker und nicht als analoge Verstärker eingesetzt.

Der Vorteil der Digitalelektronik liegt in der Tatsache, dass im Anschluss an die Digitalisierung die bei der Analogelektronik erwähnten störenden Effekte keine Rolle mehr spielen, jedoch auf Kosten des Bauteilaufwandes. Ist z. B. eine analoge Schaltung mit einem maximalen Fehler von 0,1 % behaftet, so kann dieser Fehler ab ca. 10 Bit Datenbreite von digitalen Schaltungen unterboten werden. Ein analoger Multiplizierer benötigt etwa zwanzig Transistoren, ein digitaler Multiplizierer mit derselben Genauigkeit mehr als die zwanzigfache Anzahl. Der Aufwand wächst durch die Digitalisierung also zunächst an, was aber durch die immer weiter vorangetriebene Miniaturisierung mehr als kompensiert wird. Heute lassen sich auf einem integrierten Schaltkreis eine sehr große Menge von Transistoren realisieren (die Anzahl geht typisch in die 10 Millionen). Der Vorteil ist nun, dass z. B. die Spannungspegel in erheblichem Maße variieren können, ohne die korrekte Interpretation als 1 oder 0 zu behindern. Damit ist es möglich, dass die Bauelemente der integrierten Schaltungen sehr ungenau sein dürfen, was wiederum die weitere Miniaturisierung ermöglicht. Die Eigenschaften der Schaltung werden also weitgehend von den physikalischen Eigenschaften der Bauelemente entkoppelt.

Die vereinfachte Beschreibung digitaler Schaltungen mit den zwei Zuständen H und L reicht vor allem bei immer höheren Geschwindigkeiten und Frequenzen nicht immer aus, um sie zu charakterisieren oder zu entwerfen. Im Grenzfall befindet sich die Schaltung den überwiegenden Teil der Zeit im Übergang zwischen den beiden logisch definierten Zuständen. Daher müssen in solchen Fällen oft zunehmend analoge und hochfrequenztechnische Aspekte berücksichtigt werden. Auch bei langsamen Schaltungen kann es Probleme geben, die nur durch analoge Betrachtungsweisen zu verstehen sind; als Beispiel sei das Problem der Metastabilität von Flipflops genannt.

Digitale Schaltungen – auch Schaltsysteme oder logische Schaltungen genannt – bestehen hauptsächlich aus einfachen Logikelementen, wie AND-, NAND-, NOR-, OR- oder NOT-Gattern und Komponenten, mit denen digitale Signale gespeichert werden können, z. B. Flipflops oder Zählern. Alle diese logischen Funktionen lassen sich mit im sogenannten Schalterbetrieb arbeitenden elektronischen Bauelementen (z. B. Transistoren) realisieren. Durch die Integration dieser Schaltungen auf einem Chip (monolithische Schaltung) entstehen komplexe elektronische Bauelemente wie beispielsweise Mikroprozessoren.

Die Hochfrequenzelektronik oder Hochfrequenztechnik beschäftigt sich vorwiegend mit der Erzeugung und der Ausstrahlung sowie dem Empfang und der Verarbeitung von elektromagnetischen Wellen. Anwendungen davon sind z. B. die Funktechnik mit Rundfunk, Fernsehen, Radar, Fernsteuerung, drahtlose Telefonie, Navigation, aber auch die Vermeidung unerwünschter Schwingungen (Störung, EMV) und unkontrollierter Abstrahlung (Abschirmung). Weitere Bereiche der Hochfrequenzelektronik sind Mikrowellentechnik, kabelgebundene Informationsübertragung oder Bereiche der Medizinelektronik. Der Übergang von der Niederfrequenz- zur Hochfrequenztechnik ist fließend. Er beginnt etwa dann, wenn die Frequenz f der elektromagnetischen Welle auf einer Verbindungsleitung der Länge L ein Produkt fL bildet, das zu einer merklichen Phasendrehung ßL = 2π L/λ und somit zu stehenden Wellen führt. Dabei ist λ = λ/(ε) die Wellenlänge auf der Leitung, λ = c/f die Wellenlänge im freien Raum und c die Vakuumlichtgeschwindigkeit. Die Größe ε errechnet sich im einfachsten Fall, je nach Feldverteilung, aus einer Gewichtung der verschiedenen Permittivitätswerte ε in der Leitung. Selbst verlustlose Leitungen können daher nur für kleine Phasendrehung ßL ≪ 1 (entspricht ca. 57,3°) vernachlässigt werden, also nur für fL ≪ c/[2π (ε)]. Bei einer elektronischen Schaltung mit Kabeln von L ≥ 3 m und ε = ε = 2,3 muss für ßL < 5° dann etwa f < 1 MHz. bleiben. Die praktische Hochfrequenzelektronik beginnt somit etwa ab f = 1 MHz, sie ist eine tragende Säule der Informationstechnik.

Selbst im einfachsten Fall benötigt man zwei Angaben zur Beschreibung einer Leitung:
Dabei lassen sich Z und ε in einem quasistatischen Modell auf Platinen bis in den unteren GHz-Bereich noch aus der Leitungskapazität und Leitungsinduktivität pro Längeneinheit berechnen. Ab einigen Gigahertz verfeinert man die Näherung, indem aus den Maxwellschen Gleichungen, aus den Feldern und dem sog. Eigenwert ß mit ß = (ε) 2π/λ verbesserte, frequenzabhängige Werte ε(f) und Z(f) ermittelt werden. Ab einigen 10 GHz sind die Maxwellschen Gleichungen vollnumerisch zu lösen, die Wellen breiten sich im Zick-Zack aus, und es tritt völlig analog zu Lichtwellenleitern der Multimodebetrieb auf, etwa dann, wenn sich zusätzlich auch in transversaler Richtung stehende Wellen ausbilden können. Das gilt für "jede" Leitung, genauer, für jede Struktur bis hin zu Leitungsabzweigungen, Anschlussflächen für Bauelemente und für die Struktur der Bauelemente.

Die Bauelemente R, L und C verlieren selbst in SMD-Bauform schon ab ca. 0,1 GHz ihre idealen Eigenschaften U = RI, U = L dI/dt und I = C dU/dt zwischen Strom I und Spannung U. Ein Widerstand z. B. ist mit steigender Frequenz stets durch kapazitive und bei Stromfluss durch induktive Effekte gekennzeichnet. Elektronische Bauelemente misst man daher zuvor in einer Ersatzumgebung mit 50-Ω-Anschlusskabeln (NWA = Netzwerkanalysator), wobei der Aufbau des Elementes später in der wirklichen Schaltung genau nachgebildet werden muss. Die auf den Anschlussleitungen hinlaufenden, am Messobjekt reflektierten und durch das Objekt transmittierten Wellen stehen bei den passiven Elementen und bei nichtlinearen Elementen (z. B. Transistoren) mit nur kleiner Aussteuerung in einem linearen Zusammenhang: Bei einer 2-Tor-Messung liefert ein NWA dann für jede Frequenz eine 2×2-Streumatrix (s-Parameter), die bei nichtlinearen Elementen noch vom Arbeitspunkt abhängt und das Strom-Spannungsverhalten selbst für f > 50 GHz realitätsnah beschreibt. Diese Daten spiegelt man dann in ein CAD-System ein, das die Kirchhoffschen Gesetze anwendet, um alle U und I zu ermitteln. Die Elemente L bzw. C lassen sich dabei für hohe Frequenzen durch eine Leitung mit ßL ≪ 1 und Kurzschluss bzw. Leerlauf am Ende nachbilden und ein Widerstand R durch eine verlustbehaftete Leitung realisieren, in die eine Welle hineinläuft und wie in einem Sumpf versickert.

Gewisse Bauelemente und Strukturen können aber auch als fertige Modelle aus einem CAD-System übernommen werden, sofern den Modellen vertraut wird, was einer erheblichen Gewissensfrage gleichkommt, denn die gesamte Analyse steht und fällt mit den Modellen. Neben fertigen Modellen und NWA-Messungen kann bei passiven Strukturen durch die vollnumerische Lösung der Maxwellschen Gleichungen sozusagen eine „Software-Messung“ der s-Parameter vorgenommen werden. Um die dabei dramatisch ansteigende Rechenzeit in Grenzen zu halten, greift man in einer Struktur dafür nur die kritischsten Bereiche heraus: Anschlussflächen, Kreuze, Stecker, Antennen, Abzweigungen etc.

Bei Großsignalaussteuerung nichtlinearer Elemente kann bis zu einigen Gigahertz die aus der allgemeinen Elektronik bekannte Modellierung nach SPICE versucht werden. Dabei sind die SPICE-Parameter, die die physikalischen Gleichungen der Modelle „biegsam“ gestalten, so zu wählen, dass die s-Parameter von SPICE-Modell und NWA-Messung bei allen Arbeitspunkten und allen Frequenzen so gut wie möglich übereinstimmen: Bei nur 10 Testarbeitspunkten und 50 Frequenzpunkten mit je 4 s-Parametern ergäben sich bereits 2000 zu prüfende komplexe s-Parameterwerte. Der Aufwand ist enorm und die Modellierung extrem schwierig, selbst für einen einzigen Arbeitspunkt.

Das Rauschen elektronischer Schaltungen ist schon bei mittleren Frequenzen nicht mehr gut durch SPICE-Modelle beschreibbar. Daher misst man analog zur NWA-Messung das Rauschverhalten in einer Ersatzumgebung (Rauschmessplatz). Mit den gewonnenen Rauschparametern (min. Rauschzahl bei optimaler Generatorimpedanz zuzüglich einem äquivalenten Rauschwiderstand) lässt sich im CAD-System umrechnen, wie das Bauelement in der tatsächlichen Schaltung rauscht. Ein Rauschmessplatz ist sehr komplex und erfordert a priori einen NWA.

Ohne die CAD-Systeme ist die Auswertung der vielen Gleichungen unmöglich. Eine sinnvolle Nutzung erfordert darüber hinaus aber tiefe Kenntnisse zu den programmierten Theorien und verwendeten Modellen.

Leistungselektronik bezeichnet das Teilgebiet der Elektrotechnik, das die Umformung elektrischer Energie mit elektronischen Bauelementen zur Aufgabe hat. Die Umformung elektrischer Energie mit Transformatoren oder mit rotierenden Maschinensätzen wird dahingegen nicht zur Leistungselektronik gerechnet.

Zu den wichtigen Bauelementen zählen Widerstand, Kondensator, Transistor, Diode, Spule und die Integrierte Schaltung (kurz IC).

Man spricht von "passiven Bauelementen," wenn primär Widerstände, Kondensatoren und Induktivitäten gemeint sind. Unter den "aktiven Bauelementen" werden meist alle Arten von integrierten Schaltungen, Halbleiterbauelementen und Elektronenröhren verstanden.

Alle diese Bauelemente werden in einer großen Typenvielfalt angeboten. Durch die exakt berechnete Zuordnung der logisch miteinander arbeitenden elektronischen Bauteile auf einer Platine entsteht ein elektronischer Schaltkreis.

Ein selbständig und logisch arbeitender Rechnen-Operator-Chip ist der moderne Prozessor, der nicht nur auf dem Mainboard eines Computers zu finden ist, sondern ein Bestandteil moderner Industrie- und Fahrzeugtechnik ist.

Die Elektronik umfasst heute unzählige Gebiete, von der Halbleiterelektronik über die Quantenelektronik bis hin zur Nanoelektronik. Seit dem Siegeszug des Computers, der stetigen Entwicklung der Informationstechnologie und der zunehmenden Automation hat sich die Bedeutung der Elektronik beständig erweitert. Die Elektronik nimmt heute in unserer Gesellschaft einen großen Stellenwert ein und ist aus vielen Bereichen nicht mehr wegzudenken.

Im Jahre 2007 kamen 38 % aller weltweit hergestellten Elektronikprodukte aus der Asien-Pazifik-Region.
Im Jahre 1995 lag dieser Anteil noch bei 20 %. Allein China erhöhte seinen Anteil von 3 % 1995 auf 16 % 2007. Unter den Top-10-Ländern befinden sich auch Südkorea, Malaysia, Singapur und Thailand. Der Anteil von Westeuropa lag 2007 bei 19 % der globalen Produktion (entspricht ca. 192 Mrd. Euro). Für die Leistungsreihenfolge der Größe der Elektronikfertigung in Westeuropa gilt folgende Rangliste (Stand 2006): Deutschland, Frankreich, Großbritannien, Irland, Italien.


Für freizeitmäßig betriebene Elektronik siehe Hobbyelektronik. Für Elektronik im KFZ siehe Automobilelektronik.




</doc>
<doc id="1304" url="https://de.wikipedia.org/wiki?curid=1304" title="Elektronenkonfiguration">
Elektronenkonfiguration

Die Elektronenkonfiguration gibt die Verteilung der Elektronen in der Elektronenhülle eines Atoms auf verschiedene Energiezustände bzw. Aufenthaltsräume (Atomorbitale) an.

Der Zustand jedes Elektrons der Hülle wird nach dem Bohr-Sommerfeldschen Atommodell sowie dem Orbitalmodell durch vier Quantenzahlen bestimmt:

Gemäß dem Pauli-Prinzip darf der Zustand keiner zwei Elektronen eines Atoms in allen vier Quantenzahlen übereinstimmen. Mit diesem Prinzip lässt sich zeigen, dass sich die Elektronen auf die verschiedenen erlaubten Zustände und damit auf die Schalen und Unterschalen verteilen.

Die Hauptquantenzahlen legen die Schalen fest, die Nebenquantenzahlen die Unterschalen. Jede Schale kann gemäß den Beschränkungen von formula_1, formula_2 und formula_3 mit maximal 2n² Elektronen besetzt werden. Die Schalen werden aufsteigend, beginnend bei der Kernschale, mit Großbuchstaben bezeichnet: K, L, M, N, O, P, Q... Die Orbitale werden entsprechend den Serien von Spektrallinien benannt, die ein angeregtes Elektron aussendet, wenn es in sein ursprüngliches Orbital zurückfällt; die ersten vier Serien heißen aus historischen Gründen s („sharp“), p („principal“), d („diffuse“) und f („fundamental“).

Die äußerste besetzte Schale (Valenzschale) bestimmt das chemische Verhalten und ist daher maßgeblich für die Einordnung ins Periodensystem.

Mit steigender Elektronenzahl der Elemente werden die möglichen Zustände – bei den niedrigen Energien beginnend – besetzt. Gemäß der Hundschen Regel werden dabei die Orbitale gleicher Energie zuerst einfach, dann doppelt belegt.

Die Unterschalen werden in folgender Reihenfolge besetzt (zeilenweise, d. h. periodenweise geordnet):

Im Periodensystem entspricht die Besetzung des s-Orbitals einer neuen Schale dem Sprung in eine neue Periode.

Die Elektronenkonfiguration eines Atoms wird durch die besetzten Unterschalen beschrieben:
Beispiel Chlor: 1s 2s 2p 3s 3p →[Ne] 3s 3p.

Dabei sind die Unterschalen nicht nach dem Aufbauprinzip anzugeben, sondern in der Reihenfolge der Hauptquantenzahl; also z. B. für Europium: <nowiki>[</nowiki>Xe<nowiki>]</nowiki> 4f 6s.

Daneben ist noch die Zellen- oder auch Pauling-Schreibweise als anschauliche grafische Darstellung üblich.



</doc>
<doc id="1305" url="https://de.wikipedia.org/wiki?curid=1305" title="Eritrea">
Eritrea

Eritrea ([]; "Ertra", ) ist ein Staat im nordöstlichen Afrika. Er grenzt im Nordwesten an den Sudan, im Süden an Äthiopien, im Südosten an Dschibuti und im Nordosten an das Rote Meer. Der Landesname leitet sich vom altgriechischen "Erythraia" ab, das auf die Bezeichnung "erythrà [thálassa]", ‚rotes [Meer]‘, zurückgeht. Die Eigenbezeichnung "Ertra" aus Altäthiopisch "bahïrä ertra", ‚Rotes Meer‘, bezieht sich ebenfalls auf diese alte griechische Bezeichnung des Roten Meeres. Ein Viertel der knapp 6 Millionen Einwohner (2016) zählenden Bevölkerung Eritreas konzentriert sich auf die Hauptstadtregion von Asmara, die weiteren Städte sind deutlich kleiner.

Das Hochland von Eritrea war das Königreich Medri Bahri mit der Hauptstadt Debarwa, wo der Baher Negash herrschte, und das Tiefland von Eritrea war mehr als 300 Jahre eine osmanische und ägyptische Kolonie mit der Hauptstadt Massaua. Seit 1890 war Eritrea eine italienische Kolonie. Ab 1941 stand das Land unter britischer Verwaltung und war seit 1952 föderativ mit dem damaligen Kaiserreich Abessinien in Personalunion verbunden, ehe es 1961 als Provinz Eritrea des Äthiopischen Kaiserreiches von Haile Selassie zentralistisch eingegliedert wurde. Nach dreißigjährigem Unabhängigkeitskrieg wurde Eritrea 1993 erstmals seit 1961 wieder von Äthiopien unabhängig.

Heute hat das Land eine der Form nach republikanische Verfassung und wird seit der Unabhängigkeit politisch von der autoritären "Volksfront für Demokratie und Gerechtigkeit" dominiert, die aus der Unabhängigkeitsbewegung der "Eritreischen Volksbefreiungsfront" hervorgegangen ist. Präsident ist seither Isayas Afewerki.

Die beinahe wüstenartige Trockensavanne am Roten Meer ist sehr heiß und trocken. Im Hochland des Landesinneren dagegen fallen jährlich bis zu 600 Millimeter Regen, vor allem in der Zeit von Juni bis September. Die meisten großen Städte Eritreas finden sich im Hochland, auf über 1600 Metern über dem Meer. Im südlichen Hochland befinden sich die wenigen fruchtbaren Regionen des Landes, wie die Gegend von Mendefera, das Umland von Badme und das Grenzdreieck mit Äthiopien und dem Sudan in der Region Gasc-Barca. Auch die höchste Erhebung des Landes, der Soira mit 3018 Metern, südöstlich von Asmara, liegt im Hochland von Abessinien.

Eritrea hat im Westen des Landes auch Anteil an der Sahara: westlich des Flusses Barka und nördlich des Flusses Gasc setzt sich die östliche Sahara vom Sudan her fort und endet mit dem Anstieg zum Hochland von Abessinien.

Im Grenzgebiet mit Dschibuti hat Eritrea Anteil an einer weiteren Wüste: An der südlichen Küste, in der Gegend von Assab, liegt die Danakil-Wüste, eine der heißesten und trockensten Wüsten der Welt. In der Danakilsenke befindet sich mit 110 Metern unter dem Meeresspiegel der tiefste Punkt des Landes.

Die größten Städte sind (Berechnung 2012): Asmara 665.000 Einwohner, Assab 99.000 Einwohner, Cheren 80.000 Einwohner, Massaua 52.000 Einwohner, Mendefera 25.000 Einwohner und Barentù 19.000 Einwohner.

Die Anzahl der Einwohner wird in unterschiedlichen Quellen nicht einheitlich angegeben.
Für das Jahr 2017 wird die Einwohnerzahl
bei den Vereinten Nationen mit 5,1 Millionen,
im CIA World Factbook mit 5,9 Millionen
und bei Statista mit 6,7 Millionen angegeben.


Im internationalen Vergleich ist die Versorgungsquote mit Verhütungsmitteln in Eritrea schlecht. Es ist daher von einem starken Bevölkerungswachstum betroffen, welches zu einem großen Teil auf ungeplanten Schwangerschaften beruht.

So hatten nach Angaben der Deutschen Stiftung Weltbevölkerung im Jahr 2015 nur 7 % der verheirateten Frauen Zugang zu modernen Verhütungsmitteln. Es wird daher geschätzt, dass die Bevölkerung von 6,8 Mio im Jahr 2015 auf ca. 14 Mio im Jahr 2050 anwachsen wird.

In Eritrea gibt es neun größere ethnische Gruppen. Das größte Volk des Landes sind die Tigrinya (55 Prozent, nach anderen Angaben 50 Prozent). Sie leben auch in Äthiopien in der Region Tigray und die tigrinische Sprache ist neben dem Arabischen die Amtssprache des Landes. Die Volksgruppe, die in Eritrea Tigrinya genannt wird, entspricht sprachlich und kulturell den Tigray in Äthiopien, die äthiopischen Tigray und eritreischen Tigrinya sind aber aufgrund einer über längere Zeit getrennt verlaufenden politischen Geschichte nicht mehr als eine einheitliche Gruppe zu betrachten. Historisch bezeichneten sie sich selbst als Habescha. Schon in der Vergangenheit vor der Kolonialzeit waren die Tigrinisch-Sprecher überaus vielgestaltig in Form verschiedener autonomer Provinzen und Abstammungsgruppen und politisch nur selten vereint.

Das zweitgrößte Volk sind die Tigre (30 Prozent). Zu den größeren Volksgruppen zählen noch die Saho (4 Prozent), die Bilen (2 Prozent) und die Rashaida (2 Prozent). Auch die Kunama machen zwei Prozent der Einwohner aus. Die kleinen ethnischen Gruppen Sokodas und Iliit an der sudanesischen Grenze betrachten sich als Kunama, sind aber geographisch und linguistisch getrennt (sie sprechen Dialekte des Ilit-Sokodas, auch West-Kunama genannt).

Die Minderheit der Bedscha wird offiziell als Hedareb bezeichnet, was der Name einer Untergruppe ist. Weitere Minderheiten sind die Nera und die Afar. Außerdem gibt es noch sehr kleine Gruppen westafrikanischen Ursprungs (meist Haussa-Sprecher), die in Eritrea Tokharir genannt werden.

Dabei ist zu beachten, dass die Informationslage dürftig ist. Außerdem leben inzwischen 500.000 bis eine Million Eritreer, zumeist orthodoxe Tigrinya, im Ausland, was bis zu einem Fünftel der Bevölkerung entspricht. Seit 2015 zählt Eritrea neben Nigeria und Somalia als Hauptherkunftsland afrikanischer Flüchtlinge in Europa ("siehe auch Flüchtlingskrise in Europa ab 2015"). Zahlreiche im Ausland lebende politische Flüchtlinge sind wieder in ihre Heimat zurückgekehrt. Eine verschwindend kleine Minderheit bilden europäischstämmige Eritreer, hauptsächlich im 19. Jahrhundert eingewanderte Italiener.

Die Bevölkerung Eritreas teilt sich offiziell zu fast gleichen Teilen in Muslime (Sunniten) und Christen (Eritreisch-Orthodoxe Tewahedo-Kirche, Protestanten, Katholiken, Orthodoxe). Der vom US State Department herausgegebene "International Religious Freedom Report" ging für das Jahr 2007 von 50 Prozent Muslimen und 48 Prozent Anhängern des Christentums in Eritrea aus, für das Jahr 2006 noch von 60 Prozent Muslimen und 37 Prozent Christen. Die "Association of Religion Data Archives" beziffert 50,15 Prozent Muslime und 47,91 Prozent Christen. Daneben bestehen einige kleine einheimische traditionelle Religionen. Trotz der sehr unterschiedlichen Anschauungen und des daraus resultierenden Konfliktpotenzials bildet die Bevölkerung eine nationale Einheit. Die Christen leben vorwiegend in der Hochebene um Asmara und die muslimischen Teile der Bevölkerung hauptsächlich im Tiefland und in Küstennähe.

In den letzten Jahren kam es zur systematischen Verfolgung nicht anerkannter christlicher Minderheiten durch die Regierung, weil diese nicht den ideologischen Paradigmen der Regierungsseite entsprechen. Evangelikale Nachrichtenagenturen aus den USA berichten inzwischen regelmäßig von Christenverfolgungen im Land. Amnesty International gab an, Angehörige staatlich verbotener Minderheitenkirchen seien bei extremer Hitze unter Erstickungsgefahr in Frachtcontainern gefangen gehalten worden.

Die neun Sprachen der neun größten Ethnien gelten formell als gleichberechtigte Nationalsprachen. Diese sind Tigrinisch (2,3 Millionen Sprecher), Tigre (800.000), Afar (300.000), Saho, Kunama, Bedscha, Blin, Nara (je rund 100.000) und Arabisch, das von den Rashaida als Muttersprache und von etlichen anderen Eritreern als Zweitsprache gesprochen wird. Der Staat fördert die Verwendung dieser Sprachen in den Schulen bei den jeweiligen Volksgruppen und in Sendungen des nationalen Radiosenders.

Es gibt keine offiziell festgelegte Amtssprache. De facto dienen aber vorwiegend Tigrinisch und Arabisch – die auch als Verkehrssprachen weit verbreitet sind – sowie Englisch als Arbeitssprachen der Regierung. Italienisch, ein Erbe der Kolonialzeit, wird vor allem von der älteren Bevölkerung verstanden. Viele Schilder und Läden in Asmara sind auch auf Italienisch beschriftet. Tigrinisch und Italienisch werden in der Wirtschaft, im Handel und im Gewerbe am häufigsten gebraucht. Es existiert zudem eine Schule in Asmara, in der Italienisch gelehrt wird – die "Scuola Italiana di Asmara". Italienisch verliert allerdings an Bedeutung, während die Verbreitung des Englischen zunimmt.

Die Sprachen Eritreas gehören zu zwei der großen Sprachfamilien in Afrika: Tigrinisch, Tigre und Arabisch sind semitische Sprachen, Saho, Bilen, Afar und Bedscha sind kuschitische Sprachen – beides Zweige der afroasiatischen Sprachfamilie. Nara (Baria) und Kunama/Baza gehören hingegen zur Familie der Nilosaharanischen Sprachen.

Das Dahalik, das auf Inseln des Dahlak-Archipels von einigen Tausend Personen gesprochen wird, wurde früher als Dialekt des Tigre betrachtet, ist aber nach neueren linguistischen Erkenntnissen eine eigenständige semitische Sprache.

Seit der Unabhängigkeit konnten im Bildungssektor große Fortschritte gemacht werden: Der Alphabetisierungsgrad für Menschen zwischen 15 und 24 Jahren war 2015 mit 93 % (2002: 78 %) einer der höchsten in Subsahara-Afrika.

Formal besteht Schulpflicht für Kinder im Alter von 7 bis 13 Jahren, dennoch besuchen nur zwischen 39 und 57 Prozent der Schulpflichtigen eine Grundschule und nur rund 21 Prozent eine weiterführende Schule. Die Schulen sind schlecht ausgestattet, die durchschnittliche Klassenstärke liegt bei 63 (Grundschulen) beziehungsweise 97 (weiterführende Schulen) Schülern je Klasse. Mädchen sind deutlich benachteiligt. Der Anteil der Analphabeten liegt bei 30 Prozent.

Die Lebenserwartung wird für 2010–2015 auf 63,4 Jahre geschätzt. Die Fruchtbarkeitsrate lag 2012 bei 4,7 Kindern pro Frau. Die Kindersterblichkeit liegt bei 74 auf 1000 Lebendgeburten, womit Eritrea auf dem 51. Platz weltweit liegt. Die Müttersterblichkeit konnte zwischen 1990 und 2013 um 75 % gesenkt werden.

2002 waren noch fast 89 % der Frauen zwischen 15 und 49 Jahren von der Weiblichen Genitalverstümmelung betroffen, nach 94,5 % im Jahre 1995. Deutlicher zeigte sich der Erfolg der Aufklärungsarbeit an der 2002 auch erhobenen Prävalenz unter den Töchtern, je nach Bildungsstand der Mütter 40 % bis 67,5 %, im Mittel 62,5 %. Am 31. März 2007 trat ein gesetzliches Verbot der Frauenbeschneidung in Kraft.

Quelle: UN

Seit der historisch erforschten Frühzeit um 500 v. Chr. herrschten verschiedene Mächte über das Land. Auf dem heutigen Staatsgebiet befand sich das Aksumitische Reich. Während des Mittelalters unterstand das christliche Hochland den äthiopischen Kaisern, in den Küstengegenden herrschten lokale Fürsten. Mit der Eroberung durch die Türken wurde Eritrea 1554 für mehr als 300 Jahre zur Provinz Habeş Eyaleti des Osmanischen Reiches. Während dieser Zeit wurden insbesondere die der äthiopisch-orthodoxen Kirche angehörenden Einwohner der Küstengegenden islamisiert. Die Hauptstadt auf dem Gebiet Eritreas war Massaua.

Erst 1890 wurde Eritrea eine italienische Kolonie unter dem neu geschaffenen Namen "Colonia Eritrea". Nach dem Überfall Italiens auf Äthiopien wurde Eritrea 1936 in das neu gegründete Italienisch-Ostafrika eingegliedert. Es erhielt große Gebiete Nordäthiopiens dazu, so wurde der größte Teil Tigrays Teil von Eritrea. 1941 wurde die Zugehörigkeit zu Italien durch alliierte Streitkräfte beendet. Das Gebiet wurde unter die britische Militärverwaltung gestellt und 1947 – nach der formellen Aufgabe Eritreas durch Italien – britisches Mandatsgebiet. Nach dem Zweiten Weltkrieg entschieden sich die Vereinten Nationen für eine Föderation der Provinz Eritrea mit dem Kaiserreich Abessinien.

Nachdem der äthiopische Kaiser Haile Selassie die politischen Rechte der eritreischen Bevölkerung von 1952 bis 1961 systematisch ausgehöhlt und anschließend 1961 durch die (Selbst-)Auflösung des eritreischen Parlaments Eritrea annektiert hatte, griffen eritreische Separatisten zu den Waffen. Die Unabhängigkeitsbewegungen erhielten in den 1960ern und den darauffolgenden Jahren großen Zulauf.

Der Unabhängigkeitskrieg endete nach dreißig Jahren 1991 mit dem Sieg der Eritreischen Volksbefreiungsfront (EPLF) und verschiedener weiterer äthiopischer Rebellengruppen (u. a. die EPRDF) und der Entmachtung des äthiopischen Derg-Regimes. Die EPRDF bildete eine neue Regierung und erlaubte die Unabhängigkeit Eritreas. Diese wurde nach einer durch die UN überwachten Volksabstimmung am 24. Mai 1993 erklärt, bei der 99,83 % der Teilnehmer für die Unabhängigkeit stimmten. Dieser Tag ist seither Nationalfeiertag Eritreas.

In den darauffolgenden Jahren verschlechterten sich die Beziehungen zwischen Äthiopien und Eritrea. 1998 brach ein Grenzkrieg der beiden Staaten aus, der in einer Pattsituation endete. Seitdem war die UN-Beobachtermission UNMEE in der Grenzregion stationiert, um den rechtmäßigen Grenzverlauf zu markieren, der 2002 von einer unabhängigen Grenzkommission festgelegt wurde. Im Rahmen eines Schiedsspruches der Äthiopisch-Eritreischen Grenzkommission des Ständigen Internationalen Schiedshofes in Den Haag unterzeichneten Äthiopien und Eritrea das Abkommen, in dem sich beide zur Anerkennung des Grenzverlaufs bereiterklärten. Tatsächlich bestehen jedoch weiterhin Differenzen, zumal keine der beiden Seiten alle Ansprüche erfüllt bekam. Das umstrittene Gebiet um Badme wurde der eritreischen Seite zugesprochen, Äthiopien protestierte daraufhin und verlangte eine sofortige Korrektur des Schiedsspruchs. Bis zum heutigen Tage (2015) konnte daher die Umsetzung der Grenzdemarkierung nicht wie vereinbart vollzogen werden. Sämtliche UN-Truppen, die eigentlich zur Friedenssicherung abgestellt worden waren, wurden von eritreischer Seite aus Protest gegen die äthiopische Blockadehaltung massiv in ihren Arbeiten behindert. 2008 entschied der Sicherheitsrat der Vereinten Nationen, das Mandat der UNMEE nicht weiter zu verlängern.

Eritrea besitzt eine offiziell demokratische Verfassung. Wahlen finden auf regionaler und nationaler Ebene statt (Baito). Der Präsident ist Staatsoberhaupt, Oberbefehlshaber der Streitkräfte ist Sebat Efrem. Ein UNHCR-Bericht aus Juni 2015 konstatierte "systematic, widespread and gross human rights violations".

Das Staatsoberhaupt und der Regierungschef sind die höchsten Instanzen der eritreischen Übergangsregierung. Zusammen mit der 24-köpfigen Staatsvertretung, bestehend aus 16 Ministern und weiteren Staatsvertretern, bilden sie die Exekutive Eritreas.

Die Legislative wird von einer 150 Mitglieder umfassenden eritreischen Nationalversammlung gebildet. Von den 150 sind 75 Mitglieder des Zentralkomitees der Volksfront für Demokratie und Gerechtigkeit (PFDJ) und 75 Volksvertreter, die direkt vom Volk gewählt werden. Unter diesen 75 Vertretern des Volkes müssen elf Frauen und 15 Emigranten sein. Die Nationalversammlung wählt den Präsidenten, erlässt Gesetze und Verordnungen und kümmert sich um deren Einhaltung.

Die Judikative Eritreas besteht aus einem Obersten Gerichtshof, 10 Provinzgerichten und 29 Bezirksgerichten.

Die Politik Eritreas wird von der Volksfront für Demokratie und Gerechtigkeit ("PFDJ") dominiert. Die Volksfront für Demokratie und Gerechtigkeit, die aus der früheren bewaffneten Unabhängigkeitsbewegung der Eritreischen Volksbefreiungsfront ("EPLF") hervorgegangen ist, nimmt mit ihrem Parteivorsitzenden Isayas Afewerki auch gleichzeitig den Posten des Staatspräsidenten und Regierungschefs in Anspruch. Eritrea gilt daher als Einparteienstaat. Auch wenn von offizieller Seite bekräftigt wird, dass man sich für ein Parteiengesetz einsetze, sind diese Behauptungen eher kritisch zu sehen. Neben der PFDJ gibt es noch eine Reihe anderer politischer Parteien im Lande, die aber alle nicht zu Wahlen zugelassen und damit quasi illegal sind.

Innerhalb des Landes gibt es noch einige oppositionelle Splittergruppen, die aber bisher keinen Einfluss auf die Politik des Landes nehmen konnten:


Aufgrund andauernder Menschenrechtsverletzungen wurde im Oktober 2012 Sheila Keetharuth zur Sonderberichterstatterin zur Situation der Menschenrechte für Eritrea der Vereinten Nationen ernannt. Ihr aktueller Bericht wurde dem Menschenrechtsrat im Zuge der Resolution 20/20 am 28. Mai 2013 vorgestellt. Darin stellt sie schwerwiegende Menschenrechtsverletzungen wie willkürliche Tötungen und Verhaftungen, erzwungenes Verschwindenlassen, Folter sowie fehlende Meinungs-, Religions- und Versammlungsfreiheit fest.

Auf der jährlich erscheinenden Rangliste der Pressefreiheit, die von der Pressefreiheitsorganisation "Reporter ohne Grenzen" veröffentlicht wird, nimmt das Land 2017 den 179. und damit den vorletzten Platz vor Nordkorea ein. Eritrea gehört nach dieser Darstellung mit zu den Ländern mit einer der geringsten Pressefreiheit. Amnesty International zufolge werden Regierungskritiker, Deserteure und Eritreer, die im Ausland um Asyl ersucht haben, inhaftiert. Insgesamt betrachten viele internationale Beobachter das politische System in Eritrea als repressiv oder gar als Diktatur. Die Regierung hält dem entgegen, dass sich Eritrea nach wie vor im Übergang zur Demokratie befände, von Äthiopien bedrängt würde und sich deswegen bis heute praktisch im Krieg befände. Ein Sturz der jungen Regierung würde dadurch verhindert. In Eritrea sitzen elf Journalisten in Haft.

Staatlich anerkannt sind die orthodoxe, die katholische und die evangelisch-lutherische Kirche sowie der Islam. Nicht anerkannte religiöse Minderheiten wie evangelikale Christen und die Zeugen Jehovas sind besonders seit 2002 von staatlichen Repressionen und Inhaftierung betroffen.

Zu den wegen ihres Glaubens Inhaftierten gehörte Anfang 2008 auch eine Gruppe von etwa 70 Muslimen, die sich weigerten, den von der Regierung eingesetzten Mufti als ihr Oberhaupt anzuerkennen.

Die Haftbedingungen in den mindestens 37 teils geheimen, teils offiziellen Internierungslagern und Militärgefängnissen sind prekär. Es kommt zu Folter, sexuellem Missbrauch und Gewalt. Es wird von Todesfällen berichtet.

In dem jährlich veröffentlichten Weltverfolgungsindex (WVI) von Open Doors, welcher die Länder mit der stärksten Christenverfolgung aufzeigt und analysiert, lag Eritrea 2016 an dritter Stelle. Demnach gehört das Land zu den Ländern auf der Welt, in denen Christen aufgrund ihrer Religionszugehörigkeit am stärksten unterdrückt werden.

Die Beziehungen Eritreas zu seinen Nachbarstaaten sind angespannt. Eritrea wie Äthiopien werden beschuldigt, insbesondere seit 2006/2007 ihre Streitigkeiten nunmehr als „Stellvertreterkrieg“ in Somalia auszutragen. Äthiopien unterstützt die Übergangsregierung Somalias und intervenierte von Ende 2006 bis Anfang 2009 militärisch; Eritrea beherbergt Teile der somalischen Opposition im Exil. Vorwürfe, wonach es Islamisten und andere Gegner der Übergangsregierung illegal mit Waffen beliefert habe, hat es zurückgewiesen. Die separatistische Ogaden National Liberation Front in Äthiopien hat Unterstützung von Eritrea erhalten.

Mitte 2008 kam es zu mehreren Zusammenstößen eritreischer und dschibutischer Truppen im umstrittenen Grenzgebiet beider Staaten. Die USA und der Sicherheitsrat der Vereinten Nationen beschuldigten Eritrea daraufhin der militärischen Aggression.

Im Ausland lebende Eritreer müssen eine „Aufbausteuer“ in Höhe von zwei Prozent ihres Bruttoeinkommens an den eritreischen Staat zahlen. Früher wurde diese von den Botschaften Eritreas in den jeweiligen Ländern erhoben, da Botschaften aber keine Steuern mehr eintreiben dürfen, müssen im Ausland lebende Eritreer jetzt entweder selbst in die Heimat reisen oder einen dort lebenden Verwandten mit der Zahlung beauftragen. Bei Nichtbezahlung werden keine offiziellen Dokumente ausgestellt, es besteht keine Möglichkeit, Erbschaften anzutreten und Geschäftstätigkeiten aufzunehmen, zudem drohen Repressalien gegen im Land lebende Verwandte. Schüler, Studenten oder Arbeitslose sind von der Abgabe befreit. Diese Abgabe, die von hunderttausenden Auslandseritreern erhoben wird, auch wenn sie eine andere Staatsbürgerschaft besitzen, stellt eine der größten Geldquellen der eritreischen Regierung dar.

Bis 1996 war Eritrea in zehn Regionen ("Zoba") gegliedert. Diese noch aus der italienischen Kolonialzeit stammenden Regionen und ihre Regionshauptstädte waren Akkele Guzay (Adi Keyh), Asmara (Asmara), Barka (Agordat), Denkalia (Assab), Gash Setit (Barentu), Hamasien (Asmara), Sahel (Nakfa), Semhar (Massaua), Senhit (Keren) und Seraye (Mendefera).

Mit der Verwaltungsreform vom 15. Juli 1996 wurde die Zahl der Regionen auf sechs reduziert:

Der Tourismus im Land beruht weitestgehend auf wenigen Individualurlaubern, im Ausland lebenden eritreischen Bürgern auf Heimatbesuch und einer kleinen Anzahl ausländischer Reiseveranstalter, die mit in der Regel kleinen Gruppen das Land bereisen. Themengebiete sind unter anderem archäologische Studien, italienische Kolonialgeschichte, Reisen für professionelle Fotografen zu den ethnischen Gruppen des Landes und Reisen für Eisenbahnfans. Badeurlaub wird auch mangels einer geeigneten touristischen Infrastruktur kaum angeboten.

Etwa 75 % der Bevölkerung sind in der Landwirtschaft beschäftigt. Trotzdem müssen Nahrungsmittel importiert werden, auch weil während des Krieges und darüber hinaus mindestens 300.000 Personen zum Militärdienst eingezogen waren und daher Arbeitskräfte in der Landwirtschaft und anderen Wirtschaftsbereichen fehlten. Durch Dürre und wirtschaftspolitische Inkompetenz der autoritären Regierung kam es zu schweren Hungersnöten.

Das Hauptanbaugebiet ist das westliche Tiefland und das Hochland. Angebaut wird vor allem Getreide, Baumwolle, Mais, diverse Gemüsesorten sowie auch eine Vielzahl an verschiedenen Obstsorten.

Eritrea verfügt über Bodenschätze wie Gold, Silber, Kupfer, Schwefel, Nickel, Pottasche, Marmor, Zink und Eisen. Salz wird in großem Umfang produziert. Diese Rohstoffe fördert Eritrea schon seit längerer Zeit für den weltweiten Export.

Es gibt Zement-, Textil- und Nahrungsmittelindustrie, darunter mehrere Brauereiunternehmen, Alkohol- und Weinproduktion. Eritrea verfügt über eine Vielzahl von Ersatzteil- und Möbelunternehmen. Seit einigen Jahren werden in der eritreischen Industriestadt Dekemhare Busse, Transport-, Reinigungs- und Müllwagen von dem eritreischen Unternehmen Tesinma produziert.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 2.165 Millionen US-Dollar, dem standen Einnahmen von umgerechnet 1.580 Millionen US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 10,9 Prozent des BIP.

Im Jahr 2016 betrug die Staatsverschuldung 125,5 Prozent des Bruttoinlandsproduktes (BIP) Eritreas.

2006 betrug der Anteil der Staatsausgaben (in Prozent des BIP) folgender Bereiche:


Die Streitkräfte sind aus der Eritreischen Volksbefreiungsfront (EPLF) hervorgegangen, die für die Unabhängigkeit Eritreas von Äthiopien kämpfte.
Die Beziehungen Eritreas zum Ausland sind gespannt. Unter anderem bedingt durch den dreißigjährigen Unabhängigkeitskrieg gegen Äthiopien, wird die Eigenständigkeit Eritreas stark betont, was zum Teil als Isolationismus bezeichnet wird. Es kam in der jungen Geschichte des Landes zu mehreren Grenzkonflikten, insbesondere zum erneuten Krieg gegen Äthiopien 1998–2000. Dementsprechend nimmt das Militär in Eritrea eine große Rolle ein.

Das Straßennetz in Eritrea ist für afrikanische Verhältnisse relativ gut ausgebaut. Allerdings wurde die von den Italienern sehr gut ausgebaute Infrastruktur zunächst von den Briten und später von den Äthiopiern weitestgehend zerstört, so dass heute nur noch ein kleiner Teil davon übriggeblieben ist. Die meisten Straßen sind Schotterpisten.

Zwischen Massaua und Asmara gibt es eine Eisenbahnverbindung, auf der planmäßig aber nur ein Ausflugszug mit einer Dampflokomotive recht regelmäßig zwischen Asmara nach Nefasit verkehrt. Zudem kommen immer wieder Sonderzüge für Eisenbahnfans auf die Strecke. Es wird erwogen, die historische Strecke zwischen Asmara und Agordat (westliches Tiefland) wieder aufzubauen.

Große Tiefwasserhäfen sind Massaua und Assab, in Tio befindet sich ein kleinerer Hafen im Aufbau.

Flughäfen finden sich in Asmara, außerdem in Massaua, Sawa, Tessenai und Assab. In Nakfa und Barentu gibt es lange Schotterpisten, die jedoch kaum angeflogen werden. Flugverbindungen bestehen zu den europäischen Metropolen Amsterdam und Rom mit Eritrean Airlines, von Kairo mit Egypt Air und über Sanaa bzw. Khartoum mit Yemenia.

Der wichtigste Sport in Eritrea ist der Radsport. Er kam mit den italienischen Kolonialherren ins Land und 1946 wurde erstmals die Giro d’Eritrea ausgetragen. An den Wochenenden werden heute in Eritrea anspruchsvolle Straßenrennen abgehalten. International bekannte Straßenradsportler sind Daniel Teklehaimanot, Natnael Berhane und Merhawi Kudus, die zurzeit (Stand 2015) alle bei dem südafrikanischen Radsportteam MTN-Qhubeka unter Vertrag stehen und Radrennen auf höchster sportlicher Ebene bestreiten. Im Jahr 2015 waren Teklehaimanot und Kudus die ersten Eritreer, die an der Tour de France teilnahmen. In deren Verlauf trug Teklehaimanot sogar für mehrere Tage das Gepunktete Trikot des Führenden in der Bergwertung, was auf den Straßen Asmaras mit einem Autokorso gefeiert wurde. Auch der wohl bekannteste eritreische Sportler, Zersenay Tadese, versuchte sich in seiner Jugend zunächst als Straßenradfahrer, bevor er zum Langstreckenlauf wechselte. Er ist mehrfacher Weltmeister und aktueller Weltrekordhalter im Halbmarathonlauf. Der jüngste Marathonweltmeister der Geschichte ist Ghirmay Ghebreslassie aus Eritrea. Erst 19-jährig gewann er den Marathon der Weltmeisterschaften im August 2015 in Peking.




</doc>
<doc id="1307" url="https://de.wikipedia.org/wiki?curid=1307" title="Euklid">
Euklid

Euklid von Alexandria ( "Eukleídēs", latinisiert ) war ein griechischer Mathematiker, der wahrscheinlich im 3. Jahrhundert v. Chr. in Alexandria gelebt hat.

Über das Leben Euklids ist fast nichts bekannt. Aus einer Notiz bei Pappos hat man geschlossen, dass er im ägyptischen Alexandria wirkte. Die Lebensdaten sind unbekannt. Die Annahme, dass er um 300 v. Chr. gelebt hat, beruht auf einem Verzeichnis von Mathematikern bei Proklos, andere Indizien lassen hingegen vermuten, dass Euklid etwas jünger als Archimedes (ca. 285–212 v. Chr.) war.

Aus einer Stelle bei Proklos hat man auch geschlossen, dass er um das Jahr 360 v. Chr. in Athen geboren wurde, dort seine Ausbildung an Platons Akademie erhielt und dann zur Zeit Ptolemaios’ I. (ca. 367–283 v. Chr.) in Alexandria wirkte.

Er sollte nicht mit Euklid von Megara verwechselt werden, wie das bis in die frühe Neuzeit häufig geschah, so dass der Name Euklids von Megara auch auf den Titeln der Ausgaben der Elemente erschien.

Die überlieferten Werke umfassen sämtliche Bereiche der antiken griechischen Mathematik: das sind die theoretischen Disziplinen Arithmetik und Geometrie ("Die Elemente", "Data"), Musiktheorie ("Die Teilung des Kanon"), eine methodische Anleitung zur Findung von planimetrischen Problemlösungen von bestimmten gesicherten Ausgangspunkten aus ("Porismen") sowie die physikalischen bzw. angewandten Werke ("Optik", "astronomische Phänomene").

In seinem berühmtesten Werk "Elemente" (altgriechisch "Stoicheia" ‚Anfangsgründe‘, ‚Prinzipien‘, ‚Elemente‘) trug er das Wissen der griechischen Mathematik seiner Zeit zusammen. Er zeigte darin die Konstruktion geometrischer Objekte, natürlicher Zahlen sowie bestimmter Größen und untersuchte deren Eigenschaften. Dazu benutzte er Definitionen, Postulate (nach Aristoteles Grundsätze, die akzeptiert oder abgelehnt werden können) und Axiome (nach Aristoteles allgemeine und unbezweifelbare Grundsätze). Viele Sätze der "Elemente" stammen offenbar nicht von Euklid selbst. Seine Hauptleistung besteht vielmehr in der Sammlung und einheitlichen Darstellung des mathematischen Wissens sowie der strengen Beweisführung, die zum Vorbild für die spätere Mathematik wurde.

Erhaltene Schriften von Euklid sind neben den "Elementen", den "Data" und der "Teilung des Kanons": "Optika", "Über die Teilung der Figuren" (auszugsweise erhalten in einer arabischen Übersetzung). Von weiteren Werken sind nur die Titel bekannt: u. a. "Pseudaria" (Trugschlüsse), "Katoptrika" und "Phainomena" (Astronomie).

Die "Elemente" waren vielerorts bis ins 20. Jahrhundert hinein Grundlage des Geometrieunterrichts, vor allem im angelsächsischen Raum.

Neben der pythagoreischen Geometrie enthalten Euklids "Elemente" in Buch VII-IX die pythagoreische Arithmetik, die Anfänge der Zahlentheorie (die bereits Archytas von Tarent kannte) sowie die Konzepte der Teilbarkeit und des größten gemeinsamen Teilers. Zu dessen Bestimmung fand er einen Algorithmus, den euklidischen Algorithmus. Euklid bewies auch, dass es unendlich viele Primzahlen gibt, nach ihm Satz des Euklid genannt. Auch Euklids Musiktheorie baut auf der Arithmetik auf. Ferner enthält das Buch V die Proportionslehre des Eudoxos, eine Verallgemeinerung der Arithmetik auf positive irrationale Größen.

Das bekannte fünfte Postulat der ebenen euklidischen Geometrie (heute Parallelenaxiom genannt) fordert: Wenn eine Strecke formula_1 beim Schnitt mit zwei Geraden formula_2 und formula_3 bewirkt, dass die innen auf derselben Seite von formula_1 entstehenden Winkel formula_5 und formula_6 zusammen kleiner als zwei rechte Winkel sind, dann treffen sich die beiden Geraden formula_2 und formula_3 auf eben der Seite von formula_1, auf der die Winkel formula_5 und formula_6 liegen. Schneiden also zwei Geraden eine Strecke (oder Gerade) so, dass die auf einer Seite von der Strecke und den zwei Geraden eingeschlossenen zwei Winkel kleiner als 180° sind, dann schneiden sich die beiden Geraden auf dieser Seite und begrenzen zusammen mit der Strecke (oder dritten Geraden) ein Dreieck.

Für die Wissenschaftsgeschichte ist die Beschäftigung mit dem Parallelenaxiom von großer Bedeutung, weil sie viel zur Präzisierung mathematischer Begriffe und Beweisverfahren beigetragen hat. Im Zuge dessen wurde im 19. Jahrhundert auch die Unzulänglichkeit der euklidischen Axiome offenkundig. Eine formale Axiomatik der euklidischen Geometrie findet sich in David Hilberts Werk "Grundlagen der Geometrie" (1899), das zu vielen weiteren Auflagen und anschließenden Forschungen geführt hat. Darin wird zum ersten Mal ein vollständiger Aufbau der euklidischen Geometrie geleistet, bis zu der Erkenntnis, dass jedes Modell des Hilbertschen Axiomensystems isomorph zum dreidimensionalen reellen Zahlenraum mit den üblichen Deutungen der geometrischen Grundbegriffe (wie Punkt, Gerade, Ebene, Länge, Winkel, Kongruenz, Ähnlichkeit usw.) in der Analytischen Geometrie ist.
Schon seit der Antike versuchten viele bedeutende Mathematiker vergeblich, das Parallelenaxiom mit den übrigen Axiomen und Postulaten zu beweisen (es wäre dann entbehrlich). Erst im 19. Jahrhundert wurde die Unverzichtbarkeit des Parallelenaxioms mit der Entdeckung einer "nichteuklidischen Geometrie" durch Bolyai und Lobatschewski klar. Die Poincaré'sche Halbebene H (Henri Poincaré) ist ein Modell für ein solches Axiomensystem, in dem das Parallelenaxiom nicht gilt. Somit kann das Parallelenaxiom nicht aus den übrigen Axiomen gefolgert werden (siehe nichteuklidische Geometrie).

In Euklids musiktheoretischer Schrift "Die Teilung des Kanon" (griechisch "Katatomē kanonos", lat. "Sectio canonis"), die als authentisch einzustufen ist, griff er die Musiktheorie des Archytas auf und stellte sie auf eine solidere akustische Basis, nämlich auf Frequenzen von Schwingungen (er sprach von Häufigkeit der Bewegungen). Er verallgemeinerte dabei den Satz des Archytas über die Irrationalität der Quadratwurzel formula_12 und bewies ganz allgemein die Irrationalität beliebiger Wurzeln formula_13. Der Grund für diese Verallgemeinerung ist seine Antithese gegen die Harmonik des Aristoxenos, die auf rationalen Vielfachen des Tons (Halbton … n-tel-Ton) aufbaut. Denn in der pythagoreischen Harmonik hat der Ton (Ganzton) die Proportion 9:8, was Euklid zu seiner Antithese „Der Ton ist weder in zwei noch in mehrere gleiche Teile teilbar“ veranlasste; sie setzt allerdings kommensurable Frequenzen voraus, die in der pythagoreischen Harmonik bis zum Ende des 16. Jahrhunderts (Simon Stevin) angenommen wurden. Die Antithese „Die Oktave ist kleiner als 6 Ganztöne“ stützte er auf die Berechnung des pythagoreischen Kommas. Ferner enthält Euklids "Teilung des Kanons" – wie ihr Titel signalisiert – die älteste überlieferte Darstellung eines Tonsystems am Kanon, einer geteilten Saite, und zwar eine pythagoreische Umdeutung des vollständigen diatonischen Tonsystems des Aristoxenos. Euklids Tonsystem wurde durch Boethius tradiert; es wurde in der Tonbuchstaben-Notation Odos zur Grundlage des modernen Tonsystems.

Nach Euklid sind folgende mathematische Strukturen benannt:

Zudem sind nach Euklid folgende mathematische Sätze und Beweise benannt:

Weiter sind nach Euklid benannt:



Rezeption

Arabische Überlieferung



</doc>
<doc id="1309" url="https://de.wikipedia.org/wiki?curid=1309" title="Erfindung">
Erfindung

Eine Erfindung ist eine schöpferische Leistung, durch die eine neue Problemlösung, also die Erreichung eines neuen Zieles mit bekannten Mitteln oder eines bekannten Zieles mit neuen Mitteln oder von dem ausgelösten Wiederholung eines neuen Zieles mit neuen Mitteln, ermöglicht wird. Von Erfindungen wird besonders oft im Zusammenhang mit technischen Problemlösungen gesprochen, etwa von der Erfindung des Motors oder des Dynamits. Solche Erfindungen können unter Umständen durch ein Patent oder als Gebrauchsmuster geschützt werden. Erfindungen gibt es auch im kulturellen Bereich. So gilt etwa die moderne Oper als Erfindung Claudio Monteverdis.

Vom Begriff der Erfindung ist die Entdeckung abzugrenzen. Eine Entdeckung betrifft bereits Vorhandenes, das bislang unbekannt und dessen Nutzen unbestimmt ist. Damit hat sich infolge der Entdeckung nichts geändert (außer der damit verbundene Wissens­zuwachs eines Einzelnen oder der Allgemeinheit). Beispiele sind die Entdeckung der Schwerkraft, eines Planetoiden, eines chemischen Stoffes oder einer Tierart. Eine Erfindung dagegen betrifft stets eine neue Erkenntnis, die bisher nicht dagewesen ist. Diese Sache steht jedoch mit bereits Bekanntem in einem Zusammenhang, sie tritt nicht als etwas völlig Neues auf. Es werden an bekannten Gegenständen oder Verfahren Veränderungen vorgenommen, so dass ihre Wirkung qualitativ oder quantitativ verbessert wird.

Heute neigt man dazu, Erfindungen nur auf technische Verfahren oder Gegenstände zu beziehen und abstrakte Dinge, wie etwa die Erfindung eines neuen Versmaßes, davon auszunehmen.

Eine exaktere Definition lautet: Entdeckung ist die erstmalige Beschreibung eines Naturgesetzes (z. B. elektrische Kraft zwischen Atomen, Coulombpotential) oder eines aus Naturgesetzen abgeleiteten Gesetzes (Reaktionsgeschwindigkeit einer chemischen Reaktion).
Erfindung hingegen ist die Anwendung der Naturgesetze in bisher nicht dagewesener Konstellation zur Lösung eines gegebenen Problems (Technik). Somit ist jede erstmalige Beschreibung oder Anwendung einer Technik eine Erfindung, zum Beispiel ein Sonnensegel für Raumschiffe. Ein neues Versmaß wendet keine Naturgesetze an und ist damit keine Erfindung, selbst wenn diese Schöpfung neu und genial wäre.

Erste Erfindungen machte bereits der Naturmensch. Sie betrafen insbesondere Werkzeuge, die eine bessere Verwendung von Arm und Hand zur Folge hatten. Nachdem der Mensch die Entdeckung gemacht hatte, dass ein Stein in der Hand die Wirkung des Armes erhöhte, konnte er dem Stein eine besondere Form geben, um dessen Wirkungsweise zu erhöhen. Das führte unter anderem zur Erfindung des Faustkeils, des Beils, der Axt, des Hammers, der Sichel und des Schwerts.

Kritiker argumentieren, der Mensch könne sich nicht als der erste Erfinder bezeichnen. Heute sei aus der Zoologie bekannt, dass sogar „einfache“ Tiere, wie Vögel, die erforderlichen Fähigkeiten besäßen, um Erfindungen zu machen und diese an Artgenossen weiterzugeben. Höhere Säugetiere (Schimpansen, Gorillas) seien hierin sogar sehr gut. Allerdings ist es auch bei Bejahung dieses Ansatzes kaum möglich, ein solches Geschehen in den Bereich der Technik einzuordnen, was für echte Erfindungen definitionsgemäß erforderlich wäre.

Finke und andere (1992) beschäftigten sich mit den Prozessen des kreativen Erfindens unter Berücksichtigung des Geneplore Modells. Nach diesem Modell lassen sich bei Erfindungsprozessen zwei Phasen unterscheiden:
Die preinventive forms besitzen nach Finke und andere eine funktionsunabhängige Ästhetik und zeichnen sich außerdem durch "implicit meaningfulness" aus, so dass sie vielseitig und flexibel interpretierbar sind. Diese Ergebnisse legen nahe, bei kreativen Aufgabenstellungen häufiger das Prinzip "function-follows-form" anzuwenden.

TRIZ ist ein formalisierter Prozess zu konkreten Problemlösungsansätzen, die zu Erfindungen führen können.

Unsere westliche Zivilisation beruht weitgehend auf dem Ge- und Verbrauch von Gütern (und Dienstleistungen). Diese müssen erarbeitet werden. Das wird im Allgemeinen zumindest in seiner Quantität als unangenehm erlebt, daher sind die Menschen weitgehend bestrebt, möglichst effektiv zu arbeiten (Werkzeuggebrauch) beziehungsweise die nötige Arbeit von Maschinen verrichten zu lassen – ein Ziel, dem auch die meisten Erfindungen dienen.

Dazu bedurfte es – außer der Bewältigung der damit aufgeworfenen, oft tiefgreifenden Nebenwirkungen auf anderen Gebieten – der technischen Entwicklung auf dreierlei Stufen:

1. Material: Man braucht vielerlei haltbare, belastbare Werkzeuge

2. Energie: Die Werkzeuge müssen hergestellt, dann muss damit gearbeitet werden.

3. Information: Werkzeug-Bau und -Benutzung erfordern Wissen, Wissensverarbeitung, -weitergabe.

Eine patentfähige Erfindung ist eine

Im deutschen, österreichischen und Schweizer Patentrecht ist geregelt, dass die Erfindung auf einem technischen Gebiet liegen muss. Damit wurde klargestellt, dass ein Patent nur für eine technische Erfindung erteilt werden kann, aber auch in jedem technischen Gebiet gleichermaßen erteilt werden muss.

Entdeckungen sind nicht patentierbar. Ebenso wenig werden wissenschaftliche Theorien, physikalische Gesetze oder mathematische Modelle als Erfindungen angesehen; auch sie werden entdeckt.

Auch geistig-schöpferische (sprich kreative) Werke aus Literatur, Musik oder Kunst werden nicht als Erfindung eingestuft. Ein Rechtsschutz solcher Werke kann sich aus dem Urheberrecht ergeben.

Computerprogramme sind in der Regel keine patentfähigen Erfindungen. Ausnahmen bestehen, wenn das Programm zur Steuerung von Naturkräften verwendet wird (z. B. Airbag, elektronische Motorsteuerung). Die genaue Abgrenzung wird derzeit sehr kontrovers diskutiert (siehe dazu Software-Patente).

Im Europäischen Patentübereinkommen (EPÜ) werden in Artikel 52 die Ausschlüsse vom patentrechtlichen Erfindungsbegriff aufgeführt.






</doc>
<doc id="1311" url="https://de.wikipedia.org/wiki?curid=1311" title="Eklat">
Eklat

Ein Eklat [], in der Schweiz auch Eclat geschrieben, ist ein unerfreulicher Vorfall, der in der Öffentlichkeit für Aufsehen sorgt.

Das Wort wurde in der zweiten Hälfte des 17. Jahrhunderts aus dem Französischen ("éclat", eigentlich „plötzliches lautes Geräusch, Knall, Krach“, auch „Splitter, Bruchstück“, zu "éclater" „zerplatzen, knallen“) ins Deutsche entlehnt und lange in zwei verschiedenen – bereits im Französischen vorhandenen – Bedeutungen gebraucht. Bis Mitte des 19. Jahrhunderts gebräuchlich, heute aber veraltet, ist der Wortsinn „glanzvoller Auftritt, Pracht, Prunk, Pomp, Glanz, Gloria‘“ in Wendungen wie „großen Eklat machen“ oder „mit großem Eklat feiern.“ 

Heute wird das Wort im Deutschen nur noch in der Bedeutung „ärgerliches, Aufsehen erregendes gesellschaftliches oder politisches Ereignis“ bzw. „heftiger, plötzlicher Streit“ gebraucht, also weitgehend synonym zum ebenfalls aus dem Französischen entlehnten „Skandal“. Sinnverwandt ist die „Affäre,“ womit aber eher missliche Angelegenheiten langwieriger Art bezeichnet werden.



</doc>
<doc id="1312" url="https://de.wikipedia.org/wiki?curid=1312" title="Che Guevara">
Che Guevara

Ernesto Rafael Guevara de la Serna, genannt Che Guevara oder einfach Che (* offiziell 14. Juni 1928, nach anderen Quellen bereits 14. Mai 1928 in Rosario, Argentinien; † 9. Oktober 1967 in La Higuera, Bolivien), war ein marxistischer Revolutionär, Guerillaführer, Arzt und Autor.

Er war von 1956 bis 1959 ein zentraler Anführer () der Rebellenarmee der Kubanischen Revolution und ist neben Fidel Castro deren wichtigste Symbolfigur.

Guevara stammte aus einer argentinischen bürgerlichen Familie. Bereits seine während des Medizinstudiums erstellten Reisetagebücher hatten literarische Qualität und wurden mehrmals verfilmt. Einzelne seiner Schriften und Reden beeinflussten revolutionäre Strömungen weit über Kuba hinaus. Sein Leben wie auch die Umstände seines Todes und der posthume Personenkult um ihn waren und sind Gegenstand vielfältiger Betrachtungen in Filmen, Büchern und anderen Medien.

Die US-Zeitschrift "Time" zählte ihn 1999 zu den 100 einflussreichsten Menschen des 20. Jahrhunderts. Eine Fotografie des „Che“ von Alberto Korda, "Guerrillero Heroico," gilt als berühmtestes fotografisches Abbild einer Person, sie zählt zu den Medienikonen.

Guevaras Vorfahren waren argentinische Großbürger. Bereits während seines Medizinstudiums unternahm Guevara zahlreiche Reisen, die er umfangreich kommentierte und dokumentierte. Er empörte sich über die vielfach angetroffene wirtschaftliche Ungleichheit und soziale Ungerechtigkeit in Latein- und Mittelamerika. In Guatemala lernte er seine erste Frau kennen, eine Regierungsangestellte, die ihn mit weiteren politischen Aktivisten bekannt machte. Nach dem von den USA betriebenen Sturz der dortigen Regierung Jacobo Árbenz Guzmán (am 27. Juni 1954) ging er nach Mexiko und traf dort 1955 auf Fidel Castro. Er schloss sich dessen Bewegung des 26. Juli an und ließ sich militärisch ausbilden. Im Dezember 1956 nahm er an der Landung von Castros Revolutionären auf Kuba teil, die den von den USA unterstützten Diktator Fulgencio Batista stürzen wollten. Er wurde während der Kubanischen Revolution zum Kommandanten („Comandante“) ernannt und spielte eine wichtige Rolle im – 1959 letztlich erfolgreichen – Guerillakrieg.

Guevara wurde von Castro als Industrieminister und danach als Leiter der kubanischen Zentralbank eingesetzt. Er strebte eine vollständige Verstaatlichung der kubanischen Wirtschaft und den Aufbau einer Schwerindustrie an. Kapitalflucht und die Emigration von über 10 % der Bevölkerung, nahezu der gesamten früheren Oberschicht, führten zu einem drastischen Rückgang von Wirtschaftsleistung und Produktivität. Auch die von Guevara mit anderen Ländern geschlossenen Handelsverträge verursachten in der Praxis erhebliche Schwierigkeiten. Weiterhin führte auch Ches kritische Haltung gegenüber der „entstalinisierten“ Sowjetunion und seine politischen Sympathien für das China der Kulturrevolution zu Konflikten mit Fidel Castro. Che trat 1964 nach der Rückreise von einem Konferenzauftritt in Algier, der großes internationales Aufsehen erregte, von allen Ämtern zurück und verschwand komplett aus der kubanischen Öffentlichkeit. Er versuchte vergeblich, in anderen Ländern das kubanische Revolutionsmodell voranzutreiben, so im Kongo und später in Bolivien. In Bolivien wurde er 1967 von bolivianischen Regierungssoldaten gefangengenommen und kurz darauf erschossen. Bis heute wird er in Kuba als Volksheld verehrt.

Neben seinen Reiseaufzeichnungen und Tagebüchern, die mehrfach sehr erfolgreich herausgegeben und verfilmt wurden, seinen theoretischen Schriften und seinem politischen und militärischen Handeln ist insbesondere die posthume Wirkung Che Guevaras als Märtyrer und Idol der 68er-Bewegung und der lateinamerikanischen Linken von Bedeutung. Bereits 1968 erfolgte die erste kommerzielle Verfilmung seiner Biografie, der US-Film "Che!."

Ches Selbstverpflichtung zu revolutionären Idealen machte ihn zu einem bedeutenden gesellschaftlichen Führer in Kuba. Sein Anspruch, den „Neuen Menschen“ weniger mit materiellen Anreizen als mit moralischen Ansprüchen, Selbstdisziplin und auch gewaltsamen Mitteln zu erzwingen, führte zu erheblichen Konflikten im nachrevolutionären Kuba. Seine Wirtschaftspolitik war wenig erfolgreich. Kritiker machen ihn darüber hinaus für politische Unterdrückung und die Exekution zahlreicher Gegner verantwortlich.

Als „romantischer Held“, nach Sean O’Hagan in der Nachfolge Lord Byrons gilt er bei seinen Anhängern – weit über Kuba und Südamerika hinaus auch in den Industrieländern – als Synonym für Widerstand, Emanzipation und Rebellion.

Auch Ches bekannte Abbilder entwickelten sich zu allgegenwärtigen Symbolen für Widerstand und Protest. Als Medienikone der 1960er Jahre wird das berühmte und nicht geschützte (das revolutionäre Kuba hatte Vereinbarungen zum Urheberrecht gekündigt) Porträt "Guerrillero Heroico" weltweit vermarktet. Dieses Porträt wurde auch auf der 3 CUP Banknote abgebildet.

Ernesto Guevara wurde während einer Schiffsreise bei einem Zwischenhalt in Rosario geboren. Seine Eltern Celia de la Serna y Llosa (1906–1965) und Ernesto Rafael Guevara Lynch (1901–1987) hatten auch baskische und irische Vorfahren und waren aus gutbürgerlichen Verhältnissen ausgebrochen. Sie waren kurz nach der Hochzeit im November 1927 von Buenos Aires nach Puerto Caraguatay in der Provinz Misiones gezogen, um dort eine Mateplantage zu betreiben. Das Unternehmen lief nicht besonders gut, zeitweilig litt die Familie auch unter finanziellen Engpässen, wobei sie auf geerbte Wertpapiere zurückgreifen konnte.

Im Alter von zwei Jahren erlitt Guevara seinen ersten Asthmaanfall. Die Krankheit begleitete ihn sein Leben lang und prägte seine Persönlichkeit und Entwicklung. Im Jahr 1932 zog die Familie auf ärztlichen Rat in die Stadt Alta Gracia. Zunächst wurde er zuhause von seiner Mutter unterrichtet, las viel – unter anderem Werke der europäischen Literatur in der bedeutenden Bibliothek seiner Familie – und lernte Französisch, das er noch als Erwachsener fließend sprach. Als die Asthmaschübe später seltener wurden, wurde er dazu verpflichtet, doch die Schule zu besuchen. Die Krankheit hinderte ihn auch nicht daran, mit anderen Kindern zu spielen und intensiv Sport zu treiben.

Durch seine Familie, die inzwischen durch die Geburten seiner Geschwister Celia (* 1929), Roberto (* 1932), Ana Maria (* 1934) und Juan Martín (* 1942) auf sieben Personen angewachsen war, wurde er schon früh politisch geprägt. Als nach dem Militärputsch Francos 1936 der Spanische Bürgerkrieg ausbrach, wurde ihr Haus zum Treffpunkt spanischer republikanischer Exilanten. Im Jahr 1941 wechselte er auf das Dean-Funes-Gymnasium in Córdoba, was bedeutete, dass er für den Schulweg insgesamt täglich 70 km zu bewältigen hatte.

Im Jahr 1943 wechselte Ernestos Schwester Celia auf eine Schule in Córdoba – die Eltern zogen dorthin, um den Kindern den beschwerlichen Schulweg zu ersparen. 1946 trennten sich seine Eltern. Guevara erlebte im selben Jahr das Sterben seiner Großmutter unmittelbar mit. Auch deswegen entschied er sich nach bestandener Abiturprüfung in Buenos Aires, wo er bei seiner Mutter lebte, Medizin zu studieren.

Guevara unterbrach sein Medizinstudium mehrmals für umfangreiche Reisen durch Argentinien und Südamerika. Im Oktober 1950 lernte er Maria del Carmen Ferreyra, eine Millionärstochter, kennen und lieben. Die Beziehung war nicht von Dauer. Ein Jahr vor Guevaras Staatsexamen brach er im Dezember 1951 zusammen mit einem Freund, dem angehenden Biochemiker Alberto Granado, in Córdoba auf, um mit einer Norton Modell 18 den lateinamerikanischen Kontinent zu erkunden und unter anderem – ein sehr prägendes Erlebnis – eine Lepra-Kolonie in Peru zu besuchen. Guevara war mit der Ansicht aufgebrochen, in ganz Südamerika seien die Verhältnisse ähnlich wie in Argentinien, doch durch die Reise wurde er sich angesichts des Elends der Landbevölkerung und großer sozialer Gegensätze bewusst, welche Ausnahme sein Wohlstand darstellte.
Die Reisen wurden posthum unter dem Titel "The Motorcycle Diaries" (dt. "Die Reise des jungen Che") verfilmt. Er legte nach Abschluss der Reise in den darauf folgenden sieben Monaten seine restlichen Prüfungen ab und überarbeitete auch sein Reisetagebuch, in dem er festhielt: „Dieses ziellose Streifen durch unser riesiges Amerika hat mich stärker verändert als ich glaubte“.
Sein Medizinstudium schloss er am 11. April 1953 mit dem Doktorgrad in Medizin und Chirurgie ab.

Im Juli 1953 reiste Guevara in Begleitung seines Jugendfreundes Carlos Ferrer nach La Paz in Bolivien. Dort blieben sie sechs Wochen und lernten dabei Ricardo Rojo – einen argentinischen Anwalt – kennen, der wegen seiner antiperonistischen Haltung seine Heimat hatte verlassen müssen. Während Rojo daraufhin nach Ecuador fuhr, reisten Guevara und Ferrer nach Peru. Sie besuchten Machu Picchu, Lima und erreichten schließlich Ende September Guayaquil in Ecuador, wo sie Rojo wiedertrafen. Eigentlich war geplant, als Nächstes nach Venezuela zu fahren, wo Guevara Alberto Granado wiedersehen wollte. Guevara änderte die Reiseplanung jedoch, denn Rojo hatte ihn überzeugt, mit ihm nach Guatemala zu fahren, wo eine Revolution kurz bevorstand. Am 31. Oktober fuhren sie per Schiff nach Panama und von dort aus nach Costa Rica, wo er Plantagen der United Fruit besichtigte. In Costa Rica lernte er auch zwei Kubaner kennen, die Monate zuvor vergeblich versucht hatten, den kubanischen Diktator Fulgencio Batista zu stürzen: Calixto Garcia und Severino Rossel. Unter den Überlebenden dieses gescheiterten Umsturzversuches waren auch Fidel und Raúl Castro, die er zu diesem Zeitpunkt allerdings noch nicht kennenlernte.

Mit dem Tod Stalins und dem langsam beginnenden Tauwetter im Ostblock begann Guevaras Verehrung für den sowjetischen Diktator. Noch aus Costa Rica schrieb er im Dezember 1953 in einem Brief an seine Tante Beatriz: „Vor einem Bild des alten, betrauerten Stalin habe ich geschworen, nicht eher zu ruhen, bis diese kapitalistischen Kraken vernichtet sind. In Guatemala werde ich mich schleifen und tun, was ich tun muss, um ein richtiger Revolutionär zu werden.“ Einen weiteren Brief vom April 1955 unterzeichnete er gar mit "Stalin II."

Am Silvesterabend des Jahres 1953 traf Guevara in Guatemala ein. Wenige Tage später lernte er die Peruanerin Hilda Gadea (1925–1974) kennen, seine spätere Ehefrau. Hilda hatte Wirtschaftswissenschaften studiert, war ein Mitglied der peruanischen Alianza Popular Revolucionaria Americana und arbeitete als Regierungsangestellte in Guatemala-Stadt. Sie pflegte ihn bei seinen Asthmaschüben, half ihm in finanziellen Notlagen und vermittelte ihm Grundlagen des Marxismus sowie Kontakte mit Mitgliedern der linken Regierung Arbenz. In Guatemala traf er auch Ñico López, einen Überlebenden des im Jahr 1953 gescheiterten Versuchs, Batista zu stürzen (Angriff auf die Moncada-Kaserne), durch den er später Fidel Castro kennenlernte. In Guatemala wurde er auch das erste Mal mit seinem Spitznamen „Che“ genannt.

Während seines Aufenthaltes in Guatemala erfolgte ein maßgeblich vom US-Auslandsgeheimdienst CIA organisierter Putsch gegen den guatemaltekischen Präsidenten Jacobo Árbenz Guzmán. Arbenz war 1950 gewählt worden, nachdem der Diktator Jorge Ubico Castañeda gestürzt worden war, und hatte Reformen eingeleitet, die den Armen des Landes helfen sollten. So hatte er einen Mindestlohn eingeführt und brachliegende Ländereien, die meist US-amerikanischen Firmen gehörten, verstaatlicht. Am 18. Juni 1954 marschierten Söldner, zum Schutz wirtschaftlicher Interessen von US-Firmen wie der United Fruit Company und aus Angst vor einer kommunistischen Machtergreifung, von den USA organisiert und logistisch unterstützt ins Land ein, stürzten Arbenz und setzten Castillo Armas ins Amt ein. Eine seiner ersten Amtshandlungen war die Rücknahme der Landreform.

Guevara erlebte US-amerikanische Bombenabwürfe auf Guatemala-Stadt. Viele seiner Freunde wurden nach der Machtübernahme Armas’ verhaftet, so auch Hilda Gadea. Ernesto hingegen konnte in die argentinische Botschaft fliehen, lehnte es allerdings ab, nach Hause zu fliegen. Stattdessen wartete er zwei Monate, bis ihm ein Visum gewährt wurde, das ihm ermöglichte, nach Mexiko einzureisen.

Ernesto Guevara erreichte am 21. September 1954 in Begleitung von Julio Roberto Cáceres Valle, eines guatemaltekischen Kommunisten, Mexiko-Stadt. Zusammen mit ihm schlug er sich die erste Zeit durch. Hilda Gadea folgte ihm nach ihrer Freilassung, sie trafen sich in Mexiko-Stadt wieder. Beide heirateten am 18. August 1955, am 15. Februar 1956 wurde ihr erstes Kind Hilda Beatriz geboren.

Als 1955 der Sturz Juan Peróns erfolgte und in Argentinien Aussicht auf eine Revolution bestand, wollte Ricardo Rojo nach Buenos Aires aufbrechen. Er versuchte Guevara zu überreden mitzukommen, doch dieser hatte andere Pläne. Bereits Ende 1954 hatte er weitere Exilkubaner kennengelernt, die beim gescheiterten Putschversuch 1953 mitgewirkt hatten und nun in Mexiko-Stadt lebten. Durch sie lernte er im Sommer 1955 Fidel Castro kennen. Der Anführer jener Rebellen, die 1953 durch den Angriff auf die Moncada-Kaserne von sich reden machten, war nach seiner Haftentlassung nach Mexiko ins Exil gegangen. Castro bereitete dort mit einer Gruppe von Exilkubanern unter der Hilfe von Alberto Bayo, einem Veteranen des Spanischen Bürgerkriegs und Guerilla-Experten, eine bewaffnete Expedition zurück nach Kuba vor mit dem Ziel, das Batista-Regime zu stürzen. Guevara schloss sich zunächst als Expeditionsarzt der Gruppe an. Im April 1956 wurde seine Teilnahme konkreter, als die Rebellen im 60 Kilometer von Mexiko-Stadt entfernten Chalco eine militärische Ausbildung erhielten. Im Juli wurde das Trainingslager von der Polizei entdeckt und die Rebellen landeten kurzzeitig im Gefängnis. Guevara kam erst nach zwei Monaten als letzter frei, mit der Auflage, das Land zu verlassen. Guevara ignorierte dies und tauchte bei Freunden unter. Nun drängte die Zeit – Kuba hatte von den Rebellen erfahren und Castro wollte schnell aufbrechen. Nachdem er die Motoryacht "Granma" gekauft hatte, trafen sich am 23. November 1956 die Rebellen, insgesamt 86 an der Zahl, in Tuxpan und fuhren zwei Tage später los in Richtung Kuba, das sie am 2. Dezember 1956 erreichten.

Nach der Landung der Yacht "Granma" auf Kuba wurden gleich beim ersten Gefecht die Mehrzahl der Rebellen getötet oder festgenommen. Celia Sánchez und Frank País, die eine „Zweite Front“ in den kubanischen Städten unterhielten, unterstützten die Kämpfer mit Waffen und Medikamenten. Neue Mitstreiter stießen hinzu und ermöglichten eine Fortsetzung des Guerillakampfs. Im Verlaufe der Kämpfe änderte sich Guevaras Rolle schnell von der eines Arztes zu einem direkten Teilnehmer bei bewaffneten Aktionen. Sein Einsatz und sein taktischer Überblick ließen ihn schnell zu einer gefragten militärischen Instanz werden. Gegenüber mutmaßlichen Deserteuren griff er dabei hart durch und schreckte auch nicht davor zurück, Todesurteile selbst zu vollstrecken. Als erster Guerillero nach "Comandante en Jefe" Fidel Castro wurde Guevara am 21. Juli 1957 in den Rang eines "Comandante" der Rebellenarmee der Bewegung des 26. Juli erhoben und mit der Führung der II. Kolonne betraut.

Als seine größte militärische Leistung gilt die Einnahme von Santa Clara am 29. Dezember 1958 nach zweijährigem Guerillakampf gegen die zahlenmäßig weit überlegene, bis März 1958 noch von den USA unterstützte, inzwischen aber demotivierte und überalterte Batista-Armee. Der Weg in die Hauptstadt Havanna war damit frei. Am 1. Januar 1959 flüchtete der Diktator Fulgencio Batista aus Kuba, und Castros Gruppe übernahm die Kontrolle. Am 9. Februar 1959 wurde Guevara zum „geborenen kubanischen Staatsbürger“ ernannt.

Castro wollte nach der Revolution 1959 ein insbesondere von den USA unabhängiges Kuba aufbauen. Guevara wurde neben Fidel Castro, dessen Bruder Raúl Castro, Camilo Cienfuegos und einigen anderen ein wichtiges Mitglied in der neuen kubanischen Regierung. Guevara nahm am Sowjet-Kommunismus orientierte Positionen ein, stärker noch als der vorrangig pragmatisch und realpolitisch geprägte Fidel Castro. Auch für Stalin, das chinesische Modell unter Mao und insbesondere das nordkoreanische hegte er Sympathien. Später (1965) sagte Guevara nach einer Reise nach Pjöngjang, dass Nordkorea ein Modell sei, das auch das revolutionäre Kuba anstreben solle.

Auf dem Höhepunkt seiner politischen Aktivität in Kuba war Guevara Leiter der Nationalbank Kubas und Industrieminister. Der ehemalige Guerillaführer und aus Protest gegen den Regierungskurs als Militärgouverneur der Provinz Camagüey zurückgetretene Huber Matos, der anschließend mit Guevaras Einverständnis wegen Hochverrats zu zwanzig Jahren Haft verurteilt wurde, wirft Guevara und Castro vor, die Revolution gegen Batista zur schleichenden Umgestaltung Kubas in eine kommunistische Diktatur benutzt zu haben.
Unter Guevaras Führung wurden die kubanischen Unternehmen und US-amerikanische Beteiligungen verstaatlicht. Begünstigt von einer großzügig gehandhabten Immigrationsregelung wanderte etwa ein Zehntel der Bevölkerung, unter ihnen fast die gesamte kubanische Oberschicht, in die USA aus – insbesondere nach Florida. Neben politischen Aktivitäten entfalteten einige dieser Exilkubaner – zusammen mit US-Regierungsstellen – in der Folgezeit verdeckte und offene militärische Operationen gegen Kuba. Bekannt wurde die 1961 zum Beginn der Amtszeit John F. Kennedys versuchte Invasion in der Schweinebucht unter Beteiligung von 1.500 Exilkubanern. Ihr Scheitern führte zum bis heute andauernden Wirtschaftsboykott der USA und beschleunigte die Anlehnung der kubanischen Revolution insbesondere an sowjetische Vorbilder.

Nach 1963 kam es zu kontroversen Diskussionen zwischen Fidel Castro, Guevara und den als Wirtschaftsberatern engagierten intellektuellen Marxisten Charles Bettelheim und Ernest Mandel. Guevara galt hierbei als Vertreter eines radikal zentralistischen und schnellstmöglichen Übergangs zum Sozialismus und einer moralischen Mobilisierung des „neuen Menschen“.
Als Industrieminister suchte Guevara die reine Lehre der Planwirtschaft umzusetzen und eine vollständige Verstaatlichung der kubanischen Wirtschaft anzustreben. Die Zuckerproduktion ging in der Folge um ein Drittel zurück, die Getreideproduktion halbierte sich, die Industrialisierungspläne wurden verschoben. Tschechoslowakische Wirtschaftsexperten kritisierten 1962 eine mangelhafte Umsetzung der Planwirtschaft. Bekannt waren jedoch die fehlenden Fachkenntnisse Guevaras in Wirtschaftsfragen. Auf betrieblicher Ebene lehnte er vermehrte materielle Anreize, Freiräume für private Kleinunternehmen und eine Lohndifferenzierung nach Leistung aus ethischen Gründen ab. Guevara war vielmehr von einer Pflicht zur Beteiligung an der kubanischen Revolution, dem sozialistischen Aufbau und dem Kampf gegen Angriffe auf das befreite Kuba überzeugt, was er 1965 unter dem Titel "Der Sozialismus und der Mensch in Kuba" auch schriftlich darlegte. Guevara selbst lebte seine Vorsätze und Ideale vor und verlangte die entsprechende Aufopferungsbereitschaft auch von anderen. Er war regelmäßig bei freiwilligen Arbeitseinsätzen beteiligt und verzichtete öffentlichkeitswirksam auf Vergünstigungen für sich und seine Familie.

Direkt nach dem Sieg der Revolution besaß Guevara als Kommandant der auch als Gefängnis genutzten Garnisonsfestung La Cabaña in Havanna sowie als zeitweiser Vorsitzender des als Revisionsinstanz gegründeten „Obersten Kriegsrats“ die Oberaufsicht über die revolutionären Tribunale gegen vermeintliche oder tatsächliche Gefolgsleute des Batista-Regimes. Unter seiner Verantwortung wurden zahlreiche Todesurteile gefällt und vollstreckt. Auch war er für die Errichtung von Straf- und Arbeitslagern mitverantwortlich, in denen „Gegner der Revolution“ – zu denen auch Homosexuelle zählten – interniert wurden.

Bereits im Juni 1959 unterstützte Guevara lateinamerikanische Guerillagruppen. In Honduras bereiteten sich mehrere Gruppierungen, so die "Frente Revolucionario Sandino," denen u. a. die späteren FSLN-Angehörigen Tomás Borge und Edén Pastora Gómez angehörten, auf einen Sturz der nicaraguanischen Regierung vor. Guevara entsandte zu ihrer Unterstützung ein Schiff mit 300 Handfeuerwaffen nach Puerto Cortés, das jedoch von der honduranischen Armee beim Entladen der Waffen beschlagnahmt wurde.

In La Cabaña heiratete Che Guevara am 2. Juni 1959 seine zweite Frau, Aleida March, nachdem er sich von Hilda Gadea hatte scheiden lassen. An der einfachen zivilen Zeremonie nahmen auch die Comandantes Fidel Castro und Camilo Cienfuegos sowie Celia Sánchez teil. In den nächsten Jahren bekam das Paar vier Kinder:


Che Guevaras erste Frau Hilda lebte mit der gemeinsamen Tochter Hilda „Hildita“ Beatriz Guevara Gadea (* 15. Februar 1956 in Mexiko-Stadt; † 21. August 1995 in Havanna) ab 1959 auch in Havanna. Auf den Kontakt mit seiner Exfrau soll er Aleida March zuliebe weitgehend verzichtet haben. Seine Tochter Hildita nahm er jedoch regelmäßig mit in seine neue Familie.

Einer zwischenzeitlichen außerehelichen Beziehung mit der damaligen Studentin und späteren Journalistin Lilia Rosa López (* 1940) entstammt Guevaras 1964 in Havanna geborener Sohn Omar Pérez López – der Schriftsteller und bildende Künstler erfuhr erst 1989 von Guevaras Vaterschaft.

Im Sommer 1960 besuchte Guevara, während der dortigen Kampagne des „Großen Sprungs nach vorn“, die Volksrepublik China und unterzeichnete fast unmittelbar nach dem öffentlich ausgetragenen chinesisch-sowjetischen Zerwürfnis einen Handelsvertrag mit China. Ende 1960 reiste Guevara in die Tschechoslowakei, in die Sowjetunion (bekannt wurde Guevaras Blumenniederlegung am Grabe Josef Stalins, gegen den Willen der sowjetischen Führung), in die Deutsche Demokratische Republik, nach Nordkorea und Ungarn und schloss mit diesen Ländern Handels- und Kreditvereinbarungen ab.

Zur Absicherung ihrer Politik der Konfrontation mit den USA richtete sich die kubanische Regierung zunehmend an der Sowjetunion aus. Guevara hatte mit der Sowjetunion über Waffenlieferungen verhandelt und nach dem Fehlschlagen der Invasion in der Schweinebucht zusammen mit Raúl Castro Vorbereitungen zur Stationierung russischer Atomwaffen auf Kuba getroffen, was zur weltpolitisch bedeutsamen Kubakrise 1962 führte. Guevara zeigte sich allerdings enttäuscht von der Sowjetunion, die im Sinne ihrer außenpolitischen Doktrin der „friedlichen Koexistenz“ auf dem Höhepunkt der Kubakrise einlenkte. Kurz nach der Kubakrise äußerte er gegenüber Journalisten des "Daily Worker," er hätte Atomraketen in Richtung USA abgefeuert, wenn die Sowjetunion es denn zugelassen hätte.

Am 11. Dezember 1964 hielt Guevara eine vielbeachtete Rede vor den Vereinten Nationen, in der er aus seiner Sicht die damalige Außenpolitik der USA beschrieb und sich zur Frage atomarer Bewaffnung der NATO-Länder und zur Wiedervereinigung der beiden deutschen Staaten äußerte.

Gegenüber dem pragmatischen, realpolitisch geprägten Fidel Castro begann er mit seinen radikalen, von manchen als prochinesisch interpretierten Idealen in Gegensatz zu geraten. 1964 verlagerte Castro unter Druck der Sowjetunion den Schwerpunkt der kubanischen Wirtschaft wieder auf die Zuckerrohrproduktion und verschob die von Guevara angestrebte Industrialisierung „um mindestens zehn Jahre“.

Eine weitere Reise führte Guevara 1964 als Leiter der kubanischen UN-Delegation nach New York. In einer bekannten Rede vor der UN bekannte er sich zur revolutionären Gewalt als Mittel der internationalen Politik und forderte die Übertragung der kubanischen Revolution auf andere Länder. Im Anschluss besuchte er unter anderem erneut die Volksrepublik China im Vorfeld der Kulturrevolution, die Vereinigten Arabischen Emirate, Ägypten, Algerien, Ghana und weitere afrikanische Länder. Bekannt wurden antisowjetische Vorbehalte, die er im Februar 1965 bei einem Besuch einer afrikanisch-asiatischen Solidaritätskonferenz im unabhängigen, sozialistisch regierten Algerien äußerte. Damit geriet er in offenen Konflikt mit der sowjetischen Haltung wie auch mit der kubanischen Führung.

Die Differenzen mit den Castro-Brüdern spitzten sich zu. Nach der Rückkehr in Kuba trat Guevara zur allgemeinen Verwunderung von der öffentlichen Bühne ab und von seinen Ämtern zurück. Er verließ Kuba in der Verkleidung eines Geschäftsmannes, um mit weiteren kubanischen Kämpfern die Rebellen im Kongo zu unterstützen. Am 24. April 1965 erreichte er über den Tanganjikasee den Kongo.

Im Kongo gab es schon seit 1960 bürgerkriegsähnliche Zustände und politische und militärische Bewegungen, die von den USA, der Sowjetunion oder China unterstützt wurden. Guevaras Versuch, dort eine Revolution nach kubanischem Vorbild anzuzetteln, scheiterte aber. Guevara erklärte dies (vgl. "Das Jahr, in dem wir nirgendwo waren") mit dem Phlegma sowie der fehlenden Konsequenz und Organisation der Rebellen um Laurent Kabila im Kongo. Ende 1965 kehrte er enttäuscht nach Kuba zurück. Externe Kritiker sprachen von völlig unzulänglicher Vorbereitung, einer mangelnden Einsicht in die Verhältnisse vor Ort bis hin zu Mängeln bei Sprachkenntnissen, Ausrüstung und Training. Jon Lee Anderson zitiert Warnungen Gamal Abdel Nassers, dem Guevara freundschaftlich verbunden war, vor dem Einsatz im Kongo, die er aber nicht beachtete.

Zunächst war Peru als nächster Einsatzort gedacht, doch gingen die "Comandantes" Guevara und Juan Vitalio Acuña Núñez und andere bewaffnete kubanische Kämpfer, darunter auch die deutschstämmige Tamara Bunke, 1966 schließlich nach Bolivien. Che Guevara führte selbst (allerdings unter falschem Namen) eine Gruppe von 44 Kämpfern unter dem Namen ELN (Nationale Befreiungsarmee). Er war dabei bestrebt, die Erfahrungen mit der Rebellenarmee auf Bolivien zu übertragen. Guevaras persönliche Erfahrungen sind in seinem später veröffentlichten "Bolivianischen Tagebuch" dokumentiert.
Die Gruppe operierte in den bewaldeten Berghängen des östlichen zentralbolivianischen Hochlandes. Ab März 1967 lieferten sie sich dort Scharmützel mit Regierungstruppen. Eine Kontaktaufnahme mit der bolivianischen Bevölkerung, etwa durch einen dem kubanischen Radio Rebelde vergleichbaren Sender, fand nicht statt.

Entgegen den hochgesteckten Erwartungen schlossen sich nur zwei einheimische Bauern der Truppe an – die vorwiegend Quechua sprechende indigene Landbevölkerung blieb auf Distanz zu den Spanisch sprechenden Revolutionären. Genauso blieb die erwartete Unterstützung durch bolivianische Bergarbeiter und die Kommunistische Partei Boliviens (PCB) unter Mario Monje aus.

Die Gruppe wurde im April 1967 in zwei Teile gespalten, Guevara führte die Hauptgruppe an, die Nachhut wurde von Guevaras Stellvertreter Juan Vitalio Acuña Núñez angeführt. Die zwei Gruppen konnten aber wegen des Ausfalls der Funkgeräte nicht mehr miteinander kommunizieren und sich daher auch nicht finden. Im August 1967 wurde die Nachhut aufgerieben, Acuña starb am 31. August 1967 zusammen mit Bunke in einem Hinterhalt bolivianischer Regierungstruppen bei Vado de Puerto Mauricio.

Auch Guevaras Truppe befand sich in der Defensive und bestand am Ende nur noch aus 14 Personen. Er selbst wurde am 8. Oktober 1967 nach einem Gefecht mit bolivianischem Militär bei La Higuera verwundet und zusammen mit Simeón Cuba Sanabria gefangen genommen. Fünf Mitgliedern seiner Gruppe gelang die Flucht nach Chile.
Guevara wurde nach seiner Festnahme durch eine vom späteren Minister und Botschafter Gary Prado Salmón angeführte Eliteeinheit in einem dörflichen Schulhaus in La Higuera inhaftiert. Am 9. Oktober 1967 um 13:10 Uhr wurde Guevara dort von Mario Terán, einem Feldwebel der bolivianischen Armee, auf Weisung des bolivianischen Präsidenten René Barrientos Ortuño ohne vorherige Gerichtsverhandlung und entgegen dem Verbot der Todesstrafe in der bolivianischen Verfassung exekutiert. Im Nebenraum wurde gleichzeitig sein bolivianischer Kampfgefährte Cuba erschossen.

Guevara war es im Frühjahr 1967 noch gelungen, eine Grußadresse an eine Solidaritätskonferenz der OSPAAAL (Organización de Solidaridad de los Pueblos de África, Asia y América Latina) zu versenden. Auch in Deutschland wurde das von Rudi Dutschke und Gaston Salvatore übersetzte Manuskript unter anderem mit der Aufforderung bekannt, „zwei, drei, viele Vietnams“ zu schaffen, sowie mit der Mahnung, sich als Guerilla im Kampf von „unbeugsamem Hass“ antreiben zu lassen, um eine „effektive, gewaltsame, selektive und kalte Tötungsmaschine“ darzustellen. Innerhalb der europäischen Studenten- und Protestbewegung fanden Guevaras Aussagen breiten Widerhall.

Guevara wurde im etwa 30 Kilometer von La Higuera entfernten Vallegrande aufgebahrt und sein Leichnam der Presse vorgeführt. Nach offiziellen Angaben war er im Kampf getötet worden. Später wurde er heimlich begraben, nachdem ihm seine Hände abgetrennt worden waren, um einen Nachweis zur Identifizierung zu haben. In Bolivien gab es keine Todesstrafe, und man wollte eine jahrelange Haft in einem noch nicht einmal vorhandenen Hochsicherheitsgefängnis und die zu erwartenden diplomatischen Verwicklungen vermeiden. Jahre später erst wurden die tatsächlichen Todesumstände nach und nach bekannt.

Die Bilder des toten Guevara – mit ihrer frappierenden Ähnlichkeit zu Darstellungen des toten Christus etwa von Andrea Mantegna – wurden in Zeitungsberichten als Abbild eines modernen Heiligen interpretiert, der zweimal sein Leben für fremde Länder riskiert hatte und es für ein drittes dreingegeben habe. Régis Debray, der Guevara in Bolivien begleitet hatte, bezeichnete Guevara als Mystiker, als Heiligen ohne Gottesglauben. Guevaras Beschwörung des „Neuen Menschen“, dem weniger am materiellen als am geistigen Fortschritt gelegen sei, sei anderen zufolge eher jesuitischen als linken Idealen verpflichtet gewesen. Guevara selbst wird im Umfeld seines Todesortes in Bolivien wie ein religiöser Heiliger verehrt.

Die abgetrennten Hände Guevaras wurden konserviert, zur Identifizierung nach Buenos Aires versandt und später Kuba überlassen. Guevaras Gebeine selbst wurden erst 1997 in Vallegrande entdeckt, nachdem ein ehemaliger Offizier der bolivianischen Armee den Begräbnisort enthüllte. Die sterblichen Überreste Ches und einiger seiner Begleiter wurden exhumiert und nach Kuba überführt, um dort mit einem Staatsbegräbnis in dem eigens geschaffenen Mausoleum Monumento Memorial Che Guevara in Santa Clara beigesetzt zu werden.

Ende 2007 wurden eine Haarlocke und Fingerabdrücke Guevaras und weitere Dokumente der Festnahme für insgesamt 119.500 US-Dollar versteigert.

Vor allem in Kuba gilt "el Che" bis heute als Volksheld. Schulkinder sind täglich angehalten, ihm als revolutionärem Vorbild nachzueifern. So lautet das Motto des Kinderverbandes (Organización de Pioneros José Martí): „Pioneros por el comunismo ¡Seremos como el Che!“ („Pioniere für den Kommunismus – wir werden sein wie Che!“).

Auch in die Kunst, insbesondere Kubas, fand die Verehrung des Che Eingang. So wird er in Carlos Pueblas Lied Hasta siempre, comandante zu einem revolutionär-religiösen Mythos, gleichsam zu einem Märtyrer, erhoben. In einem der bekanntesten Lieder des kubanischen Liedermachers Silvio Rodríguez „Fusil contra fusil“ klingt dies ebenfalls an. Wolf Biermann hingegen sang in seiner deutschen Fassung des „Hasta siempre“ vom „Christus mit der Knarre“, der „kein Bonze“ geworden sei und nicht „vom Schreibtisch aus den Helden“ gespielt hätte.

Sein Tod im Namen einer revolutionären Bewegung machte ihn zu einem Märtyrer linker Unabhängigkeits- und Befreiungsbewegungen in der ganzen Welt. Che Guevara ist heute eine Ikone: Sein Bild findet sich millionenfach auf Kleidungsstücken und Gebrauchsgegenständen. Sich mit ihm zu schmücken ist nicht unbedingt ein politisches Bekenntnis, verspricht aber ideellen (und für Produzenten und Händler der damit ausgestatteten Gegenstände auch finanziellen) Gewinn. Reinhard Mohr sprach von „politisch auf ganzer Linie gescheitert, als Ikone unsterblich“ angesichts des seiner Ansicht nach quasireligiösen Umgangs mit Guevara, der unter anderem von Jean-Paul Sartre posthum als „vollständigsten Mensch unserer Zeit“ beschrieben worden war. Die Verklärung Guevaras wird als Umdeuten eines kämpferischen Kommunisten in eine beliebige Ikone des Unangepasstseins kritisiert, andere sehen einen kompromisslosen Stalinisten unter der Maske des zeitlosen jugendlichen Helden kaschiert. Stephan Lahrem und Christopher Hitchens zufolge sei Guevara ein beliebtes idealisiertes Vorbild gewesen, weniger in den Entwicklungsländern als vielmehr für bürgerliche Städter in den „Wohlstandsgesellschaften“, gerade weil sein Kämpfen und Sterben für revolutionäre Ideale keineswegs einem normalen bürgerlichen Leben entsprach. Hitchens stellt Guevara eher in eine romantische als in eine orthodox linke Tradition, näher an den Reiseschriftsteller und Aufrührer Lord Byron als an Karl Marx. Um als romantische Ikone zu überdauern, müsse man nicht nur möglichst jung sterben, sondern „jung und hoffnungslos“. Guevara habe beide Kriterien erfüllt.

Auch in der Außerparlamentarischen Opposition (APO) Westeuropas während der 1960er Jahre bis hin zur deutschen RAF beriefen sich einige auf Guevaras Thesen vom Guerillakampf oder wurden von Zeitzeugen wie Régis Debray inspiriert. Bei vielen Demonstrationen der Studentenbewegung wurde neben dem Porträt des führenden nordvietnamesischen Revolutionärs Ho Chi Minh und dem Mao Zedongs auch "Guerrillero Heroico" (Der heldenhafte Guerillakämpfer), ein berühmtes Abbild Guevaras, mitgeführt. Bis heute ist das in vielen Variationen verbreitete, stark kontrastierte Abbild Ches mit Barett, rotem Stern und einem über den Betrachter hinwegweisenden Blick zu einer der bekanntesten Aufnahmen des 20. Jahrhunderts geworden. Das Bild wurde von dem kubanischen Fotografen Alberto Korda bei einem Staatsbegräbnis am 5. März 1960 aufgenommen, bei dem Guevara neben anderen offiziellen Trauergästen auf einer Tribüne stand. Nach dem Tod Guevaras wurde das Foto vom Verleger Giangiacomo Feltrinelli weltweit verbreitet.

Der mexikanische Autor und spätere Außenminister im neoliberalen Kabinett Fox, Jorge Castañeda, nutzt seine Biographie Guevaras zur Illustration der Ansicht, wonach die Verehrung Guevaras und seiner militanten Thesen und Aktionen ein Grund für die verzögerte und lange marginale Herausbildung einer lateinamerikanischen Sozialdemokratie gewesen sei.

Der aus Kuba stammende US-Amerikaner Humberto Fontova beschreibt Guevara als ineffektiven wie brutalen Taktiker. Verschiedene Kritiker führen das Scheitern der von Guevara verantworteten Wirtschafts- und Industriepolitik auf seine Persönlichkeit wie auf unzureichende wirtschaftspolitische Konzepte zurück.

Guevara wurden darüber hinaus Folter und Ermordung hunderter kubanischer Häftlinge, der Mord an Kleinbauern im Operationsbereich seiner Guerillatruppen sowie später die Freude an der Exekution von Gegnern und die Einrichtung des ersten Arbeitslagers auf Kuba vorgeworfen.

Eine entsprechende Beschreibung Guevaras als skrupellos und brutal in der linken "taz" rief im Oktober 2007 erhebliches Aufsehen in der deutschen linken Szene hervor, nachdem solche Kritik sonst eher Exilkubanern und früheren Dissidenten aus dem ehemaligen Ostblock zugeordnet worden war. Ähnlich umstritten war die Deutung Gerd Koenens, der von „phantastischen Weltbrandstiftungsszenarien“ Guevaras sprach, „die noch aus der ‚atomaren Asche‘ den Neuen Menschen entstehen sahen“.

Dem von ihm verkörperten Freiheitsideal widerspricht die häufig als stalinistisch definierte kompromisslose Politik gegenüber seinen Gegnern: Während seiner Zeit als Ankläger wurden in der als Gefängnis genutzten Festung La Cabaña als ehemalige Anhänger des Batista-Regimes, als Kollaborateure oder als Vertreter des US-Geheimdienstes beschuldigte Kubaner in revolutionären Militärtribunalen verurteilt. Diese zu Zeiten des Ausnahmezustands in der ersten Jahreshälfte 1959 abgehaltenen Verfahren entsprachen keinerlei rechtsstaatlichen Mindeststandards und lösten internationale Empörung aus. Über die Zahl der von Guevara direkt befohlenen Erschießungen gibt es keine genauen Angaben – 216 Fälle sind namentlich belegt, ein ehemaliger Angehöriger des Tribunals in La Cabaña geht von rund 400 aus, kubanische Oppositionelle rechnen teilweise mit wesentlich höheren Zahlen. Die in den Folgejahren häufigen, auch international kritisierten Tötungen rechtfertigte Guevara 1964 ausdrücklich in einem Debattenbeitrag vor der Vollversammlung der Vereinten Nationen mit der Bemerkung, Kuba befinde sich in einem Kampf auf Leben und Tod. Als Industrieminister schickte Guevara zahlreiche „mangelnder revolutionärer Moral“ beschuldigte Mitarbeiter ohne Gerichtsurteil in das sogenannte „Lager für Besserungsarbeit“ auf der Halbinsel Guanahacabibes, eines der ersten von mehreren hundert in den ersten Revolutionsjahren entstandenen Zwangsarbeitslagern.







Che Guevara – Originaltexte in Übersetzung:


</doc>
<doc id="1313" url="https://de.wikipedia.org/wiki?curid=1313" title="Estnische Sprache">
Estnische Sprache

Estnisch (Eigenbezeichnung: "eesti keel") ist eine flektierend-agglutinierende Sprache und gehört zum ostseefinnischen Zweig der Gruppe der finno-ugrischen Sprachen. Das Estnische ist eng mit dem Finnischen und dem nahezu ausgestorbenen Livischen verwandt. Eine entfernte Verwandtschaft besteht zum Ungarischen.
Estnisch ist die einzige Amtssprache der Republik Estland und wird dort von 950.000 Menschen gesprochen. Durch die historischen Ereignisse des 20. Jahrhunderts gibt es auch im Ausland estnische Gemeinden, die etwa 150.000 Sprecher zählen. Die Gesamtzahl der Sprecher des Estnischen als Muttersprache liegt bei rund 1.100.000.
Der "Sprachcode" ist codice_1 bzw. codice_2 (nach ISO 639).

Das estnische Alphabet verwendet die folgenden Buchstaben:

Hierbei kommen die Buchstaben c, f, š, z, ž, q, w, x und y nur selten, entweder in Fremdwörtern oder fremden Namensgebungen, vor. (Alle gängigen Zeichen fett) Die Vokale a, e, i, o, u, ü, ä, ö und õ können alle in der ersten Silbe eines Wortes vorkommen, in der letzten sind aber nur noch die Vokale a, e, i, u, und in einigen Fremdwörtern o ("metroo"), möglich. Wörter, die mit den Konsonanten g, b oder d beginnen, sind Fremdwörter.

Das Estnische besitzt 9 Monophthonge, die in drei Quantitätsstufen (kurz vs. lang vs. überlang) auftreten können. Die Quantität gilt hierbei als distinktives, also bedeutungsunterscheidendes Merkmal. Weiterhin gelten "Lippenrundung" (gerundet vs. ungerundet) und "Zungenstellung" (vorne vs. hinten) als distinktive Merkmale estnischer Vokale. Es gilt hierbei zu beachten, dass der für das Deutsche typische Einfluss der Quantität auf die Qualität entfällt. Während im Deutschen ein langer E-Laut [eː] in seiner kurzen Artikulation zu einem [ɛ] würde, bleibt im Estnischen die Qualität, also die Gespanntheit, erhalten, sodass [e] zu artikulieren ist.

Der Laut [ɤ], graphematisch durch das Zeichen <õ> dargestellt, ist der gleiche Laut wie das bulgarische <ъ>; im Russischen existiert mit <ы> lediglich ein dazu "ähnlicher" Laut, der jedoch im Gegensatz zum Laut [ɤ] keinen Hinterzungenvokal, sondern einen Zentralvokal darstellt.

Je nach Zählweise umfasst das Estnische zwischen 19 und 36 Diphthonge. Die Differenzen ergeben sich aus der Frage, ob es sich bei den Lautverbindungen um "verbundene" oder "verschmolzene" Einzellaute handelt.

Diese Diphthonge werden um folgende, als losere Lautverbindungen zu betrachtende, Diphthonge ergänzt:

Als schwierig erweist sich jedoch die Filterung standardsprachlicher Diphthonge von jenen, die lediglich in dialektalen Varianten des Estnischen auftreten. Eine konsequente Betrachtung der letzteren Gruppe würde dazu führen, die zweite Liste der Diphthonge erweitern zu müssen.

Das Estnische hat 17 Konsonantenphoneme, die den Vokalen gleich in drei Quantitätsstufen (kurz vs. lang vs. überlang) auftreten können. Auch bei den Konsonanten gilt die "Quantität" als distinktiv und wird durch die Merkmale von "artikulierendem Organ und Artikulationsstelle" sowie der "Artikulationsart" ergänzt.

Die Laute ʒ und kommen jedoch lediglich in Fremdwörtern vor.

Auffälligkeiten ergeben sich auch im Hinblick auf die Plosive, die im Estnischen nicht aspiriert, also behaucht, werden. Diese gelten im Estnischen weiterhin als Varianten der Phoneme /p,t,k/. Darüber hinaus wird das Graphem <nowiki></nowiki> grundsätzlich stimmlos artikuliert.

Im Estnischen liegt der Wortakzent grundsätzlich auf der ersten Silbe. Eine Ausnahme bildet hier jedoch "aitäh!" (deutsch: "danke!"). Des Weiteren ist für Lehn- und Fremdwörter charakteristisch, dass die Akzentuierung der Ausgangssprache zumeist beibehalten wurde. Bei estnischen Wörtern kann zudem ein Nebenakzent auf der dritten oder einer anderen ungeraden Silbe liegen, was vor allem im Falle der zahlreichen Komposita deutlich wird.

Das Estnische kennt keine grammatischen Geschlechter. In der dritten Person Singular wird für Personen beiderlei Geschlechts das Pronomen "tema" (Kurzform: "ta") verwendet. Das heißt, dass zwischen Maskulinum und Femininum nicht unterschieden wird. Das gilt auch für Berufsbezeichnungen, so kann das estnische Wort "õpetaja" sowohl 'Lehrerin' als auch 'Lehrer' bedeuten.

Bezüglich der grammatischen Kategorie des Kasus unterscheidet man im Estnischen 14 Fälle. Bei der estnischen Sprache handelt es sich um eine Akkusativsprache, doch ist der Akkusativ als solcher nicht mehr zu erkennen. Wie auch im Finnischen ist der historische Akkusativ im Laufe der Sprachentwicklung lautgesetzlich mit dem Genitiv zusammengefallen. In der Tat spielen die estnischen Fälle, so viele es auch sind, für die Auszeichnung von Agens und Patiens keinerlei Rolle, dieselbe wird nur durch die Wortstellung und die Verbform bewerkstelligt.

Transitiv gebrauchte transitive Verben bereiten naturgemäß die kleinsten Probleme, die Reihenfolge lautet hier: Agens Verb Patiens. Intransitiv gebrauchte transitive Verben werden in der Grundform elliptisch, also sich auf einen obliquen Patiens beziehend, verstanden. Um das involvierte Substantiv selbst zum Patiens zu machen, es sozusagen in den Absolutiv zu setzen, wird der Verbstamm um „-u“ erweitert. Dies wird am folgenden Beispiel deutlich: "muutma" (ändern): "ta muudab" (er/sie/es ändert (irgendetwas)), "ta muutub" (er/sie/es ändert (sich)).

Ursprünglich intransitive Verben werden „absolutiv“ verstanden, das dem Verb vorangehende Substantiv ist also der Patiens. Diese können indes „transitiviert“ werden, mit der Bedeutung, dass irgendetwas dazu veranlasst wird, eine bestimmte Handlung zu vollführen und anschließend wieder elliptisch gebraucht werden. Diese „Transitivierung“ geschieht durch eine Erweiterung des Verbstammes um „-ta“, wodurch im Estnischen der Kausativ gebildet wird: "langema" ((im Krieg) fallen): "ta langeb" (er/sie/es fällt (im Krieg)), "ta langetab" (er/sie/es fällt oder senkt (irgendetwas, aber vermutlich einen Baum oder den Kopf)).

Schließlich besitzt das Estnische auch die Möglichkeit, mit Hilfe von "-ise" (selbst) reflexive Konstruktionen zu bilden: "Ma küsin endalt." (Ich frage mich.)

Bei dieser quasireflexiven Konstruktion liegt indes der Verdacht nahe, dass es sich dabei um einen Germanismus handelt, denn die zuvor beschriebene Sprachkonzeption zur Auszeichnung von Agens und Patiens kommt offensichtlich ohne reflexive Konstruktionen und Passivformen aus und ist in diesem Sinne als „ergativ gedacht“ zu bezeichnen.

Ortsangaben

Gewöhnlich liest man davon, dass der Inessiv im Gegensatz zum Adessiv dann gebraucht werde, wenn etwas sich nicht an einer Seite von etwas, sondern in seinem Inneren befindet. Der Inessiv ähnelt jedoch sehr der Verwendung der Präposition „in“ im Deutschen, und von der gilt das vorige im Gegensatz zur Präposition „an“ auch keineswegs. Beispiel: „Ah, Günther ist wieder im Land.“ – was ja nicht heißt, dass Günther in der Erde steckte. Insbesondere fällt im Estnischen der Schnee "ins" und nicht "aufs" Land. Die Regel, soweit man davon sprechen kann, ist hier, dass Dinge, die nur in einem übertragenen Sinn ein Inneres haben, mit dem Adessiv gebraucht werden, z. B. an der Arbeit sein, und Dinge, die n-dimensional ausgedehnt gedacht werden, den Inessiv für ihr n-dimensional Inneres und den Adessiv für ihren (n-1)-dimensionalen Rand nach sich ziehen, wobei n aus {1,2,3}.

Allerdings befolgt das Estnische diese Regel flächiger als das Deutsche, wie z. B. den Handschuh in die Hand zu ziehen, das Hemd in den Rücken und die Mütze in den Kopf.

Verben unterliegen im Estnischen den grammatischen Kategorien Modus, Tempus, Genus verbi, Person und Numerus.

Die folgende Tabelle zeigt die Konjugation estnischer Verben am Beispiel von "kirjutama" (schreiben) im Präsens:
Die Verneinung ist im Estnischen der des Finnischen ähnlich. Beide Sprachen haben sich jedoch aus gemeinsamen Wurzeln unterschiedlich entwickelt. Während die Verneinung im Finnischen mithilfe eines Verneinungsverbs gebildet wird, ist dieses zwar im Estnischen auch vorhanden, jedoch nicht konjugierbar, sodass in der einschlägigen Literatur häufig auch von einer Verneinungspartikel, die man als ein Äquivalent zum deutschen „nicht“ betrachten kann, gesprochen wird. Mit deren Hilfe erfolgt die Verneinung wie am Verb "mängima" (deutsch: „spielen“) dargestellt:

Präsens: ei + Präsensstamm
Präteritum: ei + "nud-"Partizip
Perfekt: ei + Präsensstamm von "olla" + "nud-"Partizip
Die Struktur "ei ole" wird im Präsens auch dazu genutzt, Aussagen bzw. deren Teile zu verneinen:

"Ei ole" wird im Estnischen auch oft in seiner Kurzform "pole" gebraucht:

Ein zweites Äquivalent zum deutschen „nicht“ ist das estnische Wort "mitte". Während es einerseits, wie im ersten der folgenden Beispielsätze, den Charakter einer Konjunktion annehmen kann, um eine Gegenüberstellung auszudrücken, liegt eine häufige Gebrauchsform auch in Imperativsätzen ohne Prädikat:

Deutlich mehr als andere finno-ugrische Sprachen hat das Estnische durch den Einfluss des Deutschen Ordens im Baltikum Lehnwörter aus dem Hochdeutschen und der Niederdeutschen Sprache übernommen, beispielsweise "riik – Staat" (vgl. finnisch "valtakunta"), "müts – Mütze" (vgl. finnisch "lakki"; aber "myssy" < schwed. "mössa" ~ dt. "Mütze"), "käärid – Schere" (vgl. finnisch "sakset"), "vürts – Gewürz" (vgl. finnisch "mauste"). Andere Entlehnungen aus dem Deutschen sind "reisibüroo" und "reklaamibüroo". Die Zahl der Lehnwörter aus dem Deutschen wird auf 2000 geschätzt. Neben direkten Entlehnungen gibt es eine Reihe an deutschen Lehnübersetzungen, insbesondere bei Partikelverben. Auch gibt es etwa 350 aus dem Russischen entlehnte Wörter wie "pirukad" (vgl. russisch "пирожки").

Ähnlich wie in einigen romanischen Sprachen (Spanisch, Französisch) ist für das Estnische ein „st“ am Wortanfang untypisch, bei Entlehnungen werden (bzw. wurden) die betroffenen Wörter der estnischen Phonotaktik angepasst (z.  B. "tool" (ndd. Stohl), "tikk" (eng. stick), "tudeng" (Student), "torm" (ndd. Storm)). Diese Erscheinung nimmt jedoch im Zulauf eines neueren fremdsprachlichen Wortschatzes ab: (z.  B. "staadion", "staap" usw).

Bei Fremd- und Lehnwörtern wurden die Laute „b“, „d“ und „g“ der Ausgangssprache am Wortanfang zu „p“, „t“ und „k“: "pruukima" (ndd. bruken), "püksid" (ndd. Büx), "piljard" (Billard), "kips" (Gips).

Außer diesen Veränderungen am Wortanfang wurde früher das „f“ in ein „hv“ (gesprochen: chw) umgewandelt, z.  B. „krahv“ (Graf) und „kohv“ (ndd. "koffe"). Die hv-Kombination wird aber gegenwärtig oft [f] ausgesprochen.

Trotz der geringen Fläche Estlands von 45.227 km² weist die estnische Sprache acht Dialekte (estn. "murded") auf, die insgesamt etwa 117 Mundarten (estn. "murrak", im Estnischen gebräuchlich als Bezeichnung für eine Untergruppe eines Dialekts) vereinen. Durch Leibeigenschaft und Fronsystem waren die estnischen Bauern in ihren Kirchspielen isoliert. Ihnen war es unmöglich, sich frei im Land zu bewegen. Die Sprache entwickelte sich folglich regional isoliert und mit unterschiedlichsten Tendenzen. Die größte Konkurrenz bestand jedoch stets zwischen der nordestnischen Dialektgruppe, die sich bei der Entwicklung der heutigen Standardsprache durchsetzte, und der Gruppe der südestnischen Dialekte. Während erstere durch Tallinn als politisches Zentrum von Bedeutung war, erlangte letztere durch Tartu als erste Universitätsstadt des Landes ebenfalls schriftsprachliche Bedeutung. Die starke Ausprägung dieser Dialektgruppen lässt sich durch die einstige Teilung des heutigen Estlands in "Nordestland" und "Südestland" erklären, wobei letzteres territorial dem früheren Livland angehörte.

Die acht Hauptdialekte werden in zwei Dialektgruppen eingeteilt ("Nord-" und "Süd-Estnisch"), wobei der Dialekt der Nordostküste sowie auch der Dialekt der Inseln im Westen des Landes diesen Gruppen nicht zugeordnet werden können:

Nordestnisch 

Südestnisch (südlich von Tartu und Põltsamaa)

Küstenestnisch (östlich von Tallinn entlang der Küste bis zur Grenzstadt Narva)

Inselestnisch

Geprägt wurde die Sprachpolitik Estlands im 20. und 21. Jahrhundert durch die Geschichte, in der das Land von Dänen, Schweden, Deutschen und Russen besetzt war. Vor allem die von 1721 bis 1918 währende Zeit Estlands als Teil der Ostseeprovinz des Russischen Reiches hinterließ ihre Spuren. So kam es während dieser Zeit zu einer ausgeprägten Russifizierung, die Nationalbewusstsein und Bestrebungen nach kultureller Autonomie unterbinden sollte.

Als das Land 1918 seine Unabhängigkeit erlangt hatte, folgte in sprachpolitischer Hinsicht ein bedeutender Wandel. Nachdem der bis 1920 andauernde Unabhängigkeitskrieg durch den Frieden von Tartu beendet worden war, erlangte zunächst jeder Einwohner die Staatsbürgerschaft der Republik Estland. Während dieser Zeit erlaubten es neu verabschiedete Gesetze ethnischen Minderheiten, ihre Kultur zu bewahren und auszuleben.

Ein erneuter Wandel der Situation ging mit dem Zweiten Weltkrieg einher. Nachdem dieser beendet und Estland in die Sowjetunion eingegliedert war, sollte die Sprachpolitik erneut durch die sowjetische Besatzung bestimmt werden. So folgte eine Einführung eines nahezu selbstständigen und vom estnischen unabhängigen russischen Schulsystems und eine erneute Politik der Russifizierung. Letztere beinhaltete die Deportation mehrerer Zehntausend Esten und die Ansiedlung ethnischer Russen. So sank der Anteil der Esten an der Gesamtbevölkerung von 88 % vor Kriegsbeginn auf 61,5 % im Jahre 1989, während der Anteil der Bürger mit ostslawischen Muttersprachen im gleichen Zeitraum von 8,2 % auf 35,2 % stieg.

Nach der Wiedererlangung der Unabhängigkeit folgte 1991 eine neue Gesetzgebung, die – auch vor dem Hintergrund des Beitritts zur Europäischen Union und als Reaktion auf die Russifizierung der vergangenen Jahrzehnte – mehrfach überarbeitet wurde. Die Staatsbürgerschaft konnte nun nicht automatisch erlangt werden. Vielmehr bestand für die Angehörigen vor allem der russischsprachigen Minderheit die Möglichkeit, entweder nach Russland zurückzukehren, die estnische Staatsbürgerschaft zu beantragen oder mit einer derzeit grundsätzlich ausgestellten unbeschränkten Aufenthaltsgenehmigung als Staatenlose im Land zu verweilen. Der Erwerb der Staatsbürgerschaft setzt Sprachkenntnisse auf dem Niveau B1 des Gemeinsamen Europäischen Referenzrahmens für Sprachen voraus sowie das erfolgreiche Bestehen eines Tests zum Grundgesetz des Landes. Vor allem die Bedingung der Sprachbeherrschung wird sowohl von Angehörigen der Minderheit, die um das Russische als zweite Amtssprache bemüht sind, als auch von "Amnesty International" als diskriminierend beschrieben.
Mit dem Programm „Integration in der estnischen Gesellschaft 2000 – 2007“ (estnisch: "Riiklik programm. Integratsioon Eesti ühiskonnas 2000 – 2007.") waren Ziele wie „sprachlich-kommunikative“, „rechtlich-politische“ und „sozialwirtschaftliche Integration“ angestrebt und verfolgt worden. Das Integrationshindernis Sprache sollte beseitigt werden. Mittlerweile muss an allen Schulen des Landes Estnisch unterrichtet werden, sodass bei Beendigung der Mittelstufe das Niveau B2 erreicht ist. Ferner gibt es für Erwachsene die Möglichkeit, kostenlose Sprachkurse zu belegen und ebenfalls kostenlos ihre Sprachkenntnisse zertifizieren zu lassen.

Ethnische Minderheiten genießen weiterhin weitreichende Rechte und russischsprachige Schulen, an denen Estnisch als erste Fremdsprache gelehrt wird, werden vom Staat gefördert.

Die Bestrebungen der Regierung, möglichst viele der 2007 noch 130.000 Staatenlosen einzubürgern und die Kenntnis der estnischen Sprache zu verbreiten, führen nur langsam zu Ergebnissen. Jedoch ist festzustellen, dass 1989 etwa 67 % der Bevölkerung Estnisch beherrschten, 2008 aber bereits 82 %.




</doc>
<doc id="1314" url="https://de.wikipedia.org/wiki?curid=1314" title="Ernst Jünger">
Ernst Jünger

Ernst Jünger (* 29. März 1895 in Heidelberg; † 17. Februar 1998 in Riedlingen) war ein deutscher Schriftsteller, Offizier, Dandy und Insektenkundler. Er ist vor allem durch seine Kriegserlebnisbücher wie "In Stahlgewittern," phantastische Romane und Erzählungen und verschiedene Essays bekannt. In seinem elitären, antibürgerlichen und nationalistischen Frühwerk, das der sogenannten Konservativen Revolution zugerechnet wird, bekämpfte Jünger die Weimarer Republik entschieden. Obwohl er der NSDAP nicht beitrat und deren rassistische Ideologie ablehnte, galt er nach 1945 als intellektueller Wegbereiter des Nationalsozialismus und gehört zu den umstrittensten Autoren Deutschlands. Er erhielt verschiedene Preise und Auszeichnungen, darunter 1918 den Pour le Mérite, 1959 das Große Bundesverdienstkreuz und 1982 den Goethepreis, dessen Verleihung für einen politischen Skandal sorgte.

1895 wurde Ernst Jünger in Heidelberg als erstes von sieben Kindern des Chemikers Ernst Georg Jünger (1868–1943) und dessen späterer Frau Karoline Lampl (* 1873 in München; † 1950 in Leisnig/Sachsen) geboren. Er wurde protestantisch getauft. Zwei seiner Geschwister starben im Säuglingsalter. Jünger verbrachte seine Kindheit in Hannover, in Schwarzenberg/Erzgeb. und schließlich ab 1907 in Rehburg. Der Vater hatte im Kalibergbau beträchtliche Einkünfte erzielt.

1901 wurde Ernst Jünger am Goethegymnasium in Hannover eingeschult. 1905 bis 1907 verbrachte er auf Internaten in Hannover und Braunschweig. Ab 1907 lebte er wieder bei seiner Familie in Rehburg. Mit seinen Geschwistern besuchte er die Scharnhorst-Realschule in Wunstorf. In dieser Zeit entdeckte er neben seiner Vorliebe für Abenteuerromane auch die Liebe zur Insektenkunde.

1911 trat Jünger mit seinem Bruder Friedrich Georg dem Wunstorfer Wandervogel-Club bei. Dort fand er den Stoff für seine ersten Gedichte, die in einer Wandervogel-Zeitschrift veröffentlicht wurden. Sie brachten ihm die Anerkennung seiner Lehrer und Mitschüler ein. Er genoss von diesem Zeitpunkt an den Ruf eines Poeten und Dandys.
Im November 1913 trat Ernst Jünger als Schüler, der inzwischen ein Gymnasium in Hameln besuchte, in Verdun der Fremdenlegion bei und verpflichtete sich zu einer fünfjährigen Dienstzeit. Danach kam er in das Ausbildungslager Sidi bel Abbès in Algerien und gehörte zur 26. Instruktionskompanie. Von dort floh er mit einem Kameraden nach Marokko, wurde aber schnell aufgegriffen und zur Legion zurückgebracht. Sechs Wochen später wurde er nach einer von seinem Vater betriebenen Intervention des Auswärtigen Amtes auf Grund seines Alters wieder entlassen. Diese Episode seines Lebens wird in dem 1936 erschienenen Buch "Afrikanische Spiele" verarbeitet. Zur Strafe wurde er von seinem Vater auf ein Internat nach Hannover geschickt, wo er Banknachbar des späteren KPD-Politikers Werner Scholem war.

Am 1. August 1914, kurz nach dem Ausbruch des Ersten Weltkrieges, meldete sich Ernst Jünger beim Füsilier-Regiment „General-Feldmarschall Prinz Albrecht von Preußen“ (Hannoversches) Nr. 73 in Hannover als Kriegsfreiwilliger. Nach dem Notabitur absolvierte er die militärische Ausbildung und kam im Dezember mit einem Ersatztransport an die Champagne-Front in Frankreich. Im April 1915 wurde Jünger erstmals verwundet. Im Heimaturlaub schlug er auf Anraten seines Vaters die Offizierslaufbahn (Fahnenjunker) ein. Wieder zurück in Frankreich, wurde er am 27. November 1915 Leutnant und Zugführer und machte sich durch spektakuläre Aktionen bei Patrouillen und Stoßtrupps einen Namen. Er kämpfte über die gesamte Dauer des Krieges mit dem Ethos eines Berufssoldaten. Aber im Dezember 1915 notierte er ins Tagebuch, das er ständig mit sich führte, dass das Töten im Krieg ein „Morden“ ist, und auch, dass „[d]er Krieg in mir doch die Sehnsucht nach den Segnungen des Friedens geweckt“ hat.

Im Laufe des dritten Kriegsjahres 1916 wurde Jüngers Regiment an sämtlichen Brennpunkten der Westfront eingesetzt. Während der zweiten Somme-Schlacht wurde Jünger am Vorabend der britischen Offensive in der Ruhestellung in Combles verwundet und kam ins Lazarett. In der Folgezeit wurde sein gesamter Zug bei Guillemont aufgerieben. Im November 1916 wurde Jünger bei einem Spähtruppeinsatz zum dritten Mal verwundet und erhielt wenig später das Eiserne Kreuz erster Klasse. Im Frühjahr 1917 wurde Jünger zum Chef der 7. Kompanie ernannt. Beim Anblick grüner Wiesen im Mai 1917 fragte sich selbst ein „einst so kriegslustiger“ Jünger:

Er rettete durch einen Zufall am 29. Juli 1917 seinem Bruder Friedrich Georg Jünger auf dem Schlachtfeld von Langemark das Leben. Daraufhin folgten weitere Auszeichnungen, darunter am 4. Dezember 1917 das Ritterkreuz des Königlichen Hausorden von Hohenzollern. Im März 1918 überlebte Ernst Jünger einen Granateneinschlag, dem fast seine gesamte Kompanie zum Opfer fiel. Das Kriegsende erlebte Jünger nach einer im August 1918 vor Cambrai erlittenen Verwundung im Lazarett in Hannover. Am 22. September 1918 erhielt er den Orden Pour le Mérite, die höchste militärische Auszeichnung der Krone Preußens.

Die Gefechtspausen seines Frontalltags gegen Ende des Krieges verbrachte er vor allem damit, Werke von Nietzsche, Schopenhauer, Ariost und Kubin zu lesen. Außerdem ließ er sich aus der Heimat entomologische Zeitschriften schicken. Die 15 Kriegstagebücher wurden vor Jüngers Tod dem Deutschen Literaturarchiv Marbach übergeben. 2010 erschienen sie, herausgegeben und kommentiert von Helmuth Kiesel. Darin erscheine Ernst Jünger, so Benjamin Ziemann, weder als protofaschistische Kampfmaschine noch als Vordenker einer Amalgamierung von Mensch und Kriegstechnik, sondern als „sehr genauer Chronist“ der Gewaltpraxis im Ersten Weltkrieg. Die Notizen dienten Jünger als Rohmaterial für sein erstes Buch ("In Stahlgewittern," 1920).

Nach dem Krieg diente Jünger zunächst noch als Leutnant im "Infanterieregiment 16" der Reichswehr in Hannover. Während seiner Dienstzeit war er unter anderem mit der Ausarbeitung von Dienstvorschriften für den Infanteriekampf (Heeresdienstvorschrift 130) beim Reichswehrministerium in Berlin befasst. Im zentral gelegenen Café Kröpcke kommt er mit dem Kreis um den Verleger Paul Steegemann in Berührung, zu dem unter anderem die Dadaisten Walter Serner und Kurt Schwitters gehören. Von Thomas Mann liest er die "Betrachtungen eines Unpolitischen" (1918), später auch den "Zauberberg" (1924). Er begeistert sich besonders für den französischen Dichter Arthur Rimbaud. Vom "Trunkenen Schiff" war er 1921 so hingerissen, dass er es eines Abends zwei Kameraden vorlas. Einer der beiden war der spätere Generaloberst Werner von Fritsch. Mit Baudelaire und Rimbaud erschloss sich Jünger nicht nur die Poetik der Moderne, betont Helmuth Kiesel, sondern auch das Seinsgefühl der Obdachlosigkeit und der Selbstentfremdung.

Bald profilierte er sich als entschiedener Gegner der Republik, hielt sich aber aus den politischen Auseinandersetzungen weitgehend heraus und überarbeitete seine Kriegsaufzeichnungen, die in die Werke "In Stahlgewittern. Aus dem Tagebuch eines Stoßtruppführers" (1920), "Der Kampf als inneres Erlebnis" (1922), "Sturm" (1923), "Das Wäldchen 125" (1925) und "Feuer und Blut" (1925) einflossen. Dabei schrieb er einige kürzere Aufsätze, die Fragen der modernen Kriegsführung behandeln, im Militär-Wochenblatt. Die "Stahlgewitter" selbst wurden zunächst nicht als literarisches Werk gelesen, sondern erschienen als „eine Art von Sachbuch“ (Kiesel) in einem Militärverlag.

Nach seinem Ausscheiden aus der Reichswehr am 31. August 1923 immatrikulierte er sich in Leipzig als stud. rer. nat. Er hörte Zoologie bei dem Philosophen und Biologen Hans Driesch, dem führenden Sprecher des Neovitalismus, und Philosophie bei Felix Krüger und dessen Assistenten Ernst Hugo Fischer. Auch dürfte er Hans Freyer, der seit 1925 in Leipzig Professor war, an der Universität kennengelernt haben.
1923 trat er für kurze Zeit in das Freikorps von Gerhard Roßbach ein und war vor allem als reisender Verbindungsmann zu anderen Teilen der nationalen Bewegung aktiv. Während eines längeren Aufenthalts in München, dem Heimatort seiner Mutter, sympathisierte Jünger mit jenem Kreis von gewesenen Frontsoldaten um Erich Ludendorff und Adolf Hitler, der den November-Putsch organisierte. Eine Hitlerrede, die er dort hörte, beschrieb er rückblickend als „Elementarereignis“. Wenige Wochen vor dem gescheiterten Hitlerputsch publizierte er im "Völkischen Beobachter," dem Parteiblatt der NSDAP, seinen ersten dezidierten politischen Artikel, "Revolution und Idee," ein Plädoyer für eine „wirkliche Revolution“, deren Banner und Ausdrucksform das Hakenkreuz und die Diktatur sein sollten. In einem Münchner Vorort besuchte er Ludendorff, dem er im April 1924 eine Eloge im "Deutschen Tageblatt" widmet.
Die erste Zeitschrift, für die Jünger ab dem 6. Juni 1925 regelmäßig arbeitete, war das von ihm mitherausgegebene Blatt "Die Standarte. Beiträge zur geistigen Vertiefung des Frontgedankens," der Sonderbeilage der "Stahlhelm"-Zeitung. Hier konnte er seine politischen Schlussfolgerungen aus dem Weltkriegserlebnis publizistisch ausbreiten. Bald geriet er als Sprecher der jungen Radikalen in Gegensatz zur Stahlhelmführung und zum Legalitätskurs von Franz Seldte. Zwischen September 1925 und März 1926 publizierte er 19 Aufsätze. Bei einer Auflage von ungefähr 170.000 Exemplaren erreichten seine Ideen ein relativ breites Publikum.

Nach der Abschaffung der missliebigen Beilage im April 1926 gab Jünger zusammen mit Helmut Franke, Franz Schauwecker und Wilhelm Kleinau die "Standarte" in eigener Regie mit dem programmatischen Untertitel „Wochenschrift des neuen Nationalismus“ heraus. Ihre Auflage von vermutlich wenigen Tausend Exemplaren reichte aber nicht annähernd an "Die Standarte" heran. Nach nur fünfmonatigem Erscheinen musste die neue Standarte im August 1926 auf Anordnung des Magdeburger Oberpräsidenten Otto Hörsing eingestellt werden, weil in dem Artikel "Nationalistische Märtyrer" die Morde an Walther Rathenau und Matthias Erzberger legitimiert worden waren. Darauf kündigte der Stahlhelm auch dem Schriftleiter Helmut Franke. Jünger verließ den Verband und übernahm als Mitherausgeber die Münchner Zeitschrift Arminius, eine "Kampfschrift für deutsche Nationalisten" (so der Untertitel). Am 3. August 1925 heiratete Jünger Gretha von Jeinsen. Am 1. Mai 1926 wurde in Leipzig der Sohn Ernst geboren (in Jüngers Aufzeichnungen meist „Ernstel“ genannt). Das Studium brach er am 26. Mai ohne Abschluss ab und wandte sich ganz der Schriftstellerei zu. Im Oktober 1927 gründete Jünger mit Werner Lass die Zeitschrift "Vormarsch. Blätter der nationalistischen Jugend," die bis 1929 erschien. Von Januar 1930 bis Juli 1931 gaben sie die Zeitschrift "Die Kommenden. Überbündische Wochenschrift der deutschen Jugend" heraus. Nach 1931 schrieb er fast nur noch in Ernst Niekischs "Widerstand. Zeitschrift für nationalrevolutionäre Politik."

In seiner nationalrevolutionären Publizistik forderte Jünger aus der Verabsolutierung seiner Kriegserlebnisse heraus eine Militarisierung aller Lebensbereiche. Die Weimarer Republik bekämpfte er radikal. Er sprach sich für ihre gewaltsame Zerschlagung und die Errichtung einer nationalen Diktatur aus. Die Ideale des Humanismus, Pazifismus, ja aller bürgerlichen Ordnungs- und Zivilisiertheitsvorstellungen lehnte er ab: Stattdessen propagierte er ein Menschenbild, das keine Scheu vor Schmerz und Opfer kennt und Disziplin und Rangordnung höher achtet als das aus seiner Sicht ungerechtfertigte Postulat der Gleichheit. Nach Ansicht seines Biographen Helmuth Kiesel steckte zwar dahinter ein „früh anerzogener und durch die Lektüre Nietzsches befestigter Antidemokratismus und Antihumanismus“, aber auch der Verdacht, dass, wenn die Humanisten Recht hätten, die vier Jahre Krieg sinnlos gewesen seien. In der Erstauflage 1925 von "Wäldchen 125" findet sich der Satz:

Gegen das „geschäftsmäßige Literatenpack“, das sich für Aufklärung, Demokratie und Pazifismus einsetze, müsse „sofort die Prügelstrafe wieder eingeführt“ werden. Diese Sätze ließ er 1933 aus den folgenden Ausgaben des Buchs entfernen. Dennoch glaubt der Historiker Peter Longerich, dies sei eine „auch in der Diktion für ihn typische Aussage“. Jünger lebte 1925 im Glauben, dass „der große Krieg“ noch gar kein Ende gefunden habe. In Umkehrung des Zitats von Clausewitz sah er Politik als eine „Fortsetzung des Krieges mit veränderten Mitteln“. Diese Weltanschauung, betont Matthias Schloßberger, habe ihre Wurzeln in der Romantik und der Lebensphilosophie Nietzsches. Jünger fordert, mit „dem unheilvollen Streben nach Objektivität, die nur zur relativistischen Aufhebung der Kräfte führt, aufzuräumen“ und sich zu bewusster Einseitigkeit zu bekennen, „die auf Wertung und nicht auf ‚Verständnis‘ beruht“, und betont die Bedeutung des Gefühls der Gemeinschaft, der Verbindung mit dem Ganzen, denn das Gefühl stehe am Anfang jeder großen Tat. Wachstum sei für Jünger das natürliche Recht alles Lebendigen, das keines Beweises zu seiner Rechtfertigung bedarf: „Alles Leben unterscheidet sich und ist schon deshalb kriegerisch gegeneinander gestellt.“ Gegen das rationalistische, mechanistische, materialistische Denken des Verstandes setzt Jünger das Gefühl und den organischen Zusammenhang mit dem Ganzen. Das Gefühl der „Gemeinschaft in einem großen Schicksal“, das am Beginn des Krieges stand, das Bewusstsein der Idee der Nation und die gemeinsame „Unterwerfung unter eine Idee“ sind für Jünger Zeichen einer grundsätzlichen Revolution. Sie kann aber nur Methode sein, denn der „Frontsoldat besitzt Tradition“. Die großen Gefahren sieht Jünger „nicht im marxistischen Bollwerk“, sondern in allem, was mit dem Liberalismus zusammenhängt: „Sicher steht uns der Kommunismus als Kampfbewegung näher als die Demokratie“. Zugleich artikulierte er in dieser Phase eine extrem nationalistische Einstellung:

Das nationalistische Programm sollte auf vier Grundpfeilern basieren: Der kommende Staat müsse national, sozial, wehrhaft und autoritativ gegliedert sein. Dabei sei die Staatsform „nebensächlich, wenn nur ihre Verfassung eine scharf nationale ist“. Ob die Zuordnung Jüngers zur "Konservativen Revolution" Arthur Moeller van den Brucks gerechtfertigt ist, ist umstritten. Den Gedanken einer hierarchisch geprägten Stände-Gesellschaft lehnte er ab: „Aufgrund des Blutes und des Charakters wollen wir uns in Gemeinschaften und immer größere Gemeinschaften binden, ohne Rücksicht auf Wissen, Stand und Besitz“. Es gebe keine allgemeine Wahrheiten, jedes Gesetz werde „durch Zeit, Raum und Blut“ bestimmt. Wenn aber das „wofür“ der Entscheidung beliebig wird, dann ist diese nihilistische Position mit einer konservativen Haltung nicht mehr in Einklang zu bringen. Jünger stellte klar, dass sein Nationalismus mit dem „Konservativismus“ nicht „das mindeste zu schaffen“ habe. Seine Kritik an der parlamentarischen Demokratie trifft jeden, der sich nicht außerhalb der Ordnung des bestehenden Systems stellt.

Allgemein begrüßte Jünger die nationalsozialistische Bewegung von damals als eine der radikalsten und unbürgerlichsten. In ihr erblickte er 1925 in der "Standarte" „mehr Feuer und Blut, als die sogenannte Revolution in den ganzen Jahren aufzubringen imstande war“. Er sah in der „Gestalt des Gefreiten Hitler“ eine „Gestalt, die unzweifelhaft schon wie die Mussolinis die Vorahnung eines ganz neuen Führertypus“ erwecke. Hier deute sich bereits, Kai Köhler zufolge, eine Haltung der Überlegenheit an: Hitler sei aus Sicht dessen, der in die Zukunft blickte, eben nicht der Führer, sondern nur die Vorahnung eines kommenden Typus, dessen Merkmale der Betrachter besser zu erkennen beanspruche. Im "Stahlhelm-Jahrbuch" 1926 erklärte Jünger, im eigenen Denken gezwungen zu sein, außer dem Nationalismus noch „dem Sozialismus einen wichtigen Platz auf dem Felde unseres Denkens einzuräumen, und daß es sehr vielen so gegangen ist, das beweist die Gründung der Nationalsozialistischen Partei, die aus einem tiefen Bedürfnis heraus hervorgegangen ist.“ Weiter kommentiert er den gescheiterten Hitler-Putsch und die Neugründung der NSDAP:

Am 29. Januar 1926 sandte er Hitler sein Buch "Feuer und Blut" mit der Widmung „Dem nationalen Führer Adolf Hitler“, worauf dieser sich persönlich bei ihm bedankte. Hitler kündigte sogar einen Besuch in Leipzig an, sagte aber in letzter Minute ab. Trotz seiner Sympathie für die Idee einer „nationalen Revolution“ hielt er sich von Hitler fern. Einig mit ihm im Kampf um die Revision des Versailler Vertrages, lehnte er die Partei als Instrument ab.

Im März 1926 plädiert Jünger für die „aktive Eingliederung in das politische Kräftespiel“ und fordert die Zusammenfassung der „nationalen Frontsoldatenverbände“, der „Kräfte der radikalen, der völkischen und der nationalsozialen Gruppen“ sowie des „blutmäßige[n] Kern[s] des Frontsoldatentums der Arbeiterschaft“. Am 20. Mai 1926 kommt er wieder auf den misslungenen Hitler-Putsch zu sprechen, den er als „noch unklaren Aufstand in München“ umschreibt, bei dem allerdings der Nationalismus noch mitten im Prozess einer innerlichen Überwindung der „Formen eines alten Staates“ gesteckt habe, und spricht sich trotz der ersten Risse im Verhältnis zum "Stahlhelm" dafür aus, „unseren Einfluß in den Kampfbünden zu stärken“ und ihre „Revolutionierung“ voranzutreiben. In seinem Aufruf „Schließt euch zusammen!“ vom 3. Juni 1926 forderte er schließlich ergebnislos den Zusammenschluss der „Einzelbewegungen“ zur „nationalistischen Endfront“, denn „die Form unserer Bewegung wird auch die Form des zukünftigen Staates sein“, und bezog die NSDAP, mit deren Hilfe die Arbeiterschaft gewonnen werden sollte, ausdrücklich mit ein:

Im August 1926 sprach er in der Zeitschrift "Deutsches Volkstum" allerdings davon, dass ihm das Wort „Rasse“ in seiner neuen biologischen Anwendung „peinlich“ zu werden beginne. Im Dezember 1926 bekannte er sich offen zur bolschewistischen Wirtschaftspolitik. In seinem agonalen Weltbild, dem sogenannten Heroischen Realismus, war auch kein Platz für darwinistische Plattitüden:

Scheitert die Verwirklichung einer Idee, wie in der Novemberrevolution, so scheitert sie notwendig, weil die Zeit noch nicht reif genug war. Dieser Begriff der Zeit zeigt Jünger, dem die Arbeiten Henri Bergsons vertraut waren, in der Tradition des organologisch-lebensphilosophischen Denkens des 19. Jahrhunderts. Allerdings gab es weitgehende inhaltliche Übereinstimmungen mit der NSDAP in der Propagierung eines „nationalen Sozialismus“. Der entscheidende Unterschied zwischen „neuem Nationalismus“ und Nationalsozialismus lag nicht auf inhaltlicher Ebene, so der deutsche Historiker Daniel Morat, sondern bestand in der Organisationsform als esoterische Zirkel auf der einen und als Massenpartei auf der anderen Seite. In seinem 1927 im "Arminius" erschienenen Aufsatz „Nationalismus und Nationalsozialismus“ legte Jünger besonderen Wert auf die Bedeutung der „vorwiegend literarischen Tätigkeit“ der Vorkämpfer des "Neuen Nationalismus." Während der Nationalsozialismus „als politische Organisation auf die Gewinnung von tatsächlichen Machtmitteln angewiesen ist“, um „eine Idee zu verwirklichen“, sei es die Aufgabe des Nationalismus, „sie möglichst tief und rein zu erfassen“. Jemand, der dies tue, könne schwerer wiegen „als hundert Sitze im Parlament“.

Nach Aussage des bündischen Jugendführers Karl Otto Paetel lehnte Jünger 1927 ein von der NSDAP angebotenes Reichstagsmandat mit der Begründung ab, er halte „das Schreiben eines einzigen Verses für verdienstvoller als 60 000 Trottel zu vertreten.“ Allerdings ist dieses Angebot und seine Ablehnung durch Jünger anderweitig nicht dokumentiert. Die Antwort Jüngers würde unterstreichen, dass die Ursachen für seinen Abstand zum Nationalsozialismus weniger in der Radikalität der NSDAP lagen, sondern eher im Gegenteil: Jünger lehnte die Entscheidung Hitlers, nicht auf revolutionärem Wege, sondern im legalen Marsch durch die Institutionen an die Macht zu gelangen, als Konzession an den verhassten „Parteienstaat“ ab. Jedenfalls wandte er sich Ernst Niekisch zu, dessen „nationalbolschewistische“ Konzeption außerordentlich drastisch und strikt antibürgerlich war. Harro Segeberg bezeichnet dieses kurze Engagement als „frühen Flirt“ mit dem Nationalsozialismus jener Zeit.

Im Juli 1927 übersiedelte Jünger mit seiner Familie von Leipzig nach Berlin, um das moderne Leben in seiner „Traumstärke“ zu erfassen. Zunächst wohnte er in der Nollendorfstraße 29/3 im Ortsteil Schöneberg, in der Nähe der Motzstraße, wo der Juniklub im sogenannten Schutzbundhaus seine Zusammenkünfte abhielt. Nach einem Jahr siedelte Jünger in die Stralauer Allee (36, 1. Stock) um, in eine Arbeitergegend, unweit des Osthafens. In Berlin intensivierte sich der Austausch mit konservativen Revolutionären wie Ludwig Alwens, Franz Schauwecker, Friedrich Hielscher, Albrecht Erich Günther, Bruno und Ernst von Salomon sowie Ernst Niekisch, aber auch mit Schriftstellern der Linken wie Bertolt Brecht, Ernst Toller und Erich Mühsam. Er unterhielt Beziehungen zu Arnolt Bronnen, zu den Malern A. Paul Weber und Rudolf Schlichter, zu den Verlegern Ernst Rowohlt und Benno Ziegler, zum Philosophen Ernst Hugo Fischer, den er bereits aus Leipzig kannte, und schloss neue Freundschaften mit Valeriu Marcu, Alfred Kubin und Carl Schmitt. In der Berliner Zeit machte er sich den Lebensstil der Bohème zu eigen und bestellte endlich mit Erfolg zwei Felder zugleich: Publizistik und Essayistik, Politik und Literatur.

Bis September 1927 publizierte Jünger im "Arminius" 27 Beiträge. Dann kam es – weil Jünger nicht darüber informiert worden war, dass das Blatt von Hermann Ehrhardt finanziert wurde, dessen Legalitätskurs er nicht gutheißen mochte – zu einem Zerwürfnis, und Jünger legte seine Herausgebertätigkeit nieder, um sich für die Monatsschrift „Der Vormarsch“ zu betätigen, der ein Blatt „für die nationalistische Jugend“ sein sollte. 1928 erregte Jüngers an die Tradition des europäischen Surrealismus anknüpfendes Buch "Das abenteuerliche Herz" Aufsehen, zumal es als „Literarisierung“ des Autors und Abwendung von der Politik interpretiert wurde. In der damaligen Jüngerschen Publizistik fand der Nationalsozialismus keinerlei Erwähnung. Zum Ende der 1920er Jahre trat Jünger zunehmend in den Dialog mit politischen Gegnern der Rechten und zog sich gleichzeitig aus der politischen Publizistik zurück. An der tagespolitischen Entwicklung wollte er nicht mehr teilhaben.

In dieser Zeit kam es auch zum offenen Bruch mit Hitler, als dieser sich gegen die Landvolkbewegung gewandt hatte; Jünger hatte in ihr den Vorreiter der von ihm erhofften nationalrevolutionären Bewegung gesehen. Er vermutete, dass Hitler seinen Frieden mit dem Parlamentarismus gemacht habe und fühlte sich in seinem Verdacht bestärkt, dass der Nationalismus durch ein Bündnis mit den Bürgerlichen korrumpiert werde. Laut Wojciech Kunicki sei die „Landvolkbewegung“ um Bruno von Salomon die einzige national-revolutionäre Gruppe gewesen, die von Jünger Ende der Zwanzigerjahre noch unterstützt wurde, und zwar aufgrund ihres offenen, kompromisslosen Anarchismus, wie er Salomon am 10. September 1929 schrieb:

1929 antwortete der von Joseph Goebbels herausgegebene "Angriff" als Reaktion auf einen Artikel Jüngers im linksliberalen "Tagebuch," in dem Jünger erklärt hätte, dass der Antisemitismus für den „neuen Nationalismus“ „keine Fragestellung wesentlicher Art“ sei und dass sich der Nationalsozialismus durch seinen Legalitätskurs als Teil der bürgerlichen Ordnung erwiesen habe: „Wir debattieren nicht mit Renegaten, die uns in Schmutzblättern jüdischer Landesverräter anpöbeln. Herr Jünger aber ist damit für uns erledigt.“ Dennoch versuchte 1930 der expressionistische Dramatiker Arnolt Bronnen, Jünger mit Goebbels zu versöhnen.

Am 17. Oktober 1930 befand sich Jünger im Beethovensaal gemeinsam mit seinem Bruder Friedrich Georg zwischen den SA-Leuten, die unter der Führung Bronnens die "Deutsche Ansprache" von Thomas Mann störten, in der dieser vor den Gefahren des aufkommenden Nationalsozialismus warnte. Man hatte beschlossen, so Bronnen in seinem Protokoll, die Veranstaltung zu besuchen, um „dort selbst eine Diskussion zu entfachen“. Unabhängig davon – so Bronnen – hatte Goebbels zwanzig mit Smoking bekleidete SA-Männer geschickt, die den Auftrag hatten, sich „nur geistig“ zu betätigen, johlend, rufend, pfeifend, aber ausnahmsweise einmal nicht als Schläger. Die Störungen hätten sich zunächst auf einige Zwischenrufe beschränkt, bis die Polizei einen allgemeinen Tumult auslöste. Jünger selbst beteiligte sich nicht direkt an den Störungen, billigte sie allerdings stillschweigend, wie sich der ebenfalls anwesende Alexander Mitscherlich in seiner Autobiographie "Ein Leben für die Psychoanalyse" erinnerte.
Der französische Jüngerianer Julien Hervier meint hingegen, Jünger habe lediglich die von ihm bevorzugte Beobachtungsrolle übernommen. Dirk Heisserer und die Enkelin Barbara Bronnen weisen auch auf inkorrekte Darstellungen hin. Jünger selbst habe es seinem Privatsekretär Heinz Ludwig Arnold nach dem Krieg persönlich so erzählt: „[…] sie besetzen die erste Reihe, und als Thomas Mann seine Lesung begann, schlugen alle in der ersten Reihe große Zeitungen auf“, was Arnold als „am harmlosesten“ schildert. Jünger habe damals eine Goebbels-Lesung nach zehn Minuten demonstrativ verlassen. Er deutete auch intime Beziehungen zu der Schauspielerin Lída Baarová an, die auch mit Goebbels liiert war. Daher, glaubte Jünger, rührte Goebbels Hass auf ihn.

In seinem Tagebuch fragte sich Thomas Mann später, wie Jünger es in dieser Gesellschaft „aushalten“ konnte, eine Frage, die nur verständlich ist, wenn er bei ihm ein gewisses Niveau voraussetzte, das zu dem Umfeld der Störer nicht passte. Anders als sein Bruder Friedrich Georg, der sich süffisant über den "Zauberberg" und Manns Angriffe auf den deutschen Nationalismus geäußert hatte, hielt sich Ernst Jünger erstaunlich zurück.
Erst später finden sich vereinzelte Äußerungen. So sagte er anlässlich der bevorstehenden Verleihung des Goethepreises in einem Spiegel-Gespräch, er habe sich stets geärgert, wenn „eine Stadt in Flammen aufging“ und Mann „seine Reden“ hielt, bewundere ihn allerdings als großen Stilisten. Mann sei „einer der wenigen, der Verantwortung für die deutsche Sprache“ zeige.

Nach seinem partiellen Rückzug aus dem politischen Tagesgeschäft des neuen Nationalismus versuchte Ernst Jünger in seinen „phänomenologisch-militanten Schriften“ der frühen 1930er Jahre, sein weltanschauliches Programm geschichtsphilosophisch zu erweitern. Erst in dieser Zeit wurde seine Kriegsliteratur außerhalb nationalistischer und militärischer Kreise populär. 1930 fungierte er als Herausgeber mehrerer nationalrevolutionärer Sammelbände. Um ihn herum bildete sich ein Zirkel nationalistischer Publizisten aus sehr unterschiedlichen Flügeln, angefangen von späteren Nationalsozialisten bis hin zum Nationalbolschewisten Ernst Niekisch. Es sei der mitreißende Schwung und der glühende Idealismus, so Heimo Schwilk, die seine Zeitschriftenbeiträge und Aufrufe gerade in den prosperierenden Jahren der Weimarer Republik für die Jugend so verführerisch machen, dass ihm auch der politische Gegner die Anerkennung nicht versagen könne. Das Charakteristische der damaligen Jünger-Rezeption durch einen Teil der intellektuellen Linken enthält eine Rede Klaus Manns 1930, der von „einer gewissen missleiteten Reinheit“ Jüngers spricht: „Mit dem öffentlichen Unfug der Rechtsradikalen hat er nichts gemein. […] Seinen Gaben nach gehört er zu uns! Den Arnolt Bronnen gönnen wir gern denen da drüben. Aber ein Geist von der finsteren Glut Jüngers kann Unheil stiften.“ Im selben Jahr erschien die Abhandlung "Die totale Mobilmachung," die für Jüngers Versuch stand, sich nach dem politischen Scheitern des neuen Nationalismus aus dem engen Gedankenkorsett des Nationalismus herauszuarbeiten.

In seinen politischen Schriften dieser Zeit finden sich vereinzelt auch judenfeindliche Äußerungen, so z. B. 1930 in seinem Essay "Nationalismus und Judenfrage:"

Dort heißt es auch:

Diese Äußerungen seien im Zusammenhang mit seinem radikalen „Anti-Liberalismus und Anti-Demokratismus“ (Harro Segeberg) zu sehen und richteten sich daher in erster Linie gegen die Assimilation der deutschen Juden, die er als „Zivilisationsjuden“ abqualifiziert; Jünger bevorzugte wie damals auch sein Bruder Friedrich Georg und andere Nationalrevolutionäre das orthodoxe Judentum bzw. später den modernen Zionismus: Franz Schauwecker und Friedrich Hielscher etwa sprachen sich hierbei besonders für Martin Bubers spirituellen Zionismus aus.

1931 zog Jünger in die Berliner Dortmunder Straße, nähe Bellevue, 1932 nach Berlin-Steglitz. 1932 trat Jüngers Vater, wahrscheinlich unter Druck, der NSDAP bei. Im November, auf dem Höhepunkt der politischen und gesellschaftlichen Krise der Weimarer Republik, erschien Jüngers umfangreiches Essay "Der Arbeiter. Herrschaft und Gestalt," in dem er eine Utopie eines zukunftweisenden, kollektiv geordneten Machtstaates, vorlegte und im Rückgriff auf die mythische Figur des Juden Ahasver eine moderne metaphysische Wahrnehmungstheorie etablierte, mit der sich der Text unter der Hand von seinen imperialen und nationalistischen Phantasmen löst.

Nach der Machtübernahme der NSDAP 1933 habe diese (nach der unbelegten Angabe von Paetel) erneut vergeblich versucht, Ernst Jünger für sich zu gewinnen. 1933 kam es zu einer Hausdurchsuchung durch die Geheime Staatspolizei (Gestapo) wegen seiner Kontakte zu Kommunisten und zu Ernst Niekisch. Jedoch im "Nachrichtenblatt für die Ritter des Ordens „Pour le Mérite“" vom September 1933 bezeichnete er 1933 als „Jahr der Wiederbesinnung des deutschen Volkes auf seine großen Aufgaben“ und sprach sogar davon, dass der „neue Staat […] die Mitarbeit jeder wertvollen Kraft“ fordere. Diese Mitarbeit hat er ihm aber weitgehend vorenthalten.

In Franz Lennartz’ "Dichter unserer Zeit" aus dem Jahr 1938 wird Jünger als Mitglied der Deutschen Akademie der Dichtung geführt („schon ab 1933“). Nach Darstellung des Jünger-Biographen Heimo Schwilk wies Jünger die Aufnahme in die – nationalsozialistisch „gesäuberte“ – Akademie der Dichtung zurück, wobei die Veröffentlichung seines Absageschreibens in der Presse verhindert wurde. In einem taktischen Brief an Werner Beumelburg vom 20. November 1933 schrieb Jünger erneut, „zur positiven Mitarbeit am neuen Staate, ungeachtet mancher persönlichen Verärgerung, wie etwa der Haussuchung, die in meinen Räumen stattgefunden hat, durchaus entschlossen“ zu sein. Seine Wohnung wurde erneut von der Gestapo durchsucht, woraufhin Jünger Berlin verließ und sich nach Goslar zurückzog. Dort wurde 1934 sein Sohn Alexander geboren.

Im Juni 1934 protestierte er gegen den von ihm nicht genehmigten Abdruck eines Auszugs aus dem „Abenteuerlichen Herz“ in einer Beilage des "Völkischen Beobachters," da er nicht als ein Mitarbeiter des Parteiorgans erscheinen wollte. Die letzte illegale Zusammenkunft des oppositionellen Kreises um Ernst Niekisch fand in Jüngers Wohnung in Goslar statt, bevor die Familie 1936 nach Überlingen am Bodensee umzieht. Er verbrachte die folgenden Jahre mit mehreren ausgedehnten Auslandsreisen. Während eines Aufenthalts in Paris 1937 lernte er Joseph Breitenbach, Julien Green, Jean Schlumberger und Annette Kolb kennen. Nach Niekischs Verhaftung 1937 unterstützte Jünger dessen Familie.

Ab 1939 lebte er in Kirchhorst nahe Hannover. Im selben Jahr erschien seine Erzählung "Auf den Marmorklippen," die oft als verdeckte Kritik an der Gewaltherrschaft Hitlers interpretiert wird. Jünger selbst wehrte sich jedoch zeitlebens gegen die Interpretation der "Marmorklippen" als Widerstandsbuch gegen den Nationalsozialismus. Er sah in "Auf den Marmorklippen" vielmehr ein Werk, das man auf mehrere Staatssysteme beziehen kann, so zum Beispiel auch auf das stalinistische System in der Sowjetunion, und wollte es daher nicht explizit auf den nationalsozialistischen Staat in Deutschland festschreiben und somit die Interpretationsfreiheit einschränken. Die „Marmorklippen“ liest Helmuth Kiesel als respektables Zeugnis der Distanzierung, das der Idee eines Attentats allerdings eine „klare Absage“ erteilte. Den Umzug nach Kirchhorst bei Hannover deutet er als kluge Strategie, „für den Fall der Mobilmachung im Einzugsbereich seiner alten Einheit“ zu sein.

Kurz vor Beginn des Zweiten Weltkriegs meldete sich Jünger erneut zum Fronteinsatz. Er wurde im August 1939 zum Hauptmann befördert und zur Wehrmacht eingezogen; zunächst tat er als Kompaniechef am Westwall gegenüber der Maginot-Linie Dienst. 1940 fragte er in Erinnerung an seine Kriegslust aus dem Ersten Weltkrieg seinen vorgesetzten General: „Darf man hoffen, daß man noch ins Feuer kommt?“ In dieser Zeit erhielt er für die Bergung eines Verwundeten die Spange zum Eisernen Kreuz II. Klasse. 1941 wurde seine Einheit nach Paris verlegt. Ernst Jünger kam im Sommer des Jahres gegen den Widerstand von Wilhelm Keitel in den Stab des Militärbefehlshabers von Frankreich (MBF) unter Otto von Stülpnagel, später Chef des Generalstabes der Heeresgruppe B, wo er unter anderem für die Briefzensur in der Ic-Abteilung zuständig war. Der Stab befand sich zu der Zeit im Pariser Hôtel Majestic in der Avenue Kléber am Arc de Triomphe. Dort hatte Jünger bis zum Sommer 1944 sein Büro und wohnte daneben im Luxushotel „Raphael“. Als wichtiges Zeitdokument einer deutschen, nicht-nationalsozialistischen Sicht des Zweiten Weltkrieges entstanden die "Pariser Tagebücher," die 1949 in das Buch "Strahlungen" Eingang fanden, nachdem das Tagebuch vom Frankreichfeldzug schon 1942 unter dem Titel "Gärten und Straßen" erschienen war.

Man hat Jüngers Pariser Existenzform oft als dandyhaft bezeichnet. Die ausführlichen Schilderungen seiner Lektüre und seiner Streifzüge durch die Pariser Antiquariate, seiner Teestunden bei der Damenwelt und seiner Abende in den Salons der Kollaborationskultur sind nach dem Krieg, trotz der Selbstaufforderung, „in keinem Augenblick [zu] vergessen, daß ich von Unglücklichen, von bis in das Tiefste Leidenden umgeben bin“, vielfach kritisiert worden. Man lebte gut, mit Champagner und Austern. Er befand sich in unmittelbarer Nähe der Macht, wie Jörg Magenau betont, aber er tat so, als gehöre er selbst gar nicht dazu. Am 29. Mai 1941 wohnte er der Hinrichtung eines deutschen Deserteurs bei. Daniel Morat verglich diese Passage der "Strahlungen" mit Jüngers Originaltagebüchern und urteilte, dass er seine eigene Funktion als leitender Offizier dieser Erschießung in der Veröffentlichung weitgehend ausblendete und sich zum rein von „höherer Neugier“ angetriebenen Beobachter stilisierte. Jünger wurde auch gelegentlich als sexueller Freibeuter geschildert. Mit einer hübschen Kontoristin aus einem Warenhaus besuchte er ein Kino und berührte dort ihre Brust, während vorne auf der Leinwand deutsche Panzer durch den Sand Nordafrikas rollten. Eine „südliche Modistin“ besuchte er in ihrer Dachkammer, die ihn an den Schnürboden eines Theaters erinnerte und deren Schlafzimmer durch das Bett ganz ausgefüllt war. Er hatte aber, wie Helmuth Kiesel betont, vor allem eine enge Beziehung zur „Halbjüdin“ Sophie Ravoux. Kiesel verteidigt Jünger auch gegen den Vorwurf des Ästhetizismus und Amoralismus. Insbesondere unter den Geiselerschießungen habe er gelitten. Er übersetzte Abschiedsbriefe anlässlich des Attentates von Nantes im Oktober 1941 zum Tode verurteilter Geiseln.

Er habe in Paris viele französische Schriftsteller getroffen, wie Jean Cocteau, Henry de Montherlant, Jean Paulhan und Louis-Ferdinand Céline, auf den er neugierig gewesen sei. Am Nachmittag des 7. Dezember 1941 im Deutschen Institut traf er auf Céline, der ihn mit wüsten antisemitischen Reden in Erstaunen versetzte. Jünger tat in seinem Tagebuch seine Abscheu vor „solchen Menschen“ kund. Am 7. Juni 1942 begegnete er zum ersten Mal in der Rue Royale dem gelben Stern, getragen von drei jungen Mädchen, und notierte: „So genierte es mich sogleich, daß ich in Uniform war.“ Der französische Arzt Germain Sée berichtete, er sei im Juni 1942 in der Avenue Kléber von einem deutschen Offizier, als er den Stern trug, militärisch gegrüßt worden. Dies bestätigte Jünger nach dem Krieg und schrieb Sée, er habe „immer den Stern gegrüßt“.

1942 begannen die Arbeiten an dem Aufruf "Der Friede," der als Appell an die Jugend Europas nach einem Sturz Hitlers gedacht war. Damals gehörte Jünger zur Stabsabteilung des Militärbefehlshabers in Frankreich, des Generals der Infanterie und späteren Widerstandskämpfers Carl-Heinrich von Stülpnagel. Stülpnagel schickte Jünger am 21. November 1942 in den Kaukasus – angeblich, um die Truppenmoral vor einem eventuellen Attentat auf Adolf Hitler zu untersuchen. Dort setzte Jünger sein Tagebuchwerk unter dem Titel "Kaukasische Aufzeichnungen" fort, die ebenfalls in die "Strahlungen" aufgenommen wurden. Am Silvesterabend übertrug sich der Ekel vor den Mordaktionen, die dort von „Einsatzgruppen“ begangen wurden, auch auf das Militärische selbst: „Ein Ekel ergreift mich dann vor den Uniformen, den Schulterstücken, den Orden, dem Wein, den Waffen, deren Glanz ich so geliebt.“ Der Kaukasus wurde zum Desaster des Wahrnehmungsprogramms, das er unter anderem im „Arbeiter“ entwickelt hatte. Am 9. Januar 1943 kehrte er nach Paris zurück.

Am 26. Mai 1944 bezeichnete Jünger seinen „Ekel“ selbst als „Schwäche“ und ermahnte sich, „dergleichen“ zu betrachten „wie der Arzt den Kranken“, also sein im Kaukasus zusammengebrochenes Wahrnehmungsprogramm wieder aufzunehmen. So hält er im Tagebucheintrag vom 27. Mai 1944 fest, wie er einen alliierten Fliegerangriff vom Dach des Hotels Raphael aus beobachtete, und zwar teils durch „ein Glas Burgunder, in dem Erdbeeren schwammen“. Im Spiegel-Interview 1982 sprach er allerdings von einem mit Sekt gefüllten Sektglas. Tobias Wimbauer vertritt die These, dass es an diesem Tag bei Sonnenuntergang wahrscheinlich gar keine Bombardierungen gäbe. Jünger müsse die Szene also erfunden (oder falsch erinnert), auf literarische Vorbilder der Décadence bei Oscar Wilde und Marcel Proust zurückgegriffen und seine durch eine Pariser Liebschaft ausgelöste Ehekrise auf allegorische Weise thematisiert haben. Die Faszination angesichts des Kriegsschauspiels sei von Jünger bewusst in eine literarische Genussästhetik überführt worden. Daniel Morat kritisiert die These Wimbauers als „Versuch, durch den Hinweis auf die Literarizität dieser Szene, sie und damit Jünger vom Vorwurf des Gewaltästhetizismus freizusprechen“.

Jünger kam in Kontakt zu Widerstandskreisen innerhalb der Wehrmacht und erlebte die Auseinandersetzungen zwischen NSDAP-Stellen und der Wehrmacht im besetzten Frankreich mit. Er kannte zahlreiche Beteiligte des Attentats vom 20. Juli 1944 und hat möglicherweise etwas von den Vorbereitungen der Verschwörer geahnt. Nach dem Scheitern des Unternehmens Walküre notierte Jünger in seinem "Zweiten Pariser Tagebuch" kommentarlos eine in einem Gespräch mit ihm geäußerte Aussage von Max Hattingen, Hauptmann im Pariser Generalstab, der das Geschehen mit den Worten zusammenfasste: „Die Riesenschlange im Sack gehabt und wieder herausgelassen“. Hattingen bezeichnete damit den Tatbestand, dass es Stülpnagel zunächst gelungen war, in Paris die wichtigsten Funktionäre und Führer der SS, des SD und der Gestapo festnehmen zu lassen, um sie dann wieder in Freiheit zu setzen, nachdem das Scheitern des Attentats feststand.

Nach der Operation Overlord und der alliierten Befreiung von Paris verließ Jünger mit den abziehenden deutschen Truppen die französische Hauptstadt und kehrte nach Deutschland zurück, wo er im September 1944 – im Alter von 49 Jahren – als Hauptmann regulär aus der Wehrmacht entlassen wurde. Er zog sich nach Kirchhorst in Niedersachsen zurück, wo er gegen Kriegsende als Volkssturmkommandant befahl, keinen Widerstand gegen die anrückenden alliierten Truppen zu leisten.

Jüngers Sohn Ernst, Ernstel genannt, wurde 1944 im Alter von 17 Jahren zusammen mit seinem Freund Wolf Jobst Siedler in dem Internat Hermann Lietz-Schule Spiekeroog verhaftet, in dem sie zur Schule gingen. Die Schüler waren dort auch als Marinehelfer tätig. Ein Mitschüler hatte sie bei einer vorgesetzten Dienststelle mit der Meldung denunziert, sie hätten während des Dienstes für die Marine „fortwährend regimekritische und defätistische“ Bemerkungen gemacht. Ernstel habe sogar u. a. gesagt, „Hitler müsse ‚gehängt‘ werden“. Das waren schwere Vergehen in der Zeit des Nationalsozialismus und es bestand die Gefahr, dass es ein Strafverfahren vor dem Volksgerichtshof geben würde, bei dem solche Äußerungen in der Regel mit der Verhängung der Todesstrafe geahndet wurden. Doch dank der Fürsprache von Ernstels Vater bei militärischen Vorgesetzten der beiden Jungen fand ein Kriegsgerichtsverfahren statt, in dem beide nur zu Gefängnisstrafen verurteilt und ein halbes Jahr später auf Bewährung entlassen wurden. Ernstel meldete sich freiwillig bei den Panzergrenadieren einer SS-Einheit, um einer Verhaftung durch die Gestapo zu entgehen. Am 29. November 1944 fiel er in Italien in der Nähe von Carrara. Ernst Jünger und seine Frau hatten noch lange immer wieder Zweifel, ob ihr Sohn nicht in Wirklichkeit „liquidiert“ worden war.

Nach dem Krieg weigerte sich Jünger, den Fragebogen der Alliierten für die sogenannte Entnazifizierung auszufüllen, und erhielt daraufhin in der britischen Besatzungszone bis 1949 Publikationsverbot. Dass er als „belastet“ galt und als Wegbereiter des Nationalsozialismus gesehen wurde, nahm er hin und wollte auch darin eine Auszeichnung sehen. Dabei wartete er sehnlichst darauf, dass die „Friedensschrift“ in Deutschland erscheinen könnte, wo ab 1946 eine in Amsterdam gedruckte Ausgabe zirkulierte. Er übersiedelte 1948 nach Ravensburg in die französische Besatzungszone und wenig später auf persönliche Einladung von Friedrich von Stauffenberg nach Wilflingen, einem Ortsteil der Gemeinde Langenenslingen im Landkreis Biberach in Oberschwaben. Dort wohnte Jünger von 1951 an bis zu seinem Tode in dem 1727 vom Fürstbischof von Konstanz und Augsburg Johann Franz Schenk Freiherr von Stauffenberg erbauten Forsthaus der ehemaligen Oberförsterei der Schenken von Stauffenberg.

Jünger wurde auf den jungen Journalisten Armin Mohler aufmerksam, da dieser 1946 in der "Weltwoche" einen recht positiven Artikel über ihn veröffentlicht hatte. Von 1949 bis 1953 war Mohler Privatsekretär von Jünger. 1949 lernte Jünger den LSD-Entdecker Albert Hofmann kennen. Gemeinsam experimentierten beide mit der Droge. Jünger schrieb anschließend ein Buch über seine Erfahrungen mit LSD (Besuch auf Godenholm).

Nach der Aufhebung des Publikationsverbots 1949 erschienen die "Strahlungen," die in Deutschland Bestseller des Jahres wurden. In wenigen Wochen waren 20.000 Exemplare des Tagebuchs verkauft. Als zweites Werk erschien im Herbst 1949 der Roman "Heliopolis," an dem Jünger von Januar 1947 bis März 1949 gearbeitet hat. "Heliopolis" würdigt Helmuth Kiesel als unzeitgemäß „großartigen Roman“. Er wertet das Spätwerk aber nicht, wie etwa Peter Koslowski, als große Geschichtsphilosophie gegenüber dem Frühwerk auf.

1951 entstand Jüngers Essay "Der Waldgang," eine Art Widerstandsfibel gegen Totalitarismus und Anpassung. Fortsetzung und Abschluss dieser Thematik sind in dem 1977 erschienenen Roman "Eumeswil" zu sehen, in dem Jünger „das Gebäude seiner Weltweisheit“ (Armin Mohler) errichtete. Er entwickelt darin die Gestalt des Waldgängers zu der des Anarchen weiter, wobei er sich hauptsächlich auf Max Stirner und dessen 1844 erschienenes Buch "Der Einzige und sein Eigentum" bezieht.

Nach dem Tod seiner ersten Frau Gretha (1960) heiratete Jünger 1962 die promovierte Germanistin Liselotte Lohrer (1917–2010), die unter anderem das Cotta-Archiv im Deutschen Literaturarchiv aufgebaut und betreut hat. In seinen Schriften bezeichnet Jünger sie gewöhnlich mit ihrem Kosenamen als „das Stierlein“. Sie war auch an der Edition der Werke ihres Mannes bei Klett-Cotta beteiligt. Am 20. Juli 1977 starb Ernst Jüngers Bruder Friedrich Georg.

Am 17. Mai 1982 entschied das Kuratorium des Frankfurter Goethepreises, Ernst Jünger auszuzeichnen. Die Idee kam ursprünglich von Rudolf Hirsch, einem jüdischen Schriftsteller, der 1933 aus Deutschland emigrierte und in Jünger einen inneren Widerständler des NS-Regimes sah. Die Grünen protestierten dagegen im Stadtparlament: „Uns ist es relativ gleichgültig, ob Ernst Jünger ein guter oder schlechter Schriftsteller ist. […] Er war und ist ein durch und durch unmoralischer Mensch.“ Zur Verleihung am 28. August 1982 in der Frankfurter Paulskirche, dem Symbolort deutscher Demokratie, zeigte sich die Polizei gut gerüstet. Draußen hatten die Grünen eine große Demonstration organisiert. In der Paulskirche selbst fehlte fast die gesamte politische Prominenz. Jünger und Kulturdezernent Hilmar Hoffmann (SPD) mussten sich durch ein Spalier protestierender Gegner schreiten, die mit Eiern und Tomaten warfen, obwohl sich Hoffmann gegen den Preis ausgesprochen hatte. Auf Spruchbändern und Flugblättern wurden Jünger Sätze aus Frühwerken entgegengehalten, wie etwa: „Ich hasse die Demokratie wie die Pest.“
Ernst Jünger reiste und schrieb bis kurz vor seinem Tod. 1983 in Paris besuchte er den Philosophen Jean Beaufret. Einige Reisen zwischen 1929 und 1964 wurden in Jüngers elf Reisetagebüchern literarisch festgehalten. Die Kriminalgeschichte "Eine gefährliche Begegnung" erschien 1985. 1986 reiste er nach Kuala Lumpur, um zum zweiten Mal in seinem Leben den Halleyschen Kometen zu sehen. Darüber berichtet er im Tagebuch "Zwei Mal Halley," das zugleich einen Teil seines diaristischen Hauptwerks "Siebzig verweht" bildet. Jünger begann dieses Alterstagebuch nach seinem 70. Geburtstag (1965) und führte es bis zum Frühjahr 1996 fort.
Am 20. Juli 1993 besuchten der damalige französische Staatspräsident François Mitterrand und der damalige deutsche Bundeskanzler Helmut Kohl Jünger im Stauffenbergschen Forsthaus in Wilflingen. Am 26. September 1996 konvertierte Ernst Jünger zum römisch-katholischen Glauben. Erst nachdem er 1998 im Alter von 102 Jahren im Krankenhaus von Riedlingen gestorben war, wurde seine Konversion bekannt. An der Beerdigung Ernst Jüngers nahmen 2.000 Menschen teil, darunter Erwin Teufel, Ministerpräsident von Baden-Württemberg, ein Vertreter der Bundesregierung in Bonn und fünf Generäle der Bundeswehr.

In Gedenken an Ernst Jünger fertigte der Aachener Bildhauer Wolf Ritz eine Büste an, die anfangs in Wilflingen aufgestellt wurde, aber mittlerweile vom Deutschen Literaturarchiv Marbach übernommen worden ist.


Aus Anlass des 90. Geburtstages Ernst Jüngers stiftete das Land Baden-Württemberg mit dem Einverständnis des Schriftstellers 1985 den "Ernst-Jünger-Preis für Entomologie." Damit werden seit 1986 in dreijährigem Turnus Wissenschaftler ausgezeichnet, die mit herausragenden Arbeiten auf dem Gebiet der Insektenkunde hervorgetreten sind. Ernst Jünger war als Träger des Pour le Mérite der letzte Bezieher eines Ehrensolds gem. § 11 des Gesetzes über Titel, Orden und Ehrenzeichen aus dem Jahr 1957.

Ernst Jünger ist „der umstrittenste deutsche Schriftsteller des 20. Jahrhunderts“ (Sven-Olaf Berggötz), wenn nicht Deutschlands umstrittenster Autor überhaupt. Während seine Kritiker ihm vorwerfen, mit seinem nationalistischen Frühwerk als intellektueller Wegbereiter des Nationalsozialismus gewirkt zu haben, würdigte man seine Schriften zunächst hauptsächlich wegen ihrer ästhetischen Qualität.

Ernst Jünger galt seinen Zeitgenossen als zwar „radikalisierter“, aber ansonsten nationalsozialistischer oder „dem Nationalsozialismus gleichgestimmter“ Schriftsteller. So rechnete ihn Ernst Günther Gründel 1932 zum „orthodoxen Flügel der NSDAP“ und hielt ihn gemeinsam mit Niekisch, Schauwecker, Albrecht Erich Günther und Gregor Strasser für einen „Nationalkommunisten“. Jüngers Verhältnis zum Nationalsozialismus erschien widersprüchlich. Ernst Niekisch, der im offenen Gegensatz zum NS-Regime stand, warf ihm vor, als es um die Frage der Zugehörigkeit zur „bolschewistischen oder faschistischen Front“ gegangen sei, in die „Innerlichkeit“ ausgewichen zu sein. Arnolt Bronnen, der in dieser Frage auf der faschistischen Seite stand und gegen die Opposition des Jünger-Kreises gegen das Regime war, gab zu bedenken, der „Katechismus“ dieses Kreises habe „in einer an Nietzsche und George geschulten Form bereits die ganze Ideologie des Nationalsozialismus von Hitler bis Goebbels und Rosenberg“ enthalten. Seine Verehrung für Jünger sei der Grund für seine Hinwendung zur kommenden Nazi-Bewegung. Für die Nationalsozialisten war allerdings „Der Arbeiter“ ein skandalöses Buch schon allein deshalb, weil Jünger darin „die Grundfrage alles Daseins […], das Problem von Blut und Boden“ negierte, wie Thilo von Trotha im "Völkischen Beobachter" vom 22. Oktober 1932 kritisierte. Ernst Jünger nähere sich mit seinem literarischen Schaffen damit der „Zone der Kopfschüsse“, hieß es drohend. Andererseits wurde der „Arbeiter“ von einem Kommunistenführer wie Karl Radek gefeiert. Er versuchte seine KPD-Freunde sogar zu überzeugen, dass die Gewinnung des Ernst Jünger mehr wert sei als alle neuen Wählerstimmen zusammen.

Nach 1933 sahen Siegfried Marck, Hermann Rauschning, Golo Mann und Karl Löwith in Jünger einen Wegbereiter der deutschen Katastrophe. Im "Volksbrockhaus" 1941 ist zu lesen: „[Ernst Jünger] stellte in seinen Werken, bes. seinen Kriegsbüchern […], den nationalistischen und heldischen Gedanken dem bürgerlichen Geist entgegen“. Karl Otto Paetel, der von Jüngers regimekritischer Haltung überzeugen wollte, schrieb 1943 in der Emigrantenzeitschrift "Deutsche Blätter", „dass sich Ernst Jünger um die Tagespolitik wirklich nie gekümmert“ habe. Carl Zuckmayers positive Beurteilung Jüngers in seinem 1943/44 entstandenen Geheimreport für den amerikanischen Geheimdienst "Office of Strategic Services" (OSS) stützte sich, wie er selbst bekennt, wesentlich auf den Aufsatz Paetels.
Die Rezeption Jüngers nach dem Zweiten Weltkrieg ist durch die Tatsache geprägt, dass er für viele – auch dann, wenn ihm eine moralisch einwandfreie Haltung zugestanden wird – als geistiger oder „apolitisch-politischer“ (Volker C. Dörr) Wegbereiter des NS-Faschismus und so als desavouiert gilt. Der Kulturbund zur demokratischen Erneuerung Deutschlands, der sich in der Sowjetischen Besatzungszone gegründet hatte, griff am ersten Jahrestag des Kriegsendes in einer Diskussionsveranstaltung den „Fall Jünger“ auf. Der junge SED-Literaturkritiker Wolfgang Harich ging wenig später zum publizistischen Frontalangriff über, woraufhin in den Westzonen zahlreiche Apologeten und Kritiker öffentlich Stellung nahmen. Die heftigen Attacken aus den Reihen des Kulturbundes zielten darauf ab, Jünger als Befürworter eines sich abzeichnenden westdeutschen Staats hinzustellen. Wie nach ihm Paul Rilla, Wolfgang Weyrauch und Kurt Hiller warf Wolfgang Harich im Ost-Berliner "Aufbau," im West-Berliner "Kurier" sowie in der "Täglichen Rundschau" Jünger mit Bezug auf das "Terre-des-Hommes"-Interview und die „Friedensschrift“ vor, den „westlichen Alliierten ein unverblümtes Westblock-Angebot gemacht“ zu haben. Harich forderte daraufhin im Juli 1946, „daß diesem Mann das Handwerk gelegt“ werde, trete doch in dessen Werk „die zum Himmel stinkende innere Verfaulung eines ins Bestialische entarteten Intellekts“ zutage. Und weiter: „Das Ausglühen dieses schwärenden Ansteckungsherdes aus dem deutschen Geistesleben der Gegenwart dürfte […] als hygienische Notwendigkeit nicht mehr bestreitbar sein.“ Paul Rilla bezeichnete Jünger als einen „Gegner des Nationalsozialismus, dessen Schrittmacher er war“, und folgerte daraus, Jünger böte als Identifikationsfigur die Möglichkeit, „sich vom Hitlerismus rückwärts zu distanzieren“, sodass er auch gegenwärtig wieder „zum Schrittmacher jener getarnten Reaktion“ tauge, „die ihre Hitler-Gegnerschaft beteuert, um desto ungenierter im Trüben zu fischen“.

Bereits Anfang Mai 1946 hatte sich Karl Korn bei einer öffentlichen (und von Niekisch moderierten) Diskussion vehement für Jünger engagiert. Auf die polemischen Vorwürfe von Wolfgang Harich reagierte er beispielsweise mit einer lebhaften Apologie des Autors. In Jünger könne man eine exemplarische „Gestalt des Übergangs“ erblicken, dessen „authentischen Rufen“ nach dem „verlorenen Heil“ eine sittliche Selbsterneuerung Deutschlands jenseits der Vorgaben von Ost und West abzugewinnen sei. Ebenso verwies 1946 der nationale Bolschewik Karl Otto Paetel „auf die Figur des ‚nationalrevolutionären‘ Schriftstellers Ernst Jünger als ein Beispiel für die Existenz eines ‚Anderen Deutschland‘“. Er betrachtete "Auf den Marmorklippen" und "Gärten und Straßen" als „die wichtigsten antinationalsozialistischen Dokumente, die im Dritten Reich entstanden“ seien. Es gehe zu weit, Jünger aufgrund seines Patriotismus als „Nazimitläufer“ zu bezeichnen. Damit wandte sich Paetel besonders gegen das Kollektivschulddenken, das zu jener Zeit weit verbreitet war. Gegen Ende des dritten Zirkularbriefs „an die Freunde“ vom 1. September 1946 fragte Jünger, er sei doch neugierig, ob es seinen Gegnern gelingen werde, ihn „zum Kirchenvater des dritten Reiches zu machen“.
Der Publizist Axel Eggebrecht erklärte Jünger im September 1946 in der Radiosendung „Am Runden Tisch“ des Nordwestdeutschen Rundfunks zu einem Wegbereiter des Nationalsozialismus; bis zu einem gewissen Zeitpunkt sei er dies möglicherweise sogar bewusst gewesen, aus einer „ästhetische Freude an den gewaltsamen Kräften der Zerstörung“. Er sah in dem Schriftsteller einen „unbewußten Vor-Nazi“, von „einer inneren Wandlung könne nicht die Rede sein“. Walther von Hollander und Herbert Blank hielten Jünger jedoch für einen „Gegner des Nationalsozialismus“. Für Gottfried Stein stellte er sich „jederzeit in Wort und Haltung gegen den Hitlerismus.“ Jünger habe als einziger Kriegsbuchautor den Sinn des Krieges „ergründen und dienen“ wollen. Auch für Manfred Michler war er ein Gegner Hitlers. Nach dem fehlgeschlagenen Hitler-Attentat entging Jünger, behauptete Michler, „wie durch ein Wunder der Verhaftung, er wird jedoch aus dem Wehrdienst entlassen.“ Jünger habe aber dem Krieg, so Frank Thiess, „einen bestimmten Sinn innegelegt“. Der sozialistische Erziehungswissenschaftler Heinz-Joachim Heydorn nahm eine „vermittelnde Stellung“ ein: Auch er sah in Jünger einen, wenn auch im Wesentlichen unbewussten Vorläufer des Nationalsozialismus, auch er glaubte nicht an seine innere Wendung, lehnte die Friedensschrift ab, erwartete „aber doch noch wichtige Arbeiten für die Zukunft von Jünger.“ In nahezu allen kulturpolitischen Zeitschriften wurden 1946 Beiträge zum Fall Jünger veröffentlicht. Während im liberalen Exil und von kommunistischen Widerstandskämpfern Jünger als Wegbereiter des Faschismus kritisiert wurde, artikulierte sich in allen anderen Zeitschriften eine auf ihn gerichtete „Hoffnung“ (Dieter Bassermann). Jünger galt als der Prototyp der „Wandlung“ (Johannes R. Becher) des deutschen Nationalismus ins Abendländische, politisch ins Europäische, weltanschaulich ins Christliche. Der Student Karl Friedrich Baedeker schrieb in der "Hamburger Akademischen Rundschau," Jünger sei „die einzige Gestalt, an der die Jugend bei der Suche nach geistiger Führung nicht sogleich vorbeiläuft“. Baedeker betonte vor allem als spezifisch abendländisch die „Haltung ohne Dogmen und Programme“. Ebenso stellte Hans-Hermann Grothoff in der "Hamburger Studentenzeitschrift" die „Theorienfeindlichkeit“ des geistigen Führers Jünger heraus. 1949 am Beginn von seinen "Strahlungen" trat Jünger seinen Kritikern mit der Selbstdeutung als distanzierter Beobachter der Katastrophe entgegen: „Nach dem Erdbeben schlägt man auf die Seismographen ein. Man kann jedoch die Barometer nicht für die Taifune büßen lassen, wenn man nicht zu den Primitiven zählen will.“
Als eine Person von „unzweifelhafte[r] Integrität“ bezeichnete ihn Hannah Arendt 1950. Trotz des „unleugbaren“ Einflusses, den seine früheren Arbeiten auf einige Mitglieder der nazistischen Intelligenz ausübten, sei er „vom ersten bis zum letzten Tag“ des Regimes ein „aktiver Nazigegner“ gewesen. Die besondere Repräsentanz bestehe, so Wilhelm Grenzmann, in seiner Funktion als „ein Verkünder auch unserer Irrtümer, ja, unserer Verhängnisse“. Nach Karl Prümm mache diese Bemerkung Grenzmanns eine „psychologische Konstante“ der Nachkriegsrezeption sichtbar. Den Weg Jüngers zum antidemokratisch-autoritären Engagement bis in die unmittelbare Nähe des Nationalsozialismus hätten seine Leser weitgehend geteilt. Ihre nach 1945 so deutliche Identifikation mit seinem „Widerstand“, seiner „Wandlung“ ermögliche den „kollektiven Freispruch“, erspare die „selbstkritische Auseinandersetzung mit der Vergangenheit“. Jünger erkenne im Nationalsozialismus letztlich die Verkörperung all jener Tendenzen, die von ihm zeitlebens bekämpft wurden und die mit ihren egalitären ldeen dem Faschismus den Boden bereiteten durch Einebnung der Hierarchien bereitet hätten: Nihilismus, Technik- und Fortschrittsglaube, Amerikanismus und vor allem Demokratie und Liberalismus. Seine eigene Rolle während des Kampfes gegen die Weimarer Republik unterziehe Jünger keiner kritischen Überprüfung. Ebenso urteilt Helmut Peitsch: Jünger verkörperte, was als Literaturfunktion programmiert wäre, den von der Katastrophe zur Katharsis bewegten Deutschen. Diesen in den Literaturverhältnissen liegenden Wirkungsbedingungen entspreche seine „Selbstinszenierung“ „optimal“. So wurde er in der bürgerlichen Literaturkritik der Bundesrepublik Deutschland vorwiegend als „Abenteurer“ und Stilist oder als „Mann des Widerstands“ – des „nationalistischen Widerstands gegen das kapitalistisch-demokratische Europa von Versailles“ und des „Inneren Widerstands“ gegen das Hitlerreich – von dem Vorwurf, ein literarischer Wegbereiter des Faschismus gewesen zu sein, zu rehabilitieren versucht.

Peter de Mendelssohn dagegen behauptete 1953 von Jünger, dieser habe mit der Prägung des Begriffs „Totale Mobilmachung“ der totalen Mobilmachung „erst eigentlich Existenz und Greifbarkeit“ verliehen. Christian E. Lewalter erwiderte, Jüngers Schriften bis 1934 seien lediglich „Diagnosen der Zeit“. Tatsächlich habe er doch, so Hans-Peter Schwarz, bis in die dreißiger Jahre hinein „die kriegerische, nationale und sozialistische Diktatur nicht allein prophezeit, sondern auch postuliert“. 1962 meinte der Jünger-Kritiker Helmut Kaiser, Jünger sei ein „Schrittmacher des ‚Dritten Reiches‘“ vor allem insofern gewesen, als durch seine Schriften "Die totale Mobilmachung" (1931), "Der Arbeiter" (1932) und andere die Entwicklung zur Diktatur „von ihm als metaphysische Richtung seiner Zeit gerechtfertigt“ worden wäre. Thomas Manns Behauptung, Jünger sei ein „Wegbereiter des Barbarismus“ gewesen, wurde erst mit der Publikation seiner Briefe 1963 bekannt. Das Urteil wurde als autoritativ beglaubigtes Wort ein Topos in der Streitgeschichte der 1960er, 1970er und 1980er Jahre. In quellenkritischer Hinsicht war die Validität des Urteils – und damit auch die Bezugnahme darauf – nach Ansicht Lothar Bluhms problematisch, da Manns apodiktische Einschätzung sich nicht auf eigene Jünger-Lektüre, sondern auf Hörensagen und Second-hand-Wissen aus dem Familien- und Freundeskreis gestützt hatte. Es stellte sich die Frage, ob der inflationierte Topos „Wegbereiter“ dazu angetan ist, die Rolle eines exponierten Schriftstellers in der Vorzeit des Nationalsozialismus angemessen zu umreißen. So schrieb der französische Jüngerianer Jean-Michel Palmier 1968, Ernst Jünger wäre „dem Nationalsozialismus gegenüber immer und öffentlich abweisend gestanden“. Jakob Schissler meinte ebenso 1976: „Nur wenige der existentialistischen Äußerungen aus dem Lager der Nationalisten können aus dieser Zeit als zielgerichtet, mit dem Bewußtsein der Zukunftsperspektive ausgestattet, angesehen werden.“ Eine Ausnahme bilde die Konzeption Hans Freyers, der wohl gewusst haben möge, welche Kräfte zu welchem Zweck er lancierte; bei Niekisch und Jünger sei „dies nicht so eindeutig auszumachen“. Karl Heinz Bohrer sprach von einer „schwer wägbare[n] allgemeine[n], ideologisch-politische[n] Ausstrahlung“ und stellte fest, dass für Jünger „gar keine andere denn ästhetische Perzeptionsweise“ existiere.

Die Verleihung des Goethepreises 1982 bot dann den unmittelbaren Anlass sowohl für heftige Kontroversen als auch für einen Aufschwung in der Jünger-Forschung. In Robert Wistrichs Perspektive ließ der „halb romantische, halb technokratische Nationalismus“ Jünger als „einen Protagonisten und intellektuellen Wegbereiter des Nationalsozialismus“ erscheinen. Rolf Hochhuth erklärte apologetisch, dass die Handlungsweise eines Menschen nicht ablösbar sei „von der Epoche, die seine Sicht, sein Denken und Tun bestimmte“. Die jüngere Generation wisse eben, was danach gekommen sei, sei insofern klüger, dürfe aber diesen Zuwachs an Kenntnis nicht moralisch, jedenfalls nicht richtend, verwerten. Kritik am Parlamentarismus habe Jünger zu Zeiten geübt, als die Weimarer Republik obenauf war, was „moralisch gerechtfertigt, ja notwendig“ wäre. Peter Longerich stellte die Schriften "Die totale Mobilmachung" und "Der Arbeiter" als „wichtige Zeugnisse des Präfaschismus“ dar. Im Unterschied zu der negativen Bewertung Jüngers durch die "New German Critique" 1993, meinte Martin Konitzer in seinem ebenfalls 1993 erschienenen Buch, dass Jünger „die deutschen Widersprüchlichkeiten dieses Jahrhunderts exemplarisch zu integrieren vermochte“. Zwar sei Jünger ebenso wie Carl Schmitt „zweifellos“ ein „Wegbereiter der Diktatur“ gewesen, meinte Rudolf Augstein 1993: „Aber wir glauben zu wissen, daß alle geistig hochstehenden Wegbereiter zusammen nicht ausgereicht hätten, Hitler den Weg zu ebnen oder zu versperren.“ Bruno Reimann betonte hingegen, auch wenn „kein einzelner Akteur und Autor in einem strikt linearen Sinne verantwortlich“ sei, so hätten doch „alle, welche die nationalen und rechtsradikalen Gebetsmühlen“ drehten, dazu beigetragen – wenn auch in unterschiedlichem Ausmaß. Hochhuths „Apologie“ schilderte er als „unverblümt“ und fragte, wann die chronisch bedrohte Weimarer Demokratie je „obenauf“ gewesen wäre. Dies sei ein „deutliches Anzeichen“ dafür, dass der „Faschismus als mentales Phänomen“ keinesfalls als „historisch abgeschlossenes Faktum“ betrachtet werden könne.

1995 entflammte in den Feuilletons deutscher Zeitungen die Debatte erneut. Anlass war Jüngers 100. Geburtstag, der von Jüngerianern mit Festschriften zelebriert wurde. So schrieb Karlheinz Weißmann, dass „Jüngers sukzessive Abwendung von der NSDAP und dann von der politischen Praxis überhaupt grundsätzlichen Erwägungen folgte. An den Beiträgen seiner nationalistischen Phase war immer die Abstraktheit aufgefallen. Tatsächlich blieb die Programmatik im eigentlichen Sinne undeutlich: ‚Nationalismus‘ war für Jünger Welt-Anschauung im vollen Sinne des Wortes.“ Karlheinz Hasselbach betonte, wie der frühe Jünger als Protofaschist abgestempelt und sein Œuvre als „fascist modernism“ (Russell Berman) abgetan werde, so sei Thomas Mann im Jubiläumsjahr 1975 wegen seiner „Großbürgerlichkeit“ zum Kryptofaschisten erklärt worden. Elke Schmitter meinte dagegen, in dem „Tyrannosaurus Jünger“ das Abbild eines „faschistischen“ „Chamäleons“ zu erkennen. Seine frühen Werke befänden sich „im besten Einklang mit den Schlechtesten ihrer Zeit: beladen und wirr, raunend und düster“. „Er hat es nicht verdient, daß wir beschwichtigend daherreden“, sagte Christian Graf von Krockow, denn schließlich „gehörte er zu den Schreibtischtätern des Unheils“. Sein „Wort zu Auschwitz“ wollte Ralph Giordano hören, und Jürgen Busche meinte, aus dem Frühwerk Jüngers dringe nur „raunendes Schleichen ums Geheimnis, schlichter Unsinn oder Metaphysik im Oberton“. Der Jüngerianer Jörg Sader kritisierte die „Annahme“, Jünger sei „ideologischer Vorläufer des Dritten Reichs gewesen (und nicht lediglich Formellieferant).“ Diese widerspreche der widersprüchlichen Rezeption seines Werks, dessen Wirkung außerdem „eindeutig kaum feststellbar“ gewesen wäre. Ebenso schilderte Thomas Nevin die Einordnung Jüngers als Wegbereiter Hitlers als „absurd“, weil er die rassistische Ideologie abgelehnt habe. Die "Enzyklopädie des Nationalsozialismus" rechnete ihn dennoch unter die „Wegbereiter des Nationalsozialismus“. Elliot Neaman urteilte, diese Bezeichnung sei hinsichtlich der Rezeption Jüngers „ungenau und oberflächlich“. Die Nazis hätten den Inhalt der Schriften "Die totale Mobilmachung" und "Der Arbeiter" missachtet.

Der Historiker Hans-Ulrich Wehler betont: „Eine Figur wie Jünger brauchte auch gar nicht der NSDAP anzugehören, um als einer der intellektuellen Totengräber der Republik, gefeiert von einer riesigen Lesergemeinde, zu wirken“. Jan Ipema urteilt: Wenn man bei Jünger von einer Anerkennung Hitlers und seiner Bewegung sprechen könne, dann beziehe sich diese auf Hitlers erfolgreiches Agieren gegen die Bestimmungen des Vertrages von Versailles, nicht auf den Nationalsozialismus als „Lehre“. Die Theorie, Jünger uneingeschränkt als Vorläufer der Nazi-Bewegung zu apostrophieren, bleibe laut Matthias Heilmann „fragwürdig“. Der französische Philosoph Emmanuel Faye aber bezeichnet ihn schlicht als „Nazi“ und „pervers“. Helmuth Kiesel urteilt, Jünger selbst tendierte zum Faschismus, der allerdings vom Nationalsozialismus zu unterscheiden sei. Zwar dürfe man die „Macht des Wortes“ nicht überschätzen, doch gehöre Jünger unbestreitbar unter „die ‚Totengräber‘ der Weimarer Republik und die ‚Pioniere‘ des ‚Dritten Reiches‘“. Er dürfe deshalb „in einem allerdings sehr weiten Sinne“ (Lutz Unterseher) als einer der intellektuellen Wegbereiter des Nationalsozialismus gelten.

Jünger faszinierte seine Leser und Schriftstellerkollegen bereits zu Lebzeiten, polarisierte dabei aber auch. Einer der Leser der "Stahlgewitter" war André Gide: „unbestreitbar das schönste Kriegsbuch, das ich gelesen habe“, schrieb er am 1. Dezember 1942 in sein Tagebuch. Bertolt Brecht aber sprach Jünger kurz nach dem Zweiten Weltkrieg jeden literarischen Rang ab: „Da er selbst nicht mehr jung ist, würde ich ihn einen Jugendschriftsteller nennen, aber vielleicht sollte man ihn überhaupt nicht einen Schriftsteller nennen, sondern sagen: Er wurde beim Schreiben gesehen.“ Dagegen zählte der Avantgardist Alfred Döblin gerade Jünger, Brecht und sich selbst zu der "geistesrevolutionären Bewegung" innerhalb der deutschen Literatur. Auch wenn Thomas Mann den schriftstellerischen Rang Jüngers nicht angriff, urteilte er 1945 in privaten Briefen über ihn, er sei „ein Wegbereiter und eiskalter Genüssling des Barbarismus“ gewesen, der leider „ein viel zu gutes Deutsch schrieb für Hitler-Deutschland“. 1948 stellte Alfred Andersch die These auf, dass Jüngers Bedeutung nicht zuletzt auch durch seine Umstrittenheit bedingt sei. Er nannte ihn jedoch ohne Umschweife „den letzten aus der großen Reihe Thomas Mann, Franz Kafka, Gottfried Benn und Bertolt Brecht“.

Autoren wie Heiner Müller oder Rolf Hochhuth suchten die Verbindung mit dem alten Jünger. International sind Jüngers Schriften weit verbreitet, ihre Rezeption ist, im Gegensatz zur Situation in Deutschland, weniger auf die politische Publizistik Jüngers fixiert. Bereits seine frühen Schriften wurden in zahlreiche Sprachen übersetzt. Kritisiert wurde an Jüngers Schriften schon früh die Verherrlichung von Gewalt und seine Idealisierung von Männlichkeit in Form des „Kriegers“. Später wurde das Werk Jüngers meist aus einer ästhetischen Perspektive rezipiert, wobei hierbei die brisanten politischen Implikationen ausgeblendet wurden. Während im nationalsozialistischen Deutschland "Auf den Marmorklippen" auch als leicht entschlüsselbare Kritik am NS-Regime gelesen wurde, hat man Jüngers Texte aus der Nachkriegszeit als politisch weniger relevant angesehen. Umso mehr jedoch gelten diese Texte als ästhetisch interessant.

Einen Markstein der wissenschaftlichen Rezeption bildete diesbezüglich Karl Heinz Bohrers Studie von 1978 "Ästhetik des Schreckens," die die Verflechtung von Jüngers Texten mit der europäischen und US-amerikanischen Avantgarde zeigt. Im Gefolge dieser Forschungsöffnung fand Jünger – neben Walter Benjamin, Siegfried Kracauer und anderen – als Klassiker der modernen Medientheorie Beachtung. Hieran schlossen im Zuge der poststrukturalistischen Theoriebildung in Frankreich beispielsweise Virilio und Baudrillard an. Im deutschsprachigen Raum tritt hingegen seine ästhetische Beurteilung als Stilist meist hinter die politische zurück. Die Jünger-Forschung wollte das Risiko vermeiden, einen als „Wegbereiter des Faschismus“ verfemten Autor indirekt dadurch zu rehabilitieren, indem sie ihm literarische Qualitäten zusprach. Hierzu schrieb Bohrer:

Erst in jüngerer Zeit entstehen solche Arbeiten, die – nicht zuletzt angeregt durch die formalästhetische Analyse Bohrers – auf den „sattsam bekannten Vorwurf des Präfaschismus“ (Claudia Gerhards) verzichten und die literarischen Dimensionen von Jüngers Werk stärker thematisieren. Erst in jüngster Zeit finden sich wieder häufiger Interpretationen, die versuchen, den Nachweis subtiler, impliziter Subtexte im Werk Jüngers zu erbringen. Es gelinge ihm, politische Auffassungen auf diese Weise – gleichsam unbemerkt – zu transportieren.
2008 in Frankreich erschienen Jüngers Tagebücher in der Prestigereihe „Bibliothèque de la Pléiade“ bei Gallimard, was den französisch-deutschen Schriftsteller Georges-Arthur Goldschmidt zu einer wütenden Polemik veranlasste: Dass, so behauptete Goldschmidt in der "Frankfurter Rundschau," „dieser doch ein wenig faschistoide, großtuerische Mystagoge“ nun unter den schönen Geistern des französischen Literaturhimmels platziert werde, sei das Allerletzte, die Publikation ein klares Zeichen dafür, dass in der „Pléiade“ die deutsche Emigration und der Widerstand in den Hintergrund verschoben würden. Es gehe, wenn auch unbewusst, um „eine regelrechte Rehabilitierung der deutschen Okkupation Frankreichs“, um eine „Eloge der Kollaboration“. Die deutschsprachige Literaturkritik ist ebenfalls in ihrer Bewertung ambivalent. Jünger hatte Bewunderer wie Friedrich Sieburg und zahlreiche Gegner wie etwa Fritz J. Raddatz. Marcel Reich-Ranicki äußerte 2011 in der "Welt:" „Einen bedeutenden Roman von Jünger sehe ich nicht. Ich glaube, dass er seine Wirkung vor allem seiner Persönlichkeit zu verdanken hatte, nicht seiner Prosa. Thomas Mann hat ihn 1945 unübertrefflich charakterisiert […] Jüngers Werk ist mir fremd.“ Andererseits beurteilten jüngere Kollegen wie Denis Scheck Jünger positiver.

Wenig diskutiert wird seine oft unkonventionelle Themenwahl (in "Heliopolis" kommen Weltraumfahrt und eine Art von Mobiltelefon vor (der Phonophor), "Gläserne Bienen" beschreibt nanotechnisch betriebene Roboter). Hinzu kommen seine wissenschaftlichen Beiträge zur Insektenkunde. Zeit seines Lebens beschäftigte sich Jünger auch mit dem Thema Drogen, auch durch eigene Drogenerfahrungen u. a. mit Opium, Mescalin, Kokain und LSD, die er intensiv in seinem 1970 erschienenen Buch "Annäherungen. Drogen und Rausch" beschreibt und auch in seinen Notiz- und Tagebüchern immer wieder erwähnt. In literarischer Form verarbeitet Jünger Drogenerfahrungen z. B. in "Strahlungen" (1949), "Heliopolis" (1949) und "Besuch auf Godenholm" (1952). Zumeist nahm Jünger Drogen in gesellschaftlichen Runden ein und verwendete Dosierungen, die zu starke Räusche verhinderten. Im Jahr 2013 widmete das Literaturmuseum der Moderne dem langjährigen Briefwechsel Jüngers mit Albert Hofmann, dem Erfinder des LSD, eine umfassende Ausstellung.

Die "Ernst und Friedrich Georg Jünger Gesellschaft" bündelt die Forschung und veranstaltet jedes Jahr am Wochenende vor Ostern ein Symposium zum Werk der Brüder Jünger.

Ein Teil des Nachlasses von Ernst Jünger befindet sich im Deutschen Literaturarchiv in Marbach am Neckar. Zahlreiche Blätter weisen mittlerweile Schädigungen auf, die durch Selbstklebebänder entstanden sind.

Das Archiv zeigte im Jahre 2010 die Ausstellung "Ernst Jünger. Am Abgrund" mit zahlreichen Exponaten aus seinen Lebensabschnitten. Einzelne Exponate aus Jüngers Nachlass sind Teil der Dauerausstellung im Literaturmuseum der Moderne in Marbach, beispielsweise sein Kriegstagebuch, aus dem später "In Stahlgewittern" entstand, und sein Kalender.

Tagebücher

Romane

Erzählungen

Essays

Editionen

Korrespondenz

Sonstiges

Bibliografische Hilfsmittel und Register

Primärliteratur

Sekundärliteratur



</doc>
<doc id="1316" url="https://de.wikipedia.org/wiki?curid=1316" title="Erdbeben">
Erdbeben

Als Erdbeben werden messbare Erschütterungen des Erdkörpers bezeichnet. Sie entstehen durch Masseverschiebungen, zumeist als tektonische Beben infolge von Verschiebungen der tektonischen Platten an Bruchfugen der Lithosphäre, in weniger bedeutendem Maße auch durch vulkanische Aktivität, Einsturz oder Absenkung unterirdischer Hohlräume, große Erdrutsche und Bergstürze sowie durch Sprengungen. Erdbeben, deren Herd unter dem Meeresboden liegt, werden auch Seebeben oder unterseeische Erdbeben genannt. Diese unterscheiden sich von anderen Beben zum Teil in den Auswirkungen wie zum Beispiel der Entstehung eines Tsunamis, jedoch nicht in ihrer Entstehung.

Erdbeben bestehen beinahe in aller Regel nicht aus einer einzelnen Erschütterung, sondern ziehen meist weitere nach sich. Man spricht in diesem Zusammenhang von Vorbeben und Nachbeben mit Bezug auf ein stärkeres Hauptbeben. Treten Erdbeben über einen längeren, begrenzten Zeitraum gehäuft auf, so spricht man von einem "Erdbebenschwarm" oder "Schwarmbeben". Solche treten vor allem in vulkanisch aktiven Regionen auf. In Deutschland gibt es gelegentlich Erdbebenschwärme im Vogtland und am Hochstaufen.

Der deutlich größte Anteil aufgezeichneter Erdbeben ist zu schwach, um von Menschen wahrgenommen zu werden. Starke Erdbeben können Bauten vernichten, Tsunamis und Erdrutsche auslösen und dabei Menschen töten. Sie können die Gestalt der Erdoberfläche verändern und zählen zu den Naturkatastrophen. Die Wissenschaft, die sich mit Erdbeben befasst, heißt Seismologie. Die zehn stärksten seit 1900 gemessenen Erdbeben fanden mit einer Ausnahme alle an der Subduktionszone rund um den Pazifik, dem sogenannten Pazifischen Feuerring, statt (s. Liste unten). 

Laut einer Analyse von mehr als 35.000 Naturkatastrophen-Ereignissen seit 1900 bis 2015 durch das Karlsruher Institut für Technologie (KIT) kamen in dieser Zeit weltweit insgesamt 2,23 Millionen Menschen durch Erdbeben ums Leben.

Schon in der Antike fragten sich Menschen, wie Erdbeben und Vulkanausbrüche entstehen.
Man schrieb diese Ereignisse häufig Göttern zu (in der griechischen Mythologie dem Poseidon). Manche Wissenschaftler im alten Griechenland glaubten, die Kontinente schwämmen auf dem Wasser und schaukelten wie ein Schiff hin und her. Andere Leute glaubten, Erdbeben brächen aus Höhlen aus. In Japan gab es den Mythos des Drachen, der den Erdboden erzittern ließ und Feuer spie, wenn er wütend war. Im europäischen Mittelalter schrieb man Naturkatastrophen dem Wirken Gottes zu. Mit der Entdeckung und Erforschung des Magnetismus entstand die Theorie, man könne Erdbeben wie Blitze ableiten. Man empfahl daher Erdbebenableiter nach Art der ersten Blitzableiter.

Erst Anfang des 20. Jahrhunderts kam die heute allgemein anerkannte Theorie von der Plattentektonik und der Kontinentaldrift durch Alfred Wegener auf. Ab der Mitte des 20. Jahrhunderts wurden die Erklärungsmuster der tektonischen Beben verbreitet diskutiert. Bis zum Beginn des 21. Jahrhunderts konnte man daraus allerdings keine Technik zur sicheren Vorhersage von Erdbeben entwickeln.

Erdbeben entstehen vor allem durch dynamische Prozesse im Erdinneren. Eine Folge dieser Prozesse ist die Plattentektonik, also die Bewegung der Lithosphärenplatten, die von der oberflächlichen Erdkruste bis in den lithosphärischen Mantel reichen.

Besonders an den Plattengrenzen, an denen sich verschiedene Platten auseinander "(Spreizungszone)", aufeinander zu "(Subduktions- oder Kollisionszone)" oder aneinander vorbei "(Transformverwerfung)" bewegen, bauen sich Spannungen innerhalb des Gesteins auf, wenn sich die Platten in ihrer Bewegung verhaken und verkanten. Wird die Scherfestigkeit der Gesteine dann überschritten, entladen sich diese Spannungen durch ruckartige Bewegungen der Erdkruste und es kommt zum tektonischen Beben. Dabei kann mehr als das Hundertfache der Energie einer Wasserstoffbombe freigesetzt werden. Da die aufgebaute Spannung nicht auf die unmittelbare Nähe der Plattengrenze beschränkt ist, kann der Entlastungsbruch in selteneren Fällen auch im Inneren der Platte auftreten, wenn dort das Krustengestein eine Schwächezone aufweist.

Die Temperatur nimmt zum Erdinneren hin stetig zu, weshalb das Gestein mit zunehmender Tiefe immer leichter deformierbar wird und schon in der unteren Erdkruste nicht mehr spröde genug ist, um brechen zu können. Erdbeben haben ihren Ursprung daher meist in der oberen Erdkruste, in wenigen Kilometern Tiefe. Vereinzelt werden jedoch Beben mit Herden bis in 700 km Tiefe nachgewiesen. Solche "Tiefherdbeben" treten vor allem an Subduktionszonen auf. Dort bewegen sich zwei Platten aufeinander zu, wobei die dichtere der beiden unter jene mit der geringeren Dichte geschoben wird und in den Erdmantel abtaucht. Der abtauchende Teil der Platte (engl.: "") erwärmt sich im Mantel jedoch relativ langsam, sodass dessen Krustenmaterial auch noch in größeren Tiefen bruchfähig ist. Die Hypozentren von Erdbeben, die innerhalb eines auftreten, ermöglichen somit Schlüsse auf die Position desselben in der Tiefe (sogenannte Wadati-Benioff-Zone). Als Auslöser dieser Tiefherdbeben gilt unter anderem die Volumenänderung des Slab-Gesteins infolge von Mineralumwandlungen unter den im Mantel herrschenden Temperatur- und Druckbedingungen.

Auch in vulkanischen Zonen aufsteigendes Magma kann, meist eher schwache, Erdbeben verursachen.

Bei unterseeischen Erdbeben, beim Ausbruch ozeanischer Vulkane oder beim Auftreten unterseeischer Erdrutsche können sogenannte Tsunamis entstehen. Bei plötzlicher vertikaler Verlagerung großer Teile des Ozeanbodens entstehen Wellen, die sich mit Geschwindigkeiten von bis zu 800 Kilometern pro Stunde fortbewegen. Auf dem offenen Meer sind Tsunamis kaum wahrnehmbar; läuft die Welle jedoch in flacherem Wasser aus, steilt sich der Wellenberg auf und kann am Ufer in extremen Fällen bis zu 100 Meter Höhe erreichen. Am häufigsten entstehen Tsunamis im Pazifik. Deshalb besitzen die an den Pazifik angrenzenden Staaten ein Frühwarnsystem, das Pacific Tsunami Warning Center. Nachdem am 26. Dezember 2004 etwa 230.000 Menschen nach einem verheerenden Erdbeben im Indischen Ozean starben, wurde auch dort ein Frühwarnsystem errichtet.

Sehr flachgründige und nur lokal spürbare Erdbeben können durch Frost ausgelöst werden („Frostbeben“), wenn größere Mengen Wasser im Boden oder im Gesteinsuntergrund gefrieren und sich dabei ausdehnen. Dadurch entstehen Spannungen, die sich in kleineren Erschütterungen entladen, die dann an der Oberfläche als „Erdbeben“ und grollendes Geräusch wahrgenommen werden. Das Phänomen tritt meist zu Beginn einer strengen Frostperiode auf, wenn die Temperaturen rapide von Werten über dem Gefrierpunkt auf Werte weit unter den Gefrierpunkt gefallen sind.

Neben den natürlich verursachten und ausgelösten Erdbeben gibt es auch anthropogene (menschgemachte) Erdbeben. Diese sogenannte induzierte Seismizität ist nicht zwangsläufig absichtlich oder wissentlich herbeigeführt, wie z. B. im Fall von aktiver Seismik oder bei Atomwaffentests, sondern es sind oft Ereignisse, die als unbeabsichtigte „Nebenwirkungen“ menschlicher Aktivitäten auftreten. Zu diesen Aktivitäten gehören unter anderem die konventionelle Förderung fossiler Kohlenwasserstoffe (Erdöl und Erdgas), die durch Verringerung des Porendruckes die Spannungsverhältnisse im Gestein der Lagerstätte verändert. Anthropogene Erdbeben finden auch beim Einsturz von bergbaulich verursachten unterirdischen Hohlräumen (Gebirgsschlag) oder im Zusammenhang mit der unkonventionellen Förderung fossiler Kohlenwasserstoffe, speziell durch Fracking und die Verpressung von Fracking-Abwässern, statt. Die Magnitude dieser Erdbeben liegt in den allermeisten Fällen im Bereich von Mikrobeben oder Ultramikrobeben. Nur selten erreicht sie den Wert spürbarer Beben. Einige der stärksten anthropogenen Erdbeben ereigneten sich infolge des Aufstauens großer Wassermengen in Stauseen durch die Auflasterhöhung im Untergrund in der Nähe großer Verwerfungen. Das Wenchuan-Erdbeben in China im Jahr 2008 (Magnitude 7,9), das rund 90.000 Todesopfer forderte, gilt als Kandidat für das bislang stärkste durch Stauseen ausgelöste Erdbeben weltweit.

Die Fortpflanzungsgeschwindigkeit eines Bebens beträgt im Normalfall ca. 3,5 km/s (nicht zu verwechseln mit der oben angegebene Wellengeschwindigkeit bei Seebeben). In sehr seltenen Fällen kommt es aber zur überschallschnellen Ausbreitung des Bebens, wobei bereits Fortpflanzungsgeschwindigkeiten von ca. 8 km/s gemessen wurde. Bei einem überschallschnellen Beben breitet sich der Riss schneller aus als die seismische Welle, was normalerweise umgekehrt abläuft. Bisher konnten erst 6 überschallschnelle Beben aufgezeichnet werden.

Erdbeben erzeugen Erdbebenwellen verschiedenen Typs, die sich über und durch die ganze Erde ausbreiten und von Seismographen (bzw. Seismometern) überall auf der Erde in sogenannten Seismogrammen aufgezeichnet werden können. Die mit starken Erdbeben einhergehenden Zerstörungen an der Erdoberfläche (Spaltbildung, Schäden an Gebäuden und Verkehrsinfrastruktur usw.) sind auf die sogenannten "Oberflächenwellen" zurückzuführen, die sich an der Erdoberfläche ausbreiten und eine elliptische Bodenbewegung auslösen.

Durch Auswertung der Stärke und Laufzeiten von Erdbebenwellen kann man die Position des Erdbebenherdes bestimmen, dabei fallen auch Daten über das Erdinnere an. Die Positionsbestimmung unterliegt als Messung an Wellen der gleichen Unschärfe, die bei Wellen in anderen Bereichen der Physik bekannt sind. Im Allgemeinen nimmt die Unschärfe der Ortsbestimmung mit zunehmender Wellenlänge zu. Eine Quelle von langperiodischen Wellen kann also nicht so genau lokalisiert werden wie die von kurzperiodischen Wellen. Da schwere Erdbeben den größten Teil ihrer Energie im langperiodischen Bereich entwickeln, kann besonders die Tiefe der Quelle nicht genau bestimmt werden.

Nach einer Publikation aus dem Jahr 2017 lassen sich bei starken Erdbeben in den Seismometer­aufzeichnungen geringfügige Schwankungen des Gravitationsfelds der Erde nachweisen, die durch die Massenverschiebung ausgelöst werden. Diese Signale breiten sich mit Lichtgeschwindigkeit durch den Erdkörper aus, das heißt deutlich schneller als die primären Erdbebenwellen (P-Wellen), die für gewöhnlich als erstes von den Seismometern registriert werden und eine Geschwindigkeit von höchstens 10 km/s erreichen können. Außerdem sollen sie eine genauere Bestimmung der Magnitude eines Bebens ermöglichen, insbesondere an Messtationen, die relativ nahe am Erdbebenherd liegen. Beides bedeutete eine deutliche Verbesserung bei der Erdbebenfrühwarnung.

Durch den Vergleich der Laufzeiten der seismischen Wellen eines Erdbebens in weltweit verteilten Observatorien, wo die Signale mit Seismographen registriert werden, kann im Rahmen der physikalisch bedingten Unschärfe auf die Position den "Erdbebenherd", fachsprachlich als "Hypozentrum" bezeichnet, als Quelle der Wellen geschlossen werden. Die Quelle der seismischen Wellen kann sich im Laufe eines Bebens bewegen, so etwa bei schweren Beben, die eine Bruchlänge von mehreren hundert Kilometern aufweisen können. Nach internationaler Übereinkunft wird dabei die zuerst gemessene Position als Hypozentrum des Erdbebens bezeichnet, also der Ort, wo das Beben begonnen hat. Der Ort auf der Erdoberfläche direkt über dem Hypozentrum heißt Epizentrum. Der Zeitpunkt des Bruchbeginns wird als "Herdzeit" bezeichnet.

Die Bruchfläche, die das Erdbeben auslöst, wird in ihrer Gesamtheit als "Herdfläche" bezeichnet. In den meisten Fällen erreicht diese Bruchfläche die Erdoberfläche nicht, sodass der Erdbebenherd in der Regel nicht sichtbar wird. Im Fall eines größeren Erdbebens, dessen Hypozentrum in nur geringer Tiefe liegt, kann die Herdfläche bis an die Erdoberfläche reichen und dort zu einem deutlichen Versatz führen. Der genaue Ablauf des Bruchprozesses legt die "Abstrahlcharakteristik" des Bebens fest, bestimmt also, wie viel Energie in Form von seismischen Wellen in jede Richtung des Raumes abgestrahlt wird. Dieser Bruchmechanismus wird als Herdvorgang bezeichnet. Der Ablauf des Herdvorganges kann aus der Analyse von Ersteinsätzen an Messstationen rekonstruiert werden. Das Ergebnis einer solchen Berechnung ist die Herdflächenlösung.

Es gibt drei grundlegende Typen von Erdbebenereignissen, welche die drei Arten der Plattengrenzen widerspiegeln: In Spreizungszonen, wo die tektonischen Platten auseinanderdriften, wirkt eine Zugspannung auf das Gestein ("Extension"). Die Blöcke zu beiden Seiten der Herdfläche werden also auseinandergezogen und es kommt zu einer Abschiebung (engl.: "normal fault"), bei welcher der Block oberhalb der Bruchfläche nach unten versetzt wird. In Kollisionszonen, wo sich Platten aufeinander zubewegen, wirkt dagegen eine Kompressionsspannung. Das Gestein wird zusammengestaucht und es kommt, abhängig vom Neigungswinkel der Bruchfläche, zu einer Auf- oder Überschiebung (engl.: "reverse fault" bzw. "thrust fault"), bei welcher der Block oberhalb der Bruchfläche nach oben versetzt wird. In Subduktionszonen kann sich die abtauchende Platte mitunter großflächig verhaken, was in der Folge zu einem massiven Spannungsaufbau und letztlich zu besonders schweren Erdbeben führen kann. Diese werden gelegentlich auch als "Megathrust-Erdbeben" bezeichnet. Der dritte Herdtyp wird als Blattverschiebung (engl.: "strike-slip fault") bezeichnet, der an Transformverwerfungen vorkommt, wo sich die beteiligten Platten seitlich aneinander vorbeischieben.

In der Realität wirken die Kräfte und Spannungen jedoch zumeist schräg auf die Gesteinsblöcke, da sich die Lithosphärenplatten verkanten und dabei auch drehen können. Die Platten bewegen sich daher im Normalfall nicht gerade aufeinander zu oder aneinander vorbei, so dass die Herdmechanismen zumeist eine Mischform aus einer Auf- oder Abschiebung und einer seitwärts gerichteten Blattverschiebung darstellen. Man spricht hier von einer "Schrägaufschiebung" bzw. "Schrägabschiebung" (engl.: "oblique fault").

Die räumliche Lage der Herdfläche kann durch die drei Winkel Φ, δ und λ beschrieben werden:

Um Erdbeben miteinander vergleichen zu können, ist es notwendig, deren Stärke zu ermitteln. Da eine direkte Messung der freigesetzten Energie eines Erdbebens schon allein auf Grund der Tiefenlage des Herdprozesses nicht möglich ist, wurden in der Seismologie verschiedene Erdbebenskalen entwickelt.

Die ersten Erdbebenskalen, die Ende des 18. bis Ende des 19. Jahrhunderts entwickelt wurden, konnten nur die Intensität eines Erdbebens beschreiben, also die Auswirkungen auf Menschen, Tiere, Gebäude und natürliche Objekte wie Gewässer oder Berge.
Im Jahre 1883 entwickelten die Geologen M. S. De Rossi und F. A. Forel eine zehnstufige Skala zur Bestimmung der Intensität von Erdbeben.
Wichtiger wurde jedoch die im Jahre 1902 eingeführte zwölfteilige Mercalliskala. Sie beruht allein auf der subjektiven Einschätzung der hör- und fühlbaren Beobachtungen sowie der Schadensauswirkung auf Landschaft, Straßen oder Gebäude (Makroseismik).
1964 wurde sie zur MSK-Skala und später zur EMS-Skala weiterentwickelt.

Intensitätsskalen werden auch heute noch verwendet, wobei verschiedene Skalen existieren, die an die Bauweise und Bodenverhältnisse des jeweiligen Landes angepasst sind. Die räumliche Verteilung der Intensitäten wird häufig durch Fragebogenaktionen zuständiger Forschungseinrichtungen (in Deutschland beispielsweise bundesweit durch die BGR per Online-Formular) ermittelt und in Form von Isoseistenkarten dargestellt. Isoseisten sind Isarithmen gleicher Intensitäten. Die Möglichkeit zur Erfassung von Intensitäten beschränkt sich auf relativ dicht besiedeltes Gebiet.

Durch die Entwicklung und stetige Verbesserung von Seismometern ab der zweiten Hälfte des 19. Jahrhunderts eröffnete sich die Möglichkeit, objektive, auf physikalischen Größen basierende Messungen vorzunehmen, was zur Entwicklung der Magnitudenskalen führte. Diese ermöglichen über empirisch gefundene Beziehungen und physikalische Gesetzmäßigkeiten, von den an seismologischen Messstationen aufgezeichneten ortsabhängigen Amplitudenwerten auf die Stärke eines Bebens zurückzuschließen.

Es gibt verschiedene Methoden, die Magnitude zu berechnen. Die unter Wissenschaftlern gebräuchlichste Magnitudenskala ist heute die Momenten-Magnituden-Skala (Mw). Diese beginnt bei Mw 0 und endet bei der Mw 10,6. Man nimmt an, dass bei diesem Wert die feste Erdkruste komplett zerbricht. Die Erhöhung um eine Magnitude entspricht einer 32-fach höheren Energiefreisetzung. Von den Medien wird die in den 1930er Jahren von Charles Francis Richter und Beno Gutenberg eingeführte Richterskala am häufigsten zitiert, die auch als "Lokalbebenmagnitude" bezeichnet wird. Zur exakten Messung der Erdbebenstärke benutzt man Seismographen, die in 100 km Entfernung zum Epizentrum des Erdbebens liegen sollten. Mit der Richter-Skala werden die seismischen Wellen in logarithmischer Einteilung gemessen. Sie diente ursprünglich der Quantifizierung von Erdbeben im Raum Kalifornien. Liegt eine Erdbebenmessstation zu weit vom Erdbebenherd entfernt (> 1000 km) und ist die Stärke des Erdbebens zu groß (ab etwa Magnitude 6), kann diese Magnitudenskala jedoch nicht oder nur eingeschränkt verwendet werden. Sie ist aufgrund der einfachen Berechnung und der Vergleichbarkeit mit älteren Erdbebeneinstufungen vielfach auch in der Seismologie noch in Gebrauch.

Die zeitlich und räumlich exakte Vorhersage von Erdbeben ist nach dem heutigen Stand der Wissenschaft nicht möglich. Die verschiedenen bestimmenden Faktoren sind qualitativ weitestgehend verstanden. Auf Grund des komplexen Zusammenspiels aber ist eine genaue Quantifizierung der Herdprozesse bislang nicht möglich, sondern nur die Angabe einer Wahrscheinlichkeit für das Auftreten eines Erdbebens in einer bestimmten Region.

Allerdings kennt man "Vorläuferphänomene". Einige davon äußern sich in der Veränderung geophysikalisch messbarer Größen, wie z. B. der seismischen Geschwindigkeit, der Neigung des Erdbodens oder der elektromagnetischen Eigenschaften des Gesteins. Andere Phänomene basieren auf statistischen Beobachtungen, wie etwa das Konzept der "seismischen Ruhe", die bisweilen auf ein bevorstehendes größeres Ereignis hindeutet. Wiederholt wurde auch von ungewöhnlichem Verhalten bei Tieren kurz vor größeren Erdbeben berichtet. Dadurch gelang in einem Einzelfall im Februar 1975 die rechtzeitige Warnung der Bevölkerung vor einem Erdbeben.

Alle bekannten Vorläuferphänomene variieren jeweils sehr stark in Zeitverlauf und Größenordnung. Zudem wäre der instrumentelle Aufwand, der für eine lückenlose Erfassung dieser Phänomene erforderlich wäre, aus heutiger Sicht finanziell und logistisch nicht realisierbar.

Wegen des volkswirtschaftlichen Schadens und eventueller Opfer (Massenpanik oder Massenhysterie) ist eine Frühwarnung der Bevölkerung vor einem einzelnen Erdbeben nur sinnvoll, wenn die Zahl der zu erwartenden Opfer des Erdbebens als sehr groß eingeschätzt wird, oder wenn das Erdbeben sehr genau in Raum und Zeit vorausgesagt werden kann.

Die wichtigsten bekannten Erdbebengebiete sind in der Liste der Erdbebengebiete der Erde aufgeführt. Eine umfassende Aufstellung historisch überlieferter Erdbebenereignisse befindet sich in der Liste von Erdbeben.

Die folgende Liste wurde nach Angaben des USGS zusammengestellt. Die Werte beziehen sich, wenn nicht anders angegeben, auf die Momenten-Magnitude M, wobei zu berücksichtigen ist, dass unterschiedliche Magnitudenskalen nicht direkt miteinander vergleichbar sind.

Das Ausmaß der durch ein Erdbeben hervorgerufenen Schäden hängt zunächst von der Stärke und Dauer des Bebens ab sowie von der Besiedlungsdichte und der Anzahl und Größe der Bauwerke in dem betroffenen Bereich. Wesentlich ist aber auch die Erdbebensicherheit der Bauwerke. In der europäischen Norm EC 8 (in Deutschland DIN EN 1998-1) sind die Grundlagen für die Auslegung von Erdbebeneinwirkungen für die verschiedenen Bauarten Holz, Stahl, Stahlbeton, Verbundbauweise, Mauerwerk Bemessungskriterien definiert.







</doc>
<doc id="1317" url="https://de.wikipedia.org/wiki?curid=1317" title="Esther Friesner">
Esther Friesner

Esther Mona Friesner-Stutzman, geb. Friesner (* 16. Juli 1951 in den USA) ist eine US-amerikanische Schriftstellerin. 

Friesner studierte Drama und Spanisch am Vassar College und promovierte dann in Spanisch an der Yale University. Dort unterrichtete sie auch einige Jahre Spanisch und schrieb in ihrer Freizeit Kurzgeschichten und Romane. Schließlich begann sie hauptberuflich zu schreiben.

1982 erschien ihre erste Kurzgeschichte "The Stuff of Heroes". Bis jetzt hat sie 37 Romane veröffentlicht und mehrere Anthologien herausgegeben. Am bekanntesten ist wohl die Anthologie 'Chicks in Chainmail'. 

Friesner lebt zusammen mit ihrem Mann und ihren zwei Kindern in Connecticut.





</doc>
<doc id="1319" url="https://de.wikipedia.org/wiki?curid=1319" title="Europa (Mond)">
Europa (Mond)

Europa (auch "Jupiter II") ist der zweitinnerste Mond des Planeten Jupiter. Sie ist mit einem Durchmesser von 3121 km der kleinste der vier großen Jupitermonde und der sechstgrößte Mond im Sonnensystem. 

Europa ist ein Eismond. Obwohl die Temperatur auf der Oberfläche von Europa maximal −150 °C erreicht, lassen Messungen des äußeren Gravitationsfeldes und der Nachweis eines induzierten Magnetfeldes in der Umgebung Europas mit Hilfe der Galileo-Sonde darauf schließen, dass sich unter der mehrere Kilometer mächtigen Wassereishülle ein etwa 100 km tiefer Ozean aus flüssigem Wasser befindet.

Europas Entdeckung wird dem italienischen Gelehrten Galileo Galilei zugesprochen, der im Jahr 1610 sein einfaches Fernrohr auf den Jupiter richtete. Die vier großen Monde Io, Europa, Ganymed und Kallisto werden auch als die Galileischen Monde bezeichnet.

Benannt wurde der Mond nach Europa, einer Geliebten des Zeus aus der griechischen Mythologie. Obwohl der Name Europa bereits kurz nach seiner Entdeckung von Simon Marius vorgeschlagen wurde, konnte er sich über lange Zeit nicht durchsetzen. Erst in der Mitte des 20. Jahrhunderts kam er wieder in Gebrauch. Vorher wurden die Galileischen Monde üblicherweise mit römischen Ziffern bezeichnet und Europa war "Jupiter II".

Die Galileischen Monde sind so hell, dass man sie auch mit einem Fernglas beobachten kann.

Europa umkreist Jupiter rechtläufig in einem mittleren Abstand von 670.900 Kilometern in 3 Tagen, 13 Stunden und 14,6 Minuten. Ihre Umlaufbahn ist mit einer numerischen Exzentrizität von 0,0101 fast kreisförmig. Ihr jupiternächster und -fernster Bahnpunkt – Perijovum und Apojovum – weichen jeweils nur um 1,01 % von der großen Halbachse ab. Die Bahnebene ist nur 0,470° gegenüber Jupiters Äquatorebene geneigt.

Die Umlaufzeit von Europa steht zu ihrem inneren und äußeren Nachbarmond in einer Bahnresonanz von 2:1 bzw. 1:2; das heißt, während zwei Umläufen von Europa läuft Io genau viermal und Ganymed genau einmal um Jupiter.

Europa weist, wie die übrigen inneren Jupitermonde, eine gebundene Rotation auf, d. h. sie wendet dem Planeten stets dieselbe Seite zu.

Europa besitzt einen mittleren Durchmesser von 3121,6 Kilometern und eine mittlere Dichte von 3,01 g/cm³. Obwohl sie deutlich der kleinste der vier Galileischen Monde ist, ist ihre Masse größer als die aller kleineren Monde des Sonnensystems zusammengenommen.

Die Temperatur auf Europas Oberfläche beträgt nur 110 K (etwa −160 °C) am Äquator und 50 K (etwa −220 °C) an den Polen.

Die Oberfläche von Europa umfasst 30,6 Millionen Quadratkilometer, was ungefähr der Größe von Afrika entspricht. Mit einer Albedo von 0,64 ist sie eine der hellsten Oberflächen aller bekannten Monde im Sonnensystem: 64 % des eingestrahlten Sonnenlichts werden reflektiert. Die Oberfläche setzt sich aus Eis zusammen. Die rötlichen Färbungen sind Folge von abgelagerten Mineralien. Die Oberfläche ist außergewöhnlich eben. Sie ist von Furchen überzogen, die allerdings eine geringe Tiefe aufweisen. Nur wenige Strukturen, die sich mehr als einige hundert Meter über die Umgebung erheben, wurden festgestellt.

Europas Oberfläche weist nur sehr wenige Einschlagkrater auf, die zudem nur von geringerer Größe sind. Von den 41 benannten Kratern ist "Taliesin" mit einem Durchmesser von 50 Kilometern der größte. Der zweitgrößte Krater, "Pwyll", hat einen Durchmesser von 26 Kilometern. Pwyll ist eine der geologisch jüngsten Strukturen auf Europa. Bei dem Einschlag wurde helles Material aus dem Untergrund über hunderte von Kilometern hinweg ausgeworfen.

Die geringe Kraterdichte ist ein Hinweis darauf, dass Europas Oberfläche geologisch sehr jung ist bzw. sich regelmäßig erneuert, sodass nur Einschläge von Kometen und Asteroiden der jüngeren geologischen Vergangenheit darauf dokumentiert sind. Berechnungen des Oberflächenalters anhand der Kraterdichte ergaben ein Höchstalter von ca. 90 Millionen Jahren. Damit besitzt Europa mit die jüngste Oberfläche unter den soliden Himmelskörpern im Sonnensystem.

Ferner konnten anhand von Nahinfrarotaufnahmen der Galileo-Sonde Schichtsilikate auf Europa nachgewiesen werden. Es wird vermutet, dass sie von einem Objekt stammen, das in einem flachen Winkel eingeschlagen ist, wodurch die Einschlagsenergie des Impaktors relativ gering war, sodass dieser weder vollständig verdampfen noch sich tief in die Kruste bohren konnte. Von besonderer Bedeutung ist diese Entdeckung deshalb, weil solche Objekte oft auch organische Verbindungen, sogenannte "Bausteine des Lebens", mit sich führen.

Europas auffälligstes Merkmal ist ein Netzwerk von kreuz und quer verlaufenden Gräben und Furchen, Lineae genannt (Einzahl: Linea), die die gesamte Oberfläche überziehen. Die Lineae haben eine starke Ähnlichkeit mit Rissen und Verwerfungen auf irdischen Eisfeldern. Die größeren sind etwa 20 Kilometer breit und besitzen undeutliche äußere Ränder sowie einen inneren Bereich aus hellem Material. Die Lineae könnten durch Kryovulkanismus (Eisvulkanismus) oder den Ausbruch von Geysiren aus warmem Wasser entstanden sein, wodurch die Eiskruste auseinander gedrückt wurde.

Diese Lineae befinden sich außerdem zum allergrößten Teil an anderen Stellen, als man sie erwartet. Dies lässt sich möglicherweise dadurch erklären, dass sich zwischen Eiskruste und Mondoberfläche ein Ozean befindet. Dieser könnte entstanden sein, weil sich auf Grund der exzentrischen Umlaufbahn des Mondes um den Jupiter andauernd dessen Gravitationswirkung auf Europa ändert, sodass dieser ständig verformt wird. Durch diese Gezeitenkräfte erwärmt sich Europa und das Eis schmilzt zum Teil.

Wenn Europa auf seiner Umlaufbahn die größte Jupiterentfernung durchlief, konnten wiederholt Wasserstoff- und Sauerstoffatome über dem Südpol nachgewiesen werden. Es wird vermutet, dass sie aus der Spaltung von Wassermolekülen stammten, die freigesetzt werden, wenn sich Spalten öffnen und Wasser in den Weltraum schießt, das nach dem Aufstieg bis in eine Höhe von 200 Kilometern auf die Oberfläche zurückfällt.

Ein weiterer Typ von Oberflächenstrukturen sind kreis- und ellipsenförmige Gebilde, Lenticulae (lat. Flecken) genannt. Viele sind Erhebungen (engl. Domes), andere Vertiefungen oder ebene dunkle Flecken. Die Lenticulae entstanden offensichtlich durch aufsteigendes wärmeres Eis, vergleichbar mit Magmakammern in der Erdkruste. Die Domes wurden dabei empor gedrückt, die ebenen dunklen Flecken könnten gefrorenes Schmelzwasser sein. Chaotische Zonen, wie Conamara Chaos, sind wie ein Puzzle aus Bruchstücken geformt, die von glattem Eis umgeben sind. Sie haben das Aussehen von Eisbergen in einem gefrorenen See.

Die äußere Hülle Europas besteht aus Wasser. Basierend auf Messungen des Gravitationsfeldes wurde ihre Mächtigkeit zwischen 80 und 170 Kilometern berechnet. Diese äußere Hülle, die man in Analogie zum Aufbau des Erdkörpers als "Kruste" auffassen kann, ist differenziert in eine äußere Schicht aus Wassereis und eine innere Schicht aus flüssigem Wasser. Die innere flüssige Wasserschicht wird allgemein auch als "Ozean" bezeichnet.

Das genaue Verhältnis von Eis zu Wasser in der äußeren Hülle ist zurzeit noch unbekannt. Jedoch gibt es verschiedene Hypothesen, die auf verschiedenen Ansätzen beruhen. So kommen Berechnungen, denen die Auswertungen von Oberflächenstrukturen zugrunde liegen, auf eine Mächtigkeit der Eishülle von 2 bis 18 Kilometern. Die magnetometrischen Messungen der Galileo-Sonde legen nahe, dass der Ozean zumindest einige Kilometer mächtig sein muss, um die Messwerte erklären zu können. Andere Autoren schließen aufgrund der gleichen Daten auf eine Höchsttiefe des Ozeans von 100 Kilometern bzw. eine Höchstmächtigkeit der Eishülle von 15 Kilometern. Obwohl Europa deutlich kleiner als die Erde ist, wäre die dort vorkommende Menge an flüssigem Wasser damit mehr als doppelt so groß wie die der irdischen Ozeane. Ab etwa drei Kilometern unter der Oberfläche könnte es außerdem im Eis eingeschlossene Wasserblasen geben.

Die relativ glatte Oberfläche Europas und die darauf erkennbaren Strukturen erinnern sehr stark an Eisfelder in Polarregionen auf der Erde. Bei den sehr niedrigen Oberflächentemperaturen ist Wassereis hart wie Gestein. Die größten sichtbaren Krater wurden offensichtlich mit frischem Eis ausgefüllt und eingeebnet. Detaillierte Aufnahmen zeigen, dass sich Teile der Eiskruste gegeneinander verschoben haben und zerbrochen sind, wobei ein Muster von Eisfeldern entstand. Die Bewegung der Kruste wird durch Gezeitenkräfte hervorgerufen, die die Oberfläche um 30 m heben und senken. Die Eisfelder müssten aufgrund der gebundenen Rotation ein bestimmtes, vorhersagbares Muster aufweisen. Weitere Aufnahmen zeigen stattdessen, dass nur die geologisch jüngsten Gebiete ein solches Muster aufweisen. Andere Gebiete weichen mit zunehmendem Alter von diesem Muster ab. Das kann damit erklärt werden, dass sich Europas Oberfläche geringfügig schneller bewegt als ihr innerer Mantel und der Kern. Die Eiskruste ist vom Mondinnern durch den dazwischen liegenden Ozean mechanisch entkoppelt und wird von Jupiters Gravitationskräften beeinflusst. Vergleiche von Aufnahmen der Raumsonden Galileo und Voyager 2 zeigen, dass sich Europas Eiskruste in etwa 10.000 Jahren einmal komplett um den Mond bewegen müsste.

Die von der Voyager- und Galileosonde aufgenommenen Bilder lassen auch darauf schließen, dass die Oberfläche von Europa Subduktion unterliegt. Ähnlich wie bei der Plattentektonik auf der Erde schieben sich mächtige Eisplatten langsam übereinander, wobei die in die Tiefe gedrängten Platten aufschmelzen; an anderen Stellen entsteht dafür neues Oberflächenmaterial. Dem vorgeschlagenen zugrunde liegenden Modell zufolge besteht Europas Eismantel aus zwei Schichten. Die äußere Schicht aus festem Eis „schwimmt“ auf einer Schicht aus weicherem, konvektionierenden Eis. Dies ist der erste entdeckte Fall von Plattentektonik auf einem Himmelskörper außer der Erde.

Europa gilt zwar als Paradebeispiel für einen Eismond, aber der Anteil des Eises am Gesamtvolumen dieses Jupitermondes ist relativ gering und sein Aufbau entspricht eher dem der terrestrischen (erdähnlichen) Planeten: Im Zentrum befindet sich ein wahrscheinlich flüssiger Eisen- oder Eisen-Eisensulfid-Kern. Dieser ist von einem Mantel aus Silikatgesteinen umgeben, der den überwiegenden Teil des Volumens des Satelliten ausmacht.

Aufnahmen des Hubble-Weltraumteleskops ergaben Hinweise auf das Vorhandensein einer extrem dünnen Atmosphäre aus Sauerstoff, mit einem Druck von 10 bar. Es wird angenommen, dass der Sauerstoff durch die Einwirkung der Sonnenstrahlung auf die Eiskruste entsteht, wobei das Wassereis in Sauerstoff und Wasserstoff gespalten wird. Der flüchtige Wasserstoff entweicht in den Weltraum, der massereichere Sauerstoff wird durch Europas Gravitation festgehalten.

Bei Vorbeiflügen der Galileosonde wurde ein schwaches Magnetfeld gemessen (seine Stärke entspricht etwa ¼ der Ganymeds). Das Magnetfeld variiert, während sich Europa durch die äußerst ausgeprägte Magnetosphäre des Jupiter bewegt. Die Daten von Galileo weisen darauf hin, dass sich unter Europas Oberfläche eine elektrisch leitende Flüssigkeit befindet, etwa ein Ozean aus Salzwasser. Darüber hinaus zeigen spektroskopische Untersuchungen, dass die rötlichen Linien und Strukturen an der Oberfläche reich an Salzen wie Magnesiumoxid sind. Die Salzablagerungen könnten zurückgeblieben sein, als ausgetretenes Salzwasser verdampft war. Da die festgestellten Salze in der Regel farblos sind, dürften andere Elemente wie Eisen oder Schwefel für die rötliche Färbung verantwortlich sein.

Das mögliche Vorhandensein von flüssigem Wasser ließ Spekulationen darüber aufkommen, ob in Europas Ozeanen Formen von Leben existieren können. Auf der Erde wurden Lebensformen entdeckt, die unter extremen Bedingungen auch ohne das Vorhandensein von Sonnenlicht bestehen können, etwa in den hydrothermalen Quellen (Schwarze Raucher) oder in der Tiefsee.

Nach einem Bericht des Wissenschaftsmagazins New Scientist kamen NASA-Wissenschaftler, die die gestrichene Nasa-Mission "Jupiter Icy Moons Orbiter" planten, nach Auswertungen früherer Missionen im Frühjahr 2004 zu dem Schluss, dass der Mond Europa weitaus lebensfeindlicher sein könnte als zuvor angenommen.

So wurden auf der Oberfläche Wasserstoffperoxid und von konzentrierter Schwefelsäure bedeckte Flächen nachgewiesen. Hier geht man davon aus, dass die Säure aus dem unter der Eisschicht angenommenen Ozean stammt. Die Konzentration wird mit unterseeischem Vulkanismus erklärt, der für den Schwefel verantwortlich sein kann.

Es ist durchaus möglich, dass der Schwefel vom Jupitermond Io stammt. Mittlerweile gibt es auch Indizien dafür, dass der vermutete Ozean unter der Oberfläche Europas eine nennenswerte Salzkonzentration hat. So wurde Epsomit auf der Oberfläche nachgewiesen (eine Magnesiumsulfat-Verbindung). Epsomit könnte durch Reaktion des Schwefels vom Jupitermond Io mit Magnesiumchlorid unter Strahleneinwirkung entstanden sein. Das Magnesiumchlorid stammt mit hoher Wahrscheinlichkeit aus dem Innern Europas. Epsomit ist wesentlich leichter nachzuweisen als Natrium- oder Kaliumchlorid, das man eher auf Europa vermuten würde.

Um eine Kontaminierung Europas mit irdischen Mikroorganismen zu vermeiden, ließ man die Raumsonde Galileo, die zuletzt Europa beobachtete, in der Jupiteratmosphäre verglühen.

Bislang gibt es keine Hinweise für Leben, doch sollen spätere Missionen dies klären. Gedacht wird an eine unbemannte Kryobot-Raumsonde, die auf der Oberfläche landen, sich durch die Eiskruste durchschmelzen und eine Art „Mini-U-Boot“ in Europas Ozean ablassen soll. Bevor diese Mission überhaupt Wirklichkeit werden kann, könnte in der nächsten Dekade eine "Europa-Orbiter"-Raumsonde gestartet werden, die in eine Umlaufbahn um Europa eintreten und den Mond umfassend studieren soll. Davon erhofft man sich weitere Erkenntnisse über Europa zu sammeln und geeignete Landestellen für spätere Missionen zu finden.

Nach dem Vorbeifliegen der Sonden Pioneer 10 und Pioneer 11 in den Jahren 1973 und 1974 gab es von den größten Monden Jupiters zumindest unscharfe Fotografien. Voyager 1 und Voyager 2 lieferten beim Vorbeifliegen 1979 wesentlich genauere Bilder und Daten. 1995 begann die Sonde Galileo, acht Jahre lang den Jupiter zu umrunden. Sie führte dabei auch genaue Untersuchungen und Messungen an den Galileischen Monden durch, auf denen der größte Teil unseres heutigen Wissens über diese Himmelskörper beruht.

Für das Jahr 2020 planten die Raumfahrtagenturen NASA und ESA den Start der Europa Jupiter System Mission/Laplace Mission, welche zwei Orbiter vorsah (JEO – Jupiter Europa Orbiter und JGO – Jupiter Ganymede Orbiter), die jeweils in einen Orbit um Europa und Ganymed eintreten und das gesamte Jupitersystem mit einem revolutionären Tiefgang erforschen sollten. Der JEO (Jupiter Europa Orbiter) sollte nach der Planung 2028 in den Orbit um Europa einschwenken und mehrere Jahre lang mit verschiedenen on-board-Instrumenten Daten u. a. über Morphologie, Temperatur und Schwerkraft des Mondes sammeln. Zusätzlich sollen mittels Ice Penetrating Radar die Eigenschaften des Wassereises studiert werden, um Aufschluss über die Konsistenz bzw. das Ausmaß des Eismantels und eines eventuellen flüssigen Ozeans geben zu können.

Die NASA, die den JEO bauen wollte, stieg jedoch aus dem Projekt aus. Die ESA verwirklicht jedoch den JGO mit leicht abgewandelter Missionsplanung als JUICE. JUICE soll nach ihrer Ankunft am Jupiter im Jahr 2030 zwei Vorbeiflüge an Europa durchführen und klären, ob Europa den vermuteten Ozean unter seiner Eiskruste hat.

Die NASA plant derzeit die Mission Europa Clipper mit einem Starttermin in den 2020er-Jahren. Geplant sind mehrere Vorüberflüge an Europa, durch die detaillierte Bilder der Mondoberfläche gesammelt werden sollen. Auch eine Schmelzsonde, die sich durch den Eismantel bohren soll, könnte Teil der Mission sein: Mehrere wissenschaftliche Einrichtungen wie das Deutsche Zentrum für Luft- und Raumfahrt (DLR) arbeiten derzeit an entsprechenden Prototypen.

Die allgemein von Wissenschaftlern angestellten Spekulationen über Leben auf Europa werden hin und wieder in popkulturellen Werken aufgegriffen. So hört man in dem Science-Fiction-Film "" aus dem Jahr 1984 (Drehbuch: Arthur C. Clarke) eine Stimme aus dem Off, die eine nicht näher umrissene, hochentwickelte außerirdische Intelligenz repräsentiert, folgenden Satz sagen:

Der Science-Fiction-Film "Europa Report" aus dem Jahr 2013 handelt von einer bemannten Raumfahrtmission zum Jupitermond Europa, bei der die Crew der Landefähre auf große, komplexe, und für Menschen offenbar gefährliche Lebewesen trifft. Diese bewohnen den Ozean unterhalb der Eiskruste Europas, die in dem Film stellenweise kaum dicker als die Eisdecke auf einem zugefrorenen See im Winter ist.




</doc>
<doc id="1320" url="https://de.wikipedia.org/wiki?curid=1320" title="Erde">
Erde

Die Erde ist der dichteste, fünftgrößte und der Sonne drittnächste Planet des Sonnensystems. Sie ist Ursprungsort und Heimat aller bekannten Lebewesen. Ihr Durchmesser beträgt mehr als 12.700 Kilometer und ihr Alter etwa 4,6 Milliarden Jahre. Nach ihrer vorherrschenden geochemischen Beschaffenheit wurde der Begriff der „erdähnlichen Planeten“ geprägt. Der Erde astronomisches Symbol ist ♁ oder ⊕.

Da die Erde aufgrund ihrer zu etwa zwei Dritteln aus Wasser bestehenden sichtbaren Oberfläche vom All her vorwiegend blau erscheint, wird sie auch Blauer Planet genannt. Sie wird metaphorisch auch als „Raumschiff Erde“ bezeichnet.

Das Wort für Erde ist in fast allen Sprachen femininum. Die Erde spielt als Lebensgrundlage des Menschen in allen Religionen eine herausragende Rolle als heilige Ganzheit; in etlichen ethnischen-, Volks- und historischen Religionen entweder als diffuse Vergöttlichung einer „Mutter Erde“ oder als personifizierte Erdgöttin.

Das gemeingermanische Substantiv mhd. "erde", ahd. "erda" beruht mit verwandten Wörtern anderer indogermanischer Sprachen auf idg. "er-".

Gemäß dem ersten Keplerschen Gesetz bewegt sich die Erde um die Sonne auf einer elliptischen Bahn, die Sonne befindet sich in einem der Brennpunkte der Ellipse. Der sonnenfernste Punkt der Umlaufbahn, das Aphel, und der sonnennächste Punkt, das Perihel, sind die beiden Endpunkte der Hauptachse der Ellipse. Der Mittelwert des Aphel- und Perihelabstandes ist die große Halbachse der Ellipse und beträgt etwa 149,6 Mio. km. Ursprünglich wurde dieser Abstand der Definition der Astronomischen Einheit (AE) zugrunde gelegt, die als astronomische Längeneinheit hauptsächlich für Entfernungsangaben innerhalb des Sonnensystems verwendet wird.

Das Perihel liegt bei 0,983 AE (147,1 Mio. km) und das Aphel bei 1,017 AE (152,1 Mio. km). Die Exzentrizität der Ellipse beträgt also 0,0167. Der Perihel-Durchgang erfolgt um den 3. Januar und der Aphel-Durchgang um den 5. Juli. Für einen Sonnenumlauf benötigt die Erde 365 Tage, 6 Stunden, 9 Minuten und 9,54 Sekunden; diese Zeitspanne wird auch als siderisches Jahr bezeichnet. Das siderische Jahr ist 20 Minuten und 24 Sekunden länger als das tropische Jahr, das die Basis für das bürgerliche Jahr der Kalenderrechnung bildet. Die Bahngeschwindigkeit beträgt im Mittel 29,78 km/s, im Perihel 30,29 km/s und im Aphel 29,29 km/s; somit legt die Erde eine Strecke von der Größe seines Durchmessers in gut sieben Minuten zurück.

Zur inneren Nachbarbahn der Venus hat die Erdbahn einen mittleren Abstand von 0,28 AE (41,44 Mio. km) und bis zur äußeren Nachbarbahn des Mars sind es im Mittel 0,52 AE (78,32 Mio. km). Auf der Erdbahn befinden sich mehrere koorbitale Objekte, siehe dazu unter Erdbahn.

Der Umlaufsinn der Erde ist rechtläufig, das heißt, dass sie sich entsprechend der Regel der Drehrichtung im Sonnensystem vom Nordpol der Erdbahnebene aus gesehen dem Uhrzeigersinn entgegengesetzt um die Sonne bewegt.

Die Bahnebene der Erde wird Ekliptik genannt. Die Ekliptik ist um gut 7° gegen die Äquatorebene der Sonne geneigt. Der Nordpol der Sonne ist der Erde am stärksten gegen Anfang September zugewandt, der solare Südpol wiederum gegen Anfang März. Nur um den 6. Juni und den 8. Dezember befindet sich die Erde kurz in der Ebene des Sonnenäquators.

Die Erde rotiert rechtläufig – in Richtung Osten – in 23 Stunden, 56 Minuten und 4,09 Sekunden relativ zu den Fixsternen einmal um ihre eigene Achse. Analog zum siderischen Jahr wird diese Zeitspanne als siderischer Tag bezeichnet. Aufgrund der Bahnbewegung der Erde entlang ihrer Umlaufbahn im gleichen Drehsinn und der daraus resultierenden leicht unterschiedlichen Position der Sonne an aufeinanderfolgenden Tagen ist ein Sonnentag, der als die Zeitspanne zwischen zwei Sonnenhöchstständen (Mittag) definiert ist, etwas länger als ein siderischer Tag und wird nach Definition in 24 Stunden eingeteilt.

Aufgrund der Eigenrotation der Erde hat ein Punkt auf dem Äquator eine Geschwindigkeit von 464 m/s bzw. 1670 km/h. Infolge der dadurch bedingten Fliehkraft ist die Figur der Erde an den Polen geringfügig abgeplattet und dafür gegen ihren Äquator zu einem sogenannten Äquatorwulst verformt. Gegenüber einer volumengleichen Kugel ist der Äquatorradius der Erde 7 Kilometer größer und der Polradius 14 Kilometer kleiner. Der Durchmesser am Äquator ist etwa 43 km größer als der von Pol zu Pol. Der Gipfel des Chimborazo ist wegen seiner Nähe zum Äquator der Punkt der Erdoberfläche, der am weitesten vom Erdmittelpunkt entfernt ist.

Die Rotationsachse der Erde ist 23°26' gegen die senkrechte Achse der Ekliptik geneigt, dadurch werden die Nord- und die Südhalbkugel der Erde an verschiedenen Punkten ihrer Umlaufbahn um die Sonne unterschiedlich beschienen, was zu den das Klima der Erde prägenden Jahreszeiten führt. Die Richtung der Achsneigung fällt für die Nordhalbkugel derzeit in die ekliptikale Länge des Sternbilds Stier. In dieser Richtung steht, von der Erde aus gesehen, am 21. Juni auch die Sonne zur Sommersonnenwende. Da die Erde zwei Wochen später ihr Aphel durchläuft, fällt der Sommer auf der Nordhalbkugel in die Zeit ihres sonnenfernen Bahnbereichs.

Die Gezeitenkräfte des Mondes und der Sonne bewirken am Äquatorwulst der Erde ein Drehmoment, das die Erdachse aufzurichten versucht und zu einer Kreiselbewegung der Rotationsachse führt. Ein vollständiger Kegelumlauf dieser lunisolaren Präzession dauert etwa 25.700 bis 25.800 Jahre. Mit diesem Zyklus der Präzession verschieben sich die Jahreszeiten. Der Mond verursacht durch die Präzessionsbewegung seiner eigenen Umlaufbahn mit einer Periode von 18,6 Jahren eine zusätzliche „nickende“ Bewegung der Erdachse, die als Nutation bezeichnet wird. Der Einfluss des Mondes bewirkt zugleich eine Stabilisierung der Erdachsenneigung, die ohne ihn durch die Anziehungskraft der Planeten bis zu einer Schräglage von 85° taumeln würde. Für Einzelheiten siehe den Abschnitt Mond.

Die Gravitation von Mond und Sonne verursacht auf der Erde die Gezeiten. Der Anteil der Sonne ist dabei etwa halb so groß wie der des Mondes. Damit verbunden ist die Gezeitenreibung von Ebbe und Flut der Meere. Diese bremst die Erdrotation und verlängert dadurch gegenwärtig die Tage um etwa 20 Mikrosekunden pro Jahr. Die Gezeiten wirken sich auch auf die Landmassen aus, die sich um etwa einen halben Meter heben und senken. Die Rotationsenergie der Erde wird dabei in Wärme umgewandelt. Der Drehimpuls wird auf den Mond übertragen, der sich dadurch um etwa vier Zentimeter pro Jahr von der Erde entfernt. Dieser schon lange vermutete Effekt ist seit etwa 1995 durch Laserdistanzmessungen abgesichert. Extrapoliert man diese Abbremsung in die Zukunft, wird auch die Erde einmal dem Mond immer dieselbe Seite zuwenden, wobei ein Tag auf der Erde dann etwa siebenundvierzig Mal so lang wäre wie heute. Damit unterliegt die Erde demselben Effekt, der in der Vergangenheit schon zur gebundenen Rotation "(Korotation)" des Mondes geführt hat.

Die Erde definiert mit ihrem geochemischen Aufbau die Klasse der erdähnlichen Planeten (auch "erdartige", "terrestrische" Planeten, bzw. "Gesteinsplaneten" genannt). Die Erde ist unter den vier erdähnlichen Planeten des Sonnensystems der größte.

Die Erde setzt sich massenanteilig zusammen aus Eisen (32,1 %), Sauerstoff (30,1 %), Silizium (15,1 %), Magnesium (13,9 %), Schwefel (2,9 %), Nickel (1,8 %), Calcium (1,5 %) und Aluminium (1,4 %). Die restlichen 1,2 % teilen sich Spuren von anderen Elementen.

Die Erde besteht nach seismischen Messungen aus drei Schalen: Dem Erdkern, dem Erdmantel und der Erdkruste. Diese Schalen sind durch seismische Diskontinuitätsflächen (Unstetigkeitsflächen) voneinander getrennt. Die Erdkruste und der oberste Teil des oberen Mantels bilden zusammen die Lithosphäre. Sie ist zwischen 50 und 100 km dick und besteht aus großen und kleineren tektonischen Platten.

Ein dreidimensionales Modell der Erde heißt, wie alle verkleinerten Nachbildungen von Weltkörpern, Globus.

Der Äquatorumfang ist durch die Zentrifugalkraft der Rotation mit 40.075,017 km um 67,154 km bzw. um 0,17 % größer als der Polumfang mit 40.007,863 km (bezogen auf das geodätische Referenzellipsoid von 1980). Der Poldurchmesser ist mit 12.713,504 km dementsprechend um 42,816 km bzw. um 0,34 % kleiner als der Äquatordurchmesser mit 12.756,320 km (bezogen auf das Referenzellipsoid; die tatsächlichen Zahlen weichen davon ab). Die Unterschiede im Umfang tragen mit dazu bei, dass es keinen eindeutig höchsten Berg auf der Erde gibt. Nach der Höhe über dem Meeresspiegel ist es der Mount Everest im Himalaya und nach dem Abstand des Gipfels vom Erdmittelpunkt der auf dem Äquatorwulst stehende Vulkanberg Chimborazo in den Anden. Von der jeweils eigenen Basis an gemessen ist der Mauna Kea auf der vom pazifischen Meeresboden aufragenden großen vulkanischen Hawaii-Insel am höchsten.

Wie die meisten festen Planeten und fast alle größeren Monde, zum Beispiel der Erdmond, weist auch die Erde eine deutliche Zweiteilung ihrer Oberfläche in unterschiedlich ausgeprägte Halbkugeln auf. Die Oberfläche von ca. 510 Mio. km² unterteilt sich in eine Landhemisphäre und eine Wasserhemisphäre. Die Landhalbkugel ist die Hemisphäre mit dem maximalen Anteil an Land. Er beträgt mit 47 % knapp die Hälfte der sichtbaren Fläche. Die Fläche der gegenüberliegenden Wasserhalbkugel enthält nur 11 % Land und wird durch Ozeane dominiert.

Damit ist die Erde der einzige Planet im Sonnensystem, auf dessen Oberfläche flüssiges Wasser existiert. 96,5 % des gesamten Wassers der Erde enthalten die Meere. Das Meerwasser enthält im Durchschnitt 3,5 % Salz.

Die Wasserfläche hat in der gegenwärtigen geologischen Epoche einen Gesamtanteil von 70,7 %. Die von der Landfläche umfassten 29,3 % entfallen hauptsächlich auf sieben Kontinente; der Größe nach: Asien, Afrika, Nordamerika, Südamerika, Antarktika, Europa und Australien (Europa ist als große westliche Halbinsel Asiens im Rahmen der Plattentektonik allerdings wahrscheinlich nie eine selbstständige Einheit gewesen). Die kategorische Grenzziehung zwischen Australien als kleinstem Erdteil und Grönland als größter Insel wurde nur rein konventionell festgelegt. Die Fläche des Weltmeeres wird im Allgemeinen in drei Ozeane einschließlich der Nebenmeere unterteilt: den Pazifik, den Atlantik und den Indik. Die tiefste Stelle, das Witjastief 1 im Marianengraben, liegt 11.034 m unter dem Meeresspiegel. Die durchschnittliche Meerestiefe beträgt 3.800 m. Das ist etwa das Fünffache der bei 800 m liegenden mittleren Höhe der Kontinente (s. hypsografische Kurve).

Die größten Platten entsprechen in ihrer Anzahl und Ordnung etwa jener der von ihnen getragenen Kontinente, mit Ausnahme der pazifischen Platte. Alle diese Platten bewegen sich gemäß der Plattentektonik relativ zueinander auf den teils aufgeschmolzenen, zähflüssigen Gesteinen des oberen Mantels, der 100 bis 150 km mächtigen Asthenosphäre.

Das die Erde umgebende magnetische Feld wird von einem "Geodynamo" erzeugt. Das Feld ähnelt nahe der Erdoberfläche einem magnetischen Dipol. Die magnetischen Feldlinien treten auf der Südhalbkugel der Erde aus und durch die Nordhalbkugel wieder in die Erde ein. Im Erdmantel verändert sich die Form des Magnetfeldes. Außerhalb der Erdatmosphäre wird das Dipolfeld durch den Sonnenwind verformt.

Die geomagnetischen Pole der Erde fallen nicht genau mit den geografischen Polen der Erde zusammen. Im Jahr 2007 war die Achse des geomagnetischen Dipolfeldes um etwa 11,5° gegenüber der Erdachse geneigt.

Der Übergang zwischen Erdatmosphäre und Weltraum ist kontinuierlich und man kann daher keine scharfe Obergrenze ziehen. Die Masse beträgt etwa 5,13 × 10 kg und macht somit knapp ein Millionstel der Erdmasse aus. Der mittlere Luftdruck auf dem Niveau des Meeresspiegels beträgt unter Standardbedingungen 1013,25 hPa. In den bodennahen Schichten besteht die Lufthülle im Wesentlichen aus 78 Vol.-% Stickstoff, 21 Vol.-% Sauerstoff und 1 Vol.-% Edelgasen, überwiegend Argon. Dazu kommt ein Anteil von 0,4 Vol.-% Wasserdampf in der gesamten Erdatmosphäre. Der für den Treibhauseffekt wichtige Anteil an Kohlendioxid steigt zurzeit durch menschlichen Einfluss und liegt jetzt bei etwa 0,04 Vol.-%. 

Die auf der Erde meteorologisch gemessenen Temperaturextreme betragen −89,2 °C (gemessen am 21. Juli 1983 auf 3420 Metern Höhe in der Wostok-Station in der Antarktis) und 56,7 °C (gemessen am 10. Juli 1913 im Death Valley auf ). Die mittlere Temperatur in Bodennähe beträgt 15 °C. Die Schallgeschwindigkeit bei dieser Temperatur beträgt in der Luft auf Meeresniveau etwa 340 m/s.

Die Erdatmosphäre streut den kurzwelligen, blauen Spektralanteil des Sonnenlichts etwa fünfmal stärker als den langwelligen, roten und bedingt dadurch bei hohem Sonnenstand die Blaufärbung des Himmels. Die Oberfläche der Meere und Ozeane erscheint vom Weltall aus gesehen ebenfalls blau, weswegen die Erde seit dem Beginn der Raumfahrt auch der „Blaue Planet“ genannt wird. Dieser Effekt ist jedoch auf die stärkere Absorption roten Lichtes im Wasser selbst zurückzuführen. Die Spiegelung des blauen Himmels an der Wasseroberfläche ist dabei nur von nebensächlicher Bedeutung.

Die Erde wird anhand unterschiedlich intensiver Sonneneinstrahlung in Klimazonen eingeteilt, die sich vom Nordpol zum Äquator erstrecken – und auf der Südhalbkugel spiegelbildlich verlaufen. Die Klimate prägen die Vegetation, die in ähnlicher Weise in verschiedene zonale biogeographische Modelle gegliedert werden.

Die jahreszeitlichen Temperaturschwankungen sind umso stärker, je weiter die Klimazone vom Äquator und vom nächsten Ozean entfernt liegt.

Unter den Polargebieten versteht man zum einen die Region innerhalb des nördlichen Polarkreises, die Arktis, sowie die Region innerhalb des südlichen Polarkreises, die Antarktis, die den größten Teil des Kontinents Antarktika enthält.

Besonderes Kennzeichen der Polarregionen ist neben dem kalten Klima mit viel Schnee und Eis der bis zu einem halben Jahr dauernde Polartag mit der Mitternachtssonne bzw. die Polarnacht, aber auch die Polarlichter.

Die Vegetation der polaren- und subpolaren Ökozone reicht von den Kältewüsten (die lediglich kleine, inselartige Pflanzenvorkommen mit sehr wenigen flach wachsenden Arten aufweisen) zu den baumlosen, gras-, strauch- und moosbewachsenen Tundren.

Die gemäßigte Klimazone erstreckt sich vom Polarkreis bis zum vierzigsten Breitengrad und wird in eine kalt- und kühlgemäßigte Zone eingeteilt. Diese Zone weist einen großen Unterschied zwischen den Jahreszeiten auf, der in Richtung des Äquators jedoch etwas abnimmt.

Ein weiteres Merkmal sind die Unterschiede zwischen Tag und Nacht, die je nach Jahreszeit stark variieren. Diese Unterschiede nehmen, je näher man dem Pol kommt, immer mehr zu.

Die Vegetation wird durch Wälder (im Norden boreale Nadelwälder, im Süden nemorale Misch- und Laubwälder der feuchten Mittelbreiten) sowie Grassteppen und winterkalte Halbwüsten und -Wüsten (Prärien und Großes Becken in Nordamerika; Eurasische Steppe und Wüsten Zentralasiens) geprägt.

Die Subtropen (die zum Teil auch als "warmgemäßigte Klimazone" bezeichnet werden) liegen in der geografischen Breite zwischen den Tropen in Äquatorrichtung und den gemäßigten Zonen in Richtung der Pole, ungefähr zwischen 25° und 40° nördlicher beziehungsweise südlicher Breite. In diesen Gebieten herrschen tropische Sommer und nicht-tropische Winter vor. Sie lassen sich weiter in trockene, winterfeuchte, sommerfeuchte und immerfeuchte Subtropen unterteilen.

Eine weitverbreitete Definition definiert "subtropisches Klima" mit einer Mitteltemperatur im Jahr über 20 Grad Celsius, und einer Mitteltemperatur des kältesten Monats unterhalb der 20 Grad Marke.

Die Unterschiede zwischen Tag und Nacht fallen relativ gering aus.

Die Vegetation umfasst vor allem trockene Offenlandschaften (Heiße Halbwüsten und -Wüsten wie die Sahara und die australischen Wüsten), aber auch Waldgebiete (lichte Hartlaubwälder der winterfeuchten „Mittelmeerklimate“ und dichte Lorbeerwälder der immerfeuchten Subtropen). 

Die Tropen befinden sich zwischen dem nördlichen und südlichen Wendekreis. Die Tropen können in die wechselfeuchten und immerfeuchten Tropen unterschieden werden.

In den Tropen sind Tag und Nacht immer ungefähr gleich lang (zwischen 10,5 und 13,5 Stunden). Klimatische Jahreszeiten gibt es nur in den wechselfeuchten Tropen, und lassen sich dort lediglich in Trocken- und Regenzeit unterscheiden.

Die Tropen werden vegetationsgeographisch in die sommerfeuchten- Trocken- und Feuchtsavannen sowie die Regenwälder der immerfeuchten Tropen (Amazonasbecken, Kongobecken, Malaiischer Archipel und Neuguinea) untergliedert. In den Tropen konzentriert sich die größte Artenvielfalt und Biodiversität der Erde.

Die Jahreszeiten werden in erster Linie von der Einstrahlung der Sonne verursacht, und können infolgedessen durch Temperatur- und/oder Niederschlagsmengenschwankungen geprägt sein. In der gemäßigten Zone wird darunter gewöhnlich der Wechsel der Tageshöchst- bzw. Tagestiefsttemperaturen verstanden. In "subtropischen" (und noch ausgeprägter in "tropischen") Regionen wird dieses Temperaturregime stärker mit Schwankungen der Monatsmittel des Niederschlags überlagert, und in seiner Wahrnehmbarkeit beeinflusst.

Die Unterschiede entstehen durch die Neigung des Äquators gegen die Ekliptik. Dies hat zur Folge, dass der Zenitstand der Sonne zwischen dem nördlichen und südlichen Wendekreis hin- und herwandert (daher auch der Name Wendekreis). Dadurch entstehen neben den unterschiedlichen Einstrahlungen auch die unterschiedlichen Tag- und Nachtlängen, die mit zunehmender Polnähe immer ausgeprägter werden.

Die Wanderung erfolgt im Jahresrhythmus wie folgt:


Abweichend davon wird in der Meteorologie der Beginn der Jahreszeiten jeweils auf den Monatsanfang vorverlegt (1. Dezember, 1. März usw.).

Der Energiehaushalt der Erde wird im Wesentlichen durch die Einstrahlung der Sonne und die Ausstrahlung der Erdoberfläche bzw. Atmosphäre bestimmt, also durch den Strahlungshaushalt der Erde. Die restlichen Beiträge von zusammen etwa 0,02 % liegen deutlich unterhalb der Messungsgenauigkeit der Solarkonstanten sowie ihrer Schwankung im Lauf eines Sonnenfleckenzyklus.

Etwa 0,013 % macht der durch radioaktive Zerfälle erzeugte geothermische Energiebeitrag aus, etwa 0,007 % stammen aus der menschlichen Nutzung fossiler und nuklearer Energieträger und etwa 0,002 % verursacht die Gezeitenreibung.

Die geometrische Albedo der Erde beträgt im Mittel 0,367, wobei ein wesentlicher Anteil auf die Wolken der Erdatmosphäre zurückzuführen ist. Dies führt zu einer globalen effektiven Temperatur von 246 K (−27 °C). Die Durchschnittstemperatur am Boden liegt jedoch durch einen starken atmosphärischen Treibhauseffekt bzw. Gegenstrahlung bei etwa 288 K (15 °C), wobei die Treibhausgase Wasser und Kohlendioxid den Hauptbeitrag liefern. 

Die Wechselwirkungen zwischen Lebewesen und Klima haben heute durch den zunehmenden Einfluss des Menschen eine neue Quantität erreicht. Während im Jahr 1920 etwa 1,8 Milliarden Menschen die Erde bevölkerten, wuchs die Erdbevölkerung bis zum Jahr 2008 auf knapp 6,7 Milliarden an. In den Entwicklungsländern ist für die absehbare Zukunft weiterhin ein starkes Bevölkerungswachstum zu erwarten, während in vielen hoch entwickelten Ländern die Bevölkerung stagniert oder nur sehr langsam zunimmt, deren industrieller Einfluss auf die Natur aber weiterhin wächst. Im Februar 2005 prognostizierten Experten der Vereinten Nationen bis zum Jahr 2013 einen Anstieg der Erdbevölkerung auf 7 Milliarden und auf 9,1 Milliarden bis 2050. 

Das Streben nach steigendem Lebensstandard für alle Menschen führt zu einem weltweit zunehmendem Konsum, der wiederum mit einem steigenden Energieverbrauch einhergeht. Da der Großteil aus der Verbrennung fossiler Energieträger stammt, kommt es zu einer Erhöhung des Kohlendioxidgehaltes in der Atmosphäre. Da Kohlendioxid eines der wichtigsten Treibhausgase ist, entstand das Phänomen des anthropogenen Klimawandels, der nach Auffassung der Mehrzahl aller beteiligten Wissenschaftler zu einer deutlichen, globalen Temperatursteigerung führen wird. Die Folgen dieses Prozesses werden erhebliche Auswirkungen auf Klima, Meere, Vegetation, Tierwelt und Mensch haben. Die primären Folgen sind häufigere und verstärkte Wetterereignisse, ein steigender Meeresspiegel infolge abschmelzenden Inlandeises und der Wärmeausdehnung des Wassers, sowie eine Verlagerung der Klima- und Vegetationszonen nach Norden. Sofern die internationalen Klimaschutzbemühungen zu wenig Erfolg haben, kann es zu einem Szenario unkalkulierbarer Risiken für die Erde kommen, die von den Medien gern als „Klimakatastrophe“ bezeichnet werden.

Der Mond umkreist die Erde als natürlicher Satellit. Das Verhältnis des Durchmessers des Erdmonds zu seinem Planeten von 0,273 (mittlerer Monddurchmesser 3.476 km zu mittlerem Erddurchmesser 12.742 km) ist deutlich größer als bei den „Monden“ der anderen Planeten.

Seit Mitte der 1980er-Jahre hat sich die Ansicht durchgesetzt, dass der Mond nach einem seitlichen Zusammenstoß der Proto-Erde mit einem etwa marsgroßen Körper, Theia genannt, entstanden ist.

Die Präzessionsbewegung der Erdachse ist auch mit einer Schwankung der Achsneigung von ± 1,3° um den Mittelwert von 23,3° verbunden. Diese Schwankung würde wesentlich größer ausfallen, wenn die Präzessionsperiode von etwa 26.000 Jahren in Resonanz mit einer der zahlreichen periodischen Störungen stünde, welchen die Erdbahn infolge der Gravitationseinflüsse der anderen Planeten unterliegt. Gegenwärtig hat lediglich eine durch Jupiter und Saturn verursachte Störung mit einer Periode von 25.760 Jahren einen gewissen Einfluss, ist aber zu schwach, um größere Veränderungen zu bewirken. Wie Simulationsrechnungen zeigen, wäre im gegenwärtigen Zustand des Sonnensystems die Achsneigung der Erde instabil, wenn sie im Bereich von etwa 60° bis 90° läge; die tatsächliche Neigung von gut 23° hingegen ist weit genug von starken Resonanzen entfernt und bleibt stabil.

Hätte die Erde jedoch keinen Mond, so wäre die Präzessionsperiode etwa dreimal so groß, weil der Mond etwa zwei Drittel der Präzessionsgeschwindigkeit verursacht und bei seiner Abwesenheit nur das von der Sonne verursachte Drittel übrigbliebe. Diese deutlich längere Präzessionsperiode läge in der Nähe zahlreicher Störungen, von denen die stärksten mit Perioden von 68.750, 73.000 und 70.800 Jahren erhebliche Resonanzeffekte verursachen würden. Rechnungen zeigen, dass unter diesen Umständen alle Achsneigungen zwischen 0° und etwa 85° instabil wären. Eine typische Schwankung von 0° bis 60° würde dabei weniger als 2 Millionen Jahre erfordern.

Der große Satellit verhindert diese Resonanzen und stabilisiert so mit seiner relativ großen Masse die Neigung der Erdachse gegen die Ekliptik. Die auf diese Weise bewirkte Zügelung der Jahreszeiten schafft günstige Bedingungen für die Entwicklung des Lebens auf der Erde.

Außer dem Mond gibt es kleinere erdnahe Objekte, sogenannte koorbitale Asteroiden, die zwar nicht die Erde umkreisen, aber in einer 1:1-Bahnresonanz auf einer sogenannten Hufeisenumlaufbahn um die Sonne kreisen. Beispiele dafür sind der etwa 50 bis 110 Meter große Asteroid 2002 AA und der etwa zehn bis 30 Meter große Asteroid 2003 YN.

Auch in bzw. bei den Lagrange-Punkten L und L der Erde können sich Begleiter aufhalten, die dann Trojaner genannt werden. Bislang wurde ein einziger natürlicher Trojaner der Erde entdeckt, der etwa 300 Meter große Asteroid 2010 TK.

Die Erde entstand wie die Sonne und ihre anderen Planeten vor etwa 4,6 Milliarden Jahren aus der Verdichtung des Sonnennebels. Man nimmt heute allgemein an, dass sie während der ersten 100 Millionen Jahre intensiv von Asteroiden bombardiert wurde. Heute ist der Beschuss nur noch gering. Die meisten der Objekte, die am Himmel als Meteore erscheinen, sind kleiner als 1 cm. Auf der Erde sind im Gegensatz zum Mond fast alle Einschlagkrater durch geologische Prozesse wieder verschwunden. Die junge Erde erhitzte sich durch die kinetische Energie der Einschläge während des schweren Bombardements und durch die Wärmeproduktion des radioaktiven Zerfalls, bis sie größtenteils aufgeschmolzen war. Danach differenzierte sich gravitativ der Erdkörper in einen Erdkern und einen Erdmantel. Dabei sanken die schwersten Elemente, vor allem Eisen, zum Schwerpunkt der Erde, wobei auch Wärme freigesetzt wurde. Leichte Elemente, vor allem Sauerstoff, Silizium und Aluminium, stiegen nach oben und aus ihnen bildeten sich hauptsächlich silikatische Minerale, aus denen auch die Gesteine der Erdkruste bestehen. Da die Erde vorwiegend aus Eisen und Silikaten besteht, hat sie wie alle terrestrischen Planeten eine recht hohe mittlere Dichte von 5,515 g/cm.

Die Entwicklung der Erdoberfläche im Wechselspiel der geologischen- und biologischen Faktoren wird als Erdgeschichte bezeichnet.

Die Herkunft des Wassers auf der Erde, insbesondere die Frage, warum auf der Erde deutlich mehr Wasser vorkommt als auf den anderen erdähnlichen Planeten, ist bis heute nicht befriedigend geklärt. Ein Teil des Wassers dürfte durch das Ausgasen des Magmas entstanden sein, also letztlich aus dem Erdinneren stammen. Ob dadurch aber die heutige Menge an Wasser erklärt werden kann, ist fraglich. Weitere große Anteile könnten aber auch durch Einschläge von Kometen, transneptunischen Objekten oder wasserreichen Asteroiden (Protoplaneten) aus den äußeren Bereichen des Asteroidengürtels auf die Erde gekommen sein. Messungen des Isotopenverhältnisses von Deuterium zu Protium (D/H-Verhältnis) deuten dabei eher auf Asteroiden hin, da in Wassereinschlüssen in kohligen Chondriten ähnliche Verhältnisse gefunden wurden wie in ozeanischem Wasser, wohingegen nach bisherigen Messungen dieses Isotopen-Verhältnis von Kometen und transneptunischen Objekten nicht mit dem von irdischem Wasser übereinstimmt.

Die historische Geologie ist ein Teilgebiet der Geologie. Ihr Forschungs- und Lehrgegenstand ist die Erdgeschichte, das heißt, der Zeitraum von der Entstehung der Erde bis zur (geologischen) Gegenwart.
Die Erde ist bisher der einzige Planet, auf dem Lebensformen bzw. eine Biosphäre nachgewiesen wurden. Nach dem gegenwärtigen Stand der Forschung begann das Leben auf der Erde möglicherweise innerhalb eines relativ kurzen Zeitraums, gleich nach dem Ausklingen des letzten schweren Bombardements großer Asteroiden, dem die Erde nach ihrer Entstehung vor etwa 4,6 Milliarden Jahren bis etwa vor 3,9 Milliarden Jahren als letzte Phase der Bildung ihres Planetensystems ausgesetzt war. Nach dieser Zeit hat sich eine stabile Erdkruste ausgebildet und so weit abgekühlt, dass sich Wasser auf ihr sammeln konnte. Es gibt Hinweise, die allerdings nicht von allen damit befassten Wissenschaftlern anerkannt werden, dass sich Leben schon (geologisch) kurze Zeit später entwickelte.

In 3,85 Milliarden Jahre altem Sedimentgestein aus der Isua-Region im Südwesten Grönlands wurden in den Verhältnissen von Kohlenstoffisotopen Anomalien entdeckt, die auf biologischen Stoffwechsel hindeuten könnten; bei dem Gestein kann es sich aber auch statt um Sedimente lediglich um ein stark verändertes Ergussgestein ohne derartige Bedeutung handeln. Die ältesten direkten, allerdings umstrittenen Hinweise auf Leben sind Strukturen in 3,5 Milliarden Jahre alten Gesteinen der Warrawoona-Gruppe im Nordwesten Australiens und im Barberton-Grünsteingürtel in Südafrika, die als von Cyanobakterien verursacht gedeutet werden. Die ältesten eindeutigen Lebensspuren auf der Erde sind 1,9 Milliarden Jahre alte Fossilien aus der Gunflint-Formation in Ontario, die Bakterien oder Archaeen gewesen sein könnten.

Die chemische wie die biologische Evolution sind untrennbar mit der Klimageschichte verknüpft. Obwohl die Strahlungsleistung der Sonne anfangs deutlich geringer als heute war (vgl. Paradoxon der schwachen jungen Sonne), gibt es Hinweise auf Leben auf der Erde, grundsätzlich vergleichbar dem heutigen, „seit es Steine gibt“.

Durch den Stoffwechsel des pflanzlichen Lebens, also durch die Photosynthese, wurde die Erdatmosphäre mit molekularem Sauerstoff angereichert und bekam ihren oxidierenden Charakter. Zudem wurde die Albedo und damit die Energiebilanz durch die Pflanzendecke merklich verändert.

Die Lebensformen auf der Erde entstanden in der permanenten Wechselwirkung zwischen dem Leben und den herrschenden klimatischen-, geologischen- und hydrologischen Umweltbedingungen. Die Biosphäre ist eine systemische Ganzheit, die in großflächigen Biomen, Ökosystemen und Biotopen beschrieben wird.

Seit rund 3 bis 2 Millionen Jahren existiert die Gattung Homo auf der Erde, aus der vor rund 300.000 Jahren der anatomisch moderne Mensch hervorgegangen ist. Bis zur Erfindung von Pflanzenbau und Nutztierhaltung im Vorderen Orient (ca. 11.), in China (ca. 8.) und im mexikanischen Tiefland (ca. 6. Jahrtausend v. Chr.) lebten die Menschen ausschließlich als Jäger und Sammler. Seit dieser sogenannten neolithischen Revolution verdrängten die vom Menschen gezüchteten Kulturpflanzen und -tiere im Laufe der Ausbreitung der Zivilisationen die natürliche Pflanzen- und Tierwelt immer mehr. Spätestens seit der industriellen Revolution wurde der Einfluss des Menschen auf das Erscheinungsbild und die Entwicklung der Erde immer größer. Weiträumige Landflächen wurden in Industrie- und Verkehrsflächen umgewandelt.

Der anthropogene Wandel der Erde hatte bereits zu Beginn der Neuzeit in einigen Regionen der Erde deutliche negative Auswirkungen. So kam es etwa in Mitteleuropa seit dem 16. Jahrhundert zu einer dramatischen Holznot, die mit einer erheblichen Entwaldung einherging. Im 18. und 19. Jahrhundert entstanden daraus die ersten größeren Bewegungen für Umwelt- und Naturschutz in Europa und Nordamerika. Umweltverschmutzung und -zerstörung globalen Ausmaßes nahmen im 20. Jahrhundert rapide zu. Die 1972 erschienene Studie „Grenzen des Wachstums“ zeigte erstmals umfassend die zugrundeliegenden Zusammenhänge auf. Seit 1990 ist der 22. April als Tag der Erde der internationale Aktionstag zum Schutz der Umwelt. 1992 erfolgte eine erste „Warnung der Welt-Wissenschaftsgemeinde an die Menschheit“ zur dringenden Reduzierung schädlicher Einflüsse auf die Erde.

Das Jahr 2008 wurde von den Vereinten Nationen unter Federführung der UNESCO zum Internationalen Jahr des Planeten Erde (IYPE) erklärt. Diese bislang größte weltweite Initiative in den Geowissenschaften soll die Bedeutung und den Nutzen der modernen Geowissenschaften für die Gesellschaft und für eine nachhaltige Entwicklung verdeutlichen. Zahlreiche Veranstaltungen und interdisziplinäre Projekte auf internationaler und nationaler Ebene erstreckten sich von 2007 bis 2009 über einen Zeitraum von insgesamt drei Jahren.

2009 formulierte ein 28-köpfiges Wissenschaftlerteam unter Leitung von Johan Rockström (Stockholm Resilience Centre) die sogenannten "Planetary Boundaries", um die entscheidenden ökologischen Belastungsgrenzen der Erde zu quantifizieren. Genannt wurden:

Die nähere Zukunft der Erdoberfläche hängt sehr stark von der Entwicklung des menschlichen Umwelteinflusses ab.

Am 13. November 2017 veröffentlichten 15.372 Wissenschaftler aus 184 Ländern dazu eine „zweite Warnung an die Menschheit“, da es außer beim Schutz der Ozonschicht und den Fischfangquoten keinerlei reale Fortschritte gegeben hat: Fast alle wichtigen ökologischen Kennzahlen haben sich drastisch verschlechtert. Besonders beunruhigend sind die Trends bei der Klimaerwärmung, der Entwaldung, der Zunahme toter Gewässer und der Verringerung der Artenvielfalt. Die Wissenschaftler sehen die Lebensgrundlagen der Menschheit in ernster Gefahr und rufen zu kurzfristigen Gegenmaßnahmen auf.

Die fernere Zukunft der Erde ist eng an die der Sonne gebunden.

Weiterstrahlen der Sonne

Die Kernfusion vermindert im Zentrum der Sonne die Teilchenzahl (4 p + 2 e → He), aber kaum die Masse. Daher wird der Kern langsam schrumpfen und heißer werden. Außerhalb des Kerns wird sich die Sonne ausdehnen, das Material wird durchlässiger für Strahlung, sodass die Gesamthelligkeit der Sonne etwa um 10 % über die nächsten 1,1 Milliarden Jahre und um 40 % nach 3,5 Milliarden Jahren steigen wird.

Auswirkungen auf die Erde
Sofern obige Sonnenveränderungen als Haupteinflussfaktor auf die Erde angenommen werden, wird vermutet, dass die Erde noch etwa 500 Millionen Jahre lang ähnlich wie heute belebt bleiben könne. Danach, so zeigen Klimamodelle, wird der Treibhauseffekt instabil – höhere Temperatur führt zu mehr Wasserdampf in der Atmosphäre, was wiederum den Treibhauseffekt verstärken wird. Der warme Regen wird durch Erosion den anorganischen Kohlenstoffzyklus beschleunigen, wodurch der CO-Gehalt der Atmosphäre auf etwa 10 ppm in etwa 900 Millionen Jahren (verglichen mit 280 ppm in vorindustrieller Zeit) stark abnehmen wird, so dass mit den Pflanzen auch die Tiere verhungern werden. Nach einer weiteren Milliarde Jahren wird das gesamte Oberflächenwasser verschwunden sein und die globale Durchschnittstemperatur der Erde +70 °C erreichen.

Roter Riese
Die Leuchtkraftzunahme der Sonne wird sich fortsetzen und sich in etwa sieben Milliarden Jahren deutlich beschleunigen. Als roter Riese wird sich die Sonne bis an die heutige Erdbahn erstrecken, sodass die Planeten Merkur und Venus abstürzen und verglühen werden. Das wird, anders als zunächst gedacht, auch der Erde passieren. Zwar wird die Sonne in diesem Riesenstadium durch starken Sonnenwind etwa 30 % ihrer Masse verlieren, sodass rechnerisch der Erdbahnradius auf 1,7 AE anwachsen wird, aber die Erde wird in der nahen, sehr diffusen Sonnenoberfläche eine ihr nachlaufende Gezeitenwelle hervorrufen, die an ihrer Bahnenergie zehren und so die Flucht vereiteln wird.






</doc>
<doc id="1321" url="https://de.wikipedia.org/wiki?curid=1321" title="Mond">
Mond

Der Mond (mhd. "mâne"; ) ist der einzige natürliche Satellit der Erde. Sein Name ist etymologisch verwandt mit "Monat" und bezieht sich auf die Periode seines Phasenwechsels. Weil aber die Trabanten anderer Planeten des Sonnensystems im übertragenen Sinn meistens ebenfalls als Monde bezeichnet werden, spricht man zur Vermeidung von Verwechslungen mitunter vom Erdmond. Er ist mit einem Durchmesser von 3476 km der fünftgrößte Mond des Sonnensystems.

Weil er sich relativ nahe der Erde befindet, ist er bisher der einzige fremde Himmelskörper, den Menschen betreten haben, und auch der am besten erforschte. Trotzdem gibt es noch viele Unklarheiten, etwa in Bezug auf seine Entstehung und manche Geländeformen. Die jüngere Entwicklung des Mondes ist jedoch weitgehend geklärt.

Sein astronomisches Symbol ☾ ist die abnehmende Mondsichel, wie sie (nach rechts offen) von der Nordhalbkugel der Erde aus erscheint.

Die gemeingerm. Bezeichnung des Himmelskörpers mhd. "mān[e]", ahd. "māno" geht auf idg. "mēnōt-" „Mond; Mondwechsel, Monat“ zurück.

Der Mond umkreist die Erde bezüglich der Fixsterne in durchschnittlich 27 Tagen, 7 Stunden und 43,7 Minuten. Er umläuft von Westen nach Osten die Erde im gleichen Drehsinn, mit dem die Erde um ihre Achse rotiert. Er umkreist für einen irdischen Beobachter die Erde wegen ihrer viel schnelleren Rotation scheinbar an einem Tag – wie auch die Sonne, die Planeten und die Fixsterne – und geht daher wie diese im Osten auf und im Westen unter. Der Mond bewegt sich selbst relativ zu den Fixsternen im "rechtläufigen" Drehsinn der Erdrotation, sodass sein scheinbarer Erdumlauf etwa 50 Minuten länger als 24 Stunden dauert. Dies addiert sich in einem Monat zu einem ganzen Tag, da der Mond in dieser Zeit tatsächlich die Erde einmal umläuft.

Die scheinbaren Bahnen von Mond und Sonne liegen ähnlich, da die Mondbahn nur geringfügig (derzeit 5,2°) gegen die Ekliptik geneigt ist. Der Mond steht für einen Beobachter auf der Nordhalbkugel über 5,2° nördlich des Wendekreises (d. h. bei einer geografischen Breite über 28,6°) bei seinem täglichen Höchststand "(Kulmination)" immer im Süden, für einen Beobachter auf der Südhalbkugel südlicher als −28,6° immer im Norden (für die Sonne beträgt der analoge Winkel 23,4° – die Breite der Wendekreise).
Diese ±28,6° sind der Maximalwert. Dieser Wert schwankt in einem 18-jährigen Zyklus zwischen dem Minimum 18,3° und dem Maximum 28,6°, weil die Lage der Mondbahn (bei fast konstanter Bahnneigung von 5,2°) langsam gegenüber der Ekliptik rotiert, was von der Präzession (Kreiselbewegung) der Mondbahnebene infolge der Erdabplattung von 0,3 % verursacht wird.

Die scheinbare Größe des Mondes schwankt entfernungsabhängig zwischen 29,2′ (knapp 0,5°) und 33,3′ um einen Mittelwert von knapp 32′. Da im Mittel die scheinbare Größe der Sonne ebenfalls 32′ beträgt (31,5′ bis 32,5′), kann bei einer entsprechenden Konstellation der Mond die Sonne vollständig verdecken. Ein solches Ereignis heißt Sonnenfinsternis.

Die Bahn des Mondes um die Erde ist etwa kreisförmig, genauer elliptisch. In einem der beiden Brennpunkte der Ellipse befindet sich nicht der Erdmittelpunkt, sondern der gemeinsame Schwerpunkt, das Baryzentrum. Der mittlere Abstand des Schwerpunktes des Mondes vom Baryzentrum – die große Halbachse der Ellipse – misst 383.398 km, etwa 60 Erdradien. Der Erdmittelpunkt ist weniger als einen Erdradius vom Baryzentrum entfernt; das Baryzentrum liegt im Erdmantel. Der Abstand des Baryzentrums vom Mittelpunkt der Ellipse, ihre Exzentrizität, beträgt im Mittel 21.296 km oder 5,55 % der großen Halbachse. Um so viel ist der erdnächste Punkt der Bahn, das Perigäum, näher bzw. der erdfernste Punkt, das Apogäum, weiter als die große Halbachse vom Baryzentrum entfernt.

Der Mond umläuft zusammen mit der Erde die Sonne, durch die Bewegung um die Erde pendelt der Mond jedoch um eine gemeinsame Ellipsenbahn. Die Variation der Gravitation während dieser Pendelbewegung führt zusammen mit geringeren Störungen durch die anderen Planeten zu Abweichungen von einer exakten Keplerellipse um die Erde.

Die Durchgänge des Mondes durch die Bahnebene der Erde (die Ekliptik) nennt man "Mondknoten" (oder Drachenpunkte). Der aufsteigende Knoten ist der Übergang auf die Nordseite der Ekliptik, der absteigende markiert den Übergang auf die südliche Seite. Der erdnächste Punkt der Bahn wird nicht nach genau einem Umlauf (relativ zu den Fixsternen) des Mondes wieder erreicht. Durch diese Apsidendrehung umläuft das Perigäum die Erde in 8,85 Jahren. Auch zwei aufsteigende Knotendurchgänge erfolgen nicht exakt nach einem Umlauf, sondern bereits nach kürzerer Zeit. Die Mondknoten umlaufen die Erde folglich "retrograd", das heißt gegen die Umlaufrichtung des Mondes in 18,61 Jahren. Wenn ein Knotendurchgang mit Neumond zusammenfällt, kommt es zu einer Sonnenfinsternis, und falls der Knotendurchgang mit Vollmond zusammenfällt, kommt es zu einer Mondfinsternis.

Dieser Zyklus führt auch zu den Mondwenden: Der Aufgangsort des Mondes am Horizont schwankt während eines Monats zwischen einem südlichsten und einem nördlichsten Punkt hin und her, so wie es auch bei der Sonne im Verlauf eines Jahres der Fall ist (vgl. Obsigend und Nidsigend). Im Laufe des Zeitraumes von 18,61 Jahren verändert sich die Spanne zwischen diesen beiden Extrempunkten in ihrem Abstand: Der Zeitpunkt (zuletzt im Jahre 2006), an dem diese Punkte am weitesten auseinanderliegen, heißt "große Mondwende", der des geringsten Abstandes "kleine Mondwende". In der frühzeitlichen Astronomie spielten diese Mondwenden eine wichtige Rolle.

Die Dauer eines Bahnumlaufs des Mondes, den "Monat" (von „Mond“), kann man nach verschiedenen Kriterien festlegen, die jeweils unterschiedliche Aspekte abdecken.

Bei diesen Werten handelt es sich um Mittelwerte. Insbesondere die Längen einzelner synodischer Monate schwanken durch die Wanderung der Neumondposition über die Bahnellipse. Die Monatslänge nimmt langsam zu, siehe Abschnitt: Vergrößerung der Umlaufbahn.

Das Aussehen des Mondes, seine Lichtgestalt, variiert im Laufe seines Bahnumlaufs und durchläuft die Mondphasen:
Die Zahlen in Klammern beziehen sich auf obenstehende Abbildung. Manchmal wird die Zeitspanne seit dem letzten Neumond in Tagen angegeben und als "Mondalter" bezeichnet, beispielsweise ist Vollmond am 15. Tag des synodischen Monats und das Mondalter dann 14 Tage (wenn Neumond = 0).

Von einem Standort mittlerer und höherer nördlicher Breiten aus gesehen, erreicht der Mond seinen Höchststand über dem Horizont je im Süden, auch zu Neumond. Die schmale sichelförmige Lichtgestalt des zunehmenden Mondes wird am westlichen Abendhimmel tiefstehend kurz vor Untergang erstmals sichtbar und erscheint dem nördlich stehenden Betrachter als nach Süden zu offene bzw. nach links gekrümmte, konkav-konvexe Figur.

Einem Betrachter in südlichen Breiten erscheint sie ebenfalls tiefstehend im Westen, doch als nach rechts gekrümmt bzw. geöffnet Richtung Norden, wo für ihn der Mond den höchsten Stand erreicht wie ebenso die Sonne zu Mittag. An Beobachtungsorten in Äquatornähe erscheint die Figur im Westen eher waagrecht liegend bzw. nach oben hin offen, da hier der Höhenwinkel einer Kulmination größer ist. Diese Abhängigkeit der scheinbaren Lage der Mondfigur vom Breitengrad spiegelt sich bei der Verwendung einer symbolischen Mondsichel in Form einer Schale („Mondschiffchen“) auf der Staatsflagge einiger äquatornaher Länder wider (Beispiel: Flagge Mauretaniens).

Die nicht unmittelbar von der Sonne beleuchteten Anteile der erdzugewandten Mondseite sind dabei nie völlig dunkel, denn sie werden durch das von der sonnenbeleuchteten Erde zurückgeworfene Licht – Erdlicht oder Erdschein genannt – erhellt. Dessen Widerschein durch die Reflexion an Stellen der Mondoberfläche wird auch "Aschgraues Mondlicht" genannt. Es ist am besten in der Dämmerung einige Tage vor oder nach Neumond zu sehen, denn dann stört weder viel Tages- noch Mondlicht, und der Mond hat nahezu „Vollerde“.

Seine Ursache wurde schon von Leonardo da Vinci richtig erkannt. Mit einem Fernglas selbst geringer Vergrößerung sind auf den nur durch die Erde beschienenen Mondflächen sogar Einzelheiten erkennbar, denn aufgrund des fast vierfach größeren Durchmessers und des höheren Rückstrahlungsvermögens (Albedo) der Erde ist die „Vollerde“ rund 50-mal so hell wie der Vollmond. Messungen des aschgrauen Mondlichts erlauben Rückschlüsse auf Veränderungen der Erdatmosphäre. Bei Vollmond beträgt seine Beleuchtungsstärke 0,2 Lux.

Die ständig erdabgewandte Rückseite des Mondes unterliegt entsprechend versetzt dem Phasenwechsel: Bei Neumond wird sie vom Sonnenlicht vollständig beschienen.

Die beschienene Mondfläche (Überdeckungsgrad) kann angegeben werden mit formula_1, wobei formula_2 die Elongation (i. e., der Winkel zwischen Mond, Erde, und Sonne) ist.

Verfinsterungen treten auf, wenn die Himmelskörper Sonne und Mond mit der Erde auf einer Linie liegen. Dazu kommt es nur bei Vollmond oder Neumond und wenn der Mond sich dann nahe einem der zwei Mondknoten befindet.

Bei einer Mondfinsternis, die nur bei Vollmond auftreten kann, steht die Erde zwischen Sonne und Mond. Eine Mondfinsternis kann auf der gesamten Nachtseite der Erde beobachtet werden und dauert maximal 3 Stunden 40 Minuten.
Man unterscheidet

Eine Mondfinsternis ist vom Mond aus gesehen eine Sonnenfinsternis. Dabei verschwindet die Sonne hinter der schwarzen Erde. Bei einer totalen Mondfinsternis herrscht auf der ganzen Mondvorderseite totale Sonnenfinsternis, bei einer partiellen Mondfinsternis ist die Sonnenfinsternis auf dem Mond nur in einigen Gebieten total, und bei einer Halbschatten-Mondfinsternis herrscht auf dem Mond partielle Sonnenfinsternis. Auf dem Mond kann keine ringförmige Sonnenfinsternis beobachtet werden, da der scheinbare Durchmesser der Erde im Vergleich zu dem der Sonne viel größer ist. Lediglich wird der Rand der schwarzen Erdscheibe zu einem kupferrot schimmernden Ring, der durch die beschriebene Lichtstreuung in der Erdatmosphäre entsteht und dem Mond auf der Erde seine Farbe verleiht.

Bei einer Sonnenfinsternis, die nur bei Neumond auftreten kann, steht der Mond zwischen Sonne und Erde. Eine Sonnenfinsternis kann nur in den Gegenden beobachtet werden, die den Kern- oder Halbschatten des Mondes durchlaufen; diese Gegenden sind meist lange, aber recht schmale Streifen auf der Erdoberfläche.
Man unterscheidet

Eine "Sonnenfinsternis" wird nur vom irdischen Beobachter als solche wahrgenommen. Die Sonne leuchtet natürlich weiter, dagegen liegt die Erde im Schatten des Mondes. Entsprechend zur Mondfinsternis müsste man korrekterweise also von einer "Erdfinsternis" sprechen.

Die "Sarosperiode" kannten bereits die Chaldäern (um etwa 1000 v. Chr.), dabei wiederholen sich Finsternisse nach einem Zeitraum von 18 Jahren und 11 Tagen. Nach 223 synodischen beziehungsweise 242 drakonitischen Monaten (von lat. "draco", Drache, altes astrologisches Symbol für die Mondknoten, da man dort einen mond- und sonnenfressenden Drachen vermutete) stehen Sonne, Erde und Mond fast wieder gleich zueinander, so dass sich eine Finsternis nach 18 Jahren und 11,33 Tagen erneut ergibt. Die Sarosperiode wird dadurch verursacht, dass bei einer Finsternis sowohl die Sonne als auch der Mond nahe der Knoten der Mondbahn liegen müssen, die in 18 Jahren einmal um die Erde laufen. Die Sarosperiode nutzte Thales, die er bei einer Orientreise kennenlernte, für seine Finsternisprognose vom 28. Mai 585 v. Chr., wodurch eine Schlacht zwischen Lydern und Medern abgebrochen und ihr Krieg beendet wurde.

Ein Saros-Zyklus ist eine Folge von Sonnen- oder Mondfinsternissen, die jeweils im Abstand einer Sarosperiode aufeinanderfolgen. Da die Übereinstimmung der 223 bzw. 242 Monate nicht exakt ist, reißt ein Saros-Zyklus etwa nach 1300 Jahren ab. In diesem Zeitraum beginnen aber gleich viele neue Zyklen, und es existieren immer ungefähr 43 gleichzeitige verschachtelte Saros-Zyklen.

Der mittlere Erde-Mond-Abstand wächst aufgrund der Gezeitenreibung jährlich etwa um 3,8 cm (siehe Lunar Laser Ranging). Dabei wird Drehimpuls (hauptsächlich) der Erdrotation in Bahndrehimpuls verwandelt (hauptsächlich des Mondes, siehe Tabelle). Pro 100.000 Jahre nehmen die Tageslänge um etwa eine Sekunde, die Bahnperiode um 0,23 Millisekunden zu.

Als der Mond noch flüssig und der Erde viel näher war, bremste umgekehrt das Feld der Erde die Rotation des Mondes schnell bis zur gebundenen Rotation. Seither dreht er sich pro Umlauf genau einmal um die eigene Achse, zeigt uns stets die gleiche Seite. Als Relikt der freien Rotation ist ihr noch eine sehr geringe Pendelbewegung überlagert, die sogenannte echte Libration. Der größte Teil der Libration ist jedoch ein nur scheinbares Pendeln, bedingt durch die variable Winkelgeschwindigkeit der Bahnbewegung. Wegen der Libration und der Parallaxe, sprich durch Beobachtung von verschiedenen Punkten etwa bei Mondaufgang und Monduntergang, sind von der Erde aus insgesamt fast 59 % der Mondoberfläche einsehbar bzw. ist von Punkten dieser Fläche aus die Erde zumindest zeitweise sichtbar. Mit der Raumsonde Lunik 3 konnte 1959 erstmals auch die Rückseite des Mondes beobachtet werden.

Der mittlere Äquatordurchmesser des Mondes beträgt 3476,2 km und der Poldurchmesser 3472,0 km. Sein mittlerer Durchmesser insgesamt – als volumengleiche Kugel – beträgt 3474,2 km.

Die Gestalt des Mondes gleicht mehr der eines dreiachsigen Ellipsoids als der einer Kugel. An den Polen ist er etwas abgeplattet, und die in Richtung der Erde weisende Äquatorachse ist etwas größer als die darauf senkrecht stehende Äquatorachse. Der Äquatorwulst ist auf der erdabgewandten Seite dabei noch deutlich größer als auf der erdnahen Seite.

In Richtung Erde ist der Durchmesser durch die Gezeitenkraft am größten. Hierbei ist der erdferne Mondradius an dieser Achse größer als der erdnahe. Dies ist überraschend, und bis heute nicht schlüssig erklärt. Pierre-Simon Laplace hatte schon 1799 vermutet, dass der Äquatorwulst zur erdabgewandten Seite hin stärker ausgebildet ist und die Bewegung des Mondes beeinflusst, und dass diese Form nicht einfach ein Ergebnis der Drehung des Mondes um die eigene Rotationsachse sein kann. Seitdem rätseln Mathematiker und Astronomen, wie der Mond diese Ausbuchtung gebildet und behalten hat, nachdem sein Magma erstarrt war.

Der Mond hat keine Atmosphäre im eigentlichen Sinn – der Mondhimmel ist nicht blau –, sondern nur eine Exosphäre. Sie besteht zu etwa gleichen Teilen aus Helium, Neon, Wasserstoff sowie Argon und hat ihren Ursprung in eingefangenen Teilchen des Sonnenwindes. Ein sehr kleiner Teil entsteht auch durch Ausgasungen aus dem Mondinneren, wobei insbesondere Ar, das durch Zerfall von K im Mondinneren entsteht, von Bedeutung ist. Interessanterweise wird ein Teil dieses Argon aber durch das im Sonnenwind mittransportierte Magnetfeld wieder auf die Mondoberfläche zurückgetrieben und dort in die obersten Partikel des Regolith implantiert. Da K früher häufiger war und damit mehr Ar ausgaste, kann durch Messung des Ar/Ar-Verhältnisses von Mondmaterial bestimmt werden, zu welcher Zeit es exponiert war. Es besteht ein Gleichgewicht zwischen der Implantation und thermischem Entweichen.

Aufgrund der langsamen Rotation des Mondes und seiner nur äußerst dünnen Gashülle gibt es auf der Mondoberfläche zwischen der Tag- und der Nachtseite sehr große Temperaturunterschiede. Mit der Sonne im Zenit steigt die Temperatur auf etwa 130 °C und fällt in der Nacht auf etwa −160 °C. Die Durchschnittstemperatur über die gesamte Oberfläche beträgt 218 K = −55 °C. In manchen Gebieten gibt es lokale Anomalien, in Form von einer etwas höheren oder auch etwas niedrigeren Temperatur an benachbarten Stellen. Krater, deren Alter als relativ jung angesehen wird, wie zum Beispiel Tycho, sind nach Sonnenuntergang etwas wärmer als ihre Umgebung. Wahrscheinlich können sie durch eine dünnere Staubschicht die während des Tages aufgenommene Sonnenenergie besser speichern. Andere positive Temperaturanomalien gründen eventuell auf örtlich etwas erhöhter Radioaktivität.

Die Bestimmung der Mondmasse kann über das newtonsche Gravitationsgesetz erfolgen, indem die Bahn eines Körpers im Gravitationsfeld des Mondes untersucht wird. Eine recht gute Näherung für die Mondmasse erhält man bereits, wenn man das Erde-Mond-System als reines Zweikörperproblem betrachtet.

Erde und Mond stellen in erster Näherung ein Zweikörpersystem dar, wobei beide Partner ihren gemeinsamen Schwerpunkt formula_3 umkreisen. Beim Zweikörpersystem aus Erde und Sonne fällt dieser Schwerpunkt praktisch mit dem Sonnenmittelpunkt zusammen, da die Sonne sehr viel massereicher als die Erde ist. Bei Erde und Mond ist der Massenunterschied jedoch nicht so groß, daher liegt der Erde-Mond-Schwerpunkt nicht im Zentrum der Erde, sondern deutlich davon entfernt (aber noch innerhalb der Erdkugel). Bezeichnet man nun mit formula_4 den Abstand des Erdmittelpunkts und mit formula_5 den Abstand des Mondmittelpunkts vom Schwerpunkt formula_3, folgt aus der Definition des Schwerpunkts
dass das Massenverhältnis von Erde "M" zu Mond "m" gerade dem Verhältnis von formula_4 zu formula_5 entspricht. Somit geht es nur darum, wie groß formula_4 und formula_5 sind – also wo sich der Schwerpunkt des Systems befindet.

Ohne den Mond und dessen Schwerkraft durchliefe die Erde eine elliptische Bahn um die Sonne. Tatsächlich bewegt sich der Schwerpunkt des Erde-Mond-Systems auf einer elliptischen Bahn. Die Rotation um den gemeinsamen Schwerpunkt erzeugt eine leichte Welligkeit in der Erdbahn, die eine kleine Verschiebung der von der Erde aus gesehenen Position der Sonne verursacht. Aus der gemessenen Größe dieser Verschiebung wurde formula_4 zu etwa 4670 km berechnet, also etwa 1700 km unter der Erdoberfläche (der Radius der Erde beträgt 6378 km). Da der Mond keine genaue Kreisbahn um die Erde beschreibt, berechnet man formula_5 über die "mittlere" große Halbachse abzüglich formula_4. Es gilt also formula_5 = 384.400 km − 4.670 km = 379.730 km. Damit ergibt sich für das Massenverhältnis
Die Masse des Mondes beträgt daher etwa der Masse der Erde. Durch Einsetzen der Erdmasse "M" ≈ 5,97 · 10 kg ergibt sich die Masse des Mondes zu

Genauere Messungen vor Ort ergeben einen Wert von "m" ≈ 7,349 · 10 kg.

Die Analyse des Mondbrockens "Troctolite 76535", der mit der Mission Apollo 17 zur Erde gebracht wurde, deutet auf ein früheres dauerhaftes Magnetfeld des Erdmondes und damit auf einen ehemals oder immer noch flüssigen Kern hin. Jedoch hat der Mond inzwischen kein Magnetfeld mehr.

Der Sonnenwind und das Sonnenlicht lassen auf der sonnenzugewandten Mondseite Magnetfelder entstehen. Dabei werden Ionen und Elektronen aus der Oberfläche freigesetzt. Diese wiederum beeinflussen den Sonnenwind.
Die seltenen „Mondwirbel“ ohne Relief, sogenannte Swirls, fallen außer durch ihre Helligkeit auch durch eine Magnetfeldanomalie auf. Diese werden als Magcon (Magnetic concentration) bezeichnet. Zu ihrer Entstehung gibt es unterschiedliche Theorien. Eine davon geht von großen antipodischen Einschlägen aus, von denen Plasmawolken rund um den Mond liefen, sich auf der Gegenseite trafen und dort den eisenhaltigen Mondboden auf Dauer magnetisierten. Nach einer anderen Vorstellung könnten manche der Anomalien auch Reste eines ursprünglich globalen Magnetfeldes sein.

Der Mond hat mit 3476 km etwa ein Viertel des Durchmessers der Erde und weist mit 3,345 g/cm eine geringere mittlere Dichte als die Erde auf. Aufgrund seines im Vergleich zu anderen Monden recht geringen Größenunterschieds zu seinem Planeten bezeichnet man Erde und Mond gelegentlich auch als Doppelplanet. Seine im Vergleich zur Erde geringe mittlere Dichte blieb auch lange ungeklärt und sorgte für zahlreiche Theorien zur Entstehung des Mondes.

Das heute weithin anerkannte Modell zur Entstehung des Mondes besagt, dass vor etwa 4,5 Milliarden Jahren ein Himmelskörper von der Größe des Mars nahezu streifend mit der Protoerde kollidierte. Dabei wurde viel Materie, vorwiegend aus der Erdkruste und dem Mantel des einschlagenden Körpers, in eine Erdumlaufbahn geschleudert, ballte sich dort zusammen und formte schließlich den Mond. Der Großteil des Impaktors vereinte sich mit der Protoerde zur Erde. Nach aktuellen Simulationen bildete sich der Mond in einer Entfernung von rund drei bis fünf Erdradien, also in einer Höhe zwischen 20.000 und 30.000 km. Durch den Zusammenstoß und die frei werdende Gravitationsenergie bei der Bildung des Mondes wurde dieser aufgeschmolzen und vollständig von einem Ozean aus Magma bedeckt. Im Laufe der Abkühlung bildete sich eine Kruste aus den leichteren Mineralen aus, die noch heute in den Hochländern vorzufinden sind.

Die frühe Mondkruste wurde bei größeren Einschlägen immer wieder durchschlagen, so dass aus dem Mantel neue Lava in die entstehenden Krater nachfließen konnte. Es bildeten sich die "Maria" (Mondmeere), die erst einige hundert Millionen Jahre später vollständig erkalteten. Das sogenannte letzte große Bombardement endete erst vor 3,8 bis 3,2 Milliarden Jahren, nachdem die Anzahl der Einschläge von Asteroiden vor etwa 3,9 Milliarden Jahren deutlich zurückgegangen war. Danach ist keine starke vulkanische Aktivität nachweisbar, doch konnten einige Astronomen – vor allem 1958/59 der russische Mondforscher Nikolai Kosyrew – vereinzelte Leuchterscheinungen beobachten, sogenannte Lunar Transient Phenomena.

Im November 2005 konnte eine internationale Forschergruppe der ETH Zürich sowie der Universitäten Münster, Köln und Oxford erstmals die Entstehung des Mondes präzise datieren. Dafür nutzten die Wissenschaftler eine Analyse des Isotops Wolfram-182 und berechneten das Alter des Mondes auf 4527 ± 10 Millionen Jahre. Somit ist er 30 bis 50 Millionen Jahre nach der Herausbildung des Sonnensystems entstanden.

Das Wissen über den inneren Aufbau des Mondes beruht im Wesentlichen auf den Daten der vier von den Apollo-Missionen zurückgelassenen Seismometer, die diverse Mondbeben sowie Erschütterungen durch Einschläge von Meteoroiden und durch extra zu diesem Zweck ausgelöste Explosionen aufzeichneten. Diese Aufzeichnungen lassen Rückschlüsse über die Ausbreitung der seismischen Wellen im Mondkörper und damit über den Aufbau des Mondinneren zu, wobei die geringe Anzahl der Messstationen nur sehr begrenzte Einblicke ins Mondinnere liefert. Über die Oberflächengeologie, die bereits durch Beobachtungen von der Erde aus grob bekannt war, wurden durch die von den Apollo- und Luna-Missionen zur Erde gebrachten Mondgesteinsproben sowie durch detaillierte Kartierungen der Geomorphologie, der mineralischen Zusammensetzung der Mondoberfläche und des Gravitationsfeldes im Rahmen der Clementine- und der Lunar-Prospector-Mission neue Erkenntnisse gewonnen.

Seismisch lässt sich die Mondkruste aus Anorthosit (mittlere Gesteinsdichte 2,9 g/cm) auf der Mondvorderseite in einer durchschnittlichen Tiefe von 60 km gegen den Mantel abgrenzen. Auf der Rückseite reicht sie vermutlich bis in 150 km Tiefe. Die größere Mächtigkeit der Kruste und damit der erhöhte Anteil relativ leichten feldspatreichen Krustengesteins auf der erdabgewandten Seite könnte zumindest teilweise dafür verantwortlich sein, dass das Massezentrum des Mondes etwa 2 km näher an der Erde liegt als sein geometrischer Mittelpunkt. Unterhalb der Kruste schließt sich ein fast vollständig fester Mantel aus mafischem und ultramafischem Gestein (Olivin- und Pyroxenreiche Kumulate) an. Zwischen Mantel und Kruste wird eine dünne Schicht basaltischer Zusammensetzung vermutet, die bei der Auskristallisierung der anderen beiden Gesteinshüllen mit inkompatiblen Elementen angereichert wurde und daher einen hohen Anteil an Kalium, Rare Earth Elements (dt. Seltene Erden) und Phosphor aufweist. Diese spezielle chemische Signatur, die sich auch durch hohe Konzentrationen von Uran und Thorium auszeichnet, wird KREEP genannt. Nach traditionellen Hypothesen tritt diese sogenannte Ur-KREEP-Schicht gleichmäßig verteilt unterhalb der Mondkruste auf. Neueren, aus Daten der Lunar-Prospector-Sonde gewonnenen Erkenntnissen zufolge scheint sich KREEP aber schon während der Ausdifferenzierung von Kruste und Mantel vorwiegend in der Kruste der heutigen Oceanus-Procellarum-Mare-Imbrium-Region angereichert zu haben. Die Wärmeproduktion durch die radioaktiven Elemente wird für den vermuteten „jungen“ Vulkanismus in dieser Mondregion (bis 1,2 Milliarden Jahre vor heute) verantwortlich gemacht.

Die seismische Erkundung des Mondes erbrachte Hinweise auf Unstetigkeitsflächen (Diskontinuitäten) in 270 und 500 km Tiefe, die als Grenzflächen verschieden zusammengesetzter Gesteinshüllen gedeutet werden und deshalb als die Grenzen zwischen oberem und mittlerem (270 km) bzw. mittlerem und unterem (500 km) Mondmantel gelten. Der obere Mantel wird in diesem Modell als quarzführender Pyroxenit interpretiert, der mittlere als mit FeO-angereichterter olivinführender Pyroxenit und der untere Mantel als Olivin-Orthopyroxen-Klinopyroxen-Granat-Vergesellschaftung. Aber auch andere Interpretationen sind möglich.

Über den Mondkern ist kaum etwas bekannt und über dessen genaue Größe und Eigenschaften existieren unterschiedliche Ansichten. Durch aufwendige Aufbereitung seismischer Daten wurde nunmehr ermittelt, dass der Mondkern mit einem Radius von etwa 350 km ungefähr 20 % der Größe des Mondes besitzt (vgl. Erdkern relativ zur Größe der Erde: ≈ 50 %) und sich die Mantel-Kern-Grenze damit in einer Tiefe von etwa 1400 km befindet. Es wird angenommen, dass er, wie der Erdkern, vor allem aus Eisen besteht. Hierbei liefern die seismischen Daten (u. a. die Dämpfung von Scherwellen) Hinweise darauf, dass ein fester innerer Kern von einem flüssigen äußeren Kern umgeben ist, an den sich wiederum nach außen eine teilaufgeschmolzene Zone des untersten Mantels (PMB, "partially molten boundary layer") anschließt. Aus diesem Modell lassen sich die ungefähren Temperaturen ableiten, die im Kern des Mondes entsprechend herrschen müssen, die deutlich unter denen des Erdkerns, um die 1400 °C (± 400 °C) liegen. Unterster Mantel und Kern mit ihrem teilaufgeschmolzenen bzw. flüssigen Material werden zusammen auch als Mondasthenosphäre bezeichnet. Die sich offenbar vollständig rigide verhaltenden Bereiche darüber (mittlerer und oberer Mantel sowie Kruste), in denen keine Dämpfung von Scherwellen stattfindet, bilden entsprechend die Mondlithosphäre.

Die zurückgelassenen Seismometer der Apollo-Missionen registrierten bis zum Ende der Messungen im Jahre 1977 etwa 12.000 Mondbeben. Die stärksten dieser Beben waren mit einer Magnitude von knapp 5 um ein Vielfaches schwächer als die stärksten Erdbeben. Die meisten Mondbeben hatten Magnituden um 2. Die seismischen Wellen der Beben konnten ein bis vier Stunden lang verfolgt werden. Sie wurden im Mondinneren also nur sehr schwach gedämpft.

Bei mehr als der Hälfte der Beben befand sich das Hypozentrum in einer Tiefe von 800 bis 1000 km, oberhalb der Mondasthenosphäre. Diese Beben traten bevorzugt bei Apogäum- und Perigäumdurchgang auf, das heißt alle 14 Tage. Daneben sind auch Beben mit oberflächennahem Hypozentrum bekannt. Ursache der Beben sind mit der Erdentfernung schwankende Gezeitenkräfte. Abweichungen vom mittleren Gezeitenpotential sind am erdnächsten und erdfernsten Punkt der Mondbahn groß. Die Hypozentren der Beben verteilten sich jedoch nicht gleichmäßig über eine gesamte Mantelschale. Die meisten Beben entstanden in nur etwa 100 Zonen, die jeweils nur wenige Kilometer groß waren. Der Grund für diese Konzentration ist noch nicht bekannt.

Durch ungewöhnliche Einflüsse auf die Bahnen der Lunar-Orbiter-Missionen erhielt man Ende der 1960er Jahre erste Hinweise auf Schwereanomalien, die man Mascons ("Mass concentrations", Massenkonzentrationen) nannte. Durch Lunar Prospector wurden diese Anomalien näher untersucht, sie befinden sich meist im Zentrum der Krater und sind vermutlich durch die Einschläge entstanden. Möglicherweise handelt es sich um die eisenreichen Kerne der Impaktoren, die aufgrund der fortschreitenden Abkühlung des Mondes nicht mehr bis zum Kern absinken konnten. Nach einer anderen Theorie könnte es sich um Lavablasen handeln, die als Folge eines Einschlags aus dem Mantel aufgestiegen sind.

Der Mond besitzt nur einen sehr geringen Hauch von Atmosphäre. Deshalb schlagen bis heute ständig Meteoroiden unterschiedlicher Größe, ohne abgebremst zu werden, auf der Oberfläche ein, die das an der Mondoberfläche anstehende Krustengestein zertrümmert, ja regelrecht pulverisiert haben. Der durch diesen Prozess entstehende Mondregolith (im Englischen auch teilweise als "lunar soil", „Monderde“, bezeichnet) bedeckt weitreichende Areale der Mondoberfläche mit einer mehrere Meter dicken Schicht, unter der Details der ursprünglichen Geologie des Mondes verborgen sind. Dies erschwert die Rekonstruktion der Entstehungsgeschichte des Monds erheblich.

Der Regolith entsteht hauptsächlich aus dem normalen Material der Oberfläche. Er enthält aber auch Beimengungen, die durch Einschläge an den Fundort transportiert wurden. Obwohl er gemeinhin als "Mondstaub" bezeichnet wird, entspricht der Regolith eher einer Sandschicht. Die Korngröße reicht von Staubkorngröße direkt an der Oberfläche über Sandkörner wenig tiefer bis hin zu Steinen und Felsen, die erst später hinzukamen und noch nicht vollständig zermahlen sind. Ein weiterer wichtiger Bestandteil sind glasige Erstarrungsprodukte von Einschlägen. Das sind einmal kleine Glaskugeln, die an Chondren erinnern, und zum anderen Agglutinite, das sind durch Glas verbackene Regolithkörner. An manchen Stellen besteht das Oberflächengestein des Mondes fast zur Hälfte aus diesen Agglutiniten. Diese entstehen, wenn die durch den Einschlag erzeugten Spritzer aus geschmolzenem Gestein erst nach dem Auftreffen auf die Regolithschicht erstarren.

Im Mondmeteoriten Dhofar 280, der im Jahr 2001 im Oman gefunden wurde, wurden neue Eisen-Silizium-Mineralphasen identifiziert. Eine dieser Mineralphasen (FeSi), die damit erstmals in der Natur eindeutig nachgewiesen wurde, ist nach dem Forscher Bruce Hapke als Hapkeit benannt worden. Bruce Hapke hatte in den 1970er Jahren die Entstehung derartiger Eisenverbindungen durch Weltraum-Erosion (englisch "Space Weathering") vorhergesagt. Weltraum-Erosion verändert auch die Reflexionseigenschaften des Materials und beeinflusst so die Albedo der Mondoberfläche.

Der Mond hat kein nennenswertes Magnetfeld, das heißt, die Teilchen des Sonnenwindes – vor allem Wasserstoff, Helium, Neon, Kohlenstoff und Stickstoff – treffen nahezu ungehindert auf der Mondoberfläche auf und werden im Regolith implantiert. Dies ähnelt der Ionenimplantation, die bei der Herstellung von integrierten Schaltungen angewandt wird. Auf diese Weise bildet der Mondregolith ein Archiv des Sonnenwindes, vergleichbar dem Eis in Grönland für das irdische Klima. Dazu kommt, dass kosmische Strahlung bis zu einen Meter tief in die Mondoberfläche eindringt und dort durch Kernreaktionen (hauptsächlich Spallationsreaktionen) instabile Nuklide bildet. Diese verwandeln sich mit verschiedenen Halbwertszeiten unter anderem durch Alphazerfall in stabile Nuklide. Da beim Alphazerfall jeweils ein Helium-Atomkern entsteht, enthalten Gesteine des Mondregoliths bedeutend mehr Helium als irdische Oberflächengesteine.

Da der Mondregolith durch Einschläge umgewälzt wird, haben die einzelnen Bestandteile meist eine komplexe Bestrahlungsgeschichte hinter sich. Man kann jedoch durch radiometrische Datierungsmethoden für Mondproben herausfinden, wann sie nahe der Oberfläche waren. Damit lassen sich Erkenntnisse über die kosmische Strahlung und den Sonnenwind zu diesen Zeitpunkten gewinnen.

Der Mond ist ein extrem trockener Körper. Jedoch konnten Wissenschaftler mit Hilfe eines neuen Verfahrens im Sommer 2008 winzige Spuren von Wasser (bis zu 0,0046 %) in kleinen Glaskügelchen vulkanischen Ursprungs in Apollo-Proben nachweisen. Diese Entdeckung deutet darauf hin, dass bei der gewaltigen Kollision, durch die der Mond entstand, nicht das ganze Wasser verdampft ist.

Erstmals hat 1998 die Lunar-Prospector-Sonde Hinweise auf Wassereis in den Kratern der Polarregionen des Mondes gefunden, dies wird aus dem Energiespektrum des Neutronenflusses evident. Dieses Wasser könnte aus Kometenabstürzen stammen. Da die polaren Krater aufgrund der geringen Neigung der Mondachse gegen die Ekliptik niemals direkt von der Sonne bestrahlt werden und somit das Wasser dort nicht verdampfen kann, könnte es sein, dass dort noch im Regolith gebundenes Wassereis vorhanden ist. Der Versuch, durch den gezielten Absturz des Prospectors in einen dieser Polarkrater einen eindeutigen Nachweis zu erhalten, schlug allerdings fehl.

Im September 2009 entdeckte die indische Sonde Chandrayaan-1 Hinweise auf größere Wassermengen auf dem Mond.

Am 13. November 2009 bestätigte die NASA, dass die Daten der LCROSS-Mission auf größere Wasservorkommen auf dem Mond schließen lassen.

Im März 2010 gab der United States Geological Survey bekannt, dass bei erneuten Untersuchungen der Apollo-Proben mit der neuen Methode der Sekundärionen-Massenspektrometrie bis zu 0,6 % Wasser gefunden wurden. Das Wasser weist ein Wasserstoffisotopenverhältnis auf, welches deutlich von den Werten irdischen Wassers abweicht.

Im Oktober 2010 ergab eine weitere Auswertung der LCROSS- und LRO-Daten, dass viel mehr Wasser auf dem Mond vorhanden ist als früher angenommen, die Sonde Chandrayaan-1 fand allein am Nordpol des Mondes Hinweise auf mindestens 600 Millionen Tonnen Wassereis. Auch wurden Hydroxylionen, Kohlenmonoxid, Kohlendioxid, Ammoniak, freies Natrium und Spuren von Silber detektiert.

Wasser(eis) überdauert oberflächennah am längsten an den Polen des Mondes, da diese am wenigsten vom Sonnenlicht beschienen und erwärmt werden, und besonders in der Tiefe von Kratern. Durch Untersuchung mit Neutronenspektrometern im Orbit fanden Matthew Siegler et al. die höchsten Konzentrationen von Wasserstoff (wahrscheinlich in Form von Wassereis) etwas abseits der aktuellen Pole an zwei Stellen, die sich diametral gegenüberliegen. Sie leiten daraus die Hypothese ab, dass – etwa durch vulkanische Massenverschiebung – sich die Polachse (um insgesamt 45°/ effektiv 25°) verschoben hat.

Die Mondoberfläche misst 38 Mio. km und ist damit etwa 15 % größer als die Fläche von Afrika mit der arabischen Halbinsel. Sie ist nahezu vollständig von einer trockenen, aschgrauen Staubschicht, dem Regolith, bedeckt. Des Mondes redensartlicher „Silberglanz“ wird einem irdischen Beobachter nur durch den Kontrast zum Nachthimmel vorgetäuscht. Tatsächlich hat der Mond sogar eine relativ geringe Albedo (Rückstrahlfähigkeit), was erst deutlich wird, wenn er von außen betrachtet wird, wie vom Deep Space Climate Observatory (DSCOVR, siehe Foto, das die sogar noch hellere Mondrückseite zeigt).

Die Mondoberfläche gegliedert sich in "Terrae" und "Maria". Die "Terrae" sind ausgedehnte Hochländer und die "Maria" sind große Beckenstrukturen, die von Gebirgszügen gerahmt sind und in denen sich weite Ebenen aus erstarrter Lava befinden. Sowohl die Maria als auch die Terrae sind übersät von Kratern. Zudem gibt es zahlreiche Gräben und Rillen sowie flache Dome, jedoch keine aktive Plattentektonik wie auf der Erde. Auf dem Mond ragt der höchste Gipfel 16 km über den Boden der tiefsten Senke, was rund 4 km weniger sind als auf der Erde (Ozeanbecken einbegriffen).

Die erdnahe Mondseite wird von den meisten und größten Maria geprägt. Die Maria sind dunkle Tiefebenen, die insgesamt 16,9 % der Mondoberfläche bedecken. Sie bedecken 31,2 % der Vorderseite, und nur 2,6 % der Rückseite. Die meisten Maria gruppieren sich auffällig in der Nordhälfte der erdnahen Seite und bilden das volkstümlich sogenannte „Mondgesicht“. Die dunklen Maria hielt man in der Frühzeit der Mondforschung für Meere; deshalb werden sie nach Giovanni Riccioli als "Maria" (Singular: "Mare") dem lateinischen Wort für Meere bezeichnet.

Die Maria sind erstarrte basaltische Lavadecken im Inneren ausgedehnter kreisförmiger Becken und unregelmäßiger Einsenkungen. Die Vertiefungen sind vermutlich durch große Einschläge in der Mondfrühphase entstanden. Da in der Frühphase der Mondmantel noch sehr heiß und daher magmatisch aktiv war, wurden diese Einschlagsbecken anschließend von aufsteigendem Magma bzw. Lava gefüllt. Dies wurde vermutlich durch die geringere Krustendicke der erdnahen Mondseite, im Vergleich zur erdfernen Mondseite, stark begünstigt. Allerdings ist der ausgedehnte Vulkanismus der Mondvorderseite wahrscheinlich noch von weiteren Faktoren begünstigt worden (siehe KREEP). Die Maria haben nur wenige große Krater, und außerhalb der Krater variieren ihre Höhen nur um maximal 100 m. Zu diesen kleinen Erhebungen gehören die "Dorsa". Die Dorsa wölben sich als Rücken flach auf und erstrecken sich über mehrere dutzend Kilometer. Die Maria sind von einer 2 bis 8 m dicken Regolithschicht bedeckt, die reich an Eisen und Magnesium ist. "(Siehe auch: Liste der Ebenen des Erdmondes)"

Die Maria wurden mit Proben ihrer dunklen Basalte radiometrisch auf 3,1 bis 3,8 Milliarden Jahre datiert. Das jüngste vulkanische Mondgestein ist ein in Afrika gefundener Meteorit mit KREEP-Signatur, der ca. 2,8 Milliarden Jahre alt ist. Dazu passt jedoch die Kraterdichte in den Maria nicht, die auf ein teilweise deutlich geringeres geologisches Alter der Maria von „nur“ 1,2 Milliarden Jahren hinweist.

Nach Auswertung von Aufnahmen und Oberflächendaten des Lunar Reconnaissance Orbiters stellte ein Team von Wissenschaftlern der Arizona State University und der Westfälischen Wilhelms-Universität Münster im Oktober 2014 die These auf, dass es noch vor deutlich weniger als 100 Millionen Jahren weit verbreitet vulkanische Aktivität auf dem Mond gegeben haben könnte. Innerhalb der großen Maria existieren demnach zahlreiche kleinere Strukturen mit Abmessungen zwischen 100 m und 5 km, die als "Irregular Mare Patches" bezeichnet und als lokale Lavadecken gedeutet werden. Die geringe Größe und Dichte der Einschlagskrater in diesen „Patches“ (dt. „Flecken“ oder „Flicken“) deuten darauf hin, dass sie für Mondverhältnisse sehr jung sind, bisweilen kaum mehr als 10 Millionen Jahre. Eine dieser Strukturen namens „Ina“ war bereits seit der Apollo-15-Mission bekannt, wurde jedoch bislang als Sonderfall mit geringer Aussagekraft für die geologische Geschichte des Mondes betrachtet. Die nun festgestellte Häufigkeit der "Irregular Mare Patches" lässt den Schluss zu, dass die vulkanische Aktivität auf dem Mond nicht, wie bisher angenommen, vor etwa einer Milliarde Jahren „abrupt“ endete, sondern langsam über einen langen Zeitraum schwächer wurde, was unter anderem die bisherigen Modelle zu den Temperaturen im Mondinneren in Frage stellt.

Die Hochländer wurden früher als Kontinente angesehen und werden deshalb als "Terrae" bezeichnet. Sie weisen deutlich mehr und auch größere Krater als die Maria auf und werden von einer bis zu 15 m dicken Regolithschicht bedeckt, die reich an hellem aluminiumreichen Anorthosit ist. Die ältesten Hochland-Anorthositproben sind radiometrisch mit Hilfe der Samarium-Neodym-Methode auf ein Kristallisationsalter von 4,456 ± 0,04 Milliarden Jahren datiert worden, was als Bildungsalter der ersten Kruste und als Beginn der Kristallisation des ursprünglichen Magmaozeans interpretiert wird. Die jüngsten Anorthosite sind etwa 3,8 Milliarden Jahre alt.

Die Hochländer sind von sogenannten Tälern (Vallis) durchzogen. Dabei handelt es sich um bis zu einige hundert Kilometer lange, schmale Einsenkungen innerhalb der Hochländer. Ihre Breite beträgt oft wenige Kilometer, ihre Tiefe einige hundert Meter. Die Mondtäler sind in den meisten Fällen nach in der Nähe gelegenen Kratern benannt "(Siehe auch: Liste der Täler des Erdmondes)".

In den Hochländern gibt es mehrere Gebirge, die Höhen von etwa 10 km erreichen. Sie sind möglicherweise dadurch entstanden, dass der Mond infolge der Abkühlung geschrumpft ist und sich dadurch Faltengebirge aufwölbten. Nach einer anderen Erklärung könnte es sich um die Überreste von Kraterwällen handeln. Sie sind nach irdischen Gebirgen benannt worden, zum Beispiel Alpen, Apenninen, Kaukasus und Karpaten "(siehe auch: Liste der Berge und Gebirge des Erdmondes)".

Die Mondkrater entstanden durch Einschläge kosmischer Objekte. Sie gehören deshalb zu den sogenannten Impaktkratern. Die größten von ihnen entstanden vor etwa 3 bis 4,5 Milliarden Jahren in der Frühzeit des Mondes durch Einschläge großer Asteroiden. Der Nomenklatur von Riccioli folgend, werden sie vorzugsweise nach Astronomen, Philosophen und anderen Gelehrten benannt. Einige der großen Krater sind von sternförmigen Strahlensystemen umgeben. Diese Strahlen stehen in unmittelbarem Zusammenhang mit dem Einschlag, der auch zur Entstehung des entsprechenden Kraters führte: Es handelt sich um Auswurfmaterial (sogenannte "Ejecta"), das in Form zahlreicher Glaskügelchen erstarrt ist. Diese streuen das Licht bevorzugt in die Einfallsrichtung zurück, wodurch sich die Strahlen bei Vollmond hell vom dunkleren Regolith abheben. Besonders lang und auffällig sind die Strahlen des Kraters Tycho.

Das Größenspektrum der Einschlagskrater auf dem Mond reicht von 2240 km Durchmesser, wie im Fall des Südpol-Aitken-Beckens, bis hin zu Mikrokratern, die erst unter dem Mikroskop sichtbar werden. Mit irdischen Teleskopen kann man allein auf der Vorderseite mehr als 40.000 Krater mit einem Durchmesser von mehr als 100 m unterscheiden. Die Oberfläche der Rückseite des Mondes weist aufgrund ihres höheren durchschnittlichen geologischen Alters eine noch deutlich höhere Kraterdichte auf "(siehe auch: Liste der Krater des Erdmondes)".

Vulkanische Krater sind bislang noch nicht zweifelsfrei identifiziert worden. Da die Mondkruste einen geringeren SiO-Anteil hat als die kontinentale Erdkruste, haben sich dort keine Schichtvulkane gebildet, wie sie z. B. für den pazifischen Feuerring auf der Erde typisch sind. Aber auch Schildvulkane mit zentraler Caldera, wie sie in den Ozeanbecken der Erde oder auf dem Mars vorkommen, scheint es auf dem Mond nicht zu geben. Stattdessen fand lunarer Vulkanismus offenbar überwiegend in Form von Spalteneruptionen statt.

Auf der Mondoberfläche gibt es auch Rillenstrukturen (Rimae), über deren Ursprung vor dem Apollo-Programm lange spekuliert worden war. Man unterscheidet

Seit den Untersuchungen der Hadley-Rille durch Apollo 15 geht man davon aus, dass es sich bei den mäandrierenden Rillen um Lavaröhren handelt, deren Decke eingestürzt ist. Hochauflösende Satellitenfotos sowie doppelte Radarechos von der Mondoberfläche in den Marius Hills (Oceanus-Procellarum-Becken), wo zudem eine negative Schwereanomalie registriert wurde, lassen es als sehr wahrscheinlich erscheinen, dass es auch heute noch ausgedehnte intakte Lavaröhrensysteme gibt.

Die Entstehung der geraden Rillen ist deutlich unklarer – es könnte sich um Schrumpfungsrisse handeln, die sich in erkaltender Lava gebildet haben.

Neben den als Rimae bezeichneten Strukturen bestehen noch schmale, vertiefte Strukturen, die eine Länge bis über 400 km erreichen. Sie ähneln den langgestreckten Rillen und werden als Furchen oder Risse (Rupes) bezeichnet. Diese Furchen gelten als Beweis für das Wirken von Spannungskräften innerhalb der Mondkruste.

Über die Rückseite des Mondes war vor den ersten Raumfahrtmissionen nichts bekannt, da sie von der Erde nicht sichtbar ist, erst Lunik 3 lieferte die ersten Bilder. Sie unterscheidet sich in mehreren Aspekten von der Vorderseite. Ihre Oberfläche prägen fast nur kraterreiche Hochländer; dazu zählt auch das große Südpol-Aitken-Becken, ein 13 km tiefer Krater mit 2240 km Durchmesser, der von vielen anderen Kratern überzeichnet ist. Untersuchungen der Clementine-Mission und des Lunar Prospector legen die Vermutung nahe, dass hier ein sehr großer Einschlagkörper die Mondkruste durchstoßen und möglicherweise Mantelgesteine freigelegt hat. Die Kruste der Rückseite ist mit 150 km gegenüber 70 km der Vorderseite etwa doppelt so dick. Die Raumsonde LRO entdeckte auch Grabenstrukturen auf der Mond-Rückseite. Am Rande des Engelhardt-Kraters konnte mit dem Laser-Altimeter der Raumsonde Kaguya der höchste bekannte Punkt (10.750 m) auf dem Erdmond gemessen werden.

Die beiden Hemisphären haben sich auch unterschiedlich entwickelt, weil das geometrische Mondzentrum (Mittelpunkt der volumsgleichen Kugel) und sein Schwerpunkt um 1,8 km (1 Promille des Mondradius) voneinander abstehen. Diese Asymmetrie von innerem Aufbau und Mondkruste könnte von einer Kollision mit einem zweiten Erdtrabanten herrühren, die einige Forscher in der Frühzeit des Mondes annehmen.

Die erhalten gebliebene Redensart von der „dunklen Seite des Mondes“ (englisch "dark side of the Moon") für die erdabgewandte Mondseite ist nur symbolisch im Sinne einer unbekannten Seite zu verstehen; im eigentlichen Wortsinn ist die Redensart falsch, da – wie schon zu den Mondphasen angemerkt – Vorder- und Rückseite im Laufe der Mondrotation abwechselnd von der Sonne beschienen werden. Durch den viel geringeren Flächenanteil der dunklen Mareebenen ist die erdabgewandte Mondseite insgesamt sogar deutlich heller als die erdzugewandte.

Die Gravitation des Mondes treibt auf der Erde die Gezeiten an. Dazu gehören nicht nur Ebbe und Flut in den Meeren, sondern auch Hebungen und Senkungen des Erdmantels. Die durch die Gezeiten frei werdende Energie wird der Drehbewegung der Erde entnommen und der darin enthaltene Drehimpuls dem Bahndrehimpuls des Mondes zugeführt. Dadurch verlängert sich gegenwärtig die Tageslänge um etwa 20 Mikrosekunden pro Jahr. In ferner Zukunft wird die Erdrotation an den Mondumlauf gebunden sein, und die Erde wird dem Mond immer dieselbe Seite zuwenden.

Die Erde ist nicht perfekt kugelförmig, sondern durch die Rotation abgeflacht. Die Gezeitenkraft von Sonne und Mond erzeugt ein aufrichtendes Drehmoment, das zweimal jährlich bzw. monatlich maximal wird. Die Erde folgt diesem als Kreisel nicht direkt, sondern präzediert mit in erster Näherung konstanter Neigung der Erdachse. Wäre die Sonne die einzige Ursache für Präzession, würde sich die Neigung der Erdachse innerhalb von Millionen Jahren in weiten Bereichen ändern. Dies würde ungünstige Umweltbedingungen für das Leben auf der Erde bedeuten, weil die Polarnacht abwechselnd die gesamte Nord- bzw. Südhalbkugel erfassen würde. Die durch den Mond bewirkte schnelle Präzession stabilisiert die Neigung der Erdachse. So trägt der Mond zu dem das Leben begünstigenden Klima der Erde bei.

Nach dem Skeptic’s Dictionary habe keine ausgewertete wissenschaftliche Studie eine signifikante positive Korrelation zwischen Mondphasen und dem Auftreten von Schlafstörungen, Verkehrsunfällen, Operationskomplikationen, der Häufigkeit von Suizidhandlungen oder der Häufigkeit von Geburten ergeben.
Manche Menschen, z. B. in der Land- und Forstwirtschaft, achten seit alters her darauf, dass bestimmte Arbeiten in der Natur in der „richtigen“ Mondphase erledigt werden ("siehe auch:" Mondholz, Mondkalender).

Die tägliche Bewegung des Mondes und die darin enthaltene Information über die Himmelsrichtungen wird von Zugvögeln und einigen Arten nachtaktiver Insekten zur Navigation genutzt. Bei manchen Arten der Ringelwürmer (wie bei dem Samoa-Palolo), Krabben und Fische ("Leuresthes") ist das Fortpflanzungsverhalten sehr eng an den monatlichen Phasenwechsel des Mondes gekoppelt.

Die schon im 18. Jahrhundert erforschte Korrelation von Mondposition und Wetter ist so gering, dass ein dadurch verursachter Einfluss auf Lebewesen vollständig vernachlässigt werden kann.

Das Schlafwandeln von Menschen wird irreführend als "Mondsüchtig-Sein" interpretiert.

Bei Nacht kann durch Zusammentreffen von Mondlicht und Regentropfen ein Mondregenbogen entstehen, der analog zum physikalischen Prinzip des Regenbogens der Sonne funktioniert.

Als "Mondhof" werden farbige Ringe um den Mond bezeichnet, die durch die Beugung des Lichts an den Wassertröpfchen der Wolken verursacht werden. Dabei ist der äußerste Ring von rötlicher Farbe und hat eine Ausdehnung von etwa zwei Grad, in seltenen Fällen auch bis zu zehn Grad.

Umgangssprachlich wird der Begriff des Mondhofs auch für einen Halo um den Mond gebraucht. Dafür sind Eiskristalle in Luftschichten verantwortlich, die aus dünnem Höhennebel oder Dunst entstanden sind und das auf die Erde fallende Licht in einem sehr schwachen Winkel ablenken und dadurch eine Art leuchtenden Ringeffekt für den Betrachter hervorrufen.

Eine spezielle Haloerscheinung des Mondes ist der Nebenmond. Analog zu den Nebensonnen treten Nebenmonde mit einem Abstand von rund 22 Grad neben dem Mond auf. Wegen der geringeren Lichtstärke des Mondes sieht man sie jedoch seltener und meistens bei Vollmond.

Als "Mondtäuschung" bezeichnet man den Effekt, dass der Mond in Horizontnähe größer aussieht als im Zenit. Dies ist keine Folge der Lichtbrechung an den Luftschichten, sondern eine optische Täuschung, die von der Wahrnehmungspsychologie untersucht und erklärt wird.

Auch das Phänomen, dass die beleuchtete Seite des Mondes oft nicht genau zur Sonne zu zeigen scheint, ist eine optische Täuschung und wird dort unter der Überschrift Relativität des Blickwinkels erläutert. Man kann sich davon überzeugen, dass die beleuchtete Mondsichel tatsächlich – wie zu erwarten – jederzeit senkrecht auf der Verbindungslinie zwischen Sonne und Mond steht, indem man diese Verbindungslinie durch eine mit ausgestreckten Armen – visiert – zwischen Sonne und Mond gespannte Schnur sichtbar macht.

Der Mond ist nach der Sonne das mit Abstand hellste Objekt des Himmels; zugleich kann man seinen einzigartigen Helligkeits- und Phasenwechsel zwischen Vollmond und Neumond auch mit bloßem Auge sehr gut beobachten. Das erste Auftauchen der Mondsichel am Abendhimmel („Neulicht“) markiert in einigen Kulturkreisen den Beginn des jeweiligen Monats.

Die Mondphasen und die Sonnen- bzw. Mondfinsternisse sind mit Sicherheit schon früh von Menschen beobachtet worden. Die genaue Länge des siderischen und des synodischen Monats war schon im 5. Jahrtausend v. Chr. bekannt, ebenso die Neigung der Mondbahn gegen die Ekliptik (5,2°). Mindestens 1000 v. Chr. kannten die babylonischen Astronomen die Bedingungen, unter denen Sonnenfinsternisse auftreten, und die Vorhersage der Sonnenfinsternis vom 28. Mai 585 v. Chr. durch Thales von Milet entschied 585 v. Chr. den Krieg zwischen den Lydern und Medern. Von Anaxagoras ist die Aussage überliefert, der Mond erhalte sein Licht von der Sonne, und es gebe auf ihm Täler und Schluchten; diese und andere Lehren trugen ihm eine Verurteilung wegen Gotteslästerung ein.

Die am Mond freiäugig erkennbaren Details (siehe "Mondgesicht") werden in anderen Kulturkreisen auch als "Hase" etc. bezeichnet. Die dunklen, scharf begrenzten Flächen wurden schon früh als Meere interpretiert (diese glatten Ebenen werden daher bis heute Mare genannt), während die Natur der bei Vollmond sichtbar werdenden Strahlensysteme erst im 20. Jahrhundert geklärt werden konnte.

Einige Jahrzehnte nach der Erfindung des Fernrohrs begann um 1650 die intensive Erforschung des Mondes. Frühe Höhepunkte der Selenografie waren die Arbeiten von Johann Hieronymus Schroeter, der 1791 seine "Selenotopografie" publizierte, die genaue Kartierung der Mondkrater und Gebirge sowie deren Benennung. Es folgte die Ära der hochpräzisen Mondkarten durch Beer, Mädler und andere, ab etwa 1880 die langbrennweitige Astrofotografie (siehe auch Pariser Mondatlas) und erste geologische Deutungen der Mondstrukturen. Das durch die Raumfahrt (erste Mondumkreisung 1959) gesteigerte Interesse am Mond führte zur erstmaligen Beobachtung leuchtender Gasaustritte durch Kosyrew, doch die Vulkanismus-Theorie der Mondkrater musste der Deutung als Einschlagkrater weichen. Vorläufiger Höhepunkt waren die bemannten Mondlandungen 1969–1972, die dadurch ermöglichten zentimetergenauen Laser-Entfernungsmessungen und in den letzten Jahren die multispektrale Fernerkundung der Mondoberfläche sowie die genaue Vermessung ihres Schwerefeldes durch verschiedene Mondorbiter.

Die älteste bekannte Darstellung des Mondes ist eine 5000 Jahre alte Mondkarte aus dem irischen Knowth. Als weitere historisch bedeutende Abbildung in Europa ist die Himmelsscheibe von Nebra zu nennen.

Das Steinmonument Stonehenge diente wahrscheinlich als Observatorium und war so gebaut, dass damit auch spezielle Positionen des Mondes vorhersagbar oder bestimmbar gewesen sind.

In vielen archäologisch untersuchten Kulturen gibt es Hinweise auf die große kultische Bedeutung des Mondes für die damaligen Menschen. Der Mond stellte meist eine zentrale Gottheit dar, als weibliche Göttin, zum Beispiel bei den Thrakern Bendis, bei den Ägyptern Isis, bei den Griechen Selene, Artemis und Hekate sowie bei den Römern Luna und Diana, oder als männlicher Gott wie beispielsweise bei den Sumerern Nanna, in Ägypten Thot, in Japan Tsukiyomi, bei den Azteken Tecciztecatl und bei den Germanen Mani. Fast immer wurden Sonne und Mond dabei als entgegengesetzt geschlechtlich gedacht, auch wenn die Zuordnung variierte. In China dagegen galt der Mond als Symbol für Westen, Herbst und Weiblichkeit (Yin).

Ein häufig vorkommendes Motiv ist das Bild von den drei Gesichtern der Mondgöttin: bei zunehmendem Mond die verführerische Jungfrau voller Sexualität, bei Vollmond die fruchtbare Mutter und bei abnehmendem Mond das alte Weib oder die Hexe mit der Kraft zu heilen, zum Beispiel bei den Griechen mit Artemis, Selene und Hekate sowie bei den Kelten Blodeuwedd, Morrígan und Ceridwen.

Der Mond als Himmelskörper ist Gegenstand von Romanen und Fiktionen, von Jules Vernes Doppelroman "Von der Erde zum Mond" und "Reise um den Mond" über Paul Linckes Operette "Frau Luna" oder Hergés zweibändigem Tim-und-Struppi-Comic-Abenteuer "Reiseziel Mond" und "Schritte auf dem Mond" bis hin zu der futuristischen Vorstellung einer Besiedelung des Mondes oder dem Reiseführer "Reisen zum Mond" von Werner Tiki Küstenmacher.

Neben der mythologischen Verehrung nutzten Menschen schon sehr früh den regelmäßigen und leicht überschaubaren Rhythmus des Mondes für die Beschreibung von Zeitspannen und als Basis eines Kalenders, noch heute basiert der islamische Kalender auf dem Mondjahr mit 354 Tagen (12 synodische Monate). Mit dem Übergang zum Ackerbau wurde die Bedeutung des Jahresverlaufs für Aussaat und Ernte wichtiger. Um dies zu berücksichtigen, wurden zunächst nach Bedarf, später nach feststehenden Formeln wie zum Beispiel dem metonischen Zyklus Schaltmonate eingefügt, die das Mondjahr mit dem Sonnenjahr synchronisierten. Auf diesem lunisolaren Schema basieren zum Beispiel der altgriechische und der jüdische Kalender.

Die noch heute gebräuchliche Länge einer Woche von sieben Tagen basiert wahrscheinlich auf der zeitlichen Folge der vier hauptsächlichen Mondphasen (siehe oben). Bei der Osterrechnung spielt das Mondalter am letzten Tag des Vorjahres eine Rolle und heißt Epakte.

Von den alten Hochkulturen hatten einzig die alten Ägypter ein reines Sonnenjahr mit zwölf Monaten à 30 Tage sowie fünf Schalttage, das heißt ohne strengen Bezug zum synodischen Monat von 29,5 Tagen, vermutlich, weil für die ägyptische Kultur die genaue Vorhersage der Nilüberschwemmungen und damit der Verlauf des Sonnenjahres überlebensnotwendig war.

Wissenschaftliche Teildisziplinen, die sich mit der Untersuchung des Mondes befassen, tragen nach dem griechischen Wort für Mond, Σελήνη (Selene) gebildete Namen. Es sind:

Die früheste grobe Mondkarte mit Konturen der Albedomerkmale und dem ersten Versuch einer Nomenklatur skizzierte William Gilbert im Jahre 1600 nach dem bloßen Auge. Die erste, wenn auch ebenfalls nur skizzenhafte Darstellung der mit einem Fernrohr sichtbaren Mondstrukturen stammt von Galileo Galilei (1609), die ersten brauchbaren stammen von Johannes Hevelius, der mit seinem Werk "Selenographia sive Lunae Descriptio" (1647) als Begründer der Selenografie gilt. In der Nomenklatur der Mondstrukturen setzte sich das System von Giovanni Riccioli durch, der in seinen Karten von 1651 die dunkleren Regionen als Meere ("Mare", Plural: "Maria") und die Krater nach Philosophen und Astronomen bezeichnete. Allgemein anerkannt ist dieses System jedoch erst seit dem 19. Jahrhundert.

Tausende Detailzeichnungen von Mondbergen, Kratern und Wallebenen wurden von Johann Hieronymus Schroeter (1778–1813) angefertigt, der auch viele Mondtäler und Rillen entdeckte. Den ersten Mondatlas gaben Wilhelm Beer und Johann Heinrich Mädler 1837 heraus, ihm folgte bald eine lange Reihe fotografischer Atlanten.

Ende des 19. Jahrhunderts konnten bereits Aussagen über die Erscheinung des Mondes getroffen werden, die auch heute noch weitestgehend Gültigkeit besitzen. Der österreichische Geologe Melchior Neumayr traf diesbezüglich folgende Aussage:
Allerdings war die tatsächliche Entstehung dieser Krater bis zu diesem Zeitpunkt noch ungewiss. Neumayr nahm infolgedessen den Vulkanismus als die wahrscheinlichste Ursache dafür an:
Neumayr gibt an, dass sich einzelne Gebirge mehr als 8000 m über ihre Umgebung erhöben. Die Höhenbestimmung von Kratern, Gebirgen und Ebenen war mit teleskopischen Beobachtungen jedoch sehr problematisch und erfolgte meist durch Analyse von Schattenlängen, wofür Josef Hopmann im 20. Jahrhundert Spezialmethoden entwickelte. Erst durch die Sondenkartierungen kennt man verlässliche Werte: Die Krater, mit Durchmessern bis zu 300 km, wirken zwar steil, sind aber nur wenige Grad geneigt, die höchsten Erhebungen hingegen erreichen eine Höhe von bis zu 10 km über dem mittleren Niveau.

Den zweiten großen Sprung der Fortschritte in der Mondforschung eröffnete dreieinhalb Jahrhunderte nach der Erfindung des Fernrohrs der Einsatz der ersten Mondsonden. Die sowjetische Sonde Lunik 1 kam dem Mond rund 6000 km nahe, Lunik 2 traf ihn schließlich und Lunik 3 lieferte die ersten Bilder von seiner Rückseite. Die Qualität der Karten wurde in den 1960er Jahren deutlich verbessert, als zur Vorbereitung des Apollo-Programms eine Kartierung durch die Lunar-Orbiter-Sonden aus einer Mondumlaufbahn heraus stattfand. Die heute genauesten Karten stammen aus den 1990ern durch die Clementine- und Lunar-Prospector-Missionen.

Das US-amerikanische Apollo- und das sowjetische Luna-Programm brachten mit neun Missionen zwischen 1969 und 1976 insgesamt 382 Kilogramm Mondgestein von der Mondvorderseite zur Erde; die folgende Tabelle gibt einen Überblick darüber.

1979 wurde der erste Mondmeteorit in der Antarktis entdeckt, dessen Herkunft vom Mond allerdings erst einige Jahre später durch Vergleiche mit den Mondproben erkannt wurde. Mittlerweile kennt man noch mehr als zwei Dutzend weitere. Diese bilden eine komplementäre Informationsquelle zu den Gesteinen, die durch die Mondmissionen zur Erde gebracht wurden: Während man bei den Apollo- und Lunaproben die genaue Herkunft kennt, dürften die Meteorite, trotz der Unkenntnis ihres genauen Herkunftsortes auf dem Mond, repräsentativer für die Mondoberfläche sein, da einige aus statistischen Gründen auch von der Rückseite des Mondes stammen sollten.

Der Mond ist nach der Erde bisher der einzige von Menschen betretene Himmelskörper. Im Rahmen des Kalten Kriegs unternahmen die USA und die UdSSR einen Wettlauf zum Mond (auch bekannt als „Wettlauf ins All“) und in den 1960er Jahren als Höhepunkt einen Anlauf zu bemannten Mondlandungen, die jedoch nur mit dem Apollo-Programm der Vereinigten Staaten verwirklicht wurden. Das bemannte Mondprogramm der Sowjetunion wurde daraufhin abgebrochen.

Am 21. Juli 1969 UTC setzte mit Neil Armstrong der erste von zwölf Astronauten im Rahmen des Apollo-Programms seinen Fuß auf den Mond. Nach sechs erfolgreichen Missionen wurde das Programm 1972 wegen der hohen Kosten eingestellt. Während des ausgehenden 20. Jahrhunderts wurde immer wieder über eine Rückkehr zum Mond und die Einrichtung einer ständigen Mondbasis spekuliert, aber erst durch Ankündigungen des damaligen US-Präsidenten George W. Bush und der NASA Anfang 2004 zeichneten sich konkretere Pläne ab. Am 4. Dezember 2006 hat die NASA ernsthafte Pläne für eine stufenweise Annäherung des Menschen an den Mond bekannt gegeben. Demnach sollten, nach ersten Testflügen ab 2009, schon 2019 wieder bemannte Missionen zum Mond führen. Ab 2020 sollten vier Astronauten 180 Tage lang auf dem Mond verweilen, bis dann ab 2024 eine permanent bemannte Mondbasis am lunaren Südpol errichtet werden sollte. Wegen der am Ende nicht einhaltbaren Fertigstellungstermine der Ares-Raketen sowie der unabsehbaren Kosten stellte die Regierung unter Präsident Barack Obama dem Programm keine finanziellen Mittel mehr zur Verfügung.

Die folgende Tabelle enthält die zwölf Männer, die den Mond betreten haben. Alle waren Bürger der USA.

Als bisher letzter Mensch verließ am 14. Dezember 1972 Eugene Cernan den Mond.

Nach einer Pause in der gesamten Mondraumfahrt von gut 13 Jahren startete am 24. Januar 1990 die japanische Experimentalsonde Hiten ohne wissenschaftliche Nutzlast. Sie setzte am 19. März desselben Jahres in einer Mondumlaufbahn die Tochtersonde Hagoromo aus, schwenkte am 15. Februar 1992 selbst in einen Mondorbit ein und schlug am 10. April 1993 auf den Mond auf.

Am 25. Januar 1994 startete die US-amerikanische Raumsonde Clementine zum Mond, um dort neue Geräte und Instrumente zu testen. Am 19. Februar 1994 erreichte sie eine polare Mondumlaufbahn und kartierte von dort aus etwa 95 % der Mondoberfläche. Neben den zahlreichen Fotografien lieferte sie Hinweise auf Vorkommen von Wassereis am lunaren Südpol. Im Mai desselben Jahres vereitelte eine fehlerhafte Triebwerkszündung den geplanten Weiterflug zum Asteroiden Geographos. Die Sonde ist seit Juni 1994 außer Betrieb.

Am 11. Januar 1998 erreichte die US-amerikanische Mondsonde Lunar Prospector eine polare Mondumlaufbahn, um an den Polen den Hinweisen auf Wassereis nachzuforschen. Zusätzlich maß sie auch das lunare Schwerefeld des Mondes für eine globale Schwerefeldkarte. Am 31. Juli 1999 endete die Mission mit einem geplanten Aufschlag in der Nähe des lunaren Südpols, um in der ausgeworfenen Partikelwolke von der Erde aus Wassereis nachweisen zu können; dieser Nachweis ist jedoch nicht gelungen.

Als erste Mondsonde der ESA testete SMART-1 neue Techniken und erreichte am 15. November 2004 eine Mondumlaufbahn. Von dort suchte sie nach Wassereis, fotografierte die Mondoberfläche und untersuchte hauptsächlich deren chemische Zusammensetzung. Die Sonde schlug planmäßig am 3. September 2006 auf dem Mond ein, was von der Erde aus beobachtet werden konnte.

Am 3. Oktober 2007 erreichte die japanische Sonde Kaguya den Mond und schwenkte in eine polare Umlaufbahn ein. Der hauptsächliche Orbiter hatte zwei Hilfssatelliten in einen jeweils eigenen Mondorbit ausgesetzt: Ein VRAD-Satellit diente erdgebundenen VLBI-Messungen und ein Relaissatellit sorgte für die Weiterleitung der Funksignale. Die Beobachtung des Mondes begann Mitte Dezember 2007 und endete am 10. Juni 2009 mit Kaguyas vorgesehenem Aufschlag.

Am 24. Oktober 2007 hatte die Volksrepublik China ihre erste Mondsonde Chang’e-1 gestartet. Chang’e-1 erreichte den Mond am 5. November, und umkreiste ihn über die Pole für etwa ein Jahr. Sie analysierte die Mondgesteine spektroskopisch und kartografierte die Mondoberfläche dreidimensional. Wobei auch erstmals eine umfassende Mikrowellenkarte des Mondes entstand, die auch Bodenschätze anzeigt. Chang’e-1 schlug am 1. März 2009 gezielt auf dem Mond auf ("siehe auch:" Mondprogramm der Volksrepublik China). Die ursprüngliche Ersatzsonde von Chang’e-1 wurde zur Nachfolgesonde Chang’e-2. Sie umkreiste den Mond vom 6. Oktober 2010 bis zum 9. Juni 2011. Sie vertiefte bis April 2011 den Erfolg der China National Space Administration und bereitete die weiche Landung für Chang’e-3 vor.

Der Start der indischen Mondsonde Chandrayaan-1, und damit der ersten Raumsonde Indiens, erfolgte am 22. Oktober 2008. Sie hat zu Beginn ihrer Mission am 14. November aus ihrer polaren Umlaufbahn einen Lander in der Nähe des lunaren Südpols hart aufschlagen lassen. Mit Instrumenten aus verschiedenen Ländern sollte unter anderem eine mineralogische, eine topografische und eine Höhenkarte des Mondes erstellt werden. Der Kontakt brach jedoch am 29. August 2009 vorzeitig ab. Die Mission sollte ursprünglich zwei Jahre dauern.

Ab dem 7. März 2012 umkreisten zwei am 10. September 2011 gestartete Orbiter der NASA unter der Bezeichnung Gravity Recovery and Interior Laboratory (GRAIL) den Mond, um gemeinsam sein Schwerefeld genauer zu vermessen. Die Mission endete am 17. Dezember 2012, und beide Orbiter schlugen kontrolliert auf der Mondoberfläche ein.

Am 6. September 2013 startete die NASA den Orbiter Lunar Atmosphere and Dust Environment Explorer (LADEE) als erste Mission des neuen Lunar-Quest-Programms, der die Atmosphäre und den Staub des Mondes näher untersuchte. Des Weiteren wurde die Mission genutzt, um einen Laser als neue Kommunikationsmöglichkeit anstelle von Radiowellen zu testen. Die Mission endete am 18. April 2014 nach einmaliger Verlängerung um einen Monat mit dem Aufschlag der Sonde auf der Mondoberfläche.

Am 23. Juni 2009 um 9:47 UTC schwenkte der Lunar Reconnaissance Orbiter (LRO) der NASA auf eine polare Umlaufbahn ein, um den Mond in einer Höhe von 50 km mindestens ein Jahr lang zu umkreisen und dabei Daten für die Vorbereitung zukünftiger Landemissionen zu gewinnen. Die Geräte der US-amerikanischen Sonde liefern die Basis für hochaufgelöste Karten der gesamten Mondoberfläche (Topografie, Orthofotos mit 50 cm Auflösung, Indikatoren für Vorkommen von Wassereis) und Daten zur kosmischen Strahlenbelastung. Es wurden 5185 Krater mit einem Durchmesser von mindestens 20 km erfasst. Aus deren Verteilung und Alter wurde geschlossen, dass bis vor 3,8 Milliarden Jahren hauptsächlich größere Brocken den Mond trafen, danach vorwiegend kleinere. Die Raumsonde LRO entdeckte auch Grabenstrukturen auf der Mond-Rückseite. Wann die Mission enden soll, ist noch nicht bekannt.
Mit derselben Trägerrakete wurde auch der Lunar Crater Observation and Sensing Satellite (LCROSS) zum Mond geschickt. Er schlug am 9. Oktober im Krater Cabeus nahe dem Südpol ein. Der Satellit bestand aus zwei Teilen, der ausgebrannten Oberstufe der Rakete, die einen Krater erzeugte, und der einige Zeit vor dem Einschlag abgekoppelten Geräteeinheit, die die aufgeworfene Partikelwolke insbesondere in Hinsicht auf Wassereis analysierte, bevor sie vier Minuten später ebenfalls aufschlug.

Am 14. Dezember 2013 hat die chinesische Raumfahrtagentur mit Chang’e-3 ihre erste weiche Mondlandung durchgeführt. Die über 3,7 Tonnen schwere Sonde dient u. a. dem Transport eines 120 kg schweren Mondrovers, der seine Energie aus Radioisotopengeneratoren erhält oder mit Radionuklid-Heizelementen ausgestattet ist, um während der 14-tägigen Mondnacht nicht einzufrieren.

Im Jahr 2017 ist innerhalb des Mondprogramms der Volksrepublik China die Rückkehrmission Chang’e-5 geplant. Eine Raumsonde soll dabei 2 kg Mondgestein mit zur Erde bringen. Auch im Jahre 2020 soll eine solche Probenmission, Chang’e-6, durchgeführt werden, um Material vom Mond zur Erde zu bringen.

Die Japan Aerospace Exploration Agency (JAXA) hat für 2017 mit SELENE-2 (Abkürzung für "Selenological and Engineering Explorer 2"; wörtlich "Mondstudierender und technischer Erforscher 2") eine Nachfolgemission von Kaguya (SELENE-1) vorgesehen, die aus einem Orbiter, der vor allem als Datenrelais dienen soll, einem Lander und einem Rover bestehen soll. Lander und Rover sollen dabei zwei Wochen lang aktiv sein. Die Mission soll als Vorbereitung für eine bemannte Mondlandung in Zusammenarbeit mit der NASA dienen.

Indiens Raumfahrtagentur ISRO plant als Nachfolger von Chandrayaan-1 für 2017 mit Chandrayaan-2 ein Landegerät, das mit einem Rover weich aufsetzen soll.

Die NASA hat für Januar 2019 den Lunar Flashlight geplant, eine Mondsonden-Mission zur Suche und Untersuchung von Wassereisvorkommen auf dem Mond, um dies für Menschen, künftige Mondstationen und Mondrobotern nutzen zu können.

Für das Jahr 2024 ist von Seiten Russlands der Einsatz der Mondsonde Luna 25 geplant. Sie soll zwölf Penetratoren hauptsächlich für seismische Untersuchungen absetzen und einen Lander zur Suche nach Wassereis in einem Krater in Nähe des lunaren Südpols niedergehen lassen. Weitere Mondmissionen Luna 26 bis Luna 29 sind ebenfalls bereits in Planung. Diese Mondsonden sind Teil der Errichtung einer Station zur Kolonisation des Mondes von 2020 bis 2037.

Derzeit sehen sich private Teams für 2017 bereit, um den Google Lunar X-Prize anzutreten, hierunter sind die Astrobotic Technology, Barcelona Moon Team und Moon Express. Hierbei geht es um die Förderung von geplanten Raumflügen durch Google Inc. mit einem Preisgeld von insgesamt 30 Millionen US-Dollar.

Der Weltraumvertrag "(Outer Space Treaty)" von 1967 verbietet Staaten, einen Eigentumsanspruch auf Weltraumkörper wie den Mond zu erheben. Dieses Abkommen wurde bis heute von 192 Staaten der Vereinten Nationen ratifiziert und ist damit in Kraft. Da im Outer-Space-Treaty-Abkommen nur von Staaten die Rede ist, wird von manchen interpretiert, dass dieses Abkommen nicht für Firmen oder Privatpersonen gelte. 1979 wurde deshalb der Mondvertrag "(Agreement Governing the Activities of States on the Moon and Other Celestial Bodies)" entworfen, um diese vom "Outer Space Treaty" hinterlassene angebliche Gesetzeslücke zu schließen. Der „Moon-Treaty“-Entwurf hatte explizit die Besitzansprüche von Firmen und Privatpersonen adressiert und ausgeschlossen (Artikel 11, Absatz 2 und 3). Aus diesem Grund wird das „Moon Treaty“ oft als Hindernis für Grundstücksverkäufe zitiert; nur wurde dieses Abkommen tatsächlich nie unterschrieben oder in den Vereinten Nationen korrekt ratifiziert. Nur fünf Staaten, die alle nicht weltraumgängig sind, haben versucht, es zu ratifizieren. 187 andere Staaten sowie die USA, Russland und China haben es nicht unterschrieben und auch nicht ratifiziert. Das „Moon Treaty“ ist deshalb heute in den meisten Ländern der Erde nicht in Kraft. Die wählenden Staaten hatten damals zu viele Bedenken, dass es die profitable Nutzung des Mondes gefährden könnte, und somit wurde das Abkommen auch nicht ratifiziert (und deshalb nicht Gesetz). Daraus schlussfolgern einige, dass eine Rechtsgrundlage für Mond-Grundstücksverkäufe existiere. Es sollte ebenfalls darauf hingewiesen werden, dass die Internationale Astronomische Union sich nicht mit dem Verkauf von Himmelskörpern befasst.

Der Amerikaner Dennis M. Hope meldete 1980 beim Grundstücksamt von San Francisco seine Besitzansprüche auf den Mond an. Da niemand in der nach amerikanischem Recht ausgesetzten Frist von acht Jahren Einspruch erhob und da das Outer-Space-Treaty-Abkommen solche Verkäufe durch Privatpersonen in den USA explizit nicht verbietet, vertreibt Hope die Grundstücke über seine dafür gegründete Lunar Embassy. Da allerdings das Grundstücksamt in San Francisco für Himmelskörper nicht zuständig ist und von Hope sowohl das Gesetz, das solche Besitzansprüche regelt, als auch der Text aus dem Outer Space Treaty sehr abenteuerlich interpretiert wurden, sind die „Grundstückszertifikate“, die er verkauft, praktisch wertlos.

Weitere Erdtrabanten sind Gegenstand von unbestätigten Beobachtungsbehauptungen oder von Hypothesen für vergangene Zeitabschnitte wie die Zeit der Entstehung des Mondes.

In den Librationspunkten L4 und L5 des Erde-Mond-Systems soll es zwei Staubwolken geben.

Am 13. September 2007 haben die X-Prize Foundation und Google Inc. den Google Lunar X Prize ausgeschrieben, um damit private Investitionen in die unbemannte Raumfahrt anzuregen. Das ausgelobte Preisgeld beträgt insgesamt 30 Millionen US-Dollar. Die Fristen für die Umsetzung wurden mehrfach verlängert. Von den zahlreichen Teams haben bisher (Stand November 2016) drei die Bedingung erfüllt, bis Ende 2016 einen Startkontrakt vorzulegen. Die Missionen müssen bis Ende 2017 abgeschlossen sein.

Die Errichtung von dauerhaften Außenposten und Kolonien auf dem Mond ist bereits vor der Erfindung der Raumfahrt diskutiert worden und spielt nach wie vor in der Science-Fiction-Literatur eine Rolle. Eine NASA-Studie zum Bergbau auf dem Mond listete 1979 die dafür notwendige Technologieentwicklung auf.

Der Mond könnte auch Hinweise für die Suche nach außerirdischen Zivilisationen liefern. Wissenschaftler wie Paul Davies halten eine Suche nach Artefakten und Überresten extraterrestrischer Technologie auf der lunaren Oberfläche für förderlich.

Möglicherweise befanden sich in dem durch die Apollo-12-Mission geborgenen Kameragehäuse der Sonde Surveyor 3 Mikroben 31 Monate lang auf dem Erdtrabanten und waren danach zur Vermehrung fähig. Für Details und Zweifel siehe Vorwärts-Kontamination.






</doc>
<doc id="1325" url="https://de.wikipedia.org/wiki?curid=1325" title="Elfenbeinküste">
Elfenbeinküste

Die Elfenbeinküste (amtlicher Name: , []) ist ein Staat in Westafrika. Er grenzt an Liberia, Guinea, Mali, Burkina Faso und Ghana und im Süden an den Atlantischen Ozean.

Das Land, das am 7. August 1960 die Unabhängigkeit von Frankreich erlangte, war jahrzehntelang politisch stabil und wurde durch die Einheitspartei PDCI (Parti Démocratique de Côte d’Ivoire) des damaligen Präsidenten Houphouët-Boigny regiert. Exporterlöse aus Kakao und Kaffee garantierten einen relativen Wohlstand. Innere Spannungen führten 1990 zum Ende der PDCI-Herrschaft. Mit zunehmenden wirtschaftlichen Schwierigkeiten durch den Verfall der Kakaopreise eskalierten die Konflikte und führten zu einem bürgerkriegsähnlichen Zustand, der das Land 2002 in zwei Teile zerriss. Seit dem Friedensvertrag von 2007 wird an der Versöhnung und Wiedervereinigung der Elfenbeinküste gearbeitet. Seitdem zählt es wieder zu den am schnellsten wachsenden Wirtschaften Afrikas. Im Entwicklungsindex der Vereinten Nationen steht die Elfenbeinküste im Jahr 2016 auf Platz 171 von 188.

Seit 1983 ist Yamoussoukro die offizielle Hauptstadt. Der Regierungssitz befindet sich in der früheren Hauptstadt Abidjan, die auch weiterhin das wirtschaftliche und politische Zentrum des Landes darstellt.

"Côte d’Ivoire" [] ist Französisch und bedeutet auf Deutsch „Elfenbeinküste“. Der Name rührt her von der Jagd auf die im Lande heimischen Elefanten wegen des Elfenbeins ihrer Stoßzähne, das lange das wichtigste Exportprodukt des Landes war.

Da die unterschiedlichen Versionen des Landesnamens in verschiedenen Sprachen (Elfenbeinküste, Ivory Coast, Costa de Marfil, Costa d'Avorio etc.) in internationalen Foren zuvor häufig für Verwirrung gesorgt hatten, verfügte Präsident Houphouët-Boigny Ende 1985, dass der Landesname nur noch mit dem französischen Namen Côte d’Ivoire geführt und nicht in andere Sprachen übersetzt werden darf. Im deutschsprachigen Raum ist die altherkömmliche Bezeichnung "Elfenbeinküste" im Sprachgebrauch und in den Medien jedoch nach wie vor stärker verbreitet als der französische Ausdruck. Im offiziellen Verkehr, beispielsweise der deutschen Bundesregierung oder des EDA wird jedoch der offizielle Name "Côte d’Ivoire" verwendet. Im Lande selbst ist die Benutzung einer anderen als der offiziellen Bezeichnung unter Strafe verboten.

Die Bezeichnung für einen Einwohner der Elfenbeinküste ist – gemäß StAGN – Ivorer oder Ivorerin (eingedeutschte Version von franz. "Ivoirien" und "Ivoirienne"). Ein zu „Elfenbeinküste“ korrespondierendes Adjektiv existiert nicht, sodass man auch hier aus dem Französischen das Wort „ivorisch“ ableitet, letzteres ist die offizielle Regelung im EDA.

Der Süden der Elfenbeinküste hat eine 515 Kilometer lange Küstenlinie am Atlantischen Ozean, am westlichen Ende des Golfs von Guinea. Die Länge der Landesgrenzen zu den Nachbarstaaten sind: Burkina Faso 545 km, Ghana 720 km, Guinea 816 km, Liberia 778 km und Mali 599 km. Das Relief ist eher flach, das Oberflächenprofil ist von Ebenen und Hochebenen gekennzeichnet. Allein der Westen des Landes weist Höhen von mehr als 1.000 Metern über dem Meeresspiegel auf. Hier, genau auf der Grenze zu Guinea, befindet sich der Mont Nimba, der mit der höchste Berg beider Staaten ist. Der Norden des Landes wird auch von einem Teil der Oberguineaschwelle durchzogen. Davon abgesehen sind die restlichen Ebenen zwischen 200 und 350 Meter hoch gelegen.

Die höheren Plateaus haben schroffe Formen und sind aus hartem Material. Die niedriger gelegenen Ebenen haben sanftere Formen und sind in der Regel aus lockererem Material. Weiträumige, platte Gegenden charakterisieren die Savannenlandschaften wie auch die kleinen Savanneneinschlüsse in den Regenwaldgebieten. Das dominierende Element der Ebenen und Hochebenen ist eine eisenhaltige Kruste, die an der Oberfläche als rostfarbene Platten sichtbar ist, häufig jedoch von Sand, Kies oder feinerem Material verdeckt wird.

Gewässer bedecken 4460 km² oder 1,383 % des Territoriums der Elfenbeinküste. Dies sind einerseits der Atlantische Ozean und die angrenzenden Lagunen im Süden des Landes, wobei die bedeutendsten Lagunenkomplexe Aby-Tendo-Ehy, Ebrié und Grand-Lahou-Tadio-Makey-Tagba sind. Es gibt zahlreiche Fließgewässer, die das ganze Land entwässern. Vor allen sind hier die vier großen Flüsse Cavally (700 km), Sassandra (650 km), Bandama (1050 km) und Comoé (1160 km) zu nennen. Andere wichtige Flüsse sind entweder Nebenflüsse davon oder sie sind Küstenflüsse, die ihre eigenen Einzugsgebiete haben. Nennenswert sind der Tabou, der Néro, der San Pedro, der Bolo, der Niouniourou, der Boubo, der Agnéby, die Mé und die Bia. Die größten Seen des Landes sind Talsperren: Der Kossoustausee, der Buyostausee und der Ayaméstausee. Schließlich gibt es zahlreiche Bäche und mehrere Sumpfgebiete.

Der kristalline Unterbau besteht aus Migmatiten und Gneis (magmatischer sowie sedimentärer Herkunft), Charnockiten, Noriten sowie verschiedenen Arten von Graniten. Sie sind Teil des westafrikanischen Kratons, der vor mehr als zwei Milliarden Jahren entstanden ist. Das Phyllitgestein besteht größtenteils aus Tonschiefer und Quarziten. Dieser Sockel ist von einer dünnen Sedimentschicht bedeckt, die aus Tonsand kontinentalen Ursprungs sowie aus Ton, Sand und Schlamm maritimen Ursprungs besteht.

Die Böden der Elfenbeinküste haben die gleichen Eigenschaften wie jene der benachbarten Länder Westafrikas und vieler anderer tropischer Regionen. Sie sind locker, seltener verhärtet, aus einem Material in roten ockerfarbenen und dunklen rostbraunen Farbtönen. Es handelt sich um ferrallitische Bodentypen, die größtenteils durch Verwitterung entstanden sind.

Die Elfenbeinküste liegt zwischen 4° und 10° nördlicher Breite; der Äquator liegt etwa 400 km von der südlichen Küste des Landes entfernt und der nördliche Wendekreis ist etwa 1400 km von der Nordgrenze entfernt. An den Küsten der Elfenbeinküste herrscht deshalb ein immerfeuchtes tropisches Klima, das im äußersten Norden in ein trockenes Klima übergeht. Die mittlere Jahrestemperatur liegt bei 28 °C, jedoch kennen die Bewohner markante Temperaturunterschiede zwischen den nördlichen und südlichen Regionen ihres Landes sowie zwischen den einzelnen Jahreszeiten.

Geprägt wird das Klima durch die Windsysteme des Nordost- Passats und des Südwestmonsuns: Der Nordost-Passat (Harmattan) bringt im Winter heiße, trockene, staubbeladene Luft aus der Sahara und trocknet das Land aus. Die Herkunft des westafrikanischen Monsuns ist im Golf von Guinea, dementsprechend bringt er feuchtwarme Luft. Er bestimmt das Klima des Südens der Elfenbeinküste ganzjährig, im Norden bringt er Sommerregen.

Demnach werden in der Elfenbeinküste drei Klimazonen unterschieden.

Das Klima von Odienné, einer Stadt im Nordwesten, ist von den nahen Bergen geprägt und hat deshalb höhere Niederschlagswerte (1491 mm) und niedrigere Temperaturen als Regionen östlich davon. In Man (noch höher in den Bergen gelegen) erreichen die Niederschlagswerte sogar 1897 mm pro Jahr.

Die Vegetation lässt sich in zwei Zonen einteilen: Eine südliche, "guineische" Zone und eine nördliche "sudanesische" Zone. Die Grenze zwischen diesen beiden Zonen liegt parallel zur Küstenlinie etwa beim 8. Breitengrad. Die südliche Zone ist von immergrünem Regenwald und Mangroven (Guineische Mangroven), davon eine westlich von Abidjan, an der Mündung des Flusses Bia, und eine noch weiter westlich davon an der Mündung des Flusses Boubo geprägt. In der nördlichen Zone herrschen Trockenwälder (mit periodischem Laubwechsel) und Savannen (die Sudan-Savanne, die ein Drittel des Territoriums bedeckt, und die Guinea-Savanne) vor, wobei der Trockenwald als Übergang vom Regenwald zur Savanne gesehen werden kann. Im zentralen Teil der Elfenbeinküste ist das Guineische Wald-Savannen-Mosaik, das aus ineinandergreifenden Zonen aus Grasland, Savanne und dichtem Feuchtwald und Galeriewald an Flussläufen besteht.

Nennenswerte Vertreter der Flora in der Elfenbeinküste sind Bäume wie der Affenbrotbaum, Iroko, Tali, Amazakoue, Tiama und Movingui, die teils hohe Bedeutung für den Export von Holz haben. In den Wäldern wachsen Epiphyten und Orchideen, während Schlangenwurze, "Manniophyton", Knoblauchbaum, Milne-Redhead und Belluci Bedeutung als traditionelle Heilpflanzen haben.

Die Vegetation der Elfenbeinküste hat sich in den vergangenen Jahrzehnten durch menschliches Zutun grundlegend geändert. Ursprünglich war ein Drittel des Landes im Süden und Westen vollständig von dichten Wäldern bedeckt. Dazu kamen Baumsavannen im Zentrum und Norden sowie kleine Mangroven an der Küste. Seit der Kolonialzeit hat sich der Waldbestand stark verringert, teils durch die Anlage von Plantagen, teils durch Abholzung. Für das Jahr 2007 wurde der natürliche Waldbestand auf 6 Millionen Hektar geschätzt.

Die Fauna ist besonders artenreich. Unter den Säugetieren ist der Elefant das Tier, dessen Stoßzähne, als Elfenbein gehandelt, dem Land seinen Namen gaben. Sein in Wald und Savanne einst hoher Bestand ist mittlerweile durch Jagd und Wilderei stark reduziert, so dass er heute nur noch in Reservaten anzutreffen ist. Daneben gibt es Flusspferde, Riesenwaldschweine, Ducker, Primaten, Nagetiere, Schuppentiere, Raubkatzen wie Leoparden sowie Mangusten; in den Steppen sind Hyänen und Schakale anzutreffen. Das seltene Zwergflusspferd hat im Nationalpark Taï, im Südwesten des Landes eines seiner wichtigsten Vorkommen. Auch leben hier hunderte Arten von Vögeln (Reiher, Störche wie Wollhalsstorch und Marabu, Enten und Gänse sowie Greifvögel). In und an den Flussläufen der Savanne lebt das Panzerkrokodil, in den Flüssen der Regenwälder das Stumpfkrokodil. Schlangen wie Kobras, Mambas, Puffotter, Gabunviper und Nashornviper, Felsenpython und Königspython kommen ebenso vor wie Termiten, die die Landschaft mit zahlreichen Termitenhügeln verzieren, und Käfer wie etwa der Pillendreher. In den Flüssen leben zahlreiche Fischarten wie Buntbarsche oder der Afrikanische Vielstachler, während in den Küstengewässern Garnelen, Sandtiger- und sonstige Haie, Seenadeln, Rochen, Froschfische, Plattfische oder auch die seltene Unechte Karettschildkröte vorkommen. Zahlreiche Arten, etwa die Schimpansen, sind bereits sehr selten oder vom Aussterben bedroht.

Seit 1953 wurden acht Nationalparks ausgewiesen, der älteste ist der Nationalpark Banco. Die bekanntesten sind der Nationalpark Taï (im Südwesten des Landes) und der Nationalpark Comoé (im Nordosten), die beide auch Weltnaturerbe-Gebiete sind. Weitere Nationalparks heißen Nationalpark Marahoué (im Zentrum, westlich des Kossoustausees), Nationalpark Mont Sangbé und Nationalpark Mont Péko (beide im Westen) sowie, an der Küste westlich und östlich von Abidjan liegend, der Nationalpark Azagny und der Nationalpark Îles Ehotilé.

Als drittes Weltnaturerbe-Gebiet wurde das Mont Nimba Strict Nature Reserve auf die UNESCO-Welterbe-Liste gesetzt; mit einem größeren Teil setzt sich das Strenge Naturschutzgebiet (Kategorie Ia der IUCN-Richtlinien) grenzüberschreitend in Guinea fort.

Die Bevölkerung der Elfenbeinküste zeichnet sich, ähnlich wie jene der meisten Entwicklungsländer, durch ein schnelles Wachstum aus. Zwischen 1975 und 2005, in nur 30 Jahren, verdreifachte sich die Bevölkerung von 6,7 Millionen auf fast 20 Millionen. Dieses Wachstum geht zu einem gewissen Teil auf Einwanderung zurück; die Volkszählung 1998 ergab, dass 26 % der Bevölkerung Nicht-Ivorer waren. Diese Einwanderer stammen zum Großteil aus den Nachbarländern und wurden vor dem Bürgerkrieg von der relativ hohen wirtschaftlichen Entwicklung und der sozialen und politischen Stabilität angezogen. Insgesamt leben zwei Millionen Menschen aus Burkina Faso in der Elfenbeinküste, die den größten Ausländeranteil stellen. Daneben wanderten zahlreiche Personen aus Mali, Guinea, dem Senegal, Liberia und Ghana ein. Ferner findet man Libanesen, die vor allem Handel betreiben, Asiaten und Europäer. Ausländer, die eingebürgert wurden, machen nur 0,6 % aus.

Die zusammengefasste Fruchtbarkeitsziffer liegt bei 4,6 Kindern pro Frau. Dies liegt unter anderem daran, dass nur 8 % der verheirateten Frauen moderne Verhütungsmethoden zur Verfügung stehen (Stand 2012). Jugendliche machen einen sehr starken Bevölkerungsanteil aus: 2012 waren 41 % der Bevölkerung unter 15 Jahre alt und nur 4 % über 65 Jahre alt. Ebenso ist die Bevölkerung ungleich über das Territorium des Landes verteilt. 57 % Landbevölkerung stehen 43 % Stadtbevölkerung gegenüber, wobei die Stadtbevölkerung um 4,2 % jährlich zunimmt. Der Trend der Landflucht hat sich durch den Bürgerkrieg noch verstärkt.

Als "Stadt" werden in der Elfenbeinküste urbane Räume mit mindestens 3.000 Einwohnern, in denen mehr als 50 % der Bevölkerung einer nicht-landwirtschaftlichen Erwerbsbetätigung nachgeht, definiert. 
2016 lebten 55 % der Bevölkerung in Städten oder städtischen Räumen womit die Elfenbeinküste zu den am stärksten urbanisierten Ländern in Afrika gehört. Die größten Metropolregionen sind (Stand Zensus 2014):
Weitere Städte sind in der "Liste der Städte in der Elfenbeinküste" aufgelistet.

Der ivorische Staat erkennt circa 60 Volksgruppen an, die lange Zeit friedlich zusammenlebten. Eheschließungen zwischen Angehörigen verschiedener Ethnien sind vor allem in den Städten nicht mehr selten. Die Völker werden in vier Kultur- und Sprachgruppen unterteilt:

Aufgrund der Landflucht und der zunehmenden Verstädterung findet man in den Städten praktisch alle Ethnien. Vor allem in den kleineren Städten gibt es eine gewisse Tendenz, in eigenen Vierteln zusammenzuleben.

Neben der Amtssprache Französisch werden in der Elfenbeinküste 77 verschiedene Sprachen und Idiome gesprochen. Die größten sind das Baoulé und das Dioula, daneben werden auch Senufo-Sprachen, Yacouba, Anyi, Attie, Guéré, Bété, Abé, Kulango, Mahou, Tagwana, Wobé und Lobi gesprochen.

Die mit Abstand am weitesten verbreitete Sprache ist Dioula, das von insgesamt 61 % der Bevölkerung vor allem im Norden gesprochen und verstanden wird und als Handelssprache eine große Bedeutung hat. Allerdings ist seit der französischen Kolonialzeit die einzige Amts- und Unterrichtssprache des Landes Französisch.

In der Elfenbeinküste herrscht eine hohe religiöse Diversität. Die am meisten verbreiteten Religionen sind das Christentum (32,8 %) und der Islam (38,6 %); dabei ist der Norden eher muslimisch geprägt, während der Süden christlich geprägt ist. 11,9 % der Bevölkerung praktiziert traditionelle westafrikanische Religionen – vor allem die Religion der Akan –, die bis zu einem gewissen Ausmaß auch die Ausübung der anderen Religionen beeinflussen. Der Islam begann sich im äußersten Norden der Elfenbeinküste ab dem 11. Jahrhundert auszubreiten. Das Christentum wurde an der Küste im 17. Jahrhundert durch Missionare eingeführt.

Die gegenwärtige Entwicklung ist durch eine wachsende Islamisierung geprägt. Noch kurz vor der Jahrtausendwende bekannten sich 40 % der Einwohner zu den traditionellen westafrikanischen Religionen. Der Islam, zu dem sich Mitte der 1980er Jahre erst rund 24 % der Gesamtbevölkerung bekannten, ist seitdem, vor allem durch Mission unter den Anhängern der traditionellen westafrikanischen Religionen (besonders der Senufo) die am stärksten wachsende Religionsgemeinschaft. 2004 waren bereits 35 % der Einwohner Sunna-Muslime. Als Dachverband der muslimischen Organisationen der Elfenbeinküste fungiert der 1993 gegründete „Nationale Islamische Rat“ ("Conseil national islamique"; CNI). Innerhalb dieses Dachverbandes spielt die muslimische Studentenorganisation "Association des élèves et étudiants musulmans de Côte d'Ivoire" (AEEMCI) eine wichtige Rolle. Die jährliche Wallfahrt nach Mekka wird von der "Association musulmane pour l'organisation du pèlerinage à la Mecque" (AMOP) organisiert. Eine wichtige Untergruppe innerhalb der Muslime der Elfenbeinküste stellen die Yacoubisten, die Anhänger von Yacouba Sylla, dar.

Generell herrscht in der Elfenbeinküste religiöse Toleranz und friedliches Miteinander. Die religiösen Feiertage werden frei von den jeweiligen Gläubigen begangen und von allen akzeptiert. Die Elfenbeinküste ist offiziell ein laizistischer Staat, wenngleich Repräsentanten des Staates zu religiösen Zeremonien entsandt werden und spezielle konfessionelle Schulen finanzielle Zuwendungen vonseiten des Staates erhalten.

Zahlreiche Ivorer leben im Ausland, wenngleich ihre genaue Zahl nicht feststellbar ist, da ein Teil von ihnen in ihren Aufenthaltsländern illegal eingewandert ist. Schätzungen gehen von etwa 1,5 Millionen Auslands-Ivorern aus. Begehrteste Ziele ivorischer Auswanderer sind Frankreich, Belgien, die Schweiz, Italien, Deutschland, die USA und Kanada. Diese Auswanderer haben eine große Bedeutung für die ivorische Wirtschaft: Sie überweisen einerseits hohe Summen, um die daheim gebliebenen Angehörigen zu unterstützen, andererseits sind Heimkehrer aus dem Ausland bedeutende Teilnehmer am Immobilienmarkt.

In der Elfenbeinküste, dem bedeutendsten Exportland von Kakao, werden laut Menschenrechtsorganisationen etwa 12.000 Kinder als Sklaven auf Kakaoplantagen eingesetzt.

Während einerseits ausländische Investoren das hohe Bildungsniveau der Eliten schätzen, so stellen andererseits mangelnde Bildung und Analphabetismus große Probleme dar. 2016 lag die Alphabetisierungsrate in der Elfenbeinküste bei 43,1 % (Frauen: 32,5 % Männer: 53,1 %). Es wird geschätzt, dass mehr als vier Millionen Jugendliche keine Ausbildung und keinen Arbeitsplatz haben.

Der Staat gab 2001 4,6 % des Bruttoinlandsprodukts bzw. 21,5 % seines Budgets für Bildungszwecke aus. Davon entfielen 43 % auf die Grundschulen, 36 % auf die weiterführende Bildung und 20 % auf die Universitäten.

Das Bildungssystem der Elfenbeinküste ist stark an jenes von Frankreich angelehnt und wurde kurz vor der Unabhängigkeit eingeführt. Es besteht Schulpflicht und die Schulbildung ist kostenfrei, um den Schulbesuch der Kinder im schulpflichtigen Alter zu fördern bzw. zu ermöglichen. Das Bildungssystem umfasst eine Grundschule und eine weiterführende Schule, an die sich die tertiäre Bildung anschließt.

Vor dem Grundschulbesuch gibt es optionale Kindergärten, von denen 2001/2002 auf dem gesamten Gebiet des Landes 391 Einrichtungen registriert wurden. 2005 existierten nur im von den Regierungstruppen kontrollierten Landessüden 600 Kindergärten mit 2109 Erziehern und 41.445 Kindern.

Die Grundschulausbildung dauert sechs Jahre und endet mit dem "Certificat d’études primaires", das zum Aufstieg in die weiterführende Schule berechtigt. Im Jahre 2001 existierten nach der Statistik des Bildungsministeriums 8050 öffentliche Grundschulen mit 43.562 Lehrkräften und 1.872.856 Schülern. Daneben gab es 925 private Grundschulen mit 78.406 Lehrern und 2.408.980 Schülern.

Der Anteil derjenigen Kinder, die eine Grundschule besuchen, lag 2001/2002 bei 79,5 % (für Mädchen nur 67,3 %) und selbst dies erst nach großen Anstrengungen der Regierung in Zusammenarbeit mit der Afrikanischen Entwicklungsbank im Rahmen des Projekts "Projet BAD éducation IV". Die Schulbesuchsquote fiel während des Bürgerkriegs auf 54,4 % (für Mädchen 49,1 %) im Jahr 2005. Generell hat das Schulwesen im Bürgerkrieg einen hohen Schaden erlitten, viele Schulgebäude wurden zerstört und Lehrer verließen unsichere Gegenden.

Die weiterführende Bildung dauert sieben Jahre. In der weiterführenden Bildung dominieren die privaten Einrichtungen: 370 der 522 im Jahr 2005 gezählten Gymnasien waren privat. Nur etwa 20 % der Jugendlichen bekommen eine weiterführende Bildung. Nach dem ersten, vier Jahre dauernden Abschnitt der weiterführenden Bildung bekommt man das "Diplôme national du brevet" und nach drei weiteren Jahren das "Baccalauréat".

Bereits in den 1960er Jahren wurden in der Elfenbeinküste akademische Bildungseinrichtungen gegründet, um eigene Spezialisten ausbilden zu können. Bis 1992 waren alle diese Hochschulen und Institute staatlich, seitdem wurden zahlreiche Privathochschulen gegründet.

Im Jahr 2004/05 wurden 149 akademische Bildungseinrichtungen gezählt, die von 146.490 Studenten besucht wurden, davon 35 % Mädchen. Darunter fielen drei staatliche Universitäten, vier staatliche Hochschulen "(grandes écoles)" und sieben Privatuniversitäten. Zu den bedeutenderen Einrichtungen gehören das Institut national polytechnique Houphouët-Boigny (INPHB), die École normale supérieure (ENS) und die Agence nationale de la formation professionnelle. Das Ansehen der ivorischen Universitäten ist insbesondere seit dem Bürgerkrieg, als alle Universitäten zum Umzug nach Abidjan gezwungen waren und viele Akademiker das Land verließen, gering.

Das Gesundheitssystem der Elfenbeinküste in Afrika hat durch den Bürgerkrieg schwer gelitten. Viele Einrichtungen wurden geplündert oder zerstört, das Personal musste aus Sicherheitsgründen in den Städten konzentriert werden oder hat das Land gar ganz verlassen.

Häufigste Krankheiten sind, bedingt durch das tropische Klima, Malaria, Cholera, Typhus, Tuberkulose, Gelbfieber sowie Hepatitis A und Hepatitis B. Ein Großteil der Erkrankungen kann auf verschmutztes Trinkwasser zurückgeführt werden; mehr als die Hälfte der armen Haushalte hat keinen Zugang zu sauberem Wasser, wobei dieser Prozentsatz im ländlichen Norden viel höher ist. Die Kindersterblichkeit ist zwischen 1994 und 2007 von 89 auf 117 pro Tausend Lebendgeburten gestiegen. Die Säuglingssterblichkeit lag 2012 bei 73 pro 1.000 Geburten, die Müttersterblichkeit bei 400 pro 100.000 Geburten. Etwa 7 % der Bevölkerung sind mit HIV infiziert "(siehe auch: HIV/AIDS in Afrika)", auch andere sexuelle übertragbare Krankheiten breiten sich, bedingt durch frühe sexuelle Aktivität und mangelnde Aufklärung schnell aus; unfachmännische Abtreibungen sind häufig.

Ein Verbesserungsplan für das Gesundheitswesen wurde von der Regierung 1995 in Angriff genommen. Die politischen Wirren haben jedoch dazu geführt, dass dieses Programm nicht zu Ende geführt wurde. Die Regierung versucht nun, medizinisches Personal in die früheren Kriegsgebiete zurückzubringen.
Quelle: UN

Bis zur Kolonialisierung wies der Südteil der Elfenbeinküste keine Staatenbildung auf. Der Nordteil hingegen kam ab dem 11. Jahrhundert in den Einfluss der Sahelreiche, etwa des Malireiches ab dem 13. Jahrhundert. Gleichzeitig kam der Islam durch Handel und kriegerische Auseinandersetzungen in diese Region. Im 17. Jahrhundert war der Stadtstaat Kong der mächtigste Staat der Region und ein Zentrum islamischer Gelehrsamkeit.

Die Portugiesen trieben seit dem 15. Jahrhundert Handel mit den Küstenstämmen, wurden aber seit dem 17. Jahrhundert von den Franzosen verdrängt, die 1843 den Marinestützpunkt Grand-Bassam errichteten und das Gebiet 1893 zur französischen Kolonie Côte d’Ivoire erklärten. Die Niederschlagung von Aufständen, besonders die des islamischen Führers Samory Touré, beschäftigte die französische Kolonialverwaltung mehrere Jahre. 1895 wurde Côte d’Ivoire ein Teil Französisch-Westafrikas in dem auch der Code de l’indigénat galt. 1956 erhielt es innere Selbstverwaltung und wurde 1958 autonome Republik innerhalb der französischen Gemeinschaft.

Am 7. August 1960 erhielt Côte d’Ivoire die volle Unabhängigkeit unter Félix Houphouët-Boigny, der bis zu seinem Tode 1993 Staatspräsident (bis 1990 auch Regierungschef) war. Houphouët-Boigny, der Gründer der Einheitspartei „Parti Democratique de Côte d’Ivoire“ (PDCI), verfolgte eine prowestliche Politik. Im Gegensatz zu anderen Staaten, die unter anderem durch Namensänderung ihr koloniales Erbe in den Hintergrund rückten und mit Bezeichnungen aus der vorkolonialen Zeit eine unabhängige Identität schaffen wollten, hielt die Elfenbeinküste auch nach der Erlangung der Unabhängigkeit im Jahr 1960 an den engen Verbindungen zu Frankreich fest.

Unruhen unter der Bevölkerung führten dazu, dass 1990 ein Mehrparteiensystem sowie das Amt des Ministerpräsidenten eingeführt wurden. Die prowestliche und marktwirtschaftlich orientierte Politik Houphouët-Boignys machte aus Côte d’Ivoire einen der reichsten Staaten Westafrikas und führte zu politischer Stabilität.

Nachfolger Houphouët-Boignys wurde 1993 Henri Konan Bédié (PDCI). Die von der Opposition boykottierten Wahlen im Oktober 1995 bestätigten Bédié im Präsidentenamt. Eine Änderung der präsidialen Verfassung von 1960 verlängerte 1998 die Amtszeit des Präsidenten von fünf auf sieben Jahre und stärkte seine exekutiven Befugnisse.

Der Verfall der Kakaopreise führte 1999 zu wirtschaftlichen Krisenerscheinungen. Im Dezember 1999 wurde Bédié, der oppositionelle Kreise zunehmend unterdrückt hatte, in einem unblutigen Putsch vom Militär unter Führung von General Robert Guéï gestürzt. Das Land fiel damit in eine tiefe Krise. Unter dem Schlagwort "Ivoirité" kam es zu fremdenfeindlichen Tendenzen und zur Diskriminierung der im Norden des Landes ansässigen Ethnien. Im Jahre 2000 gewann Laurent Gbagbo Präsidentschaftswahlen, von denen der Oppositionskandidat (Alassane Ouattara) ausgeschlossen worden war. Dies wurde damit begründet, dass Ouattaras Eltern aus dem Nachbarland Burkina Faso stammen. Der andauernde Streit darum, wer ein wahrer „Ivorer“ sei und wer nicht, führte schließlich 2002 zu einem bewaffneten Aufstand gegen Gbagbo und zu der darauf folgenden Krise.

Im September 2002 erhob sich ein Teil der Armee („Forces Nouvelles“) gegen die Regierung und brachte die nördliche Hälfte des Staates unter ihre Kontrolle. Diese Entwicklung hatte ihren Hintergrund in ethnischen Spannungen; in der Elfenbeinküste leben viele aus den angrenzenden Staaten eingewanderte Menschen. Es war aber auch ein Konflikt um Land und den Zugang zu Ressourcen.

Im Auftrag der UNO wurden zur Trennung der Rebellen im Norden und dem südlichen Landesteil mehr als 6300 Blauhelme im Land stationiert (Opération des Nations Unies en Côte d’Ivoire). Zusätzlich waren etwa 4500 französische Soldaten im Land. Letztere agierten ebenfalls im Auftrag der UNO, waren aber schon vor der Krise in Côte d’Ivoire stationiert. Die frühere Kolonialmacht Frankreich setzte einen Friedensplan durch, der eine Machtteilung zwischen Gbagbos FPI und den "Forces Nouvelles" der Rebellen vorsah. Der Krieg wurde somit für beendet erklärt.

Anfang November 2004 eskalierte die Situation erneut, als am 4. November Regierungstruppen Ziele im Norden des Landes aus der Luft angriffen. Gleichzeitig wurden in Abidjan Büros von Oppositionsparteien und unabhängigen Zeitungen verwüstet. Am dritten Tag der Luftangriffe kamen neun französische Soldaten ums Leben. Als Reaktion darauf wurde von den französischen Streitkräften die gesamte Luftwaffe (zwei Kampfflugzeuge, fünf Kampfhubschrauber) Côte d’Ivoires binnen eines Tages vernichtet. Letzteres wurde von der UNO nachträglich für gerechtfertigt erklärt.

Dem südlichen Landesteil unter Gbagbo wurde vorgeworfen, die Teilung der Macht eigentlich nicht gewollt zu haben. Gbagbo habe die Lage seit längerem unter anderem mit Aufrufen zu Hass und Gewalt über TV und Radio destabilisiert. Bis zum 15. November 2004 wurden rund 6000 Ausländer via Luftbrücke evakuiert.

Unter südafrikanischer Vermittlung einigten sich Armee und Rebellen am 9. Juli 2005 neuerlich auf ein Entwaffnungs- und Machtteilungsabkommen. Dieses sollte den Weg freimachen zu Präsidentschaftswahlen am 30. Oktober 2005. Der Bürgerkrieg wurde zum zweiten Mal für beendet erklärt.

Weder die Entwaffnung noch Wahlen wurden jedoch umgesetzt. Gründe dafür waren Unstimmigkeiten bei der Vorgehensweise zur Erfassung der Wähler und über das Ausstellen von Identitätspapieren. Die UNO beschloss eine Verlängerung der Amtszeit von Präsident Gbagbo um ein Jahr, und stellte ihm den parteilosen Charles Konan Banny als Premierminister an die Seite.

Mitte Januar 2006 eskalierte die Situation erneut: Es kam in mehreren Orten zu gewalttätigen Demonstrationen mit Toten und Verletzten. Nach einem einschlägigen UN-Beschluss Anfang Februar 2006 wurden Konten von drei Gegnern des Friedensprozesses eingefroren. Die Sanktionen richteten sich gegen Ble Goude und Eugene Djue, die als Anführer militanter Jugendgruppen und Anhänger von Staatspräsident Laurent Gbagbo galten, sowie gegen Rebellenführer Fofie Kouakou. Die Audiences foraines genannte Registrierung von bisher papierlosen Bürgern im Hinblick auf die vereinbarten Wahlen kam nur schleppend vorwärts. Die Opposition behauptete, sie würde von Mitgliedern der Regierungspartei hintertrieben und teilweise verhindert.

Am 4. März 2007 wurde, nach langwierigen Verhandlungen zwischen Präsident Gbagbo, Rebellenführer Guillaume Soro und dem burkinischen Präsidenten Blaise Compaoré, ein neuer Friedensvertrag unterzeichnet. Dieser Vertrag sah, im Unterschied zu den vorigen Abkommen, neben Machtteilung auch einen ständigen Konzertationsrahmen vor, in dem neben Gbagbo, Soro und Compaoré auch Bédié und Ouattara vertreten waren. Soro wurde zum Premierminister der neu zu bildenden Regierung ernannt. Dieser Vertrag von Ouagadougou enthielt detaillierte Vereinbarungen zur Ausgabe von Identitätspapieren, Aufstellen des Wählerverzeichnisses sowie die Schaffung einer nationalen Armee.

Wenige Wochen später wurde mit dem Abbau der Pufferzone begonnen und es gab erste gemeinsame Patrouillen aus Regierungssoldaten und Rebellen der Forces Nouvelles (FN). Im Juli 2007 besuchte Präsident Gbagbo zum ersten Mal seit fünf Jahren den von den Rebellen gehaltenen Norden. Er nahm dort an einer offiziellen Friedenszeremonie teil, bei der in Anwesenheit zahlreicher afrikanischer Staatschefs Waffen verbrannt wurden.

Schließlich wurden die Präsidentschaftswahlen mit einem ersten Wahlgang am 31. Oktober 2010 durchgeführt. Bei einer Wahlbeteiligung von etwa 80 Prozent gewannen unter 14 Kandidaten der damals amtierende Präsident Gbagbo mit 38 Prozent sowie als Kandidaten der Opposition Alassane Ouattara (RDR) mit 32 Prozent und Henri Konan Bédié (PDCI) mit 25 Prozent die meisten Stimmen. Eine Stichwahl zwischen Gbagbo und Ouattara fand am 28. November 2010 statt. Davor kündigten beide an, das Auszählungsergebnis überprüfen zu lassen. Aus der Stichwahl ging laut dem Ergebnis der Wahlkommission CEI (Commission électorale indépendante) Alassane Ouattara mit 54 % der Stimmen als Sieger hervor. Der Verfassungsrat erklärte jedoch die Ergebnisse in vier Regionen für nichtig. Dadurch habe nun Gbagbo die Stichwahl gewonnen. Daraufhin legten sowohl der bisherige Amtsinhaber Laurent Gbagbo als auch Alassane Ouattara den Amtseid ab. Gemäß dem Mandat der UN-Mission UNOCI musste der Sondergesandte Choi Young-jin das Wahlergebnis zertifizieren. Nach seiner Prüfung erklärte er das Ergebnis der Wahlkommission für gültig.
Gbagbo wurde von den Vereinten Nationen, den USA und der Europäischen Union nicht mehr als rechtmäßig gewählter Präsident anerkannt. Der Internationale Währungsfonds drohte mit einem Boykott des Landes. Nach der Festnahme Gbagbos am 11. April 2011 war der Machtkampf zugunsten Ouattaras entschieden.

Ab den Präsidentschaftswahl 2010 kam es zwischen Anhängern beider Lager zu einer Regierungskrise mit gewaltsamen Auseinandersetzungen und Todesopfern. Auch ein Blauhelm-Konvoi wurde angegriffen. Dabei wurden auch schwere Waffen gegen Zivilisten eingesetzt. Bis Ende März 2011 waren eine Million Menschen auf der Flucht vor dem Bürgerkrieg. Am 11. April 2011 wurde der abgewählte Präsident Laurent Gbagbo von den Truppen des international anerkannten Wahlsiegers Ouattara nach langwierigen Kämpfen mit Unterstützung von militärischen Kräften der UNO und Frankreichs festgenommen. Damit hatte sich Ouattara als rechtmäßiger Präsident und sein Premierminister Guillaume Soro weitgehend durchgesetzt.
Gbagbo wurde im November 2011 dem Internationalen Gerichtshof in Den Haag überstellt. Ouattara musste sich den Vorwurf der „Siegerjustiz“ gefallen lassen. Bis 2012 wurde kein einziges der zahlreichen Menschenrechts- und Kriegsverbrechen seiner Militärs verfolgt, Verantwortliche benannt oder gar angeklagt, insbesondere nicht für das Massaker von Duékoué, bei dem laut dem Internationalen Roten Kreuz 800 Menschen von Ouattara-Militärs brutal ermordet wurden.

Im Länderbericht Freedom in the World 2017 der US-amerikanischen Nichtregierungsorganisation Freedom House wird das politische System des Landes als „teilweise frei“ bewertet. In der Kategorie „politische Rechte“ erhält die Elfenbeinküste die Note 4, bei der Wahrung der Bürgerrechte erhält das Land ebenfalls die Note 4 (die 1 ist die beste Note und die 7 die schlechteste).

Nach seiner Unabhängigkeit hat die Elfenbeinküste ein präsidentielles Regierungssystem eingeführt. Es existiert formelle Gewaltentrennung in die Exekutive, die Legislative und die Judikative. Dazu kommen Institutionen wie der "Conseil économique et social" und der "Médiateur de la République".

Die im Jahre 2000 verabschiedete Verfassung garantiert Grundrechte und Grundfreiheiten, wie es internationale Abkommen und Verträge verlangen. Ebenfalls ist seit 2000 die Todesstrafe abgeschafft.

Die Realität sieht jedoch anders aus. Im Bürgerkrieg kam und kommt es sowohl durch die Rebellen- als auch durch die Regierungstruppen zu massiven Übergriffen wie Mord, Folter, Verschwindenlassen unliebsamer Personen und sexueller Gewalt. Die Beschneidung weiblicher Genitalien ist offiziell verboten, dennoch wird sie häufig praktiziert; das Gleiche gilt für Kinderarbeit.

Die Exekutive fiel bis 1990 allein dem Staatspräsidenten zu. Seitdem sind die Kompetenzen auf den Präsidenten als Staatsoberhaupt und auf den Premierminister als Regierungschef verteilt.

Der Staatspräsident wird in direkter, allgemeiner Wahl gewählt. Es werden zwei Durchgänge abgehalten, wobei ein Kandidat die einfache Mehrheit erreichen muss. Das Mandat dauert fünf Jahre und der Präsident kann einmal wiedergewählt werden. Er ist der alleinige Chef der Exekutive; zu seinen Aufgaben gehört es, die nationale Unabhängigkeit zu bewahren, die Integrität des Territoriums aufrechtzuerhalten und internationale Abkommen und Verträge einzuhalten. Er ist Oberbefehlshaber der Streitkräfte, wacht über die Einhaltung der Verfassung und über die Kontinuität des Staates. Er ist Chef der Verwaltung und ernennt zivile wie militärische Beamte. In Krisenzeiten erhält der Präsident Sondervollmachten. Im Fall des Todes, des Zurücktretens oder der Absetzung des Präsidenten übernimmt der Präsident der Nationalversammlung dieses Amt für eine Dauer bis zu 90 Tagen.

Der Premierminister wird vom Staatspräsidenten ernannt und kann von ihm wieder entlassen werden. Verfassungsgemäß hat der Premierminister keine eindeutig exekutive Funktion. Er vertritt jedoch den Staatspräsidenten, wenn er außerhalb des Landes ist. Der Premierminister muss nicht aus der parlamentarischen Mehrheit hervorgehen. Die Regierung, die dem Premierminister untersteht, wird vom Staatspräsidenten auf Vorschlag des Premierministers ernannt. Er leitet die Regierung und kann gewisse Autoritäten an die Minister delegieren.

Die Elfenbeinküste verfügt über ein Einkammerparlament, die Nationalversammlung "(Assemblée nationale)". Die Anzahl der Parlamentssitze wurde bei der letzten Parlamentswahl von 225 auf 255 erhöht. Das Parlament hat ferner ein Büro, mehrere technische Kommissionen und parlamentarische Gruppen. Die Abgeordneten werden in allgemeinen Wahlen direkt für eine Amtszeit von 5 Jahren gewählt. Abweichend von dieser rechtlichen Regelung wurde unter Präsident Gbagbo nach dem Ausbruch des Bürgerkriegs (2002) und der sich anschließenden faktischen Zweiteilung des Landes die eigentlich 2005 fällige Neuwahl des Parlaments nicht durchgeführt.

In der Nationalversammlung wird über Gesetze und Steuern abgestimmt, des Weiteren hat sie verfassungsgemäß die Kontrolle über die Tätigkeiten der Exekutive. Um die Unabhängigkeit der Nationalversammlung zu sichern, sind die Abgeordneten immun vor Strafverfolgung aufgrund der Ausübung ihrer Abgeordnetentätigkeit und auch für Strafverfolgung wegen Vergehen außerhalb ihrer Abgeordnetenfunktion muss es die Zustimmung des Parlaments geben.

Gewählter Parlamentsvorsitzender ist seit dem 12. März 2012 der ehemalige Rebellenanführer Guillaume Soro.
Bei den Parlamentswahlen am 11. Dezember 2011 errang die RDR von Staatspräsidenten Alassane Ouattara einen deutlichen Sieg. Die Partei seines Amtsvorgängers Laurent Gbagbo, die FPI, boykottierte die Wahl. Die mit der RDR verbündete PDCI büßte gegenüber der Wahl von 2000 einige Sitze ein. Aufgrund des Wahlboykotts der FPI ist die Opposition im Parlament kaum vertreten.

Kurz vor der Unabhängigkeit der Elfenbeinküste wurden im Jahr 1956/57 erste pluralistische Wahlen organisiert, um die Territorialversammlung und Gemeinderäte zu wählen. Alle Sitze wurden von der Parti Démocratique de Côte d’Ivoire, einer Teilbewegung des Rassemblement Démocratique Africain gewonnen. Kurz nach dieser Wahl entscheiden sich alle politischen Mitbewerber, sich der PDCI-RDA im Rahmen eines "nationalen Konsensus" unterzuordnen. Die PDCI-RDA wird somit zur einzigen Partei des Landes. Dieses Einparteiensystem bleibt praktisch bis 1990 bestehen, auch wenn zeitweise vorsichtige Schritte zur Bildung einer Opposition unternommen wurden oder einzelne Krisen das Land erschütterten (etwa die Sanwi-Affaire 1959–1966, das angebliche Komplott gegen den Präsidenten 1963/64, Guébié-Affaire 1970 oder der gescheiterte Putsch 1973).

Dieses System endet mit den Massendemonstrationen im Jahr 1990 und der Rückkehr zum Mehrparteiensystem, wie es eigentlich seit 1960 in der Verfassung der Republik verankert gewesen wäre. Im gleichen Jahr werden zahlreiche neue Parteien gegründet. Die Parteien, die momentan politischen Einfluss haben, sind der sozialistische Front Populaire Ivoirien (FPI) unter Pascal Affi N’Guessan, die rechtsliberale Parti Démocratique de Côte d’Ivoire – Rassemblement démocratique africain (PDCI-RDA) unter Henri Konan Bédié und der liberale Rassemblement des Républicains (RDR) unter Alassane Ouattara. Nennenswert, jedoch mit weniger politischem Gewicht ausgestattet, sind die Union pour la Démocratie et la Paix en Côte d’Ivoire (UDPCI) von Albert Mabri Toikeusse und die sozialistische Parti Ivoirien des Travailleurs (PIT) unter Francis Wodié.

Die Elfenbeinküste hat aus der Kolonialzeit ein Justizsystem geerbt, das zwei parallele Rechtsprechungen aufwies – einerseits das französische Recht, andererseits das lokale Gewohnheitsrecht. Dies resultierte aus zwei verschiedenen Gesetzgebungen, die wiederum zwischen den verschiedenen Bevölkerungsschichten und deren Status unterschieden. Frankreich behielt damals für "normale" Ivorer einen anderen rechtlichen Status bei als für Franzosen und Gleichgestellte.

Nach der Unabhängigkeit ging man daran, einen Justizapparat aufzubauen, der sowohl modern als auch an die Notwendigkeiten des Landes angepasst war. Es wurden neue Strukturen aufgebaut und das entsprechende Personal ausgebildet. Obwohl seit 1960 viele Veränderungen geschehen sind, bleiben die französischen Einflüsse im ivorischen Justizsystem stark.

Die Justizgewalt wird in zwei Instanzen unter der Kontrolle des obersten Gerichts "(Cour suprême)" ausgeübt. Der Verfassungsrat sowie der Hohe Gerichtshof "(Haute cour de justice)", sind Sondergerichtsbarkeiten.

Der "Conseil économique et social" "(Wirtschafts- und Gesellschaftsrat)" ist ein Konsultativorgan, das in der Verfassung der Elfenbeinküste vorgesehen ist. Er ist dafür eingerichtet, die wichtigsten wirtschaftlichen und gesellschaftlichen Tätigkeiten zu repräsentieren, die Zusammenarbeit verschiedener Wirtschaftszweige und die Wirtschafts- und Sozialpolitik der Regierung zu verbessern. Gesetzesprojekte aus Wirtschafts- und Sozialpolitik werden ihm zur Kommentierung vorgelegt. Der Staatspräsident kann diese Einrichtung zu allen wirtschaftlichen und Sozialfragen konsultieren.

Die Mitglieder dieser Institution werden für fünf Jahre ernannt. Das Auswahlkriterium ist, wie sehr die jeweiligen Personen zur Entwicklung des Landes beigetragen haben. Momentan hat der "Conseil économique et social" 125 Mitglieder. Sein Vorsitzender ist seit dem 19. Mai 2011 Marcel Zady Kessy.

Der "Médiateur de la République" "(Vermittler der Republik)" ist ebenfalls ein verfassungsmäßig vorgesehenes Organ. Er ist eine unabhängige Verwaltungseinheit, der etwa die Funktion eines Ombudsmanns einnimmt. Der Vorsitzende dieser Organisation wird vom Staatspräsidenten nach Vorschlag des Präsidenten der Nationalversammlung ernannt, seine Amtszeit dauert sechs Jahre und ist nicht verlängerbar. Er kann auch nicht vor Ende seiner Amtszeit abgesetzt werden; er kann nur durch den Verfassungsrat ("Conseil constitutionnel") seines Amtes enthoben werden. Er ist in der Ausübung seines Amtes immun. Er kann nicht gleichzeitig zu dieser Funktion ein anderes politisches Amt oder eine öffentliche Funktion bekleiden, ebenso darf er keine andere berufliche Funktion ausüben. Die Funktion des "Médiateur de la République" wird gegenwärtig von N’Golo Coulibaly bekleidet.

Der ivorische Staat hat seit seiner Existenz nie konsequent am Aufbau eigener Streitkräfte gearbeitet. Er verließ sich anstelle dessen auf die abschreckende Wirkung der französischen Militärpräsenz in der Region. Mit Frankreich existieren Verteidigungs- und Militärhilfeabkommen inklusive Geheimklauseln.

Im August 2002 erklärte der damalige Präsident Gbagbo grundsätzlich, dass Westafrika eine militärische Kooperationslogik statt gegenseitige Abschreckung benötige. Aufgrund des Bürgerkriegs blieb es in der Folge aber bei dieser Erklärung.

Beim Militär ist momentan für die Regierung die wichtigste Aufgabe, die Milizen abzurüsten und deren Söldner wieder in die Gesellschaft einzubinden, um in der Folge eine reguläre nationale Armee aufzubauen. Die eigens für die Entwaffnung von Zivilisten eingerichtete Nationale Kommission für den Kampf gegen die Verbreitung von Leicht- und Kleinwaffen schätzt die Zahl der im Land im Umlauf befindlichen Waffen auf drei Millionen.

Seit dem 28. September 2011 unterteilt sich die Elfenbeinküste in 12 Distrikte sowie die zwei autonomen Stadtdistrikte Abidjan und Yamoussoukro. Bis dahin hatten 19 Regionen die oberste Verwaltungsebene gebildet. Die Distrikte unterteilen sich in 31 Regionen, die Regionen in 107 Departements und diese wiederum in 197 Gemeinden.

Die Elfenbeinküste verfügt über die stärkste Wirtschaft der westafrikanischen Wirtschafts- und Währungsunion, zu deren gesamtem BIP sie 40 % beiträgt. Das Pro-Kopf-BIP liegt ebenfalls über dem westafrikanischen Durchschnitt, jedoch unter dem gesamtafrikanischen. Die Wirtschaft hat sich mithin von den Wirren des Bürgerkrieges, der 1,7 Millionen Menschen in die Flucht trieb, die offizielle Verwaltung zusammenbrechen ließ, die Produktion behinderte und die Arbeitslosigkeit in die Höhe schnellen ließ, erholt. Das zeigen nicht zuletzt die Investitionen, die sich 2007 gegenüber 2006 vervierfachten und etwa 520 Millionen Euro betrugen. Im Jahre 2015 wuchs die Wirtschaft des Landes mit 9,2 % und gehörte damit zu den am schnellsten wachsenden Volkswirtschaften weltweit.

Die Elfenbeinküste ist auch ein von Armut gekennzeichnetes Land. Als arm gilt in der Elfenbeinküste jemand, der weniger als 162 800 XOF (250 Euro) pro Jahr zum Leben hat. Landesweit fallen 43,2 % der Menschen unter diese Armutsgrenze, in einigen ländlichen Savannengebieten gelten weit mehr als die Hälfte der Menschen als arm.

Im Global Competitiveness Index, der die Wettbewerbsfähigkeit eines Landes misst, belegte die Elfenbeinküste Platz 99 von 138 Ländern (Stand 2016–17). Im Index der Wirtschaftlichen Freiheit belegte das Land 2017 Platz 75 von 180 Ländern. 

Die Landwirtschaft ist nach wie vor der dominierende Wirtschaftszweig der Elfenbeinküste. Sie beschäftigt zwei Drittel der ivorischen Arbeitskräfte und bestreitet die Exporterlöse zu 70 %, auch wenn sie nur 23 % zum BIP beiträgt.

Das Land ist weltgrößter Kakaoproduzent und -exporteur, mit einer Ernte von 1,335 Millionen Tonnen 2003/2004. Damit hat es einen Anteil von 40 % an der weltweiten Gesamtproduktion. Den Kakao ernten zum Teil Kindersklaven. War der Kakao einst das wichtigste Exportprodukt, so hat es diesen Status mittlerweile an die Erdölprodukte verloren. Zudem ist die Kakaoernte in den vergangenen Jahren stark gesunken. Dies lag einerseits am niedrigen Erzeugerpreis für Kakaobohnen, was viele Pflanzer auf andere Erzeugnisse umsteigen ließ und Reinvestitionen der Gewinne in die Plantagen unattraktiv machte. Daneben erheben der Staat und lokale Rebellen hohe Abgaben auf Agrarerzeugnisse, was den Schmuggel in die Nachbarländer fördert. Die schlechte Sicherheitslage vertrieb die Wanderarbeiter und ließ Lagerkapazitäten verfallen. Ein weiteres wichtiges Exportprodukt ist der Kaffee, dessen Ernte 2003/2004 etwa 250.000 Tonnen betrug, was die Elfenbeinküste zu der Zeit zum siebtgrößten Kaffeeproduzenten machte. Angebaut wird vor allem die Sorte "Robusta". Insgesamt leben vom Kaffee- und Kakaoanbau direkt oder indirekt sechs Millionen Menschen.

Mit 130.500 Tonnen an produziertem Rohkaffee, was einen weltweiten Anteil von 1,2 % ausmacht, stand die Elfenbeinküste im Jahr 2014 auf Platz 12 der Anbauländer von Kaffee.

Weitere wichtige Produkte sind Palmöl, Kokosnüsse, Baumwolle (Export von Rohbaumwolle: 105.423 Tonnen im Jahr 2004, vor allem in die Volksrepublik China, nach Indonesien, Thailand und Taiwan), Kautschuk, Kolanüsse (weltgrößter Produzent mit 65.216 Tonnen) und Zuckerrohr. Tropische Früchte wie Ananas, Bananen, Mangos, Papaya, Avocado und Zitrusfrüchte werden nach Europa exportiert. Kaschubäume, die ursprünglich nur im Landesnorden wuchsen, werden jetzt auch südlich davon angebaut; die Ernte an Cashewnüssen betrug 2006 235.000 Tonnen, davon gingen 210 in den Export. Weiterhin nennenswert sind die Produktion von Zitronen, Bergamotten und von Bitterorangen.
An Ackerfrüchten werden vor allem Mais (608.032 Tonnen auf 278.679 Hektar), Reis (673.006 Tonnen auf 340.856 Hektar), Yams (4.970.949 Tonnen auf 563.432 Hektar), Maniok (2.047.064 Tonnen auf 269.429 Hektar) und Kochbananen (1.519.716 Tonnen auf 433.513 Hektar) angebaut. Nur etwa 10.000 Hektar landwirtschaftliche Nutzfläche der Elfenbeinküste werden künstlich bewässert. Es wird jedoch geschätzt, dass die Bewässerung von 600.000 Hektar Land ökonomisch sinnvoll wäre.

Der Ausbau der Viehzucht ist ein Entwicklungsziel der Regierung, weil der Bedarf der Bevölkerung an tierischen Produkten teils noch durch Importe gedeckt werden muss. Obwohl die Jagd aus Naturschutzgründen bereits im Jahr 1974 offiziell verboten wurde, ist Wild noch ein bedeutender Fleischlieferant. Auch bei Fischprodukten ist die Elfenbeinküste auf Importe angewiesen (204.757 Tonnen im Jahr 2000), trotz ihrer 500 km langen Küste. Aus diesem Grund fördert die Regierung das Anlegen von Fischteichen.

Die wichtigste natürliche Ressource der Elfenbeinküste ist Holz, wovon das Land mehr exportiert als das viel größere Brasilien. Die schnell fortschreitende Abholzung wird kurzfristig jedoch, von dem ökologischen Problem abgesehen, zum Versiegen dieser Einnahmequelle und Ressource führen. Im Jahr 2008 waren nur etwa 10 % landwirtschaftlich nutzbar, wobei dieser Wert seit der Unabhängigkeit des Landes konstant leicht gestiegen ist und seit 2000 etwa gleich geblieben ist. 1970 lag dieser Wert bei etwa 5 %.

Erdöl, das vor der Küste vorkommt, ist seit 2005 das wichtigste Exportprodukt der Elfenbeinküste. Die Erdölreserven werden auf etwa 600 Millionen Barrel geschätzt, 2007 wurden jedoch nur 17,4 Millionen Barrel gefördert. Damit gehört die Elfenbeinküste nicht zu den großen afrikanischen Erdölproduzenten. Ob die relativ niedrige Fördermenge an technischen Problemen liegt oder ob die Regierung die Fördermengen fälscht, um die Einnahmen aus dem Erdölexport am Staatshaushalt vorbeischleusen zu können, ist nicht geklärt.

Neben Erdöl wird auch Gas produziert, wobei sich hier die Reserven auf 23.690 Milliarden Kubikmeter belaufen dürften. 2006 wurden 53,8 Millionen MMBtu gefördert.

Die Industrie trug 2005 nur etwa 23,1 % zum Bruttoinlandsprodukt bei, 2000 waren es noch 24,5 %. Sie wird von kleinen und mittleren Betrieben dominiert; trotz aller Probleme, deren sie sich gegenübersieht, ist sie die am meisten diversifizierte in Westafrika. Sie stellt 40 % des Potentials der WAEMU-Länder. Die kleinen und mittleren Unternehmen waren auch von den Krisenjahren am schwersten betroffen, während die großen internationalen Firmen den Bürgerkrieg in der Regel gut überstanden haben.

Ein wichtiger Zweig ist die Raffinierung des Rohöls. Die Elfenbeinküste kann momentan 70.000 Barrel Rohöl pro Tag verarbeiten, wobei neben eigenem Öl auch Öl der Nachbarländer raffiniert wird. Kapazitäten für die Verarbeitung von weiteren 60.000 Barrel sind im Bau.

2007 wurden 1059 kg Gold produziert.

Aufgrund des Wiederaufbaubedarfes, aber auch wegen des Baubeginns an einigen Infrastrukturprojekten, kann die Bauindustrie der Elfenbeinküste starke Zuwächse verzeichnen. Ebenso wird erwartet, dass die Lebensmittelindustrie von den steigenden Nahrungsmittelpreisen und der steigenden Inlandsnachfrage profitieren wird. Insgesamt verlassen aber viele Produkte das Land in unverarbeitetem Zustand. Politische Instabilität und Korruption haben inländische wie ausländische Investoren von kapitalintensiven Projekten abgehalten. Der Umfang ausländischer Investitionen an der Elfenbeinküste liegt unter dem Durchschnitt der in Afrika südlich der Sahara getätigten Auslandsinvestitionen.

Am 18. Mai 2015 wurde die erste industrielle Schokoladenfabrik (10.000 t/Jahr) im Land von Präsident Alassane Ouattara eröffnet.

Der Tourismus hat ein hohes Potential an der Elfenbeinküste. Das Land hat 520 km Atlantikküste mit zahlreichen Stränden, zahlreiche Nationalparks mit seltener Flora und Fauna und zahlreiche Ethnien mit vielfältiger Kultur, so dass Touristen genügend Attraktionen geboten werden können.

Die ivorische Regierung hat dies erkannt und auch einiges an gesetzlicher Grundlage und materieller Infrastruktur geschaffen. Die Elfenbeinküste blieb trotzdem bis in die 1980er Jahre zunächst primär ein Ziel für Geschäftsreisende, wenngleich sich einige Ausländer dauerhaft in dem Land ansiedelten, um dort zu leben. Die antifranzösischen Ausschreitungen (die sich auch gegen Nicht-Franzosen richteten), die folgende Evakuierung und der Bürgerkrieg haben jedoch den Fremdenverkehr vollständig zum Erliegen gebracht.

Die Elfenbeinküste ist Mitgliedsstaat der WAEMU. Es hat daher keine eigene Währung, keine eigene Zentralbank und muss die Geldpolitik deshalb mit den anderen WAEMU-Staaten koordinieren.

Das Budget der Regierung für 2008 betrug 2129 Milliarden XOF, wovon drei Viertel aus Steuereinnahmen stammen. Der Rest stammt aus anderen ivorischen Quellen, Schuldenaufnahme und Stützungszahlungen aus dem Ausland. Besonders hoch sind die Ausgaben für Abrüstung, soziale Reintegration, für die Organisation einer Wahl, für den Aufbau einer nationalen Armee und für das Wiedererlangen der staatlichen Kontrolle über das gesamte Territorium.

Angesichts dessen, dass der informelle Sektor etwa 40 % der Wirtschaftsleistung erbringt, versucht der Staat, seine Steuereinnahmen zu erhöhen und die Eintreibung zu rationalisieren. Zu diesem Zweck laufen Programme, eine einheitliche Mehrwertsteuerrechnung einzuführen und um die Zollabwicklung zu verbessern. Alles in allem ist der Staatshaushalt in etwa ausgeglichen.

Das Bankensystem der Elfenbeinküste findet langsam zur Normalität zurück. Nachdem während des Bürgerkrieges alle Banken im Norden der Elfenbeinküste schließen mussten, öffnen die Bankfilialen nach und nach wieder. Dies gilt auch für den Mikrofinanzsektor. Die Banken werden durch etwa 20 % fauler Kredite in ihren Büchern belastet; an vielen dieser faulen Kredite trägt der Staat Schuld, weil er seine Rechnungen nicht bezahlt.

Die Elfenbeinküste ist Mitglied in mehreren regionalen Organisationen, die die wirtschaftliche Integration zum Ziel haben. Die wichtigsten sind die Westafrikanische Wirtschafts- und Währungsunion UEMOA und die Westafrikanische Wirtschaftsgemeinschaft ECOWAS.

Die Elfenbeinküste hatte in der Vergangenheit dank der Kakao- und Erdölexporte immer eine positive Handelsbilanz. Im Jahre 2007 exportierte die Elfenbeinküste Waren im Wert von 6,2 Milliarden Euro und importierte Waren im Wert von 4 Milliarden Euro. Wichtigste Exportgüter sind Erdölprodukte und Rohöl, Kakao, Holz, Kaffee, Cashew-Nüsse, Baumwolle, Naturkautschuk, Palmöl, Fisch, Textilien, Zement und Tropenfrüchte. Importiert werden hingegen Rohöl und Erdölprodukte, industrielle Rohstoffe, Lebensmittel, Getränke sowie Investitionsgüter. Wichtigste Zielmärkte für die Exporte der Elfenbeinküste sind die EU (41,1 %) und hier vor allem Frankreich, die anderen Länder der UEMOA (12,6 %), die USA (7,1 %) und asiatische Staaten (4,3 %). Importiert wird vor allem aus der EU (32,7 %), aus Asien (17,4 %), aus den USA (2,9 %) und den UEMOA-Staaten (0,9 %).

Seit 2010 steigen die Importe rasch an, während die Kakao- und Ölexporte sanken. 2012 war das erste Jahr mit einem Außenhandelsdefizit.

Die Exporte nach Deutschland bestehen fast nur aus Kakao und Erdöl. Der in Deutschland verarbeitete Kakao stammte 2007 zu etwa 60 % aus der Elfenbeinküste. Aus Deutschland importiert werden vor allem Fahrzeuge, Maschinen und Pharmazeutika. Für die deutschen Exporte spielt die Elfenbeinküste als Markt nur eine sehr untergeordnete Rolle, sie belegen in der Außenhandelsstatistik den 114. Platz.

Ausländische Investitionen stammen in der Elfenbeinküste vor allem aus Frankreich, Südafrika, Großbritannien und den Nachbarländern. Sie sind jedoch im Jahresvergleich sehr stark schwankend.

Die Außenverschuldung betrug 2007 64 % des BIP und 124 % der Exporte eines Jahres. Es zählt somit zu der Gruppe der Hochverschuldeten Entwicklungsländer. Im April 2002 waren bereits umfangreiche Schuldenstreichungen von Seiten der G8 zugesagt. Der Bürgerkrieg hat diesen Prozess jedoch verzögert. 2012 erfolgte ein Schuldenschnitt.

Die Elfenbeinküste ist Mitglied der International Cocoa Organization.

Ein großes Problem des Staates ist der hohe Grad an Korruption. Côte d’Ivoire belegte 2010 mit Platz 146 von 178 einen der untersten Plätze in der Statistik von Transparency International. 2017 hatte sich das Land auf Platz 103 von 180 verbessert.

Exemplarisch dafür steht der Giftmüllskandal aus dem Jahr 2006: Anfang September 2006 wurde bekannt, dass von einem ausländischen Schiff aus auf mehreren Deponien, aber auch in der offenen Kanalisation und in Straßengräben in Abidjan über 500 Tonnen Giftmüll abgeladen wurde. Dieses führte zu über 1500 Erkrankungen und mindestens acht Todesfällen. Etwa 15.000 Bewohner klagen über Vergiftungserscheinungen. Als Reaktion auf diesen Giftmüllskandal erklärte die Übergangsregierung von Ministerpräsident Banny am 6. September ihren Rücktritt, um rund zehn Tage später mit minimalen Änderungen wieder ihr Amt anzutreten. Während Präsident Gbagbo ausländische Mächte für diesen „Anschlag“ auf die Elfenbeinküste verantwortlich macht, sind Regimekritiker und die Opposition sich einig, dass die erst wenige Wochen zuvor gegründete verantwortliche Firma dem Verkehrsminister und Gbagbos Frau Simone gehörten und Schmiergelder in Millionenhöhe geflossen seien. Ob von den 150 Millionen Euro, die das Unternehmen Trafigura an Entschädigungen zahlte, jemals etwas an die Opfer weitergegeben wurde, ist ebenfalls zweifelhaft.

Der Staatshaushalt umfasste 2016 Ausgaben von umgerechnet 8,170 Mrd. US-Dollar, dem standen Einnahmen von umgerechnet 6,839 Mrd. US-Dollar gegenüber. Daraus ergibt sich ein Haushaltsdefizit in Höhe von 3,7 % des BIP.

Die Staatsverschuldung betrug 2016 17,2 Mrd. US-Dollar oder 48,7 % des BIP. Nach den Unruhen von 2010/2011 wurde die Elfenbeinküste im Februar 2011 zahlungsunfähig. Im Juni 2012 wurden ihr nach einem Abkommen mit den Gläubigern 7,7 Mrd. US-Dollar Schulden erlassen.

2006 betrug der Anteil der Staatsausgaben (in % des BIP) folgender Bereiche:

2013 betrug der Anteil der Staatsausgaben insgesamt 24,6 % des BIP, der Anteil der Staatseinnahmen 21,7 % des BIP. Für Sozialausgaben wurden 1,7 % des BIP aufgewendet.

Das Straßennetz der Elfenbeinküste ist im Vergleich westafrikanischer Staaten gut ausgebaut. Im Jahr 2000 hatte es eine Länge von 85.000 Kilometern, davon 75.500 Kilometer unbefestigt, 6500 Kilometer asphaltierte Straßen und 150 Kilometer Autobahn. Per Straße ist das Land mit seinen Nachbarn Ghana, Liberia, Mali und Burkina Faso verbunden. Die Autoflotte der Elfenbeinküste wird auf 600.000 Fahrzeuge geschätzt, davon drei Viertel Gebrauchtwagen aus anderen Ländern. Jedes Jahr gibt es 20.000 Neuzulassungen. Der öffentliche Verkehr wird fast zur Gänze auf der Straße abgewickelt, entweder in Linienbussen oder in Sammeltaxis, die in der Elfenbeinküste "Taxi-Brousse" heißen.

Etwa drei Viertel der Straßen befindet sich in "gutem Zustand", wobei die strategisch wichtige Nord-Süd-Verbindung in besonders schlechtem Zustand ist. Im Jahr 2009 wurde nur etwa ein Viertel dessen ausgegeben, was notwendig gewesen wäre, um das Straßennetz zu warten und wieder in Stand zu setzen.

Nur etwa ein Drittel der Landbevölkerung hat im Umkreis von zwei Kilometer Zugang zu einer ganzjährig befahrbaren Straße. Es wären etwa 20.000 km neuer Straßen notwendig, um 80 % der Ackerbaugebiete, und damit 50 % der Landbevölkerung, zu erschließen. Neben der Unterfinanzierung trägt auch die Duldung überladener LKWs zu schlechten Straßenzuständen bei. Korrupte Polizisten nehmen an den ivorischen Straßen jährlich schätzungsweise zwischen 200 und 290 Millionen US-Dollar an illegalem Wegzoll von Spediteuren und Reisenden ein. Dieser außerordentlich hohe Wert schwächt die Wettbewerbsfähigkeit der Elfenbeinküste als Transitland für den Handel seiner Nachbarstaaten.

Die Abidjan-Niger-Bahn verbindet das Land mit Ouagadougou, der Hauptstadt von Burkina Faso. Diese während der Kolonialzeit erbaute Strecke hat besonders für die Binnenstaaten Burkina Faso, Niger und Mali eine hohe Bedeutung. Sie ist etwa 1260 Kilometer lang, etwa die Hälfte davon verläuft innerhalb der Elfenbeinküste. Seit 1995 wird die Linie vom privaten Konsortium Sitarail betrieben, das seitdem Warentransport und Produktivität ständig steigern konnte und heute der erfolgreichste Bahnbetreiber Westafrikas ist, wenngleich die Indikatoren weit entfernt von jenen eines europäischen Betreibers liegen. Obwohl sich die Gesellschaft nach dem Bürgerkrieg, aufgrund dessen das Frachtvolumen von 800 auf 100 Millionen Tonnen eingebrochen war, erholt hat und nun jährlich knapp eine Million Tonnen befördert, war sie nicht in der Lage, die bei Konzessionsvergabe zugesagten Investitionen zu tätigen. Der Investitionsbedarf, um Anlagen und Fahrzeuge instandzusetzen und zu modernisieren, wird auf 230 Millionen US-Dollar im Zeitraum zwischen 2008 und 2020 geschätzt.

Es existieren drei internationale Flughäfen in der Elfenbeinküste, in Abidjan, Yamoussoukro und Bouaké. Daneben gibt es regionale Flughäfen in 14 weiteren Städten und 27 Flugfelder. Die meisten sind jedoch seit Ausbruch des Bürgerkrieges außer Betrieb. Seitdem zu Beginn der 2000er Jahre zahlreiche afrikanische Fluglinien bankrott gingen, hat sich das Angebot an Verbindungen von Abidjan aus stark verschlechtert, der Inlandsluftverkehr kam sogar ganz zum Erliegen. Der Luftverkehr der Elfenbeinküste hat wie jener seiner Nachbarländer ein Sicherheitsproblem: Keiner der Flughäfen und keine der Fluglinien hat die internationalen Sicherheitsaudits bestanden. Einheimische Fluglinien mussten in der Vergangenheit öfters den Flugbetrieb einstellen, wie Air Afrique 2002 und Air Ivoire 2011. Seit 2012 bietet Air Côte d’Ivoire Flüge innerhalb Westafrikas an.

In der Elfenbeinküste befinden sich zwei Seehäfen, der Port autonome d’Abidjan und der Port autonome de San-Pédro. Bis zum Jahr 2002 war der Hafen von Abidjan der wichtigste und größte Westafrikas. Er ist nicht nur für die Elfenbeinküste von Bedeutung, sondern auch für die der nördlich angrenzenden Binnenstaaten. 2005 wurden im Hafen von Abidjan 18,7 Millionen Tonnen umgeschlagen, im Hafen von San-Pedro eine Million Tonnen. Durch den Bürgerkrieg ist der Umschlag jedoch eingebrochen. Seit 2007 hat sich die Situation normalisiert, 2008 wurde ein Container-Terminal in Betrieb genommen und 2010 ein Programm gestartet, um mit 50 Millionen US-Dollar die Anlagen zu modernisieren. Somit schlägt der Hafen von Abidjan die Güter zwar schneller um als seine Mitbewerber in den Nachbarländern, hat aber auch höhere Kosten. Entscheidend für die Elfenbeinküste wird es sein, ob die Entwicklung und der Erhalt der Verkehrs-Infrastruktur im Hinterland gelingt.

Die Elfenbeinküste hat – wie viele andere afrikanische Staaten – einen Boom im Telekommunikationssektor erlebt. Im Jahr 2005 wurde ein Rahmen geschaffen, der zu starkem Wettbewerb unter den Anbietern von Mobilfunk-Leistungen führte. Da das gesamte Land kostendeckend mit Mobilfunk versorgt werden kann, gehört die Elfenbeinküste zu den für die Anbieter interessantesten afrikanischen Ländern. Der Zugang zu Mobilfunk stieg in der Folge von 9 % der Ivorer im Jahr 2005 auf 51 % im Jahr 2008. Die Preise sind jedoch im afrikanischen und internationalen Vergleich hoch. Die Festnetztelefonie spielt in der Elfenbeinküste seitdem keine nennenswerte Rolle mehr. Die Elfenbeinküste ist an das internationale Unterseekabel South Atlantic 3 angeschlossen. Da der staatliche Telekom-Betreiber das Monopol über diesen Knoten besitzt, sind die Preise für Internet-Zugang relativ hoch. 2016 nutzten 22 Prozent der Bevölkerung das Internet.

Im Jahr 2005 wurden 5,31 Milliarden kWh elektrische Energie erzeugt, davon etwa 73 % aus Wärmekraftwerken, die mit heimischem Erdgas betrieben werden. 27 % stammen aus Wasserkraft. Die Erzeugung, Übertragung, Verteilung, Abrechnung und der internationale Handel mit elektrischer Energie liegt bei der 1990 gegründeten Compagnie Ivoirienne d’électricité, die die Konzession dazu bis zum Jahr 2020 erhalten hat. Die Elfenbeinküste ist ein Exporteur von elektrischer Energie und war dies auch während der Krise; Abnehmer sind vor allem die Nachbarländer Ghana, Mali, Burkina Faso und Togo. Im Jahre 2005 hatte trotz allem weniger als die Hälfte der Bevölkerung Zugang zu Elektrizität, auf dem Land war es gar nur ein Viertel. Während des Bürgerkrieges wurden die Wartung und der Ausbau der Anlagen vernachlässigt, was sich in zahlreichen Stromausfällen äußert, speziell nach dem Ende des Krieges, als der Bedarf anstieg. Der Energiesektor der Elfenbeinküste produziert jährlich ein Defizit von 200 bis 300 Millionen US-Dollar, da er in seinem Netz hohe Verluste hat und die gestiegenen Brennstoffpreise nicht an die Verbraucher weitergeben kann. Für die Periode von 2006 bis 2015 wurde ein Finanzierungsbedarf von knapp 1 Milliarde US-Dollar ermittelt, um das System zu erhalten, neue Kapazitäten zu schaffen und 73 % der Bevölkerung mit elektrischer Energie zu versorgen.

Mehr als die Hälfte der armen Haushalte hat keinen Zugang zu sauberem Wasser, wobei dieser Prozentsatz im ländlichen Norden viel höher ist. Die Wasserversorgung wird seit 1959 von der privaten SODECI betrieben, die die Erweiterungen des Wassernetzes selbst finanziert hat und trotz aller Krisen einen stabilen Betrieb sichern konnte. Soweit sie ist aufgrund eines zu niedrigen Wasserpreises nicht in der Lage, ihre Kosten zu decken. Das Abwassersystem ist indes weit weniger entwickelt: etwa ein Drittel der Bevölkerung hatte 2008 nicht einmal Zugang zu einer Latrine.

Bekannt sind die traditionellen Holzmasken der im Westen des Landes siedelnden Yakuba (Dan), die ein idealisiertes menschliches Gesicht zeigen und im Schlammbad geschwärzt werden. Die Yakuba kennen eine große Zahl von Maskengestalten, die Buschgeister repräsentieren und verschiedene soziale, politische und religiöse Aufgaben erfüllen.

Die Elfenbeinküste hat, wie viele andere afrikanische Kulturräume auch, eine dichterische Tradition, die ausschließlich mündlich weitergegeben wurde. Schriftliteratur hingegen gibt es erst seit dem 20. Jahrhundert in französischer Sprache.

Die Elfenbeinküste hat eine für afrikanische Verhältnisse gut etablierte Verlagslandschaft und zahlreiche Autoren verschiedenster Genres mit unterschiedlichem Bekanntheitsgrad. Besonders lebhaft ist das Theater, wohl weil es im traditionellen Drama verwurzelt ist und auch wegen der hohen Analphabetenrate. Die bekanntesten Dramatiker sind François-Joseph Amon d’Aby, Germain Coffi Gadeau und Bernard Binlin Dadié, ein Journalist, Erzähler, Dramaturg, Romancier und Dichter, der die ivorische Literatur der 1930er Jahre dominierte. Wichtige Romanciers sind Aké Loba ("Ein schwarzer Student in Paris", 1960) und Ahmadou Kourouma "(Der schwarze Fürst)", der den Prix du Livre Inter im Jahre 1998 für sein Werk "Die Nächte des großen Jägers" erhielt, der ein Klassiker der afrikanischen Literatur ist.

Zur neueren Generation ivorischer Autoren zählen die in Paris geborene und heute in Johannesburg lebende Véronique Tadjo (* 1955), die Lyrikerin und Romanautorin Tanella Boni (* 1954) und die beiden ungeheuer produktiven Autoren Isaie Biton Koulibaly (* 1949) und Camara Nangala (* 1955).

Die verschiedenen Ethnien der Elfenbeinküste haben teilweise unterschiedliche musikalische Traditionen, so dass die traditionelle Musik des Landes recht mannigfaltig ist. In vielen Musikstilen gibt es polyphonen Gesang oder zweistimmigen Call and Response, häufig zusammen mit dem polyrhythmischen Einsatz von Rasseln, Glocken, einfachen Trommeln oder Talking Drums. Bei den Senufo wird der Gesang zumeist von einem Balafon begleitet, bei den Dan eher von schnarrenden Trommeln. Zu den sehr alten Instrumenten gehören Flöten, hölzerne Eintonhörner, Schlitztrommeln, Xylofone, dreieckige Rahmenzithern und Musikbögen.

Bereits Ende des 19. Jahrhunderts wurden Eintonhörner durch westliche Blasinstrumente ersetzt. Aus dem britischen Militärmusikerbe, das im Nachbarland Ghana gepflegt wurde, der Zeremonialmusik lokaler Clanchefs und mit aus Frankreich eingeführten Instrumenten entwickelte sich die lebhafte Musik großer Repräsentationskapellen, wie die "Sankro Brass Band," die "Asiakwa Brass Band" oder "Les Fanfares de Sankadiokro."

Als Vater der heutigen ivorischen Popmusik gilt Ernesto Djédjé, der Rhythmen der Bété populär machte; seinen Musikstil nannte er Ziglibithy. Er ist für seinen Hit "Gnoantre-Ziboté" (1977) auch außerhalb des Landes bekannt geworden. Nach ihm kamen Luckson Padaud mit dem Laba-laba-Stil, und Gnaore Djimi mit Polihet. In den 1990er Jahren entstand der Zoblazo, indem Meiway traditionelle Rhythmen aus der südlichen Elfenbeinküste mit elektronischen Instrumenten und Unterhaltungslyrik versetzte. Weitere sehr junge Stilrichtungen sind Zouglou "(Magic System)" und Coupé Decalé. Es gibt keine nationale Musikkultur, die Elfenbeinküste ist aber Gastland für viele Musiker der Nachbarländer, die in Abidjan die besseren Studiomöglichkeiten finden.

Die populärsten ausländischen Musikstile, die in die Elfenbeinküste kamen, sind Reggae und Hip-Hop. Die beiden wichtigsten Reggae-Künstler des Landes sind Alpha Blondy, dessen Afro-Reggae seit seinem Auftreten in der Fernsehshow "First chance" (1983) in ganz Westafrika populär wurde, und Tiken Jah Fakoly, der wegen seiner politischen Texte ins Exil gehen musste. Bedeutende ivorische Hip-Hop-Musiker sind All Mighty, Rudy Rudiction, M. C. Claver und Angelo.

In der Elfenbeinküste sind zahlreiche Bauwerke aus kolonialem Erbe erhalten. Dazu zählen zunächst der Palais du Gouverneur in Grand-Bassam, der in Frankreich vorgefertigt und 1893 in der Elfenbeinküste aufgebaut und erweitert wurde. In Brand-Bassam stehen viele weitere pittoreske Gebäude in kolonialem Stil, wie das "maison Varlet" oder das "maison Ganamet", die von reichen Händlern gebaut wurden, die einheimische Baumaterialien verwendeten.

Im Norden des Landes sind einige Moscheen im sudanesischen Stil erhalten geblieben, der während der Herrschaft des Malireiches in dieser Region eingeführt wurde. Die bedeutendsten dieser Bauwerke sind die Moschee von Kaouara (Département Ouangolodougou), die Moschee von Tengréla, die Moschee von Kouto, die Moschee von Nambira (Unterpräfektur M’Bengué), und speziell die beiden Moscheen von Kong.

Moderne Religiöse Bauwerke sind die Cathédrale Saint-Paul in Abidjan und die Notre-Dame-de-la-Paix de Yamoussoukro in Yamoussoukro.

Die Völker der Elfenbeinküste haben eine lange Tradition, Gebrauchsutensilien, Statuen oder Masken aus verschiedenen Materialien künstlerisch herzustellen. Aus Holz, Bronze, Raphia, Rattan oder auch Bambus werden Körbe, Skulpturen, Möbel, Masken oder Statuen hergestellt.

Die Masken der Dan, Baoulé, Gouro, Guere oder Bété sind die bekanntesten. Die Baoulé verstehen sich sehr gut auf die Weberei und die Sénoufo sind, unter anderem, bekannt für ihre Malereien auf Stoff. Kleine Figuren aus Kupfer, die früher zum Wiegen von Gold benutzt wurden, sind heute Verzierung, besonders bei den Akan. Die Katiola wiederum sind berühmt für ihre Töpfereierzeugnisse, die von den Frauen in Handarbeit hergestellt werden.

Viele Kunstartikel kommen heute in den touristisch geprägten Städten an der Küste (also Grand-Bassam oder Assinie) zum Verkauf.

Während die traditionelle Volkskunst eher anonym ist, stammen auch einige bekannte Künstler aus der Elfenbeinküste, etwa die Maler Gilbert G. Groud oder Michel Kodjo, die häufig beachtete Werke hervorbringen, oder der Karikaturist Zohoré Lassane, der das Humor- und Satireblatt "Gbich!" gegründet hat.

Die Küche der Elfenbeinküste ist aufgrund der vielfältigen ethnischen Zusammensetzung des Landes ebenfalls sehr facettenreich, hat aber viele Ähnlichkeiten mit der Küche der anderen westafrikanischen Staaten. Als Grundnahrungsmittel finden Getreide und Wurzeln Verwendung, vor allem Reis, Mais, Hirse, Grieß, Maniok, Yams, Taro, Süßkartoffeln und auch Kochbananen. Wichtigster Fleischlieferant ist das Geflügel, seltener Rind oder Schwein, an der Küste auch Fisch und Meeresfrüchte. Als Gemüse werden Zwiebeln, Tomaten, Auberginen, Bohnen, Avocados, Karotten, Okra und Spinat bevorzugt.
Das tropische Klima bietet zahlreiche Früchte wie Bananen, Papaya, Ananas, Granatapfel, Kokosnuss, Mangos, Apfelsinen, Mandarinen, Melonen, Brotfrüchte, Guaven, Zitronen, Orangen und Grapefruits. Das Essen ist in der Regel scharf bis sehr scharf gewürzt und wird mit den Fingern gegessen. Spezialitäten sind z. B. Attiéké, eine Art Couscous aus Maniok, oder Alloco, frittierte Kochbananenchips.

In der Elfenbeinküste sind, wie in vielen anderen westafrikanischen Staaten auch, "Maquis" sehr verbreitet, wo einfaches Essen in der Regel unter freiem Himmel serviert wird.

Das wichtigste Medium in der Elfenbeinküste ist das Radio. Die staatliche Radiodiffusion-Télévision ivoirienne betreibt zwei Stationen namens La Chaine Nationale und Frequence 2. Daneben gibt es speziell in den Städten zahlreiche Privatsender (etwa Radio Nostalgie in Abidjan) und die ländlichen Gegenden werden von nicht-kommerziellen Sendern mit wenig Leistung abgedeckt, die teils auch von der römisch-katholischen Kirche betrieben werden. Die ONUCI betreiben den Sender Onuci FM. Neben den einheimischen Sendern werden ausländische Stationen wie Radio Africa No. 1, Radio France Internationale oder BBC Afrique empfangen.

Radiodiffusion-Télévision ivoirienne betreibt auch zwei Fernsehsender, nämlich La Première und TV2. Privatfernsehen gibt es in der Elfenbeinküste nicht.

Printmedien haben eine sehr geringe Verbreitung. Die wichtigsten Zeitungen sind die staatliche "Fraternité Matin", die privaten "Soir Info", "Le Nouveau Reveil", "L’Inter", das Oppositionsblatt "Le Patriote", "Notre Voie" der Regierungspartei und "Nord-Sud". Erstere hat eine Auflage von 30.000, die Letzteren kommen über 10.000 nicht hinaus.

Die staatliche Nachrichtenagentur heißt Agence Ivoirienne de Presse.

Bei der Rangliste der Pressefreiheit 2017, welche von Reporter ohne Grenzen herausgegeben wird, belegte die Elfenbeinküste Platz 81 von 180 Ländern. Bei der Situation der Pressefreiheit im Land gibt es laut der Nichtregierungsorganisation „erkennbare Probleme“.

Der wichtigste und meist betriebene Sport in der Republik Côte d’Ivoire ist der Fußball. Die ivorische Fußballnationalmannschaft ist derzeit eine der zehn erfolgreichsten Nationalmannschaften Afrikas. Die größten Erfolge bei internationalen Turnieren waren bisher der Gewinn des Afrika-Cups 1992 und 2015, zwei zweite Plätze 2006 und 2012, ein vierter Platz beim Konföderationen-Pokal 1992, dritte Plätze bei den Afrika-Cups 1965, 1968, 1986 und 1994 und ein vierter Platz 1970. Am 8. Oktober 2005 qualifizierte sich die Mannschaft, neben den Mannschaften Tunesiens, Togos, Ghanas und Angolas, für die Fußball-Weltmeisterschaft 2006, ein bedeutender Meilenstein in der ivorischen Fußballgeschichte. Dort errang die Mannschaft einen 3:2-Sieg gegen die Auswahl von Serbien und Montenegro. Bei den WM-Qualifikationen 2010 und 2014 gelang der Elfenbeinküste als Tabellenerster die Teilnahmen an den Endrunden in Südafrika und in Brasilien.




</doc>
<doc id="1327" url="https://de.wikipedia.org/wiki?curid=1327" title="Ennio Morricone">
Ennio Morricone

Ennio Morricone (* 10. November 1928 in Trastevere, Rom) ist ein italienischer Komponist und Dirigent. Er hat auch unter den Pseudonymen Dan Savio und Leo Nichols gearbeitet und komponierte die Filmmusik von mehr als 500 Filmen. Sein Name wird vornehmlich mit dem Filmgenre des Italo-Western in Verbindung gebracht, da er die Musik für 30 solcher Filme geschrieben hat.

Besondere Bekanntheit erlangten seine Filmmusiken zum Italowestern-Klassiker "Zwei glorreiche Halunken" (Original: "Il buono, il brutto, il cattivo"), zum Western-Epos "Spiel mir das Lied vom Tod", zu Roland Joffés Drama "Mission" und für Giuseppe Tornatores Film "Cinema Paradiso". 2007 erhielt Morricone den Oscar für sein Lebenswerk sowie 2016 für die Musik zum Film "The Hateful Eight".

Morricone studierte im Konservatorium von Santa Cecilia Trompete und Chormusik und erhielt 1946 sein Konzertdiplom als Trompeter. Ein Jahr später folgte ein erstes Engagement als Theaterkomponist. 1953 begann er mit der Gestaltung des Abendprogramms eines italienischen Rundfunksenders. Für seine Ausbildung als Komponist am Konservatorium, die er 1954 mit einem Diplom abschloss, zeichnete Goffredo Petrassi verantwortlich. 1956 heiratete er Maria Travia. Er etablierte sich ab Mitte der 50er Jahre mit Kammermusik- und Orchesterwerken in der musikalischen Avantgarde seines Landes. 1958 besuchte Morricone die Internationalen Ferienkurse für Neue Musik in Darmstadt. Im selben Jahr unterschrieb Morricone einen Arbeitsvertrag als Musikassistent bei der staatlichen Rundfunkanstalt Radiotelevisione Italiana, wo er als Arrangeur tätig war. Auch für zahlreiche Schallplattenaufnahmen im Genre Pop (beispielsweise für Gino Paoli) schrieb er Arrangements und leitete Band und Orchester.

Morricone komponierte 1961 seine erste Filmmusik für Luciano Salces "Il Federale", doch ließ der internationale Erfolg noch einige Jahre auf sich warten. 1964 begann er seine erfolgreiche Zusammenarbeit mit Sergio Leone (die beiden gingen in dieselbe Schulklasse) und Bernardo Bertolucci. In dieser Zeit schrieb er unter anderem die Musik für Leones Filme "Für eine Handvoll Dollar", "Zwei glorreiche Halunken" und "Spiel mir das Lied vom Tod". Morricones Kompositionen unterschieden sich stark von den traditionellen symphonischen Western-Soundtracks aus Hollywood und wirkten durch ihre ungewöhnlichen Soundelemente (Maultrommeln, Pfiffe, Schreie, Kojotengeheul, Eulenrufe, Glocken, Spieluhren, Peitschenknallen, Schläge auf Amboss etc.) stilbildend und innovativ. Mit einigen seiner Kompositionen konnte der Komponist sogar Hitparadenerfolge verbuchen. Im Genre des Italo-Westerns orientierten sich zahlreiche Komponisten an dem von Morricone entwickelten Stil.

Seit Mitte der 1990er Jahre komponierte Morricone in jedem Jahr die Filmmusik für etwa 15 Filme. Außerdem spielte er von 1964 bis in die siebziger Jahre im von Franco Evangelisti initiierten Improvisationsensemble Gruppo di Improvvisazione Nuova Consonanza. Zusammen mit anderen Komponisten gründete Morricone 1984 in Rom das I.R.TE.M, eine Forschungsanstalt für musikalisches Theater.

In mehr als vierzig Jahren künstlerischen Schaffens schrieb Morricone über 500 Filmmusiken und arbeitete dabei mit namhaften italienischen und internationalen Regisseuren zusammen. Er dirigierte eine große Anzahl von Orchestern, wobei er für zahlreiche Konzerte sowie Filmmusikaufnahmen mit dem Roma Sinfonietta Orchestra zusammenarbeitete. Mit diesem Ensemble gab Morricone am 2. Februar 2007 auch ein Ehrenkonzert zum Amtsantritt des Generalsekretärs der Vereinten Nationen, Ban Ki-moon.

Neben Film- und Bühnenmusik hat er aber auch weiterhin Kammermusiken für Solisten (Gitarre, Klavier, Violine, Cello) und diverse Formationen (Trio, Quintett, Sextett, Piano und Instrumente, Gesang und Instrumente, Chöre etc.) sowie Kantaten und Messen geschrieben:


Sein Sohn Andrea Morricone ist ebenfalls Komponist. Beide komponierten für "Cinema Paradiso".

Morricone trug mit seiner Musik wesentlich zum Welterfolg der Leone-Western bei, die heute Kultfilme sind, wie zum Beispiel "Spiel mir das Lied vom Tod". Leone bekannte einmal, dass Morricone für ihn mehr ein Drehbuchautor sei als ein Komponist, denn durch die Musik könne er etwas mitteilen, was er sonst hätte zeigen müssen. Und vom Komponisten ist überliefert:

Oscars (Academy Awards)

Golden Globes

Goldene Himbeere

BAFTA Award

David di Donatello

Grammy

ASCAP Award

Nastro d’Argento für die beste Filmmusik ("Nastro d’Argento/Migliore colonna sonora")

Weitere Auszeichnungen

Darüber hinaus wurde er mit dem Verdienstorden der Italienischen Republik ausgezeichnet und in die Ehrenlegion aufgenommen.
2016 wurde er mit einem Stern auf dem Hollywood Walk of Fame verewigt.

Weltweit wurden mehr als 50 Millionen Alben von Morricone verkauft. Seine Kompositionen werden teilweise in andere Genres übertragen:




</doc>
<doc id="1328" url="https://de.wikipedia.org/wiki?curid=1328" title="Emil Nolde">
Emil Nolde

Emil Nolde (* 7. August 1867 als Hans Emil Hansen in Nolde bei Buhrkall, Provinz Schleswig-Holstein; † 13. April 1956 in Seebüll) war einer der führenden Maler des Expressionismus. Er ist einer der großen Aquarellisten in der Kunst des 20. Jahrhunderts. Nolde ist bekannt für seine ausdrucksstarke Farbwahl.

Emil Nolde wurde einige Kilometer östlich von Tondern im Ortsteil Nolde von Buhrkall (heute Burkal) als viertes von fünf Kindern geboren. Sein Geburtsort Buhrkall im nördlichen Teil des Herzogtums Schleswig gehörte von 1867 bis 1920 zu Preußen und damit zum Deutschen Reich. Sein Vater war Nordfriese und stammte aus der Gegend um Niebüll; er sprach nordfriesisch, seine Mutter sprach südjütisch. Emil Nolde besuchte deutsche Schulen. Seine Jugendjahre auf dem elterlichen Hof in Nolde waren geprägt von harter Arbeit und einem relativ kargen Leben. Er war der jüngste von drei Brüdern und hatte eine jüngere Schwester.

Von 1884 bis 1888 ließ er sich als Schnitzer und Zeichner an der Kunstgewerbeschule in Flensburg (heute Museumsberg Flensburg) ausbilden. Er war dort an der Restaurierung des Brüggemann-Altars beteiligt. Danach arbeitete er für verschiedene Möbelfabriken, unter anderem in München, Karlsruhe und Berlin. 1892 trat er am Gewerbemuseum in St. Gallen eine Stellung als Lehrer für gewerbliches und ornamentales Entwurfszeichnen an, die er bis 1897/1898 innehatte. In dieser Zeit lernte er Hans Fehr kennen, mit dem er lange verbunden blieb. Er arbeitete zunächst an einer Reihe von Landschaftsaquarellen und Zeichnungen der Bergbauern. Nolde wurde schließlich durch kleine farbige Zeichnungen der Schweizer Berge bekannt. Er ließ Postkarten dieser Arbeiten drucken, die ihm dann ein Leben als freier Künstler erlaubten.

Er ging nach München, wurde allerdings von der Akademie abgelehnt und begann zunächst ein Studium an der privaten Malschule Adolf Hölzels in Dachau, bevor er im Herbst 1899 mit der Malerin Emmi Walther über Amsterdam nach Paris reiste und sich an der Académie Julian anmeldete. 1900 mietete er ein Atelier in Kopenhagen. 1902 heiratete er dort die 23 Jahre alte dänische Schauspielerin Ada Vilstrup (1879–1946). Mit ihr zog er auf die Insel Alsen. Dort wohnten sie zwischen 1903 und 1916 sommers in einem Fischerhaus in Sjellerupskov bei Guderup. Als Atelier diente eine Bretterbude direkt am Strand. Ab 1905 lebte er im Winter meist in Berlin, zunächst Tauentzienstraße, dann Bayernallee 11 in Berlin-Westend.

Seit 1902 nannte sich Nolde nach seinem nordschleswigschen Heimatdorf. Um 1903 malte er noch „lyrische“ Landschaften. Er wurde Mitglied der Schleswig-Holsteinischen Kunstgenossenschaft und nahm zwischen 1903 und 1912 an fünf Ausstellungen teil. 1904 war er auf der Jahresausstellung im Flensburger Museum mit den Gemälden „In der Räuberstube“ und „Sommernacht“ vertreten. Von 1906 bis 1907 war er Mitglied der Künstlergruppe "Brücke" und begegnete in Berlin Edvard Munch. In der kurzen Zeit, in der er „Brücke“-Mitglied war, brachte er die Radierung in die Gemeinschaft und vermittelte Kontakte zu dem Hamburger Sammler und Kunstmäzen Gustav Schiefler. 1909 wurde Nolde Mitglied der Berliner Secession.

Als deren Jury im Jahr darauf Werke Georg Tapperts und vieler meist expressionistischer Künstler zurückwies, kam es zum Bruch der Berliner Secession. Auf Initiative Tapperts, gefolgt von Max Pechstein und weiteren Künstlern, so auch Nolde, bildete sich die Neue Secession. Sie eröffnete am 15. Mai ihre erste Ausstellung unter dem Titel „Zurückgewiesene der Secession Berlin 1910“.

Erste religiöse Bilder entstanden: "Abendmahl," "Pfingsten" und "Verspottung". Zwischen 1910 und 1912 hatte er erste Erfolge mit Ausstellungen in Hamburg, Essen und Hagen. Bilder vom Nachtleben in Berlin, wo er gemeinsam mit seiner Frau Ada regelmäßig die Wintermonate verbrachte, Theaterzeichnungen, Maskenstillleben, 20 "Herbstmeere", das neunteilige "Das Leben Christi" entstanden. Von Herbst 1913 bis Ende August 1914 unternahm er als Mitglied der Medizinisch-demographischen Deutsch-Neuguinea-Expedition des Reichskolonialamtes eine Reise in die Südsee. Während der Rückreise begann der Erste Weltkrieg. Nolde zog 1916 in das kleine Bauernhaus "Utenwarf" () an der Westküste nahe Tondern und der Vidå (dt. Wiedau). Die heftigen Auseinandersetzungen um die deutsch-dänische Grenzziehung nach dem Ersten Weltkrieg waren ihm zuwider, und obwohl er sich als Deutscher fühlte, nahm er komplikationslos die dänische Staatsbürgerschaft an, als sein Geburtsort nach der Volksabstimmung in Schleswig 1920 an Dänemark fiel. Nolde behielt bis an sein Lebensende die dänische Staatsbürgerschaft, verstand sich aber zeitlebens als Angehöriger der deutschsprachigen Minderheit in Nordschleswig.

Erst als das Land um Utenwarf zunehmend erschlossen und entwässert wurde, zog er mit seiner Frau auf die deutsche Seite der Grenze, da ihn dort die Landschaft an seine Heimat bei "Nolde" erinnerte. Das Ehepaar erwarb 1926 eine leerstehende Warft, die sie Seebüll nannten und auf der bis 1930 das gleichnamige Wohn- und Atelierhaus des Malers erbaut wurde, gelegen nahe Neukirchen im Amt Wiedingharde des damaligen Kreises Südtondern. Sie wohnten zunächst in dem benachbarten Bauernhaus „Seebüllhof“, das sie gemeinsam mit der Warft und den umliegenden Weideflächen erworben hatten. Der Umzug in das neu erbaute Haus „Seebüll“ erfolgte im Jahr 1930. Das Wohngebäude ist ein zweigeschossiger Kubus mit Flachdach, an den eingeschossige Anbauten über dreieckigem Grundriss angefügt sind. 1937 wurde dem Wohnhaus ein Atelierhaus mit Bildersaal angefügt. Der Bau wurde nach Entwürfen Emil Noldes aus Backstein errichtet. Die Farbigkeit im Innern des Wohnhauses korrespondiert mit den kräftigen Farben der Gartenpflanzen.

Neben dem Haus legten Ada und Emil Nolde einen Garten an, dessen Wege in Form der Initialen E und A verlaufen. Zum Garten gehören zwei Gebäude: ein 1935/1936 errichtetes, reetgedecktes Gartenhaus, das sogenannte „Seebüllchen“, sowie die Begräbnisstätte von Ada und Emil Nolde. Diese befindet sich in einem ehemaligen Erdschutzbunker, der 1946, als Ada starb, in eine Gruft umgewandelt wurde. An der Stirnwand schuf Nolde das Mosaik "Madonna mit Kind". Der Garten Noldes ist ein individuelles Gartenkunstwerk, das die zeitgenössische Reformbewegung aufnimmt, die sich gegen industrielle und genormte Kunstformen richtet. So entstand in der weiten Marschlandschaft in Bepflanzung und Ausstattung ein recht geschlossener, heimatbezogener Bauerngarten, auch wenn dieser keine für diese Gärten typische auf das Haus bezogene Mittelachse aufweist und Haus und Garten getrennte Einheiten bilden.

Zu seinem 60. Geburtstag wurde ihm 1927 eine Jubiläumsausstellung in Dresden gewidmet.

Nolde war früh der Überzeugung, die „germanische Kunst“ sei allen anderen weit überlegen. Im August 1934 bezeugte er mit seiner Unterschrift unter den Aufruf der Kulturschaffenden, dass er "zu des Führers Gefolgschaft" gehörte. Er wurde 1934 Mitglied einer der verschiedenen nationalsozialistischen Parteien in Nordschleswig, der Nationalsozialistischen Arbeitsgemeinschaft Nordschleswig (NSAN). Die konkurrierenden nationalsozialistischen Parteien wurden 1935 aufgrund von Bemühungen des Gauleiters Hinrich Lohse in Schleswig-Holstein zur NSDAP-Nordschleswig (NSDAP-N) zusammengefasst.

Nolde war auch antisemitisch eingestellt, wie aus vielen Dokumenten hervorgeht – so auch aus seinem Buch "Jahre der Kämpfe," das 1934 erschien. Er kämpfte gegen jüdische Kunsthändler wie Paul Cassirer und Maler wie Max Liebermann. Zugleich sprach er sich dagegen aus, moderne französische Malerei der Stilrichtungen Impressionismus, Kubismus, Surrealismus, Fauvismus und Primitivismus im deutschen Kunsthandel anzubieten. Eine Äußerung galt Rosa Schapire, einer Kunsthistorikerin, die den noch unbekannten Künstler durch Vorträge und Ausstellungsberichte gefördert hatte:
Zu Beginn der Zeit des Nationalsozialismus schätzten Teile der Nazi-Führung seine Kunst und seine kunstpolitische Einstellung. 1933 veranstaltete der NS-Studentenbund eine Ausstellung mit seinen Werken. Auch Joseph Goebbels gehörte neben Albert Speer zu den Förderern von Nolde.

Trotzdem wurde Nolde, ohne dass er das ernstgenommen hätte, von einem Teil der NS-Führung verfolgt – dazu gehörten Alfred Rosenberg und Adolf Hitler selbst. Nolde war daher sehr überrascht, dass seine Werke von den Nationalsozialisten als „Entartete Kunst“ diffamiert wurden. Unter anderem wurde Noldes Gemälde "Leben Christi" in der Ausstellung „Entartete Kunst“ im Jahr 1937 gezeigt. Weitere Gemälde wurden in der folgenden Aktion „Entartete Kunst“ beschlagnahmt und im Rahmen der Verwertung verkauft. Noch am 2. Juli 1938 machte Nolde in einem Schreiben an Goebbels geltend, dass er sich „als fast einziger deutscher Künstler im offenen Kampf gegen die Überfremdung der deutschen Kunst“ sähe, und wies darauf hin, dass er sofort nach der Gründung der NSDAP-Nordschleswig deren Mitglied geworden sei.

Die Verfolgung im Rahmen der nationalsozialistischen Kunstpolitik bedeutete jedoch nicht den Abbruch von Noldes Karriere. Die beiden Bände seiner Biographie blieben verfügbar und verkauften sich abgesehen von einem Einbruch 1938 weiterhin gut. Er setzte sich für die Rückgabe der beschlagnahmten Leihgaben ein und erhielt diese auch zurück. Seine Werke wurden sogar aus der Wanderausstellung „Entartete Kunst“ entfernt. Auch finanziell bedeutete das Jahr 1937 keine Zäsur für Nolde. Im Frühjahr 1937 veranstaltete die Galerie Ferdinand Möller in Berlin eine Ausstellung seiner Aquarelle, in der Arbeiten für 20.000 Reichsmark verkauft wurden. Die finanzielle Lage Noldes war zu dieser Zeit so gut, dass der ehemalige Direktor des Museum Folkwang in Essen, Ernst Gosebruch, vermerkte, dass der Künstler seine Hauptwerke im eigenen Besitz behielt, da er nicht zu deren Verkauf gezwungen sei. Auch seine Verfemung in der Münchner Ausstellung 1937 bedeutete keine Veränderung der Nachfragesituation. Auch nach 1937 waren Werke Noldes in vielen deutschen Galerien für moderne Kunst weiterhin Kommissionsware. Finanziell gehörte Nolde zu den erfolgreichsten deutschen Künstlern der 1930er und 1940er Jahre. Abgesehen von 1938 hatte er zwischen 1937 und 1941 seine höchsten Jahreseinkommen zu verzeichnen. Die Steuerakten weisen noch höhere Einnahmen aus als er im Rahmen des Entnazifizierungsverfahren angab. Nach dem Krieg meldete er für das Jahr 1941 Einnahmen von über 50.000 Reichsmark. Laut Daten der Reichskammer der bildenden Künste verdienten im Jahr 1939 nur 0,7 % der Künstler im Deutschen Reich mehr als 1000 Reichsmark im Monat. Seine gute wirtschaftliche Lage brachte Nolde Neider aus der Künstlerschaft ein und verdeutlichte den Kulturfunktionären zudem, dass sie ihre Politik auf dem Kunstmarkt nicht hatten durchsetzen können.

Unter diesen Vorzeichen setzte die Reichskammer der bildenden Künste ab Oktober 1940 die Anordnung gegen den „Vertrieb minderwertiger Kunsterzeugnisse“ ein. Diese sollte sich eigentlich gegen billige und massenhaft produzierte Kunstreproduktionen und sogenannten „Kitsch“ richten, um so den Markt für Künstler zu schützen. Aufgrund dieser Vorzeichen gingen die Noldes auch erst einmal davon aus, dass die Verordnung sie nicht betreffen würde. Dennoch fragte die Reichskammer der bildenden Künste wegen Informationen zu Verkäufen und Ausstellungen an und forderte Abbildungen von Werken der Jahre 1938 bis 1940 an. In dieser Situation machten sich die Noldes ihre Kontakte zu hochrangigen Nationalsozialisten zu nutze. So baten sie Heinrich Hansen, einen der höchstrangigen Offiziellen im Reichsministerium für Volksaufklärung und Propaganda, um Unterstützung. Im Februar 1941 fragte die Reichskammer erneut wegen Abbildungen an. Im gleichen Monat ließ Hans Schweitzer in der Galerie von Alex Vömel in Düsseldorf ein Gemälde und Aquarelle Noldes beschlagnahmen, die zur Begutachtung nach Berlin geschickt wurden. Auch der Sicherheitsdienst des Reichsführers SS übte wegen der weiteren Verfügbarkeit von „entarteten“ Kunstwerken erhöhten Druck auf die Reichskammer auf. Am 23. August 1941 erhielt Nolde das Schreiben Adolf Zieglers, in dem er wegen „mangelnder Zuverlässigkeit“ aus der Reichskammer der bildenden Künste ausgeschlossen wurde.

Dieser Ausschluss bedeutete jedoch kein Malverbot wie es insbesondere nach Ende des Zweiten Weltkriegs kolportiert wurde, sondern lediglich das Verbot von Verkäufen, Ausstellungen und Reproduktionen seiner Werke. Privat konnte Nolde weiterhin malen, von ihm konsultierte Juristen meinten zudem, dass Schenkungen an Freunde wohl keine Umgehung des Berufsverbots bedeutet hätten. Um seine Werke wieder in der Öffentlichkeit verbreiten zu können, hätte er sie erneut dem Ausschuss zur Kontrolle der Kunsterzeugnisse vorlegen müssen. Die irrtümliche Bezeichnung als Malverbot findet sich für die Zeit des Nationalsozialismus nur in einem einzigen Brief von Ada Nolde. Erst nach dem Krieg wurde das Berufs- zum Malverbot hin überformt, damit Nolde seine eigene Opferrolle betonen konnte. Im Rahmen der Rehabilitation des Expressionismus wurde dieses Narrativ von vielen Autoren aufgegriffen und weitergetragen. In diesem Kontext entstanden die sogenannten "Ungemalten Bilder" und deren Rezeptionsgeschichte. Die Opfergeschichte Noldes wurde in der Figur des Malers Max Ludwig Nansen im Roman "Deutschstunde" von Siegfried Lenz rezipiert. Die Noldes vervielfältigten den Brief Zieglers mit dem Ausschluss und ließen ihn unter Unterstützern zirkulieren. Diese reagierten, indem sie ihn in der Folge bei der Materialbeschaffung unterstützten. Beispielsweise ließ Otto Andreas Schreiber ihm regelmäßig Farben zukommen. Auch wurde Noldes Vertrauen in den Nationalsozialismus trotz aller Verfolgungserfahrungen nie vollständig zerstört. 1942 kam zwar ein Treffen mit Baldur von Schirach in Wien nicht zustande, dieser nahm jedoch einige seiner Werke bei sich auf und versprach, sich für den Künstler einzusetzen. Und noch 1943 dachte er darüber nach, einen SA-Mann zu malen. 1944 wurde Noldes Wohnung in Berlin durch Bomben zerstört.

Nach 1945 erhielt Nolde zahlreiche Auszeichnungen und Ehrungen. Im Jahr 1946 starb seine erste Frau; zwei Jahre später heiratete er Jolanthe Erdmann (* 9. Oktober 1921 in Berlin; † 13. Juni 2010 in Heidelberg), Tochter des Komponisten und Pianisten Eduard Erdmann. Bis 1951 malte er noch über 100 Gemälde und bis 1956 viele Aquarelle. Emil Nolde war Teilnehmer der documenta 1 (1955), seine Werke wurden auch posthum auf der documenta II (1959), und auf der documenta III im Jahr 1964 in Kassel gezeigt. Emil Nolde starb am 13. April 1956 in Seebüll, wo er – neben seiner 1946 verstorbenen ersten Frau Ada – im Garten seine letzte Ruhestätte fand.

Emil Nolde war Vorstandsmitglied im Deutschen Künstlerbund.


Anwesen und künstlerischer Nachlass wurden Ausgangsvermögen der "Stiftung Seebüll Ada und Emil Nolde", die im ehemaligen Wohn- und Atelierhaus des Malers das Nolde-Museum errichtete. Die Stiftung präsentiert dort in jährlich wechselnden Ausstellungen rund 160 Werke von Nolde. Im ehemaligen Atelier des Malers hat sein bedeutendstes religiöses Werk – das neunteilige Altarwerk "Das Leben Christi" von 1911/1912 – seinen festen Platz gefunden. Zum 50. Todesjahr Noldes war die Ausstellung 2006 dem Alterswerk gewidmet. Die Ausstellungen hier und im daneben errichteten Dokumentations- und Veranstaltungsgebäude ziehen jedes Jahr rund 80.000 Besucher an.

Von 2007 bis März 2014 gab es eine Dependance der Stiftung in der Jägerstraße 54/55 am Gendarmenmarkt in Berlin. Dort wurden im Rahmen von wechselnden Ausstellungen neben Werken von Nolde auch Exponate anderer Künstler präsentiert.

Das Leben Emil Noldes in der Zeit des „Malverbots“ ab 1941 spiegelt sich in dem Roman und Film "Deutschstunde" von Siegfried Lenz wider. In dem Buch "Nolde und ich. Ein Südseetraum" von Hans Christoph Buch wird von Noldes Reise in die Südsee erzählt. "Träume am Meer – Der Maler Emil Nolde" unter der Regie von Wilfried Hauke wurde 2006 gedreht.

Der Hamburger Richter und Kunstsammler Gustav Schiefler erstellte den ersten zweibändigen Katalog des Graphischen Werkes von Nolde.






</doc>
<doc id="1329" url="https://de.wikipedia.org/wiki?curid=1329" title="Eiffelturm">
Eiffelturm

Der Eiffelturm (, ) ist ein 324 Meter hoher Eisenfachwerkturm in Paris. Er steht im 7. Arrondissement am nordwestlichen Ende des Champ de Mars (Marsfeld), nahe dem Ufer der Seine. Das von 1887 bis 1889 errichtete Bauwerk wurde als monumentales Eingangsportal und Aussichtsturm für die Weltausstellung zur Erinnerung an den 100. Jahrestag der Französischen Revolution errichtet. Der nach dem Erbauer Gustave Eiffel benannte und zum Errichtungszeitpunkt noch 312 Meter hohe Turm war von seiner Erbauung bis zur Fertigstellung des Chrysler Building 1930 in New York das höchste Bauwerk der Welt. Mit der Ausstrahlung des ersten öffentlichen Radioprogramms 1921 in Europa und des ersten französischen Fernsehprogramms 1935 trug das Bauwerk als Sendeturm zur Geschichte des Hörfunks und des Fernsehens bei. Der Fernsehturm ist die wichtigste Sendeanlage des Großraums Paris und beherbergt als Turmrestaurant das mit einem Michelin-Stern ausgezeichnete Restaurant "Le Jules Verne".

Als höchstes Bauwerk von Paris prägt er das Stadtbild bis heute und zählt mit rund sieben Millionen zahlenden Besuchern pro Jahr zu den meistbesuchten Wahrzeichen der Welt. Der Turm ist eine der bekanntesten Ikonen der Architektur und Ingenieurskunst. Der Eiffelturm ist das Vorbild vieler Nachahmerbauten und wird in Kunst und Kultur im Zusammenhang mit Paris und Frankreich vielfach aufgegriffen. Er gilt als nationales Symbol der Franzosen und avancierte zu einer weltweiten Ikone der Moderne. Seit 1964 ist der Eiffelturm als "monument historique" denkmalgeschützt, und 1986 nahm die American Society of Civil Engineers das Bauwerk in die Liste der historischen Meilensteine der Ingenieurbaukunst auf.

Mit den technischen Möglichkeiten der Industrialisierung kamen auch Ideen auf, hohe Bauwerke zu errichten. Insbesondere Turmbauwerke spiegelten den damaligen Zeitgeist wider.

Bereits im Jahr 1833 schlug der Engländer Richard Trevithick vor, eine 1000 Fuß (304,80 Meter) hohe, von 1000 Stützen getragene gusseiserne Säule mit dem Durchmesser von 30 Metern an der Basis und 3,60 Metern an der Spitze zu bauen. Trevithick starb jedoch kurz nach Veröffentlichung seiner Pläne. Die amerikanischen Ingenieure Thomas Curtis Clarke (1848–1901) und David Reeves griffen die Idee auf und wollten für die Weltausstellung 1876 in Philadelphia einen solchen Turm "(Centennial Tower)" errichten. Die Konstruktion sah eine zylindrische Eisenröhre mit 9 Meter Durchmesser als Kern vor, die mit Stahlseilen abgespannt werden sollte. Verwirklicht wurde das Vorhaben nicht. Nach heutigem Wissensstand wäre dieses Bauwerk den Windschwingungen zum Opfer gefallen.

1881 kehrte der französische Ingenieur Amédée Sébillot von einer Amerikareise mit der Idee zurück, das gesamte Stadtgebiet von Paris mit einem Leuchtfeuer auf einem „Sonnenturm“ zu beleuchten. Nachdem die französische Regierung im Mai 1884 das Vorhaben der Weltausstellung für das Jahr 1889 verkündet hatte, fertigte er zusammen mit dem Erbauer des Palais du Trocadéro, Jules Bourdais, entsprechende Pläne an. Der Entwurf, der an eine romantisierende Rekonstruktion des sagenumwobenen Leuchtturms von Pharos mit vielen Verzierungen erinnerte, stieß auf große Vorbehalte und wurde bis zum offiziellen Planungswettbewerb im Mai 1886 öffentlich diskutiert. Mangels technischer Umsetzbarkeit blieben sowohl der amerikanische Centennial Tower als auch der Sonnenturm unverwirklicht.

Im Juni 1884 stellten die beiden Ingenieure Maurice Koechlin und Émile Nouguier, beide aus dem Büro von Gustave Eiffel, einen Entwurf für einen 300 Meter hohen Metallmast vor, der auf vier Füßen ruhen sollte. Die Stahlfachwerkkonstruktion war so entwickelt, dass die Streben durch ihre Neigungswinkel Seitenwinden möglichst geringen Widerstand boten. Die Form der Turmstützen ähnelte der Momentenlinie eines vertikalen Kragarms bei Windbelastung. Damit sollten die Seitenwinde maximal nach unten abgeleitet werden, was dem hohen Bauwerk eine extrem hohe Standsicherheit verschaffen sollte. Eiffel und sein Büro hatten in den Jahren davor bereits grundlegende Erfahrungen im Brückenbau gesammelt. Die größten Eisenbahnbrücken jener Zeit stammten von Eiffel, wie beispielsweise das Garabit-Viadukt, welches das Tal der Truyère in 122 Meter Höhe überspannt. Die Pylone aus dem Brückenbau standen beim Turmprojekt Pate. Am 18. September 1884 ließ sich Eiffel den Entwurf patentieren.
Der ingenieurtechnisch ausgereifte Entwurf entsprach jedoch ästhetisch nicht den Vorstellungen Eiffels. Das pylonartige Bauwerk erinnerte zu sehr an einen überdimensionierten Freileitungsmast – die Werkbezeichnung deutete dies mit "pylône de 300 mètres de hauteur" an. Eiffel erkannte, dass der allzu technische Entwurf im Vergleich mit den kunstvollen Bauwerken der Weltausstellung nicht überzeugen konnte, und beauftragte im Frühjahr 1886 den Architekten Stephen Sauvestre, die Form des Turms zu überarbeiten, um die Akzeptanz zu erhöhen. Zu den auffälligsten, von Sauvestre vorgenommenen Veränderungen zählt der monumentale, für die Tragfähigkeit nicht notwendige Bogen mit der ersten Etage. Er wurde dem Anspruch, als Eingangsportal für die Weltausstellung zu dienen, deutlich besser gerecht und ließ den Turm weniger nüchtern erscheinen. Sauvestre versah das Bauwerk mit gemauerten Sockeln, ließ die nach oben strebenden Pfeiler früher zusammenlaufen, änderte die Aufteilung der Geschosse und fügte eine Reihe von Verzierungen hinzu. Die ursprünglich vorgesehene Spitze in Pyramidenform veränderte der Architekt zu einer zwiebelförmigen Laterne.

Erst dieser Entwurf überzeugte Eiffel so, dass er die Nutzungsrechte für den „300-Meter-Turm“ erwarb. Eiffel pries das Konzept vor dem Ausstellungskommissariat nicht nur als Ausstellungsbauwerk, sondern stellte die wissenschaftliche Bedeutung für die Meteorologie, Astronomie und die Aerodynamik heraus. Eiffel hob den Namen Koechlins nicht besonders hervor. Dies führte dazu, dass der Turm bereits in der Projektphase mit dem Ingenieur Eiffel in Verbindung gebracht wurde und schon vor seiner Errichtung die Bezeichnung "Eiffelturm" erhielt; Eiffel selbst hatte ihn nie so bezeichnet. Im Frühjahr 1885 wurden die Baukosten auf 3.155.000 Francs geschätzt und die Turmmasse mit 4810 Tonnen projektiert. Am Ende kam die reine Stahlkonstruktion des Eiffelturms auf eine Masse von 7300 Tonnen und die Baukosten erhöhten sich auf mehr als das Zweieinhalbfache.
Am 1. Mai 1886 schrieb der Handelsminister Édouard Lockroy den Ideenwettbewerb für die Gebäude der Pariser Weltausstellung aus, der sich an französische Architekten und Ingenieure richtete. Es nahmen rund 100 Bewerber teil, viele von ihnen griffen die Idee eines Turmbauwerks auf. Nach der ersten Auswahl blieben drei Vorlagen übrig, darunter befanden sich neben Eiffels Beitrag die Entwürfe von Ferdinand Dutert und Jean Camille Formigé. Eiffel ließ die stark verzierte Fassung Sauvestres nochmals unter Verzicht auf einige Zierelemente überarbeiten und gewann mit diesem Kompromissvorschlag den Wettbewerb. Er unterschrieb am 8. Januar 1887 einen Vertrag mit der Stadt, die eine Subvention in Höhe von 1,5 Millionen Goldfranken zur Verfügung stellte, und bereits am 26. Januar wurde mit dem Bau begonnen. Da Eiffel die restlichen Baukosten von insgesamt über sieben Millionen Franken selbst zu tragen hatte, sicherte ihm der 18 Paragraphen umfassende Vertrag eine zwanzigjährige Nutzungskonzession zu. Den Vertrag unterzeichnete Eiffel persönlich, nicht im Namen seiner Baufirma. Die Finanzierung der restlichen Kosten erfolgte über eine Aktiengesellschaft mit einem Grundkapital von fünf Millionen Franken, von denen er die Hälfte übernahm; die andere Hälfte stellten zwei Pariser Großbanken als Kredite zur Verfügung. Den Inhabern von Eiffelturm-Aktien, welche die höchsten Renditen in der französischen Börsengeschichte ausschütteten, war es erlaubt, den Turm einmal im Jahr kostenfrei zu benutzen.

Auch wenn Eiffel den Turm als geschlossenes Projekt aus seiner Hand anpries und sich damit eine fremde Idee zu eigen machte, gilt es historisch als gesichert, dass ohne Eiffels persönliches und unternehmerisches Engagement der Bau in dieser Form nie zustande gekommen wäre.

Unter regem Interesse der Öffentlichkeit begannen am 28. Januar 1887 die Bauarbeiten mit den Grabungsarbeiten für die Fundamente. Dafür wurden insgesamt 30.973 Kubikmeter Erdreich ausgehoben. Da die Fundamente unter dem Niveau des Seineflussbetts gründen, leitete man Druckluft in die wasserdichte Metallverschalung, damit die Arbeiten unterhalb des Wasserspiegels ausgeführt werden konnten. Dieses Verfahren hatte Gustave Eiffel bereits 1857 beim Bau der 500 Meter langen Eisenbahnbrücke von Bordeaux erprobt.
Eiffel verwendete als Baumaterial im Puddelverfahren produziertes Schmiedeeisen, was zu seiner besonderen Haltbarkeit beigetragen hat. Da die Eisenverbindung mit geringem Kohlenstoffgehalt nicht geschweißt, sondern nur genietet werden konnte, ließ Eiffel in seinem Firmensitz in Levallois-Perret die notwendigen Einzelteile im Baukastenprinzip vorproduzieren und in Paris vor Ort zusammensetzen. Die Teile wurden exakt berechnet, geschnitten und mit den Löchern für das spätere Nieten versehen. Für die Vorproduktion bis zur Errichtung hatte Eiffel einen festen Ablaufplan. Fehlerhafte Teile wurden wieder zur Fabrik zurückgeschickt und nicht vor Ort angepasst. Ein Stab von etwa 40 technischen Zeichnern, Architekten und Ingenieuren erfasste in 700 Gesamtansichten und 3.600 Werkzeichnungen das gesamte, aus 18.038 Einzelteilen bestehende Bauwerk.
Am 1. Juli 1887 begann die Errichtung der vier Turmfüße. Die zunächst freitragend montierten Sparren wurden von 30 Meter hohen provisorischen Baugerüsten getragen. Am 7. Dezember 1887 erfolgte die Montage der ersten Etage, auf deren Höhe ein 45 Meter hohes Gerüst zur Abstützung der Horizontalbalken diente. Oberhalb der Etage stützten sich die Strebepfeiler von selbst. Alle Werkstücke wurden von dampfgetriebenen Kränen auf den Führungsschienen positioniert, auf denen später die unteren Fahrstühle verkehren sollten. Einer der heikelsten Bauabschnitte war die Verbindung der vier horizontalen Tragbalken in der ersten Etage. Für deren exakte Ausrichtung nutzte Eiffel sogenannte Sandkisten, mit denen die Träger millimetergenau ausgependelt werden konnten. In zwei Pfeilern befanden sich manuell mit Handpumpen bedienbare hydraulisch verstellbare Hubspindeln, mit denen die Sparren auf ihre Position gebracht wurden. Damit war eine sehr präzise Justierung der Balken möglich. Nachdem die Tragpfeiler fest miteinander verbunden waren, ersetzte man die Hubspindeln durch verankerte Stahlkeile. Die sorgfältige Planung und Ausführung führte dazu, dass die Nietlöcher erst ab einer Höhe von 57 Metern angepasst werden mussten. Vermutlich wurde die hohe Präzision erzielt durch Zusammenlegen der Teile in der Werkstatt und anschließendes Aufreiben der Nietlöcher. Eiffel selbst führte dazu aus:

Am 14. August 1888 wurde die zweite Etage errichtet und der sich nach oben anschließende Teil freitragend montiert. Gleichzeitig stattete man die Plattformen aus. Die im Werk vorgebohrten Einzelteile wurden vor Ort mit konischen Dornen unter Schlageinwirkung in ihre endgültige Position gebracht. Insgesamt halten im Eiffelturm 2,5 Millionen Niete die Bauteile zusammen. Das Vernieten führten jeweils vier Männer durch. Der erste Arbeiter ließ den Niet heißstauchen und brachte ihn mithilfe einer kleinen Esse zum Glühen. Als zweiten Schritt führte ein anderer Arbeiter den Niet an das Bohrloch. Ein dritter schlug den Schließkopf in Form. In einem letzten Schritt wurde der Bolzen gestaucht.
An den Bauarbeiten waren bis zu 250 Personen beteiligt, rund 150 davon waren für das Vernieten der Bauteile vor Ort eingesetzt. Neben Zimmerleuten befanden sich unter den Bauarbeitern auch Schornsteinfeger, da sie das Arbeiten in großen Höhen gewohnt waren. Die Arbeitsschichten dauerten in den Wintermonaten neun und in den Sommermonaten zwölf Stunden. Im September 1888 kam es zu einem Streik der Arbeiter; drei Monate später legten sie erneut die Arbeit nieder und forderten mehr Lohn. Gustave Eiffel verhandelte mit ihnen und richtete in der ersten bereits fertiggestellten Plattform eine Kantine für sie ein. Während der gesamten Arbeiten kam es zu einem einzigen tödlichen Unfall. Ein italienischer Arbeiter verunglückte beim Einbau der Aufzüge nach der offiziellen Eröffnung.

Gleichzeitig mit der freitragenden Montage der obersten Stockwerke ab Dezember 1888 wurden die Plattformen ausgestattet. Nachdem am 15. März die Laterne auf der Spitze des Turms errichtet worden war, konnten wenige Tage später, am 31. März 1889, planmäßig wenige Wochen vor Eröffnung der Weltausstellung, die Arbeiten abgeschlossen werden.

Bereits vor dem Baubeginn formierte sich unter Intellektuellen und Künstlern Widerstand gegen den Bau des Eiffelturms. Der Kunst- und Kulturhistoriker Jacob Burckhardt sah in dem Bauwerk eine Reklame für die gedankenlosen Tagediebe in ganz Europa und Amerika. Zahlreiche Persönlichkeiten, darunter Charles Gounod, Alexandre Dumas, Charles Garnier, William Adolphe Bouguereau und auch Guy de Maupassant als einer der stärksten Kritiker, veröffentlichten am 14. Februar 1887, wenige Tage nach Baubeginn, in der damals renommierten Zeitung "Le Temps" einen "Protest der Künstler":

Das Protestschreiben blieb kein Einzelfall; weitere begleiteten die Bauarbeiten. Léon Bloy beschrieb den Eiffelturm als „wirklich tragische Straßenlaterne“, Paul Verlaine als „Skelett von einem Glockenturm“ und François Coppée als „Eisenmast mit starrer Takelage, unvollkommen, konfus und unförmig“. Die starke Ablehnung richtete sich zum einen gegen die für die damalige Zeit immense Höhe, zum anderen empfand man die offen zur Schau gestellte Konstruktionsweise aus Eisen mit fehlender Fassade als geradezu skandalös.

Ein weiterer Kritikpunkt der Gegner war der Umstand, dass der Turm nicht wie die andere Festarchitektur nach der Ausstellung wieder abgebaut werden, sondern dauerhaft stehen bleiben sollte. Der Protest, der sich vor allem aus dem akademisch-elitären Umfeld rekrutierte, ließ sich auch durch Eiffels gewieftes Entgegenkommen nicht beruhigen, den Turm für einen Bruchteil der Baukosten in Einzelteile zu zerlegen und ihn an anderer Stelle wieder aufzubauen. Jeder praktische Aspekt, der sich den Notwendigkeiten des Alltags unterwarf, konnte dem hehren Kunstbegriff der Traditionalisten nicht genügen – Industrie und Kunst hatten in ihren Augen strikt getrennt zu bleiben. Die Proteste vieler Kunstschaffenden dürfen jedoch nicht darüber hinwegtäuschen, dass in der breiten Masse das Bauwerk von Anfang an überaus beliebt war und die Baustelle rege besucht wurde.

Befürchtungen und Widerstand blieben jedoch nicht auf polemische Schriften beschränkt. Sogar zahlreiche Techniker befürchteten, die Fundamente des Turms könnten dem eisernen Fachwerk nicht standhalten. Ein Mathematiker prophezeite den Einsturz des Stahlgebildes, sobald dieses eine Höhe von 228 Metern überschreite. Ein Anlieger am Champ de Mars strengte gar einen Prozess gegen den Staat und die Stadt an, aus Angst, der Eiffelturm könne einstürzen und sein Haus zerstören. Das Gericht erlaubte den Weiterbau nur mit der Auflage, dass Gustave Eiffel bei etwaigen Schäden zu haften habe.

Am Eröffnungstag, dem 31. März 1889, bestieg Gustave Eiffel – da der Fahrstuhl noch nicht fertiggestellt war – mit einer Delegation gegen 13:30 Uhr den Turm und hisste an dessen Spitze eine französische Trikolore, die 7 Meter lang und 4,40 Meter breit war.

Der in der Presse offen ausgetragene Protest gegen den Eiffelturm verstummte nach seiner Eröffnung fast vollständig und schlug teilweise sogar in Begeisterung und Stolz um. In einer Pressemeldung hieß es dazu:

Heinrich Schliemann, dem ein Aufstieg auf den Eiffelturm bereits vor der offiziellen Eröffnung ermöglicht worden war, pries das Bauwerk in einem Brief an Rudolf Virchow am 24. Mai 1889 als Wunderwerk der ingenieurtechnischen Fähigkeiten, ohne das der vierte Teil der Ausstellung – Schliemann meinte die vierte Weltausstellung in Paris – keinen Reiz hätte. Trotz der Euphorie, die ihn als gelungene nationale Selbstdarstellung und Demonstration des technischen Fortschritts rühmte, blieb auch unversöhnliche Kritik. Auf jeden Fall erregte er die Gemüter jener Zeit sehr stark und übte eine enorme Anziehungskraft auf die Menschen aus, so der französische Philosoph, Schriftsteller und Literaturkritiker Roland Barthes.

Für die breite Öffentlichkeit war der Turm erst seit dem Eröffnungstag der Weltausstellung, dem 15. Mai 1889, zugänglich. Das Eintrittsgeld betrug 1889 für die erste Etage zwei, für die zweite drei und für die dritte fünf Francs. Der Eintritt zur Weltausstellung kostete einen Franc. Insgesamt bestiegen während der Weltausstellung 1889 1.896.987 Menschen den Eiffelturm. Damit amortisierten sich seine Baukosten bereits zu drei Vierteln. Zahlreiche prominente Persönlichkeiten der Zeitgeschichte statteten dem höchsten Bauwerk der Welt ebenfalls einen Besuch ab. Am Eröffnungstag erschien eine Sonderausgabe der Tageszeitung "Le Figaro" direkt aus dem Eiffelturm. Die Redaktion hatte ihre Arbeitsräume aus diesem Anlass in der zweiten Aussichtsplattform eingerichtet. Besucher, welche die Zeitung an diesem Tag direkt bei der Redaktion kauften, erhielten eine signierte Ausgabe als „Zertifikat“ für die erfolgte Turmbesteigung. Zum Zeitpunkt der Eröffnung und Schließung jedes Messetages wurde jeweils ein Schuss einer Salutkanone von der Spitze des Turms abgefeuert.
Der erste Eintrag im Gästebuch des Eiffelturms war der des britischen Kronprinzen, des späteren Königs Eduard VII., der am 10. Juni 1889 den Turm zusammen mit fünf Familienmitgliedern bestieg und den Eiffel persönlich führte. Am 1. August 1889 besuchte der damalige Schah von Persien Nāser ad-Din Schah das neue Bauwerk. Außerdem finden sich dort die Unterschriften weiterer prominenter Zeitgenossen wie Georgs von Griechenland, des späteren Königs von Belgien Albert I., des russischen Zaren Nikolaus II., Sarah Bernhardts und des japanischen Kaisersohnes Yoshihito. Der Erfinder Thomas Edison überreichte Gustave Eiffel am 10. September 1889 eine Widmung für die „Errichtung des gigantischen und originellen Musterstücks moderner Baukunst“ und nahm bei seinem Besuch die Stimme Eiffels auf. In der dritten Plattform unterhalb der Turmspitze ist dieses Ereignis im ehemaligen Büro Eiffels mit Wachsfiguren nachgebildet. Auch der Pazifist Mahatma Gandhi, der damals in London studierte, bestieg während der Weltausstellung den Eiffelturm.

Als der Eiffelturm eröffnet wurde, war er mit einer Gesamthöhe von damals 312 Metern offiziell das höchste Bauwerk der Welt und löste damit das 169,3 Meter hohe Washington Monument, einen Obelisken aus weißem Marmor in den Vereinigten Staaten, als Rekordhalter ab. Das höchste begehbare Gebäude jener Zeit war die 167,5 Meter hohe Synagoge Mole Antonelliana in Turin, die 1888 fertiggestellt wurde.

Der Erfolg und das Fortbestehen des Eiffelturms über die zwanzigjährige Konzession hinaus war ungewiss. Eiffel versuchte immer wieder durch die Einbindung von Gelehrten und eigene Forschungen den Nutzen des Bauwerkes darzulegen.

Am 5. November 1898 konnten Eugène Ducretet und Ernest Roger eine drahtlose Telegraphenverbindung zwischen dem Eiffelturm und dem vier Kilometer entfernten Panthéon herstellen. Die elektromagnetische Informationsübermittlung blieb zunächst rein militärischen Zwecken vorbehalten. Im selben Jahr wurde auf dem Eiffelturm eine Wetterwarte eingerichtet. Durch den enormen Höhenunterschied von 300 Metern war es möglich, vielfältige physikalische Experimente durchzuführen. So wurden zur Eichung von Luftdruckmessern ein übergroßes Manometer installiert, spektroskopische Messungen durchgeführt, ein Foucaultsches Pendel eingerichtet und Windgeschwindigkeit und Atmosphärentemperatur gemessen. Sogar Experimente zur Heilwirkung von Höhenluft führte man durch. Für seine astronomischen und physiologischen Beobachtungen richtete Eiffel ein eigenes Büro in der dritten Plattform ein. Besondere Bekanntheit erlangten Eiffels Messungen zur Aerodynamik. Eine erste Versuchsreihe begann er 1903: Er spannte zwischen der zweiten Plattform und dem Erdboden ein Kabel, an dem er verschiedene Profile nach unten gleiten ließ. 1904 konnten Zeitsignale auf der Wellenlänge 2000 Meter mit unterschiedlichen Apparaten empfangen werden. 1909 erweiterte er seine Studien durch Eröffnung eines Windkanals am Fuße des Turms und einer größeren Anlage an der Rue Boileau im Jahre 1912.

Für die Weltausstellung 1900, die zum fünften Mal in Paris stattfinden sollte, erwog Eiffel verschiedene Umbaupläne. Die allgemeine Empfindung der Ästhetik des Turmes hatte sich in den wenigen Jahren seines Bestehens derart gewandelt, dass seine Optik wegen ihrer Modernität und Radikalität überholt wirkte. Gefragt waren üppigere Formen, wie in der Belle Époque üblich. Die Ausstellung zeichnete sich insgesamt durch eine retrospektive Ausrichtung aus und war damit eher eine Schlussfeier des 19. als eine Eröffnungsfeier des 20. Jahrhunderts.

Aus diesem Grund versuchten Veranstalter und Architekten den Eiffelturm hinter einer Stilhülle zu verbergen. Die Vorschläge dazu reichten von relativ moderaten Veränderungen wie dem Anbringen von Schnörkeln, Wimpeln, Balkonen und Girlanden bis hin zu massiven Umbauplänen, die eine völlige Neukonzeption des Turms vorsahen. Der Entwurf von Guillemonats sah beispielsweise vor, den Turm bis zur ersten Plattform abzutragen und einen riesigen Globus darauf zu errichten. Zum massivsten Umbauvorschlag mit der Projektbezeichnung „la Tour Eifel (sic!) dans le mont Samson“ zählt der eines gewissen Samson, der den Turm als Stützgerüst für einen künstlichen Berg vorsah und den Eiffelturm damit komplett hinter einer Bergkulisse mit Dörfern, Straßen und Vegetation hätte verschwinden lassen. Abgesehen von der Tatsache, dass Samson nicht wusste, wie man den Eiffelturm korrekt buchstabierte, zeugte auch die wenig professionell ausgeführte Planskizze von mangelnder Seriosität. Der Umbauvorschlag von Gautier wollte den Eiffelturm als Stützkonstruktion für ein riesenhaftes pagodenähnliches Tor verwenden. Sowohl ein Abriss als auch die angestrebten Umbauvorschläge scheiterten am Eigentumsrecht Eiffels.
Am 28. Dezember 1897 einigte man sich schließlich, den Eiffelturm weitgehend unverändert in die Weltausstellung zu integrieren. Sein technisches Aussehen wurde lediglich durch eine neue Lichtinstallation, welche die Konturen des Bauwerks hervorhob, zu überspielen versucht. Eiffel beließ es dabei, das Bauwerk nach oben hin in abgetönter orangeroter Farbe neu zu streichen und den Plattformen ein neues äußeres Aussehen zu geben. Neben einer neuen Aufzuganlage stellte er auch seinen Salon in der dritten Plattform der Öffentlichkeit zur Verfügung. Doch zur Weltausstellung zog der Turm mit etwa einer Million nur noch halb so viele Besucher an; die Zahl sank in den Folgejahren weiter ab und pendelte sich bis zu Beginn des Ersten Weltkriegs auf jährlich rund 180.000 ein. Rein wirtschaftlich gesehen spielte es eine untergeordnete Rolle, denn die Baukosten des Eiffelturms waren bereits nach eineinhalb Jahren amortisiert. Eiffel war durch sein alleiniges Vermarktungsrecht und sein prosperierendes Unternehmen bereits mehrfacher Millionär geworden und konnte sich neben einem Stadtpalais in Paris weitere Häuser in Sèvres, Beaulieu-sur-Mer an der Côte d’Azur und in Vevey am Genfersee leisten.

Neben der wissenschaftlichen Nutzung wuchs vor allem auch der militärische Wert des Turmes. Am 15. Dezember 1893 erlaubte Eiffel dem Kriegsminister Auguste Mercier, auf dem Turm Antennen zu befestigen, und übernahm sogar deren Kosten. Am 21. Januar 1904 unterstützte er Hauptmann Gustave-Auguste Ferrié, einen Offizier der Pioniertruppen, die drahtlose Telegraphie für die militärische Nutzung voranzutreiben. Ferrié richtete das militärische Netzwerk ein und wurde zum zweitwichtigsten Mann neben Eiffel. Nachdem bereits 1898 eine drahtlose Verbindung hergestellt worden war, wurden 1903 zwischen dem Eiffelturm und einigen Militäranlagen in Paris weitere Funkverbindungen geschaffen und ein Jahr später die Verbindung in den Osten Frankreichs erweitert. 1906 wurde ein Radiosender auf dem Turm eingerichtet. Die nach 20 Jahren ausgelaufene Konzession wurde am 1. Januar 1910 um weitere 70 Jahre verlängert. Mit der gestiegenen strategischen Bedeutung war auch der Fortbestand des Eiffelturms gesichert; sie war sogar ausschlaggebend für die Fortsetzung der Konzession, denn der wissenschaftliche Nutzen blieb real betrachtet eher bescheiden.

Ab dem 23. Mai 1910 diente der Eiffelturm der französischen Marine regelmäßig als Zeitzeichensender. Das Signal konnte nachts bis zu einer Entfernung von 5200 Kilometern und tagsüber bis etwa zur Hälfte dieser Strecke empfangen werden. Hauptmann Ferrié machte es möglich, einen internationalen Standard bei der Zeitmessung festzulegen.

Im selben Jahr konnten erste Funkverbindungen mit Luftschiffen und ein Jahr später mit Flugzeugen hergestellt werden.

Die wissenschaftliche Nutzung und Messungen am Eiffelturm gingen weit über die Sende- und Übertragungstechnik hinaus. Der Jesuitenbruder und Physiker Theodor Wulf (1868–1946) maß 1910 vier Tage lang an der Spitze und am Fuße des Turms die Strahlungsenergie und stellte einen signifikanten Unterschied fest, mit dem es ihm letztlich möglich war, die kosmische Strahlung nachzuweisen. Während des Ersten Weltkrieges musste der französische Physiker und spätere Nobelpreisträger Louis de Broglie sein Studium zwangsweise unterbrechen und leistete seinen Militärdienst bis 1919 auf der funktelegraphischen Station des Eiffelturms ab.

Mit Beginn des Ersten Weltkriegs wurde der Eiffelturm für die Öffentlichkeit gesperrt. Er hatte sich als Telekommunikationszentrum für das Militär etabliert, das dort verschlüsselte feindliche Funksprüche abfing, deren Nachrichteninhalt entziffert werden konnte. Zu den bedeutendsten Fällen gehört ein als Radioprogramm getarnter Funkspruch, der zur Verhaftung der Spionin Mata Hari führte, sowie das "Radiogramme de la Victoire" ().

Bereits vor dem Ersten Weltkrieg fanden erfolgreiche Tests zur drahtlosen Übertragung von Telegraphie-Signalen statt. Am 24. Dezember 1921 begann nun auch das Senden von Tonsignalen. Lucien und Sacha Guitry strahlten erstmals vom Eiffelturm ihr Radioprogramm "(Radio Tour Eiffel)" aus. Damit schrieben sie Rundfunkgeschichte, denn die ausgestrahlte Sendung war in Europa die erste öffentliche Radiosendung. Ein Jahr später, am 6. Februar 1922, wurde im Nordpfeiler ein temporäres Studio eingerichtet, aus dem Guitry, Yvonne Printemps und Direktor Ferrié sendeten.

Im Mai 1925 gab sich der Betrüger Victor Lustig als stellvertretender Generaldirektor des Postministeriums aus und fälschte eine Ausschreibung, die den Eiffelturm zum Verkauf anbot. Lustig schaffte es, ihn an André Poisson zu veräußern, der sich damit den Aufstieg in die Pariser Geschäftswelt erhoffte. Um Poissons anfängliche Zweifel zu zerstreuen, mimte Lustig ein Geständnis, er sei ein korrupter Beamter, der für seinen teuren Lebensstil etwas dazuverdienen wolle. Lustig tauchte nach Abschluss des Handels unter und setzte sich nach Wien ab. Als der Schwindel aufflog, zog Poisson es aus Scham vor, den Betrug nicht der Polizei anzuzeigen. Nach einem Monat versuchte Lustig den Betrug zu wiederholen. Der Käufer schöpfte jedoch Verdacht und ging zur Polizei, worauf Lustig floh.

1925 ließ Édouard Belin das erste Fernsehsignal vom Turm ausstrahlen. Damit wurde der Eiffelturm zum ersten Fernmelde- sowie Fernsehturm und blieb bis 1953 weltweit, wie weiter unten erwähnt, auch der höchste Turm dieser Art.

Im Jahr 1929 strahlte der Eiffelturm die Daten von 350 Wetterstationen aus und ermöglichte damit einen Austausch zwischen Europa, Nordafrika und den Inseln im Atlantischen Ozean einschließlich Islands und der Kapverdischen Inseln.

Mit der Einweihung des 319 Meter hohen Chrysler Building in New York City 1930 verlor das Pariser Wahrzeichen den Titel des höchsten Bauwerks der Welt, den es fast 41 Jahre innegehabt hatte. Bis zur Fertigstellung des Tokyo Tower im Jahr 1953 blieb es noch der höchste Fernsehturm.

Die erste offizielle Fernsehübertragung vom Eiffelturm am 26. April 1935 um 20:15 Uhr war die Geburtsstunde des Fernsehens in Frankreich. Genutzt wurde wie schon bei den Sendungen von Édouard Belin die Technik des sogenannten "„mechanischen“" und teilweise des „elektronischen“ Fernsehens. Dazu strahlte ein 500-Watt-Sender auf der Wellenlänge 175 Meter, der allerdings bald danach durch einen 10 Kilowatt starken Sender ersetzt wurde. Das Programm strahlte man in einer halbelektronischen 60-Zeilen-Norm mit 25 Bildern pro Sekunde aus, die im Dezember von einer 180-Zeilen-Norm ersetzt wurde.

Während der Weltfachausstellung 1937, die bereits im Zeichen der konkurrierenden Weltmächte und des drohenden Konfliktes mit dem „Dritten Reich“ stand, wurde unterhalb der ersten Plattform des Eiffelturms ein riesiger, vom Architekten André Grasset gestalteter Kronleuchter aufgehängt. Darüber hinaus tauchte man den Turm mit 30 Projektoren in ein weißes Licht mit blauen und roten Blitzen. Die Veranstaltung war die letzte der sechs Weltausstellungen in Paris. Der Eiffelturm war seit 1889 fester Bestandteil der Ausstellungsarchitektur, zog aber mit jedem Mal weniger Besucher an.

Beim Staatsbesuch des britischen Königs Georg VI. im Jahr 1938 wurde ihm zu Ehren der Union Jack seitlich am Turm gehisst. Die 120 Kilogramm schwere Flagge war 30 Meter breit und 40 Meter lang.
Mit der Besetzung von Paris 1940 wurden die Aufzugkabel abgetrennt. Eine Reparatur war aufgrund der mangelnden Güterversorgung während des Zweiten Weltkrieges praktisch unmöglich. Für die deutschen Truppen und Adolf Hitler bedeutete dies, dass sie den Eiffelturm nur über die Treppe besteigen konnten. Deutsche Soldaten erhielten den Auftrag, bis zur Spitze hochzusteigen, um eine Hakenkreuzflagge an der Spitze des Turms zu hissen. Da sie zu groß war, wurde sie bereits nach wenigen Stunden weggeweht und etwas später durch eine kleinere ersetzt. Auf der ersten Aussichtsplattform ließ die Wehrmacht zudem ein Transparent mit der Aufschrift „Deutschland siegt auf allen Fronten“ anbringen. Als Hitler Paris am 24. Juni 1940 selbst besuchte, zog er es vor, den Eiffelturm nicht zu besteigen. Darauf hieß es, dass Hitler zwar Frankreich, aber nicht den Eiffelturm erobert habe. Auch die Hakenkreuzflagge wurde während der Besatzungszeit von einem Franzosen in einer heimlichen Aktion durch die französische Trikolore ersetzt. Trotz der Widrigkeiten inszenierten die Deutschen den Eiffelturm zu propagandistischen Zwecken. Hitler ließ sich zusammen mit weiteren namhaften Größen seines Regimes wie beispielsweise Albert Speer und Arno Breker in verschiedenen Posen vor dem Eiffelturm fotografieren, um vor der heimischen Bevölkerung den Sieg über die Franzosen zu demonstrieren.

Wie schon während des Ersten Weltkrieges blieb der Turm auch während des Zweiten Weltkrieges für die Öffentlichkeit geschlossen. US-amerikanische Truppen befreiten Paris am 25. August 1944 und installierten auf der zweiten Etage des Eiffelturms ihre Sendestationen, um mit deren Hilfe mit den Streitkräften am Ärmelkanal kommunizieren zu können. Nach der Wiedereröffnung für den Publikumsverkehr im Juni 1946 bestiegen in dem folgenden halben Jahr über 600.000 Besucher den Turm. Mit dem stärker werdenden Tourismus stieg auch die Besucherzahl anhaltend auf jährlich über eine Million und steigerte sich in den folgenden Jahrzehnten kontinuierlich.

Die 1950er Jahre waren geprägt durch das sich durchsetzende Medium des Fernsehens. Im April 1952 wurde das erste Mal eine Livesendung von Paris nach London gesendet. Die technische Schwierigkeit bestand in der Überbrückung der unterschiedlichen Übertragungsstandards zwischen Frankreich und Großbritannien. Mit der Sendung schrieb der Eiffelturm ein weiteres Mal Fernsehgeschichte. Die Show moderierten Georges de Caunes und Jacqueline Joubert vom französischen Fernsehen und Miss Reeves von der BBC. Ein Jahr später folgte der nächste Meilenstein mit der Einrichtung des Eurovision-Verbundes. Damit konnte am 2. Juni die Krönung von Elisabeth II. an alle Teilnehmerstaaten der Eurovision übertragen werden; in Frankreich wurde die Zeremonie landesweit vom Eiffelturm aus ausgestrahlt. 1956 brach im Senderaum ein Feuer aus und zerstörte die Turmspitze sowie die Sendeeinrichtungen. Ein Jahr später errichtete man neue Antennenplattformen und installierte neue Antennen. Nach dem Umbau strahlte der Turm Radio- und drei Fernsehprogramme aus. Seine neue Antenne erhöhte das Wahrzeichen auf insgesamt 320,75 Meter.
Zum 75-jährigen Jubiläum des Eiffelturms im Jahr 1964 lud die Betreibergesellschaft insgesamt 75 um das Jahr der Errichtung des Turms 1889 geborene Pariser Bürger zu einer festlichen Gala ein. Zu den berühmtesten Gästen zählte Maurice Chevalier. Im Laufe der Jahre wurden nicht nur zu den Jubiläen besondere Ereignisse am Eiffelturm veranstaltet. Durch die vermehrte Vergabe von Dreherlaubnissen für Kinofilme stieg der Status des Bauwerks weiter. Neben immer wieder außergewöhnlichen, meist sportlichen Aktionen wurde der Eiffelturm auch zunehmend im Alltagsleben der Pariser verankert, beispielsweise durch die Eröffnung einer Schlittschuhbahn in der ersten Etage im Winter. Aufgrund des traditionellen, an akademisch-klassizistischen Idealen orientierten Kunstverständnisses in Frankreich tat sich der Eiffelturm auch noch im fortgeschrittenen 20. Jahrhundert schwer mit seiner Anerkennung als Kulturdenkmal. Erst am 24. Juni 1964 wurde das Bauwerk in das "Inventaire des monuments historiques" eingetragen.

Nachdem zum 1. Januar 1980 die an Eiffel vergebene und inzwischen auf seinen Erben übergegangene Konzession ausgelaufen war, übernahm die Société nouvelle d’exploitation de la tour Eiffel (SNTE), eine zu 100 % der Stadt Paris gehörende Tochtergesellschaft, den Betrieb des Wahrzeichens. Sie kümmert sich seither um die Erhaltung und Vermarktung des Bauwerks. Am 9. September 1983 wurde der hundertmillionste Besucher des Eiffelturms mit einem Geschenk begrüßt. Die Sängerin Mireille Mathieu überreichte der Frau die Schlüssel für einen Citroën BX.

Seit 1991 gehört das Seineufer mit seinen Bauwerken zwischen dem Pont de Sully und dem Pont d’Iéna am Eiffelturm in Paris zum Kulturerbe der UNESCO.

Im Jahr 2000 übernahm die Rundfunkgesellschaft TDF die Montage von UHF-Antennen und ließ seine Gesamthöhe von 318,7 Meter auf seine heutige Höhe von 324 Meter anwachsen. 2005 strahlte der Eiffelturm erstmals digitales Fernsehen aus. Die SNTE ging zum 1. Januar 2006 für zunächst zehn Jahre in die Société d’exploitation de la tour Eiffel (SETE) über. Von Februar 2012 bis 2013 wurden umfangreiche Renovierungen und Umgestaltungen in der ersten Plattform durchgeführt, die zu diesem Zeitpunkt von etwa der Hälfte der Besucher gemieden wurde. Unter anderem wurden an Anlehnung der ursprünglichen Glassäle drei überdachte kastenförmige, dunkelrote Pavillons – teilweise mit Glasboden – aus Stahl und Glas konstruiert, die an den Seitenwänden zwischen den Pfeilern und parallel zu ihnen verlaufen. Während der 25 Millionen Euro teuren Umbaumaßnahmen wurde der Besucherbetrieb weiter aufrechterhalten. Darüber hinaus wurden Behindertenaufzüge und eine Konferenzhalle eingerichtet.

Der Eiffelturm befindet sich im Westen des 7. Arrondissements der Pariser Innenstadt am nordwestlichen Ende des Champ de Mars. Er steht auf 33 Meter Höhe über Meer nicht weit vom Ufer der Seine entfernt, wo sich auch Anlegestellen von Ausflugsbooten befinden. Unweit davon liegt südwestlich des Eiffelturms die langgestreckte Île aux Cygnes (Schwaneninsel) in der Seine. In der unmittelbaren Sichtachse des Bauwerks steht südöstlich die École Militaire und nordwestlich auf dem gegenüberliegenden Flussufer über der 1937 auf 35 Meter verbreiterten Pont d’Iéna das Palais de Chaillot. Südöstlich der École Militaire befindet sich der Sitz der UNESCO in einem 1958 erbauten Gebäude mit Y-förmigem Grundriss. Rund drei Kilometer Luftlinie in südöstlicher Richtung entfernt steht etwas nördlich der exakten Sichtachse das 210 Meter hohe Bürohochhaus Tour Montparnasse. Nordöstlich befindet sich in der Nähe des Eiffelturms das Völkerkundemuseum Musée du quai Branly.

Folgende für den Fahrzeugverkehr freigegebene Straßen tangieren das Turmareal: südwestlich die Avenue Gustave Eiffel, im Nordosten die Avenue de la Bourdonnais, im Nordwesten der stark befahrene Quai Brandly, der in den Pont d’Iéna über die Seine übergeht, und im Südosten die Avenue de Suffren. Die vier Straßen begrenzen ein bewaldetes, parkähnliches rechteckiges Grundstück, auf dem mittig der Eiffelturm steht. Die Durchfahrt ist für den motorisierten Verkehr nicht gestattet.

Die dem Eiffelturm nächsten Haltestellen der Métro Paris sind Bir-Hakeim (Tour Eiffel) der Linie 6 und École Militaire der Linie 8. Die Linie C der Pariser S-Bahn RER hält südwestlich des Turms am Bahnhof Champ de Mars – Tour Eiffel. In unmittelbarer Nähe des Eiffelturms halten verschiedene Buslinien.

Wie Gustave Eiffel in einem Vortrag vor der "Société des Ingénieurs civils" am 30. März 1885 erklärte, ging es in der Architektur des Turms darum,

So ausgeklügelt die Architektur zur Optimierung der Windlast ist, so vergleichsweise schlicht ist die grundsätzliche Konzeption des Eiffelturms, der die großen Eisenbahnbrücken aus Eisenfachwerk zum baulichen Vorbild hat. Sechzehn vertikal versetzte und in Vierergruppen zusammengefasste Hauptstreben ragen bogenförmig in die Höhe und werden über die drei horizontalen Besucherplattformen verbunden. Oberhalb der zweiten Plattform werden die Streben zu einem Pylon vereint.

Der Eiffelturm steht auf einer Höhe von 30,5 Meter über dem Meeresspiegel am nordwestlichen Ende des Champ de Mars (→ Lage). Das Bauwerk steht auf vier mächtigen Stützpfeilern aus Eisenfachwerk mit einer Breite von jeweils 26,08 Metern; sie leiten das gesamte Gewicht in das bis in 15 Meter Tiefe reichende Fundament weiter. Die Pfeiler ruhen auf massivem Mauerwerk und sind mit 16 Sparren im 54-Grad-Winkel im Boden verankert. Schrauben von 7,80 Metern Länge verbinden dabei den Gusseisen-Schuh mit dem Unterbau.

Die Turmkonstruktion ist durch den Unterbau so gelagert, dass sie je nach Windlast einen Druck von etwa 5 kg/cm² an den Erdboden abgibt. Das entspricht etwa dem Bodendruck, den ein auf einem Stuhl sitzender Erwachsener auf den Boden ausübt – ein Vergleich, den Eiffel selbst errechnete und in seiner Publikation "La Tour de 300 mètres" angab.

Die Pfeiler haben im unteren Bereich zueinander einen Abstand von 74,24 Meter, was einer Spreizung des Turms an der Basis von insgesamt 124,90 Metern entspricht. Der Grundriss der Standfläche ist quadratisch. Der Eiffelturm wurde so konstruiert, dass jeder seiner Pfeiler exakt auf eine Himmelsrichtung ausgerichtet ist. Die Nord- und Westpfeiler zeigen in Richtung der Seine, die Ost- und Südpfeiler in Richtung des Champ de Mars. In jedem der Pfeiler befinden sich Eingänge mit Kartenverkaufsständen, Treppenhäuser und Aufzüge, die je nach Besucherandrang und Anlass unterschiedlich geöffnet sein können. Der Abstand der Pfeiler, die über mächtige Bögen miteinander verbunden sind, verringert sich mit zunehmender Höhe. Die ebenfalls aus filigran wirkendem Eisenfachwerk gefertigten Bögen 39 Meter über dem Boden und mit einem Durchmesser von 74 Metern haben rein dekorativen Charakter und keine tragende Funktion. Zwischen den Pfeilerfüßen ist der Durchgang ausschließlich Fußgängern vorbehalten. Am Nordpfeiler steht zu Ehren des Erbauers Gustave Eiffel eine goldfarbene Büste auf einem länglichen Sockel.

Südwestlich des Westpfeilers ragt ein von Sträuchern überwucherter roter Backstein-Schornstein an einer künstlichen Grotte hervor. Er stammt aus dem Jahr 1887 und wurde während der Bauphase für die Errichtung des Südpfeilers verwendet.

Die erste Etage oberhalb der Bögen auf 57,6 Meter Höhe bietet auf einer Nutzfläche von 4415 Quadratmetern Platz für rund 3000 Besucher gleichzeitig. Auf dieser Ebene befinden sich das Restaurant "58 Tour Eiffel", ein Selbstbedienungslokal und der Kinosaal "Cineiffel", der auch als Ausstellungsraum genutzt werden kann. Der Rundumbalkon auf dieser Ebene ist an der Brüstung mit Panoramatafeln ausgestattet, damit die von dort sichtbaren Pariser Sehenswürdigkeiten besser lokalisiert werden können. Es gibt einen Andenkenladen und im Südpfeiler eine kleine, täglich geöffnete Postannahmestelle "(Bureau de Poste Tour Eiffel)", die einen eigenen Poststempel als Erinnerungsbeleg führt. Im ersten Stock bietet sich der 300 Quadratmeter große mietbare "Gustave-Eiffel-Saal" für Tagungen, Konferenzen, Konzerte oder Empfänge an.

Zu Beginn hatte der Eiffelturm auf seiner ersten Etage aufwändig verglaste Säle, die von außen durch bogenförmige Dachkonstruktionen auffielen. Darin befanden sich unter anderem vier Restaurationsbetriebe, die thematisch verschieden ausgestaltet waren. Zwischen dem Nord- und Ostpfeiler war das russische Restaurant angesiedelt, das heute Gustave-Eiffel-Raum heißt. Zwischen dem Süd- und dem Westpfeiler war die anglo-amerikanische Bar, zwischen dem Ost- und Südpfeiler befand sich das französische Restaurant und zwischen dem Nord- und Westpfeiler war das flämische Restaurant angesiedelt. Letzteres wurde nach der Ausstellung 1889 in ein niederländisches Restaurant umgebaut und nach 1900 als Theatersaal genutzt. Sämtliche dieser Bauten und die historischen Ornamente wurden im Zuge der Weltausstellung 1937 abgebrochen und durch von außen weniger auffällige ersetzt, um sie dem geänderten Geschmack anzupassen.

Entlang eines Frieses auf der ersten Etage sind 72 Namen bedeutender Wissenschaftler und Techniker angebracht, auf jeder Seite 18. Durch einen Neuanstrich des Turms Anfang des 20. Jahrhunderts verschwanden die Namen; in den Jahren 1986 und 1987 wurden sie wieder sichtbar gemacht. Es handelt sich vornehmlich um Ingenieure und Mathematiker, die während der Französischen Revolution und der ersten Hälfte des 19. Jahrhunderts gewirkt haben. Die Auswahl der Namen traf Eiffel selbst; für einige Namen wurde er kritisiert. Er überging bewusst Wissenschaftler mit langen Familiennamen und auch Frauen, die sich in der Wissenschaft verdient gemacht haben, beispielsweise auch die bedeutende französische Mathematikerin Sophie Germain.

Auf 115,7 Metern Höhe befindet sich die zweite Etage mit einer Fläche von 1430 Quadratmetern, die Platz für rund 1600 Besucher gleichzeitig bietet. Bis zur zweiten Etage gelangt man wahlweise über den Fahrstuhl oder eines der in den Pfeilern befindlichen Treppenhäuser; von der Basis bis zur zweiten Etage führen 704 Treppenstufen hinauf. Auf dieser Ebene findet der Umstieg zu den Fahrstühlen statt, die bis zur Spitze weiter führen.

Hier befindet sich das Restaurant "Jules Verne" mit 95 Sitzplätzen. Es bietet eine gehobene Gastronomie, wurde vom Guide Michelin mit einem Stern ausgezeichnet und erhielt vom Gault-Millau 16 von 20 möglichen Punkten. Das Restaurant mit einer Grundfläche von 500 Quadratmetern liegt leicht erhöht auf dem Südpfeiler auf einer Höhe von 123 Metern und ist über einen separaten Aufzug erreichbar. Seit 2007 steht es unter der Leitung des Kochs Alain Ducasse.

Außerdem gibt es auf dieser Ebene einen Schnellimbiss und einen Andenkenladen. In eigens für die Besucher eingerichteten Schaukästen wird die Geschichte zum Eiffelturm in Wort und Bild nacherzählt.

Die dritte und oberste Plattform befindet sich auf 276,1 Metern Höhe und hat eine Fläche von 250 Quadratmetern. Diese Etage ist für die Öffentlichkeit ausschließlich über die Aufzüge erreichbar. Es besteht jedoch eine durchgehende Treppe, die beginnend im Ostpfeiler bis zur Spitze 1665 Stufen hat. Sie ersetzte 1983 die ursprüngliche Treppe mit 1710 Stufen und ist leichter und weniger gefährlich. Bis heute (2014) ist die oberste Aussichtsplattform die zweithöchste öffentlich zugängliche Europas; höhere Aussichtsplattformen hat lediglich der Fernsehturm Ostankino in Moskau.

Oberhalb der überdachten Plattform gelangt man über Treppen auf die durch Stahlgitter gesicherte, rund 100 Quadratmeter große Freiluftplattform. Der gesamte Bereich der dritten Etage kann bis zu 400 Personen gleichzeitig aufnehmen. Bei gutem Wetter kann man von hier aus bis 80 Kilometer weit sehen. Tafeln weisen in der entsprechenden Himmelsrichtung auf große Städte in der Welt hin und geben die Luftlinie vom Eiffelturm aus an. Neben einer Champagner-Bar wurde das Arbeitszimmer Eiffels originalgetreu restauriert und mit Wachsfiguren ausstaffiert, welche Eiffel, seine Tochter Claire und den amerikanischen Erfinder Thomas Edison zeigen, wie sie den Phonographen ausprobieren, den Edison für Eiffel als Präsent zur Turmeröffnung mitgebracht hatte.
Oberhalb der Besucherplattform auf einer Höhe von 295 Metern befindet sich für jede Himmelsrichtung je ein Leuchtfeuer. Die Bewegung wird durch eine Software gesteuert und ist so synchronisierbar, dass mit den Leuchtfeuern ein durchgängig drehbares Kreuz simuliert wird. Auf dieser Höhe befinden sich auch mehrere Richtantennen. Darüber sind die Dipolantennen für die Radiofrequenzen angebracht; diese befinden sich auf 291 Meter und 294 Meter. Im unteren Bereich des eigentlichen Antennenmastes, der sich von der ehemaligen Laterne erhebt, sind auf mehreren Etagen in alle Himmelsrichtungen weitere Doppel-Dipolantennen angebracht, die sich auf 299 Meter und 304 Meter befinden. Darüber befinden sich die UHF-Antennen – erkennbar durch die abschirmenden, auffällig weißen Wetterschutzkästen. Die Turmspitze wird bekrönt von weiteren in die vier Himmelsrichtungen weisenden Dipolantennen, meteorologischen Messinstrumenten und einer Wartungsplattform.

An der Spitze des Turms befinden sich über 120 Antennen zur Übertragung von dutzenden Radio- und Fernsehprogrammen (→ Nutzung als Sendeturm). Die Antennenhöhe variierte dabei im Laufe der Jahrzehnte. Seit seiner Eröffnung ist die eigentliche bauliche Struktur 300,51 Meter hoch und erreichte mit der Laterne und dem Fahnenmast an seiner Spitze eine Gesamthöhe von 312,27 Metern. Die Laterne ist durch die zusätzliche Montage von Antennenplattformen nur noch im oberen Drittel durch die gebogenen, zum Antennenmast zusammenlaufenden Fachwerkträger zu erkennen. Mit einer neuen Antenne veränderte sich 1991 die Gesamthöhe auf 317,96 Meter und der Umbau von 1994 an der Turmspitze machte ihn insgesamt 318,70 Meter hoch. Die letzte Veränderung in der Gesamthöhe erfuhr der Turm im Jahr 2000, als er auf die heutige Höhe von 324 Metern anwuchs.

Aufgrund von Windeinwirkungen schwankte die Turmspitze während eines Sturms 1999 bis zu ca. 13 Zentimeter aus seiner Ruhelage. Die Ausdehnung infolge von starker Sonneneinstrahlung kann 18 Zentimeter ausmachen, bei extremen Schwankungen sogar bis zu 30 Zentimeter.

Die Auffahrt im Eiffelturm wird von insgesamt neun verschiedenen Aufzügen ermöglicht – fünf in den Turmpfeilern, die zwischen dem Eingang und der zweiten Etage verkehren, und zwei Paar Aufzüge mit Doppelkabinen zwischen der zweiten und dritten Etage.

Zwischen dem Erdgeschoss und der zweiten Etage verkehren teilweise doppelstöckige Schrägaufzüge, die sich dem variablen Neigungswinkel von 54° bis 76° der Turmpfeiler anpassen. Der weitere Aufstieg erfolgt nach einem Umstieg im zweiten Stockwerk über einen Vertikalaufzug. Spezielle Fahrstühle für den Eiffelturm, für den die enorme Höhe wie auch die Neigung der Turmpfeiler charakteristisch sind, stellten für die damalige Zeit eine technische Herausforderung an die Industrie dar, die zu jener Zeit selbst erst seit einigen Jahren bestand, denn der erste Hydraulikaufzug wurde bei der Weltausstellung 1867 präsentiert. Trotz vieler Erneuerungen und Modernisierungen arbeiten die Aufzüge vom Grundprinzip so, wie sie Eiffel zur Erbauung des Turms konzipiert hatte. Der Maschinenraum mit dem Hydraulikantrieb der Aufzüge ist im Rahmen von Sonderführungen im Untergeschoss des Bauwerks zu besichtigen.

Eiffel setzte beim Bau des Turms bewusst auf unterschiedliche Techniken und Hersteller, um bei einem Fehler unabhängig zu bleiben. Im Nord- und Südpfeiler arbeiteten bis 1910 Otis-Aufzüge. Die zweigeschossigen Kabinen wurden mittels Kabelzug emporgezogen. Bis 1897 befanden sich im Ost- und im Westpfeiler Aufzüge von Roux, Combaluzie und Lepape; sie konnten mit Hilfe einer endlosen Doppelkette bis zu 200 Personen befördern. Beide Systeme wurden durch eine hydraulische Förderanlage betrieben. Anlässlich der Weltausstellung 1900 ersetzte Eiffel die Fahrstühle und auch die Dampfmaschinen, welche die Hydraulik antrieben, durch Elektromotoren. Zwei historische Anlagen, 1899 von "Fives-Lilles" im Ost- und Westpfeiler installiert, sind heute noch bei einer Sonderführung zu besichtigen. Die Aufzüge wurden in den Jahren 1986 und 1987 modernisiert und seit den 1990er Jahren mehrfach generalüberholt. Im Jahr 2010 baute man moderne und klimatisierte zweigeschossige Fahrkabinen ein, die jeweils 56 Besucher transportieren können.

Im Südpfeiler befindet sich ein Schrägaufzug von Otis, der seit 1983 ausschließlich für Besucher des Jules-Verne-Restaurants verwendet wird. 1989 wurde dieser Aufzug durch einen vier Tonnen tragenden Lastenaufzug ergänzt. Im Nordpfeiler wurde 1965 ein Schrägaufzug von "Schneider" eingebaut; er wurde in den 1990er Jahren grundlegend überholt. Die Kapazität des Aufzugs im Nordpfeiler beträgt 920 Personen pro Stunde, der im Ost- und Westpfeiler schafft 650 Personen in der Stunde. Der kleine Aufzug zum Restaurant kann maximal zehn Personen pro Fahrt befördern. Der Warenaufzug im Südpfeiler kann wahlweise 30 Personen oder vier Tonnen Güter pro Fahrt transportieren.

Die ursprünglichen Vertikalaufzüge für die Passage von der zweiten zur dritten Etage wurden von Léon Edoux, einem Klassenkameraden von Eiffel, gebaut. Die ebenfalls hydraulisch betriebenen Fahrstühle benutzten anstelle eines Gegengewichts zwei gegenläufige Fahrkörbe, die sich gegenseitig im Gleichgewicht hielten. Das Prinzip erforderte, dass die Besucher auf halber Höhe – etwa auf 228 Meter – die Kabinen wechseln mussten. Die eigens dafür als Steg genutzte Zwischenplattform ist heute noch am Turmschaft erkennbar. Da der hydraulische Druck zum Antrieb der Fahrstühle mit Wasser aufgebaut wurde, das in Tanks in den Aussichtsplattformen untergebracht war, konnten in den Wintermonaten die Aufzüge nicht benutzt werden. Diese Aufzüge verkehrten fast 100 Jahre und wurden erst 1983 durch elektrische Fahrstühle der Firma Otis ersetzt. Die insgesamt vier Fahrkörbe verbinden die zweite und dritte Aussichtsplattform direkt miteinander. Diese Anlage kann bis zu 1140 Personen pro Stunde befördern.

Die Eisenfachwerkkonstruktion des Eiffelturms aus Puddeleisen wird mit mehreren Farbschichten vor Rost und Verwitterung geschützt. Bereits Gustave Eiffel betonte, dass der Anstrich für die Haltbarkeit von großer Bedeutung sei. Die erste Streichung erhielt der Turm bereits zwei Jahre nach seiner Eröffnung und er wurde bisher 19-mal neu angestrichen, zuletzt von März 2009 bis Oktober 2010 zum 120-jährigen Bestehen des Bauwerks. Damit wird der Eiffelturm im Durchschnitt alle sieben Jahre komplett neu lackiert. Die Anstricharbeiten werden von 25 Malern von Hand erledigt und kosten jeweils rund drei Millionen Euro.
Für die Fläche von 250.000 Quadratmetern werden etwa 60 Tonnen Lack – inklusive 10 Tonnen Primer – benötigt, wovon sich rund 45 Tonnen durch Erosion abschmirgeln. Die speziell ausgebildeten Maler werden während der Arbeiten mit rund 60 Kilometern Sicherheitsseilen gesichert. Von der Turmbasis bis zu seiner Spitze wird der verwendete Lack leicht abgetönt, um den Turm vor dem Hintergrund einheitlich gleichfarbig aussehen zu lassen.

Der Eiffelturm wurde farblich mehrfach neu gestaltet. Während zu Beginn der Errichtung des Turms noch ein venezianisches Rot vorherrschte, schwenkte man zur Eröffnung 1889 auf Rotbraun um. Dies wurde bereits 1892 durch Ockerbraun ersetzt. 1899 verwendete man ein in fünf Töne abgestuftes Gelborange und 1907 lackierte man das Wahrzeichen in Gelbbraun. Es folgten Orangengelb und Kastanienbraun, bis man seit 1968 den letzten Wechsel auf einen Bronzebraunton vollzog. Der Farbton „Eiffelturmbraun“ enthält die Farbpigmente Rot, Schwarz und Gelb. Diese werden vom deutschen Spezialchemie-Konzern Lanxess hergestellt und vom norwegischen Lackhersteller Jotun eigens für den Eiffelturm gemischt. Der urheberrechtlich geschützte Speziallack zeichnet sich durch ein hohes Maß an Haltbarkeit und Flexibilität aus und hält das Abplatzen unter Wind und Temperaturschwankungen so gering wie möglich.

Bereits zur Zeit seiner Fertigstellung war der Eiffelturm mit Gaslaternen beleuchtet. An der Turmspitze befanden sich zudem zwei auf Schienen verschiebbare Leuchtprojektoren, die mit einem hellen Leuchtfeuer den Pariser Nachthimmel in die Farben der französischen Trikolore eintauchten. Im Jahr 1900 wich die Gasbeleuchtung einer moderneren elektrischen Lichterkette aus 5000 Glühbirnen, welche die Konturen des Turms nachzeichnete. 1907 brachte man auf der ersten Aussichtsplattform eine sechs Meter hohe Uhr mit leuchtenden Ziffern an. Davor signalisierte man um 12 Uhr mit einem abgefeuerten Kanonenschuss die Mittagszeit.

Vom 4. Juli 1925 an leuchtete am Eiffelturm eine aus 250.000 Glühbirnen bestehende Reklame mit den auf drei Seiten des Eiffelturms vertikal angebrachten Lettern CITROËN. Die von André Citroën entworfene Werbung war damals die größte Leuchtreklame der Welt. 1933 ergänzte Citroën die Reklame um eine Uhr mit 15 Metern Durchmesser und farbigen Zeigern. Die Leuchtschrift des kostspieligen Lichtspektakels konnte bis zu einer Entfernung von 40 Kilometern entziffert werden und wurde 1936 wieder eingestellt.
Zur internationalen Werkausstellung 1937 in Paris hüllte der Architekt André Granet den Eiffelturm in ein Spitzenkleid aus Lichtstrahlen. 1985 installierte der Lichtingenieur Pierre Bideau eine neue Leuchteinheit am Eiffelturm, die zum Jahreswechsel 1986 eingeweiht wurde. Sie besteht aus 352 Natrium-Hochdruckscheinwerfern von je 600 Watt Stärke in Gruppen von vier bis sieben Leuchteinheiten und weist insgesamt eine Leistung von 320 kW auf. Die Anlage strahlt von unten bis zur Spitze und beleuchtet das Bauwerk vom Turminnern, wodurch sie die Struktur besser sichtbar macht. Der jährliche Stromverbrauch beträgt rund 680.000 kWh und sank mit der Neuinstallation um rund 40 %. Eine Glühbirne hat bei dieser Dauerbelastung eine mittlere Lebenserwartung von gut 6000 Stunden. 20.000 Lampen bringen seit dem 21. Juni 2003 von der Dämmerung bis 1 Uhr morgens – in den Sommermonaten bis 2 Uhr morgens – den Turm zu Beginn jeder Stunde für fünf Minuten wie einen Diamanten zum Glitzern.

2015 erfolgte im Rahmen einer energetischen Sanierung des Turmes eine erneute Änderung der Beleuchtung. Die zuvor installierten Lampen wurden durch energiesparende LED-Beleuchtung ausgetauscht. Zudem wurde eine Wärmepumpenheizung, eine Photovoltaikanlage sowie zwei Kleinwindenergieanlagen mit Horizontalrotor eingebaut, um einen Teil des Energiebedarfs des Turms mittels erneuerbarer Energien zu decken.

Am 5. April 1997 – genau 1000 Tage vor Beginn des Jahres 2000 – eröffnete der damalige Pariser Bürgermeister Jean Tiberi einen Countdown auf 100 Metern Höhe am Schaft des Eiffelturms. Auf der Nordwestseite zum Trocadéro leuchteten Tag und Nacht 33 Meter hohe, 12 Meter breite, 50 Tonnen schwere und aus 1342 Projektoren zusammengesetzte Leuchtziffern, welche die verbleibenden Tage bis zum Jahr 2000 anzeigten. Die Neujahrsnacht am 1. Januar 2000 wurde mit einem Feuerwerk am Eiffelturm eingeläutet. Die Countdown-Anzeige wechselte den Schriftzug zu "2000" und leuchtete das ganze Jahr hindurch.

Der Eiffelturm erhielt in der Vergangenheit zu bestimmten Anlässen entsprechende Sonderbeleuchtungen. So strahlten im Jahr seines hundertjährigen Bestehens 1989 die Lettern "100 ans" (100 Jahre) vom Turmschaft. Im Zuge des französisch-chinesischen Kulturaustauschprogramms wurde der Turm zwischen dem 24. und 29. Januar 2004 – der Zeit, in der die Chinesen Neujahr feiern – in rotes Licht getaucht. Der Einweihung der Lichtzeremonie wohnten der französische und der chinesische Kulturminister bei sowie die Bürgermeister von Paris und Peking. Das 20-jährige Jubiläum des Europatages am 9. Mai 2006 wurde am Eiffelturm mit blauem Licht gewürdigt. Am 1. Februar 2007 beteiligte sich der Eiffelturm an der Umweltschutzaktion Earth Hour und schaltete an diesem Tag von 19:55 Uhr bis 20:00 Uhr die Beleuchtung vollständig aus, um für das Energiesparen zu werben. Diese Aktion wurde im selben Jahr am 22. Oktober wiederholt. Zur Rugby-Union-Weltmeisterschaft 2007 vom 7. bis zum 20. Oktober wurde der untere Teil des Turms bis zur zweiten Aussichtsplattform in grünem Licht bestrahlt, was die Spielfläche symbolisierte. Zusätzlich wurde der Eiffelturm mit einem überdimensionalen Tor und einem Rugbyball bestrahlt. 2008 wurde von Juli bis Dezember anlässlich der Ratspräsidentschaft Frankreichs der Turm blau beleuchtet und zeigte die zwölf gelben Sterne der Europaflagge.

Nach den Anschlägen vom 13. November 2015 in Paris erstrahlte der Eiffelturm drei Tage lang in den französischen Landesfarben; aufgeteilt jeweils durch die drei Plattformen. Aus Solidarität in aller Welt wurden viele Monumente weltweit wie z. B. das Brandenburger Tor oder das Sydney Opera House ebenfalls in den Farben (→Illumination von Gebäuden) der französischen Trikolore angestrahlt.

Da in Frankreich keine Panoramafreiheit gilt, beansprucht die Betreibergesellschaft SETE das Urheberrecht für nächtliche Aufnahmen, in denen der bestrahlte Eiffelturm als Hauptobjekt zu sehen ist, obwohl am Bauwerk selbst keine Urheberrechte mehr bestehen. Sie sieht die Illumination als Kunstwerk für sich an, wobei diese Einstellung umstritten ist und gerichtlich nie bestätigt wurde. Das Gerichtsurteil von 1992, auf das sich gestützt wird, bezieht sich ausschließlich auf eine Lichtshow aus dem Jahr 1989 und nicht die tägliche nächtliche Beleuchtung des Turmes.
Private Bilder ohne kommerzielle Nutzung stellen unabhängig davon generell keinen Verstoß dar. Lediglich bei Bildern mit einer kommerziellen Nutzung ist eine Genehmigung erforderlich, wenn das Bauwerk urheberrechtlich geschützt ist. Detailaufnahmen oder Panoramaaufnahmen, bei denen der Eiffelturm nur als Beiwerk sichtbar ist, können unabhängig vom Zweck genehmigungsfrei veröffentlicht werden.

Der Eiffelturm ist grundsätzlich an 365 Tagen im Jahr ohne Ruhetag für die Öffentlichkeit zugänglich. Lediglich bei starken Stürmen kann es zur Schließung oder zu Einschränkungen kommen. Insgesamt sind am oder für das Wahrzeichen mehr als 600 Menschen beschäftigt. Darunter sind 280 Verwaltungsangestellte, die für die SETE arbeiten. Etwa 240 sind in den Restaurationsbetrieben angestellt, 50 im Souvenirverkauf und 50 üben weitere, meist technische Tätigkeiten aus. Im Turm befindet sich eine Poststelle, und ein eigenes Einsatzkommando der Polizei bewacht das Monument. Durch die vergleichsweise hohen Einnahmen bedingt gehört der Eiffelturm zu den wenigen französischen Sehenswürdigkeiten, die ganz ohne staatliche Subventionen auskommen.

Im Jahr seiner Eröffnung bestiegen im Rahmen der Weltausstellung 1889 knapp 1,9 Millionen der insgesamt 32,3 Millionen Ausstellungsbesucher auch den Eiffelturm. In den folgenden zehn Jahren ebbte die Besucherzahl auf ein Mittel von rund 250.000 ab. Während der Weltausstellung 1900 verbuchte der Eiffelturm trotz deutlich mehr Ausstellungsbesucher (50,8 Millionen) lediglich eine Besucherzahl von knapp über 1 Million. In den Folgejahren sank die Zahl weiter unter das Niveau der ersten zehn Jahre, bis der Turm während des Ersten Weltkrieges in den Jahren 1915 bis 1918 für die Öffentlichkeit gesperrt wurde. Mit der Wiedereröffnung 1919 stieg die Zahl der jährlichen Besucher auf knapp 480.000. Zwei markante Ausreißer gab es 1931 und 1937 zur Pariser Kolonial- bzw. zur Weltfachausstellung mit jeweils über 800.000 Gästen. Wegen des Zweiten Weltkrieges musste der Eiffelturm zum zweiten Mal für mehrere Jahre (1940–1945) geschlossen werden. Nach dem Krieg eröffnete der Eiffelturm im Juni 1946 wieder. Die Zahl stieg bereits Anfang der 1950er Jahre kontinuierlich auf rund 1 Million Besucher an und steigerte sich in den folgenden Jahrzehnten durch den Tourismus immer weiter. Mitte der 2000er Jahre kletterte die Besuchermarke auf über 6,5 Millionen und erreichte 2011 mit knapp 7 Millionen ihren vorläufigen Rekord. Der erwirtschaftete Umsatz 2011 erreichte dabei 85,7 Millionen Euro. Mit der gestiegenen Besucherzahl, die an Spitzentagen rund 35.000 erreicht, steigen nicht nur die Wartezeiten auf mehrere Stunden an, sondern droht der Sehenswürdigkeit eine Überfüllung. Einschließlich 2011 haben seit Eröffnung über 260 Millionen das Bauwerk besichtigt und machen es damit zu dem meistbesichtigten der Welt.

Laut einer statistischen Befragung von 7.989 Besuchern ergab sich 2009 folgendes Profil: Der überwiegende Teil der Besucher kommt aus Westeuropa (43 %), dem metropolitanen Frankreich (29 %) und Nordamerika (11 %). Abgesehen von Frankreich sind die stärksten Besucherländer Deutschland mit 8,5 %, das Vereinigte Königreich mit 8,1 %, gefolgt von den Vereinigten Staaten (7,6 %), Spanien (7,3 %), Italien (4,8 %) und Australien (4,1 %). Teilt man das Alter der Besucher in die Kategorien „unter 25“, von „26 bis 35“, von „36 bis 45“ und „darüber“ auf, so nehmen sie jeweils rund ein Viertel ein. Über 56 Jahre waren nur 6,4 %. Der größte Teil der Besucher kommt mit ihrer Familie (63,8 %), rund 23 % besuchen den Eiffelturm mit Freunden und 7,8 % in organisierten Reisegruppen. Knapp die Hälfte (46,1 %) reist mit der Metro an, 17,3 % kommen zu Fuß, 12 % benutzen das eigene Auto und 7,5 % werden mit dem Bus gebracht. Etwa 46 % der befragten Personen waren vorher schon einmal auf dem Eiffelturm.

Die Allgegenwart des Eiffelturms im Pariser Stadtbild verleitete immer wieder Menschen zu wagemutigen Abenteuern oder sportiven Höchstleistungen. Am 13. Juli 1901 entging der brasilianische Flieger Alberto Santos Dumont mit seinem Luftschiff nur knapp einer Kollision, als er das Fluggerät zwischen Saint-Cloud und Champ de Mars manövrierte.
Der Turm inspirierte einige Menschen, mit selbstgebastelten fallschirmähnlichen Konstruktionen einen Sprung vom Eiffelturm zu wagen. Zu den tragischen Figuren gehört der Schneider Franz Reichelt, der sich einen Gehrock mit breitem Cape schneiderte und Sprungfedern daran montierte. Sein angekündigtes Vorhaben lockte zahlreiche Schaulustige an. Nach einigem Zaudern sprang der gebürtige Österreicher Reichelt am 4. Februar 1912 mit seiner flugunfähigen Ausstattung von der ersten Plattform vor den Augen der anwesenden Journalisten und Zuschauer und verunglückte dabei tödlich. Von diesem Ereignis existiert sogar ein historisches Filmdokument.

Der Franzose Marcel Gayet kam 1928 bei einem ähnlichen Versuch durch einen Sprung von der ersten Etage ums Leben. Weitere Versuche mit neuartigen Fallschirmen glückten, was die Macher der James-Bond-Filme zu einer entsprechenden Szene inspirierte. Der damals 23-jährige Flieger Léon Collot verunglückte im November 1926 bei dem Versuch, mit seinem Leichtflugzeug den Turmbogen an der Basis zu durchfliegen. Er wurde von der Sonne geblendet und verfing sich in einer Radioantenne, die damals noch zwischen Turmspitze und Boden gespannt war.

Das Pariser Wahrzeichen war auch Schauplatz vieler Suizide. Der erste Selbstmord wurde am 15. Juni 1898 gemeldet, als sich eine Frau erhängt hatte. Insgesamt haben sich etwa 400 Menschen am Eiffelturm das Leben genommen.

Der Eiffelturm regte die Menschen immer wieder zu artistischen oder sportlichen Herausforderungen an. Graf Lambert überflog am 18. Oktober 1909 den Turm erfolgreich mit seinem Flugzeug. Daneben war der Turm auch Schauplatz von nicht alltäglichen Leistungen, Spaßrekorden oder sonstigen medial beachteten Aktionen. Bereits 1905 lobte die Zeitung "Le Sport" einen Wettbewerb für die schnellste Besteigung bis zur zweiten Plattform aus. Am Treppenlauf-Wettbewerb am 26. November beteiligten sich 227 Läufer. Der Gewinner Forestier schaffte dies in 3 Minuten 12 Sekunden und erhielt für seine Leistung ein Peugeot-Fahrrad. Zum 75. Geburtstag des Eiffelturms kletterten im Mai 1964 die Bergsteiger Guido Magnone und René Desmaison offiziell genehmigt den Eiffelturm an seiner Außenseite hoch. Das Spektakel wurde über Eurovision gesendet.

Am 4. Juni 1948 stieg ein 85-jähriger Elefant, der aus dem Zirkus "Bouglione" entlaufen war, bis zur ersten Plattform empor. 1983 fuhren Charles Coutard und Joël Descuns mit ihren Motocross-Motorrädern die Treppen im Eiffelturm hinauf und hinunter. Ein Jahr später gelang es Amanda Tucker und Mike MacCarthy, ohne offizielle Erlaubnis mit ihren Fallschirmen von der dritten Plattform abzuspringen. Der Neuseeländer A. J. Hackett wagte 1987 erstmals einen Bungee-Sprung von der zweiten Aussichtsplattform.

Dem Hochseilartisten Philippe Petit glückte 1989 die Überquerung von rund 800 Metern auf einem vom Palais de Chaillot zur zweiten Etage des Eiffelturms gespannten Kabel über die Seine. Petit setzte sich etwa 15 Jahre für die Bewilligung dieses Vorhabens ein. Den etwa einstündigen Lauf verfolgten rund 250.000 Zuschauer.

1995 brach der Triathlet Yves Lossouarn den Rekord für die Turmbesteigung. Bis zur Spitze benötigte er 8 Minuten und 51 Sekunden. In dem vom Fernsehsender "arte" initiierten Sportereignis ging er als Sieger eines Starterfelds von 75 Athleten hervor. Auch Base-Jumper sprangen mehrfach vom Eiffelturm, darunter auch der bekannte Schweizer Ueli Gegenschatz, der am 1. April 2008 von der höchsten Plattform herunter sprang.

Neben den vier Weltausstellungen in den Jahren 1889, 1900, 1931 und 1937 wurde der Eiffelturm immer wieder für Konzerte oder andere Großveranstaltungen als Kulisse oder Veranstaltungsort verwendet.

Am 25. September 1962 sang Édith Piaf auf der ersten Etage des Eiffelturms vor 25.000 Zuhörern ihr letztes Konzert. Gleichzeitig wurde die Veranstaltung als Werbeplattform für den Film Der längste Tag genutzt. Die Chansonniers Charles Aznavour und Georges Brassens gaben 1966 ebenfalls am Eiffelturm ein Konzert. Am 14. Juli 1995 hielt Jean Michel Jarre unter der Schirmherrschaft der UNESCO ein Konzert für mehr Toleranz am Fuße des Eiffelturms ab. Das weltweit übertragene Konzert hatte 1,2 Millionen Zuhörer.

Zum 12. Weltjugendtag 1997 versammelten sich am 21. August rund 300.000 Pilger auf dem Champ de Mars vor dem Eiffelturm, wo der damalige Papst Johannes Paul II. eine Ansprache hielt.

Das Orchestre de Paris und das Boston Symphony Orchestra hielten im Mai 2000 unter der Leitung von Seiji Ozawa ein freies Konzert vor dem Eiffelturm, der zu diesem Anlass extra beleuchtet wurde. Den Konzerten wohnten rund 800.000 Menschen bei. Im selben Jahr gab am 10. Juni Johnny Hallyday vor 600.000 Zuschauern ein Freiluftkonzert, das von einer licht- und pyrotechnischen Show begleitet wurde.

Der Bau des Eiffelturms brachte der Stadt einen beträchtlichen Prestigezuwachs und löste eine weltweite Turmbauwelle aus. Viele andere Städte, in der Anfangszeit besonders in der bedeutenden Kolonialmacht des Vereinigten Königreichs Großbritannien, versuchten dem Projekt nachzueifern. Zu den ersten Nachbauten zählt der in den Jahren 1891 bis 1894 erbaute 158,1 Meter hohe Blackpool Tower im englischen Badeort Blackpool. Dieser Turm gilt trotz seiner starken Anleihen beim Eiffelturm architektonisch als gelungen und wurde in den Denkmalschutz mit der höchsten Klassifizierungsstufe (Grade I) in England aufgenommen. Der Turm erwächst aus einem großen, mehrstöckigen Basishaus im viktorianischen Stil und beherbergt eine Reihe von Attraktionen, unter anderem einen renommierten Zirkus. Der Blackpool Tower wurde in den letzten Jahren aufwändig restauriert und gilt nach wie vor als Anziehungspunkt für Touristen in der Region von Nordwest-England. Weniger erfolgreich war der nach einem ähnlichen Konzept errichtete New Brighton Tower (Baubeginn 1896); er musste, weil die Stahlgitterkonstruktion marode geworden war, in den 1920er Jahren abgetragen werden. Auch dieser 172,8 Meter hohe Turm hatte ein Basishaus mit einem vielfältigen Freizeitangebot, unter anderem befand sich dort der größte Ballsaal Großbritanniens. Beide Türme waren nach ihrer Erbauung jeweils die höchsten Bauwerke des Landes.

Auch die britische Hauptstadt London schrieb 1890 ein ehrgeiziges Turmbauprojekt aus. Die Projektvorschläge sahen Türme aus Stahl zwischen 300 und 456 Meter Höhe vor. Ein Jahr später begann der Bau des Watkin’s Tower, der auf 358 Meter und damit rund 50 Meter höher als der Eiffelturm projektiert war. Der Initiator des Vorhabens, Sir Edward Watkin, versuchte ursprünglich Gustave Eiffel selbst als Konstrukteur anzuwerben; der Franzose lehnte jedoch aus patriotischen Gründen ab. Als dem Projekt die finanziellen Mittel ausgingen, so dass nur ein Turmstumpf von 47 Metern übrig blieb, wurde der Turm 1907 abgebrochen. Auch weitere Turmprojekte hatten mäßigen Erfolg. Der Turmbau in Douglas, dem Hauptort der Isle of Man, musste bereits kurz nach dem Einbau der Fundamente im Oktober 1890 wieder beendet werden. Der 70 Meter hohe pyramidenförmige Turm im Seebad in Morecambe, ein Bauwerk, das sich architektonisch allerdings deutlich vom Eiffelturm unterschied, wurde zu Beginn des Ersten Weltkrieges zugunsten der Munitionsproduktion abgerissen. Auch in Deutschland gab es teilweise abenteuerliche Projektvorschläge, den Eiffelturm zu übertrumpfen. So wurde 1913 ein fragwürdiger Entwurf zum "Rheinturm" – heutzutage trägt der Fernsehturm von Düsseldorf den Namen Rheinturm – vorgestellt, einem 500 Meter hoch messenden Stahlfachwerkturm mit stilistisch starker Anlehnung an den Eiffelturm. Das Vorhaben wurde nie umgesetzt.

Im Januar 1890 war in St. Petersburg eine 60 Meter hohe vergängliche Nachbildung des Eiffelturms aus Eis zu bestaunen. 1891 errichtete man anlässlich der Industrieausstellung in Prag den 60 Meter hohen Aussichtsturm Petřín, der Formen des Eiffelturms aufgriff. Aber auch in Frankreich selbst eiferte man dem Pariser Vorbild nach. Der zwischen 1892 und 1894 errichtete, 85,9 Meter hohe Tour métallique de Fourvière in Lyon gibt die konstruktive Grundform des oberen Teils des Eiffelturms wieder. Der Turm war öffentlich zugänglich und beherbergte auch ein Restaurant. Seit 1953 dient er nur noch als Radio- und Fernsehturm. Mit der Ausbreitung von Funk- und Radiowellen wurden besonders ab den 1920er Jahren weitere Turmbauten notwendig. Auch wenn die Formgebung dieser Bauwerke teilweise wenig Ähnlichkeit mit dem Pariser Turm aufwies, genügte oft die Übereinstimmung von vier Turmfüßen und der konstruktionsbedingten Notwendigkeit einer Verjüngung zur Spitze hin, dass diese Bauwerke im Volksmund mit dem Eiffelturm in Verbindung gebracht wurden oder werden. Beispiele dafür sind der Sender Gleiwitz („Schlesischer Eiffelturm“), der ehemalige Sender Ismaning („Bayerischer Eiffelturm“) oder der Bismarckturm in Wiesbaden („Wiesbadener Eiffelturm“). Auch der Berliner Funkturm aus der Mitte der 1920er Jahre folgt diesem Konstruktionsprinzip.

In den 1950er Jahren wurden in Japan mehrere vom Architekten Tachū Naitō (1886–1970) entworfene Fernseh- und Aussichtstürme erbaut, die ästhetisch zwar technischer ausfallen, sich aber dennoch am Design des Eiffelturms orientieren. Im Jahr 1954 entstand der Fernsehturm Nagoya, 1956 der Fernsehturm Tsutenkaku, 1957 der Fernsehturm Sapporo und 1958 der Tokyo Tower. Besonders der Tokyo Tower, der mit seinen 333 Metern den Eiffelturm sogar um einige Meter überragt, wird häufig im Zusammenhang mit der Nachahmung der baulichen Struktur genannt. Architektonisch wird er wegen seiner Proportionen in der Verjüngung der Stahlgitterstruktur nach oben und den Turmkörben sowie der Wahl der Diagonalverbände als weniger gelungen angesehen.

Mit der Etablierung von Fernsehturmbauwerken aus Stahlbeton in vertikaler Kragarmbauweise, begonnen mit dem Stuttgarter Fernsehturm Mitte der 1950er Jahre, nahmen die gestalterischen Ähnlichkeiten der Türme zum Eiffelturm deutlich ab. Dennoch wurde der Eiffelturm in zahlreichen Repliken aufgrund seiner symbolhaften Ausstrahlung immer wieder aufgegriffen. Besonders in Frankreich und den Vereinigten Staaten sind teilweise nur wenige Meter hohe Nachbildungen in Kreisverkehren, als Werbeträger oder in Vorgärten anzutreffen. Vor allem die Freizeitindustrie hat die starke Werbekraft für sich entdeckt und versucht, durch Nachbauten immer wieder Aufmerksamkeit auf sich zu lenken. Zu den bekanntesten Repliken zählt eine 108 Meter hohe Nachbildung in dem Freizeitpark "Window of the World" im chinesischen Shenzhen. Eine weitere 108 m Höhe Nachbildung China – im Maßstab 1:3 – steht 9275 km vom Original entfernt in Tianducheng. Der mit 165 Metern bisher höchste Nachbau steht in Las Vegas. Der Nachbau aus dem Jahr 1999 – über 100 Jahre nach Erbauung des Eiffelturms – am Hotelkomplex des Paris Las Vegas beherbergt auch ein Turmrestaurant und hat wie sein Vorbild besteigbare Aussichtsplattformen. Im selben Maßstab wurde 2016 am Cotei Strip in Macau ein weiterer Nachbau des Eiffelturms verwirklicht.

Darüber hinaus wird der Pariser Eiffelturm in fast allen Miniaturenparks nachgebildet.

Der Eiffelturm wird dem Baustil des Historismus der Gründerzeit, den er selbst einleitete, zugeordnet, wodurch er sich von der klassischen Architektur des 19. Jahrhunderts unterscheidet. Er ist eine wichtige Wegmarke des Funktionalismus, der sich in Europa nur vereinzelt durch Ingenieurbauten wie beispielsweise den Kristallpalast in London manifestierte. Damit stellt der moderne Ingenieurbau die nach der Gotik verloren gegangene Einheit von Konstruktion und Baugestalt wieder her, was dem Eiffelturm eine vergleichbare Position einbringt wie einem historischen Sakralbau. Sein baulicher Ansatz des breit ausgreifenden Fundaments wegen der nach unten zunehmenden Beanspruchung durch Winddruck hat sein Naturvorbild bei den Bäumen, die im Boden mit einem weit verzweigten Wurzelwerk verhaftet sind und deren Stamm sich in der Höhe verjüngt. (→Architektur) Damit nimmt er nicht nur formal, sondern auch technologisch eine Vorreiterrolle ein, da bis zur Errichtung des Stuttgarter Fernsehturms praktisch alle freistehenden Sendetürme aus Stahl- oder Eisenfachwerk nach dem Vorbild Eiffels errichtet wurden. Die architektonisch herausragende Stellung und Bewertung des Eiffelturms beruht nicht nur auf seiner weitreichenden Wirkung, sondern auch darauf, dass er ohne jedes historische Vorbild entstanden ist.

Viele traditionelle Künstler, denen die Antike Vorbild war, sahen im Eiffelturm eine Vermischung von Kunst und Alltag und lehnten ihn deshalb vehement ab. Insbesondere wandte sich die Kulturwelt des damaligen Frankreich gegen jeden staatlichen Druck, künstlerische und industrielle Arbeitskraft zur Produktverbesserung zusammenzuführen. Der Schriftsteller Charles Baudelaire, der den Eiffelturm jedoch nicht erlebte, drückte es wie folgt aus:

Damit einher ging eine Diskussion über die Radikalisierung des Kunstbegriffs; sie wurde durch die Errichtung des Eiffelturms weiter angeheizt. Der prinzipiell idealisierte Charakter der Kunst war sozial vornehmlich auf die Oberklasse bezogen und nahm mit dem neuen Pariser Wahrzeichen plötzlich auch in der Lebensgestaltung der einfachen Leute eine Rolle ein. Es verband sich die Abneigung gegenüber dem Volksvergnügen mit der Angst vor aufständischem Potential des gemeinen Volkes gegenüber der Oberschicht. Die Pariser Oberklasse hielt sich zum großen Teil vom Turm wie auch von der Massenveranstaltung der Weltausstellung fern. Der Widerwille der Oberschicht gegenüber dem „kleinen Mann“ spiegelt sich auch in der Legende wider, nach der Guy de Maupassant eigens auf den Turm gestiegen sein soll, weil dies der einzige Ort sei, an dem er ihn nicht sehen müsse, obgleich er einer der stärksten Gegner des von Hitze, Staub, und Gestank geprägten Massenbetriebs war. Die republikanische Presse war dem Projekt insgesamt gewogener als die religiös-konservativen Kräfte mit häufig monarchistischen Tendenzen. Bereits die in der Festrede zur Fertigstellung des Eiffelturms unterstrichene kollektive Leistung von Baumeistern auf der einen und Bauausführenden auf der anderen Seite stieß an einigen Stellen auf Ablehnung. Die Nähe zur Arbeiterklasse thematisierte Eiffel sogar selbst, als er sich in einem Plakat mit Maßwerkzeug, aber auch mit Arbeiterkluft zeigte und damit die Verbindung von geistiger und körperlicher Anstrengung verkörperte. Für die alten Eliten war diese Zuwendung zu den Massen eine Bedrohung ihres Führungsanspruchs.

Auch außerhalb Frankreichs wurde die Erbauung des höchsten Turms der Welt rezipiert. Besonders Deutschland, das nach dem Deutsch-Französischen Krieg in einem gespannten Verhältnis zu Frankreich stand, kommentierte die Weltausstellung und den Eiffelturm implizit politisch gefärbt. Dabei wird die ambivalente Beurteilungen zwischen bewundernder Impression und einem gewissen Unwohlsein zum Tenor der Meinungen. Die Deutsche Rundschau charakterisiert den Eiffelturm emphatisch als Maschinenungeheuer. In der mythischen Parallele zum Babel-Turm schwingen neben der Begeisterung für die Bezwingung der Mächte immer auch die Bedenken mit, diese Mächte herausgefordert zu haben. Auch aus der Sicht des Journalisten Eugen von Jagow, der sogar den ätherischen Charakter der transparenten Architektur hervorhebt und sich einer gewissen Faszination nicht entziehen kann, geißelt ebendiese Form als unarchitektonisch, verwirrend und lässt sie letztlich an sich selbst scheitert. Die schiere Höhe, die den Kölner Dom fast um das Doppelte überrage, imponiere ihm, an künstlerischer Größe und Erhabenheit sei ihm das alte Kirchengebäude aber bei weitem überlegen. Seine Schlussfolgerung ist, dass es sich bei dem Turm mehr um einen Triumph der Wissenschaft als der Kunst handelt. In einer insgesamt kunstfeindlichen Zeit sei er das Symbol der Moderne. Gerade diese Gegenüberstellung von quantitativer und qualitativer Größe entspricht der vorherrschenden Argumentationsstrategie, den elitär verstandenen Kunstbegriff von der Massenkultur abzugrenzen. Die Interpretation als Sieg der Massen gegenüber dem Individuum wird zum Teil der Auseinandersetzung mit der Demokratie, die seit Alexis de Tocqueville als Amerikakritik topischen Charakter erhielt.

Ungeachtet der Kontroverse warf der Eiffelturm bereits vor seiner Errichtung seinen Schatten und inspirierte Jules Verne in dem im August 1886 erschienenen Science-Fiction-Roman "Robur der Sieger", seine Eindrücke von einem Turm am Champ de Mars zu verarbeiten:

Nach seiner Eröffnung gehörten die Dichter zu den ersten, die den Eiffelturm beschrieben. Der Schriftsteller Blaise Cendrars besang den „Turm, Turm der Welt, Turm in Bewegung“. Der chilenische Lyriker Vicente Huidobro beschrieb ihn als „Himmelsgitarre“ („Guitarra del cielo“) und brachte 1918 einen Gedichtband mit dem Titel "Tour Eiffel" heraus. Der auch als Regisseur tätige Schriftsteller Jean Cocteau veröffentlichte das Libretto "Les mariés de la tour Eiffel" (deutsch: Hochzeit auf dem Eiffelturm), das am 18. Juni 1921 als Ballett im Théâtre des Champs-Élysées in Paris aufgeführt wurde. Die absurde, surrealistische Geschichte eines Hochzeitspaares spielt auf dem Eiffelturm, der inmitten der namibischen Wüste steht. Der Lyriker Guillaume Apollinaire verarbeitete seine Erlebnisse des Ersten Weltkrieges mit einem Gedicht in dem Buch "Calligrammes" in Form des Eiffelturms. Die mit einem Willkommensgruß an die Welt beginnende Poesie endet mit gigantischen, französischen Beschimpfungen der Deutschen.

Auch in der Musik wird der Eiffelturm immer wieder verarbeitet. Unter anderem besangen ihn Michel Emer in "Paris, mais c’est la Tour Eiffel", Charles Trenet in "Y’a d’la joie, la Tour Eiffel part en balade", Léo Ferré in "Paris portait sa grande croix", Jacques Dutronc in "La Tour Eiffel a froid aux pieds" und Pascal Obispo in "Je suis tombé pour elle". Der estnische Komponist Arvo Pärt schuf 2009 die ein Jahr später in Paris uraufgeführte symphonische Dichtung "Silhouette – Hommage à Gustave Eiffel" für Streichorchester und Schlagzeug, die die Architektur des Turms in der Struktur der Komposition nachbildet und zugleich durch die Gestaltung der Luftbewegungen, die durch das Gestänge fließen, poetisiert.
Mit dem Eiffelturm setzte sich auch die Malerei intensiv auseinander. Er wurde in fast allen Stilen seit Ende des 19. Jahrhunderts von einer Vielzahl von international bedeutenden bildenden Künstlern gemalt. Das höchste Pariser Wahrzeichen trug gerade durch seinen technischen Charakter zu einer Debatte in der Kunst bei, welche ganz neue Ansätze für architektonische und räumliche Ausdrucksformen fand.

Bereits 1888 – also noch vor seiner Fertigstellung – malte Georges Seurat ein Bild mit dem Werktitel "La Tour Eiffel", das heute in den Fine Arts Museums of San Francisco ausgestellt ist. Zu den berühmtesten Malern, die den Eiffelturm malten, zählen unter anderem Henri Rousseau, Paul Signac, Pierre Bonnard, Maurice Utrillo, Marcel Gromaire, Édouard Vuillard. Raoul Dufy malte 1890 "Seine Grenell", das Bild befindet sich im Privatbesitz. Marc Chagall malte 1913 "Paris Through the Window", wo er das Pariser Stadtbild mit dem beherrschenden Eiffelturm und daneben einen Fallschirmspringer darstellte. Chagall griff 1954 das Motiv des Turmes ein weiteres Mal in "Champ-de-Mars" auf.

Robert Delaunay erstellte sogar eine ganze Bilderserie, in der er den Turm aus vielen Perspektiven kubistisch darstellte. Zu den bekanntesten zählt "The Red Tower" aus dem Jahr 1911, das sich im Solomon R. Guggenheim Museum befindet, und " La ville de Paris" von 1910/12, das im Centre Georges Pompidou hängt. Delaunay nutzte die Architektur und Lichtwirkung des Bauwerks, um den Zusammenklang und das Wechselspiel der Farben zu untersuchen.

Auch zeitgenössische Maler greifen den Eiffelturm als Motiv immer wieder auf.

Aufgrund seiner Bedeutung und Bekanntheit zieht der Eiffelturm immer wieder in Filme ein. In fast allen Filmen mit dem Handlungsort Paris ist er Erkennungsmerkmal der städtischen Skyline. Darüber hinaus war er auch oft selbst Schauplatz von filmischen Handlungen. Die Filmgenres, in denen er thematisiert wurde, reichen vom reinen Dokumentarfilm bis hin zu Kriminalfilmen, romantischen Liebeskomödien und zu Action-, Science-Fiction- und Katastrophenfilmen. Die vielfältige Einbeziehung des Eiffelturms erklärt sich zum einen aus seinem starken Symbolcharakter, zum anderen deswegen, weil Turm und Kino im selben zeitlichen Abschnitt (→ Filmgeschichte) entstanden.
Zu den ersten Filmen überhaupt gehört das dokumentarisch ausgeführte "Panorama pendant l’ascension de la tour Eiffel" der Brüder Lumière aus dem Jahr 1897, in dem die Turmauffahrt gezeigt wird, sowie "Images de l’exposition 1900" von Georges Méliès. Der erste Science-Fiction-Stummfilm, bei dem der Eiffelturm als Objekt einbezogen wurde, war "Paris qui dort" (deutsch: "Paris im Schlaf") von Regisseur René Clair aus dem Jahr 1925. Er wird aufgrund seiner irrealen Atmosphäre den Avantgardefilmen zugeordnet. Dabei erwacht ein Mann auf dem Eiffelturm nach einem Anschlag eines verrückten Wissenschaftlers, findet Paris als Geisterstadt vor und sucht mit wenigen ebenfalls verschont gebliebenen Menschen nach einem Ausweg. 1928 thematisierte Clair in "La Tour" die Architektur und Strenge der Konstruktion des Eiffelturms.

In der Liebeskomödie "Ninotschka" von 1939 verfolgt Graf Leon, gespielt von Melvyn Douglas, die ihm noch unbekannte Ninotschka, dargestellt von Greta Garbo, auf einem Spaziergang zum Eiffelturm, den sie besichtigen will. Dort treffen sich die beiden, und Leon erklärt Ninotschka detailreich die Vorzüge der Eisenkonstruktion. Im Thriller "Der Mann vom Eiffelturm" aus dem Jahr 1949 ist der Turm zentraler Handlungsort und erscheint im Filmtitel und im Plakat. Der Regisseur Burgess Meredith spielt in seinem Film selbst mit. Gegen Ende des Films kommt es zu einer spektakulären Kletterpartie auf den Eisenstreben des Wahrzeichens.

Kurze Einblendungen des Eiffelturms – meist um auf die Stadt Paris hinzuweisen – gab es beispielsweise in den Filmen "Casablanca", "Die Brücke am Kwai", "Das Glück kam über Nacht " oder in "Sie küßten und sie schlugen ihn". Im Truffaut-Krimi "Auf Liebe und Tod" schlägt Fanny Ardant als Barbara Becker mit einer eisernen Eiffelturm-Nachbildung einen Priester nieder. Die Szene wird auch im ursprünglichen Filmplakat thematisiert. In der Agentenfilmparodie "Der große Blonde mit dem schwarzen Schuh" wird die Spitze des Eiffelturms als Agentenhauptquartier benutzt. Im Agentenfilm "James Bond 007 – Im Angesicht des Todes" von 1985 findet eine spektakuläre Verfolgung statt, die mit einem Fallschirmsprung vom Bauwerk endet.

In Endzeitfilmen wurde der Eiffelturm häufig zur Steigerung der emotionalen Wirkung zerstört oder als Ruine dargestellt. Dies geschieht beispielsweise in der H.-G.-Wells-Literaturverfilmung "Kampf der Welten" von 1953, im US-amerikanischen Film "Independence Day" von 1996, in der Science-Fiction-Persiflage "Mars Attacks!" von 1996, in "Alien – Die Wiedergeburt" von 1997 und im Katastrophenfilm "Armageddon – Das jüngste Gericht" von 1998. Im Actionfilm "G.I. Joe – Geheimauftrag Cobra" wird der Eiffelturm zum Angriffsziel eines kriminellen Sprengkommandos.

Der Eiffelturm und seine Geschichte waren auch mehrmals Gegenstand von Dokufiktion-Verfilmungen (→ Filme über den Eiffelturm).

Hohe Türme haben nicht nur aufgrund der biblischen Vorlage des Turmbaus zu Babel einen kulturellen Hintergrund, sondern gelten auch als Sinnbild für die Überwindung der Schwerkraft, als Zeichen der Herrschaft über den Raum und damit auch oft über die Menschen im Umkreis. In diesem Kontext ist der ursprüngliche Widerstand gegen den Eiffelturm als ein besonders herausragendes Beispiel der beherrschenden Macht von technischen Türmen wie Hochöfen, Fördertürme, Gasometern, Silos oder Industrieschornsteinen, die im 19. Jahrhundert entstanden, zu sehen. Andererseits erfüllte Eiffel mit der Errichtung seines Turms anscheinend einen Menschheitstraum, nachdem rund 100 Jahre zuvor von Montgolfière bereits der Traum vom Fliegen verwirklicht worden war.
Der Eiffelturm hatte über die architektonische Leistung hinaus eine starke Bedeutung für das französische Nationalbewusstsein. Das Bauwerk präsentiert sich als historische Erinnerung an die Französische Revolution und unterstreicht die aufstrebende Wirtschaftsmacht Frankreichs im ausgehenden 19. Jahrhundert. Der Stolz auf diese Vergangenheit und die Emanzipation von der Monarchie prägte den Geist der Weltausstellungen, die 1867 und 1878 in Paris stattgefunden hatten. Dieses offene Bekenntnis zu demokratischen Idealen und damit zur antimonarchischen Haltung stand der weltweiten Akzeptanz des Ausstellungsprojektes im Wege, besonders bei monarchisch geprägten Staaten. Im historischen Kontext hat der Eiffelturm damit die Funktion eines Revolutionsdenkmals. Eugène-Melchior de Vogue sah ihn gar als neue Kirche der innerweltlichen Vollendung. Damit verkörpert der Eiffelturm den Triumph der Französischen Revolution, die Dritte Französische Republik und das Industrielle Zeitalter. Für die breite Öffentlichkeit hatte der Turm große Anziehungskraft; vor allem die einfachen Leute aus den Provinzen Frankreichs wollten sich das Wunderwerk unbedingt ansehen. Der Eiffelturm war aber auch ein Treffpunkt für die unterschiedlichen gesellschaftlichen Schichten, die zu jener Zeit im öffentlichen Leben strikt getrennt waren. Aus diesem Grund trug das Bauwerk dazu bei, dass ganz in der republikanischen Gesinnung die Trennungslinie zwischen den Klassen verwischt wurde.

Damit positioniert sich der Eiffelturm als eine moderne Form der Festarchitektur und als Medium, das auf gewaltlose Weise das französische Volk auf republikanische Werte einschwor. Weil er Frankreichs Stärke und Macht so nachhaltig repräsentiert, wird der Eiffelturm, der den Spitznamen "la dame de fer" (deutsch: die eiserne Dame) erhielt, wie nur wenige andere Bauwerke unmittelbar mit Frankreich in Verbindung gebracht. Bereits 1987 gaben 25 % der Franzosen bei einer Umfrage, welches historische Bauwerk ihr Land am besten repräsentiere, den Eiffelturm an, deutlich vor dem Schloss Versailles mit 17 %, dem Triumphbogen und dem Place de la Bastille mit jeweils 13 %. Der französische Philosoph Roland Barthes leitet die weltweite Allgegenwart des Turmes aus seiner Zeichenhaftigkeit her und schreibt:

Auf der 200-Franc-Banknote war der Eiffelturm bis zur Euroeinführung auf der Vorder- und Rückseite als stilisierte Silhouette zu sehen. Während auf der Vorderseite das Porträt Gustave Eiffels zu sehen war, war auf der Rückseite zusätzlich zur Silhouette ein Blick durch die Turmbasis der vier Pfeiler dargestellt. Zum 100-jährigen Bestehen des Eiffelturms brachte 1989 die Banque de France in einer Auflage von 800.000 Stück eine 5-Franc-Gedenkmünze aus Silber heraus. Anlässlich des 125. Jahrestages der Einweihung des Eiffelturms erscheint in Frankreich am 3. März 2014 eine 50-Euro-Goldmünze in einer limitierten Auflage von 1000 Exemplaren. Die Münze zeigt auf ihrer Vorderseite das Logo der UNESCO und einen Stadtplanausschnitt, auf welchem sich der Eiffelturm befindet. Die Rückseite thematisiert die Stahlstrebenkonstruktion durch eine stilisierte und detaillierte Ansicht.

Die französische Post hielt sich zu Beginn mit einer Würdigung des Bauwerks auf einer Briefmarke zurück. Das erste Postwertzeichen Frankreichs mit dem Eiffelturm als Hauptmotiv erschien im Jahr des 40-jährigen Bestehens am 5. Mai 1939 (Yvert et Tellier Nr. 429) mit einer Auflage von 1.140.000 Stück. Der Turm wird auf der roséfarbenen Briefmarke mit Frankatur 90c+50c im 45-Grad-Winkel dargestellt. Allerdings erschien bereits 1936 eine Serie mit einem Postflugzeug über dem Himmel von Paris, in der man zur Silhouette der Stadt dazugehörig im Hintergrund den Eiffelturm sah. Zum Hauptmotiv wurde er wieder 1989 zu seinem 100-jährigen Bestehen sowie 2009 und 2010. Zu einigen Kongressen und Veranstaltungen, die in Paris stattfanden, wurde der Eiffelturm in den vergangenen Jahrzehnten immer wieder als Symbol auf Briefmarken verwendet. Beispielsweise kam zu den Weltmeisterschaften im Gewichtheben 2011 in Paris eine große Blockausgabe heraus, in welcher der Eiffelturm grafisch als Gewichtheber dargestellt eine Hantel hebt, dessen beide Hantelscheiben je eine runde Briefmarke zum Nennwert 60 bzw. 89 Cent haben. Insgesamt trugen bis 2011 weit über 30 französische Briefmarken den Eiffelturm als Motiv.

Die werbetechnische Vermarktung des Eiffelturms begann bereits vor seiner Fertigstellung. Gustave Eiffel organisierte in regelmäßigen Abständen entsprechende Maßnahmen – nicht zuletzt um ein Gegengewicht zu den immer wieder laut werdenden Proteststimmen zu schaffen. Schon im Frühjahr 1886 wurden Artikel, Broschüren und verschiedene Abbildungen produziert und verbreitet. Das machte den Turm bereits vor seiner Eröffnung weltberühmt. Das Ausmaß bewog den Journalisten und Dramaturgen Henry Buguet (1845–1920) bereits am 13. September 1888 in "Le Soir" zu folgender entrüsteten Frage:

Die massenweise Herstellung von Eiffelturm-Souvenirs setzte bereits mit Eröffnung des Wahrzeichens ein. Bereits damals kannte die Formenvielfalt kaum Grenzen. Dies wurde auch von der heutigen Betreibergesellschaft fortgesetzt. Neben Bastelbögen, Anhängern, Kerzen, Schneekugeln, Geschirr oder Lampenfüßen werden zahllose Turmmodelle aus unterschiedlichen Materialien feilgeboten. Im Eiffelturm gibt es insgesamt acht offizielle Souvenirläden auf den ersten beiden Ebenen verteilt und im Erdgeschoss am Turmfuß; sie führen über 700 verschiedene Produkte. Nach Angaben des Betreibers kaufen jährlich über eine Million Besucher in den Läden ein. Die Nachfrage an Eiffelturm-Reproduktionen wird auch von zahlreichen fliegenden Schwarzmarkthändlern rund um den Turm zu befriedigen versucht.

Nachdem bereits in den 1920er und 1930er Jahren die Automarke Citroën durch eine auffällige Leuchtreklame am Eiffelturm auf sich aufmerksam gemacht hatte, nutzten viele namhafte französische Marken das berühmte Wahrzeichen für ihre Werbezwecke, darunter Air France, La Samaritaine, Yves Saint Laurent, Jean Paul Gaultier, Nina Ricci, Alain Afflelou oder Campari. Der Eiffelturm bediente schon früh die universelle Idee vom materiellen und sozialen Fortschritt und erfülle damit gleichsam einen „julesvernesken“ Traum über die Natur im Sinne des Aufklärungsjahrhunderts, so Architekturhistoriker Bertrand Lemoine. Das erkläre den Erfolg als Werbeträger, der auch für Modernität und Ehrgeiz stehe. Dabei scheint die Wirkungskraft des Eiffelturms bis heute ungebrochen, denn auch zeitgenössische Werbung nimmt nach wie vor Bezug auf das Bauwerk, wie in den 2000er Jahren in einem Werbespot von IBM. Viele Werbemotive mit dem Eiffelturm haben die Gemeinsamkeit, dass sie entweder auf einen außergewöhnlichen Erfolg hinweisen oder die Stadt Paris beziehungsweise das Land Frankreich hervorgehoben wird. Sowohl für den Erfolg im Allgemeinen wie für Frankreich steht der Eiffelturm als Sinnbild. Beispielsweise zeigte 1952 die Air France in einem Werbeplakat alle bedeutsamen Pariser Bauten vereint in den dominierenden Umrissen des Eiffelturms, dahinter ist eine stilisierte Landmasse zu sehen, die für das gesamte Land steht. Das vom französischen Grafiker Bernard Villemot (1911–1989) entworfene Plakat ist inzwischen zu einem Klassiker geworden und wird heute noch als Reproduktion angeboten.
Verschiedene Spielwarenhersteller wie MB/Hasbro oder Ravensburger haben von der berühmten Pariser Sehenswürdigkeit ein 3D-Puzzle herausgebracht. Lego stellte aus 3428 Teilen einen Bausatz des Eiffelturms im Maßstab 1:300 her. Das aufgebaute Modell aus dem Jahr 2007 hat eine Höhe von 1,08 Metern und ist mittlerweile eine begehrte Rarität. Von einer koreanischen Firma wird der Turm auch als Modellbausatz im Maßstab 1:160 angeboten. Das rund zwei Meter hohe Bronzemodell wiegt etwa 25 Kilogramm. Darüber hinaus gibt es von verschiedenen anderen Herstellern auch Modelle aus Papier, Holz oder Streichhölzern aber auch Poster, Bilder und Wandtattoos.

Bereits die Olympischen Sommerspiele 1900 in Paris warben im offiziellen Plakat mit dem Eiffelturm zur Veranstaltung. Der 1969 gegründete französische Fußballverein Paris FC führt als Emblem den Eiffelturm; er wurde im Laufe der Jahre immer wieder abgewandelt. Das aktuelle Logo zeigt den Turm in stilisierten Pinselstrichen. Auch der 1970 gegründete Fußballverein Paris Saint-Germain führt im Emblem den Eiffelturm. In den Niederlanden hat sich sogar eine in der 1. Basketballliga spielende Mannschaft nach dem Eiffelturm benannt. Die in ’s-Hertogenbosch ansässigen "EiffelTowers Den Bosch" tragen ebenfalls den Pariser Turm in ihrem Logo.

Zu den umfassendsten technischen Darstellungen des Eiffelturms gehört die am 1. Juni 1900 erschienene Publikation von Gustave Eiffel selbst. Der aufwändig auf Velinpapier gedruckte Großfolio-Band in zwei Ausgaben mit dem Titel "La tour de trois cents mètres" (deutsch: "Der 300-Meter-Turm") ist in acht Teile gegliedert und präsentiert das Bauwerk in rund 4300 Plänen, Zeichnungen und doppelseitig erstellten Tafeln sowie zeitgenössischen Fotografien. Die Pläne sind generell im Maßstab 1:200 abgedruckt, kleinere Details werden im Maßstab 1:50, 1:20 oder 1:10 wiedergegeben. Alle Bauteile sind mit Größenangaben versehen und der Text behandelt präzise die Ursprünge, das Bauprinzip, die Kosten, die Ausführung der Arbeiten an den Fundamenten und der Metallkonstruktion sowie die Erneuerungsarbeiten für die Weltausstellung 1900. Sogar dem Aufstellen der Gerüste widmet Eiffel ein eigenes Kapitel. Diese sehr umfassende Darstellung spiegelt den enzyklopädischen Geist in der Tradition der Aufklärung wider. Neben den technisch-ingenieurwissenschaftlichen Aspekten galt es auch als Würdigung an alle Mitarbeiter. Alle 326 Ingenieure, Vorarbeiter und Arbeiter, die an Entwurf und der Erbauung des Eiffelturms beteiligt waren, werden am Anfang des Buches namentlich erwähnt. Gleichzeitig diente das monumentale Buch nicht nur als Bestandsaufnahme, sondern auch als Bilanz, Geschenk und Werbemittel, in welchem Eiffel seine Leistung für die Nachwelt erhalten wollte.

Für den Erbauer selbst symbolisierte der Turm das „Jahrhundert der Industrie und der Wissenschaft“, das seiner Ansicht nach besonders in der nachrevolutionären Zeit Frankreichs begann. Aus diesem Grund ließ er zur Erinnerung die Namen von 72 Wissenschaftlern an das Tragwerk des Turms anbringen. Gleichzeitig nutzte er alle bis dahin verfügbaren technischen Mittel wie beispielsweise die elektrische Beleuchtung und die Aufzugtechnik, um im Bauwerk die industriell-wissenschaftlichen Errungenschaften zu vereinen.

Der Eiffelturm ist der höchste Fernsehturm Frankreichs und gleichzeitig wichtigster Sender für terrestrische Übertragung in der Region Paris, vor allem für UKW-Rundfunkprogramme sowie digitales Fernsehen. Der Turm ist Träger für über 120 Sendeantennen. Die Übertragungsinfrastruktur wird von TDF betrieben. Derzeit (2013) strahlt der Eiffelturm über 30 Radio- und rund 40 Fernsehprogramme aus.

Vor der Umstellung auf DVB-T diente der Sendestandort weiterhin für analoges Fernsehen (→ Séquentiel couleur à mémoire, SECAM):









</doc>
<doc id="1330" url="https://de.wikipedia.org/wiki?curid=1330" title="Entropiekodierung">
Entropiekodierung

Die Entropiekodierung ist eine Methode zur verlustfreien Datenkompression, die jedem einzelnen Zeichen eines Textes eine unterschiedlich lange Folge von Bits zuordnet. Typische Vertreter sind die Huffman-Kodierung und die arithmetische Kodierung.

Im Gegensatz dazu stehen Stringersatzverfahren, die eine Folge von Zeichen des Originaltextes durch eine Folge von Zeichen eines anderen Alphabets ersetzen.

Da eine bestimmte Mindestanzahl von Bits notwendig ist, um alle Zeichen voneinander zu unterscheiden, kann die Anzahl der Bits, die den Zeichen zugeordnet werden, nicht unbegrenzt klein werden. Die optimale Anzahl von Bits, die einem Zeichen mit einer gegebenen Wahrscheinlichkeit zugeordnet werden sollte, wird durch die Entropie bestimmt.

Entropiekodierer werden häufig mit anderen Kodierern kombiniert. Dabei dienen vorgeschaltete Verfahren dazu, die Entropie der Daten zu verringern. Häufig sind dies Prädiktionsverfahren, Verfahren wie die Burrows-Wheeler-Transformation, aber oft auch andere Komprimierer. LHarc zum Beispiel verwendet einen LZ-Kodierer und gibt die von diesem Kodierer ausgegebenen Zeichen an einen Huffman-Kodierer weiter. Auch Deflate und Bzip besitzen als letzte Stufe einen Entropiekodierer.

Die Entropiekodierung bestimmt zunächst die Häufigkeit von Symbolen (Zeichen). Jedes Symbol wird durch ein bestimmtes Bitmuster dargestellt. Dabei sollen häufige Symbole durch kurze Bitmuster dargestellt werden, um die Gesamtzahl der benötigten Bits zu minimieren.

Mathematische Algorithmen zur Abschätzung der optimalen Länge des Codes eines jeden Symbols führen meist zu nicht ganzzahligen Ergebnissen. Bei der Anwendung müssen die Längen auf ganzzahlige Werte gerundet werden, wodurch man einen Teil der Verdichtung verliert.

Die Shannon-Fano-Kodierung schlägt eine Möglichkeit vor, die die Anzahl der Bits auf ganze Zahlen rundet. Dieser Algorithmus liefert aber in bestimmten Fällen nicht die optimale Lösung. Deshalb wurde der Huffman-Code entwickelt, der beweisbar immer eine der optimalen Lösungen mit ganzen Bits liefert. Beide Algorithmen erzeugen einen präfixfreien Code variabler Länge, indem ein binärer Baum konstruiert wird. In diesem Baum stehen die „Blätter“ für die zu kodierenden Symbole und die inneren Knoten für die abzulegenden Bits.

Neben diesen Verfahren, die individuelle Tabellen speziell angepasst auf die zu kodierenden Daten erstellen, gibt es auch Varianten, die feste Codetabellen verwenden. Der Golomb-Code kann zum Beispiel bei Daten verwendet werden, bei denen die Häufigkeitsverteilung bestimmten Regeln unterliegt. Diese Codes haben Parameter, um ihn auf die exakten Parameter der Verteilung der Häufigkeiten anzupassen.

Diese Verfahren treffen aber die von der Entropie vorgeschriebene Anzahl von Bits nur in Ausnahmefällen. Das führt zu einer nicht optimalen Kompression.

Ein Beispiel: Eine Zeichenkette mit nur 2 verschiedenen Symbolen soll komprimiert werden. Das eine Zeichen hat eine Wahrscheinlichkeit von formula_1, das andere von formula_2. Die Verfahren von Shannon und Huffman führen dazu, dass beide Zeichen mit je einem Bit abgespeichert werden. Das führt zu einer Ausgabe, die so viele Bits enthält wie die Eingabe an Zeichen.

Ein optimaler Entropie-Kodierer würde aber nur formula_3 Bits für das Zeichen A verwenden und dafür formula_4 Bits für B. Das führt zu einer Ausgabe, die nur formula_5 Bits pro Zeichen enthält (maximale Entropie).

Mit einem arithmetischen Kodierer kann man sich der optimalen Codierung weiter annähern. Dieses Verfahren sorgt implizit für eine gleichmäßigere Verteilung der Bits auf die zu codierenden Zeichen, ohne dass explizit für jedes Zeichen ein individuelles Codezeichen konstruiert wird. Aber auch mit diesem Verfahren kann im Allgemeinen nicht die maximale Entropie erreicht werden. Dies liegt daran, dass es weiterhin einen „Verschnitt“ gibt, der darauf beruht, dass nur ganzzahlige Bitwerte tatsächlich auftreten können, während die Entropie meist Bruchteile von Bits erfordert. Im oben genannten Beispiel erreicht auch der Arithmetische Codierer nur eine Codelänge von einem Bit. Der Verschnitt verschwindet allerdings im Allgemeinen mit zunehmender Länge der zu codierenden Daten, so dass im Grenzwert die Entropie maximiert werden kann.

Um die Anzahl der Bits für jedes Zeichen festlegen zu können, muss man zu jedem Zeitpunkt des Kodierungsprozesses möglichst genaue Angaben über die Wahrscheinlichkeit der einzelnen Zeichen machen. Diese Aufgabe hat das Modell. Je besser das Modell die Wahrscheinlichkeiten vorhersagt, desto besser die Kompression. Das Modell muss beim Komprimieren und Dekomprimieren genau die gleichen Werte liefern. Im Laufe der Zeit wurden verschiedene Modelle entwickelt.

Beim Statischen Modell wird vor der Kodierung der Zeichenfolge eine Statistik über die gesamte Folge erstellt. Die dabei gewonnenen Wahrscheinlichkeiten werden zur Kodierung der gesamten Zeichenfolge verwendet.

Vorteile:

Nachteile:

In diesem Modell verändern sich die Wahrscheinlichkeiten im Laufe des Kodierungsprozesses. Dabei gibt es mehrere Möglichkeiten:


Vorteile:

Nachteile:

Normalerweise arbeitet man bei dynamischen Modellen nicht mit Wahrscheinlichkeiten, sondern mit den Häufigkeiten der Zeichen.

Dynamische Modelle lassen auch noch andere Variationsmöglichkeiten zu.

Man kann Statistik-Daten altern, indem man von Zeit zu Zeit die Häufigkeiten der Zeichen halbiert. Damit verringert man den Einfluss von weit zurückliegenden Zeichen.

Für noch nie vorgekommene Zeichen gibt es mehrere Varianten:

Das Level eines Modells bezieht sich darauf, wie viele Zeichen der Historie vom Modell für die Berechnung der Wahrscheinlichkeiten herangezogen werden. Ein Level-0-Modell betrachtet keine Historie und gibt die Wahrscheinlichkeiten global an. Ein Level-1-Modell dagegen betrachtet das Vorgängerzeichen und trifft in Abhängigkeit von diesem Zeichen seine Aussage über die Wahrscheinlichkeit. (Soll deutscher Text kodiert werden, ist zum Beispiel die Wahrscheinlichkeit des Buchstabens „U“ nach einem „Q“ viel höher, oder die Wahrscheinlichkeit eines Großbuchstabens in der Mitte eines Wortes viel kleiner als nach einem Leerzeichen). Das Level kann theoretisch beliebig hoch sein, erfordert dann aber enormen Speicherplatz und große Datenmengen, um aussagekräftige Statistiken zu erhalten.

PPM-Algorithmen verwenden einen arithmetischen Kodierer mit einem dynamischen Modell des Levels 5.




</doc>
<doc id="1331" url="https://de.wikipedia.org/wiki?curid=1331" title="Engerling">
Engerling

Als Engerling (aus mittel- bzw. althochdeutsch "enger(l)inc" bzw. "engiring" = kleiner Wurm, Made) bezeichnet man die Käferlarven der Überfamilie Scarabaeoidea (Blatthornkäfer). Dazu gehören als bekannteste Käferarten nicht nur die Mai- und Junikäfer, sondern u. a. auch die Gartenlaubkäfer, Rosenkäfer und Nashornkäfer.

Während die Larven von Mai-, Juni- und Gartenlaubkäfer als Schädlinge gelten, sind die Larven der Rosen- und Nashornkäfer Nützlinge, die sehr wertvoll in Komposthaufen und darüber hinaus gemäß Bundesartenschutzverordnung „besonders geschützte“ Arten sind. Nach Bundesnaturschutzgesetz ist es danach verboten, „sie zu fangen, zu verletzen oder zu töten oder ihre Entwicklungsformen aus der Natur zu entnehmen, zu beschädigen oder zu zerstören“. Außerdem dürfen ihre „Fortpflanzungs- oder Ruhestätten“ nicht beschädigt oder zerstört werden; es ist außerdem verboten, sie zu kaufen, zu verkaufen oder in Besitz zu nehmen.

Mai- und Junikäfer-Larven sind vorn und hinten etwa gleich dick, am Kopf haben sie sehr lange, kräftige Beine mit deutlich ausgebildeten „Knicken“. Sie erreichen eine Länge von 5–7 cm und sind im Kompost praktisch nie zu finden.
Maikäfer-Engerlinge schlüpfen nach vier bis sechs Wochen aus dem Ei und werden fünf bis sechs Zentimeter lang. Der Körper ist eher weißlich, wohingegen der Kopf braun ist. Sie leben je nach Art zwischen zwei und vier Jahren in der Erde. Zunächst ernähren sie sich von Humus, dann von zarten Gras- und Krautwurzeln (z. B. Löwenzahnwurzeln) und später auch von Baumwurzeln. Der Wurzelfraß kann im Extremfall zum Absterben von ausgewachsenen Buchen führen. Wegen der Ernährung von lebenden Pflanzenwurzeln ist Kompost ein völlig ungeeigneter Lebensraum für diese Art. Bei günstiger Sommerwitterung verpuppt sich der Maikäfer-Engerling und wird nach vier bis sechs Wochen zum Käfer. In dieser Form überwintert er in einer Erdhöhle und gräbt sich, je nach Witterung, im April bis Mai des folgenden Jahres aus dem Erdboden.

Rosenkäfer-Larven ("Cetonia aurata") sind hinten etwas dicker als vorn, haben nur kleine Stummelbeinchen, kommen typischerweise im Kompost vor und werden zu unrecht oft aus Angst vor Pflanzenschäden getötet. Sie ernähren sich von verrottendem Holz und Pflanzenmaterial und sind durch die damit einhergehende beschleunigende Zersetzung sehr nützlich und wertvoll im Kompost.

Die Larven des Nashornkäfers ("Oryctes nasicornis") sind sehr groß. Sie werden etwa 7 cm groß, oft sogar noch größer.


</doc>
<doc id="1332" url="https://de.wikipedia.org/wiki?curid=1332" title="Eiffel">
Eiffel

Eiffel bezeichnet:
Eiffel ist Bestandteil des Namens:
Eiffel ist der Familienname folgender Personen:
Siehe auch: 


</doc>
<doc id="1336" url="https://de.wikipedia.org/wiki?curid=1336" title="Epaulette">
Epaulette

Eine Epaulette (frz. "épaulette", zu "épaule" „Schulter“) ist ein Schulterstück einer Uniform. Im Deutschen bezeichnet man so üblicherweise eine spezielle Form, die sich von der einfachen „Schulterklappe“ unterscheidet. Diese Epauletten in engeren Sinn bestehen aus den "Halbmonden", dem "Feld", dem "Schieber" sowie meist auch "Raupen" oder "Fransen". Gehalten werden sie durch je einen Schulterknopf und je eine Passante, das ist ein quergesetzter Streifen aus farbigem Tuch oder Tresse auf der Schulter, durch die die Epaulette geschoben wird.
Ursprünglich waren die Epauletten ein Schulterschutz gegen Säbelhiebe, verhinderten jedoch auch ein Abrutschen der Schulterriemen. Im 19. Jahrhundert dienten Epauletten in vielen Staaten (Ausnahme war Österreich) als Rangabzeichen der Offiziere, wurden aber auch Bestandteil der Gala-Uniform mancher höheren Zivilbeamten. Die Generale trugen in den meisten Armeen volle Epauletten mit dicken Raupen (starre Kantillen), die Stabsoffiziere mit dünnen Fransen (lose Kantillen). Die Konterepauletten der Subalternoffiziere hatten keine Fransen. Im französischen Heer trugen bei verschiedenen Garde- und Elite-Truppenteilen Mannschaften Epauletten aus Wolle (einzelne Truppenteile bis heute). Dies setzte sich unter napoleonischem Einfluss vorübergehend auch in einigen Rheinbundstaaten und anderen Verbündeten Frankreichs durch. In Preußen trugen nur die Mannschaften der Ulanen den Offiziersepauletten ähnliche Epauletten, bei den übrigen deutschen Staaten wurde dies im Verlauf des 19. Jahrhunderts übernommen. Die Offiziere der deutschen und russischen Armee trugen im Feld Schulterstücke.
Auch bei den Offizieren der deutschen Marine wurden Epauletten zur Großen Uniform bis 1939 getragen.

In Frankreich tragen die Mannschaften der Fremdenlegion Stoffepauletten. Heute werden Epauletten ebenfalls in der Mode zur Betonung der Schulter verwendet.

Im Schach gibt es das Epaulettenmatt. Hier stehen an beiden „Schulterseiten“ des Königs eigene Figuren und versperren Fluchtfelder.


</doc>
<doc id="1337" url="https://de.wikipedia.org/wiki?curid=1337" title="Eulersche Zahl">
Eulersche Zahl

Die Eulersche Zahl

benannt nach dem Schweizer Mathematiker Leonhard Euler, ist eine irrationale und sogar transzendente reelle Zahl. Gelegentlich wird sie auch nach dem schottischen Mathematiker John Napier als Napiers Konstante bezeichnet.

Sie ist die Basis des natürlichen Logarithmus und der (natürlichen) Exponentialfunktion. Diese (spezielle) Exponentialfunktion wird aufgrund dieser Beziehung zur Zahl formula_2 häufig kurz formula_2-Funktion genannt.

Die Eulersche Zahl spielt in der gesamten Analysis und allen damit verbundenen Teilgebieten der Mathematik, besonders in der Differential- und Integralrechnung, eine zentrale Rolle. Sie gehört zu den wichtigsten Konstanten der Mathematik.

Im Formelsatz wird formula_2 nach DIN 1338 und ISO 80000-2 nicht kursiv gesetzt, um die Zahl von einer Variablen zu unterscheiden. Allerdings ist auch die kursive Schreibweise verbreitet.

Unter der Zahl formula_2 ist nach Leonhard Euler der Grenzwert der folgenden unendlichen Reihe zu verstehen:

Für formula_7 ist dabei formula_8 die Fakultät von formula_9, also im Falle formula_10 das Produkt formula_11 der natürlichen Zahlen von formula_12 bis formula_9, während formula_14 definiert ist.

Wie schon Euler bewies, erhält man die Eulersche Zahl formula_2 ebenfalls als funktionalen Grenzwert:

was insbesondere bedeutet, dass er sich auch als Grenzwert der Folge formula_17 mit formula_18 ergibt:

Dem liegt zugrunde, dass

gilt, formula_2 also der Funktionswert der Exponentialfunktion (oder auch „formula_2-Funktion“) an der Stelle formula_12 ist. Die obige Reihendarstellung von formula_2 ergibt sich in diesem Zusammenhang dadurch, dass man die Taylorreihe der Exponentialfunktion um die Entwicklungsstelle formula_25 an der Stelle formula_12 auswertet.

Ein alternativer Zugang zur Definition der Eulerschen Zahl ist derjenige über Intervallschachtelungen, etwa in der Weise, wie es in "Theorie und Anwendung der unendlichen Reihen" von Konrad Knopp dargestellt wird. Danach gilt für alle formula_27:

Als frühestes Dokument, das die Verwendung des Buchstabens formula_2 für diese Zahl durch Leonhard Euler aufweist, gilt ein Brief Eulers an Christian Goldbach vom 25. November 1731. Als nächste gesicherte Quelle für die Verwendung dieses Buchstabens gilt Eulers Werk "Mechanica sive motus scientia analytice exposita, II" aus dem Jahre 1736.

In der im Jahre 1748 erschienenen "Introductio in Analysin Infinitorum" greift Euler diese Bezeichnung wieder auf.

Es gibt keine Hinweise darauf, dass diese Wahl des Buchstabens formula_2 in Anlehnung an seinen Namen geschah. Unklar ist auch, ob er dies in Anlehnung an die Exponentialfunktion oder aus praktischen Erwägungen der Abgrenzung zu den viel benutzten Buchstaben "a, b, c" oder "d" machte. Obwohl auch andere Bezeichnungen in Gebrauch waren, etwa "c" in d’Alemberts "Histoire de l’Académie," hat sich formula_2 durchgesetzt.

Die Eulersche Zahl formula_2 ist eine transzendente ( nach Charles Hermite, 1873) und damit irrationale Zahl (). Sie lässt sich also (wie auch die Kreiszahl formula_33 nach Ferdinand von Lindemann 1882) weder als Bruch zweier natürlicher Zahlen noch als Lösung einer algebraischen Gleichung darstellen und besitzt folglich eine unendliche nichtperiodische Dezimalbruchentwicklung. Das Irrationalitätsmaß von formula_2 ist 2 und somit so klein wie möglich für eine irrationale Zahl, insbesondere ist formula_2 nicht liouvillesch. Es ist nicht bekannt, ob formula_2 zu irgendeiner Basis normal ist.

In der Eulerschen Identität
werden fundamentale mathematische Konstanten in Zusammenhang gesetzt: Die ganze Zahl 1, die Eulersche Zahl formula_2, die imaginäre Einheit formula_39 der komplexen Zahlen und die Kreiszahl formula_33.

Die Eulersche Zahl formula_2 ist die durch
eindeutig bestimmte positive reelle Zahl formula_43.

Sie tritt auch in der asymptotischen Abschätzung der Fakultät auf ("siehe" Stirlingformel):

Es gilt mit der Cauchy-Produktformel der jeweils absolut konvergenten Reihen:

formula_45
mit der Produktreihe formula_46.

Eine geometrische Interpretation der Eulerschen Zahl liefert die Integralrechnung. Danach ist formula_2 diejenige eindeutig bestimmte Zahl formula_48, für die der Inhalt der Fläche unterhalb des Funktionsgraphen der reellen Kehrwertfunktion formula_49 im Intervall formula_50 exakt gleich formula_12 ist:

Die Eulersche Zahl lässt sich auch durch
oder durch den Grenzwert des Quotienten aus Fakultät und Subfakultät beschreiben:

Eine Verbindung zur Verteilung der Primzahlen wird über die Formeln
deutlich, wobei formula_57 die Primzahlfunktion und das Symbol formula_58 das Primorial der Zahl formula_59 bedeutet.

Auch eher von exotischem Reiz als von praktischer Bedeutung ist die "catalansche Darstellung"

Im Zusammenhang mit der Zahl formula_2 gibt es spätestens seit dem Erscheinen von Leonhard Eulers "Introductio in Analysin Infinitorum" im Jahre 1748 eine große Anzahl Kettenbruchentwicklungen für formula_2 und aus formula_2 ableitbare Größen.

So hat Euler die folgende klassische Identität für formula_2 gefunden:

Die Identität (1) weist offenbar ein regelmäßiges Muster auf, das sich bis ins Unendliche fortsetzt. Sie gibt einen regulären Kettenbruch wieder, der von Euler aus dem folgenden abgeleitet wurde:

Letzterer Kettenbruch ist seinerseits ein Spezialfall des folgenden mit formula_67:

Eine andere klassische Kettenbruchentwicklung, die jedoch "nicht regelmäßig" ist, stammt ebenfalls von Euler:

Auf Euler und Ernesto Cesàro geht eine weitere Kettenbruchentwicklung der "Eulerschen Zahl" zurück, die von anderem Muster als in (1) ist:

Im Zusammenhang mit der "Eulerschen Zahl" existiert darüber hinaus eine große Anzahl von allgemeinen kettenbruchtheoretischen Funktionalgleichungen. So nennt Oskar Perron als eine von mehreren die folgende allgemein gültige Darstellung der formula_2-Funktion:

Ein weiteres Beispiel hierfür ist die von Johann Heinrich Lambert stammende Entwicklung des Tangens Hyperbolicus, die zu den "lambertschen Kettenbrüchen" gerechnet wird:

Das folgende Beispiel macht die Berechnung der Eulerschen Zahl nicht nur anschaulicher, sondern es beschreibt auch die Geschichte der Entdeckung der Eulerschen Zahl: Ihre ersten Stellen wurden von Jakob Bernoulli bei der Untersuchung der Zinseszinsrechnung gefunden.

Den Grenzwert der ersten Formel kann man folgendermaßen deuten: Jemand zahlt am 1. Januar einen Euro auf der Bank ein. Die Bank garantiert ihm eine momentane Verzinsung zu einem Zinssatz formula_77 pro Jahr. Wie groß ist sein Guthaben am 1. Januar des nächsten Jahres, wenn er die Zinsen zu gleichen Bedingungen anlegt?

Nach der Zinseszinsformel wird aus dem Startkapital formula_78 nach formula_79 Verzinsungen mit Zinssatz formula_43 das Kapital

In diesem Beispiel sind formula_82 und formula_83, wenn der Zinszuschlag jährlich erfolgt, oder formula_84, wenn der Zinszuschlag formula_79-mal im Jahr erfolgt, also bei unterjähriger Verzinsung.

Bei jährlichem Zuschlag wäre
Bei halbjährlichem Zuschlag hat man formula_87,
also schon etwas mehr. Bei täglicher Verzinsung formula_89 erhält man
Wenn die Verzinsung kontinuierlich in jedem Augenblick erfolgt, wird formula_79 unendlich groß, und man bekommt die oben angegebene erste Formel für formula_2.

formula_2 ist auch häufig in der Wahrscheinlichkeitstheorie anzutreffen: Beispielsweise sei angenommen, dass ein Bäcker für jedes Brötchen eine Rosine in den Teig gibt und diesen gut durchknetet. Danach enthält statistisch gesehen jedes formula_2-te Brötchen keine Rosine. Die Wahrscheinlichkeit formula_95, dass bei formula_79 Brötchen keine der formula_79 Rosinen in einem fest gewählten ist, ergibt im Grenzwert für formula_98 (37%-Regel):

Im vierzigsten Band von Crelles Journal aus dem Jahre 1850 gibt der Schweizer Mathematiker Jakob Steiner eine Charakterisierung der Eulerschen Zahl formula_2, wonach formula_2 als Lösung einer Extremwertaufgabe verstanden werden kann. Steiner zeigte nämlich, dass die Zahl formula_2 charakterisierbar ist als diejenige eindeutig bestimmte positive reelle Zahl, die beim Wurzelziehen mit sich selbst die größte Wurzel liefert. Wörtlich schreibt Steiner: "„Wird jede Zahl durch sich selbst radicirt, so gewährt die Zahl e die allergrößte Wurzel.“"

Steiner behandelt hier die Frage, ob für die Funktion

das globale Maximum existiert und wie es zu bestimmen ist. Seine Aussage ist, dass es existiert und dass es angenommen wird in und nur in formula_104.

In seinem Buch "Triumph der Mathematik" gibt Heinrich Dörrie eine elementare Lösung dieser Extremwertaufgabe. Sein Ansatz geht von der folgenden wahren Aussage über die reelle Exponentialfunktion aus:

Nach der Substitution formula_106 folgt für alle reellen Zahlen formula_107

mittels einfacher Umformungen weiter

und schließlich für alle positiven formula_107 durch Radizieren

Für die Zahl formula_2 und daraus abgeleitete Größen gibt es verschiedene näherungsweise Darstellungen mittels Brüchen. So fand Charles Hermite die folgenden Bruchnäherungen:

Hier weicht der erstgenannte Bruch um weniger als 0,0003 Prozent von formula_2 ab.

Die optimale Bruchnäherung im dreistelligen Zahlenbereich, also die optimale Bruchnäherung formula_116 mit formula_117, ist

Diese Näherung ist jedoch nicht die beste Bruchnäherung im Sinne der Forderung, dass der Nenner höchstens dreistellig sein soll. Die in diesem Sinne "beste Bruchnäherung" ergibt sich als 9. Näherungsbruch der Kettenbruchentwicklung der Eulerschen Zahl:

Aus den Näherungsbrüchen der zu formula_2 gehörenden Kettenbruchentwicklungen (s. o.) ergeben sich Bruchnäherungen beliebiger Genauigkeit für formula_2 und daraus abgeleitete Größen. Mit diesen findet man sehr effizient beste Bruchnäherungen der Eulerschen Zahl in beliebigen Zahlenbereichen. So erhält etwa im fünfstelligen Zahlenbereich die beste Bruchnäherung

die zeigt, dass die von Charles Hermite für die Eulersche Zahl im fünfstelligen Zahlenbereich gefundene Bruchnäherung noch nicht optimal war.

In gleicher Weise hat etwa C. D. Olds gezeigt, dass durch die Näherung

für die Eulersche Zahl eine weitere Verbesserung, nämlich

zu erzielen ist.

Insgesamt beginnt die Folge der besten Näherungsbrüche der Eulerschen Zahl, die sich aus ihrer regelmäßigen Kettenbruchdarstellung ergeben, folgendermaßen:

Zur Berechnung der Nachkommastellen wird meist die Reihendarstellung
ausgewertet, die schnell konvergiert. Wichtig bei der Implementierung ist dabei Langzahlarithmetik, damit die Rundungsfehler nicht das Ergebnis verfälschen. Ein Verfahren, das ebenfalls auf dieser Formel beruht, aber ohne aufwendige Implementierung auskommt, ist der Tröpfelalgorithmus zur Berechnung der Nachkommastellen von formula_2, den A. H. J. Sale fand.




</doc>
<doc id="1340" url="https://de.wikipedia.org/wiki?curid=1340" title="Etymologie">
Etymologie

Die Etymologie (von altgriechisch "etymología" = „Ableitung eines Wortes aus seiner Wurzel und Nachweisung seiner eigentlichen, wahren Bedeutung“) befasst sich mit der Herkunft und Geschichte der Wörter. Im Verständnis der Sprachwissenschaft ist die "Wortherkunft" die Erklärung der Entstehung eines Worts oder Morphems in einer gegebenen Gestalt und Bedeutung. Als sprachgeschichtlich (diachron) ausgerichtete Erklärungsweise ist sie Bestandteil der historischen Sprachwissenschaft, ihre Ergebnisse werden gesammelt in etymologischen Wörterbüchern und werden als Zusatzinformation auch in Wörterbüchern und Lexika anderer Art aufgenommen.

Älteren Epochen diente die Etymologie als Erklärung einer im Wort angelegten „Wahrheit“ (), die mithilfe von Ähnlichkeiten der Wortgestalt zu anderen Wörtern erschlossen und als Aussage über die vom Wort bezeichnete Sache oder als eigentliche, ursprüngliche Wortbedeutung verstanden wurde. Als rhetorisches Argument ("argumentum a nomine") dient die Etymologie in Form eines Hinweises oder einer Berufung auf die angenommene Herkunft und ursprüngliche Bedeutung eines Worts traditionell dem Zweck, die eigene Argumentation durch einen objektiven sprachlichen Sachverhalt zu stützen und ihr so besondere Überzeugungskraft zu verleihen.

"Etymologie" ist ein griechisches Fremdwort und leitet sich von dem altgriechischen Wort "etymología" her. Dieses enthält seinerseits die Bestandteile "étymos („wahr, echt, wirklich“)" und "lógos" („Wort“) enthält und bedeutet in einem umfassenderen Sinn so viel wie „Erklärung der einem Wort innewohnenden Wahrheit“. Im Deutschen wird dafür auch das Synonym "Wortherkunft" verwendet.

Die Verbindung der Bestandteile ist im Griechischen seit dem 1. Jahrhundert v. Chr. belegt (Dionysios von Halikarnassos), ebenso schon die Entlehnung "etymologus" ins Lateinische (Varro), doch soll nach dem späteren Zeugnis von Diogenes Laertius (VII.7) auch schon Chrysippos im 3. Jahrhundert v. Chr. Werke mit dem Titel ‚"Über etymologische Themen"‘ und ‚"Etymologisches"‘ verfasst haben.

Bereits im griechischen Altertum gab es philosophische Strömungen, die der „Richtigkeit“ der „Namen“ nachgingen, allerdings wurde für diese Tätigkeit in der Regel nicht der Begriff „Etymologie“ verwendet. So fragte sich bereits Heraklit von Ephesos (um 500 v. Chr.), inwiefern der Name eines Dinges die Wahrheit einer Sache widerspiegele. Also, inwiefern der Name tatsächlich dem durch ihn bezeichneten Gegenstand entspricht. Später beschäftigte sich Platon in seinem Dialog "Kratylos" eingehend mit der Richtigkeit der Namen. In diesem Dialog lässt Platon einen Vertreter der mystisch-religiösen These, laut derer alle Wörter ihre Bedeutung von Natur aus haben und keiner Definition bedürfen, antreten gegen einen Vertreter der eher modernen, im "Kratylos" erstmals bezeugten Gegenthese, laut derer der Zusammenhang von Wörtern und ihrer Bedeutung auf der willkürlichen Festlegung durch den Menschen beruht. (Zur Diskussion um die Sprachrichtigkeit in der Antike vgl. Siebenborn 1976.) Die Etymologie war ein Teil der antiken Grammatik und wurde neben den Philosophen vornehmlich von den sogenannten Grammatikern betrieben, allerdings aus heutiger Sicht ohne verlässliche Methodik, so dass die auf bloßer Spekulation aufgrund vager Analogien in Klang oder Schriftbild beruhenden Herleitungen einer kritischen Prüfung durch die moderne Sprachwissenschaft meist nicht standgehalten haben. Man spricht daher von Pseudetymologien (auch Pseudo-Etymologien), die sich nicht wesentlich von sogenannten Vulgäretymologien oder Volksetymologien unterscheiden, einem Phänomen, das auch heute noch im außerwissenschaftlichen Bereich eine nicht unerhebliche Rolle spielt und nicht selten sogar argumentative Verwendung findet, z. B. die fälschliche Herleitung von Dichten von "dicht" statt von lateinischem "dictare". Die Etymologie eines Worts wurde in der Antike als so bedeutender Teil der Bedeutungserklärung angesehen, dass sogar Enzyklopädien wie die des spätantiken Grammatikers Isidor von Sevilla den Titel "Etymologiarum sive originum libri" (Etymologien oder Ursprünge [der Wörter], kurz: "Etymologiae") tragen konnten.

Auch andere Kulturen, insbesondere solche mit langer Schrifttradition wie Indien und China, haben sich früh mit Etymologie beschäftigt, die ihnen unter anderem ein tieferes Verständnis der überlieferten Texte ermöglichen sollte.

Den Höhepunkt der „wahrheitssuchenden Etymologie“ finden wir bei Isidor von Sevilla Anfang des 7. Jh. n. Chr., also im Frühmittelalter. In seinem Hauptwerk Etymologiae libri viginti gibt er zahlreiche Beispiele von Etymologien, deren Wahrheitsgehalt jedoch im Zweifel steht. Auch Isidor von Sevilla benannte viele Etymologien, um Dinge verständlich zu erklären, die historisch gesehen zu bezweifeln sind. Zum Beispiel: „persona est Exegese, Physiologus“, (der Tiernamen aus der Wortgestalt zu erklären sucht), oder die Legenda aurea, die vor der Vita eines Heiligen zunächst seinem Namen breite Aufmerksamkeit widmet. Auch Petrus Heliae versteht die etymologische Bedeutung von Wörtern als Synonym für einen „Wahrspruch“ an sich (veriloquium): „denn wer etymologisiert, zeigt den wahren, d. h. den ersten Ursprung des Worts an.“ (Übersetzung in: Arens 1969, 39)

Heutzutage ist Etymologie innerhalb der historisch vergleichenden Sprachwissenschaft die Disziplin, welche Entstehung und geschichtliche Veränderung einzelner Wörter aufspürt und in etymologischen Wörterbüchern festhält. Historische Linguistik sucht nach wiederkehrenden Erscheinungen des Sprachwandels und leitet aus ihnen Lautgesetze ab, die es ihrerseits erleichtern, Veränderungen eines Worts im Verlaufe der Geschichte zu beobachten.
Zusätzlich zur rein linguistischen Beschäftigung mit Etymologie bringt die sprachgeschichtliche Forschung außerdem Nutzen für das genauere Verständnis von Texten und einzelnen Begriffen. Ein weiteres Anwendungsgebiet besteht in der Übertragung der Ergebnisse auf die Archäologie. Hier können sprachgeschichtliche Verhältnisse Anhaltspunkte für verschiedene archäologische Fragestellungen liefern, so etwa im Fall der Rekonstruktion von frühzeitlichen Wanderungsbewegungen. Auch soziolinguistische Rückschlüsse auf Sozial- und Kulturgeschichte stehen dabei im Blickfeld.

Die Zeitschrift "Studia Etymologica Cracoviensia" befasst sich ausschließlich mit etymologischen Themen.

Im Rahmen der Sprachwissenschaft will Etymologie mehr über die einzelnen Phänomene der geschichtlichen Veränderung einer Sprache herausfinden. Aus dem so gewonnenen Wissen soll ein erweitertes Verständnis über die Entwicklungsgeschichte einer Einzelsprache sowie der Umstände des Sprachwandels im Allgemeinen folgen. Das klassische Verständnis der Etymologie und praktische Anwendungen wie oben erwähnt steht dabei zumeist im Hintergrund. In der alltäglichen, nicht-wissenschaftlichen Beschäftigung mit Etymologie hat sich hingegen der normative Charakter der frühen Etymologie mehr oder weniger ausgeprägt erhalten. So wird etwa anhand der Geschichte eines Worts demonstriert, dass eine bestimmte, moderne Verwendungsweise falsch ist, da sie nicht der historischen entspricht bzw. sich nicht an der in der Wortgeschichte offenbar werdenden eigentlichen Wort-"Bedeutung" orientiert.

Vertreter einer abgeschwächten Variante dieses Arguments lehnen diese moderne Auffassung nicht grundsätzlich ab, erhoffen sich jedoch aus der Beschäftigung mit der Entwicklungsgeschichte eines Worts neue und weitere, vertiefende Aspekte für ein Verständnis seiner Bedeutung. Hier wird davon ausgegangen, dass diese Aspekte im Lauf der Zeit gleichsam verloren gegangen sind und durch sprachgeschichtliche Untersuchungen wieder bewusst gemacht werden können. Begründet wird dies damit, dass das Denken nur in den Bildern der Wahrnehmung als Abbild der Wirklichkeit erfolgen könne und somit allein schon die Wahrnehmung und in der Folge auch das Denken sowohl vom bewussten wie auch vom unbewussten Inhalt eines Begriffs wie auch dessen Gestalt geprägt sei. Die Etymologie wird hier als Weg gesehen, diese unbewussten Teile wahrnehmbar zu machen und so der Wahrnehmung und dem Denken diese verloren gegangenen Inhalte erneut erschließen zu können. So soll – ganz in der Tradition antiker Denker – ein Beitrag zum Reichtum der Sprache und des Denkens geleistet werden.

Unabhängig von der Frage, ob die jeweils angeführte wortgeschichtliche Herleitung inhaltlich korrekt ist oder nicht, geraten Vertreter beider Auffassungen dann in Widerspruch zu modernen sprachwissenschaftlichen Grundannahmen, wenn sie auf einer engen und unmittelbaren Beziehung zwischen einem gedanklichen Konzept und der Gestalt des Worts, mit dem es ausgedrückt wird, bestehen. Dieser Auffassung steht die funktionale Ansicht der Sprachwissenschaft entgegen, dass eine konkrete Wortform ihre Bedeutung ausschließlich per Arbitrarität und Konvention erhalte. Arbitrarität und Konvention sind Schlüsselbegriffe des Verständnisses von Zeichen in der Linguistik seit Beginn des 20. Jahrhunderts; man beruft sich dazu auf Ferdinand de Saussure (frz. 1916; dt. Übers. 1931/1967). Sie besagen, dass das Verhältnis zwischen der Form und der Bedeutung von Zeichen, d. h. auch von Wörtern, arbiträr (willkürlich) und durch gesellschaftliche Konvention bedingt sei. Für sich genommen habe ein Wort somit keine "eigentliche" Bedeutung und Wirkung außer der, die sich in der jeweiligen Gegenwart aus der üblichen Verwendung ergibt. Die Existenz einer darüber hinaus dem Wort in irgendeiner Weise noch zusätzlich anhängenden Bedeutung, die man in irgendeiner Form herausfinden könnte oder sollte, wird hier bezweifelt. Unter einer solchen Annahme können die von den „normativen“ "Etymologen" vorgebrachten Interpretationen der Wortbedeutung nicht mehr Gültigkeit für sich beanspruchen als jede alternativ vorgeschlagene Neuinterpretation auch.

Die Auffassung von der Arbitrarität der Zeichen wird durch die Natürlichkeitstheorie in der modernen Linguistik durch die Entdeckung ergänzt, dass viele Aspekte der Sprache ikonisch (abbildend) sind, also gar nicht ausschließlich arbiträr (willkürlich).

Etymologische Erklärungen werden darüber hinaus auch häufig zur Untermauerung von Ideologien jedweder Couleur herangezogen, z. B. Esoterik, politische Religionen, u. v. a. m. So versuchen beispielsweise Nationalisten die vermeintliche Überlegenheit der eigenen Kultur anhand ihrer Wirkung auf den Wortschatz einer anderen Sprache zu „beweisen“ oder erwünschte verwandtschaftliche Beziehungen zweier Kulturen aus einer vermuteten Sprachverwandtschaft zu rekonstruieren. Der etymologischen Erklärung scheint eine besondere, unmittelbar einleuchtende „Beweiskraft“ zu eigen zu sein, indem schon Bekanntes (ein Wort) von bislang unbekannter Seite dargestellt wird.


Für etymologische Wörterbücher der deutschen Sprache siehe den Artikel Etymologisches Wörterbuch.





</doc>
<doc id="1343" url="https://de.wikipedia.org/wiki?curid=1343" title="Endemit">
Endemit

Als Endemiten (von ; ungenau oft auch Endemismen im Plural) werden in der Biologie Pflanzen oder Tiere bezeichnet, die im Gegensatz zu den Kosmopoliten nur in einer bestimmten, räumlich abgegrenzten Umgebung vorkommen. Diese sind in diesem Gebiet "endemisch".

Dabei kann es sich um Arten, Gattungen oder Familien von Lebewesen handeln, die ausschließlich auf bestimmten Inseln oder Inselgruppen, Gebirgen, in einzelnen Tälern oder Gewässersystemen heimisch sind. Beispiel: Die Darwinfinken sind auf den Galapagosinseln endemisch, da sie weltweit nirgendwo sonst vorkommen.

Eine Festlegung, bis zu welcher Flächengröße dieser Begriff verwendet wird, gibt es nicht. Für einen ganzen Kontinent endemische Arten, aber auch höhere taxonomische Einheiten, finden sich etwa für Amerika („Neuwelt“-Species) oder Australien. Kontinentübergreifende Vorkommen finden sich dann beispielsweise in der Pflanzenfamilie der Bromeliengewächse, ursprünglich in Amerika, und sonst nur in einer Region Westafrikas.

Vor allem in der Botanik ist die Unterscheidung in „Paläoendemiten“ (auch: Reliktendemiten) und „Neoendemiten“ (auch: Entstehungsendemismus) üblich.

Paläoendemiten sind Arten mit ursprünglich vermutlich weiterer Verbreitung, die durch Änderung der Lebensbedingungen oder neue Konkurrenten in ein Reliktareal, meist eine Insel oder ein Gebirge, abgedrängt worden sind. Ein Beispiel wäre der Wurzelnde Kettenfarn ("Woodwardia radicans"), der heute in den Lorbeerwäldern der Kanarischen Inseln und in eng begrenzten Gebieten (meist auf den Inseln) am Mittelmeer mit ähnlich niederschlagsreichem Lokalklima vorkommt. Man nimmt an, dass es sich um das Reliktareal einer im Tertiär unter wärmeren und feuchteren Lebensbedingungen weiter verbreiteten Art handelt.

Neoendemiten sind Arten, die sich erst vor (erdgeschichtlich) kurzer Zeit aus weit verbreiteten Pflanzentaxa unter besonderen Standortbedingungen entwickelt haben. Dies nimmt man zum Beispiel für die zahlreichen Arten der Nelkengattung "Dianthus" auf Berggipfeln im Mittelmeerraum oder für die zahlreichen Tragant-("Astragalus-")Arten in abgegrenzten Regionen Zentralanatoliens an. Als Kuriosum kommen sogar sogenannte heimatlose Arten vor. Dies sind neophytische Neo-Endemiten, die sich (meist durch Hybridisierung) erst seit wenigen hundert Jahren in ihrer neuen Heimat aus ursprünglich vom Menschen aus anderen Erdteilen eingeführten Arten entwickelt haben. Bekannt ist dies etwa von Kleinarten der Nachtkerzen ("Oenothera") aus dem "biennis"-Artkomplex.

Je kleiner der zur Verfügung stehende Lebensraum ist, desto größer ist meist die Gefährdung der endemischen Taxa. Schon geringe Veränderungen im Habitat können zum Aussterben des gesamten Taxons führen.

Die Anwendung des Begriffs „Endemit“ auf politische Grenzen ist nur im Rahmen der Roten Liste gefährdeter Arten üblich.

Als Inselendemiten bezeichnet man Arten, welche sich an den Lebensraum einer bestimmten Insel angepasst haben. Als Inselendemit ist ein Tier/eine Pflanze zu bezeichnen, dessen/deren Vorfahren an eine Insel (meist weiter vom Festland entfernt) angetrieben wurden und sich da aufgrund von bestimmten abiotischen oder biotischen Umweltfaktoren der selbigen verändert haben, so dass diese Tiere bzw. Pflanzen infolgedessen nur auf dieser Insel heimisch sind. Interessant unter diesen ist unter anderem auch, dass sich bei einigen Arten besonders angepasste Unterarten gebildet haben, die jeweils verschiedene Lebensräume der Insel bewohnen. Die meisten dieser Unterarten gehen allerdings auf eine Art zurück, welche die Insel erreichte. Interessante Beispiele hierfür sind unter anderem auch die Anolis-Echsen auf den Westindischen Inseln, die Finken auf einigen Pazifikinseln (Galapagos, Hawaii) oder die Riesenschildkröten auf den Galapagosinseln. Eine weitere Besonderheit unter Inselendemiten ist ein langsamer Fortpflanzungszyklus, durch den die Individuenzahl der jeweiligen Arten nur langsam oder nicht ansteigen kann. So können viele endemische Vögel nur ein Ei pro Jahr legen, was sich so auswirkt wie eine „Bevölkerungsregulierung“ in der Evolution.

Hier einige Beispiele für Inselendemiten:

Bei einigen Endemiten ist es aufgrund des Nichtvorhandenseins von Fressfeinden wie Raubtieren oder anderen Bedrohungen zum Inselgigantismus gekommen. Inselgigantismus kann auftreten, wenn eine bestimmte Art auf eine Insel gelangt ist, auf der für sie kaum Gefahr besteht und auf der sie einen idealen Lebensraum vorfindet. Infolgedessen sind auf einigen Inseln zum Teil riesige Arten entstanden.

Hier einige Beispiele:
Inselverzwergung jedoch tritt ein, wenn innerhalb einer Art aufgrund eines geringeren Nahrungsangebotes die kleineren Exemplare durch ihren geringeren Nahrungsbedarf besser angepasst sind, oder wenn bei Selektion durch Raubtiere kleinere Exemplare bessere Möglichkeiten haben, sich den Beutegreifern zu entziehen.

Hier einige Beispiele:







</doc>
<doc id="1344" url="https://de.wikipedia.org/wiki?curid=1344" title="Epidemie">
Epidemie

Eine Epidemie (von griechisch επιδημία "epidēmía" „Aufenthalt, Ankunft; von Krankheiten: im Volk verbreitet“, zu επί "epí" „auf“ und δήμος "dḗmos" „Volk“), deutsch auch Seuche, ist die zeitliche und örtliche Häufung einer Krankheit innerhalb einer menschlichen Population, wobei es sich dabei im engeren Sinn um Infektionskrankheiten handelt. Aus epidemiologischer Sichtweise wird von einer Epidemie gesprochen, wenn in einem bestimmten Zeitraum die Inzidenz (als Anzahl der neuen Erkrankungsfälle) zunimmt. Demgegenüber wird als Endemie das andauernd gehäufte Auftreten einer Krankheit in einem begrenzten Bereich bezeichnet. Die Inzidenz in diesem Gebiet bleibt (mehr oder weniger) gleich, ist aber im Verhältnis zu anderen Gebieten erhöht. Eine Abnahme der Erkrankungshäufigkeit wird als Regression, eine länder- und kontinentübergreifende Ausbreitung als Pandemie bezeichnet. Entsprechend der Ausbreitungsgeschwindigkeit einer Erkrankung kann eine Einteilung in "Explosiv-" und "Tarditivepidemien" erfolgen.

Da sich die Endung "-demie" sprachlich auf Menschen bezieht, sind in der Veterinärmedizin auch die Bezeichnungen Epizootie statt "Epidemie" und Panzootie statt "Pandemie" üblich.

Epidemisch auftretende Krankheiten sind viele Tropenkrankheiten wie die Dengue, aber auch Cholera, Grippe, Typhus, Pest und Kinderlähmung. Milzbrand-Epidemien traten früher häufiger im Abstromgebiet von Gerbereien auf.

Im Falle der Grippe spricht das US-amerikanische CDC als leitende staatliche Behörde zum Schutz der dortigen Bevölkerung vor Krankheiten und Seuchen von einer Influenzaepidemie, wenn in einem bestimmten Winter die Übersterblichkeit an Grippe und Lungenentzündung gegenüber einem durchschnittlichen Winter um mehr als 7,5 Prozent erhöht ist.

Von einer heimlichen Epidemie wird im Falle der Chlamydiose bei Jugendlichen (als einer in dieser Bevölkerungsgruppe kaum bekannten sexuell übertragbaren Erkrankung) gesprochen.

Viele Betroffene suchen im Internet nach Information zu Krankheiten. Durch die Auswertung dieser Big Data kann es gelingen, Epidemien kostengünstig und frühzeitig zu erkennen. Auch die Auswertung von persönlichen Nachrichtendiensten im Internet kann für diese Bewertung herangezogen werden.

Es wird allerdings auch darauf hingewiesen, dass die häufige Suche nach Krankheiten oder Erwähnung von Krankheiten im Internet nicht immer durch eine erhöhte Prävalenz dieser Krankheit bedingt sein muss und daher zu überhöhten Prognosen führen kann, wenn schlechte Algorithmen zur Auswertung verwendet werden beziehungsweise wenn nicht noch zusätzliche Datenquellen herangezogen werden.

Das Wort "Epidemie" kommt aus dem Griechischen von "epí" („über“) und "démos" („Volk“). Das deutsche Wort "Seuche" (mittelhochdeutsch "siuche") dagegen ist verwandt mit "siech". Es wird heute meist für epidemisch auftretende Tierkrankheiten (z. B. Maul- und Klauenseuche) verwendet, deren überregionale Ausbreitung oft auch als Seuchenzug bezeichnet wird.

Als Begründer der "Historischen Seuchenpathologie" gilt Justus Hecker.

Im Unterschied zur Endemie, bei der sich eine Krankheit mit einer Basisreproduktionszahl formula_1 von exakt 1 verbreitet, jedes infizierte Individuum im statistischen Mittel also genau eine Folgeinfektion bewirkt und die Krankheit so dauerhaft in der Population verbleibt, verbreitet sich eine Epidemie mit einer Reproduktionsrate größer 1. Dies bedeutet, dass die Anzahl der Neuinfektionen innerhalb der Population zunächst stark ansteigt, hierdurch jedoch der Anteil anfälliger, aber nicht infizierter Individuen schnell reduziert wird. In Folge sinkt die Zahl der Neuinfektionen nach einiger Zeit immer weiter ab, bis die Krankheit letztlich in der Population ausstirbt.





</doc>
<doc id="1345" url="https://de.wikipedia.org/wiki?curid=1345" title="Évariste Galois">
Évariste Galois

Évariste Galois (* 25. Oktober 1811 in Bourg-la-Reine; † 31. Mai 1832 in Paris) war ein französischer Mathematiker. Er starb im Alter von nur 20 Jahren bei einem Duell, erlangte allerdings durch seine Arbeiten zur Lösung algebraischer Gleichungen, der so genannten Galoistheorie, postum Anerkennung.

Galois besuchte das College Louis-le-Grand in Paris, scheiterte zweimal an der Aufnahmeprüfung zur "École polytechnique" und begann ein Studium an der "École normale supérieure." Mit 17 Jahren veröffentlichte er eine erste Arbeit über Kettenbrüche; wenig später reichte er bei der Académie des Sciences eine Arbeit über die Gleichungsauflösung ein, die den Kern der heute nach ihm benannten Galoistheorie enthielt. Die Akademie lehnte das Manuskript ab, ermutigte Galois aber, eine verbesserte und erweiterte Fassung einzureichen. Dieser Vorgang wiederholte sich zweimal unter Beteiligung von Augustin-Louis Cauchy, Joseph Fourier und Siméon Denis Poisson. Galois reagierte verbittert, beschuldigte die Akademie, Manuskripte veruntreut zu haben, und beschloss, sein Werk auf eigene Kosten drucken zu lassen.

Als Republikaner war Galois vom Ausgang der Julirevolution enttäuscht und exponierte sich politisch zunehmend; er wurde von seiner Hochschule verwiesen und zweimal verhaftet. Der ersten Verhaftung wegen eines bei einem Bankett mit dem blanken Messer in der Hand ausgebrachten Trinkspruchs auf den neuen König Louis-Philippe, der als versteckte Morddrohung ausgelegt wurde, folgte am 15. Juni 1831 ein Freispruch. Nur einen Monat später nahm Galois in der Uniform der wegen politischer Unzuverlässigkeit inzwischen aufgelösten Artillerie-Garde und schwer bewaffnet an einer Demonstration zum 14. Juli teil, wurde erneut verhaftet und nach dreimonatiger Untersuchungshaft zu sechs Monaten Haft im Gefängnis Sainte-Pélagie verurteilt. Im März 1832 wurde er wegen einer Cholera-Epidemie mit anderen Häftlingen ins Sanatorium Sieur Faultrier verlegt. Am 29. April wurde er aus der Haft entlassen.

Am Morgen des 30. Mai 1832 erlitt Galois bei einem Pistolenduell in der Nähe des Sieur Faultrier einen Bauchdurchschuss, wurde von seinem Gegner und seinem eigenen Sekundanten allein zurückgelassen, Stunden später von einem Bauern aufgefunden und in ein Krankenhaus gebracht, wo er tags darauf „in den Armen“ seines Bruders Alfred starb. Der Duellgegner war ein republikanischer Gesinnungsgenosse, Perschin d’Herbinville, und nicht, wie gelegentlich vorgebracht (Leopold Infeld in "Wen die Götter lieben"), ein agent provocateur der Regierung. Der Anlass für das Duell war ein Mädchen, Stéphanie-Félicie Poterin du Motel, die Tochter eines am Sieur Faultrier tätigen Arztes. Mit ihr tauschte Galois nach seiner Entlassung aus dem Sanatorium Briefe aus, und ihr Name findet sich auf seinem letzten Manuskript; sie scheint sich aber von ihm distanziert zu haben.

Trotzdem halten sich hartnäckig Stimmen, die sagen, das Duell sei inszeniert gewesen, da Galois kaum Interesse an Stéphanie hatte und sein Gegner ein bekannter Schütze war, ja es wurde sogar behauptet, er hätte sich in diesem Duell für die republikanische Sache geopfert. Andere Einschätzungen sprechen von inszeniertem Selbstmord aufgrund seiner unglücklichen Liebe. Solche Duelle „um der Ehre willen“ waren andererseits damals ziemlich häufig.

In der Nacht vor seinem Duell schrieb er einen Brief an seinen Freund Auguste Chevalier, in dem er diesem die Bedeutung seiner mathematischen Entdeckungen ans Herz legte und ihn bat, seine Manuskripte Carl Friedrich Gauß und Carl Gustav Jacob Jacobi vorzulegen; außerdem fügte er Randbemerkungen wie „je n’ai pas le temps“ (mir fehlt die Zeit) in seine Schriften ein. Chevalier schrieb Galois’ Arbeiten ab und brachte sie unter den Mathematikern seiner Zeit in Umlauf, u. a. auch an Gauß und Jacobi, von denen aber keine Reaktion bekannt ist. Die Bedeutung der Schriften erkannte erst 1843 Joseph Liouville, der den Zusammenhang mit Cauchys Theorie der Permutationen sah und sie in seinem Journal veröffentlichte.

Galois begründete die heute nach ihm benannte Galoistheorie, die sich mit der Auflösung algebraischer Gleichungen, d. h. mit der Faktorisierung von Polynomen befasst. Das damalige Grundproblem der Algebra umfasste die allgemeine Lösung algebraischer Gleichungen mit Radikalen (d. h. Wurzeln im Sinne von Potenzen mit gebrochenen Exponenten), wie sie für Gleichungen zweiten, dritten und vierten Grades schon länger bekannt waren. Galois erkannte die dahinter stehenden Konstruktionen der Gruppentheorie. Unabhängig (und Galois nicht bekannt) hatte Niels Henrik Abel bewiesen, dass eine allgemeine polynomiale Gleichung von höherem Grad als 4 im Allgemeinen nicht durch Radikale aufgelöst werden kann. Galois untersuchte Gruppen von Vertauschungen der Nullstellen des Gleichungspolynoms (auch "Wurzeln" genannt), insbesondere die sogenannte Galoissche Gruppe "G", deren Definition bei Galois noch ziemlich kompliziert war. In heutiger Sprache ist das die Gruppe der Automorphismen des Erweiterungskörpers "L" über dem Grundkörper, der durch Adjunktion aller Nullstellen definiert ist. Galois erkannte, dass sich die Untergruppen von "G" und die Unterkörper von "L" bijektiv entsprechen.

Man zeigt dann zum Beispiel, dass im Falle der allgemeinen Gleichung 5. Grades für die zugehörige Gruppe – die Symmetrische Gruppe "S" der Permutationen von 5 Objekten – keine Kompositionsreihe einer Kette von Normalteilern mit zyklischen Faktorgruppen existiert, die den Automorphismengruppen der durch Adjunktion von Wurzeln gebildeten Zwischenkörpern entsprechen. "S" ist keine auflösbare Gruppe, da sie als echten Normalteiler nur die einfache Untergruppe "A" enthält, die alternierende Gruppe der geraden Permutationen von 5 Objekten. Das verallgemeinert sich in dem Satz, dass für "n" > 4 die symmetrische Gruppe "S" den einzigen echten nichttrivialen Normalteiler "A" besitzt, der nichtzyklisch und einfach ist, d. h. ohne nichttriviale Normalteiler. Daraus folgt die allgemeine Nichtauflösbarkeit von Gleichungen höheren als 4. Grades durch Radikale.

Wegen dieser von ihm gefundenen Begriffe und Sätze ist Galois einer der Begründer der Gruppentheorie. In Anerkennung seiner grundlegenden Arbeit wurden die mathematischen Strukturen Galoiskörper (endlicher Körper), Galoisverbindung und Galoiskohomologie nach ihm benannt. Wie anderen, besonders berühmten Mathematikern ist auch ihm ein Symbol gewidmet: GF("q") steht für Galois Field (Galoiskörper) mit "q" Elementen und ist in der Literatur so etabliert wie etwa die Gaußklammer oder das Kronecker-Symbol.

Er lieferte damit auch die Grundlagen für Beweise der allgemeinen Unlösbarkeit von zwei der drei klassischen Probleme der antiken Mathematik, der Dreiteilung des Winkels und der Verdoppelung des Würfels (jeweils mit Zirkel und Lineal, also mit Quadratwurzeln und linearen Gleichungen). Diese Beweise können jedoch auch einfacher, also ohne Galoistheorie, geführt werden. Das dritte Problem, die Quadratur des Kreises, wurde durch den Beweis der Transzendenz von formula_1 durch Ferdinand Lindemann ad acta gelegt.

In dem Brief an Auguste Chevalier deutet Galois auch Arbeiten über elliptische Funktionen an.

Seit dem 2. Februar 1999 trägt der Asteroid (9130) Galois seinen Namen.






</doc>
<doc id="1348" url="https://de.wikipedia.org/wiki?curid=1348" title="Eulersche Formel">
Eulersche Formel

Die nach Leonhard Euler genannte eulersche Formel bzw. Eulerformel, in manchen Quellen auch eulersche Relation, ist eine Gleichung, die eine grundsätzliche Verbindung zwischen den trigonometrischen Funktionen und den komplexen Exponentialfunktionen mittels komplexer Zahlen darstellt. 

Die eulersche Formel erschien erstmals 1748 in Leonhard Eulers zweibändiger "Introductio in analysin infinitorum", zunächst unter der Prämisse, dass der Winkel eine reelle Zahl ist. Diese Einschränkung jedoch erwies sich bald als überflüssig, denn die eulersche Formel gilt gleichermaßen für alle reellen wie komplexen Argumente. Dies ergibt sich aus der reellen eulerschen Formel in Verbindung mit dem Identitätssatz für holomorphe Funktionen. 

Die eulersche Formel bezeichnet die für alle formula_1 gültige Gleichung

wobei die Konstante formula_3 die eulersche Zahl (Basis der natürlichen Exponentialfunktion bzw. des natürlichen Logarithmus) und die Einheit formula_4 die imaginäre Einheit der komplexen Zahlen bezeichnen. Als Folgerung aus der eulerschen Formel ergibt sich für alle formula_5 die Gleichung

Die eulersche Formel lässt sich auf einfache Weise aus den taylorschen Reihenentwicklungen der Funktionen formula_7 und formula_8, formula_9, herleiten:

Für formula_11 ergibt sich aus der eulerschen Formel die sogenannte eulersche Identität
die einen einfachen Zusammenhang zwischen vier der bedeutendsten mathematischen Konstanten herstellt: der eulerschen Zahl formula_3, der Kreiszahl formula_14, der imaginären Einheit formula_4 sowie der reellen Einheit formula_16. Die folgende umgeformte Variante der Gleichung wird bisweilen – obwohl komplizierter – bevorzugt, da in ihr mit der Null noch eine weitere mathematisch bedeutende Konstante hinzukommt:

Erweitert man die Definition des Zahlenwerts von formula_18 als Grenzwert formula_19 auf die komplexe Zahlenebene mit formula_20, so ergibt sich dementsprechend für formula_21 der Wert formula_22. Die nebenstehende Animation zeigt die zu einem Streckenzug in der komplexen Ebene verbundenen Zwischenergebnisse der Berechnung des Ausdrucks formula_23: Sie veranschaulicht, dass dieser Streckenzug für wachsendes formula_24 die Form eines Kreisbogens annimmt, dessen linkes Ende sich tatsächlich der Zahl formula_22 auf der reellen Achse nähert.

Da es mit Hilfe der Eulerformel möglich ist, trigonometrische Funktionen als Linearkombinationen imaginärer Exponentialfunktionen darzustellen, ist sie ein zentrales Bindeglied zwischen Exponentialfunktionen und Trigonometrie:

Die exponentielle Schreibweise der trigonometrischen Funktionen Sinus und Cosinus leitet sich daraus her, dass eine Gleichung aufgestellt wird, in der eine komplexe Zahl mit Betrag formula_16 und Winkel formula_28 mit ihrer komplex konjugierten Zahl addiert (um auf den Cosinus zu kommen) bzw. subtrahiert wird (um auf den Sinus zu kommen). Auf der anderen Seite der Gleichung steht die Entsprechung in trigonometrischer Form (der Zusammenhang ist durch die eulersche Identität gegeben):

Im Endeffekt ermöglicht die Eulerformel damit eine völlig neue Sicht auf die trigonometrischen Funktionen, da die in der herkömmlichen Trigonometrie allein mit reellen Argumenten verwendeten Funktionen Sinus und Kosinus nun auch noch eine Bedeutung in der komplexen Analysis erhalten. Mehr noch: Versieht man sie mit imaginären Argumenten, wird dadurch eine Brücke zu den Hyperbelfunktionen geschlagen:

Wie zu sehen, entsprechen die beiden erhaltenen Funktionen genau den Definitionen des Sinus Hyperbolicus und Kosinus Hyperbolicus.

Eine Folge der Verbindung von trigonometrischen Funktionen und Exponentialfunktion aus der Eulerformel ist der Moivresche Satz (1730).

Ausgehend davon findet die eulersche Formel auch zur Lösung zahlreicher anderer Probleme Anwendung, etwa bei der Berechnung der Potenz formula_33 der imaginären Einheit mit sich selbst. Obwohl das erhaltene Resultat mehrdeutig ist, bleiben alle Einzellösungen im reellen Bereich mit einem Hauptwert von formula_34

Eine praktisch wichtige Anwendung der eulerschen Formel findet sich im Bereich der Wechselstromtechnik, namentlich bei der Untersuchung und Berechnung von Wechselstromkreisen mit Hilfe komplexer Zahlen.




</doc>
<doc id="1349" url="https://de.wikipedia.org/wiki?curid=1349" title="Einkorn">
Einkorn

Einkorn ("Triticum monococcum"), auch "Blicken" oder "Kleiner Spelz" genannt, ist eine der ältesten domestizierten Getreidearten. Einkorn stammt vom wilden Weizen ("Triticum boeoticum" Boiss.) ab, der im Gegensatz zu Einkorn eine brüchige Ährchengabel (Rhachis) hat.
Einkorn galt als Vorläufer von Emmer, Dinkel und Saatweizen, bis durch genetische Untersuchungen festgestellt wurde, dass Emmer von Wildem Emmer aus der Südosttürkei abstammt.

Das Ursprungsgebiet von domestiziertem Einkorn ist umstritten. "Heun" und andere argumentieren anhand genetischer Untersuchungen von Einkorn aus der Türkei, dem Kaukasus und dem Libanon für eine Herkunft aus der südöstlichen Türkei (Karacadağ), während Martin K. Jones et al. (2006) und andere den Ursprung in der südlichen Levante sehen, wo Emmer, Einkorn und Gerste seit dem präkeramischen Neolithikum A 8000–7700 v. Chr. domestiziert wurden. Die Nachweise werden allerdings angezweifelt. Sicher domestiziertes Einkorn stammt aus dem präkeramischen Neolithikum B (6700–6000 v. Chr.), zum Beispiel aus Jericho und Tell Aswad II.

Am oberen Euphrat wurde domestiziertes Einkorn in den vorkeramischen Schichten von Mureybit, Nevalı Çori (7200 v. Chr.) Jerf el Ahmar, Abu Hureyra (7800–7500 v. Chr.) und Dja'de gefunden. Aus Siedlungen wie Cafer Höyük, Nevalı Çori und Cayönü liegt jedoch auch wildes Einkorn vor, was auf die Bedeutung dieser Pflanze als Sammelpflanze bereits im Mesolithikum bzw. Epipaläolithikum hinweist.

Einkorn und Emmer gehören zu den wichtigsten Kulturpflanzen der Bandkeramik. Erst in der späten Bandkeramik gewinnt auch Binkelweizen ("T. compactum") an Einfluss. So war Einkorn in der linearbandkeramischen Kultur (zwischen 5700 und 4100 v. Chr.), die sich aufgrund des allgemeinen Klimawandels als neolithische Kultur mit Ackerbau und Viehzucht in Mitteleuropa behaupten konnte, ein fester Bestandteil der agrarischen Produktion. Diese Kultur mit ihren fast dörflichen Siedlungen nutzte zunächst die tief liegenden Lößflächen für ihren Feldbau. Angebaut wurden neben Einkorn ("Triticum monococcum") noch Emmer ("Triticum dicoccum"), Dinkel (Triticum aestivum subsp. spelta), Lein (Linum usitatissimum) und die Hülsenfrüchte Linse und Erbse, vermutlich im Schwendbau. So weisen geoklimatische bzw. geoökologische Forschungen auf ein sehr mildes Klima während der Ausbreitung der bandkeramischen Kultur in Mitteleuropa hin.

Reste von Einkorn wurden unter anderem bei der steinzeitlichen Gletschermumie „Ötzi“ in den Alpen gefunden.

Einkorn ist relativ anspruchslos in Bezug auf die Qualität des Bodens. Außerdem ist es resistent gegen viele Schädlinge wie Wurzelfäule, Spelzenbräune oder den Mutterkorn-Pilz und kann sich besser gegen die Konkurrenz von Ackerunkräutern durchsetzen als moderne Weizensorten. Allerdings ist der Ertrag erheblich geringer als bei den modernen Weizen-Sorten, auf sandigen Böden werden Erträge von lediglich 12 bis 21 dt/ha erzielt.

Wurde der Anbau von Einkorn im 20. Jahrhundert wirtschaftlich nahezu bedeutungslos, so wird Einkorn heute doch z. B. in der Schweiz, in Deutschland, in Österreich im Waldviertel, in Italien als lokale Spezialität im Piemont und der Provinz Brescia und in der Türkei weiterhin angebaut. Zunehmend werden auch verarbeitete Produkte wie Nudeln, Brot und Bier aus Einkorn angeboten.

Obwohl wesentlich ertragsärmer als Saat-Weizen, enthält Einkorn mehr Mineralstoffe und Aminosäuren als dieser. Ein hoher Gelbpigmentgehalt an Beta-Carotin gibt dem Einkorn-Mehl eine gelbliche Farbe. Mit Einkorn-Malz als Zutat kann auch Bier gebraut werden.




</doc>
<doc id="1350" url="https://de.wikipedia.org/wiki?curid=1350" title="Emmer (Getreide)">
Emmer (Getreide)

Emmer ("Triticum dicoccum"), auch Zweikorn genannt, ist eine Pflanzenart aus der Gattung Weizen ("Triticum"). Er ist, zusammen mit Einkorn, eine der ältesten kultivierten Getreidearten. Diese Weizenart mit lang begrannten, meist zweiblütigen Ährchen wird heute in Europa kaum noch angebaut – wenn, dann im Wesentlichen der Schwarze Emmer. Daneben gibt es den Weißen und den Roten Emmer.

Seine Stammform ist der Wilde Emmer ("Triticum dicoccoides").

Wilder Emmer ist der Urvater der „Emmerreihe“ (mit 2n = 4x = 28 Chromosomen). Zu ihr gehören auch der Hartweizen ("Triticum durum") und Kamut. Emmer ist tetraploid und ein natürlich entstandener Additionsbastard zweier diploider Süßgräser: "Triticum urartu", das mit wildem Einkorn ("Triticum boeoticum") eng verwandt ist, und einer bisher nicht genau identifizierten Aegilops-Art, die mit "Aegilops searsii" und "Aegilops speltoides" eng verwandt ist.

Die Wildform des Emmers kann mit domestiziertem tetraploidem Weizen fruchtbare Kreuzungen eingehen.

Wilder Emmer kommt in der Südosttürkei, in Syrien, im Libanon, in Jordanien, Palästina, Israel und im östlichen Irak und Iran vor (Fruchtbarer Halbmond). Teilweise wächst er zusammen mit dem wilden tetraploiden Weizen ("Triticum araraticum"), mit dem er leicht verwechselt werden kann. Nach genetischen Untersuchungen ist der domestizierte Emmer am nächsten mit den Wildarten im südöstlichen Kleinasien verwandt.

Domestizierter Emmer hat ein AABB-Genom und eine feste Rhachis, die verhindert, dass sich das Getreide selbst aussäen kann. Bei Wildem Emmer bricht die Ährchengabel, wenn das Korn reif ist, und es kann sich verbreiten. Domestizierter Emmer wurde in Tell Aswad, in Abu Hureyra (Schicht 2) und Cayönü gefunden. Ob der Emmer aus Nevali Cori vollständig domestiziert ist, ist unklar. Seit dem präkeramischen Neolithikum B kommt domestizierter Emmer regelmäßig vor.

Emmer gehört zu den ältesten kultivierten Getreidearten. Sein Ursprung liegt im Nahen Osten, wo er seit mindestens 10.000 Jahren angebaut wird. In Europa verbreitete sich der Anbau von Emmer und Einkorn während der frühesten neolithischen Besiedlung. Durch die Ausbreitung des Ackerbaus kam der Emmer von Westpersien über Ägypten, Nordafrika und den Balkan bis nach Mitteleuropa. 

Der Emmer galt zur Römerzeit als „Weizen von Rom“. Erst ab der Neuzeit verlor er in Europa an Bedeutung; im Laufe des 20. Jahrhunderts stieg die Anbaufläche für Emmer jedoch wieder an.

Emmer gehört zu den Gräsern, deren Körner – genau wie Nüsse – zu den einsamigen Schließfrüchten zählen.
Der Emmer hat zwei Körner pro Ährchen, die fest von Spelzen umschlossen sind. Emmer kann bis zu 1,50 m hoch wachsen. Die enorme Höhe führt unter Umständen aber zu geringer Standfestigkeit. 
Die Blattöhrchen des Emmer sind groß und bewimpert, die Blatthäutchen sind mittelgroß und stumpfgezahnt. Die Deckspelze sind begrannt, und die Hüllspelzen sind lang und gezackt. Die Grannenlänge beträgt einige Zentimeter.

Emmergetreide ist eiweiß- und mineralstoffreich. Trotz seiner mäßigen Klebereigenschaften ist Emmer auch für die Brotherstellung geeignet. Vollkornbackwaren verleiht Emmer einen herzhaften und leicht nussigen Geschmack. Ebenso wird der Emmer für die Bierherstellung eingesetzt. Das Emmerbier ist dunkel und sehr würzig.

Die Ähren werden in der Floristik bei Gestecken verwendet. Die gekochten Körner können als Einlage für Suppen und Eintöpfe, aber auch in Salaten, Aufläufen oder Bratlingen verwendet werden. Eine italienische Spezialität ist die Zweikornsuppe ("Minestra di farro" oder: "Zuppa al farro"), ein deftiger Emmereintopf, ein klassisches „Armeleutegericht“ aus den ländlichen Gebieten der Toskana.

Vor dem Zweiten Weltkrieg wurde Emmer noch spärlich in Thüringen und in Süddeutschland angebaut.

Heute ist Emmer in Mitteleuropa nach wie vor ein Nischenprodukt, gewinnt aber regional an Bekanntheit. In Nordbayern wird im Raum Coburg wieder Emmer angebaut und dort unter anderem für die Bierherstellung verwendet (auf Emmerbier hat sich z. B. das Riedenburger Brauhaus spezialisiert). Der Emmeranbau wurde dort im Rahmen eines Projektes zur Förderung des Anbaus alter Kulturarten sowie seltener Ackerwildkräuter wieder aufgenommen. Forscher der Universität Hohenheim (Stuttgart) und Vertreter des baden-württembergischen Landesinnungsverbandes der Bäcker haben einen „Arbeitskreis Spelzgetreide“ (Einkorn, Emmer und Dinkel) gegründet, in dem auch Müller und Nudelfabrikanten vertreten sind, und wollen den Anbau dieser frühen Weizensorten darüber fördern.

In Österreich wird Emmer im Burgenland und in Niederösterreich angebaut und über Bio-Läden und Supermärkte vertrieben.
In der Schweiz wird der Weiße Emmer im Schaffhauser Klettgau wie auch im Zürcher Weinland seit Mitte der 1990er Jahre wieder angebaut. Zu den daraus verarbeiteten Produkten zählen neben Emmerkörnern und -mehlen auch Spezialbrote, Teigwaren, Emmer-Schwarzbier und Emmerschnaps.

In Italien wird der Emmeranbau ausgeweitet. Insbesondere in der gebirgigen Gegend von Garfagnana in der Toskana genießt Emmer, in Italien farro genannt, gesetzlichen Schutz und besonderen Status ("Indicazione Geografica Protetta"). Die Produktion wird durch eine Genossenschaft ("Consorzio Produttori Farro della Garfagnana") überwacht. Produkte aus Emmer werden in Reformläden in Italien und sogar in Großbritannien angeboten.

In Finnland wird Emmer auf dem Gut Malmgård in Uusimaa seit 2009 mit guten Erträgen angebaut. Im Jahr 2012 wurde dort auf 18 ha Emmer angebaut.

Da Emmer auf schwachen Böden in Gebirgsgegenden gute Erträge verspricht und resistent gegenüber Krankheiten ist, wird er auch z. B. in Tschechien, der Slowakei (an der Grenze zwischen den beiden Ländern), Spanien (Asturien), Griechenland, Albanien und der Türkei angebaut.

Emmer ist traditionelle Frucht in Äthiopien, wo er Potential zur Verbesserung der Nahrungssicherheit besitzt.

Emmer hat keine besonders hohen Ansprüche an die Bodenart und den pH-Wert des Bodens.

Die Grundbodenbearbeitung sollte mit dem Pflug oder Grubber erfolgen. Die Saatbettbereitung sollte aufgrund der Strukturreserve und des Windhalms nicht zu fein sein. Dies entspricht der Saatbettbereitung für Weizen. Die Stoppelbearbeitung ist für die schnellere Rotte der Ernterückstände sehr sinnvoll.

Die Vorfruchtwirkung von Emmer ist mäßig, da die Übertragung von Krankheiten wie z. B. Halmbruch möglich ist. Die Selbstverträglichkeit ist nicht optimal, und aus diesem Grund sollte zwei Jahre danach kein Wintergetreide angebaut werden. Eine gute Vorfruchtwirkung auf Emmer haben Kartoffeln und Mais.

Der Saatzeitpunkt liegt zwischen Mitte September und Mitte Oktober. Der Emmer ist sehr winterhart, da er Temperaturen bis ca. –20 °C aushält. Die Saatstärke sollte zwischen 150 und 200 kg/ha und die Saattiefe 4–6 cm betragen. Der Reihenabstand beträgt 10–25 cm. Da Emmer Spelzen hat, die rau und behaart sind, kann es zu Problemen bei der Aussaat kommen. Diese Probleme können umgangen werden, wenn man das Emmerkorn entspelzt.

Die häufigsten Krankheiten sind Pilzkrankheiten wie z. B. Mehltau, Roste, DTR, Septoria tritici und Halmbruch. Durch die Spelzen ist das Korn vor Ährenseptoria (Phaeosphaeria nodorum) und Fusarien geschützt. Bei früher Saat hat die Fritfliege als Schädling die bedeutendste Rolle.

Für die Unkrautregulierung kann ab dem Dreiblattstadium gestriegelt werden. Bei starkem Befall ist auch schonendes Striegeln ab dem Zweiblattstadium möglich.

Die Ernte des Emmer erfolgt Anfang bis Mitte August. Beim Dreschen mit einem Mähdrescher ist zu beachten, dass bei einer niedrigen Trommeldrehzahl und weniger Wind als bei Weizen geerntet wird. Eine Trocknung der Körner ist nicht erforderlich, wenn die Kornfeuchte unter 14,5 % liegt.
Die Lagerung des Korns ist sowohl mit als auch ohne Spelzen möglich.

Der Ertrag von Schwarzem Emmer liegt mit rund 30 Dezitonnen/Hektar weit unter dem Ertrag von Weizen mit 75 dt/ha. Aus diesem Grund ist der Anbau von Emmer seit dem Ersten Weltkrieg stark gesunken.

Emmer ist wenig standfest und enorm lagergefährdet.

Durch natürliche Selektion entstand aus dem Emmer der Schwarze Emmer ("Triticum dicoccon" var. "atratum"). Dieser wird als Wintergetreide angebaut, da er einen höheren Ertrag hat als Emmer. UV-bedingte Mutationen sind beim Schwarzen Emmer kaum möglich, da er sich durch seine schwarze Färbung gut davor schützen kann. Aus diesem Grund ist er genetisch das beständigste Getreide. Die Schwarzfärbung wird durch Beta-Carotin verursacht.




</doc>
<doc id="1351" url="https://de.wikipedia.org/wiki?curid=1351" title="Erbse">
Erbse

Die Erbse ("Pisum sativum"), auch Gartenerbse oder Speiseerbse genannt, ist eine Pflanzenart aus der Gattung Erbsen ("Pisum") in der Unterfamilie Schmetterlingsblütler (Faboideae) innerhalb der Familie der Hülsenfrüchtler (Fabaceae, Leguminosae). Ursprünglich aus Kleinasien stammend, ist die Erbse seit Jahrtausenden eine wichtige Nutzpflanze. Sie enthält viel Protein und wird als Gemüse und als Tierfutter verwendet.

Die Erbse ist eine einjährige, krautige Pflanze. Das Wurzelsystem ist in der oberen Bodenschicht stark verzweigt und kann in geeigneten Böden eine Tiefe von 1 Meter erreichen. Die niederliegenden oder kletternden Stängel werden 0,5 bis 2 Meter lang und sind einfach oder am Grund verzweigt, hohl, kantig, kahl und bläulichgrün.

Die Laubblätter besitzen ein bis drei Fiederpaare und verzweigte Blattranken. Die Fiederblätter sind eiförmig bis breit-elliptisch, gerundet, ganzrandig (oder entfernt gezähnt). Sie sind 2 bis 7 Zentimeter lang und 1,5 bis 4 Zentimeter breit. Die Nebenblätter sind mit 4 bis 10 Zentimetern relativ groß und breit halbherzförmig. Am unteren Rand sind die Nebenblätter entfernt gezähnt bis ausgebuchtet und am Grund haben sie meist einen violetten Punkt. Die Spaltöffnungen befinden sich auf der Ober- und Unterseite der Blattspreite.

Ein bis drei Blüten stehen in einem traubigen Blütenstand und die Blütenstandsachse endet oft in einer Granne. Der Blütenstiel ist 5 bis 10 Millimeter lang.

Die zwittrigen Blüten sind zygomorph und fünfzählig mit doppelter Blütenhülle. Die fünf Kelchblätter sind glockig verwachsen und am Rücken ausgesackt. Die Kelchzähne sind eiförmig-lanzettlich. Die unteren Kelchzähne sind etwa dreimal so lang wie die Kelchröhre, sowie schmaler und länger als die oberen. Die 15 bis 36 Millimeter lange Blütenkrone hat den typischen Aufbau von Schmetterlingsblüten. Bei der Unterart "Pisum sativum" subsp. "sativum" ist die Fahne weiß, bei der Unterart "Pisum sativum" subsp. "elatius" ist die Fahne blasslilafarben und die Flügel sind dunkelpurpurfarben.

Die Hülsenfrüchte sind 3 bis 12 Zentimeter lang, 1 bis 2,5 Zentimeter dick und je nach Sorte grün, gelb oder bräunlich, selten schwarz. Die Hülsenfrüchte enthalten vier bis zehn Samen, die wie die Pflanze Erbsen genannt werden.

Die Samen weisen einen Durchmesser von 3 bis 9 Millimetern auf und sind je nach Sorte unterschiedlich gefärbt. Das Hilum ist bei einem Durchmesser von etwa 2 Millimetern elliptisch bis kreisrund.

Die Chromosomenzahl beträgt 2n = 14.

An den Seitenwurzeln befinden sich die Wurzelknöllchen. Die Erbse geht eine Symbiose spezifisch mit dem stickstoffbindenden Knöllchenbakterien "Rhizobium leguminosarum" symbiovar viciae ein, die bei "Pisum sativum" und anderen Schmetterlingsblütlern erstmals durch den Italiener Malphigi 1675 in seinem Werk "Anatome plantarum" beschrieben wurde. Außerdem ist eine arbuskuläre Mykorrhiza mit dem Pilz "Glomus intraradices" (jetzt "Rhizophagus intraradices") und anderen Pilzarten bedeutsam, die vor allem die Phosphorversorgung verbessert.

Blütenökologisch handelt es sich um „Nektarführende Schmetterlingsblumen (= Schiffchenblumen)“. Der Bestäubungsmechanismus stellt eine Kombination aus Pump- und Bürstenmechanismus dar. Die Blüten duften nach Honig. Die unteren Kronblätter sind so eng miteinander verbunden, dass nur Hummeln zum Nektar gelangen können, aber selbst diese besuchen die Blüten wenig. In Mitteleuropa wird die Erbse nur von wenigen Bienen besucht. Der Samenansatz erfolgt daher bei uns überwiegend über eine Selbstbestäubung der kleistogamen Blüten. Zumindest in Mitteleuropa ist die Erbse ganz überwiegend autogam.

Die Blütezeit reicht von Mai bis Juni, wobei eine Blüte etwa drei Tage und ein Exemplar zehn bis 21 Tage blüht.

Die aufgeblähten Hülsenfrüchte wirken als Austrocknungsstreuer. Es liegen typische Rollsamen mit einer in diesem Fall durchscheinenden Samenschale vor, so dass einige Merkmale der Folgegeneration bereits auf der Mutterpflanze an den Samen zu erkennen sind.

Die Erbse wird von einer Vielzahl an pilzlichen Schädlingen befallen. So kommen die Rostpilze "Uromyces viciae-fabae" var. "viciae-fabae" und "Uromyces pisi" auf Blättern vor. Der Echte Mehltau "Erysiphe pisi" und der Falsche Mehltau "Peronospora viciae" kommen ebenfalls auf Blättern vor., ebenso "Alternaria alternata". In der Wurzel kommen weit verbreitete Pilze wie "Fusarium oxysporum", "Rhizoctonia solani",
"Sclerotinia sclerotiorum", "Thielaviopsis basicola" und "Pythium" spp. vor.

"Pisum sativum" mit dem Chromosomensatz 2n = 14 ist ein klassisches Objekt der Mutationsforschung. Besonders auffällig sind die doppelt gefiederten Mutanten, bei denen alle Fiedern zu Ranken umgebildet sind, so dass, wie bei der Ranken-Platterbse "Lathyrus aphaca", die Photosynthese fast nur von den großen Nebenblättern übernommen wird.

Grüne, unreife Erbsen enthalten 18 bis 20 % Trockensubstanz, die sich folgendermaßen verteilt: 5–8 % Protein, 0,5 % Fett, 10–15 % Kohlenhydrate. Reife Samen enthalten 20–25 % Eiweiß, 1–3 % Fett und 60 % Kohlenhydrate. Marquard gibt folgende Prozentzahlen, bezogen auf das Trockengewicht, an: 25,7 % Rohprotein, 1,4 % Rohfett, 53,7 % Kohlenhydrate, 18,7 % Ballaststoffe und 2,9 % Mineralstoffe.

Die für den Menschen essentiellen Aminosäuren sind in Erbsen wie folgt vorhanden (in Gramm pro 16 Gramm Stickstoff): (Cystein 1,0), Methionin 0,9, Lysin 7,3, Isoleucin 4,2, Leucin 7,0, Phenylalanin 4,4, (Tyrosin 3,1), Threonin 3,8, Tryptophan 1,5, Valin 4,7.

Der durchschnittliche Mineralstoffgehalt beträgt:

Trockenspeiseerbsen besitzen einen Tanningehalt von 0,9 bis 1,4 %, der Tanningehalt von Futtererbsen liegt zwischen 1,5 und 2,5 %.

Erbsen enthalten wie die meisten Leguminosen Phytoöstrogene, die die Fruchtbarkeit von Säugetieren reduzieren. In Indien verwendeten Frauen Suppe aus Erbsenhülsen zur Verzögerung der Empfängnis.

Erbsensamen enthalten in geringem Ausmaß auch cyanogene Glycoside (Linamarin), etwa 2,3 mg HCN pro 100 g.

Die Erstveröffentlichung zu "Pisum sativum" erfolgte 1753 durch Carl von Linné in "Species Plantarum", 2, S. 727.

Innerhalb der weitgefassten Art "Pisum sativum" existiert ein breitgefächerter Schwarm unklar abgrenzbarer Formen, Kultivaren und Landrassen, die von verschiedenen Autoren als mehr als 100 Unterarten oder Varietäten beschrieben worden sind. Diese sind nach genetischen Analysen stark durch Hybridisierung und Introgression geprägt. Darunter ist auch die vermutliche wilde Stammform der kultivierten Erbse, meist als Unterart "Pisum sativum" subsp. "elatius" bezeichnet. Die wildwachsenden Sippen besitzen ein großes Areal, das vom mediterranen Südeuropa und Nordafrika, westlich bis Spanien, über Vorder- und Zentralasien und Iran bis Turkmenistan reicht. Die genetischen Analysen bestätigen einen Ursprung der Kulturform daraus im „Fruchtbaren Halbmond“ in Westasien. Die genetische Variabilität der Wildform ist, wie zu erwarten, erheblich höher als diejenige der Kulturform, und schließt diese mit ein. Viele Autoren erkennen daneben eine zweite wilde Unterart an, die "Pisum sativum" subsp. "syriacum" oder "Pisum sativum" subsp. "pumilio" genannt wird; diese ist östlicher verbreitet und kommt von Zentralanatolien an ostwärts vor. Ihre genetische Basis ist unklar, sie ist zudem durch einen breiten Schwarm von Mischformen mit "elatius" verbunden.

Die Erbse wird heute weltweit angebaut. Es sind sehr viele Varietäten und Convarietäten beschrieben worden. Die wichtigsten sind:


Die Anbaufläche für trockene Erbsen in Deutschland lag im Jahr 2016 bei 87.500 Hektar mit Schwerpunkt in Ostdeutschland. Der Anbau ist in den letzten beiden Jahrzehnten rückläufig (2001 noch 139.000 Hektar), insbesondere bei Futtererbsen für die Viehzucht.

Die Kulturform ist heute weltweit in gemäßigten Gebieten verbreitet, bis zu 67° nördlicher Breite etwa in Skandinavien. In den Alpen wächst sie bis in Höhenlagen von 2000 Metern.

Die Erbse gedeiht am besten auf Lehmböden mit ausreichend Humus und Kalk, ausgeglichener Wasserführung und guter Durchlüftung, etwa Löß- und tiefgründigen Kalkböden. Die Bodenreaktion soll im neutralen bis schwach basischen Bereich, etwa zwischen pH 6 und 7, liegen. Nicht geeignet sind schwere Tonböden, Sand- und Moorböden. Die Erbse hat eine starke Unverträglichkeit zu sich selbst, daher müssen Anbaupausen von sechs bis acht Jahren eingehalten werden. Sie gilt aufgrund des frühen Erntetermins und der positiven Beeinflussung der Bodenstruktur als gute Vorfrucht für Raps und Wintergetreide. Erbsen werden in Mitteleuropa im Frühjahr, von März bis Anfang April, mittels Drillsaat ausgesät. Auch Mischanbau mit Ackerbohne oder Getreiden kommt vor. Als stickstoff-fixierende Leguminose ist nur wenig oder keine Stickstoffdüngung notwendig. Erbsen sind recht empfindlich gegenüber Unkraut, so dass meist Herbizide eingesetzt werden.

Auf Erbsen und Erbsenpflanzen als Nahrungsgrundlage haben sich der Erbsenkäfer, der Erbsenwickler und die Erbsenblattlaus spezialisiert. Problematisch werden oft auch Blattrandkäfer (Gattung "Sitona").

2016 wurden laut der Ernährungs- und Landwirtschaftsorganisation FAO weltweit etwa 4,8 Millionen Tonnen grüne Erbsen und 14,3 Millionen Tonnen trockene Erbsen geerntet.

Folgende Tabellen geben eine Übersicht über die 10 jeweils größten Produzenten von grünen und trockenen Erbsen weltweit.
2016 lagen die Erntemengen für trockene Erbsen in Deutschland bei 260.200 t, in Österreich bei 19.155 t und in der Schweiz bei 9.021 t.

Der größte Teil der Trockenerbsen wird in der Tierernährung als Erbsenschrot verfüttert, ebenso Erbsenfuttermehl aus der Nahrungsmittelproduktion und Erbsenkleie als Rückstand in der Schälmüllerei. Auch Erbsenstroh wird wegen seines hohen Nährstoffgehalts verfüttert. Die Erbse wird als Grünfutter und -dünger verwendet.

Für die menschliche Ernährung fanden ursprünglich ebenfalls Trockenerbsen Verwendung, die hauptsächlich als Mus zubereitet wurden. Heute noch verbreitet ist die Erbsensuppe. Im 19. Jahrhundert entstand die Erbswurst. Getrocknete Erbsen werden als ganze Erbsen (mit Samenschale) oder als halbe Erbsen (deren Samenschale entfernt wurde) benutzt. Heute werden Erbsen in Mitteleuropa hauptsächlich grün zubereitet. Häufig finden Erbsen in Form von Konserven und tiefgekühlt Verwendung; seltener frisch, da Erbsen nicht besonders lange haltbar sind und rasch an Geschmack verlieren. Im Gegensatz zu früher wird sie als Gemüsebeilage verwendet, weniger als Hauptnahrungsmittel.

Gekeimte Erbsen könnten nach Untersuchungen von Urbano 2005 die Nährstoffe besser verdaulich machen.

Züchtungen der Markerbse werden als nachwachsende Rohstoffe für die Gewinnung von Stärke eingesetzt, z. B. zur Herstellung biologisch abbaubarer Folien.

Ab etwa 8000 v. Chr. ist der Anbau von Erbsen durch archäologische Funde belegt, damit gehört sie mit zu den ältesten Kulturpflanzen. Bei vielen der ältesten Funde ist allerdings die Unterscheidung zwischen angebauten und wild gesammelten Erbsen mitunter schwierig, das wichtigste Merkmal, die Struktur der Samenschale, ist meist nicht erhalten. Funde liegen aus zahlreichen Siedlungen des präkeramischen Neolithikums aus dem fruchtbaren Halbmond Vorderasiens vor. Die bisher ältesten Funde stammen aus Aswad in Syrien und sind etwa 10.500 bis 10.200 Jahre alt, Funde aus Çayönü in Anatolien und Jericho im Jordantal sind nur wenig jünger. Schon ab ca. 7.000 v. Chr. liegen auch Funde aus Ausgrabungen von Zypern und aus dem Ägäisraum vor. Funde aus Nea Nikomedeia sind ca. 8.400 bis 8.200 Jahre alt. Auch in Bulgarien ist die Kultur fast ebenso alt.

In Deutschland war die Erbse, wie auch die Linse, neben Getreide das Grundnahrungsmittel der ältesten Ackerbauern, den Bandkeramikern. An jeder zweiten Getreidefundstelle kommen auch Erbsen vor, Nordgrenze war der nördliche Rand der Mittelgebirge. Aus der Mittleren Jungsteinzeit liegen anteilsmäßig wesentlich weniger Erbsenfunde vor, die Ursache dafür ist ungeklärt, lag aber möglicherweise in einer vermehrten Nutztierhaltung. In der Bronzezeit, ab etwa 1800 v. Chr., nahm der Anteil der Hülsenfrüchte und damit auch der Erbsen wieder zu.

Im Altertum wurde die Erbse in Europa ebenfalls weit verbreitet angebaut. Die antiken griechischen und römischen Autoren erwähnen sie aber nur selten und beiläufig. Auch im Capitulare de villis Karls des Großen werden Erbsen erwähnt ("pisos mauriscos"). Im 13. Jahrhundert erwähnte Petrus de Crescentia aus Bologna weißsamige Erbsen. In den Kräuterbüchern des 16. Jahrhunderts werden "Kleine Felderbsen" mit weißen Blüten und "Große Gartenerbsen" mit rosa oder roten Blüten unterschieden, z. B. bei Leonhart Fuchs. Eine Tradition als Heilpflanze scheint es nicht zu geben, Madaus' sonst umfassendes "Lehrbuch der biologischen Heilmittel" erwähnt die Erbse gar nicht.

Bis ins 17. Jahrhundert wurde die Erbse als Trockengemüse verwendet und im Allgemeinen als Mus gegessen. Erst ab dem 16. oder 17. Jahrhundert wurden Sorten gezüchtet, die man unreif und grün verspeiste oder als Zuckererbsen mit der Hülse. Zu Beginn waren diese Erbsen sehr teuer und etwa am Hof König Ludwig XIV. sehr beliebt. Die Trockenerbsen wurden jedoch erst durch die modernen Konservierungstechniken (Konserven, Tiefkühlen) vom Speisezettel verdrängt. Sie erleben mit der Vollwertküche wieder eine kleine Renaissance.

Erbsen galten einerseits als Totenspeise. Wer in der Karwoche Erbsen aß, sollte bald eine Leiche im Haus haben. Auch das Verspeisen von Erbsen während der zwölf Rauhnächte sollte zu verschiedenen Unglücksfällen führen. In Böhmen war es Brauch, am Heiligen Abend in die Ecken der Stuben kreuzweise Erbsenmus zu streuen, wohl ein Relikt aus der Verehrung der Totengeister, später sagte man „für die Mäuse“. In manchen Gegenden ist Erbsensuppe fixer Bestandteil des Leichenschmauses, so in Mecklenburg. In Freiburg im Breisgau wurde sie bei der Totenwache gereicht.

Erbsen galten auch als Fruchtbarkeitsbringer, da die verstorbenen Ahnen auch die Fruchtbarkeit brachten. Einige Bräuche in diesem Zusammenhang waren/sind: Erbsen als erstes Futter für die Schweine an Neujahr (Ostpreußen); Schlagen eines Sackes mit Erbsen an Obstbäume, damit sie so viel Früchte wie Erbsen im Sack tragen; Erbsen als Hochzeitsspeise; Erbsen zum Bewerfen des Brautpaares. Als Fruchtbarkeitsbringer sei auch der Erbsenbär erwähnt, der etwa im rheinländischen Karneval oder im alemannischen Raum vorkommt, oder in Ostdeutschland bis ins 20. Jahrhundert Bestandteil des Brautzugs war. Der Erbsenbär war in germanischer Zeit eine Verkörperung des Gewittergottes Thor (Donar), von daher kommt auch der Brauch in manchen Gebieten Deutschlands, am Donnerstag Erbsensuppe zu essen (z. B. Schwaben).

In der Bibel werden Erbsen nicht erwähnt. In Märchen sind sie profanes Nahrungsmittel, z. B. in Basiles "Der Floh", "Der Dummling", "Der goldene Stamm", im berühmten "Aschenputtel" und in "Der junge Riese" aus Grimms Märchen. In "Die zwölf Jäger", "Der Räuberbräutigam", "Das blaue Licht" sollen ausgestreute Erbsen den Bräutigam oder Übeltäter entdecken. In Hans Christian Andersens "Die Prinzessin auf der Erbse" wird damit vornehme Herkunft geprüft, "Fünf aus einer Schote" hingegen zeigt existentielle Not, wie auch Bechsteins Sage Nr. 715 "Der Erbsenacker". Erbsenmus galt als Leibspeise von Zwergen und Heinzelmännchen, vgl. Grimms Sage Nr. 156 "Schmied Riechert".




</doc>
<doc id="1352" url="https://de.wikipedia.org/wiki?curid=1352" title="Erasmus Reinhold">
Erasmus Reinhold

Erasmus Reinhold (* 22. Oktober 1511 in Saalfeld/Saale; † 19. Februar 1553 ebenda) war ein deutscher Astronom und Mathematiker. Er ist einer der ersten Verfechter des kopernikanischen Weltbilds und entwickelte dieses weiter. Er kann daher als Bindeglied zwischen Kopernikus und Kepler betrachtet werden.

Reinhold wurde seit dem Wintersemester 1530/31 an der Universität Wittenberg ausgebildet und wurde dort auch Rektor. 1535 wurde er Magister der sieben freien Künste und am 30. April 1536 fand er Aufnahme in den Senat der philosophischen Fakultät. 1536 erhielt er dort eine Professur für höhere Mathematik von Philipp Melanchthon, ähnlich wie Rheticus. Reinhold identifizierte und beschrieb eine große Anzahl von Sternen. Er hatte sich auch an den organisatorischen Aufgaben der Hochschule beteiligt. So war er im Wintersemester 1540/41, sowie im Sommersemester 1549 Dekan der philosophischen Fakultät und wurde im Wintersemester WS 1549/50 Rektor der Alma Mater.

Er benutzte moderat pragmatisch die Lehren des Nikolaus Kopernikus. Von Herzog Albrecht von Brandenburg-Ansbach wurde er unterstützt, welcher den Druck der Preußischen Tafeln finanzierte, woher diese ihren Namen erhielten. Diese astronomischen Tafeln halfen das kopernikanische System im ganzen Deutschen Reich und darüber hinaus bekannt zu machen. Reinhold war für seine genauen Messwerte bekannt. Bezeichnend ist, dass Tycho Brahe so sehr an Reinholds Messungen interessiert war, dass dieser selbst die Reise nach Saalfeld zu Reinhold antrat.

1582 wurden die Berechnungen des Kopernikus und die Preußischen Tafeln zur Grundlage für die Gregorianische Kalenderreform genutzt.

Der Mondkrater Reinhold ist nach ihm benannt.
In seiner Heimatstadt Saalfeld sind ein Gymnasium und eine Straße nach ihm benannt.





</doc>
<doc id="1353" url="https://de.wikipedia.org/wiki?curid=1353" title="Elfenbein">
Elfenbein

Elfenbein bezeichnet im engeren Sinne die Substanz der Stoßzähne von Elefant und Mammut, wobei der getötete Elefant Hauptquelle von Elfenbein ist, während das ausgestorbene Mammut das fossile Elfenbein liefert. Im weiteren Sinne wird unter Elfenbein auch das Zahnbein der Stoß- und Eckzähne verschiedener Säugetiere verstanden (Walross, Pottwal, Narwal, Flusspferd).

Elfenbein ist seit alters her Werkstoff zur Herstellung von Gebrauchs- und Schmuckgegenständen. Die steigenden Ansprüche einer wachsenden Weltbevölkerung haben beim Elfenbein Probleme geschaffen, die den Bestand vor allem des afrikanischen Elefanten gefährden.

Das deutsche Wort "Elfenbein" (mittelhochdeutsch "helfenbein", althochdeutsch "helfantbein") bedeutet "Elefantenknochen". Es geht zurück auf das altgriechische ἐλέφας und das lateinische "elephantus", was zunächst das Material bezeichnete und später auch auf das Tier übertragen wurde, als Griechen und Römer es kennenlernten. Im alten Rom diente es zur Herstellung von Zahnersatz. Hier wurde es auch "indisches Horn" genannt, wie den Epigrammen Martials zu entnehmen ist.

Geschichtlich ist "Elfenbein" in der Regel nur auf das Stoßzahnmaterial der Elefanten und Mammuts bezogen worden. Dementsprechend unterscheiden Kunstgeschichte und Antiquitätenhandel dieses von anderem Zahnmaterial. Auch im Artenschutzrecht geht es bei dieser Bezeichnung um die Stoßzähne der Elefanten, insbesondere des afrikanischen Elefanten.

Während der längsten Zeitspanne der Menschheitsgeschichte, der Steinzeit, diente die Jagd allein dem Nahrungserwerb. Bei den unverdaulichen Teilen (Felle, Häute, Horn, Knochen, Elfenbein) fand eine Resteverwertung statt. Eine Änderung trat ein mit dem Aufkommen von Ackerbau und Viehzucht und der Entstehung von Hochkulturen, vor allem im afro-asiatischen Raum. Elfenbein wurde Bestandteil dieser Kulturen und es begann die Jagd auf Elefanten allein des Elfenbeins wegen. Die Wehrhaftigkeit des Elefanten und die Gefahren der Jagd machten Elfenbein zu einem kostbaren Rohstoff, der später auch Eingang in die Kulturen der Griechen und Römer fand. Die Seltenheit des Materials über mehr als 2000 Jahre hinweg sicherte Elfenbein eine ähnliche Wertschätzung wie Gold.

Eine erste folgenschwere Änderung wurde vorbereitet durch die Übernahme des Elfenbeinhandels durch die Kolonialmächte England, die Niederlande und Portugal, die für ein Überangebot sorgten. Ende des 19. Jahrhunderts wurden jährlich über 800 Tonnen Elfenbein nach Europa eingeführt, was zur Verbilligung des Elfenbeins und zur industriellen Verarbeitung beitrug (Griffe aller Art, Klaviertasten, Gefäße, Schmuck, Knöpfe, Spielwürfel, Dominosteine, Billardkugeln). Hochrechnungen aus dem Jahr 1894 sprachen von 80.000 getöteten Tieren pro Jahr.

Die zweite, weitaus ernstere Entwicklung bei dem inzwischen enorm dezimierten Elefantenbestand betrifft die Gegenwart. Schätzungen zufolge sank der Bestand innerhalb von nur 30 Jahren (1979–2007) von 1,3 Millionen auf 500.000 bis 700.000. Ursache ist die ständig steigende Nachfrage aus Souvenirhandel und den zu Wohlstand gekommenen Mittelschichten der aufstrebenden Völker Asiens. Gegenstände aus Elfenbein werden als Statussymbole hochgeschätzt. Hier ein Umdenken herbeizuführen bzw. der Wilderei von Elefanten entgegenzuwirken, haben sich internationale Tierschutz- und Umweltorganisationen zur Aufgabe gemacht.

Elfenbein ist das Zahnbein der aus dem Oberkiefer herauswachsenden Stoßzähne. Da diese nicht dem Zerkleinern der Nahrung dienen, sind weder Zahnschmelz noch Zahnwurzel vorhanden. Stoßzähne sind innen hohl (mit massiver Spitze) und bis zu einem gewissen Grade elastisch. Sie dienen als Waffe, die lebenslang stetig nachwächst.
Beim Elfenbein des Elefanten handelt es sich um ein relativ weiches Material, das sich mit spanenden Werkzeugen leicht bearbeiten lässt (siehe Artikel Elfenbeinschnitzerei). Die Farbe ist ein warmes Weiß mit Abstufungen, Farbabweichungen sind selten. Als besonders wertvoll gilt gleichmäßig helles Elfenbein.

Die Härte von Elfenbein nach der von 1 bis 10 reichenden Mohs-Skala wird in der Literatur mit 2 bis 3 angegeben, womit es etwa die Härte von Gold hat. Die Schwankungen ergeben sich aus dem Nahrungsangebot. Je mehr Mineralstoffe der Elefant zu sich nimmt, desto härter ist sein Stoßzahn. Die Dichte beträgt 1,7 bis 1,85 g/cm³ und liegt damit zwischen den Werten von Knochen und Leichtbeton. Elfenbein besteht zu etwa 56–59 % aus Calciumphosphat (Hauptbestandteil des Dentins) und einem geringen Anteil Calciumcarbonat („Kalk“). Die verbindende Substanz ist eine knorpelähnliche organische Masse, in die Wasser eingelagert ist. Beim Trocknen verliert Elfenbein rund 20 % an Gewicht. Die Trocknung muss schonend erfolgen, um Rissbildung zu vermeiden. Elfenbein wird durch kochendes Wasser biegsam und lässt sich verformen. Es kann gefärbt und gebleicht werden, verliert jedoch nicht die Neigung zu vergilben.

Mit bloßem Auge ist im Querschnitt – im Unterschied zur Knochensubstanz – eine netzartige Zeichnung (Retzius'sche oder Schregersche Linien) zu erkennen, umgangssprachlich auch als Maserung bezeichnet. Die unterschiedlichen Schnittwinkel der sich kreuzenden Linien ermöglichen eine Zuordnung nach Tierart. Unter dem Mikroskop und durch spektroskopische Verfahren kann zwischen Asiatischem Elefanten, Afrikanischem Steppenelefanten, Afrikanischem Waldelefanten und Mammut unterschieden werden. Auch kommt neuerdings hochauflösende Röntgen-Computer-Tomographie (HRXCT) zum Einsatz. Diese zerstörungsfreien Untersuchungsmethoden dienen der Identifizierung und Herkunftsbestimmung des Elfenbeins und sind damit Grundlage für zollamtliche Maßnahmen (siehe Abschnitt "Elfenbein und Artenschutz").

Eine weitere – allerdings teurere - Methode stellen DNA-Analysen dar, die jedoch wegen der systembedingten Materialentnahmen nicht für Antiquitäten geeignet sind. Mittels DNA-Analyse lässt sich sogar eine genauere lokale Herkunft ermitteln.

Elfenbein gehört wegen seiner Seltenheit und Schönheit seit alters her zu den kostbaren Rohstoffen, das in allen Kulturen als edles Material für kunstvoll gearbeitete Gegenstände mit höfischer, kultischer oder religiöser Bestimmung galt. Daneben fand es zu allen Zeiten auch Verwendung im profanen Bereich.

Heute hat Elfenbein als Rohstoff in Europa seit Jahrzehnten praktisch keine Bedeutung mehr. Zur Herstellung von Gebrauchsgegenständen kommen preiswerte Kunststoffe zum Einsatz, die in allen Bereichen bessere Dienste leisten. Im Kunsthandwerk wird seit langem auf Elfenbein verzichtet. Ausnahme ist die Restaurierung antiker Stücke, wofür Material aus legalen Altbeständen verwendet werden kann. Der Hauptrohstoff der heutigen Elfenbeinschnitzer ist fossiles Mammutelfenbein. Hierfür bestehen keine Handelsverbote.

Im Gegensatz dazu gilt Elefanten-Elfenbein mit einem geschätzten Schwarzmarktpreis von 2.200 Euro pro Kilogramm (2013) vor allem in China als prestigeträchtiges Material für Luxusgegenstände wie beispielsweise ein innen und außen mit Elfenbein verkleidetes Nobelauto.

Der Elfenbeinhandel ist Teil der beispiellosen Ausplünderung eines ganzen Erdteils, wobei auch einst der Mensch zur Ware gemacht wurde (Sklavenhandel). Der Kampf um die Rohstoffe Afrikas, der von den europäischen Kolonialmächten einst entfacht wurde und heute mit weiteren Beteiligten noch im Gange ist, wird unter anderem als Ursache für die Destabilisierung in manchen Regionen und die wachsende Zahl gescheiterter Staaten angesehen. Fehlende oder beschädigte staatliche Ordnung begünstigt den Raubbau und im Falle des Elfenbeins eine ausufernde Wilderei. Die unkontrollierte Ausbeutung verhinderte weitgehend die Entwicklung einer auf Nachhaltigkeit gerichteten Wirtschaftsweise und verlagerte die Wertschöpfung in andere Teile der Welt.

Der Elfenbeinhandel lag über Jahrhunderte hinweg in den Händen afrikanischer und arabischer Kaufleute. Archaische Jagdmethoden (Pfeil und Bogen, Speer, Fallgruben) und die Erschwernisse des Transportes (Trägerkolonnen, Einbäume) verhinderten eine Überjagung und setzten dem Handelsvolumen natürliche Grenzen. Das änderte sich mit dem Eintreffen der Europäer und ihrem technologischen Vorsprung im Schiffbau (Karavelle) und in der Waffentechnik (Feuerwaffen). Den ersten verhaltenen und friedlichen Schritten folgten bald Kolonisierung, Missionierung, Übernahme des Handels und Verlagerung der Handelsplätze. Haupthandelsplätze für Elfenbein wurden Amsterdam und London. Diesen Status verloren sie erst während des Zweiten Weltkrieges an Plätze in Ostasien, hier vor allem an die damalige britische Kronkolonie Hongkong.

Während der Zeit, als Europäer – hauptsächlich Großbritannien – den Elfenbeinhandel dominierten, gab es weder den Begriff der Wilderei, noch den des illegalen Handels. Alles, was die Kolonialmächte im Rahmen ihrer auf Bereicherung angelegten Unternehmungen taten, galt als legal, insbesondere die Überbejagung mit der starken Dezimierung der Elefantenbestände im 19. Jahrhundert. Erst durch den augenfälligen Schwund der Elefanten und die immer jünger werdende Jagdbeute – erkennbar an den kleineren Stoßzähnen - setzte ein Umdenken ein.

Nach dem Washingtoner Artenschutzübereinkommen (CITES), das 179 der 193 Mitgliedstaaten der Vereinten Nationen unterzeichnet haben (Stand 2013), ist der Handel mit Elfenbein eingeschränkt. Legal ist der Handel mit Elfenbein-Antiquitäten, die vor dem 1. Juni 1947 hergestellt worden sind, was von einem öffentlich anerkannten Gutachter für Artenschutz bescheinigt werden muss. Ferner gibt es Ausnahmeregelungen für einzelne Staaten. In Thailand ist der Handel mit Elfenbein gestattet, das von den eigenen 4.000 asiatischen Zuchtelefanten stammt. Ebenfalls unter Auflagen erlaubt ist der Elfenbeinhandel (seit 1999) den vier südafrikanischen Staaten Namibia, Botswana, Simbabwe und der Republik Südafrika, weil deren Elefantenpopulationen als stabilisiert angesehen werden.

Diese vier Länder durften 1999 und 2008 insgesamt 151 Tonnen Elfenbein an Händler aus Japan und China versteigern. Artenschutzorganisationen hatten vor diesen Verkäufen gewarnt, weil sie befürchteten, dass auf diesem Weg gewildertes Elfenbein in den Markt geschleust werden könnte, was nach Einschätzung von Beobachtern auch tatsächlich der Fall war.

Der illegale Handel wird durch die große Nachfrage und den Schmuggel von gewildertem Elfenbein in Gang gehalten. Vom Zoll unentdeckte Schmuggelware, die den Empfänger erreicht, kann verbotenerweise als legales Elfenbein deklariert und innerhalb des Empfängerlandes gehandelt und verarbeitet werden. Internationale Abkommen entfalten hier keine Wirkung. Als Haupthandelsplatz für illegales Elfenbein gilt Hongkong. Das Gesamtvolumen des Schwarzmarktes kann nur abgeschätzt werden. Man nimmt an, dass die Zollbehörden nur etwa jede zehnte Lieferung entdecken.

Die Kontrolle des generell geltenden Elfenbein-Handelsverbotes wird durch verschiedene Umstände erschwert bzw. unmöglich gemacht. Große formale Hindernisse bestehen darin, dass nicht alle Staaten das Artenschutzübereinkommen unterzeichnet haben und es keine Zwangsmittel zur Durchsetzung des Abkommens gibt. Die Praxis zeigt außerdem die Hilflosigkeit gegenüber der auf allen Ebenen verbreiteten Korruption. Im Handel ist es praktisch unmöglich, zwischen legalem und illegalem Elfenbein zu unterscheiden.

Der Begriff Wilderei entstammt dem Jagdrecht der europäischen Staaten. Im Zuge der Europäischen Expansion (Kolonialismus) wurden diese Normen auch Völkern mit grundlegend anderer Jagdauffassung und archaischen Jagdmethoden aufgezwungen, denen der Tatbestand der Wilderei fremd war.
Europäer betrachteten sich in weiten Teilen der Welt als Jagdherren, wobei Wildtiere als herrenlos galten und zum Vergnügen abgeschossen werden durften. Britische Gouverneure in Indien setzten sogar Belohnungen aus. Bis weit ins 20. Jahrhundert hinein war die Großwildjagd eine gesellschaftlich anerkannte Veranstaltung, die kaum einer Genehmigung bedurfte. Hohe Kosten für Ausstattung und Logistik verhinderten allerdings die schädliche Entwicklung zum Breitensport.

Erst das Ende des Kolonialismus nach dem Zweiten Weltkrieg und die Bildung unabhängiger Staaten brachten hier die Wende. An die Stelle von Rechtlosigkeit und Anarchie oder nationaler Regelungen traten im Laufe der Zeit immer mehr Abmachungen der Staatengemeinschaft und eine internationale Ächtung der Wilderei.

Die bisher umgesetzten Maßnahmen konnten aber nicht verhindern, dass die steigende Nachfrage nach Elfenbein hauptsächlich durch Wilderei und Schmuggel gedeckt wird. Schätzungen zufolge werden derzeit in Afrika zirka 38.000 Elefanten pro Jahr gewildert. Eine seit Jahren zu beobachtende, bedrohliche Entwicklung in Teilen Afrikas stellen die Aktivitäten der verschiedenen Rebellen- und Terrorgruppen dar, die stark bewaffnet als Wilderer auftreten und sich aus dem Elfenbeingeschäft finanzieren.

Dem Artenschutz, das heißt der Erhaltung der Artenvielfalt, kann auf verschiedene Weise gedient werden. Die älteste und bekannteste Maßnahme ist die Einrichtung von Schutzgebieten.
Zur Durchsetzung des Schutzgedankens werden Wildhüter eingesetzt, die anfangs unbewaffnet waren oder leichte Polizeiwaffen zur Selbstverteidigung hatten. Da jedoch Wilderer inzwischen bandenmäßig organisiert und stark bewaffnet auftreten, wurden die Einsatzkräfte mit Sturmgewehren und anderen automatischen Waffen ausgerüstet. Eine Steigerung im Kampf gegen die Wilderei stellen Hubschrauber, Überwachungs-Drohnen und Bluthunde dar.

Da die Weite der zu überwachenden Gebiete oft nur zufällige Erfolge zulässt, kommen zu den Bemühungen vor Ort Maßnahmen gegen Schmuggel an den Grenzen. Hierzu zählen vor allem die Zollkontrollen in Seehäfen und Flughäfen, und zwar sowohl in den Herkunftsländern als auch in den Empfängerländern – zum Teil mit Hilfe von Spürhunden. Das stetige Anwachsen der Zolllager und die Angst vor Diebstahl haben weltweit zu der Überzeugung geführt, dass beschlagnahmtes Elfenbein endgültig aus dem Verkehr gezogen werden muss.

Die medienwirksame, das heißt öffentliche Zerstörung von illegalem Elfenbein gilt als das eindeutige Null-Toleranz-Signal gegen Wilderei und Schmuggel. Seit 1989 konnten in spektakulären Aktionen etwa 180 Tonnen geschmuggelten Elfenbeins vernichtet werden. Den Anfang machte Kenia, das 12 Tonnen verbrannte. Weitere afrikanische Staaten (Sambia 10 Tonnen, Gabun 5 Tonnen) folgten dem Beispiel. Kenia übergab 2011 abermals fast 5 Tonnen Elfenbein den Flammen. Die bisher größte Menge Elfenbein (105 Tonnen) wurde von Kenia am 30. April 2016 verbrannt. Bis 2016 zerstörten folgende Staaten ihre beschlagnahmten Elfenbeinbestände: Philippinen 5 Tonnen, USA 6 Tonnen, China 6 Tonnen, Frankreich 3 Tonnen, Dubai 18 Tonnen (1992 und 2015), Kongo 5 Tonnen.

2008 verpflichteten sich 17 afrikanische Staaten mit der Elefanten-Deklaration von Bamako, den im Washingtoner Artenschutzabkommen erlaubten Handel mit beschlagnahmtem Elfenbein einzustellen.

Von IFAW und WWF in China durchgeführte Marktforschungsstudien ergaben die verbreitete Ansicht, Elefanten-Stoßzähne würden ähnlich dem Geweih vom lebenden Tier abgeworfen. Mit Aufklärungsarbeit hoffen die Artenschutz-Organisationen, in China eine ähnliche Entwicklung in Gang zu setzen wie in Japan 30 Jahre vorher. Japan stand seinerzeit mit 470 Tonnen pro Jahr an der Spitze des Weltverbrauchs an Elfenbein. Der heutige Verbrauch beträgt nicht mehr als ein Zehntel.

Unabhängig davon hatte die chinesische Regierung angekündigt, die Resolution der CITES-Artenschutzkonferenz vom Oktober 2016 umzusetzen und den Handel mit Elfenbein und seinen Produkten zu unterbinden. Nach einer zwölfmonatigen Übergangszeit trat am 31. Dezember 2017 ein generelles Handelsverbot in Kraft.

Als früheste Zeugnisse menschlichen Kunstschaffens gelten steinzeitliche Figuren aus Mammut-Elfenbein (siehe Abschnitt "Kunstgeschichte des Elfenbeins").

Nachdem die letzten Mammuts vor etwa 4000 Jahren ausgestorben waren, kommen die Stoßzähne nur noch in fossiler Form vor. Sie stammen hauptsächlich aus dem nördlichen Teil Sibiriens, wo sie während des arktischen Sommers ausgegraben werden, wenn der Permafrostboden auftaut und die Schätze freigibt. Eine systematische Gewinnung ist wegen der Größe des Landes nicht möglich. Eine mitunter gefährliche Suche wird an den Steilküsten der Polarmeere betrieben, an denen durch Erdabbrüche Stoßzähne freigelegt werden. Auch in Kanada und Alaska wird Mammut-Elfenbein gefunden.

Die Herkunft aus einer längst vergangenen Epoche der Menschheitsgeschichte macht Mammut-Elfenbein zu einem faszinierenden und einzigartigen Rohstoff. Mit seinen Verfärbungen findet es besonders in der modernen Schmuckherstellung Verwendung.

Die Farbpalette reicht von beige bis dunkelbraun, von blau bis grün in allen Nuancen, bis hin zu schwarz. Stoßzähne nehmen die Farben der Mineralien an, denen sie in der Erde ausgesetzt sind. Die drei Handelsklassen richten sich nach dem Verwitterungsgrad der Stoßzähne. Die Ausbeute bei gut erhaltenen Funden (Handelsklasse A) ist relativ hoch, da Mammut-Stoßzähne durchgehend massiv sind. Mammut-Elfenbein hat eine Dichte von 2 bis 2,2 g/cm³ und ist etwa ein Fünftel schwerer als Elefanten-Elfenbein. Die Schnitzqualität ist etwa gleich. Die Härte beträgt auf der Mohs-Skala zumeist 2,75 – 3,5 und entspricht der Härte von Gold.

Der Handel mit Mammut-Elfenbein ist seit Jahrhunderten belegt. Nach China wurde es bereits in der frühen Kaiserzeit geliefert und auch die Griechen der Antike kannten es, wie Theophrast berichtete. China ist auch heute (2014) der größte Importeur.

Solange Elefanten-Elfenbein frei verfügbar war, hatte das eiszeitliche Elfenbein auf Grund der Risse und Verfärbungen keinen großen Markt. Russland exportierte um 1900 lediglich 20 Tonnen pro Jahr. Die Nachfrage stieg erst, als die Handelsverbote für Elefanten-Elfenbein in Kraft traten. Seitdem beläuft sich der Export sibirischen Elfenbeins auf jährlich etwa 60 Tonnen. Der Handel unterliegt keinerlei Beschränkungen.

Im Mittelalter galt der Stoßzahn des Narwals wegen seiner Seltenheit und der rätselhaften Herkunft als kostbarster Stoff, der zeitweise mit dem zehnfachen Wert des Goldes aufgewogen wurde. Er beflügelte die Phantasie und wurde für das heilbringende Horn des sagenhaften Einhorns gehalten (siehe ausführliche Darstellung im Artikel Ainkhürn).

Die spiralartig gewundenen Stoßzähne des Narwals gelangten meist unzerteilt als bestaunte Stücke in die Raritäten-Sammlungen der europäischen Höfe. Einzelne Zähne wurden auch zu Insignien weltlicher und geistlicher Herrscher (Zepter, Bischofsstab, Thron) verarbeitet.

Ebenfalls wertvoll ist das Elfenbein der Walross-Eckzähne, die zeitlebens nachwachsen und eine Länge von 50 Zentimeter und mehr erreichen. Die intensive Bejagung seit dem 16. Jahrhundert führte zu starker Dezimierung bzw. gebietsweiser Ausrottung. Nach dem Washingtoner Artenschutzübereinkommen von 1973 ist die Jagd auf Walrosse und die Verwertung nur den arktischen Küstenvölkern gestattet, die ihre Lizenzen seit einigen Jahren aber auch an Hobbyjäger abtreten. Kunsthandwerkliche Arbeiten aus Walrosselfenbein haben eine lange Tradition und reichen etwa 2000 Jahre zurück (siehe auch Artikel Scrimshaw). Ebenfalls aus Walross-Elfenbein wurden Harpunenspitzen hergestellt.

Ein nie vergilbendes Elfenbein liefern die etwa 30 Zentimeter langen Eckzähne der Flusspferde. Aus ihnen wurden früher hauptsächlich künstliche Zähne hergestellt. Die ehemals im Nil beheimateten Flusspferde, auch "Nilpferde" genannt, waren bereits Anfang des 19. Jahrhunderts ausgerottet.

Bereits in der Steinzeit fertigten Menschen aus Elfenbein Gebrauchsgegenstände (Nadeln, Speerspitzen) und kleine Skulpturen. Die ältesten bisher gefundenen Kunstwerke sind Skulpturen aus Mammutelfenbein, wie etwa die Venus vom Hohlefels, der Löwenmensch, für die ein Alter von über 40 000 Jahren angenommen wird.

Aus Ägypten sind Grabbeigaben aus Elfenbein ab 4000 v. Chr. bekannt (Badari-Kultur).

In Mesopotamien und Syrien wurden Funde aus der Bronzezeit geborgen, wobei meist Eck- und Schneidezähne von Nilpferden Verwendung fanden. Schnitzereien und Reliefarbeiten, die als Intarsien in Holzobjekte oder Möbel eingesetzt waren, konnten an mehreren Fundorten wie Qatna, Ebla, Ugarit, Alalach gesichert werden.

Mit dem Aufstieg der Phönizier zur bedeutenden Handelsmacht im Mittelmeer (ab 1000 v. Chr.) gelangten die begehrten Elfenbeinarbeiten phönizischer Kunsthandwerker in viele Länder Europas und Vorderasiens. Nach der Ausrottung der damals auch in Syrien heimischen Elefanten wurde der Rohstoff unter anderem auf den Transsahara-Karawanenstraßen aus dem Innern Afrikas herangeschafft. Als berühmteste semitische Elfenbeinarbeit gilt der im Alten Testament beschriebene Thron des Salomo.

Eine einzigartige Verwendung fand Elfenbein bei der Gestaltung der Zeus-Statue in Olympia, eines der Sieben Weltwunder der Antike, die der griechische Bildhauer Phidias etwa 430 v. Chr. schuf. Die etwa zwölf Meter hohe Kolossalstatue ist nicht mehr erhalten. Ebenfalls von Phidias stammte die in gleicher Chryselephantin-Technik ausgeführte Statue der Athene für den Parthenon in Athen (Nachbildung s. Foto rechts). Auch aus dem archaischen Griechenland sind Gold-Elfenbein-Skulpturen überliefert (Foto links).

Bei den Römern erfreute sich Elfenbein als Werkstoff für Schmuck, Kleinkunst, Musikinstrumente, Intarsien und Möbelverzierungen großer Beliebtheit. In der Kaiserzeit gelangte Elfenbein bevorzugt bei den Konsulardiptychen zum Einsatz.

Elfenbein erfuhr mit dem Christentum einen Bedeutungszuwachs hin zum Inbegriff der Reinheit und wurde zum idealen Material für die sakralen Gegenstände (Behälter für Hostien und Reliquien, Kruzifixe, Triptychen, Bischofsstäbe, Buchdeckel für die heiligen Schriften). Die Elfenbeinkunst setzte sich über Karolinger und Ottonen mit ihren Klosterwerkstätten (Lorsch, St. Gallen, Reichenau, Echternach) fort und war im 11. und 12. Jahrhundert im christlichen Abendland allgemein verbreitet. Auch gelangten orientalische Schnitzarbeiten durch die Kreuzfahrer nach Europa und in den sakralen Gebrauch.

In der Gotik wurden Elfenbeinschnitzereien zunehmend für den Profangebrauch hergestellt, wobei französische und venezianische Werkstätten die Führung übernahmen. Eine Unterbrechung der Elfenbeintradition gab es in der nachfolgenden Renaissance, in der andere Materialien bevorzugt wurden. Zur eigentlichen Blüte gelangte die Elfenbeinschnitzerei im 17. Jahrhundert, als deutsche Fürsten miteinander wetteiferten, berühmte Künstler in ihre Dienste zu nehmen oder sich gar selbst als Elfenbeinschnitzer zu versuchen. Aus dieser Zeit des Barock stammen die vielen virtuos gearbeiteten Stücke der höfischen Sammlungen.
Der künstlerische Stillstand setzte mit dem Vordringen von Maschinen und den neuen Bearbeitungsmöglichkeiten (Passigdrehbank) ein. Damit einher ging die vermehrte Verwendung von Elfenbein für Gebrauchsgüter aller Art. Ein letztes Aufleuchten erlebte die Elfenbeinkunst als Kleinplastik im Jugendstil und in der Zeit des Art déco, insbesondere in der Gold-Elfenbein-Technik (Chryselephantin).

Das deutsche Zentrum der Elfenbeinschnitzerei war und ist Erbach im Odenwald, wo 1966 das Deutsche Elfenbeinmuseum eröffnet wurde. Die dortige Elfenbeinverarbeitung begründete 1783 Franz I., letzter regierender Graf von Erbach (1754–1823), worauf sich viele Künstler in dem Ort niederließen.

Bei Gebrauchsgütern haben Kunststoffe, insbesondere die formstabilen Kunstharze, Elfenbein völlig verdrängt. Gründe sind die leichte Verfügbarkeit der Ausgangsmaterialien mit ihrem günstigen Preis und die je nach Anforderung zu bestimmenden Eigenschaften der Kunststoffe. Damit sind sie Elfenbein in allen Einsatzbereichen überlegen.

Beispielsweise besitzen Billardkugeln aus Kunstharz eine größere Haltbarkeit und bessere Rolleigenschaften. Zu den Stoffen, die Elfenbein ersetzt haben, gehört auch das Porzellan, insbesondere die künstlerisch gestalteten Manufakturerzeugnisse aus Biskuitporzellan, das Mitte des 18. Jahrhunderts erfunden wurde.

Seit Mitte des 19. Jahrhunderts war die Steinnuss für etwa 100 Jahre ein beliebter Ersatz für Elfenbein. Die Steinnuss (Elfenbeinnuss) ist der Samen der Steinnusspalmen (Elfenbeinpalme) Südamerikas. Durch monatelange Trocknung erhält die Steinnuss die Härte von Knochen. Es hat die Farbe von sehr hellem Elfenbein und kann wie dieses bearbeitet und beliebig eingefärbt werden. Die Mitte der 1960er Jahre einsetzende Rückbesinnung auf Naturprodukte und Nachhaltigkeit hat zur Wiederentdeckung der Steinnuss geführt. Aus ihr werden Skulpturen, Spielsteine, Schachfiguren, Knöpfe und vieles andere gefertigt.



Kunstgeschichte:





</doc>
<doc id="1354" url="https://de.wikipedia.org/wiki?curid=1354" title="European Article Number">
European Article Number

Die European Article Number (EAN) ist die frühere (2009 abgelöste) Bezeichnung für die Globale Artikelidentifikationsnummer (Global Trade Item Number, abgekürzt GTIN). Sie stellt eine international unverwechselbare Produktkennzeichnung für Handelsartikel dar. Die Nummer besteht aus 8 bzw. 13 Ziffern, von denen die ersten 2 oder 3 bzw. 7, 8 oder 9 Ziffern zentral durch die GS1-Gruppe verwaltet und an Hersteller auf Antrag als Global Location Number vergeben werden. In Deutschland fallen für die Vergabe Lizenzgebühren an die GS1 Germany an.

Die EAN/GTIN wird in der Regel als maschinenlesbarer Strichcode auf die Warenpackung aufgedruckt und kann von Barcodescannern decodiert werden, beispielsweise an Scannerkassen.

Ein häufiger Verständnisfehler ist die Bezeichnung des GS1-Präfixes als Ländercode. Es ist lediglich ein Präfix, das den lokalen GS1-Organisationen zugeordnet wird. Die GS1-Kunden, die eine Firmennummer bei GS1 kaufen, erhalten immer eine Firmenkennung, die aus dem GS1-Präfix mit der dahinter gestellten Firmennummer besteht. Ein GS1-Kunde kann seine Firmennummer von einem beliebigen GS1-Standort kaufen und dann wieder in vielen anderen Ländern produzieren. (Siehe GS1 General Specification Kapitel 2.1.2.1.1. Zitat: „The GS1 Company Prefix is allocated by a GS1 Member Organisation to a system user. It makes the ID number unique worldwide but does not identify the origin of the item. GS1 Company Prefixes starting with GS1 Prefixes 000 to 019, 030 to 039, 060 to 099, 100 to 139, 300 to 969, or 977 to 979 in the first three digits are used in this Element String.“)

Des Weiteren wird die Nummer (GTIN bzw. EAN) oft mit dem Barcodetyp EAN (ISO/IEC 15420) gleichgesetzt. Tatsächlich ist das eine der Barcode als Datenträger, der eine Nummer kodiert und transportiert, und die Nummer ist die GTIN bzw. eine andere der unten beschriebenen Varianten. Weitere Codes, die die GTIN kodieren können, sind beispielsweise GS1-128, GS1-DataMatrix und ITF-14.

Bereits 1973 wurde in den USA der "Universal Product Code" UPC mit 12-stelligen Nummern eingeführt. Ein Jahr später machte man sich in Europa die ersten Gedanken über ein ähnliches System, das zum UPC kompatibel sein sollte. Dazu wurde UPC um eine führende Ziffer auf dreizehn Stellen ergänzt. Ein UPC-A-Code wird zu einem EAN-Code, indem man eine führende Null hinzufügt.

1977 wurde die "European Article Association" gegründet, die später in "EAN International" umbenannt wurde und seit 2004 unter dem Namen "GS1 Global" läuft. Sie hat Mitgliedsorganisationen in über einhundert Staaten und umfasst auch den amerikanischen Uniform Product Code vom "Uniform Code Council" (heute GS1 US).

Zum 1. Januar 2005 wurden de facto die EAN-13 auch in Nordamerika eingeführt, doch wurde gleichzeitig ein Übergang auf die 14-stelligen GTIN empfohlen (siehe Weblinks).

Zur Strategie der EAN/GTIN gehört die durchgängige Verwendung von Techniken der Automatischen Identifikation und Datenerfassung. Genutzt werden konsistente Verfahren für den Datenaustausch über EAN128 zwischen den beteiligten Unternehmen sowie die elektronische Datenübermittlung per EDI, wozu der EANCOM-Standard entwickelt wurde. Aktuell wird der Standard auch im Elektronischen Produktcode weiterentwickelt.


Die 13 Ziffern der Globalen Artikelidentnummer (ehemals EAN-13, heute GTIN) bedeuten:

In Deutschland werden seit dem 1. Januar 2001 7-, 8- und 9-stellige Basisnummern vergeben.

Präfixe sind nicht „sprechende“ Bestandteile wie „Herkunftskennzeichen“. Es handelt sich um Nummernkreise der jeweiligen GS1-Mitgliedsgesellschaften. Dabei gibt es auch gegenseitige Abtretungen von Teilserien oder die exterritoriale Teilnahme von Herstellern. Eine von den übrigen Stellen der Nummer losgelöste Verarbeitung kann daher zu Missverständnissen führen. Betriebe können eine Global Location Number lizenzieren, die von der zuständigen GS1-Mitgliedsgesellschaft einmalig vergeben und verwaltet wird. Die Artikelnummer ist für jeden Mitgliedsbetrieb frei verfügbar. Je nach Firmensitz bzw. Zulieferfirma und zuständiger GS1-Mitgliedsgesellschaft weisen daher die Präfixe nicht auf das Herstellerland hin.

Die Prüfziffer dient der Datensicherheit und wird aus der gewichteten Quersumme abgeleitet (siehe unten).

Die verkürzte Version "EAN-8" ist speziell für kleine Artikel gedacht, auf denen eine EAN-13 mehr als 25 % des Platzes auf der Vorderseite benötigen würde. Sie hat folgenden Aufbau:

Eine EAN-8 / GTIN-Kurznummer ist in der Regel extra bei der lokalen GS1-Organisation zu beantragen. Eine EAN-8 mit der Startziffer 2 kann lizenzfrei innerhalb der eigenen Organisation verwendet werden, sie ist aber nicht weltweit eindeutig. Jede andere EAN-8 muss unter Beifügung eines Musters des Artikels "einzeln" beantragt und bezahlt werden.

Beispiele:

Die 13-stellige EAN-13/GTIN-13 nach dem System GS1 wird wie folgt erzeugt:

"Beispiele" (der fettgedruckte Teil bleibt jeweils gleich):
Auch ist analog die Einbettung der ISMN "(International Standard Music Number)" für gedruckte Noten möglich.

Die ISBN-13 und der GS1-Pressecode eines Buches sind (bis auf die Notation ohne bzw. mit Bindestrichen) identisch. Eine EAN-13 für Bücher wird auch „Bookland“-Nummer genannt. Die 4. bis maximal 8. Ziffer ist demnach (entsprechend der 1. bis maximal 5. Ziffer der ISBN) ein Code für den Sprachraum – zum Beispiel 3 für Deutschland, 57 für Dänemark oder 99953 für Paraguay.

Manche Hersteller von Software und Multimediaprodukten vergeben für ihre Produkte gleich mehrere EANs.

Darüber hinaus ist eine Erweiterung des GS1-Pressecodes möglich, ein "AddOn"-Code (Zusatzidentifikation) von 2 oder 5 Ziffern. Im AddOn können zum Beispiel Preise, Ausgabenvariante oder der Monat von Zeitschriften codiert werden. Im EAN-13/UPC-Symbols (Barcode) werden diese verkleinert nachgestellt.

In Deutschland verkaufte Zeitschriften werden nicht mit dem entsprechenden GS1-Pressecode (ISSN-Einbettung) gekennzeichnet. Sie erhalten stattdessen eine Standard-GTIN-13 mit folgendem Aufbau:

Die so gebildete GTIN-13 enthält dadurch zwar nicht mehr die gesamte ISSN, dafür jedoch den Preis, der aufgrund der Buchpreisbindung überall in Deutschland gleich ist. Dies ist insbesondere vorteilhaft für kleinere Verkaufsstellen, die kein Warenwirtschaftssystem haben, mit dem der GS1-Pressecode dem Preis zugeordnet werden kann.

Seit 1. Januar 2012 werden abweichend die Ländercodes 439 bzw. 434 für Zeitschriften verwendet, bei denen dem Heft ein Datenträger mit einer FSK- oder USK-Altersbeschränkung beiliegt (meist DVDs mit Filmen oder Computerspielen). Dies ermöglicht es Scannerkassen, den Verkäufer aufzufordern, das Alter des Käufers zu überprüfen.

Durch einen 2- oder 5-stelligen "AddOn"-Code werden die Folgennummer und ggf. die Ausgabenvariante des Heftes codiert, bei Tageszeitungen außerdem der Wochentag.

Stylenummernüberschneidungen treten vor allem auf, wenn Produkte Lebenszyklen unterliegen. Nach dem Durchlauf eines Zyklus werden häufig die Artikelnummern des vorherigen Zyklus verwendet. Ein Zyklus dauert in der Regel zwei Jahre und wird in Saisons unterteilt. Die Saisons werden durch Buchstaben unterschieden, wodurch es eine maximale Anzahl an Saisons gibt. Zudem werden in den Artikelnummern, welche nur fünfstellig sind, auch Division- und Class-Zugehörigkeiten versteckt, was den Wiederholungseffekt noch verstärken könnte.

Für "Supermärkte" oder andere Einzelhändler steht ein spezielles GS1-Präfix zur Verfügung. Es findet ausschließlich intern Verwendung und dient beispielsweise dazu, die vor Ort abgewogenen Lebensmittel mit einem Barcode versehen zu können.
Dieser Code wird vor allem für Obst und Gemüse sowie Fleisch- und Wurstwaren verwendet. Außerdem benutzen verschiedene Lebensmittel-Discounter, wie zum Beispiel ALDI, diese geschäftsinterne Instore-Artikelnummer in der verkürzten Form.

Beginnt ein EAN mit einem Ländercode Japans (450-459,490-499), dann nennt man diesen EAN auch JAN. Es gibt keine weiteren Unterschiede.
Grundlage ist hier JIS-X-0501 (JIS: Japanese Industrial Standards).

Die Code-Familien UPC-A, EAN-8 und EAN-13 benutzen alle die gleiche Codierung. Die codierte Information wird durch eine numerische Klarschriftangabe unter dem Barcode wiederholt.

Der gesamte Code besteht aus 95 gleich breiten Bereichen (zwei Randmarker à 3 Bit, ein Mittenmarker à 5 Bit, 12 Ziffern à 7 Bit: 2×3 + 5 + 12×7 = 95). Jeder dieser Bereiche kann schwarz (steht für 1) oder weiß (steht für 0) sein. Es folgen maximal vier schwarze Bereiche aufeinander, diese bilden zusammen eine "Linie". Ebenso folgen maximal vier weiße Bereiche aufeinander und bilden zusammen einen "Freiraum". Neben den Bereichen, die die Ziffern codieren, gibt es drei Bereiche, die Besonderes kodieren: Die Folge 101 am Beginn und Ende des Codes (Randzeichen) sowie die Folge 01010 in der Mitte des Codes (Trennzeichen).


In der folgenden Tabelle ist die entsprechende Zuordnung aufgelistet.

Für jede Ziffer gibt es drei einander sehr ähnliche Codes: "links ungerade (LU)", "links gerade (LG)" und "rechts gerade (RG)":


Der vollständige EAN-13 lautet daher: 4 003994 155486.

Durch das Trennzeichen in der Mitte ist es für einen einfachen Linienscanner ausreichend, jeweils nur eine Hälfte des Codes zu erfassen. Dies ermöglicht eine Coderekonstruktion durch die Leseeinrichtung bei einer Schrägabtastung der Codierung bis zu einem Winkel von etwa 45°.

Die GTIN-13 / EAN-13 kodiert ein dreizehntes Zeichen durch die Wahl von "gerade" und "ungerade" im linken Teil des Codes.

Die GTIN-12 / UPC-A kodiert nur 12 Ziffern, weil im linken Teil des Codes immer alle "ungerade" gewählt werden.

Die Prüfziffer der GTIN (ehem. EAN) ist die letzte Ziffer formula_1. Sie errechnet sich, indem die einzelnen Ziffern von rechts nach links, beginnend mit der vorletzten (formula_2), abwechselnd mit 3 und 1 multipliziert und anschließend diese Produkte addiert werden:

formula_3

Die Prüfziffer formula_4 ergänzt diese Summe dann zum nächsten Vielfachen von 10.

Die Probe hierzu: formula_5

Dasselbe Verfahren ist auch für andere Produkt-Kennzahlen üblich. Ein Prüfziffernrechner hierzu ist im Internet zu finden → siehe Weblinks.

"Vereinfachte Berechnung für 13-stellige EANs:"

Die Summe aller ungeraden Stellen plus dreimal die Summe aller geraden Stellen muss durch 10 teilbar sein.

Das nächste Vielfache von 10 ist 90, die Prüfziffer damit 90 − 89 = 1

"alternativ für 13-stellige EANs:"

Der folgende Java-Code setzt die Berechnung der Prüfziffer nach der obigen Beschreibung um: Die Berechnung startet mit der letzten Ziffer und beginnt die Multiplikation mit 3. Nach der Summenbildung der Produkte wird die Prüfziffer zurückgegeben.
public int calculateCheckDigit(int[] digits) {

Besonders schnell können die Prüfziffern durch den Datentyp Integer berechnet werden. Da bei der Berechnung der EAN-Prüfziffern die Gesamtsumme 216 nicht überschritten werden kann, können vier Bytes parallel berechnet werden; somit kann die Effizienz zur Berechnung der Prüfziffer um den Faktor vier erhöht werden.

int calc_ean13(char* ean12) {

In einem Tabellenkalkulationsprogramm kann die Prüfziffer mithilfe folgender Formel berechnet werden; die EAN-13 ohne Prüfziffer (12 Ziffern) befindet sich dabei in Zelle A1:

codice_1

oder kürzer:

codice_2

Während ein GTIN (ehemals EAN) einen Artikel nur der Art nach identifiziert (zum Beispiel Cola-Dose 0,33 L), kann über einen EPC durch einen zusätzlichen serialisierten Nummernteil jeder einzelne Artikel unterschieden werden (z. B. könnte jede Cola-Dose von jeder anderen unterschieden werden). Im EPC sind in der Regel die GS1-Nummernsysteme wie GTIN (ehem. EAN) für Artikel, NVE (SSCC) für Transporteinheiten und GRAI für Mehrwegtransportbehältnisse verschlüsselt. Um festzustellen, welches Nummernsystem im EPC verschlüsselt ist, enthält er hierzu zusätzlich einen Header. Über diesen Header kann dann vom RFID-Schreib-/Lesegerät gezielt auf bestimmte EPC zugegriffen werden. Die für den EPC zulässige RFID-Technologie wurde wie der EPC selbst von EPCglobal standardisiert. Nach dem Standard EPC Gen 2, der auch als ISO 18000-6 Teil C veröffentlicht wurde, sind nur solche RFID-Transponder für die Speicherung eines EPC zugelassen, die im Frequenzbereich um 900 MHz (UHF) arbeiten.



Datenbank-Abfrage-Tools

Rechner, Generatoren, Erkennungs-Software


</doc>
<doc id="1356" url="https://de.wikipedia.org/wiki?curid=1356" title="Erdbohne">
Erdbohne

Die Erdbohne ("Macrotyloma geocarpum"), auch Kandelabohne genannt, ist eine Pflanzenart aus der Gattung ("Macrotyloma") in der Unterfamilie Schmetterlingsblütler (Faboideae) innerhalb der Familie der Hülsenfrüchtler (Fabaceae). Diese Nutzpflanze ist nahe verwandt mit einer Reihe anderer „Bohnen“ genannter Feldfrüchte, insbesondere der indischen Pferdebohne ("Macrotyloma uniflorum"). Die Erdbohne hat ihren Ursprung wahrscheinlich in den Savannen Westafrikas und wird immer seltener, gewöhnlich für den Eigenbedarf, angebaut.

"Macrotyloma geocarpum" wächst als einjährige krautige Pflanze und erreicht Wuchshöhen von 25 bis 30 Zentimetern. Wenige Tage nach der Aussaat erfolgt epigäisch die Keimung. Es wird eine kräftige Pfahlwurzel gebildet. Die behaarten Stängel können selbständig aufrecht wachsen oder als sogenannte „Runnertypen“ sich niederliegend weit ausbreiten. Niederliegende Stängel bilden an den Knoten (Nodien) viele Wurzeln an denen auch Wurzelknöllchen vorhanden sind.

Schnell fallen die Keimblätter (Kotyledonen) ab und es werden ein oder zwei Paar lanzettliche Primärblätter gebildet. Die wechselständigen angeordneten Laubblätter sind Blattstiel und Blattspreite gegliedert. Die relativ langen Blattstiele stehen aufrecht. Die unpaarig gefiederten Blattspreiten besitzen nur drei Fiederblätter. Die Fiederblätter sind eiförmige bis lanzettlich.

Schon vier bis sechs Wochen nach der Aussaat werden in den Blattachseln die ersten kleinen Blütenstände gebildet. Der mehr oder weniger behaarte Blütenstandsschaft endet in einer Verdickung. In einem Blütenstand stehen nur zwei kurz gestielte Blüten zusammen. Die zwittrigen Blüten sind zygomorph und fünfzählig mit doppelter Blütenhülle. Die fünf Kelchblätter sind zu einem kurzen sowie breiten Kelch verwachsen, der in fünf ungleichen Kelchzähnen endet. Die fünf Kronblätter sind hellgelb oder elfenbeinfarben. Der auf der Innenseite im oberen Bereich behaarte Griffel endet spitz und die Narbe befindet sich seitlich unter seinem spitzen Ende. Es erfolgt Selbstbefruchtung und etwa zwei Tage danach verlängern sich die Blütenstiele geotrop nach unten bis der Fruchtknoten den Boden erreicht und 1 bis 2 Zentimeter ins Erdreich eindringt.

Oberirdisch liegende Hülsenfrüchte bleiben lange grün. Unterirsche Hülsenfrüchte sind von bleicher Farbe und bleiben bis zur Reife pergamentartig dünn. Die Früchte reifen im Boden, es handelt sich also um eine geokarpe = bodenfrüchtige Leguminose, wie die Erdnuss ("Arachis hypogaea") oder die Bambara-Erdnuss ("Vigna subterranea"), daher auch die Trivialnamen wie Erdbohne und das Artepitheton "geocarpum". Die unterirdischen, dünnschaligen, bleichen Hülsenfrüchte bestehen aus zwei bis drei Gliedern oder Segmente und sind 1,5 bis 3,0 Zentimeter lang. In jedem Segment ist ein Samen enthalten, also befinden sich in jeder Hülsenfrucht zwei bis drei Samen. Die Samen sind bei einer Länge von 8 bis 12 Millimeter, einer Breite von 4 bis 7 Millimeter und einer Dicke 2 bis 3 Millimeter bohnenartig bis eiförmig und leicht abgeflacht. Die Samenschale ist einfarbig weiß bis braun oder schwarz oder gesprenkelt. Der eingesenkte, punktförmige Nabel ist hell und von einem dreieckigen dunkelfarbigen Fleck umrandet. Das Tausendkorngewicht beträgt zwischen 50 und 150 g. Etwa sechs bis neun Wochen nach der Befruchtung erfolgt die Samenreife.

Die Chromosomenzahl beträgt 2n = 20.

Die Erdbohne ist eine tropische Pflanze mit hohem Temperatur- und Wasserbedarf.

Die Erdbohne gilt als wohlschmeckend und wird oft in unreifer Form geerntet. Der Eiweißgehalt der Samen (20 Prozent) ist relativ hoch, bei einem nur geringen Fettgehalt (kleiner als 2 Prozent).

Die Samen werden grün oder im reifen Zustand gegessen. Wegen ihres angenehmen Geschmackes wird die Erdbohne in manchen Gebieten gegenüber der Erdnuss oder Bambara-Erdnuss bevorzugt. Die reifen Samen werden häufig in einem Holzmörser aus den Hülsenfrüchten geschält und durch Windsichtern (Worfeln) das Korn gereinigt. Unbeschädigte Körner können mit Holzasche vermischt über 2 Jahre gelagert werden, ohne ihre Keimfähigkeit zu verlieren. Gekochte grüne oder reife Samen werden als Brei gegessen. Aus den Samen wird Mehl gemahlen, das der Zubereitung von Gebäck und anderen Gerichten dient. Die Samen der Erdbohne werden mit Salz geröstet.

Die Erdbohne besitzt einen guten Nährwert mit hohen Rohproteingehalt. Wenn die Mutter gestorben ist erhalten die Kinder der Sisaala in Ghana während der Trauerzeit die Erdbohne als einzige Nahrung.

Die Erdbohne wird in der Volksmedizin verwendet. Ein Sud aus den Samen wird gegen Durchfall eingesetzt. Als Brechmittel bei Vergiftungen dienen zerstoßene Samen mit Wasser oder lokalem Bier gemischt.

Alle Pflanzenteile können als Viehfutter verwendet, beispielsweise auf dem Feld abgeweidet werden.

Das Ursprungsgebiet der Erdbohne liegt in den wechselfeuchten bis halbtrockenen Savannengebieten Westafrikas. Der Anbau findet fast von Senegal bis ins nördliche Nigeria statt, mit einem Schwerpunkt im nördlichen Ghana. Von einem Anbau in Madagaskar wird berichtet. Es ist jedoch allgemein festzustellen, dass der Anbau zurück geht und hauptsächlich zur Eigenversorgung betrieben wird.

Samen der in Kamerun und der Zentralafrikanischen Republik vorkommenden Wildform wurden 2010 gesammelt und vom "Millennium Seed Bank Project" mit der Methode der „Ex-Situ-Konservation“ eingelagert. Der Standort der gesammelten Exemplare liegt in trockener Baumsavanne mit "Combretum"-, "Boswellia"- sowie "Gardenia"-Arten. Das derzeitige Areal ist größer als 20000 km². In der Roten Liste der gefährdeten Arten der IUCN gilt "Macrotyloma geocarpum" als „Least Concern“ = „nicht gefährdet“, da die Bestände stabil sind.

Die Erstbeschreibung erfolgte 1908 durch Hermann August Theodor Harms unter dem Namen (Basionym) "Kerstingiella geocarpa" in "Berichte der Deutschen Botanischen Gesellschaft", Band 26 A, S. 230, Tafel 3. Die Neukombination zu "Macrotyloma geocarpum" wurde 1977 durch Robert Joseph Jean-Marie Maréchal und Jean C. Baudet in "Bulletin du Jardin Botanique National de Belgique", Band 47, 1–2, S. 50. veröffentlicht.

Maréchal & Baudet veröffentlichten 1977 zwei Varietäten:





</doc>
<doc id="1362" url="https://de.wikipedia.org/wiki?curid=1362" title="Early Bird">
Early Bird

Early Bird (engl. „früher Vogel“) bezeichnet: 



</doc>
<doc id="1363" url="https://de.wikipedia.org/wiki?curid=1363" title="Euripides">
Euripides

Euripides (; * 480 v. Chr. oder 485/484 v. Chr. auf Salamis; † 406 v. Chr. in Pella) ist einer der großen klassischen griechischen Dramatiker.

Euripides ist nach Aischylos und Sophokles der jüngste der drei großen griechischen Tragödiendichter. Von seinen etwa 90 Tragödien sind 18 erhalten. Außerdem ist eines seiner Satyrspiele überliefert. Mit seinen Stücken, vor allem "Medea", "Iphigenie in Aulis", "Elektra" und "Die Bakchen", ist Euripides einer der am meisten gespielten Dramatiker der Weltliteratur.

Vom Leben des Euripides ist wenig Sicheres überliefert. Einige Informationen ergeben sich aus einem Vorwort zu byzantinischen Euripides-Handschriften. Demnach war er der Sohn des Mnisarchos und der Kleito aus dem Binnen-Demos Phlya der attischen Phyle Kekropis. Während des zweiten Perserkriegs flohen seine Eltern 480 v. Chr. nach Salamis und so wurde er hier geboren. Er soll immer wieder nach Salamis zurückgekehrt sein um in einer Höhle zurückgezogen seine Dramen zu verfassen. Die Höhle des Euripides konnte 1997 im Süden der Insel identifiziert werden. Auch soll er Fackelträger bei den Riten des Apollon Zosterios gewesen sein. Zudem soll er bei Anaxagoras, Prodikos und Protagoras Vorträge gehört haben. Wichtige Lebensdaten ergeben sich vor allem durch seine Teilnahme an den öffentlichen Tragödienwettbewerben.
Euripides führte zwischen 455 und 408 v. Chr. regelmäßig im tragischen Agon zu Athen Tetralogien auf (eine Tragödien-Trilogie und ein Satyrspiel eher grotesken Charakters). Das erste aufgeführte Stück hieß "Die Peliaden" (verschollen), mit dem Euripides den dritten Platz belegte. Sein erster Sieg fällt in das Jahr 441 v. Chr. Im Jahre 428 v. Chr. siegte er mit dem erhalten gebliebenen "Der bekränzte Hippolytos", der die Bearbeitung eines einige Jahre zuvor aufgeführten und heftig kritisierten anderen Hippolytos-Stückes war. Insgesamt siegte er zu Lebzeiten viermal und mit einer postum aufgeführten Tetralogie, zu welcher das berühmte Stück "Die Bakchen" gehört.

Der Dichter war ein Freund des Sokrates, der – obwohl er kein Freund derartiger Veranstaltungen war – sogar bis zum Peiraius ging, wenn Euripides ein Stück dort aufführen ließ.

Kurz nach den Dionysien 408 v. Chr. folgte Euripides der Einladung des makedonischen Königs Archelaos I., in dessen Hauptstadt Pella er zu Frühjahrsbeginn 406 v. Chr. verstarb; der Sage nach wurde er in Bromiskos von wilden Hunden zerrissen. Diese Sage ist eher sinnbildlich zu verstehen als Umschreibung seines Werkes, in dem die dionysisch-eruptive Ekstase eine zentrale Rolle spielt.

Liste der verlorenen, erhaltenen oder fragmentarisch überlieferten Stücke des Euripides mit den überlieferten oder erschlossenen Aufführungsdaten.

Sophokles soll auf die Nachricht vom Tod des Euripides Trauergewänder angelegt haben; seine Schauspieler und Choristen traten unbekränzt auf. In Athen wurden ihm zu Ehren ein Kenotaph – ein (leeres) Erinnerungsgrabmal – errichtet und drei seiner nachgelassenen Stücke postum gekrönt.

Schon bald nach dem Tod des Euripides erkannte man seine überragende Bedeutung an, was sich unter anderem darin niederschlug, dass er während der gesamten Antike der am häufigsten aufgeführte und gelesene Tragiker war. Von besonderer Bedeutung ist sein Einfluss auf die Neue Komödie, insbesondere deren Hauptvertreter Menander.

Von den Großmeistern der athenischen Tragödie war Euripides der problematischste und modernste, was ihm Kritik einbrachte.

Aristophanes ist für ein von den grotesken Verzerrungen der Alten Komödie gekennzeichnetes Euripides-Bild verantwortlich, das bis in die Neuzeit bestimmend gewesen ist.

Eine kritische Auseinandersetzung von Christoph Martin Wieland mit Euripides ließ Johann Wolfgang Goethe zu seiner Farce "Götter, Helden und Wieland" hinreißen. Obgleich von Goethe darin lächerlich gemacht, zeigte Wieland doch Verständnis für Goethes Sturm und Drang und empfahl den Lesern seiner Zeitschrift sogar die Farce als gelungene Lektüre.


Übersichtsdarstellungen

Einführungen und Untersuchungen

Rezeption




</doc>
<doc id="1364" url="https://de.wikipedia.org/wiki?curid=1364" title="Erika Fuchs">
Erika Fuchs

Johanna Theodolinde Erika Fuchs, geborene Petri (* 7. Dezember 1906 in Rostock; † 22. April 2005 in München), war eine deutsche Übersetzerin. Von 1951 bis 1988 übersetzte sie für das deutsche Micky-Maus-Heft.

Erika Fuchs war das zweite der insgesamt sechs Kinder von Auguste geb. Horn (1878–1964) und August Petri (1873–1954). Auguste Horn stammte aus München, war ausgebildete Sängerin, arbeitete als Volksschullehrerin und hatte in Augsburg unterrichtet. Sie lernte den aus dem Land Lippe-Detmold stammenden August Petri in einem Studentencorps kennen, wo man gegenüber der emanzipierten Auguste leichte Vorbehalte hatte. Bald nach Erikas Geburt zog die Familie in das damalige Reichenbach in Schlesien, von dort im Jahr 1912 nach Belgard an der Persante. August Petri hatte dort den Posten eines Direktors der Überlandwerke für Hinterpommern inne. Sein Beruf erlaubte es der Familie, in einigem Wohlstand zu leben; so besaßen die Petris das einzige Auto im Ort, und die Kinder wuchsen in einem großen Haus mit Dienstboten auf. Zum Personal der Familie gehörten ein Kinder- und ein Stubenmädchen; auch eine Köchin und ein Gärtner arbeiteten im Haushalt.

Von ihrem Vater wurden die Kinder sehr streng erzogen; Erika Fuchs berichtet später: „Bei uns daheim wurde nicht argumentiert und nicht ausdiskutiert. Da wurde befohlen und gehorcht“; da die sechs Kinder aber altersmäßig nur neun Jahre auseinanderlagen, führten sie ein ziemlich eigenständiges und ungebundenes Leben, über das Frau Fuchs sagt: „Jedenfalls hatten wir einen ganz ungeheuren Auslauf.“

Im Elternhaus spielte Musik eine wichtige Rolle, die Mutter hatte regelmäßig Gäste zu Besuch, die sie beim Gesang begleiteten, und auch bei der Haus- und Küchenarbeit wurde gern gesungen.

Erika Fuchs besuchte drei Jahre die Volksschule in Belgard, danach ab Ostern 1913 die Höhere Töchterschule, über die sie urteilte: „Wir trieben viel Unsinn und lernten wenig. Vom geistigen Reichtum in der Welt erfuhren wir erst, als wir eine richtige Studienrätin für Deutsch und Geschichte bekamen.“ Diese Lehrerin lud die Schülerinnen regelmäßig zu sich nach Hause ein und machte sie mit den Werken bedeutender Künstler bekannt. Begeistert von dem, was sie bei ihrer Lehrerin an Wissenswertem erfuhren, das nicht in der Schule gelehrt wurde, beschlossen Erika Fuchs und ihre Freundin Asta Hampe im Jahr 1921, dass sie das Gymnasium besuchen wollten. Erikas Vater unterstützte sie in diesem Ansinnen, allerdings gab es in Belgard kein Mädchengymnasium, so dass eine Abstimmung im Stadtrat nötig wurde, um den beiden Mädchen den Besuch des Knabengymnasiums zu ermöglichen. Der Stadtrat stimmte zu und Erika und ihre Freundin wurden zunächst für ein Jahr vom Schulunterricht freigestellt, um den nötigen Lehrstoff in Griechisch und Latein nachzuholen. Da Asta Hampe nach Hamburg umzog, war Erika Fuchs schließlich das erste Mädchen, das im örtlichen Knabengymnasium den Unterricht besuchte. Im Jahr 1926 schloss sie ihre Schulzeit mit dem Abitur ab.

Anschließend studierte sie Kunstgeschichte im Hauptfach, daneben Archäologie und mittelalterliche Geschichte. Im ersten Sommersemester war sie in Lausanne, das Wintersemester 1926/27 verbrachte sie in München, das dritte und vierte Semester in London, um dann von 1928 bis zum Examen im Wintersemester 1931/32 wieder in München zu studieren. Während des Studiums reiste sie viel ins Ausland; sie verbrachte einige Monate in Florenz und fuhr nach Holland, England, Italien und in die Schweiz. Am 17. Juli 1931 folgte die Promotion über den Barock-Bildhauer Johann Michael Feichtmayr (1709–1772) mit dem Titel "Johann Michael Feichtmayr: Ein Beitrag zur Geschichte des deutschen Rokoko". Ihre Arbeit, für die sie umfangreiche Recherchen in Kirchenarchiven durchgeführt hatte und die mit 160 eigenen Fotografien illustriert war, wurde mit magna cum laude bewertet. Sie erschien erst 1935 im Druck, noch unter dem Geburtsnamen.

Ihren Mann Günter Fuchs (1907–1984) hatte sie bereits während des Studiums kennengelernt. Er war Fabrikant und Erfinder, seit 1973 Honorarprofessor der Technischen Universität München, an der er „Technische Morphologie“ lehrte. Von 1931 bis 1984 leitete er das Unternehmen „Summa Feuerungen“, eine Fabrik für moderne Öfen. Sie heirateten im Jahr 1932 und hatten zwei Söhne. Seit 1933 lebte das Paar in Schwarzenbach an der Saale, Landkreis Hof. Günter Fuchs baute im Haus alles bis hin zu den Möbeln selbst; die Möbel befinden sich heute im Münchner Stadtmuseum.

Ging es um technische Dinge in den Comic-Geschichten, befragte Erika Fuchs ihren Mann: „Was er real und vernünftig macht, verwurschtle ich wieder, damit es ein bißchen verrückt wird“, erläuterte sie im Jahr 1978. Ihr Mann war aber nicht nur in technischen Dingen bewandert, er war auch ein Spezialist für Klassiker-Zitate.

Erika Fuchs starb am 22. April 2005 im Alter von 98 Jahren in München. Beigesetzt wurde sie neben ihrem 1984 verstorbenen Mann Günter Fuchs auf dem Friedhof von Schwarzenbach an der Saale.

Nach dem Zweiten Weltkrieg arbeitete sie als Übersetzerin, zuerst für die deutsche Ausgabe des Reader’s Digest, bevor sie weitere Anstellungen zum Übersetzen bei anderen amerikanischen Zeitschriften führten. 1951 schließlich wurde sie Chefredakteurin der neu gegründeten deutschen "Micky Maus", bei deren Gestaltung sie in den nachfolgenden Jahren viel Einfluss hatte. 1988 trat sie in den Ruhestand.

Bekannt wurde sie vor allem durch ihre Übersetzungen der amerikanischen Disney-Comics, insbesondere der Geschichten von Carl Barks rund um die Familie Duck. Ihre Übersetzungen enthielten – anders als die englischen Vorlagen – zahllose versteckte Zitate und literarische Anspielungen. So war sie als hervorragende Literaturkennerin der festen Überzeugung, man könne als Übersetzerin von Comics nicht gebildet genug sein. Die Nähe zur deutschen Klassik scheint etwa auf, wenn Tick, Trick und Track sich angelehnt an Schillers Version des Rütlischwurs versprechen: „Wir wollen sein ein einig Volk von Brüdern, in keiner Not uns waschen und Gefahr.“ Auch der fast immer als ihre Schöpfung bezeichnete Spruch „Dem Ingeniör ist nichts zu schwör“ ist keine eigene Erfindung, sondern eine Abwandlung der ersten Zeile des "Ingenieurlieds" von Heinrich Seidel (1842–1906), veröffentlicht 1889 im "Glockenspiel" („Dem Ingenieur ist nichts zu schwere“).

Für bildlich schwer Darstellbares verwendete Erika Fuchs durchgehend auf den Wortstamm verkürzte Verben (Inflektive), und zwar nicht nur für Geräusche (Onomatopoesie, zum Beispiel "schluck, stöhn, knarr, klimper"), sondern auch für lautlose (psychische) Vorgänge ("grübel, zitter"). Diese Verwendung der Inflektivform wird ihr zu Ehren auch als "Erikativ" bezeichnet. Dieser Ausdruck, zuerst scherzhaft in der Newsgroup "de.etc.sprache.deutsch" gebraucht, hat sich bereits ins englische Wiktionary verbreitet.

Der Einfluss Fuchs’scher Sprache auf den alltäglichen Sprachgebrauch und in der Popkultur ist bis heute enorm. Erika Fuchs hat in den Augen ihrer Bewunderer mit ihren eigenschöpferischen Übersetzungen eine eigene Welt erschaffen, die in der sogenannten „Donaldistik“ Ausdruck findet. Unter anderem im Feuilleton der Frankfurter Allgemeinen Zeitung erscheinen regelmäßig von Erika Fuchs stammende Donald-Duck-Zitate in schöngeistigem Zusammenhang – vornehmlich als Titelzeilen und Bildunterschriften.

1994 wurde sie mit der Morenhovener Lupe ausgezeichnet. 2001 erhielt sie den Sonderpreis zum Heimito von Doderer-Literaturpreis und den Roswitha-Preis der Stadt Bad Gandersheim.

Erika Fuchs war Ehrenmitglied der D.O.N.A.L.D. ("Deutsche Organisation nichtkommerzieller Anhänger des lauteren Donaldismus"). Mitglieder dieses Vereins, namentlich Patrick Bahners und Andreas Platthaus, schmückten zeitweise die Überschriften des ehrwürdigen FAZ-Feuilletons mit Fuchs'schen Sprachkleinodien, was sich aber oft nur dem Kenner erschloss.

Zudem wurde sie von dem Disney- und Carl-Barks-Verehrer Gottfried Helnwein 1991 für einen Gemälde-Zyklus ("48 Portraits", Öl und Acryl auf Leinwand, jedes Bild 70 × 55 cm) fotografiert und gemalt, der unter dem Motto „Die 48 bedeutendsten Frauen des Jahrhunderts“ ausgestellt wurde. Der Kunstsammler und Museumsgründer Peter Ludwig erwarb kurz darauf die "48 Portraits", die heute Bestandteil der Sammlung des Museums Ludwig in Köln sind. 

2005 widmete der deutsche Rockmusiker Farin Urlaub ihr sein zweites Soloalbum "Am Ende der Sonne".

Der Asteroid "(31175) Erikafuchs" wurde am 21. August 2013 nach ihr benannt.

Die Stadt Schwarzenbach an der Saale beschloss 2012 die Einrichtung eines Museums für Erika Fuchs. Das "Erika-Fuchs-Haus – Museum für Comic und Sprachkunst" wurde nach dreijähriger Bauzeit am 1. August 2015 eröffnet.





</doc>
<doc id="1365" url="https://de.wikipedia.org/wiki?curid=1365" title="Englische Sprache">
Englische Sprache

Die englische Sprache (Eigenbezeichnung: []) ist eine ursprünglich in England beheimatete germanische Sprache, die zum westgermanischen Zweig gehört. Sie entwickelte sich ab dem frühen Mittelalter durch Einwanderung nordseegermanischer Völker nach Britannien, darunter der Angeln – von denen sich der Name „Englisch“ herleitet – sowie der Sachsen. Die Frühformen der Sprache werden daher auch manchmal Angelsächsisch genannt.

Die am nächsten verwandten lebenden Sprachen sind die friesischen Sprachen und das Niederdeutsche auf dem Festland. Im Verlauf seiner Geschichte hat das Englische allerdings starke Sonderentwicklungen ausgebildet: Im Satzbau wechselte das Englische, im Gegensatz zu allen westgermanischen Verwandten auf dem Kontinent, in ein Subjekt-Verb-Objekt-Schema über und verlor die Verbzweiteigenschaft. Die Bildung von Wortformen (Flexion) bei Substantiven, Artikeln, Verben und Adjektiven wurde stark abgebaut. Im Wortschatz wurde das Englische in einer frühen Phase zunächst vom Sprachkontakt mit nordgermanischen Sprachen stark beeinflusst, der sich durch die zeitweilige Besetzung durch Dänen und Norweger im 9. Jahrhundert ergab. Später ergab sich nochmals eine starke Prägung durch den Kontakt mit dem Französischen aufgrund der normannischen Eroberung Englands 1066. Aufgrund der vielfältigen Einflüsse aus westgermanischen und nordgermanischen Sprachen, dem Französischen sowie den klassischen Sprachen besitzt das heutige Englisch einen außergewöhnlich umfangreichen Wortschatz.

Die englische Sprache wird mit dem lateinischen Alphabet geschrieben. Eine wesentliche Fixierung der Rechtschreibung erfolgte mit Aufkommen des Buchdrucks im 15./16. Jahrhundert, trotz gleichzeitig fortlaufenden Lautwandels. Die heutige Schreibung des Englischen stellt daher eine stark historische Orthographie dar, die von der Abbildung der tatsächlichen Lautgestalt vielfältig abweicht.

Ausgehend von seinem Entstehungsort England breitete sich das Englische über die gesamten Britischen Inseln aus und verdrängte allmählich die zuvor dort gesprochenen (v. a. keltischen) Sprachen. In seiner weiteren Geschichte ist das Englische vor allem infolge der Besiedlung Amerikas sowie der Kolonialpolitik Großbritanniens in Australien, Afrika und Indien zu einer Weltsprache geworden, die heute (global) weiter verbreitet ist als jede andere Sprache (die Sprache mit der größten Zahl an Muttersprachlern ist jedoch Mandarin-Chinesisch). Englischsprachige Länder und Gebiete bzw. ihre Bewohner werden auch anglophon genannt.

Das Englische wird in den Schulen vieler Länder als erste Fremdsprache gelehrt und ist offizielle Sprache der meisten internationalen Organisationen, wobei viele davon daneben noch andere offizielle Sprachen nutzen. In Westdeutschland verständigten sich die Länder 1955 im Düsseldorfer Abkommen darauf, an den Schulen Englisch generell als Pflichtfremdsprache einzuführen.

Heute sprechen weltweit etwa 330 Millionen Menschen Englisch als Muttersprache. Die Schätzungen zur Zahl der Zweitsprachler schwanken je nach Quelle massiv, da unterschiedliche Grade des Sprachverständnisses herangezogen werden. Hier finden sich Zahlen von unter 200 Millionen bis über 1 Milliarde Menschen.

Der englische Sprachraum:

Englisch ist Amtssprache in folgenden Staaten und Territorien:

Englisch ist zudem eine Amtssprache supranationaler Organisationen wie der Afrikanischen Union, der Organisation Amerikanischer Staaten, der UNASUR, der CARICOM, der SAARC, der ECO, der ASEAN, des Pazifischen Inselforums, der Europäischen Union, des Commonwealth of Nations und eine der sechs Amtssprachen der Vereinten Nationen.
Auch die Einführung von Englisch als Verwaltungs- und anschließend als Amtssprache in den Teilstaaten der Europäischen Union wird diskutiert. Einer repräsentativen YouGov-Umfrage von 2013 zufolge würden es 59 Prozent der Deutschen begrüßen, wenn die englische Sprache in der gesamten Europäischen Union den Status einer Amtssprache erlangen würde (zusätzlich zu den bisherigen Sprachen), in anderen Ländern Europas liegen die Zustimmungsraten teilweise bei über 60 Prozent.

Die englische Sprache dient zudem als Verkehrs-, Handels-, Geschäfts- oder Bildungssprache unterschiedlich intensiv in folgenden Ländern und Regionen:

Das Englische gehört zu den indogermanischen Sprachen, die ursprünglich sehr stark flektierende Merkmale aufwiesen. Alle indogermanischen Sprachen weisen diese Charakteristik bis heute mehr oder weniger auf. Allerdings besteht in allen diesen Sprachen eine mehr oder weniger starke Neigung von flektierenden zu isolierenden Formen. Im Englischen war diese Tendenz bislang besonders stark ausgeprägt. Heute trägt die englische Sprache überwiegend isolierende Züge und ähnelt strukturell teilweise eher isolierenden Sprachen wie dem Chinesischen als den genetisch eng verwandten Sprachen wie dem Deutschen.

Zudem hat sich die englische Sprache heute durch die globale Verbreitung in viele Varianten aufgeteilt. Viele europäische Sprachen bilden auch völlig neue Begriffe auf Basis der englischen Sprache (Anglizismen, Scheinanglizismen). Auch in einigen Fachsprachen werden die Termini von Anglizismen geprägt, vor allem in stark globalisierten Bereichen wie z. B. Informatik oder Wirtschaft.

Der Sprachcode ist codice_1 oder codice_2 (nach ISO 639-1 bzw. 2). Der Code für Altenglisch bzw. Angelsächsisch (etwa die Jahre 450 bis 1100 n. Chr.) ist codice_3, jener für Mittelenglisch (etwa 1100 bis 1500) codice_4.

Die Sprachstufen des Englischen lassen sich wie folgt bestimmen:

Durch die weltweite Verbreitung der englischen Sprache hat diese zahlreiche Varietäten entwickelt oder sich mit anderen Sprachen vermischt.

Folgende Sprachvarietäten werden unterschieden:
Für den raschen Erwerb des Englischen wurden immer wieder vereinfachte Formen konstruiert, so Basic English bzw. Simple English oder Einfaches Englisch (vorgestellt 1930, 850 Wörter), Globish (vorgestellt 1998, 1500 Wörter) und Basic Global English (vorgestellt 2006, 750 Wörter). Daneben hat sich eine Reihe von Pidgin- und Kreolsprachen auf englischem Substrat (vor allem in der Karibik, Afrika und Ozeanien) entwickelt.

In andere Sprachen eindringende Anglizismen werden manchmal mit abwertenden Namen wie „Denglisch“ (Deutsch und Englisch) oder „Franglais“ (Französisch und Englisch) belegt. Dabei handelt es sich nicht um Varianten des Englischen, sondern um Erscheinungen in der jeweils betroffenen Sprache. Der scherzhafte Begriff „Engrish“ wiederum bezeichnet keine spezifische Variante der englischen Sprache, sondern bezieht sich allgemein auf das in Ostasien und Teilen von Südostasien anzutreffende Charakteristikum, die Phoneme „l“ und „r“ nicht zu unterscheiden.

Die Entwicklung des Englischen zur lingua franca im 20. Jahrhundert beeinflusst die meisten Sprachen der Welt. Mitunter werden Wörter ersetzt oder bei Neuerscheinungen ohne eigene Übersetzung übernommen. Diese Entwicklung wird von manchen skeptisch betrachtet, insbesondere dann, wenn es genügend Synonyme in der Landessprache gibt. Kritiker merken auch an, es handle sich des Öfteren (beispielsweise bei "Handy" im Deutschen) um Scheinanglizismen.

Mitunter wird auch eine unzureichende Kenntnis der englischen Sprache für die Vermischung und den Ersatz bestehender Wörter durch Scheinanglizismen verantwortlich gemacht. So sprechen einer Studie der GfK zufolge nur 2,1 Prozent der deutschen Arbeitnehmer verhandlungssicher Englisch. In der Gruppe der Unter-30-Jährigen bewerten jedoch über 54 Prozent ihre Englischkenntnisse als gut bis exzellent. Zu besseren Sprachkenntnissen könne demzufolge effizienterer Englischunterricht beitragen, und statt der Ton-Synchronisation von Filmen und Serien solle eine Untertitelung der englischsprachigen Originale mit Text in der Landessprache erfolgen. Dies würde zugleich zu einer besseren Abgrenzung zwischen den Sprachen und einer Wahrung lokaler Sprachqualität beitragen.
Im Dezember 2014 forderte der Europapolitiker Alexander Graf Lambsdorff, neben Deutsch die englische Sprache als Verwaltungs- und später als Amtssprache in Deutschland zuzulassen, um die Bedingungen für qualifizierte Zuwanderer zu verbessern, den Fachkräftemangel abzuwenden und Investitionen zu erleichtern.

Eine große Klasse von Unterschieden zwischen der deutschen und der englischen Sprache sind auf die zweite Lautverschiebung zurückzuführen. Dabei liegt die Neuerung auf Seiten der deutschen Sprache; die englische Sprache bewahrt hier den altertümlichen germanischen Zustand. Beispiele sind:


Es gibt jedoch auch Unterschiede, bei denen die deutsche Sprache konservativer ist:


Beim Project Gutenberg stehen zahlreiche Texte frei zur Verfügung.

Mit den typischen Fehlern, die beim Erlernen und Übersetzen der englischen Sprache auftreten können, beschäftigen sich folgende Beiträge:


vgl. Fremdsprachendidaktik







</doc>
<doc id="1366" url="https://de.wikipedia.org/wiki?curid=1366" title="Euromünzen">
Euromünzen

Die Euromünzen sind die in derzeit 19 Ländern der Europäischen Union sowie den Nicht-EU-Staaten Andorra, Monaco, San Marino und Vatikanstadt in Umlauf gebrachten Münzen der gemeinsamen europäischen Währung Euro. Ein Euro wird unterteilt in 100 Cent; es gibt acht Nennwerte für Münzen.

Die Euromünzen wurden zusammen mit den Eurobanknoten ab dem 1. Januar 2002 eingeführt. Das Prägejahr der Münzen kann aber bis 1999 zurückgehen, also bis zu dem Jahr, in dem die Währung offiziell als Buchgeld eingeführt wurde.

Gemeinsam ist den 1- und 2-Euro-Münzen ein Aufbau aus "Ring" und "Kern" (auch "Pille"). Beim 1-Euro-Stück besteht der Ring aus Messing und der Kern aus Kupfernickel. Beim 2-Euro-Stück ist es umgekehrt. Vor der Prägung spricht man auch von "Rohlingen" und "Ronden".

Die Wahl des Münzmetalls war eine Frage der Zweckmäßigkeit und der Kosten. Münzlegierungen dürfen insbesondere nicht rostempfindlich sein und sollen sich im Gebrauch wenig abnutzen; Hautkontakt soll zudem keine Allergien auslösen. Wichtig ist auch, dass der Metallwert unter dem Nennwert der Münze bleibt – sonst bestünde die Gefahr, dass die Münzen eingeschmolzen und als Ware gehandelt werden.

Die Herstellungskosten einer 1-Cent-Münze entsprachen 2004 dem Nominalwert. Die Herstellungskosten einer 2-Euro-Münze hingegen betrugen seinerzeit in Deutschland nur 13 Cent. Die Seigniorage genannte Differenz zum Nominalwert kommt der jeweiligen Zentralbank zugute.

Euromünzen haben auf Grund ihres Anteils an Nickel ein hohes Allergiepotenzial.
Der bimetallische Aufbau von 1- und 2- Euromünzen führt bei Kontakt mit menschlichem Schweiß zu einem galvanischen Potential von 30–40 mV. Hierdurch können die Grenzwerte der Europäischen Direktive 94/27/EEC für Nickel bis um das 320-fache überschritten werden.

Schätzungen von Allergologen zufolge leiden in Deutschland bis zu elf Prozent der Frauen und sechs Prozent der Männer unter einer Nickelallergie. Kommen sie länger mit dem Metall in Kontakt, rötet sich die Haut, juckt und bildet Bläschen.
Nach Angaben des Ärzteverbandes Deutscher Allergologen haben Kassierer und Bankangestellte ein deutlich erhöhtes Allergierisiko.

Euromünzen wurden und werden in folgenden Prägestätten und mit diesen Münzstättenzeichen (Mzz) geprägt:
Euromünzen wurden und werden mit folgenden Münzmeisterzeichen (Mmz) geprägt:

Münzmeisterzeichen sind zu unterscheiden von Münzsignaturen, die den Münzgestalter oder den Münzgraveur benennen.

Zwischen Ende 2009 und Anfang 2011 wurden, vorwiegend von Flugbegleitern, Münzen im Nennwert von 866.000 Euro mit dem Ersuchen um Einziehung und Ersatz bei der Bundesbank eingeliefert. Die Münzen stammten überwiegend aus Frankreich, Belgien, Österreich oder Spanien und waren von oder im Auftrag europäischer Zentralbanken durch Münztrennung entwertet, als Metallschrott weiterveräußert und unautorisiert nachträglich in China wieder zusammengesetzt worden.
Der erhobene Vorwurf des Sichverschaffens und Inverkehrbringens von falschem Geld hatte vor dem Bundesgerichtshof keinen Bestand.
Da die Münzen außerhalb des allgemeinen Zahlungsverkehrs eingeliefert wurden, bestand keine Gefahr, dass sie wieder in den Umlauf gelangten, da diese nicht nur erkennbar unfachmännisch zusammengesetzt, sondern auch stark beschädigt und daher nicht mehr umlauffähig waren. Die Deutsche Bundesbank hatte über einen Zeitraum von mehr als einem Jahr mehrere hundert durchsichtige „Safebags“ mit solchen Münzen beanstandungsfrei angenommen und deren Nennwert erstattet. Diese Praxis der Bundesbank spricht gegen die Annahme, die amtliche Entwertung von Bicolormünzen erfolge im Wege der Trennung von Ring und Pille. Die Strafkammer konnte keine Erkenntnisse über das Entwertungsverfahren bei Euromünzen in anderen Euro-Ländern gewinnen, da sämtliche Anfragen der Ermittlungsbehörden an die jeweiligen Landeszentralbanken Frankreichs, Belgiens, Österreichs und Spaniens unbeantwortet blieben.

Trotz ihrer hohen technischen Qualität werden auch Euromünzen gefälscht. Durch die Vielzahl von Produktionsstandorten und Trägermaterialien erhöht sich die Gefahr, dass geheime Informationen hinsichtlich der Sicherheitsmerkmale in die falschen Hände geraten. Am 22. Juli 1998 forderte die Kommission die baldige Errichtung eines harmonisierten Rechtsrahmens und den Aufbau einer von der EZB unterhaltenen EU-Geldfälschungsdatenbank. Im Oktober 1998 wurde vereinbart, diese durch eine Datenbank über Münzfälschung zu ergänzen. Am 3. Februar 1999 beschloss der Wirtschafts- und Finanzausschuss den "Counterfeit Organisation and Investigation Network" / C.O.I.N. genannten Plan zur Errichtung eines Europäischen Zentrums für technische und wissenschaftliche Analysen (ETSC / "European Technical and Scientific Center") zur Analyse und Klassifizierung aller weltweit aufgespürten Fälschungen von Euro-Münzen sowie nationale Münzanalysezentren zur Erfassung aller in ihrem Staatsgebiet auftretenden Fälschungsfälle. Der Rat Justiz und Inneres (JAI) beschloss am 29. April 1999, den Aufgabenbereich von Europol um die Bekämpfung der Geldfälschung zu erweitern.

2012 wurden nach Angaben der EZB rund 184.000 Falschmünzen aus dem Verkehr gezogen. Somit kam auf 100.000 echte Münzen eine Fälschung. Die 2-Euro-Münze macht fast zwei Drittel aller entdeckten Falschmünzen aus.
Die Deutsche Bundesbank registrierte 2013 rund 52.000 Münzfälschungen. Anfang Dezember 2014 wurden in Italien 306.000 in China produzierte falsche Ein-Euro- und Zwei-Euro-Münzen mit einem Gesamt-Nennwert von 556.000 Euro sichergestellt.

Die zum 1. Januar 2013 in Deutschland in Kraft getretene "Bargeldprüfungsverordnung", die aufgrund einer EU-Verordnung zur Festlegung von Maßnahmen zum Schutz des Euro gegen Geldfälschung erlassen wurde, sieht vor, dass Banken auch Münzen auf Fälschungen prüfen oder prüfen lassen müssen, so wie es zuvor schon bei Geldscheinen vorgeschrieben war. Während einer Übergangsfrist konnten Bankinstitute ihre alten Automaten zum Einzahlen von Münzen noch verwenden. Ab Anfang 2015 sind nur noch zertifizierte Geräte zugelassen. Eine neue „Bandstraße“, die Münzen sammelt, sortiert und auf Echtheit prüft, kostet 200.000 Euro. Demzufolge delegieren z. B. Sparkassen das Zählen und Prüfen von Münzen an Werttransportunternehmen, und Kunden – beispielsweise Kaufleute, die ihre Tageseinnahmen abliefern – müssen für jede Münzeinzahlung dann eine Gebühr von fünf Euro zahlen.

Hintergrund ist, dass sich die Deutsche Bundesbank mehr und mehr von der Versorgung mit Hartgeld zurückzieht. Im Jahr 2000 konnten Läden ihre Einnahmen in 129 Filialen der Landeszentralbanken einzahlen; doch von denen sind mittlerweile drei Viertel geschlossen. 2015 sank ihre Zahl auf 34 Filialen. Münzen bekommen Banken von der Bundesbank auch nicht mehr in kleinen Stückelungen, sondern nur noch in Form eines 700 kg wiegenden Containers.

Im Dezember 2017 waren rund 126 Mrd. Euromünzen mit einem Gesamtwert von rund 28 Mrd. Euro im Umlauf.
Der Umlaufwert je Münzsorte steigt mit ihrem Nennwert: 1 % des Umlaufwerts aller Münzen liegt in den 1-Cent-Münzen, 2 % in den 2-Cent-Münzen, 4 % in den 5-Cent-Münzen, 5 % in den 10-Cent-Münzen, 8 % in den 20-Cent-Münzen, 11 % in den 50-Cent-Münzen, 26 % in den 1-Euro- sowie 43 % in den 2-Euro-Münzen (im Gesamtwert von etwa 28 Mrd. Euro).
Stückzahlmäßig allerdings sind die Münzen umso zahlreicher im Umlauf, je kleiner ihr Nennwert ist; nur 1-Euro-Münzen sind etwas häufiger als 50-Cent-Münzen. Es laufen 34 Mrd. 1-Cent-Münzen um, doch nur gut 6 Milliarden 2-Euro-Münzen.

Im Vergleich zu den Eurobanknoten machen die Münzen nur 2,5 % des gesamten Bargeldumlaufs von 1.198,706 Mrd. Euro aus. Jedoch sind die Umlaufmengen der einzelnen Banknoten geringer.

Der Rat beschloss unter anderem: "„Monaco, San Marino und Vatikanstadt […] können unter bestimmten Bedingungen Euro-Münzen prägen. Zu diesem Zweck werden Frankreich (für Monaco) und Italien (für den Vatikan und San Marino) vom Rat ermächtigt, Verhandlungen zu führen und die Vereinbarung im Namen der Gemeinschaft abzuschließen.“" Am 14. Juli 2009 erstattete die Kommission dem Rat einen Bericht über die Währungsvereinbarungen mit den Kleinstaaten. Darin wird festgestellt:

Nach dem neuen Berechnungsverfahren würde sich die Obergrenze für ein Jahr (n) aus einem festen und einem variablen Bestandteil zusammensetzen:

Mit dem so beschlossenen neuen Berechnungsverfahren erhöhte sich die Obergrenze für die Prägeauflage der Vatikanstadt auf 2.300.000 EUR im Jahr 2010, die von Monaco auf 2.340.000 EUR im Jahr 2012 und die von San Marino auf 2.600.000 EUR im Jahr 2013.

Am 1. April 2012 trat eine Währungsvereinbarung mit einem weiteren europäischen Zwergstaat, Andorra, in Kraft. Am 29. Dezember 2014 begann Andorra mit der Ausgabe eigener Euromünzen.

Die aktuellen, zwischen 2011 und 2012 abgeschlossenen Währungsvereinbarungen mit Andorra, Monaco und San Marino sehen vor, dass mindestens 80 % der Umlaufmünzen zum Nennwert ausgegeben werden. Lediglich im Abkommen mit der Vatikanstadt, das bereits am 17. Dezember 2009 getroffen wurde, ist noch eine niedrigere Quote, mindestens 51 %, festgeschrieben.

In einigen Euroländern wurde die Ausgabe der 1- und 2- Cent-Münzen ganz eingestellt:

Während die ausgezeichneten Preise weiterhin auf 1 Cent genau sein können, werden die dann an der Kasse in bar zu zahlenden Gesamtbeträge auf den nächsten durch 5 Cent teilbaren Betrag gerundet, d. h. Beträge, die auf 1, 2, 6 oder 7 Cent enden, werden nach unten, Beträge, die auf 3, 4, 8 oder 9 Cent enden, nach oben gerundet.

Da die 1- und 2-Cent-Münzen in der gesamten europäischen Währungsunion gesetzliches Zahlungsmittel sind, müssen sie aber von den Händlern angenommen werden.

Die Produktion der Euro-Münzen ist Sache der einzelnen Mitgliedsstaaten. Da die Entscheidungen dezentral getroffen werden, erfolgt keine koordinierte Produktion und/oder Lagerung, so dass Effizienzgewinne aus einer Zusammenfassung verloren gehen. So ist es möglich, dass einige Länder zusätzliche Euro-Münzen prägen lassen, während andere Länder Lagerüberschüsse dieses Nennwerts haben. Daher bietet es sich an, Verbesserungsmöglichkeiten hinsichtlich der kleineren Stückelungen (1, 2 und 5 Cent) zu prüfen, auf die im Durchschnitt insgesamt etwa 80 % der Produktion neuer Münzen entfallen. Verglichen mit ihrem Nennwert, erzielen sie keine oder nur geringe monetäre Einkünfte, verursachen aber hohe Produktions- und Transportkosten. Da unterschiedliche nationale Seiten den Austausch oder die Übertragung von Münzvorräten unter den Ländern einschränken und einer erhöhten Effizienz der Produktion größerer Mengen entgegenstehen, könnte eine „Standardseite“ statt der nationalen Seite einen Teil des Bedarfs an den drei kleinsten Münz-Stückelungen decken.

Eine Empfehlung der Kommission vom 22. März 2010 über den Geltungsbereich und die Auswirkungen des Status der Euro-Banknoten und -Münzen als gesetzliches Zahlungsmittel lautet unter Art. 9.: "Status der 1- und 2-Euro-Cent-Münzen als gesetzliches Zahlungsmittel und Rundungsregeln: In den Mitgliedstaaten, in denen Rundungsregeln angenommen wurden und die Preise folglich auf die nächsten 5 Cent auf bzw. abgerundet werden, sollten 1- und 2-Euro-Cent-Münzen weiterhin als gesetzliches Zahlungsmittel gelten und als solches angenommen werden. Die Mitgliedstaaten sollten allerdings keine neuen Rundungsregeln annehmen, da dadurch die Entlastung von einer Zahlungsverpflichtung durch Zahlung des exakten geschuldeten Betrags beeinträchtigt wird und dies in einigen Fällen zu einem Aufschlag bei Barzahlungen führen kann."

Am 4. Juli 2012 bestimmte eine Verordnung des Europäischen Parlaments und des Rates, die Kommission solle eine Folgenabschätzung über eine fortgesetzte Ausgabe von 1- und 2-Cent-Münzen vornehmen.<ref name="vo651/2012"></ref> Die EU-Kommission machte daraufhin am 14. Mai 2013 Vorschläge für eine Vergünstigung oder eine Abschaffung der 1- und 2-Cent-Münzen. Währungskommissar Olli Rehn stellte fest, die Kosten der Herstellung und Herausgabe dieser Münzen überstiegen ihren Wert. Zugleich müssten die Zentralbanken gerade von diesen Münzen besonders viele Exemplare herausgeben. Insgesamt seien in den letzten elf Jahren 45,8 Milliarden solcher Kleinstmünzen in Umlauf gebracht worden. Die Ausgabe der Kleinstmünzen habe die Euro-Staaten seit dem Start der Gemeinschaftswährung im Jahr 2002 zusammen etwa 1,4 Milliarden Euro gekostet. Die Kosten für die Cent-Münzen könnten etwa durch eine andere Materialmischung oder ein effizienteres Prägungsverfahren reduziert werden.

Bei Einführung des Euro-Bargelds, im Januar 2002, machten die 1- und 2-Cent-Münzen 31,2 % der Umlaufmünzen aus, im April 2017 hingegen 48,1 %. Die weit überdurchschnittliche Kleinmünzproduktion ist lt. Bundesbankvorstand Carl-Ludwig Thiele darauf zurückzuführen, dass rund drei Viertel der Kleinmünzen verloren gehen oder gehortet werden. Höchstens jede fünfte 1-Cent-Münze und jede vierte 2-Cent-Münze werde im Bargeldumlauf genutzt.

Abweichend von der in der Numismatik üblichen Bezeichnungsweise wird in den amtlichen Schreiben der EU die Wertseite als Vorderseite und die Bildseite als Rückseite bezeichnet. Die hier verwendeten Bezeichnungen orientieren sich an den von der EU verwendeten. In der Verordnung 729/2014 definiert die EU die Begriffe „Umlaufmünzen“, „Gedenkmünzen“ und „reguläre Münzen“ (an Stelle des hier verwendeten Begriffs „Kursmünzen“).

Alle Euro-Münzen haben gemeinsame Vorderseiten, die den Wert der Münze angeben. Sie wurden vom Gewinner des zu diesem Gestaltungsthema erfolgten Wettbewerbs, dem belgischen Designer Luc Luycx, entworfen, dessen Signet „LL“ aufgeprägt ist. Ende 1997 nahm Luc Luycx verschiedene Änderungen an seinen Münzentwürfen vor. Damit sollte den Anträgen einiger Mitgliedstaaten Rechnung getragen werden, die darauf abzielten, die Qualität der geographischen Darstellung zu verbessern:
Luxemburg war auf den 1- und 2-Euro-Münzen, Portugal auf den 2-Euro-Münzen nicht zu erkennen, Dänemark brachte seine Verwunderung darüber zum Ausdruck, dass die Insel Fünen auf einigen Münzen als Teil des Festlands gezeichnet war, Griechenland hielt den Küstenverlauf der Peloponnes auf den 10-, 20- und 50-Cent-Münzen für falsch und wünschte die Abbildung Kretas auf den 1- und 2-Euro-Münzen, Schweden wollte die Insel Gotland, Finnland die Ålandinseln und das Vereinigte Königreich die Hebriden abgebildet sehen, die Form Deutschlands war auf den 10-, 20- und 50-Cent-Münzen nicht korrekt wiedergegeben, es fehlte die Grenze zwischen der Republik Irland und Nordirland sowie zwischen Spanien und Portugal, und die Spanier legten Wert darauf, die Kanarischen Inseln auf den 1- und 2-Euro-Münzen abgebildet zu sehen. Um die Anträge an objektiven Kriterien messen zu können, wurde beschlossen, nur Inseln von über 2500 km² und Inselgruppen von über 5000 km² zu berücksichtigen.

Die seit dem Prägejahr 1999 unveränderten 1-, 2- und 5-Cent-Münzen bilden die nördliche Hemisphäre mit dem östlichen Mittelmeer im Zentrum ab, also Europa als Kontinent in Nachbarschaft zu Asien und Afrika. Die seinerzeit 15 die EU bildenden Staaten sind strukturiert dargestellt. Auf den 10-, 20- und 50-Cent-Münzen bis 2007 sind die einzelnen EU-Staaten wie singuläre Puzzlesteine zu sehen; die Erweiterung der Europäischen Union von 2004 wurde jedoch bei nachfolgenden Prägungen nicht berücksichtigt. EU-Mitglieder, die nicht am Euro teilnehmen, z. B. Großbritannien, sind dargestellt. Die 1- und 2-Euro-Münzen bis 2007 zeigen das Gebilde der EU-Länder vor der ersten Osterweiterung. Ab 2007 wurde auf den 10-, 20- und 50-Cent-, sowie den 1- und 2-Euro-Münzen Europa ohne Ländergrenzen als Ganzes dargestellt (siehe unten Abschnitt "Neugestaltung"). Alle Münzen zeigen auch noch zwölf Sterne als Symbol Europas (siehe auch: Symbolik der Europaflagge).

Mit der Einführung des Euro am 1. Januar 2007 in Slowenien wurden die gemeinsamen Vorderseiten der Münzen einer Neugestaltung unterzogen. Statt wie bisher die 15 alten Mitgliedstaaten der Europäischen Union zeigen die revidierten Geldstücke die geographischen Umrisse Europas. In der generellen Übersicht wurde Island ausgespart, Zypern nicht der tatsächlichen Lage entsprechend angedeutet und bei den 1- und 2-Euro-Münzen nur ein Teil Finnlands gezeigt. Bei den 1- und 2-Euro-Münzen sowie den 10-, 20- und 50-Cent-Münzen wurde auf die Darstellung von Staatsgrenzen verzichtet. Die 1-, 2- und 5-Cent-Münzen wurden nicht geändert, obwohl auf dem abgebildeten Globus nur das Staatsgebiet der ersten 15 EU-Mitglieder markiert ist. Die meisten Euro-Länder begannen mit der Einführung der neuen Vorderseiten 2007. In Italien, Österreich, Portugal, San Marino und in der Vatikanstadt wurde die neue Vorderseite erst 2008 eingeführt. Die älteren Münzen mit der Darstellung der 15 EU-Staaten bleiben weiterhin gültig.
Der Euro wurde am 1. Januar 1999 als Buchgeld, drei Jahre später als Bargeld eingeführt. Bereits 1998 begann man in Deutschland mit der Prägung von Euromünzen, die allerdings bis einschließlich des Prägejahres 2001 das Ausgabejahr 2002 auswiesen. Auch Irland, Italien, Luxemburg, Österreich, Portugal, San Marino und die Vatikanstadt sowie das 2001 der Eurozone beigetretene Griechenland nannten auf ihren ersten, schon vor dem Jahr 2002 geprägten Euromünzen das Ausgabejahr 2002. Die ersten Euromünzen Belgiens, Finnlands, Frankreichs, der Niederlande und Spaniens hingegen nannten die Prägejahre 1999 bis 2001, die Monacos 2001. Belgien, Monaco, die Niederlande und Spanien vermieden so das Risiko, auf den Münzen das zum Zeitpunkt der Prägung noch in der Zukunft liegende Ausgabejahr zu nennen, in dem das abgebildete Staatsoberhaupt möglicherweise nicht mehr im Amt sein könnte. Luxemburg und die Vatikanstadt scheuten dieses Risiko offenbar nicht. 

Obwohl seit Anfang 2015 19 Staaten der EU an der Währungsunion beteiligt sind, gibt es Münzsätze von 23 verschiedenen Ländern.

In Andorra, das per Währungsabkommen mit der Eurozone assoziiert ist,
gilt der Euro seit 1. April 2012 als gesetzliches Zahlungsmittel; die Ausgabe eigener Euromünzen erfolgte jedoch erst zum Jahreswechsel 2014/2015.
Monaco, San Marino und die Vatikanstadt sind ebenfalls keine EU-Mitglieder, befanden sich jedoch vor der Euroeinführung aufgrund von Währungsvereinbarungen in Währungsunion mit Frankreich bzw. Italien. Daher wurde es als notwendig erachtet, die jeweiligen Währungsvereinbarungen durch neue bilaterale Übereinkommen mit der Europäischen Union zu ersetzen, welche diesen Zwergstaaten das Recht einräumen, eigene Euromünzen zu prägen.

Jedes Land, das am Euro teilnimmt, hat seine eigene Gestaltung der Rückseite. Alle Münzen zeigen auf dieser Seite die Jahreszahl und die zwölf Sterne der EU-Flagge.

Die nationalen Seiten sind in eigenen Artikeln beschrieben:

Die Empfehlung der Kommission der Europäischen Gemeinschaften vom 29. September 2003 zu einem einheitlichen Vorgehen bei Änderungen der Gestaltung der nationalen Rückseiten der Euromünzen sah ein Moratorium für Veränderungen der Münzbilder der nationalen Vorderseiten der Euro- bzw. Cent-Umlaufmünzen vor. Bis Ende 2008 sollten diese unverändert bleiben, es sei denn, der auf der Münze abgebildete Staatschef wurde abgelöst. Es wurde lediglich bestimmt, dass „das Münzbild auf der nationalen Seite von zwölf Sternen umrandet sein und eine Jahreszahl aufweisen soll“.
Das ließ den nationalen Münzgestaltern einen breiten Freiraum. So wurden die zwölf Sterne nicht immer in gleichen Abständen am Rand der Münze platziert, sondern zum Beispiel auch eng beieinander in Gruppen angeordnet, so dass Platz für das Münzmotiv oder die Beschriftung freigehalten wurde. Die 2-Euro-Gedenkmünzen der deutschen Bundesländerserie konnten dadurch auf dem Ring die Bezeichnung „Bundesrepublik Deutschland“ tragen.

Es änderte sich, als am 19. Dezember 2008 neue Leitlinien erlassen wurden, die bestimmten, dass auf der nationalen Seite der Euro-Umlaufmünzen das nationale Motiv sowie die Jahreszahl und der deutlich angegebene und leicht erkennbare volle oder abgekürzte Name des Ausgabestaats von den zwölf europäischen Sternen umrandet werden sollen. Die Sterne sollten wie auf der europäischen Flagge angeordnet sein. Auf eine Wiederholung des Nennwerts der Münze oder der Währungsbezeichnung bzw. ihrer Unterteilung auf der nationalen Seite sollte verzichtet werden, außer wenn im entsprechenden Land ein anderes als das lateinische Schriftsystem verwendet wird. Die Ausgabestaaten sollten zwar die Möglichkeit haben, ihre nationalen Seiten der Euro-Umlaufmünzen den Gestaltungsleitlinien anzupassen, ansonsten sollten die Motive nicht verändert werden, es sei denn, es sei ein Wechsel des auf der Münze abgebildeten oder genannten Staatsoberhaupts erfolgt. Scheidet das Staatsoberhaupt (etwa durch Tod oder Amtsverzicht) aus dem Amt, ist eine sofortige Neugestaltung der Münzen möglich. Allerdings ist es nicht mehr gestattet, für den Zeitraum zwischen dem Ausscheiden des alten Staatsoberhaupts und der Bestellung des neuen einen eigenen Münzsatz herauszugeben (wie es 2005 in der Vatikanstadt, während der Sedisvakanz nach dem Tod von Papst Johannes Paul II., geschah). Den Ausgabestaaten solle gestattet werden, Münzmotive, die das Staatsoberhaupt darstellen, alle fünfzehn Jahre zu aktualisieren, um Änderungen im Erscheinungsbild des Staatsoberhaupts Rechnung zu tragen.

Durch diese Leitlinien ist die kreisförmige Anordnung der zwölf Sterne in 30°-Intervallen vorgeschrieben, wie die Platzierung der die Stunden anzeigenden Ziffern einer Uhr. Das durch die Europaflagge vorgegebene Verhältnis der Größe eines Sternes zum Radius des Kreises, auf dem die Mittelpunkte der Sterne liegen, kann auf den Münzen nicht streng eingehalten werden – die Sterne wären dann etwa bei den 2-Euro-Münzen deutlich größer als der beprägbare Teil des Ringes.

Die z. B. von Deutschland 2013 (für die Gedenkmünze Baden-Württemberg) gewählte Darstellung, bei der die Sterne so groß wie möglich den Ring füllen, kommt dem Erscheinungsbild der Europaflagge am nächsten. Die z. B. von den Niederlanden 2013 (für die Thronwechsel-Gedenkmünze) gewählte Darstellung mit deutlich kleineren, an den Außenrand gerückten und zudem strukturierten Sternen zeigt aber, dass auch diese Leitlinien noch Spielraum bieten.

Den Gestaltungsleitlinien wird teilweise nicht entsprochen. So ist die Darstellung des – stilisierten – Landeskürzels "RF" auf der französischen Gedenkmünze 2012/2
sowie das "FI" auf der finnischen Gedenkmünze 2013/2
weder deutlich noch leicht erkennbar. Die Luxemburger Gedenkmünze 2014/2
weist den Ausgabestaat nicht einmal durch ein Landeskürzel aus.

Die nationalen Seiten der Kursmünzen mehrerer Euroländer entsprechen nicht den 2008 erlassenen Gestaltungsleitlinien.
Die Münzen Österreichs zeigen den jeweiligen Nennwert in Worten oder Ziffern sowie die Angabe "EURO CENT" bzw. "EURO", ferner fehlt – wie auch bei den deutschen und griechischen Kursmünzen – die Landesbezeichnung. Bei den niederländischen Kursmünzen der ersten Prägeserie (1999–2013) waren Landesbezeichnung und Jahresangabe nicht von den zwölf europäischen Sternen umrandet und die Sterne zudem nicht wie auf der europäischen Flagge angeordnet. Das gleiche trifft auf die luxemburgischen Centmünzen zu. Bei den slowenischen Kursmünzen sind Landesbezeichnung und Jahresangabe ebenfalls nicht von den Europasternen umgeben und die 10-Cent-Münze zeigt die Sterne nicht wie auf der Europaflagge. Letzteres trifft auch auf die italienische 2-Cent-Münze zu.

Die bisherigen Euro-Länder müssen die Darstellungen ihrer Kursmünzen jedoch nicht sogleich an die Gestaltungsleitlinien anpassen; eine Frist zur Überarbeitung wurde in den Leitlinien von 2008 nicht festgelegt. Finnland begann mit der Neugestaltung bereits 2007, Belgien folgte 2008 und Spanien 2010. Abmessungen, Masse und andere technische Spezifikationen der Münzen bleiben unverändert, um den Übergang von alten zu neuen Münzen nicht zu erschweren.

Bei den drei auf die 2008 erlassenen Gestaltungsleitlinien folgenden Ausgaben der deutschen Bundesländerserie (2010, 2011 und 2012) wurde das aus Platzgründen eingeführte Landeskürzel "D" (das die Langfassung "Bundesrepublik Deutschland" ersetzte) zwischen den Sternen in „12- und 1-Uhr-Position“ (bezeichnet in Analogie zum Zifferblatt) platziert, die zweigeteilte Jahreszahl zwischen dem „7- und dem 5-Uhr-Stern“. Offenbar bestand also ein Interpretationsspielraum darüber, was „von den zwölf europäischen Sternen umrandet“ bedeutet. Beginnend mit den Gedenkmünzen 2013 befinden sich das "D" und die Jahreszahl innerhalb des Sternenkreises auf der Pille – wo es allerdings u. U. mit dem (nur unwesentlich kleineren) Münchener Münzstättenzeichen "D" verwechselt werden kann.

Die Richtlinien zur Neugestaltung der nationalen Rückseiten wurden am 18. Juni 2012 konkretisiert und am 24. Juni 2014 neu gefasst. Nunmehr bleibt der Ring allein der Darstellung der zwölf Sterne vorbehalten, ausgenommen auf den Ring ragende einzelne Gestaltungselemente, solange alle Sterne deutlich und vollständig sichtbar bleiben. Ggfs. notwendige Anpassungen der nationalen Seiten von Kursmünzen an die Gestaltungsleitlinien können jederzeit vorgenommen werden, sind jedoch laut Artikel 1 g der Verordnung binnen fünfzig Jahren, spätestens zum 20. Juni 2062, zu vollziehen. Unbeschadet etwaiger Änderungen, die erforderlich sind, um Münzfälschungen zu verhindern, dürfen die auf den nationalen Seiten der Kursmünzen verwendeten Gestaltungen nur alle 15 Jahre geändert werden. Diese 15-Jahres-Intervalle betreffen nunmehr alle Kursmünzen, nicht mehr nur die mit Abbildungen von Staatsoberhäuptern. Bereits vorgenommene und genehmigte Änderungen von nationalen Münzseiten bleiben bestehen. Alle bisherigen Euromünzen behalten ihren Wert und bleiben im Umlauf.

2-Euro-Gedenkmünzen unterscheiden sich von Kursmünzen (lt. EU: „regulären Münzen“) nur dadurch, dass ihre nationale Seite durch eine spezielle Gedenkseite ersetzt wird. Die gemeinsame Seite sowie alle weiteren Eigenschaften wie Nennwert, Farbe, Dicke und Durchmesser sind unverändert. Die Auflagen dieser Gedenkmünzen sind festgelegt. 2-Euro-Gedenkmünzen sind (im Gegensatz zu Sammlermünzen) für den Umlauf bestimmt und in allen Euroländern gültig.
Seit 2004 können alle Staaten des Euroraumes 2-Euro-Gedenkmünzen herausgeben; den Anfang machte Griechenland anlässlich der Olympischen Spiele 2004. Jedes Ausgabeland konnte bis zum 16. August 2012 nur "eine" Gedenkmünze im Jahr in Umlauf bringen. Seit diesem Stichtag darf jeder Mitgliedstaat, dessen Währung der Euro ist, pro Jahr zwei 2-Euro-Gedenkmünzen prägen. Zusätzliche Gedenkmünzen sind möglich, wenn die Position des Staatsoberhaupts vorübergehend nicht oder nur vorläufig besetzt ist, oder wenn die Staaten der Eurozone eine gemeinsame Gedenkmünze ausgeben.

Im März 2007 erschien eine Gemeinschaftsausgabe der 2-Euro-Gedenkmünzen zum 50. Jahrestag der Unterzeichnung der "Römischen Verträge". Sie wurde von allen 13 damaligen den Euro emittierenden EU-Staaten (d. h. ohne Beteiligung Monacos, San Marinos und der Vatikanstadt) ausgegeben. Die Münze ist in allen Ländern gleich gestaltet und unterscheidet sich nur durch den jeweiligen Landesnamen und die Sprache der Inschrift "Römische Verträge – 50 Jahre". Die zweite gemeinsame 2-Euro-Gedenkmünze erschien am 1. Januar 2009 aus Anlass des zehnjährigen Bestehens der Europäischen Wirtschafts- und Währungsunion. Die dritte Gemeinschaftsausgabe wurde am 1. Januar 2012 anlässlich des zehnjährigen Jubiläums der Einführung des Euro-Bargelds ausgegeben, eine vierte ist 2015 zum 30-jährigen Jubiläum der Europaflagge erschienen.

Bis Ende 2014 hatten 21 der seinerzeit 22 Euro-Länder insgesamt 978 Millionen 2-Euro-Gedenkmünzen ausgegeben. Nach Angaben der EZB betrug das Volumen umlaufender 2-Euro-Münzen – Kursmünzen und Gedenkmünzen – zu diesem Zeitpunkt 5284 Millionen. Die bis Ende 2014 ausgegebenen 2-Euro-Gedenkmünzen stellten demnach maximal (auch defekte Gedenkmünzen werden ggfs. aus dem Verkehr gezogen und durch neue Kursmünzen ersetzt) 18,5 % der zu diesem Zeitpunkt umlaufenden 2-Euro-Münzen.

Mit Stand Ende 2015 ergaben sich folgende Zahlen: 

Neben den in allen Teilnehmerländern als gesetzliches Zahlungsmittel gültigen Euro-Kursmünzen ist jedes an der Währungsunion teilnehmende Land berechtigt, Sammlermünzen herauszugeben.

Sammlermünzen unterscheiden sich im Nennwert sowie in mindestens zwei der drei Kriterien Dicke, Durchmesser und Farbe von den Kursmünzen.
Diese Sammlermünzen sind nur im herausgebenden Land gesetzliches Zahlungsmittel. Da Sammler- und Materialwert der Goldmünzen deren Nennwert deutlich übersteigen, spielen sie im Zahlungsverkehr selten eine Rolle. Auch die deutschen Silbermünzen, die zum Nennwert ausgegeben werden, kommen im Umlauf selten vor.

Aufgrund einer EU-Verordnung vom 4. Juli 2012 müssen die Sammlermünzen andere Nennwerte aufweisen als die regulären Umlaufmünzen (1, 2, 5, 10, 20 und 50 Cent sowie 1 und 2 Euro). Dieser Forderung wird in der Regel dadurch entsprochen, dass Sammlermünzen Nennwerte von mehr als 2 Euro aufweisen. Oftmals entsprechen sie den Nennwerten der Banknoten (5, 10, 50, 100 und 200 Euro), jedoch gibt es in manchen Staaten auch Sammlermünzen mit ungewöhnlichen Nennwerten:


Diese Prägungen sind keine offiziellen Zahlungsmittel und sind daher nicht als "Münzen", sondern als Medaillen zu bezeichnen. Die Mustermünzen und Testprägungen kommen in aller Regel nicht von der Zentralbank bzw. den nationalen Münzprägestätten. Vielmehr handelt es sich um Fantasieprägungen durch Privatpersonen oder Münzhandelsfirmen; das Design gibt daher auch nicht das mögliche Aussehen späterer offizieller Europrägungen dieser Länder wieder. Mustermünzen und Testprägungen werden für praktisch alle EU-Länder, die den Euro noch nicht eingeführt haben, angeboten. In einigen Ländern, so auch in Deutschland, stehen diese Medaillen unter Umständen in einem Widerspruch zu gültigen Münzverordnungen.

Weiterhin gab es sogenannte Eurovorläufer, regional gültige Pseudo-Währungen.

Zudem wurden Entwürfe für dänische Euromünzen vorgestellt. Ihre hypothetische Einführung war für 2004 gedacht, jedoch lehnten die Dänen in einer Volksabstimmung im Jahr 2000 den Euro ab. Inzwischen entsprechen die Entwürfe nicht mehr den geltenden Gestaltungsrichtlinien.





</doc>
<doc id="1367" url="https://de.wikipedia.org/wiki?curid=1367" title="Eurobanknoten">
Eurobanknoten

Die Eurobanknoten bilden zusammen mit den Euromünzen das Bargeld des Euro. Die Eurobanknoten wurden am ersten Geltungstag, dem 1. Januar 2002, in Umlauf gebracht, während die Euromünzen in so genannten Starterkits bereits einige Tage zuvor ausgegeben wurden. Die Währung selbst wurde bereits am 1. Januar 1999 zunächst nur als Buchgeld eingeführt. Von den Eurobanknoten gibt es bisher zwei Serien. Bei der ersten Serie gibt es sieben Nennwerte, von der zweiten Serie sind bisher der 5-, der 10-, der 20- und der 50-Euro-Schein in den Verkehr gebracht worden. Am 4. Mai 2016 beschloss der Rat der Europäischen Zentralbank (EZB) die Abschaffung des 500-Euro-Scheines. Die 500-Euro-Banknoten werden aber noch bis Ende 2018 von der EZB ausgegeben.

Bereits am 15. November 1994 legte der Rat des Europäischen Währungsinstituts (EWI) die Stückelungen der neuen Banknoten fest: 5, 10, 20, 50, 100, 200 und 500 ECU. Erst über ein Jahr später wurde in Madrid der Name „Euro“ für die neue Währung vom Europäischen Rat festgelegt.

Der Gestaltungswettbewerb, an dem die von den nationalen Zentralbanken nominierten Grafiker und Designer-Teams teilnahmen, startete am 12. Februar 1996 und lief bis zum 13. September desselben Jahres. Die Dänische Zentralbank beteiligte sich nicht am Wettbewerb.

In den Jahren 1995/96 wurden die Vorgaben für den Wettbewerb erarbeitet. So wurden unter anderem die beiden Themen „Zeitalter und Stile in Europa“ und „abstraktes/modernes Design“ festgelegt. Des Weiteren sollten die Währungsbezeichnung und die Abkürzungen der ausgebenden Stelle die einzigen Wörter auf den Banknoten sein. Weitere wichtige Rahmenbedingungen bei der Gestaltung waren, dass die Geldscheine eindeutig „als Europäische Banknoten erkennbar sein sollten und eine kulturelle und politische Aussage enthalten, die alle Europäer anspricht“. Zudem mussten alle Entwürfe „die Gleichstellung von Mann und Frau berücksichtigen und jede Art nationaler Voreingenommenheit vermeiden“. Vorgegeben waren u. a. auch die Größe der einzelnen Banknote und deren Grundfarben, die Epochen für das Thema „Zeitalter und Stile“ und die zwölf Sterne der EU, die auf der Vorderseite abgebildet sein mussten. Auf der Rückseite war die Nutzung der Sterne optional.

Am 26./27. September wählte ein Expertengremium zu beiden Themen die jeweils fünf besten Entwürfe aus. Es folgte eine Umfrage unter knapp 2000 EU-Bürgern, bevor der Rat des EWI die von Robert Kalina gestalteten Entwürfe als Gewinner bestimmte. Zur Begründung heißt es: „weil sie historische Entwicklungen in den Bereichen Technik, Kunst und Kommunikation in einer harmonischen Darstellung vereinen; sie stehen stellvertretend für den Beginn eines neuen Europa mit einem gemeinsamen kulturellen Erbe und der Vision einer gemeinsamen Zukunft im neuen Jahrtausend“. Die auf der Vorderseite der Banknoten abgebildeten Tore und Fenster symbolisieren „den Geist der Offenheit und der Zusammenarbeit“. Die auf den Rückseiten abgebildeten Brücken symbolisieren die „Verbundenheit zwischen den Völkern Europas und zwischen Europa und der übrigen Welt“.

Die Scheine zeigen fiktive Motive der europäischen Architektur, jeweils aus verschiedenen kunstgeschichtlichen Epochen. Es wurde bewusst darauf verzichtet, reale Personen oder Bauwerke darzustellen, um zu vermeiden, dass sich – auch unabsichtlich – einzelne Eurostaaten bevorzugt oder benachteiligt fühlen.

Die erstmalige Präsentation der Entwürfe erfolgte am 13. Dezember auf Pressekonferenzen in Dublin und Frankfurt. Am 30. August 2001 stellte Wim Duisenberg, der damalige Präsident der EZB, in Frankfurt das endgültige Erscheinungsbild der Eurobanknoten vor.

Schon vor Einführung des Euro gab es Diskussionen um kleinere Stückelungen. Insbesondere Italien hatte sich für einen Ein-Euro-Schein starkgemacht. Im Oktober 2003 kam es zu einem neuerlichen Vorstoß des italienischen Finanzministers Giulio Tremonti, der durch seinen österreichischen Amtskollegen Karl-Heinz Grasser unterstützt wurde. Der Gedanke war, die Menge der umlaufenden Münzen zu verringern und die Inflation zu senken, da viele Menschen den Wert von Gütern als zu gering einschätzen würden, solange sie mit Münzen bezahlen. Die Europäische Zentralbank, die allein über die Stückelung der Banknoten entscheidet, hatte im November 2004 nach Auswertung einer Studie beschlossen, keine Ein-Euro-Banknote auszugeben. Die Forderung nach einer Banknote im Wert von zwei Euro wurde nach dieser Entscheidung nicht weiterverfolgt. Im Mai 2012 wurde erneut ein Vorstoß unternommen, die Einführung durch die Zentralbank prüfen zu lassen. Die Überlegung kam im Rahmen der Diskussion über die Abschaffung von 1- und 2-Cent-Münzen auf.

Abschaffung der 500-Euro-Banknote

Im April 2013 wurden Erwägungen um die Abschaffung des größten Euro-Geldscheines, der 500-Euro-Banknote, laut. Der Analyst der Bank of America, Athanasios Vamvakidis, schrieb, dass diese Banknoten oft zum „Matratzen-Geld“ würden. Eine Studie der EZB besagt, dass nur etwa 30 % der umlaufenden 500-Euro-Banknoten für Zahlungen genutzt werden und eine weitere Studie der britischen Regierung geht davon aus, dass 90 % der im Land gehandelten Scheine dieser Größenordnung in den Händen des organisierten Verbrechens seien. Der EZB-Vizepräsident Vítor Constâncio bestätigte in Brüssel, dass die Abschaffung sicherlich eine Diskussion wert sei.

Im Februar 2016 kam die Debatte um die Abschaffung des 500-Euro-Scheins wieder auf. Der Rat der EZB beauftragte den Banknotenausschuss, technische Details für einen möglichen Einzug des 500ers zu klären. EZB-Chef Mario Draghi argumentierte, dass der 500-Euro-Schein zunehmend für kriminelle Aktivitäten wie Geldwäsche verwendet werde.
Am 4. Mai 2016 beschloss der EZB-Rat, welcher aus sechs Direktoriumsmitgliedern und den Notenbankchefs der 19 Euro-Länder besteht, gegen die Stimmen von Deutschland, Österreich und Estland die Abschaffung dieser Banknote. Viele Kritiker dieses Vorgehens meinen, dass dadurch die Kriminalität nicht eingeschränkt werden könne und andere Gründe wie die leichtere Einführung von Negativzinsen hinter dieser Entscheidung stünden. Es wird befürchtet, dass dies ein Schritt zur Abschaffung des Bargeldes sei. Seitens der EZB und nationaler Notenbanken wird dies dementiert. So habe sich der EZB-Rat klar für den Erhalt der 100- und 200-Euro-Banknoten ausgesprochen. Draghi äußerte aber bereits im Februar, dass man auch die 200-Euro-Noten abschaffen könnte.

Es werden zwar seit 2014 keine 500-Euro-Noten mehr gedruckt, der Schein wird aber voraussichtlich noch bis Ende 2018, wenn die Einführung der zweiten Serie abgeschlossen sein soll, von den Notenbanken ausgegeben. Seine Gültigkeit im Zahlungsverkehr sollte der Schein erst Jahre später – zusammen mit den anderen Banknoten der ersten Serie – verlieren. Bei den Notenbanken kann er zeitlich unbegrenzt eingetauscht werden.

Nach Angaben der Europäischen Zentralbank waren im November 2014 circa 16,7 Mrd. Eurobanknoten mit einem Gesamtwert von 981 Mrd. Euro im Umlauf. Häufigste Banknote ist die zu 50 Euro; auch wertmäßig entfällt auf sie mit knapp über einem Drittel des im Umlauf befindlichen Wertes der größte Anteil. Ein weiteres Drittel des Bargeldumlaufs deckt die 500-Euro-Note ab, die nach der 200-Euro-Note die zweitseltenste ist. Die Euromünzen machen mit 24,730 Mrd. Euro nur 2,5 % des gesamten Bargeldumlaufs (aus Noten und Münzen) von 1.005,63 Mrd. Euro aus.

Die Euro-Banknoten der ersten Serie gibt es in Stückelungen zu 5, 10, 20, 50, 100, 200 und 500 Euro.

Die Banknoten wurden nach einem EU-weiten Wettbewerb, an dem 29 Designer teilnahmen und 44 Entwürfe einreichten, vom Österreicher Robert Kalina gestaltet.

Die Vorderseite (oder "recto") stellt ein oder mehrere Fenster oder Tore dar, während auf der Rückseite (oder "verso") eine Brücke zu sehen ist, die die Verbindung der einzelnen Länder innerhalb der Europäischen Union symbolisieren soll. Dabei sind keine realen Bauwerke abgebildet, sondern eine Zusammenstellung aus Stilmerkmalen einzelner Epochen in einer archetypischen Abbildung (entsprechende Brücken wurden später in der niederländischen Stadt Spijkenisse nachgebaut). Auf der Rückseite sind das europäische Festland, Nordafrika sowie ein Teil des asiatischen Gebietes der Türkei abgebildet. Außerdem sind mit den Azoren, Madeira, Französisch-Guayana, Guadeloupe, Martinique, Réunion, Kanarische Inseln auch Inseln und Territorien abgebildet, in denen der Euro offizielles Zahlungsmittel ist. Es fehlen Malta, aufgrund des gewählten Maßstabes, und Zypern, das derzeit östlichste Land der EU. Diese beiden Länder waren zum Zeitpunkt der Euro-Einführung noch nicht Mitglied der EU.

Allen Noten gemeinsam sind die europäische Flagge, die Abkürzung der Europäischen Zentralbank in den verschiedenen Arbeitssprachen der EU im Jahr 2002, die Jahreszahl der Erstausgabe (2002), die Unterschrift des amtierenden EZB-Präsidenten, eine Europakarte (inklusive der französischen Überseedépartements) auf der Rückseite, beidseitig der Name „Euro“ in lateinischen („EURO“) und griechischen Buchstaben („ΕΥΡΩ“) der Schriftart Frutiger.

Die fünf Abkürzungen der Europäischen Zentralbank sind: BCE (Französisch, Irisch, Italienisch, Portugiesisch, Spanisch), ECB (Dänisch, Englisch, Niederländisch, Schwedisch), EZB (Deutsch), ΕΚΤ (Griechisch; Buchstaben Epsilon, Kappa und Tau des griechischen Alphabets) sowie EKP (Finnisch)

Da am 1. November 2003 Wim Duisenberg seinen Präsidentenposten an Jean-Claude Trichet abgab, wechselte auf den nachfolgend gedruckten Scheinen auch die Unterschrift, aber nicht die Jahreszahl "2002". Banknoten mit der Unterschrift von Mario Draghi kamen im März 2012 erstmals in Umlauf.
In dieser Serie wurden keine 5-Euro-Banknoten mit der Unterschrift von Mario Draghi ausgegeben.


Die Euro-Banknoten weisen verschiedene Sicherheitsmerkmale auf, mit denen Fälschungen verhindert oder zumindest erschwert werden sollen. Weiterhin soll mit der sogenannten „EURion-Konstellation“ das Vervielfältigen durch Kopierer oder Scanner verhindert werden.





Bisher sind keine gefälschten Eurobanknoten bekannt, bei denen der Farbwechsel der Wertzahl vollständig nachgeahmt werden konnte. Entweder schillert die Farbe beim Kippen nur auf oder wechselt nicht vollständig zu Braun, sondern zu Dunkelrot bzw. Dunkellila (gilt für eine sehr gute Fälschung eines 200-Euro-Scheines aus Bulgarien). Der Unterschied zum echten Farbwechsel ist sehr auffällig und leicht zu erkennen.

Hält man das Farbelement schräg gegen das Licht, wird bei waagerechter Haltung ein grüner Schimmer sichtbar, der den vollständigen Farbwechsel bestätigt. Auf allen Banknoten ab 50 Euro vollzieht sich der gleiche Farbwechsel, sodass man anhand einer Vergleichsbanknote den Farbwechsel bei allen Scheinen überprüfen kann. Nicht selten wird der Farbwechsel auf Falschgeld gar nicht imitiert, sondern die Wertzahl ist nur in Lila aufgedruckt.

Es gibt auf den Euro-Banknoten auch versteckte Sicherheitsmerkmale, so ein als "M-Feature" ("M" für "maschinenlesbar") bezeichnetes Merkmal, eine Beschichtung mit einem Oxidgemisch verschiedener Lanthanoide, das mit Hilfe starker Lichtblitze ausgelesen eine charakteristische Antwort liefert. Diese Sicherheitsmerkmale werden automatisiert in den Filialen der nationalen Zentralbanken des Eurosystems überprüft. Bisher konnte dieser Test Fälschungen sicher erkennen. Jede Banknote soll im Durchschnitt alle drei Monate in einer Zentralbankfiliale auf diese Merkmale überprüft und so der Umlauf von Falschgeld entdeckt und unterdrückt werden. Bisher konnten allerdings alle Fälschungen auch anhand der bekannten Sicherheitsmerkmale erkannt werden.

Das Fälschen der Banknoten ist verboten. Das Copyrightzeichen © weist auf die Beanspruchung von Schutzrechten bezüglich der Vervielfältigung hin. Bei der Verwendung des Copyrightzeichens sind nach ISO 16016 noch die Angabe des Rechteinhabers (EZB) und die Jahreszahl der Erstveröffentlichung (2002) nötig.

Bis 2003 stieg die Fälschungsrate der Eurobanknoten deutlich an, sodass an einer Erweiterung der Sicherheitsmerkmale gearbeitet wird, unter Umständen die Integration von elektronischen Chips zur Identifikation der Banknote, auch wenn die Anzahl der Fälschungen, die dem Banknotenumlauf entnommen wurden, seit 2004 nur noch geringfügig steigt.

Jeder nationalen Zentralbank (NZB) des Eurosystems wurde ein individueller Kennbuchstabe zugeteilt, der als erstes Zeichen der Seriennummer auf allen Banknoten erscheint, deren Druck die nationale Zentralbank in Auftrag gegeben hat. Ein Buchstabe einer bestimmten NZB bedeutete dabei ursprünglich (bei der Grundausstattung zur Euroeinführung 2002), dass diese NZB den Schein auch in ihrem Zuständigkeitsbereich in Umlauf gebracht hat. Bei späteren Ausgaben kann es auch bedeuten, dass die NZB die Banknoten im Rahmen des dezentralen Poolingverfahrens der Banknotenherstellung einer anderen Zentralbank für die Ausgabe in deren Zuständigkeitsbereich zur Verfügung gestellt hat. Banknoten mit dem Buchstaben einer NZB wurden auch nicht immer von einer nationalen Druckerei gedruckt, siehe Druckereikennung. Die Buchstaben W, K und J wurden bisher nicht genutzt, sie sind für EU-Staaten reserviert, die derzeit (Stand 1. Januar 2002) nicht am Euro teilnehmen. Nach der Euro-Einführung in Estland wurde der nächste „freie“ Buchstabe D vergeben, die Buchstaben A, B und C sind derzeit noch unbelegt.

Dem NZB-Buchstaben folgen eine zehnstellige Nummer und eine Prüfziffer (1–9). Die Prüfziffer ist dabei so gewählt, dass folgende Prüfbedingung erfüllt ist: Ersetzt man den Buchstaben der Seriennummer durch seinen ASCII-Wert (A = 65 … Z = 90), so ergibt sich insgesamt eine Zahl, die durch 9 teilbar ist, der Neunerrest ist also 0. Dies lässt sich einfach prüfen, indem man die wiederholte Quersumme der Ziffernfolge bildet, diese ergibt beim Neunerrest 0 immer "9" (ansonsten sind Neunerrest und wiederholte Quersumme identisch). Dies gilt offenbar nicht nur bei den Banknoten der ersten Serie; auch alle bei der Vorstellung der zweiten Serie zu sehenden Seriennummern lassen sich so prüfen, indem man hier beide Buchstaben durch ihren jeweiligen ASCII-Wert ersetzt.

Es existieren auch andere, dazu äquivalente Prüfverfahren: Statt des ASCII-Werts kann man auch die Zuordnung A = 2 … Z = 27 oder A = 11 … Z = 36 benutzen. Ersetzt man den Buchstaben durch seine Position im Alphabet (A = 1 … Z = 26), so ist bei Banknoten der ersten Serie der Neunerrest grundsätzlich "8". Bildet man den Neunerrest nur der elf Ziffern der Seriennummer ohne den Buchstaben, so ergibt sich ein Wert, der der folgenden Tabelle zu entnehmen ist.

Die Erfahrung zeigt zwar, dass viele Fälscher falsche Prüfziffern auf ihre Scheine drucken. Aber auch bei einer willkürlichen Nummernvergabe einschließlich der als Prüfziffer nie verwendeten „0“ hätten 10 % der gefälschten Banknoten eine korrekte Prüfziffer. Deshalb können Scheine mit einer ungültigen Prüfziffer zwar als gefälscht erkannt werden. Eine gültige Prüfziffer stellt umgekehrt aber nur ein notwendiges, jedoch kein hinreichendes Kriterium für die Echtheit eines Scheines dar. Zur Echtheitskontrolle sollte man sich daher zusätzlich anderer Methoden bedienen (siehe unten).


Beim Druck der Eurobanknoten wurde nicht in allen Ländern die laufende Seriennummer, beginnend mit *0000000001* und streng aufsteigend, angebracht. Da immer mehrere Banknoten gleichzeitig gedruckt werden (20 bis 60 Scheine auf einer Druckplatte), sind Teile der Seriennummer zur Anzeige der Position des Scheins auf dem ungeschnittenen Druckbogen vorgesehen. Andere Nummerierungen sehen Nummernkreise für die verschiedenen Wertstufen vor. Diese Bereiche sind je nach auftraggebendem Land an unterschiedlichen Stellen in der Seriennummer untergebracht und schränken den verfügbaren Nummernraum ein:

Schlüssel zu den Buchstaben:

D: Nummernkreis für den Wert des Scheins, wobei z. B. bei der Seriennummer alle 5-Euro-Scheine mit einer 1 anfangen, alle 10-Euro-Noten mit einer 2 usw., oder 063 bis 076 für 5 Euro, 077 bis 096 für 10 Euro usw. Länder, die eine solche Systematik benutzen, können maximal nur 10 Mrd. Banknoten herausgeben.

N: Fortlaufende Nummer. Diese Nummer wird für jede Wertstufe einzeln von …0001 an lückenlos aufsteigend verwendet. Beispielsweise wurde ein Schein mit fortlaufender Nummer "5678912345*" später gedruckt als ein Schein desselben Wertes mit der Nummer "1234567891*".

P: Position des Scheins auf dem Druckbogen, wobei die verschiedenen möglichen Positionen fortlaufend nummeriert werden.

X: Position des Scheines auf der x-Achse der Druckplatte.

Y: Position des Scheines auf der y-Achse der Druckplatte.

Von den mit diesen Systemen möglichen über 500 Mrd. Nummernkombinationen waren Ende 2008 schon 49,63 Mrd. verbraucht. Das System erschöpft sich aufgrund der signifikant unterschiedlichen Auflagen der verschiedenen Herausgeber aber schon viel eher. So wurden bis zur Einführung der neuen Serie über 8,2 Mrd. Seriennummern deutscher 10-Euro-Banknoten verbraucht, die deutschen 50-Euro-Scheine sind im Jahr 2015 schon mit Seriennummern bis X96 im Umlauf. Dort wurde also bereits fast der gesamte Nummernraum ausgeschöpft.

Etwas versteckt auf der Vorderseite befindet sich eine weitere kurze Zeichenfolge, der "Plattencode", wobei der erste Buchstabe die Druckerei kennzeichnet, die die Banknote hergestellt hat. Diese "Druckereikennung" lässt nicht zwangsläufig auf die NZB-Kennung schließen, denn Banknoten, die von einem bestimmten Land herausgegeben wurden, können in einem anderen Land gedruckt worden sein. Deutschland, Großbritannien und Frankreich haben jeweils zwei Druckereien in der Codeliste. Die Codes A, C und S wurden für Druckereien reserviert, die derzeit keine Eurobanknoten herstellen. Die drei folgenden Ziffern geben die Nummer der Druckplatte bzw. die Serie an. Die letzten beiden Stellen dieser kurzen Zeichenfolge geben die relative "Position der Banknote" auf dem Druckbogen an, also von A1 (oben links) bis J6 (unten rechts), je nach Größe des Druckbogens.

Beispiel:

Druckereikennung

Die Buchstaben B, I, O, Q sind nicht vergeben. Die Vergabe der Buchstaben erfolgte ähnlich wie bei den Seriennummern, invers alphabetisch bezüglich des Landes, für das die Druckerei voraussichtlich Banknoten drucken würde. Die Druckerei Valora in Portugal kam erst später dazu und bekam damit das U.

Von den angeführten Druckereien haben die beiden deutschen den größten Anteil am Druckvolumen bewältigt. Von den 14,8899 Mrd. zum 1. Januar 2002 hergestellten Euro-Banknoten stammen 4,7829 Mrd. von der deutschen NZB, der Deutschen Bundesbank. Auch von den 51,613 Mrd. Euromünzen der Erstproduktion stammt rund ein Drittel, nämlich 17 Mrd., aus Deutschland.

Am 2. November 2006 wurde in der Öffentlichkeit bekannt, dass schätzungsweise tausend Banknoten chemisch so behandelt wurden, dass sie bei Kontakt mit Feuchtigkeit (beispielsweise Schweiß auf der Hand) anfangen, sich aufzulösen.
Erstmals aufgetreten ist dieser Effekt im Juni und Juli 2006 in der Region Berlin/Potsdam. Es kann nicht ausgeschlossen werden, dass die Ursache eine mutwillige chemische Behandlung war. Bei der Chemikalie handelt es sich (höchstwahrscheinlich) um Sulfate, mit denen die Scheine bepudert sind. Dies konnte jedoch nicht verifiziert werden. Es steht mit Sicherheit fest, dass die Scheine echt sind, es sich also nicht um Fälschungen handelt.

Als Konservierungsstoffe wurden den humanen Hormonhaushalt schädigende metallorganische Verbindungen des Zinns (zum Beispiel Tributylzinnhydrid) eingesetzt, was Ärzte kritisch sehen.
Die EZB gibt an, in neuen Scheinen seit 2002 kein TBT mehr einzusetzen.

2005 begann die Entwicklung einer zweiten Generation von Euro-Banknoten. Zunächst wurden geeignete Sicherheitsmerkmale aus rund 200 auf dem Markt befindlichen Echtheitsbeweisen festgelegt. Bereits seit 2008 publizierte die EZB in größeren Intervallen, dass eine zweite Eurobanknoten-Serie in Arbeit sei. Die ursprünglich genannten Ausgabetermine von erst 2010 und hiernach 2011 konnten aufgrund nötiger Weiterentwicklungen gegen Falsifikate von Banknoten nicht eingehalten werden.

Am 9. November 2012 gab die Europäische Zentralbank in einer Mitteilung des Präsidenten Draghi bekannt, dass ab dem Jahr 2013 eine neue Banknotenserie, die so genannte „Europa-Serie“ eingeführt werde. Am 10. Januar 2013 wurden die neuen Fünf-Euro-Noten präsentiert, sie sind seit dem 2. Mai 2013 in Umlauf. Die 10-Euro-Note folgte am 13. Januar 2014; in den Zahlungsverkehr gelangte sie am 23. September 2014. Die 20-Euro-Note wurde am 24. Februar 2015 erstmals gezeigt und am 25. November 2015 eingeführt. Die neue 50-Euro-Note wurde am 5. Juli 2016 vorgestellt und am 4. April 2017 eingeführt. Die Einführung der 100- und 200-Euro-Noten war zunächst für Ende 2018 zeitgleich mit der Einstellung der Ausgabe der 500-Euro-Noten geplant. Mittlerweile wird von der EZB ein Zeitpunkt Anfang 2019 für die Einführung genannt. 

Die zweite Serie der Eurobanknoten sollte ursprünglich, wie die erste Serie, die Nennwerte 5 €, 10 €, 20 €, 50 €, 100 €, 200 € und 500 € umfassen. Da jedoch mittlerweile die Abschaffung der 500-Euro-Note beschlossen wurde, wird es bei der neuen Serie nur die Stückelungen von 5 € bis 200 € geben.

Mit der Gestaltung der zweiten Serie der Euro-Banknoten wurde Reinhold Gerstetter beauftragt, der schon die letzte Banknoten-Serie der Deutschen Mark gestaltet hat. Die Stückelungen, die Hauptfarben sowie das Leitmotiv „Zeitalter und Stile“ bleiben erhalten, jedoch werden neue und verbesserte Sicherheitsmerkmale eingeführt. Gleichzeitig wurden bereits einzelne Sicherheitsmerkmale präsentiert:


Seit dem Beitritt Bulgariens zur EU wird die Währungsbezeichnung "EURO" neben der bisherigen Benennung in lateinischen und griechischen Buchstaben ("EYPΩ") nun auch in der kyrillischen Schreibweise „ЕВРО“ aufgeführt; das kyrillische „В“ entspricht dem lateinischen V oder W, das „Р“ dem R. Die Scheine tragen die Jahreszahl der Erstausgabe.

Die neuen Euro-Noten halten weiterhin das Prinzip hoch, dass der Euro grundsätzlich die Währung für alle EU-Mitglieder werden soll. Und gestalterisch haben sie eine mögliche Erweiterung der heutigen Euro-Zone daher bereits vorweggenommen. Auf den Scheinen sind folglich alle EU-Mitglieder repräsentiert, nicht nur die Euro-Staaten. Daher kommen nun Hinweise auf jene Länder hinzu, die bisher nicht vertreten waren, weil sie zum Zeitpunkt der Einführung des Euro noch nicht Mitglied der EU waren.

Die Akronyme der Europäischen Zentralbank wurden auch um die Schreibweisen in den neuen EU-Sprachen ergänzt: zu den fünf bisherigen Abkürzungen BCE (Französisch, Irisch, Italienisch, Portugiesisch, Rumänisch, Spanisch), ECB (Dänisch, Englisch, Lettisch, Litauisch, Niederländisch, Schwedisch, Slowakisch, Slowenisch, Tschechisch), EKP (Estnisch, Finnisch), ΕΚΤ (Griechisch) sowie EZB (Deutsch) kamen neu die Varianten BĊE (Maltesisch), EBC (Polnisch), EKB (Ungarisch), ESB (Kroatisch) und ЕЦБ (Bulgarisch; in kyrillischen Buchstaben) hinzu. Auf den Scheinen stehen sie in der Abfolge BCE, ECB, ЕЦБ, EZB, EKP, ΕΚΤ, (ESB), EKB, BĊE, EBC. Die Abkürzung ESB (Kroatisch) erscheint nicht auf den Banknoten von 5 bis 20 Euro. Die Reihenfolge orientiert sich in etwa an der von der EU festgelegten protokollarischen Reihenfolge für Amtssprachen; nur die bulgarische Abkürzung steht an dritter statt an erster Stelle.

Zusätzlich sind nun auch die Staaten Malta und Zypern auf der Europakarte abgebildet, welche 2004 der EU beigetreten sind und 2008 den Euro eingeführt haben.

Der Plattencode befindet sich auf allen (bisher vorgestellten) Banknoten rechts am oberen Bildrand. Eine Oberflächenbeschichtung soll die Haltbarkeit der neuen Scheine gegenüber den alten erhöhen, dies führt zu einem leicht erhöhten Gewicht und geringfügig gestiegenen Produktionskosten. Wie im Oktober 2014 bekannt wurde, lassen sich auf dieser neu eingeführten Lackierung mit bisherigen Methoden kaum Fingerabdrücke nachweisen.

Gegenüber der ersten Euro-Serie wurden die vorhandenen Sicherheitsmerkmale verbessert und weiterentwickelt, jedoch wurde auf den Wasserzeichen-Strichcode und das Durchsichtsregister verzichtet.

Die neuen verbesserten Sicherheitsmerkmale beinhalten:

Im Gegensatz zur Vorgängerserie gibt es auf den Banknoten der Europa-Serie keinen Hinweis mehr auf die auftraggebende Zentralbank, nur noch die Druckerei ist gekennzeichnet.

Die Seriennummern der Europaserie beginnen mit zwei Buchstaben, gefolgt von einer Folge von zehn Ziffern. Diese (vollständige) Seriennummer wird als Langform bezeichnet, während eine Kurzform dieser Nummer in Form der letzten sechs Ziffern der Langform um 90° gedreht auf der Note hinzugefügt wurde. Die erste Stelle der Seriennummer gibt dabei die Druckerei an, in der die Banknote hergestellt wurde, während die zweite Stelle wie die folgenden zehn Ziffern zur eindeutigen Kennzeichnung der Banknote innerhalb der Ausgaben einer Druckerei gehört. Werden die zwei Buchstaben durch ihre Position im Alphabet (A = 1 … Z = 26) ersetzt, so ist bei Banknoten der zweiten Serie der Neunerrest mit den zehn Ziffern grundsätzlich 7. Bildet man den Neunerrest nur der zehn Ziffern der Seriennummer ohne die beiden Buchstaben, so ergibt sich ein Wert, der der folgenden Tabelle zu entnehmen ist. Der Neunerrest wird zusätzlich in der unteren Tabelle (Spalte: Neunerrest) im Bedarfsfall mit angegeben.

Wie schon bei der ersten Euro-Banknotenserie befindet sich auf der Vorderseite der Plattencode. Auch dessen Aufbau hat sich nicht geändert, nur werden die neu zugeteilten Druckereikennungen verwendet.

Jeder Druckerei der Eurobanknoten wurde ein individueller Kennbuchstabe zugeteilt, der in der Seriennummer und im Plattencode vorhanden ist:
Obwohl die Deutsche Bundesbank nach eigenen Angaben den Herstellern von Automaten frühzeitig angeboten hatte, die neuen 5-Euro-Scheine zu testen, kam es nach der Ausgabe der Scheine zu größeren Problemen bei der Akzeptanz der Scheine durch Automaten. Die Deutsche Bahn etwa musste einräumen, dass wegen eines fehlenden Software-Updates ungefähr die Hälfte der Automaten die neuen Scheine nicht akzeptiere. Viele Aufsteller von Zigarettenautomaten versahen ihre Geräte mit einem Zusatzschild, welches auf die spätere Akzeptanz des neuen Scheins „vertröstete“, aber auch Parkhausbetreiber und nahezu alle anderen Aufsteller von Automaten mit Akzeptanz von Geldscheinen waren von dem Problem betroffen. Selbst einen Monat nach der Einführung der neuen Scheine hatte sich die Situation nur geringfügig verbessert. Dies ist ein Grund dafür, warum beim 10-Euro-Schein die Zeit zwischen Präsentation und Einführung deutlich verlängert wurde. Den Automatenaufstellern sollte mehr Zeit zur Umstellung bleiben.

Möchte ein Bargeldakteur, also Kreditinstitute oder Wertdienstleister, Bargeld ohne Beteiligung der zuständigen nationalen Zentralbank (in Deutschland die Deutsche Bundesbank und in Österreich die Oesterreichische Nationalbank) in den Umlauf geben, so ist er dazu verpflichtet, vorher die Umlauffähigkeit und die Echtheit der Noten und Münzen zu überprüfen.

In Unicode 6.0 wurde im Unicodeblock Verschiedene piktografische Symbole auf Position U+1F4B6 das Zeichen (Emoji) „💶“ eingeführt, das eine „Banknote mit Eurozeichen“ darstellt.

Seit 2015 gibt es den 0-Euro-Schein in verschiedenen Designs. Er ist als Sammlerstück gedacht und kann – logischerweise – nicht als Zahlungsmittel verwendet werden. Der Schein wurde von der Europäischen Zentralbank als Euro-Souvenir genehmigt. Der 0-Euro-Schein ist so groß wie der 20-Euro-Schein und ist mit Sicherheitsmerkmalen versehen. Da er nicht im normalen Geldumlauf erscheint, kann man den Schein an derzeit 20 Verkaufsstellen in Deutschland erwerben.



</doc>

<doc id="1" url="https://de.wikipedia.org/wiki?curid=1" title="Alan Smithee">
Alan Smithee

Alan Smithee steht als Pseudonym für einen fiktiven Regisseur, der Filme verantwortet, bei denen der eigentliche Regisseur seinen Namen nicht mit dem Werk in Verbindung gebracht haben möchte. Von 1968 bis 2000 wurde es von der Directors Guild of America (DGA) für solche Situationen empfohlen, seither ist es Thomas Lee. "Alan Smithee" ist jedoch weiterhin in Gebrauch.

Alternative Schreibweisen sind unter anderem die Ursprungsvariante "Allen Smithee" sowie "Alan Smythee" und "Adam Smithee". Auch zwei teilweise asiatisch anmutende Schreibweisen "Alan Smi Thee" und "Sumishii Aran" gehören – so die Internet Movie Database – dazu.

Das Pseudonym entstand 1968 infolge der Arbeiten am Western-Film "Death of a Gunfighter" (deutscher Titel "Frank Patch – Deine Stunden sind gezählt"). Regisseur Robert Totten und Hauptdarsteller Richard Widmark gerieten in einen Streit, woraufhin Don Siegel als neuer Regisseur eingesetzt wurde.

Der Film trug nach Abschluss der Arbeiten noch deutlich Tottens Handschrift, der auch mehr Drehtage als Siegel daran gearbeitet hatte, weshalb dieser die Nennung seines Namens als Regisseur ablehnte. Totten selbst lehnte aber ebenfalls ab. Als Lösung wurde "Allen Smithee" als ein möglichst einzigartiger Name gewählt (bei der späteren Variante "Alan Smithee" war das Anagramm "The Alias Men" vermutlich kein Entstehungsgrund).

In den zeitgenössischen Kritiken wurde der Regisseur u. a. von Roger Ebert mit den Worten gelobt: 

1997 kam die Parodie "An Alan Smithee Film: Burn Hollywood Burn" (deutscher Titel "Fahr zur Hölle Hollywood") in die Kinos, was das Pseudonym einem größeren Publikum bekannt machte, nicht zuletzt weil Arthur Hiller, der eigentliche Regisseur des Films, selbst seinen Namen zurückzog und analog zum Filmtitel das Pseudonym "Alan Smithee" benutzte. Der Film gilt als einer der schlechtesten Filme der 1990er Jahre und gewann fünf Goldene Himbeeren.

Der Film "Supernova" ist der erste Post-Smithee-Film, dort führte ein gewisser "Thomas Lee" alias Walter Hill die Regie.
Die Verwendung dieses oder eines anderen Pseudonyms ist für Mitglieder der DGA streng reglementiert. Ein Regisseur, der für einen von ihm gedrehten Film seinen Namen nicht hergeben möchte, hat nach Sichtung des fertigen Films drei Tage Zeit, anzuzeigen, dass er ein Pseudonym verwenden möchte. Der Rat der DGA entscheidet binnen zwei Tagen über das Anliegen. Erhebt die Produktionsfirma Einspruch, entscheidet ein Komitee aus Mitgliedern der DGA und der Vereinigung der Film- und Fernsehproduzenten, ob der Regisseur ein Pseudonym angeben darf. Über die Beantragung muss der Regisseur Stillschweigen halten, ebenso darf er den fertigen Film nicht öffentlich kritisieren, wenn die DGA ihm die Verwendung eines Pseudonyms zugesteht. Ein Antrag des Regisseurs auf Pseudonymisierung kann abgelehnt werden, so durfte Tony Kaye den Namen Smithee bei dem Film "American History X" nicht einsetzen, obwohl er den Antrag stellte.

Auch bei nicht-US-amerikanischen Produktionen wird der Name verwendet, wie etwa beim Pilotfilm der Fernsehserie "Schulmädchen". 2007 sendete die ARD am 8. und 9. August den zweiteiligen TV-Film "Paparazzo". Auch in diesem Werk erscheint anstatt des eigentlichen Regisseurs Stephan Wagner Alan Smithee im Abspann.

Regisseure, die das Pseudonym benutzt haben:

Der Pilotfilm der Serie "MacGyver" und die fünfte Folge der ersten Staffel führen einen Alan Smithee als Regisseur. Auf der TV-Serien-Seite "TV Rage" wird Jerrold Freedman als Regisseur des Pilotfilms angegeben. Der Regisseur der fünften Folge ist unbekannt.

Zu den Drehbuchautoren, die das Pseudonym benutzt haben, gehören Sam Raimi und Ivan Raimi, die das Drehbuch zu "Die total beknackte Nuß" als "Alan Smithee, Jr." und "Alan Smithee, Sr." schrieben.

Auch in Computerspielen wird dieses Pseudonym angegeben: Im Abspann des Ego-Shooters "Marine Sharpshooter IV" aus dem Jahr 2008 wird als Art Director des Spiels "Alan Smithee" genannt.

2014 produzierte die New Yorker Performance-Kompanie Big Dance Theater "Alan Smithee Directed this Play", das im August des Jahres auch in Berlin bei Tanz im August aufgeführt wurde.




</doc>
<doc id="3" url="https://de.wikipedia.org/wiki?curid=3" title="Actinium">
Actinium

Actinium ist ein radioaktives chemisches Element mit dem Elementsymbol Ac und der Ordnungszahl 89. Im Periodensystem der Elemente steht es in der 3. IUPAC-Gruppe, der Scandiumgruppe. Das Element ist ein Metall und gehört zur 7. Periode, d-Block. Es ist der Namensgeber der Gruppe der Actinoide, der ihm folgenden 14 Elemente.

Das Actinium wurde im Jahr 1899 von dem französischen Chemiker André-Louis Debierne entdeckt, der es aus Pechblende isolierte und ihm zunächst Ähnlichkeiten mit dem Titan oder dem Thorium zuschrieb; seine Bezeichnung leitete er wegen der Radioaktivität von griechisch ἀκτίς "aktís" ‚Strahl‘ ab. Friedrich Giesel entdeckte das Element unabhängig davon im Jahr 1902 und beschrieb eine Ähnlichkeit zum Lanthan; er gab ihm den Namen Emanium, eine Bildung zu lateinisch "emano" ‚ausfließen‘, ebenfalls mit Bezug zur abgegebenen Strahlung. Nachdem Actinium und Emanium im Jahre 1904 als identisch erkannt worden waren, wurde Debiernes Namensgebung der Vorzug gegeben, da er es zuerst entdeckt hatte.

Die Geschichte der Entdeckung wurde in Publikationen von 1971 und später im Jahr 2000 immer noch als fraglich beschrieben. Sie zeigen, dass die Publikationen von 1904 einerseits und die von 1899 und 1900 andererseits Widersprüche aufweisen.

Da in Uranerzen nur wenig Actinium vorhanden ist, spielt diese Quelle keine Rolle für die Gewinnung. Technisch wird das Isotop Ac durch Bestrahlung von Ra mit Neutronen in Kernreaktoren hergestellt.

Durch den schnellen Zerfall des Actiniums waren stets nur geringe Mengen verfügbar. Die erste künstliche Herstellung von Actinium wurde im Argonne National Laboratory in Chicago durchgeführt.

Das Metall ist silberweiß glänzend und relativ weich. Aufgrund seiner starken Radioaktivität leuchtet Actinium im Dunkeln in einem hellblauen Licht.

Actinium ist das namensgebende Element der Actinoiden, ähnlich wie Lanthan für die Lanthanoiden. Die Gruppe der Elemente zeigt deutlichere Unterschiede als die Lanthanoide; daher dauerte es bis 1945, bis Glenn T. Seaborg die wichtigsten Änderungen zum Periodensystem von Mendelejew vorschlagen konnte: die Einführung der Actinoide.

Es ist sehr reaktionsfähig und wird von Luft und Wasser angegriffen, überzieht sich aber mit einer Schicht von Actiniumoxid, wodurch es vor weiterer Oxidation geschützt ist. Das Ac-Ion ist farblos. Das chemische Verhalten von Actinium ähnelt sehr dem Lanthan. Actinium ist in allen zehn bekannten Verbindungen dreiwertig.

Bekannt sind 26 Isotope, wovon nur zwei natürlich vorkommen. Das langlebigste Isotop Ac (Halbwertszeit 21,8 Jahre) hat zwei Zerfallskanäle: es ist ein Alpha- und Beta-Strahler. Ac ist ein Zerfallsprodukt des Uranisotops U und kommt zu einem kleinen Teil in Uranerzen vor. Daraus lassen sich wägbare Mengen Ac gewinnen, die somit ein verhältnismäßig einfaches Studium dieses Elementes ermöglichen. Da sich unter den radioaktiven Zerfallsprodukten einige Gammastrahler befinden, sind aber aufwändige Strahlenschutzvorkehrungen nötig.

Actinium wird zur Erzeugung von Neutronen eingesetzt, die bei Aktivierungsanalysen eine Rolle spielen. Außerdem wird es für die thermoionische Energieumwandlung genutzt.

Beim dualen Zerfall des Ac geht der größte Teil unter Emission von Beta-Teilchen in das Thoriumisotop Th, aber ca. 1 % zerfällt durch Alpha-Emission zu Francium Fr. Eine Lösung von Ac ist daher als Quelle für das kurzlebige Fr verwendbar. Letzteres kann dann regelmäßig abgetrennt und untersucht werden.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.

Nur eine geringe Anzahl von Actiniumverbindungen ist bekannt. Mit Ausnahme von AcPO sind sie alle den entsprechenden Lanthanverbindungen ähnlich und enthalten Actinium in der Oxidationsstufe +3. Insbesondere unterscheiden sich die Gitterkonstanten der jeweiligen Lanthan- und Actinium-Verbindungen nur in wenigen Prozent.

Actinium(III)-oxid (AcO) kann durch Erhitzen des Hydroxids bei 500 °C oder des Oxalats bei 1100 °C im Vakuum erhalten werden. Das Kristallgitter ist isotyp mit den Oxiden der meisten dreiwertigen Seltenerdmetalle.

Actinium(III)-fluorid (AcF) kann entweder in Lösung oder durch Feststoffreaktion dargestellt werden. Im ersten Fall gibt man bei Raumtemperatur Flusssäure zu einer Ac-Lösung und fällt das Produkt aus. im anderen Fall wird Actinium-Metall mit Fluorwasserstoff bei 700 °C in einer Platinapparatur behandelt.

Actinium(III)-chlorid (AcCl) wird durch Umsetzung von Actiniumhydroxid oder -oxalat mit Tetrachlormethan bei Temperaturen oberhalb von 960 °C erhalten.

Die Reaktion von Aluminiumbromid und Actinium(III)-oxid führt zum Actinium(III)-bromid (AcBr) und Behandlung mit feuchtem Ammoniak bei 500 °C führt zum Oxibromid AcOBr.

Gibt man Natriumdihydrogenphosphat (NaHPO) zu einer Lösung von Actinium in Salzsäure, erhält man weiß gefärbtes Actiniumphosphat (AcPO · 0,5 HO); ein Erhitzen von Actinium(III)-oxalat mit Schwefelwasserstoff bei 1400 °C für ein paar Minuten führt zu schwarzem Actinium(III)-sulfid (AcS).




</doc>
<doc id="5" url="https://de.wikipedia.org/wiki?curid=5" title="Ang Lee">
Ang Lee

Ang Lee (; * 23. Oktober 1954 in Pingtung, Taiwan) ist ein US-amerikanisch-taiwanischer Filmregisseur, Drehbuchautor und Produzent. Er ist als vielfach ausgezeichneter Regisseur bekannt für so unterschiedliche Filme wie "Eat Drink Man Woman", die Jane-Austen-Adaption "Sinn und Sinnlichkeit" und den Martial Arts-Film "Tiger and Dragon". Für seine Filme "Brokeback Mountain" (2005) und "" (2012) wurde er jeweils mit dem Oscar in der Kategorie "Beste Regie" ausgezeichnet.

Ang Lee wurde 1954 in Taiwan geboren. Seine Eltern, Emigranten aus China, lernten sich in Taiwan kennen, Lee ist ihr ältester Sohn. Die Großeltern väterlicher- und mütterlicherseits sind im Zuge der kommunistischen Revolution in China ums Leben gekommen. Da sein Vater als Lehrer häufiger die Arbeitsstelle wechselte, wuchs Ang Lee in verschiedenen Städten Taiwans auf.

Entgegen den Wünschen seiner Eltern, wie sein Vater eine klassische akademische Laufbahn einzuschlagen, interessierte sich Lee für das Schauspiel und absolvierte mit ihrem Einverständnis zunächst ein Theater- und Filmstudium in Taipeh. Im Anschluss daran ging er 1978 in die USA, um an der Universität von Illinois in Urbana-Champaign Theaterwissenschaft und -regie zu studieren. Nach dem Erwerb seines B.A. in Illinois verlegte er sich ganz auf das Studium der Film- und Theaterproduktion an der Universität von New York, das er 1985 mit einem Master abschloss. Danach entschloss er sich, mit seiner ebenfalls aus Taiwan stammenden Ehefrau zusammen in den USA zu bleiben.

Sein Interesse verschob sich trotz erster Erfahrungen mit dem Super-8-Film in Taiwan erst spät ganz auf Filmregie und -produktion – auch weil Lee seinen Berufswunsch seiner Familie und insbesondere seinem Vater gegenüber lange Zeit nicht eingestehen wollte.
Nach dem Studium konnte er zunächst keine eigenen Projekte umsetzen. Erst ab 1992, als er seinen ersten Langfilm fertigstellte, zeichnete sich eine kontinuierliche Karriere als Regisseur ab.

Als seine bisher größte Erfolge – sowohl beim Publikum als auch bei der Kritik – gelten das Martial Arts-Drama "Tiger and Dragon" mit einer pan-asiatischen Starbesetzung und der Post-Western-Liebesfilm Brokeback Mountain mit Heath Ledger und Jake Gyllenhaal. Für letzteren bekam Lee 2006 als erster asiatisch-stämmiger und nicht-weißer Regisseur den Oscar für die beste Regie. Außerdem wurden Lees Filme, neben vielen weiteren Preisen, mit mittlerweile zwei Goldenen Bären der Berlinale und zwei Goldenen Löwen der Filmfestspiele von Venedig ausgezeichnet.

Lee ist seit 1983 mit der Mikrobiologin Jane Lin verheiratet. Mit ihren Söhnen Haan (* 1984) und Mason (* 1990) leben die beiden in White Plains, Westchester County, im Bundesstaat New York. Ang Lee besitzt die US-amerikanische Staatsbürgerschaft.

Nach seinen ersten Filmerfahrungen in Taiwan setzte sich Lee erst wieder während seines Studiums in den USA ernsthaft mit dem Filmemachen auseinander. Im Rahmen seines Studiums in New York drehte er einige Kurzfilme und wirkte unter anderem beim Abschlussdreh seines Studienkollegen Spike Lee als Regieassistent mit. Sein eigener Abschlussfilm "Fine Line" gewann 1985 zwei Preise beim renommierten Filmfest seiner Universität. Erst 1992 gelang es ihm, nach dem Gewinn eines hochdotierten Drehbuchwettbewerbs in Taiwan, den ersten einer Reihe von drei Filmen zu drehen, die west-östliche Konflikte taiwanischer Familien zum Thema haben.

Diese ersten drei Langfilme, die Lee realisieren konnte, werden im Allgemeinen unter dem Begriff "Father Knows Best" gefasst. Diese Bezeichnung geht auf die wiederkehrende Figur des chinesischen Familienoberhaupts, gespielt jeweils vom taiwanischen Schauspieler Sihung Lung, zurück. Die drei Filme thematisieren, wie später noch öfter bei Ang Lee, familiäre Probleme, die aus dem Konflikt zwischen Selbstbestimmung und Tradition, zwischen Innen und Außen, zwischen Ost und West sowie zwischen den Generationen herrühren. Die Filme sind allesamt US-amerikanisch-taiwanische Koproduktionen. Anders als bei allen bislang folgenden Projekten handelt es sich bei den ersten Filmen Lees nicht um Adaptionen, sondern um Filme nach von ihm selbst geschriebenen Originaldrehbüchern.

Der erste Film, "Schiebende Hände" (1992), handelt vom Einzug eines chinesischen Vaters bei seinem erwachsenen Sohn und der US-amerikanischen Schwiegertochter in New York und den interkulturellen Problemen, die in der neuen Wohngemeinschaft entstehen. Dies war die erste Zusammenarbeit zwischen Lee und dem Drehbuchautor und Produzenten James Schamus – seitdem bildeten die beiden bei jedem Film Lees eine enge Arbeitsgemeinschaft. Wie in den beiden folgenden Filmen schrieben sie auch gemeinsam das Drehbuch. In allen weiteren Filmen Lees (mit Ausnahme des Kurzfilms "The Hire: Chosen") hat Schamus seither entscheidende Funktionen ausgeübt.

Auch die regelmäßige Zusammenarbeit mit dem Filmeditor Tim Squyres nahm in Lees Erstling ihren Anfang. Mit Ausnahme des Erfolgsfilms "Brokeback Mountain" von 2005 hat Squires jeden Film, den Ang Lee gedreht hat, geschnitten.

Nach dem Erfolg seines Erstlings konnte Lee als Nächstes "Das Hochzeitsbankett" (1993) drehen, eine Komödie über die fingierte Eheschließung eines homosexuellen Exil-Taiwaners in den USA. Erneut taucht hier die Figur des strengen, aber weisen Familienoberhaupts auf. Hatte "Schiebende Hände" zunächst vor allem in Taiwan für Aufmerksamkeit (und Preise) gesorgt, wurde mit dem zweiten Langfilm Lees auch Europa auf den aufstrebenden Regisseur aufmerksam: Der Film erhielt bei der Berlinale 1993 den "Goldenen Bären" als "Bester fremdsprachiger Film" und war zudem für einen Oscar nominiert. Er gilt darüber hinaus als einer der profitabelsten Low-Budget-Filme des Jahres 1993. Mit nur einer Million US-Dollar Produktionskosten erzielte er ein Einspielergebnis von über 23 Millionen US-Dollar.

Sihung Lung ist auch im letzten Teil der Trilogie, "Eat Drink Man Woman" (1994), die „kongeniale Verkörperung des chinesischen Familienoberhaupts“, das „Zentrum dieser Maskeraden, in denen es darum geht, ein altes Gesicht zu wahren und dann zu lernen, es zu verlieren, um ein neues, lebenstauglicheres zu gewinnen.“ Dieses Mal ist er der verwitwete Vater dreier Töchter, die ihr Leben und ihre Lieben auf unterschiedliche Art angehen und dabei ebenfalls innerfamiliäre Konflikte klären müssen. "Eat Drink Man Woman" wurde, anders als seine Vorgänger, in Taipeh gedreht. Im Mittelpunkt des Films stehen (der Titel deutet es an) die Liebe und das Essen. Ang Lee, privat ein passionierter Koch, legte hierbei besonders großen Wert auf die kulinarische Komponente als Stilmittel und konzipierte die Hauptfigur des älteren Witwers als berühmten Koch.

Mit dem Angebot der Produzentin Lindsay Doran, die von der britischen Schauspielerin Emma Thompson verfasste Adaption des Romans "Verstand und Gefühl" von Jane Austen in Großbritannien zu drehen, eröffnete sich Lee eine lange ersehnte neue Perspektive jenseits asiatisch geprägter Stoffe.

In einer neuen Trilogie setzt er sich mit unterschiedlichen Kulturen auseinander:

"Tiger and Dragon" sowie "Hulk" sind sehr unterschiedliche Action-Filme. Mit "Tiger and Dragon" gewann Lee zwei Golden Globes. Das Werk wurde außerdem mit vier Academy Awards (Oscars) prämiert, darunter der Trophäe für den besten fremdsprachigen Film. Für diesen Film wurde er 2001 auch mit einem Chlotrudis Award ausgezeichnet, seinen zweiten Chlotrudis erhielt er 2006 für "Brokeback Mountain".

Für "Brokeback Mountain" wurde Lee mit einer Vielzahl von Filmpreisen geehrt, darunter der Oscar für die beste Regie, der Goldene Löwe der Filmfestspiele von Venedig sowie die Auszeichnung der Hollywood Foreign Press Association als bester Regisseur des Jahres. 2007 verfilmte er mit "Gefahr und Begierde" eine Kurzgeschichte von Eileen Chang. Der Thriller spielt zur Zeit des Zweiten Weltkriegs in Shanghai und handelt von einer jungen chinesischen Agentin (gespielt von Tang Wei), die beauftragt wird, einen hochrangigen Verräter (Tony Leung Chiu Wai) zu liquidieren. Lees erste chinesischsprachige Spielfilmproduktion seit "Tiger and Dragon" war 2007 im offiziellen Wettbewerb der 64. Filmfestspiele von Venedig vertreten und brachte ihm erneut den Goldenen Löwen ein. Im selben Jahr wurde "Gefahr und Begierde" als offizieller taiwanischer Beitrag für die Nominierung um den besten fremdsprachigen Film bei der Oscar-Verleihung 2008 ausgewählt, später aber auf Empfehlung der Academy of Motion Picture Arts and Sciences wieder zurückgezogen und durch Chen Huai-Ens "Lian xi qu" ersetzt.

Ende Februar 2009 wurde bekannt gegeben, dass Lee die Jury der 66. Filmfestspiele von Venedig leiten werde. Zwei Monate später erhielt er für seine Komödie "Taking Woodstock" eine Einladung in den Wettbewerb der 62. Internationalen Filmfestspiele von Cannes.

2013 wurde er in die Wettbewerbsjury des 66. Filmfestivals von Cannes berufen.

Ang Lee ist ein international anerkannter und erfolgreicher Regisseur und gilt als einer der vielseitigsten Filmemacher der letzten Jahre. Häufig behandelt Lee in seinen Filmen das Thema Familie auf eine Art und Weise, die autobiographische Züge seines eigenen Lebens trägt. Er lässt seine Umgebung ganz bewusst auf sich einwirken und bringt diese in seine Filme ein.

Kennzeichnend für die meisten seiner Filme ist eine wenig geradlinige Erzählstruktur, die die Charaktere und die Geschichte aus verschiedenen Blickwinkeln darstellt. Er verknüpft die Konflikte des menschlichen Lebens mit traditionellen und innovativen Stilelementen.

Für Ang Lee sind die klassisch-soliden Erzählstrukturen zu langweilig, daher kombiniert er verschiedene Genres und Epochen. Er selbst sagte einmal:










</doc>
<doc id="7" url="https://de.wikipedia.org/wiki?curid=7" title="Anschluss (Soziologie)">
Anschluss (Soziologie)

Anschluss ist in der Soziologie ein Fachbegriff aus der Systemtheorie von Niklas Luhmann und bezeichnet die in einer sozialen Begegnung auf eine Selektion der anderen Seite folgende, selbst gewählte Selektion. Diese Selektionen beziehen sich aufeinander.

Die Anschlussfähigkeit ist die Kapazität von Systemen zu gewährleisten, dass sich an die Selektionen eines Systems weitere anschließen können. Alle sozialen Systeme reproduzieren sich über Kommunikation (z. B. Wirtschaftssystem oder Politik) oder Handlungen (Medizin und Erziehungssystem). Dies gelingt nur, wenn die einzelnen Einheiten aneinander anschlussfähig sind, was durch einen systemspezifischen Code geleistet wird, der als zentrale Logik (Leitunterscheidung) aller Kommunikation zugrunde liegt und sie als systemzugehörig erkennbar macht. Im Wirtschaftssystem beispielsweise sorgt der Code "zahlen/nicht zahlen" dafür, dass die Kommunikationen sich auf sich selbst beziehen und sich selbst reproduzieren können, also dass auf jede Zahlung eine neue erfolgt. Dies funktioniert über das generalisierte Kommunikationsmedium Geld, das die letzte Zahlung mit der jetzigen verknüpft. Würde das Geld nicht mehr akzeptiert, folgt der Zahlung keine weitere Zahlung mehr und das System hätte seine Anschlussfähigkeit verloren. Die Anschlussfähigkeit innerhalb eines Systems wird als Selbstreferenz bezeichnet, im Gegensatz zum fremdreferentiellen Bezug auf die Umwelt (Welt, andere Systeme).

Den Begriff hat Luhmann auf eine Anregung eines Bielefelder Kollegen, des Philosophen Jürgen Frese entwickelt. Frese zeigte in einem Sektionsreferat des Achten Deutschen Kongresses für Philosophie in Heidelberg (1966, gedruckt 1967) mit dem Titel „Sprechen als Metapher für Handeln“, dass es fruchtbar ist, von den dominanten Handlungsmodellen Arbeit und Konsum abzurücken und ergänzend Sprechen als Modell für Handeln zu nutzen. Frese schreibt: „Die wichtigste Errungenschaft, die die Sprachmetapher für die Aufhellung des nicht-sprachlichen Handelns einbringt, ist ihre Leistung, Reihenbildung erklärbar zu machen. Fassen wir Satz und Handlung zum neutralen und an andere Philosopheme anschließbaren Begriff des Aktes zusammen, so können wir ... sagen: Der Sinn eines Aktes ist das als eine bestimmte Situation gegebene Ensemble der Möglichkeiten, an diesen Akt weitere Akte anzuschließen; d. h. der Sinn eines Aktes ist die Mannigfaltigkeit der Anschließbarkeiten, die er eröffnet.“ Diese Idee wurde von Luhmann aufgegriffen und im Rahmen seiner Systemtheorie weiterentwickelt. Frese selbst baute sie im Rahmen seiner Lehre von den Formularen weiter aus.



</doc>
<doc id="10" url="https://de.wikipedia.org/wiki?curid=10" title="Aussagenlogik">
Aussagenlogik

Die Aussagenlogik ist ein Teilgebiet der Logik, das sich mit Aussagen und deren Verknüpfung durch Junktoren befasst, ausgehend von strukturlosen Elementaraussagen (Atomen), denen ein Wahrheitswert zugeordnet wird. In der "klassischen Aussagenlogik" wird jeder Aussage genau einer der zwei Wahrheitswerte „wahr“ und „falsch“ zugeordnet. Der Wahrheitswert einer zusammengesetzten Aussage lässt sich ohne zusätzliche Informationen aus den Wahrheitswerten ihrer Teilaussagen bestimmen.

Historisch geht die Aussagenlogik zurück bis zu Aristoteles, der erstmals aussagenlogische Grundsätze diskutierte, nämlich in seiner Metaphysik den Satz vom Widerspruch und den Satz vom ausgeschlossenen Dritten, und der in seiner ersten Analytik den indirekten Beweis thematisierte. Die zweiwertige aussagenlogische Semantik entwickelten etwas später die megarischen Philosophen Diodoros Kronos und Philon. Die Aussagensemantik und -axiomatik kombinierte der Stoiker Chrysippos von Soli, der den ersten aussagenlogischen Kalkül formulierte. Die Weiterentwicklung der Aussagenlogik der Stoa durch das Mittelalter wird oft übersehen. Eine erste vollständige und entscheidbare Formalisierung für aussagenlogische Tautologien – allerdings noch nicht für das aussagenlogische Schließen – schuf George Boole 1847 mit seinem algebraischen Logikkalkül. Den ersten aussagenlogischen Kalkül mit Schlussregeln formulierte Gottlob Frege im Rahmen seiner Begriffsschrift 1879. Er war die Vorlage für den Aussagenkalkül von Bertrand Russell 1910, der sich später durchsetzte (s. u.).
Da in der heutigen Mathematik die klassische Aussagenlogik maßgeblich wurde, wird in diesem Artikel dieser moderne Haupttypus der Aussagenlogik behandelt. Allgemein ist die klassische Logik durch zwei Eigenschaften charakterisiert:


Das Prinzip der Zweiwertigkeit wird oft mit dem Satz vom ausgeschlossenen Dritten verwechselt.

Die "klassische Aussagenlogik" ist jenes Gebiet der klassischen Logik, das die innere Struktur von Sätzen (Aussagen) daraufhin untersucht, aus welchen anderen Sätzen (Teilsätzen) sie zusammengesetzt sind und wie diese Teilsätze miteinander verknüpft sind. Die innere Struktur von Sätzen, die ihrerseits nicht in weitere Teilsätze zerlegt werden können, wird von der Aussagenlogik nicht betrachtet. Ein Beispiel: Die Aussage „Alle Katzen sind Hunde, und die Erde ist eine Scheibe“ ist mit dem Bindewort „und“ aus den beiden kürzeren Aussagen „Alle Katzen sind Hunde“ und „Die Erde ist eine Scheibe“ zusammengesetzt. Diese beiden Aussagen lassen sich ihrerseits nicht mehr in weitere Aussagen zerlegen, sind aus aussagenlogischer Sicht also elementar oder atomar. Andere, auf die Aussagenlogik aufbauende logische Systeme betrachten die innere Struktur solcher atomaren Aussagen; ein wichtiges Beispiel ist die Prädikatenlogik.

In Abgrenzung zur klassischen Logik entstehen "nichtklassische Logiksysteme", wenn man das Prinzip der Zweiwertigkeit, das Prinzip der Extensionalität oder sogar beide Prinzipien aufhebt. Nichtklassische Logiken, die durch die Aufhebung des Prinzips der Zweiwertigkeit entstehen, heißen mehrwertige Logik. Die Zahl der Wahrheitswerte (in diesem Falle üblicher: Pseudowahrheitswerte) kann dabei endlich sein (z. B. dreiwertige Logik), ist aber oft auch unendlich (z. B. Fuzzy-Logik). Hingegen verwenden Logiken, die durch die Aufhebung der Extensionalität entstehen, Junktoren (Konnektive), bei denen sich der Wahrheitswert des zusammengesetzten Satzes nicht mehr eindeutig aus dem Wahrheitswert seiner Teile bestimmen lässt. Ein Beispiel für nichtextensionale Logik ist die Modallogik, die die einstelligen nichtextensionalen Operatoren „es ist notwendig, dass“ und „es ist möglich, dass“ einführt.

Logische Systeme stehen innerhalb der Logik nicht in einem Konkurrenzverhältnis um Wahrheit oder Richtigkeit. Die Frage, welches logische System für einen bestimmten Zweck genutzt werden soll, ist eher eine pragmatische.

Oft werden logische Systeme und logische Fragestellungen mit außerlogischen Fragen verwechselt oder vermischt, z. B. mit der metaphysischen Frage, welches logische System „richtig“ sei, d. h. die Wirklichkeit beschreibe. Zu dieser Frage gibt es unterschiedliche Standpunkte einschließlich des positivistischen Standpunkts, dass diese Frage sinnlos sei. Diese Fragen fallen aber in andere Gebiete, z. B. Philosophie, Wissenschaftstheorie und Sprachwissenschaft.

Wenn in diesem Artikel die klassische Aussagenlogik behandelt wird, so ist das also nicht als metaphysische Festlegung zu verstehen oder gar als Behauptung, dass „alle Aussagen wahr oder falsch sind“. Es ist lediglich so, dass die klassische Aussagenlogik einfach nur solche Aussagen behandelt, die wahr oder falsch sind. Das ist eine große formale Vereinfachung, die dieses System relativ leicht erlernbar sein lässt. Braucht man aus metaphysischen oder pragmatischen Gründen mehr als zwei Wahrheitswerte, kann die klassische Aussagenlogik als Ausgangspunkt dienen, um ein geeignetes logisches System aufzustellen.

Eine Aussage A ist ein Satz, der entweder wahr (w, wahr, true, 1) oder nicht wahr (f, falsch, false, 0) ist. Das gilt sowohl für einfache als auch für verknüpfte Aussagen. „Halbwahrheiten“ gibt es nicht. Eine Aussage kann sowohl der gewöhnlichen Sprache entstammen als auch der Sprache der Mathematik. Eine Elementaraussage ist eine Aussage, die keine aussagenlogischen Verknüpfungen ("nicht, und, oder, wenn … dann, genau dann wenn") enthält.

Beispiele für Elementaraussagen:
formula_2 ist offensichtlich wahr, formula_4 dagegen ist falsch. formula_1 muss man zunächst prüfen,
bevor man entscheiden kann, ob formula_1 wahr oder falsch ist. Ob formula_3 wahr ist, kann man derzeit nicht entscheiden. Das wird sich erst am Ende der nächsten Fußballsaison
herausstellen.

In der klassischen Aussagenlogik ist eine Aussage entweder wahr oder nicht wahr, auch wenn man den Wahrheitsgehalt nicht kennt. Das ist zum Beispiel bei den ungelösten mathematischen Problemen der Fall.

"Anmerkung:" formula_4 ist eine All-Aussage; die Struktur solcher Aussagen ist Gegenstand der Prädikatenlogik. Im Sinne der Aussagenlogik ist es eine Elementaraussage.

Die "Verneinung" bzw. "Negation" (auch: "Satzverneinung", "äußere Verneinung", "kontradiktorisches Gegenteil") einer Aussage A ist diejenige Aussage ¬A, die genau dann wahr ist, wenn A falsch ist, und die genau dann falsch ist, wenn A wahr ist. Einfacher: Die Verneinung einer Aussage A dreht den Wahrheitswert von A in sein Gegenteil um.

Man erhält die Verneinung einer Aussage A immer dadurch, dass man ihr die Formulierung „Es ist nicht der Fall, dass“ voranstellt. Zwar lässt sich ein natürlichsprachlicher Satz auch verneinen, indem man das Wort „nicht“ oder eine andere negative Formulierung an geeigneter Stelle einfügt – es ist aber nicht immer ganz einfach, zu erkennen, welche Formulierung zu verwenden und an welcher Stelle einzufügen ist. Formal schreibt man für „nicht A“ in der gebräuchlichsten Notation (Schreibweise) ¬A, auf Englisch und in der Schaltalgebra auch „NOT A“, gelegentlich auch „~A“.
Wir verneinen die obigen Beispiele:

Allgemein gilt für die Verneinung:

Eine "Konjunktion" ist eine aus zwei Aussagen zusammengesetzte Aussage, die die Wahrheit all ihrer Teilaussagen behauptet. Umgangssprachlich verbindet man zwei Aussagen A und B durch das Bindewort „und“ zu einer Konjunktion „A und B“, in der logischen Sprache verwendet man meist das Zeichen formula_22 (Schreibweise: formula_23), gelegentlich auch das kaufmännische Und, den Ampersand (&).

Die Aussage formula_23 ist immer dann wahr, wenn sowohl A als auch B jeweils wahr sind.
Andernfalls ist formula_23 falsch, nämlich dann, wenn entweder A oder B oder beide Aussagen falsch sind.

Beispiele für eine "Und"-Verknüpfung:

A: 9 ist durch 3 teilbar
B: 9 ist eine Quadratzahl
Diese Teilaussagen und ihre Negationen werden nun durch formula_22 miteinander verknüpft:

Nur formula_32 ist wahr, weil formula_15 wahr ist und auch formula_34 wahr ist.
formula_35 ist falsch, weil formula_16 falsch ist.
formula_37 ist falsch, weil formula_38 falsch ist.
formula_39 ist falsch, weil sowohl formula_16 als auch formula_38 falsch
ist.

Eine "Disjunktion" ist eine zusammengesetzte Aussage, die behauptet, dass mindestens eine ihrer Teilaussagen wahr ist. Die Disjunktion in diesem Sinn wird auch "nichtausschließendes Oder" genannt. (Aber Achtung: Die Bezeichnung „Disjunktion“ wurde und wird oft auch für das ausschließende Oder, „entweder … oder“, verwendet – man denke an das Konzept der disjunkten Mengen. Einige Autoren verwenden daher für das Nichtausschließende Oder den Begriff "Adjunktion".)
Das Formelzeichen „formula_42“ stammt von dem lateinischen Wort „vel“, was auf deutsch „oder“ bedeutet.

Die Aussage formula_43 ist immer dann wahr, wenn mindestens eine der Teilaussagen A oder B wahr ist, bzw. wenn beide Teilaussagen wahr sind. Andernfalls ist formula_43 falsch, nämlich dann, wenn sowohl A als auch B falsch sind.

Beispiel für eine "Oder"-Verknüpfung:

Diese Teilaussagen und ihre Negationen werden nun durch formula_42 miteinander verknüpft:

formula_51 ist wahr, weil sowohl formula_15 als auch formula_34 wahr sind.
formula_54 ist wahr, weil formula_34 wahr ist.
formula_56 ist wahr, weil formula_15 wahr ist.
Nur formula_58 ist falsch, weil sowohl formula_16 als auch formula_38 falsch sind.

Die "materiale Implikation", auch "Konditional" oder "Subjunktion" genannt, drückt die "hinreichende Bedingung" aus: Sie sagt, dass die Wahrheit des einen Satzes eine hinreichende Bedingung für die Wahrheit des anderen Satzes ist. Man schreibt

oder auch

oder auch nur

In einem Konditional nennt man A das "Antezedens", B das "Konsequens" oder "Sukzedens".

Beispiele:

Die Lesart „wenn … dann“ ist insofern problematisch, als mit dem natürlichsprachlichen „wenn … dann“ vor allem inhaltliche Zusammenhänge wie Kausalität oder zeitliche Nähe ausgedrückt werden. All das macht die materiale Implikation nicht, sie nennt nur den formalen Zusammenhang: „Dass es regnet, ist eine hinreichende Bedingung dafür, dass die Straße nass ist“. Zur Frage, "warum" das eine hinreichende Bedingung ist – ob auf Grund eines kausalen Zusammenhangs oder auch nur rein zufällig –, nimmt die materiale Implikation nicht Stellung.
Als "Umkehrschluss" bezeichnet man den Schluss von formula_61 auf formula_64. Für die Beispiele bedeutet das:

Umgangssprachlich lässt man sich gelegentlich zu weiteren – "falschen" – Aussagen
verleiten:

Das bedeutet: Wenn die Folgerung formula_61 wahr ist, dann erhält man aus der
Aussage ¬A keine Aussage über B; B kann wahr oder falsch sein. („Ex falso sequitur quodlibet“ – „Aus Falschem folgt Beliebiges“)

Die Implikation ist ein wichtiges Mittel in der Mathematik. Die meisten mathematischen Beweise verwenden das Konzept der Implikation.

Das "Bikonditional", oft auch "objektsprachliche Äquivalenz" oder "materiale Äquivalenz" genannt, drückt die "hinreichende und notwendige Bedingung" aus, sagt also, dass eine Aussage A genau dann zutrifft, wenn eine Aussage B zutrifft. Man schreibt:

und liest

Auch beim Bikonditional wird eine rein formale Aussage getroffen, die nichts über einen allfälligen inhaltlichen Zusammenhang von A und B aussagt.

Statt formula_66 zu sagen, kann man auch sagen, dass A eine hinreichende Bedingung für B und dass B eine hinreichende Bedingung für A ist, also formula_68. Tatsächlich sind diese beiden Aussagen logisch äquivalent.
Beispiel:

Das Bikonditional als zusammengesetzte Aussage innerhalb der logischen Sprache (siehe Objektsprache) wird oft mit dem Konzept der logischen Äquivalenz verwechselt oder vermischt. Die logische Äquivalenz ist eine metasprachliche, meist natürlichsprachlich formulierte Eigenschaft zweier Aussagen der logischen Sprache. Ein Zusammenhang zwischen logischer Äquivalenz und Bikonditional besteht nur insofern, als das Metatheorem gilt, dass ein Bikonditional formula_66 genau dann eine Tautologie ist, wenn die beiden Aussagen A und B logisch äquivalent sind.

Das ausschließende Oder (Kontravalenz oder Antivalenz), „entweder A oder B“, besagt, dass genau eine der beiden von ihm verknüpften Aussagen wahr ist. Entsprechend ist ein ausschließendes Oder nicht nur dann falsch, wenn sowohl A als auch B falsch sind, sondern auch, wenn "beide" wahr sind. (Einige Autoren verwenden für das Ausschließende Oder den Begriff "Alternative".)

Obwohl das ausschließende Oder ein Konzept ist, mit dem man in der natürlichen Sprache immer wieder zu tun hat, wird es in den meisten logischen Sprachen nicht als eigenständiger Junktor eingeführt. Stattdessen wird das ausschließende Oder zum Beispiel als verneintes Bikonditional ausgedrückt, also als formula_70.

Große Bedeutung genießt das ausschließende Oder hingegen in der Schaltalgebra, wo es meist als XOR "(eXclusive OR)" aufgeschrieben wird.

Die Verneinung der Konjunktion „A und B“ (in der logischen Schreibweise: formula_23) lautet „Es ist nicht der Fall, dass A und B zutreffen“ (in der logischen Schreibweise: formula_72).
Diese ist logisch äquivalent mit der Aussage
„A ist nicht der Fall, oder B ist nicht der Fall (oder beides)“ (in logischer Schreibweise: formula_73).

Ein Beispiel:

Wenn man die Aussage
verneinen möchte, dann kann man entweder sagen
oder man sagt

In der Schaltalgebra wird sehr oft der Junktor NAND verwendet, wobei „A NAND B“ denselben Wahrheitswertverlauf hat wie der Ausdruck formula_72.

Die Verneinung der Disjunktion „A oder B (oder beides)“ (in der logischen Schreibweise: formula_75) lautet „Es ist nicht der Fall, dass A oder B zutrifft“ (in logischer Schreibweise: formula_76).
Diese ist logisch äquivalent mit der Aussage
„A ist nicht der Fall, und B ist nicht der Fall“ (in logischer Schreibweise: formula_77).

Ein Beispiel:

Wenn man die Aussage
verneinen möchte, so sagt man
Nach dem Gesetz von De Morgan kann man nun aber auch sagen:
oder in schönerem Deutsch

In der Schaltalgebra wird das Konnektiv NOR verwendet, das denselben Wahrheitswertverlauf hat wie die Aussage formula_76.

Dieser Abschnitt soll den zunächst oft als kontraintuitiv empfundenen Zusammenhang zwischen hinreichender und notwendiger Bedingung, wie er im Abschnitt über die materiale Implikation angesprochen wurde, wiederaufgreifen und näher ausführen.

Betrachten wir noch einmal die materiale Implikation formula_79.

Man sagt: A ist "hinreichend" für B: Schon wenn A der Fall ist, ist auch B der Fall.

Umgekehrt kann man aber auch sagen: B ist "notwendig" für A. Ohne B kann A nicht erfüllt sein.

Wie kommt dieser Zusammenhang zustande?

Wir wissen, dass die Wahrheit von A die Wahrheit von B nach sich zieht, denn A ist ja hinreichende Bedingung für B. Somit ist es einfach nicht möglich, dass A eintritt, ohne dass B damit ebenfalls eintreten würde: B ist also gezwungenermaßen der Fall, wenn A der Fall ist. B ist „notwendig“ für A.

Dieser Zusammenhang ist in Wahrheit also ziemlich einfach; Hauptgrund dafür, dass er anfangs oft als kontraintuitiv empfunden wird, ist wahrscheinlich die Schwierigkeit, zwischen den vielen Bedeutungen des umgangssprachlichen „wenn … dann“ einerseits und der rein formalen hinreichenden und notwendigen Bedingung andererseits strikt zu trennen.

Mit dem umgangssprachlichen „wenn … dann“ möchte man fast immer einen inhaltlichen (kausalen oder auch temporalen) Zusammenhang zwischen Antecedens und Konsequens ausdrücken: „Regen verursacht Straßennässe“, „Zuerst fällt der Regen, erst nachher wird die Straße nass“. Wenn man die hinreichende Bedingung in diesem Sinn missversteht, dann ist es klar, dass die in umgekehrter Reihenfolge formulierte notwendige Bedingung „Nur wenn die Straße nass ist, regnet es“ seltsam aussieht: „Regen verursacht doch Straßennässe. Wie kann daraus je gefolgert werden, dass Straßennässe Regen verursacht?“

All dies sagt die materiale Implikation aber nicht aus. „A ist eine hinreichende Bedingung für B“ meint schlicht, dass wenn die Aussage A wahr ist, auch die Aussage B wahr ist – zeitlos und zusammenhanglos, nicht etwa „später“ oder „weil“.

Analog sagt die notwendige Bedingung, „B ist eine notwendige Bedingung für A“, lediglich das aus, dass B wahr ist, sofern A es ist. Genau das ist aber die Definition des Konditionals A → B.

Spätestens beim "lauten" Lesen von Sätzen wie:

wird der selbstbewusste Laie verlangen, dass ihm erklärt wird, was das soll.

Die Antwort des Logikers: Es soll versucht werden, Sicherheit in die Regeln des logischen Schließens zu bringen. Seit den Sophisten ist dem Abendland klar, dass scheinbar zwingende Schlüsse zu offensichtlich absurden Ergebnissen führen können. Immer wieder wurden Paradoxien formuliert und von großen Denkern als Herausforderung empfunden. Logiker versuchen deshalb, die Regeln des Argumentierens so streng wie möglich zu fassen.

Das einleitende Beispiel macht klar, dass dazu eine "Trennung der Sprachebenen" unerlässlich ist: Die formale Aussage A∧B soll dadurch erklärt werden, dass auf einer metasprachlichen Ebene über die Aussage A wie auch über die Aussage B geredet wird.

"Ein" Versuch dies durchzuführen, besteht darin, die Aussagenlogik als formales System, konkret als Kalkül (eine bestimmte Art eines formalen Systems) zu definieren. Die Begriffe „wahr“ und „falsch“ kommen in diesem System zunächst überhaupt nicht vor. Stattdessen werden Axiome gesetzt, die einfach als Zeichenketten angesehen werden, aus denen weitere ableitbare Zeichenketten aufgrund von bestimmten Schlussregeln hergeleitet werden.

Das Ziel dabei ist einerseits, dass in einem formalen System nur Zeichenketten (Sätze) hergeleitet werden können, die bei einer plausiblen Interpretation auch wahr sind. Andererseits sollen alle Sätze, die als „wahr“ interpretierbar sind, auch hergeleitet werden können. Das erste ist die Forderung nach "Korrektheit", das zweite die nach "Vollständigkeit" des formalen Systems; beide Eigenschaften sind unter Kalkül: Der Begriff Kalkül in der Logik beschrieben.

Für die klassische Aussagenlogik, mit der wir es hier zu tun haben, gibt es Kalküle (formale Systeme), die sowohl korrekt als auch vollständig sind. Für komplexere logische Systeme (z. B. Mengenlehre) ist es aber "unmöglich", einen vollständigen Kalkül aufzustellen, der auch korrekt ist – diese Erkenntnis wurde 1931 von Kurt Gödel bewiesen (Gödelscher Unvollständigkeitssatz).

Es gibt viele verschiedene Möglichkeiten, die Syntax („Grammatik“) einer logischen Sprache formal zu definieren; meist geschieht das im Rahmen eines Kalküls. Die folgende Definition ist daher nur als Beispiel dafür zu verstehen, wie ein Kalkül für die klassische Aussagenlogik aussehen kann. Weitere Beispiele für konkrete Kalküle finden sich unter Baumkalkül, Begriffsschrift, Systeme natürlichen Schließens, Sequenzenkalkül oder Resolutionskalkül. Ein weiterer axiomatischer Kalkül ist als Beispiel im Artikel Hilbert-Kalkül angegeben, ein graphischer Kalkül im Artikel Existential Graphs.

Als "Bausteine" der aussagenlogischen Sprache sollen "Satzbuchstaben" („atomare Formeln“, Satzkonstanten), "Junktoren" und "Gliederungszeichen" verwendet werden. Satzbuchstaben sollen die Zeichen P, P, P, … sein. Junktoren sollen die Zeichen ¬, ∧, ∨, → und ↔ sein. Als Gliederungszeichen sollen die runden Klammern dienen.

Formal lässt sich das z. B. auf folgende Weise ausdrücken:

Sei V die (abzählbar unendliche) Menge der "atomaren Formeln" (Satzbuchstaben):

Sei J die Menge der Junktoren und Gliederungszeichen:

Das "Alphabet" der logischen Sprache sei die Menge V ∪ J, also die Vereinigungsmenge von atomaren Formeln, Junktoren und Gliederungszeichen.

Die "Formationsregeln" legen fest, wie man aus den Bausteinen der aussagenlogischen Sprache Sätze (Formeln) bilden kann. Hier sollen "aussagenlogische Formeln" als Worte über dem Alphabet der logischen Sprache, also über V ∪ J wie folgt induktiv definiert werden:


"Schlussregeln" sind allgemein Transformationsregeln (Umformungsregeln), die auf bestehende Formeln angewandt werden und aus ihnen neue Formeln erzeugen. Wenn man einen Kalkül für ein logisches System aufstellt, dann wählt man die Transformationsregeln so, dass sie aus bestehenden Formeln solche Formeln erzeugen, die aus den Ausgangsformeln semantisch "folgen" – deshalb die Bezeichnung „Schlussregel“ ("eine Schlussfolgerung ziehen").

Innerhalb der Syntax sind die Schlussregeln allerdings rein formale Transformationsregeln, denen für sich keinerlei inhaltliche Bedeutung zukommt.

An konkreten Schlussregeln sollen hier nur zwei angegeben werden: Der Modus ponendo ponens und die Substitutionsregel.



"Axiome" sind ausgezeichnete (im Sinn von: hervorgehobene) Formeln der aussagenlogischen Sprache. Die Auszeichnung besteht darin, dass sie innerhalb eines Beweises oder einer Herleitung (siehe unten) ohne weitere Rechtfertigung verwendet werden.

Pragmatisch wählt man solche Formeln als Axiome, die semantisch gesehen Tautologien sind, also immer zutreffen, und die dabei helfen, Beweise zu verkürzen. Innerhalb der Syntax sind die Axiome allerdings rein formale Objekte, denen keinerlei inhaltliche Bedeutung oder Rechtfertigung zukommt.

Axiome sind im Allgemeinen optional, d. h. ein Kalkül kann auch ganz ohne Axiome auskommen, wenn er ausreichend viele bzw. mächtige Schlussregeln hat. Axiomfreie Kalküle sind zum Beispiel die Systeme natürlichen Schließens oder Baumkalküle.

Hier soll exemplarisch ein axiomatischer Kalkül gezeigt werden, und zwar jener, den Whitehead und Russell in ihren 1910–1913 entstandenen Principia Mathematica vorstellten. Der Principia Mathematica-Kalkül für die Aussagenlogik umfasst die folgenden Axiome (von denen das vierte redundant, d. h. nicht unbedingt erforderlich, weil aus den anderen Axiomen herleitbar ist):


Um aus diesen Axiomen auch solche gültigen Sätze herleiten zu können, die andere als die in den Axiomen vorkommende Junktoren enthalten, werden diese durch folgende Festlegung auf die vorhandenen Junktoren zurückgeführt:


Alternativ zu – wie hier – konkreten Axiomen kann man auch "Axiomenschemata" angeben, in welchem Fall man auch ohne Substitutionsregel auskommt. Interpretiert man die obigen Axiome als Axiomenschemata, dann stünde z. B. das erste Axiomenschema, formula_91, für unendlich viele Axiome, nämlich alle Ersetzungsinstanzen dieses Schemas.

Eine Herleitung ist eine Liste von aufsteigend nummerierten Sätzen, die mit einer oder mehreren Annahmen (den Prämissen der Herleitung) oder Axiomen beginnt. Alle auf diese folgenden Sätze sind entweder ebenfalls Axiome (bei manchen Kalkülen sind auch weitere Annahmen zulässig) oder sind aus einer oder mehreren der vorangehenden Zeilen durch Anwendung von Schlussregeln entstanden. Der letzte Satz in der Liste ist die Konklusion der Herleitung.

Eine Herleitung ohne Prämissen heißt "Beweis". Oft werden aber die Wörter „Herleitung“ und „Beweis“ synonym gebraucht.

Wenn es gelingt, aus einer Menge von Annahmen (Prämissen) Δ eine Konklusion P herzuleiten, dann schreibt man auch formula_99.

Gelingt es, einen Satz P ohne die Verwendung von Annahmen herzuleiten (zu beweisen), dann schreibt man auch: formula_100. In diesem Fall wird P "Theorem" genannt.

Das Zeichen formula_101 geht auf die Begriffsschrift zurück, jenes Werk, in dem Gottlob Frege 1879 die erste Formalisierung der Prädikatenlogik angegeben hat.

In der klassischen Aussagenlogik wählt man die Schlussregeln so, dass sich mit ihrer Hilfe "alle" gültigen Argumente (und "nur" gültige Argumente) herleiten lassen; die Frage der Gültigkeit wird im folgenden Abschnitt, „Semantik“, behandelt.

Außerhalb der Logik bezeichnet Semantik ein Forschungsgebiet, das sich mit der Bedeutung von Sprache und deren Teilen befasst. Oft wird auch das Wort "Semantik" gleichbedeutend mit dem Wort "Bedeutung" verwendet.

Auch innerhalb der Logik geht es bei Semantik um Bedeutung: Darum nämlich, den Ausdrücken einer formalen Sprache – zum Beispiel der hier behandelten Sprache der Aussagenlogik – eine Bedeutung zuzuordnen. In der Logik wird auch das meist sehr formal unternommen.

Im Zentrum der (formalen) Semantik steht eine Auswertungsfunktion (andere Bezeichnungen lauten Bewertungsfunktion, Denotationsfunktion, Wahrheitswertefunktion), die den Formeln der logischen Sprache eine Bedeutung zuordnet. Formal gesprochen ist die Auswertungsfunktion eine Abbildung von der Menge der Formeln der Sprache in die Menge der Wahrheitswerte. Oft wird die Auswertungsfunktion mit dem Großbuchstaben V bezeichnet.

In der klassischen Aussagenlogik ist die Auswertungsfunktion sehr einfach: Das Prinzip der Zweiwertigkeit fordert, dass sie für jede zu bewertende Formel genau einen von genau zwei Wahrheitswerten liefern muss; und das Prinzip der Extensionalität fordert, dass die Bewertungsfunktion beim Bewerten eines komplexen Satzes nur die Bewertung von dessen Teilsätzen berücksichtigen muss.

Jedem Atom, also jedem Satzbuchstaben (Atom) wird durch Festsetzung ein Wahrheitswert zugeordnet. Man sagt: Die Atome werden interpretiert. Es wird also z. B. festgelegt dass P wahr ist, dass P falsch ist und dass P ebenfalls falsch ist. Damit ist der Bewertung der Bausteine der logischen Sprache Genüge getan. Formal ist eine solche Bewertung – "Interpretation" genannt und oft mit dem Kleinbuchstaben v bezeichnet – eine Funktion im mathematischen Sinn, d. h. eine Abbildung von der Menge der Atome in die Menge der Wahrheitswerte.

Wenn die Auswertungsfunktion V auf ein Atom angewandt wird, d. h. wenn sie ein Atom bewerten soll, liefert sie die Interpretation dieses Atoms im Sinn des obigen Absatzes. Mit anderen Worten, sie liefert den Wert, den die Bewertung v dem Atom zuordnet.

Um die zusammengesetzten Formeln bewerten zu können, muss für jeden Junktor definiert werden, welchen Wahrheitswert die Bewertungsfunktion für die unterschiedlichen Wahrheitswertkombinationen liefert, den seine Argumente annehmen können. In der klassischen Aussagenlogik geschieht das meist mittels Wahrheitstabellen, weil es nur überschaubar wenige Möglichkeiten gibt.

Der einstellige Junktor ¬, die Negation, ist in der klassischen Aussagenlogik so definiert, dass er den Wahrheitswert seines Arguments ins Gegenteil umkehrt, also „verneint“: Ist die Bewertung einer Formel X wahr, dann liefert die Bewertungsfunktion für ¬X falsch; wird aber X falsch bewertet, dann liefert die Bewertungsfunktion für ¬X wahr. Die Wahrheitstabelle sieht folgendermaßen aus:

Die Wahrheitswertverläufe der verwendeten zweistelligen Konnektive sind in der klassischen Aussagenlogik wie folgt definiert:

Allgemein gibt es für die klassische Aussagenlogik vier einstellige und sechzehn zweistellige Junktoren. Die hier behandelte logische Sprache beschränkt sich nur deshalb auf die Junktoren ¬, ∧, ∨, → und ↔, weil diese am gebräuchlichsten sind und weil sie auch inhaltlich noch am ehesten aus der Alltagssprache bekannt sind. Aus formaler Sicht ist die einzige Bedingung, die man bei der Wahl von Junktoren erfüllen möchte, die, dass sich mit den gewählten Junktoren auch alle anderen theoretisch möglichen Junktoren ausdrücken lassen; man sagt: Dass die Menge der gewählten Junktoren funktional vollständig ist. Diese Anforderung ist bei der hier getroffenen Wahl erfüllt.

Näheres zur Frage, wie viele und welche Junktoren es gibt und wie viele Junktoren man benötigt, um funktionale Vollständigkeit zu erreichen, ist im Kapitel Junktor beschrieben.

"Semantische Gültigkeit" ist eine Eigenschaft von Formeln oder von Argumenten. (Ein Argument ist die Behauptung, dass aus einigen Aussagen – den Prämissen – eine bestimmte Aussage – die Konklusion – folgt.)

Eine "Formel" der aussagenlogischen Sprache heißt genau dann semantisch gültig, wenn die Formel unter allen Interpretationen – d. h. unter allen Zuordnungen von Wahrheitswerten zu den in ihr vorkommenden Atomen – wahr ist; wenn sie sozusagen allgemeingültig ist; mit anderen Worten: Wenn die Wahrheitstabelle für diese Aussage in jeder Zeile das Ergebnis "wahr" zeigt. Man nennt semantisch gültige Formeln auch Tautologien und schreibt, wenn formula_102 eine Tautologie ist, formal wie folgt:

Ein "Argument" heißt genau dann semantisch gültig, wenn unter der Voraussetzung, dass alle Prämissen wahr sind, auch die Konklusion wahr ist. In der Formulierung von Gottfried Wilhelm Leibniz: "Aus Wahrem folgt nur Wahres." Diese Definition muss natürlich ebenfalls formal gefasst werden, und das geschieht wie folgt: Ein Argument ist genau dann semantisch gültig, wenn alle Zuordnungen von Wahrheitswerten zu den in Prämissen und Konklusion vorkommenden Atomen, unter denen die Bewertungsfunktion für alle Prämissen den Wert "wahr" liefert, auch für die Konklusion den Wert "wahr" liefert.

Um auszudrücken, dass aus einer Menge formula_104 von Formeln (der Prämissenmenge) eine Formel formula_102 (die Konklusion) semantisch folgt, schreibt man formal wie folgt:

Beachte die graphische Ähnlichkeit und die inhaltliche Verschiedenheit zwischen formula_99 (Kapitel „Herleitung und Beweis“) und formula_106 ("Siehe:" Semantische Folgerung): Die erste Formulierung – formula_99 – drückt die "syntaktische" Gültigkeit des Arguments aus, sagt also, dass aus den Formeln in formula_104 mit den Schlussregeln des gewählten Kalküls die Formel formula_102 "hergeleitet" werden kann. formula_106 hingegen behauptet die "semantische" Gültigkeit, die in der klassischen Aussagenlogik wie in den vorangegangenen Absätzen als das Leibniz’sche "Aus Wahrem folgt nur Wahres" definiert ist.

Neben der Eigenschaft der Gültigkeit (Allgemeingültigkeit) gibt es einige andere wichtige Eigenschaften: Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit. Im Gegensatz zur Gültigkeit, die Eigenschaft von Formeln oder von Argumenten sein kann, sind Erfüllbarkeit, Widerlegbarkeit und Unerfüllbarkeit Eigenschaften von Sätzen oder von Satzmengen.


Die Frage, ob eine Formel (oder eine Formelmenge) eine der genannten Eigenschaften hat, ist ebenso wie die Frage, ob eine Formel allgemeingültig, d. h. eine Tautologie ist, für allgemeine Formeln nicht effizient lösbar: Zwar ist die Wahrheitstafel ein Entscheidungsverfahren für jede dieser Fragen, doch umfasst eine Wahrheitstafel für eine Aussage bzw. eine Aussagemenge in n Atomen formula_113 Zeilen; das Wahrheitstafelverfahren ist nichts anderes als ein Brute-Force-Verfahren.

Jede dieser Fragestellungen kann auf die Frage zurückgeführt werden, ob eine bestimmte Formel erfüllbar ist:

Die Frage, ob eine Aussage erfüllbar ist, wird Erfüllbarkeitsproblem oder "SAT-Problem" (nach dem englischen Wort für Erfüllbarkeit, "satisfiability") genannt. Das SAT-Problem spielt eine wichtige Rolle in der theoretischen Informatik und Komplexitätstheorie. Das Erfüllbarkeitsproblem für allgemeine (beliebige) Formeln ist NP-vollständig, d. h. (unter der Voraussetzung, dass P ungleich NP) nicht in polynomialer Laufzeit lösbar.

Für bestimmte echte Teilmengen der Formeln der aussagenlogischen Sprache ist das SAT-Problem dennoch schneller, d. h. in polynomial beschränkter Rechenzeit lösbar. Eine solche Teilmenge sind die Horn-Formeln, das sind Konjunktionen von Disjunktionen, deren Disjunkte verneinte oder unverneinte Atome sind, wobei innerhalb einer solchen Disjunktion allerdings höchstens ein Atom unverneint sein darf.

Wenn man die Semantik betrachtet, die hier für die klassische Aussagenlogik aufgestellt wurde, dann erkennt man gewisse Gesetzmäßigkeiten. Wird z. B. die Auswertungsfunktion auf eine Aussage der Form X ∧ W angewendet, wobei W eine beliebige wahre Aussage sein soll, dann stellt man fest, dass die Auswertungsfunktion für X ∧ W immer den Wahrheitswert "wahr" liefert, wenn V(X)=wahr ist (das heißt V(X∧W)=V(X)). Von der Struktur her gleichwertige Gesetzmäßigkeiten gelten auch in anderen Semantiken, auch in solchen, die für ganz andere, nichtlogische Systeme aufgestellt werden. Für die Arithmetik gilt z. B., dass die dortige Bewertungsfunktion (hier V genannt) für einen Ausdruck der Form X + Y immer den Wert von X liefert, sofern der Wert von Y null ist: V(X+Y)=V(X), wenn V(Y) = null ist.

Eine formale Wissenschaft, die solche strukturellen Gesetzmäßigkeiten untersucht, ist die abstrakte Algebra (meist Teilgebiet der Mathematik, aber auch der Informatik). In der abstrakten Algebra wird zum Beispiel untersucht, für welche Verknüpfungen es ein neutrales Element gibt, d. h. ein Element "N", das für eine Verknüpfung "op" dazu führt, dass (für beliebiges X) gilt: X "op" "N" = X. So würde man aus algebraischer Sicht sagen, dass es für die klassische aussagenlogische Konjunktion genau ein neutrales Element gibt, nämlich "wahr", und dass es für die Addition in der Arithmetik ebenfalls genau ein neutrales Element gibt, nämlich die Zahl Null. Nur am Rande sei erwähnt, dass es auch für andere Junktoren neutrale Elemente gibt; das neutrale Element für die Disjunktion ist "falsch": V(X ∨ F) = V(X), wenn V(F)=falsch ist.

Die formale Algebra betrachtet formale Semantiken rein nach ihren strukturellen Eigenschaften. Sind diese identisch, dann besteht zwischen ihnen aus algebraischer Sicht kein Unterschied. Aus algebraischer Sicht, genauer: Aus Sicht der formalen Algebra ist die Semantik für die klassische Aussagenlogik eine zweiwertige Boolesche Algebra. Andere formale Systeme, deren Semantiken jeweils eine Boolesche Algebra bilden, sind die Schaltalgebra und die elementare Mengenlehre. Aus algebraischer Sicht besteht daher zwischen diesen Disziplinen kein Unterschied.

Jede aussagenlogische Formel lässt sich in eine äquivalente Formel in
konjunktiver Normalform und eine äquivalente Formel in
disjunktiver Normalform umformen.

In der Metatheorie werden die Eigenschaften von logischen Systemen untersucht: Das logische System ist in der Metatheorie der Untersuchungsgegenstand.

Eine metatheoretische Fragestellung ist zum Beispiel die, ob in einem Kalkül ein Widerspruch hergeleitet werden kann.

Der vorliegende Abschnitt soll einige wichtige metatheoretische Fragestellungen aus dem Blickwinkel der Aussagenlogik betrachten.





Ein metatheoretisches Resultat ist zum Beispiel die Feststellung, dass alle korrekten Kalküle auch konsistent sind. Ein anderes metatheoretisches Resultat ist die Feststellung, dass ein konsistenter Kalkül nicht automatisch korrekt sein muss: Es ist ohne weiteres möglich, einen Kalkül aufzustellen, in dem zwar kein Widerspruch hergeleitet werden kann, in dem aber z. B. die nicht allgemeingültige Aussage der Form „A ∨ B“ hergeleitet werden kann. Ein solcher Kalkül wäre aus ersterem Grund konsistent, aus letzterem Grund aber nicht korrekt.

Ein weiteres, sehr einfaches Resultat ist die Feststellung, dass ein vollständiger Kalkül nicht automatisch auch korrekt oder nur konsistent sein muss. Das einfachste Beispiel wäre ein Kalkül, in dem "jede" Formel der aussagenlogischen Sprache herleitbar ist. Da jede Formel herleitbar ist, sind alle Tautologien herleitbar, die ja Formeln sind: Das macht den Kalkül vollständig. Da aber jede Formel herleitbar ist, ist insbesondere auch die Formel P ∧ ¬ P und die Formel A ∨ B herleitbar: Ersteres macht den Kalkül inkonsistent, letzteres inkorrekt.

Das Ideal, das ein Kalkül erfüllen sollte, ist Korrektheit und Vollständigkeit: Wenn das der Fall ist, dann ist er der ideale Kalkül für ein logisches System, weil er alle semantisch gültigen Sätze (und nur diese) herleiten kann. So sind die beiden Fragen, ob ein konkreter Kalkül korrekt und/oder vollständig ist und ob es für ein bestimmtes logisches System überhaupt möglich ist, einen korrekten und vollständigen Kalkül anzugeben, zwei besonders wichtige metatheoretische Fragestellungen.

Die klassische Aussagenlogik, wie sie hier ausgeführt wurde, ist ein formales logisches System. Als solches ist sie eines unter vielen, die aus formaler Sicht gleichwertig nebeneinander stehen und die ganz bestimmte Eigenschaften haben: Die meisten sind konsistent, die meisten sind korrekt, etliche sind vollständig, und einige sind sogar entscheidbar. Aus formaler Sicht stehen die logischen Systeme in keinem Konkurrenzverhalten hinsichtlich Wahrheit oder Richtigkeit.

Von formalen, innerlogischen Fragen klar unterschieden sind außerlogische Fragen: Solche nach der Nützlichkeit (Anwendbarkeit) einzelner Systeme für einen bestimmten Zweck und solche nach dem philosophischen, speziell metaphysischen Status einzelner Systeme.

Die Nützlichkeitserwägung ist die einfachere, bezüglich deren Meinungsunterschiede weniger tiefgehend bzw. weniger schwerwiegend sind. Klassische Aussagenlogik zum Beispiel bewährt sich in der Beschreibung elektronischer Schaltungen (Schaltalgebra) oder zur Formulierung und Vereinfachung logischer Ausdrücke in Programmiersprachen. Prädikatenlogik wird gerne angewandt, wenn es darum geht, Faktenwissen zu formalisieren und automatisiert Schlüsse daraus zu ziehen, wie das unter anderem im Rahmen der Programmiersprache Prolog geschieht. Fuzzy-Logiken, nonmonotone, mehrwertige und auch parakonsistente Logiken sind hochwillkommen, wenn es darum geht, mit Wissensbeständen umzugehen, in denen Aussagen mit unterschiedlich starkem Gewissheitsgrad oder gar einander widersprechende Aussagen abgelegt werden sollen und dennoch sinnvolle Schlüsse aus dem Gesamtbestand gezogen werden sollen. Auch wenn es je nach Anwendungsfall sehr große Meinungsunterschiede geben kann, welches logisches System besser geeignet ist, ist die Natur des Problems für alle Beteiligten unmittelbar und in gleicher Weise greifbar. Einzelwissenschaftliche Überlegungen und Fragestellungen spielen sich überwiegend in diesem Bereich ab.

(Noch) kontroverser als solche pragmatischen Überlegungen sind Fragestellungen philosophischer und metaphysischer Natur. Geradezu paradigmatisch ist die Frage, „welches logische System richtig ist“, wobei „richtig“ hier gemeint ist als: Welches logische System nicht nur einen Teilaspekt der Wirklichkeit modellhaft vereinfacht, sondern die Wirklichkeit, das Sein als Ganzes adäquat beschreibt. Zu dieser Fragestellung gibt es viele unterschiedliche Meinungen einschließlich der vom philosophischen Positivismus eingeführten Meinung, dass die Fragestellung als Ganzes sinnlos ist.

In den Bereich metaphysischer Fragestellungen fällt auch die Frage, ob es so etwas wie ein "metaphysisches" Prinzip der Zweiwertigkeit gebe, ob also Aussagen über die Wirklichkeit durchgehend ins Schema wahr/falsch passen oder nicht. Diese Frage ist unabhängig von der Frage, ob die Beschäftigung mit zwei- oder mehrwertigen Logiken praktisch sinnvoll ist: Selbst wenn ein metaphysisches Prinzip der Zweiwertigkeit herrscht, könnte man anwendungspraktisch mehrwertige Logiken nützen, etwa dazu, epistemische Sachverhalte zu fassen, zum Beispiel aus Aussagen zu schließen, die zwar metaphysisch wahr oder falsch sind, von denen aber nicht oder noch nicht bekannt ist, welches von beidem der Fall ist. Umgekehrt kann man auch dann, wenn ein solches metaphysisches Prinzip nicht gilt, zweiwertige Logik wegen ihrer Einfachheit für solche Anwendungen bevorzugen, bei denen nur mit solchen Sätzen umgegangen werden muss, die tatsächlich wahr oder falsch sind.

Die Frage nach einem metaphysischen Prinzip der Zweiwertigkeit ist wie die meisten metaphysischen Fragen nicht endgültig zufriedenstellend beantwortet. Ein früher Einwand gegen ein solches Prinzip, den Aristoteles zur Diskussion stellte, war das Thema der Aussagen über zukünftige Sachverhalte („Morgen wird es regnen“). Wenn Aussagen über Zukünftiges schon heute wahr oder falsch wären, so wird argumentiert, dann müsse die Zukunft bis ins letzte Detail vorbestimmt sein. Ein anderer Einwand, der vorgebracht wird, ist, dass es Aussagen gibt, deren Wahrheit praktisch oder theoretisch nicht festgestellt werden kann – zum Beispiel lässt sich die Wahrheit von „Der Rasen vor dem Weißen Haus bestand am 1. Februar 1870 aus genau 6.120.375,4 Grashalmen“ einfach nicht feststellen.

Befürworter eines metaphysischen Zweiwertigkeitsprinzips berufen sich oft auf das Verhalten von Metatheoretikern, also von Mathematikern oder Logikern, die Aussagen "über" formale Systeme treffen: Egal wie mehrwertig oder nichtklassisch das untersuchte System ist, die dabei getroffenen Metavermutungen, Metabehauptungen und Metafeststellungen sind immer zweiwertig: Ein Kalkül, auch ein parakonsistenter oder nonmonotoner, wird immer als "entweder" konsistent "oder" inkonsistent betrachtet, und ein logisches System ist immer "entweder" korrekt oder inkorrekt, vollständig oder nicht vollständig, entscheidbar oder unentscheidbar, niemals „ein bisschen“ von beidem. Befürworter deuten das als Hinweis darauf, dass es in der Wirklichkeit tatsächlich eine strenge Unterscheidung nach wahr und falsch gebe oder dass es zumindest sinnvoll ist, eine solche anzunehmen.

Eine andere philosophische Fragestellung ist die nach dem metaphysischen Status des Untersuchungsgegenstands der Logik, also danach, was logische Systeme, Kalküle, Wahrheitswerte eigentlich „sind“.

Der platonische Standpunkt besteht darin, dass die in der Logik verwendeten Zeichen und Konstrukte eine außerlogische Bedeutung haben, dass sie Namen für real existierende (wenn auch natürlich nicht-physikalische) Gegenstände sind. In diesem Sinn gäbe es so etwas wie "das Wahre" und "das Falsche", abstrakte Gegenstände, die von den Zeichen „wahr“ und „falsch“ benannt werden.

Der Gegenpol zum Platonismus wäre der Nominalismus, der Existenz nur den Zeichen zuspricht, die in der Logik manipuliert werden. Gegenstand der Logik sind Zeichen, und die Tätigkeit der Logiker ist die Manipulation von Zeichen. Die Zeichen bezeichnen aber nichts, so etwas wie das Wahre oder das Falsche gibt es also nicht. Im Grundlagenstreit der Mathematik entspräche der nominalistischen Position die formalistische Richtung.

Eine Mittelstellung nähme der philosophische Konstruktivismus ein, demzufolge die Zeichen zwar keine unabhängig existierenden Gegenstände bezeichnen, durch den Umgang mit den Zeichen aber Gegenstände konstruiert werden.




</doc>
<doc id="13" url="https://de.wikipedia.org/wiki?curid=13" title="Liste von Autoren/A">
Liste von Autoren/A


</doc>
<doc id="14" url="https://de.wikipedia.org/wiki?curid=14" title="Liste von Autoren/H">
Liste von Autoren/H


</doc>
<doc id="15" url="https://de.wikipedia.org/wiki?curid=15" title="Liste von Autoren/C">
Liste von Autoren/C


</doc>
<doc id="16" url="https://de.wikipedia.org/wiki?curid=16" title="Liste von Autoren/I">
Liste von Autoren/I


</doc>
<doc id="17" url="https://de.wikipedia.org/wiki?curid=17" title="Liste von Autoren/K">
Liste von Autoren/K



</doc>
<doc id="18" url="https://de.wikipedia.org/wiki?curid=18" title="Liste von Autoren/J">
Liste von Autoren/J


</doc>
<doc id="19" url="https://de.wikipedia.org/wiki?curid=19" title="Liste von Autoren/V">
Liste von Autoren/V


</doc>
<doc id="20" url="https://de.wikipedia.org/wiki?curid=20" title="Liste von Autoren/G">
Liste von Autoren/G


</doc>
<doc id="21" url="https://de.wikipedia.org/wiki?curid=21" title="Liste von Autoren/W">
Liste von Autoren/W


</doc>
<doc id="22" url="https://de.wikipedia.org/wiki?curid=22" title="Liste von Autoren/B">
Liste von Autoren/B


</doc>
<doc id="23" url="https://de.wikipedia.org/wiki?curid=23" title="Liste von Autoren/D">
Liste von Autoren/D


</doc>
<doc id="24" url="https://de.wikipedia.org/wiki?curid=24" title="Liste von Autoren/S">
Liste von Autoren/S


</doc>
<doc id="25" url="https://de.wikipedia.org/wiki?curid=25" title="Liste von Autoren/T">
Liste von Autoren/T


</doc>
<doc id="26" url="https://de.wikipedia.org/wiki?curid=26" title="Liste von Autoren/M">
Liste von Autoren/M



</doc>
<doc id="27" url="https://de.wikipedia.org/wiki?curid=27" title="Liste von Autoren/O">
Liste von Autoren/O


</doc>
<doc id="28" url="https://de.wikipedia.org/wiki?curid=28" title="Liste von Autoren/F">
Liste von Autoren/F


</doc>
<doc id="29" url="https://de.wikipedia.org/wiki?curid=29" title="Liste von Autoren/E">
Liste von Autoren/E


</doc>
<doc id="30" url="https://de.wikipedia.org/wiki?curid=30" title="Liste von Autoren/L">
Liste von Autoren/L


</doc>
<doc id="31" url="https://de.wikipedia.org/wiki?curid=31" title="Liste von Autoren/N">
Liste von Autoren/N


</doc>
<doc id="32" url="https://de.wikipedia.org/wiki?curid=32" title="Liste von Autoren/P">
Liste von Autoren/P


</doc>
<doc id="33" url="https://de.wikipedia.org/wiki?curid=33" title="Liste von Autoren/Q">
Liste von Autoren/Q



</doc>
<doc id="34" url="https://de.wikipedia.org/wiki?curid=34" title="Liste von Autoren/R">
Liste von Autoren/R


</doc>
<doc id="35" url="https://de.wikipedia.org/wiki?curid=35" title="Liste von Autoren/U">
Liste von Autoren/U


</doc>
<doc id="36" url="https://de.wikipedia.org/wiki?curid=36" title="Liste von Autoren/Y">
Liste von Autoren/Y


</doc>
<doc id="37" url="https://de.wikipedia.org/wiki?curid=37" title="Liste von Autoren/Z">
Liste von Autoren/Z


</doc>
<doc id="38" url="https://de.wikipedia.org/wiki?curid=38" title="Anthony Minghella">
Anthony Minghella

Anthony Minghella, CBE (* 6. Januar 1954 auf der Isle of Wight, Großbritannien; † 18. März 2008 in London) war ein britischer Filmregisseur, Filmproduzent, Drehbuchautor, Dramatiker, Hörspiel-Autor, Theater- und Opern-Regisseur.

Minghella war der Sohn italienisch-schottischer Eltern, die auf der Isle of Wight eine Fabrik für Eiscreme betrieben. Nach seinem Schulabschluss studierte er an der Universität Hull, wo er eine Zeit lang als Dozent tätig war. 1978 drehte er einen ersten Kurzfilm. Seit 1981 war er als Autor und Story Editor tätig. Er wurde mit Theaterstücken, Rundfunkhörspielen, der Fernsehserie "Inspector Morse" und vielen Drehbüchern für Film und Fernsehen bekannt. Er entwickelte die Drehbücher für die 1988 erfolgreich ausgestrahlte Fernsehserie The Storyteller von Muppets-Erfinder Jim Henson.

Auch als Produzent war er erfolgreich, darunter für die Filme "Der stille Amerikaner", "Die Dolmetscherin" und "Der Vorleser", für den er 2008 posthum für den Oscar (Kategorie „Bester Film“) nominiert wurde. Gemeinsam mit seinem Freund und Kollegen Sydney Pollack gründete er die Produktionsfirma Mirage Enterprises. Der Regisseur Minghella galt als ein guter Schauspielerführer: Unter seiner Regie brachten es zahlreiche Darsteller zu Oscar-Nominierungen, zwei Schauspielerinnen erhielten die Auszeichnung als „Beste Nebendarstellerin“: Juliette Binoche ("Der englische Patient") und Renée Zellweger ("Unterwegs nach Cold Mountain").

Gegen Ende seines Lebens kehrte Minghella zu seinen Anfängen im Radio und auf der Bühne zurück: 2006 wurde sein Hörspiel "Eyes Down Looking" mit Jude Law zu Ehren von Samuel Beckett auf BBC Radio 3 ausgestrahlt, ein Jahr zuvor hatte seine Inszenierung der Puccini-Oper Madame Butterfly in der English National Opera in London Premiere und wurde auch in der Nationaloper von Vilnius und in der Metropolitan Opera in New York gezeigt. Am Ende des Films "Abbitte" von Joe Wright (2007) hat er einen Kurzauftritt als Talkshow-Moderator neben Vanessa Redgrave. Seine letzte Arbeit als Drehbuchautor war das Skript für den Musical-Film "Nine" (gemeinsam mit Michael Tolkin). Zu seinen letzten Regiearbeiten zählt der Pilotfilm zur Krimiserie "Eine Detektivin für Botswana" (Originaltitel: ), den die BBC fünf Tage nach seinem Tod erstmals ausstrahlte.

Minghella war mit der aus Hongkong stammenden Choreographin, Produzentin und Schauspielerin Carolyn Choa ("Wie verrückt und aus tiefstem Herzen") verheiratet. Der Ehe entstammen zwei Kinder, die in der Filmbranche tätig sind: Tochter Hannah Minghella in der Produktion und Sohn Max Minghella als Schauspieler ("Agora – Die Säulen des Himmels"). Die Tante Edana Minghella und der Onkel Dominic Minghella (u. a. für die deutsche Fernsehserie "Doktor Martin") sind Drehbuchautoren.

Minghella starb im Alter von 54 Jahren in einem Londoner Krankenhaus an inneren Blutungen infolge der Operation eines Tonsillenkarzinoms und eines Karzinoms im Nacken.

1984 erhielt Minghella den Londoner Kritikerpreis als meistversprechender junger Dramatiker, 1986 den Kritikerpreis für sein Stück "Made in Bangkok" als bestes Stück der Saison. 1997 erhielt er für "Der englische Patient" den Oscar in der Rubrik "Beste Regie", 1999 eine Oscar-Nominierung in der Kategorie „Bestes adaptiertes Drehbuch“ für "Der talentierte Mr. Ripley", bei dem er auch Regie führte.

2001 wurde Minghella zum Commander of the British Empire (CBE) ernannt. Von 2003 bis 2007 war er Präsident des British Film Institute. Seit 1997 trägt das Anthony Minghella Theatre auf der Isle of Wight seinen Namen.




</doc>
<doc id="39" url="https://de.wikipedia.org/wiki?curid=39" title="US-amerikanischer Film">
US-amerikanischer Film

Die Geschichte des US-amerikanischen Films ist ein Kapitel der Filmgeschichte, das gerade wegen der hervorgehobenen Stellung der Vereinigten Staaten als Filmnation sowohl für die Filmkunst als auch für die Ökonomie des Films relevant ist. Weltruhm erlangte Hollywood, ein Stadtteil von Los Angeles, als Zentrum der US-amerikanischen Filmindustrie, weshalb der Name oft auch als Synonym für die gesamte amerikanische Film-Branche steht. Synonym für Hollywoods Filmindustrie wird wiederum der Begriff "Traumfabrik" ( "Dreamfactory") verwendet.
Bis 1912 konzentrierten sich die US-amerikanischen Filmunternehmen auf den inneramerikanischen Filmwettbewerb. Erst danach stieg ihr Einfluss auf dem Weltmarkt. Und zwar so rapide, dass sie bereits 1914, zu Beginn des Ersten Weltkriegs, die Hälfte der Welt-Filmproduktion stellten.

Der harte Wettkampf zwischen dem Edison Trust und den von Carl Laemmle angeführten „Independents“ hatte wirksame Instrumente geschaffen, die, am nationalen Konkurrenten erprobt und verfeinert, nun mit zunehmender Härte die internationalen Mitbewerber trafen. Dennoch war die Vormachtstellung Hollywoods längst nicht unangreifbar, erst eine politische Entwicklung verschaffte ihr die nötige Ruhe zur Restrukturierung: Der Krieg in Europa.

Die französische Filmproduktion, Hauptkonkurrent der US-Amerikaner, kam mit dem Ausbruch des Krieges sofort und vollständig zum Erliegen, denn Pathé wandelte seine Rohfilm-Fabrik in eine Munitionsfabrik um und seine Studios in Kasernen. Ähnlich, und doch weniger extrem, brach die italienische Produktion beim Kriegseintritt des Landes 1916 ein.

Nachdem absehbar war, dass der Krieg sehr lange dauern konnte, bemühten sich die Franzosen, wieder ins Geschäft zu kommen. Die Position, die sie vor Ausbruch des Krieges innehatten, erreichten sie nicht mehr. Zudem beschloss das Deutsche Reich 1916 das generelle Filmeinfuhrverbot, was die europäischen Filmnationen ihres wichtigsten Absatzmarktes beraubte. Auch der Export nach Übersee gestaltete sich zunehmend schwierig, denn die Militärs beanspruchten viele Transportkapazitäten für sich. Außerdem führten deutsche U-Boote und kleinere Kreuzer einen Handelskrieg gegen die Entente-Mächte, wobei auch zivile Frachter versenkt wurden, da man die Entente verdächtigte, sie für Waffenlieferungen zu missbrauchen (z. B. die Versenkung der RMS Lusitania).

Die Macht der Motion Picture Patents Company (MPPC) war 1914 bereits weitgehend gebrochen, die später folgenden Gerichtsurteile waren nur noch Formalitäten. Sowohl die nationale als auch die internationale Konkurrenz der Independents waren also ausgeschaltet. Die US-Filmwirtschaft verlor zwar einen Teil des europäischen Absatzmarktes, doch der Bedarf an frischen Filmen innerhalb der Vereinigten Staaten war höher als in ganz Europa zusammen, so gab es beispielsweise 1916 bereits ca. 28.000 Kinos in ganz Amerika.
Auch in der übrigen Welt nahmen die Hollywood-Unternehmen eine dominierende Stellung ein, sie stellten zum Beispiel einen Großteil der in Australien und Südamerika gezeigten Filme, die ab ca. 1916 direkt vertrieben wurden (früher war es üblich, an lokale Zwischenhändler zu verkaufen).

Nach Robert C. Allen und Douglas Gomery basiert der freie Wettbewerb zwischen Unternehmen auf vier Punkten:


Der erste Versuch, den freien Wettbewerb zu zerstören und ein Oligopol zu bilden, wurde mittels der Patente betrieben. MPPC versuchte, den Zugang fremder Unternehmen zu behindern, indem sie diesen durch Lizenzgebühren den Wettbewerb erschwerte. Um das System durchzusetzen, sollte zudem eine hohe Marktdurchdringung erfolgen. Auf ihrem Höhepunkt kontrollierte die MPPC via Lizenz den Großteil der Kinos. Auch der Zugang zu Filmmaterial war nicht ohne Lizenz möglich, da Eastman Kodak einen Exklusivvertrag mit der MPPC geschlossen hatte.

Der Edison-Trust attackierte also vor allem die Punkte 2-4. Das System scheiterte endgültig mit der Annullierung der Edison-Patente durch den Obersten Gerichtshof der Vereinigten Staaten, sein Niedergang jedoch hatte schon wesentlich früher begonnen.

Den freien Zugang zum Filmmaterial erlangten die Independents durch den Bau eigener Kameras und durch die Aufhebung des Patents auf Rohfilme 1912. Und um mit dem Trust konkurrieren zu können, begannen sie, ihre Filme von denen der MPPC unterscheidbar zu machen. Hierbei entstanden der Feature Film und das „Starsystem“.

Die MPPC war zwar nicht blind gegenüber diesen Neuerungen, auch sie drehte Feature Films, durch ihre Struktur und vor allem durch ihre Kundenstruktur, war sie dennoch nicht in der Lage, mit diesen neuen Instrumenten zu experimentieren. Der Trust wollte Massenware verkaufen um eine bestimmte Marge zu erwirtschaften. Teure Stars hätten nur die Kosten hochgetrieben, und Feature Films bargen ein nicht zu unterschätzendes Risiko, für das die Kunden des Trusts nicht aufkommen wollten. So konnten die „Independents“ den ersten Punkt des freien Wettbewerbs unterhöhlen und einzigartige Filmerlebnisse statt austauschbarer Produkte bieten, was dem Publikumsinteresse deutlich entgegenkam und vor allem finanzkräftigere Mittelschichten erschloss.

Der Feature Film kommt ca. 1909 auf und wird nur von den Independents ernsthaft weiterentwickelt, beispielsweise von Famous Players, die später nur noch Features produzieren. Famous Players sind auch die erste Gesellschaft, die das Starsystem konsequent nutzt, nach früheren Versuchen, z. B. von I.M.P.

Durch die oben genannten Schritte schaffen es die Independents, sich eine Position im Markt zu sichern und immer weiter auszubauen. Für nationales und internationales Wachstum fehlen ihnen effiziente Strukturen, zum Beispiel in der Distribution. Noch bis in die Mitte der 1910er Jahre hält sich das alte States-Rights-System, in dem der Produzent lokale Franchise-Rechte seines Films an einen Distributor verkauft, der diese dann innerhalb seines festgelegten Gebiets an Kinos weiter verleiht.

Diese Situation ändert sich erstmals 1914 mit der Fusion von elf regionalen Distributoren zu Paramount, die als erste landesweite Rechte handelt. Durch ihre schiere Größe kann das Unternehmen wesentlich kosteneffizienter arbeiten als die Mitbewerber, ganz abgesehen davon, dass dieses System auch für die Produktionsgesellschaft erhebliche Vorteile mit sich bringt. Das alte System kommt bis 1918 zum Erliegen.

Kurz nach ihrer Gründung schließt Paramount Fünfjahresverträge mit Famous Players, Lasky und Bosworth ab, die später auf 25 Jahre verlängert werden. Hier zeichnet sich ein Trend ab, der 1914 zunehmend an Bedeutung gewinnt: Die Verflechtung der bisher getrennten Bereiche Distribution, Produktion und Vorführung, ein Phänomen, das in der Fachliteratur als Vertikale Integration bezeichnet wird. Die Bindung durch die Fünfjahresverträge ist vorteilhaft für alle Beteiligten: Jeder profitiert vom Erfolg des anderen. Wenn das Lasky-Programm sehr gut ist, wird das Paramount-Sortiment von mehr Kinos gekauft, wovon auch Famous Players und Bosworth profitieren, da ihr Programm so auch eine größere Verbreitung findet. Die Kooperation führt dann auch, zwei Jahre später, zur Fusion der genannten und noch einiger weiterer Unternehmen.

Doch es lassen sich durchaus auch frühere Beispiele für vertikale Integration finden. So sind 1912 unter dem Namen Universal erstmals alle drei Bereiche des Filmbusiness vereint. Es fehlte allerdings eine große First-Run-Kinokette. Dennoch schien der Branche die Fusion so bedrohlich, dass die Gründung von Mutual eine direkte Gegenmaßnahme darstellen sollte. Auch hier fanden sich viele Unternehmen unter einem Dach zusammen, denen es explizit nur um Distribution und Produktion ging.

Auch William Fox besitzt 1913 ein Distributions- und ein Produktionsunternehmen, die allerdings erst später zusammengeführt werden. Von Seiten der Kinokettenbesitzer ist zunächst wenig zu hören, erst 1915 schließen sich drei große Ketten, Rowland, Clarke und Mayer, zur Metro Pictures Corporation zusammen, einer Produktionsgesellschaft.

Die wirklich große Reaktion der Kinobesitzer kam erst 1917. Zu diesem Zeitpunkt war die fusionierte Paramount zur dominanten Gesellschaft geworden, die ihre Filme mittels Block-Booking vertrieb. Das hieß, um einen Film mit einem Star vom Kaliber einer Mary Pickford zu bekommen, musste man ein komplettes Paket erwerben, dessen große Mehrheit bestenfalls als durchschnittlich zu bezeichnen war. Andererseits konnte man dem Kauf der Pakete schlecht entgehen, wenn man nicht sein Publikum an ein anderes Kino verlieren wollte, das ebendiesen Mary-Pickford-Film zeigte.

Um dieses System zu durchbrechen, schlossen sich 26 der größten nationalen First-Run-Kinokettenbesitzer zum First National Exhibitors Circuit zusammen. Mit ihrer erheblichen Kaufkraft wollten sie gemeinsame Einkäufe tätigen und auch distribuieren. Zuerst war es das Ziel, Stars zu kaufen, ihre Filme zu finanzieren und im Gegenzug das Aufführungsrecht zu erwerben sowie das Recht, die entstandenen Filme regional weiter zu verleihen.

Sehr bald kam auch eine eigene Produktion dazu. Zwischen 1917 und 1918 nahm First National Charlie Chaplin und Mary Pickford für jeweils eine Million Dollars unter Vertrag. Beide erhielten vollständige künstlerische Freiheit. First National kontrollierte zu diesem Zeitpunkt bereits ca. 600 Kinos, 200 davon Erstaufführungshäuser.

Aus den First-Run-Kinos stammten bis zu 50 Prozent der Einnahmen der Produzenten, außerdem waren Kinos die verlässlichsten Geldverdiener im recht unsteten Filmgeschäft, da das Betreiberrisiko viel geringer war als beispielsweise in der Produktion. Darüber hinaus entschied der Erfolg in den First-Runs über eine lukrative Distribution.

Wenn Paramount also seine Abnehmer und sein Publikum nicht verlieren wollte, musste ein Gegenschlag erfolgen. Also stieg die Gesellschaft, mit finanzieller Unterstützung des Bankhauses Kuhn, Loeb & Co., ins Geschäft mit den Kinos ein, anfangs mit einer Summe von 10 Millionen Dollars. Somit wurde Paramount der erste vollintegrierte, oder komplett vertikal integrierte Filmkonzern.

So wurden aus den alten Independents die Inhaber des zweiten Oligopols. Am Ende der 1910er Jahre war der erste Punkt des freien Wettbewerbs durch das Starsystem und Feature-Filme außer Kraft gesetzt, der zweite Punkt durch die schiere Größe der Unternehmen: Weniger als zehn Unternehmen kontrollierten über 50 Prozent des Marktes. Durch die Vereinigung der Distribution und durch den beginnenden Kampf um die Kinos waren auch die letzten beiden Bedingungen für einen funktionierenden Wettbewerb ausgehebelt.

Ein neues Unternehmen konnte weder einen genügenden Zugang zu den Kinos noch Zugriff auf die Stars, also auf die essentiellen Ressourcen der Filmproduktion erhalten. Auch waren die Produktionskosten stark gestiegen. Zwischen 50.000 und 100.000 US-Dollar pro Film waren normal, nach oben gab es keine Beschränkungen. Ein Großteil dieses Geldes floss in die Taschen der Stars, der Rest wurde in bessere Ausstattung investiert, eine weitere Hürde für Neueinsteiger.

Um dem Trend zu höheren Gagen entgegenzuwirken, und um, wie später in einer Anhörung des Obersten Gerichtshofs bekannt wurde, ein Monopol zu errichten, planten First National und Paramount eine Fusion im Wert von 40 Millionen US-Dollar. Es war geplant, mit jedem bedeutenden Kinobesitzer in den Vereinigten Staaten einen Fünf-Jahres-Vertrag abzuschließen. Die Stars hätten dann keine Grundlage mehr für irgendwelche Forderungen gehabt.

Die Pläne zu diesem Merger wurden von einem Privatdetektiv aufgedeckt, der im Auftrag von Charlie Chaplin, Mary Pickford, Douglas Fairbanks und D. W. Griffith herausfinden sollte, warum weder First National noch Paramount ihre Verträge verlängerte. Natürlich waren sie entsetzt über solche Aussichten und beschlossen, dem entgegenzuwirken, indem sie ihr eigenes Unternehmen gründeten.

1919 entstand United Artists als Gesellschaft für den Filmvertrieb. Finanziert wurde das Unternehmen durch die Morgan-Gruppe sowie durch eine Einlage von 100.000 US-Dollar für Vorzugs-Anteilscheine durch die Eigentümer. Daneben existierten auch normale Anteilscheine, bei deren Weiterverkauf United Artists ein Vorkaufsrecht hatte.

Die Gesellschaft hatte keine eigenen Studios, sondern nutzte die Studios seiner Mitglieder. Sie war errichtet worden als reine Dienstleistungsgesellschaft, die nicht auf Rendite arbeiten sollte, sondern den Besitzern größtmögliche Autonomie und Profite aus dem Geschäft mit ihren Filmen einräumte. Es gab kein Block-Booking, jeder Film wurde individuell vertrieben und musste allein durch seine künstlerischen Qualitäten überzeugen. Die Verleihgebühren der United Artists lagen deutlich unter denen von First National und Paramount, stellten also eine erhebliche Bedrohung für die marktbeherrschende Stellung der beiden dar.

Die Fusion der beiden Giganten war auch gescheitert, weil ihr wichtigstes Kapital, die Stars, sich auf und davon gemacht hatte. First National war also immer noch Konkurrent Paramounts, und die United Artists mit ihren qualitativ sehr hochwertigen Filmen und ihrer enormen Beliebtheit brachten das Unternehmen weiter in Bedrängnis. Also versuchte Paramount das, was man heute eine feindliche Übernahme nennen würde: Stück für Stück wurden die in der First National zusammengeschlossenen Kinoketten aufgekauft.

Auch andere Unternehmen versuchten nun, Kontrolle über die Erstaufführungshäuser zu erlangen, sogar United Artists sah sich später, 1924, mangels Abnehmern gezwungen, eine eigene Kette zu gründen. Wie auch schon in der Vergangenheit, wurden die Kämpfe um die Kinos mit harten Bandagen ausgetragen, vor allem Paramounts „dynamite gang“, auch „wrecking crew“ genannt, wurde ihrem Ruf gerecht. Eine weit verbreitete Methode, Kinos an sich zu binden, war das Blocksystem.

Seit 1917 begannen US-amerikanische Unternehmen, ihre Gewinne auf der Basis von in- und ausländischen Verkäufen zu schätzen. Aus dieser Gewinnschätzung ergab sich das Budget der Produktion, das dadurch erhöht wurde, was für die ausländische Konkurrenz doppelt schlecht war. Die Produktionskosten eines Filmes wurden in den Vereinigten Staaten amortisiert, und später wurden die Filme billig im Ausland angeboten, wodurch die internationale Konkurrenz nicht mehr mithalten konnte.

US-amerikanische Filme galten als qualitativ besser und waren im Erwerb trotzdem günstiger als z. B. deutsche Produktionen. Auch waren die Infrastruktur und die Rationalisierung der Produktionsabläufe nirgends so weit gediehen wie in Hollywood, ein Resultat auch des wachsenden Einflusses der Banken.

Als der Erste Weltkrieg vorbei war, und die Menschen in den bislang abgeschnittenen Ländern wie Deutschland oder Österreich erstmals wieder Hollywood-Produktionen zu sehen bekamen, erlebten sie einen wahren Quantensprung in der Qualität. Die führenden europäischen Filmproduktionsländer, deren isolierte Filmindustrien fünf Jahre lang unter dem Ersten Weltkrieg gelitten hatten, und zudem mit viel geringeren Budgets zu kämpfen hatten, konnten der Konkurrenz aus den Vereinigten Staaten nur noch wenig entgegensetzen. Bis 1927 erhöhte sich der Anteil der amerikanischen Filmproduktion an der Weltfilmproduktion auf nahezu 90 %, was zu Beginn der 1920er Jahre die Filmwirtschaft in England, Frankreich, Italien, Deutschland und Österreich schwer in Bedrängnis brachte und die dortige Filmproduktion stark zurückgehen ließ. Zahlreiche europäische Filmproduktionsgesellschaften mussten schließen. 1925 wurden alleine nach Österreich 1200 US-Produktionen exportiert, obwohl der Bedarf der dortigen Kinos auf lediglich rund 350 geschätzt wurde. In vielen Ländern wurden Filmkontingente eingeführt, die die erlaubte Anzahl an Filmimporten aus den Vereinigten Staaten regelten.

Da rund 45 % der Gewinne zu dieser Zeit aus Europa kamen, wurden die Restriktionen in Europa von den amerikanischen Filmmagnaten mit Argwohn betrachtet. Zumeist erfolglos wurde gegen Einfuhrbeschränkungen Lobbying betrieben. In Ungarn jedoch wurden die geplanten Einfuhrbeschränkungen nicht eingeführt, nachdem die US-amerikanische Filmindustrie den ungarischen Behörden damit gedroht hatte, keine Filme mehr in Ungarn zu zeigen.

1927 waren nach Zahlen des US-Handelsdepartements beim amerikanischen Film 350.000 Personen beschäftigt. Zur Filmproduktion wurden rund 500.000 Kilometer Filmband verbraucht, wofür mehr Silber benötigt wurde, als der Umlauf an Silbermünzen in den Vereinigten Staaten ausmachte. Es wurden Filme im Ausmaß von 75.000 Kilometer Filmband und einem damaligen Wert von rund 320 Millionen Mark exportiert. Ende des Jahres 1927 zählten die Vereinigten Staaten 21.642 Kinos, die in jenem Jahr insgesamt 3 Milliarden Mal besucht wurden, was wiederum einen Erlös aus dem Eintrittsgeld von rund 2,5 Milliarden Dollar ergab.

Während Amerika den weltweiten Filmmarkt fast ohne nennenswerte Konkurrenz dominierte, hatten ausländische Produktionen am US-Markt kaum eine Chance. Spielten in manchen Ländern jährlich bis zu 1000 oder mehr US-Filmproduktionen in den Kinos, liefen in den gesamten Vereinigten Staaten im Jahr 1927 nur 65 ausländische Filme, davon 38 aus Deutschland, neun aus England, sechs aus Frankreich, vier aus Russland, je zwei aus Österreich und Italien und je einer aus China und Polen. Selbst diese Filme waren zumeist nur wenig verbreitet und liefen fast ausschließlich auf so genannten Filmkunstbühnen.

Ab 1933, verstärkt jedoch ab Beginn des Zweiten Weltkriegs und der Ausbreitung des Deutschen Reichs auf immer weitere Teile Europas, setzte eine Emigrationswelle von zumeist jüdischen Filmschaffenden aus Europa ein. Waren deren Auswanderungsziele zu Beginn noch häufig europäische Städte mit Filmindustrie wie Wien, Paris oder London, kristallisierte sich bald die aufstrebende Filmindustrie Hollywoods als begehrtestes und vielversprechendstes Ziel der Emigranten heraus – verstärkt durch gezieltes Anwerben europäischer Filmgrößen durch Hollywood-Studiobosse.

Von den etwa 2000 jüdischen Filmschaffenden, die im Deutschen Reich keine Arbeit mehr fanden und auswandern mussten, fanden sich letztendlich rund 800 in Hollywood wieder – darunter fast die gesamte Elite des deutschsprachigen Filmschaffens dieser Zeit. Vielen gelang dort eine ruhmvolle Karriere, viele, vor allem jene, die 1938 und noch später ohne Arbeitsangebot in Hollywood ankamen, konnten nicht mehr an ihre bisherige Karriere anschließen und kamen nur in schlecht bezahlten und unbedeutenden Positionen unter oder mussten nach einer Weile gar das Filmgeschäft aufgeben. Statt der bisher aus Berlin und Wien gewohnten Kaffeehäuser, wo man sich einst regelmäßig traf, wurden nun große Appartements und Villen von in Hollywood erfolgreichen Emigranten neue Treffpunkte. Beliebte Treffpunkte der Film- und Theaterschaffenden waren die Adressen von Henry Koster, Paul Henreid, Ernst Deutsch-Dryden, Paul Kohner und später auch von Sam Spiegel. Die literarische Emigration, inklusive Drehbuchautoren, traf sich häufig bei Salka Viertel und bei Brecht.


Hollywood

Experimentalfilm

Dokumentarfilm

Independent film



</doc>
<doc id="51" url="https://de.wikipedia.org/wiki?curid=51" title="Vorsätze für Maßeinheiten">
Vorsätze für Maßeinheiten

Vorsätze für Maßeinheiten, Einheitenvorsätze, "Einheitenpräfixe" oder kurz "Präfixe" oder "Vorsätze" dienen dazu, Vielfache oder Teile von Maßeinheiten zu bilden, um Zahlen mit vielen Stellen zu vermeiden.

SI-Präfixe sind für die Verwendung im Internationalen Einheitensystem (SI) definierte Dezimalpräfixe. Sie basieren auf Zehnerpotenzen mit ganzzahligen Exponenten. Man unterscheidet zwischen dem Namen des Präfix und seinem Symbol. Die Symbole sind international einheitlich. Die Namen unterscheiden sich je nach Sprache.

Die Zeichen für Teile einer Einheit werden als Kleinbuchstaben geschrieben, während die meisten Zeichen für Vielfache einer Einheit als Großbuchstaben geschrieben werden. Ausnahmen von dieser Systematik sind aus historischen Gründen die Zeichen für Deka (da), Hekto (h) und Kilo (k).

Das Mikro-Zeichen „µ“ stammt als einziges Präfix-Symbol aus der griechischen Schrift, was beim Maschinenschreiben und Drucken in der Praxis Schwierigkeiten bereitet hat. In der elektrotechnischen Literatur wurde deshalb ersatzweise häufig ein „u“ verwendet. Das wurde in der Internationalen Norm ISO 2955 von 1983, die 2001 zurückgezogen wurde, auch so empfohlen. Für Deutschland gelten weiterhin die Empfehlungen der DIN-Norm DIN 66030 „Informationstechnik – Darstellung von Einheitennamen in Systemen mit beschränktem Schriftzeichenvorrat“ vom Mai 2002. In Österreich sieht das Maß- und Eichgesetz „μ“ vor. Beim Austausch medizinischer Daten gemäß dem HL7-Standard ist das „u“ anstelle von „µ“ zugelassen.

Die Einheitenvorsatzzeichen werden wie die Einheitenzeichen in aufrechter (nicht kursiver) Schrift geschrieben, unabhängig von der Schriftart (Schriftauszeichnung) des umgebenden Textes. Zwischen Einheitenvorsatzzeichen und Einheitenzeichen wird kein Zwischenraum geschrieben.


Der Name eines Einheitenvorsatzes bildet mit dem zugehörigen Einheitennamen ein zusammengesetztes Wort. Beispiele sind "Nanometer" oder "Milligramm." Wenn aus dem Zusammenhang klar ist, welche Einheit gemeint ist, wird dieses zusammengesetzte Wort in der Umgangssprache häufig auf den Vorsatz verkürzt. So ist von "Kilo" die Rede, wenn "Kilogramm" (kg) gemeint ist. Im technischen Bereich wird der Mikrometer (µm) kurz als "My" [] bezeichnet; im Englischen ist die Bezeichnung "micron" für Mikrometer üblich. Im Österreichischen wird das Kurzwort "Deka" für die Masseeinheit "Dekagramm" (dag) verwendet, unter Handwerkern auch "Zenti" für Zentimeter (cm).

Im Flächenmaß Hektar verschwindet ausnahmsweise an der Wortfügestelle das „o“ von „hekto“, was den Doppelselbstlaut vermeidet.

Bis 1960 waren in Frankreich die Vorsätze „Myria“ (gr. = zehntausend) mit dem Zeichen ma für das 10‑fache und "dimi" mit Zeichen dm für das 10‑fache genormt. Statt "myria" wurde Anfang des 19. Jahrhunderts auf einen Vorschlag von Thomas Young hin z. T. auch "myrio" geschrieben.

Bis um 1900 wurde im Deutschen „Centimeter“ mit C geschrieben.

Früher waren in Deutschland auch das Symbol D und in Großbritannien dk für "Deka" üblich, in Österreich war das Zeichen dk bis Mitte der 1950er Jahre gesetzlich vorgeschrieben.

In DIN 1301 Teil 1 vom Dezember 1993 wurde der SI-Vorsatz für 10 „Yocto“ geschrieben; diese Schreibweise wurde in der Ausgabe vom Oktober 2002 zu „Yokto“ korrigiert.

Bis 1950 wurden die elektrische Kapazität von Kondensatoren, aber auch die Selbstinduktion von Spulen in cm (Centimeter) des CGS-Einheitensystems angegeben. Bisweilen wurde pF (Piko-Farad) auch als µµF geschrieben. Der besseren Lesbarkeit wegen findet sich auf oft kleinen Bauteilen statt µF gelegentlich uF (siehe dazu Abschnitt "Typographie"), MF oder (im Englischen) MFD.; geläufig ist auch KV statt kV für Spannung und MEGOHM statt MΩ für Widerstand.

Im Sprachgebrauch von Internetbenutzern wird mitunter das SI-Präfix k verwendet, in Kontexten, wo das sonst kaum üblich ist, z. B. bei Zeit- und Stückzahlangaben. Vergleiche auch den besonders speziellen Fall der Bezeichnung Y2K für das Jahr-2000-Problem oder W2K für Microsoft Windows 2000. Im kaufmännisch-technischen Umfeld wird das Präfix k außerdem häufig mit Währungseinheiten verwendet, etwa als k€. Die dort ebenfalls verwendete Kombination T€ stammt nicht aus dem SI, sondern bedeutet „Tausend Euro“.

In der Datenverarbeitung werden SI-Präfixe auch für Datenmengen (Bits und Bytes) verwendet, allerdings oft in der Bedeutung als Binärpräfix (Vielfache von 1024, z. B. 2, 2, 2 usw.). Bis heute werden bei Datenmengen je nach Kontext, unter Umständen je nach betrachtetem Speichermedium, die SI-Präfixe als Dezimalpräfixe oder Binärpräfixe verwendet, was insbesondere bei höheren Werten zu erheblichen Abweichungen führt.

Die für die Normung in der Elektrotechnik zuständige International Electrotechnical Commission hat daher zuerst in der Norm IEC 60027-2 (ersetzt durch IEC 80000-13:2008) besondere, an die SI-Präfixe angelehnte Binärpräfixe gemäß unten stehender Tabelle definiert und empfiehlt deren Verwendung für Datenmengen. Die dezimalen SI-Präfixe sollen bei Datenmengen das gleiche bedeuten wie bei SI-Einheiten (Dezimalpräfixe). Das für die SI-Präfixe zuständige Internationale Büro für Maß und Gewicht (BIPM) empfiehlt ebenfalls die Anwendung dieser Norm:

Das binäre Präfixsymbol entsteht durch das Anhängen von -i an das entsprechende dezimale Präfixsymbol. Ki wird dabei im Gegensatz zu k groß geschrieben. Das binäre Präfix selbst entsteht durch das Anhängen von -bi an die ersten beiden Buchstaben des entsprechenden dezimalen Präfixes.

Beispiel für die Verwendung: Ein Kibibyte wird geschrieben als 1 KiB = 2 B = 1024 B, wobei B für Byte steht.



</doc>
<doc id="53" url="https://de.wikipedia.org/wiki?curid=53" title="Abkürzungen/Gesetze und Recht">
Abkürzungen/Gesetze und Recht

Viele, aber nicht alle Fachautoren kürzen ohne Punkt und Leerzeichen ab: So wird die Abbreviatur für "in der Regel", normalerweise "i. d. R.", mit "idR" abbreviert.
Ungeachtet dessen werden Akronyme, wie sonst auch, ohne Punkte gebildet.
Folgende Liste von Abkürzungen aus der Rechtssprache versucht nur sprachlich korrekte Abbreviaturen, also mit Punkt und Leerzeichen, aufzuführen, sofern nicht die geraffte Schreibweise in den Alltag Einzug gefunden hat und dort dominiert (z. B. "eG", "GbR" oder "MdB") oder die Abbreviatur nur in juristischen Schriften und dort auch nur zusammengeschrieben auftaucht.







Referatsleiter der Generalprokurator am OGH in Österreich "oder" erster schweizerischer Bundesanwalt "oder" Geschäftsanweisung "oder" Allgemeine Luftfahrt (General Aviation) "oder" Gemeinschaftsaufgabe "oder" Gutachterausschuss



















(Tierschutztransportverordnung) (siehe Tiertransport#Gesetzliche Bestimmungen)










</doc>
<doc id="54" url="https://de.wikipedia.org/wiki?curid=54" title="Liste von Abkürzungen (Computer)">
Liste von Abkürzungen (Computer)

Dies ist eine Liste technischer Abkürzungen, die im IT-Bereich verwendet werden.




</doc>
<doc id="56" url="https://de.wikipedia.org/wiki?curid=56" title="Liste von Unternehmen mit Namensherkunftserklärungen">
Liste von Unternehmen mit Namensherkunftserklärungen

Dies ist eine Liste von Firmen (Namen der Unternehmen) sowie ihrer Herkunft (Etymologie). Häufig sind die Firmen Abkürzungen oder Akronyme der Namen der Gründer, voriger Gesellschaften oder des Unternehmensziels.






























</doc>
<doc id="61" url="https://de.wikipedia.org/wiki?curid=61" title="ISO 4217">
ISO 4217

ISO 4217 ist die von der Internationalen Organisation für Normung publizierte Norm für Währungs-Abkürzungen, die im internationalen Zahlungsverkehr zur eindeutigen Identifizierung benutzt werden sollen. Am 1. August 2015 wurde die neue Version ISO 4217:2015 veröffentlicht. Diese 8. Version ersetzt den Vorgänger aus dem Jahr 2008.

Die Abkürzungen umfassen jeweils drei Buchstaben. Die ersten beiden sind üblicherweise die Landeskennung nach ISO 3166-1 ALPHA-2 (beispielsweise "AU" für Australien), der letzte Buchstabe ist in der Regel der Anfangsbuchstabe des Währungsnamens, so beispielsweise "D" für "Dollar". Gemeinsam ergibt dies "AUD" als genormte Abkürzung für den Australischen Dollar.

Währungen, die nicht von einem Einzelstaat herausgegeben werden, haben als ersten Buchstaben ein "X"; die beiden folgenden Buchstaben geben den Namen der Währung an. Dies ist sowohl bei den meisten Währungsunionen der Fall (z. B. der Ostkaribische Dollar ("XCD")), als auch bei den IWF-Sonderziehungsrechten ("XDR").

Von diesen Regeln wird in den folgenden Fällen abgewichen:

Auch für nicht-geldliche Wertaufbewahrungs- und Transaktionsmittel gibt es Kodierungen. So wird eine Feinunze Gold (= 31,1034768 Gramm) beispielsweise mit "XAU" abgekürzt (zusammengesetzt aus X und dem chemischen Symbol für Gold: "Au"), Silber entsprechend mit "XAG". Transaktionen, in denen keine Währung verwendet wird, werden mit "XXX" gekennzeichnet.

Neben der Buchstabenkodierung werden auch dreistellige Zifferncodes verwendet. Dabei bedeuten die Zahlenbereiche

Teilweise wird bei Änderung der Währung und des Buchstaben-Codes die bisherige numerische Kodierung beibehalten, insbesondere wenn sich lediglich der Name des Zahlungsmittels geändert hat.
Auch wenn eine direkte Währungsumstellung erfolgt, bleibt meist der numerische Code unverändert.
Diese Codes sind deshalb ohne Kenntnis des Zeitpunkts nicht immer so eindeutig einer bestimmten Währung des betreffenden Landes zuzuordnen, wie das mit den Buchstaben-Codes möglich ist. Allerdings sind sie für konkrete finanzielle Transaktionen vorgesehen, so dass die Angabe bei mehreren Möglichkeiten eigentlich nur bedeutet: „In der am Tag der Wertstellung gültigen Landeswährung“.

Der Standard definiert drei Listen, die selbst im Standardtext nicht enthalten sind, sondern auf der ISO-Website in ihrer jeweils aktuellen Fassung zur Verfügung gestellt werden:



</doc>
<doc id="76" url="https://de.wikipedia.org/wiki?curid=76" title="Achsensprung (Film)">
Achsensprung (Film)

Ein Achsensprung ist ein Filmschnitt, mit dem die Beziehungsachse der Figuren oder Gruppen übersprungen wird. Blickachsen
oder Beziehungsachsen zwischen den Akteuren untereinander oder dem Point of Interest des Protagonisten bilden eine gedachte Linie. Auf die Leinwand projiziert, stellt diese Linie eine „links-rechts-“ und „oben-unten-Beziehung“ zwischen den Akteuren dar. Mit Achsensprung bezeichnet man einen Schnitt, bei dem sich dieses Verhältnis umkehrt. Es wird zwischen Seitenachsensprung und dem Höhenachsensprung unterschieden. Letzterer wird als weniger desorientierend vom Zuschauer empfunden, da die Leinwand weniger hoch als breit ist. 
Ein Achsensprung kann beim Zuschauer Desorientierung verursachen, da die Anordnung und Blickrichtung der Akteure im Frame sich relativ zum Zuschauer zu verändern scheint.

Aktionsachse (Handlungsachse)
ist die gedachte Linie, in deren Richtung sich die Handlung oder das Inertialsystem der Filmwelt bewegt. Bei einer Autofahrt zum Beispiel ist die Aktionsachse so stark, dass die Beziehungsachsen an Bedeutung verlieren. Die Orientierung bleibt trotz eventuellem Achsensprung bewahrt. Wenn man aus der Fahrerseite filmt, bewegt sich die Landschaft scheinbar von rechts nach links; filmt man aus der Beifahrerseite, bewegt sie sich scheinbar von links nach rechts. Diese Änderung der Bewegungsrichtung ist aber nicht irritierend. Analog werden zwei Autos, die bei einer Parallelmontage in die gleiche Richtung fahren (oft von links nach rechts, weil das unserer Leserichtung entspricht), als einander verfolgend wahrgenommen; wenn eines jedoch von links nach rechts und das andere von rechts nach links fährt, erwartet der Zuschauer einen Zusammenstoß.

Im Continuity Editing des klassischen Hollywoodkinos wird der Achsensprung als Fehler betrachtet und dementsprechend vermieden. 

Der Grundsatz, Achsensprünge zu vermeiden, wird "180-Grad-Regel" genannt.

In manchen Fällen kann ein bewusster Achsensprung auch Stilmittel sein, um beispielsweise Verwirrung oder einen Kippmoment zu symbolisieren; Stanley Kubrick wird in diesem Zusammenhang häufig genannt. In Werbespots werden Achsensprünge oft verwendet, um einen rasanten Effekt zu bewirken. Bekannt ist auch eine Szene aus "Herr der Ringe", in welcher Sméagol mit sich selbst spricht. Da er mit den Schnitten wechselnd von der einen zur anderen Seite spricht (Achsensprung), entsteht der Eindruck zweier gleich aussehender Personen, womit der gespaltene Charakter der Figur unterstrichen wird.

Im Gegensatz zum Achsensprung handelt es sich hierbei um eine Bewegung der Kamera (Steadicam oder einer Dollyfahrt) über die Achse oder um eine Änderung der Bewegungsachse bzw. der Blickrichtung der Figuren, wodurch eine neue Achse definiert wird. Der Achsenwechsel wird vom Zuschauer nicht als störend wahrgenommen, weil sich die Bewegung fließend vollzieht. Diese Bewegung wird mitunter auch als Crab bezeichnet. Außerdem kann ein Zwischenschnitt in eine Totale eine Achsenüberschreitung möglich machen, da so die räumliche Anordnung der Akteure für den Zuschauer deutlich wird, oder der Zwischenschnitt auf einen Closeup, da sich der Betrachter danach wieder neu räumlich orientiert.




</doc>
<doc id="77" url="https://de.wikipedia.org/wiki?curid=77" title="Alfred Hitchcock">
Alfred Hitchcock

Sir Alfred Joseph Hitchcock KBE (* 13. August 1899 in Leytonstone, England; † 29. April 1980 in Los Angeles, Kalifornien) war ein britischer Filmregisseur, Drehbuchautor, Filmproduzent und Filmeditor. Er siedelte 1939 in die USA über und nahm am 20. April 1955 zusätzlich die amerikanische Staatsbürgerschaft an.

Hitchcock gilt hinsichtlich seines Stils bis heute als einer der einflussreichsten Filmregisseure. Er etablierte die Begriffe Suspense und MacGuffin. Sein Genre war der Thriller, charakteristisch seine Verbindung von Spannung mit Humor. Die wiederkehrenden Motive seiner Filme waren Angst, Schuld und Identitätsverlust. Mehrfach variierte er das Thema des unschuldig Verfolgten.

Hitchcock legte großen Wert auf die künstlerische Kontrolle über das Werk des Autors. Sein Gesamtwerk umfasst 53 Spielfilme und gehört gemessen am Publikumserfolg sowie der Rezeption durch Kritik und Wissenschaft zu den bedeutendsten der Filmgeschichte. Auch dank seiner bewussten Selbstvermarktung zählt Hitchcock heute zu den bekanntesten zeitgeschichtlichen Persönlichkeiten. Er ist dem Autorenfilm zuzurechnen.

Am 3. Januar 1980 wurde er von Königin Elisabeth II. zum Knight Commander des Order of the British Empire ernannt.

Alfred Hitchcock wurde am 13. August 1899 als jüngster Sohn des Gemüsehändlers William Hitchcock (1862–1914) und dessen Ehefrau Emma Jane Whelan (1863–1942) in Leytonstone bei London geboren. Durch den Altersunterschied von sieben beziehungsweise neun Jahren zu seinen Geschwistern, durch seine römisch-katholische Erziehung in einem von der anglikanischen Kirche geprägten Land und nicht zuletzt durch sein Äußeres – er war klein und schon als Kind korpulent – hatte er eine einsame Kindheit.
Zwischen 1910 und 1913 war er Schüler des St.-Ignatius-College, einer Londoner Jesuiten-Schule. Er verließ das College mit knapp 14 Jahren und besuchte stattdessen Abendkurse auf der Londoner Universität, diverse Handwerkskurse und später wenige Monate lang die "School of Engineering and Navigation". Zudem belegte er Kurse in Technischem Zeichnen sowie in Kunstgeschichte an der Londoner Kunstakademie. Seine Freizeit verbrachte er großteils mit dem Lesen von Fahrplänen und dem Studium von Stadtplänen und Landkarten. Mit fortschreitendem Alter flüchtete er sich in Romane, besuchte Theatervorstellungen und ging oft ins Kino. Außerdem verfolgte er Mordprozesse im Gerichtshof Old Bailey und besuchte gerne das Black Museum von Scotland Yard. Der Tod des Vaters Ende 1914, zu dem er kein enges Verhältnis hatte, band Hitchcock noch enger an seine Mutter.

1915 nahm er eine Stelle als technischer Angestellter bei der "W.T. Henley Telegraph Company" an, die elektrische Leitungen herstellte. Wegen seines zeichnerischen Talents wurde er bald in die Werbeabteilung versetzt. Unter seinem bis zuletzt gebrauchten Spitznamen „Hitch“ veröffentlichte er in der Betriebszeitschrift seine ersten gruseligen Kurzgeschichten.

Im Frühjahr 1920 hörte Hitchcock von der Neugründung eines Studios der amerikanischen Produktionsgesellschaft Paramount Famous Players-Lasky im Londoner Stadtbezirk Islington. Er bewarb sich mit einer Mappe mit Illustrationen und wurde als Zeichner von Zwischentiteln angestellt. In den Jahren 1921 und 1922 zeichnete er die Titel für mindestens zwölf Filme. Nebenbei entwarf er Kostüme, Dekorationen und Szenenbilder. Auch durch Überarbeitungen von Drehbüchern machte er auf sich aufmerksam. Bei zwei Filmen arbeitete er mit George Fitzmaurice zusammen, dessen genaue Produktionsplanung ihn sehr beeinflusste.

1922 bekam Hitchcock Gelegenheit, sich als Regisseur zu versuchen. Mit dem Autor Seymour Hicks stellte er die letzten Filmszenen von "Always Tell Your Wife" fertig, nachdem der ursprüngliche Regisseur gefeuert worden war. Bald darauf konnte er einen eigenen Film drehen, "Number 13" (in einigen Quellen "Mrs. Peabody)", der jedoch unvollendet blieb, da "Famous Players-Lasky" im Laufe der Dreharbeiten das Studio wegen finanzieller Schwierigkeiten schließen musste. Das leerstehende Gelände wurde an unabhängige Produzenten vermietet, darunter auch an Michael Balcon, der das Studio 1924 schließlich erwarb. Er stellte Hitchcock als Regieassistent ein, sowie (auf dessen Empfehlung) die Filmeditorin Alma Reville. Die beiden kannten sich seit 1921, seitdem sie gelegentlich an denselben Filmen gearbeitet hatten. Bis 1925 entstanden fünf Filme, bei denen Hitchcock dem Regisseur Graham Cutts assistierte und zu Cutts’ wachsendem Unmut mehr und mehr künstlerischen Einfluss gewann. Neben dem Drehbuch kümmerte er sich auch um die Bauten, das Szenenbild, die Besetzung, die Kostüme sowie die Ausstattung und nahm so mit der Zeit die Aufgaben eines Produktionsleiters wahr.

Hitchcocks letzte Zusammenarbeit mit Graham Cutts führte ihn 1924/25 nach Deutschland. Der unter der Beteiligung der deutschen UFA produzierte Film "Die Prinzessin und der Geiger" entstand in den Babelsberger Filmstudios – damals den modernsten der Welt. Dabei hatte Hitchcock die Möglichkeit, Friedrich Wilhelm Murnau bei den Arbeiten an "Der letzte Mann" zu beobachten; von diesem beeindruckt übernahm er einige Techniken Murnaus für die Szenenbilder seiner aktuellen Produktion. Durch diesen und weitere Besuche konnte Hitchcock fließend Deutsch sprechen; später sprach er zum Beispiel einige Trailer seiner Filme selbst.

Zurück in England übertrug ihm Michael Balcon 1925 die Regie für einen eigenen Film. Das Projekt führte den jungen Hitchcock wieder nach Deutschland. Nur die Münchner Lichtspielkunst (Emelka) fand sich bereit, den Film des unbekannten Regie-Debütanten mitzuproduzieren. Für das Melodram "Irrgarten der Leidenschaft" (1925) verpflichtete Balcon kostspielige Stars aus Hollywood. Alma Reville, mittlerweile Hitchcocks Verlobte, war als Regieassistentin und Editorin Mitglied des sehr kleinen Filmteams. Balcon war mit Hitchcocks ambitionierter Arbeit zufrieden und vertraute ihm eine weitere deutsch-englische Koproduktion an: "Der Bergadler" wurde noch im selben Jahr, diesmal in Tirol, gedreht. Doch beide Filme, die 1925 beziehungsweise 1926 in Deutschland in den Kinos anliefen, wurden in England zunächst nicht veröffentlicht. Der englische Verleiher und Geldgeber C. M. Woolf war, im Gegensatz zu Balcon, nicht von Hitchcocks betont expressionistischem Stil überzeugt. "Der Bergadler" ist der einzige von Hitchcocks Filmen, der nicht mehr erhalten ist.

Mit dem 1926 gedrehten Film "Der Mieter" um einen einzelgängerischen Pensionsgast, der verdächtigt wird, ein Serienmörder zu sein, hatte Hitchcock sein Thema gefunden. Doch nicht zuletzt wegen dessen expressionistischer Bildgestaltung lehnte es Woolf abermals ab, den Film zu veröffentlichen. Balcon zog daraufhin den jungen Ivor Montagu hinzu, der Erfahrung mit Filmüberarbeitungen hatte; mit Hitchcock zusammen wurden einige Änderungen vorgenommen. Der überragende Erfolg bei einer Pressevorführung ebnete dann den Weg zur Veröffentlichung seiner ersten beiden Filme. "Der Mieter" kam 1927 in kurzer Abfolge mit "Irrgarten der Leidenschaft" und "Der Bergadler" in die Kinos und bedeutete für Hitchcock den Durchbruch als Regisseur.

Für Balcons Gainsborough Pictures drehte Hitchcock 1927 noch die zwei Melodramen "Abwärts" und "Easy Virtue". Beiden Filmen war kein Erfolg beschieden. Bereits zuvor hatte er beschlossen, bei deutlich höherem Gehalt zu der neu gegründeten Firma British International Pictures (BIP) des Produzenten John Maxwell zu wechseln. Dort entstand mit dem Boxerdrama "Der Weltmeister" sein erster Film nach einem Originaldrehbuch. Die Presse reagierte äußerst positiv.
Obwohl die drei folgenden Stummfilme "The Farmer’s Wife," "Champagne" und "Der Mann von der Insel Man" abgesehen von einzelnen Szenen als Fingerübungen gelten, hatte sich Hitchcock in Großbritannien innerhalb kurzer Zeit einen Namen gemacht: Die junge britische Filmindustrie, sehr darauf bedacht, sich von der amerikanischen abzuheben, war nur allzu gerne bereit, ihn als kommenden Regiestar zu feiern.

Im Dezember 1926 heirateten Alfred Hitchcock und Alma Reville, die für die Hochzeit zum katholischen Glauben konvertierte. 1928 wurde ihre gemeinsame Tochter Patricia geboren. Beruflich blieb Alma bis zum Schluss seine engste Mitarbeiterin und Beraterin.

Das Aufkommen des Tonfilms hielten viele Regisseure für das Ende ihrer Kunstform. Hitchcock hingegen nutzte das Potential der neuen Technik. "Erpressung" (1929) wurde ursprünglich als Stummfilm produziert. Die Produzenten erlaubten Hitchcock jedoch, eine Filmrolle mit Tonmaterial nachzudrehen. Er versah daraufhin einzelne Schlüsselszenen mit wirkungsvollen Toneffekten und gesprochenem Dialog, wobei die tschechische Schauspielerin Anny Ondra, die ihre Rolle stumm spielen musste, von der englischen Schauspielerin Joan Barry simultan synchronisiert wurde. "Erpressung" war der erste britische Tonfilm und wurde ein großer Erfolg. Hitchcock nutzte seine gewonnene Popularität und gründete mit der "Hitchcock Baker Productions Ltd." eine Gesellschaft zur Vermarktung seiner Person.

Auf Geheiß seines Studios drehte er "Juno and the Paycock" (1930) sowie einige Szenen für die Musikrevue "Elstree Calling". Mit "Mord – Sir John greift ein!" fand er wieder zu seinem Thema und auch nach Deutschland zurück: In Berlin stellte er die deutsche Sprachversion des Films unter dem Titel "Mary" her. Es folgten drei Filme, von denen Hitchcock nur die Komödie "Endlich sind wir reich" wirklich interessierte: In dem zusammen mit seiner Frau und Val Valentine verfassten Drehbuch verarbeitete er unter anderem die Erfahrungen seiner noch jungen Ehe. Den ihm aufgezwungenen Thriller "Nummer siebzehn" beschloss Hitchcock aus Protest zu sabotieren und zu einer wirren, albernen Parodie zu machen. Die turbulente Verbindung zwischen Humor und Spannung lässt "Nummer siebzehn" aus heutiger Sicht als einen Vorläufer späterer Klassiker des Genres erscheinen. Hitchcocks Vertrag mit der "British International Pictures" endete nach sechs Jahren mit einem Einsatz als Produzent "(Lord Camber’s Ladies)". Die Zusammenarbeit hatte zunehmend unter dem Konflikt zwischen Hitchcocks Streben nach künstlerischer Kontrolle und den Vorschriften des Studios gelitten. Doch auch den folgenden Film "Waltzes from Vienna", ein „Musical ohne Musik“ (Hitchcock) für den unabhängigen Produzenten Tom Arnold, drehte er betont lustlos: „Ich hasse dieses Zeug. Melodrama ist das einzige, was ich wirklich kann.“

Unmittelbar nach "Waltzes from Vienna" nahm er die fruchtbare Zusammenarbeit mit dem Produzenten Michael Balcon wieder auf. Als erster Film für die "Gaumont British" entstand der Thriller "Der Mann, der zuviel wußte" (1934). Das Drehbuch erarbeitete Hitchcock im Wesentlichen mit seiner Frau Alma und dem Drehbuchautor Charles Bennett. Der Film wurde sowohl von der Kritik als auch vom Publikum enthusiastisch aufgenommen. Der humorvolle Spionagethriller "Die 39 Stufen" (1935, Drehbuch: Charles Bennett) gilt als Blaupause späterer Verfolgungsthriller. Eine turbulente Szene folgt auf die nächste, es gibt keine Übergänge und kaum Zeit für den Zuschauer, über die manches Mal fehlende Logik nachzudenken. Hitchcock ordnete nach eigenem Bekunden alles dem Tempo unter. Der überragende Erfolg des Films sollte ihm recht geben. Es folgten "Geheimagent" (1936) und "Sabotage" (1936), die insbesondere in Hitchcocks eigener späterer Bewertung gegenüber den beiden Vorgängerfilmen abfielen. Doch die psychologisch vielschichtige Behandlung des Themas „Schuld“ weist bereits auf spätere Werke hin.

Nach "Sabotage" endete abrupt die zweite erfolgreiche Phase der Zusammenarbeit mit Michael Balcon, als die Produktionsfirma Gaumont British von deren Besitzern geschlossen und Balcon entlassen wurde. Die beiden folgenden Filme drehte Hitchcock daher wieder für die Gainsborough Pictures – diesmal allerdings ohne seinen ehemaligen Förderer. "Jung und unschuldig" (1937) war eine weitere, unbeschwerte Variation der Geschichte vom unschuldig Verfolgten. Der gefeierte Thriller "Eine Dame verschwindet" (1938) spielt überwiegend in einem fahrenden Zug. Die Dreharbeiten fanden jedoch ausschließlich in einem kleinen Londoner Studio statt, was dank technisch anspruchsvoller Rückprojektionen möglich wurde.

Hitchcock festigte mit diesen sechs Filmen seine Ausnahmestellung innerhalb des britischen Kinos. Ende der 1930er Jahre beauftragte er die "Selznick-Joyce-Agentur", deren Mitinhaber Myron Selznick, der ältere Bruder des Hollywood-Moguls David O. Selznick, war, seine Interessen wahrzunehmen. Hitchcock, dessen Ruf mittlerweile bis nach Hollywood gelangt war, unterzeichnete schließlich 1938 einen Vertrag für die Produktionsgesellschaft von David O. Selznick, der damals gerade mit der Vorproduktion zu "Vom Winde verweht" beschäftigt war. In Gedanken bereits in Hollywood, drehte Hitchcock in England noch einen letzten Film für die Produktionsfirma des nach England emigrierten deutschen Produzenten Erich Pommer. Doch der Kostümfilm "Riff-Piraten" wurde von der Presse durchweg verrissen.

In seinen ersten Jahren in Hollywood stieß Hitchcock auf unerwartete Schwierigkeiten. David O. Selznick übte starke Kontrolle über die Filme seines Studios aus und achtete darauf, dass sich der freiheitsliebende Hitchcock möglichst eng an die literarische Vorlage seines ersten Hollywoodfilmes hielt. Trotz dieser Spannungen wurde "Rebecca" für den britischen Regisseur ein erfolgreicher Einstand in Hollywood: Das psychologisch dichte und düster-romantische Melodram war 1940 elfmal für den Oscar nominiert und gewann schließlich zwei der Trophäen (Kamera und Produktion).

In den nächsten Jahren machte Selznick sein Geld mit Hitchcock, indem er ihn für beträchtliche Summen an andere Studios auslieh. Der Krieg in Europa weitete sich aus, als der unabhängige Produzent Walter Wanger Hitchcock für ein aktuelles Kriegsdrama engagierte. "Der Auslandskorrespondent" blieb Hitchcocks Naturell entsprechend jedoch ein weitgehend unpolitischer Spionagethriller. Nur der nachgedrehte Schlussmonolog, gerichtet an die noch neutralen USA, wirkte aufrüttelnd. Kurz nach Fertigstellung des Films wurde England von Deutschland bombardiert. Der rechtzeitig ausgewanderte Hitchcock musste sich daraufhin scharfe Kritik von ehemaligen britischen Kollegen, allen voran Michael Balcon, gefallen lassen.

Mit "Verdacht" (1941, RKO), der ersten Zusammenarbeit mit Cary Grant, und "Saboteure" (1942, Universal) blieb Hitchcock bei seinen klassischen Themen. Zwischen diesen Produktionen drehte er, für ihn und andere ungewohnt, seine einzige Screwball-Komödie. Obwohl damals durchaus positiv aufgenommen, zeigte er sich mit "Mr. und Mrs. Smith" (1941, RKO) nicht zufrieden.
Weit mehr am Herzen lag ihm die Arbeit an dem Drama "Im Schatten des Zweifels" (1943, Universal). Hitchcocks Filme gelten allgemein als stark von seinem Charakter geprägt. Dieses Familienmelodram wird als einer seiner persönlichsten Filme bezeichnet: In allen Hauptfiguren spiegeln sich demnach Eigenschaften und Ängste Hitchcocks. Als während der Dreharbeiten Hitchcocks Mutter in London starb, verstärkte dies die autobiografischen Tendenzen.

Wie viele britische Regisseure leistete Hitchcock seine Beiträge für die Kriegspropaganda und drehte unter anderem Kurzfilme zur Unterstützung der französischen Résistance. Auch in seine nächste Hollywood-Produktion arbeitete er stark propagandistische Töne ein, doch sein stets bewusst irritierender Umgang mit Klischees sorgte diesmal für Kontroversen: In einem kleinen Rettungsboot sehen sich englische und amerikanische Schiffbrüchige einem intellektuell überlegenen Nazi gegenüber. Dennoch wurde der formalistisch strenge Psychothriller "Das Rettungsboot" (1943, 20th Century Fox) dreimal für den Oscar nominiert (Drehbuch, Kamera und Regie).

Psychologie, wichtige Komponente seines Werks, stand im Mittelpunkt von "Ich kämpfe um dich" (1945), der nach langer Zeit wieder für Selznick entstand. Dieser war vom Thema Psychoanalyse schnell begeistert und ließ Hitchcock ungewohnt viel freie Hand, doch kürzte er den Film nach der ersten Probevorführung um rund zwanzig Minuten. Die erfolgreiche Zusammenarbeit mit Ingrid Bergman in der Hauptrolle wurde in der folgenden Produktion "Berüchtigt" (1946) fortgesetzt, die Selznick allerdings wieder an RKO verkaufte. Die Geschichte um eine Spionin (Bergman), die aus Pflichtgefühl von ihrem Liebhaber (Cary Grant) gedrängt wird, mit dem Feind zu schlafen, bot für Hitchcocks Obsessionen eine breite Projektionsfläche.

Mit dem Gerichtsdrama "Der Fall Paradin" (1947) lief der Vertrag Hitchcocks mit Selznick aus. Selznick behielt, bei der Stoffauswahl angefangen, bei dieser relativ chaotisch verlaufenden Produktion die Oberhand. Dass Hitchcock währenddessen Vorbereitungen für seine eigene Produktionsfirma traf, verstärkte die Spannungen zwischen den machtbewussten Männern. Dennoch bot Selznick Hitchcock – erfolglos – eine Vertragsverlängerung an.

Bereits im April 1946, rund zwei Jahre bevor der Vertrag mit Selznick auslaufen sollte, gründete Hitchcock mit dem befreundeten Kinokettenbesitzer Sidney Bernstein die Produktionsfirma Transatlantic Pictures, für die er seinen ersten Farbfilm inszenierte, "Cocktail für eine Leiche" (1948) mit James Stewart in einer der Hauptrollen. Der Film blieb jedoch vor allem wegen eines anderen Hitchcock-Experiments in Erinnerung; jede Einstellung des kammerspielartigen Films dauert so lange, wie es das Filmmaterial in der Kamera erlaubte, also rund zehn Minuten. Durch geschickte Übergänge sollte so der Eindruck entstehen, dass sich die Geschichte in Echtzeit und von nur einer Kamera gefilmt ereignete.

"Sklavin des Herzens" (1949), ein für Hitchcock untypischer, melodramatischer Kostümfilm, war vor allem ein Vehikel für Ingrid Bergman. Trotz der Starbesetzung und der technischen Raffinessen wurde er kommerziell ein ähnlicher Misserfolg wie "Cocktail für eine Leiche" - Transatlantic ging daraufhin in Konkurs.

Nachdem sein Berater und Agent Myron Selznick 1944 gestorben war, wurden Hitchcocks Interessen von mehreren anderen Personen wahrgenommen, bevor er 1948 mit Lew Wasserman zusammentraf. Wasserman war seit 1946 Präsident der weltgrößten Künstleragentur Music Corporation of America (MCA), der sich Hitchcock 1948 anschloss. Es begann eine enge wie äußerst lohnende Zusammenarbeit.

Hitchcock schloss mit Warner Bros. einen lukrativen Vertrag über vier Filme ab, bei denen er als Regisseur und Produzent, angefangen bei der Stoffauswahl, völlig freie Hand hatte. Der erste dieser Filme war der Thriller "Die rote Lola" (1950) mit Marlene Dietrich, der im Londoner Theatermilieu spielte. Eines seiner Lieblingsmotive stellte er auf den Kopf; am Ende entpuppt sich der „unschuldig Verfolgte“ als der wahre Mörder. Hitchcock drehte in seiner Heimat, spürte allerdings wieder die alten Ressentiments, die nach seiner Auswanderung entstanden waren. Der Film selbst war nicht sonderlich erfolgreich.

Im April 1950 begann Hitchcock, regelmäßige Kolloquien an den Universitäten von Kalifornien und Südkalifornien abzuhalten, in denen unter anderem Previews seiner aktuellen Filme gezeigt wurden. Diese Tradition sollte er die kommenden 20 Jahre beibehalten.

"Der Fremde im Zug" (1951, nach einem Roman von Patricia Highsmith) brachte schließlich nach fünf Jahren Flaute wieder einen überragenden Erfolg. Mit diesem Film begann die dreizehnjährige Zusammenarbeit mit dem Kameramann Robert Burks. Wie schon in "Die rote Lola" spielte Hitchcocks Tochter Patricia eine Nebenrolle. 1952 folgte mit "Ich beichte" der eindeutigste filmische Bezug auf Hitchcocks starke katholische Prägung. Obwohl von der Kritik geschätzt, floppte der Film an den Kinokassen, was Hitchcock vor allem der Humorlosigkeit des Publikums anlastete.

Als Anfang der 1950er Jahre das Fernsehen Einzug in die Wohnzimmer hielt, versuchte die Kinoindustrie, mit neuen technischen Verfahren wie dem Breitbildformat Cinemascope oder dem 3D-Verfahren den Zuschauerschwund aufzuhalten. So drängte Warner Bros. Hitchcock, seinen nächsten Film in 3D zu drehen. Über diese Entscheidung, die zur Einschränkung der Bewegungsfreiheit der Kamera führte, war Hitchcock nicht glücklich; er setzte auch nur wenige explizite 3-D-Effekte ein. "Bei Anruf Mord" (1954) ist die Verfilmung eines damals sehr populären Theaterstücks von Frederick Knott, der auch das Drehbuch schrieb. Mit Hauptdarstellerin Grace Kelly drehte Hitchcock im Anschluss noch zwei weitere Filme, ehe sie sich aus dem Filmgeschäft zurückzog.

Die Erfahrung mit dem aufgezwungenen 3D-Verfahren zeigte Hitchcock die Grenzen bei Warner Brothers. Er schloss daher 1953 einen Vertrag mit Paramount ab, der ihm völlige künstlerische Freiheit garantierte. 1954 begann die für Hitchcock erfolgreichste Zeit mit "Das Fenster zum Hof". Neben Grace Kelly ist ein weiteres Mal James Stewart zu sehen. Die Hauptfigur sitzt während des gesamten Films im Rollstuhl und beobachtet durch ein Teleobjektiv das Geschehen in den gegenüberliegenden Wohnungen – sozusagen stellvertretend für den Zuschauer, aber auch stellvertretend für Hitchcock selbst, der in diesem Film den voyeuristischen Aspekt des Filmemachens aufzeigt.
"Über den Dächern von Nizza" (1955) ist ein leichter, romantischer Thriller, in dem neben Grace Kelly – nach zwei Jahren Filmpause – wieder Cary Grant spielte. Wohl um dem Glamour dieses an der Côte d’Azur angesiedelten Films etwas entgegenzusetzen, drehte Hitchcock noch im selben Jahr die kostengünstig produzierte schwarze Komödie "Immer Ärger mit Harry," in der Shirley MacLaine neben John Forsythe ihren ersten Filmauftritt hatte. Edmund Gwenn, der bereits in früheren Hitchcock-Filmen mitgewirkt hatte, spielte fast achtzigjährig eine seiner wenigen Hauptrollen. Obwohl Hitchcock in vielen seiner Filme schwarzen Humor untergebracht hat, ist es eine der wenigen echten Komödien von ihm.

1955 nahm Hitchcock – rund fünf Jahre nach seiner Frau – die amerikanische Staatsbürgerschaft an. Im selben Jahr begann er mit Doris Day und James Stewart die Dreharbeiten zu "Der Mann, der zuviel wußte" (1956), dem einzigen Remake eines seiner Filme in seiner Karriere. Ebenfalls 1955 startete die wöchentliche Fernsehserie "Alfred Hitchcock Presents" (ab 1962 "The Alfred Hitchcock Hour)". Hitchcock war Produzent, trat in vielen Folgen als Moderator auf und inszenierte insgesamt 18 Folgen. Auch für die Fernsehserien "Suspicion" und "Startime" nahm er für je eine Folge im Regiestuhl Platz. Nach zehn Jahren beendete er seine Fernseharbeit, an der er zunehmend das Interesse verloren hatte. Hinzu kam, dass die Produktion den Auftraggebern zu teuer wurde und die Zeit von Serien mit jeweils abgeschlossenen Folgen, sogenannten „Anthologies“, zu Ende ging.

Mit "Der falsche Mann" wurde er 1956 einem seiner Grundprinzipien, der strikten Trennung von Leben und Fiktion, untreu. In dem Schwarzweißfilm mit Henry Fonda und Vera Miles wird an authentischen Schauplätzen die auf Tatsachen beruhende Geschichte eines zu unrecht Verurteilten erzählt. Der Film entstand noch einmal für Warner Bros., da Hitchcock dem Studio bei seinem Ausscheiden noch einen Film ohne Regiegage zugesagt hatte. Allerdings war "Der falsche Mann", der viele Stilelemente des Film noir und ein trostloses Ende aufweist, kommerziell ein Flop.

1957 drehte Hitchcock seinen letzten Film für Paramount: "Vertigo – Aus dem Reich der Toten" (1958 veröffentlicht). Das Drehbuch entstand in gemeinsamer intensiver Arbeit von Hitchcock und Samuel A. Taylor. In wenige seiner Filmfiguren projizierte Hitchcock wohl so viel von seiner eigenen Persönlichkeit wie in den von James Stewart verkörperten Scottie Ferguson, der versucht, eine Frau nach seinen Vorstellungen umzuformen. Zu seiner Entstehungszeit nicht besonders erfolgreich, zählt der Film inzwischen – ähnlich wie der folgende "Der unsichtbare Dritte" – zu den bedeutendsten Werken Hitchcocks. Hitchcock und sein Drehbuchautor Ernest Lehman konzipierten "Der unsichtbare Dritte" (1959, MGM) als eine Abfolge von Abenteuern, in denen ein Unschuldiger (Cary Grant in seinem letzten Hitchcock-Film) um seine Reputation und sein Leben kämpft. Die elegante Leichtigkeit der Erzählung beeinflusste viele nachfolgende Abenteuer- und Agentenfilme, was sich u. a. in den James-Bond-Filmen der darauf folgenden Jahre zeigt. Für Hitchcock selbst blieb es für lange Zeit der letzte vorwiegend heitere Film.

Das im Anschluss vorbereitete Projekt mit Audrey Hepburn in der Hauptrolle wurde durch Hitchcock gestoppt, als Hepburn wegen einer geplanten Vergewaltigungsszene absagte. Mit seiner bewusst kostengünstigen Produktion "Psycho" (1960) folgte Hitchcocks wohl bekanntester Film: Die in einer Woche Dreharbeit entstandene „Duschszene“ zählt heute zu seinen meistanalysierten Filmszenen. Ungewöhnlich war auch der Tod einer Hauptfigur nach nur einem Drittel des Films. Die zeitgenössischen Kritiken fielen unerwartet barsch aus, doch das Publikum machte "Psycho" zu Hitchcocks größtem kommerziellen Erfolg.

Nachdem zwei angedachte Projekte scheiterten – unter anderem, weil Walt Disney dem "Psycho"-Regisseur die Dreherlaubnis in Disneyland verweigerte – nahm Hitchcock seinen nächsten Film erst Mitte 1961 in Angriff: "Die Vögel" (1963), ein weiterer Horrorfilm, der nicht zuletzt durch seine Dramaturgie und die eingesetzte Tricktechnik – etwa den Sodium Vapor Process – stilbildend wirkte. Der deutsche Komponist Oskar Sala setzte statt Filmmusik elektronisch erzeugte Geräusche ein. Seine Hauptdarstellerin Tippi Hedren hatte Hitchcock im Werbefernsehen entdeckt. Obwohl sie keine Filmerfahrung besaß, nahm er sie für die nächsten sieben Jahre unter Vertrag.

"Die Vögel" entstand für Universal, die kurz zuvor teilweise von MCA übernommen worden waren und für die Hitchcock von nun an alle seine Filme drehen sollte. Lew Wasserman, bis zu diesem Zeitpunkt Agent Hitchcocks, wurde Präsident von Universal und gab seine Agententätigkeit auf. Hitchcock selbst trat seine Rechte an "Psycho" und seiner Fernsehserie ab und wurde im Gegenzug der drittgrößte Aktionär von MCA.

Nach "Die Vögel" gibt es in Hitchcocks Werk einen Bruch. Die folgenden drei Filme der 1960er Jahre blieben künstlerisch und kommerziell hinter den vorangegangenen Erfolgen zurück. Konflikte mit seiner Hauptdarstellerin Tippi Hedren prägten die Dreharbeiten so weit, dass er das Gelingen seines nächsten Films "Marnie" (1964) bewusst zu sabotieren schien. Das Psychogramm einer verstörten, traumatisierten Frau bedient sich psychologischer Erklärungsmodelle, die überholt und undifferenziert wirken, und enthält für Hitchcock untypisch viele handwerkliche Fehler. Dieser erste künstlerische und kommerzielle Misserfolg seit rund fünfzehn Jahren war in mehrfacher Hinsicht ein Wendepunkt in Hitchcocks Karriere. Tippi Hedren war die letzte typische „Hitchcock-Blondine“, "Marnie" der letzte Film, den Hitchcocks langjähriger Kameramann Robert Burks drehte. Kurz nach Abschluss der Dreharbeiten verstarb zudem Hitchcocks Filmeditor George Tomasini, mit dem er zehn Jahre lang zusammengearbeitet hatte.

Filmproduktionen wurden immer aufwändiger, der Erfolg an der Kinokasse immer wichtiger. Diverse Projekte, die Hitchcock reizten und die er mehr oder weniger intensiv plante, kamen so aus Angst der Produzenten nicht zustande – etwa "Mary Rose", die geplante Verfilmung eines skurrilen Theaterstücks. Mit "R.R.R.R.", einem Drehbuch mit zahlreichen Verwicklungen über eine italienische Ganoven-Familie in New York, wollte er Jahre nach "Der unsichtbare Dritte" wieder einen komischen Thriller drehen und damit alle Nachahmer "(Charade", "Topkapi" und andere) übertreffen. Das weit fortgeschrittene Projekt scheiterte schließlich an unüberbrückbaren sprachlichen und kulturellen Problemen mit den italienischen Mitarbeitern.

Am 7. März 1965 erhielt Hitchcock für seinen „historischen Beitrag zum amerikanischen Kino“ den "Milestone Award" der "Producers Guild Of America" – die erste von vielen Ehrungen für sein Lebenswerk.

Mit "Der zerrissene Vorhang" (1966) kehrte Hitchcock schließlich zum Genre des Spionagefilms zurück, in dem er bereits in den 1930er Jahren in England große Erfolge gefeiert hatte. Die Premiere dieses 50. Hitchcock-Filmes sollte von einer groß angelegten Marketingkampagne begleitet werden. Nicht nur aus diesem Grund setzte Universal die aktuellen Stars Paul Newman und Julie Andrews gegen Hitchcocks Widerstand als Hauptdarsteller durch. Überdies kam es zum Bruch mit dem Komponisten Bernard Herrmann, als dieser nicht die von Universal gewünschte, auch für den Schallplattenverkauf geeignete Unterhaltungsmusik vorlegte. Auch an anderen wichtigen Positionen seines Stabes musste Hitchcock auf vertraute Mitarbeiter verzichten. "Der zerrissene Vorhang" fällt handwerklich und dramaturgisch gegenüber Hitchcocks letzten Filmen (einschließlich "Marnie") deutlich ab und wurde von der Kritik durchweg verrissen.

Universal forderte von ihm zeitgemäßere Themen ein. Als das von ihm und Howard Fast detailliert ausgearbeitete Drehbuch über einen homosexuellen Frauenmörder abgelehnt wurde, zog er sich für ein Jahr ins Privatleben zurück. Anfang 1968 entschloss er sich unter dem Druck der langen Pause seit dem letzten Film und der noch längeren Zeitspanne seit dem letzten Erfolg, den Spionageroman "Topas" von Leon Uris zu verfilmen, dessen Rechte Universal kurz zuvor erworben hatte. "Topas" wurde dann fast ausschließlich mit europäischen Schauspielern besetzt und völlig ohne Hollywood-Stars. Die für amerikanische Zuschauer bekanntesten Gesichter waren der Fernsehschauspieler John Forsythe und der aus Kanada stammende John Vernon. Das endgültige Drehbuch wurde erst während der laufenden Dreharbeiten geschrieben, der Schluss nach einer katastrophalen Preview improvisiert. Publikum und Kritik reagierten mit Ablehnung auf Hitchcocks bis dahin teuersten Film, doch er zeigte sich zuversichtlich: „Ich habe meinen letzten Film noch nicht gedreht. "Topas" ist mein 51. Film, aber wann ich meinen letzten Film drehen werde, ist von mir, meinen Finanziers und Gott noch nicht entschieden worden.“

Im Spätsommer 1970 nahm Hitchcock sein nächstes Projekt in Angriff und reiste dafür wieder in seine Heimat, wo er diesmal begeistert empfangen wurde. "Frenzy" (1972) spielt in London, dem Hitchcock eine liebevolle Hommage erweist, und ist in seinen Worten „die Geschichte eines Mannes, der impotent ist, und sich deshalb durch Mord ausdrückt“. Zunächst verliefen die Drehbucharbeit und auch die Dreharbeiten, die Hitchcock so ernst nahm wie lange nicht mehr, weitgehend reibungsfrei. Doch als seine Frau Alma einen Herzinfarkt erlitten hatte, wurde Hitchcock müde und untätig; die Crew war, ähnlich wie bei den drei vorangegangenen Filmen, wieder weitgehend auf sich alleine gestellt. Dennoch wurde "Frenzy" ein brutaler, zum Teil bitterer, von tiefschwarzem britischen Humor durchzogener Film und ein großer Erfolg. Nur in England war man enttäuscht und bemängelte vor allem die anachronistisch wirkende Darstellung von London und des britischen Lebens.

Im Frühjahr 1973 entschloss sich Hitchcock, den Roman "The Rainbird Pattern" von Victor Canning zu verfilmen. Doch die Arbeit am Drehbuch mit Ernest Lehman "(Der unsichtbare Dritte)" ging diesmal nicht mehr so reibungslos vonstatten: Hitchcock war merklich müde geworden, seine körperlichen Schmerzen betäubte er zunehmend mit Alkohol. Zwei Jahre benötigte die Fertigstellung des Drehbuchs, so lange wie nie zuvor in seiner Karriere.

Mit "Familiengrab," wie der Film schließlich hieß, kehrte Hitchcock zum scheinbar heiteren, diesmal jedoch morbid akzentuierten Unterhaltungsthriller zurück. Wie stets legte er Wert auf eine ausgeklügelte Bildsprache, die erneut mit Hilfe von Storyboards erarbeitet wurde.
Die Dreharbeiten gestalteten sich reibungslos und in einer entspannten Atmosphäre. Hitchcock, der sich im Rahmen seiner gesundheitlichen Möglichkeiten mit einem lange nicht gezeigten Elan in die Dreharbeiten einbrachte, zeigte sich zu Neuerungen bereit: Er war offen für Improvisationen seiner Schauspieler und nahm noch während der Dreharbeiten Änderungen am Ablauf vor. Die Überwachung der Schnittarbeiten musste er weitgehend seinen Mitarbeiterinnen Peggy Robertson und Suzanne Gauthier überlassen, da sich sein Gesundheitszustand deutlich verschlechterte. Zudem erlitt Alma einen zweiten Schlaganfall.

"Familiengrab" wurde nach seiner Premiere im Frühjahr 1976 überwiegend freundlich aufgenommen, und Hitchcock schöpfte aus der Sympathie, die ihm entgegenschlug, für kurze Zeit Kraft, neue Filmideen aufzugreifen. Sein erst Anfang 1978 in Angriff genommenes Projekt, die Verfilmung des Romans "The Short Night" von Ronald Kirkbride, wurde aufgrund seines sich weiter verschlechternden Gesundheitszustands jedoch etwa ein Jahr später von Universal gestoppt. Im März 1979 wurde er vom American Film Institute für sein Lebenswerk geehrt. Zwei Monate später schloss er sein Büro auf dem Gelände der "Universal"-Studios. Am 3. Januar 1980 wurde Hitchcock in den britischen Adelsstand erhoben.

Am Morgen des 29. April 1980 starb Alfred Hitchcock in seinem Haus in Los Angeles an Nierenversagen. Sein Körper wurde eingeäschert, die Asche an einem unbekannten Ort verstreut.

In rund fünfzig Jahren hat Alfred Hitchcock dreiundfünfzig Spielfilme als Regisseur begonnen und beendet. Die weitaus größte Zahl dieser Filme gehört dem Genre des Thrillers an und weist ähnliche Erzählmuster und Motive auf, wiederkehrende Elemente, visuelle Stilmittel und Effekte, die sich wie ein roter Faden durch sein Gesamtwerk ziehen.

Das Grundmotiv in Hitchcocks Filmen bildet meist die Angst der Protagonisten vor der Vernichtung ihrer (bürgerlichen) Existenz. Dabei bezieht sich diese Angst nicht nur auf Mörder, Gangster oder Spione, welche die bürgerliche Ordnung angreifen; die Hauptfiguren finden sich häufig in der Lage wieder, sogar von Vertretern des Gesetzes bedroht zu werden.

Zu diesem Motiv der Angst kommt – Hitchcocks katholischer Prägung entsprechend – jenes von Schuld und Sühne hinzu. Der unschuldig Verfolgte in seinen Filmen ist „unschuldig, aber nur in Bezug auf das, was man ihm vorwirft.“ Das heißt, die Figur wird durch das, was ihr im Laufe des Filmes widerfährt, im übertragenen Sinne für andere Defizite oder Vergehen bestraft: In "Bei Anruf Mord" etwa wird die Hauptfigur des Mordes verdächtigt; tatsächlich musste sie aus Notwehr töten. Das folgende Unheil kann jedoch als Strafe für den von ihr begangenen Ehebruch angesehen werden.

Eine Variation dieses Themas ist die Übertragung der Schuld auf eine andere Person. Unschuldige werden zu Schuldigen (oder Mitschuldigen) an Verbrechen anderer, da sie aus persönlichen Gründen nicht zur Aufklärung beitragen können. Zentral sind hierbei die beiden Filme "Ich beichte" und "Der Fremde im Zug", in denen die jeweiligen Protagonisten von Morden, die andere begangen haben, profitieren und, nachdem sie selbst unter Verdacht geraten, keine Möglichkeit haben, sich zu entlasten. In "Vertigo" macht der wahre Mörder die Hauptfigur durch ein Komplott zunächst scheinbar zum Schuldigen am Tod der ihr anvertrauten Person. Später macht sich das Opfer der Intrige tatsächlich am Tod der Frau schuldig, die er liebt.

Falsche Verdächtigungen, aber auch ausgeprägte Schuldkomplexe, gehen bei Hitchcocks Filmen mit der Bedrohung der Identität einher. Seine traumatisierten oder verfolgten Figuren nehmen selbst falsche Namen an oder werden – aus unbekannten Gründen – für jemand anderen gehalten. Das Motiv des Identitätsverlusts spielt Hitchcock, angefangen von seinem ersten bis zu seinem letzten Film, in unterschiedlichsten Varianten durch, besonders einprägsam in "Vertigo": Die weibliche Hauptfigur wird zunächst im Rahmen eines Mordkomplotts in eine andere Person (die anschließend ermordet wird) verwandelt und nimmt daraufhin wieder ihre eigentliche Identität an, nur um anschließend wieder in die andere Person zurückverwandelt zu werden.

Oft stehen Schuld und Bedrohung in Zusammenhang mit sexuellen Aspekten. In "Der Fall Paradin" genügt bereits der Gedanke an Ehebruch, um das Leben des Protagonisten zu gefährden. In "Berüchtigt" ist der Zusammenhang zwischen Sex, Schuld und Bedrohung zentrales Thema. Hitchcocks Verbindung von Sex und Gewalt wird in Mordszenen deutlich, die er oft wie Vergewaltigungen inszeniert, etwa der Schlusskampf zwischen Onkel und Nichte Charlie in "Im Schatten des Zweifels", die Scherenszene in "Bei Anruf Mord" und die Duschszene in "Psycho". Darüber hinaus spielt Sexualität gerade in abnorm empfundenen Erscheinungsformen eine große Rolle in seinem Werk. Aufgrund der Auflagen der Zensur werden jedoch Homosexualität, die in Verbindung mit Schuld und Verderben regelmäßig vorkommt, oder Nekrophilie (in "Vertigo") nur in einzelnen Gesten oder Schlüsselszenen angedeutet. Auch Fetischismus ("Erpressung", "Vertigo", "Psycho") und Voyeurismus ("Das Fenster zum Hof", "Psycho") spielen in seinen Filmen eine gewisse Rolle. In mehreren Filmen wird zudem ein erotischer Bezug der männlichen Hauptfiguren zu ihren Müttern angedeutet, etwa in "Psycho" und "Die Vögel". Zentral in diesem Zusammenhang ist "Berüchtigt". Hier verhalten sich Claude Rains und Leopoldine Konstantin in manchen Schlüsselszenen wie ein Ehepaar. Dieser Eindruck wird durch den geringen Altersunterschied der Schauspieler von nur vier Jahren verstärkt.

Unter den in Hitchcocks Bildsprache verwendeten Symbolen finden sich Vögel als Vorboten des Unglücks (etwa in "Erpressung", später als vorherrschendes Thema in "Die Vögel"), Treppen, die Verlust oder Freiheit bedeuten können ("Berüchtigt", "Psycho", "Vertigo" und andere), sowie Handschellen und andere Fesseln, um Abhängigkeit und Ausgeliefertsein auszudrücken, meist im sexuellen Kontext (zum Beispiel in "Der Mieter"). Auch Spiegel tauchen bei Hitchcock regelmäßig auf – in Zusammenhang mit dem Verlust oder der Erkenntnis der eigenen Persönlichkeit oder als allgemeines Symbol für Täuschungen (einprägende Beispiele: "Vertigo" und "Psycho").

Die meisten Protagonisten in Hitchcocks Thrillern sind Normalbürger, die zu Beginn der Geschichte in der Regel nichts mit kriminellen Machenschaften zu tun haben. Meist werden sie durch Zufall oder unbekannte Umstände in geheimnisvolle und bedrohliche Vorgänge gezogen. Dem Zuschauer wird so das beunruhigende Gefühl vermittelt, dass auch er jederzeit in derartige Situationen geraten könnte. Professionelle Agenten oder Spione findet man dagegen nur selten unter den Hauptfiguren, obwohl Hitchcock viele Filme drehte, die im Agentenmilieu spielen. Hitchcock drehte bis auf eine Ausnahme ("Erpressung", 1929) auch nie einen Film, in dem die Arbeit der Polizei im Mittelpunkt steht; aktive Polizisten tauchen ansonsten nur als Nebenfiguren und üblicherweise als Hindernis auf.

Der Prototyp des Antihelden bei Hitchcock sind die von James Stewart gespielten Figuren: In "Cocktail für eine Leiche" muss der von Stewart dargestellte Lehrer erkennen, dass zwei seiner Studenten eine seiner Theorien zum Anlass nahmen, einen Mord zu verüben und diesen mit seinen Thesen zu rechtfertigen; am Ende steht er hilflos vor diesem menschlichen Abgrund, in den er nicht nur hineingezogen wurde, sondern den er sogar mit heraufbeschworen hat. In "Das Fenster zum Hof" stellt Stewart eine Figur dar, die bindungsscheu sowie körperlich beeinträchtigt und voyeuristisch veranlagt ist und dadurch in Schwierigkeiten kommt.

Es gibt nur wenige positive, ungebrochene Helden bei Hitchcock. Ein Schauspieler, der diesen seltenen Rollentypus verkörperte, war Cary Grant in "Über den Dächern von Nizza" und in "Der unsichtbare Dritte". Diese Figuren meistern die Herausforderungen zwar mit Charme und Leichtigkeit, doch stehen sie in Verdacht, kriminell zu sein beziehungsweise verlieren sie zeitweise die Kontrolle, womit selbst sie keine gänzlich unantastbaren Helden sein können. Aber sogar Cary Grant spielt in zwei seiner Hitchcock-Filme Figuren, deren Schattenseiten sich zeitweise vor deren positive Merkmale schieben.

Im Laufe der Karriere Hitchcocks gewinnen ambivalente oder gar negativ gezeichnete Hauptfiguren immer stärker an Gewicht. Diese "Antihelden" weisen physische oder psychische Probleme auf, sind Verlierertypen oder unsympathisch. Durch ihr obsessives Fehlverhalten wirken sie schwach und können Schaden anrichten. Diese Figuren dienen zwar kaum als Vorbild, doch soll deren ambivalente Persönlichkeit dazu beitragen, dass sich der Zuschauer in ihnen wiederfinden kann.

In vielen Filmen bedient Hitchcock auf den ersten Blick das klassische Motiv der schwachen, zu beschützenden Frau. Doch während das Klischee verlangt, dass der strahlende Held sie rettet, ist sie bei Hitchcock oft auf sich alleine gestellt. In einigen Fällen ist der vermeintliche Beschützer schwach oder zu sehr mit sich selbst beschäftigt, als dass er der bedrohten Frau helfen könnte, wie zum Beispiel Ingrid Bergman und Cary Grant in "Berüchtigt". In anderen Fällen geht von der männlichen Hauptfigur (in der Regel dem Ehemann) sogar ein tatsächliches oder vermeintliches Bedrohungspotential aus. Klassische Beispiele: Joan Fontaine und Cary Grant in "Verdacht" sowie Grace Kelly und Ray Milland in "Bei Anruf Mord".

Die Rollenverteilung zwischen Mann und Frau kehrt Hitchcock in einigen Filmen gänzlich um: Die Frau ist dem Mann, der zunehmend passiver wird, überlegen und wendet das Geschehen zum Guten. Beispiele sind "Jung und unschuldig" (die Tochter des Polizeichefs verhilft einem Verdächtigen zur Flucht und löst letztendlich den Fall), "Ich kämpfe um dich" (eine Psychologin dringt in das Unterbewusste des Mordverdächtigen ein und rettet ihn vor der sicheren Verurteilung) sowie "Der Mann, der zuviel wußte" (die Ehefrau verhindert zuerst einen geplanten Mord und rettet dann das eigene Kind vor den Verbrechern).

Der Typ, der sich dabei im Laufe der Zeit herauskristallisierte, ist jener der jungen, schönen, kühlen, hintergründigen und undurchsichtigen Blondine. Die oberflächliche Kühle der Hitchcock-Blondine verbirgt jedoch eine stark entwickelte Sexualität. Besonders deutlich wird dies in "Der unsichtbare Dritte", wenn Eva Marie Saint zunächst gegenüber Cary Grant zweideutige Bemerkungen macht, dann plötzlich den völlig überraschten Fremden küsst und ihn ohne Zögern in ihrem Schlafwagenabteil unterbringt. Nicht der Mann, sondern die (blonde) Frau spielt hier den aktiven Part und zeigt damit die Fragilität des männlichen, bürgerlichen Weltbildes.

Hitchcock legt durch seine Gestaltung von Figuren und Dramaturgie dem Zuschauer eine Identifikation mit dem Schurken nahe.
Seine Antagonisten wirken zuweilen auffällig sympathisch und übertreffen mitunter die Ausstrahlung der Hauptfiguren.
Oft konkurrieren Held und Bösewicht um dieselbe Frau; die Liebe des Gegenspielers erscheint dabei tiefer und aufrichtiger als die des Helden. Besonders auffällig ist dies in "Berüchtigt" (Claude Rains gegenüber Cary Grant) und in "Der unsichtbare Dritte" (James Mason wiederum gegenüber Cary Grant). Selbst ein ausgesprochen heimtückischer Schurke wie Ray Milland in "Bei Anruf Mord" wirkt in einzelnen Momenten gegenüber dem unbeholfenen Robert Cummings sympathischer, in jedem Fall jedoch gegenüber der Polizei vertrauenserweckender. Oft sind sie die eigentlichen Hauptfiguren, wie Joseph Cotten als charmanter Witwenmörder in "Im Schatten des Zweifels" oder Anthony Perkins als linkischer, von seiner Mutter gepeinigter Mörder in "Psycho".

In vielen seiner Filme – ab Mitte der 1940er Jahre regelmäßig – tauchen dominante Mütter auf, die einen beunruhigenden Einfluss auf ihre meist erwachsenen Kinder ausüben und zum Teil Auslöser oder Ursache dramatischer Ereignisse sind. Erstmals uneingeschränkt bösartig erscheint die Mutter in "Berüchtigt" (1946), die ihren Sohn zum Mord an der Frau, die er liebt, antreibt.
Der extremste Fall tritt in "Psycho" (1960) zutage, wo die tote Mutter noch von ihrem Sohn Besitz ergreift und ihn zu ihrem mordenden Werkzeug werden lässt. Daneben gibt es eine Vielzahl weniger dämonischer Variationen, wobei Besitzergreifung allen Mutter-Typen gemein ist:
In "Die Vögel" (1963) erträgt es die Mutter von Mitch (Jessica Tandy) nicht, dass ihr erwachsener Sohn (Rod Taylor) sich für eine andere Frau interessiert. In "Marnie" (1964) wird das Leben der Tochter durch einen von der Mutter übertragenen Schuldkomplex beinahe zerstört.

In zwei Filmen variiert Hitchcock dieses Rollenmuster: In "Rebecca" und "Sklavin des Herzens" übernehmen Haushälterinnen die Funktion der dämonischen Mutter.

Üblicherweise positiv besetzte Figuren wie Polizisten, Richter oder andere Vertreter des Staates erscheinen oft zwiespältig: Sie sind nicht in der Lage, die Helden zu beschützen, oder stellen sogar eine Bedrohung für diese dar. Polizisten verdrehen das Recht, sie handeln aus persönlichen Motiven, sie glauben dem ersten Anschein und schützen den tatsächlich Schuldigen aufgrund von dessen vordergründig tadellosem Erscheinen, sie sind tollpatschig oder arbeiten schlampig. Dieses Rollenmuster durchzieht Hitchcocks gesamtes Werk, von "Der Mieter" (1927) bis "Frenzy" (1972).

Darüber hinaus finden sich vereinzelt Geheimdienstmitarbeiter unter den Nebenfiguren, die sich als Gegner (das Ehepaar Drayton in "Der Mann, der zuviel wußte", 1956) oder als Helfer offenbaren, wobei auch letztere Schwierigkeiten bringen können – beispielsweise der „General“ (Peter Lorre) in "Geheimagent" oder Leo G. Carroll als CIA-Mitarbeiter in "Der unsichtbare Dritte". Indem die feste Trennlinie zwischen Gut und Böse verschwimmt, wird das Gefühl der Verunsicherung beim Zuschauer gesteigert.

Hitchcocks Credo lautete: „For me, the cinema is not a slice of life, but a piece of cake.“ (etwa: „Für mich ist das Kino nicht ein Stück aus dem Leben, sondern ein Kinderspiel.“) Film war für ihn eine artifizielle Kunstform. Nur einmal – in "Der falsche Mann" – wich er von diesem Grundsatz ab. Aber auch hier liegt der Akzent auf jenen Elementen, die nicht dokumentarisch sind – etwa der subjektiven Perspektive des unschuldig Verdächtigten und seiner hilflosen Frau. Einigen seiner weiteren Filme liegen zwar auch reale Ereignisse zugrunde ("Der zerrissene Vorhang", "Das Fenster zum Hof", "Der Auslandskorrespondent" oder "Cocktail für eine Leiche"), doch werden diese so weit fiktionalisiert, dass außer dem Grundmotiv kein Bezug zu der ursprünglichen Geschichte übrig bleibt.

Eine nicht verwirklichte Idee für "Der unsichtbare Dritte", die der Regisseur im Interview mit Truffaut erwähnt, verdeutlicht Hitchcocks Vorstellungen davon, die Realität zu transzendieren: Er wollte zeigen, wie unter den Augen Cary Grants auf einem Fließband ein Auto zusammengebaut wird und anschließend aus dem fertiggestellten Auto eine Leiche fällt – nach realistischen Maßstäben unmöglich. Doch Hitchcocks Begründung für das Verwerfen der Idee zeigt, dass er sich in solchen Fragen nicht an der Wahrscheinlichkeit orientierte: „Wir haben die Idee in der Geschichte nicht richtig unterbringen können, und selbst eine willkürliche Szene kann man nicht ohne Motiv ausführen.“ Den Vorwurf, Gesetze der Plausibilität zu missachten, nahm er bewusst in Kauf: „Wenn man alles analysieren wollte und alles nach Erwägungen der Glaubwürdigkeit und Wahrscheinlichkeit konstruieren, dann würde keine Spielfilmhandlung dieser Analyse standhalten, und es bliebe einem nur noch eines übrig: Dokumentarfilme zu drehen.“ Hitchcock vertraute darauf, dass die Zuschauer unwahrscheinliche Details akzeptieren würden, da er diese nur verwendete, um die Handlung zu dramatisieren, voranzutreiben oder zu straffen.

Für bewusste Irritation sorgte auch Hitchcocks Spiel mit filmtypischen Klischees. So vermied er es insbesondere bei den Nebenrollen, Schauspieler nach festgelegtem Typ zu besetzen. Auch bei der Wahl seiner Spielorte entzog sich Hitchcock den Genre-Gesetzen. So ließ er Verbrechen und bedrohliche Szenen häufig nicht in unheimlichen, dunklen Räumen stattfinden, sondern bei hellem Tageslicht und an scheinbar harmlosen Orten wie einem mit Menschen übersäten Marktplatz ("Der Mann, der zuviel wußte" [1956] und "Der Auslandskorrespondent"), in einer menschenleeren Landschaft, auf einer öffentlichen Versteigerung und in einer Hotelhalle ("Der unsichtbare Dritte"), auf einer idyllischen Bergstraße ("Über den Dächern von Nizza"), auf einer Party ("Berüchtigt" und "Jung und unschuldig") in einer voll besetzten Konzerthalle (beide "Der Mann, der zuviel wußte") oder in einem mit lauter freundlichen Menschen besetzten Eisenbahnzug ("Eine Dame verschwindet").

Die klassische, auf das Überraschungsmoment aufbauende Form des Kriminalfilms ist der Whodunit. Bis auf wenige Ausnahmen bediente sich Hitchcock jedoch einer anderen Form des Spannungsaufbaus, des sogenannten Suspense: Dem Zuschauer sind ab einem gewissen Zeitpunkt bestimmte Informationen oder Umstände bekannt, von denen die handelnden Personen nichts wissen. Er fiebert in besonderer Weise mit den Helden, er sieht Ereignisse kommen, möchte den Figuren helfen, kann es aber nicht. In einigen Filmen wird das klassische Suspense dahingehend variiert, dass handelnde Personen die Rolle des Zuschauers übernehmen. Ein Beispiel von vielen: In "Das Fenster zum Hof" dringt Lisa in die Wohnung des verdächtigen Nachbarn ein, um nach Beweisen für einen möglichen Mord zu suchen. Ihr Partner Jeff beobachtet das Geschehen von der gegenüber liegenden Wohnung aus und sieht dabei den Nachbarn vorzeitig zurückkommen. Er vermutet sie in Lebensgefahr, kann ihr aber nicht helfen.

Für einige markante Szenen baute Hitchcock zudem bewusst eine Suspense-Situation auf, um den Zuschauer mit einem umso gewaltigeren Überraschungseffekt schockieren zu können. Ein berühmtes Beispiel findet sich in "Psycho": Zum einen ist Marion Crane mit verschiedenen Insignien einer typischen Hauptfigur eines Hitchcockfilms ausgestattet, so dass kaum jemand erwartet, dass sie bereits in der ersten Hälfte des Films stirbt. Zum anderen schaltet Hitchcock der Duschszene selbst einen Suspense-Moment vor. Norman Bates beobachtet Marion Crane durch ein Loch in der Wand beim Entkleiden. Sie geht unter die Dusche. Der Zuschauer wird nun eben keinen Mord, sondern schlimmstenfalls eine Vergewaltigung durch Norman befürchten. Der bestialische Mord ist somit völlig überraschend und damit ein Grund für die Berühmtheit der Szene.

Ein von Hitchcock in seinen Thrillern sehr häufig verwendetes Mittel war der sogenannte MacGuffin: ein Element, das die Handlung vorantreibt oder sogar initiiert, obwohl es für die Entwicklung der Figuren und für den Zuschauer inhaltlich völlig bedeutungslos, geradezu austauschbar ist. Der MacGuffin in "Der unsichtbare Dritte" sind schlicht „Regierungsgeheimnisse“, über die der Held oder der Zuschauer während der gesamten Handlung nichts Weiteres erfährt. In "Psycho" benutzt Hitchcock unterschlagenes Geld, das die Sekretärin zur Flucht treibt und so in „Bates Motel“ führt, um das Publikum anfangs gezielt in die Irre zu führen und für einen Kriminalfall zu interessieren, der mit der eigentlichen Handlung nur am Rande zu tun hat. Die mysteriösen „39 Stufen“ im gleichnamigen Film sind eine Geheimorganisation, über die bis kurz vor Ende des Films überhaupt nichts bekannt ist, außer dass sie gefährlich ist. Ein besonders außergewöhnlicher MacGuffin ist die als Volksliedmelodie getarnte Geheimdienstinformation aus "Eine Dame verschwindet".

Beeinflusst vom Stummfilm beruhte Hitchcocks Filmverständnis auf dem Anspruch, alles Wichtige in seinen Filmen visuell und so wenig wie möglich durch Dialoge auszudrücken. Seine typischen Kameraeinstellungen geben im Bild genau das wieder, was für das Verständnis der Szene wesentlich ist – auch um dem Zuschauer nicht die Möglichkeit zu geben, sich durch unwesentliche Details ablenken zu lassen. So wirken beispielsweise Kuss-Szenen bei Hitchcock immer sehr intim, da er gewöhnlich mit der Kamera sehr nahe an die beiden sich Küssenden heranfuhr und den Zuschauer sozusagen zum dritten Umarmenden machte. Zu den berühmtesten Beispielen dieser visuellen Erzählweise zählen die Duschszene aus "Psycho", der Flugzeugangriff auf Cary Grant und die Jagd auf Mount Rushmore in "Der unsichtbare Dritte", die Versammlung der Vögel auf dem Klettergerüst in "Die Vögel" oder die zehnminütige Konzertszene in der Royal Albert Hall in "Der Mann, der zuviel wußte" von 1956.

Hitchcocks visueller Arbeitsstil drückt sich unter anderem in den Expositionen vieler seiner Filme aus. Er bringt den Zuschauern die handelnden Figuren und die Umstände der folgenden Handlung ohne die Verwendung von Dialogen nahe. Die Länge dieser Einführungen variiert zwischen wenigen Sekunden und mehreren Minuten. Erstmals verfolgte Hitchcock diese Technik 1929 in seinem ersten Tonfilm "Erpressung".

Zudem tauchen in Hitchcocks Filmen immer wieder ungewöhnliche filmische Operationen auf, um die Stimmung und Spannung bewusst zu verstärken, beispielsweise eine gegenläufige Zoom-Fahrtbewegung in "Vertigo" (später auch als „Vertigo-Effekt“ bezeichnet), lange Kamerafahrten wie die aus einer Totale eines großen Raums bis in die Naheinstellung eines Schlüssels in einer Hand (in "Berüchtigt") oder auf ein zuckendes Auge (in "Jung und unschuldig") sowie die aus ungefähr siebzig Einstellungen bestehende fünfundvierzig Sekunden lange Mordszene unter der Dusche in "Psycho", unmittelbar gefolgt von einer etwa einminütigen Kamerafahrt ohne einen einzigen Schnitt. Der Production Designer Robert Boyle, mit dem Hitchcock bei fünf Filmen zusammenarbeitete, meinte: „Keiner der Regisseure, mit denen ich je zusammengearbeitet habe, wusste so viel über Film wie er. Viele der Regisseure, mit denen ich gearbeitet habe, wussten eine ganze Menge, aber sie besaßen nicht die technischen Fähigkeiten, die er hatte. Er suchte immer nur den visuellen Ausdruck, und so etwas wie eine zufällige Einstellung gab es bei ihm nicht.“

Nur einmal griff Hitchcock aus Experimentierfreude auf einen filmtechnischen Kniff zurück, der sich nicht unmittelbar aus der Dramaturgie ergab. In "Cocktail für eine Leiche" (1948) drehte er bis zu zehn Minuten lange Einstellungen, die er zum großen Teil sogar über unsichtbare Schnitte ineinander übergehen ließ. Er wollte damit bei dieser Theaterverfilmung die Einheit von Zeit und Raum dokumentieren. Später gab er zu, dass es ein Fehler war, damit gleichzeitig den Schnitt als wesentliches gestaltendes Instrument der Dramaturgie aus der Hand gegeben zu haben.

Inspiriert von amerikanischen und deutschen Filmemachern, setzte Hitchcock schon bei seinen ersten Filmen Licht- beziehungsweise Schatteneffekte ein. Typisch für Hitchcock sind Linien und Streifen in Form von Schatten (durch Gitter, Jalousien oder Ähnliches verursacht), die vor allem auf Gesichter fallen und eine unheilvolle Atmosphäre verstärken sollen. Darüber hinaus verwendet er in einzelnen Szenen sehr starke, zum Teil unnatürlich wirkende Kontraste, um einen äußeren oder inneren Gut-Böse-Gegensatz zu visualisieren.

Dieses Hell-Dunkel-Spiel unterstützte Hitchcock durch die Kostüme der Figuren. So ließ er Ingrid Bergman am Anfang von "Berüchtigt" gestreifte Kleidung tragen, um ihre Zerrissenheit zu unterstreichen. In "Der Mieter" trug Ivor Novello zu Beginn Schwarz, später, um seine Unschuld auch nach außen hin deutlich zu machen, Weiß. Die Methode, durch die Farbgebung der Kostüme den emotionalen Zustand der Figuren zu unterstreichen, behielt Hitchcock auch für die Farbfilme bei. In "Bei Anruf Mord" wurden die Kostüme von Grace Kelly mit der Dauer des Films immer trister und grauer, entsprechend ihrer inneren Gemütsverfassung.
Zu Hitchcocks Farbwahl von Grace Kellys Kleidern in "Das Fenster zum Hof" sagte die Kostümbildnerin Edith Head: „Für jede Farbe und jeden Stil gab es einen Grund; er war sich seiner ganzen Entscheidung absolut sicher. In einer Szene sah er sie in blassem Grün, in einer anderen in weißem Chiffon, in einer weiteren in Gold. Er stellte im Studio tatsächlich einen Traum zusammen.“ In seinen späteren Filmen, allen voran "Marnie" und "Vertigo", gab es eine ausgefeilte, die Kostüme, die Dekors und die Beleuchtung umfassende Farbdramaturgie.

Nach Hitchcocks Filmverständnis schafft sich der Film seine eigene Realität und soll oder darf kein Abbild des wahren Lebens sein. Die Nutzung sämtlicher Möglichkeiten, genau das wiederzugeben, was der Regisseur sich vorstellt, ist nach diesem Verständnis nicht nur legitim, sondern erforderlich. Hitchcock hat die Entwicklung der Tricktechnik aufmerksam beobachtet und schon sehr früh – gelegentlich zum Missfallen seiner Produzenten – neue Trickverfahren eingesetzt, zum Beispiel das Schüfftan-Verfahren (in "Erpressung") oder das Matte Painting. In seinen englischen Thrillern, vor allem in "Nummer siebzehn" und "Jung und unschuldig", arbeitete Hitchcock bei Verfolgungsjagden oft und erkennbar mit Modellen. In "Eine Dame verschwindet" sind die Rückprojektionen während der Zugfahrt aber bereits so ausgereift, dass sie noch Jahrzehnte später überzeugen. Ähnliches gilt für die Schlussszene von "Der Fremde im Zug", in der zwei Männer auf einem sich immer schneller drehenden Karussell kämpfen – in einer virtuosen Kombination von Realeinstellungen, Modellen und Rückprojektionen. "Die Vögel" (1963) beinhaltet rund vierhundert Trickeinstellungen, für die Hitchcock auf sämtliche damals verfügbaren Tricktechniken zurückgriff, unter anderem auch auf das ansonsten für Animationsfilme verwendete Rotoskopieverfahren.

Hitchcock hat seit dem Aufkommen des Tonfilms Musik und Toneffekte eingesetzt, um die Dramaturgie bewusst zu unterstützen. Den Umgang Hitchcocks mit dem Medium Ton beschrieb die Schauspielerin Teresa Wright ("Im Schatten des Zweifels") folgendermaßen: „Wenn ein Schauspieler mit den Fingern trommelte, war das nicht ein zweckloses Trommeln, es hatte einen Rhythmus, ein musikalisches Muster – es war wie ein Geräusch-Refrain. Ob jemand nun ging oder mit Papier raschelte oder einen Umschlag zerriss oder vor sich hin pfiff, ob das Flattern von Vögeln oder ein Geräusch von draußen war, alles wurde sorgfältig von ihm orchestriert. Er komponierte die Toneffekte wie ein Musiker Instrumentenstimmen.“ Gegenüber Truffaut erwähnte Hitchcock, dass er nach dem Endschnitt eines Films seiner Sekretärin ein „Tondrehbuch“ diktiert, das alle von ihm gewünschten Geräusche enthält.

In "Mord – Sir John greift ein!" (1930) versteckte Hitchcock, da ein nachträgliches Bearbeiten der Tonspur zu diesem Zeitpunkt technisch noch nicht möglich war, gar ein komplettes Orchester hinter den Kulissen, um die entsprechenden Stellen musikalisch zu untermalen. Weitere klassische Beispiele für Hitchcocks dramaturgischen Musikeinsatz sind "Geheimagent" (1936, der Dauerakkord des toten Organisten in der Kirche), "Eine Dame verschwindet" (1938, die Melodie mit dem Geheimcode und der „Volkstanz“), "Im Schatten des Zweifels" (1943, der „Merry-Widow“-Walzer), "Der Fremde im Zug" (1951, die Szenen auf dem Rummelplatz) und "Das Fenster zum Hof" (1954, die im Laufe des Films entstehende Komposition des Klavierspielers). In "Der Mann, der zuviel wußte" (1956) schließlich wird Musik, sowohl orchestral wie auch gesungen, aktiv inszeniert und dramaturgisch eingebunden: Sie spielt eine wesentliche Rolle in der Gesamtdramaturgie des Films.

Die Musik der Filme aus den späten 1950er und frühen 1960er Jahren, der Zeit als Hitchcock mit dem Komponisten Bernard Herrmann zusammenarbeitete, ist tragendes Element der jeweiligen Filme. Kritiker bescheinigen der Musik der Filme "Vertigo", "Der unsichtbare Dritte" und "Psycho" sowie den Toneffekten von Oskar Sala zu "Die Vögel", wesentlich zum jeweiligen Gesamteindruck des Films beizutragen.

Hitchcock war beeindruckt von den Filmen, die er in seiner Jugend und in seinen frühen Jahren im Filmgeschäft sah, etwa jenen von D. W. Griffith, Charlie Chaplin, Buster Keaton und Douglas Fairbanks senior. Als Stummfilmregisseur in England übernahm er vom US-Film unter anderem die Technik, mit Hilfe von Beleuchtungseffekten Tiefe zu schaffen und den Vorder- vom Hintergrund abzusetzen, was bis in die 1930er Jahre im britischen Film unüblich war. Angetan war er auch von den deutschen Stummfilmregisseuren wie Fritz Lang und Ernst Lubitsch. F. W. Murnaus "Der letzte Mann", dessen Dreharbeiten Hitchcock 1922 in München beobachtete, bezeichnete er später als den fast perfekten Film: „Er erzählte seine Geschichte ohne Titel; von Anfang bis Ende vertraute er ganz auf seine Bilder. Das hatte damals einen ungeheueren Einfluss auf meine Arbeit.“ Einfluss auf Hitchcocks Arbeit hatte auch "Das Cabinet des Dr. Caligari", den Robert Wiene 1919 drehte. Die Betonung des Visuellen im deutschen Expressionismus prägte seinen eigenen Umgang mit filmischen Mitteln.

Abgesehen von diesen stilistischen Einflüssen vermied es Hitchcock jedoch, Szenen oder Einstellungen bekannter Filme zu zitieren. Als Ausnahme kann "Panzerkreuzer Potemkin" (1925) des sowjetischen Regisseurs Eisenstein angesehen werden. In "Die 39 Stufen", "Über den Dächern von Nizza" und einigen weiteren Filmen erinnern die vor Entsetzen schreienden Frauen an Einstellungen aus der berühmten und oft zitierten Szene an der Hafentreppe in Odessa.

Es gibt außerdem in Hitchcocks Werk aus den 1940er und 1950er Jahren einige motivische und visuelle Überschneidungen mit der Gattung des Film noir, die den amerikanischen Kriminalfilm in jener Zeit bestimmte, etwa in "Im Schatten des Zweifels" und "Berüchtigt", und besonders in "Der falsche Mann", wo das Motiv der allgegenwärtigen Bedrohung der Hauptfiguren eine Rolle spielt. Darüber hinaus bediente er sich gerne einer ähnlich kontrastreichen Bildgestaltung, die er sich in den Grundzügen allerdings bereits in den 1920er Jahren angeeignet hatte. Auch "Vertigo" erinnert in der Grundkonstellation und der alptraumhaften Zwanghaftigkeit der Geschehnisse an einige Filme des Genres, wie zum Beispiel "Frau ohne Gewissen", hebt sich jedoch formal und stilistisch deutlich vom Film noir ab.
Als typischer Vertreter des Genres kann Hitchcock jedenfalls nicht angesehen werden.

Seine Vorliebe für Blondinen erklärte Hitchcock gegenüber Truffaut wie folgt: „Ich finde, die englischen Frauen, die Schwedinnen, die Norddeutschen und die Skandinavierinnen sind interessanter als die romanischen, die Italienerinnen und die Französinnen. Der Sex darf nicht gleich ins Auge stechen. Eine junge Engländerin mag daherkommen wie eine Lehrerin, aber wenn Sie mit ihr in ein Taxi steigen, überrascht sie Sie damit, dass sie Ihnen in den Hosenschlitz greift.“ Ähnlich äußerte er sich 1969 gegenüber "Look" über die Truffaut-Schauspielerin Claude Jade, die bei ihm in "Topaz" gespielt hatte: „Claude Jade ist eine eher ruhige junge Dame, doch für ihr Benehmen auf dem Rücksitz eines Taxis würde ich keine Garantie übernehmen“.

Dass Hitchcock zu seinen jungen blonden Schauspielerinnen ein besonderes Verhältnis hatte und ihnen mehr Aufmerksamkeit widmete als allen anderen, war schon früh bekannt. Die Sorgfalt, mit der Hitchcock bereits in den 1930er und 1940er Jahren Madeleine Carroll, Carole Lombard und insbesondere Ingrid Bergman in Szene setzte, entwickelte sich mit der Zeit zu einer sich steigernden Verquickung privater und beruflicher Interessen, die sich zu einer Obsession ausweitete. Mit Vera Miles probte er die Nervenzusammenbrüche, welche sie in "Der falsche Mann" darstellen sollte, wochenlang jeweils mehrere Stunden täglich. Für sie wie für Kim Novak ließ er von der Kostümbildnerin eine komplette Garderobe schneidern, die für ihr privates Leben gedacht war. Tippi Hedren ("Die Vögel") ließ er sogar von zwei Crew-Mitgliedern beschatten und begann, ihr Vorschriften für ihr Verhalten im Privatleben zu machen.

Diese Vereinnahmung hatte ihren Höhepunkt in sich über Tage hinziehenden Aufnahmen von auf sie einstürzenden, echten Vögeln. Nach einem eindeutigen, erfolglosen Annäherungsversuch während der Arbeiten zu "Marnie" kam es schließlich zum Bruch. Die zuvor offen bekundete Zuneigung schlug ins Gegenteil um, und Hitchcock ließ keine Gelegenheit aus, Tippi Hedren bei anderen herabzusetzen. Sie blieb die letzte typische „Hitchcock-Blondine“. Zwar hielt sich Hitchcock darüber stets äußerst bedeckt, doch es gilt als gesichert, dass der Regisseur sich von diesen Schwierigkeiten lange nicht erholen konnte und in seiner kreativen Schaffenskraft beeinträchtigt war.
Ebenso gelten Filme wie "Vertigo" und "Berüchtigt", aber auch "Marnie" oder "Im Schatten des Zweifels", die von neurotischen Männern handeln, die Frauen manipulieren, als stark autobiographisch.

Auch die Verbindung zwischen Sex und Gewalt faszinierte Hitchcock, was vor allem in seinen späteren Werken immer deutlicher zutage tritt. Mehrfach inszenierte er vollendete oder versuchte Vergewaltigungen (schon früh in "Erpressung", später dann in "Marnie" und "Frenzy"). In drei nicht realisierten Projekten sollten Vergewaltiger oder Vergewaltigungen eine zentrale Rolle spielen. Morde inszenierte er einige Male als Vergewaltigungen, mit dem Messer als Phallus-Symbol. Doch auch der Tod durch Erwürgen oder Strangulieren übte eine gewisse Faszination auf ihn aus. Einige Würgeszenen gehören zu den bemerkenswertesten Mordszenen seiner Karriere, etwa in "Cocktail für eine Leiche", "Bei Anruf Mord", "Der zerrissene Vorhang" und "Frenzy". Sich selbst ließ er oft in „Würgerposen“ ablichten.

Ähnlich offen kokettierte Hitchcock zeit seines Lebens mit seiner panischen Angst vor der Polizei. Hitchcock erzählte gerne, dass er mit fünf Jahren, nachdem er etwas angestellt hatte, von seinem Vater mit einem Zettel auf das nahegelegene Polizeirevier geschickt worden sei. Der Polizist las den Zettel und sperrte Alfred für fünf oder zehn Minuten in eine Zelle mit dem Kommentar, dies würde die Polizei mit ungezogenen Jungen so machen. In seinen Filmen geht von Polizisten stets eine latente Gefahr aus.

Zu der Frage, inwieweit das von Hitchcock in seinen Filmen transportierte Bild der besitzergreifenden Mutter von der eigenen Mutter geprägt ist, gab es von ihm selbst keinerlei Aussagen. Das wenige, was man aus seiner Kindheit weiß, legt jedoch autobiographische Ursprünge nahe. Hitchcocks Mutter starb nach langer Krankheit im August 1942 während der Dreharbeiten zu "Im Schatten des Zweifels". Dieser bereits von vornherein stark autobiographisch geprägte Film nimmt eindeutig Bezug auf Hitchcocks Verhältnis zu ihr: Der Name Emma scheint nicht die einzige Gemeinsamkeit zwischen ihr und der dominanten Mutterfigur im Film zu sein. Zudem ist im Film noch von einer anderen gebieterischen, jedoch kranken Mutter die Rede – jener des Krimi-Besessenen Herb Hawkins, der wiederum als Selbstprojektion Hitchcocks gilt.

Auffallend oft sind Toiletten in Hitchcocks Filmen zu sehen oder zu hören, in denen konspirative Dinge irgendwelcher Art stattfinden. Laut seinem Biographen Donald Spoto hatte er eine „pubertäre Fixierung“, die in seiner viktorianischen Erziehung begründet lag. Hitchcock äußerte sich zwar oft und gerne über menschliche Körperfunktionen, wollte aber den Eindruck erwecken, er selbst habe mit solchen Dingen nichts zu tun. Bezugnehmend auf seine Körperfülle, deutete Hitchcock hingegen mehrfach an, dass für ihn Essen eine Art Ersatzbefriedigung sei. So gibt es in einigen Hitchcockfilmen eine symbolische Verbindung von Essen, Sex und Tod.

In den USA galt zwischen 1934 und 1967 der "Hays Code", auch "Production Code" genannt, eine Sammlung von Richtlinien über die Einhaltung der gängigen Moralvorstellungen und über die Zulässigkeit der Darstellung von Kriminalität, Gewalt und Sexualität im Film.

So musste Hitchcock zum Beispiel das geplante Ende für "Verdacht" fallen lassen, weil es Anfang der 1940er Jahre nicht möglich war, den Selbstmord einer schwangeren Frau zu zeigen. Noch bis kurz vor Schluss der Dreharbeiten hatte er kein passendes Ende für den Film gefunden. In "Berüchtigt" musste Hitchcock einen Dialog streichen, in dem sich ein Vertreter der US-Regierung positiv über die Möglichkeit einer Ehescheidung äußerte. Bei "Saboteure" drehte er politisch heikle Textstellen zur Sicherheit alternativ in entschärften Versionen.

Doch in vielen Fällen gelang es ihm, die Beschränkungen durch die Zensur kreativ zu umgehen. So war es damals unter anderem nicht erlaubt, eine Toilette zu zeigen. Daher verzerrte Hitchcock in "Mr. und Mrs. Smith" die eindeutigen Geräusche einer Toilette so, dass man sie für eine Dampfheizung halten konnte. In "Psycho" zeigte er eine Toilette, in der ein Papierzettel hinuntergespült wurde. Indem er das Bild der Toilette mit einer dramaturgischen Funktion versah – das Verschwinden eines Beweisstücks musste erklärt werden – verhinderte er, dass die Szene geschnitten wurde. Niemals wurde eine Toilette zu Zeiten des Hays Code expliziter gezeigt.

Da auch die Länge von Küssen im Film damals auf drei Sekunden begrenzt war, inszenierte Hitchcock den Kuss zwischen Ingrid Bergman und Cary Grant in "Berüchtigt" als Folge von einzelnen, durch kurze Dialogsätze unterbrochenen Küssen. Hitchcocks größter Sieg gegen die Zensur war die Schlussszene von "Der unsichtbare Dritte": Cary Grant und Eva Marie Saint befinden sich in einem Schlafwagen. Er zieht sie zu sich nach oben in das obere Bett, und sie küssen sich. Es erfolgt ein Umschnitt, und man sieht einen Zug in einen Tunnel rasen – eine der explizitesten Andeutungen des Sexualakts in einem US-Film zu Zeiten des "Production Code".

Einer der wichtigsten Aspekte der Arbeitsweise Alfred Hitchcocks war, dass er im Idealfall von der Stoffauswahl bis zum Endschnitt nichts dem Zufall überließ, sondern die völlige Kontrolle über die Herstellung des Films beanspruchte.

Wenn Hitchcock existierende Vorlagen benutzte, etwa Romane oder Bühnenstücke, übernahm er nur einzelne Grundmotive der Handlung und entwickelte daraus zusammen mit dem jeweiligen Drehbuchautor oft eine völlig neue Geschichte. Hochwertige, komplexe Literatur sperrte sich gegen diesen Umgang und Hitchcock scheute daher deren Verfilmung – auch aus Respekt vor dem Werk.
Hitchcock war meist an der Drehbucherstellung beteiligt, wurde aber nach 1932 bei keinem seiner Filme offiziell als Autor in Vor- oder Abspann erwähnt: „Ich will nie einen Titel als Produzent oder Autor. Ich habe das Design des Films geschrieben. Mit anderen Worten, ich setze mich mit dem Autor zusammen und entwerfe den ganzen Film vom Anfang bis zum Ende.“ Der Autor Samuel A. Taylor: „Mit ihm zu arbeiten, bedeutete auch mit ihm zu schreiben, was auf die wenigsten Regisseure zutrifft. Hitchcock behauptete nie, selbst ein Schriftsteller zu sein, aber in Wahrheit schrieb er doch seine eigenen Drehbücher, denn er sah bereits jede Szene deutlich in seinem Kopf vor sich und hatte eine sehr genaue Vorstellung davon, wie sie ablaufen sollte. Ich merkte, dass ich nur noch die Figuren persönlicher und menschlicher zu gestalten brauchte und sie weiter entwickeln musste.“ Gelegentlich veränderte Hitchcock im Nachhinein noch die Dialoge ganzer Szenen, etwa um die Spannungs-Dramaturgie zu verbessern (Beispiel: "Das Rettungsboot") oder um autobiographische Bezüge einzubauen (Beispiel: "Ich beichte").
Auch wenn ihm geschliffene Dialoge wichtig waren, legte Hitchcock sein Hauptaugenmerk stets auf die Ausdruckskraft der Bilder. So wurde im Idealfall jede einzelne Einstellung des Films vor Drehbeginn in Storyboards festgelegt.

Seit Beginn seiner Regisseurtätigkeit verfolgte er das Ziel, jegliche Improvisation so weit es geht zu vermeiden. Gegenüber Truffaut erklärte er: „Ich habe Angst davor gehabt, im Atelier zu improvisieren, weil, selbst wenn man im Augenblick Ideen hat, bestimmt keine Zeit bleibt nachzuprüfen, was sie taugen. [… Andere Regisseure] lassen ein ganzes Team warten und setzen sich hin, um zu überlegen. Nein, das könnte ich nicht.“

Nach eigenen Aussagen bereitete Hitchcock die Planung eines Projekts mehr Freude als die eigentlichen Dreharbeiten: Durch zu viele Einflüsse – Produzenten, Technik, Schauspieler, Zeitdruck – sah er die angestrebte Kontrolle über sein Werk bedroht. Außerdem sah er im Idealfall die kreative Arbeit am Film mit Beginn der Dreharbeiten als abgeschlossen an: „Ich drehe einen vorgeschnittenen Film. Mit anderen Worten, jedes Stück Film ist entworfen, um eine Funktion zu erfüllen.“

Diese Grundsätze waren jedoch eher eine Idealvorstellung Hitchcocks. Tatsächlich wurde es ihm spätestens ab 1948 zur Gewohnheit, beim Drehen Alternativen auszuprobieren. Doch auch hier bemühte er sich um möglichst exakte Vorausplanung: Ein Beispiel hierfür ist die Belagerung des Hauses durch die Vögel in "Die Vögel". Gegenüber Truffaut beschrieb Hitchcock, wie er die ursprünglich geplante Szene noch unmittelbar am Drehort umschrieb und bis ins kleinste Detail skizzierte, so dass sie kurz darauf entsprechend diesen neuen Entwürfen gedreht werden konnte. Darüber hinaus wurde Hitchcock im Laufe seiner Karriere immer freier, auch kurzfristig vom festgelegten Drehbuch abzuweichen. Entgegen seinen Gewohnheiten ließ er sogar Improvisationen der Schauspieler zu, wenn auch nur bei eher unwichtigen Szenen.
Bill Krohn ging 1999 in "Hitchcock at Work" ausführlich auf Hitchcocks Arbeitsweise ein. Er rekonstruierte auf Basis von Originalunterlagen wie Drehbuchversionen, Skripte, Storyboards, Memos, Produktionsnotizen etc. und mit Hilfe von Beteiligten die Produktionsgeschichte diverser Filme (darunter Hitchcocks berühmteste) und widerlegt Hitchcocks Bekenntnis zum „vorgeschnittenen Films“: So kam es bei vielen Filmen vor, dass Hitchcock entscheidende Schlüsselszenen in verschiedenen Varianten drehte und meist erst im Schneideraum über die endgültige Form einzelner Szenen entschied.

Im Laufe der Jahre entwickelte sich mit verschiedenen Autoren eine besonders kreative Zusammenarbeit. Hervorzuheben sind Eliot Stannard, Angus McPhail, Charles Bennett, Ben Hecht und John Michael Hayes.
Obwohl Samuel A. Taylor "(Vertigo)" und auch Ernest Lehman "(Der unsichtbare Dritte)" nur je zwei Drehbücher zu tatsächlich realisierten Filmen schrieben, gehörten sie zu den wenigen Mitarbeitern, die mit ihm in den letzten Jahren seiner Karriere regelmäßig zusammenarbeiteten und bis kurz vor seinem Tod Kontakt hatten.
Doch auch mit namhaften Theater- oder Romanautoren arbeitete Hitchcock mehrfach bei der Drehbucherstellung zusammen, reibungslos mit Thornton Wilder und George Tabori, konfliktbeladen mit John Steinbeck, Raymond Chandler und Leon Uris.
Der Kult, den Hitchcock gern um seine Person betrieb, und sein manchmal diktatorischer Stil, führte auch zu Konflikten mit befreundeten Autoren. John Michael Hayes, der im Streit von Hitchcock schied: „Ich tat für ihn, was jeder andere Autor für ihn tat – ich schrieb! Wenn man aber Hitchcocks Interviews liest, kann man den Eindruck bekommen, er habe das Drehbuch geschrieben, die Charaktere entwickelt, die Motivation beigesteuert.“
Wenn Hitchcock mit der Arbeit eines Autors nicht zufrieden war, oder wenn er seine Autorität angegriffen fühlte, dann ersetzte er Autoren kurzerhand durch andere.

Cary Grant und James Stewart wurden innerhalb der jeweils vier Filme, die sie für Hitchcock drehten, zu Hitchcocks Alter Ego. Grant wurde zu dem, „was Hitchcock gerne gewesen wäre“, wie es Hitchcocks Biograph Donald Spoto formulierte, während Stewart vieles wäre, „von dem Hitchcock dachte, er sei es selbst“. Mit einigen seiner Schauspieler verband Hitchcock zudem eine langjährige persönliche Freundschaft, allen voran mit Grace Kelly. Darüber hinaus sind die als neurotisch zu bezeichnenden Beziehungen zu seinen blonden Hauptdarstellerinnen – insbesondere mit Tippi Hedren – bekannt.
Am Anfang von Hitchcocks Karriere galt Film in England als Unterhaltung für die Unterschicht. Aus dieser Zeit stammt Hitchcocks oft zitierter Ausspruch „Alle Schauspieler sind Vieh“, der sich auf diejenigen Theaterschauspieler bezog, die nur mit Widerwillen und des Geldes wegen nebenher als Filmschauspieler arbeiteten. Die Aussage verselbständigte sich später und wurde oft als genereller Ausdruck der Geringschätzung Hitchcocks Schauspielern gegenüber angesehen. Tatsächlich hatte er auch später oft Probleme mit Schauspielern, die eigene Vorstellungen durchsetzen wollten, anstatt sich in die vorgefertigte Planung des Regisseurs einzufügen. Anhänger des Method Actings wie Montgomery Clift und Paul Newman waren Hitchcock daher genauso lästig wie Exzentriker oder Egomanen. Große Achtung hatte Hitchcock hingegen vor Schauspielern, die sein Filmverständnis teilten oder sich zumindest seiner Arbeitsweise anpassten, und gewährte etwa Joseph Cotten und Marlene Dietrich große künstlerische Freiheiten. Oft waren es jedoch die Produzenten, die über die Besetzung der Hauptrollen entschieden. Umso mehr nutzte Hitchcock seine größere Freiheit bei den zu besetzenden Nebenrollen, wobei er gerne auf Theaterschauspieler zurückgriff, die er noch aus seiner Zeit in London in bester Erinnerung hatte, zum Beispiel Leo G. Carroll in insgesamt sechs Filmen oder Cedric Hardwicke in "Verdacht" und "Cocktail für eine Leiche".

Die bekannte Kostümbildnerin Edith Head, mit der er ab "Das Fenster zum Hof" bei fast allen Filmen zusammenarbeitete, meinte: „Loyalität war Hitchcock besonders wichtig. Er war Mitarbeitern gegenüber so loyal, wie er es von ihnen erwartete.“

Bei fünf Filmen war Robert F. Boyle für das "Production Design" verantwortlich; er gehörte bis zu Hitchcocks Tod zu dessen engsten Mitarbeitern. Außerdem griff er im Laufe seiner Karriere gern auf Albert Whitlock als Szenenbildner zurück. Äußerst zufrieden war Hitchcock, dem die Ausdruckskraft der Bilder stets wichtig war, auch mit dem Art Director Henry Bumstead. Der Titeldesigner Saul Bass entwarf nicht nur einige Filmtitel für die Vorspanne sowie Plakate, sondern war bereits bei den Arbeiten an vielen Storyboards maßgeblich beteiligt.

Wichtigster Kameramann in seinen frühen Jahren bei den British International Pictures war John J. Cox. Über Hitchcock sagte Kameramann Robert Burks, der mit Ausnahme von "Psycho" an allen Filmen zwischen 1951 und 1964 beteiligt war: „Man hatte nie Ärger mit ihm, solange man etwas von seiner Arbeit verstand und sie ausführte. Hitchcock bestand auf Perfektion.“ Mit Leonard J. South, ehemaliger Assistent Burks’, arbeitete Hitchcock über einen Zeitraum von insgesamt 35 Jahren zusammen.

Von den Komponisten der Filmmusiken ist Louis Levy hervorzuheben, der die Soundtracks für die frühen englischen Filme von "Der Mann, der zuviel wußte" bis "Eine Dame verschwindet" beisteuerte. Als der Hitchcock-Komponist schlechthin gilt Bernard Herrmann, der ab "Immer Ärger mit Harry" bis einschließlich "Marnie" (1964) alle Filmmusiken für Hitchcock komponierte.

Der Filmeditor George Tomasini war bis zu seinem Tod 1964 ein Jahrzehnt lang enger Mitarbeiter Hitchcocks. Zu Beginn seiner Karriere wirkte seine Frau Alma als Editorin bei seinen Filmen mit; sie blieb bis zuletzt eine der einflussreichsten Mitarbeiterinnen.

Schon zu Beginn seiner Karriere war Hitchcock die Bedeutung der Vermarktung der eigenen Person bewusst: Viele seiner späteren Tätigkeiten sind Teil einer Strategie, sich und seinen Namen als Marke zu etablieren. Bereits 1927 führte Hitchcock ein stilisiertes Selbstporträt als Logo, das bis heute bekannt ist. Anfang der 1930er Jahre, als er mit dem Erfolg seiner Filme in England populär wurde, gründete er mit der Hitchcock Baker Productions Ltd. eine Gesellschaft, die bis zu seiner Übersiedlung nach Amerika ausschließlich dafür zuständig war, für ihn und mit seiner Person Öffentlichkeitsarbeit zu betreiben. Anschließend wurden diese Aufgaben von der Künstleragentur Selznick-Joyce, danach von der Music Corporation of America (MCA) wahrgenommen, wobei der Präsident der MCA, Lew Wasserman, zu seinem persönlichen Agenten wurde. 1962 wurde unter Herman Citron eine neue Gesellschaft gegründet, die Hitchcocks Interessen vertrat und seinen Namen vermarktete. Diese Selbstvermarktung diente auch dazu, eine Machtposition im Produktionsprozess seiner Filme zu erlangen, und war somit Teil seines Kampfes um künstlerische Unabhängigkeit.

Aus Mangel an Statisten in seinen ersten britischen Filmen sah man Hitchcock immer wieder im Hintergrund auftauchen. Daraus entwickelte er eines seiner bekanntesten Markenzeichen: Hitchcocks obligatorischer Cameo-Auftritt. Da das Publikum mit der Zeit immer weniger auf die Handlung achtete, als vielmehr auf Hitchcock lauerte, legte er in späteren Filmen diesen Running Gag möglichst weit an den Filmanfang.

In drei Filmen hatte Hitchcock keinen eigentlichen Cameo-Auftritt. In zwei von diesen Filmen trat er auf Fotos in Erscheinung: "Das Rettungsboot" spielt ausschließlich in einem kleinen Rettungsboot auf dem Meer. Er ist daher in einer zufällig im Boot liegenden Zeitung in einer Werbeanzeige für eine Diät auf einem „Vorher-Nachher-Foto“ zu sehen. Auch in "Bei Anruf Mord" war kein Auftritt möglich. Stattdessen taucht Hitchcock auf einem an der Wand hängenden Foto einer Wiedersehensfeier von College-Absolventen auf. In "Der falsche Mann" schließlich tritt er am Anfang des Films persönlich auf und spricht den Prolog. Dies ist gleichzeitig seine einzige Sprechrolle in einem seiner Kinofilme.

Während von den Filmgesellschaften üblicherweise für die Vermarktung eigene Abteilungen oder externe Agenturen beauftragt werden, trugen bei Hitchcocks Filmen die Werbekampagnen deutlich die Handschrift des Regisseurs. Seine Kino-Trailer waren häufig nicht nur Zusammenschnitte des angekündigten Films: Mit steigendem Wiedererkennungswert seiner Person stellte Hitchcock in der Rolle eines „Master of Ceremony“ seine eigenen Filme vor und führte den Zuschauer humorvoll durch die Kulissen.

Auf den Rat seines Agenten Lew Wasserman hin stieg Hitchcock 1955 in das Fernsehgeschäft ein. Hitchcock gründete die Fernsehproduktionsfirma "Shamley Productions" und produzierte bis 1965 seine eigene wöchentliche Fernsehserie. Am Anfang vieler Folgen begrüßte Hitchcock das Publikum, indem er mit ungerührter Miene makabre Ansagetexte sprach. Die Moderationen, die ihn zu einer nationalen Berühmtheit machten, wurden von dem Bühnenautor James D. Allardice verfasst, der fortan bis zu seinem Tod 1966 für Hitchcock auch als Redenschreiber arbeitete. Als Titelmusik für die Serie "Alfred Hitchcock Presents" verwendete Hitchcock das Hauptthema von Charles Gounods "Marche funèbre d’une marionette" (Trauermarsch einer Marionette), das sich im Weiteren zu einer Erkennungsmelodie für Hitchcocks Öffentlichkeitsarbeit entwickelte.

1956 schloss Hitchcock einen Lizenzvertrag mit HSD Publications ab, der die Überlassung seines Namens für das Krimi-Magazin "Alfred Hitchcock’s Mystery Magazine" zum Inhalt hatte. Die Zeitschrift enthält Mystery- und Kriminalgeschichten, Buchrezensionen und Rätsel und erscheint noch heute. Einführungen und Vorworte, die mit seinem Namen unterschrieben waren, wurden stets von Ghostwritern verfasst.

Von 1964 bis 1987 erschien in den USA die Jugend-Krimi-Reihe „The Three Investigators“, auf Deutsch seit 1968 "Die drei ???". Der Journalist und Autor Robert Arthur kannte Alfred Hitchcock persönlich und bat ihn, seinen Namen zur Vermarktung dieser geplanten Buchreihe verwenden zu dürfen. Schließlich baute er die Figur „Alfred Hitchcock“ in die Handlung ein. Anders als in Europa hielt sich der Erfolg der Bücher in den USA in Grenzen. In Deutschland, wo die Bücher besonders populär waren, entstand die gleichnamige Hörspielreihe. Durch diese bis heute erfolgreichste Hörspielproduktion der Welt wurde der Name Hitchcock auch bei vielen bekannt, die mit seinem filmischen Schaffen nicht vertraut waren.

Viele Elemente aus seinem Werk sind inzwischen in das Standardrepertoire des Kinos eingegangen, ohne dass sie noch bewusst oder direkt mit Hitchcock in Verbindung gebracht werden, insbesondere der Einsatz von Suspense als spannungserzeugendem Mittel oder die Verwendung von MacGuffins als handlungsvorantreibendes Element. Darüber hinaus gibt es seit den 1940er Jahren unzählige Beispiele für Thriller oder Dramen, teils von sehr namhaften Regisseuren, in denen typische Motive Hitchcocks oder seine Stilelemente bewusst kopiert oder variiert werden. Manche dieser Filme sind als Hommage des jeweiligen Regisseurs an Hitchcock zu verstehen, in anderen Fällen wurde Hitchcocks Stil übernommen, da er sich als erfolgreich und wirksam erwiesen hat.

Insbesondere Hitchcocks Erfolgsfilme aus den 1950er bis Anfang der 1960er Jahre inspirierten in den Folgejahren Hollywood-Produktionen, die inhaltlich oder stilistisch oft mit Hitchcock in Verbindung gebracht werden. 

Zu den vielen Hollywood-Regisseuren, die Alfred Hitchcock mehr oder weniger direkt beeinflusste, zählt Brian De Palma, der mit vielen Verweisen und Zitaten auf Hitchcocks Werk arbeitet. Überdies übernahm er in einigen Filmen Grundstrukturen aus dessen Filmen. So entwickelt er in "Dressed to Kill" (1980) das Grundmotiv aus "Psycho" weiter und zitiert aus weiteren Hitchcock-Filmen. 1976 lehnte sich "Schwarzer Engel" stark an "Vertigo" an. 1984 spielt de Palma in "Der Tod kommt zweimal" mit eindeutigen Bezügen auf "Das Fenster zum Hof" und "Vertigo".

Auch wenn Steven Spielberg selten direkt stilistische Motive kopiert oder adaptiert und nur wenige seiner Filme thematische Parallelen aufzeigen, erinnert "Der weiße Hai" (1975) in Spannungsaufbau und Dramaturgie an "Die Vögel" und ist die "Indiana-Jones-"Filmreihe (1981–1989) stark von "Der unsichtbare Dritte" (1959) beeinflusst. Auch ein Film wie "Schindlers Liste" (1993) wäre in dieser Form ohne den Einfluss Hitchcocks nicht möglich gewesen. Der von Hitchcocks Kameramann Irmin Roberts entwickelte Vertigo-Effekt wird bisweilen auch als „Jaws Effect“ bezeichnet, da Spielberg diese relativ schwierig umzusetzende Kameraeinstellung im "Weißen Hai" (Originaltitel: "Jaws") als einer der ersten prominenten Regisseure 16 Jahre nach "Vertigo" einsetzte. Inzwischen gehört dieser emotional sehr wirkungsvolle Kameratrick zum Standardrepertoire des Hollywood-Kinos.

Weitere amerikanische Regisseure, die erkennbar von Hitchcock beeinflusst wurden oder sich auf dessen Werk berufen, sind John Carpenter, David Fincher, David Mamet, Quentin Tarantino, Martin Scorsese, David Lynch und M. Night Shyamalan.

Bereits seit Mitte der 1950er Jahre war Hitchcock insbesondere in Frankreich bei den Vertretern der Nouvelle Vague hoch angesehen. 1957 veröffentlichen die damaligen Filmkritiker und späteren Regisseure Éric Rohmer und Claude Chabrol das erste Buch über ihn. 1956 erschien ein Sonderheft der "Cahiers du cinéma," das maßgeblich zu Hitchcocks Popularität in Frankreich beitrug. Als er im Mai 1960 zu einem Filmfestival reiste, das die Cinémathèque française ihm zu Ehren in Paris abhielt, wurde er von Dutzenden jungen Filmemachern frenetisch gefeiert. Die internationale Ausgabe der "Herald Tribune" schrieb, dass Hitchcock in dieser Woche „das Idol der französischen Avantgarde geworden“ sei.

Im August 1962 gab Hitchcock dem damals dreißigjährigen französischen Filmkritiker und Regisseur François Truffaut ein fünfzigstündiges Interview. Truffaut befragte Hitchcock chronologisch zu dessen bisherigen achtundvierzig Filmen. Das Interview erschien 1966 als "Mr. Hitchcock, wie haben Sie das gemacht?" in Buchform und gilt als Standardwerk der Filmliteratur. Einzelne Filme Truffauts zeigen den Einfluss Hitchcocks deutlich, etwa "Die Braut trug schwarz" (1968) oder "Das Geheimnis der falschen Braut" (1969), die Geschichte eines Mannes, der einer Betrügerin und Mörderin verfällt und auch nicht von ihr lassen kann, als sie ihn zu töten versucht. Der Film ist stark von verschiedenen inhaltlichen und stilistischen Motiven aus "Vertigo", "Marnie" und "Verdacht" beeinflusst. Sein letzter Film "Auf Liebe und Tod" (1983), in dem ein unschuldiger Mann eines Mordes beschuldigt wird, ist voll von hitchcockschen Motiven und Anspielungen auf dessen Werk. Weitere Filme, die Truffaut selbst in der Tradition Hitchcocks sah, waren "Die süße Haut" und "Fahrenheit 451". 1968/69 besetzte Hitchcock die bevorzugte Schauspielerin Truffauts, Claude Jade, für seinen Film "Topas".

In vielen Filmen von Claude Chabrol wird eine scheinbar heile bürgerliche Welt angegriffen und durcheinandergebracht. Die hitchcockschen Hauptmotive der Schuldübertragung sowie der doppelten oder der gespaltenen Persönlichkeit tauchen bei Chabrol immer wieder auf. Einige Beispiele sind "Schrei, wenn du kannst" (1959), "Das Auge des Bösen" (1962), "Der Schlachter" (1970) und "Masken" (1987). Neben Chabrol und Truffaut haben sich in Frankreich unter anderen auch Henri-Georges Clouzot und René Clément des hitchcockschen Repertoires bedient.

Außerhalb Frankreichs war in Europa der unmittelbare Einfluss Hitchcocks auf andere Filmemacher deutlich geringer. Einige europäische oder europäischstämmige Regisseure haben jedoch einzelne Filme gedreht, denen eine Stilverwandtschaft anzuerkennen ist oder die unmittelbar als Hommage an Hitchcock gedacht sind, zum Beispiel "Ministerium der Angst" von Fritz Lang (1943), "Der dritte Mann" von Carol Reed (1949), "Zeugin der Anklage" von Billy Wilder (1957), "Frantic" von Roman Polański (1988) und "Schatten der Vergangenheit" von Kenneth Branagh (1991).

Alle Filme, an denen Hitchcock beteiligt war, in der Reihenfolge ihrer Produktion:

In diese Phase fällt auch Hitchcocks einzige Mitarbeit an einem längeren Dokumentarfilm ("German Concentration Camps Factual Survey") von Mai bis Juli 1945 in London. Er hat dies später im Interview als seinen Beitrag zum Krieg bezeichnet. Der Film wurde nicht fertiggestellt.

Zwischen 1955 und 1965 trat der Regisseur in insgesamt 360 Folgen der Fernsehserien "Alfred Hitchcock Presents" (267 Folgen) und "The Alfred Hitchcock Hour" (93 Folgen) in der Rolle des Gastgebers auf.

Hitchcock wurde sechsmal für den Oscar nominiert: fünfmal für die Beste Regie, einmal für den Besten Film (als Produzent). Alle sechs Mal ging er leer aus, was ihn zu dem Kommentar veranlasste: „Immer nur Brautjungfer, nie die Braut“. Dennoch blieb er nicht oscarlos, denn 1968 gewann er den Irving G. Thalberg Memorial Award als Spezialoscar für besonders kreative Filmproduzenten. Zudem wurde Rebecca 1941 mit dem Oscar für den besten Film ausgezeichnet, den aber nicht Hitchcock, sondern der Produzent David O. Selznick entgegennehmen durfte.

Er wurde mit zwei Sternen auf dem Hollywood Walk of Fame geehrt. Den einen in der Kategorie Film findet man bei der Adresse 6506 Hollywood Blvd, den anderen in der Kategorie Fernsehen am 7013 Hollywood Blvd.

Regie: Julian Jarrold; Besetzung: Toby Jones (Alfred Hitchcock), Sienna Miller (Tippi Hedren), Imelda Staunton (Alma Reville Hitchcock), Conrad Kemp (Evan Hunter), Penelope Wilton (Peggy Robertson)
Regie: Sacha Gervasi; Besetzung: Anthony Hopkins (Alfred Hitchcock), Helen Mirren (Alma Reville Hitchcock), Scarlett Johansson (Janet Leigh), Danny Huston (Whitfield Cook), Toni Collette (Peggy Robertson), Michael Stuhlbarg (Lew Wasserman), Michael Wincott (Ed Gein), Jessica Biel (Vera Miles), James D’Arcy (Anthony Perkins)



Sortiert in der chronologischen Reihenfolge der jeweiligen Originalausgabe.



Hauptquellen sind die beiden Biografien von Taylor und Spoto sowie die Bücher von Truffaut und Krohn.



</doc>
<doc id="78" url="https://de.wikipedia.org/wiki?curid=78" title="Auteur-Theorie">
Auteur-Theorie

Die Auteur-Theorie (von frz. „Auteur“ = Autor) ist eine Filmtheorie und die theoretische Grundlage für den Autorenfilm – insbesondere den französischen – in den 1950er Jahren, der sich vom „Produzenten-Kino“ abgrenzte. Auch heute noch wird die Definition des "Auteur"-Begriffs ständig weiterentwickelt. Im Zentrum des Films steht für die Auteur-Theorie der Regisseur oder Filmemacher als geistiger Urheber und zentraler Gestalter des Kunstwerks.

Ende der 1940er Jahre wurde eine erste Auteur-Theorie von dem französischen Filmkritiker Alexandre Astruc formuliert, indem er die Frage nach dem geistigen Besitz eines Films aufwarf.
Im traditionellen Schaffensprozess lassen sich die Anteile von Drehbuchautor, Kameramann und Filmregisseur am Gesamtwerk nur schwer zuordnen. Durch die Zuteilung der Teilaufgaben als Honorartätigkeit durch die Filmgesellschaften leide die Kreativität, so die These. Im Umkehrschluss fordert diese Theorie die Zusammenführung der Tätigkeiten zu einer kreativen Einheit. Er formulierte seinen Entwurf in dem Aufsatz „"La caméra-stylo"“. Die Kamera sollte wie ein Stift verwendet werden. Er war sich sicher, dass bedeutende Schriften in Zukunft nicht mehr als Text, sondern mit der „Kamera geschrieben“ würden.

Doch durchgesetzt haben sich solche und ähnliche Ideen der Auteur-Theorie erst in den 1950er Jahren. Deren gängiger Begriff als Wegbereiter für die heutige Auteur-Theorie lautete zunächst "politique des auteurs" (Autoren-Politik), was erst im Laufe der Zeit zur "Theorie" umgeformt wurde. Das Wort "politique" bzw. Politik stand hier also eher für Parteilichkeit, welche für filmwissenschaftliche Diskussionen eher hinderlich ist (siehe unten).

Die "politique des auteurs" wurde zu dieser Zeit von einer Gruppe von jungen Filmkritikern um André Bazin entwickelt, die für die Filmzeitschrift "Cahiers du cinéma" schrieben. Eine wesentliche Rolle spielte dabei François Truffaut: Im Januar 1954 veröffentlichte er seinen Aufsehen erregenden Aufsatz "Eine gewisse Tendenz im französischen Film" ("Une certaine tendance du cinéma français"), in dem er sich mit scharfer Polemik gegen den etablierten französischen „Qualitätsfilm“ wandte. Bei diesem trat der Regisseur gegenüber dem Drehbuchautor und dem Autor der literarischen Vorlage oft in den Hintergrund. Truffaut plädierte dagegen für einen Film, bei dem Form und Inhalt vollständig vom Regisseur selbst als dem eigentlichen „auteur“ des Films bestimmt werden. Er fand das bei traditionell als Autoren ihrer Filme betrachteten europäischen Regisseuren wie Luis Buñuel, Jean Renoir und Roberto Rossellini, außerdem aber auch und vor allem bei Regisseuren wie Alfred Hitchcock, Howard Hawks, Fritz Lang und Vincente Minnelli, die (zum großen Teil als Vertragsregisseure) im Studiosystem Hollywoods arbeiteten, deren Filme aber trotzdem einen persönlichen Stil aufweisen.

Das Konzept des Regisseurs als "auteur" seiner Filme wurde für die Filmkritik der "Cahiers du cinéma" bestimmend, und damit für die Regisseure der Nouvelle Vague, die daraus hervorgingen, neben Truffaut etwa Jean-Luc Godard, Jacques Rivette oder Claude Chabrol – Filmemacher, die sich zur Umsetzung ihrer künstlerischen Ziele einer jeweils ganz eigenen filmischen Form bedienten.

Roland Barthes hingegen misst in seinem Essay "La mort de l'auteur" (1968, "Der Tod des Autors") dem Autor für die Literatur eine weitaus geringere Bedeutung bei, als es bisher der Fall war. Der „Auteur-Dieu“ („Autoren-Gott“) wird von Barthes durch den „écrivain“ (den Schriftsteller) ersetzt und folgt damit einer Kritik, die Julia Kristeva bereits 1967 in ihrem Aufsatz "Bakhtine, le mot, le dialogue et le roman" ("Bachtin, das Wort, der Dialog und der Roman", 1972) aufbrachte.

Für den europäischen Film blieb die Auteur-Theorie aber noch bis in die 1970er prägend. Danach setzte auch hier eine Abkehr von der „verhängnisvollen Macht der Regisseure“ (Günter Rohrbach) ein. Wirtschaftlicher Druck zwang zur Rückkehr zu einer arbeitsteiligen Produktionsweise, wie sie für den Produzenten-Film charakteristisch ist. Damit einher ging notwendigerweise auch wieder die Einigung aller Beteiligten auf einen kleinsten gemeinsamen Nenner und somit auch häufig eine gewisse Banalisierung der Filminhalte, die umso stärker zu Tage tritt, je weniger der Produzent als Projektverantwortlicher in den eigentlichen schöpferischen Prozess eingebunden ist.

In der Filmwissenschaft wurden auch immer neue Autorschaften von Teammitgliedern entdeckt. In der Realität ist Film Teamarbeit und es ist dem Film nicht anzusehen, ob zum Beispiel die Idee für eine Einstellung nun vom Regisseur oder vom Kameramann stammt. Im Dogma-Film ist der Kameramann nicht weisungsgebunden. Die „Polnische Schule“ bindet den Kameramann bereits in den Prozess des Drehbuchschreibens ein. Unerfahrene Regisseure sind meist sehr auf die Kreativität des Kameramanns oder der Kamerafrau und anderer Teammitglieder angewiesen.

Durch das Aufkommen digitaler Aufnahmetechniken wie Digital Video seit Ende der 1990er Jahre sehen viele Filmemacher, wie etwa Wim Wenders, wieder günstigere Bedingungen für individuelle, subjektive Produktionen gegeben.

Die von François Truffaut und Jean-Luc Godard proklamierte „politique des auteurs“ (Autorenpolitik) der fünfziger Jahre war ursprünglich ein Versuch, bestimmte Regisseure wie Alfred Hitchcock als Künstler anzuerkennen, die ihre völlig eigene Bildsprache entwickelten oder, wie Truffaut selber, sämtliche Aspekte ihrer Filme selbst bestimmten. Ein Autorenfilmer ist demnach ein Regisseur, der einen Film – möglichst ohne Kompromisse – so gestaltet, wie er ihn selbst haben möchte.

Die „politique des auteurs“ geriet schnell in die Kritik. Kritiker wie Andrew Sarris und Peter Wollen wiesen auf ein empirisches Problem hin: Niemand kann beweisen, wie viel Einfluss der Regisseur wirklich auf seine Filme hatte bzw. welchen Einfluss Form und Inhalt wirklich auf das haben, was wir als Autorschaft wahrnehmen.

Als Beispiel hierfür gilt der Vorspann von "Vertigo – Aus dem Reich der Toten" (1958), den Alfred Hitchcock nicht selbst angefertigt hat, oder die Tatsache, dass viele seiner Filme auf einer Buchvorlage fremder Autoren basieren und selbst die Drehbücher selten von ihm selbst stammten. Gerade Hitchcock aber ist eine zentrale Figur in der „politique des auteurs“.

Wie der Name „politique des auteurs“ sagt, handelte es sich um eine Politik, einen gezielten polemischen Eingriff. Der Village-Voice-Kritiker Andrew Sarris übersetzte „politique des auteurs“ jedoch 1962 mit „auteur theory“, wobei unklar blieb, in welchem Sinne es sich hier tatsächlich um eine "Theorie" handelt. Sarris popularisierte diese „Theorie“ im englischen Sprachraum und benutzte sie vor allem, um die absolute Überlegenheit des Hollywood-Kinos darzulegen, war er doch davon überzeugt, es sei „the only cinema in the world worth exploring in depth beneath the frosting of a few great directors at the top“. Nun war die Frage: Wo ist die Grenze? "Wen" oder vielmehr "was" nehmen wir als Autor wahr?

Soziologisch gesehen war die Autorentheorie eine Distinktionsstrategie junger Kritiker, die auf sich aufmerksam machen wollten. Godard hat dies später offen zugegeben: „Wir sagten von Preminger und den anderen Regisseuren, die für Studios arbeiteten, wie man heute fürs Fernsehen arbeitet: ‚Sie sind Lohnempfänger, aber gleichzeitig mehr als das, denn sie haben Talent, einige sogar Genie …‘, aber das war total falsch. Wir haben das gesagt, weil wir es glaubten, aber in Wirklichkeit steckt dahinter, dass wir auf uns aufmerksam machen wollten, weil niemand auf uns hörte. Die Türen waren zu. Deshalb mussten wir sagen: Hitchcock ist ein größeres Genie als Chateaubriand.“

In den siebziger Jahren folgte dann die stärkste Kritik an der „politique des auteurs“. Roland Barthes proklamierte bereits 1968 vor einem poststrukturalistischen Hintergrund den „Tod des Autors“. Der Autor wurde nun aufgrund des empirischen Dilemmas der Beweisbarkeit von Autorschaften als Image-Figur erkannt, die sich aus ihrer Umwelt formt und in die Werke einschreibt. Auch von feministischer Seite wurde die „politique des auteurs“ scharf angegriffen, diene sie doch dazu, den kollektiven Charakter des Filmemachens zu verdecken und in der Tradition patriarchaler Heldenverehrung Männer zu Superstars zu stilisieren. Claire Johnston verteidigte den Ansatz insofern, als dieser einer zu monolithischen Sicht des Hollywood-Kinos entgegenwirke.

In den neunziger Jahren schließlich ging die Tendenz zu der Annahme, dass Autorschaften zum Großteil (z. T. kommerziell) konstruiert sind. Timothy Corrigan nennt dies den „commercial auteur“. Es wird damit gerechnet, dass das Publikum den Film eines als Autor bekannten Regisseurs als z. B. „Der neue Woody Allen!“ wahrnimmt, ohne wirklich zu wissen, wie viel Einfluss Woody Allen tatsächlich auf den Film hatte.
Dana Polan verfolgte einen weiteren interessanten Ansatz: Er sieht den „auteurist“ als Hauptverantwortlichen für konstruierte Autorenbilder. Das sind Kritiker, die den Autor als höchste Instanz suchen und damit – wie François Truffaut – auf einen Filmemacher als Künstler hinweisen wollen und nebenbei ihre eigene Erkenntniskraft zelebrieren. Der Begriff dafür lautet „Auteur Desire“. Dieser Ansatz zeigt noch einmal den größten Vorwurf gegenüber der „politique des auteurs“ auf. Doch trotzdem ist die Nennung eines Regisseurs parallel zu – beispielsweise – einem Buchautor als Schöpfergeist auch unter reflektierenden Filmkritikern und -wissenschaftlern weiterhin außerordentlich beliebt. Steckt also doch mehr dahinter?

Ein neuerer Ansatz, die kontextorientierte Werkanalyse von Jan Distelmeyer, versucht diese Frage zu klären. Als Grundlage dienen Publikums- und Kritikerrezeption auf der einen Seite und die Konstruktion des Autors aus Biografie, Filmindustrie und kulturellem Umfeld auf der anderen Seite.
Diese zweiseitige Annäherung erkennt das empirische Dilemma der Definition von „auteur“ an und maßt sich auch keine Bestimmung dessen an, was jetzt eigentlich das Werk von Autor XYZ ist. Viele andere Filmtheoretiker verfolgen heutzutage ähnliche Konzepte. Doch auch eine solch freie Handhabung kann das Problem nicht vollständig lösen, da die wichtigsten Elemente variabel sind und sich so einer eindeutigen Aussage verschließen.

Der Schwerpunkt kritischer Tendenzen liegt also zum Großteil in der Empirie. Einen Filmemacher als „auteur“ anzuerkennen fordert uneingeschränktes Vertrauen in seine Aussagen, wie viel Einfluss er auf seine eigenen Filme hatte. Da dies in Zeiten einer sehr starken Vermarktung aller möglichen mehr oder weniger (un)abhängigen Regisseure seitens von Filmindustrie und Verleih ein fast aussichtsloses Unterfangen ist, ist ein Restzweifel und das stete Hinterfragen der „auteur“-Definition angebracht.



</doc>
<doc id="79" url="https://de.wikipedia.org/wiki?curid=79" title="Aki Kaurismäki">
Aki Kaurismäki

Aki Olavi Kaurismäki (* 4. April 1957 in Orimattila) ist ein vielfach preisgekrönter finnischer Filmregisseur.

Aki Kaurismäki studierte an der Universität Tampere Literatur- und Kommunikationswissenschaften. Neben diversen Aushilfsjobs, etwa als Briefträger oder in der Gastronomie, war er Herausgeber eines universitären Filmmagazins. Darüber hinaus schrieb er von 1979 bis 1984 Filmkritiken für das Magazin "Filmihullu". Das erste Drehbuch folgte 1980 für den mittellangen Film "Der Lügner (Valehtelija)", bei dem sein Bruder Mika Regie führte.

Kaurismäkis Filme thematisieren häufig Schicksale von gesellschaftlichen Außenseitern in städtischen Zentren wie Helsinki. Sie sind nicht nur für ihre sparsamen Dialoge, sondern auch für einen skurril-lakonischen Humor bekannt. Kaurismäki arbeitet regelmäßig mit einem festen Stamm befreundeter Schauspieler und Musiker, die seine Filme auch stilistisch geprägt haben: Matti Pellonpää, Kati Outinen, Kari Väänänen und Sakke Järvenpää. Als Reminiszenz an Alfred Hitchcock hat er in seinen Filmen gelegentlich Cameo-Auftritte, was auch Hitchcock zu tun pflegte.

In Deutschland wurden seine Filme zum ersten Mal 1986 auf dem Filmfestival "Grenzland-Filmtage" in Selb gezeigt. Aki Kaurismäki führte dabei die Filme "Der Lügner, Calamari Union" und "Crime and Punishment" persönlich vor. Während des Festivals schrieb er das Drehbuch für seinen Film "Schatten im Paradies," den er 1988 erneut persönlich bei den Grenzland-Filmtagen in Selb präsentierte. Dieser Film brachte ihm den internationalen Durchbruch. Ein Großteil der Filmmusik kam von der Band "Nardis" aus Erlangen, die Kaurismäki 1986 auf den Grenzland-Filmtagen kennengelernt hatte.
Dem breiten deutschen Publikum bekannt wurde der finnische Regisseur durch seine Teilnahme an der Berlinale 1988. Für großes Aufsehen sorgte Kaurismäki im Herbst 2006, als er sich weigerte, seinen Film "Lichter der Vorstadt" als offiziellen finnischen Beitrag für eine Oscar-Nominierung in der Kategorie Bester fremdsprachiger Film zuzulassen, obwohl das Drama von der finnischen Filmkammer einstimmig ausgewählt worden war. Kaurismäki begründete seine Ablehnung mit seiner seit Jahren vertretenen kritischen Haltung gegen den Irak-Krieg der USA.

Zusammen mit seinem Bruder Mika Kaurismäki gründete er das Midnight Sun Film Festival im lappischen Sodankylä sowie die Verleihfirma Villealfa. Der Name geht zurück auf die Figur Ville Alfa, den Protagonisten im Film "Der Lügner". Gleichzeitig handelt es sich um ein Anagramm von "Alphaville," einem Film von Jean-Luc Godard.

1989 emigrierte Kaurismäki mit seiner Frau nach Portugal, weil „es in ganz Helsinki keinen Platz mehr gebe, wo er seine Kamera noch postieren könne“.

Rainer Gansera, der für die Zeitschrift epd Film mit dem „Chef-Melancholiker des europäischen Autorenkinos“ 2006 in Hof gesprochen hat, zeigte sich auch von seinem Auftreten persönlich beeindruckt und beschrieb atmosphärisch:

Als persönliche Leitbilder will Kaurismäki Bresson, Ozu und Godard gesehen haben, der Ausbildung an den Filmhochschulen seines Landes dagegen habe er nicht viel Positives abgewinnen können.
Bei "Pandora" sind Ende 2006 als „Aki Kaurismäki DVD-Collection“ 14 Spielfilme und fünf Kurzfilme (mit digital restaurierten Bildern) in vier Boxen erschienen.

2011 stellte Kaurismäki nach fünf Jahren mit "Le Havre" einen Spielfilm fertig, der ihm wieder einmal eine Einladung in den Wettbewerb der Filmfestspiele von Cannes einbrachte. Der in Frankreich gedrehte Film handelt von einem Schuhputzer aus der gleichnamigen Hafenstadt, der sich eines illegalen Flüchtlingskindes aus Afrika annimmt. "Le Havre" gewann in Cannes den FIPRESCI-Preis.

Für den Spielfilm "Die andere Seite der Hoffnung" erhielt Kaurismäki 2017 eine Einladung in den Wettbewerb der 67. Internationalen Filmfestspiele Berlin. Der Film spielt in Helsinki und erzählt von der Begegnung eines älteren finnischen Handelsvertreters (dargestellt von Sakari Kuosmanen) mit einem jungen syrischen Flüchtling (Sherwan Haji). Der Film kam am 3. Februar 2017 in die finnischen Kinos.







</doc>
<doc id="81" url="https://de.wikipedia.org/wiki?curid=81" title="Anime">
Anime

Anime (jap. , [], im Deutschen häufig []), (Plural: "Animes") ist eine Verkürzung des japanischen Lehnwortes "animēshon" (, von ) und bezeichnet in Japan produzierte Zeichentrickfilme. In Japan selbst steht "Anime" für alle Arten von Animationsfilmen, für die im eigenen Land produzierten ebenso wie für importierte. Er bildet das Pendant zum Manga, dem japanischen Comic. Japan besitzt die umfangreichste Trickfilmkultur weltweit.

Animes decken ein breitgefächertes Themenspektrum für alle Altersstufen ab. Von Literaturverfilmungen (z. B. "Das Tagebuch der Anne Frank" oder "Heidi") über Horror bis hin zu Science Fiction werden nahezu alle Bereiche und Altersklassen abgedeckt. Auch gibt es Genres bei Anime, die ausschließlich in diesen und Mangas vorkommen (z. B. Mecha-Serien über überdimensional große Roboter). In Japan können für Kinder und Jugendliche geeignete Produktionen ernsthafte Themen haben oder realistischere Gewaltdarstellungen enthalten, als das in westlichen Produktionen der Fall ist. Aber auch sexuelle Anspielungen, bei denen sich die Charaktere zuweilen sehr freizügig geben, werden geduldet. Dies führt dazu, dass sie in anderen Ländern vor der Ausstrahlung bearbeitet werden.

Pornographische Animes, sogenannte Hentai, machen nur einen kleinen Teil des japanischen Kaufvideo-Marktes aus; im Fernsehen und im Kino werden diese in Japan überhaupt nicht gezeigt. Viele Animes beinhalten jedoch erotische Ansätze, ohne dem Hentai-Genre zugeordnet werden zu können, insbesondere die des Genres Etchi.

Anime- und Manga-spezifische Genre:

Anime – dort auch unter den veraltenden Bezeichnungen "dōga" (, „bewegte Bilder“) und "manga eiga" (, „Manga-Film“) bekannt – sind ein fester Bestandteil des japanischen Kulturgutes. Zu den erfolgreichsten Kinofilmen in Japan zählen viele Animes, so "Prinzessin Mononoke", "Pokémon: Der Film" und "Chihiros Reise ins Zauberland". Nach einer Umfrage sind die 100 beliebtesten Zeichentrickserien in Japan alle Anime, mit Ausnahme von "Tom und Jerry". Zudem ist die Unterhaltungsindustrie in Japan, die Animes wie Mangas produziert, mit 80 Milliarden Euro Umsatz im Jahr wirtschaftlich bedeutend. Pro Jahr kommen bis zu 200 neue Serien auf den Markt.

Neben Fernsehserien und Kinofilmen werden Animes seit den frühen 1980er Jahren als Original Video Animation, kurz "OVA", für den Kaufvideo- und DVD-Markt produziert. Die Zielgruppe sind meist junge Erwachsene, daher sind die Inhalte in der Regel mit viel Fanservice versehen. Diese wurden aber weitestgehend durch Mitternachtsanime ersetzt. Seit 2000 gibt es auch Serien direkt für das Internet, Original Net Animation (ONA) genannt.

Anime-Fernsehserien haben für gewöhnlich 12–13, 24–26, sowie seltener 52 oder mehr Folgen, so dass bei wöchentlicher Ausstrahlung eine Laufzeit von einem viertel, halben oder ganzen Jahr erreicht wird. Ein solches Vierteljahresintervall wird als "cours" (, "kūru") bezeichnet. Die "cours" sind dabei saisonal, d. h., es gibt Winter-, Frühlings-, Sommer- und Herbst-Cours, die im Januar, April, Juli bzw. Oktober beginnen. Die meisten Anime-Serien sind nicht als Endlosserien ausgelegt, obwohl insbesondere Verfilmungen langer Manga-Serien auf weit mehr als 100 Folgen kommen können.

Im Jahr 1963 wurden sieben Serien gesendet, dies wird generell als der Beginn von Anime-TV-Serien angesehen. 1978 wurde die 50er-Grenze mit 52 Serien gebrochen. 1998 wurde die 100er-Grenze mit 132 Serien erreicht. Mit 233 Serien wurde die 200er-Grenze im Jahr 2004 erreicht. Seitdem hat sich die Anzahl der Serien mehr oder weniger etabliert, jedoch gab es Jahre wie 2006 und 2014, in denen die 300er Grenze erreicht wurde.

Der Anstieg der Anime-Anzahl in den 90ern ist darauf zurückzuführen, dass seit 1996 die Mitternachtsprogrammplätze für Anime verwendet werden; aber auch darauf, dass durch den großen Erfolg (und die Kontroverse) von "Neon Genesis Evangelion" immer mehr Studios, Videounternehmen und Verlage Werke produzieren ließen. Diese schließen sich dann oft mit Merchandising-Partnern zu Produktionskomitees (, "seisaku iinkai") zusammen und kaufen einen Mitternachtsprogrammplatz – daher auch als Mitternachtsanimes (, "shin’ya anime") bezeichnet – bei mehreren Sendern, üblicherweise für ein bis zwei "cours". Der größte Teil dieser Programmierungen geschieht auf Regionalsendern, die keinem der großen Networks angeschlossen sind. Da diese auf UHF-Band ausstrahlen, werden derartige Anime auch UHF-Anime (UHF) genannt. Mitternachtsanimes erreichen durchschnittliche Einschaltquoten von etwa 2 %, während 4 bis 5 % schon außergewöhnlich hoch sind. Einschaltquoten spielen bei Mitternachtsanimes und damit den meisten Animes seit den späten 90ern kaum eine Rolle, sondern die Ausstrahlung dient der Werbung für die DVD- oder Blu-Ray-Veröffentlichungen, mit denen und den Merchandise-Artikeln der Gewinn gemacht wird. Abhängig von deren Verkaufszahlen entscheidet sich dann, ob weitere Staffeln produziert werden. Viele der Anime, die ein bestehendes Werk adaptieren, dienen letztendlich aber auch der Bewerbung der Vorlage, so dass für das auftraggebende Produktionsunternehmen auch die Animeverkäufe zweitrangig sein können, sofern die Vorlagenverkäufe anziehen, was sich daran äußert, dass teilweise auch nur wenig erfolgreiche Anime Fortsetzungen bekommen können.

Anders sieht dies bei am Tage ausgestrahlten Animes aus, die meist langläufig sind (über zwei "cours") und sich zudem auch entweder an ein junges oder ein Familienpublikum richten. Seit der Einführung der Mitternachtsanime hat sich die Anzahl der Serien mit hohen Einschaltquoten verringert, und auch die Art der Serien im Tagesprogramm hat sich verändert.

Anime mit den höchsten Einschaltquoten:

Anime mit den höchsten Heimvideoverkäufen:

Durch die sich erholende Wirtschaft während der 1990er Jahre, die starke Berichterstattung über die steigende Beliebtheit von Animes im Ausland und den „Moe-Boom“ investierten aber auch branchenfremde Unternehmen wie Finanz- und neue IT-Unternehmen in diesen früheren Nischenmarkt. Der Rückgang seit 2006 wird auf die sinkenden Geburtenraten und die wirtschaftliche Rezession zurückgeführt.

Japanische Fernsehsender gehen aber auch dazu über, den ausländischen Markt direkt zu beliefern. In den USA wird der Rückgang der Marktgröße für Animes von 4,8 Mrd. Dollar im Jahr 2003 auf 2,8 Mrd. Dollar für 2007 hauptsächlich mit der Fansubbing-Szene in Zusammenhang gesetzt, die Serien bereits kurz nach deren Erstausstrahlung im japanischen Fernsehen untertitelt über Filesharing verbreitet. Im Januar 2009 begann TV Tokyo als erster größerer Fernsehsender, seine Animes nur Stunden nach deren Ausstrahlung im japanischen Fernsehen englisch untertitelt auf einer abopflichtigen Website zu veröffentlichen. Heute wird ein großer Teil der Neuerscheinungen gleichzeitig zur japanischen Ausstrahlung (Simulcast) auf Websites mit englischen (Funimation und Crunchyroll), aber auch deutschen (siehe Abschnitt Anime auf Videoportalen) Untertiteln gestreamt.

Viele Animes beruhen auf erfolgreichen Mangas, sowie, vor allem in jüngerer Zeit, auf Light Novels. Es wird aber auch aufgrund eines erfolgreichen Animes ein entsprechender Manga gezeichnet. Vergleichsweise selten sind „Anime-Comics“ bei denen der Manga nicht neu gezeichnet, sondern aus Einzelbildern des Animes und eingefügten Sprechblasen zusammengesetzt wird.

Oft ist auch die Computerspiel-Industrie an der Anime-Produktion beteiligt, die auf Grundlage der Animes Computer- und Konsolenspiele produziert. Da den Produktionskomitees Unternehmen unterschiedlicher Branchen angehören können (neben Buch-, Spieleverlagen, Studios, auch Lebensmittelfirmen, die Kapital einbringen und sich die Rechte am Werk aufteilen), können zum Anime zeitgleich auch Manga, Romane und weitere Artikel erscheinen. Teilweise werden diese Franchises dann gezielt zur Werbung für ein Produkt oder eine Produktgruppe eingesetzt.

Wie in Kinofilmen wird im Anime die Musik als wichtiges künstlerisches Mittel benutzt. Mit Anime-Soundtracks wird in Japan sehr viel Geld gemacht, da diese sich häufig ebenso gut verkaufen wie Chartstürmer-Alben. Aus diesem Grund wird Animemusik häufig von erstklassigen Musikern komponiert und aufgeführt. Fähige Komponisten für die Hintergrundmusik sind bei den Fans hochangesehen. Zu den bekannteren Komponisten zählen z. B. Joe Hisaishi, Yuki Kajiura, Yōko Kanno und Kenji Kawai.

Am häufigsten wird Musik in Animes als Thema für einen Charakter genutzt, oder um als Hintergrundmusik die Stimmung einer Szene wiederzugeben. Serien haben ein Vorspannlied als Einleitung. Dieses Thema passt für gewöhnlich zum Gesamtton der Sendung und dient dazu, den Zuschauer für das anschließende Programm zu begeistern. Zwischen- und Abspannlieder kommentieren oft die Handlung oder die Sendung als Ganzes und dienen häufig dazu, eine besonders wichtige Szene hervorzuheben. Diese Lieder werden häufig von bekannten Musikern oder japanischen Idolen gesungen, aber auch von den Synchronsprechern (Seiyū), die dadurch wiederum zu Idolen werden. Somit sind sie ein sehr wichtiger Bestandteil des Programms.

Zusätzlich zu diesen Musikthemen veröffentlichen die Sprecher eines bestimmten Animes auch CDs für ihren Charakter, "Image Album" genannt. Trotz dem Wort "image" beinhalten sie nur Musik und/oder Textpassagen, in denen der Sprecher zum Hörer spricht oder über sich singt, wodurch der Zuhörer glaubt, dass der Charakter selber singt oder redet. Eine weitere Variante von Anime-CD-Veröffentlichungen sind "Drama-CDs": Hörspiele, in denen die Sprecher eine Geschichte erzählen, die häufig im Anime nicht vorkommt.

Eines der bekanntesten japanischen Anime-Studios ist Studio Ghibli, das seit 1985 unter der Leitung von Hayao Miyazaki Filme produziert, z. B. "Prinzessin Mononoke" 1997, "Chihiros Reise ins Zauberland" 2001 oder "Das wandelnde Schloss" 2004. Seinen bisher größten weltweiten Erfolg hatte Studio Ghibli mit "Chihiros Reise ins Zauberland". Der Film erhielt neben zahlreichen internationalen Zuschauer- und Kritikerpreisen im Jahr 2002 den Goldenen Bären auf der Berlinale und im Jahr 2003 den Oscar als bester Animationsfilm, was ihn zum meistausgezeichneten Zeichentrickfilm aller Zeiten macht.

Im Serienbereich sind Toei Animation bedeutend, da sie mit Fernsehserien von frühen Science-Fiction Klassikern wie "Uchū Senkan Yamato" (alias "Space Battleship Yamato") und "Captain Future", und etwas später "Sailor Moon" und "Dragon Ball", für die internationale Verbreitung von Anime verantwortlich waren.

Weitere bekannte Anime-Studios:

Laut einer im Jahr 2013 durchgeführten Studie arbeiten japanische Anime-Zeichner im Durchschnitt 10 bis 11 Stunden pro Arbeitstag bzw. 263 Stunden pro Monat bzw. 4,6 freie Tage/Monat. Animatoren verdienen pro Jahr durchschnittlich (Mittelwert) 3,3 Millionen Yen (ca. 23.000 €) bzw. am häufigsten (Modalwert) 4,0 Mio. Yen (28.000 €), angefangen bei Einstiegspositionen wie Zwischenzeichnern mit 1,1 Mio. Yen (8000 €) über Schlüsselzeichner mit 2,8 Mio. Yen (20.000 €) und Storyboarder/3D-Animatoren mit 3,8 Mio. Yen (26.000 €) bis zu Regisseuren mit 6,5 Mio. Yen (45.000 €). Zeichner werden häufig nach einem Schema bezahlt, bei dem sie zusätzlich zu einem festen Lohn noch nach fertiggestellten Einzelbildern bzw. -szenen bezahlt werden.

Als erster Anime in Deutschland wurde ab dem 16. März 1961 der Film "Der Zauberer und die Banditen" von Toei Animation aus dem Jahr 1959 in den Kinos gezeigt. Seither sind im deutschen Kino mehrere Anime-Filme gezeigt worden, darunter "Akira" (1991), "Ghost in the Shell" (1997), "Perfect Blue" (2000) und einige Produktionen von Studio Ghibli wie "Prinzessin Mononoke" (2001) und "Chihiros Reise ins Zauberland" (2003). Die bisher höchsten Zuschauerzahlen hatten die drei im Kino gezeigten Filme zur "Pokémon"-Serie.

Seit 2016, finden vermehrt Event Kino Vorstellungen statt, beispielsweise in Rahmen des "AkibaPass" Festivals oder der "Kaze Anime Night".

Die erste Anime-Serie im deutschen Fernsehen war "Speed Racer" (Tatsunoko Productions, 1967), die ARD zeigte von November 1971 bis Dezember 1971 aber nur drei von ursprünglich acht geplanten Folgen. Nach Protesten von Eltern, Pädagogen und Medien wurde die bereits angekündigte vierte Folge kurzfristig gestrichen. Im Frühjahr 1973 wurden zwei weitere Folgen ohne Vorankündigung als Ersatz für ausgefallene Asterix-Sendungen ins Programm genommen, danach wurde die Serie nach erneuten Protesten vollständig abgesetzt.

Sowohl die ARD als auch das ZDF hatten die in Italien und Frankreich erfolgreiche Serie "UFO Robot Grendizer" ("Goldorak") wegen zu hoher Brutalität abgelehnt.

Mit Ausnahme von "Captain Future", gegen das es ebenfalls zahlreiche Proteste von Eltern gab und vor dem in den 1980er-Jahren sogar in einigen Schulbüchern gewarnt wurde, umfassten Animes im öffentlichen-rechtlichen Fernsehen nur Serien für jüngere Kinder. Auch waren diese teilweise Koproduktionen, wie "Wickie und die starken Männer," "Die Biene Maja" und "Nils Holgerson". Dabei wurde stets darauf geachtet, dass die Serien im zeichnerischen Stil wie in der Handlung dem westlichen Empfinden und gesellschaftlichen Vorstellungen entsprachen. Die Programme waren dennoch Gegenstand teils heftiger Kritik, die Sendungen würden den Geschmack des jungen Publikums verderben. Der Verantwortliche wurde nach Erstausstrahlung der "Biene Maja" als „Insekten-Jupp“ bezeichnet oder gar kriminell genannt.

Mit dem Aufkommen des Privatfernsehens in den späten 80ern und 90ern wurde eine Vielzahl von Anime-Serien im deutschen Fernsehen ausgestrahlt. Jedoch waren die meisten dieser Serienlizenzen keine bewussten Einkäufe seitens der Fernsehsender, sondern kamen durch italienische/europäische Programmpakete, in denen neben westlichen Zeichentrickserien auch vereinzelt Anime enthalten waren, auf den deutschen Markt. Viele davon sind in anderen europäischen Ländern und Japan schon weit vor der deutschen Erstausstrahlung gezeigt worden.

Folgend eine Liste alter Serien im deutschen Fernsehen vor dem "Moon-Toon-Zone"- und "Anime@RTL2"-Blocks auf RTL2.
Im August 1999 sind Animes durch den Programmblock "Moon Toon Zone" von RTL2 ein durchschlagender Erfolg geworden. Dieser Block bestand aus "Sailor Moon, Dragon Ball" und "Pokemon". Er wurde mit "Anime@RTL2" ab 2001 und "PokitoTV" im Jahr 2004 ausgebaut. Durch den Erfolg der RTL-2-Ausstrahlungen begann das bewusste Lizenzieren von Anime durch RTL2 und andere TV-Sender. Der Pay-TV-Sender K-Toon, MTV, VIVA und VOX sendeten Animes für ein älteres Publikum. Die von VOX gezeigten Animes stammten nicht vom Sender selbst, sondern wurden von Fremdanbietern den an dctp verkauften Programmblöcken zur Verfügung gestellt. Ebenso gab es wieder vereinzelte Serien in den Zeichentrickprogrammen der ProSiebenSat.1-Gruppe.

Seit 2007 hat RTL2 nur drei neue Anime-Serien ins Programm genommen, bei denen es sich nicht um Fortsetzungen handelte. 2013 wurde das Programm bei RTL2 vollständig abgesetzt. Von 2007 bis Juni 2016 gab es mit Animax Deutschland ein eigener PayTV-Sender für den deutschsprachigen Raum. Animax stellte am 30. Juni 2016 die lineare Verbreitung im deutschsprachigen Raum ein und firmierte sich zum 1. Juli 2016 in ein reines Video-on-Demand-Angebot um. Heute senden nur noch ProSieben MAXX (seit 2013) und Nickelodeon regelmäßig Animes.

Erste Nachbearbeitungen von Anime-Serien gab es in Deutschland mit der Ausstrahlung von "Captain Future" beim ZDF. Bis ca. 2001 wurden italienische ("Mila Superstar") oder französische ("Dragon Ball") Fassungen übernommen. Amerikanische Fassungen ("Saber Rider und die Starsheriffs", "Samurai Pizza Cats", "Pokemon") werden seit Anbeginn und bis heute international vermarktet. In Deutschland gibt es seit 1993 die FSF (Freiwillige Selbstkontrolle Fernsehen), die Fernsehinhalte dahingehend prüft, ob sie für das Tagesprogramm geeignet sind und eventuell Schnittauflagen nach deutschem Jugendschutz erteilt. RTL2 hat mehrere FSK-12-Inhalte (z. B. Folgen in "Dragon Ball", "Lady Oscar", "Ein Supertrio") während des Kinderprogramms unbearbeitet am Nachmittag gezeigt. Es ist nicht bekannt, ob RTL2 damals die Inhalte von der FSF hat prüfen lassen. Laut eines dementierten Interviews im Jahre 2005 gab es keine Beschwerden bezüglich Gewaltdarstellungen von Serien im RTL2-Programm, jedoch subjektive Ansichten. RTL2 nahm zunehmend Schnitte vor, die nicht dem Muster der FSF entsprachen. Negativ aufgefallen ist RTL2 vor allem durch den Schnitt von Folgen, die auf Heimvideo FSK-0 oder FSK-6 bekamen. Bei "Naruto" und "Dragon Ball GT" gab es nicht nur Schnitte, sondern auch Dialogverharmlosungen und weitere Bearbeitungen. Die von Fans oft kritisierte Fassung von Naruto wird von RTL2 wegen ihrer hinter den Erwartungen gebliebenen Quoten als Grund genannt, dass das „Anime-Fieber“ im TV vorbei sei. Von der FSF geprüfte Ausstrahlungen von Tele 5 und ProSieben Maxx zeigten, das RTL2 nicht nach Muster der FSF arbeitete.

Die ersten deutschen Kauf-Animes gab es im Jahr 1975 auf sogenannten TED-Bildplatten (Abkürzung von '), analogen Vinyl-Bildplatten, die eine Spieldauer von ca. 10 Minuten pro Seite hatten und nur von einem einzigen Abspielgerät der Firma Telefunken gelesen werden konnten. Sie verschwanden bereits im folgenden Jahr wieder vom Markt. Bei den darauf angebotenen Anime handelte es sich um einzelne Folgen der Serien ', "Hotte Hummel" und '. Der erste Anime-Spielfilm, den es in Deutschland zu kaufen gab, war der Film "Perix der Kater und die 3 Mausketiere" (jap. , ') von Toei Animation aus dem Jahr 1969, der Ende der 1970er-Jahre von der Firma "piccolo film" stark gekürzt auf Super-8-mm-Film angeboten wurde. In der DDR lief er als "Der gestiefelte Kater" im Fernsehen und Kino.

In den 1980er-Jahren erschienen zahlreiche Animes auf VHS-Kassetten, wie im Fernsehen kamen die Veröffentlichungen nicht durch bewusstes lizenzieren japanischer Lizenzgeber, sondern waren französischer, amerikanischer und italienischer Abstammung. Die ersten VHS-Kassetten, mit denen gezielt Fans japanischer Animationen angesprochen werden sollten und bei denen ausdrücklich Japan als Produktionsland genannt wurde, stammten aus dem Jahr 1986. Damals wurden unter dem Label „“ einzelne Folgen der Serien "Die Abenteuer der Honigbiene Hutch", "Demetan der Froschjunge" und "Macross" veröffentlicht, wobei darauf geachtet wurde, das Originalmaterial möglichst unverändert zu lassen – der japanische Vor- und Abspann blieben erhalten, und vorkommende Songs wurden japanisch belassen. „“ verschwand jedoch bereits 1987 wieder vom Markt.

Liste der Animevideos, vor dem ersten Animelabel, OVA Films:

Der erfolgreiche Versuch eines eigenen deutschen Anime-Labels begann mit OVA Films. OVA Films veröffentlichte 1995 den Film ', der in einer offiziellen Auflage von 2500 VHS-Kassetten erschien und zugleich der erste deutsche Kauf-Anime im japanischen Original mit deutschen Untertiteln war. In der Folgezeit wurden immer mehr Anime-Kaufvideos von OVA Films veröffentlicht (z. B. ', ', '). Das dabei auftretende Problem, dass die Fans Originalfassungen mit Untertiteln bevorzugten, während für den Massenmarkt eher synchronisierte Fassungen erforderlich waren, löste sich mit dem Aufkommen der DVD, auf der beide Formate gleichzeitig angeboten werden konnten. OVA Films schlossen sich immer mehr Labels an, wie Anime Virtual (jetzt "Kazé Deutschland"), Anime House, Panini Video, ADV Films, Beez Entertainment, Universum Anime, Tokyopop und Red Planet. Jedoch sind diese Unternehmen oftmals, mit Ausnahme von Universum Anime, Kazé und Anime House, später insolvent gegangen. Danach neu hinzugekommene Labels sind Nipponart, Peppermint Anime, KSM Anime, Universal Pictures, FilmConfect Anime und AniMoon.

Im September 2007 startete Kazé Deutschland das Video-on-Demand-Portal "Anime on Demand".

Von 2011 bis 2016 bot das Video-Portal MyVideo auch offiziell Anime-Serien und -Filme an. Das Angebot umfasste überwiegend ältere Titel sowie kürzlich bei ProSieben Maxx ausgestrahlte TV-Folgen.

Im April 2013 startete der Anime-Publisher "Peppermint Anime" auf der Videoplattform Vimeo den Kanal "Peppermint TV", auf dem die Serie "Valvrave The Librator" als deutscher Simulcast angeboten wurde, d. h., die Folgen waren direkt nach ihrer Erstausstrahlung in Japan mit deutschen Untertiteln verfügbar. Seit dem 29. März 2016 betreibt Peppermint Anime, unter dem Namen AkibaPass ein eigenes Video-on-Demand-Portal. Seitdem nutzt Peppermint Anime den Kanal auf Vimeo nur noch für Werbezwecke.

Das US-Portal Crunchyroll kündigte im April 2013 an, ebenfalls deutsche Untertitel anzubieten und begann im Dezember 2013 mit deutschen Simulcasts für mehrere Serien. 

Von April 2013 bis Juni 2015 bot auch RTL II Animes im Internet an. Das Angebot sollte als Ersatz zum eingestellten Programmblock im TV dienen. Auch andere TV-Sender bieten Animes nach Ausstrahlung zeitlich befristet auf ihren Websites an.

Im Juli 2013 begannen der Publisher Kazé mit "Space Dandy" auf "Anime on Demand" sowie Nipponart mit "Chaika, die Sargprinzessin" auf Clipfish mit Simulcasts. Letztere Website richtete im selben Zeitraum einen separaten Animebereich ein. Das von japanischen Animationsstudios und Publishern betriebene Portal "Daisuki.net" bot im Dezember 2013 den Fernsehfilm "" als deutschen Simulcaststream an. Beim deutschen Netflix-Start zum September 2014 war neben anderen Animeserien auch "Knights of Sidonia" im Katalog. Es wird als „Netflix-Original“ beworben, da die Website die exklusiven Vertriebsrechte in all ihren Territorien besitzt. Viewster begann im Dezember 2014 mit regulären deutschen Simulcasts von Anime der aktuellen Saison.

Seit September 2017 gibt es eine deutsche Sprachversion des französischen On-Demand-Portals Wakanim. Die ersten deutsch lokalisierten Titel sind von "Peppermint Anime" bereitgestellt worden. Derzeit sind die Titel nur in Originalton mit deutschen Untertitel abrufbar. Aber auch selbstlizenzierte Animes als Simulcast werden auf der deutschen Sprachversion zu sehen sein. Der erste Simulcast auf Wakanim ist UQ Holder und ist am 2. Oktober 2017 gestartet. Dabei kooperiert Wakanim mit dem deutschen Publisher peppermint Anime. Auf Wakanim erscheinen die Simulcast von peppermint Anime und auf Akiba Pass sollen dann die synchronisierten Titel erscheinen.

Das einzige derzeitige professionelle deutschsprachige Anime-Fachmagazin ist die "AnimaniA", die seit September 1994 erscheint. Dazu kommen Jugendmagazine mit eigenen Anime-Bereichen, wie "Mega Hiro", "Koneko" und "Kids Zone". Der Verein Anime no Tomodachi gibt seit 1997 die Zeitschrift "Funime" heraus.

Von August 1995 bis Dezember 1996 erschien das Magazin "A.M.I." (die Erstausgabe noch unter dem Namen "Project A-nime") im Schaefers-Verlag. Das Magazin wurde nach fünf Ausgaben eingestellt. Weiterhin erschien zwischen Januar 2001 und September 2007 das Magazin "MangasZene".

Außerhalb Asiens sind hauptsächlich die USA, Frankreich und Italien für die Verbreitung von Anime in Nachbarländern wie Spanien, Portugal, Arabien, Lateinamerika und auch Deutschland verantwortlich.
Anime-Serien sind in den Westen in den Vereinigten Staaten erstmals im Fernsehen aufgetaucht. Dort sind in den 60er-Jahren unter anderem "Astro Boy", "Kimba, der weiße Löwe", "Gigantor" und "Speed Racer" gelaufen. Danach waren Anime-Serien weniger präsent, als es in Europa der Fall war. Die populärsten Serien waren Serien im Science-Fiction-Bereich wie "Star Blazer", "Voltron" und "Robotech". In den späten 90ern ist wie in Deutschland die internationale Vermarktung der Serien "Sailor Moon", "Pokemon" und "Dragon Ball Z" für die Wahrnehmung von Anime im Speziellen verantwortlich gewesen. Auch gab es Koproduktionen zwischen den USA und Japan, dazu zählen "Das Letzte Einhorn" und "Transformers". Erfolgreiche Ausstrahlungen von Anime-Serien hatten Einfluss auf die Cartoon-Industrie in den USA selbst. Serien wie "Galaxy Rangers" in den 80ern sowie "Avatar – Der Herr der Elemente", "Monsuno" und "Teen Titans" in den 2000ern waren von der Anime-Ästhetik beeinflusst.

Im US-Fernsehen werden für Anime, die im Kinderprogramm laufen, die umfangreichsten Bearbeitungsmaßnahmen unternommen. Diese Fassungen werden dann oft international vermarktet. Der amerikanische Jugendschutz ist im Vergleich zum europäischen Standard in Deutschland, Frankreich etc. weitaus strenger. In den USA stehen den Unternehmen umfangreiche Mittel zur Verfügung, um Bilder zu retuschieren, Namen zu ändern, Folgen auszulassen, zusammenzuschneiden und somit die Handlung zu verändern. Auch die Musik wurde teilweise verändert. Freizügige, gewalttätige, religiöse oder japanisch-kulturelle Inhalte und auch Bezüge zu Alkohol, Waffen und Drogen werden entfernt. Ernsthafte Themen wie der Tod werden umschrieben oder ausgelassen. Diese Maßnahmen unterscheiden sich von Serie zu Serie und auch von Firma zu Firma. Die konsequentesten und umfangreichsten Bearbeitungen finden bei 4Kids ("One Piece", "Yu-Gi-Oh"), Harmony Gold ("Robotech"), Saban Brands ("Digimon", "Glitter Force") und DiC (Sailor Moon) statt.

Weitgehend unbearbeitete Serien haben Popularität durch Videokassetten oder durch Nachtprogramme von Sendern wie Cartoon Network oder SyFy gewonnen. Speziell im Nachtprogrammblock von Cartoon Network sind "Cowboy Bebop" und "Big O" sehr populär geworden. "Space Dandy" (vom Regisseur von "Cowboy Bebop") "" und eine zweite Staffel von "Big O" wurde von amerikanischen Geldern mitfinanziert. Netflix plant, mehrere Serien mitzufinanzieren, die dann als Netflix Original beworben werden.

In Frankreich sind Anime zum ersten Mal mit den Serien "Choppy und die Prinzessin" (Erstausstrahlung folgte in Deutschland in 1996) und "Kimba, der weiße Löwe" (vier Jahre vor der Deutschen Ausstrahlung) im Jahr 1974 aufgetaucht. Ähnlich wie "bei Wickie und die starken Männer" und "Die Biene Maja" gab es französisch-japanische Koproduktionen ("Barbapapa", "Odysseus 31" und "Die geheimnisvollen Städte des Goldes"). Mit der Toei-Produktion "Grendizer" (auch genannt "Goldorak") wurde 1978 eine Serie ausgestrahlt, die maßgeblich dafür verantwortlich war, dass im Kinderprogramm vermehrt auf Anime gesetzt wurde. Die Serie erreichte hohe Einschaltquoten, löste aber auch große Anfeindungen und Proteste gegenüber gewalthaltigen japanischen Produktionen aus. TF1 ist der größte Privatsender Frankreichs und setzte im Kinderprogramm stark auf Anime, viele verschiedene Serien waren verantwortlich für die große Fanszene in Frankreich. Hat RTL2 insgesamt etwa 60 Serien gezeigt, waren es auf TF1 weit über 100 Serien. AB Productions hat die Serien jedoch als billiges Kinderprogramm angesehen und diese Massen an Serien dann so im Schnitt und Dialog zusammengestutzt. 1997 wurde das Programm auf TF1 nach Protesten und einen Konflikt über Anime, der über 15 Jahre anhielt, vollständig abgesetzt. Danach haben sich verschiedene Spartensender gefunden, die ein Animeprogramm sendeten, während im Kinderprogramm der großen Sender ausschließlich auf sehr kindgerechte Anime gesetzt wurde.

Space Adventure Cobra gilt als der Anime mit dem höchsten Kultstatus in Frankreich, Realverfilmungen und Fortsetzungen als Koproduktion sind geplant. 2004 wurde Ghost in the Shell 2: Innocence bei den Internationalen Filmfestspielen von Cannes 2004 nominiert. Wie in den USA hatten Anime Einfluss auf die heimische Zeichentrickindustrie in Frankreich. "Totally Spies!" und "Wakfu" sind ästhetisch an Anime angelehnt.

In Italien war die Resonanz auf Anime durchwegs positiv. Seit "Goldorak" wurde beinahe jedes Genre und Format von Japan übernommen. In Italien wurden die meisten Anime außerhalb Japans im Fernsehen und Kino gezeigt. Während in Deutschland in den 70ern und 80ern nur knapp 20 Serien gezeigt wurden, waren es in Italien bereits über 200. Der Grund für diese Massenimporte war, dass Italien bereits 1976 das Fernsehen privatisierte und daraus eine Vielfalt an Sendern hervorging. Auch waren Anime die preiswertesten Zeichentrickproduktionen. Koproduktionen mit Japan wie "Calimero", "Z wie Zorro" und "Die Abenteuer des Sherlock Holmes" sind entstanden. Eine Vielzahl der Sendungen, die in Kinderprogrammen der großen Sender liefen (Rai und Mediaset), wurden konsequent bearbeitet. So hat man Gewalt und freizügige Szenen geschnitten – aber auch Zensur und Veränderungen im Dialog wurden vorgenommen. Thematiken wie der Tod, sexuelle Anspielungen, japanische kulturelle Inhalte, sowie drastische Bilder und Zustände wurden sehr kindgerecht und abgeflacht aufbereitet. Durch die Thematik der Serie "Detektiv Conan" haben sich aber solche Dialogumschreibungen wieder gelegt, und diese werden inzwischen auch in anderen Serien nicht mehr verwendet. In den 70ern, 80ern und 90ern sind verschiedene Serien unverändert auf verschiedenen Lokalsendern gelaufen, jedoch geriet "Fist of the North Star" in starke Kritik, weshalb fortan auf diesen kleineren Sendern auf Anime verzichtet wurde. 1999 begann mit MTV Italy die erste Ausstrahlung von Anime explizit für ein älteres Publikum zu einer passenden Sendezeit.

In Italien sind speziell Anime für jüngere Mädchen beliebter als in vielen anderen Ländern, speziell "Rock ’n’ Roll Kids" ist für vier Staffeln als Realserie umgesetzt worden. Der mitunter populärste Anime ist "Lupin III". Italien war Mitfinanzierer einer neuen Serie des Franchises, "Lupin Sansei".

Verschiedene Animeserien sind in Spanien zunächst auf dem öffentlich-rechtlichen Sender Televisión Español gelaufen, jedoch geriet "Saint Seiya" in die Kritik und wurde abgesetzt. Auch Koproduktionen wie "Um die Welt mit Willy Fog", "D’Artagnan und die drei MuskeTiere" und "Roy, the Little Cid" sind entstanden. Mit dem Aufkommen des Privatfernsehens im Jahre 1990 startete der Sender Telecinco. Er setzte "Saint Seiya" fort und importierte fast 100 weitere Animeserien. Genau wie in Frankreich und Italien hat sich die Wahrnehmung für Anime weit vor Deutschland und den USA entwickelt. Jedoch kamen viele dieser Serien in die Kritik aufgrund von Gewaltdarstellungen oder auch wegen freizügigeren Szenen (kurze Röcke bei "Sailor Moon" oder Nacktheit bei "Ranma 1/2") und wurden 1999 zeitweilig mit Disneycartoons ersetzt.

Die zwei kulturell bedeutendsten Anime in Spanien sind "Mazinger Z" und "Shin Chan". Zu "Mazinger Z" gibt es eine Statue in der Stadt Tarragona und "Shin Chan" hatte mitunter eine größere Beliebtheit im Fernsehen als manche Hauptnachrichtensendungen.

Japanische Animationsfilme haben weltweit eine Fangemeinde, die sich in großen Teilen mit der von Mangas überschneidet. Viele Veranstaltungen widmen sich daher beiden Medien. Eine Fanszene entwickelte sich zunächst in Japan und ab den 1980er Jahren in kleinerem Maße in den USA und Frankreich. Mit zunehmender Verbreitung und Popularität von Animes wie auch Mangas nach der Veröffentlichung von "Akira" im Westen und umso mehr nach dem Erfolg von Fernsehserien, darunter "Sailor Moon" und "Dragon Ball", entwickelte sich in Nordamerika und Europa eine größere Fangemeinde. Als die deutsche Fanszene um das Jahr 2000 herum wuchs, war sie noch sehr jung. Einer Umfrage von Sozioland aus dem Jahr 2005 sowie einer Untersuchung des französischen "Centre d'Études et de Recherches Internationales" zufolge waren die meisten zwischen 14 und 25 Jahren alt. Nur wenige waren über 25, spielten jedoch in der Fanszene eine wichtige Rolle, gründeten die ersten Magazine und Veranstaltungen. 70 bis 85 Prozent der Befragten waren weiblich.

Bedeutende Veranstaltungen, auf denen sich Fans vorrangig treffen, sind Anime- und Manga-Conventions. Diese Conventions bieten Verkaufsstände, Workshops, Autogrammstunden, Konzerte oder Videoabende, Fans verkleiden sich oft als Figur aus einem Anime (Cosplay). Eine der weltweit größten Conventions ist die Japan Expo in Frankreich mit über 230.000 Besuchern. Darüber hinaus finden viele weitere Veranstaltungen in Europa und Nordamerika statt. Die größte Veranstaltung in Deutschland ist seit 2016 die DoKomi in Düsseldorf mit 40.000 Besuchern über zwei Tage. Zuvor waren lange Zeit die AnimagiC und die Connichi die von der Besucherzahl größten Anime-Conventions mit bis zu 25.000 Besuchern. Daneben ist Anime auch bei Veranstaltungen zu Japan, Animationsfilm, Comic oder Literatur ein Thema, so finden sich in Deutschland beim Japantag oder der Frankfurter Buchmesse Veranstaltungen zum japanischen Animationsfilm.





</doc>
<doc id="82" url="https://de.wikipedia.org/wiki?curid=82" title="Actionfilm">
Actionfilm

Der Actionfilm (von engl. "action": Tat, Handlung, Bewegung) ist ein Filmgenre des Unterhaltungskinos, in welchem der Fortgang der äußeren Handlung von zumeist spektakulär inszenierten Kampf- und Gewaltszenen vorangetrieben und illustriert wird. Es geht eher um stimulierende Aktionen als um inhaltliche Zusammenhänge, empathisches Miterleben der Gefühlswelt der Protagonisten oder künstlerisch-ästhetische Bildwelten. Hauptbestandteile von Actionfilmen sind daher meist aufwendig gedrehte Stunts, Schlägereien, Schießereien, Explosionen und Verfolgungsjagden.

Der Actionfilm ist seit den 1960er-Jahren ein eigenständiges Filmgenre, doch seine Konventionen sind bereits seit dem Beginn der Filmgeschichte bekannt. Künstler aus dem Vaudeville wie Buster Keaton ließen ihr Können in artistischer Bewegung in Verbindung mit Tricktechnik in ihr Filmschaffen einfließen.

Der Actionfilm als eigenes Genre hat seinen Ursprung im Kriminalfilm, in dem in den 1950er-Jahren Aktion und explizite Darstellung von physischer Gewalt zunehmend an Bedeutung gewann, etwa in Stanley Kubricks "Die Rechnung ging nicht auf" (1956). Alfred Hitchcock präsentierte in "Der unsichtbare Dritte" (1959) erstmals eine geschlossene filmische Welt, die ausschließlich als Herausforderung für die physische Aktion der Hauptfigur dient. Dieses Konzept der geschlossenen Actionwelt, die rein zum Ausleben von Körperakrobatik und zur Demonstration spektakulärer Gewaltanwendungstechniken existiert, fand seine Fortsetzung in den Filmen der James-Bond-Reihe und in Fernsehserien wie "Kobra, übernehmen Sie".

Dieser von realistischer Darstellung und moralischer Wertung weit entfernten Illusionstendenz stehen die Regisseure der Bewegung des New Hollywood gegenüber, die in offener Form Aktion und Gewaltanwendung inszenierten. Sie reagierten auf gesellschaftliche und politische Entwicklungen wie die Protestbewegung und den Vietnamkrieg und suchten den Kontext der Darstellung zu Fragen der Moral, etwa zu den Folgen von Gewaltanwendung auf den menschlichen Körper. Beispiele für diese realistischere und ernüchternde Herangehensweise sind Arthur Penns "Bonnie und Clyde" (1967) und Sam Peckinpahs "The Wild Bunch – Sie kannten kein Gesetz" (1969).

Mit den Bruce-Lee-Filmen fand eine Ära der Überbetonung physischer Kräfte und des Körperkultes im Actionfilm ihren Anfang. Stilmittel wie Zeitlupe und Tonverfremdungen führten zur Entwicklung und Definition des Subgenres des Martial-Arts-Films. In den 1980er Jahren beherrschte der Actionfilm das Mainstreamkino mit Stars wie Arnold Schwarzenegger und Sylvester Stallone, die durch Bodybuilding den Körperkult auf einen Höhepunkt führten. Reaktionäre Themen wie Rachephantasien und das stereotype Aufbauen von Feindbildern beherrschten das Actionkino.

In den 1990er Jahren wurde das Genre zunehmend ironisiert und spiegelte sich selbst, etwa in Filmen wie "Last Action Hero" (John McTiernan, 1993) und "True Lies" (James Cameron, 1994). McTiernans "Stirb-langsam"-Reihe (1988 bis 2013) brach ebenfalls ironisch mit dem Heldenbild des Actionfilms und ließ ihren Protagonisten, dargestellt von Bruce Willis, entmystifiziert als leidensfähigen Jedermann gegen das Böse siegen. Stars wie Jackie Chan vereinnahmten den Stunt als Teil der künstlerischen Darstellung und zogen einen Teil ihrer Popularität aus der Tatsache, auch gefährliche Action grundsätzlich selbst zu bewerkstelligen.

Die Bewegung, Grundmotiv des Films, dient im Actionfilm in erster Linie Schauzwecken und hat weniger erzählerische Funktionen. Oft werden im Actionfilm in der Art einer Nummernrevue geschlossene Sequenzeinheiten aneinandergereiht, die der Zurschaustellung unterschiedlichster bewegungsgetriebener Konflikt- oder Duellsituationen dienen, etwa Shootouts, Verfolgungsjagden, Körperkämpfe oder Explosionen. Subjekte der Aktion sind speziell im US-amerikanischen Actionfilm häufig sich verfolgende Fahrzeuge, etwa in "Brennpunkt Brooklyn" (William Friedkin, 1971), "Bullitt" (Peter Yates, 1968) oder "Dirty Harry" (Don Siegel, 1971). Der dargestellten Gewalt wird häufig in wirklichkeitsfremder Weise der Realitätsbezug genommen. Filmische Mittel wie Konvergenzmontage und Parallelmontage strukturieren diese Nummern, etwa um einen Spannungsbogen in einer Last-Minute-Rescue aufzulösen.

In den Plots geht es meist um den Kampf zwischen Gut und Böse, die Identifikationsfigur ist häufig ein physisch starker männlicher Held (oder eine weibliche Heldin, siehe beispielsweise ""), der/die in der Regel eindeutige moralische Prinzipien vertritt, die den ethischen und weltanschaulichen Grundlagen der westlichen Kultur entsprechen (Gut gegen Böse, Beschützen der Schwachen, Gerechtigkeit, Sühne für erlittenes Unrecht, Verteidigung und Bewahrung der vertrauten Lebensweise usw.). Häufig fließen erzählerische Elemente aus verwandten Genres in den Actionfilm ein, unter anderem aus dem Abenteuerfilm, dem Kriegsfilm, dem Kriminalfilm, dem Psychothriller, dem Horrorfilm und dem Science-Fiction-Film.




</doc>
<doc id="83" url="https://de.wikipedia.org/wiki?curid=83" title="Al Pacino">
Al Pacino

Alfredo James „Al“ Pacino (* 25. April 1940 in New York) ist ein US-amerikanischer Schauspieler, Filmregisseur und -produzent. Er gilt als einer der herausragenden Charakterdarsteller im zeitgenössischen US-amerikanischen Film und Theater. Im Laufe seiner Karriere wurde er unter anderem mit dem Oscar, dem Golden Globe Award, dem Tony Award und der National Medal of Arts ausgezeichnet.

Eine seiner bekanntesten Rollen ist die des "Michael Corleone" in der von Francis Ford Coppola inszenierten Filmtrilogie „Der Pate“.

Al Pacino, geboren in Manhattan, ist der Sohn von Salvatore Pacino, geboren in der sizilianischen Stadt Corleone, und von Rose Gerard, der Tochter eines italienischen Einwanderers und einer italienisch-amerikanischen Mutter, die in New York geboren wurde. Seine Eltern ließen sich scheiden, als er zwei Jahre alt war. Nach der Scheidung zogen Al und seine Mutter in die Bronx, und Pacino wuchs bei seinen sizilianischen Großeltern, die aus der Heimatstadt seines Vaters eingewandert waren, in der New Yorker South Bronx auf.

Sein Vater Salvatore, der nach Covina zog, arbeitete als Versicherungsagent und besaß ein Restaurant, „Pacino’s Lounge“. Das "Pacino’s" wurde in wirtschaftlich schweren Zeiten in den frühen 90er Jahren geschlossen; heute trägt es den Namen "Citrus Grill." Salvatore Pacino starb am 1. Januar 2005 im Alter von 82 Jahren.

Al Pacino ist der Stiefsohn der Schauspielerin und Maskenbildnerin Katherin Kovin-Pacino und hat vier Schwestern: Josette, eine Lehrerin, die Zwillinge Roberta und Paula sowie Desiree, die sein Vater in seiner vierten Ehe adoptierte.

Mit 17 Jahren wurde Pacino der Schule verwiesen und ging fortan auf die "Manhattan School of Performing Arts." Nebenher arbeitete er in kleineren Theatern als Platzanweiser und Kartenabreißer.

Pacino interessierte sich schon als Kind für die Schauspielerei. Er verfeinerte sein Talent an zwei renommierten New Yorker Schauspielschulen, in Herbert Berghofs HB Studio und später bei Lee Strasberg im The Actors Studio. Dort spielte er in mehreren erfolgreichen Theaterstücken wie in seinem Debütstück "The Connection" und in "The Indian Wants the Bronx," für das er mit einem Obie-Award ausgezeichnet wurde.

1969 wirkte er in seiner ersten Hollywood-Produktion "Ich, Natalie" mit. 1971 erhielt er neben Kitty Winn eine Rolle in dem Film "Panik im Needle Park," die ihm den Weg für die Rolle des Michael Corleone in Francis Ford Coppolas "Der Pate" (1972) ebnete und ihm 1973 seine erste Oscar-Nominierung einbrachte.

Nach "Hundstage" wurde es stiller um Pacino. Erst in den 1980er Jahren brachte er sich durch Filme wie Brian De Palmas "Scarface" (1983) und "Sea of Love – Melodie des Todes" (1989) wieder ins Gespräch. Nach einer erneuten Zusammenarbeit mit Coppola in "Der Pate III" (1990) folgte der Thriller "Heat" (1995) mit Schauspielkollege Robert De Niro. Die männliche Hauptrolle in dem Film "Pretty Woman" lehnte er ab.

Seine Darstellung des AIDS-kranken Schwulenhassers Roy Cohn in der Miniserie "Engel in Amerika" (2003) brachte ihm zahlreiche Preise ein und wurde von der Kritik hoch gelobt.

Pacino ist dafür bekannt, seine Rollen bis zum Extrem auszufüllen. Während sein Spiel in den 1970er Jahren - insbesondere in der "Der Pate" - dabei zumeist minimalistisch war, änderte es sich mit seinem Comeback in den 1980er Jahren radikal. Pacinos exaltierte Darstellungen in Filmen wie "Scarface, Im Auftrag des Teufels, An jedem verdammten Sonntag" oder auch "Der Duft der Frauen" wurden von Kritikern oftmals als Overacting bezeichnet. Für einen Großteil des Publikums zeichnet ihn allerdings genau diese Art und Weise als einen der größten Charakterdarsteller der Gegenwart aus. Viele seiner Filme, darunter auch "Glengarry Glen Ross," der zunächst floppte, zählen heute zu den Besten ihres Genres.

Neben seiner Karriere als Filmschauspieler arbeitet er weiterhin regelmäßig an verschiedenen Theatern – sowohl als Darsteller wie auch als Regisseur und Produzent. Für seine Rollen in den Bühneninszenierungen von "The Basic Training Of Pavlo Hummel" von David Rabe und "Does A Tiger Wear A Necktie?" von Don Petersen erhielt er jeweils einen Tony Award.

Als langjähriges Mitglied von David Wheelers "Experimental Theatre Company" in Boston stand er unter anderem in "Richard III." und Bertolt Brechts "Der aufhaltsame Aufstieg des Arturo Ui" auf der Bühne. In New York und London spielte Pacino in David Mamets "American Buffalo," in New York war er der Titelheld in "Richard III." und spielte den Mark Anton in "Julius Cäsar". Außerdem stand er im "Square Theatre" in New York in Oscar Wildes "Salome" auf der Bühne und wirkte in der Uraufführung von Ira Levins Theaterstück "Chinese Coffee" mit.
In der Theatersaison 2010/2011 spielte Pacino in der "Shakespeare in the Park"-Produktion "Der Kaufmann von Venedig" den "Shylock". Mit der Inszenierung gewann Heather Lind, die als Shylocks Tochter "Jessica" auftrat, einen Theatre World Award für ein herausragendes Broadway-Debüt.

Pacinos erstes eigenständiges Projekt war 1996 "Looking for Richard," eine dokumentarische und künstlerische Filmstudie über den Charakter von Shakespeares "Richard III.," bei dem er Regie führte, die Produktion übernahm, das Drehbuch schrieb und die Hauptrolle spielte.

Pacino war auch Produzent, Hauptdarsteller und Koautor des Independent-Kurzfilms "The Local Stigmatic," einer Verfilmung des gleichnamigen Theaterstücks von Heathcote Williams, das sowohl im New Yorker Museum of Modern Art als auch im "Public Theatre" aufgeführt wurde.

Al Pacino war nie verheiratet. Er hat drei Kinder, eine Tochter mit Jan Tarrant und Zwillinge (Tochter und Sohn, * 2001) mit Beverly D’Angelo.

Al Pacino wurde im Lauf der Jahrzehnte von verschiedenen deutschen Sprechern synchronisiert. Nachdem er im ersten Jahrzehnt seiner Karriere in der Regel von Lutz Mackensy gesprochen wurde (u.a. "Der Pate I und II, Serpico, Hundstage)," übernahm mit "Scarface" (1983) Frank Glaubrecht die Synchronisation des Schauspielers. Seit 1995 ist Glaubrecht alleiniger Sprecher "(Heat, City Hall, Insomnia" etc.) und er kann mittlerweile als Pacinos Standardstimme bezeichnet werden.

In den frühen 1990er Jahren wurde Pacino auch von Joachim Kammer "(Dick Tracy)," Gottfried Kramer "(Der Pate 3)" und Klaus Kindler "(Carlito's Way)" gesprochen.



</doc>
<doc id="88" url="https://de.wikipedia.org/wiki?curid=88" title="Alkohole">
Alkohole

Alkohole (, eigentlich: feines Antimonpulver) sind organische chemische Verbindungen, die eine oder mehrere an aliphatische Kohlenstoffatome gebundene Hydroxygruppen (–O–H) besitzen. Alkohole, die sich von den Alkanen ableiten, werden Alkanole genannt. Um eine klare Abgrenzung der Alkohole von Halbacetalen oder Carbonsäuren sicherzustellen, kann man ergänzen, dass das Kohlenstoffatom (sp-hybridisiert, siehe auch Enole) mit der Hydroxygruppe nur noch mit Kohlenstoff- oder Wasserstoffatomen gebunden sein darf.

Ist die Hydroxygruppe an ein Kohlenstoffatom gebunden, das Teil eines aromatischen Ringes ist, so werden die Verbindungen als Phenole bezeichnet. Sie zählen nicht zu den Alkoholen, da diese Hydroxygruppen analog einer Carboxygruppe sauer reagieren.

Der Name einfacher Alkohole ergibt sich als Zusammensetzung aus dem Namen des ursprünglichen Alkans und der Endung . Zusätzlich wird die Position der OH-Gruppe durch eine vorangestellte Zahl verdeutlicht, zum Beispiel . Eine veraltete, bis 1957 gültige Bezeichnung für Alkohole ist – nach einem Vorschlag von Hermann Kolbe – "Carbinole".

Die Gruppe wird nach verschiedenen Kriterien (Zahl der Nichtwasserstoffnachbarn, Wertigkeit, Vorhandensein von Doppel-/Dreifachbindungen und Kettenlänge) eingeteilt.

Man unterscheidet Alkohole nach der Zahl der Nichtwasserstoffnachbarn des Kohlenstoffatoms, an welchem sich die Hydroxygruppe befindet. Bei primären Alkoholen trägt es zwei, bei sekundären ein und bei tertiären kein Wasserstoffatom. Ein Sonderfall ist das Methanol, das neben der Hydroxygruppe drei Wasserstoffatome am Kohlenstoffatom trägt.
Ist mehr als eine Hydroxygruppe in einem Alkoholmolekül vorhanden, wird deren Anzahl durch Einfügen einer der Anzahl der Hydroxygruppen entsprechenden griechischen Silbe (-di-, -tri- usw.) vor der Endung angegeben und man spricht von mehrwertigen Alkoholen. Ein ist das (Trivialname Ethylenglycol), ein das (Trivialname Glycerin). Die Zahl vor der Endung gibt die Position der funktionellen Gruppe(n) an. Dies gilt auch für einwertige Alkohole, zum Beispiel (Trivialname Isopropanol).

In Bezug auf das Vorhandensein von Doppel- bzw. Dreifachbindungen unterscheidet man Alkanole, Alkenole und Alkinole sowie den Spezialfall der meist instabilen Enole.

Über die Kettenlänge werden Alkohole ebenfalls unterschieden. Die Bezeichnung "Fettalkohole" verwendet man für Alkohole mit endständiger primärer mit gerader Kette und einer Länge von sechs (Hexanol) bis hin zu 22 (Behenylalkohol) Kohlenstoffatomen. Sie werden meist durch Reduktion der aus Fettsäuren gewonnen. Die höheren primären Alkohole mit 24 bis 36 Kohlenstoffatome bezeichnet man als Wachsalkohole.

In allen aliphatischen Alkoholen ist die Hydroxygruppe an ein sp-hybridisiertes Kohlenstoffatom (C-Atom mit 4 Substituenten) gebunden. Ein Spezialfall sind die meist instabilen Enole, bei denen die Hydroxygruppe an ein sp-hybridisiertes Kohlenstoffatom gebunden ist.

Niedrigmolekulare Alkohole sind Flüssigkeiten, die einen charakteristischen Geruch und einen brennenden Geschmack besitzen. Höhere Alkohole sind meist feste Verbindungen mit nur schwach ausgeprägtem Geruch. Aufgrund von intermolekularen Wasserstoffbrückenbindungen besitzen die Alkohole im Vergleich zu Kohlenwasserstoffen gleicher Molekülmasse relativ hohe Schmelz- und Siedepunkte. Wichtigstes gemeinsames Merkmal der Alkohole ist die Hydrophilie. Diese Eigenschaft nimmt mit zunehmender Länge des Alkylrestes ab und mit der Anzahl der Hydroxygruppen zu. Besonders die kurzkettigen Alkohole werden aufgrund ihres amphiphilen Charakters oft als Lösungsmittel verwendet.

Sauerstoff ist elektronegativer als Wasserstoff und Kohlenstoff, d. h., er zieht Elektronen stärker an als diese. Das führt zu einer unsymmetrischen Verteilung der Elektronen entlang der , man spricht von einer polaren Bindung, es bildet sich ein molekularer Dipol aus. Diese Dipole können untereinander Wasserstoffbrückenbindungen ausbilden, die die Anziehung der einzelnen Moleküle untereinander drastisch verstärken.
Dies führt für Alkohole zu relativ hohen Siedepunkten gegenüber den um eine Methyleneinheit verlängerten Homologen ihrer Stammverbindung, die eine annähernd gleiche molarer Masse besitzen. So hat beispielsweise das unpolare Ethan (CH) (M = 30) einen Siedepunkt von −89 °C, während Methanol (CHOH) (M = 32) diesen erst bei 65 °C erreicht.

Zusammenfassend:

Die OH-Gruppe ist ebenfalls in der Lage, Wasserstoffbrückenbindungen mit Wasser einzugehen. Sie erhöht damit die Hydrophilie, die Wasserlöslichkeit, der Verbindung. Organische Alkylreste selbst sind nicht wasserlöslich, also hydrophob. Die Wasserlöslichkeit sinkt daher mit der Größe des organischen Anteils und steigt mit der Zahl der Hydroxygruppen. Die Propanole und "tert"-Butanol sind bei Raumtemperatur noch in jedem Verhältnis mit Wasser mischbar, alle langkettigeren Alkohole lösen sich nur noch in zunehmend kleinen Mengen. Größere Mengen gelöster anorganischer Salze können auch bei den kurzkettigen Alkoholen eine Phasentrennung bewirken („Salzfracht“).

Zusammenfassend:

Der pK-Wert (Säurestärke) von Alkoholen liegt bei etwa 15. Sie reagieren somit in wässriger Lösung näherungsweise neutral. Es ist möglich, sie mit einer starken Base zu deprotonieren. Die deprotonierte Form eines Alkohols heißt "Alkoholat". Die Acidität von Alkoholen nimmt in der Reihe von Methanol über primäre, sekundäre und tertiäre
Alkohole ab. Ebenso ist es möglich, sie in gewissem Umfang mit starken Säuren zu protonieren:
Industriell werden Alkoholate durch Umsetzung der entsprechenden Alkohole mit elementarem Natrium hergestellt:

Im IR-Spektrum von Alkoholen ist deutlich die breite Bande der O–H-Valenzschwingung im Bereich von 3200–3650 cm zu erkennen. Die Breite des Peaks wird durch Wasserstoffbrückenbindungen mit Wassermolekülen verursacht und ist in Spektren von wasserfreien Alkoholen in einem engeren Bereich von 3620–3650 cm zu finden.

Unterhalb von 140 °C bildet sich der Ester der Schwefelsäure.

Bei etwa 140 °C findet die Kondensationsreaktion zu einem Ether statt.

Oberhalb von 170 °C werden primäre Alkohole zu Alkenen dehydratisiert. (Eliminierung)

Die Selenoxid-Eliminierung ist eine milde Variante der Eliminierung.

Mit Carbonsäuren reagieren Alkohole unter Wasserabgabe zu Estern, diese Reaktion wird auch Veresterung genannt. Diese Reaktion wird durch Säuren katalysiert.

Primäre Alkohole lassen sich zu Aldehyden und Carbonsäuren, sekundäre Alkohole zu Ketonen oxidieren. Tertiäre Alkohole lassen sich nicht weiter oxidieren, es sei denn unter Zerstörung des Kohlenstoffgerüsts.

Zur Oxidation von primären Alkoholen zur Carbonsäure können Chrom(VI)-haltige Oxidationsmittel eingesetzt werden, wie sie z. B. bei der Jones-Oxidation Anwendung finden. Als chromfreies, weniger giftiges Reagenz steht wässriges Rutheniumtetroxid zur Verfügung.

Die Oxidation eines primären Alkohols kann unter Verwendung bestimmter Chrom(VI)-Verbindungen wie dem Collins-Reagenz auch nur bis zur Stufe des Aldehyds erfolgen. Entscheidend ist, dass wasserfreie Lösungsmittel eingesetzt werden. Ist kein Wasser anwesend, kann keine Hydratisierung zum geminalen Diol des Aldehyds (Aldehydhydrate) stattfinden.

Da lösliche Chromate sehr giftig sind, sowie karzinogene und mutagene Eigenschaften besitzen, wurden alternative Methoden zur Oxidation von Alkoholen entwickelt. Eine häufig zur Anwendung kommende Methode ist die Swern-Oxidation mit aktiviertem Dimethylsulfoxid. Fast alle Methoden eignen sich ebenfalls für die Oxidation sekundärer Alkohole zu Ketonen. Die folgende Aufzählung liefert eine Übersicht der wichtigsten Methoden.

Oxidation zur Carbonsäure/zum Keton:

Oxidation zum Aldehyd/zum Keton:


Mit Aldehyden reagieren Alkohole in Gegenwart saurer Katalysatoren zu Halbacetalen bzw. Acetalen.

Viele Alkohole sind wichtige Lösungsmittel, die sowohl in der Industrie, als auch im Haushalt eingesetzt werden; die mengenmäßig wichtigsten sind Methanol, Ethanol, 2-Propanol und "n"-Butanol. Im Jahr 2011 wurden weltweit etwa 6,4 Mio. Tonnen dieser alkoholischen Lösungsmittel nachgefragt.

Wenn man zu einer flüssigen Alkoholprobe Natrium hinzufügt, so entsteht Wasserstoff, welchen man mit der Knallgasprobe nachweisen kann. Diese Methode gilt zwar als Alkoholnachweis, ist jedoch nicht eindeutig, da alle ausreichend protischen Substanzen, zum Beispiel Carbonsäuren, anorganische Säuren oder Wasser, die gleiche Reaktion eingehen.

Der Umsatz von Alkoholen mit Dichromaten in schwefelsaurer Lösung ist geeignet, um Alkohole quantitativ nachzuweisen und wurde früher in den Alcotest-Röhrchen eingesetzt:

Das Nachweisprinzip beruht auf dem Farbumschlag von gelb-orange (saure Dichromatlösung) nach grün (Chrom(III)-Ionen) und kann spektralphotometrisch gemessen werden.

Eine weitere Möglichkeit besteht in der Umsetzung mit Ammoniumcer(IV)-nitrat. Hierbei wird eine konzentrierte Lösung von Ammoniumcer(IV)-nitrat mit einer verdünnten Lösung der unbekannten Substanz versetzt. Enthält die unbekannte Substanz Alkohol-Gruppen, färbt sich das Gemisch rot (manchmal auch grün). Enthält die Substanz Phenole, fällt ein brauner Niederschlag aus.
Der Grund für diese Farbreaktion ist eine Komplexbildung, genauer gesagt eine Ligandensubstitution, bei der ein Alkohol/Phenol mit dem Sauerstoffatom am Cer(IV) koordiniert. Durch die Veränderung der Ligandensphäre verändert sich die Farbe des Cer(IV) von hellgelb zu rot/grün/braun. Leicht oxidierbare Alkohole/Phenole können einen negativen Nachweis ergeben, indem sie das Cer(IV) zu Cer(III) reduzieren.

Der Nachweis des Substitutionsgrades eines Alkohols, also ob es sich dabei um einen primären, sekundären oder tertiären Alkohol handelt, erfolgt über nucleophile Substitution der OH-Gruppe gegen Chlorid durch die Lucas-Probe. Die Substitution hat zur Folge, dass sich die entstehende Substanz nicht mehr in Wasser löst und damit eine eigene Phase ausbildet.
Dabei ist die Geschwindigkeit dieser Phasenbildung entscheidend:


Voraussetzung für diesen Test ist, dass sich der ursprüngliche Alkohol in Wasser löst. Auch darf keine andere unter den Reaktionsbedingungen substituierbare Gruppe vorliegen.

Die eindeutige Identifizierung eines unbekannten Alkohols erfolgt entweder spektroskopisch oder durch Synthese eines charakteristischen Derivates, das einen Schmelzpunkt hat, der von den Schmelzpunkten gleicher Derivate ähnlicher Alkohole gut zu unterscheiden ist. Oftmals werden sie über Ester der oder der identifiziert. Hierzu wird die zu analysierende Substanz in Gegenwart geringer Mengen Schwefelsäure umgesetzt. Die Schmelzpunkte dieser Derivate sind in der Regel scharf.

Die Derivate der besitzen in der Regel höhere Schmelzpunkte als die der . Sie werden dann bevorzugt gewählt, wenn der Schmelzpunkt mit der zu niedrig ist und keine genaue Bestimmung mehr möglich wird.



</doc>
<doc id="89" url="https://de.wikipedia.org/wiki?curid=89" title="The Visitors">
The Visitors

The Visitors ist das achte und letzte Studioalbum der schwedischen Popgruppe ABBA. Es wurde erstmals am 30. November 1981 in Schweden veröffentlicht. Die Aufnahmen im Studio dauerten von März bis November 1981. Etwa gleichzeitig mit der LP wurde "One of Us" als Single herausgegeben und zum letzten internationalen Charterfolg der Gruppe, während die nachfolgende Singleauskopplung "Head over Heels" floppte. "The Visitors" war zudem die erste veröffentlichte Audio-CD der Musikgeschichte und wurde am 17. August 1982 vorgestellt.
Im Vergleich zu den Vorgängeralben von ABBA weist dieses einige musikalische Eigenheiten auf, die zuvor in den Produktionen der Gruppe kaum Anwendung fanden. So wird kein einziges Stück auf der gesamten LP von den beiden ABBA-Sängerinnen Agnetha Fältskog und Anni-Frid Lyngstad durchgehend gemeinsam gesungen. Die Lead Vocals wurden in jedem der neun Lieder auf eine der beiden aufgeteilt. Zudem wurden viele Elemente, die bis dato charakteristisch für die Musik der Gruppe gewesen war, durch neue ersetzt. So handeln die Texte von Trennung, Abschiedschmerz und Kriegsangst. 

Benny Andersson und Björn Ulvaeus begannen im Februar 1981 mit dem Komponieren für ein neues Studioalbum. Die am 12. Februar 1981 öffentlich bekannt gegebene Scheidung der beiden ABBA-Mitglieder Andersson und Lyngstad überschattete dabei die Arbeit innerhalb der Band. Die Aufnahmen für drei neue Stücke begannen am 16. März 1981 in den Polar Music Studios in Stockholm. Darunter befand sich mit "When All Is Said And Done" eine Pop-Ballade, deren Text von einer zerbrochenen Beziehung handelt und sich auf die Trennung von Lyngstad und Andersson beziehen sollte. Dementsprechend übernahm Lyngstad die Lead Vocals und ließ in ihre stimmliche Darbietung auch die emotionalen Aspekte mit einfließen, die angesichts ihrer damaligen Privatsituation gegeben waren. 

Auch "Slipping Through My Fingers" gehörte zu den ersten drei eingespielten Songs. Ulvaeus hatte die Idee für den Text, nachdem er seine Tochter Linda dabei beobachtete wie sie zur Schule ging. Er handelt von den gemischten Gefühlen der Eltern beim Heranwachsen ihrer Kinder und der damit verbundenen Einsamkeit und Nostalgie. Hier übernahm Agnetha Fältskog den Leadgesang und erzählte später, der Song habe sich „sehr echt angefühlt“. Eine symbolische Kulisse der Küche und des Frühstückstisches, die im Text des Songs vorkommen, befindet sich heute im ABBA-Museum auf Djurgården in Stockholm.

Ein weiterer Song, dessen Grundspuren im März 1981 eingespielt wurde, war "Two for the Price of One", bei dem zur Abwechslung Ulvaeus den Hauptgesang übernahm. Dieser meinte später, der Song hätte trotz des trivialen Textes ein Hit werden können, hätten Fältskog und Lyngstad die Lead Vocals gesungen. Etwa zur selben Zeit wurde das bisherige analoge Aufnahmegerät im Tonstudio durch ein digitales mit 32 Spuren ersetzt. Zwischen 27. und 29. April 1981 wurden "Slipping Through My Fingers" und "Two for the Price of One" für das Fernsehspecial "Dick Cavett Meets ABBA" live aufgezeichnet, neben Darbietungen einiger älterer ABBA-Songs. 

Am 26. Mai wurde ein erster Versuch unternommen, "Like an Angel Passing Through My Room" aufzunehmen. Diese erste Demoversion trug den Arbeitstitel "Twinkle Twinkle Little Star" und wurde von Ulvaeus gesungen. Eine weitere Alternative zur selben Melodie entstand mit "Another Morning Without You", bei der Lyngstad solo sang. Bis zum Ende der Aufnahmesessions wurden weitere Varianten mit verschiedenen Instrumentierungen und Lead Vocals ausprobiert. Eine Version, die beide ABBA-Sängerinnen beinhaltete, wurde später verworfen und durch eine Variante ersetzt, in der Lyngstad vollständig alleine singt. Für die Deluxe Edition des Albums wurde 2012 ein neun-minütiges Medley zusammengestellt, das die Entstehung von "Like an Angel Passing Through My Room" über mehrere Alternativversionen dokumentiert.

Am 2. September 1981 war Aufnahmebeginn für "I Let the Music Speak" und "Head over Heels". Besonders bei letzterem hatten die Musiker Probleme. Da die neue digitale Studiotechnik sehr präzise arbeitete, konnte Toningenieur Michael B. Tretow nur noch mäßig mit Verzögerung und Verschiebung der Spuren experimentieren, was die Aufnahmen „trockener“ und „kälter“ wirken ließ. So wirkte "Head over Heels" trotz eingängiger Melodie und ausgefeiltem Arrangement „wie eingefroren“, zählt aber dennoch zu den schwungvolleren und fröhlicheren Songs des Albums. Währenddessen zeugte "I Let the Music Speak" wieder einmal von den Ambitionen der beiden Komponisten, ein Musical zu schreiben. Parallel wurde "Should I Laugh or Cry" produziert, das später nicht auf der Titelliste des Albums erschien, weil es als „nicht stimmig genug“ angesehen wurde. Stattdessen wurde es im Dezember 1981 als B-Seite veröffentlicht.

Am 15. Oktober 1981 gingen die Aufnahmen mit "Soldiers, One of Us" und das für das Album namensgebende Lied "The Visitors" zu Ende. Der Text des ersteren handelte in Zusammenhang mit dem Kalten Krieg von Kriegsangst und der Furcht vor einer möglichen neuen faschistischen Bewegung. "One of Us" handelte wiederum von den Folgen einer zerbrochenen Beziehung und wurde am 21. Oktober begonnen. Das Stück wurde später als erste Single aus dem Album ausgekoppelt und avancierte zum letzten internationalen Hit der Gruppe. Der Titelsong wurde ab 22. Oktober 1981 aufgenommen. Der Text von "The Visitors" handelt von Überwachung und Besuchen der Sicherheitspolizei bei Dissidenten in der Sowjetunion. Mitte November waren die Aufnahmen und Nachbearbeitungen von allen Songs abgeschlossen.

Das Album-Cover wurde von Rune Söderqvist zusammen mit dem Fotografen Lars Larsson entworfen. Söderqvist stellte die Gruppe im kalten, ungeheizten Atelier des Malers Julius Kronberg im Freilichtmuseum Skansen, Stockholm, in einer düsteren Stimmung dar. Im Hintergrund hing ein großes Gemälde von Julius Kronberg, auf dem ein Engel dargestellt war. Das Cover zeigte die Gruppe zum ersten Mal nicht mehr als Gemeinschaft, sondern jedes Mitglied für sich allein. Diese düstere Stimmung innerhalb der Gruppe spiegelte sich sowohl in den Melodien als auch in den Texten vieler Songs wider. Das Cover ist laut "The Making of The Visitors" auch das Resultat einer gewissen Erschöpfung aufgrund des jahrelangen Zusammenseins als Gruppe.
Seite 1:

Seite 2:








</doc>
<doc id="93" url="https://de.wikipedia.org/wiki?curid=93" title="Aluminium">
Aluminium

Aluminium ist ein chemisches Element mit dem Elementsymbol Al und der Ordnungszahl 13. Im Periodensystem gehört Aluminium zur dritten Hauptgruppe und zur 13. IUPAC-Gruppe, der Borgruppe, die früher auch als "Gruppe der Erdmetalle" bezeichnet wurde. Es gibt zahlreiche Aluminiumverbindungen.

Aluminium ist ein silbrig-weißes Leichtmetall. In der Erdhülle ist es, nach Sauerstoff und Silicium das dritthäufigste Element und in der Erdkruste das häufigste Metall.

In der Werkstofftechnik werden mit „Aluminium“ alle Werkstoffe auf Basis des Elementes Aluminium verstanden. Dazu zählt Reinaluminium (mindestens 99,0 % Al), Reinstaluminium (min 99,7 % Al) und insbesondere die Aluminiumlegierungen die Festigkeiten besitzen die mit Stahl vergleichbar sind, bei nur einem Drittel seiner Dichte.

Entdeckt wurde Aluminium, das in der Natur nur in Form von chemischen Verbindungen aber nicht als Metall vorkommt, im frühen 19. Jahrhundert. Im frühen 20. Jahrhundert setzte die industrielle Massenproduktion ein.

Die Gewinnung erfolgt in Aluminiumhütten ausgehend von dem Mineral Bauxit zunächst im Bayer-Verfahren, mit dem Aluminiumoxid gewonnen wird, und anschließend im Hall-Héroult-Prozess einer Schmelzflusselektrolyse, bei der schließlich Aluminium gewonnen wird. Weltweit wurden 2016 insgesamt 115 Mio. Tonnen Aluminiumoxid (AlO) produziert. Daraus hat man 54,6 Mio. Tonnen Primäraluminium gewonnen.

Das Metall ist sehr unedel und reagiert an frisch angeschnittenen Stellen bei Raumtemperatur mit Luft und Wasser zu Aluminiumoxid. Dies bildet aber sofort eine dünne, für Luft und Wasser undurchlässige Schicht (Passivierung) und schützt so das Aluminium vor Korrosion. Reines Aluminium weist geringe Festigkeiten auf; bei Legierungen sind sie deutlich höher. Die elektrische und thermische Leitfähigkeit ist hoch, weshalb Aluminium für leichte Kabel und Wärmetauscher verwendet wird.

Eines der bekanntesten Produkte ist Alufolie. Weitere sind Bauteile in Fahrzeugen und Maschinen, elektrische Leitungen, Rohre, Dosen und Haushaltsgegenstände. Das Aluminiumrecycling erreicht weltweit Raten von etwa 40 %.

1782 vermutete Lavoisier als erster, dass es sich bei der 1754 von Marggraf aus einer Alaunlösung gewonnenen Alaunerde ("alumina", abgeleitet von lateinisch ‚Alaun‘) um das Oxid eines bislang unbekannten Elements handle. Dessen Darstellung glückte schließlich 1825 dem Dänen Hans Christian Ørsted durch Reaktion von Aluminiumchlorid (AlCl) mit Kaliumamalgam, wobei Kalium als Reduktionsmittel diente:

Davy, der sich lange Zeit ebenfalls an der Darstellung des neuen Elements versucht hatte, führte ab 1807 die Namensvarianten "alumium", "aluminum" und "aluminium" ein, von welchen die letzten beiden im Englischen bis heute nebeneinander fortbestehen.

1827 gelang es Friedrich Wöhler mit der gleichen Methode wie Ørsted, jedoch unter Verwendung metallischen Kaliums als Reduktionsmittel, reineres Aluminium zu gewinnen. Henri Étienne Sainte-Claire Deville verfeinerte den Wöhler-Prozess im Jahr 1846 und publizierte ihn 1859 in einem Buch. Durch diesen verbesserten Prozess stieg die Ausbeute bei der Aluminiumgewinnung, und in der Folge fiel der Preis des Aluminiums, der zuvor höher denn jener von Gold gewesen, innerhalb von zehn Jahren auf ein Zehntel.

1886 wurde unabhängig voneinander durch Charles Martin Hall und Paul Héroult das nach ihnen benannte Elektrolyseverfahren zur Herstellung von Aluminium entwickelt: der Hall-Héroult-Prozess. 1889 entwickelte Carl Josef Bayer das nach ihm benannte Bayer-Verfahren zur Isolierung von reinem Aluminiumoxid aus Bauxiten. Aluminium wird noch heute nach diesem Prinzip großtechnisch hergestellt.

Am Ende des 19. Jahrhunderts stand das Metall in solchem Ansehen, dass man daraus gefertigte Metallschiffe auf den Namen Aluminia taufte.

Aluminium ist mit einem Anteil von 7,57 Gewichtsprozent nach Sauerstoff und Silicium das dritthäufigste Element der Erdkruste und damit das häufigste Metall. Allerdings kommt es aufgrund seines unedlen Charakters praktisch ausschließlich in gebundener Form vor. Die größte Menge befindet sich chemisch gebunden in Form von Alumosilicaten, in denen es in der Kristallstruktur die Position von Silicium in Sauerstoff-Tetraedern einnimmt. Diese Silicate sind zum Beispiel Bestandteil von Ton, Gneis und Granit.

Seltener wird Aluminiumoxid in Form des Minerals Korund und seiner Varietäten Rubin (rot) und Saphir (farblos, verschiedenfarbig) gefunden. Die Farben dieser Kristalle beruhen auf Beimengungen anderer Metalloxide. Korund hat mit fast 53 Prozent den höchsten Aluminiumanteil einer Verbindung. Einen ähnlich hohen Aluminiumanteil haben die noch selteneren Minerale Akdalait (etwa 51 Prozent) und Diaoyudaoit (etwa 50 Prozent). Insgesamt sind bisher (Stand: 2017) 1156 aluminiumhaltige Minerale bekannt.

Das einzige wirtschaftlich wichtige Ausgangsmaterial für die Aluminiumproduktion ist Bauxit. Vorkommen befinden sich in Südfrankreich (Les Baux), Guinea, Bosnien und Herzegowina, Ungarn, Russland, Indien, Jamaika, Australien, Brasilien und den Vereinigten Staaten. Bauxit enthält ungefähr 60 Prozent Aluminiumhydroxid (Al(OH) und AlO(OH)), etwa 30 Prozent Eisenoxid (FeO) und Siliciumdioxid (SiO).

Bei der Herstellung unterscheidet man "Primäraluminium", auch "Hüttenaluminium" genannt, das aus Bauxit gewonnen wird, und "Sekundäraluminium" aus Aluminiumschrott. Die Wiederverwertung benötigt nur etwa 5 Prozent der Energie der Primärgewinnung.

Infolge der Passivierung kommt Aluminium in der Natur auch sehr selten elementar (gediegen) vor. Aluminium ist daher von der International Mineralogical Association (IMA) als eigenständiges Mineral anerkannt (Interne Eingangs-Nr. der IMA: "1980-085" und "1980-085a"). Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Aluminium unter der System-Nummer "1.AA.05" (Elemente – Metalle und intermetallische Verbindungen – Kupfer-Cupalit-Familie – Kupfergruppe) beziehungsweise in der veralteten 8. Auflage unter "I/A.03" eingeordnet. Die vorwiegend im englischsprachigen Raum verwendete führt das Element-Mineral unter der System-Nr. 01.01.01.05.

Erstmals entdeckt wurde Aluminium 1978 durch B. V. Oleinikov, A. V. Okrugin, N. V. Leskova in Mineralproben aus der Billeekh Intrusion und dem Dyke "OB-255" in der Republik Sacha ("Jakutien") im russischen Föderationskreis Ferner Osten. Insgesamt sind weltweit bisher rund 20 Fundorte (Stand 2017) für gediegen Aluminium bekannt, so unter anderem in Aserbaidschan, Bulgarien, der Volksrepublik China (Guangdong, Guizhou, Jiangsu und Tibet) und in Venezuela. Zudem konnte gediegen Aluminium in Gesteinsproben vom Mond, das die Sonde der Luna-20-Mission vom Krater Apollonius mitbrachte, nachgewiesen werden. Aufgrund der extremen Seltenheit hat gediegenes Aluminium keine Bedeutung als Rohstoffquelle.

In der Natur kommt gediegen Aluminium meist in Form körniger Mineral-Aggregate und Mikronuggets, kann in seltenen Fällen aber auch tafelige Kristalle bis etwa einen Millimeter Größe entwickeln. Frische Mineralproben sind von metallisch glänzender, silberweißer Farbe. An der Luft dunkeln die Oberflächen durch Oxidierung nach und wirken grau. Auf der Strichtafel hinterlässt Aluminium einen dunkelgrauen Strich.

Je nach Fundort enthält Aluminium oft Fremdbeimengungen von anderen Metallen (Cu, Zn, Sn, Pb, Cd, Fe, Sb) oder tritt eingewachsen in bzw. mikrokristallin verwachsen mit Hämatit, Ilmenit, Magnetit, Moissanit und Pyrit bzw. Jarosit auf.

Typmaterial, das heißt Mineralproben aus der Typlokalität des Minerals, wird im Geologischen Museum der Akademie der Wissenschaften in Jakutsk in der russischen Teilrepublik Sacha (Jakutien) aufbewahrt.

Aluminiummetall wird elektrolytisch aus einer Aluminiumoxidschmelze hergestellt. Da dieses aus den auf der Erde allgegenwärtigen Alumosilicaten nur schwer isoliert werden kann, erfolgt die großtechnische Gewinnung aus dem relativ seltenen, silikatärmeren Bauxit. Zur Gewinnung von reinem Aluminiumoxid aus Silikaten gibt es seit langem Vorschläge, deren Umsetzung allerdings bis heute nicht wirtschaftlich möglich ist.

Das im Erz enthaltene Aluminiumoxid/-hydroxid-Gemisch wird zunächst mit Natronlauge aufgeschlossen (Bayer-Verfahren, Rohrreaktor- oder Autoklaven-Aufschluss), um es von Fremdbestandteilen wie Eisen- und Siliciumoxid zu befreien, und wird dann überwiegend in Wirbelschichtanlagen (aber auch in Drehrohröfen) zu Aluminiumoxid (AlO) gebrannt.

Der trockene Aufschluss (Deville-Verfahren) hat dagegen keine Bedeutung mehr. Dabei wurde feinstgemahlenes, ungereinigtes Bauxit zusammen mit Soda und Koks in Drehrohröfen bei rund 1200 °C kalziniert und das entstehende Natriumaluminat anschließend mit Natronlauge gelöst.
Die Herstellung des Metalls erfolgt in Aluminiumhütten durch Schmelzflusselektrolyse von Aluminiumoxid nach dem Kryolith-Tonerde-Verfahren (Hall-Héroult-Prozess). Zur Herabsetzung des Schmelzpunktes wird das Aluminiumoxid zusammen mit Kryolith geschmolzen (Eutektikum bei 963 °C). Bei der Elektrolyse entsteht an der den Boden des Gefäßes bildenden Kathode Aluminium und an der Anode Sauerstoff, der mit dem Graphit (Kohlenstoff) der Anode zu Kohlenstoffdioxid und Kohlenstoffmonoxid reagiert. Die Graphitblöcke, welche die Anode bilden, brennen so langsam ab und werden von Zeit zu Zeit ersetzt. Die Graphitkathode (Gefäßboden) ist gegenüber Aluminium inert. Das sich am Boden sammelnde flüssige Aluminium wird mit einem Saugrohr abgesaugt.

Aufgrund der hohen Bindungsenergie durch die Dreiwertigkeit des Aluminiums ist der Prozess recht energieaufwändig. Pro produziertem Kilogramm Rohaluminium müssen 12,9 bis 17,7 Kilowattstunden an elektrischer Energie eingesetzt werden. Eine Reduzierung des Strombedarfs ist nur noch in geringem Ausmaß möglich, weil die Potentiale für energetische Optimierungen weitgehend erschlossen sind. Aluminiumherstellung ist daher nur in der Nähe preiswert zur Verfügung stehender Elektroenergie wirtschaftlich, beispielsweise neben Wasserkraftwerken, wie in Rheinfelden oder (ehemals) in Ranshofen unweit des Inns.

Die nachfolgende Tabelle zeigt die Aluminiumproduktion und die maximal mögliche Produktionsleistung der Hüttenwerke nach Ländern.

Um Aluminium zu recyceln, werden Aluminiumschrotte und „Krätzen“ in Trommelöfen eingeschmolzen. „Krätze“ ist ein Abfallprodukt bei der Verarbeitung von Aluminium und bei der Herstellung von Sekundäraluminium. Krätze ist ein Gemisch aus Aluminiummetall und feinkörnigen Oxidpartikeln und wird beim Schmelzen von Aluminium bei 800 °C aus dem Aluminiumoxid der normalen Aluminiumkorrosion und als Oxidationsprodukt (Oxidhaut) beim Kontakt von flüssigem Aluminium mit Luftsauerstoff gebildet. Damit beim Aluminiumgießen keine Aluminiumoxidpartikel in das Gussteil gelangen, wird die Krätze durch "Kratz"vorrichtungen von der Oberfläche des Metallbads abgeschöpft.

Um die Bildung von Krätze zu verhindern, wird die Oberfläche der Schmelze mit Halogenidsalzen (rund zwei Drittel NaCl, ein Drittel KCl und geringe Mengen Calciumfluorid CaF) abgedeckt (siehe dazu Aluminiumrecycling). Dabei entsteht als Nebenprodukt Salzschlacke, die noch ca. 10 Prozent Aluminium enthält, die, entsprechend aufbereitet, als Rohstoff für mineralische Glasfasern dient.

Allerdings steht das Sekundäraluminium im Ruf, dass beim Recycling pro Tonne jeweils 300 bis 500 Kilogramm Salzschlacke, verunreinigt mit Dioxinen und Metallen, entsteht, deren mögliche Wiederverwertung aber Stand der Technik ist.

Aluminium erstarrt ausschließlich in einem kubisch flächenzentriertem Raumgitter in der . Der Gitterparameter beträgt bei Reinaluminium 0,4049 nm (entspricht 4,05 Å) bei 4 Formeleinheiten pro Elementarzelle.

Leerstellen kommen mit einer Dichte von 1,3 × 10 bei 500 °C vor, bei Raumtemperatur sind es nur noch 10. Durch Abschrecken können auch größere Leerstellendichten bei Raumtemperatur vorkommen, was für einige Eigenschaften von Aluminiumwerkstoffen von Bedeutung ist, da die Leerstellen die Diffusion begünstigen. Durch Umformen bei Raumtemperatur kann die Leerstellendichte auf 10 erhöht werden. Die Versetzungs­dichte liegt bei 10, einem für Metalle typischen Bereich und führt zur guten Umformbarkeit von Aluminium. Stapelfehler konnten bei Aluminium nicht nachgewiesen werden, was mit der hohen Stapelfehlerenergie von 103 bis 200 (10 J/cm²) erklärt wird. Dies führt dazu, dass die Festigkeitsteigerung beim Kaltwalzen und -schmieden nur gering ausfällt und manche Aluminiumwerkstoffe sogar anschließend zur Entfestigung neigen.

Mit einer Dichte von 2,6989 g/cm³ (etwa ein Drittel von Stahl) ist Aluminium ein typisches Leichtmetall, was es als Werkstoff für den Leichtbau interessant macht. Die Dichte der Legierungen weicht meist nur um etwa +3 % bis -2 % ab. Spezielle Legierungen mit Lithium haben eine 15 % geringere Dichte. Aluminium zählt somit zu den leichtesten Werkstoffen, übertroffen nur noch von Magnesium.

Aluminium ist ein relativ weiches und zähes Metall. Die Zugfestigkeit von absolut reinem Aluminium liegt bei 45 N/mm², die Streckgrenze bei 17 N/mm² und die Bruchdehnung bei 60 %, während bei handelsüblich reinem Aluminium die Zugfestigkeit bei 90 N/mm² liegt, die Streckgrenze bei 34 N/mm² und die Bruchdehnung bei 45 %. Die Zugfestigkeit seiner Legierungen liegt dagegen bei bis zu 710 N/mm² (Legierung 7068). Sein Elastizitätsmodul liegt bei etwa 70.000 MPa, einem häufig angegebenen Wert. Für Reinaluminium wird ein Wert von 66,6 kN/mm² angegeben, die Werte schwanken jedoch von 60 bis 78 kN/mm². Der G-Modul liegt bei 25,0 kN/mm², die Querkontraktionszahl (Poissonzahl) bei 0,35.

Die Schmelztemperatur liegt bei 660,2 °C und die Siedetemperatur bei 2470 °C. Die Schmelztemperatur ist deutlich niedriger als die von Kupfer (1084,6 °C), Gusseisen (1147 °C) und Eisen (1538 °C), was Aluminium zu einem guten Gusswerkstoff macht.

Bei einer Sprungtemperatur von 1,2 K wird reines Aluminium supraleitend.

Die Wärmeleitfähigkeit liegt mit 235 W/(K m) relativ hoch. Die Wärmeleitfähigkeit von Kupfer liegt zwar etwa doppelt so hoch, dafür ist die Dichte etwa viermal größer, weshalb Aluminium für Wärmetauscher in Fahrzeugen genutzt wird. Der Wärmeausdehnungskoeffizient ist durch den recht niedrigen Schmelzpunkt mit 23,1 μm·m·K recht hoch.

Die Schwindung, also die Volumenabnahme beim Erstarren liegt bei 7,1 %.

Da thermische und elektrische Leitfähigkeit bei Metallen von denselben Mechanismen dominiert werden, ist Aluminium mit formula_2 auch ein sehr guter elektrischer Leiter. In der Rangfolge der Elemente mit der größten spezifischen Leitfähigkeit steht Aluminium wie auch bei der Wärmeleitfähigkeit hinter Silber, Kupfer und Gold an vierter Stelle. Durch die Kombination von hohem spezifischem Leitwert, geringer Dichte, hoher Verfügbarkeit und (im Vergleich zu anderen Materialien) geringen Kosten ist Aluminium in der Elektrotechnik – speziell in der Energietechnik, wo große Leiterquerschnitte benötigt werden – neben Kupfer zum wichtigsten Leitermaterial geworden.

Aluminium ist paramagnetisch, wird also von Magneten angezogen, der Effekt ist jedoch sehr schwach ausgeprägt. Die Magnetische Suszeptibilität liegt bei Raumtemperatur bei 0,62x10 m³/kg, womit Aluminium praktisch gesehen unmagnetisch ist.

Das reine Leichtmetall Aluminium hat aufgrund einer sich sehr schnell an der Luft bildenden dünnen Oxidschicht ein stumpfes, silbergraues Aussehen. Diese passivierende Oxidschicht macht reines Aluminium bei pH-Werten von 4 bis 9 sehr korrosionsbeständig, sie erreicht eine Dicke von etwa 0,05 µm.

Diese Oxidschicht schützt auch vor weiterer Oxidation, ist aber bei der elektrischen Kontaktierung und beim Löten hinderlich. Sie kann durch elektrische Oxidation (Eloxieren) oder auf chemischem Weg verstärkt werden.

Die Oxidschicht kann mittels Komplexbildungsreaktionen aufgelöst werden. Einen außerordentlich stabilen und wasserlöslichen Neutralkomplex geht Aluminium in neutraler chloridischer Lösung ein. Folgende Reaktionsgleichung veranschaulicht den Vorgang:

Dies geschieht vorzugsweise an Stellen, wo die Oxidschicht des Aluminiums bereits geschädigt ist. Es kommt dort durch Bildung von Löchern zur Lochfraßkorrosion. Kann die chloridische Lösung dann an die freie Metalloberfläche treten, so laufen andere Reaktionen ab. Aluminium-Atome können unter Komplexierung oxidiert werden:

Liegen in der Lösung Ionen edlerer Metalle vor, so werden sie reduziert und am Aluminium abgeschieden. Auf diesem Prinzip beruht die Reduktion von Silberionen, die auf der Oberfläche von angelaufenem Silber als Silbersulfid vorliegen, hin zu Silber.

Aluminium reagiert heftig mit wässriger Natriumhydroxidlösung (NaOH) unter Bildung von Wasserstoff. Diese Reaktion wird in chemischen Rohrreinigungsmitteln ausgenutzt.
Die Reaktion von Aluminium mit NaOH läuft in zwei Schritten ab: der Reaktion mit Wasser und die Komplexierung des Hydroxids zu Natriumaluminat.

Bei der Reaktion mit Wasser
entsteht zunächst Aluminiumhydroxid.

In der Regel wird anschließend die Oberfläche getrocknet, dabei wird das Hydroxid in das Oxid umgewandelt:

Dies passiert jedoch nicht bei der Reaktion von Aluminium in wässriger Natronlauge.

Nun folgt der 2. Schritt, die Komplexierung des Hydroxids zu Natriumaluminat:
Durch die Komplexierung wird das gallertartige Hydroxid wasserlöslich und kann von der Metalloberfläche abtransportiert werden. Dadurch ist die Aluminiumoberfläche nicht mehr vor dem weiteren Angriff des Wassers geschützt und Schritt 1 läuft wieder ab.

Mit dieser Methode lassen sich – ebenso wie bei der Reaktion von Aluminium mit Säuren – pro zwei Mol Aluminium drei Mol Wasserstoffgas herstellen.

Mit Brom reagiert Aluminium bei Zimmertemperatur unter Flammenerscheinung. Hierbei ist zu beachten, dass das entstehende Aluminiumbromid mit Wasser unter Bildung von Aluminiumhydroxid und Bromwasserstoffsäure reagiert.

Mit Quecksilber bildet Aluminium ein Amalgam. Wenn Quecksilber direkt mit Aluminium zusammenkommt, d. h., wenn die Aluminiumoxidschicht an dieser Stelle mechanisch zerstört wird, frisst Quecksilber Löcher in das Aluminium; unter Wasser wächst dann darüber Aluminiumoxid in Gestalt eines kleinen Blumenkohls. Daher wird Quecksilber in der Luftfahrt als Gefahrgut und „ätzende Flüssigkeit“ gegenüber Aluminiumwerkstoffen eingestuft.

Auch mit Salzsäure reagiert Aluminium sehr heftig unter Wasserstoffentwicklung, von Schwefelsäure wird es langsam aufgelöst. In Salpetersäure wird es passiviert.

In Pulverform (Partikelgröße kleiner 500 µm) ist Aluminium vor allem dann, wenn es nicht phlegmatisiert ist, aufgrund seiner großen Oberfläche sehr reaktiv. Aluminium reagiert dann mit Wasser unter Abgabe von Wasserstoff zu Aluminiumhydroxid. Feinstes, nicht phlegmatisiertes Aluminiumpulver wird auch als Pyroschliff bezeichnet. Nicht phlegmatisierter Aluminiumstaub ist sehr gefährlich und entzündet sich bei Luftkontakt explosionsartig von selbst.

In der Natur kommt ausschließlich das Isotop Al vor, das stabil ist und im Kern 14 Neutronen und 13 Protonen enthält. Es absorbiert keine Neutronen, weshalb Aluminium auch in Kernreaktoren genutzt wird. Alle anderen Isotope werden künstlich erzeugt und sind radioaktiv. Das stabilste dieser Isotope ist Al mit einer Halbwertszeit von einer Millionen Jahren. Durch Elektroneneinfang entsteht daraus Mg, durch Beta-Zerfall Mg und durch Einfangen eines Neutrons und anschließenden Gamma-Zerfall Al. Die Isotope Al bis Al (Außer Al und Al) haben Halbwertszeiten zwischen wenigen Sekunden und einigen hundert Sekunden. Al zerfällt mit einer Halbwertszeit von nur 0,13 Sekunden.

Aluminiumlegierungen sind Legierungen, die überwiegend aus Aluminium bestehen. Für andere Legierungen, die Aluminium enthalten, siehe Abschnitt #Weitere Anwendungen.

Aluminium kann mit zahlreichen Metallen legiert werden, um bestimmte Eigenschaften zu fördern oder andere, ungewünschte Eigenschaften zu unterdrücken. Bei einigen Legierungen ist die Bildung der schützenden Oxidschicht (Passivierung) stark gestört, wodurch die daraus gefertigten Bauteile teils korrosionsgefährdet sind. Nahezu alle hochfesten Aluminiumlegierungen sind von dem Problem betroffen.

Es gibt Aluminiumknetlegierungen, die zur Weiterverarbeitung durch Walzen, Schmieden und Strangpressen gedacht sind und Gusswerkstoffe. Diese werden in Gießereien verwendet.

Im Allgemeinen werden Aluminiumlegierungen in die zwei große Gruppen der Knet- und Gusslegierungen eingeteilt:

Außerdem wird unterschieden zwischen naturharten Legierungen – welche sich durch eine Wärmebehandlung nicht härten lassen – und aushärtbaren:

Aluminium ist nach Stahl der zweitwichtigste metallische Werkstoff. 2016 wurden weltweit 115 Mio. Tonnen produziert.
Der Aluminiumpreis bewegte sich am Weltmarkt seit 1980 um den Wert von 2000 Dollar pro Tonne (Reinheit von 99,7 %). Er ist jedoch relativ volatil, 2016 fiel er auf um die 1500 Dollar pro Tonne, während er 2017 wieder bei annähernd 2000 Dollar lag.

Aluminium weist eine hohe spezifische Festigkeit auf. Verglichen mit Stahl sind Bauteile aus Aluminium bei gleicher Festigkeit etwa halb so schwer, weisen jedoch ein größeres Volumen auf. Deshalb wird es gern im Leichtbau verwendet, also dort, wo es auf geringe Masse ankommt, die zum Beispiel bei Transportmitteln zum geringeren Treibstoffverbrauch beiträgt, vor allem in der Luft- und Raumfahrt. Auch im Kraftfahrzeugbau gewann es aus diesem Grund an Bedeutung; hier standen früher der hohe Materialpreis, die schlechtere Schweißbarkeit sowie die problematische Dauerbruchfestigkeit und die Verformungseigenschaften bei Unfällen (geringes Energieaufnahmevermögen in der sogenannten Knautschzone) im Wege. Die Haube des Washington-Denkmals, ein 3 kg schweres Gussstück, galt bis 1884 als eines der größten Aluminiumwerkstücke. Beim Bau von kleinen und mittleren Schiffen und Booten wird die Korrosionsbeständigkeit von Aluminium gegenüber Salzwasser geschätzt. Der Fahrzeugbau (inklusive Schiffen, Flugzeugen und Schienenfahrzeugen) machte 2010 mit ca. 35 Prozent den größten Anteil an der weltweiten Verwendung von Aluminium aus.

In Aluminiumlegierungen werden Festigkeiten erreicht, die denen von Stahl nur wenig nachstehen. Daher ist die Verwendung von Aluminium zur Gewichtsreduzierung überall dort angebracht, wo Materialkosten eine untergeordnete Rolle spielen. Insbesondere im Flugzeugbau und in der Weltraumtechnik sind Aluminium und Duraluminium weit verbreitet. Der größte Teil der Struktur heutiger Verkehrsflugzeuge wird aus Aluminiumblechen verschiedener Stärken und Legierungen genietet.

Bei Fahrzeugen spielt deren Masse eine Rolle: Je leichter ein Fahrzeug ist desto geringer ist der Treibstoffverbrauch. In Deutschland werden knapp 50 % des Aluminiums im Fahrzeugbau verwendet (Stand: 2015).

Bei Autos werden Aluminiumwerkstoffe verwendet für verschiedene Motor­komponenten – darunter der Motorblock, die Zylinderkolben für die spezielle Kolbenlegierungen existieren, die Zylinderköpfe – wo vor allem die geringe Wärmeausdehnung und Korrosionsanfäligkeit sowie die hohe Warmfestigkeit ausschlaggebend sind zusammen mit der guten Gießbarkeit da diese Komponenten üblicherweise gegossen werden. Weitere Anwendungen bei Fahrzeugen sind für Gehäuse von Getrieben, als Wärmeabschirmung und als Wärmetauscher – bei den letzten beiden in Form von Reinaluminium. Im Fahrwerk wird Aluminium genutzt als Schmiedeteile für Hinterachsen, Achsträger, Querlenker und Räder. In der Karosserie wird Aluminium verwendet für Türen, Motorhauben, Stoßfänger und Kotflügel, sowie in der Rohwagenstruktur.

Bei Nutzfahrzeugen wird Aluminium angewandt für Bordwände, Ladebordwände, Aufbauten, zur Ladungssicherung, Druckluftbehälter, Treibstofftanks und als Unterbauschutz. Der Leichtbau mit Aluminium wird bei Nutzfahrzeugen stark durch die gesetzliche Maximallast pro Achse beeinflusst: Bei geringerem Fahrzeuggewicht ist eine höhere Nutzlast möglich.

Auch bei Schienenfahrzeugen wird reichlich Aluminium verwendet. Voraussetzung waren dafür zwei wichtige andere Entwicklungen: Bestimmte Schweißverfahren die für Aluminiumwerkstoffe geeignet sind (WIG-Schweißen / MIG-Schweißen) in den 1950ern und das Strangpressen von Großprofilen. Die Verwendung von Aluminium hat die gesamte Bauweise von Schienenfahrzeugen verändert. Bis etwa 1970 waren Konstruktionen aus Stahlrohren üblich, danach vermehrt verschweißte Profile aus Aluminium.

Bereits in der Anfangsphase der Luftfahrt wurden Aluminiumwerkstoffe genutzt, 1903 beispielsweise Magnalium für die Beschläge eines Flugzeuges, das noch größtenteils aus Holz, Draht und Tuch bestand. Das erste flugfähige Ganzmetallflugzeug stammt aus dem Jahre 1915, bestand allerdings aus Stahlblechen in Schalenbauweise. Die entscheidende Entwicklung zur Verwendung von Aluminium im Flugzeugbau stammt von 1906 von Alfred Wilm, der mit dem Duraluminium eine aushärtbare Aluminium-Kupfer-Legierung fand, die sehr hohe Festigkeiten aufweist und sich daher ausgezeichnet für den Leichtbau eignet. Genutzt werden für Flugzeuge AlCu und AlZnMg. Die Gesamtmasse von Flugzeugen geht zu 60 % auf Aluminium zurück. Die Verbindung der aus Blechen gestanzten, geschnittenen oder getriebenen, aus dem Vollen gefrästen oder aus Profilen bestehenden Werkstücke erfolgt meist durch Nieten, da die meistverwendeten Werkstoffe schlecht schweißbar sind.

Aluminium ist ein guter elektrischer Leiter. Es weist nach Silber, Kupfer und Gold die vierthöchste elektrische Leitfähigkeit aller Metalle auf. Ein Leiter aus Aluminium hat bei gegebenem elektrischen Widerstand eine kleinere Masse, aber ein größeres Volumen als ein Leiter aus Kupfer. Daher wird meistens dann Kupfer als elektrischer Leiter verwendet, wenn das Volumen eine dominante Rolle spielt, wie beispielsweise bei den Wicklungen in Transformatoren. Aluminium hat dann als elektrischer Leiter Vorteile, wenn das Gewicht eine wesentliche Rolle spielt, beispielsweise bei den Leiterseilen von Freileitungen. Aus dem Grund der Gewichtsreduktion werden auch in Flugzeugen wie dem Airbus A380 Aluminiumkabel verwendet.

Aluminium wird unter anderem auch zu Stromschienen in Umspannwerken und zu stromführenden Gussteilen verarbeitet. Für Elektroinstallationen gibt es kupferkaschierte Aluminiumkabel, der Kupferüberzug ist zur Verbesserung der Kontaktgabe. In diesem Anwendungsbereichen sind primär Rohstoffpreise entscheidend, da Aluminium preisgünstiger als Kupfer ist. Für Oberleitungen bei elektrischen Bahnen ist es dagegen aufgrund seiner schlechten Kontakt- und Gleiteigenschaften ungeeignet, in diesem Bereich wird trotz des höheren Gewichts primär Kupfer eingesetzt.

Beim Kontaktieren unter Druck ist Aluminium problematisch, da es zum Kriechen neigt. Außerdem überzieht es sich an Luft mit einer Oxidschicht. Nach längerer Lagerung oder Kontakt mit Wasser ist diese isolierende Schicht so dick, dass sie vor der Kontaktierung beseitigt werden muss. Vor allem im Kontakt mit Kupfer kommt es zu Bimetallkorrosion. Bei ungeeigneten Kontaktierungen in Klemmen kann es bei Aluminiumleitern in Folge zu Ausfällen und Kabelbränden aufgrund sich lösender Kontakte kommen. Crimpverbindungen mit passenden Hülsen und Werkzeugen sind jedoch sicher. Als Zwischenlage zwischen Kupfer und Aluminium können Verbindungsstücke aus Cupal die Kontaktprobleme vermeiden.

Hervorzuheben ist das geringe Absinken der spezifischen elektrischen Leitfähigkeit von Aluminium bei Zusatz von Legierungsbestandteilen, wohingegen Kupfer bei Verunreinigungen eine deutliche Verringerung der Leitfähigkeit zeigt.

Die Elektronikindustrie setzt Aluminium aufgrund der guten Verarbeitbarkeit und der guten elektrischen und Wärme-Leitfähigkeit ein.

In integrierten Schaltkreisen wurde bis in die 2000er Jahre ausschließlich Aluminium als Leiterbahnmaterial eingesetzt. Bis in die 1980er-Jahre wurde es auch als Material für die Steuerelektrode (Gate) von Feldeffekttransistoren mit Metall-Isolator-Halbleiter-Struktur (MOSFET bzw. MOS-FET) verwendet. Neben dem geringen spezifischen Widerstand sind für die Verwendung die gute Haftung auf und geringe Diffusion in Siliciumoxiden (Isolationsmaterial zwischen den Leiterbahnen) sowie die einfache Strukturierbarkeit mithilfe von Trockenätzen ausschlaggebend. Seit Anfang der 2000er-Jahre wird Aluminium jedoch zunehmend durch Kupfer als Leiterbahnmaterial ersetzt, auch wenn dafür aufwendigere Strukturierungsverfahren (vgl. Damascene- und Dual-Damascene-Prozess) und Diffusionsbarrieren notwendig sind. Der höheren Fertigungsaufwand wird durch den geringeren spezifischen Widerstand, der im Fall von kleinen Strukturen bei Aluminium viel früher signifikant ansteigt und anderen Eigenschaften (z. B. Elektromigrationverhalten) überwogen und die Aluminium-Prozesse konnte die gestiegenen Anforderungen (Taktfrequenz, Verlustleistung, etc.) in mit hohen Frequenzen arbeitenden Schaltkreisen nicht mehr genügen (siehe auch RC-Glied).

Aluminium wird jedoch weiterhin auch in mikroelektronischen Produkten verwendet, so wird es wegen seiner guten Kontaktierbarkeit durch andere Metalle in den letzten Leiterbahnebenen eingesetzt, um den elektrischen Kontakt zu den bei der Flip-Chip-Montage eingesetzten Lotkügelchen herzustellen. Ähnlich verhält es sich bei Leistungshalbleitern, bei denen in der Regel alle Leiterbahnebenen aus Aluminium bestehen. Allgemein und insbesondere bei Leistungshalbleitern wird das Material für Bonddrähte (Verbindungsdrähte zwischen Chip und Gehäuseanschluss) verwendet.

Mit der Einführung der High-k+Metal-Gate-Technik hat Aluminium nach gut 25 Jahren Abstinenz auch im Bereich des Gates an Bedeutung gewonnen und wird neben anderen als Material zur Einstellung der Austrittsarbeit eingesetzt.

In der Verpackungsindustrie wird Aluminium zu Getränke- und Konservendosen sowie zu Aluminiumfolie verarbeitet. Dabei macht man sich die Eigenschaft der absoluten Barrierewirkung gegenüber Sauerstoff, Licht und anderen Umwelteinflüssen zunutze. Ausschlaggebend für die Verwendung von Aluminium als Verpackung ist nicht die geringe Dichte, sondern die gute Verarbeitbarkeit durch Walzen und die Ungiftigkeit. Dünne Folien werden in Stärken von sechs Mikrometern hergestellt und dann zumeist in Verbundsystemen eingesetzt, beispielsweise in Tetra Paks. Kunststofffolien können durch Bedampfen mit Aluminium mit einer dünnen Schicht versehen werden, welche dann eine hohe (aber nicht vollständige) Barrierefunktion aufweist. Grund dieser Barrierewirkung ist nicht das reine Aluminium, sondern die Passivschicht aus Böhmit. Wird diese verletzt, so kann Gas ungehindert durch den Werkstoff Aluminium strömen. Genutzt werden meist Reinaluminium, AlMn (Legierungen mit Mangan) und AlMg (Legierungen mit Magnesium).

Aus Aluminium werden auch Kochtöpfe und andere Küchengeräte, wie die klassische italienische Espressokanne, sowie Reise- und Militär-Geschirr hergestellt.

Aluminium wird für eine Vielzahl von Behältern und Gehäusen verarbeitet, da es sich gut durch Umformen bearbeiten lässt. Gegenstände aus Aluminium werden häufig durch eine Eloxalschicht vor Oxidation und Abrieb geschützt.

2017 entfielen 17 % der europäischen Aluminiumverwendung auf Verpackungen.

Aluminium wird aufgrund seines hohen Reflexionsgrades als Spiegelbeschichtung von Oberflächenspiegeln, unter anderem in Scannern, Kraftfahrzeug-Scheinwerfern und Spiegelreflexkameras aber auch in der Infrarotmesstechnik eingesetzt. Es reflektiert im Gegensatz zu Silber auch Ultraviolettstrahlung.
Aluminium-Spiegelschichten werden meist durch eine Schutzschicht vor Korrosion und Kratzern geschützt.

Aluminiumpulver und Aluminiumpasten werden zur Herstellung von Porenbeton eingesetzt. Man verwendet Verbindungen wie Aluminiumhydroxysulfat, Aluminiumdihydroxyformiat oder amorphes Aluminiumhydroxid als alkalifreie Spritzbetonbeschleuniger.

Aluminium wird als Konstruktionswerkstoff verwendet für tragende Teile von Bauwerken und als Funktionswerkstoff als dekorative, korrisionsbeständige Teile. Neben der Witterungsbeständigkeit ist vor allem die gute Verarbeitbarkeit ausschlaggebend, insbesondere bei handwerklicher Fertigung. Das Baugewerbe ist der Hauptabnehmer für Aluminiumprofile. Genutzt wird Aluminium hauptsächlich für Fensterrahmen, Türen und Elemente von Fassaden. Besonders bekannt ist die Fassade des Imperial War Museums in Manchester. Genutzt werden vor allem die Aluminium-Mangan-Legierungen die geringe Festigkeit und gute Korrosionsbeständigkeit haben. Teilweise wird Aluminium auch für den Brückenbau angewandt, wo sonst der Stahlbau vorherrscht. Für den konstruktiven Ingenieurbau werden Legierungen mit höherer Festigkeit genutzt, darunter AlMg und AlSi.
Bleche und Verbundplatten aus Aluminiumlegierungen erreichen Brandschutzklassen von 'nicht brennbar' bis 'normal entflammbar'.
Ein Wohnungsbrand entwickelt im Vollbrand 1000 °C Hitze was ungeachtet der Brandschutzklasse Löcher in die Aluminiumlegierung brennt, die zwischen 600 °C und 660 °C nach unten fließt oder tropft.
In der Raketentechnik besteht der Treibstoff von Feststoffraketen zu maximal 30 Prozent aus Aluminiumpulver, das bei seiner Verbrennung viel Energie freisetzt. Aluminium wird in Feuerwerken (s. a. Pyrotechnik) verwendet, wo es je nach Körnung und Mischung für farbige Effekte sorgt. Auch in Knallsätzen findet es oft Verwendung.

Bei der Aluminothermie wird Aluminium zur Gewinnung anderer Metalle und Halbmetalle verwendet, indem das Aluminium zur Reduktion der Oxide genutzt wird. Ein wichtiges Verfahren der Aluminothermie ist die Thermitreaktion, bei der Aluminium mit Eisen(III)-oxid umgesetzt wird. Bei dieser stark exothermen Reaktion entstehen Temperaturen bis zu 2500 °C und flüssiges Eisen, das zum aluminothermischen Schweißen genutzt wird, z. B. zum Fügen von Bahngleisen. Weitere Anwendungen der Reduktionswirkung von Aluminium werden für Laborzwecke ermöglicht, indem Aluminiumamalgam verwendet wird.

Aluminium dient als Pigment für Farben (Silber- oder Goldbronze). Farbig eloxiert ist es Bestandteil vieler Dekorationsmaterialien wie Flitter, Geschenkbänder und Lametta. Zur Beschichtung von Oberflächen wird es beim Aluminieren verwendet.

Mit Aluminium werden Heizelemente von Bügeleisen und Kaffeemaschinen umpresst.

Bevor es gelang, Zinkblech durch Titanzusatz als so genanntes Titanzink verarbeitbar zu machen, wurde Aluminiumblech für Fassaden- und Dachelemente (siehe Leichtdach) sowie Dachrinnen eingesetzt.

Wegen seiner hohen Wärmeleitfähigkeit wird Aluminium als Werkstoff für stranggepresste Kühlkörper und wärmeableitende Grundplatten verwendet. Aluminium-Elektrolytkondensatoren verbauen Aluminium als Elektrodenmaterial und Gehäusewerkstoff, weiters wird es zur Herstellung von Antennen und Hohlleitern verwendet.

Aluminium kommt in einigen Legierungen vor. Neben den Aluminiumlegierungen die überwiegend aus Aluminium bestehen, kommt es noch vor in den Kupferlegierungen Aluminiumbronze, Aluminiummessing, Isabellin, zu etwa gleichen Teilen Al und Kupfer in der Devardaschen Legierung, als Hauptlegierungselement für Magnesiumlegierungen sowie in Alnico und Sendust, zwei Eisenlegierungen mit besonderen magnetischen Eigenschaften. In vielen Titanlegierungen kommt ebenfalls Aluminium vor, insbesondere in Ti6Al4V, der Sorte die etwa 50 % aller Titanlegierungen ausmacht. Dort ist Aluminium mit 6 Massenprozent enthalten.

Bei der Verarbeitung wird unterschieden, ob es sich um Gusslegierungen handelt oder um Knetlegierungen:

Danach werden die Einzelteile verbunden durch Schweißen, Nieten, Löten und ähnliche Verfahren.

Das Gießen von Aluminium wird als Aluminiumguss bezeichnet. Es gehört aufgrund seines vergleichsweise geringen Schmelzpunktes von 660 °C (Gusseisen etwa 1150 °C, Stahl 1400 °C bis 1500 °C) und seiner guten Gießbarkeit zu den häufig in der Gießerei verwendeten Werkstoffen. AlSi, spezielle Gusslegierungen mit Silicium, haben sogar Schmelzpunkte um 577 °C. Der Massenanteil von Aluminium aller in Gießereien erzeugten Produkte beträgt etwa 11 % (Gusseisen 76 %, Stahlguss 9 %) und ist damit in der Gießerei das mit Abstand wichtigste Nichteisenmetall (NE-Metalle) noch vor Kupfer mit 1,5 %. Der Anteil am NE-Metallguss von Aluminium beträgt etwa 87 %. In Deutschland wurden 2011 etwa 840.000 Tonnen Aluminium in Gießereien verarbeitet; Etwa 76 % des Nichteisenmetall-Gusses wird von der Automobilbranche abgenommen.

Aus dem niedrigen Schmelzpunkt folgt ein geringer Energieeinsatz beim Schmelzvorgang sowie eine geringere Temperaturbelastung der Formen.
Aluminium eignet sich grundsätzlich für alle Gussverfahren, insbesondere für Druckguss bzw. Aluminiumdruckguss, mit denen auch kompliziert geformte Teile gefertigt werden können. In der Gießerei werden besondere Aluminiumgusslegierungen verarbeitet, größtenteils die Aluminium-Silicium-Legierungen. In den Hüttenwerken werden dagegen meist Knetlegierungen erzeugt, die zur Weiterbearbeitung durch Walzen, Schmieden und Fließpressen gedacht sind. Diese werden in den Hüttenwerken vergossen zu Barren (Blockguss) oder zu Rundbarren, die theoretisch endlos sein können (Strangguss). Seit den 1930er Jahren kommt der Strangguss vermehrt zum Einsatz. Dafür gibt es spezielle Anlagen die bis zu 96 Rundbarren gleichzeitig herstellen können mit Gießlängen zwischen 3 und 7 Metern teils bis zu 10 Metern. Die Durchmesser liegen bei 75 bis 700 mm. Bleche werden manchmal hergestellt durch Gießen direkt auf eine Walze die die Schmelze kühlt. Das Rohblech wird danach direkt kaltgewalzt ohne Warmwalzen, was Kosten von bis zu 60 % spart.

Etwa 74 Prozent des Aluminiums wird durch Umformen bearbeitet. Hierzu zählt unter anderem das Walzen, Schmieden, Strangpressen und Biegen.

Rein- und Reinstaluminium lässt sich wegen der niedrigen Festigkeit gut umformen und verfestigt sich bei Kaltumformung, wobei große Formänderungen möglich sind. Die Verfestigung lässt sich durch Rekristallisationsglühen beseitigen. Knetlegierungen mit AlMg und AlMn erreichen ihre höhere Festigkeit durch die Legierungselemente und durch Kaltverformung. Die aushärtbaren Legierungen AlMgSi, AlZnMg, AlCuMg und AlZnMgCu scheiden bei Umformung festigkeitssteigernde Phasen aus; sie lassen sich relativ schwierig umformen.

Gegossene Barren werden häufig durch Walzen weiterverarbeitet, entweder zu dicken Platten die anschließend durch Fräsen zu Endprodukten werden, zu Blechen die durch Stanzen und Biegen weiterverarbeitet werden oder zu Folien. Beim Walzen ändert sich die Mikrostruktur der Werkstoffe: Kleine kugelförmige Bestandteile die häufig nach dem Gießen vorliegen, werden plattgedrückt und in die Länge gezogen. Das Gefüge wird dadurch einerseits feiner und gleichmäßiger, andererseits aber auch Richtungsabhängig. Die Kapazität einer Aluminium-Warmwalzanlage liegt bei etwa 800.000 Tonnen pro Jahr. Verarbeitet werden Barren mit bis zu 30 Tonnen Masse. Sie haben Abmessungen von bis zu 8,7 Metern Länge, 2,2 Metern Breite und 60 cm Dicke. Noch größere Barren können technisch verarbeitet werden, die Gefügequalität nimmt dann aber ab. Nach dem Warmwalzen liegt der Werkstoff meist mit Dicken von etwa 20 bis 30 mm vor. Anschließend folgt das Kaltwalzen auf Enddicke. Kaltwalzwerke haben Kapazitäten von 300.000 bis 400.000 Jahrestonnen. Verbundwerkstoffe können durch Walzplattieren hergestellt werden. Dabei wird ein- oder zweiseitig eine Schicht aus einem anderen Werkstoff aufgebracht. Häufig wird auf korrosionsanfälliges Kernmaterial eine Schicht aus korrosionsbeständigem Reinaluminium aufgebracht.


Aluminium lässt sich durch Strangpressen in komplizierte Konstruktionsprofile formen; hierin liegt ein großer Vorteil bei der Fertigung von Hohlprofilen (z.B. für Fensterrahmen, Stäbe, Balken), Kühlkörper­profilen oder in der Antennentechnik. Die Herstellung von Halbzeug oder Bauteilen geschieht aus Vormaterial wie etwa Walzbarren, Blech oder Zylindern. Aluminiumlegierungen lassen sich deutlich besser strangpressen als andere Werkstoffe, weshalb ein großer Teil des Aluminiums mit diesem Verfahren verarbeitet wird. Dabei wird das Ausgangsmaterial durch ein hohles Werkzeug gepresst. Es entsteht Endlosmaterial das in der gewünschten Länge abgesägt wird. Es können auch komplizierte Querschnitte hergestellt werden, beispielsweise Hohlprofile oder welche mit Hinterschneidungen. Der Querschnitt ist allerdings über die Länge konstant. Mit hochfesten Legierungen sind große Mindestwanddicken erforderlich und das Pressen dauert lange, weshalb eher die mittelfesten, aushärtbaren Legierungen bevorzugt werden. Die Aushärtung wird meist direkt im Anschluss durchgeführt. Beim Strangpressen wird der Werkstoff auf Temperaturen von etwa 450 bis 500 °C erwärmt um die Umformbarkeit zu erhöhen, was gleichzeitig zum Lösungsglühen genutzt wird. Direkt nach dem Strangpressen wird das Werkstück durch Luft oder Wasser starkt abgekühlt und so abgeschreckt was zu höheren Festigkeiten führt.

Ein Mischverfahren aus Gießen und Schmieden ist Cobapress, welches speziell für Aluminium ausgelegt ist und häufig in der Automobilbranche genutzt wird. Moderne Walzwerke sind sehr teuer, aber auch sehr produktiv.

Zum Zerspanen zählt das Drehen, Bohren und Fräsen. Aluminiumwerkstoffe sind gut spanbar. Ihre genauen Eigenschaften hängen jedoch von der Legierung und Gefügezustand ab. Zu beachten ist, dass die bei der Bearbeitung auftretenden Temperaturen schnell im Bereich des Schmelzpunktes liegen können. Bei gleichen Schnittparametern wie bei Stahl resultiert bei Aluminium allerdings eine geringere mechanische und thermische Belastung. Als Schneidstoff wird oft Hartmetall für untereutektische oder Diamant für die stark verschleißenden übereutektischen Legierungen verwendet.
Insbesondere die Bearbeitung von eloxierten Werkstücken erfordert harte Werkzeuge, um Verschleiß durch die harte Eloxalschicht zu vermeiden. Die beim Schleifen von Aluminium entstehenden Schleifstäube können zu einem erhöhten Explosionsrisiko führen.

Grundsätzlich sind alle Aluminium-Werkstoffe zum Schweißen geeignet, wobei jedoch reines Aluminium zu Poren in der Schweißnaht neigt. Außerdem neigt die Aluminiumschmelze zu Reaktionen mit der Atmosphäre, weshalb fast immer unter Schutzgas geschweißt wird. Gut geeignet sind das MIG- und Plasmaschweißen sowie das WIG-Schweißen. Bei letzterem wird bei Nutzung von Wechselstrom Argon als Schutzgas verwendet, und bei Gleichstrom Helium. Für das Laserschweißen eignen sich sowohl Kohlendioxid- als auch Festkörperlaser, allerdings nicht für alle Legierungen. Wegen der hohen Wärmeleitfähigkeit erstarrt die Schmelze sehr schnell, sodass die Schweißnaht zu Poren und Rissen neigt. Das Widerstandspunktschweißen erfordert, verglichen mit Stahl, höhere Ströme und kürzere Schweißzeiten sowie teilweise spezielle Geräte, da die handelsüblichen nicht geeignet sind. Für das Elektronenstrahlschweißen eignen sich alle Legierungen, jedoch neigen Magnesium und Zinn zum Verdampfen während des Schweißvorgangs. Lichtbogenhandschweißen wird nur noch selten verwendet, meist zur Gussnachbesserung. Löten gestaltet sich wegen der sich bildenden Oxidschicht an Luft schwierig. Genutzt werden sowohl Hart- als auch Weichlöten mit speziellen Flussmitteln. Alternativ kann Aluminium auch ohne Flussmittel mit Ultraschall gelötet werden, dabei wird die Oxidschicht mechanisch während des Lötvorganges aufgebrochen.

Aluminium ist kein essentielles Spurenelement und gilt für die menschliche Ernährung als entbehrlich. Im menschlichen Körper befinden sich durchschnittlich etwa 50 bis 150 Milligramm Aluminium. Diese verteilen sich zu ungefähr 50 Prozent auf das Lungengewebe, zu 25 Prozent auf die Weichteile und zu weiteren 25 Prozent auf die Knochen. Aluminium ist damit ein natürlicher Bestandteil des menschlichen Körpers.

99 bis 99,9 Prozent der üblicherweise in Lebensmitteln aufgenommenen Menge von Aluminium (10 bis 40 mg pro Tag) werden unresorbiert über den Kot wieder ausgeschieden. Chelatbildner "(Komplexbildner)" wie Citronensäure können die Resorption auf 2 bis 3 Prozent steigern. Auch die Aufnahme von Aluminiumsalzen über den Magen-Darm-Trakt ist gering; sie variiert aber in Abhängigkeit von der chemischen Verbindung und ihrer Löslichkeit, dem pH-Wert und der Anwesenheit von Komplexbildnern. Die Eliminierung von in den Organismus gelangten wasserlöslichen Aluminiumsalzen erfolgt vorwiegend über den Urin, weniger über den Kot. Bei Dialysepatienten mit einer eingeschränkten Nierenfunktion besteht daher ein erhöhtes Risiko einer Akkumulation im Körper mit toxischen Effekten, etwa Knochenerweichungen und Schäden des Zentralnervensystems; zusätzlich sind Dialysepatienten aufgrund für sie notwendiger pharmazeutischer Produkte (Phosphatbinder) einer höheren Aluminiumzufuhr ausgesetzt.

Im Blut ist Al überwiegend (zu etwa 80 %) an Transferrin gebunden. 16 Prozent liegen als [Al(PO)(OH)], 1,9 Prozent als Citrat-Komplex, 0,8 Prozent als Al(OH) und 0,6 Prozent als [Al(OH)] vor.

Aluminium in Form verschiedener Salze (Phosphate, Silikate) ist Bestandteil vieler Pflanzen und Früchte, denn gelöste Al-Verbindungen werden durch Regen aus den Böden von den Pflanzen aufgenommen, bei Säurebelastung der Böden infolge sauren Regens ist dies vermehrt der Fall (siehe dazu auch Waldschäden).

Ein großer Teil des Bodens auf der Welt ist chemisch sauer. Liegt der pH-Wert unter 5,0, werden Al-Ionen von den Wurzeln der Pflanzen aufgenommen. Dies ist bei der Hälfte des bebaubaren Lands auf der Welt der Fall. Die Ionen schädigen insbesondere das Wurzelwachstum der Feinwurzeln. Die Pflanze, wenn sie nicht Aluminium-tolerant ist, steht dann unter Stress. Zahlreiche Enzyme und signalübertragende Proteine sind betroffen; die Folgen der Vergiftung sind noch nicht vollständig bekannt. In sauren metallhaltigen Böden ist Al das Ion mit dem größten Potenzial zur Schädigung. Von der Modellpflanze "Arabidopsis" sind Transgene bekannt, die deren Aluminium-Toleranz heraufsetzen und auch bei Kulturpflanzen sind tolerante Sorten bekannt.

Der saure Regen hat beispielsweise in Schweden in den Sechzigerjahren des letzten Jahrhunderts die Seen übersäuert, wodurch mehr Al-Ionen in Lösung gingen und empfindliche Fische verendeten. Auch in Norwegen wurde dieser Zusammenhang bei einem Forschungsprojekt in den 1970er-Jahren festgestellt.

Bei pH-Werten über 5,0 ist Aluminium als polymeres Hydroxykation an der Oberfläche von Silicaten gebunden. Bei pH-Werten von 4,2 bis 5 steigt Anteil von mobilen Kationen.

Bei Erhöhung der Schwefelsäurekonzentration durch sauren Regen bildet sich Aluminiumhydroxysulfat:

Die meisten Lebensmittel enthalten in Spurenmengen auch Aluminium. Unverarbeitete pflanzliche Lebensmittel enthalten durchschnittlich weniger als 5 mg/kg in der Frischmasse. Dabei streuen die Werte aufgrund unterschiedlicher Sorten, Anbaubedingungen und Herkunft in erheblichem Maße. So weisen beispielsweise Salat und Kakao deutlich höhere Durchschnittswerte auf. Schwarzer Tee kann Gehalte von bis zu 1042 mg/kg in der Trockenmasse aufweisen.

Beim Kochen oder Aufbewahren in Aluminiumgeschirr oder in Alufolie kann es (außer bei sauren Lebensmitteln) nach einer Schätzung zu einer maximalen zusätzlichen Aufnahme von 3,5 mg/Tag/Person kommen. Bei sauren Lebensmitteln wie Sauerkraut oder auch Tomaten können aufgrund der Säurelöslichkeit wesentlich höhere Werte erreicht werden.

Trink- und Mineralwässer weisen mit durchschnittlich 0,2–0,4 mg/l im Gegensatz zur Nahrung geringe Gehalte auf und stellen somit nur einen kleinen Beitrag zur täglichen Aluminium-Aufnahme. Die Trinkwasserverordnung legt einen Grenzwert von 0,2 mg/l fest. Trinkwasser darf in Deutschland, Österreich und der Schweiz keine höheren Werte aufweisen.

Nach einer Schätzung nimmt der erwachsene Europäer im Durchschnitt zwischen 1,6 und 13 mg Aluminium pro Tag über die Nahrung auf. Dies entspricht einer wöchentlichen Aufnahme von 0,2 bis 1,5 mg Aluminium pro kg Körpergewicht bei einem 60 kg schweren Erwachsenen. Die großen Unsicherheiten beruhen auf den unterschiedlichen Ernährungsgewohnheiten und der variablen Gehalte an Aluminium in den Lebensmitteln.

Die Europäische Behörde für Lebensmittelsicherheit (Efsa) legt eine tolerierbare wöchentliche Aufnahme (TWI) von 1 mg Aluminium pro kg Körpergewicht fest.

Aluminium ist als Lebensmittelzusatzstoff unter der Bezeichnung E 173 ausschließlich als Farbstoff für Überzüge von Zuckerwaren und als Dekoration von Kuchen und Keksen erlaubt. Weiterhin ist Aluminium zum Färben von Arzneimitteln und Kosmetika zugelassen.

Bei der Untersuchung von Laugengebäck (Brezeln, Stangen, Brötchen) aus Bäckereien wurde Aluminium nachgewiesen, das in das Lebensmittel gelangt, wenn bei der Herstellung von Laugengebäck Aluminiumbleche verwendet werden.

Während Bier in Aluminiumfässern transportiert wird, hat sich für den Weintransport der Werkstoff Aluminium nicht durchgesetzt. Ein kurzfristiger Kontakt schadet nicht, doch können nach längerem Kontakt Weinfehler in Geruch und Geschmack oder als Trübung auftreten, vor allem beim offenen Stehen an der Luft.

Bei eingeschränkter Nierenfunktion und bei Dialyse-Patienten führt die Aufnahme von Aluminium zu progressiver Enzephalopathie (Gedächtnis- und Sprachstörungen, Antriebslosigkeit und Aggressivität) durch Untergang von Hirnzellen und zu fortschreitender Demenz, zu Osteoporose (Arthritis) mit Knochenbrüchen und zu Anämie (weil Aluminium dieselben Speichereiweiße wie Eisen besetzt).

Speziell im Hinblick auf die Verwendung in Deodorants und Lebensmittel-Zusatzstoffen werden die gesundheitlichen Auswirkungen von Aluminium kontrovers diskutiert. So wurde Aluminium mehrfach kontrovers als Faktor im Zusammenhang mit der Alzheimer-Krankheit in Verbindung gebracht.

Laut einer Studie des Bundesinstituts für Risikobewertung (BfR) vom Juli 2007 wurde im allgemeinen Fall zum Zeitpunkt der Erstellung der Studie aufgrund der vergleichsweise geringen Menge kein Alzheimer-Risiko durch Aluminium aus Bedarfsgegenständen erkannt; jedoch sollten vorsorglich keine sauren Speisen in Kontakt mit Aluminiumtöpfen oder -folie aufbewahrt werden. Eine Neubewertung erfuhr im Februar 2014 die Verwendung von aluminiumhaltigen Deodorants und Kosmetikartikel durch das Bundesinstitut für Risikobewertung: Aluminiumsalze aus solchen Produkten können durch die Haut aufgenommen werden, und die regelmäßige Benutzung über Jahrzehnte hinweg könnte möglicherweise zu gesundheitlichen Beeinträchtigungen beitragen.

Die britische Alzheimer-Gesellschaft mit Sitz in London vertritt den Standpunkt, dass die bis 2008 erstellten Studien einen kausalen Zusammenhang zwischen Aluminium und der Alzheimer-Krankheit nicht überzeugend nachgewiesen haben. Dennoch gibt es einige Studien, wie z. B. die PAQUID-Kohortenstudie in Frankreich, mit einer Gesundheitsdatenauswertung von 3777 Personen im Alter ab 65 Jahren seit 1988 bis zur Gegenwart, in welchen eine Aluminium-Exposition als Risikofaktor für die Alzheimer-Krankheit angegeben wird. Demnach wurden viele senile Plaques mit erhöhten Aluminium-Werten in Gehirnen von Alzheimer-Patienten gefunden. Es ist jedoch unklar, ob die Aluminium-Akkumulation eine Folge der Alzheimer-Krankheit ist, oder ob Aluminium in ursächlichem Zusammenhang mit der Alzheimer-Krankheit zu sehen ist.

Aluminium gehört zu den nicht essentiellen Spurenelementen, bei der Toxizität kommt es im Wesentlichen auf die Menge an: 0,01 mg/l Aluminium im Blut gilt als Normalwert, Werte über 0,06 mg/l sprechen für übermäßige Belastung und Werte über 0,2 mg/l im Blut gelten als toxisch.

Die Herstellung von Aluminium ist sehr energieaufwendig. Allein für die Schmelzflusselektrolyse zur Gewinnung eines Kilogramms Aluminium werden je nach Errichtungsdatum und Modernität der Anlage zwischen 12,9 und 17,7 kWh elektrische Energie benötigt. Bei der Stromerzeugung für die Produktion von einem Kilogramm Aluminium werden im deutschen Kraftwerkspark 8,4 kg CO freigesetzt, im weltweiten Durchschnitt etwa 10 kg. Es ist aber auch zu bedenken, dass aufgrund des Kostenfaktors Energie die Elektrolyse verstärkt an Orten erfolgt, an denen auf billige, CO-emissionsarme Wasserkraft zurückgegriffen werden kann, wie etwa in Brasilien, Kanada, Venezuela oder Island. Allerdings ist auch bei Verwendung von Elektrizität aus vollständig regenerativen Energien die Produktion von Aluminium nicht CO-frei, da der bei der Schmelzflusselektrolyse entstehende Sauerstoff mit dem Kohlenstoff der Elektroden zu CO reagiert. Die Verbrauchswerte für Roh-Aluminium erhöhen sich durch Transport- und Verarbeitungsanteile für das Wiederaufschmelzen, Gießen, Schleifen, Bohren, Polieren etc. auf ca. 16,5 kg CO pro kg Aluminium-Konsumgut.

Die europaweite Recyclingrate von Aluminium liegt bei 67 Prozent. In Österreich gelangen (laut einer Studie aus dem Jahr 2000) 16.000 Tonnen Aluminium pro Jahr "über Verpackungen" in den Konsum, ebenso gelangen 16.000 Tonnen Aluminium ohne Wiederverwertung in den Hausmüll (dabei sind unter anderem auch die Aluminiumhaushaltsfolien eingerechnet, die nicht als „Verpackung“ gelten). 66 Prozent der Verpackungen im Restmüll sind Aluminium[getränke]dosen. Diese liegen nach der Müllverbrennung in der Asche noch metallisch vor und machen in Europa durchschnittlich 2,3 Prozent der Asche aus. In der EU werden durchschnittlich 70 Prozent des in der Bodenasche enthaltenen Aluminiums zurückgewonnen.

Durch den Abbau des Erzes Bauxit werden große Flächen in Anspruch genommen, die erst nach einer Rekultivierung wieder nutzbar werden. Um eine Tonne Aluminium herzustellen, werden vier Tonnen Bauxit benötigt. Dies erzeugt zehn Tonnen Abraum. Zudem entstehen bei der Herstellung des Aluminiumoxids nach dem Bayer-Verfahren ca. drei Tonnen von eisenreichem alkalischen Rotschlamm, der kaum wiederverwertet wird und dessen Deponierung oder sonstige „Entsorgung“ große Umweltprobleme aufwirft (siehe entsprechende Abschnitte unter "Rotschlamm" und "Bauxitbergbau in Australien").

Positiv ist hingegen die gute Wiederverwendbarkeit von Aluminium hervorzuheben, wobei die Reststoffe streng getrennt erfasst und gereinigt werden müssen (Aluminiumrecycling, Recycling-Code-41 (ALU)). Aluminium ist dabei besser rezyklierbar als Kunststoffe, wegen Downcycling bei nicht sortenreiner Erfassung jedoch etwas schlechter wiederverwertbar als Stahl. Beim Aluminiumrecycling wird nur 5 Prozent der Energiemenge der Primärproduktion benötigt.
Durch Leichtbau mit Aluminiumwerkstoffen (beispielsweise Aluminiumschaum, Strangpressprofile) wird Masse von beweglichen Teilen und Fahrzeugen gespart, was zur Einsparung von Treibstoff führen kann.

Aluminium ist durch seine Selbstpassivierung korrosionsbeständiger als Eisen und erfordert daher weniger Korrosionsschutzmaßnahmen.

Aluminiumsalze weist man durch Glühen mit verdünnter Kobaltnitratlösung (Co(NO)) auf der Magnesia-Rinne nach. Dabei entsteht das Pigment Thénards Blau, ein Cobaltaluminiumspinell mit der Formel CoAlO. Es wird auch Kobaltblau oder Cobaltblau, Dumonts Blau, Coelestinblau, Cobaltaluminat oder – nach dem Entdecker des Pigments, Josef Leithner, – Leithners Blau genannt.

Die Probelösung wird alkalisch gemacht, um Aluminium als Aluminiumhydroxid Al(OH) zu fällen. Der Niederschlag wird abfiltriert und mit einigen Tropfen Phenolphthalein versetzt, dann gewaschen, bis keine Rotfärbung durch Phenolphthalein mehr vorhanden ist. Anschließend festes Natriumfluorid (NaF) auf den Niederschlag streuen: Es bildet sich eine Rotfärbung durch Phenolphthalein, verursacht von freigesetzten Hydroxidionen bei der Bildung von Kryolith Na[AlF].

Die Probe wird mit Salzsäure (HCl) versetzt und eventuell vorhandenes Aluminium somit gelöst. Anschließend wird die Probelösung mit Kaliumhydroxid (KOH) stark alkalisch gemacht. Gibt man nun einige Tropfen der Probelösung zusammen mit der gleichen Menge Morin-Lösung auf eine Tüpfelplatte und säuert anschließend mit konzentrierter Essigsäure ("Eisessig", CHCOOH) an, so ist unter UV-Strahlung (λ = 366 nm) eine grüne Fluoreszenz beobachtbar. Der Nachweis ist dann sicher, wenn diese Fluoreszenz bei Zugabe von Salzsäure wieder verschwindet.

Grund hierfür ist, dass Al(III) in neutralen sowie essigsauren Lösungen in Verbindung mit Morin eine fluoreszierende kolloidale Suspension bildet.

Unter besonderen Bedingungen tritt Aluminium auch einwertig auf. Diese Verbindungen werden zur Gewinnung von hochreinem Aluminium genutzt (Subhalogeniddestillation).







</doc>
<doc id="95" url="https://de.wikipedia.org/wiki?curid=95" title="Antimon">
Antimon

Antimon [] (von lateinisch "Antimonium", vermutlich von arabisch „al-ithmîd(un)“ (, Antimonsulfid bzw. Stibnit)) ist ein chemisches Element mit dem Elementsymbol Sb (von ‚(Grau-)Spießglanz‘) und der Ordnungszahl 51. Im Periodensystem steht es in der 5. Periode und der 5. Hauptgruppe, bzw. 15. IUPAC-Gruppe oder Stickstoffgruppe. In der stabilen Modifikation ist es ein silberglänzendes und sprödes Halbmetall.

Es wird auch vermutet, dass der Name auf das spätgriechische "anthemon" (dt. "Blüte") zurückgeht. Damit sollten die stängelartigen Kristalle von Antimonsulfid beschrieben werden, die büschelförmig erschienen und wie eine Blüte aussähen. Im 11. Jahrhundert findet sich der lateinische Begriff für die mineralische Arzneidroge "antimonium" zur innerlichen Therapie von Krankheiten dann bei Constantinus Africanus.

Im 17. Jahrhundert ging der Name Antimon als Bezeichnung auf das Metall über. Die koptische Bezeichnung für das Schminkpuder Antimonsulfid ging über das Griechische in das Lateinische "stibium" über. Die vom schwedischen Mediziner und Chemiker Jöns Jakob Berzelius („Vater der modernen Chemie“) benutzte Abkürzung Sb wird noch heute als Elementsymbol genutzt.

Eine späte legendäre Volksetymologie, die von Samuel Johnson in seinem Wörterbuch verewigt wurde, besagt, dass der deutsche Mönch Basilius Valentinus die Beobachtung machte, dass Schweine durch die Aufnahme von Antimon schnell fett wurden. Er probierte dies auch an seinen Ordensbrüdern aus, woraufhin diese allerdings starben, sodass der Begriff „antimoine“ ("antimönchisch") geprägt wurde, aus dem später „Antimon“ entstanden sei.

Als Typlokalität für gediegenes Antimon gilt die Silbermine in der schwedischen Gemeinde Sala im Västmanland. Allerdings war metallisches Antimon schon den Chinesen und Babyloniern bekannt. Einige seiner Verbindungen wurden schon in der Bronzezeit als Zuschlag zu Kupfer verwendet, um Bronze herzustellen (Funde von Velem-St. Vid in Ungarn).

Antimon ist ein selten vorkommendes Element. Da es in der Natur auch gediegen (das heißt in elementarer Form) gefunden werden kann, wird es von der International Mineralogical Association (IMA) unter der System-Nr. 1.CA.05 als Mineral anerkannt.

Weltweit konnte gediegenes Antimon bisher (Stand: 2011) an rund 300 Fundorten nachgewiesen werden. So unter anderem in mehreren Regionen von Australien; in den bolivianischen Departements La Paz und Potosí; Minas Gerais in Brasilien; Schwarzwald, Fichtelgebirge, Oberpfälzer Wald, Odenwald und im Harz in Deutschland; Seinäjoki in Finnland; mehreren Regionen von Frankreich; Lombardei, Piemont, Sardinien und Trentino-Südtirol in Italien; einigen Regionen von Kanada; einigen Regionen von Österreich; Ost- und Westsibirien und Ural in Russland; neben Västmanland noch Dalarna, Gästrikland, Närke, Södermanland, Värmland und Västerbotten in Schweden; in einigen Regionen der Slowakei; Böhmen und Mähren in Tschechien sowie in vielen Regionen der USA. Eine der weltweit bedeutendsten Lagerstätten für gediegen Antimon und Antimonerze ist der "Murchison greenstone belt" in der Murchison Range von Südafrika.

Bisher sind 264 Antimon-Minerale bekannt (Stand: 2010). Industriell genutzt wird überwiegend das Sulfid-Mineral Stibnit SbS (Grauspießglanz) mit einem Gehalt von maximal 71,7 % Sb. Das Mineral mit dem höchsten Sb-Gehalt in einer chemischen Verbindung ist die natürliche Antimon-Arsen-Legierung Paradocrasit (max. 92 %). Allerdings kommt sie mit nur drei Fundorten, im Gegensatz zum Stibnit (rund 2500 Fundorte), sehr viel seltener vor. Weitere Quellen für Antimon sind die Minerale Valentinit SbO (Weißspießglanz), Breithauptit NiSb (Antimonnickel, Nickelantimonid), Kermesit SbSO (Rotspießglanz) und SbS (Goldschwefel).

Technisch wird Antimon aus dem Antimonglanz gewonnen. Ein Verfahren beruht auf dem Abrösten und der Reduktion mit Kohlenstoff (Röstreduktionsverfahren):

Eine andere Möglichkeit besteht darin, die Reduktion mit Eisen durchzuführen (Niederschlagsverfahren):

Weltweit wurden zu Beginn des 21. Jahrhunderts zwischen 110.000 und 160.000 Tonnen pro Jahr an Antimon gefördert. Seit 1900 hat sich damit die Fördermenge mehr als verzehnfacht.

87 % der Antimonproduktion findet in China statt (Stand: 2015).

Antimon kann in drei verschiedenen Modifikationen auftreten, wobei metallisches bzw. "graues" Antimon die beständigste Modifikation ist.

Durch Abschrecken von Antimondampf an kalten Flächen entsteht amorphes, "schwarzes" und sehr reaktives Antimon, welches sich durch Erhitzen wieder in metallisches Antimon umwandelt. Durch elektrolytische Herstellung entsteht "explosives" Antimon, das beim Ritzen explosionsartig aufglühend und funkensprühend in metallisches Antimon übergeht. Diese Form enthält jedoch immer etwas Chlor und kann nicht als Modifikation betrachtet werden. "Gelbes" Antimon ist ebenfalls keine eigenständige Modifikation, sondern eine hochpolymere chemische Verbindung mit Wasserstoff.

Unter Normalbedingungen kristallisiert Antimon trigonal in rhomboedrischer Aufstellung in der nach der Hermann-Mauguin-Symbolik beschriebenen Raumgruppe  mit den Gitterparametern "a" = 431 pm und "c" = 1127 pm sowie sechs Formeleinheiten pro Elementarzelle.

Metallisches Antimon ist silberweiß, stark glänzend, blättrig-grobkristallin. Es lässt sich aufgrund seiner Sprödigkeit leicht zerkleinern. Elektrische und thermische Leitfähigkeit sind gering.

Mit naszierendem Wasserstoff reagiert Antimon zum instabilen Antimonhydrid SbH. Von Luft und Wasser wird Antimon bei Raumtemperatur nicht angegriffen. Oberhalb des Schmelzpunkts verbrennt es in Luft mit bläulich-weißer Flamme zu Antimon(III)-oxid. In heißen konzentrierten Mineralsäuren löst es sich auf. Mit den Halogenen reagiert es schon bei Raumtemperatur heftig zu den entsprechenden Halogeniden.

In Verbindungen liegt Antimon überwiegend in den Oxidationsstufen +3 und +5 vor. In Metallantimoniden wie Kaliumantimonid KSb bildet es Sb-Ionen.

Es existieren zwei stabile Antimon-Isotope: Sb und Sb.

Der überwiegende Teil des hergestellten Antimons wird zu Legierungen verarbeitet und zeigt dabei folgende Eigenschaften:

Wichtige Legierungen:

Antimon (bzw. ein aus Antimonerz gewonnenes Präparat) wurde im 16. und 17. Jahrhundert zu einem (iatrochemischen) „Leitarzneimittel“, war aber – wie auch andere paracelsische Medikamente – umstritten und in Frankreich zwischen 1615 und 1688 auch verboten.

Brechweinstein wurde lange als brechreizerregendes Mittel verwendet (Antimonpille), heute wird es noch manchmal verwendet, um den Mageninhalt von Vögeln zu untersuchen.

Sowohl Schistosomiasis als auch Trypanosomen wurden beginnend Anfang des 19. Jahrhunderts mit Brechweinstein (Kaliumantimonyltartrat) bekämpft. Brechweinstein wurde hergestellt, indem man für einen Tag Wein in einem Antimonbecher lagerte, und diesen dann austrank. Inzwischen kommen effektivere und verträglichere Medikamente zur Anwendung.

Antimonpräparate werden meist als weniger toxische pentavalente Formen zur medikamentösen Therapie der Leishmaniose und Schistosomiasis eingesetzt, allerdings in entwickelten Ländern nicht mehr als Mittel der ersten Wahl. Hierbei hemmt Antimon das Enzym Phosphofructokinase, das den geschwindigkeitsbestimmenden Schritt der Glykolyse darstellt.


Antimon kann bereits bei Ingestion von 200 bis 1200 mg tödlich sein. In der Toxikologie sind drei Antimon-Formen bekannt, von denen das gasförmige Antimonhydrid (Stiban, SbH) die gefährlichste Form ist, die eine massive Hämolyse induziert. Nach der Toxizität folgt Brechweinstein mit dreiwertigem („trivalentem“) Antimon, während fünfwertiges Antimon am wenigsten toxisch ist.

Das trivalente Antimon wird innerhalb der ersten zwei Stunden nach der Einnahme zu 95 % in rote Blutkörperchen aufgenommen und damit vorwiegend in stark durchbluteten Organen angereichert. Die Exkretion erfolgt vorwiegend durch Bindung an Glutathion über die Galle mit entsprechend hohem enterohepatischen Kreislauf, und nur ein geringer Teil wird über die Nieren ausgeschieden. Kaliumantimonyltartrat wird zu 90 % innerhalb des ersten Tages nach Aufnahme ausgeschieden, die übrigen 10 % aufgrund einer langsameren Eliminationskinetik über 16 Tage.

Es wird vermutet, dass Antimon ähnlich wie Arsen die Funktion des Pyruvatdehydrogenase-Komplexes hemmt und somit zu einem Mangel des intrazellulären Energieträgers Adenosintriphosphat (ATP) führt. Dabei kommt es zur Bildung von Chelatkomplexen zwischen dem Antimon und Thiol-Gruppen der entsprechenden Enzyme. Im Körper wirkt es in zahlreichen Organen toxisch, so im Verdauungstrakt, in der Leber, in den Nieren, im Herz und im Zentralnervensystem. Die höchste Konzentration erreicht Antimon in der Leber, wo es zu einer Hepatitis bis hin zum Leberversagen kommen kann. Am Herzen kommt es zu EKG-Veränderungen mit Inversion und Verminderung der T-Welle und verlängertem QT-Intervall. Ein akutes Nierenversagen kann zur temporären oder permanenten Hämodialyse führen.

Therapeutisch erfolgt bei einer Antimon-Vergiftung neben unterstützenden Maßnahmen wie Infusionstherapie (sowohl zum Ausgleich des Flüssigkeitsverlustes durch das Erbrechen als auch zum Schutz der Nieren), und engmaschiger Überwachung der Vitalfunktionen und des EKGs die Gabe von Aktivkohle, N-Acetylcystein als Vorläufer des Glutathions zur vermehrten Sekretion und eines Chelatbildners, z. B. Dimercaprol.

Ergebnisse aus Untersuchungen deuten darauf hin, dass Antimonverbindungen Haut und Schleimhäute reizen. Diese Verbindungen lösen sich vermutlich aus Kunststoff und Textilien.

Von den Antimonverbindungen sind seitens der EU Antimonfluorid als giftig (T) und die Chloride als ätzend (C) eingestuft, außerdem als umweltgefährlich (N); alle anderen Antimonverbindungen als gesundheitsschädlich (Xn) und umweltgefährlich (N). Antimon selbst ist dort nicht aufgeführt, laut Sicherheitsdatenblatt ist es als "reizend" gekennzeichnet.

Die Internationale Agentur für Krebsforschung (IARC) stuft Antimon(III)-oxid als "möglicherweise krebserzeugende Substanz" ein. 
In der EU gilt für Trinkwasser ein Grenzwert von 5 µg/l. Untersuchungen von in PET-Flaschen abgefüllten Fruchtsäften (für die keine Richtlinien existieren) ergaben Antimonkonzentrationen bis zu 44,7 µg/l in unverdünnten Saftkonzentraten. 
Die Auswirkungen von Antimon auf menschliche Gesundheit werden unter REACH im Jahr 2018 im Rahmen der Stoffbewertung von Deutschland geprüft.

Vorproben:

Flammenfärbung: Flamme fahlblau, wenig charakteristische
Phosphorsalzperle: Farblos (gestört durch alle Elemente, die eine farbige Perle erzeugen)

Nachweisreaktion:

Reduktion durch unedle Metalle, zum Beispiel Eisen, Zink oder Zinn.

In nicht zu sauren Lösungen reduzieren unedle Metalle Antimon-Kationen Sb(III), Sb(V) und Sb(III)/(V) zu metallischem Antimon:

Die auf Antimon zu prüfende Substanz wird in salzsaure Lösung gegeben und mit Eisenpulver versetzt. Es entsteht ein schwarzer, flockiger Niederschlag aus metallischem Antimon in der Lösung oder direkt am Eisen. Auch der Nachweis an einem Eisennagel ist möglich. Dabei ist eine schwarze Ablagerung am Nagel ein Nachweis für Antimon, welches sich hier elementar niedergeschlagen hat.

Die Marshsche Probe gestattet einen eindeutigen Nachweis von Antimon. Wenn die pyrolytisch abgeschiedene Substanz (dunkel glänzender Spiegel) sich nicht in ammoniakalischem Wasserstoffperoxid löst, sind Arsen und Germanium als mögliche Alternativen ausgeschlossen.

Die hochempfindliche Bestimmung winziger Antimonspuren erfolgt durch die Hydridtechnik der Atomspektrometrie. Hierbei wird im Prinzip die Marshsche Probe mit der Atomabsorptionsspektrometrie gekoppelt. Die Matrixeffekte der Probelösung lassen sich dadurch sehr wirksam unterdrücken.

Eine weitere Methode besteht darin, eine wässrige Lösung, in der Antimonionen enthalten sind, mit Rhodamin-B-Lösung zu versetzen. Es bildet sich ein farbiger Komplex, der mit Isopropylether extrahierbar ist. Dieser Nachweis ist allerdings recht unspezifisch, da auch Gold-, Cadmium-, Gallium, Thallium-, Uran- und Wolfram-ionen farbige Komplexe bilden.











</doc>
<doc id="96" url="https://de.wikipedia.org/wiki?curid=96" title="Argon">
Argon

Argon (griechisch "" „träge“) ist ein chemisches Element mit dem Symbol Ar (bis 1957 nur A) und der Ordnungszahl 18. Im Periodensystem steht es in der 8. Hauptgruppe bzw. der 18. IUPAC-Gruppe und zählt daher zu den Edelgasen. Wie die anderen Edelgase ist es ein farbloses, äußerst reaktionsträges, einatomiges Gas. In vielen Eigenschaften wie Schmelz- und Siedepunkt oder Dichte steht es zwischen dem leichteren Neon und dem schwereren Krypton.

Argon ist das häufigste auf der Erde vorkommende Edelgas, der Anteil an der Atmosphäre beträgt etwa 0,934 %. Damit ist Argon der dritthäufigste Bestandteil der Erdatmosphäre, nach Stickstoff und Sauerstoff. Dies ist großteils auf den Zerfall des Kaliumisotops K zurückzuführen, bei dem Ar entsteht.

Argon war das erste Edelgas, das als Stoff entdeckt und gewonnen wurde, daher der Name, der im Grunde zu jedem Edelgas passt. Helium (von griechisch "helios" für „Sonne“) wurde vorher lediglich spektroskopisch im Sonnenlicht sowie in irdischen Proben nachgewiesen und Neon erst später entdeckt. Argon wurde 1894 von Lord Rayleigh und William Ramsay durch fraktionierte Destillation von flüssiger Luft gefunden. Als preiswertestes Edelgas wird Argon in großen Mengen als Schutzgas etwa beim Schweißen und in der Produktion von manchen Metallen, aber auch als Füllgas von Glühlampen verwendet.

Einen ersten Hinweis auf das später entdeckte Argon fand Henry Cavendish, der 1783 die Reaktivität der Luft erforschte. Er erzeugte elektrische Entladungen in einer bestimmten Menge Luft, die mit Sauerstoff im Verhältnis von 5:3 angereichert war. Stickstoff und Sauerstoff reagierten miteinander und die entstandenen Stickoxide konnten ausgewaschen werden. Dabei blieb stets ein kleiner Rest nicht-reagierten Gases zurück. Cavendish erkannte jedoch nicht, dass es sich dabei um ein anderes Element handelte und setzte seine Experimente nicht fort.

Nachdem John William Strutt, 3. Baron Rayleigh 1892 die Dichte von aus Luft isoliertem Stickstoff bestimmt hatte, fiel ihm auf, dass aus Ammoniak gewonnener Stickstoff eine niedrigere Dichte aufwies. Es gab verschiedene Spekulationen zu diesem Befund; so meinte James Dewar, es müsse sich um ein N, also ein Stickstoff-Analogon zu Ozon handeln. Rayleigh wiederholte Cavendishs Experimente, indem er in einer luftgefüllten Glaskugel elektrische Funken erzeugte und so Stickstoff und Sauerstoff zur Reaktion brachte. Nach Bestätigung von Cavendishs Ergebnis eines unreaktiven Rückstandes untersuchte William Ramsay diesen ab 1894 durch Überleitung über heißes Magnesium genauer. Da Magnesium mit Stickstoff zum Nitrid reagiert, konnte er dem Gemisch weiteren Stickstoff entziehen. Dabei stellte er eine Erhöhung der Dichte fest und fand schließlich ein bislang unbekanntes, reaktionsträges Gas. Am 31. Januar 1895 gaben Ramsay und Rayleigh schließlich die Entdeckung des neuen Elements bekannt, das sie nach dem altgriechischen "argos", „träge“, "Argon" nannten. Als William Ramsay ab 1898 das aus der Luft isolierte Argon weiter untersuchte, entdeckte er darin drei weitere Elemente, die Edelgase Neon, Krypton und Xenon.

Erste technische Anwendungen fand das Gas in der Elektroindustrie: Es wurden unter anderem Gleichrichter auf der Basis der Glimmentladung in Argon hergestellt, die sogenannten "Tungar-Röhren."

Argon zählt im Universum zu den häufigeren Elementen, in seiner Häufigkeit ist es vergleichbar mit Schwefel und Aluminium. Es ist im Universum nach Helium und Neon das dritthäufigste Edelgas. Dabei besteht das primordiale Argon, das etwa in der Sonne oder Gasplaneten wie Jupiter gefunden wird, hauptsächlich aus den Isotopen Ar und Ar, während das dritte stabile Isotop, Ar, dort nur in geringer Menge vorkommt. Das Verhältnis von Ar zu Ar beträgt etwa 5,7.

Auf der Erde ist Argon dagegen das häufigste Edelgas. Es macht 0,934 % des Volumens der Atmosphäre (ohne Wasserdampf) aus und ist damit nach Stickstoff und Sauerstoff der dritthäufigste Atmosphärenbestandteil. Die Zusammensetzung des terrestrischen Argons unterscheidet sich erheblich von derjenigen des primordialen Argons im Weltall. Es besteht zu über 99 % aus dem Isotop Ar, das durch Zerfall des Kaliumisotops K entstanden ist. Die primordialen Isotope sind dagegen nur in geringen Mengen vorhanden.

Da das Argon durch den Kaliumzerfall in der Erdkruste entsteht, findet man es auch in Gesteinen. Beim Schmelzen von Gesteinen im Erdmantel gast das Argon, aber auch das bei anderen Zerfällen entstehende Helium aus. Es reichert sich daher vorwiegend in den Basalten der ozeanischen Erdkruste an. Aus den Gesteinen wird das Argon an das Grundwasser abgegeben. Daher ist in Quellwasser, vor allem wenn es aus größerer Tiefe kommt, Argon gelöst.

Die Gewinnung des reinen Argons erfolgt ausschließlich aus der Luft, in der Regel im Rahmen der Luftverflüssigung im Linde-Verfahren. Das Argon wird dabei nicht in der Haupt-Rektifikationskolonne des Verfahrens von den Hauptluftbestandteilen getrennt, sondern in einer eigenen Argon-Kolonne. In dieser wird durch Rektifikation zunächst Rohargon hergestellt, das noch etwa 3–5 % Sauerstoff und 1 % Stickstoff enthält.

Anschließend wird das Rohargon in weiteren Stufen gereinigt. Das Gasgemisch wird zunächst auf Raumtemperatur erwärmt und auf 4–6 bar verdichtet. Um den restlichen Sauerstoff zu entfernen, wird danach Wasserstoff eingespritzt, der an Edelmetall-Katalysatoren mit dem Sauerstoff zu Wasser reagiert. Nachdem dieses entfernt wurde, wird in einer weiteren Kolonne das Argon, das sich am unteren Ende der Kolonne anreichert, vom restlichen Stickstoff getrennt, so dass Argon mit einer Reinheit von 99,9999 % (Argon 6.0) produziert werden kann.

Weitere Quellen für die Gewinnung von Argon sind die Produktion von Ammoniak im Haber-Bosch-Verfahren sowie die Synthesegasherstellung, etwa zur Methanolproduktion. Bei diesen Verfahren, die Luft als Ausgangsstoff nutzen, reichern sich Argon und andere Edelgase im Produktionsprozess an und können aus dem Gasgemisch isoliert werden. Wie beim Linde-Verfahren werden auch hier die verschiedenen Gase durch Adsorption oder Rektifikation voneinander getrennt und so reines Argon gewonnen.

Argon ist bei Normalbedingungen ein einatomiges, farbloses und geruchloses Gas, das bei 87,15 K (−186 °C) kondensiert und bei 83,8 K (−189,3 °C) erstarrt. Wie die anderen Edelgase außer dem Helium kristallisiert Argon in einer kubisch dichtesten Kugelpackung mit dem Gitterparameter "a" = 526 pm bei 4 K.

Wie alle Edelgase besitzt Argon nur abgeschlossene Schalen (Edelgaskonfiguration). Dadurch lässt sich erklären, dass das Gas stets einatomig vorliegt und die Reaktivität gering ist.

Mit einer Dichte von 1,784 kg/m bei 0 °C und 1013 hPa ist Argon schwerer als Luft, es sinkt also ab. Im Phasendiagramm liegt der Tripelpunkt bei 83,8 K und 689 hPa, der kritische Punkt bei 150,86 K, 4896 kPa sowie einer kritischen Dichte von 0,536 g/cm.

In Wasser ist Argon etwas löslich. In einem Liter Wasser können sich bei 0 °C und Normaldruck maximal 5,6 g Argon lösen.

Als Edelgas reagiert Argon fast nicht mit anderen Elementen oder Verbindungen. Bislang ist nur das experimentell dargestellte Argonfluorohydrid HArF bekannt, das durch Photolyse von Fluorwasserstoff in einer Argonmatrix bei 7,5 K gewonnen wird und anhand neuer Linien im Infrarotspektrum identifiziert wurde. Oberhalb von 27 K zersetzt es sich. Nach Berechnungen sollten weitere Verbindungen des Argons metastabil sein und sich verhältnismäßig schwer zersetzen; diese konnten jedoch experimentell bislang nicht dargestellt werden. Beispiele hierfür sind das Chloranalogon des Argonfluorohydrides HArCl, aber auch um Verbindungen, bei denen das Proton durch andere Gruppen ersetzt ist, etwa FArCCH als organische Argonverbindung und FArSiF mit einer Argon-Silicium-Bindung.

Argon bildet einige Clathrate, in denen es physikalisch in Hohlräume eines umgebenden Kristalls eingeschlossen ist. Bei −183 °C ist ein Argon-Hydrat stabil, jedoch ist die Geschwindigkeit der Bildung sehr langsam, da eine Umkristallisierung stattfinden muss. Ist das Eis mit Chloroform gemischt, bildet sich das Clathrat schon bei −78 °C. Stabil ist auch ein Clathrat von Argon in Hydrochinon.

Insgesamt sind 23 Isotope sowie ein weiteres Kernisomer von Argon bekannt. Von diesen sind drei, nämlich die Isotope Ar, Ar und Ar, stabil und kommen in der Natur vor. Dabei überwiegt bei weitem Ar mit einem Anteil von 99,6 % am natürlichen irdischen Isotopengemisch. Ar und Ar sind mit einem Anteil von 0,34 % beziehungsweise 0,06 % selten. Von den instabilen Isotopen besitzen Ar mit 269 Jahren und Ar mit 32,9 Jahren die längsten Halbwertszeiten. Alle anderen Isotope besitzen kurze Halbwertszeiten im Bereich von unter 10 ps bei Ar bis 35,04 Tagen bei Ar.

Ar wird für die Altersbestimmung von Gesteinen genutzt (Kalium-Argon-Datierung). Dabei wird ausgenutzt, dass instabiles K, das in diesen enthalten ist, langsam zu Ar zerfällt. Je mehr Kalium zu Argon zerfallen ist, desto älter ist das Gestein. Das kurzlebige Isotop Ar kann zur Überprüfung von Gasleitungen verwendet werden. Durch das Durchleiten von Ar kann die Leistungsfähigkeit einer Belüftung oder Dichtigkeit einer Leitung festgestellt werden.

"→ Liste der Argon-Isotope"

Wie die anderen Edelgase hat Argon auf Grund der Reaktionsträgheit keine biologische Bedeutung und ist auch nicht toxisch. In höheren Konzentrationen wirkt es durch Verdrängung des Sauerstoffs erstickend. Bei Drücken von mehr als 24 bar wirkt es narkotisierend.

Als günstigstes und in großen Mengen verfügbares Edelgas wird Argon in vielen Bereichen verwendet. Die Produktion betrug 1998 weltweit etwa 2 Milliarden m³ bzw. 2 km³. Der größte Teil des Argons wird als Schutzgas verwendet. Es wird immer dann genutzt, wenn der billigere Stickstoff nicht anwendbar ist. Dazu zählen vor allem Schweißverfahren für Metalle, die mit Stickstoff bei hohen Temperaturen reagieren, etwa Titan, Tantal und Wolfram. Auch beim Metallinertgasschweißen und Wolfram-Inertgasschweißen, die etwa beim Schweißen von Aluminiumlegierungen oder hoch legierten Stählen angewendet werden, dient Argon als Inertgas. Weiterhin wird es in der Metallurgie als Schutzgas, etwa für die Produktion von Titan, hochreinem Silicium oder der Schmelzraffination sowie zum Entgasen von Metallschmelzen genutzt.

Argon ist ein Lebensmittelzusatzstoff (E 938) und dient als Treib- und Schutzgas bei der Verpackung von Lebensmitteln und der Weinherstellung.

Argon wird als gasförmiges Löschmittel vorwiegend für den Objektschutz, vor allem bei elektrischen und EDV-Anlagen eingesetzt und wirkt dabei durch Sauerstoffverdrängung. Für diesen Zweck wird reines Argon oder ein Gasgemisch zusammen mit Stickstoff verwendet.

In der Analytik wird Argon als Träger- und Schutzgas für die Gaschromatographie und das induktiv gekoppelte Plasma (ICP-MS, ICP-OES) verwendet.

Glühlampen werden häufig mit Argon-Stickstoff-Gemischen gefüllt, weil eine Gasfüllung die Sublimation des Glühfadens vermindert. Argon hat dabei eine geringere Wärmeleitfähigkeit als leichtere Gase, ist aber preiswerter als andere schwerere und damit noch geringer wärmeleitende Gase wie Krypton oder Xenon. Ein Vorteil der geringeren Wärmeleitfähigkeit ist eine höhere mögliche Glühtemperatur und damit höhere Lichtausbeute. Ebenfalls wegen der geringen Wärmeleitfähigkeit wird es als Füllgas für Isolierglasscheiben verwendet. Auch in Gasentladungslampen dient Argon als Leuchtgas mit einer typischen violetten Farbe. Wird etwas Quecksilber dazugegeben, ändert sich die Farbe ins Blaue. Weiterhin ist Argon das Lasermedium in Argon-Ionen-Lasern.

Im Bereich der Stahlerzeugung kommt Argon eine besonders wichtige Rolle im Bereich der Sekundärmetallurgie zu. Mit der Argon-Spülung kann die Stahllegierung entgast und gleichzeitig homogenisiert werden, speziell wird dabei der unerwünschte, gelöste Stickstoff aus der Schmelze entfernt.

Beim Tauchen wird Argon – insbesondere bei der Nutzung des Helium­haltigen Trimix als Atemgas – dazu verwendet, um Trockentauchanzüge zu füllen bzw. damit zu tarieren. Hierbei wird ebenfalls die geringe Wärmeleitfähigkeit des Gases genutzt, um das Auskühlen des Anzugträgers zu verzögern.

Seit Mai 2014 ist Argon auf der Dopingliste der Welt-Anti-Doping-Agentur (WADA). Durch den bei der Inhalation von Argon entstehenden Sauerstoffmangel wird offensichtlich die Bildung von körpereigenem Erythropoetin (EPO) aktiviert. Aus demselben Grund ist auch Xenon auf der Dopingliste.



</doc>
<doc id="97" url="https://de.wikipedia.org/wiki?curid=97" title="Arsen">
Arsen

Arsen [] ist ein chemisches Element mit dem Elementsymbol As und der Ordnungszahl 33. Im Periodensystem der Elemente steht es in der 4. Periode und der 5. Hauptgruppe, bzw. 15. IUPAC-Gruppe oder Stickstoffgruppe. Arsen kommt selten gediegen vor, meistens in Form von Sulfiden. Es gehört zu den Halbmetallen, da es je nach Modifikation metallische oder nichtmetallische Eigenschaften zeigt.

Umgangssprachlich wird auch das als Mordgift bekannte Arsenik meist einfach „Arsen“ genannt. Arsenverbindungen kennt man schon seit dem Altertum. Als mutagenes Klastogen können Arsenverbindungen als Gift wirken, welches Chromosomenaberrationen hervorrufen und somit karzinogene Wirkung besitzen kann.

Arsen wird zur Dotierung von Halbleitern und als Bestandteil von III-V-Halbleitern wie Galliumarsenid genutzt. Die organische Arsenverbindung Arsphenamin ("Salvarsan") war Anfang des 20. Jahrhunderts ein Durchbruch in der Behandlung der Syphilis. Heute wird Arsentrioxid als letzte Behandlungsoption in der Therapie der Promyelozytenleukämie angewendet.

Der Name Arsen geht unmittelbar auf zurück, der Bezeichnung des Arsenminerals Auripigment. Sie findet sich schon bei Dioskurides im 1. Jahrhundert. Die griechische Bezeichnung scheint ihrerseits ihren Ursprung im Altpersischen "(al-)zarnik" (goldfarben, Auripigment, „Arsen“) zu haben und gelangte wohl durch semitische Vermittlung ins Griechische. Volksetymologisch wurde der Name fälschlicherweise vom (alt- und neu-)griechischen Wort αρσενικός "arsenikós" abgeleitet, das sich etwa mit männlich/stark übersetzen lässt. Erst seit dem 19. Jahrhundert ist die Bezeichnung Arsen gebräuchlich. Das Elementsymbol wurde 1814 von Jöns Jakob Berzelius vorgeschlagen.

Der erste Kontakt von Menschen mit Arsen lässt sich aus dem 3. Jahrtausend v. Chr. nachweisen: In den Haaren der im Gletschereis erhaltenen Mumie des volkstümlich Ötzi genannten Alpenbewohners ließen sich größere Mengen Arsen nachweisen, was archäologisch als Hinweis darauf gedeutet wird, dass der betroffene Mann in der Kupferverarbeitung tätig war – Kupfererze sind oft mit Arsen verunreinigt. Im klassischen Altertum war Arsen in Form der Arsen-Sulfide Auripigment (AsS) und Realgar (AsS) bekannt, die etwa von dem Griechen Theophrastos, dem Nachfolger Aristoteles, beschrieben wurden. Auch der griechische Philosoph Demokrit hatte im 5. Jahrhundert v. Chr. nachweislich Kenntnisse über Arsenverbindungen. Der Leidener Papyrus X aus dem 3. Jahrhundert nach Chr. lässt darauf schließen, dass sie benutzt wurden, um Silber goldartig und Kupfer weiß zu färben. Der römische Kaiser Caligula hatte angeblich bereits im 1. Jahrhundert nach Chr. ein Projekt zur Herstellung von Gold aus dem (goldgelben) Auripigment in Auftrag gegeben. Die Alchimisten, die Arsen-Verbindungen nachweislich der Erwähnung im antiken Standardwerk "Physica et Mystica" kannten, vermuteten eine Verwandtschaft mit Schwefel und Quecksilber. Arsen(III)-sulfid kam als Malerfarbe und Enthaarungsmittel zum Einsatz sowie zur äußerlichen als auch inneren Behandlung von Lungenkrankheiten.

Im Mittelalter wurde Arsenik (Arsen(III)-oxid) im Hüttenrauch (staubbeladenes Abgas metallurgischer Öfen) gefunden. Albertus Magnus beschrieb um 1250 erstmals die Herstellung von Arsen durch Reduktion von Arsenik mit Kohle. Er gilt daher als Entdecker des Elements, auch wenn es Hinweise darauf gibt, dass das elementare Metall schon früher hergestellt wurde. Paracelsus führte es im 16. Jahrhundert in die Heilkunde ein. Etwa zur gleichen Zeit wurden Arsenpräparate in der chinesischen Enzyklopädie "Pen-ts'ao Kang-mu" des Apothekers Li Shi-zhen beschrieben. Dieser Autor hebt insbesondere die Anwendung als Pestizid in Reisfeldern hervor.

Im 17. Jahrhundert wurde das gelbe Auripigment bei holländischen Malern als "Königsgelb" populär. Da sich das Pigment über längere Zeiträume hinweg in Arsen(III)-oxid umwandelt und von der Leinwand bröckelt, entstehen Schwierigkeiten bei der Restaurierung. Ab 1740 wurden Arsenpräparate in Europa mit Erfolg als Beizmittel im Pflanzenschutz eingesetzt. Diese Nutzung verbot man jedoch 1808 wegen ihrer hohen Giftigkeit wieder. Der Einsatz von Arsenzusätzen für den Bleiguss beruht auf der größeren Härte solcher Bleilegierungen, typische Anwendung sind Schrotkugeln. Obwohl die Giftigkeit und die Verwendung als Mordgift bekannt war, ist Arsen im beginnenden 19. Jahrhundert eines der bedeutendsten Asthmamittel. Grundlage sind anscheinend Berichte, in denen den Chinesen nachgesagt wurde, sie würden Arsen in Kombination mit Tabak rauchen, um Lungen zu bekommen, die stark wie Blasebälge seien. Ebenfalls bis ins 19. Jahrhundert fanden Arsenverbindungen äußerlich und innerliche Anwendungen bei bösartigen Geschwülsten, Hauterkrankungen und (etwa in Form der Fowlerschen Tropfen) bei Fieber.

Arsen wurde in Form von Kupferarsenaten in Farbmitteln wie dem Pariser Grün eingesetzt, um Tapeten zu bedrucken. Bei hoher Feuchtigkeit wurden diese Pigmente durch Schimmelpilzbefall in giftige flüchtige Arsenverbindungen umgewandelt, die nicht selten zu chronischen Arsenvergiftungen führten.

Doch auch in Kriegen fand Arsen Verwendung: Im Ersten Weltkrieg wurden Arsenverbindungen in chemischen Kampfstoffen (Blaukreuz) oder Lewisit eingesetzt. Bei den Opfern bewirkten sie durch Angriff auf Haut und Lungen grausame Schmerzen und schwerste körperliche Schädigungen.

Arsen kommt in geringen Konzentrationen von bis zu 10 ppm praktisch überall im Boden vor. Es ist in der Erdkruste ungefähr so häufig wie Uran oder Germanium. In der kontinentalen Erdkruste kommt Arsen mit durchschnittlich 1,7 ppm vor, wobei es durch seinen lithophilen Charakter (= Silikat liebend) in der oberen Kruste angereichert ist (2 ppm gegenüber 1,3 ppm in der unteren Kruste); damit liegt Arsen in der Tabelle der häufigsten Elemente an 53. Stelle.

Arsen ("Scherbenkobalt") kommt in der Natur gediegen, das heißt in elementarer Form, vor und ist daher von der International Mineralogical Association (IMA) als eigenständiges Mineral anerkannt. Gemäß der Systematik der Minerale nach Strunz (9. Auflage) wird Arsen unter der System-Nr. 1.CA.05 (Elemente – Halbmetalle (Metalloide) und Nichtmetalle – Arsengruppen-Elemente) (8. Auflage: "I/B.01-10") eingeordnet. Die im englischsprachigen Raum ebenfalls geläufige Systematik der Minerale nach Dana führt das Element-Mineral unter der System-Nr. 01.03.01.01.

Weltweit sind zurzeit (Stand: 2011) rund 330 Fundorte für gediegenes Arsen bekannt. In Deutschland wurde es an mehreren Fundstätten im Schwarzwald (Baden-Württemberg), im bayerischen Spessart und Oberpfälzer Wald, im hessischen Odenwald, in den Silberlagerstätten des Westerzgebirges (Sachsen), am Hunsrück (Rheinland-Pfalz) sowie im Thüringer Wald gefunden. In Österreich trat Arsen an mehreren Fundstätten in Kärnten, Salzburg und der Steiermark zutage. In der Schweiz fand sich gediegen Arsen in den Kantonen Aargau und Wallis.

Weitere Fundorte sind in Australien, Belgien, Bolivien, Bulgarien, Chile, China, Finnland, Frankreich, Griechenland, Irland, Italien, Japan, Kanada, Kasachstan, Kirgisistan, Madagaskar, Malaysia, Marokko, Mexiko, Mongolei, Neuseeland, Norwegen, Österreich, Peru, Polen, Rumänien, Russland, Schweden, Slowakei, Spanien, Tschechien, Ukraine, Ungarn, im Vereinigten Königreich (Großbritannien) und in den Vereinigten Staaten (USA) bekannt.

Weit häufiger kommt das Element allerdings in verschiedenen intermetallischen Verbindungen mit Antimon (Allemontit) und Kupfer (Whitneyit) sowie in verschiedenen Mineralen vor, die überwiegend der Klasse der Sulfide und Sulfosalze angehören. Insgesamt sind bisher (Stand: 2011) 565 Arsenminerale bekannt. Die höchsten Konzentrationen an Arsen enthalten dabei unter anderem die Minerale Duranusit (ca. 90 %), Skutterudit und Arsenolith (jeweils ca. 76 %), die allerdings selten zu finden sind. Weit verbreitet sind dagegen Arsenopyrit ("Arsenkies"), Löllingit, Realgar ("Rauschrot") und Auripigment ("Orpiment", "Rauschgelb"). Weitere bekannte Minerale sind Cobaltit ("Kobaltglanz"), Domeykit ("Arsenkupfer"), Enargit, Gersdorffit ("Nickelarsenkies"), Proustit ("Lichtes Rotgültigerz", "Rubinblende"), Rammelsbergit sowie Safflorit und Sperrylith.

Arsenate finden sich häufig in phosphathaltigen Gesteinen, da sie eine vergleichbare Löslichkeit aufweisen und das häufigste Sulfidmineral Pyrit kann bis zu einigen Massenprozent Arsen einbauen.

Arsen wird heutzutage als Nebenprodukt der Verhüttung von Gold-, Silber-, Zinn-, Kupfer-, Cobalt- und weiteren Buntmetallerzen sowie bei der Verarbeitung von Phosphatrohstoffen gewonnen. Die größten Produzenten im Jahr 2009 waren China, Chile, Marokko und Peru. Arsen ist nur schwer wasserlöslich und findet sich daher nur in geringen Spuren, etwa 1,6 ppb (Milliardstel Massenanteilen) in Meeren und Ozeanen.

In der Luft findet man Arsen in Form von partikulärem Arsen(III)-oxid. Als natürliche Ursache dafür hat man Vulkanausbrüche identifiziert, die insgesamt jährlich geschätzte 3000 Tonnen in die Erdatmosphäre eintragen. Bakterien setzen weitere 20.000 Tonnen in Form organischer Arsenverbindungen wie Trimethylarsin frei. Ein großer Teil am freigesetzten Arsen entstammt der Verbrennung fossiler Brennstoffe wie Kohle oder Erdöl. Die geschätzten Emissionen, verursacht durch den Straßenverkehr und stationäre Quellen, betrugen 1990 in der Bundesrepublik Deutschland 120 Tonnen (20 Tonnen in den alten, 100 Tonnen in den neuen Bundesländern). Die Außenluftkonzentration von Arsen liegt zwischen 0,5 und 15 Nanogramm pro Kubikmeter.

Arsen fällt in größeren Mengen als Nebenprodukt bei der Gewinnung von Kupfer, Blei, Cobalt und Gold an. Dies ist die Hauptquelle für die kommerzielle Nutzung des Elements.

Es kann durch thermische Reduktion von Arsen(III)-oxid mit Koks oder Eisen und durch Erhitzen von Arsenkies (FeAsS) oder Arsenikalkies (FeAs) unter Luftabschluss in liegenden Tonröhren gewonnen werden. Dabei sublimiert elementares Arsen, das an kalten Oberflächen wieder in den festen Aggregatzustand zurückkehrt.

Für die Halbleitertechnik wird Arsen, dessen Reinheit über 99,99999 Prozent betragen muss, durch Reduktion von mehrfach destilliertem Arsen(III)-chlorid im Wasserstoffstrom hergestellt:

Früher wurde es auch durch Sublimation aus Lösungen in flüssigem Blei erzeugt. Dabei wird der Schwefel der Arsen-Erze durch das Blei in Form von Blei(II)-sulfid gebunden. Die hierbei erzielten Reinheiten von über 99,999 Prozent waren für Halbleiteranwendungen nicht ausreichend. Eine andere Möglichkeit besteht im Auskristallisieren bei hohen Temperaturen aus geschmolzenem Arsen oder in der Umwandlung in Monoarsan, einer anschließenden Reinigung sowie der Zersetzung bei 600 °C in Arsen und Wasserstoff.

Arsen bildet mit Stickstoff, Phosphor, Antimon und Bismut die 5. Hauptgruppe des Periodensystems und nimmt wegen seiner physikalischen und chemischen Eigenschaften den Mittelplatz in dieser Elementgruppe ein. Arsen hat eine relative Atommasse von 74,92159. Der Radius des Arsen-Atoms beträgt 124,5 Pikometer. In kovalent gebundenem Zustand ist er etwas kleiner (121 Pikometer). Aufgrund der Abgabe der äußeren Elektronen (Valenzelektronen) bei der Ionisierung reduziert sich der Radius beträchtlich auf 34 Pikometer (As; das äußerste p- und das äußerste s-Atomorbital bleiben unbesetzt) beziehungsweise 58 Pikometer (As; nur das p-Orbital ist unbesetzt). In chemischen Komplexverbindungen ist das As-Kation von vier Bindungspartnern (Liganden), As von sechs umgeben. Arsen tritt allerdings nur sehr selten in eindeutig ionischer Form auf.

Der Wert für die Elektronegativität liegt nach Pauling auf der von 0 (Metalle) bis 4 (Nichtmetall) reichenden Skala bei 2,18 und ist damit mit dem Wert des Gruppennachbarn Phosphor vergleichbar. Der Halbmetall-Charakter des Arsens zeigt sich zudem darin, dass die benötigte Dissoziationsenergie von 302,7 kJ/mol, also die Energie, die aufgebracht werden muss, um ein einzelnes Arsen-Atom aus einem Arsen-Festkörper herauszulösen, zwischen der des Nichtmetalls Stickstoff (473,02 kJ/mol; kovalente Bindung) und des Metalls Bismut (207,2 kJ/mol; metallische Bindung) liegt. Unter Normaldruck sublimiert Arsen bei einer Temperatur von 613 °C, geht also aus dem festen Aggregatzustand direkt in die Gasphase über. Arsendampf ist zitronengelb und setzt sich bis ungefähr 800 °C aus As-Molekülen zusammen. Oberhalb von 1700 °C liegen As-Moleküle vor.

Arsen zeigt je nach Verbindungspartner Oxidationsstufen zwischen −3 und +5. Mit elektropositiven Elementen wie Wasserstoff oder Metallen bildet es Verbindungen, in denen es eine Oxidationsstufe von −3 einnimmt. Beispiele dafür sind Monoarsan (AsH) und Arsenkupfer (CuAs). In Verbindungen mit elektronegativen Elementen wie den Nichtmetallen Sauerstoff, Schwefel und Chlor besitzt es die Oxidationsstufe +3 oder +5; erstere ist dabei gegenüber den in derselben Hauptgruppe stehenden Elementen Stickstoff und Phosphor tendenziell bevorzugt.

Arsen kommt wie andere Elemente der Stickstoffgruppe in verschiedenen allotropen Modifikationen vor. Anders als beim Stickstoff, der in Form zweiatomiger Moleküle mit kovalenter Dreifachbindung vorkommt, sind die entsprechenden As-Moleküle instabil und Arsen bildet stattdessen kovalente Netzwerke aus.

Graues oder metallisches Arsen ist die stabilste Form. Es hat eine Dichte von 5,73 g/cm. Seine Kristalle sind stahlgrau, metallisch glänzend und leiten den elektrischen Strom.

Betrachtet man den strukturellen Aufbau des grauen Arsens, dann erkennt man Schichten aus gewellten Arsen-Sechsringen, welche die Sesselkonformation einnehmen. Darin bilden die Arsen-Atome eine Doppelschicht, wenn man sich den Aufbau der Schicht im Querschnitt ansieht. Die Übereinanderlagerung dieser Doppelschichten ist sehr kompakt. Bestimmte Atome der nächsten darüberliegenden oder darunterliegenden Schicht sind von einem Bezugsatom fast ähnlich weit entfernt wie innerhalb der betrachteten Doppelschicht. Dieser Aufbau bewirkt, dass die graue Arsen-Modifikation wie die homologen Elemente Antimon und Bismut sehr spröde ist. Deswegen werden diese drei Elemente häufig auch als Sprödmetalle bezeichnet.

Wird Arsen-Dampf, in dem Arsen gewöhnlich als As-Tetraeder vorliegt, schnell abgekühlt, so bildet sich das metastabile gelbe Arsen mit einer Dichte von 1,97 g/cm. Es besteht ebenfalls aus tetraedrischen As-Molekülen. Gelbes Arsen ist ein Nichtmetall und leitet infolgedessen den elektrischen Strom nicht. Es kristallisiert aus Schwefelkohlenstoff und bildet kubische, stark lichtbrechende Kristalle, die nach Knoblauch riechen. Bei Raumtemperatur und besonders schnell unter Lichteinwirkung wandelt sich gelbes Arsen in graues Arsen um.

Schwarzes Arsen selbst kann seinerseits in zwei verschiedenen Formen vorkommen. "Amorphes schwarzes Arsen" entsteht durch Abkühlung von Arsen-Dampf an 100 bis 200 °C warmen Oberflächen. Es besitzt keine geordnete Struktur, sondern liegt in einer amorphen, glasartigen Form vor, analog zum roten Phosphor. Die Dichte beträgt 4,7 bis 5,1 g/cm. Oberhalb 270 °C wandelt sich das schwarze Arsen in die graue Modifikation um. Wird glasartiges, amorphes schwarzes Arsen bei Anwesenheit von metallischem Quecksilber auf 100 bis 175 °C erhitzt, so entsteht das metastabile "orthorhombische schwarze Arsen", das mit dem schwarzen Phosphor vergleichbar ist.

Natürlich gebildetes orthorhombisches schwarzes Arsen ist in der Natur als seltenes Mineral Arsenolamprit bekannt.

Bei der Reduktion von Arsenverbindungen in wässriger Lösung entstehen ähnlich wie beim Phosphor Mischpolymerisate. Bei diesen bindet ein Teil der freien Valenzen des Arsens Hydroxygruppen (–OH). Man nennt diese Form des Arsens "braunes Arsen".

Arsen reagiert heftig mit Oxidationsmitteln und Halogenen. So verbrennt Arsen an der Luft mit bläulicher Flamme zu einem weißen Rauch von giftigem Arsen(III)-oxid.

Ohne äußere Wärmezufuhr findet die Reaktion mit Chlor unter Feuererscheinung zu Arsen(III)-chlorid statt.

Eine weitere Oxidation ist möglich.

Analoge Reaktionsgleichungen gelten für die entsprechenden Reaktionen mit Fluor.
Stark oxidierende Säuren, wie konzentrierte Salpetersäure oder Königswasser, wandeln Arsen in Arsensäure um.

Ist die Oxidationsstärke weniger groß – etwa bei Verwendung von verdünnter Salpetersäure oder Schwefelsäure – entsteht Arsenige Säure.

Unter sauren Bedingungen und bei Anwesenheit von nichtpassivierten unedlen Metallen, insbesondere Zink, reagiert Arsen mit dem gebildeten Wasserstoff zu Monoarsan.

Mit basischem Natriumhydroxid bildet sich das entsprechende Arsenitsalz.

Vom Arsen sind künstlich hergestellte, radioaktive Isotope mit Massenzahlen zwischen 65 und 87 bekannt. Die Halbwertszeiten liegen zwischen 96 Millisekunden (As) und 80,3 Tagen (As). Natürlich vorkommendes Arsen besteht zu 100 Prozent aus dem Isotop As, es ist daher ein anisotopes Element. Der entsprechende Arsen-Kern besteht also aus genau 33 Protonen und 42 Neutronen. Physikalisch zählt man ihn daher zu den ug-Kernen (u steht hier für ungerade, g für gerade). Sein Kernspin beträgt 3/2.

Arsen wird Bleilegierungen zugesetzt, um ihre Festigkeit zu verbessern und das Blei gießbar zu machen. Vor allem die fein strukturierten Platten von Akkumulatoren könnten ohne Arsen nicht gegossen werden. Historisch war Arsen eine wichtige Zutat von Kupferlegierungen, die dadurch besser verarbeitbar wurden. Metallisches Arsen wurde früher gelegentlich zur Erzeugung mattgrauer Oberflächen auf Metallteilen verwendet, um eine Alterung vorzutäuschen.

In der Elektronik spielt es als mindestens 99,9999 Prozent reines Element für Gallium-Arsenid-Halbleiter, sogenannte III-V-Halbleiter (aufgrund der Kombination von Elementen aus der 3. und 5. Hauptgruppe des Periodensystems), sowie für Epitaxieschichten auf Wafern in Form von Indiumarsenidphosphid und Galliumarsenidphosphid eine wesentliche Rolle in der Herstellung von Hochfrequenzbauelementen wie Integrierten Schaltkreisen (ICs), Leuchtdioden (LEDs) beziehungsweise Laserdioden (LDs). Es gibt Anfang 2004 weltweit nur drei Hersteller von hochreinem Arsen, zwei in Deutschland und einen in Japan.

Arsen wird in Form seiner Verbindungen in einigen Ländern als Schädlingsbekämpfungsmittel im Weinbau, als Fungizid (Antipilzmittel) in der Holzwirtschaft, als Holzschutzmittel, als Rattengift und als Entfärbungsmittel in der Glasherstellung verwendet. Der Einsatz ist umstritten, da die eingesetzten Arsenverbindungen (hauptsächlich Arsen(III)-oxid) giftig sind.

Die Verwendung arsenhaltiger Mineralien als Heilmittel ist bereits in der Antike durch Hippokrates und Plinius bezeugt. Sie wurden als Fiebermittel, als Stärkungsmittel und zur Therapie von Migräne, Rheumatismus, Malaria, Tuberkulose und Diabetes eingesetzt. Im 18. Jahrhundert wurde eine Mischung aus Kaliumarsenit und Lavendelwasser als Fowler'sche Lösung bekannt, die lange als medizinisches Wundermittel galt und als Fiebersenker, Heilwasser und sogar als Aphrodisiakum Anwendung fand. Kaliumarsenit war als Bestandteil der Fowler'schen Lösung bis in die 1960er Jahre in Deutschland als Mittel zur Behandlung der Psoriasis im Einsatz.

Constantinus Africanus (1017–1087) empfahl eine Arsenapplikation zur Bekämpfung von Zahnschmerzen. Bereits um 2700 vor Christus soll die Anwendung von Arsen zur Behandlung eines schmerzenden Zahnes in der chinesischen Heilkunst beschrieben worden sein. In dem Mitte des 10. Jahrhunderts erschienenen Werk „"Liber Regius"“ empfahl der arabische Arzt Haly Abbas (ʿAli ibn al-ʿAbbās; † 944) ebenfalls den Einsatz von Arsenik zur Devitalisation der Pulpa. Arsen(III)-oxid wurde bis in die Neuzeit zur Devitalisation der Zahnpulpa verwendet und verschwand in den 1970er Jahren wegen der krebserregenden Wirkung, Entzündungen des Zahnhalteapparates, des Verlustes eines oder mehrerer Zähne einschließlich Nekrosen des umliegenden Alveolarknochens, Allergien und Vergiftungserscheinungen aus dem Therapiespektrum.

Einen Aufschwung erlebten arsenhaltige Arzneimittel zu Beginn des 20. Jahrhunderts. Harold Wolferstan Thomas und Anton Breinl konnten 1905 beobachten, dass das arsenhaltige Präparat Atoxyl Trypanosomen, zu denen die Erreger der Schlafkrankheit gehören, abtötet. 1920 wurde eine Weiterentwicklung, das Tryparsamid, in der Zeit von 1922 bis 1970 im tropischen Afrika zur Therapie der Schlafkrankheit eingesetzt. Es war bedeutsam für die Eingrenzung dieser Epidemie in der ersten Hälfte des vorigen Jahrhunderts, konnte jedoch zur Erblindung führen. Das in den 1950er Jahren entwickelte Melarsoprol war über mehrere Jahrzehnte das Mittel der Wahl zur Behandlung der Schlafkrankheit und wird heute noch eingesetzt, da keine effektiven Nachfolgepräparate zur Verfügung stehen.

Ebenfalls angeregt durch die Trypanosomen-toxische Wirkung von Atoxyl entwickelte Paul Ehrlich das arsenhaltige Arsphenamin (Salvarsan). Das 1910 in die Therapie der Syphilis eingeführte Mittel stellte das erste auf theoretischen Vorüberlegungen beruhende, systematisch entwickelte, spezifisch wirkende Chemotherapeutikum dar und war Vorbild für die Entwicklung der bis heute verwendeten Sulfonamide. Es wurde lange Zeit auch bei der Behandlung von Dysenterie eingesetzt.

Im Jahr 2000 wurde ein arsenikhaltiges Präparat unter dem Namen "Trisenox" in den USA zur Behandlung der promyelozytären Leukämie (APL) zugelassen. Seit 2002 besteht für "Trisenox" in Europa eine Zulassung zur Behandlung der APL, (Vertrieb in EU und USA: Cephalon). Seine Wirksamkeit bei der Krebstherapie wird auch auf die antiangioneogenetische Wirkung zurückgeführt.

Die verschiedenen Arsensulfide sind Bestandteil von Arzneimitteln der Chinesischen Medizin.

Aufgrund der toxischen Eigenschaften von Arsenverbindungen wurde früher überwiegend Arsenik zur Haltbarmachung von Wirbeltieren ("Taxidermie") als Insektizid verwendet. Viele andere Stoffe, wie auch Lindan, wurden zum selben Zweck verwendet, wie es die Fachliteratur der Präparatoren aus der Zeit von 1868 bis 1996 beschreibt. Solche Stoffe sind jedoch auch für Menschen giftig und stellen heute an Präparatoren besondere Anforderungen, da diese auch in Kontakt mit derart kontaminierten Präparaten kommen.

Die biologische Bedeutung des Arsens für den Menschen ist nicht vollständig geklärt. Es gilt als Spurenelement im Menschen, Mangelerscheinungen wurden bisher aber nur an Tieren nachgewiesen. Der notwendige Bedarf liegt, falls er bestehen sollte, zwischen 5 und 50 µg pro Tag. Eine tägliche Arsenaufnahme von – je nach Wahl der Nahrungsmittel – bis zu einem Milligramm gilt als harmlos. In einer neuen Studie konnte eine erhöhte Arsenbelastung durch hohe Arsengehalte im Grundwasser von Reisanbaugebieten mit der Entstehung von Krebserkrankungen in Verbindung gebracht werden. Die Förderung der Krebsentwicklung ist jedoch dosisabhängig und nur bei Verzehr von belastetem Reis als täglichem Grundnahrungsmittel gegeben. Es gibt bei regelmäßigem Verzehr von Arsenverbindungen, speziell Arsentrioxid eine Gewöhnung, die beim Absetzen der Dosis sogar von Entzugserscheinungen begleitet werden. Menschen, die eine solche Gewöhnung erworben haben, werden Arsenikesser genannt.

Meerestiere wie Muscheln oder Garnelen enthalten besonders viel Arsen, letztere bis zu 175 ppm. Vermutlich agiert es durch die Bindung an freie Thiolgruppen in Enzymen als Inhibitor, verhindert also deren Wirkung.

Für viele Tiere ist Arsen ein essentielles Spurenelement. So zeigen Hühner oder Ratten bei arsenfreier Ernährung deutliche Wachstumsstörungen; dies hängt wahrscheinlich mit dem Einfluss des Elements auf die Verstoffwechslung der Aminosäure Arginin zusammen. Zahlreiche Algen und Krebstiere enthalten organische Arsen-Verbindungen wie das schon erwähnte Arsenobetain. Arsen führt zur verstärkten Bildung der sauerstofftransportierenden roten Blutkörperchen. Aus diesem Grund wurde es früher dem Futter von Geflügel und Schweinen zugesetzt, um eine schnellere Mästung zu ermöglichen. Trainer von Rennpferden benutzten es zum illegalen Doping ihrer Tiere – heute kann der Zusatz von Arsen zur Nahrung allerdings leicht im Urin nachgewiesen werden.

Lösliche Arsenverbindungen werden leicht über den Magen-Darm-Trakt aufgenommen und rasch innerhalb von 24 Stunden im Körper verteilt. Man findet den größten Teil des aufgenommenen Arsens in den Muskeln, Knochen, Nieren und Lungen. Im Menschen wurde es zusammen mit Thallium in fast jedem Organ nachgewiesen. Blut enthält bis zu 8 ppb Arsen, in den anderen Organen des Körpers wie etwa den Knochen hat es einen Anteil von zwischen 0,1 und 1,5 ppm, in Haaren liegt der Anteil bei etwa 1 ppm. Der Gesamtgehalt von Arsen im Körper eines Erwachsenen liegt im Durchschnitt bei etwa 7 Milligramm.

Organische Arsenverbindungen wie die aus Fischen und Meeresfrüchten stammende Dimethylarsinsäure, Trimethylarsenoxid, Trimethylarsin sowie Arsenobetain verlassen den menschlichen Körper fast unverändert innerhalb von zwei bis drei Tagen über die Nieren. Anorganische Arsenverbindungen werden in der Leber zu Monomethylarsonsäure (MMAA) und Dimethylarsinsäure (DMAA) umgewandelt und anschließend ebenso über die Nieren ausgeschieden.

Bei Pflanzen erhöht das Element den Kohlenhydrat-Umsatz. Der Gebänderte Saumfarn "(Pteris vittata)" nimmt das Halbmetall bevorzugt aus dem Boden auf und kann bis zu fünf Prozent seines Trockengewichts an Arsen aufnehmen. Aus diesem Grund wird die schnellwachsende Pflanze zur biologischen Säuberung arsenkontaminierter Böden eingesetzt.

Die stimulierende Wirkung des Arsens ist vermutlich auch Ursache des früher in einigen Alpengegenden verbreiteten Arsenikessens. Im 17. Jahrhundert verzehrten manche der dortigen Bewohner lebenslang zweimal wöchentlich bis zu 250 Milligramm Arsen – bei Männern, weil es bei der Arbeit in den Höhenlagen half, bei Frauen, da es angeblich zu einer kräftigen Gesichtsfarbe beitrug. In der Wissenschaft lange als Märchen abgetan, nahm ein Bauer aus den Steirischen Alpen 1875 vor der in Graz versammelten deutschen Fachwelt eine Dosis von 400 Milligramm Arsentrioxid zu sich, die sich später auch in seinem Urin nachweisen ließ. Die Dosis lag weit über dem Doppelten der für normale Menschen tödlichen Arsenmenge, zeigte aber keinerlei negative Auswirkungen auf den Bauern. Ähnliches wird von Bewohnern einer Siedlung in der hochgelegenen chilenischen Atacamawüste berichtet, deren Trinkwasser hochgradig mit Arsen belastet ist, die jedoch keinerlei Vergiftungssymptome zeigen. Heute geht man davon aus, dass eine langsame Gewöhnung an das Gift mit sukzessive steigenden Dosen physiologisch möglich ist.

Über den Bakterienstamm GFAJ-1 wurde 2010 berichtet, dass er unter bestimmten Bedingungen in arsenathaltigen Nährmedien in der Lage sei, Arsenat anstatt Phosphat in Biomoleküle wie die DNA einzubauen, ohne dabei abzusterben, was bisher eher als unmöglich galt. Der Befund scheint jedoch auf unsauberen Arbeitsmethoden zu basieren, die Befunde konnten nicht repliziert werden.

Arsen-Stäube sind leicht entzündlich.
Dreiwertige lösliche Verbindungen des Arsens sind hoch toxisch, weil sie biochemische Prozesse wie die DNA-Reparatur, den zellulären Energiestoffwechsel, rezeptorvermittelte Transportvorgänge und die Signaltransduktion stören. Dabei kommt es mutmaßlich nicht zu einer direkten Einwirkung auf die DNA, sondern zu einer Verdrängung des Zink-Ions aus seiner Bindung zu Metallothioneinen und damit zur Inaktivierung von Tumor-Repressor-Proteinen (siehe auch Zinkfingerprotein). Arsen(III)- und Zink(II)-Ionen haben vergleichbare Ionenradien und damit ähnliche Affinität zu diesen Zink-Finger-Proteinen, allerdings führt Arsen dann nicht zur Aktivierung der Tumor-Repressor-Proteine.

Eine akute Arsenvergiftung führt zu Krämpfen, Übelkeit, Erbrechen, inneren Blutungen, Durchfall und Koliken, bis hin zu Nieren- und Kreislaufversagen. Bei schweren Vergiftungen fühlt sich die Haut feucht und kalt an und der Betroffene kann in ein Koma fallen. Die Einnahme von 60 bis 170 Milligramm Arsenik gilt für Menschen als tödliche Dosis (LD = 1,4 mg/kg Körpergewicht); meist tritt der Tod innerhalb von mehreren Stunden bis wenigen Tagen durch Nieren- und Herz-Kreislauf-Versagen ein. Eine chronische Arsenbelastung kann Krankheiten der Haut und Schäden an den Blutgefäßen hervorrufen, was zum Absterben der betroffenen Regionen (Black Foot Disease) sowie zu bösartigen Tumoren der Haut, Lunge, Leber und Harnblase führt. Diese Symptome wurden auch als Reichensteiner Krankheit bezeichnet, nach einem Ort in Schlesien, dessen Trinkwasser durch den Arsenik-Abbau bis zu 0,6 mg Arsen pro Liter enthielt.

Die chronische Arsen-Vergiftung führt über die Bindung an Sulfhydryl-Gruppen von Enzymen der Blutbildung (zum Beispiel Delta-Amino-Laevulin-Säure-Synthetase) zu einem initialen Abfall des Hämoglobins im Blut, was zu einer reaktiven Polyglobulie führt. Des Weiteren kommt es bei chronischer Einnahme von Arsen zur Substitution der Phosphor-Atome im Adenosin-Triphosphat (ATP) und damit zu einer Entkopplung der Atmungskette, was zu einer weiteren reaktiven Polyglobulie führt. Klinisch finden sich hier nach Jahren der As-Exposition Trommelschlägelfinger, Uhrglasnägel, Mees-Nagelbänder und Akrozyanose (Raynaud-Syndrom), mit Folge der "Black Foot Disease".

Metallisches Arsen dagegen zeigt wegen seiner Unlöslichkeit nur eine geringe Giftigkeit, da es vom Körper kaum aufgenommen wird (LD = 763 mg/kg Ratte, oral). Es sollte aber, da es sich an der Luft leicht mit seinen sehr giftigen Oxiden wie dem Arsenik überzieht, stets mit größter Vorsicht behandelt werden. Anders verhält es sich mit Arsenik, das in früheren Zeiten als Stimulans von Arsenikessern benutzt wurde, um einer Arsenvergiftung vorzubeugen. Der Mechanismus dieser Immunisierung gegen Arsen ist nicht bekannt.

Anionisches Arsen tritt als Arsenit ([AsO]) und Arsenat ([AsO]) in vielen Ländern im Grundwasser in hohen Konzentrationen auf. Durch Auswaschungen aus arsenhaltigen Erzen in Form von drei- und fünfwertigen Ionen trinken weltweit über 100 Millionen Menschen belastetes Wasser. Besonders in Indien, Bangladesh und Thailand, wo im 20. Jahrhundert mit internationaler Unterstützung zahlreiche Brunnen gegraben wurden, um von mit Krankheitserregern kontaminiertem Oberflächenwasser auf Grundwasser ausweichen zu können, führte diese unerkannte Belastung des Trinkwassers zu chronischer Arsenvergiftung bei weiten Teilen der betroffenen Bevölkerung. Das Problem kann, wo es bekannt wird, chemisch durch Oxidation der Arsenverbindungen und nachfolgende Ausfällung mit Eisenionen behoben werden. Von der Rice University wurde eine kostengünstige Filtermöglichkeit mit Nano-Magnetit entwickelt

Die Weltgesundheitsorganisation (WHO) empfiehlt seit 1992 einen Grenzwert für Arsen im Trinkwasser von 10 Mikrogramm pro Liter. Der Wert wird in vielen Staaten Europas und in den USA immer noch überschritten. In Deutschland wird er dagegen seit 1996 eingehalten. Eine Richtlinie der Europäischen Union (EU) von 1999 schreibt einen Höchstwert von 10 Mikrogramm pro Liter Trinkwasser EU-weit vor. Die USA verpflichteten sich im Jahre 2001, diesen Grenzwert ab 2006 einzuhalten.

Das im Grundwasser vorkommende Arsen reichert sich in Reis zehnmal so stark an wie in anderen Getreidearten; auf dem Weltmarkt angebotene Sorten enthalten zwischen 20 und 900 Mikrogramm Arsen pro Kilogramm. Im Jahr 2005 senkte die chinesische Regierung den zulässigen Gehalt anorganischer Arsenverbindungen von 700 auf 150 Mikrogramm pro Kilogramm Lebensmittel, im Juli 2014 beschloss die Codex Alimentarius-Kommission erstmals einen Höchstwert von 200 Mikrogramm für polierten Reis. Die für Lebensmittelsicherheit zuständige EU-Kommission diskutiert für Erzeugnisse aus Puffreis einen um 15 Prozent höheren Grenzwert und für spezielle Produkte für Kleinkinder einen nur halb so hohen (d. h. 100 Mikrogramm pro kg).

Für andere belastete Lebensmittel wie Bier oder Fruchtsäfte gibt es noch keine Grenzwerte, obwohl sie mehr Arsen enthalten können, als für Trinkwasser zulässig ist. Verbraucherorganisationen fordern für Apfelsaft einen Grenzwert von 3, höchstens aber 4,4 ppb (entspricht Mikrogramm pro kg).

Fische und Meeresfrüchte weisen zwar hohe Gehalte an Arsen auf, jedoch nahezu ausschließlich in der als unbedenklich geltenden organisch gebundenen Form. Grenzwerte wie z. B. für Quecksilber oder Cadmium gibt es nicht.

Das neue Chemikaliengesetz der EU umgesetzt in der Gefahrstoffverordnung Deutschlands von 2005 verbietet im Anhang 4 die „gewerbliche“ (nicht private) Verarbeitung von arsenhaltigen Mitteln und Zubereitungen, die mehr als 0,3 Gewichtsprozent an Arsen aufweisen. Derartige Grenzwertregelungen sind gegeben, da Arsen – genau wie Cadmium – in den Verzinkereien der Galvanikindustrie weltweit der Zinkschmelze zugesetzt wird, um die Haftungseigenschaften des Zinks an der Eisenoberfläche des zu verzinkenden Metallstückes zu verbessern. Auf Grund der Temperatur im Zink-Schmelzbad von 460 °C bis 480 °C kommt es zum Verdampfen von Arsen, Cadmium und anderen leicht flüchtigen Metallen und deren Anreicherung in der Luft des Arbeitsplatzes. So können jedoch zulässige Grenzwerte kurzfristig um das Tausendfache überschritten werden, mit der Folge der aerogen-alveolaren Aufnahme in den Körper. Messungen ergaben, dass Arsen (und Cadmium) im hochreinen Zink (99,995 Reinheitsgrad, DIN-1179-Reinheitsgrad) mit weniger als 0,0004 Gewichts-% ausgewiesen waren und nach Zugabe von 450 Gramm dieses hochreinen Zinks in die Zinkschmelze zu einem Anstieg der Cd-/As-Konzentration von 3 bis 7 µg/m³ Luft auf über 3000 µg/m³ Luft führten. Für Arsen wurde diese Tatsache überraschend in einer Verzinkerei durch Messung der Arsen-Konzentration in Zinkschmelze, Blut und Urin festgestellt (unveröffentlicht). Bei Galvanik-Arbeitern wird die Urin-Arsen-Konzentration mit 25 bis 68 µg/l Urin gemessen, im Vergleich zu unbelasteter Bevölkerung mit 0,1 µg Arsen/l Urin.

Für die Entfernung von ionischem Arsen aus dem Trinkwasser gibt es Verfahren, die auf Adsorption an Aktivkohle, aktiviertem Aluminiumoxid oder Eisenhydroxid-Granulat beruhen. Daneben werden Ionenaustauscher verwendet. Es ist möglich, Arsen mittels gentechnisch veränderten Pflanzen aus dem Boden zu entfernen, die es in Blättern speichern. Zur Phytosanierung von Trinkwasser bietet sich die Dickstielige Wasserhyazinthe an, die Arsen insbesondere in ihr Wurzelgewebe einlagert und so eine Abreicherung des kontaminierten Wassers bewirkt. Organische Arsenverbindungen in belasteten Böden können enzymatisch mit Hilfe von Pilzen abgebaut werden.

In Bangladesh wird nach einem Verfahren der schweizerischen Forschungseinrichtung EAWAG versucht, Arsen mit Hilfe von transparenten PET-Flaschen und Zitronensaft abzureichern. Bei dieser SORAS "(Solar Oxidation and Removal of Arsenic)" genannten Methode oxidiert Sonnenlicht das Arsen; die Inhaltsstoffe des Zitronensafts helfen bei der Ausfällung. Mit dieser kostengünstigen Methode lässt sich der Arsengehalt um 75 bis 90 Prozent senken.

In Gewässern des Yellowstone-Nationalparks, die sich aus Geysiren und anderen Thermalquellen vulkanischen Ursprungs speisen, wurden eukaryontische Algen der Gattung "Cyanidioschyzon" gefunden, die die hohen Arsenkonzentrationen der Gewässer tolerieren und sie zu biologisch weniger verfügbaren organischen Verbindungen oxidieren können. An einer Nutzung zur Abreicherung in Trinkwasser wird gearbeitet.

Als Antidote bei akuten Arsenvergiftungen stehen die schwefelhaltigen Komplexbildner Dimercaptopropansulfonsäure (=DMPS), Dimercaptobernsteinsäure und das ältere, schlechter verträgliche Dimercaprol zur Verfügung. Sie sind noch bei starken Arsendosen effektiv, wenn die Vergiftung rechtzeitig diagnostiziert wird. Ihr Stellenwert bei der Behandlung chronischer Arsenvergiftungen ist hingegen umstritten. Aktivkohle ein bis mehrere Stunden nach der Einnahme kann das Metall ebenfalls binden und zur Ausscheidung bringen.

Indische Forscher haben im Tierversuch herausgefunden, dass die Einnahme von Knoblauch zur Senkung der Arsengehalte im Blut und der Erhöhung der Arsengehalte im Urin führen kann. Erklärt wird dies über eine Ausfällung des Arsen bei Reaktion mit schwefelhaltigen Substanzen wie etwa Allicin, das Bestandteil des Knoblauchs ist. Zur Prophylaxe werden zwei bis drei Knoblauchzehen täglich empfohlen.

Arsenverbindungen zeigen beim Verbrennen eine wenig charakteristische fahlblaue Flammenfärbung. Bei der Glühröhrchenprobe erhitzt man Arsenverbindungen, welche teilweise sublimieren und sich an kalten Oberflächen in Form von schwarzem Arsen, weißem Arsen(III)-oxid oder gelbem Arsentrisulfid wieder niederschlagen.

Bei der Flammen-AAS werden die Arsenverbindungen in einer reduzierenden Luft-Acetylen-Flamme ionisiert. Anschließend wird eine Atomabsorptionsmessung bei 189,0 nm beziehungsweise 193,8 nm durchgeführt. Nachweisgrenzen bis zu 1 µg/ml wurden beschrieben. Häufig wird das Arsen auch mit Hilfe von NaBH in das gasförmige Arsin (AsH) überführt (Hydridtechnik). In der Quarzrohrtechnik wird AsH zuerst bei rund 1000 °C in einem elektrisch beheizten Quarzröhrchen thermisch in seine atomaren Bestandteile zersetzt, um anschließend die Absorption bei o.g. Wellenlängen zu bestimmen. Die Nachweisgrenze bei dieser Technik liegt bei 0,01 µg/l. Eine weitere Methode ist die sog. Graphitrohrtechnik, bei der das Arsen einer festen Probe bei 1700 °C und höher verflüchtigt und anschließend die Extinktion bei 193,8 nm gemessen wird.

Die Kopplung von Hydridtechnik mit dem induktiv gekoppelten Plasma/ laserinduzierter Fluoreszenzmessung ist eine sehr nachweisstarke Methode zur Bestimmung von Arsen. Mittels Hydriderzeugung freigesetztes AsH wird dabei im Plasma atomisiert und mit einem Laser zur Emission angeregt. Mit dieser Methode wurden Nachweisgrenzen von 0,04 ng/mL erreicht.

Bei der Massenspektrometrie wird die Arsenspezies zunächst durch ein induktiv gekoppeltes Argonplasma (ICP-MS) thermisch ionisiert. Anschließend wird das Plasma in das Massenspektrometer geleitet. Eine Nachweisgrenze von 0,2 µg/l wurde für Arsenit beschrieben.

Weitverbreitet ist die photometrische Erfassung von As als Arsenomolybdänblau. As(V) reagiert zunächst mit (NH)MoO. Danach folgt eine Reduktion mit SnCl oder Hydrazin zu einem blauen Komplex. Die Photometrie erfolgt bei 730 nm und ist somit nahezu störungsfrei. Die Nachweisgrenzen können durch Verwendung von basischen Farbstoffen als Komplexbildner verbessert werden.

Eine sehr empfindliche Arsenbestimmung im ppt-Bereich ist mittels Neutronenaktivierungsanalyse möglich. Sie kommt insbesondere dann zur Anwendung, wenn die Probe eine komplexe Zusammensetzung aufweist oder schwierig aufzuschließen ist. Allerdings gibt diese Methode keinen Hinweis auf die chemische Verbindung, in der das Arsen vorliegt. Bei der Wechselwirkung von Neutronen mit der Probe, die das natürliche Isotop Arsen-75 enthält, wird das schwerere Isotop Arsen-76 gebildet, das jedoch instabil ist und sich unter einem β-Zerfall in Selen-76 umwandelt. Gemessen werden dabei die β-Strahlen, über die ein Rückschluss auf die Menge des Arsens möglich ist.

Bei Biosensoren wird die Biolumineszenz bei Kontakt von in Wasser gelöstem Arsen mit genetisch modifizierten Bakterien (z. B. "Escherichia coli" K12) und eines Lichtmessgeräts (Luminometer) detektiert. Die vorhandene Arsenkonzentration korreliert dabei direkt mit der emittierten Lichtmenge.

Chemische Verbindungen von Arsen und Wasserstoff (→ Arsane) sind im Vergleich zu den entsprechenden Verbindungen der Hauptgruppennachbarn Stickstoff und Phosphor nicht sehr zahlreich und sehr instabil. Es sind zurzeit drei Arsane bekannt.

Arsen bildet mit Halogenen binäre Verbindungen vom Typ AsX, AsX und AsX (X bezeichnet das entsprechende Halogen).

Wichtige Sauerstoffsäuren sind:
Das wichtigste Arsenoxid ist Arsen(III)-oxid (Arsentrioxid auch Arsenik oder Weißarsenik, AsO, das Anhydrid der Arsenigen Säure), das in der Gasphase in Form von Doppelmolekülen mit der Formel AsO vorliegt. Es ist amphoter und weist damit auf den Halbmetallcharakter des Arsens hin. Neben AsO kennt man AsO (Arsenpentaoxid, das Anhydrid der Arsensäure) und das gemischte Anhydrid der Arsenigen Säure und Arsensäure AsO (Arsentetraoxid)

Ein historisch wichtiges Färbe- und Pflanzenschutzmittel ist ein Kupfer-Arsen-Oxid mit dem Trivialnamen Schweinfurter Grün (Cu(AsO)·Cu(CHCOO)).

Es bestehen zwei wichtige Arsensulfide, die beide als Minerale in der Natur vorkommen.

Wichtige Verbindungen von Arsen mit Metallen sind

In Analogie zu den Aminen und Phosphinen findet man entsprechende Verbindungen mit Arsen anstelle von Stickstoff oder Phosphor. Sie werden als Arsine bezeichnet.

Zu den Arsoranen, Verbindungen vom Typ RAs, wobei R für fünf – möglicherweise unterschiedliche – organische Gruppen steht, zählt man etwa Pentaphenylarsen oder Pentamethylarsen. Fehlt eine der fünf Gruppen, bleibt ein einfach positiv geladenes Ion zurück (R steht wiederum für – möglicherweise verschiedene – organische Gruppen), das man als Arsoniumion (AsR) bezeichnet.
Analog zu den Carbonsäuren lassen sich zwei Klassen arseno-organischer Säuren bilden:

Zudem sind Heteroaromaten mit Arsen als Heteroatom bekannt, wie Arsabenzol, das aus einem Benzolring besteht, in dem ein Kohlenstoffatom durch Arsen ersetzt ist und das somit analog zu Pyridin aufgebaut ist.
Auch homocyclische Arsenverbindungen existieren. Beispiele sind
deren Moleküle einen Fünf- beziehungsweise Sechsring aus Arsenatomen als Rückgrat aufweisen, an den nach außen hin je eine Methylgruppe pro Arsenatom gebunden ist. Eine polycyclische Variante bildet das nebenstehende Molekül, dessen Rückgrat sich aus einem Sechs- und zwei angehefteten Fünfringen zusammensetzt (R steht für jeweils eine "tert"-Butylgruppe).

Schließlich lassen sich Arsenpolymere darstellen, lange Kettenmoleküle, die als Polyarsine bezeichnet werden. Sie bestehen aus einer zentralen „Strickleiter“ der Arsenatome, an die außen auf jeder Seite je „Sprosse“ eine Methylgruppe angeheftet ist, so dass sich die chemische Formel (AsCH) ergibt, wobei die natürliche Zahl "n" weit über 100 liegen kann. Polyarsine zeigen deutliche Halbleitereigenschaften.

In der Bioorganik spielen Arsenolipide, Arsenosaccharide und arsenhaltige Glycolipide eine bedeutende Rolle. Wichtige Vertreter dieser Stoffklassen sind zum Beispiel Arsenobetain, Arsenocholin und unterschiedlich substituierte Arsenoribosen. Sie treten vor allem kumuliert in maritimen Lebewesen auf und können auf diesem Weg in die menschliche Nahrungskette gelangen. Arsenhaltige Biomoleküle konnten in Algen, Meeresschwämmen und in Fischgewebe nach erfolgter Extraktion mittels HPLC-ICP-MS nachgewiesen werden. Die Analytik von Organo-Arsenverbindungen (einschließlich ihrer Speziation) ist sehr aufwändig.

Das Element Arsen erreichte zweifelhafte Berühmtheit als Mordgift, belegt durch geschichtliche Aufzeichnungen sowie die Instrumentalisierung in Literatur und Film. Es handelte sich bei dem Mordgift allerdings nie um elementares Arsen, sondern um dessen Verbindungen.

In Italien und Frankreich starben Herzöge, Könige und Päpste an vorsätzlich herbeigeführten Arsenvergiftungen. Im Frankreich des 17. Jahrhunderts steht die Marquise de Brinvilliers, die ihren Vater und zwei Brüder mit einer Arsenikmischung vergiftete, im Mittelpunkt eines Giftskandals. In Deutschland brachte die Serienmörderin Gesche Gottfried aus Bremen 15 Menschen zu Tode. Aufsehen erregte auch der Fall der Serienmörderin Anna Margaretha Zwanziger zu Beginn des 19. Jahrhunderts. Die Urheber der Morde blieben jedoch meist unerkannt, da Arsen bis 1836 in kleinen Mengen nicht nachgewiesen werden konnte. Erst die durch James Marsh entwickelte und nach ihm benannte Marshsche Probe machte es möglich, Spuren des Elementes zu identifizieren und somit eine unnatürliche Todesursache nachzuweisen. Im 19. und 20. Jahrhundert fanden weiter vorsätzliche Vergiftungen mit arsenhaltigen Mitteln statt – zum einen, weil sie leicht als Herbizide verfügbar waren, zum anderen ließ sich bei chronischer Gabe kleiner Dosen ein krankheitsbedingter Tod vortäuschen. Im September 1840 fiel im Prozess gegen Marie Lafarge das erste Urteil, das alleine auf den Ergebnissen der Marshschen Probe beruhte. Im Fall der Marie Besnard, die angeblich zwischen 1927 und 1949 für mehrere Todesfälle in ihrem Umfeld in Loudun verantwortlich sein sollte, konnte ein eindeutiger Beweis nicht erbracht werden, weil Untersuchungsergebnisse widersprüchlich waren, und sie musste 1954 letztendlich freigesprochen werden.
Jahrelang glaubte die Fachwelt, dass der Tod des ehemaligen französischen Kaisers Napoleon Bonaparte mit 51 Jahren auf der Insel St. Helena einem Giftanschlag mit Arsen zugeschrieben werden muss. Zumindest hatte man in seinen Haaren hochkonzentrierte Spuren des Giftes entdeckt. Heute existieren verschiedene andere Thesen zur Erklärung des Faktenbefunds. Eine Möglichkeit besteht darin, dass das Arsen "nach" seinem Tod den Haaren beigegeben wurde, um diese zu konservieren, eine damals durchaus übliche Methode. Möglich ist ein Übermaß der Benutzung der arsenhaltigen Fowlersche Lösung, die zu seiner Zeit bei vielen seiner Zeitgenossen als medizinisches Wundermittel galt. Die dritte und heute als wahrscheinlichste angesehene Möglichkeit ist, dass sich Napoleon durch organische Arsenverbindungen vergiftete, die Schimmelpilze beständig aus seinen mit grünen Arsenpigmenten gefertigten Tapeten freisetzten. Deren hoher Arsengehalt ist durch eine 1980 in einem Notizbuch aufgefundene Materialprobe schlüssig belegt.

Der berühmte Philosoph René Descartes starb 1650 wenige Monate nach seiner Ankunft am Hofe der schwedischen Königin Christine. Der Verdacht, er sei von einem der Jesuiten, die sich am Hofe der protestantischen Königin aufhielten, aus religionspolitischen Gründen mit Arsen vergiftet worden, verstärkte sich, als Christine später tatsächlich zum Katholizismus konvertierte, konnte aber nicht erhärtet werden, so dass die offizielle Todesursache, Lungenentzündung, sich in den Biographien etablierte. Erst kürzlich wurde anhand von neu aufgefundenen und neu interpretierten Dokumenten der alte Verdacht erhärtet und behauptet, dass der „Giftmord an Descartes in sehr hohem Maße wahrscheinlich, um nicht zu sagen, fast sicher“ erscheint.

Im Jahre 1900 kam es im britischen Manchester zu einer Massenvergiftung, von der mehrere Tausend Menschen betroffen waren. Wie sich herausstellte, hatten alle Bier derselben Brauerei getrunken. In Vorstufen der Bierproduktion wurde anscheinend Schwefelsäure eingesetzt, die ihrerseits aus Schwefel hergestellt wurde, der aus mit Arsenopyrit kontaminierten Sulfidmineralen stammte. Etwa 70 Menschen erlagen ihren Vergiftungen.

In den Jahren 2010 und 2011 starben in Österreich zwei Männer an einer Arsenvergiftung. Am 11. April 2013 wurde am Landesgericht Krems eine 52-jährige Polin des Mordes an den beiden für schuldig befunden und von dem Geschworenengericht nicht rechtskräftig zu lebenslanger Haft verurteilt.
Noch in den 1950er Jahren auf dem Höhepunkt des Kalten Krieges erkrankte die US-amerikanische Botschafterin, Clare Booth Luce, in Rom durch eine Vergiftung mit dem aus Tapeten freigesetzten Arsen. Die Tatsache, dass die Krankheit auf die schimmelpilzbefallenen Tapeten und nicht auf gegnerische Geheimagenten zurückgeführt werden konnte, trug in diesem Fall nicht nur zur Genesung der Botschafterin, sondern auch zum Erhalt des Friedens bei.

In Friedrich Schillers bürgerlichem Trauerspiel „Kabale und Liebe“ vergiftet der junge Major Ferdinand von Walter erst seine Geliebte Luise Millerin und dann sich selbst. Allerdings tritt in „Kabale und Liebe“ der Tod unrealistischerweise binnen Minuten ein.

Die Protagonistin des berühmten Romans "Madame Bovary" von Gustave Flaubert, die unglücklich verheiratete Landarztgattin Emma Bovary, stirbt am Ende des Romans durch Suizid mit Arsen in Form eines weißen Pulvers. Der Spross einer Arztfamilie Flaubert beschreibt die Vergiftungssymptome und den äußerst qualvollen Tod der Bovary sehr detailliert.

Im Roman „Starkes Gift“ („Strong Poison“) von Dorothy L. Sayers ist das Opfer mit Arsen vergiftet worden. Die Verdächtige, Krimi-Schriftstellerin Harriet Vane, hat sich zur fraglichen Zeit intensiv mit Arsenmorden beschäftigt und sich dazu sogar vom Apotheker beraten lassen.

Der berühmte Detektiv „Kalle Blomquist“ aus dem gleichnamigen Kinderbuch von Astrid Lindgren wendete die Marshsche Probe an, um ein mit Arsen vergiftetes Stück Schokolade zu überprüfen.

In dem Theaterstück von Joseph Kesselring "Arsen und Spitzenhäubchen" (englisch: "Arsenic and Old Lace") vergiften zwei alte Damen in gutmeinender Absicht ältere einsame Herren mit einer Arsen-, Strychnin- und Zyankali-Mischung. Bekannt wurde das Stück durch die gleichnamige Verfilmung von Frank Capra mit Cary Grant, Peter Lorre und Priscilla Lane in den Hauptrollen.




</doc>
<doc id="98" url="https://de.wikipedia.org/wiki?curid=98" title="Astat">
Astat

Astat [] (von : „unbeständig, unstet“) ist ein radioaktives chemisches Element mit dem Elementsymbol At und der Ordnungszahl 85. Im Periodensystem steht es in der 7. Hauptgruppe bzw. der 17. IUPAC-Gruppe und zählt damit zu den Halogenen. Astat entsteht beim natürlichen Zerfall von Uran. Astat ist eines der seltensten natürlich vorkommenden Elemente der Erde, das bei Bedarf künstlich erzeugt werden muss.

Als Dmitri Mendelejew 1869 sein Periodensystem festlegte, sagte er die Existenz einiger zu dieser Zeit noch nicht entdeckter Elemente voraus, darunter eines, das den Platz unter Iod einnehmen würde. In der Folge versuchten einige Wissenschaftler dieses Element, das als „Eka-Iod“ bezeichnet wurde, zu finden.

Im Jahre 1931 behauptete Fred Allison, er und seine Mitarbeiter am Alabama Polytechnic Institute (heute Auburn University) hätten das fehlende Element entdeckt und gaben ihm die Bezeichnung "Alabamine" (Ab). Ihre Entdeckung konnte jedoch nicht bestätigt werden und wurde später als falsch erkannt.

Ebenfalls auf der Suche nach einem Mitglied der Familie des radioaktiven Thoriums fand der Chemiker De Rajendralal Mitra im Jahre 1937 in Dhaka, Bangladesch (damals Britisch-Indien), zwei neue Elemente. Das erste nannte er "Dakin" (Eka-Iod), wohl nach der englischen Bezeichnung für Dhaka (Dacca), das andere "Gourium". Beide Entdeckungen konnten jedoch nicht bestätigt werden.

Der Name "Helvetium" wurde wiederum von dem Schweizer Chemiker Walter Minder vorgeschlagen, als er die Entdeckung des Elements 85 im Jahr 1940 ankündigte. Er änderte im Jahr 1942 jedoch seinen Vorschlag in "Anglohelvetium".

Bestätigt werden konnte die Entdeckung des Astat (altgriechisch ἀστατέω = „unbeständig sein“, aufgrund des radioaktiven Zerfalls) erstmals im Jahre 1940 durch die Wissenschaftler Dale Corson, Kenneth MacKenzie und Emilio Gino Segrè, die es in der University of California künstlich durch Beschuss von Bismut mit Alphateilchen herstellten.

Drei Jahre später konnte das kurzlebige Element von Berta Karlik und Traude Bernert auch als Produkt des natürlichen Zerfallsprozesses von Uran gefunden werden.

Astat wird durch Beschuss von Bismut mit Alphateilchen im Energiebereich von 26 bis 29 MeV hergestellt. Man erhält dabei die relativ langlebigen Isotope At bis At, die dann im Stickstoffstrom bei 450 bis 600 °C sublimiert und an einer gekühlten Platinscheibe abgetrennt werden.

Bei diesem radioaktiven Element wurde mit Hilfe von Massenspektrometrie nachgewiesen, dass es sich chemisch wie die anderen Halogene, besonders wie Iod verhält (es sammelt sich wie dieses in der Schilddrüse an). Astat ist stärker metallisch als Iod.
Forscher am Brookhaven National Laboratory haben Experimente zur Identifikation und Messung von elementaren chemischen Reaktionen durchgeführt, die Astat beinhalten. 

Mit dem On-line-Isotopen-Massenseparator (ISOLDE) am CERN wurde 2013 das Ionisationspotenzial von Astat mit 9,31751(8) Elektronenvolt bestimmt.

Astat hat etwa 20 bekannte Isotope, die alle radioaktiv sind; das langlebigste ist At mit einer Halbwertszeit von 8,3 Stunden.

Organische Astatverbindungen dienen in der Nuklearmedizin zur Bestrahlung bösartiger Tumore. Astat-Isotope eignen sich aufgrund der kurzen Halbwertszeiten innerlich eingenommen als radioaktive Präparate zum Markieren der Schilddrüse. Das Element wird in der Schilddrüse angereichert und in der Leber gespeichert.

Die chemischen Eigenschaften von Astat konnten aufgrund der geringen Mengen bisher nur mit Tracerexperimenten festgestellt werden. Sie ähneln stark denjenigen des Iods, wobei es aber ein schwächeres Oxidationsmittel ist. Bisher konnten diverse Astatide, Interhalogenverbindungen und organische Verbindungen nachgewiesen werden. Auch die Anionen der entsprechenden Sauerstoffsäuren sind bekannt. Wegen des im Vergleich zu anderen Halogenen elektropositiveren Charakters wird es von Silber nur unvollständig ausgefällt. Dafür existiert das komplexstabilisierte Kation At(Py) (Py=Pyridin), wodurch Astat auch kathodisch abgeschieden werden kann. Nachgewiesen wurde auch das Hydrid, Astatwasserstoff HAt. Es spielt in der technischen Chemie jedoch keine Rolle.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen und eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielen. Auch Letzteres gilt nur, wenn es sich um eine dafür relevante Stoffmenge handelt.


</doc>
<doc id="99" url="https://de.wikipedia.org/wiki?curid=99" title="Alkalimetalle">
Alkalimetalle

Als Alkalimetalle werden die chemischen Elemente Lithium, Natrium, Kalium, Rubidium, Caesium und Francium aus der 1. Hauptgruppe des Periodensystems bezeichnet. Sie sind silbrig glänzende, reaktive Metalle, die in ihrer Valenzschale ein einzelnes Elektron besitzen. Obwohl Wasserstoff in den meisten Darstellungen des Periodensystems in der ersten Hauptgruppe steht und zum Teil ähnliche chemische Eigenschaften wie die Alkalimetalle aufweist, kann er nicht zu diesen gezählt werden, da er unter Standardbedingungen weder fest ist noch metallische Eigenschaften aufweist.

Der Name der Alkalimetalle leitet sich von dem arabischen Wort für „Pottasche“ ab, die alte Bezeichnung für aus Pflanzenaschen gewonnenes Kaliumcarbonat. Humphry Davy stellte im Jahre 1807 erstmals das Element Kalium durch eine Schmelzflusselektrolyse aus Kaliumhydroxid dar. Letzteres gewann er aus Kaliumcarbonat. In einigen Sprachen spiegelt sich dies im Namen wider. So heißt Kalium beispielsweise im Englischen und Französischen "potassium" und im Italienischen "potassio".

Alkalimetalle sind metallisch glänzende, silbrig-weiße (Ausnahme: Caesium hat bei geringster Verunreinigung einen Goldton), weiche Leichtmetalle. Sie sind mit dem Messer schneidbar. Alkalimetalle haben eine geringe Dichte. Sie reagieren mit vielen Stoffen, so beispielsweise mit Wasser, Luft oder Halogenen teilweise äußerst heftig unter starker Wärmeentwicklung. Insbesondere die schwereren Alkalimetalle können sich an der Luft selbst entzünden. Daher werden sie unter Schutzflüssigkeiten, wie Paraffin oder Petroleum (Lithium, Natrium und Kalium), bzw. unter Luftabschluss in Ampullen (Rubidium und Caesium) aufbewahrt.

Als Elemente der ersten Gruppe des Periodensystems besitzen sie nur ein schwach gebundenes s-Elektron, das sie leicht abgeben. Ihre ersten Ionisierungsenergien und ihre Elektronegativitäten sind entsprechend klein. In Verbindungen kommen sie alle fast ausschließlich als einwertige Kationen vor, wenngleich sogar Verbindungen bekannt sind, in denen diese Metalle anionisch vorliegen (z. B. Natride, komplexiert mit sogenannten Kryptanden).

Alkalimetalle und ihre Salze besitzen eine spezifische Flammenfärbung:

Aufgrund dieser Flammenfärbung werden Alkalimetallverbindungen für Feuerwerke benutzt.

In der Atomphysik werden Alkalimetalle eingesetzt, da sie sich aufgrund ihrer besonders einfachen elektronischen Struktur besonders leicht mit Lasern kühlen lassen.

Alle Alkalimetalle kristallisieren in der kubisch-raumzentrierten Struktur. Lediglich Lithium und Natrium kristallisieren in der hexagonal-dichtesten Packung, wenn tiefe Temperaturen vorherrschen.

Der Radius der Elementatome sowie der Kationen nimmt mit steigender Massenzahl zu. Viele andere Eigenschaften der Alkalimetalle zeigen einen Trend innerhalb der Gruppe von oben nach unten:

Die Alkalimetalle reagieren mit Wasserstoff unter Bildung salzartiger "Alkalimetallhydride":

Die thermische Beständigkeit der Hydride nimmt vom Lithiumhydrid (LiH) zum Caesiumhydrid (CsH) ab. Alkalihydride werden u. a. als Reduktions- oder Trockenmittel eingesetzt.

Mit Sauerstoff reagieren die Metalle unter Bildung fester, weißer "Alkalimetalloxide" (Lithiumoxid), "Alkalimetallperoxide" (Natriumperoxid) und "Alkalimetallhyperoxide" (Kaliumhyperoxid, Rubidiumhyperoxid, Caesiumhyperoxid):

Die Reaktion mit Wasser zu "Alkalimetallhydroxiden" erfolgt unter Freisetzung von Wasserstoff:

Vom Lithium zum Caesium steigt die Reaktivität stark an; ab dem Kalium erfolgt Selbstentzündung. Hochgeschwindigkeitsaufnahmen der Reaktion von Alkalimetallen mit Wasser legen eine Coulomb-Explosion nahe. Die Alkalimetallhydroxide sind farblose Feststoffe, die sich in Wasser unter starker Erwärmung leicht lösen und dabei stark basisch reagieren. Die Hydroxide und ihre Lösungen wirken stark ätzend.

Mit Halogenen reagieren die Alkalimetalle zu den salzartigen "Alkalimetallhalogeniden":

Die Reaktivität steigt vom Lithium zum Caesium und sinkt vom Fluor zum Iod. So reagiert Natrium mit Iod kaum und mit Brom sehr langsam, während die Reaktion von Kalium mit Brom und Iod explosionsartig erfolgt.

Alkalimetalle können Halogenkohlenwasserstoffen unter Explosionserscheinungen das Halogen unter Bildung von Kohlenstoff und dem entsprechenden Alkalimetallhalogenid entziehen:

Alkalimetalle ergeben mit flüssigem Ammoniak intensiv blau gefärbte Lösungen. Diese Lösungen, die aus positiven Alkalimetall-Ionen und solvatisierten Elektronen besteht, sind ein sehr starkes Reduktionsmittel und werden beispielsweise für die Birch-Reduktion eingesetzt. Wird diesen Lösungen ein geeigneter Komplexbildner zugesetzt, können sich entsprechende Salze mit Alkalimetall-Anionen, die sogenannten Alkalide, bilden.

Wasserstoff, das erste Element der 1. Hauptgruppe, ist unter Normalbedingungen ein Nichtmetall. Er wird deshalb nicht zu den Alkalimetallen gezählt, teilt jedoch mit ihnen einige Eigenschaften. Wasserstoff tritt wie die Alkalimetalle stets einwertig auf und wandelt sich unter extrem hohem Druck in eine metallische Hochdruckmodifikation um, den metallischen Wasserstoff. Umgekehrt haben auch einige Alkalimetalle unter bestimmten Bedingungen Eigenschaften wie Wasserstoff, z. B. besteht Lithium als Gas zu 1 % aus zweiatomigen Molekülen.




</doc>
<doc id="100" url="https://de.wikipedia.org/wiki?curid=100" title="Actinoide">
Actinoide

Actinoide [] („Actiniumähnliche“; griech.: Endung "-οειδής" "(-oeides)" „ähnlich“) ist eine Gruppenbezeichnung bestimmter ähnlicher Elemente. Zugerechnet werden ihr das Actinium und die 14 im Periodensystem folgenden Elemente: Thorium, Protactinium, Uran und die Transurane Neptunium, Plutonium, Americium, Curium, Berkelium, Californium, Einsteinium, Fermium, Mendelevium, Nobelium und Lawrencium. Im Sinne des Begriffs gehört Actinium nicht zu den Actiniumähnlichen, jedoch folgt die Nomenklatur der IUPAC hier dem praktischen Gebrauch. Die frühere Bezeichnung Actinide entspricht nicht dem Vorschlag der Nomenklaturkommission, da nach diesem die Endung „-id“ für binäre Verbindungen wie z. B. Chloride reserviert ist; die Bezeichnung ist aber weiterhin erlaubt. Alle Actinoide sind Metalle und werden auch als "Elemente der Actiniumreihe" bezeichnet.


Im Jahr 1934 publizierte die deutsche Chemikerin Ida Noddack eine Arbeit
über drei Lücken im Periodischen System der Elemente, die später mit den Elementen Francium, Astat und Promethium gefüllt wurden. Gleichsam nebenbei merkte sie an, dass es denkbar sei, dass "bei der Beschießung schwerer Kerne mit Neutronen diese Kerne in mehrere größere Bruchstücke zerfallen". Aber nicht nur das. Denkbar sei auch, dass Elemente mit Ordnungszahlen Z > 92, also Transurane, gebildet werden könnten.

Tatsächlich synthetisierten Edwin M. McMillan und Philip H. Abelson erstmals im Jahr 1940 die ersten nicht in der Natur vorkommenden Actinoid-Nuklide U, Np und Pu durch Beschuss von Uran mit Neutronen.

Da im Jahr 1940 noch kein Kernreaktor in Betrieb war, konnten die Neutronen nur aus einer Neutronenquelle stammen.

Obwohl bei dieser Kernreaktion letztlich auch ein Plutonium-Isotop entsteht, konnte Plutonium wahrscheinlich wegen der geringen Ausbeute noch nicht nachgewiesen werden. Als Entdeckungsjahr von Plutonium gilt das Jahr 1941, wie die Tabelle "Entdeckung der Transurane" zeigt.

Plutonium wurde von den US-Amerikanern Glenn T. Seaborg, J. W. Kennedy, E. M. McMillan, Michael Cefola und Arthur Wahl entdeckt. Ende 1940 stellten sie das Plutonium-Isotop Pu durch Beschuss des Uran-Isotops U mit Deuteronen her, die in einem Zyklotron beschleunigt worden waren. Nachdem der eindeutige Nachweis für das Element 94 erbracht worden war, erhielt es 1941 den Namen "Plutonium". Der Name lag nahe, da die beiden Vorgängerelemente nach den Planeten Uranus und Neptun benannt worden waren. Details über die Kernreaktionen sind im Artikel "Plutonium" nachzulesen.

Damit eine Transmutation mit elektrisch geladenen Teilchen wie Deuteronen stattfinden kann, müssen diese Teilchen auf eine Energie beschleunigt werden, die ausreicht, um die Coulombbarriere von Urankernen zu überwinden oder diese zumindest zu durchtunneln. Das war erstmals mit einem Zyklotron möglich. Die erste wägbare Menge Plutonium von etwa 4 µg wurde 1942 isoliert.
Ende 1942 wurde der erste Kernreaktor Chicago Pile in Betrieb genommen. Mit Kernreaktoren konnten vergleichsweise größere Mengen der Elemente Plutonium und Americium gewonnen werden, die als Targetmaterial dienten. In Verbindung mit geladenen Teilchen, mit beschleunigten α-Teilchen, wurden dann die Nuklide der Elemente Curium, Berkelium und Californium entdeckt. Der Vorteil des Verfahrens, der Beschuss beschleunigter geladener Teilchen auf leichtere Actinoide, ist, dass auch massereiche, neutronenarme Nuklide erhalten werden können, die durch eine Neutronenbestrahlung nicht gebildet werden.

Das effektivste Verfahren, um synthetische Actinoide zu erzeugen, ist die Transmutation von Uran- oder Thorium-Nukliden im Kernbrennstoff eines Kernreaktors durch Neutroneneinfang oder (n,2n)-Reaktionen. Dieses Verfahren ist auf nicht allzu massereiche Actinoid-Nuklide beschränkt, etwa bis zu Massenzahlen von A = 252. Rechnerisch (s. u) werden meist nur Nuklide bis A < 248 einbezogen. 

Americium zum Beispiel wurde in einem Zweistufenprozess entdeckt. In der ersten Stufe werden synthetische Actinoide in einem Kernreaktor gebildet, zum Beispiel Pu. Plutonium wird nach Entnahme des Kernbrennstoffs durch Wiederaufarbeitung chemisch extrahiert. Dann wird das Plutonium erneut im Reaktor oder mittels einer Quelle mit Neutronen bestrahlt. Der gegenwärtig betriebene 85 MW High-Flux-Isotope Reactor am Oak Ridge National Laboratory in Tennessee, USA ist auf die Herstellung von Transcuriumelementen (Z > 96) speziell ausgelegt.

Die massereichen Elemente Einsteinium und Fermium wurden durch den Test der ersten amerikanischen Wasserstoffbombe, Ivy Mike, am 1. November 1952 auf dem Eniwetok-Atoll auf unsere Erde gebracht. Nur wenige Wochen später wurden sie im Lawrence Berkeley National Laboratory, das an der geheimen Waffentwicklung nicht beteiligt war, auf Filterpapieren und in Ablagerungen an Korallen völlig unerwartet entdeckt. Bei der Wasserstoffbombenexplosion waren bis zu 16 Neutronen von einem Kern von U eingefangen worden. Dieser Zwischenkern zerfiel sehr schnell über eine Kette von Betazerfällen in Nuklide der bis dahin unbekannten Elemente. Das Uran stammte aus der Ummantelung der Wasserstoffbombe, die aus fünf Tonnen Natururan bestand.

Aus Gründen der militärischen Geheimhaltung durften die Ergebnisse zunächst nicht publiziert werden. Um sich dennoch die Priorität der Entdeckungen der beiden neuen Elemente zu sichern, wurden parallel erfolgreich Schwerionenreaktionen durchgeführt. U wurde mit N-Ionen, die an einem Zyklotron erzeugt wurden, bombardiert. Auf diesem Wege wurde zunächst Einsteinium synthetisiert und in der Publikation auf die Entdeckung von 1952 verwiesen. Ähnlich verfuhr man mit Fermium, das durch Beschuss mit Pu mit N-Ionen erzeugt wurde.

Wie die Tabelle zeigt, führten Schwerionenreaktionen auch zu den Elementen Mendelevium, Nobelium und Lawrencium mit den Ordnungszahlen 101 bis 103. Mit Schwerionenreaktionen wurden auch die Elemente mit höheren Ordnungszahlen synthetisiert.

Der Kernreaktor nimmt, was die Actinoide betrifft, nicht nur deshalb eine herausragende Stellung ein, weil er ohne Actinoide nicht betrieben werden könnte, sondern weil es nur innerhalb eines Kernreaktors möglich ist, größere wägbare Mengen von „höheren“ Actinoid-Nukliden zu bilden. Dieser Abschnitt beschreibt, welche Actinoid-Nuklide das sind und in welchem Massenanteil (relativ zur ursprünglichen Schwermetallmasse) sie gebildet werden.

In einem Kernreaktor werden zu Beginn des Betriebs große Mengen von Actinoiden in Form von Kernbrennstoff eingebracht, zum Beispiel mit U angereichertes Uran. In einem Leistungsreaktor sind Aktinoidmassen in der Größenordnung von 100 t erthalten. Aus diesem Kernbrennstoff werden neben der gewünschten Energiefreisetzung durch Kernspaltung synthetische „höhere“ Actinoide durch Transmutation erzeugt.

Die Abbildung "Nuklidkarte: Bildung und Zerfall von Actinoiden in einem Kernreaktor" ist eine Nuklidkarte in der Anordnung nach Segrè. Das heißt, die Anzahl der Neutronen wird nach rechts zunehmend, die Anzahl der Protonen nach oben zunehmend aufgetragen. Um die Zeichnungsfläche sinnvoll zu nutzen, ist der Nuklidpfad am roten Punkt unterteilt.

Jedes Quadrat der Abbildung stellt ein Actinoid-Nuklid dar, das in einem Kernreaktor auftreten kann. Die Abbildung zeigt auch das Netzwerk von Bildung und Zerfall von Actinoid-Nukliden, ausgelöst durch den Neutroneneinfang freier Neutronen und durch andere Kernreaktionen. Ein ähnliches Schema findet man in einer Arbeit aus dem Jahr 2014. Dort sind die Isotope von Thorium und Protactinium, die zum Beispiel in Salzschmelzenreaktoren eine große Rolle spielen, und die Isotope der Elemente Berkelium und Californium nicht enthalten.

Die Bildung von Actinoid-Nukliden wird in erster Linie geprägt durch:

Außer diesen neutronen- oder gammainduzierten Kernreaktionen wirkt sich auch die radioaktive Umwandlung der Actinoid-Nuklide auf den Nuklid-Bestand in einem Reaktor zu einem gegebenen Zeitpunkt (Abbrandzustand) aus. Diese Zerfallsarten sind in der Abbildung durch Diagonalpfeile markiert.

Eine große Rolle für die Bilanz der Teilchendichten der Nuklide spielt der Beta-Minus-Zerfall, markiert mit aufwärts zeigenden Diagonalpfeilen. Die Quadrate der Nuklide dieses Zerfallstyps haben in der Nuklidkarte eine hellblaue Füllfarbe. Zwei Nuklide sind nur zum Teil betaaktiv. Das rosafarben hinterlegte Nuklid Np wandelt sich zu 86 % durch Elektroneneinfang und zu 14 % durch Beta-Minus-Zerfall (hellblaue Ecke) um. Ebenfalls zwei Zerfallsarten, aber in nahezu umgekehrtem Verhältnis (83 % Beta-Minus-Zerfall, 17 % Elektroneneinfang) zeigt auch der Grundzustand von Am (rosa Ecke). Dieses Nuklid besitzt außerdem einen langlebigen metastabilen Zustand (weiß hinterlegt), der in der Regel durch Am symbolisiert wird. Nuklide mit Positronen-Emissionen (Beta-Plus-Zerfälle) kommen in einem Kernreaktor nicht vor.

Die Actinoid-Nuklide, die sich unter Aussendung von α-Teilchen spontan umwandeln, sind in der Nuklidkarte der Abbildung mit einer gelben Füllfarbe versehen worden. Der α-Zerfall spielt wegen der langen Halbwertszeiten (T) bei Bildung und Zerfall der Actinoide während der Aufenthaltsdauer des Brennstoffs in einem Leistungsreaktor (max. ca. 3 Jahre) so gut wie keine Rolle. Ausnahmen sind die beiden relativ kurzlebigen Nuklide Cm (T = 163 d) und Pu (T = 2.9 a). Nur für diese beiden Fälle ist der α-Zerfall durch lange, abwärts zeigende Pfeile auf der Nuklidkarte markiert.

Alle in der Abbildung angegebenen Halbwertszeiten sind die aktuellen evaluierten Nukleardaten NUBASE2012, abgerufen über den Nukleardaten-Viewer JANIS 4. Manche Halbwertszeiten sind gerundet.

Nur bei der ersten Inbetriebnahme des Reaktors kennt man die Zusammensetzung an Nukliden des Kernbrennstoffs genau. Es ist aber für jeden Zeitpunkt (Abbrandzustand) erforderlich, grundlegende physikalische Größen wie den Neutronenfluss zu kennen. Zu den grundlegenden physikalischen Größen gehören auch die Teilchendichten und "Massendichten" aller im Reaktor gebildeten Actinoid-Nuklide. Das betrifft sowohl die der anfangs eingebrachten (oder was davon noch vorhanden ist) als auch die der im Reaktorbetrieb gebildeten Nuklide. Die tatsächlichen Teilchendichten (und Massendichten) der Actinoid-Nuklide in Abhängigkeit vom Abbrand im laufenden Betrieb zu messen, ist nicht möglich. Erst nach der Entladung von Kernbrennstoff können diese Größen im Prinzip chemisch oder massenspektrometrisch untersucht werden. Das ist sehr aufwendig. Deshalb kommt einer Berechnung, die den Betriebsablauf eines Kernreaktors begleitet, ein hoher Stellenwert zu.

Die Entwicklung der Teilchendichten (und Massendichten) von Nukliden in einem Kernreaktor wird in Abhängigkeit vom mittleren Abbrand (engl. Burnup, Depletion) vereinfacht in sog. Abbrandprogrammen berechnet, zum Beispiel:



Für eine detaillierte Analyse werden hochkomplexe Reaktorprogrammsysteme "()" eingesetzt, deren Leistungsumfang weit über den der zuvor genannten Programme hinausgeht und deren Anwendung eine lange Einarbeitungszeit erfordert, zum Beispiel:



In letzteren Programmsystemen sind neben der anfänglichen Materialzusammensetzung auch geometrische Details von Reaktorbauteilen (Zellen) vorzugeben. Von Zeit zu Zeit werden solche Rechnungen mit den Ergebnissen von chemischer und massenspektrometrischer Analyse von entladenem Kernbrennstoff verglichen
und gegebenenfalls genauere Messungen noch unsicherer Nukleardaten oder genauere Berechnungsmethoden angestoßen.

Die Abbildung "Entwicklung der Massen der Actinoiden ..." zeigt die Zu- oder Abnahme der Massen der 14 häufigsten Actinoid-Nuklide in einem Druckwasserreaktor, der mit angereichertem Uran betrieben wird. Die Zahlenwerte wurden für eine größere Brennstoff-Zelle in Abhängigkeit vom Abbrand (der spezifischen Energiefreisetzung) berechnet. Die Berechnungen wurden im Jahr 2005 mit dem Programmsystem HELIOS 1.8 ausgeführt. Als Anfangsanreicherung des Nuklids U wurde für dieses Beispiel 4 % gewählt. Die Punkte auf den Kurven der Abbildung markieren die Schrittweiten in der Abbrandrechnung. Die Schrittweite ist anfangs kleiner, um auch diejenigen Spaltprodukte genauer zu erfassen, die ihre Sättigung sehr schnell erreichen. Das trifft vor allem auf die starken Neutronenabsorber Xe und Sm zu. Eine ähnliche Abbildung, eingeschränkt auf Uran- und Plutonium-Isotope, findet man im Lehrbuch "Neutron Physics" von Paul Reuss.

Die Masse jedes Nuklids wird durch die anfangs eingesetzte Masse an Schwermetall, der Masse des Urans, geteilt. Dargestellt sind die Massenanteile aller Nuklide, die anfangs vorhanden waren (drei Uran-Nuklide) oder die mit einer Masse von mindestens 10 g pro eingesetzter Tonne Schwermetall nach einem Abbrand von maximal 80 MWd/kg gebildet werden.

Wie die Abbildung zeigt, nehmen die Massen der drei anfangs vorhandenen Uranisotope U, U und U mit steigendem Abbrand monoton ab. Gleichzeitig nehmen die Massen der höheren Actinoide fast linear zu (man beachte die logarithmische Skalierung der Ordinate). Von allen synthetischen Actinoiden nimmt die Masse des Nuklids Pu am stärksten zu. Damit wächst auch die Anzahl der Spaltungen der Kerne des Nuklids Pu. Ab einem Abbrand von ca. 45 MWd/kg nimmt dessen Masse wieder geringfügig ab.

Würde man die Ordinate nach unten auf einen Massenanteil von mindestens 1 g pro eingesetzter Tonne Schwermetall herabsetzen, kämen auf der Abbildung ab einem Abrand von ca. 45 MWd/kg das Nuklid Am und ab einem Abrand von ca. 60 MWd/kg das Nuklid Cm hinzu.

Zusammengefasst: In einem Druckwasserreaktor mit Uran-Brennelementen (ohne MOX-Brennelemente) werden aus den ursprünglich vorhandenen Actinoid-Nukliden U und U (und einem geringen Anteil U) maximal 13 synthetische Actinoid-Nuklide innerhalb der üblichen Betriebszeiten gebildet, deren Anteil größer als 1 g je Tonne Startmasse Schwermetall (SM) ist, also 1 ppm. Das sind die Nuklide U, Np, Pu, Pu, Pu, Pu, Pu, Am, Am, Am, Cm, Cm und Cm. Nuklide der Elemente Berkelium und Californium werden in einem Kernreaktor ebenfalls, aber nur in sehr geringen Mengen gebildet.

Die Actinoide können, wie erwähnt, durch chemische Aufarbeitung von entladenem Brennstoff extrahiert werden. Der Massenanteil von Curium zum Beispiel beträgt ca. 0,00024 bei einem (gegenwärtig üblichen) Abbrand von 60 MWd/kg:
wobei formula_3 die Masse des Curiums und formula_4 die Startmasse des Schwermetalls bedeuten.
In einem Leistungsreaktor beträgt die anfängliche Schwermetallmasse ca. formula_5, verteilt auf 193 Brennelemente. Angenommen, alle Brennelemente seien entladen worden, die diesen Abbrandzustand erreicht haben. Folglich ist die Masse des Curiums
Im gesamten Reaktor sind bei diesem mittleren Abbrand im Brennstoff ca. 24 kg Curium entstanden.

Anzumerken ist, dass Leistungsreaktoren nicht betrieben werden, um Actinoide zu gewinnen, sondern um möglichst viele Actinoide zu spalten und Energie freizusetzen. Die Gesamtmasse aller Actinoide verringert sich durch Kernspaltung, und zwar bei einem mittleren Abbrand von 60 MWd/kg um insgesamt nur ca. 6 %. Diese findet sich in der Masse der Spaltprodukte wieder.

Die hervorgehobene Stellung der Actinoide, man denke an ihre Bedeutung zur nuklearen Energiefreisetzung und an Kernwaffen, werden durch die Eigenschaften ihrer Atomkerne determiniert.







Einige physikalische Eigenschaften der Actinoid-Elemente findet man unter dem Stichwort der Namen der Elemente. Die Voraussetzung dafür, dass klassische physikalische Eigenschaften, zum Beispiel Kristallstruktur, Massendichte, Schmelzpunkt, Siedepunkt oder elektrische Leitfähigkeit gemessen werden können, ist es, dass das Element in wägbaren Mengen gewonnen werden kann. Die Anzahl der tatsächlich gemessenen physikalischen Eigenschaften nimmt mit wachsender Ordnungszahl des Elements schnell ab. Zum Beispiel ist Californium das letzte Actinoid-Element, von dem die Massendichte gemessen werden konnte.

Zur Atomphysik der Actinoide ist anzumerken:




Alle Actinoide bilden dreifach geladene Ionen, sie werden wie das Actinium als Untergruppe der 3. Nebengruppe aufgefasst. Die „leichteren“ Actinoide (Thorium bis Americium) kommen in einer größeren Anzahl von Oxidationszahlen vor als die entsprechenden Lanthanoide.

Eigenschaften beziehen sich auf das häufigste bzw. stabilste Isotop.

Die vierwertigen Oxide der Actinoide kristallisieren im kubischen Kristallsystem; der Strukturtyp ist der CaF-Typ (Fluorit) mit der und den Koordinationszahlen "An"[8], O[4].

Die dreiwertigen Chloride der Actinoide kristallisieren im hexagonalen Kristallsystem. Die Struktur des Uran(III)-chlorids ist die Leitstruktur für eine Reihe weiterer Verbindungen. In dieser werden die Metallatome von je neun Chloratomen umgeben. Als Koordinationspolyeder ergibt sich dabei ein dreifach überkapptes, trigonales Prisma, wie es auch bei den späteren Actinoiden und den Lanthanoiden häufig anzutreffen ist. Es kristallisiert im hexagonalen Kristallsystem in der und zwei Formeleinheiten pro Elementarzelle.




</doc>
<doc id="101" url="https://de.wikipedia.org/wiki?curid=101" title="Americium">
Americium

Americium ist ein chemisches Element mit dem Elementsymbol Am und der Ordnungszahl 95. Im Periodensystem steht es in der Gruppe der Actinoide (7. Periode, f-Block) und zählt auch zu den Transuranen. Americium ist neben Europium das einzige nach einem Erdteil benannte Element. Es ist ein leicht verformbares radioaktives Metall silbrig-weißen Aussehens.

Von Americium gibt es kein stabiles Isotop. Auf der Erde kommt es ausschließlich in künstlich erzeugter Form vor. Das Element wurde erstmals im Spätherbst 1944 erzeugt, die Entdeckung jedoch zunächst nicht veröffentlicht. Kurioserweise wurde dessen Existenz in einer amerikanischen Radiosendung für Kinder durch den Entdecker Glenn T. Seaborg, den Gast der Sendung, der Öffentlichkeit preisgegeben.

Americium wird in Kernreaktoren gebildet, eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100 g des Elements. Es wird als Quelle ionisierender Strahlung eingesetzt, z. B. in der Fluoreszenzspektroskopie und in Ionisationsrauchmeldern. Das Americiumisotop Am wurde wegen seiner gegenüber Plutonium (Pu) wesentlich längeren Halbwertszeit von 432,2 Jahren zur Befüllung von Radionuklidbatterien (RTG) für Raumsonden vorgeschlagen, welche dann hunderte Jahre lang elektrische Energie zum Betrieb bereitstellen würden.

Americium wurde im Spätherbst 1944 von Glenn T. Seaborg, Ralph A. James, Leon O. Morgan und Albert Ghiorso im 60-Zoll-Cyclotron an der Universität von Kalifornien in Berkeley sowie am metallurgischen Laboratorium der Universität von Chicago (heute: Argonne National Laboratory) erzeugt. Nach Neptunium und Plutonium war Americium das vierte Transuran, das seit dem Jahr 1940 entdeckt wurde; das um eine Ordnungszahl höhere Curium wurde als drittes schon im Sommer 1944 erzeugt. Der Name für das Element wurde in Anlehnung zum Erdteil Amerika gewählt – in Analogie zu Europium, dem Seltene-Erden-Metall, das im Periodensystem genau über Americium steht: "The name americium (after the Americas) and the symbol Am are suggested for the element on the basis of its position as the sixth member of the actinide rare-earth series, analogous to europium, Eu, of the lanthanide series."

Zur Erzeugung des neuen Elements wurden in der Regel die Oxide der Ausgangselemente verwendet. Dazu wurde zunächst Plutoniumnitratlösung (mit dem Isotop Pu) auf eine Platinfolie von etwa 0,5 cm aufgetragen; die Lösung danach eingedampft und der Rückstand dann zum Oxid (PuO) geglüht. Nach dem Beschuss im Cyclotron wurde die Beschichtung mittels Salpetersäure gelöst, anschließend wieder mit einer konzentrierten wässrigen Ammoniaklösung als Hydroxid ausgefällt; der Rückstand wurde in Perchlorsäure gelöst. Die weitere Trennung erfolgte mit Ionenaustauschern. In ihren Versuchsreihen wurden der Reihe nach vier verschiedene Isotope erzeugt: Am, Am, Am und Am.

Als erstes Isotop isolierten sie Am aus einer Plutonium-Probe, die mit Neutronen bestrahlt wurde. Es zerfällt durch Aussendung eines α-Teilchens in Np. Die Halbwertszeit dieses α-Zerfalls wurde zunächst auf 510 ± 20 Jahre bestimmt; der heute allgemein akzeptierte Wert ist 432,2 a.

Als zweites Isotop wurde Am durch erneuten Neutronenbeschuss des zuvor erzeugten Am gefunden. Durch nachfolgenden raschen β-Zerfall entsteht dabei Cm, das zuvor schon entdeckte Curium. Die Halbwertszeit dieses β-Zerfalls wurde zunächst auf 17 Stunden bestimmt, der heute als gültig ermittelte Wert beträgt 16,02 h.

Erstmals öffentlich bekannt gemacht wurde die Entdeckung des Elements in der amerikanischen Radiosendung "Quiz Kids" am 11. November 1945 durch Glenn T. Seaborg, noch vor der eigentlichen Bekanntmachung bei einem Symposium der American Chemical Society: Einer der jungen Zuhörer fragte den Gast der Sendung, Seaborg, ob während des Zweiten Weltkrieges im Zuge der Erforschung von Nuklearwaffen neue Elemente entdeckt wurden. Seaborg bejahte die Frage und enthüllte dabei auch gleichzeitig die Entdeckung des nächsthöheren Elements, Curium.

Americium (Am und Am) und seine Produktion wurde später unter dem Namen "„Element 95 and method of producing said element“" patentiert, wobei als Erfinder nur Glenn T. Seaborg angegeben wurde.

In elementarer Form wurde es erstmals im Jahr 1951 durch Reduktion von Americium(III)-fluorid mit Barium dargestellt.

Americiumisotope entstehen im r-Prozess in Supernovae und kommen auf der Erde wegen ihrer im Vergleich zum Alter der Erde zu geringen Halbwertszeit nicht natürlich vor.

Heutzutage wird jedoch Americium als Nebenprodukt in Kernkraftwerken erbrütet; das Americiumisotop Am entsteht als Zerfallsprodukt (u. a. in abgebrannten Brennstäben) aus dem Plutoniumisotop Pu. Eine Tonne abgebrannten Kernbrennstoffs enthält durchschnittlich etwa 100 g verschiedener Americiumisotope. Es handelt sich dabei hauptsächlich um die α-Strahler Am und Am, die aufgrund ihrer relativ langen Halbwertszeiten in der Endlagerung unerwünscht sind und deshalb zum Transuranabfall zählen. Eine Verminderung der Langzeitradiotoxizität in nuklearen Endlagern wäre durch Abtrennung langlebiger Isotope aus abgebrannten Kernbrennstoffen möglich. Zur Beseitigung des Americiums wird derzeit die Partitioning & Transmutation-Strategie untersucht.

Americium fällt in geringen Mengen in Kernreaktoren an. Es steht heute in Mengen von wenigen Kilogramm zur Verfügung. Durch die aufwändige Gewinnung aus abgebrannten Brennstäben hat es einen sehr hohen Preis. Seit der Markteinführung 1962 soll der Preis für Americium(IV)-oxid mit dem Isotop Am bei etwa 1500 US-Dollar pro Gramm liegen. Das Americiumisotop Am entsteht in geringeren Mengen im Reaktor aus Am und ist deshalb mit 160 US-Dollar pro Milligramm Am noch wesentlich teurer.

Americium wird über das Plutoniumisotop Pu in Kernreaktoren mit hohem U-Anteil zwangsläufig erbrütet, da es aus diesem durch Neutroneneinfang und zwei anschließende β-Zerfälle (über U und Np) entsteht.

Danach wird, wenn es nicht zur Kernspaltung kommt, aus dem Pu, neben anderen Nukliden, durch stufenweisen Neutroneneinfang (n,γ) und anschließenden β-Zerfall Am oder Am erbrütet.

Das Plutonium, welches aus abgebrannten Brennstäben von Leistungsreaktoren gewonnen werden kann, besteht zu etwa 12 % aus dem Isotop Pu. Deshalb erreichen erst 70 Jahre, nachdem der Brutprozess beendet wurde, die abgebrannten Brennstäbe ihren Maximalgehalt von Am; danach nimmt der Gehalt wieder (langsamer als der Anstieg) ab.

Aus dem so entstandenen Am kann durch weiteren Neutroneneinfang im Reaktor Am entstehen. Bei Leichtwasserreaktoren soll aus dem Am zu 79 % Am und zu 10 % Am entstehen:

zu 79 %: formula_5

zu 10 %: formula_6

Für die Erbrütung von Am ist ein vierfacher Neutroneneinfang des Pu erforderlich:

Metallisches Americium kann durch Reduktion aus seinen Verbindungen erhalten werden. Zuerst wurde Americium(III)-fluorid zur Reduktion verwendet. Dieses wird hierzu in wasser- und sauerstofffreier Umgebung in Reaktionsapparaturen aus Tantal und Wolfram mit elementarem Barium zur Reaktion gebracht.

Auch die Reduktion von Americium(IV)-oxid mittels Lanthan oder Thorium ergibt metallisches Americium.

Im Periodensystem steht das Americium mit der Ordnungszahl 95 in der Reihe der Actinoide, sein Vorgänger ist das Plutonium, das nachfolgende Element ist das Curium. Sein Analogon in der Reihe der Lanthanoiden ist das Europium.

Americium ist radioaktives Metall. Frisch hergestelltes Americium ist ein silberweißes Metall, welches jedoch bei Raumtemperatur langsam matt wird. Es ist leicht verformbar. Sein Schmelzpunkt beträgt 1176 °C, der Siedepunkt liegt bei 2607 °C. Die Dichte beträgt 13,67 g·cm. Es tritt in zwei Modifikationen auf.

Die bei Standardbedingungen stabile Modifikation α-Am kristallisiert im hexagonalen Kristallsystem in der mit den Gitterparametern "a" = 346,8 pm und "c" = 1124 pm sowie vier Formeleinheiten pro Elementarzelle. Die Kristallstruktur besteht aus einer doppelt-hexagonal dichtesten Kugelpackung (d.h.c.p.) mit der Schichtfolge ABAC und ist damit isotyp zur Struktur von α-La.

Bei hohem Druck geht α-Am in β-Am über. Die β-Modifikation kristallisiert im kubischen Kristallsystem in der Raumgruppe  mit dem Gitterparameter "a" = 489 pm, was einem kubisch flächenzentrierten Gitter (f.c.c.) beziehungsweise einer kubisch dichtesten Kugelpackung mit der Stapelfolge ABC entspricht.

Die Lösungsenthalpie von Americium-Metall in Salzsäure bei Standardbedingungen beträgt −620,6 ± 1,3 kJ·mol. Ausgehend von diesem Wert erfolgte die erstmalige Berechnung der Standardbildungsenthalpie (Δ"H") von Am auf −621,2 ± 2,0 kJ·mol und des Standardpotentials Am / Am auf −2,08 ± 0,01 V.

Americium ist ein sehr reaktionsfähiges Element, das schon mit Luftsauerstoff reagiert und sich gut in Säuren löst. Gegenüber Alkalien ist es stabil.

Die stabilste Oxidationsstufe für Americium ist +3, die Am(III)-Verbindungen sind gegen Oxidation und Reduktion sehr stabil. Mit dem Americium liegt der erste Vertreter der Actinoiden vor, der in seinem Verhalten eher den Lanthanoiden ähnelt als den d-Block-Elementen.

Es ist auch in den Oxidationsstufen +2 sowie +4, +5, +6 und +7 zu finden. Je nach Oxidationszahl variiert die Farbe von Americium in wässriger Lösung ebenso wie in festen Verbindungen:Am (gelbrosa), Am (gelbrot), AmO (gelb), AmO (zitronengelb), AmO (dunkelgrün).

Im Gegensatz zum homologen Europium – Americium hat eine zu Europium analoge Elektronenkonfiguration – kann Am in wässriger Lösung nicht zu Am reduziert werden.

Verbindungen mit Americium ab Oxidationszahl +4 aufwärts sind starke Oxidationsmittel, vergleichbar dem Permanganat-Ion (MnO) in saurer Lösung.

Die in wässriger Lösung nicht beständigen Am-Ionen lassen sich nur noch mit starken Oxidationsmitteln aus Am(III) darstellen. In fester Form sind zwei Verbindungen des Americiums in der Oxidationsstufe +4 bekannt: Americium(IV)-oxid (AmO) und Americium(IV)-fluorid (AmF).

Der fünfwertige Oxidationszustand wurde beim Americium erstmals 1951 beobachtet. In wässriger Lösung liegen primär AmO-Ionen (sauer) oder AmO-Ionen (alkalisch) vor, die jedoch instabil sind und einer raschen Disproportionierung unterliegen:

Etwas beständiger als Am(IV) und Am(V) sind die Americium(VI)-Verbindungen. Sie lassen sich aus Am(III) durch Oxidation mit Ammoniumperoxodisulfat in verdünnter Salpetersäure herstellen. Der typische rosafarbene Ton verschwindet in Richtung zu einer starken Gelbfärbung. Zudem kann die Oxidation mit Silber(I)-oxid in Perchlorsäure quantitativ erreicht werden. In Natriumcarbonat- oder Natriumhydrogencarbonat-Lösungen ist eine Oxidation mit Ozon oder Natriumperoxodisulfat gleichfalls möglich.

Eine biologische Funktion des Americiums ist nicht bekannt. Vorgeschlagen wurde der Einsatz immobilisierter Bakterienzellen zur Entfernung von Americium und anderen Schwermetallen aus Fließgewässern. So können Enterobakterien der Gattung "Citrobacter" durch die Phosphataseaktivität in ihrer Zellwand bestimmte Americiumnuklide aus wässriger Lösung ausfällen und als Metall-Phosphat-Komplex binden. Ferner wurden die Faktoren untersucht, die die Biosorption und Bioakkumulation des Americiums durch Bakterien und Pilze beeinflussen.

Das Isotop Am hat mit rund 5700 barn den höchsten bisher (10/2008) gemessenen thermischen Spaltquerschnitt. Damit geht eine kleine kritische Masse einher, weswegen Am als Spaltmaterial vorgeschlagen wurde, um beispielsweise Raumschiffe mit Kernenergieantrieb anzutreiben.

Dieses Isotop eignet sich prinzipiell auch zum Bau von Kernwaffen. Die kritische Masse einer reinen Am-Kugel beträgt etwa 9–14 kg. Die Unsicherheiten der verfügbaren Wirkungsquerschnitte lassen derzeit keine genauere Aussage zu. Mit Reflektor beträgt die kritische Masse noch etwa 3–5 kg. In wässriger Lösung wird sie nochmals stark herabgesetzt. Auf diese Weise ließen sich sehr kompakte Sprengköpfe bauen. Nach öffentlichem Kenntnisstand wurden bisher keine Kernwaffen aus Am gebaut, was mit der geringen Verfügbarkeit und dem hohen Preis begründet werden kann.

Aus denselben Gründen wird Am auch nicht als Kernbrennstoff in Kernreaktoren eingesetzt, obwohl es dazu prinzipiell sowohl in thermischen als auch in schnellen Reaktoren geeignet wäre. Auch die beiden anderen häufiger verfügbaren Isotope, Am und Am können in einem schnellen Reaktor eine Kettenreaktion aufrechterhalten. Die kritischen Massen sind hier jedoch sehr hoch. Sie betragen unreflektiert 57,6–75,6 kg bei Am und 209 kg bei Am, so dass sich durch die Verwendung keine Vorteile gegenüber herkömmlichen Spaltstoffen ergeben.

Entsprechend ist Americium rechtlich nach dem deutschen Atomgesetz nicht den Kernbrennstoffen zugeordnet. Es existieren jedoch Vorschläge, sehr kompakte Reaktoren mit einem Americium-Inventar von lediglich knapp 20 g zu konstruieren, die in Krankenhäusern als Neutronenquelle für die Neutroneneinfangtherapie verwendet werden können.

Von Americium sind 16 Isotope und 11 Kernisomere mit Halbwertszeiten zwischen Bruchteilen von Mikrosekunden und 7370 Jahren bekannt. Es gibt zwei langlebige α-strahlende Isotope Am mit 432,2 und Am mit 7370 Jahren Halbwertszeit. Außerdem hat das Kernisomer Am mit 141 Jahren eine lange Halbwertszeit. Die restlichen Kernisomere und Isotope haben mit 0,64 µs bei Am bis 50,8 Stunden bei Am kurze Halbwertszeiten.

Am ist das am häufigsten erbrütete Americiumisotop und liegt auf der Neptunium-Reihe. Es zerfällt mit einer Halbwertszeit von 432,2 Jahren mit einem α-Zerfall zu Np. Am gibt nur mit einer Wahrscheinlichkeit von 0,35 % die gesamte Zerfallsenergie mit dem α-Teilchen ab, sondern emittiert meistens noch ein oder mehrere Gammaquanten.

Am ist kurzlebig und zerfällt mit einer Halbwertszeit von 16,02 h zu 82,7 % durch β-Zerfall zu Cm und zu 17,3 % durch Elektroneneinfang zu Pu. Das Cm zerfällt zu Pu und dieses weiter zu U, das auf der Uran-Radium-Reihe liegt. Das Pu zerfällt über die gleiche Zerfallskette wie Pu. Während jedoch Pu als Seitenarm beim U auf die Zerfallskette kommt, steht Pu noch vor dem U. Pu zerfällt durch α-Zerfall in U, den Beginn der natürlichen Uran-Radium-Reihe.

Am zerfällt mit einer Halbwertszeit von 141 Jahren zu 99,541 % durch Innere Konversion zu Am und zu 0,459 % durch α-Zerfall zu Np. Dieses zerfällt zu Pu und dann weiter zu U, das auf der Uran-Radium-Reihe liegt.

Am ist mit einer Halbwertszeit von 7370 Jahren das langlebigste Americiumisotop. Es geht zunächst durch α-Strahlung in Np über, das durch β-Zerfall weiter zu Pu zerfällt. Das Pu zerfällt durch α-Strahlung zu Uran U, dem offiziellen Anfang der Uran-Actinium-Reihe.

Die Americiumisotope mit ungerader Neutronenzahl, also gerader Massenzahl, sind gut durch thermische Neutronen spaltbar.

"→ Liste der Americiumisotope"

Für die Verwendung von Americium sind vor allem die beiden langlebigsten Isotope Am und Am von Interesse. In der Regel wird es in Form des Oxids (AmO) verwendet.
Die α-Strahlung des Am wird in Ionisationsrauchmeldern genutzt. Es wird gegenüber Ra bevorzugt, da es vergleichsweise wenig γ-Strahlung emittiert. Dafür muss aber die Aktivität gegenüber Radium ca. das Fünffache betragen. Die Zerfallsreihe von Am „endet“ für den Verwendungszeitraum quasi direkt nach dessen α-Zerfall bei Np, das eine Halbwertszeit von rund 2,144 Millionen Jahren besitzt.

Am wurde wegen seiner gegenüber Pu wesentlich längeren Halbwertszeit zur Befüllung von Radionuklidbatterien (RTG) von Raumsonden vorgeschlagen. Dank seiner Halbwertszeit von 432,2 Jahren könnte ein RTG mit Am-Füllung hunderte Jahre lang – anstatt nur einige Jahrzehnte (wie mit einer Pu-Füllung) – elektrische Energie zum Betrieb einer Raumsonde bereitstellen. Es soll voraussichtlich in den Radionuklidbatterien zum Einsatz kommen, deren Entwicklung die ESA erwägt und deren Entwicklung in den 2020er-Jahren abgeschlossen werden könnte.

Am als Oxid mit Beryllium verpresst stellt eine Neutronenquelle dar, die beispielsweise für radiochemische Untersuchungen eingesetzt wird. Hierzu wird der hohe Wirkungsquerschnitt des Berylliums für (α,n)-Kernreaktionen ausgenutzt, wobei das Americium als Produzent der α-Teilchen dient. Die entsprechenden Reaktionsgleichungen lauten:

Derartige Neutronenquellen kommen beispielsweise in der Neutronenradiographie und -tomographie zum Einsatz.

Neben dem häufig verwendeten Po als Ionisator zur Beseitigung von unerwünschter elektrostatischer Aufladung kam auch Am zum Einsatz. Dazu wurde z. B. die Quelle am Kopf einer Bürste montiert mit der man langsam über die zu behandelnden Oberflächen strich und dadurch eine Wiederverschmutzung durch elektrostatisch angezogene Staubpartikel vermeiden konnte.

Americium ist Ausgangsmaterial zur Erzeugung höherer Transurane und auch der Transactinoide. Aus Am entsteht zu 82,7 % Curium (Cm) und zu 17,3 % Plutonium (Pu). Im Kernreaktor wird zwangsläufig in geringen Mengen durch Neutroneneinfang aus Am das Am erbrütet, das durch β-Zerfall zum Curiumisotop Cm zerfällt.

In Teilchenbeschleunigern führt zum Beispiel der Beschuss von Am mit Kohlenstoffkernen (C) beziehungsweise Neonkernen (Ne) zu den Elementen Einsteinium Es beziehungsweise Dubnium Db.

Mit seiner intensiven Gammastrahlungs-Spektrallinie bei 60 keV eignet sich Am gut als Strahlenquelle für die Röntgen-Fluoreszenzspektroskopie. Dies wird auch zur Kalibrierung von Gammaspektrometern im niederenergetischen Bereich verwendet, da die benachbarten Linien vergleichsweise schwach sind und so ein einzeln stehender Peak entsteht. Zudem wird der Peak nur vernachlässigbar durch das Compton-Kontinuum höherenergetischer Linien gestört, da diese ebenfalls höchstens mit einer um mindestens drei Größenordnungen geringeren Intensität auftreten.

Einstufungen nach der CLP-Verordnung liegen nicht vor, weil diese nur die chemische Gefährlichkeit umfassen, welche eine völlig untergeordnete Rolle gegenüber den auf der Radioaktivität beruhenden Gefahren spielt. Eine chemische Gefahr liegt überhaupt nur dann vor, wenn es sich um eine dafür relevante Stoffmenge handelt.

Da von Americium nur radioaktive Isotope existieren, darf es selbst sowie seine Verbindungen nur in geeigneten Laboratorien unter speziellen Vorkehrungen gehandhabt werden. Die meisten gängigen Americiumisotope sind α-Strahler, weshalb eine Inkorporation unbedingt vermieden werden muss. Das breite Spektrum der hieraus resultierenden meist ebenfalls radioaktiven Tochternuklide stellt ein weiteres Risiko dar, das bei der Wahl der Sicherheitsvorkehrungen berücksichtigt werden muss. Am gibt beim radioaktiven Zerfall große Mengen relativ weicher Gammastrahlung ab, die sich gut abschirmen lässt.

Nach Untersuchungen des Forschers Arnulf Seidel vom Institut für Strahlenbiologie des Kernforschungszentrums Karlsruhe erzeugt Americium (wie Plutonium), bei Aufnahme in den Körper, mehr Knochentumore als dieselbe Dosis Radium.

Die biologische Halbwertszeit von Am beträgt in den Knochen 50 Jahre und in der Leber 20 Jahre. In den Gonaden verbleibt es dagegen offensichtlich dauerhaft.

"→ Kategorie: "

Von Americium existieren Oxide der Oxidationsstufen +3 (AmO) und +4 (AmO).

Americium(III)-oxid (AmO) ist ein rotbrauner Feststoff und hat einen Schmelzpunkt von 2205 °C.

Americium(IV)-oxid (AmO) ist die wichtigste Verbindung dieses Elements. Nahezu alle Anwendungen dieses Elements basieren auf dieser Verbindung. Sie entsteht unter anderem implizit in Kernreaktoren beim Bestrahlen von Urandioxid (UO) bzw. Plutoniumdioxid (PuO) mit Neutronen. Es ist ein schwarzer Feststoff und kristallisiert – wie die anderen Actinoiden(IV)-oxide – im kubischen Kristallsystem in der Fluorit-Struktur.

Halogenide sind für die Oxidationsstufen +2, +3 und +4 bekannt. Die stabilste Stufe +3 ist für sämtliche Verbindungen von Fluor bis Iod bekannt und in wässriger Lösung stabil.

Americium(III)-fluorid (AmF) ist schwerlöslich und kann durch die Umsetzung einer wässrigen Americiumlösung mit Fluoridsalzen im schwach Sauren durch Fällung hergestellt werden:

Das tetravalente Americium(IV)-fluorid (AmF) ist durch die Umsetzung von Americium(III)-fluorid mit molekularem Fluor zugänglich:

In der wässrigen Phase wurde das vierwertige Americium auch beobachtet.

Americium(III)-chlorid (AmCl) bildet rosafarbene hexagonale Kristalle. Seine Kristallstruktur ist isotyp mit Uran(III)-chlorid. Der Schmelzpunkt der Verbindung liegt bei 715 °C. Das Hexahydrat (AmCl·6 HO) weist eine monokline Kristallstruktur auf.

Durch Reduktion mit Na-Amalgam aus Am(III)-Verbindungen sind Am(II)-Salze zugänglich: die schwarzen Halogenide AmCl, AmBr und AmI. Sie sind sehr sauerstoffempfindlich, und oxidieren in Wasser unter Freisetzung von Wasserstoff zu Am(III)-Verbindungen.
Von den Chalkogeniden sind bekannt: das Sulfid (AmS), zwei Selenide (AmSe und AmSe) und zwei Telluride (AmTe und AmTe).

Die Pentelide des Americiums (Am) des Typs AmX sind für die Elemente Phosphor, Arsen, Antimon und Bismut dargestellt worden. Sie kristallisieren im NaCl-Gitter.

Americiummonosilicid (AmSi) und Americium„disilicid“ (AmSi mit: 1,87 < x < 2,0) wurden durch Reduktion von Americium(III)-fluorid mit elementaren Silicium im Vakuum bei 1050 °C (AmSi) und 1150–1200 °C (AmSi) dargestellt. AmSi ist eine schwarze Masse, isomorph mit LaSi. AmSi ist eine hellsilbrige Verbindung mit einem tetragonalen Kristallgitter.

Boride der Zusammensetzungen AmB und AmB sind gleichfalls bekannt.

Analog zu Uranocen, einer Organometallverbindung in der Uran von zwei Cyclooctatetraen-Liganden komplexiert ist, wurden die entsprechenden Komplexe von Thorium, Protactinium, Neptunium, Plutonium und auch des Americiums, (η-CH)Am, dargestellt.




</doc>
<doc id="102" url="https://de.wikipedia.org/wiki?curid=102" title="Atom">
Atom

<onlyinclude>Atome (von "átomos"‚ unteilbar) sind die Bausteine, aus denen alle festen, flüssigen oder gasförmigen Stoffe bestehen. Alle Materialeigenschaften dieser Stoffe sowie ihr Verhalten in chemischen Reaktionen werden durch die Eigenschaften und die räumliche Anordnung der Atome, aus denen sie aufgebaut sind, festgelegt. Jedes Atom gehört zu einem bestimmten chemischen Element und bildet dessen kleinste Einheit. Zurzeit sind 118 Elemente bekannt, von denen etwa 90 auf der Erde natürlich vorkommen. Atome verschiedener Elemente unterscheiden sich in ihrer Größe und Masse und vor allem in ihrer Fähigkeit, mit anderen Atomen chemisch zu reagieren und sich zu Molekülen oder festen Körpern zu verbinden. Die Durchmesser von Atomen liegen im Bereich von 6 ·10 m (Helium) bis 5 ·10 m (Cäsium), ihre Massen in einem Bereich von 1,7 ·10 kg (Wasserstoff) bis knapp 5 ·10 kg (die derzeit schwersten synthetisch hergestellten Kerne).

Atome sind nicht unteilbar, wie zum Zeitpunkt der Namensgebung angenommen, sondern zeigen einen wohlbestimmten Aufbau aus noch kleineren Teilchen. Sie bestehen aus einem Atomkern und einer Atomhülle. Der Atomkern hat einen Durchmesser von etwa einem Zehn- bis Hunderttausendstel des gesamten Atomdurchmessers, enthält jedoch über 99,9 % der Atommasse. Er besteht aus positiv geladenen Protonen und einer Anzahl von etwa gleich schweren, elektrisch neutralen Neutronen. Diese Nukleonen sind durch die starke Wechselwirkung aneinander gebunden. Die Hülle besteht aus negativ geladenen Elektronen. Sie trägt mit weniger als 0,06 % zur Masse bei, bestimmt jedoch die Größe des Atoms. Der positive Kern und die negative Hülle sind durch elektrostatische Anziehung aneinander gebunden. In der elektrisch neutralen Grundform des Atoms ist die Anzahl der Elektronen in der Hülle gleich der Anzahl der Protonen im Kern. Diese Zahl legt den genauen Aufbau der Hülle und damit auch das chemische Verhalten des Atoms fest und wird deshalb als "chemische Ordnungszahl" bezeichnet. Alle Atome desselben Elements haben die gleiche chemische Ordnungszahl. Sind zusätzliche Elektronen vorhanden oder fehlen welche, ist das Atom negativ bzw. positiv geladen und wird als Ion bezeichnet.

Die Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike, war jedoch bis in die Neuzeit umstritten. Der endgültige Nachweis konnte erst Anfang des 20. Jahrhunderts erbracht werden und gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie.</onlyinclude> Einzelne Atome sind selbst mit den stärksten Lichtmikroskopen nicht zu erkennen. Eine direkte Beobachtung einzelner Atome ist erst seit Mitte des 20. Jahrhunderts mit Feldionenmikroskopen möglich, seit einigen Jahren auch mit Rastertunnelmikroskopen und hochauflösenden Elektronenmikroskopen. Die Atomphysik, die neben dem Aufbau der Atome auch die Vorgänge in ihrem Inneren und ihre Wechselwirkungen mit anderen Atomen erforscht, hat entscheidend zur Entwicklung der modernen Physik und insbesondere der Quantenmechanik beigetragen.

Die Vorstellung vom atomaren Aufbau der Materie existierte bereits in der Antike. Aufgrund ihrer extrem geringen Größe sind einzelne Atome selbst mit den stärksten Lichtmikroskopen nicht zu erkennen, noch Anfang des 20. Jahrhunderts war ihre Existenz umstritten. Der endgültige Nachweis gilt als eine der bedeutendsten Entdeckungen in Physik und Chemie. Einen entscheidenden Beitrag lieferte Albert Einstein 1905, indem er die bereits seit langem bekannte, im Mikroskop direkt sichtbare Brownsche Bewegung kleiner Körnchen durch zufällige Stöße von Atomen oder Molekülen in deren Umgebung erklärte. Erst seit wenigen Jahrzehnten erlauben Feldionenmikroskope und Rastertunnelmikroskope, seit einigen Jahren zudem auch Elektronenmikroskope, einzelne Atome direkt zu beobachten.

Das Konzept des Atomismus, nämlich dass Materie aus Grundeinheiten aufgebaut ist – „kleinsten Teilchen“, die nicht immer weiter in kleinere Stücke zerteilt werden können – existiert seit Jahrtausenden, genauso wie das Gegenkonzept, Materie sei ein beliebig teilbares Kontinuum. Doch diese Ideen beruhten zunächst ausschließlich auf philosophischen Überlegungen und nicht auf empirischer experimenteller Untersuchung. Dabei wurden den Atomen verschiedene Eigenschaften zugeschrieben, und zwar je nach Zeitalter, Kultur und philosophischer Schule sehr unterschiedliche.

Eine frühe Erwähnung des Atomkonzepts in der Philosophie ist aus Indien bekannt. Die Nyaya- und Vaisheshika-Schulen entwickelten ausgearbeitete Theorien, wie sich Atome zu komplexeren Gebilden zusammenschlössen (erst in Paaren, dann je drei Paare).

Experimentell arbeitende Naturwissenschaftler machten sich Ende des 18. Jahrhunderts die Hypothese vom Atom zu eigen, weil diese Hypothese im Rahmen eines Teilchenmodells der Materie eine elegante Erklärung für neue Entdeckungen in der Chemie bot. Doch wurde gleichzeitig die gegenteilige Vorstellung, Materie sei ein Kontinuum, von Philosophen und auch unter Naturwissenschaftlern noch bis ins 20. Jahrhundert hinein aufrechterhalten.

In der griechischen Philosophie ist die Atomvorstellung erstmals im 5. Jahrhundert v. Chr. bei Leukipp überliefert. Sein Schüler Demokrit systematisierte sie und führte den Begriff "átomos" ein, was etwa „das Unzerschneidbare“ bedeutet, also ein nicht weiter zerteilbares Objekt. Diese Bezeichnung wurde Ende des 18. Jahrhunderts für die damals hypothetischen kleinsten Einheiten der chemischen Elemente der beginnenden modernen Chemie übernommen, denn mit chemischen Methoden lassen sich Atome in der Tat nicht „zerschneiden“.

Im Rahmen der wissenschaftlichen Erforschung konnte die Existenz von Atomen bestätigt werden. Es wurden viele verschiedene Atommodelle entwickelt, um ihren Aufbau zu beschreiben. Insbesondere das Wasserstoffatom als das einfachste aller Atome war dabei wichtig. Einige der Modelle werden heute nicht mehr verwendet und sind nur von wissenschaftsgeschichtlichem Interesse. Andere gelten je nach Anwendungsbereich als Näherung noch heute. In der Regel wird das einfachste Modell genommen, welches im gegebenen Zusammenhang noch ausreicht, um die auftretenden Fragen zu klären.

Robert Boyle vertrat 1661 in seinem Werk "The Sceptical Chymist" die Meinung, die Materie sei aus diversen Kombinationen verschiedener "corpuscules" aufgebaut und nicht aus den vier Elementen der Alchemie: Wasser, Erde, Feuer, Luft. Damit bereitete er die Überwindung der Alchemie durch den Element- und Atombegriff der modernen Chemie vor.

Daniel Bernoulli zeigte 1740, dass der gleichmäßige Druck von Gasen auf die Behälterwände und insbesondere das Gesetz von Boyle und Mariotte sich durch zahllose Stöße kleinster Teilchen erklären lässt. Damit wurde seine Forschung zum Vorläufer der kinetischen Gastheorie und statistischen Mechanik.

Ab Ende des 18. Jahrhunderts wurde die Vorstellung von Atomen genutzt, um die wohlbestimmten Winkel an den Kanten und Ecken der Edelsteine auf die verschiedenen möglichen Schichtungen von harten Kugeln zurückzuführen.

Nachdem Antoine Lavoisier 1789 den heutigen Begriff des chemischen Elements geprägt und die ersten Elemente richtig identifiziert hatte, benutzte 1803 John Dalton das Atomkonzept, um zu erklären, wieso Elemente immer in Mengenverhältnissen kleiner ganzer Zahlen miteinander reagieren (Gesetz der multiplen Proportionen). Er nahm an, dass jedes Element aus gleichartigen Atomen besteht, die sich nach festen Regeln miteinander verbinden können und so Stoffe mit anderen Materialeigenschaften bilden. Außerdem ging er davon aus, dass alle Atome eines Elements die gleiche Masse hätten, und begründete den Begriff Atomgewicht.

Die Beobachtungen zum chemischen und physikalischen Verhalten von Gasen konnte Amedeo Avogadro 1811 dahingehend zusammenfassen, dass zwei näherungsweise ideale Gase bei gleichen Werten von Volumen, Druck und Temperatur des Gases immer aus gleich vielen identischen Teilchen („Molekülen“) bestehen. Die Moleküle bestehen bei elementaren Gasen wie Wasserstoff, Sauerstoff oder Stickstoff immer aus zwei Atomen des Elements (Avogadrosches Gesetz).

1866 konnte Johann Loschmidt die Größe der Luftmoleküle bestimmen, indem er mit der von James C. Maxwell aus der kinetischen Gastheorie gewonnenen Formel die von George Stokes gemessenen Werte für die innere Reibung in Luft auswertete. Damit konnte er das Gewicht eines Luftmoleküls bestimmen. Außerdem erhielt er die nach ihm benannte Loschmidtsche Zahl als Anzahl der Luftmoleküle pro Kubikzentimeter.

Infolge der Arbeiten von Avogadro und Stanislao Cannizzaro wurde angenommen, dass Atome nicht als einzelne Teilchen auftreten, sondern nur als Bestandteile von Molekülen aus mindestens zwei Atomen. Doch 1876 gelang August Kundt und Emil Warburg der erste Nachweis eines einatomigen Gases. Sie bestimmten den Adiabatenexponenten von Quecksilber-Dampf bei hoher Temperatur und erhielten einen Wert, wie er nach der kinetischen Gastheorie nur für Teilchen in Gestalt echter Massepunkte auftreten kann. Ab 1895 kamen entsprechende Beobachtungen an den neu entdeckten Edelgasen hinzu.

Nach Erscheinen seiner Dissertation über die Bestimmung von Moleküldimensionen schlug Albert Einstein im selben Jahr 1905 ein Experiment vor, um die Hypothese von der Existenz der Atome anhand der Zitterbewegung kleiner Partikel in Wasser quantitativ zu prüfen. Nach seiner Theorie müssten die Partikel aufgrund der Unregelmäßigkeit der Stöße durch die Wassermoleküle kleine, aber immerhin unter dem Mikroskop sichtbare Bewegungen ausführen. Es war Einstein dabei zunächst nicht bekannt, dass er damit die seit 1827 bekannte Brownsche Bewegung von Pollen quantitativ erklärt hatte, für deren Ursache schon 1863 Christian Wiener erstmals Molekularstöße angenommen hatte. Der französische Physiker Jean Perrin bestimmte auf der Grundlage von Einsteins Theorie die Masse und Größe von Molekülen experimentell und fand ähnliche Ergebnisse wie Loschmidt. Diese Arbeiten trugen entscheidend zur allgemeinen Anerkennung der bis dahin so genannten „Atomhypothese“ bei.

Joseph John Thomson entdeckte 1897, dass die Kathodenstrahlen aus Teilchen bestimmter Ladung und Masse bestehen, deren Masse kleiner als ein Tausendstel der Atommasse ist. Diese Teilchen wurden als "Elektronen" bezeichnet und erwiesen sich als ein Bestandteil aller Materie, was dem Konzept des Atoms als unzerteilbarer Einheit widersprach. Thomson glaubte, dass die Elektronen dem Atom seine Masse verliehen und dass sie im Atom in einem masselosen, positiv geladenen Medium verteilt seien wie „Rosinen in einem Kuchen“ (Thomsonsches Atommodell).

Die kurz zuvor entdeckte Radioaktivität wurde 1903 von Ernest Rutherford und Frederick Soddy mit Umwandlungen verschiedener Atomsorten ineinander in Verbindung gebracht. Sie konnten 1908 nachweisen, dass α-Teilchen, die bei Alphastrahlung ausgesandt werden, Helium-Atome bilden.

Zusammen mit seiner Forschergruppe beschoss Ernest Rutherford 1909 eine Goldfolie mit α-Teilchen. Er stellte fest, dass die meisten der Teilchen die Folie fast ungehindert durchdrangen, einige wenige aber um sehr viel größere Winkel abgelenkt wurden, als nach Thomsons Modell möglich. Rutherford schloss daraus, dass fast die ganze Masse des Atoms in einem sehr viel kleineren, geladenen Atomkern in der Mitte des Atoms konzentriert sei (Rutherfordsches Atommodell). Die stark abgelenkten α-Teilchen waren diejenigen, die einem Kern zufällig näher als etwa ein Hundertstel des Atomradius gekommen waren. Die Ladungszahl des Atomkerns entpuppte sich als die chemische Ordnungszahl des betreffenden Elements, und α-Teilchen erwiesen sich als die Atomkerne des Heliums.

Der Chemiker Frederick Soddy stellte 1911 fest, dass manche der natürlichen radioaktiven Elemente aus Atomen mit unterschiedlichen Massen und unterschiedlicher Radioaktivität bestehen mussten. Der Begriff Isotop für physikalisch verschiedene Atome desselben chemischen Elements wurde 1913 von Margaret Todd vorgeschlagen. Da die Isotope desselben Elements an ihrem chemischen Verhalten nicht zu unterscheiden waren, entwickelte der Physiker J.J. Thomson ein erstes Massenspektrometer zu ihrer physikalischen Trennung. Damit konnte er 1913 am Beispiel von Neon nachweisen, dass es auch stabile Elemente mit mehreren Isotopen gibt.

1918 fand Francis William Aston mit einem Massenspektrometer von erheblich größerer Genauigkeit heraus, dass fast alle Elemente Gemische aus mehreren Isotopen sind, wobei die Massen der einzelnen Isotope immer (nahezu) ganzzahlige Vielfache der Masse des Wasserstoffatoms sind. Rutherford wies 1919 in der ersten beobachteten Kernreaktion nach, dass durch Beschuss mit α-Teilchen aus den Kernen von Stickstoffatomen die Kerne von Wasserstoffatomen herausgeschossen werden können. Diesen gab er den Namen Proton und entwickelte ein Atommodell, in dem die Atome nur aus Protonen und Elektronen bestehen, wobei die Protonen und ein Teil der Elektronen den kleinen, schweren Atomkern bilden, die übrigen Elektronen die große, leichte Atomhülle. Die Vorstellung von Elektronen im Atomkern stellte sich jedoch als falsch heraus und wurde fallengelassen, nachdem 1932 von James Chadwick das Neutron als ein neutraler Kernbaustein mit etwa gleicher Masse wie das Proton nachgewiesen wurde. Damit entstand das heutige Atommodell: Der Atomkern ist zusammengesetzt aus so vielen Protonen wie die Ordnungszahl angibt, und zusätzlich so vielen Neutronen, dass die betreffende Isotopenmasse erreicht wird.

1913 konnte Niels Bohr, aufbauend auf Rutherfords Atommodell aus Kern und Hülle, erstmals erklären, wie es in den optischen Spektren reiner Elemente zu den Spektrallinien kommt, die für das jeweilige Element absolut charakteristisch sind (Spektralanalyse nach Robert Wilhelm Bunsen und Gustav Robert Kirchhoff 1859). Bohr nahm an, dass die Elektronen sich nur auf bestimmten quantisierten Umlaufbahnen (Schalen) aufhalten und von einer zur anderen „springen“, sich jedoch nicht dazwischen aufhalten können. Beim Quantensprung von einer äußeren zu einer weiter innen liegenden Bahn muss das Elektron eine bestimmte Menge an Energie abgeben, die als Lichtquant bestimmter Wellenlänge erscheint. Im Franck-Hertz-Versuch konnte die quantisierte Energieaufnahme und -abgabe an Quecksilberatomen experimentell bestätigt werden. Das Bohrsche Atommodell ergab zwar nur für Systeme mit lediglich einem Elektron (Wasserstoff und ionisiertes Helium) quantitativ richtige Resultate. Jedoch bildete es im Laufe des folgenden Jahrzehnts das Fundament für eine Reihe von Verfeinerungen, die zu einem qualitativen Verständnis des Aufbaus der Elektronenhüllen aller Elemente führten. Damit wurde das Bohrsche Atommodell zur Grundlage des populären Bildes vom Atom als einem kleinen Planetensystem.

1916 versuchte Gilbert Newton Lewis, im Rahmen des Bohrschen Atommodells die chemische Bindung durch Wechselwirkung der Elektronen eines Atoms mit einem anderen Atom zu erklären. Walther Kossel ging 1916 erstmals von abgeschlossenen „Elektronenschalen“ bei den Edelgasen aus, um zu erklären, dass die chemischen Eigenschaften der Elemente grob periodisch mit der Ordnungszahl variieren, wobei sich benachbarte Elemente durch ein oder zwei zusätzliche oder fehlende Elektronen unterscheiden. Dies wurde bis 1921 von Niels Bohr zum „Aufbauprinzip“ weiterentwickelt, wonach mit zunehmender Kernladungszahl jedes weitere Elektron in die jeweils energetisch niedrigste Elektronenschale der Atomhülle, die noch Plätze frei hat, aufgenommen wird, ohne dass die schon vorhandenen Elektronen sich wesentlich umordnen.

Aufbauend auf dem von Louis de Broglie 1924 postulierten Welle-Teilchen-Dualismus entwickelte Erwin Schrödinger 1926 die Wellenmechanik. Sie beschreibt die Elektronen nicht als Massenpunkte auf bestimmten Bahnen, sondern als dreidimensionale "Materiewellen". Als Folge dieser Beschreibung ist es unter anderem unzulässig, einem Elektron gleichzeitig genaue Werte für Ort und Impuls zuzuschreiben. Dieser Sachverhalt wurde 1926 von Werner Heisenberg in der Unschärferelation formuliert. Demnach können statt der Bewegung auf bestimmten Bahnen nur "Wahrscheinlichkeitsverteilungen" für Wertebereiche von Ort und Impuls angegeben werden, eine Vorstellung, die nur schwer zu veranschaulichen ist. Den quantisierten Umlaufbahnen des Bohrschen Modells entsprechen hier stehende Materiewellen oder „Atomorbitale“. Sie geben unter anderem an, wie sich in der Nähe des Atomkerns die Aufenthaltswahrscheinlichkeit der Elektronen konzentriert, und bestimmen damit die wirkliche Größe des Atoms.

Die Beschreibung der Eigenschaften der Atome gelang mit diesem ersten vollständig quantenmechanischen Atommodell sehr viel besser als mit den Vorläufermodellen. Insbesondere ließen sich auch bei Atomen mit mehreren Elektronen die Spektrallinien und die Struktur der Atomhülle in räumlicher und energetischer Hinsicht darstellen, einschließlich der genauen Möglichkeiten, mit den Atomhüllen anderer Atome gebundene Zustände zu bilden, also stabile Moleküle. Daher wurde das Bohrsche Atommodell zugunsten des quantenmechanischen Orbitalmodells des Atoms verworfen.

Das Orbitalmodell ist bis heute Grundlage und Ausgangspunkt genauer quantenmechanischer Berechnungen fast aller Eigenschaften der Atome. Das gilt insbesondere für ihre Fähigkeit, sich mit anderen Atomen zu einzelnen Molekülen oder zu ausgedehnten Festkörpern zu verbinden. Bei Atomen mit mehreren Elektronen muss dafür außer dem Pauli-Prinzip auch die elektrostatische Wechselwirkung jedes Elektrons mit allen anderen berücksichtigt werden. Diese hängt u. a. von der Form der besetzten Orbitale ab. Andererseits wirkt sich umgekehrt die Wechselwirkung auf die Form und Energie der Orbitale aus. Es ergibt sich das Problem, die Orbitale in selbstkonsistenter Weise so zu bestimmen, dass sich ein stabiles System ergibt. Die Hartree-Fock-Methode geht von Orbitalen einer bestimmten Form aus und variiert diese systematisch, bis die Rechnung eine minimale Gesamtenergie ergibt. Wenn man die Orbitale nach der Dichtefunktionaltheorie bestimmen will, geht man von einer ortsabhängigen Gesamtdichte der Elektronen aus und bildet daraus eine Schrödingergleichung zur Bestimmung der Orbitale der einzelnen Elektronen. Hier wird die anfänglich angenommene Gesamtdichte variiert, bis sie mit der Gesamtdichte, die aus den besetzten Orbitalen zu berechnen ist, gut übereinstimmt.

Das Orbitalmodell bei einem Atom mit mehr als einem Elektron ist physikalisch als eine Näherung zu bezeichnen, nämlich als eine 1-Teilchen-Näherung. Sie besteht darin, dass jedem einzelnen Elektron ein bestimmtes Orbital zugeschrieben wird. Ein so gebildeter Zustand gehört zu der einfachsten Art von Mehrteilchenzuständen und wird hier als Konfiguration des Atoms bezeichnet. Genauere Modelle berücksichtigen, dass nach den Regeln der Quantenmechanik die Hülle auch in einem Zustand sein kann, der durch Superposition verschiedener Konfigurationen entsteht, wo also mit verschiedenen Wahrscheinlichkeitsamplituden gleichzeitig verschiedene Elektronenkonfigurationen vorliegen (eine sogenannte Konfigurationsmischung). Hiermit werden die genauesten Berechnungen von Energieniveaus und Wechselwirkungen der Atome möglich. Wegen des dazu nötigen mathematischen Aufwands werden jedoch, wo es möglich ist, auch weiterhin einfachere Atommodelle genutzt. Zu nennen ist hier das Thomas-Fermi-Modell, in dem die Elektronenhülle pauschal wie ein im Potentialtopf gebundenes ideales Elektronengas, das Fermigas, behandelt wird, dessen Dichte wiederum die Form des Potentialtopfs bestimmt.

Die Elektronen der Atomhülle sind aufgrund ihrer negativen Ladung durch elektrostatische Anziehung an den positiven Atomkern gebunden. Anschaulich bilden sie eine Elektronenwolke ohne scharfen Rand. Ein neutrales Atom enthält genauso viele Elektronen in der Hülle wie Protonen im Kern. Die Hülle hat einen etwa zehn- bis hunderttausend Mal größeren Durchmesser als der Kern, trägt jedoch weniger als 0,06 % zur Atommasse bei. Sie ist für energiereiche freie Teilchen (z. B. Photonen der Röntgenstrahlung oder Elektronen und Alphateilchen der radioaktiven Strahlung mit Energien ab einigen hundert Elektronenvolt (eV)) sehr durchlässig. Daher wird das Atom zuweilen als „weitgehend leer“ beschrieben.

Für geladene Teilchen geringer Energie im Bereich bis zu einigen zehn eV ist die Hülle aber praktisch undurchdringlich. In diesem Bereich liegen auch die kinetische Energie und die Bindungsenergie der Elektronen im äußeren Teil der Hülle. Daher erfahren zwei Atome immer eine starke Abstoßungskraft, wenn sie sich so weit annähern, dass sich ihre Hüllen merklich überschneiden würden. Der Bereich der kinetischen Energien ganzer Atome und Moleküle, wie sie unter normalen Bedingungen auf der Erde vorkommen, liegt noch deutlich darunter. Z. B. beträgt die thermische Energie formula_1 (formula_2 Boltzmannkonstante, formula_3 absolute Temperatur), die für die Größenordnung dieses Energiebereichs typisch ist, bei Raumtemperatur nur ungefähr 0,025 eV. Unter diesen Bedingungen ist die Atomhülle daher erstens stabil, weil ihr keine Elektronen entrissen werden, und zweitens undurchdringlich, weil sie sich nicht merklich mit den Hüllen anderer Atome überschneidet. Damit wird das Atom zum universellen Baustein der alltäglichen makroskopischen Materie. Seine, wenn auch nicht ganz scharf definierte, Größe verdankt es der gegenseitigen Undurchdringlichkeit der Hüllen.

Wenn sich die Hüllen zweier Atome aber nur geringfügig mit ihren äußeren Randbereichen überschneiden, kann zwischen ihnen eine anziehende Kraft entstehen. Sie ist die Ursache für die Entstehung von stabilen Molekülen, also den kleinsten Teilchen einer chemischen Verbindung. Bedingung ist, dass insgesamt ein Gewinn an Bindungsenergie damit einhergeht, dass ein oder zwei Elektronen von einer Hülle ganz oder mit gewisser Wahrscheinlichkeit zu der anderen Hülle überwechseln oder an beiden Hüllen beteiligt sind. Das ist nur bei genau passendem Aufbau beider Hüllen gegeben. Daher treten chemische Bindungen nur bei entsprechend geeigneten Kombinationen von Atomen auf.

Bei größeren Abständen, etwa bei einigen Atomdurchmessern, ziehen sich hingegen Atome aller Arten gegenseitig schwach an, unabhängig von der Möglichkeit, eine chemische Bindung einzugehen. Diese Van-der-Waals-Kräfte bewirken, dass jedes Gas bei genügend niedriger Temperatur zu einer Flüssigkeit oder einem Feststoff kondensiert. Sie sind also für den Wechsel der Aggregatzustände verantwortlich und wirken zwischen den neutralen Atomen bzw. Molekülen, sind aber auch elektrischen Ursprungs. Sie werden dadurch erklärt, dass sich zwei Atome durch leichte räumliche Verschiebung ihrer Elektronenwolken gegenseitig elektrische Dipolmomente induzieren, die einander elektrostatisch anziehen.

Der Chemiker Otto Hahn, ein Schüler Rutherfords, versuchte im Jahr 1938, durch Einfang von Neutronen an Urankernen Atome mit größerer Masse (Transurane) herzustellen, wie das bei leichteren Elementen seit Jahren gelungen war. Fritz Straßmann wies jedoch überraschenderweise nach, dass dabei das viel leichtere Barium entstanden war. Die Physiker Lise Meitner und Otto Frisch konnten den Vorgang als Kernspaltung identifizieren, indem sie mittels einer Ionisationskammer mehrere radioaktive Spaltprodukte nachwiesen.

Ab den 1950er Jahren konnten Atome durch die Entwicklung verbesserter Teilchenbeschleuniger und Teilchendetektoren beim Beschuss mit Teilchen sehr hoher Energie untersucht werden. Ende der 1960er Jahre zeigte sich in der „tiefinelastischen Streuung“ von Elektronen an Atomkernen, dass auch Neutronen und Protonen keine unteilbaren Einheiten sind, sondern aus Quarks zusammengesetzt sind.

1951 entwickelte Erwin Müller das Feldionenmikroskop und konnte damit von einer Nadelspitze erstmals ein Abbild erzeugen, das auf direkte Weise so stark vergrößert war, dass einzelne Atome darin sichtbar wurden (wenn auch nur als verschwommene Flecken). 1953 entwickelte Wolfgang Paul die magnetische Ionenfalle (Paulfalle), in der einzelne Ionen gespeichert und mit immer höherer Genauigkeit untersucht werden konnten.

1985 entwickelte eine Arbeitsgruppe um Steven Chu die Laserkühlung, ein Verfahren, die Temperatur einer Ansammlung von Atomen mittels Laser­strahlung stark zu verringern. Im selben Jahr gelang es einer Gruppe um William D. Phillips, neutrale Natriumatome in einer magneto-optischen Falle einzuschließen. Durch Kombination dieser Verfahren mit einer Methode, die den Dopplereffekt nutzt, gelang es einer Arbeitsgruppe um Claude Cohen-Tannoudji, geringe Mengen von Atomen auf Temperaturen von einigen Mikrokelvin zu kühlen. Mit diesem Verfahren können Atome mit höchster Genauigkeit untersucht werden; außerdem ermöglichte es auch die experimentelle Realisierung der Bose-Einstein-Kondensation.

Anfang der 1980er Jahre wurde von Gerd Binnig und Heinrich Rohrer das Rastertunnelmikroskop entwickelt, in dem eine Nadelspitze eine Oberfläche mittels des Tunneleffekts so fein abtastet, dass einzelne Atome sichtbar werden. Damit wurde es auch möglich, Atome einzeln an bestimmte Plätze zu setzen. In den 1990er Jahren konnten Serge Haroche und David Wineland in Experimenten die Wechselwirkung eines einzelnen Atoms mit einem einzelnen Photon erfolgreich untersuchen. In den 2000er Jahren wurde die Handhabbarkeit einzelner Atome unter anderem genutzt, um einen Transistor aus nur einem Metallatom mit organischen Liganden herzustellen.

Viele dieser Entdeckungen wurden mit dem Nobelpreis (Physik oder Chemie) ausgezeichnet.

Die Unterscheidung und Bezeichnung verschiedener Atomsorten geht zunächst vom Aufbau des Atomkerns aus, während der Zustand der Hülle gegebenenfalls durch zusätzliche Symbole angegeben wird. Kennzahlen sind die Protonenzahl (Ordnungszahl, Kernladungszahl) "Z", die Neutronenzahl "N" des Kerns, und die daraus gebildete Massenzahl "A=Z+N". Je nach ihrer Protonenzahl gehören die Atome zu einem der 118 bekannten chemischen Elemente, von Wasserstoff mit "Z"=1 bis Oganesson mit "Z"=118. Davon sind 91 in natürlichen Vorkommen entdeckt worden, 27 nur nach künstlicher Herstellung durch Kernreaktionen. Die Ordnung der Elemente wird im Periodensystem – wichtig für die Chemie – graphisch veranschaulicht. Darin werden die Elemente mit aufsteigender Ordnungszahl in Form einer Tabelle angeordnet. Jede Zeile wird als Periode des Periodensystems bezeichnet und endet, wenn das jeweilige Orbital mit Elektronen voll besetzt ist (Edelgas). In den nächsten Zeilen wiederholt sich aufgrund der schrittweisen Elektronenbesetzung der nächsten Orbitale der chemische Charakter der Elemente. So stehen Elemente mit ähnlichen chemischen Eigenschaften in einer Spalte untereinander; sie bilden eine Gruppe des Periodensystems.

Atome eines Elements, die sich in der Neutronenzahl unterscheiden, gehören zu verschiedenen Isotopen des Elements. Insgesamt bestehen die 118 Elemente aus etwa 2800 Isotopen, wovon 2500 künstlich erzeugt wurden. Isotope werden – bis auf die Ausnahmen der Wasserstoffisotope Deuterium und Tritium – nach dem chemischen Element und der Massenzahl bezeichnet. Das Symbol für ein bestimmtes Isotop des Elements X hat die Form formula_4, formula_5 oder X-"A" (Beispiele: formula_6, formula_7, Pb-208). Die Angabe der Protonenzahl "Z" ist redundant, da sich "Z" schon aus der Ordnungszahl des Elements ergibt.

Nuklid ist die ganz allgemeine Bezeichnung für Atomarten, unabhängig davon, ob sie zum gleichen Element gehören oder nicht. Die Nuklidkarte oder Isotopenkarte – wichtig für die Kernphysik und ihre Anwendungen – ist eine Tabelle, in der jede Atomart einen eigenen Platz erhält. Dazu wird auf einer Achse die Anzahl der Protonen, auf der anderen die der Neutronen aufgetragen. Häufig wird die Stabilität und bei instabilen Nukliden auch die Art der Umwandlung oder die Größenordnung der Halbwertszeit durch bestimmte Farben dargestellt.

Der Atomkern eines Nuklids formula_8 kann im energetischen Grundzustand und in verschiedenen Anregungszuständen vorliegen. Wenn darunter relativ langlebige, sogenannte metastabile Zustände sind, werden diese als Isomere bezeichnet und als eigene Nuklide gezählt (Symbol formula_9, formula_10 o. ä.). Nach dieser Definition sind mit dem Stand von 2003 insgesamt etwa 3200 Nuklide bekannt.

In der Kernphysik werden Nuklide mit unterschiedlichen Protonenzahlen, aber gleicher Massenzahl formula_11 als Isobare bezeichnet. Seltener werden unter dem Namen Isotone Nuklide mit verschiedenen Protonenzahlen, aber gleicher Neutronenzahl zusammengefasst.

Nur etwa 250 Isotope von 80 Elementen haben einen stabilen Kern. Alle anderen Atome sind instabil und wandeln sich über kurz oder lang in Atome eines stabilen Isotops um. Da sie dabei im Allgemeinen ionisierende Strahlung aussenden, heißen sie auch Radioisotope oder Radionuklide. Auf der Erde wurden in den natürlichen Vorkommen neben allen 250 stabilen Isotopen 30 Radioisotope gefunden, die sich auf 10 radioaktive Elemente verteilen und die natürliche Radioaktivität verursachen. Viele weitere kurzlebige Isotope existieren im Inneren von Sternen, insbesondere während der Supernova-Phase.

Als Rydberg-Atom wird ein Atom bezeichnet, in dem ein Elektron in einem so hohen Energiezustand angeregt ist, dass es den Atomkern, teilweise auch den gesamten Atomrumpf, bestehend aus dem Atomkern und den restlichen Elektronen, in weitem Abstand umkreist und sein Verhalten damit dem eines klassischen Teilchens ähnelt. Rydberg-Atome können über 100.000 mal größer sein als nicht angeregte Atome. Da sie extrem empfindlich auf äußere Felder reagieren, kann man mit ihnen z. B. die Wechselwirkung mit einem einzelnen Photon im Detail untersuchen. Sind zwei oder mehr Elektronen in solchen Zuständen angeregt, spricht man von planetarischen Atomen.

Im teils übertragenen Sinn werden als exotische Atome auch solche Systeme bezeichnet, die in physikalischer Hinsicht gewisse Ähnlichkeiten zu den gewöhnlichen Atomen aufweisen. In ihnen kann z. B. eines der Protonen, Neutronen oder Elektronen durch ein anderes Teilchen derselben Ladung ersetzt worden sein. Wird etwa ein Elektron durch ein schwereres Myon ersetzt, bildet sich ein myonisches Atom. Als Positronium wird ein exotisches Atom bezeichnet, in dem ein Elektron statt an ein Proton an ein Positron, das ist das positiv geladene Antiteilchen des Elektrons, gebunden ist. Auch Atome, die gänzlich aus Antiteilchen zur normalen Materie aufgebaut sind, sind möglich. So wurden erstmals 1995 am Genfer CERN Antiwasserstoffatome künstlich hergestellt und nachgewiesen. An solchen exotischen Atomen lassen sich unter anderem fundamentale physikalische Theorien überprüfen.

Des Weiteren wird der Name Atom manchmal auch für 2-Teilchen-Systeme verwendet, die nicht durch elektromagnetische Wechselwirkung zusammengehalten werden, sondern durch die starke Wechselwirkung. Bei einem solchen Quarkonium handelt es sich um ein kurzlebiges Elementarteilchen vom Typ Meson, das aus einem Quark und seinem Antiteilchen aufgebaut ist. Ein Quarkonium-Atom lässt sich in seinen verschiedenen metastabilen Zuständen so durch Quantenzahlen klassifizieren wie das Wasserstoffatom.

Etwa eine Sekunde nach dem Urknall kamen die ständigen Umwandlungen zwischen den Elementarteilchen zur Ruhe, übrig blieben Elektronen, Protonen und Neutronen. In den darauf folgenden drei Minuten verbanden sich in der primordialen Nukleosynthese die vorhandenen Neutronen mit Protonen zu den einfachsten Kernen: Deuterium, Helium, in geringerem Umfang auch Lithium und möglicherweise in noch kleineren Mengen Beryllium und Bor. Die übrigen Protonen (86 %) blieben erhalten. Die ersten neutralen Atome mit dauerhaft gebundenen Elektronen wurden erst 380.000 Jahre nach dem Urknall in der Rekombinationsphase gebildet, als das Universum durch Expansion so weit abgekühlt war, dass die Atome nicht sogleich wieder ionisiert wurden.

Die Kerne aller schwereren Atome wurden und werden durch verschiedene Prozesse der Kernfusion erzeugt. Am wichtigsten ist die stellare Nukleosynthese, durch die in Sternen zunächst Helium, anschließend auch die schwereren Elemente bis zum Eisen gebildet werden. Elemente mit höheren Kernladungszahlen als Eisen entstehen in explosionsartigen Vorgängen wie im r-Prozess in Supernovae und im s-Prozess in AGB-Sternen, die kurz vor dem Ende ihrer Lebensdauer sind.

Kleine Mengen verschiedener Elemente und Isotope werden auch dadurch gebildet, dass schwere Kerne wieder geteilt werden. Das geschieht durch radioaktive Zerfälle (siehe Zerfallsreihe), die u. a. für einen Teil des Vorkommens von Helium und Blei verantwortlich sind, und Spallationen, die für die Entstehung von Lithium, Beryllium und Bor wichtig sind.

Im beobachtbaren Universum liegen die Atome mit einer mittleren Dichte von 0,25 Atome/m³ vor. Nach dem Urknallmodell (Lambda-CDM-Modell) bilden sie etwa 4,9 % der gesamten Energiedichte. Der Rest, dessen Natur noch weitgehend unklar ist, setzt sich aus etwa 27 % dunkler Materie und 68 % dunkler Energie zusammen, sowie kleinen Beiträgen von Neutrinos und elektromagnetischer Strahlung. Im Inneren einer Galaxie wie etwa der Milchstraße ist im interstellaren Medium (ISM) die Dichte der Atome wesentlich höher und liegt zwischen 10 und 10 Atome/m. Die Sonne befindet sich in der weitgehend staubfreien lokalen Blase, daher ist die Dichte in der Umgebung des Sonnensystems nur etwa 10 Atome/m. In festen Himmelskörpern wie der Erde beträgt die Atomdichte etwa 10 Atome/m.

In der Verteilung der unterschiedlichen Elemente dominiert im Universum Wasserstoff mit rund 75 % der Masse, danach folgt Helium mit etwa 25 %. Alle schwereren Elemente sind viel seltener und machen nur einen kleinen Teil der im Universum vorhandenen Atome aus. Ihre Häufigkeiten werden von den verschiedenen Mechanismen der Nukleosynthese bestimmt.

Im Sonnensystem sind Wasserstoff und Helium vorwiegend in der Sonne und den Gasplaneten enthalten. Dagegen überwiegen auf der Erde die schweren Elemente. Die häufigsten Elemente sind hier Sauerstoff, Eisen, Silicium und Magnesium. Der Erdkern besteht vorwiegend aus Eisen, während in der Erdkruste Sauerstoff und Silicium vorherrschen.

Die beiden Hauptbestandteile eines Atoms sind der Atomkern und die Atomhülle. Die Hülle besteht aus Elektronen. Sie trägt mit weniger als 0,06 % zur Masse des Atoms bei, bestimmt aber dessen Größe und dessen Verhalten gegenüber anderen Atomen, wenn sie einander nahe kommen. Der Kern besteht aus Protonen und Neutronen, ist im Durchmesser zehn- bis hunderttausendmal kleiner als die Hülle, enthält aber mehr als 99,9 % der Masse des Atoms.

Die in einem Atom vorhandenen Protonen und Neutronen, zusammen auch als Nukleonen bezeichnet, sind aneinander gebundenen und bilden den Atomkern. Die Nukleonen zählen zu den Hadronen. Das Proton ist positiv geladen, das Neutron ist elektrisch neutral. Proton und Neutron haben einen Durchmesser von etwa 1,6 fm (Femtometer) und sind selber keine Elementarteilchen, sondern nach dem Standardmodell der Elementarteilchenphysik aus den punktförmigen Quarks aufgebaut. Jeweils drei Quarks binden sich durch die starke Wechselwirkung, die durch Gluonen vermittelt wird, zu einem Nukleon. Die starke Wechselwirkung ist darüber hinaus für den Zusammenhalt der Nukleonen im Atomkern verantwortlich, insbesondere ist die Anziehung bis zu etwa 2,5 fm Abstand deutlich stärker als die gegenseitige elektrische Abstoßung der Protonen. Unterhalb von etwa 1,6 fm wird die starke Wechselwirkung der Hadronen jedoch stark abstoßend. Anschaulich gesprochen verhalten sich die Nukleonen im Kern also etwa wie harte Kugeln, die aneinander haften. Daher steigt das Volumen des Kerns proportional zur Nukleonenzahl (Massenzahl) formula_11. Sein Radius beträgt etwa formula_13 fm.

Der leichteste Atomkern besteht aus nur einem Proton. Mehrere Protonen stoßen sich zwar gemäß der Elektrostatik ab, können zusammen mit einer geeigneten Anzahl von Neutronen aber ein stabiles System bilden. Doch schon bei kleinen Abweichungen von dem energetisch günstigsten Zahlenverhältnis ist der Kern instabil und wandelt sich spontan um, indem aus einem Neutron ein Proton wird oder umgekehrt und die frei werdende Energie und Ladung als Betastrahlung abgegeben wird. Kerne mit bis zu etwa 20 Protonen sind nur bei annähernd gleich großer Neutronenzahl stabil. Darüber steigt in den stabilen Atomkernen das Verhältnis von Neutronen zu Protonen von 1:1 bis auf etwa 1,5:1, weil bei größeren Protonenzahlen wegen ihrer elektrostatischen Abstoßung die Anzahl der Neutronen schneller anwachsen muss als die der Protonen (Details siehe Tröpfchenmodell). Die Bindungsenergie liegt in stabilen Kernen (abgesehen von den leichtesten) oberhalb von 7 MeV pro Nukleon (siehe Abbildung) und übertrifft damit die Bindungsenergie der äußeren Elektronen der Atomhülle oder die chemische Bindungsenergie in stabilen Molekülen um das ca. 10-fache. Kerne mit bestimmten Nukleonenzahlen, die als Magische Zahl bezeichnet werden, beispielsweise Helium-4, Sauerstoff-16 oder Blei-208, sind besonders stabil, was mit dem Schalenmodell des Atomkerns erklärt werden kann.

Oberhalb einer Zahl von 82 Protonen (also jenseits von Blei) sind alle Kerne instabil. Sie wandeln sich durch Ausstoßen eines Kerns He-4 in leichtere Kerne um (Alphastrahlung). Dies wiederholt sich, zusammen mit Betastrahlung, so lange, bis ein stabiler Kern erreicht ist; mehrere Zerfallsstufen bilden eine Zerfallsreihe. Auch zu den Protonenzahlen 43 (Technetium) und 61 (Promethium) existiert kein stabiler Kern. Daher kann es insgesamt nur 80 verschiedene stabile chemische Elemente geben, alle weiteren sind radioaktiv. Sie kommen auf der Erde nur dann natürlich vor, wenn sie selber oder eine ihrer Muttersubstanzen eine genügend lange Halbwertzeit haben.

Da der Großteil der Atommasse von den Neutronen und Protonen stammt und diese etwa gleich schwer sind, wird die Gesamtzahl dieser Teilchen in einem Atom als Massenzahl bezeichnet. Die genaue Masse eines Atoms wird oft in der atomaren Masseneinheit u angegeben; ihr Zahlenwert ist dann etwa gleich der Massenzahl. Kleinere Abweichungen entstehen durch den Massendefekt der Atomkerne. Die atomare Masseneinheit ergibt sich aus der Definition der SI-Einheit des Mols in der Art und Weise, dass ein Atom des Kohlenstoffisotops C (im Grundzustand inklusive seiner Hüllenelektronen) eine Masse von exakt 12 u besitzt. Damit beträgt 1 u gleich 1,66053904 · 10 kg. Ein Atom des leichtesten Wasserstoffisotops hat eine Masse von 1,007825 u. Das schwerste stabile Nuklid ist das Bleiisotop Pb mit einer Masse von 207,9766521 u.

Da makroskopische Stoffmengen so viele Atome enthalten, dass die Angabe ihrer Anzahl als natürliche Zahl unhandlich wäre, erhielt die Stoffmenge eine eigene Einheit, das Mol. Ein Mol sind etwa 6,022 · 10 Atome (oder auch Moleküle oder andere Teilchen; die betrachtete Teilchenart muss immer mitgenannt werden). Die Masse von 1 Mol Atomen der Atommasse "X" u ist daher exakt "X" g. Daher ist es in der Chemie üblich, Atommassen statt in u auch indirekt in g/mol anzugeben.

In welcher Art ein instabiler Atomkern zerfällt, ist für das jeweilige Radionuklid typisch. Bei manchen Nukliden können die (untereinander völlig gleichen) Kerne auch auf verschiedene Arten zerfallen, so dass mehrere Zerfallskanäle mit bestimmten Anteilen beteiligt sind. Die wichtigsten radioaktiven Zerfälle sind


Die Energien der Strahlungen sind für das jeweilige Nuklid charakteristisch, ebenso wie die Halbwertszeit, die angibt, wie lange es dauert, bis die Hälfte einer Probe des Nuklids zerfallen ist.

Durch Anlagerung eines Neutrons kann sich ein Kern in das nächstschwerere Isotop desselben Elements verwandeln. Durch den Beschuss mit Neutronen oder anderen Atomkernen kann ein großer Atomkern in mehrere kleinere Kerne gespalten werden. Einige schwere Nuklide können sich auch ohne äußere Einwirkung spontan spalten.

Größere Atomkerne können aus kleineren Kernen gebildet werden. Dieser Vorgang wird Kernfusion genannt. Für eine Fusion müssen sich Atomkerne sehr nahe kommen. Diesem Annähern steht die elektrostatische Abstoßung beider Kerne, der sogenannte Coulombwall, entgegen. Aus diesem Grund ist eine Kernfusion (außer in bestimmten Experimenten) nur unter sehr hohen Temperaturen von mehreren Millionen Grad und hohen Drücken, wie sie im Inneren von Sternen herrschen, möglich. Die Kernfusion ist bei Nukliden bis zum Nickel-62 eine exotherme Reaktion, so dass sie im Großen selbsterhaltend ablaufen kann. Sie ist die Energiequelle der Sterne. Bei Atomkernen jenseits des Nickels nimmt die Bindungsenergie pro Nukleon ab; die Fusion schwererer Atomkerne ist daher endotherm und damit kein selbsterhaltender Prozess. Die Kernfusion in Sternen kommt daher zum Erliegen, wenn die leichten Atomkerne aufgebraucht sind.

Die Atomhülle besteht aus Elektronen, die aufgrund ihrer negativen Ladung an den positiven Atomkern gebunden sind. Sie wird oft auch als Elektronenhülle bezeichnet. Bei einem neutralen Atom beträgt die durchschnittliche Bindungsenergie der formula_14 Elektronen der Hülle etwa formula_15. Sie nimmt daher mit steigender Teilchenzahl erheblich zu, im Gegensatz zur durchschnittlichen Bindungsenergie pro Nukleon im Kern. Zur Erklärung wird angeführt, dass zwischen Nukleonen nur Bindungskräfte kurzer Reichweite wirken, die kaum über die benachbarten Teilchen hinausreichen, während die Hülle durch die elektrostatische Anziehungskraft gebunden ist, die als langreichweitige Wechselwirkung mit größerem Abstand vom Kern vergleichsweise schwach abnimmt.

Abgesehen von der Masse, die zu über 99,9 % im Atomkern konzentriert ist, ist die Atomhülle für praktisch alle äußeren Eigenschaften des Atoms verantwortlich. Der Begriff Atommodell bezieht sich daher im engeren Sinn meist nur auf die Hülle (siehe Liste der Atommodelle). Ein einfaches Atommodell ist das Schalenmodell, nach dem die Elektronen sich in bestimmten Schalen um den Kern anordnen, in denen jeweils für eine bestimmte Anzahl Elektronen Platz ist. Allerdings haben diese Schalen weder einen bestimmten Radius noch eine bestimmte Dicke, sondern überlappen und durchdringen einander teilweise.

Wesentliche Eigenschaften der Hülle sind oben unter Quantenmechanische Atommodelle und Erklärung grundlegender Atomeigenschaften dargestellt. In den nachfolgenden Abschnitten folgen weitere Details.

Die Atomhülle bestimmt die Stärke und Abstandsabhängigkeit der Kräfte zwischen zwei Atomen. Im Abstandsbereich mehrerer Atomdurchmesser polarisieren sich die gesamten Atomhüllen wechselseitig, sodass durch elektrostatische Anziehung anziehende Kräfte, die Van-der-Waals-Kräfte, entstehen. Sie bewirken vor allem die Kondensation der Gase zu Flüssigkeiten, also einen Wechsel der Aggregatzustände.

Die (näherungsweise) Inkompressibilität der Flüssigkeiten und Festkörper hingegen beruht darauf, dass alle Atome bei starker Annäherung einander stark abstoßen, sobald sich ihre Hüllen im Raum merklich überschneiden und daher verformen müssen. Außer im Fall zweier Wasserstoff­atome, die jeweils nur ein Elektron in der Hülle haben, spielt die elektrostatische Abstoßung der beiden Atomkerne dabei nur eine geringe Rolle.

In einem mittleren Abstandsbereich zwischen dem Vorherrschen der schwach anziehenden Van-der-Waals-Kräfte und der starken Abstoßung kommt es zwischen zwei oder mehr zueinander passenden Atomhüllen zu einer besonders starken Anziehung, der chemischen Bindung. Bei Atomen bestimmter Elemente kann diese Anziehung zu einem stabilen Molekül führen, das aus Atomen in zahlenmäßig genau festgelegter Beteiligung und räumlicher Anordnung aufgebaut ist. Die Moleküle sind die kleinsten Stoffeinheiten der chemischen Verbindungen, also der homogenen Materialien in all ihrer Vielfalt. Vermittelt über die Hüllen ihrer Atome ziehen auch Moleküle einander an. Ein fester Körper entsteht, wenn viele Moleküle sich aneinander binden und dabei, weil es energetisch günstig ist, eine feste Anordnung einhalten. Ist diese Anordnung regelmäßig, bildet sich ein Kristallgitter. Infolge dieser Bindung ist der feste Körper nicht nur weitgehend inkompressibel wie eine Flüssigkeit, sondern im Unterschied zu dieser auch auf Zug belastbar und deutlich weniger leicht verformbar. Verbinden sich Atome metallischer Elemente miteinander, ist ihre Anzahl nicht festgelegt und es können sich nach Größe und Gestalt beliebige Körper bilden. Vor allem chemisch reine Metalle zeigen dann meist auch eine große Verformbarkeit. Verbindungen verschiedener Metalle werden Legierung genannt. Die Art der Bindung von Metallatomen erklärt, warum Elektronen sich fast frei durch das Kristallgitter bewegen können, was die große elektrische Leitfähigkeit und Wärmeleitfähigkeit der Metalle verursacht. Zusammengefasst ergeben sich aus der Wechselwirkung der Atomhüllen miteinander die mechanische Stabilität und viele weitere Eigenschaften der makroskopischen Materialien.

Aufgrund des unscharfen Randes der Atomhülle liegt die Größe der Atome nicht eindeutig fest. Die als Atomradien tabellierten Werte sind aus der Bindungslänge gewonnen, das ist der energetisch günstigste Abstand zwischen den Atomkernen in einer chemischen Bindung. Insgesamt zeigt sich mit steigender Ordnungszahl eine in etwa periodische Variation der Atomgröße, die mit der periodischen Variation des chemischen Verhaltens gut übereinstimmt. Im Periodensystem der Elemente gilt allgemein, dass innerhalb einer Periode, also einer Zeile des Systems, eine bestimmte Schale aufgefüllt wird. Von links nach rechts nimmt die Größe der Atome dabei ab, weil die Kernladung anwächst und daher alle Schalen stärker angezogen werden. Wenn eine bestimmte Schale mit den stark gebundenen Elektronen gefüllt ist, gehört das Atom zu den Edelgasen. Mit dem nächsten Elektron beginnt die Besetzung der Schale mit nächstgrößerer Energie, was mit einem größeren Radius verbunden ist. Innerhalb einer Gruppe, also einer Spalte des Periodensystems, nimmt die Größe daher von oben nach unten zu. Dementsprechend ist das kleinste Atom das Heliumatom am Ende der ersten Periode mit einem Radius von 32 pm, während eines der größten Atome das Caesium­atom ist, das erste Atom der 5. Periode. Es hat einen Radius von 225 pm.

Die dem Schalenmodell zugrundeliegenden Elektronenschalen ergeben sich durch die Quantisierung der Elektronenenergien im Kraftfeld des Atomkerns nach den Regeln der Quantenmechanik. Um den Kern herum bilden sich verschiedene Atomorbitale, das sind unscharf begrenzte Wahrscheinlichkeitsverteilungen für "mögliche" räumliche Zustände der Elektronen. Jedes Orbital kann aufgrund des Pauli-Prinzips mit maximal zwei Elektronen besetzt werden, dem Elektronenpaar. Die Orbitale, die unter Vernachlässigung der gegenseitigen Abstoßung der Elektronen und der Feinstruktur theoretisch die gleiche Energie hätten, bilden eine Schale. Die Schalen werden mit der Hauptquantenzahl durchnummeriert oder fortlaufend mit den Buchstaben "K, L, M,"… bezeichnet. Genauere Messungen zeigen, dass ab der zweiten Schale nicht alle Elektronen einer Schale die gleiche Energie besitzen. Falls erforderlich, wird durch die Nebenquantenzahl oder Drehimpulsquantenzahl eine bestimmte Unterschale identifiziert.

Sind die Orbitale, angefangen vom energetisch niedrigsten, so weit mit Elektronen besetzt, dass die gesamte Elektronenzahl gleich der Protonenzahl des Kerns ist, ist das Atom neutral und befindet sich im Grundzustand. Werden in einem Atom ein oder mehrere Elektronen in energetisch höherliegende Orbitale versetzt, ist das Atom in einem angeregten Zustand. Die Energien der angeregten Zustände haben für jedes Atom wohlbestimmte Werte, die sein Termschema bilden. Ein angeregtes Atom kann seine Überschussenergie abgeben durch Stöße mit anderen Atomen, durch Emission eines der Elektronen (Auger-Effekt) oder durch Emission eines Photons, also durch Erzeugung von Licht oder Röntgenstrahlung. Bei sehr hoher Temperatur oder in Gasentladungen können die Atome durch Stöße Elektronen verlieren (siehe Ionisationsenergie), es entsteht ein Plasma, so z. B. in einer heißen Flamme oder in einem Stern.
Da die Energien der Quanten der emittierten Strahlung je nach Atom bzw. Molekül und den beteiligten Zuständen verschieden sind, lässt sich durch Spektroskopie dieser Strahlung die Quelle im Allgemeinen eindeutig identifizieren. Beispielsweise zeigen die einzelnen Atome ihr elementspezifisches optisches Linienspektrum. Bekannt ist etwa die Natrium-D-Linie, eine Doppellinie im gelben Spektralbereich bei 588,99 nm und 589,59 nm, die auch in nebenstehender Abbildung mit D-1 bezeichnet wird. Ihr Aufleuchten zeigt die Anwesenheit von angeregten Natrium-Atomen an, sei es auf der Sonne oder über der Herdflamme bei Anwesenheit von Natrium oder seinen Salzen. Da diese Strahlung einem Atom auch durch Absorption dieselbe Energie zuführen kann, lassen sich die Spektrallinien der Elemente sowohl in Absorptions- als auch in Emissionsspektren beobachten. Diese Spektrallinien lassen sich auch verwenden, um Frequenzen sehr präzise zu vermessen, beispielsweise für Atomuhren.

Obwohl Elektronen sich untereinander elektrostatisch abstoßen, können zusätzlich bis zu zwei weitere Elektronen gebunden werden, wenn es bei der höchsten vorkommenden Elektronenenergie noch Orbitale mit weiteren freien Plätzen gibt (siehe Elektronenaffinität).
Chemische Reaktionen, d. h. die Verbindung mehrerer Atome zu einem Molekül oder sehr vieler Atome zu einem Festkörper, werden dadurch erklärt, dass ein oder zwei Elektronen aus einem der äußeren Orbitale eines Atoms (Valenzelektronen) unter Energiegewinn auf einen freien Platz in einem Orbital eines benachbarten Atoms ganz hinüberwechseln (Ionenbindung) oder sich mit einer gewissen Wahrscheinlichkeit dort aufhalten (kovalente Bindung durch ein bindendes Elektronenpaar). Dabei bestimmt die Elektronegativität der Elemente, bei welchem Atom sich die Elektronen wahrscheinlicher aufhalten. In der Regel werden chemische Bindungen so gebildet, dass die Atome die Elektronenkonfiguration eines Edelgases erhalten (Edelgasregel). Für das chemische Verhalten des Atoms sind also Form und Besetzung seiner Orbitale entscheidend. Da diese allein von der Protonenzahl bestimmt werden, zeigen alle Atome mit gleicher Protonenzahl, also die Isotope eines Elements, nahezu das gleiche chemische Verhalten.

Nähern sich zwei Atome über die chemische Bindung hinaus noch stärker an, müssen die Elektronen eines Atoms wegen des Pauli-Prinzips auf freie, aber energetisch ungünstige Orbitale des anderen Atoms ausweichen, was einen erhöhten Energiebedarf und damit eine abstoßende Kraft nach sich zieht.

Mit großer Genauigkeit wird die Wechselwirkung zwischen Kern und Hülle schon durch den einfachen Ansatz beschrieben, in dem der Kern eine punktförmige Quelle eines elektrostatischen Felds nach dem Coulomb-Gesetz darstellt. Alle genannten Atommodelle beruhen hierauf. Aufgrund zusätzlicher Effekte, die in erweiterten Modellen behandelt werden, sind nur extrem kleine Korrekturen nötig, die unter dem Namen Hyperfeinstruktur zusammengefasst werden. Zu berücksichtigen sind hier drei Effekte: erstens die endliche Ausdehnung, die jeder Kern besitzt, zweitens eine magnetische Dipolwechselwirkung, wenn sowohl Kern als auch Hülle eine Drehimpulsquantenzahl von mindestens ½ haben, und drittens eine elektrische Quadrupolwechselwirkung, wenn beide Drehimpulsquantenzahlen mindestens 1 sind.

Die endliche Ausdehnung des Kerns – verglichen mit einer theoretischen Punktladung – bewirkt eine schwächere Anziehung derjenigen Elektronen, deren Aufenthaltswahrscheinlichkeit bis in den Kern hineinreicht. Betroffen sind nur "s"-Orbitale (Bahndrehimpuls Null). Bei Atomen mittlerer Ordnungszahl liegt die Korrektur in der Größenordnung von 1 %. Die magnetischen Dipol- bzw. elektrischen Quadrupol-Momente von Hülle und Kern bewirken eine Kopplung mit der Folge, dass die Gesamtenergie eines freien Atoms je nach Quantenzahl seines Gesamtdrehimpulses äußerst geringfügig aufgespalten ist. Im H-Atom beträgt die Aufspaltung etwa ein Millionstel der Bindungsenergie des Elektrons (siehe 21-cm-Linie). Anschaulich gesprochen hängt die Energie davon ab, in welchem Winkel die Achsen des magnetischen Dipolmoments bzw. elektrischen Quadrupolmoments von Kern und Hülle zueinander stehen.

Auch bei Atomen in Flüssigkeiten und Festkörpern treten diese Wechselwirkungen in entsprechend modifizierter Form auf. Trotz der Kleinheit der dadurch verursachten Effekte haben sie eine große Rolle in der Atom- und Kernforschung gespielt und sind in besonderen Fällen auch bei modernen Anwendungen wichtig.

Indirekte Möglichkeiten, Atome zu erkennen, beruhen auf der Beobachtung der von ihnen ausgehenden Strahlung. So kann aus Atomspektren beispielsweise die Elementzusammensetzung entfernter Sterne bestimmt werden. Die verschiedenen Elemente lassen sich durch charakteristische Spektrallinien identifizieren, die auf Emission oder Absorption durch Atome des entsprechenden Elements in der Sternatmosphäre zurückgehen. Gasentladungslampen, die dasselbe Element enthalten, zeigen diese Linien als Emissionslinien. Auf diese Weise wurde z. B. 1868 Helium im Spektrum der Sonne nachgewiesen – über 10 Jahre bevor es auf der Erde entdeckt wurde.

Ein Atom kann ionisiert werden, indem eines seiner Elektronen entfernt wird. Die elektrische Ladung sorgt dafür, dass die Flugbahn eines Ions von einem Magnetfeld abgelenkt wird. Dabei werden leichte Ionen stärker abgelenkt als schwere. Das Massenspektrometer nutzt dieses Prinzip, um das Masse-zu-Ladung-Verhältnis von Ionen und damit die Atommassen zu bestimmen.

Die Elektronenenergieverlustspektroskopie misst den Energieverlust eines Elektronenstrahls bei der Wechselwirkung mit einer Probe in einem Transmissionselektronenmikroskop.

Eine direkte Abbildung, die einzelne Atome erkennen lässt, wurde erstmals 1951 mit dem Feldionenmikroskop (oder Feldemissionsmikroskop) erzielt. Auf einem kugelförmigen Bildschirm, in dessen Mittelpunkt sich eine extrem feine Nadelspitze befindet, erscheint ein etwa millionenfach vergrößertes Bild. Darin sind die obersten Atome, die die Spitze bilden, nebeneinander als einzelne Lichtpunkte zu erkennen. Dies kann heute auch im Physikunterricht an der Schule vorgeführt werden. Das Bild entsteht in Echtzeit und erlaubt z. B. die Betrachtung der Wärmebewegung einzelner Fremdatome auf der Spitze.

Auch das Rastertunnelmikroskop ist ein Gerät, das einzelne Atome an der Oberfläche eines Körpers sichtbar macht. Es verwendet den Tunneleffekt, der es Teilchen erlaubt, eine Energiebarriere zu passieren, die sie nach klassischer Physik nicht überwinden könnten. Bei diesem Gerät tunneln Elektronen zwischen einer elektrisch leitenden Spitze und der elektrisch leitenden Probe. Bei Seitwärtsbewegungen zur Abrasterung der Probe wird die Höhe der Spitze so nachgeregelt, dass immer derselbe Strom fließt. Die Bewegung der Spitze bildet die Topographie und Elektronenstruktur der Probenoberfläche ab. Da der Tunnelstrom sehr stark vom Abstand abhängt, ist die laterale Auflösung viel feiner als der Radius der Spitze, manchmal atomar.

Eine tomographische Atomsonde erstellt ein dreidimensionales Bild mit einer Auflösung unterhalb eines Nanometers und kann einzelne Atome ihrem chemischen Element zuordnen.




</doc>
<doc id="104" url="https://de.wikipedia.org/wiki?curid=104" title="Arzt">
Arzt

Ein Arzt bzw. eine Ärztin ist ein medizinisch ausgebildeter Heilkundiger. Der Arztberuf beschäftigt sich mit der Vorbeugung (Prävention), Erkennung (Diagnose), Behandlung (Therapie) sowie Nachsorge von Krankheiten, Leiden oder gesundheitlichen Beeinträchtigungen und umfasst auch ausbildende Tätigkeiten.

Der Arzt stellt sich damit in den Dienst der Gesundheit und ist bei seinem Handeln moralischen und ethischen Grundsätzen verpflichtet (vergleiche Genfer Deklaration des Weltärztebundes). Die Vielfalt an Krankheiten und Behandlungsmöglichkeiten hat in der Humanmedizin (und Tierheilkunde) mit der Zeit zu einer großen Anzahl von Fachgebieten und weiteren Differenzierungen geführt (→ Liste medizinischer Fachgebiete).

Die Bezeichnung "Arzt" (, ) zog während des Mittelalters aus der lateinischen Gelehrtensprache ins Deutsche ein, und zwar über die latinisierte Variante (spätlateinisch auch ) des griechischen , klassische Aussprache [], ‚Oberarzt‘, ‚Leibarzt‘ (seit dem 2. Jahrhundert die Amtsbezeichnung von Leibärzten bei Hofe und von öffentlich bestallten Gemeindeärzten), einer Zusammensetzung aus , ‚Kommando‘ und . In vielen fachsprachlichen Komposita tritt das ursprüngliche griechische Wort bzw. die latinisierte Form als Wortbestandteil auf: "iatrogen" „durch ärztliches Handeln verursacht“; "Psychiater" „Seelenarzt“ usw. Über mittelhochdeutsche Vermittlung gelangte das Wort in andere Sprachen, so , .

Die germanische Bezeichnung für den Heilberuf () ist beispielsweise im dänischen , im schwedischen , im englischen (vgl. Bald’s Leechbook), oder im deutschen Familiennamen "Lachmann" erhalten und hat sich in andere Sprachen verbreitet, z. B. , . Im polnischen und tschechischen ist die germanische Wurzel mit einem slawischen Suffix ("-arz", "-ař") verbunden.

Die lateinische Bezeichnung (ursprünglich als allgemeine, vom Ausbildungsstand unabhängige, Berufszeichnung; seit dem 10. Jahrhundert dann vom bzw. , dem Wundarzt, unterschieden), oder eine davon abgeleitete Form findet sich vor allem in den romanischen Sprachen, etwa , , , , , aber unter romanischem Einfluss auch in anderen Sprachen: , . Die Bezeichnung meinte meist einen akademisch ausgebildeten Arzt.

In vielen Sprachen wird der Arzt umgangssprachlich nach seinem zumeist geführten akademischen Grad "Doktor" genannt. Gelegentlich ebenfalls als Arzt wurden vor allem volksmedizinisch arbeitende "Laienärzte" bezeichnet.

Die Funktion des Arztes ist eine der ältesten der Menschheit. Medizingeschichtlich gesehen entstand der Arztberuf aus dem Stand der Heilkundigen, die schon unter den Priestern des Altertums zu finden waren. Erste schriftliche Belege des Arztberufs stammen aus Mesopotamien und wurden im 3. Jahrtausend v. Chr. verfasst.

Die Ausbildung von Ärzten der Antike fand in sogenannten "Ärzteschulen" (z. B. Schule von Kos, Schule von Knidos, Alexandrinische Schule) statt, die sich hinsichtlich ihrer Wissenvermittlung an unterschiedlichen ärztlichen Theorien (z. B. Methodiker, Pneumatiker, Hippokratiker) und philosophischen Strömungen (z. B. Epikureer, Stoiker) ausrichteten.

Die moderne Ausbildung von Ärzten begann im 18. Jahrhundert mit der Erweiterung des naturwissenschaftlichen Wissens und der Einführung von systematischem praktischem Unterricht am Krankenbett.

Eine einheitliche Prüfungsordnung (siehe auch Approbationsordnung) für Ärzte gab es in Deutschland erstmals 1883.

2014 war der Anteil der Ärztinnen an der Gesamtzahl der berufstätigen Ärzte bereits auf 45,5 Prozent gestiegen, wenngleich der Anteil der Frauen 2015 zu Beginn des Studiums bei fast zwei Dritteln lag.

Während die körperliche Gesundheit von männlichen Ärzten mit derjenigen der allgemeinen männlichen Bevölkerung vergleichbar zu sein scheint, scheint die körperliche Gesundheit von Ärztinnen besser zu sein als die der allgemeinen weiblichen Bevölkerung.

Hinsichtlich der psychischen Gesundheit fällt auf, dass Depressionen und Suchterkrankungen bei Ärzten häufiger vorkommen als in der restlichen Bevölkerung. Ein weiteres bei Medizinern häufig auftretendes Krankheitsbild ist das Burnout-Syndrom, das bereits bei Medizinstudenten in einer erhöhten Rate nachgewiesen werden kann.

Mehrere Studien zeigten eine gegenüber der allgemeinen Bevölkerung erhöhte Suizidrate unter Ärzten. Das gegenüber der Normalbevölkerung erhöhte relative Risiko, einen Suizid zu begehen, lag für Ärzte bei 1,1–3,4 und für Ärztinnen bei 2,5–3,7. Da in den Studien meist nur eine kleine Zahl von Suiziden untersucht wurde, waren die Vertrauensbereiche des wahren Wertes der Risikoerhöhung weit. Es wird vermutet, dass eine beträchtliche Anzahl von Selbstmorden nicht erfasst wird, da diese fälschlicherweise als Vergiftungen oder Unfälle deklariert werden. Von den verschiedenen beruflichen Spezialisierungen sind insbesondere Psychiater, Anästhesisten und Allgemeinmediziner von einer erhöhten Suizidrate betroffen. Als Ursachen des erhöhten Suizidrisikos werden verschiedene Faktoren diskutiert. Ein Persönlichkeitsprofil mit zwanghaften Zügen kann infolge der beruflichen Anforderungen zu einer depressiven Störung führen. Die Schwierigkeiten, Familie und Karrierewunsch miteinander zu vereinbaren, können insbesondere bei Ärztinnen zu Erschöpfung und Depression führen. Suchterkrankungen (wie beispielsweise Alkohol-, Drogen- und Medikamentenabhängigkeit), die bei Ärzten häufiger auftreten, gehen ihrerseits meistens mit Depressionen und einer erhöhten Suizidrate einher. Dieses für Ärzte und Ärztinnen festgestellte Risikoprofil ist berufsunabhängig und trifft für die meisten Suizidenten zu.

Psychische Probleme korrelieren häufig mit Zeitdruck und mangelnder Autonomie am Arbeitsplatz sowie belastenden Patient-Arzt-Beziehungen. Ärzte werden seltener krankgeschrieben und zeigen eine mangelhafte Inanspruchnahme medizinischer Versorgungsleistungen. Häufig behandeln sich Ärzte selbst. Die eigenständige Behandlung eigener psychischer Störungen ist jedoch häufig ineffektiv.

Die heiligen Zwillingsbrüder Cosmas und Damian gelten wegen ihres Arztberufs unter anderem auch als Schutzpatrone der Ärzte. Ein weiterer Schutzpatron ist der heilige Pantaleon, einer der Vierzehn Nothelfer.

Der Arzt gehört in Deutschland (seit 1935) zu den Freien Berufen und ist (seit 1887) ein klassischer Kammerberuf.

Ärzte unterliegen einer staatlichen Überwachung der Zulassung (Approbation in Deutschland, s. u. in anderen EU-Ländern) und unter anderem dem Arztwerberecht, welches weitgehende Einschränkungen in der Publikation und Veröffentlichungen bedeutet. Ärzte haften ihren Patienten zwar in der Regel nicht auf Erfolg ihres Handelns, können ihnen aber unter dem Gesichtspunkt der Arzthaftung zum Schadenersatz verpflichtet sein.

Die freie Ausübung der Heilkunde ist in Deutschland nur approbierten Ärzten erlaubt. Mit festgelegten Einschränkungen dürfen auch Heilpraktiker Kranke behandeln, wobei die klar festgelegten Grenzen einzuhalten sind. Ausnahmsweise werden spezielle Bereiche der Diagnostik und Therapie auch (meist auf Veranlassung von Ärzten) von Angehörigen der Gesundheitsfachberufe durchgeführt.

Ab dem Zeitpunkt der ärztlichen Approbation darf der Arzt die gesetzlich geschützte Bezeichnung „Arzt“ führen und erhält mit ihr die staatliche Erlaubnis zur eigenverantwortlichen und selbstständigen ärztlichen Tätigkeit. Die bundesweit einheitliche Approbationsordnung regelt das zuvor erfolgreich abzuleistende mindestens sechsjährige Medizinstudium bezüglich der Dauer und der Inhalte der Ausbildung in den einzelnen Fächern, sowie der Prüfungen. Das Studium der Medizin umfasst u. a. drei Examina, sowie ein Jahr praktische Tätigkeit (sogenanntes „Praktisches Jahr“). Von Oktober 1988 bis Oktober 2004 war zur Erlangung der Vollapprobation zusätzlich eine 18-monatige, gering bezahlte Tätigkeit als Arzt im Praktikum unter Aufsicht eines approbierten Arztes gesetzlich vorgeschrieben. Meist arbeitet ein approbierter Arzt für mehrere Jahre als Assistenzarzt an von der Landesärztekammer anerkannten Weiterbildungsstätten (wie 1956 Krankenhäuser, 35,6 % waren 2015 in privater Trägerschaft; seltener einzelne Großpraxen), um sich auf einem oder mehreren Spezialgebieten der Medizin anrechenbar weiterzubilden und eventuell nach zusätzlich mindestens vierjähriger Weiterbildungszeit eine Facharztprüfung abzulegen. Die Anforderungen dazu sind in den Weiterbildungsordnungen der Landesärztekammern geregelt. Niedergelassene Ärzte arbeiten in freier Praxis, gegebenenfalls auch mit mehreren Ärzten in einer Berufsausübungsgemeinschaft (früher: Gemeinschaftspraxis) oder Praxisgemeinschaft (s. a. Vertragsarztrechtsänderungsgesetz). Honorarärzte arbeiten auf Honorarbasis für verschiedene Kliniken oder niedergelassene Ärzte.

Jeder Arzt ist meldepflichtiges Pflichtmitglied der Ärztekammer (des Bundeslandes), in deren Gebiet er wohnt bzw. seine ärztliche Tätigkeit ausübt. Im Jahr 2012 waren in Deutschland bei den Landesärztekammern 459.021 Ärzte gemeldet. Zur Behandlung von Versicherten der gesetzlichen Krankenversicherungen benötigt der Arzt eine Zulassung (Facharzt in eigener Praxis) oder Ermächtigung (als Arzt in einem Krankenhaus oder ähnlicher Institution) und ist dann auch Pflichtmitglied der Kassenärztlichen Vereinigung seines Niederlassungsbezirks. Die kassenärztliche Zulassung besitzen 135.388 Ärzte (Ende 2008): selbstständige 58.095 Hausärzte und 77.293 Fachärzte. In den Kliniken sind 146.300 Ärzte angestellt. Ende 2013 arbeiteten 35.893 ausländische Ärzte in Deutschland, öfter im Osten. 2013 betrug die Zahl der berufstätigen Ärzte in Deutschland 357.252.

Strafrechtlich sind ärztliche Eingriffe der Körperverletzung gleichgesetzt. Diese ist nicht strafbar, wenn die Einwilligung der behandelten Person nach einer Aufklärung vorliegt und die Handlung auf dem Stand des aktuellen medizinischen Wissens vorgenommen wird (§§ 223 ff. StGB). Ausnahmen bestehen, wenn der Patient aufgrund seines Zustandes (z. B. Bewusstlosigkeit) nicht in der Lage ist, seine Entscheidung mitzuteilen, und durch die Unterlassung des Eingriffs die Gefahr von negativen gesundheitlichen Folgen oder sogar dem Tod des Patienten besteht. Zudem können eingeschränkt- oder nichteinwilligungsfähige Personen, wie z. B. Kinder oder in bestimmten Fällen seelisch Erkrankte, auch gegen ihren Willen behandelt werden. Hierfür existieren strenge rechtliche Regelungen und Verfahrenswege, bei welchen neben dem Arzt auch andere Institutionen, z. B. Amtsgericht oder gesetzlicher Betreuer, an der Entscheidung mitwirken.

Vor Inkrafttreten des Gesetzes zur Bekämpfung von Korruption im Gesundheitswesen haben niedergelassene, für die vertragsärztliche Versorgung zugelassene Ärzte die Tatbestandsmerkmale des StGB nicht erfüllt, da diese laut Beschluss des Bundesgerichtshofs (BGH) vom 29. März 2012 weder als Amtsträger i. S. d. § 11 I Nr. 2c StGB noch als Beauftragte der gesetzlichen Krankenkassen i. S. d. § 299 StGB handelten. Die Gesetzeslücke wurde ab 4. Juni 2016 geschlossen, indem StGB (Bestechlichkeit im Gesundheitswesen) und StGB (Bestechung im Gesundheitswesen) hinzugefügt, sowie § 300 und § 302 StGB geändert wurden.

Die Verordnung von rezeptpflichtigen Arzneimitteln und die meisten invasiven Maßnahmen sind in Deutschland ausnahmslos dem approbierten Arzt vorbehalten. Hierbei ist er persönlich zur Einhaltung des anerkannten wissenschaftlichen Standes und medizinethischer Vorgaben verpflichtet. Die Genfer Deklaration orientierte sich 1948 am Eid des Hippokrates. Weiter unterliegen Ärzte speziellen Regelungen, wie dem Berufs- und Standesrecht, welches auch an die Genfer Konvention anknüpft. Insbesondere ist auch im Strafrecht die Einhaltung der ärztlichen Schweigepflicht nach § 203 StGB festgehalten.

In Deutschland gibt es aus historischen Gründen unterschiedliche medizinische akademische Grade. Diese weisen im Gegensatz zum Facharzttitel nicht auf eine besondere Fachkompetenz hin, sondern dienen als Beleg einer wissenschaftlichen Leistung in einem medizinischen Bereich:


Laut einer Studie des Instituts für Qualität und Wirtschaftlichkeit im Gesundheitswesen haben deutsche Ärzte trotz längerer Arbeitszeiten je Patient die kürzeste Sprechzeit in Europa. Sie liegt 30 % niedriger als der europäische Durchschnitt.

Klinikärzte verbringen rund 44 % ihrer Zeit für Schreibtätigkeiten und Protokolle (Stand: 2014/2015). Laut einem Projektbericht des Statistischen Bundesamts vom August 2015 wenden Arzt-, Psychotherapeuten- und Zahnarztpraxen jährlich durchschnittlich 96 Tage Zeit für die Erfüllung von Informationspflichten auf, wobei dieser Wert den gesamten Zeitaufwand aller Praxismitarbeiter darstellt und sämtliche Informationspflichten, auch die der gemeinsamen Selbstverwaltung, umfasst.

Laut der deutschlandweiten MB-Online-Befragung des Marburger Bunds „MB-Monitor“ von 2017 sind 66 % der Krankenhausärzte der Auffassung, dass ihnen nicht ausreichend Zeit für die Behandlung ihrer Patienten zur Verfügung steht.

Die Einkommen von Ärzten in Deutschland variieren, da das Spektrum medizinischer Tätigkeiten breit gefächert ist. Auch finden sich unter Ärzten Unterschiede bei der Arbeitszeit, insbesondere zwischen klinisch tätigen (bspw. 24 Stdn.-Schichten sowie eine hohe Anzahl an Überstunden) und niedergelassenen (hoher Anteil „nicht-medizinischer“-Tätigkeit aufgrund der Selbständigkeit).

Nach dem Bericht des Statistischen Bundesamtes, über die Kostenstruktur bei Arztpraxen aus dem Jahr 2017, lag der durchschnittliche Reinertrag (Gesamteinnahmen abzüglich der Kosten) je niedergelassenem Praxisinhaber im Jahr 2015 bei 192.000 Euro brutto.

Der Reinertrag je Praxisinhaber (nach Fachrichtung) kann aus der folgenden Tabelle abgelesen werden. 

Um einem Mangel an Landärzten entgegenzuwirken, wollte die Bundesregierung 2011 in einem neuen „Versorgungsgesetz“ das Einkommen von Landärzten erhöhen. Unter einer Vielzahl von Gesetzen war das GKV-Versorgungsstrukturgesetz 2012 und Juni 2015 das Gesetz zur Stärkung der Versorgung in der gesetzlichen Krankenversicherung.

Neben den strengen rechtlichen Vorgaben zur Ausübung seines Berufs ist der Arzt auch bei der Außendarstellung bzw. Werbung zu seinen Leistungen und seiner Praxis umfangreichen Verordnungen und Gesetzen unterworfen. Im Unterschied zu anderen Branchen ist Ärzten anpreisende oder vergleichende Werbung absolut verboten. Seit dem 105. Deutschen Ärztetag 2002 sind sachliche, berufsbezogene Informationen über ihre Tätigkeit gestattet. Hauptkriterium ist dabei das schützenswerte Interesse des mündigen Patienten. Umstritten war ab 1998 die Individuelle Gesundheitsleistung eingeführt worden.
Ende 2006 waren in Deutschland ca. 407.000 Ärzte gemeldet, davon 95.700 ohne ärztliche Tätigkeit (siehe Abb.). Die Kassenzulassung besaßen 59.000 Hausärzte und 60.600 Fachärzte. In den Krankenhäusern waren 148.300 Ärzte angestellt.

Im Jahr 2011 wurden in Deutschland rund 342.100 berufstätige Ärzte und rund 107.300 Ärzte ohne ärztliche Tätigkeit gezählt. Auf durchschnittlich 239 Einwohner kam ein berufstätiger Arzt.

Die chronologische Entwicklung kann aus der folgenden Tabelle und der Abbildung abgelesen werden.

In der Gesamtzahl approbierter Ärzte sind auch die nicht (mehr) berufstätigen und die nicht ärztlich tätigen Ärzte enthalten. Die Bundesärztekammer und die Kassenärztliche Bundesvereinigung haben für Deutschland 385.149 Ärztinnen und Ärzte gezählt, die 2017 ärztlich tätig waren, und damit 6.542 Ärzte mehr als im Vorjahr. Der Anteil von Frauen stieg weiter an und erreichte 2017 46,8 %, nach 46,5 % im Vorjahr. Auch der Anteil älterer Ärzte stieg weiterhin an. 2017 waren 18,4 % der Ärzte 60 Jahre oder älter (2016: 17,9 %). Insgesamt waren 2017 172.647 Ärztinnen und Ärzte in der vertragsärztlichen Versorgung, also als Niedergelassene tätig, selbständig oder bei einem Vertragsarzt angestellt.

Arztbesuche: Deutsche Erwachsene (zwischen 18 und 79 Jahren) gehen im Durchschnitt 9,2-mal pro Jahr zum Arzt.

In Österreich ist man mit der Sponsion zunächst "Doktor der gesamten Heilkunde" (Doctor medicinae universae/Dr. med. univ.). Mittlerweile handelt es sich entgegen der Bezeichnung nicht um einen Doktorgrad, sondern um einen Diplomgrad ähnlich dem Magister oder dem Diplomingenieur. Vor dem Wintersemester 2002/03 war das Medizinstudium in Österreich ein Doktoratsstudium, welches auch Übergangsregelungen kannte. Der eigentliche Doktorgrad der Medizin ("Doctor scientae medicinae" bzw. "Dr. scient. med.") kann seitdem im Anschluss an das Diplomstudium in einem dreijährigen Doktoratsstudium erworben werden.

Selbständig als Arzt tätig werden darf man nur, wenn für drei Jahre im Rahmen des „Turnus“ verschiedene (definierte) Disziplinen durchlaufen wurden und die Arbeit vom jeweiligen Abteilungsvorstand positiv bewertet wurde. Danach ist eine weiter abschließende Prüfung abzulegen. Damit hat man das „jus practicandi“ erworben, also die Berechtigung zur selbständigen Berufsausübung als Arzt für Allgemeinmedizin. Alternativ kann sofort nach der Sponsion die (meist sechsjährige) Ausbildung zu einem Facharzt erfolgen, nach der wiederum eine Prüfung abzulegen ist. Viele Fachärzte absolvieren den Turnus vor Beginn der Ausbildung ganz oder teilweise. Es hat sich in Österreich eingebürgert, die Ausbildung zum Allgemeinmediziner zuvor abzuleisten. Viele Krankenhäuser nehmen nur Assistenzärzte mit abgeschlossener Turnusausbildung in Dienst, da diese einen Nacht- oder Wochenenddienst alleine ableisten dürfen.
Ärzte aus anderen EU-Staaten können um Anerkennung als "approbierte Ärzte" ansuchen.

Am 14. Dezember 2010 hat die EU-Kommission in ihrem Amtsblatt C377/10 eine Änderungsmitteilung für die , Anhang 5.1.1. veröffentlicht, wonach ab diesem Zeitpunkt sämtliche Absolventen des österreichischen Medizinstudiums bereits mit der Promotion ihr Grunddiplom abgeschlossen haben und somit innerhalb des gesamten EU- und EWR-Raumes sowie der Schweiz und Liechtenstein eine selbständige Tätigkeit bzw. Ausbildung zum Facharzt unter denselben Voraussetzungen wie einheimische Mediziner aufnehmen dürfen. Bis dahin hatten Mediziner aus Österreich erst mit dem Abschließen der Ausbildung zum Allgemeinmediziner bzw. Facharzt ein Anrecht auf automatische Anrechnung ihres Diploms in den übrigen Mitgliedsstaaten.

Der (niedergelassene) Arzt gehört in Österreich zu den Freien Berufen (Berufe von öffentlicher Bedeutung).

In der Schweiz ist man nach dem mit dem Staatsexamen abgeschlossenen sechsjährigen Studium zunächst eidgenössisch diplomierter Arzt und als solcher zur Arbeit als Assistenzarzt in Krankenhäusern und Arztpraxen befugt.

Die Weiterbildung zum zur selbständigen Berufsausübung befugten Facharzt dauert je nach Fach zwischen drei („praktischer Arzt“) und 8 Jahren nach dem Studienabschluss. Für einen Facharzttitel muss zudem eine Facharztprüfung abgelegt werden. Danach darf sich der Arzt „Facharzt für <Fachgebiet> FMH“ nennen. Die Erlaubnis zur Praxiseröffnung ist kantonal geregelt, die Zulassung zur Berufsausübung zulasten der Krankenkassen wird vom Krankenkassenzentralverband Santésuisse erteilt, ist aber nur eine Formalität. Aktuell besteht aber ein Praxiseröffnungs-Stopp, welcher die Berufsausübung zulasten der Krankenkassen einschränkt. Lediglich bei Bedarfsnachweis, z. B. bei einer Praxisübernahme, ist eine Zulassung möglich.

Die jeweilige Fachgesellschaft prüft, ob jeder Facharzt seiner Fortbildungspflicht (je nach Fachgebiet 60–100 Stunden pro Jahr) nachkommt.

Seit dem 1. Januar 2005 gilt für die Assistenzärzte und Oberärzte eine durch das landesweit gültige Arbeitszeitgesetz begründete maximale Wochenarbeitszeit von 50 Stunden. Bis dahin waren Verträge mit der Formulierung „Die Arbeitszeit richtet sich nach den Bedürfnissen des Spitals“ üblich, wodurch Arbeitszeiten oft über 60 und 70 Stunden pro Woche, ohne finanziellen Ausgleich zu leisten waren.

Die Entgelte der Assistenzärzte liegen deswegen auf dem Niveau der Pflegenden im oberen Kader (Pflegedienstleitungen).

Die Leitenden Ärzte und Chefärzte sind finanziell in der Gesamtvergütung deutlich höher gestellt. Sie sind aus dem Arbeitszeitgesetz ausgegliedert.



Nationales:


</doc>
<doc id="105" url="https://de.wikipedia.org/wiki?curid=105" title="Anthropologie">
Anthropologie

Anthropologie (, und "-logie:" Menschenkunde) ist die Wissenschaft vom Menschen. Sie wird im deutschen Sprachraum und in vielen europäischen Ländern vor allem als Naturwissenschaft verstanden. Die "naturwissenschaftliche" oder "Physische Anthropologie" betrachtet den Menschen im Anschluss an die Evolutionstheorie von Charles Darwin als biologisches Wesen.

Dieser naturalistischen Betrachtung des Menschen stehen verschiedene andere Ansätze gegenüber, beispielsweise die philosophische Anthropologie. Hier wird der Mensch nicht nur als Objekt, sondern auch als Subjekt wissenschaftlich untersucht. Dabei geht es unter anderem um qualitative Eigenschaften wie die Personalität, die Entscheidungsfreiheit und die Möglichkeit zur Selbstbestimmung. In der deutschen Wissenschaftspolitik ist die Anthropologie als Kleines Fach eingestuft.

Die Bezeichnung "Anthropologie" geht zurück auf den deutschen Philosophen, Arzt und Theologen Magnus Hundt (1449–1519).
Zu den ersten Dozenten für das Fach gehörte der Anatom und Physiologe Heinrich Palmatius Leveling, der die Anthropologie 1799 an der Ingolstädter Universität als Vorlesung anbot. Ein Lehrstuhl für „Allgemeine Naturgeschichte und Anthropologie“ wurde 1826 in München eingerichtet. Auf den ersten eigenständigen Lehrstuhl Deutschlands für Anthropologie wurde am 1. August 1886 Johannes Ranke berufen, dem 1905 der Schweizer Rudolf Martin (1864–1925) folgte.

Die biologische Anthropologie ist mit ihren Teilgebieten Primatologie, Evolutionstheorie, Paläoanthropologie, Bevölkerungsbiologie, Industrieanthropologie, Genetik, Sportanthropologie, Wachstum (Auxologie), Konstitution und Forensik ein Fachbereich der Humanbiologie. Ihr Ziel ist die Beschreibung, Ursachenanalyse und evolutionsbiologische Interpretation der Verschiedenheit biologischer Merkmale der Hominiden (Familie der Primaten, die fossile und rezente Menschen einschließt). Ihre Methoden sind sowohl beschreibend als auch analytisch.

Institutionen im deutschsprachigen Raum gibt es an Universitäten und an Museen in Tübingen, Kiel, Hamburg, Berlin, Göttingen, Jena, Gießen, Mainz, Ulm, Freiburg im Breisgau, München, Zürich und Wien. Meist ist dort die Bezeichnung nur „Anthropologie“, Zusätze wie „biologisch“ wurden in jüngerer Zeit notwendig, weil der konkurrierende US-amerikanische Begriff der auch hier bekannt ist.

Die forensische Anthropologie ist eine der drei gerichtlichen Wissenschaften vom Menschen, neben der Rechtsmedizin und der forensischen Odontologie.

Gebiete der forensischen Anthropologie:

Die forensische Anthropologie dient mit den Mitteln der Anthropologie bei der Aufklärung von Verbrechen. Forensische Anthropologen haben vor allem mit der Identifikation von Bankräubern, Schnellfahrern etc. zu tun, aber auch häufig mit stark verwesten oder vollständig skelettierten Leichen. Nicht selten sind sie die letzte Hoffnung zur Aufklärung eines Verbrechens. In Deutschland gibt es eine starke institutionelle Dominanz der Rechtsmedizin, aber gerade das verhindert manchmal den Zugang zu der eigenständigen Kompetenz der Anthropologie.

Die Sozialanthropologie gilt als Wissenschaft der kulturellen und sozialen Vielfalt – oder allgemeiner als „Wissenschaft vom Menschen in der Gesellschaft“. Sie analysiert die soziale Organisation des Menschen. Im deutschen Sprachraum war der Begriff „Sozialanthropologie“ eine seit den 1960er Jahren gebrauchte Bezeichnung für die britische oder die französische , wurde dann aber zugunsten der Fachbezeichnung „Ethnosoziologie“ aufgegeben (Fachbereich der Ethnologie). In den letzten Jahren ist jedoch eine Renaissance des Anthropologie-Begriffs zu beobachten, die einer durch Transnationalisierungs- und Globalisierungs­prozesse veränderten Forschungslandschaft Rechnung tragen möchte.

Die Kulturanthropologie ist eine empirisch gestützte Wissenschaft von der Kultur (im Sinne von „menschliche Kultur“). Sie entwickelte sich im 20. Jahrhundert aus der Volkskunde, hat ihren Schwerpunkt im Gegensatz zu dieser aber in interkulturellen, ethnologischen und soziologischen Themen und Modellen. Unter den anthropologischen Fachrichtungen nimmt die Kulturanthropologie eine Mittelposition zwischen den biologisch und den philosophisch orientierten Richtungen ein; sie ist in ihrem Themenspektrum am weitesten gefasst.

Im deutschen Sprachraum hat sich bisher keine genauere Definition des Forschungsgegenstandes durchgesetzt. In den USA dagegen bezeichnet die Ethnologie (Völkerkunde).

Die Rechtsanthropologie bildet eine eigenständige Unterform der Kulturanthropologie. Sie untersucht Inhalt und Funktionsweisen rechtlicher Strukturen des Menschen unterschiedlicher kultureller Traditionen von Stämmen und Völkern (siehe auch Rechtsethnologie). Zudem bezeichnet dieser Begriff eine rechtswissenschaftliche Forschungsrichtung, die sich den naturalen Grundkonstanten von Gesetzgebung und Rechtsprechung verschrieben hat. Dabei beschäftigt sich die Rechtsanthropologie vorwiegend mit dem (westlich-demokratischen) „Menschenbild der Verfassung“, das demgegenüber vom im Willen freien und eigenverantwortlich handelnden Menschen ausgeht. Dafür wählt sie zumeist einen pragmatisch-dualen Ansatz. Der Begriff Kultur, gelegentlich auch der politischere Begriff der Zivilisation, beschreibt dann die sozial-reale Welt, in der der Mensch beide Sichtweisen vereint.

Die philosophische Anthropologie ist die Disziplin der Philosophie, die sich mit dem Wesen des Menschen befasst. Die moderne philosophische Anthropologie ist eine sehr junge philosophische Fachrichtung, die erst im frühen 20. Jahrhundert als Reaktion auf den Verlust von Weltorientierung entstand. Mit Ausnahme von René Descartes, der bereits Mitte des 17. Jahrhunderts in seinen "Meditationen über die erste Philosophie" (1641) gewisse Zweifel am mittelalterlich-christlichen Weltbild hegt und Position zu Verhältnis von Körper und Seele bezieht. Er vermittelt ein neues philosophisches Gedankengut wie: "Das Denken (=Bewusstsein ) ist es; es allein kann von mir nicht abgetrennt werden; ich bin; ich existiere - das ist gewiss [...] Demnach bin ich genau genommen ein denkendes Ding, d. h. Geist bzw. Seele bzw. Verstand [...]"

Historische Anthropologie bezeichnet einerseits die anthropologische Forschung in der Geschichtswissenschaft, andererseits eine transdisziplinäre Forschungsrichtung, die die historische Veränderlichkeit von Grundphänomenen des menschlichen Daseins untersucht. Dabei bezieht sie die Geschichtlichkeit ihrer Blickrichtungen und methodischen Herangehensweisen sowie die Geschichtlichkeit ihres Gegenstandes, also das Erscheinungsbild des Menschen in den unterschiedenen Epochen, aufeinander.

Die theologische Anthropologie als Teilbereich der Systematischen Theologie deutet den Menschen aus christlich-theologischer Sicht. Dabei beschäftigt sie sich besonders mit dem Wesen des Menschen und der Bestimmung des Menschen vor Gott. Im Unterschied dazu untersucht die Religionsethnologie als Fachgebiet der Ethnologie (Völkerkunde) die Religionen bei den weltweit rund 1300 ethnischen Gruppen und indigenen Völkern, in Abgrenzung zur Religionssoziologie vor allem bei (ehemals) schriftlosen Kulturen.

Die Industrieanthropologie als Disziplin der Anthropologie untersucht die Gebrauchstauglichkeit () und Benutzerfreundlichkeit von Arbeitsplätzen, von Bedienelementen sowie von Produkten.

In den Sozialwissenschaften weit verbreitet ist die Vorstellung, dass der Mensch seinem Wesen nach in seinen Antrieben und Bedürfnissen unbestimmt ist, weshalb erst in Vergesellschaftungsprozessen eine Orientierung und Stabilisierung des Verhaltens und Antriebslebens entstehen kann. Dieses Menschenbild bildet die allgemeine anthropologische Voraussetzung für die Analyse von sozialen Prozessen, so etwa bei Karl Marx, Max Weber, George Herbert Mead oder Talcott Parsons.

Darüber hinaus gibt es in den Sozialwissenschaften zwei klassische Menschenbilder, die als analytische und idealtypische Modelle fungieren: der homo oeconomicus der Wirtschaftswissenschaften und der homo sociologicus der Soziologie. Eine „realistische“ Variante des individualistischen "homo oeconomicus" ist das RREEMM-Modell des Menschen, allerdings wird in der sozialwissenschaftlichen Theoriebildung wegen Operationalisierungsproblemen auch weiterhin überwiegend auf die einfacheren Modelle zurückgegriffen.

Das Konzept der reflexiven Anthropologie, das Gesa Lindemann im Anschluss an die historisch-reflexive Anthropologie Helmuth Plessners entwickelt hat, sieht ein neuartiges Verhältnis von Anthropologie und Soziologie vor. Anthropologie bzw. anthropologische Annahmen werden nicht als sozialtheoretisches Fundament begriffen, sondern zum Gegenstand der Beobachtung gemacht. Bei diesem Ansatz geht es um die Bearbeitung der Frage, wie in Gesellschaften der Kreis sozialer Personen begrenzt wird und welche Funktion der Anthropologie in der Moderne zukommt.

In dem verwendeten Schema kann die Psychologie des Menschen nicht gut untergebracht werden, denn die Psychologie vereint geisteswissenschaftliche, biologische, verhaltens- und sozialwissenschaftliche Konzepte und Methoden. Als Wissenschaft vom Erleben und Verhalten des Menschen einschließlich der biologischen bzw. neurowissenschaftlichen Grundlagen ist die Psychologie von vornherein interdisziplinär ausgerichtet. Wegen dieses umfassenden Blicks auf den Menschen kann die empirische Psychologie in ein besonderes Spannungsverhältnis zur Philosophischen Anthropologie geraten, die ebenfalls einen umfassenden theoretischen Ansatz hat, jedoch die empirischen Humanwissenschaften kaum noch zu integrieren vermag. Wichtige Themen der Psychologischen Anthropologie sind u. a. das Menschenbild, die Persönlichkeitstheorien, die Grundlagen von Motiven, Emotionen in der Neurobiologie und Psychophysiologie, die Beiträge der Kognitionswissenschaft, Sozialpsychologie und Kulturpsychologie, alle Bereiche der Angewandten Psychologie und so weiter.

Die pädagogische Anthropologie ist der Teilbereich der Pädagogik, der sich mit dem Ertrag anthropologischer Fragen, den Zugangsweisen und den Ergebnissen innerhalb der Pädagogik befasst. Grob lassen sich hier zwei Richtungen unterscheiden: Die "Realanthropologie" widmet sich der empirischen Betrachtung der Wirklichkeit des Menschen unter dem Fokus, der sich aus der Pädagogik ergibt. Die "Sinnanthropologie" fragt nach dem Sinn und den Zielen menschlichen Handelns, die in den pädagogischen Kontext eingearbeitet werden. Die Sinnanthropologie weist so besondere Bezüge zur Bildungstheorie auf, indem sie aus einem je spezifischen Menschenbild Bildungsansprüche ableitet. Sie weist innerhalb der verschiedenen Anthropologien eine besondere Nähe zur philosophischen und theologischen Anthropologie auf. Die Realanthropologie steht besonders der biologischen, daneben auch der philosophischen Anthropologie nahe.

Die Einteilung setzt sich fort in der Unterscheidung zwischen integrativen und philosophischen Ansätzen. Die "integrativen" Ansätze versuchen vor allem, anthropologische Erkenntnisse verschiedener Teildisziplinen (insbesondere der Biologie, der Soziobiologie und so weiter) für pädagogische Fragen nutzbar zu machen. Vertreter dieses Ansatzes sind unter anderem Heinrich Roth und Annette Scheunpflug. Der "philosophische" Ansatz hat sich in verschiedenen Richtungen ausdifferenziert. So besteht Otto Friedrich Bollnows Ansatz darin, anthropologische Fragen (beispielsweise nach dem Wesen des Menschen und seiner Bestimmung) für pädagogische Zusammenhänge nutzbar zu machen. Ähnlich wie andere Autoren orientierte er sich in seinen Arbeiten aber auch an der Phänomenologie. Er versuchte also nicht, aus der Philosophie (oder etwa der Biologie) ein Menschenbild zu gewinnen und es pädagogisch auszuwerten, sondern widmete sich dem pädagogischen Handeln und darin auftretenden Phänomenen wie Krise oder Begegnung unmittelbar, um sie als Bestimmungsgrößen des Menschen zu reflektieren. Der Mensch kommt bei diesen Untersuchungen im Hinblick auf Erziehung in drei Rollen vor: als Erziehender, als Zögling und als Erzieher.

In der neueren pädagogischen Anthropologie wird zum einen der integrative Ansatz fortgeführt (beispielsweise auch in der Betrachtung neuerer humanmedizinischer Ergebnisse für Pädagogik). Die philosophische Anthropologie wird heute verstärkt als historische pädagogische Anthropologie fortgesetzt, indem reflektiert wird, dass anthropologische Kenntnisse sowohl auf bestimmte Menschen in bestimmten Epochen bezogen als auch aus einer je spezifischen historischen Position heraus gewonnen werden und deshalb keine überzeitlich allgemeine Gültigkeit beanspruchen können.

Kybernetische Anthropologie bezeichnet den Versuch der begrifflichen Kopplung von Anthropologie und Kybernetik mit dem Vorhaben, den Gegensatz zwischen Natur- und Geisteswissenschaften zu überwinden. Die Cyberanthropologie ist ein neueres Fachgebiet der Ethnologie (Völkerkunde) oder Sozialanthropologie und untersucht transnational zusammengesetzte Online-Gemeinschaften unter Berücksichtigung kybernetischer Perspektiven.

Die medizinische Anthropologie beschäftigt sich mit der Wechselwirkung von Kultur und Medizin.

Siehe auch:

Manchmal wird „Anthropologie“ als Oberbegriff für mehrere der oben genannten Einzel- und Humanwissenschaften aufgefasst. Insbesondere in den USA gibt es dementsprechende Bestrebungen, biologische Anthropologie, Kulturanthropologie, Ethnolinguistik und Archäologie unter einem Dach zu vereinen (Interdisziplinarität).

Die "Systematische Anthropologie", ein 1977 veröffentlichtes Werk der deutschen Ethnologen Wolfgang Rudolph und Peter Tschohl, bringt anthropologisch grundlegende Erkenntnisse in einen integrierten Zusammenhang. Mit Hilfe eines eigenen Begriffssystems wird ein gesamtanthropologisches Modell entwickelt, das die Grenzen und Überschneidungen von Disziplinen wie Ethnologie, Biologie, Humangenetik, Psychologie, Soziologie, Philosophie, Geschichte theoretisch auflöst. „Ziel der Untersuchung ist eine wissenschaftliche Theorie, die dasjenige abdeckt, was systematisch sinnvoll zu einem ’Mensch’ genannten Untersuchungsgegenstand gerechnet werden kann, und die damit nicht von einer einzelnen Fachrichtung beherrscht wird.“

Die Untersuchung erschließt ausgehend von allgemeinen Bedingungen der Gesamtwirklichkeit die besonderen Bedingungen des biotischen und humanen Bereichs. Dafür wurde eine global orientierte Auswahl an Studien ausgewertet und die daraus entwickelte interdisziplinäre Systematik theoretisch konsequent ausformuliert. So lautet ein zentrales Untersuchungsergebnis in Kurzform: „Anthropologie ist zu explizieren als Theorie der Klassenexistenz ‚Menschliche Existenz‘ ME. Sie hat damit den vorverständlichen Gegenstandsbereich Mensch als Existenzklasse M aufzufassen und systematisch darzulegen.“ Gegenstand ist die menschliche Existenz als empirisch beschreibbare Tatsache.

Die Theorie transportiert einen damals fortschrittlichen, humanen und weit gefassten Kulturbegriff. Wegen technokratisch anmutender Formulierung wurde sie aber nur in der ethnologisch und soziologisch orientierten Fachwelt rezipiert. Gerüst und Inhalt der Theorie müssten heute aktualisiert werden, bieten jedoch „eine Basis für Einzeluntersuchungen von beliebigen Ausschnitten des Gegenstandsbereichs Mensch“.

Die "Basis-Theorie der Anthropologie" ist Orientierungswissen, das Zusammenhänge zwischen den Disziplinen und Schulen der Humanwissenschaften aufzeigt. Ein Bezugsrahmen ergibt aus den vier Grundfragen der biologischen Forschung (nach Nikolaas Tinbergen): Verursachungen (= Ursache-Wirkungs-Beziehungen bei den Funktionsabläufen), Ontogenese, Anpassungswert, Phylogenese. Diese vier Aspekte sind jeweils auf verschiedenen Bezugsebenen zu berücksichtigen (vgl. Nicolai Hartmann), beispielsweise Zelle, Organ, Individuum, Gruppe:

Dem tabellarischen Orientierungsrahmen aus Grundfragen und Bezugsebenen lassen sich alle anthropologischen Fragestellungen (siehe PDF-Übersichtstabelle, Absatz A), ihre Ergebnisse (siehe Tabelle, Absatz B) und Spezialgebiete zuordnen (siehe Tabelle, Absatz C); er ist Grundlage für eine Strukturierung der Ergebnisse. Mit Hilfe der Basistheorie kann die anthropologische Forschung in Theorie und Empirie vorangetrieben und fundiertes sowie spekulatives Wissen besser auseinandergehalten werden (betrifft z. B. den Schulenstreit in der Psychotherapie).


Allgemein:

Geschichte:

Vergleichende Anthropologie:

Pädagogische Anthropologie:

Spezielle Themen:



</doc>
<doc id="107" url="https://de.wikipedia.org/wiki?curid=107" title="Alexander der Große">
Alexander der Große

Alexander der Große () bzw. "Alexander III. von Makedonien" (* 20. Juli 356 v. Chr. in Pella; † 10. Juni 323 v. Chr. in Babylon) war von 336 v. Chr. bis zu seinem Tod König von Makedonien und Hegemon des Korinthischen Bundes.

Alexander dehnte die Grenzen des Reiches, das sein Vater Philipp II. aus dem vormals eher unbedeutenden Kleinstaat Makedonien sowie mehreren griechischen Poleis errichtet hatte, durch den sogenannten Alexanderzug und die Eroberung des Achämenidenreichs bis an den indischen Subkontinent aus. Nach seinem Einmarsch in Ägypten wurde er dort als Pharao begrüßt. Nicht zuletzt aufgrund seiner großen militärischen Erfolge wurde das Leben Alexanders ein beliebtes Motiv in Literatur und Kunst, während Alexanders Beurteilung in der modernen Forschung, wie auch schon in der Antike, zwiespältig ausfällt.

Mit seinem Regierungsantritt begann das Zeitalter des Hellenismus, in dem sich die griechische Kultur über weite Teile der damals bekannten Welt ausbreitete. Die kulturellen Prägungen durch die Hellenisierung überstanden den politischen Zusammenbruch des Alexanderreichs und seiner Nachfolgestaaten und wirkten noch jahrhundertelang in Rom und Byzanz fort.
Alexander wurde im Jahre 356 v. Chr. als Sohn König Philipps II. von Makedonien und der Königin Olympias geboren. Viele Einzelheiten seiner Biografie, vor allem aus der Kindheit, wurden später legendenhaft ausgeschmückt oder frei erfunden. So berichtet der Geschichtsschreiber Plutarch, dass Alexander ohne Zweifel seinen Stammbaum väterlicherseits auf Herakles und Karanos, den ersten König der Makedonen, zurückverfolgen konnte, wodurch Plutarch zugleich die Abstammung Alexanders vom Göttervater Zeus implizit hervorhebt.
Ebenso berichtet er, dass Olympias und Philipp Träume gehabt hätten, die ihnen der Seher Aristander so deutete, dass ihnen die Geburt eines Löwen bevorstehe. Olympias nahm für sich in Anspruch, in direkter Linie von dem griechischen Heros Achilleus und Aiakos, einem weiteren Sohn des Zeus abzustammen. Nach einer (vermutlich ebenfalls legendären) Erzählung Plutarchs soll Alexander in jungen Jahren sein Pferd Bukephalos, das ihn später bis nach Indien begleitete, gezähmt haben, nachdem es zuvor niemandem gelungen war, es zu bändigen. Alexander erkannte, was den Fehlschlägen der anderen zugrunde lag: Das Pferd schien den eigenen Schatten zu scheuen. Daraufhin habe Philipp zu ihm gesagt:

Abgesehen von den Legenden ist wenig über Alexanders Kindheit bekannt. Makedonien war ein Land, das im Norden des Kulturraums des antiken Griechenlands lag. Es wurde von vielen Griechen als „barbarisch“ (unzivilisiert) angesehen, obwohl das Königsgeschlecht als griechisch anerkannt wurde. In der Antike gab es ohnehin keinen einheitlichen Staat Griechenland, sondern eine durch gemeinsame Kultur, Religion und Sprache verbundene Gemeinschaft der griechischen Klein- und Stadtstaaten. Im frühen 5. Jahrhundert v. Chr. wurden erstmals Makedonen zu den Olympischen Spielen zugelassen, nachdem König Alexander I. eine Abstammung aus dem griechischen Argos in Anspruch genommen hatte. Noch heute birgt die Diskussion um die ethnische Zugehörigkeit politischen Konfliktstoff.

Aus den verfügbaren Quellen ist ersichtlich, dass das Makedonische, von dem nur wenige Wörter überliefert sind, für die Griechen wie eine fremde Sprache klang. Ob das Makedonische ein griechischer Dialekt oder eine mit dem Griechischen verwandte eigenständige Sprache war, ist immer noch umstritten.

Kulturell und gesellschaftlich unterschieden sich die Makedonen recht deutlich von den Griechen: keine städtische Kultur, als Binnenreich kaum Kontakte zum mediterranen Kulturraum, Königtum, was in Griechenland nicht die Regel war. Auf viele Griechen wird die makedonische Gesellschaft archaisch gewirkt haben. Erst im 6. Jahrhundert v. Chr. verstärkte sich der griechische kulturelle Einfluss in der makedonischen Oberschicht.

Alexanders Vater Philipp II. hatte das bisher eher unbedeutende Makedonien, das vor ihm Streitobjekt der Adelsfamilien des Hoch- und des Tieflands gewesen war, zur stärksten Militärmacht der damaligen Zeit gemacht. Er hatte Thessalien und Thrakien erobert und alle griechischen Stadtstaaten mit Ausnahme Spartas in ein Bündnis unter seiner Führung gezwungen (Korinthischer Bund). Schon an diesen Kriegszügen war Alexander beteiligt, etwa in der Schlacht von Chaironeia (338 v. Chr.), in der die griechischen Städte unter Führung Athens unterworfen wurden. Die makedonische Phalanx erwies sich dabei als ein wichtiges Element für den militärischen Erfolg, zentral war jedoch die Rolle der Hetairenreiterei. Alexanders spätere Erfolge gehen zweifellos zu einem bedeutenden Teil auf die Militärreformen seines Vaters zurück. Philipp umgab sich außerdem mit sehr fähigen Offizieren, wie etwa Parmenion, die auch einen großen Anteil an Alexanders späteren Siegen hatten.

Philipp holte den griechischen Philosophen Aristoteles in die makedonische Hauptstadt Pella und beauftragte ihn, Alexander in Philosophie, Kunst und Mathematik zu unterrichten. Der Einfluss des Aristoteles sollte wohl nicht zu hoch veranschlagt werden, doch sicher war Alexander sehr gebildet; seine Abschrift der "Ilias" hütete er wie einen Schatz, und er brachte der griechischen Kultur große Bewunderung entgegen.

Das Verhältnis zwischen Vater und Sohn war keineswegs frei von Konflikten, gerade in Hinsicht auf die Liebschaften des Vaters. Philipp hatte 337 v. Chr. Kleopatra, die Nichte seines Generals Attalos, als Nebenfrau geheiratet. Während eines Banketts soll Attalos Öl ins Feuer gegossen und gesagt haben, er hoffe, dass Philipp nun einen legitimen Erben erhalten würde. Alexander sei daraufhin wutentbrannt aufgefahren und habe Attalos angeschrien:
Alexander warf einen Becher nach Attalos und wollte auf ihn losgehen. Auch Philipp erhob sich und zog sein Schwert, jedoch nicht um Alexander in Schutz zu nehmen, sondern um Attalos zu helfen. Da aber Philipp bereits betrunken war, stolperte er und fiel hin. Alexander soll ihn höhnisch angeblickt haben und sich den versammelten Makedonen zugewandt haben:

Alexander befürchtete nun offenbar, von der Thronfolge ausgeschlossen zu werden. Schließlich floh er mit seiner Mutter über Epeiros nach Illyrien. Nach einem halben Jahr kehrte er nach Pella zurück, doch seine Thronfolge blieb weiterhin unsicher.

Philipp wurde im Sommer 336 v. Chr. in der alten Hauptstadt Aigai (auch bekannt als Vergina) während der Hochzeit seiner Tochter Kleopatra mit dem König Alexander von Epeiros von dem Leibgardisten Pausanias ermordet. Das Motiv des Täters scheint offensichtlich: Pausanias war ein Vertrauter Philipps gewesen und war von Attalos beleidigt worden; dabei fühlte er sich von Philipp ungerecht behandelt. Es gab aber bald darauf Gerüchte, wonach Alexander an der Tat beteiligt gewesen war. Die Mutmaßungen über die Hintergründe des Mordes und über eine Verwicklung von Olympias und Alexander sind weitgehend spekulativ, auch wenn eine Mitwisserschaft nicht ausgeschlossen werden kann.

Im Jahre 336 v. Chr. folgte der zwanzigjährige Alexander seinem Vater auf den Thron. Dass es keinen nennenswerten Widerstand gab, ist offenbar Antipater zu verdanken, der das Heer dazu bewog, Alexander als König anzuerkennen. Schon in den ersten Tagen ließ er Mitglieder des Hofstaats exekutieren, die das Gerücht gestreut hatten, Alexander habe etwas mit der Ermordung seines Vaters zu tun gehabt. Als nächstes wandte er sich seinem Erzfeind Attalos zu, der sich auf der Flucht befand, jedoch von seinem Verwandten (Stiefvater) Parmenion getötet wurde. Sowohl Antipater als auch Parmenion standen deswegen lange in Alexanders besonderer Gunst und profitierten nicht unerheblich davon: Antipater blieb während des Asienfeldzugs als Reichsverweser in Makedonien, während Parmenion sich seine Unterstützung mit großem Einfluss im Heer vergelten ließ.

Noch 336 ließ sich Alexander in Korinth die Gefolgschaft der griechischen Städte versichern. Die Völker in Thrakien und Illyrien versuchten jedoch, die Situation zu nutzen und die makedonische Herrschaft abzuwerfen. Alexander zog im Frühjahr 335 v. Chr. mit 15.000 Mann nach Norden ins heutige Bulgarien und Rumänien, überquerte die Donau und warf die thrakische Revolte nieder. Anschließend verfuhr er ebenso mit den Illyrern (siehe auch: Balkanfeldzug Alexanders des Großen).

Während Alexander im Norden kämpfte, beschlossen die Griechen im Süden, dass dies der Zeitpunkt sei, sich von Makedonien zu befreien. Ihr Wortführer war Demosthenes, der die Griechen davon zu überzeugen versuchte, dass Alexander in Illyrien gefallen und Makedonien herrscherlos sei. Als erste erhoben sich die Einwohner Thebens und vertrieben die makedonischen Besatzungssoldaten aus der Stadt.

Alexander reagierte augenblicklich und marschierte direkt von seinem Illyrienfeldzug südwärts nach Theben. Die Phalanx seines Generals Perdikkas eroberte die Stadt, wo Alexander zur Bestrafung sämtliche Gebäude mit Ausnahme der Tempel und des Wohnhauses des Dichters Pindar zerstören ließ. Sechstausend Einwohner wurden getötet, die übrigen 30.000 wurden in die Sklaverei verkauft. Die Stadt Theben existierte nicht mehr und sollte erst zwanzig Jahre später wieder aufgebaut werden, aber nie mehr zur alten Bedeutung zurückfinden.

Abgeschreckt von Alexanders Strafgericht brachen die anderen Städte Griechenlands ihre Revolte ab und ergaben sich. Von den Korinthern ließ sich Alexander von neuem die Gefolgschaft versichern und verschonte sie daraufhin, da er sie als Verbündete in seinem Persienfeldzug brauchte.

Das Perserreich war zu Alexanders Zeit die größte Territorialmacht der Erde. Die Perserkönige hatten in den zurückliegenden Jahrhunderten Palästina, Mesopotamien, Ägypten und Kleinasien erobert und zwischen 492 und 479 v. Chr. mehrere Versuche unternommen, auch Griechenland zu unterwerfen (siehe Perserkriege). Aus Sicht von Griechen wie Isokrates ebenso wie der älteren Forschung war das Reich aber um 340 v. Chr. geschwächt und hatte seinen Zenit überschritten. In der neueren Forschung wird dies allerdings bestritten; so war den Persern wenige Jahre vor dem Alexanderzug die Rückeroberung des zwischenzeitlich abgefallenen Ägypten gelungen. Ob Persien für die Makedonen eine leichte Beute war, ist daher umstritten.

Als sich Alexander 334 v. Chr. dem Perserreich zuwandte, wurde dies von Dareios III. aus dem Haus der Achämeniden beherrscht. Schon Alexanders Vater Philipp hatte Pläne für einen Angriff auf die Perser geschmiedet, angeblich, um Rache für die Invasion Griechenlands rund 150 Jahre zuvor zu nehmen, wobei es sich dabei eher um Propaganda handelte und machtpolitische Gründe den Ausschlag gegeben haben dürften. Eine Armee unter Parmenion, einem der fähigsten makedonischen Generäle, war bereits über den Hellespont nach Asien gegangen, wurde von den Persern aber zurückgeschlagen. Alexander überschritt den Hellespont im Mai 334 mit einer Armee aus etwa 35.000 Makedonen und Griechen, um in die Kämpfe einzugreifen, während rund 12.000 Makedonen unter Antipatros Makedonien und Griechenland sichern sollten.

In der Schlacht am Granikos (Mai 334 v. Chr.) kam es zur ersten Begegnung mit den persischen Streitkräften unter der Führung eines Kriegsrates der Satrapen. Der für die Perser kämpfende Grieche Memnon von Rhodos führte 20.000 griechische Söldner, doch konnte er sich im Kriegsrat mit einer defensiven Taktik nicht durchsetzen. Alexander errang auch aufgrund einer ungünstigen Aufstellung der Perser einen deutlichen Sieg. Memnon konnte mit einem Teil der Söldner entkommen. Dadurch war die Befreiung der Städte Ioniens möglich geworden, die Alexander als Motiv für seinen Feldzug genannt hatte. Nach dem Sieg ernannte Alexander eigene Statthalter für die bisherigen Satrapien und übernahm damit die politischen und wirtschaftlichen Strukturen der persischen Verwaltung Kleinasiens.

In Lydien zog Alexander kampflos in Sardes ein. Er weihte den örtlichen Tempel dem Zeus und nutzte die Reichtümer der Stadt, um seine Männer zu bezahlen. Dann zog er weiter nach Ephesos. Dort war kurz zuvor Memnon mit den Resten der Söldner vom Granikos hindurchgezogen und hatte Unruhen unter den städtischen Parteien entfacht. Alexander ließ die alten Institutionen wiederherstellen und regelte die Befugnisse des Tempels der Artemis. Nach einer Ruhe- und Planungspause brach der König mit dem Gros des Heeres nach Milet auf, der größten Stadt an der Westküste Kleinasiens. Der dortige Satrap kapitulierte als Einziger nicht, da ihm die Ankunft einer persischen Hilfsflotte von 400 Schiffen versprochen worden war. Da auch Alexander von dieser Flotte gehört hatte, wies er Nikanor, einen Bruder Parmenions, an, mit 160 Schiffen die Einfahrt zur Bucht von Milet zu versperren. Anschließend gelang ihm die Einnahme der Stadt (→ Belagerung von Milet).

Die Perser, die immer noch unter dem Befehl Memnons standen (allerdings hatten Unstimmigkeiten im persischen Oberkommando einen effektiven Widerstand erschwert), sammelten sich nun in Halikarnassos, der Hauptstadt Kariens, und bereiteten die Stadt auf eine Belagerung vor. Die Kämpfe waren für Alexander sehr verlustreich. Zwischenzeitlich handelte er einen Waffenstillstand aus, um die makedonischen Gefallenen zu bergen – etwas, was er nie zuvor getan hatte und nie wieder tun sollte. Als er letztlich die Mauern durchbrach, entkam Memnon mit dem Großteil seiner Soldaten auf Schiffen aus der fallenden Stadt (→ Belagerung von Halikarnassos). Indem Alexander der karischen Satrapentochter Ada die Herrschaft über Halikarnassos versprach, sicherte er sich das Bündnis mit dem Volk Kariens. Manche Quellen sprechen davon, dass Ada Alexander adoptierte. Hier zeigte Alexander erstmals seine Taktik, Großzügigkeit gegenüber besiegten Völkern walten zu lassen, um sie nicht gegen die Makedonen aufzubringen.

Das ursprüngliche Ziel des Persienfeldzugs, die Eroberung der Westküste Kleinasiens, war hiermit erreicht. Dennoch beschloss Alexander, die Expedition fortzusetzen. Entlang der Küsten Lykiens und Pamphyliens traf die makedonisch-griechische Streitmacht auf keinerlei nennenswerten Widerstand. Eine Stadt nach der anderen ergab sich kampflos. Alexander ernannte seinen Freund Nearchos zum Statthalter von Lykien und Pamphylien.

Im Winter 334/333 v. Chr. eroberte Alexander das anatolische Binnenland. Er stieß vom Süden vor, sein General Parmenion von Sardes im Westen. Die beiden Armeen trafen sich in Gordion, der Hauptstadt der persischen Satrapie Phrygien. Hier soll Alexander der Große der Legende nach den Gordischen Knoten mit seinem Schwert durchschlagen haben, über den ein Orakel prophezeit hatte, nur derjenige, der diesen Knoten löse, könne die Herrschaft über Asien erringen. Es gibt aber auch die Version, dass Alexander mit der Breitseite des Schwertes auf die Wagendeichsel schlug, so dass der Druck den Knoten auseinanderriss.

Die Makedonen blieben einige Zeit in Gordion, um Nachschub an Männern und die Einfuhr der Ernte abzuwarten. Während dieser Zeit starb Memnon, der Befehlshaber der persischen Armee, im August 333 v. Chr. an einer Krankheit. Zu seinem Nachfolger wurde Pharnabazos ernannt, und da sich die Perser bereits wieder formierten, brach Alexander erneut auf. In Gordion ließ er seinen General Antigonos als Statthalter Phrygiens zurück und übertrug ihm die Aufgabe, den Norden Anatoliens zu unterwerfen und die Nachschubwege zu sichern.

Durch Kappadokien marschierte Alexanders Heer nach Kilikien. Dort nahm er nach einem kurzen Gefecht die Hauptstadt Tarsos ein, wo er bis zum Oktober blieb.

In Tarsos erfuhr Alexander, dass Dareios III. die Bedrohung endlich ernst genug nahm, um selbst ein Heer aus dem persischen Kernland nach Westen zu führen. Plutarch zufolge war dieses persische Heer 600.000 Mann stark – eine Angabe, die sicherlich maßlos übertrieben ist: Der berühmte Althistoriker Karl Julius Beloch, der den Quellen immer sehr skeptisch gegenüberstand, schätzte die tatsächliche Zahl der Perser auf höchstens 100.000, die Stärke des makedonischen Heeres dagegen auf ca. 25 – 30.000 Mann.

Dareios gelang es, Alexanders Armee im Norden zu umgehen und Issos zu besetzen, wodurch er die Nachschubwege blockierte. Auch ließ Dareios die in Issos zurückgebliebenen Verwundeten töten. In der Schlacht bei Issos trafen die Armeen im Kampf aufeinander, bis Dareios aufgrund der großen Verluste der Perser vom Schlachtfeld floh. Die Makedonen beklagten 450 Tote und 4000 Verwundete. Unbekannt sind die persischen Verluste, sie dürften aber weit höher gewesen sein. Insgesamt hatte die persische Führung während der Schlacht mehrere Fehler begangen, angefangen bei der Aufstellung – man hatte auf die Umgruppierungen Alexanders nicht reagiert. Auch als Symbol kam der Schlacht große Bedeutung zu: Dareios hatte sich seinem Gegner als nicht gewachsen gezeigt.

Zur Sicherung des Lagers der Perser sandte Alexander seinen General Parmenion nach Damaskus. Neben dem reichen Kriegsschatz befanden sich hier auch mehrere Mitglieder der königlichen Familie. Zu den Gefangenen, die in die Hände der Makedonen fielen, gehörten die Mutter des Dareios, seine Frau Stateira, ein fünfjähriger Sohn und zwei Töchter. Alexander behandelte sie mit Respekt. Außerdem wurde Barsine gefangen genommen, die Witwe des Memnon. Es kam zu einer Liebesaffäre zwischen Alexander und Barsine, aus der später ein Sohn hervorgehen sollte, der Herakles genannt wurde.

Schon bald bat Dareios Alexander um den Abschluss eines Freundschaftsvertrags und die Freilassung seiner Familie. Alexander antwortete, Dareios solle zu ihm kommen und Alexander als „König von Asien“ anerkennen, dann würde seine Bitte erfüllt; andernfalls solle er sich auf den Kampf vorbereiten.

Nach der Schlacht gründete Alexander die erste Stadt in Asien, die er nach sich benannte: Alexandretta, das heutige İskenderun. Hier siedelte er die 4000 Verwundeten der Schlacht an.

Der Ausgang der Schlacht überraschte die antike Welt. Die Erwartungen der Herrscher von Karthago, in Italien, Sizilien, von Sparta bis Zypern, die Kalkulationen der Handelsherren im westlichen Mittelmeerraum, in Athen, auf Delos und in Phönizien erfüllten sich nicht: „… statt der erwarteten Siegesnachricht aus Kilikien kam die von der gänzlichen Niederlage des Großkönigs, von der völligen Vernichtung des Perserheeres.“

Auch die Delegationen aus Athen, Sparta und Theben, die im Hauptquartier des Großkönigs in Damaskus den Verlauf der Feldzüge verfolgten, wurden von Alexanders Feldherrn Parmenion gefangen gesetzt.
Alexander selbst widerstand der Versuchung, den Krieg durch einen Marsch nach Babylon rasch zu entscheiden, doch hatte er es nicht einfach, seine Befehlshaber und Gefährten von einer Defensivstrategie zu überzeugen.

Nach wie vor beherrschte die persische Flotte das östliche Mittelmeer – sie verfügte zwar über keine Häfen mehr in Kleinasien, jedoch nach wie vor in Phönizien. Durch die Münzgeldtribute hier waren die finanziellen Mittel der Perser noch wenig eingeschränkt, und auch Ägypten stand ihnen noch als logistische und militärische Basis zur Verfügung.
Die kommenden Winterstürme ließen zwar keine Flottenunternehmungen mehr erwarten und damit auch keine Gefahr einer raschen Erhebung der Griechen gegen Makedonien – insbesondere des Spartanerkönigs Agis IV. –, doch kam es nun auch auf das Verhalten der phönizischen Geschwader an, die einen Großteil der persischen Flotte stellten. Zwar verblieben sie in dieser Jahreszeit noch in der Fremde, doch nahm Alexander an, dass er diese Kontingente durch eine sofortige Besetzung ihrer Heimatstädte zumindest neutralisieren könne.
„Auch die kyprischen Könige glaubten, für ihre Insel fürchten zu müssen, sobald die phönikische Küste in Alexanders Gewalt war.“
Nach einer Besetzung Phöniziens und Ägyptens könne dann ein Feldzug nach Asien von einer gesicherten Basis aus geführt werden, obwohl die Perser natürlich auch Zeit für neue Rüstungen gewannen. Die Versammlung stimmte Alexanders Plan zu.

Die Schlacht von Issos hatte noch keine grundsätzliche Entscheidung gebracht:
Entgegen den Erwartungen wurde das makedonische Heer nicht vernichtet, und Alexander besaß mit der persischen Kriegskasse in Damaskus die Mittel zur Fortführung des Feldzuges. Eine Entscheidung des Krieges war dadurch nicht bewirkt worden.
Eingezogen wurden in Damaskus „2600 Talente in Münzgeld und 500 Pfund Silber“, die „(ausreichten), alle Soldschulden der Armee und Sold für etwa sechs weitere Monate zu bezahlen …“

Während die Städte in der nördlichen Hälfte Phöniziens – Marathos, Byblos, Arados, Tripolis und Sidon – sich dem Makedonen bereitwillig ergaben, war die dominierende Handelsmetropole Tyros allenfalls zu einem Vergleich bereit. Sie baute dabei auf ihre Insellage knapp vor der Küste, auf ihre vor Ort verfügbare eigene Flotte und die Unterstützung ihrer mächtigen Tochterstadt Karthago. Nachdem Alexander der Zutritt zur Stadt verwehrt worden war – sein Prüfstein war das Verlangen nach einem Opfer im Tempel des Stadtgottes Melkart, des tyrischen Herakles –, brach der König die Verhandlungen ab. Er beschloss, Tyros um jeden Preis einzunehmen, denn er plante schon den Vorstoß nach Ägypten und wollte eine feindliche Stadt, die sowohl mit den Persern als auch mit rebellischen Kräften in Griechenland kooperieren würde, nicht unbezwungen in seinem Rücken lassen. Eine von Arrian überlieferte angebliche Rede Alexanders vor seinen Offizieren, in der die strategischen Überlegungen erläutert werden, ist allerdings eine literarische Fiktion, die auf der Kenntnis des späteren Verlaufs des Feldzugs beruht. Vor dem Beginn der Belagerung bot Alexander den Tyrern Schonung an, falls sie kapitulierten. Sie töteten jedoch seine Unterhändler und warfen die Leichen von den Stadtmauern. Damit war der Weg zu einer Einigung endgültig versperrt.

Ohne Flotte blieb nur die Möglichkeit eines Dammbaues durch das zumeist seichte Gewässer, das die vorgelagerte Inselstadt von der Küste trennte, und der Versuch, mit Belagerungsmaschinen Teile der Mauern zu zerstören. Die Finanzierung dieser aufwendigen Methode, die eine entwickelte Technik und die dafür entsprechenden Materialien und Fachkräfte erforderte, konnte Alexander durch die Beute aus dem persischen Hauptquartier in Damaskus bewerkstelligen.

Ein erster Dammbau wurde von den Tyrern erfolgreich bekämpft, es gelang ihnen bei stürmischem Wetter mit einem Brander die zwei Belagerungstürme an der Spitze des Dammes zu entzünden und durch Begleitschiffe mit Geschützen jeden Löschversuch zu vereiteln. Der Sturm riss zudem den vorderen Teil des Dammes weg.

Der Vorfall löste im makedonischen Heer Entmutigung aus, zumal wieder Gesandte des Dareios eintrafen und ein neues Friedensangebot des Großkönigs überbrachten, das Alexander „den Besitz des Landes diesseits des Euphrat“, 10.000 Talente Lösegeld für seine gefangene Gemahlin und die Hand seiner Tochter anbot. In diese Zeit fiel auch die – vermutlich von Kallisthenes übermittelte – Reaktion des Befehlshabers Parmenion: Wäre er Alexander, so würde er akzeptieren. Alexander entgegnete, das würde er auch tun, wenn er Parmenion wäre. Alexander ließ Dareios mitteilen, er, Alexander, werde sich nehmen, was er wolle; wenn Dareios etwas von ihm erbitten wolle, solle er zu ihm kommen.

Der Damm wurde in größerer Breite wiederhergestellt und neue Türme gebaut.
In der Zwischenzeit – nach den Winterstürmen – trafen auch die phönizischen Flottenkontingente und die Geschwader der Könige von Zypern in ihren Heimathäfen ein und standen nun Alexander zur Verfügung; insgesamt 250 Schiffe, darunter auch Vier- und Fünfruderer.

Diese Bundesgenossenschaft lag auch in der Feindschaft der kleineren Städte Phöniziens gegen Tyros begründet: Die Metropole hatte zwanzig Jahre zuvor zwar einen Aufstand unter Führung von Sidon gegen die Perser befürwortet und Hilfe zugesagt, dann jedoch den Verlauf der Auseinandersetzungen abgewartet und war von den Persern für diese Haltung belohnt worden. Nach der Niederschlagung der Erhebung und der Zerstörung von Sidon errang Tyros die Vorherrschaft unter den phönizischen Handelsstädten.

Während die neu gewonnene Flotte ausgerüstet wurde, unternahm Alexander eine Expedition durch das küstennahe Gebirge des Antilibanon, um die Festungen von Gebirgsstämmen zu bezwingen, den Nachschub (Holz für den Maschinenbau) und die Verbindung nach Damaskus zu sichern.

Die Karthager konnten den Tyrern nicht helfen, da sie sich im Krieg mit Syrakus befanden. Nach weiteren wechselvollen Kämpfen um die Stadtmauern und zur See, die die Tyrer immer mehr Schiffe kosteten, war die Zeit zum Sturmangriff reif. Alexander beschloss einen kombinierten Land- und Seeangriff. Auf der durch den Damm erreichbaren Seite gelang es, Breschen in die Mauern zu schlagen und ein Landeunternehmen durchzuführen, die phönizischen Schiffe sprengten die Sperrketten im Südhafen und bohrten die dort liegenden Schiffe in den Grund, die zyprische Flotte verfuhr ebenso im Nordhafen – dort gelang es den Truppen, zusätzlich in die Stadt einzudringen. Die überlieferte Zahl von 8000 Gefallenen der Stadt soll sich auf die gesamte Belagerungszeit beziehen. Ob die anschließende angebliche Kreuzigung von 2000 Kämpfern den Tatsachen entspricht, ist umstritten. Im Vorfeld des letzten Angriffes ließ Alexander Schiffe der Karthager und seiner verbündeten Phönizier zur Evakuierung der Bevölkerung passieren. In Heiligtümer oder Tempel Geflüchtete wurden verschont.

Zahlreiche Einwohner – die überlieferte Zahl von 30.000 gilt allerdings als stark übertrieben – wurden in die Sklaverei verkauft. Das war in der Antike eine gängige Praxis, um die Kriegskassen aufzufüllen. Alexander soll allerdings sehr selten zu diesem Mittel gegriffen haben, da er die Bevölkerung für sich gewinnen wollte, denn er konnte sich eine ständige Bedrohung durch Aufständische in seinem kaum durchgängig besetzbaren Hinterland nicht leisten.

Tyros wurde wieder aufgebaut und neu besiedelt, um unter makedonischer Hoheit die beherrschende Position in Phönizien zu sichern. Die Nachricht von diesem mit modernster Kriegstechnik errungenen Sieg – die Belagerungstürme sollen eine Höhe von 45 Metern erreicht haben – machte in der antiken Welt weit über die betroffene Region hinaus einen starken Eindruck.

Alexander, der während der Belagerung auch die Verwaltung und Logistik in den neu gewonnenen Gebieten ordnete, „brach etwa Anfang September 332 von Tyros auf.“ Die Städte und Stämme im südlichen Syrien ergaben sich bis auf die Hafenstadt Gaza.

Die Stadt war seit Jahrhunderten der Hauptumschlagplatz des Gewürzhandels. Mit einer Eroberung der Stadt konnte Alexander einen der lukrativsten Handelsbereiche zwischen Ost und West unter seine Kontrolle bringen, doch standen den Makedonen damit nicht nur Perser, sondern auch arabische Söldnertruppen gegenüber. Mit entsprechender Härte wurde der Kampf geführt.

Einen unmittelbaren Gewinn konnte sich Alexander von einer Eroberung nicht versprechen, denn die Gewürzhandelsgeschäfte des Jahres waren abgeschlossen, da „die Route nur einmal im Jahr befahren wurde.“ Wetterverhältnisse und „Orientierungsschwächen beschränkten die Aktivitäten mediterraner Seefahrt auf das halbe Jahr zwischen Mai und Oktober, in dem das Wetter in der Regel verläßlich gut war. […] Faktisch lag der Zeitpunkt Mitte August (Hesiod, 700 v. Chr.), denn es stand auch noch die Rückreise an.“ Organisiert war diese Fahrt bis in die Spätantike als riesiges „Kauffahrtgeschwader“ zuerst entlang den östlichen Küsten – vor allem Kornfrachter, Sklaven- und Baumaterial-Transporten sowie Postschiffen und anderen, die dann übers Meer von Kriegsschiffen begleitet wurden. Durch die Belagerung von Tyros waren die Handelsunternehmen 332 v. Chr. schon stark beeinträchtigt worden.

Alexander nahm sofort den Hafen von Gaza zum Antransport der zerlegten Belagerungsmaschinen in Beschlag. Die Stadt selbst lag nahe dem Meer auf einem flachen Hügel. Gaza war auch der letzte freie Ankerplatz für die persische Flotte in Syrien und somit auch an der kompletten östlichen Mittelmeerküste. Die Flotte war mittlerweile in Auflösung begriffen, da die griechischen Kontingente nun ebenfalls – klimabedingt – im Herbst in ihre Heimathäfen zurück segelten.

Mit erneut hohem Aufwand schütteten die Makedonen einen Damm zur Südseite der Stadt auf, der danach mit weiteren, konzentrisch angelegten Dämmen ergänzt wurde. Die Kämpfe – vor allem mit den arabischen Söldnern – wurden als „wild“ bezeichnet, Alexander wurde zweimal verwundet; durch einen Messerstich und – gefährlicher – mit einem Katapultpfeil, der durch den Panzer in die Schulter drang. Nach zwei Monaten und dem vierten Ansturm fiel die Stadt, um die 10.000 Verteidiger sollen umgekommen sein, Frauen und Kinder wurden als Sklaven verkauft.

Dass der Kommandant Batis wie Hektor durch Achilles vor Troja um die Stadt geschleift worden sein soll, wird angezweifelt. „Alexander zog die Bevölkerung der umliegenden philistäischen und arabischen Ortschaften in die Stadt; eine dauernde Besatzung machte sie zu einem Waffenplatz, der für Syrien und für Ägypten gleich wichtig war.“

Es wird davon ausgegangen, dass der Gewürztransport nach Gaza danach in der „Felsenstadt“ Petra – der davor liegenden Station der Weihrauchstrasse – angehalten wurde. Petra war „zentrales Weihrauchlager“, da die Stadt in einem Talkessel gewaltige Lagerhallen (Höhlen) besaß. „In Petra saßen die Ökonomen, die kontrollierten, was sie zu welchem Preis an die mediterranen Küsten bringen wollten.“ Für 332 war das Geschäft allerdings schon gelaufen.

Den jahreszeitlichen Bedingungen zufolge kehrten im Herbst auch die Kauffahrtsflotten zurück und trafen in Phönizien überall in Häfen ein, die von den Makedonen kontrolliert wurden. Die Auflösung der persischen Kriegsflotte im Herbst war ebenfalls eine Routineangelegenheit, doch war es allen Beteiligten klar, dass die Kontingente auf Grund der makedonischen Besetzung sämtlicher Festlandshäfen im östlichen Mittelmeer im nächsten Frühjahr nicht wieder unter persischem Kommando zusammengeführt werden würden.

Während Alexander mit dem Heer 332 v. Chr. den größten Teil des Jahres mit Belagerungen zur Vervollständigung seiner Blockade der persischen Seemacht verbrachte – und dabei die phönizischen Hafenstädte und ihren Handel unter seine Kontrolle nahm –, war die Flotte der Perser durch den bereits im Frühjahr erfolgten Abzug der phönikischen und kyprischen Kontingente geschwächt und verhielt sich defensiv.

Die Admirale Pharnabazos und Autophradates versuchten – meist mit Hilfe begünstigter oder eingesetzter Machthaber – die wichtigsten Inseln unter ihrer Kontrolle zu behalten. In Griechenland, das Alexanders Statthalter Antipater bis auf die Peloponnes fest im Griff hatte, rührte sich kein Widerstand.

Lediglich der Spartanerkönig Agis III. setzte noch auf die persische Karte und hatte Kreta durch seinen Bruder und Mitregenten Agesilaos besetzen lassen.

Doch schon im Vorjahr, noch während des Aufenthalts in Gordion 333 v. Chr. hatte Alexander „Amphoteros, den Bruder des Orestiden Krateros“ beauftragt, „‚in Übereinstimmung mit den Abmachungen des Bündnisses‘ eine neue griechische Flotte auszurüsten.“ Dank „der erbeuteten Schätze aus Sardis“ gelangen die Anfänge dazu und nach dem Sieg bei Issos und dem darauf folgenden Winter, der keine Flottenunternehmungen zuließ, stand Alexanders neue Flotte im Frühjahr 332 bereit.

Nun konnten die makedonischen Nauarchen Hegelochos und Amphoteros ihrerseits systematisch die Inseln besetzen – von Tenedos und Chios (wo der persische Admiral Pharnabazos mit der Besatzung von 15 Trieren in Gefangenschaft geriet) – bis nach Kos und schließlich Lesbos. Dort handelte der athenische Söldnerführer Chares mit zweitausend Mann freien Abzug aus und begab sich nach Tainaron, dem Hafen und Söldnermarkt südlich von Sparta.

Amphoteros unterwarf zuletzt noch die kretischen Stützpunkte, während Hegelochos bereits nach Ägypten steuerte, „um selbst die Meldung vom Ausgang des Kampfes gegen die persische Seemacht zu überbringen, zugleich die Gefangenen abzuliefern […] So war mit dem Ausgang des Jahres 332 der letzte Rest einer persischen Seemacht, die das makedonische Heer im Rücken zu gefährden und dessen Bewegungen zu hindern vermocht hätte, vernichtet.“

Nach der Eroberung von Gaza machte sich Alexander mit einem Teil seines Heeres auf den Weg nach Ägypten.

Ägypten war in den vorangegangenen sieben Jahrzehnten mehrfach von den Persern angegriffen und besetzt worden und ging ihnen regelmäßig durch Aufstände wieder verloren. Erst seit drei Jahren war es wieder in der Hand des Großkönigs, doch „Ägypten war von Truppen entblößt, weil der Satrap Sabakes mit einem großen Aufgebot nach Issos gekommen und selbst dort gefallen war. […] Mazakes, vom Großkönig [..] zum (neuen) Satrapen ernannt, konnte nicht an Widerstand denken.“ Er übergab unter Auslieferung von 800 Talenten für freies Geleit die Grenzfestung Pelusion.

Ein Teil der makedonischen Flotte segelte nun den Nil aufwärts zur Hauptstadt Memphis während sich Alexander mit den Truppen auf dem Landmarsch über Heliopolis dorthin begab. In Memphis opferte Alexander dem ägyptischen Gott Apis anstatt ihn zu verachten, wie der persische Großkönig Artaxerxes III., der den heiligen Stier des Gottes töten ließ. „Als Gegengabe scheint Alexander als Pharao des Oberen und Unteren Ägyptens gekrönt worden zu sein, wenngleich diese Ehrung nur in dem „frei erfundenen“ "Alexander-Roman" erwähnt wird.“ „Die Krönung kann nicht auf einen Monat genau datiert werden, bestätigt wird sie aber durch die Pharaonentitel, die ihm in ägyptischen Tempelinschriften zugeschrieben sind.“ Der Verlag veröffentlichte dazu das Foto eines Reliefs im Amun-Tempel von Luxor.

Alexander zog danach am westlichen Nil entlang nordwärts und gründete im Januar 331 v. Chr. an der Mittelmeerküste Alexandria, die bedeutendste all seiner Stadtgründungen.

Im März zog Alexander von Paraetonium aus 400 km südwestwärts durch die Wüste zum Orakel von Siwa, einem dem Gott Amun geweihten Tempel. Was er dort an Botschaften empfing, ist unbekannt. Antike Quellen berichten, Alexander habe dort erfahren, dass er der Sohn des Zeus sei; so soll ihn der oberste Priester als „Sohn des Zeus“ begrüßt haben. Jedoch hatte Alexander sich schon vorher als Sohn des Zeus bezeichnet.
Von Siwa kehrte Alexander nach Memphis zurück, verweilte dort einige Wochen und führte seine Truppen dann zurück nach Palästina.

Im Mai 331 kehrte Alexander nach Tyros zurück. Er befahl hier den Wiederaufbau der Stadt, die er mit befreundeten Phöniziern wieder besiedeln ließ. 15.000 zusätzliche Soldaten waren im Frühling aus Makedonien entsandt worden, und bei Tyros trafen sie im Juli mit Alexander zusammen. Seine Armee bestand nun aus 40.000 Fußsoldaten und 7000 Reitern.

Alexander zog ostwärts durch Syrien und überquerte den Euphrat. Sein Plan mag gewesen sein, von hier aus südwärts nach Babylon zu ziehen, doch eine Armee unter dem persischen Satrapen Mazaeus verstellte den Weg. Alexander vermied die Schlacht, die ihn viele Männer gekostet hätte, und zog stattdessen nordwärts. Derweil zog Dareios selbst eine neue große Streitmacht in Assyrien zusammen, und dieses Heer war es, das Alexander treffen wollte. Im September 331 v. Chr. überquerte das Heer den Tigris.
Am 20. September, unmittelbar vor der Schlacht, kam es zu einer Mondfinsternis, die die Perser verunsicherte und von ihnen als schlechtes Omen gedeutet wurde. Das Heer Alexanders lagerte 11 km von der persischen Armee entfernt bei einem Dorf namens Gaugamela, weshalb die folgende Schlacht als Schlacht von Gaugamela bekannt wurde. Am 1. Oktober kam es zum Kampf. Wenngleich das Heer des Dareios auch diesmal den Truppen Alexanders zahlenmäßig weit überlegen war, siegte abermals Alexander. Er vermochte aber nicht, Dareios selbst zu töten oder gefangen zu nehmen. Obwohl dieser damit erneut entkommen war, war seine Armee praktisch vernichtet. Alexander dagegen hatte nun die Herrschaft über die Satrapie Babylonien gewonnen und konnte ungehindert ins reiche Babylon einziehen. Mazaeus, der sich nach der Schlacht von Gaugamela nach Babylon zurückgezogen hatte, übergab die Stadt an Alexander, der sie durch das Ischtar-Tor betrat und sich zum „König von Asien“ ausrufen ließ.

Während die Griechen die Völker Asiens zuvor als Barbaren verachtet hatten, sah Alexander sie mit anderen Augen. Fasziniert von der Pracht Babylons befahl er die Schonung aller Bauwerke. Alexander verzieh dem persischen Satrapen Mazaeus und ernannte ihn zu seinem Statthalter in Babylon.

Nach fünfwöchigem Aufenthalt zog Alexander weiter ostwärts, um die großen persischen Städte im Kernland anzugreifen. Susa ergab sich kampflos. Im Januar 330 v. Chr. erreichten die Makedonen die persische Hauptstadt Persepolis. Zahlreiche Einwohner begingen vor seinem Einzug Selbstmord oder flohen. Die ältere Meinung, Alexander habe die Stadt plündern und den Königspalast niederbrennen lassen, ist inzwischen von der jüngeren Quellenkritik relativiert worden. Archäologische Funde bestätigen, dass lediglich die Gebäude, die Xerxes I. errichtet hatte, brannten, was die Darstellung Arrians wahrscheinlicher macht.

Zwar war Persien nun in Alexanders Hand, doch König Dareios III. war noch immer am Leben und auf der Flucht. Da Alexander mitgeteilt worden war, dass Dareios sich in Medien aufhalte, folgte er seiner Spur im Juni nach Nordwesten nach Ekbatana. Doch auch Dareios’ Anhängerschaft hatte jetzt keine Hoffnung mehr, Persien zurückzugewinnen. Die Vollkommenheit der Niederlage ließ nur die Möglichkeit zu, sich zu ergeben oder zeitlebens zusammen mit Dareios zu fliehen. Bisthanes, ein Mitglied der Königsfamilie, entschied sich, in Ekbatana zu bleiben, wo er Alexander empfing und ihm die Stadt übergab. Alexander zeigte sich wiederum großzügig und ernannte einen Perser zu seinem Statthalter in Medien. In Ekbatana entließ Alexander auch die griechischen Verbündeten und die thessalischen Reiter, was als Zeichen zu verstehen war, dass der vom Korinthischen Bund beschlossene „Rachefeldzug“ damit beendet war. Teile des Bundesheeres wurden jedoch von Alexander als Söldner angeworben.

Dareios setzte inzwischen seine Flucht fort. Er hoffte, Zuflucht in Baktrien zu erhalten, wo ein Verwandter namens Bessos Satrap war. Bessos aber setzte Dareios gefangen und schickte einen Unterhändler zu Alexander. Er bot ihm an, Dareios an die Makedonen zu übergeben, wenn im Gegenzug Baktrien frei bliebe. Alexander ging nicht auf die Verhandlungen ein und setzte die Verfolgung fort. Bessos tötete seine Geisel im Juli und floh seinerseits. Die Leiche des Dareios wurde von Alexander nach Persepolis gebracht und dort feierlich beigesetzt.

In der Zwischenzeit hatte Alexander erkannt, dass er zur Sicherung der Herrschaft über das Perserreich die Unterstützung der persischen Adligen brauchte. Er nutzte Dareios’ Ermordung daher, die Perser zu einem Rachezug gegen Bessos aufzurufen, der sich nun den Namen Artaxerxes gegeben hatte und sich Großkönig von Persien nannte. Die Soldaten waren wenig begeistert davon, dass sie den Tod ihres Erzfeindes vergelten und zudem gemeinsam mit Persern kämpfen sollten. Außerdem war ihnen das Land im Nordosten vollkommen unbekannt. Die dortigen Provinzen Baktrien und Sogdien lagen in etwa auf den Territorien der heutigen Staaten Afghanistan, Usbekistan und Turkmenistan.

Im August 330 v. Chr. brach Alexander zu einem neuen Feldzug auf und eroberte zunächst Hyrkanien, die persische Satrapie an der Südküste des Kaspischen Meeres. Unter jenen, die mit Alexander kämpften, war Oxyartes, ein Bruder des Dareios. Statt von Hyrkanien den direkten Weg nach Baktrien zu wählen, ging Alexander über Aria, dessen Satrap Satibarzanes an Dareios’ Gefangennahme beteiligt gewesen war. Alexander eroberte die Hauptstadt Artacoana, verkaufte die Einwohner in die Sklaverei und benannte die Stadt in Alexandreia um; der heutige Name der Stadt ist Herat.

Auf seinem weiteren Weg kam es zu einem Zwischenfall, als Philotas, der Sohn des Parmenion, beschuldigt wurde, einen Anschlag auf Alexanders Leben unternommen zu haben. Ob dieser Versuch wirklich unternommen worden war, ist unklar. Vielleicht diente die Affäre Alexander bloß als Vorwand, sich Parmenions zu entledigen, der zum Wortführer seiner Kritiker avanciert war. Sie missbilligten Alexanders Neigung, die Perser zu ehren und ihre Gewänder zu tragen, und sahen dies als Anbiederung an ein barbarisches Volk an. Philotas wurde an Ort und Stelle mit einem Speer getötet. Ein Kurier wurde dann zu den Adjutanten des in Ekbatana gebliebenen Parmenion gesandt. Sie töteten Parmenion auf Alexanders Befehl.

Nach beschwerlicher Reise entlang des Flusses Tarnak erreichte Alexander im April 329 das Zentrum des heutigen Afghanistan und gründete Alexandria am Hindukusch (heute Chârikâr). Von hier aus wollte Alexander das Gebirge überschreiten und auf diesem Wege in Baktrien einfallen. Einer Legende zufolge fand man hier den Berg, an den der Titan Prometheus gekettet worden war.

Als die Nachricht nach Baktrien gelangte, dass Alexander dabei war, den Hindukusch zu übersteigen, fürchteten die Einwohner von Baktra (heute Balch) die Bestrafung ihrer Stadt und vertrieben Bessos. Die beschwerliche Überquerung des Gebirges hatte die Soldaten indessen gezwungen, manche ihrer Lasttiere zu schlachten. Als sie erschöpft in Baktrien ankamen, wurde das Land ihnen kampflos übergeben. Alexander ernannte seinen persischen Vertrauten Artabazos, den Vater der Barsine, zum Satrapen.

Alexander hielt sich nicht lange in Baktra auf und folgte weiterhin Bessos, der nordwärts zum Oxus (Amudarja) geflohen war. Der 75 km lange Marsch durch wasserlose Wüste wurde vielen zum Verhängnis. Bessos hatte inzwischen alle Schiffe zerstören lassen, mit denen man den Amudarja hätte überqueren können. Die Makedonen brauchten fünf Tage, um genügend Flöße für die Überquerung des Flusses anzufertigen. Dann setzten sie über in die Satrapie Sogdien im heutigen Turkmenistan.

Die Begleiter des Bessos wollten nun nicht länger fliehen. Sie meuterten gegen ihn, nahmen ihn gefangen und händigten ihn an Alexander aus. Dieser zeigte sich gnadenlos und ließ Bessos die Nase und die Ohren abschneiden. Anschließend übergab Alexander den Verstümmelten an Dareios’ Bruder Oxyartes, damit er ihn nach Medien an den Ort brächte, wo Dareios ermordet worden war. Dort wurde Bessos gekreuzigt.

Alexander ging indessen weiter nach Norden und erreichte die sogdische Hauptstadt Marakanda (heute Samarkand). Alle Satrapien des Perserreichs unterstanden nun Alexander, und niemand außer ihm selbst erhob mehr Anspruch auf den Königstitel über Persien.

Nach der Einnahme von Marakanda zog Alexander noch weiter bis zum Syrdarja und gründete dort im Mai 329 v. Chr. die Stadt Alexandria Eschatê („das entfernteste Alexandria“), das heutige Chudschand in Tadschikistan. Etwa gleichzeitig erhob sich die Bevölkerung Sogdiens gegen ihn. Anführer der Rebellion, die Alexander erhebliche Schwierigkeiten bereite, war ein Mann namens Spitamenes, der zuvor Bessos verraten und an Alexander übergeben hatte. Die Sogdier, die Alexander zunächst begrüßt hatten, nun jedoch sahen, dass eine Fremdherrschaft durch eine andere ersetzt wurde, machten die makedonischen Besatzungen nieder. Alexander zog Truppen zusammen und marschierte von einer rebellischen Stadt zur anderen, belagerte sieben von ihnen und tötete anschließend sämtliche männlichen Einwohner, wohl um ein abschreckendes Exempel zu statuieren. In der Zwischenzeit eroberte Spitamenes Marakanda zurück, doch Alexander gewann die Stadt erneut, Spitamenes allerdings entkam. Da das Heer geschwächt und stark reduziert war, musste Alexander von der Verfolgung ablassen. Im Zorn brannte er Dörfer und Felder jener Bauern nieder, die die sogdische Revolte unterstützt hatten. Für den Winter 329/328 v. Chr. zog er sich nach Baktra zurück und erwartete neue Truppen, die bald darauf aus dem Westen eintrafen und bitter benötigt wurden.

Im Frühling 328 v. Chr. kehrte Alexander nach Sogdien zurück. Den Quellen zufolge gründete er am Amudarja ein weiteres Alexandria, das vielleicht mit der heutigen Siedlung Ai Khanoum identisch ist. Der Kampf gegen die sogdischen Rebellen dauerte das ganze Jahr. Erst Monate später zeigte sich, dass die Anhänger des Spitamenes ihren Befehlshaber zu verlassen begannen. Das Haupt des Rebellenführers wurde Alexander schließlich im Dezember 328 überbracht.

Während der Sieg gefeiert wurde, kam es zu einem Streit zwischen Alexander und seinem General Kleitos. Kleitos, der altmakedonisch gesinnt war, sollte demnächst nach Baktrien aufbrechen. Grund war vermutlich sein Alter, aber Kleitos sah dies als Herabsetzung an. Es ist auch möglich, dass Kleitos bei dieser Gelegenheit Kritik an der Proskynese, einem von Alexander übernommenen persischen Hofritual, geübt hat. Die Streitenden waren zu diesem Zeitpunkt betrunken, und Kleitos hatte Alexanders Vater Philipp zu loben begonnen. Hierdurch fühlte sich Alexander so beleidigt, dass es zum Streit kam, in dessen Verlauf Alexander vergeblich nach seinen Waffen suchte, da sie vorsichtshalber von einem Leibwächter beiseitegelegt worden waren. Alexander, der möglicherweise Verrat befürchtete, rief in höchster Erregung auf Makedonisch nach einer Lanze, entriss einer Wache eine und tötete mit ihr Kleitos, seinen Lebensretter am Granikos. Als Alexander wieder bei Besinnung war, bereute er diese Tat zutiefst: Es heißt, er solle geklagt und geweint und versucht haben, sich das Leben zu nehmen. Er sah diese Tat jedenfalls als einen seiner schwersten Fehler an. Alexanders Neigung zu übermäßigem Alkoholgenuss – er trank allerdings fast ausschließlich in Gesellschaft – blieb eine Schwäche, bei der er häufig die Selbstkontrolle verlor. Das gemeinsame Trinken der Männer selbst gehörte fest zum gesellschaftlichen Leben in der griechischen Welt (siehe Symposion).

Im folgenden Jahr 327 v. Chr. eroberte Alexander noch zwei sogdische Bergfestungen. Dann war niemand mehr übrig, der ihm Widerstand hätte leisten können. Zwei Jahre hatten die Sogdier sich gegen Alexander erhoben und ihn in immer neue Scharmützel verwickelt. Nach dieser Zeit waren die meisten von ihnen tot oder versklavt. Bevor Alexander nach Baktrien zurückkehrte, ließ er 11.000 Mann Besatzung in den eroberten Gebieten Sogdiens zurück.

Zurück in Baktra gab Alexander eine Reihe von Befehlen, die seine makedonische Generalität weiter von ihm entfremdete. Da sich baktrische Reiter bei den Feldzügen in Sogdien als hilfreich erwiesen hatten, befahl Alexander seinen Generälen, 30.000 junge Perser und Baktrier zu Phalanx-Soldaten auszubilden. Auch in die Kavallerie wurden Einheimische integriert. Die Soldaten akzeptierten die Auflagen widerstrebend, denn noch immer trauten sie den Persern nicht.

Alexander heiratete in Baktra die sogdische Prinzessin Roxane, Tochter eines Mannes namens Oxyartes (nicht identisch mit dem gleichnamigen Bruder des Dareios). Durch diese politische Heirat gedachte er zur Befriedung Sogdiens beizutragen. Dafür schickte Alexander seine langjährige Geliebte Barsine und den gemeinsamen unehelichen Sohn Herakles fort. Die Heirat war auch eine Beleidigung für Alexanders Verbündeten Artabazos, den Vater der Barsine, seinen Statthalter in Baktrien.

Außerdem versuchte Alexander, das persische Hofritual der Proskynese einzuführen: Jeder, der vor den König treten wollte, musste sich vor ihm verbeugen und das Gesicht auf den Boden pressen. Freie Makedonen und Griechen unterzogen sich einer solchen Unterwerfungsgeste allerdings nur vor den Göttern. Es heißt, dass mehrere von Alexanders Generälen sich weigerten, sich derart vor ihm zu erniedrigen. Fortan galt sie nur noch für Perser.

Alexanders Anordnungen wurden als so befremdlich empfunden, dass es diesmal zur offenen Revolte unter den griechischen Soldaten zu kommen drohte. Im Rahmen der sogenannten Pagenverschwörung ließ Alexander auch eine Reihe von einstigen Gefolgsleuten hinrichten, darunter seinen Hofbiografen Kallisthenes.

Nach der Eroberung des gesamten Perserreichs fasste Alexander den Beschluss, sein Imperium weiter nach Osten auszudehnen. Indien war für die Griechen ein halblegendäres Land, über das sie kaum etwas wussten. Das Land, das damals Indien genannt wurde, ist nicht identisch mit dem heutigen Staat Indien. Es begann dort, wo Persien endete, im Osten Afghanistans, und umfasste Pakistan und das heutige Indien. Eine definierte Ostgrenze gab es nicht, da kein Reisender jemals weit nach Indien vorgedrungen war. Die westlichsten Teile jenes Indiens hatten zu Zeiten Dareios’ I. zu Persien gehört, wobei Indien selbst kein geeinter Staat war, sondern aus einer Vielzahl wenig bekannter Kleinstaaten bestand. Für den Indienfeldzug gab es keinerlei militärische Notwendigkeit. Die Gründe werden auch heute noch in der Forschung diskutiert, ohne dass bisher eine Einigung erzielt worden wäre. Möglicherweise waren es Alexanders Neugier und Kriegslust, eine Art irrationales Streben und Sehnsucht nach Erfolgen "(pothos)"; aber auch Thesen wie die von dem Bestreben, seine Autorität durch immer neue militärische Siege zu festigen, werden angeführt. Jedenfalls sollte sich der Indienfeldzug als schwere Belastungsprobe erweisen.

Anfang des Jahres 326 v. Chr. stieß Alexander mit zwei Heeren ins Tal des Flusses Kabul vor, das damals ein Teil Indiens war. Der Vorstoß war von besonderer Grausamkeit gekennzeichnet. Immer seltener ließ Alexander gegenüber eroberten Regionen Großmut walten. Städte und Dörfer wurden zerstört und ihre Bevölkerung ermordet. Die zwei Armeen trafen einander am Indus. Alexander machte das Land zwischen Kabul und Indus zur Provinz Gandhara und ernannte seinen Gefolgsmann Nikanor zu deren Statthalter.

Am anderen Ufer des Indus wurden Alexanders Truppen von Omphis empfangen, dem König von Taxila, das etwa 30 km vom heutigen Islamabad entfernt lag. Hier traf Alexander einen Mann namens Kalanos, den er aufforderte, ihn auf seinen weiteren Feldzügen zu begleiten. Kalanos stimmte zu und wurde Alexanders Ratgeber; offensichtlich war er bei den kommenden Verhandlungen mit indischen Führern sehr von Nutzen.

Vom Hof des Omphis aus rief Alexander die anderen Staaten des Punjab auf, sich ihm zu unterwerfen und ihn als Gott anzuerkennen. Dies verweigerte Poros, der König von Pauravas, das von Taxila durch den Fluss Hydaspes (heute Jhelam) getrennt war. Im Mai überquerte Alexander während eines Platzregens den Hydaspes und besiegte eine berittene Einheit unter dem Sohn des Poros. Die Griechen und Perser zogen weiter ostwärts. Zahlenmäßig waren sie dem kleinen Heer des Poros, das sie erwartete, überlegen, doch kamen sie in dem üppig bewaldeten Land mit seinen ständigen Regenfällen schwer zurecht. Außerdem waren Berichte zu ihnen gedrungen, dass Poros eine Einheit von Kriegselefanten unterhielt, mit denen sich die Griechen nie zuvor gemessen hatten. In der Schlacht am Hydaspes wurden die Inder besiegt. In dieser Schlacht soll Alexanders Pferd Bukephalos im Hydaspes zu Tode gekommen sein, obwohl andere Quellen sagen, es sei schon vor der Schlacht an Altersschwäche eingegangen. Seinem langjährigen Reittier zu Ehren gründete Alexander die Stadt Bukephala (heute wahrscheinlich Jhelam in Pakistan). Poros wurde begnadigt und zu Alexanders Statthalter in Pauravas ernannt.

Weiter im Osten am Ganges lag das Königreich Magadha, das selbst den Menschen des Punjab kaum bekannt war. Alexander wollte auch dieses Land erobern. Bei heftigem Monsunregen quälte sich die weitgehend demoralisierte Armee ostwärts und hatte einen Hochwasser führenden Fluss nach dem anderen zu überqueren. Ende Juli stand die Überquerung des Hyphasis (heute Beas) an, und von Magadha waren die Soldaten noch weit entfernt. Hier meuterten die Männer und weigerten sich weiterzugehen; ihr einziges Bestreben war die Heimkehr. Alexander war außer sich, wurde aber letztlich zur Umkehr gezwungen. Am Ufer des Hyphasis gründete er ein weiteres Alexandreia und siedelte hier viele Veteranen an, die damit wenig Hoffnung hegen durften, jemals wieder nach Griechenland zurückzukehren.

Der beschwerliche Rückweg zum Hydaspes dauerte bis zum September. In Bukephala war mit dem Bau von 800 Schiffen begonnen worden, die den Fluss abwärts zum Indischen Ozean segeln sollten. Dies waren jedoch nicht genug, um Alexanders gesamte Armee zu transportieren, so dass Fußsoldaten die Schiffe am Ufer begleiten mussten. Im November brachen sie von Bukephala auf, doch nach zehn Tagen trafen sie am Zusammenfluss des Hydaspes mit dem Acesines (heute Chanab) auf Stromschnellen, in denen mehrere Schiffe kenterten und viele Griechen ihr Leben verloren.

Der weitere Weg führte durch indische Staaten, die Alexander nicht unterworfen hatte. Immer wieder wurde das Heer angegriffen, und die Perser und Griechen zerstörten Städte und Dörfer, wo sie ihnen in den Weg kamen. Im Kampf gegen die Maller wurde Alexander bei der Erstürmung einer Stadt (vielleicht Multan) durch einen Pfeil schwer verletzt. Das Geschoss drang in seine Lunge; obwohl Alexander überlebte, sollte er den Rest seines Lebens unter den Folgen dieser Verwundung leiden. Vom Krankenlager aus befahl er, dass am Zusammenfluss von Acesines und Indus ein weiteres Alexandreia (nahe dem heutigen Uch) gegründet und Roxanes Vater Oxyartes zum Statthalter der neuen Provinz ernannt werden solle.

Als Nächstes griff Alexander die Staaten von Sindh an, um seiner Armee den Weg nach Süden freizukämpfen. Die Könige Musicanos, Oxicanos und Sambos wurden unterworfen. Musicanos, der später eine Rebellion anzettelte, wurde letztlich gekreuzigt. Erst als der Monsun wieder einsetzte, erreichte das Heer 325 v. Chr. die Indusmündung und den Indischen Ozean. Alexander gründete hier die Stadt Xylinepolis (heute Bahmanabad) und machte die Flotte gefechtsbereit. Während etwa ein Viertel der Armee so auf dem Seeweg die Rückkehr antreten sollte, musste der Großteil über den Landweg nach Persien zurückkehren. Im August 325 v. Chr. machte sich das Landheer unter Alexanders Führung auf den Weg. Die Flotte unter dem Befehl des Nearchos brach einen Monat später überstürzt auf, da sich die Einheimischen zu erheben begonnen hatten. Praktisch unmittelbar nach dem Abzug des Heeres fielen die gerade eroberten Kleinstaaten Indiens ab und erhoben sich gegen die in den neuen Städten zurückgebliebenen Veteranen, über deren weiteres Schicksal in den wenigsten Fällen etwas bekannt ist.

Das heutige Belutschistan war damals als Gedrosien bekannt. Obwohl die Perser vor der Durchquerung der gedrosischen Wüste warnten, ging Alexander dieses Risiko ein, wahrscheinlich weil dieser Weg der kürzeste war. Die Hintergründe sind in der Forschung jedoch umstritten. Ob er wirklich die sagenhafte Königin Semiramis übertreffen wollte, ist wenigstens fraglich; wenn, dann ging es Alexander wohl darum, die Rückschläge des Indienfeldzugs durch dieses Unternehmen zu relativieren. Auch die Stärke seines Heeres zu diesem Zeitpunkt ist ungewiss, von wohl sicher übertriebenen 100.000 Mann bis zu wahrscheinlich realistischeren 30.000. Die sechzigtägigen Strapazen ließen zahllose Soldaten durch Erschöpfung, Hitzschlag oder Verdursten ums Leben kommen; dabei spielte auch der Umstand eine Rolle, dass Alexanders Führer offenbar recht unfähig waren. Im Dezember erreichten die Soldaten Pura (heute Bampur), einen der östlichsten Vorposten Persiens, und waren damit in Sicherheit.

Alexander gründete im Januar 324 v. Chr. ein weiteres Alexandreia; heute "Golashkerd". Auf dem Weg westwärts stieß er in Susa auf Nearchos und seine Männer, die den Seeweg weitgehend unversehrt überstanden hatten. Neue Feiern wurden genutzt, um 10.000 persische Frauen mit Soldaten zu verheiraten – die Massenhochzeit von Susa.

Die Ehen wurden von Alexander als Notwendigkeit gesehen, um das Zusammenwachsen von Persern und Makedonen/Griechen weiter voranzutreiben. Er selbst heiratete zwei Frauen, nämlich Stateira, eine Tochter des Dareios, und Parysatis. Er war somit nun mit drei Frauen verheiratet. Die Hochzeiten wurden nach persischem Ritual begangen. Schon Alexanders Vater hatte die Ehe mit mehreren Frauen als diplomatisches Mittel zur Stabilisierung und Ausweitung seines Machtbereiches eingesetzt.

In der Forschung wurde dies als Versuch interpretiert, eine Art „Verschmelzungspolitik“ zu betreiben (Johann Gustav Droysen). Der britische Historiker Tarn sah darin gar den Versuch einer „Vereinigung der Menschheit“; viele andere moderne Historiker wie Badian oder Bosworth lehnen dies jedoch ab.

Um weitere Attribute eines persischen Staates zu übernehmen, ernannte Alexander seinen langjährigen Freund Hephaistion (und nach dessen Tod Perdikkas) zum "Chiliarchen" (Wesir) und seinen General Ptolemaios zum Vorkoster. Beide Titel waren im Westen unbekannt. Außerdem wurden gegen mehrere Statthalter, die sich bereichert hatten oder ihren Aufgaben nicht sachgerecht nachgekommen waren, Prozesse eröffnet. Harpalos, ein Jugendfreund Alexanders und sein Schatzmeister, befürchtete aufgrund seines Verhaltens einen solchen Prozess. Er setzte sich mit 6000 Söldnern und 5000 Talenten Silber nach Griechenland ab, wurde jedoch bald darauf auf Kreta ermordet.

Die Neuerungen Alexanders vergrößerten die Kluft zwischen ihm und seiner makedonischen Generalität. Da die Zahl der Soldaten iranischer Herkunft im Heer die der Makedonen zu übertreffen begann, fürchteten sie, bald gänzlich bedeutungslos zu sein. Perser durften nun auch höhere Ränge in der Armee bekleiden, was die Makedonen als unerhört ansahen. Als die Armee die Stadt Opis am Tigris erreichte, erlaubte Alexander vielen Makedonen die Rückkehr nach Hause. Was sie vorher ersehnt hatten, sahen sie nun als Affront, da dies das erste Zeichen ihrer Ersetzung durch Orientalen zu sein schien. Quellen berichten, dass manche der Soldaten Alexander wüste Beleidigungen entgegen geschrien hätten. Alexander reagierte, indem er sie ihrer Stellungen enthob und drohte, die persischen Soldaten gegen sie zu schicken. Die Soldaten entschuldigten sich, und ihnen wurde verziehen. 11.500 griechische Soldaten wurden in den Folgetagen nach Hause geschickt.

Im Herbst des Jahres 324 v. Chr. ging Alexander nach Ekbatana, wo Hephaistion nach einem von vielen Trinkgelagen erkrankte und starb. Alexander, der wohl lange Jahre Hephaistions Geliebter gewesen war (zumindest bis zum Feldzug im Iran), war außer sich vor Trauer. Er ließ laut Plutarch den Arzt seines Freundes kreuzigen, die Haare von Pferden und Maultieren abrasieren und opfern, fastete mehrere Tage und richtete dann ein monumentales Begräbnis aus. Danach ließ er sämtliche Kossaier umbringen. Die Beziehung zwischen Alexander und Hephaistion wird oft mit der zwischen Achilleus und Patroklos gleichgesetzt. Denn da sich das Geschlecht von Alexanders Mutter Olympias auf den Helden aus dem Trojanischen Krieg zurückführte, verglich Alexander selbst sich mit Achilles und seinen Freund mit Patroklos.

Alexander hatte, so wie auch sein Vater Philipp und viele andere Makedonen bzw. Griechen seiner Zeit, Beziehungen sowohl zu Frauen – er hatte mehrere, deren bekannteste und wohl ernsthafteste die zu Roxane war – als auch zu Männern, wobei diese teils auch sexueller Natur waren. Gleichgeschlechtliche Beziehungen wurden zu jener Zeit nicht geächtet, es kam aber sehr wohl auf den sozialen Status der Partner an.

Alexander ließ den persischen königlichen Schatz ausmünzen und warf damit das Vermögen der Achämeniden in das Austauschsystem des Nahen Ostens, womit ein steiler Anstieg im Volumen der Markttransaktionen im Mittelmeergebiet finanziert wurde. Dass der attische Münzfuß nunmehr – außer im ptolemäischen Ägypten – allgemein in der hellenistischen Welt galt, erleichterte den internationalen Handel und die Schifffahrt.

Bei den Olympischen Spielen des Jahres 324 v. Chr. ließ Alexander das sogenannte Verbanntendekret verkünden, mit dem er den griechischen Poleis befahl, die jeweils aus politischen Gründen ins Exil getriebenen Bürger wieder aufzunehmen. Dies stellte einen massiven Eingriff in die Autonomie der Städte dar, führte zu heftigen Konflikten in den Gemeinwesen und war letztlich der Anlass dafür, dass sich Athen und mehrere andere Städte nach dem Tod des Königs im Lamischen Krieg gegen die makedonische Herrschaft erhoben.

Im Februar 323 v. Chr. kehrte Alexander nach Babylon zurück. Hier bereitete er neue Feldzüge vor, die zur Einnahme der Arabischen Halbinsel führen sollten. Ob er überdies, wie Diodor berichtet, auch plante, anschließend den westlichen Mittelmeerraum mit Karthago zu erobern, ist seit langer Zeit umstritten. In der neueren Forschung geht man zumeist davon aus, dass Alexander in der Tat eine solche Expedition vorbereiten ließ, da den Makedonen im Jahr 322 während des Lamischen Krieges eine sehr große Flotte zur Verfügung stand, die mutmaßlich ursprünglich für das Unternehmen gegen Karthago gebaut worden war. Im Mai, kurz vor dem geplanten Aufbruch des Heeres gen Arabien, verkündete Alexander, dass sein toter Freund Hephaistion fortan als Halbgott zu verehren sei, nachdem ein Bote aus der Oase Siwa eingetroffen war, wo Alexander wegen einer Vergöttlichung Hephaistions angefragt hatte. Aus diesem Anlass veranstaltete er Feiern, bei denen er sich wieder dem unmäßigen Trunk hingab. Am nächsten Tag erkrankte er an einem Fieber, und am 10. Juni starb er schließlich.

Hinsichtlich der Todesursache wurden seither mehrere Thesen diskutiert, darunter eine, nach der Alexander am West-Nil-Fieber erkrankte. Auch eine Alkoholvergiftung wird immer wieder in Erwägung gezogen. Nach einer in der Antike verbreiteten Überlieferung ist er hingegen vergiftet worden (angeblich mit dem giftigen Wasser des Styx). Wahrscheinlicher ist, dass seine körperliche Schwächung durch zahlreiche Kampfverletzungen und übermäßigen Weinkonsum zu einer Krankheit geführt hat. Da die Ärzte damals auf die reinigende Wirkung von herbeigeführtem Erbrechen und Durchfall vertrauten, war es üblich, Weißen Germer in geringen Dosen zu verabreichen. Die überlieferten Symptome Alexanders sind typisch für eine Vergiftung durch Weißen Germer. Möglicherweise verschlechterten die Ärzte seinen Zustand daher durch wiederholte Gaben des Mittels.

Der Leichnam Alexanders soll zur Konservierung in Honig gelegt worden sein. Entgegen dem Wunsch des Verstorbenen, im Ammonium von Siwa begraben zu werden, wurde er in Alexandria beigesetzt.

Alexanders letzte Worte auf die Frage, wem er sein Reich hinterlassen werde, sollen gelautet haben:
"Dem Besten." Des Weiteren äußerte Alexander eine dunkle Prophezeiung: Er glaube, "dass seine Freunde große Begräbnisspiele für ihn veranstalten werden". Seinen Siegelring übergab er Perdikkas, der nach Hephaistions Tod sein engster Vertrauter gewesen war.

Alexander hatte eine Beisetzung im Ammonheiligtum der Oase Siwa gewünscht. Erst nach zweijährigen Vorbereitungen setzte sich der Leichenzug in Babylon in Bewegung. Er wurde in Syrien von Ptolemaios, dem künftigen König Ptolemaios I., in Empfang genommen und nach Ägypten geleitet. Dort wurde der Leichnam aber nicht in die Oase gebracht, sondern zunächst in Memphis bestattet. Später (wohl noch in der Regierungszeit Ptolemaios’ I., spätestens einige Jahre nach seinem Tod) wurde er nach Alexandria verlegt, nachdem dort eine prächtige Grabstätte für ihn errichtet worden war. Sie wurde unter König Ptolemaios IV. durch ein neues Mausoleum ersetzt, das dann auch als Grabstätte der Ptolemäer diente, die sich wie alle Diadochen auf Alexanders Vorbild beriefen. Die mumifizierte Leiche befand sich in einem goldenen Sarkophag, der aber im 1. Jahrhundert v. Chr. von König Ptolemaios X. durch einen gläsernen ersetzt wurde, der den Blick auf den einbalsamierten Leichnam freigab. Dieser Schritt Ptolemaios' X., der später irrtümlich als Grabschändung gedeutet wurde, sollte den Alexanderkult fördern.

Für Caesar, Augustus, Septimius Severus und Caracalla sind Besuche am Grab bezeugt. Möglicherweise wurde es während der Stadtunruhen in der Spätantike oder bei einer Naturkatastrophe zerstört. In den Wirren der Spätantike ging die Kenntnis über den Ort der Grabstätte verloren (zumindest die Leiche soll laut Libanios noch Ende des 4. Jahrhunderts zu sehen gewesen sein). Der Kirchenvater Johannes Chrysostomos († 407) stellte in einer Predigt die rhetorische Frage nach dem Ort des Alexandergrabs, um die Vergänglichkeit des Irdischen zu illustrieren; er konnte also mit Sicherheit davon ausgehen, dass keiner seiner Hörer wusste, wo sich das berühmte Bauwerk befunden hatte. Die Erinnerung daran blieb aber noch in islamischer Zeit erhalten; im 10. Jahrhundert wurde eine angebliche Grabstätte gezeigt. Im 15. und 16. Jahrhundert berichteten europäische Reisende von einem kleinen Gebäude in Alexandria, das als Alexandergrab ausgegeben wurde. Seit dem 18. Jahrhundert sind viele Lokalisierungsversuche unternommen worden, die bisher alle fehlgeschlagen sind.

Nach Alexanders Tod erwies sich die Loyalität zu seiner Familie, die keinen herrschaftsfähigen Nachfolger stellen konnte, als sehr begrenzt. Zwar wurde zunächst der Erbanspruch seines geistesschwachen Halbbruders und auch der seines postum geborenen Sohnes anerkannt, doch hatte diese Regelung keinen Bestand. Seine Mutter Olympias von Epirus, seine Frau Roxane, sein Sohn Alexander IV., sein illegitimer Sohn Herakles, seine Schwester Kleopatra, seine Halbschwester Kynane, deren Tochter Eurydike und sein Halbbruder Philipp III. Arrhidaios fanden einen gewaltsamen Tod. Statt der Angehörigen des bisherigen makedonischen Königsgeschlechts übernahmen Alexanders Feldherren als seine Nachfolger (Diadochen) die Macht. Da keiner von ihnen stark genug war, sich als Alleinherrscher durchzusetzen, kam es zu einer langen Reihe von Bürgerkriegen, in denen man in wechselnden Koalitionen um die Macht rang. Im Verlauf der Diadochenkriege wurde das riesige Reich in Diadochenreiche aufgeteilt. Drei dieser Reiche erwiesen sich als dauerhaft: das der Antigoniden in Makedonien (bis 148 v. Chr.), das der Seleukiden in Vorderasien (bis 64 v. Chr.) und das der Ptolemäer in Ägypten (bis 30 v. Chr.). Alexander hinterließ zahlreiche neu gegründete Städte, von denen viele seinen Namen trugen; die bedeutendste war Alexandreia in Ägypten.

Alexander wurde schon zu Lebzeiten eine mythische Gestalt, wozu sein Anspruch auf Gottessohnschaft beitrug. Die zeitgenössischen erzählenden Quellen sind nicht oder nur in Fragmenten erhalten. Dabei handelte es sich, neben den Fragmenten der angeblichen Kanzleidokumente Alexanders "(Ephemeriden)", größtenteils um Berichte von Teilnehmern des Alexanderzugs. Der Hofhistoriker Kallisthenes begleitete Alexander, um die Taten des Königs aufzuzeichnen und zu verherrlichen. Sein Werk „Die Taten Alexanders“ reichte vielleicht nur bis 331 v. Chr., hatte jedoch einen enormen Einfluss auf die späteren Alexanderhistoriker. Weitere Verfasser von Alexandergeschichten waren König Ptolemaios I. von Ägypten, der als Offizier und Hofbeamter in der Nähe Alexanders gelebt hatte, Aristobulos, der für Alexander Unvorteilhaftes leugnete oder abschwächte, sowie Alexanders Flottenbefehlshaber Nearchos und dessen Steuermann Onesikritos. Die stärkste Nachwirkung unter diesen frühen Alexanderhistorikern erzielte Kleitarchos, der zwar ein Zeitgenosse, aber selbst kein Feldzugsteilnehmer war, sondern in Babylon Informationen von Offizieren und Soldaten Alexanders zusammentrug und zu einer rhetorisch ausgeschmückten Darstellung verband, wobei er auch sagenhafte Elemente einbezog. Zu diesen frühen Legenden gehörte beispielsweise die falsche Behauptung, Alexander und Dareios seien einander wiederholt im Nahkampf begegnet.

Im 2. Jahrhundert n. Chr. schrieb der römische Senator Arrian auf der Grundlage der älteren Quellen, unter denen er Ptolemaios und Aristobulos bevorzugte, seine "Anabasis", die verlässlichste antike Alexanderquelle. Wahrscheinlich behandelte auch Strabon in seinen nicht erhaltenen "Historika Hypomnemata" („Historische Denkwürdigkeiten“) das Leben Alexanders; seine erhaltene "Geographie" enthält Informationen aus verlorenen Werken der frühen Alexanderhistoriker.

Weitere Nachrichten finden sich im 17. Buch der Universalgeschichte Diodors, der sich auf Kleitarchos stützte. Plutarch verfasste eine Lebensbeschreibung Alexanders, wobei es ihm mehr auf das Verständnis des Charakters unter moralischem Gesichtspunkt als auf den historischen Ablauf ankam. Quintus Curtius Rufus schrieb eine in der Antike wenig beachtete Alexandergeschichte. Justin wählte für seine Darstellung aus seiner (verlorenen) Vorlage, der Universalgeschichte des Pompeius Trogus, vor allem Begebenheiten aus, die geeignet waren, seine Leserschaft zu unterhalten.

Die Berichte von Curtius, Diodor und Pompeius Trogus hängen von einer gemeinsamen Quelle ab; das Nachrichtenmaterial, das sie übereinstimmend überliefern, stammt wohl von Kleitarchos. Diese Tradition "(Vulgata)" bietet teils wertvolle Informationen; Curtius wird in der französischen Forschung leicht gegenüber Arrian favorisiert. Zusätzliches Material ist bei Athenaios sowie in der Metzer Epitome und dem Itinerarium Alexandri überliefert. Nur wenige Fragmente sind von den Werken des Chares von Mytilene und des Ephippos von Olynth erhalten.

Als Quelle für den historischen Alexander von relativ geringem Wert, aber literarisch von außerordentlicher Bedeutung ist der „Alexanderroman“. Mit diesem Begriff bezeichnet man eine Vielzahl von antiken und mittelalterlichen Biografien Alexanders, welche seine Taten roman- und märchenhaft schildern und verherrlichen. Im Lauf der Jahrhunderte wurde der Stoff fortlaufend literarisch bearbeitet und ausgeschmückt. Die griechische Urfassung in drei Büchern, die den Ausgangspunkt für alle späteren Versionen und Übersetzungen in viele Sprachen bildet, ist wahrscheinlich im späten 3. Jahrhundert in Ägypten entstanden. Ihr unbekannter Autor, der wohl ein Bürger von Alexandria war, wird als Pseudo-Kallisthenes bezeichnet, weil ein Teil der handschriftlichen Überlieferung das Werk irrtümlich dem Alexanderhistoriker Kallisthenes von Olynth zuschreibt. Diesem Werk lagen ältere, nicht erhaltene romanhafte Quellen, fiktive Briefe Alexanders und kleinere Erzählungen zugrunde. Der bekannteste unter den Briefen ist ein angeblich von Alexander an Aristoteles gerichtetes Schreiben über die Wunder Indiens, das in verkürzter Fassung in den Roman eingebaut wurde und auch separat überliefert ist.

Die gängige Bezeichnung „Roman“ ist irreführend, da der Verfasser und seine antike und mittelalterliche Leserschaft an dem Anspruch festhielten, der Inhalt sei Geschichtsschreibung und nicht literarische Erfindung.

Die Idee des historischen Alexander, er sei ein Sohn des ägyptischen Gottes Ammon (Amun), verfremdet der Romanautor, indem er aus Alexander ein uneheliches Kind macht. Alexanders Vater ist im Roman der aus Ägypten nach Makedonien geflohene König und Zauberer Nektanebos, der als Ammon auftritt (gemeint ist der Pharao Nektanebos II.). Nektanebos verführt die Königin Olympias während der Abwesenheit ihres Gemahls Philipp. Später tötet Alexander, der als Sohn Philipps aufwächst, seinen leiblichen Vater; erst dann erfährt er seine wahre Abstammung. So macht der ägyptische Autor Alexander zum Ägypter. Eine weitere wesentliche Neuerung des Pseudo-Kallisthenes ist die Einführung eines nicht historischen Italienzugs Alexanders, auf dem der Makedone nach Rom kommt. Rom unterstellt sich ihm ebenso wie alle anderen Reiche des Westens kampflos. Dann unterwirft er in schweren Kämpfen die Völker des Nordens, bevor er gegen das Perserreich zieht. Hier zeigt sich das literarische Bedürfnis, den Helden auch den Westen und Norden erobern zu lassen, damit seine Weltherrschaft vollendet wird. Roxane ist im Roman eine Tochter des Perserkönigs Dareios, die dieser sterbend Alexander zur Frau gibt. Das letzte der drei Bücher, das den Indienfeldzug und den Tod des Helden behandelt, ist besonders stark von Wundern und phantastischen Elementen geprägt. Es schildert auch Alexanders angeblichen Besuch bei der Königin Kandake von Meroe, wobei der König in Verkleidung auftritt, aber enttarnt wird (eine Episode, der spätere Bearbeiter des Stoffs eine ursprünglich völlig fehlende erotische Komponente verleihen). Schließlich wird Alexander vergiftet.

Im frühen 4. Jahrhundert fertigte Iulius Valerius eine freie lateinische Übersetzung des Alexanderromans an "(Res gestae Alexandri Magni)". Dabei nahm er Hunderte von Erweiterungen, Änderungen und Auslassungen vor. Er beseitigte Ungereimtheiten und Formulierungen, die den Makedonenkönig in ein ungünstiges Licht rücken konnten, und fügte für Alexander vorteilhafte Details ein. Sein Alexander ist eine mit allen Herrschertugenden ausgestattete Idealgestalt; er begeht zwar Fehler, lernt aber daraus.

Ein weiterer Bestandteil der antiken Alexandersage sind fiktive Dialoge des Königs mit den indischen Brahmanen sowie Briefe, die angeblich zwischen ihnen ausgetauscht wurden. Dabei versuchen die Inder, die Überlegenheit östlicher Weisheit und einer einfachen, naturnahen Lebensweise gegenüber der griechischen Zivilisation und dem Machtstreben Alexanders aufzuzeigen. Auch dieses Schrifttum war sowohl griechisch als auch lateinisch verbreitet. Da es um grundsätzliche Fragen der Lebensführung und um Askese ging, war die Wirkung in christlicher Zeit beträchtlich.

Die Herrscher, die nach Alexanders Tod in den verschiedenen Teilen seines Reichs an die Macht kamen, waren nicht mit ihm blutsverwandt, und soweit in Makedonien Loyalität zur herkömmlichen Ordnung vorhanden war, galt sie dem Herrscherhaus insgesamt, wobei es nicht speziell auf die verwandtschaftliche Nähe zu Alexander ankam. Daher gab es in den Diadochenreichen wenig Anlass für einen offiziellen staatlichen Alexanderkult; dieser blieb den einzelnen Städten überlassen. Erst in hoch- und späthellenistischer Zeit wurde der politische Rückgriff auf Alexander zu einem wichtigen propagandistischen Mittel. Einen Sonderfall bildete jedoch Ägypten, dessen neue Hauptstadt Alexandria eine Gründung Alexanders und der Ort seines Grabes war. Die dort regierenden Ptolemäer förderten von Anfang an den Alexanderkult im Rahmen ihrer Propaganda. Er bildete aber zunächst keinen zentralen Bestandteil ihrer Herrschaftslegitimation und wurde erst von Ptolemaios X., der den Doppelnamen „Ptolemaios Alexandros“ führte, intensiv politisch instrumentalisiert.

Ein prominenter Gegner der Römer, König Mithridates VI. von Pontos († 63 v. Chr.), fiel durch seine mit Nachdruck betriebene Alexander-Imitation auf. Er bekleidete sich mit dem Mantel Alexanders, den er von den Ptolemäern erbeutet hatte, und illustrierte so seinen Anspruch, Vorkämpfer des Griechentums und Retter der hellenistischen Monarchie vor den Römern zu sein. Später erbeutete der römische Feldherr Gnaeus Pompeius Magnus, der Mithridates besiegte, diesen Mantel und trug ihn bei seinem Triumphzug. Mit Pompeius, dessen Beiname „der Große“ an Alexander erinnerte, begann die offenkundige römische Alexander-Imitation, zunächst als Reaktion auf die Propaganda des Mithridates. Mehrere römische Feldherren und Kaiser stellten sich propagandistisch in Alexanders Nachfolge; sie verglichen sich mit ihm und versuchten, seine Erfolge im Osten zu wiederholen. Dabei steigerte sich die Verehrung Alexanders in manchen Fällen zu einer demonstrativen Nachahmung von Äußerlichkeiten. Zu den Verehrern und Nachahmern Alexanders zählten unter den Kaisern insbesondere Trajan, Caracalla und (mit Vorbehalten) Julian. Augustus trug zeitweilig auf seinem Siegelring ein Bildnis Alexanders, Caligula legte sich den aus Alexandria geholten angeblichen Panzer Alexanders an, Nero stellte für einen geplanten Kaukasusfeldzug eine neue Legion auf, die er „Phalanx Alexanders des Großen“ nannte, Trajan setzte sich einen Helm auf, den Alexander getragen haben soll. Kaiser Severus Alexander, der ursprünglich Alexianus hieß, änderte seinen Namen in Anknüpfung an den Makedonen.

Einen sehr tiefen und dauerhaften Eindruck hinterließ in Griechenland die Zerstörung Thebens. Sie wurde nicht nur von den Zeitgenossen, sondern jahrhundertelang (noch in der römischen Kaiserzeit) als unerhörte Grausamkeit empfunden, die man Alexander zur Last legte, und als historisches Musterbeispiel einer entsetzlichen Katastrophe zitiert. Besonders die antiken Redner kamen mit Vorliebe darauf zu sprechen und nutzten diese Gelegenheit, bei ihrem Publikum starke Emotionen zu wecken. Es hieß, Alexander habe wie ein wildes Tier und als Unmensch "(apánthrōpos)" gehandelt. Noch in byzantinischer Zeit wurde diese Deutungstradition rezipiert.

Aus philosophischer Sicht wurde Alexander meist negativ beurteilt, da seine Lebensweise einen Kontrast zu den philosophischen Idealen der Mäßigung, Selbstbeherrschung und Seelenruhe bildete. Insbesondere die Stoiker kritisierten ihn heftig und warfen ihm Hochmut vor; ihre Kritik richtete sich auch gegen Aristoteles (den Gründer einer rivalisierenden Philosophenschule), der als Erzieher Alexanders versagt habe. Auch die Kyniker pflegten Alexander abschätzig zu beurteilen, wobei die Anekdote von der Begegnung des Königs mit dem berühmten kynischen Philosophen Diogenes von Sinope den Ansatzpunkt bildete. Ihr zufolge hatte Diogenes Alexander, der ihm einen Wunsch freistellte, nur gebeten: „Geh mir aus der Sonne“, und Alexander soll gesagt haben: „Wenn ich nicht Alexander wäre, wollte ich Diogenes sein.“ In der von Aristoteles gegründeten Philosophenschule der Peripatetiker war die Ablehnung Alexanders ebenfalls ausgeprägt, wenn auch nicht durchgängig. Ihr Anlass waren anscheinend ursprünglich Spannungen zwischen Aristoteles und Alexander, die noch in der römischen Kaiserzeit ein spätes Echo in einem haltlosen Gerücht fanden, wonach Aristoteles ein Gift zubereitet hatte, mit dem Alexander ermordet wurde. Das negative Alexander-Bild der Philosophen teilte auch Cicero. Er überliefert die berühmte Anekdote von dem gefangenen Seeräuber, der von Alexander wegen seiner Übeltaten zur Rede gestellt wurde, worauf der Pirat erwiderte, er handle in kleinem Maßstab aus demselben Antrieb, aus dem der König weltweit dasselbe tue.

Besonders drastisch drückte Seneca die stoische Sichtweise aus. Er bezeichnete Alexander als wahnsinnigen Burschen, zum Bersten aufgeblasenes Tier, Räuber und Plage der Völker. Ähnlich äußerte sich Senecas Neffe, der Dichter Lucan. Der philosophisch orientierte Kaiser Julian, der Alexander als Feldherrn bewunderte, kritisierte ihn zugleich scharf wegen Maßlosigkeit und unphilosophischer Lebensführung.

Unter den philosophisch orientierten Autoren gab es auch eine kleine Minderheit, die Alexander Lob spendete. Dazu gehörte Plutarch, der in seinen zwei Deklamationen „Über das Glück oder die Tugend Alexanders des Großen“ aus dem König einen Philosophenherrscher machte, dessen Eroberungen barbarischen Völkern Recht und Frieden brachten und die Unterworfenen so humanisierten. Bei diesen Jugendwerken Plutarchs handelte es sich allerdings um rhetorische Stilübungen, die nicht notwendigerweise seine wirkliche Auffassung spiegeln. In seiner Lebensbeschreibung Alexanders äußerte sich Plutarch weit kritischer, bemühte sich aber auch um eine Rechtfertigung Alexanders. Dion von Prusa, der den an Alexander anknüpfenden Kaiser Trajan bewunderte, würdigte die heldenhafte Gesinnung des Makedonenkönigs.

Bei den Römern war ein beliebtes Thema die hypothetische Frage, wie ein militärischer Konflikt zwischen dem Römischen Reich und Alexander verlaufen wäre. Der Historiker Livius befasste sich eingehend damit und kam zum Ergebnis, dass die römischen Heerführer dem Makedonenkönig überlegen waren. Alexander habe seine Siege der militärischen Untüchtigkeit seiner Gegner verdankt. Diese Einschätzung verband Livius mit einem vernichtenden Urteil über Alexanders Charakter, der durch die Erfolge des Königs verdorben worden sei. Ähnlich urteilte Curtius Rufus, der die Siege des Makedonen mehr auf Glück als auf Tüchtigkeit zurückführte und meinte, die Herausbildung tyrannischer Züge in Alexanders Charakter sei ein Ergebnis übermäßigen Erfolgs gewesen.

Aus jüdischer Sicht fiel das Urteil über Alexander sehr vorteilhaft aus. Der jüdische Geschichtsschreiber Flavius Josephus beschreibt Gunstbezeugungen des Makedonen für die Juden und behauptet, Alexander habe sich, als er nach Jerusalem kam, vor dem Gott, den die Juden verehrten, niedergeworfen. Dabei handelt es sich um eine jüdische Abwandlung einer griechischen Erzählung.

Im 4. Jahrhundert wurden im Osten des Reichs Bronzemünzen Alexanders wie Amulette getragen.

Unter den Kirchenvätern hebt sich Orosius als radikalster Kritiker Alexanders ab. In seiner auf Justin fußenden "Historia adversus paganos" („Geschichte gegen die Heiden“) schildert er ihn als blutdürstigen, grausamen Unmenschen und großen Zerstörer.

Die mittelalterliche Alexander-Rezeption war außerordentlich intensiv und vielfältig. Dabei stand das Sagengut im Vordergrund. Die antike Gestalt wurde mittelalterlichen Vorstellungen angepasst; beispielsweise erhält der König eine Ritterpromotion (Schwertleite). Besonders Dichter regte der Stoff im Westen ebenso wie im Orient zur Bearbeitung an; es entstanden über 80 Dichtungen in 35 Sprachen.

Die grundlegenden antiken Quellen, die im Mittelalter in West- und Mitteleuropa zur Verfügung standen, waren neben Pseudo-Kallisthenes der eifrig rezipierte Curtius Rufus, der nur als Nebenquelle dienende Justin und der viel beachtete Orosius, dessen negative Bewertung Alexanders allerdings wenig Beachtung fand. Besonders die märchenhaften Elemente des Alexanderromans machten Eindruck und regten die Phantasie der Bearbeiter zu weiteren Ausformungen an. Der Roman wurde in zahlreiche europäische Sprachen übersetzt, wobei lateinische Fassungen die Grundlage bildeten; hinzu kamen die teils stark abweichenden Versionen in orientalischen Sprachen (Armenisch, Altsyrisch, Hebräisch, Arabisch, Persisch, Türkisch, Äthiopisch, Koptisch).

Eine wesentliche Rolle spielte ferner die Prophetie im biblischen Buch Daniel über den Untergang der aufeinanderfolgenden Weltreiche; in diesem Licht erschien Alexander, der nach mittelalterlicher Deutung das zweite der vier Weltreiche vernichtete und das dritte gründete, als Werkzeug Gottes. Auch dem ersten Kapitel des ersten Makkabäerbuchs war eine knappe Zusammenfassung von Alexanders Lebensgeschichte zu entnehmen; dort las man, dass er bis ans Ende der Welt gelangte und „die Welt vor ihm verstummte“. Dieser biblische Hintergrund verlieh ihm zusätzliche Bedeutung.

Im Spätmittelalter zählte man Alexander zum Kreis der Neun Guten Helden, einem in der volkssprachlichen Literatur beliebten Heldenkatalog, der für die Zeit des Alten Testaments, die griechisch-römische Antike und die christliche Zeit jeweils die drei größten Helden benannte; für die Antike waren es Hektor, Alexander und Caesar. Noch breiter als in der Literatur wurde diese Heldenreihe in der Bildenden Kunst (Skulptur, Malerei, Textilkunst) rezipiert.

Das Alexanderbild in der lateinischsprachigen Welt des Mittelalters war großenteils vom lateinischen Alexanderroman geprägt. Im Frühmittelalter ging die Hauptwirkung nicht von der ursprünglichen Fassung der von Iulius Valerius stammenden Übersetzung aus, von der nur drei vollständige Handschriften überliefert waren; weit bekannter war ein in mehr als 60 Handschriften erhaltener, spätestens im 9. Jahrhundert entstandener Auszug "(Epitome)" aus diesem Werk. Um 968/969 fertigte der Archipresbyter Leo von Neapel eine neue lateinische Übersetzung des Pseudo-Kallisthenes aus dem Griechischen an, die "Nativitas et victoria Alexandri Magni" („Geburt und Sieg Alexanders des Großen“), die mehrfach – zuletzt noch im 13. Jahrhundert – überarbeitet und erweitert wurde; die überarbeiteten Fassungen sind unter dem Titel "Historia de preliis Alexandri Magni" („Geschichte von den Schlachten Alexanders des Großen“) bekannt. Der Dichter Quilichinus von Spoleto schrieb 1237/1238 eine Versfassung der "Historia de preliis" in elegischen Distichen, die im Spätmittelalter populär wurde. Noch weit einflussreicher war aber die schon zwischen 1178 und 1182 verfasste "Alexandreis" Walters von Châtillon, ein Epos in zehn Büchern auf der Grundlage der Darstellung des Curtius Rufus, das zur Schullektüre wurde und im 13. Jahrhundert als Schulbuch Vergils Aeneis an Beliebtheit übertraf. Walter verzichtete fast gänzlich auf die Auswertung des im Alexanderroman vorliegenden Materials. Für ihn war Alexander der stets siegreiche Held, der sich selbst ebenso wie alle Feinde überwand und so unsterblichen Ruhm erlangte.

Das Verhältnis dieser Autoren und ihres Publikums zu Alexander war vor allem von Bewunderung für außerordentliche Heldentaten und von Staunen über das Märchenhafte und Exotische geprägt. Besondere Beachtung fand Alexanders Tod; er bot Anlass zu unzähligen religiös-erbaulichen Betrachtungen, die auf die Endlichkeit und Nichtigkeit aller menschlichen Größe angesichts des Todes abzielten. Auf diesen Aspekt wiesen unter anderem viele Kleindichtungen hin, darunter insbesondere fingierte Grabschriften Alexanders.

Besonders fasziniert waren mittelalterliche Leser von einer Erzählung von Alexanders Himmelsflug und Tauchexpedition, die Leo von Neapel nach dem griechischen Roman wiedergab. Dieser Sage zufolge wollte der König nicht nur auf der Erdoberfläche die äußersten Grenzen erreichen, sondern auch den Himmel und die Tiefe des Ozeans erkunden. Zu diesem Zweck ersann und baute er mit seinen Freunden ein von Greifen gezogenes Luftfahrzeug und ein von Ketten gehaltenes gläsernes Tauchfahrzeug. Der Himmelsflug wurde von mittelalterlichen Künstlern häufig abgebildet.

Aus dem 12. Jahrhundert stammt das "Iter ad Paradisum" („Paradiesfahrt“), die lateinische Version einer jüdischen Sage über Alexanders Versuch, das irdische Paradies zu finden, den in der Genesis beschriebenen Garten Eden.

Neben der Heldenverehrung kamen vereinzelt auch extrem negative Deutungen der Persönlichkeit Alexanders vor. So setzten ihn im 12. Jahrhundert die prominenten Theologen Hugo von St. Viktor und Gottfried von Admont mit dem Teufel gleich.

Erzählungen aus dem Alexanderroman wurden in Weltchroniken und Enzyklopädien aufgenommen, was ihre Rezeption zusätzlich erweiterte.

Die lateinische Überlieferung bildete die Grundlage für die volkssprachliche Rezeption. In den volkssprachlichen Literaturen entstanden zahlreiche Prosawerke und Dichtungen über Stoffe der Alexandersage, wobei vor allem die verschiedenen lateinischen Fassungen des Pseudo-Kallisthenes, die "Historia Alexandri" des Curtius Rufus und die "Alexandreis" Walters von Châtillon verarbeitet wurden.

Alberich von Bisinzo (Albéric de Pisançon), der im frühen 12. Jahrhundert die älteste volkssprachliche Alexander-Biografie verfasste, ein nur teilweise erhaltenes Gedicht in frankoprovenzalischem Dialekt, verwarf nachdrücklich die Legende von Alexanders unehelicher Geburt und hob seine hochadlige Abstammung von väterlicher und mütterlicher Seite hervor. Er betonte auch die hervorragende Bildung des Herrschers, die – einem mittelalterlichen Bildungsideal entsprechend – neben dem Griechischen (das der Makedone wie eine Fremdsprache lernen musste) auch Latein- und Hebräischkenntnisse umfasst habe. Nach der Mitte des 12. Jahrhunderts entstanden weitere französische Gedichte, die einzelne Episoden aus Alexanders Leben (Belagerung von Tyros, Indienfeldzug, Lebensende) behandelten. Sie wurden im späten 12. Jahrhundert zur „Standardversion“ des altfranzösischen "Roman d’Alexandre" (auch: "Roman d’Alixandre") zusammengefügt, die von allen im romanischen Sprachraum verbreiteten volkssprachlichen Bearbeitungen des Stoffs die stärkste Wirkung erzielte. Dieses Epos besteht aus über 20 000 Versen, Zwölf- und Dreizehnsilbern; vom "Roman d’Alexandre" erhielt dieses Versmaß später die Bezeichnung Alexandriner. Der Roman schildert Alexanders Leben durch Verknüpfung von vier Gedichten unterschiedlichen Ursprungs. Dabei kommt zum Altbestand der Alexanderlegende noch eine Reihe von frei erfundenen Personen und Begebenheiten hinzu. Der Autor stellt Alexander im Stil der Chanson de geste wie einen sehr standesbewussten, ritterlichen Lehnsherrn des Mittelalters dar. Er hebt dabei besonders die Großzügigkeit seines Helden hervor und präsentiert das Ideal eines harmonischen Verhältnisses zwischen König und Vasallen. Neben epischen Partien, besonders in den Kampfschilderungen, finden sich auch stärker romanhafte und vom Phantastischen geprägte. Mehrere Dichter fügten später Ergänzungen hinzu, insbesondere die einem Publikumsbedürfnis entsprechende Darstellung der Rache für den Giftmord an Alexander. In England schrieb Thomas von Kent im späten 12. Jahrhundert einen Alexanderroman in Alexandrinern in anglonormannischer Sprache mit dem Titel "Le roman de toute chevalerie". Er akzeptierte im Gegensatz zu allen älteren romanhaften Bearbeitungen des Stoffs problemlos die Vorstellung, dass Alexander aus einem Ehebruch seiner Mutter hervorging, was für die früheren Autoren ein nicht akzeptabler Makel gewesen war.

Im 15. Jahrhundert entstanden Prosafassungen des "Roman d’Alexandre". Der altfranzösische Prosa-Alexanderroman fand weite Verbreitung. Einen Höhepunkt erreichte die Alexander-Bewunderung im Herzogtum Burgund am Hof Herzog Philipps des Guten († 1467) und seines Nachfolgers, Karls des Kühnen.

Die bedeutendste spanische Bearbeitung des Stoffs ist "El libro de Alexandre". Dieses Epos umfasst über 10 000 Verse (Alexandriner) und ist damit die umfangreichste epische Dichtung Spaniens aus dem 13. Jahrhundert. Der unbekannte Verfasser, ein vorzüglich gebildeter Geistlicher, verfolgt ein moralisches Ziel; er will dem Leser anhand der erzählten Begebenheiten die vorbildliche Tugendhaftigkeit des Helden vor Augen stellen.

In Italien entstand eine Reihe von volkssprachlichen Werken über Alexanders Lebens in Prosa und in Versen, deren Grundlage meist die lateinische "Historia de preliis" war. Die älteste vollständig erhaltene italienische Alexanderdichtung ist die "Istoria Alexandri regis" von Domenico Scolari aus der ersten Hälfte des 14. Jahrhunderts. Scolari christianisiert seinen Helden weitgehend; Alexander ist ein frommer, geradezu heiliger Wundertäter. Als Universalmonarch beglückt er die Welt durch Recht und Frieden. Im 15. Jahrhundert erreichte das Interesse an der Alexandersage in Italien seinen Höhepunkt.

Die deutschsprachige Alexandersage und Alexanderdichtung setzte um die Mitte des 12. Jahrhunderts mit dem Alexanderlied des Pfaffen Lamprecht ein, der sich eng an Alberichs Versroman hielt. Die drei erhaltenen, später bearbeiteten Fassungen von Lamprechts Gedicht, der „Vorauer Alexander“, der „Straßburger Alexander“ und der „Basler Alexander“, setzten jedoch in der Bewertung Alexanders unterschiedliche Akzente. Im „Vorauer Alexander“ wird deutliche Kritik am König geübt. Alexander handelt zwar nach dem Willen Gottes, wird aber als hochmütig und herrschsüchtig dargestellt; die Zerstörung von Tyros wird als schweres Unrecht verurteilt, da die Tyrer als treue Untertanen des Perserkönigs nur ihre Pflicht erfüllten. Überdies erscheint er als mitleidlos, da er nicht über den Tod der vielen Gefallenen trauert. Andererseits verfügt er aber über Umsicht, die ihn seine Neigung zu jähzorniger Unbeherrschtheit überwinden lässt, womit er ein Beispiel gibt und sich von dem sehr negativ gezeichneten Dareios abhebt. Alexander wird bewusst als zwiespältige Persönlichkeit gezeichnet. Ein einfacheres Alexanderbild entwirft ein aus ritterlich-aristokratischer Sicht wertender Autor im „Straßburger Alexander“; hier wird der König als vorbildlicher Kämpfer, Feldherr und Herrscher idealisiert. Als solcher handelt er nicht eigenmächtig, sondern sucht den Rat seiner Vasallen. Er ist klug, gerecht und gütig, und seine schon in der Antike negativ bewertete Neigung zum Jähzorn wird als einigermaßen berechtigt dargestellt. Allerdings ist er nicht frei von Hochmut; zum vollkommenen Herrscher fehlt ihm die Mäßigung, die er aber in seiner letzten Lebensphase doch noch erlangt, womit er das Ideal restlos verwirklicht. Im „Basler Alexander“ dominiert ein anderes, in der mittelalterlichen Alexander-Rezeption ebenfalls zentrales Element, die Freude am Wunderbaren, Seltsamen und Exotischen. Diese Behandlung des Stoffs zielt auf das Unterhaltungsbedürfnis eines breiten, nicht mehr primär an ritterlichen Idealen orientierten spätmittelalterlichen Publikums.

Im 13. Jahrhundert verfasst der Dichter Rudolf von Ems das (allerdings unfertig gebliebene) Epos "Alexander". Er schildert den König als vorbildlich tugendhaften Helden und ritterlichen Fürsten, der sich durch seine moralischen Qualitäten als Herrscher legitimiert. Alexander vollzieht als Werkzeug Gottes dessen Willen. Durch ihn werden die Perser, die mit ihrem Verhalten den Zorn des Allmächtigen hervorgerufen haben, gezüchtigt. Sein Handeln ist Teil der Heilsgeschichte, er kann christlichen Herrschern als Vorbild dienen. Ulrich von Etzenbach beschreibt in seinem zwischen 1271 und 1282 entstandenen Gedicht "Alexander" (28.000 Verse) den König nicht nur als edlen Ritter, sondern auch als überaus frommen Mann Gottes, der seine Siege seinem gottgefälligen Verhalten und Gottvertrauen verdankt; die ihm zugeschriebenen Tugenden stammen aus der Heiligendarstellung. Ulrich missbilligt allerdings einzelne Taten wie die Ermordung Parmenions; darin unterscheidet er sich von Rudolf, bei dem Alexander makellos ist und Parmenion sein Schicksal selbst verschuldet. 1352 vollendet der nur aus seinem einzigen Werk bekannte Dichter Seifrit seine Alexanderdichtung, in der er besonders die Rolle Alexanders als Weltherrscher betont und sich bemüht, von seinem Helden den gängigen Vorwurf des Hochmuts fernzuhalten.

Im 14. und im 15. Jahrhundert war der Alexanderstoff in neuen Prosabearbeitungen weit verbreitet; die eine befindet sich im "Großen Seelentrost" (Mitte des 14. Jahrhunderts), die andere ist Johann Hartliebs "Histori von dem grossen Alexander", die nach der Mitte des 15. Jahrhunderts entstand. Beide dienten einem moralischen Zweck, doch ihre Verfasser gingen dabei auf völlig entgegengesetzte Weise bewertend vor. Im Großen Seelentrost bietet Alexander das abschreckende Lehrbeispiel eines durch und durch gierigen Menschen, den seine Neugier, Besitzgier und Machtgier letztlich ins Verderben führt, denn er versucht die dem Menschen gesetzten Grenzen zu überschreiten. Bei Hartlieb hingegen ist er ein Vorbild an Mannes- und Fürstentugend und überdies von einem wissenschaftlichen Erkenntnisstreben beseelt. Für mittelalterliche Verhältnisse auffallend ist die positive Wertung der Wissbegierde, eines auf die Natur gerichteten Forscherdrangs, der Alexander zugeschrieben wird.

Im 15. Jahrhundert wurden auch Alexanderdramen geschaffen und aufgeführt, doch sind ihre Texte nicht erhalten.

Während die mit literarischem Anspruch gestalteten Werke Alexander in der Regel verherrlichen oder zumindest in überwiegend positivem Licht erscheinen lassen, werden im religiös-erbaulichen und moralisch belehrenden Prosaschrifttum oft negative Züge des Makedonenkönigs betont; dort wird er als abschreckendes Beispiel für Maßlosigkeit und Grausamkeit angeführt. Sein Himmelsflug dient Geistlichen wie Berthold von Regensburg als Symbol für frevelhaften Übermut. Andererseits heben bedeutende Dichter wie Walther von der Vogelweide und Hartmann von Aue Alexanders vorbildliche "milte" (Freigebigkeit) hervor.

Trotz des traditionell großen Interesses am Alexanderstoff in England gab es erst im Spätmittelalter einen Alexanderroman in englischer Sprache, die mittelenglische Dichtung "Kyng Alisaunder", die wohl aus dem frühen 14. Jahrhundert stammt. Sie schildert den König als Helden und hebt seine Großmut hervor, verschweigt aber auch nicht seine Maßlosigkeit und Unbesonnenheit. Eine Reihe von weiteren Schilderungen von Alexanders Leben fußte auf der "Historia de preliis Alexandri Magni", die im mittelalterlichen England beliebt war.

Auch für die volkstümliche byzantinische Alexander-Rezeption bildete der Roman des Pseudo-Kallisthenes den Ausgangspunkt. Er lag zunächst in einer mittelgriechischen Prosabearbeitung aus dem 7. Jahrhundert vor. In spätbyzantinischer Zeit entstanden mehrere Neufassungen. Hier hat Alexander die Gestalt eines byzantinischen Kaisers angenommen; er ist von Gott gesandt und mit allen Ritter- und Herrschertugenden ausgestattet, wird aber nicht zum Christen gemacht, sondern dem Bereich des Alten Testaments zugeordnet. Er ist mit dem Propheten Jeremia befreundet und wird von ihm beschützt. 1388 entstand das byzantinische Alexandergedicht.

Die beliebteste Szene aus der Alexandersage war in Byzanz der Himmelsflug, der in der Bildenden Kunst oft dargestellt wurde.

In den süd- und ostslawischen Literaturen wurde der Alexanderstoff stark rezipiert, wobei der Weg des Überlieferungsguts vom griechischen Alexanderroman über kirchenslawische Bearbeitungen in die Volkssprachen führte. Eine altbulgarische Fassung des Romans "(Aleksandria)" wurde zum Ausgangspunkt der Rezeption in russischen Chroniken. In Russland war der Alexanderroman im Hochmittelalter in mehreren Versionen verbreitet. Im 14. Jahrhundert begann eine neue Version zu dominieren, die vom byzantinischen Volksroman ausging und sich durch stark ausgeprägte Merkmale des mittelalterlichen Ritterromans auszeichnete. Besonders beliebt war die serbische Fassung („serbischer Alexander“ oder „serbische Alexandreis“), die auch in Russland Verbreitung fand und Vorlage für die spätmittelalterliche georgische Prosaübersetzung war. In Russland, der Ukraine, Bulgarien und Rumänien setzte sich dieser Typus der Alexanderlegende durch.

In der mittelalterlichen arabischsprachigen Literatur war Alexander unter dem Namen „al-Iskandar“ bekannt, da der Anfang seines Namens mit dem arabischen Artikel "al" verwechselt wurde. Er wurde schon in der vorislamischen Dichtung erwähnt. Folgenreich war seine Identifizierung mit der koranischen Figur des Dhū l-Qarnain („der Zweihörnige“), von dem in Sure 18 erwähnt wird, dass er einen Damm gegen Gog und Magog errichtete (Verse 83–98). Diese Identifizierung wurde von den muslimischen Gelehrten mehrheitlich, aber nicht einhellig akzeptiert. Nach heutigem Forschungsstand ist die Ableitung der Figur Dhū l-Qarnains von Alexander sowie die Herkunft des Motivs aus der altsyrischen christlichen Alexanderlegende eine gesicherte Tatsache. Die im Orient verbreitete Bezeichnung Alexanders als „zweihörnig“ taucht schon in einer spätantiken Alexanderlegende in altsyrischer Sprache auf, wo Alexander ein christlicher Herrscher ist, dem Gott zwei Hörner auf dem Kopf wachsen ließ, womit er ihm die Macht verlieh, die Königreiche der Welt zu erobern. Den ursprünglichen Anlass zur Bezeichnung „der Zweihörnige“ bot die antike bildliche Darstellung Alexanders mit Widderhörnern, die auf seine Vergöttlichung deutete. Der Gott Zeus Ammon (Amun), als dessen Sohn Alexander sich betrachtete, wurde als Widder oder widderköpfig dargestellt.

Im Koran wird die Geschichte des Zweihörnigen dem Propheten geoffenbart, denn er soll sie mitteilen, wenn er danach gefragt wird. Alexander erscheint darin als frommer Diener Gottes, dem die Macht auf der Erde gegeben war und „ein Weg zu allem“. Er gelangte bis zum äußersten Westen der Welt, wo die Sonne „in einer verschlammten Quelle untergeht“, und erlangte die Herrschaft über das dort lebende Volk (hier ist ein Nachhall von Pseudo-Kallisthenes zu erkennen, der Alexander nach Italien kommen und den gesamten Westen einnehmen ließ). Dann schlug der Zweihörnige den Weg zum äußersten Osten ein und gelangte an den Ort, wo die Sonne aufgeht (daher deuteten die mittelalterlichen Koranausleger die Zweihörnigkeit meist als Zeichen für die Herrschaft über Westen und Osten). Schließlich begab er sich in eine andere Richtung und kam in eine Gegend, wo Menschen lebten, die von Angriffen zweier Völker, der Yāǧūǧ und Māǧūǧ (biblisch Gog und Magog), bedroht waren und ihn um Hilfe baten. Zum Schutz der Bedrohten baute er, ohne einen Lohn zu verlangen, zwischen zwei Berghängen einen gigantischen Wall aus Eisen, den die Angreifer nicht übersteigen oder durchbrechen konnten. Dieser Schutzwall wird bis zum Ende der Welt bestehen. – Eine altsyrische Version der Sage von Alexanders Aussperrung von Gog und Magog (in den "Revelationes" des Pseudo-Methodius) wurde ins Griechische und ins Lateinische übersetzt und fand in Europa viel Beachtung.

Auch die voranstehende Passage der 18. Sure (Verse 59–81) scheint von der Alexanderlegende beeinflusst zu sein, obwohl in der Version des Korans Mose statt Alexander der Protagonist ist. Ein dort erzähltes Wunder (Wiederbelebung eines getrockneten Fisches) stammt anscheinend aus dem Alexanderroman; es kommt auch in einer spätantiken altsyrischen Version der Legende vor. Es ist davon auszugehen, dass der Stoff des Alexanderromans zur Entstehungszeit des Korans bereits in arabischer Übersetzung verbreitet war.

Die islamische Wertschätzung für Alexander, die sich aus seiner Schilderung im Koran ergab, führte dazu, dass einige Autoren ihn zu den Propheten zählten.

Die mittelalterlichen arabischsprachigen Historiker behandelten die Regierung Alexanders eher knapp. Im Gegensatz zu den europäischen christlichen Chronisten gingen bedeutende muslimische Geschichtsschreiber wie Ṭabarī, Masʿūdī, Ibn al-Aṯīr und Ibn Chaldūn auf die Alexandersage nicht oder nur nebenbei ein; sie hielten sich primär an die Überlieferung über den historischen Alexander. Ṭabarī betrachtete seine Quellen kritisch; er stützte sich insbesondere auf die Darstellung des bedeutenden Gelehrten Ibn al-Kalbī († 819/821) und stellte die Vernichtung des Perserreichs als notwendig und berechtigt dar, da Dareios tyrannisch regiert habe. Die Auseinandersetzung mit dem Legendenstoff war kein Thema der Geschichtsschreiber, sondern ein Anliegen der Theologen, die sich mit der Koranauslegung befassten. Reichhaltiges Legendenmaterial über Alexander war im muslimischen Spanien (Al-Andalus) verbreitet; dort hieß es, er habe die Iberische Halbinsel als König beherrscht und in Mérida residiert.

Außerdem kommt Alexander auch in der arabischen Weisheitsliteratur vor, wo er als Gelehrter und Musikliebhaber beschrieben wird. Sehr oft taucht sein Name in Spruchsammlungen auf, wobei die Sprüche teils ihm zugeschrieben werden, teils von ihm handeln.

Im Persischen wurde Alexander "Iskandar", "Sikandar" oder "Eskandar" genannt. In der Spätantike war im persischen Sassanidenreich eine Legende verbreitet, wonach er der persischen Religion, dem Zoroastrismus, einen schweren Schlag versetzte, indem er religiöse Schriften vernichten ließ. Daher war Alexander bei den Anhängern dieser Religion verhasst und wurde als teuflisches Wesen betrachtet. Nach der Islamisierung wirkte sich diese Sage aber im gegenteiligen Sinne aus, denn nun machte man aus Alexander einen Vorkämpfer des Monotheismus gegen heidnische Götzendiener.

Der berühmte persische Dichter Firdausī († 1020) baute eine Version der Alexanderlegende in das iranische Nationalepos "Šāhnāmeh" ein, wobei er in manchen Einzelheiten von Pseudo-Kallisthenes abwich. Für ihn war Alexander ein „römischer Kaiser“ und Christ, der unter dem Kreuzeszeichen kämpfte; offenbar dachte er dabei an die byzantinischen Kaiser. Außerdem machte er – wie schon Ṭabarī, der persischer Abstammung war – Alexander zu einem Halbbruder des Dareios, womit er ihn für das Persertum vereinnahmte; aus der Vernichtung des Perserreichs wurde ein Bruderzwist innerhalb der iranischen Herrscherfamilie.

1191 schuf der persische Dichter Nezāmi das "Eskandar-Nāme" („Alexander-Buch“). Sein Alexander ist völlig islamisiert; er ist ein monotheistischer Held, der den Zoroastrismus der Perser mit Feuer und Schwert ausrottet und dafür den Beifall des Dichters erhält. Er unterwirft nicht nur Indien, sondern auch China und gelangt im Westen bis nach Spanien. Wie schon bei Firdausī sucht Alexander auch Mekka auf und reinigt dort die Kaaba. Außerdem ist er auch Philosoph und ein großer Förderer der Wissenschaft; er befiehlt den Gelehrten, das Wissen aller Völker zusammenzutragen. Das "Eskandar-Nāme" wurde zum Vorbild für einige spätere Dichtungen ähnlicher Art.

Die Handschriften der persischen Alexander-Bücher wurden trotz des islamischen Bilderverbots ab dem 14. Jahrhundert mit Buchmalerei geschmückt. In Nordindien sorgten die Mogul-Kaiser des 16. Jahrhunderts für die Bebilderung solcher Bücher.

Im Jahr 1390 verfasste der türkische Dichter Tāǧ ed-Dīn Ibrāhīm Aḥmedī das türkische Alexanderepos "Iskendernāme", die erste türkische Bearbeitung des Alexanderstoffs. Dafür bildete Nezāmis „Alexanderbuch“ die Grundlage, doch verfügte Aḥmedī auch über andere Quellen, aus denen er zusätzliches Sagenmaterial bezog. Sein Werk war im Osmanischen Reich lange berühmt und gelangte auch nach Iran und Afghanistan.

Die jüdische Alexanderrezeption war von dem Umstand geprägt, dass der Makedone schon in der Antike als Freund des jüdischen Volkes und Diener Gottes betrachtet wurde. In der mittelalterlichen hebräischen Alexanderliteratur floss Material aus unterschiedlichen Traditionen zusammen. Einerseits handelte es sich um Stoff aus dem griechischen Alexanderroman bzw. der "Historia de preliis", andererseits um einzelne Sagen jüdischer Herkunft (Verhalten Alexanders in Jerusalem, seine Schutzmaßnahme gegen Gog und Magog, sein Aufenthalt im irdischen Paradies und weitere Geschichten).

Die hebräische Überlieferung wurde nicht nur von der griechischen und lateinischen beeinflusst, sondern wirkte auch ihrerseits auf die westeuropäische Alexandersage ein. Weit verbreitet war in der lateinischsprachigen Welt eine von Petrus Comestor eingeführte Variante der Erzählung von Gog und Magog, wonach Alexander nicht die wilden Völker Gog und Magog, sondern die zehn jüdischen Stämme aussperrte, um sie für ihre Abwendung vom wahren Gott zu bestrafen.

Ins christliche Äthiopien gelangte der Alexanderroman auf dem Umweg über eine arabische Fassung. Der Stoff wurde für die Bedürfnisse eines geistlich orientierten Publikums stark umgestaltet. Alexander wird zu einem christlichen König, der den christlichen Glauben predigt. Er lebt keusch und ist ein Vorbild der Tugendhaftigkeit. Er stirbt wie ein Einsiedler, nachdem er sein Vermögen an die Armen verteilt hat. Durch diese besonders weitreichende Umarbeitung des Romans wird er zu einem Erbauungsbuch.

Petrarca behandelte in seinem Werk „Über berühmte Männer“ auch Alexander, wobei er sich an Curtius Rufus hielt, dessen negative Äußerungen er herausgriff; Positives verschwieg er.

Die außerordentliche Bekanntheit der Legendengestalt Alexander hielt auch in der Frühen Neuzeit an. So schrieb der Chronist Johannes Aventinus († 1534), es sei „kein Herr, kein Fürst unseren Leuten, auch dem gemeinen ungelehrten Mann, so bekannt“ wie Alexander. Andererseits drangen aber in der Renaissance die Humanisten zum historischen Alexander vor und taten die Alexandersage als Märchen ab. Die Wiederentdeckung griechischer Quellen (insbesondere Arrians), die im Mittelalter unbekannt waren, ermöglichte einen neuen Zugang zur Epoche Alexanders. Schon der Portugiese Vasco da Lucena, der 1468 am Hof Karls des Kühnen von Burgund die erste französische Übersetzung der Alexanderbiografie des Curtius Rufus anfertigte, übte scharfe Kritik an der Legende, in deren Übertreibungen und Wunderglauben er eine Verdunkelung der wahren historischen Leistung Alexanders sah.

1528/29 schuf der Maler Albrecht Altdorfer sein berühmtes Gemälde Die Alexanderschlacht. Charles Le Brun malte ab den frühen sechziger Jahren des 17. Jahrhunderts eine Reihe von Szenen aus Alexanders Leben für König Ludwig XIV.

Auf Dichter und Romanautoren übte die Gestalt Alexanders weiterhin eine starke Faszination aus. Ab dem 17. Jahrhundert handelt es sich allerdings großenteils um Werke, deren Handlung sich – ganz im Gegensatz zur traditionellen Alexandersage – um frei erfundene erotische Verwicklungen dreht und nur noch geringe Ähnlichkeit mit dem ursprünglichen Legendenstoff aufweist.

Hans Sachs schrieb 1558 eine "Tragedia von Alexandro Magno", die in sieben Akten die ganze Geschichte Alexanders darstellt. In Frankreich verfasste Jacques de la Taille 1562 die Tragödien "La Mort de Daire" und "La Mort d'Alexandre", und Alexandre Hardy wählte dieselben Titel für zwei seiner Tragödien ("La Mort d'Alexandre", 1621, und "La Mort de Daire", 1626). Im weiteren Verlauf des 17. Jahrhunderts folgten zahlreiche Tragödien und Tragikomödien, darunter Racines "Alexandre le Grand" (Uraufführung 1665). Noch intensiver war die Rezeption in italienischer Sprache. Antonio Cesti komponierte die Oper "Alessandro vincitor di se stesso" (Uraufführung Venedig 1651), Francesco Lucio ein „dramma musicale“ "Gl'amori di Alessandro Magno e di Rossane" (Libretto von Giacinto Andrea Cicognini, 1651); zahlreiche Dramen, Melodramen, Opern und Ballette folgten. Unter den Opern waren besonders erfolgreich "Alessandro Magno in Sidone" von Marc’Antonio Ziani (1679, Libretto von Aurelio Aureli), die „tragicommedia per musica“ "Alessandro in Sidone" von Francesco Bartolomeo Conti (1721, Libretto: Apostolo Zeno) und "Alessandro nell’Indie" von Leonardo Vinci (1729, Libretto: Pietro Metastasio) sowie vor allem "Alessandro" von Händel (Uraufführung in London 1726, Libretto von Paolo Antonio Rolli). Gluck verwertete Elemente des Alexanderstoffs sowohl in seiner Oper "Poro (Alessandro nell’India)" (Uraufführung: Turin 1744, Libretto von Metastasio) als auch in dem Ballett "Alessandro".

Zu Beginn des 17. Jahrhunderts schrieb in Spanien der Dichter Lope de Vega die Tragikomödie "Las grandezas de Alejandro".

Der englische Schriftsteller John Lyly schrieb die Komödie "Campaspe" (Uraufführung 1584), die auch unter dem Titel "Alexander and Campaspe" bekannt ist und von einem Aufenthalt Alexanders in Athen handelt. John Dryden dichtete 1692 die Ode "Alexander’s Feast", welche die Basis für das Libretto des 1736 vollendeten und uraufgeführten gleichnamigen Oratoriums von Georg Friedrich Händel (HWV 75) bildete.

In Griechenland wurde von 1529 bis ins frühe 20. Jahrhundert die Alexanderlegende in gedruckten Volksbüchern verbreitet, zunächst vorwiegend in Versform ("Rimada", 14 Drucke von 1529 bis 1805), ab dem 18. Jahrhundert meist in Prosa "(Phyllada)". Von insgesamt 43 Drucken der "Phyllada" aus dem Zeitraum von ca. 1680 bis 1926 erschienen 20 in der zweiten Hälfte des 19. Jahrhunderts.

Seit der Unabhängigkeitserklärung der früheren jugoslawischen Teilrepublik Mazedonien, der heutigen Republik Mazedonien, im Jahr 1991 knüpft der neue souveräne Staat demonstrativ an die Tradition des antiken Reichs Makedonien an und betrachtet dies als einen wesentlichen Aspekt seiner nationalen Identität. Von offizieller mazedonischer Seite wird behauptet, es gebe eine ethnische und kulturelle Kontinuität vom antiken Makedonien zum heutigen Mazedonien. Im Rahmen solcher Traditionspflege förderten mazedonische Behörden auch auf kommunaler Ebene die Verehrung Alexanders des Großen, was sich unter anderem in der Errichtung von Alexander-Denkmälern und in der Benennung von Straßen äußert. Im Dezember 2006 wurde der Flughafen von Skopje, der Hauptstadt Mazedoniens, nach Alexander benannt "(Aerodrom Skopje „Aleksandar Veliki“)"; dort wurde eine große Alexander-Büste aufgestellt. 2009 wurde die Errichtung einer zwölf Meter hohen Reiterstatue auf einem zehn Meter hohen Sockel im Zentrum von Skopje beschlossen, die Alexander nachempfunden war. Im Juni 2011 wurde dieser Beschluss, der in Griechenland Irritation auslöste, umgesetzt.

Von griechischer Seite wird die Behauptung einer kulturellen Kontinuität zwischen den antiken Makedonen und den heutigen Staatsbürgern der Republik Mazedonien nachdrücklich zurückgewiesen. Daher erscheint auch die mazedonische Alexander-Rezeption aus griechischer Sicht als Provokation, da die gesamte Alexander-Tradition ausschließlich ein Teil des griechischen kulturellen Erbes sei.

Im Februar 2018 beschloss die neue mazedonische Regierung angesichts von Fortschritten bei den Verhandlungen mit Griechenland zum mazedonischen Namensstreit, den Flughafen von Skopje und eine Autobahn, die den Namen „Alexander von Makedonien“ trug, wieder umzubenennen.

In der Moderne hat sich die Belletristik stärker als früher um Nähe zum historischen Alexander bemüht. Zu den bekannteren historischen Romanen aus der ersten Hälfte des 20. Jahrhunderts gehören "Alexander in Babylon" von Jakob Wassermann (1905), "Alexander. Roman der Utopie" von Klaus Mann (1929), der Alexander als gescheiterten Utopisten darstellt, und "Iskander" von Paul Gurk (1944). Weitere belletristische Darstellungen von Alexanders Leben stammen von Mary Renault, Roger Peyrefitte, Gisbert Haefs und Valerio Massimo Manfredi.

Arno Schmidt lässt in seiner Erzählung "Alexander oder Was ist Wahrheit" (2005) den Ich-Erzähler Lampon eine Wandlung vom Verehrer zum Gegner Alexanders durchmachen.

Den Ausgangspunkt der modernen wissenschaftlichen Auseinandersetzung mit Alexander bildete die 1833 erschienene „Geschichte Alexanders des Großen“ von Johann Gustav Droysen. Droysen betonte die aus seiner Sicht positiven kulturellen Folgen von Alexanders Politik einer „Völkervermischung“ statt einer bloßen makedonischen Herrschaft über unterworfene Barbaren. Er lobte die Wirtschaftspolitik, die Städtegründungen und die Förderung der Infrastruktur und meinte, auf religiösem Gebiet habe Alexanders Politik die Entstehung einer Weltreligion vorbereitet. Dieser Sichtweise war eine starke Nachwirkung beschieden. Im englischen Sprachraum war ihr Hauptvertreter im 20. Jahrhundert William W. Tarn, dessen 1948 erschienene Alexander-Biografie den Eroberer als Idealisten beschreibt, der eine zivilisatorische Mission erfüllen wollte.

Dieser Einschätzung, deren Grundidee schon bei Plutarch auftaucht, steht eine dezidiert negative Wertung gegenüber, welche Kernpunkte der antiken Alexanderkritik aufgreift. Die Vertreter dieser Richtung (siehe bereits die negative Charakterisierung durch Karl Julius Beloch sowie später Ernst Badian und ähnlich Fritz Schachermeyr, daran anschließend Albert B. Bosworth, Ian Worthington, Wolfgang Will) unterscheiden sich hinsichtlich der Gewichtung verschiedener Einzelaspekte. Grundsätzlich aber sehen sie in dem Eroberer Alexander primär einen Zerstörer, dessen Fähigkeiten sich auf Militärisches beschränkten. Politisch sei er an seinen Fehlern gescheitert. Er habe impulsive, irrationale Entscheidungen getroffen und sich mit den Säuberungen unter seinen Vertrauten und Offizieren schließlich in die Isolation manövriert, da er niemandem mehr vertrauen konnte.

Die militärischen Leistungen Alexanders, die früher einhellige Anerkennung fanden, werden von den modernen Kritikern relativiert; so charakterisiert Badian den Rückmarsch aus Indien als eine von Alexander verschuldete militärische Katastrophe. Waldemar Heckel hingegen hob in jüngerer Zeit Alexanders strategische Fähigkeiten hervor und wandte sich zugleich gegen ein romantisierendes Alexanderbild. Vor einer überzogenen Kritik, wodurch sozusagen das Pendel von der Heldenverehrung Alexanders in das andere Extrem umzuschlagen droht, warnte etwa Frank L. Holt, der diesen Trend als „new orthodoxy“ bezeichnete.

Neben diesen stark wertenden Darstellungen stehen Untersuchungen vor allem aus neuerer und neuester Zeit, deren Autoren von vornherein darauf verzichten, die Persönlichkeit Alexanders zu erfassen, ein Werturteil über sie abzugeben und seine verborgenen Motive zu erkunden (was aufgrund der Quellenlage sehr schwierig ist, worauf u. a. Gerhard Wirth hingewiesen hat). Diese Forscher untersuchen vielmehr Alexanders Selbstdarstellung, deren Wandel und die sich daraus ergebenden politischen Folgen.









</doc>
<doc id="108" url="https://de.wikipedia.org/wiki?curid=108" title="Antike">
Antike

Antike (von „alt, altertümlich, altehrwürdig“) bezeichnet eine Epoche im Mittelmeerraum, die etwa von 800 v. Chr. bis ca. 600 n. Chr. reicht, wobei der Beginn teilweise noch deutlich früher angesetzt wird. Die Antike unterscheidet sich von vorhergehenden und nachfolgenden Epochen durch gemeinsame und durchgängige kulturelle Traditionen. Sie umfasst die Geschichte des antiken Griechenlands, des Hellenismus und des Römischen Reichs. Insbesondere das Römische Reich vereinte den Mittelmeerraum seit dem 1. Jahrhundert n. Chr. politisch und kulturell.

In einem erweiterten Sinne umfasst die Antike auch die Geschichte der altorientalischen nahöstlichen Hochkulturen Ägyptens, Mesopotamiens, Assyriens, Persiens und Kleinasiens, die etwa mit dem Beginn der Schriftlichkeit um 3500 v. Chr. einsetzt. Der größere Zeitraum von etwa 3500 v. Chr. bis zum Ende der Antike wird bevorzugt als Altertum bezeichnet. Die darauffolgende Epoche ist das Mittelalter (mit einem breiten, regional unterschiedlichem Übergangszeitraum, siehe Spätantike und Frühmittelalter).

Im Sinne der klassischen Altertumswissenschaften bezeichnet der historische Begriff "Antike" meist die Zeit von der allmählichen Herausbildung der griechischen Staatenwelt bis zum Ende des weströmischen Reichs im Jahr 476 bzw. bis zum Tod des oströmischen Kaisers Justinian 565. Seit den Arbeiten des belgischen Historikers Henri Pirenne wird oft auch das Jahr 632, also der Tod Mohammeds und die darauf folgende islamische Expansion, als Datum für das Ende der Antike vorgeschlagen.

Der Anfang der antiken griechisch-römischen Kultur im klassischen Sinne wird im Allgemeinen mit der Entstehungszeit der Homerischen Epen und dem Beginn der griechischen Kolonisation des Mittelmeerraums im 8. Jahrhundert v. Chr. angesetzt. Die Griechen verbreiteten ihre Kultur in den folgenden Jahrhunderten im gesamten Mittelmeerraum und an den Küsten seiner Nebenmeere und seit Alexander dem Großen auch im Orient und nach Zentralasien hinein. Die Römer brachten die antike Zivilisation bis nach Mittel- und Nordwesteuropa, wo sie sich seit dem Frühmittelalter zur christlich-abendländischen Kultur wandelte.

Je nach Forschungsrichtung werden aber auch die minoische und mykenische Kultur von etwa 1900 bis 1100 v. Chr. sowie die so genannten „Dunklen Jahrhunderte“ 1200 bis 750 v. Chr. zur Antike gerechnet.

Auch zwischen Antike, Völkerwanderung und Mittelalter lässt sich – wie bei allen Periodisierungen in der Geschichtswissenschaft – keine für alle Regionen, staatlichen und kulturellen Traditionen gültige Trennlinie ziehen. Je nach Betrachtungsweise sind unter anderem folgende Jahre als Epochengrenzen zwischen der Spätantike und Frühmittelalter vorgeschlagen worden:

In der neueren Forschung wird meistens ein später Zeitpunkt favorisiert (565 bzw. die Zeit um 600 n. Chr.). Generell erscheint es ohnehin sinnvoll, von einem Übergangszeitraum ab ca. 500 bis ca. 600 n. Chr. auszugehen, anstatt feste Daten zu wählen.

Der Begriff Antike wurde lange Zeit räumlich mit der griechischen, hellenistischen und später römischen Welt gleichgesetzt. Über diese Definition, die durch die Klassische Altertumswissenschaft geprägt wurde, geht der universalhistorische Antike-Begriff hinaus, der unter anderem von dem Historiker Eduard Meyer im 19. Jahrhundert gefordert wurde. In jüngerer Zeit wurde er von dem deutschen Althistoriker Josef Wiesehöfer wieder aufgegriffen. Die Mehrheit der heutigen Forscher ordnet jedoch den Alten Orient und das alte Ägypten zwar dem „Altertum“, nicht aber der „Antike“ zu.

Die Ursprünge der europäischen Antike liegen im Dunkeln. Ihre Vorgeschichte ist etwa in der Zeit von ca. 2000 bis ca. 1600 v. Chr. im Mittelhelladikum anzusiedeln. Zu Beginn dieses Zeitabschnitts – teils auch schon im letzten Abschnitt des Frühhelladikums FH III ca. 2200–2000 v. Chr. – wanderten wahrscheinlich indogermanische Stämme, von Norden kommend, in Griechenland ein. Offenbar unter dem Einfluss der minoischen Kultur auf Kreta, der ersten Hochkultur Europas, die ihre Blüte von ca. 1900 bis 1450 v. Chr. hatte, entwickelte sich auf dem Festland aus der Kultur des Mittelhelladikums die mykenische Kultur (ca. 1600 bis 1050/00 v. Chr.). Sie hatte ihren Ausgangspunkt vermutlich in der Argolis und erscheint unvermittelt mit reichen Schachtgräbern ab ca. 1600 v. Chr. Unter anderem übernahm die mykenische Kultur von der minoischen die Schrift. Die auf Kreta (unter anderem) verwendete sog. Linear A-Schrift des 17. bis 15. Jahrhunderts v. Chr. wurde zur sog. Linear B-Schrift (15. bis 12. Jahrhundert v. Chr.) weiterentwickelt. Dieser begegnet man auf zahlreichen Tontäfelchen unter anderem der Paläste in Pylos, Theben, Mykene auf dem griechischen Festland und in den zu jener Zeit mittlerweile mykenisch beherrschten Zentren Kydonia und Knossos auf Kreta.
Bekannt sind die prächtigen Zentren der mykenischen Kultur. Zu den bedeutenden Fundorten gehören Mykene, Pylos und Tiryns auf der Halbinsel Peloponnes, Orchomenos und Gla (letzteres kein Palastzentrum) in Boiotien sowie das stark mykenisch geprägte Milet in Westkleinasien. Die Zentren hatten Oberstädte (Akropolen), Burgen genannt, die im 13. Jahrhundert v. Chr. in einigen Fällen stark befestigt bzw. deren Befestigungen stark ausgebaut wurden (Mykene, Tiryns, Athen). Reiche Kuppelgräber, feine, teils reich bemalte Keramik, kunstvolle Gold-, Silber- und Fayence-Arbeiten usw. zeugen vom Reichtum und von der Spezialisierung des Wirtschaftssystems, das in Teilen Griechenlands ab ca. 1400 v. Chr. von mächtigen Palastzenten, die größere Regionen beherrschten, zentral gesteuert wurde (so in Böotien, Attika, Messenien und in der Argolis; s. dazu auch Mykenische Palastzeit). Intensive Handelskontakte wurden mit dem Nahen Osten, Assyrien und Ägypten gepflegt. Mykenische Keramik war in weiten Teilen des Mittelmeergebiets beliebt; möglicherweise ließen sich in manchen Siedlungen Süditaliens (Roca Vecchia, Punta Meliso, Scoglio del Tonno) sogar Handwerker nieder.

Etwa für den Zeitraum 1200 bis 750 v. Chr. setzt man traditionell das "Dunkle Zeitalter" an, aus dem vergleichsweise wenig überliefert ist. Zu Beginn dieser Phase wurden viele der Zentren des griechischen Festlands zerstört, womit die Grundlage der Palastkultur unterging. Die mykenische Kultur bestand jedoch noch etwa 150 Jahre weiter, erlebte in einigen Regionen ab Mitte des 12. Jahrhunderts sogar eine gewisse Nachblüte, bevor der Übergang in die sogenannte Protogeometrische Periode (ca. 1050/00–900 v. Chr.) erfolgte. Ungefähr zur gleichen Zeit, als sich um 1200 v. Chr. in Griechenland - und auch an anderen Regionen des östlichen Mittelmeerraums (s. auch Ende des Hethiterreichs, Seevölker) - Zerstörungen und Umwälzungen ereigneten, entstanden auf Zypern und einigen Orten Südkleinasiens (z. B. Tarsus und Mersin) mykenisch geprägte Siedlungen. Westhandel, speziell mit Italien und Sardinien, wurde auch im 12. Jahrhundert v. Chr. weiterhin betrieben, teilweise auch noch im 11. Jahrhundert v. Chr. Der Überlieferung nach setzte ca. 1050 v. Chr. die sehr umstrittene "Ionische Wanderung" ein, in deren Verlauf die Einwohner des griechischen Festlandes die Inseln der Ägäis und die Westküste Kleinasiens kolonisierten. Auf dem griechischen Festland bietet sich ein diffuses Bild: Wenige Siedlungen wurden bisher entdeckt und die meisten machen einen – im Vergleich zur mykenischen Zeit – ärmlichen Eindruck. Ganz anders hingegen Lefkandi auf Euböa: dort wurden neben einer Siedlung mit einem großen Gebäude des Fürsten von Lefkandi Gräber gefunden, die sehr reich ausgestattet waren.

Das Dunkle Zeitalter hellt sich in den letzten Jahrzehnten – dank vieler neuer Funde, vor allem, aber nicht nur, aus der mykenischen Spätphase des 12./11. Jahrhunderts v. Chr. – immer mehr auf. Nach Annahme der Homer-Forschung spiegeln unterschiedliche Passagen der Ilias die Verhältnisse dieser Zeit wider. Sie war offenbar auch für die Entwicklung der griechischen Gesellschaft zur Polis hin wichtig. Ab dem 8. Jahrhundert waren die Kontakte zum Vorderen Orient wieder sehr intensiv, und es entstanden Handelsstationen auf Zypern (Kition) und in Syrien (Al Mina). Vermutlich bereits im späten 9. Jahrhundert v. Chr. hat man von den Phöniziern das Alphabet vermittelt bekommen.

Mit dem so genannten archaischen Zeitalter begann im frühen 8. Jahrhundert v. Chr. die eigentliche Antike. Seit dem Jahr 776 v. Chr. ist die Siegerliste der Olympischen Spiele überliefert. Von etwa 770 bis 540 v. Chr. breiteten sich die Griechen während der Großen Kolonisation im westlichen Mittelmeer (vor allem Sizilien und Unteritalien, siehe auch Magna Graecia, und bis Marseille), an der nördlichen Ägäis und am Schwarzen Meer aus. In Kleinasien waren Griechen bereits vorher ansässig. In dieser Zeit (etwa zwischen 750 und 650 v. Chr.) wurden vermutlich auch die Homerischen Epen ("Ilias" und "Odyssee") schriftlich fixiert, die ältesten Literaturdenkmäler des Abendlands. Die ältesten tatsächlich erhaltenen Papyrusfragmente dieser Texte stammen aus dem 3. Jahrhundert v. Chr., die ältesten Codices mit längeren Textpassagen tauchen im Mittelalter (ca. 10. Jahrhundert n. Chr.) auf, wie generell der Großteil der erhaltenen antiken Literatur vor allem in mittelalterlichen Handschriften überliefert ist. Hesiod wirkte ebenfalls etwa in der Zeit um 700 v. Chr.

Zugleich bildete sich das System der griechischen Stadtstaaten, der Poleis, heraus, wobei diese in der Mehrzahl nur eine sehr kleine Bevölkerung umfassten. Der werdende Militärstaat Sparta im Süden der Peloponnes unterwarf zwischen 720 und 600 v. Chr. Messenien und kontrollierte damit den gesamten südwestlichen Teil der Halbinsel. Die Stadt mit ihrer oligarchischen Verfassung kann als das erste Beispiel für die fortan herrschende Polis-Struktur gelten.

Auch in vielen anderen griechischen Stadtstaaten regelten Verfassungen das Zusammenleben der Bürger, aber auch die Tyrannis, wie sie um 650 v. Chr. beispielsweise in Korinth und Megara bestand, war keine Seltenheit. In Athen bildete sich unter wechselnden Voraussetzungen schließlich ein demokratisches System heraus. Nach den Gesetzgebungen Drakons (621 v. Chr.) und Solons (594/593 v. Chr.) gelang es Peisistratos und seinen Söhnen etwa zwischen 561 und 510 v. Chr. zwar noch einmal, eine Tyrannis zu errichten. Bis 501 v. Chr. brachten die Reformen des Kleisthenes von Athen aber den Durchbruch für die Attische Demokratie.

Mit Athens Unterstützung der kleinasiatischen Griechenstädte im Ionischen Aufstand um 500 v. Chr. begann ein annähernd zweihundertjähriger Konflikt mit dem Perserreich, zunächst in Gestalt der drei Perserkriege, die der Historiker Herodot, der „Vater der Geschichtsschreibung“ (mit ihm lässt man traditionell die griechische Geschichtsschreibung beginnen, vgl. "Liste der griechischsprachigen Geschichtsschreiber der Antike"), in seinen "Historien" geschildert hat, wenngleich nicht immer zuverlässig. Als die Perser zu einer Strafexpedition in Griechenland einfielen, wurden sie 490 v. Chr. von den Athenern in der Schlacht bei Marathon besiegt. Zehn Jahre später unterlag der persische Großkönig Xerxes I. der athenischen Flotte unter Themistokles in der Schlacht von Salamis und 479 v. Chr. den vereinigten Heeren der griechischen Poleis in der Schlacht von Plataiai. Die Perser waren vorerst zurückgedrängt, die griechischen Stadtstaaten in Kleinasien aus der Abhängigkeit befreit.
Nach der erfolgreichen Verteidigung und mit der Gründung des Attischen Seebunds 477 v. Chr. unter der auf die eigene Seemacht gestützte Vorherrschaft Athens setzte eine etwa 50-jährige Blütezeit der Stadt (die Pentekontaetie) ein, die bis zum Ausbruch des Peloponnesischen Krieges 431 v. Chr. (bzw. bis zum Tod des leitenden Staatsmannes Perikles im Jahr 429 v. Chr.) reichte. Die Akropolis mit dem Parthenon­tempel wurde damals unter der Regie des Phidias zum glanzvoll-repräsentativen Zentrum der Seemacht Athen ausgebaut. Die klassischen Tragödien von Aischylos, Sophokles und Euripides kamen – meist im Rahmen festlicher Dichterwettbewerbe – im Theater zur Aufführung. Kaufleute und Gewerbetreibende, Künstler und Gelehrte zog die Metropole an. Auf der Agora wirkte neben den Sophisten der Philosoph Sokrates auf seine Mitbürger ein, dessen Lehren Platon später zu einem Werk von herausragender philosophie­geschichtlicher Bedeutung verarbeitete. Athen mit seinen zu gleichberechtigter politischer Mitwirkung gelangten (männlichen) Vollbürgern beanspruchte nunmehr, die „Schule von Hellas“, zu sein. Seine durchaus auch aggressive äußere Machtentfaltung in und mit dem Attischen Seebund führte allerdings schon während der Pentekontaetie zu Spannungen, vor allem gegenüber der konkurrierenden griechischen Großmacht Sparta.

Die zunehmende Rivalität zwischen der Seemacht Athen und der Landmacht Sparta mündete 431 v. Chr. in den fast 30 Jahre währenden Peloponnesischen Krieg, den die zeitgenössischen Historiker Thukydides und (im Anschluss an Thukydides) Xenophon eindringlich beschrieben haben. Der sehr wechselhaft verlaufende und mit einer als beispiellos empfundenen Brutalität geführte Konflikt endete, auch auf Grund der Unterstützung Spartas durch das Perserreich, 404 v. Chr. mit der vollständigen Niederlage Athens und mit der Errichtung einer zeitweiligen spartanischen Hegemonie über Griechenland.

In der ersten Hälfte des 4. Jahrhunderts v. Chr. führten die griechischen Städte einen fast permanenten Krieg gegeneinander und in wechselnden Koalitionen, auch unter fortwährender Einmischung der Perserkönige. Die Sehnsucht nach einem Allgemeinen Frieden wurde auch zu propagandistischen Zwecken eingesetzt (Königsfrieden von 386 v. Chr.). 371 v. Chr. löst Theben unter Epaminondas nach der Schlacht bei Leuktra Sparta als Hegemon ab. Doch auch Thebens Vorherrschaft bestand nur bis rund 362 v. Chr. und endete mit dem Tod Epaminondas.

Insgesamt schwächte der Peloponnesische Krieg die griechischen Polis so stark, dass Philipp II. von Makedonien dem andauernden Machtkampf ein Ende setzen konnte, indem er Griechenland gewaltsam mit seinem hervorragend geschulten Heer einigte. Der von Athenern wie Demosthenes als nicht-griechischer Barbar betrachtete König errang mit seinem geschulten Heer in der Schlacht von Chaironeia 338 v. Chr. die Hegemonie über Hellas, die im Jahr darauf im Korinthischen Bund bekräftigt wurde.

Auf Sizilien behauptete sich derweil das mächtige Syrakus gegenüber der Handelsrepublik Karthago, welche mit den griechischen Poleis "(Westgriechen)" seit dem frühen 5. Jahrhundert v. Chr. im Konflikt lag. Auf Sizilien hielt sich zudem, im Gegensatz zum Mutterland, in vielen Städten die Tyrannis als Regierungsform (Dionysios I. von Syrakus, Agathokles von Syrakus und andere).

Nach der Ermordung Philipps 336 v. Chr. führte sein Sohn Alexander der Große ein griechisch-makedonisches Heer nach Asien und eroberte in wenigen Jahren mit dem Perserreich ein Weltreich. Der Alexanderzug bahnte der griechischen Kultur im ganzen damals bekannten Orient den Weg, von Ägypten über Mesopotamien und Persien bis zu den Grenzen Indiens und Turkestans. Nach Alexanders Tod 323 v. Chr. in Babylon teilten seine Nachfolger, die Diadochen, in lange währenden Kriegen das Reich unter sich auf. In allen Teilreichen bildete der Hellenismus in den folgenden Jahrhunderten die prägende Kultur.

Das Zeitalter des Hellenismus kennzeichnet ein nahezu ständiger Kampf der drei Großmächte (Ptolemäer, Seleukiden und Antigoniden) um die Vorherrschaft. Dennoch wuchs die Bevölkerung im gesamten Mittelmeerraum stetig und ermöglichte so das Wachstum größerer Städte und Metropolen mit Einwohnern über 100.000 Menschen. Auch breitete sich in dieser Zeit der Fernhandel (bis hin nach China) und die Güterproduktion für große städtische Märkte aus. Verschiedene Wissenschaften blühten auf, bspw. in Alexandria. Zu Beginn des 2. Jahrhunderts v. Chr. tauchte erstmals Rom als bedeutende Macht in Griechenland auf und dehnte nach und nach seinen Einfluss aus. 146 v. Chr. unterstellte das Römische Reich die Mitglieder des unterlegenen Achaiischen Bundes der Provinz Macedonia; Korinth als führende Polis wurde zerstört. Doch blieben viele Poleis wie Athen und Sparta zumindest vorerst formell unabhängig.

Bald darauf folgte der Erwerb Pergamons durch Rom und 64/63 v. Chr. die Beseitigung der Überreste des Seleukidenreiches. Als letzter Nachfolgestaat des Alexanderreichs wurde im Jahre 30 v. Chr. das ptolemäische Ägypten, dessen letzte Herrscherin Kleopatra VII. war, ins Römische Reich eingegliedert. Damit war die hellenistische Staatenwelt als machtpolitischer Faktor ausgelöscht. Die griechische Kultur lebte jedoch im Römischen Reich sowie später im Byzantinischen Reich fort.

Nach den Griechen wurden die Römer zu den zweiten Trägern und Vermittlern der antiken Kultur und prägten diese für mehrere hundert Jahre. Je weiter sie als Eroberer in außeritalische Länder vordrangen, desto stärker ließen sie sich von deren Kultur inspirieren und beeinflussen. Sie adaptierten teilweise lokale Gebräuche. Literatur, Philosophie, Kunst, Architektur und Alltagskultur der Griechen und der Länder der Levante, Waffentechniken der Gallier oder Germanen und religiöse Einflüsse aus Ägypten wurden von den Römern aufgenommen. Nicht zuletzt durch die kulturelle Ausstrahlung und Heterogenität der Stadt Rom, die sich in der römischen Kaiserzeit zur Millionenstadt entwickelte, wurden solche Einflüsse im Imperium verbreitet.

Rom, der Legende nach 753 v. Chr. gegründet, entstand neueren Forschungen zufolge erst gegen Ende des 7. Jahrhunderts v. Chr. aus dem Zusammenschluss mehrerer dörflicher Siedlungen an einer Furt am Unterlauf des Tibers. Politisch und kulturell stand Rom lange unter etruskischem Einfluss. Die Etrusker wiederum unterhielten schon früh Kontakt mit griechischen Kolonisten.

Um 500 v. Chr. befreiten sich die Römer vom etruskischen Stadtkönigtum und bildeten wohl um 475 v. Chr. eine republikanische Regierungsform aus. In den Zwölftafelgesetzen, die um 450 v. Chr. entstanden, wurden die ersten zivil-, straf- und prozessrechtlichen Normen des römischen Rechts festgehalten. Die Verfassung sah von da an ein Zusammenwirken der drei Institutionen Senat, Magistratur und Volksversammlung vor, die sich in ihrer Macht theoretisch gegenseitig beschränkten. Die offizielle Bezeichnung der Republik lautete "S.P.Q.R." für "Senatus Populusque Romanus" (dt.: Senat und Volk von Rom). Machtpolitisch dominierte der Senat, der sich aus Angehörigen der adligen Familien, der Patrizier zusammensetzte und später der plebejischen Patrizier. Aus ihm gingen auch die Konsuln hervor, die beiden auf ein Jahr gewählten obersten Magistrate der Republik. Das (zunächst) höchste den nichtadeligen Plebejern zugängliche Amt war das des Volkstribunen, der ein Veto­recht gegen Senatsbeschlüsse besaß.
Mit der Legion entwickelten die Römer eine effektive Streitmacht. Bis zum Jahr 272 v. Chr. unterwarfen sie ganz Italien südlich der Poebene. Mit den Punischen Kriegen gegen die Seemacht Karthago im 3. und 2. Jahrhundert v. Chr. begann der Aufstieg Roms zur antiken Weltmacht, die für die folgenden Jahrhunderte die gesamte Mittelmeer­welt beherrschen sollte. Nach 200 v. Chr. nahm Rom zunehmend Einfluss auf die Politik der hellenistischen Großmächte und wurde zur Protektoratsmacht im östlichen Mittelmeerraum. 148 v. Chr. wurde das Makedonien der Antigoniden, 63 v. Chr. das Reich der Seleukiden, und schließlich 30 v. Chr. das Ägypten der Ptolemäer römische Provinz.

Die Römische Republik ermöglichte durch die Herstellung von innerem Frieden ein weiteres, kontinuierliches Bevölkerungswachstum, auch durch die ständige Neugründung von Kolonien in eroberten Ländern. Durch die Ansiedlung von Veteranen aus den Legionen vorheriger Kriege konnte die Republik zudem einen verlässlichen Einfluss in diesen Ländern gewinnen und gleichzeitig mit einem stetigen Bevölkerungszuwachs neue Gebiete kultivieren. Handel und Verkehr konnten auch dank der Römerstraßen zunehmen, welche zunächst häufig aus militärischen Gründen angelegt wurden und die wachsenden Reichsstädte und Kolonien miteinander verbanden. Entlang der Straßen entwickelten sich Streckenposten und Marktflecken zu Städten. Mit diesen infrastrukturellen Neuerungen ging im Reich ein Wachstum der wirtschaftlichen Produktion und somit auch der verfügbaren Steuermittel einher.

Mit dem Wachstum der Republik an Größe, Macht und Wohlstand kam es jedoch im Inneren zu einer Reihe von Krisen, in denen sich der Kampf der an den überkommenen sozioökonomischen Strukturen festhaltenden Optimaten gegen die auf Reformen drängenden Popularen spiegelte. In der Epoche der Bürgerkriege erreichte die Krise der späten Römischen Republik ihren Höhepunkt und es zeichnete sich ab, dass die Republik als inhaltlich gelebte Staatsform die Erfolge nicht mehr meistern konnte, die sie gezeitigt hatte: So wurde der Prinzipat möglich, also die Umwandlung der Republik in eine Monarchie mit republikanischer Fassade. Bereits Gaius Iulius Caesar hatte als Diktator auf Lebenszeit "(dictator perpetuus)" eine quasi-monarchische Stellung erlangt. Als erster römischer Kaiser gilt jedoch sein Großneffe und Erbe Augustus, dem es gelang, mit dem Prinzipat eine dauerhafte monarchische Staatsordnung an die Stelle der zerrütteten Republik zu setzen (wobei jedoch die entmachteten Staatsorgane der Republik, z. B. der Senat, formal noch lange fortbestanden).

Das von Augustus errichtete Kaisertum (Prinzipat) wurde von ihm und seinem Nachfolger Tiberius für rund 60 Jahre sicher geführt. Augustus bewahrte noch bewusst eine republikanische Fassade, während unter Tiberius das Kaisertum zur Normalität wurde. Unter Caligula, Claudius und Nero traten jedoch zeitweilig Zerfallserscheinungen auf. Nach dem Krisenjahr 68/69 (Vierkaiserjahr) traten die Flavier (Vespasian, Titus, Domitian) die Regierung an, die sowohl außen- als auch innenpolitisch insgesamt recht erfolgreich herrschten. Nach der Ermordung Domitians, der 96 einer Verschwörung zum Opfer fiel, folgte eine weitere kurze Krise des Herrschaftssystems, die jedoch unter den so genannten Adoptivkaisern weitgehend behoben werden konnte.

Das Imperium erlebte seine größte Blüte und Ausdehnung dann auch unter ebendiesen „Adoptivkaisern“ (das Kaisertum war auch weiterhin formal nicht erblich) in der ersten Hälfte des 2. Jahrhunderts: Einer Expansion unter Trajan (vor allem im Balkanraum und im Osten gegen das Partherreich) folgte eine Rücknahme und Sicherung der Grenzen unter Hadrian. Bald nach der Mitte des 2. Jahrhunderts n. Chr. wuchs jedoch der Druck auf die ausgedehnten Reichsgrenzen. Im Norden und Nordosten bedrängten die Germanen, im Osten die Parther (die sich trotz mancher Niederlage behaupten konnten) das Reich. Mark Aurel, der „Philosophenkaiser“ im Geiste der Stoa, sah sich bald nach Übernahme der Herrschaft nahezu ständig zur kriegerischen Verteidigung der Reichsgrenzen genötigt. Mit seinem Tod endete 180 n. Chr. ein als Blütezeit betrachtetes Zeitalter des Imperiums.

Nach dem schwachen Commodus, der 192 ermordet wurde, stabilisierten die Kaiser aus dem Hause der Severer, hervorzuheben ist besonders Septimius Severus, die Grenzen wenigstens teilweise. Kaiser Caracalla gewährte 212 mit der Constitutio Antoniniana allen freien Reichsbürgern das Bürgerrecht. Nach der Ermordung des Severus Alexander 235 kam es jedoch unter den so genannten Soldatenkaisern zur Reichskrise des 3. Jahrhunderts, die aber erst um 260 ihren Höhepunkt erreichte. Dieser Zeitraum war geprägt von raschen Regierungswechseln, zeitweiligen und regional unterschiedlichen ökonomischen Problemen, zentrifugalen Tendenzen im Inneren (zeitweilige Abspaltung des "Imperium Galliarum"; Verlust mehrerer Provinzen an Palmyra) und dem stetig wachsenden Druck auf die Grenzen. Neben den verschiedenen Germanenstämmen (wie den Alamannen und Goten), übte nun vor allem das Sassanidenreich im Osten einen enormen Druck aus: Nach dem Sturz des letzten Partherkönigs im Jahr 224 (bzw. 226), erneuerten die Sassaniden das Perserreich und erwiesen sich in der Regel als den Römern gleichwertige Gegner, wenngleich auch sie mit einer gefährdeten Grenze konfrontiert waren (im Nordosten, siehe Iranische Hunnen). Die Zeit der Soldatenkaiser wird allerdings in der neueren Forschung keineswegs mehr als eine reine Krisenzeit begriffen, sondern vielmehr als eine (wenngleich teils von Krisensymptomen begleiteten) Transformationsphase.
Mit der Einführung der Tetrarchie (293) und zahlreichen inneren Reformen gelang es Kaiser Diokletian (seit 284 Kaiser) gegen Ende des 3. Jahrhunderts noch einmal, das Reich zu stabilisieren. Diese Zeit der beginnenden Spätantike ist gekennzeichnet von Umbrüchen, die zum Teil eine Abkehr von bis dahin wesentlichen Bestandteilen der antiken Kultur darstellten. Dazu gehört vor allem die von Kaiser Konstantin I. initiierte Anerkennung und Privilegierung des Christentums, das unter Diokletian noch verfolgt worden war. Die Hinwendung zu dem neuen Glauben ging schließlich mit der Ablehnung des religiösen Pluralismus der Antike einher. Ein letzter Versuch, die alten Kulte durch die Verbindung mit neuplatonischem Gedankengut wieder zu beleben, scheiterte mit dem Tod Kaiser Julians im Jahr 363; alle nachfolgenden Kaiser waren Christen. Teilweise stießen auch bestimmte Formen der Philosophie auf Ablehnung, wenngleich das Christentum nun selbst stark von der griechischen Philosophie geprägt wurde und zwischen 300 und 600 eine massive Transformation durchlief, bspw. mit dem Ersten Konzil von Nicäa. Die Platonische Akademie in Athen, oft als „Hort des Heidentums“ bezeichnet, wurde 529 geschlossen, während die bereits christianisierte Schule von Alexandria noch bis zum Beginn des 7. Jahrhunderts bestehen blieb.

Kaiser Valentinian I. festigte den Westen des Reiches, doch kam es 378 unter seinem Bruder Valens zur Niederlage von Adrianopel und zu einer neuen Krise. Kaiser Theodosius I. wiederum konnte den Osten des Reiches stabilisieren und war zugleich der letzte Kaiser, der "de facto" über das gesamte "Imperium Romanum" herrschte. Er erklärte das Christentum schließlich 392 zur Staatsreligion und verbot alle heidnischen Kulte wie die Olympischen Spiele. Allerdings lassen sich noch bis mindestens in das 6. Jahrhundert hinein bedeutende heidnische Minderheiten auf dem Boden des Imperiums nachweisen.
Nach der faktisch endgültigen Teilung des Reiches unter den beiden Söhnen des Theodosius 395 erwies sich letztlich nur das von Konstantinopel, dem früheren Byzantion, aus regierte Oströmische Reich auf die Dauer eines weiteren Jahrtausends als lebensfähig. Es bewahrte viele antike Traditionen; unter anderem blieb das Lateinische in dem überwiegend griechischsprachigen Reich noch bis ins 7. Jahrhundert Amtssprache. Das so genannte Weströmische Reich hingegen zerbrach aufgrund endloser innerer Kriege, gepaart mit äußerem Druck (siehe Völkerwanderung). Germanische Kriegerverbände traten an die Stelle der kollabierenden Reichsregierung und ergriffen, zunächst als "foederati", seit dem 5. Jahrhundert direkt Besitz von weströmischen Provinzen. Ihre Anführer traten hier oft an die Stelle der römischen Autoritäten. Rom selbst wurde 410 von den Westgoten und 455 von den Vandalen geplündert, von der Millionenstadt der hohen Kaiserzeit schrumpfte sie auf schätzungsweise 200.000 Einwohner zum Ende des 5. Jahrhunderts.

Die Spätantike sah auch das langsame Verschwinden der klassisch-antiken Stadt ("polis" bzw. "civitas"). In der Forschung ist umstritten, ob es sich hierbei um einen Niedergang oder eher um einen Wandel handelt – diese Frage stellt sich auch für viele andere Aspekte der Epoche (z. B. im wirtschaftlichen Bereich, wobei viele Provinzen weiterhin aufblühten). Im Westen (das Ostreich war davon nicht betroffen und durchlief erst im 7. Jahrhundert eine Krisenzeit, siehe unten) lösten sich im 5. Jahrhundert zunehmend die politischen Strukturen auf, während das reguläre Heer (zumindest nach Ansicht der älteren Forschung) immer stärker „barbarisiert“ wurde und die Bedeutung der nichtrömischen "foederati" besonders im Westen immer mehr zunahm. Die geringer werdenden Steuereinnahmen durch den Verlust von Provinzen und Steuermitteln führten dazu, dass die Regierung in Ravenna immer hilfloser wurde; die kaiserliche Autorität schwand dahin, während die eigentliche Macht nun meist bei hohen Militärs wie Aetius oder Ricimer lag, die gegeneinander oft blutige Bürgerkriege führten und das Westreich so weiter schwächten.

476 setzte der General Odoaker, der Kommandeur der föderierten Truppen in Italien, dann den letzten Westkaiser Romulus Augustulus ab, da dieser überflüssig geworden sei, und unterstellte sich der nominellen Oberherrschaft des oströmischen Kaisers. Die Geschichtswissenschaft sah in diesem von den Zeitgenossen nur wenig beachteten Akt früher oft das Ende der Antike. Heute wird dagegen auch das 6. Jahrhundert noch zur Antike gezählt, da vor allem im Osten römisch-antike Strukturen fortbestanden und dem oströmischen Kaiser Justinian (527–565) für kurze Zeit noch einmal eine Rückeroberung großer Teile des Westreiches gelang. Dass diese letztlich dennoch scheiterte, hatte auch mit dem Druck zu tun, den die Sassaniden seit 540 erneut auf die Ostgrenze des Reiches ausübten (siehe auch Römisch-Persische Kriege und Herakleios). Im Oströmischen Reich lebten antike Kultur und Geisteswelt zwar noch bis weit ins Mittelalter fort. Die islamische Expansion des 7. Jahrhunderts führte allerdings auch hier zu erheblichen Veränderungen und gilt als der entscheidende Einschnitt, der das Ostrom der Spätantike vom Byzantinischen Reich des Mittelalters trennt.

Antike Traditionen hatten starke und prägende Auswirkungen auf den weiteren Verlauf der Weltgeschichte, insbesondere auf die Entwicklung der westlichen Welt, die in der Antike ihre Wurzeln hat. Neuzeitliche Aufklärer, Philosophen, Staatstheoretiker, Wissenschaftler, Künstler und andere knüpften immer wieder an die Ionische Naturphilosophie, die attische Demokratie, das römische Recht, den religiösen Pluralismus, das antike Schönheitsideal und andere Hinterlassenschaften der Antike an.

Antike Traditionen gerieten auch im Mittelalter nie völlig in Vergessenheit. In den Klöstern des Abendlandes wurde umfangreiches antikes Schriftgut bewahrt. Auch die Romidee blieb im Heiligen Römischen Reich lebendig. Im 8. Jahrhundert kam es zur ersten, sogenannten Karolingischen Renaissance. Auch byzantinische und arabische Gelehrte stützten sich auf antikes Wissen und gaben es indirekt an das mittelalterliche Europa weiter.

Als man im Italien des 15. Jahrhunderts die – meist römischen – Überreste der Antike neu zu schätzen lernte und in der Kunst nachahmte, bezeichnete man dies als "Renaissance". Die "Wiedergeburt" der Antike und des antiken Geistes setzte der jahrhundertelangen Dominanz religiösen Denkens in Europa ein Ende und mündete schließlich in das Zeitalter der europäischen Aufklärung und in die Moderne. Fast alle Ideen der neuzeitlichen Aufklärung haben antike Vorläufer. Ohne griechische Wissenschaft und Philosophie, ohne die damals entstandenen politischen Ideen, ohne das römische Recht, ohne Architektur und Kunst der Griechen und Römer ist die westliche Kultur der Neuzeit nicht denkbar.

So trat infolge der Arbeiten von Johann Joachim Winckelmann seit dem 18. Jahrhundert die „klassische“ griechische Kunst – oder vielmehr das, was man idealisierend für diese hielt – zunehmend ins Zentrum des Interesses. Im 19. Jahrhundert sprach man im Zusammenhang mit den Arbeiten von Architekten und Künstlern wie Karl Friedrich Schinkel, Leo von Klenze und Bertel Thorvaldsen von einer Renaissance der griechischen Antike, heute vom Neuhumanismus.

Erst nach dem Zweiten Weltkrieg verlor die griechisch-römische Zivilisation zunehmend die Vorbildfunktion, die man ihr in Europa und Nordamerika jahrhundertelang zugesprochen hatte. Ein entscheidender Einschnitt war hier das Verschwinden des griechischen und stark auch des lateinischen Unterrichtsfaches von den Sekundarschulen. Ein weiterer Aspekt war, dass in der ersten Hälfte des 20. Jahrhunderts Elemente der antiken Tradition von Anhängern totalitärer Ideologien willkürlich aufgegriffen und so zweckentfremdet wurden. Der Führerkult des faschistischen Regimes in Italien griff direkt auf das antike Rom zurück und knüpfte (nach dem Verständnis des Regimes) an den Caesarenkult an, wobei bereits der Terminus "fascismo" vom lateinischen Begriff "fasces" abgeleitet ist. Benito Mussolini wurde als Nachfolger des Augustus in eine Reihe mit den römischen Caesaren gestellt, und es wurde eine „Wiedererrichtung“ des antiken Römischen Reiches angestrebt. Auch das NS-Regime in Deutschland orientierte sich teils an antiken Vorbildern, so etwa im Zusammenhang mit der ideologisch begründeten Lobpreisung Spartas.

Der Bedeutungsverlust nach dem Ende des Zweiten Weltkrieges hat für die Altertumswissenschaften allerdings immerhin den Vorteil, dass nun ein unverstellterer, neutraler Blick auf die Antike leichter möglich ist.

Bis heute erhaltene Zeugnisse der Antike sind – neben überlieferten Texten philosophischer, literarischer oder historischer Natur – zahlreiche Objekte der griechischen und römischen Kunst: von großen Skulpturen bis zur Kleinkunst, Töpferei, Münzen etc. Wichtige Antikensammlungen befinden sich in Rom, Athen, Neapel, Paris, London, München, Sankt Petersburg, Wien und Berlin. Für die Kenntnis des antiken Alltags sind vor allem archäologische Ausgrabungen wie die in Pompeji, Olympia, Delphi oder Pergamon von Bedeutung.

Der Großteil der antiken Literatur (und damit auch der Geschichtsschreibung) ist nicht erhalten, sodass unser Wissen über die Antike durch die Überlieferungslage beeinflusst wird (siehe auch Antike Geschichtsschreibung und hinsichtlich der griechischen Geschichtsschreibung die Liste der griechischsprachigen Geschichtsschreiber der Antike). Es wurde geschätzt, dass uns kaum 10 % der griechischen Literatur überliefert ist. Andere Forscher sind noch weit pessimistischer und gehen eher von einer Verlustrate um 99 % aus. In Teilen sieht es besonders trostlos aus (Archaik, Hellenismus), in anderen Bereichen etwas besser (klassische Zeit Griechenlands sowie Spätantike). Insgesamt ist die Quellenlage jedoch problematisch; man muss in allen Bereichen davon ausgehen, dass vieles spurlos verloren ist und sich auch viele Ereignisse und Zusammenhänge unserer Kenntnis entziehen. Neben den erzählenden Quellen müssen daher natürlich auch Inschriften und Papyri sowie archäologische und numismatische Quellen etc. herangezogen werden. Eine Zusammenfassung mit ausführlichen Angaben bieten die jeweiligen Artikel (Geschichtsschreibung u. ä.) in den entsprechenden Lexika (siehe unten).

Im Folgenden seien einige der wichtigsten antiken Geschichtsschreiber und ihre (oft nur teilweise) erhaltenen Texte genannt:

Siehe auch die online verfügbaren Quellensammlungen wie LacusCurtius oder das Perseus Project.

Verschiedenes:

Rom:

Listen:

Quellenausgaben mit Übersetzungen bieten neben anderen Reihen die Sammlung Tusculum und die Loeb Classical Library. Eine äußerst wichtige Sammlung der erhaltenen Reste ansonsten verlorener griechischer Geschichtsschreiber stellt der "Jacoby" dar:

Allgemein: Das zentrale bibliographische Nachschlagewerk der Altertumswissenschaft stellt immer noch die "L’Année philologique" dar (L’Année Philologique. Bibliographie critique et analytique de l’Antiquité greco-latine, hrsg. von J. Marouzeau und J. Ernst, Paris 1923ff.). Kostenlos nutzbar ist zudem die umfangreiche Gnomon-Datenbank. Ausführliche Angaben sind außerdem entweder den Bibliographien der unten genannten Werke (besonders sei dabei auf The Cambridge Ancient History und Oldenbourg Grundriss der Geschichte hingewiesen) zu entnehmen oder den Bibliographien, die in der ausführlichen HU-Linkliste aufgeführt sind (siehe beispielsweise KU Eichstätt (kommentiert)). 

Es sei außerdem auf die hier verlinkten Artikel verwiesen, wo sich zahlreiche weiterführende Literaturangaben finden.







Nur in Auswahl. Es sei auch auf die oben genannten Fachlexika verwiesen.

Allgemein

Griechenland – Hellas

Rom

Persien/Iran

Germanen

Kelten

Skythen, Hunnen und andere Steppenvölker

Geschichtsschreibung

Militärgeschichte

Religionsgeschichte

siehe auch: "Reallexikon für Antike und Christentum"

Entdeckungsfahrten

Wirtschaftsgeschichte

Nachwirkungen



</doc>
<doc id="109" url="https://de.wikipedia.org/wiki?curid=109" title="Anthony Hope">
Anthony Hope

Anthony Hope war das Pseudonym von Sir Anthony Hope Hawkins (* 9. Februar 1863 in London; † 8. Juli 1933), einem britischen Rechtsanwalt und Schriftsteller. 

Anthony Hope war ein Sohn von Reverend Edward Connerford Hawkins, einem anglikanischen Geistlichen, und Jane Isabella Grahame. Er verließ die Universität Oxford 1885 mit einem first-class degree und wurde Anwalt in London. Er heiratete 1903, hatte zwei Söhne und eine Tochter. Während des Ersten Weltkrieges arbeitete er im Ministry of Information. 1918 wurde er für seine Verdienste während des Krieges zum Ritter geschlagen.

Sein erstes Buch war "A Man of Mark" (1890), später schrieb er "The Dolly Dialogues" (1894). Den größten Erfolg hatte er mit "The Prisoner of Zenda" (dt. „Der Gefangene von Zenda“). Anschließend verfasste er "Rupert of Henzau" (1898) und viele weitere Bücher.


</doc>
<doc id="110" url="https://de.wikipedia.org/wiki?curid=110" title="Ångström (Einheit)">
Ångström (Einheit)

Das Ångström [] (nach dem schwedischen Physiker Anders Jonas Ångström) ist eine Maßeinheit der Länge. Das Einheitenzeichen ist Å (A mit Ring).

Das Ångström ist "keine" SI-Einheit. Da sie nicht in der Einheitenrichtlinie aufgeführt wird, ist sie in der EU "keine" gesetzliche Einheit, nach der schweizerischen Einheitenverordnung auch nicht in der Schweiz. In DIN 1301-3 ist sie ausdrücklich als nicht mehr zugelassene Einheit aufgelistet.

Trotzdem wird das Ångström in manchen Bereichen weiterhin benutzt, um mit „einfachen“ Zahlenwerten arbeiten zu können, insbesondere in der Kristallographie und der Chemie. So ist 1 Å die typische Größenordnung für Atomradien, Abstände von Atomen in Kristallstrukturen und Bindungslängen in Molekülen. Der Radius isolierter neutraler Atome beträgt zwischen 0,3 und 3 Å. Daher wird das Ångström oft als Einheit für Abstände in atomaren Größenordnungen verwendet, z. B. für die Dicke sehr dünner Schichten, sowie für die Angabe der verwendeten Wellenlänge der Röntgenstrahlung bei ihrer Ermittlung in Röntgenbeugungsexperimenten wie der Kristallstrukturanalyse.

In der Thermodynamik wird die mittlere freie Weglänge der sich bewegenden Moleküle häufig in Ångström angegeben. Auch in der Optik und der Astronomie wird es zur Angabe einer Wellenlänge genutzt (allerdings weniger in deutschsprachigen, sondern eher in englischsprachigen Fachpublikationen).

Einen ähnlichen Versuch, zu einfach handhabbaren Zahlenwerten zu kommen, unternahm 1925 Manne Siegbahn mit der Definition der X-Einheit, die etwa 10 m entsprach. Das Ångström setzte sich aber durch.

Laut Unicode-Standard soll die Längeneinheit Ångström durch den Großbuchstaben Å (codice_1) dargestellt werden. Unicode enthält zwar auch ein Zeichen namens ANGSTROM SIGN (Ångströmzeichen, codice_2: Å), dieses wurde jedoch lediglich zur Kompatibilität mit älteren Zeichenkodierungsstandards aufgenommen und soll in neu erstellten Texten "nicht" verwendet werden.


</doc>
<doc id="111" url="https://de.wikipedia.org/wiki?curid=111" title="Ampere">
Ampere

Das Ampere [] mit Einheitenzeichen A, benannt nach dem französischen Mathematiker und Physiker André-Marie Ampère, ist die SI-Basiseinheit der elektrischen Stromstärke und zugleich SI-Einheit der abgeleiteten Größe „magnetische Durchflutung“.

Obwohl man den Nachnamen des Namensgebers mit "accent grave" schreibt („Ampère“), wird die SI-Einheit im deutschen und englischen Sprachraum üblicherweise "ohne" Akzent geschrieben, also „Ampere“.

Mit der Definition des Ampere im SI-Einheitensystem ist zugleich der Wert der magnetischen Feldkonstante μ festgelegt.

Bevor das Ampere als internationale Einheit der Stromstärke festgelegt wurde, gab es eine Reihe von unterschiedlichen Einheiten und Definitionen. In Deutschland und einigen anderen Ländern war die „Webersche Einheit“ der Stromstärke in Gebrauch, dabei war 1 Weber-Einheit = 0,1 Ampere. In Großbritannien schlug man zunächst vor, die Einheit der Stromstärke mit „Galvat“, nach dem italienischen Biophysiker Luigi Galvani, zu benennen, die in etwa dem heutigen Ampere entsprochen hätte. Später wurde ebenfalls eine „Weber-Einheit“ für die Stromstärke eingeführt, die aber einen 10 mal so hohen Wert hatte wie die in Deutschland gebräuchliche. Noch verwickelter wurde es dadurch, dass der Name Weber auch für die Einheit der elektrischen Ladung benutzt wurde, so dass dann die Stromstärke gleich „Weber-Einheit/Sekunde“ war. Zeitweise gab man der Weber-Einheit auch den Namen „Farad“, womit später die Einheit der elektrischen Kapazität benannt wurde.

Würde man die Stromstärke mit einer abgeleiteten Einheit messen, wie das etwa beim CGS-Einheitensystem geschieht, so ließen sich die elektrischen Größen durch die Basiseinheiten nur mit nicht ganzzahligen Exponenten ausdrücken. Um das zu vermeiden, wurde schon 1939 die Einheit der Stromstärke als weitere Basiseinheit vorgeschlagen.

1898 wurde 1 Ampere im „Gesetz, betreffend die elektrischen Maßeinheiten“ des Deutschen Kaiserreichs als die Stärke desjenigen Stromes definiert, der aus einer wässrigen Silbernitrat-Lösung mittels Elektrolyse in einer Sekunde 1,118 mg Silber abscheidet. Das so definierte Ampere ist später als internationales Ampere bezeichnet worden; das mit den restlichen Basiseinheiten kompatible dagegen als absolutes Ampere.

Seit 1948 wird das Ampere wie folgt über die Lorentzkraft zweier Leiter aufeinander definiert:

Ein Ampere entspricht einem Fluss von 1 Coulomb pro Sekunde durch den Leiterquerschnitt:

Dies bedeutet einen Durchsatz von 6,24151·10 (etwa 6 Trillionen) Elektronen pro Sekunde.

Im Oktober 2005 beschloss das Internationale Komitee für Maß und Gewicht (CIPM) die Vorbereitungen für eine Neudefinition der Einheiten Kilogramm, Ampere, Kelvin und Mol zu treffen, basierend auf Naturkonstanten, um diese auf der nächsten Generalkonferenz im Jahr 2011 beschließen zu können.
Daraufhin wurde im Jahr 2006 ein Vorschlag zur Umsetzung veröffentlicht. Gemäß diesem Vorschlag wäre das Ampere definiert durch den Fluss einer bestimmten Menge von Partikeln mit der Elementarladung pro Zeit.



</doc>
<doc id="112" url="https://de.wikipedia.org/wiki?curid=112" title="Acre">
Acre

Acre (Plural deutsch "Acre" oder "Acres;" , Plural "acres") ist eine von den Britischen Inseln stammende angloamerikanische Maßeinheit zur Flächenbestimmung von Grundstücken und entspricht grob 4047 m² beziehungsweise 40,47 Ar. Neben dem Acre wird im heutigen angloamerikanischen Flächenmaßsystem zur Land- und Grundvermessung praktisch nur noch die Flächeneinheit Square Foot verwendet; in den Vereinigten Staaten wird die Größe von Grundstücken allein mit diesen beiden Flächeneinheiten angegeben. Weit verbreitet sind diese Einheiten auch in Großbritannien, Kanada, Indien, Australien und anderen Commonwealth-Staaten, obwohl dort heute ein metrisches Hauptmaßsystem besteht.

Heutzutage wird der Acre direkt mit 43.560 Square Feet definiert, weil die zur ursprünglichen Definition verwendeten Längenmaße Furlong und Rod mittlerweile ungebräuchlich sind.

Unter Einbezug der Meile sowie den heutzutage nicht mehr gebräuchlichen Flächenmaßen Rood und Square Rod (auch als Square Perch, Perch, Square Pole, Pole bezeichnet) ergibt sich die folgende Umrechnung:
Der Acre ist das Hauptmaß für Grundstücksflächen. In der Regel werden nicht mehr als zwei Nachkommastellen angegeben, womit eine Genauigkeit von ±20 m² vorliegt. Sobald genauere Angaben erforderlich sind, beispielsweise bei Bauland, wird die Flächeneinheit Square Foot verwendet.

Bei landwirtschaftlich genutzten Grundstücken werden die Flächen in "Workable Acre" und "Non Workable Acre" unterteilt. Damit gemeint sind die tatsächlich nutzbare Fläche und die für landwirtschaftliche Zwecke nicht nutzbare Fläche, wie beispielsweise Ödland.

Auch sehr große Grundflächen werden in Acre angegeben, beispielsweise 87.000 ac (≈ 350 km²). Eine Umrechnung in Quadratmeilen erfolgt in der Regel nicht.


Die Einheit "acre," von altenglisch "æcer" ‚Acker, Feld‘, bezeichnete ursprünglich den Landstreifen, der mit einem Ochsengespann in einem Tag gepflügt werden konnte. Unter König Eduard I. sowie erneut unter Eduard III. und Heinrich VIII. wurde der Acre gesetzlich als ein Stück Land mit der Länge von 40 Rods (oder Perches; = 1 Furlong oder 660 Feet) und der Breite von 4 Rods (oder Perches; = 66 Feet oder [seit 1620] 1 Chain) beziehungsweise 160 Square Rods (Quadratruten) bei welcher Grundstücksform auch immer definiert.

Mit seiner Größe von grob 40 Ar ist der Acre typologisch vergleichbar mit dem Morgen, dem Tagewerk (oder "Tagwan"), dem Joch (oder Juchart) und dem Mannwerk.

Die weltweit unterschiedlichen angloamerikanischen Längeneinheiten wurden 1959 vereinheitlicht. In den Vereinigten Staaten ergab dies für die bis dahin verwendeten Längenmaße gegenüber den neuen „internationalen“ Längenmaßen eine Vergrößerung um den Faktor 1,000002. Bei der Landvermessung hätte dies bei großen Entfernungen zu spürbaren Differenzen geführt. Bei einer Streckenlänge von beispielsweise 1000 km wäre der Unterschied 2 m. Um die bestehenden Werte nicht ändern zu müssen, wurde deshalb in den Vereinigten Staaten das alte Maßsystem – nur für den Zweck der Landvermessung – beibehalten und das bisher verwendete Längenmaß "Foot" erhielt die Bezeichnung "Survey Foot".

In den Vereinigten Staaten basiert die Flächeneinheit Acre auf dem "Survey Foot" (der Zusatz "Survey" wird in der Regel weggelassen), sonst ist der "International Foot" die Grundlage. Der auf den "U.S. Survey Foot" basierende "Acre" ist um etwa 162 cm² geringfügig größer.

Obgleich auf den Britischen Inseln die Größe des Acres seit dem Hochmittelalter mit 160 Square Rods definiert war, war dessen Fläche je nach Ort und Zeit uneinheitlich, da die Längeneinheit Rod oder Perch verschiedenen Fuß-Äquivalenten entsprach. Erst mit der Neudefinition der "Imperial Units" durch den "Weights and Measures Act" von 1824 wurde ein für das gesamte Britische Weltreich einheitlicher Acre geschaffen.

Vor der Einführung des Imperial Standard Acre (Statute Acre) gab es unter anderem den alten schottischen Acre, den neuen schottischen Acre, auch als Cunningham Acre bezeichnet, oder den irischen bzw. Plantation Acre. Beispielsweise hat der Cunningham Acre etwa die 1,3-fache Größe, der Plantation Acre grob die 1,6-fache Größe des heutigen Acres. Einige dieser veralteten Maße waren teilweise bis ins 20. Jahrhundert gebräuchlich, so in abgelegenen Gebieten Irlands.



</doc>
<doc id="113" url="https://de.wikipedia.org/wiki?curid=113" title="Apostilb">
Apostilb

Apostilb (Einheitenzeichen asb; als alternativer Name wurde 1942 das Blondel vorgeschlagen) ist eine veraltete Einheit der Leuchtdichte selbstleuchtender Körper und gilt seit 1978 "nicht mehr" als offizielle Einheit. Es ist eine Untereinheit des Stilb und darüber verknüpft mit dem Lambert:

Die entsprechende SI-Einheit ist cd/m² bzw. das Nit:


</doc>
<doc id="114" url="https://de.wikipedia.org/wiki?curid=114" title="Ar (Einheit)">
Ar (Einheit)

Das oder der Ar, in der Schweiz die Are, ist eine Flächenmaßeinheit im metrischen System von 100 m mit dem Einheitenzeichen a (oft jedoch nicht oder falsch abgekürzt: Ar bzw. ar). 100 a ergeben 1 ha. Ein Quadrat mit dem Flächeninhalt von 1 a hat eine Kantenlänge von zehn Metern, man spricht daher auch von einem "Quadratdekameter" (dam). 

Das Ar ist keine SI-Einheit; im Gegensatz zum Hektar ist sie aus Sicht des Internationalen Einheitensystems nicht einmal zum Gebrauch mit dem SI zugelassen.

In der EU und der Schweiz ist der Ar bzw. die Are gesetzliche Einheit für die Angabe der Fläche von Grund- und Flurstücken.

Im Jahr 1793 wurde in Frankreich das Meter als der 10-millionste Teil des Erdquadranten auf dem Meridian von Paris festgelegt. Zugleich wurde die Einheit "are" in Anlehnung an das lateinische Wort "ārea" (Fläche, freier Platz) für die Fläche von 100 m neu geschaffen. Sie war anfangs die einzige gebräuchliche metrische Flächeneinheit, samt ihren Teilen und Vielfachen Zentiar (1 ca = 1 m) und Hektar (1 ha = 100 a).

Im Jahr 1868 wurde die Maßeinheit unter der Bezeichnung Ar auch in Deutschland amtlich eingeführt.


Außer Ar und Hektar sind diese Vielfachen und Teile im deutschen Sprachraum ungebräuchlich und nur noch von historischem Interesse.

Das Dekar wird als Flächenmaß in der bulgarischen Landwirtschaft, in Griechenland (Stremma), in der Türkei und einigen Staaten des Nahen Ostens (metrisches Dunam) verwendet.



</doc>
<doc id="115" url="https://de.wikipedia.org/wiki?curid=115" title="Arbeit (Sozialwissenschaften)">
Arbeit (Sozialwissenschaften)

Arbeit ist eine zielbewusste und sozial durch Institutionen (Bräuche) abgestützte besondere Form der Tätigkeit, mit der Menschen seit ihrer Menschwerdung in ihrer Umwelt zu überleben versuchen.

Es ist bereits strittig, ob man zielgerichtete körperliche Anstrengung von Tieren (zum Beispiel den instinktiven Nestbau oder das andressierte Ziehen eines Pfluges) als „Arbeit“ bezeichnen kann. Die philosophische Anthropologie geht zumeist davon aus, dass „Arbeit“ erst im Tier-Mensch-Übergangsfeld erscheint (vgl. zum Beispiel Friedrich Engels’ "Anteil der Arbeit an der Menschwerdung des Affen", MEW 20). Dabei wird meist angenommen, dass die Resultate menschlicher Arbeit (als „Gegenstände“) wie in einem Spiegel dem Menschen sich selbst zeigen, so dass er angesichts ihrer des Selbstbewusstseins mächtig wird. Das könnten aber auch andere menschliche Tätigkeiten bewirken, so dass „Arbeit“ gerade in ihren ursprünglichen Formen schwer von anderen menschlichen Überlebensstrategien wie Spiel oder Kunst zu trennen ist. Seit der Urgeschichte ist (so Karl Bücher) ein Basiszusammenhang von Arbeit und Rhythmus anzunehmen (vgl. das Arbeitslied).

Im Vergleich zu modernen Erwerbstätigen hatten Jäger und Sammler laut zahlreichen Studien mehr Zeit zur freien Verfügung. "Siehe hierzu": Abschnitt „Alltag und Lebenserwartung“ im Artikel „Jäger und Sammler“.

Das Wort "Arbeit" ist gemeingermanischen Ursprungs ("*arbējiðiz", got. "arbaiþs"); die Etymologie ist unsicher; evtl. verwandt mit indoeurop. "*orbh-" „verwaist“, „Waise“, „ein zu schwerer körperlicher Tätigkeit verdungenes Kind“ (vgl. Erbe); evtl. auch verwandt mit aslaw. "robota" („Knechtschaft“, „Sklaverei“, vgl. Roboter).

Im Alt- und Mittelhochdeutschen überwiegt die Wortbedeutung „Mühsal“, „Strapaze“, „Not“; redensartlich noch heute "Mühe und Arbeit" (vgl. Psalm 90, lateinisch "labor et dolor").

Das französische Wort "travail" hat eine ähnliche, sogar noch extremere Wortgeschichte hinter sich: es leitet sich von einem frühmittelalterlichen Folterinstrument ab.

Das italienische "lavoro" und englische "labour" (amerikanisch "labor") gehen auf das lateinische "labor" zurück, das ebenfalls primär „Mühe“ bedeutet.

Viele Redensarten sind mit ihr verbunden. So wurde harte körperliche Arbeit früher als "Kärrnerarbeit" bezeichnet, und eine "Schweinearbeit" bedeutet unangenehm viel Arbeit: "Wer die Arbeit kennt und sich nicht drückt, | der ist verrückt."

Die Geschichtsschreibung der Arbeit begann erst im 20. Jahrhundert (zuerst in Frankreich, England und den USA) zu entstehen. Eine frühe Ausnahme innerhalb der deutschen Historiker war Karl Lamprecht (1856–1915). Ein neueres Buch stammt von Arne Eggebrecht und anderen.

Aristokratische Autoren wie Xenophon, Platon, Aristoteles und Cicero würdigten den Großteil der täglichen Arbeit (Handwerker, Bauern, Kaufleute) herab. Sie galt ihnen (insbesondere körperliche) Arbeit als Zeichen der Unfreiheit. Sklaven ("dúloi") und Handwerker ("bánausoi") waren „der Notwendigkeit untertan“ und konnten nur durch diese als „unfrei“ verstandene Arbeit ihre Lebensbedürfnisse befriedigen. Geistige Arbeit blieb der "scholé" (gespr. "s|cholé") vorbehalten, was etwa „schöpferische Muße“ beschrieb, wovon das deutsche Wort Schule herrührt.

In Europa blieben – vor allem in der Landwirtschaft – Formen unfreier Arbeit von Männern und Frauen, auch Kindern und Alten, lange erhalten (Fron, Lasswirtschaft), am stärksten im Russischen Reich; im Deutschen Reich wurden deren letztes Überbleibsel (die Schollengebundenheit in den beiden Mecklenburgs) erst durch die Novemberrevolution 1918 beseitigt. Noch heute existieren in großen Teilen der Welt unterschiedliche Erscheinungsformen unfreier Arbeit, von der Arbeitspflicht bis hin zur Arbeitsversklavung und Zwangsarbeit.

Eine positive Bewertung von Arbeit als „produktiver Betätigung zur Befriedigung eigener oder fremder Bedürfnisse“ war im Rittertum und in der Mystik angelegt. Durch Reformation und Aufklärung rückte sie in den Vordergrund: Eine neue Sicht der Arbeit als sittlicher Wert und Beruf (als Berufung verstanden) des Menschen in der Welt wurde von Martin Luther mit seiner Lehre vom allgemeinen Priestertum ausgeprägt. Schärfer noch wurde im Calvinismus die Nicht-Arbeit überhaupt verworfen "(siehe auch: Protestantische Ethik)".

In der Frühphase der Aufklärung wurde Arbeit zum Naturrecht des Menschen erklärt (Jean-Jacques Rousseau). Damit wurde das feudalistische Prinzip der Legitimation kritisiert. Eigentum entsteht "einzig" durch Arbeit, niemand hat ein von Gott gegebenes Anrecht auf Eigentum. Güter, die nicht durch menschliche Arbeit entstanden sind, sind Gemeinbesitz.

Adam Smith unterscheidet produktive und unproduktive Arbeit. "Produktive" Arbeit nennt er die Arbeit, deren Resultat ein verkäufliches Produkt ist. Dazu wird nicht nur der eigentliche Wertschöpfungsprozess (beim Schmied: der Vorgang des Schmiedens selbst) gerechnet, sondern auch alle Arbeiten, die indirekt zur Vervollkommnung des Gutes beitragen (beim Schmied: das Erhalten der Glut, das Pflegen von Hammer und Amboss). "Unproduktiv" ist hingegen die Arbeit, die nicht in einem verkäuflichen Produkt resultiert (zum Beispiel die mütterliche Hausarbeit). Andere Arbeiten sind von diesem Standpunkt aus nicht unnütz, da sie notwendig sind, um produktive Arbeit leisten zu können, und werden heute zum Beispiel als "reproduktiv" bezeichnet (beispielsweise Beamte, Verwalter, Soldaten).

Der Frühsozialist Charles Fourier proklamierte 1808 ein "Recht auf Arbeit."

In der deutschen Philosophie (Immanuel Kant, Johann Gottfried Herder, Georg Wilhelm Friedrich Hegel, Johann Gottlieb Fichte) wird die Arbeit zur Existenzbedingung und sittlichen Pflicht erklärt. Kant räumte in seiner "Anthropologie in pragmatischer Hinsicht" (1798, §87) jedoch ein, dass Faulheit eine Schutzfunktion habe: „"Denn die Natur hat auch den Abscheu für anhaltende Arbeit manchem Subjekt weislich in seinen für ihn sowohl als andere heilsamen Instinkt gelegt: weil dieses etwa keinen langen oder oft wiederholenden Kräfteaufwand ohne Erschöpfung vertrug, sondern gewisser Pausen der Erholung bedurfte."“

Nach Karl Marx' Werttheorie ist die „menschliche Arbeitskraft“ als alleinige Kraft fähig, das Kapital (als eine Ansammlung geronnener Arbeit) zu vergrößern (Mehrwert zu akkumulieren). Sie tut dies im Kapitalismus unausweichlich.

Praktisch spiegelt dies wider, dass in der Phase der Industrialisierung freie Arbeit augenfällig zur Ware wurde und vorwiegend die düsteren Züge der damaligen Lohnarbeit annahm. So zum Beispiel in Gestalt der Kinderarbeit, des Arbeiterelends (der „Verelendung“), der Arbeitsunfälle und -krankheiten, der drückenden Akkordarbeit – alles dies sind Merkmale der allgemein so empfundenen „Sozialen Frage“

Deren Folgen wurden schon seit Hegel als „Entfremdung“ charakterisiert: Der Arbeiter hat zu seinem eigenen Arbeitsprodukt, aber auch zu dem Unternehmen, für das er arbeitet, nur noch das bare Lohnverhältnis und kann dem gemäß nicht mehr stolz auf sie sein – in diesem 'Spiegel' erkennt er sich selbst jedenfalls nicht mehr wieder.

Für Ernst Jünger war Arbeit nicht Tätigkeit schlechthin, sondern der Ausdruck eines „besonderen Seins, das seinen Raum, seine Zeit, seine Gesetzmäßigkeit zu erfüllen sucht“ („"Der Arbeiter"“). Daher kenne Arbeit auch keinen Gegensatz außer sich selbst. Das Gegenteil von Arbeit sei nicht Ruhe oder Muße, da es keinen Zustand gebe, der nicht als Arbeit begriffen werden könne.

Neben der „produktiven“ Eigenschaft der Arbeit wird neuerdings (Lars Clausen) ihre „destruktive“ Seite hervorgehoben: am auffälligsten als (harte, lebensgefährliche) Arbeit der Soldaten, aber auch bei selbst-, mitmenschen- oder umweltzerstörerischer Arbeit ist Destruktives seit je Wesensbestandteil aller Arbeit. (Anders die „vernichtende Tätigkeit“, die alltags als Vandalismus auftreten kann und einen organisatorischen Höhepunkt im KZ hatte.)

Der Soziologe Rudi Dutschke und der Politologe Bernd Rabehl meinten 1967 in einem Gespräch mit Hans Magnus Enzensberger, der technische Fortschritt könne die Erwerbsarbeit in Zukunft erheblich reduzieren: „"Dabei muß man bedenken, dass wir fähig sein werden, den Arbeitstag auf fünf Stunden zu reduzieren durch moderne Produktionsanlagen, dadurch dass die überflüssige Bürokratie wegfällt. Der Betrieb wird zum Zentrum der politischen Selbstbestimmung, der Selbstbestimmung über das eigene Leben. Man wird also im Betrieb täglich debattieren, es wird langsam ein Kollektiv entstehen, ein Kollektiv ohne Anonymität, begrenzt auf zwei- bis dreitausend Leute, die also immer noch eine direkte Beziehung zueinander haben."“

In der Zeit der 1950er und 1960er Jahre gab der technische Fortschritt sogar in der calvinistisch geprägten nordamerikanischen Gesellschaft tatsächlich wieder dem Gedanken Raum, dass Fortschritt zu mehr Freizeit führen könne. Zeugnisse für die Hoffnungen gaben die Schöpfungen einer bunten Pop-Kultur mit ihren Science-Fiction-Träumen wie beispielsweise der Zeichentrickserie „Die Jetsons“, in der technikgestütztes Faulenzen ohne moralische Bedenken als Ideal dargestellt werden konnte.

Angesichts global unterschiedlicher Entwicklungen zeigte sich jedoch, dass ein Ausruhen auf erreichtem Wohlstand in einer Region als Gelegenheit zum wirtschaftlichen Aufholen in anderen Regionen verstanden wurde. In jenem Zeitraum wurde besonders in "Japan" technischer Fortschritt in erster Linie als Weg begriffen, große wirtschaftliche Fortschritte zu erzielen. Bis heute begrenzt somit ein Wettbewerb, in dem der verliert, der zuerst bremst, die Möglichkeit, aus technischem und technologischem Fortschritt mehr selbstbestimmte freie Zeit zu gewinnen.

Zudem prägte Robert Solow in der Wirtschaft bereits 1956 mit seinem Wachstumsmodell die Auffassung, dass technologische Innovation in erster Linie als ein Multiplikator des Faktors Arbeit aufträte, womit er in der Dogmengeschichte der Wirtschaft einen Ankerpunkt schuf, der bis heute den Raum des Denkbaren gegenüber möglichen Alternativen wirkungsvoll abgrenzt. So schafft in der heutigen Arbeitswelt technischer Fortschritt dort, wo er Freiräume erweitert, vorwiegend und sogar mit zunehmender Geschwindigkeit immer neue Arbeit. Dort, wo Technik schon vor Beginn des Industriezeitalters die Menschen von Arbeit befreite, wurden sie oft nicht freier, sondern arbeitslose Geächtete.

In Deutschland nahm zwischen 1960 und 2010 das Arbeitsvolumen pro Kopf kontinuierlich um 30 Prozent ab.

Nach wie vor wird „Erwerbsarbeit“ nicht mit „Arbeit“ überhaupt gleichgesetzt. Wo „Arbeit“ auch heute noch nicht „Ware“ ist, sind zwei wesentliche Aspekte hervorzuheben:


In den wohlhabenden Staaten der Welt (zu denen auch Deutschland zählt), wird die Erwerbsarbeit knapp. Es findet eine zunehmende Flexibilisierung, Virtualisierung, Automatisierung und Subjektivierung der Arbeit statt, prekäre Arbeitsverhältnisse nehmen zu. Inhaltlich verschiebt sich die Arbeit immer mehr in den tertiären Sektor (Dienstleistungen) und in „Niedriglohnländer“ (Offshoring), zumal da die Jugend- und Langzeit-Arbeitslosigkeit die „Arbeit“ trotz ihres zentral wichtigen Charakters als Überlebenstätigkeit aus dem Feld der Erfahrung Vieler rücken.

In ärmeren Ländern herrschen zugleich – zum Teil – Verhältnisse, die mit denen in der Industrialisierungsphase Europas vergleichbar sind: Kinderarbeit, Billiglohnarbeit und fehlende soziale Absicherung sind dort häufig anzutreffende Bestandteile der Arbeitswelt.

Dort, wo Arbeit für andere verrichtet wird, ist nach wie vor der Unterschied bedeutsam
Ein Wandel einer Tätigkeit von der unentgeltlichen zur entgeltlichen Form wird auch als Kommerzialisierung bezeichnet.

Die unentgeltliche Arbeit umfasst also historisch "sehr viele" Formen, die auch heute vorkommen, aber nicht immer als „Arbeit“ betrachtet werden. Beispiele sind


Unter Erwerbsarbeit versteht man eine Arbeitsleistung gegen Entgelt (Arbeitslohn) im Gegensatz zu unentgeltlicher Arbeit (wie Subsistenzarbeit, Sklavenarbeit, weiblicher Hausarbeit oder ehrenamtlicher Arbeit).

Erwerbsarbeit wird in einem Beschäftigungsverhältnis (Lohnarbeit) oder in selbständiger und neuerdings auch in scheinselbständiger Form geleistet. 

Beispiele sind


Das deutsche Privatrecht unterscheidet hier analog zwischen Werkvertrag (der Erfolg wird geschuldet) und Dienstvertrag (der Dienst wird geschuldet).

Hierzu gehören zahlreiche freiwillige oder gesetzlich vorgesehene Arbeiten, die gering entgolten werden. Teils sind die Arbeitenden zur Verrichtung der betreffenden Tätigkeiten rechtlich verpflichtet, teils fühlen sie sich ethisch hierzu verpflichtet. Zu den Mischformen gehören auch solche ehrenamtlichen Tätigkeiten, für die eine Aufwandsentschädigung gezahlt wird, die über den tatsächlichen Aufwand hinausgeht.

Was die zentrale Stellung der Arbeit in kollektiven Wertsystemen angeht, sagen Kritiker der Arbeit, unterscheiden sich Staatsformen und Herrschaftsmodelle erstaunlich wenig.

Als Kritiker der Arbeit war Paul Lafargue, Autor des Pamphlets "Le droit à la paresse" (‚Das Recht auf Faulheit‘; 1883), in der alten Arbeiterbewegung ein Außenseiter. Lafargue verstand sich als revolutionärer Sozialist und dementsprechend schätzte er die kapitalistische Arbeitsethik ein. „Die kapitalistische Moral, eine jämmerliche Kopie der christlichen Moral, belegt das Fleisch des Arbeiters mit einem Bannfluch: Ihr Ideal besteht darin, die Bedürfnisse des Produzenten auf das geringste Minimum zu reduzieren, seine Genüsse und Leidenschaften zu ersticken und ihn zur Rolle einer Maschine zu verurteilen, aus der man ohne Rast und ohne Dank Arbeit nach Belieben herausschindet.“ Lafargues Manifest erschien 1887 auf Deutsch. Lafargue zitierte Lessing:

Die radikalen Kritiker der Arbeit lehnen den Arbeitszwang ab – für Reiche wie für Arme. Damit unterscheiden sie sich von Sozialisten, die sich über den Müßiggang der Reichen empören und fordern, dass alle arbeiten müssen. Hintergrund der Ablehnung des Arbeitszwangs ist die reale Möglichkeit der Aufhebung der Arbeit. Schon Lafargue meinte, dass 3 Stunden Arbeit ausreichen müssten. "Aufhebung der Arbeit" bedeutet jedoch nicht nur Verringerung der Arbeitszeit durch Automation und Abschaffung der Produktion von Gütern, die nur um des Profits willen hergestellt werden.

Unter kapitalistischen Bedingungen sind Arbeitslose wie abhängig Beschäftigte und auch diejenigen, die auf das sogenannte Berufsleben vorbereitet werden, gleichermaßen dem System der Lohnarbeit unterworfen. Auch wer freie Zeit hat, kann diese nicht frei nutzen, sei es weil andere, mit denen man etwas zusammen tun möchte, arbeiten müssen, sei es weil die gesamte Umwelt von kommerziellen Zwängen geprägt ist. Aufhebung der Arbeit bedeutet, dass auch weiterhin notwendige Tätigkeiten wie zum Beispiel die Pflege gebrechlicher Menschen, einen anderen Charakter annehmen, wenn sie in einem anderen nicht-hierarchischen Kontext ausgeübt werden. Dass die Menschen ohne den Zwang zu Arbeit einfach nichts tun und verhungern würden, ist nach Ansicht der Kritiker der Arbeit nicht zu erwarten, da sie ja bereits unter kapitalistischen Bedingungen freiwillig konstruktiv zusammenarbeiten.

Die Tradition der Ablehnung der Arbeit wurde nach dem Zweiten Weltkrieg von einer Gruppe junger Menschen in Paris wiederbelebt. Unter ihnen war Guy Debord. Der Slogan „Ne travaillez jamais“ (‚Arbeitet niemals‘) kehrte dann im Pariser Mai 1968 wieder. Die Ablehnung der Arbeit spielte auch in Italien in den Kämpfen der 1960er und 1970er Jahre eine zentrale Rolle.

Der Postanarchist Bob Black rief 1985 die Proletarier dieser Welt auf, sich zu entspannen, da niemand jemals arbeiten solle. Bob Black versteht sich als Antimarxist und postleftistischer (Individual-)Anarchist. Er ruft dazu auf, alle Arbeitsplätze so umzugestalten, dass sie wie ein Spiel sind. Er findet es merkwürdig, dass die einen sich auf dem Feld abrackern, während andere in ihrer Freizeit, welche nur das ebenfalls fremdbestimmte und durchorganisierte Gegenstück zur Arbeit sei, bei der Gärtnerei entspannen. Zentral in seiner Kritik ist neben diesem Punkt(en) auch der Charakter der Fremdbestimmtheit der Arbeit, ob nun im Staatssozialismus oder im Kapitalismus. Im Anschluss an Michel Foucault kritisiert er Disziplinierung und die Disziplinargesellschaft, und betont die zentrale Rolle der Arbeit bei der Disziplinierung: Gefängnisse und Fabriken seien zur selben Zeit entstanden, die Schulen seien dafür da, Leistungsgedanken und -bereitschaft und Gehorsam einzuüben und es gebe „mehr Freiheit in jeder einigermaßen entstalinisierten Diktatur als an einem gewöhnlichen amerikanischen Arbeitsplatz“. Eine ähnliche Kritik hatte allerdings auch schon Gustav Landauer vorgetragen. Auch er wollte den Arbeitstag ähnlich neu gestalten.

Von einer deutschen Tradition der Arbeitskritik kann man dennoch kaum reden. Seit den 1990er Jahren bemüht sich allerdings die wertkritische Gruppe "Krisis" um eine Erneuerung der Kritik der Arbeit. Sie veröffentlichte ein "Manifest gegen die Arbeit". Die Krisis versteht sich als postmarxistisch, bzw. grenzt sie sich ab vom traditionellen Marxismus.

Aktuell in der Kritik der Arbeit ist die Kritik der Identifikation mit der Arbeit als zentralem Element männlicher Identität.

Im Bereich der Feldforschung wurde eine Studie der Österreichischen Wirtschaftspsychologischen Forschungsstelle berühmt. Sie hieß "Die Arbeitslosen von Marienthal" (1933) und beschäftigt sich mit den Folgen plötzlich eintretender Arbeitslosigkeit für eine Dorfgemeinschaft.

Darstellungen und Schilderungen der täglichen Arbeit am unteren Rand der Gesellschaft finden sich innerhalb der Belletristik etwa bei den österreichischen Autoren Franz Innerhofer und Gernot Wolfgruber, dem Deutschen Hans Dieter Baroth und bei George Orwell ("Erledigt in Paris und London").







</doc>
<doc id="116" url="https://de.wikipedia.org/wiki?curid=116" title="Atomare Masseneinheit">
Atomare Masseneinheit

Die atomare Masseneinheit (Einheitenzeichen: u für unified atomic mass unit," veraltet amu für atomic mass unit") ist eine Maßeinheit der Masse. Ihr Wert ist auf der Masse eines Atoms des Kohlenstoff-Isotops C festgelegt. Die atomare Masseneinheit ist zum Gebrauch mit dem Internationalen Einheitensystem (SI) zugelassen und eine gesetzliche Maßeinheit. 

Sie wird bei der Angabe nicht nur von Atom-, sondern auch von Molekülmassen verwendet. In der Biochemie, in den USA auch in der organischen Chemie, wird die atomare Masseneinheit auch als Dalton bezeichnet (Einheitenzeichen: Da), benannt nach dem englischen Naturforscher John Dalton.

Die so gewählte atomare Masseneinheit hat die praktisch nützliche Eigenschaft, dass alle bekannten Kern- und Atommassen nahe bei ganzzahligen Vielfachen von formula_1 liegen; die Abweichungen betragen in allen Fällen unter formula_2. Die betreffende ganze Zahl heißt Massenzahl des Kerns oder Atoms und ist gleich der Anzahl der Nukleonen im Kern.

1 u entspricht der Masse eines isolierten Atoms des Kohlenstoff-Isotops C im Grundzustand, also

Somit ergibt sich als Umrechnung in die SI-Einheit Kilogramm:

Da der Kern des C-Atoms 12 Nukleonen enthält, ist die Einheit u annähernd gleich der Masse eines Nukleons, also eines Protons oder Neutrons. Deshalb entspricht der Zahlenwert der Atommasse in u annähernd der Massenzahl oder Nukleonenzahl, also der Zahl der schweren Kernbausteine des Atoms.

Eine atomare Masseneinheit entsprach der Masse eines Sauerstoff-Atoms. Dabei bezogen sich die Chemiker auf die durchschnittliche Masse eines Atoms im natürlich vorkommenden Isotopengemisch des Elements O, die Physiker aber auf die Masse des Atoms des Hauptisotops O.

Die Differenz zwischen der „chemischen“ Definition und der „physikalischen“ Definition war Anlass, eine vereinheitlichte Definition einzuführen. Über die Verhandlungen in den zuständigen Gremien wird berichtet, dass die Chemiker zunächst nicht bereit waren, auf die Definition der Physiker mit O einzuschwenken, da dies erhebliche Verluste beim Verkauf von chemischen Substanzen zur Folge gehabt hätte. Schließlich überzeugten die Physiker die Chemiker mit dem Vorschlag, C als Basis zu nehmen, wodurch der Unterschied zur chemischen Definition nicht nur viel geringer war, sondern auch in die „richtige Richtung“ ging und sich positiv in den Verkaufserlösen auswirken würde.

Zwischen dem neuen und den beiden veralteten Werten der Einheit gilt die Beziehung

Die Differenz zwischen der alten physikalischen und der heutigen Definition ist auf den Massendefekt zurückzuführen, der bei O höher ist als bei C.

In der deutschen Übersetzung der Broschüre des Internationalen Büros für Maß und Gewicht werden die (vereinheitlichte) atomare Masseneinheit und das Dalton synonym genannt. In den gesetzlichen Regelungen der EU-Richtlinie 80/181/EWG für die Staaten der EU und im Bundesgesetz über das Messwesen in der Schweiz kommt der Ausdruck „Dalton“ nicht vor. Das Dalton kann als besonderer Name für die atomare Masseneinheit betrachtet werden, aber die Bezeichnung Dalton ist weder gesetzlich noch DIN-normgerecht. Begrifflich gibt es Überlappungen zur molaren Masse, sowie zur Molekülmasse, die sich in der Praxis deutlich zeigen: So wird etwa die Masse großer Moleküle wie Proteine, DNA und anderer Biomoleküle mit der atomaren Masseneinheit - meist in Kilodalton - charakterisiert, da es zahlenmäßig keine Unterschiede zur Angabe in kg/mol gibt. 

Sowohl für die atomare Masseneinheit als auch für das Dalton ist die Verwendung von Vorsätzen für dezimale Vielfache und Teile zulässig. Gebräuchlich sind das Kilodalton, 1 kDa = 1000 Da, sowie das Megadalton, 1 MDa = 1.000.000 Da.

Das Mol ist (per Definition) die Stoffmenge eines Systems, das aus ebenso vielen Einzelteilchen besteht, wie Atome in 12 Gramm des Nuklids Kohlenstoff-12 enthalten sind. Diese Anzahl von Einzelteilchen pro Mol ist die Avogadro-Konstante formula_8, deren Wert formula_9   beträgt. Die Atomare Masseneinheit ist (per Definition) auf 1⁄12 der Masse eines Atoms des Nuklids Kohlenstoff-12 festgelegt.

Nimmt man nun 1 Mol von diesem Nuklid, dessen Atome eine Masse von je 12 u haben, so erhält man

Teilt man das ganze durch 12 mol, erhält man:

Daher haben die Masse eines Teilchens in u und dessen molare Masse in formula_12 den gleichen Zahlenwert. Der Umkehrschluss gilt in dieser Form nur für Reinelemente. Bei Mischelementen gibt es dagegen nicht "die" Masse eines Teilchens, da es Teilchen unterschiedlicher Masse gibt.



</doc>
<doc id="118" url="https://de.wikipedia.org/wiki?curid=118" title="Anglizismus">
Anglizismus

Als Anglizismus bezeichnet man eine Ausdrucksweise oder eine Bedeutung aus der englischen Sprache, die in eine andere Sprache eingeflossen ist. Betroffen davon sind alle Bereiche eines Sprachsystems, von der Lautung über die Formenlehre, Syntax, Semantik bis zum Wortschatz, sowie die Bereiche Sprachgebrauch und Sprachebene (Fachsprache, Alltagssprache, Slang und anderes).

Findet die Übernahme Akzeptanz von Seiten der Sprachgemeinschaft, werden die Ausdrücke als Fremd- und Lehnwort bzw. als neue Bedeutung eines deutschen Wortes oder als neue Satzkonstruktion übernommen. Werden die englischen Einflüsse nicht allgemein akzeptiert, etwa weil sie auf einen Jargon oder die Jugendsprache beschränkt sind, spricht man von Neudeutsch oder abwertend von Denglisch. Im Laufe des Generationenwechsels kann sich sowohl diese Wertung als auch der Gebrauch von Anglizismen ändern. Insbesondere in der Jugendsprache verschwinden viele Ausdrücke mit der nächsten Generation wieder, da sie nicht mehr als neu und der Jugend vorbehalten empfunden werden.

Der Begriff Anglizismus umfasst alle englischen Sprachvarietäten; Einflüsse speziell aus dem britischen Englisch werden auch "Britizismen" und solche aus dem amerikanischen Englisch "Amerikanismen" genannt.

Im Deutschen treten Anglizismen am häufigsten auf der lexikalischen Ebene in Erscheinung. Man kann folgende Phänomene unterscheiden:

Weitere Übernahmeerscheinungen sind auf anderen Sprachebenen zu verzeichnen:

Sprachwissenschaftliche Untersuchungen der Universität Bamberg stellen anhand von Material aus der Zeitung Die Welt eine Zunahme von Anglizismen in der deutschen Sprache fest. So hat sich von 1994 bis 2004 die Verwendung von Anglizismen
Entgegen der allgemeinen Annahme, dass es beim Sprachkontakt vorwiegend zur Übernahme von Substantiven komme, wurden im untersuchten Zeitraum insgesamt etwa gleich viele Wörter aus jeder dieser drei Wortarten vom Englischen ins Deutsche entlehnt, allerdings bleiben die Substantive durchschnittlich länger im Gebrauch erhalten.

Die Anzahl der Anglizismen hat zugenommen; ebenso die Häufigkeit, mit der diese verwendet werden. Klassifiziert man die Anglizismen nach Bereichen, lässt sich feststellen, dass der Bereich „Wirtschaft“ am stärksten gewachsen ist, vor allem im Marketing und Vertrieb (siehe Geml/Lauer, 2008). Einzige Ausnahme bildet der Bereich „Wissenschaft und Technik“, in welchem eine Abnahme um den Faktor 1,6 zu verzeichnen ist. Insgesamt lässt sich festhalten, dass der Gebrauch von Anglizismen in zehn Jahren um den Faktor 1,7 zugenommen hat. Hingegen hat die Entlehnungshäufigkeit im Vergleich zum Zeitraum 1954–1964 abgenommen. Das heißt, es werden mehr Anglizismen verwendet, die Geschwindigkeit der Übernahme hat aber abgenommen. Der Grund hierfür könnte ein Sättigungsprozess sein.

In einer weiteren Untersuchung wurde ein großes Textkorpus der Gegenwart (1995–2004) mit insgesamt 381191 Lemmata ausgewertet; darunter wurden 13301 = 3,5 % Anglizismen festgestellt. Das Textkorpus hat einen Umfang von rund 10.3 Millionen Token (= einzelne Wortformen), darunter 52647 = 0,5 % Anglizismen. Von den 13301 Anglizismen sind 12726 (95,68 %) (48190 Token = 91,53 %) Substantive, 307 (2,30 %) (1654 Token = 3,14 %) Adjektive, 255 (1,92 %) (2371 Token = 4,50 %) Verben und 13 (0,10 %) (432 Token = 0,82 %) Adverbien.

Angaben dazu, wann welcher Anglizismus ins Deutsche gelangt ist, kann man vor allem aus Herkunftswörterbüchern (= etymologischen Wörterbüchern) gewinnen. Sie haben den Nachteil, dass sie nur einen Kernbestand des Wortschatzes enthalten, und zwar vor allem den Teil, der etymologisch besonders interessant ist. Es stellt sich also die Frage, ob der Trend der Entlehnungen, der in einem solchen Wörterbuch nachweisbar ist, auch für die Gesamtsprache repräsentativ ist. Dies muss man sich bewusst machen; mangels anderer Möglichkeiten bleibt aber nichts anderes übrig, wenn man sich eine Vorstellung von dem Verlauf der Entlehnungen machen will.

Eine solche Untersuchung hat Körner am Beispiel von "Duden. Das Herkunftswörterbuch 2001" durchgeführt, indem sie alle Entlehnungen erfasste, für die nach Auskunft dieses Wörterbuchs festgestellt werden kann, in welchem Jahrhundert sie aus welcher Sprache ins Deutsche gelangt sind. Speziell für die aus dem Englischen stammenden Entlehnungen kam Körner zu folgendem Ergebnis:

Das Wörterbuch enthält 16781 datierbare Stichwörter, darunter 5244 Entlehnungen (Lehnwörter und Fremdwörter). Unter den Entlehnungen sind 519 datierbare Anglizismen. Man sieht, dass diese Entlehnungen aus dem Englischen erst recht spät einsetzen und dann aber eine erhebliche Dynamik entwickeln. Im 20. Jahrhundert erreichen die Anglizismen 3,1 % des gesamten erhobenen Wortschatzes beziehungsweise 9,9 % der Entlehnungen.

Besonders schon vor längerer Zeit entlehnte Wörter haben eine Anpassung der Schreibweise erfahren, etwa "Keks" gegenüber älterem "Cakes". Bei vor allem über den schriftlichen Verkehr übernommenen Anglizismen kann sich die Aussprache bei gleichbleibendem Schriftbild nach deutschen Aussprachegewohnheiten richten; so wird "Jute" heute im Deutschen gewöhnlich [] ausgesprochen, während ältere Wörterbücher noch die Aussprache [] verzeichnen.

Eine repräsentative Umfrage über die Verständlichkeit von zwölf gebräuchlichen englischen Werbeslogans für deutsche Kunden ergab im Jahr 2003, dass einige der Slogans von weniger als 10 % der Befragten verstanden wurden. Acht der zwölf untersuchten Unternehmen hätten ihre Werbeslogans seitdem geändert. 2008 störten sich in einer Umfrage der Gesellschaft für deutsche Sprache 39 % der Befragten an Lehnwörtern aus dem Englischen. Die Ablehnung war in den Bevölkerungsgruppen am größten, die Englisch weder sprechen noch verstehen konnten (58 % Ablehnung bei der Gruppe der über 59-Jährigen, 46 % Ablehnung bei ostdeutschen Umfrageteilnehmern).

Die Entwicklung des Englischen zur lingua franca im 20. Jahrhundert beeinflusst die meisten Sprachen der Welt. Mitunter werden Worte ersetzt oder bei Neuerscheinungen ohne eigene Übersetzung übernommen. Diese Entwicklung wird vor allem dann skeptisch betrachtet, wenn es genügend Synonyme in der Landessprache gibt. Kritiker merken auch an, es handle sich häufig (beispielsweise bei "Handy" im Deutschen) um Scheinanglizismen.

Mitunter wird auch eine unzureichende Kenntnis der englischen Sprache für die Vermischung und den Ersatz bestehender Worte durch Scheinanglizismen verantwortlich gemacht. So sprechen einer Studie der GfK zufolge nur 2,1 Prozent der deutschen Arbeitnehmer verhandlungssicher Englisch. In der Gruppe der Unter-30-Jährigen bewerten jedoch über 54 Prozent ihre Englischkenntnisse als gut bis exzellent. Für bessere Sprachkenntnisse könne demzufolge effizienterer Englischunterricht beitragen, und statt der Ton-Synchronisation von Filmen und Serien solle eine Untertitelung der englischsprachigen Originale mit deutschem Text erfolgen. Dies würde zugleich zu einer besseren Abgrenzung zwischen den Sprachen und einer Wahrung deutscher Sprachqualität beitragen.

Im Dezember 2014 forderte der Europapolitiker Alexander Graf Lambsdorff, neben Deutsch die englische Sprache als Verwaltungs- und später als Amtssprache in Deutschland zuzulassen, um die Bedingungen für qualifizierte Zuwanderer zu verbessern, den Fachkräftemangel abzuwenden und Investitionen zu erleichtern. Einer repräsentativen YouGov-Umfrage zufolge würden es 59 Prozent der Deutschen begrüßen, wenn die englische Sprache in der gesamten Europäischen Union den Status einer Amtssprache erlangen würde.

Ähnliche Kritik wie gegenüber den Anglizismen traf bereits ab Ende des 19. Jahrhunderts die aus dem Französischen, Lateinischen oder Griechischen stammenden Begriffe. Vereine wie der Allgemeine Deutsche Sprachverein versuchten im Rahmen des deutschen Sprachpurismus, diese Begriffe durch deutsche zu ersetzen. So sind französische, lateinische oder griechische Fremdwörter durch deutsche Wortschöpfungen ersetzt worden, z. B. "Fahrkarte" für "Billet", "Abteil" für "Coupé" und "Bahnsteig" für "Perron". Im Postwesen wurden auf Geheiß Bismarcks vom Generalpostmeister Heinrich von Stephan über 700 französischsprachige Begriffe durch deutsche Neuschöpfungen ersetzt. Zwar war die damalige Öffentlichkeit empört und man verhöhnte ihn als »Generalsprachmeister«, trotzdem sind Begriffe wie "eingeschrieben", "postlagernd" und "Empfangsschein" heute in den allgemeinen Sprachgebrauch übergegangen und ersetzen die Fremdwörter "rekommandiert", "poste restante" und "Rezepisse".

Viele Unternehmen setzen Anglizismen in Stellenangeboten bzw. -beschreibungen ein. Kritiker vermuten, dass weniger attraktive Stellen dadurch aufgewertet werden sollen. Häufig verwendete Begriffe sind "Area-Manager" (weniger als der klassische Abteilungsleiter), Facility-Manager (Hausmeister), "Key Account Manager" (Betreuer wichtiger Kunden) oder "Case Manager" (ein Fallbearbeiter, siehe Fallmanagement). Um diese Entwicklung zu karikieren, wird gelegentlich der Euphemismus "WC-Manager" (Klomann/-frau) genannt. In Frankreich stoßen Lehnwörter und Anglizismen noch stärker auf Kritik und sollen auch durch gesetzgeberische Maßnahmen wie die Loi Toubon eingedämmt werden. Eine aktive Sprachpolitik, wie sie unter anderem in Frankreich und Island betrieben wird, um eine Anreicherung der Sprache mit Anglizismen zu unterbinden, findet in Deutschland seit Mitte des 20. Jahrhunderts nicht mehr statt.

Der Sprachwissenschaftler Rudolf Hoberg sah 2013 keine Bedrohung durch Anglizismen. Die deutsche Sprache habe schon immer englische Ausdrücke aufgenommen: „Nach der letzten Duden-Ausgabe haben wir etwa 3,5 Prozent Anglizismen, aber 20 Prozent andere Fremdwörter, über die sich die Leute meistens gar nicht aufregen“. Ebenso lehnt er gesetzliche Regelungen wie Sprachquoten in Frankreich oder Verfassungsänderungen wie in Österreich ab, die keine Erfolge zeigten. Der Germanist Karl-Heinz Göttert nannte die Aufregung über Anglizismen „komisch“: „Sie machen weniger als zwei Prozent des deutschen Wörterschatzes aus. Da gab und gibt es ganz andere Fremdwortschwemmen. Das Englische selbst hat im Mittelalter ein Drittel aus dem Französischen entlehnt. Und die japanische Sprache hat aus dem Chinesischen 50 Prozent übernommen.“ Sie seien „ein Beweis dafür, dass Nehmersprachen kreativ und nicht knechtisch mit dem Einfluss der Gebersprachen umgehen.“ Er wandte sich gegen eine „Leitkultur“ und kritisierte den Sprachpurismus mit den Worten: „Schon Jakob Grimm hat sich deshalb gegen den ärgerlichen Purismus gewendet. Es wäre besser, der Verein Deutsche Sprache würde sich auf die Grimm'sche Tradition besinnen, statt einen Grimm-Preis für Verdienste beim Anglizismen-Kampf zu vergeben.“

Auch rechtsextreme Organisationen wie die NPD stören sich oft an Anglizismen und versuchen beispielsweise das nicht allgemein anerkannte Wort „Weltnetz“ statt „Internet“ zu etablieren.

Siehe dazu Loi Toubon





</doc>
<doc id="120" url="https://de.wikipedia.org/wiki?curid=120" title="Astronom">
Astronom

Ein Astronom (von "ástron" ‚Stern, Gestirn‘ und νόμος "nómos" ‚Gesetz‘) ist eine (meist akademisch gebildete) Person, die sich wissenschaftlich mit der Astronomie beschäftigt.

Beschränkt man den Begriff Astronom auf jene Wissenschaftler, die sich hauptberuflich der Astronomie widmen, dann sind meist zwei der folgenden Tätigkeiten Gegenstand des Berufs:

Der Beruf des Fachastronomen setzt im Regelfall ein Hochschulstudium der Astronomie oder verwandter Naturwissenschaften voraus, etwa ein Diplom der Physik oder Astronomie (nur in Österreich), manchmal auch Studienabschlüsse aus Mathematik, Geodäsie, Aeronautik und anderen. Das Verfassen einer Dissertation schließt sich in den meisten Fällen an, denn die abgeschlossene Promotion gilt oft als Einstellungsvoraussetzung.

Das Berufsbild des Astronomen hat sich in den letzten Jahrzehnten stark gewandelt. In der Vergangenheit beobachteten Astronomen überwiegend den Himmel mittels optischer Teleskope an Sternwarten. Heute arbeiten die meisten Astronomen an sehr spezialisierten Fragestellungen und überwiegend am Computer. Sie verwenden elektromagnetische Signale aus allen Wellenlängenbereichen, von der kurzwelligen Gammastrahlung bis zu den längsten Radiowellen. Viele Messdaten werden auch über das Internet verbreitet – insbesondere bei regelmäßigen internationalen Messkampagnen wie im IVS – beziehungsweise vom Netz übernommen.

Daher arbeiten Astronomen heute kaum mehr am Fernrohr selbst, sondern nur einen vergleichsweise kurzen Teil ihrer Arbeitszeit in den Kontrollräumen der Sternwarten. Die dort gewonnenen Daten werden meist außerhalb der Nachtdienste ausgewertet und aufbereitet. Immer mehr gewinnt das so genannte „service mode observing“ (Beobachtung auf Abruf) an Bedeutung: es werden nur Beobachtungsziel und -art angegeben werden, während die Beobachtungen unabhängig oder automatisiert an den Teleskopen beziehungsweise von Erdsatelliten durchgeführt werden.

Da viele Studenten des Faches später auf anderen Gebieten arbeiten, hängt es von ihrem Selbstverständnis ab, ob sie sich auch weiterhin als Astronom bezeichnen. Inwieweit wissenschaftlich tätige Amateurastronomen als Astronomen im eigentlichen Sinn zu nennen sind, ist ebenfalls offen. Besonders in früheren Jahrhunderten ist eine Trennung zwischen Fachastronom und Amateur wenig zweckmäßig, wie etwa das Beispiel von Wilhelm Olbers zeigt.

Da die Astronomie nach wie vor eine Wissenschaft ist, die auch im professionellen Bereich von einzelnen und kleinen Forschungsgruppen geprägt ist, haben auch Amateure mit der entsprechenden Begabung und Ausrüstung die Möglichkeit, mitzuwirken. Amateure sind oft dort erfolgreich, wo eine kontinuierliche Beobachtung notwendig ist, aber wegen der Kosten durch Großteleskope kaum professionell machbar ist, etwa die Asteroiden- und Kometenüberwachung oder auf dem Gebiet veränderlicher Sterne sowie der Astrometrie.

Da Astronomen naturgemäß – wenn auch durch die moderne Beobachtungs- und Informationstechnik nicht mehr so ausgeprägt wie früher – ihre Tätigkeit in der Nacht ausüben, erfordert die Arbeit eines Berufsastronomen klare Regelungen zur Dienstzeit.

Die Zeiten des „astronomischen Schlafmangels“, über die auch berühmte Astronomen manchmal in ihren Briefen oder Berichten geklagt haben, sind allerdings größtenteils vorüber. Moderne Sternwarten sind meistens mit Technologien ausgerüstet, die ein gewisses Maß an Fernbedienung erlauben oder sogar international anbieten, wie z. B. einige Observatorien auf Hawaii oder ESO-Sternwarten wie in Chile. Da visuelle Messungen oder Kontrollen nur mehr selten erforderlich sind und elektro-optische Sensoren grundsätzlich auch eine Funktionskontrolle über EDV oder über das Internet erlauben, werden durchgehend nächtliche Arbeitszeiten zunehmend seltener.




</doc>
<doc id="121" url="https://de.wikipedia.org/wiki?curid=121" title="Alan Turing">
Alan Turing

Alan Mathison Turing OBE, FRS [] (* 23. Juni 1912 in London; † 7. Juni 1954 in Wilmslow, Cheshire) war ein britischer Logiker, Mathematiker, Kryptoanalytiker und Informatiker. Er gilt heute als einer der einflussreichsten Theoretiker der frühen Computerentwicklung und Informatik. Turing schuf einen großen Teil der theoretischen Grundlagen für die moderne Informations- und Computertechnologie. Als richtungsweisend erwiesen sich auch seine Beiträge zur theoretischen Biologie.

Das von ihm entwickelte Berechenbarkeitsmodell der Turingmaschine bildet eines der Fundamente der Theoretischen Informatik. Während des Zweiten Weltkrieges war er maßgeblich an der Entzifferung der mit der „Enigma“ verschlüsselten deutschen Funksprüche beteiligt. Der Großteil seiner Arbeiten blieb auch nach Kriegsende unter Verschluss.

Turing entwickelte 1953 eines der ersten Schachprogramme, dessen Berechnungen er mangels Hardware selbst durchführte. Nach ihm benannt sind der Turing Award, die bedeutendste Auszeichnung in der Informatik, sowie der Turing-Test zum Überprüfen des Vorhandenseins von künstlicher Intelligenz.

Im März 1952 wurde Turing wegen seiner Homosexualität, die damals noch als Straftat verfolgt wurde, zur chemischen Kastration verurteilt. Turing erkrankte in Folge der Hormonbehandlung an einer Depression und starb etwa zwei Jahre später durch Suizid. Im Jahr 2009 sprach der damalige britische Premierminister Gordon Brown eine offizielle Entschuldigung im Namen der Regierung für die „entsetzliche Behandlung“ Turings aus und würdigte dessen „außerordentliche Verdienste“ während des Krieges; eine Begnadigung wurde aber noch 2011 trotz einer Petition abgelehnt. Am Weihnachtstag, dem 24. Dezember 2013 sprach Königin Elisabeth II. posthum ein „Royal Pardon“ (Königliche Begnadigung) aus.

Alan Turings Vater Julius Mathison Turing war britischer Beamter beim Indian Civil Service. Er und seine Frau Ethel Sara (geborene Stoney) wünschten, dass ihre Kinder in Großbritannien aufwachsen. Deshalb kehrte die Familie vor Alans Geburt aus Chatrapur, damals Britisch-Indien, nach London-Paddington zurück, wo Alan Turing am 23. Juni 1912 zur Welt kam. Da der Staatsdienst seines Vaters noch nicht beendet war, reiste dieser im Frühjahr 1913 erneut nach Indien, wohin ihm seine Frau im Herbst folgte. Turing und sein älterer Bruder John wurden nach St. Leonards-on-the-Sea, Hastings, in die Familie eines pensionierten Obersts und dessen Frau in Pflege gegeben. In der Folgezeit pendelten die Eltern zwischen England und Indien, bis sich Turings Mutter 1916 entschied, längere Zeit in England zu bleiben, und die Söhne wieder zu sich nahm.

Schon in frühester Kindheit zeigte sich die hohe Begabung und Intelligenz Turings. Es wird berichtet, dass er sich innerhalb von drei Wochen selbst das Lesen beibrachte und sich schon früh zu Zahlen und Rätseln hingezogen fühlte.

Im Alter von sechs Jahren wurde Turing auf die private Tagesschule St. Michael’s in St. Leonards-on-the-Sea geschickt, wo die Schulleiterin frühzeitig seine Begabung bemerkte. 1926, im Alter von 14 Jahren, wechselte er auf die Sherborne School in Dorset. Sein erster Schultag dort fiel auf einen Generalstreik in England. Turing war jedoch so motiviert, dass er die 100 Kilometer von Southampton zur Schule allein auf dem Fahrrad zurücklegte und dabei nur einmal in der Nacht an einer Gaststätte Halt machte; so berichtete jedenfalls die Lokalpresse.

Turings Drang zur Naturwissenschaft traf bei seinen Lehrern in Sherborne auf wenig Gegenliebe; sie setzten eher auf Geistes- als auf Naturwissenschaften. Trotzdem zeigte Turing auch weiterhin bemerkenswerte Fähigkeiten in den von ihm geliebten Bereichen. So löste er für sein Alter fortgeschrittene Aufgabenstellungen, ohne zuvor irgendwelche Kenntnisse der elementaren Infinitesimalrechnung erworben zu haben.

Im Jahr 1928 stieß Turing auf die Arbeiten Albert Einsteins. Er verstand sie nicht nur, sondern entnahm einem Text selbständig Einsteins Bewegungsgesetz, obwohl dieses nicht explizit erwähnt wurde.

Turings Widerstreben, für Geisteswissenschaften genauso hart wie für Naturwissenschaften zu arbeiten, hatte zur Folge, dass er einige Male durch die Prüfungen fiel. Weil dies seinen Notendurchschnitt verschlechterte, musste er 1931 auf ein College zweiter Wahl gehen, das King’s College, Cambridge, entgegen seinem Wunsch, am Trinity College zu studieren. Er studierte von 1931 bis 1934 unter Godfrey Harold Hardy (1877–1947), einem respektierten Mathematiker, der den Sadleirian Chair in Cambridge innehatte, das zu der Zeit ein Zentrum der mathematischen Forschung war.

In seiner für diesen Zweig der Mathematik grundlegenden Arbeit "On Computable Numbers, with an Application to the “Entscheidungsproblem”" (28. Mai 1936) formulierte Turing die Ergebnisse Kurt Gödels von 1931 neu. Er ersetzte dabei Gödels universelle, arithmetisch-basierte formale Sprache durch einen einfachen gedanklichen Mechanismus, eine abstrakt-formale Zeichenketten verarbeitende mathematische Maschine, die heute unter dem Namen Turingmaschine bekannt ist. („Entscheidungsproblem“ verweist auf eines der 23 wichtigsten offenen Probleme der Mathematik des 20. Jahrhunderts, vorgestellt von David Hilbert 1900 auf dem 2. Internationalen Mathematiker-Kongress in Paris („Hilbertsche Probleme“).) Turing bewies, dass solch ein Gerät in der Lage ist, „jedes vorstellbare mathematische Problem zu lösen, sofern dieses auch durch einen Algorithmus gelöst werden kann“.

Turingmaschinen sind bis zum heutigen Tag einer der Schwerpunkte der Theoretischen Informatik, nämlich der Berechenbarkeitstheorie. Mit Hilfe der Turingmaschine gelang Turing der Beweis, dass es keine Lösung für das Entscheidungsproblem gibt. Er zeigte, dass die Mathematik in gewissem Sinne unvollständig ist, weil es allgemein keine Möglichkeit gibt festzustellen, ob eine beliebige syntaktisch korrekt gebildete mathematische Aussage beweisbar oder widerlegbar ist. Dazu bewies er, dass das Halteproblem für Turingmaschinen nicht lösbar ist, d. h., dass es nicht möglich ist, algorithmisch zu entscheiden, ob eine Turingmaschine, angesetzt auf eine Eingabe (initiale Bandbelegung), jemals zum Stillstand kommen wird, das heißt die Berechnung terminiert. Turings Beweis wurde erst nach dem von Alonzo Church (1903–1995) mit Hilfe des Lambda-Kalküls geführten Beweis veröffentlicht; unabhängig davon ist Turings Arbeit beträchtlich einfacher und intuitiv zugänglich. Auch war der Begriff der „Universellen (Turing-) Maschine“ neu, einer Maschine, welche jede beliebige andere Turing-Maschine simulieren kann. Die Eingabe für diese Maschine sind also ein verschlüsseltes Programm, das von der universellen Maschine interpretiert wird, und der Startwert, auf den es angewendet werden soll.

Alle bis heute definierten Berechenbarkeitsbegriffe haben sich (bis auf die Abbildung von Worten auf Zahlen und umgekehrt) als äquivalent erwiesen.

1938 und 1939 verbrachte Turing zumeist an der Princeton University und studierte dort unter Alonzo Church. 1938 erwarb er den Doktortitel in Princeton. Seine Doktorarbeit führte den Begriff der „Hypercomputation“ ein, bei der Turingmaschinen zu sogenannten Orakel-Maschinen erweitert werden. So wurde das Studium von nicht-deterministisch lösbaren Problemen ermöglicht.

Nach seiner Rückkehr nach Cambridge im Jahr 1939 besuchte Turing Vorlesungen des österreichisch-britischen Philosophen Ludwig Wittgenstein (1889–1951) über die Grundlagen der Mathematik. Die beiden diskutierten und stritten vehement: Turing verteidigte den mathematischen Formalismus, während Wittgenstein der Meinung war, dass Mathematik überbewertet sei und keine absolute Wahrheit zutage bringen könne.

Während des Zweiten Weltkriegs war Turing einer der herausragenden Wissenschaftler bei den erfolgreichen Versuchen in Bletchley Park, verschlüsselte deutsche Funksprüche zu entziffern. Er steuerte einige mathematische Modelle bei, um sowohl die Enigma (siehe auch: Letchworth-Enigma) als auch die Lorenz-Schlüsselmaschine (siehe auch: "Turingery") zu brechen. Die Einblicke, die Turing bei der Kryptanalyse der "Fish"-Verschlüsselungen gewann, halfen später bei der Entwicklung des ersten digitalen, programmierbaren elektronischen Röhrencomputers ENIAC. Konstruiert von Max Newman und seinem Team und gebaut in der "Post Office Research Station" in Dollis Hill von einem von Tommy Flowers angeführten Team im Jahr 1943, entzifferte Colossus die Lorenz-Maschine. Auch konzipierte Turing die nach ihm benannten "Bombes". Sie waren Nachfolgerinnen der von dem Polen Marian Rejewski entwickelten "Bomba" und dienten zur Ermittlung der Schlüssel von Enigma-Nachrichten. Dabei handelte es sich um ein elektromechanisches Gerät, das im Prinzip mehrere Enigma-Maschinen beinhaltete und so in der Lage war, viele mögliche Schlüsseleinstellungen der Enigma-Nachrichten durchzutesten und zu eliminieren, bis eine mögliche Lösung gefunden war ("Reductio ad absurdum"; ).

Turings Mitwirkung als einer der wichtigsten Codeknacker bei der Entzifferung der Enigma war bis in die 1970er Jahre geheim; nicht einmal seine engsten Freunde wussten davon. Die Entzifferung geheimer deutscher Funksprüche war eine kriegsentscheidende Komponente für den Sieg der Alliierten im U-Boot-Krieg und im Afrikafeldzug.

Von 1945 bis 1948 war Turing im "National Physical Laboratory" in Teddington tätig, wo er am Design der ACE ("Automatic Computing Engine") arbeitete. Der Name der Maschine ist abgeleitet von der Analytical Engine des Mathematikers Charles Babbage, dessen Werk Turing zeitlebens bewunderte.

Ab 1948 lehrte Turing an der Universität Manchester und wurde im Jahr 1949 stellvertretender Direktor der Computerabteilung. Hier arbeitete er an der Software für einen der ersten echten Computer, den Manchester Mark I und gleichzeitig weiterhin verschiedenen theoretischen Arbeiten. In "Computing machinery and intelligence" ("Mind", Oktober 1950) griff Turing die Problematik der künstlichen Intelligenz auf und schlug den Turing-Test als Kriterium vor, ob eine Maschine dem Menschen vergleichbar denkfähig ist. Da der Denkvorgang nicht formalisierbar ist, betrachtet der Test nur die Antworten einer Maschine im Dialog mit einem Menschen, d. h. das kommunikative Verhalten der Maschine. Wenn dieses von einem menschlichen Verhalten nicht unterscheidbar erscheint, soll von maschineller Intelligenz gesprochen werden. Er beeinflusste durch die Veröffentlichung die Entwicklung der Künstlichen Intelligenz maßgeblich.

1952 schrieb er ein Schachprogramm. Da es keine Computer mit ausreichender Leistung gab, um es auszuführen, übernahm Turing dessen Funktion und berechnete jeden Zug selbst. Dies dauerte bis zu 30 Minuten pro Zug. Das einzige schriftlich dokumentierte Spiel verlor er gegen einen Kollegen.

Von 1952 bis zu seinem Tod 1954 arbeitete Turing an mathematischen Problemen der theoretischen Biologie. Er veröffentlichte 1952 eine Arbeit zum Thema "The Chemical Basis of Morphogenesis". In diesem Artikel wurde erstmals ein Mechanismus beschrieben, wie Reaktions-Diffusions-Systeme spontan Strukturen entwickeln können. Dieser als Turing-Mechanismus bekannte Prozess steht noch heute im Mittelpunkt vieler chemisch-biologischer Strukturbildungstheorien. Turings weiteres Interesse galt dem Vorkommen der Fibonacci-Zahlen in der Struktur von Pflanzen. Spätere Arbeiten blieben bis zur Veröffentlichung seiner gesammelten Werke 1992 unveröffentlicht.

1952 half der 19-jährige Arnold Murray, zu dem Turing eine gleichgeschlechtliche Beziehung hatte, einem Komplizen dabei, in Turings Haus einzubrechen. Turing meldete daraufhin einen Diebstahl bei der Polizei, die ihm als Folge der Ermittlungen eine sexuelle Beziehung zu Murray vorwarf. Da homosexuelle Handlungen zu dieser Zeit – wie in den meisten anderen Ländern – in England strafbar waren, wurde Turing wegen „grober Unzucht und sexueller Perversion“ angeklagt. Turing sah keinen Anlass, sich wegen dieser Vorwürfe zu rechtfertigen.

Nach seiner Verurteilung zu einer Gefängnisstrafe wurde er vor die Wahl gestellt, die Haftstrafe anzutreten oder – da zu seiner Zeit Homosexualität von weiten Teilen der Psychiatrie als Krankheit angesehen wurde – sich behandeln zu lassen. Er entschied sich für die ärztliche Behandlung, zu der auch eine medikamentöse Behandlung mit dem Hormon Östrogen gehörte. Diesem wurde eine triebhemmende Wirkung zugeschrieben. Diese dauerte ein Jahr und führte zu Nebenwirkungen wie der Vergrößerung der Brustdrüse. Auch wenn er seine körperlichen Veränderungen mit Humor kommentierte, muss die Verweiblichung seiner Konturen den sportlichen Läufer und Tennisspieler schwer getroffen haben. Turing erkrankte an einer Depression. Im Herbst 1952 begann Turing seine Therapie bei dem aus Berlin stammenden und seit 1939 in Manchester lebenden Psychoanalytiker Franz Greenbaum. Dieser war ein Anhänger C.G. Jungs und war ihm von Freunden als für seinen Fall verständnisvoll empfohlen worden. Turing entwickelte auch ein freundschaftliches Verhältnis zur Familie Greenbaum, die er auch privat besuchte.

1954 starb Turing, wahrscheinlich entsprechend der offiziellen Feststellung durch Suizid, an einer Cyanidvergiftung, dem Anschein nach von einem vergifteten Apfel herrührend, den man halb aufgegessen neben ihm auffand. Die Ermittler versäumten es jedoch, den Apfel auf Gift untersuchen zu lassen. Es wird berichtet, dass Turing seit 1938, nachdem er den Film „Schneewittchen und die sieben Zwerge“ gesehen hatte, immer wieder die Verse "Dip the apple in the brew / Let the sleeping death seep through" („Tauch den Apfel tief hinein / bis das Gift wird in ihm sein“, in der deutschen Version des Films: „Apfel färbt sich strahlend rot / lockt Schneewittchen in den Tod“) sang. Der These, dass Turings Tod ein Unfall im Zusammenhang mit einem chemischen Versuch war, wird von Andrew Hodges, einem seiner Biographen, entschieden widersprochen. Unter seinen Biographen ist die Annahme verbreitet, die Auswirkungen der Hormonbehandlung seien die Hauptursache für den Suizid gewesen.

Ab etwa den späten 2000er Jahren unternahmen britische Bürger eine Reihe von öffentlichkeitswirksamen Aktivitäten, um das von Turing erlittene Unrecht bekannt zu machen und seine formale Rehabilitierung zu erreichen, also einen Widerruf oder eine Aufhebung des damaligen Urteils. Dies führte im Jahr 2013 zum Erfolg.

Im Jahr 2009 unterzeichneten rund 30.000 Briten eine bei der Regierung eingereichte Online-Petition, in der eine posthume Entschuldigung von der britischen Regierung gefordert wurde. Der Initiator der Kampagne, der britische Programmierer John Graham-Cumming, regte an, Alan Turing den Titel „Sir Alan“ zu verleihen. Am 10. September 2009 veröffentlichte der damalige britische Premierminister Gordon Brown eine Erklärung, in der er, im Namen der britischen Regierung, die Verfolgung Turings bedauerte und seinen außerordentlichen Beitrag während des Zweiten Weltkriegs würdigte. Dabei spielte er auch auf den strategischen Vorteil der Alliierten durch die Entschlüsselung der „Enigma“ an und unterstrich deren Bedeutung:

Da die Strafverfolgung seiner sexuellen Ausrichtung damals gesetzeskonform war, wurde eine nachträgliche Aufhebung der Verurteilung Turings zunächst von offizieller Seite als unmöglich dargestellt. Noch 2012 weigerte sich die Regierung von Browns Nachfolger David Cameron, 49.000 Homosexuelle, die nach dem "Criminal Law Amendment Act" von 1885 verurteilt worden waren, postum zu rehabilitieren.

Im Jahr 2013 wurde bekannt, dass die britische Regierung die Absicht hatte, Turing zu rehabilitieren. Das Oberhausmitglied John Sharkey, Baron Sharkey beantragte dies. Das konservative Mitglied des Oberhauses Tariq Ahmad, Baron Ahmad of Wimbledon kündigte die Zustimmung der Regierung an. Der Liberaldemokrat Sharkey hatte in den 1960er Jahren in Manchester Mathematik bei Turings einzigem Doktoranden Robin Gandy studiert. Eine dritte Lesung des Antrags beraumte die Regierung für Ende Oktober an.

Am 24. Dezember 2013 wurde Alan Turing durch ein allein dem Monarchen zustehendes besonderes Gnadenrecht durch ein sogenanntes "Royal Pardon" begnadigt. Justizminister Chris Grayling hatte diese Begnadigung bei Elisabeth II. beantragt. Turing gilt damit auch als offiziell rehabilitiert.

Im April 2016 entschuldigte sich Robert Hannigan, der damalige Leiter des britischen Geheimdienstes GCHQ, für die Behandlung von Homosexuellen durch seine Institution und bezog dies ausdrücklich auf Alan Turing.

Anfang 2015 verlangten Mitglieder der Familie Alan Turings unter weiterer, teils prominenter Unterstützung (Stephen Fry, Turing-Darsteller Benedict Cumberbatch) in einer Petition an das britische Parlament die Rehabilitation auch aller anderen der in England unter den Homosexuellen-Gesetzen Verurteilten. Die Petition wurde von ca. 500.000 Personen unterschrieben und sollte von Turings Großneffen Nevil Hunt und der Großnichte Rachel Barns überreicht werden.

Am 21. Oktober 2016 lehnte das britische Parlament einen Gesetzesentwurf ab, der eine Rehabilitation in Form einer generellen Rehabilitation aller lebenden, früher für Homosexualität verurteilten Personen vorsah. Dieser Gesetzesentwurf ging einigen zu weit, anderen nicht weit genug. Am 31. Januar 2017 wurde von Königin Elisabeth II. ein Gesetz in Kraft gesetzt, das aufbauend auf der Begnadigung von Turing allen Männern die Strafe aufhebt, falls zu dem Zeitpunkt beide über 16 Jahre alt waren, als sie den geahndeten Akt in gegenseitigem Einvernehmen vollzogen. Ausgenommen sind weiterhin nur Verurteilungen wegen homosexueller Handlungen in öffentlichen Toiletten. Das Gesetz schließt auch bereits verstorbene Personen ein. Ein noch lebender Betroffener kann beantragen, dass die Strafe aus seiner polizeilichen Führungsakte gestrichen wird und Historiker können darauf hinweisen, dass eine Verurteilung verstorbener Personen nach geltendem Recht ungültig ist. Das Gesetz, das von Justizminister Sam Gyimah als „Turings Gesetz“ bezeichnet wurde, ist eine Ergänzung zum "Policing and Crime Act" und nimmt keinen Bezug auf andere Gesetze, unter denen homosexuelle Handlungen verfolgt werden konnten. Von Michael Cashman, einem der Initiatoren des Gesetzes, wurden jedoch weitere Vereinbarungen abgesichert, die einen entsprechend umfassenden Straferlass für alle homosexuellen Handlungen ermöglichen.

Am 2. März 1999 wurde der Asteroid (10204) Turing nach ihm benannt.

Eine Turing-Statue wurde am 23. Juni 2001 in Manchester enthüllt. Sie steht im Sackville Park, zwischen den wissenschaftlichen Gebäuden der Universität Manchester und dem bei Homosexuellen beliebten Viertel der Canal Street.

An seinem 50. Todestag, dem 7. Juni 2004, wurde zum Gedenken an Turings frühzeitigen Tod eine Tafel an seinem früheren Haus „Hollymeade“ in Wilmslow enthüllt.

Der Turing Award wird jährlich von der "Association for Computing Machinery" an Personen verliehen, die bedeutende Beiträge zur Informatik geleistet haben. Er wird weithin als „Nobelpreis“ der Informatik angesehen.

Der "Bletchley Park Trus"t hat am 19. Juni 2007 eine Statue Turings in Bletchley Park enthüllt. Die Skulptur wurde von Stephen Kettle gestaltet, der als Material für sein Kunstwerk walisischen Schiefer verwendete.

Im „Turing-Jahr 2012“ fanden zu Alan Turings hundertstem Geburtstag weltweit Veranstaltungen zur Würdigung seiner Leistungen und zum Gedenken daran statt.

Im Jahr 2014 wurde er in die "Hall of Honor" (Ehrenhalle) des US-Geheimdienstes NSA ("National Security Agency") aufgenommen.


Wichtige Veröffentlichungen

Englische Ausgaben

Deutsche Ausgabe und Übersetzungen






Artikel


</doc>
<doc id="123" url="https://de.wikipedia.org/wiki?curid=123" title="Archäologie">
Archäologie

Die Archäologie ( und λόγος "lógos" ‚Lehre‘; wörtlich also „Lehre von den Altertümern“) ist eine Wissenschaft, die mit naturwissenschaftlichen und geisteswissenschaftlichen Methoden die kulturelle Entwicklung der Menschheit erforscht. Sie hat sich weltweit zu einem Verbund unterschiedlichster theoretischer und praktischer Fachrichtungen entwickelt.

Die Archäologie interessiert sich ausschließlich für den Menschen und seine materiellen Hinterlassenschaften, wie etwa Gebäude, Werkzeuge und Kunstwerke. Sie umfasst einen Zeitraum von den ersten Steinwerkzeugen vor etwa 2,5  Millionen Jahren bis in die nähere Gegenwart. Aufgrund neuer Funde in Afrika, die etwa 3,3 Millionen Jahre alt sind, wird auch ein deutlich früherer Beginn der Werkzeugherstellung in Betracht gezogen. Materielle Hinterlassenschaften der jüngsten Geschichte (beispielsweise Konzentrationslager und Bunkerlinien aus dem Zweiten Weltkrieg) werden heute ebenfalls mit archäologischen Methoden ausgewertet, auch wenn dieser Ansatz einer „zeitgeschichtlichen“ Archäologie fachintern umstritten ist.

Obwohl die Archäologie eine verhältnismäßig junge Wissenschaft ist, ist es kaum mehr möglich, alle Zeiträume zu überblicken, so dass sich verschiedene Fachrichtungen herausbildeten. Dabei können die Epochen regional unterschiedlich datiert sein, teilweise sind sie nicht überall dokumentierbar. Neben der Orientierung an Epochen (z. B. Mittelalterarchäologie) oder Regionen (z. B. Vorderasiatische Archäologie) gibt es auch die Spezialisierung auf bestimmte Themengebiete (z. B. Christliche Archäologie, Rechtsarchäologie).

Obwohl die Methodik sich großteils ähnelt, können die Quellen unterschiedlich sein. In der Vor- und Frühgeschichte hat man es hauptsächlich mit materieller Kultur zu tun, in der Frühgeschichte kann dabei teils auf Schriftquellen zurückgegriffen werden. Diese stehen für Archäologen im Gegensatz zu Wissenschaftlern anderer Teildisziplinen der Geschichtswissenschaft aber nicht im Mittelpunkt.
Erkenntnisse zu Umwelt, Klima, Ernährung oder zum Alter von Funden tragen zur Rekonstruktion vergangener Kulturen bei.

In Europa entwickelte sich die Archäologie um 1450, weil man Zeugnisse für die in den Quellen der Antike geschilderten Ereignisse finden wollte. Cyriacus von Ancona (* um 1391; † um 1455), ein italienischer Kaufmann und Humanist, gilt als einer der Gründungsväter der modernen klassischen Archäologie.

Die in der Renaissance einsetzende Wiedergeburt klassisch-antiker Gelehrsamkeit führt im 15. und 16. Jahrhundert zu einem gesteigerten Interesse an griechischen und römischen Altertümern und zu einer Welle der Sammelleidenschaft antiker Kunstgegenstände. Doch auch weniger reisefreudige Gelehrte beginnen sich für die vorhandenen Zeugnisse vergangener Zeiten zu interessieren.

Ab Mitte des 16. Jahrhunderts tritt an die Stelle der Sammelleidenschaft die akribische Erfassung der Denkmäler. In dieser Zeit werden zahlreiche Enzyklopädien und Kataloge veröffentlicht, welche im späten 16. Jahrhundert vielfach mit Kupferstichen und Holzschnitten illustriert werden. In England veröffentlicht William Camden (1551–1632) im Jahre 1586 seine "Britannia", einen Katalog der sichtbaren Altertümer. Bemerkenswert ist, dass er bereits Bewuchsmerkmale in Kornfeldern bemerkt und als solche interpretiert.

Michele Mercati (1541–1593) gilt als der erste europäische Gelehrte, der Steinwerkzeuge als solche einstufte; sein Werk wird jedoch erst 1717 veröffentlicht. Trotz großer Popularität hat die Archäologie als Wissenschaft noch keinen Stellenwert, denn es herrscht die Ansicht vor, dass ausschließlich historische Quellen und die Bibel zur Interpretation der Vergangenheit geeignet seien. So gilt es noch lange als ein Faktum, dass – wie James Ussher aus der Bibel ableitete – die Menschheit im Oktober 4004 v. Chr. entstand. 1655 wagt es Isaac de La Peyrère, die sogenannten Donnerkeile (Steinzeitartefakte) menschlichen Aktivitäten zuzuordnen, welche vor Adam lebten (Präadamiten-Hypothese). Nach einer Intervention der Inquisition widerruft er seine Theorie.

In Skandinavien werden Bodendenkmäler schon früh beachtet. Bereits 1588 gräbt man einen Dolmen bei Roskilde aus. Im Jahre 1662 erhält Uppsala einen Lehrstuhl für Altertumskunde. 1685 wird in Houlbec-Cocherel in Nordfrankreich eine neolithische Grabkammer ausgegraben. Sie gilt als die älteste archäologische Grabung, weil hier 1722 der erste erhaltene Grabungsbericht erstellt wurde. Der Kieler Professor Johann Daniel Major führt um 1690 umfangreiche Ausgrabungen in Jütland durch und lässt zahlreiche Hügelgräber öffnen. Sein Ziel ist es, die Herkunft der Einwohner der Halbinsel mit archäologischen Methoden zu klären.

Bernard de Montfaucons "L’Antiquité expliquée" erscheint ab 1719. In zehn Bänden stellt er Kunstgegenstände aus dem Mittelmeerraum dar. Montfaucons Werk bleibt für lange Zeit das Standardwerk.

Archäologische Forschungsmethoden setzten sich nun sukzessiv durch. Oftmals trafen einzelne Gelehrte schon früh bahnbrechende Schlussfolgerungen, welche aber oft – da noch nicht zeitgemäß – unbeachtet blieben. Einer der Bahnbrecher war der französische Amateurarchäologe Jacques Boucher de Perthes, der als Erster prähistorische Artefakte richtig zuordnete, wofür ihm aber erst mehr als 20 Jahre später, durch die Bestätigung Charles Lyells (1797–1875), Anerkennung zuteilwurde. Eine wichtige Erkenntnis war die Entdeckung des stratigraphischen Prinzips. Bereits lange vorher war die Zusammengehörigkeit und somit Gleichaltrigkeit von Funden, welche sich in einer Schicht befanden (beispielsweise ein Steinartefakt im Fundzusammenhang mit einer ausgestorbenen Tierart), immer wieder diskutiert worden, wurde aber nicht allgemein akzeptiert.

Ein Modell, das in seinen Grundzügen noch heute gilt, wurde von 1836 von Christian Jürgensen Thomsen veröffentlicht. Er war Kurator in Kopenhagen und erfand das „Dreiperiodensystem“, das die Vorgeschichte der Menschheit in drei Phasen einteilt, nämlich die Steinzeit, die Bronzezeit und die Eisenzeit. Etwa 30 Jahre später, um 1865, unterschied J. Lubbock die Steinzeit noch in die des geschlagenen und die des geschliffenen Steins. Die Begriffe „Paläolithikum“ (Altsteinzeit) und „Neolithikum“ („Neusteinzeit“/Jungsteinzeit) waren geboren. Die Epochen sind in sich vielfach untergliedert, aber die damals gefundene Unterteilung gilt – mit Einschränkungen – bis heute.
Die ersten großen Ausgrabungen fanden in den antiken Städten Pompeji und Herculaneum statt. Beide waren am 24. August 79 n. Chr. durch den Ausbruch des Vesuvs ausgelöscht worden. Pompeji wurde Ende des 16. Jahrhunderts beim Bau einer Wasserleitung wiederentdeckt. 1748 begannen die Grabungen. In Herculaneum wurde erstmals 1709 gegraben, 1738 ließ Karl III. von Neapel die Stadt gezielt ausgraben. 1768 konnte das Theater, die Basilika und die Villa dei Papiri freigelegt werden.

Mit seinem "Sendschreiben von den Herculanischen Entdeckungen", der ersten archäologischen Publikation, begründete Johann Joachim Winckelmann 1762 die neue Wissenschaft der Archäologie und gilt seither als "Vater der (klassischen) Archäologie". Winckelmann ist auch der Erste, der eine Periodisierung und geschichtliche Einordnung der griechischen Kunst versucht. Seine Entwicklungsstufen (alter Stil – hoher Stil – schöner Stil – Stil der Nachahmer – Verfall der Kunst) sind durch die enthaltene Wertung jedoch überholt. Für die Verbreitung seiner Forschung und deren Rezeption in der zeitgenössischen Literatur und Kunst war der Göttinger Professor Christian Gottlob Heyne entscheidend, der mit Winckelmann korrespondierte, seine Schriften rezensierte und bekanntmachte und in seinen Vorlesungen verwendete. 1802 wurde an der Christian-Albrechts-Universität zu Kiel der erste Lehrstuhl für klassische Archäologie eingerichtet.

Die ägyptischen Baudenkmäler, allen voran die Pyramiden, waren bereits im Altertum beliebte Reiseziele (siehe Weltwunder). Im 17. Jahrhundert hatte sich die Erkenntnis durchgesetzt, dass es sich hierbei um Königsgräber handelt. Die Ägyptologie nahm mit Napoléon Bonapartes Ägypten-Feldzug 1798 ihren Anfang. In Begleitung des Heeres befanden sich auch Wissenschaftler. Von besonderer Bedeutung ist der Fund des Steins von Rosetta, welcher 1822 Jean-François Champollion die Entzifferung der Hieroglyphen ermöglichte.

Von besonderer Bedeutung für die ägyptische Archäologie ist Auguste Mariette (1821–1881), welcher ab 1858 als Direktor des ägyptischen Altertümerdienstes mehr als dreißig Fundstätten ausgrub. Seine Methoden waren brachial (beispielsweise Sprengladungen). Die Feststellung der Fundumstände und wissenschaftliche Auswertungen waren damals noch nicht festgelegt, aber er beendete die Ära der reinen Schatzsucher (so Giovanni Battista Belzoni, 1778–1823), welche zuvor zahllose Funde nach Europa geschafft hatten. Mariette selbst hat seit 1850 rund 7000 Objekte nach Paris (Louvre) gebracht. Nun setzte er sich jedoch vehement dafür ein, dass Ägyptens Altertümer nicht mehr außer Landes verschleppt wurden. Zur Aufbewahrung der Funde gründete Mariette den Vorläufer des Ägyptischen Nationalmuseums in Kairo. Karl Richard Lepsius (1810–1884) erstellte zwischen 1842 und 1845 eine umfassende Aufnahme ägyptischer und nubischer Denkmäler. 1859 wurde das Ergebnis in den zwölf Bänden der "Denkmaeler aus Aegypten und Aethiopien" veröffentlicht, welche allein 894 Farbtafeln enthalten. Um die archäologische Erforschung Griechenlands machte sich um 1840 besonders Ludwig Ross verdient, der als erster systematische Ausgrabungen auf der Akropolis von Athen durchführte.

Mitte des 19. Jahrhunderts entwickelt sich die Archäologie zunehmend zur Wissenschaft. Unterscheiden sich die Ausgräber bisher nur unwesentlich von Schatzsuchern und Grabräubern, werden nun die Grabungstechniken verfeinert, eine gute Dokumentation und exakte Einordnung der Funde immer wichtiger.

Antoine Ives Goguet (1716–1758) hat bereits 1738 die Auffassung vertreten, es müsse drei Stufen prähistorischer Technologie (Steinzeit, Bronzezeit, Eisenzeit) gegeben haben. Durchsetzen konnte sich das Dreiperiodensystem jedoch erst mit dem Dänen Christian Jürgensen Thomsen (1788–1865), welcher erstmals ein Museum (1819) nach diesem Prinzip ordnet. Sir John Lubbock (1834–1913) führt 1865 eine weitere Unterteilung der Steinzeit in Paläolithikum (Altsteinzeit) und Neolithikum (Jungsteinzeit) ein.

Erst ab 1859 wird das hohe Alter der Menschheit allgemein anerkannt. Im selben Jahr erscheint Darwins "Entstehung der Arten". Der bereits 1856 entdeckte Fund des Neandertalers, welcher von Johann Carl Fuhlrott und Hermann Schaaffhausen vergeblich als eiszeitlich eingestuft wurde, kann sich als solcher in Deutschland erst ab 1902 durchsetzen, als Rudolf Virchow stirbt, der als pathologische Autorität jede weiterführende Diskussion unterbunden hatte.

In Schweden entwickelt Oscar Montelius (1843–1921) ein System der differenzierten Typologie zur Einordnung (Periodisierung) von Fundstücken und schafft die Grundlage einer relativen Chronologie.

1853/54 werden aufgrund eines ungewöhnlich niedrigen Wasserstandes bei Obermeilen am Zürichsee hölzerne Pfeiler, Steinbeile und Keramik entdeckt. Die Siedlung wird von Ferdinand Keller untersucht. Lange Zeit glaubt man, bei diesen Feuchtbodensiedlungen habe es sich um Pfahlbauten im Wasser gehandelt. Ab den 1920er Jahren entspann sich eine heftige Diskussion um die Lage der Pfahlbauten. Es konkurrierten Ufer- und Wasserpfahlbauten. Heute weiß man, dass es Land- und Wasserpfahlbauten gab. Die neuen Untersuchungen in Hornstaad am Bodensee belegen Pfahlbauten im Wasser, bis zu 5 Meter vom Seeboden abgehoben. Rekonstruktionen (beispielsweise in Unteruhldingen am Bodensee) zeigen nicht nur die verschiedenen Lösungsvorschläge der Archäologie, sondern auch den aktuellen Forschungsstand nach den Befunden der Unterwasserarchäologie (Pfahlbaumuseum Unteruhldingen).

1846 beginnen die Ausgrabungen in Hallstatt. Die archäologische Erforschung der Kelten beginnt 1858, als Oberst Schwab die ersten Ausgrabungen in La Tène am Neuenburgersee (Schweiz) durchführt. 1872 wird die Eisenzeit Europas erstmals in eine ältere Phase (Hallstattzeit) und einer jüngeren (Latènezeit) unterteilt.

Édouard Lartet (1801–1871) untersucht 1860 eine Fundstätte in den Pyrenäen (Massat) und findet dabei auch eine Geweihspitze mit eingraviertem Bärenkopf, der erste Fund jungpaläolithischer Kunst. Später gräbt er mehrere französische Höhlenfundplätze (Gorge d’Enfer, Laugerie Haute, La Madeleine und Le Moustier) aus. Besondere Aufmerksamkeit erlangen die großartigen Höhlenmalereien, welche 1879 in der Höhle von Altamira entdeckt werden.

Die Entwicklung der Klassischen Archäologie in der zweiten Hälfte des 19. Jahrhunderts wird von Heinrich Schliemann (1822–1890) dominiert. Der Geschäftsmann und „Hobbyarchäologe“ Schliemann gilt als Begründer der Vorgeschichtsarchäologie Griechenlands und des ägäischen Raumes. 1869 gräbt er auf Ithaka und 1871 beginnt er in Hissarlik zu graben. Dort vermutet er das Troja Homers und wird recht behalten, obwohl er sich in der Bauperiode selbst täuschte. Seine Ausgrabungsmethoden waren sehr umstritten, so mancher Fachmann hält von Schliemanns Fähigkeiten nichts. Sein Ruhm stützt sich vor allem auf die wertvollen Funde (beispielsweise „Schatz des Priamos“). Seine Entdeckung prähistorischer (vorhomerischer) Kulturen und Siedlungen löst zahlreiche weitere Grabungen im ägäischen Raum aus. Lange unterschätzt wurden die durch ihn bewirkten methodischen Fortschritte, wie die Betonung der Stratigraphie oder der Einsatz der Fotografie als Mittel der archäologischen Dokumentation.

1892 erhielt der Gründer des Instituts für Ur- und Frühgeschichte an der Universität Wien, Moritz Hoernes, die erste das Gesamtgebiet der Prähistorischen Archäologie umfassende Lehrbefugnis Europas.

In Ägypten leistet ab 1880 Sir William Matthew Flinders Petrie (1853–1942) als Forscher und Ausgräber Pionierarbeit. Ein Meilenstein der archäologischen Forschung sind seine "Methoden und Ziele der Archäologie", die er 1904 veröffentlicht. Darin legt Flinders Petrie vier Prinzipien dar:

1913 erscheint der erste Band des "Handbuchs der Archäologie", Herausgeber ist Heinrich Bulle (1867–1945). Als vorbildliche Grabung dieser Zeit gilt die 1922 begonnene Ausgrabung des Gräberfeldes von Assini (Argolis), welche von schwedischen Archäologen vorgenommen wird. Der gesamte Aushub wird gesiebt und eine erstklassige Grabungsdokumentation erstellt. Der berühmteste archäologische Fund des 20. Jahrhunderts gelingt Howard Carter (1873–1939) im selben Jahr. Er findet nach sechsjähriger Suche das Grab des Tut-anch-Amun.

Pionier der Luftbildarchäologie war nach dem Ersten Weltkrieg der britische Pilot Osbert G. S. Crawford, er fotografiert vom Flugzeug aus archäologische Fundstätten in England.

Gustaf Kossinna (1858–1931) stellt 1920 seine siedlungsarchäologischen Methoden vor. Seine Interpretationen, welche den Germanen eine überragende kulturelle Bedeutung zuschreiben, dienen dem Nationalsozialismus als Beweis für die Überlegenheit der Germanen und der arischen Rasse. Die Diskreditierung in der Nachkriegszeit führte dazu, dass auf Jahrzehnte eine Anbindung archäologischer Funde an ethnische Gruppen obsolet war.

Die erste ordentliche Professur wurde 1927 in Marburg geschaffen und im folgenden Jahr mit Gero Merhart von Bernegg aus Bregenz besetzt. Er hatte sich 1924 mit "Die Bronzezeit am Jenissei" habilitiert. Bei ihm promovierten bis zu seiner Zwangspensionierung durch die Nationalsozialisten im Jahr 1942 29 Studenten, nach dem Krieg kamen fünf weitere hinzu. Ab 1950 dominierte in Deutschland die "Marburger Schule", die diese Akademiker bildeten. Gero von Merhart, wie er meist genannt wird, legte das Fach auf strenge Erfassung, Systematisierung und Katalogisierung fest und mied weitgehend die kulturgeschichtliche Deutung.

Thor Heyerdahl fuhr 1947 mit einem Floß von Südamerika nach Polynesien und kann als einer der Begründer der Experimentellen Archäologie betrachtet werden.

Im 20. Jahrhundert greift die Archäologie vermehrt auf Techniken anderer Wissenschaften zurück. Als Beispiele seien die 1949 entwickelte C-Datierung zur Datierung von organischen Stoffen und die Strontiumisotopenanalyse zur Erforschung der Wanderbewegungen der ur- und frühzeitlichen Menschen genannt. Die Archäologie hat sich zur Verbundwissenschaft entwickelt. Die Erforschung der 1991 in den Ötztaler Alpen gefundenen vorgeschichtlichen Leiche (Similaun-Mann/Ötzi) ist hierfür beispielhaft. Mit Hilfe der DNA-Analyse konnten weltweit erstmals die Verwandtschaftsbeziehungen von 40 Individuen aus einer bronzezeitlichen Begräbnisstätte in der Lichtensteinhöhle rekonstruiert werden.

Zusammen mit anderen Gedächtnisinstitutionen sind archäologische Funde und Ausgrabungsstätten das besonders sensible kulturelle Gedächtnis und oft wirtschaftliche Grundlage (z. B. Tourismus) eines Staates, einer Kommune oder einer Region. Gerade archäologische Funde und Ausgrabungsstätten haben auch politische Brisanz und sind in vielen bewaffneten modernen Konflikten des 21. Jahrhunderts als Teil des kulturellen Erbes eines der Primärziele und damit von Zerstörung und Plünderung bedroht. Oft soll dabei das kulturelle Erbe des Gegner nachhaltig beschädigt oder gar vernichtet werden beziehungsweise werden dabei archäologische Funde gestohlen und verbracht. Internationale und nationale Koordinationen hinsichtlich militärischer und ziviler Strukturen zum Schutz von archäologische Funde und Ausgrabungsstätten betreibt das Internationale Komitee vom Blauen Schild (Association of the National Committees of the Blue Shield, ANCBS) mit Sitz in Den Haag. Umfangreiche Missionen dazu gab es zum Beispiel 2011 in Agypten und in Libyen, 2013 in Syrien, 2014 in Mali bzw. im Irak und seit 2015 im Jemen.

Archäologie ist ein Sammelbegriff vieler archäologischer Disziplinen, welche meist bestimmte Zeitabschnitte oder Regionen bezeichnen. Die einzelnen Disziplinen unterscheiden sich nicht nur im behandelten Forschungsgegenstand, sondern auch in den verwendeten Methoden, z. B. bei der Unterwasserarchäologie. Daneben bilden archäologische Methoden einen Teilaspekt einer eigenständigen Wissenschaft, beispielsweise in der Forensik. In Fächern wie der Altamerikanistik oder auch der Klassischen Archäologie können die inhaltlichen Schwerpunkte nicht-archäologischer Natur sein.

Die Disziplinen der Archäologie unterscheiden sich thematisch, zeitlich und räumlich. Dementsprechend unterschiedlich sind die Quellen derer sie sich bedienen. Während in der Prähistorischen Archäologie keine oder sehr spärlich schriftliche Quellen vorliegen und man sich vorwiegend auf die materiellen Hinterlassenschaften dieses Zeitabschnitts beruft, können andere archäologische Fachrichtungen zusätzlich Schriftquellen auswerten.







Die nachfolgenden Disziplinen stellen geografische Schwerpunkte dar:















Archäologische Forschungsmethoden gliedern sich in solche der Quellenerschließung und solche der Interpretation.
In der Öffentlichkeit wird meist nur die Erschließung der Quellen zur Kenntnis genommen.
Zur Quellenerschließung zählt auch die typologische und chronologische Auswertung.
Erst nach der Quellenerschließung und Aufbereitung folgt die historische Interpretation.

Die Ausgrabung ist zwar die bekannteste Forschungsmethode, jedoch nur ein kleiner Teilbereich der archäologischen Arbeit. Die Dokumentation, Auswertung, Konservierung und Archivierung der Funde stellt den weitaus größten Teil der archäologischen Tätigkeit dar. Außerdem muss die Grabung sorgfältig vorbereitet werden.

Die Prospektion umfasst zerstörungsfreie Methoden, mit deren Hilfe eine Untersuchung potenzieller oder bekannter Fundplätze ermöglicht wird. Dazu gehören die Geländebegehung (Survey), die Luftbildarchäologie und geophysikalische Methoden (Geoelektrik, elektromagnetische Induktion, geomagnetische Kartierung sowie Bodenradar und LIDAR). Ebenfalls prospektiv einsetzen lässt sich die Phosphatanalyse.

Eingeleitet wird eine Ausgrabung durch archäologische Voruntersuchungen. Zum Einsatz kommen hier Suchgräben, magnetische Sondierung, Bodenwiderstandsmessung, Luftbilder und andere Methoden der Bodenforschung. Die Voruntersuchungen dienen dazu, sich ein Bild der potenziellen Grabungsstelle zu machen, um die eigentliche Grabung besser planen zu können.

Die meisten Fundplätze werden heute durch Baumaßnahmen entdeckt. Über Notgrabungen, auch Rettungsgrabungen genannt, versucht die archäologische Denkmalpflege diese Befunde vor ihrer endgültigen Zerstörung auszuwerten.
Seltener sind Forschungsgrabungen, bei denen unter primär wissenschaftlichen Interessen Fundplätze zur Grabung ausgewählt und ohne äußeren Zeitdruck untersucht werden können.

Bei der Grabung werden verschiedene Grabungstechniken angewandt. Eine moderne Grabung ist befundorientiert, d. h. die Funde werden in ihrer räumlichen und zeitlichen Einbettung auf Befunde bezogen.

Da jede Ausgrabung zur Zerstörung eines Befundes führt, soll eine exakte Dokumentation den Fundplatz, zumindest auf dem Papier, auch später bis ins Detail rekonstruierbar machen.
Die wichtigsten Arbeitsmittel der Ausgrabung sind deshalb, neben der Kelle, „Papier und Buntstift“.

Die Bauforschung ist ein wesentlicher Teil sowohl der klassischen Archäologie als auch der Archäologie des Mittelalters; wohingegen sie in der Ur- und Frühgeschichte mangels aufgehend erhaltener Bauwerke nur eine untergeordnete Rolle spielt.
Eine der Dokumentationsmethoden ist die Photogrammetrie.

Gerade am sehr populären Beispiel der Gletschermumie Ötzi ist zu erkennen, dass die Ausgrabung nur einen Bruchteil der archäologischen Arbeit darstellt. Der 1991 entdeckte Fund wird bis heute wissenschaftlich untersucht.

Die Typologie ist die Klassifikation von Objekten nach Kriterien von Form und Material. Sie ist grundlegend für die Einordnung des Fundmaterials, da sie Vergleiche mit Fundsituationen an anderen Fundplätzen ermöglicht und zur Grundlage von Kombinationsanalysen (zur relativchronologischen Datierung wie zur sozioökonomischen Einordnung) und Verbreitungsanalysen wird.

Wie bei der Prospektion und der Altersbestimmung werden auch für Materialbestimmungen moderne naturwissenschaftliche Techniken eingesetzt (siehe Archäometrie).
Zur Identifikation und Detailuntersuchung von Artefakten dienen u. a. die Mikroskopie, Infrarot- und Ultraschallaufnahmen, Röntgen, chemische Analysen, Spektralanalysen und Laserscans.

Ein Schwerpunkt der Fundanalyse ist die Datierung der Befunde (z. B. Grab) anhand der Funde (z. B. Grabbeigabe). Bei der Altersbestimmung wird zwischen absoluter Chronologie und relativer Chronologie unterschieden.

Die relative Chronologie setzt einen Fund dabei in Bezug zu einem anderen. Ist er jünger, älter oder gar gleichzeitig? J.J. Winckelmanns „vergleichendes Sehen“ ist eine der ersten Methoden zur relativen Chronologie.

Bei der absoluten Chronologie wird ein Fund ein absolutes Datum (Jahr, Jahrhundert) zugeordnet

Die Methoden der Interpretation sind in der Regel eher geisteswissenschaftlich. Für die "prähistorische" Archäologie ist der Analogieschluss die wesentliche Möglichkeit der Interpretation. In der "historischen" Archäologie (z. B. Klassische Archäologie oder Archäologie des Mittelalters) ist es der Vergleich mit Informationen aus anderen Quellen, wie schriftlicher oder bildlicher Überlieferung.

In Deutschland gehört die Archäologie zu den Aufgaben der Bundesländer (Landesarchäologe), meist als Bereich des Denkmalamtes als Bodendenkmalpflege organisiert. Größere Städte haben oft eine eigene Stadtarchäologie. Mehrere Institutionen fördern Forscher und Projekte durch Archäologiepreise.

Deutsche Grabungen im Ausland werden hingegen im Rahmen von Forschungsprojekten der Universitäten, des Deutschen Archäologischen Instituts oder des Römisch-Germanischen Zentralmuseums durchgeführt.

Die Archäologie gehört in Amerika zur Anthropologie (Völkerkunde) und hat aus diesem Grund eine völlig andere Ausrichtung als die europäische Forschung. Dies folgt vor allem aus dem Umstand, dass zum Zeitpunkt der Besiedlung der neuen Welt zuerst ethnographische Untersuchungen an noch existierenden Ureinwohnern stattfanden. Die eher spärlichen präkolumbischen Funde sind ein weiterer Grund für den in der Erforschung kultureller Prozesse liegenden Schwerpunkt amerikanischer Archäologie.

Als Pionier der amerikanischen Archäologie gilt Thomas Jefferson (1743–1826), welcher ab 1784 einige Grabhügel untersucht, um ihr Alter zu bestimmen. Jefferson setzt dabei erstmals eine Methode ein, die als Vorläufer der Dendrochronologie angesehen werden kann: er zählt die Jahresringe der auf den Grabhügeln stehenden Bäume.

Die ersten großen Ausgrabungen in Mittelamerika werden Ende des 19. Jahrhunderts im Mayazentrum Copán durchgeführt. 1911 entdeckt Hiram Bingham die Inkastadt Machu Picchu.

Im Jahre 1990 fanden Archäologen in der Nähe von Mexiko-Stadt über 10.000 Artefakte aus der Zeit der spanischen Eroberung des Landes. Man fand nicht nur menschliche Knochen, sondern auch Waffen, Kleidung, Haushaltsgeräte und Gegenstände aus dem persönlichen Besitz von Hernán Cortés. Die Fundstelle Tecoaque (vorspanischer Name: Zultepec) wurde als Weltkulturerbe vorgeschlagen.

1863 wird in Indien die Archaeological Survey of India gegründet. 1921/1922 entdeckt man eine der ältesten Hochkulturen der Menschheit, die Indus-Kultur. Ausgegraben werden u. a. die Städte Harappa und Mohenjo-Daro.

Archäologie in China beginnt mit dem schwedischen Geologen J. Gunnar Andersson (1874–1960), der 1921 bei Yang Shao Tsun in Honan eine neolithische Wohnhöhle entdeckt und damit beweist, dass China in vorgeschichtlicher Zeit bewohnt war. 1928 wird Anyang ausgegraben, die Hauptstadt der Shang-Dynastie des 2. Jahrtausends v. Chr.

1974 wird die Terrakottaarmee rund um das Grab des chinesischen Kaisers Qin Shihuangdi bei Xi’an entdeckt.
Afrika ist nicht nur in paläoanthropologischer Hinsicht die Wiege der Menschheit, sondern auch die unserer Kultur. Nur in Afrika kommen Steingeräte vor, die 2,5 Millionen Jahre alt sind und deren Herstellung mit den ersten Homo-Arten unserer Spezies in Verbindung gebracht wird. Die betreffenden Werkzeuge – einfache Geröllgeräte vom Oldowan-Typ, später die Faustkeile, um die Leitformen zu nennen – kommen auch in anderen Teilen der Welt vor, nur sind sie dort deutlich jünger. In Europa datieren die ältesten Stellen auf eine Million Jahre. Neue, etwa 3,3 Millionen Jahre alte Funde in Lomekwi 3, Kenia, werden als Beleg für eine eigenständige archäologische Kultur interpretiert, vorschlagsweise "Lomekwian" genannt. 

Bereits seit dem 17. Jahrhundert ist der Nordosten Afrikas Gegenstand intensiver Forschungen durch die Ägyptologie und Koptologie. Diese Region des Kontinents ist auch im internationalen Vergleich hervorragend dokumentiert. Da jedoch die ältesten Schriftquellen im subsaharischen Afrika nicht weiter als 600 Jahre zurückreichen, kommt der Archäologie gerade hier eine besondere Bedeutung zu. Aufgrund der kurzen Forschungstradition im Vergleich zu Mitteleuropa steht man hier allerdings noch vielfach am Anfang.

Die Vermittlung archäologischer Forschungsergebnisse erfolgt auf verschiedene Weise:











Vereine und Organisationen



</doc>
<doc id="124" url="https://de.wikipedia.org/wiki?curid=124" title="American Standard Code for Information Interchange">
American Standard Code for Information Interchange

Der (ASCII, alternativ US-ASCII, oft [] ausgesprochen, ) ist eine 7-Bit-Zeichenkodierung; sie entspricht der US-Variante von ISO 646 und dient als Grundlage für spätere, auf mehr Bits basierende Kodierungen für Zeichensätze.

Der ASCII-Code wurde zuerst am 17. Juni 1963 von der American Standards Association (ASA) als Standard ASA X3.4-1963 gebilligt und 1967/1968 wesentlich sowie zuletzt im Jahr 1986 (ANSI X3.4-1986) von ihren Nachfolgeinstitutionen aktualisiert. Die Zeichenkodierung definiert 128 Zeichen, bestehend aus 33 nicht druckbaren sowie 95 druckbaren Zeichen. Druckbare Zeichen sind, beginnend mit dem Leerzeichen:
Die druckbaren Zeichen umfassen das lateinische Alphabet in Groß- und Kleinschreibung, die zehn arabischen Ziffern sowie einige Interpunktionszeichen (Satzzeichen, Wortzeichen) und andere Sonderzeichen. Der Zeichenvorrat entspricht weitgehend dem einer Tastatur oder Schreibmaschine für die englische Sprache. In Computern und anderen elektronischen Geräten, die Text darstellen, wird dieser in der Regel gemäß ASCII oder abwärtskompatibel (ISO 8859, Unicode) dazu gespeichert.

Die nicht druckbaren Steuerzeichen enthalten Ausgabezeichen wie Zeilenvorschub oder Tabulatorzeichen, Protokollzeichen wie Übertragungsende oder Bestätigung und Trennzeichen wie Datensatztrennzeichen.

Jedem Zeichen wird ein Bitmuster aus 7 Bit zugeordnet. Da jedes Bit zwei Werte annehmen kann, gibt es 2 = 128 verschiedene Bitmuster, die auch als die ganzen Zahlen 0–127 (hexadezimal 00h–7Fh) interpretiert werden können.

Das für ASCII nicht benutzte achte Bit kann für Fehlerkorrekturzwecke (Paritätsbit) auf den Kommunikationsleitungen oder für andere Steuerungsaufgaben verwendet werden. Heute wird es aber fast immer zur Erweiterung von ASCII auf einen 8-Bit-Code verwendet. Diese Erweiterungen sind mit dem ursprünglichen ASCII weitgehend kompatibel, so dass alle im ASCII definierten Zeichen auch in den verschiedenen Erweiterungen durch die gleichen Bitmuster kodiert werden. Die einfachsten Erweiterungen sind Kodierungen mit sprachspezifischen Zeichen, die nicht im lateinischen Grundalphabet enthalten sind, vgl. unten.

Die ersten 32 ASCII-Zeichencodes (von 0x00 bis 0x1F) sind für Steuerzeichen "(control character)" reserviert; siehe dort für die Erklärung der Abkürzungen in obiger Tabelle. Diese Zeichen stellen keine Schriftzeichen dar, sondern dienen (oder dienten) zur Steuerung von solchen Geräten, die den ASCII verwenden (etwa Drucker). Steuerzeichen sind beispielsweise der Wagenrücklauf für den Zeilenumbruch oder "Bell" (die Glocke); ihre Definition ist historisch begründet.

Code 0x20 "(SP)" ist das Leerzeichen (engl. "space" oder "blank"), das in einem Text als Leer- und Trennzeichen zwischen Wörtern verwendet und auf der Tastatur durch die Leertaste erzeugt wird.

Die Codes 0x21 bis 0x7E stehen für druckbare Zeichen, die Buchstaben, Ziffern und Interpunktionszeichen (Satzzeichen, Wortzeichen) umfassen. Die Buchstaben sind lediglich Klein- und Großbuchstaben des lateinischen Alphabets. In nicht-englischen Sprachen verwendete Buchstabenvarianten – beispielsweise die deutschen Umlaute – sind im ASCII-Zeichensatz nicht enthalten. Ebenso fehlen typografisch korrekte Gedankenstriche und Anführungszeichen, die Typografie beschränkt sich auf den Schreibmaschinensatz. Der Zweck war "Informationsaustausch", nicht Drucksatz.

Code 0x7F (alle sieben Bits auf eins gesetzt) ist ein Sonderzeichen, das auch als "Löschzeichen" bezeichnet wird "(DEL)". Dieser Code wurde früher wie ein Steuerzeichen verwendet, um auf Lochstreifen oder Lochkarten ein bereits gelochtes Zeichen nachträglich durch das Setzen aller Bits, das heißt durch Auslochen aller sieben Markierungen, löschen zu können – einmal vorhandene Löcher kann man schließlich nicht mehr rückgängig machen.
Bereiche ohne Löcher (also mit dem Code 0x00) fanden sich vor allem am Anfang und Ende eines Lochstreifens "(NUL)".

Aus diesem Grund gehörten zum eigentlichen ASCII nur 126 Zeichen, denn den Bitmustern 0 (0000000) und 127 (1111111) entsprachen keine Zeichencodes. Der Code 0 wurde später in der Programmiersprache C als „Ende der Zeichenkette“ interpretiert; dem Zeichen 127 wurden verschiedene grafische Symbole zugeordnet.

Eine frühe Form der Zeichenkodierung war der Morsecode. Er wurde mit der Einführung von Fernschreibern aus den Telegrafennetzen verdrängt und durch den Baudot-Code und Murray-Code ersetzt. Vom 5-Bit-Murray-Code zum 7-Bit-ASCII war es dann nur noch ein kleiner Schritt – auch ASCII wurde zuerst für bestimmte amerikanische Fernschreibermodelle, wie den Teletype ASR33, eingesetzt.

Die erste Version, noch ohne Kleinbuchstaben und mit kleinen Abweichungen vom heutigen ASCII bei den Steuer- und Sonderzeichen, entstand im Jahr 1963.
1968 wurde dann der bis heute gültige ASCII festgelegt.
Diese Fassung gebar die Caesar-Verschlüsselung ROT47 als Erweiterung von ROT13. Während ROT13 nur das lateinische Alphabet um dessen halbe Länge rotiert, rotiert ROT47 alle ASCII-Zeichen zwischen 33 (codice_1) und 126 (codice_2).

In den Anfängen des Computerzeitalters entwickelte sich ASCII zum Standard-Code für Schriftzeichen. Zum Beispiel wurden viele Terminals (VT100) und Drucker nur mit ASCII angesteuert.

Für die Kodierung lateinischer Zeichen wird fast nur bei Großrechnern die zu ASCII inkompatible 8-Bit-Kodierung EBCDIC verwendet, die IBM parallel zu ASCII für sein System/360 entwickelte, damals ein ernsthafter Konkurrent. Die Handhabung des Alphabets ist in EBCDIC schwieriger, denn es ist dort auf zwei auseinander liegende Codebereiche verteilt. IBM selbst verwendete ASCII für interne Dokumente. ASCII wurde durch Präsident Lyndon B. Johnsons Anordnung 1968 gestützt, es in den Regierungsbüros zu verwenden.

Mit dem Internationalen Alphabet 5 (IA5) wurde 1963 eine 7 Bit Codierung auf Basis des ASCII als ISO 646 normiert. Die Referenzversion (ISO 646-IRV) entspricht dabei bis auf eine Position dem ASCII. Um Buchstaben und Sonderzeichen verschiedener Sprachen darstellen zu können (beispielsweise die deutschen Umlaute), wurden 12 Zeichenpositionen zur Umdefinition vorgesehen (<code>#$@[\]^`
Der 1936 entdeckte Asteroid (3568) ASCII wurde 1988 nach der Zeichenkodierung benannt.






</doc>
<doc id="130" url="https://de.wikipedia.org/wiki?curid=130" title="Außenbandruptur des oberen Sprunggelenkes">
Außenbandruptur des oberen Sprunggelenkes

Das Außenband des oberen Sprunggelenkes setzt sich zusammen aus drei Bändern (der „laterale Bandapparat“): "Ligamentum fibulotalare anterius" und "posterius" sowie "Ligamentum fibulocalcaneare". Beim Umknicken nach außen (Supinationstrauma) kommt es meist zur Zerrung oder Riss (Ruptur) des "Lig. fibulotalare anterius" oder/und des Lig. calcaneofibulare, seltener ist die komplette Ruptur aller drei Bänder.

Bei einer Verletzung der Außenbänder des oberen Sprunggelenks ist wegen der Wahl der richtigen Therapie vor allem die Frage wichtig, ob es sich um eine Bänderdehnung oder einen Bänderriss handelt. Geübten Untersuchern gelingt diese Unterscheidung nach Expertenmeinung in 90 % der Fälle alleine mit dem Schubladentest, also ohne die Anfertigung von Röntgen-Bildern oder den Einsatz anderer gerätemedizinischer Untersuchungsmethoden: Der Patient liegt dafür in Rückenlage. Der Untersucher umgreift mit einer Hand von unten die Ferse, mit der anderen Hand wird gefühlvoll von oben gegen das Schienbein gedrückt. Liegt lediglich eine Zerrung des vorderen Außenbandes vor, ist keine Schubladenbewegung möglich. Dagegen kann der Fuß bei einem Riss deutlich gegenüber dem Schien- und Wadenbein nach vorne (bei liegendem Patienten nach oben) aus dem Gelenk geschoben werden. Da sich die normale Schubladenbewegung im oberen Sprunggelenk im gesunden Zustand von Mensch zu Mensch stark unterscheidet, ist es wichtig, die Untersuchung zuvor am gesunden Sprunggelenk des anderen Beins durchzuführen. Auf diese Weise lässt sich herausfinden, welches Ausmaß der Schubladenbewegung beim betroffenen Menschen noch als nicht gerissen anzusehen ist.

Zusätzlich können bildgebende Verfahren wie Röntgen sinnvoll sein, um einen Bruch (Fraktur) der angrenzenden Knochen auszuschließen. In seltenen Fällen kann zudem eine Magnetresonanztomographie sinnvoll sein.

Sehr in die Kritik geraten sind bei der Diagnosestellung die bis vor einiger Zeit üblichen sogenannten „gehaltenen Röntgen-Aufnahmen“. Dabei wird auf einem Röntgenbild festgehalten, wie weit sich das Gelenk mit einer fest definierten Kraft aufklappen lässt. Aus dem Aufklappwinkel, der im Röntgenbild eingezeichnet werden kann, wurde dann auf den Verletzungsgrad geschlossen. Der Grund für die Kritik ist, dass sich mit solchen gehaltenen Aufnahmen vor allem das mittlere Außenband überprüfen lässt, das allerdings nur sehr selten isoliert reißt, sondern fast immer nur in Kombination mit dem vorderen Außenband. Da für die Auswahl der Therapie vor allem die Frage wichtig ist, ob es sich um einen Riss oder eine Zerrung handelt, nicht aber ob ein oder zwei Bänder gerissen sind, reicht der Schubladentest, der das vordere Außenband überprüft, in den meisten Fällen als alleinige Untersuchung aus.

Während noch vor einigen Jahren die Außenbandruptur regelhaft genäht wurde, ist heute bei gleich guten Behandlungsergebnissen die konservative Behandlung durch Schienung des Sprunggelenkes über sechs Wochen Standard. Nur bei kompletter Zerreißung aller drei Bänder und Operationswunsch (z. B. Profisportler) wird noch eine operative Behandlung empfohlen.

Es wird teils auch für die ersten ca. 8 Tage ein Spaltgips in regelrechter Stellung verordnet, bis die Schwellung etwas abgeklungen ist.

Liegt am Gelenk keine Schwellung (mehr) vor, werden oft Orthesen (z. B. Aircast-Schiene) eingesetzt. Diese erfüllen zwei Funktionen: das Gelenk wird so gesichert, dass die gerissenen Bänder nicht belastet werden können; die Beweglichkeit des Gelenks in horizontaler Richtung („rauf/runter“ = Flexion/Extension) wird aber kaum eingeschränkt. Damit sind zum Beispiel Spazieren oder Radfahren weiter möglich. Die Bänder wachsen eher belastungsgerecht zusammen, Probleme mit einem versteiften Gelenk wie bei kompletter Fixierung treten nicht auf. Als besonders günstig haben sich hierbei sogenannte modulare Orthesen erwiesen, die eine Anpassung der Bewegungsfähigkeit mit Orthese an den Heilungsverlauf ermöglichen.


</doc>
<doc id="132" url="https://de.wikipedia.org/wiki?curid=132" title="Alphabet">
Alphabet

Ein Alphabet (frühneuhochdeutsch von kirchenlateinisch "", von "alphábētos") ist die Gesamtheit der kleinsten Schriftzeichen bzw. Buchstaben einer Sprache oder mehrerer Sprachen in einer festgelegten Reihenfolge. Die Buchstaben können über orthographische Regeln zu Wörtern verknüpft werden und damit die Sprache schriftlich darstellen. Die im Alphabet festgelegte Reihenfolge der Buchstaben erlaubt die alphabetische Sortierung von Wörtern und Namen beispielsweise in Wörterbüchern. Nach einigen Definitionen ist mit "Alphabet" nicht der Buchstabenbestand in seiner festgelegten Reihenfolge gemeint, sondern die Reihenfolge selbst.

Die Bezeichnung "Alphabet" geht auf die ersten beiden Buchstaben des griechischen Alphabets zurück (Alpha – α, Beta – β). Ausgehend von den ersten drei Buchstaben des deutschen Alphabets (bzw. des lateinischen Alphabets) sagt man auch Abc (die Schreibweise Abece verdeutlicht die Aussprache, wird aber selten verwendet).

Alphabetschriften gehören wie Silbenschriften zu den phonographischen Schriften und stehen damit im Gegensatz zu piktografischen oder logografischen Systemen, bei denen die Zeichen für Begriffe stehen (z. B. "Rind, Sonnenaufgang, Freundschaft"). Im Unterschied zu Silbenschriften bezeichnen alphabetische Buchstaben in der Regel jeweils nur "einen" Laut (Phonem). Eine Zwischenform aus Alphabetschrift und Silbenschrift stellen die sogenannten Abugidas dar, zu denen die indischen Schriften gehören (siehe auch Schrifttypen).

Das Alphabet dient auch dem Erlernen des Lesens und des Schreibens; eine Merkhilfe dazu waren die Buchstabentafeln. Jemand, der lesen kann, wird fachsprachlich ebenfalls als "Alphabet" bezeichnet, das Gegenteil ist der Analphabet. Ein wichtiges Ziel jeglicher Kulturpolitik ist die Alphabetisierung der jeweiligen Bevölkerung - also die Beherrschung des Lesens und des Schreibens durch alle.

Das deutsche Alphabet ist eine Variante des lateinischen Alphabets. Von diesem stammen 26 Buchstaben:

Im deutschen Alphabet kommen dazu noch die drei Umlaute (Ä/ä, Ö/ö, Ü/ü) sowie das Eszett (ẞ/ß).

Die Buchstaben eines Alphabetes sind schriftliche Symbole für die kleinsten "bedeutungsunterscheidenden" lautlichen Einheiten der Sprache, die Phoneme; zum Beispiel unterscheiden und in und die Bedeutung der Wörter (siehe auch Minimalpaar und Allophon).

In einem "idealen Alphabet" entspricht jeder Buchstabe einem Phonem und umgekehrt. In der Praxis finden sich aber immer Abweichungen:


Darüber hinaus geht die einmal festgelegte Korrespondenz von Phonem und Graphem auch durch den Sprachwandel verloren (vergleiche englisch und gegenüber lateinisch ).

Fehlen in einem Schriftsystem Zeichen für Phoneme, können sprachliche (inhaltliche) Unterschiede eventuell nicht schriftlich wiedergegeben werden. So bestanden einige Alphabete ursprünglich nur aus Konsonanten (Konsonantenschrift). Später wurden sie mit Zeichen für Vokale ergänzt, die als kleine Zusätze (z. B. Punkte, Striche) zu den Konsonanten gesetzt werden konnten (z. B. arabisches und hebräisches Alphabet).

Sind hingegen in einem Schriftsystem Zeichen für Phoneme im Übermaß vorhanden, können semantische (inhaltliche) Unterschiede selbst bei gleicher Lautung schriftlich ausgedrückt werden. Zum Beispiel im Deutschen und .

Die Schriftsysteme für die meisten europäischen Sprachen nutzen Varianten des lateinischen Alphabets. Dabei wurden den Zeichen für lateinische Laute ähnliche Laute der jeweiligen Sprache zugeordnet. Dieselben Zeichen standen in den verschiedenen Sprachen für teilweise unterschiedliche Laute. Zudem ist es im Zuge der Sprachentwicklung zu weiteren Veränderungen der Aussprache gekommen (vgl. im Deutschen und Englischen).

Da die Zahl und Art der Phoneme in den verschiedenen Sprachen unterschiedlich ist, genügte der Zeichenvorrat des lateinischen Alphabetes oft nicht. Deshalb wurden zur Darstellung der betreffenden Phoneme Buchstabenkombinationen (z. B. , , ) und diakritische Zeichen eingeführt (z. B. auf , ).

Daneben wurden Varianten der ursprünglichen lateinischen Zeichen ( > , > ) und Ligaturen ( > , / > , / > ) zu eigenständigen Zeichen weiterentwickelt und gelegentlich auch Buchstaben aus anderen Alphabeten übernommen ().

Ein absolut "phonetisches" Alphabet wäre in der Praxis unbrauchbar, weil es aufgrund der unzähligen Nuancen einer Sprache sehr viele Zeichen hätte. Ein in Bezug auf die phonetische Wiedergabe optimiertes Alphabet ist das IPA, welches möglichst vielen Lautnuancen ein grafisches Zeichen zuordnet.

Eine "phonemische" Schreibweise behandelt unterschiedliche Aussprachen desselben Phonems gleich. So wird beispielsweise in der deutschen Orthografie die regional unterschiedliche (phonetische) Aussprache des Phonems in als norddeutsch und hochdeutsch nicht berücksichtigt.
Daneben sorgen morphemische Schreibungen für ein konstanteres Schriftbild bei der Flexion, z. B. schreibt man wegen des Plurals nicht *, sondern , und bei der Derivation, z. B. statt .

Wenn Menschen einander mündlich die korrekte Schreibweise eines Wortes mitteilen, indem sie nacheinander alle Buchstaben jenes Wortes nennen, so bezeichnet man diesen Vorgang als Buchstabieren (Verb: "buchstabieren").
Dabei werden Konsonantenbuchstaben meist mit Hilfe von zusätzlichen Vokalen ausgesprochen, im Deutschen zum Beispiel [beː] für B oder [kaː] für K (siehe Benennung der Buchstaben). Um Missverständnisse auszuschließen, können auch festgelegte Namen oder Wörter ausgesprochen werden, die mit dem betreffenden Buchstaben beginnen, zum Beispiel „Anton“ für A oder „Berta“ für B (siehe Buchstabiertafel).

Aus den in Vorderasien gebräuchlichen Keilschriften entwickelten Händler in Ugarit um 1400 v. Chr. die erste alphabetische Schrift, die sogenannte ugaritische Schrift. Aus dieser Schrift hat sich um 1000 v. Chr. unter anderem das phönizische Alphabet entwickelt, das wiederum Ausgangspunkt für die heute gebräuchlichen Alphabete war. Die Phönizier verwendeten dabei Elemente vorhandener Bilderschriften. Sie lösten die Zeichen vollständig von ihrer bildlichen Bedeutung und wiesen ihnen Lautwerte zu. Die phönizische Schrift verlief von rechts nach links. Trotz der großen Unterschiede in der Gestalt der Zeichen lassen sich die Buchstaben der Phönizier mit den Keilschrift-Zeichen der ugaritischen Schrift in Verbindung bringen.

Die phönizische Schrift war eine reine Konsonantenschrift. Dies entsprach der Struktur der semitischen Sprachen. Die hebräische und die arabische Schrift, die daraus entstanden, verzichten bis heute (weitgehend) auf Vokale. Als die Griechen etwa im 10. oder 9. Jahrhundert v. Chr. die phönizische Schrift übernahmen, benutzen sie Zeichen für bestimmte semitische Konsonanten, die in ihrer Sprache nicht vorkamen, zur Bezeichnung von Vokalen, z. B. wurde aus dem Zeichen H für einen rauen Hauchlaut im griechischen Alphabet ein Zeichen für einen Vokal (siehe Buchstabe Eta). Einige Zeichen für Konsonanten, die die phönizische Sprache nicht kannte, wurden neu geschaffen, z. B. das Psi. Im Jahre 403 v. Chr. wurde in Athen das Alphabet normiert. Es wurde so zum Schriftsystem für ganz Griechenland.

Anfang des 4. Jahrhunderts v. Chr. brachten griechische Siedler das Alphabet nach Italien, wo die Etrusker (in der heutigen Toskana) es im Laufe des 4. Jahrhunderts übernahmen. Im 3. Jahrhundert v. Chr. orientierten sich die Römer an der griechisch-etruskischen Schrift und überlieferten sie im 1. Jahrhundert v. Chr. nach Mitteleuropa.

Durch das Alphabet entstand ein System mit vergleichsweise wenigen Zeichen. Um die Aufzeichnungen der alten Ägypter verstehen zu können, musste man Hunderte, später sogar Tausende Hieroglyphen lernen. Nun genügten zwei Dutzend Zeichen, um sämtliche Gedanken, die überhaupt formulierbar sind, zu notieren. Die Einfachheit dieses Systems begünstigte dessen Verbreitung über die halbe Welt.

„Die menschlichen Sprechwerkzeuge können zwar eine riesige Zahl von Lauten erzeugen, doch beruhen fast alle Sprachen auf dem formalen Wiedererkennen von nur ungefähr vierzig dieser Laute durch die Mitglieder einer Gesellschaft.“ (Jack Goody).

Die Reihenfolge des griechischen und lateinischen Alphabets folgt global (mit wenigen Ausnahmen) der Reihenfolge des phönizischen Alphabets, da die Zeichen auch mit einem Zahlwert gekoppelt waren.

Die Buchstaben (Schriftzeichen eines Alphabets) bestehen meist aus Linien und können beispielsweise auf Papier geschrieben werden. Das bestimmende Merkmal eines Buchstabens ist jedoch nicht die Form, sondern seine Funktion, einen Sprachlaut oder eine Lautverbindung zu repräsentieren. Deshalb spricht man im weiteren Sinn auch bei den folgenden Zeichensystemen von Alphabeten:

Diese Zeichensysteme kodieren eigentlich Buchstaben – und nur indirekt Laute. Zudem enthalten sie auch Zeichen für Ziffern und teilweise weitere Zeichen (Satzzeichen, Steuerzeichen, Zeichen für Wörter).

In der Informatik werden die Begriffe "Alphabet" und "Buchstabe" in einem verallgemeinerten Sinn verwendet. Ein „Buchstabe“ kann hier auch eine Ziffer oder ein sonstiges Symbol sein – „Alphabete“ und „Wörter“ können solche beliebigen Symbole enthalten. Siehe hierzu Alphabet (Informatik) und formale Sprache.





</doc>
<doc id="133" url="https://de.wikipedia.org/wiki?curid=133" title="Arthur Harris">
Arthur Harris

Sir Arthur Travers Harris, 1. Baronet GCB OBE AFC, genannt "Bomber-Harris", (* 13. April 1892 in Cheltenham; † 5. April 1984 in Goring-on-Thames) war ein hochrangiger Offizier der Royal Air Force, zuletzt im Rang eines Marshal of the Royal Air Force. Während des Zweiten Weltkriegs war er ab Februar 1942 Oberbefehlshaber des RAF Bomber Command und gehört wegen der von ihm angeordneten Flächenbombardements deutscher Städte zu den umstrittensten Personen des Luftkriegs im Zweiten Weltkrieg.

Arthur Travers Harris wurde in Cheltenham während eines Urlaubs seiner Eltern geboren. Der Vater war Angehöriger der britischen Beamtenschaft in Indien (Indian Civil Service (ICS)). Nach der Schulzeit in Dorset stand laut Biographie von Norman Longmate für Harris im Alter von 16 Jahren eine Entscheidung zwischen der Armee und den Kolonien an. Harris entschied sich 1908 für letztere. In Rhodesien war er nach eigener Auskunft mit Goldwäsche, Kutschfahrten und Landwirtschaft beschäftigt.

1914 trat er als Trompeter in das 1st Rhodesian Regiment der Britischen Armee ein. Er diente für die Südafrikanische Union im Krieg in Deutsch-Südwestafrika (heute Namibia), bevor er 1915 nach England zurückkehrte und in das neu aufgestellte Royal Flying Corps eintrat. Harris war in Frankreich und England im Einsatz und errang auf den Doppeldeckern Sopwith 1½ Strutter und Sopwith Camel fünf Luftsiege, worauf ihm das Air Force Cross (AFC) verliehen wurde. Bei Kriegsende hatte er den Rang eines Majors.

1919 verblieb er bei der neu gegründeten Royal Air Force und wurde im April 1920 Staffelführer und Kommandant des Fliegerhorstes Digby und der No. 3 Flying Training School.

Später diente er unter anderem in Britisch-Indien, in Mesopotamien und in Persien.

In Mesopotamien kommandierte er ein Transportgeschwader Vickers Vernon.
Von 1930 an war er im Luftstab für den Nahen Osten tätig, wo er an der Niederschlagung verschiedener Aufstände der dortigen Bevölkerung gegen die britische Kolonialherrschaft beteiligt war. Er begründete dies damit, dass seiner Ansicht nach Araber nur eine „Politik der harten Hand“ verstünden („The only thing the Arab understands is the heavy hand“).

Am 14. Februar 1942 erfolgte die Anweisung „Area Bombing Directive“ des britischen Luftfahrtministeriums.
Harris wurde im Februar 1942 zum Oberkommandierenden des Bomber Command der RAF ernannt. Basierend auf Vorlagen von Frederick Lindemann, einem engen Berater Churchills, von dem die Wortschöpfung "Flächenbombardements" (Carpet Bombing) stammt, war Harris der Ansicht, allein durch Flächenbombardierungen der Städte das Deutsche Reich zur Kapitulation zwingen zu können.

Harris unterstützte maßgeblich die Entwicklung eines geplanten Feuersturms (Zitat A. Harris bei den Planungen des Luftangriffs auf Lübeck am 29. März 1942: „Historischer Stadtkern brennt gut“). In der ersten Welle wurden neben Spreng- und Brandbomben vor allem große Luftminen (Blockbuster – „Wohnblockknacker“) abgeworfen, die die Dächer abdeckten und Fenster zerstörten, um den Kamineffekt zu verstärken. In einer zweiten Welle wurden Brandbomben abgeworfen, die in kürzester Zeit einen Flächenbrand entstehen ließen. Dies gelang jedoch aufgrund meteorologischer und städtebaulicher Faktoren nicht immer.

Um die deutsche Flugabwehr und die nach dem sog. „Himmelbett-Verfahren“ arbeitende Nachtjagd, z. B. entlang der Kammhuber-Linie durch lokale Überlastung zu überrumpeln, entwickelte er die Methode der Bomberströme, bei der möglichst viele Bomber auf demselben Kurs einfliegend in kurzer Zeit ein Ziel angriffen, statt einzeln und in breiter Front einzufliegen. Zur Demonstration der Wirksamkeit seines Konzeptes zog Harris im Frühjahr 1942 für die Operation Millennium alle verfügbaren Bomber zusammen, um Ende Mai mit 1047 Maschinen auf Köln den ersten „Tausend-Bomber-Angriff“ durchzuführen. Dieser Angriff war entscheidend, um die zahllosen britischen Skeptiker von der Wirksamkeit von Luftangriffen zu überzeugen und die betriebene Auflösung des Bomber Command zu verhindern.

Die technischen Voraussetzungen für präzise Schläge gegen strategische Punkte wie Fabriken für Flugzeuge, Panzer und anderes Rüstungsmaterial befanden sich in der Mitte des Krieges noch in der Entwicklung. Die schweren Verluste der 8. USAAF bei ihren Angriffen 1943 und Anfang 1944 bestätigten sein Festhalten am Nachtangriff vorerst bis zum Einsatz von neuen amerikanischen Langstreckenbegleitjägern, wobei die Nachtangriffe der RAF durch die Schaffung der 24-Stunden-Bedrohung auch für den Erfolg der amerikanischen Tagesangriffe auf strategische Punktziele weiterhin bedeutend blieben.

Unter seiner Führung wurden von der RAF zahlreiche deutsche Städte schwer zerstört, so bei der Operation Gomorrha gegen Hamburg im Juli/August 1943, Kassel (22. Oktober 1943), Frankfurt am Main (22. März 1944), Braunschweig (15. Oktober 1944), Nürnberg (2. Januar 1945), Magdeburg (16. Januar 1945), Dresden am 13./14. Februar 1945 sowie Pforzheim (23. Februar 1945), Mainz (27. Februar 1945), Würzburg (16. März 1945), Hanau (19. März 1945) und Hildesheim (22. März 1945).

Bei den Flächenbombardements wurden – neben den im Stadtgebiet befindlichen Industrieanlagen – die Zivilbevölkerung und die Infrastruktur der Stadt primäres Ziel der Angriffe. Seiner Meinung nach sollten ganz bewusst zivile Ziele angegriffen werden, um die Moral und den Widerstandswillen der deutschen Bevölkerung zu brechen (sogenanntes "Moral Bombing"). Zu Beginn der Bombardierungen äußerte sich Harris zu seiner Motivation: „"Die Nazis starteten (‚entered‘) den Krieg mit der ziemlich kindischen Vorstellung, dass sie jeden anderen nach Belieben bombardieren könnten und niemand würde zurückbomben. In Rotterdam, London, Warschau und an beinahe einem halben Hundert anderer Stätten führten sie ihre ziemlich naive Theorie aus. Sie säten Wind und jetzt ernten sie Sturm“". In seinen Memoiren schrieb er später: "„Trotz all dem, was in Hamburg geschehen ist, bleibt das Bomben eine relativ humane Methode“".

Neben den Bombenangriffen auf Deutschland wurden insbesondere in Italien mehrere Großstädte bombardiert, was etwa in Mailand, Neapel und Palermo beträchtliche Schäden auch in Wohngebieten verursachte.

Am 15. September 1945 schied Harris im Streit mit dem neuen Premierminister Clement Attlee aus der Royal Air Force aus und zog sich verbittert nach Südafrika zurück. Seine Ehrungen durch die Ernennung zum Baronet 1953 (eine Erhebung zum Peer hatte er abgelehnt) sowie die Enthüllung eines von Veteranen finanzierten Denkmals 1992 vor der Kirche St Clement Danes in London durch die Königinmutter Elizabeth waren in der britischen Bevölkerung stark umstritten. Innerhalb von 24 Stunden wurde das Denkmal mit roter Farbe überschüttet und später noch mehrfach beschädigt, woraufhin es für mehrere Monate unter Bewachung stand. In diesem Zusammenhang ist auch von Bedeutung, dass seine Luftkriegs-Strategie für die Besatzungen der Flugzeuge verlustreich war. Nahezu 45 % kehrten nicht heim, insgesamt kamen 55.573 Flieger bei den Angriffen auf Deutschland um. Auch deswegen wurde Harris oft „Butcher“ (engl. für Metzger oder Schlächter) genannt.

Die historische wie rechtliche Qualifizierung der alliierten Luftkriegsstrategie und damit der Position Harris’ bleibt umstritten. Nach sachlichen oder militärischen Kriterien war die gezielte Zerstörung von Wohngebieten und Innenstädten umstritten. Zwar waren militärisch gesehen die strategischen Folgen des Luftkriegs allgemein erheblich, da angesichts der Angriffe die deutsche Rüstungsproduktion zu umfangreichen produktionsbehindernden Verlagerungen gezwungen wurde – laut Albert Speer führten die alliierten Luftangriffe bei den Luftfahrzeugen zu einer Halbierung der möglichen Produktion. Über eine Million Soldaten wurden bei der Flakartillerie eingesetzt und fehlten dadurch an den Fronten, zusätzlich wurde eine halbe Million Behelfspersonal herangezogen, darunter viele Jugendliche als Flakhelfer.

All dies war aber in erster Linie auf die gegen die Rüstungsindustrie geführten Tagangriffe der USAAF und nicht auf die gegen die Zivilbevölkerung gerichteten und von Arthur Harris verantworteten Nachtangriffe der Royal Air Force zurückzuführen. Die bekanntesten Einsätze des Bomber Command außerhalb von Harris’ Strategie waren: Die Angriffe auf die Talsperren (Operation Chastise), die Versenkung des Schlachtschiffs "Tirpitz" (November 1944), die Bombardierung von U-Boot-Bunkern der Kriegsmarine an der französischen Atlantikküste und die Zerstörung von Anlagen des deutschen V-Waffen-Programms (Operation Hydra, Éperlecques, Mimoyecques) sowie die direkte taktische Unterstützung während der Landung alliierter Truppen in der Normandie (Operation Overlord).

Harris hat seinen Standpunkt insbesondere in seinem Buch "Bomber Offensive" dargestellt, das seinen Lebensweg beschreibt. Er argumentiert, das nationalsozialistische Deutsche Reich habe damit begonnen, die Zivilbevölkerung zum Objekt von Terrorangriffen zu machen (Guernica, Coventry, Rotterdam, Warschau, London). Aufgabe der britischen Verantwortlichen sei es gewesen, für ein schnelleres Ende des Krieges zu sorgen und eigene Opfer zu vermeiden, die etwa ein Landkrieg oder Stellungskrieg wie im Ersten Weltkrieg mit sich gebracht hätte. Vor dem Eintritt der Vereinigten Staaten in den Krieg (Dezember 1941) beziehungsweise vor dem D-Day, der alliierten Landung in der Normandie am 6. Juni 1944, hätte angesichts der Insellage Großbritanniens einzig die Offensivstrategie des Bomber Commands der Royal Air Force die Sicherheit des Vereinigten Königreichs garantieren können.

Des Weiteren unterstreicht Harris die Bedeutung der Luftunterstützung für einen erfolgreichen Einsatz von Landtruppen. Er verweist zum Vergleich auf die deutsche Blitzkriegstrategie zu Beginn des Krieges, bei der das schnelle Vordringen des Heeres, insbesondere der Panzer, nur aufgrund massiver und rasch abrufbarer Luftunterstützung (Bomber und Jagdflieger) möglich gewesen sei. Die Tatsache, dass die deutsche Luftwaffe gegen Ende des Krieges zum großen Teil zerstört oder durch die Verteidigung des eigenen Territoriums gegen die alliierten Bomber gebunden waren, habe dazu geführt, dass dem deutschen Heer die notwendige Unterstützung durch die Luftwaffe fehlte. Die alliierte Luftüberlegenheit habe den britischen und US-amerikanischen Truppen sowie der Roten Armee entscheidend geholfen, die Deutschen zurückzudrängen.

In dem 1954 gedrehten britischen Spielfilm "Mai 1943 – Die Zerstörung der Talsperren" "(The Dam Busters)" von Michael Anderson wird Arthur T. Harris von Basil Sydney dargestellt.






</doc>
<doc id="134" url="https://de.wikipedia.org/wiki?curid=134" title="Arthur Wellesley, 1. Duke of Wellington">
Arthur Wellesley, 1. Duke of Wellington

Arthur Wellesley, 1. Duke of Wellington (* vermutlich 1. Mai 1769 in Dublin, Irland; † 14. September 1852 in Walmer Castle bei Deal, Kent, England), war Feldmarschall und der herausragende britische Militärführer der napoleonischen Zeit sowie britischer Außen- und zweimal Premierminister. Er siegte über Napoleon in der Schlacht bei Waterloo. 2002 wurde er in einer Umfrage der BBC auf Platz 15 der 100 größten Briten gewählt. Arthur war außerdem Ritter des Hosenbandordens (Knight of the Order of the Garter), Großkreuzritter des Order of the Bath, Großkreuzritter des Guelphen-Ordens, Mitglied des Privy Council und Mitglied der Royal Society. Die HMS Duke of Wellington, ein 131-Kanonen-Schiff der britischen Royal Navy, wurde 1852 nach ihm benannt. Sie war das Flaggschiff von Sir Charles Napier, damals Konteradmiral der Blauen Flagge. Auch die HMS Iron Duke, das Flaggschiff der Grand Fleet im Ersten Weltkrieg war nach ihm benannt.

Wellesley stammte aus verarmtem englisch-irischen Adel und war der dritte überlebende Sohn von Garret Wesley, 1. Earl of Mornington und Anne, der Tochter von Arthur Hill-Trevor, Viscount Dungannon. Der Tag seiner Geburt ist nicht sicher bekannt. Vermutlich wurde er in "Mornington House", "24 Upper Merrion Street" in Dublin geboren. Sein älterer Bruder Richard Colley Wesley (20. Juni 1760 bis 26. September 1842) folgte dem Vater als Earl of Mornington (1781 bis 1799) bevor er 1799 zum Marquis erhoben wurde. Er wählte den Titel Marquess Wellesley, so dass sein Bruder Arthur bei seiner Erhebung zum Herzog auf den Familiennamen verzichten musste und den Titel eines Herzogs von Wellington wählte.

Als Kind kränklich und wenig ehrgeizig, aber musikalisch begabt (er spielte gerne und oft Violine), stand er ganz im Schatten seiner beiden älteren Brüder. Nach dem Besuch des Eton College von 1781 bis 1785, wo er sich wenig hervortat, sandten ihn seine Eltern zunächst zum 73. Infanterie-Regiment der British Army, in das er am 7. März 1787 eintrat, damit „wenigstens“ ein „passabler“ Soldat aus ihm würde. Danach besuchte er die Militärakademie in Angers (Frankreich). Zur gleichen Zeit studierte ein anderer, im selben Jahr geborener Kadett an der Militärakademie in Brienne: Napoleon Bonaparte.

Im Jahre 1788 wurde Wellesley zum Leutnant befördert. Nach verschiedenen Zwischenstationen bei der Kavallerie und den 12. und 18. leichten Dragonern wurde er 1793 Oberstleutnant des "33rd Foot", ein schneller Aufstieg, der durch das damals übliche Kaufsystem ermöglicht wurde. Während der ganzen Zeit war er Adjutant des Vizekönigs von Irland und nebenbei (1790–1797) Abgeordneter von "Trim" (seinem Familiensitz) im irischen Parlament ("Irish House of Commons" in Dublin, das 1800 aufgelöst wurde, als Irland unter Kriegsrecht gestellt wurde).

Seine aktive militärische Karriere begann 1794, als er im Ersten Koalitionskrieg mit dem Duke of York nach Flandern ging und dort am erfolglosen Feldzug gegen die Franzosen teilnahm. Er kommandierte beim Rückzug die Nachhut.

1796 wurde Wellesley zum Oberst befördert und ging mit seinem Regiment nach Indien, wo im Jahr darauf sein älterer Bruder Richard Generalgouverneur werden sollte. Als 1799 der Vierte Mysore-Krieg gegen den Sultan von Mysore, Tipu Sultan, ausbrach, kommandierte er seine erste Division. Er führte einen sehr erfolgreichen Feldzug im Zweiten Marathenkrieg und konnte dabei seine militärischen Fähigkeiten erheblich ausbauen. Er wurde Oberbefehlshaber der britischen Streitkräfte in Indien und zwang ganz Südindien unter britische Herrschaft. Am 11. August 1803 nahm er die Festung Ahmednagar und besiegte eine überlegene Streitmacht der Marathen in der Schlacht von Assaye. In den nächsten Wochen gelang es seinen Truppen, Burhanpur und die Festung Asirgarh einzunehmen. Er stieß auf Hyderabad vor, siegte am 29. November in der Schlacht von Argaon und stürmte die Festung Gawilgarh. 1805 kehrte er als dekorierter und zum Ritter geschlagener (1804) "Major General Sir Arthur Wellesley" nach Großbritannien zurück, gemeinsam mit Richard, dessen Amtszeit als Generalgouverneur ebenfalls abgelaufen war.

1807 nahm Wellesley als Lieutenant General an einer Expedition nach Dänemark teil. Anfang August 1808 landete er mit 13.000 Mann in Portugal und besiegte zwei Wochen darauf die französischen Truppen in der Schlacht bei Vimeiro. Wellesleys Vorgesetzte Burrard und Dalrymple, die erst nach dem Ende der Schlacht in Portugal eingetroffen waren, schlossen Ende August die Konvention von Cintra ab, welche den Franzosen nicht nur den freien Abzug gewährte, sondern ihnen auch den Erhalt der Kriegsbeute und den Rücktransport auf britischen Schiffen garantierte. Diese Bedingungen, welche von der britischen Öffentlichkeit als Sieg für Frankreich empfunden wurden, führten dazu, dass Wellesley, gemeinsam mit Burrard und Dalrymple, nach Großbritannien zurückbefohlen wurde. Dort hatten sie sich einer Anhörung vor einem Militärgericht zu stellen. Wellesley wurde jedoch entlastet und wurde im Gegensatz zu den beiden anderen Beteiligten, rehabilitiert. Am 27. Oktober 1807 schlossen Frankreich und Spanien im Geheimen den Vertrag von Fontainebleau (1807). In diesem vereinbarten sie die Eroberung und Teilung Portugals. Damit die französischen Truppen Portugal auf dem Landweg erreichen konnten, gewährte Spanien den Franzosen den Durchmarsch durch spanisches Hoheitsgebiet.

Im Frühjahr 1809 versuchten die Franzosen ein zweites Mal, Portugal zu erobern. Wellesley kehrte nach Portugal zurück und übernahm den Oberbefehl der dortigen britisch-portugiesischen Truppen. Am 12. Mai 1809 schlug er Marschall Nicolas Soult in der Zweiten Schlacht bei Oporto. Durch den Sieg in der Schlacht bei Talavera de la Reina am 28. Juli beendete Wellesley die französischen Ambitionen.

Wellesley gab zum Schutze Portugals am 20. Oktober 1809 beim britischen Ingenieur Richard Fletcher die Befestigung der Linien von Torres Vedras in Auftrag, unter dessen Leitung sie von portugiesischen Militäreinheiten und Arbeitern errichtet wurden. Der Vormarsch der Franzosen unter Marschall Masséna hatte am 27. September 1810 in der Schlacht von Buçaco einen empfindlichen Rückschlag erhalten, jedoch blieb die Hauptstadt Lissabon weiterhin bedroht. Am 3. April 1811 scheiterte mit der Schlacht von Sabugal der letzte Versuch Frankreichs, Portugal zu erobern. Durch diese Erfolge schwenkte die Stimmung in Spanien auf die britische Seite um, und Wellesley wurde auch Oberkommandierender der spanischen Streitkräfte. Lord Beresford erhielt den Oberbefehl über die reorganisierte portugiesische Truppenmacht. Nachdem bekannt wurde, dass die französischen Truppen im Westen Spaniens reduziert wurden, marschierte Wellington nach Ciudad Rodrigo und nahm diese Stadt nach kurzer Belagerung am 19. Januar 1812 ein, wofür er durch den Prinzregenten Georg zum Earl of Wellington erhoben wurde. Ab 27. März 1812 begannen die Verbündeten die dritte Belagerung von Badajoz, Wellington nahm die Stadt nach drei Wochen unter Verlust von etwa 4.000 Mann auf britischer Seite am 7. April ein. Die Eroberung erlaubte es den Briten, eigene Operationen im zentralen Spanien einzuleiten. Während sich ein britisches Korps unter General Hill zwischen den französischen Armeen Marmont und Soult gegen den Tajo vorschob, wandte sich die britische Hauptmacht nach Leon. Am 21. Juli erwarteten die Franzosen den Gegner am Tormes und in Stellungen auf den Arapilen und am 22. Juli schlug Wellington den Gegner in der Schlacht von Salamanca. Wellington konnte infolge dieser Kämpfe am 12. August Madrid besetzen, wurde aber kurz darauf wieder aus der Stadt vertrieben und musste die geplante Belagerung von Burgos aufgeben.

Nach der Niederlage Napoleons in Russland und dem Beginn der Kämpfe in Deutschland erhielten die französischen Truppen in Spanien keine Verstärkung mehr. Wellington verbrachte den Winter damit, seine Armee zu reorganisieren und plante für das Frühjahr 1813 die Iberische Halbinsel gänzlich freizukämpfen. Im Mai 1813 begann Wellingtons abschließende Offensive von Portugal aus zwischen Duero und Tajo, in der er zunächst die nördlichen Provinzen Spaniens eroberte und sein Hauptquartier von Lissabon nach Santander verlegte. Wellingtons Truppen marschierten durch das kastilische Hochland in Richtung auf Kantabrien, um die französische Hauptmacht durch Abschneidung der Verbindungswege zum Rückzug aus Zentralspanien zu zwingen. Er griff die französische Hauptmacht unter Joseph Bonaparte am 21. Juni 1813 in der entscheidenden Schlacht von Vitoria mit drei Kolonnen an. Die Schlacht beendete Napoleons Herrschaft in Spanien. Am 7. Juli begann Wellington die Belagerung von San Sebastian. Im Herbst 1813 rang er, inzwischen zum Feldmarschall befördert, mit den Truppen des neuen französischen Oberbefehlshabers Soult auf breiter Front um die Übergänge in den Pyrenäen. In Südfrankreich eindringend, lieferte er sich mit Soult noch am 10. April 1814 die blutige Schlacht bei Toulouse, dann folgte mit der Abdankung Napoleons, das Kriegsende.

Wellesley wurde am 11. Mai 1814 vom Prinzregenten in Vertretung von dessen Vater Georg III., zum (ersten) Duke of Wellington ernannt.

Der Herzog nahm im Frühjahr 1815 unter Lord Castlereagh auch an mehreren Sitzungen des Wiener Kongress teil. Im Februar wurde Wellington nach dessen Abberufung nach England, Hauptbevollmächtigter in Wien, bevor er März 1815 nach der Rückkehr Napoleons aus Elba den Oberbefehl im neuen Krieg gegen Frankreich erhielt. Im Raum Brüssel sammelte Wellington das verbündete Heer gegen Napoleon, darunter nur etwa 35.000 Briten und wartete die geplante Vereinigung mit den Preußen ab. Am 18. Juni in der Schlacht von Waterloo (auch „Schlacht von Belle-Alliance“) von Napoleon angegriffen, hielten Wellingtons Truppen den französischen Angriffen solange erfolgreich stand, bis die Ankunft der Preußen den Sieg der Alliierten entschied. Das bekannte Zitat „Ich wollte, es wäre Nacht, oder die Preußen kämen“ wird Wellesley beim Warten auf die Ankunft Blüchers zugeschrieben, ist aber nicht verbürgt. Die Schlacht ging mit der Hilfe Blüchers zu Wellesleys Gunsten aus, Napoleon zog sich geschlagen zurück, für ihn bedeutete diese Schlacht das Ende seiner militärischen Karriere. Wellington hingegen wurde von den Briten als Held gefeiert, unter den Militärstrategen galt er fortan als Meister der Defensive. 

1827/28 und noch einmal von 1842 bis zu seinem Tod war Wellesley Oberbefehlshaber der britischen Armee. Ab 1829 hatte er auch das Amt des Lord Warden of the Cinque Ports inne. Nach seinem Tod wurde er am 18. November 1852 in einem Staatsbegräbnis in der Krypta der St Paul’s Cathedral beigesetzt. Im Hauptschiff der Kathedrale wurde ihm ein monumentales Grabdenkmal gesetzt.

Im Jahr 1806 zog er als Abgeordneter für den Wahlkreis Rye in Sussex ins britische House of Commons ein. 1807 wurde er Chief Secretary for Ireland, dieses Amt gab er jedoch noch im gleichen Jahr zugunsten seiner militärischen Karriere wieder auf. Nach dem Wiener Kongress wandte er sich wieder einer politischen Laufbahn zu und erhielt 1818 ein Amt in der Tory-Regierung unter Lord Liverpool. Am 17. August 1827 wurde er Oberkommandierender der britischen Armee, doch übernahm er 1828 nach dem Tod von Canning und dem Sturz von Lord Goderich widerstrebend das Amt des Premierministers. Er führte eine erzkonservative Regierung und eine isolationistische Politik. So beendete er trotz des Siegs in der Schlacht von Navarino die Unterstützung des griechischen Freiheitskampfes. Infolge dieser Politik lehnte Prinz Leopold die ihm angebotene griechische Krone ab. Innenpolitisch setzte Wellington gegen große innerparteiliche Widerstände 1829 das Wahlrecht für Katholiken durch. Gleichzeitig versuchte er eine weitere Wahlrechtsreform zu verhindern, weswegen er bei weiten Teilen der Bevölkerung höchst unpopulär wurde. Die Verzögerung der Wahlrechtsreform und seine Unbeliebtheit weiteten sich zu Unruhen aus. Dennoch erklärte er in völliger Verkennung der Lage bei der Parlamentseröffnung nach dem Tod Georgs IV., dass er eine Wahlrechtsreform weiter ablehne. Diese Erklärung führte zum Sturz seiner Regierung am 22. November 1830. Sein Nachfolger als Premierminister Earl Grey nahm sofort eine Wahlrechtsreform in Angriff nahm und brachte den Reform Act am 1. März 1831 gegen den Widerstand Wellingtons ins Unterhaus ein. Nachdem das Gesetz das House of Commons passiert hatte, verweigerte das House of Lords am 8. Oktober 1831 seine Zustimmung. Trotz der Gefahr eines drohenden revolutionären Umsturzes blockierte das Oberhaus das Gesetz weiter. Am 9. Mai 1832 wurde Wellington erneut zum Regierungschef ernannt. Wilhelm IV. bewog ihn, ein gemäßigtes Kabinett zu bilden. Da zahlreiche Sparer aus Protest ihre Einlagen aus der Bank of England abzogen, drohte eine Finanzkrise, so dass Wellington schon am 15. Mai wieder aufgab. Sein Nachfolger wurde wieder Earl Grey, unter dem am 4. Juni 1832 die Wahlrechtsreform vom Oberhaus verabschiedet wurde. 

Die nächsten beiden Jahre verbrachte Wellington in der Opposition. Bei der Trauerfeier für Lord Althorp im Oberhaus entließ Wilhelm IV. unerwartet das Whig-Kabinett und beauftragte Wellington am 17. November 1834 mit der Bildung einer Minderheitsregierung. Dieser schlug schließlich Robert Peel, seinen langjährigen politischen Weggefährten, als Premierminister vor, während er das Amt des Außenministers übernahm. Dies war die letzte britische Regierung, die ein Monarch ohne Mehrheit im Unterhaus ernannte, und scheiterte schon im April 1835. Peel wurde im September 1841 erneut zum Premierminister ernannt, und Wellington wurde in dieser Regierung als Oberkommandierender der Armee Minister ohne Geschäftsbereich sowie "Leader" des House of Lords. Als Peel 1846 zurücktrat, legte auch Wellington am 27. Juni sein Amt als Leader nieder und zog sich aus der Öffentlichkeit zurück. Das Amt des Oberbefehlshabers der Armee behielt er allerdings bis zu seinem Tod.

1805 heiratete er Kitty Pakenham, die Tochter des 2. Baron Longford. Eine Verbindung, die ihm noch ein Jahrzehnt zuvor verweigert worden war, da der damalige "Sir Arthur" dem Brautvater als Ehemann nicht gut genug gewesen war. Aus der Ehe gingen folgende Kinder hervor:


Kittys Bruder Ned war in Spanien einer von Wellingtons wichtigsten Generälen. Wellington war seit dem 7. Dezember 1790 ein Mitglied im Bund der Freimaurer ("Trim No. 494") und gehörte dem renommierten Londoner Travellers Club an.






</doc>
<doc id="135" url="https://de.wikipedia.org/wiki?curid=135" title="Astronomie">
Astronomie

Die Astronomie (griechisch für „Sternenkunde“, von "ástron" „Stern“ und "nómos" „Gesetz“) ist die Wissenschaft von den Gestirnen. Sie untersucht mit naturwissenschaftlichen Mitteln die Positionen, Bewegungen und Eigenschaften der Objekte im Universum, also der Himmelskörper (Planeten, Monde, Asteroiden, Sterne einschließlich der Sonne, Sternhaufen, Galaxien und Galaxienhaufen), der interstellaren Materie und der im Weltall auftretenden Strahlung. Darüber hinaus strebt sie nach einem Verständnis des Universums als Ganzes, seiner Entstehung und seinem Aufbau.

Obwohl die Astronomie nur an wenigen Schulen ein Unterrichtsfach ist, finden ihre Forschungsergebnisse in der Öffentlichkeit viel Interesse; als Amateurastronomie ist sie ein weit verbreitetes Hobby. Dies hängt einerseits mit dem „erhebenden“ Eindruck zusammen, den der Sternhimmel auch bei freisichtiger Beobachtung macht, andererseits mit ihrer thematischen Vielfalt, der Berührung philosophischer Fragen und der Verbindung zur Raumfahrt.

Im Gegensatz zu früheren Zeiten wird die Astronomie als Naturwissenschaft heute streng abgegrenzt von der Astrologie, die aus Stellung und Lauf der Gestirne auf irdische Geschehnisse schließen will.

An den Universitäten wurde die Astronomie um etwa 1800 zu einer eigenen Studienrichtung, wird aber heute zunehmend dem Physik-Studium zugeordnet. In der deutschen Hochschulpolitik wird sie gemeinsam mit der Astrophysik als Kleines Fach eingestuft .

Die Astronomie gilt als eine der ältesten Wissenschaften. Ihre Anfänge liegen im Nachdenken über die Himmelserscheinungen, in der kultischen Verehrung der Gestirne und im Erarbeiten von Kalender bzw. Zeitbestimmung . In einem jahrtausendelangen Prozess – besonders gut erkennbar in der Himmelskunde Mesopotamiens und Griechenlands – trennten sich zunächst Astronomie und („Natur“)-Religion, später Astronomie und Meteorologie, in der Frühmoderne dann Astronomie und Astrologie . Wesentliche Meilensteine für unser Wissen über das Weltall waren die Erfindung des Fernrohrs vor etwa 400 Jahren, das die kopernikanische Wende vollendete, sowie später im 19. Jahrhundert die Einführung der Fotografie und Spektroskopie.

Seit den 1960er-Jahren haben Astronomen mit der unbemannten und bemannten Raumfahrt die Möglichkeit, die Erdatmosphäre zu überwinden und ohne ihre Einschränkungen zu beobachten – also ohne Luftunruhe und in allen Bereichen des elektromagnetischen Spektrums. Dazu kommt erstmals die Möglichkeit, die untersuchten Objekte direkt zu besuchen und dort andere als nur rein beobachtende Messungen durchzuführen. Parallel dazu werden immer größere Teleskope für bodengebundene Beobachtungen gebaut.

Die astronomische Wissenschaft unterteilt sich allgemein nach den untersuchten Objekten, sowie danach, ob die Forschung theoretischer oder beobachtender Natur ist. Wichtige grundlegende Fachgebiete sind die "beobachtende Astronomie", die "Astrophysik", die "Astrometrie" und die "Himmelsmechanik", während die "theoretische Astronomie" analytische und numerisch-physikalische Modelle der Himmelskörper und Phänomene entwickelt.

Die wichtigsten Untersuchungsgebiete der Himmelskunde sind die Physik des Sonnensystems, insbesondere die "Planetologie", die "Galaktische Astronomie", die die Milchstraße und ihr Zentrum erforscht, die "Extragalaktische Astronomie", die den Aufbau anderer Galaxien und ihrer aktiven Kerne, oder Gammablitze als die energiereichsten Vorgänge im Universum untersucht, sowie die "relativistische Astrophysik", die sich etwa mit Schwarzen Löchern beschäftigt. Die "Stellarastronomie" untersucht Geburt, Entwicklung und Tod der Sterne.
Die "Kosmologie" hat die Geschichte und die Entstehung des Universums zum Gegenstand, während die "Kosmogonie" die Geschichte unseres eigenen Sonnensystems beschreibt. Sie erlebt derzeit eine Erweiterung durch das neueste Fachgebiet "Exoplanetologie".

Die Integration vieler Messmethoden bringt es mit sich, dass man die "Beobachtende Astronomie" immer weniger nach benutzten Wellenlängenbereichen ("Radioastronomie", "Infrarotastronomie", "Visuelle Astronomie", "Ultraviolettastronomie", "Röntgenastronomie" und "Gammaastronomie") einteilt, weil die Forschergruppen und (im Idealfall) auch der einzelne Wissenschaftler Informationen aus allen diesen Quellen heranziehen kann.

Die bis etwa 1900 vorherrschenden Methoden der "klassischen Astronomie" sind weiterhin als Basis für andere Teilgebiete unentbehrlich. Sie erforschen als "Positionsastronomie" mittels astrometrischer Verfahren, der Himmelsmechanik und "Stellarstatistik" den Aufbau des Weltalls und katalogisieren die Himmelskörper (v. a. durch Sternkataloge, Bahnbestimmungen und Ephemeriden). Im Gegensatz zu diesen überwiegend geometrischen Verfahren erforscht die "Astrophysik" mit ihren heute sehr vielfältigen Beobachtungstechniken die Physik der astronomischen Objekte und des ferneren Weltalls. Daneben kann die "Raumfahrt" als "experimentelle Astronomie" angesehen werden, und die Kosmologie als theoretische Disziplin.

Mit der Astronomie sehr eng verbunden sind die Physik und die Mathematik; die Fachgebiete haben sich vielfach befruchtet und sind auch im Astronomie-Studium als Einheit zu sehen. Das Universum erweist sich in vielen Fällen als Laboratorium der Physik, viele ihrer Theorien können nur in seinen Weiten und an heißen, energiereichen Objekten getestet werden. Nicht zuletzt waren die aufwändigen Berechnungen der Astronomie Triebfeder der modernen numerischen Mathematik und der Datenverarbeitung.

Traditionell ist die Zusammenarbeit der Astronomie mit der Geodäsie ("Astrogeodäsie", Orts- und Zeitbestimmung, Bezugsysteme, Navigation), mit der Zeit- und Kalenderrechnung ("Astronomische Chronologie") sowie mit der Optik (Entwicklung "astronomischer Instrumente und Sensoren"). Instrumentell und methodisch sind auch starke Bezüge zur Technik, Raumfahrt und Mathematik gegeben (Messgeräte, Satellitentechnik, Modellierung von Bahnen und Himmelskörpern). Geodätische Methoden werden auch zur Bestimmung des Gravitationsfeldes sowie der Figur anderer Himmelskörper angewandt.

In den letzten Jahrzehnten ist auch die Zusammenarbeit der Astronomie mit der modernen Geologie und der Geophysik immer wichtiger geworden, da sich das Arbeitsgebiet der Geowissenschaften mit Teilen der "Planetologie" deckt. Die Mineralogie analysiert die Gesteine der Erde mit ähnlichen Methoden wie jene anderer Himmelskörper. Die "Kosmochemie" als Teil der Chemie untersucht die Entstehung und Verteilung der chemischen Elemente und Verbindungen im Universum und die chemische Evolution, die "Astrobiologie" die Umstände von Entstehung, Ursprung und Existenz von Leben im Universum.

Des Weiteren kommt es zunehmend zu interdisziplinärer Forschung mit ursprünglich eher geisteswissenschaftlich ausgerichteten Disziplinen der Wissenschaft:
Die Astronomiegeschichte als Teil der Geschichtswissenschaften untersucht die "Geschichte der Astronomie".
Bauten und Funde aus vor- und frühgeschichtlicher Zeit werden vermehrt in astronomischem Zusammenhang interpretiert ("Archäoastronomie"). Da sich die Astronomie außerdem im Rahmen der "Kosmologie" mit den Fragen nach der Entstehung, der Entwicklung und dem Ende des Universums beschäftigt, gibt es darüber hinaus Schnittpunkte zu Theologie und Philosophie.



"Siehe auch: Abschnitt Literatur unter Amateurastronomie"




</doc>
<doc id="137" url="https://de.wikipedia.org/wiki?curid=137" title="Angelina Jolie">
Angelina Jolie

Angelina Jolie Pitt [], DCMG (* 4. Juni 1975 als "Angelina Jolie Voight" in Los Angeles, Kalifornien) ist eine US-amerikanische Schauspielerin, Filmregisseurin, Filmproduzentin und Drehbuchautorin.

Sie wurde mit der Darstellung der Videospielheldin Lara Croft in "" (2001) international bekannt. Weitere kommerzielle Erfolge hatte sie mit den Filmen "Mr. & Mrs. Smith" (2005), "Wanted" (2008), "Salt" (2010) und "Maleficent – Die dunkle Fee" (2014). Für ihre schauspielerischen Leistungen erhielt Jolie drei Golden Globes, zwei Screen Actors Guild Awards und für ihre Rolle einer psychisch Kranken in dem Film "Durchgeknallt" (1999) einen Oscar als beste Nebendarstellerin. Mit dem Kriegsdrama "In the Land of Blood and Honey" gab Jolie 2011 ihr Debüt als Spielfilmregisseurin und Drehbuchautorin.

Sie ist zudem Sondergesandte des UN-Flüchtlingshochkommissars Filippo Grandi, Mitglied des Council on Foreign Relations und war Sonderbotschafterin für das UNO-Hochkommissariat für Flüchtlinge.

Angelina Jolie ist die Tochter der Schauspieler Jon Voight und Marcheline Bertrand. Sie ist die jüngere Schwester des Schauspielers James Haven und die Nichte des Songwriters Chip Taylor. Ihre Taufpaten sind die Schauspieler Jacqueline Bisset und Maximilian Schell. Jolie hat deutsche und slowakische Vorfahren väterlicherseits sowie kanadische, niederländische, deutsche und angeblich irokesische Vorfahren mütterlicherseits. Letzteres wurde von Jon Voight in einem Interview allerdings bestritten. Die irokesische Abstammung Bertrands sei erfunden worden, um ihr aus Karrieregründen ein exotisches Image zu verleihen. Ein Urgroßelternpaar Jolies stammt aus Büren in Westfalen, ein anderes aus Košice in der Slowakei.

Jolies Eltern, die am 12. Dezember 1971 geheiratet hatten, trennten sich 1976. Bertrand reichte 1978 die Scheidung ein, die am 14. April 1980 rechtskräftig wurde. Nach der Trennung ihrer Eltern wuchs Jolie zusammen mit ihrem Bruder bei ihrer Mutter auf, die ihre eigenen Schauspielambitionen aufgab und mit den Kindern und ihrem neuen Lebensgefährten Bill Day nach Palisades in den US-Bundesstaat New York zog, wo Jolie im Nachbarort Tappan die William O. Schaefer Elementary School besuchte. Zeit mit ihrem Vater verbrachte Jolie daraufhin meist nur während der Schulferien oder wenn er sie und ihren Bruder zu Dreharbeiten mitnahm. Jolie erklärte später, dass nicht ihr berühmter Vater, der 1979 für seine Rolle in dem Film "Coming Home – Sie kehren heim" (1978) den Oscar als bester Hauptdarsteller gewann, ihr Interesse an der Schauspielerei geweckt habe, sondern die regelmäßigen Kino- und Theaterbesuche mit ihrer Mutter und ihrem Bruder während ihrer Kindheit.

Als Jolie elf Jahre alt war, zog die Familie zurück nach Los Angeles. Dort besuchte sie bis 1989 die El Rodeo Elementary School im Stadtteil Beverly Hills. In ihrer Zeit an der Beverly Hills High School fühlte sie sich oftmals isoliert unter ihren Mitschülern, die größtenteils aus wohlhabenden Familien stammten, während ihre Mutter mit einem bescheidenen Einkommen auskommen musste. Jolie trug Kleidung aus Secondhand-Läden und wurde von anderen Schülern auf Grund ihrer ausgeprägten Gesichtszüge und äußerst schlanken Erscheinung geneckt. Mit wachsender Unzufriedenheit durchlebte sie in ihrer Jugend eine Phase autoaggressiven Verhaltens; sie beschrieb dies später mit den Worten: „Ich sammelte Messer und hatte immer bestimmte Dinge um mich. Aus irgendeinem Grund war das Ritual, mich selbst zu schneiden und die Schmerzen zu spüren, vielleicht sich lebendig zu fühlen und ein Gefühl der Befreiung zu verspüren, irgendwie therapeutisch für mich.“ Heute blickt Jolie auf diese Phase ihres Lebens mit der Bemerkung zurück: „Im Herzen bin ich noch immer nur ein Punk-Kid mit Tattoos.“

Neben der Schule nahm Jolie Schauspielunterricht am Lee Strasberg Theatre and Film Institute, an dem sie zwei Jahre lang das Method Acting erlernte und in mehreren Bühnenproduktionen auftrat. Mit 14 Jahren erhielt sie einen Vertrag als Fotomodell bei der Agentur Finesse Model Management; ihre Versuche, in diesem Geschäft Fuß zu fassen, blieben jedoch ohne nennenswerten Erfolg.

Mit 16 Jahren machte Jolie im Rahmen eines eigenverantwortlichen Lernprogramms vorzeitig ihren Schulabschluss und mietete sich ein eigenes Apartment in der Nähe der Wohnung ihrer Mutter. Sie dachte einige Zeit darüber nach, Bestattungsunternehmerin zu werden, nachdem ihr Großvater gestorben war, entschied sich aber letztlich doch für die Schauspielerei.

In "Zwei in der Tinte" stand Jolie gemeinsam mit ihren Eltern zum ersten Mal für einen Film vor der Kamera. Während ihr Vater in der Komödie an der Seite von Ann-Margret die männliche Hauptrolle spielte, waren Jolie und ihre Mutter in kleineren Nebenrollen zu sehen. Voight beschrieb das Verhalten seiner damals fünfjährigen Tochter während der Dreharbeiten im Jahr 1980 als „gelangweilt“. „Sie war nicht gerade begeistert, mitzuwirken. Aber sie hat uns die Schau gestohlen, weil sie so ehrlich und echt war.“

Ihre ersten professionellen Engagements als Schauspielerin erhielt Jolie in den Musikvideos zu Lenny Kravitz’ "Stand by My Woman", Antonello Vendittis "Alta Marea" (beide 1991), The Lemonheads’ "It’s About Time" und Meat Loafs "Rock and Roll Dreams Come Through" (beide 1993). Außerdem stand sie für fünf Studentenfilme ihres Bruders vor der Kamera, als dieser die USC School of Cinematic Arts in Los Angeles besuchte. Sie spielte auch in den Kurzfilmen "Angela & Viril" sowie "Alice & Viril" (beide 1993) von Regisseur Steven Shainberg mit.

In dem darauf folgenden Low-Budget-Film "Cyborg 2" (1993) verkörperte sie einen menschenähnlichen Roboter, der darauf programmiert ist, sich mit Verführungskünsten den Weg ins Hauptquartier des Feindes zu bahnen und dort zu explodieren. Jolie über den Film: „Nachdem ich ihn gesehen hatte, ging ich nach Hause und musste mich übergeben.“ Die New York Times schrieb jedoch über ihre Darstellung: „Auch wenn sich ihr Schauspiel in "Cyborg 2" noch in seiner Versuch und Irrtum-Phase befand, enthielt es bereits die Saat ihrer heutigen Darstellungsweise. Bereits als Teenager […] wusste Jolie, die Leinwand mit ihrer Präsenz auszufüllen.“

Nach einer Rolle in dem Thriller "Without Evidence" (1995) spielte Jolie an der Seite von Jonny Lee Miller und Matthew Lillard die Hackerin Kate „Acid Burn“ Libby in dem Spielfilm "Hackers – Im Netz des FBI" (1995). Die New York Times schrieb: „Kate (Angelina Jolie) fällt auf. Sie zieht ein noch mürrischeres Gesicht als die übrigen Darsteller und sie ist diese seltene Hackerin, die bewusst in einem durchsichtigen Top an ihrer Tastatur sitzt. Trotz ihres verdrießlichen Auftretens, und das ist alles, was die Rolle erfordert, hat Frau Jolie das süße engelhafte Aussehen ihres Vaters Jon Voight geerbt.“ Der Film spielte keinen Gewinn ein, entwickelte sich aber zu einem Kulthit, nachdem er auf Video erschienen war.

In der Komödie "Liebe und andere …" (1996), einer modernen Adaption von Romeo und Julia unter zwei rivalisierenden italienischen Restauranteigentümern in der New Yorker Bronx, trat sie in der Rolle der Gina Malacici vor die Kamera. Im Roadmovie "Nichts als Trouble mit den Frauen" (1996) spielte sie den Teenager Eleanor Rigby, der sich in Danny Aiellos Filmfigur verliebt, während dieser versucht, ihre Mutter (Anne Archer) zu erobern. Im Jahr 1996 erschien sie außerdem in dem Film "Foxfire" als Margret „Legs“ Sadovsky, eines von fünf Mädchen, die einen ungewöhnlichen Bund eingehen, nachdem sie einen Lehrer zusammengeschlagen haben, der sie sexuell belästigt hatte. Die Los Angeles Times schrieb über Jolies Leistung: „Es bedurfte einer Menge, diese Figur zu entwickeln, aber Jolie, Jon Voights umwerfende Tochter, hat die Präsenz, das Stereotyp zu überwinden. Obwohl die Geschichte von Maddy erzählt wird, ist Legs das Thema und der Katalysator.“

1997 spielte Jolie zusammen mit David Duchovny in dem Thriller "Playing God". Der Film erzählt die Geschichte eines Chirurgen, der seine Approbation verliert und tief in die kriminelle Unterwelt hineingezogen wird, wo er Jolies Figur Claire trifft. Der Film fand bei den Kritikern wenig Beifall, sodass Roger Ebert zu erklären versuchte: „Angelina Jolie findet eine gewisse Wärme in einer Rolle, die normalerweise hart und aggressiv ist; sie erscheint zu nett, um die Freundin eines Verbrechers zu sein, und vielleicht ist sie es auch.“ Danach wirkte sie in dem Fernsehfilm "True Women" (1997) mit, einem historisch-romantischen Drama im Wilden Westen, basierend auf dem gleichnamigen Roman von Janice Woods Windle. Im selben Jahr spielte sie außerdem eine Stripperin in dem Musikvideo der Rolling Stones zu "Anybody Seen My Baby?"

Jolies Karriere erhielt Auftrieb, als sie 1998 für ihre Rolle in der Filmbiografie "Wallace" mit dem Golden Globe als Beste Nebendarstellerin in einem Fernsehfilm ausgezeichnet wurde und eine Nominierung für den Emmy erhielt. Unter der Regie von John Frankenheimer sowie an der Seite von Gary Sinise und Mare Winningham verkörperte Jolie in dem Film Cornelia Wallace, die zweite Ehefrau von George Wallace, seinerzeit Gouverneur von Alabama und Anhänger der Rassentrennung, der angeschossen und querschnittsgelähmt wurde, als er für die US-amerikanische Präsidentschaft kandidierte.

1998 spielte Jolie im HBO-Projekt "Gia – Preis der Schönheit", einem Fernsehfilm über das Leben des lesbischen Supermodels Gia Carangi mit. Der Film beschreibt eine Welt von Sex und Drogen sowie Carangis emotionalen Niedergang und ihren Tod durch AIDS. Vanessa Vance von Reel.com schrieb: „Angelina Jolie erntete große Anerkennung für ihre Rolle als Gia, und es ist leicht zu verstehen warum. Jolie ist ergreifend in ihrer Darstellung, die den Film mit Nerv, Charme und Verzweiflung füllt, und ihre Rolle ist möglicherweise das schönste Wrack, das jemals gefilmt wurde.“ Jolie gewann ihren zweiten Golden Globe und erhielt erneut eine Nominierung für den Emmy, außerdem ihren ersten Screen Actors Guild Award. Jolie zog es in ihren Anfangsjahren häufig vor, entsprechend Lee Strasbergs Method Acting auch in Drehpausen vollkommen in ihrer Rolle zu bleiben. Während der Dreharbeiten zu "Gia" erklärte sie ihrem damaligen Ehemann Jonny Lee Miller, sie sei nicht in der Lage, ihn anzurufen. „Ich sagte ihm: ‚Ich bin allein; ich sterbe; ich bin lesbisch; ich werde dich in den nächsten Wochen nicht sehen.‘“

Nach "Gia" zog Jolie kurzzeitig nach New York, da sie das Gefühl hatte, sie habe „nichts mehr zu geben.“ Sie schrieb sich an der New York University ein, um Film zu studieren, und besuchte Kurse für Drehbuchautoren. Später beschrieb sie diese Zeit als „einfach gut, um mich selbst zu finden.“ 1998 kehrte sie als Gloria McNeary in dem Gangsterfilm "Hell’s Kitchen – Vorhof zur Hölle" auf die Leinwand zurück und trat im selben Jahr auch als junge Partygängerin Joan in dem Episodenfilm "Leben und lieben in L.A." auf. Das Ensemble umfasste unter anderem Sean Connery, Gillian Anderson, Ryan Phillippe und Jon Stewart. Der Film erhielt überwiegend positive Kritiken und Jolie erntete besonderes Lob. Der San Francisco Chronicle schrieb: „Jolie, die sich durch ein überzogenes Skript kämpft, ist eine Sensation als die verzweifelte Klubgängerin, die lernen muss, was sie bereit ist, aufs Spiel zu setzen.“ Das amerikanische National Board of Review zeichnete sie als beste Nachwuchsdarstellerin aus.

1999 erschien sie neben John Cusack, Billy Bob Thornton und Cate Blanchett in Mike Newells Komödiendrama "Turbulenzen – und andere Katastrophen", ein Film über die Rivalität zweier Fluglotsen. Sie spielte Thorntons verführerische Ehefrau Mary Bell, und im darauffolgenden Jahr heiratete sie Thornton auch im echten Leben. Der Film hinterließ gemischte Reaktionen, Jolies Figur wurde besonders kritisiert. Die Washington Post schrieb: „Mary (Angelina Jolie) ist eine völlig lächerliche Autorenkreation; eine Frau, die über sterbende Hibiskuspflanzen weint, eine Menge türkiser Ringe trägt und furchtbar einsam wird, wenn ihr Mann nachts nicht nach Hause kommt.“ Dann arbeitete sie mit Denzel Washington zusammen in "Der Knochenjäger" (1999), einer Adaption des gleichnamigen Romans von Jeffery Deaver. Sie spielte Amelia Donaghy, eine Polizeibeamtin, die vom Suizid ihres Vaters gequält wird und nur widerwillig zustimmt, dem ehemaligen Detective Rhyme zu helfen, einen Serienmörder zu jagen. Der Film spielte weltweit 151 Mio. US-Dollar ein, wurde jedoch überwiegend negativ besprochen. Die Detroit Free Press schrieb: „Jolie, auch wenn sie immer köstlich anzusehen ist, ist schlicht und einfach fehlbesetzt.“

Danach nahm Jolie die Nebenrolle Lisa Rowe in "Durchgeknallt" (1999) an. Der Film erzählt die Geschichte der Psychiatriepatientin Susanna Kaysen und basiert auf Kaysens Memoiren "Girl, Interrupted". Das Psychodrama war ursprünglich als Comeback für die Hauptdarstellerin Winona Ryder konzipiert, wurde stattdessen aber zu Jolies endgültiger Etablierung in Hollywood. Sie gewann ihren dritten Golden Globe, ihren zweiten Screen Actors Guild Award und den Oscar als beste Nebendarstellerin. Variety schrieb, „Jolie ist ausgezeichnet als das extravagante, unverantwortliche Mädchen, das sich letztendlich als viel entscheidender für Susannas Rehabilitation erweist als die Ärzte“ und Roger Ebert urteilte über ihre Leistung: „Jolie entwickelt sich zu einem der großen Freigeister gegenwärtiger Filme, eine lose Kanone, die dennoch tödlich ins Ziel trifft.“

Im Sommer 2000 spielte Jolie in ihrem ersten Blockbuster, "Nur noch 60 Sekunden" die Rolle der Sarah „Sway“ Wayland, die Ex-Freundin eines Autodiebs, der von Nicolas Cage verkörpert wird. Die Rolle war verhältnismäßig klein und die Washington Post kritisierte: „Alles was sie in diesem Film tut, ist herumstehen, sich abkühlen und ihre fleischigen, pulsierenden Muskelröhren zur Schau stellen, die so provozierend um ihre Zähne herum nisten.“ Sie erklärte später, der Film sei für sie nach der anstrengenden Rolle in "Durchgeknallt" eine willkommene Abwechslung gewesen – und es wurde zunächst ihr kommerziell erfolgreichster Film mit einem internationalen Einspielergebnis von 237 Mio. US-Dollar.

Obwohl sie nach dem Oscargewinn für ihre schauspielerischen Fähigkeiten bekannt war, hatten ihre Filme bis dahin selten ein breites Publikum erreicht, doch "" (2001) machte sie zu einem internationalen Superstar. Die Titelrolle des bekannten Videospiels verlangte von Jolie einen britischen Akzent und ein umfassendes Martial-Arts-Training. Sie erhielt große Anerkennung für ihre Darbietung, der Film wurde jedoch allgemein negativ aufgenommen. Das Slant Magazine schrieb: „Angelina Jolie wurde geboren, um Lara Croft zu spielen, aber Regisseur Simon West erlaubt ihr nur einen Ausflug in ein billiges Computerspiel.“ Der Film wurde trotzdem zu einem großen internationalen Erfolg, er spielte weltweit 275 Millionen US-Dollar ein und begründete Jolies Reputation als weiblicher Action-Star.

Anschließend erschien Jolie als Katalogbraut Julia Russell neben Antonio Banderas in "Original Sin" (2001), einem auf Cornell Woolrichs Roman "Waltz into Darkness" basierenden Thriller. Der Film fiel bei der Kritik weitgehend durch und die "New York Times" bemerkte: „Die Geschichte sinkt steiler in sich zusammen als Frau Jolies gewagtes Dekolleté.“ 2002 spielte sie Lanie Kerrigan in "Leben oder so ähnlich", einem Film über eine ehrgeizige Fernsehreporterin, der prophezeit wird, binnen einer Woche zu sterben. Der Film erhielt negative Kritiken, auch wenn Jolies Spiel häufig positiv hervorgehoben wurde. Paul Clinton von CNN urteilte: „Jolie ist ausgezeichnet in ihrer Rolle. Trotz eines teilweise lächerlichen Plots in der Mitte des Films ist die Oscar-gekrönte Schauspielerin äußerst glaubhaft in ihrer Selbstfindung um die wahre Bedeutung vom erfüllten Leben.“
Jolie kehrte 2003 in ihrer Rolle als Lara Croft in "" zurück. Die Fortsetzung erwies sich als weniger erfolgreich als der erste Teil, spielte aber dennoch 157 Mio. US-Dollar an den internationalen Kinokassen ein. Im selben Jahr trat sie außerdem in "Jenseits aller Grenzen" auf, einem Film über humanitäre Hilfe in Afrika. Der Film fiel bei Kritikern und Publikum durch und Jolie wurde für die Goldene Himbeere als schlechteste Schauspielerin nominiert. Die Los Angeles Times schrieb: „Jolie kann Lebhaftigkeit und Glaubwürdigkeit in Figuren bringen, die eine für sie nachvollziehbare Realität haben, wie sie es in ihrer Oscar-Rolle in ‚Durchgeknallt‘ bewies. Sie kann auch bekannte Cartoons spielen, was sie in den Lara-Croft-Filmen zeigte. Aber der Limbo eines gespaltenen Charakters, einer schlecht geschriebenen Figur in einer von Fliegen befallenen und mit Blut und Eingeweiden übersäten Welt, besiegt sie völlig.“ Weiterhin war sie im Musikvideo zu "Did My Time" der Band Korn zu sehen.

2004 war Jolie zusammen mit Ethan Hawke und Kiefer Sutherland in dem Thriller "Taking Lives" auf der Leinwand zu sehen. Sie spielte Illeana Scott, eine FBI-Profilerin, die die Polizei in Montreal dabei unterstützen soll, einen Serienmörder zu überführen. Der Film stieß auf gemischte Reaktionen und brachte ihr eine zweite Himbeeren-Nominierung ein. Der Hollywood Reporter schrieb: „Angelina Jolie spielt eine Rolle, die sich wie etwas anfühlt, das sie schon einmal getan hat, aber sie fügt einen unverkennbaren Schuss von Aufregung und Glamour hinzu.“ Jolie lieferte die Stimme für Lola, einem Fisch im DreamWorks SKG Animationsfilm "Große Haie – Kleine Fische" (2004); weitere Rollen wurden von Will Smith, Martin Scorsese, Renée Zellweger, Jack Black und Robert De Niro gesprochen. Jolie übernahm 2004 auch einen kurzen Gastauftritt als Franky in "Sky Captain and the World of Tomorrow" neben Jude Law, einem Science-Fiction-Film, der komplett vor einem Bluescreen gedreht wurde und bei dem fast alle Sets und Requisiten in der Nachbearbeitung computergeneriert eingefügt wurden. Jolie spielte außerdem Olympias in "Alexander" (2004), Oliver Stones Filmbiographie über das Leben von Alexander dem Großen. Der Film fiel in den Vereinigten Staaten durch, was Stone mit der Darstellung von Alexander als homosexuell in Verbindung brachte, außerhalb Nordamerikas spielte er jedoch 133 Mio. US-Dollar ein. Newsday schrieb über Jolies Leistung: „Jolie ist die Einzige im gesamten Film, die Spaß mit ihrer Rolle zu haben scheint, und man vermisst sie jedes Mal, wenn sie nicht auf der Leinwand zu sehen ist.“

Jolies einziger Film aus dem Jahr 2005, Doug Limans Actionkomödie "Mr. & Mrs. Smith", wurde ihr größter kommerzieller Erfolg. Der Film erzählt eine Geschichte von gelangweilten Eheleuten, die herausfinden, dass sie beide ein Doppelleben als Profikiller führen. Jolie spielte die Agentin Jane Smith neben Brad Pitt. Der Film wurde überwiegend positiv aufgenommen und besonders die gute Chemie zwischen den beiden Hauptdarstellern hervorgehoben. Die "Star Tribune" erklärte: „Während die Geschichte willkürlich erscheint, lebt der Film von seinem geselligen Charme, der galoppierenden Energie und der thermonuklearen Chemie zwischen den beiden Stars.“ Der Film spielte weltweit über 478 Mio. US-Dollar ein und wurde zu einem der größten Erfolge des Kinojahres.

In dem folgenden Jahr übernahm Jolie neben Matt Damon eine Nebenrolle in Robert De Niros "Der gute Hirte", einem Film über die frühe Geschichte der CIA, erzählt aus der Sicht von Edward Wilson. Jolie trat als Margaret Russell auf, Wilsons vernachlässigte Ehefrau, die zunehmend unter den Auswirkungen der Arbeit ihres Ehemanns leidet. Die "Chicago Tribune" kommentierte: „Jolie altert überzeugend im Laufe des Films und ist erfreulich unbesorgt, wie ihre spröde Figur beim Publikum ankommen könnte.“

Jolie spielte außerdem Mariane Pearl in Michael Winterbottoms Dokumentardrama "Ein mutiger Weg" (2007) über die Entführung und Ermordung des Wall-Street-Journal-Reporters Daniel Pearl in Pakistan. Der Film basiert auf Mariane Pearls Memoiren "Ein mutiges Herz: Leben und Tod des Journalisten Daniel Pearl" und hatte seine Uraufführung bei den Filmfestspielen in Cannes. Der Hollywood Reporter beschrieb Jolies Darstellung als „akkurat und bewegend, respektvoll gespielt und den schwierigen Akzent dabei fest im Griff.“ Sie erhielt für die Rolle ihre vierte Golden-Globe- und die dritte Screen-Actors-Guild-Award-Nominierung. Daneben trat sie als Grendels Mutter in einer Nebenrolle in Robert Zemeckis’ animiertem Epos "Die Legende von Beowulf" (2007) auf, der mit Hilfe der Motion-Capture-Technik gefilmt wurde.

Im Sommer 2008 war sie in dem Actionfilm "Wanted", einer Adaption der gleichnamigen Graphic Novel von Mark Millar zu sehen sowie in dem Animationsfilm "Kung Fu Panda" (DreamWorks SKG) als Stimme der Tigerin zu hören. "Wanted", der in Deutschland keine Jugendfreigabe erhielt, löste eine Diskussion um die Darstellung von Gewalt im Kino aus, war jedoch mit einem Einspielergebnis von 343 Mio. US-Dollar weltweit erfolgreich.

Clint Eastwood wählte sie als Hauptdarstellerin für seinen Thriller "Der fremde Sohn". Dieser Film erhielt sehr gute Kritiken und wurde 2008 bei den Filmfestspielen von Cannes gezeigt. Jolie wurde für ihre Darstellung der um ihren Sohn kämpfenden Christine Collins erstmals für den Oscar als Beste Hauptdarstellerin nominiert. 2009 begann sie mit den Dreharbeiten zu dem Action-Thriller "Salt", in dem sie die Agentin Evelyn Salt spielt, die der Spionage bezichtigt wird und daraufhin eine neue Identität annehmen muss. "Salt" kam im Sommer 2010 in die deutschen Kinos.

Im Februar 2010 begannen die Dreharbeiten zu dem Film "The Tourist", bei dem Florian Henckel von Donnersmarck Regie führte und Jolie an der Seite von Johnny Depp die Hauptrolle spielte. Der Film feierte seine Weltpremiere am 7. Dezember 2010 in New York und spielte weltweit über 278 Mio. US-Dollar an den Kinokassen ein. Sowohl Depp als auch Jolie wurden für ihre schauspielerischen Leistungen in dem Thriller für den Golden Globe Award nominiert, gingen aber bei der Verleihung am 16. Januar 2011 leer aus. Bei den Teen Choice Awards 2011 wurde Jolie für ihre Darbietung in "The Tourist" als beste Action-Schauspielerin ausgezeichnet.

Am 18. Juni 2012 begannen die Dreharbeiten zu "Maleficent – Die dunkle Fee", in dem Jolie die titelgebende Hauptrolle spielt. "Maleficent" ist eine auf dem Disney-Zeichentrickklassiker "Dornröschen" (1959) basierende Realverfilmung, deren Weltpremiere am 7. Mai 2014 in London stattfand. In mehreren Interviews betonte Jolie, dass sie selbst Maleficent schon als Kind bewundert habe.

Mit dem Dokumentarfilm "A Place in Time" gab Jolie 2007 ihr Regiedebüt. Der Film beschreibt das Geschehen an 27 verschiedenen Orten der Welt innerhalb einer Woche. An dem Projekt, das vor allem zur Vorführung an Schulen gedacht ist, wirkten unter anderen ihre Schauspielkollegen Jude Law, Hilary Swank, Colin Farrell und ihr Ex-Mann Jonny Lee Miller mit.

Im Herbst 2010 fanden die Dreharbeiten zu "In the Land of Blood and Honey" statt. Das Kriegsdrama, bei dem Jolie Regie führte und für das sie das Drehbuch schrieb, erzählt eine Liebesgeschichte während des Bosnienkrieges von 1992 bis 1995. Jolie besetzte die Rollen ausschließlich mit bosnischen, serbischen und kroatischen Schauspielern wie Zana Marjanović, Nikola Djuricko und Rade Šerbedžija, die den Krieg selbst miterlebt hatten. „[Sie] waren außergewöhnlich. Ich fühlte mich privilegiert und geehrt, mit ihnen arbeiten zu dürfen und freue mich sehr darauf, dass alle bald deren unglaubliches Talent sehen können“, sagte Jolie der Branchenzeitschrift "The Hollywood Reporter". Nachdem Gerüchte über die Filmhandlung in Umlauf gebracht worden waren, wonach der Film die Liebe einer bosnischen Frau zu ihrem serbischen Vergewaltiger thematisieren würde, rief dies scharfe Kritik und Proteste unter anderem von der bosnischen Vereinigung "Women Victims of War" hervor. Bosniens Kulturminister Gavrilo Grahovac entzog Jolie daraufhin vorübergehend die Drehgenehmigung für die Hauptstadt Sarajevo, weshalb große Teile des Films in Budapest gedreht wurden. Die Gerüchte sollten sich später als falsch erweisen. Der Film lief am 23. Dezember 2011 in den amerikanischen Kinos an. Er wurde als bester fremdsprachiger Film bei den Golden Globe Awards 2012 nominiert.

Im Oktober 2013 begann Jolie in Australien unter dem Titel "Unbroken" mit der Verfilmung der Lebensgeschichte von Louis Zamperini. Der Film, für den Ethan und Joel Coen das Drehbuch schrieben, basiert auf Laura Hillenbrands Buch "" aus dem Jahr 2010. Von August bis November 2014 fanden auf Malta die Dreharbeiten des Filmdramas "By the Sea" statt, für das sie das Drehbuch geschrieben hatte und bei dem sie die Regie und die Hauptrolle an der Seite von Brad Pitt übernahm. Es war das erste Mal seit Mr. & Ms. Smith, dass Jolie und Pitt wieder Seite an Seite vor der Kamera standen. Die Kritiken für den Film waren weitestgehend negativ.

Bei den Dreharbeiten zu "Lara Croft: Tomb Raider" im zu großen Teilen verminten Kambodscha kam Jolie zum ersten Mal persönlich mit konkreten humanitären Problemen in Kontakt. Sie wandte sich an das UN-Flüchtlingshilfswerk UNHCR, um weitere Informationen über internationale Krisenherde zu erhalten und stimmte in den darauf folgenden Monaten zu, verschiedene Flüchtlingslager zu besuchen. Im Februar 2001 brach sie zu ihrer ersten Reise auf, einer achtzehntägigen Mission durch Sierra Leone und Tansania; sie berichtete später, wie schockiert sie von den Bedingungen war, die sie dort vorfand. Sie kehrte für zwei Wochen nach Kambodscha zurück und besuchte danach afghanische Flüchtlinge in Pakistan, für die sie im Rahmen eines internationalen UNHCR-Dringlichkeitsappells eine Million US-Dollar spendete. Sie bestand darauf, alle im Zusammenhang ihrer Reisen entstandenen Kosten selbst zu übernehmen, und teilte bei ihren Besuchen die spärlichen Arbeitsbedingungen und Unterbringungen mit den Helfern vor Ort.

UNHCR zeigte sich von Jolies Interesse für Flüchtlinge beeindruckt und ernannte sie am 27. August 2001 im Genfer Hauptquartier Palais des Nations zur UNHCR-Sonderbotschafterin. In einer Pressekonferenz erklärte sie ihre Beweggründe, der Flüchtlingsorganisation beizutreten: „Wir können uns nicht vor Informationen verschließen und die Tatsache ignorieren, dass es Millionen von Menschen auf der Welt gibt, die leiden. Ich möchte helfen. Ich glaube nicht, dass ich mich dabei von anderen Menschen unterscheide. Ich denke, wir wünschen uns alle Gerechtigkeit und Gleichheit, eine Chance für ein Leben mit Bedeutung. Wir alle würden gerne daran glauben, dass uns jemand beistünde, sollten wir einmal in eine schlechte Situation geraten.“

Während ihrer ersten drei Jahre als Sonderbotschafterin konzentrierte Jolie ihre Bemühungen auf Reisen und besuchte Flüchtlinge in verschiedenen Teilen der Welt. Auf die Frage, was sie zu erreichen erhoffe, antwortete sie: „Mehr Bewusstsein über die Lage dieser Menschen zu schaffen. Ich denke, sie sollten dafür gelobt werden, was sie überlebt haben und nicht auf sie herab gesehen werden.“ 2002 besuchte Jolie das "Tham Hin"-Flüchtlingslager in Thailand und kolumbianische Flüchtlinge in Ecuador. Sie reiste außerdem zu UNHCR-Einrichtungen im Kosovo und stattete dem Kakuma-Flüchtlingslager in Kenia, das Vertriebene aus dem Sudan aufnahm, einen Besuch ab. Während der Dreharbeiten zu "Jenseits aller Grenzen" besuchte sie außerdem angolanische Flüchtlinge in Namibia. Im Film wurde das humanitäre Engagement Jolies für das UN-Flüchtlingshilfswerk UNHCR eingebunden.

2003 unternahm Jolie eine sechstägige Mission nach Tansania, wo sie Lager für kongolesische Flüchtlinge in der westlichen Grenzregion besuchte, und sie reiste für eine Woche nach Sri Lanka. Sie begab sich außerdem auf eine viertägige Mission in den Nordkaukasus in Russland und veröffentlichte zum Kinostart von "Jenseits aller Grenzen" im Oktober 2003 das Buch "Tagebuch einer Reise – Begegnungen mit Flüchtlingen in Afrika, Kambodscha, Pakistan und Ecuador", eine Zusammenstellung von Notizen ihrer frühen Reisen (2001–2002). Bei einem privaten Aufenthalt in Jordanien im Dezember 2003 besuchte sie irakische Flüchtlinge in der jordanischen Wüste und sudanesische Flüchtlinge in Ägypten.

Angelina Jolie und Brad Pitt unterstützen die SOS-Kinderdörfer bereits seit Längerem mit größeren finanziellen Beiträgen. Dabei sorgen sie insbesondere für Darfur und Haiti. Angelina Jolie hat sich schon im Jahr 2003 ein persönliches Bild von der Situation vor Ort verschafft und die Kinder im Katastrophengebiet in Haiti, genauer gesagt im SOS-Kinderdorf Santo bei Port-au-Prince, besucht.

Auf ihrer ersten UNO-Reise innerhalb der USA begab sich Jolie im Jahr 2004 nach Arizona, wo sie Asylbewerber in drei Einrichtungen besuchte, und sie besichtigte in Phoenix Unterbringungen für Kinder und Jugendliche ohne Begleitung oder rechtlichen Beistand. Als Reaktion auf die sich durch den Darfur-Konflikt verschlechternde humanitäre Situation im Westen Sudans flog sie im Juni 2004 nach Tschad und inspizierte Flüchtlingslager im Grenzgebiet zu Darfur. Vier Monate später kehrte sie in die Region zurück und begab sich direkt nach West-Darfur. Jolie besuchte 2004 auch afghanische Flüchtlinge in Thailand und stattete während eines privaten Aufenthalts im Libanon zur Weihnachtszeit dem regionalen UNHCR-Büro in Beirut einen Besuch ab und traf sich dort mit jungen Flüchtlingen und Krebspatienten.

Jolie besuchte im Mai 2005 afghanische Flüchtlinge in Pakistan und traf sich mit Pakistans Präsidenten Pervez Musharraf und Premierminister Shaukat Aziz. Sie kehrte im November zusammen mit Brad Pitt nach Pakistan zurück, um die Folgen des Erdbebens in Kaschmir zu sehen. 2006 besuchten Jolie und Pitt eine vom Hip-Hop-Musiker Wyclef Jean und seiner Wohltätigkeitsorganisation Yéle Haïti unterstützte Schule in Haiti und statteten im November während der Dreharbeiten zu "Ein mutiger Weg" in Indien afghanischen und birmanischen Flüchtlingen in Neu-Delhi einen Besuch ab. Jolie verbrachte den ersten Weihnachtstag 2006 mit kolumbianischen Flüchtlingen in San José, Costa Rica, wo sie Geschenke verteilte und sich mit Regierungsbeamten traf. Im Februar 2007 kehrte Jolie für eine zweitägige Mission nach Tschad zurück, um sich ein Bild von der sich verschlechternden Sicherheitslage für Flüchtlinge aus Darfur zu machen; Jolie und Pitt spendeten daraufhin eine Million US-Dollar an drei Hilfsorganisationen in Tschad und Darfur. Im August 2007 unternahm Jolie ihre erste Reise nach Syrien und in den Irak, wo sie neben irakischen Flüchtlingen auch US-Truppen traf. Sechs Monate später kehrte sie in den Irak zurück. Dabei reiste sie in die Grüne Zone nach Bagdad und traf sich unter anderem mit dem irakischen Ministerpräsidenten Dschawad al-Maliki und dem US-Oberbefehlshaber in der Region, General David Petraeus.

Mit zunehmender Erfahrung begann Jolie humanitäre Probleme auch auf einer politischen Ebene zu thematisieren. Sie nimmt regelmäßig an den Feierlichkeiten zum Weltflüchtlingstag in Washington, D.C. teil und war 2005 und 2006 Gastrednerin auf dem Weltwirtschaftsforum in Davos. Daneben versucht sie, Einfluss auf die Gesetzgebung in Washington zu nehmen. Sie traf sich seit 2003 mindestens zwanzig Mal mit Kongressabgeordneten und Senatoren. Sie erklärte: „Auch wenn ich es vorziehen würde, nie nach Washington kommen zu müssen, ist das der Ort, um etwas zu bewegen.“

Jolie unterstützte unter anderem ein Gesetz zum Schutz von minderjährigen Asylbewerbern und sie war im März 2005 an der Gründung einer nationalen Organisation beteiligt, die minderjährige Asylbewerber, die ohne Eltern oder Verwandte in die USA einreisen, kostenlos vor Gericht vertritt; Jolie finanzierte die Einrichtung mit einer Spende von 500.000 US-Dollar für die ersten zwei Jahre. Daneben unterstützte sie verschiedene Gesetzesvorhaben des US-Kongresses, die Entwicklungshilfe für Kinder in der Dritten Welt zu verbessern.

Neben ihren politischen Aktivitäten begann Jolie, das öffentliche Interesse an ihrer Person darauf zu verwenden, humanitäre Probleme in den Massenmedien zu platzieren. Im Mai 2005 filmte sie die MTV-Sendung, "The Diary Of Angelina Jolie & Dr. Jeffrey Sachs in Africa", eine Dokumentation, die sie und den bekannten Wirtschaftswissenschaftler Jeffrey Sachs auf einer Reise nach Sauri, einer entlegenen Gruppe von Dörfern im westlichen Kenia, begleitete. Dort arbeitet Sachs’ Team des UN-Millennium-Projekts mit Einheimischen zusammen, um Armut, Hunger und Krankheiten zu beenden. Im September 2006 verkündete Jolie die Schaffung der Jolie/Pitt Foundation; die Stiftung tätigte zur Gründung zwei Spenden von jeweils einer Million US-Dollar an Global Action for Children und Ärzte ohne Grenzen.

Jolie erntete breite Anerkennung für ihre humanitäre Arbeit. 2003 war sie die erste Preisträgerin des neu geschaffenen "Citizen of the World Award" des Verbandes der UNO-Korrespondenten und 2005 erhielt Jolie den "Global Humanitarian Award" von der UNA-USA, einer amerikanischen Einrichtung zur Unterstützung der UNO. Kambodschas König Norodom Sihamoni verlieh Jolie am 12. August 2005 die kambodschanische Staatsbürgerschaft als Dank für ihre Arbeit zur Erhaltung der Umwelt in seinem Land; sie sicherte fünf Millionen US-Dollar zu, um die Tierwelt innerhalb eines Nationalparks in der nordwestlichen Provinz Battambang zu erhalten, in der sie ein Haus besitzt. 2007 wurde Jolie Mitglied des Council on Foreign Relations und mit dem "Freedom Award" des International Rescue Committee ausgezeichnet.

2010 unterstützte Jolie die Initiative Ein Logo für Menschenrechte.

Im April 2012 wurde Jolie zur Ehrenbürgerin Sarajevos ernannt. In der Begründung hieß es, sie habe mit ihrem Regiedebüt "In the Land of Blood and Honey" dazu beigetragen, ein Stück Geschichte zu wahren und „die Prinzipien der Menschlichkeit, Demokratie, ebenso wie die Toleranz und die Solidarität von Menschen unterschiedlicher ethnischer Herkunft, Religion und kulturellem Hintergrund zu schützen.“ Am 16. November 2013 wurde Angelina Jolie bei den Governors Awards in Los Angeles mit dem Jean Hersholt Humanitarian Award („Ehrenoscar“) für ihr humanitäres Engagement unter anderem als Sondergesandte des UN-Flüchtlingshochkommissariats ausgezeichnet.

Nach dem Erscheinen ihres Films "In the Land of Blood and Honey" führte Jolie zusammen mit dem britischen Außenminister William Hague eine zweijährige Kampagne gegen Vergewaltigung als Kriegstaktik, die im Juni 2014 mit einer Gipfelkonferenz in London abgeschlossen wurde. Ziel der Kampagne war es, die Verdrängung und Banalisierung des Themas zu beenden und die Weltgemeinschaft zum Engagement gegen sexuelle Gewalt in Konflikten aufzurufen. Aufgrund ihres Engagements wurde Angelina Jolie 2014 von Königin Elizabeth II. mit dem Ordenszeichen Honorary Dame Commander des Most Distinguished Order of St. Michael and St. George geehrt.

Seit 2012 lässt sich Jolie von der britischen Politikerin und Menschenrechtlerin Arminka Helic und der britischen Außenpolitik-Spezialistin Chloe Dalton beraten.

Am 28. März 1996 heiratete Jolie den britischen Schauspieler Jonny Lee Miller, den sie während der Dreharbeiten zu "Hackers – Im Netz des FBI" kennengelernt hatte. Jolie und Miller trennten sich ein Jahr später, wurden im Februar 1999 geschieden, blieben aber befreundet. Während der Dreharbeiten zu "Foxfire" (1996) ging Jolie eine sexuelle Beziehung mit ihrer Filmpartnerin Jenny Shimizu ein. Als sie 2003 in einem Interview mit Barbara Walters gefragt wurde, ob sie bisexuell sei, bestätigte Jolie dies.

Am 5. Mai 2000 heiratete Jolie den 20 Jahre älteren Schauspieler Billy Bob Thornton, ihren Filmpartner aus "Turbulenzen – und andere Katastrophen". Am 10. März 2002 adoptierten sie einen kambodschanischen Jungen (* 5. August 2001) aus einem Waisenhaus in Phnom Penh und gab ihm den Namen Maddox Chivan. Nach der Scheidung von Thornton am 27. Mai 2003 erhielt Jolie das alleinige Sorgerecht.

Im Juli 2002 reichte Jolie einen Antrag auf Namensänderung ein, um Voight als Familiennamen zu streichen und ihren bürgerlichen Namen in Angelina Jolie zu ändern; die Änderung wurde am 12. September 2002 offiziell bestätigt. Im August desselben Jahres sagte Jon Voight im US-amerikanischen Fernsehen, seine Tochter habe „ernste emotionale Schwierigkeiten“. Jolie erklärte 2004, sie sei nicht länger an einer Beziehung zu ihrem Vater interessiert. Sie gab an, dass sie die genauen Gründe für die Entfremdung von ihrem Vater nicht öffentlich machen wolle, aber sie glaube, es sei schädlich für sie, sich weiterhin mit ihrem Vater einzulassen, da sie gerade ein Kind adoptiert habe.

Im Frühjahr 2005 geriet Jolie ins Visier der Boulevardpresse. Sie sei der Trennungsgrund des Schauspielerehepaares Brad Pitt und Jennifer Aniston. Die Boulevardmedien spekulierten, ob sie und Pitt während der Dreharbeiten von "Mr. & Mrs. Smith" eine Affäre begonnen hätten. Jolie bestritt dies in verschiedenen Interviews. Am 6. Juli 2005 adoptierte Jolie einen Säugling aus einem Waisenhaus in Addis Abeba und gab dem äthiopischen Mädchen (* 8. Januar 2005) den Namen Zahara Marley. Nach einem Jahr intensiver Berichterstattung der Boulevardmedien, in dem sich beide – auch nach der Scheidung Pitts von Aniston im Oktober 2005 – nie zum Wesen ihrer Beziehung geäußert hatten, offenbarte Jolie am 11. Januar 2006 gegenüber der Zeitschrift "People", dass sie ein Kind von Pitt erwarte. Kurz darauf wurde die von Pitt beantragte Adoption von Jolies Adoptivkindern rechtsgültig.

Die erste leibliche Tochter des Paares wurde am 27. Mai 2006 in Namibia geboren. Für die Erlaubnis zur Veröffentlichung der ersten Fotos von Shiloh Nouvel Jolie-Pitt zahlte die Zeitschrift "People" dem Paar 4,1 Millionen US-Dollar – den bis dahin höchsten Preis für sogenannte „Celebrity-Fotos“. Am 15. März 2007 adoptierte Jolie – wegen des vietnamesischen Adoptionsrechts zunächst allein – einen Jungen (* 29. November 2003) aus einem Waisenhaus in Ho-Chi-Minh-Stadt. Ein Jahr später wurde auch Pitts Adoption von Pax Thien rechtskräftig. Beim Filmfestival von Cannes im Mai 2008 bestätigte Jolie ihre Schwangerschaft mit Zwillingen. Knox Léon und Vivienne Marcheline Jolie-Pitt wurden am 12. Juli 2008 in Nizza geboren. Die Rechte an den ersten öffentlichen Fotos der Zwillinge gingen für 14 Millionen US-Dollar erneut an die Zeitschrift "People".

Am 14. Mai 2013 veröffentlichte Jolie im Op-Ed der "New York Times" einen Debattenbeitrag mit dem Titel "My Medical Choice", in dem sie davon berichtete, dass sie sich einer beidseitigen prophylaktischen Mastektomie unterzogen habe, um ihr hohes individuelles Brustkrebsrisiko zu minimieren. Ohne Operation habe ihr Risiko, an Brustkrebs zu erkranken, aufgrund einer Mutation im BRCA1-Gen 87 % betragen. Ihr Risiko eines Eierstockkrebses sei auf 50 % geschätzt worden. Die Berichterstattung bewirkte, dass sich Frauen weltweit vermehrt für Brust-Diagnostik und genetische Beratung interessierten ("Jolie-Effekt"). Am 24. März 2015 veröffentlichte Jolie unter dem Titel "Diary of a Surgery" einen weiteren Gastbeitrag in der "New York Times", in dem sie mitteilte, dass sie sich inzwischen auch einer prophylaktischen Entfernung beider Eierstöcke und der Eileiter unterzogen habe.

Nachdem Pitts Managerin im April 2012 bereits die Verlobung des Paares bekanntgegeben hatte, heirateten Jolie und Pitt am 23. August 2014 auf ihrem Weingut Château Miraval an der Côte d’Azur im Beisein von Familie und Freunden. Ihr bürgerlicher Name lautet seit der Eheschließung Jolie Pitt.

Angelina Jolie hat mindestens 20 Tätowierungen (Stand Februar 2016). Darunter befindet sich ein traditionelles Khmer-Tattoo, das Unglück und Unfälle abwenden soll, ein Ausspruch von Tennessee Williams „A prayer for the wild at heart, kept in cages“, ein zwölf Zoll großer Tiger und die geographischen Koordinaten der Geburtsorte von Brad Pitt und ihrer Kinder. Sie ließ verschiedene Tätowierungen entfernen, darunter auch den Schriftzug "Billy Bob", den Vornamen ihres zweiten Ehemanns. Im September 2016 reichte Jolie die Scheidung von Pitt ein.

Jolie ist heute eine der bekanntesten Persönlichkeiten weltweit. Laut Q-Score-Index von Marketing Evaluations Inc. kannten Jolie nach ihrem Oscargewinn 31 % der Befragten in den Vereinigten Staaten im Jahr 2000, 2006 war sie bereits für 81 % aller Amerikaner ein Begriff. In einer globalen Studie auf 42 internationalen Märkten von ACNielsen aus dem Jahr 2006 wurde Jolie zusammen mit Brad Pitt zur weltweit bevorzugten Werbeträgerin für Marken und Produkte ermittelt. Daneben wurde Jolie 2006 und 2008 von Time in deren jährliche Liste der 100 einflussreichsten Personen der Welt aufgenommen. Vom US-amerikanischen Wirtschaftsmagazin Forbes wurde Jolie auf der sogenannten „Celebrity 100“, einer Rangliste der einflussreichsten Prominenten, 2006 auf Position 35 und 2007 auf Rang 14 geführt.

Im Februar 2007 wurde sie im Rahmen der britischen Fernsehshow "The 100 Greatest Sex Symbols" vor Elvis Presley und Marilyn Monroe zum größten Sexsymbol aller Zeiten gewählt. 2008 wählten die Leser der deutschen FHM Jolie auf einer Liste der 100 schönsten Frauen auf Platz 12.

2008 zählte Jolie laut Forbes Magazine zu den am besten verdienenden Schauspielerinnen in Hollywood. Zwischen Juni 2007 und Juni 2008 erhielt sie Gagen in Höhe von 14 Mio. US-Dollar. 2011 hatte sie in einem neuerlichen Forbes-Ranking die Spitzenposition durch ihr Mitwirken in "Salt" und "The Tourist" (30 Mio. US-Dollar) gemeinsam mit Sarah Jessica Parker inne.

Das Ausmaß der Berichterstattung über Angelina Jolie wird auch daran deutlich, dass die Schauspielerin zwischen Mai 2011 und Mai 2012 auf den Titelblättern von mindestens 78 Zeitschriften zu sehen war.

Schauspielerin

Die deutsche Synchronstimme von Jolie ist seit dem Jahr 2000 bis auf wenige Ausnahmen Claudia Urbschat-Mingues.

Regisseurin

Drehbuchautorin

Produzentin

Synchronsprecherin

Musikvideos

Academy Award of Merit („Oscar“)

British Academy Film Award

Emmy Award

Golden Globe Award

Nickelodeon Kids’ Choice Awards

National Board of Review Award

Screen Actors Guild Award

Negativpreise

Hollywood Film Award



Englischsprachige Literatur und Quellen



</doc>
<doc id="139" url="https://de.wikipedia.org/wiki?curid=139" title="Archimedes">
Archimedes

Archimedes von Syrakus (griechisch Ἀρχιμήδης "Archimḗdēs"; * um 287 v. Chr. vermutlich in Syrakus; † 212 v. Chr. ebenda) war ein griechischer Mathematiker, Physiker und Ingenieur. Er gilt als einer der bedeutendsten Mathematiker der Antike. Seine Werke waren auch noch im 16. und 17. Jahrhundert bei der Entwicklung der höheren Analysis von Bedeutung.

Über das Leben des Archimedes ist wenig bekannt und vieles gilt als Legende.

Archimedes, geboren ca. 287 v. Chr. wahrscheinlich in der Hafenstadt Syrakus auf Sizilien, war der Sohn des Pheidias, eines Astronomen am Hof Hierons II. von Syrakus. Mit diesem und dessen Sohn und Mitregenten Gelon II. war er befreundet und möglicherweise verwandt.

Bei einem längeren Aufenthalt in Alexandria lernte Archimedes die dortigen Mathematiker Konon, Dositheos und Eratosthenes kennen, mit denen er später weiter korrespondierte.

Als er nach Syrakus zurückgekehrt war, betrieb er Mathematik und praktische Physik (Mechanik). Seine Wurfmaschinen wurden bei der Verteidigung von Syrakus gegen die römische Belagerung im Zweiten Punischen Krieg eingesetzt. Bei der Eroberung von Syrakus 212 v. Chr. nach dreijähriger Belagerung durch den römischen Feldherrn M. Claudius Marcellus wurde er sehr zum Bedauern von Marcellus, der ihn lebend gefangensetzen wollte, von einem römischen Soldaten getötet. Über die Umstände referiert Plutarch in seiner Biographie des Marcellus mehrere überlieferte Versionen, nach einer war er mit einem mathematischen Beweis beschäftigt und forderte einen beim Plündern der Stadt eindringenden Soldaten auf, ihn nicht zu stören, worauf der ihn erschlug. Sprichwörtlich wurden die Worte "Noli turbare circulos meos" (lateinisch für: „Störe meine Kreise nicht“), die Archimedes dabei gesprochen haben soll.

Nach Plutarch hatte Archimedes sich testamentarisch ein Grab mit der Darstellung von Kugel und Zylinder gewünscht, da er offensichtlich auf seine Abhandlung "perì sphaíras kaì kylíndrou" („Über Kugel und Zylinder“) besonders stolz war. Cicero berichtet in den Tuskulanischen Gesprächen, dass er in seiner Zeit als Quästor in Sizilien (75. v. Chr.) nach dem Grab suchte und es von Gestrüpp zugewuchert fand.

Eine von seinem Freund Heracleides geschriebene Biographie ist nicht erhalten.

Die erhaltenen Hauptschriften sind:

Hinzu kommen:

Die hier angegebene Reihenfolge der Hauptschriften bis zur "Sandrechnung" entspricht der chronologischen Reihenfolge, wie sie von Thomas Heath angegeben wurde, wobei die "Quadratur der Parabel" zwischen den Büchern 1 und 2 von "Gleichgewicht ebener Flächen" eingeordnet wurde und "Über die Methode" zwischen "Gleichgewicht ebener Flächen", Buch 2, und "Über Kugel und Zylinder". An der Chronologie gab es aber auch Kritik.

In der "Quadratur der Parabel" wird der kürzliche Tod seines Freundes Konon erwähnt, so dass sich diese Schrift um 240 v. Chr. datieren lässt. Nach der erwähnten relativen Datierung sind die meisten Werke des Archimedes erst danach entstanden. Das Buch über Spiralen wurde nach Archimedes Angaben viele Jahre nach dem Tod des Konon geschrieben, so dass es nach Ivo Schneider etwa 230 v. Chr. zu datieren ist. Schneider ordnet die Methodenlehre Ende der 220er Jahre ein und die "Schwimmenden Körper" als letztes Werk in die letzten acht Lebensjahre, aber wohl vor 216 v. Chr. wegen der nachfolgenden Kriegsereignisse.

Es gibt Hinweise auf einige heute verloren gegangene Schriften, zum Beispiel über Polyeder und über Hebel (von Pappos erwähnt), über die Darstellung von Zahlen (von Archimedes in seinem "Sandrechner" erwähnt) und über Spiegel ("Catoptrica", von Theon von Alexandria erwähnt). Aus der Unvollständigkeit der "mechanischen Schriften" des Archimedes ("Gleichgewicht ebener Flächen", "Quadratur der Parabel") und mehrerer Hinweise bei Archimedes (und zum Beispiel bei Heron von Alexandria) wurde auf die Existenz verloren gegangener Teile seiner Mechanik geschlossen, die A. G. Drachmann zu rekonstruieren versuchte. Diese teilweise rekonstruierten mechanischen Schriften stehen chronologisch am Anfang der Werke des Archimedes.

Es gibt einige Hinweise auf verloren gegangene Schriften des Archimedes in arabischer Übersetzung, so ein Buch über das Parallelenpostulat, das im Bücherkatalog von Ibn al-Nadim aufgeführt ist und möglicherweise die Behandlung des Themas bei Thabit Ibn Qurra beeinflusste. Von Thabit Ibn Qurra stammt auch die Übersetzung einer Abhandlung von Archimedes über die Konstruktion des regulären Heptagons, die erhalten ist. Die Konstruktion darin ist unvollständig, wurde aber von Abu Sahl al-Quhi vervollständigt.

Archimedes war sowohl in der Mathematik als auch im Bereich der heutigen Physik gleichermaßen schöpferisch tätig.

Es wurden ihm auch die Erfindung und Kombination verschiedener Maschinenelemente (wie Schrauben, Seilzüge mit Wellrädern, Flaschenzüge und Zahnräder) zugeschrieben, die er auch praktisch demonstrierte. Nach Plutarch bevorzugte er abstraktes Denken und sah auf praktische Anwendungen und die Arbeiten eines Ingenieurs, obwohl er sich ihnen im Auftrag seines Königs Hieron widmete, mit Verachtung herab. Aus diesem Grund hinterließ er auch keine Abhandlung über praktische Erfindungen. Seine Schriften zur Mechanik und Hydrostatik sind nach dem Vorbild der Geometrie streng axiomatisch aufgebaut.

Archimedes formulierte die Hebelgesetze (in seiner Schrift "Über das Gleichgewicht ebener Flächen") und schuf dadurch die theoretische Grundlage für die spätere Entwicklung der Mechanik. Er selbst entwickelte aus dem Hebelgesetz bereits die wissenschaftlichen Grundlagen der Statik für statisch bestimmte Systeme. Die Beschreibung des Hebels selbst findet sich schon in älteren griechischen Schriften aus der Schule des Aristoteles.

Er soll (wie Pappos und andere überlieferten) gesagt haben: „Gebt mir einen festen Punkt, und ich hebe die Welt aus den Angeln“. Darauf gründet sich der Begriff des archimedischen Punktes. Als er sich einmal gegenüber Hieron so äußerte, verlangte dieser nach Plutarch einen praktischen Beweis, und Archimedes bewerkstelligte unter anderem mit Flaschenzügen (Plutarch) und Seilwinden die Bewegung eines großen voll beladenen Schiffs durch einen einzigen Mann.

Nach Vitruv sollte Archimedes den Goldgehalt einer vom Herrscher Hieron II. den Göttern geweihten Krone prüfen, ohne sie jedoch zu beschädigen. Der König verdächtigte den Goldschmied, ihn betrogen zu haben. Um die gestellte Aufgabe zu lösen, tauchte er einmal die Krone und dann einen Goldbarren (sowie einen Silberbarren), der genauso viel wog wie die Krone, in einen vollen Wasserbehälter und maß die Menge des überlaufenden Wassers. Die Krone verdrängte mehr Wasser als der Goldbarren. Dadurch war bewiesen, dass die Krone ein kleineres spezifisches Gewicht hatte und daher nicht ganz aus Gold gefertigt war. Archimedes soll der Legende nach das Archimedische Prinzip beim Baden entdeckt haben. Aus dem randvollen Wasserbehälter sei jene Wassermenge ausgelaufen, die er beim Hineinsteigen ins Bad mit seinem Körpervolumen verdrängte. Glücklich über seine Entdeckung soll er mit dem Ausruf „Heureka!“ (altgriechisch: , „Ich hab’s gefunden!“) nackt auf die Straße gelaufen sein. Die Anekdote von der Überprüfung des Goldgehalts der Krone Hierons durch Wasserverdrängung ist aber kritisiert worden – diese wäre mit den Mitteln der damaligen Zeit nur schwer durchzuführen gewesen und ist wahrscheinlich eine Legende. Schon Galileo Galilei vermutete deshalb 1586, Archimedes hätte stattdessen eine Waage benutzt zur Messung der Gewichte unter Auftrieb.

Das Archimedische Prinzip kann bei jedem schwimmenden Körper Anwendung finden. Es stellt beim Schiffbau eine zwingend zu berücksichtigende Tatsache dar. Bei seinen hydrostatischen Experimenten entdeckte er zudem das Prinzip der kommunizierenden Gefäße.

Archimedes bewies, dass sich der Umfang eines Kreises zu seinem Durchmesser genauso verhält wie die Fläche des Kreises zum Quadrat des Radius. Er nannte dieses (heute als Pi oder Kreiszahl bezeichnete) Verhältnis noch nicht π (Pi), gab aber eine Anleitung, wie man sich dem Verhältnis bis zu einer beliebig hohen Genauigkeit nähern kann, vermutlich das älteste numerische Verfahren der Geschichte. Mit seinen Überlegungen zur Flächen- und Volumenberechnung (u. a. mit einer exakten Quadratur der Parabel) nahm Archimedes Ideen der Integralrechnung viel später folgender Denker vorweg. Er ging dabei über die Eudoxos von Knidos zugeschriebene Exhaustionsmethode (Ausschöpfungsmethode) hinaus; beispielsweise wandte er bereits eine Form des Prinzips von Cavalieri an.

1906 fand Johan Ludvig Heiberg (1854–1928), ein dänischer Philologe und Professor an der Universität Kopenhagen, in Istanbul ein auf das 10. Jahrhundert datiertes Manuskript, das unter anderem eine Abschrift von Archimedes’ Schrift "Die Methode" enthielt.

Darin gibt er eine mechanische Methode preis, mit der er viele seiner Resultate erzielt hatte, bevor er sie in geometrisch strenger Weise bewies. Die Methode entspricht einem "Wiegen" der zu vergleichenden Volumina bzw. Flächenstücke, allerdings in geometrischer Form. Bei seiner Beschreibung erwähnt Archimedes auch ein älteres Verfahren von Demokrit, bei dem es sich möglicherweise um das Wiegen von Modellen handelt.

Außerdem entwickelte Archimedes ein stellenwertbasiertes Zahlensystem mit der Basis 10.

Er benutzte es, um astronomisch große Zahlen (bis zur Größe von 10) mathematisch fassen zu können – dies in einer Zeit, in der seine Mitwelt eine Myriade (lit. 10.000) bereits mit „unendlich“ gleichsetzte. Anlass dafür war die Abhandlung "Über schwimmende Körper und die Sandzahl", auch kurz "Sandrechner" genannt, die er dem Sohn von Hieron II., Gelon, widmete. Darin heißt es: "„Es gibt Leute, König Gelon, die der Meinung sind, die Zahl des Sandes sei unendlich groß […] Andere glauben zwar nicht, dass die Zahl unendlich sei, aber doch, dass noch keine Zahl genannt worden sei, die seine Menge übertreffen könnte.“ " Da Gelon als König angesprochen wird, entstand die Schrift nach 240 v. Chr., als er Mitregent wurde (und vor Gelons Tod 216 v. Chr.).

Er widerlegte diese Vorstellungen, indem er in der Abhandlung die Anzahl der Sandkörner, die alle Strände der Erde bedeckten, abschätzte und "benannte". Er ging sogar noch weiter und berechnete die Anzahl der Sandkörner, die man benötigte, um das ganze Universum mit Sand anzufüllen. Damals stellte man sich das Universum allerdings noch wesentlich kleiner vor – nämlich als Kugel von etwa der Größe unseres Sonnensystems. Archimedes’ Rechnung besagt demnach, dass in eine gedachte Kugel von der Größe unseres Sonnensystems etwa 10 Sandkörner hineinpassen würden.

Obwohl nach ihm benannt, stammt das archimedische Axiom nicht von Archimedes, sondern geht auf Eudoxos von Knidos zurück, der dieses Prinzip im Rahmen seiner Größenlehre einführte.

Nach Pappos stammen die Archimedischen Körper von ihm.

Archimedes hat die Technik seiner Zeit und die spätere Entwicklung der Technik, insbesondere der Mechanik, maßgeblich beeinflusst. Er selbst konstruierte allerlei mechanische Geräte, nicht zuletzt auch Kriegsmaschinen.

Archimedes wird die Erfindung der sogenannten "archimedischen Schraube" zugeschrieben, zu der er angeregt wurde, nachdem er bei seinem Studienaufenthalt in Ägypten die dortigen einfachen Vorrichtungen zur Feldbewässerung gesehen hatte. Das Prinzip der archimedischen Schraube kommt heutzutage in modernen Förderanlagen, sogenannten Schneckenförderern zum Einsatz.
Möglicherweise wurde sie von Archimedes als Lenzpumpe für Schiffe entwickelt, denn nach Athenäus von Naukratis beauftragte König Hieron Archimedes mit dem Bau des größten Schiffs der damaligen Zeit, der Syracusia.

Archimedes soll nach Plutarch die Römer bei ihrer langwierigen Belagerung mit den von ihm entwickelten Kriegsmaschinen aufgehalten haben: So entwickelte er beispielsweise Wurfmaschinen und Katapulte oder auch Seilwinden, welche ein komplettes Schiff, voll beladen und mit gesamter Besatzung, durch Ziehen an einem einzigen Seil bewegten. Auch mächtige Greifarme, die feindliche Boote packten und angeblich in Stücke rissen, gehörten dazu.

Die "Kralle von Archimedes" soll eine Waffe gegen angreifende Flotten gewesen sein, die in der Stadtmauer von Syrakus eingebaut war und bei dessen Belagerung gegen die Römische Flotte eingesetzt wurde. Die genaue Funktion dieser Waffe ist allerdings unklar. In alten Schriften wird die Waffe als ein Hebel mit einem großen Eisenhaken dargestellt. Bereits im Jahre 425 v.Chr. verfügte die Stadt Syrakus über eine als „Eisenhand“ beschriebene Seekriegswaffe, mit der man Schiffe entern konnte (Thukydides, Pel. Kr. IV, 25), möglicherweise ein Enterhaken.

Außerdem soll Archimedes die Schiffe der Römer sogar über große Entfernung mit Hilfe von Spiegeln, die das Sonnenlicht umlenkten und fokussierten, in Brand gesteckt haben. Das wird von Lukian von Samosata und später von Anthemios von Tralleis berichtet. Dazu gibt es eine über 300 Jahre währende, heftige Kontroverse. Historisch sprechen die Quellenlage, Übersetzungsfragen ("pyreia" wurde oft mit Brennspiegel übersetzt, obwohl es nur „Entzündung“ heißt und auch Brandpfeile umfasst) und das erst Jahrhunderte spätere Auftauchen der Legende dagegen. Physikalische Gegenargumente sind die notwendige Mindestgröße und Brennweite eines solchen Spiegels, die zu erreichende Mindesttemperatur zur Entzündung von Holz (etwa 300 Grad Celsius) und die Zeit, die das zu entzündende Holzstück konstant beleuchtet bleiben muss. Technische Gegenargumente diskutieren die Herstellbarkeit solcher Spiegel zur damaligen Zeit, die Montage eines Spiegels oder Spiegelsystems und die Bedienbarkeit. Ein moderner Kritiker der Legende war der Pyrotechniker Dennis L. Simms. Zur Machbarkeit wurden mehrfach Experimente durchgeführt. Studenten des Massachusetts Institute of Technology und der University of Arizona haben 2005 erfolgreich mit 127 kleinen Spiegeln ein 30 Meter entferntes Modell einer Schiffswand entzündet, nachdem der Versuch zuvor mit zwei Spiegeln misslungen war. Allerdings musste der Himmel wolkenlos sein und das Schiff für rund 10 Minuten konstant bestrahlt werden. Ein unter Beteiligung der MIT-Studenten im Hafen von San Francisco an einem Fischerboot wiederholter Versuch in der Fernsehsendung MythBusters mit 500 Freiwilligen (gesendet im Januar 2006), der zu ähnlichen Ergebnissen kam, wurde deshalb als Fehlschlag eingestuft. Zusätzlich wurde angemerkt, dass das Meer in Syrakus im Osten liegt, die römische Flotte also am Morgen hätte angreifen müssen, und dass Wurfgeschosse und Brandpfeile effektiver gewesen wären. Möglicherweise entstand die Geschichte als Rückschluss aus der verlorenen Schrift von Archimedes "Katóptrika" ("Optik").

Nach Cicero ("De re publica") brachte Marcellus zwei von Archimedes entwickelte mechanische Planetarien zurück nach Rom. Ähnliche Geräte wurden nach Cicero schon von Eudoxos von Knidos und Thales von Milet gebaut – archäologische Beweise für solche Instrumente fanden sich später im Antikythera-Mechanismus. Möglicherweise handelt die verlorengegangene, von Pappos erwähnte Schrift des Archimedes "Über die Herstellung von Sphären" vom Bau von Planetarien.

Ihm wird auch die Erfindung eines Odometers zugeschrieben. Ein entsprechendes Odometer mit einem Zählmechanismus mit Bällen wurde von Vitruv beschrieben. Vitruv verrät den Erfinder nicht (nur, dass er von den "Alten" überliefert wurde), doch wurde auch hier Archimedes als Erfinder vermutet. Auch ein Wasseruhr-Mechanismus, der Bälle als Zähl-Hilfsmittel freigibt, beschrieben in einem arabischen Manuskript, wurde ihm zugeschrieben.

Leonardo da Vinci und Petrarca (der sich auf eine Cicero-Handschrift berief) schrieben Archimedes die Erfindung einer Dampfkanone zu. Leonardo fertigte auch Rekonstruktionsskizzen für die von ihm Architronito genannte Maschine an. Es gab später Versuche von Nachbauten, wie von dem Griechen Ioannis Sakas 1981 und dem italienischen Ingenieur Cesare Rossi von der Universität Neapel 2010. Rossi gab dort auch den Brennspiegeln eine neue Interpretation – sie hätten demnach die Hitze für die Dampferzeugung geliefert. In den überlieferten antiken Schriften von und über Archimedes finden sich dafür aber keine Hinweise und Experten wie Serafina Cuomo sehen darin nur einen weiteren Beweis für den legendären Ruf von Archimedes, dem man alle möglichen Erfindungen zuschrieb. Prinzipiell war den Griechen die Dampfkraft bekannt (Heronsball, 1. Jahrhundert n. Chr.).

Die Kenntnis der Werke des Archimedes war trotz seiner von Legenden gespeisten Bekanntheit in der Antike nicht sehr verbreitet, im Gegensatz etwa zu Euklid, der sein Buch im damaligen wissenschaftlichen Zentrum Alexandria zusammenstellte. Allerdings wird er von den Mathematikern Heron, Pappos und Theon in Alexandria häufig erwähnt. Die Schriften wurden zwischen dem 6. und 10. Jahrhundert in Byzanz systematisch gesammelt und kommentiert. Bekannt ist der Kommentar des Eutokios (der von Ende des 5. Jahrhunderts bis Anfang des 6. Jahrhunderts lebte) zu den wichtigsten Archimedes-Schriften (Über Kugel und Zylinder, Kreismessung, Gleichgewicht ebener Flächen), der auch im Mittelalter in Westeuropa viel zur Kenntnis der Werke beitrug und anregend wirkte. Bei der ersten Zusammenstellung der Schriften in Byzanz spielten die Architekten der Hagia Sophia Isidor von Milet und Anthemios von Tralleis eine wichtige Rolle. Weitere Schriften kamen hinzu, bis im 9. Jahrhundert Leon von Thessaloniki die als Kodex A (Heiberg) bekannte Sammlung fast aller überlieferten Archimedischen Schriften (außer "Stomachion", "Rinderproblem", "Über die Methode" und "Über schwimmende Körper") herausbrachte. Das war eine der beiden Quellen für die lateinischen Übersetzungen von Wilhelm von Moerbeke (abgeschlossen 1269). Das andere ihm zur Verfügung stehende griechische Manuskript des Archimedes enthielt "Gleichgewicht ebener Flächen", "Quadratur der Parabel", "Über schwimmende Körper", vielleicht auch "Über Spiralen" und wurde von Heiberg Kodex B genannt. Das 1906 von Heiberg entdeckte Archimedes-Palimpsest (Kodex C, das vorher in Jerusalem war, es enthielt "Über die Methode", "Stomachion" und "Über Schwimmende Körper") war den Übersetzern in Mittelalter und Renaissance unbekannt. Die Kodices A und B kamen aus dem Besitz der normannischen Könige in Sizilien in den Vatikan, wo Moerbeke sie für seine Übersetzung benutzte. Während Moerbekes Übersetzungs-Manuskript im Vatikan erhalten ist, ist Kodex B verloren. Von Kodex A sind dagegen mehrere Abschriften erhalten (neun sind bekannt), die zum Beispiel im Besitz von Kardinal Bessarion (heute in der Biblioteca Marciana) und Giorgio Valla waren. Das Original von Kodex A ist ebenfalls verschwunden.

Die Übersetzungen Wilhelms von Moerbeke regten insbesondere die Gelehrten der Pariser Schule an (Nicole Oresme, Johannes de Muris).

Es gibt auch eine arabische Textüberlieferung. Archimedes' wichtigste Werke "Über Kugel und Zylinder" und "Über Kreismessung" wurden schon im 9. Jahrhundert ins Arabische übersetzt und mindestens bis ins 13. Jahrhundert immer wieder neu herausgegeben. Sie wirkten auch ab dem 12. Jahrhundert im Westen. Insbesondere eine Übersetzung der Kreismessung aus dem Arabischen ins Lateinische, die wahrscheinlich von Gerhard von Cremona (12. Jahrhundert) stammt, war im Mittelalter einflussreich. Von ihm stammt auch eine lateinische Übersetzung eines Traktats der Banū Mūsā Brüder, das weitere Ergebnisse von Archimedes enthielt: neben Kreismessung und Satz des Heron (den die Araber häufig Archimedes zuschrieben) Teile aus "Über Kugel und Zylinder". Dieses als "Verba filiorum" bekannte Manuskript regte zum Beispiel auch Leonardo Fibonacci und Jordanus Nemorarius an. Beide wirkten als Mathematiker vor der Zeit, in der Moerbekes Übersetzung entstand.

Um 1460 ließ Papst Nikolaus V. von Jakob von Cremona eine neue Übersetzung ins Lateinische anfertigen, basierend auf Kodex A. Sie enthielt auch die von Moerbeke noch nicht übersetzten Teile des Werks (Sandrechner und Kommentar des Eutokios zur Kreismessung). Da ihm Kodex B nicht zur Verfügung stand, enthält die Ausgabe nicht "Über schwimmende Körper". Diese Übersetzung wurde unter anderem von Nikolaus von Kues benutzt.

Die erste gedruckte Ausgabe (von Auszügen abgesehen, die Giorgio Valla 1501 druckte) waren die lateinischen Übersetzungen von Kreismessung und Quadratur der Parabel von Luca Gaurico in Venedig 1503 (nach einem Manuskript aus Madrid). Sie wurden 1543 von Nicolo Tartaglia wieder veröffentlicht zusammen mit Moerbekes Übersetzungen von "Gleichgewicht ebener Flächen" und "Über schwimmende Körper".

Die erste Ausgabe des griechischen Textes erschien 1544 in Basel (herausgegeben von Thomas Venatorius, deutsch Gechauff) zusammen mit einer lateinischen Übersetzung von Jakob von Cremona (korrigiert von Regiomontanus). Die Ausgabe enthielt auch die Kommentare von Eutokios. Für den lateinischen Text benutzte er eine von Regiomontanus um 1468 nach Deutschland gebrachte Abschrift der Übersetzung von Jakob von Cremona (bearbeitet von Regiomontanus) sowie für den griechischen Text eine von Willibald Pirckheimer aus Rom nach Nürnberg gebrachte Handschrift. Sie war eine Abschrift von Kodex A, weshalb in dieser "Editio Princeps"-Ausgabe auch "Über Schwimmende Körper" fehlt. 1558 erschien eine lateinische Übersetzung einiger Hauptschriften von Federicus Commandinus in Venedig. Wichtige weitere Ausgaben vor der Heiberg-Ausgabe waren von D´Rivault (Paris 1615), der nur die Propositionen auf Griechisch bringt und die Beweise in Latein, und von Giuseppe Torelli (Oxford 1794).

Ein Bildnis von Archimedes ist auf der höchsten Mathematikerauszeichnung, der Fields-Medaille, geprägt.

Ihm zu Ehren wurde auf dem Mare Imbrium ein Mondkrater "Archimedes" genannt; siehe Archimedes (Mondkrater).

István Száva schrieb den Roman "Der Gigant von Syrakus" (Prisma, Leipzig 1960, Corvina, Budapest 1960, 1968, 1978).



Übersichtsdarstellungen
Gesamtdarstellungen und Untersuchungen
Rezeption

Digitalisate:

Von Archimedes

Über Archimedes


</doc>
<doc id="140" url="https://de.wikipedia.org/wiki?curid=140" title="Aristoteles">
Aristoteles

<onlyinclude>Aristoteles (, Betonung lateinisch und deutsch: Aristóteles; * 384 v. Chr. in Stageira; † 322 v. Chr. in Chalkis auf Euböa) war ein griechischer Gelehrter. Er gehört zu den bekanntesten und einflussreichsten Philosophen und Naturforschern der Geschichte. Sein Lehrer war Platon, doch hat Aristoteles zahlreiche Disziplinen entweder selbst begründet oder maßgeblich beeinflusst, darunter Wissenschaftstheorie, Naturphilosophie, Logik, Biologie, Physik, Ethik, Staatstheorie und Dichtungstheorie. Aus seinem Gedankengut entwickelte sich der Aristotelismus.
</onlyinclude> 

Der aus einer Arztfamilie stammende Aristoteles kam mit siebzehn Jahren nach Athen. Im Jahr 367 v. Chr. trat er in Platons Akademie ein. Dort beteiligte er sich an Forschung und Lehre. Nach Platons Tod verließ er 347 Athen. 343/342 wurde er Lehrer Alexanders des Großen, des Thronfolgers im Königreich Makedonien. 335/334 kehrte er nach Athen zurück. Er gehörte nun nicht mehr der Akademie an, sondern lehrte und forschte selbständig mit seinen Schülern im Lykeion. 323/322 musste er wegen politischer Spannungen Athen erneut verlassen und begab sich nach Chalkis, wo er bald darauf verstarb.

Die an eine breite Öffentlichkeit gerichteten Schriften des Aristoteles in Dialogform sind verloren. Die erhalten gebliebenen Lehrschriften waren größtenteils nur für den internen Gebrauch im Unterricht bestimmt und wurden fortlaufend redigiert. Themenbereiche sind:

Logik, Wissenschaftstheorie, Rhetorik: In den logischen Schriften arbeitet Aristoteles auf der Grundlage von Diskussionspraktiken in der Akademie eine Argumentationstheorie (Dialektik) aus und begründet mit der Syllogistik die formale Logik. Auf der Basis seiner Syllogistik erarbeitet er eine Wissenschaftstheorie und liefert unter anderem bedeutende Beiträge zur Definitionstheorie und Bedeutungstheorie. Die Rhetorik beschreibt er als die Kunst, Aussagen als plausibel zu erweisen, und rückt sie damit in die Nähe der Logik.

Naturlehre: Aristoteles’ Naturphilosophie thematisiert die Grundlagen jeder Naturbetrachtung: die Arten und Prinzipien der Veränderung. Der damals aktuellen Frage, wie Entstehen und Vergehen möglich ist, begegnet er mit Hilfe seiner bekannten Unterscheidung von Form und Materie: Dieselbe Materie kann unterschiedliche Formen annehmen. In seinen naturwissenschaftlichen Werken untersucht er auch die Teile und die Verhaltensweisen der Tiere sowie des Menschen und ihre Funktionen. In seiner Seelenlehre – in der „beseelt sein“ „lebendig sein“ bedeutet – argumentiert er, dass die Seele, die die verschiedenen vitalen Funktionen von Lebewesen ausmache, dem Körper als seine Form zukomme. Er forscht aber auch empirisch und liefert bedeutende Beiträge zur zoologischen Biologie.

Metaphysik: In seiner Metaphysik argumentiert Aristoteles (gegen Platons Annahme von abstrakten Entitäten) zunächst dafür, dass die konkreten Einzeldinge (wie Sokrates) die Substanzen, d. h. das Grundlegende aller Wirklichkeit sind. Dies ergänzt er um seine spätere Lehre, wonach die Substanz konkreter Einzeldinge ihre Form ist.

Ethik und Staatslehre: Das Ziel des menschlichen Lebens, so Aristoteles in seiner Ethik, ist das gute Leben, das Glück. Für ein glückliches Leben muss man Verstandestugenden und (durch Erziehung und Gewöhnung) Charaktertugenden ausbilden, wozu ein entsprechender Umgang mit Begierden und Emotionen gehört. Seine politische Philosophie schließt an die Ethik an. Demnach ist der Staat als Gemeinschaftsform eine Voraussetzung für das menschliche Glück. Aristoteles fragt nach den Bedingungen des Glücks und vergleicht zu diesem Zweck unterschiedliche Verfassungen. Die Staatsformenlehre, die er entwickelt hat, genoss über viele Jahrhunderte unangefochtene Autorität.

Dichtungstheorie: In seiner Theorie der Dichtung behandelt Aristoteles insbesondere die Tragödie, deren Funktion aus seiner Sicht darin besteht, Furcht und Mitleid zu erregen, um beim Zuschauer eine Reinigung von diesen Emotionen zu bewirken "(katharsis)."

Das naturwissenschaftliche Forschungsprogramm des Aristoteles wurde nach seinem Tod von seinem Mitarbeiter Theophrastos fortgesetzt, der auch die aristotelische Schule, den Peripatos, im juristischen Sinne gründete. Die Aristoteles-Kommentierung setzte erst im 1. Jahrhundert v. Chr. ein und wurde insbesondere von Platonikern betrieben. Durch die Vermittlung von Porphyrios und Boethius wurde die aristotelische Logik für das lateinischsprachige Mittelalter wegweisend. Seit dem 12./13. Jahrhundert lagen alle grundlegenden Werke des Aristoteles in lateinischer Übersetzung vor. Sie waren für den Wissenschaftsbetrieb der Scholastik bis in die Frühe Neuzeit maßgeblich. Die Auseinandersetzung mit der aristotelischen Naturlehre prägte die Naturwissenschaft des Spätmittelalters und der Renaissance. Im arabischsprachigen Raum war Aristoteles im Mittelalter der am intensivsten rezipierte antike Autor. Sein Werk hat auf vielfältige Weise die Geistesgeschichte geprägt; wichtige Unterscheidungen und Begriffe wie „Substanz“, „Akzidenz“, „Materie“, „Form“, „Energie“, „Potenz“, „Kategorie“, „Theorie“ und „Praxis“ gehen auf Aristoteles zurück.

Aristoteles wurde 384 v. Chr. in Stageira, einer damals selbständigen ionischen Kleinstadt an der Ostküste der Chalkidike, geboren. Daher wird er mitunter „der Stagirit“ genannt. Sein Vater Nikomachos war Leibarzt des Königs Amyntas III. von Makedonien, seine Mutter Phaestis stammte aus einer Arztfamilie von Chalkis auf Euboia. Nikomachos starb, bevor Aristoteles volljährig wurde. Proxenos aus Atarneus wurde zum Vormund bestimmt.

367 v. Chr. kam Aristoteles als Siebzehnjähriger nach Athen und trat in Platons Akademie ein. Dort beschäftigte er sich zunächst mit den mathematischen und dialektischen Themen, die den Anfang der Studien in der Akademie bildeten. Schon früh begann er Werke zu verfassen, darunter Dialoge nach dem Vorbild derjenigen Platons. Er setzte sich auch mit der zeitgenössischen Rhetorik auseinander, insbesondere mit dem Unterricht des Redners Isokrates. Gegen das auf unmittelbaren Nutzen abzielende pädagogische Konzept des Isokrates verteidigte er das platonische Erziehungsideal der philosophischen Schulung des Denkens. Er nahm eine Lehrtätigkeit an der Akademie auf. In diesem Zusammenhang entstanden als Vorlesungsmanuskripte die ältesten seiner überlieferten Lehrschriften, darunter die logischen Schriften, die später unter der Bezeichnung "Organon" („Werkzeug“) zusammengefasst wurden. Einige Textstellen lassen erkennen, dass der Hörsaal mit Gemälden geschmückt war, die Szenen aus dem Leben von Platons Lehrer Sokrates zeigten.

Nach Platons Tod verließ Aristoteles 347 v. Chr. Athen. Möglicherweise war er nicht damit einverstanden, dass Platons Neffe Speusippos die Leitung der Akademie übernahm; außerdem war er in politische Schwierigkeiten geraten. Im Jahr 348 v. Chr. hatte König Philipp II. von Makedonien die Chalkidike erobert, Olynthos zerstört und auch Aristoteles’ Heimatstadt Stageira eingenommen. Dieser Feldzug wurde von der antimakedonischen Partei in Athen als schwere Bedrohung der Unabhängigkeit Athens erkannt. Wegen der traditionellen Verbundenheit der Familie des Aristoteles mit dem makedonischen Hof richtete sich die antimakedonische Stimmung auch gegen ihn. Da er kein Athener Bürger, sondern nur ein Metöke von zweifelhafter Loyalität war, war seine Stellung in der Stadt relativ schwach.

Er folgte einer Einladung des Hermias, der die Städte Assos und Atarneus an der kleinasiatischen Küste gegenüber der Insel Lesbos beherrschte. Zur Sicherung seines Machtbereichs gegen die Perser war Hermias mit Makedonien verbündet. In Assos fanden auch andere Philosophen Zuflucht. Der sehr umstrittene Hermias wird von der ihm freundlichen Überlieferung als weiser und heldenhafter Philosoph, von der gegnerischen als Tyrann beschrieben. Aristoteles, der mit Hermias befreundet war, blieb zunächst in Assos, aber 345/344 v. Chr. übersiedelte er nach Mytilene auf Lesbos. Dort arbeitete er mit seinem aus Lesbos stammenden Schüler Theophrastos zusammen, der sein Interesse für Biologie teilte. Später begaben sich die beiden nach Stageira.

343/342 v. Chr. ging Aristoteles auf Einladung von Philipp II. nach Mieza, um dessen damals dreizehnjährigen Sohn Alexander (später „der Große“ genannt) zu unterrichten. Der Unterricht endete spätestens 340/339 v. Chr., als Alexander für seinen abwesenden Vater die Regentschaft übernahm. Aristoteles ließ für Alexander eine Abschrift der Ilias anfertigen, die der König als Verehrer des Achilleus später auf seinen Eroberungszügen mit sich führte. Das Verhältnis zwischen Lehrer und Schüler ist nicht näher bekannt; es hat zu Legendenbildung und vielen Spekulationen Anlass gegeben. Sicher ist, dass ihre politischen Überzeugungen grundverschieden waren; ein Einfluss des Aristoteles auf Alexander ist nicht erkennbar. Aristoteles soll am makedonischen Hof den Wiederaufbau seiner zerstörten Heimatstadt Stageira erreicht haben; die Glaubwürdigkeit dieser Nachricht ist aber zweifelhaft.

Die Hinrichtung des Hermias durch die Perser 341/340 berührte Aristoteles tief, wie ein dem Andenken des Freundes gewidmetes Gedicht zeigt.

Als nach dem Tode des Speusippos 339/338 v. Chr. in der Akademie das Amt des Scholarchen (Schulleiters) frei wurde, konnte Aristoteles nur wegen seiner Abwesenheit an der Wahl des Nachfolgers nicht teilnehmen; er galt somit weiterhin als Akademiemitglied. Später ging er mit seinem Großneffen, dem Geschichtsschreiber Kallisthenes, nach Delphi, um im Auftrag der dortigen Amphiktyonen eine Siegerliste der Pythischen Spiele anzufertigen.

Mit der Zerstörung der rebellischen Stadt Theben 335 v. Chr. brach der offene Widerstand gegen die Makedonen in Griechenland zusammen, und auch in Athen arrangierte man sich mit den Machtverhältnissen. Daher konnte Aristoteles 335/334 v. Chr. nach Athen zurückkehren. Er begann dort wieder zu forschen und zu lehren, war aber nun nicht mehr an der Akademie tätig, sondern in einem öffentlichen Gymnasium, dem Lykeion. Dort schuf er eine eigene Schule, deren Leitung nach seinem Tod Theophrastos übernahm. Neue Grabungen haben möglicherweise die Identifizierung des Gebäudekomplexes ermöglicht. Im juristischen Sinne hat aber erst Theophrastos die Schule gegründet und das Grundstück erworben, und die später üblichen Bezeichnungen Peripatos und Peripatetiker speziell für diese Schule sind für die Zeit des Theophrastos noch nicht bezeugt. Die Fülle des Materials, das Aristoteles sammelte (etwa zu den 158 Verfassungen der griechischen Stadtstaaten), lässt darauf schließen, dass er über zahlreiche Mitarbeiter verfügte, die auch außerhalb von Athen recherchierten. Er war wohlhabend und besaß eine große Bibliothek. Sein Verhältnis zu dem makedonischen Statthalter Antipatros war freundschaftlich.

Nach dem Tod Alexanders des Großen 323 v. Chr. setzten sich in Athen und anderen griechischen Städten zunächst antimakedonische Kräfte durch. Delphi widerrief ein Aristoteles verliehenes Ehrendekret. In Athen kam es zu Anfeindungen, die ihm ein ruhiges Weiterarbeiten unmöglich machten. Daher verließ er 323/322 v. Chr. Athen. Angeblich äußerte er bei diesem Anlass, dass er nicht wollte, dass die Athener sich ein zweites Mal gegen die Philosophie vergingen (nachdem sie bereits Sokrates zum Tode verurteilt hatten). Er zog sich nach Chalkis auf Euboia in das Haus seiner Mutter zurück. Dort starb er im Oktober 322 v. Chr.

Aristoteles war mit Pythias, einer Verwandten seines Freundes Hermias, verheiratet. Von ihr hatte er eine Tochter, die ebenfalls Pythias hieß. Nach dem Tod seiner Gattin wurde Herpyllis, die niedriger Herkunft war, seine Lebensgefährtin; sie war möglicherweise die Mutter seines Sohnes Nikomachos. In seinem Testament, dessen Vollstreckung er Antipatros anvertraute, regelte Aristoteles unter anderem die künftige Verheiratung seiner noch minderjährigen Tochter und traf Vorkehrungen zur materiellen Absicherung von Herpyllis.

"Hinweis: Belege aus Werken des Aristoteles sind folgendermaßen angegeben: Titelangabe (Abkürzungen werden an der ersten Stelle im Kapitel per Link aufgelöst) und gegebenenfalls Buch- und Kapitelangabe sowie Bekker-Zahl. Die Bekker-Zahl gibt eine genaue Stelle im Corpus an. Sie ist in guten modernen Ausgaben vermerkt."

Aufgrund von Brüchen und Inkonsequenzen im Werk des Aristoteles ist die Forschung von der früher verbreiteten Vorstellung abgekommen, das überlieferte Werk bilde ein abgeschlossenes, durchkomponiertes System. Diese Brüche gehen vermutlich auf Entwicklungen, Perspektivwechsel und unterschiedliche Akzentuierungen in verschiedenen Kontexten zurück. Da eine sichere chronologische Reihenfolge seiner Schriften nicht bestimmt werden kann, bleiben Aussagen über Aristoteles’ tatsächliche Entwicklung Vermutungen. Zwar bildet sein Werk "de facto" kein fertiges System, doch besitzt seine Philosophie Eigenschaften eines "potentiellen" Systems.

Verschiedene antike Verzeichnisse schreiben Aristoteles fast 200 Titel zu. Sofern die Angabe des Diogenes Laertios stimmt, hat Aristoteles ein Lebenswerk von über 445.270 Zeilen hinterlassen (wobei in dieser Zahl zwei der umfangreichsten Schriften – die "Metaphysik" und die "Nikomachische Ethik" – vermutlich noch nicht berücksichtigt sind). Nur etwa ein Viertel davon ist überliefert.

In der Forschung werden zwei Gruppen unterschieden: "exoterische" Schriften (die für ein breiteres Publikum veröffentlicht worden sind) und "esoterische" (die zum internen Gebrauch der Schule dienten). Alle exoterischen Schriften sind nicht oder nur in Fragmenten vorhanden, die meisten esoterischen überliefert. Die Schrift "Die Verfassung der Athener" galt als verloren und wurde erst Ende des 19. Jahrhunderts in Papyrusform gefunden.

Die exoterischen Schriften bestanden vor allem aus Dialogen in der Tradition Platons, z. B. der "Protreptikos" – eine Werbeschrift für die Philosophie –, Untersuchungen wie "Über die Ideen," aber auch propädeutische Sammlungen. Cicero lobt ihren „goldenen Fluss der Rede“. Die auch Pragmatien genannten esoterischen Schriften sind vielfach als Vorlesungsmanuskripte bezeichnet worden; gesichert ist dies nicht und für einige Schriften oder Abschnitte auch unwahrscheinlich. Weitgehend herrscht die Auffassung, dass sie aus der Lehrtätigkeit erwachsen sind. Weite Teile der Pragmatien weisen einen eigentümlichen Stil voller Auslassungen, Andeutungen, Gedankensprünge und Dubletten auf. Daneben finden sich jedoch auch stilistisch ausgefeilte Passagen, die (neben den Dubletten) deutlich machen, dass Aristoteles wiederholt an seinen Texten gearbeitet hat, und die Möglichkeit nahelegen, dass er an die Veröffentlichung mindestens einiger der Pragmatien gedacht hat. Aristoteles setzt bei seinen Adressaten große Vorkenntnisse fremder Texte und Theorien voraus. Verweise auf die exoterischen Schriften zeigen, dass deren Kenntnis ebenfalls vorausgesetzt wird.

Nach dem Tod des Aristoteles blieben seine Manuskripte zunächst im Besitz seiner Schüler. Als sein Schüler und Nachfolger Theophrast starb, soll dessen Schüler Neleus die Bibliothek des Aristoteles erhalten und mit dieser – aus Ärger darüber, nicht zum Nachfolger gewählt worden zu sein – mit einigen Anhängern Athen Richtung Skepsis in der Nähe Trojas in Kleinasien verlassen haben. Die antiken Berichte erwähnen eine abenteuerliche und zweifelhafte Geschichte, nach der die Erben des Neleus die Manuskripte zur Sicherung vor fremdem Zugriff im Keller vergruben, wo sie dann aber verschollen blieben. Weitgehend gesichert ist, dass im ersten Jahrhundert v. Chr. Apellikon von Teos die beschädigten Manuskripte erworben und nach Athen gebracht hat und dass sie nach der Eroberung von Athen durch Sulla im Jahr 86 v. Chr. nach Rom gelangt sind. Dessen Sohn beauftragte Mitte des Jahrhunderts Tyrannion, die Manuskripte zu sichten und durch weiteres Material zu ergänzen.

Auch wenn mit der Bibliothek des Aristoteles seine Manuskripte jahrhundertelang verschollen waren, ist es unbestritten, dass seine Lehre im Hellenismus mindestens teilweise bekannt war, vor allem durch die exoterischen Schriften und indirekt wohl auch durch Theophrasts Wirken. Daneben müssen einige Pragmatien bekannt gewesen sein, von denen es möglicherweise Abschriften in der Bibliothek des Peripatos gab.

Auf der Grundlage der Arbeit Tyrannions besorgte dessen Schüler Andronikos von Rhodos in der zweiten Hälfte des ersten Jahrhunderts v. Chr. die erste Ausgabe der aristotelischen Pragmatien, die wohl nur zum Teil auf den Manuskripten des Aristoteles beruhte. Die Schriften dieser Edition bilden das Corpus Aristotelicum. Vermutlich gehen einige Zusammenstellungen von zuvor ungeordneten Büchern sowie einige Titel auf diese Ausgabe zurück. Möglicherweise hat Andronikos auch darüber hinaus Eingriffe in den Text – wie etwa Querverweise – vorgenommen. Im Fall der zahlreichen Dubletten hat er möglicherweise verschiedene Texte zum selben Thema hintereinander angeordnet. Die heutige Anordnung der Schriften entspricht weitgehend dieser Ausgabe. Die zu seiner Zeit noch vorliegenden exoterischen Schriften berücksichtigte Andronikos nicht. Sie gingen in der Folgezeit verloren.

Heutige Ausgaben beruhen auf Abschriften, die auf die Andronikos-Ausgabe zurückgehen. Mit über 1000 Handschriften ist Aristoteles unter den nichtchristlichen griechischsprachigen Autoren derjenige mit der weitesten Verbreitung. Die ältesten Handschriften stammen aus dem 9. Jahrhundert. Das Corpus Aristotelicum ist wegen seines Umfangs nie vollständig in einem einzigen Kodex enthalten. Nach der Erfindung des Buchdrucks erschien 1495–1498 die erste Druckausgabe von Aldus Manutius. Die von Immanuel Bekker 1831 besorgte Gesamtausgabe der Berliner Akademie ist die Grundlage der modernen Aristotelesforschung. Sie beruht auf Kollationen der besten damals zugänglichen Handschriften. Nach ihrer Seiten-, Spalten- und Zeilenzählung (Bekker-Zählung) wird Aristoteles heute noch überall zitiert. Für einige wenige Werke bietet sie noch immer den maßgeblichen Text; die meisten liegen jedoch heute in neuen Einzelausgaben vor.

Aristoteles’ Werk deckt weite Teile des zu seiner Zeit vorhandenen Wissens ab. Er teilt es in drei Bereiche:
Das theoretische Wissen wird um seiner selbst willen gesucht. Praktisches und poietisches Wissen hat einen weiteren Zweck, die (gute) Handlung oder ein (schönes oder nützliches) Werk. Nach der Art der Gegenstände untergliedert er das theoretische Wissen weiter: (i) Die Erste Philosophie („Metaphysik“) behandelt (mit der Substanztheorie, der Prinzipientheorie und der Theologie) Selbstständiges und Unveränderliches, (ii) die Naturwissenschaft Selbstständiges und Veränderliches und (iii) die Mathematik behandelt Unselbständiges und Unveränderliches (Met. VI 1).

Eine Sonderstellung scheinen die in dieser Einteilung nicht vorkommenden Schriften zu haben, die erst nach dem Tod des Aristoteles im sogenannten "Organon" zusammengestellt worden sind.

Die wichtigsten Schriften lassen sich grob folgendermaßen gliedern:

Mit dieser Einteilung der Wissenschaften geht für Aristoteles die Einsicht einher, dass jede Wissenschaft aufgrund ihrer eigentümlichen Objekte auch eigene Prinzipien besitzt. So kann es in der praktischen Wissenschaft – dem Bereich der Handlungen – nicht dieselbe Genauigkeit geben wie im Bereich der theoretischen Wissenschaften. Es ist zwar eine Wissenschaft der Ethik möglich, aber ihre Sätze gelten nur in der Regel. Auch kann diese Wissenschaft nicht für alle möglichen Situationen die richtige Handlungsweise vorgeben. Vielmehr vermag die Ethik nur ein nicht-exaktes Wissen im Grundriss zu liefern, das zudem allein noch nicht zu einer erfolgreichen Lebensführung befähigt, sondern hierfür an Erfahrungen und bestehende Haltungen anschließen muss (EN I 1 1094b12–23).

Aristoteles war davon überzeugt, dass die „Menschen für das Wahre von Natur aus hinlänglich begabt sind“ (Rhet. I 1, 1355a15–17). Daher geht er typischerweise zunächst (allgemein oder bei Vorgängern) anerkannte Meinungen "(endoxa)" durch und diskutiert deren wichtigsten Probleme "(aporiai)," um einen möglichen wahren Kern dieser Meinungen zu analysieren (EN VII 2). Auffällig ist seine Vorliebe, in einer Allaussage zu Beginn einer Schrift die Grundlage für die Argumentation zu legen und den spezifischen Gegenstand abzustecken.

Der Themenbereich Sprache, Logik und Wissen ist vor allem in den Schriften behandelt, die traditionell unter dem Titel "Organon" (griech. Werkzeug, Methode) zusammengestellt sind. Diese Zusammenstellung und ihr Titel stammen nicht von Aristoteles, und die Reihenfolge ist nicht chronologisch. Die Schrift "Rhetorik" gehört dem "Organon" nicht an, steht ihm aber inhaltlich wegen ihrer Art der Behandlung des Gegenstands sehr nahe. Eine Berechtigung für die Zusammenstellung besteht in dem gemeinsamen methodologisch-propädeutischen Charakter.

In folgendem Abschnitt – der als der einflussreichste Text in der Geschichte der Semantik gilt – unterscheidet Aristoteles vier Elemente, die in zwei verschiedenen Beziehungen zueinander stehen, einer Abbildungsbeziehung und einer Symbolbeziehung:

Gesprochene und geschriebene Worte sind demnach bei den Menschen verschieden; geschriebene Worte symbolisieren gesprochene Worte. Seelische Widerfahrnisse und die Dinge sind bei allen Menschen gleich; seelische Widerfahrnisse bilden die Dinge ab. Demnach ist die Beziehung von Rede und Schrift zu den Dingen durch Übereinkunft festgelegt, die Beziehung der mentalen Eindrücke zu den Dingen hingegen naturgegeben.

Wahrheit und Falschheit kommt erst der Verbindung und Trennung von "mehreren" Vorstellungen zu. Auch die einzelnen Wörter stellen noch keine Verbindung her und können daher je allein nicht wahr oder falsch sein. Wahr oder falsch kann somit erst der ganze Aussage"satz" "(logos apophantikos)" sein.

Einige sprachlich-logische Feststellungen sind für Aristoteles’ Philosophie fundamental und spielen auch außerhalb der (im weiteren Sinne) logischen Schriften eine bedeutende Rolle. Hierbei geht es insbesondere um das Verhältnis von Prädikaten und (wesentlichen) Eigenschaften.

Unter einer Definition versteht Aristoteles primär keine Nominaldefinition (die er auch kennt; siehe An. Post. II, 8–10), sondern eine Realdefinition. Eine Nominaldefinition gibt nur Meinungen an, welche sich mit einem Namen verbinden. Was diesen Meinungen in der Welt zugrunde liegt, gibt die Realdefinition an: eine Definition von X gibt notwendige Eigenschaften von X an und was es heißt, ein X zu sein: das Wesen. Möglicher Gegenstand einer Definition ist damit (nur) das, was ein (universales) Wesen aufweist, insbesondere Arten wie "Mensch." Eine Art wird definiert durch die Angabe einer (logischen) Gattung und der artbildenden Differenz. So lässt sich "Mensch" definieren als "vernunftbegabtes" (Differenz) "Lebewesen" (Gattung). Individuen lassen sich mithin nicht durch Definition erfassen, sondern nur ihrer jeweiligen Art zuweisen.

Aristoteles lehrt, dass es zehn nicht aufeinander zurückführbare Aussageweisen gibt, die auf die Fragen "Was ist X?," "Wie beschaffen ist X?," "Wo ist X?" etc. antworten (→ die vollständige Liste). Die Kategorien haben sowohl eine sprachlich-logische als auch eine ontologische Funktion, denn von einem zugrunde liegenden Subjekt "(hypokeimenon)" (z. B. Sokrates) werden einerseits Prädikate ausgesagt, und ihm kommen andererseits Eigenschaften zu (z. B.: weiß, Mensch). Entsprechend stellen die Kategorien die allgemeinsten Klassen sowohl von Prädikaten als auch des Seienden dar. Dabei hebt Aristoteles die Kategorie der Substanz, die notwendig zukommende, wesentliche Prädikate enthält, von den anderen ab, die akzidentelle Prädikate enthalten.

Wenn man von Sokrates "Mensch" prädiziert (aussagt), so handelt es sich um eine wesentliche Aussage, die vom Subjekt (Sokrates) angibt, "was" er ist, also die Substanz benennt. Dies unterscheidet sich offensichtlich von einer Aussage wie "Sokrates ist auf dem Marktplatz," mit der man etwas Akzidentelles angibt, nämlich "wo" Sokrates ist (also den Ort benennt).

Aristoteles unterscheidet zwei Typen von Argumenten oder Erkenntnismitteln: Deduktion "(syllogismos)" und Induktion "(epagôgê)." Die Übereinstimmung mit den modernen Begriffen Deduktion und Induktion ist dabei weitgehend, aber nicht vollständig. Deduktionen und Induktionen spielen in den verschiedenen Bereichen der aristotelischen Argumentationstheorie und Logik zentrale Rollen. Beide stammen ursprünglich aus der Dialektik.

Nach Aristoteles besteht eine Deduktion aus Prämissen (Annahmen) und einer von diesen verschiedenen Konklusion. Die Konklusion folgt mit Notwendigkeit aus den Prämissen. Sie kann nicht falsch sein, wenn die Prämissen wahr sind.

Die Definition der Deduktion "(syllogismos)" ist also weiter als die der (unten behandelten) – traditionell Syllogismus genannten – Deduktion, die aus zwei Prämissen und drei Termen besteht. Aristoteles unterscheidet dialektische, eristische, rhetorische und demonstrative Deduktionen. Diese Formen unterscheiden sich vor allem nach der Art ihrer Prämissen.

Der Deduktion stellt Aristoteles explizit die Induktion gegenüber; deren Bestimmung und Funktion ist allerdings nicht so klar wie die der Deduktion. Er nennt sie

Aristoteles ist klar, dass ein derartiges Übergehen von singulären zu allgemeinen Sätzen ohne weitere Bedingungen nicht logisch gültig ist (An. Post. II 5, 91b34 f.). Entsprechende Bedingungen werden beispielsweise in dem ursprünglichen, argumentationslogischen Kontext der Dialektik erfüllt, da der Kontrahent einen durch Induktion eingeführten Allgemeinsatz akzeptieren muss, wenn er kein Gegenbeispiel nennen kann.

Vor allem aber hat die Induktion die Funktion, in anderen, nicht folgernden Kontexten durch das Anführen von Einzelfällen das Allgemeine "deutlich" zu machen – sei es als didaktisches, sei es als heuristisches Verfahren. Eine derartige Induktion stellt plausible Gründe dafür bereit, einen allgemeinen Satz für wahr zu halten. Aristoteles rechtfertigt aber nirgends ohne weitere Bedingungen induktiv die Wahrheit eines solchen Satzes.

Die in der "Topik" behandelte Dialektik ist eine Form der Argumentation, die (ihrer genuinen Grundform nach) in einer dialogischen Disputation stattfindet. Sie geht vermutlich auf Praktiken in Platons Akademie zurück. Die Zielsetzung der Dialektik lautet:

Die Dialektik hat demnach keinen bestimmten Gegenstandsbereich, sondern kann universal angewendet werden. Aristoteles bestimmt die Dialektik durch die Art der Prämissen dieser Deduktion. Ihre Prämissen sind anerkannte Meinungen "(endoxa)," das heißt
Für dialektische Prämissen ist es unerheblich, ob sie wahr sind oder nicht. Weshalb aber "anerkannte" Meinungen? In ihrer Grundform findet Dialektik in einem argumentativen Wettstreit zwischen zwei Gegnern statt mit genau zugewiesenen Rollen. Auf ein vorgelegtes Problem der Form ‚Ist S P oder nicht?‘ muss der Antwortende sich auf eine der beiden Möglichkeiten als These festlegen. Das dialektische Gespräch besteht nun darin, dass ein Fragender dem Antwortenden Aussagen vorlegt, die dieser entweder bejahen oder verneinen muss. Die beantworteten Fragen gelten als Prämissen. Das Ziel des Fragenden besteht nun darin, mithilfe der bejahten oder verneinten Aussagen eine Deduktion zu bilden, so dass die Konklusion die Ausgangsthese widerlegt oder aus den Prämissen etwas Absurdes oder ein Widerspruch folgt.
Die Methode der Dialektik weist zwei Bestandteile auf:
Für 2. bieten die verschiedenen Typen (a)–(ciii) "anerkannter" Meinungen dem Fragenden Anhaltspunkte dafür, welche Fragen der jeweilige Antwortende bejahen wird, das heißt, welche Prämissen er verwenden kann. Aristoteles fordert dazu auf, Listen solcher anerkannter Meinungen anzulegen (Top. I 14). Vermutlich meint er nach den Gruppen (a)–(ciii) getrennte Listen; diese werden wiederum nach Gesichtspunkten geordnet.

Für 1. hilft dem Dialektiker in seinem Argumentationsaufbau das Instrument der Topen. Ein Topos ist eine Konstruktionsanleitung für dialektische Argumente, das heißt zur Auffindung geeigneter Prämissen für eine gegebene Konklusion. Aristoteles listet in der "Topik" etwa 300 dieser Topen auf. Der Dialektiker kennt diese Topen auswendig, die sich aufgrund ihrer Eigenschaften ordnen lassen. Die Basis dieser Ordnung stellt das System der Prädikabilien dar.

Nach Aristoteles ist die Dialektik für dreierlei nützlich: (1) als Übung, (2) für die Begegnung mit der Menge und (3) für die Philosophie. Neben (1) der Grundform des argumentativen Wettstreits (bei der es eine Jury und Regeln gibt und die wahrscheinlich auf Praktiken in der Akademie zurückgeht) gibt es mit (2) auch Anwendungsweisen, die zwar dialogisch, aber nicht als regelbasierter Wettstreit angelegt sind, sowie mit (3) solche, die nicht dialogisch sind, sondern in denen der Dialektiker im Gedankenexperiment (a) Schwierigkeiten nach beiden Seiten hin durchgeht "(diaporêsai)" oder auch (b) Prinzipien untersucht (Top. I 4). Für ihn ist die Dialektik aber nicht wie bei Platon "die" Methode der Philosophie oder eine Fundamentalwissenschaft.

Aristoteles definiert Rhetorik als „Fähigkeit, bei jeder Sache das möglicherweise Überzeugende "(pithanon)" zu betrachten“ (Rhetorik I 2, 1355b26 f.). Er nennt sie ein Gegenstück "(antistrophos)" zur Dialektik. Denn ebenso wie die Dialektik ist die Rhetorik ohne abgegrenzten Gegenstandsbereich, und sie verwendet dieselben Elemente (wie Topen, anerkannte Meinungen und insbesondere Deduktionen), und dem dialektischen Schließen entspricht das auf rhetorischen Deduktionen basierende Überzeugen.

Der Rhetorik kam im demokratischen Athen des vierten Jahrhunderts eine herausragende Bedeutung zu, insbesondere in der Volksversammlung und den Gerichten, die mit durch Los bestimmten Laienrichtern besetzt waren. Es gab zahlreiche Rhetoriklehrer, und Rhetorikhandbücher kamen auf.

Aristoteles’ dialektische Rhetorik ist eine Reaktion auf die Rhetoriktheorie seiner Zeit, die – wie er kritisiert – bloße Versatzstücke für Redesituationen bereitstellt und Anweisungen, wie man durch Verleumdung und die Erregung von Emotionen das Urteil der Richter trüben kann. Im Gegensatz dazu beruht seine dialektische Rhetorik auf der Auffassung, dass wir dann am meisten überzeugt sind, wenn wir meinen, dass etwas bewiesen worden ist (Rhet. I 1, 1355a5 f.). Dass die Rhetorik sachorientiert sei und das jeweils in der Sache liegende Überzeugungspotential entdecken und ausschöpfen müsse, drückt er ebenfalls in der Gewichtung der drei Überzeugungsmittel aus. Diese sind:
Das Argument hält er für das wichtigste Mittel.

Unter den Argumenten unterscheidet Aristoteles das Beispiel – eine Form der Induktion – und das Enthymem – eine rhetorische Deduktion (wobei wiederum das Enthymem wichtiger als das Beispiel ist). Das Entyhmem ist eine Art der dialektischen Deduktion. Sein besonderes Merkmal aufgrund der rhetorischen Situation ist, dass seine Prämissen nur "die" anerkannten Meinungen sind, die von "allen" oder den "meisten" für wahr gehalten werden. (Die verbreitete, kuriose Ansicht, das Enthymem sei ein Syllogismus, in dem eine der zwei Prämissen fehle, vertritt Aristoteles "nicht;" sie basiert auf einem schon in der antiken Kommentierung belegten Missverständnis von 1357a7 ff.) Der Redner überzeugt demnach die Zuhörer, indem er eine Behauptung (als Konklusion) aus den Überzeugungen (als Prämissen) der Zuhörer herleitet. Die Konstruktionsanleitungen dieser Enthymeme liefern rhetorische Topen, z. B.:
An den zeitgenössischen Rhetoriklehrern kritisiert Aristoteles, dass sie die Argumentation vernachlässigten und ausschließlich auf Emotionserregung abzielten, etwa durch Verhaltensweisen wie Jammern oder Mitbringen der Familie zur Gerichtsverhandlung, wodurch ein sachbezogenes Urteil der Richter verhindert werde. Aristoteles’ Theorie zufolge können alle Emotionen definiert werden, indem drei Faktoren berücksichtigt werden. Man fragt: (1) Worüber, (2) wem gegenüber und (3) in welchem Zustand empfindet jemand die jeweilige Emotion? So lautet die Definition von Zorn:

Wenn der Redner mit diesem Definitionswissen den Zuhörern deutlich machen kann, dass der entsprechende Sachverhalt vorliegt und sie sich im entsprechenden Zustand befinden, empfinden sie die entsprechende Emotion. Sofern der Redner mit dieser Methode bestehende Sachverhalte eines Falles hervorhebt, lenkt er damit nicht – wie bei den kritisierten Vorgängern – von der Sache ab, sondern fördert nur dem Fall angemessene Emotionen und verhindert somit unangemessene.
Schließlich soll der Charakter des Redners "aufgrund seiner Rede" für die Zuhörer glaubwürdig, das heißt tugendhaft, klug und wohlwollend erscheinen (Rhet. I 2, 1356a5–11; II 1, 1378a6–16)

Die sprachliche Form dient ebenfalls einer argumentativ-sachorientierten Rhetorik. Aristoteles definiert nämlich die optimale Form "(aretê)" dadurch, dass sie primär klar, dabei aber weder banal noch zu erhaben ist (Rhet. III 2, 1404b1–4). Durch solche Ausgewogenheit fördert sie das Interesse, die Aufmerksamkeit und das Verständnis und wirkt angenehm. Unter den Stilmitteln erfüllt insbesondere die Metapher diese Bedingungen.

Besteht in einer Methode des konsistenten Argumentierens, so besteht seine syllogistische in einer Theorie des Beweisens selbst. In der von ihm begründeten Syllogistik zeigt Aristoteles, welche Schlüsse gültig sind. Hierfür verwendet er eine Form, die in der Tradition wegen der Bedeutung dieser Logik schlicht "Syllogismus" (die lateinische Übersetzung von "syllogismos") genannt wird. Jeder Syllogismus ist eine (besondere Form der) Deduktion "(syllogismos)", aber nicht jede Deduktion ist ein Syllogismus (und zwar weil Aristoteles’ sehr allgemeine Definition der Deduktion viele mögliche Argumenttypen beschreibt). Aristoteles verwendet selbst auch keinen eigenen Begriff, um den Syllogismus von anderen Deduktionen abzugrenzen.

Ein Syllogismus ist eine spezielle Deduktion, die aus genau zwei Prämissen und einer Konklusion besteht. Prämissen und Konklusion weisen zusammen genau drei verschiedene Begriffe, Terme (in der Tabelle dargestellt durch A, B, C) auf. Die Prämissen haben genau einen Term gemeinsam (in der Tabelle B), der in der Konklusion nicht vorkommt. Durch die Stellung des gemeinsamen Terms, des Mittelterms (hier immer B) unterscheidet Aristoteles folgende syllogistische Figuren:

Ein Prädikat (P) (z. B. 'sterblich') kann einem Subjekt (S) (z. B. 'Grieche') entweder zu- oder abgesprochen werden. Dies kann in partikulärer oder in allgemeiner Form geschehen. Somit gibt es vier Formen, in denen S und P miteinander verbunden werden können, wie die folgende Tabelle zeigt (nach "De interpretatione" 7; die Vokale werden seit dem Mittelalter für den jeweiligen Aussagetypus und auch in der Syllogistik verwendet).

Der Syllogismus verwendet genau diese vier Aussagetypen in folgender Form:

Aristoteles untersucht folgende Frage: Welche der 192 möglichen Kombinationen sind logisch gültige Deduktionen? Bei welchen Syllogismen ist es nicht möglich, dass, wenn die Prämissen wahr sind, die Konklusion falsch ist? Er unterscheidet vollkommene Syllogismen, die unmittelbar einsichtig sind, von unvollkommenen. Die unvollkommenen Syllogismen führt er mittels Konversionsregeln auf die vollkommenen zurück (dieses Verfahren nennt er "analysis") oder beweist sie indirekt.
Ein vollkommener Syllogismus ist der – seit dem Mittelalter so genannte – "Barbara:"
"Weitere gültige Syllogismen und deren Beweise finden sich im Artikel Syllogismus."

Die in den "Analytica Priora" ausgearbeitete Syllogistik wendet Aristoteles in seiner Wissenschaftstheorie, den "Analytica Posteriora" an.

Aristoteles entwickelt zudem eine modale Syllogistik, die die Begriffe "möglich" und "notwendig" einschließt. Diese Modalsyllogistik ist sehr viel schwieriger zu interpretieren als die einfache Syllogistik. Ob eine konsistente Interpretation dieser modalen Syllogistik überhaupt möglich ist, ist noch heute umstritten. Interpretatorisch problematisch, aber auch bedeutend ist Aristoteles’ Definition von "möglich." Er unterscheidet hierbei die sogenannte einseitige und die zweiseitige Möglichkeit:
Damit lässt sich der Indeterminismus, den Aristoteles vertritt, als der Zustand charakterisieren, der kontingent ist.

In der aristotelischen Logik wird zwischen folgenden konträren und kontradiktorischen Satzarten unterschieden:

Diese „kanonischen Sätze“ gehören zum Fundament der traditionellen Logik und werden unter anderem bei einfacher bzw. eingeschränkter Konversion angewandt.

Aristoteles unterscheidet verschiedene Stufen des Wissens, die sich folgendermaßen darstellen lassen (Met. I 1; An. post. II 19):

Mit dieser Stufung beschreibt Aristoteles auch, wie Wissen entsteht: Aus Wahrnehmung entsteht Erinnerung und aus Erinnerung durch Bündelung von Erinnerungsinhalten Erfahrung. Erfahrung besteht in einer Kenntnis einer Mehrzahl konkreter Einzelfälle und gibt nur das "Dass" an, ist bloße Faktenkenntnis. Wissen hingegen (oder Wissenschaft; "epistêmê" umfasst beides) unterscheidet sich von Erfahrung dadurch, dass es
In diesem Erkenntnisprozess schreiten wir nach Aristoteles von dem, was "für uns" bekannter und näher an der sinnlichen Wahrnehmung ist, zu dem vor, was "an sich" oder "von Natur aus" bekannter ist, zu den Prinzipien und Ursachen der Dinge. Dass Wissen an oberster Stelle steht und überlegen ist, bedeutet aber nicht, dass es im konkreten Fall die anderen Stufen in dem Sinne enthält, dass es sie ersetzte. Im Handeln ist zudem die Erfahrung als Wissen vom Einzelnen den Wissensformen, die aufs Allgemeine gehen, mitunter überlegen (Met. 981a12–25).

Unter einer Ursache "(aitia)" versteht Aristoteles in der Regel nicht ein von einem verursachten Ereignis B verschiedenes Ereignis A. Die Untersuchung von Ursachen dient nicht dazu, Wirkungen vorherzusagen, sondern Sachverhalte zu erklären. Eine aristotelische Ursache gibt einen Grund als Antwort auf bestimmte Warum-Fragen an. (Aristoteles unterscheidet vier Ursachentypen, die genauer hier im Abschnitt Naturphilosophie behandelt werden.)

Nach Aristoteles hat Ursachenwissen die Form einer bestimmten Deduktion: der Demonstration "(apodeixis)" eines Syllogismus mit wahren Prämissen, die Ursachen für den in der Konklusion ausgedrückten Sachverhalt angeben. Ein Beispiel:

Aristoteles spricht davon, dass die Prämissen einiger Demonstrationen Prinzipien ("archē;" wörtl. Anfang, Ursprung) sind, erste wahre Sätze, die selbst nicht demonstrativ bewiesen werden können.

Neben den Prinzipien können auch die Existenz und die Eigenschaften der behandelten Gegenstände einer Wissenschaft sowie bestimmte, allen Wissenschaften gemeinsame Axiome nach Aristoteles nicht durch Demonstrationen bewiesen werden, wie beispielsweise der Satz vom Widerspruch. Vom Satz des Widerspruchs zeigt Aristoteles, dass er nicht geleugnet werden kann. Er lautet: X kann Y nicht zugleich in derselben Hinsicht zukommen und nicht zukommen (Met. IV 3, 1005b19 f.). Aristoteles argumentiert, dass, wer dies leugnet, etwas und somit etwas Bestimmtes sagen muss. Wenn er z. B. ‚Mensch‘ sagt, bezeichnet er damit Menschen und nicht Nicht-Menschen. Mit dieser Festlegung auf etwas Bestimmtes setze er aber den Satz vom Widerspruch voraus. Dies gelte sogar für Handlungen, insofern eine Person etwa um einen Brunnen herumgeht und nicht in ihn hinein fällt.

Dass diese Sätze und auch Prinzipien nicht demonstriert werden können, liegt an Aristoteles’ Lösung eines Begründungsproblems: Wenn Wissen Rechtfertigung enthält, dann führt dies in einem konkreten Fall von Wissen entweder (a) zu einem Regress, (b) einem Zirkel oder (c) zu fundamentalen Sätzen, die nicht begründet werden können. Prinzipien in einer aristotelischen demonstrativen Wissenschaft sind solche Sätze, die nicht demonstriert, sondern auf andere Weise gewusst werden (An. Post. I 3).

Aristoteles spricht zudem davon, dass, sofern die Prämissen Prinzipien sind, sie auch Definitionen darstellen können. Wie sich Demonstration, Ursache und Definition zueinander verhalten, illustriert folgendes Beispiel:
Der Mond weist zum Zeitpunkt t eine Finsternis auf, weil (i) immer, wenn etwas im Sonnenschatten der Erde ist, es eine Finsternis aufweist und (ii) der Mond zum Zeitpunkt t im Sonnenschatten der Erde liegt.
"Demonstration:"

"Mittelterm:" Verdecken der Sonne durch die Erde.
"Ursache:" Verdecken der Sonne durch die Erde kommt dem Mond zum Zeitpunkt t zu.

Die Definition wäre hier etwa: "Mondfinsternis ist der Fall, in dem die Erde die Sonne verdeckt." Sie erklärt nicht das Wort ‚Mondfinsternis‘. Vielmehr gibt sie an, "was" eine Mondfinsternis ist. Indem man die Ursache angibt, schreitet man von einem Faktum zu seinem Grund fort. Das Verfahren der Analyse besteht darin, bottom-up zu einem bekannten Sachverhalt die nächste Ursache zu suchen, bis eine letzte Ursache erreicht ist.

Das aristotelische Wissenschaftsmodell wurde in der Neuzeit und bis ins 20. Jahrhundert als ein Top-down-Beweisverfahren verstanden. Die unbeweisbaren Prinzipien seien notwendig wahr und würden durch Induktion und Intuition "(nous)" erlangt. Alle Sätze einer Wissenschaft würden – in einer axiomatischen Struktur – aus ihren Prinzipien folgen. Wissenschaft beruht demnach auf zwei Schritten: Zunächst würden die Prinzipien intuitiv erfasst, dann würde top-down aus ihnen Wissen demonstriert.

Gegner dieser Top-down-Interpretation stellen vor allem infrage, dass für Aristoteles

Eine Interpretationsrichtung behauptet, die Demonstration habe didaktische Funktion. Da Aristoteles in den naturwissenschaftlichen Schriften seine Wissenschaftstheorie nicht befolge, lege diese nicht dar, wie Forschung "durchgeführt," sondern wie sie didaktisch "präsentiert" werden soll.

Eine andere Auslegung weist auch die didaktische Interpretation zurück, da sich sehr wohl Anwendungen des wissenschaftstheoretischen Modells in den naturwissenschaftlichen Schriften finden ließen. Vor allem aber kritisiert sie die erste Lesart dahingehend, dass sie nicht zwischen Wissens"ideal" und Wissens"kultur" unterscheide; denn Aristoteles halte Prinzipien für fallibel und die Funktion der Demonstration für heuristisch. Sie liest die Demonstration bottom-up: Zu bekannten Sachverhalten würden mithilfe der Demonstration deren Ursachen gesucht. Die wissenschaftliche Forschung gehe von den für uns bekannteren empirischen (meist universalen) Sätzen aus. Zu einer solchen Konklusion werden Prämissen gesucht, die für den entsprechenden Sachverhalt Ursachen angeben.

Der wissenschaftliche Forschungsprozess besteht nun darin, beispielsweise die Verknüpfung von Schwere und Statue oder Mond und Finsternis in der Weise genauer zu analysieren, dass man Mittelterme sucht, die sie als Ursachen miteinander verknüpfen. Im einfachsten Fall gibt es dabei nur einen Mittelterm, in anderen mehrere. Top-down wird dann das Wissen von den erklärenden Prämissen zu den erklärten universalen empirischen Sätzen präsentiert. Dabei geben die Prämissen den Grund für den in der Konklusion beschriebenen Sachverhalt an. Das Ziel jeder Disziplin besteht in einer derartigen demonstrativen Darstellung des Wissens, in der die nicht demonstrierbaren Prinzipien dieser Wissenschaft Prämissen sind.

Wie die Prinzipien nach Aristoteles erfasst werden, bleibt undeutlich und ist umstritten. Vermutlich werden sie durch Allgemeinbegriffe gebildet, die durch einen induktiven Vorgang entstehen, einen Aufstieg innerhalb der oben beschriebenen Wissensstufen: Wahrnehmung wird Erinnerung, wiederholte Wahrnehmung verdichtet sich zu Erfahrung, und aus Erfahrung bilden wir Allgemeinbegriffe. Mit dieser auf der Wahrnehmung basierenden Konzeption der Bildung von Allgemeinbegriffen weist Aristoteles sowohl Konzeptionen zurück, die die Allgemeinbegriffe aus einem höheren Wissen ableiten, als auch diejenigen, die behaupten, Allgemeinbegriffe seien angeboren. Vermutlich auf Grundlage dieser Allgemeinbegriffe werden die Prinzipien, Definitionen gebildet. Die Dialektik, die Fragen in der Form ‚Trifft P auf S zu oder nicht?‘ behandelt, ist vermutlich ein Mittel, Prinzipien zu prüfen. Das Vermögen, das diese grundlegenden Allgemeinbegriffe und Definitionen erfasst, ist der Geist, die Einsicht "(nous)."

In Aristoteles’ Naturphilosophie bedeutet Natur "(physis)" zweierlei: Zum einen besteht der primäre Gegenstandsbereich aus den von Natur aus bestehenden Dingen (Menschen, Tiere, Pflanzen, die Elemente), die sich von Artefakten unterscheiden. Zum anderen bilden die Bewegung "(kínēsis)" und Ruhe "(stasis)" den Ursprung, beziehungsweise das Grundprinzip "(archē)" aller Natur (Phys. II 1, 192b14). Bewegung bedeutet wiederum Veränderung "(metabolē)" (Phys. II 1,193a30). So ist beispielsweise die Ortsbewegung eine Form der Veränderung. Ebenso stellen die „Eigenbewegungen“ des Körpers, wenn dieser (zum Beispiel durch Nahrungsaufnahme) wächst oder abnimmt, eine Veränderung dar. Beide Begriffe, kínēsis und metabolē, sind für Aristoteles folglich nicht trennbar. Gemeinsam bilden sie das Grundprinzip und den Anfang aller Naturdinge. Bei Artefakten kommt das Prinzip jeder Veränderung von außen (Phys. II 1, 192b8–22). Die Wissenschaft der Natur hängt in der Folge von den Arten der Veränderung ab.

Ein Veränderungsprozess von X ist gegeben, wenn X, das (i) der Wirklichkeit nach die Eigenschaft F und (ii) der Möglichkeit nach G aufweist, die Eigenschaft G verwirklicht. Bei Bronze (X), die der Wirklichkeit nach ein Klumpen ist (F) und der Möglichkeit nach eine Statue (G), liegt Veränderung dann vor, wenn die Bronze "der Wirklichkeit" nach die Form einer Statue (G) "wird;" der Prozess ist abgeschlossen, wenn die Bronze diese Form "besitzt." Oder wenn der ungebildete Sokrates gebildet wird, so verwirklicht sich ein Zustand, welcher der Möglichkeit nach schon vorlag. Der Veränderungsprozess ist also durch seinen Übergangsstatus gekennzeichnet und setzt voraus, dass etwas, das der Möglichkeit nach vorliegt, verwirklicht werden kann (Phys. III 1, 201a10–201b5).

Für alle Veränderungsprozesse hält Aristoteles (in Übereinstimmung mit seinen naturphilosophischen Vorgängern) Gegensätze für grundlegend. Er vertritt darüber hinaus die These, dass in einem Veränderungsprozess diese Gegensätze (wie "gebildet-ungebildet") immer "an" einem Substrat oder Zugrundeliegenden "(hypokeimenon)" auftreten, so dass sein Modell folgende drei Prinzipien aufweist:
Wird der ungebildete Sokrates gebildet, so ist er dabei an jedem Punkt der Veränderung Sokrates. Entsprechend bleibt die Bronze Bronze. Das Substrat der Veränderung, an dem diese sich vollzieht, bleibt dabei mit sich selbst identisch. Den Ausgangszustand der Veränderung fasst Aristoteles dabei als einen Zustand, dem die entsprechende Eigenschaft des Zielzustands ermangelt (Privation) (Phys. I 7).

Aristoteles unterscheidet vier Arten der Veränderung:
Bei jeder Veränderung – so Aristoteles – gibt es ein zugrunde liegendes, numerisch identisches Substrat (Physik I 7, 191a13–15). Im Falle qualitativer, quantitativer und örtlicher Veränderung ist dies ein konkretes Einzelding, das seine Eigenschaften, seine Größe oder seine Position verändert. Wie verhält sich dies aber beim Entstehen/Vergehen konkreter Einzeldinge? Die Eleaten hatten die einflussreiche These vertreten, Entstehen sei nicht möglich, da sie es für widersprüchlich hielten, wenn Seiendes aus Nicht-Seiendem hervorginge (bei Entstehen aus Seiendem sahen sie ein ähnliches Problem). Die Lösung der Atomisten, Entstehen sei ein Prozess, in dem durch Mischung und Trennung unvergänglicher und unveränderlicher Atome aus alten neue Einzeldinge hervorgehen, führt nach Aristoteles’ Ansicht Entstehen illegitimerweise auf qualitative Veränderung zurück (Gen. Corr. 317a20 ff.).

Aristoteles’ Analyse von Entstehen/Vergehen basiert auf der innovativen Unterscheidung von Form und Materie (Hylemorphismus). Er akzeptiert, dass kein konkretes Einzelding aus Nichtseiendem entstehe, analysiert den Fall "Entstehen" jedoch folgendermaßen. Ein konkretes Einzelding des Typs F entsteht nicht aus einem nicht-seienden F, sondern aus einem zugrunde liegenden Substrat, das nicht die Form F aufweist: der Materie.

Ein Ding entsteht, indem Materie eine neu hinzukommende Form annimmt. So entsteht eine Bronzestatue, indem eine Bronzemasse eine entsprechende Form annimmt. Die fertige Statue besteht "aus" Bronze, die Bronze liegt der Statue als Materie zugrunde. Die Antwort auf die Eleaten lautet, dass einer nicht-seienden Statue die Bronze als Materie entspricht, die durch Hinzukommen einer Form zur Statue wird. Der Entstehungsprozess ist dabei von verschiedenen Seinsgraden gekennzeichnet. Die tatsächliche, aktuale, geformte Statue entsteht aus etwas, das potentiell eine Statue ist, nämlich Bronze als Materie (Phys. I 8, 191b10–34).

Materie und Form sind Aspekte eines konkreten Einzeldings und treten nicht selbständig auf. Materie ist immer Stoff eines bestimmten Dings, das schon eine Form aufweist. Sie ist ein relativer Abstraktionsbegriff zu Form. Indem eine derartige Materie in einer neuen Weise strukturiert wird, entsteht ein neues Einzelding. Ein Haus setzt sich aus Form (dem Bauplan) und Materie (Holz und Ziegel) zusammen. Die Ziegel als Materie des Hauses sind durch einen bestimmten Prozess auf eine bestimmte Weise geformter, konfigurierter Lehm. Unter Form versteht Aristoteles seltener die äußere Gestalt (dies nur bei Artefakten), in der Regel die innere Struktur oder Natur, dasjenige, was durch eine Definition erfasst wird. Die Form eines Gegenstandes eines bestimmten Typs beschreibt dabei Voraussetzungen, welche Materie für diesen geeignet ist und welche nicht.

Bewegungen erfolgen nach Aristoteles entweder naturgemäß oder naturwidrig (gewaltsam). Nur Lebewesen bewegen sich aus eigenem Antrieb, alles andere wird entweder von etwas bewegt oder es strebt möglichst geradlinig seinem natürlichen Ort entgegen und kommt dort zum Stillstand.

Der natürliche Ort eines Körpers hängt von der in ihm vorherrschenden Materieart ab. Wenn Wasser oder Erde vorherrscht, bewegt sich der Körper zum Mittelpunkt der Erde, dem Zentrum der Welt, wenn Feuer oder Luft dominiert, strebt er nach oben. Erde ist ausschließlich schwer, Feuer absolut leicht, Wasser relativ schwer, Luft relativ leicht. Der natürliche Ort des Feuers ist oberhalb der Luft und unterhalb der Mondsphäre. Leichtigkeit und Schwere sind Eigenschaften von Körpern, die mit deren Dichte nichts zu tun haben. Mit der Einführung der Vorstellung einer absoluten Schwere und absoluten Leichtigkeit (Schwerelosigkeit des Feuers) verwirft Aristoteles die Auffassung Platons und der Atomisten, die alle Objekte für schwer hielten und das Gewicht als relative Größe auffassten.

Das fünfte Element, der Äther des Himmels, ist masselos und bewegt sich ewig in gleichförmiger Kreisbewegung um das Zentrum der Welt. Der Äther füllt den Raum oberhalb der Mondsphäre; er ist keinerlei Veränderung außer der Ortsbewegung unterworfen. Die Annahme, auf der Erde und am Himmel gälten verschiedene Gesetze, ist für Aristoteles nötig, weil die Bewegung der Planeten und Fixsterne nicht zur Ruhe kommt.

Aristoteles nimmt an, dass für jede Ortsbewegung ein Medium, das entweder als bewegende Kraft wirkt oder der Bewegung Widerstand leistet, erforderlich ist; eine kontinuierliche Bewegung im Vakuum ist prinzipiell unmöglich. Aristoteles schließt sogar die Existenz eines Vakuums aus.

Die Bewegungslehre des Aristoteles war bis zur Entwicklung eines neuen Trägheitsbegriffs durch Galilei und Newton einflussreich.

Um Wissen von Veränderungsprozessen und somit von der Natur zu besitzen, muss man – so Aristoteles – die entsprechenden Ursachen "(aitiai)" kennen (Phys. I 1, 184a10–14). Aristoteles behauptet, es gebe genau vier Ursachentypen, die jeweils auf verschiedene Weise auf die Frage "Warum" antworten und die in der Regel bei einer vollständigen Erklärung alle angegeben werden müssen (Phys. II 3, 194b23–35):

Der aristotelische Ursachenbegriff unterscheidet sich weitgehend vom modernen. In der Regel treffen zur Erklärung desselben Sachverhaltes oder Gegenstandes verschiedene Ursachen zugleich zu. Die Formursache fällt oft mit der Bewegungsursache und der Finalursache zusammen. Die Ursache eines Hauses sind so Ziegel und Holz, der Bauplan, der Architekt und der Schutz vor Unwetter. Letztere drei fallen oft zusammen, insofern beispielsweise der Zweck "Schutz vor Unwetter" den Bauplan (im Geist) des Architekten bestimmt.

Die Finalursache ist vom Standpunkt der neuzeitlichen mechanistischen Physik aus kritisiert worden. Von einer insgesamt teleologisch ausgerichteten Natur wie bei Platon setzt sich Aristoteles jedoch weitgehend ab. Finale Ursachen treten für ihn in der Natur vor allem in der Biologie auf, und zwar beim funktionellen Aufbau von Lebewesen und der Artenreproduktion.

Metaphysik als Erste Philosophie

Aristoteles gebraucht den Ausdruck „Metaphysik“ nicht. Gleichwohl trägt eines seiner wichtigsten Werke traditionell diesen Titel. Die "Metaphysik" ist eine von einem späteren Herausgeber zusammengestellte Sammlung von Einzeluntersuchungen, die ein mehr oder weniger zusammenhängendes Themenspektrum abdecken, indem sie nach den Prinzipien und Ursachen des Seienden und nach der dafür zuständigen Wissenschaft fragen. Ob der Titel ("ta meta ta physika:" die <Schriften, Dinge> nach der Physik) einen bloß bibliografischen oder einen sachbezogenen Hintergrund hat, ist unklar.

Aristoteles spricht in der "Metaphysik" von einer allen anderen Wissenschaften vorgeordneten Wissenschaft, die er Erste Philosophie, Weisheit "(sophia)" oder auch Theologie nennt. Diese Erste Philosophie wird in dieser Sammlung aus Einzeluntersuchungen auf drei Weisen charakterisiert:

Ob oder inwieweit diese drei Projekte zusammenhängende Aspekte derselben Wissenschaft oder voneinander unabhängige Einzelprojekte sind, ist kontrovers. Aristoteles behandelt später metaphysisch genannte Themen auch in anderen Schriften.

Im Corpus Aristotelicum finden sich in zwei Werken, den frühen "Kategorien" und der späten "Metaphysik," unterschiedliche Theorien des Seienden.

Die "Kategorien," die die erste Schrift im "Organon" bilden, sind vermutlich das einflussreichste Werk des Aristoteles und der Philosophiegeschichte überhaupt.

Die frühe Ontologie der "Kategorien" befasst sich mit den Fragen ‚Was ist das eigentlich Seiende?‘ und ‚Wie ist das Seiende geordnet?‘ und ist als Kritik an der Position Platons zu verstehen. Der mutmaßliche Gedankengang lässt sich folgendermaßen skizzieren. Unterschieden werden Eigenschaften, die Einzeldingen zukommen (P kommt S zu). Dafür liegen zwei Deutungsmöglichkeiten nahe: Das eigentlich Seiende, die Substanz "(ousia)" sind

Aristoteles selbst berichtet (Met. I 6), Platon habe gelehrt, man müsse von den wahrnehmbaren Einzeldingen getrennte, nicht sinnlich wahrnehmbare, unveränderliche, ewige Urbilder unterscheiden. Platon nahm an, dass es Definitionen (und damit aus seiner Sicht auch Wissen) von den Einzeldingen, die sich beständig ändern, nicht geben kann. Gegenstand der Definition und des Wissens sind für ihn die Urbilder (Ideen) als das für die Ordnungsstruktur des Seienden Ursächliche. Verdeutlichen lässt sich dies an einer von allen Menschen getrennten, einzelnen und numerisch identischen Idee des Menschen, die für das jeweilige Menschsein ursächlich ist und die Erkenntnisgegenstand ist für die Frage ‚Was ist ein Mensch?‘.

Aristoteles’ Einteilung des Seienden in den "Kategorien" scheint sich von der skizzierten Position Platons abzugrenzen. Er orientiert sich dabei an der sprachlichen Struktur einfacher Sätze der Form ‚S ist P‘ und der sprachlichen Praxis, wobei er die sprachliche und die ontologische Ebene nicht explizit voneinander scheidet.

Einige Ausdrücke – wie ‚Sokrates‘ – können nur die Subjektposition S in dieser sprachlichen Struktur einnehmen, alles andere wird von ihnen prädiziert. Die Dinge, die in diese Kategorie der Substanz fallen und die er "Erste Substanz" nennt, sind ontologisch selbständig; sie bedürfen keines anderen Dinges, um zu existieren. Daher sind sie ontologisch primär, denn alles andere ist von ihnen abhängig und nichts würde ohne sie existieren.

Diese abhängigen Eigenschaften bedürfen eines Einzeldings, einer ersten Substanz als eines Trägers, "an" der sie vorkommen. Derartige Eigenschaften (z. B. weiß, sitzend) können einem Einzelding (etwa Sokrates) jeweils zukommen oder auch nicht zukommen und sind daher akzidentelle Eigenschaften. Dies betrifft alles außerhalb der Kategorie der Substanz.

Für einige Eigenschaften (z. B. ‚Mensch‘) gilt nun, dass sie in der Weise von einem Einzelding (z. B. Sokrates) ausgesagt werden können, dass ihre Definition (vernünftiges Lebewesen) auch von diesem Einzelding gilt. Sie kommen ihm daher "notwendig" zu. Dies sind die Art und die Gattung. Aufgrund dieses engen Bezugs, in dem die Art und die Gattung angeben, "was" eine erste Substanz jeweils ist (etwa in der Antwort auf die Frage ‚Was ist Sokrates?‘: ‚ein Mensch‘), nennt Aristoteles sie zweite Substanz. Dabei hängt auch eine zweite Substanz von einer ersten Substanz ontologisch ab.


Aristoteles vertritt also folgende Thesen:

Für Platon ergibt sich als Konsequenz aus seiner Auffassung von den Ideen die Annahme, dass im eigentlichen, unabhängigen Sinne allein die unveränderlichen Ideen existieren; die Einzeldinge existieren nur in Abhängigkeit von den Ideen. Diese ontologische Konsequenz kritisiert Aristoteles eingehend in der "Metaphysik." Er hält es für widersprüchlich, dass die Anhänger der Ideenlehre einerseits die Ideen dadurch von den Sinnesobjekten abgrenzen, dass sie ihnen das Merkmal der Allgemeinheit und damit Undifferenziertheit zuweisen, und andererseits zugleich für jede einzelne Idee eine separate Existenz annehmen; dadurch würden die Ideen selbst Einzeldinge, was mit ihrem Definitionsmerkmal Allgemeinheit unvereinbar sei (Met. XIII 9, 1086a32–34).

In der "Metaphysik" vertritt Aristoteles im Rahmen seines Vorhabens, das Seiende als Seiendes zu untersuchen, die Auffassung, dass alles Seiende entweder eine Substanz ist oder auf eine bezogen ist ("Metaphysik" IV 2). In den "Kategorien" hatte er ein Kriterium für Substanzen formuliert und Beispiele (Sokrates) für diese gegeben. In der "Metaphysik" thematisiert er nun abermals die Substanz, um nach den Prinzipien und Ursachen einer Substanz, eines konkreten Einzeldings zu suchen. Hier fragt er nun: Was macht etwa Sokrates zu einer Substanz? Substanz ist hier also ein zweistelliges Prädikat "(Substanz von X)," so dass man die Frage so formulieren kann: "Was ist die Substanz-X einer Substanz?" Dabei spielt die Form-Materie-Unterscheidung, die in den "Kategorien" nicht präsent ist, eine entscheidende Rolle.

Aristoteles scheint die Substanz-X vor allem mit Hilfe zweier Kriterien zu suchen, die in der Theorie der "Kategorien" auf die erste und die zweite Substanz verteilt sind:

Das Kriterium (ii) wird genauer erfüllt, indem Aristoteles das "Wesen" als Substanz-X bestimmt. Mit Wesen meint er dabei, was ontologisch einer Definition entspricht (Met. VII 4; 5, 1031a12; VIII 1, 1042a17). Das Wesen beschreibt die notwendigen Eigenschaften, ohne die ein Einzelding aufhören würde, ein und dieselbe Sache zu sein. Fragt man: "Was ist die Ursache dafür, dass diese Materieportion Sokrates ist?," so ist Aristoteles’ Antwort: "Das Wesen von Sokrates, welches weder ein weiterer Bestandteil "neben" den materiellen Bestandteilen" ist (dann bedürfte es eines weiteren Strukturprinzips, um zu erklären, wie es mit den materiellen Bestandteilen vereint ist) "noch etwas "aus" materiellen Bestandteilen" (dann müsste man erklären, wie das Wesen selbst zusammengesetzt ist).

Aristoteles ermittelt die Form "(eidos)" eines Einzeldings als sein Wesen und somit als Substanz-X. Mit Form meint er weniger die äußere Gestalt als vielmehr die Struktur: Die Form

Dass die Form als Substanz-X auch das genannte Kriterium (ii), selbständig zu sein, erfüllen muss, und dies teilweise als Kriterium für etwas Individuelles aufgefasst wird, ist einer von vielen Aspekten in folgender zentralen interpretatorischen Kontroverse: Fasst Aristoteles die Form (A) als etwas Allgemeines oder (B) als etwas (dem jeweiligen Einzelding) Individuelles auf? Als Problem formuliert: Wie kann die Form, das "eidos," zugleich Form eines Einzeldings und Gegenstand des Wissens sein?
Für (A) spricht insbesondere, dass Aristoteles an mehreren Stellen davon ausgeht, dass die Substanz-X und somit die Form definierbar ist (Met. VII 13) und dies für ihn (wie für Platon) nur auf Allgemeines zutrifft (VII 11, 1036a; VII 15, 1039b31–1040a2).
Für (B) hingegen spricht vor allem, dass Aristoteles kategorisch die unplatonische Position zu vertreten scheint: Kein Allgemeines kann Substanz-X sein (Met. VII 13). Nach (B) besitzen Sokrates und Kallias zwei auch qualitativ verschiedene Formen. Definierbar müssten dann zu separierende, überindividuelle Aspekte dieser beiden Formen sein. Die Interpretation (A) hingegen löst das Dilemma etwa, indem sie die Aussage "Kein Allgemeines ist Substanz-X" als "Nichts allgemein Prädizierbares ist Substanz-X" interpretiert und so entschärft. Die Form werde nicht auf herkömmliche Weise (wie die Art ‚Mensch‘ von ‚Sokrates‘ in den "Kategorien") prädiziert und sei daher nicht im problematischen Sinne allgemein. Vielmehr werde die Form von der unbestimmten Materie in einer Weise ‚prädiziert‘, die einen Einzelgegenstand erst konstituiere.

Die für die Ontologie wichtige Beziehung zwischen Form und Materie wird durch ein weiteres Begriffspaar genauer erläutert: Akt "(energeia, entelecheia)" und Potenz "(dynamis)."

Für die Form-Materie-Unterscheidung ist die später ontologisch genannte Bedeutung von Potenz oder Vermögen wichtig.
Potentialität ist hier ein Zustand, dem ein anderer Zustand – Aktualität – gegenübersteht, indem ein Gegenstand der Wirklichkeit nach F oder dem Vermögen, der Möglichkeit nach F ist. So ist ein Junge der Möglichkeit nach ein Mann, ein ungebildeter Mensch der Möglichkeit nach ein gebildeter (Met. IX 6).

Dieses (hier diachron beschriebene) Verhältnis von Aktualität und Potentialität bildet die Grundlage für das (auch synchron zu verstehende) Verhältnis von Form und Materie, denn Form und Materie sind Aspekte eines Einzeldings, nicht dessen Teile. Sie sind im Verhältnis von Aktualität und Potentialität miteinander verbunden und konstituieren so (erst) das Einzelding. Die Materie eines Einzeldings ist demnach genau das "potentiell," was die Form des Einzeldings und das Einzelding selbst "aktual" sind (Met. VIII 1, 1042a27 f.; VIII 6, 1045a23–33; b17–19). Zum einen ist zwar (diachron betrachtet) eine bestimmte Portion Bronze potentiell eine Kugel wie auch eine Statue. Zum anderen aber ist (synchron als konstituierender Aspekt) die Bronze an einer Statue potentiell genau das, was die Statue und deren Form aktual sind. Die Bronze der Statue ist ein Konstituens der Statue, ist aber nicht mit ihr identisch. Und so sind auch Fleisch und Knochen potentiell das, was Sokrates oder seine Form (die für einen Menschen typische Konfiguration und Fähigkeiten seiner materiellen Bestandteile,→ ) aktual sind.

So wie die Form gegenüber der Materie ist für Aristoteles auch die Aktualität gegenüber der Potentialität primär (Met. IX 8, 1049b4–5). Unter anderem ist sie der Erkenntnis nach primär. Man kann nur dann ein Vermögen angeben, wenn man Bezug auf die Wirklichkeit nimmt, zu der es ein Vermögen ist. Das Sehvermögen etwa lässt sich nur bestimmen, indem man auf die Tätigkeit ‚Sehen‘ Bezug nimmt (Met. IX 8, 1049b12–17). Des Weiteren ist die Aktualität im entscheidenden Sinne auch zeitlich früher als die Potentialität, denn ein Mensch entsteht durch einen Menschen, der aktual Mensch ist (Met. IX 8, 1049b17–27).

Aristoteles unterscheidet im Vorfeld seiner Theologie drei mögliche Substanzen: (i) sinnlich wahrnehmbare vergängliche, (ii) sinnlich wahrnehmbare ewige und (iii) nicht sinnlich wahrnehmbare ewige und unveränderliche (Met. XII 1, 1069a30-1069b2). (i) sind die konkreten Einzeldinge (der sublunaren Sphäre), (ii) die ewigen, bewegten Himmelskörper, (iii) erweist sich als der selbst unbewegte Ursprung aller Bewegung.

Aristoteles argumentiert für einen göttlichen Beweger, indem er feststellt, dass, wenn alle Substanzen vergänglich wären, alles vergänglich sein müsste, die Zeit und die Veränderung selbst jedoch notwendig unvergänglich sind (Phys. VIII 1, 251a8–252b6; Met. XII 6, 1071b6–10). Aristoteles zufolge ist die einzige Veränderung, die ewig existieren kann, die Kreisbewegung (Phys. VIII 8–10; Met. XII 6,1071b11). Die entsprechende beobachtbare kreisförmige Bewegung der Fixsterne muss daher als Ursache eine ewige und immaterielle Substanz haben (Met. XII 8, 1073b17–32). Enthielte das Wesen dieser Substanz Potentialität, könnte die Bewegung unterbrochen werden. Daher muss sie reine Aktualität, Tätigkeit sein (Met. XII, 1071b12–22). Als letztes Prinzip muss dieser Beweger selbst unbewegt sein.

Nach Aristoteles bewegt der unbewegte Beweger „wie ein Geliebtes“, nämlich als Ziel (Met. XII 7, 1072b3), denn das Begehrte, das Gedachte und insbesondere das Geliebte kann bewegen, ohne bewegt zu sein (Met. XII 7, 1072a26). Seine Tätigkeit ist die lustvollste und schönste. Da er immaterielle Vernunft "(nous)" ist und seine Tätigkeit im Denken des besten Gegenstandes besteht, denkt er sich selbst: das „Denken des Denkens“ "(noêsis noêseôs)" (Met. XII 9, 1074b34 f.). Da nur Lebendiges denken kann, muss er zudem lebendig sein. Den unbewegten Beweger identifiziert Aristoteles mit Gott (Met. XII 7, 1072b23 ff.).

Der unbewegte Beweger bewegt die gesamte Natur. Die Fixsternsphäre bewegt sich, da sie mit der Kreisbewegung die Vollkommenheit nachahmt. Die anderen Himmelskörper werden vermittelt über die Fixsternsphäre bewegt. Die Lebewesen haben Anteil an der Ewigkeit, indem sie mittels der Fortpflanzung ewig bestehen (GA II 1, 731b31–732a1).

Stellung der Biologie

Nicht nur in der Philosophiegeschichte, sondern auch in der Geschichte der Naturwissenschaften nimmt Aristoteles einen bedeutenden Platz ein. Ein großer Teil seiner überlieferten Schriften ist naturwissenschaftlich, von denen die bei weitem bedeutendsten und umfangreichsten die biologischen Schriften sind, die fast ein Drittel des überlieferten Gesamtwerks umfassen. Vermutlich in Arbeitsteilung wurde die Botanik von seinem engsten Mitarbeiter Theophrast, die Medizin von seinem Schüler Menon bearbeitet.

Aristoteles vergleicht das Studium unvergänglicher Substanzen (Gott und Himmelskörper) und vergänglicher Substanzen (der Lebewesen). Beide Forschungsgebiete haben ihren Reiz. Die unvergänglichen Substanzen, die höchsten Erkenntnisgegenstände zu untersuchen, bereiten zwar die größte Freude, aber das Wissen über Lebewesen ist leichter zu erlangen, da sie uns näher stehen. Er betont den Wert der Erforschung auch niederer Tiere und weist darauf hin, dass auch diese etwas Natürliches und Schönes zeigen, das sich nicht in ihren zerlegten Bestandteilen erschöpft, sondern erst durch die Tätigkeiten und das Zusammenwirken der Teile hervortritt (PA I 5, 645a21–645b1).

Aristoteles als empirischer Forscher

Aristoteles hat selbst empirische Forschung betrieben, jedoch vermutlich nicht Experimente im – erst in der neuzeitlichen Naturwissenschaft eingeführten – Sinne einer methodischen Versuchsanordnung angestellt.

Sicher ist, dass er selbst Sezierungen vornahm. Einem Experiment am nächsten kommt die in festgelegten zeitlichen Abständen wiederholte Untersuchung von befruchteten Hühnereiern, mit dem Ziel zu beobachten, in welcher Reihenfolge die Organe entstehen (GA VI 3, 561a6–562a20). Experimente sind jedoch in seiner eigentlichen Domäne – der deskriptiven Zoologie – auch nicht das wesentliche Instrument der Forschung. Neben eigenen Beobachtungen und einigen wenigen Textquellen stützte er sich hier auch auf Informationen von einschlägig Berufstätigen wie Fischern, Bienenzüchtern, Jägern und Hirten. Er ließ die Inhalte seiner Textquellen teilweise empirisch überprüfen, übernahm aber auch unkritisch fremde Irrtümer. Ein verlorenes Werk bestand vermutlich großenteils aus Zeichnungen und Diagrammen von Tieren.

Aufgrund des lange vorherrschenden Interpretationsmodells der Wissenschaftstheorie des Aristoteles und der Vernachlässigung der biologischen Schriften, ging man früher davon aus, dass er diese Theorie nicht auf die Biologie angewendet hat. Demgegenüber wird heute durchaus angenommen, dass seine Vorgehensweise in der Biologie von seiner Wissenschaftstheorie beeinflusst war, wenngleich Umfang und Grad umstritten sind.

Faktensammlungen

Von Aristoteles ist keine Beschreibung seines naturwissenschaftlichen Vorgehens überliefert. Erhalten sind neben der allgemeinen Wissenschaftstheorie nur Texte, die ein Endprodukt der wissenschaftlichen Forschung darstellen. Die biologischen Schriften sind in einer bestimmten Reihenfolge angeordnet, die der Vorgehensweise entspricht.

Die erste Schrift "(Historia animalium)" beschreibt die verschiedenen Tierarten und ihre spezifischen Differenzen. Sie bietet die Sammlung des Faktenmaterials wie z. B., dass alle Lebewesen mit Lungen Luftröhren aufweisen. Dabei wird nicht erörtert, ob etwas notwendig oder unmöglich so sei. In der Faktensammlung ordnet Aristoteles die Lebewesen nach verschiedenen Einteilungsmerkmalen wie blutführend, lebendgebärend usw. Nach Merkmalen geordnet stellt er allgemeine Relationen zwischen verschiedenen Aspekten der Beschaffenheit fest. So bemerkt er beispielsweise: Alle Vierfüßler, die lebendgebärend sind, weisen Lungen und Luftröhren auf (HA II 15, 505b32 f.). Erst die an dieses Werk anschließenden und darauf aufbauenden Schriften "De generatione animalium" (Über die Entstehung der Tiere) und "De partibus animalium" (Über die Teile der Tiere) befassen sich mit den Ursachen, welche die Fakten erklären.

Ursachenwissen

Die Faktensammlung ist die Voraussetzung dafür, Wissen auf der Grundlage von Ursachenkenntnis zu erreichen. Zentral für die Biologie sind dabei finale Ursachen, die den Zweck der Bestandteile des Körpers angeben. Die Ursache für die Existenz einer Luftröhre bei allen Lebewesen, die eine Lunge besitzen, besteht für Aristoteles in der Funktionsweise der Lunge. Die Lunge kann – anders als der Magen – nicht unmittelbar an den Mund anschließen, da sie eines zweigeteilten Kanals bedarf, so dass Einatmen und Ausatmen auf optimale Weise möglich ist. Da dieser Kanal eine gewisse Länge aufweisen muss, haben alle Lebewesen mit Lunge einen Hals. Fische haben daher keinen Hals, weil sie keine Luftröhre benötigen, da sie mit Kiemen atmen (PA III 3, 664a14–34).

Finale Ursachen in der Biologie

Die Verwendung finaler Erklärungen in der Biologie (und auch anderen Forschungsgebieten des Aristoteles) ist insbesondere in der Frühen Neuzeit und bis ins 20. Jahrhundert vielfach kritisiert worden. Unter finalen Erklärungen oder Ursachen versteht Aristoteles hier allerdings in der Regel keine übergreifenden Zwecke, die etwa eine bestimmte Spezies hätte. Ihm geht es vielmehr um eine interne Funktionsbestimmung der Organismen und ihrer Teile.

Aristoteles hat über 500 Spezies untersucht. Seine Schriften behandeln systematisch die inneren und äußeren Teile der einzelnen Tiere, Bestandteile wie Blut und Knochen, Arten der Fortpflanzung, die Nahrung, den Lebensraum und das Verhalten. Er beschreibt das Verhalten von Haustieren, exotischen Raubtieren wie dem Krokodil, Vögeln, Insekten und Meerestieren. Zu diesem Zweck ordnet er die Lebewesen.

Einteilung der Arten

Aristoteles unterscheidet zwei Hauptgruppen von Lebewesen: blutführende und blutlose Tiere. Dies entspricht der Einteilung in Vertebraten und Invertebraten. Diese ordnet er nach größten Gattungen:


Vermutlich war es nicht Aristoteles’ Absicht, eine vollständige Taxonomie zu schaffen. Das System einer Taxonomie ist für ihn auch kein Hauptgegenstand. Ziel seiner Untersuchungen war eher eine Morphologie, eine Klassifikation der Lebewesen anhand charakteristischer Merkmale. So hat er die Gattungen zwischen den genannten sowie Untergattungen nicht terminologisch fixiert.

Beispiel einer Beschreibung. Der Krake

Aristoteles und die Erkenntnisse der modernen Biologie

In vielen Fällen hat sich Aristoteles als Biologe geirrt. Einige seiner Irrtümer erscheinen reichlich kurios, wie die Beschreibung des Bisons, das sich „durch Ausschlagen und Ausstoßen seines Kots, welchen es bis siebeneinhalb Meter weit von sich schleudern kann, verteidigt“ (HA IX 45, 630b8 f.). Offenbar war seine Informationsquelle über dieses exotische Tier nicht sehr verlässlich. Weitere bekannte Irrtümer sind unter anderem die Behauptung, der Mann habe mehr Zähne als die Frau (HA II 3, 501b19), das Gehirn sei ein Kühlorgan und das Denken geschehe in der Herzgegend (PA II 7, 652b21–25; III 3, 514a16–22) sowie das Konzept der Telegonie, wonach eine vorangegangene Trächtigkeit den Phänotyp von Nachkommen aus späteren Trächtigkeiten beeinflussen könne.

Aristoteles hat aber auch auf der Grundlage seiner Beobachtungen Einsichten gewonnen, die nicht nur zutreffen, sondern die erst in der Moderne wiederentdeckt oder bestätigt worden sind. Beispielsweise erwähnt er bei der Beschreibung des angeführten Kraken, dass die Paarung durch einen Fangarm des Männchens geschieht, der gegabelt ist – die sogenannte Hektokotylisation –, und beschreibt diesen Fortpflanzungsvorgang (HA V 5, 541b9–15; V 12, 544a12; GA V 15, 720b33). Dieses Phänomen war bis ins 19. Jahrhundert nur durch Aristoteles bekannt; die genaue Art der Fortpflanzung wurde erst 1959 vollständig verifiziert.

Bedeutender noch ist seine Hypothese, nach der die Teile eines Organismus in einer hierarchischen Ordnung ausgebildet werden und nicht – wie die (bereits von Anaxagoras vertretene) Präformationslehre annimmt – vorgebildet sind (GA 734a28–35). Diese Auffassung von der embryonalen Entwicklung ist in der Neuzeit unter der von Aristoteles noch nicht verwendeten Bezeichnung Epigenesis bekannt geworden. Ihre empirische Grundlage waren für Aristoteles seine Sezierungen. In der Neuzeit war aber die Präformationslehre vom 17. bis in das 19. Jahrhundert hinein die allgemein akzeptierte Theorie, und Vertreter der Epigenesis wie William Harvey (1651) und Caspar Friedrich Wolff (1759) fanden mit ihren embryologischen Untersuchungen, die klar zeigten, dass die Embryonen sich aus ganz undifferenzierter Materie entwickeln, wenig Beachtung. Diese Einsicht setzte sich erst im frühen 19. Jahrhundert durch und verdrängte schließlich die präformistischen Spekulationen. Endgültig wurde erst im 20. Jahrhundert in der Experimentalbiologie durch Hans Driesch und Hans Spemann bestätigt, dass die embryonale Entwicklung eine Kette von Neubildungen, ein epigenetischer Prozess ist. Ferner gibt es eine Analogie zwischen der aristotelischen zielhaften Epigenesis und der Genetik.

Ausgangssituation

Lebewesen unterscheiden sich von anderen natürlichen und künstlichen Objekten dadurch, dass sie lebendig sind. Bei Homer ist die Seele "(psychê)" das, was einen Leichnam verlässt. Im Laufe des 6. und 5. Jahrhundert v. Chr. findet der Begriff zunehmend eine deutliche Ausweitung: beseelt "(empsychos)" zu sein bedeutet lebendig zu sein und das Konzept Seele weist nun auch kognitive und emotionale Aspekte auf. Aristoteles nimmt diesen Sprachgebrauch auf. In seiner Seelentheorie ist er mit zwei Positionen konfrontiert: zum einen mit dem Materialismus vorsokratischer Naturphilosophen, die behaupten, die Seele bestehe aus einer besonderen Art Materie, zum anderen mit der dualistischen Position Platons, für den die Seele unsterblich, immateriell und ihrer Natur nach eher etwas Intelligibles ist.

Hinsichtlich der Streitfrage zwischen Materialismus und Dualismus, ob Körper und Seele miteinander identisch sind oder nicht, ist Aristoteles der Auffassung, dass die Frage falsch gestellt ist. Dies erläutert er mit einem Vergleich: Die Frage "Sind Körper und Seele identisch?" ist ebenso unsinnig wie die Frage "Sind Wachs und seine Form identisch?" (An. II 1, 412b6–9). Zustände der Seele sind zwar immer auch Zustände des Körpers, aber eine Identität von Körper und Seele verneint Aristoteles ebenso wie die Unsterblichkeit der Seele.

Bestimmung der Seele

Was die Seele ist, bestimmt Aristoteles mittels seiner Unterscheidung von Form und Materie. Die Seele verhält sich zum Körper wie die Form zur Materie, das heißt wie eine Statuenform zur Bronze. Form und Materie eines Einzeldings sind aber nicht zwei verschiedene "Objekte," nicht dessen Teile, sondern Aspekte ebendieses Einzeldings.

Die Seele definiert Aristoteles als „erste Wirklichkeit "(entelecheia)" eines natürlichen organischen Körpers“ (An. II 1, 412b5 f.). Eine Wirklichkeit oder Aktualität ist die Seele, weil sie als Form den Aspekt des Lebendigen an der potentiell belebten Materie (nämlich der organischen) darstellt. Eine "erste" Wirklichkeit ist sie, insofern das Lebewesen auch dann lebendig ist, wenn es nur schläft und keine weiteren Tätigkeiten ausübt (die ebenfalls Aspekte des Seelischen sind). (An. II 1, 412a19–27).

Fähigkeiten
Die weiteren seelischen Aspekte sind die Funktionen, die für ein Lebewesen charakteristisch sind, seine spezifischen Fähigkeiten oder Vermögen "(dynamis)." Aristoteles unterscheidet vor allem folgende Fähigkeiten:


Ernährungs- und Fortpflanzungsvermögen kommen – als grundlegendes Vermögen alles Lebendigen – auch den Pflanzen zu, Wahrnehmungsvermögen (und Fortbewegungsfähigkeit) weisen nur die Tiere (einschließlich des Menschen) auf. Das Denken besitzt allein der Mensch.

Wahrnehmungsvermögen

Aristoteles unterscheidet folgende fünf Sinne und behauptet, dass es nicht mehr geben kann:

Wahrnehmung "(aisthesis)" fasst Aristoteles allgemein als ein Erleiden oder eine qualitative Veränderung (An. II 5, 416b33 f.). Das, was die Sinne wahrnehmen, ist dabei jeweils durch ein kontinuierliches Gegensatzpaar bestimmt: Sehen durch hell und dunkel, Hören durch hoch und tief, Riechen und Schmecken durch bitter und süß; Tasten weist verschiedene Gegensatzpaare auf: hart und weich, heiß und kalt, feucht und trocken.

Aristoteles behauptet, dass beim Wahrnehmungsvorgang das jeweilige Organ "wie" das Wahrgenommene wird (An. 418a3–6). Des Weiteren sagt er, dass das Organ die Form „ohne die Materie“ aufnimmt, so „wie das Wachs das Siegel des Ringes ohne Eisen und ohne Gold aufnimmt“ (An. II 12, 424a18 f.). Dies ist von manchen Kommentatoren, darunter Thomas von Aquin, so interpretiert worden, dass das Organ keine natürliche Veränderung "(mutatio naturalis)," sondern eine geistige "(mutatio spiritualis)" erfahre. Andere Interpreten meinen, dass „ohne Materie“ schlicht bedeutet, dass zwar keine Partikel in das Organ gelangen, dieses sich aber tatsächlich dem Wahrnehmungsobjekt entsprechend verändert.

Den Tastsinn besitzen alle Lebewesen, welche Wahrnehmung besitzen. Der Tastsinn ist ein Kontaktsinn, das heißt zwischen Wahrnehmungsorgan und Wahrgenommenem befindet sich kein Medium (An. II 11, 423a13 f.). Der Geschmacksinn ist eine Art Tastsinn (An. II 10, 422a8 f.). Die drei Distanzsinne Riechen, Hören und Sehen hingegen benötigen ein Medium, das den Eindruck vom Wahrgenommenen zum Organ transportiert.

Vernunft

Die Vernunft oder das Denkvermögen "(nous)" ist spezifisch für den Menschen. Aristoteles definiert sie als „das, womit die Seele denkt und Annahmen macht“ (An. III 4, 429a22 f.). Die Vernunft ist unkörperlich, da sie anderenfalls in ihren möglichen Denkgegenständen eingeschränkt wäre, was aber nicht der Fall sein darf (An. III 4, 429a17–22). Allerdings ist sie körpergebunden, da sie auf Vorstellungen "(phantasmata)" angewiesen ist. Vorstellungen bilden das Material der Denkakte, sie sind konservierte Sinneswahrnehmungen. Das entsprechende Vorstellungsvermögen ("phantasia;" weder interpretierend noch produktiv im Sinne von Phantasie) ist auf Sinneseindrücke angewiesen, wenngleich Sinneseindruck und Vorstellung qualitativ mitunter stark voneinander abweichen können, etwa bei Halluzinationen. Das Vorstellungsvermögen ist den Wahrnehmungsvermögen zugeordnet (An. III 8, 428b10–18). Insofern die Vernunft also in ihrer Tätigkeit an Vorstellungen gebunden ist, ist sie auch an einen Körper gebunden.

Glück "(eudaimonia)" und Tugend oder Bestzustand "(aretê)" sind die in Aristoteles’ Ethik zentralen Begriffe. Aristoteles vertritt die These, dass das Ziel aller absichtlichen Handlungen das im „guten Leben“ verwirklichte Glück ist. Die Ausbildung von Tugenden ist nach seiner Ansicht wesentlich dafür, dieses Ziel zu erreichen (→ Tugendethik).

Strebenshierarchie der Güter

In ihren (absichtlichen) Handlungen streben alle Menschen nach etwas, das ihnen gut erscheint. Einige dieser erstrebten Güter werden nur als Mittel erstrebt, um andere Güter zu erreichen, andere sind sowohl Mittel als auch selbst ein Gut. Da das Streben nicht unendlich sein kann, muss es ein oberstes Gut und letztes Strebensziel geben. Dieses wird nur um seiner selbst willen erstrebt. Es wird offenbar allgemein „Glück“ "(eudaimonia)" genannt (EN I 1).

Definition des Glücks als des obersten Guts

Um umrisshaft zu bestimmen, worin das Glück als oberstes Gut für den Menschen besteht, fragt Aristoteles: Worin besteht die spezifische Funktion "(telos)" oder Aufgabe "(ergon)" des Menschen? Sie besteht im Vermögen der Vernunft "(logos)," das ihn von anderen Lebewesen unterscheidet. Der für den Menschen spezifische Seelenteil verfügt über dieses Vermögen der Vernunft; der andere Seelenteil, der sich aus Emotionen und Begierden zusammensetzt, ist zwar selbst nicht vernünftig, kann sich aber durch die Vernunft leiten lassen. Um das Glück zu erlangen, muss das Individuum das Vermögen Vernunft gebrauchen, nicht bloß besitzen, und zwar auf Dauer und in einem Bestzustand "(aretê)." Demgemäß ist „das Gut für den Menschen“, das Glück, eine
Um den Zustand der Vortrefflichkeit zu erreichen, muss man den beiden Seelenteilen entsprechend (a) Verstandestugenden und (b) Charaktertugenden ausbilden. Tugenden sind für Aristoteles Haltungen, zu denen jeder Mensch die Anlage besitzt, die sich jedoch durch Erziehung und Gewöhnung erst ausbilden müssen.

Verstandestugenden

Unter den Verstandestugenden beziehen sich einige auf das Wissen von Unveränderlichem oder die Herstellung von Gegenständen. Allein die Klugheit "(phronêsis)" ist mit dem Handeln verknüpft, und zwar als Tugend mit dem Ziel eines guten Lebens. Sie ist – neben den Charaktertugenden – notwendig, um in konkreten Entscheidungssituationen im Hinblick auf das gute Leben handeln zu können. Im Bereich menschlicher Handlungen gibt es – anders als in den Wissenschaften – keine Beweise, und um klug zu sein, bedarf es dabei auch der Erfahrung. Die Funktion der Klugheit besteht darin, die Mitte "(mesotês)" zu wählen.

Charaktertugenden

Charaktertugenden sind Haltungen "(hexeis)," für die kennzeichnend ist, dass man sie loben und tadeln kann. Sie werden durch Erziehung und Gewöhnung ausgeprägt, wobei dies nicht als eine Konditionierung zu verstehen ist. Zwar hängt von Kindheit an sehr viel von der Gewöhnung ab (EN II 1, 1103b24), Charaktertugenden liegen jedoch erst vor, wenn jemand sich wissentlich für die entsprechenden Handlungen entscheidet, und zwar nicht wegen möglicher Sanktionen, sondern um der tugendhaften Handlungen selbst willen, und wenn er dabei auch nicht ins Wanken gerät (EN II 3, 1105a26–33). Auch unterscheidet sich der Tugendhafte vom Selbstbeherrschten (der dieselben Handlungen ausführen mag, sich aber dazu zwingen muss) dadurch, dass er an der Tugend Freude empfindet (EN II 2, 1104b3 ff.).

Durch Gewöhnung ausgeprägt werden die Charaktertugenden, indem Übermaß und Mangel vermieden werden.

Das Instrument der Mitte bestimmt die Charaktertugenden genauer. So ist beispielsweise die Tugend der Tapferkeit eine Mitte zwischen den Lastern Tollkühnheit und Feigheit. Grundlage für die Tugenden sind dabei sowohl die Handlungen als auch die Emotionen und Begierden. Nicht tapfer, sondern tollkühn ist jemand, der entweder in einer bestimmten Situation völlig furchtlos ist, obwohl die Situation bedrohlich ist, oder der in einer ernsten Bedrohungssituation seine Furcht ignoriert. Die Mitte besteht also – hier wie bei den anderen Charaktertugenden – darin, angemessene Emotionen zu haben und demgemäß angemessen zu handeln. Dabei ist diese Lehre von der Mitte vermutlich nicht in konkreten Situationen als normativ handlungsleitend, sondern nur als Beschreibungsinstrument der Charaktertugenden aufzufassen. Sie ist auch "keine arithmetische" Mitte, sondern eine Mitte "für uns" "(pros hêmas)," die die jeweilige Emotion, die Person sowie die Situation berücksichtigt.
Diese Tabelle zeigt einige wichtige Charaktertugenden (EN II 7):
Aristoteles definiert die Charaktertugend dementsprechend als
Im Kontext der Analyse des guten Lebens unterscheidet Aristoteles drei Lebensformen, die verschiedene Ziele verfolgen:


Das Genussleben im Sinne einer bloßen Befriedigung der Begierden hält Aristoteles für sklavisch und verwirft es. Gelderwerb und Reichtum als Ziel hält er nicht für eine Lebensform, da Geld immer nur Mittel zu einem Zweck, aber nie selbst Ziel ist. Er plädiert für das theoretische Leben als beste Lebensform. Die beste Tätigkeit, die in der Glücksdefinition gesucht wird, ist diejenige des Theoretikers, der Erste Philosophie, Mathematik usw. betrachtet, denn sie bedeutet Muße, dient keinem anderen Zweck, betätigt mit den Verstandestugenden das Beste im Menschen und weist die besten Erkenntnisgegenstände auf (EN X 7, 1177a18–35).

Obwohl er das theoretische Leben für das bestmögliche hält, weist er darauf hin, dass die Betrachtung als Lebensform den Menschen als Menschen übersteigt und eher etwas Göttliches ist (EN X 7, 1177b26–31). Das zweitbeste Leben ist das politische. Es besteht in der Betätigung der Charaktertugenden, die den Umgang mit anderen Menschen sowie mit unseren Emotionen bestimmen. Da Charaktertugenden und Verstandestugenden einander nicht ausschließen, meint Aristoteles möglicherweise, dass selbst der Theoretiker, insofern er ein soziales und mit Emotionen ausgestattetes Wesen ist, sich im Sinne des zweitbesten Lebens betätigen muss.

Aristoteles fasst die Betätigung der Verstandestugenden (zumindest der Klugheit) und der Charaktertugenden als wesentliche Elemente des Glücks auf. Aber auch äußere oder körperliche Güter und auch die Lust hält er für Bedingungen, die hilfreich oder sogar notwendig sind, um glücklich zu werden. Güter wie Reichtum, Freunde und Macht verwenden wir als Mittel. Fehlen einige Güter, wird das Glück getrübt, wie bei körperlicher Verunstaltung, Einsamkeit oder missratenen Kindern (EN I 9, 1099a31–1099b6).

Aristoteles meint, das Genussleben führe nicht zum Glück. Er hält die Lust nicht für das oberste Gut. Gegenüber lustfeindlichen Positionen macht er jedoch geltend, dass das gute Leben Lust einschließen müsse und bezeichnet die Lust als ein Gut (EN VII 14). Auch meint er, man könne einen Tugendhaften, der „auf das Rad geflochten“ sei, nicht als glücklich bezeichnen (EN VII 14, 1153b18–20).

Gegen Platons Auffassung, Lüste seien Prozesse "(kinêsis)," die einen Mangel beseitigen (wie Lust beim Durstlöschen), und somit sei das Vollenden des Prozesses besser als dieser selbst, argumentiert Aristoteles dafür, dass Lüste Tätigkeiten "(energeia)" sind, die kein Ziel außer sich aufweisen. Paradigmatische Fälle sind Wahrnehmen und Denken.

Mit diesem Lustkonzept, das Lust als „unbehinderte Tätigkeit“ oder „Vervollkommnung der Tätigkeit“ definiert (EN VII 13, 1153a14 f.; X 4, 1174b33), macht er geltend, dass die Betätigung der Verstandestugenden und der Charaktertugenden lustvoll sein kann. Ob Lüste gut oder schlecht sind, hängt davon ab, ob die entsprechenden Tätigkeiten gut oder schlecht sind. Bei körperlichen Lüsten ist Letzteres etwa der Fall, wenn sie im Übermaß auftreten oder wenn sie gute Handlungen verhindern und so dem Glück abträglich sind.

Die politische Philosophie des Aristoteles schließt an seine Ethik an. Als umfassende Form aller Gemeinschaften besteht der Staat "(polis)" um des höchsten Gutes willen, des Glücks (EN I 1, 1094a26–b11; Pol. I 1, 1252a1–7). Die politische Philosophie fragt also nach den Bedingungen des Glücks hinsichtlich des Lebens im Staat. Hierfür analysiert er die Bestandteile jeder menschlichen Gemeinschaft und jedes Staates und untersucht, welche Verfassung "(politeia)" die beste ist und für welche besonderen Bedingungen welche Verfassung die richtige ist.

Aus der Sicht von Aristoteles besteht der Staat von Natur aus (Pol. I 2, 1253a1). Betrachtet man die Teile des Staates, so liegen zunächst zwei grundlegende Beziehungen vor: die zwischen Mann und Frau, deren Zweck die Fortpflanzung ist, und die von Herr und Sklave mit dem Zweck, den Lebensunterhalt zu sichern. Beide gemeinsam ergeben die kleinste Gemeinschaft: den Haushalt.

Aristoteles rechtfertigt die Sklaverei. Er vertritt die These, dass es Sklaven gibt, die von Natur aus zu nichts anderem bestimmt sind als zur Sklaverei. Das begründet er damit, dass solche „Sklaven von Natur“ nur in geringem Maße Anteil an der Vernunft hätten; daher sei es nicht nur gerechtfertigt, sondern sogar für sie selbst vorteilhaft, dass sie ihr Leben als Sklaven verbringen müssen (Pol. I 5, 1254b20–23; 1255a1 f.). Allerdings ist sein Konzept unklar und widersprüchlich, da er die Freilassung von Sklaven grundsätzlich billigt und für die Unterscheidung zwischen akzidentellen Sklaven (etwa durch Kriegsgefangenschaft) und Sklaven von Natur keine klaren Kriterien nennt. Sein Rat, Sklaven als Lohn die Freiheit zu versprechen (Pol. VII 10, 1330a20 f.), widerspricht der Vorstellung eines „Sklaven von Natur“.

Entsprechend argumentiert er auch für eine Unterordnung der Frau (Pol. VII 10, 1330a20 f.). Es sei für sie besser, vom Mann beherrscht zu werden, da ihre Urteilskraft schwächer sei als die männliche (Pol. I 5, 1254b10–15; I 13, 1259a12).

Mehrere Haushalte ergeben ein Dorf, in dem Arbeitsteilung bessere Versorgung ermöglicht, und mehrere Dörfer einen Staat. Dieser ist autark in dem Sinne, dass er die Bedingungen für ein gutes Leben bereitstellen kann. Aristoteles unterscheidet den Grund der Entstehung des Staates von seinem Zweck. Der Staat entsteht zum Zweck des Überlebens, des Lebens an sich, sein Zweck aber ist das "gute" Leben: εὖ ζῆν = eu zēn = gut leben (Pol. I 2, 1252a25–1253a1).

Nach Aristoteles gehört es zur Natur des Menschen, in Gemeinschaft zu leben, denn er ist ein "„zôon politikon“", ein Lebewesen in der Polisgemeinschaft (Pol. I 2, 1253a3). Nur im Staat kann der Mensch das gute Leben verwirklichen. Wer des Staates nicht bedürfe, sei „entweder ein Tier oder ein Gott“ (Pol. I 2, 1253a29).

Eine Polis (ein Staat) besteht aus den freien Bürgern. Der Zweck des Staates ist immer das gute Leben. Militär- oder Handelsbündnisse, also Verträge, machen noch keinen Staat aus. Kennzeichnendes Merkmal eines bestimmten Staates ist seine Verfassung.

Der Bürger

Bürger sind die mit dem Bürgerrecht ausgestatteten Einwohner, die sich aktiv am politischen Geschehen (am Richten und Regieren) beteiligen (Pol. III 1, 1275a22). Den Bürger bestimmt Aristoteles also primär nicht über die Herkunft oder den Wohnort, sondern über die Partizipation an den politischen Institutionen des Staates. Entsprechend den damaligen Verhältnissen in Athen betrachtet Aristoteles Frauen, Kinder, Sklaven und Fremde nicht als Bürger. Ein Bürger darf auch nicht für seinen Lebensunterhalt arbeiten müssen. Lohnarbeiter und Handwerker können somit keine Bürger sein (Pol. III 5, 1278a11). Die jeweilige Verfassung eines Staates bestimmt genauer, wer Bürger ist und wer nicht.

Theorie der Verfassungen

In seiner Unterscheidung der verschiedenen Verfassungen stellt Aristoteles zwei Fragen:
Bei der ersten Frage unterscheidet er drei mögliche Antworten: einer, wenige, viele. Bei der zweiten Frage unterscheidet er zwei mögliche Zustände und Nutznießer: die Verfassung ist gerecht, wenn zum Nutzen aller regiert wird; sie ist ungerecht oder verfehlt, wenn allein zum Nutzen der Herrschenden regiert wird (Pol. III 6, 1279a17–21). Auf dieser Grundlage entwirft er eine erste Staatsformenlehre mit sechs Verfassungen (Pol, III 6–8):
Die verschiedenen Verfassungen wenden auf unterschiedliche Weise die distributive Gerechtigkeit an (Pol. III 9, 1280a7–22). Distributive Gerechtigkeit bestimmt er als die Verteilung proportional zur Leistung oder Würde (EN V 6).
Kritik an schlechten Verfassungen

Unter den schlechten, nicht am Gemeinwohl orientierten Verfassungen hält er die Tyrannis für die schlechteste, denn in ihr herrscht der Tyrann über den Staat im Sinne einer despotischen Alleinherrschaft wie der Herr über den Sklaven (Pol. III 8, 1279b16).

Für etwas weniger schlecht erachtet er die durch die Herrschaft der Reichen gekennzeichnete Oligarchie, die ebenso wie die Tyrannis sehr instabil ist (Pol. V 12). Für den Grundirrtum der Oligarchie hält Aristoteles die Auffassung, dass die, die in "einer" Hinsicht (Besitz) ungleich sind, in "allen" Hinsichten ungleich seien. Entsprechend besteht der Grundirrtum der Demokratie in der Ansicht, dass die, die in "einigen" Hinsichten gleich sind, dies in "allen" seien (Pol. V 1, 1301a25–36).

Die Demokratie hält Aristoteles für weniger schlecht als die Tyrannis und Oligarchie. Sie ist neben Gleichheit durch Freiheit gekennzeichnet. Freiheit bedeutet dabei, so zu leben wie man will, Gleichheit, dass das Regieren und Regiertwerden reihum geht (1317b2–12). Die absolute Freiheit, so zu leben wie man will, hält Aristoteles insofern für problematisch, als sie mit der Herrschaft der Verfassung in Konflikt steht (Pol. V 9, 1310a30–35). Gleichheit kritisiert er, wenn sie als totale arithmetische interpretiert wird, die dazu führe, dass die Herrschaft der Unvermögenden die Besitzenden enteignet. Dafür, dass Aristoteles die Beteiligung des „einfachen Volkes“ an der Herrschaft durchaus nicht rundweg abgelehnt hat, spricht ferner seine so genannte „Summierungsthese“ (Pol. III 11, 1281 a38–b9) und eine differenzierte Untersuchung der Formen der Volksherrschaft im Rahmen seiner zweiten Staatsformenlehre.

Gute Verfassungen

Unter den guten Verfassungen ist die Monarchie (unter der Aristoteles nicht zwingend ein Königtum, sondern nur eine dem Gemeinwohl dienende Alleinherrschaft versteht) am wenigsten gut. Insofern sie nicht gesetzgebunden ist, ist sie eine bloße Herrschaftsform, teilweise kaum eine Verfassung, und insofern problematisch, als nur das Gesetz unbeeinflusst von Emotionen herrschen kann.

Unter einer Aristokratie versteht er eine Herrschaft der Guten, das heißt derjenigen, die am meisten Anteil an der Tugend "(aretê)" haben, was nicht unbedingt Herrschaft eines Geburtsadels bedeuten muss. Da das Ziel des Staates, das gute Leben, in einer Aristokratie im höchsten Maße verwirklicht wird, hält Aristoteles sie (neben einer bestimmten Form der Monarchie, nämlich der Königsherrschaft) für die beste Verfassung (Pol. IV 2, 1289a30–32).

Aristoteles diskutiert Verfassungstheorie allerdings nicht ohne Realitätsbezug. Oft ist aus seiner Sicht eine absolut beste Verfassung in einem bestimmten Staat nicht möglich. Was am besten für einen konkreten Staat ist, muss immer relativ zu den Umständen bestimmt werden (Pol. IV 1, 1288b21–33). Solche Überlegungen durchziehen die ganze Verfassungstheorie. Sie zeigen sich insbesondere im Modell der Politie, die Aristoteles als die bestmögliche für die meisten zeitgenössischen Staaten ansieht (Pol. IV 11, 1295a25). Sie ist eine Mischverfassung, die Elemente der Demokratie und der Oligarchie enthält. Dabei wird für die Bestrebungen nach Gleichheit auf der einen und nach Reichtum auf der anderen Seite ein Ausgleich geschaffen. Dieser Ausgleich wird unter anderem durch Ämterzuteilung nach Klassenzugehörigkeit erreicht (Pol. V 8, 1308b26). Auf diese Weise wird nach seiner Auffassung die Stabilität erhöht und sozialen Unruhen vorgebeugt (die in griechischen Staaten häufig waren). Besondere Stabilität verleiht dem Staat ein breiter Mittelstand (Pol. IV 11, 1295b25–38).

Mimêsis

Der zentrale Begriff der aristotelischen Theorie der Dichtung, die er in seiner zu Lebzeiten nicht veröffentlichten "Poetik" "(poiêtikê)" ausarbeitet, ist die "mimêsis," das heißt die „Nachahmung“ oder „Darstellung“. Neben der Dichtung im engeren Sinne (Epik, Tragödie, Komödie und Dithyrambendichtung) zählen auch Teile der Musik und der Tanz für Aristoteles zu den mimetischen Künsten (Poet. 1, 1447a). Abbildende Künste wie Malerei und Plastik behandelt Aristoteles nicht weiter, sondern erwähnt nur, dass sie ebenfalls nach dem Prinzip der Nachahmung arbeiten (Poet. 1, 1447a19 f.). Gemeinsam ist allen mimetischen Künsten die zeitliche Sukzession. Insofern lässt sich "mimêsis" als ästhetisches Handeln auffassen.

In der Lust an der "mimêsis" sieht Aristoteles eine anthropologische, allen Menschen gemeinsame Grundgegebenheit. Denn die Freude an ihr sowie an ihren Produkten ist den Menschen angeboren, da sie gerne lernen (Poet. 4, 1448b5-15). Im Gegensatz zu den anderen mimetischen Künsten ist für die Dichtung die Verwendung von Sprache spezifisch. Alle Dichtung ist zudem Darstellung von Handlungen; allerdings nicht von tatsächlich Geschehenem, sondern von dem, „was geschehen könnte, das heißt das nach den Regeln der Wahrscheinlichkeit oder Notwendigkeit Mögliche“ (Poet. 9, 1451a37 f.). Dargestellt werden Handlungen, die etwas über den Menschen im Allgemeinen aussagen, nicht über zufällige und beliebige Verhältnisse. Ziel ist nicht die Nachahmung von Menschen; nicht auf Figuren oder Charaktere, sondern auf Handlungen kommt es an; Erstere sind nur Mittel (Poet. 6, 1450a26–23).

Arten der Dichtung

Aristoteles klassifiziert vier Formen der existierenden Dichtung nach zwei Kriterien: (i) der Art der Darstellung von Handlung und (ii) der Art der dargestellten Figuren.
Dramatische Darstellung ist dadurch gekennzeichnet, dass die jeweilige Figur selbst die Handlung darstellt, berichtende dadurch, dass über die Handlung berichtet wird. Mit „besser“ und „schlechter“ sind die Figuren und ihre Handlungen gemeint. Bessere Figuren oder Charaktere sind etwas besser als wir selbst, schlechtere schlechter; beides aber nie so weit, dass wir uns nicht mehr mit ihnen identifizieren können (Poet. 5, 1449a31–1449b13). Aristoteles vertritt dabei die Hypothese, dass die Tragödie aus dem Epos und die Komödie aus dem Spottlied entstanden ist (Poet. 4, 1449a2–7).

Eine Untersuchung der Komödie kündigt Aristoteles an. Sie ist aber – wie auch eine des Spottliedes – nicht überliefert. Das Epos behandelt er recht kurz. Seine überlieferte Dichtungstheorie ist daher primär eine Tragödientheorie.

Tragödie

Aristoteles definiert die Tragödie als eine
Dieser kurze Satz ist eine der meistdiskutierten Passagen im gesamten Werk des Aristoteles. (3) nennt das dramatisch-darstellende Element. (1) nennt (neben oben schon genannten Aspekten) die (später sogenannte) Einheit der Handlung. Die Einheit des Ortes und der Zeit wurde in der Renaissance der aristotelischen Tragödientheorie zugeschrieben, er vertrat sie aber selbst so nicht. (2) bezieht sich darauf, dass die Sprache der Tragödie Melodie und Rhythmus aufweist. Die weitaus meiste Aufmerksamkeit hat (4) erhalten, insbesondere (4b).

Emotionserregung und Katharsis

In (4) beschreibt Aristoteles die Funktion der Tragödie, das was sie leisten soll. Weitgehend unumstritten ist nur
(4a): Beim Zuschauer sollen durch die dargestellte Handlung die Emotionen Mitleid und Furcht erregt werden. Unklar ist allerdings, ob "eleos" und "phobos" tatsächlich mit „Mitleid“ und „Furcht“ oder mit „Elementareffekten“ „Jammer“ und „Schauder“ wiederzugeben sind. Dass die Handlung selbst und nicht die Aufführung die entscheidende Rolle bei der Emotionserregung spielt, ist daraus ersichtlich, dass Aristoteles auch die gelesene Tragödie durch seine Theorie berücksichtigt sieht. Mitleid wird erregt, wenn die Protagonisten unverdient Unglück erleiden, Furcht, wenn sie dabei dem Zuschauer (oder Leser) ähnlich sind.

(4b) ist höchst kontrovers, da die Funktionsweise nicht weiter erläutert ist. Das Wort "Katharsis," das als Metapher (wie „Reinigung“ im Deutschen) einen Sinnüberschuss aufweist, hat zu den verschiedensten Deutungen Anlass gegeben, insbesondere weil es schon vor Aristoteles verwendet wurde, nämlich unter anderem in der Medizin (Reinigung durch Brech- und Abführmittel) und in religiösen Kulten (Reinigung von unreinen Personen durch religiöse Praktiken). Die grammatikalische Konstruktion "Reinigung der Emotionen" lässt dabei verschiedene Deutungen zu, worin die Reinigung besteht. Vermutlich sollen die Emotionen "selbst" (durch eine Emotionserregung) gereinigt werden; die Aussage ist aber auch als Reinigung "von" den Emotionen verstanden worden.

Der normativ-deskriptive Charakter der Tragödientheorie

Aristoteles’ Tragödientheorie weist zwei Typen von Aussagen auf. Zum einen untersucht er die Grundlagen der Dichtung, unterscheidet verschiedene Arten von ihr und nennt Teile einer Tragödie und deren Funktionsweise. Zum anderen spricht er aber auch davon, was eine "gute" Tragödie ist und was der Dichter entsprechend machen "soll." So äußert er etwa, dass in einer guten Tragödie ein Protagonist weder aufgrund seines guten noch seines schlechten Charakters vom Glück ins Unglück gerät, sondern aufgrund eines Fehlers "(hamartia)," beispielsweise wie Ödipus aufgrund von Unwissenheit. Nur eine schlechte Tragödie würde zeigen, wie ein guter Charakter vom Glück ins Unglück oder ein schlechter vom Unglück ins Glück gerät. Der Grund hierfür ist die Funktion der Tragödie, das Bewirken von Mitleid und Furcht. In schlechten Tragödien würden Mitleid und Furcht nicht erregt werden, in guten ist dies aufgrund der Beschaffenheit des Protagonisten und des Fehlers als Ursache des Unglücks der Fall (Poet. 13, 1452b28–1453a12).

Von Aristoteles ist zudem ein Hymnos an Aretê überliefert, den er in Erinnerung an seinen Freund Hermias verfasst hat.

Die Lehre des Aristoteles hat auf seine Schule, den Peripatos, nach seinem Tode weit weniger Einfluss ausgeübt als Platons Lehre auf dessen Akademie. Aristoteles wurde keine Verehrung zuteil, die mit derjenigen Platons bei den Platonikern vergleichbar wäre. Dies bedeutete einerseits Offenheit und Flexibilität, andererseits Mangel an inhaltlich begründetem Zusammenhalt. Die Peripatetiker widmeten sich vor allem empirischer Naturforschung und befassten sich unter anderem auch mit Ethik, Seelenlehre und Staatstheorie. Dabei kamen Aristoteles’ Schüler Theophrastos, sein Nachfolger als Leiter der Schule, und dessen Nachfolger Straton zu teilweise anderen Ergebnissen als der Schulgründer. Nach Stratons Tod (270/268 v. Chr.) begann eine Periode des Niedergangs.
Das Studium und die Kommentierung der Schriften des Aristoteles wurde damals im Peripatos anscheinend vernachlässigt, jedenfalls weit weniger eifrig betrieben als das Platonstudium in der konkurrierenden Akademie. Erst im ersten Jahrhundert v. Chr. sorgte Andronikos von Rhodos für eine Zusammenstellung der Lehrschriften (Pragmatien) des Aristoteles, und auch bei deren Auslegung durch die Peripatetiker kam es zu einem Aufschwung. Die für die Öffentlichkeit bestimmten „exoterischen“ Schriften, insbesondere die Dialoge, waren lange populär, gingen aber in der römischen Kaiserzeit verloren. Cicero hat sie noch gekannt. Die Peripatetiker betrachteten die Lehrschriften als speziell für ihren internen Unterrichtsgebrauch bestimmt. In der römischen Kaiserzeit war der einflussreichste Repräsentant des Aristotelismus Alexander von Aphrodisias, der gegen die Platoniker die Sterblichkeit der Seele vertrat.

Obwohl Aristoteles großen Wert auf die Widerlegung von Kernbestandteilen des Platonismus gelegt hatte, waren es gerade die Neuplatoniker, die in der Spätantike einen maßgeblichen Beitrag zur Erhaltung und Verbreitung seiner Hinterlassenschaft leisteten, indem sie seine Logik übernahmen, kommentierten und in ihr System integrierten. Eine besonders wichtige Rolle spielten dabei im 3. Jahrhundert n. Chr. Porphyrios, im 5. Jahrhundert Proklos, Ammonios Hermeiou (der in Alexandria die Tradition der Aristoteles-Kommentierung begründete) und im 6. Jahrhundert Simplikios, der bedeutende Aristoteleskommentare verfasste. Im 4. Jahrhundert schrieb Themistios Paraphrasen zu Werken des Aristoteles, die eine starke Nachwirkung erzielten. Er war unter den spätantiken Kommentatoren der einzige (wenn auch neuplatonisch beeinflusste) Aristoteliker; die anderen befassten sich mit dem Aristotelismus aus neuplatonischer Perspektive und strebten eine Synthese platonischer und aristotelischer Auffassungen an, wobei oft ein Übergewicht der platonischen erkennbar ist. Noch zu Beginn des 7. Jahrhunderts kommentierte der angesehene, in Konstantinopel lehrende christliche Philosoph Stephanos von Alexandria Werke des Aristoteles.

Bei den prominenten antiken Kirchenvätern war Aristoteles wenig bekannt und unbeliebt, manche verachteten und verspotteten seine Dialektik. Sie verübelten ihm, dass er das Weltall für ungeschaffen und unvergänglich hielt und die Unsterblichkeit der Seele bezweifelte (oder nach ihrem Verständnis bestritt). Ein positiveres Verhältnis zu Aristoteles hatten hingegen manche christliche Gnostiker und andere häretische Christen: Arianer (Aëtios, Eunomius), Monophysiten, Pelagianer und Nestorianer – ein Umstand, der den Philosophen für die kirchlichen Autoren erst recht suspekt machte. Syrer – monophysitische wie nestorianische – übersetzten das "Organon" in ihre Sprache und setzten sich intensiv damit auseinander. Im 6. Jahrhundert schrieb Johannes Philoponos Aristoteles-Kommentare, übte aber auch scharfe Kritik an der aristotelischen Kosmologie und Physik. Er war mit seiner Impetustheorie ein Vorläufer spätmittelalterlicher und frühneuzeitlicher Kritik an der aristotelischen Bewegungslehre.

Im Byzantinischen Reich des Frühmittelalters wurde Aristoteles wenig beachtet. Sein Einfluss machte sich vorwiegend indirekt geltend, nämlich über die meist neuplatonisch gesinnten spätantiken Autoren, die Teile seiner Lehre übernommen hatten. Daher war Vermischung mit neuplatonischem Gedankengut von vornherein gegeben. Bei Johannes von Damaskus tritt die aristotelische Komponente deutlich hervor. Im 11. und 12. Jahrhundert kam es zu einer Wiederbelebung des Interesses an aristotelischer Philosophie: Michael Psellos, Johannes Italos und dessen Schüler Eustratios von Nikaia (beide wegen Häresie verurteilt) sowie der primär philologisch orientierte Michael von Ephesos schrieben Kommentare. Die Kaisertochter Anna Komnena förderte diese Bestrebungen.

Im islamischen Raum setzte die Wirkung der Werke des Aristoteles früh ein und war breiter und tiefer als in der Spätantike und im europäischen Früh- und Hochmittelalter. Der Aristotelismus dominierte qualitativ und quantitativ gegenüber der übrigen antiken Tradition. Schon im 9. Jahrhundert waren die meisten Werke des Aristoteles, häufig durch vorangehende Übersetzung ins Syrische vermittelt, in arabischer Sprache verfügbar, ebenso antike Kommentare. Hinzu kam ein reichhaltiges unechtes (pseudo-aristotelisches) Schrifttum teilweise neuplatonischen Inhalts, darunter Schriften wie die "Theologie des Aristoteles" und der "Kalam fi mahd al-khair" "(Liber de causis)." Die aristotelischen Ideen waren von Anfang an mit neuplatonischen vermischt, und man glaubte an eine Übereinstimmung der Lehren Platons und des Aristoteles. In diesem Sinne deuteten al-Kindi (9. Jahrhundert) und al-Farabi (10. Jahrhundert) und die ihnen folgende spätere Tradition den Aristotelismus; bei ibn Sina (Avicenna) trat das neuplatonische Element stärker in den Vordergrund. Einen relativ reinen Aristotelismus vertrat hingegen im 12. Jahrhundert ibn Rušd (Averroes), der zahlreiche Kommentare schrieb und die aristotelische Philosophie gegen al-Ghazali verteidigte.
Im lateinischen Mittelalter war zunächst bis ins 12. Jahrhundert nur ein kleiner Teil des Gesamtwerks des Aristoteles verbreitet, nämlich zwei der logischen Schriften ("Kategorien" und "De interpretatione"), die Boethius im frühen 6. Jahrhundert übersetzt und kommentiert hatte, zusammen mit der Einleitung des Porphyrios zur Kategorienlehre. Dieses Schrifttum, später als "Logica vetus" bezeichnet, bildete die Grundlage des Logikunterrichts. Mit der großen Übersetzungsbewegung des 12. und 13. Jahrhunderts änderte sich diese enge Begrenzung. Im 12. Jahrhundert wurden die bisher fehlenden logischen Schriften ("Analytica priora" und "posteriora," "Topik," "Sophistische Widerlegungen") in lateinischer Sprache verfügbar; sie machten die "Logica nova" aus. Dann wurden eines nach dem anderen fast alle restlichen Werke zugänglich (teils erst im 13. Jahrhundert). Die meisten Schriften wurden mehrmals ins Lateinische übertragen (entweder aus dem Arabischen oder aus dem Griechischen). Michael Scotus übersetzte Aristoteleskommentare des Averroes aus dem Arabischen. Sie wurden eifrig benutzt, was in der zweiten Hälfte des 13. Jahrhunderts zur Entstehung des lateinischen Averroismus führte, der ein für damalige Verhältnisse relativ konsequenter Aristotelismus war.

Im Lauf des 13. Jahrhunderts wurden die Schriften des Aristoteles als Standardlehrbücher zur Grundlage der an den Universitäten (in der Fakultät der Freien Künste) betriebenen scholastischen Wissenschaft; 1255 wurden seine Logik, Naturphilosophie und Ethik an dieser Fakultät der Pariser Universität als Lehrstoff vorgeschrieben. Die Führungsrolle kam der Pariser und der Oxforder Universität zu. Wegweisend waren die Aristoteleskommentare des Albertus Magnus. Das Verfassen von Aristoteleskommentaren wurde eine Hauptbeschäftigung der Magister, und viele von ihnen hielten die kommentierten Lehrbücher für irrtumsfrei. Besonders intensiv studierte man neben der aristotelischen Methodik die Wissenschaftstheorie, um sie als Basis für ein hierarchisch geordnetes System der Wissenschaften zu verwenden.

Widerstand erhob sich allerdings von theologischer Seite gegen einzelne Lehren, vor allem gegen die Thesen von der Ewigkeit der Welt und der absoluten Gültigkeit der Naturgesetze (Ausschluss von Wundern), sowie gegen den Averroismus. Daher kam es 1210, 1215, 1231, 1245, 1270 und 1277 zu kirchlichen Verurteilungen von Lehrsätzen und zu Aristotelesverboten. Sie richteten sich aber nur gegen die naturphilosophischen Schriften oder gegen einzelne Thesen und konnten den Siegeszug des Aristotelismus nur vorübergehend hemmen. Diese Verbote betrafen nur Frankreich (vor allem Paris), in Oxford galten sie nicht. Aristoteles wurde „der Philosoph“ schlechthin: mit "Philosophus" (ohne Zusatz) war immer nur er gemeint, mit "Commentator" Averroes. Gegenpositionen (vor allem in der Erkenntnistheorie und Anthropologie) vertraten Anhänger der platonisch beeinflussten Lehren des Augustinus, besonders Franziskaner („Franziskanerschule“). Ein prominenter Kritiker des Aristotelismus war der Franziskaner Bonaventura. Ein anderer Franziskaner, Petrus Johannis Olivi, stellte um 1280 missbilligend fest: „Man glaubt ihm (Aristoteles) ohne Grund – wie einem Gott dieser Zeit.“ Schließlich setzte sich das von dem Dominikaner Thomas von Aquin abgewandelte und weiterentwickelte aristotelische Lehrsystem (Thomismus) durch, zunächst in seinem Orden und später in der gesamten Kirche.

Allerdings schrieb man weiterhin neuplatonische Schriften zu Unrecht dem Aristoteles zu, wodurch das Gesamtbild seiner Philosophie verfälscht wurde. Dante würdigte in seiner "Göttlichen Komödie" Bedeutung und Ansehen des Aristoteles, indem er ihn als „Meister“ darstellte, der von den anderen antiken Philosophen bewundert und geehrt wird; jedoch verwarf Dante manche aristotelische Lehren.

Die "Politik" des Aristoteles wurde erst um 1260 von Wilhelm von Moerbeke ins Lateinische übersetzt und dann von Thomas von Aquin und anderen Scholastikern kommentiert und zitiert. Besonders die Rechtfertigung der Sklaverei bzw. Knechtschaft stieß bei den Gelehrten auf Interesse und grundsätzliche Zustimmung. Die "Politik" regte Kommentatoren und Verfasser politischer Traktate zu Erörterungen über Vor- und Nachteile von Erb- bzw. Wahlmonarchie sowie von absoluter bzw. ans Gesetz gebundener Herrschaft an.

In der Epoche des Übergangs vom Spätmittelalter zur Frühen Neuzeit setzte sich Nikolaus von Kues kritisch mit Aristoteles auseinander. Er stellte sich Aristoteles als fiktiven Gesprächspartner vor, dem man die Berechtigung der cusanischen Lehre von der "Coincidentia oppositorum" einsichtig machen könnte, obwohl Aristoteles sie nach seinem Satz vom Widerspruch hätte verwerfen müssen.

In der Renaissance fertigten Humanisten neue, viel leichter lesbare Aristotelesübersetzungen ins Lateinische an, weshalb man weniger auf die Kommentare angewiesen war. Bedeutend sind u. a. die Übersetzungen der "Nikomachischen Ethik" und der "Politik" durch Leonardo Bruni. Man begann aber auch, die griechischen Originaltexte zu lesen. Es kam zu heftigem Streit zwischen Platonikern und Aristotelikern, wobei die beteiligten Humanisten mehrheitlich zu Platon neigten. Es gab in der Renaissance aber auch bedeutende Aristoteliker wie Pietro Pomponazzi (1462–1525) und Jacopo Zabarella (1533–1589), und es entstanden damals im Abendland mehr Aristoteleskommentare als während des gesamten Mittelalters. Wie im Mittelalter herrschte auch noch bei vielen Renaissance-Gelehrten das Bestreben vor, platonische und aristotelische Standpunkte untereinander und mit der katholischen Theologie und Anthropologie zu versöhnen. Seit dem 15. Jahrhundert war es aber möglich, dank des besseren Zugangs zu den Quellen das Ausmaß der fundamentalen Gegensätze zwischen Platonismus, Aristotelismus und Katholizismus besser zu verstehen. Bei der Vermittlung dieser Erkenntnisse spielte der byzantinische Philosoph Georgios Gemistos Plethon eine wichtige Rolle. Unabhängig davon herrschte der (neu)scholastische Aristotelismus, der die mittelalterliche Tradition fortsetzte, mit seiner Methode und Terminologie an Schulen und Universitäten noch bis tief in die Neuzeit, auch in den lutherischen Gebieten, obwohl Martin Luther den Aristotelismus ablehnte.

Im sechzehnten Jahrhundert unternahmen Bernardino Telesio und Giordano Bruno Frontalangriffe auf den Aristotelismus, und Petrus Ramus trat für eine nichtaristotelische Logik ein (Ramismus). Bereits Giovanni Battista Benedetti (1530–1590) widerlegte 1554 in seinem Werk "Demonstratio proportionum motuum localium contra Aristotilem et omnes philosophos" in einem simplen Gedankenexperiment die aristotelische Annahme, dass Körper im freien Fall umso schneller fallen, je schwerer sie sind: Zwei gleiche Kugeln, die durch eine (masselose) Stange fest verbunden werden, fallen mit derselben Geschwindigkeit wie jede der beiden Kugeln allein.

Aber erst seit dem 17. Jahrhundert verdrängte ein neues Wissenschaftsverständnis die aristotelisch-scholastische Tradition. Den Umschwung in der Physik leitete Galileo Galilei ein. 1647 konnte die von Aristoteles aufgestellte Hypothese eines Horror Vacui von Blaise Pascal mit dem Versuch Leere in der Leere widerlegt werden. Erst in der 1687 veröffentlichten Schrift "Philosophiae Naturalis Principia Mathematica" von Isaac Newton wurde mit dem Trägheitsprinzip ein Fundament der neuen klassischen Mechanik errichtet, das die aristotelischen Annahmen ersetzte.

In der Biologie konnten sich aristotelische Auffassungen bis ins 18. Jahrhundert halten. Sie erwiesen sich teilweise als fruchtbar. So ging William Harvey bei der Entdeckung des Blutkreislaufs von dem Prinzip des Aristoteles aus, dass die Natur nichts Unnötiges hervorbringt, und wendete es auf die Beschaffenheit der Blutgefäße und Herzkammern an. Charles Darwin bezeichnete 1879 Aristoteles als „einen der größten Beobachter (wenn nicht den größten), die jemals gelebt haben“.

Sehr stark und anhaltend war die Nachwirkung von Aristoteles’ "Poetik", insbesondere seiner Tragödientheorie (→ Regeldrama). Sie prägte Theorie und Praxis des Theaters während der gesamten Frühen Neuzeit, abgesehen von manchen gewichtigen Ausnahmen besonders in Spanien und England (Shakespeare). Die "Poetik" lag seit 1278 in lateinischer Übersetzung vor, 1498 und 1536 erschienen humanistische Übersetzungen. Auf ihr fußte die "Poetik" des Julius Caesar Scaliger (1561), die Dichtungslehre von Martin Opitz (1624), die französische Theaterlehre des 17. Jahrhunderts "(doctrine classique)" und schließlich die von Johann Christoph Gottsched geforderte Regelkunst ("Critische Dichtkunst," 1730).

Im 19. Jahrhundert setzte insbesondere in Deutschland die intensive philologische Auseinandersetzung mit dem Werk des Aristoteles ein. 1831 erschien die von der Preußischen Akademie der Wissenschaften in Auftrag gegebene und durch Immanuel Bekker besorgte Gesamtausgabe. Hermann Bonitz verfasste zahlreiche Übersetzungen und den noch heute maßgeblichen "Index Aristotelicus." Ende des 19. Jahrhunderts wurde unter der Leitung von Hermann Diels ebenfalls in der in Berlin ansässigen Akademie die 15.000 Seiten umfassende Ausgabe der antiken griechischen Aristoteles-Kommentare "(Commentaria in Aristotelem Graeca)" veröffentlicht.

Infolge der intensiven philologischen Auseinandersetzung wurde Anfang des 20. Jahrhunderts das lange vorherrschende Bild, das Corpus Aristotelicum sei ein als Ganzes komponiertes philosophisches System, vor allem von Werner Jaeger revidiert. Die moderne Aristotelesforschung wurde in der ersten Hälfte des 20. Jahrhunderts neben Jaeger vor allem von W. D. Ross in Oxford bestimmt; zahlreiche Schüler sorgten für eine zunehmende Beschäftigung mit Aristoteles nicht nur in den philologischen, sondern auch den philosophischen Abteilungen angelsächsischer Universitäten, die bis heute anhält.

Heideggers Seinsanalyse der Fundamentalontologie geschah in intensiver Auseinandersetzung mit Aristoteles, was auch für Schüler wie Hans Georg Gadamer gilt. Den größten Einfluss hatte Aristoteles im 20. Jahrhundert in der Ethik (Tugendethik) und der politischen Philosophie (in Deutschland insbesondere in der Schule um Joachim Ritter, im angelsächsischen Raum im Kommunitarismus). In der zweiten Hälfte des 20. Jahrhunderts griff die zuvor metaphysikkritische analytische Philosophie Aristoteles’ Substanztheorie explizit (etwa David Wiggins: "Sameness and Substance", die Vier-Kategorien-Ontologie von Jonathan Lowe oder die Ontologie von Barry Smith) oder seinen Essentialismus implizit auf (z. B. Kripke).

Nach ihm ist der Mondkrater Aristoteles benannt.


Sammlungen

Einzelausgaben

Biographie

Einführungen

Gesamtdarstellungen

Kompendien

Hilfsmittel

Übersichts- und Gesamtdarstellungen

Epochenübergreifende Untersuchungen zu einzelnen Themen

Antike

Mittelalter

Neuzeit

Über Aristoteles


Texte von Aristoteles



</doc>

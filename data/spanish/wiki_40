<doc id="8165" url="https://es.wikipedia.org/wiki?curid=8165" title="Barroco">
Barroco

El Barroco fue un período de la historia en la cultura occidental originado por una nueva forma de concebir el arte (el «estilo barroco») y que, partiendo desde diferentes contextos histórico-culturales, produjo obras en numerosos campos artísticos: literatura, arquitectura, escultura, pintura, música, ópera, danza, teatro, etc. Se manifestó principalmente en la Europa occidental, aunque debido al colonialismo también se dio en numerosas colonias de las potencias europeas, principalmente en Latinoamérica. Cronológicamente, abarcó todo el siglo XVII y principios del XVIII, con mayor o menor prolongación en el tiempo dependiendo de cada país. Se suele situar entre el Manierismo y el Rococó, en una época caracterizada por fuertes disputas religiosas entre países católicos y protestantes, así como marcadas diferencias políticas entre los Estados absolutistas y los parlamentarios, donde una incipiente burguesía empezaba a poner los cimientos del capitalismo.

Como estilo artístico, el Barroco surgió a principios del siglo XVII (según otros autores a finales del XVI) en Italia —período también conocido en este país como "Seicento"—, desde donde se extendió hacia la mayor parte de Europa. Durante mucho tiempo (siglos XVIII y XIX) el término «barroco» tuvo un sentido peyorativo, con el significado de recargado, engañoso, caprichoso, hasta que fue posteriormente revalorizado a finales del siglo XIX por Jacob Burckhardt y, en el XX, por Benedetto Croce y Eugenio d'Ors. Algunos historiadores dividen el Barroco en tres períodos: «primitivo» (1580-1630), «maduro» o «pleno» (1630-1680) y «tardío» (1680-1750).

Aunque se suele entender como un período artístico específico, estéticamente el término «barroco» también indica cualquier estilo artístico contrapuesto al clasicismo, concepto introducido por Heinrich Wölfflin en 1915. Así pues, el término «barroco» se puede emplear tanto como sustantivo como adjetivo. Según este planteamiento, cualquier estilo artístico atraviesa por tres fases: arcaica, clásica y barroca. Ejemplos de fases barrocas serían el arte helenístico, el arte gótico, el romanticismo o el modernismo.

El arte se volvió más refinado y ornamentado, con pervivencia de un cierto racionalismo clasicista pero adoptando formas más dinámicas y efectistas y un gusto por lo sorprendente y anecdótico, por las ilusiones ópticas y los golpes de efecto. Se observa una preponderancia de la representación realista: en una época de penuria económica, el hombre se enfrenta de forma más cruda a la realidad. Por otro lado, a menudo esta cruda realidad se somete a la mentalidad de una época turbada y desengañada, lo que se manifiesta en una cierta distorsión de las formas, en efectos forzados y violentos, fuertes contrastes de luces y sombras y cierta tendencia al desequilibrio y la exageración.

Se conoce también con el nombre de barroquismo el abuso de lo ornamental, el recargamiento en el arte.

El término «barroco» proviene de un vocablo de origen portugués ("barrôco"), cuyo femenino denominaba a las perlas que tenían alguna deformidad (como en castellano el vocablo «barruecas»). Fue en origen una palabra despectiva que designaba un tipo de arte caprichoso, grandilocuente, excesivamente recargado. Así apareció por vez primera en el "Dictionnaire de Trévoux" (1771), que define «en pintura, un cuadro o una figura de gusto barroco, donde las reglas y las proporciones no son respetadas y todo está representado siguiendo el capricho del artista».

Otra teoría lo deriva del sustantivo "baroco", un silogismo de origen aristotélico proveniente de la filosofía escolástica medieval, que señala una ambigüedad que, basada en un débil contenido lógico, hace confundir lo verdadero con lo falso. Así, esta figura señala un tipo de razonamiento pedante y artificioso, generalmente en tono sarcástico y no exento de polémica. En ese sentido lo aplicó Francesco Milizia en su "Dizionario delle belle arti del disegno" (1797), donde expresa que «barroco es el superlativo de bizarro, el exceso del ridículo».

El término «barroco» fue usado a partir del siglo XVIII con un sentido despectivo, para subrayar el exceso de énfasis y abundancia de ornamentación, a diferencia de la racionalidad más clara y sobria de la Ilustración. En ese tiempo, barroco era sinónimo de otros adjetivos como «absurdo» o «grotesco». Los pensadores ilustrados vieron en las realizaciones artísticas del siglo anterior una manipulación de los preceptos clasicistas, tan cercanos a su concepto racionalista de la realidad, por lo que sus críticas al arte seiscentista convirtieron el término «barroco» en un concepto peyorativo: en su "Dictionnaire d'Architecture" (1792), Antoine Chrysostome Quatremère de Quincy define lo barroco como «un matiz de lo extravagante. Es, si se quiere, su refinamiento o si se pudiese decir, su abuso. Lo que la severidad es a la sabiduría del gusto, el barroco lo es a lo extraño, es decir, que es su superlativo. La idea de barroco entraña la del ridículo llevado al exceso».

Sin embargo, la historiografía del arte tendió posteriormente a revalorizar el concepto de lo barroco y a valorarlo por sus cualidades intrínsecas, al tiempo que empezó a tratar el Barroco como un período específico de la historia de la cultura occidental. El primero en rechazar la acepción negativa del Barroco fue Jacob Burckhardt ("Cicerone", 1855), afirmando que «la arquitectura barroca habla el mismo lenguaje del Renacimiento, pero en un dialecto degenerado». Si bien no era una afirmación elogiosa, abrió el camino a estudios más objetivos, como los elaborados por Cornelius Gurlitt ("Geschichte des Barockstils in Italien", 1887), August Schmarsow ("Barock und Rokoko", 1897), Alois Riegl ("Die Entstehung der Barockkunst in Rom", 1908) y Wilhelm Pinder ("Deutscher Barock", 1912), que culminaron en la obra de Heinrich Wölfflin ("Renaissance und Barock", 1888; "Kunstgeschichtliche Grundbegriffe", 1915), el primero que otorgó al Barroco una autonomía estilística propia y diferenciada, señalando sus propiedades y rasgos estilísticos de una forma revalorizada. Posteriormente, Benedetto Croce ("Saggi sulla letteratura italiana del Seicento", 1911) efectuó un estudio historicista del Barroco, enmarcándolo en su contexto socio-histórico y cultural, y procurando no emitir ninguna clase de juicios de valor. Sin embargo, en "Storia dell'età barocca in Italia" (1929) volvió a otorgar un carácter negativo al Barroco, al que calificó de «decadente», justo en una época en que surgieron numerosos tratados que reivindicaban la valía artística del período, como "Der Barock als Kunst der Gegenreformation" (1921), de Werner Weisbach, "Österreichische Barockarchitektur" (1930) de Hans Sedlmayr o "Art religieux après le Concile de Trente" (1932), de Émile Mâle.

Posteriores estudios han dejado definitivamente asentado el concepto actual de Barroco, con pequeñas salvedades, como la diferenciación efectuada por algunos historiadores entre «barroco» y «barroquismo», siendo el primero la fase clásica, pura y primigenia, del arte del siglo XVII, y el segundo una fase amanerada, recargada y exagerada, que confluiría con el Rococó —en la misma medida que el manierismo sería la fase amanerada del Renacimiento—. En ese sentido, Wilhelm Pinder ("Das Problem der Generation in der Kunstgeschichte", 1926) sostiene que estos estilos «generacionales» se suceden sobre la base de la formulación y posterior deformación de unos determinados ideales culturales: así como el manierismo jugó con las formas clásicas de un Renacimiento de corte humanista y clasicista, el barroquismo supone la reformulación en clave formalista del sustrato ideológico barroco, basado principalmente en el absolutismo y el contrarreformismo.

Por otro lado, frente al Barroco como un determinado período de la historia de la cultura, a principios del siglo XX surgió una segunda acepción, la de «lo barroco» como una fase presente en la evolución de todos los estilos artísticos. Ya Nietzsche aseveró que «el estilo barroco surge cada vez que muere un gran arte». El primero en otorgar un sentido estético transhistórico al Barroco fue Heinrich Wölfflin ("Kunstgeschichtliche Grundbegriffe", 1915), quien estableció un principio general de alternancia entre clasicismo y barroco, que rige la evolución de los estilos artísticos.

Recogió el testigo Eugenio d'Ors ("Lo barroco", 1936), que lo definió como un «eón», una forma transhistórica del arte («"lo" barroco» frente a «"el" barroco» como período), una modalidad recurrente a todo lo largo de la historia del arte como oposición a lo clásico. Si el clasicismo es un arte racional, masculino, apolíneo, lo barroco es irracional, femenino, dionisíaco. Para d'Ors, «ambas aspiraciones [clasicismo y barroquismo] se complementan. Tiene lugar un estilo de economía y razón, y otro musical y abundante. Uno se siente atraído por las formas estables y pesadas, y el otro por las redondeadas y ascendentes. De uno a otro no hay ni decadencia ni degeneración. Se trata de dos formas de sensibilidad eternas».

El siglo XVII fue por lo general una época de depresión económica, consecuencia de la prolongada expansión del siglo anterior causada principalmente por el descubrimiento de América. Las malas cosechas conllevaron el aumento del precio del trigo y demás productos básicos, con las subsiguientes hambrunas. El comercio se estancó, especialmente en el área mediterránea, y solo floreció en Inglaterra y Países Bajos gracias al comercio con Oriente y la creación de grandes compañías comerciales, que sentaron las bases del capitalismo y el auge de la burguesía. La mala situación económica se agravó con las plagas de peste que asolaron Europa a mediados del siglo XVII, que afectaron especialmente a la zona mediterránea. Otro factor que generó miseria y pobreza fueron las guerras, provocadas en su mayoría por el enfrentamiento entre católicos y protestantes, como es el caso de la Guerra de los Treinta Años (1618-1648). Todos estos factores provocaron una grave depauperación de la población; en muchos países, el número de pobres y mendigos llegó a alcanzar la cuarta parte de la población.

Por otro lado, el poder hegemónico en Europa basculó de la España imperial a la Francia absolutista, que tras la Paz de Westfalia (1648) y la Paz de los Pirineos (1659) se consolidó como el más poderoso estado del continente, prácticamente indiscutido hasta la ascensión de Inglaterra en el siglo XVIII. Así, la Francia de los Luises y la Roma papal fueron los principales núcleos de la cultura barroca, como centros de poder político y religioso —respectivamente— y centros difusores del absolutismo y el contrarreformismo. España, aunque en decadencia política y económica, tuvo sin embargo un esplendoroso período cultural —el llamado Siglo de Oro— que, aunque marcado por su aspecto religioso de incontrovertible proselitismo contrarreformista, tuvo un acentuado componente popular, y llevó tanto a la literatura como a las artes plásticas a cotas de elevada calidad. En el resto de países donde llegó la cultura barroca (Inglaterra, Alemania, Países Bajos), su implantación fue irregular y con distintos sellos peculiarizados por sus distintivas características nacionales.
El Barroco se forjó en Italia, principalmente en la sede pontificia, Roma, donde el arte fue utilizado como medio propagandístico para la difusión de la doctrina contrarreformista. La Reforma protestante sumió a la Iglesia Católica en una profunda crisis durante la primera mitad del siglo XVI, que evidenció tanto la corrupción en numerosos estratos eclesiásticos como la necesidad de una renovación del mensaje y la obra católica, así como de un mayor acercamiento a los fieles. El Concilio de Trento (1545-1563) se celebró para contrarrestar el avance del protestantismo y consolidar el culto católico en los países donde aún prevalecía, sentando las bases del dogma católico (sacerdocio sacramental, celibato, culto a la Virgen y los santos, uso litúrgico del latín) y creando nuevos instrumentos de comunicación y expansión de la fe católica, poniendo especial énfasis en la educación, la predicación y la difusión del mensaje católico, que adquirió un fuerte sello propagandístico —para lo que se creó la Congregación para la Propagación de la Fe—. Este ideario se plasmó en la recién fundada Compañía de Jesús, que mediante la predicación y la enseñanza tuvo una notable y rápida difusión por todo el mundo, frenando el avance del protestantismo y recuperando numerosos territorios para la fe católica (Austria, Baviera, Suiza, Flandes, Polonia). Otro efecto de la Contrarreforma fue la consolidación de la figura del papa, cuyo poder salió reforzado, y que se tradujo en un ambicioso programa de ampliación y renovación urbanística de Roma, especialmente de sus iglesias, con especial énfasis en la Basílica de San Pedro y sus aledaños. La Iglesia fue el mayor comitente artístico de la época, y utilizó el arte como caballo de batalla de la propaganda religiosa, al ser un medio de carácter popular fácilmente accesible e inteligible. El arte fue utilizado como un vehículo de expresión "ad maiorem Dei et Ecclesiae gloriam", y papas como Sixto V, Clemente VIII, Paulo V, Gregorio XV, Urbano VIII, Inocencio X y Alejandro VII se convirtieron en grandes mecenas y propiciaron grandes mejoras y construcciones en la ciudad eterna, ya calificada entonces como "Roma triumphans, caput mundi" («Roma triunfante, cabeza del mundo»).

Culturalmente, el Barroco fue una época de grandes adelantos científicos: William Harvey comprobó la circulación de la sangre; Galileo Galilei perfeccionó el telescopio y afianzó la teoría heliocéntrica establecida el siglo anterior por Copérnico y Kepler; Isaac Newton formuló la teoría de la gravitación universal; Evangelista Torricelli inventó el barómetro. Francis Bacon estableció con su "Novum Organum" el método experimental como base de la investigación científica, poniendo las bases del empirismo. Por su parte, René Descartes llevó a la filosofía hacia el racionalismo, con su famoso «pienso, luego existo».
Debido a las nuevas teorías heliocéntricas y la consecuente pérdida del sentimiento antropocéntrico propio del hombre renacentista, el hombre del Barroco perdió la fe en el orden y la razón, en la armonía y la proporción; la naturaleza, no reglamentada ni ordenada, sino libre y voluble, misteriosa e inabarcable, pasó a ser una fuente directa de inspiración más conveniente a la mentalidad barroca. Perdiendo la fe en la verdad, todo pasa a ser aparente e ilusorio (Calderón: "La vida es sueño"); ya no hay nada revelado, por lo que todo debe investigarse y experimentarse. Descartes convirtió la duda en el punto de partida de su sistema filosófico: «considerando que todos los pensamientos que nos vienen estando despiertos pueden también ocurrírsenos durante el sueño, sin que ninguno entonces sea verdadero, resolví fingir que todas las cosas que hasta entonces habían entrado en mi espíritu, no eran más verdaderas que las ilusiones de mis sueños» ("Discurso del método", 1637). Así, mientras la ciencia se circunscribía a la búsqueda de la verdad, el arte se encaminaba a la expresión de lo imaginario, del ansia de infinito que anhelaba el hombre barroco. De ahí el gusto por los efectos ópticos y los juegos ilusorios, por las construcciones efímeras y el valor de lo transitorio; o el gusto por lo sugestivo y seductor en poesía, por lo maravilloso, sensual y evocador, por los efectos lingüísticos y sintácticos, por la fuerza de la imagen y el poder de la retórica, revitalizados por la reivindicación de autores como Aristóteles o Cicerón.

La cultura barroca era, en definición de José Antonio Maravall, «dirigida» —enfocada en la comunicación—, «masiva» —de carácter popular— y «conservadora» —para mantener el orden establecido—. Cualquier medio de expresión artístico debía ser principalmente didáctico y seductor, debía llegar fácilmente al público y debía entusiasmarle, hacerle comulgar con el mensaje que transmitía, un mensaje puesto al servicio de las instancias del poder —político o religioso—, que era el que sufragaba los costes de producción de las obras artísticas, ya que Iglesia y aristocracia —también incipientemente la burguesía— eran los principales comitentes de artistas y escritores. Si la Iglesia quería transmitir su mensaje contrarreformista, las monarquías absolutas vieron en el arte una forma de magnificar su imagen y mostrar su poder, a través de obras monumentales y pomposas que transmitían una imagen de grandeza y ayudaban a consolidar el poder centralista del monarca, reafirmando su autoridad.

Por ello y pese a la crisis económica, el arte floreció gracias sobre todo al mecenazgo eclesiástico y aristocrático. Las cortes de los estados monárquicos —especialmente los absolutistas— favorecieron el arte como una forma de plasmar la magnificencia de sus reinos, un instrumento propagandístico que daba fe de la grandiosidad del monarca (un ejemplo paradigmático es la construcción de Versalles por Luis XIV). El auge del coleccionismo, que conllevaba la circulación de artistas y obras de arte por todo el continente europeo, condujo al alza del mercado artístico. Algunos de los principales coleccionistas de arte de la época fueron monarcas, como el emperador Rodolfo II, Carlos I de Inglaterra, Felipe IV de España o la reina Cristina de Suecia. Floreció notablemente el mercado artístico, centrado principalmente en el ámbito holandés (Amberes y Ámsterdam) y alemán (Núremberg y Augsburgo). También proliferaron las academias de arte —siguiendo la estela de las surgidas en Italia en el siglo XVI—, como instituciones encargadas de preservar el arte como fenómeno cultural, de reglamentar su estudio y su conservación, y de promocionarlo mediante exposiciones y concursos; las principales academias surgidas en el siglo XVII fueron la Académie Royale d'Art, fundada en París en 1648, y la Akademie der Künste de Berlín (1696)

El Barroco fue un estilo heredero del escepticismo manierista, que se vio reflejado en un sentimiento de fatalidad y dramatismo entre los autores de la época. El arte se volvió más artificial, más recargado, decorativo, ornamentado. Destacó el uso ilusionista de los efectos ópticos; la belleza buscó nuevas vías de expresión y cobró relevancia lo asombroso y los efectos sorprendentes. Surgieron nuevos conceptos estéticos como los de «ingenio», «perspicacia» o «agudeza». En la conducta personal se destacaba sobre todo el aspecto exterior, de forma que reflejara una actitud altiva, elegante, refinada y exagerada que cobró el nombre de "préciosité".

Según Wölfflin, el Barroco se define principalmente por oposición al Renacimiento: frente a la visión lineal renacentista, la visión barroca es pictórica; frente a la composición en planos, la basada en la profundidad; frente a la forma cerrada, la abierta; frente a la unidad compositiva basada en la armonía, la subordinación a un motivo principal; frente a la claridad absoluta del objeto, la claridad relativa del efecto. Así, el Barroco «es el estilo del punto de vista pictórico con perspectiva y profundidad, que somete la multiplicidad de sus elementos a una idea central, con una visión sin límites y una relativa oscuridad que evita los detalles y los perfiles agudos, siendo al mismo tiempo un estilo que, en lugar de revelar su arte, lo esconde».

El arte barroco se expresó estilísticamente en dos vías: por un lado, hay un énfasis en la realidad, el aspecto mundano de la vida, la cotidianeidad y el carácter efímero de la vida, que se materializó en una cierta «vulgarización» del fenómeno religioso en los países católicos, así como en un mayor gusto por las cualidades sensibles del mundo circundante en los protestantes; por otro lado, se manifiesta una visión grandilocuente y exaltada de los conceptos nacionales y religiosos como una expresión del poder, que se traduce en el gusto por lo monumental, lo fastuoso y recargado, el carácter magnificente otorgado a la realeza y la Iglesia, a menudo con un fuerte sello propagandístico.
El Barroco fue una cultura de la imagen, donde todas las artes confluyeron para crear una obra de arte total, con una estética teatral, escenográfica, una "mise en scène" que pone de manifiesto el esplendor del poder dominante (Iglesia o Estado), con ciertos toques naturalistas pero en un conjunto que expresa dinamismo y vitalidad. La interacción de todas las artes expresa la utilización del lenguaje visual como un medio de comunicación de masas, plasmado en una concepción dinámica de la naturaleza y el espacio envolvente.

Una de las principales características del arte barroco es su carácter ilusorio y artificioso: «el ingenio y el diseño son el arte mágico a través del cual se llega a engañar a la vista hasta asombrar» (Gian Lorenzo Bernini). Se valoraba especialmente lo visual y efímero, por lo que cobraron auge el teatro y los diversos géneros de artes escénicas y espectáculos: danza, pantomima, drama musical (oratorio y melodrama), espectáculos de marionetas, acrobáticos, circenses, etc. Existía el sentimiento de que el mundo es un teatro ("theatrum mundi") y la vida una función teatral: «todo el mundo es un escenario, y todos los hombres y mujeres meros actores» ("Como gustéis", William Shakespeare, 1599). De igual manera se tendía a teatralizar las demás artes, especialmente la arquitectura. Es un arte que se basa en la inversión de la realidad: en la «simulación», en convertir lo falso en verdadero, y en la «disimulación», pasar lo verdadero por falso. No se muestran las cosas como son, sino como se querría que fuesen, especialmente en el mundo católico, donde la Contrarreforma tuvo un éxito exiguo, ya que media Europa se pasó al protestantismo. En literatura se manifestó dando rienda suelta al artificio retórico, como un medio de expresión propagandístico en que la suntuosidad del lenguaje pretendía reflejar la realidad de forma edulcorada, recurriendo a figuras retóricas como la metáfora, la paradoja, la hipérbole, la antítesis, el hipérbaton, la elipsis, etc. Esta transposición de la realidad, que se ve distorsionada y magnificada, alterada en sus proporciones y sometida al criterio subjetivo de la ficción, pasó igualmente al terreno de la pintura, donde se abusa del escorzo y la perspectiva ilusionista en aras de efectos mayores, llamativos y sorprendentes.
El arte barroco buscaba la creación de una realidad alternativa a través de la ficción y la ilusión. Esta tendencia tuvo su máxima expresión en la fiesta y la celebración lúdica; edificios como iglesias o palacios, o bien un barrio o una ciudad entera, se convertían en teatros de la vida, en escenarios donde se mezclaba la realidad y la ilusión, donde los sentidos se sometían al engaño y el artificio. En ese aspecto tuvo especial protagonismo la Iglesia contrarreformista, que buscaba a través de la pompa y el boato mostrar su superioridad sobre las iglesias protestantes, con actos como misas solemnes, canonizaciones, jubileos, procesiones o investiduras papales. Pero igual de fastuosas eran las celebraciones de la monarquía y la aristocracia, con eventos como coronaciones, bodas y nacimientos reales, funerales, visitas de embajadores o cualquier acontecimiento que permitiese al monarca desplegar su poder para admirar al pueblo. Las fiestas barrocas suponían una conjugación de todas las artes, desde la arquitectura y las artes plásticas hasta la poesía, la música, la danza, el teatro, la pirotecnia, arreglos florales, juegos de agua, etc. Arquitectos como Bernini o Pietro da Cortona, o Alonso Cano y Sebastián Herrera Barnuevo en España, aportaron su talento a tales eventos, diseñando estructuras, coreografías, iluminaciones y demás elementos, que a menudo les servían como campo de pruebas para futuras realizaciones más serias: así, el baldaquino para la canonización de Santa Isabel de Portugal sirvió a Bernini para su futuro diseño del baldaquino de San Pedro, y el "quarantore" (teatro sacro de los jesuitas) de Carlo Rainaldi fue una maqueta de la iglesia de Santa Maria in Campitelli.

Durante el Barroco, el carácter ornamental, artificioso y recargado del arte de este tiempo traslucía un sentido vital transitorio, relacionado con el "memento mori", el valor efímero de las riquezas frente a la inevitabilidad de la muerte, en paralelo al género pictórico de las "vanitas". Este sentimiento llevó a valorar de forma vitalista la fugacidad del instante, a disfrutar de los leves momentos de esparcimiento que otorga la vida, o de las celebraciones y actos solemnes. Así, los nacimientos, bodas, defunciones, actos religiosos, o las coronaciones reales y demás actos lúdicos o ceremoniales, se revestían de una pompa y una artificiosidad de carácter escenográfico, donde se elaboraban grandes montajes que aglutinaban arquitectura y decorados para proporcionar una magnificencia elocuente a cualquier celebración, que se convertía en un espectáculo de carácter casi catártico, donde cobraba especial relevancia el elemento ilusorio, la atenuación de la frontera entre realidad y fantasía.

Cabe destacar que el Barroco es un concepto heterogéneo que no presentó una unidad estilística ni geográfica ni cronológicamente, sino que en su seno se encuentran diversas tendencias estilísticas, principalmente en el terreno de la pintura. Las principales serían: naturalismo, estilo basado en la observación de la naturaleza pero sometida a ciertas directrices establecidas por el artista, basadas en criterios morales y estéticos o, simplemente, derivados de la libre interpretación del artista a la hora de concebir su obra; realismo, tendencia surgida de la estricta imitación de la naturaleza, ni interpretada ni edulcorada, sino representada minuciosamente hasta en sus más pequeños detalles; clasicismo, corriente centrada en la idealización y perfección de la naturaleza, evocadora de elevados sentimientos y profundas reflexiones, con la aspiración de reflejar la belleza en toda su plenitud.

Por último, cabe señalar que en el Barroco surgieron o se desarrollaron nuevos géneros pictóricos. Si hasta entonces había preponderado en el arte la representación de temas históricos, mitológicos o religiosos, los profundos cambios sociales vividos en el siglo XVII propiciaron el interés por nuevos temas, especialmente en los países protestantes, cuya severa moralidad impedía la representación de imágenes religiosas por considerarlas idolatría. Por otro lado, el auge de la burguesía, que para remarcar su estatus invirtió de forma decidida en el arte, trajo consigo la representación de nuevos temas alejados de las grandilocuentes escenas preferidas por la aristocracia. Entre los géneros desarrollados profusamente en el Barroco destacan: la pintura de género, que toma sus modelos de la realidad circundante, de la vida diaria, de temas campesinos o urbanos, de pobres y mendigos, comerciantes y artesanos, o de fiestas y ambientes folklóricos; el paisaje, que eleva a categoría independiente la representación de la naturaleza, que hasta entonces solo servía de telón de fondo de las escenas con personajes históricos o religiosos; el retrato, que centra su representación en la figura humana, generalmente con un componente realista aunque a veces no exento de idealización; el bodegón o naturaleza muerta, que consiste en la representación de objetos inanimados, ya sean piezas de ajuar doméstico, flores, frutas u otros alimentos, muebles, instrumentos musicales, etc.; y la "vanitas", un tipo de bodegón que alude a lo efímero de la existencia humana, simbolizado generalmente por la presencia de calaveras o esqueletos, o bien velas o relojes de arena.

La arquitectura barroca asumió unas formas más dinámicas, con una exuberante decoración y un sentido escenográfico de las formas y los volúmenes. Cobró relevancia la modulación del espacio, con preferencia por las curvas cóncavas y convexas, poniendo especial atención en los juegos ópticos ("trompe-l'œil") y el punto de vista del espectador. También cobró una gran importancia el urbanismo, debido a los monumentales programas desarrollados por reyes y papas, con un concepto integrador de la arquitectura y el paisaje que buscaba la recreación de un "continuum" espacial, de la expansión de las formas hacia el infinito, como expresión de unos elevados ideales, sean políticos o religiosos.

Al igual que en la época anterior, el motor del nuevo estilo volvió a ser Italia, gracias principalmente a la comitencia de la Iglesia y a los grandes programas arquitectónicos y urbanísticos desarrollados por la sede pontificia, deseosa de mostrar al mundo su victoria contra la Reforma. La principal modalidad constructiva de la arquitectura barroca italiana fue la iglesia, que se convirtió en el máximo exponente de la propaganda contrarreformista. Las iglesias barrocas italianas se caracterizan por la abundancia de formas dinámicas, con predominio de las curvas cóncavas y convexas, con fachadas ricamente decoradas y repletas de esculturas, así como gran número de columnas, que a menudo se desprenden del muro, y con interiores donde predominan igualmente la forma curva y una profusa decoración. Entre sus diversas planimetrías destacó —especialmente entre finales del siglo XVI y principios del XVII— el diseño en dos cuerpos, con dos frontones concéntricos (curvo el exterior y triangular el interior), siguiendo el modelo de la fachada de la Iglesia del Gesù de Giacomo della Porta (1572).

Uno de sus primeros representantes fue Carlo Maderno, autor de la fachada de San Pedro del Vaticano (1607-1612) —al que además modificó la planta, pasando de la de cruz griega proyectada por Bramante a una de cruz latina—, y la Iglesia de Santa Susana (1597-1603). Pero uno de los mayores impulsores del nuevo estilo fue el arquitecto y escultor Gian Lorenzo Bernini, el principal artífice de la Roma monumental que conocemos hoy día: baldaquino de San Pedro (1624-1633) —donde aparece la columna salomónica, posteriormente uno de los signos distintivos del Barroco—, columnata de la Plaza de San Pedro (1656-1667), San Andrés del Quirinal (1658-1670), Palacio Chigi-Odescalchi (1664-1667). El otro gran nombre de la época es Francesco Borromini, arquitecto de gran inventiva que subvirtió todas las normas de la arquitectura clásica —a las que pese a todo aún se aferraba Bernini—, a través del uso de superficies alabeadas, bóvedas nervadas y arcos mixtilíneos, creando una arquitectura de carácter casi escultórico. Fue autor de las iglesias de San Carlo alle Quattre Fontane (1634-1640), Sant'Ivo alla Sapienza (1642-1650) y Sant'Agnese in Agone (1653-1661). El tercer arquitecto de renombre activo en Roma fue Pietro da Cortona, que también era pintor, circunstancia quizá por la cual creó volúmenes de gran plasticidad, con grandes contrastes de luz y sombra (Santa Maria della Pace, 1656-1657; Santi Luca e Martina, 1635-1650). Fuera de Roma cabe destacar la figura de Baldassare Longhena en Venecia, autor de la Iglesia de Santa Maria della Salute (1631-1650); y Guarino Guarini y Filippo Juvara en Turín, autor de la Capilla del Santo Sudario (1667-1690) el primero, y de la Basílica de Superga (1717-1731) el segundo.

En Francia, bajo los reinados de Luis XIII y Luis XIV, se iniciaron una serie de construcciones de gran fastuosidad, que pretendían mostrar la grandeza del monarca y el carácter sublime y divino de la monarquía absolutista. Aunque en la arquitectura francesa se percibe cierta influencia de la italiana, esta fue reinterpretada de una forma más sobria y equilibrada, más fiel al clasicismo renacentista, por lo que el arte francés de la época se suele denominar como clasicismo francés.

Las primeras realizaciones de relevancia corrieron a cargo de Jacques Lemercier (Iglesia de la Sorbona, 1635) y François Mansart (Palacio de Maisons-Lafitte, 1624-1626; Iglesia de Val-de-Grâce, 1645-1667). Posteriormente, los grandes programas áulicos se centraron en la nueva fachada del Palacio del Louvre, de Louis Le Vau y Claude Perrault (1667-1670) y, especialmente, en el Palacio de Versalles, de Le Vau y Jules Hardouin-Mansart (1669-1685). De este último arquitecto conviene también destacar la Iglesia de San Luis de los Inválidos (1678-1691), así como el trazado de la Plaza Vendôme de París (1685-1708).

En España, la arquitectura de la primera mitad del siglo XVII acusó la herencia herreriana, con una austeridad y simplicidad geométrica de influencia escurialense. Lo barroco se fue introduciendo paulatinamente sobre todo en la recargada decoración interior de iglesias y palacios, donde los retablos fueron evolucionando hacia cotas de cada vez más elevada magnificencia. En este período fue Juan Gómez de Mora la figura más destacada, siendo autor de la Clerecía de Salamanca (1617), el Ayuntamiento (1644-1702) y la Plaza Mayor de Madrid (1617-1619). Otros autores de la época fueron: Alonso Carbonel, autor del Palacio del Buen Retiro (1630-1640); Pedro Sánchez y Francisco Bautista, autores de la Colegiata de San Isidro de Madrid (1620-1664).

Hacia mediados de siglo fueron ganando terreno las formas más ricas y los volúmenes más libres y dinámicos, con decoraciones naturalistas (guirnaldas, cartelas vegetales) o de formas abstractas (molduras y baquetones recortados, generalmente de forma mixtilínea). En esta época conviene recordar los nombres de Pedro de la Torre, José de Villarreal, José del Olmo, Sebastián Herrera Barnuevo y, especialmente, Alonso Cano, autor de la fachada de la Catedral de Granada (1667).

Entre finales de siglo y comienzos del XVIII se dio el estilo churrigueresco (por los hermanos Churriguera), caracterizado por su exuberante decorativismo y el uso de columnas salomónicas: José Benito Churriguera fue autor del Retablo Mayor de San Esteban de Salamanca (1692) y la fachada del palacio-iglesia de Nuevo Baztán en Madrid (1709-1722); Alberto Churriguera proyectó la Plaza Mayor de Salamanca (1728-1735); y Joaquín Churriguera fue autor del Colegio de Calatrava (1717) y el claustro de San Bartolomé (1715) en Salamanca, de influencia plateresca. Otras figuras de la época fueron: Teodoro Ardemans, autor de la fachada del Ayuntamiento de Madrid y el primer proyecto para el Palacio Real de La Granja de San Ildefonso (1718-1726); Pedro de Ribera, autor del Puente de Toledo (1718-1732), el Cuartel del Conde-Duque (1717) y la fachada de la Iglesia de Nuestra Señora de Montserrat de Madrid (1720); Narciso Tomé, autor del Transparente de la Catedral de Toledo (1721-1734); el alemán Konrad Rudolf, autor de la fachada de la Catedral de Valencia (1703); Jaime Bort, artífice de la fachada de la Catedral de Murcia (1736-1753); Vicente Acero, que proyectó la Catedral de Cádiz (1722-1762); y Fernando de Casas Novoa, autor de la fachada del Obradoiro de la Catedral de Santiago de Compostela (1739-1750).

En Alemania, hasta mediados de siglo no se iniciaron construcciones de relevancia, debido a la Guerra de los Treinta Años, y aún entonces las principales obras fueron encargadas a arquitectos italianos. Sin embargo, a finales de siglo hubo una eclosión de arquitectos alemanes de gran valía, que hicieron obras cuyas innovadoras soluciones apuntaban ya al Rococó: Andreas Schlüter, autor del Palacio Real de Berlín (1698-1706), de influencia versallesca; Matthäus Daniel Pöppelmann, autor del palacio Zwinger de Dresde (1711-1722); y Georg Bähr, autor de la Iglesia de Frauenkirche de Dresde (1722-1738). En Austria destacaron Johann Bernhard Fischer von Erlach, autor de la iglesia de San Carlos Borromeo en Viena (1715-1725); Johann Lukas von Hildebrandt, autor del palacio Belvedere de Viena (1713-1723); y Jakob Prandtauer, artífice de la abadía de Melk (1702-1738). En Suiza cabe nombrar la abadía de Einsiedeln (1691-1735), de Kaspar Moosbrugger; la iglesia de los jesuitas de Solothurn (1680), de Heinrich Mayer; y la Colegiata de Sankt Gallen (1721-1770), de Kaspar Moosbrugger, Michael Beer y Peter Thumb.

En Inglaterra pervivió durante buena parte del siglo XVII un clasicismo renacentista de influencia palladiana, cuyo máximo representante fue Inigo Jones. Posteriormente se fueron introduciendo las nuevas formas del continente, aunque reinterpretadas nuevamente con un sentido de mesura y contención pervivientes de la tradición palladiana. En ese sentido la obra maestra del período fue la catedral de San Pablo de Londres (1675-1711), de Christopher Wren. Otras obras de relevancia serían el castillo de Howard (1699-1712) y el palacio de Blenheim (1705-1725), ambos de John Vanbrugh y Nicholas Hawksmoor.

En Flandes, las formas barrocas, presentes en un desbordado decorativismo, convivieron con antiguas estructuras góticas, órdenes clásicos y decoración manierista: cabe destacar las iglesias de Saint-Loup de Namur (1621), Sint-Michiel de Lovaina (1650-1666), Saint-Jean-Baptiste de Bruselas (1657-1677) y Sint-Pieter de Malinas (1670-1709). En los Países Bajos, el calvinismo determinó una arquitectura más simple y austera, de líneas clásicas, con preponderancia de la arquitectura civil: Bolsa de Ámsterdam (1608), de Hendrik de Keyser; Palacio Mauritshuis de La Haya (1633-1644), de Jacob van Campen; Ayuntamiento de Ámsterdam (1648, actual Palacio Real), de Jacob van Campen.

En los países nórdicos, el protestantismo propició igualmente una arquitectura sobria y de corte clásico, con modelos importados de otros países, y características propias tan solo perceptibles en la utilización de diversos materiales, como los muros combinados de ladrillo y piedra de cantería, o los techos de cobre. En Dinamarca destacan el edificio de la Bolsa de Copenhague (1619-1674), de Hans van Steenwinkel el Joven; y la iglesia de Federico V (1754-1894), de Nicolai Eigtved. En Suecia cabe destacar el palacio de Drottningholm (1662-1685) y la iglesia de Riddarholm (1671), de Nicodemus Tessin el Viejo, y el palacio Real de Estocolmo (1697-1728), de Nicodemus Tessin el Joven.

En Portugal, hasta mediados de siglo —con la independencia de España— no se inició una actividad constructora de envergadura, favorecida por el descubrimiento de minas de oro y diamantes en Minas Gerais (Brasil), que llevó al rey Juan V a querer emular las cortes de Versalles y el Vaticano. Entre las principales construcciones destacan: el monasterio de Zafra (1717-1740), de Johann Friedrich Ludwig; el palacio Real de Queluz (1747), de Mateus Vicente; y el Santuario de Bom Jesus do Monte, en Braga (1784-1811), de Manuel Pinto Vilalobos.

En Europa oriental, Praga (Chequia) fue una de las ciudades con un mayor programa constructivo, favorecido por la aristocracia checa: Palacio Czernin (1668-1677), de Francesco Caratti; Palacio Arzobispal (1675-1679), de Jean-Baptiste Mathey; Iglesia de San Nicolás (1703-1717), de Christoph Dientzenhofer; Santuario de la Virgen de Loreto (1721), de Christoph y Kilian Ignaz Dietzenhofer. En Polonia destacan la Catedral de San Juan Bautista de Breslavia (1716-1724), de Fischer von Erlach; el Palacio Krasiński (1677-1682), de Tylman van Gameren; y el Palacio de Wilanów (1692), de Agostino Locci y Andreas Schlüter. En Rusia, donde el zar Pedro I el Grande llevó a cabo un proceso de occidentalización del estado, se recibió la influencia del barroco noreuropeo, cuyo principal exponente fue la Catedral de San Pedro y San Pablo de San Petersburgo (1703-1733), obra del arquitecto italiano Domenico Trezzini. Más tarde, Francesco Bartolomeo Rastrelli fue el exponente de un barroco tardío de infuencia francoitaliana, que ya apuntaba al Rococó: Palacio de Peterhof, llamado «el Versalles ruso» (1714-1764, iniciado por Le Blond); Palacio de Invierno en San Petersburgo (1754-1762); y Palacio de Catalina en Tsárskoye Seló (1752-1756). En Ucrania, el Barroco se distingue del occidental por medio de una ornamentación más moderada y unas formas más simples: Monasterio de las Cuevas de Kiev, Monasterio de San Miguel de Vydubichi en Kiev. En el Imperio Otomano el arte occidental influyó durante el siglo XVIII a las tradicionales formas islámicas, como se denota en la Mezquita de los Tulipanes (1760-1763), obra de Mehmet Tahir Ağa. Otro exponente fue la Mezquita Nuruosmaniye (1748-1755), obra del arquitecto griego Simon el Rum y patrocinada por el sultán Mahmud I, el cual mandó traer planos de iglesias europeas para su construcción.

La arquitectura barroca colonial se caracteriza por una profusa decoración (Portada de La Profesa, México; fachadas revestidas de azulejos del estilo de Puebla, como en San Francisco Acatepec en San Andrés Cholula y San Francisco de Puebla), que resultará exacerbada en el llamado «ultrabarroco» (Fachada del Sagrario de la Catedral de México, de Lorenzo Rodríguez; Iglesia de Tepotzotlán; Templo de Santa Prisca de Taxco). En Perú, las construcciones desarrolladas en Lima y Cuzco desde 1650 muestran unas características originales que se adelantan incluso al Barroco europeo, como en el uso de muros almohadillados y de columnas salomónicas (Iglesia de la Compañía, Cuzco; San Francisco, Lima). En otros países destacan: la Catedral Metropolitana de Sucre en Bolivia; el Santuario del Señor de Esquipulas en Guatemala; la Catedral de Tegucigalpa en Honduras; la Catedral de León en Nicaragua; la Iglesia de la Compañía en Quito, Ecuador; la Iglesia de San Ignacio en Bogotá, Colombia; la Catedral de Caracas en Venezuela; la Audiencia de Buenos Aires en Argentina; la Iglesia de Santo Domingo en Santiago de Chile; y la Catedral de La Habana en Cuba. También conviene recordar la calidad de las iglesias de las misiones jesuitas en Paraguay y de las misiones franciscanas en California.

En Brasil, al igual que en la metrópoli, Portugal, la arquitectura tiene una cierta influencia italiana, generalmente de tipo borrominesco, como se percibe en las iglesias de San Pedro dos Clérigos en Recife (1728) y Nuestra Señora de la Gloria en Outeiro (1733). En la región de Minas Gerais destacó la labor de Aleijadinho, autor de un conjunto de iglesias que destacan por su planimetría curva, fachadas con efectos dinámicos cóncavo-convexos y un tratamiento plástico de todos los elementos arquitectónicos (São Francisco de Assis en Ouro Preto, 1765-1775).

En las colonias portuguesas de la India (Goa, Damao y Diu) floreció un estilo arquitectónico de formas barrocas mezcladas con elementos hindúes, como la Catedral de Goa (1562-1619) y la Basílica del Buen Jesús de Goa (1594-1605), que alberga la tumba de San Francisco Javier. El conjunto de iglesias y conventos de Goa fue declarado Patrimonio de la Humanidad en 1986.

En Filipinas destacan las iglesias barrocas de Filipinas (designadas como Patrimonio de la Humanidad en 1993), con un estilo que es una reinterpretación de la arquitectura barroca europea por los chinos y los artesanos filipinos: Iglesia de San Agustín (Manila), Iglesia de Nuestra Señora de la Asunción (Santa María, Ilocos Sur), Iglesia de San Agustín (Paoay, Ilocos Norte) e Iglesia de Santo Tomás de Villanueva (Ming-ao, Iloílo).

Durante el Barroco la jardinería estuvo muy vinculada a la arquitectura, con diseños racionales donde cobró preferencia el gusto por la forma geométrica. Su paradigma fue el jardín francés, caracterizado por mayores zonas de césped y un nuevo detalle ornamental, el parterre, como en los Jardines de Versalles, diseñados por André Le Nôtre. El gusto barroco por la teatralidad y la artificiosidad conllevó la construcción de diversos elementos accesorios al jardín, como islas y grutas artificiales, teatros al aire libre, "ménageries" de animales exóticos, pérgolas, arcos triunfales, etc. Surgió la "orangerie", una construcción de grandes ventanales destinada a proteger en invierno naranjos y otras plantas de origen meridional. El modelo de Versalles fue copiado por las grandes cortes monárquicas europeas, con exponentes como los jardines de Schönbrunn (Viena), Charlottenburg (Berlín), La Granja (Segovia) y Petrodvorets (San Petersburgo).

La escultura barroca adquirió el mismo carácter dinámico, sinuoso, expresivo, ornamental, que la arquitectura —con la que llegará a una perfecta simbiosis sobre todo en edificios religiosos—, destacando el movimiento y la expresión, partiendo de una base naturalista pero deformada a capricho del artista. La evolución de la escultura no fue uniforme en todos los países, ya que en ámbitos como España y Alemania, donde el arte gótico había tenido mucho asentamiento —especialmente en la imaginería religiosa—, aún pervivían ciertas formas estilísticas de la tradición local, mientras que en países donde el Renacimiento había supuesto la implantación de las formas clásicas (Italia y Francia) la perduración de estas es más acentuada. Por temática, junto a la religiosa tuvo bastante importancia la mitológica, sobre todo en palacios, fuentes y jardines.

En Italia destacó nuevamente Gian Lorenzo Bernini, escultor de formación aunque trabajase como arquitecto por encargo de varios papas. Influido por la escultura helenística —que en Roma podía estudiar a la perfección gracias a las colecciones arqueológicas papales—, logró una gran maestría en la expresión del movimiento, en la fijación de la acción parada en el tiempo. Fue autor de obras tan relevantes como "Eneas, Anquises y Ascanio huyendo de Troya" (1618-1619), "El rapto de Proserpina" (1621-1622), "Apolo y Dafne" (1622-1625), "David lanzando su honda" (1623-1624), el Sepulcro de Urbano VIII (1628-1647), "Éxtasis de Santa Teresa" (1644-1652), la Fuente de los Cuatro Ríos en Piazza Navona (1648-1651) y "Muerte de la beata Ludovica Albertoni" (1671-1674). Otros escultores de la época fueron: Stefano Maderno, a caballo entre el Manierismo y el Barroco ("Santa Cecilia", 1600); François Duquesnoy, flamenco de nacimiento pero activo en Roma ("San Andrés", 1629-1633); Alessandro Algardi, formado en la escuela boloñesa, de corte clásico ("Decapitación de San Pablo", 1641-1647; "El papa San León deteniendo a Atila", 1646-1653); y Ercole Ferrata, discípulo de Bernini ("La muerte en la hoguera de Santa Inés", 1660).

En Francia la escultura fue heredera del clasicismo renacentista, con preeminencia del aspecto decorativo y cortesano, y de la temática mitológica. Jacques Sarrazin se formó en Roma, donde estudió la escultura clásica y la obra de Miguel Ángel, cuya influencia se trasluce en sus "Cariátides" del Pavillon de l'Horloge del Louvre (1636). François Girardon trabajó en la decoración de Versalles, y es recordado por su "Mausoleo del Cardenal Richelieu" (1675-1694) y por el grupo de "Apolo y las Ninfas" de Versalles (1666-1675), inspirado en el "Apolo de Belvedere" de Leócares ("circa" 330 a. C.-300 a. C.). Antoine Coysevox también participó en el proyecto versallesco, y entre su producción destaca la "Glorificación de Luis XIV" en el Salón de la Guerra de Versalles (1678) y el "Mausoleo de Mazarino" (1689-1693). Pierre Puget fue el más original de los escultores franceses de la época, aunque no trabajó en París, y su gusto por el dramatismo y el movimiento violento le alejaron del clasicismo de su entorno: "Milón de Crotona" (1671-1682), inspirada en el "Laocoonte".
En España perduró la imaginería religiosa de herencia gótica, generalmente en madera policromada —a veces con el añadido de ropajes auténticos—, presente o bien en retablos o bien en figura exenta. Se suelen distinguir en una primera fase dos escuelas: la castellana, centrada en Madrid y Valladolid, donde destaca Gregorio Fernández, que evoluciona de un manierismo de influencia juniana a un cierto naturalismo ("Cristo yacente", 1614; "Bautismo de Cristo", 1630), y Manuel Pereira, de corte más clásico ("San Bruno", 1652); en la escuela andaluza, activa en Sevilla y Granada, destacan: Juan Martínez Montañés, con un estilo clasicista y figuras que denotan un detallado estudio anatómico ("Cristo crucificado", 1603; "Inmaculada Concepción", 1628-1631); su discípulo Juan de Mesa, más dramático que el maestro ("Jesús del Gran Poder", 1620); Alonso Cano, también discípulo de Montañés, y como él de un contenido clasicismo ("Inmaculada Concepción", 1655; "San Antonio de Padua", 1660-1665); y Pedro de Mena, discípulo de Cano, con un estilo sobrio pero expresivo ("Magdalena penitente", 1664). Desde mediados de siglo se produce el «pleno barroco», con una fuerte influencia berniniana, con figuras como Pedro Roldán (Retablo Mayor del Hospital de la Caridad de Sevilla, 1674) y Pedro Duque Cornejo (Sillería del coro de la Catedral de Córdoba, 1748). Ya en el siglo XVIII destacó la escuela levantina en Murcia y Valencia, con nombres como Ignacio Vergara o Nicolás de Bussi, y la figura principal de Francisco Salzillo, con un estilo sensible y delicado que apunta al rococó ("Oración del Huerto", 1754; "Prendimiento", 1763).

En Alemania meridional y Austria la escultura tuvo un gran auge en el siglo XVII gracias al impulso contrarreformista, tras la anterior iconoclasia protestante. En un principio las obras más relevantes fueron encargadas a artistas holandeses, como Adriaen de Vries ("Aflicción de Cristo", 1607). Como nombres alemanes cabe destacar a: Hans Krumper ("Patrona Bavariae", 1615); Hans Reichle, discípulo de Giambologna (coro y grupo de "La Crucifixión" de la Catedral de San Ulrico y Santa Afra de Augsburgo, 1605); Georg Petel ("Ecce Homo", 1630); Justus Glesker ("Grupo de la Crucifixión", 1648-1649); y el también arquitecto Andreas Schlüter, que recibe la influencia berniniana ("Estatua ecuestre del Gran Elector Federico Guillermo I de Brandemburgo", 1689-1703). En Inglaterra se combinó la influencia italiana, presente especialmente en el dinámico dramatismo de los monumentos funerarios, y la francesa, cuyo clasicismo es más apropiado para las estatuas y los retratos. El escultor inglés más importante de la época fue Nicholas Stone, formado en Holanda, autor de monumentos funerarios como el de Lady Elisabeth Carey (1617-1618) o el de "sir" William Curle (1617).

En los Países Bajos la escultura barroca se limitó a un único nombre de fama internacional, el también arquitecto Hendrik de Keyser, formado en el manierismo italiano ("Monumento funerario de Guillermo I", 1614-1622). En Flandes en cambio sí surgieron numerosos escultores, muchos de los cuales se instalaron en el país vecino, como Artus Quellinus, autor de la decoración escultórica del Ayuntamiento de Ámsterdam. Otros escultores flamencos fueron: Lukas Fayd'herbe ("Tumba del arzobispo André Cruesen", 1666); Rombout Verhulst ("Tumba de Johan Polyander van Kerchoven", 1663); y Hendrik Frans Verbruggen (Púlpito de la Catedral de San Miguel y Santa Gúdula de Bruselas, 1695-1699).

En América destacó la obra escultórica desarrollada en Lima, con autores como el catalán Pedro de Noguera, inicialmente de estilo manierista, que evolucionó hacia el Barroco en obras como la sillería de la Catedral de Lima; el vallisoletano Gomes Hernández Galván, autor de las Tablas de la Catedral; Juan Bautista Vásquez, autor de una escultura de la Virgen conocida como "La Rectora", actualmente en el Instituto Riva-Agüero; y Diego Rodrigues, autor de la imagen de la "Virgen de Copacabana" en el Santuario homónimo del Distrito del Rímac de Lima. En México destacó el zamorano Jerónimo de Balbás, autor del Retablo de los Reyes de la Catedral Metropolitana de la Ciudad de México. En Ecuador destacó la escuela quiteña, representada por Bernardo de Legarda y Manuel Chili (apodado Caspicara). En Brasil destacó nuevamente la figura del Aleijadinho, que se encargó de la decoración escultórica de sus proyectos arquitectónicos, como la iglesia de São Francisco de Assis en Ouro Preto, donde realizó las esculturas de la fachada, el púlpito y el altar; o el Santuario del Buen Jesús de Congonhas, donde destacan las figuras de los doce profetas.

La pintura barroca tuvo un marcado acento diferenciador geográfico, ya que su desarrollo se produjo por países, en diversas escuelas nacionales cada una con un sello distintivo. Sin embargo, se percibe una influencia común proveniente nuevamente de Italia, donde surgieron dos tendencias contrapuestas: el naturalismo (también llamado caravagismo), basado en la imitación de la realidad natural, con cierto gusto por el claroscuro —el llamado tenebrismo—; y el clasicismo, que es igual de realista pero con un concepto de la realidad más intelectual e idealizado. Posteriormente, en el llamado «pleno barroco» (segunda mitad del siglo XVII), la pintura evolucionó a un estilo más decorativo, con predominio de la pintura mural y cierta predilección por los efectos ópticos ("trompe-l'oeil") y las escenografías lujosas y exuberantes.

Como hemos visto, en un primer lugar surgieron dos tendencias contrapuestas, naturalismo y clasicismo. La primera tuvo su máximo exponente en Caravaggio, un artista original y de vida azarosa que, pese a su prematura muerte, dejó numerosas obras maestras en las que se sintetizan la descripción minuciosa de la realidad y el tratamiento casi vulgar de los personajes con una visión no exenta de reflexión intelectual. Igualmente fue introductor del tenebrismo, donde los personajes destacan sobre un fondo oscuro, con una iluminación artificial y dirigida, de efecto teatral, que hace resaltar los objetos y los gestos y actitudes de los personajes. Entre las obras de Caravaggio destacan: "Crucifixión de San Pedro" (1601), "La vocación de San Mateo" (1602), "Entierro de Cristo" (1604), etc. Otros artistas naturalistas fueron: Bartolomeo Manfredi, Carlo Saraceni, Giovanni Battista Caracciolo y Orazio y Artemisia Gentileschi. También cabe mencionar, en relación con este estilo, un género de pinturas conocido como «bambochadas» (por el pintor holandés establecido en Roma Pieter van Laer, apodado "il Bamboccio"), que se centra en la representación de personajes vulgares como mendigos, gitanos, borrachos o vagabundos.

La segunda tendencia fue el clasicismo, que surgió en Bolonia, en torno a la denominada escuela boloñesa, iniciada por los hermanos Annibale y Agostino Carracci. Esta tendencia suponía una reacción contra el manierismo, buscando una representación idealizada de la naturaleza, representándola no como es, sino como debería ser. Perseguía como único objetivo la belleza ideal, para lo que se inspiraron en al arte clásico grecorromano y el arte renacentista. Este ideal encontró un tema idóneo de representación en el paisaje, así como en temas históricos y mitológicos. Los hermanos Carracci trabajaron juntos en un principio (frescos del Palazzo Fava de Bolonia), hasta que Annibale fue llamado a Roma para decorar la bóveda del Palazzo Farnese (1597-1604), que por su calidad ha sido comparada con la Capilla Sixtina. Otros miembros de la escuela fueron: Guido Reni ("Hipómenes y Atalanta", 1625), Domenichino ("La caza de Diana", 1617), Francesco Albani ("Los Cuatro Elementos", 1627), Guercino ("La Aurora", 1621-1623) y Giovanni Lanfranco ("Asunción de la Virgen", 1625-1627).

Por último, en el «pleno barroco» culminó el proceso iniciado en la arquitectura y la escultura, tendentes a la monumentalidad y el decorativismo, a la figuración recargada y ampulosa, con gusto por el "horror vacui" y los efectos ilusionistas. Uno de sus grandes maestros fue el también arquitecto Pietro da Cortona, influido por la pintura veneciana y flamenca, autor de la decoración de los palacios Barberini y Pamphili en Roma y Pitti en Florencia. Otros artistas fueron: il Baciccia, autor de los frescos de la Iglesia del Gesù (1672-1683); Andrea Pozzo, que decoró la bóveda de la Iglesia de San Ignacio de Roma (1691-1694); y el napolitano Luca Giordano, artífice de la decoración del Palazzo Medici-Riccardi de Florencia (1690), y que también trabajó en España, donde es conocido como Lucas Jordán.

En Francia también se dieron las dos corrientes surgidas en Italia, el naturalismo y el clasicismo, aunque el primero no tuvo excesivo predicamento, debido al gusto clasicista del arte francés desde el Renacimiento, y se dio principalmente en provincias y en círculos burgueses y eclesiásticos, mientras que el segundo fue adoptado como «arte oficial» por la monarquía y la aristocracia, que le dieron unas señas de identidad propias con la acuñación del término clasicismo francés. El principal pintor naturalista fue Georges de La Tour, en cuya obra se distinguen dos fases, una centrada en la representación de tipos populares y escenas jocosas, y otra donde predomina la temática religiosa, con un radical tenebrismo donde las figuras se vislumbran con tenues luces de velas o lámparas de bujía: "Magdalena penitente" (1638-1643), "San Sebastián cuidado por Santa Irene" (1640). También se engloban en esta corriente los hermanos Le Nain (Antoine, Louis y Mathieu), centrados en la temática campesina pero alejados del tenebrismo, y con cierta influencia bambochante.

La pintura clasicista se centra en dos grandes pintores que desarrollaron la mayor parte de su carrera en Roma: Nicolas Poussin y Claude Lorrain. El primero recibió la influencia de la pintura rafaelesca y de la escuela boloñesa, y creó un tipo de representación de escenas —de temática generalmente mitológica— donde evoca el esplendoroso pasado de la antigüedad grecorromana como un paraíso idealizado de perfección, una edad dorada de la humanidad, en obras como: "El triunfo de Flora" (1629) y "Los pastores de la Arcadia" (1640). Por su parte, Lorrain reflejó en su obra un nuevo concepto en la elaboración del paisaje basándose en referentes clásicos —el denominado «paisaje ideal»—, que evidencia una concepción ideal de la naturaleza y del hombre. En sus obras destaca la utilización de la luz, a la que otorga una importancia primordial a la hora de concebir el cuadro: "Paisaje con el embarque en Ostia de Santa Paula Romana" (1639), "Puerto con el embarque de la Reina de Saba" (1648).

En el pleno barroco la pintura se enmarcó más en el círculo áulico, donde se encaminó principalmente hacia el retrato, con artistas como Philippe de Champaigne ("Retrato del cardenal Richelieu", 1635-1640), Hyacinthe Rigaud ("Retrato de Luis XIV", 1701) y Nicolas de Largillière ("Retrato de Voltaire joven", 1718). Otra vertiente fue la de la pintura académica, que buscaba sentar las bases del oficio pictórico sobre la base de unos ideales clasicistas que, a la larga, acabaron constriñéndolo en unas rígidas fórmulas repetitivas. Algunos de sus representantes fueron: Simon Vouet ("Presentación de Jesús en el templo", 1641), Charles Le Brun ("Entrada de Alejandro Magno en Babilonia", 1664), Pierre Mignard ("Perseo y Andrómeda", 1679), Antoine Coypel ("Luis XIV descansando después de la Paz de Nimega", 1681) y Charles de la Fosse ("Rapto de Proserpina", 1673).

En España, pese a la decadencia económica y política, la pintura alcanzó cotas de gran calidad, por lo que se suele hablar, en paralelo a la literatura, de un «Siglo de Oro» de la pintura española. La mayor parte de la producción fue de temática religiosa, practicándose en menor medida la pintura de género, el retrato y el bodegón —especialmente "vanitas"—. Se percibe la influencia italiana y flamenca, que llega sobre todo a través de estampas: la primera se produce en la primera mitad del siglo XVII, con predominio del naturalismo tenebrista; y la segunda en el siguiente medio siglo y principios del XVIII, de procedencia rubeniana.

En la primera mitad de siglo destacan tres escuelas: la castellana (Madrid y Toledo), la andaluza (Sevilla) y la valenciana. La primera tiene un fuerte sello cortesano, por ser la sede de la monarquía hispánica, y denota todavía una fuerte influencia escurialense, perceptible en el estilo realista y austero del arte producido en esa época. Algunos de sus representantes son: Bartolomé y Vicente Carducho, Eugenio Cajés, Juan van der Hamen y Juan Bautista Maíno, en Madrid; Luis Tristán, Juan Sánchez Cotán y Pedro Orrente, en Toledo. En Valencia destacó Francisco Ribalta, con un estilo realista y colorista, de temática contrarreformista ("San Bruno", 1625). También se suele incluir en esta escuela, aunque trabajó principalmente en Italia, a José de Ribera, de estilo tenebrista pero con un colorido de influencia veneciana ("Sileno borracho", 1626; "El martirio de San Felipe", 1639). En Sevilla, tras una primera generación que aún denota la influencia renacentista (Francisco Pacheco, Juan de Roelas, Francisco de Herrera el Viejo), surgieron tres grandes maestros que elevaron la pintura española de la época a cotas de gran altura: Francisco de Zurbarán, Alonso Cano y Diego Velázquez. Zurbarán se dedicó principalmente a la temática religiosa —sobre todo en ambientes monásticos—, aunque también practicó el retrato y el bodegón, con un estilo simple pero efectista, de gran atención al detalle: "San Hugo en el refectorio de los Cartujos" (1630), "Fray Gonzalo de Illescas" (1639), "Santa Casilda" (1640). Alonso Cano, también arquitecto y escultor, evolucionó de un acentuado tenebrismo a un cierto clasicismo de inspiración veneciana: "Cristo muerto en brazos de un ángel" (1650), "Presentación de la Virgen en el Templo" (1656).

Diego Velázquez fue sin duda el artista de mayor genio de la época en España, y de los de más renombre a nivel internacional. Se formó en Sevilla, en el taller del que sería su suegro, Francisco Pacheco, y sus primeras obras de enmarcan en el estilo naturalista de moda en la época. En 1623 se estableció en Madrid, donde se convirtió en pintor de cámara de Felipe IV, y su estilo fue evolucionando gracias al contacto con Rubens (al que conoció en 1628) y al estudio de la escuela veneciana y el clasicismo boloñés, que conoció en un viaje a Italia en 1629-1631. Entonces abandonó el tenebrismo y se aventuró en un profundo estudio de la iluminación pictórica, de los efectos de luz tanto en los objetos como en el medio ambiente, con los que alcanza cotas de gran realismo en la representación de sus escenas, que sin embargo no está exento de un aire de idealización clásica, que muestra un claro trasfondo intelectual que para el artista era una reivindicación del oficio de pintor como actividad creativa y elevada. Entre sus obras destacan: "El aguador de Sevilla" (1620), "Los borrachos" (1628-1629), "La fragua de Vulcano" (1630), "La rendición de Breda" (1635), "Cristo crucificado" (1639), "Venus del espejo" (1647-1651), "Retrato de Inocencio X" (1649), "Las meninas" (1656) y "Las hilanderas" (1657).

En la segunda mitad de siglo los principales focos artísticos fueron Madrid y Sevilla. En la capital, el naturalismo fue sustituido por el colorido flamenco y el decorativismo del pleno barroco italiano, con artistas como: Antonio de Pereda ("El sueño del caballero", 1650); Juan Ricci ("Inmaculada Concepción", 1670); Francisco de Herrera el Mozo ("Apoteosis de San Hermenegildo", 1654); Juan Carreño de Miranda ("Fundación de la Orden Trinitaria", 1666); Juan de Arellano ("Florero", 1660); José Antolínez ("El tránsito de la Magdalena", 1670); Claudio Coello ("Carlos II adorando la Sagrada Forma", 1685); y Antonio Palomino (decoración del Sagrario de la Cartuja de Granada, 1712). En Sevilla destacó la obra de Bartolomé Esteban Murillo, centrado en la representación sobre todo de Inmaculadas y Niños Jesús —aunque también realizó retratos, paisajes y escenas de género—, con un tono delicado y sentimentalista, pero de gran maestría técnica y virtuosismo cromático: "Adoración de los pastores" (1650); "Inmaculada Concepción" (1678). Junto a él destacó Juan de Valdés Leal, antítesis de la belleza murillesca, con su predilección por las "vanitas" y un estilo dinámico y violento, que desprecia el dibujo y se centra en el color, en la materia pictórica: lienzos de las Postrimerías del Hospital de la Caridad de Sevilla (1672).

La separación política y religiosa de dos zonas que hasta el siglo anterior habían tenido una cultura prácticamente idéntica pone de manifiesto las tensiones sociales que se vivieron en el siglo XVII: Flandes, que seguía bajo el dominio español, era católica y aristocrática, con predominio en el arte de la temática religiosa, mientras que los recién independizados Países Bajos fueron protestantes y burgueses, con un arte laico y más realista, con gusto por el retrato, el paisaje y el bodegón.

En Flandes la figura capital fue Peter Paul Rubens, formado en Italia, donde recibió la influencia de Miguel Ángel y de las escuelas veneciana y boloñesa. En su taller de Amberes empleó a gran cantidad de colaboradores y discípulos, por lo que su producción pictórica destaca tanto por su cantidad como por su calidad, con un estilo dinámico, vital y colorista, donde destaca la rotundidad anatómica, con varones musculosos y mujeres sensuales y carnosas: "El desembarco de María de Médicis en el puerto de Marsella" (1622-1625), "Minerva protege a Pax de Marte" (1629), "Las tres Gracias" (1636-1639), "Rapto de las hijas de Leucipo" (1636), "Juicio de Paris" (1639), etc. Discípulos suyos fueron: Anton van Dyck, gran retratista, de estilo refinado y elegante (""Sir" Endymion Porter y Anton van Dyck", 1635); Jacob Jordaens, especializado en escenas de género, con gusto por los temas populares ("El rey bebe", 1659); y Frans Snyders, centrado en el bodegón ("Bodegón con aves y caza", 1614).

En Holanda destacó especialmente Rembrandt, artista original de fuerte sello personal, con un estilo cercano al tenebrismo pero más difuminado, sin los marcados contrastes entre luz y sombra propios de los caravaggistas, sino una penumbra más sutil y difusa. Cultivó todo tipo de géneros, desde el religioso y mitológico hasta el paisaje y el bodegón, así como el retrato, donde destacan sus autorretratos, que practicó a lo largo de toda su vida. Entre sus obras destacan: "Lección de anatomía del Dr. Nicolaes Tulp" (1632), "La ronda de noche" (1642), "El buey desollado" (1655), y "Los síndicos de los pañeros" (1662). Otro nombre relevante es Frans Hals, magnífico retratista, con una pincelada libre y enérgica que antecede al impresionismo ("Banquete de los arcabuceros de San Jorge de Haarlem", 1627). El tercer nombre de gran relevancia es Jan Vermeer, especializado en paisajes y escenas de género, a los que otorgó un gran sentido poético, casi melancólico, donde destaca especialmente el uso de la luz y los colores claros, con una técnica casi puntillista: "Vista de Delft" (1650), "La lechera" (1660), "La carta" (1662). El resto de artistas holandeses se especializaron por lo general en géneros: de interior y temas populares y domésticos (Pieter de Hooch, Jan Steen, Gabriel Metsu, Gerard Dou); paisaje (Jan van Goyen, Jacob van Ruysdael, Meindert Hobbema, Aelbert Cuyp); y bodegón (Willem Heda, Pieter Claesz, Jan Davidsz de Heem).

En Alemania hubo escasa producción pictórica, debido a la Guerra de los Treinta Años, por lo que muchos artistas alemanes tuvieron que trabajar en el extranjero, como es el caso de Adam Elsheimer, un notable paisajista adscrito al naturalismo que trabajó en Roma ("La huida a Egipto", 1609). También en Roma se afincó Joachim von Sandrart, pintor y escritor que recopiló diversas biografías de artistas de la época ("Teutschen Academie der Edlen Bau-, Bild- und Mahlerey-Künsten", 1675). Igualmente, Johann Liss estuvo peregrinando entre Francia, Países Bajos e Italia, por lo que su obra es muy variada tanto estilísticamente como de géneros ("La inspiración de San Jerónimo", 1627). Johann Heinrich Schönfeld pasó buena parte de su carrera en Nápoles, elaborando una obra de estilo clasicista e influencia poussiniana ("Desfile triunfal de David", 1640-1642). En la propia Alemania, se desarrolló notablemente el bodegón, con artistas como Georg Flegel, Georg Hinz y Sebastian Stoskopff. En Austria destacó Johann Michael Rottmayr, autor de los frescos de la Iglesia colegial de Melk (1716-1722) y la Iglesia de San Carlos Borromeo de Viena (1726). En Inglaterra, la escasa tradición pictórica autóctona hizo que la mayoría de encargos —generalmente retratos— fuese confiada a artistas extranjeros, como el flamenco Anton van Dyck ("Retrato de Carlos I de Inglaterra", 1638), o el alemán Peter Lely ("Louise de Kéroualle", 1671).

Las primeras influencias fueron del tenebrismo sevillano, principalmente de Zurbarán —algunas de cuyas obras aún se conservan en México y Perú—, como se puede apreciar en la obra de los mexicanos José Juárez y Sebastián López de Arteaga, y del boliviano Melchor Pérez de Holguín. La Escuela cuzqueña de pintura surgió a raíz de la llegada del pintor italiano Bernardo Bitti en 1583, que introdujo el manierismo en América. Destacó la obra de Luis de Riaño, discípulo del italiano Angelino Medoro, autor de los murales del templo de Andahuaylillas. También destacaron los pintores indios Diego Quispe Tito y Basilio Santa Cruz Puma Callao, así como Marcos Zapata, autor de los cincuenta lienzos de gran tamaño que cubren los arcos altos de la Catedral de Cuzco. En Ecuador se formó la escuela quiteña, representada principalmente por Miguel de Santiago y Nicolás Javier de Goríbar.

En el siglo XVIII los retablos escultóricos empezaron a ser sustituidos por cuadros, desarrollándose notablemente la pintura barroca en América. Igualmente, creció la demanda de obras de tipo civil, principalmente retratos de las clases aristocráticas y de la jerarquía eclesiástica. La principal influencia fue la murillesca, y en algún caso —como en Cristóbal de Villalpando— la de Valdés Leal. La pintura de esta época tiene un tono más sentimental, con formas más dulces y blandas. Destacan Gregorio Vázquez de Arce en Colombia, y Juan Rodríguez Juárez y Miguel Cabrera en México.

Las artes gráficas tuvieron una gran difusión durante el Barroco, continuando el auge que este sector tuvo durante el Renacimiento. La rápida profusión de grabados a todo lo largo de Europa propició la expansión de los estilos artísticos originados en los centros de mayor innovación y producción de la época, Italia, Francia, Flandes y Países Bajos —decisivos, por ejemplo, en la evolución de la pintura española—. Las técnicas más empleadas fueron el aguafuerte y el grabado a punta seca. Estos procedimientos permiten a un artista confeccionar un diseño sobre una placa de cobre en sucesivas etapas, pudiendo ser retocado y perfeccionado sobre la marcha. Los diversos grados de desgastamiento de las placas permitían realizar unas 200 impresiones al aguafuerte —aunque siendo solo las 50 primeras de una calidad excelente—, y unas 10 a la punta seca.

En el siglo XVII los principales centros de producción de grabados estaban en Roma, París y Amberes. En Italia fue practicado por Guido Reni, con un dibujo claro y firme de corte clasicista; y Claude Lorrain, autor de aguafuertes de gran calidad, especialmente en los sombreados y la utilización de líneas entrelazadas para sugerir distintos tonos, por lo general en paisajes. En Francia destacaron: Abraham Bosse, autor de unos 1500 grabados, generalmente escenas de género; Jacques Bellange, autor de representaciones religiosas, influido por Parmigianino; y Jacques Callot, formado en Florencia y especializado en figuras de mendigos y seres deformes, así como escenas de la novela picaresca y la commedia dell'arte —su serie de "Grandes miserias de la guerra" influyó en Goya—. En Flandes, Rubens fundó una escuela de burilistas para divulgar más eficazmente su obra, entre los que destacó Lucas Vorsterman I; también Anton van Dyck cultivó el aguafuerte. En España el grabado fue practicado principalmente por José de Ribera, Francisco Ribalta y Francisco Herrera el Viejo. Uno de los artistas que más empleó la técnica del grabado fue Rembrandt, que alcanzó cotas de gran maestría no solo en el dibujo sino también en la creación de contrastes entre luces y sombras. Sus grabados fueron muy cotizados, como se puede comprobar con su aguafuerte "Cristo curando a un enfermo" (1648-1650), que se vendió por cien florines, una cifra récord en la época.
Las artes decorativas y aplicadas también tuvieron una gran expansión en el siglo XVII, debido principalmente al carácter decorativo y ornamental del arte barroco, y al concepto de «obra de arte total» que se aplicaba a las grandes realizaciones arquitectónicas, donde la decoración de interiores tenía un papel protagonista, como medio de plasmar la magnificencia de la monarquía o el esplendor de la Iglesia contrarreformista. En Francia, el lujoso proyecto del Palacio de Versalles conllevó la creación de la Manufacture Royale des Gobelins —dirigida por el pintor del rey, Charles Le Brun—, donde se manufacturaban todo tipo de objetos de decoración, principalmente mobiliario, tapicería y orfebrería. La confección de tapices tuvo un significativo incremento en su producción, y se encaminó a la imitación de la pintura, con la colaboración en numerosos casos de pintores de renombre que elaboraban cartones para tapices, como Simon Vouet, el propio Le Brun o Rubens en Flandes —país que también fue un gran centro productor de tapicería, que exportaba a todo el continente, como los magníficos tapices de "Triunfos del Santo Sacramento", confeccionados para las Descalzas Reales de Madrid—.

La orfebrería también alcanzó niveles de elevada producción, especialmente en plata y piedras preciosas. En Italia surgió una nueva técnica para revestir telas y objetos como altares o tableros de mesa con piedras semipreciosas como el ónice, la ágata, la cornalina o el lapislázuli.
En Francia, como el resto de manufacturas fue objeto de protección real, y fue tal la profusión de objetos de plata que en 1672 se promulgó una ley que limitaba la producción de objetos de este metal. La cerámica y el vidrio continuaron generalmente con las mismas técnicas de elaboración que en el período renacentista, destacando la cerámica blanca y azul de Delft (Holanda) y el vidrio pulido y tallado de Bohemia.
El vidriero de Murano Nicola Mazzolà fue artífice de un tipo de vidrio que imitaba la porcelana china.
También continuó la elaboración de vidrieras para iglesias, como las de la iglesia parisina de Saint-Eustache (1631), diseñadas por Philippe de Champaigne.
Uno de los sectores que cobró más relevancia fue la ebanistería, caracterizada por las superficies onduladas (cóncavas y convexas), con volutas y diversos motivos como cartelas y conchas. En Italia destacaron: el armario toscano de dos cuerpos, con balaustradas de bronce y decoración de taracea de piedras duras; el escritorio ligur de dos cuerpos, con figuras talladas y superpuestas ("bambochos"); y el sillón entallado veneciano ("tronetto"), de exuberante decoración. En España surgió el "bargueño", cofre rectangular con asas, con numerosos cajones y compartimentos. El mobiliario español continuó con la decoración de estilo mudéjar, mientras que el Barroco se denotaba en las formas curvas y el uso de columnas salomónicas en las camas. Aun así, predominó la austeridad de signo contrarreformista, como se denota en el sillón llamado "frailero" (o "misional" en Hispanoamérica). La edad de oro de la ebanistería se produjo en la Francia de los Luises, donde se alcanzaron altos niveles de calidad y refinamiento, sobre todo gracias a la obra de André-Charles Boulle, creador de una nueva técnica de aplicación de metales (cobre, estaño) sobre materiales orgánicos (carey, madreperla, marfil) o viceversa. Entre las obras de Boulle destacan las dos cómodas del Trianón, en Versalles, y el reloj de péndulo con el "Carro de Apolo" en Fontainebleau.

La literatura barroca, como el resto de las artes, se desarrolló bajo preceptos políticos absolutistas y religiosos contrarreformistas, y se caracterizó principalmente por el escepticismo y el pesimismo, con una visión de la vida planteada como lucha, sueño o mentira, donde todo es fugaz y perecedero, y donde la actitud frente a la vida es la duda o el desengaño, y la prudencia como norma de conducta. Su estilo era suntuoso y recargado, con un lenguaje muy adjetivado, alegórico y metafórico, y un empleo frecuente de figuras retóricas. Los principales géneros que se cultivaron fueron la novela utópica y la poesía bucólica, que junto al teatro —que por su importancia se trata en otro apartado—, fueron los principales vehículos de expresión de la literatura barroca. Como ocurrió igualmente con el resto de las artes, la literatura barroca no fue homogénea en todo el continente, sino que se formaron diversas escuelas nacionales, cada una con sus peculiaridades, hecho que fomentó el auge de las lenguas vernáculas y el progresivo abandono del latín.

En Italia, la literatura se forjó sobre los cimientos de la dicotomía realismo-idealismo renacentista, así como el predominio nuevamente de la religión sobre el humanismo. Su principal sello lingüístico fue el uso y abuso de la metáfora, que lo impregna todo, con un gusto estético un tanto retorcido, con preferencia por lo deforme sobre lo bello. La principal corriente fue el marinismo —por Giambattista Marino—, un estilo ampuloso y exagerado que pretende sorprender por el virtuosismo del lenguaje, sin prestar especial atención al contenido. Para Marino, «el fin del poeta es el asombro»: su principal obra, "Adonis" (1623), destaca por su musicalidad y por la abundancia de imágenes, con un estilo elocuente y, pese a todo, sencillo de leer. Otros poetas marinistas fueron: Giovanni Francesco Busenello, Emanuele Tesauro, Cesare Rinaldi, Giulio Strozzi, etc.

En Francia surgió el preciosismo, una corriente similar al marinismo que otorga especial relevancia a la riqueza del lenguaje, con un estilo elegante y amanerado. Estuvo representado por Isaac de Benserade y Vincent Voiture en poesía, y Honoré d'Urfé y Madeleine de Scudéry en prosa. Más adelante surgió el clasicismo, que propugnaba un estilo simple y austero, sujeto a cánones clásicos —como las tres unidades aristotélicas—, con una rígida reglamentación métrica. Su iniciador fue François de Malherbe, cuya poesía racional y excesivamente rígida le restaba cualquier atisbo de emocionalidad, al que siguieron: Jean de La Fontaine, un impecable fabulista, de intención didáctica y moralizadora; y Nicolas Boileau-Despréaux, poeta elegante pero falto de creatividad, por su insistencia en someter la imaginación al imperio de la norma y la reglamentación. Otros géneros cultivados fueron: el burlesco (Paul Scarron), la elocuencia (Jacques-Bénigne Bossuet), la novela psicológica (Madame de La Fayette), la novela didáctica (François Fénelon), la prosa satírica (Jean de La Bruyère, François de la Rochefoucauld), la literatura epistolar (Jean-Louis Guez de Balzac, la Marquesa de Sévigné), la religiosa (Blaise Pascal), la novela fantástica (Cyrano de Bergerac) y el cuento de hadas (Charles Perrault).
En Inglaterra surgió el eufuismo —por "Eufues o la Anatomía del Ingenio", de John Lyly (1575)—, una corriente similar al marinismo o el preciosismo, que presta más atención a los efectos lingüísticos (antítesis, paralelismos) que al contenido, y que mezcla elementos de la cultura popular con la mitología clásica. Este estilo fue practicado por Robert Greene, Thomas Lodge y Barnabe Rich. Posteriormente surgió una serie de poetas llamados «metafísicos», cuyo principal representante fue John Donne, que renovó la lírica con un estilo directo y coloquial, alejado de fantasías y virtuosismos lingüísticos, con gran realismo y trasfondo conceptual ("Sonetos sagrados", 1618). Otros poetas metafísicos fueron: George Herbert, Richard Crashaw, Andrew Marvell y Henry Vaughan. Figura aparte y de mayor relevancia es John Milton, autor de "El paraíso perdido" (1667), de influjo puritano, con un estilo sensible y delicado, y que gira en torno a la religión y el destino del hombre, al que otorga un espíritu de rebeldía que sería recogido por los románticos. El último gran poeta de la época es John Dryden, poeta y dramaturgo de tono satírico. En prosa destacó la época de la Restauración, con un estilo racional, moral y didáctico, con influencia del clasicismo francés, representado por diversos géneros: literatura religiosa (John Bunyan, George Fox); narrativa (Henry Neville); y memorias y diarios (Samuel Pepys).

En Alemania, la literatura estuvo influida por la "Pléiade" francesa, el gongorismo español y el marinismo italiano, aunque tuvo un desarrollo diferenciado por la presencia del protestantismo y el mayor peso social de la burguesía, que se denota en géneros como el "Schuldrama" («teatro escolar») y el "Gemeindelied" («canto parroquial»). Pese al desmembramiento del territorio alemán en numerosos estados, surgió una conciencia nacional de la lengua común, que fue protegida a través de las "Sprachgesellschaften" («sociedades de la lengua»). En el terreno de la lírica destacaron las denominadas Primera escuela de Silesia, representada por Martin Opitz, Paul Fleming, Angelus Silesius y Andreas Gryphius, y Segunda escuela de Silesia, donde destacan Daniel Casper von Lohenstein y Christian Hofmann von Hofmannswaldau. En la narrativa destaca igualmente Lohenstein, autor de la primera novela alemana plenamente barroca ("La maravillosa historia del gran príncipe cristiano alemán Hércules"), y Hans Jakob Christoph von Grimmelshausen, autor de "El aventurero Simplicíssimus" (1669), una novela costumbrista similar al género picaresco español.

En Portugal, la anexión a la corona española originó un período de cierta decadencia, viéndose la literatura portuguesa sometida al influjo de la española. Numerosos poetas siguieron el estilo épico de Camões, el gran poeta renacentista autor de "Os Lusíadas", al que imitaron autores como Vasco Mouzinho de Quebedo ("Alfonso Africano", 1616), Francisco Sá de Meneses ("Malaca conquistada", 1634), Gabriel Pereira de Castro ("Lisboa edificada", 1686), y Braz Garcia de Mascarenhas ("Viriato trágico", 1699). En la primera mitad de siglo destacaron el novelista y poeta Francisco Rodrigues Lobo, autor de novelas pastoriles que alternan el verso y la prosa ("El pastor peregrino", 1608); y Francisco Manuel de Melo, autor de poemas gongorinos, diálogos y tratados históricos ("Obras métricas", 1665). También destacó la prosa religiosa, cultivada por Bernardo de Brito, João de Lucena, António Vieira y Manuel Bernardes.

En Holanda, la independencia de España supuso una revitalización de la literatura, donde el siglo XVII suele ser descrito como una «Edad de oro» de las letras neerlandesas. Sin embargo, estilísticamente la literatura holandesa de la época no encaja del todo en los cánones del Barroco, debido principalmente a las peculiaridades sociales y religiosas de este país, como se ha visto en el resto de las artes. En Ámsterdam surgió el denominado "Muiderkring" («Círculo de Muiden»), un grupo de poetas y dramaturgos liderado por Pieter Corneliszoon Hooft, escritor de poesía pastoral y de tratados de historia, que sentaron las bases de la gramática holandesa. A este círculo perteneció también Constantijn Huygens, conocido por sus epigramas espirituales. La cumbre de la poesía lírica de la Edad de oro holandesa fue Joost van den Vondel, que influido por Ronsard destacó por el verso sonoro y rítmico, relatando con un estilo algo satírico los principales acontecimientos de su época ("Los misterios del altar", 1645). En Middelburg destacó Jacob Cats, autor de poemas didácticos y morales. En prosa cabe citar a Johan van Heemskerk, autor de "Arcadia Bátava" (1637), el primer romance escrito en holandés, género que fue rápidamente imitado, como en el "Mirandor" (1675) de Nikolaes Heinsius el Joven.

En España, donde el siglo XVII sería denominado el «Siglo de oro», la literatura estuvo más que en ningún otro sitio al servicio del poder, tanto político como religioso. La mayoría de obras van encaminadas a la exaltación del monarca como elegido por Dios, y de la Iglesia como redentora de la humanidad, al mismo tiempo que se procura una evasión de la realidad para diluir la penosa situación económica de la mayoría de la población. Sin embargo, pese a estas limitaciones, la creatividad de los escritores de la época y la riqueza del lenguaje desarrollado produjeron un elevado nivel de calidad, que convierten a la literatura española de la época en el paradigma de la literatura barroca y en una de las más altas cimas de la historia de la literatura. La descripción de la realidad se basa en dos ejes vertebradores: la transitoriedad de los fenómenos terrenales, donde todo es vanidad ("vanitas vanitatum"); y el omnipresente recuerdo de la muerte ("memento mori"), que hace apreciar con más intensidad la vida ("carpe diem").

La base conceptual de la literatura barroca española proviene de la cultura grecolatina, aunque adaptada, como se ha descrito, a la apología político-religiosa. Así, la estética literaria se vertebra alrededor de tres tópicos de origen clásico: la contraposición entre juicio e ingenio, que si bien en el humanismo renacentista estaban equilibrados, en el Barroco será el segundo el que asumirá mayor relevancia; el tópico horaciano "delectare et prodesse" («deleitar y aprovechar»), por el que se produce una simbiosis entre los recursos estilísticos y el proselitismo a favor del poder establecido, y por el que en última instancia se llega a la fórmula "ars gratia artis" («el arte por el arte»), en que la literatura se abandona al placer de la simple belleza; y el también tópico horaciano "ut pictura poesis" («la poesía como la pintura»), máxima por la cual el arte debe imitar la naturaleza para conseguir la perfección —como expresó Baltasar Gracián: «lo que es para los ojos la hermosura, y para los oídos la consonancia, eso es para el entendimiento el concepto»—.
En la lírica se dieron dos corrientes: el culteranismo (o cultismo), liderado por Luis de Góngora (por lo que también se le llama «gongorismo»), donde destacaba la belleza formal, con un estilo suntuoso, metafórico, con abundancia de paráfrasis y una gran proliferación de latinismos y juegos gramaticales; y el conceptismo, representado por Francisco de Quevedo y donde predominaba el ingenio, la agudeza, la paradoja, con un lenguaje conciso pero polisémico, con múltiples significados en pocas palabras. Góngora fue uno de los mejores poetas de principios del siglo XVII, actividad que cultivaba en sus ratos libres como sacerdote. Su obra está influida por Garcilaso, aunque sin el sentido armónico y equilibrado que mostró este en toda su producción. El estilo de Góngora es más ornamental, musical, colorista, con abundancia de hipérbatos y metáforas, por lo que resulta difícil de leer y se dirige especialmente a minorías cultas. En cuanto a temática, predomina la amorosa, la satírica-burlesca y la religioso-moral. Empleó métricas como las silvas y las octavas reales, pero también formas más populares como sonetos, romances y redondillas. Sus principales obras son la "Fábula de Polifemo y Galatea" (1613) y "Soledades" (1613). Otros poetas culteranistas fueron: Juan de Tassis, conde de Villamediana, Gabriel Bocángel, Pedro Soto de Rojas, Anastasio Pantaleón de Ribera, Salvador Jacinto Polo de Medina, Francisco de Trillo y Figueroa, Miguel Colodrero de Villalobos y fray Hortensio Félix Paravicino.

Por su parte, Quevedo osciló en su vida personal entre importantes cargos políticos o la cárcel y el destierro, según su relación temperamental con las autoridades. En su obra se vislumbra un sentimiento desgarrado por la realidad cotidiana de su país, donde predomina el desengaño, la presencia del dolor y la muerte. Esta visión se desarrolla en dos líneas contrapuestas: o bien la cruda descripción de la realidad, o bien burlándose de ella y caricaturizándola. Sus poemas fueron publicados tras su muerte en dos volúmenes: "Parnaso español" (1648) y "Las tres Musas" (1670). Otros poetas conceptistas fueron: Alonso de Ledesma, Miguel Toledano, Pedro de Quirós y Diego de Silva y Mendoza, conde de Salinas. Aparte de estas dos corrientes merece destacarse la figura de Lope de Vega, un gran dramaturgo que también cultivó la poesía y la novela, tanto de inspiración religiosa como profana, a menudo con un trasfondo autobiográfico. Utilizó principalmente la métrica de romances y sonetos, como en "Rimas sacras" (1614) y "Rimas humanas y divinas del licenciado Tomé de Burguillos" (1634); y también realizó poemas épicos, como "La Dragontea" (1598), "El Isidro" (1599) y "La Gatomaquia" (1634).
La prosa estuvo dominada por la gran figura de Miguel de Cervantes, que si bien se sitúa entre el Renacimiento y el Barroco supuso una figura de transición que marcó a una nueva generación de escritores españoles. Militar en su juventud —participó en la batalla de Lepanto—, estuvo prisionero de los turcos durante cinco años; posteriormente ocupó diversos cargos burocráticos, que compaginó con la escritura, que si bien le proporcionó una inicial fama no impidió que muriese en la pobreza. Cultivó la novela, el teatro y la poesía, aunque esta última con escaso éxito. Pero indudablemente su talento estaba en la prosa, que oscila entre el realismo y el idealismo, a menudo con una fuerte intención moralizadora, como en sus "Novelas ejemplares". Su gran obra, y una de las cumbres de la literatura universal, es "Don Quijote" (1605), la historia de un hidalgo que emprende una serie de alocadas aventuras creyéndose un gran paladín como los de las novelas de caballería. Si bien la primera intención de Cervantes era hacer una parodia, conforme se fue gestando la historia adquirió un fuerte sello filosófico y de introspección de la mente y el sentimiento humanos, pasando del humor a la fina ironía que, sin embargo, está exenta de resentimiento o acritud, y pone de manifiesto que la cualidad esencial del ser humano es su capacidad de soñar.

Otro terreno donde se desarrolló la prosa barroca española fue la novela picaresca, continuando la tradición iniciada el siglo anterior con el "Lazarillo de Tormes" (1554). Estuvo representada principalmente por tres nombres: Francisco de Quevedo, autor de "La vida del Buscón" (1604), de aspecto amargo y crudamente realista; Mateo Alemán, que firmó el "Guzmán de Alfarache" (en dos partes: 1599 y 1604), quizá la mejor en su género, donde el pícaro es más un filósofo que un pobre vagabundo; y Vicente Espinel, que en "El escudero Marcos de Obregón" (1618) ofrece una visión agridulce del pícaro, que pese a sus infortunios encuentra el lado amable de la vida. Otro género fue el de la novela pastoril, cultivada principalmente por Lope de Vega, autor de "La Arcadia" (1598) y "La Dorotea" (1632), esta última un drama en prosa cuyos largos diálogos la hacen irrepresentable como drama teatral. Por último, otra vertiente de la prosa de la época fue la conceptista, que en paralelo a la poesía desarrolló un estilo de escritura intelectual y cultivado, que se servía de los recursos de la lingüística y la sintaxis para describir la realidad circundante, generalmente de forma realista y desengañada, reflejando la amargura de una época donde la mayoría sobrevivía en duras condiciones sociales. Su principal exponente fue Baltasar Gracián, autor de "Agudeza y arte de ingenio" (1648), un tratado que desarrolla las posibilidades de la retórica; y "El Criticón" (1651-1655), novela de corte filosófico cuyo argumento es una alegoría de la vida humana, que oscila entre la civilización y la naturaleza, entre la cultura y la ignorancia, entre el espíritu y la materia. Como escritor conceptista también merece nombrarse a Luis Vélez de Guevara, autor de "El diablo cojuelo" (1641), novela satírica cercana a la picaresca pero sin sus elementos más comunes, por lo que cabría más calificarla de costumbrista.

En Latinoamérica, la literatura recibió en general los principales influjos de la metrópoli, aunque con diversas peculiaridades regionales. Destacaron especialmente el teatro y la poesía, esta última de influencia principalmente gongorina, a la que se sumaba el sello indígena y el estilo épico iniciado con "La Araucana" de Alonso de Ercilla: tenemos así "El Bernardo" (1624) de Bernardo de Balbuena; "Espejo de paciencia" (1608), del cubano Silvestre de Balboa; o "La Cristiada" (1611), de Diego de Hojeda. En México la poesía gongorina alcanzó cotas de gran calidad, con poetas como Luis de Sandoval y Zapata, Carlos de Sigüenza y Góngora, Agustín de Salazar y Torres y, principalmente, Sor Juana Inés de la Cruz, que inició un tipo de poesía didáctica y analítica que entroncaría con la Ilustración ("Inundación castálida", 1689). La prosa tuvo escasa producción, debido a la prohibición desde 1531 de cualquier introducción en las colonias de «literatura de ficción», y destacó solamente en el terreno de la historiografía: "Histórica relación del Reyno de Chile" (1646), de Alonso de Ovalle; "Historia general de las conquistas del Nuevo Reino de Granada" (1688), de Lucas Fernández de Piedrahita; "Historia de la conquista y población de la Provincia de Venezuela" (1723), de José de Oviedo y Baños. En Brasil destacó Gregório de Matos Guerra, autor de sátiras y poesías religiosas y seculares con influencia de Góngora y Quevedo.

Si bien resulta complicado literariamente hablar de teatro barroco en Europa, el Barroco supuso un período de esplendor del teatro como género literario y como espectáculo que se extendió desde Italia al resto de Europa en el siglo XVII. Los teatros nacionales, que se conformaron durante el siglo XVII, tienen características propias y diversas.

Durante el Barroco se definieron los límites estructurales de la sala y se introdujo la utilización de medios y aparatos mecánicos que potenciasen el componente visual del espectáculo.
Las realizaciones sobre el edificio teatral, las maquinarias y tramoyas ("tramoggie") desarrolladas en Italia se llevaron al resto de países europeos (España, Francia y Austria principalmente).
El nuevo teatro dejó de ser un ambiente único para dividirse en sala y escenario, separados y comunicados a la vez por el proscenio. Descorrido el telón, el escenario se presentaba como una escena ilusoria, apoyada en el notable desarrollo de la escenografía. La aplicación de la perspectiva de la escena a la italiana, respuesta a una visión del mundo que confiaba en las leyes científicas, alcanzó una gran sofisticación, con complicadísimos juegos de planos y puntos de fuga. La evolución de los corrales de comedias hasta las salas a la italiana propició la aparición de los edificios y salas teatrales contemporáneos.

El teatro del Barroco fue un espectáculo global que se convirtió en un negocio de distintas variantes. Por un lado estaba el teatro popular, que se trasladó del espacio público a locales específicamente dedicados a ello, como los corrales de comedias en España o los teatros isabelinos en Inglaterra. En Madrid, las cofradías de socorro (instituciones de asistencia social que, bajo advocación religiosa, proliferaron conforme crecía la Villa convertida en corte real) consiguieron el monopolio de la gestión comercial del teatro popular, lo que contribuyó a su desarrollo debido a la utilidad pública de la beneficencia, permitiendo superar la reticencia de predicadores, eclesiásticos e intelectuales hacia el teatro comercial profano, que consideraban «una fuente de pecado y malas costumbres». Se trataba de un teatro narrativo; en ausencia de telón y escenografía, los cambios de localización y tiempo se introducían a través del texto y eran habituales largos soliloquios, apartes y discursos prolongados.

El uso de artificios visuales y decoración transformó la escena con representaciones donde predominaba el espectáculo visual sobre el texto, diferenciándose de la «vulgar comedia». La «comedia de teatro» fue un género ligado al desarrollo de técnicas y artes, especialmente el edificio y la maquinaria teatral que se creaba para su representación. Arquitectos y escenógrafos de Italia llevaron los inventos sobre el edificio teatral, las maquinarias y tramoyas ("tramoggie") al resto de países europeos, España, Francia y Austria principalmente.

Las veladas teatrales del Barroco, fueran en los teatros de corrales o en los escenarios cortesanos, no consistían como actualmente en la representación de una sola pieza u obra; se trataba de toda una "fiesta teatral", una sucesión de piezas de distintos géneros entre los que ocupaba un papel primordial la comedia. Estas sesiones seguían una estructuración fija, en la que piezas menores de distintos géneros se intercalaban entre los actos del drama principal, normalmente una comedia o un auto sacramental. Estos géneros se diferenciaban básicamente por su función dentro de la representación y por el mayor o menor peso del componente cantado, bailado o representado. La fiesta teatral barroca pervivió, con ligeras variaciones, durante los dos primeros tercios del siglo XVIII.

A finales del siglo XVI una serie de artistas e intelectuales desarrollaron en Florencia una estética teatral que buscaba imitar «la grandiosidad e impacto expresivo del espectáculo griego»; partiendo de los textos de Aristóteles y Platón, la nueva estética giraba sobre los recursos expresivos de la voz en la declamación y sobre el papel de la música como soporte y acompañamiento del texto poético.
El ulterior desarrollo de sus teorías dio origen a nuevos géneros musicales como la ópera, la semiópera y la zarzuela.

En Italia triunfaba la Commedia dell'Arte, teatro popular basado en la improvisación que se extendió por toda Europa y perduró hasta principios del siglo XIX. Las compañías italianas adaptaron para su repertorio una buena cantidad de comedias españolas. Los componentes trágicos propios del teatro español, que no eran del gusto del público italiano, eran minimizados o eliminados de la obra, al tiempo que se dilataban o introducían situaciones cómicas que permitieran la aparición de los personajes y máscaras propios, como "Pulcinella" o el "Dottore".
Su acogida fue tan buena que la antigua "Vía del Teatro dei Fiorentini" de Nápoles (por entonces parte de la corona de España) llevaba para 1630 el sobrenombre de "Via della commedia spagnola".

En Francia, la tardía influencia del Renacimiento condujo a sus dramaturgos a desarrollar un teatro clasicista dirigido a una audiencia privilegiada. Autores como Molière, Racine y Corneille se pronunciaron a favor de los preceptos clásicos del teatro y la regla de las tres unidades dramáticas, basados en la "Poética" de Aristóteles.

No obstante, la obra de dramaturgos como Corneille acusa la influencia del teatro barroco español. Se dio así el Debate de los antiguos y los modernos ("Querelle des Anciens et des Modernes"), entre los partidarios del clasicismo y una generación de dramaturgos (la Generación de 1628) que defendían la libertad creadora y el respeto al gusto del público. Enmarcada en este debate, "Le Cid" (1637) de Corneille fue protagonista de una de las polémicas más célebres de la historia literaria de Francia, la "Querelle du Cid". Pese a ser una de las obras más aplaudidas del siglo XVII francés, "Le Cid" fue fuertemente criticada por no respetar los preceptos clásicos, especialmente la verosimilitud, el decoro y la finalidad educativa.

En 1680 Luis XIV fundó la Comédie-Française, compañía nacional francesa de teatro, producto de la fusión de varias compañías teatrales y le otorgó el monopolio de las representaciones en francés en París y sus arrabales. Su nombre surgió por contraposición con la Comédie Italienne (comedia italiana), una compañía italiana especializada en representaciones de la Comedia del arte con la que sostenían una especial competencia.

La influencia renacentista fue también tardía en Inglaterra, por lo que no suele hablarse de teatro barroco en la literatura inglesa del XVII, sino del teatro isabelino y de la comedia de la Restauración. Entre los dramaturgos de la época isabelina cabe destacar a Christopher Marlowe, iniciador de la nueva técnica teatral que puliría William Shakespeare, máximo exponente de la literatura inglesa y uno de los más célebres escritores de la literatura universal.
Como en España, el teatro se profesionalizó y trasladó el escenario de las plazas a salas públicas y privadas especialmente destinadas al espectáculo dramático. Entre los primeros teatros construidos en Londres se cuentan "The Theatre" (El Teatro), "The Curtain" (El Telón), "The Swan" (El Cisne), "The Globe" (El Globo) y "The Fortune" (La Fortuna).

Tras un paréntesis de dieciocho años en los que la facción puritana del parlamento inglés consiguió mantener los teatros ingleses clausurados, la Restauración monárquica de Carlos II en 1660 abrió paso a la comedia de la Restauración, una manifestación de las propuestas estéticas italianas de carácter popular, libertino, frívolo y extravagante.

Comparado con al extraordinario desarrollo en el contexto europeo, el teatro alemán del siglo XVII no realizó grandes aportes. El dramaturgo alemán más conocido podría ser Andreas Gryphius, que tomó como modelos el teatro de los jesuitas, al neerlandés Joost van den Vondel y a Corneille. Cabe mencionar también a Johannes Velten, quien combinó la tradición de los comediantes ingleses y la comedia del arte con el teatro clásico de Corneille y Molière. Su compañía de teatro ambulante se cuenta entre las más importantes del siglo XVII.

El Barroco tuvo su realización más característica en la católica y contrarreformista España, que sucedió a Italia en el liderazgo literario que le había pertenecido durante el Renacimiento. El teatro hispano del Barroco buscaba contentar al público con una realidad idealizada, en la que se manifiestan fundamentalmente tres sentimientos: el religioso católico, el monárquico y patrio y el del honor, procedente del mundo caballeresco.

Suelen apreciarse dos períodos o ciclos en el teatro barroco español, cuya separación se acentuó hacia 1630; un primer ciclo cuyo principal exponente sería Lope de Vega y en el que cabe mencionar también a Tirso de Molina, Gaspar de Aguilar, Guillén de Castro, Antonio Mira de Amescua, Luis Vélez de Guevara, Juan Ruiz de Alarcón, Diego Jiménez de Enciso, Luis Belmonte Bermúdez, Felipe Godínez, Luis Quiñones de Benavente o Juan Pérez de Montalbán; y un segundo ciclo, del que sería exponente Calderón de la Barca y que incluye a dramaturgos como Antonio Hurtado de Mendoza, Álvaro Cubillo de Aragón, Jerónimo de Cáncer y Velasco, Francisco de Rojas Zorrilla, Juan de Matos Fragoso, Antonio Coello y Ochoa, Agustín Moreto o Francisco de Bances Candamo.
Se trata de una clasificación relativamente laxa, puesto que cada autor tiene su propio hacer y puede en ocasiones adherirse a uno u otro planteamiento de la fórmula establecida por Lope. La «manera» de Lope es quizá más libre que la de Calderón, más sistematizada.

Félix Lope de Vega y Carpio introdujo con su "Arte nuevo de hacer comedias en este tiempo" (1609) la "comedia nueva", con la que estableció una nueva fórmula dramática que rompía con las tres unidades aristotélicas de la escuela de poética italiana (acción, tiempo y lugar), así como con una cuarta unidad, también esbozada en Aristóteles, la de estilo, tanto mezclando en una misma obra elementos trágicos y cómicos como valiéndose de distintos tipos de verso y estrofa según lo que se representa. Aunque Lope tenía un buen conocimiento de las artes plásticas, no dispuso durante la mayor parte de su carrera ni del teatro ni de la escenografía que se desarrolló posteriormente. La comedia lopesca otorgaba un papel secundario a los aspectos visuales de la representación teatral, que descansaban sobre el propio texto.

Tirso de Molina fue, junto a Lope de Vega y Calderón, uno de los tres dramaturgos más importantes de la España del Siglo de Oro. Su obra, que destaca por su sutil inteligencia y por una profunda comprensión de la humanidad de sus personajes, puede considerarse un puente entre la primitiva comedia lopesca y el más elaborado drama calderoniano. Aunque parte de la crítica discute su autoría, Tirso de Molina es conocido sobre todo por dos obras magistrales, "El condenado por desconfiado" y "El burlador de Sevilla", principal fuente del mito de Don Juan.

La llegada a Madrid de Cosme Lotti llevó a la corte española las técnicas teatrales más avanzadas de Europa. Sus conocimientos técnicos y mecánicos se aplicaron en exhibiciones palaciegas llamadas «fiestas» y en «fastuosos despliegues sobre ríos o fuentes artificiales» denominados «naumaquias». Tuvo a su cargo los diseños de los jardines del Buen Retiro, de la Zarzuela y de Aranjuez y la construcción del edificio teatral del Coliseo del Buen Retiro. Las fórmulas lopescas comenzaron a verse desplazadas por el afianzamiento del teatro palaciego y el nacimiento de nuevos conceptos cuando comenzó la carrera como dramaturgo de Pedro Calderón de la Barca.
Marcado al principio por las innovaciones de la comedia nueva lopesca, el estilo de Calderón marcó algunas diferencias, con un mayor cuidado constructivo y atención a su estructura interna. Sus obras alcanzaron una gran perfección formal, un lenguaje más lírico y simbólico. La libertad, el vitalismo y la espontaneidad lopesca dieron paso en Calderón a la reflexión intelectual y la precisión formal. En sus comedias predominan la intención ideológica y doctrinal sobre las pasiones y la acción, e hizo que el auto sacramental alcanzase sus más altas cotas. La "comedia de teatro" es un género «politécnico, multiartístico, híbrido en cierta manera». El texto poético se imbricó con medios y recursos procedentes de la arquitectura, la pintura y la música, liberándose de la descripción que en la comedia lopesca suplía la falta de decorados y dedicándose al diálogo de la acción.

Siguiendo la evolución marcada desde España, a finales del siglo XVI las compañías de comediantes, esencialmente trashumantes, comenzaron a profesionalizarse. Con la profesionalización vino la regulación y la censura: al igual que en Europa, el teatro oscilaba entre la tolerancia e incluso protección del gobierno y el rechazo (con excepciones) o la persecución por parte de la Iglesia. El teatro resultaba útil a las autoridades como instrumento de difusión del comportamiento y modelos deseados, el respeto al orden social y a la monarquía, escuela del dogma religioso.

Los corrales se administraban en beneficio de hospitales que compartían los beneficios de las representaciones. Las compañías itinerantes (o «de la legua»), que llevaban el teatro en tablados improvisados al aire libre por las regiones que no disponían de locales fijos, precisaban una licencia virreinal para poder trabajar, cuyo precio o "pinción" era destinado a limosnas y obras piadosas. Para las compañías que trabajaban de forma estable en las capitales y ciudades importantes una de sus principales fuentes de ingresos era la participación en las festividades del "Corpus Christi", que les proporcionaba no solo beneficios económicos, sino también reconocimiento y prestigio social. Las representaciones en el palacio virreinal y las mansiones de la aristocracia, donde representaban tanto las comedias de su repertorio como producciones especiales con grandes efectos de iluminación, escenografía y tramoya, eran también una importante fuente de trabajo bien pagada y prestigiosa.

Nacido en el virreinato de Nueva España aunque asentado posteriormente en España, Juan Ruiz de Alarcón es la figura más destacada del teatro barroco novohispano. Pese a su acomodo a la nueva comedia de Lope, se han señalado su «marcado laicismo», su discreción y mesura y una agudísima capacidad de «penetración psicológica» como características distintivas de Alarcón frente a sus coetáneos españoles. Cabe destacar entre sus obras "La verdad sospechosa", una comedia de caracteres que reflejaba su constante propósito moralizante. La producción dramática de Sor Juana Inés de la Cruz la sitúa como segunda figura del teatro barroco hispanoamericano. Cabe mencionar entre sus obras el auto sacramental "El divino Narciso" y la comedia "Los empeños de una casa".

Entre los especialistas se acepta que la música entre los albores del siglo XVII y mediados del siglo XVIII tiene una serie de características que permite clasificarla como un período estilístico, el denominado Barroco en la historia musical occidental. También hay coincidencia en que, aunque el período pueda acotarse entre 1600 y 1750, algunas de las características de esta música ya existían en la Italia de la segunda parte del siglo XVI y otras se mantuvieron en zonas periféricas de Europa hasta finales del siglo XVIII. Algunos autores dividen a su vez el barroco musical en tres subperíodos: temprano, hasta mediados del siglo XVII; medio, hasta finales del siglo XVII; y tardío, hasta las muertes de Bach y Händel.

Se han estudiado paralelismos y similitudes entre los rasgos musicales de esta época con los de las otras artes de este período histórico como la arquitectura y la pintura. Sin embargo, otros autores estiman excesivas esas analogías, prefiriendo señalar los rasgos estilísticos únicamente musicales que pueden ser calificados de barrocos simplemente por ser contemporáneos de las artes plásticas y la literatura, y por tener una unidad espiritual y artística con el período post-Renacimiento.

La música barroca a menudo tenía una textura homofónica, donde la parte superior desarrollaba la melodía sobre una base de bajos con importantes intervenciones armónicas. La polaridad que resultó del triple y del bajo llevó desde la transición entre los siglos XVI y XVII al uso habitual del bajo continuo: una línea de bajo instrumental sobre la que se improvisaban en acordes los tonos intermedios. El bajo continuo era una línea independiente que duraba toda la obra, por eso recibe el nombre de continuo. Apoyado en la base del bajo se improvisaban melodías mediante acordes con un instrumento que los pudiese producir, normalmente un teclado. Estos acordes se solían especificar en el pentagrama mediante números junto a las notas del bajo, de allí el nombre de bajo cifrado. El bajo continuo fue esencial en la música barroca, llegándose a denominar la «época del bajo continuo».

Entre los muchos compositores barrocos destacan los italianos Claudio Monteverdi, Arcangelo Corelli, Alessandro Scarlatti, Domenico Scarlatti, Antonio Vivaldi y Tommaso Albinoni; los franceses Jean-Baptiste Lully, François Couperin, Jean Philippe Rameau y Marc-Antoine Charpentier; los alemanes Heinrich Schütz, Georg Philipp Telemann, Johann Pachelbel y Johann Sebastian Bach; y los ingleses Henry Purcell y Georg Friedrich Händel (alemán de nacimiento).

Para Francisco Camino, en los 150 años de este período la música occidental cobró un gran impulso, convirtiéndose en una de las artes "más variadas, extendidas y vigorosas". La variedad la aportaban los géneros y formas que se establecieron en este período: aria de capo, cantata, ópera, oratorio, sonata (para tres instrumentos o para uno solo), concierto grosso, concierto para un instrumento solista, preludio, fuga, fantasía, coral, suite y tocata. La extensión geográfica de la música barroca alcanzó a toda Europa desde Italia: la música sonaba en todos los lugares, palacios, teatros, iglesias, conventos, colegios, etc. El vigor de las formas barrocas se siguió expandiendo en los siglos siguientes con una fuerza que hoy todavía continúa.

Fue un período de experimentación dominado por la supremacía de los estilos monódicos en diversos géneros, como el madrigal, el aria para solista, la ópera, el concierto sacro vocal, la sonata para solista o la sonata en trío. Así describe el musicólogo Adolfo Salazar la innovación de la monodia frente a la polifonía: se abandonaron las múltiples combinaciones de voces superpuestas del Renacimiento, la nueva idea de la monodia abogaba por una sola voz protagonista que buscaba que el texto solista y la melodía desnuda se escucharan con claridad expresando los afectos. Acompañando al texto y sustentando la melodía, el bajo continuo con sus acordes se ocupaba de la armonía.

Los compositores abandonaron las formas renacentistas de continuidad melódica y rítmica plana con texturas homogéneas y se decantaron por la discontinuidad dentro de la misma obra. Se buscaba el contraste de diversas formas: entre suave y fuerte, entre solos y tuttis, entre los variados colores vocales o instrumentales, entre rápido y lento, entre diferentes voces e instrumentos.

La nueva estética musical cambió el estilo vocal; buscando ser más expresivo se dejaron de emplear las voces polifónicas renacentistas. La nueva forma se basaba sobre todo en una voz solista. En Venecia comenzó la ópera y se construyeron teatros de ópera financiados por las familias nobles poderosas, lo que favoreció el desarrollo de la misma y el público de la ciudad se volcó en ello. Los compositores experimentaron con el nuevo estilo y, entre ellos, Monteverdi exploró todas las posibilidades del teatro musical, tanto vocales como instrumentales, llegando en sus últimas óperas a desarrollar completamente el género, siendo el primero en dotar a los elementos esenciales (drama, música, acción y expresión) de unidad y cohesión.

La ópera tiene un papel destacado en la cultura desde entonces. En Venecia en el siglo XVII se estima que se estrenaron más de mil óperas y otras mil en el siglo XVIII. En los 400 años de historia de la ópera en Italia se han estrenado unas 30 000 óperas y en el mundo se estiman unos 50 000 estrenos de obras de ópera.

Fue un período de consolidación. La disonancia se acotó de forma más estricta, mientras que el recitativo expresivo desarrollado en el tiempo anterior perdió relieve. Entre las innovaciones de este período medio apareció el estilo vocal belcantista, que fomentó el virtuosismo del cantante. Se desarrolló el lenguaje tonal, fomentando la aparición de nuevas formas y géneros musicales. El contrapunto se volvió a desarrollar, aunque de manera totalmente nueva. Así, la cantata, formada a partir de arias y recitativos, relegó a la monodia lírica. El oratorio y la cantata religiosa en los países protestantes elevaron el concierto sacro. La sonata para solista y para trío alcanzó un modelo estable.

Desde su amplio florecimiento en Italia, la nueva música se difundió por toda Europa. El compositor italiano Jean-Baptiste Lully emigró a Francia y allí adaptó la nueva música y la ópera al gusto francés, dando preferencia en la misma al ballet, que tenía un gran predicamento en la corte francesa. Bajo la dirección de Lully, los instrumentistas de la corte francesa adquirieron un gran nivel técnico y su orquesta de cuerda, que acompañaba sus óperas, era muy admirada en toda Europa. Aunque los instrumentos de cuerda habían dominado el panorama musical, en Francia instrumentos como la trompeta y el oboe tuvieron un importante desarrollo técnico y sus intérpretes impulsaron las posibilidades de estos instrumentos, que fueron incorporados a las orquestas.

En Italia en este período el violín se destacó como el instrumento más importante de la orquesta. Luthiers italianos, como las familias Amati, Stradivarius y Guarneri, perfeccionaron la construcción del violín, estudiaron sus óptimas medidas, el grosor de sus tablas hasta conseguir una sonoridad más potente e intensa manteniendo su calidad. De sus talleres salieron el violín y sus familiares, la viola actual, el violonchelo y el contrabajo. Paralelo al perfeccionamiento en la construcción de la familia de cuerda, la intuición de los compositores mejoró la intensidad y calidad de su sonoridad gracias a nuevas técnicas de ejecución de estos instrumentos. Los principales compositores italianos compusieron fundamentalmente para estos instrumentos de cuerda, tanto en grandes masas orquestales como en pequeños grupos de instrumentos. El concierto y la sonata para instrumento solo o en trío adquirieron un gran desarrollo.

El florecimiento de la música italiana originó que los artistas italianos fuesen reclamados en toda Europa y estos fueron emigrando e instalándose en otros países, difundiendo el estilo de música de su país. A finales del siglo XVII, la música italiana, tanto la instrumental como la vocal, tenía una gran influencia en Europa, con especial énfasis en la ópera. En Austria y Alemania se impuso tanto la música italiana como la francesa, en cambio en Inglaterra predominó la influencia italiana.

En el Barroco tardío, la tonalidad quedó definitivamente establecida mediante normas adquiriendo esquemas más amplios, mientras la armonía se fusionó con la polifonía. Con estas técnicas compositivas las formas alcanzaron grandes dimensiones. También el estilo antiguo de música instrumental y religiosa que se había mantenido durante todo el siglo XVII se renovó en el estilo fugado tonalmente ordenado por J. S. Bach y otros compositores.

La música italiana continuó un desarrollo inmenso, especialmente la ópera. Arcangelo Corelli y otros compositores italianos exploraron y extendieron la música instrumental, que alcanzó en desarrollo a la música vocal. Apareció el concierto para un instrumento solista que los compositores, especialmente Antonio Vivaldi, consolidaron y llevaron a su esplendor. La ópera se enriqueció con el incremento de la participación orquestal y los compositores exploraron las potencialidades expresivas del género. Algunos cantantes de ópera alcanzaron gran fama y eran muy populares, especialmente los "castrati".

La música barroca llegó a su plenitud en las composiciones de Johann Sebastian Bach y Georg Friedrich Händel, junto a la de Domenico Scarlatti, Antonio Vivaldi, Jean Philippe Rameau y Georg Philipp Telemann.

Johann Sebastian Bach, desde la tradición de la iglesia alemana protestante, fusionó los conocimientos musicales de su época. Analizó la obra de los otros compositores copiando y arreglando sus partituras. Así conoció los estilos de los principales compositores de Italia, Francia, Alemania y Austria. De los italianos y, sobre todo, de Vivaldi, aprendió a desarrollar los temas con concisión y en grandes proporciones, así como a ajustar el esquema armónico. Los elementos que asimiló los desarrolló en toda su potencialidad, lo que unido a su maestría en el contrapunto, dio origen a su personal "estilo bachiano". Bach compuso sus obras maestras a partir de 1720, cuando un nuevo estilo forjado en los teatros de ópera italianos se extendía ya por Europa, pareciendo ya anticuada su forma de componer. Por ello el conocimiento completo de su obra debió esperar al siglo XIX.

En cambio Haendel, también alemán de nacimiento pero con una formación musical tanto alemana como italiana, se estableció en Londres y compuso en un lenguaje musical totalmente cosmopolita: óperas italianas, creó el oratorio inglés y dio nuevos significados a otros estilos tradicionales.

La danza no tenía en el siglo XVII la misma consideración de arte que tiene hoy día, y era considerada más bien un pasatiempo, un acto lúdico, aunque con el tiempo fue cobrando protagonismo y empezó a ser considerada como una actividad elevada. Asimismo, si bien en un principio era tan solo un acompañamiento de otras actividades, como el teatro o diversos géneros musicales, progresivamente fue cobrando autonomía respecto a estas modalidades, hasta que en el siglo XVIII se consolidó definitivamente como una actividad artística autónoma. A finales del siglo XVI el principal país donde se otorgaba una cierta importancia a la danza era Francia, con el denominado "ballet de cour", el cual incluso hizo evolucionar la música instrumental, de melodía única pero con una rítmica adaptada a la danza. Aun así, su utilización en la corte francesa era más que nada un acto propagandístico con el que demostrar la magnificencia de la realeza, o con que agasajar a visitantes y diplomáticos, y donde se valoraban más la escenografía, el porte y la elegancia que la coreografía o la habilidad física.

Sin embargo, a principios del siglo XVII el epicentro de la danza varió de Francia a Inglaterra, donde fue favorecida por los Tudor —y posteriormente los Estuardo— con un tipo de espectáculo llamado "masque", donde se conjugaba la música, la poesía, el vestuario y la danza. Una variante de esta modalidad fue la "antimasque", aparecida en 1609 como un complemento a la anterior, donde frente al canto y al diálogo se desarrolló un tipo de espectáculo donde predominaba la actuación y el gesto, el movimiento puramente coreográfico. Con el tiempo, la "antimasque" se separó de la "masque" y pasó a ser un espectáculo autónomo, poniendo los cimientos de la danza moderna.

A mediados del siglo XVII, sin embargo, las mayores innovaciones se dieron nuevamente en Francia, gracias sobre todo al patrocinio del rey Luis XIV, así como al mecenazgo del cardenal Mazarino, que introdujo el gusto por la ópera —género recién surgido en Italia—, en cuyas representaciones era habitual la presencia de ballets en los entreactos. Sin embargo, el hecho de que las óperas eran representadas por aquel entonces en italiano hizo que el público francés prefiriese los ballets que acompañaban a las óperas a estas mismas, por lo que poco a poco fueron ganando importancia. De ello se dio cuenta el músico Jean-Baptiste Lully, que empezó una serie de reformas que convirtieron el ballet en un arte escénico, cercano al que conocemos hoy día. Lully fue el autor del "Ballet Royal de la Nuit" (1653), un gran espectáculo que duró trece horas y donde intervino el propio rey caracterizado de Apolo dios del sol —de donde viene su apodo de Rey Sol—.

Luis XIV favoreció la profesionalización de la danza, para lo que creó la Academia real de Danza en 1661, la primera de esta modalidad en el mundo. En ella desarrolló su labor Pierre Beauchamp, quizá el primer coreógrafo profesional, creador de la "danse d'école", el primer sistema pedagógico de la danza. Beauchamp introdujo el "en dehors" —la rotación de las piernas hacia fuera, uno de los pasos tradicionales del ballet clásico—, así como las cinco posiciones de los pies, que varían en diferentes grados de apertura respecto al eje central del cuerpo. Por otro lado, la Academia favoreció la transformación del ballet en grandes espectáculos donde, además de la danza, destacaban los elementos dramático y musical. Así como el principal referente musical fue, como se ha visto, Lully, a nivel dramático jugó un papel esencial Molière, creador del "comédie-ballet", un género de danza inspirado en la "commedia dell'arte" italiana. Por último, cabría mencionar a Raoul Auger-Feuillet, que en 1700 desarrolló un nuevo sistema de notación de danza, gracias al cual han sobrevivido numerosas coreografías de la época.






</doc>
<doc id="8166" url="https://es.wikipedia.org/wiki?curid=8166" title="Síntoma">
Síntoma

En el ámbito de las ciencias de la salud, un síntoma es la referencia subjetiva que da un enfermo de la percepción que reconoce como anómala o causada por un estado patológico o una enfermedad, a diferencia de un signo, que es un dato "objetivo", observable por parte del especialista.

El síntoma es un aviso útil de que la salud puede estar amenazada sea por algo psíquico, físico, social o combinación de las mismas.

La palabra "síntoma" fue heredada al español a través del latín desde el griego ("sýmptōma"), y en realidad es un sustantivo creado a partir del verbo ("sympíptō"), que literalmente significa "caer al mismo tiempo" y, en un sentido más amplio, "concurrir", "ocurrir al mismo tiempo". Galeno hablaba del síntoma como una situación distinta de la enfermedad, los síntomas como "sombras que acompañan a la enfermedad".

El término «síntoma» no se debe confundir con el término «signo», ya que este último es un dato objetivo y objetivable.

En medicina y en las ciencias de la salud en general, se entiende por signo clínico cualquier manifestación objetivable consecuente a una enfermedad o alteración de la salud, y que se hace evidente en la biología del enfermo.

La semiología clínica es la disciplina de la que se vale el médico para indagar, mediante el examen psicofísico del paciente, sobre los diferentes signos que puede presentar.

Un signo clínico es un elemento clave que el médico puede percibir en un examen físico, en contraposición a los síntomas que son los elementos subjetivos, percibidos sólo por el paciente.

Ejemplos de signos clínicos:

Ejemplos de síntomas:


Desde el punto de vista del psicoanálisis, el síntoma es una formación de compromiso (junto con el sueño), el chiste y los actos fallidos (errores al hablar o al escribir) entre el sistema consciente y el sistema inconsciente.



</doc>
<doc id="8168" url="https://es.wikipedia.org/wiki?curid=8168" title="Diwali">
Diwali

El Diwali (también Divali, Deepavali, Deepawali o "festival de las luces"), es un festival hindú que dura cinco días que se celebra en el mes de Kartika. 
El festival comienza en el día denominado Dhanteras, que se celebra el décimo tercer día lunar de Krishna Paksha (cuarto menguante) del mes Ashvin del calendario hindú y finaliza en Bhau-beej, celebrado el segundo día lunar de Shukla Paksha (cuarto creciente) del mes Kartik Dhanteras, y por lo general cae dieciocho días después de Dussehra. En el calendario Gregoriano, Diwali cae entre mediados de octubre a mediados de noviembre. Festividades similares son celebradas por los miembros de varias religiones en India, como el hinduismo, el sijismo y el jainismo.

Durante el Diwali, celebrado una vez al año, la gente estrena ropa nueva, comparte dulces y hacen explotar petardos y fuegos artificiales. Es la entrada del año nuevo hindú, y una de las noches más significativas y alegres del año.

La divinidad que preside esta festividad es Lakshmí, consorte del dios Vishnú. Ella es quien otorga la prosperidad y la riqueza, por eso es especialmente importante para la casta de los comerciantes "(vaisyas)". También el dios Ganesha es especialmente venerado ese día. En el este del país se venera particularmente a la diosa Kali.

En esa ocasión, los sikhs celebran la liberación de su sexto gurú, Hargonbind, y hacen un homenaje a los diez gurús espirituales del sikhismo.

La fiesta tiene lugar en el decimoquinto día de la quincena oscura del mes de kārttika (que cada año puede caer entre el 21 de octubre y el 18 de noviembre), y puede durar cuatro o cinco días. Conmemora la muerte del demonio Narakasura a manos de Krishna y la liberación de dieciséis mil doncellas que éste tenía prisioneras. Celebra también el regreso a la ciudad de Ayodhyā del príncipe Rāma tras su victoria sobre Rāvaṇa, rey de los demonios. Según la leyenda, los habitantes de la ciudad llenaron las murallas y los tejados con lámparas para que Rāma pudiera encontrar fácilmente el camino. De ahí comenzó la tradición de encender multitud de luces durante la noche.

Las casas se limpian de forma especial y se adornan con diversos motivos y lámparas de aceite o velas que se encienden al atardecer. Es usual celebrar una comida compuesta de sabrosos platos y dulces, hacer regalos a las personas cercanas y familiares, los fuegos artificiales y los juegos. Es el momento para renovar los libros de cuentas, hacer limpieza general, reemplazar algunos enseres del hogar y pintarlo y decorarlo para el año entrante. Es tradición que la diosa favorecerá de forma especial a quienes se reconcilien con sus enemigos.

Se aconseja instalar un altar en un lugar preferente de las casas donde este presente una imagen de Lakshmí a la que se le ofrecerán flores, incienso y monedas mientras se repite el mantra:

Al anochecer se abren todas las ventanas y puertas de las casas y en cada una de ellas se realiza un ofrecimiento de luz con una lámpara de aceite o una vela, repitiendo el mismo mantra, para que Lakshmí entre para el resto del año. También se lanzan barcos de papel o lamparillas encendidas a los ríos sagrados, cuanto más lejos vayan, mayor será la felicidad en el año venidero y se elaboran unos diseños llamados "manorā", que son unos dibujos hechos en las paredes y que se adornan durante el festival. A la salida del sol es de ritual lavarse la cabeza, lo que tiene el mismo mérito que bañarse en el sagrado río Gangā (el Ganges).

El simbolismo de la fiesta consiste en la necesidad del hombre de avanzar hacia la luz de la Verdad desde la ignorancia y la infelicidad, es decir, obtener la victoria del dharma (la virtud) sobre adharma (falta de virtud).

Según los jainistas, en este día de Diwali el Tirthankara Mahavirá alcanzó la liberación (moksha/nirvana/siddha) (549 – 477 a. C.). Mahavira es conocido como el creador del jainismo.


</doc>
<doc id="8171" url="https://es.wikipedia.org/wiki?curid=8171" title="Río">
Río

Un río es una corriente natural de agua que fluye con continuidad. Posee un caudal determinado, rara vez es constante a lo largo del año, y desemboca en el mar, en un lago o en otro río, en cuyo caso se denomina afluente. La parte final de un río es su desembocadura. Algunas veces terminan en zonas desérticas donde sus aguas se pierden por infiltración y evaporación por las intensas temperaturas.

Por lo general los ríos, especialmente los más grandes, se dividen en tres partes principales de acuerdo con su capacidad erosiva y de transporte de sedimentos:

El curso superior de un río es donde estos nacen. Generalmente, coincide con las áreas montañosas de una cuenca determinada. Aquí el potencial erosivo es mucho mayor y los ríos suelen formar valles en forma de V al encajarse en el relieve. Cuando esta parte de un río se encuentra en un clima seco pueden denominarse a veces barrancos, ramblas o torrentes.

Generalmente, en el curso medio de un río suelen alternarse las áreas o zonas donde el río erosiona y donde deposita parte de sus sedimentos, lo cual se debe, principalmente, a las fluctuaciones de la pendiente y a la influencia que reciben con respecto al caudal y sedimentos de sus afluentes. A lo largo del curso medio, la sección transversal del río habitualmente se irá suavizando, tomando forma de palangana seccionada en lugar de la forma de V que prevalece en el curso superior.
A lo largo del curso medio, el río sigue teniendo la suficiente energía como para mantener un curso aproximadamente recto, excepto que haya obstáculos, como por ejemplo diversas curvas o montículos.

Es la parte en donde el río fluye en áreas relativamente planas, donde suele formar meandros: establece curvas regulares, pudiendo llegar a formar lagos en herradura. Al fluir el río, acarrea grandes cantidades de sedimentos, los que pueden dar origen a islas sedimentarias, llamadas deltas y también puede ocasionar la elevación del cauce por encima del nivel de la llanura, por lo que muchos ríos suelen discurrir paralelos al mismo por no poder desembocar por la mayor elevación del río principal: son los ríos tipo Yazoo. De un río que termina en una boca muy ancha y profunda se denomina estuario.

El río principal suele ser definido como el curso con mayor caudal de agua (medio o máximo) o bien con mayor longitud o mayor área de drenaje. Este concepto de río principal, como el de "nacimiento" de un río o la distinción entre río principal y afluente, son arbitrarios.

En muchos casos se presentan dudas acerca del nombre y recorrido de los ríos, sobre todo en cuencas hidrográficas de relieve heterogéneo y de gran extensión, en las que no ha existido un criterio común acerca de las dimensiones del río principal y de sus afluentes. En otros casos, existen varias denominaciones para un mismo río, a lo largo de su recorrido. Ejemplos de ríos cuyos nombres se han discutido con relación a dónde podemos fijar su nacimiento o con afluentes más importantes que el río principal podemos señalar:

En otros casos, un mismo río tiene nombres distintos a lo largo de su recorrido, especialmente en los casos en que se forman brazos o canales a partir de un cauce y cada uno de esos brazos toma un nombre distinto. Es el caso del Apure (Apure Seco, Apure Viejo y Apurito). También el Magro podría incluirse en este caso (rambla de la Torre, río Magro, Alcalá, rambla de Algemesí, etc.)

Los ríos pueden ser clasificados desde diversos puntos de vista, entre los que se destacan:

Algunos ríos cortos y torrentes pueden fluir desde su cabecera o inicio hasta el mar sin convertirse en afluentes o tributarios de otro mayor, ni recibir agua de otros ríos. En general, un río forma parte de una red de drenaje (o sistema fluvial) ocupando una cuenca hidrográfica. Algunas cuencas abarcan pocos kilómetros cuadrados, en cambio la cuenca del Amazonas se extiende a lo largo de 6,14 millones de km² (véase la ).

Las cuencas de los ríos y sus redes de drenaje pueden cambiar de forma natural en periodos relativamente cortos de tiempo como consecuencia de capturas fluviales.

Los ríos erosionan rocas y sedimentos, llegando a abrir cauces y valles, modelando el paisaje en lo que se denomina modelado fluvial. El cauce profundo del río Colorado, ha recortado en algunos lugares hasta una profundidad de 1,5km, formando el Gran Cañón. Y el cañón del río Majes, en el Perú, es todavía más profundo, con unos 3km de profundidad.

Los valles fluviales en general tienen forma de V, sobre todo, en las zonas montañosas de levantamiento reciente, pero esta forma se modifica a lo largo del curso del río, ampliando además su tamaño, pendiente, perfil transversal, capacidad de transporte de sedimentos y otras muchas características.

La flora y fauna de los ríos son diferentes a la que se encuentra en los océanos porque el agua tiene distintas características, especialmente la salinidad. Las especies que habitan los ríos se han tenido que adaptar a las corrientes y a los desniveles. Sin embargo, existen numerosas excepciones, como es el caso de los salmones que desovan en las cuencas superiores o montañosas de los ríos o el de los tiburones de agua dulce de Nicaragua, y también en el caso de las especies marinas que penetran en los deltas oceánicos llevados por la pleamar de las mareas y corrientes oceánicas, tal como sucede en los deltas del Orinoco y del Amazonas. Lo mismo sucede con los estuarios de los ríos, aunque en este caso, la entrada de especies marinas en los ríos suele ser momentánea durante el flujo o pleamar lo cual se debe a que se vacían durante el reflujo o bajamar mientras que en los deltas, lo que cambia durante las mareas es la mayor o menor salinidad de sus aguas.

Algunos peces de agua dulce son: 

El agua es un recurso renovable en peligro por culpa de la actividad humana. Toda el agua pura procedente de las lluvias, ya antes de llegar al suelo recibe su primera carga contaminante, cuando disuelve sustancias como anhídrido carbónico, óxido de azufre y de nitrógeno que la convierten en lluvia ácida. Ya en el suelo, el agua discurre por la superficie o se filtra hacia capas subterráneas. Al atravesar los campos el agua del río se carga de pesticidas y cuando pasa por ciudades arrastra productos como naftas, aceites de automóvil, metales pesados, etc. Los ríos muestran una cierta capacidad de deshacerse de los contaminantes, pero para eso necesitan tener un tramo muy largo en el cual las bacterias puedan realizar su trabajo depurador. En un río contaminado por materia orgánica se distinguen tres zonas a partir del punto de contaminación:

a) Zona polisaprobia: Es la más contaminada. Elevada población de bacterias.

b) Zona mesosaprobia: Contaminación media. Las bacterias ya han eliminado gran parte de la contaminación orgánica.

c) Zona oligosaprobia: El agua está en condiciones similares a las que tenía antes de que se hubiera producido la contaminación.

Resulta difícil medir la longitud exacta de un río debido a las propiedades del terreno por donde fluye. A continuación se listan los 10 mayores ríos del mundo con una longitud aproximada:

Todo Estado ejerce soberanía territorial sobre el curso o porción del curso de un río que forma parte de su territorio.

Existen dos categorías de ríos: 

Los ríos internacionalizados son aquellos en los cuales existe libertad de navegación que según sea más o menos amplia puede ser a favor de todas las banderas o solo de los ribereños.

La policía y administración de un río abierto a la libre navegación suscita numerosos problemas: el régimen aduanero, el pilotaje, los reglamentos de puerto, las tasas, etc. 
La norma general es que en principio la administración de cada sector del río es ejercida por el respectivo ribereño.
Una fórmula que ha llegado a evitar problemas, sobre todo en los ríos europeos donde la concentración de la navegación es muy grande, consiste en el establecimiento de comisiones internacionales de administración fluvial.

Potamología es el estudio de las aguas fluviales (del griego Ποταμός "potamós" = ‘río’), que abarca conceptos como los de su caudal, cauce, cuenca, curso o corriente, régimen fluvial, dinámica fluvial, perfiles (longitudinal y transversal), afluentes y su importancia, ecología, flora, fauna, recursos hídricos e hidroeléctricos o navegación fluvial, entre otros. Vendría a ser una parte de la hidrografía.


</doc>
<doc id="8174" url="https://es.wikipedia.org/wiki?curid=8174" title="VIH/sida">
VIH/sida

La infección por el virus de la inmunodeficiencia humana y el síndrome de inmunodeficiencia adquirida (VIH/sida) son un espectro de enfermedades causadas por la infección causada por el virus de la inmunodeficiencia humana (VIH). Tras la infección inicial, una persona puede no notar síntoma alguno o bien puede experimentar un periodo breve de cuadro tipo influenza. Típicamente, le sigue un periodo prolongado sin síntomas. A medida que la infección progresa, interfiere más con el sistema inmunitario, aumentando el riesgo de infecciones comunes como la tuberculosis, además de otras infecciones oportunistas y tumores que raramente afectan a las personas con un sistema inmunitario indemne. Estos síntomas tardíos de infección se conocen como sida, etapa que a menudo también está asociada con pérdida de peso.

El VIH se contagia principalmente por sexo desprotegido (incluido sexo anal y oral), transfusiones de sangre contaminada, agujas hipodérmicas y de la madre al niño durante el embarazo, parto o lactancia. Algunos fluidos corporales, como la saliva y las lágrimas, no transmiten el VIH. Entre los métodos de prevención se encuentran el sexo seguro, los programas de intercambio de agujas, el tratamiento a los infectados y la circuncisión. La infección del bebé a menudo puede prevenirse al dar medicación antirretroviral tanto a la madre como el niño. No hay ninguna cura o vacuna; no obstante, el tratamiento antirretroviral puede retrasar el curso de la enfermedad y puede llevar a una expectativa de vida cercana a la normal. Se recomienda iniciar el tratamiento apenas se haga el diagnóstico. Sin tratamiento, el tiempo de sobrevida promedio después de la infección es 11 años.

En 2014 aproximadamente 36,9 millones de personas vivían con VIH y causó 1,2 millones de muertes. La mayoría de los infectados viven en el África subsahariana. Entre su descubrimiento y el 2014 el sida ha causado un estimado de 39 millones muertes en todo el mundo. El VIH/sida se considera una pandemia: un brote de enfermedad presente en un área grande y con propagación activa. Sobre la base de estudios genéticos, se ha determinado que el VIH es una mutación del VIS que se transmitió a los humanos entre 1910 y 1930, en el centro-oeste de África. El sida fue reconocido por primera vez por los Centros para el Control y Prevención de Enfermedades de los Estados Unidos en 1981 y su causa (la infección por VIH) se identificó a principios de dicha década.

El VIH/sida ha tenido un gran impacto en la sociedad, tanto enfermedad como fuente de discriminación. La enfermedad también tiene fuertes impactos económicos. Hay muchas ideas equivocadas sobre el VIH/sida como la creencia de que puede transmitirse por contacto casual no sexual. La enfermedad ha sido centro de muchas controversias relacionadas a la religión, incluida la decisión de la Iglesia católica de no apoyar el uso de preservativo como prevención. El VIH/sida ha atraído la atención internacional médica y política así como financiación masiva desde su identificación en los años 1980.

En la siguiente tabla se contemplan los diferentes estados de la infección por VIH.



El VIH se multiplica, después de la fase aguda primaria de la infección, en los órganos linfoides, sobrecargándolos con un esfuerzo que termina por provocar una reducción severa de la producción de linfocitos. El debilitamiento de las defensas abre la puerta al desarrollo de infecciones oportunistas por bacterias, hongos, protistas y virus. En muchos casos los microorganismos responsables están presentes desde antes, pero desarrollan una enfermedad sólo cuando dejan de ser contenidos por los mecanismos de inmunidad celular que el VIH destruye. Ninguna de estas enfermedades agrede sólo a los VIH positivos, pero algunas eran casi desconocidas antes de la epidemia de VIH y en muchos casos las variantes que acompañan o definen al sida son diferentes por su desarrollo o su epidemiología.

La era del sida empezó oficialmente el 5 de junio de 1981, cuando los Centers for Disease Control and Prevention (CDC) —Centros para el Control y Prevención de Enfermedades de Estados Unidos— convocaron una conferencia de prensa donde describieron cinco casos de neumonía por "Pneumocystis carinii" en Los Ángeles. Al mes siguiente se constataron varios casos de sarcoma de Kaposi, un tipo de cáncer de piel. Las primeras constataciones de estos casos fueron realizadas por el Dr. Michael Gottlieb de San Francisco.

Pese a que los médicos conocían tanto la neumonía por "Pneumocystis carinii" como el sarcoma de Kaposi, la aparición conjunta de ambos en varios pacientes les llamó la atención. La mayoría de estos pacientes eran hombres homosexuales sexualmente activos, muchos de los cuales también sufrían de otras enfermedades crónicas que más tarde se identificaron como infecciones oportunistas. Las pruebas sanguíneas que se les hicieron a estos pacientes mostraron que carecían del número adecuado de un tipo de células sanguíneas llamadas T CD4+. La mayoría de estos pacientes murieron en pocos meses.

Por la aparición de unas manchas de color rosáceo en el cuerpo del infectado, la prensa comenzó a llamar al sida, la «peste rosa», causando una confusión, atribuyéndola a los homosexuales, aunque pronto se hizo notar que también la padecían los inmigrantes haitianos en Estados Unidos, los usuarios de drogas inyectables y los receptores de transfusiones sanguíneas, lo que llevó a hablar de un "club de las cuatro haches" que incluía a todos estos grupos considerados de riesgo para adquirir la enfermedad. En 1982, la nueva enfermedad fue bautizada oficialmente con el nombre de "Acquired Immune Deficiency Syndrome (AIDS)", nombre que sustituyó a otros propuestos como "Gay-related immune deficiency" (GRID).

Hasta 1984 se sostuvieron distintas teorías sobre la posible causa del sida. La teoría con más apoyo planteaba que el sida era una enfermedad básicamente epidemiológica. En 1983 un grupo de nueve hombres homosexuales con sida de Los Ángeles, que habían tenido parejas sexuales en común, incluyendo a otro hombre en Nueva York que mantuvo relaciones sexuales con tres de ellos, sirvieron como base para establecer un patrón de contagio típico de las enfermedades infecciosas.

Otras teorías sugieren que el sida surgió a causa del excesivo uso de drogas y de la alta actividad sexual con diferentes parejas. También se planteó que la inoculación de semen en el recto durante la práctica de sexo anal, combinado con el uso de inhalantes con nitrito llamados "poppers", producía supresión del sistema inmunológico. Pocos especialistas tomaron en serio estas teorías, aunque algunas personas todavía las promueven y niegan que el sida sea producto de la infección del VIH.

La teoría más reconocida actualmente, sostiene que el VIH proviene de un virus llamado «virus de inmunodeficiencia en simios» (SIV, en inglés), el cual es idéntico al VIH y causa síntomas similares al sida en otros primates. Según un estudio publicado en 2014, el virus entraría en los seres humanos por primera vez en los años 20 del siglo XX, en el centro de África.

En 1984, dos científicos franceses, Françoise Barré-Sinoussi y Luc Montagnier del Instituto Pasteur, aislaron el virus de sida y lo purificaron. Robert Gallo, estadounidense, pidió muestras al laboratorio francés, y adelantándose a los franceses lanzó la noticia de que había descubierto el virus y que había realizado la primera prueba de detección y los primeros anticuerpos para combatir a la enfermedad. Después de diversas controversias legales, se decidió compartir patentes, pero el descubrimiento se le atribuyó a los dos investigadores originales que aislaron el virus, y solo a ellos dos se les concedió el Premio Nobel conjunto, junto a otro investigador en el 2008, reconociéndolos como auténticos descubridores del virus, aceptándose que Robert Gallo se aprovechó del material de otros investigadores para realizar todas sus observaciones. En 1986 el virus fue denominado VIH (virus de inmunodeficiencia humana). El descubrimiento del virus permitió el desarrollo de un anticuerpo, el cual se comenzó a utilizar para identificar dentro de los grupos de riesgo a los infectados. También permitió empezar investigaciones sobre posibles tratamientos y una vacuna.

En esos tiempos las víctimas del sida eran aisladas por la comunidad, los amigos e incluso la familia. Los niños que tenían sida no eran aceptados por las escuelas debido a las protestas de los padres de otros niños; éste fue el caso del joven estadounidense Ryan White. La gente temía acercarse a los infectados ya que pensaban que el VIH podía contagiarse por un contacto casual como dar la mano, abrazar, besar o compartir utensilios con un infectado.

En un principio la comunidad homosexual fue culpada de la aparición y posterior expansión del sida en Occidente. Incluso algunos grupos religiosos llegaron a decir que el sida era un castigo de Dios a los homosexuales (esta creencia aún es popular entre ciertas minorías de creyentes cristianos y musulmanes). Otros señalan que el estilo de vida «depravado» de los homosexuales era responsable de la enfermedad. Aunque en un principio el sida se expandió más deprisa a través de las comunidades homosexuales, y que la mayoría de los que padecían la enfermedad en Occidente eran homosexuales, esto se debía, en parte, a que en esos tiempos no era común el uso del condón entre homosexuales, por considerarse que éste era sólo un método anticonceptivo. Por otro lado, la difusión del mismo en África fue principalmente por vía heterosexual.

El sida pudo expandirse rápidamente al concentrarse la atención sólo en los homosexuales, esto contribuyó a que la enfermedad se extendiera sin control entre heterosexuales, particularmente en África, el Caribe y luego en Asia.

Gracias a la disponibilidad de tratamiento antirretrovirales, las personas con VIH pueden llevar una vida normal, la correspondiente a una enfermedad crónica, sin las infecciones oportunistas características del sida no tratado. Los antirretrovirales están disponibles mayormente en los países desarrollados. Su disponibilidad en los países en desarrollo está creciendo, sobre todo en América Latina; pero en África, Asia y Europa Oriental muchas personas todavía no tienen acceso a esos medicamentos, por lo cual desarrollan las infecciones oportunistas y mueren algunos años después de la seroconversión.

El VIH está emparentado con otros virus que causan enfermedades parecidas al sida. Se cree que este virus se transfirió de los animales a los humanos a comienzos del siglo XX. Existen dos virus diferenciados que causan sida en los seres humanos, el VIH-1 y el VIH-2. Del primero la especie reservorio son los chimpancés, de cuyo virus propio, el SIVcpz, deriva. El VIH-2 procede del SIVsm, propio de una especie de monos de África Occidental. En ambos casos la transmisión entre especies se ha producido varias veces, pero la actual pandemia resulta de la extensión del grupo M del VIH-1, procedente según estimaciones de una infección producida en África Central, donde el virus manifiesta la máxima diversidad, en la primera mitad del siglo XX.

La pandemia actual arrancó en África Central, pero pasó inadvertida mientras no empezó a afectar a población de países ricos, en los que la inmunosupresión del sida no podía confundirse fácilmente con depauperación debida a otras causas, sobre todo para sistemas médicos y de control de enfermedades muy dotados de recursos. La muestra humana más antigua que se sepa que contiene VIH fue tomada en 1959 a un marino británico, quien aparentemente la contrajo en lo que ahora es la República Democrática del Congo. Otras muestras que contenían el virus fueron encontradas en un hombre estadounidense que murió en 1969 y en un marino noruego en 1976. Se cree que el virus se contagió a través de actividad sexual, posiblemente a través de prostitutas, en las áreas urbanas de África. A medida que los primeros infectados viajaron por el mundo, fueron llevando la enfermedad a varias ciudades de distintos continentes.

En la actualidad, la manera más común en que se transmite el VIH es a través de actividad sexual desprotegida y al compartir agujas entre usuarios de drogas inyectables. El virus también puede ser transmitido desde una madre embarazada a su hijo (transmisión vertical). En el pasado también se transmitió el sida a través de transfusiones de sangre y el uso de productos derivados de ésta para el tratamiento de la hemofilia o por el uso compartido de material médico sin esterilizar; sin embargo, hoy en día esto ocurre muy raramente, salvo lo último en regiones pobres, debido a los controles realizados sobre estos productos. No es posible para los mosquitos u otros insectos transmitir el VIH.

No todos los pacientes infectados con el virus VIH tienen sida. El criterio para diagnosticar el sida puede variar de región en región, pero el diagnóstico generalmente requiere:


La persona infectada por el VIH es denominada «seropositiva» o «VIH positivo» (VIH+) y a los no infectados se les llama «seronegativos» o «VIH negativo» (VIH–). La mayoría de las personas seropositivas no saben que lo son.

La infección primaria por VIH es llamada «seroconversión» y puede ser acompañada por una serie de síntomas inespecíficos, parecidos a los de una gripe, por ejemplo, fiebre, dolores musculares y articulares, dolor de garganta y ganglios linfáticos inflamados. En esta etapa el infectado es más transmisor que en cualquier otra etapa de la enfermedad, ya que la cantidad de virus en su organismo es la más alta que alcanzará. Esto se debe a que todavía no se desarrolla por completo la respuesta inmunológica del huésped. No todos los recién infectados con VIH padecen de estos síntomas y finalmente todos los individuos se vuelven asintomáticos.

Durante la etapa asintomática, cada día se producen varios miles de millones de virus VIH, lo cual se acompaña de una disminución de las células T CD4+. El virus no sólo se encuentra en la sangre, sino en todo el cuerpo, particularmente en los ganglios linfáticos, el cerebro y las secreciones genitales.

El tiempo que demora el diagnóstico de sida desde la infección inicial del virus VIH es variable. Algunos pacientes desarrollan algún síntoma de inmunosupresión muy pocos meses después de haber sido infectados, mientras que otros se mantienen asintomáticos hasta 20 años.

La razón por la que algunos pacientes no desarrollan la enfermedad y por qué hay tanta variabilidad interpersonal en el avance de la enfermedad, todavía es objeto de estudio. El tiempo promedio entre la infección inicial y el desarrollo del sida varía entre ocho a diez años en ausencia de tratamiento.

El 3 de octubre de 2016, apareció otra posible cura al aplicarle un tratamiento nuevo a un paciente británico, ya que al realizarle exámenes de sangre, no se encontró rastro alguno del virus presente en él. El es parte de una serie de pruebas llevadas a cabo por investigadores de las universidades de Oxford, Cambridge, Imperial College London, University College London y King's College London.

La nueva terapia trabaja en dos fases. En la primera, una vacuna ayuda a que el cuerpo reconozca las células infectadas de VIH para poder destruirlas. En la segunda fase, una nueva droga llamada Vorinostat activa las células T latentes para ser identificadas por el sistema inmune.

Los resultados de las pruebas se espera que estén listos para el 2018.

En los países occidentales el índice de infección con VIH ha disminuido ligeramente debido a la adopción de prácticas de sexo seguro por los varones homosexuales y (en menor grado) a la existencia de distribución gratuita de jeringas y campañas para educar a los usuarios de drogas inyectables acerca del peligro de compartir las jeringas. La difusión de la infección en los heterosexuales ha sido un poco más lenta de lo que originalmente se temía, posiblemente porque el VIH es ligeramente menos transmisible por las relaciones sexuales vaginales —cuando no hay otras enfermedades de transmisión sexual presentes— de lo que se creía antes.

Sin embargo, desde finales de los años 1990, en algunos grupos humanos del Primer Mundo los índices de infección han empezado a mostrar signos de incremento otra vez. En el Reino Unido el número de personas diagnosticadas con VIH se incrementó un 26% desde 2000 a 2001. Las mismas tendencias se notan en EE.UU. y Australia. Esto se atribuye a que las generaciones más jóvenes no recuerdan la peor fase de la epidemia en los ochenta y se han cansado del uso del condón. El sida continúa siendo un problema entre las prostitutas y los usuarios de drogas inyectables. Por otro lado el índice de muertes debidas a enfermedades relacionadas con el sida ha disminuido en los países occidentales debido a la aparición de nuevas terapias de contención efectivas (aunque más costosas) que aplazan el desarrollo del sida.

En países subdesarrollados, en particular en la zona central y sur de África, las malas condiciones económicas (que llevan por ejemplo a que en los centros de salud se utilicen jeringas ya usadas) y la falta de educación sexual debido a causas principalmente religiosas, dan como resultado un altísimo índice de infección (ver sida en África). En algunos países más de un cuarto de la población adulta es VIH-positiva; solamente en Botsuana el índice llega al 35,8% (estimado en 1999, fuente en inglés World Press Review). La situación en Sudáfrica —con un 66% de cristianos y con el presidente Thabo Mbeki, que comparte, aunque ya no de manera oficial, la opinión de los «disidentes del sida»— se está deteriorando rápidamente. Sólo en 2002 hubo casi 4,7 millones de infecciones. Otros países donde el sida está causando estragos son Nigeria y Etiopía, con 3,7 y 2,4 millones de infectados el año 2003, respectivamente. Por otro lado, en países como Uganda, Zambia y Senegal se han iniciado programas de prevención para reducir sus índices de infección con VIH, con distintos grados de éxito.

Las tasas de infección de VIH también han aumentado en Asia, con cerca de 7,5 millones de infectados en el año 2003. En julio de 2003, se estimaba que había 4,6 millones de infectados en India, lo cual constituye aproximadamente el 0,9% de la población adulta económicamente activa. En China, la cantidad de infectados se estimaba entre 1 y 1,5 millones, aunque algunos creen que son aún más los infectados. Por otra parte, en países como Tailandia y Camboya se ha mantenido constante la tasa de infección por VIH en los últimos años.

Recientemente ha habido preocupación respecto al rápido crecimiento del sida en la Europa oriental y Asia central, donde se estima que había 1,7 millones de infectados a enero de 2004. La tasa de infección del VIH ha ido en aumento desde mediados de los 1990, debido a un colapso económico y social, aumento del número de usuarios de drogas inyectables y aumento del número de prostitutas. En Rusia se reportaron 257 000 casos en 2004 de acuerdo a información de la Organización Mundial de la Salud; en el mismo país existían 15 000 infectados en 1995 y 190 000 en 2002. Algunos afirman que el número real es cinco veces el estimado, es decir, cerca de un millón. Ucrania y Estonia también han visto aumentar el número de infectados, con estimaciones de 500 000 y 3700 a comienzos de 2004, respectivamente.

Según el Fondo de las Naciones Unidas para las Mujeres (UNIFEM), a pesar de que la infección del VIH comenzó concentrándose básicamente en hombres, a día de hoy, las mujeres suponen el 50% de las personas infectadas con el VIH. Incluso en regiones como el África Subsahariana, las mujeres representan el 60% del total de la población con VIH.

Los hombres que tienen sexo con hombres corresponden al 2% de la población estadounidense, pero el 55% de los casos de VIH en dicho país en 2013.

Una vez que un individuo contrae el VIH, es altamente probable que en el transcurso de su vida llegue a desarrollar sida. Si bien algunos portadores permanecen en estado asintomático por largos períodos de tiempo, la única manera de evitar el sida consiste en la prevención de la infección por VIH. La única vía para la transmisión del virus es a través de los fluidos corporales como la sangre. Este virus no se puede transmitir a través de la respiración, la saliva, el contacto casual por el tacto, dar la mano, abrazar, besar en la mejilla, masturbarse mutuamente con otra persona o compartir utensilios como vasos, tazas o cucharas. En cambio, teóricamente es posible que el virus se transmita entre personas a través del beso boca a boca, si ambas personas tienen llagas sangrantes o encías llagadas, pero ese caso no ha sido documentado y además es considerado muy improbable, ya que la saliva contiene concentraciones mucho más bajas que por ejemplo el semen.

La infección por VIH por las relaciones sexuales ha sido comprobado de hombre a mujer, de mujer a hombre, de mujer a mujer y de hombre a hombre. El uso de condones de látex se recomienda para todo tipo de actividad sexual que incluya penetración. Es importante enfatizar que se debe usar el condón hecho del material látex, pues otro condón (de carnero) que existe en el mercado, hecho a base de material orgánico, no es efectivo para la prevención. Los condones tienen una tasa estimada del 90-95% de efectividad para evitar el embarazo o el contagio de enfermedades, y usado correctamente, esto es, bien conservado, abierto con cuidado y correctamente colocado, es el mejor medio de protección contra la transmisión del VIH. Se ha demostrado repetidamente que el VIH no pasa efectivamente a través de los condones de látex intactos.

El sexo anal, debido a la delicadeza de los tejidos del ano y la facilidad con la que se llagan, se considera la actividad sexual de más riesgo. Por eso los condones se recomiendan también para el sexo anal. El condón se debe usar una sola vez, tirándolo a la basura y usando otro condón cada vez. Debido al riesgo de rasgar (tanto el condón como la piel y la mucosa de la paredes vaginales y anales) se recomienda el uso de lubricantes con base acuosa. La vaselina y los lubricantes basados en aceite o petróleo no deben usarse con los condones porque debilitan el látex y lo vuelven propenso a rasgarse.

En términos de trasmisión del VIH, se considera que el sexo oral tiene menos riesgos que el vaginal o el anal. Sin embargo, la relativa falta de investigación definitiva sobre el tema, sumada a información pública de dudosa veracidad e influencias culturales, han llevado a que muchos crean, de manera incorrecta, que el sexo oral es seguro. Aunque el factor real de trasmisión oral del VIH no se conoce aún con precisión, hay casos documentados de transmisión a través de sexo oral por inserción y por recepción (en hombres). Un estudio concluyó que el 7,8% de hombres recientemente infectados en San Francisco probablemente recibieron el virus a través del sexo oral. Sin embargo, un estudio de hombres españoles que tuvieron sexo oral con compañeros VIH+ a sabiendas de ello no identificó ningún caso de trasmisión oral. Parte de la razón por la cual esa evidencia es conflictiva es porque identificar los casos de transmisión oral es problemático. La mayoría de las personas VIH+ tuvieron otros tipos de actividad sexual antes de la infección, por lo cual se hace difícil o imposible aislar la transmisión oral como factor. Factores como las úlceras bucales, etc., también son difíciles de aislar en la transmisión entre personas "sanas". Se recomienda usualmente no permitir el ingreso de semen o fluido preseminal en la boca. El uso de condones para el sexo oral (o protector dental para el cunnilingus) reduce aún más el riesgo potencial. El condón que haya sido utilizado ya para la práctica del sexo oral, debe desecharse. En caso de que exista coito posterior, se utilizará un nuevo profiláctico; ya que las microlesiones que se producen en el látex por el roce con las piezas dentarias, permiten el paso del virus.

Se sabe que el VIH se transmite cuando se comparten agujas entre usuarios de drogas inyectables, y éste es uno de las maneras más comunes de transmisión. Todas las organizaciones de prevención del sida advierten a los usuarios de drogas que no compartan agujas, y que usen una aguja nueva o debidamente esterilizada para cada inyección. Los centros y profesionales del cuidado de la salud y de las adicciones disponen de información sobre la limpieza de agujas con lejía. En los Estados Unidos y en otros países occidentales están disponibles agujas gratis en algunas ciudades, en lugares de intercambio de agujas, donde se reciben nuevas a cambio de las usadas, o en sitios de inyecciones seguras.

Los trabajadores médicos pueden prevenir la extensión del VIH desde pacientes a trabajadores y de paciente a paciente, siguiendo normas universales de asepsia o aislamiento contra sustancias corporales, tales como el uso de guantes de látex cuando se ponen inyecciones o se manejan desechos o fluidos corporales, y lavándose las manos frecuentemente.

El riesgo de infectarse con el virus VIH a causa de un pinchazo con una aguja que ha sido usada en una persona infectada es menor de 1 entre 200. Una apropiada profilaxis postexposición con medicamentos anti-VIH logra contrarrestar ese riesgo, reduciendo al mínimo la probabilidad de seroconversión.

Un estudio de 2005 informaba que el estar circuncidado podría reducir significativamente la probabilidad de que un hombre se infecte de una mujer seropositiva por penetración vaginal. Los rumores en este sentido, producidos a partir de trabajos anteriores no concluyentes, han aumentado ya la popularidad de la circuncisión en algunas partes de África. Un trabajo relacionado estima que la circuncisión podría convertirse en un factor significativo en la lucha contra la extensión de la epidemia.

Investigaciones recientes confirmaron que de hecho existen personas más resistentes al Virus, debido a una mutación en el genoma llamada "CCR5-delta 32". Según se cree, habría aparecido hace 700 años, cuando la peste bubónica diezmó a Europa. La teoría dice que los organismos con ese gen impiden que el virus ingrese en el glóbulo blanco. Este mecanismo es análogo al de la peste negra. El VIH se desarrolla en estas personas de manera más lenta, y han sido bautizados como "no progresores a largo plazo".

Después de la sangre, la saliva fue el segundo fluido del cuerpo donde el VIH se aisló. El origen del VIH salivar son los linfocitos infectados de las encías (gingiva). Estas células emigran dentro de la saliva en una tasa de un millón por minuto. Esta migración puede aumentar hasta 10 veces (diez millones de células por minuto) en enfermedades de la mucosa oral, las cuales son frecuentes en un huésped inmunodeficiente (tal como un individuo con infección por VIH). Estudios inmunocitoquímicos recientes muestran que en los pacientes con sida hay una concentración más alta de VIH en los linfocitos salivares que en los linfocitos de la sangre periférica. Esto sugiere que los linfocitos infectados reciben una estimulación antigénica por la flora oral (bacterias en la boca) lo que da lugar a una mayor expresión del virus". (A. Lisec, "Za zivot", izdanje "U pravi trenutak", Dakovo 1994. s.27O-271.)

Edward Green, director del Aids Prevention Research Project de Harvard, asegura que «El preservativo no detiene el sida. Sólo un comportamiento sexual responsable puede hacer frente a la pandemia».
Por otra parte, según algunos estudios, los programas que preconizan la abstinencia sexual como método preventivo exclusivo no han demostrado su utilidad para disminuir el riesgo de contagio del virus en países desarrollados.

En el África subsahariana, y otros países subdesarrollados, se ha mostrado eficaz en la lucha contra el sida el fomento de la monogamia y el retraso de la actividad sexual entre los jóvenes.

Según un estudio publicado en la revista científica especializada Science Translational Medicine, un equipo de investigadores del Servicio de Enfermedades Infecciosas y Sida del Hospital Clínico de Barcelona ha dado un paso más en este camino al presentar una vacuna terapéutica que ha mostrado en los ensayos resultados alentadores.

En las pruebas realizadas a 36 pacientes que seguían una terapia antirretroviral (conocida como TAR), tras la vacunación de prueba "cambió el equilibrio virus / huésped a favor del huésped", o lo que es lo mismo, el virus perdía la batalla de la infección. Según los datos, tras 12 semanas, la reducción de la carga viral gracias a la vacuna era del 90 por ciento, aunque posteriormente el virus se hace resistente y consigue paliar el efecto de la vacuna.

Para conseguir frenar el avance del virus del sida los investigadores pulsaron células dendríticas (aquellos leucocitos que presentan antígenos al sistema inmunitario) de los propios pacientes con VIH y las inactivaron con calor. De este modo, cuando las células dendríticas "presentaban" al virus a los linfocitos encargados de eliminar al agente infeccioso externo, el VIH no conseguía infectar al linfocito (como ocurre normalmente), sino que consigue transmitir adecuadamente el mensaje para activar el sistema inmunitario y terminar con el agente externo infectante.

En 2017 se consiguió que un grupo de vacas generaran anticuerpos contra el virus del sida después de recibir una inyección de proteínas con la molécula BG505 SOSIP.

Actualmente existen medicamentos, llamados antirretrovirales, que inhiben enzimas esenciales, la transcriptasa inversa, retrotranscriptasa o la proteasa, con lo que reducen la replicación del VIH. De esta manera se frena el progreso de la enfermedad y la aparición de infecciones oportunistas, así que aunque el sida no puede propiamente curarse, sí puede convertirse con el uso continuado de esos fármacos en una enfermedad crónica compatible con una vida larga y casi normal. La enzima del VIH, la retrotranscriptasa, es una enzima que convierte el ARN a ADN, por lo que se ha convertido en una de las principales dianas en los tratamientos antirretrovirales.

En el año 2007 la Agencia Europea de Medicamentos (EMEA por sus siglas en inglés) autoriza el fármaco Atripla que combina tres de los antirretrovirales más usuales en una única pastilla. Los principios activos son el efavirenz, la emtricitabina y el disoproxilo de tenofovir. El medicamento está indicado para el tratamiento del virus-1 en adultos.
El común denominador de los tratamientos aplicados en la actualidad es la combinación de distintas drogas antiretrovilares, comúnmente llamada "cóctel". Estos "cócteles" reemplazaron a las terapias tradicionales de una sola droga que sólo se mantienen en el caso de las embarazadas VIH positivas. Las diferentes drogas tienden a impedir la multiplicación del virus y, hacen más lento el proceso de deterioro del sistema inmunitario. El "cóctel" se compone de dos drogas inhibidoras de la transcriptasa inversa (las drogas) AZT, DDI, DDC, 3TC y D4T) y un inhibidor de otras enzimas las proteasas.

Al inhibir diferentes enzimas, las drogas intervienen en diferentes momentos del proceso de multiplicación del virus, impidiendo que dicho proceso llegue a término.
La ventaja de la combinación reside, justamente, en que no se ataca al virus en un solo lugar, sino que se le dan "simultáneos y diferentes golpes". Los inhibidores de la transcriptasa inversa introducen una información genética equivocada" o "incompleta" que hace imposible la multiplicación del virus y determina su muerte. Los inhibidores de las proteasas actúan en las células ya infectadas impidiendo el «ensamblaje» de las proteínas necesarias para la formación de nuevas partículas virales.

En 2010 se comprobó la eliminación del virus de un paciente con leucemia al recibir un trasplante de médula de un donante con una muy rara mutación genética que lo vuelve inmune a una infección con VIH; se recuperó de ambas enfermedades. Siendo una mutación muy rara y una operación con altos riesgos, la posibilidad de que esto se vuelva una solución práctica es casi inexistente de momento. A pesar de los resultados, las operaciones de este tipo exigen dosis de inmunosupresores para toda la vida. El defecto genético en cuestión hace que las células T no expresen el receptor CCR5 o CXCR4 que el virus necesita reconocer para entrar a la célula.

En las personas con enteropatía por el VIH, se ha documentado que la dieta sin gluten produce la mejoría de la diarrea y permite la recuperación de peso. De hecho, las lesiones intestinales halladas en estos casos son similares a las que provoca la enfermedad celíaca.

Según un trabajo elaborado en el año 2007 por científicos de las universidades de Ulm y Hannover, en conjunto con científicos españoles, se ha descubierto una proteína en el semen humano, que facilita la transmisión del virus VIH.

Con frecuencia la cantidad de virus existente en el semen no alcanza los niveles mínimos esperables para que pueda generarse contagio. Sin embargo esta proteína llamada SEVI, desempeña un rol de facilitador para la propagación de la infección, con concentraciones de VIH en semen que de otro modo jamás hubieran producido contagio.

Esta proteína se manifiesta en dos formatos o arquitecturas diferentes. Es la SEVI de estructura amiloidea, la que cuenta con capacidad de convertirse en patógena o mutar sus propiedades biológicas. Esta proteína favorece considerablemente el contagio por semen, facilitando la infección y distribución del virus.

El SEVI actúa concentrando el virus en la superficie de la célula, que luego va a ingresar en forma masiva hacia el citoplasma.




</doc>
<doc id="8182" url="https://es.wikipedia.org/wiki?curid=8182" title="Absceso">
Absceso

Un absceso es una infección e inflamación del tejido del organismo caracterizado por la hinchazón y la acumulación de pus. Puede ser externo y visible, sobre la piel, o bien interno. Cuando se encuentra supurado se denomina apostema.

Los abscesos aparecen cuando se infecta un área de tejido y el cuerpo es capaz de "aislar" la infección y evitar que se extienda. Los glóbulos blancos, que son la defensa del organismo contra algunos tipos de infección, migran a través de las paredes de los vasos sanguíneos al área de la infección y se acumulan dentro del tejido dañado. Durante este proceso, se forma el pus, que es una acumulación de líquidos, glóbulos blancos vivos y muertos (principalmente Polimorfonucleados Neutrófilos, PMNN), tejido muerto y bacterias o cualquier otro material o invasor extraño.

Los abscesos pueden formarse en casi cualquier parte del organismo y pueden ser causados por organismos infecciosos, parásitos y materiales extraños. Los abscesos en la piel son fácilmente visibles, de color rojo, elevados y dolorosos; mientras que los abscesos que se forman en otras áreas del cuerpo pueden no ser tan obvios, pero pueden causar mucho daño si comprometen órganos vitales.

Con frecuencia se obtiene una muestra de líquido del absceso y se le hace un cultivo para determinar los organismos causantes del mismo. Ver los tipos individuales de abscesos.

Se debe buscar asistencia médica si la persona cree tener algún tipo de absceso. No se deben tomar antibióticos si no han sido prescritos por un médico.

Generalmente el médico, si es necesario, drena el absceso mediante técnicas quirúrgicas y luego administrará un antibiótico contra la infección.

La mayoría de los tipos de abscesos son tratables.

La prevención de los abscesos depende de su localización y causa. Por ejemplo, una buena higiene es importante para la prevención de los abscesos cutáneos y con la higiene dental y los cuidados de rutina se previenen los abscesos dentales.

Absceso cerebral

Celulitis (inflamación)

Empiema


</doc>
<doc id="8183" url="https://es.wikipedia.org/wiki?curid=8183" title="Arteria">
Arteria

Una arteria es cada uno de los vasos que llevan la sangre desde el corazón hacia los capilares del cuerpo. Nacen de un ventrículo; sus paredes son muy resistentes y elásticas.
Etimología: el término "arteria" proviene del griego "ἀρτηρία", «tubo, conducción (que enlaza)».

El sistema circulatorio, compuesto por arterias y venas, es fundamental para mantener la vida. Su función es la entrega de oxígeno y nutrientes a todas las células, así como la retirada del dióxido de carbono y los productos de desecho, el mantenimiento del pH fisiológico, y la movilidad de los elementos, las proteínas y las células del sistema inmunitario. En los países desarrollados, las dos causas principales de fallecimiento, el infarto de miocardio y el derrame cerebral, son ambos el resultado directo del deterioro lento y progresivo del sistema arterial, un proceso que puede durar muchos años. (Ver aterosclerosis).

Las arterias son conductos membranosos, elásticos, con ramificaciones divergentes, encargados de distribuir por todo el organismo la sangre expulsada de las cavidades ventriculares del corazón en cada sístole.

Cada vaso arterial consta de tres capas concéntricas:

Los límites entre las tres capas están generalmente bien definidos en las arterias. Las arterias presentan siempre una lámina elástica interna separando la íntima de la media, y (a excepción de las arteriolas) presentan una lámina elástica externa que separa la media de la adventicia. La lámina elástica externa se continúa a menudo con las fibras elásticas de la adventicia.

En la circulación general o sistémica, la sangre que sale impulsada del corazón pasa a través de un sistema de vasos arteriales de diámetro cada vez más reducido, hasta llegar a los tejidos, para volver después al corazón a través del sistema venoso. En esquema, el ciclo se puede resumir como sigue:

Además de en el diámetro, los distintos vasos presentan diferencias en la composición de las tres capas.

Conforman las grandes arterias, como la aorta, la arteria pulmonar, la carótida, la arteria subclavia o el tronco braquiocefálico. En este caso, la media está formada por una sucesión de láminas elásticas concéntricas, entre las que se disponen las células musculares lisas. Las láminas elásticas externa e interna son más difíciles de distinguir que en las arterias musculares, debido a la importancia del componente elástico de la media. El predominio de componentes elásticos es fundamental para la propiedad pulsátil de las arterias.

Constituyen las arterias pequeñas y medianas del organismo. La media forma una capa compacta, esencialmente muscular, con una fina red de láminas elásticas. No hay lámina externa elástica. Ejemplo: las arterias coronarias.

Son las arterias más pequeñas y contribuyen de manera fundamental a la regulación de la presión sanguínea, mediante la contracción variable del músculo liso de sus paredes, y a la regulación del aporte sanguíneo a los capilares.

De hecho, la regulación principal del flujo sanguíneo global y de la presión sanguínea general se produce mediante la regulación colectiva de las arteriolas: son los principales tubos ajustables en el sistema sanguíneo, donde tiene lugar la mayor caída de presión. La combinación del gasto cardíaco y la resistencia vascular sistémica, que se refiere a la resistencia colectiva de todas las arteriolas del organismo, son los principales determinantes de la presión arterial en un momento dado.

Los capilares son las regiones del sistema circulatorio donde tiene lugar el intercambio de sustancias con los tejidos adyacentes: gases, nutrientes o materiales de desecho. Para favorecer el intercambio, los capilares presentan una única célula endotelial que los separa de los tejidos. Además, los capilares no están rodeados por músculo liso. El diámetro de un capilar es menor que el diámetro de un glóbulo rojo (que normalmente mide 7 micrómetros de diámetro exterior), por lo que a su paso por los capilares, los glóbulos rojos deben deformarse para poder atravesarlos. El pequeño diámetro de los capilares proporciona una gran superficie para favorecer el intercambio de sustancias.

En los distintos órganos, los capilares realizan funciones similares, pero se especializan en una u otra:

El sistema arterial es la porción del sistema circulatorio que posee la presión más elevada. La presión arterial varía entre el pico producido durante la contracción cardíaca, lo que se denomina presión sistólica, y un mínimo, o presión diastólica entre dos contracciones, cuando el corazón se expande y se llena. Esta variación de la presión en las arterias produce el pulso, que puede observarse en cualquier arteria, y que refleja la actividad cardíaca. Las arterias, debido a sus propiedades elásticas, también ayudan al corazón a bombear sangre, generalmente oxigenada, hacia los tejidos periféricos.
Entre los griegos clásicos, las arterias se consideraban como "tubos huecos" responsables del transporte de aire a los tejidos, conectadas a la tráquea. Esta interpretación se debe a que, en los organismos muertos, las arterias se encuentran vacías, porque toda la sangre pasa al sistema venoso.

En la edad media, se consideraba que las arterias transportaban un fluido, denominado "sangre espiritual" o "espíritu vital", diferente del contenido de las venas. Esta teoría se remonta hasta Galeno. En el periodo medieval tardío, la tráquea, y los ligamentos también se denominaban "arterias".

William Harvey describió y popularizó el concepto moderno del sistema circulatorio y las funciones de arterias y venas en el siglo XVII. Aunque el español Miguel Servet describió la circulación pulmonar un cuarto de siglo antes que Harvey naciera, lo escribió en un libro de Teología (Christianismi Restitutio, publicado en 1553), que fue considerado como herejía y le condujo a la hoguera. En consecuencia, casi todas las copias del mismo fueron quemadas excepto tres, que fueron descubiertas décadas más tarde.

Alexis Carrel a principios del siglo XX fue el primero en describir la técnica de sutura de vasos y anastomosis, y realizó con éxito muchos trasplantes de órganos en animales, abriendo así la vía a la moderna cirugía vascular.




</doc>
<doc id="8185" url="https://es.wikipedia.org/wiki?curid=8185" title="Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear">
Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear

La Asociación Internacional de Médicos para la Prevención de la Guerra Nuclear, AIMPGN (en inglés International Physicians for the Prevention of Nuclear War, IPPNW) es una organización creada por médicos y organizaciones médicas soviéticos y estadounidenses en los tiempos de la guerra fría para la prevención de la guerra nuclear y desarme de las armas nucleares. La AIMPGN, que mereció el en 1985 y ahora está presente en más de 60 países, fue fundada en 1980 por los doctores Bernard Lown (Estados Unidos) y Evgueni Chazov (Unión Soviética).

Los programas y campañas prioritarios de la AIMPGN son los siguientes:

La AIMPGN se define como una federación independiente de organizaciones médicas nacionales de 58 países, representando a decenas de miles de médicos, estudiantes de medicina, trabajadores de la salud y ciudadanos comprometidos con la meta común de crear un mundo más seguro y pacífico libre de la amenaza de la aniquilación nuclear.



</doc>
<doc id="8188" url="https://es.wikipedia.org/wiki?curid=8188" title="Acondicionamiento de aire">
Acondicionamiento de aire

El acondicionamiento de aire se define según la normativa española como el "proceso, o procesos,de tratamiento de aire que modifica sus condiciones para adecuarlas a unas necesidades determinadas". Hay multitud de actividades que requieren unas condiciones de aire específicas o determinadas como: laboratorios de metrología y calibración, salas de ordenadores, salas de exposiciones, quirófanos y salas de vigilancia intensiva (UVI), salas blancas en general, fabricación de dulces, fabricación de textiles, etc. Un sinfín de procesos industriales que precisan unas condiciones ambientales fijas, que pueden ser muy diferentes de las condiciones de confort, pero determinantes para la manipulación o la calidad del producto final.

El acondicionamiento del aire se realiza mediante Unidades de Tratamiento de Aire (UTA), que son aparatos modulares en los que en cada módulo se realiza un tratamiento y se agrupan en función de las condiciones finales de aire requeridas. El tratamiento de aire más completo, es la climatización, en la que se necesitan la mayor parte de los módulos existentes, para garantizar las condiciones del bienestar térmico de las personas. Es, probablemente, por esta razón, por lo que las UTAs se conocen normalmente como climatizadores. Los módulos de calor y frío, funcionan con baterías de agua caliente y fría respectivamente, que obtienen de generadores independientes; la producción de agua caliente suele confiarse a calderas y la de agua fría a máquinas frigoríficas llamadas enfriadoras. 

La ciencia que estudia las propiedades de la mezcla aire-vapor de agua y establece las relaciones entre ellas para su cálculo y tratamiento, se llama psicrometría. Las fórmulas establecidas por la misma, facilitan también la construcción de diagramas de aire húmedo que facilitan el cálculo y proporcionan un resultado visual de la transformación.

La primera unidad de aire acondicionado eléctrica moderna fue inventada por Willis Carrier en 1902. en Buffalo (New York).

El módulo situado en cabeza de cualquier UTA, es siempre un ventilador que mueve un caudal másico de aire formula_1 tomado del ambiente a tratar, lo hace pasar por todos los módulos instalados en su aspiración y lo impulsa, ya tratado, de nuevo al ambiente.

En aquellas instalaciones en las que existe una amplia red de retorno o en aquellas en las que existe enfriamiento gratuito (free-cooling) del aire, se instalan dos ventiladores; uno en la impulsión y otro en el retorno, que suelen ser del mismo caudal y con una presión disponible correspondiente a la pérdida de carga de la parte de red de distribución a la que abastecen.

La función de filtrado se cumple en el "módulo de filtración" y en etapas de filtración instaladas en puntos clave de la distribución. Consiste en tratar el aire mediante filtros adecuados a fin de quitarle polvo, impurezas y partículas en suspensión. El grado de filtrado necesario dependerá del tipo de instalación de acondicionamientos a efectuar.
Para la limpieza del aire se emplean filtros que normalmente son del tipo mecánico, compuestos por sustancias porosas por las que se obliga a pasar al aire y en las que deja las partículas que lleva en suspensión.
En las instalaciones comunes de confort se usan filtros de poliuretano, lana de vidrio, microfibras sintéticas o metálicas de alambre con tejido de distinta malla de acero o aluminio embebidos en aceite. 
El filtro es el primer elemento, y muy comúnmente, también el último a instalar en la circulación del aire, porque no solo protege a los locales acondicionados, sino también al mismo equipo de acondicionamiento.

En el calentamiento sensible, el aire pasa a través del módulo de calefacción, que consiste en una batería por la que circula agua generalmente procedente de una caldera. En el paso, el aire aumenta su temperatura de formula_2 a formula_3 y su entalpía sin modificar la humedad específica, de tal forma que a la entrada:
Y a la salida:

Restando miembro a miembro:

es la energía térmica recibida por cada kg de aire, para pasar de formula_8 a formula_9 . La humedad máxima correspondiente a formula_10 habrá aumentado, por lo que su humedad relativa habrá disminuido.
La potencia de la batería de calor será el producto de la energía recibida por kg, multiplicada por el caudal másicoformula_11.

En los sistemas con expansión directa, la batería de agua caliente se sustituye por el condensador de una máquina frigorífica o bomba de calor, de forma que es la condensación del refrigerante la que aporta el calor necesario, por intercambio directo con el aire.

En el "calentamiento sensible", la disminución de la humedad relativa unida a la recirculación continua de un caudal formula_13, sin ningún tipo de renovación, hacen que al cabo de no mucho tiempo, el aire se deteriore, y sobre todo en el acondicionamiento de confort (climatización), que el usuario sienta la sequedad y el enrarecimiento del aire.
Debido a esto, las instalaciones solo calefacción no son las más aconsejables. Una mejora del sistema es incorporar ventilación, es decir, una entrada de una cantidad de aire exterior que diluya la concentración de contaminantes en el aire.

Si se añade una cantidad de aire exterior, en algún punto se produce una mezcla de dos flujos de aire con diferentes condiciones. El aire nuevo del edificio o aire de ventilación penetra a través de una reja de toma de aire, ubicada en el exterior, en un recinto llamado "módulo de mezcla", en él se mezcla el aire nuevo con el aire de retorno de los locales, regulándose sus caudales respectivos mediante "persianas" de accionamiento manual o automático.

Efectuando un balance de materia y de energía se obtiene:
Por conservación de la masa:

Por conservación de cantidad de vapor o balance de agua:
Donde formula_16 es la razón de humedad (kg vapor de agua/kg) de las corrientes.

Por conservación de energía:
Dondeformula_18 es la entalpía en (J/kg) de las corrientes.

De estas tres ecuaciones se obtiene:
También se utiliza con una aproximación bastante buena:

En el enfriamiento sensible, el aire pasa a través del módulo de refrigeración, que consiste en una batería por la que circula agua procedente de una enfriadora. En el paso, el aire disminuye su temperatura de formula_21 a formula_22 y su entalpía disminuye sin modificar la humedad específica. Para que ocurra el proceso de esta forma, es necesario que la temperatura de la batería, o del agua que circula por ella, esté por encima de la temperatura de rocío correspondiente al estado 1. De no ser así, habrá condensación y por tanto disminución de la humedad específica.
Por lo demás todo es al contrario que en el calentamiento sensible. La capacidad de absorción de humedad disminuye y por tanto aumenta la humedad relativa.

formula_24 será negativo , ya que formula_25. El signo no tiene más significado que la indicación del sentido del flujo de la energía térmica, que en este caso es extraída en lugar de aportada.

Es la transformación en la que aumenta el contenido de vapor del aire húmedo sin modificar su temperatura seca.
Este proceso se consigue con la aportación de una pequeña cantidad de vapor de agua al aire. La cantidad de vapor añadido por cada kg de aire seco será:
y se suele realizar por un generador de vapor.
Cuando se añade vapor a temperatura igual o superior a 100ºC, el proceso resultante determina una humidificación y ligero calentamiento del aire. En todo caso, la variación de temperatura seca es muy pequeña por lo que en la práctica se considera como un proceso isotérmico.
En este proceso hay una mezcla de dos masas, la del aire formula_27 y la del vapor de agua formula_28, con lo que la suma de ellos será:
Haciendo un balance de cantidad de agua se tiene que:
Sustituyendo : 
Por tanto:
De una manera menos precisa se puede calcular con caudales volumétricos, es decir:
Es la transformación inversa a la anterior y se puede realizar con algún tipo de producto desecante o absorbente que elimine la humedad del aire sin variar su temperatura, como gel de sílice.

Cuando se hace pasar una corriente de aire húmedo por una superficie fría que se mantiene a una temperatura inferior a la de rocío del mismo, condensará una parte del agua que contiene y por tanto disminuirá su humedad. En condiciones ideales, el aire abandonaría el sistema con una temperatura seca igual a la temperatura de la batería y con una humedad relativa del 100%.
La temperatura de rocío de la batería, formula_34, es la temperatura del aire tratado a la salida de la misma y coincide muy aproximadamente con la temperatura del fluido frío que circula por el interior de los tubos, debido a la pequeña resistencia térmica de la pared de los mismos.
De la entalpía que pierde el aire, una parte será calor sensible (disminución de la temperatura seca) y otra parte será calor latente (disminución de vapor de agua).
El calor total eliminado en la batería es el extraído del aire, más el calor que lleva el agua condensada. Este segundo término se desprecia porque suele ser muy pequeño:

Un proceso es adiabático cuando se realiza sin intercambio de calor con el medio exterior. Ocurre cuando un flujo de aire atraviesa una cortina de agua. El agua, impulsada por una bomba, se pulveriza en el aire y vaporiza parte de ella, aumentando la humedad específica del aire. Si la cámara fuera lo suficientemente larga el aire saldría saturado. La temperatura del agua deberá ser igual a la temperatura húmeda del aire.
El aire evoluciona manteniéndose constante su temperatura húmeda. Esto implica que el aire sale prácticamente con la misma entalpía que entró, aunque con un mayor contenido de humedad específica. El aire se enfría porque cede el calor necesario para la vaporización del agua que se incorpora al aire como calor latente. 

En la práctica las cámaras nunca son tan largas como para que el aire salga completamente saturado, por lo que la humidificación terminará en algún punto antes de llegar a la saturación. Considerando que el aire entra en la cámara en la condición 1 y sale en la 2, la eficiencia de la humidificación será:

siendo formula_37 la temperatura húmeda del aire e igual a la temperatura del agua y formula_38 y formula_39, las condiciones en la saturación.
Esta ecuación proporciona las condiciones de salida del aire a su paso a través de una cortina de agua que se encuentra a una temperatura formula_40 y tiene una eficiencia formula_41.
La humidificación de una corriente de aire mediante agua líquida pulverizada se consigue con los módulos llamados lavadores. También se consigue aunque con menos eficiencia, mediante un módulo con un panel adiabático.

En las zonas o espacios que requieren ambiente controlado, es indispensable un buen diseño y funcionamiento del sistema de tratamiento de aire. Temperatura, presión, humedad, limpieza y calidad de aire, así como su distribución y velocidad en el ambiente tratado, son parámetros que deben ser controlados para alcanzar y mantener las condiciones especificadas.

Las zonas de ambiente controlado pueden tener usos diversos y requerimientos muy especiales: Zonas limpias, zonas estériles, zonas de seguridad biológica, zonas antideflagrantes, etc... El sistema debe cumplir la normativa especificada para cada uso, sin perjuicio de las necesidades y características requeridas por los tratamientos de cada instalación. El control de las presiones diferenciales y del escalado de las mismas, creando sobrepresiones o depresiones en distintas zonas, permite reducir la introducción o retención de cualquier tipo de contaminación: microbiológica, por partículas de polvo, cruzada entre productos, o cualquier otra contaminación externa, incluida la que pueden producir los propios operarios. Por otra parte, los sistemas de distribución y de extracción de aire deben estar diseñados para conseguir un barrido máximo del ambiente, minimizando la retención de partículas en suspensión.
Cada vez más, el consumo energético de la instalación es otro de los factores relevantes a considerar, no solo desde el punto de vista económico, sino también de la eficiencia energética.

Los equipamientos propios de estas instalaciones son:


Todo este equipamiento lleva asociado un sistema de control que permite gestionar y visualizar el estado de las variables que son determinantes para la funcionalidad del proceso. Este sistema de control gestiona los ciclos de funcionamiento de los procesos, registrando o visualizando los valores de cada variable.
De esta manera, se obtiene el control directo de cada uno de los parámetros de la instalación, proporcionando en tiempo real la información de lo que está pasando, pudiéndose tomar decisiones sobre cada uno de ellos, tales como; selección de las condiciones interiores, fijación de consignas o parámetros de funcionamiento, temporizaciones, etc.
Adicionalmente a la optimización del proceso, es conveniente adoptar un "sistema de gestión integral" que posibilite la operación y regulación en toda la instalación del consumo energético, así como una disminución de los costos de mantenimiento.





</doc>
<doc id="8192" url="https://es.wikipedia.org/wiki?curid=8192" title="COP">
COP

COP puede referirse a:


</doc>
<doc id="8196" url="https://es.wikipedia.org/wiki?curid=8196" title="Ludopatía">
Ludopatía

La ludopatía consiste en un trastorno en el que la persona se ve obligada, por una urgencia psicológicamente
incontrolable, a jugar y apostar, de forma persistente y progresiva, afectando de forma negativa a la vida personal, familiar y vocacional. Aunque en anteriores ediciones del manual diagnóstico DSM había sido clasificado como un trastorno del control de los impulsos, ha sido conceptualizado y tratado como una adicción sin sustancia, hasta que en el DSM-V ha sido incluido finalmente dentro de la categoría de "Trastornos relacionados con sustancias y trastornos adictivos".

El juego patológico se clasifica en el DSM-IV-R en trastornos del control de los impulsos, que también incluyen la cleptomanía, piromanía y tricotilomanía, en los que estaría implicada la impulsividad, pero no presenta comorbilidad con dichos trastornos. Si bien el sistema DSM (III, III-R y IV1) y la CIE-102 incluye este trastorno entre las alteraciones debidas a un bajo control de los impulsos, lo cierto es que los criterios diagnósticos operativos DSM tienen exactamente el mismo diseño que el de las adicciones a sustancias, lo que muestra la concepción subyacente para la enfermedad en ese sistema: se trata de un problema adictivo "sin sustancia" incluido en un apartado diferente al suyo.
La ludopatía se caracteriza fundamentalmente porque existe una dificultad para controlar los impulsos, y que en cierto sentido tiende a manifestarse en practicar, de manera compulsiva, uno o más juegos de azar. Puede afectar en la vida diaria de la persona que se ve afectada por esta adicción, de tal forma que la familia, el sexo o incluso la alimentación pasa a ser algo totalmente secundario. Por todo ello, no se debe de confundir la ludopatía con un vicio, ya que en estos casos nos encontramos ante una grave enfermedad crónica, una adicción.

El juego patológico fue reconocido oficialmente como entidad nosológica de salud mental en el año 1980 cuando la Sociedad Americana de Psicología (APA) lo incluye por primera vez como trastorno en el Manual Diagnóstico y Estadístico de los Trastornos Mentales, en su tercera edición (DSM-III).

De acuerdo con el DSM-IV, el juego patológico se define actualmente de manera separada a la de un episodio maniaco. Sólo cuando el juego se da de forma independiente de otros trastornos impulsivos, del pensamiento o del estado de ánimo se considera como una patología aparte. Para recibir el diagnóstico, el individuo debe cumplir al menos cinco de los siguientes síntomas:


Según Becoña, las fases de la adicción son tres:

1. Etapa Dorada: El jugador es más consciente de lo que gana que de lo que gasta.

2. Etapa de desesperación: El jugador se percata de lo perdido.

3. Aceptación: El jugador toma conciencia de su problema.

De acuerdo con el Illinois Institute for Addiction Recovery, las últimas evidencias indican que el juego patológico es una adicción similar a las químicas. Se ha visto que algunos jugadores patológicos tienen menores niveles de norepinefrina que los jugadores normales.

De acuerdo con un estudio dirigido por Alec Roy, M.D., antiguo miembro del National Institute on Alcohol Abuse and Alcoholism, la norepinefrina se secreta en condiciones de estrés o amenaza, de modo que los jugadores patológicos juegan para elevar sus niveles.

Abundando en esto, de acuerdo con un informe de la Harvard Medical School Division on Addictions se generó un experimento en el que a los sujetos se les presentaban situaciones en las que podían ganar o perder en un entorno que simulaba un casino. Las reacciones de los sujetos se medían utilizando RMNf, una técnica de neuroimagen muy similar a la Resonancia magnética nuclear. Y de acuerdo con el doctor Hans Breiter, codirector del Centro de neurociencia de la motivación y la emoción del Hospital General de Massachusetts, las "recompensas en metálico en un ambiente que reproduce un ambiente de juego produce una activación cerebral muy similar a la que se observa en un adicto a la cocaína recibiendo una dosis."

Las deficiencias de serotonina también pueden contribuir a una conducta compulsiva, lo cual incluye una adicción al juego.

A medida que se acumulan las deudas los afectados pueden recurrir a "soluciones" desesperadas para conseguir dinero para "recuperar" a través del juego, como pequeños hurtos, o pedir nuevos créditos para tapar las deudas más difíciles de ocultar. La existencia del hecho delictivo depende de las circunstancias facilitadoras del medio para cometerlo y de la personalidad base del afectado.

Como consecuencia de la enfermedad, el afectado puede tener depresión, ansiedad, ataques cardíacos (consecuencia del estrés), puede tener ideaciones suicidas por desesperación si no recibe tratamiento.

Por otro lado un número considerable de afectados tiene TDAH.

También se sabe que algunos antiparkinsonianos pueden provocar ludopatía.

En un estudio de 1991 sobre relaciones en varones estadounidenses se encontró que el 10% de los jugadores compulsivos se habían casado tres veces o más. Sólo el 2% de los no jugadores se habían casado más de dos veces.
Un estudio de la Comisión para el juego del Reino Unido, el "British Gambling Prevalence Survey 2007", concluyó que aproximadamente el 0.6% de la población adulta tenía problemas con el juego, el mismo porcentaje que en 1999. La mayor prevalencia de la ludopatía se encontró entre los participantes en apuestas por diferencias (14.7%), Terminales de apuestas fijas e intercambio de apuestas (11.2%)

En el meta-análisis de Shaffer y Hall en 1996 sobre la prevalencia del juego patológico entre adolescentes (de 13 a 20 años) la media estimada para el juego patológico o para graves problemas con el juego oscilaba entre el 4.4% y el 7.4%.

El conocimiento científico disponible parece indicar que la ludopatía es una tendencia interna y que los ludópatas tienden a arriesgar dinero en cualquier juego disponible, más que en uno en particular, generando ludopatía en otros individuos que, de otro modo, serían "normales". No obstante, las investigaciones también indican que los ludópatas en juegos de desarrollo rápido. Por ello es mucho más probable que pierdan dinero en la ruleta o en una máquina tragaperras, en el que los ciclos terminan rápido y existe una constante tentación de jugar una y otra vez o aumentar las apuestas, en oposición a las loterías nacionales, en las que el jugador debe esperar hasta el próximo sorteo para ver los resultados.

Henry Lesieur, un psicólogo del programa de tramiento para jugadores del Hospital de Rhode Island afirma que el 30 por ciento de los beneficios de las máquinas de juego proceden de ludópatas. En un estudio reciente en Cataluña, en 2007, se ha estimado que 76,35% de usuarios de máquinas tragaperras de los bares y restaurantes tenían una problable ludopatia o afectación en el control.

Se ha implicado a los agonistas de la dopamina, en particular el pramipexol (Mirapex) en el desarrollo del juego compulsivo y de otros patrones de conducta con excesos.

Para Isabel Sánchez Sosa, coordinadora de la Asociación de Jugadores Compulsivos de Argentina, "en el país la ludopatía está creciendo muchísimo porque la oferta es impresionante" y en ese sentido aseveró que la presencia de los bingos es una cuestión común en todos los barrios. En la provincia de Buenos Aires, se instalaron 46 bingos en los últimos quince años. A principios de los 90 no había ninguno. 

En Colombia, el psiquiatra Pablo Rodríguez afirma que la ludopatía puede destruir a una persona tanto como el alcoholismo y la drogadicción. Relató muchos casos en que personas perdieron hogares, automóviles, empresas, además de los familiares y amigos. "La ludopatía es una adicción sin fondo. Se puede caer más y más y más". A veces, en los casos extremos, se requiere que el paciente sea internado y medicado. En la mayoría de los casos funcionan los tratamientos ambulatorios, las consultas semanales, las reuniones con Jugadores Anónimos, el afecto pero también la firmeza: la familia no debe pagarle las deudas a un ludópata."

En Chile, Estudio de la U. de Santiago y la Corporación de Juego Responsable dice que el 2,4 % de los jugadores en el país son patológicos y que el 80 % de los ludópatas en Chile son mujeres. A su vez, el 35,2% de este tipo de jugadores se encuentra en el estrato etario que oscila entre 31 a 40 años. Daniel Martínez, psiquiatra y director de Buenas Prácticas de Juego Responsable de la CJR, aclara que los hombres prefieren los juegos de azar y casinos, relacionado al deseo de competencia. En las mujeres, en cambio, se da una mayor atracción a las máquinas tragamonedas, lo que aumenta el porcentaje de mujeres con patologías. "Ellas juegan como una forma de desconectarse de sus emociones. Además, muchas han vivido en función de los hijos y crianza, y cuando los hijos se van se quedan sin actividades y encuentran en el juego una forma de disfrutar y entretenerse. Es una característica del juego el desconectarse rápidamente". 

En España, más de medio millón de personas padecen de ludopatía. Desde que se aprobara la Ley de Regulación del Juego en 2011, el número de casinos y casas de apuestas –online especialmente- ha aumentado exponencialmente. Éste se ha producido debido al incremento de usuarios, pero también al bajo coste que supone mantener un casino o una casa de apuestas "online", si lo comparamos con uno real. Además, el problema de las máquinas "tragaperras" parece haber pasado a un segundo plano y el juego "online" le ha ganado terreno siendo actualmente la segunda causa más frecuente de ludopatía.

Lizbeth García Quevedo, directora de la Coordinación con Entidades Federativas (CONADIC), habló de la ludopatía como una fuerte adicción en México: "Tiene conductas muy similares, por eso algunos expertos lo consideran una adicción porque se parece en las conductas, en los orígenes, algunos factores de riesgo que pueden disparar juego patológico, también pueden disparar consumo de drogas". En México podría haber entre uno y tres millones de personas adictas al juego. "Que estén al pendiente de que están haciendo sus hijos, y que por el otro lado motiven el juego pro activo, el juego sano", comentó Lizbeth García Quevedo. El documento de la Secretaría de Salud destaca que un estudio sobre juego patológico que analizó 46 estudios realizados en Canadá, Estados Unidos, Australia, Suecia, Noruega, Inglaterra, Suiza y España, reveló que la prevalencia de ludopatía es relativamente más elevada entre adolescentes, lo cual traza la continuidad del problema considerando que muchos jugadores patológicos declaran que comenzaron sus conductas de juego a temprana edad.

Según la Organización Mundial de la Salud (OMS), la ludopatía es un trastorno adictivo en el que la persona se siente obligada, por una urgencia psicológicamente incontrolable a jugar y apostar, lo que puede conducir a la destrucción de su vida personal, afectando severamente su entorno familiar. En Panamá, la adicción al juego ha crecido vertiginosamente en los últimos años, llegando a convertirse en un problema grave que afecta a cada vez más personas de numerosos sectores sociales, especialmente a los más pobres y a los adultos mayores. El doctor Carlos Smith, especialista en adicciones y coordinador del Centro de Estudios y Tratamiento de Adicciones, señaló que en el principio el azar puede ser recreacional y placentero, para posteriormente convertirse en una actividad que amenaza la integridad del sujeto que la práctica, y de su entorno socio-familiar.

Según EsSalud, el número de enfermos sube un 33% cada año. 5% de los personas en Perú, están afectados. Aunque en Perú no existen datos certeros para graficar la dimensión del problema, todo indica que tiende a crecer. El Instituto Nacional de Salud Mental (INSM) calcula el 5% de la población de Lima Metropolitana tiene complicaciones asociadas a la ludopatía.

Según cifras del Seguro Social de Salud (EsSalud), la ludopatía en Perú crece a un ritmo de 33% cada año. Sin embargo, los números del INSM indican que el porcentaje puede ser mayor. En el primer semestre de este año, la entidad ha diagnosticado 72 nuevos casos de este trastorno.

En Uruguay, el jugador compulsivo promedio tiene entre 40 y 55 años, revela un estudio. Una encuesta efectuada por la Dirección de Casinos, entre quienes han solicitado ayuda por la ludopatía, señala que el 86% de la enfermedad en la actualidad es causada por las máquinas tragamonedas (o máquinas de azar), seguido de manera lejana en un 10% por la ruleta. Con 1% se encuentran la quiniela, tómbola, 5 de Oro y carreras de caballos, y con 2% los juegos de cartas. Otro dato de importancia que ha arrojado la investigación, es que el 65% de los pacientes ludópatas presentan antecedentes de haberse criado con algún familiar que presentaba problemas con el juego. 

En Venezuela, en los últimos años, ha incrementado el índice de ludopatía, mayormente viéndose afectados las personas jóvenes, en apuestas deportivas denominadas "parley" y carreras de caballo. El psiquiatra César Sánchez Bello, aseguró que ha notado un incremento notable en personas enfermas al juego que van a consultas psiquiátricas por la adicción, en su mayoría por apuestas deportivas, superando a las personas que solicitan atención por adicción a las apuestas y juegos en casinos o bingos. También resaltó la importancia de prevenir que más personas caigan en la ludopatía, lo cual no solo afecta al paciente, sino al núcleo familiar, laboral y amistades.

El instrumento más habitual para detectar una "probable conducta de juego patológico" es el South Oaks Gambling Screen (SOGS) desarrollado por Lesieur y Blume (1987) en el South Oaks Hospital de New York City. Este test es sin duda el instrumento más citado en la literatura científica psicológica. En estos últimos años el uso del SOGS ha decaído debido a las crecientes críticas, entre las que se encuentran las que afirman que sobreestima los falsos positivos.

Los criterios diagnósticos del DSM-IV son una alternativa al SOGS, y se centran en las motivaciones psicológicas subyacentes al problema del juego, y fueron desarrolladas por la American Psychiatric Association. Se compone de diez criterios diagnósticos. Una prueba basada en los criterios del DSM-IV criteria es el National Opinion Research Center DSM Screen for Gambling Problems (NODS). Esta medición es utilizada con bastante frecuencia. El Canadian Problem Gambling Severity Index (PGSI) es otro instrumento de evaluación PGSI se centra en los daños y consecuencias asociadas con la ludopatía.

Existen una gran variedad de tratamientos para el juego patológico que incluyen el consejo, los grupos de autoayuda y la medicación psiquiátrica. Sin embargo, no se considera que ninguno de estos tratamientos sea el más eficaz, y no se ha aprobado ninguna medicación por parte de la FDA para el tratamiento del juego patológico.

Jugadores anónimos es un tratamiento comúnmente utilizado para la ludopatía. Modelado con base en el tratamiento de Alcohólicos Anónimos, utiliza un modelo en 12 pasos que hace hincapié en un enfoque de ayuda mutua.

Se ha visto que un enfoque, la terapia cognitivo-conductual reduce los síntomas y las urgencias relacionadas con el juego. Este tipo de terapia se centra en la identificación de los procesos mentales relacionados con el juego, las distorsiones cognitivas y del ánimo que incrementan la vulnerabilidad al juego incontrolado. Además, esta terapia utilizan técnicas de adquisición de competencias orientadas a la prevención de las recaídas, asertividad y rechazo del juego, resolución de problemas y refuerzo de las actividades e intereses inconsistentes con el juego. 

Existen evidencias de que la paroxetina es eficiente en el tratamiento del juego patológico. Además, para pacientes que sufren la comorbididad del trastorno bipolar y el juego patológico, la administración continuada de litio se ha mostrado eficaz en ensayos preliminares. El fármaco antagonista de los opiáceos conocido como malmefeno también ha resultado exitoso en los ensayos para el tratamiento del juego compulsivo.

En la ficción, Dostoievski escribió la obra "El jugador", en parte autobiográfica. En el psicoanálisis, Sigmund Freud escribió un ensayo basado en esta obra.

En el arte, Michelangelo Merisi da Caravaggio en 1594 realizó su obra "Jugadores de cartas".

En la serie de televisión estadounidense "How I Met Your Mother" se comenta numerosas veces la ludopatía de uno de sus protagonistas principales, Barney Stinson.

En la serie de televisión estadounidense "Los Simpson" el personaje de Marge Simpson sufre de algunas enfermedades, entre ellas la más destacada es la ludopatía que se ve claramente en el capítulo Springfield (Or, How I Learned to Stop Worrying and Love Legalized Gambling) (temporada 5), cuando el Sr. Burns abre un casino en Springfield.

En la octava temporada de la serie de televisión española "Cuéntame cómo pasó", el personaje Antonio Alcántara queda atrapado en el juego del póker y, como consecuencia, llega a deber la cantidad aproximada de 300.000 pesetas de la época. Para zanjar la deuda, recurrirá a su hermano Miguel, quien le dará un dinero ganado por la venta de unas tierras del pueblo. Finalmente salda la deuda en el capítulo 139 pero, aun así, la relación con su mujer Mercedes queda afectada.

En el video juego Left 4 Dead 2 uno de los personajes principales Nick Es un ludópata que estaba detenido, arrestado por fraude.




</doc>
<doc id="8197" url="https://es.wikipedia.org/wiki?curid=8197" title="Psicología médica">
Psicología médica

La psicología médica trabaja desde aspectos tan diversos como la genética y la robótica hasta los conocimientos que están en relación con factores medioambientales e influidos por aspectos económicos, políticos y socio-culturales. No es por lo tanto una sorpresa que esta disciplina, por ejemplo, estudia al individuo a nivel neural, endócrino e inmunológico por un lado y por otro las relaciones en los niveles personales, familiares y sociales, como también utilice la alta tecnología para hacer sus investigaciones, diagnósticos y tratamientos.

Tal como muchas disciplinas, la psicología médica también hace uso de la información de las ciencias sociales. Dentro de ellas, la antropología, la psicología social y la sociología aportan a la psicología médica valiosos datos sobre el funcionamiento de los grupos humanos (la familia, las sociedades, las culturas y sus interacciones con el individuo).

Actualmente, los profesionales de la salud están favorecidos con: (a) los avances del conocimiento cada vez más minuciosos de la estructura y el funcionamiento de las partes que integran el organismo humano; (b) los métodos para identificar las disfunciones de los órganos y determinar su patología; (c) de sus recursos para prevenir y combatir las enfermedades. Estos avances son tan amplios que tienen que aceptar sus limitaciones dentro de su área de servicio y aprovechar de los otros profesionales para formar un equipo que permita revisar al individuo de manera integral.

Hace más de 130 años Claude Bernard dijo: “no hay enfermedades sino enfermos”, sin embargo muchos profesionales de la salud todavía no han asimilado esta frase. Más bien siguen la antigua división teórica cartesiana donde el cuerpo y la mente son tratados como entidades separadas y erróneamente lo aplican en su trato diario con los enfermos en lugar de considerar a la persona como un todo.

La formación del profesional de la salud debe ser integral. Necesita tanto del conocimiento científico como el saber afrontar, comprender y relacionarse con las personas, de como los seres humanos se relacionan entre sí e integrarlo de acuerdo a su salud. El profesional de salud necesita estar alerta que de su actitud hacia las personas también dependerá la facilidad o la dificultad para establecer un diagnóstico preciso y la instauración del tratamiento apropiado de las personas que sirve.

En algún momento psicología médica fue llamada la "psicología para los médicos". Morales Meseguer al hablar de la psicología médica, circunscrita dentro de la práctica médica, suscitaba la cuestión si debía hablarse de una disciplina formal o simplemente de un sector aplicativo de conocimientos y recomendó reflexionar la doble imagen que despierta, aborda los problemas psicológicos que se emplea en la práctica médica, que cabe atribuir a un “saber psicológico” que los factores psíquicos participan en la determinación de las enfermedades humanas y desde luego con su tratamiento.

La psicología médica resulta una empresa atractiva, hay quienes la entienden como una consecuencia a la existencia de la práctica médica (Alonso Fernández, 1989) o como la psicología en la educación médica, en la investigación y en la práctica clínica (Kerejarto, 1978) o como la aplicación de los métodos y conceptos psicológicos a los problemas médicos (Rachman, 1977).

El concepto antiguo de esta disciplina es que trata de aplicar los conocimientos y experiencias de la psicología general a los problemas de la medicina, abarcando todos los aspectos psicológicos de la actividad profesional del médico, la relación médico-paciente y la actitud del individuo o grupo, frente a la enfermedad y otros factores como la biografía personal o familiar, expectativa de muerte, curación o situaciones y conflictos vitales.Tradicionalmente, la psicología médica tenía como meta el preparar al médico en los conocimientos psicológicos con el objeto de que pueda comprender mejor al enfermo. Desde esa perspectiva, la psicología médica tiene dos funciones: formativa e informativa. Formativa: cambios en la personalidad, cambios en las motivaciones, cambios en las actitudes. Informativa: en las teorías de la personalidad, la relación médico-paciente, la personalidad de los médicos, diagnósticos personales y comprensivos, procedimientos psicoterapéuticos.

La psicología médica integra los conocimientos de las ciencias médicas y psicológicas que luego son usados por el profesional en beneficio de la persona. Es distinta a la psiquiatría que estudia los trastornos mentales y el modo en que son prevenidos, diagnósticados y tratados.

El diagnóstico emitido por el profesional experto se basa en el estudio de los signos y síntomas, el rol de los factores biológicos, psicológicos, culturales y sociales que inician o facilitan, mantienen, modifican y/o eliminan una enfermedad, la relación profesional de la salud-paciente y éstos con su medio, el comportamiento del enfermo ante el diagnóstico y el tratamiento, los recursos psicológicos para el tratamiento de la enfermedad.
Actualmente, los psicólogos clínicos (psicólogos clínicos y de la salud, neuropsicólogos clínicos), han ampliado su campo de trabajo así como lo están haciendo otros profesionales de la Salud. De ese modo, actualmente gracias a los estudios post-doctorales en el área de Medicina y Farmacología los psicólogos licenciados pueden ampliar su área de acción a fin de proveer un servicio más completo a sus pacientes.

En inglés, "Medical Psychology" (Médico Psicólogo), se refiere al doctor en psicología clínica, neuropsicología o con otra especialidad en el area la salud que tiene una maestría post doctoral o post PhD y capacidad de prescribir. Sus estudios adicionales son en las áreas de medicina y farmacología. Por parte de psicólogos clínicos en EE. UU. y el Canadá, es el término que emplean estos para diferenciarse de aquellos psicólogos que no pueden prescribir. Muchos de los estados, provincias y territorios están siguiendo el proceso legal que permita a los psicólogos médicos prescribir.

Es importante anotar que Medical Psychology es una sub-especialización dentro de la Psicología Clínica y aceptada por el Association of State and Provincial Psychologists Board (ASPPB) como tal.



</doc>
<doc id="8200" url="https://es.wikipedia.org/wiki?curid=8200" title="Elie Wiesel">
Elie Wiesel

Eliezer Wiesel (en húngaro: Wiesel Lázár; Sighetu Marmației, 30 de septiembre de 1928-Nueva York, 2 de julio de 2016) fue un escritor de lengua yiddish y francesa, de nacionalidad estadounidense, superviviente de los campos de concentración nazis. Dedicó toda su vida a escribir y hablar sobre los horrores del Holocausto, con la firme intención de evitar que se repita en el mundo una barbarie similar. Fue galardonado con el Premio Nobel de la Paz en 1986.

Wiesel nació en 1928 y a los 15 años fue detenido por los alemanes, al igual que los demás judíos de su pueblo, cuando el nazi Ferenc Szálasi tomó el poder por la fuerza derrocando al regente húngaro Miklós Horthy. Sobrevivió a los campos de concentración de Auschwitz y Buchenwald, siendo liberado por las fuerzas aliadas el 11 de abril de 1945. Su padre Shlomo, su madre Sarah y su hermana menor Judith "«Tzipora»" perecieron; sin embargo, sus dos hermanas mayores Hilda y Bea lograron permanecer con vida. Estudió en la universidad de la Sorbona, en París, y posteriormente trabajó en periódicos de Israel, Francia y Estados Unidos, donde se estableció en 1956.

Autor de tres novelas sobre sus vivencias durante aquellos años de represión y muerte ("La noche", "El alba" y "El día", publicadas en español bajo el título de "Trilogía de la noche"), ganó el en 1986.
El 16 de mayo de 1944, la familia Wiesel, como otras tantas familias judías, se embarcó en un tren rumbo al campo de exterminio de Birkenau. "Es la primera parada, luego vienen Auschwitz y Buchenwald. Es noche cerrada, tinieblas exteriores a las que son arrojadas, junto a tantos judíos asesinados o supervivientes, nuestras entrañas de humanidad, nuestro manantial de profunda compasión." "La noche" (1956-1958) fue el título que más fama le dio a Wiesel.

Elie Wiesel murió el 2 de julio de 2016 en Manhattan, Nueva York, a la edad de 87 años.




</doc>
<doc id="8201" url="https://es.wikipedia.org/wiki?curid=8201" title="Lech Wałęsa">
Lech Wałęsa

Lech Wałęsa (Popowo, voivodato de Cuyavia y Pomerania, 29 de septiembre de 1943) es un político polaco, antiguo sindicalista y activista de los derechos humanos. Fue cofundador de Solidaridad, el primer sindicato libre en el Bloque del Este, ganó el en 1983, y fue presidente de Polonia de 1990 a 1995, siendo sucedido por Aleksander Kwaśniewski.

Lech Wałęsa nació el 29 de septiembre de 1943 en Popowo, Polonia, hijo de un carpintero. Estudió primaria y formación profesional antes de entrar en el Astillero Lenin, en Gdańsk, como técnico electricista en 1967. En 1969 se casó con Danuta Gołoś, y la pareja tuvo ocho hijos.

Fue miembro del comité ilegal de huelga en el astillero de Gdańsk en 1970. Tras el sangriento final de la huelga, en la que resultaron muertos alrededor de 80 trabajadores por la policía antidisturbios, Wałęsa fue detenido y condenado por «comportamiento antisocial», pasando un año en prisión.

A causa de razones político partidarias, se han hecho muchas acusaciones a gran parte de los políticos de Polonia, incluso una publicación editada por el Instituto de Memoria Nacional Polaca en 2008, aseguraba que Walesa había colaborado con los servicios secretos comunistas polacos en los años setenta, antes de pasar a la oposición.

En 1976 Wałęsa perdió su trabajo en el astillero de Gdańsk por recoger firmas para la petición de construir un monumento en memoria de los trabajadores asesinados. Debido a su inclusión en una lista negra informal, no pudo encontrar otro trabajo y fue mantenido, por un tiempo, por amigos personales muy cercanos.

En 1978 junto a Andrzej Gwiazda y Aleksander Hall, organizó el movimiento clandestino "Sindicato libre de Pomerania (Wolne Związki Zawodowe Wybrzeża)". Fue detenido varias veces en 1979 por desarrollar una organización «anti-estado», pero no fue declarado culpable en el juicio y fue liberado a principios de 1980, tras lo cual volvió al astillero de Gdańsk.

El 14 de agosto de 1980, tras el comienzo de una huelga laboral en el Astillero Lenin de Gdańsk, Wałęsa escaló su muro ilegalmente y se convirtió en líder de la huelga. Esta huelga fue seguida de forma espontánea por otras, por toda Polonia. Varios días después detuvo a los trabajadores que querían dejar el astillero de Gdańsk y los persuadió para organizar el Comité de Coordinación de Huelga "(Międzyzakładowy Komitet Strajkowy)" para dirigir y apoyar la huelga general espontánea en Polonia.

En septiembre de ese año, el gobierno comunista firmó y acordó con el Comité de Coordinación de Huelga permitir la legalización de la organización, pero no sindicatos realmente libres. El Comité de Coordinación de Huelga se legalizó como Comité de Coordinación Nacional del Sindicato Libre Solidaridad, y Wałęsa fue elegido presidente de ese comité.

Wałęsa permaneció en ese puesto hasta diciembre de 1981, cuando el Primer Ministro Wojciech Jaruzelski declaró la ley marcial. Fue encarcelado durante 11 meses en el sureste de Polonia, cerca de la frontera con la Unión Soviética hasta el 14 de noviembre de 1982.

En 1983 solicitó volver al Astillero de Gdańsk, a su antiguo puesto de electricista. Mientras fue tratado formalmente como un "simple empleado", estuvo prácticamente bajo arresto domiciliario hasta 1987. También en 1983 recibió el . No pudo recoger el premio por sí mismo, por miedo a que el gobierno no le dejase volver. Su mujer, Danuta Wałęsa, recibió el premio en su lugar. Wałęsa donó el importe del premio al movimiento Solidaridad, temporalmente exiliado, y con sede en Bruselas. La decisión de otorgarle el Premio Nobel de la Paz no ha carecido de polémica para los comunistas, considerando que Wałęsa contribuyó a la desestabilización política y económica de Polonia en su lucha contra el régimen comunista.

De 1987 a 1990, Wałęsa organizó y lideró un semi-ilegal Comité Ejecutivo Temporal del Sindicato Solidaridad.

En 1988 organizó una huelga laboral en el Astillero de Gdańsk, demandando únicamente la re-legalización del sindicato Solidaridad. Después de ocho días, el gobierno accedió a entrar en conversaciones en una mesa redonda en septiembre. Wałęsa fue el líder informal del lado no gubernamental durante estas conversaciones. En ellas, el gobierno firmó y aceptó el restablecimiento del sindicato Solidaridad y organizar elecciones "semi-libres" al parlamento de Polonia.

En 1989 Wałęsa organizó y lideró el Comité Ciudadano del Presidente del Sindicato Solidaridad. Formalmente era únicamente un cuerpo de asesores, pero en la práctica era un tipo de partido político, que ganó las elecciones parlamentarias de 1989. (La oposición tomó todos los escaños del Sejm que estuvieron sujetos a elecciones libres y todos menos uno de los escaños del recientemente restablecido Senado; de acuerdo con los acuerdos de la mesa redonda sólo los miembros del Partido Comunista y sus aliados podían ocupar el restante 64% de los escaños del Sejm). Mientras que técnicamente era únicamente el presidente del Sindicato Solidaridad, Wałęsa tenía un papel clave en la política polaca. A finales de 1989 persuadió a líderes de aliados formales de los comunistas para formar una coalición gubernamental no comunista, que sería el primer gobierno no comunista en la esfera de influencia del bloque soviético. Después de este acuerdo, para gran sorpresa del Partido Comunista, el parlamento eligió a Tadeusz Mazowiecki como Primer Ministro de Polonia. Así, Polonia, que seguía siendo en teoría un país comunista, empezó a cambiar su economía a un sistema de libre mercado.

El 9 de diciembre de 1990, Wałęsa ganó las elecciones presidenciales y se convirtió en presidente de Polonia para los siguientes cinco años. Durante su presidencia, empezó una llamada «guerra en la cabeza» que prácticamente suponía un cambio de gobierno anual. Según sus detractores, en este periodo Wałęsa sustrajo miles de documentos secretos del Ministerio del Interior de la RPP con información sobre sus actividades durante la década de 1980; algunos expertos en el tema han afirmado tener pruebas de que los archivos nunca fueron devueltos y de que el antiguo líder de Solidaridad trabajaba para los servicios de inteligencia occidental bajo seudónimo de «Bolek»; un comunista desertor, pero los defensores de Walesa niegan tales acusaciones y otros remiten a la intrincada política interior en Polonia como causa de estas acusaciones.

En 1993 dijo desvincularse totalmente del movimiento Solidaridad, luego de que no le permitieran crear un nuevo movimiento de cara a los comicios de septiembre de ese año. Bajo su presidencia Polonia cambió completamente, de un régimen comunista bajo la influencia de la Unión Soviética a un país capitalista con una economía de libre mercado de rápido crecimiento y un régimen político con un sistema multipartidista. Sin embargo, su estilo de presidencia fue fuertemente criticado por la mayoría de los partidos políticos, y perdió mucho del apoyo público inicial a finales de 1995.

Wałęsa perdió las elecciones presidenciales de 1995. Después de las elecciones anunció que iría a un retiro político, pero ha permanecido activo, tratando de establecer su propio partido político. En 1997 apoyó y ayudó a organizar un nuevo partido llamado Acción Electoral Solidaridad ("Akcja Wyborcza Solidarność"), que ganó las elecciones al parlamento. Sin embargo, su apoyo fue de menor significado y Wałęsa ocupó una posición muy baja en este partido. El líder real del partido y su principal organizador fue el nuevo líder del sindicato Solidaridad, Marian Krzaklewski.


Lech Wałęsa volvió a optar a la presidencia en las elecciones de 2000, pero recibió únicamente el 1% de los votos. Muchos polacos estuvieron descontentos con el hecho de que una vez más intentará recuperar su poder político tras haber anunciado su retirada. Desde ese momento ha estado dando conferencias sobre la historia y la política de Europa Central en varias universidades extranjeras.
El 10 de mayo de 2004, el aeropuerto internacional de Gdańsk fue renombrado oficialmente Aeropuerto de Gdańsk-Lech Wałęsa en su honor. Su firma se ha incorporado al logotipo del aeropuerto. Hubo alguna controversia acerca de si el nombre debía escribirse "Lech Walesa" (sin diacríticos, pero más fácilmente reconocible en el mundo) o "Lech Wałęsa" (con la grafía polaca, pero más difícil de escribir o pronunciar para extranjeros). Un mes más tarde, Wałęsa acudió a los Estados Unidos representando a Polonia para los funerales de estado de Ronald Reagan.

Lech Walesa ha dejado ver en múltiples ocasiones su descontento con el rumbo político que ha tomado el país, en particular con los hermanos Kaczyński, quienes fueron sus antiguos compañeros sindicales y a quienes ha acusado de demagogos oportunistas que han llevado a Polonia en la dirección equivocada.

Además de su Premio Nobel, Wałęsa ha recibido otros premios internacionales. Ha sido distinguido con el título de doctor "honoris causa", por varias universidades europeas y estadounidenses. La última, en enero de 2011, fue la Universidad Europea de Madrid, España.

En 2008 el gobierno de Venezuela comunicó a Walesa que lo consideraba persona no grata.

Wałęsa es conocido por su ferviente catolicismo. El 1 de marzo de 2013, Wałęsa declaró que los diputados homosexuales deberían sentarse fuera del Parlamento, puesto que representan a una minoría.Según declararía después Lech Wałęsa: "«"No quiero que esta minoría, con la que no estoy de acuerdo pero que tolero, se manifieste en la calle y haga girar la cabeza a mis hijos y nietos"»".

Wałęsa también dijo que los homosexuales tienen poca importancia como minoría y por lo tanto tienen que «ajustarse a las cosas pequeñas», a lo que Robert Biedroń, miembro del Parlamento respondió: «Si aceptamos las reglas propuestas por Lech Wałęsa, ¿donde se sentarán los negros? Ellos también son minoría. ¿Y qué tal las personas con discapacidad?».




</doc>
<doc id="8202" url="https://es.wikipedia.org/wiki?curid=8202" title="Teresa de Calcuta">
Teresa de Calcuta

Teresa de Calcuta (Uskub, Imperio otomano —actual Skopie, Macedonia—; 26 de agosto de 1910-Calcuta, India; 5 de septiembre de 1997), de nombre secular Agnes Gonxha Bojaxhiu () y también conocida como Santa Teresa de Calcuta o Madre Teresa de Calcuta, fue una monja católica de origen albanés naturalizada india, que fundó la congregación de las Misioneras de la Caridad en Calcuta en 1950. Durante más de 45 años atendió a pobres, enfermos, huérfanos y moribundos, al mismo tiempo que guiaba la expansión de su congregación, en un primer momento en la India y luego en otros países del mundo. Tras su muerte, fue beatificada por el papa Juan Pablo II. Su canonización fue aprobada por el papa Francisco en diciembre de 2015, después de que la Congregación para las Causas de los Santos reconociera como extraordinaria la curación de un brasileño enfermo en estado terminal. El acto oficial de canonización tuvo lugar en Roma en la mañana del domingo 4 de septiembre de 2016.

Agnes descubrió su vocación desde temprana edad, y para 1928 ya había decidido que estaba destinada a la vida religiosa. Fue entonces cuando optó por cambiar su nombre a «Teresa» en referencia a la santa patrona de los misioneros, Teresa de Lisieux. Si bien dedicó los siguientes 20 años a enseñar en el convento irlandés de Loreto, comenzó a preocuparse por los enfermos y por los pobres de la ciudad de Calcuta. Esto la llevó a fundar una congregación con el objetivo de ayudar a los marginados de la sociedad, primordialmente enfermos, pobres y personas que no tenían hogar.

En la década de 1970 era conocida internacionalmente y había adquirido reputación de persona humanitaria y defensora de los pobres e indefensos, en parte por el documental y libro "Something Beautiful for God", de Malcolm Muggeridge. Obtuvo el Premio Nobel de la Paz en 1979 y el más alto galardón civil de la India, el Bharat Ratna, en 1980, por su labor humanitaria. A ellos se sumaron una decena de premios y reconocimientos de primer nivel, tanto nacionales como internacionales.

Recibió elogios de muchas personas, gobiernos y organizaciones. Sin embargo, afrontó también una serie de críticas, como las objeciones de Christopher Hitchens, Michael Parenti, Aroup Chatterjee y el Consejo Mundial Hindú, que le achacaron una mentalidad reaccionaria y criticaron la deficiente atención en sus centros. En 2010, en el centenario de su nacimiento, fue homenajeada alrededor del mundo, y su trabajo elogiado por la presidenta india Pratibha Patil.

Agnes Gonxha Bojaxhiu («gonxha» significa «capullo de rosa» o «pequeña flor» en albanés) nació el 26 de agosto de 1910 en Uskub, entonces parte del Imperio otomano y actualmente Skopie, República de Macedonia, pero solía considerar como su fecha de nacimiento el 27 de agosto, ya que ese fue el día en que la bautizaron. Fue la menor de los hijos de un matrimonio acomodado de Shkodër, integrado por Nikollë Bojaxhiu (1878-1919) y Dranafile Bernai (1889-1972). Su familia pertenecía a la población albanesa proveniente de Kosovo asentada en Shkodër —su padre posiblemente era originario de Prizren y su madre de una villa cercana a Đakovica—. Su padre, involucrado en la política de Albania, murió repentina y misteriosamente en 1919 cuando Agnes contaba con apenas ocho años luego de ser trasladado al hospital, por causas desconocidas, aunque se presume que fue a causa de un envenenamiento. Tras la muerte de este, su madre la educó en el seno de la religión católica.

En su niñez, Agnes asistió a la escuela estatal y participó como soprano solista del coro de su parroquia; en ausencia del director, se encargaba incluso de la dirección del grupo. Además, pertenecía a una congregación mariana fundada en 1563 y conocida como Sodalicio de Nuestra Señora, donde comenzó a interesarse por las historias de los misioneros jesuitas de Yugoslavia que estaban en Bengala. Desde entonces sintió el deseo de trabajar al igual que ellos en la India. De acuerdo con la biografía escrita por Joan Graff Clucas, desde temprana edad Agnes se mostró fascinada por las historias de vida de los misioneros y sus obras en Bengala. A la edad de cinco años hizo la Primera comunión y a los seis, la Confirmación; con doce años ya estaba convencida de que debía dedicarse a la religión. Su resolución definitiva fue tomada el 15 de agosto de 1928, mientras rezaba en la capilla de la Virgen Negra de Letnice, donde acudía con frecuencia de peregrinación.

El 26 de septiembre de 1928, poco después de haber cumplido 18 años, se dirigió con una amiga a la Abadía de Loreto, perteneciente a la congregación religiosa católica Instituto de la Bienaventurada Virgen María, en Rathfarnham, Irlanda. A partir de ese momento, jamás volvería a ver a su madre o a su hermana. Si bien originalmente acudió a ese lugar para aprender inglés (que era el idioma que las hermanas de Loreto enseñaban a los niños en la India), una vez ahí fue admitida como postulante y en noviembre de 1928 se trasladó por vía marítima hacia Calcuta, sitio a donde arribó el 6 de enero de 1929. En Darjeeling, cerca de las montañas del Himalaya, inició su noviciado y aprendió bengalí además de enseñar en la escuela de Santa Teresa, que se hallaba cerca de su convento. Después de hacer sus votos de pobreza, castidad y obediencia como monja el 24 de mayo de 1931, fue trasladada al Colegio de Santa María en Entally, al este de Calcuta. En ese período, eligió ser llamada con el mismo nombre que Teresa de Lisieux, la santa patrona de los misioneros. Sin embargo, debido a que una enfermera en el convento ya había elegido ese nombre, Agnes optó por usar el término castellanizado de «Teresa» (en vez de «Thérèse»). El 14 de mayo de 1937, Teresa hizo sus votos solemnes mientras enseñaba en el colegio del convento de Loreto. Trabajó ahí por casi veinte años como profesora de historia y geografía hasta que, en 1944, se convirtió en directora del centro.

Si bien disfrutaba enseñar en el colegio, cada vez se perturbaba más en razón de la pobreza existente en Calcuta. La hambruna de 1943 en Bengala trajo consigo miseria y muerte a la ciudad, mientras que la ola de violencia hindú-musulmana suscitada en agosto de 1946 hundió a la población en la desesperación y el terror.

El 11 de septiembre de 1946, nombrada ya encargada de un colegio de las Hermanas Santa Ana, Teresa experimentó lo que más tarde describió como la «llamada dentro de la llamada», en referencia a haber escuchado a Dios pidiéndole que dedicara su vida a los menos privilegiados de la sociedad. Esto ocurrió justamente en un viaje en tren rumbo al convento de Loreto, en Darjeeling, desde Calcuta para su retiro anual. «Estaba por dejar el convento y ayudar a los pobres mientras vivía entre ellos. Fue una orden. Fallar habría significado quebrantar la fe».

Tras haber recibido capacitación médica básica en París con el apoyo financiero de un empresario indio católico, comenzó a trabajar entre los pobres en 1948 enseñándoles a leer. Tras adoptar la ciudadanía india en 1950, recibió formación como enfermera durante tres meses en Patna con las Hermanas Misioneras Médicas de Norteamérica y finalmente se asentó en los barrios más pobres. Al principio, inauguró una escuela en Motijhil (Calcuta) y pronto empezó a enfocarse en las necesidades de los indigentes y de los hambrientos. A comienzos de 1949, se le unió un grupo de mujeres jóvenes y sentó las bases para crear una nueva comunidad religiosa que ayudara a los «más pobres entre los pobres». Pronto sus esfuerzos atrajeron la atención de funcionarios indios, entre ellos el primer ministro, quienes le expresaron su aprecio.

Teresa escribió en su diario personal que su primer año de trabajo con los pobres estuvo repleto de dificultades. No tenía ingresos y por ello se veía en la necesidad de pedir donaciones de alimentos y suministros. Según relató, durante los primeros meses experimentó duda, soledad e incluso, la tentación de volver a su vida en el convento. En sus propias palabras:

En 1948, envió un pedido al Vaticano para iniciar una congregación diocesana; sin embargo, en la India existían serias dificultades políticas como consecuencia de su reciente independización. Por lo tanto, podría ser mal visto que una europea se dedicara a los pobres en la situación de ese entonces. Su permiso para abandonar el convento se le concedió en agosto de 1948 cuando abandonó el lugar solamente con cinco rupias para ayudar a los más necesitados. Madre Teresa comenzó a portar un sari blanco de algodón decorado con bordes azules en sustitución de su tradicional hábito de Loreto. El 7 de octubre de 1950, la Santa Sede le autorizó a inaugurar su nueva congregación, a la cual denominó las Misioneras de la Caridad. Según Teresa, su misión desde entonces fue cuidar a «los hambrientos, los desnudos, los que no tienen hogar, los lisiados, los ciegos, los leprosos, toda esa gente que se siente inútil, no amada, o desprotegida por la sociedad, gente que se ha convertido en una carga para la sociedad y que son rechazados por todos».

Aunque inicialmente la congregación tenía solo trece miembros en Calcuta, con el tiempo llegó a poseer más de cuatro mil integrantes presentes en orfanatos, hospicios y centros de sida de todo el mundo. La congregación ofreció caridad y cuidado a los refugiados, entre los que se contaban ciegos, discapacitados, alcohólicos, ancianos, pobres, personas sin hogar y víctimas de inundaciones, epidemias o hambrunas.

En 1952, inauguró el primer hogar para moribundos en Calcuta. Luego de obtener ayuda de diversos funcionarios indios, se convirtió un abandonado templo hindú en el Hogar para moribundos «Kalighat», un hospicio gratuito para los pobres. Tiempo después su nombre se modificó a «Kalighat, la casa del corazón puro». Todos aquellos que llegaban a Kalighat recibían atención médica y se les ofrecía la oportunidad de morir con dignidad de acuerdo a los rituales de su fe; los musulmanes leían el Corán, los hindúes recibían agua del Ganges y los católicos obtenían los últimos ritos. Según Teresa, «para personas que vivieron como animales, una muerte hermosa es morir como ángeles, amados y queridos».

En 1955, con el creciente aumento de niños abandonados, abrió la institución «Hogar del Niño del Inmaculado Corazón» para los huérfanos y los jóvenes sin hogar. Posteriormente, fundó el centro «Shanti Nagar» para aquellos individuos que padecían la enfermedad de Hansen, comúnmente conocida como lepra, junto con otras clínicas similares donde las Misioneras de la Caridad proporcionaban atención médica y alimentos.

En 1964, el papa Pablo VI, en ocasión de su viaje a Bombay por un congreso eucarístico, le regaló un vehículo Lincoln tipo limusina color blanco que luego fue subastado por la Madre Teresa; con el dinero obtenido, organizó un establecimiento para leprosos denominado «Ciudad de la Paz», muy similar a «Don de la Paz», un centro de rehabilitación fundado por Teresa con el dinero que obtuvo junto con el premio Juan XXIII en 1971. La Fundación Joseph P. Kennedy Jr. le concedió un bono de 15 000 USD que se destinó a un centro médico en Dum Dum. Para la década de 1960, ya había establecido una gran cantidad de hospicios, orfanatos y casas de leprosos en toda la India.

Su orden comenzó a propagarse por el mundo a partir de 1965, cuando su congregación se estableció en Venezuela con tan solo cinco hermanas. Hacia 1968, Madre Teresa había inaugurado establecimientos en Roma, Tanzania y Austria e incluso se extendió por gran parte de Asia, África, Europa y Estados Unidos. En el momento de su fallecimiento, la orden operaba 610 misiones en 123 países, incluidas tareas en hospicios y hogares para personas con sida, lepra y tuberculosis, comedores populares, programas de asesoramiento para niños y familias, orfanatos y escuelas.

La rama masculina de su congregación fue fundada en 1963 —los Hermanos Misioneros de la Caridad—. En esa ocasión, se inscribieron laicos católicos y no católicos como colaboradores de Teresa y compañeros de los enfermos. En respuesta a las peticiones de muchos sacerdotes, en 1981 inició el Movimiento Corpus Christi y en 1984 fundó los Padres Misioneros de la Caridad junto al padre Joseph Langford para combinar los objetivos profesionales de las hermanas con los recursos del sacerdocio ministerial. En 2007, la orden contaba con un número aproximado de 450 hermanos y 5000 monjas en todo el mundo que operaban 600 misiones en escuelas y hogares en 120 países.

Entre el 26 de marzo y el 16 de diciembre de 1971, ocurrió la Guerra de Liberación de Bangladesh, confrontación bélica entre la India y Pakistán, en la cual se produjeron violaciones a mujeres, razón por la cual muchas se habrían suicidado, enloquecido o huido. Además, se les había prohibido contraer matrimonio y tener hijos durante ese período. La Madre Teresa junto a sus hermanas establecieron sitios para acogerlas y brindarles todos los cuidados necesarios. El gobierno, por su parte, otorgó la asistencia de unas 15 hermanas más debido a la gran cantidad de refugiadas. Luego fueron alentadas para que volvieran a reconstruir su matrimonio, adoptar hijos y regresar a sus pueblos, motivo por el cual recibieron el agradecimiento del primer ministro, que relató que esas jóvenes deberían ser consideradas como «heroínas nacionales».

En 1982, a la altura del asedio de Beirut, la Madre Teresa rescató a 37 niños que estaban atrapados en un hospital de esa región tras negociar un cese al fuego entre el ejército israelí y las guerrillas palestinas. Acompañada por trabajadores de la Cruz Roja, se trasladó a través de la zona de guerra hacia el hospital devastado para evacuar a los pacientes jóvenes.

A finales de la década de 1980, amplió sus esfuerzos en los países comunistas que habían ignorado a las Misioneras de la Caridad anteriormente y se embarcó en decenas de proyectos. Visitó la República Soviética de Armenia después del terremoto de Spitak en 1988 y se reunió con Nikolai Ryzhkov, presidente del Consejo de Ministros. Además, viajó para asistir y atender a varios hambrientos en Etiopía al igual que a las víctimas del accidente de Chernóbil —motivo por el cual obtuvo la Medalla de Oro del Comité Soviético de Paz; cabe señalar que la Unión Soviética se consideraba una nación atea— y las de un terremoto de Armenia. En 1991, la Madre Teresa volvió por primera vez a su tierra natal y abrió una casa de Hermanos Misioneros de la Caridad en Tirana.

Para 1996, Teresa regentaba 517 misiones en más de 100 países. Con el paso de los años, las ayudantes de la Madre Teresa pasaron de ser trece a miles, colaborando en aproximadamente 450 centros de todo el mundo. La primera casa de los Misioneros de la Caridad en Estados Unidos se estableció en el sur del distrito del Bronx, Nueva York, en 1984, con el fin de operar en 19 establecimientos de todo el país.

Por otra parte, Teresa de Calcuta identificó como potencial patrono al padre Damián de Veuster, el apóstol de los leprosos, con un carisma similar al que caracteriza a la orden de las Misioneras de la Caridad. La Madre Teresa pidió explícitamente a Juan Pablo II por un santo que permitiera a la congregación continuar su trabajo de amor y curación:

La Madre Teresa estuvo presente en la misa de beatificación de Damián de Veuster en Bruselas, el 4 de junio de 1995, y le atribuyó más tarde «la eliminación del miedo de los corazones de los leprosos para reconocer la enfermedad, proclamarla y solicitar medicina, y el nacimiento de la esperanza de ser curados» y el cambio de actitud de la gente y de los gobernantes hacia las víctimas de la lepra: «más preocupación, menos miedo, y disposición para ayudar –en cualquier tiempo y en todo tiempo–».

Con el paso de los años, la salud de la Madre Teresa empezó a deteriorarse cada vez más a un ritmo acelerado. En 1983, sufrió un ataque cardíaco en Roma mientras visitaba al papa Juan Pablo II. Después de un segundo ataque en 1989, recibió un marcapasos artificial. En 1991, se sobrepuso de una neumonía durante una estancia en México, para lo cual fue tratada en un hospital de California. Afectada por nuevas dolencias cardíacas, ofreció renunciar a su puesto como líder de las Misioneras de la Caridad, pero las monjas de la orden, en un sufragio secreto, votaron unánimemente a favor de que se quedara y la Madre Teresa aceptó continuar con su labor al frente de la orden. En 1993 fue ingresada en el Hospital de las Naciones Unidas a raíz de una congestión pulmonar que le provocó, entre otros síntomas, fiebre. Ese mismo año desarrolló malaria, la cual se agravó debido a sus problemas pulmonares y cardíacos, y en Roma se rompió tres costillas.

En abril de 1996, la Madre Teresa se cayó y se fracturó la clavícula. Para agosto, sufría de insuficiencia en el ventrículo izquierdo de su corazón. Recibió una cirugía cardíaca pero su salud declinó de forma notable. Cuando enfermó nuevamente, tomó la controvertida decisión de internarse en un hospital bien equipado de California, lo que originó diversas críticas. Al ser hospitalizada por problemas cardíacos de nuevo, el arzobispo de Calcuta, Henry Sebastian D'Souza, ordenó a un sacerdote llevar a cabo un exorcismo en la Madre Teresa con su permiso porque pensaba que ella podía haber sido atacada por el diablo.

El 13 de marzo de 1997, renunció como jefa de las Misioneras de la Caridad debido a sus enfermedades y padecimientos. La hermana María Nirmala Joshi fue elegida para tomar su lugar pero rehusó adoptar el título de Madre. En sus palabras, «nadie puede reemplazar a la Madre Teresa». Teresa de Calcuta falleció el 5 de septiembre de 1997 a los 87 años a causa de un paro cardíaco, después de amanecer con fuertes dolores de espalda y problemas respiratorios. Se hallaba de reposo en Santo Tomás (Calcuta) una semana antes de su muerte, en septiembre de 1997. El gobierno indio le concedió un funeral de Estado y, como parte de este, su féretro fue trasladado por gran parte de la ciudad en el mismo carruaje en el que fueron llevados los restos de Mahatma Gandhi y Jawaharlal Nehru.

La Madre Teresa fue reconocida por primera vez por el gobierno indio cuando obtuvo el galardón «Padma Shri» en agosto de 1962 y el premio «Jawaharlal Nehru» para el Entendimiento Internacional en 1969. Continuó recibiendo más premios notables en la India en los siguientes años, incluyendo el «Bharat Ratna» (el más importante entregado a un civil en la India) el 22 de marzo de 1980, el «Rajiv Gandhi Sadbhavana» en 1993 y el galardón artístico «Dayawati Modi» en 1995. Su biografía oficial inclusive fue escrita por un ciudadano indio, Navin Chawla, y publicada en 1992.

El 28 de agosto de 2010, en conmemoración a su centenario, el gobierno indio emitió monedas especiales de cinco rupias con su imagen y la presidenta Pratibha Patil expresó: «Vestida con un sari blanco con bordes azules, ella y las hermanas de las Misioneras de la Caridad se convirtieron en un símbolo de esperanza para muchos ancianos, indigentes, desempleados, enfermos y abandonados por sus familias».

En 1962, el presidente de Filipinas le entregó el premio «Ramón Magsaysay», destinado a «perpetuar su ejemplo de integridad en el gobierno, valiente servicio a la gente y el idealismo pragmático en una sociedad democrática, destacando el trabajo en el suroeste de Asia.» A principios de la década de 1970, la Madre Teresa se había convertido en una figura relevante para la religión en todo el mundo. Su popularidad se debía posiblemente en gran parte al documental de 1969 "Something Beautiful for God", de Malcolm Muggeridge, quien publicó luego un libro con el mismo título en 1972. Por entonces, Muggeridge se hallaba en una etapa de búsqueda espiritual personal. Durante el rodaje, el material grabado se rodó en lugares con poca iluminación por lo que se creyó que iba a ser de baja calidad, pero al momento de editar el contenido el equipo se percató que el material se hallaba en condiciones aceptables. Tiempo después, Muggeridge definió el hecho como un milagro atribuido a la propia Madre Teresa, aunque esto fue negado por otros integrantes del filme que dijeron que se debió a que habían usado un nuevo tipo de película ultrasensitiva Kodak. Más tarde, Muggeridge se convirtió al catolicismo.
En esa misma época, el mundo católico comenzó a honrarla públicamente. El 6 de enero de 1971, el papa Pablo VI le entregó el premio internacional por la paz «Juan XXIII», elogiando su labor con los pobres, su manifestación de caridad cristiana y sus esfuerzos por la paz. El 16 de octubre de 1971, también se hizo acreedora del premio «Good Samaritan» por la Fundación Joseph P. Kennedy Jr, tras hablar en un simposio sobre el trato que había mantenido hasta entonces con toda la gente rechazada en las calles de Calcuta. En abril de 1973, se convirtió en la primera ganadora del premio Templeton otorgado en Londres por su labor de ayuda a los pobres y necesitados de Calcuta. De acuerdo a la descripción en la página web oficial del galardón: «su trabajo heroico trajo un verdadero cambio a aquellos a los que ella sirvió y continúa inspirando a millones en todo el mundo».

Fue honrada por gobiernos y organizaciones civiles, así como también resultó designada Compañera de Honor de la Orden de Australia en 1982 por «el servicio a la comunidad de Australia y de la humanidad en general». El Reino Unido y Estados Unidos le concedieron premios en varias ocasiones, entre ellos la Orden de Mérito en 1983 y la ciudadanía honoraria de Estados Unidos el 16 de noviembre de 1996. Su país natal, Albania, le otorgó el Honor de Oro de la Nación en 1994. Universidades, tanto de Occidente como de la India, le otorgaron títulos honoríficos.
Otros premios internacionales que recibió incluyen el «Mater et magistra» otorgado el 19 de junio de 1974 en los Estados Unidos por la Tercera Orden de San Francisco de Asís, una medalla acuñada exclusivamente para ella por la Organización para la Agricultura y la Alimentación de la ONU otorgada en Roma en agosto de 1975, el premio «Pacem in Terris» en 1976, el premio internacional «Balzan» (Roma, 1978) para la «promoción de la humanidad, la paz y la hermandad entre los pueblos» y el reconocimiento internacional «Albert Schweitzer» (Estados Unidos, 23 de octubre de 1975).

En 1979, recibió el premio Nobel de la Paz al «trabajo emprendido en la lucha por superar la pobreza y la angustia, que también constituyen una amenaza para la paz». Teresa rehusó asistir al banquete ceremonial ofrecido a los premiados y pidió que los fondos de 192 000 USD se entregaran a los pobres de la India. Cuando la Madre Teresa recibió el premio, se le preguntó: «¿Qué podemos hacer para promover la paz mundial?» y respondió «Vete a casa y ama a tu familia». En su conferencia sobre el premio que le entregó el rey Olaf V de Noruega, la religiosa dijo: «Lo acepto para la gloria de Dios y de su pueblo, el más pobre entre los pobres». También apuntó que el aborto es «uno de los mayores destructores de la paz».

Al momento de su muerte, el primer ministro de Pakistán Nawaz Sharif dijo que era «una persona extraña y única que vivió mucho tiempo para propósitos más elevados. Su devoción por la vida para el cuidado de los pobres, los enfermos y los desfavorecidos es uno de los mejores ejemplos de servicio a nuestra humanidad». El ex-secretario general de la ONU Javier Pérez de Cuéllar expresó: «Ella es la Naciones Unidas, la paz en el mundo». Por su parte, el presidente Bill Clinton la definió como una «gigante de nuestra era», y luego de su muerte, Juan Pablo II declaró: «Sigue viva en mi memoria su diminuta figura, doblada por una existencia transcurrida al servicio de los más pobres entre los pobres, pero siempre cargada de una inagotable energía interior, la energía del amor de Cristo». Durante su vida, la Madre Teresa fue nombrada 18 veces en las encuestas Gallup sobre los hombres y mujeres más admirados del año, siendo electa en la categoría de las 10 mujeres más apreciadas por los estadounidenses en todo el mundo. En aquel rubro, ocupó el primer lugar varias veces en las décadas de 1980 y 1990. En 1999, fue considerada dentro de las «mujeres más admiradas del siglo XX» por una encuesta de Estados Unidos, en la cual sobrepasó a los otros candidatos por un amplio margen, posicionándose en el primer puesto en las principales categorías demográficas excepto en la de los más jóvenes.

La Madre Teresa ha sido tildada por uno de sus detractores, Christopher Hitchens, de tener una visión fundamentalista dentro de la propia ortodoxia de la Iglesia. Durante el Concilio Vaticano II, manifestó su oposición a cualquier reforma de la Iglesia católica. Según ella, lo que se necesitaba era más trabajo y más fe, no una revisión doctrinal. Por otra parte, una cuestión clave en la crítica a sus enseñanzas es su prédica constante del consuelo y el conformismo. Después de la explosión de la planta química de la multinacional Union Carbide en Bhopal (India), se presentó inmediatamente en el lugar de la tragedia, donde 2.500 personas habían muerto. «Perdonad, perdonad, perdonad», repitió nada más al bajarse del avión, sin motivar a que los afectados iniciaran acciones legales o se persiguiera a los culpables. «Estás sufriendo como Cristo en la cruz, así que Jesús te debe estar besando», le dijo Teresa de Calcuta a un enfermo de cáncer que se retorcía de dolor ante las cámaras. Desde su lecho, le respondió: «Por favor, dígale que pare de besarme». Esto último fue objeto de críticas igualmente pues Teresa sentía que el sufrimiento en las personas las hacía acercarse más a Jesús.

A este planteamiento respondió el doctor en sociología William A. Donohue, presidente de la Liga Católica por Derechos Religiosos y Civiles en los Estados Unidos:

A las críticas acerca de su firme posición contra el aborto y el divorcio respondió: «No importa quién lo dice, deben aceptar con una sonrisa y hacer su propio trabajo». Igualmente, su oposición a la inseminación artificial y el uso de anticonceptivos fue objeto de críticas; en sus palabras: «Yo no le daría un bebé de una de mis casas en adopción a una pareja que usa anticonceptivos. Los que usan anticonceptivos no comprenden el amor».

Las opiniones de los hindúes respecto de la Madre Teresa no eran uniformemente favorables. El importante partido político Bharatiya Janata Party, se opuso a la Madre pero la elogió después de su muerte, enviando un representante para su funeral. La organización Consejo Mundial Hindú, en cambio, se opuso a la decisión del gobierno de realizarle un funeral de Estado. Incluso, un recordatorio de la revista "Frontline" negó unas acusaciones propiciadas por Giriraj Kishore como «completamente falsas» y se publicó que lo que habían hecho «no influye en la percepción pública de su trabajo, especialmente en Calcuta». El autor del homenaje, a pesar de alabar su «desinteresada atención», su energía y vitalidad, fue crítico de sus campañas públicas en contra del aborto.

La calidad de la atención ofrecida a los pacientes con enfermedades terminales en los hogares para moribundos fue criticada igualmente por la prensa médica. El doctor Robin Fox, de la revista médica "The Lancet", hizo referencia a la insuficiencia de médicos, de tratamientos sistemáticos y de analgesia, mientras que Mary Loudon del "British Medical Journal", reportó la reutilización de agujas hipodérmicas, malas condiciones de vida, incluyendo el uso de agua fría para el aseo de los refugiados y un mal enfoque sobre la enfermedad y el sufrimiento, ya que se inhibió el uso de variados elementos indicados para la atención médica moderna como así también el diagnóstico sistemático. El doctor Robin Fox, editor de "The Lancet", tras su visita a los centros de Calcuta en 1994, constató que a los pacientes no se les diagnosticaban las enfermedades ni se les administraban analgésicos eficientes. Describió la atención médica como «fortuita, con voluntarios sin conocimientos médicos que tuvieron que tomar decisiones sobre el cuidado del paciente debido a la falta de médicos». Señaló que su orden no distinguía entre los pacientes curables e incurables, motivo por el cual gente que podía sobrevivir corría el riesgo de morir por infecciones o falta de tratamiento.

El autor de un periódico católico, David Scott, escribió que la Madre Teresa se «limitó a mantener viva a la gente en lugar de luchar contra la pobreza en sí». A su vez Sanal Edamaruku, presidente de la organización Rationalist International, criticó por escrito el hecho de que en algunos casos no otorgaban analgésicos en sus casas para moribundos y que podían oírse los gritos de sufrimiento por parte de la gente que tendría gusanos en sus heridas abiertas sin obtener alivio del dolor. En un principio, los analgésicos fuertes, incluso en los casos difíciles, no se daban.

En respuesta a las críticas de Robin Fox, y comentando explícitamente el tema de la disponibilidad de analgésicos, tres investigadores de instituciones inglesas (David Jeffrey, Joseph O'Neill y Gilly Burn) que acreditan trabajos científicos sobre la práctica de la medicina en la India, escribieron en la revista médica "The Lancet": «Incluso en 1994, la mayoría de los pacientes con cáncer vistos (en la India) no tenían acceso a ninguna analgesia, debido a la falta de medicamentos adecuados, de conocimiento acerca del uso de los medicamentos por parte de los médicos, como así también, en algunos casos, el desconocimiento sobre el manejo del dolor, agravado por la falta de recursos. La Madre Teresa es digna de elogio por, al menos, ofrecer bondad. Si Fox fuera a visitar las principales instituciones que están a cargo de la profesión médica en la India, rara vez vería él limpieza, atención de heridas y llagas, o bondad. Además, la analgesia podría no estar disponible».

Colette Livermore, una ex misionera de la caridad, describió las razones por las cuales abandonó la congregación en su libro "Hope Endures: Leaving Mother Teresa, Losing Faith, and Searching for Meaning". Según la propia Livermore, encontró lo que la religiosa denominó la «teología del sufrimiento», a la que definió como defectuosa. Sin embargo, calificó a Teresa como una persona buena y valiente. Aunque ella instruyó a sus seguidores sobre la importancia de la difusión del Evangelio a través de acciones en lugar de lecciones teológicas, Livermore no podía conciliar esto con algunas prácticas de la organización. Los ejemplos que citó fueron negarse innecesariamente a ayudar a los necesitados cuando éstos se acercaron a las monjas en un momento equivocado de acuerdo con sus agendas de horarios y desalentar a las monjas de buscar la formación médica para tratar las enfermedades que enfrentaban (con la justificación de que Dios permite a los débiles e ignorantes).

Chatterjee confesó que la Madre y sus biógrafos oficiales (entre los que más se destacan, Navin Chawla) se habían negado a colaborar en sus investigaciones y que Teresa no pudo «defenderse» de la cobertura crítica de la prensa occidental, dando como ejemplo el informe publicado por el diario británico "The Guardian", que atacó la condición de sus orfanatos, y del documental "Mother Teresa: Time for Change?", que fue difundido en varios países europeos.

Otras críticas de Hitchens estuvieron relacionados con los orígenes de algunas donaciones y las personas con quienes se vinculó. La religiosa aceptó dinero de la familia Duvalier (François Duvalier y su hijo Jean-Claude fueron dictadores de Haití) y los elogió públicamente. En el programa de la CBS "Sixty Minutes" afirmó públicamente de Michèle Bennett, esposa de Baby Doc: «Nunca he visto a los pobres ser tan familiares con sus jefes de Estado como lo son con ella. Para mí es una bella lección». Las imágenes de Teresa de Calcuta pronunciando estas palabras fueron reproducidas durante al menos una semana por la televisión pública haitiana. A ello replicó Donohue:

Hitchens también señaló que la Madre Teresa aceptó 1,25 millones de USD de Charles Keating, quien también le concedió el uso de un avión y portaba un crucifijo que ella le diera. Según Hitchens, Teresa lo apoyó después de su detención enviando una carta al juez del caso: «No sé nada de los negocios de Charles Keating. Solo sé que ha sido generoso con los pobres de Dios». Hitchens escribió que el fiscal Paul W. Turley habría quedado perplejo al leer la carta manuscrita de Teresa de Calcuta. En enero de 1992, Charles Keating, el «rey de los bonos basura», había estafado a 17 000 pequeños inversores en uno de los mayores escándalos de Estados Unidos. Según Hitchens, la justicia no atendió la que él llamó «petición de clemencia» de Teresa de Calcuta, y Keating fue condenado a 10 años de cárcel. En Calcuta, la directora de las Misioneras de la Caridad habría recibido una carta del fiscal en la que se le informaba de la naturaleza del dinero estafado: «Le ruego que devuelva el dinero que robó Keating a las personas que lo ganaron con su trabajo. La Madre Teresa no contestó. Pero William A. Donohue replicó a Hitchens:

En 1996, Irlanda celebró un referendo acerca de si su Constitución debería seguir prohibiendo el divorcio. La Madre Teresa tomó un avión desde Calcuta para apoyar la campaña a favor del voto negativo. Sin embargo, ese mismo año Teresa concedió una entrevista en la que decía que confiaba en que su amiga Diana de Gales fuera más feliz una vez que se hubiera librado de lo que evidentemente era un matrimonio desafortunado.

En el primer aniversario de su deceso, la revista alemana "Stern" lanzó un artículo que hablaba sobre las cuestiones financieras y el gasto de las donaciones. La prensa médica realizó críticas derivadas de diversas perspectivas y prioridades sobre las necesidades de los pobres. Otros comentarios provinieron de Tariq Ali, un miembro del comité de la editorial New Left Review, y del periodista de investigación irlandés Donal MacIntyre. Christopher Hitchens y la revista alemana "Stern" expresaron que la Madre Teresa no centró el uso del dinero en la reducción de la pobreza o en la mejora de las condiciones de sus centros, sino que lo utilizó para la apertura de nuevos conventos y el aumento de la labor misionera. William A Donohue repuso a Hitchens:
Un aspecto particularmente notable de la Madre Teresa es la profunda crisis de fe a la que se enfrentó por casi cinco décadas de su vida, misma que quedó evidenciada en el libro "Mother Teresa: Come Be My Light" editado por Brian Kolodiejchuk que recopila las cartas privadas escritas por la misionera. A pesar de ello, esto no fue obstáculo para el proceso de su beatificación llevado a cabo en 2003.

Tras analizar sus obras y logros, el papa Juan Pablo II dijo: «¿Dónde encontraba la Madre Teresa la fuerza y la perseverancia para ponerse totalmente al servicio de los demás? En la contemplación silenciosa de Jesucristo, su Santo Rostro, su Sagrado Corazón». En privado y durante casi 50 años hasta el final de su vida, la Madre Teresa experimentó dudas sobre sus creencias religiosas, en las cuales «no sentía la presencia de Dios en lo absoluto», «ni en su corazón ni en la eucaristía», según dijo su postulador, el reverendo Brian Kolodiejchuk. La Madre Teresa no solo sobrellevó el dolor provocado por su falta de fe, sino que también sintió graves dudas sobre la existencia de Dios:
Con referencia a las palabras anteriores, su postulador, Kolodiejchuk (el funcionario responsable de reunir las pruebas para su beatificación), indicó que existía el riesgo de que algunos pudieran malinterpretar lo dicho pero que la fe de la Madre de que Dios estaba trabajando a través de ella se mantuvo intacta, y si bien se lamentaba por el sentimiento de pérdida de cercanía con Dios, no puso en duda su existencia. Muchos otros santos tuvieron experiencias similares de aridez espiritual. Contrariamente a las creencias erróneas por parte de algunos que expresaron que esas dudas serían un impedimento para la canonización, este proceso se llevó a cabo sin ningún obstáculo en dicho rubro.

La Madre Teresa sintió, después de una década de dudas, un período breve de fe renovada. En el momento de la muerte del papa Pío XII en otoño de 1958, orando por él en una misa de réquiem, dijo que había sido relevada de la «larga oscuridad: aquel sufrimiento extraño». Sin embargo, cinco semanas más tarde, admitió regresar a sus dificultades para creer. Escribió muchas cartas a sus confesores y superiores durante un período de 66 años. Si bien había pedido que las mismas fueran destruidas por temor a que la gente «vaya a pensar más en mí y menos en Jesús», fueron recopiladas en "Mother Teresa: Come Be My Light" (Ed. Doubleday). En una carta que se dio a conocer públicamente a un confidente espiritual, el reverendo Michael van der Peet, Teresa escribió: «Jesús tiene un amor muy especial para ti. [Pero] En cuanto a mí, el silencio y el vacío son tan grandes, que miro y no veo, escucho y no oigo, mi lengua se mueve [en la oración] pero no habla... Quiero que reces por mí, que yo le dejo tener una mano libre».

Muchos medios informativos se refirieron a los escritos de la Madre Teresa como una indicación de crisis de fe. Algunos de sus críticos, como Christopher Hitchens, tomaron sus escritos como una evidencia de que su imagen pública fue creada principalmente para publicidad a pesar de sus creencias y acciones personales. Sin embargo, otros, como Brian Kolodiejchuk, editor de "Come Be My Light", la compararon con el poeta místico del siglo XVI San Juan de la Cruz, quien acuñó el término de «noche oscura del alma» para describir a una etapa particular del crecimiento de algunos maestros espirituales. El Vaticano indicó que las cartas no detendrían su camino hacia la santidad. De hecho, Kolodiejchuk fue su postulador.

En su primera encíclica, "Deus caritas est", Benedicto XVI mencionó a Teresa de Calcuta en tres ocasiones y también utilizó su obra para referirse a uno de los principales puntos de la encíclica. «La Beata Teresa de Calcuta es un ejemplo evidente de que el tiempo dedicado a Dios en la oración no sólo deja de ser un obstáculo para la eficacia y la dedicación al amor al prójimo, sino que es en realidad una fuente inagotable para ello». La Madre Teresa especificó que «sólo por la oración mental y la lectura espiritual podemos cultivar el don de la oración».

Aunque no hubo una conexión directa entre la congregación de Teresa y las órdenes franciscanas, confesó ser admiradora de San Francisco de Asís. En consecuencia, la vida de Teresa de Calcuta y el carácter de la orden muestran cierta influencia de la espiritualidad franciscana. Las hermanas de las Misioneras de la Caridad recitan la oración de paz de San Francisco todas las mañanas durante la acción de gracias después de la Comunión, y muchos de los votos y el énfasis de su ministerio son similares. San Francisco también hizo hincapié en la pobreza, castidad, obediencia y sumisión a Cristo, dedicando gran parte de su vida al servicio de los pobres, especialmente a los leprosos en la zona donde vivía.

Tras su muerte, la Santa Sede consideró que se podría iniciar el proceso de beatificación, considerado como el tercero de los cuatro pasos para alcanzar la canonización, en donde el papa declara al beato digno de veneración universal, aunque para ello se deben comprobar dos milagros (uno más adicional al milagro con el que se le catalogó como beata). El milagro que requería su beatificación sucedió en 1998 cuando, de manera aparentemente inexplicable, Mónica Besra, una mujer que padecía un tumor maligno en el abdomen, fue sanada. Besra comentó que había sido acogida en Roma por las Misioneras de la Caridad tras haber sido desahuciada por los médicos. Una de las hermanas le colocó sobre el abdomen una imagen de la Virgen María, que había permanecido sobre la túnica de la Madre Teresa durante la celebración de los premios Nobel. La sanación de aquella mujer ocurrió el 5 de septiembre de 1998, exactamente un año después del deceso de la misionera. Distintos médicos indios, la Asociación de Ciencias y Racionalismo de la India e incluso el marido de la propia Besra pusieron en duda su curación milagrosa al asegurar que la enfermedad desapareció por los medicamentos que debió ingerir durante nueve meses. Por otra parte, otro medio científico cita la curación de Mónica Besra de su tumor como uno de los elementos principales en el proceso de canonización de Teresa de Calcuta.

El proceso de beatificación de la Madre comenzó dos años después de su muerte gracias a una dispensa papal que evitaba el transcurso de cinco años desde su deceso, tal como establece el Derecho Canónico. El Vaticano citó a Christopher Hitchens para exponer algún testimonio que pudiera comprometer y entorpecer el proceso de beatificación. «Fue hablando con ella cuando descubrí, y me aseguró, que no estaba trabajando para aliviar la pobreza», dijo Hitchens. «Ella trabajaba para ampliar la cantidad de católicos. Me dijo: "No soy una trabajadora social. No lo hago por eso. Lo hago por Cristo. Lo hago por la Iglesia"». La Congregación para las Causas de los Santos se encargó de investigar sus declaraciones pero fueron desestimadas posteriormente.

El 19 de octubre de 2003, ante la presencia de unas 300 000 personas en la Plaza de San Pedro, fue proclamada beata por el papa Juan Pablo II. A la celebración asistieron medio millar de las Misioneras de la Caridad, 150 cardenales y 400 obispos. El papa también declaró formalmente el 5 de septiembre como la festividad de la Madre Teresa.

El 18 de diciembre de 2015, el papa Francisco aprobó la canonización de Teresa de Calcuta luego de que la Congregación para las Causas de los Santos reconociera como «extraordinaria» la curación de un brasileño enfermo con múltiples tumores cerebrales e hidrocefalia obstructiva, que había sido sujeto a trasplante renal y terapia inmunosupresora en 2008 sin resultados. La Iglesia católica sostuvo que esa patología cerebral resolvió de forma instantánea, completa y permanente el 9 de diciembre de 2008, y que tal resolución fue declarada por unanimidad como «científicamente inexplicable» por parte de un colegio integrado por siete médicos. La Iglesia atribuyó el milagro a Teresa de Calcuta, fallecida 11 años antes, debido a que la esposa del enfermo dijo haberse encomendado a la Madre Teresa para que salvase a su marido. El acto oficial de canonización, previsto dentro del jubileo de la Misericordia, tuvo lugar en la Plaza de San Pedro en la Ciudad del Vaticano en la mañana del 4 de septiembre de 2016. Decenas de miles de personas se reunieron para la ceremonia, entre ellas 15 delegaciones gubernamentales oficiales y 1500 personas sin hogar de toda Italia. La ceremonia fue televisada en directo por el canal del Vaticano. Skopie, la ciudad natal de Madre Teresa, anunció una semana de celebración por su canonización y en la India, se celebró una misa especial en la sede principal de las Misioneras de la Caridad en Calcuta.

El 6 de septiembre de 2017, Teresa de Calcuta fue nombrada copatrona de la Arquidiócesis de Calcuta, junto con san Francisco Javier, que lo era desde 1986.

En 1984, la editorial estadounidense Marvel Comics publicó una historieta basada en su vida y su trabajo. Los autores fueron el guionista David Michelinie y los dibujantes John Tartaglione y Joe Sinnott.

En marzo de 1998, se aplicó una placa a su residencia natal, que dice: «Aquí está la vivienda en la que el 26 de agosto de 1910 nació Agnes Gonxha Bojaxhiu, la Madre Teresa». Ese mismo año Lion Communications (Polygram Records) comercializó un álbum musical, "Mother, We'll Miss You", a manera de tributo póstumo que incluye la participación de varios cantantes de diferentes países entre ellos José Feliciano, el cantante gospel Walt Whitman y el grupo Soul Children of Chicago. El mismo contó con la producción del cantante escocés Dave Kelly. Tras el lanzamiento del compilatorio, diversos periódicos estadounidenses, como "Boston Globe" y "Philadelphia Inquirer", publicaron artículos relacionados con la vida y obra de la Madre Teresa.

Fue homenajeada asimismo a través de museos, nombrándosela patrona de varias iglesias, y con varios monumentos y caminos. En 2002, se le puso su nombre al Aeropuerto Internacional de Albania, algo similar a lo ocurrido con una plaza en Tirana donde se halla un monumento dedicado a la misionera, con una de las principales calles de Pristina, capital de Kosovo, y con un hospital civil albanés, el Hospital de la Madre Teresa (lugar de fallecimiento de Leka de Albania).

El 30 de agosto de 2009, se nombró «Mother Teresa Way» a una calle del barrio de Bronx. Ese tramo forma parte de la avenida Lydig y se logró su designación luego de que la Sociedad Albanesa de Estados Unidos insistiera durante 16 años para que la ciudad accediera a dedicar una vía a la Madre Teresa. En Skopie, se inauguró un museo que cuenta con una variada cantidad de objetos y pertenencias de la religiosa. En una de sus salas se halla una réplica de su vivienda natal realizada por el artista Vojo Georgievski y también posee un parque conmemorativo con su nombre.

La Universidad de la Mujer Madre Teresa, en Kodaikanal, se estableció en 1984 como una universidad pública por la gobernación de Tamil Nadu. Varios tributos han sido publicados en periódicos de la India y revistas con autoría de su biógrafo, Navin Chawla.

Indian Railways introdujo un nuevo tren, «Mother Express», en alusión a la Madre Teresa, el 26 de agosto de 2010 para conmemorar el centenario de su nacimiento. El gobierno del estado de Tamil Nadu organizó celebraciones con ocasión de su centenario el 4 de diciembre de 2010 en Chennai, encabezadas por el jefe de Gobierno Muthuvel Karunanidhi.

En Argentina, se entrega desde 2000 el premio Madre Teresa con el objetivo de «promover valores en la sociedad generando en la comunidad, en especial entre los jóvenes y adolescentes, alicientes y modelos a seguir e imitar; para su crecimiento humano y espiritual».




• «La madre Teresa no era una Santa» Artículo crítico con su inminente proceso de santificación.


</doc>
<doc id="8204" url="https://es.wikipedia.org/wiki?curid=8204" title="Willy Brandt">
Willy Brandt

Herbert Ernst Karl Frahm, más conocido como Willy Brandt (Lübeck, 18 de diciembre de 1913-Unkel, 8 de octubre de 1992), fue un político socialdemócrata alemán que ocupó el cargo de canciller de Alemania Occidental entre 1969 y 1974.

Willy Brandt fue el "nom de guerre" que asumió tras ser víctima de la persecución política del régimen de la Alemania nazi.

Miembro de las Juventudes Socialistas desde 1930, militó en el ala izquierda de la organización. Al ser expulsada ésta del Partido Socialdemócrata de Alemania (SPD) en 1931, pasó a formar parte del Partido de los Trabajadores Socialistas de Alemania (SAP). Fue representante de este Partido en la Guerra Civil Española. Al llegar los nazis al poder en Alemania, se refugió en Noruega, donde se nacionalizó noruego y trabajó como periodista. Al producirse la invasión alemana durante la Segunda Guerra Mundial, se trasladó a Suecia, y al término del conflicto regresó a Alemania, recuperó la nacionalidad alemana e ingresó en el SPD.

Establecido en Berlín Occidental, fue alcalde de la ciudad desde 1957, por lo que tuvo que enfrentar la crisis que supuso la construcción del Muro de Berlín en 1961. Presidente del SPD a partir de 1964, evolucionó hacia posturas más centristas y en 1966 fue Vicecanciller y Ministro de Asuntos Exteriores en el gabinete de la «Gran Coalición». Finalmente, en 1969 fue nombrado canciller de la RFA.

Willy Brandt nació con el nombre de Herbert Ernst Karl Frahm en la ciudad hanseática de Lübeck. Su madre, Martha Frahm, era una madre soltera que trabajaba como cajera en una tienda de la cooperativa "Konsumverein". Su padre era contable en Hamburgo, se llamaba John Möller, quien nunca conoció a Brandt. Como su madre trabajaba seis días a la semana, fue criado principalmente por el padrastro de su madre, Ludwig Frahm, y su segunda esposa, Dora.

Después de pasar la universidad en 1932, trabajó en FH Bertling, corredor naviero y armador. Se alistó en las "Juventudes Socialistas" en 1929 y en el Partido Socialdemócrata (SPD) en 1930. Dejó el SPD para afiliarse a un partido todavía más izquierdista, el Partido de los Trabajadores (SAPD), que se alió con el Partido Obrero de Unificación Marxista en España y con el Partido Laborista Independiente en Gran Bretaña. En 1933, usando sus conexiones con el puerto y sus buques, dejó Alemania y se fue a Noruega para escapar de la persecución nazi. Fue en esta época cuando adoptó el seudónimo de "Willy Brandt" para evitar ser detectado por los agentes nazis. En 1934, participó en la fundación de la Oficina Internacional de Organizaciones Juveniles Revolucionarias y fue elegido para ocupar la Secretaría.

Brandt estuvo en Alemania de septiembre a diciembre de 1936, tras adoptar la identidad de un estudiante noruego llamado Gunnar Gaasland. Más tarde continuó en Berlín como corresponsal de guerra y hablaba alemán con acento noruego. El auténtico Gunnar Gaasland se había casado en 1936 con Gertrud Meyer, compañera de Brandt desde su juventud en Lübeck y que lo había acompañado a Noruega en 1933. Gracias al matrimonio con Gaasland, Gertrud, que hasta 1939 siguió viviendo con Brandt, pudo conseguir la nacionalidad noruega y evitar la deportación. En abril de 1937 Brandt viajó como representante del SAPD a España en plena Guerra Civil Española. Nada más llegar a Barcelona estalló una guerra interna entre las fuerzas de la República y los grupos anarquistas y libertarios, entre estos últimos el POUM con el que confraternizaba el partido de Brandt.

En 1938, el gobierno alemán le revocó la ciudadanía, por lo que solicitaron la ciudadanía noruega. En 1940, fue arrestado en Noruega por las fuerzas de ocupación alemanas, pero no fue identificado porque llevaba un uniforme noruego. Tras su liberación, huyó a la neutral Suecia. En agosto de 1940, se naturalizó noruego, recibiendo su pasaporte de la Embajada de Noruega en Estocolmo, donde vivió hasta el final de la guerra. Willy Brandt dio conferencias en Suecia el 1 de diciembre de 1940 en la universidad Bommersvik sobre los problemas experimentados por los socialdemócratas en la Alemania nazi y los países ocupados al inicio de la Segunda Guerra Mundial. En el exilio en Noruega y Suecia, Brandt aprendió noruego y sueco. Brandt hablaba noruego con fluidez, y mantuvo una estrecha relación con Noruega.

A finales de 1946, Brandt regresó a Berlín, trabajando para el gobierno noruego. En 1948, se afilió al Partido Socialdemócrata de Alemania (SPD) y volvió a ser un ciudadano alemán, y se aprobó finalmente su seudónimo de Willy Brandt como su nombre legal.

Desde el 3 de octubre de 1957 hasta 1966, Willy Brandt fue durante un período de creciente tensión en las relaciones Este-Oeste que llevó a la construcción del muro de Berlín. En el primer año de Brandt como alcalde, también desempeñó el cargo de Presidente del Bundesrat en Bonn. Brandt se manifestó en contra de la represión soviética de la Revolución Húngara de 1956 y contra la propuesta de Nikita Jrushchov de 1958 de que Berlín recibiera el estatus de "ciudad libre". Fue apoyado por la influyente editorial Axel Springer. En su tiempo de Alcalde, recibió visitas notables de John F. Kennedy y Robert F. Kennedy para apoyar a Berlín Oeste en los momentos más difíciles.

Brandt fue elegido presidente del Partido Socialdemócrata en 1964, cargo que ocupó hasta 1987, más que cualquier otro presidente del Partido desde su fundación por August Bebel. Brandt fue el candidato del Partido Socialdemócrata a la Cancillería en 1961, pero perdió ante el conservador Konrad Adenauer de la Unión Demócrata Cristiana de Alemania (CDU). En 1965, Brandt volvió a presentarse como candidato, pero perdió ante el popular Ludwig Erhard. El gobierno de Erhard fue de corta duración. Sin embargo, se formó en 1966 una gran coalición entre el SPD y la CDU, con Brandt como ministro de Asuntos Exteriores y Vicecanciller.

En las elecciones de 1969, de nuevo con Brandt como el principal candidato, el Partido Socialdemócrata se hizo más fuerte, y después de tres semanas de negociaciones, el partido formó un gobierno de coalición con el pequeño Partido Democrático Liberal (FDP). Brandt fue elegido Canciller de la República Federal de Alemania.

Como canciller, Brandt desarrolló su "Neue Ostpolitik" (Nueva Política Oriental). Brandt fue activo en la creación de un grado de acercamiento a la República Democrática Alemana y también en mejorar las relaciones con la Unión Soviética, Polonia, Checoslovaquia y otros países del Bloque del Este. Un momento seminal fue en diciembre de 1970 con la famosa Genuflexión de Varsovia en la que Brandt, al parecer espontáneamente, se arrodilló ante el monumento a las víctimas del Levantamiento del Gueto de Varsovia. El levantamiento se produjo durante la Ocupación de Polonia (1939-1945), y el monumento está dedicado a los fallecidos en el motín -sofocado por tropas alemanas- y a los residentes del gueto que fueron trasladados a campos de exterminio.

En 1971, Brandt recibió el Premio Nobel de la Paz por su labor en la mejora de las relaciones con Alemania Oriental, Polonia y la Unión Soviética.

Brandt negoció un tratado de paz entre la República Federal de Alemania y Polonia, además de los acuerdos sobre las fronteras entre los dos países, lo que significaba el verdadero final de la Segunda Guerra Mundial. Brandt negoció tratados paralelos y acuerdos entre la República Federal y Checoslovaquia.

En Alemania Occidental, la "Neue Ostpolitik" de Brandt fue muy polémica, dividiendo a la población en dos campos: uno, que abarcaba todos los partidos conservadores y, en particular, los antiguos residentes de la zona oriental de Alemania ("die Heimatvertriebenen") expulsados del este por la limpieza étnica de las potencias aliadas. Estos grupos manifestaron su fuerte oposición a la política de Brandt, calificándola de "ilegal" y de "alta traición".

El otro campo apoyó y alentó la "Neue Ostpolitik" como "Wandel durch Annäherung" ("El cambio a través del acercamiento"), fomentando el cambio a través de una política de compromiso con los comunistas del bloque del este, en lugar de tratar de aislar a aquellos países diplomática y comercialmente. Los partidarios de Brandt afirmaron que la política había ayudado a romper "el Bloque Oriental con mentalidad de asedio", y también contribuyó a aumentar su conciencia de las contradicciones de su marca de socialismo/comunismo, que, junto con otros eventos, finalmente llevó a la caída del comunismo europeo.

La Alemania Occidental, en la década de 1960, se vio sacudida por disturbios estudiantiles y en general de "cambio de los tiempos", que no todos los alemanes estaban dispuestos a aceptar o aprobar. Había parecido un país estable, una pacífica nación, contenta con el resultado de su "milagro económico". La generación del "baby-boom" alemana quería llegar a un acuerdo con la generación de los padres, profundamente conservadora, burguesa y exigente. Los alumnos nacidos en la posguerra eran los más abiertos, y acusaron a la "generación de los padres" de su pasado nazi. Lo que es peor, les acusaron de ser obsoletos y anticuados. En comparación con sus antecesores, la generación "escéptica" fue mucho más caprichosa, dispuesta a abrazar la ideología socialista más extrema (como el maoísmo) y los héroes públicos (como Ho Chi Minh, Fidel Castro y el Che Guevara), mientras vivían en un estilo de vida más promiscuo. Los estudiantes y jóvenes aprendices podían darse el lujo de salir de las casas de sus padres, y la política de izquierda se consideraba elegante, así como tomar parte en manifestaciones políticas al estilo estadounidense en contra de las fuerzas militares estadounidenses en Vietnam del Sur.

El antecesor de Brandt como canciller, Kurt Georg Kiesinger, había sido miembro del partido nazi, y fue una pasada de moda intelectual alemana burguesa y conservadora. Brandt, después de haber luchado contra los nazis y de haberse enfrentado a la Alemania Oriental durante varias crisis al tiempo que era alcalde de Berlín oeste, se convirtió en una figura polémica, pero creíble, en varias facciones diferentes. Como dijo el Ministro de Relaciones Exteriores en el gabinete de gran coalición de Kiesinger, Brandt ayudó a obtener más beneficio de la homologación internacional para la Alemania Occidental y puso los cimientos de su futuro en la "Neue Ostpolitik". Había una gran diferencia de opinión entre Kiesinger y Brandt en las encuestas efectuadas en la Alemania Occidental.

Los dos estadistas habían llegado a sus propios términos con los estilos de vida del nuevo "baby-boom". Kiesinger les consideró "una multitud vergonzosa de pelo largo abandonados que necesitaba un baño y alguien para disciplinarlos". Por otra parte, Brandt necesitaba tiempo para ponerse en contacto y para ganar credibilidad entre la "Außerparlamentarische Opposition" (APO) (oposición extraparlamentaria). Los estudiantes cuestionaron la sociedad de la Alemania Occidental en general, buscaban reformas sociales, legales y políticas. Además, los disturbios llevaron a un renacimiento de los partidos de derecha.

Brandt, sin embargo, representaba una figura de cambio y siguió un curso de reformas sociales, legales y políticas. En 1969, Brandt obtuvo una pequeña mayoría, formando una coalición con el FDP. En su primer discurso ante el Bundestag como canciller, Brandt expuso su trayectoria política de las reformas y terminó el discurso con sus famosas palabras: ""Wir wollen mehr Demokratie wagen"" (literalmente: "aventuremos más democracia"). Este discurso de Brandt fue popular tanto en el SPD como entre la mayoría de los estudiantes y otros jóvenes del "baby boomers", que soñaban con un país que fuese más abierto y más colorido que la frugal y aún un poco autoritaria Alemania occidental que había sido construida después de la Segunda Guerra Mundial. Sin embargo, la "Neue Ostpolitik" de Brandt perdió para él una gran parte de los refugiados alemanes del Este, votantes que se habían manifestado significativamente en favor de los socialdemócratas en los años de posguerra.

La Ostpolitik de Brandt dio lugar a un colapso de la coalición de la estrecha mayoría de Brandt que había disfrutado el Bundestag. En octubre de 1970, los diputados del Partido Democrático Liberal Erich Mende, Heinz Starke y Siegfried Zoglmann cruzaron la frontera para unirse a la CDU. El 23 de febrero de 1972, el diputado socialdemócrata Herbert Hupka, que también era líder de la "Bund der Vertriebenen" (Asociación de desplazados), se unió a la CDU en desacuerdo con los esfuerzos de reconciliación de Brandt hacia el este. El 23 de abril de 1972, Wilhelm Helms (FDP) abandonó la coalición. Políticos del FDP, como Knud von Kühlmann-Stumm y Gerhard Kienbaum, también declararon que votarían contra Brandt, por lo que Brandt había perdido la mayoría. El 24 de abril de 1972, se propuso un voto de censura y se votó tres días más tarde. Si esta moción hubiera salido adelante, Rainer Barzel habría sustituido a Brandt como Canciller. Para sorpresa de todos, el movimiento fracasó: Barzel sólo obtuvo 247 votos de un total de 260 votos; para la mayoría absoluta se requerían 249 votos. Asimismo hubo 10 votos en contra y 3 votos nulos. Mucho más tarde se reveló que dos miembros del Bundestag (Julius Steiner y Leo Wagner, tanto de la CDU y la CSU) habían sido sobornados por la Stasi de Alemania Oriental para que votasen a favor de Brandt.

Aunque Brandt se mantuvo como canciller, había perdido su mayoría. Sus iniciativas posteriores en el Parlamento, sobre todo en el presupuesto, fracasaron. A causa de este estancamiento, el Bundestag fue disuelto y se convocaron nuevas elecciones. Durante la campaña de 1972, muchos artistas populares de Alemania Occidental, intelectuales, escritores, actores y profesores dieron apoyo a Brandt y al partido socialdemócrata. Entre ellos se encontraban Günter Grass, Walter Jens e incluso el futbolista Paul Breitner. La "Ostpolitik" de Brandt, así como sus políticas internas reformistas, eran populares entre la generación joven, y llevó a que su partido SPD consiguiera su mejor resultado electoral federal a finales de 1972. El "Willy-Wahl" (elección de Willy) de Brandt fue el principio del fin, y el papel de Brandt en el gobierno comenzó a declinar.

Muchas de las reformas de Brandt toparon con la resistencia de los gobiernos de los estados federados (dominados por la CDU/CSU). El espíritu de optimismo reformista se vio interrumpido por la crisis del petróleo de 1973 y la gran huelga de los servicios públicos de 1974, que dio lugar a que los sindicatos, dirigidos por Heinz Kluncker, exigieran un incremento salarial importante, pero ello redujo el margen de maniobra de Brandt para hacer nuevas reformas. Se decía que Brandt era más soñador que un gerente y fue perseguido personalmente por la depresión. Para hacer frente a cualquier intento de simpatizar con el comunismo o de ser blandos con los extremistas de izquierda, Brandt implementó una legislación estricta que prohibía el empleo de personas "radicales" en el servicio público.

Alrededor de 1973, los organismos de seguridad de la Alemania Occidental recibieron información de que uno de los asistentes personales de Brandt, Günter Guillaume, era un espía de los servicios de inteligencia de la Alemania Oriental. A Brandt se le pidió que siguiese trabajando como de costumbre, y él accedió a hacerlo, incluso pasó unas vacaciones privadas con Guillaume. Guillaume fue detenido el 24 de abril de 1974, y muchos culparon a Brandt por tener un espía comunista en su círculo íntimo. Así, caído en desgracia, Brandt renunció a su cargo como Canciller el 6 de mayo de 1974. No obstante, Brandt se mantuvo en el Bundestag y como Presidente del SPD hasta 1987.

Este asunto de espionaje es reconocido por haber sido simplemente el detonante de la renuncia de Brandt, pero no la causa fundamental. Brandt se vio sacudido por escándalos sobre el adulterio de serie, y al parecer también luchó con el alcohol y la depresión. También fueron las consecuencias económicas sobre la República Federal de Alemania de la crisis del petróleo de 1973 que causó el estrés suficiente para acabar con Brandt como Canciller. Como el propio Brandt dijo más tarde: "Yo estaba agotado, por razones que no tenían nada que ver con el proceso sucede hoy en día." Parece que "el proceso" fue el desarrollo del escándalo de espionaje de Guillaume.

Guillaume era un agente de espionaje de la Alemania Oriental, que había sido supervisado por Markus Wolf, el jefe de la "Administración Principal de Inteligencia" del Ministerio de Seguridad del Estado de Alemania Oriental. Wolf declaró después de la reunificación que la renuncia de Brandt no había sido prevista, y que la colocación y el manejo de Guillaume había sido uno de los mayores errores de los servicios secretos de la Alemania Oriental. Brandt fue sucedido como el Canciller de Alemania por su compañero socialdemócrata, Helmut Schmidt. Para el resto de su vida, Brandt mantuvo la sospecha de que su colega socialdemócrata (y viejo rival) Herbert Wehner había sido uno de los responsables de la caída de Brandt. Sin embargo, son escasas las pruebas de esta sospecha.

Después de su mandato como Canciller, Brandt mantuvo su escaño en el Bundestag y siguió siendo presidente del Partido Social Demócrata hasta 1987. A partir de 1987, Brandt renunció para convertirse en el presidente honorario del partido. Brandt fue también miembro del Parlamento Europeo desde 1979 hasta 1983.

Durante dieciséis años, Brandt fue el presidente de la Internacional Socialista (1976-1992), período durante el cual el número de partidos miembros de la Internacional Socialista, en su mayoría europeos, creció hasta que hubo más de un centenar de agrupaciones socialistas, socialdemócratas y obreras en todo el mundo. Durante los primeros siete años, este crecimiento del número de miembros del SI fue motivado por los esfuerzos del Secretario General de la Internacional Socialista, el sueco Bernt Carlsson. Sin embargo, a principios de 1983, surgió una controversia acerca de lo que percibió Carlsson como un enfoque autoritario del presidente de la SI. Carlsson luego reprendió Brandt diciendo: "Esta es una Internacional Socialista, no una Internacional Alemana".

A continuación, en contra de algunas voces de oposición, Brandt decidió trasladar el próximo Congreso de la Internacional Socialista de Sídney, Australia, a Portugal. A raíz de este Congreso de la IS en abril de 1983, Brandt dio represalias en contra de Carlsson, lo que le obligó a dimitir de su cargo. Sin embargo, el Primer Ministro de Austria, Bruno Kreisky, argumentó a favor de Brandt: "Es una cuestión de si es mejor ser puro o ser más numerosos".

En octubre de 1979, Brandt se reunió con los disidentes de Alemania Oriental, Rudolf Bahro y sus partidarios, quienes habían escrito la alternativa. Bahro y sus partidarios fueron atacados por la Alemania del Este, la Stasi, que era la organización de seguridad, encabezada por Erich Mielke, por sus escritos, que sentaron las bases teóricas de una oposición de izquierda al fallo del partido SED y sus aliados dependientes, y que promovió nuevas y modificadas partes. Todo esto se describe ahora como "el cambio desde dentro". Brandt había pedido la liberación de Bahro y dio la bienvenida a las teorías de Bahro, que avanzaron en el debate dentro de su propio Partido Socialdemócrata. 

A finales de 1989, Brandt se convirtió en uno de los primeros líderes de izquierda en la Alemania Occidental a favor de una reunificación rápida y pública de Alemania, en lugar de promover algún tipo de federación de dos Estados u otro tipo de arreglo provisional. A partir de 1989, Willy Brandt estuvo al tanto del proceso de la histórica reunificación de las dos Alemanias. Primero tuvo la satisfacción de ver la caída del célebre Muro de Berlín que dividía las dos partes de la capital alemana en noviembre de 1989 y finalmente estuvo presente en la emotiva ceremonia de la histórica y notable reunificación alemana que tuvo lugar el 3 de octubre de 1990 en Berlín al lado del entonces Canciller Helmut Kohl al pie del Palacio de Reichstag y a pocos metros de la Puerta de Brandeburgo, hoy símbolo de toda Berlín reunificada.

Una de las últimas apariciones públicas de Brandt fue en un vuelo a Bagdad, Irak, para liberar a rehenes occidentales en poder de Saddam Hussein, tras la invasión iraquí de Kuwait en 1990. Brandt logró la liberación de un gran número de ellos, y el 9 de noviembre de 1990 su avión aterrizó con 174 rehenes liberados a bordo en el aeropuerto de Francfort del Meno.

Desde 1941 hasta 1946 Brandt estuvo casado con Anna Carlotta Thorkildsen (hija de un padre noruego y una madre germano-estadounidense). Tuvieron una hija, Nina Brandt (nacida en 1940). Después de que Brandt y Thorkildsen se divorciaran en 1946, Brandt se casó con la noruega Rut Hansen en 1948. Hansen y Brandt tuvieron tres hijos: Peter Brandt (nacido en 1948), Lars Brandt (nacido en 1951) y Matthias Brandt (nacido en 1961). Hoy en día Peter es historiador, Lars es artista y Matthias es actor. Después de 32 años de matrimonio, Willy Brandt y Rut Brandt Hansen se divorciaron en 1980, y desde el día en que se divorciaron, nunca volvieron a verse. El 9 de diciembre de 1983, Brandt se casó con Brigitte Seebacher (nacida en 1946).

Rut Brandt Hansen vivió varios años más después de divorciarse de Willy Brandt, pero murió el 28 de julio de 2006 en Berlín.

En 2003, Matthias Brandt hizo el papel de Guillaume en la película de ""Im Schatten der Macht"" ("A la sombra del poder"), dirigida por el cineasta alemán Oliver Storz. Esta película trata sobre el asunto Guillaume y la renuncia de Willy Brandt a la cancillería. Matthias causó una pequeña controversia en Alemania cuando se anunció que iba a representar al hombre que traicionó a su padre y que le llevó a dimitir en 1974. A principios de ese año, cuando Brandt y Guillaume tomaron unas vacaciones juntos en Noruega, era Matthias, que tenía entonces 12 años de edad, quien fue el primero en descubrir que Guillaume y su esposa "estaban escribiendo cosas misteriosas en máquinas de escribir toda la noche".

A principios de 2006, Lars Brandt publicó una biografía de su padre llamada ""Andenken"" ("Recuerdo"). Este libro ha sido objeto de cierta controversia. Algunos lo ven como un recuerdo amoroso de la relación padre-hijo, pero otros lo etiquetan como una declaración brutal de un hijo que sigue pensando que nunca había tenido un padre que realmente lo amaba.

Willy Brandt murió de cáncer de colon en su casa en Unkel, una ciudad a orillas del río Rin, el 8 de octubre de 1992, y se le dio un funeral de Estado. Está enterrado en el cementerio de Zehlendorf, en Berlín.

Cuando el SPD trasladó su sede de Bonn a Berlín a mediados de la década de 1990, la nueva sede fue nombrada el "Willy Brandt Haus" (Casa Willy Brandt). Uno de los edificios del Parlamento Europeo en Bruselas lleva su nombre desde 2008.

En 2009, la Universidad de Érfurt renombró a su escuela de postgrado de la administración pública como la "Escuela de Políticas Públicas Willy Brandt". Una escuela secundaria privada de lengua alemana en Varsovia, Polonia, también lleva su nombre.

El 11 de diciembre de 2009, el nombre de Willy Brandt se adjuntó al Aeropuerto Internacional Berlín-Brandeburgo.




</doc>
<doc id="8205" url="https://es.wikipedia.org/wiki?curid=8205" title="Jimmy Carter">
Jimmy Carter

James Earl Carter, Jr. (Plains, Georgia, Estados Unidos, 1 de octubre de 1924), mejor conocido como Jimmy Carter, es un político estadounidense del Partido Demócrata que fue el trigésimo noveno presidente de los Estados Unidos (1977-1981), antes había ejercido de gobernador del estado de Georgia (1971-1975) y de senador en la Asamblea General de Georgia (1962-1966). Carter fue galardonado con el en 2002, por sus esfuerzos «para encontrar soluciones pacíficas a los conflictos internacionales, impulsar la democracia y los derechos humanos y fomentar el desarrollo económico y social».

Su mandato como presidente de Estados Unidos, estuvo marcado por importantes éxitos en política exterior, como los tratados sobre el Canal de Panamá, los Acuerdos de paz de Camp David (tratado de paz entre Egipto e Israel), el tratado SALT II con la URSS y el establecimiento de relaciones diplomáticas con la República Popular China y vivió sus momentos más tensos con la crisis de los rehenes en Irán. En política interior, su gobierno creó los ministerios de Energía y Educación y reforzó la legislación sobre protección medioambiental.

Desde que abandonó la Casa Blanca, se ha dedicado a la mediación en conflictos internacionales y al apoyo de causas humanitarias. En 1982, fundó junto con su esposa Rosalynn, el Centro Carter, una organización no gubernamental que lucha por el avance de los derechos humanos, la mediación en conflictos internacionales y que ha estado presente como observador en distintos procesos electorales.

Carter nació el 1 de octubre de 1924 en Plains, un pequeño pueblo agrícola, cercano a Americus, en el estado de Georgia.

Los antepasados de Carter procedían del sur de Inglaterra (su familia paterna llegó a las Colonias americanas en 1635), y han vivido en el Estado de Georgia durante varias generaciones. Carter ha documentado antepasados suyos que lucharon en la revolución estadounidense y su abuelo, L.B. Walker Carter (1832–1874), combatió en el ejército de los Estados Confederados durante la guerra de Secesión.

Su padre fue James Earl Carter, un acomodado empresario agrícola que cultivaba algodón y cacahuetes y que ejercía el papel tradicional de terrateniente del sur de Estados Unidos. Carter lo describe como un brillante granjero y un estricto segregacionista que trataba a los trabajadores negros con respeto y justicia. Su madre fue Bessie Lillian Gordy, una enfermera diplomada en la Wise Clinic en Plains que transmitió a su hijo su afición por la lectura.

Jimmy era el mayor de los cuatro hijos de la pareja. En su infancia, durante la Gran depresión, la familia se trasladó a vivir a una granja que su padre había comprado, situada en Archery cerca de Plains. Según narra Carter en su libro de memorias "An Hour Before Daylight: Memoirs of a Rural Boyhood", en la granja, a pesar de ser una de las familias más prósperas de la comunidad, carecían de electricidad y agua corriente. La mayoría de sus vecinos eran afroamericanos, aparceros o peones de la explotación de su padre, Carter estaba en contacto permanente con ellos, comía en sus hogares y, cuando sus padres se encontraban fuera, pasaba la noche en casa de Rachel y Jack Clark, empleados de la granja familiar. Este contacto interracial solo era posible en la granja puesto que el rígido código legal de segregación racial existente establecía la separación en la escuela, la iglesia y otros lugares públicos. En cierta ocasión, Carter fue al cine a la ciudad de Americus, con su mejor amigo A.D. Davis, para ello tuvieron que viajar en vagones separados en el tren, en compartimentos para "blancos" y para "personas de color". Al llegar a la ciudad tuvieron que caminar hasta el teatro juntos, pero de forma separada, también tuvieron que separarse para ver la película y de nuevo para volver hasta su casa. Carter afirma que "No recuerdo siquiera cuestionar la separación racial obligatoria, que aceptábamos como la respiración o como despertar en Archery cada mañana".

Desde una edad temprana, Carter demostró ser un alumno aplicado al que le gustaba mucho la lectura, estudió secundaria en la Plains High School. Al terminar el instituto, en 1941, tenía la intención de ingresar en la Academia Naval de Estados Unidos, pero para acceder a esta institución era necesario el respaldo de un senador o congresista de Estados Unidos, que su padre no consiguió hasta el verano de 1942. Mientras tanto, se matriculó en la Universidad Georgia Southwestern College en Americus y en el Instituto de Tecnología de Georgia para mejorar su preparación en Ciencias. En verano de 1943 ingresó en la Academia Naval de los Estados Unidos en Annapolis, donde se graduó como alférez en 1946, en el puesto 59 de los 820 alumnos de su promoción, obteniendo igualmente una licenciatura en Ciencias. En febrero de ese mismo año contrajo matrimonio con Rosalynn Smith, una de las mejores amigas de su hermana. Posteriormente, cursó física nuclear y tecnología de reactores en el Union College, aunque no llegó a completar esos estudios.

En 1948 accedió a la Escuela de Submarinos, posteriormente fue destinado al Pacífico y escogido por el almirante Hyman Rickover para participar en el entonces novedoso programa de submarinos nucleares. En 1953, a pesar de que su intención era proseguir su carrera en la marina, el fallecimiento de su padre le llevó a dimitir de sus cargos militares para asumir la dirección del negocio familiar de cultivo de cacahuates, en su pueblo natal.

También desde muy joven mostró un profundo sentimiento cristiano, impartiendo clases en la escuela dominical. Durante su carrera política manifestó que Jesucristo había marcado su vida; de hecho, durante su mandato presidencial oraba varias veces al día.

Jimmy Carter comenzó su carrera política participando en las juntas locales que administraban algunas escuelas, hospitales y bibliotecas de su comarca. En 1961, fue elegido miembro del Senado de Georgia, permaneciendo en este cargo durante dos legislaturas.

Su elección de 1961, la narró en su libro "Turning Point: A Candidate, a State, and a Nation Come of Age"; la elección estuvo envuelta en un ambiente de corrupción dirigido por Joe Hurst, sheriff del Condado de Quitman, durante las votaciones se produjeron graves abusos, como el voto de personas fallecidas y recuentos llenos de listas de personas que supuestamente habían acudido a votar en orden alfabético. En este entorno fraudulento, significó un desafío, ganar su elección. Esta convocatoria electoral supuso también el final del régimen de voto imperante en el Estado de Georgia, al declarar la Corte Suprema de los Estados Unidos inconstitucional (sentencia Gray v. Sanders), en 1963, el sistema de "votos por condados" en lugar de "por personas".
En 1964 fue reelegido para ejercer un segundo mandato de dos años. En 1966, Carter rechazó ser candidato a una tercera reelección, para iniciar su candidatura para Gobernador del Estado. Su puesto en el senado estatal fue ocupado por su primo hermano, Hugh Carter, elegido por el Partido Demócrata.

En 1966, cuando terminaba su legislatura en el senado de Georgia, se planteó presentarse como candidato a la Cámara de Representantes de los Estados Unidos, pero cuando su rival republicano, Howard Callaway, retiró su candidatura a esta cámara para postularse como candidato para gobernador del estado de Georgia, Carter que no quería ver a un gobernador republicano en su Estado, también se retiró de la carrera para el Congreso y se incorporó como candidato a gobernador. En las primarias demócratas, Carter fue el tercero más votado por detrás de Ellis Arnall y Lester Maddox. La participación de Carter fue importante y trascendente ya que forzó una Segunda vuelta electoral en la que venció Maddox, candidato partidario de la segregación racial y que había sido el segundo más votado en la primera vuelta. Durante este proceso, Carter se presentó como una alternativa moderada, tanto frente a Arnall, más liberal, como frente al más conservador Maddox. Aunque Carter fue derrotado, la fortaleza de su posición se contempló como un éxito para un senador del Estado poco conocido.
En las elecciones participaron tres candidatos, Maddox por el partido demócrata, Callaway por los republicanos y Arnal que concurrió fuera de las listas. Callaway fue el más votado, pero Maddox fue nombrado gobernador del Estado por la Asamblea General de Georgia.

Durante los siguientes cuatro años, Carter volvió a su explotación agrícola, dedicándose a preparar y planificar cuidadosamente la siguiente campaña para gobernador de 1970, participando durante esos cuatro años en más de 1.800 actos políticos por todo el estado de Georgia.

En las elecciones de 1970, realizó una ardua campaña populista para la primarias del partido demócrata contra del ex-gobernador Carl Sanders, etiquetando a su oponente como "Cufflinks Carl". Carter nunca fue un segregacionista y se negó a unirse al segregacionista "Consejo de Ciudadanos Blancos", lo que provocó el boicot a su empresa de cacahuetes. Su familia fue también una de las dos únicas que votaron a favor de admitir a los negros a la Plains Baptist Church.
Sin embargo según el historiador E. Stanly Godbold, durante esta campaña, pronunció las palabras que los segregacionistas querían escuchar, se opuso a los transporte de escolares destinados a favorecer la integración, se pronunció a favor de las escuelas privadas y manifestó su disposición a invitar al gobernador de Alabama, George Wallace, conocido por su postura contraria a la integración racial, para que pronunciara un discurso en su campaña. En el mismo sentido sus ayudantes de campaña hicieron pública una fotografía de su oponente junto a dos jugadores de baloncesto negros. Después de su estrecha victoria sobre Sanders en las primarias, Carter fue elegido Gobernador al derrotar al candidato republicano, Hal Suit.

Tras su elección como gobernador, Carter hizo una declaración en su discurso inaugural que desagradó profundamente a los segregacionistas: «Les digo con toda franqueza, que el tiempo de la discriminación racial ha terminado.
Ninguna persona sea pobre, campesina, débil, o negra debería tener que soportar la carga adicional de ser privado de la oportunidad de una educación, un puesto de trabajo o la simple justicia.»

El senador de Georgia, Leroy Johnson, uno de los primeros senadores de raza negra de este estado, reflexionaba sobre esta declaración, afirmando: "Nos quedamos muy contentos. Muchos de los segregacionistas blancos estaban disgustados y estoy convencido de que aquellas personas que lo apoyaron, no lo habrían hecho si hubieran sabido que iba a pronunciar esa declaración."

Carter juró como 76º gobernador de Georgia, el 12 de enero de 1971 y ocupó este cargo hasta el 14 de enero de 1975. Su vicegobernador fue su predecesor en el cargo, Lester Maddox, con el que mantuvo constantes enfrentamientos públicos durante sus cuatro años de mandato.

Como ya se ha manifestado, Carter declaró ya en su discurso inaugural que la época de la segregación racial había terminado, y que la discriminación racial no tenía cabida en el futuro del Estado y fue el primer cargo público estatal en el denominado "Sur Profundo" en pronunciarse públicamente en este sentido. Posteriormente, Carter nombró bastantes afroamericanos para cargos públicos y fue denominado a menudo como uno de los "Gobernadores del Nuevo Sur". Mucho más moderado que sus predecesores, apoyó la lucha contra la segregación racial y la ampliación de los derechos de los afroamericanos.

Carter se oponía personalmente al aborto, aunque apoyó su legalización después de la histórica sentencia de 1973 de la Corte Suprema de los Estados Unidos, a raíz del Caso Roe contra Wade, en la que se reconocía, aunque con limitaciones, el derecho a la interrupción voluntaria del embarazo. Posteriormente, como presidente no apoyó el incremento de fondos federales para la práctica de abortos y fue criticado por la Unión Estadounidense por las Libertades Civiles por no hacer lo suficiente para encontrar alternativas al aborto.

Carter mejoró la eficiencia del gobierno mediante la fusión de alrededor de 300 agencias estatales en 30. Uno de sus ayudantes recordaba así al gobernador Carter: ""estaba allí con nosotros, trabajando igual de duro, profundizando en cada pequeño problema. Era su programa y trabajó en él tan duro como cualquiera y el producto final era claramente suyo"".

También impulsó reformas durante su legislatura como, la prestación de ayuda estatal iguales a las escuelas de las zonas ricas y pobres de Georgia, la creación de centros comunitarios para niños con discapacidad mental y la ampliación de los programas educativos para presos. Carter se sintió especialmente orgulloso de un programa que presentó para el nombramiento de magistrados y funcionarios del gobierno estatal, basado en el mérito, en lugar de la influencia política.

En 1972, cuando, George McGovern, senador de los Estados Unidos por Dakota del Sur, se presentó a las primarias del partido demócrata para elegir el candidato a presidente de Estados Unidos, Carter convocó una conferencia de prensa en Atlanta para advertir que McGovern era inelegible, criticando que era demasiado liberal tanto en política exterior como en política nacional. Cuando la nominación de McGovern ya era inevitable, Carter presionó para intentar convertirse en su vicepresidente. Durante la Convención Nacional Demócrata de 1972, apoyó la candidatura del senador Henry M. Jackson, de Washington. Sin embargo, Carter recibió 30 votos, en la caótica votación para vicepresidente en la convención. McGovern ofreció el segundo lugar a Reubin Askew, uno de los "nuevos gobernadores del sur", que la rechazó.

Durante sus campañas presidenciales, manifestó siempre su posición contraria a la pena de muerte (en la misma postura se encontraban el candidato demócrata que le precedió, George McGovern, y los dos siguientes, Walter Mondale y Michael Dukakis). En la actualidad, Carter es conocido por su oposición frontal a esta pena en todas sus formas y en su discurso del Premio Nobel, instó a la "prohibición de la pena de muerte".

Después de que en 1972 la Corte Suprema de los Estados Unidos anulase la pena de muerte en el Estado de Georgia, Carter propuso sustituirla por la cadena perpetua en la legislación estatal (una opción que antes no existía). Cuando la asamblea de Georgia aprobó una nueva ley de pena de muerte, Carter, a pesar de expresar sus reservas sobre su constitucionalidad, firmó el 28 de marzo de 1973 la nueva normativa que autorizaba la pena de muerte en casos de asesinato, violación y otros delitos y ponía en práctica los procedimientos judiciales que se ajustaban a los requisitos constitucionales recientemente anunciados. En 1976, el Tribunal Supremo confirmó esta nueva legislación en el caso de asesinato Coker v. Georgia, en el que la Corte Suprema dictaminó que la pena de muerte era inconstitucional en su aplicación a los delitos de violación.

El 31 de marzo de 1971, el teniente del ejército norteamericano, William Calley, fue condenado a cadena perpetua por el asesinato de 22 civiles vietnamitas en la masacre de My Lai, en Vietnam. El presidente Nixon, tres días después de la sentencia, conmutó esta pena por un arresto domiciliario de carácter permanente. Jimmy Carter, disconforme con la condena de cadena perpetua, instituyó el "Día del luchador estadounidense" pidiendo a los georgianos que condujesen durante una semana sus automóviles con las luces encendidas en apoyo a Calley. El Gobernador de Indiana pidió también que todas las banderas del estado ondeasen a media asta en honor a Calley y los gobernadores de Utah y Mississippi también mostraron su desacuerdo con el veredicto.

Cuando Carter inició su candidatura a la presidencia de Estados Unidos en 1976, era considerado un político con poca experiencia y escaso reconocimiento a nivel nacional, sólo era reconocido por el 2% de los votantes, y con pocas posibilidades contra políticos más conocidos a nivel nacional. Cuando comunicó a su familia su intención de postularse a la presidencia de Estados Unidos, su madre le preguntó: "¿Presidente de qué?". Sin embargo, durante 1976, el escándalo Watergate seguía todavía fresco en la mente de los votantes y su posición como un político sin gran experiencia y ajeno a la politiquería de Washington D.C., se convirtió en un factor atractivo para los votantes, la pieza central de su plataforma de campaña fue la reorganización del gobierno.

Carter pronto se convirtió en el favorito al ganar el caucus de Iowa y las primarias de Nuevo Hampshire. Utilizó una estrategia de dos frentes: En el sur, en el que la mayoría había aceptado tácitamente a George Wallace de Alabama, Carter se presentó como un hijo predilecto de carácter moderado. Cuando Wallace demostró ser una fuerza agotada, Carter barrió en la región. En el Norte, donde Carter tenía pocas posibilidades de obtener grandes mayorías, hizo un llamamiento en gran medida a los votantes conservadores cristianos y a la población rural. Ganó en varios estados del Norte, construyendo el mayor bloque. La estrategia de Carter consistía en alcanzar una región antes que los otros candidatos pudiesen extender su influencia allí, viajó más de 50.000 kilómetros, visitó 37 estados y pronunció más de 200 discursos antes de que otros candidatos hubieran anunciado que estaban en la carrera por la presidencia. Rechazó, desde un principio, presentarse como un candidato de carácter regional, demostrando ser el único demócrata con una verdadera estrategia nacional y así finalmente logró la candidatura de su partido.

Su avance fue lento, según una encuesta de Gallup, todavía el 26 de enero de 1976, Carter era la primera opción de sólo un cuatro por ciento de los votantes demócratas. Sin embargo, a mediados de marzo, según Shoup, Carter no solo iba muy por delante de los otros contendientes demócrata, sino que también marchaba por delante del presidente Ford por un pequeño porcentaje.

Eligió al senador Walter Mondale como candidato a la vicepresidencia. Atacó la falta de transparencia de la política de Washington en sus discursos, y ofreció un bálsamo de carácter religioso para sanar las heridas de la nación.

En su campaña electoral, Carter apeló a vagos principios morales, criticó la burocracia de Washington y prometió sinceridad y honradez. Planteó su campaña como un "outsider", dirigida a los electores que estaban hartos de políticos profesionales y de soluciones convencionales. Inició la campaña presidencial con una ventaja considerable sobre Ford, que fue reduciéndose en el transcurso de la misma, y que finalmente le permitió a Carter la victoria el 2 de noviembre de 1976 por un margen estrecho. Carter ganó el voto popular por un 50,1 % contra 48,0 % de Ford y recibió 297 votos electorales contra 240 de Ford, convirtiéndose en el primer presidente del denominado Sur Profundo desde la elección de Zachary Taylor en 1848.

Su comportamiento público desafió las normas establecidas, durante la campaña, a pesar de su proclamada condición de devoto cristiano y catequista, concedió una entrevista a Robert Scheer para la revista Playboy, en la que reconocía que "He mirado a muchas mujeres con lujuria y he cometido adulterio muchas veces en el fondo de mi corazón". La entrevista llegó a los quioscos un par de semanas antes de las elecciones.

Jimmy Carter fue el trigésimo noveno Presidente de los Estados Unidos desde 1977 hasta 1981. Carter se destacó por un estilo relativamente heterodoxo, que no encajó en el establishment de Washington, ni contó con un apoyo sólido de su partido, y por sus originales opiniones y juicios, sin tener un programa demasiado definido. Su administración trató de hacer un gobierno "competente y compasivo", pero se encontró con una grave crisis económica, que dificultó el logro de sus objetivos, caracterizada por la subida de los precios de la energía y la estanflación. Al final de su periodo de gobierno, Carter había logrado reducir sustancialmente el desempleo y el déficit público, pero no fue capaz de acabar por completo con la recesión. Carter creó los departamentos de educación y de Energía, estableció una política energética nacional y reformó la seguridad social. En asuntos exteriores, Carter inició los Acuerdos de Camp David, los tratados del Canal de Panamá y la segunda ronda de los Acuerdos SALT. A lo largo de su trayectoria como presidente, Carter subrayó firmemente los derechos humanos. Devolvió la Zona del Canal de Panamá a Panamá, enfrentándose a las críticas en su país por su decisión, que fue vista como otra señal de debilidad de Estados Unidos y de su hábito de dar marcha atrás cuando ante la confrontación. El último año de su mandato presidencial estuvo marcado por varias crisis importantes, como la toma en 1979 de la embajada estadounidense en Irán y retención de rehenes por estudiantes iraníes, el intento sin éxito de rescate de los rehenes, una grave escasez de combustible y el comienzo de la Guerra de Afganistán.

En su discurso de toma de posesión pronunció:

Carter había hecho campaña con la promesa de eliminar los trapicheos de la denominada ""presidencia imperial"" que imperaba con Richard Nixon y comenzó su presidencia, de acuerdo con esa promesa, el día de su toma de posesión; al ir caminando por la Avenida Pennsylvania, desde el Capitolio hasta la Casa Blanca, en su desfile inaugural, rompiendo así con el protocolo y la historia reciente. Sus primeros pasos en la Casa Blanca fueron más lejos en esta dirección, reduciendo la plantilla de asesores en un tercio, suprimiendo los conductores para los miembros del gabinete y poniendo a la venta el yate presidencial, el USS Sequoia.

En el primer día de Carter en el cargo, el 20 de enero de 1977, cumplió una promesa de campaña mediante la publicación de un Decreto Ley que declaraba una amnistía incondicional para los insumisos de la Guerra de Vietnam.

Bajo la tutela de Carter, se aprobó la Ley de Desregulación de Aerolíneas de 1978, que eliminó la Junta Aeronáutica Civil, también impulsó la desregulación del transporte por carretera, ferrocarril, comunicaciones, finanzas e industrias.

Entre los presidentes que han servido al menos un periodo completo de presidencia, Carter es el único que no realizó ningún nombramiento en la Corte Suprema.

Carter fue el primer presidente que se propuso abordar el tema de los derechos de los homosexuales. Se opuso a la Iniciativa Briggs, un proyecto de ley de California que prohibía a los homosexuales y a los defensores de los derechos de los homosexuales ser maestros de escuelas públicas. La administración de Carter fue la primera en reunirse con un grupo de activistas de derechos de los homosexuales y en los últimos años actuó en favor de las uniones civiles y puso fin a la prohibición de homosexuales en el ejército. en este sentido declaró que ""se opone a todas las formas de la discriminación por motivos de orientación sexual y cree que debe haber igualdad de protección bajo la ley para las personas que difieren en la orientación sexual"".

A pesar de pedir una reforma del sistema fiscal durante su campaña presidencial, cuando llegó al poder no hizo gran cosa para cambiarlo.

Durante todos los años de la presidencia se produjo un déficit en el presupuesto del gobierno federal, aunque el porcentaje de deuda sobre el PIB, decreció ligeramente. Su Ley de Conservación de Intereses Nacionales de las Tierras de Alaska convirtió 103 millones de acres (417.000 km2) en Parque nacional en Alaska.

Carter realizó una exitosa campaña electoral definiéndose como un "extraño en Washington", en la que criticaba al presidente Gerald Ford y al Congreso de los Estados Unidos, controlado por los demócratas. Como presidente, continuó con esta línea, su negativa a jugar con las "reglas de Washington" contribuyó a una difícil relación de la administración Carter con el Congreso. Hamilton Jordan y Frank Moore, en particular, se enfrentaron desde el principio, con los líderes demócratas, como el Portavoz de la Cámara de Representantes, Tip O'Neill. Las relaciones con el Capitolio se agriaron por llamadas telefónicas no devueltas, insultos (tanto reales como imaginarios) y una falta de voluntad para intercambiar favores políticos y debilitó la capacidad del presidente para impulsar su ambiciosa agenda.

Durante los primeros 100 días de su presidencia, Carter remitió una carta al Congreso, proponiendo el rechazo de varios proyectos. Entre los que manifestaron su oposición a esa propuesta, se encontraba el senador Russell B. Long, un poderoso demócrata del Comité de Finanzas del Senado. El plan de Carter fue revocado y el sentimiento de amargura se convirtió en un problema para Carter. El plan de Carter fue rechazado, lo que produjo un sentimiento de amargura en Carter.

El rechazo abrió una brecha entre la Casa Blanca y el Congreso, Carter manifestó que la oposición más intensa y creciente a sus políticas provenían del ala liberal del Partido Demócrata de los Estados Unidos, que atribuyó a la ambición de Ted Kennedy para reemplazarlo como presidente.

Pocos meses después de iniciado su mandato, y pensando que tenía el apoyo de cerca de 74 congresistas, Carter publicó una "lista negra" de 19 proyectos que, según Carter, suponían un "pork barrel" de gasto público, manifestando que vetaría cualquier iniciativa legislativa que incluyera cualquier proyecto de esta lista.

Esta lista se encontró con la oposición del líder del partido demócrata. Carter había incluido un proyecto de ley de ríos y puertos como innecesario y el portavoz de la Cámara de Representantes, Tip O'Neill, pensó que era desaconsejable que el presidente interfiriese en asuntos que tradicionalmente habían formado parte de la esfera de competencias del Congreso. Tras estos hechos, Carter quedó aún más debilitado y tuvo que firmar un proyecto de ley que contenía proyectos de su lista negra.

Más tarde, el Congreso rechazó aprobar las principales disposiciones de su ley de protección de los consumidores y su paquete de reforma laboral y Carter vetó un paquete de obras públicas calificándolas de "inflacionarias", puesto que contenía lo que él consideraba gastos innecesarios. Los líderes del Congreso percibieron que el apoyo público a la iniciativa legislativa de Carter era débil, y se aprovecharon de ella. Después de destripar el proyecto de ley de protección al consumidor, transformaron su plan de impuestos en nada más que gastos de especial interés, tras lo cual, Carter se refirió al Comité de impuestos del Congreso como "manada de lobos".

En 1973, durante la administración de Nixon, la Organización de Países Exportadores de Petróleo (OPEP) redujo los suministros de petróleo disponible en el mercado mundial, en parte debido a la depreciación del dólar causada por la salida del patrón oro de Nixon y en parte como reacción contra Estados Unidos, por el envío de armas a Israel durante la Guerra de Yom Kippur. Esto provocó la crisis del petróleo de 1973 que supuso una subida brusca de los precios del petróleo que empujó la inflación y desaceleró el crecimiento. El gobierno de EE.UU., tras el anuncio, impuso controles de precios en la gasolina y en el petróleo, que provocaron escasez y largas colas en las gasolineras. Las colas se evitaron al suprimir los controles de precios en la gasolina, estos controles del petróleo se mantuvieron hasta la presidencia de Reagan. Cuando en 1977 Carter llegó a la Casa Blanca, dijo a los estadounidenses que la crisis energética era "un peligro claro y presente para la nación" y "el equivalente moral de la guerra" y diseñó un plan para intentar hacer frente al problema y manifestó que la oferta mundial de petróleo probablemente sólo podría cubrir la demanda estadounidense durante seis u ocho años más.

En 1977, Carter convenció a los demócratas en el Congreso para crear el Departamento de Energía de los Estados Unidos con el objetivo de promover el ahorro de energía. Carter estableció controles de precio al petróleo y al gas natural e instaló paneles solares para calentar agua caliente sanitaria, en la Casa Blanca, y habilitó una estufa de leña en su vivienda. Ordenó a la Administración de Servicios Generales cortar el agua caliente en algunas instalaciones federales, y pidió que los adornos públicos navideños permaneciesen sin luz en las navidades de 1979 y 1980. Se establecieron a nivel nacional, controles en los termostatos de los edificios gubernamentales y comerciales para que no sobrepasasen temperaturas en el invierno por encima de 18,33 °C ni disminuyesen en el verano por debajo de 25 °C.

Como reacción a la crisis energética y las creciente preocupación sobre la contaminación del aire, Carter también firmó la Ley Nacional de Energía y la Ley Política de Regulación de los Servicios Públicos. El propósito de estas leyes fue estimular la conservación energética y el desarrollo de los recursos energéticos nacionales, incluidos los renovables, como la energía solar y la eólica.

Sin embargo, durante la crisis de 1979, Carter reintegró algunos controles de precios en la gasolina, lo que volvió a ocasionar colas en las gasolineras. Durante su discurso del "malestar", anunció una liberalización gradual de los controles de precios, junto con la imposición de un "Impuesto sobre beneficios excepcionales" para financiar el iniciativas de eficiencia energética. El impuesto se instauró en 1980 gravando la producción nacional de petróleo, el impuesto se derogó en 1988, cuando los precios se derrumbaron, haciendo posible la supresión del impuesto. Este tributo no era un gravamen sobre los beneficios, sino un impuesto especial sobre la diferencia entre un " precio base y el precio de mercado.

La historia económica de la Administración Carter se puede dividir en dos períodos más o menos iguales. Los dos primeros años fueron una época de continua recuperación de la severa recesión de 1973-1975, que había dejado la inversión en capital fijo en su nivel más bajo desde la recesión de 1970 y el desempleo en el 9%. Los otros dos años estuvieron marcados por una inflación de dos dígitos con unos tipos de interés muy altos, la escasez de petróleo y el bajo crecimiento económico. la economía del país creció a un promedio de 3,4% durante la administración Carter (a la par con el promedio histórico). Sin embargo, cada uno de estos períodos de dos años, difieren radicalmente.

La economía de Estados Unidos, que había crecido un 5% en 1976, siguió a un ritmo similar durante 1977 y 1978. El desempleo disminuyó desde el 7,5% en enero de 1977 al 5,6% en mayo de 1979, con más de 9 millones de nuevos empleos netos creados durante ese íntervalo y el ingreso familiar per cápita creció un 5% entre 1976 y 1978. La recuperación de la inversión empresarial en evidencia durante el año 1976 se fortaleció también. La inversión privada fija (maquinaria y construcción) creció un 30% de 1976 a 1979, las ventas de viviendas y la construcción creció de igual manera en 1978 y la producción industrial, la producción de vehículos y las ventas lo hicieron en casi un 15%, con la excepción de la viviendas nuevas iniciadas, que se mantuvo ligeramente por debajo de su pico de 1972, cada uno de estos puntos de referencia alcanzaron niveles récord en 1978 o 1979.

La crisis energética de 1979 terminó este período de crecimiento, la inflación y las tasas de interés permanecieron altas, mientras que el crecimiento económico, la creación de empleo y la confianza de los consumidores se redujo drásticamente. La política monetaria relativamente flexible adoptada por el presidente de la Reserva Federal, William G. Miller, había contribuido a la generación de una inflación algo más elevada, con aumentos del 5,8% en 1976 al 7,7% en 1978. La brusca y repentina subida de los precios del crudo, por parte de la OPEP, condujo a la inflación a alcanzar niveles de dos dígitos, con un promedio del 11,3% en 1979 y el 13,5% en 1980. La repentina escasez de gasolina al comienzo de la temporada de vacaciones de 1979, exacerbó el problema, y vendría a simbolizar la crisis entre el público en general, la escasez, originada en el cierre de las instalaciones de refinamiento de Amerada Hess, dio lugar a una demanda del gobierno federal contra la empresa.

Carter, como su antecesor el presidente Ford, pidió al Congreso la imposición de controles en los precios de la energía, productos médicos y precios al consumidor, pero no pudo lograr la aprobación de estas medidas debido a la fuerte oposición del Congreso, pero utilizando una ley de conservación y política energética, aprobada por el congreso durante la presidencia de Gerald Ford, que daba a los presidentes la autoridad para desregular los precios del petróleo en el mercado norteamericano, consiguió fomentar la producción y el ahorro de petróleo. Las importaciones de este producto, que habían alcanzado un récord de 2.400 millones de barriles en 1977 (el cincuenta por ciento del suministro norteamericano), se redujeron a la mitad entre 1979 y 1983.

En 1987 el periodista William Greider escribió "Los secretos del Templo", un libro publicado poco después del desplome de la bolsa, en el que manifiesta que Volcker y sus protegidos posteriores han dominado la Reserva Federal posteriormente por lo menos durante varias décadas a través de varias administraciones de EE.UU. Durante la administración de Carter, la economía sufrió una inflación de dos dígitos, junto con tasas de interés muy altas, la escasez de petróleo, el alto desempleo y el crecimiento económico lento. El crecimiento de la productividad en los Estados Unidos se había reducido a una tasa media anual del 1%, frente al 3,2% de la década de 1960. Hubo también un creciente déficit del presupuesto federal, que aumentó a $ 66.000 millones. La década de 1970 se describe como un período de estanflación, así como mayores tasas de interés. La inflación de precios (un aumento del nivel general de precios) creó incertidumbre en la presupuestación y planificación y provocó huelgas por aumentos de sueldo más probable. Carter, como Nixon, pidió al Congreso que se impusiesen controles de precios sobre la energía, la medicina y los precios al consumidor, pero el Congreso no estaba de acuerdo.

A raíz de una reestructuración del gabinete en el que Carter solicitó la dimisión de varios miembros de su gabinete, Carter nombró a G. William Miller como Secretario del Tesoro, Miller había estado sirviendo como Presidente de la Reserva Federal. Para reemplazarlo y con el fin de calmar los mercados, Carter nombró a Paul Volcker como presidente de la Reserva Federal. Volcker llevó a cabo una política monetaria restrictiva para bajar la inflación. Volcker y Carter tuvieron éxito, pero sólo tras pasar primero por una fase recesiva durante la que la economía se desaceleró y creció el desempleo. El alivio de la inflación vio sus resultados durante el primer mandato de Ronald Reagan, quien volvió a nombrar a Volcker como presidente de la Reserva.

Bajo la dirección de Volcker, la Reserva Federal elevó la tasa de descuento desde el 10%, que estaban cuando Volcker asumió la presidencia en agosto de 1979 hasta el 12%, en un plazo de dos meses. La tasa preferencial de interés alcanzó el 21,5% en diciembre de 1980, la tasa más alta en la historia de Estados Unidos. Carter aprobó mediante decreto ley un programa de medidas de austeridad que trató de justificar porque la inflación había alcanzado "un estado de crisis", la inflación y el tipo de interés a corto plazo alcanzaron el dieciocho por ciento en febrero y marzo de 1980. Las inversiones en renta fija (bonos, tanto en poder de Wall Street y las pensiones pagadas a los jubilados) eran cada vez de menos valor. Las altas tasas de interés conduciría a una fuerte recesión en la década de 1980, que coincidió con la campaña de Carter para la reelección.

En 1979, cuando se inició la crisis del petróleo, Carter estaba planeando dar su quinto mayor discurso sobre la energía, sin embargo, sintió que el pueblo estadounidense ya no le escuchaba. Carter se retiró a la residencia presidencial de Camp David. Durante más de una semana, un velo de secreto envolvió sus actuaciones, convocó en su residencia a decenas de destacados líderes del Partido Demócrata, miembros del Congreso, gobernadores, dirigentes sindicales, académicos y miembros del clero. Su analista, Pat Caddell, le dijo que el pueblo de los Estados Unidos se enfrentaba a una crisis de confianza motivada por los asesinatos de John F. Kennedy, Robert F. Kennedy y Martin Luther King, Jr; la guerra de Vietnam y el Escándalo Watergate. El 15 de julio de 1979, Carter dio un discurso televisado a nivel nacional en el que identificó lo que él creía que era una "crisis de confianza" entre el pueblo estadounidense. Esto llegó a ser conocido como su "discurso del malestar", aunque nunca apareció esta palabra en el discurso:

El discurso fue escrito por Hendrik Hertzberg y Gordon Stewart. Aunque se ha dicho a menudo que fue mal recibido, The New York Times publicó el siguiente titular una semana después "El discurso eleva la valoración de Carter hasta el 37%, el público se siente de acuerdo con la crisis de confianza, tocada la fibra sensible.

La posterior pérdida de la reelección de Carter provocó que otros políticos descartaran solicitar el ahorro de energía a los estadounidenses de una manera similar. Tres días después del discurso, Carter pidió la dimisión de todos los miembros de su gabinete, y en última instancia aceptó la de los cinco que se habían enfrentado con la Casa Blanca al máximo, incluyendo el secretario de Energía, James Schlesinger, y el Jefe de Salud, Educación y Bienestar, Joseph A. Califano, conocido como partidario del senador Ted Kennedy. Carter admitió más tarde en sus memorias que simplemente debería haber pedido la dimisión de sólo los cinco miembros que renunciaron. En 2008, una información de Noticias de los EE.UU. y World Report indicaron:

After campaigning that he would never appoint a Chief of Staff, Carter appointed Jordan as a new White House Chief of Staff. Many in the administration chafed when Jordan circulated a "questionnaire" that read more like a loyalty oath. "I think the idea was that they were going to firm up the administration, show that there was real change by these personnel changes, and move on," remembers Mondale. "But the message the American people got was that we were falling apart." Carter later admitted in his memoirs that he should simply have asked only those five members for their resignations. In 2008, a U.S. News and World Report piece stated:

Después de una campaña que no volvería a nombrar a un Jefe de Estado Mayor, Carter nombró a Jordania como jefe de la Casa Blanca de personal nuevo. Muchos en la administración irritado cuando Jordania hizo circular un "cuestionario" que se parecen más a un juramento de lealtad. "Creo que la idea era que iban a concretar el gobierno, muestran que hubo un cambio real de estos cambios de personal, y seguir adelante", recuerda Mondale. "Pero el mensaje al pueblo estadounidense consiguió fue que se estaban viniendo abajo". Carter admitió más tarde en sus memorias que sólo debería haber pedido sólo los cinco miembros de su renuncia. En 2008, una pieza de Noticias EE.UU. y World Report declaró:

Durante su primer mes en el cargo, Carter disminuyó el presupuesto de defensa en seis mil millones de dólares. Uno de sus primeros actos fue ordenar la retirada unilateral de todas las armas nucleares de Corea del Sur y anunciar su intención de reducir el número de tropas estadounidenses estacionadas en ese país. Algunos militares criticaron esta decisión en conversaciones privadas y en testimonios ante comités del Congreso, en 1977, el general John K. Singlaub, jefe del Estado Mayor de las fuerzas de EE.UU. en Corea del Sur, criticó públicamente la decisión de Carter de disminuir las tropas destinadas allí. El 21 de marzo de 1977, Carter lo relevó de su cargo, manifestando que sus opiniones manifestadas públicamente, resultaban "incompatibles con la política anunciada de seguridad nacional".

Carter tenía previsto la retirada en 1982 de todas las tropas, excepto 14.000 soldados de la Fuerza Aérea de EE.UU. y especialistas en logística, pero, en 1978, después de haber recortado solo 3.600 soldados, se vio obligado a abandonar el proyecto por la presión del Congreso y las oposición de sus generales.

El secretario de Estado del gabinete de Carter, Cyrus Vance, y el asesor de Seguridad Nacional, Zbigniew Brzezinski prestaron mucha atención al conflicto árabe-israelí. Las comunicaciones diplomáticas entre Israel y Egipto aumentaron significativamente después de la Guerra de Yom Kippur de 1973 y la administración de Carter creía que era el momento adecuado para una solución global del conflicto.

A mediados de 1978, Carter estaba muy preocupado, puesto que había expirado sólo unos meses antes el Tratado de Separación entre Egipto e Israel. Carter decidió destinar un enviado especial a Oriente Medio. El embajador de Estados Unidos iba y venía entre El Cairo (Egipto) y Tel Aviv intentando reducir las discrepancias entre los dos países. Se sugirió entonces que los cancilleres se reunieran en el castillo de Leeds (Inglaterra) para discutir las posibilidades de paz. Trataron de llegar a un acuerdo, pero los ministros de exteriores no lo lograron. Posteriormente se alcanzaron los Acuerdos de Camp David de 1978, uno de los logros más importantes de Carter durante su presidencia.

Los acuerdos fueron un convenio de paz entre Israel y Egipto negociado por Carter, que culminó las negociaciones anteriores, realizadas en Oriente Medio. En estas negociaciones el rey Hasan II de Marruecos actuó como negociador entre los intereses árabes en Israel, y Nicolae Ceausescu de Rumania actuaba como intermediario entre Israel y la Organización para la Liberación de Palestina. Cuando las negociaciones iniciales finalizaron, el presidente egipcio Anwar Sadat se acercó a Carter para pedirle ayuda. Carter invitó al primer ministro israelí Menájem Beguin y Anwar el-Sadat a Camp David para continuar las negociaciones. Llegaron el 8 de agosto de 1978, ninguno de los líderes se habían encontrado desde la reunión de Viena. El presidente Carter actuó como mediador entre los dos líderes y habló con cada uno por separado para intentar alcanzar un acuerdo. Transcurrido un mes sin llegar a ninguna resolución, el presidente Carter decidió reunirse con ambos a lo largo de un viaje a Gettysburg, (Pensilvania) para romper el punto muerto. Allí les mostró el terreno donde se había desarrollado una batalla de la Guerra Civil Estadounidense, les explicó la historia de la batalla e hizo hincapié en lo importante que era alcanzar la paz con el fin de llevar la prosperidad al pueblo. Una lección que contribuyó a que cuando Beguin y Sadat regresaron a Camp David, comprendieran que debían firmar algún tipo de acuerdo.

El 12 de septiembre de 1978, el presidente Carter sugirió que se dividiesen las negociaciones sobre el Tratado de Paz en dos marcos: El marco 1 abordaría Cisjordania y la Franja de Gaza, mientras que el marco 2 se ocuparía del Sinaí.


Se preveía que después de estos pasos, el Estado de Palestina podría ser negociado.

El marco 1 no fue muy bien recibido por palestinos y jordanos que se opusieron al hecho de que Beguin y Sadat tomaron decisiones sobre su destino final, sin consultar con ellos o sus dirigentes.



El presidente Carter admitió que a pesar de todo "todavía existían grandes dificultades y muchas cuestiones difíciles de resolver.".

La reacción a esta propuesta en el mundo árabe fue muy negativa. En noviembre de 1978, se celebró una reunión de emergencia convocada por la Liga Árabe en Damasco. Una vez más, Egipto fue el tema principal de la reunión, y se condenó la propuesta de tratado que Egipto iba a firmar. Sadat fue también atacado por la prensa árabe por romper filas con la Liga Árabe y haber traicionado al mundo árabe. Las discusiones relativas al futuro tratado de paz se llevaron a cabo en ambos países. Israel insistió durante las negociaciones en que el tratado entre Israel y Egipto debía sustituir todos los demás tratados de Egipto, incluidos los firmados con la Liga Árabe y los países árabes. Israel también quería el acceso al petróleo descubierto en la región del Sinaí. El presidente Carter intervino e informó a los israelíes que EE. UU. proporcionaría a Israel el suministro de petróleo que necesitaba para los próximos 15 años, si Egipto decidía no proporcionárselo a Israel.

El Parlamento israelí aprobó el tratado con una cómoda mayoría. Por otra parte, el gobierno egipcio estaba discutiendo acerca de diferentes cuestiones. No les gustaba el hecho de que el tratado propuesto sustituiría a todos los demás tratados. Además los egipcios se sentían decepcionados porque no habían logrado vincular la cuestión del Sinaí al problema de Palestina.

El 26 de marzo de 1979, Egipto e Israel firmaron el tratado de paz en Washington D.C., el papel de Carter fue esencial. Aaron David Miller entrevistó a muchos funcionarios en su libro "The Much Too Promised Land" (2008) y concluyó lo siguiente: "No importa a quien se le pregunte, estadounidenses, egipcios, o israelíes, la mayoría están de acuerdo: sin Carter no habría existido ningún tratado de paz."

Carter inicialmente se apartó de la política establecida de contención hacia la Unión Soviética. Promovió una política exterior que situó los derechos humanos entre sus prioridades, lo cual supuso una ruptura con la actitud de sus predecesores que no prestaban atención al incumplimiento de los derechos humanos que habían cometido los países aliados de Estados Unidos. La Administración Carter dejó de dar apoyo al régimen de Somoza en Nicaragua, históricamente respaldado por Estados Unidos y dieron su ayuda al nuevo gobierno del Frente Sandinista de Liberación Nacional que asumió el poder después del derrocamiento de Somoza. Sin embargo, Carter ignoró una petición del Arzobispo Óscar Romero en El Salvador para enviar ayuda militar a ese país. Romero fue asesinado más tarde por sus críticas por la violación de derechos humanos en El Salvador. Carter también fue criticado por la activista feminista Andrea Dworkin por desentenderse de los derechos de las mujeres en Arabia Saudita.

Carter continuó la política de sus antecesores de imposición de sanciones a Rodesia, y después de que el obispo Abel Muzorewa fuera elegido primer ministro, protestó por la exclusión de las elecciones de Robert Mugabe y Joshua Nkomo. La fuerte presión de Estados Unidos y el Reino Unido lograron nuevas elecciones en Rodesia (hoy Zimbabue), que llevaron a la elección de Robert Mugabe como Primer Ministro; después de lo cual, se levantaron las sanciones, y se le concedió al país el reconocimiento diplomático. Carter también fue conocido por sus críticas a Alfredo Stroessner de Paraguay y Augusto Pinochet de Chile, aunque ambos Stroessner y Pinochet, asistieron a la firma del Tratado del Canal de Panamá, también protestó contra el Apartheid en Sudáfrica.

Carter continuó la política de Richard Nixon para normalizar las relaciones con la República Popular China. El consejero de Seguridad Nacional, Zbigniew Brzezinski y Michel Oksenberg, viajaron a Pekín a principios de 1978, donde junto con Leonard Woodcock, director de la oficina de enlace, establecieron las bases de un acuerdo para alcanzar relaciones diplomáticas y comerciales completas con la República Popular de China. En el Comunicado Conjunto sobre el Establecimiento de Relaciones Diplomáticas (Joint Communiqué on the Establishment of Diplomatic Relations) de 1 de enero de 1979, Estados Unidos transfirió el reconocimiento diplomático de Taipéi a Pekín y reiteró el "Comunicado de Shanghái" que suponía el reconocimiento de la existencia de una única China y admitía que Taiwán formaba parte de ella. Pekín, por su parte, aceptó que Estados Unidos siguiera manteniendo relaciones comerciales, culturales y contactos de carácter no oficial con Taiwán. Estados Unidos siguió manteniendo contactos con Taiwán gracias a la Ley de Relaciones de Taiwán.

Uno de los momentos más controvertidos de la presidencia de Carter fue la negociación y firma de los Tratados del Canal de Panamá, en septiembre de 1977. Estos tratados, que en esencia suponían la transferencia del Canal, de los estadounidenses a la República de Panamá, fueron rechazados por el Partido Republicano, con el argumento de que se estaba transfiriendo un asentamiento estadounidense de gran valor estratégico a un país inestable y corrupto, dirigido por el general Omar Torrijos, que no había sido elegido democráticamente. Aquellos que apoyaban los tratados, por el contrario, defendían que el canal había sido construido dentro del territorio panameño y que por tanto, Estados Unidos mediante su control había ocupado parte de otro país, por lo que el acuerdo tenía por objeto devolver a Panamá la total soberanía sobre su territorio. Tras la firma de los Tratados del Canal en junio de 1978, Carter visitó Panamá con su esposa y doce senadores de Estados Unidos, en medio de disturbios estudiantiles generalizados en contra del gobierno de Torrijos. Más adelante Carter instó al régimen de Torrijos para que frenara su política dictatorial y avanzara gradualmente hacia la democracia en Panamá.

Una de las claves de la política exterior de Carter, que conllevó un laborioso trabajo, fue la firma del Tratado SALT II (Conversaciones De Limitación de Armas Estratégicas»), que redujo el número de armas nucleares producidas o mantenidas tanto por los Estados Unidos como por la Unión Soviética. El trabajo de Gerald Ford y Richard Nixon había conducido al tratado SALT I, que redujo el número de armas nucleares producidos, pero Carter deseaba profundizar más en la reducción de este tipo de armamento. El objetivo principal de Carter, como señaló en su discurso inaugural, consistía en la desaparición completa de las armas nucleares en el mundo.

Con este fin, Carter y el líder de la Unión Soviética, Leonid Brezhnev, llegaron en 1979, al acuerdo del Tratado SALT II. Sin embargo el Congreso negó su ratificación, ya que muchos pensaban que la firma de los tratados debilitaría las defensas de Estados Unidos. Tras la intervención soviética en Afganistán a finales de 1979, Carter retiró el tratado de la consideración del Congreso y nunca se ratificó, aunque fue firmado por Carter y Brezhnev. Aun así, ambas potencias cumplieron los compromisos establecidos en las negociaciones firmadas por ambos dignatarios.

Jimmy Carter se sorprendió mucho de la intervención soviética en Afganistán de diciembre de 1979 y rápidamente tomó medidas, entre ellas armar a los muyahidines. El vicepresidente Walter Mondale manifestó públicamente su desaprobación de la agresiva política que había adoptado la Unión Soviética.

Los soviéticos habían mantenido conversaciones previas con los dirigentes afganos que parecían indicar que no tenían intención de intervenir, sin embargo el Politburó, con muchas dudas, había considerado seriamente la posibilidad de actuar militarmente. Se ha argumentado que la ayuda financiera de Estados Unidos a los disidentes afganos, que incluían islamistas y otros militantes muyahidines afganos, y el deseo soviético de proteger al gobierno afgano de izquierda, fueron los factores que terminaron de convencer a los soviéticos para intervenir.

Por otra parte, buscando desestabilizar la zona, la CIA desde comienzos de la década de 1970 junto con los ingleses apoyaba los militantes muyahidines afganos y en 1975 había participado en un fallido intento de guerra civil, organizado desde Pakistán, que fue un rotundo fracaso; que proporcionó dinero y armas a los insurgentes fundamentalistas a través de Inter-Services Intelligence (ISI) (servicios secretos de Pakistán, en un programa denominado Operación Ciclón.

Estados Unidos comenzó a enviar en secreto ayuda financiera limitada para las facciones islamistas afganas el 3 de julio de 1978. En diciembre de 1979 la URSS derrocó al primer ministro Jafizulá Amín, a quien acusaba de agente de la CIA y que previamente había dado un golpe de Estado contra el gobierno legítimo de Nur Taraki. Los políticos estadounidenses, tanto republicanos como demócratas, decían que los soviéticos se estaban posicionando para dominar el petróleo del Medio Oriente. Otros creían que la Unión Soviética temía que la revolución e islamización de Afganistán se extendiera a la población musulmana de la URSS.

Después del derrocamiento de Amín, Carter anunció lo que se conoce como la Doctrina Carter, consistente en el compromiso norteamericano de usar la fuerza si fuera necesario para acceder a los recursos petrolíferos del Golfo Pérsico. El aumento de tensión entre los bloques causado por está doctrina culminó con el boicoteo de los Juegos Olímpicos de Moscú de 1980, al que la URSS y sus aliados contestarían con su ausencia en los Juegos de Los Ángeles en 1984.

Asimismo se puso fin al tratado de Trigo Ruso, que tenía por objeto establecer un comercio con la URSS y disminuir las tensiones de la Guerra Fría. Las exportaciones de cereales había sido beneficiosas para los agricultores, y el embargo de Carter marcó el inicio de dificultades para los agricultores estadounidenses.

Carter y Brzezinski iniciaron un programa encubierto de entrenamiento de los muyahidines en Pakistán y Afganistán con objeto de frustrar los planes soviéticos. Las políticas diplomáticas de Carter hacia Pakistán cambiaron drásticamente. La administración había cortado la ayuda financiera al país a principios de 1979 cuando los fundamentalistas religiosos, alentados por la corriente de la dictadura militar islámica en Pakistán, quemaron la Embajada de los EE.UU. allí. La participación internacional en Pakistán, aumentó considerablemente con la intervención soviética. Al entonces presidente de Pakistán, el general Muhammad Zia-ul-Haq, se le ofrecieron 400 millones de dólares para subsidiar a los anticomunistas muyahidines en Afganistán. El general Zia declinó la oferta como insuficiente y EE.UU. se vio obligado a aumentar la ayuda a Pakistán.

Reagan posteriormente expandió en gran medida este programa. Los críticos con esta política culparon a Carter y Reagan de la inestabilidad de los gobiernos post-soviéticos de Afganistán, que ocasionó el surgimiento de una teocracia islámica en la región.

El principal conflicto de Estados Unidos en materia de los derechos humanos llegó como consecuencia de las relaciones de Carter con el Sha de Irán. El sha Mohammad Reza Pahlavi, había sido un fuerte aliado de los Estados Unidos desde la Segunda Guerra Mundial y uno de los denominados "pilares gemelos" en los que se basaba la política estratégica de EE.UU. en el Oriente Medio, (el otro era Arabia Saudita). Sin embargo, el Shah había ejercido un gobierno fuertemente autocrático, que fue visto como desafiadoramente cleptocrático en su país. En 1953 organizó, junto con la administración de Eisenhower, un golpe de estado para eliminar al elegido primer ministro, Mohammed Mossadegh.

En una visita de estado a Irán durante 1978, Carter habló públicamente en favor del Shah, denominándolo ""líder de la sabiduría suprema"" y pilar de la estabilidad en el volátil Oriente Medio, en un discurso que nunca fue difundido en la televisión estadounidense. Cuando poco después estalló la Revolución iraní y el Shah fue derrocado, Estados Unidos no intervino directamente y el shah tuvo que marchar hacia un exilio permanente, en enero de 1979. Carter inicialmente le negó la entrada a Estados Unidos, incluso por razones de urgencia médica.

A pesar de su negativa inicial a la entrada del Shah en los Estados Unidos, el 22 de octubre de 1979, Carter le concedió permiso de entrada y asilo temporal durante la duración de su tratamiento contra el cáncer, el Sha volvió a Panamá el 15 de diciembre de 1979. Sin embargo, en noviembre de ese mismo año, como respuesta a la entrada del Shah en EE.UU., militantes iraníes tomaron la embajada estadounidense en Teherán, reteniendo a 52 estadounidenses como rehenes. Los iraníes exigían a cambio de su liberación:


Aunque ese mismo año, el Sha salió de Estados Unidos, moriría en Egipto en 1980. La crisis de los rehenes continuó y dominó el último año de la presidencia de Carter. La subsiguiente respuesta a la crisis - desde la estrategia de "Rose Garden" de permanecer dentro de la Casa Blanca, hasta el intento fallido de rescatar a los rehenes por medios militares (Operación Garra de Águila), fueron en gran medida los responsables en la derrota electoral de Carter en las en las presidenciales de 1980. El 14 de noviembre de 1979, después de la toma de los rehenes, Carter dictó la Orden Ejecutiva 12170, que bloqueaba las propiedades del Gobierno iraní, congelando cuentas bancarias del gobierno iraní en los bancos de Estados Unidos, por un total de 8.000 millones de dólares estadounidenses de ese tiempo. Estos embargos se utilizaron como moneda de cambio para la liberación de los rehenes.

En los días anteriores a que Ronald Reagan asumiera la presidencia de Estados Unidos después de su victoria electoral, el diplomático argelino Abdulkarim Ghuraib había iniciado las negociaciones entre EE.UU. e Irán, que culminaron en los "Acuerdos de Argel" del 19 de enero de 1981, justo un día antes de que finalizase la presidencia de Carter. Los acuerdos implicaban el compromiso de Irán de liberar a los rehenes de inmediato. Además, las Órdenes Ejecutivas 12277 y 12.285 dictadas por Carter, suponían la liberación de todos los bienes pertenecientes al gobierno iraní y de todos los activos pertenecientes al Sha que se encontraban en los Estados Unidos, así como la garantía de que los rehenes no ejercerían ningún tipo de reclamación legal contra el gobierno iraní con motivo del secuestro. Irán, igualmente, accedió a colocar 100.000 millones de dólares de los activos congelados en una cuenta de garantía bloqueada, acordando Irán y Estados Unidos la creación de un tribunal para dirimir las denuncias de ciudadanos norteamericanos por las pérdidas ocasionadas por el Gobiernop iraní. Este tribunal conocido como Tribunal de Reclamaciones Irán-Estados Unidos, otorgaría más de dos mil millones de dólares a reclamantes de EE.UU. y ha sido descrito como uno de los órganos de arbitraje más importantes en la historia del derecho internacional. 

Aunque la liberación de los rehenes fue negociada y garantizada por el gobierno de Carter, los rehenes no fueron liberados hasta el 20 de enero de 1981, momentos después de que Reagan hubiese tomado posesión como Presidente.

Hacia finales de 1979, Carter marchaba muy por detrás en las preferencias de los ciudadanos para las elecciones de 1980, incluso los observadores políticos pensaban que podía ser sustituido por Edward Kennedy en el partido demócrata. Sin embargo la toma de los rehenes de la embajada en Irán y la invasión rusa de Afganistán mejoraron su imagen y lo empujaron lo suficiente para lograr la nominación de su partido. El posterior fracaso del rescate de los rehenes hundió de nuevo su imagen de cara a la reelección.

Carter perdió las elecciones contra el republicano Ronald Reagan. La distribución de votos fue 43,9 millones votos, que representaban el 50,7% del total, para Reagan y 35,5 millones de votos, que representaban el 41%, para Carter. El candidato independiente John B. Anderson obtuvo 5,7 millones de votos, un 6,6% del total. Sin embargo, el hecho de que los apoyos a Carter no se concentraran en una región geográfica concreta, provocó que Reagan obtuviera una victoria arrolladora del 91% de los compromisarios, dejando a Carter con sólo seis estados y el Distrito de Columbia. Reagan consiguió un total de 489 votos electorales frente a 49 de Carter.

La derrota de Carter marcó la primera vez que un presidente electo no lograba obtener un segundo mandato desde Herbert Hoover en 1932. Carter cumpliría su promesa de la liberación con vida de los 52 rehenes de la embajada de Estados Unidos en Irán, pero no logró asegurar su liberación antes de las elecciones. Aunque Carter negoció en última instancia su liberación, Irán no accedió a la misma hasta unos minutos después de que Ronald Reagan asumiese el cargo de presidente. En reconocimiento de la intervención de Carter, Reagan le pidió que fuera a Alemania Occidental para darles la bienvenida por su liberación.

Durante la campaña, Carter fue objeto de burla por el denominado incidente del conejo, un encuentro con un conejo de natación mientras pescaba en el estanque de su granja, el 20 de abril de 1979.

En 1981, Carter regresó al cultivo de cacahuates en Georgia, que había colocado en un fideicomiso ciego durante su presidencia para evitar incluso la apariencia de conflicto de intereses. Encontró que los fideicomisarios habían manejado mal la actividad, dejándolo con más de un millón de dólares en deuda. En los años siguientes, ha llevado una vida activa, con la fundación del Centro Carter, la constitución de su biblioteca presidencial, la enseñanza en la Universidad Emory en Atlanta y con la publicación de varios libros.

Al dejar el cargo, su presidencia fue valorada por la mayoría como un fracaso. En la clasificación histórica de presidentes de Estados Unidos, la presidencia de Carter ha oscilado entre el puesto 19 y el 34. Si bien la presidencia de Carter recibió críticas mixtas de algunos historiadores, su lucha por la paz por encima de todo y sus esfuerzos humanitarios desde que abandonó la presidencia le han llevado a ser ampliamente reconocido como uno de los más exitosos expresidentes en la historia de EE.UU.

Jimmy Carter y su vicepresidente Walter Mondale han sido el equipo más longevo después de la presidencia en la historia estadounidense. El 11 de diciembre de 2006, cumplieron 25 años y 325 días desde que dejaron sus cargos, superando el récord anterior establecido por el presidente John Adams y Thomas Jefferson como vicepresidente, que murieron el 4 de julio de 1826. En agosto de 2012 ha superado a Herbert Hoover como presidente que más ha vivido desde el término de su mandato.

Jimmy Carter es uno de los cuatro presidentes, y el único en la historia moderna de Estados Unidos, que no tuvo la oportunidad de nombrar a un juez para que integrase en la Corte suprema.

"El diario The Independent" publicó:

Carter inició su mandato con un índice de 66% de aprobación, pero esta se redujo a un 34% en el momento de dejar el cargo, con el 55% de desaprobación.

Las encuestas a comienzos de la campaña presidencial de 1976 sugirieron que muchos no le perdonaban a Gerald Ford su relación con Richard Nixon y el escándalo Watergate. Carter, en comparación, parecía un honesto, sincero y bien intencionado sureño.

Esta situación cambió cuando Carter se postuló para la reelección, cuando la autoconfianza de Ronald Reagan contrastó con el temperamento serio e introspectivo de Carter. La atención personal de Carter a los detalles y su aparente indecisión y debilidad se vieron acentuados por el encanto que Reagan despertó entre los votantes. En última instancia, la combinación de los problemas económicos, la crisis de los rehenes de Irán, y la falta de cooperación de Washington le hizo fácil a Reagan presentar a Carter como un líder ineficaz.

Desde que dejó la presidencia, la reputación de Carter ha mejorado mucho. El índice de aprobación presidencial de Carter, que tuvo su cima en un 31% justo antes de las elecciones de 1980, se encontraba a principios de 2009 en el 64%. La pospresidencia de Carter también ha tenido una acogida favorable. Carter explica que una gran parte de este cambio se debió al sucesor de Reagan, George HW Bush, quien lo buscó activamente y fue mucho más amable e interesado en su consejo de lo que había sido Reagan. Carter ha mantenido relaciones de trabajo con los expresidentes George H.W. Bush, Bill Clinton y George W. Bush, y a pesar de sus diferencias políticas, los tres se han convertido en buenos amigos durante años trabajando juntos en varios proyectos humanitarios.

Como presidente, Carter expresó su objetivo de hacer un gobierno que fuera "competente y compasivo." En la búsqueda de esa visión, ha estado implicado en una variedad de políticas públicas nacionales e internacionales, la resolución de conflictos, derechos humanos y causas benéficas.

En 1982, fundó el Centro Carter en Atlanta para promover los derechos humanos y aliviar el sufrimiento humano innecesario. Esta institución no gubernamental, sin ánimo de lucro, promueve la democracia, busca la mediación y la prevención de conflictos y supervisa el proceso electoral en apoyo de elecciones libres y justas. Además, trabaja para mejorar la salud global a través del control y la erradicación de enfermedades como la Dracunculiasis, la oncocercosis, el paludismo, el tracoma, la filariasis linfática y la esquistosomiasis. También lucha para disminuir el estigma de enfermedades mentales y mejorar la nutrición mediante el aumento de la producción de los cultivos en África. Un gran logro de este Centro ha sido la eliminación de más del 99% de los casos de enfermedad del gusano de Guinea, un parásito debilitante que ha existido desde la antigüedad, de 3,5 millones de casos estimados en 1986 se ha pasado a menos de 10.000 en 2007. El Centro Carter ha supervisado 70 elecciones en 28 países desde 1989. Se ha trabajado para resolver los conflictos en Haití, Bosnia, Etiopía, Corea del Norte, Sudán, Venezuela y otros países. El Centro Carter apoya activamente a los defensores de los derechos humanos en todo el mundo y han intervenido con jefes de estado en su nombre.

En 2002, el expresidente Carter recibió el Premio Nobel de la Paz por su trabajo para encontrar soluciones pacíficas a los conflictos internacionales, promover la democracia, los derechos humanos y el desarrollo económico y social a través del Centro Carter. Tres presidentes, Theodore Roosevelt, Woodrow Wilson y Barack Obama, han recibido el premio durante sus presidencias, Carter es único en recibir el premio por sus acciones después de dejar la presidencia. Es, junto con Martin Luther King, Jr., uno de los dos georgianos nativos en recibir el Nobel.

En 1994, Corea del Norte había expulsado a los investigadores del Organismo Internacional de Energía Atómica y amenazaba con empezar a procesar combustible nuclear gastado. En respuesta, el entonces presidente Bill Clinton presionó para que se impusieran sanciones y ordenó el envío de grandes cantidades de tropas y vehículos a la zona para prepararse para la guerra.

Clinton reclutó a Carter en secreto, para llevar a cabo una misión de paz en Corea del Norte, bajo la apariencia de una misión privada de Carter, Clinton vio en Carter una manera de que el presidente de Corea del Norte, Kim Il-sung, diese marcha atrás sin quebrantar su reputación.

Carter negoció un acuerdo con Kim Il-sung, pero fue más allá y diseñó también un tratado, que anunció en la CNN, sin la autorización de la Casa Blanca, como una manera de forzar a los EE.UU. a la acción. La Administración de Clinton firmó una versión posterior del Agreed Framework, en virtud del cual Corea del Norte acordó congelar y finalmente desmantelar su programa nuclear y cumplir con sus obligaciones de no proliferación, a cambio de entregas de petróleo, la construcción de dos reactores de agua ligera para reemplazar sus reactores de grafito y discusiones para el eventual establecimiento de relaciones diplomáticas. El acuerdo fue aclamado en su momento como un logro diplomático importante. Sin embargo, en diciembre de 2002, los Acuerdos de Framework se vinieron abajo como consecuencia de una disputa entre el gobierno de George W. Bush y el gobierno de Corea del Norte de Kim Jong-il. En 2001, el presidente George W. Bush había decidido adoptar una posición de confrontación hacia Corea del Norte y en enero de 2002, incluyó a este país como parte de un "eje del mal". Mientras tanto, Corea del Norte comenzó a desarrollar la capacidad de enriquecer uranio. Bush y otros opositores de la Administración estadounidense a los Acuerdos de Framework creían que el gobierno de Corea del Norte nunca estuvo dispuesto a ceder su programa de armas nucleares, por el contrario los partidarios del acuerdo pensaban que este podría haber sido un éxito y que fue socavado.

Carter y los expertos del Centro Carter asistieron a las negociaciones no oficiales entre israelíes y palestinos para el diseño de un modelo de acuerdo de paz, llamado el Acuerdo de Ginebra, en el período 2002-2003. Carter también se ha convertido en los últimos años en un crítico frecuente de las políticas de Israel en el Líbano, Cisjordania y Gaza.

En abril de 2008, el periódico árabe con sede en Londres, Al-Hayat, informó que Carter se reunió con el líder de Hamás, Khaled Mashaal en su visita a Siria. El Centro Carter inicialmente no confirmó ni negó la historia. El Departamento de Estado de los Estados Unidos considera Hamás como una organización terrorista. En este viaje de Medio Oriente, Carter también colocó una ofrenda floral en la tumba de Yasser Arafat en Ramallah, el 14 de abril de 2008. Carter dijo el 23 de abril que ni Condoleezza Rice ni nadie en el Departamento de Estado le había advertido de que no se reuniera con los líderes de Hamás durante su viaje. Carter conversó con Mashaal respecto a varios asuntos, entre ellos "fórmulas de intercambio de prisioneros para obtener la liberación del cabo Shalit."

En mayo de 2007, mientras que el argumento de que Estados Unidos debe hablar directamente a Irán, Carter afirmó que Israel tenía 150 armas nucleares en su arsenal.

En diciembre de 2008, Carter visitó Damasco nuevamente, donde se reunió con presidente sirio Bashar Assad, y el líder del Hamás. Durante su visita, concedió una entrevista exclusiva a la revista "Adelante", la primera entrevista para cualquier presidente estadounidense, actual o anterior, con medios de comunicación sirios que han tenido éxito.

Carter celebró cumbres en Egipto y Túnez en el período 1995-1996 para abordar la violencia en la región de los Grandes Lagos de África y jugó un papel clave en la negociación del Acuerdo de Nairobi de 1999 entre Sudán y Uganda.

El 18 de julio de 2007, Carter se reunió con Nelson Mandela en Johannesburgo, Sudáfrica, para anunciar su participación en una nueva organización humanitaria llamada Global Elders. En octubre de 2007, Carter visitó Darfur con varios de los miembros de los Global Elders, entre ellos Desmond Tutu. Las seguridades sudanesas le impidieron visitar a un líder tribal de Darfur, lo que provocó una acalorada discusión.

El 18 de junio de 2007, Carter, acompañado de su esposa, llegó a Dublín, Irlanda, para sostener conversaciones con la presidenta Mary McAleese y Bertie Ahern en materia de derechos humanos. El 19 de junio, Carter asistió y habló en el Foro Anual de Derechos Humanos en Croke Park. Un acuerdo entre la Ayuda Irlandesa y el Centro Carter, se firmó también en este día.

En noviembre de 2008, Carter, el ex-secretario general de las Naciones Unidas, Kofi Annan, y Graca Machel, esposa de Nelson Mandela, intentaron entrar en Zimbabue, para inspeccionar la situación de los derechos humanos, siéndole impedido el paso por parte del gobierno del presidente Robert Mugabe.

Carter encabezó una misión a Haití en 1994 con el senador Sam Nunn y el expresidente del Estado Mayor Conjunto de los Estados Unidos, el general Colin Powell, para evitar una invasión multinacional encabezada por Estados Unidos y restaurar el poder al presidente democráticamente electo de este país, Jean-Bertrand Aristide.

Carter visitó Cuba en mayo de 2002 y mantuvo conversaciones con Fidel Castro y el gobierno cubano. Se le permitió dirigirse al público sin censura en la televisión y la radio nacionales cubanas, con un discurso que escribió y presentó en español. En el discurso, pidió a los Estados Unidos poner fin a "un embargo económico ineficaz de 43 años de edad" y a Castro a celebrar elecciones libres, mejorar los derechos humanos, y permitir más libertades civiles. Se reunió con los disidentes políticos, visitó un hospital de SIDA, una escuela de medicina, un centro de biotecnología, una cooperativa de producción agrícola y una escuela para niños discapacitados y realizó un lanzamiento de honor en un partido de béisbol en La Habana. La visita de Carter marcó la primera vista a la isla desde la Revolución cubana de 1959, de un presidente de los Estados Unidos, dentro o fuera de la presidencia.

Carter visitó como observador las elecciones de Venezuela el 15 de agosto de 2004. Los observadores de la Unión Europea se había negado a participar, manifestando que el gobierno de Hugo Chávez les imponía demasiadas restricciones. El Centro Carter declaró que el proceso "sufrió numerosas irregularidades", pero dijo que no observó ni recibió "pruebas de fraude que hubieran cambiado el resultado de la votación". En la tarde del 16 de agosto de 2004, el día después de la votación, Carter y el Secretario General de la Organización de los Estados Americanos (OEA), César Gaviria, dieron una conferencia de prensa conjunta en la que aprobaban los resultados preliminares anunciados por el Consejo Nacional Electoral. Las conclusiones de la monitores ", coincidió con las declaraciones parciales anunciados hoy por el Consejo Nacional Electoral", dijo Carter, mientras que Gaviria agregó que los miembros de la misión de observación electoral de la OEA no había "encontrado ningún elemento de fraude en el proceso." Dirigiendo sus comentarios a las figuras de la oposición que hicieron las denuncias de "fraude generalizado" en la votación, Carter pidió a todos los venezolanos a "aceptar los resultados y trabajar juntos para el futuro". Sin embargo, una encuesta a pie de urnas realizado por Penn, Schoen & Berland Associates (PSB ) había predicho que Chávez perdería por 20%, cuando los resultados electorales mostraron una victoria del 20%, Schoen, comentó: "Creo que fue un fraude masivo".

A raíz de la ruptura de relaciones diplomáticas entre Ecuador y Colombia en marzo de 2008, Carter medió un acuerdo entre los presidentes de ambos países para el restablecimiento de relaciones diplomáticas de bajo nivel, que fue anunciado el 8 de junio de 2008.

En 2001, Carter criticó el controvertido indulto de Marc Rich, aprobado por el presidente Bill Clinton, calificándolo de "vergonzoso" sugiriendo que las contribuciones financieras de Rich al Partido Demócrata fueron un factor en la decisión de Clinton.

Carter también ha criticado la presidencia de George W. Bush y la guerra de Irak. En un editorial de 2003 del "The New York Times", Carter advirtió contra las consecuencias de una guerra en Irak y llamó a la moderación en el uso de la fuerza militar. En marzo de 2004, Carter también acusó a George W. Bush y Tony Blair de librar una guerra innecesaria, basada en mentiras y malas interpretaciones para derrocar a Saddam Hussein. En agosto de 2006, Carter criticó a Blair por ser "servil" a la administración de Bush y acusó a Blair de dar apoyo incondicional a las políticas de Bush en Irak. En mayo de 2007 en una entrevista con Arkansas Democrat-Gazette de Arkansas, manifestó:

El 19 de mayo de 2007, Blair hizo su última visita a Irak antes de dimitir como primer ministro británico y Carter aprovechó la ocasión para criticarlo una vez más. Carter dijo a la BBC que Blair era "aparentemente servil" a Bush y lo criticó por su "apoyo incondicional" de la guerra de Irak. Carter describió las acciones de Blair como "abominables" y afirmó que "el apoyo del primer ministro británico casi sin desviarse de las políticas desacertadas del presidente Bush en Irak habían sido una gran tragedia para el mundo. Carter manifestó que creía que Blair se había distanciado de la administración de Bush durante el período previo a la invasión de Irak en 2003, se pudo haber hecho una diferencia crucial a la opinión pública y política de América, y por lo tanto la invasión no hubiera seguido adelante. Carter expresó su esperanza de que el sucesor de Blair, Gordon Brown, fuese "menos entusiasta" con la política de Bush en Irak.

En junio de 2005, Carter instó el cierre de la prisión de la Bahía de Guantánamo en Cuba, que ha sido un punto focal para las demandas recientes de abusos de prisioneros.

En septiembre de 2006, Carter fue entrevistado por la BBC Newsnight que era un programa que trataba de asuntos de actualidad, expresando su preocupación por la creciente influencia de la Derecha Religiosa en la política de los EE.UU.

El 3 de junio de 2008, cerca del final de las primarias, Carter, en su condición de expresidente, fue nombrado superdelegado en la Convención Nacional Demócrata, anunciando su respaldo al Senador y posterior presidente, Barack Obama.

En 2009 puso peso detrás de las acusaciones de presidente venezolano Hugo Chávez, referentes a la participación de Estados Unidos en el intento golpe de estado en Venezuela de 2002 perpetrado por una Junta cívica-militar, manifestando que Washington conocía la existencia del golpe y pudo haber tomado parte.

Carter ha continuado manifestándose en contra de la pena de muerte en los EE.UU. y en el resto del mundo. Más recientemente, en su carta al gobernador de Nuevo México, Bill Richardson, le instó a firmar un proyecto de ley para eliminar la pena de muerte e instituir la cadena perpetua sin libertad condicional. La ley fue aprobado por la Cámara y el Senado estatal. Carter escribió:

Carter también ha pedido la conmutación de la pena de muerte para muchos presos ya condenados, entre los que se incluyen Brian K. Baldwin (ejecutado en 1999 en Alabama), Kenneth Foster (pena conmutada en Texas en 2007) y Troy Anthony Davis (de Georgia).

En una entrevista de 2008 con Amnistía Internacional, Carter criticó el uso de la tortura en la Bahía de Guantánamo, diciendo que "contraviene los principios básicos sobre los que se fundó esta nación." Dijo que el próximo presidente debería pedir disculpas públicamente en su toma de posesión, y declarar que los Estados Unidos "nunca más torturaría a los prisioneros".

Carter ha sido un autor prolífico en su post-presidencia, escribió entre 21 y 23 libros. Entre ellos se encuentra uno co-escrito con su esposa, Rosalynn, y un libro para niños ilustrado por su hija, Amy. Cubren una gran variedad de temas, incluyendo el trabajo humanitario, el envejecimiento, la religión, los derechos humanos, y la poesía.

Su hija más joven, Amy Carter, que tenía 9 años cuando empezó la presidencia de su padre, fue objeto de constante atención de los medios de comunicación, ya que desde la presidencia de John F. Kennedy, a principios de los sesenta, no habían vivido niños en la Casa Blanca.

Carter y su esposa, Rosalynn Carter, son también muy conocidos por su trabajo como voluntarios con la organización Habitat for Humanity, una institución filantrópica con sede en Georgia que ayuda a trabajadoras de bajos ingresos a construir y comprar sus propias casas. Es profesor de escuela dominical y diácono en la Iglesia Maranatha Bautista en su ciudad natal de Plains (Georgia). En el año 2000, Carter rompió los vínculos con la Convención Bautista del Sur, diciendo que sus doctrinas del grupo no se alineaban con sus creencias cristianas. En abril de 2006, Carter, el expresidente Bill Clinton y el presidente Bill Mercer de la Underwood University iniciaron el Nuevo Pacto Bautista. El movimiento ampliamente inclusiva busca unir a los bautistas de todas las razas, las culturas y las afiliaciones de convenciones. Dieciocho líderes bautistas que representaban a más de 20 millones de bautistas en América del Norte apoyado al grupo como una alternativa a la Convención Bautista del Sur. El grupo celebró su primera reunión en Atlanta, del 30 de enero al 1 de febrero de 2008.

Carter tiene aficiones que incluyen la pintura, la pesca con mosca, trabajar la madera, el ciclismo, el tenis y el esquí.

Los Carter tienen tres hijos, una hija, ocho nietos, tres nietas, y dos bisnietos. Su hijo mayor, Jack, fue el candidato demócrata al Senado de los Estados Unidos en Nevada en 2006, perdiendo ante el que estaba actualmente, John Ensign. El hijo de Jack, Jason, fue elegido como presidente del Senado estatal de Georgia en 2010.

El 12 de agosto de 2015 dio a conocer que había sido diagnosticado de cáncer metastático tras haber sido sometido a una cirugía para extirpar un tumor en su hígado.




</doc>
<doc id="8206" url="https://es.wikipedia.org/wiki?curid=8206" title="Rendimiento">
Rendimiento

El término rendimiento puede referirse a:



</doc>
<doc id="8207" url="https://es.wikipedia.org/wiki?curid=8207" title="Coeficiente">
Coeficiente

Coeficiente hace referencia a varios artículos:

Expresión numérica que mediante alguna fórmula determina las características o propiedades de un cuerpo:





</doc>
<doc id="8208" url="https://es.wikipedia.org/wiki?curid=8208" title="Provincia de Ávila">
Provincia de Ávila

Ávila es una provincia del centro de España perteneciente a la comunidad autónoma de Castilla y León. Su capital es la ciudad de Ávila y está formada por 248 municipios. Limita al norte con la provincia de Valladolid, al sur con Toledo y Cáceres, al este con Segovia y la Comunidad de Madrid, y al oeste con Salamanca. Según el padrón de población del INE en la provincia contaba habitantes (en la nº 47 de las 50 provincias españolas) y con una densidad de población de  hab./km².

El escudo de la Provincia de Ávila reúne los blasones de poblaciones que han sido cabeza de los antiguos partidos judiciales; como Arenas de San Pedro con su castillo incendiado. En su parte central figuran las armas de la ciudad de Ávila, un campo de gules o rojo en el que aparece representado el rey Alfonso VII de León en el ábside de la Catedral de Ávila junto al lema: "Ávila del Rey".

La provincia, que tiene una extensión de 8051,15 km², está situada al sur de la comunidad autónoma de Castilla y León. Limita con las provincias de Madrid, Toledo (Castilla-La Mancha) y Cáceres (Extremadura), aparte de con las provincias castellano y leonesas de Salamanca, Segovia y Valladolid también Ávila tiene 167 015 habitantes y 27,75 por kilómetro cuadrado.

Es propio de esta provincia su gran diversidad orográfica. Es la provincia de mayor altitud en promedio de España, con una altura media de 1131,8 m. Se distinguen tres grandes regiones:


La provincia de Ávila, que es atravesada de suroeste a noreste por el Sistema Central, es divisoria de aguas entre la cuenca hidrográfica del Duero y la del Tajo. Entre las sierras de Gredos, y la alineación de Serrota - Paramera nacen el río Tormes y el río Alberche. 

El río Tormes recoge aguas del alto Gredos, sobre todo la escorrentía del Circo de Gredos, y lleva una dirección este-oeste hasta Barco de Ávila, donde gira hacia el norte, para recibir las aguas del río Corneja, camino de Salamanca para desembocar en el río Duero. 

El río Alberche, nace en la vertiente sur de la Sierra de Villafranca, y recoge aguas de la vertiente sur de La Serrota, encharcando el valle alto con un recorrido meandriforme lleva una dirección oeste-este, antes de girar hacia el sur y encajarse en las cercanías de la Cueva del Maragato y hasta la Venta de Rasquilla, en donde gira de nuevo hacia el este. El tramo alto, hasta la Venta Rasquilla con su codo de captura, pertenecía al río Tormes, que lleva sus aguas al Duero, pero la erosión remontante del Alberche afluente del río Tajo consiguió captar estas aguas de entre sierras hacia el sur. 

El río Tiétar recorre el sur de la provincia, de este a oeste camino del río Tajo del que es afluente, recibiendo las aguas de la vertiente sur (la de solana y a la vez de barlovento) de Gredos, la más lluviosa, por ser el primer murallón con el que se encuentran la borrascas atlánticas que entran por el suroeste de la Península Ibérica. 

Entre La Serrota y la Sierra de Ávila, nace el río Adaja, que tras recorrer el Valle de Amblés de oeste a este, y llegar a Ávila gira hacia el norte camino del río Duero, para ser represado en la Presa de Fuentes Claras y en la de las Cogotas, antes de llegar a Arévalo, donde recibe las aguas del río Arevalillo. 

También van al Duero el río Voltoya, que desemboca en el Eresma —afluente a su vez del Adaja—, del que se abastece Ávila ciudad, mediante el agua embalsada en el embalse de Serones; así como el río Trabancos y el Zapardiel.

Otros ríos de menos importancia son el río Aravalle, el Almar, el Gamo, el Margañán que acaban en el Tormes; y el río Cofio que desemboca en el Alberche.

A destacar dos acuíferos: El Acuífero del valle de Amblés y el de los Arenales.

La orografía de la provincia es la causa de la diversidad en el clima de esta. En las series climáticas referidas al periodo 1960-1996 la estación meteorológica más lluviosa de la provincia fue la correspondiente al municipio de Guisando, en la falda sur de la sierra de Gredos, con un valor de 1931,1 mm. El promedio de precipitación media anual de las 79 estaciones termopluviométricas fue en ese mismo periodo de 728,6 mm, mientras que la temperatura media de estas fue de 11,8 ºC. La provincia se divide principalmente en 4 variedades climáticas atendiendo a la clasificación climática agraria de Papadakis: «mediterráneo subtropical», «mediterráneo templado», «mediterráneo templado fresco» y «mediterráneo continental».

Existen tres dominios principales de precipitación: En la vertiente de la sierra de Gredos las precipitaciones son muy abundantes, sobrepasándose con frecuencia los 1500 mm en promedio anual. Le corresponde una temperatura media anual en torno a los 15 ºC. La zona montañosa central entre las sierras de Gredos y Ávila presenta unos niveles decrecientes en sentido sur-norte, desde el máximo en la sierra de Gredos (los 1500 m mencionados anteriormente) a un mínimo de unos 500 mm. La temperatura media anual —aunque variable en función de la altitud— podría situarse en 9 ºC. Por último la llanura sedimentaria septentrional presenta unas precipitaciones muy escasas —con frecuencia por debajo de los 400 mm anuales- y una importante aridez estival. Por último la temperatura media anual en esta zona es de alrededor de 12 ºC.

Según el "Atlas climático ibérico" de la Agencia Estatal de Meteorología y de acuerdo a la clasificación climática de Köppen, en la provincia se dan las variedades de clima mediterráneo "Csb" (templado con verano seco y templado) y "Csa" (templado con verano seco y caluroso); el segundo principalmente en la parte sur de la provincia. En las cumbres más altas se da un clima mediterráneo de montaña de tipo "Dsb" con temperaturas medias del mes más frío por debajo de los 0 ºC.

La diversidad orográfica antes indicada hace de Ávila una de las provincias interiores españolas más ricas en ecosistemas y por tanto en biodiversidad. Así se pueden distinguir cuatro tipos de ecosistemas principales:

Situada en la parte norte de la provincia y compuesta por grandes planicies de campos de cultivo con bosques isla diseminados, la mayor superficie forestal se concentra en el corredor del río Adaja desde Villanueva de Gómez hasta Arévalo de unos 30 kilómetros de largo con bosque de pinar, en superficie le siguen los pinares próximos a Nava de Arévalo. En las márgenes de los principales ríos se encuentran interesantes bosques de ribera.

En esta zona el principal cultivo es cereal de secano, si bien en los últimos años se ha desarrollado extraordinariamente el cultivo de regadío irrigado desde el embalse de Las Cogotas (desde 2010 la zona de Nava de Arévalo riega con este embalse; y se prevé la puesta en regadío de 7000 ha en la zona)y con pozos subterráneos, hecho que tras la sobreexploatación, ha propiciado, el casi agotamiento del acuífero de Los Arenales y por tanto, ha aumentado el nivel de nitritos y arsénico en algunas muestras. Existen varias especies de aves y mamíferos, algunas de ellas de gran valor como la avutarda y el águila imperial.

Se da principalmente en la parte central y sur de la provincia, en la falda de sus principales sierras. Pese a no ser muy abundantes por la tala abusiva realizada a principios del siglo XX se extienden por grandes zonas alrededor de la capital.

Los encinares existentes en las inmediaciones de Bonilla de la Sierra y en el valle del Corneja son especialmente valiosos por su antigüedad, porte y magnífico estado de conservación.

Los encinares proporcionan cobijo y alimento a gran variedad de fauna, siendo de especial valor ecológico el águila imperial que llegó a ser animal prácticamente extinguido, pero en los años 90 comenzó a recuperarse.

Al contrario que en otras provincias castellano-leonesas, Ávila cuenta en su extremo meridional con una zona templada, de clima mediterráneo, caracterizada por la existencia de bosques de coníferas y frondosas, además de vides, olivares, naranjos, higueras, cerezos y plantaciones de tabaco, pimentón y sandías.

Esta parte de la provincia, al abrigo de la Sierra de Gredos, es la que mayor diversidad biológica posee; pero también la más amenazada por la especulación urbanística, las explotaciones mineras, el trazado indiscriminado de infraestructuras y los incendios forestales.

El ecosistema de alta montaña se puede encontrar principalmente en la Sierra de Gredos, Sierra de Béjar y también en las inmediaciones del Pico Zapatero (Sierra de la Paramera) y en La Serrota. 

Pese a la gran presión humana que sufre, especialmente los fines de semana, la sierra de Gredos conserva una de las poblaciones más importantes de cabra montés. La preocupación de la Corona por este animal evitó su extinción y hoy en día coloca a esta población en unos niveles que permiten afirmar su supervivencia.

Otra especie seriamente amenazada que vuelve a estos lugares a finales de 1990 es el lobo, aunque su población aún no se ha asentado definitivamente.


Antes de la llegada de los romanos el territorio actual de la provincia estaba principalmente habitada por los vetones. El límite nororiental del territorio vetón se ha fijado unos pocos kilómetros al norte de la capital provincial, en Cardeñosa. Los vacceos también ocuparon una parte del territorio actual de la provincia, en la actual comarca de La Moraña. 

La mayor parte de la población se concentraba en la parte central de la provincia. Los pobladores prerromanos —los vetones— crearon en este periodo grandes poblados fortificados emplazados en elevaciones del terreno., como El Raso, Las Cogotas, el Castro de la Mesa de Miranda, el Castro de la Era de los Moros o Ulaca. El castro de mayores dimensiones e importancia debió de ser el Castro de Ulaca. Una estimación sobre la población de este último ha arrojado un dato aproximado de unos 5900 habitantes. Los estudios de los ajuares de las tumbas encontradas apuntan a una sociedad vetona jerarquizada y piramidal, que estaría dominada por una élite militar, y en cuyo escalón más bajo no se descarta que hubiera podido haber quizás algunos esclavos. La base económica de los vetones fue la ganadería —en la que probablemente destacaría el ganado vacuno, con un papel secundario del ganado porcino, caprino y ovino—. Debido a que el territorio no disfrutaba de las mejores condiciones para el aprovechamiento agrícola, la agricultura quedó relegada a un segundo plano en cuanto a importancia; la caza se debió ver beneficiada por una buena calidad cinegética del territorio, mientras que la recolección de bellotas —complementada por las castañas o las nueces— debió de tener una notable importancia en la dieta de los vetones.

Los vetones erigieron un gran número de esculturas de piedra con forma de toros y cerdos, los verracos, en el territorio de la actual provincia. Aunque también se encuentran en zonas que correspondían a otros pueblos prerromanos de la península Ibérica, los hallazgos se corresponden principalmente a zonas vetonas. Sólo en la provincia de Ávila se ha encontrado más 43 % del total de ejemplares documentados. Su función es discutida todavía hoy en día; bien pudieron tener una función relacionada con ritos funerarios, de indicadores de cañadas ganaderas, de delimitación de territorios, de protectores del ganado, o serían estatuas a las que se otorgaba un papel mágico relacionado con la fertilidad.

En los años 192 y 193 a. C. los vetones, en alianza en primera instancia con vacceos y celtíberos, fueron derrotados por tropas romanas comandadas por el pretor Marco Fulvio Nobilior en Toledo, que en el 193 a. C. tomó la ciudad. Se considera probable que las fuerzas vetonas correspondieran a las tribus más orientales, que habitaban el actual territorio de la provincia. No existen fuentes escritas que mencionen a los vetones significativamente entre este suceso y el comienzo de las Guerras Lusitanas (155 a. C). Durante este último conflicto, los vetones se unieron al grueso de las fuerzas de los lusitanos liderados por Púnico. Los académicos opinan mayoritariamente que este apoyo vetón debió mantenerse hasta el final de la guerra. Posteriormente la falta de noticias desde la guerra sertoriana sugiere que el territorio vetón estaba ya pacificado por aquel entonces.

Augusto reorganizó en dos ocasiones las fronteras de las provincias de la Hispania Romana (datadas tentativamente alrededor del 27 a. C. y del 7-2 a. C.). Sin embargo no debieron modificar sustancialmente la adscripción de la actual provincia de Ávila. Las informaciones contradictorias que referirían el territorio de los vetones tanto a la Citerior (la nueva Tarraconense) como a la Lusitania, ha llevado a una mayoría de investigadores a concluir que el territorio vetón estaría dividido entre las dos grandes provincias, aunque el territorio de la actual provincia de Ávila habría pertenecido mayoritariamente a Lusitania exceptuando algunas zonas del norte y del este.

Ya en el siglo I a. C., con la romanización del territorio se produjeron cambios en el tipo de poblamiento, y se favoreció el desarrollo de núcleos en zonas llanas cerca de las redes viarias —sin una gran preocupación por elegir emplazamientos en función de sus posibilidades defensivas— y el abandono de la mayoría de los castros que todavía existían por entonces, especialmente los más alejados de las calzadas. Ya fuera fundación "ex novo" o no, la ciudad de Ávila se convierte en este periodo romano en el único gran núcleo urbano en la zona.

Del periodo tardorromano existe una importante villa romana con su necrópolis adjunta —la llamada Villa romana de El Vergel— en el término municipal de San Pedro del Arroyo, en concreto al noreste de la localidad. Está fechada en torno a los siglos III-IV.


Durante este periodo es destacable en el territorio de la actual provincia —enmarcado esencialmente entonces dentro de la provincia de Lusitania— la existencia de una sede episcopal en la ciudad de Ávila, mencionada en el año 610 en un documento firmado por el obispo Iustiniano. En el plano arqueológico son reseñables los yacimientos de Los Corralillos (en Diego Álvaro) y de Solosancho, en los que se encontraron tumbas, construcciones, objetos de adorno, pizarras, cerámica y restos de arneses. La muestra arqueológica más característica de la época visigoda en la provincia son las pizarras, encontradas principalmente en Diego Álvaro pero también en Arevalillo, Cabezas del Villar, Chamartín y Solosancho. Otras piezas aisladas han sido halladas también en Cardeñosa (una patena), Arevalillo y San Miguel de Serrezuela (restos cerámicos), Adanero (un jarro litúrgico) y Candeleda (monedas).


Tras consolidarse la reconquista en las extremaduras castellanas a partir de la toma de Toledo en 1085, el territorio del Obispado de Ávila se va conformando de tal modo que establece el Territorio histórico de Ávila desde la Edad Media hasta la Edad Moderna.

En el siglo XVI en el territorio de la actual provincia se contabilizaba un total de 17 comunidades de villa y tierra: La Adrada, Arenas, Arévalo, Ávila, El Barco, Bohoyo, Bonilla de la Sierra, Candeleda, Madrigal, El Mirón, Mombeltrán, Las Navas, Piedrahíta, Vahíllo, Villanueva del Campillo, Villafranca de la Sierra y Villatoro.

Con la reordenación administrativa del Conde de Floridablanca de 1785, bajo reinado de Carlos III, Ávila era una de las 31 provincias del reino de España. La provincia estaba a su vez dividida en nueve partidos judiciales: partido de Villatoro (con siete pueblos), partido de Bonilla (con ocho pueblos), partido de Villafranca (con tres pueblos) y los estados de las Navas (con tres pueblos), La Adrada (con siete pueblos), Miranda (con ocho pueblos), Mombeltrán (con doce pueblos) —la mayor parte del valle del Tiétar estaba incluido en la provincia de Toledo por aquel entonces—, el partido o Tierra de Ávila, formado por los sexmos de San Juan, Cobaleda, San Vicente, San Pedro, Serrezuela, Santiago y Santo Tomé; y el partido o Tierra de la Villa de Arévalo, formado por los sexmos de Orvita, La Vega, Sinlabajos, Aldeas y Ragama. Además existían pueblos sueltos o eximidos de sexmo o de Partido.
A comienzos del siglo XIX se llevó a cabo otra serie de reformas administrativas que afectó a la provincia de Ávila, que cedió a la provincia de Toledo los estados de La Adrada, Mombeltrán, Navamorcuende, Miranda y Oropesa. En resumen, en el año 1805 todos los municipios del actual partido judicial de Arenas de San Pedro pertenecían administrativamente a la provincia de Toledo.

En 1820 la provincia de Ávila quedó organizada en ocho partidos: Ávila, Arévalo, Madrigal, Peñaranda, Villafranca, Mombeltrán, Navamorcuende y Oropesa. En 1821 se puso en tela de juicio la continuidad de la provincia, sobre la base de su pequeño tamaño. En la reforma de 1822 algunas localidades pertenecientes al partido de Arévalo pasaron a pertenecer a Segovia y a Valladolid, mientras que algunos pueblos del distrito de Villacastín pasaron por contrapartida a pertenecer a Ávila. En 1833, tras la muerte de Fernando VII, el país vuelve a sufrir una nueva reordenación provincial, llevada a cabo por el ministro de Fomento Javier de Burgos. En dicha división, que fijó la mayor parte de los límites actuales de las provincias en España, la provincia de Ávila queda dividida en seis partidos: Ávila, Barco de Ávila, Arenas de San Pedro, Piedrahíta, Arévalo y Cebreros.

El territorio de la provincia se vio perjudicado por la ocupación francesa durante la Guerra de Independencia —saqueos de Ávila, Arévalo y Arenas de San Pedro—. Durante la Primera Guerra Carlista también fueron frecuentes las partidas de carlistas; una de ellas, por ejemplo, saqueó Candeleda en 1836. La agricultura y la ganadería eran la base de la economía. El sector industrial —artesano— tenía poca importancia en el siglo XIX. Es de reseñar la existencia de dos modelos productivos: en el norte un monocultivo cerealista, capaz de producir excedentes en buenos años pero más expuesto a la miseria y hambruna de la población durante las crisis y en el sur un aprovechamiento más variado del sector agrario-ganadero, menos expuesto a las hambrunas periódicas pero también con menos capacidad para crear excedentes, excepciones hechas del pimentón de Candeleda o de las judías del Barco.

Durante el régimen de la Restauración, la provincia se dividía en cuatro distritos uninominales para las elecciones a Cortes: Arenas de San Pedro, Arévalo, Ávila y Piedrahíta. Los escaños de dos de los cuatro distritos (Arenas de San Pedro y Piedrahíta) fueron principalmente copados por la familia Silvela.
No sería hasta 1965 cuando la provincia de Ávila adoptaría el número de partidos judiciales existentes en la actualidad, cuatro, los partidos de Arévalo, Arenas de San Pedro, Ávila y Piedrahíta. Esta demarcación quedaría fijada definitivamente por la Ley de Demarcación y de planta Judicial el 28 de diciembre de 1988.<ref name="Ley 38/1988, de 28 de diciembre, de Demarcación y de Planta Judicial"></ref>

La unidad administrativa básica en la que se divide la provincia son los municipios. Existen 248 en la actualidad. El municipio con más habitantes es la capital provincial. El resto de municipios no alcanzan la cifra de 10 000 ciudadanos empadronados. Es destacable un elevado número de ellos con poblaciones por debajo de los 500 habitantes. La extensión promedio del municipio en la provincia es de 32,46 km². Aparte de la capital provincial, entre las localidades destacan en cuanto a población Arévalo y Madrigal de las Altas Torres en la parte norte de la provincia (en la comarca tradicionalmente conocida como La Moraña). En el suroeste de la provincia sobresalen El Barco de Ávila y Piedrahíta. En el más poblado sur de la provincia, en la vertiente meridional de la sierra de Gredos los municipios con más habitantes de la comarca del valle del Tiétar son los de Arenas de San Pedro, Candeleda, Sotillo de la Adrada, La Adrada, Piedralaves, Casavieja, Mombeltrán y El Arenal. En la parte este de la provincia, en zonas como el valle del Alberche y la Tierra de Pinares caracterizadas por una mayor cercanía a la capital del estado, Madrid, destacan municipios como Las Navas del Marqués, El Tiemblo, Cebreros, Navaluenga, El Hoyo de Pinares, El Barraco o Burgohondo. De acuerdo al padrón municipal del INE los 20 municipios más poblados de la provincia en 2017 fueron:

La provincia de Ávila es la 17.ª de España en que existe un mayor porcentaje de habitantes concentrados en su capital (34,17 %, frente a 31,96 % del conjunto de España).

En la actualidad sólo existen 2 entidades locales menores (la denominación con la que se conoce en la comunidad autónoma de Castilla y León a las entidades de ámbito territorial inferior al municipio). Se tratan de Balbarda y Blacha, ambas pertenecientes al municipio de La Torre.
Atendiendo a la administración judicial la provincia de Ávila está dividida en cuatro partidos judiciales. Esta organización fue fijada en 1988 con la Ley de Demarcación y de planta judicial. Las cabezas de cada partido son los municipios homónimos.

Dentro de la provincia múltiples municipios mancomunan entre sí obras y servicios como la recogida de basuras, el abastecimiento de agua, la protección civil, o el fomento del turismo entre otros. Nótese que cada municipio puede pertenecer a diferentes comunidades, ya que éstas no comparten los mismas funciones.

Las mancomunidades inscritas en el registro de entidades locales son las siguientes:



La provincia no cuenta con una división comarcal administrativamente funcional, pero la diputación realiza una «comarcalización» del territorio con objeto de la promoción turística.

Según la Diputación Provincial:

En 2015 la Junta de Castilla y León presentó un plan de organización del territorio, que refrendado por 2/3 de los procuradores de las Cortes, dividiría la comunidad autónoma en unidades básicas (UBOST) para la prestación de servicios y organización del territorio, adjuntando un borrador con las diferentes divisiones en cada provincia. El borrador para la provincia de Ávila incluía 20 unidades básicas de ordenación del territorio rurales y una urbana (el municipio de Ávila). Una de las UBOST rurales, Tierra de Arévalo, incluyó en el borrador municipios de la provincia de Segovia, mientras que una UBOST de la provincia de Segovia incluyó el municipio abulense de Maello.


La diputación es la institución a la que corresponde el gobierno y la administración de la provincia de Ávila. La sede de la diputación se encuentra en la ciudad de Ávila, en el edificio del Torreón de los Guzmanes. De acuerdo a los resultados de las elecciones de locales de 2015, la composición del pleno de la diputación —formado por un total de 25 diputados provinciales y establecido mediante elección indirecta a partir de los resultados en las elecciones de los municipios— para la legislatura 2015-2019 es de 14 diputados del Partido Popular, 7 del Partido Socialista Obrero Español, 1 de Izquierda Unida y 1 de Unión Progreso y Democracia, 1 de Ciudadanos-Partido de la Ciudadanía y otro de Trato Ciudadano.

La provincia cuenta con una delegación territorial de la Junta de Castilla y León —el órgano de gobierno de la comunidad autónoma— situada en la ciudad de Ávila. La circunscripción electoral para las elecciones a Cortes de Castilla y León coincide con las provincias, eligiéndose los procuradores de la provincia de Ávila mediante un sistema proporcional de listas cerradas tipo D'Hondt.
Le corresponden por tanto a Ávila 7 procuradores en las Cortes de Castilla y León —el parlamento autonómico—.


La provincia también es circunscripción electoral para las elecciones generales en España. De acuerdo a la ley electoral a la provincia de Ávila le corresponden 3 escaños en el Congreso de los Diputados además de elegir 4 senadores.

De los habitantes empadronados en 2012 en la provincia, aproximadamente un tercio vive en la capital, la ciudad de Ávila. La densidad de población de la provincia, de solamente  hab./km², es muy inferior a la del promedio del estado,  hab./km². 

A partir de las últimas décadas del siglo XVI y hasta la década de 1630 se produjo un gran decrecimiento de la población, pudiendo llegar este a un 40 %. La fase de crecimiento que continuó a este fue débil y no exenta de recesos puntuales. En los albores de la Edad Contemporánea es probable que aún no se hubiera recuperado el máximo poblacional del siglo XVI. Tras la Guerra de Independencia, la población aumentó de manera más significativa que en el periodo anterior.

Los municipios que cuentan con un mayor crecimiento debido a su cercanía con la Comunidad de Madrid son Sotillo, La Adrada, Las Navas del Marqués y Arenas de San Pedro. Otros, como Arévalo, crecen debido a la cercanía a Valladolid, la capital de Castilla y León.

El casco histórico de la ciudad de Ávila, en el que destaca su recinto amurallado medieval y su catedral gótica, figura desde 1985 en la lista de lugares calificados por la UNESCO como Patrimonio de la Humanidad.
En la provincia existían en 2009 un total de 97 ítems catalogados como bienes de interés cultural, por debajo en este aspecto de otras provincias de la comunidad. Entre ellos se encuentran castillos y fortificaciones como las Murallas de Ávila, la Muralla de Madrigal de las Altas Torres, el Castillo-Palacio de Magalia en Las Navas del Marqués, el castillo de la Triste Condesa en Arenas de San Pedro o el castillo de Manqueospese; yacimientos arqueológicos como los castros de Ulaca, el Raso o el de la Mesa de Miranda; y edificios religiosos como la catedral del Salvador en Ávila, o las iglesias de Santa María la Mayor y San Martín en Arévalo. Los cascos históricos de Ávila y de las localidades de Arévalo, Piedrahíta, Guisando y Bonilla de la Sierra reciben también la calificación de bien de interés cultural con la categoría de «conjunto histórico».
Entre los castillos construidos en la Edad Media y comienzos de la Edad Moderna en la provincia se encuentran las siguientes fortificaciones:
En la provincia de Ávila existe un notable número de verracos de piedra. Los verracos son esculturas zoomorfas que suelen representar principalmente toros o cerdos erigidas en un área donde predominaba la cultura vetona —un pueblo prerromano de origen celta de la Edad del Hierro— que comprendía buena parte de la meseta norte y del valle del Tajo en España y también en Portugal. Algunos ejemplos de estas esculturas en Ávila son:


La autopista A-6 de la red estatal —o Autopista del Noroeste—, que comunica la capital del estado, Madrid, con el noroeste de España, tiene un tramo que discurre por el noreste de la provincia en dirección SE-NO. Pasa entre otras localidades por Arévalo o Sanchidrián. Sus tramos de peaje reciben la denominación AP-6. La autovía A-50 es otra vía rápida que comunica la capital de provincia con Salamanca, capital de la provincia homónima.

La provincia cuenta con servicios de tren de viajeros de media y larga distancia operados por Renfe Operadora, que comunican la capital provincial y otras localidades de la provincia, como Arévalo, con ciudades como Madrid y Valladolid.
El 1 de mayo de 2007 entra en vigor en la capital abulense, y en diversos municipios del sur de la provincia, al igual que en otros municipios segovianos, el abono de transportes de la Comunidad de Madrid

Dicho abono está formado por un título concertado con las líneas de autobús y tren que enlazan con la capital madrileña, más el título C2 que permite la movilidad por toda la red de transportes de toda la Comunidad de Madrid.

Entre sus platos más emblemáticos se encuentran las patatas revolconas, la sopa de ajo castellana , las judías del Barco de Ávila guisadas, el chuletón de Ávila, el cochinillo asado (denominado también lechón asado), el cocido, la morcilla de cebolla, los torreznos, los huevos rotos, la ternera abulense, el hornazo, la gallina en pepitoria, las manos de cerdo, la sopa de pan, los huevos al plato, el conejo a la cazadora (asado), la bolla de chicharrones y las famosas Yemas de Ávila también conocidas como Yemas de Santa Teresa.





</doc>
<doc id="8209" url="https://es.wikipedia.org/wiki?curid=8209" title="Casandra">
Casandra

En la mitología griega, Casandra (en griego antiguo, Κασσάνδρα: "la que enreda a los hombres" o "hermana de los hombres") era hija de Hécuba y Príamo, reyes de Troya. Casandra fue sacerdotisa de Apolo, con quien pactó, a cambio de un encuentro carnal, la concesión del don de la profecía. Sin embargo, cuando accedió a los arcanos de la adivinación, Casandra rechazó el amor del dios; este, viéndose traicionado, la maldijo escupiéndole en la boca: seguiría teniendo su don, pero nadie creería jamás en sus pronósticos. Tiempo después, ante su anuncio repetido de la inminente caída de Troya, ningún ciudadano dio crédito a sus vaticinios. Ella, junto con Laocoonte, fueron los únicos que predijeron el engaño en el Caballo de Troya.

Apolo amaba a Casandra pero, cuando ella no le correspondió, él la maldijo: su don se convertiría en una fuente continua de dolor y frustración. En algunas versiones de este mito, Apolo escupe en su boca al maldecirla; en otras versiones griegas este acto suele suponer la pérdida del don recientemente adquirido, pero el caso de Casandra es diferente. En "Orestes" ella promete a Apolo que se convertirá en su consorte, pero no lo cumple, por lo que desata su ira.

Aunque Casandra previó la destrucción de Troya, la muerte de Agamenón y su propia desgracia, fue incapaz de evitar estas tragedias, tal era la maldición de Apolo. Su familia creía que estaba loca y, en algunas versiones, la mantuvieron encerrada en casa o encarcelada, lo que la hace enloquecer. En otras versiones, simplemente era una incomprendida.

Una vez concluida la guerra de Troya, durante el saqueo de la ciudad, Áyax, hijo de Oileo encontró a Casandra refugiada bajo un altar dedicado a Atenea. Aunque la princesa se agarró a la sagrada estatua de la diosa, (bien fuera el Paladio, bien otra estatua distinta), en el frenesí del saqueo Áyax desoyó los ruegos, y la arrastró junto con la estatua. Según algunas fuentes la violó en ese preciso lugar; para otras fuentes, el sacrilegio cometido por Áyax había consistido en no respetar la sagrada estatua de la diosa. Este hecho condenó al guerrero, pues Poseidón, impelido por la humillada Atenea, hundió su barco causando una tormenta en las cercanías del promontorio de las rocas Giras, donde Áyax murió ahogado, o clavado a las rocas por el tridente de Poseidón según otra variante de la leyenda.

Casandra fue entregada como concubina al Rey Agamenón de Micenas. Éste ignoraba que, mientras guerreaba en Troya, su esposa Clitemnestra se había hecho amante de Egisto. Cuando Agamenón y Casandra regresaron a Micenas, Clitemnestra le pidió a su marido que anduviera por encima de una alfombra morada, el color que simboliza a los dioses. A pesar de que Casandra le avisó reiteradamente que no lo hiciera, el Rey la ignoró y cruzó la alfombra, cometiendo así un sacrilegio. Clitemnestra y Egisto asesinaron a ambos. En algunas versiones, Casandra y Agamenón habían tenido gemelos: Telédamo y Pélope. Ambos fueron asesinados también por Egisto.

Télefo, hijo de Heracles, también amaba a Casandra. Sin embargo, ella se burlaba de él y le ayudó a seducir a Laódice, hermana de Casandra. 

Hay versiones alternativas de la historia en las que Casandra, siendo niña, pasó la noche en el templo de Apolo con su hermano gemelo Héleno y las serpientes del templo chuparon y limpiaron sus orejas, por lo que ambos serían capaces a partir de entonces de oír el futuro. Este es un tema recurrente en la mitología griega. Otras versiones sugieren que Casandra consiguió la habilidad de entender el idioma de los animales, en lugar de conocer el futuro.

Casandra aparece en el quinto libro de Geoffrey Chaucer, "Troilo y Crésida" ("Troilus and Criseyde", 1385), como la hermana de Troilo. Éste sueña un día que su amada Crésida está enamorada de un cerdo, y pide consejo a Casandra. Ésta interpreta correctamente el sueño y le dice que Crésida ya no lo ama porque ahora quiere a Diomedes, guerrero griego (uno de cuyos ancestros era famoso por haber matado un gigantesco y feroz jabalí: el Jabalí de Calidón). Debido a la maldición, Troilo no cree a Casandra.

El mito también fue abordado por la escritora alemana Christa Wolf en su obra "Kassandra". El libro cuenta la historia desde el punto de vista de Casandra en el momento de su muerte. 

El grupo sueco ABBA en el tema "Cassandra" habla sobre una persona que se arrepiente de no haberle creído sobre su poder y hace referencia a varios puntos del mito. El tema fue escrito por Benny Andersson y Björn Ulvaeus y aparece en The Visitors (álbum).

La autora Marion Zimmer Bradley escribió una novela de fantasía histórica llamada "La antorcha" ("The Firebrand", 1987) que recuenta la "Ilíada", también desde el punto de vista de Casandra.

En la novela de Markus Sedwick "The Foreshadowing", Alexandra, el personaje principal, tiene el don de ver el futuro, aunque principalmente ve la muerte y el sufrimiento ajeno. Además, al crecer en la Inglaterra de la Primera Guerra Mundial, su poder es temido y puesto en duda. En la novela, ella lee el mito de Casandra y se da cuenta del paralelismo con su propia existencia. 

En "Age of Bronze: Sacrifice", de Eric Shanower, Casandra es violada en su infancia por un malvado sacerdote que pretende ser un dios.

En la novela de Clemence McLearn "Inside the Walls of Troy", Casandra tiene una gran amistad con la reina Helena de Esparta cuando llegó a Troya con el príncipe Paris. Casandra odiaba a Helena con toda su alma, pero se rindió a su alegría y felicidad continua y se convirtió en su confidente. Al final de la historia, Casandra no es violada ni se va con Agamenón. Simplemente se queda con sus hermanas Políxena y Laódice en el templo de Atenea. El resto de la historia no se cuenta.

En la literatura moderna, Casandra es a menudo usada como modelo de tragedia y romance, y a menudo simboliza el arquetipo de alguien cuya visión profética es oscurecida por la locura, convirtiendo sus revelaciones en cuentos o afirmaciones inconexas que no son comprendidas plenamente hasta que ocurre lo vaticinado.

El «síndrome de Casandra» es un concepto ficticio usado para describir a quien cree que puede ver el futuro, pero no puede hacer nada por evitarlo. Por ejemplo, en la película "Doce Monos". la Doctora Kathryn Railly investiga este síndrome y a quienes lo sufren.

En la película de Woody Allen "Poderosa Afrodita", Casandra aparece como uno de los personajes avisando al protagonista de su mal futuro.

El escritor argentino Roberto Mateo, en su novela "La impronta de Casandra", toma la idea original del mito y la modifica dándole otros matices; como ejemplo, Apolo no sólo le dio el poder de predecir el futuro sino que, a pedido de ella misma, recibe el don de la inmortalidad, el don de la palabra justa y el deseo de convertirse en protectora de los artistas de la palabra. Con este giro en el mito original, el autor de esta novela consigue traer a Casandra hasta la época actual, generándole encuentros a través de la historia con escritores que en algún momento de su obra la mencionan; por este paseo histórico Casandra conoce, influye y ampara a Eurípides, Esquilo, Schiller y Rossetti, al igual que al personaje central de esta novela, a quien conoce en última instancia.

El poeta español Ernesto Filardi trata el mito de Casandra en la pieza homónima del poemario "Penúltimo Momento" (Madrid, Sial, 2005). En el poema se establece una identificación de Troya con una relación de amor acabada, mientras Casandra se identifica con la amada que desde tiempo atrás ya anunciaba que la relación no tenía futuro.

El grupo de rock argentino de los 70 Sui Generis le dedica un tema al mito de Casandra titulado "El tuerto y los ciegos". Charly García es su autor y figura por primera vez en el disco "Pequeñas anécdotas sobre las instituciones", aparecido en 1974.

En el año 2007, el cantautor español Ismael Serrano compuso una canción llamada "Casandra" para su disco "Sueños de un hombre despierto". Así mismo, el compositor canario Pedro Guerra compuso una canción con el mismo título.









</doc>
<doc id="8210" url="https://es.wikipedia.org/wiki?curid=8210" title="Las Navas del Marqués">
Las Navas del Marqués

Las Navas del Marqués es un municipio de España perteneciente a la provincia de Ávila, en la comunidad autónoma de Castilla y León. Con habitantes en era el cuarto municipio más poblado de la provincia, tras la capital provincial, Arévalo y Arenas de San Pedro.

El escudo y bandera municipales fueron aprobados oficialmente por decreto el . El blasón en el que se basa el escudo heráldico municipal es el siguiente:
La bandera se define así:
El escudo procede de las antiguas armas de la Casa Dávila, que ostentaba el Marquesado de Las Navas y el Señorío de Villafranca.

El municipio, situado en el extremo oriental de la provincia de Ávila, limita al norte con El Espinar, Villacastín, Navas de San Antonio (Segovia) y Peguerinos (Ávila), al este con Santa María de la Alameda y Valdemaqueda (Madrid), al sur con San Bartolomé de Pinares y al oeste con Navalperal de Pinares (Ávila).

Rodrigo Méndez Silva, en el siglo XVII, escribe en su obra "Población de España":

Si bien la tradición sitúa el nacimiento de Las Navas en la época de Nabucodonosor II, por judíos, no existe ninguna prueba que lo atestigüe y la teoría se considera, en la actualidad, descabellada. Sí existen pruebas de asentamientos humanos durante la Alta Edad Media, probablemente pastores, que utilizaban estas tierras al tratarse de "tierra de nadie", enclavadas entre el norte cristiano y el sur musulmán.
Las navas debe su nombre al marqués Don Pedro de Dávila, que mandó construir el castillo-palacio de Magalia hacia el 1540.

Se puede decir que la historia de Las Navas, la historia del último siglo, se va conociendo poco a poco. Y mal. No hay un estudio serio. Hay estudios parciales. Tenemos la "Alcabala del Viento. Respuestas Generales del Catastro de Ensenada. Nº 50. Las Navas del Marqués", que se cita, más abajo como libro. Así como las repercusiones de la Desamortización de Mendizabal. Y en cuanto al hecho más relevante y trágico como fue la guerra del 36-39 se pasa de puntillas aún. Se debe sin duda a que a perdura en el recuerdo y algunos de los que participaron viven. Se sabe que por aquí estuvo la mítica Columna Mangada, fiel a la República y que se dieron combates encarnizados. El pueblo, como la mayoría de los pueblos, quedó dividido entre los partidarios de una parte y de la otra.

Las tropas nacionales de Rada y Angulo tomaron Las Navas el .

El término municipal de Las Navas del Marqués cuenta con dos edificios de interés histórico catalogados con la figura de protección de Bien de Interés Cultural. Se tratan del Palacio Castillo de Magalia y del Convento de Santo Domingo y San Pablo.

El Castillo-Palacio de Magalia es de estilo renacentista. Pedro Dávila y Zúñiga ordenó su construcción hacia 1540. Fue declarado monumento histórico-artístico nacional (antecedente de la figura de bien de interés cultural) el .

El convento fue fundado en 1545 sobre unos terrenos cedidos a la Orden de predicadores. Los muros son de cantería de granito grande pero irregular, salvo en la fachada con cadenas de sillares, contrafuertes y ángulos. Muestra en planta una sola nave con cinco tramos, crucero ligeramente destacado y cabecera poligonal. La decoración de la cabecera es sobria y de estilo herreriano. Fue declarado por Real Decreto bien de interés cultural el . Fue abandonado tras la desamortización de Mendizábal pasando a manos particulares tras la subasta que tuvo lugar en 1845.


Iglesia de tres naves. En su interior cuenta con un retablo barroco y un órgano del siglo XVII.




Torre construida en 1873 en hierro y madera, atribuida popularmente a Gustave Eiffel, se empleó como atalaya forestal.

La cercanía a Madrid y el clima más suave de Las Navas en verano han atraído a numerosas personalidades de la cultura. Así, en Las Navas solían veranear Aniceto Marinas, Eusebio García Luengo, Manuel Villegas López, José García Nieto, Luís Ponce, Rafael Montesinos, José Posadas, Fernando Quiñones, Víctor Ruiz Iriarte, Eugenio Mediano Flores, Martín Albizanda, Dámaso Santos, Charles David Ley, etc. Aquí descubrió su vocación poética Vicente Aleixandre de la mano de Dámaso Alonso, también veraneante. En Las Navas escribió Camilo José Cela su "Pabellón de Reposo" así como numerosos artículos, algunos sobre el pueblo. En Las Navas escribió Juan Antonio Bardem el guion de su película "Calle Mayor". Y Las Navas ha sido refugio de varios pintores: Evaristo Guerra, Manuel Calvo, Ángel Aragonés. Y tienen su residencia, casi habitual, Agustín García Calvo, Isabel Escudero o Fanny Rubio. Todos ellos han atraído hacia el pueblo a numerosas personalidades de la cultura y han contribuido a enriquecer la vida cultural de la villa. También veraneaba en el municipio la Infanta Eulalia, hermana de Isabel II, aunque esta tenía el chalet en la Ciudad Ducal.

Entre los pintores que registraron su paso por Las Navas del Marqués está el Valenciano José Garnelo y Alda (Enguera 1866-Montilla 1944) que nos dejó para la posteridad una de sus obras más conseguidas la Capea en Las Navas del Marqués. Otro artista singular que dejó huella de su talento en Las Navas fue el escultor Segoviano Don Aniceto Marinas que esculpió los dos cristos de estilo clásico que se veneran en el pueblo el Cristo de Gracia de la Ermita del Santísimo Cristo de Gracia, y el Cristo de la Salud con altar en la Iglesia de San Juan Bautista (Las Navas del Marqués).

Es esta una tradición que viene de antiguo pues ya en el siglo XVI, Lope de Vega, escribió una comedia titulada "El Marqués de Las Navas" cuyo protagonista es Pedro Dávila, de cuyo padre fue secretario el poeta, donde se dice:

También Benito Arias Montano, en el mismo siglo que Lope de Vega, visitó Las Navas del Marqués (concretamente en el verano 1567) donde escribió la obra "Comentario al profeta Oseas", terminando el escrito con un poema en latín donde recuerda a Las Navas:
Benito Arias Montano termina su obra, con estas palabras:

Cuya traducción es: ""Las Navas del Marqués, en el campo abulense, junto al Monasterio de San Lorenzo 'El Escorial', en el mes de agosto de 1567""

No solo de visitantes se nutre la orla de personalidades de Las Navas del Marqués, también hay algunos naveros que han destacado, y destacan, en algún campo: junto a Tomás García Yebra, periodista que ya lleva escritos dos libros sobre la historia de Las Navas, está otro periodista destacado que murió en un accidente de aviación, Manuel de Dompablo y Bernaldo de Quirós . Fue natural de Las Navas el Maestre de Campo y Caballero de Santiago Pedro Esteban Dávila, Gobernador del Río de la Plata desde 1631 a 1637. En la guerra del 36-39, un personaje que no se ha estudiado en absoluto, el guerrillero Florencio Moral 'El Murallas' que tuvo en jaque al ejército rebelde a la República durante un tiempo. Y hoy en día el escultor Javier Herranz Arcones o los artesanos Antonio Segovia y Mariano Rodríguez 'Cacha' que ya tienen en su haber algunas exposiciones de sus obras. Periodistas en ejercicio ha dado Las Navas del Marqués, como Félix Rosado, redactor en Diario de Ávila, Estrella Digital y Madrid Press, y autor de varios relatos, como Álvaro Mateos, redactor jefe de la Cadena COPE en Castilla-La Mancha. También podemos destacar a Sobanski, una figura en ciernes de la fotografía actual. Otro navero famoso en los últimos años es José Ignacio Salmerón, más conocido como Sinacio, conocido cómico y guionista, entre otras facetas.



Las fiestas patronales se celebran en la semana del segundo domingo de julio y están dedicadas al Cristo de Gracia. Durante cinco días todo el pueblo se lanza a la calle y disfruta de las tradicionales actividades, procesiones en honor al Santísimo Cristo de Gracia, Verbenas hasta altas horas de la madrugada, corridas de toros, concurso de potes naveros, el día de las tapas y otros actos con gran participación.

Además, la semana del segundo domingo de agosto, se celebra la conocida como "Semana Grande", en honor del Cristo de la Salud, con romería el día 15 al sitio denominado "El Valladar" y a la ermita del Barrio de la Estación.

La tradición folclórica es compartida con el resto de Castilla y así es popular la jota. Sin embargo, existe como peculiaridad el conocido como "Baile de Tres", que lo baila un hombre con dos mujeres y tiene por música "el Gerineldo". Romance medieval del que se conservan numerosas versiones, entre ellas una versión sefardí, en Tanger. La versión navera comienza así:

Destacan el pote navero que es un guiso a base de patatas y torreznos, las morcillas y los cachuelos. Y en la repostería, empanadas, tortas, mantecadas, bollas de leche con anises y los helados. No nos olvidemos de su excelente carne de ternera, chuletones y solomillos asados, que pueden degustarse en cualquiera de sus restaurantes.

Se pueden destacar muchas rutas, todas ellas en constante contacto con la naturaleza.Entre ellas se pueden destacar:



Al tratarse de un pueblo que recibe muchos visitantes, existen en Las Navas numerosos restaurantes, asadores, mesones, cafeterías y bares que se extienden por las calles principales y alrededores de la Villa. Están especializados en sopa castellana, carnes a la parrilla, cordero y cabrito asado, chuletón de ternera, truchas y guisos de caza.

Las Navas del Marqués cuenta además con una amplia gama de pastelerías, bares de copas, discotecas y hostales.





</doc>
<doc id="8211" url="https://es.wikipedia.org/wiki?curid=8211" title="Sierra de Malagón">
Sierra de Malagón

Sierra de Malagón es una sierra situada geográficamente dentro de la sierra de Guadarrama, perteneciente ésta a su vez al sistema Central de la península ibérica. Al sur y oeste del puerto de Guadarrama se levanta el bloque de Malagón, una morfoestructura de forma cuadrangular que constituye el elemento principal de transición o enlace con el gran conjunto montañoso de Gredos y las Parameras de Ávila. El borde oriental de este complejo relieve presenta gran continuidad con la alineción de La Peñota y se define como un horst que se eleva sobre el piedemonte meridional del Sistema Central siguiendo fallas de dirección NNE. Desde la citada Peñota hacia el sur, cumbres y laderas son graníticas, pero al sur de Cuelgamuros son los gneises del afloramiento metamórfico de El Escorial-Villa del Prado los que forman las sierras de San Juan (1735 m y Abantos 1757 m. En ellos se ha reconocido la falla de Santa María de la Alameda, otro de los grandes cabalgamientos hercínicos. En los flancos de los pliegues hercinianos, marcado en estos gneises se han señalado algunas formas secundarias de relieves, pero estas alineaciones son fundamentalmente horsts disimétricos que se elevan sobre las rampas del piedemonte de El Escorial y enlazan con las superficies culminantes del dorso de Malagón. Al su del El Escorial el pequeño horst de las Machotas constituye un relieve granítico semidomático de topografía muy intrincada. Morfoestructuras similares aparecen al sur, jalonando también el bloque de Malagón: se tratat de una serie de "morros" graníticos y la larga alineación de la "Almenara" (1259 m) que constituye el borde oriental de la fosa de Robledo de Chavela. 
El borde septentrional del bloque de Malagón forma parte de la sierra del mismo nombre, que como la anterior es disimétrica; en este caso, el borde norte del bloque es un escarpe que se eleva sobre las fosas de El Espinar y el Voltoya. El máximo levantamiento se localiza en el sector oriental, donde esta sierra enlaza con el Guadarrama propiamente dicho en Cabeza Líjar (1824 m), culminando en Cueva Valiente (1904 m). La vertiente que cierra por el sur las depresión de El Espinar es la más elevada y escarpada, apareciendo cortada trnsversalmente por fallas NE. Al este la vertiente avanza hacia el norte en el contrafuerte de la Cabeza Renales (1757 m) que sirve de límite entre esta fosa y la del Voltoya de dirección E-O. Esta cabeza sirve, por tanto, de límite a las dos fosas y, al sur de ella, el relieve de Malagón va perdiendo altura. Las citadas fosas quedan cerradas al norte por un conjunto de cerros que las aíslan de las rampas del piedemonte: los del "Caloco" (1567 m) y "Rinconada" (1358 m) cierran la depresión de El Espinar; los de "Peña Morena" y "Atalaya" (1506 m), la del Voltoya. A poniente de esta fosa la pequeña sierra abulense de Ojos Albos (Cruz de Hierro 1660 m), constituida fundamentalmente por cuarcitas y pizarras ordovícicas, cierra la depresión a lo largo de otra fractura NE, la falla de la Paramera de Ávila-Cruz de Hierro.
El llamado "dorso" del bloque de Malagón constituye una especie de plano inclinado hacia el SSO, modelado en sus áreas culminantes por superficies de erosión y articulado internamente por un conjunto de fracturas de tendencia ortogonal. A lo largo de las más activas se han producido desniveles que no han roto la fisonomía general de las superficies de erosión, y que han sido resaltados por los ríos que se encajan en gargantas como las del Cofio y las del río de la Aceña.
Termina al oeste de la falla N-S de El Herradón y, al sur, con la depresión de El Tiemblo-Cebreros- San Martín , cortada por el río Alberche (que la atraviesa para salir a la cuenca del Tajo, prolongándose hasta las Parameras de Ávila, con las que enlaza por la Cuerda de los Polvisos La falda norte de la Sierra de Malagón es suave en unos puntos y quebrada en otros, pasando rápidamente a la gran llanura de Campo Azálvaro, ya en tierras segovianas. Su vertiente sur, por el contrario, tiene más extensión. Sus contrafuertes, alineados de norte a sur, van perdiendo altitud hasta alcanzar prácticamente la margen izquierda del río Alberche. En estas estribaciones se suceden ásperas cumbres, fuertes pendientes y escasas llanuras, con espacios suaves entre collados, cerros y cañadas que toman el nombre de hoyos o navas. Multitud de vallejos y pequeños arroyos surcan el término, aportando sus irregulares caudales al Cofio.


</doc>
<doc id="8215" url="https://es.wikipedia.org/wiki?curid=8215" title="Los autos locos">
Los autos locos

Los autos locos (en inglés Wacky Races) es una serie de dibujos animados de la productora estadounidense Hanna-Barbera Productions sobre un grupo de 11 coches de carrera que compiten entre sí en diferentes carreras, con sus pilotos intentando ganar el título de «Piloto más loco del mundo». La serie era inusual por el gran número de personajes habituales, 23 en total.

Esta serie fue inspirada por la película "La carrera del siglo" (1965). Se emitió originalmente en el canal de televisión estadounidense CBS entre el 14 de septiembre de 1968 y el 5 de septiembre de 1970. Se produjeron diecisiete episodios, cada uno de los cuales incluía dos carreras distintas, haciendo pues un total de 34 carreras.

Entre los corredores estaban el estereotipado villano propio de la serie, Pierre Nodoyuna , y su secuaz, el perro Patán. Nodoyuna lograba una gran ventaja y entonces, como el Coyote en los dibujos animados de El Coyote y el Correcaminos, llevaba a cabo todo tipo de elaborados ardides para hacer que los demás corredores cayeran en trampas, se desviasen, pinchasen o se detuvieran, sólo para ver cómo le salía el tiro por la culata espectacularmente. La lección a aprender podría haber sido que Pierre contaba con uno de los coches probablemente más rápidos de la serie y habría ganado varias carreras si se hubiera concentrado en ellas, en lugar de preparar las trampas. Como Wile E. Coyote, Pierre Nodoyuna jamás ganaba. Muchos de los planes de Pierre son sospechosamente familiares a los usados en episodios del Correcaminos, lo que puede deberse al hecho de que Mike Maltese fuese guionista de ambas series.

Uno de los planes originales para la serie era que las propias carreras serían parte de un concurso de televisión real con Merrill Heatter y Bob Quigley, el equipo responsable de la serie de televisión "Hollywood Squares". El plan de Heatter y Quigley era que los concursantes apostasen a qué auto loco cruzaría primero la línea de meta.

Los once participantes con sus números eran (entre paréntesis se indican los nombres originales):


Anualmente, en el de Inglaterra, se exhiben réplicas bastante bien elaboradas de los autos locos.
El programa mostraba el resultado de las carreras al final de cada episodio, pero nunca hubo un determinado sistema de puntuación general. Si se utilizara el sistema actual del COI, este sería el orden de los resultados:

El personaje de Penélope Glamour protagonizó en 1969 una secuela, "Los peligros de Penélope Glamour". También en 1969, Pierre Nodoyuna y Patán aparecieron en su propia secuela, "El escuadrón diabólico", donde eran aviadores de la Primera Guerra Mundial y trataban de capturar a un palomo mensajero enemigo.

La idea básica en la que se basaban "Los autos locos" fue usada de nuevo por Hanna-Barbera en años sucesivos. A finales de los años 1970, la serie "La carrera espacial de Yogi" estaba protagonizada por personajes de Hanna-Barbera como el oso Yogi, Huckleberry Hound y otros que competían entre ellos por el espacio exterior (y daban esquinazo a un villano y su secuaz canino). A principios de los años 1990, la serie sindicada "Wake, Rattle and Roll" incluía un segmento titulado "Fender Bender 500", que una vez más protagonizaban Pierre Nodoyuna y Patán (y su "Súper Ferrari Especial"), esta vez compitiendo contra el oso Yogi, Winsome Witch y otras estrellas de Hanna-Barbera, en esta serie si se ven ganar algunas de las carreras en las que participan.

Sobre el año 2000, se produjo un videojuego basado en la serie de televisión para PS1 y Dreamcast.



Se produjeron diecisiete episodios, cada uno incluía dos carreras distintas, haciendo un total de 34 carreras.



</doc>
<doc id="8216" url="https://es.wikipedia.org/wiki?curid=8216" title="La carrera del siglo">
La carrera del siglo

The Great Race es una película estadounidense de 1965, del género comedia, dirigida por Blake Edwards, protagonizada por Jack Lemmon, Tony Curtis, Natalie Wood, Peter Falk, Keenan Wynn y Vivian Vance en los papeles principales. El guion fue escrito por Blake Edwards y Arthur A. Ross, con la música de Henry Mancini y la dirección de fotografía de Russell Harlan y estaba basado en la carrera: 1908 New York to Paris Race. Obuvo el (Treg Brown).

A principios del siglo XX, se celebra una loca carrera de coches de carácter internacional. Los participantes salen de Nueva York y la meta está en París. En medio, multitud de disparates y situaciones rocambolescas tienen lugar.
El coche de Tony Curtiss, el "Leslie Special", fue construido inspirándose en el Thomas Flyer, el coche que ganó la carrera de Nueva York a París en 1908. De acuerdo con el Petersen Automotive Museum, cuatro "Leslie Specials" fueron construidos para la película. Uno de ellos está expuesto en el Tupelo Automobile Museum en Tupelo (Misisipi).
Otro fue pintado de verde oscuro para aparecer en la película de 1970 "La balada de Cable Hogue"— es el coche que aparece justo al final y aun conserva las letras "Leslie Special".

El coche del villano se bautizó como "Hannibal Twin-8"; se montaron ocho. Uno está en el Petersen Automobile Museum, otro está en el Volo Auto Museum en Illinois.

Ambos vehículos estuvieron previamente expuestos en el museo Movie World's "Cars of the Stars" en Buena Park, California, hasta que fue cerrado a finales de los 70s.

La escena de la pelea de tartas en la pastelería real fue filmada a lo largo de cinco días. Durante ese tiempo se lanzaron más de 4,000 tartas, es la mayor pelea de tartas de la historia del cine. La escena dura algo más de cuatro minutos y costo US$200,000.

Se usaron tartas decoradas con grosellas, arándanos, fresas y limón.
Edwards les dijo a los actores que, por sí sola, la pelea no tenía gracia, la gracia estaba en crear tensión manteniendo inmaculada la ropa blanca de Leslie hasta el momento adecuado.

El rodaje de la escena se interrumpió el fin de semana y cuando volvieron el lunes, los residuos de tarta apestaban tanto que el plato tuvo que ser desinfectado.

Al principio los actores lo encontraron divertido, pero al final estaban hartos. Wood se atragantó con una tarta y Lemmon dijo que "cuando una de esas tartas te da en toda la cara es como una tonelada de cemento". Al final de la toma, cuando Edwars dijo "corten!", fue acribillado por cientos de tartas que el equipo había reservado especialmente para ese momento.

Toda la escena es un homenaje a Mack Sennett y otros artistas del cine mudo que usaron las peleas de tartas en sus comedias, como Charlie Chaplin; Stan Laurel y Oliver Hardy; y the Three Stooges.

Antes de que la película fuera oficialmente lanzada, la banda sonora fue pre-grabada en Hollywood por RCA Victor Records para lanzamiento en vinilo LP. Henry Mancini estuvo seis semanas componiendo la música, y las sesiones de grabación englobaban 80 músicos. Mancini colaboró con el letrista Johnny Mercer en varias canciones incluyendo "The Sweetheart Tree", un vals lanzado como single. La canción suena repetidamente en la película como tema principal instrumental (La versión a coro suena en el intermedio que en algunas ediciones no está incluido) y es interpretada en pantalla por Natalie Wood con la voz doblada por Jackie Ward (no acreditada).[20] La canción fue nominada al Oscar a la mejor canción original aquel año. Tres temas recurrentes aparecen en la película: El primero o tema principal aparece como balada, con banjo o con un estilo de 1900's. El segundo es "The great Race march" con las tres primeras notas del himno americano. Y el tercer tema, es la melodía del profesor Fate, cómica, suena siempre que Fate conspira con sus inventos para ganar a Leslie. Banda sonora:








</doc>
<doc id="8219" url="https://es.wikipedia.org/wiki?curid=8219" title="Henry Kissinger">
Henry Kissinger

Henry Alfred Kissinger, (en inglés [ˈkɪsɪndʒɚ]), nacido Heinz Alfred Kissinger (Fürth, Alemania, 27 de mayo de 1923), es un político germano-estadounidense de origen judío que tuvo una gran influencia sobre la política internacional, no solo de Estados Unidos con respecto a los demás países sino que también directamente sobre variedad de otras naciones. Ejerció como secretario de Estado durante los mandatos presidenciales de Richard Nixon y Gerald Ford, jugando este papel preponderante en la política exterior de Estados Unidos entre 1969 y 1977 y fue consejero de Seguridad Nacional durante todo el mandato inicial presidencial del primero.

Kissinger se caracterizó por llevar las riendas de un proceder internacional fuerte pero al mismo tiempo negociador, siendo el artífice de la denominada «política de distensión» con la Unión Soviética y China, país con el cual logró, durante el mandato de Richard Nixon, consolidar relaciones pacíficas.

Tuvo que hacerse cargo de poner fin a la muy criticada Guerra de Vietnam y gestionar la crisis de la Guerra de Yom Kippur, concibiendo una nueva visión de como llevar la política exterior estadounidense, al colocar como último recurso la intervención militar, siendo este nuevo proceder el que lo llevó a obtener el Premio Nobel de la Paz en 1973, gracias al alto al fuego que logró establecer en Vietnam.

Aun así, la controversia ha persistido sobre su figura, debido, mayormente, a la intervención de la CIA en varios Golpes de Estado sucedidos en Latinoamérica durante la década de 1970. Sus críticos lo consideran instigador de genocidios sistemáticos de grupos políticos, estando probadamente ligado a varios regímenes dictatoriales latinoamericanos, tales como el Régimen Militar chileno de Augusto Pinochet o el Proceso de Reorganización Nacional de Argentina, así como por ser el responsable de planes represivos como lo sería la Operación Cóndor, cuya célula de origen habría sido la Escuela de las Américas. Todo esto ha ocasionado que existan numerosas iniciativas que persiguen conseguir su procesamiento ante instancias judiciales internacionales, así como la retirada de su Premio Nobel.

En la actualidad, ha pasado a actuar principalmente desde el sector privado, fundó la Kissinger Associates, y es accionista y cofundador de la Kissinger & McLarty Associates, así como miembro de las juntas directivas y asesor de las empresas The Hollinger Group y Gulfstream Aerospace. Además es rector de la Universidad de Georgetown y sirvió en Indonesia como Asesor General de Gobierno.

Igualmente en el 2001, Kissinger fue llamado por el gabinete de George W. Bush para liderar un comité de crisis internacional a causa de los ataques del 11-S así como para que a través de su firma prestase asesoría diplomática y política al gobierno, no obstante Kissinger se retiró poco después de este proyecto.

Henry Kissinger es por mucho una de las figuras políticas y de la diplomacia más relevantes de la Historia de los Estados Unidos, tanto como controvertida. Si bien sus méritos en la política internacional son notables (apertura de relaciones con la URSS, China, entre otros), su negativa a devolver el Premio Nobel de la Paz que recibió gracias al alto al fuego que hubo en la Guerra de Vietnam y que posteriormente se rompió, así como las decenas de acusaciones de colaborar e incluso promover regímenes dictatoriales y acciones terroristas en diferentes partes del mundo, que cometieron severas violaciones a los Derechos Humanos, han ocasionado que su persona haya sido duramente criticada desde numerosas entidades tanto como por personalidades de la política o intelectuales, siendo algunos de los más conocidos el juez español Baltasar Garzón, asesor del Tribunal de la Haya, quien intentó fallidamente procesarlo por violaciones a los Derechos Humanos, y el periodista y escritor Christopher Hitchens, autor del best-seller "Juicio a Kissinger".

Henry Kissinger también ha recibido críticas por ser uno de los miembros fundadores y todavía activo, del polémico Grupo Bilderberg, entidad no gubernamental, en la que se reúnen varias de las personas más poderosas e influyentes de todo el mundo, incluyendo monarcas, aristócratas, políticos, empresarios y magnates.

Heinz Alfred Kissinger nació en Fürth, Baviera, en una familia de judíos alemanes. Su padre, Louis Kissinger (1887-1982), fue un maestro de escuela; su madre, Paula Stern Kissinger (1901-1998), fue un ama de casa. Kissinger tiene un hermano menor, Walter Kissinger. El apellido Kissinger fue adoptado en 1817 por su tatarabuelo Löb Meyer y hace referencia a la ciudad de Bad Kissingen. En 1938, huyendo de la persecución nazi, su familia se mudó a Nueva York. Kissinger pasó sus años de escuela secundaria en la sección de "Washington Heights" del alto Manhattan como parte de la comunidad de inmigrantes judíos alemanes allí. Aunque Kissinger rápidamente se asimiló a la cultura estadounidense, nunca perdió su pronunciado acento alemán, debido a la timidez infantil que le hizo reacio a hablar. Después de su primer año en la "George Washington High School", comenzó a asistir a la escuela por la noche y trabajó en una fábrica de brochas de afeitado durante el día.

Después de la escuela secundaria, Kissinger se inscribió en el City College de Nueva York, para estudiar contabilidad y además estudió Ciencias Políticas en la Universidad de Harvard, sobresaliendo académicamente como estudiante a tiempo parcial, y continuó trabajando mientras estudiaba, desempeñándose como profesor de la propia Harvard, de la cual además recibió la mención honorífica de Summa Cum Laude al graduarse en 1950 y posteriormente obtiene de la misma institución su Maestría y también su PhD en 1952 y 1954 respectivamente.

En 1952, estando todavía estudiando en Harvard, se desempeñó como Asesor de la Junta de Estrategia Psicológica. Elaboró su tesis doctoral acerca de las hazañas como estadistas de Castlereagh y Metternich, titulada "Paz, Legitimidad y Equilibrio".

Permaneció en Harvard, como miembro de la facultad en el Departamento de Gobierno y desempeñándose como profesor y catedrático. Además de 1956 a 1958 trabajó como Director del Proyecto de Estudios Especiales, el cual fue creado por él mismo y avalado por la "Rockefeller Brothers Fundation". Fue Director del programa de estudios de defensa de Harvard entre 1958 y 1971. También fue Director del Seminario Internacional de Harvard entre 1951 y 1971.

Además, como parte de su carrera, pasó a actuar como consultor, asesor y miembro de juntas directivas de variedad de empresas, de las cuales, la más sonada en sus inicios y donde actuó como asesor fue la Corporación RAND, una compañía de producción industrial cuyo cliente más importante era nada menos que el Ejército de los Estados Unidos, estando esta compañía estrechamente ligada con el gobierno y con múltiples programas de desarrollo tecnológico y armamentístico, todas estas cuestiones le valían a la misma el ser acusada de ser una organización militarista y frecuentemente ser involucrada en todo tipo de «teorías conspirativas» y acusaciones de «supuestos planes secretos» para fines bélicos.

Sus estudios se vieron interrumpidos a principios de 1943, cuando fue reclutado por el Ejército de Estados Unidos ante la entrada del país a la Segunda Guerra Mundial.

Kissinger recibió su formación militar básica en el "Campamento Croft" en Spartanburg, Carolina del Sur, donde fue nacionalizado estadounidense a su llegada. El ejército lo envió a estudiar ingeniería en el "Lafayette Collegede" en Pennsylvania, pero el programa fue cancelado y Kissinger fue reasignado a la 84.ª División de infantería. Allí hizo amistad con Fritz Kraemer, un oficial e inmigrante de su natal Alemania, con el que estableció una gran amistad, a pesar de la diferencia de edad, además de ser quien le señaló a Kissinger su gran inteligencia y fluidez con el alemán, talentos que Kraemer le incentivó a usar en su favor y que aprovechó para insertar al joven Henry Kissinger en la sección de "Inteligencia Militar de la División", para la cual desempeñó la misión de encargarse de las tareas de inteligencia de más riesgo durante la Batalla de las Ardenas.

Durante el avance estadounidense sobre Alemania, Kissinger fue de gran utilidad, siendo asignado a la "Bekennende" de la ciudad de "Krefeld", debido a la falta de traductores de alemán en el personal de Inteligencia de la División. Kissinger se basó en su conocimiento de la sociedad alemana para ir destituyendo y sacando a los nazis de los cargos civiles, así como para salvaguardar a las tropas de los espías del gobierno del Tercer Reich y así restaurar una administración civil eficiente en la ciudad, una vez derrotadas las fuerzas nazis, una tarea que se realizó durante 8 días.

Posteriormente, Kissinger, fue reasignado a los "Cuerpos de Inteligencia", ahora con el rango de sargento. Se le colocó a cargo de un equipo en Hannover, el cual había sido asignado para hacer un seguimiento de oficiales de la Gestapo y otros saboteadores, lo que llevó a cabo con éxito, siendo condecorado con la "Estrella de Bronce".

En junio de 1945, Kissinger fue nombrado comandante de un destacamento de la CIC en el distrito de Bergstraße en Hesse, con la responsabilidad de «desnazificar» el distrito, para lo cual se le dio plenitud de poderes en los ámbitos civil y militar. No obstante; aún cuando poseía autoridad absoluta y poderes de arresto, Kissinger tuvo cuidado de evitar abusos contra la población local por parte de su comando.

En 1946, Kissinger fue reasignado a la "Escuela de Inteligencia del Comando Europeo" en el "Campamento de Rey", para ejercer como profesor y adiestrar a los nuevos cuerpos de "Inteligencia Militar", llegando a servir en este papel como empleado civil incluso después de su separación del ejército.

Sus cargos académicos y sus conexiones políticas lo lleva a formar parte del Partido Republicano y a comenzar a ascender en la escena política nacional.

En 1955, se convierte en "Asesor del Consejo Nacional de Seguridad" y de la "Junta de Coordinación de Operaciones de Seguridad". En 1955 y 1956, fue también Director de Estudio en las Armas Nucleares y la política exterior en el "Consejo de Relaciones Exteriores". Publicó su libro de las armas nucleares y la política exterior al año siguiente. De 1956 a 1958 trabajó como director de su "Proyecto de Estudios Especiales" avalado por la "Rockefeller Brothers Foundation". Fue Director del programa de estudios de defensa de Harvard entre 1958 y 1971. También fue Director del seminario internacional de Harvard entre 1951 y 1971. Fuera de la academia, se desempeñó como consultor de varios organismos del Gobierno, incluyendo la "Oficina de Investigación de Operaciones", el Control de armas y desarme y el "Departamento de Estado" y la "Corporación RAND", una compañía de desarrollo industrial, tecnológico y armamentístico.

Deseoso de tener una mayor influencia en la política estadounidense, Kissinger fue partidario y asesor de Nelson Rockefeller, gobernador de Nueva York, que buscó la nominación del Partido Republicano para Presidente en 1960, 1964 y 1968; sin embargo, Kissinger obtendría su tan anhelado ascenso político, con «el candidato más improbable» de ese partido.

Richard Nixon, quien se vio reforzado tras el asesinato tanto de John F Kennedy como de Robert Kennedy, y por el desastre en el que estaba resultando la Guerra de Vietnam, condujo una campaña fuerte y exitosa, ganando la Presidencia en 1968.

Richard Nixon, convencido de las habilidades de Henry Kissinger, lo nombra Consejero de Seguridad Nacional, dejándole así la puerta abierta para comenzar su carrera en la alta política y poniéndolo al frente de todo lo referente a orden interno, seguridad y demás cuestiones referentes al ámbito.

Kissinger, pasará a desempeñar a plenitud su cargo, y se ocupará de organizar todo lo referente a la defensa y se convertirá en el asesor más cercano del presidente Nixon, así como en uno de sus más cercanos aliados y también en el único dentro del gabinete y en general de toda la Administración de Richard Nixon, que era capaz de enfrentarse de frente al presidente, aún cuando eran bien sabidos los desencuentros entre ambos.

Pero Kissinger no sólo se limitó a cumplir sus funciones, sino que pasó a involucrarse en prácticamente todas las actividades del gobierno de Richard Nixon, algo que lo convirtió en indispensable para el propio presidente, quien frecuentemente hacía cambios completos de gabinetes y de directivas pero era incapaz de destituir a Kissinger, dado que aún cuando su cargo y su sueldo no le remuneraran ni le otorgaran responsabilidades sobre decenas de ámbitos, Kissinger se involucraba de lleno en cada asunto de la política estadounidense.

Robert Greene explica en su libro las 48 Leyes del Poder, esta estrategia que Kissinger empleó para prevalecer de la siguiente manera:

Kissinger, por otro lado tuvo que hacer frente a las protestas, manifestaciones y demás muestras de rebeldías por parte de las comunidades pacifista y de los sectores estudiantiles, obsesionados con el fin de la Guerra de Vietnam, así como también se encargó de tomar cartas sobre este asunto aconsejando a Nixon al respecto. Igualmente, como Asesor de Seguridad Nacional, en 1974, Kissinger dirigió el muy debatido 200° Memorando de Estudios de Seguridad Nacional

Posteriormente, Nixon se lanzaría en un intento de ser reelecto, obteniendo la nominación del Partido Republicano, con bastante ventaja y holgura, pasando a ganar las elecciones de 1972. Para el momento, Kissinger era uno de los políticos y personalidades más poderosas del gobierno y dentro del Partido Repúlbicano. Su apoyo a la candidatura de Nixon fue crucial; con ella, la mayoría de los miembros del partido respaldaron al presidente Richard Nixon y gracias a las políticas aplicadas y sugeridas por Kissinger, gozaba de una considerable popularidad y aceptación, todos elementos que favorecieron la aspiración de reelección de Nixon.

Así, Richard Nixon, una vez reelecto, en 1972, premió a Kissinger con la Secretaría de Estado de los Estados Unidos, puesto con el que Kissinger acabaría de grabar su nombre en la historia.

Kissinger había sido Asesor de Seguridad Nacional durante prácticamente todo el primer gobierno del presidente Richard Nixon, y continuó ejerciendo dicha posición hasta el final del mismo, tras lo cual su posición oficial sería únicamente la de Secretario de Estado, ejerciendo así este cargo desde de 1973 hasta 1977. Kissinger ya había tenido una implicación amplia en los asuntos internacionales desde su anterior puesto, pero ahora con la secretaría encargada del ámbito, pasaría a «gobernar la política internacional» al máximo.

Apenas asumió su nueva posición, Kissinger se dispuso a cambiar el enfoque que hasta ahora se le había venido dando a la política internacional, asumiendo un papel, no de consejo y obediencia como normalmente se entendería, sino dominante, fuerte y hasta cierto punto autosuficiente, respecto a la política exterior de Estados Unidos entre 1969 y 1977.

En ese período, extendió la política de distensión. Esta política llevó a una relajación significativa en las tensiones con la Unión Soviética y desempeñó un papel crucial en 1971 las conversaciones con el primer ministro chino Zhou Enlai. Las conversaciones concluyeron con un acercamiento entre Estados Unidos y la República Popular de China y la formación de un nuevo alineamiento estratégico chino-estadounidense antisoviético. Fue condecorado con el Premio Nobel de la Paz de 1973 por ayudar a establecer una cesación del fuego y el retiro de los Estados Unidos de Vietnam. El alto el fuego, sin embargo, no fue duradero. Mientras que el otro premiado, el vietnamita Le Duc Tho, devolvió su premio por considerar que no lo merecía por haber retornado la guerra, Kissinger no lo devolvió.

Kissinger favoreció el mantenimiento de relaciones diplomáticas amistosas con las dictaduras militares de derechas en el Cono Sur y otras partes de Latinoamérica y más aún, está acusado de planificar el asesinato una gran cantidad de militantes de izquierda en Chile, y posteriormente, en la Argentina y el Uruguay, así como de promover y respaldar a las propias dictaduras y desarrollar la "Academia de las Américas", y la Operación Cóndor.

Ya como Asesor de Seguridad Nacional bajo el primer gobierno Nixon, Kissinger fue pionero en la política de distensión con la Unión Soviética, que buscaba una reducción de las tensiones entre las dos superpotencias. Como parte de esta estrategia, llevó a cabo las conversaciones de limitación de armas estratégicas (que culminaron en el "Tratado SALT I" - Strategic Arms Limitation Talks) y el Tratado sobre misiles anti-balísticos con Leonid Brezhnev, Secretario General del Partido Comunista soviético. Las negociaciones sobre el desarme estratégico originalmente debían iniciar bajo la administración de Lyndon Baynes Johnson, pero fueron pospuestas en protesta por la invasión de Checoslovaquia, realizada por las tropas del Pacto de Varsovia en agosto de 1968.

Kissinger, también negoció el acercamiento con la República Popular de China, al ver de antemano una gran oportunidad de futura inversión para Estados Unidos; además también determinó que su acercamiento con China, rival de la URSS, sería un muy eficiente mecanismo de presión para la última, que de acuerdo a lo que Kissinger planteaba, reaccionaría tratando de acercarse aún más a Estados Unidos.
Así, Kissinger, intentó colocar presión diplomática en la Unión Soviética. Hizo dos viajes a la República Popular de China en julio y octubre de 1971 (el primero de los cuales se hizo en secreto) para conferir con el Ministro Zhou Enlai, entonces a cargo de la política exterior China. Esto allanó el camino para la innovadora Cumbre de 1972 entre Nixon, Zhou y el Presidente del Partido Comunista de China, Mao Zedong, así como para la formalización de las relaciones entre los dos países, tras 23 años de aislamiento diplomático y hostilidad mutua.

El resultado fue la formación de una alianza tácita, antisoviética, entre China y los Estados Unidos, cuyo fin era lograr crear aún más presión sobre la URSS y comenzar a ganarle terreno, quitándole el protagonismo internacional. Mientras que la diplomacia de Kissinger llevó a cabo intercambios económicos y culturales entre las dos partes y el establecimiento de oficinas de enlace en las capitales china y estadounidense, todo ello con varias dificultades en el proceso, pues la plena normalización de las relaciones con la República Popular de China no se produciría hasta 1979, en primera instancia debido al escándalo Watergate, que eclipsó el gobierno de Richard Nixon y condujo a su renuncia y en segunda porque los Estados Unidos y la ONU continuaron reconociendo al Gobierno de Taiwán, como «el verdadero gobierno chino», siendo este país el que conservó la silla de China en el Consejo de Seguridad hasta el año de 1979.

La participación de Kissinger en Indochina comenzó desde antes de su nombramiento, incluso como Consejero de Seguridad Nacional de Nixon. En Harvard, trabajó como asesor en política exterior en la Casa Blanca y el departamento de Estado. Kissinger dice que «en agosto de 1965 [...] [Henry Cabot Lodge], un viejo amigo que actuaba como embajador en Saigón, le había pedido que visita Vietnam como su asesor. Kissinger accedió a esta proposición al realizar una gira por Vietnam en primer lugar por dos semanas en octubre y noviembre de 1965, una vez más para unos diez días en julio de 1966 y una tercera vez por unos días en octubre de 1966 [...] Lodge me dio carta blanca para examinar cualquier asunto de mi elección. Me convenció del sinsentido de victorias militares en Vietnam y me hizo caer en cuenta acerca de la realidad política que yacía en nuestro país, en la cual fácilmente, se podría llevar a cabo nuestra retirada final».

En una iniciativa de paz de 1967, él mismo medió entre Washington y Hanói, lo que representó el inicio de las conversaciones entre ambos bandos.

Richard Nixon había sido elegido en 1968 con la promesa de alcanzar la «paz con honor» y poner fin a la Guerra de Vietnam. En la Oficina, y asistida por Kissinger, Nixon implementó una política de vietnamización encaminada a retirar gradualmente las tropas durante la expansión de la función de combate del Ejército de Vietnam del Sur, propiciando defender de forma independiente su régimen contra el Frente nacional para la liberación de Vietnam del Sur, una organización guerrillera comunista y el ejército norvietnamita (Ejército Popular de Vietnam). Kissinger desempeñó un papel clave en una campaña secreta en Camboya, donde se procedería a realizar un bombardeo masivo, para interrumpir las operaciones de unidades del Ejército Popular de Vietnam y Viet Cong, así como para desmantelarlas. Planificó y puso en marcha el lanzamiento de incursiones en Vietnam del Sur desde dentro de las fronteras de Camboya y reabastecer sus fuerzas mediante la Ruta Ho Chi Minh y otras rutas, así como la incursión de Camboya de 1970 y posterior bombardeo generalizado de Camboya.

La campaña de bombardeo contribuyó al caos de la Guerra Civil de Camboya, que incapacitó a las fuerzas del dictador lon Nol para retener extranjeros. Además apoyó la insurgencia de los Jemeres rojos, que buscaron derrocar a dicho dictador en 1975. Documentos descubiertos en los archivos soviéticos después de 1991 revelan que la invasión del Norte Vietnamita de Camboya en 1970, fue lanzada a petición explícita de los Jemeres Rojos y negociada por Pol Pot junto con su segundo al mando, Nuon Chea. El bombardeo estadounidense de Camboya causó la muerte de unos 40.000 combatientes y civiles.

El biógrafo de Pol Pot y reconocido historiador, especializado en la Historia de Camboya, David P. Chandler, afirma:

Por otro lado, el escritor y periodista británico, además de un reconocido crítico del Secretario Kissinger, Christopher Hitchens, asegura en su libro "Juicio a Kissinger":

Junto con miembro del Politburó Vietnam del Norte, Le Duc Tho, Kissinger fue galardonado con el Premio Nobel de la Paz el 10 de diciembre de 1973, por su trabajo en la negociación de la cesación del fuego contenidas en los Acuerdos de paz de París, donde se establecían las intenciones de «poner fin a la guerra y restablecer la paz en Vietnam», habiendo sido estos firmados en enero. Tho rechazó el premio, alegando que la paz no había sido, realmente restaurada en Vietnam del Sur. Kissinger, por su lado escribió al "Comité del Nobel" que aceptaba el premio «con humildad». El conflicto continuó hasta una invasión del sur por el ejército de Vietnam del Norte, que resultó en una victoria norvietnamita en 1975 y la evolución posterior del pathet Lao en Laos hacia la condición de mascarón de proa.

"Artículo principal: Guerra indo-pakistaní de 1971"
"Véase también: Guerra Civil Camboyana"

Bajo la dirección de Kissinger, el Gobierno de Estados Unidos apoyó a Pakistán en la Guerra de liberación de Bangladesh en 1971. Kissinger estaba particularmente preocupado acerca de la expansión Soviética en el Asia meridional como resultado del Tratado de Amistad, recientemente firmado por la India y la URSS, por lo cual trató de demostrar a la República Popular de China (aliado de Pakistán y enemigo tanto de la India como de la Unión Soviética) el valor de una alianza táctica con los Estados Unidos, planeando y en efecto logrando usar a China, como enclave de la influencia estadounidense sobre la región, estrategia que resultó más que eficiente.

La Guerra indo-pakistaní de 1971, es también uno de los flancos de donde Kissinger recibe mayor cantidad de críticas, debido no a la guerra, ni a sus acciones "per se", sino a la actitud que tomó al respecto, criticando fuertemente tanto en público como en privado a la primera ministra hindú Indira Gandhi y según fuentes internas, Kissinger habría llegado a insultarla, incluso en reuniones privadas con Richard Nixon, además de tener una actitud despreciativa hacia la población india en general, tratándolos como una «raza inferior».

Al respecto de la crítica situación que se daba en el Oriente Medio con el estado de Israel y también con el problema de los judíos retenidos en la URSS, donde eran llevados a campos de concentración soviéticos, Richard Nixon, se sentía particularmente preocupado por la posición del único hombre que no podía despedir de su gobierno y que de paso, era el encargado de la política exterior estadounidense, siendo Kissinger judío. Ante esto, Nixon, realmente creía que su Secretario de Estado, asumiría una posición más que activa con respecto esto, favoreciendo a la comunidad judía internacionalmente y al propio Israel, lo cual podría poner en riesgo la delicada posición de Estados Unidos al respecto.

Richard Nixon llegó incluso a considerar el sacar de este asunto a Kissinger. De hecho, según notas tomadas por H.R. Haldeman, Nixon habría ordenado lo siguiente:

A pesar de todo lo que Nixon había considerado y previsto, Kissinger resultó estar más que desinteresado del asunto de Israel y de los judíos, de hecho, los consideraba un estorbo para el proceder político internacional de Estados Unidos.

Llegando aún más lejos, en 1973, Kissinger no sentía que lo que estaba ocurriendo en la Unión Soviética con respecto a los judíos, donde eran perseguidos, discriminados, y no podían ocupar cargos de importancia, fuese asunto de interés para la política exterior de los Estados Unidos. A tal punto llegó esto, que en una conversación con Nixon, poco después de una reunión con Golda Meir, el 1 de marzo de 1973, Kissinger dijo:

Además, Kissinger sentía que el gobierno de Israel no era más que una molestia, un grupo de enfermos que constantemente pedían ayuda al gobierno estadounidense, criticándolos por no ser más efectivos.

En una conversación con el entonces Secretario de Defensa, Robert McNamara, dijo sobre los dirigentes israelíes:

El 6 de octubre de 1973, coincidiendo con la festividad hebrea de Yom Kipur, de donde recibió su nombre la guerra, Egipto y Siria, atacaron conjuntamente y por sorpresa a Israel, traspasando la línea establecida por el armisticio del Sinaí y de los Altos del Golán, todos estos territorios que habían sido conquistados por Israel durante la Guerra de los Seis Días en 1967. Los líderes de Egipto y Siria, Anwar el-Sadat y Hafez al-Asad respectivamente, estaban siendo respaldados y aconsejados por la URSS y su presidente Leonid Brézhnev.

Estados Unidos estaba estrechamente vinculado con Israel, pero su "política de distensión" con la Unión Soviética no le permitía actuar directamente en favor de dicho país, además la situación económica que se presentaría, de entrar a la guerra, sería difícil, así que Kissinger optó por la paz. Trató de negociar el fin de la Guerra del Yom Kippur, tras los sucesos ocurridos.

No obstante, Nixon decidió ir en contra de la oposición inicial de Kissinger. Ordenó que el ejército de Estados Unidos llevase a cabo la mayor operación militar aérea de la historia para ayudar a Israel el 12 de octubre de 1973. Esta acción estadounidense fue el desencadenante de la Crisis del petróleo de 1973, que afectó a los Estados Unidos y sus aliados de Europa Occidental.

Comienza, entonces, el 17 de octubre de 1973, la Crisis Petrolera de 1973, a raíz de la decisión de la Organización de Países Árabes Exportadores de Petróleo (que agrupaba a los países miembros árabes de la OPEP más Egipto y Siria), de no exportar más petróleo a los países que apoyasen a Israel durante la Guerra del Yom Kippur, medida que incluía a Estados Unidos y a sus aliados de Europa Occidental.

Al mismo tiempo, los miembros de la Organización de Países Árabes Exportadores de Petróleo acordaron utilizar su influencia sobre el mecanismo que fijaba el precio mundial del petróleo para cuadruplicar su precio, después de que fracasaran las tentativas previas de negociar con las "Siete Hermanas".

El aumento del precio unido a la gran dependencia que tenía el mundo industrializado del petróleo de la OPEP, provocó un fuerte efecto inflacionista y una reducción de la actividad económica de los países afectados. Estos países respondieron con una serie de medidas permanentes para frenar su dependencia exterior, siendo Kissinger clave para esta operación, pero la prioridad número uno del Secretario de Estado, así como de todo el gobierno estadounidense no era otra que poner fin a la Guerra del Yom Kipur, para luego tratar de levantar el bloqueo económico impuesto por parte de la Organización de Países Árabes Exportadores de Petróleo.
Entre tanto, ahora con apoyo militar y logístico estadounidense, Israel avanzaba con una serie de excepcionales victorias militares que lo llevaban a recuperar los territorio perdidos al inicio de la guerra y ganó nuevos territorios en Siria y Egipto, incluidas las tierras al este, de los previamente capturados, del Golán aunque perdieron parte del territorio en el lado oriental del Canal de Suez que había estado en manos israelíes desde el fin de la Guerra de seis días. Kissinger presionó a los israelíes, para que cedieran parte de los territorios recién capturados de nuevo a sus vecinos árabes, contribuyendo a las primeras fases de no-agresión entre Israel y Egipto. Esto condujo a una reanudación de las relaciones entre Estados Unidos y Egipto, amargas desde la década de 1950, dando como resultado que el país pasase de su anterior postura independiente a mantener una estrecha asociación con los Estados Unidos.

Finalmente, el 11 de febrero de 1973, Kissinger presenta su "Plan para el Desarrollo de Energías Alternativas", último paso con el cual le asesta un golpe fatal al bloqueo petrolífero. Ahora con el panorama internacional a favor, el Secretario de Estado estadounidense, se anota su gran éxito, cuando los Ministros de Energía árabes, a excepción de Libia, anuncian el fin del embargo contra Estados Unidos.

El 31 de octubre de 1973, el Ministro de Relaciones Exteriores egipcio Ismail Fahmi se reúne con Richard Nixon y Henry Kissinger, luego de lo cual, tras una semana, se pone fin a los combates en la Guerra de Yom Kipur.

EL proceso de paz en el Medio Oriente finalizaría cinco años después, en 1978, cuando el Presidente estadounidense Jimmy Carter mediará los Acuerdos de Camp David, durante los cuales Israel devolvería la península del Sinaí a cambio de un acuerdo egipcio para reconocer el estado de Israel.

"Consulte también: Régimen Militar (Chile)"
"Consulte también: Proceso de Reorganización Nacional"
"Consulte también: Dictadura cívico-militar en Uruguay (1973-1985)"
"Consulte también: Operación Cóndor"

Bajo "el mando diplomático" de Kissinger Estados Unidos continuó reconociendo y manteniendo relaciones con gobiernos de derecha, ya fuesen dictatoriales o democráticos, respaldando y estableciendo tratados y alianzas estratégicas con cada uno de ellos, con el fin último de asegurar el predominio de las tendencias capitalistas y neoliberalistas, por sobre las izquierdistas, comunistas y socialistas en la región. Pero Kissinger llegaría más lejos que cualquiera de sus predecesores en esta región.

Kissinger respaldó y fue clave para el ascenso de tres importantes dictaduras en el Cono Sur, el Régimen militar de Augusto Pinochet en Chile y el Proceso de Reorganización Nacional en Argentina y la Dictadura cívico-militar en Uruguay (1973-1985). Además desarrolló y aplicó la Operación Cóndor, un plan ofensivo en contra de las organizaciones populares en Latinoamérica.

Tras la Revolución Cubana, liderada por Fidel Castro, que se había consolidado en el poder desde 1960, Estados Unidos había venido teniendo todo tipo de confrontaciones diplomáticas con Cuba. El país, ahora orientado hacia el comunismo y el socialismo, se convirtió en un problema para EE.UU., que aspiraba mantener a la región latinoamericana limpia de cualquier tendencia de izquierda.

Bajo tales aspiraciones, las relaciones entre Cuba y Estados Unidos fueron interrumpidas y el primero trató de consolidar todo tipo de problemas para la Isla Rebelde, logrando aislarla internacionalmente, además de aplicar embargos económicos, e incluso durante el gobierno de Kennedy, se llevó a cabo la Invasión de la Bahía de Cochinos, un ataque al territorio cubano.

No obstante, Kissinger inicialmente consideraba que era momento de mejorar las relaciones entre Cuba y Estados Unidos, rotas desde 1961 (todo el comercio de Exxon Mobil cubano había sido bloqueado en febrero de 1962, unas semanas después de la exclusión de Cuba de la Organización de los Estados Americanos, debido a la presión estadounidense). Sin embargo, rápidamente cambió de opinión y siguió la política de John F. Kennedy. Después de la participación de las fuerzas armadas revolucionarias de Cuba en las luchas de liberación en Angola y Mozambique, Kissinger determinó que a menos que Cuba retirase sus fuerzas no podrían normalizarse las relaciones entre ambos países, a lo cual Cuba se negó.

La Alianza para el progreso de John F. Kennedy finalizó en 1973. En consecuencia, era necesario empezar a determinar cuales serían las acciones a tomar con respecto a un valioso activo comercial internacional, el Canal de Panamá. Kissinger, se encargó de que en 1974 comenzaran las negociaciones sobre un nuevo asentamiento en el Canal de Panamá. Si bien el Secretario de Estado estadounidense, no logró llevar a cabo la firma de un nuevo tratado, sí fue el responsable de prepararla, siendo a escasos meses de haber asumido la presidencia, que Jimmy Carter llevó a cabo los Tratados Torrijos-Carter y la entrega del canal al control panameño.

El candidato presidencial del Partido Socialista de Chile, Salvador Allende, había sido elegido democráticamente en 1970 para el puesto de presidente, con un 36,3 %, una mayoría relativa.

Automáticamente, en Estados Unidos, la reacción no se hizo esperar, la preocupación general era incontrolable en Washington, D.C. debido al nuevo presidente chileno, abiertamente socialista y con inclinaciones a favor de Fidel Castro y la Revolución Cubana. Allende intentaría llevar a cabo pacíficamente su programa de gobierno de profundas reformas para la época en lo político, económico y cultural.

Kissinger fue, particularmente, el mayor defensor de la necesidad de intervenir en Chile, considerando su gobierno como «comunista», y una amenaza de germen peligroso para el orden en la región, llegando a afirmar lo siguiente:

Kissinger era presidente del poderoso Comité 40, una organización de alto nivel y al lado de representantes del Departamento del Estado, de la CIA y del Pentágono, tomaban decisiones y medidas acerca de las situaciones relacionadas con el comunismo nacional e internacionalmente. La administración de Nixon autorizó la Agencia Central de Inteligencia (CIA) para fomentar un golpe de Estado militar que impidiese la toma de posesión de Allende, pero el plan no tuvo éxito. El grado de participación de Kissinger en estos planes es mayor al que se creía, existiendo documentos y archivos desclasificados, que demuestran que Kissinger no sólo era consciente de las operaciones de EE.UU. y la CIA en Chile, sino que él era el principal artífice detrás de tales operaciones.

Mónica Gónzalez, reconocida periodista chilena ganadora del premio Cabot de la Universidad de Columbia y autora del libro La Conjura afirma:

Además, la misma Mónica González, afirmó respecto a Kissinger lo siguiente:

Las relaciones entre los gobiernos de Estados Unidos y Chile permanecieron heladas durante el mandato de Salvador Allende, siendo el hito que marcó el inicio de este distanciamientos la completa nacionalización de la industria minera de cobre chilena, hasta entonces, prácticamente propiedad de Estados Unidos, mediante la filial chilena de la estadounidense ITT Corporation, así como otras empresas chilenas. Estados Unidos afirmó que el gobierno chileno había devaluado enormemente una compensación equitativa para la nacionalización restando lo que consideró «beneficios extraordinarios». Por lo tanto, Estados Unidos consideró implementar sanciones de orden económico, pero nunca llegaron a ser aplicadas. La CIA también proporcionó financiamiento para las huelgas masivas antigubernamentales en 1972 y 1973.

La CIA, actuando en virtud de la aprobación del "Comité 40" (del que Kissinger era presidente), participó en varias acciones encubiertas en Chile durante este período, ideando lo que en efecto fue un golpe de estado constitucional y, cuando esto fracasó, permaneció en contacto con elementos antigubernamentales. La CIA se enteró de una gran cantidad de asociaciones, planes y organizaciones clandestinas que buscaban establecer una dictadura militar. Aunque intencionalmente se negó a ayudar materialmente a cualquiera de ellos, también alienta a varios de estos grupos y no hizo nada para evitarlos. Aseguró a los conspiradores que tal evento sería bienvenido en Washington y Estados Unidos se encargaría de omitir cualquier mención acerca de posibles violaciones de los derechos humanos.

El 11 de septiembre de 1973, el presidente Allende se suicidó durante el golpe de estado lanzado por el comandante en jefe de Ejército Augusto Pinochet, quien se convirtió en Presidente de facto de Chile y que ejercería una dictadura sobre el país, durante 17 años. Un documento publicado por la CIA en 2000 titulado "CIA, Actividades en Chile" reveló que la CIA apoyó activamente la junta militar tras el derrocamiento de Allende y que tenía muchos de los oficiales de Pinochet en pagos contactos de la CIA, a pesar de que muchos eran conocidos por estar involucrados en notorias violaciones de los derechos humanos, hasta que el candidato Demócrata Jimmy Carter derrotó al Presidente Gerald Ford en 1976.

El 16 de septiembre de 1973, cinco días después de que Augusto Pinochet había asumido el poder, el siguiente intercambio sobre el golpe de estado tuvo lugar entre Kissinger y el Presidente Nixon:

En 1976, Kissinger canceló una carta que fue enviada a Chile advirtiéndoles contra la realización de cualquier asesinato político. Orlando Letelier fue asesinado poco después, en Washington, D.C. con un coche bomba el 21 de septiembre de 1976, justo el día que la carta iba a ser enviada. El Embajador de Estados Unidos en Chile dijo que Pinochet podría tomar como un insulto cualquier insinuación de estar involucrado en el asesinato (lo estaba).

Kissinger tomó una línea similar a la que había practicado en Chile cuando la milicia argentina, dirigida por Jorge Rafael Videla, derrocó el gobierno de Isabel Perón en 1976 con un proceso llamado "Proceso de reorganización nacional" por los militares, con la que consolidó el poder, lanzamiento de brutales represalias y desapariciones contra grupos guerrilleros como ERP y Montoneros. Durante una reunión con el Ministro de Relaciones Exteriores argentino, vicealmirante César Augusto Guzzetti, Kissinger le aseguró que Estados Unidos era un aliado, pero le urgió a "volver a procedimientos normales", rápidamente antes de que el Congreso de Estados Unidos volviese a reunirse y tuvieron la oportunidad de considerar las sanciones.

En ocasión del golpe de estado de Argentina, el 24 de marzo de 1976, alentó y apoyó a la Junta Militar a que tomara el poder. Lo han acusado de complicidad y del estímulo en la eliminación y desaparición sistemática de miles de personas cometidas por la Junta Militar de Argentina del autodenominado proceso de reorganización nacional o ""guerra sucia"" según han denominado varios historiadores. La aceptación de la ""guerra sucia"" argentina por parte de Kissinger (quien en aquel momento era Secretario de Estado de Estados Unidos) ha sido confirmada oficialmente por archivos desclasificados el Archivo Nacional de Seguridad de los Estados Unidos ("The National Security Archive"). En estos archivos se confirma de Kissinger, entre otras frases hacia la Junta militar en referencia a la guerra sucia:

En 1978 Kissinger visitó Argentina invitado por el dictador Videla. Lo hacía a título particular ya que había cesado en su cargo al ganar las elecciones presidenciales de 1976 el demócrata Jimmy Carter. En Argentina, Kissinger elogió a los militares por sus esfuerzos en lo que denominada "combatir el terrorismo". Ello provocó la indignación del embajador estadounidense, Raul Castro, que siguiendo las nuevas directrices de Carter estaba presionando a la junta militar para que respetase los derechos humanos.

La Operación Cóndor, fue un plan de coordinación de operaciones entre las cúpulas de los regímenes dictatoriales del Cono Sur de América —Chile, Argentina, Brasil, Paraguay, Uruguay y Bolivia— y con la CIA de los EE. UU., llevada a cabo en las décadas de 1970 y 1980.

Enmarcada en la Doctrina Truman, esta coordinación se tradujo en "el seguimiento, vigilancia, detención, interrogatorios con apremios psico-físicos, traslados entre países y desaparición o muerte de personas consideradas por dichos regímenes como "subversivas del orden instaurado o contrarias al pensamiento político o ideológico opuesto, o no compatible con las dictaduras militares de la región". El Plan Cóndor se constituyó en una organización clandestina internacional para la práctica del terrorismo de Estado que instrumentó el asesinato y desaparición de decenas de miles de opositores a las mencionadas dictaduras, la mayoría de ellos pertenecientes a movimientos de la izquierda política.

Se afirma que Kissinger, en su odisea por mantener Latinoamérica libre del comunismo y cualquier otra corriente de izquierda, habría sido el responsable o por lo menos un personaje clave en la creación, planificación y puesta en marcha de la Operación Cóndor, dado que para el año en que ésta comenzó a funcionar, 1970, Kissinger ya era Consejero de Seguridad Nacional, además de que es bien sabido que a causa de él, el Cono Sur pasó a estar dominado por dictaduras y en su desempeño como Secretario de Estado, mantuvo una política eficaz en contra de cualquier intento de expansión ideológica y política de orden izquierdista.

La Operación Cóndor, se desarrolló durante más de siete años, bajo la supervisión de Kissinger, siendo sólo en sus etapas finales, ya hacia 1980, cuando este dejó de ejercer la Secretaría de Estado de EE.UU., las únicas en las que quizá no haya tenido injerencia.

De acuerdo a las investigaciones de varios historiadores del siglo XX, han afirmado que durante el período del Frente Nacional, Kissinger ayudó a la emancipación y posterior extensión del mismo en la presidencia colombiana, aparte de que desde su puesto logró promover la asesoría militar al ejército de la nación suramericana por medio de asesores de la CIA en la enseñanza de técnicas de tortura y asesinato encubierto a líderes sindicales de la oposición local, en el marco de la muy célebre y odiada serie de dictaduras amparadas en el continente y conocidas por ser sostenidas desde las operaciones de asesinatos sistemáticos promovidas desde la Operación Cóndor.

En 1974 un golpe militar izquierdista derroca al gobierno de Marcelo Caetano en Portugal, a través de la denominada Revolución de los Claveles.
La "Junta de Salvación Nacional" liderada por el nuevo Primer Ministro de Portugal, Adelino da Palma Carlos, consolidados como el nuevo gobierno del país ibérico, rápidamente concede la independencia a las colonias de Portugal, creando una cadena de reacciones internacionales, con respecto al destino de África.

Cuba apoya abiertamiente a través de sus tropas en Angola, al "Movimiento Popular de izquierda para la liberación de Angola (MPLA)" en su lucha contra los rebeldes UNITA y el FNLA de derecha durante la Guerra Civil angoleña. Kissinger se encargó de apoyar a FNLA, liderado por Roberto Holden y a la UNITA, liderada por Jonas Savimbi, los insurgentes de La resistencia nacional mozambiqueña (RENAMO), así como la invasión de apoyo de la CIA de Angola por tropas sudafricanas. El FNLA fue derrotado y UNITA se vio obligado a llevar su lucha en la selva. Sólo bajo la Presidencia de Ronald Reagan Estados Unidos volvería a dar apoyo a la UNITA.

En septiembre de 1976, Kissinger participó activamente en las negociaciones acerca de la Guerra de Bush Rhodesia. Kissinger, junto con el primer ministro de Sudáfrica John Vorster, presionó al Primer Ministro de Rhodesia, Ian Smith para acelerar la transición de gobierno en favor de las mayorías negras del país. Con el FRELIMO en control de Mozambique y Sudáfrica, al Kissinger retirar su apoyo a manera de presión, el aislamiento de Rhodesia fue total. Según la autobiografía de Smith, Kissinger le comentó la admiración que la Sra. Kissinger le profesaba, pero Smith dijo que él pensó que Kissinger le estaba pidiéndo firmar el "Acta de defunción" de Rhodesia. Kissinger, llevando el peso de los Estados Unidos y acorralando a otras partes interesadas para presionar a Rhodesia, aceleró el fin del régimen de la minoría.

Hacia 1975, España entró en un período de crisis interna, con la enfermedad de Francisco Franco, situación que fue aprovechada por el rey Hassan II para enviar a más civiles y militares al territorio de Sahara Occidental, con el fin de reclamarlo y anexarlo a sus dominios.

Henry Kissinger, jugó un papel clave en este proceso, siendo él, quien planificó, asesoró y organizó a los marroquíes para la Marcha Verde (1975), y auspició las negociaciones entre los representantes marroquíes y el Gobierno español, que finalizaron con la salida del Ejército español de los territorios del Sáhara y el traspaso del mismo a Marruecos y Mauritania.

Estados Unidos, encausado por Kissinger, proporcionó a Marruecos equipos, armamentos, logística y una completa organización para la operación, mientras que Arabia Saudí aportó grandes sumas de dinero para la misma, que era en extremo favorecedora, siéndolo todavía hoy en día, para los intereses de orden militares, estratégicos y económicos de los EE. UU., sobre la región. El conflicto del Sáhara Occidental, continúa todavía sin resolverse.

El proceso de descolonización portuguesa atrajo la atención de Estados Unidos a la antigua colonia portuguesa de Timor Oriental, que se encuentra en el archipiélago indonesio y declaró su independencia en 1975. El presidente indonesio Suharto fue un fuerte aliado de EE.UU. en el sudeste asiático y comenzó a movilizar el ejército indonesio, preparándose para el estado naciente, que se había vuelto cada vez más dominado por el partido popular de FRETILIN izquierdista. En diciembre de 1975, Suharto discutió los planes de invasión durante una reunión con Kissinger y el Presidente Ford en la capital Indonesia de Yakarta. Ford y Kissinger dejaron en claro que las relaciones con Indonesia continuarían siendo fuertes y que no se oponían a la propuesta de anexión. La venta de armas de Estados Unidos a Indonesia continuó, y Suharto siguió adelante con el plan de anexión.

Se acusa a Kissinger de muchas violaciones a los derechos humanos, y de ser instigador de genocidios sistemáticos de grupos políticos.

El juez español Baltasar Garzón, célebre, entre otras muchas cosas, por haber tratado de llevar a juicio a Augusto Pinochet e intentar esclarecer los atentados contra la población civil en Argentina durante el directorio militar de Videla, envió una comisión rogatoria referente a violaciones de los Derechos Humanos a los Estados Unidos, pero el Departamento del Estado de EE.UU. la rechazó. Igualmente existen numerosas iniciativas que persiguen conseguir su procesamiento ante instancias judiciales internacionales, así como la retirada de su Premio Nobel.

Henry Kissinger también es uno de los más conocidos miembros del Grupo Bilderberg junto a David Rockefeller, además de ostentar la posición de ser uno de su miembros fundadores originales. Ambos colaboraron de pleno con el príncipe Bernardo de Lippe-Biesterfeld, junto con la Banca Rothschild, en los planes de nacimiento, organización y expansión del Grupo Bilderberg, entidad foco de una gran teoría conspirativa.

El Grupo Bilderberg, en sus reuniones anuales agrupa a algunos de los , para discutir acerca de todo lo que está pasando en el mundo y sus opiniones al respecto y en lo que al público concierne.

Kissinger, también ha sido objeto de críticas a causa de su relación con la muy polémica y criticada Corporación RAND, una poderosa empresa de desarrollo tecnológico, industrial y energético, que posee numerosas alianzas, contratos y acuerdos comerciales con el gobierno de los Estados Unidos, específicamente con las Fuerzas Armadas del mismo y con los Departamentos de Seguridad Nacional.

Kissinger, habría fungido de asesor de esta compañía desde el año de 1951, además de mantener estrechos vínculos con ella durante todo el período de tiempo que ejerció como Consejero de Seguridad Nacional y también como Secretario Estado.

Esta cercanía entre Henry Kissinger y la Corporación RAND, le ha valido al primero el recibir las mismas críticas que se le hacen a la empresa, acusándola de ser una organización militarista y de promover los conflictos militares, llegando al punto de haber teorías de conspiración, que vinculan a los conflictos bélicos que hubo durante el período de Kissinger en la Casa Blanca, con la Corporación RAND, basándose en la suposición de que estos conflictos representaron oportunidades multimillonarias de negocios para la misma. A pesar de todo, ninguna de estas acusaciones disponen de algún tipo de prueba que pueda justificarlas y lo cierto es que los acontecimientos que involucran a la RAND, ""per se"", se basan en suposiciones que son difíciles de verificar, debido a la falta de detalles acerca del trabajo, de alto secreto, llevado a cabo por la RAND para agencias de inteligencia y de defensa.

En ocasión del golpe de Estado de Argentina, el 24 de marzo de 1976, a Kissinger se le critica por haber alentado y apoyado a la Junta Militar a que tomara el poder, además de haberla respaldado tras el golpe y de paso, por haberla utilizado como herramienta para fortificar la influencia de Estados Unidos sobre el Cono Sur. Igualmente, ha sido acusado de complicidad y del estímulo en la eliminación y desaparición sistemática de miles de opositores, cometidas por la Junta Militar de Argentina del autodenominado Proceso de Reorganización Nacional o ""guerra sucia"" según han denominado varios historiadores. La aceptación de la ""guerra sucia"" argentina por parte de Kissinger (quien en aquel momento era Secretario de Estado de Estados Unidos) ha sido confirmada oficialmente por archivos desclasificados el "Archivo Nacional de Seguridad de los Estados Unidos" ("The National Security Archive"). En estos archivos se confirma de Kissinger, emitió entre muchas otras frases hacia la Junta militar, la siguiente, en referencia a la ""guerra sucia"":

Se afirma su participación en la organización del golpe de Estado contra el gobierno de Salvador Allende en Chile. Se lo acusa además de haber organizado la denominada Operación Cóndor, un plan sistemático de eliminación de opositores dirigido a "combatir el comunismo" en Latinoamérica.
Fue sometido a proceso en Estados Unidos por el asesinato del comandante en jefe del Ejército chileno René Schneider, fallando en 2006 la Corte Suprema de ese país que su responsabilidad había sido política y no criminal.

Durante el mandato del Frente Nacional, se supone que ayudó a la emancipación y posterior extensión del mismo en la presidencia colombiana, aparte de que desde su puesto logró promover la asesoría militar al ejército de la nación suramericana por medio de asesores de la CIA en la enseñanza de técnicas de tortura y asesinato encubierto a líderes sindicales de la oposición local, en el marco de la muy célebre y odiada serie de dictaduras amparadas en el continente y conocidas por ser sostenidas desde las operaciones de asesinatos sistemáticos promovidas desde la Operación Cóndor.

Henry Kissinger apoyó al régimen indonesio del general Suharto, acusado del genocidio contra la población de Timor Oriental, además de estar involucrado con las actividades de este gobierno, así como también en la invasión a Timor Oriental. Además sirvió a dicho gobierno como "Asesor General de Gobierno", otra acción que le ha sido tremendamente criticada.

Se conoce su implicación directa en los bombardeos secretos de Laos y Camboya, ordenados sin permiso del Congreso. Dichos bombardeos sirvieron para que los jemeres rojos accedieran al poder, del que se servirían para asesinar a más de dos millones de personas.

Kissinger asesoró y preparó a los marroquíes para la Marcha Verde (1975), y auspició las negociaciones entre los representantes marroquíes y el Gobierno español, que concluyeron con la retirada del Ejército español del Sáhara y la entrada al territorio por Marruecos y Mauritania. Estados Unidos proporcionó a Marruecos logística y armamento, y Arabia Saudí aportó grandes sumas de dinero para esta operación, que favorecía (y aún hoy favorece) a los intereses estratégicos y comerciales de los EE.UU. El conflicto del Sáhara Occidental aún no se ha resuelto.

Tras su salida de la Secretaría de Estado, inmediatamente le fue ofrecida a Kissinger, dictar una cátedra en la Universidad de Columbia, oferta que aceptó gustoso. Además fundó su propia firma de Asesoría Política y Diplomática, llamada "Kissinger & Associates" y es cofundador y actual accionista de la "Kissinger & McLarty Associates", otra compañía de asesoría. Además pasó a ejercer como "profesor de la Universidad de Georgetown" y a ser miembro de las juntas directiva y asesor general de las compañías: Hollinger Group, un grupo de medios masivos, canales de televisión, revistas, editoriales, periódicos y demás pulicaciones con sede en Chicago y la Gulfstream Aerospace, una compañía especializada en la industria del transporte aéreo y en la construcción de aviones.

Además ha continuado publicando libros, artículos y ensayos, así como también ha dado conferencias y es asesor de múltiples otras compañías.

Su firma fue contratada por el gobierno de George W Bush, para dirigir un comité de acción tras los atentados del 11-S y para prestar sus servicios como compañía asesora, pero se desentendió de este contrato poco antes de que Bush hiciera pública su idea de invadir Irak y de comenzar la Guerra de Afganistán. Además, prestó sus servicios como "Asesor Político General al Gobierno de Indonesia."

El Secretario Kissinger ha escrito muchos libros y artículos sobre política exterior de Estados Unidos, los asuntos internacionales, y la historia diplomática. Entre los premios que ha recibido se encuentran:


Hablar de política exterior en el siglo XX sin nombrar a Kissinger es algo que resulta imposible. Henry Kissinger fue Secretario de Estado de los Estados Unidos de América en un período de tiempo bastante agitado en la política internacional y su proceder, así como su capacidad para solventar todo tipo de problemas que se le presentase, lo convirtieron en una referencia de la diplomacia estadounidense, siendo en gran medida el creador de variedad de tendencias o métodos de acción, en este ámbito, que todavía siguen usándose hoy por hoy (intervenciones militares, negociaciones, acciones en búsqueda de determinados intereses, entre otros.).

Kissinger es admirado por unos a la vez que es odiado y temido por otros. La mayor crítica que se le hace es el sobreponer los intereses de su nación por encima del bienestar político de las demás, siendo responsable o estando vinculado a una gran variedad de dictaduras, lo que ha llevado a que muchos lo acusen de estar ligado a las múltiples violaciones a los Derechos Humanos acaecidas durante las mismas, algo que es y seguirá siendo motivo de especulación y controversia.

A pesar de todo, Kissinger inició la aplicación de un nuevo enfoque con respecto a la Guerra Fría, el cual, lastimosamente, no sería continuado por sus sucesores, que hubiera dado como resultado en el futuro una relación productiva no violenta entre la URSS y EE. UU. Muchos consideran esta política de distensión con la URSS la más adecuada, entendiendo el expansionismo e imperialismo como el enfoque correcto, que es retomado por la administración Reagan, con su proyecto "Guerra de las Galaxias" y sus ofensivas internacionales.

Entre los logros de Kissinger destaca la apertura de relaciones entre China y Estados Unidos, la aplicación de una vasta serie de medidas para la producción energética alternativa, durante y posteriormente a la Crisis del Petróleo de 1973, la firma de los" Tratados SALT I" y la preparación del terreno para la posterior firma de su sucesor, los "Tratados SALT II", el inicio del proceso de colaboración y negociación en el Medio Oriente, específicamente en el conflicto del mundo árabe, sentando las bases para la realización de los Acuerdos de Camp David, y poner fin a un muy criticado y rechazado conflicto, la Guerra de Vietnam, sobrellevar con éxito tanto la Guerra del Yom Kipur como la Guerra indo-pakistaní de 1971 y en líneas generales, conseguir con su política consolidar el poderío internacional estadounidense, así como extender y asegurar sus zonas de influencia a lo largo del mundo.

Es por esta inmensa gama de actividades que Henry Kissinger es recordado como el más emblemático Secretario de Estado de Estados Unidos, al punto de que este puesto es directamente identificado con su persona. Pocos Secretarios de Estado, por no decir ninguno, han tenido un nivel de participación y protagonismo tan grande como el suyo en los sucesos acaecidos a lo largo del mundo, convirtiéndolo en un auténtico ícono para muchas corrientes capitalistas y de derechas, tanto de Estados Unidos como del mundo.













Otras fuentes




1. Obras de Henry Kissinger

Autobiografía

Memoirs, etc.

2. Libros sobre Kissinger (biografías)


3. Otros libros sobre Kissinger



</doc>
<doc id="8221" url="https://es.wikipedia.org/wiki?curid=8221" title="Tsunami">
Tsunami

Un tsunami (del japonés «津» "tsu", puerto o bahía, y «波» "nami", ola) o maremoto (del latín "mare", mar y "motus", movimiento) es un evento complejo que involucra un grupo de olas de gran energía y de tamaño variable que se producen cuando algún fenómeno extraordinario desplaza verticalmente una gran masa de agua. Este tipo de olas remueven una cantidad de agua muy superior a las olas superficiales producidas por el viento. Se calcula que el 90% de estos fenómenos son provocados por terremotos, en cuyo caso reciben el nombre más correcto y preciso de «maremotos tectónicos». La energía de un maremoto depende de su altura, de su longitud de onda y de la longitud de su frente. La energía total descargada sobre una zona costera también dependerá de la cantidad de picos que lleve el tren de ondas. Es frecuente que un tsunami que viaja grandes distancias, disminuya la altura de sus olas, pero siempre mantendrá una velocidad determinada por la profundidad sobre la cual el tsunami se desplaza. Normalmente, en el caso de los tsunamis tectónicos, la altura de la onda de tsunami en aguas profundas es del orden de 1.0 metros, pero la longitud de onda puede alcanzar algunos cientos de kilómetros. Esto es lo que permite que aún cuando la altura en océano abierto sea muy baja, esta altura crezca en forma abrupta al disminuir la profundidad, con lo cual, al disminuir la velocidad de la parte delantera del tsunami, necesariamente crezca la altura por transformación de energía cinética en energía potencial. De esta forma una masa de agua de algunos metros de altura puede arrasar a su paso hacia el interior.

Antes, el término "tsunami" también sirvió para referirse a las olas producidas por huracanes y temporales que, como los maremotos, podían entrar tierra adentro, pero éstas no dejaban de ser olas superficiales producidas por el viento, aunque se trata aquí de un viento excepcionalmente potente.

Tampoco se deben confundir con la ola producida por la marea conocida como macareo. Éste es un fenómeno regular y mucho más lento, aunque en algunos lugares estrechos y de fuerte desnivel pueden generarse fuertes corrientes.

La mayoría de los tsunamis son originados por terremotos de gran magnitud bajo la superficie acuática. Para que se origine un tsunami el fondo marino debe ser movido abruptamente en sentido vertical, de modo que una gran masa de agua del océano es impulsada fuera de su equilibrio normal. Cuando esta masa de agua trata de recuperar su equilibrio genera olas. El tamaño del tsunami estará determinado por la magnitud de la deformación vertical del fondo marino entre otros parámetros como la profundidad del lecho marino. No todos los terremotos bajo la superficie acuática generan maremotos, sino sólo aquellos de magnitud considerable con hipocentro en el punto de profundidad adecuado.

Un tsunami tectónico producido en un fondo oceánico de 5 km de profundidad removerá toda la columna de agua desde el fondo hasta la superficie. El desplazamiento vertical puede ser tan sólo de centímetros; pero, si se produce a la suficiente profundidad, la velocidad será muy alta y la energía transmitida a la onda será enorme. Aun así, en alta mar la ola pasa casi desapercibida, ya que queda camuflada entre las olas superficiales. Sin embargo, destacan en la quietud del fondo marino, el cual se agita en toda su profundidad.
La zona más afectada por este tipo de fenómenos es el océano Pacífico, debido a que en él se encuentra la zona más activa del planeta, el cinturón de fuego. Por ello, es el único océano con un sistema de alertas verdaderamente eficaz.

No existe un límite claro respecto de la magnitud necesaria de un sismo como para generar un tsunami. Los elementos determinantes para que ocurra un tsunami son la magnitud del sismo originador, la profundidad del hipocentro y la morfología de las placas tectónicas involucradas. Esto hace que para algunos lugares del planeta se requieran grandes sismos para generar un tsunami, en tanto que para otros baste para ello la existencia de sismos de menor magnitud. En otros términos, la geología local, la magnitud y la profundidad focal son parte de los elementos que definen la ocurrencia o no de un tsunami de origen tectónico.

La velocidad de las olas puede determinarse a través de la ecuación:

formula_1,

donde D es la profundidad del agua que está directamente sobre el sismo y g, la gravedad terrestre (9,8 m/s²).

A las profundidades típicas de 4-5 km las olas viajarán a velocidades en torno a los 600 kilómetros por hora o más. Su amplitud superficial o altura de la cresta H puede ser pequeña, pero la masa de agua que agitan es enorme, y por ello su velocidad es tan grande; y no sólo eso, pues la distancia entre picos también lo es. Es habitual que la longitud de onda de la cadena de maremotos sea de 100 km, 200 km o más.
El intervalo entre cresta y cresta (período de la onda) puede durar desde menos de diez minutos hasta media hora o más. Cuando la ola entra en la plataforma continental, la disminución drástica de la profundidad hace que su velocidad disminuya y empiece a aumentar su altura. Al llegar a la costa, la velocidad habrá decrecido hasta unos 50 kilómetros por hora, mientras que la altura ya será de unos 3 a 30 m, dependiendo del tipo de relieve que se encuentre. La distancia entre crestas (longitud de onda L) también se estrechará cerca de la costa.

Debido a que la onda se propaga en toda la columna de agua, desde la superficie hasta el fondo, se puede hacer la aproximación a la teoría lineal de la hidrodinámica. Así, el flujo de energía E se calcula como:

formula_2,

siendo 'd' la densidad del fluido.

La teoría lineal predice que las olas conservarán su energía mientras no rompan en la costa. La disipación de la energía cerca de la costa dependerá, como se ha dicho, de las características del relieve marino. La manera como se disipa dicha energía antes de romper depende de la relación H/h, sobre la cual hay varias teorías. Una vez que llega a tierra, la forma en que la ola rompe depende de la relación H/L. Como L siempre es mucho mayor que H, las olas romperán como lo hacen las olas bajas y planas. Esta forma de disipar la energía es poco eficiente, y lleva a la ola a adentrarse tierra adentro como una gran marea.

A la llegada a la costa la altura aumentará, pero seguirá teniendo forma de onda plana. Se puede decir que hay un trasvase de energía de velocidad a amplitud. La ola se frena pero gana altura. Pero la amplitud no es suficiente para explicar el poder destructor de la ola. Incluso en un maremoto de menos de 5 m los efectos pueden ser devastadores. La ola es mucho más de lo que se ve. Arrastra una masa de agua mucho mayor que cualquier ola convencional, por lo que el primer impacto del frente de la onda viene seguido del empuje del resto de la masa de agua perturbada que presiona, haciendo que el mar se adentre más y más en tierra. Por ello, la mayoría de los maremotos tectónicos son vistos más como una poderosa riada, en la cual es el mar el que inunda a la tierra, y lo hace a gran velocidad.

Antes de su llegada, el mar acostumbra a retirarse a distancias variables de la costa, que en caso de fondos relativamente planos, puede llegar a varios centenares de metros, como una rápida marea baja. Desde entonces hasta que llega la ola principal pueden pasar de 5 a 10 minutos, como también existen casos en los que han transcurrido horas para que la marejada llegue a tierra. A veces, antes de llegar la cadena principal del maremoto, los que realmente arrasarán la zona, pueden aparecer «micromaremotos» de aviso. Así ocurrió el 26 de diciembre de 2004 en las costas de Sri Lanka donde, minutos antes de la llegada de la ola fuerte, pequeños maremotos entraron unos cincuenta metros playa adentro, provocando el desconcierto entre los bañistas antes de que se les echara encima la ola mayor. Según testimonios, «se vieron rápidas y sucesivas mareas bajas y altas, luego el mar se retiró por completo y solo se sintió el estruendo atronador de la gran ola que venía».

Debido a que la energía de los maremotos tectónicos es casi constante, pueden llegar a cruzar océanos y afectar a costas muy alejadas del lugar del suceso. La trayectoria de las ondas puede modificarse por las variaciones del relieve abisal, fenómeno que no ocurre con las olas superficiales. Los maremotos tectónicos, dado que se producen debido al desplazamiento vertical de una falla, la onda que generan suele ser un tanto especial. Su frente de onda es recto en casi toda su extensión. Solo en los extremos se va diluyendo la energía al curvarse. La energía se concentra, pues, en un frente de onda recto, lo que hace que las zonas situadas justo en la dirección de la falla se vean relativamente poco afectadas, en contraste con las zonas que quedan barridas de lleno por la ola, aunque éstas se sitúen mucho más lejos. El peculiar frente de onda es lo que hace que la ola no pierda energía por simple dispersión geométrica, sobre todo en su zona más central. El fenómeno es parecido a una onda encajonada en un canal o río. La onda, al no poder dispersarse, mantiene constante su energía. En un maremoto sí existe, de hecho, cierta dispersión pero, sobre todo, se concentra en las zonas más alejadas del centro del frente de onda recto.

En la imagen animada del maremoto del océano Índico se puede observar cómo la onda se curva por los extremos y cómo Bangladés, al estar situado justo en la dirección de la falla fracturada, apenas sufre sus efectos, mientras que Somalia, a pesar de encontrarse mucho más lejos, cae justo en la dirección de la zona central de la ola, que es donde la energía es mayor y se conserva mejor.

El profesor Manuel García Velarde sostiene que los maremotos son ejemplos paradigmáticos de este tipo especial de ondas no lineales conocidas como solitones u ondas solitarias. El concepto de solitón fue introducido por los físicos N. Zabusky y M. Krustal en 1965, aunque ya habían sido estudiados, a finales del siglo XIX, por D. Korteweg y G. de Vries, entre otros.

El fenómeno físico (y concepto matemático) de los solitones fue descrito, en el siglo XIX, por J. S. Russell en canales de agua de poca profundidad, y son observables también en otros lugares. Manuel García Velarde dice:

Existen otros mecanismos generadores de maremotos menos corrientes que también pueden producirse por erupciones volcánicas, deslizamientos de tierra, meteoritos o explosiones submarinas. Estos fenómenos pueden producir olas enormes, mucho más altas que las de los maremotos corrientes. Se trata de los llamados megamaremotos, término que, si bien no es científico, puede usarse de forma poco rigurosa para referirse a los maremotos generados por causas no tectónicas. De todas estas causas alternativas, la más común es la de los deslizamientos de tierra producidos por erupciones volcánicas explosivas, que pueden hundir islas o montañas enteras en el mar en cuestión de segundos. También existe la posibilidad de desprendimientos naturales tanto en la superficie como debajo de ella. Este tipo de maremotos difieren drásticamente de los maremotos tectónicos.

En primer lugar, la cantidad de energía que interviene. Está el terremoto del océano Índico de 2004, con una energía desarrollada de unos 32.000 MT. Solo una pequeña fracción de ésta se traspasará al maremoto. Por el contrario, un ejemplo clásico de megamaremoto sería la explosión del volcán Krakatoa, cuya erupción generó una energía de 300 MT. Sin embargo, se midió una altitud en las olas de hasta 50 m, muy superior a la de las medidas por los maremotos del océano Índico. La razón de estas diferencias estriba en varios factores. Por una parte, el mayor rendimiento en la generación de las olas por parte de este tipo de fenómenos, menos energéticos pero que transmiten gran parte de su energía al mar. En un seísmo (o sismo), la mayor parte de la energía se invierte en mover las placas. Pero, aun así, la energía de los maremotos tectónicos sigue siendo mucho mayor que la de los megamaremotos. Otra de las causas es el hecho de que un maremoto tectónico distribuye su energía a lo largo de una superficie de agua mucho mayor, mientras que los megamaremotos parten de un suceso muy puntual y localizado. En muchos casos, los megamaremotos también sufren una mayor dispersión geométrica, debido justamente a la extrema localización del fenómeno. Además, suelen producirse en aguas relativamente poco profundas de la plataforma continental. El resultado es una ola con mucha energía en amplitud superficial, pero de poca profundidad y menor velocidad. Este tipo de fenómenos son increíblemente destructivos en las costas cercanas al desastre, pero se diluyen con rapidez. Esa disipación de la energía no sólo se da por una mayor dispersión geométrica, sino también porque no suelen ser olas profundas, lo cual conlleva turbulencias entre la parte que oscila y la que no. Eso comporta que su energía disminuya bastante durante el trayecto.
El ejemplo típico más cinematográfico, de megamaremoto es el causado por la caída de un meteorito en el océano. De ocurrir tal cosa, se producirían ondas curvas de gran amplitud inicial, bastante superficiales, que sí tendrían dispersión geométrica y disipación por turbulencia, por lo que, a grandes distancias, quizá los efectos no serían tan dañinos. Una vez más los efectos estarían localizados, sobre todo, en las zonas cercanas al impacto. El efecto es exactamente el mismo que el de lanzar una piedra a un estanque. Evidentemente, si el meteorito fuera lo suficientemente grande, daría igual cuán alejado se encontrara el continente del impacto, pues las olas lo arrasarían de todas formas con una energía inimaginable. Maremotos apocalípticos de esa magnitud debieron producirse hace 65 millones de años cuando un meteorito cayó en la actual península de Yucatán. Este mecanismo generador es, sin duda, el más raro de todos; de hecho, no se tienen registros históricos de ninguna ola causada por un impacto.

Algunos geólogos especulan que un megamaremoto podría producirse en un futuro próximo (en términos geológicos) cuando se produzca un deslizamiento en el volcán de la parte inferior de la isla de La Palma, en las islas Canarias (cumbre Vieja). Sin embargo, aunque existe esa posibilidad (de hecho algunos valles de Canarias, como el de Güímar, en Tenerife, o el del Golfo, en El Hierro, se formaron por episodios geológicos de este tipo), no parece que eso pueda ocurrir a corto plazo, sino dentro de cientos o miles de años. Esta especulación ha causado una cierta polémica, siendo tema de discusión entre distintos geólogos. Un maremoto es un peligro para el lugar en que se encuentre o se origine, pero también este fenómeno tiene ventajas hacia nuestro planeta.

Se conservan muchas descripciones de olas catastróficas en la Antigüedad, especialmente en la zona mediterránea.

Algunos autores afirman que la leyenda de la Atlántida está basada en la dramática desaparición de la civilización minoica que habitaba en Creta en el siglo XVI a. C. Según esta hipótesis, las olas que generó la explosión de la isla volcánica de Santorini destruyeron al completo la ciudad de Teras, que se situaba en ella y que era el principal puerto comercial de los minoicos. Dichas olas habrían llegado a Creta con 100 o 150 m de altura, asolando puertos importantes de la costa norte de la isla, como los de Cnosos. Supuestamente, gran parte de su flota quedó destruida y sus cultivos malogrados por el agua de mar y la nube de cenizas. Los años de hambruna que siguieron debilitaron al gobierno central, y la repentina debilidad de los antaño poderosos cretenses los dejó a merced de las invasiones. La explosión de Santorini pudo ser muy superior a la del volcán Krakatoa.

Los Investigadores Antonio Rodríguez Ramírez y Juan Antonio Morales González , de los Departamentos de Geodinámica-Paleontología y Geología de la Facultad de Ciencias Experimentales de la Universidad de Huelva, ha estudiado abundantes restos de tsunamis en el Golfo de Cádiz. Estos estudios se han centrado en el estuario del Tinto-Odiel y en el del Guadalquivir. Las evidencias más antiguas corresponden al Guadalquivir con un episodio del 1500-2000 años antes de nuestra era, afectando a áreas que distan más de 15 km de la costa. En el estuario del Tinto Odiel aparecen depósitos sedimentarios relacionados con tsunamis históricos del 382-395, 881, 1531 y 1755.

En el 218 a.C. y 210 a.C. hubo un tsunami en la península Ibérica. Se tomó el Golfo de Cádiz como objeto de estudio principal y se ha llegado a la conclusión de que hubo una gigantesca ruptura de estratos. Un tsunami se hace reconocible por los destrozos impresionantes de los que quedan restos detectables siglos después; estos desastres ambientales de transformación del paisaje costero a través de la paleogeografía se puede reconstruir. Las ondas de tsunami llegan a zonas donde no llega habitualmente el agua marina y esos restos son los que prueban esas catástrofes. Ésta se ha registrado en el estuario del Guadalquivir y en el área de Doñana. Luego el estudio se ha ampliado a la costa atlántica y se ha comparado con las consecuencias paleogeográficas producidas en el gran tsunami y terremoto de Lisboa de 1755.

Este estudio nos señala que existen zonas predispuestas a que haya tsunamis, es decir a sufrir esta expulsión de energía por parte de la naturaleza.

El 8 de julio a las 04:45 toda el área central de Chile fue remecida por un fuerte terremoto que causó daños en Valparaíso, La Serena, Coquimbo, Illapel, Petorca y Tiltil. El tsunami resultante afectó alrededor de 1.000 km de costa. Por primera vez en su historia, el puerto de Valparaíso fue inundado y severamente dañado. En las partes bajas de El Almendral todas las casas, fortificaciones y bodegas fueron destruidas por la inundación. También inundó el sector cubierto hoy en día por la avenida Argentina, llegando hasta los pies de Santos Ossa.

El terremoto y tsunami de 1730 inundó Valparaíso, arrasó Concepción, hizo retroceder las aguas del río Valdivia e incluso llegó a Perú. El tsunami también cruzó el Océano Pacífico hasta Japón, donde inundó casas y campos de arroz en la península de Oshika en Sendai.

El denominado terremoto de Lisboa de 1755, ocurrido el 1 de noviembre de dicho año, y al que se ha atribuido una magnitud de 9 en la escala de Richter (no comprobada ya que no existían sismógrafos en la época), tuvo su epicentro en la falla Azores-Gibraltar, a 37° de latitud Norte y 10° de longitud Oeste (a 800 km al suroeste de la punta sur de Portugal). Además de destruir Lisboa y hacer temblar el suelo hasta Alemania, el terremoto produjo un gran maremoto que afectó a todas las costas atlánticas. Entre treinta minutos y una hora después de producirse el sismo, olas de entre 6 y 20 metros sobre el puerto de Lisboa y sobre ciudades del suroeste de la península Ibérica mataron a millares de personas y destruyeron poblaciones. Más de un millar de personas perecieron solamente en Ayamonte y otras tantas en Cádiz; numerosas poblaciones en el Algarve resultaron destruidas y las costas de Marruecos y Huelva quedaron gravemente afectadas. Antes de la llegada de las enormes olas, las aguas del estuario del Tajo se retiraron hacia el mar, mostrando mercancías y cascos de barcos olvidados que yacían en el lecho del puerto.
Las olas se propagaron, entre otros lugares, hasta las costas de Martinica, Barbados, América del Sur y Finlandia.

El 27 de agosto de 1883 a las diez y cinco (hora local), la descomunal explosión del Krakatoa, que hizo desaparecer al citado volcán junto con aproximadamente el 45% de la isla que lo albergaba, produjo una ola de entre 15 y 42 metros de altura, según las zonas, que acabó con la vida de aproximadamente 20.000 personas.

La unión de magma oscuro con magma claro en el centro del volcán fue lo que originó dicha explosión. Pero no sólo las olas mataron ese día. Enormes coladas piroclásticas viajaron incluso sobre el fondo marino y emergieron en las costas más cercanas de Java y Sumatra, haciendo hervir el agua y arrasando todo lo que encontraban a su paso. Asimismo, la explosión emitió a la estratosfera gran cantidad de aerosoles, que provocaron una bajada global de las temperaturas. Además, hubo una serie de erupciones que volvieron a formar un volcán, que recibió el nombre de Anak Krakatoa, es decir, ‘el hijo del Krakatoa’.

En la madrugada del 28 de diciembre de 1908 se produjo un terrible terremoto en las regiones de Sicilia y de Calabria, en el sur de Italia. Fue acompañado de un maremoto que arrasó completamente la ciudad de Mesina, en Sicilia.
La ciudad quedó totalmente destruida y tuvo que ser levantada de nuevo en el mismo lugar. Se calcula que murieron cerca de 70.000 personas en la catástrofe (200.000 según estimaciones de la época).
La ciudad contaba entonces con unos 150.000 habitantes. También la ciudad de Regio de Calabria, situada al otro lado del estrecho de Mesina, sufrió importantes consecuencias. Fallecieron unas 15.000 personas, sobre una población total de 45.000 habitantes.

Un terremoto en el océano Pacífico provocó un maremoto que acabó con 165 vidas en Hawái y Alaska. Este maremoto hizo que los estados de la zona del Pacífico creasen un sistema de alertas, que entró en funcionamiento en 1949.

El 9 de julio de 1958, en la bahía Lituya, al noreste del golfo de Alaska, un fuerte sismo, de 8,3 grados en la escala de Richter, hizo que se derrumbara prácticamente una montaña entera, generando una pared de agua que se elevó sobre los 580 metros, convirtiéndose en la ola más grande de la que se tenga registro, llegando a calificarse el suceso de "megatsunami".

El terremoto de Valdivia (también llamado el Gran Terremoto de Chile), ocurrido el 22 de mayo de 1960, es el sismo de mayor magnitud registrado hasta ahora por sismógrafos a nivel mundial. Se produjo a las 15:11 (hora local), tuvo una magnitud de 9,5 en la escala de Richter y una intensidad de XI a XII en la escala de Mercalli, y afectó al sur de Chile. Su epicentro se localizó en Valdivia, a los 39,5º de latitud sur y a 74,5º de longitud oeste; el hipocentro se localizó a 6 km de profundidad, aproximadamente 700 km al sur de Santiago. El sismo causó un maremoto que se propagó por el océano Pacífico y devastó Hilo a 10.000 km del epicentro, como también las regiones costeras de Sudamérica. El número total de víctimas fatales causadas por la combinación de terremoto-maremoto se estima en 3.000.

En los minutos posteriores un maremoto arrasó lo poco que quedaba en pie. El mar se recogió por algunos minutos y luego una gran ola se levantó acabando a su paso con casas, animales, puentes, botes y, por supuesto, muchas vidas humanas.
Cuando el mar se recogió varios metros, la gente pensó que el peligro había pasado y en vez de alejarse caminaron hacia las playas, recogiendo pescados, moluscos y otros residuos marinos. Para el momento en que se percataron de la gran ola, ya era demasiado tarde.

Como consecuencia del terremoto se originó un tsunami que arrasó con algunos lugares de las costas de Japón (142 muertes y daños por 50 millones de dólares), Hawái (61 fallecimientos y 75 millones de dólares en daños), Filipinas (32 víctimas y desaparecidos). La costa oeste de Estados Unidos también registró un maremoto, que provocó daños por más de medio millón de dólares estadounidenses.

Un terremoto importante de magnitud 8,1 grados Richter ocurrió a las 02:59:43 (UTC) el 12 de diciembre de 1979 a lo largo de la costa pacífica de Colombia y el Ecuador. El terremoto y el maremoto asociado fueron responsables de la destrucción de por lo menos seis municipios de pesca y de la muerte de centenares de personas en el departamento de Nariño en Colombia. El terremoto se sintió en Bogotá, Pereira, Cali, Popayán, Buenaventura, Medellín y otras ciudades y partes importantes en Colombia, y en Guayaquil, Esmeraldas, Quito y otras partes de Ecuador. El maremoto de Tumaco causó, al romper contra la costa, gran destrucción en la ciudad de Tumaco y las poblaciones de El Charco, San Juan, Mosquera y Salahonda en el Pacífico colombiano. Este fenómeno dejó un saldo de 259 muertos, 798 heridos y 95 desaparecidos.

Un terremoto ocurrido en las costas del pacífico de Nicaragua, de entre 7,2 y 7,8 grados en la escala de Richter, el 2 de septiembre de 1992, provocó un maremoto con olas de hasta 10 metros de altura, que azotó gran parte de la costa del Pacífico de este país, provocando más de 170 muertos y afectando a más de 40.000 personas, en al menos una veintena de comunidades, entre ellas San Juan del Sur. 

Un maremoto (tsunami) imprevisto ocurrió a lo largo de la costa de Hokkaido en Japón, como consecuencia de un terremoto, el 12 de julio de 1993. Como resultado, 202 personas de la pequeña isla de Okushiri perdieron la vida, y centenares resultaron heridas.
Este maremoto provocó que algunas oficinas cayeran en quiebra, las olas adquirieron una altura de 31 metros, pero sólo atacó a esta isla.

Hasta la fecha, el maremoto más devastador ocurrió el 26 de diciembre de 2004 en el océano Índico, con un número de víctimas directamente atribuidas al maremoto (tsunami) de aproximadamente 280.000 personas. Las zonas más afectadas fueron Indonesia y Tailandia, aunque los efectos destructores alcanzaron zonas situadas a miles de kilómetros: Malasia, Bangladés, India, Sri Lanka, las Maldivas e incluso Somalia, en el este de África. Esto dio lugar a la mayor catástrofe natural ocurrida desde el Krakatoa, en parte debido a la falta de sistemas de alerta temprana en la zona, quizás como consecuencia de la poca frecuencia de este tipo de sucesos en esta región.

El terremoto fue de 9,1 grados: el tercero más poderoso tras el terremoto de Alaska (9,2) y de Valdivia (Chile) de 1960 (9,5). En Banda Aceh formó una pared de agua de 10 o 18 m de altura penetrando en la isla 1 o 3 km desde la costa al interior; solo en la isla de Sumatra murieron 228.440 personas o más. Sucesivas olas llegaron a Tailandia, con olas de 15 metros que mataron a 5.388 personas; en la India murieron 10.744 personas y en Sri Lanka, hubo 30.959 víctimas. Este tremendo tsunami fue debido además de a su gran magnitud (9,1), a que el epicentro estuvo solo a 9 km de profundidad, y a que la rotura de la placa tectónica fue de 1.600 km de longitud (600 km más que en el terremoto de Chile de 1960).

Un temblor destructivo que alcanzó una magnitud de 6,2 MW y VIII en la de Mercalli afectó a las 13.53 horas a la Región de Aysén, en Chile.
El fenómeno, que se extendió por 30 segundos, tuvo lugar en el Fiordo Aysén, 20 km al noroeste de Puerto Chacabuco. Posteriormente, se produjo una réplica de menor intensidad, que se extendió por 20 segundos. Se produjeron diversos tipos de remociones en masa en laderas de las riberas del Fiordo Aysén, tres de las cuales generaron tsunamis que causaron la muerte de tres personas y la desaparición de siete, y severos daños en las instalaciones de las salmoneras.

El terremoto de Chile de 2010 fue un fuerte sismo ocurrido a las 3:34:17 hora local (UTC-3), del 27 de febrero de 2010, que alcanzó una magnitud de 8,8 MW de acuerdo al Servicio Sismológico de Chile y al Servicio Geológico de Estados Unidos. El epicentro se ubicó en la costa frente a la localidad de Cobquecura, aproximadamente 150 km al noroeste de Concepción y a 63 km al suroeste de Cauquenes, y a 47,4 km de profundidad bajo la corteza terrestre.

Un fuerte tsunami impactó las costas chilenas como producto del terremoto, destruyendo varias localidades ya devastadas por el impacto telúrico. El Archipiélago de Juan Fernández, pese a no sentir el sismo, fue impactado por las marejadas que arrasaron con su único poblado, San Juan Bautista, en la Isla Robinson Crusoe. La alerta de tsunami generada para el océano Pacífico se extendió posteriormente a 53 países ubicados a lo largo de gran parte de su cuenca, llegando a Perú, Ecuador, Colombia, Panamá, Costa Rica, la Antártida, Nueva Zelanda, la Polinesia Francesa y las costas de Hawái.

El sismo es considerado como el segundo más fuerte en la historia del país y uno de los diez más fuertes registrados por la humanidad. Sólo es superado a nivel nacional por el cataclismo del terremoto de Valdivia de 1960, el de mayor intensidad registrado mediante sismómetros. El sismo chileno fue 31 veces más fuerte y liberó cerca de 178 veces más energía que el devastador terremoto de Haití ocurrido el mes anterior. La energía liberada fue cercana a 100 000 bombas atómicas como la liberada en Hiroshima en 1945.

El 11 de marzo de 2011 un terremoto magnitud 9.0 en la escala de Richter golpeó Japón.

Tras el sismo se generó una alerta de maremoto (tsunami) para la costa pacífica del Japón y otros países, incluidos Nueva Zelanda, Australia, Rusia, Guam, Filipinas, Indonesia, Papúa Nueva Guinea, Nauru, Hawái, islas Marianas del Norte, Estados Unidos, Taiwán, América Central, México y las costas de América del Sur, especialmente Colombia, Ecuador, Perú y Chile. La alerta de tsunami emitida por el Japón fue la más grave en su escala local de alerta, lo que implica que se esperaba una ola de 10 metros de altura. La agencia de noticias Kyodo informó que un tsunami de 4 m de altura había golpeado la Prefectura de Iwate en Japón. Se observó un tsunami de 10 metros de altura en el aeropuerto de Sendai, en la prefectura de Miyagi, que quedó inundado, con olas que barrieron coches y edificios a medida que se adentraban en tierra.

Se habrían detectado, horas más tarde, alrededor de 105 réplicas del terremoto, una alerta máxima nuclear y 1.000 veces más radiación de lo que producía el Japón mismo debido a los incendios ocasionados en una planta atómica. Se temía más tarde una posible fuga radiactiva.

Finalmente el tsunami azotó las costas de Hawái y toda la costa sudamericana con daños mínimos gracias a los sistemas de alerta temprana liderados por el Centro de Alerta de Tsunamis del Pacífico.

Muchas ciudades alrededor del Pacífico, sobre todo en México, Perú, Japón, Ecuador, Estados Unidos y Chile disponen de sistemas de alarma y planes de evacuación en caso de un maremoto peligroso. Diversos institutos sismológicos de diferentes partes del mundo se dedican a la previsión de maremotos, y la evolución de éstos es monitorizada por satélites. El primer sistema, bastante rudimentario, para alertar de la llegada de un maremoto fue puesto a prueba en Hawái en los años veinte. Posteriormente se desarrollaron sistemas más avanzados debido a los maremotos del 1 de abril de 1946 y el 23 de mayo de 1960, que causaron una gran destrucción en Hilo (Hawái). Los Estados Unidos crearon el Centro de Alerta de Maremotos del Pacífico ("Pacific Tsunami Warning Center") en 1949, que pasó a formar parte de una red mundial de datos y prevención en 1965.

Uno de los sistemas para la prevención de maremotos es el proyecto CREST (Consolidated Reporting of Earthquakes and Seaquakes) (Información Consolidada sobre Terremotos y Maremotos), que es utilizado en la costa noroeste estadounidense (Cascadia), en Alaska y en Hawái por el Servicio Geológico de los Estados Unidos, la "National Oceanic and Atmospheric Administration" (la Administración Nacional Oceánica y Atmosférica de EE. UU.), la red sismográfica del noreste del Pacífico y otras tres redes sísmicas universitarias.

La predicción de maremotos sigue siendo poco precisa. Aunque se puede calcular el epicentro de un gran terremoto subacuático y el tiempo que puede tardar en llegar un maremoto, es casi imposible saber si ha habido grandes movimientos del suelo marino, que son los que producen maremotos. Como resultado de todo esto, es muy común que se produzcan alarmas falsas. Además, ninguno de estos sistemas sirve de protección contra un maremoto imprevisto.
A pesar de todo, los sistemas de alerta no son eficaces en todos los casos. En ocasiones el terremoto generador puede tener su epicentro muy cerca de la costa, por lo que el lapso entre el sismo y la llegada de la ola será muy reducido. En este caso, las consecuencias son devastadoras, debido a que no se cuenta con tiempo suficiente para evacuar la zona y el terremoto por sí mismo ya ha generado una cierta destrucción y caos previo, lo que hace que resulte muy difícil organizar una evacuación ordenada. Éste fue el caso del maremoto del año 2004 pues, aun contando con un sistema adecuado de alerta en el océano Índico, quizá la evacuación no habría sido lo suficientemente rápida.

Como ya se mencionó, los terremotos son la gran causa de los maremotos. Para que un terremoto origine un maremoto, el fondo marino debe ser movido abruptamente en sentido vertical, de modo que el océano es impulsado fuera de su equilibrio normal. Cuando esta inmensa masa de agua trata de recuperar su equilibrio, se generan las olas. El tamaño del maremoto estará determinado por la magnitud de la deformación vertical del fondo marino. No todos los terremotos generan maremotos, sino sólo aquellos de magnitud considerable (primera condición), que ocurren bajo el lecho marino (segunda condición) y que sean capaces de deformarlo (tercera condición). Si bien cualquier océano puede experimentar un maremoto, es más frecuente que ocurran en el océano Pacífico, cuyas márgenes son más comúnmente asiento de terremotos de magnitudes considerables (especialmente las costas de Chile, Perú y Japón). Además, el tipo de falla que ocurre entre las placas de Nazca y placa sudamericana, llamada falla de subducción, esto es, que una placa se va deslizando bajo la otra, hacen más propicia la deformidad del fondo marino y, por ende, el surgimiento de los maremotos.

Un informe publicado por el PNUE sugiere que el tsunami del 26 de diciembre de 2004 provocó menos daños en las zonas en que existían barreras naturales, como los manglares, los arrecifes coralinos o la vegetación costera. Un estudio japonés sobre este tsunami en Sri Lanka estableció, con ayuda de una modelización sobre imágenes satelitales, los parámetros de resistencia costera en función de las diferentes clases de árboles.

Las marejadas se producen habitualmente por la acción del viento sobre la superficie del agua, sus olas suelen presentar una ritmicidad de 20 segundos, y suelen propagarse unos 150 m tierra adentro, como máximo total, tal y como observamos en los temporales o huracanes. De hecho, la propagación se ve limitada por la distancia, de modo que va perdiendo intensidad al alejarnos del lugar donde el viento la está generando.

Un maremoto, en cambio, presenta un comportamiento opuesto, ya que el brusco movimiento del agua desde la profundidad genera un efecto de «latigazo» hacia la superficie, el cual es capaz de lograr olas de magnitud impensable. Los análisis matemáticos indican que la velocidad es igual a la raíz cuadrada del producto del potencial gravitatorio (9,8 m/s²) por la profundidad. Para tener una idea, tomemos la profundidad habitual del océano Pacífico, que es de 4000 m. Esto daría una ola que podría moverse a unos 200 m/s, o sea, a 700 km/h. Y, como las olas pierden su fuerza en relación inversa a su tamaño, al tener 4000 m puede viajar a miles de kilómetros de distancia sin perder mucha fuerza.

Sólo cuando llegan a la costa comienzan a perder velocidad, al disminuir la profundidad del océano. La altura de las olas, sin embargo, puede incrementarse hasta superar los 30 metros (lo habitual es una altura de 6 o 7 m). Los maremotos son olas que, al llegar a la costa, no rompen. Al contrario, un maremoto sólo se manifiesta por una subida y bajada del nivel del mar de las dimensiones indicadas. Su efecto destructivo radica en la importantísima movilización de agua y las corrientes que ello conlleva, haciendo en la práctica un río de toda la costa, además de las olas 'normales' que siguen propagándose encima del maremoto y arrasando, a su paso, con lo poco que haya podido resistir la corriente.

Las fallas presentes en las costas del océano Pacífico, donde las placas tectónicas se introducen bruscamente bajo la placa continental, provocan un fenómeno llamado subducción, lo que genera maremotos con frecuencia. Derrumbes y erupciones volcánicas submarinas pueden provocar fenómenos similares.

La energía de los maremotos se mantiene más o menos constante durante su desplazamiento, de modo que, al llegar a zonas de menor profundidad, por haber menos agua que desplazar, la altura del tsunami se incrementa de manera formidable. Un maremoto que mar adentro se sintió como una ola no perceptible, debido a su larga longitud de onda puede, al llegar a la costa, destruir hasta kilómetros tierra adentro. Las turbulencias que produce en el fondo del mar arrastran rocas y arena, lo que provoca daño erosivo en las playas que puede alterar la geografía durante muchos años.

Japón, por su ubicación geográfica, es el país más golpeado por los maremotos.





</doc>
<doc id="8222" url="https://es.wikipedia.org/wiki?curid=8222" title="Petrolero">
Petrolero

Un petrolero es un tipo de buque cisterna diseñado específicamente para el transporte de crudo o productos derivados del petróleo. Actualmente todos los petroleros en construcción son del tipo de doble casco, en detrimento de los más antiguos diseños de un solo casco (monocasco), debido a que son menos sensibles a sufrir daños y provocar vertidos en accidentes de colisión con otros buques o encallamiento.

A partir de este tipo de barcos, surgió el superpetrolero, de mayor capacidad de carga y destinado al transporte de crudo desde Medio Oriente alrededor del Cuerno de África. El superpetrolero "Knock Nevis" es la embarcación más grande del mundo.

Además del transporte por oleoducto, los petroleros son el único medio de transportar grandes cantidades de crudo, a pesar de que algunos han provocado considerables desastres ecológicos al hundirse cerca de la costa provocando el vertido de su carga al mar. Los desastres más famosos han sido los causados por los petroleros "Torrey Canyon", "Exxon Valdez", "Amoco Cadiz", "Erika", "Prestige", "Mar Egeo", "Urquiola", ...

Los buques petroleros, gaseros y cargueros se clasifican según su capacidad de carga en:




</doc>
<doc id="8223" url="https://es.wikipedia.org/wiki?curid=8223" title="Monocasco">
Monocasco

Se denomina monocasco a cierto tipo de chasis de vehículos construidos de una sola pieza como así también a las embarcaciones cuyos cascos tienen una sola pared. El vocablo monocasco, derivado de la palabra francesa «monocoque», significa «un solo caparazón».

Son los buques que no poseen una doble barrera de separación a lo largo de toda la eslora de carga entre los tanques de carga (p.e. tanques de crudo) y el mar, a diferencia de los más modernos diseños de doble casco, en la marina mercante.

El uso de petroleros monocasco está prohibido en toda la UE, debido a que son más sensibles a sufrir daños y provocar vertidos en accidentes de colisión con otros buques o embarrancamiento. Esta medida se adoptó tras el hundimiento del Prestige.

Respecto a la resistencia global de diseño, los parámetros son similares a los de los buques de doble casco.

En la navegación de recreo se llama monocasco a las embarcaciones de un solo casco y una sola quilla, a diferencia de los multicasco, como catamaranes y trimaranes.

Se denominan monocasco (o «carrocería autoportante») a las carrocerías de los vehículos que incluyen el chasis y el habitáculo de componentes y de pasajeros en una sola pieza con punteras que sirven de soporte al motor. Este sistema es el usado en casi la totalidad de los turismos desde los años 1980.
El primer automóvil en incorporar esta técnica constructiva fue el Lancia Lambda, de 1923. Luego otros de gran serie fueron el Chrysler Airflow y el Citroën Traction Avant. 

Tradicionalmente la carrocería se montaba sobre el chasis de bastidores, actualmente esta práctica solo es usada en los vehículos que tengan que desplazar grandes cargas como camionetas pickup/camiones y en algunos vehículos deportivos utilitarios.
Los últimos coches con chasis independiente sobre bastidores fueron norteamericanos, en especial el Ford Crown Victoria hasta el 2011 y modelos del Chevrolet Caprice hasta 1996.
Otros vehículos utilizan una sistema mixto, en la cual un chasis «semimonocasco» se combina con un chasis de bastidor parcial (subchasis) que soporta el motor, el puente delantero y la transmisión, ejemplos de esta técnica son el Chevrolet Camaro, Opel Vectra y varios superdeportivos como el Lamborghini Aventador LP700-4.

Hoy en día casi todos los automóviles se construyen con la técnica de monocasco, realizándose las uniones entre las distintas piezas mediante soldadura de punto. Existen vehículos en los cuales hasta los cristales forman parte de sus estructuras, brindando fortaleza y rigidez a todo el conjunto.



</doc>
<doc id="8225" url="https://es.wikipedia.org/wiki?curid=8225" title="Robert Koch">
Robert Koch

Heinrich Hermann Robert Koch (Clausthal, Reino de Hannover, 11 de diciembre de 1843-Baden-Baden, Gran Ducado de Baden, Imperio alemán, 27 de mayo de 1910) fue un médico y microbiólogo alemán.

Se hizo famoso por descubrir el bacilo de la tuberculosis en 1882, presentando sus hallazgos el 24 de marzo de 1882, así como el bacilo del cólera en 1883 y por el desarrollo de los postulados de Koch. Recibió el Premio Nobel de Medicina en 1905. Es considerado el fundador de la bacteriología.

El trabajo de Koch consistió en aislar el microorganismo causante de esta enfermedad y hacerlo crecer en un cultivo puro, utilizando este cultivo para inducir la enfermedad en animales de laboratorio, en su caso la cobaya, aislando de nuevo el germen de los animales enfermos para verificar su identidad comparándolo con el germen original.
Recibió el Premio Nobel de Fisiología y Medicina en 1905 por sus trabajos sobre la tuberculosis.

Probablemente tan importante como su trabajo en la tuberculosis sean los llamados "Postulados de Koch" que establecen las condiciones para que un organismo sea considerado la causa de una enfermedad.

Robert Koch nació en Clausthal en las montañas del Harz, entonces parte del reino de Hannover, como hijo de un minero. Luego de la Guerra austro-prusiana, en 1866, esa ciudad sería parte de Prusia. Estudió medicina bajo la tutela de Friedrich Gustav Jakob Henle en la Universidad de Gotinga y se graduó en 1866. Entonces sirvió en la Guerra Franco-Prusiana y posteriormente se convirtió en médico oficial del distrito en Wollstein (Wolsztyn), la Prusia polaca. Trabajando con muy pocos recursos, llegó a ser uno de los fundadores de la bacteriología junto con Louis Pasteur.

Después de que Casimir Davaine demostrara la transmisión directa del bacilo del carbunco (también llamado ántrax) entre las vacas, Koch estudió con profundidad esta enfermedad. Inventó métodos para extraer el bacilo de las muestras de sangre y hacerlo crecer en cultivos puros. Descubrió que, mientras que era incapaz de sobrevivir durante periodos largos en el exterior del huésped, podía crear endosporas que sí podían hacerlo. También descubrió el agente causante de la enfermedad del carbunco. 

Esas endosporas, incrustadas en el suelo, eran la causa de los inexplicables brotes "espontáneos" de ántrax. Koch publicó sus descubrimientos en 1876 y fue premiado con un trabajo en la Oficina Imperial de Sanidad en Berlín en 1880. En 1881, promovió la esterilización de los instrumentos quirúrgicos mediante el calor.

En Berlín mejoró los métodos que había usado en Wollstein, incluyendo las técnicas de tinción y purificación y los medios de crecimiento bacteriano, como las placas de agar (gracias al consejo de Angelina y Walther Hesse) y la placa de Petri (llamada así por su inventor, su ayudante Julius Richard Petri); estos dispositivos aún se utilizan actualmente. Con estas técnicas, fue capaz de descubrir la bacteria causante de la tuberculosis ("Mycobacterium tuberculosis") en 1882 (anunció el descubrimiento el 24 de marzo). La tuberculosis era la causa de una de cada siete muertes a mitad del siglo XIX.

En 1883, Koch trabajó en un equipo de investigación francés en Alejandría, Egipto, estudiando el cólera. También trabajó en la India, donde aisló e identificó la bacteria vibrio que causaba el cólera. La bacteria había sido aislada previamente por el anatomista italiano Filippo Pacini en 1854, aunque su trabajo había sido ignorado por la presencia de la teoría miasmática de la enfermedad. Koch desconocía el trabajo de Pacini e hizo su descubrimiento independientemente, y su gran preeminencia permitió que el descubrimiento fuera difundido más ampliamente para el beneficio general. Sin embargo, en 1965 la bacteria fue renombrada "Vibrio cholerae (Pacini 1854)".

En 1885, fue nombrado profesor de higiene en la Universidad de Berlín, y en 1891 se convirtió en Profesor Honorario de la Facultad de Medicina y director del Instituto Prusiano de Enfermedades Infecciosas (renombrado como Instituto Robert Koch en su honor), una posición a la que renunció en 1904. Comenzó a viajar por todo el mundo, estudiando enfermedades de Sudáfrica, India y Java. Visitó en Mukteshwar lo que ahora se llama Instituto de Investigación Veterinaria India (IVRI, Indian Veterinary Research Institute en inglés), a petición del Gobierno de la India para investigar una plaga en el ganado. El microscopio que usó durante este periodo se conserva en el museo mantenido por el IVRI.

Probablemente tan importante como su trabajo en la tuberculosis, por el que fue galardonado con el Premio Nobel en 1905, son los postulados de Koch, que afirman que para establecer que un organismo sea la causa de una enfermedad, este debe:

Los pupilos de Koch descubrieron los organismos responsables de la difteria, el tifus, la neumonía, la gonorrea, la meningitis cerebroespinal, la lepra, la peste pulmonar, el tétanos y la sífilis, entre otros, usando sus métodos.

Murió el 27 de mayo de 1910 por un ataque al corazón en Baden-Baden, a la edad de 66 años.



</doc>
<doc id="8226" url="https://es.wikipedia.org/wiki?curid=8226" title="Iván Pávlov">
Iván Pávlov

Iván Petróvich Pávlov (; Riazán, -Leningrado, 27 de febrero de 1936) fue un fisiólogo y psicólogo ruso.

Era hijo de Piotr Pávlov (1823-1899), patriarca ortodoxo, y Varvára Uspénskaya (1826-1890). Comenzó a estudiar teología, pero la dejó para empezar medicina y química en la Universidad de San Petersburgo, siendo su principal maestro Vladímir Béjterev. Tras terminar el doctorado en 1883, amplió sus estudios en Alemania, donde se especializó en fisiología intestinal y en el funcionamiento del sistema circulatorio, bajo la dirección de Ludwid y Haidenhein.

En 1890 obtuvo la plaza de profesor de fisiología en la Academia Médica Imperial y fue nombrado director del Departamento de Fisiología del Instituto de Medicina Experimental de San Petersburgo. En la siguiente década centró su trabajo en la investigación del aparato digestivo y el estudio de los jugos gástricos, El científico dedicó más de 10 años a aprender a hacer orificios en el tracto intestinal. Era una operación muy complicada, ya que el jugo gástrico al salir del intestino corroía los tejidos de éste y los de la pared abdominal. La técnica de Pávlov se basaba en introducir un tubo metálico por una pequeña incisura. Era imprescindible una sutura habilidosa de la piel y de la membrana mucosa y cerrar la salida de la cánula con un tapón. De esta manera pudo obtener jugo gástrico de cualquier parte del tracto intestinal, desde las glándulas salivales hasta el intestino grueso, trabajos por los que ganó el premio Nobel de Fisiología o Medicina en 1904, convirtiéndose así en el primer ruso que recibió esta distinción, Los resultados de las investigaciones de Pávlov fueron publicadas en 1897 en el libro "The Work of the Digestive Glands".

Pávlov es conocido sobre todo por formular la ley del reflejo condicional que, por un error en la traducción de su obra al idioma inglés, fue llamada «reflejo condicionado», la cual desarrolló a partir de 1901 con su asistente Iván Filíppovich Tolochínov, al tiempo que en EE. UU. Edwin Burket Twitmyer realizaba observaciones similares. Pávlov observó que la salivación de los perros que utilizaban en sus experimentos se producía ante la presencia de comida o de los propios experimentadores, y luego determinó que podía ser resultado de una actividad psicológica, a la que llamó «reflejo condicional». Esta diferencia entre «condicionado» y «condicional» es importante, pues el término «condicionado» se refiere a un estado, mientras que el término «condicional» se refiere a una relación, que es precisamente el objeto de su investigación.

Realizó el conocido experimento consistente en hacer sonar un metrónomo (a 100 golpes por minuto, aunque popularmente se cree que empleó una campana) justo antes de dar alimento en polvo a un perro, llegando a la conclusión de que, cuando el perro tenía hambre, comenzaba a salivar nada más al oír el sonido del metrónomo (aparato que en ocasiones usan los músicos para marcar el ritmo). Tolochinov, que llamó al fenómeno «reflejo a distancia», comunicó los primeros resultados en el Congreso de Ciencias Naturales en Helsinki en 1903. Posteriormente ese mismo año, Pávlov realizó una exposición detallada de los resultados en el 14º Congreso Médico Internacional en Madrid, donde leyó su trabajo bajo el título "The Experimental Psychology and Psychopathology of Animals".

La Guerra civil rusa y la llegada de los bolcheviques no influyeron en sus investigaciones. A pesar de no sentir simpatía por el nuevo régimen, no sufrió represalias por parte de los comunistas. Después de la Revolución de Octubre fue nombrado director de los laboratorios de fisiología en el Instituto de Medicina Experimental de la Academia de Ciencias de la URSS. En cierta ocasión llegó a declarar: «Por este experimento social que están realizando, yo no sacrificaría los cuartos traseros de una rana.» No hay evidencia de que se haya involucrado en la Revolución de Octubre ni, en general, en el movimiento comunista.

En la década de 1930 volvió a destacarse al anunciar el principio según el cual la función del lenguaje humano es resultado de una cadena de reflejos condicionales que contendrían palabras.

La fundación del conductismo como tal ha sido criticada por algunos filósofos y psicólogos al considerarla una escuela de la Psicología que se centra en la interacción entre el comportamiento y el ambiente, y cómo se puede aprender.

En agosto de 1935 la Unión Soviética celebró el Congreso Mundial de Fisiología en Moscú y Leningrado con la asistencia de más de 900 científicos del mundo. Iván Pávlov fue nombrado como el fisiólogo más importante del mundo. Pávlov clausuró las jornadas con un emotivo discurso: "Mi vida entera se compone de experimentos, nuestro gobierno también experimenta, solo que a más alto nivel".

El 27 de febrero de 1936 Iván Pávlov murió de neumonía. Está enterrado en San Petersburgo.

Las observaciones originales de Pávlov eran simples. Si se ponen alimentos o ciertos ácidos diluidos en el hocico de un perro hambriento, éste empieza a segregar un flujo de saliva procedente de determinadas glándulas. Este es el reflejo de salivación, pero eso no es todo. Pávlov observó que el animal también salivaba cuando la comida todavía no había llegado al hocico: la comida simplemente vista u olida provocaba una respuesta semejante. Además, el perro salivaba ante la mera presencia de la persona que por lo general le acercaba la comida o cualquier otro estímulo que sistemáticamente la anunciara. Esto llevó a Pávlov a desarrollar un método experimental para estudiar la adquisición de nuevas conexiones de estímulo-respuesta. Indudablemente, las que había observado en sus perros no podían ser innatas o connaturales de esta clase de animal, por lo que concluyó que debían ser aprendidas (en sus términos, condicionales). El primer paso, cuando se realiza este experimento, es familiarizar al perro con la situación experimental que va a vivir, hasta que no dé muestras de alteración, sobre todo cuando se le coloca el arnés y se lo deja solo en una sala aislada. Se practica una pequeña abertura o fisura en la quijada del perro, junto al conducto de una de las glándulas salivares. Luego, se le coloca un tubito (cánula) de cristal para que salga por él la saliva en el momento en que se activa la glándula salivar. La saliva va a parar a un recipiente de cristal con marcas de graduación, para facilitar su cuantificación.

Uno de sus textos fundamentales, "Reflejos condicionados", se publicó en español en 1929 (Javier Morata, Madrid) con prólogo de Gregorio Marañón y unas palabras del propio autor para la edición española. En 1997 apareció una nueva edición de este texto (Editorial Morata, Madrid).

La magnitud de las respuestas a los diferentes estímulos puede medirse por el volumen total o el número de gotas segregadas en una determinada unidad de tiempo. Desde la habitación contigua, y a través de un cristal, el experimentador puede observar el comportamiento del perro, aplicando los estímulos y valorando las respuestas. Antes de empezar el experimento, Pávlov midió las reacciones de salivación a la comida en el hocico, que fue considerable, mientras que salivó muy poco sometido al estímulo del sonido. A continuación, inició las pruebas de condicionamiento. Hizo sonar el metrónomo (estímulo neutral), e inmediatamente después presentó comida al animal (estímulo incondicional), con un intervalo muy breve. Repitió la relación entre este par de estímulos muchas veces durante varias semanas, siempre cuando el perro estaba hambriento. Después, transcurridos varios días, hizo sonar solamente el metrónomo y la respuesta salival apareció al oírse el sonido, a pesar de que no se presentó la comida.

Se había establecido una relación condicional entre la respuesta de salivar y el sonido que originalmente no provocaba la salivación. Se dice entonces que la salivación del perro ante la comida es una respuesta incondicional; la salivación tras oír la campana es una respuesta condicional que depende de la relación que en la historia del sujeto ha existido entre el sonido y la comida. El estímulo del sonido del metrónomo que originalmente era neutro funciona ahora como un estímulo condicional. Este estímulo condicional (sonido), funciona para el sujeto con esa historia como una señal que avisa que el estímulo incondicional (comida), está a punto de aparecer.

Finalmente, se llamó refuerzo al fortalecimiento de la asociación entre un estímulo incondicional con el condicional. El reforzamiento es un acontecimiento que incrementa la probabilidad de que ocurra una determinada respuesta ante ciertos estímulos. La definición de condicionamiento clásico o respondiente es la formación (o reforzamiento) de una asociación entre un estímulo originalmente neutro y una respuesta (por lo general un reflejo o una secreción glandular, como en el caso de la salivación).Los principios del condicionamiento respondiente se utilizan, entre otros, para la adquisición de hábitos como el control de esfínteres. Los estímulos pueden clasificarse en sensoriales, propioceptivos y verbal.

Así denominó a la relación por la cual en el sistema nervioso central, en especial en el cerebro se establece una asociación, por ejemplo, entre un sonido, con el posible alimento: el sonido (u otro estímulo sustitutivo) funciona como una señal. Pávlov consideró que la mayoría de los animales se rige por un «pensamiento» basado en este sistema de sustituciones reflejas, un primer sistema de señales.

Pero, a diferencia de otros autores, Pávlov consideró que muchos «comportamientos humanos» son más complejos que un sistema de reflejos condicionales simples en un modelo «estímulo/respuesta» lineal. En el "Homo sapiens", Pávlov consideró que se produce un salto cualitativo respecto al primer sistema de señales; en el humano la cuestión ya no se restringe solamente a reflejos condicionales o a estímulos que funcionan de manera sustitutiva directa de la realidad. La complejidad de las funciones psicológicas humanas facilita un segundo sistema de señales que es el lenguaje verbal o simbólico. En éste las sustituciones a partir de los estímulos parecen ser infinitas y, sin embargo, altamente ordenadas (lógicas). En gran medida Pávlov postula tal capacidad del segundo sistema de señales porque considera que en el ser humano existe una capacidad de autocondicionamiento (aprendizaje dirigido por uno mismo) que, aunque parezca contradictorio, le es liberador: el ser humano puede reaccionar ante estímulos que él mismo va generando y que puede transmitir (ver información).

La psicología preeminentemente experimental de Pávlov y sus epígonos se denomina reflexología, lo que lleva a confusión a algunas personas, que la confunden con la reflexogenoterapia, una forma de terapia a veces llamada «reflexología».

Pávlov ha influido en su país, durante el siglo XX, de un modo determinante sobre otros importantes investigadores de la Psicología: Lúriya, Leóntiev, Vygotski, Béjterev, Shaunyán, etc. Fuera de Rusia, Watson incorporó a su propia obra la terminología y conceptos pavlovianos. Algunas de las partes de la obra de Pávlov, que por lo general han permanecido ignoradas consistieron en las variaciones sistemáticas que introdujo en sus experimentos.

Por ejemplo, mostró que el intervalo óptimo entre la presentación del estímulo condicional y el incondicional para favorecer el aprendizaje (es decir, la presentación de una respuesta condicional) es de 0.5 segundos. Intervalos mayores o menores entre los estímulos requerían mayor cantidad de ensayos para que se diera el aprendizaje, y con frecuencia las respuestas son más débiles.

De manera semejante, mostró que el orden en la secuencia de presentación de los estímulos era crucial. Si intentaba lograr el establecimiento de nuevas relaciones condicionales presentando primero el estímulo incondicional y luego el neutro (al cual se intentaba que funcionara como condicional), el aprendizaje no ocurría.

Mostró asimismo que no todas las relaciones entre estímulos generaban nuevas respuestas, pues en caso de reflejos, como el rotuliano (estirar la pierna ante un ligero golpe en cierta región de la rodilla), no se aprendía a responder ante los estímulos que «anunciaban» el golpe (Millenson, 1974).

Pávlov también estudió fenómenos como la "generalización", es decir, la presentación de respuestas condicionales ante estímulos parecidos al estímulo condicional original. Descubrió que, a diferencia de los reflejos incondicionales (no aprendidos), la magnitud de la respuesta no era directamente proporcional a la intensidad de los estímulos (es decir, a mayor intensidad del estímulo, dentro de ciertos límites, se presenta una mayor magnitud en la respuesta), sino que en el caso de las relaciones condicionales, la mayor magnitud en la respuesta depende de qué tanto se parezca el estímulo que se presenta respecto al estímulo condicional original. Esto da lugar a una graduación (a veces llamada gradiente), de modo que estímulos ligeramente de menor o mayor intensidad respecto al estímulo condicional original dan lugar a respuestas condicionales de mayor magnitud que las que se presentan ante estímulos de mayor intensidad que el estímulo condicional, aunque la mayor magnitud de la respuesta condicional siempre se da ante el estímulo condicional original (Millenson, 1974).

Por otra parte, Pávlov estudió igualmente la «discriminación de estímulos», esto es, que tanto el sujeto aprende a comportarse de manera diferente ante estímulos distintos que anuncian a otros estímulos. En uno de los ejemplos más conocidos, logró que sus sujetos salivaran ante círculos que anunciaban la presencia de comida y se comportaran de la manera típica de su especie ante estímulos aversivos, tales como descargas eléctricas, en presencia de elipses. Es decir, los perros brincaban, aullaban, se tensaban, etc., ante elipses, pero salivaban ante círculos, si en su historia, cada uno de esos estímulos se presentaba consistentemente como «anuncio» de los estímulos incondicionales correspondientes (choques eléctricos ante las elipses y comida ante los círculos) (Millenson, 1974).

Pávlov estudió muchos otros aprendizajes, tanto en animales como en seres humanos, incluyendo lo que se denominó la inducción de «neurosis experimental», y prácticamente fundó el estudio experimental del comportamiento considerado «anormal» o «psicopatológico», así como su contraparte para modificar varios comportamientos indeseables, incluyendo fobias, tics y comportamientos «neuróticos», de manera que los sujetos aprendieran comportamientos adaptables y eliminaran la ansiedad y otras reacciones indeseables (Sandler y Davidson, 1980).
Pávlov es un ejemplo de que los grandes descubrimientos científicos con frecuencia incluyen una combinación de eventos «accidentales» y una observación de los mismos por personas con suficiente preparación como para no considerarlos como fallas o excepciones, sino como objetos de interés por sí mismos, los cuales son función de su relación con una o más variables independientes.

Uno de estos casos, de acuerdo con Sandler y Davidson (1980), ocurrió cuando una fuerte inundación puso en peligro la integridad de los perros con los que Pávlov experimentaba, pues el sótano en el que se encontraban sus jaulas comenzó a llenarse de agua. Pávlov y algunos de sus ayudantes fueron al laboratorio a pesar de las condiciones ambientales y pusieron a salvo a los perros. El hecho pudo no haber trascendido, pero ocurrió que, cuando se intentó reinstalar a los perros en el sótano, varios aspectos de su comportamiento presentaron variaciones «extrañas». Aunque antes se habían comportado de manera dócil ante los investigadores, ahora eran hostiles; además, dejaron de comer con regularidad, se aislaron, dejaron de tener relaciones sexuales y con frecuencia aullaban como si hubiera otros perros o personas, aunque no estuvieran ahí. Este comportamiento se podría considerar como «neurótico». Por otra parte, dicho comportamiento se aminoraba cuando los perros eran trasladados a ambientes muy diferentes al del sótano. Pávlov razonó, en sus términos, que la presencia intempestiva e intensa de fuertes estímulos aversivos había ocasionado un condicionamiento ante los estímulos que estaban presentes en el sótano.

Después de reflexionar sobre esto, instauró una manera sistemática para revertir los efectos de ese condicionamiento. Empezó dejando a los perros en un ambiente bastante diferente al del sótano y, cuando los perros se comportaron de manera «normal», comenzó a sustituir de manera cuidadosa y gradual distintos estímulos del nuevo ambiente (desvanecimiento por sustracción) por otros que habían estado presentes en el sótano (desvanecimiento por adición). Al final, los perros pudieron regresar al sótano, mientras su comportamiento permaneció completamente «normal».

Pávlov también notó que podía inducir comportamientos «neuróticos» al presentar discriminaciones muy difíciles. En el caso mencionado del círculo (ante el cual se presentaba comida) y la elipse (ante la que se presentaba una descarga eléctrica), los sujetos se comportaban de manera apropiada ante cada uno, después de una serie de ensayos (digamos, por ejemplo, 50 ensayos). Sin embargo, cuando el círculo y la elipse se hicieron cada vez más semejantes, llegó un punto en el cual los sujetos se comportaban de manera semejante a la de los perros que habían sufrido la experiencia aversiva en el sótano. Pero al restablecer las condiciones originales respecto al círculo y la elipse, los sujetos volvieron a comportarse gradualmente del modo adecuado ante cada uno, aunque el número de ensayos requeridos era aproximadamente el doble que el original (digamos, 100 ensayos). A medida que los sujetos discriminaron adecuadamente el círculo de la elipse, su comportamiento fuera de la situación experimental también cambió de «neurótico» a «normal».

El razonamiento de Pávlov fue del tipo: si se pudo inducir un comportamiento neurótico bajo ciertas condiciones (neurosis experimental), también se puede modificar si se cambian las variables independientes de las cuales es función. Pávlov de esta manera inauguró lo que se puede considerar la modificación experimental del comportamiento en Rusia.

Tanto el estudio científico del comportamiento «anormal», como su modificación, fueron influidos de manera notable por el tipo de hallazgos y razonamientos de Pávlov.






</doc>
<doc id="8227" url="https://es.wikipedia.org/wiki?curid=8227" title="Emil Adolf von Behring">
Emil Adolf von Behring

Emil Adolf von Behring (n. Hansdorf, Prusia Oriental, 15 de marzo de 1854 - Marburgo, Alemania, 31 de marzo de 1917) fue un bacteriólogo alemán que recibió el primer Premio Nobel en Fisiología o Medicina en 1901.

Emil Adolf von Behring nació el 15 de marzo de 1854 en Hansdorf, localidad de la antigua Prusia Oriental, que actualmente corresponde con Ławice en Polonia. Fue el mayor de 13 hijos del segundo matrimonio de un profesor de colegio.

Ingresó en la Academia de Medicina Militar en Berlín en 1874, obteniendo la licenciatura en 1878 y aprobando el examen estatal en 1880. Trabajó como cirujano militar. Realizó prácticas de trabajo sobre los problemas relacionados con las enfermedades contagiosas. En 1889 abandonó el ejército para ingresar como ayudante de Robert Koch en el Instituto de Higiene de la Universidad de Berlín.

En 1891 se trasladó al Instituto de Enfermedades Infecciosas, dirigido por el mismo Koch. Fue catedrático de la Universidad de Halle en 1894 y, en 1895, director del Instituto de Higiene de Marburgo, hasta su fallecimiento el 31 de marzo de 1917.

En 1890 descubrió la antitoxina del tétanos junto con el bacteriólogo japonés Shibasaburo Kitasato. Descubrieron que al inyectar el suero sanguíneo de un animal afectado por el tétanos a otro, se genera inmunidad a la enfermedad en el segundo. Comprobaron que los animales inmunizados contra el tétanos presentaban esta cualidad porque debían disponer de alguna sustancia capaz de controlar la infección. Analizando la sangre de cuyes inmunizados contra el tétanos, comprobó que al inyectar el suero de estos animales en otros no inmunizados se podían conseguir buenos resultados terapéuticos.

En 1891 trataron con suero a una niña enferma de difteria salvando su vida. Esto le hizo sospechar a Behring la existencia de unas sustancias (que llamó antitoxinas) que eliminaban las toxinas segregadas por las bacterias, lo que supuso un gran avance en el conocimiento de las defensas corporales.

Poco después hizo públicos los resultados de su trabajo sobre la aplicación del suero contra la difteria, en el que demostraba que el poder de resistencia a la enfermedad no reside en las células del cuerpo, sino en el suero sanguíneo libre de células. Por este trabajo obtuvo el primer Premio Nobel de Fisiología y Medicina en 1901.

En el caso del tétanos y la difteria, von Behring provocaba la inmunidad con el suero de un animal previamente infectado. Tras nuevos trabajos en Marburgo con otras antitoxinas, introdujo en 1913 un sistema de inoculación, todavía en vigor, capaz de inmunizar a los niños contra la difteria.




</doc>
<doc id="8228" url="https://es.wikipedia.org/wiki?curid=8228" title="Ronald Ross">
Ronald Ross

Ronald Ross (Almora, India, 13 de mayo de 1857 - Londres, 16 de septiembre de 1932) fue un naturalista, médico, matemático, zoólogo, entomólogo escocés, quien relacionó la malaria con los mosquitos.

Estudió medicina en el Hospital de St. Bartholomew de Londres. Médico militar en 1881, once años después comenzó a investigar la transmisión y el control de la malaria. Mientras dirigía una expedición por África Occidental en 1889, identificó la presencia de mosquitos portadores de la enfermedad y organizó su exterminio a gran escala.

En 1895 Ross puso en marcha una serie de experimentos que demostraron que la malaria es transmitida por mosquitos; descubrió también el ciclo vital del parásito de la malaria en el mosquito Anopheles. Por este descubrimiento fue galardonado en 1902 con el Premio Nobel de Fisiología y Medicina.

En 1913 fue nombrado médico de enfermedades tropicales del King's College Hospital, en Londres. Poco después fue nombrado director jefe del Instituto y Hospital para Enfermedades Tropicales Ross de Londres. Ross fue elegido miembro de la Royal Society en 1901 y nombrado sir en 1911.

También publicó poemas, novelas y estudios matemáticos.





























</doc>
<doc id="8230" url="https://es.wikipedia.org/wiki?curid=8230" title="Camillo Golgi">
Camillo Golgi

Bartolomeo Camillo Emilio Golgi (Corteno Golgi, Italia, 7 de julio de 1843 - Pavía, 21 de enero de 1926) fue un médico y citólogo italiano. Ideó los métodos de tinción celular a base de cromato de plata, procedimiento que permitió (tanto a él mismo como a otros investigadores) realizar importantes descubrimientos, especialmente acerca de las neuronas y su fisiología. Recibió el Premio Nobel de Medicina (conjuntamente con el español Santiago Ramón y Cajal) en 1906.

Estudió medicina en la Universidad de Pavía, donde se graduó en 1865. Trabajó algún tiempo en la clínica psiquiátrica del criminólogo Cesare Lombroso, pero pronto se interesó por la histología. En 1872 comenzó a trabajar en el pabellón de incurables de un hospital de Abbiategrasso. Ejerció como profesor de anatomía en las Universidades de Turín y Siena y como catedrático de histología en la Universidad de Pavía, de la que llegó a ser decano de la Facultad de Medicina y rector.

A pesar de los escasos medios con que contaba, logró importantes resultados con sus experimentos, entre los que destaca el método de la tintura mediante cromato de plata, que supuso una revolución en el estudio en laboratorio de los tejidos nerviosos. Empleando este método, identificó una clase de célula nerviosa dotada de unas extensiones (o dendritas) mediante las cuales se conectan entre sí otras células nerviosas. Este descubrimiento permitió a Wilhelm von Waldeyer-Hartz formular la hipótesis de que las células nerviosas son las unidades estructurales básicas del sistema nervioso, hipótesis que más tarde demostraría Santiago Ramón y Cajal, quien desarrolló la teoría neuronal.

En 1876, tras su regreso a la Universidad de Pavía, continuó el examen de las células nerviosas, y obtuvo pruebas de la existencia de una red irregular de fibrillas, cavidades y gránulos, que en su honor en adelante se denominaría aparato de Golgi y que desempeña un papel esencial en operaciones celulares como la construcción de la membrana, el almacenamiento de lípidos y proteínas o el transporte de partículas a lo largo de la membrana plasmática.

Entre 1885 y 1893 dedicó sus investigaciones al estudio del paludismo, y llegó a resultados tan importantes como la distinción entre el paludismo terciano y cuartano, en cuanto patologías provocadas por dos especies diferentes de un mismo protozoo parásito denominado Plasmodium, así como la identificación del acceso febril como originado por la liberación por parte de dicho organismo de esporas en el flujo sanguíneo.

En 1906 Golgi recibió el Premio Nobel de Medicina conjuntamente con Santiago Ramón y Cajal (1852-1934) por sus estudios sobre la estructura del sistema nervioso.

Murió en Pavía, Italia, en enero de 1926.

En Pavia existen varios lugares que rinden homenaje a Golgi.

Llevan el nombre de Golgi en su memoria:




</doc>
<doc id="8232" url="https://es.wikipedia.org/wiki?curid=8232" title="Derrame pleural">
Derrame pleural

El derrame pleural es una acumulación patológica de materia prima en el espacio pleural. También se le conoce como pleuresía o síndrome de interposición líquida. Es una enfermedad frecuente con más de 50 causas reconocidas incluyendo enfermedades locales de la pleura, del pulmón subyacente, enfermedades sistémicas, disfunción de órganos y fármacos.

En condiciones anatómicas y fisiológicas, existe una escasa cantidad de líquido pleural de no más 10-15 ml en cada hemitórax, que lubrica y facilita el desplazamiento de las dos hojas pleurales que delimitan la cavidad pleural. Existe un trasiego fisiológico de líquido que se filtra, pero cuando hay un desequilibrio entre la formación y la reabsorción se produce el derrame pleural.
Tanto la pleural visceral como la parietal tienen irrigación sanguínea dependiente de la circulación sistémica, pero difieren en el retorno venoso. Los capilares de la pleura visceral drenan en las venas pulmonares mientras que los de la parietal lo hacen en la vena cava.

El líquido puede tener dos orígenes distintos, puede ser el resultado de un exudado o de un trasudado.

El trasudado se da en casos de insuficiencia cardíaca congestiva (ICC) en un 40-72%, mientras que el exudado es más frecuente en cuadros paraneumónicos (50-70%), neoplasias (42-60%) y tuberculosis (23,5%).

Se dan principalmente en la Insuficiencia cardíaca ICC, el 80% son derrames bilaterales. Otras causas son la cirrosis hepática, la insuficiencia renal crónica, el síndrome nefrótico, la diálisis peritoneal.

El líquido del hidrotórax tiene un pH neutro, se caracteriza por tener menor densidad y menor concentración de proteínas < 3gr/dl), mientras que aumentan las LDH.

Las efusiones trasudativas y exudativas de la pleura se diferencian comparando la bioquímica del líquido pleural de aquellos de la sangre. De acuerdo al meta-análisis, dichas efusiones exudativas pleurales necesitan al menos uno de los siguientes criterios:

Su clínica dependerá en gran medida de la enfermedad de base. Habitualmente cursan con disnea, ortopnea, nicturia y edemas maleolares. Sus signos más característicos son la matidez hídrica a la percusión y la disminución tanto del murmullo vesicular como de las vibraciones vocales.

Las causas que producen el exudado pueden tener diversos orígenes:

En su clínica, predominan dolor, tos, disnea, cianosis, fiebre y arritmias. Sus signos son similares a los del hidrotórax, disminución de las vibraciones vocales y del murmullo vesicular y matidez.

Aunque es fundamental la historia clínica y examen físico en el cual los pacientes presentan datos sugestivos del derrame como: disminución de los movimientos respiratorios del lado afectado, disminución de vibraciones vocales, matidez a la percusión, así como disminución o ausencia de los ruidos respiratorios, es de gran relevancia también la toracocentesis, que permite analizar el líquido, realizando citología, antibiogramas y la bioquímica. La broncoscopia, puede aportar también información. Si el diagnóstico no es concluyente, se puede recurrir a la realización de una biopsia transparietal o por toracoscopía, e incluso la toracotomía.

Se indica en derrames pleurales de etiología desconocida y con más de 1 cm hasta la pared. Su realización nos permite analizar el líquido:

El derrame se hace visible en la radiografía cuando es mayor de 75 ml, puede aparecer libre o loculado. En caso de que existan dudas, es recomendable la realización de una radiografía en decúbito lateral del lado afecto. El derrame pleural puede presentar imágenes radiológicas atípicas como:

En algunos casos la ecografía de tórax puede proporcionar más información, siendo su mayor utilidad la detección de anormalidades subpulmonares y subfrénicas. También se utiliza para guiar la toracocentesis en derrames pequeños o loculados. No obstante no es práctico recomendar esta exploración a todos los pacientes.

La TAC se utilizará en caso de que exista patología pulmonar asociada o cuando se requiera definir mejor la localización anterior o posterior del proceso. También será útil para diferenciar una lesión pleural de una lesión en el parénquima pulmonar.

En casos de derrame masivo, el mediastino puede ser empujado por la presión ejercida por el líquido, pero si el mediastino está centrado hay que sospechar de obstrucción bronquial proximal. Las técnicas de diagnóstico por imagen ayudan a determinar este desplazamiento.

El tratamiento debe abordar la enfermedad causante y al derrame en sí. El trasudado generalmente responden al tratamiento de la causa subyacente, y la toracentesis terapéutica sólo se indica cuando exista un derrame masivo que cause disnea severa.

Los casos de derrame pleural causado por enfermedad maligna deben ser tratados con quimioterapia o radioterapia. Se puede intentar una pleurodesis química instalando algunos compuestos dentro del espacio pleural para producir una reacción fibrosa que oblitere el espacio en algunos pacientes seleccionados que tienen derrame maligno persistente a pesar de quimio o radioterapia.

El derrame paraneumónico no complicado por lo general responde a la terapia con antibióticos sistémicos. En los casos complicados se requiere del drenaje por un tubo de toracostomía en los casos de empiema, cuando la glucosa de líquido sea menor de 40 mg/dL o el pH sea menor de 7.2.

La mayoría de los hemotórax se drenan con un sello de tórax. Se requiere de toracotomía cuando no pueda controlarse el sangrado, para remover una cantidad de coágulos, o para tratar otras complicaciones del traumatismo torácico. Por el contrario los hemotórax pequeños y estables pueden resolverse de manera conservadora.




</doc>
<doc id="8233" url="https://es.wikipedia.org/wiki?curid=8233" title="Hemotórax">
Hemotórax

El "'hemotórax" es la presencia de sangre en la cavidad pleural. Generalmente está causado por lesiones torácicas, (arterias)pero puede haber otras causas, tales como cáncer pulmonar o pleural, o incluso cirugías torácicas o del corazón.

En una lesión traumática con un objeto contundente, una costilla puede herir parte del tejido del pulmón o de una arteria, causando que la sangre entre en el espacio pleural, en el caso de una lesión cortopunzante o una herida de bala, puede haber compromiso de pulmón. Un hemotórax puede ir asociado con un neumotórax (entrada de aire en el espacio pleural), y dependiendo de la cantidad de sangre, el hemotórax puede complicarse con un estado de shock.

La cantidad de sangre acumulada varia de acuerdo con el diámetro del vaso sanguíneo roto y tiempo que ha transcurrido desde que se produjo la lesión, así tenemos que en un adulto se puede acumular 3 litros o más en cada espacio pleural. El origen de este sangrado puede ser: vasos intercostales, pulmones, vasos bronquiales, vasos pulmonares y los grandes vasos torácicos.


Los síntomas del hemotórax son: dificultad para respirar, dolor torácico, ansiedad o inquietud, y frecuencia cardíaca acelerada. El médico puede confirmar su diagnóstico con un examen físico que puede revelar una disminución de ruidos respiratorios, la aparición de matidez a la percusión, o por medio de una radiografía de tórax.

El tratamiento consiste en estabilizar al paciente, detener la hemorragia y extraer la sangre del espacio pleural, sin embargo, también se debe considerar la sangre, es decir, emplear la sangre extraída del tórax como una transfusión. Además se debe tratar la causa del hemotórax, pero en el caso de una lesión traumática, dependiendo de la gravedad, la simple colocación de un tubo de drenaje es suficiente, sin necesidad de una cirugía. El pronóstico es casi siempre favorable dependiendo de la causa del hemotórax y de la rapidez con la que se aplicó el tratamiento...

Por lejos, la causa más común del hemotórax es el trauma, por un objeto contundente o por herida penetrante, resultando en la ruptura de la membrana serosa tanto pleural como parietal. Esta ruptura provoca que la sangre se vacíe en el espacio pleural, igualando las presiones entre este y los pulmones. La pérdida de sangre puede ser masiva en personas con esta condición, puesto que cada lado del tórax puede contener 30-40% del volumen sanguíneo total de una persona. Incluso pequeñas lesiones en la caja torácica pueden llevar a una hemotórax de importancia.


</doc>
<doc id="8235" url="https://es.wikipedia.org/wiki?curid=8235" title="Acutancia">
Acutancia

La acutancia de una imagen es el grado de contraste que se observa en el límite entre detalles que difieren por su luminancia o densidad óptica. Cuanto más contrastado sea el límite entre una zona oscura y otra más clara, mayor es la acutancia y con ella la nitidez percibida en la imagen. La definición o resolución de la imagen "no" crecen cuando aumenta la acutancia, pero sí la capacidad para distinguir los detalles y la sensación subjetiva, que es la de un aumento de definición, de detalle. El sistema perceptivo visual humano es capaz de distinguir detalles más pequeños cuando su contraste es mayor.

La acutancia también describe la capacidad de registro de un observador o de una cámara fotográfica en términos de la definición del contraste percibida.

En este sentido la acutancia puede variar según el tipo de soporte sensible a la luz que emplee dicha cámara, sea de película emulsionada, negativo o sensor CCD de una cámara digital.


</doc>
<doc id="8236" url="https://es.wikipedia.org/wiki?curid=8236" title="Domingo Santos">
Domingo Santos

Pedro Domingo Mutiñó, cuyo seudónimo es Domingo Santos, es un editor y escritor español, nacido en Barcelona en 1941. Se le considera uno de los más notorios escritores españoles de ciencia ficción contemporáneos. Ha utilizado también otros seudónimos como Peter Danger o Peter Dean.

Pedro Domingo colaboró con la revista Bang! Fundó luego la revista española de ciencia ficción "Nueva Dimensión" junto a Sebastián Martínez y Luis Vigil.

Publicó su primera novela en 1959 y desde entonces ha alternado las actividades de escritor, editor, recopilador, director de colecciones o traductor, siendo uno de los máximos promotores del género. 

Autor de más de una veintena de novelas, entre sus obras destaca Gabriel, una de sus mejores, donde relata la historia de un robot demasiado humano que se encuentra en una especie de cruzada. Gabriel fue publicada en la colección Nebulae en los años 60 y traducida a diversos idiomas, constituyéndose en la primera novela de este género que traspasó las fronteras españolas. 

Fue jurado del Premio UPC durante los primeros cinco años de vida, y posteriormente ha sido finalista del galardón (1996) y ganador de la mención de la edición de 1997. 

Su nombre está ligado también al Premio Domingo Santos, que cada año organiza el congreso español de ciencia ficción (HispaCon), a instancias de la AEFCFT. 


Y en colaboración con Luis Vigil:







</doc>
<doc id="8238" url="https://es.wikipedia.org/wiki?curid=8238" title="Gu ta gutarrak">
Gu ta gutarrak

Gu ta gutarrak ("Nosotros y los nuestros", en euskera) es un relato de ciencia ficción, en castellano y en clave de humor, escrito por la matemática argentina Magdalena Mouján Otaño y en el que se narra la paradójica conclusión de un viaje de un grupo de vascos en una máquina del tiempo. Obtuvo el primer premio en el concurso de cuentos de la Segunda Convención de Ciencia Ficción de la República Argentina, en Mar del Plata, en julio de 1968.

En 1970, la revista española "Nueva Dimensión" publicó el relato en su número 14. A pesar de haber sido presentado éste a Depósito Previo, y de haber sido convenientemente aprobada su distribución, pocos días después el Tribunal de Orden Público ordenó retirar de la circulación el número, pues consideraba que el cuento de Mouján Otaño contravenía el artículo segundo de la Ley de Prensa, obra del ministro de Información y Turismo, Manuel Fraga Iribarne. Según el fiscal especial que cursó la denuncia, "Gu ta gutarrak" atentaba contra la unidad de España. Consecuentemente, tras el secuestro cautelar del número 14, se sustituyeron las páginas del relato por varias historietas de Johnny Hart reunidas bajo el título de "Formicología", para poder continuar con su venta. El juicio contra "Nueva Dimensión" no llegó a llevarse a cabo, pero el caso produjo gran polémica en el "fandom" internacional. Cien números después, en la edición de julio-agosto de 1979, "Nueva Dimensión" publicó de nuevo el relato, como recordatorio de estos hechos y a modo de desagravio contra su autora.



</doc>
<doc id="8246" url="https://es.wikipedia.org/wiki?curid=8246" title="Yom Kipur">
Yom Kipur

Yom Kipur es la conmemoración judía del Día de la Expiación, perdón y del arrepentimiento de corazón o de un arrepentimiento sincero. Son diez días de arrepentimiento.

Es uno de los "Yamim Noraim" (en hebreo, «Días Terribles»). Ellos comprenden "Rosh Hashaná" (Año Nuevo Judío), diez días del arrepentimiento, y su culminación, con el "Yom Kipur". En el calendario hebreo, "Yom Kipur" comienza en el anochecer del noveno día del mes de "Tishrei" y continúa hasta el anochecer del siguiente día. 

"Yom Kipur" es el día judío del arrepentimiento, considerado el día más santo y más solemne del año. Su tema central es la expiación y la reconciliación. La comida, la bebida, el baño o cualquier tipo de limpieza corporal como el lavado de dientes, la utilización de cuero, el untamiento de cremas o bálsamos en el cuerpo y las relaciones conyugales están prohibidos. El ayuno empieza en el ocaso y termina al anochecer del día siguiente. Los servicios de oración de "Yom Kipur" comienzan con la oración conocida como "Kol Nidre", que debe ser recitada antes de la puesta del sol. El "Kol Nidre" (en arameo «todos los votos») es una abrogación pública de votos religiosos hechos por judíos durante el año precedente. Esto sólo concierne a los votos incumplidos hechos entre la persona y Dios, y no anula votos hechos a otras personas. 

Un "talit" (manto de oración cuadrangular) se pone para las oraciones de la tarde - el único servicio de la tarde del año en el cual se hace esto. El culto de "Ne'ilah" es un culto especial que se celebra solo durante el día de "Yom Kipur", y marca el cierre de las fiestas. "Yom Kipur" culmina con el sonar del "shofar", que marca la conclusión del ayuno. Siempre se observa como un día festivo, tanto dentro como fuera de los límites de la Tierra de Israel. 

Los judíos sefardíes (los judíos de origen español, portugués y norteafricano) se refieren a "Yom Kipur" como «el ayuno blanco» y ello se debe a la tradición de vestirse de blanco durante los "Yamim Noraim". 

Yom Kipur en el imaginario colectivo

Mes séptimo: Tishrei. 




</doc>
<doc id="8247" url="https://es.wikipedia.org/wiki?curid=8247" title="Macana">
Macana

La macana es un arma ofensiva, a manera de machete o de porra, hecha con maderas duras y a veces con filo de pedernal, que usaban los nativos americanos. El término macana de origen Taíno se emplea ampliamente para referirse a las mazas de madera que utilizaban los guerreros de los pueblos precolombinos en América central y Sudamérica, aunque también suele usarse para designar a los garrotes pesados.

Algunas macanas de las que se tiene constancia son el macuahuitl usado por los mexicas, que tenía navajas u hojas de obsidiana o pedernal incrustados en sus lados, y una especie de macana-lanza utilizada por los incas y otros pueblos andinos, que consistía en un asta de madera con una piedra u otro objeto contundente en un extremo, que tenía regularmente la forma de una estrella para maximizar el daño al golpear, pudiendo provocar heridas muy graves como fracturas y desgarraduras. Era el arma más común en el arsenal del ejército inca.

En el español moderno el término se usa (de forma coloquial) para referirse a un arma utilizada por guardias de seguridad o policías antidisturbios, con una forma muy similar a las tonfas de Okinawa.


</doc>
<doc id="8249" url="https://es.wikipedia.org/wiki?curid=8249" title="Aceite vegetal">
Aceite vegetal

Un aceite vegetal es un triglicérido extraído de una planta. El término "aceite vegetal" puede definirse estrechamente como referido sólo a los aceites vegetales que son líquidos a temperatura ambiente, o definidos ampliamente sin tener en cuenta el estado de la materia de la sustancia a una temperatura dada. Por esta razón, los aceites vegetales que son sólidos a temperatura ambiente a veces se llaman grasas vegetales. En contraste con estos triglicéridos, las ceras vegetales carecen de glicerina en su estructura. Aunque muchas partes de la planta pueden producir aceite, en la práctica comercial, el aceite se extrae principalmente de las semillas.

En el envasado de alimentos, el término "aceite vegetal" se utiliza a menudo en las listas de ingredientes en lugar de especificar la planta exacta que se está utilizando, especialmente cuando el aceite utilizado es menos conveniente para el consumidor o si se utiliza una mezcla, como los aceites de palma, colza, soja y cártamo (mientras que el aceite de coco y el aceite de oliva pueden ser percibidos como más deseables).

Los aceites extraídos de las plantas se han utilizado desde tiempos antiguos y en muchas culturas. Como ejemplo, en una cocina de 4.000 años de edad, desenterrada en el Parque Estatal de Charlestown de Indiana, el arqueólogo Bob McCullough de la Universidad de Indiana-Universidad Purdue Fort Wayne encontró evidencia de que grandes losas de roca se utilizaba para aplastar nueces de nogal y el aceite era entonces extraído con agua hirviendo. La evidencia arqueológica muestra que las aceitunas fueron convertidas en aceite de oliva por 6000 a. C. y 4500 a. C. en el actual Israel y Palestina.

"Ver también: Aceite de cocina"

Muchos aceites vegetales se consumen directamente, o indirectamente como ingredientes en los alimentos – un papel que comparten con algunas grasas animales, incluyendo mantequilla, ghee, manteca de cerdo y Schmaltz. Los óleos cumplen una serie de objetivos en este papel:
En segundo lugar, los aceites se pueden calentar y utilizar para cocinar otros alimentos. Los aceites adecuados para este objetivo deben tener un alto punto de inflamación. Tales aceites incluyen los aceites de cocina principales – soja, rabina, canola, girasol, cártamo, cacahuete, algodón, etc. Los aceites tropicales, tales como el coco, la palma, y los aceites de salvado de arroz, se valoran particularmente en las culturas asiáticas para cocinar a alta temperatura, debido a sus puntos de inflamación inusualmente altos.

Los aceites vegetales no saturados pueden ser transformados a través de la "hidrogenación" parcial o completa en aceites de mayor punto de fusión. El proceso de hidrogenación implica "aspersión" el aceite a alta temperatura y presión con hidrógeno en presencia de un catalizador, típicamente un compuesto de níquel en polvo. A medida que cada doble enlace carbono-carbono se reduce químicamente a un solo enlace, dos átomos de hidrógeno forman enlaces individuales con los dos átomos de carbono. La eliminación de los enlaces dobles mediante la adición de átomos de hidrógeno se denomina saturación; a medida que aumenta el grado de saturación, el aceite progresa hacia ser completamente hidrogenado. Un aceite puede ser hidrogenado para aumentar la resistencia a la rancia (oxidación) o para cambiar sus características físicas. A medida que aumenta el grado de saturación, aumenta la viscosidad del aceite y el punto de fusión.

El uso de aceites hidrogenados en los alimentos nunca ha sido completamente satisfactorio. Porque el brazo central del triglicérido es blindado algo por los ácidos grasos del extremo, la mayor parte de la hidrogenación ocurre en los ácidos grasos del extremo, así haciendo que la grasa resultante más quebradiza. Una margarina hecha de aceites naturalmente más saturados será más plástica (más "extendible") que una margarina hecha de aceite de soja hidrogenado. Mientras que la "hidrogenación completa" produce ácidos grasos en gran parte saturados, la hidrogenación parcial da como resultado la transformación de los ácidos grasos cis insaturados a los ácidos grasos trans no saturados en la mezcla de aceite debido al calor usado en la hidrogenación. Los aceites parcialmente hidrogenados y sus grasas trans se han relacionado con un mayor riesgo de mortalidad por enfermedad coronaria, entre otros riesgos de salud aumentados.

En los Estados Unidos, el Estándar de la Identidad para un producto etiquetado como "margarina de aceite vegetal" especifica solamente canola, cártamo, girasol, maíz, soja, o el aceite de cacahuete puede ser utilizado. Los productos no etiquetados "margarina de aceite vegetal" no tienen esa restricción.

Los aceites vegetales se utilizan como ingrediente o componente en muchos productos manufacturados.

Muchos aceites vegetales se utilizan para hacer jabones, productos de la piel, velas, perfumes y otros productos de cuidado personal y cosméticos. Algunos aceites son particularmente convenientes como aceites de secado, y se utilizan en la fabricación de pinturas y de otros productos de tratamiento de madera. El aceite Dammar (una mezcla de aceite de linaza y resina Dammar), por ejemplo, se utiliza casi exclusivamente para tratar los cascos de los barcos de madera. Los aceites vegetales se utilizan cada vez más en la industria eléctrica, ya que los aisladores como aceites vegetales no son tóxicos para el medio ambiente, biodegradables si se derraman y tienen altos puntos de inflamación y fuego. Sin embargo, los aceites vegetales son menos estables químicamente, por lo que se utilizan generalmente en sistemas donde no están expuestos al oxígeno, y son más caros que el destilado de petróleo crudo. Los tetraésteres sintéticos, que son similares a los aceites vegetales pero con cuatro cadenas de ácidos grasos comparados con los tres normales encontrados en un éster natural, son fabricados por la esterificación de Fischer. Tetraésteres generalmente tienen alta estabilidad a la oxidación y han encontrado uso como lubricantes de motores. El aceite vegetal se utiliza para producir el líquido hidráulico y lubricante, ambos biodegradables.

Un factor limitante en los usos industriales de los aceites vegetales es que todos estos aceites son susceptibles a volverse rancios. Los aceites que son más estables, tales como aceite de behen o aceite mineral, se prefieren así para los usos industriales. El aceite de ricino tiene aplicaciones industriales numerosas, debido a la presencia del grupo hidroxilo en el ácido graso. El aceite de ricino es un precursor del nilón 11.

El aceite vegetal se utiliza en la producción de algunos alimentos para mascotas. AAFCO define el aceite vegetal, en este contexto, como el producto de origen vegetal obtenido extrayendo el aceite de las semillas o de las frutas que se procesan para los propósitos comestibles.

"Artículo principal: Aceite vegetal combustible"

Los aceites vegetales también se usados para hacer biodiesel, que se puede utilizar como el diesel convencional. Algunas mezclas del aceite vegetal se utilizan en vehículos sin modificar pero el aceite vegetal recto, también conocido como aceite vegetal puro, necesitan ser vehículos especialmente preparados que tengan un método de calentar el aceite para reducir su viscosidad. El uso de aceites vegetales como energía alternativa está aumentando y la disponibilidad de biodiesel en todo el mundo va en aumento.

El NNFCC estima que el ahorro neto total de gases de efecto invernadero al utilizar aceites vegetales en lugar de alternativas basadas en combustibles fósiles para la producción de combustible, oscila entre el 18 y el 100%.

El proceso de producción del aceite vegetal implica el retiro del aceite de los componentes de la planta, típicamente semillas. Esto se puede hacer mediante extracción mecánica utilizando un molino de aceite o una extracción química utilizando un disolvente. El aceite extraído puede ser purificado y, si es necesario, refinado o alterado químicamente.

Los aceites se pueden quitar vía extracción mecánica, llamado "machacamiento" o "prensado." Este método se utiliza típicamente para producir los aceites más tradicionales (por ejemplo, oliva, coco etc.), y es preferido por la mayoría de los clientes de la "comida saludable" en los Estados Unidos y en Europa. Hay varios tipos diferentes de extracción mecánica. El expulsor-prensado para la extracción es común, aunque la prensa de tornillo, la prensa de espolón, y el Ghani (mortero) son también usados. Las prensas oleaginosas se utilizan comúnmente en los países en desarrollo, entre las personas para las que otros métodos de extracción serían prohibitivamente costosos; el Ghani se utiliza sobre todo en la India. La cantidad de aceite extraída usando estos métodos varía extensamente, según lo demostrado en la tabla siguiente para extraer la mantequilla de mowra en la India:
El procesamiento de aceite vegetal en aplicaciones comerciales es comúnmente realizado por extracción química, utilizando extractos de disolventes, lo que produce mayores rendimientos y es más rápido y menos costoso. El disolvente más común es el hexano derivado del petróleo. Esta técnica se utiliza para la mayor parte de los "más nuevos" aceites industriales tales como soja y aceites de maíz.

El dióxido de carbono supercrítico se puede utilizar como alternativa no tóxica a otros solvente.

Los aceites pueden ser parcialmente hidrogenados para producir varios aceites de ingredientes. Los aceites ligeramente hidrogenados tienen características físicas muy similares al aceite de soja regular, pero son más resistentes a volverse rancios. Los aceites de la margarina necesitan ser sobre todo sólidos en 32 °C (90 °F) de modo que la margarina no se derrita en cuartos calientes, con todo necesita ser totalmente líquido en 37 °C (98 °F), de modo que no deje un sabor "mantecoso" en la boca.

Endurecer el aceite vegetal se hace levantando una mezcla de aceite vegetal y un catalizador en un espacio prácticamente al vacío a temperaturas muy altas, e introduciendo el hidrógeno. Esto hace que los átomos de carbono del aceite rompan dobles lazos con otros carbones, cada carbono formando un nuevo enlace simple con un átomo de hidrógeno. La adición de estos átomos de hidrógeno al aceite lo hace más sólido, eleva el punto de humo, y hace que el aceite sea más estable.

Los aceites vegetales hidrogenados difieren en dos formas principales de otros aceites que están igualmente saturados. Durante la hidrogenación, es más fácil para el hidrógeno entrar en contacto con los ácidos grasos en el extremo del triglicérido, y menos fácil para que entre en contacto con el ácido graso del centro. Esto hace que la grasa resultante sea más frágil que un aceite tropical; las margarinas de soja son menos "separables". La otra diferencia es que los ácidos grasos trans (a menudo llamados grasas trans) se forman en el reactor de hidrogenación, y pueden ascender hasta un 40 por ciento en peso de un aceite parcialmente hidrogenado. Los aceites hidrogenados, especialmente los aceites parcialmente hidrogenados con sus mayores cantidades de ácidos grasos trans, se piensan cada vez más como insalubres.

En el procesamiento de aceites comestibles, el aceite se calienta bajo vacío a cerca del punto de humeo, y el agua se introduce en la parte inferior del aceite. El agua se convierte inmediatamente al vapor, que burbujea a través del aceite, llevando con él cualquier producto químico que sea soluble en agua. El burbujeo de vapor elimina las impurezas que pueden impartir sabores y olores no deseados al aceite. La desodorización es clave para la fabricación de aceites vegetales. Casi todos los aceites de soja, maíz y canola encontrados en los estantes de los supermercados pasan por una etapa de desodorización que elimina las cantidades trazas de olores y sabores, y aligera el color del aceite.

El aceite vegetal puede provenir de frutos o semillas como:

Según el Departamento de Agricultura de Estados Unidos el consumo mundial en el año 2007/08 de aceites vegetales fue:

La mayor parte de los aceites vegetales se usan para alimentar el ganado. El aceite vegetal más usado para consumo humano es el de girasol.

El aceite de palma, que es sólido a temperatura ambiente, se usa especialmente para jabones y cosméticos.

La mayor parte del aceite de colza producido en Europa se usa para producción de biodiésel, aunque puede ser producido con otros como el de girasol o el de marihuana. Aunque también se ha extendido el uso de estos aceites vegetales como combustibles para los motores diésel.

El aceite vegetal también se puede utilizar como combustible en vehículos híbridos o adaptados.



</doc>
<doc id="8251" url="https://es.wikipedia.org/wiki?curid=8251" title="Guerra de Vietnam">
Guerra de Vietnam

La guerra de Vietnam ("Vietnam War" en inglés, "Chiến tranh Việt Nam" en vietnamita) llamada también Segunda Guerra de Indochina o guerra contra los Estados Unidos para los vietnamitas, fue un conflicto bélico librado entre 1955 y 1975 para impedir la reunificación de Vietnam bajo un gobierno comunista. 
Participó la República de Vietnam (Vietnam del Sur) con el apoyo de Estados Unidos y otras naciones contra la guerrilla local del Frente Nacional de Liberación de Vietnam (Viet Cong) y el ejército de la República Democrática de Vietnam (Vietnam del Norte), respaldados por China y la Unión Soviética. Se calcula que murieron entre un millón y 5,7 millones de personas. Estados Unidos contabilizó 58 159 bajas y más de 1700 desaparecidos, constituyendo la contienda más larga de dicho país hasta la guerra de Afganistán.

El conflicto comenzó por un intento de unificar las dos Vietnam en un único gobierno de coalición entre nacionalistas,comunistas y neutrales, según la propuesta inicial. Las acciones de los Estados Unidos para evitar dicha reunificación, unidas a una sucesión de dictaduras violentas, corruptas e ineficientes impuestas por Estados Unidos, provocaron el levantamiento en armas de varios grupos unidos bajo el autodenominado Frente de Liberación Nacional, Viet Cong, rápidamente apoyado por la entonces Unión Soviética y la China de Mao. Inicialmente Saigón fue perdiendo terreno. 

Con la entrada masiva de los Estados Unidos se recuperó parte de lo perdido. Pero, tras los sucesos de 1968, empezó la retirada progresiva de las tropas estadounidenses y la firma de los Acuerdos de paz de París en 1973 tras los cuales el Sur luchó solo contra el Ejército de la República Democrática de Vietnam hasta su propia derrota final y la consiguiente reunificación del país el 2 de julio de 1976 como la República Socialista de Vietnam de régimen comunista. Las tres primeras etapas se distinguieron por transcurrir sin la formación de las tradicionales líneas de frente, donde proliferaron acciones terroristas y la guerra de guerrillas, frente a las misiones de «búsqueda y destrucción», el uso de bombardeos masivos y el empleo extensivo de armas químicas, por parte de los Estados Unidos. La última fase fue una guerra convencional. Pero el fin de la contienda solo resultó una pausa en los enfrentamientos de Indochina. Después se producirían las invasiones de Camboya y Laos por Vietnam y de este por China. Por el contrario Estados Unidos vivió un repliegue de la política exterior. 

La cobertura realizada por los medios de comunicación fue permanente, estando considerado como el primer conflicto televisado de la Historia. Esto permitió la denuncia de las frecuentes violaciones y abusos contra los Derechos Humanos cometidos por los dos bandos. Sin embargo, se discute si dicha cobertura constituyó la causa principal de la creciente oposición por parte de la opinión pública occidental hacia la intervención estadounidense. 

Esta oposición y el hecho de ser la única derrota militar de los Estados Unidos hasta la actualidad, creó un sentimiento de mala conciencia en el pueblo estadounidense ante una guerra considerada injusta, el llamado síndrome de Vietnam. El síndrome dio paso a un movimiento pacifista y se prolongó hasta los años 1980, durante los mandatos de Ronald Reagan. La guerra de Vietnam se convirtió en un icono, perdurado en la actualidad, de los grupos sociales y partidos de izquierda en gran parte del mundo, además de un ejemplo imitado por otros combatientes posteriores.

La historia de Vietnam comenzó en el siglo XII, con un grupo de pueblos desplazados desde el sur de China por la invasión mongola y que colonizaron la cuenca baja del río Rojo. En el acuerdo de paz firmado por el rey vietnamita Trần Nhân Tông en 1257, Vietnam accedió a pagar tributos a China para evitar más enfrentamientos. En esos 700 años de historia como pueblo, Vietnam fue alternando su posición de reino invadido por otros pueblos, sobre todo mongoles y chinos, a la de invasor de sus vecinos; pues siempre mostró interés en anexionar Laos y Camboya, cuando no toda la península de Indochina. Este período de independencia y expansión del reino concluyó a finales del siglo XIX cuando el país fue invadido por los europeos, sobre todo Francia y España.

Durante la Segunda Guerra Mundial, el imperio nipón también invadió buena parte de Asia, incluida Indochina. Mantuvo, eso sí, a los administradores franceses en sus puestos para no alterar el orden en la colonia. La contienda pareció enseñar dos lecciones. Por una parte, que los europeos en general y los franceses en particular distaban mucho de ser invencibles. Por otra que tratar de apaciguar al agresor solo le hace más agresivo, como se vio tras la Conferencia de Múnich. La primera lección contribuyó a espolear los levantamientos en Asia y África contra Francia, los Países Bajos, Portugal o el Reino Unido. La segunda, a una visión del comunismo como nuevo poder agresor, algo apoyado en teorías como la defendida tanto por Estados Unidos como por la URSS que postulaban una inevitable implantación del comunismo en todo el mundo, por la fuerza según los primeros y por las ventajas de su sistema según los segundos.

Para responder a los distintos movimientos independentistas, los gobiernos europeos enviaron a lo mejor de sus ejércitos contra los movimientos de liberación en Indochina, Indonesia, Guinea o la India. Sin embargo, el cambio en la percepción de los occidentales y el agotamiento provocado por la Segunda Guerra Mundial hacía muy difícil volver a la situación anterior. Como contestación a esta real o supuesta expansión del comunismo, en la década de 1940, Harry S. Truman ayudó a la monarquía griega a ganar su guerra civil contra las milicias del Partido Comunista de Grecia (KKE). También parecía obtener éxitos en Malasia, Indonesia o Filipinas, naciones con posibilidad de cambiar de aliados. No tuvo suerte con la China de Mao, que sí adoptó el régimen comunista. La guerra de Corea, ante la invasión de Corea del Norte, pareció dar un respiro, pero historiadores como John lo consideran una derrota en la práctica. Sí lo fueron para la Casa Blanca el paso de Vietnam del Norte, Birmania y Cuba a la esfera socialista, sin mencionar todas las naciones europeas bajo la ocupación soviética. Estados Unidos temía quedar rodeado por una constelación comunista de la que Vietnam del Sur sería una pieza más y el desencadenante de una sucesión de pérdidas en toda la península con la consiguiente pérdida de prestigio internacional (la así llamada «teoría del dominó»). En opinión de los distintos gobiernos estadounidenses, si la URSS no veía una oposición decidida, podrían repetirse las consecuencias de Múnich y revivir las acciones expansionistas del régimen nacional-socialista. En la década de 1950, Dwight D. Eisenhower profundizó en la doctrina de Truman con apoyo económico a militares golpistas de dictaduras como Filipinas, Singapur o Corea; además apostó por la Carrera espacial contra la Unión Soviética para conseguir que Indonesia y otros países de la región no cambiaran de bando.

Según John , los distintos gobernantes de la URSS también se veían amenazados por los occidentales. Estadounidenses y europeos habían apoyado al Ejército Blanco en su guerra civil. Habían confiado en Alemania y los invadió. Con el final de la Segunda Guerra Mundial veían cómo su territorio era rodeado por bases estadounidenses con armas nucleares en Alemania occidental, Japón, Turquía. Pero además, los posibles gobiernos que podían simpatizar con la URSS eran hostigados por occidentales, caso de Nasser en Egipto, o depuestos directamente como Lumumba en el Congo.

Francia deseó restablecer su mandato colonial en Indochina tras la rendición de Japón, pero Hồ Chí Minh había declarado la independencia de la República Democrática de Vietnam el 2 de septiembre de 1945. Según los franceses lo recibieron en París como guerrillero y no como jefe de estado. Un frente de nacionalistas y comunistas llamado "Viet-Nam Doc Lap Dong Minh Hoi" o Liga por la Independencia de Vietnam, Viet Minh en su contracción vietnamita, aceptó al principio el retorno de los franceses para evitar la amenaza de China; pero pronto la tensión con las fuerzas coloniales se hizo insoportable. Dicha liga estaba dirigida en lo político por Hồ Chí Minh, partidario de aguardar, y en lo militar por Vo Nguyen Giap, finalmente deseoso de comenzar los ataques. En 1946 se produjeron los primeros tiroteos en lo que se conoce a veces por la Guerra de Indochina, pese a no existir consenso entre los autores.

Francia contaba con el apoyo de buena parte de la colonia, especialmente los vietnamitas monárquicos. Sin embargo, los distintos gobiernos de París no deseaban enviar reclutas ni gastar muchos recursos en el conflicto, por lo que acudieron a Estados Unidos en busca de fondos y armas. Harry S. Truman, en 1950, comenzó contribuyendo con el 15 % de los gastos militares aproximadamente. Cuatro años después, Dwight D. Eisenhower ya soportaba más del 80 % del esfuerzo bélico para levantar, por ejemplo, una base fortificada en Dien Bien Phu, donde un tercio del material llevado allí formaba parte de la ayuda estadounidense. Dicha base perseguía cortar la conexión entre el Viet Minh y la guerrilla que operaba en Laos, además de pretender librar una batalla convencional donde las fuerzas de Vo Nguyen Giap se presumían inferiores. Giap estaba siguiendo el espíritu contenido en la frase:
Sin embargo, en Dien Bien Phu Giap "recogió el guante", emprendió una batalla convencional hasta convertirla en una de las mayores derrotas de Francia. En aquel valle, el Ejército Colonial francés perdió lo mejor de su fuerza de combate, poniendo al gobierno de París en desventaja para terminar la conferencia de Ginebra de 1954. Eisenhower no proporcionó las decenas de aviones necesarios que solicitaron los franceses, pero sí ofreció a los franceses dos armas nucleares, éstos las rechazaron por no considerarlas útiles.

Tras la derrota y los acuerdos firmados en la ciudad suiza, la Indochina francesa se dividía en las naciones independientes de Camboya y Laos, más Vietnam separado a su vez por el paralelo 17, el norte sería una zona para la reagrupación del Viet Minh y el sur para el ejército colonial francés, a la vez de concentración de la población simpatizante de cada bando enfrentado. Las dos divisiones pasaron a llamarse República Democrática de Vietnam, más conocida por Vietnam del Norte, y el Imperio de Annan, bajo el mando del emperador Bao Dai; pero se incluyó una cláusula por la cual se celebraría un referéndum en 1958 para decidir si los dos Vietnam seguirían separados o se reunificaban.

El 30 de abril de 1955 el general Ngo Dinh Diem dio un golpe de Estado con el apoyo de la CIA, declaró la República de Vietnam e impuso una dictadura basada en tres personas: él mismo, su hermano Ngo Dinh Nhu y la mujer de su hermano. También canceló las elecciones de 1956 ante su previsible derrota frente a Lao Dong. Para el referéndum para la reunificación tampoco se celebró al alegar el presidente Diem que los ciudadanos del Norte no eran libres para expresarse. Pero, según , la verdadera razón radicaba en las muchas posibilidades de que ganase el "Sí" en el sur, algo no deseado por los dirigentes de Saigón ni por la Administración Eisenhower.

La escasa entidad de Vietnam del Sur como país y la enorme corrupción existente en el gobierno provocaron que la dictadura de Ngo Dinh Diem se hiciese impopular, además los gobernantes de Saigón solían ser católicos, cuando el país era mayoritariamente budista, y no dudaban en reprimir a los seguidores de Buda, años después, las protestas contra dicha represión dieron la vuelta al mundo cuando un monje budista se inmoló con combustible en plena calle, el ritual bonzo. Ante esta situación ocurrieron dos acciones paralelas y complementarias:

Los casos de Indonesia, Filipinas y especialmente Singapur, Corea del Sur y Taiwán estaban siendo éxitos, tanto políticos como económicos porque sus regímenes dictatoriales permanecían estables y sus productos interiores brutos crecían; por lo que repetir la misma estrategia en Indochina se consideraba posible. Para ello, Eisenhower apoyó al régimen de Diem y al de sus sucesores con 1.200 millones de dólares en cinco años y el envío de 700 asesores militares. El presidente Kennedy profundizó en la misma política.

Acciones armadas venían produciéndose desde el 1 de noviembre de 1955; pero debería llegar el año 1959 cuando comenzó la verdadera lucha. En ese momento antiguos guerrilleros del Viet Minh, monjes budistas, campesinos y varios grupos más empezaron a integrar el que después se llamaría Frente de Liberación Nacional. Las acciones armadas fueron la respuesta violenta a las políticas gubernamentales contra la población civil y los sucesivos incumplimientos de sus compromisos. Sus objetivos eran derrocar a Ngo Dinh Diem y reunificar el país. Este deseo de unidad nacional expresado en la frase «lucharemos durante mil años» fue algo que los estadounidenses no llegaron a entender y a la larga constituyó una causa más de su derrota.

La táctica del FNLV consistía en la guerra de guerrillas, que tantos éxitos les trajo en el conflicto anterior contra el régimen colonial francés. Así en julio de 1959 el comandante Dale Buis y el sargento Chester Ovnard fueron los primeros estadounidenses muertos en Vietnam durante los ataques a la base de Bien Hoa, pero en 1959 el FNL principalmente asesinaba a líderes locales leales al gobierno de Saigón. Sería en la siguiente década cuando comenzaron a emplear las pocas armas de que disponían, teniendo como núcleo a unos 10.000 veteranos de la lucha del Viet Minh contra los franceses ayudados por los comunistas del Norte. Por su parte, Vietnam del Norte necesitó varios años para organizar la estructura estatal y tomar las riendas de todo el país, por lo que hasta 1959 no pudo contar con dos comandos para el envío de suministros al Sur, principalmente por mar aunque también mandaron algunos suministros a través de la que se llamaría Ruta Ho Chi Minh, en honor del primer presidente del Vietnam moderno. Esta vía, finalmente clave para la victoria, distaba mucho de ser una carretera, o incluso un camino, sino miles de caminos, túneles y variantes, a través de Laos y Camboya.
La insurgencia se vio favorecida por el propio ERVN, ejército de Vietnam del Sur. Este resultaba muy ineficaz luchando en su propio país. Su armamento resultaba poco adecuado, contaba con escasos pilotos de helicópteros nativos; pero quizá su peor defecto era la gran corrupción e ineptitud de sus oficiales, la mayoría puestos por compromisos políticos entre familias de las élites católicas de Saigón. Consecuentemente los soldados del Sur no confiaban en sus mandos, se arriesgaban lo imprescindible, incluso viendo luchar a sus compañeros a escasas decenas de metros, y no recibían una mínima preparación militar, hasta el punto de hacer guardia con una radio a todo volumen. Pero el poco espíritu de lucha no faltaba solo en el Ejército, los dirigentes en Saigón irritaban a los estadounidenses queriendo negociar con el FNLV en lugar de combatirlo. En aquel momento la única experiencia exitosa para invertir ese tipo de situaciones la desarrollaron los británicos durante la llamada Emergencia Malaya con el nombre de Campaña Corazones y Mentes. Como en Malasia, los dirigentes vietnamitas y sus asesores estadounideneses trataron de crear "nuevas aldeas" con el doble propósito de controlar a la población y separarla de los guerrilleros a los que informaban y alimentaban. Se llamó "Strategic Hamlet Program" o Programa de Aldeas Estratégicas y comenzó en enero de 1962. Llegó a crear 7.200 nuevos núcleos urbanos con unos 8'732.000 habitantes y resultó un completo fracaso porque los soldados del Sur no estaban entrenados para ganarse la confianza de los aldeanos; además, siendo tantas aldeas no se podían defender y solo lograron enemistarse con la población al trasladarla por la fuerza.

Si existía descontrol en las zonas rurales, no era menor en el gobierno. En 1963 el presidente Diem fue asesinado en un nuevo golpe militar patrocinado por la administración estadounidense de John Fitzgerald Kennedy, a quien no le convenía apoyar a un general católico dentro de un país con mayoría budista. Nguyen Van Thieu sustituyó a Diem, siendo uno más de los diez gobiernos que llegó a tener el país en un solo año, aumenta el periodo a 18 meses.

A pesar de las ventajas reunidas por los insurgentes y de la incompetencia de sus enemigos, sus principales victorias y la dominación masiva de territorio se dieron a partir del verano de 1964, cuando llegaron los hombres del Norte, como se les ha llamado algunas veces a los soldados del Ejército de Vietnam del Norte o EVN. Para Vietnam del Norte la cancelación del referéndum de reunificación no se vio como un escollo insalvable. Tanto el presidente Hồ Chí Minh y el ministro de Defensa Vo Nguyen Giap en particular como el politburó en general consideraban que la independencia de Francia constituía un paso dentro de una estrategia más larga que podía incluir hasta la posterior dominación de toda Indochina, viejo sueño vietnamita desde la Edad Media. Según esta estrategia, la reunificación por votación o por fuerza sería inevitable. Al no ser convocado el referéndum, quedaba la vía militar. Pero esta tampoco sería sencilla entre otros motivos porque, pese a lo impopular del régimen de Saigón, no todos los vietnamitas del Sur veían con buenos ojos a los comunistas. Aproximadamente un millón de personas habían emigrado al Sur huyendo de Hanoi al producirse la división del país, frente apenas cien mil que se desplazaron hacia el norte. Tampoco el EVN confiaba mucho en sus aliados del FNL y estos no terminaban de vencer sus reticencias a obedecer las órdenes dadas desde Hanoi. Por estas razones el régimen del Sur no se desmoronó inmediatamente; pero fue cediendo territorio poco a poco.

El Frente de liberación nacional de Vietnam o FNLV es más conocido por la contracción "Vietcong", del vietnamita "Vietnam Congsan", la cual se traduciría como "Vietnam Rojo" según . Pero existen discrepancias sobre los grupos a los que se refería una y otra palabra, pues no eran dos nombres para la misma organización sino el FNL el brazo político del Vietcong. En cualquier caso los vietcong no eran comunistas en su mayoría y su líder, Nguyen Huo Tho, tampoco lo era; pero no hay consenso sobre su autonomía respecto a Hanoi: autores como indican que la independencia del FLN de Hanói era solamente nominal. por contra afirman que su dependencia del Norte fue siempre considerable, como también el acatamiento de las órdenes dadas desde allí.

Como tal "frente" estaba integrado por una variedad de voluntarios, como monjes budistas, miembros de minorías y uno de cada quince antiguos combatientes del Viet Minh. En total debían ser poco más de 3000 guerrilleros en 1960, aunque le resultaba fácil conseguir voluntarios para terminar con un gobierno incompetente, represivo y corrupto. El FNL poseía fiereza, determinación y gran capacidad de sacrificio, algo que sorprendió a muchos soldados del Sur y después los estadounidenses, a menudo salidos de un reemplazo forzoso. Un miembro del FNLV escribió:

Al principio estaban mal armados. El FLNV obtenía la mayor parte de su material del ejército del Sur y utilizaban técnicas ancestrales para fabricar trampas, como las estacas punji cubiertas de excrementos para acelerar la gangrena. De las granadas, obuses y bombas sin explotar podía obtener unas 800 toneladas mensuales de explosivos para trampas. A esto se sumaban las pocas ayudas que conseguían en los países vecinos y las, en principio, escasas aportaciones del Norte. Ante dichas carencias, las armas constituían una prioridad, las demás necesidades ocupaban un segundo plano, por lo que sufría escasez de medicamentos, víveres e incluso agua. Afortunadamente para ellos, cerca de Saigón y otros lugares, contaban con una infraestructura de túneles subterráneos excavados durante la invasión japonesa y ampliados progresivamente durante la guerra contra Francia. En ellos podían descansar, preparar las incursiones y a veces recibir atención médica. Casi lo contrario al bando enemigo, donde la superioridad logística no acarreaba más que envidia y odio, y con ello ganas de golpear con más fuerza. Un ex guerrillero recordaba:
Además, su adaptación al terreno les permitía vivir escondidos o trabajando durante el día, para realizar por la noche todo tipo de ataques y sabotajes, empleando el terreno, la vegetación y armas ligeras. De esta forma la noche realmente les pertenecía, porque durante esas horas, eran ellos quienes dominaban la jungla y a los aldeanos. No perder el apoyo de la población local resultaba de gran utilidad al FNLV al tener acceso a comida e información, imprescindible para el éxito de sus ataques. Para mantener este apoyo el Frente realizaba campañas de adoctrinamiento y también de terror contra la población civil que consideraba colaboracionistas con Saigón, en ocasiones realizando empalamientos para intimidar a los aldeanos. Algunas fuentes cifran en 30 000 el número de civiles asesinados.

Pese a sus logros iniciales, en el verano de 1964 Hanoi consideró que dicha fuerza no podría ganar la Contienda sola, por lo que comenzaron los envíos de unidades enteras del EVN, mejor equipadas, entrenadas y armadas, además de mandadas por oficiales expertos. Gracias a ello en parte, los guerrilleros sorprendieron a los estadounidenses organizando ataques a nivel de división, es decir, una unidad atacaba a otra inferior en número y cuando se solicitaban refuerzos para repeler la agresión, los refuerzos eran atacados por un contingente aún mayor. Así se conseguía aumentar la impaciencia y la desmoralización entre auxiliados y auxiliadores. Si los refuerzos eran demasiado grandes el FNLV y el EVN siempre podían desaparecer en la selva.

Las Fuerzas Armadas de la República Democrática de Vietnam también llamado Ejército Popular de Vietnam, EPV o EVN, por "Ejército de Vietnam del Norte", contaban en 1960 con unos 200 000 hombres entre las tres ramas. Al Sur bajaron en 1961 15 000, de los cuales la inmensa mayoría eran fuerzas terrestres con experiencia en la guerra de guerrillas, herederos del Viet Minh.

La guerra de Vietnam se ha comparado con cualquier otra confrontación donde los Estados Unidos, u otra potencia, no gana con la claridad que se espera de su armamento. Sin embargo, la de Vietnam cuenta con dos diferencias que no se han repetido desde entonces:

Con todo, el EVN demostró ser una maquinaria bélica muy eficaz, con soldados motivados y mandos preocupados por su tropa, casi lo contrario de los sudvietnamitas y estadounidenses. Tanto es así que contaba con una de las unidades más famosas, los «Voluntarios de la muerte vietnamitas» dispuestos a morir encadenados a los árboles para cubrir a sus compañeros. Asimismo, no se constató ningún caso de agresiones a un mando del EVN, cuando en el otro bando no bajaban de decenas al año.

En cuanto al material existe la idea de una desproporción enorme de un bando respecto al otro. Estados Unidos utilizó en Vietnam los más sofisticados productos electrónicos de que disponía, como detectores de movimiento, bombas "lazy dogs" cargadas con miles de cuchillas, helicópteros artillados... pero también adolecía de armamento mal diseñado frente a un enemigo equipado con algunas de las mejores armas del mundo. En varias publicaciones se han destacado proezas aéreas como las realizada por el teniente Randall Cunnigham a los mandos de su Phantom; pero lo cierto es que los pilotos vietnamitas derribaron multitud de cazas y bombarderos pilotando MiG-17 y MiG-21, pese a contar con menor mantenimiento y sobre todo menor entrenamiento que sus enemigos. Un vietnamita que no deseaba ser identificado lo describía de la siguiente manera:

El Ejército de la República de Vietnam o ERVN contaba en 1960 con unos 150 000 hombres, reclutados para un servicio de tres años de los que servían entre 60 y 90 días seguidos en campaña. Por lo tanto y en teoría, suponían una fuerza como mínimo de doble tamaño a la suma del FNLV y el contingente inicial del EVN. Sin embargo su número es engañoso por las decenas de miles de deserciones anuales, 132 000 solo en 1966. Pero, aun suponiendo que su cuantía teórica fuese real, no constituían un rival para el EVN, ni siquiera para el FNLV. Por ejemplo, enero de 1963 una división del ERVN, unos 10 000 hombres, no fue capaz de derrotar en Ap Bac a tres compañías del FNLV, unos 340 guerrilleros. Las razones eran variadas, como contar con poco poder aéreo, pocas piezas de artillería, utilizar fusiles estadounidenses no adaptados al clima y a la constitución de los vietnamitas, no contar con personal de mantenimiento suficiente...; pero especialmente por su baja moral. Los soldados del sur estaban mal pagados, sus familias debían seguirlos y vivir en chabolas cerca de las bases y la corrupción reinaba entre los mandos, además de la incompetencia. Los oficiales eran nombrados por afinidad política en lugar de por méritos y no solían arriesgarse a ir junto a sus hombres al combate. Un motivo más de desmoralización eran el odio que solía tenerles el pueblo por las torturas, robos y otros delitos que cometían.

Sin embargo, no todas las unidades del ERVN tenían bajo desempeño. Los Ranger o la 1ª División de Infantería estaban mejor pagados y contaban con mandos más competentes, además dichas unidades las integraban en ocasiones exconvictos alistados para huir de la cárcel, por lo que contaban con alguna motivación para luchar. Estas unidades realizaron actos de valor reconocidos por los estadounidenses, como su participación en el levantamiento del Sitio de Khe Sanh, pero solo constituían el 5 % del ejército.

A lo largo de la década de 1960, los militares estadounidenses se habían visto envueltos en refriegas con algún muerto, en ocasiones dos. Fue la llamada "etapa de los asesores". Al principio los asesores militares estadounidenses estaban en el sureste asiático para formar una fuerza de irregulares en las Tierras Altas Centrales e instruir al Ejército de Vietnam del Sur en tácticas, mantenimiento de aeronaves y otras funciones auxiliares. No tenían permiso para intervenir en los combates, mucho menos para preparar acciones contra los guerrilleros; pero más de una vez se saltaron esta prohibición en la que sería, quizá, la primera de una larga lista de violaciones jurídicas e ilegalidades que harían famosa esta guerra.

Los informes enviados a Washington por los asesores y otros expertos concluían que la situación era muy mala y el Sur seguía con gobiernos precarios perdiendo claramente la guerra civil, así a finales de 1964 aproximadamente el 60 % del país estaba en poder del FLN y no había expectativas de un cambio en la tendencia. Las infiltraciones comunistas se habían triplicado, llegando a unos 34000 efectivos.

Para , lo que finalmente desencadenó la intervención total estadounidense fue una reunión mantenida por Johnson con sus asesores el 21 de julio de 1964. En ella trató de idear una manera para forzar al gobierno de Saigón a luchar en lugar de negociar, pero la opinión imperante fue que la retirada de los asesores residentes en el país no conseguiría eso, sino la rápida conquista por parte de Hanói. La intervención directa, opinaban los asesores presidenciales, resultaría muy larga, costosa y sangrienta por ser el Sur un país poco interesado en su supervivencia. Sin embargo, de no hacerlo y en opinión de los mismos asesores, Estados Unidos parecería un tigre de papel y podría degenerar en la Tercera Guerra Mundial, al no ver los soviéticos obstáculos insalvables para su expansión.

Doce días después de la reunión se produjo el "Incidente del Golfo de Tonkin", con un primer ataque al destructor estadounidense USS "Maddox" el dos agosto de 1964. Al día siguiente se unió al USS "Maddox" el USS "Turner Joy" y la noche del cuatro de agosto supuestamente se produjo un nuevo ataque, pese a no existir pruebas de dicho acto. El presidente Lyndon B. Johnson ordenó el 5 de agosto a los navíos USS "Ticonderoga" y USS "Constellation" acciones de represalia contra la flota norvietnamita. Siendo ciertos o no alguno de los ataques, el incidente legitimó a Johnson para solicitar y conseguir del Congreso el 6 de agosto la llamada Resolución del Golfo de Tonkín. Esta resolución conferiría plenos poderes para que los asesores militares presentes en Vietnam realizaran operaciones fuera del recinto de sus bases, además de incrementar la cantidad de tropas en ese país, al estar en campaña electoral Johnson necesitaba mostrar una imagen de fuerza frente al comunismo.

El 2 de marzo de 1965 se autorizó la Operación Rolling Thunder, planificada desde hacía un año, con cien cazabombarderos y 200 toneladas de bombas cada uno con el objetivo de atacar instalaciones nortvietnamitas y doblegar su voluntad «destruyendo acero y hormigón», en palabras del presidente Johnson. En ese mismo mes desembarcaron en la base aérea de Da Nang 3500 marines para protegerla y unirse a 60 000 soldados ya destinados en Vietnam como asesores. Todo ello se realizó sin consultar a la opinión pública estadounidense, pero sí con su apoyo mayoritario; aunque ya en ese momento se organizaron protestas en contra y denuncias ante el descaradamente clasista sistema de reclutamiento. Desde el punto de vista del Derecho Internacional Estados Unidos no estaba en guerra contra ninguna nación, para ser así debería haber existido una declaración previa, tampoco fue una invasión de Vietnam del Sur, solo la llegada de más asesores.

 indica que la Casa Blanca marcó una meta propagandística y otra militar desde un principio.

El objetivo político pretendía dar a conocer las acciones del Norte y del FNLV tanto a los miembros del Congreso como a la opinión pública estadounidense y mundial, con el fin último de aislar internacionalmente a Vietnam del Norte y marginar al FNLV como interlocutor, en caso de llegar a una negociación. Para ello se utilizarían los medios de comunicación y las acciones diplomáticas.

El presidente Johnson trató de atraer a tantos países como pudo con la Campaña Más banderas, para dar una idea de que el "Mundo Libre" estaba luchando contra el comunismo, pese a que el adjetivo "Libre" es más un eufemismo que una realidad debido a la presencia de países como Corea del Sur o Filipinas. Muchas naciones enviaron ayuda, principalmente en forma de suministros médicos, algo bien visto por la población del país emisor y receptor; pero solo siete destinaron soldados a la Península: la dictadura sudcoreana envió en 1965 200 hombres y fue aumentando el contingente hasta 47 829 soldados en 1967; Tailandia contribuyó con un total de 11 568 soldados, además permitió a Estados Unidos emplear su territorio para operar bombarderos B-52, cazas, aviones de reconocimiento y el Centro de Vigilancia de la Infiltración; Australia terminó destinando una división, primero con asesores en 1962, después con 1400 soldados, algunos veteranos de la lucha en las junglas malayas, y finalmente con un número máximo de 7672 soldados y oficiales en 1967, por lo que se convirtieron en un importante aliado estadounidense y experimentado en un territorio muy hostil como es la selva, se retirarían en diciembre de 1972. Participaciones más pequeñas fueron la de Filipinas (2000 soldados), Taiwán (31 hombres) y España con varios grupos de 13 médicos militares.

En el campo militar el objetivo marcado era demostrar al FNLV y a Hanoi que no podrían ganar la guerra debido a las numerosas bajas y derrotas que les infligirían. Por tanto, como indicó el propio presidente de los Estados Unidos, sería una guerra diferente, donde no existiría una capital que tomar o unas líneas de frente que romper. Para infligir esas derrotas y esas pérdidas, el presidente Johnson deseaba utilizar más los bombardeos que las acciones de infantería, pero para esto sería necesario levantar una serie de bases navales y aéreas que cubrieran todo el país. A su vez, dichas bases necesitaban tener garantizada su seguridad, por lo que se consideró necesario:
La primera misión podía seguir en manos la flota de "aguas azules" destinada en el sureste asiático que habían realizado un buen trabajo hasta entonces. Para la segunda y la tercera se necesitan envíos masivos de hombres y material. Así, a finales de 1965, ya eran más de 100 000 los efectivos desplazados a Vietnam y se habían destinado 1000 millones de dólares para el envío de casi diez millones de toneladas mensuales en suministros y equipo. Toda esta ingente cantidad de materiales requería una enorme cadena logística que lastró mucho al Ejército. Por ejemplo, solo uno de cada siete soldados estadounidenses se vio realmente envuelto en combate, los demás pertenecían a cuerpos logísticos, administrativos, médicos, mecánicos, etc.

Para cumplir la tercera meta militar, el despliegue de potencia de fuego con la que cubrir a la infantería en misiones de búsqueda y destrucción, Estados Unidos haría uso de todo su poder aéreo. Por ejemplo, si las piezas artilleras aerotransportadas no podían descargarse por lo espeso de la selva, aviones de distintos tipos lanzarían bombas de cientos de kilos que abrían un cráter y permitir el aterrizaje.

En batallas más o menos convencionales, los guerrilleros vietnamitas aún tenían cartas que jugar frente a los soldados del Sur y lo demostraron en el mes de junio. El 51º batallón del ERVN cayó en un ataque sorpresa cerca del golfo de Tonkín y fue desintegrado por completo. Pero el resultado fue diferente cuando los estadounidenses entraron en acción:
Los éxitos en la Operación Starlight y en Ia Drang llevaron al jefe de las fuerzas estadounidenses en Vietnam, general William Westmoreland, a solicitar y conseguir los medios para realizar las acciones que pensaba le llevarían a la victoria, entre las que destacaron:
Asimismo en diciembre de 1965, la Fuerza Aérea puso en marcha el Programa Big Belly, para permitir que los B-52 transportar casi 10 000 kg de bombas y en abril del año siguiente fueron desplazados a la isla de Guam para poder alcanzar Vietnam del Sur. Desde allí se realizaron una media de 300 salidas al mes.

El primer año de la guerra, Estados Unidos venció en la práctica en la totalidad de las batallas, gracias a su potencia de fuego y a poder abastecer a sus hombre por aire sin sufrir los numerosas ataques que tantas pérdidas les costaron a los franceses . Esto les hizo pensar en una victoria rápida; pero de la que podían obtener experiencia en combate para sus oficiales, por lo que decidieron enviar allí a todos los posibles, rotando cada seis meses en lugar de cada doce. Esto causó un primer problema. Las estadísticas informaban de que un militar comenzaba a desenvolverse bien a los noventa días de servir en Vietnam y alcanzaba su óptimo operativo a los diez meses. La continua rotación fue imprimiendo un sentimiento en las unidades de ser mandadas por novatos ineptos, lo que les hacía candidatos a las temidas emboscadas, por tanto, no dudaban en eliminar a sus jefes y a cualquier recluta no demasiado hábil.

Westmoreland y sus aliados lanzaron una misión tras otra de las que se puede destacar la Operación Market Time, para cortar los suministros llegados por mar, y Operación Prairia, con el fin de detener los combates en el llamado "Cerro de los murmullos" en la zona desmilitarizada. También se autorizó el empleo del Agente Naranja para eliminar la cubierta vegetal que protegía las guaridas y las posiciones desde las que los guerrilleros atacaban a las tropas regulares. Todo ello daba una visión optimista a las opiniones públicas estadounidenses y de los distintos países que los apoyaban; pero la imagen que se tenía al llegar a cualquier parte de Vietnam del Sur era de inseguridad. Así lo comprobaron los soldados españoles cuando aterrizaron en Saigón en abril de 1966. Los edificios oficiales se veían protegidos por sacos terreros, el autobús que los transportaba llevaba las ventanillas cubiertas por rejas para impedir la entrada de granadas. Incluso en el propio hotel Península, donde se alojaron, tuvieron que interrumpir la emisión de una película por explosiones cercanas y el posterior contraataque helitransportado. Eso dentro de la propia capital del país.

Sin embargo, 1966 no resultó tan exitoso a los estadounidenses como 1965. El Mando de la Asesoría Militar y el propio Westmoreland reconocieron que el número de bajas estadounidenses resultaron desproporcionadamente altas y el número de victorias se había reducido, los vietnamitas estaban empezando a llevar la iniciativa. Westmoreland solicitó y obtuvo más soldados. Por lo demás, el método era seguir empleando la artillería, la aviación y el alto explosivo. De esta forma las operaciones siguieron sucediéndose una tras otra:

Gracias a toda esta ayuda y esfuerzo, el gobierno de Saigón fue recuperando buena parte del territorio perdido los años anteriores y en 1967 en Estados Unidos se creía que la victoria estaría de su lado en no mucho tiempo. Pero la desmesurada potencia de fuego utilizada estaba resultando contraproducente en muchas ocasiones. Un aldeano comentaba:

Del mismo modo, el empleo de un arma tan devastadora como los bombarderos estratégicos B-52 causó rechazo en buena parte del mundo, incluido en el propio Estados Unidos.

Los vietnamitas aprendieron mucho más de su oponente de aquellos reveses y decidieron seguir las siguientes pautas:
Así, la guerra de Vietnam se convirtió en una serie de larguísimos momentos de inactividad o de marcha, interrumpidos por algunos instantes de lucha sangrienta. Estas acciones tuvieron éxito dañando la moral estadounidense. Tanto el Mando de la Asesoría Militar en Vietnam, como los oficiales y soldados se sentía desmotivado por estas tácticas. Un miembro de las Fuerzas Especiales afirmó años después:

Si dura resultaba la campaña para los soldados, no lo era mucho menos para el Mando de la Asesoría Militar en Vietnam. El deseo de conseguir una batalla campal llegó a ser la particular obsesión del Pentágono, que organizaba operaciones con el fin de localizar el Cuartel General del FNLV o CGVC, ejército de Vietnam del Norte enviado al Sur. En su mente seguía fija la idea de que los guerrilleros defenderían aquella valiosa posesión con ahínco y, por tanto, tendrían una oportunidad para destruirlos. Pero por más operaciones que llevaron a cabo, el CGVC nunca apareció, suponiendo que el CGVC no fuera en realidad una oficina en Hanói. Nuevamente se hacía cierta la metáfora de Ho Chi Minh de la lucha entre el tigre y el elefante. Esa metáfora encierra la esencia cruel y a veces atroz de aquella guerra, como suelen ser todas las guerras de guerrillas.Un miembro del FNLV lo explicó así:

Asimismo la cita contiene otra de las bazas que supo jugar el pueblo vietnamita: la utilización del terreno en su propio beneficio. En la jungla podían ocultarse sin ser vistos, ni siquiera con visores Starlight o de infrarrojos. Sabían utilizar las ventajas que ofrecía la hostil selva, algo que los estadounidenses no llegaron a comprender del todo, como demuestra el deseo de terminar con la vegetación con defoliantes o convertir el terreno en un cenagal baldío a base de bombas.

Puesto que la flota de Estados Unidos hacía imposible el abastecimiento por mar, desde 1966 Vietnam del Norte decidió reforzar, ampliar y utilizar profusamente la ruta que abrió en 1959. Pese a que se ha sobrevalorado su importancia, ésta ruta constituyó una pieza clave en la victoria del Norte gracias a los suministros transportados por ella, como también por el acceso que proporcionó al EVN al interior de Vietnam del Sur. Nunca pudo ser cortada ni detenida totalmente, pese a utilizarse todo tipos de técnicas, desde los bombardeos masivos hasta el sembrado de sensores inteligentes que detectaban las vibraciones producidas al caminar por personas o su sudor; pero por los animales, la vegetación, el clima, los innumerables caminos y la perseverancia de los vietnamitas, todos los esfuerzos resultaron inútiles. Con el tiempo, la Ruta fue sembrándose de zonas donde descansar y reponerse, además de cultivar alimentos para aliviar la presión sobre las mercancías transportadas. Estos centros fueron objetivos de bombardeos, de ataques por parte de mercenarios contratados por la CIA e incluso de incursiones en Camboya y Laos (ver más adelante). Pero nuevamente volvieron a resultar inútiles. Con el avance de la Guerra, la Ho Chi Minh fue una de las piezas claves para poder lanzar la ofensiva del Tet, después la ofensiva de Pascua y por último la ofensiva de primavera, que terminó con Vietnam del Sur. Incluso sería la vía de infiltración para ocupar Laos años más tarde y convertirlo en un protectorado vietnamita de facto.

Pese a las bajas y las manifestaciones en contra, la sensación mayoritaria entre los estadounidenses era de ir por el buen camino. Existían informes de inteligencia anunciando una gran ofensiva comunista, pero dichos informes no eran lo suficientemente claros o fiables, ya el año anterior se había lanzado una gran operación, la Cedar Falls, a raíz de otra también gran operación de inteligencia, la Rendezvous, y no se consiguieron más contactos con el FNL de los habituales. Con estos antecedentes las acciones de 1968 fueron una sorpresa para prácticamente todos los militares, políticos y analistas estadounidenses y dieron al traste con todas las expectativas estadounidenses de ir ganando.

El 21 de enero de 1968 dos divisiones del EVN y efectivos del FNLV comenzaron un fuerte bombardeo sobre la base de Khe Sanh, que permaneció sitiada durante 77 días. Pronto la prensa y el propio presidente Johnson realizaron paralelismos con Dien Bien Phu, la gran derrota francesa en Indochina. Los distintos informes negaban que las dos situaciones se pareciesen, pero el presidente se mostró muy preocupado ante la posibilidad de perder la Base y verse frente a una derrota de gran repercusión mediática.

El Mando de la Asesoría Militar en Vietnam realizó un esfuerzo considerable por mantener la posesión en su poder. No dejó de mandar suministros, cuando los aterrizajes fueron imposibles, desarrollaron la salida de la carga con paracaídas, llevaron a cabo bombardeos masivos, socorrieron a los sitiados movilizando unos 30 000 efectivos por medio de operación como la Pegasus, Los marines tomaron las colinas que rodeaban las instalaciones para no repetir la experiencia francesa... Parecía que aquella lucha sería una de las pocas de gran envergadura que las mermadas fuerzas guerrilleras podían realizar tras casi tres años de contienda.

El esfuerzo en mantener Keh Sanh fue tan grande que Westmoreland y su estado mayor decidieron abandonar la posición de Lang Vei a su suerte, aun cuando el EVN decidió utilizar por primera vez vehículos blindados. Los nueve Boinas Verdes y los cientos de montañeses no pudieron aguantar el ataque ante la inexistencia de refuerzos provenientes de Keh Sanh, siendo esta quizá la única batalla perdida por Estados Unidos en toda la contienda.

El sitio no terminó finalmente con una carga o un combate cuerpo a cuerpo; sino, según el oficial de marines Willian N. Dabney, con los atacantes calcinados por las bombas de napalm. Aun con todo el esfuerzo realizado, el 5 de julio la base se abandonó. La razón esgrimida fue: ya no resultaba necesaria tras haber retenido allí considerables tropas del EVN y el FNLV que, de otro modo, podría haber causado mucho daño. Por supuesto el abandono del lugar levantó críticas sobre la utilidad de desplazar tantos hombres y material para defender una posición innecesaria, esfuerzos que hubiesen sido útiles para disminuir los efectos de la ofensiva del Tet. Fuera como fuese, el sitio supuso una inversión del apoyo popular a la política de Johnson en el Sureste asiático.

A finales de enero de ese año, cuando se celebraba el año nuevo vietnamita —la festividad del Tet—, 38 de las 52 capitales provinciales de Vietnam del Sur fueron atacadas, y muchas prácticamente tomadas. Saigón estuvo en estado de sitio, la propia embajada de Estados Unidos fue asaltada por un comando suicida que casi llegó al interior del edificio y Hué, la antigua capital del Imperio vietnamita, cayó en poder de los rebeldes, tardando varios días en ser recuperada, tras lo cual se descubrió la llamada masacre de Hué con miles civiles asesinados sistemáticamente por los norvietnamitas.

La sorpresa fue total para los estadounidenses y para el ERVN, pese a los informes advirtiendo de la movilización. La inteligencia militar no pudo obtener información clara y concisa de lo que estaba pasando ni de lo que se avecinaba. Sin embargo la ofensiva también guardaba una pequeña sorpresa para el mando norvietnamita: los soldados del Sur resistieron el ataque con pocas deserciones y ganaron varias luchas encarnizadas.

Pronto la situación se invirtió. El poder aéreo barrió casi por completo a los guerrilleros del FNLV, unos 40 000 muertos según los estadounidenses, y pocos días después todo el territorio ganado por los guerrilleros era recuperado, habiendo perdido el EVN buena parte de los efectivos que tan penosamente consiguió llevar al Sur. La ofensiva del Tet volvía a ser un fracaso como también lo había sido 16 años antes. Pero peor resultó la situación para el FLN. Pese a que lanzaron posteriormente la llamada «ofensiva del mini Tet», la mayoría de sus efectivos habían caído muertos o prisioneros debido a las órdenes emitidas por Giap de resistir en sus posiciones. Las guerrillas desaparecieron del campo en los años posteriores y poco después el Programa Phoenix se ocuparía de descabezar el mermado FLN, en ocasiones literalmente.

Mucho se ha discutido sobre si la orden de resistir dada por Giap al FNLV ante un enemigo mucho más poderoso fue un error o un plan preconcebido. Giap había perdido antes muchas batallas contra los franceses y tuvo problemas para conservar su puesto en el Viet Minh tras algunas derrotas, como la del Vinh Yen. Sin embargo, el politburó de Hanói siempre consideró al FNLV como un aliado poco cómodo, aunque solo fuese por el hecho de ser una fuerza potencialmente independiente con base en el Sur. Autores como han indicado la posibilidad de que fuesen órdenes dadas con el fin ulterior de inmolar dicha fuerza. Sin embargo, el hecho de perder a miles o decenas de miles de personas era una práctica ya practicada por Giap y Ho Chi Minh desde la independencia, como asesinar a miles de "terratenientes" o provocar la huida de un millón de católicos. Así lo reconoció Giap con la frase autocrítica: "recurrimos al terror que se extendió en demasía", por lo que implícitamente se admitía un nivel de terror aceptable. El recurso de los asesinatos masivos se constató nuevamente tras recuperar Hué, donde descubrieron fosas con 3000 asesinatos perpetrados por el EVN con el fin de terminar con cualquier organización que no fuese la impuesta por ellos. Por lo tanto, no existe consenso sobre si la orden se debió a un plan para terminar con el FNLV o fue fruto del desinterés por la vida de sus hombres y la propia ineptitud de Giap en asuntos tácticos.

Paradójicamente, una victoria militar como la del Tet hizo ver a los estadounidenses que sus enemigos no solo podían dar un buen susto a sus soldados; sino que conservaban la capacidad de atacar cualquier lugar de Vietnam del Sur, incluso su embajada. ¿Habían resultado inútiles tantos bombardeos, tres años de lucha con abundantes muertos, la riada de millones enviados, y la multitud de manifestaciones y contramanifestaciones?

De poco sirvieron los comunicados sobre el gran número de bajas infligidas al FNL y al EVN, la resistencia que demostró el ERVN o los hallazgos de las Matanzas de Hué. Las manifestaciones de protesta se multiplicaron. Mucho más cuando en 1969 se hicieron públicos los sucesos acaecidos un año antes en el pueblo de My Lai, donde el ejército estadounidense pareció seguir el comportamiento de los nazis en Oradour-sur-Glane. Un acicate que dejaba a pocos indiferentes, especialmente al constatar que el sistema para medir el cumplimiento de los objetivos podía haber convertido al acto de My Lai en la punta del iceberg.

Pero si en Estados Unidos la población estaba dividida, en el sureste asiático la moral era muy baja, hasta el punto de que autores como lo han denominado «El colapso de la moral» por motivos como:
Los militares estadounidenses hicieron esfuerzos por atender y cuidar a sus hombres donde quiera que sirviesen. Estados Unidos siempre se ha enorgullecido de abastecer bien a sus soldados llevándoles cervezas frías, regalos de casa e incluso periódicos. Dicho esfuerzo se incrementó dándoles semanas libres en el destino de Asia que prefiriesen, contratando a investigadores para que analizaran los problemas raciales, llevándoles a estrellas de la música y el humor, etc., pero la moral seguía muy baja, por lo que Lyndon Johnson relevó a Westmoreland y ordenó los primeros planes de retirada.

Pese a que este término y esta idea ya la planteó el presidente John Fitzgerald Kennedy a principios de los años 1960, no fue hasta la victoria de Nixon cuando comenzó a llevarse a la práctica por el analista Henry Kissinger. La vietnamización perseguía fortalecer y preparar al ERVN, para luego traspasarle la responsabilidad de defender el territorio del Sur. Al mismo tiempo debía crear un contexto para desahogar al régimen del presidente Thieu del acoso constante al que le sometían el FNLV y el EVN. Esto se realizó dando instrucción a los vietnamitas y "entregar ingentes cantidades de armas al ejército de Nguyen van Thieu". Además, la vietnamización supondría para Washington y Saigón una posición más fuerte de cara a unas futuras negociaciones con los comunistas, ya iniciadas en secreto por Kissinger en París durante febrero de 1969.

Se discute si, tras la Ofensiva del Têt en 1968, el presidente Johnson decidió el progresivo abandono del conflicto o si esta decisión se tomó unos meses después, tras la batalla de la Colina de la Hamburguesa. Lo que indudablemente sí sucedió fue la percepción de no contar ya con la opinión pública. Pese a todo, los envíos de tropas continuaron y en 1969 se aumentó el número de estadounidenses a más de 500 000; pero para entonces el Presidente ya sabía que aquella guerra le había costado la reelección y anunció su abandono de la política.

En enero de 1969 y Richard Nixon fue elegido nuevo presidente. Los ejes sobre los que basaría su política vietnamita serían:

El segundo punto del proyecto lo fue cumpliendo progresivamente. No se puede decir lo mismo de los demás. Este hecho, el prometer una cosa dentro de un tema de capital importancia, hacer exactamente lo contrario y volver a ganar las elecciones, ha quedado como ejemplo en muchos estadounidenses de cómo un gran "vendedor de autos" puede arrastrar a todo un pueblo. También prueba la determinación de Nixon para no ser el primer presidente de Estados Unidos en perder una guerra, incluso con bombardeos superiores a los de la Segunda Guerra Mundial.

Otro problema pendiente fue la recopilación de abundante información sobre la organización y disposición de las fuerzas enemigas. Hacia 1969 la CIA, cuyos agentes llevaban mucho tiempo insistiendo en que aquella guerra no podía ganarse por medios convencionales, ya tenía depurado su Programa Phoenix que había comenzado en 1967. Dicho programa pretendía terminar con sus enemigos de una manera más selectiva que con bombardeos y explosivo de alto poder. Pero, pese a los esfuerzos de varios mandos y oficiales, el Programa Phoenix terminaría siendo más terrorismo de estado que una fuente de información fidedigna, pues los agentes de la CIA tuvieron poco menos que carta blanca para matar a cualquier persona considerada miembro del FLN.

Mientras, el Ejército de los Estados Unidos llevó a cientos de oficiales del ERNV a cursos de instrucción para mandos, pilotos y personal de mantenimiento del costoso material que les regalaría. Con todo su fuerza aérea se colocó en la cuarta más grande de Asia. Pero los progresos resultaron muy lentos y se veían entorpecidos por la corrupción crónica o la selección de mandos según los "compromisos" de los dirigentes políticos, no por sus cualidades militares. En esta misma línea los oficiales estadounidenses comenzaron a ver que regalarles helicópteros y sustituirlos cuando fueran derribados no conducía a nada si los pilotos continuaban teniendo una capacitación mediocre a lo sumo. Pese a todos estos fallos, la retirada de tropas comenzó en 1970, empezando por el personal de infantería para terminar con los pilotos de los que siempre estaba necesitado el ERVN.

Al mismo tiempo un problema más estaba creciendo. Los dos neutrales vecinos de Vietnam del Sur, Laos y Camboya, se mostraban incapaces de contener la agresión de sus guerrilleros comunistas ni cortar la Ruta Ho Chi Minh. Si Estados Unidos pretendía que su aliado pudiera sobrevivir a una guerra con el Norte debía cortar esa vía de infiltración y, de paso, terminar con el Cuartel del ejército norvietnamita o del FLN ubicado en Camboya, según suposiciones de la inteligencia estadounidense.

En marzo de 1969 Richard Nixon, recién elegido, inició una campaña de bombardeos secretos sobre Laos y Camboya. Con el nombre en código de Operación Menú, la Fuerza Aérea atacó los dos países con el máximo secreto. Los pilotos debían despegar, ir a una posición determinada y esperar órdenes. Una vez allí los controladores les daban las coordenadas que debían atacar. A la vuelta, los mismos controladores deberían destruir todo documento sobre estas incursiones en territorio neutral. Pese a todas las precauciones, en menos de un mes "The New York Times" ya publicaba noticias sobre estos ataques, filtradas por miembros de la Fuerza Aérea disconformes con estas operaciones. Laos fue la nación más bombardeada de la Tierra, con más de 2 500 000 bombas de todos los tamaños. Estos bombardeos perseguían cortar la Ruta Ho Chi Minh, pero también demostrar a Vietnam del Norte que la nueva presidencia estaba dispuesta a todo con tal de terminar con aquella guerra, incluso la opción nuclear. Pero los vietnamitas del Norte no se amedrentaron.

El 14 de abril de 1970, el ERVN realizó una primera incursión en territorio camboyano y el 29 de abril el teniente general Do Cao Tri lanzó a sus 12 000 hombres sobre el "Pico de Loro" (véase mapa). Pero sería el 1 de mayo cuando el general Robert Shoemaker ordenó a los oficiales destacados en la frontera con Camboya avanzar sobre el "Pico de Loro" y "el Anzuelo". Algunos soldados aceptaron con resignación participar en estas incursiones, pero la mayoría vio con regocijo el poder golpear el santuario del FNLV y vengar todos los muertos que habían llegado flotando por el río Mekong.

La incursión estuvo precedida de grandes bombardeos que causaron muchos muertos entre los campesinos, lo que a la larga fue terrible para el gobierno pro-occidental de Camboya. Nixon era consciente de las repercusiones que traerían aquellas acciones; pero, como él mismo había declarado, prefería perder la reelección a ser el primer presidente en perder una guerra.

Las acciones en El Anzuelo encontraron alguna resistencia del EVN que, como era la costumbre, desaparecía en la selva tras un breve tiroteo. Ni siquiera en el pueblo de Snuol hubo amago de oponerse a la potencia de fuego desplegada por los M60 Patton. Pronto los emplazamientos de artillería del EVN fueron capturados y se enviaron cien M551 Sheridan que sí encontraron resistencia; pero la vencieron pronto. Saquearon el poblado, interrogaron a los campesinos y finalmente lo arrasaron. En esos interrogatorios los camboyanos informaron que había toda una ciudad guerrillera en la jungla. Poco después un helicóptero Loach avistó una casa bien camuflada y comenzó el bombardeo de artillería y aviación. Cuando los infantes pudieron entrar en lo que ellos mismos llamaron "La Ciudad", encontraron 400 cabañas de paja y 180 escondites con suministros médicos, alimentos y ropa, además de 480 fusiles y 120 000 cartuchos.

El 30 de junio, todos los soldados volvieron a sus bases, dejando graves pérdidas a la 9ª División vietnamita, encargada de la defensa en retaguardia. Por el otro bando se contabilizaron 354 estadounidenses muertos y 1689 heridos. El ERNV dijo haber perdido 866 hombres y tener heridos a otros 3274. El Presidente anunció la muerte de 11 349 enemigos y la captura de suficientes suministros y armas para cubrir las necesidades de todo un año, aunque la propia CIA calificó ese recuento de "altamente sospechoso". Para Nixon era como un regalo de Navidad y ordenó el envío de 31 000 soldados más a Camboya para destruir todo lo que no se pudiera transportar. Sin embargo, el famoso cuartel del EVN para Vietnam del Sur (el COSVN) no apareció y sí fuertes manifestaciones en Estados Unidos, siendo la de Kent State la más dura de todas..

Por contra, los bombardeos de la Operación Menú y los de la posterior "Freedom Deal" causaron mucho daño y mucho rencor, colocando a la población camboyana en contra de su gobierno, aliado de los Estados Unidos. La USAF prosiguió los bombardeos en torno a la capital camboyana para impedir a los Jemeres Rojos tomarla, pero la caída de Nom Pen era cuestión de tiempo en cuanto las bombas cesaron. Además, el apoyo del depuesto príncipe Norodum Sihanouk a los sanguinarios Jemeres Rojos legitimó a estos en su insurgencia, que databa de 1969 en el norte del país. Con esa base de odio hacia un gobierno indolente y corrupto, el terreno estaba abonado para la llegada de Pol Pot, sus genocidas seguidores y los campos de la muerte.

En 1971 Laos era considerado el país más atravesado por la Ruta Ho Chi Minh, por lo que debía ser golpeada con gran contundencia. El 18 de enero de 1971, aniversario de una famosa victoria vietnamita sobre los chinos en 1427, el alto mando militar sudvietnamita ordenó al general de división Le Truong Tan comenzar la Operación Lam Son 719. Tenía como objetivos desbaratar cualquier posible ofensiva comunista sobre Vietnam del Sur durante todo un año y lograr con ello:

La Operación perseguía abrir un corredor de 25 km de ancho por 35 de largo entre la frontera de Vietnam del Sur y la ciudad laosiana de Tchepone. Eso cortaría la Ruta Ho Chi Minh, permitiría capturar abundante material y detendría las operaciones de los guerrilleros en el Sur. Se destinaron las mejores divisiones sudvietnamitas, como los marines, paracaidistas y Rangers, además de vehículos blindados. Todo bajo el mando del general Xuan Lam. Por desgracia para los sudvietnamitas, el EVN esperaba un ataque así y concentró cerca de la frontera un cuerpo de ejército formado por las divisiones 304, 308 y 320, más un regimiento acorazado y otro de artillería.

Cuando la ofensiva no había pasado la mitad del camino planificado, las numerosas bajas sufridas en la Carretera 9 y en las colinas al norte de Laos obligaron a detener el avance survietnamita. Poco después comenzó una evacuación con helicópteros de apoyo que terminó en una retirada. En este punto hay discrepancias ente los autores. Para ninguno de los dos bandos logró una victoria decisiva, pues los survietnamitas llegaron a Tchepone, pero la encontraron casi desierta. Para "no hay ninguna duda de que el intento de atacar el saliente laosiano fue un desastre", porque, si la incursión fue mal, la retirada resultó pésima, terminando en una desbandada y en una carnicería.

Las imágenes de decenas de helicópteros regresando a Vietnam del Sur atestados de soldados gravemente heridos, algunos atemorizados, echó por tierra las esperanzas de poder contar con el ERVN para defender Vietnam del Sur. Lan Som 719 había costado casi 9000 hombres. Sin embargo, dos años después, los sudvietnamitas demostraron que aún les quedaban cartas por jugar frente al mismo enemigo que tan duramente los había expulsado de Laos.

A las 02:00 horas del 30 de marzo de 1972, 12 000 proyectiles de la artillería y misiles del EVN atacaron las posiciones del ERVN en la Zona Desmilitarizada, un bombardeo parecido al de Khe Sanh. Seguidamente las divisiones 304 y 308 junto a 300 blindados se lanzaron contra las posiciones survietnamitas con el fin de arrollarlas cercar Quang Tri y volver a ocupar Hué, repitiendo el éxito de 1968. A estas acciones las llamaron la Ofensiva de Pascua.

El cinco de abril, desde Camboya, otro contingente avanzó por la región del "Anzuelo" y el "Pico de Loro", cercando las ciudades de An Loc y Tay Ninh, en el camino a Saigón. Una tercera oleada salió del sur de Camboya para infiltrarse en el delta del Mekong. Con todo, esto solo resultó un señuelo para distraer la atención del ataque principal, lanzado días después en el centro del país, sobre la ciudad de Kontum.

Las imágenes de carreteras inundadas por desplazados, aviones de transporte tratando de levantar sus rampas con hombres colgados de ellas y vehículos atestados de asustados vietnamitas, parecían dar la idea de que aquel régimen terminaría en pocos días. Giap lanzó sobre el Sur la práctica totalidad de su ejército con la intención del aterrorizar a sus enemigos y dar el golpe de gracia al régimen de Saigón. Sin embargo la realidad resultó diferente. Fue necesario una crisis así para que el timorato presidente Thieu relevara del mando al general Giai y al teniente general Ngo Dzu, para colocar al frente de sus hombres a Ngo Quang Truong, calificado por algunos como el mejor oficial de Vietnam del Sur. Este detuvo las retiradas y ordenó que todos los desertores y saqueadores fueran ejecutados. Con el nuevo mando y, quizá, con el miedo a nuevas matanzas sistemáticas, Hué pudo ser salvada, al mismo tiempo que Kontum y An Loc resistieron un ataque tras otro. Todo esto aumentó la confianza de los soldados en su ejército.

Al otro lado del Pacífico, Nixon declaró que lanzaría un ataque como el que jamás habrían visto y lo cumplió. Los 700 aviones desplazados al sureste asiático, los buques fondeados en las aguas de Vietnam del Sur y los B-52 de Tailandia y Guam realizaron un bombardeo que detuvo en unas ocasiones y desintegró en otras a las unidades del Norte. Giap volvió a su táctica de lanzar oleada tras oleada cosechando parecidos resultados a los de Dien Bien Phu.. El poder aéreo estadounidense aniquiló a buena parte de los efectivos norvietnamitas y los tanques T-54 recién traídos de la URSS fueron destrozados por los cazas o por los soldados del ERVN con sus lanzacohetes portátiles M72 LAW.

Finalmente las incursiones del EVN se detuvieron tras haber incrementado el terreno en su poder del 3,7 % al 9,7 %. Las pérdidas para Hanoi rondaron los 190 000 muertos, heridos o prisioneros, por lo que pasó el resto de 1972 tratando de mantener el terreno conquistado. Pese a todo, el 15 % de estas conquistas las perdería en los siguientes años frente al ERVN luchando ya en solitario.

Ciertamente el programa de vietnamización había logrado éxitos como:

Sin embargo el EVN y el FNLV habían logrado por su parte:

Los logros obtenidos por los vietnamitas comunistas coinciden casi totalmente con lo que la Administración Nixon pretendía evitar. Autores como afirman que Saigón podía resistir, pero necesitaba el apoyo aéreo estadounidense. Esta opinión parece confirmarla las órdenes de Nixon para bombardear masivamente a Vietnam del Norte y minar tanto los puertos como los estuarios, acciones casi desesperadas para obligar a Vietnam del Norte a negociar, frente a unas elecciones cada vez más cercanas. Así Nixon podría presentarse como el "pacificador", término que le gustaba.

Pese a lo que los acontecimientos demostraron después, en 1972 y 1973 la derrota del Sur no estaba clara para ninguna de las dos partes. Por un lado combatían ya solos, pero por el otro recuperaban territorio y Estados Unidos les había entregado 2500 millones de dólares en armas y municiones, suficiente para resistir varios años. Tampoco las circunstancias internacionales se lo ponían fáciles a ninguno de los dos bandos. Pese al generoso arsenal que habían dejado los estadounidenses, su salida de la guerra redujo en dos ocasiones consecutivas las ayudas económicas al régimen de Saigón, primero Nixon las bajó a mil millones de dólares anuales y, tras su dimisión en agosto de 1974, el Congreso las dejó en 700 millones. Este recorte aumentó aún más en 1975, lo que obligó a dejar en tierra 200 aviones, la mitad de la fuerza aérea survietnamita.

La crisis del petróleo incrementó el precio de los alimentos y otros productos de primera necesidad en todo el Sur, obligando a muchos soldados a realizar trabajos extras fuera de las filas o a dejar su puesto para poder ganar el sustento de su familia. En cualquier caso supuso una merma de tiempo para entrenamiento y operaciones.

En el Norte las cosas no marchaban mucho mejor. La política de acercamiento a China emprendida por Estados Unidos, la famosa Diplomacia del Ping Pong de 1971 con la visita del propio Nixon a Pekín al año siguiente, hacía pensar en una disminución de la ayuda militar del gigante asiático a Vietnam del Norte. La URSS también bajó los fondos para el armamento regalado. Debía preocuparse de otros temas como la seguridad de su frontera con China, en la que llegaron a darse enfrentamientos esporádicos.

Nixon se mostró implacable con los bombardeos para obligar a los norvietnamitas a sentarse a la mesa de París cada vez que abandonaban la negociación. Se negociaron todos los detalles para que pareciera una paz honrosa, mientras continuaban los bombardeos y los combates.

El 8 de mayo de 1972 Richard Nixon suspendió las negociaciones de París por los continuos ataques del EVN y ordenó la Operación Linebacker con el fin de minar los puertos, destruir objetivos militares, vías férreas, instalaciones petrolíferas, aeródromos y los muelles de todo Vietnam del Norte. En esta ocasión los Phantom y los B-52 iban equipados con bombas inteligentes guiadas por láser, que tan famosas se harían en conflictos posteriores, y afirmaban atacar únicamente blancos militares o económicos, nunca zonas habitadas por civiles. De esta forma, decían ellos, la ferocidad de las bombas se vería compensada con su precisión. Esta vez se lanzaron 155 548 toneladas de bombas en 41 000 misiones. Las fábricas fueron casi destruidas por completo, lo mismo que las vías férreas, incluido el famoso puente de Thanh Hoa, las ciudades aún intactas de Hanói y Haiphong tampoco se salvaron. Sin embargo lo vietnamitas lo veían de una forma muy diferente, como comentaba un miembro de su comunidad:

El Presidente tenía muy presente que la Operación Rolling Thunder había desgastado mucho a su antecesor y una campaña mucho más dura haría lo mismo con él; pero era un hombre enérgico y no dudó en ordenar la salida de los B-52. Según las fuentes occidentales los bombardeos no perseguían solo llevar a Vietnam del Norte de nuevo a la mesa de negociaciones, sino demostrar a Vietnam del Sur que les seguirían apoyando pese a retirar sus soldados. Desde el punto de vista estadounidense las operaciones Linebacker menguaron la moral vietnamita y el gobierno de Hanói comenzó a pensar en volver a negociar. Ciertamente la situación en la que los aviones estadounidenses colocaron al pueblo vietnamita fue muy dura, un vietnamita relataba:
El 23 de octubre los bombardeos cesaron y se retomaron las negociaciones. Cuando los norvietnamitas volvieron a sentarse, Nixon lo presentó como una victoria; pero lo cierto es que Hanoi no cambió sustancialmente sus exigencias que obligaban al Sur, entre otras cosas, a no poder reconquistar territorio ni les exigía a ellos abandonar lo tomado. Pese a ello, unos meses después, los jerarcas de Vietnam del Norte se retiraron nuevamente. Por aquellas fechas habían recibido de la URSS misiles SAM (acrónimo en inglés de Surface-to-Air Missile, superficie-aire) y confiaban en presentar resistencia a los nuevos ataques. Nixon reanudó los bombardeos, la conocida extraoficialmente como Operación Linebacker II. Entre el 18 y el 29 de diciembre de 1972 cayeron 20 370 toneladas de bombas, matando a 1000 personas, deteniendo las comunicaciones internas, dañando la red eléctrica y terminando con la totalidad de la fuerza aérea norvietnamita. Solo se detuvieron el día de Navidad. Con todo, el precio fue alto para la USAF. Los norvietnamitas derribaron 26 aviones, quince de ellos B-52, y capturaron a varios pilotos, lo que aumentaba algo su margen de negociación en París porque la opinión pública estadounidenses siempre se mostró muy preocupada por el paradero de los pilotos desaparecidos en combate. Para Nixon se habían logrado casi todos los objetivos, para muchos vietnamitas la conclusión fue otra:

El 27 de enero de 1973 la delegación de Vietnam del Sur, la norvietnamita, la estadounidense y la del Gobierno Provisional de la República de Vietnam del Sur (el FNLV o Vietcong) firmaron los Acuerdos de paz de París. El documento se componía de 23 artículos con las misiones de cada bando. Fue arduamente preparado hasta en los más mínimos detalles. Lo firmado suponía:

Estos acuerdos daban a Estados Unidos un respiro. Con el final de su participación en la Guerra ahorraba unos 8100 millones de dólares y una gran tensión interna. Sin embargo para Vietnam, tanto del Norte como del Sur, no era más que una pausa en la lucha. Una survietnamita comentó en Saigón:

Por su parte, el gobierno de Saigón anunció que no permitiría elecciones en su territorio y acogió la noticia con indiferencia, convencido de afrontar un ataque del Norte. Estados Unidos había prometido continuar ayudándolo económicamente, pero dicha promesa quedó en poco menos que papel mojado tras la dimisión de Nixon por el caso Watergate, ya que Gerald Ford deseaba olvidar la guerra cuanto antes, igual que muchos estadounidenses.

Pese a que la victoria no se veía a corto plazo y a que los hombres de Giap no podían emprender una gran ofensiva tras las pérdidas cosechadas en la de Pascua, sí había indicios de que lo tomado en dicha ofensiva, y fijado en los Acuerdos de Paz de París, constituía una base sólida para el ataque final porque:

Lo que trataba de conseguir Vietnam del Norte era una posición más fuerte hasta recuperarse para la campaña final. Sin embargo, el general norvietnamita Tran Van Tra pedía una gran acometida. Él insistía en que se podía conseguir una victoria rápida partiendo de las Tierras Altas Centrales para tomar la ciudad de Pleiku y después cortar su conexión con Ban Me Thuot, algo parecido a lo intentado en 1965. En un principio se aplazó la petición, pero finalmente Hanoi decidió comenzar la ofensiva, y el general Van Tieng Dung fue enviado al Sur para preparar todas las actuaciones. Así, a principio de 1974 son atacadas las zonas de Quang Nam y Quang Ngai, en mayo se registraron intensos combates en Ben Cat y en la primavera de 1974 el EVN había recuperado lo perdido en el delta del Mekong; pero Thuong Duc fue reconquistada por el ERVN.

El 1 de marzo de 1975, el EVN cortó los enclaves terrestres con Ban Me Thuot, la ciudad cayó el 13 de ese mismo mes. El ataque hizo tomar al presidente Thieu dos de tantas decisiones equivocadas, pero que en aquellos momentos resultaron extraordinariamente trágicas:

La retirada se convirtió en una desbandada. La presión del ejército enemigo, el pánico de los civiles que huyeron aterrados y la ineptitud del mando ante quizá la operación más difícil que se le pueden pedir a un oficial, minaron por completo la cohesión y espíritu de lucha de los soldados. Estos huyeron entre la multitud que bajaba despavorida en lugar de defender las ciudades citadas. En un intento de evitar una derrota catastrófica, el presidente del Sur decretó en marzo la movilización general para tratar de contener la ofensiva que muy pocos veían remediable. El esfuerzo resultó inútil, Hué cayó el 25 de marzo y Da Nang el 30, perdiendo dos de las mejores unidades del Sur, la División de Infantería de Marina y la 1ª División. Las Tierras Altas Centrales cayeron en poder del Norte dos días después tras cundir el pánico en ellas.

Como reconoció posteriormente el general Van Tieng Dung, aquel fue un golpe de suerte con el que no contaban. Ante estas noticias, Le Duc Tho y los militares a las órdenes de Giap enviaron sendos cables aprobando la movilización solicitada por Dung. Finalmente se optó por atacar la región de Tay Nguyen al tener el Sur solo dos divisiones diseminadas; pero ni siquiera estas ofrecieron gran resistencia. Todo el país era un caos.

Al gobierno de Saigón solo le quedaba jugar la carta de luchar en las provincias del sur, las más ricas, a la espera del monzón que detuviera o ralentizara todo. De aguantar hasta las lluvias Saigón ganaría tiempo para conseguir apoyo aéreo estadounidense; pero en esta ocasión solo lograron buenas palabras, mientras el FNLV organizaba un Gobierno Revolucionario Provisional.

Aquel desmoronamiento en la parte norte del país y en las Tierras Altas Centrales cambió la percepción de Hanói sobre una victoria para 1976. También lo cambió para Saigón, que trató de entablar negociaciones con los comunistas. Estos exigieron y consiguieron la desaparición de Thieu de la escena política, dejó el poder el 21 de abril siendo sustituido por el general Duong Van Minh. A finales de marzo, el Buró Político se reunió nuevamente y decidió lanzar la Ofensiva de Primavera, llamada por ellos "Campaña Ho Chi Minh". Dung recordó el discurso lanzado tras la reunión:
El 22 de abril, varios aviones A-37 capturados al enemigo volaron hasta Tan Son Nhut. Valiéndose de su apariencia, atacaron la torre de control y destruyeron numerosos cazas. El humo pudo verse desde Saigón, con la consiguiente sensación de pánico. Mientras, unidades enteras se rendían al paso de los comunistas que avanzaban tomando una ciudad tras otra bajo el lema:
A las 00:00 del 29 de abril, la Hora H, Saigón fue atacada por todas las direcciones, excepto desde el mar. Por la zona desmilitarizada penetraron más unidades, lo mismo que desde Laos y desde el centro norte de Camboya.
Por la mañana, artillería norvietnamita bombardeó el puente Newport, la última conexión de Saigón con el mundo exterior. Tras horas de intensa lucha, la ciudad quedó completamente aislada.

En una plantación de caucho próxima a Dau Giay, aguardaba una unidad de ataque en profundidad formada por una brigada de tanques, un regimiento de infantería y otras unidades. Llevaban los vehículos camuflados con ramas, los brazos con cintas rojas para distinguirse y uniformes impecables para tomar la capital, mientras el general Cao Van Vien firmó la orden de resistir con la frase «defender hasta la muerte, hasta el final, la porción de la tierra que nos queda», poco después desertaba de su puesto y huía del país.
A las 15:00 del 29 de abril los transportes, los blindados y tanques de la unidad de ataque en profundidad salieron del bosque y llegaron a la capital aplastando toda resistencia. Al día siguiente penetraron en Saigón mientras la gente trataba de huir por cualquier medio. Tomaron el cuartel general del Estado Mayor, el Palacio de la Independencia, el cuartel general de la Zona Capital Especial, el Directorio General de la Policía y el aeródromo de Tan Son Nhut. La rapidez del avance sorprendió a los periodistas cuando recibieron la noticia de que habían penetrado en el palacio presidencial, por lo cual la tripulación de un tanque norvietnamita tuvo la cortesía de repetir el acto poco después para que lo pudiesen fotografiar. Saigón había caído.

Muchas personas trataron de huir en balsas y botes por mar, creando un problemas para las otras naciones que no estaban muy predispuestas a recibirlos. Algunas se suicidaban para evitar posibles represalias de los norvietnamitas, especialmente las que habían abandonado el Norte por cuestiones políticas o religiosas 25 años antes, mientras otras saqueaban todo lo que podían. Según Jonathan , Vietnam del Sur era un país en descomposición, carente de autoridad o incluso conciencia de país.

Estados Unidos inició la "Operación Frequent Wind" con el fin de sacar a su personal diplomático, sus ciudadanos y colaboradores vietnamitas, como el presidente Ky. Los dos portaaviones situados cerca de Saigón no daban abasto. Algunos helicópteros survietnamitas aterrizaron en dichos barcos sin que nadie los hubiese invitado. Los marinos tuvieron que arrojar al mar varios aparatos para dejar espacio en cubierta. Todo esto fue televisado y fotografiado, "venciendo la prepotencia del ejército más poderoso del mundo".

Los comunistas subieron las escaleras del palacio con sus banderas. Llegaron al despacho del presidente y entraron. Con cierta dignidad Minh dijo:

La respuesta fue:

Al reconstruir el escenario los historiadores se siguen haciendo la pregunta ¿el presidente Johnson arrastró a Vietnam a la guerra o se vio arrastrado por sus consejeros? El primer punto de vista es defendido por , para quien Vietnam del Sur no quería la guerra, sino la reunificación. Fueron los temores estadounidenses quienes le llevaron a continuar un enfrentamiento hasta la derrota total. La otra línea de pensamiento la soportan autores como el ex consejero presidencial John Kenneth Galbraith para quien Johnson no deseaba inmiscuirse tanto en Vietnam, pero el peso de sus consejeros para que interviniese fue demasiado grande.

Para entre otros, Estados Unidos en general y su ejército en particular tuvieron buena parte de culpa. Pese al extraordinario esfuerzo realizado y a la sensación inicial de triunfo, Estados Unidos no comprendió del todo el tipo de guerra y el tipo de pueblo contra quien luchaba. Así aquel atacaba donde su enemigo podía encajar mejor los golpes, en las bajas humanas, mientras se desgastaba un poco más cada vez. Una línea de pensamiento similar la defendió Robert McNamara quien, pese a ser uno los primeros y más fervientes defensores de la intervención, comenzó a tener dudas en 1966 y a plantearse abiertamente la imposibilidad de ganar ya en 1967. Según él, la iniciativa de los combates la llevaban los comunistas; ellos podían elegir cuantas bajas sufrir y cuantas infligir a sus oponentes, de esta forma:

La CIA mantuvo una opinión similar al postular la imposibilidad de ganar el conflicto por medios únicamente militares.

Esta incomprensión se palpa en las continuas estadísticas e informes cuantitativos solicitados y manejados por los mandos, en varios casos exagerando los resultados, pero sin prestar excesiva atención a los discursos de los dirigentes comunistas, ni ganándose la confianza de los aldeanos, quienes podían proporcionarles buena información. Así los militares estadounidenses se comportaban como en cualquier guerra convencional, donde lo importante son los datos del potencial enemigo, en lugar de una guerra de guerrillas, donde lo vital es separar a los guerrilleros del apoyo popular. Al abandonar este aspecto algunos problemas no disminuyeron sino lo contrario:
El Ejército de Estados Unidos defendió su actuación alegando que había luchado bien. Según ellos, fueron otros factores como las restricciones impuestas por los políticos o la creación de una larga cadena logística las que contribuyeron decididamente a la derrota. Por su parte Harry G. los culpan de la derrota, no tanto por combatir bien o mal; sino por no haber suministrado al ejecutivo estadounidense información precisa de cómo ganar la guerra, además de no haber plasmado correctamente la situación vivida. En este caso hacen ver que muchas veces ni el propio Ejército conocía dicha situación. Pese a las toneladas de documentos incautados al enemigo en las distintas operaciones, a la dispersión de miles de sensores por la selva, al empleo de los muy sofisticados, para la época, ordenadores de tercera generación, el uso masivo de fotografía aérea y por satélite; no se llegó a conocer la situación real. Las distintas agencias de inteligencia, hasta quince a veces, no fueron conscientes de los preparativos para la ofensiva del Tet, ni la magnitud de los complejos de túneles que tanto ayudaron a ella, ni la existencia o no de un cuartel general del EVN en territorio survietnamita... Así se llegaba en muchas ocasiones a situaciones donde los agentes marcaban como blancos importantes lugares que no sabían realmente si lo eran o no; pero que en caso de serlo les haría subir puntos. Naturalmente esos lugares debían ser inspeccionados por la infantería, que se jugaba la vida por los agentes, en lugar de trabajar estos para evitar esos riesgos.

También se ha indicado la diferencia económica entre los combatientes. Para los hombres provenientes de regiones templadas, la jungla les puede resultar un lugar hostil, amiga de sus enemigos y enemiga suya, como creían los británicos en Birmania durante la Segunda Guerra Mundial. Los vietnamitas debían alimentarse de serpientes, ratas, lagartos y, cuando había suerte, arroz; por esta razón, podían sobrevivir de la selva cuando los alimentos faltaban sin que se resintiera su moral y cuando aquellos llegaban, se vivían momentos de euforia y satisfacción. Mientras, veían a los estadounidenses disfrutar de todo tipo de manjares, disponer de abundante dinero y recibir incluso cervezas frías en pleno campo, lo cual aumentaba la distancia con sus aliados y el odio de sus enemigos.

Otro factor apuntado en varias ocasiones fue la presencia de la prensa y su influencia negativa en la opinión pública. En 1965 la mayoría de los estadounidenses estaban a favor de la intervención, no fue hasta 1968 cuando los porcentajes comenzaron a invertirse. La publicación de las matanzas como la de My Lay, la presencia casi constante de la guerra en los informativos nocturnos, la revelación de los bombardeos secretos, las acciones del movimiento pacifista hablando con conocimiento de causa por tener a veteranos en sus filas o las declaraciones de algunos políticos cambiando de actitud, caso del propio McNamara, fueron presentando a la Guerra como algo injusto; siendo la subsiguiente falta de apoyo popular decisiva para la derrota. Por su parte, indica que no se puede perder lo que nunca se tuvo. Según él, las operaciones en Vietnam comenzaron sin consultar al pueblo estadounidense, pues la autorización del Congreso era para intervenir en los "alrededores de las bases", y cuando se solicitó el apoyo de la opinión pública esta se negó a concederlo, sorprendiendo a unos dirigentes convencidos de pisar la antesala de la Tercera Guerra Mundial.

Otro caso de laboriosidad sin desesperanza lo dieron los habitantes de Vietnam del Norte tras los bombardeos. Un miembro de la comunidad lo relataba de la siguiente manera:

En el aspecto político el Norte fue más o menos estable mientras en Saigón se sucedían los golpes militares y los deseosos de terminar la guerra cuanto antes, mientras los atentados y ataques se repetían sin que los sudvietnamitas pareciesen querer arriesgarse. Por supuesto había excepciones entre los soldados y los oficiales, pero constituían una minoría. Para testigos y escritores como Jonathan en este conflicto no es que la voluntad general fuese un factor de gran importancia, es que resultó el factor decisivo. Por tanto, quebrarla debió haber sido el objetivo perseguido por Estados Unidos y no lo consiguió.

Probablemente pocas guerras hayan tenido tantas repercusiones en la Historia contemporánea como el de Vietnam y también pocos han atraído más atención de novelistas y cineastas.

Los bombardeos masivos y la crueldad de la guerra retransmitida por vez primera con una libertad pocas veces repetida, comenzaron a cambiar la imagen que tenían los estadounidenses de sí mismos. La idea de un país enorme machacando a otro pequeño y la de sus soldados cometiendo matanzas fuera y dentro resultaron demoledoras, dejando aplastado el espíritu del Destino Manifiesto. En las elecciones de 1968 un presidente dedicado a las reformas sociales como Lyndon Johnson se enfrentó a fuertes desafíos por parte de dos demócratas opuestos a la guerra: los senadores Eugene McCarthy y Robert Kennedy, hermano del asesinado presidente Kennedy y asesinado también al final de la campaña. El 31 de marzo, en vista de una humillante derrota manifestada por las encuestas y de la incesante prolongación del conflicto en Vietnam, Johnson se retiró de la contienda presidencial y ofreció negociar el fin de la guerra. Más tarde, la reelección de Nixon en 1972 provocó un éxodo masivo de ciudadanos descontentos a países como Canadá.
La oposición a la guerra se extendió dentro y fuera de Estados Unidos entre la juventud, avivando el movimiento hippie que había comenzado antes. Las universidades estadounidenses fueron escenario de manifestaciones contra la implicación de Estados Unidos en esa guerra no declarada e injustificada en opinión de muchos. Hubo encuentros violentos entre los estudiantes y la policía con disparos y muertos. En octubre de 1967, 200 000 manifestantes marcharon frente al Pentágono, exigiendo la paz, siendo uno de los puntos más álgidos del movimiento pacifista. También es cierto que dicha situación coincidió con uno de los momentos de máxima prosperidad económica, lo que confería mucha seguridad a la juventud y posibilidades de cambiar de costumbres. Pero el factor principal de protesta resultó el servicio militar, obligatorio para todos los varones estadounidenses y con él la posibilidad de ser enviado a Vietnam.

El trauma de Vietnam les duró mucho más a los militares que a la sociedad en general. Las referencias a esta contienda en cualquier guion de cine que requiera ayuda del Pentágono son discutidas hasta la saciedad, incluso con amenaza de romper la colaboración si no se atiende a sus demandas. También lo fueron para los miembros de la administración Nixon que buscaron enemigos comunistas por el mundo para luchar contra ellos tras la derrota en Vietnam, apoyando militar y económicamente a dirigentes poco cualificados, como Holden Roberto, y cuando estos fallaron recurrieron a los mercenarios, alegando razones estratégicas inexistentes, para no reconocer la inquina que tenían por la derrota. Dicha derrota fue la principal causa esgrimida por políticos como Charlie Wilson para financiar a los muyahidines afganos en su guerra contra los soviéticos, aunque dicho apoyo se volvería contra ellos cuando uno de sus "protegidos", Osama Bin Laden, organizó los Atentados del 11 S y varios cabecillas más se manifestaron a favor.

Estados Unidos lanzó más de siete millones de toneladas de bombas sobre Indochina durante la guerra, más del triple de las 2,1 millones de toneladas lanzadas por Estados Unidos en Europa y Asia durante toda la Segunda Guerra Mundial y más de diez veces de las lanzadas en la guerra de Corea. 500 000 toneladas se lanzaron sobre Camboya, un millón sobre Vietnam del Norte y cuatro millones sobre Vietnam del Sur. En términos per cápita, las dos millones de toneladas de bombas que Estados Unidos lanzó sobre Laos convirtió al país asiático en la nación más bombardeada de la historia de la humanidad. El "New York Times" señaló que se lanzó casi una tonelada de bombas por cada habitante de Laos. Solo en este país, unos 80 millones de bombas, casi una de cada tres lanzadas, no explotaron y aún permanecen en su mayoría dispersas por todo el país. Esto ha provocado que vastas extensiones agrícolas no puedan cultivarse y más de 20 000 laosianos han muerto o resultado heridos desde el fin del conflicto, una cifra que aumenta en unas 50 personas cada año. Debido a que la Fuerza Aérea debía realizar muchas misiones con el fin de asegurarse financiación adicional durante la elaboración de sus presupuestos anuales, en muchas ocasiones el gran tonelaje de bombas gastado no se correspondían con el daño que provocaban.

La derrota de Saigon y sus Aliados fue proclamada como fiesta nacional vietnamita bajo el nombre "Día de la Paz", pero no trajo la paz al sureste asiático. Pocos años después la nación invadía Camboya y los hombres de las balsas (refugiados) siguieron aumentando sin que ningún país quisiera hacerse cargo de ellos. Aunque la invasión de su vecino trajo la liberación de los camboyanos del régimen quizá más sanguinario del Planeta, no logró la paz. Las luchas contra lo que quedaba de los Jemeres Rojos se prolongaron durante más de una década, con continuos anuncios de retirada que se aplazaban o no se cumplían, hasta que en los años 1990 se celebraron elecciones en aquel país (ver Historia de Camboya).

El antiguo Vietnam del Norte perdió el 70 % de su infraestructura industrial y de transportes, además de 3000 escuelas, 15 centros universitarios y 10 hospitales. Por su parte, el medio ambiente vietnamita quedó profundamente dañado por la utilización del Agente Naranja y otras armas químicas que defoliaron grandes extensiones de selva, con pocas posibilidades de recuperarse por la invasión del bambú y otras plantas. Pero peor aún fueron los efectos para la población en contacto con esas sustancias, aparentemente inocuas para los humanos, con miles de abortos prematuros, nacimientos con malformaciones, y esterilidad, especialmente dolorosa para las mujeres de medios rurales. A esto debe añadirse todos los hijos ilegítimos de rasgos caucásicos y africanos dejados en la pobreza y marginación.

Asimismo, la Guerra causó muchos daños a la agricultura y los campesinos, especialmente a los niños, debido a miles de municiones, explosivos y minas sin estallar ni retirar de bosques y arrozales. Estos efectos provocaron la baja de producción en las explotaciones agrícolas y el aumento de la población urbana que huía del campo, convertido en campo de batalla. Se han contabilizado 10 500 000 refugiados generando unas pérdidas estimadas en 200 000 millones de dólares.

Las enormes infraestructuras de túneles excavados por todo Vietnam ahora forman parte de las atracciones que visitan los turistas. Se pueden ver las entradas camufladas, recorrer sus galerías, sentarse en las salas de reuniones e incluso disparar los AK-47. Este «turismo de guerra» ha contribuido a levantar la economía del país, debilitada tras la caída de la URSS.

Uno de los aspectos más controvertidos del esfuerzo militar de Estados Unidos en el sudeste asiático fue el empleo generalizado de productos químicos defoliantes entre 1961 y 1971. Utilizados para defoliar grandes extensiones de campo y así evitar que el Viet Cong fuera capaz de ocultar sus armas y campamentos bajo el follaje, estos productos químicos acabarían cambiando el paisaje, causando enfermedades, defectos en bebés de padres expuestos y el envenenamiento de la cadena alimentaria.

Ya en los comienzos del esfuerzo bélico estadounidense en Indochina, se decidió su uso, dado que el enemigo ocultaba sus operaciones bajo las selvas de triple cúpula arbórea, y un primer paso útil sería defoliar ciertas áreas de estas características. Esto fue llevado a la práctica en la conocida como operación Ranch Hand. Empresas como Dow Chemical y Monsanto fabricaron los herbicidas necesarios. Funcionarios estadounidenses señalaron que los británicos ya habían utilizado previamente los químicos 2,4,5-T y el ácido 2,4-diclorofenoxiacético de una forma prácticamente idéntica y a gran escala durante la «Emergencia Malaya» de los años 50, con el fin de destruir los bosques y cultivos que los insurgentes comunistas utilizaban para ocultarse y preparar emboscadas contra convoyes. Incluso el secretario de Estado, Dean Dusk, había dicho al presidente John F. Kennedy que «el uso de defoliantes no viola ninguna norma del derecho internacional relativo a la conducción de la guerra química, y es una táctica de guerra aceptada con un precedente establecido por los británicos durante la situación de emergencia en Malasia, donde utilizaron aviones para la destrucción de cultivos por aspersión de químicos».

Los defoliantes, que se distribuían en bidones pintados con bandas codificadas por colores, incluían los llamados «herbicidas arco iris», entre los que se encontraban el Agente Rosa, el Agente Verde, el Agente Púrpura, el Agente Azul, el Agente Blanco y el más famoso y utilizado de ellos, el Agente Naranja, que contenía dioxina como subproducto de su proceso de fabricación. Entre 41,6 y 45,4 millones de litros de Agente Naranja se rociaron sobre Vietnam del Sur entre 1961 y 1971. El principal área de operaciones de Ranch Hand fue el delta del río Mekong, donde las patrulleras de la Armada de los Estados Unidos eran vulnerables a los ataques lanzados desde la maleza a orillas del agua. 
En 1961 y 1962, la administración Kennedy autorizó el uso de productos químicos para la destrucción de los cultivos de arroz. Entre 1961 y 1967, Estados Unidos roció 75,7 millones de litros de herbicidas sobre una superficie de 24 000 kilómetros cuadrados de cultivos y bosques, el 13 % del territorio de Vietnam del Sur. En 1965, el 42 % de los herbicidas se habían pulverizado sobre cultivos para alimentación. Otro objetivo en la utilización de químicos consistía en forzar a la población civil a desplazarse a zonas controladas por los sudvietnamitas.

Según el gobierno de Vietnam, 400 000 personas murieron por el uso de Agente Naranja y 500 000 niños nacieron con enfermedades congénitas; en 2006, también el gobierno vietnamita estimó que unas cuatro millones de víctimas sufrían envenenamiento por dioxina, a lo que el gobierno de Estados Unidos contestó negando que existieran evidencias científicas concluyentes entre el Agente Naranja y las víctimas vietnamitas envenenadas con dioxina. Al respecto, en algunas zonas del sur de Vietnam a principios del siglo XXI, los niveles de dioxina eran 100 veces mayores que el estándar internacional aceptado. El Departamento de Asuntos de los Veteranos de Estados Unidos ha enumerado numerosas enfermedades que sufren los niños de los veteranos que estuvieron expuestos al Agente Naranja, como cáncer de próstata y respiratorios, mieloma múltiple, diabetes mellitus tipo 2, linfoma o neuropatía periférica, entre otras.

Pese a ser uno de los conflictos más documentados por no aplicarse la censura militar, en países como España se produjo un "olvido interesado" durante los años 1980, siendo muy escasa la producción histórica. En ocasiones se tiene la sensación, comenta , de encontrarse ante un conflicto muy difícil de analizar, por la enorme cantidad de material existente (traducciones, reportajes, crónicas...). Frente a este fenómeno aparece en contraposición lo poco tratado que ha sido el punto de vista vietnamita para Occidente, creándose así, según Tad Szulc, una visión mitificada ante la falta de análisis de mayor profundidad.

Otra nefasta consecuencia fue la falta de atención prestada por Occidente al genocidio camboyano por ser un pueblo subdesarrollado que había logrado derrotar también a un aliado de Estados Unidos; por lo tanto, en la mentalidad izquierdista/revolucionaria, no podía ser malo y las informaciones aportadas por organizaciones como Amnistía Internacional se calificaban de falsas o manipuladas por los servicios de inteligencia estadounidenses.

La impresión de que un pueblo pobre, pero muy motivado podía derrotar a la mayor potencia mundial empleando la guerra de guerrillas caló muy hondo en la mayoría de los países. Hasta el punto de considerarse el medio definitivo de lucha de los militarmente débiles contra los militarmente fuertes, debieron llegar movimientos como el de los Sin Tierra latinoamericanos para desvincularse de dicha lucha. Esta supuesta invencibilidad de las guerrillas ha quedado también como un mito, pero la Historia posterior ha desmentido este supuesto:


Por su parte, Estados Unidos también aprendió muchísimo de lo vivido en Vietnam. Los políticos de aquel país tuvieron cuidado después en no hacer combatir a sus asesores al lado de las fuerzas locales en ninguna parte del mundo y, cuando estas acciones se llevaron a cabo, los distintos gobiernos reaccionaron con cierta rapidez.

La utilización masiva del helicóptero en una guerra asimétrica se demostró correcta, pese a la derrota final. Tanto es así que se han empleado masivamente durante las Invasiones de Irak y Afganistán se han demostrado como el mejor método para combatir a un enemigo disperso y extremadamente móvil. Así, la mayoría de los ejércitos de principios del siglo XXI tendieron a reforzar y diversificar sus flotas de helicópteros frente a los llamativos, pero menos eficaces, cazas y bombarderos.

Aunque inicialmente la guerra de Vietnam no llamó excesivamente la atención de la industria cinematográfica, desde finales de la década de 1970 y principios de la década de 1980, la producción de películas sobre el tema floreció con títulos muy destacados como "Apocalypse Now" o "Platoon". Esta atención de Hollywood contrasta con el escaso interés mostrado por el cine francés hacia su derrota.

Al contrario que los historiadores e incluso la propia sociedad estadounidense, el cine sí supo asimilar el fracaso, en opinión de Marc Leppson. De esta manera se pasó de la patriótica y poco creíble "Los boinas verdes", donde se muestra a unos entregados miembros de las Fuerzas Especiales de Estados Unidos en su lucha contra los malvados comunistas, soslayando las terribles torturas que el libro original relataba, a una más crítica "Apocalypse Now", que prefirió alcanzar presupuestos astronómicos antes que rendirse a la censura del Pentágono a cambio de sus helicópteros. Otro éxito fue la cinta "El Cazador" o "The Deer Hunter" de Michael Cimino con Robert De Niro, Christopher Walken, John Savage y Meryl Streep, de 1978, reflejó la influencia de la guerra en unos trabajadores del metal en Pittsburgh. Obtuvo cinco premios Oscar.

Por su parte, Oliver Stone realizó entre 1986 y 1993 tres obras sobre estos sucesos: "Platoon", "Nacido el 4 de julio" y "Cielo y Tierra". "Platoon" recibió cuatro premios Oscar y dejó algo descolocados a los veteranos estadounidenses porque no los retrataba como héroes, pues aparece el fragging, las violaciones a niñas, los asesinatos, el incendio de aldeas; pero también los muestra en situaciones muy duras, aceptadas por su condición de pobres, junto a héroes, como el sargento Elias Grodin interpretado por Willem Dafoe. "Nacido el 4 de julio" solo obtuvo dos premios Oscar, uno al mejor director, pero arrasó en los Globo de Oro. En "El cielo y la tierra", basada en los libros de Le Ly Hayslip, intentó acercarse a la visión vietnamita del conflicto. En el llamado a veces "año de las películas de Vietnam" cabe destacar títulos clásicos como "Full Metal Jacket" (conocida en España como "La chaqueta metálica" y en Latinoamérica como "Cara de guerra" o "Nacido para matar") de Stanley Kubrick y "La colina de la Hamburguesa", de John Irvin.

Ciertamente los distintos estudios han creado cintas de todo tipo. De esta forma quizá la visión cinematográfica más ficticia de este conflicto sea la dada por "Rambo"; un héroe que, en palabras de Marc Leppson, se parece tanto a un veterano de Vietnam como Superman a un policía. No obstante otras cintas ofrecen análisis más próximos a la realidad, como el mostrado por Francis Ford Coppola en "Jardines de Piedra", donde los maduros veteranos le dicen al impulsivo muchacho que aquella guerra no la pueden ganar y él les responde que olvidan su potencia de fuego portada por sus helicópteros contra los "arcos y flechas" vietnamitas, metáfora sobre una de las causas de la derrota, el pensar que se puede vencer a un pueblo subdesarrollado solo a base de bombas.

En el siglo XXI, la mala conciencia estadounidenses quedó limpia, según , estrenándose obras como "Forrest Gump" o "Across the Universe" (2007) dirigida por Julie Taymor y situada en Nueva York durante los movimientos antibélicos principalmente formados por jóvenes, al ver amigos y familiares ir contra su voluntad a Vietnam.

Una ausencia en muchos de estos largometrajes es la postura vietnamita, con alguna excepción como "Vietnam Vietnam", mostrando la participación australiana y las acciones del FNLV en los poblados. Fue en 2002 cuando se estrenó "We Were Soldiers" ("Cuando éramos soldados" en España, "Fuimos héroes" en Latinoamérica) adentrándose un poco más en la vida en los túneles vivida por los soldados del Norte, al estar basada en la reunión mantenida por con sus antiguos adversarios vietnamitas.





</doc>
<doc id="8254" url="https://es.wikipedia.org/wiki?curid=8254" title="Arminianismo">
Arminianismo

El arminianismo es una doctrina teológica cristiana fundada por Jacobo Arminio en la Holanda de comienzos del siglo XVII, a partir de la impugnación del dogma calvinista de la doble predestinación.

Sustenta la salvación en la cooperación del hombre con la gracia divina a través de la fe. Frente al concepto calvinista de predestinación (o “elección”) incondicional, el arminianismo enseña que la predestinación se ha basado en la presciencia de Dios, quien tiene el conocimiento previo de quién creerá y quién no creerá en Cristo; la voluntad del hombre, por asistencia divina, es hecha libre para creer o rechazar a Cristo.

Después de la muerte de Arminio (en 1609), sus principios se formularon en el manifiesto de cinco puntos "Remonstrans", publicado en 1610 (por lo que sus seguidores también pasaron a denominarse “remonstrantes”).

En 1618 el arminianismo fue condenado por el sínodo de Dort o de Dordrecht, convocado a instancias del estatúder de Holanda Mauricio de Nassau, que apoyaba a los calvinistas intransigentes y monárquicos (Franciscus Gomarus y los denominados “gomaristas” o “contrarremonstrantes”). Johan van Oldenbarnevelt y otros dirigentes principales del arminianismo fueron entonces ejecutados, mientras que otros muchos, entre los que se encontraban Hugo Grocio y Simón Episcopius, tuvieron que exiliarse.

La teología arminiana contribuyó a la aparición del metodismo en Inglaterra. No todos los predicadores metodistas del siglo XVIII fueron arminianos, pero sí la mayor parte, como el propio John Wesley.

Arminio afirmaba firmemente la necesidad de la gracia de Dios para la redención de todo ser humano, pero consideraba que la gracia puede ser rechazada por el hombre en su libre albedrío. El arminianismo se opone a la postura calvinista, donde esta última enseña que algunos están predestinados para salvación y otros para perdición. Arminio consideraba que la expiación de Cristo es para todos y no sólo para algunos elegidos, aunque no todos la aceptan y por lo tanto no reciben sus beneficios. Por lo tanto según los arminianos es posible “caer de la gracia” y no es correcto pensar que los que ya recibieron la gracia nunca se perderán. El calvinismo sostiene que: “Ya siendo salvo el individuo, siempre salvo”.

El arminianismo enseña que la destitución de Dios por causa de la rebelión es posible a pesar de haber sido parte de Su institución.

La posición arminianista empieza desde la perdición y separación de Dios, del mismísimo Luzbel (el diablo). Habiendo sido él un querubín, ocupando el más alto rango angelical, puesto sobre los ángeles creados, conociendo a Dios íntimamente, habiendo sido parte de Su reino por milenios, no obstante, decide por su libre albedrío rebelarse contra el Creador. Él junto con los ángeles que le siguieron, fueron destituidos de la gloria de Dios. Adán, habiendo sido creado y criado por Dios mismo hasta cierta edad, cuando él ya pudo valerse por sí solo, junto con Eva su mujer, deciden por esa libertad otorgada comer del fruto prohibido, trayendo sobre sí y sobre la humanidad el pecado y la destitución. El pueblo judío fue liberado de la esclavitud de Egipto, lo cual tipifica ser liberado del pecado. Sin embargo, por sus tendencias pecaminosas no heredaron la tierra prometida. Solo Caleb y Josué con los suyos y la segunda y tercera generación de judíos entró en ella. El argumento más poderoso del arminianismo, sin duda alguna, es el siguiente: “Si un número predeterminado de seres humanos ya estaba predeterminado para salvación, la venida de Jesús, el Hijo de Dios, no hubiese sido requerida”. El pasado, presente y futuro son simultáneos para Dios. Él en su presciencia ya sabe quiénes lograron entrar en Su presencia, pero nosotros los hombres no. Por lo tanto, no podemos determinar quiénes califican y quiénes no.
Todos fuimos predestinados para salvación, es decir, con el objetivo de ser salvos. Pero eso no quiere decir que necesariamente todos seremos salvos, porque aunque Dios nos predestinó para salvación, también nos dio libertad para salvarnos o perdernos: el libre albedrío.

¿Existen personas que nacen condenadas al tormento eterno, incluso si se arrepienten y aceptan lo que hizo Jesús en la cruz? Eso no armonizaría con el carácter de Dios; pues Él dice: A los cielos y a la tierra llamo por testigos hoy contra vosotros, que os he puesto delante la vida y la muerte, la bendición y la maldición; "escoge", pues, la vida, para que vivas tú y tu descendencia.

Denominaciones arminianas son las diferentes Iglesias metodistas (Iglesia Metodista Episcopal, Iglesia Metodista Unida, Iglesia Metodista Libre), la Iglesia del Nazareno, el Ejército de Salvación (The Salvation Army), la Iglesia Adventista del Séptimo Día, la Iglesia Wesleyana, la Iglesia de Dios, la mayoría de las Iglesias pentecostales, la Iglesia Internacional del Evangelio Cuadrangular, las Iglesias de Cristo, las Asambleas de Dios, y otras del movimiento restauracionista (menonitas en su mayoría). Muchos anglocatólicos (como C.S. Lewis), así como la Iglesia copta, la Iglesia católica y la Iglesia ortodoxa creen en la libertad de la voluntad humana y que toda persona tiene la posibilidad de recibir salvación y que, una vez que recibe la salvación, también la puede perder.

Cabe anotar igualmente que cuando se habla de perder la salvación, no es porque Dios la arrebata nuevamente después de haberla otorgado en Jesús, sino que es el mismo hombre quien la desecha una vez que rompe su comunión con Dios a través del pecado.

1.- Libre albedrío o habilidad humana. Aunque la naturaleza humana fue totalmente afectada por la caída, sin embargo, Dios en su gracia capacita la voluntad del pecador para que libremente se arrepienta y crea, o rehuse hacerlo. Cada pecador, capacitado por la gracia de Dios, tiene libertad para creer o rehusar creer, y su destino eterno depende de cómo use dicha libertad. La libertad con la que Dios capacita al hombre caído, consiste en poder escoger libremente entre el bien y el mal en la esfera de lo espiritual. El pecador puede cooperar con el Espíritu de Dios y ser regenerado o resistir la gracia de Dios y perderse para siempre. El pecador necesita la asistencia del Espíritu Santo, pero no tiene que ser regenerado por el Espíritu antes de 
que pueda creer, ya que la fe es un don de Dios que el hombre puede recibir o rechazar libremente, y precede al nuevo nacimiento. La fe es un don de Dios; y el hombre lo puede recibir y ejercer para vida eterna, o rechazarlo para condenación.

2.- Elección condicional. Dios escogió para salvación, antes de la fundación del mundo, a todas aquellas personas que, asistidas por su gracia habilitadora, creen en Cristo. Esto se debe al hecho de que Dios vio de antemano que dichos individuos habrían de responder positivamente a su llamado, arrepintiéndose y creyendo en Cristo. Dios escogió sólo a aquellos que él vio de antemano que voluntariamente creerían en el evangelio, asistidos por su gracia resistible.

3.- Redención universal o expiación general. La obra redentora de Cristo brinda a todos los hombres la oportunidad de ser salvos, y garantizó la salvación de todos los que habían creído y preservado hasta la muerte de Cristo, y también garantizó la salvación de todos los que habrían de creer y perseverar después de la muerte de Cristo. A pesar de que Cristo murió por todos los hombres, sólo los que creen en él son salvados. Su muerte es suficiente para la salvación de todos los hombres, pero sólo eficaz en los que creen.

4.- El Espíritu Santo puede ser resistido eficazmente. Él Espíritu Santo convence de pecado al mundo, y hace todo lo que se ha determinado para traer a cada pecador a la salvación. El llamado del 
Espíritu, sin embargo, puede ser resistido, ya que el hombre es hecho libre por la gracia de Dios. El Espíritu no regenera al pecador hasta que éste cree; la fe (que es un don de Dios que el hombre puede recibir o rechazar libremente) precede al nuevo nacimiento. Dios ha determinado que su llamado, a través del Espíritu Santo, pueda ser libre y voluntariamente aceptado o resistido. El Espíritu Santo obra eficazmente trayendo a Cristo sólo a aquellos que no le resisten. El Espíritu no imparte vida hasta que el pecador responde, arrepintiéndose y creyendo voluntariamente en Cristo. Dios, por tanto, ha determinado que Su gracia no actué de forma irresistible; sino que la misma puede ser resistida por el hombre.

5.- El caer de la gracia o el perder la salvación. Algunos arminianos creen que el ser humano, una vez salvo, no perderá su salvación y otros piensan que la salvación pueda perderse por no perseverar en la fe. 



</doc>
<doc id="8255" url="https://es.wikipedia.org/wiki?curid=8255" title="Náhuatl">
Náhuatl

El náhuatl (autoglotónimo "nāhuatlahtōlli", que deriva de "nāhua-tl", «sonido claro o agradable» y "tlahtōl-li", «lengua o lenguaje») es una macrolengua yuto-nahua que se habla en México. Existe, por lo menos, desde el siglo V, aunque al darse la diferenciación entre el yuto-nahua del sur y el proto-nahua (c. siglo III) ya es posible hablar de una lengua náhuatl. Con la expansión de la Cultura Coyotlatelco durante los siglos V y VI en Mesoamérica, el náhuatl comenzó su rápida difusión por el Eje Neovolcánico llegando una rama a extenderse por la costa del Pacífico dando origen al pochuteco y otra rama hacia Veracruz que daría origen al pipil de Centroamérica. Así poco a poco el náhuatl pasó por encima de otras lenguas mesoamericanas hasta convertirse en "lingua franca" de buena parte de la zona mesoamericana; en una primera etapa fue promovida en el área central de México por los tepanecas, posteriormente en una segunda etapa esta lengua, en parte, fue impuesta en los territorios conquistados por el Imperio mexica, también llamado Imperio azteca, desde el siglo XIV.

El náhuatl comenzó a perder hablantes conforme se fueron imponiendo los españoles en el continente, junto con el español como nueva lengua dominante en Mesoamérica; sin embargo, los europeos siguieron usando el náhuatl con propósitos de conquista a través de los misioneros, llevando la lengua a regiones donde previamente no había influencia náhuatl.

El náhuatl es la lengua nativa con mayor número de hablantes en México, con más de un millón y medio, la mayoría bilingüe con el español.

El náhuatl pertenece a la familia yuto-nahua (uto-azteca) el cual tiene una división prehistórica en “yuto-nahua del norte” y “yuto-nahua del sur”, de esta última rama se desarrollaron cuatro grupos, de estos el grupo “Nahuatlano” también llamado “nahuano” o “aztecano” es el que da origen al náhuatl. La división Yuto-nahua del sur / Nahuatlano es el momento en que surge el Proto-nahua, que es el ancestro de todas las variantes. Según algunos autores la primera división del proto-nahua dio origen al extinto pochuteco, quedando por otro lado lo que los lingüistas norteamericanos llaman "General Aztec" o "náhuatl nuclear" según el INALI, el que a su vez se divide en dos ramas, tenemos "Náhuatl Occidental" y el "Náhuatl Oriental", por último, la rama occidental se divide en "Náhuatl de la Periferia Occidental" y "Náhuatl central". Todas las variantes dialectales actuales se desprenden de estos grupos.

La sub-clasificación actual del náhuatl se basa en las investigaciones de Canger (1980, 1988) y Lastra de Suárez (1986). Canger inicialmente introdujo el esquema de una agrupación central y dos grupos periféricos; Lastra concordó con esta noción aunque difiere en algunos detalles. Canger y Dakin (1985) replantearon una división básica más antigua de la comunidad de hablantes del proto-nahua en sólo dos ramas, la Occidental y la Oriental y así justificar y comprender las variaciones de las orientales que muestran una mayor profundidad temporal. Canger originalmente consideró la zona central como una sub-área innovadora dentro de la rama occidental, pero en 2011, sugirió que surgió como una lengua koiné urbana con características de ambas áreas, tanto occidental como oriental. Canger (1988) incluyó provisionalmente los dialectos de la Huasteca en el grupo central, mientras Lastra de Suárez (1986) los sitúa en la periferia oriental; Kaufman (2001) y la mayoría de los investigadores actuales aceptan estas conclusiones.

Desde un punto de vista tipológico, resalta su importancia como ejemplo de lengua polisintética y aglutinante, particularmente en la morfología verbal y en la formación del léxico. Tipológicamente es además una lengua de núcleo final, en la que el modificador suele preceder al núcleo modificado.

Sobre la cuestión del punto geográfico de origen, los lingüistas durante el siglo XX coincidieron en que la familia de lenguas yuto-nahuas se originó en el suroeste de los Estados Unidos. Tanto la evidencia de la arqueología y la etnohistoria es compatible con una difusión hacia el sur a través del continente americano; este movimiento de comunidades hablantes se da en varias oleadas desde los desiertos del norte de México al centro de México. El proto-nahua por lo tanto surgió en la región entre Chihuahua y Durango donde al ocupar una mayor extensión de territorio, formó rápidamente dos variantes, una que continuó dispersando hacia el sur con cambios innovadores mientras la otra con rasgos conservadores del yuto-nahua se desplazó al oriente.

La migración propuesta de hablantes de la lengua proto-nahua en la región mesoamericana se ha colocado en algún momento alrededor del año 500, hacia el final del período Clásico Temprano en la cronología mesoamericana. Antes de llegar al centro de México, grupos pre-nahuas probablemente pasaron un periodo de tiempo en contacto con las lenguas cora y huichol del occidente de México (que también son uto-aztecas).

El surgimiento del náhuatl y sus variantes por lo tanto se da durante la época del apogeo de Teotihuacán. Las rutas comerciales teotihuacanas sirvieron para una rápida difusión de la nueva lengua. La identidad de la lengua hablada por los fundadores de Teotihuacán nos es desconocida, sin embargo, durante mucho tiempo ha sido objeto de debate; de esta manera en los siglos XIX y XX algunos investigadores creían que Teotihuacán había sido fundada por hablantes de náhuatl; más tarde hacia finales del siglo pasado la investigación lingüística y arqueológica tiende a contradecir ese punto de vista. Ahora se cree que es más probable que la lengua teotihuacana estuviera relacionada con el totonaco o fuera de origen mixe-zoqueano. Buena parte de la migración nahua al centro de México fue consecuencia y no causa de la caída de Teotihuacán. Desde estas épocas tempranas se dieron préstamos entre las diferentes familias lingüísticas e incluso a nivel morfosintáctico.

En Mesoamérica las familias de la lengua maya, otomangue y mixe-zoque habían coexistido durante milenios. La interacción de estas lenguas generó una serie de rasgos comunes en todas ellas que permiten que entendamos la zona mesoamericana como una sola a nivel lingüístico, independientemente de la evolución de cada lengua en su propio grupo. Después que los nahuas llegaron a la zona de alta cultura de Mesoamérica, su lengua también adoptó algunos de los rasgos que definen el área lingüística mesoamericana; así por ejemplo los nahuas adoptaron el uso de sustantivos relacionales y una forma de construcción posesiva típica de las lenguas mesoamericanas.

Teotihuacán ejercía un poder centralista y marcaba las pautas de los señoríos locales, quienes al parecer tenían que ser legitimados desde la metrópolis. Tras el colapso de la gran ciudad surgieron modelos nuevos para detentar el poder, junto con estos modelos al parecer se fue promoviendo la lengua náhuatl la cual se considera difundida por la cultura Coyotlatelco, sin embargo, la lengua no solo fue hablada por sus nativos, sino que poco a poco fue adoptada por las poblaciones otomangues con mayor antigüedad y que habían dependido de Teotihuacán. Al fundarse Tula Chico en el siglo VII ya se sentía la influencia nahua pero no era muy intensa; trescientos años después con la re-fundación de esta ciudad por el año 900, que a partir de entonces será conocida como “Tollan” (Tula), sus fundadores son reconocidos por las fuentes como “nahuas-chichimecas” quienes comparten el poder con los nonohualcas. Es en este momento que el náhuatl adquiere relevancia política, poco después se volverá el idioma oficial de los tepanecas (que hablaban originalmente una variante del otomí), y ya en el siglo XIV fue adoptado por los acolhuas de Tetzcoco.

Aunque los mexicas se cree que siempre hablaron el náhuatl es posible que también lo hayan adoptado. La influencia política y lingüística de este grupo llegó a extenderse en la América media y el náhuatl se convirtió en una lengua franca entre los comerciantes y las élites en Mesoamérica, por ejemplo entre los mayas quiché. Tenochtitlan creció hasta convertirse en el mayor centro urbano mesoamericano, esto atrajo a los hablantes de náhuatl de otras áreas donde se había extendido por siglos previamente, con lo que se dio a luz a una nueva forma urbana de náhuatl con rasgos de muchos dialectos. Esta variedad urbanizada de Tenochtitlan-Tlatelolco es lo que llegó a ser conocido como náhuatl clásico y fue ampliamente documentado en la época colonial.

Con la llegada de los españoles al corazón de México en 1519 la situación del idioma náhuatl cambiaría de manera significativa; por un lado comienza un desplazamiento por la lengua española; por el otro, su uso oficializado para la comunicación con los nativos generó el establecimiento de nuevos asentamientos; a la vez se dio la creación de una amplia documentación en escritura latina, con lo cual se asienta un registro fidedigno para su preservación y comprensión, por lo que el idioma siguió siendo importante en las comunidades nahuas bajo el dominio español.

Los españoles se dieron cuenta de la importancia que tenía la lengua y prefirieron continuar con su uso que cambiarla, también encontraron que el aprendizaje de todas las lenguas indígenas de lo que ellos llamarían Nueva España era imposible en la práctica, por lo que se concentraron en el náhuatl. Inmediatamente después de la Conquista, los misioneros fransicanos fundaron escuelas —como el Colegio de Santa Cruz de Tlatelolco en 1536— para la nobleza indígena con el propósito de re-educarlos dentro de los cánones occidentales, donde aprendían teología, gramática, música, matemáticas. A la vez los misioneros emprendieron la redacción de gramáticas, llamadas en esa época “artes”, de las lenguas indígenas para su uso por parte de sacerdotes. La primera gramática náhuatl escrita en 1531 por los franciscanos se encuentra perdida, la más antigua que se preserva fue escrita por Andrés de Olmos y publicada en 1547. Hacia el año de 1645 tenemos noticia de cuatro obras más publicadas cuyos autores son, en 1571 Alonso de Molina, en 1595 Antonio del Rincón, en 1642 Diego de Galdo Guzmán y en 1645 Horacio Carochi. Este último es considerado hoy en día el más importante de los gramáticos de la época colonial. Carochi ha sido especialmente importante para los investigadores que trabajan en la nueva filología, debido a su enfoque científico que precede a las investigaciones lingüistas modernas, analiza más a detalle los aspectos fonológicos que sus predecesores e incluso sucesores, quienes no habían tomado en cuenta la pronunciación del cierre glotal (saltillo) que es en realidad una consonante o la longitud vocal.
En 1570 el rey Felipe II de España decretó que el náhuatl debía convertirse en la lengua oficial en la Nueva España con el fin de facilitar la comunicación entre los españoles y los nativos de las colonias. Durante este período la Corona española permite un alto grado de autonomía en la administración local de los pueblos indígenas, y en muchos pueblos la lengua náhuatl era la oficial de hecho, tanto escrita como hablada. Durante los siglos XVI y XVII, el náhuatl clásico se utilizó como lengua literaria, y un gran corpus de documentos de ese período sobrevivió hasta nuestros días. Las obras de este período incluyen historias, crónicas, poesía, obras de teatro, obras canónicas cristianas, descripciones etnográficas y documentos administrativos. Como ejemplos podemos citar el "Códice Florentino", un compendio de doce volúmenes de la cultura mexica compilado por el franciscano Bernardino de Sahagún; la "Crónica Mexicáyotl" de Fernando Alvarado Tezozómoc que relata el origen y el linaje real de Tenochtitlán; los "Cantares mexicanos" que son una colección de poemas en náhuatl; el diccionario compilado por Alonso de Molina náhuatl-español y español-náhuatl el cual sigue siendo básico para la lexicología moderna; y el "Huei tlamahuiçoltica", una de las descripciones en náhuatl de la aparición de la Virgen de Guadalupe.

Durante un tiempo, la situación lingüística en la Nueva España se mantuvo relativamente estable pero en 1686 el rey Carlos II emitió una real cédula que prohíbe el uso de cualquier idioma distinto del español en todo el Imperio español, reiterándola en 1691 y 1693, en las que dicta la creación de la “parcela escolar” para la enseñanza del idioma imperial. Otro decreto el 10 de mayo de 1770, ahora de Carlos III, estableció la creación de nuevos centros de enseñanza completamente en castellano para la nobleza indígena y se deshizo del náhuatl clásico como lengua literaria, aunque hasta la Independencia de México en 1821, los tribunales españoles aún admitían testimonios en náhuatl y documentación como prueba en los juicios, con traductores judiciales que exponían en español.

La situación indígena y del habla náhuatl al inicio del movimiento de la Independencia en realidad había sido sostenido pues el 66 % de la población era indígena de los 6 millones de habitantes del país. Los indicadores demográficos muestran un crecimiento paralelo al de la población mestiza de México. Las comunidades nahuas ya habían asimilado el cristianismo de manera sincrética, además eran parte fundamental de la fuerza productiva del país; su desarrollo local se fincaba en una tradición ya consumada durante los últimos 300 años y que había generado pocos cambios en su organización social y cultural, de hecho, muchas de esas manifestaciones sobrevivieron hasta nuestros días.

A lo largo de la época moderna, la situación de las lenguas indígenas ha aumentado en precariedad cada vez más en México, y el número de hablantes de prácticamente todas las lenguas indígenas ha disminuido. A pesar de que el número absoluto de hablantes de náhuatl en realidad ha aumentado en el último siglo, las poblaciones indígenas se han vuelto cada vez más marginadas en la sociedad mexicana. Los grandes cambios en las comunidades indígenas se dieron a partir de las reformas agrarias emergidas del revolucionario Plan de Ayutla por medio de la “Ley Lerdo” a mediados del siglo XIX, con lo cual se instalaba la noción de “ejido” que fraccionaba las tierras comunales y a partir de entonces los indígenas se vieron forzados a pagar una serie de nuevos impuestos y que bajo la coacción de hacendados y gobierno no pudieron pagar creándose los grandes latifundios, lo que provocó que poco a poco fueran perdiendo sus tierras, su identidad, su lengua, e incluso su libertad.

Este proceso aceleró los cambios en la relación asimétrica entre las lenguas indígenas y el castellano, así el náhuatl se vio cada vez más influenciado y modificado; como primera consecuencia es observable una zona de una rápida pérdida del habla y las costumbres cercana a las grandes ciudades, como segunda consecuencia vemos zonas donde la “castellanización” es más fuerte provocando un bilingüismo activo, en una tercera zona se mantuvieron los hablantes indígenas más aislados y conservaron más puras sus tradiciones. Las políticas porfirianas tendían a la eliminación de las lenguas nativas, buscando el desarrollo y el progreso del país bajo un nacionalismo mexicano, política seguida por los gobiernos post-revolucionarios. Sólo hasta el gobierno cardenista surge un verdadero interés institucional por comprender y estudiar la cultura indígena, intentando revertir la tendencia de la incorporación forzada a la cultura nacional, lo que de hecho no pasó y continuó la pérdida hasta los ochentas.

Cambios significativos se dieron por lo menos desde mediados de la década de 1980, aunque las políticas educativas en México se centraron en la castellanización de las comunidades indígenas, para enseñar puramente español y desalentar el uso de las lenguas nativas, tuvo como resultado que hoy en día un buen número de hablantes de náhuatl estén en posibilidad de escribir tanto su lengua como el español; aun así su tasa de alfabetización en español sigue siendo muy inferior a la media nacional. A pesar de ello, el náhuatl todavía es hablado por más de un millón de personas, de los cuales alrededor del 10 % son monolingües. La supervivencia del náhuatl en su conjunto no está en peligro inminente, pero la supervivencia de ciertos dialectos sí lo está; y algunos dialectos ya se han extinguido durante las últimas décadas del siglo XX.

La década de 1990 vio la aparición de cambios diametrales en las políticas del gobierno mexicano hacia los derechos indígenas y lingüísticos. La evolución de los acuerdos en el ámbito de los derechos internacionales combinada con presiones internas condujeron a reformas legislativas y la creación de organismos gubernamentales descentralizados; así, ya para el 2001 el Instituto Nacional Indigenista desapareció para darle paso a la CDI ("Comisión Nacional para el Desarrollo de los Pueblos Indígenas") y el INALI creado en 2003 con responsabilidades para la promoción y protección de las lenguas indígenas, en particular la Ley general de Derechos Lingüísticos de los Pueblos indígenas reconoce todas las lenguas indígenas del país, incluyendo el náhuatl, como "idiomas nacionales" y da a los indígenas el derecho a utilizarlos en todas las esferas de la vida pública y privada. En el artículo 11, que garantiza el acceso a la educación obligatoria, bilingüe e intercultural. Esta ley da origen al "Catálogo de las Lenguas Indígenas Nacionales" en 2007.

En 1895, el náhuatl era hablado por más del 5 % de la población. Para el año 2000, esta proporción había caído a 1,49 %. Teniendo en cuenta el proceso de marginación combinada con la tendencia de la migración a las zonas urbanas y a los Estados Unidos, algunos lingüistas están advirtiendo sobre la muerte inminente de las lenguas. En la actualidad se habla en náhuatl sobre todo en las zonas rurales por una clase empobrecida de agricultores de subsistencia indígenas. De acuerdo con el Instituto Nacional de Estadísticas de México, el INEGI, el 51 % de los hablantes de náhuatl están involucrados en el sector agrícola y 6 de cada 10 no reciben sueldos o ganan menos del salario mínimo.

En México la lengua náhuatl se habla principalmente en cinco estados: Guerrero, Puebla, Hidalgo, San Luis Potosí y Veracruz, donde tiene en cada estado una población arriba de 100 mil hablantes. En los estados de Morelos y Tlaxcala cuenta con una población dispersa o en localidades pequeñas; en promedio se puede hablar de una población de alrededor de 20 mil hablantes. En los estados de Tabasco, Michoacán, México, Oaxaca, Nayarit y Durango, así como los habitantes de Milpa Alta la presencia es mínima y con posibilidad de perderse (excepto en Oaxaca).

Las variantes dialectales se agrupan en tres ramas, Náhuatl central, Náhuatl de la Periferia Occidental y Náhuatl Oriental. Algunos de los dialectos son:

Náhuatl central es una derivación del “náhuatl occidental” que durante los siglos XIV y XV desarrolló varias innovaciones a la vez que asimiló algunas características de las demás variantes. En el mismo siglo XV ya presentaba una diferenciación y eran considerados diferentes las hablas del centro (valle de México), de Morelos o de Tlaxcala-Puebla.

Náhuatl clásico es la denominación de la lengua registrada durante el Virreinato; ésta en realidad refleja una serie de variantes. Una gran parte de que parezca solo un idioma es debido al esfuerzo de los frailes que quisieron trasmitir, por medio de la lengua más culta, la fe cristiana. Posteriormente las autoridades virreinales, con una forma más coloquial, dieron cabida a la elaboración de documentos (testamentos, pleitos territoriales, denuncias, etc.) en lengua náhuatl. Los libros y documentos escritos en la ciudad de México contienen un estilo más elegante, el que quisieron rescatar los religiosos. Obra diversa procede de otras ciudades importantes como Colhuacán, Chalco, Cuauhtitlan, Tlaxcala, Tecamachalco. Las diferencias más marcadas entre la variante de México y otras las podemos ver al comparar el Nican mopohua con textos de Jalisco (Yáñez, 2001) o de Guatemala (Dakin, 1996).

Náhuatl de Morelos es una denominación genérica que designa a toda la población hablante de esta lengua en el estado, por lo que se presta a confusión. Para Ethnologue existen solo dos variantes, una con el código ISO 639-3 nhm que representa la mayoría de los nahuas, y con código nhg clasifica a los hablantes de Tetelcingo.

El INALI divide el estado en cuatro variantes; el más representativo y al que le correspondería el código “nhm” lo denomina mexicano de Tetela del Volcán (es el náhuatl de Hueyapan y Santa Catarina); las otras variante son el mexicano de Temixco (náhuatl de Cuentepec); el mexicano central bajo (hablado en los municipios de Ayala y Jojutla) y por supuesto el náhuatl de Tetelcingo que denomina mexicano de Puente de Ixtla.
La población total nahuahablante en el estado es de 19 241 personas.

Náhuatl de Tlaxcala es una de las variantes que presenta una gran asimetría en su relación con el español, buena parte de los pueblos históricos que la hablan están siendo absorbidos por la mancha urbana de la Zona Metropolitana de Puebla-Tlaxcala, por lo que la han modificada al correr de los años su estructura y fonología; es hablado con más frecuencia en los municipios de la región occidental del Volcán la Malintzi y sur del estado, como Tetlahnocan, Contla de Juan Cuamatzi, Chiautempan, Teolocholco y San Pablo del Monte. Es denominado por el INALI mexicano del oriente central y tiene alrededor de 19 mil hablantes.

El náhuatl de Tetelcingo o mösiehuali aunque emparentado con el náhuatl clásico tuvo una evolución que ha obligado a los investigadores a desarrollar un sistema de escritura muy particular. El primer estudio de esta variante lo hizo William Cameron Townsend en 1935. Es hablado por menos de 3500 personas en el municipio de Cuautla de Morelos (Morelos).

Por algunos considerada como la más conservadora o la que retomó características arcaicas como el uso del absolutivo en -t. Se tiene tres zonas muy marcadas, por una parte tenemos el “náhuatl de La Huasteca” con gran influencia de las variantes centrales; las variantes del centro de Veracruz y sur de Puebla con rasgos del Golfo; y las denominadas variantes del “Istmo” (o Golfo) con una fuerte sustitución del –tl por –t y un gran parecido al nawat Pipil.

El náhuatl guerrerense o náhuatl de Guerrero durante mucho tiempo fue clasificado en la rama central, sin embargo los estudios revelan un substrato de elementos orientales y los elementos centrales al parecer son innovaciones que se dieron a partir del siglo XIII. En el estado de Guerrero se presenta una variación dialectal que da origen a cuatro distintas variantes: dos de la “Periferia Occidental”; el “náhuatl de Tlamacazapa” y el “náhuatl de Coatepec”. El grueso de hablantes (cerca de 150 000) que son propiamente del “náhuatl de Guerrero” se ubican en la región centro-montaña del estado; la cuarta variante es el “náhuatl de Ometepec”, cerca de la Costa Chica y la cual tiene 430 hablantes. Esta última variante también es clasificada en la rama occidental.

El INALI engloba tres variantes bajo el nombre de náhuatl del Istmo (también llamado náhuatl istmeño), según los estudios del "Instituto Lingüístico de Verano" (SIL por sus siglas en inglés) le corresponde a la forma más extrema hacia el sur, en el municipio de Cosoleacaque, el código ISO 639-3 nhk, al de Mecayapan el código nhx y al de Pajapan aplica el código nhp.

La variante más representativa y estudiada es la de Mecayapan. Tiene alrededor de 20 000 hablantes, distribuidos también en el pueblo de Tatahuicapan.

En su proceso fonológico evolutivo esta lengua emparentada con el nawat tabasqueño y la lengua pipil usa de las letras /b/, /d/, /g/ y /r/. Además esta variante se caracteriza por el uso de vocales largas.

La región norte de Puebla presenta una geografía bastante accidentada, lo que provoca más aislamiento entre las comunidades; esta situación generó que en un territorio reducido tengan tres variantes según Ethnologue, cuatro según el INALI. Las denominaciones son confusas pues utilizan casi las mismas palabras como “norte”, “sierra”, como veremos a continuación. El náhuatl del noreste central, (denominación del INALI) es equivalente al "Nahuatl, Northern Puebla" de Ethnologue con código NCJ. Para la institución mexicana es la variante hablada en los municipios de Acaxochitlán (Hidalgo), en los municipios poblanos de Chiconcuauhtla, Honey, Huauchinango, Jopala, Juan Galindo, Naupan, Pahuatlán, Tlaola, Tlapapcoya, Xicotepec, Zihuateutla. Esta variante en realidad es derivada del náhuatl central. Tiene 71 040 hablantes. Uno de los primeros estudios de la zona lo hizo Yolanda Lastra en Acaxochitlán en 1980 (véase bibliografía).

Propiamente las variantes orientales son el náhuatl de la sierra noreste de Puebla, hablado en los municipios de Atempan, Ayotoxco, Cuautempan, Cuetzalan, Chignautla, Hueyapan, Hueytamalco, Huitzilán de Serdán, Ixtacamaxtitlán, Jonotla, Nauzontla, Tenampulco, Tetela de Ocampo, Teziutlán, Tlatlauquitepec, Tuzamapan, Xiutetelco, Xochiapulco, Xochitlán, Yaonahuac, Zacapoaxtla, Zautla, Zapotitlán, Zaragoza, Zoquiapan. Ethnologue le asigna el código AZZ ("Nahuatl, Highland Puebla"). Tiene 134 737 hablantes. En esta variante es muy marcado el uso de /t/ en las palabras en lugar de /tl/; los nativos llaman a su lengua “mexikanotlajtol” y se autodenominan “maseualmej” como equivalente de "gente indígena".

La otra variante llamada náhuatl de la sierra oeste de Puebla es hablada en Ahuacatlán, Aquixtla, Chignauapan, Tepetzintla, Zacatlán. Se le asocia el código NHI ("Nahuatl, Zacatlán-Ahuacatlán-Tepetzintla"). Tiene 19 482 hablantes.

La cuarta variante que reconoce el INALI la denomina náhuatl alto del norte de Puebla y es hablada únicamente en los municipios de Francisco Z. Mena y Venustiano Carranza. Tiene 1350 hablantes.

El náhuatl de La Huasteca, es una de las variedades con mayor número de hablantes. Aunque se reconocen como tres regiones distintas con particularidades la inteligibilidad es muy alta entre ellas. En el estado de Hidalgo (principalmente en los municipios de Huejutla, Jaltocán, Pisaflores y Tenango de Doria) es denominado por el INALI mexicano de la Huasteca hidalguense; en el noroeste de Veracruz es llamado náhuatl de la Huasteca veracruzana; y en el sureste de San Luis Potosí lo clasifican como náhuatl de la Huasteca potosina. Tiene alrededor de 464 mil hablantes en las tres áreas.

Corresponden a esta rama las variantes de los estados de Michoacán, Colima, Jalisco, Nayarit y Durango. Aunque prácticamente está extinto en Jalisco y Colima, no ha sido declarado así por ninguna institución. Además se incluyen las hablas de las poblaciones de Tlamacazapa (municipio de Taxco, Guerrero), la de Coatepec Costales (municipio de Teloloapan, Guerrero) y la de Temascaltepec (Estado de México). Estas variantes son reconocidas por el uso de /l/ donde las variantes centrales usan /tl/. También están variantes presentan mayores cambios fonológicos y de morfo-sintaxis, por lo que muestra una mayor diferenciación respecto a otras regiones.

El náhuatl de Michoacán o mexicano central de occidente es hablado en la costa de Michoacán y parte de la sierra pegada a esta; ha sufrido una gran pérdida de hablantes pero es a la vez una de las que más se apega a sus tradiciones. Para recuperar su lengua se han implementado la educación indígena obligatoria durante los tres primeros ciclos escolares de la educación básica y se promueve la escritura literaria. Tiene 2809 hablantes.

El náhuatl de Jalisco o mexicano del occidente era una variante que se habló al sur del estado y compartía rasgos con el “náhuatl de Michoacán”. Fue la primera variante en tener su propia gramática, fue elaborada en 1692 por Fray Juan Guerra y se llamó ""Arte de la lengua mexicana. Que fue usual entre los indios del obispado de Guadalajara y de parte de los de Durango y Michoacán"". Las imposiciones por parte de los gobiernos locales durante el Porfiriato fueron causa de una rápida pérdida del habla a principios del siglo XX, los gobiernos post-revolucionarios no mostraron una mejor disposición ni siquiera para su estudio, por lo que en la década de los sesentas y setentas cuando lingüistas e investigadores quisieron estudiarlo ya estaba moribundo.

Náhuatl de Durango o mexicano del noroeste es una variedad que también se le conoce como “náhuatl mexicanero” y se habla en el estado de Durango, en las poblaciones de San Pedro de las Jícoras, San Juan de Buenaventura entre otras. Los nahuas de esta región tienen una interacción muy activa con los tepehuanes, coras y huicholes con los que comparten territorio. Su supervivencia se ha debido al aislamiento y lo poco accesible de sus comunidades; viven de una economía autosuficiente; su lengua es la usual en la vida cotidiana. Aunque en su mayoría son bilingües no aprenden el español sino hasta después de los 5 ó 6 años. Cuenta con alrededor de 1300 hablantes.

El pipil (o nawat) es una de las variantes relacionada históricamente con los habitantes de Cuzcatlán, hoy El Salvador y parte de Nicaragua. Se tiene registro que también era hablada en poblaciones de Honduras. Hoy en día en El Salvador está en desuso. Ethnologue reporta 500 personas según estudio del 2015.

El pochuteco fue un idioma nahua que se habló en la Costa Sur de Oaxaca, particularmente en el actual municipio de Pochutla, fue estudiado en 1912 por el antropólogo norteamericano Franz Boas, quien escribió una descripción hasta 1917 en "International Journal of American Linguistics". En la actualidad se considera por algunos estudiosos como la primera variante que evolucionó antes del surgimiento de las ramas oriental y occidental, sin embargo, su estudio no fue completo por lo que en realidad su clasificación es dudosa.

La enseñanza de las lenguas nativas al igual que el español o cualquier idioma en el mundo se da en el seno materno de un hogar, es decir, se trasmite de padres a hijos. Esto conlleva toda una serie de factores socioculturales que determinan distintos grados de asimilación y proporcionan distintos niveles de comprensión e interpretación.

Esencialmente podemos considerar dos tipos de hablantes: quienes simplemente reproducen el habla de su entorno incluso sin necesidad de aprender a escribirlo y por otro lado, quienes lo cultivan ya sea oral o escrito para desarrollar una mayor capacidad de comunicación clara.

La lengua náhuatl durante la época prehispánica carecía de un sistema de escritura para su enseñanza, lo cual era de hecho innecesario, ya que aparte del aprendizaje en el hogar existían instituciones de educación obligatoria donde aprendían a distinguir el habla común (macehuallahtolli) del habla elegante (tecpillahtolli). En las escuelas indígenas (Telpochcalli, Calmecac o Cuicacalli) se ponía mucho énfasis en la adquisición de habilidades en oratoria, se aprendían de memoria largos discursos morales, históricos, obras de teatro y cantos. Esto hacía de la enseñanza de la lengua y en sí de la educación, un modelo de éxito.

Al llegar los españoles y reconocer la utilidad del sistema lo aprovecharon, permitiendo que el náhuatl continuara en uso. Los peninsulares se vieron obligados a aprender el idioma nativo, lo que provocó a su vez que escribieran tratados para su comprensión y utilización para la catequización del indígena. En realidad la enseñanza del náhuatl siguió dándose de manera natural, por medio del uso generalizado. La creación de centros de estudio durante el Virreinato fue para el conocimiento y desarrollo de las ciencias —entre ellas la escolástica— y no para la alfabetización de los indígenas, sólo la nobleza nahua tenía derecho a aprender a leer y escribir su lengua, el grueso del pueblo tenía que acudir ante “escribanos” para la creación de algún documento.

La situación colonial se extendió hasta el siglo XX, donde al inicio de la revolución mexicana la población indígena era 90% analfabeta. Con la creación de la Secretaria de Educación Pública durante el mandato de Álvaro Obregón, se planteó un departamento de educación indígena, sin embargo, continúa el debate entre el uso o desuso de las lenguas vernáculas; es hasta 1940 que con el apoyo del presidente Lázaro Cárdenas se implementan programas de educación en lengua materna, cuyos experimentos pilotos funcionan pero al establecerlos a nivel nacional pierden solidez. Gobiernos posteriores se centran en la castellanización, dejando de lado la enseñanza de las lenguas nativas, a mediados de los ochentas se cuenta ya con una tasa mayor al 70% de indígenas alfabetizados, sin embargo, sólo el 20% sabe escribir su propio idioma.

Nuevos programas pilotos durante los ochentas y noventas por parte de la SEP fueron la base de una educación bicultural enfocada primero en la escritura y habla de la lengua materna, y a partir del cuarto ciclo escolar el aprendizaje del español. Durante el ciclo 1993-94 aparecen los primeros libros de texto gratuito en lengua náhuatl que cubrían hasta el tercer grado de educación primaria, línea que continúa hasta la actualidad (2016) ya que no se han elaborado los materiales para los grados superiores.

Junto con la educación básica la SEP por medio del Instituto Nacional para la Educación de los Adultos (INEA) ha desarrollado material para alfabetizar a personas mayores de 15 años, esto durante la primera década de este siglo, cubriendo nueve zonas escolares indígenas y un número de variantes casi iguales, las cuales son:


El INEA trabaja bajo el programa “Modelo Educación para la Vida y el Trabajo”, abreviado “MEVyT Indígena”, también conocido como MIB. Consta de siete módulos, cuatro de los cuales totalmente en náhuatl; estos materiales están disponibles en línea para todo público y de manera gratuita.

La gramática más antigua que conocemos de la lengua náhuatl, titulada "Arte de la lengua mexicana", fue elaborada por el franciscano fray Andrés de Olmos. Fue concluida el primero de enero de 1547 en el convento de Hueytlalpan, ubicado en el Estado mexicano de Puebla. Destaca el hecho de haber sido desarrollada antes que muchas gramáticas de lenguas europeas como la francesa, y tan sólo 55 años después de la "Gramática de la lengua castellana" de Antonio de Nebrija.

El náhuatl clásico y la mayoría de variedades modernas emplean las siguientes consonantes:

Esta distinción es importante y permite diferenciar muchos pares como "te-" 'piedra(s)' / "tē-" 'gente', "cuahuitl" 'árbol' / "cuāuhtli" 'águila' o "chichi" 'perro' / "chīchī" 'mamar'. Este tipo de ortografía se encuentra en uso activo en la de Wikipedia o "Huiquipedia".

El náhuatl tiene una morfología nominal reducida, la mayoría de nombres tiene dos formas diferentes según el caso (poseído/no-poseído) o sólo una forma indistinguida. El estado absolutivo en singular se marca con /-tl/ y el poseído con /-w/ (ambos sufijos derivados del proto-utoazteca /*-ta/ y /*-wa/). En cuanto a las marcas de plural se usan sobre todo los sufijos /-meh/ (procedente del proto-utoazteca /*-mi/) y a veces /-tin/ y /-h/, y en menor grado se usa la reduplicación de la sílaba inicial "coyotl" 'coyote' "cocoyoh" 'coyotes', aunque esto en náhuatl a diferencia de lenguas uto-aztecas como el guarijío o el pima es marginal.

La morfología verbal a diferencia de la nominal usa un gran número de morfemas prefijos o clíticos que indican, sujeto, objeto, direccional, marca reflexiva. El número se indica además de prefijo de persona mediante sufijos variados, igualmente el tiempo y el modo se indica mediante sufijos. La raíz verbal cambia de forma para indicar aspecto así para el náhuatl clásico se consideran tres tipos (llamados “"temas"”: largo, breve, medio) e incluye gran número de sufijos derivativos.

El orden de los constituyentes es bastante libre aunque en los dialectos modernos tiende a SVO en oposición a VSO que eran más frecuentes en las etapas más antiguas de la lengua. Además las variantes modernas tienden a incluir interjecciones, conjunciones y adverbios prestados del español.

El adjetivo suele preceder al nombre pero al igual que en la sintaxis en las variantes modernas el acomodo refleja la influencia del español, aunque en los sintagmas todavía prevalece que el modificador o complemento suele preceder al núcleo sintáctico. Eso se refleja también en el hecho de que la lengua tiene sufijos que funcionan como postposiciones (núcleos del sintagma apositivo) en lugar de usar preposiciones como en el español.

El náhuatl se distingue por usar un número reducido de lexemas para construir gran cantidad de palabras, lo cual hace que casi toda palabra admita una descomposición en raíces, en su mayoría bisilábicas. Esto da a las palabras una gran transparencia en términos semánticos. Algunos ejemplos de composición léxica se encuentran en topónimos y nombres propios:


Ciertas áreas del léxico moderno han sido muy influidas por el español, así el sistema numérico de base vigesimal ha sido abandonado en favor del sistema decimal del español, quedando solo las formas de 1 a 20 del sistema nativo básicamente. Otras áreas del léxico como la vivienda, el vestido y ciertos términos agrícolas han incorporado también términos del español considerados que describen mejor la realidad tecnológica más moderna.

Originalmente se trataba de una escritura pictográfica con rasgos silábicos o fonéticos tipo rebus. Este sistema de escritura fue adecuado para mantener registros tales como genealogías, información astronómica y listas de tributos, pero no representaba el vocabulario total de la lengua hablada de la forma en que los sistemas de escritura del "viejo mundo" o la escritura maya podían hacerlo. Ya en los tiempos coloniales además los códices fueron usados para enseñar la doctrina cristiana dando origen a los catecismos testerianos que podían ser “leídos” a la usanza anterior a la conquista de México.

Los españoles introdujeron el alfabeto latino, el cual fue utilizado para registrar una gran cantidad de poesía y prosa mexicana, algo que de cierta forma compensó la pérdida de miles de manuscritos mexicas quemados por los invasores europeos. Importantes trabajos de léxico, como el "Vocabulario" de Fray Alonso de Molina en 1571, y descripciones gramaticales, por ejemplo, el "Arte" de Horacio Carochi en 1645, fueron producidos usando variaciones de esta ortografía. La ortografía de Carochi usó cuatro diferentes diacríticos: el macrón (ā) para las vocales largas; el acento (á) para vocales cortas; el circunflejo (â) y el grave (à) para el saltillo en diferente posición.

Actualmente, existen dos convenciones diferentes que usan diferentes subconjuntos del alfabeto latino: la ortografía tradicional y la ortografía moderna que ha aceptado la SEP. La Secretaría de Educación Pública de México (SEP) es la institución que regula las reglas de la ortografía, y la que ha establecido un sistema de escritura práctico que se enseña en los programas de educación primaria bilingües en las comunidades indígenas. El siguiente cuadro recoge convenciones usadas en la ortografía clásica y en la ortografía de las variantes modernas para transcribir los diferentes fonemas:

A continuación se reproduce una lista de cognados de varias variantes dialectales en diferentes grupos que permiten reconocer los parentescos más cercanos y la evolución fonológica según la región:

Entre los pueblos nahuas existía un gran aprecio por la poesía, llamada «"In Xōchitl In Cuīcatl"», que significa «La Flor y El Canto» literalmente, aunque hay quien la interpreta como «palabra florida o florecida». La poesía era una de las actividades especialmente practicada entre la clase noble. Huexotzingo, Texcoco, Culhuacan eran las ciudades más renombradas por sus poesías. Ocasionalmente se organizaban encuentros poéticos en donde se reunían incluso aquellos dirigentes de ciudades en guerra. El más famoso ocurrió en Huexotzingo en 1490, organizado por Tecayehuatzin, señor de ese lugar. Detalles de este encuentro y muchas otras poesías se hallan en varios manuscritos recopilados después de la Conquista. El más famoso se llama Cantares mexicanos, y data del siglo XVI. Existe también otra recopilación de poesía, hecha por Juan Bautista Pomar, nieto de Nezahualcóyotl.

Bernardino de Sahagún menciona que los mexicas disfrutaban de representaciones dramáticas, algunas cómicas, otras eróticas, y otras sobre la vida de sus dioses; estas representaciones se transformaron en los tiempos coloniales con tonos cristianos-sincréticos dando origen a la danzas de conquista y a las representaciones de “pastorelas” hasta nuestros días.

De los miles de manuscritos prehispánicos, solo sobrevive una docena de códices, dado que los europeos tenían la creencia que los pobladores indígenas eran adoradores del diablo y por consiguiente quemaron y destruyeron prácticamente toda su obra.

Existen también relaciones y documentos en náhuatl producidos por los «pilli» y tlacuilos, que poco después de la Conquista comenzaron a aprender a usar la escritura europea, como «Los anales de Tlatelolco» y el original náhuatl del Códice Florentino.

Algunos autores novohispanos como Sor Juana Inés de la Cruz, escribieron algunas obras en náhuatl.
Poesía náhuatl ("vid" Nezahualcóyotl) y
Nican Mopohua ("vid" Antonio Valeriano).

El náhuatl clásico se caracteriza por la abundancia de recursos literarios, siendo particularmente importantes los siguientes:


Los archivos musicales conventuales y catedralicios de México también dan cuenta de música en lengua náhuatl, tal es el caso de los motetes "In ilhuicac cihuapille" y "Dios itlazo nantzine" atribuibles a Hernando Franco y extraídos del "Códice Valdés". Gaspar Fernandes, Maestro de Capilla de la Catedral de Puebla en el siglo XVII, compuso una enorme cantidad de villancicos en náhuatl solo, o náhuatl y español; varios de ellos se encuentran en el “Cancionero Musical de Gaspar Fernandes”, importante documento conservado en el Archivo Musical de la Catedral de Oaxaca.

Xochipitzahuac es la canción más popular en el mundo náhuatl. El canto a la virgen de Guadalupe se remonta al periodo colonial novohispano, se desconoce el autor pero se cree que esta canción ha sido de dominio popular, según investigaciones dicen que Xochipitzahuac se comenzó a cantar después de las apariciones de la Virgen María. En la actualidad la canción se sigue cantando en honor a la virgen de Guadalupe, diversas agrupaciones la han cantado como un huapango o con acompañamiento instrumentos cuerda (guitarras, bandurrias o violines).

La convivencia de cinco siglos entre el náhuatl y el español en México ha tenido un impacto en ambas lenguas, y es el español la lengua que mayor integración ha tenido en el náhuatl respecto a otros idiomas. La influencia del náhuatl en el español se refleja especialmente en la gran cantidad de préstamos léxicos que el español ha tomado del náhuatl. En menor medida el español mexicano presenta marginalmente algunos rasgos fonéticos en parte atribuibles al náhuatl, incluyendo la africada lateral alveolar sorda t͡ɬ, pronunciada en el dígrafo tl. La influencia en la gramática en el español mexicano es todavía menos clara pero se ha argumentado que podría haber influido en la frecuencia de ciertas construcciones y tendencias ya presentes en el español general.

Por otra parte el amplio bilingüismo náhuatl-español entre los hablantes de náhuatl ha influido en el náhuatl, tanto a nivel léxico como a nivel gramatical también.

En la gramática, uno puede citar como influencia del náhuatl el uso del sufijo -le para darle un carácter enfático al imperativo. Por ejemplo: "brinca -> bríncale, come -> cómele, pasa -> pásale, etcétera". Se considera que este sufijo es un cruce del pronombre de objeto indirecto español le con las interjecciones excitativas nahuas, tales como cuele. Sin embargo, este sufijo no es un verdadero pronombre de objeto indirecto, ya que se usa aún en construcciones no verbales, tales como: "hijo -> híjole, ahora -> órale, que hubo -> quihúbole, etcétera."

La RAE acepta alrededor de 200 préstamos del náhuatl al español, incluyendo:

acocil, aguacate, ahuehuete, ajolote, amate, atole, ayate,
cacahuate, camote, canica, capulín, chalmichi, chamagoso, chapopote, chapulín,
chayote, chicle, chile, chipotle,
chocolate, comal, copal,
coyote, cuate, ejote, elote, epazote, escuincle,
guacamole, guajolote, huachinango, huipil, hule, jacal, jícama,
jícara, jitomate, macana, mecate, mezcal, milpa, mitote, mole,
nopal, ocelote, ocote, olote, papalote,
pepenar, petaca, petate, peyote, pinole, piocha, popote, tlalcoyote, pilcate, quetzal, tamal, tejocote, tianguis, tiza, tomate,
tule, zacate, zapote, zopilote.

Además ha donado un sinfín de topónimos, incluyendo "México" (Mēxihco), "Guatemala" (Cuauhtemallān).

Las variedades modernas de náhuatl muestran diferente grado de impacto por el español. La influencia del español se refleja especialmente en el préstamo masivo de preposiciones, conjunciones y nexos del español. Esto ha generado una reestructuración de ciertas partes de la sintaxis especialmente en el orden sintáctico y en el uso de ciertas construcciones. También el viejo sistema de cuenta en base 20 ha sido abandonado en favor del sistema decimal del español, por lo que sólo se usan los nombres nativos de números para números inferiores a diez o veinte.

También la generalización del uso del numeral "ce" '1' como equivalente del español 'un(o), una' es notorio en algunas variantes de náhuatl. Y también naturalmente se da una influencia del léxico del español, especialmente para designar realidades tecnológicas nuevas o términos un tanto técnicos para el modo de vida rural de la mayoría de hablantes.

Entre los préstamos del español al náhuatl son los siguientes: "axno" (< asno), "cahuayoh" (< caballo), "cafetzin" (< café), "col", "hicox" (< higo), "huino" (< vino), "limon" (limón), "melon", "manzana", perexil (< perejil), "rahuano" (< rábano), "torazno" (< durazno), etc.







</doc>
<doc id="8257" url="https://es.wikipedia.org/wiki?curid=8257" title="Neurociencia">
Neurociencia

La neurociencia es un campo de la ciencia que estudia el sistema nervioso y todos sus aspectos; como podrían ser su estructura, función, desarrollo ontogenético y filogenético, bioquímica, farmacología y patología; y de cómo sus diferentes elementos interactúan, dando lugar a las bases biológicas de la cognición y la conducta.

El estudio biológico del cerebro es un área multidisciplinar que abarca muchos niveles de estudio, desde el puramente molecular hasta el específicamente conductual y cognitivo, pasando por el nivel celular (neuronas individuales), los ensambles y redes pequeñas de neuronas (como las columnas corticales) y los ensambles grandes (como los propios de la percepción visual) incluyendo sistemas como la corteza cerebral o el cerebelo, e incluso, el nivel más alto del sistema nervioso.

En el nivel más alto, las neurociencias se combinan con la psicología para crear la neurociencia cognitiva, una disciplina que al principio fue dominada totalmente por psicólogos cognitivos. Hoy en día, la neurociencia cognitiva proporciona una nueva manera de entender el cerebro y la consciencia, pues, se basa en un estudio científico que une disciplinas tales como la neurobiología, la psicobiología o la propia psicología cognitiva, un hecho que con seguridad cambiará la concepción actual que existe acerca de los procesos mentales implicados en el comportamiento y sus bases biológicas.

Las neurociencias ofrecen un apoyo a la psicología con la finalidad de entender mejor la complejidad del funcionamiento mental. La tarea central de las neurociencias es la de intentar explicar cómo funcionan millones de células nerviosas en el encéfalo para producir la conducta, y cómo a su vez, estas células están influidas por el medio ambiente. Tratando de desentrañar la manera de cómo la actividad del cerebro se relaciona con la psiquis y el comportamiento, revolucionando la manera de entender nuestras conductas y lo que es más importante aún: cómo aprende, cómo guarda información nuestro cerebro y cuáles son los procesos biológicos que facilitan el aprendizaje.

Algunos de los problemas aún no resueltos de la neurociencia son:


Las neurociencias exploran campos tan diversos como:

Entre las áreas relacionadas con la neurociencia se encuentran:

En 1791 Luigi Galvani, un biólogo de Bolonia, descubrió la existencia de actividad eléctrica en los animales. Había colgado la pata de una rana en un gancho de cobre suspendido de un balcón de hierro. La interacción entre los dos metales hacía que la pata se contrajera.

Hermann von Helmholtz descubrió que la generación de electricidad por parte de los axones de las células nerviosas no es un producto secundario de su actividad, sino un medio para transmitir mensajes de un extremo a otro. Logró medir, en 1859, la velocidad de propagación de tales mensajes, y llegó a la conclusión de que se propagan a 27 metros por segundo.

Camillo Golgi desarrolló un método de tinción con cromato de plata, que permite colorear una neurona entre muchas otras. Compartió el Premio Nobel de Medicina de 1906 con Santiago Ramón y Cajal.

Santiago Ramón y Cajal dio a la célula nerviosa el nombre de neurona, unidad elemental del sistema de señalización del sistema nervioso. Descubre que el axón de una neurona sólo se comunica con las dendritas de otra en regiones especializadas: las sinapsis. Además, una neurona determinada sólo se comunica con ciertas células, y no con otras. En el interior de la neurona, las señales fluyen en una dirección única. Este principio permite determinar el flujo de la información en los circuitos neurales. Encontró que existen tres tipos principales de neuronas: sensorial, motora e interneurona.

Charles Sherrington estudió los fundamentos neurales del comportamiento reflejo. Descubrió que es posible inhibir las neuronas además de excitarlas, y que la integración de esas señales determina la acción del sistema nervioso.

Edgar Adrian ideó métodos para registrar los potenciales de acción, que son las señales eléctricas utilizadas por las neuronas para la comunicación. Descubre que son señales de tipo "todo o nada", es decir, o bien se presentan completas o bien no se presentan en absoluto. Compartió el Premio Nobel de Medicina con Charles Sherrington.

Julius Bernstein, discípulo de Wilhelm Helmholtz, propuso en 1902 la hipótesis de la membrana porosa para describir el proceso de conducción eléctrica en las neuronas. Dedujo que hay una diferencia de potencial entre el interior y el exterior de la célula nerviosa, incluso cuando la célula está en reposo.

Alan Hodgkin y Andrew Huxley desarrollaron investigaciones sobre el axón gigante de las células nerviosas de los calamares. Confirman la hipótesis de Julius Bernstein de que el potencial de membrana en reposo se genera por el desplazamiento de iones de potasio hacia el exterior de la célula y de iones de sodio hacia su interior. Compartieron el Premio Nobel de Medicina de 1963 con John Eccles, por la investigación sobre las bases iónicas de la transmisión nerviosa.

Henry Dale y Otto Loewi propusieron la teoría química de la transmisión sináptica. Descubrieron, en forma independiente, que cuando el potencial de acción de una neurona del sistema nervioso autónomo llega a los terminales del axón, causa la liberación de una sustancia química en la hendidura sináptica. Recibieron el Premio Nobel de Medicina de 1936.

Edwin Furshpan y David Potter descubrieron, en una langosta de río, que también es posible la transmisión eléctrica entre dos células nerviosas, si bien la mayoría de las sinapsis son de origen químico.

Bernard Katz descubrió que cuando un potencial de acción ingresa en la terminal presináptica causa la apertura de los canales de calcio, lo que permite la afluencia de este elemento químico al interior de la célula. La abundancia de calcio, a su vez, determina la liberación de los neurotransmisores en la hendidura sináptica. El neurotransmisor se une a los receptores superficiales de la neurona postsináptica, y las señales químicas se retraducen a señales eléctricas. Compartió el Premio Nobel de Medicina de 1970 con Ulf von Euler y Julius Axelrod por los estudios realizados sobre neurotransmisores.

Rodolfo Llinás cambió el dogma establecido desde que Ramón y Cajal enunció su ley de la polarización sobre el aspecto funcional de las neuronas. Rodolfo Llinás presentó el nuevo punto de vista funcional sobre la neurona en su artículo "". Rodolfo Llinás con sus colaboradores investigó durante los años 80 el funcionamiento electrofisiológico de las neuronas en los vertebrados, descubriendo las propiedades electrofisiológicas. Anteriormente se habían observado propiedades intrínsecas en los invertebrados pero se pensaba que éstas eran únicamente una cuestión relativa a esa línea, pero Llinás y sus colaboradores demostraron que las neuronas de los vertebrados tienen propiedades electrofisiológicas intrínsecas. El nuevo punto de vista funcional sobre la neurona quedó resumido en lo que hoy es conocido por la Ley de Llinás.

El descubrimiento de cada sustancia química considerada mediadora de la intercomunicación neuronal aportaba nuevos elementos de conocimiento de la compleja red de conexiones entre células nerviosas y de sus correspondientes características funcionales.

Eric Kandel esclareció el papel de los transmisores en el complejo proceso de la memoria y el aprendizaje, estableciendo que la memoria es evocada por cambios directos en los millones y millones de sinapsis que forman los puntos de contacto entre las neuronas.

Antonio Alcalá Malavé consiguió en 2002 descubrir que las áreas cerebrales 17,18 y 19 de Brodman servían además de para inducir el fenómeno físico y químico de la visión, para informar del riesgo cardiovascular y algunas demencias. Ese "informe biológico" se traduce como fallo visual en la calidad, cantidad, color y contraste de las imágenes que llegaban al cerebro o que eran procesadas por el mismo aunque ya aberradas. Sus trabajos son verificables por campimetra computarizada y análisis computarizado cromático.

Roderick MacKinnon obtuvo en 2004 la primera imagen tridimensional de los átomos que forman la proteína de los dos canales iónicos: un canal pasivo de potasio y un canal de potasio activado por voltaje. Recibió el Premio Nobel de Química.

En 2014 los psicólogos y neurocientíficos noruegos Edvard Moser y su esposa May-Britt Moser compartieron con el británico John O’Keefe el Premio Nobel de Fisiología o Medicina por sus estudios sobre las células de lugar del hipocampo: una clase de neuronas que codifica la ubicación espacial en la que se encuentran los mamíferos como las ratas y los seres humanos, y les permiten orientarse en el espacio. Ciertos grupos de neuronas hipocampales se activan o no, dependiendo del lugar de una habitación en el que un sujeto se encuentre en un momento determinado.

Además de la secuencia histórica asociada a la neurona y a los conjuntos neuronales, es posible seguir la evolución de las neurociencias considerando la secuencia histórica de las teorías destinadas a establecer la función de cada sector del cerebro, o bien la consideración de que no existiría una locación concreta de las funciones cerebrales.

El neurólogo alemán Franz Joseph Gall (1758-1828) desarrolló el sistema frenológico, mediante el cual cada facultad psíquica tendría su asiento en determinado grupo de células cerebrales. Así, toda la corteza cerebral estaría constituida por "órganos" distintos.

El fisiólogo francés Pierre Flourens efectuaba la ablación de partes del cerebro de animales y estudiaba su conducta. De manera que, según lo que los animales dejaban de hacer, podía inferir las funciones de la parte extraída. Observó que con el tiempo se restablecía la función original, con independencia de la parte dañada.

Luego del fallecimiento de un paciente con trastornos en el lenguaje, el neurólogo y antropólogo francés Paul Broca estudió su cerebro y encontró una lesión en el tercio posterior de la circunvolución frontal inferior del hemisferio izquierdo. Estudió a otros pacientes con problemas similares y encontró las mismas lesiones en la ahora denominada "área de Broca". Este especialista llegó a afirmar: "Nosotros hablamos con el hemisferio izquierdo".

Carl Wernicke descubrió la que ahora se denomina "área de Wernicke", zona del cerebro cuyas lesiones producen perturbaciones en la comprensión del habla. Sus descubrimientos, junto a los de Paul Broca, estimularon los estudios localizacionistas durante el siglo XIX.

Walter Rudolf Hess descubrió la organización funcional del cerebro medio como coordinador de las actividades de los órganos internos. Empleando estimulación eléctrica en ciertas zonas del mesencéfalo, Hess pudo reproducir funciones autónomas espontáneas, modificaciones en la respiración o la circulación, entre otras respuestas.

Los estudios de Roger W. Sperry permitieron determinar que, aunque cada uno de los dos hemisferios del cerebro (izquierdo y derecho) intercambia información con el otro a través del cuerpo calloso y otras comisuras más pequeñas, existen notables diferencias en la forma de procesamiento de la información entre uno y otro.

David H. Hubel y Torsten Wiesel descubrieron las características del procesamiento de la información visual. Estudiando su desarrollo en gatos pequeños, detectaron la capacidad de las neuronas corticales para reorganizarse ante situaciones de privación sensorial y determinaron que la reorganización de las neuronas corticales ocurre sólo en periodos determinados.







</doc>
<doc id="8258" url="https://es.wikipedia.org/wiki?curid=8258" title="Niels Ryberg Finsen">
Niels Ryberg Finsen

Niels Ryberg Finsen (Thorshavn, Islas Feroe, 15 de diciembre de 1860 - Copenhague, 24 de septiembre de 1904) fue un médico danés, conocido por haber descubierto el efecto germicida de la luz ultravioleta. Recibió el Premio Nobel en 1903.

En 1882, viaja a Copenhague para estudiar medicina en su universidad, aprobando su examen final en 1890. Posteriormente sería profesor en dicha universidad.

Los hallazgos de Émile Duclaux sobre la capacidad destructora de los rayos ultravioleta, aplicados a colonias de bacterias fueron la base de su trabajo. Alcanzó la fama por sus investigaciones sobre los efectos fisiológicos de la luz, ya que descubrió las propiedades estimulantes y bactericidas de los rayos actínicos (azul, violeta y ultravioleta). Desarrolló una lámpara eléctrica de arco voltaico (luz de Finsen) para el tratamiento del lupus tuberculoso y otras afecciones cutáneas similares. 

Con objeto de poder continuar su trabajo y las aplicaciones de sus métodos fototerapéuticos se creó en Copenhague el Instituto Finsen en 1896.

Su enfermedad le impidió en 1903 recoger el Premio Nobel de Medicina que le fue otorgado.

Entre sus publicaciones más conocidas figuran una obra destinada a explicar la teoría general del efecto de la luz sobre el organismo vivo (1895) y otra sobre el empleo médico de los rayos de luz química concentrados ("Om Anvendelse in Medicinen af koncentrerede kemiske lysstraaler", 8 vv. 1896).




</doc>
<doc id="8259" url="https://es.wikipedia.org/wiki?curid=8259" title="Charles Louis Alphonse Laveran">
Charles Louis Alphonse Laveran

Charles Louis Alphonse Laveran (París, 18 de junio de 1845 - ibíd. 18 de mayo de 1922) fue un médico y naturalista francés, ganador del premio Nobel de Medicina en 1907, por su descubrimiento de los parásitos protozoarios como agente causal de enfermedades infecciosas como la malaria y tripanosomiasis. Fue el primer científico francés en obtener el premio Nobel. 

Alphonse Laveran nació en Boulevard Saint-Michel en París. Sus padres fueron Louis Théodore Laveran y Marie-Louise Anselme Guénard de la Tour Laveran. Fue hijo varón único con su hermana. Su familia tenía ambiente militar. Su padre fue médico del ejército y Profesor de medicina militar en la École de Val-de-Grâce. Su mamá fue hija de un comandante del ejército. Siendo muy joven su familia fue a Argelia (en esa época colonia del Imperio Francés), acompañando a su padre en su servicio. Fue educao en París, y completó su educación en Collège Saint Barbe. Siguió a su padre en la medicina militar ingresando a Public Health School en Estraburgo en 1863. En 1866 fue estudiante médico residente en los hospitales civiles de Estraburgo. En 1867, presentó su tesis basada en la regeneración de los nervios con la cual obtuvo el título de médico de la University of Strasbourg.

Laveran fue médico Asistente Mayor en el ejército francés durante la Guerra Franco-Prusiana. Estuvo apostado en Metz, donde el ejército francés sería derrotado y la plaza ocupado por los alemanes. Trabajó en el hospital de Lille y en el Martin Hospital (ahora St Martin's House) en París. En 1874 calificó en un examen de competencia para posteriormente ser nombrado Chair of Military Diseases and Epidemics at the École de Val-de-Grâce, una posición que su padre había ocupado. Su tenencia terminó en 1878 y se fue a Argelia, donde permaneció hasta 1883. De 1884 a 1889 fue Profesor Military Hygiene at the École de Val-de-Grâce. (Profesor de Higiene Militar en la escuela de Val-du-Grâce). En 1894 asumió el cargo de Chief Medical Officer of the military hospital at Lille and then Director of Health Services of the 11th Army Corps (Director Médico Oficial del hospital Militar en Lille y Director de los Servicios de Salud el 11° Cuerpo del Ejército en Nantes. Por ese tiempo fue promovido al rango de Principal Medical Officer of the First Class (Oficial Médico Principal de Primera Clase). En 1896 fue Director del Instituto Pasteur como jefe honorario de tiempo completo en las enfermedades tropicles.


Laveran se casó con Sophie Marie Pidancet en 1885. No tuvieron hijos.
En 1922 sufrió un enfermedad indefinida por algunos meses y murió en París. Está enterrado en el Cimetière du Montparnasse (Cementerio de Montparnasse) en París. Fue ateo.





</doc>
<doc id="8260" url="https://es.wikipedia.org/wiki?curid=8260" title="Emil Theodor Kocher">
Emil Theodor Kocher

Emile Theodor Kocher (n. Burgdorf, (Cantón de Berna), 25 de agosto de 1841 - Berna, 27 de julio de 1917), fue un médico suizo galardonado con el Premio Nobel de Medicina en 1909 por sus trabajos sobre los tratamientos sobre las afecciones de la glándula tiroides. 

Kocher estudió en Berna, París, Berlín y Londres, obteniendo el doctorado en Berna en 1865. En 1872 sucedió a Georg Albert Lucke como profesor de cirugía de la Universidad de Berna y director de la Clínica Universitaria Quirúrgica de Berna, cargos que ocupó hasta 1911.

Publicó trabajos relativos a la glándula tiroides (ideando un método para su trasplante), tratamientos antisépticos, infecciones quirúrgicas, heridas de bala y osteomielitis aguda. Descubrió un método para reducir la luxación del hombro e introdujo nuevas técnicas en las operaciones de hernia y de cáncer de estómago. Realizó mejoras técnicas a numerosos tipos de operaciones quirúrgicas del pulmón, el estómago y la vesícula biliar.

En 1878 practicó con éxito la primera extirpación del bocio, operación que tuvo ocasión de repetir más de 2000 veces dado que vivía en una de las zonas más bociógenas del mundo. En 1912 preparó un coaguleno esterilizado que, inyectado, aumenta la coagulación de la sangre y sirve para prevenir y tratar hemorragias internas. 

Sus éxitos en el campo de la cirugía fueron debidos en buena parte por la mejora del instrumental y por la utilización sistemática de la asepsia. Se le debe la invención de las pinzas hemostáticas de grapas que llevan su nombre. Considerado como el mejor cirujano de su época, fue elegido presidente del Primer Congreso Internacional de Cirugía (1905) 

En 1909 se le otorgó el Premio Nobel de Medicina por sus trabajos sobre la fisiología, patología y cirugía de la glándula tiroides. Con el importe del premio ayudó a fundar el Instituto Kocher en Berna, escuela de cirugía en la que se formaron grandes cirujanos. 




</doc>
<doc id="8263" url="https://es.wikipedia.org/wiki?curid=8263" title="Pavlov">
Pavlov

Pavlov (o "Pávlov") es un apellido ruso, cuya forma femenina es Pavlova (o "Pávlova"). Puede hacer referencia a:


También, puede referirse a:

</doc>
<doc id="8266" url="https://es.wikipedia.org/wiki?curid=8266" title="Síndrome de Kocher">
Síndrome de Kocher

El síndrome de Kocher-Debre-Semelaigne es una enfermedad rara de niños caracterizada por hipotiroidismo moderado o severo de larga duración asociado a una pseudohipertrofia muscular o aumento de la masa muscular por aumento de volumen de los tejidos intersticiales pero con atrofia de células musculares con la subsecuente miotonia.

Aunque el síndrome fue reportado inicialmente por Emil Theoro Kocher (1892) fueron Rober Debré y George Semelaigne quienes lo describieron más ampliamente en 1935 destacando la asociación entre el problema endocrino y el trastorno muscular.

Los pacientes, generalmente niños entre los 18 meses y los 10 años, presentan signos clínicos del hipotiroidismo o manifestaciones típicas de cretinismo, incluyendo: disminución de la actividad y aumento del sueño, dificultad en la alimentación y estreñimiento, ictericia prolongada, facies mixedematosa, fontanelas grandes (especialmente las posteriores), macroglosia, abdomen distendido con hernia umbilical, e hipotonía. La pseudohipertrofia afecta a los músculos de extremidades, cinturas, tronco, manos y pies, siendo más prominente en las extremidades lo que conlleva un aspecto atlético del paciente.

Se desconoce la fisiopatología de la afección pero se ha propuesto que la pseudohipertrofia muscular puede ser el resultado de un hipotiroidismo de larga duración. El síndrome puede presentarse con todas las formas de hipotiroidismo.

El diagnóstico se basa en los signos clínicos y en los cambios miopáticos. El tratamiento se basa en suplementos tiroideos con el cual pueden revertirse la pseudohipertrofia y los síntomas clínicos complementado con medidas para la rehabilitación consistentes en terapia física, ejercicio dirigido moderado y terapia ocupacional, entre otros.

En adultos una afección similar se denomina síndrome de Hoffman.



</doc>
<doc id="8272" url="https://es.wikipedia.org/wiki?curid=8272" title="Astarté">
Astarté

Astarté (en fenicio 𐤀‏𐤔‏𐤕‏𐤓‏𐤕‏ , "ʾAshtart") es la asimilación fenicia-cananea de una diosa mesopotámica que los sumerios conocían como Inanna, los acadios, asirios y babilonios como Ishtar y los israelitas como Astarot.

Representaba el culto a la madre naturaleza, a la vida y a la fertilidad, así como la exaltación del amor y los placeres carnales. Con el tiempo, se tornó también en diosa de la guerra y recibió cultos sanguinarios de sus devotos. Se la solía representar desnuda o apenas cubierta con un fino cinturón, de pie sobre un león.

Astarté es cognada en nombre, origen y funciones con la diosa Ishtar de los textos de la Mesopotamia. Otra transliteración es ’Ashtart.


Todas ellas estaban identificadas invariablemente con el lucero del alba o planeta Venus.

De acuerdo con el libro "The Early History of God", Astarté sería la encarnación correspondiente a la Edad de Hierro (después del 1200 a. C.) de la diosa Ashera, de la Edad de Bronce (antes del 1200 a. C.).

Los nombres Astarté e Ishtar están relacionados con el planeta Venus. Son cognados de:




</doc>
<doc id="8273" url="https://es.wikipedia.org/wiki?curid=8273" title="RAE (desambiguación)">
RAE (desambiguación)

El acrónimo RAE puede referirse a los siguientes organismos:



</doc>
<doc id="8275" url="https://es.wikipedia.org/wiki?curid=8275" title="Ishtar">
Ishtar

Ištar o "Ishtar" era la diosa babilónica del amor y la belleza, de la vida, de la fertilidad, y patrona de otros temas menores. Se asociaba principalmente con la sexualidad: su culto implicaba la prostitución sagrada; la ciudad sagrada Uruk se llamaba la "ciudad de las cortesanas sagradas", y ella misma fue la "cortesana de los dioses". Ištar tenía muchos amantes; sin embargo, como señala Guirand: 

Incluso para los dioses el amor de Ištar fue fatal. En su juventud la diosa había amado a Tammuz, dios de la cosecha y, de acuerdo con la epopeya de Gilgamesh, este amor causó la muerte de Tammuz. 

Se asocia en otras regiones con diosas como Inanna en Sumeria, Anahit en la antigua Armenia (Urartu), Astarté (Asera) en Canaán, Fenicia y en las religiones abrahámicas. Ištar, Inanna y estas diosas representan el arquetipo de la diosa madre.

En Sumeria era conocida como Inanna (siendo dos diosas distintas que representan lo mismo) y posteriormente en Babilonia, y en su zona de influencia cultural en todo Oriente Medio, recibe los títulos honoríficos de Reina del Cielo y Señora de la Tierra.

Para Joseph Campbell Ištar/Inanna, que amamanta al dios Tammuz, es la misma diosa que Afrodita y que la egipcia Isis, que alimenta a Horus.

Hija de Sin, dios de la Luna, y Nannar, la Luna. Hermana menor de Ereškigal y hermana gemela de Šamaš, en sumerio Utu, dios del Sol. Compañera de Tammuz, en sumerio Dumuzi.

Su número asociado en el panteón de la mitología mesopotámica es el 15.

Como primer arquetipo psicológico de la dinámica femenina en la historia, y en contraposición a su hermana Ereškigal o a Ki, la diosa de la tierra, Ištar no se puede considerar dentro del grupo de las diosas madre, puesto que su relación con los humanos es más como inspiración para la acción vital que como refugio. Con este carácter, Ištar aparece en la epopeya de Gilgamesh.

Se la asocia al planeta Venus, estrella de la mañana y del anochecer. Su símbolo es una estrella de ocho puntas. En su honor, los astrónomos han llamado Ishtar Terra a un continente de Venus. Su animal asociado es el león.

Ishtar era hija de Sin (dios lunar) o de Anu. En carácter de hija de aquél, era la dama bélica; como descendiente de éste, el exponente del amor, la licenciosidad y la intemperancia y la violencia caprichosa hasta el extremo. 

Bajo el aspecto guerrero se le rendía culto en Agadé y en Sippar, con el nombre de Anunit. También tiene un carácter astral, ya que personifica a varios astros: a Venus, al Sol, la Luna, y a las estrellas reunidas en constelaciones. 

Ishtar estaba asociada al planeta Venus como estrella de la mañana, y en las fronteras de Babilonia se la representaba mediante una estrella de ocho puntas. También, de pie, completamente desnuda, con las manos encima del vientre, o sosteniéndose los senos, o blandiendo un arco sobre un carro tirado por leones. 

En su aspecto de divinidad amorosa, Isthar es la protectora de las prostitutas y de los amoríos extramaritales, que por cierto no tenían connotación especial en Babilonia, ya que el matrimonio era un contrato solemne que perpetuaba la familia como sostén del Estado y como generadora de riquezas, pero en el que no se hablaba de amor o de fidelidad amorosa. 

Ištar no es una diosa del matrimonio, ni es una diosa madre. El matrimonio sagrado o la sacra hierogamia, que se representaba todos los años en el templo babilónico, no tiene un implicación moral ni es modelo de matrimonios terrestres, es un rito de fertilidad altamente estilizado con tonos litúrgicos. 

Su versión sumeria, Inanna, fue muy venerada a partir del reinado de Sargón.

También en la Biblia, en el libro del profeta Jeremías, se le nombra en el capítulo 44 refiriéndose a ella como La Reina del Cielo. Jeremías hace la denuncia de que no se debe adorar a dioses falsos.

Ishtar recibió culto en el templo babilónico llamado E.tur.kalam.ma. En 1778 a. C. Hammurabi construyó un trono para hacer culto a Ishtar, y en 1775 a. C. confeccionó imágenes para esta misma.

Su primer esposo fue su hermano Tammuz. Al morir Tammuz, Ishtar descendió a los infiernos para arrancarle a su hermana, la terrible Ereškigal, el poder sobre la vida y la muerte.

Después de darle instrucciones a su sirviente Papsukal, de ir a rescatarla si no regresaba, descendió a la tierra de las tinieblas, Irkalla. Comenzó valiente y desafiante, gritando al portero que abriera la puerta antes de que la echase abajo. Pero en cada una de las siete puertas era despojada de una de sus prendas, y con ellas se iba despojando de su poder, hasta que llegó desnuda e indefensa ante Ereškigal, que la mató y colgó su cuerpo en un clavo. 

Con su muerte, todo el mundo comenzó a languidecer. Pero el fiel Papsukal llegó hasta los dioses y les pidió que creasen un ser capaz de entrar en el mundo de los muertos y resucitase a Ishtar con la comida y el agua de la vida. Así es como Ishtar volvió a la vida, pero tenía que pagar el precio: durante seis meses al año, Tammuz debe vivir en el mundo de los muertos. Mientras está allí, Isthar ha de lamentar su pérdida; en primavera, vuelve a salir y todos se llenan de gozo.

Algnos consideran a esta leyenda como el origen de la llamada "Danza de los siete velos".

Existen otros nombres relacionados con esta diosa: Astarté, Astaroth, Esther y Stára (en persa).



</doc>
<doc id="8277" url="https://es.wikipedia.org/wiki?curid=8277" title="Estrecho de Malaca">
Estrecho de Malaca

El estrecho de Malaca es un largo estrecho de mar del sudeste de Asia localizado entre la costa occidental de la península malaya y la isla indonesa de Sumatra, un importante corredor marítimo que une, al norte, el mar de Andamán, mar marginal del océano Índico, y al sur el mar de la China Meridional.

Se entiende que el límite occidental del océano Pacífico está en el estrecho de Malaca.

El estrecho se extiende en dirección SE-NO y tiene aproximadamente 800 km de longitud, con una anchura entre 50 km y 320 km. Tiene sólo 2,8 km de ancho en su punto más angosto, el estrecho de Philips, en el estrecho de Singapur. En su parte media se encuentra su mínima profundidad la que condiciona el calado de los buques que lo atraviesan (proximidades de Port Kelang, "One Fathom Bank"). En la parte sureste, el estrecho comunica con el estrecho de Singapur y está cerrado por varias islas del grupo del archipiélago de Riau que permiten la navegación por varios canales de paso.

Ha adquirido un importante papel estratégico, siendo la principal vía de abastecimiento de petróleo de dos de los principales consumidores mundiales, Japón y China. En promedio, 150 barcos pasan a diario a través del estrecho que es una ruta de navegación importantísima ya que vincula todo el mar de la China Meridional con el océano Índico y con Europa vía canal de Suez.
Los puertos más importantes son Malaca (Malasia) y Singapur, en el extremo meridional de este estrecho, uno de los más grandes del mundo en cuanto a volumen de carga anual, aunque Singapur está a orillas de otro estrecho independiente, el estrecho de Singapur.

El estrecho de Malaca recibe el nombre por la ciudad que se encuentra a sus orillas.

Desde una perspectiva económica y estratégica, el estrecho de Malaca es una de las rutas de navegación más importantes del mundo. El estrecho es el principal canal de transporte marítimo entre el océano Índico y el océano Pacífico, y une las principales economías asiáticas como India, China, Japón y Corea del Sur. Más de 50.000 buques pasan por el estrecho por año, llevando aproximadamente una cuarta parte del comercio mundial de mercancías como el petróleo, manufacturas chinas, café indonesio.

Alrededor de una cuarta parte del total de petróleo transportado por mar pasa a través del estrecho, principalmente de los proveedores del golfo Pérsico a los mercados asiáticos como China, Japón y Corea del Sur. En 2006, se estima que 15 millones de barriles por día (2.400.000 m³/d) fueron transportados a través del estrecho.

El tamaño máximo de los buques que pueden hacer la travesía del estrecho se denomina Malaccamax. El estrecho no es lo suficientemente profundo (unos 25 metros o 82 pies) para permitir que algunos de los más grandes buques (la mayoría de los petroleros) lo utilicen. En el canal de Phillips, cerca de Singapur, el estrecho de Malaca se reduce a 2,8 km de anchura, formando uno de los estrangulamientos de tráfico más importantes del mundo. Desde la erupción del Krakatoa en 1883, la navegación de grandes buques por el estrecho de Sonda no es segura, por lo que un buque que exceda Malaccamax debe utilizar como alternativa el estrecho de Lombok.

La piratería en el estrecho ha aumentado en los últimos años. Hubo alrededor de 25 ataques contra buques en 1994, 220 en 2000, y poco más de 150 en 2003 (un tercio del total mundial). Después, los ataques aumentaron de nuevo en el primer semestre de 2004, lo que llevó a que las armadas de Malasia, Indonesia y Singapur intensificaran sus patrullas en la zona en julio de 2004. Posteriormente, los ataques a buques en el Estrecho de Malaca ha caído, a 79 en 2005 y 50 en 2006.

En el estrecho hubo 34 naufragios documentados, algunos que datan de la década de 1880, recogidos en el Plan de Separación de Tráfico («Traffic Separation Scheme», TSS), el canal para buques comerciales. Estos pecios plantean un riesgo de colisión en los estrechos y zonas de aguas superficiales.

Otro riesgo es la neblina anual causada por los incendios forestales que asolan Sumatra, que puede reducir la visibilidad a 200 m, lo que obliga a que los buques ralenticen su discurrir por el concurrido estrecho. Los buques de más de 350 m utilizan de forma habitual el estrecho.

Tailandia ha elaborado varios planes para disminuir la importancia económica del estrecho. El gobierno tailandés, a lo largo de su historia, ha propuesto varias veces cortar un canal a través del istmo de Kra, con un ahorro de aproximadamente 960 km de viaje desde el océano Índico hasta el Pacífico. Esto también cortaría Tailandia en dos, aislando las provincias de mayoría musulmana del sur, Narathiwat, Yala y en especial la separatista Pattani. China se ha ofrecido a sufragar los gastos, según un informe filtrado al diario estadounidense "The Washington Times" en 2004. Sin embargo, y pese al apoyo de varios políticos de Tailandia, los prohibitivos costos financieros y ecológicos sugieren que el canal no seguirá adelante.

Una segunda alternativa es la construcción de un oleoducto a través del istmo para transportar petróleo a los buques que esperarían al otro lado. Los defensores dicen que reduciría el costo de la entrega de petróleo a Asia en cerca de 0,50 $/barril (3 $/m³). Myanmar también ha hecho una propuesta similar de oleoductos. Existe también una propuesta para un oleoducto de petróleo crudo desde Oriente Medio hasta Xinjiang (China), cuya construcción comenzó en octubre de 2004.

Los primeros comerciantes que procedían de Egipto, Roma, Arabia, África, Turquía, Persia e India, utilizaban el estrecho para llegar al estado malayo de Kedah, antes de llegar a Cantón. Kedah sirvió como puerto occidental de la península malaya. Estos comerciantes navegaban a Kedah, entre junio y noviembre, gracias a los vientos del monzón. Regresaban entre diciembre y mayo. Kedah proporcionaba alojamiento, entibadores, pequeñas embarcaciones, balsas de bambú, elefantes, y también facilitaba la recaudación de tasas por los bienes que eran transportados por tierra hacia los estados orientales de la península Malaya, como Kelantan. Los buques llegaban desde China para comerciar a estos puestos y puertos orientales. Kedah y Funan eran ya puertos famosos en el siglo VI, antes de que las expediciones comerciales comenzaran a utilizar el estrecho de Malaca como ruta marítima.

La máxima autoridad internacional en materia de delimitación de mares a efectos de navegación marítima, la IHO (International Hydrographic Organization: Organización Hidrográfica Internacional), considera el estrecho de Malaca como uno de sus mares, que forma parte del conjunto «estrechos de Malaca y Singapur». En su publicación de referencia mundial, «Limits of oceans and seas» (Límites de océanos y mares, 3.ª edición de 1953), le asigna el número de identificación 46a y lo define de la siguiente manera:



</doc>
<doc id="8279" url="https://es.wikipedia.org/wiki?curid=8279" title="Paul Ehrlich">
Paul Ehrlich

Paul Ehrlich (Strehlen, Silesia; hoy Strzelin, Polonia; 14 de marzo de 1854 – Hamburgo, Imperio alemán; 20 de agosto de 1915) fue un eminente médico y bacteriólogo alemán, ganador del premio Nobel de Medicina en 1908.

Su laboratorio descubrió la arsfenamina (conocida entonces con el nombre de "Salvarsan"), el primer tratamiento medicinal eficaz contra la sífilis, iniciando y dando nombre al concepto de quimioterapia. Ehrlich popularizó en medicina el concepto de "bala mágica", como un producto específico capaz de eliminar por completo un determinado organismo patógeno sin efectos secundarios relevantes.

También hizo una contribución decisiva en el campo de la inmunología con el desarrollo de un suero para combatir la difteria y concibió un método terapéutico para la normalización de estos sueros.

Nacido el 14 de marzo de 1854 en Strehlen (Silesia, en lo que hoy es el suroeste de Polonia), Paul Ehrlich fue el segundo hijo de Rosa (Weigert) y de Ismar Ehrlich. Su padre era un posadero y destilador de licores y el colector de lotería real en Strehelen, un pueblo de unos 5.000 habitantes en la provincia de Baja Silesia, ahora en Polonia. Su abuelo, Heymann Ehrlich, había sido un exitoso destilador y tabernero. Ismar Ehrlich era uno de los dirigentes de la comunidad judía local.

Después de la escuela primaria, Paul asistió a la escuela secundaria "Maria Magdalene Gymnasium" de larga tradición en Breslavia (Breslau, actual Wrocław), donde conoció a Albert Neisser, que más tarde se convirtió en un colega profesional. Siendo todavía un escolar (inspirado por su primo Karl Weigert, que era dueño de uno de los primeros microtomos), quedó fascinado por el proceso de tinción de tejidos mediante sustancias colorantes para su observación microscópica. Conservó ese interés durante sus estudios médicos posteriores en la universidad.

Estudió en la Universidad de Breslavia y más tarde en las de Estrasburgo, Friburgo de Brisgovia y Leipzig, donde acabó sus estudios, doctorándose en 1878 con una tesis sobre la teoría y práctica de la tinción histológica. A él se debe la demostración de la existencia de la barrera hematoencefálica al tintar con anilina la sangre de un ratón y demostrar que esta sustancia no tintaba el cerebro.

Después de obtener un nuevo doctorado en 1882, comenzó a trabajar de ayudante en la clínica de la Universidad de Berlín a las órdenes de Theodor Frerichs, (el fundador de la medicina clínica experimental), centrándose en histología, hematología y tinciones. Fue nombrado profesor auxiliar de la misma en 1889 y al año siguiente catedrático de medicina interna. Fue director del Hospital de la Caridad, en Berlín, donde impulsó el campo de la hematología, desarrollando métodos para la detección y diferenciación de diversas enfermedades de la sangre.

Se casó con Hedwig Pinkus (entonces de 19 años de edad) en 1883. La pareja tuvo dos hijas, Stephanie y Marianne.
Después de completar su formación clínica y habilitación en la prominente escuela de medicina y hospital de enseñanza Charité de Berlín en 1886, Ehrlich viajó a Egipto y otros países en 1888 y 1889, en parte para curar un caso de tuberculosis que había contraído en el laboratorio. A su regreso estableció un consultorio médico privado y un pequeño laboratorio en Berlín-Steglitz.

En 1891, Robert Koch invitó a Ehrlich para que se integrara al personal de su Instituto de Enfermedades Infecciosas en Berlín, del que se derivó en 1896 el Instituto para la Investigación y Ensayo Serológicos ("Institut für Serumforschung und Serumprüfung"), establecido para la especialidad de Ehrlich, quien fue su primer director.
En 1896 fue nombrado director del Real Instituto Prusiano de Investigaciones y Ensayos de Sueros, donde desarrolló diversos métodos de tinción de los tejidos con anilina para estudiar las reacciones microquímicas de las toxinas. Una de sus mayores innovaciones consistió en el uso de diferentes tintes (azules de metileno y de indofenol) como tintes selectivos para diferentes tipos de células. En este sentido, fue el primero en investigar las vías del sistema nervioso, inyectando azul de metileno en las venas de conejos vivos, obteniendo extraordinarios resultados experimentales al tratar con un derivado azoico a animales que sufrían la enfermedad del sueño. En 1904 curó un ratón infectado de tripanosomiasis, inyectándole en la corriente sanguínea el colorante hoy conocido como rojo de trípano.

Inmunología: Su principal contribución a la medicina fue la teoría de la inmunidad de cadena lateral, que establecía la base química para la especificidad de la respuesta inmunológica y que explica cómo los receptores de la parte externa de las células se combinan con toxinas para producir cuerpos inmunes capaces de combatir la enfermedad. Su teoría era que las células tienen en su superficie moléculas receptoras específicas (cadenas laterales) que sólo se unen a determinados grupos químicos de las moléculas de toxina; si las células sobreviven a esta unión, se produce un excedente de cadenas laterales, algunas de las cuales son liberadas a la sangre en forma de antitoxinas circulantes (lo que hoy se denomina anticuerpos). 

Quimioterapia: También hizo importantes aportaciones en el campo de la quimioterapia, que incluyen el descubrimiento -en 1901- del 606 (por ser fruto de 606 experimentos), la que él mismo llamó "bala mágica" o salvarsán (arsfenamina), una preparación de arsénico orgánico empleada en el tratamiento de la sífilis y de la fiebre recurrente, y del neosalvarsán (neoarsfenamina). El neosalvarsán fue conocido durante mucho tiempo como «Ehrlich 914» por tratarse del 914º compuesto preparado por Ehrlich y su ayudante para combatir esas enfermedades.

En 1899 su instituto se trasladó a Fráncfort y se renombró como Instituto de Terapia Experimental ("Institut für experimentelle Therapie "), con Max Neisser como uno de sus colaboradores más importantes. En 1906 Ehrlich se convirtió en el director de la Casa Georg Speyer en Frankfurt, una fundación de investigación privada afiliada a su instituto. Aquí descubrió en 1909 el primer fármaco dirigido contra un patógeno específico: Salvarsan, un tratamiento para la sífilis, que era en ese momento una de las enfermedades infecciosas más letales en Europa. Entre los científicos extranjeros invitados que trabajaron con Ehrlich había dos ganadores del Premio Nobel, Henry Hallett Dale y Paul Karrer. El instituto fue renombrado Instituto Paul Ehrlich en honor de Ehrlich en 1947.

En 1908 compartió el Premio Nobel de Fisiología y Medicina con el bacteriólogo ruso Ilya Mechnikov en reconocimiento al trabajo de ambos en el terreno de la química inmunológica.

En 1914 Ehrlich firmó el controvertido Manifiesto de los Noventa y Tres, que era una defensa de la política militarista de Alemania en la Primera Guerra Mundial. El 17 de agosto 1915 Ehrlich sufrió un ataque al corazón y murió el 20 de agosto en Bad Homburg. El emperador alemán Guillermo II escribió en un telegrama de pésame, "Yo, junto con todo el mundo civilizado, lloro la muerte de este investigador meritorio por su gran servicio a la ciencia médica y la humanidad sufriente; el trabajo de su vida le asegura la fama inmortal y el agradecimiento tanto de sus contemporáneos como de las generaciones de la posteridad."

Paul Ehrlich fue enterrado en el Antiguo cementerio judío, Frankfurt (Bloque 114 N)





</doc>
<doc id="8282" url="https://es.wikipedia.org/wiki?curid=8282" title="Nota (sonido)">
Nota (sonido)

En música, una nota es un sonido determinado por una vibración cuya frecuencia fundamental es constante. Así pues, por ejemplo, el término «nota musical» se emplea para hacer alusión a un sonido con una determinada frecuencia en sí; mientras que para aludir al signo que se utiliza en la notación musical para representar la altura y la duración relativa de un sonido, se suele emplear la acepción «figura musical».

Tras varias reformas y modificaciones, las notas pasaron a ser las que se conocen actualmente:


El ejemplo anterior muestra una escala de "do" mayor. Actualmente la escala musical diatónica (sin alteraciones ni cambios en la tonalidad) está compuesta por siete sonidos.
En el caso de la mencionada escala mayor de "do", las notas son las siguientes: 

Los intervalos musicales correspondientes a cada una de las siete notas diatónicas son:

La convención de nomenclatura de nota especifica un monosílabo o bien una letra, cualquier alteraciones y un número de octava. Cualquier nota está a una distancia de un número entero de semitonos del "la" central. ("la") Esta distancia se denota "n". Si la nota está por encima de "la", entonces "n" es positivo, y si está por debajo de "la", entonces "n" es negativo. En el temperamento igual la frecuencia de la nota ("la") es:

Por ejemplo, se puede encontrar la frecuencia de "do", el primer "do" por encima de "la". Hay tres semitonos entre "la" y "do" ("la" → "la" → si → do), y la nota está por encima de "la", por lo que "n" = 3. La frecuencia de la nota será:

Para encontrar la frecuencia de una nota que está por debajo de "la", el valor de "n" es negativo. Por ejemplo, el "fa" por debajo de "la" es "fa". Hay cuatro semitonos ("la" → "la" → "sol" → "sol" → "fa"), y la nota está por debajo de "la", por lo que "n" = -4. La frecuencia de la nota será:

Finalmente puede observarse a partir de esta fórmula que las octavas automáticamente producen potencias de dos veces la frecuencia original, ya que "n" es un múltiplo de 12 (12 "k", donde "k" es el número de octavas hacia arriba o hacia abajo), y por lo que la fórmula se reduce a:

produciendo un factor de 2. De hecho, este es el medio por el que se obtiene esta fórmula, combinado con la noción de intervalos igualmente espaciados.

La distancia de un semitono en el temperamento igual se divide en 100 cents. Así 1200 cents equivalen a una octava, una relación de frecuencias de 2:1. Esto implica que un cent es precisamente igual a la raíz 1200.ª de 2, que es aproximadamente 1,000578.

Para el uso con el estándar MIDI (Musical Instrument Digital Interface), una asignación de frecuencias se define como:

Donde "p" es el número de nota MIDI. Y en sentido contrario, para obtener la frecuencia a partir de una nota MIDI "p", la fórmula se define como:

Para las notas en temperamento igual del "la"440, esta fórmula proporciona el número de nota MIDI estándar ("p"). Cualquier otra frecuencia llena el espacio entre los números enteros de manera uniforme. Esto permite que los instrumentos MIDI sean afinados con gran precisión en cualquier escala microtonal, incluidas las afinaciones tradicionales no-occidentales.

Además de los sonidos representados por estos siete monosílabos o notas, existen otros cinco sonidos que se obtienen subiendo o bajando uno o más semitonos. Para subir o bajar los sonidos se usan alteraciones como el bemol, el sostenido, el doble bemol, el doble sostenido y el becuadro. El bemol () baja un semitono la nota a la que acompaña, mientras que el sostenido () la sube un semitono. Para nominarlos, se usan las siete notas acompañadas o no, según corresponda, del nombre de la alteración. De esta forma, cada uno de los doce sonidos posee tres nomenclaturas, a excepción uno. Así, "do", "re" y "si" son el mismo sonido. A este fenómeno se le denomina enarmonía.
En el actual sistema de afinación (el temperamento igual), no hay diferencia entre las notas enarmónicas: por ejemplo, "do" sostenido suena exactamente igual que "re" bemol.
En los variados sistemas de afinación antiguos entre ambas notas había una diferencia audible que se denomina la coma.





</doc>
<doc id="8285" url="https://es.wikipedia.org/wiki?curid=8285" title="Negra">
Negra

La negra es una figura musical que equivale a ¼ del valor de la figura redonda. El antepasado de la negra es la semiminima de la notación mensural.

Las figuras de negras se representan con una cabeza de nota ovalada coloreada en negro (de ahí su nombre) y con una plica vertical sin adornos (como la blanca). La dirección de la plica depende de la posición de la nota. Al igual que sucede con todas las figuras que llevan plicas, las negras se dibujan con la plica a la derecha de la cabeza de la nota y hacia arriba, cuando el sonido representado está por debajo de la tercera línea del pentagrama. Mientras que, cuando la nota está en dicha línea media o por encima de esta, se dibujan con la plica a la izquierda de la cabeza de la nota y hacia abajo. No obstante, esta regla no es absoluta ya que puede variar cuando es necesario ligar varias notas o cuando se representa más de una voz (ver Figura 1). De hecho en las obras polifónicas la orientación de las plicas ayuda a distinguir las diferentes voces.

El silencio de negra es su silencio equivalente. La negra, como todas las figuras musicales, tiene un silencio de su mismo valor y supone que durante ese tiempo no se emite sonido alguno. Suele representarse mediante el símbolo aunque en ocasiones podemos encontrarnos con el símbolo antiguo . Ejemplos del antiguo símbolo se encuentran en música inglesa hasta principios del siglo XX. Se encuentran muestras de la forma más antigua en obras de los editores de música ingleses de principios del siglo XX, por ejemplo en la partitura vocal "Misa de Requiem" de Wolfgang Amadeus Mozart, editada por William Thomas Best y publicada en Londres: Novello, 1879.

En Unicode el símbolo de negra es U+2669 (♩).

En un compás de subdivisión binaria (2/4; 3/4; 4/4; etc.) la negra equivale a un tiempo. Por lo tanto, en un compás de 4/4 esta figura ocupa la cuarta parte de un compás. 
Si se le añade un puntillo, la duración total resultante es su valor habitual más la mitad de tal valor. Así por ejemplo si su duración son 2 corcheas, con el puntillo pasaría a durar 3 (2 + 1). 

La figura de negra equivale a la cuarta parte de una redonda, a la mitad de una blanca, a 2 corcheas, 4 semicorcheas, 8 fusas o 16 semifusas. 
Por encima de la redonda hay algunas figuras de mayor duración pero han caído en desuso en la notación musical actual. Son: la cuadrada que equivale a ocho negras, la "longa" que equivale a 16 negras y la "maxima" que equivale a 32 negras. Por debajo de la semifusa también existen otras figuras de menor duración que tampoco se utilizan hoy en día. Son: la garrapatea que equivale a 1/128 de la redonda y la semigarrapatea que equivale a 1/256 de la redonda, esto es, 1/64 pulsos de negra.

Los nombres que se le dan a esta figura y a su silencio en diferentes lenguas varían enormemente:

Los nombres en catalán, español y francés de la nota significan «negro», acepción que deriva del hecho de que la "semiminima" era la nota más larga que se rellenaba coloreando con negro en la notación mensural blanca, lo cual se ha mantenido en la grafía actual.

El término empleado en inglés británico, usado en Reino Unido y Canadá, para la negra viene del vocablo del francés antiguo "crochet", que significa «pequeño gancho», diminutivo de «gancho», debido al gancho que se añadía a la plica para representar en la nota en la notación negra. Sin embargo, el término en francés moderno "croche" se refiere a la corchea debido a que el gancho apareció en la corchea en la notación blanca. 

Por su parte, en Estados Unidos escogen la acepción "quarter note" que significa «cuarto de nota» en relación con el valor de la redonda, llamada «nota completa» en esta nomenclatura. Los términos americanos son calcos semánticos de los términos alemanes, ya que cuando las orquestas estadounidenses se establecieron por primera vez en el siglo XIX fueron pobladas en gran medida por emigrantes alemanes.

Las denominaciones griega y china también aluden al concepto de «cuarto de nota» para la figura y «cuarto de pausa» para el silencio. La expresión pausa deriva del griego, en que todos los silencios musicales se llaman «pausas». 




</doc>
<doc id="8286" url="https://es.wikipedia.org/wiki?curid=8286" title="Redonda (figura)">
Redonda (figura)

Una redonda es una figura musical que posee una duración de cuatro pulsos de negra en la notación musical actual. El origen histórico de la redonda es la semibrevis o semibreve de la notación mensural.

Las figuras de redondas se representan con una cabeza de nota ovalada hueca (como la blanca), pero sin barra vertical o plica (ver Figura 1).

El silencio de redonda es su silencio equivalente. La redonda, como todas las figuras musicales, tiene un silencio de su mismo valor y supone que durante ese tiempo no se emite sonido alguno. Este signo se representa mediante un guion o barra horizontal que se encuentra por debajo de la cuarta línea del pentagrama.

En un compás de subdivisión binaria (2/4; 3/4; 4/4; etc.) la redonda equivale a cuatro tiempos. Por lo tanto, en un compás de 4/4 esta figura dura un compás completo. Las referencias de los tiempos en música se suelen determinar con respecto a la duración de la negra. Así pues, si en una composición la negra dura un segundo (60 negras por minuto), entonces la duración de la redonda es de 4 segundos.

La figura de redonda equivale a 2 blancas, 4 negras, 8 corcheas, 16 semicorcheas, 32 fusas o 64 semifusas. 
Por encima de la redonda hay algunas figuras de mayor duración pero han caído en desuso en la notación musical actual. Son: la cuadrada que equivale a ocho negras, la "longa" que equivale a 16 negras y la "maxima" que equivale a 32 negras. Por debajo de la semifusa también existen otras figuras de menor duración que tampoco se utilizan hoy en día. Son: la garrapatea que equivale a 1/128 de la redonda y la semigarrapatea que equivale a 1/256 de la redonda, esto es, 1/64 pulsos de negra.

En Unicode el símbolo de semifusa es U+1D15D.

En los preludios no medidos de la tradición francesa del siglo XVII, todos los sonidos eran representados mediante redondas independientemente de su duración. Será el intérprete quien determine esas duraciones dependiendo del carácter de la pieza, lo que determinó que esta fuera una música improvisada.

Esta figura suele ser empleada en orquestación para pedales armónicos.

La redonda y el silencio de redonda también pueden ser utilizados en músicas de ritmo libre, tales como el canto anglicano, para ser aplicadas a lo largo de un compás completo, con independencia de la duración de dicho compás. El silencio de redonda puede ser empleado de esta forma en casi todas o todas las formas de música.

Los nombres que se le dan a esta figura y a su silencio en diferentes lenguas varían enormemente:

Los nombres en francés y en español para esta figura musical (ambos significan «redondo») derivan del hecho de que la "semibreve" se distinguía por su forma redonda y sin plicas, lo cual sigue manteniéndose en la forma moderna (en contraste con la cuadrada o valores más cortos que llevan plicas). El nombre griego significa «entera».

La redonda deriva de la "semibreve" de la notación mensural y este es el origen del nombre que se utiliza en Reino Unido y Canadá "semibreve". En Estados Unidos se optó por el término "whole note", que es un calco semántico del alemán "ganze Note". Esto se debe a que cuando las orquestas estadounidenses se establecieron por primera vez en el siglo XIX fueron pobladas en gran medida por emigrantes alemanes. Michael Miller escribió, 




</doc>
<doc id="8292" url="https://es.wikipedia.org/wiki?curid=8292" title="Economía informal">
Economía informal

Se denomina economía informal o economía irregular a la actividad económica oculta sólo por razones de elusión fiscal o de controles administrativos (por ejemplo, el trabajo doméstico no declarado, la venta ambulante espontánea o la infravaloración del precio escriturado en una compraventa inmobiliaria).

La economía informal o irregular forma parte de la economía sumergida al lado de las actividades económicas ilegales (por ejemplo, la facturación falsa o falseada, el tráfico de drogas, el tráfico de armas, la prostitución, el blanqueo de capitales, el crimen organizado y el terrorismo).

La economía sumergida, en la medida en que se transforma en Renta Nacional y acaba integrada en la Demanda agregada, resulta incluida en el dato estadístico del Producto Interior Bruto (PIB), como consecuencia del ajuste de los métodos empleados para estimarlo.

Aunque la economía informal se ha asociado frecuentemente a países en desarrollo y economías emergentes, todos los sistemas económicos, sin excepción, participan de ella.

El daño económico que causa la economía informal al fisco no solamente se cifra en el lucro cesante tributario; se extiende al daño emergente consistente en el disfrute indebido de subvenciones, subsidios, pensiones y demás rúbricas del presupuesto de gastos públicos.

Asimismo, la economía informal constituye uno de los supuestos más dañinos de competencia desleal entre los agentes económicos.

La economía sumergida en España representaría más del 23% del PIB. 

En España es el equivalente a 240.000 millones de euros. 

Existen numerosas acepciones en el habla popular para designar esta economía: "pagar bajo cuerda"; "pagar bajo la mesa"; "fuera de los libros" (de contabilidad); "pagar en B" (la tesorería dispondría de una Caja A, que sí formaría parte del Balance, y una Caja B extracontable); "pagar en negro"; "sobresueldo" (cuando la nómina se paga en dos partes, siendo una la que cuenta para el balance, y la otra entregada en mano (generalmente dentro de un sobre) y extracontable).

Una de las principales causas:

La migración y los costos (actualmente los costos son considerablemente elevados, aproximadamente 6000 dólares para un terreno)Cada vez que hay una crisis en el Perú, la economía y los comercios informales aumentan notoriamente ya que no se pagan impuestos y el 100% de la ganancia es para el vendedor. En cambio en los negocios formales todo se apega a la ley por lo cual es necesario aportar con los impuestos debidos y las normativas requeridas

La economía informal bajo cualquier sistema de gobierno es diversa e incluye miembros ocasionales de pequeña escala (a menudo vendedores ambulantes y recicladores de basura) así como empresas más grandes y regulares (incluidos sistemas de tránsito como el de Lima, Perú). Las economías informales incluyen a los trabajadores de la confección que trabajan desde sus hogares, así como al personal empleado informalmente en las empresas formales. Los empleados que trabajan en el sector informal se pueden clasificar como trabajadores asalariados, trabajadores no asalariados o una combinación de ambos.

Las estadísticas sobre la economía informal no son confiables en virtud del tema, pero pueden proporcionar una imagen tentativa de su relevancia. Por ejemplo, el empleo informal representa el 58.7% del empleo no agrícola en Medio Oriente - Norte de África -, 64.6% en América Latina, 79.4% en Asia y 80.4% en África subsahariana. Si se incluye el empleo agrícola, los porcentajes aumentan más allá del 90% en algunos países como India y muchos países del África subsahariana. Las estimaciones para los países desarrollados son de alrededor del 15%. En encuestas recientes, la economía informal en muchas regiones ha disminuido en los últimos 20 años hasta 2014. En África, la participación de la economía informal ha disminuido a una estimación de alrededor del 40% de la economía.

En los países en desarrollo, la mayor parte del trabajo informal, alrededor del 70%, es por cuenta propia. El empleo asalariado predomina. La mayoría de los trabajadores de la economía informal son mujeres. Las políticas y los desarrollos que afectan a la economía informal tienen, por lo tanto, un efecto distintivo de género.

Uno de los beneficios de tener un negocio informal es el no pago de impuestos o de seguro social, por ende se obtiene mayor ganancia neta, sin embargo no paga impuestos, por ende no aporta ingresos al estado.


[[Categoría:Economía informal| ]
ECONOMÍA FORMAL
La economía formal es la actividad que está debidamente registrada ante las autoridades fiscales, es la actividad que reporta sus movimientos económicos por medio del pago de impuestos ante el Sistema de Administración Tributaria, para que este se encuentre enterado de los ingresos, gastos, costos, proveedores y clientes de una empresa grande, mediana, pequeña o micro, pero también de una persona física con actividad empresarial.

En resumen: son las actividades que están dentro del marco institucional que regula las actividades económicas de un país, en el cual deberían estar incluidos los trabajadores por cuenta propia y las pequeñas empresas. Cabe decir que en el sector formal de la economía se cumplen más las leyes laborales así como...

</doc>
<doc id="8293" url="https://es.wikipedia.org/wiki?curid=8293" title="Teoría de grafos">
Teoría de grafos

La teoría de grafos, también llamada teoría de gráficas, es una rama de las matemáticas y las ciencias de la computación que estudia las propiedades de los grafos, y que no deben ser confundidos con las gráficas que tienen una acepción muy amplia. Formalmente, "un grafo" formula_1 es una pareja ordenada en la que formula_2 es un conjunto no vacío de vértices y formula_3 es un conjunto de aristas. Donde formula_2 consta de pares no ordenados de vértices, tales como {formula_5}formula_6 entonces decimos que formula_7 e formula_8 son adyacentes; y [en el grafo] se representa mediante una línea no orientada que una dichos vértices. Si el grafo es dirigido se le llama "digrafo", se denota formula_9, y entonces el par formula_10 es un par ordenado, y se representa con una flecha que va de formula_7 a formula_8, y decimos que formula_13. 

La teoría de grafos tiene sus fundamentos en las matemáticas discretas y de las matemáticas aplicadas. Esta teoría que requiere de diferentes conceptos de diversas áreas como combinatoria, álgebra, probabilidad, geometría de polígonos, aritmética y topología. Actualmente ha tenido mayor influencia en el campo de la informática, las ciencias de la computación y telecomunicaciones. Debido a la gran cantidad de aplicaciones en la optimización de recorridos, procesos, flujos, algoritmos de búsquedas, entre otros, se generó toda una nueva teoría que se conoce como análisis de redes.

El origen de la teoría de grafos se remonta al siglo XVIII con el problema de los puentes de Königsberg, el cual consistía en encontrar un camino que recorriera los siete puentes del río Pregel () en la ciudad de Königsberg, actualmente Kaliningrado, de modo que se recorrieran todos los puentes pasando una sola vez por cada uno de ellos. El trabajo de Leonhard Euler sobre el problema titulado "Solutio problematis ad geometriam situs pertinentis" ("La solución de un problema relativo a la geometría de la posición") en 1736, es considerado el primer resultado de la teoría de grafos. También se considera uno de los primeros resultados topológicos en geometría (que no depende de ninguna medida). Este ejemplo ilustra la profunda relación entre la teoría de grafos y la topología.

Luego, en 1847, Gustav Kirchhoff utilizó la teoría de grafos para el análisis de redes eléctricas publicando sus leyes de los circuitos para calcular el voltaje y la corriente en los circuitos eléctricos, conocidas como leyes de Kirchhoff, considerado la primera aplicación de la teoría de grafos a un problema de ingeniería.

En 1852 Francis Guthrie planteó el problema de los cuatro colores el cual afirma que es posible, utilizando solamente cuatro colores, colorear cualquier mapa de países de tal forma que dos países vecinos nunca tengan el mismo color. Este problema, que no fue resuelto hasta un siglo después por Kenneth Appel y Wolfgang Haken en 1976, puede ser considerado como el nacimiento de la teoría de grafos. Al tratar de resolverlo, los matemáticos definieron términos y conceptos teóricos fundamentales de los grafos.

En 1857, Arthur Cayley estudió y resolvió el problema de enumeración de los isómeros, compuestos químicos con idéntica composición (fórmula) pero diferente estructura molecular. Para ello representó cada compuesto, en este caso hidrocarburos saturados CH, mediante un grafo árbol donde los vértices representan átomos y las aristas la existencia de enlaces químicos.

El término "«grafo»", proviene de la expresión "graphic notation" («notación gráfica») usada por primera vez por Edward Frankland y posteriormente adoptada por Alexander Crum Brown en 1884, y hacía referencia a la representación gráfica de los enlaces entre los átomos de una molécula.

El primer libro sobre teoría de grafos fue escrito por Dénes Kőnig y publicado en 1936.





Existen diferentes formas de representar un grafo (simple), además de la geométrica y muchos métodos para almacenarlos en una computadora. La estructura de datos usada depende de las características del grafo y el algoritmo usado para manipularlo. Entre las estructuras más sencillas y usadas se encuentran las listas y las matrices, aunque frecuentemente se usa una combinación de ambas. Las listas son preferidas en grafos dispersos porque tienen un eficiente uso de la memoria. Por otro lado, las matrices proveen acceso rápido, pero pueden consumir grandes cantidades de memoria.





Un problema común, denominado problema de isomorfismo de subgrafos, es encontrar un grafo fijo como subgrafo de un grafo dado. Una razón para estar interesado en esta cuestión es que muchas propiedades de grafos son heredadas de subgrafos, lo que significa que un grafo tiene una propiedad si y solo si todos sus subgrafos a su vez la poseen. Desafortunadamente, encontrar subgrafos máximos de un cierto tipo suele ser un problema NP-completo. Por ejemplo:

Un problema similar es encontrar subgrafo inducido en un grafo dado. De nuevo, algunas propiedades importantes son heredadas con respecto a subgrafos inducidos, lo que significa que un grafo tiene una propiedad si y solo si todos los subgrafos inducidos la tienen. Encontrar subgrafos inducidos máximos de un determinado tipo es, de nuevo, un problema NP-completo. Como ejemplo:

Otro nuevo problema es el problema del menor contenido, que es encontrar un grafo fijo como menor de un grafo dado. Un menor o subcontración de un grafo es cualquier grafo obtenido tomando un subgrafo y contrayendo algunos bordes. Muchas propiedades de grafos son heredadas de menores, lo que significa que un grafo la tiene sólo si todos sus menores la tienen también. Por ejemplo, el teorema de Wagner estipula que:

Un problema de las mismas características es el problema de la subdivisión del contenido. Una subdivisión o homeomorfismo de un grafo es cualquier grafo obtenido subdividiendo algunos bordes. La subdivisión del contenido está relacionada con las propiedades de los grafos tales como la "planeza". Por ejemplo, el teorema de Kuratowski establece que:
Otro problema en la subdivisión de contenido es la conjetura de Kelmans-Seymour:
Otro problemas de clases tienen que ver con el alcance para la cual varias especies y generalizaciones de grafos están determinadas por sus subgrafos de puntos eliminados. Por ejemplo, la conjetura de la reconstrucción.

Un ciclo es una sucesión de aristas adyacentes, donde no se recorre dos veces la misma arista, y donde se regresa al punto inicial. Un ciclo hamiltoniano tiene además que recorrer todos los vértices exactamente una vez (excepto el vértice del que parte y al cual llega).

Por ejemplo, en un museo grande (al estilo del Louvre), lo idóneo sería recorrer todas las salas una sola vez, esto es buscar un ciclo hamiltoniano en el grafo que representa el museo (los vértices son las salas, y las aristas los corredores o puertas entre ellas).

Se habla también de Camino hamiltoniano si no se impone regresar al punto de partida, como en un museo con una única puerta de entrada. Por ejemplo, un caballo puede recorrer todas las casillas de un tablero de ajedrez sin pasar dos veces por la misma: es un camino hamiltoniano. Ejemplo de un ciclo hamiltoniano en el grafo del dodecaedro.

Hoy en día, no se conocen métodos generales para hallar un ciclo hamiltoniano en tiempo polinómico, siendo la búsqueda por fuerza bruta de todos los posibles caminos u otros métodos excesivamente costosos. Existen, sin embargo, métodos para descartar la existencia de ciclos o caminos hamiltonianos en grafos pequeños.

El problema de determinar la existencia de ciclos hamiltonianos, entra en el conjunto de los NP-completos.

Cuando un grafo o multigrafo se puede dibujar en un plano sin que dos segmentos se corten, se dice que es plano.

Un juego muy conocido es el siguiente: Se dibujan tres casas y tres pozos. Todos los vecinos de las casas tienen el derecho de utilizar los tres pozos. Como no se llevan bien en absoluto, no quieren cruzarse jamás. ¿Es posible trazar los nueve caminos que juntan las tres casas con los tres pozos sin que haya cruces?

Cualquier disposición de las casas, los pozos y los caminos implica la presencia de al menos un cruce.

Sea K el grafo completo con "n" vértices, K es el grafo bipartito de "n" y "p" vértices.

El juego anterior equivale a descubrir si el grafo bipartito completo K es plano, es decir, si se puede dibujar en un plano sin que haya cruces, siendo la respuesta que no. En general, puede determinarse que un grafo "no" es plano, si en su diseño puede encontrase una estructura análoga (conocida como "menor") a K o a K.

Establecer qué grafos son planos no es obvio, y es un problema que tiene que ver con topología.

Si G=(V, E) es un grafo no dirigido, una coloración propia de G, ocurre cuando coloreamos los vértices de G de modo que si {a, b} es una arista en G entonces a y b tienen diferentes colores. (Por lo tanto, los vértices adyacentes tienen colores diferentes). El número mínimo de colores necesarios para una coloración propia de G es el número cromático de G y se escribe como C (G).
Sea G un grafo no dirigido sea λ el número de colores disponibles para la coloración propia de los vértices de G. Nuestro objetivo es encontrar una función polinomial P (G,λ), en la variable λ, llamada polinomio cromático de G, que nos indique el número de coloraciones propias diferentes de los vértices de G, usando un máximo de λ colores.

Descomposición de polinomios cromáticos. Si G=(V, E) es un grafo conexo y e pertenece a Ε, entonces: P (G,λ)=P (G+e,λ)+P (G/e,λ), donde G/e es el grafo se obtiene por contracción de aristas.

Para cualquier grafo G, el término constante en P (G,λ) es 0

Sea G=(V, E) con |E|>0 entonces, la suma de los coeficientes de P (G,λ) es 0.

Sea G=(V, E), con "a", "b" pertenecientes al conjunto de vértices "V" pero {a, b}=e, no perteneciente a al conjunto de aristas E. Escribimos G+e para el grafo que se obtiene de G al añadir la arista e={a, b}. Al identificar los vértices a y b en G, obtenemos el subgrafo G++e de G.0000

Otro problema famoso relativo a los grafos: ¿Cuántos colores son necesarios para dibujar un mapa político, con la condición obvia que dos países adyacentes no puedan tener el mismo color? Se supone que los países son de un solo pedazo, y que el mundo es esférico o plano. En un mundo en forma de toroide; el teorema siguiente no es válido:

Cuatro colores son siempre suficientes para colorear un mapa.

El mapa siguiente muestra que tres colores no bastan: Si se empieza por el país central a y se esfuerza uno en utilizar el menor número de colores, entonces en la corona alrededor de a alternan dos colores. Llegando al país h se tiene que introducir un cuarto color. Lo mismo sucede en i si se emplea el mismo método.

La forma precisa de cada país no importa; lo único relevante es saber qué país toca a qué otro. Estos datos están incluidos en el grafo donde los vértices son los países y las aristas conectan los que justamente son adyacentes. Entonces la cuestión equivale a atribuir a cada vértice un color distinto del de sus vecinos.

Hemos visto que tres colores no son suficientes, y demostrar que con cinco siempre se llega, es bastante fácil. Pero el teorema de los cuatro colores no es nada obvio.
Prueba de ello es que se han tenido que emplear ordenadores para acabar la demostración (se ha hecho un programa que permitió verificar una multitud de casos, lo que ahorró muchísimo tiempo a los matemáticos). Fue la primera vez que la comunidad matemática aceptó una demostración asistida por ordenador, lo que creó en su día una cierta polémica dentro de dicha comunidad.

Un grafo es "simple" si a lo sumo existe una arista uniendo dos vértices cualesquiera. Esto es equivalente a decir que una arista cualquiera es la única que une dos vértices específicos.

Un grafo que no es simple se denomina multigrafo.

Un grafo es conexo si cada par de vértices está conectado por un camino; es decir, si para cualquier par de vértices (a, b), existe al menos un camino posible desde "a" hacia "b".

Un grafo es doblemente conexo si cada par de vértices está conectado por al menos dos caminos disjuntos; es decir, es conexo y no existe un vértice tal que al sacarlo el grafo resultante sea disconexo.

Es posible determinar si un grafo es conexo usando un algoritmo Búsqueda en anchura (BFS) o Búsqueda en profundidad (DFS).

En términos matemáticos la propiedad de un grafo (fuertemente) conexo permite establecer una relación de equivalencia para sus vértices, la cual lleva a una partición de estos en "componentes (fuertemente) conexos", es decir, porciones del grafo, que son (fuertemente) conexas cuando se consideran como grafos aislados. Esta propiedad es importante para muchas demostraciones en teoría de grafos.

Un grafo es "completo" si existen aristas uniendo "todos" los pares posibles de vértices. Es decir, todo par de vértices (a, b) debe tener una arista "e" que los une.

El conjunto de los grafos completos es denominado usualmente formula_17, siendo formula_18 el grafo completo de "n" vértices.

Un formula_18, es decir, grafo completo de formula_15 vértices tiene exactamente formula_21 aristas.

La representación gráfica de los formula_18 como los vértices de un polígono regular da cuenta de su peculiar estructura.

Un grafo G es bipartito si puede expresar como formula_23 (es decir, sus vértices son la unión de dos grupos de vértices), bajo las siguientes condiciones:

Bajo estas condiciones, el grafo se considera bipartito, y puede describirse informalmente como el grafo que une o relaciona dos conjuntos de elementos diferentes, como aquellos resultantes de los ejercicios y puzzles en los que debe unirse un elemento de la columna A con un elemento de la columna B.

Dos grafos formula_26 y formula_27 son homeomorfos si ambos pueden obtenerse a partir del mismo grafo con una sucesión de subdivisiones elementales de aristas.

Un grafo que no tiene ciclos y que conecta a todos los puntos, se llama un árbol. En un grafo con n vértices, los árboles tienen exactamente n - 1 aristas, y hay n árboles posibles. Su importancia radica en que los árboles son grafos que conectan todos los vértices utilizando el menor número posible de aristas. Un importante campo de aplicación de su estudio se encuentra en el análisis filogenético, el de la filiación de entidades que derivan unas de otras en un proceso evolutivo, que se aplica sobre todo a la averiguación del parentesco entre especies; aunque se ha usado también, por ejemplo, en el estudio del parentesco entre lenguas.

En muchos casos, es preciso atribuir a cada arista un número específico, llamado "valuación", "ponderación" o "coste" según el contexto, y se obtiene así un grafo valuado.
Formalmente, es un grafo con una función v: A → R.

Por ejemplo, un representante comercial tiene que visitar "n" ciudades conectadas entre sí por carreteras; su interés previsible será minimizar la distancia recorrida (o el tiempo, si se pueden prever atascos). El grafo correspondiente tendrá como vértices las ciudades, como aristas las carreteras y la valuación será la distancia entre ellas.
Y, de momento, no se conocen métodos generales para hallar un ciclo de valuación mínima, pero sí para los caminos desde "a" hasta "b", sin más condición.

En un grafo, la distancia entre dos vértices es el menor número de aristas de un recorrido entre ellos. El diámetro, en una figura como en un grafo, es la mayor distancia de entre todos los pares de puntos de la misma.

El diámetro de los K es 1, y el de los K, es 2. Un diámetro infinito puede significar que el grafo tiene una infinidad de vértices o simplemente que no es conexo. También se puede considerar el diámetro promedio, como el promedio de las distancias entre dos vértices.

Una aplicación de este concepto es la hipótesis conocida como los seis grados de separación, que plantea que, si cada uno de los habitantes de la Tierra se representa por un vértice y dos personas están conectadas por una arista si se conocen personalmente, la distancia entre dos personas escogidas al azar entre todos los habitantes de la Tierra es de seis aristas o menos.

El mundo de Internet ha puesto de moda esa idea del diámetro: Si descartamos los sitios que no tienen enlaces, y escogemos dos páginas "web" al azar: ¿En cuántos "clics" se puede pasar de la primera a la segunda? El resultado es el diámetro de la Red, vista como un grafo cuyos vértices son los sitios, y cuyas aristas son lógicamente los enlaces.

Este concepto refleja mejor la complejidad de una red que el número de sus elementos.

Gracias a la teoría de grafos se pueden resolver diversos problemas como por ejemplo la síntesis de circuitos secuenciales, contadores o sistemas de apertura. Se utiliza para diferentes áreas por ejemplo, Dibujo computacional, en toda las áreas de Ingeniería.

Los grafos se utilizan también para modelar trayectos como el de una línea de autobús a través de las calles de una ciudad, en el que podemos obtener caminos óptimos para el trayecto aplicando diversos algoritmos como puede ser el algoritmo de Floyd.

Para la administración de proyectos, utilizamos técnicas como técnica de revisión y evaluación de programas (PERT) en las que se modelan los mismos utilizando grafos y optimizando los tiempos para concretar los mismos.

Una importante aplicación de la teoría de grafos es en el campo de la informática, ya que ha servido para la resolución de importantes y complejos algoritmos. Un claro ejemplo es el Algoritmo de Dijkstra, utilizado para la determinación del camino más corto en el recorrido de un grafo con determinados pesos en sus vértices.

Dentro de este campo, un grafo es considerado un tipo de dato abstracto TAD.

El científico estadounidense Donald Knuth estableció los grafos planos como base de determinados estudios y descubrimientos realizados por él.

Por otra parte, destaca el Algoritmo de Kruskal, el cual nos permite buscar un subconjunto de aristas que incluye todos los vértices, estableciendo como mínimo el valor de las aristas.

La teoría de grafos también ha servido de inspiración para las ciencias sociales, en especial para desarrollar un concepto no metafórico de red social que sustituye los nodos por los actores sociales y verifica la posición, centralidad e importancia de cada actor dentro de la red. Esta medida permite cuantificar y abstraer relaciones complejas, de manera que la estructura social puede representarse gráficamente. Por ejemplo, una red social puede representar la estructura de poder dentro de una sociedad al identificar los vínculos (aristas), su dirección e intensidad y da idea de la manera en que el poder se transmite y a quiénes.

Se emplea en problemas de control de producción, para proyectar redes de ordenadores, para diseñar módulos electrónicos modernos y proyectar sistemas físicos con parámetros localizados (mecánicos, acústicos y eléctricos).

Se usa para la solución de problemas de genética y problemas de automatización de la proyección (SAPR). Apoyo matemático de los sistemas modernos para el procesamiento de la información. Acude en las investigaciones nucleares (técnica de diagramas de Feynman).

Los grafos son importantes en el estudio de la biología y hábitat. El vértice representa un hábitat y las aristas (o "edges" en inglés) representa los senderos de los animales o las migraciones. Con esta información, los científicos pueden entender cómo esto puede cambiar o afectar a las especies en su hábitat.

Entre las aplicaciones de la Teoría de gráficas que se han vuelto importantes en la actualidad podemos encontrar el estudio de las redes sociales, cuya importancia radica en el adecuado almacenamiento de datos, puesto que el costo del tiempo de búsqueda de la información de cada miembro que pertenece a esta red puede tornarse demasiado alto debido al número de usuarios. Por ejemplo, el número de usuarios que hay actualmente en una importante red social tan solo en México es de 49 millones -cifra reportada por el periódico El economista en 2014-, si este número lo multiplicamos por 194 que es el número aproximado de países que hay en el mundo, se percibe la posibilidad de un grave problema de almacenamiento para los servidores que hay destinados para ello y para la búsqueda de información. Este mismo fenómeno pasa en otras redes de fotografías, mensajes, etc. El modelado de este tipo de problemas ha sido abordado principalmente por estudiantes de doctorado de universidades como Stanford, Massachusetts Institute of Technology (MIT), Berkeley, Oxford, Rice y también por la NASA; en México, tanto el Instituto Politécnico Nacional (IPN) como la Universidad Nacional Autónoma de México (UNAM) son los principales promotores en estas áreas a través de los grupos académicos de combinatoria y de computación científica. Podemos considerar que este tipo de problemas son tratados por expertos en matemáticas y ciencias de la computación, debido a su alto grado de complejidad.

El cerebro humano es una red compleja que interactúa en regiones conectadas por tractos de sustancia blanca. La caracterización de características estructurales y funcionales de una red tal en sujetos sanos y personas enfermas tiene la posibilidad de mejorar nuestra comprensión de la fisiopatología y las manifestaciones neurológicas y condiciones psiquiátricas. Esto ha llevado al uso de nuevas herramientas para el análisis de sistemas complejos para hacer frente enfermedades cerebrales. Entre estos, la teoría de grafos es un marco matemático que permite describir una red en forma de una gráfica, que consiste en una colección de los nodos (es decir, regiones del cerebro) y los bordes (es decir, estructurales y conexiones funcionales) .El uso de la teoría de grafos, distinto modificaciones de la topología de red cerebro han sido identificados durante el desarrollo y el envejecimiento normal, y se rompieron conectividades funcionales y estructurales han sido asociado con varios trastornos neurológicos y psiquiátricos, incluyendo demencia, esclerosis lateral amiotrófica, y la esquizofrenia. En este último enfoque se ha contribuido a probar la teoría de esta condición como un síndrome de desconexión. En la esclerosis múltiple (MS), la ocurrencia de la desconexión ha sido corroborada por estudios de resonancia magnética estructural de topología de la red cerebral que mostró una disminución de la conectividad estructural de las regiones de los lóbulos fronto-temporal.

Otra aplicación de las gráficas consiste en tomar datos de resonancia magnética del cerebro adquiridos en condición ausente ( " estado de reposo ") requieren nuevos análisis de datos técnicas que no dependen de un modelo de activación, una alternativa son los métodos libre de parámetro sobre la base de una forma particular de la centralidad del vector propio asociado a un nodo llamado de centralidad; la centralidad del vector propio asigna atributos de un valor a cada voxel en el cerebro de manera que un voxel recibe un valor grande si está fuertemente correlacionada con muchos otros nodos que son centrales dentro de la red; el algoritmo PageRank de Google es una variante del vector propio centralidad el cual es utilizado en las búsquedas que se efectúan en internet. Hasta el momento, otras medidas de centralidad - en particular " centralidad de intermediación " - se han aplicado a datos de la fMRI usando un conjunto pre-seleccionado de nodos que consisten en varios cientos de elementos. Centralidad del Vector Propio es computacionalmente mucho más eficiente que centralidad de intermediación y no requiere de umbrales de valores de similitud de modo que se puede aplicar a miles de voxels en una región de interés que cubren la totalidad del cerebro que habría sido inviable el uso de centralidad de intermediación. Centralidad del Vector Propio se puede utilizar en una variedad de diferentes medidas de similitud. (Lohmann et al., 2010).
“La teoría de redes complejas juega un papel importante en una amplia variedad de disciplinas,
que van desde la informática, sociología, ingeniería y física, para molecular
y la biología de la población. Dentro de los campos de la biología y la medicina, el potencial de
aplicaciones de análisis de redes incluyen, por ejemplo, la identificación objetivo de drogas, determinando una función del gen de la proteína, o diseñar estrategias eficaces para el tratamiento de diversas enfermedades o proporcionar el diagnóstico precoz de trastornos. “ (Pavlopoulos et al., 2011)
La teoría de gráficas, es adecuada para que los informáticos modelen problemas, pero también es adecuado para los matemáticos que tienen interés en la complejidad computacional. La mayoría de los conceptos clásicos de la teoría de grafos teórica y aplicada (árboles de expansión, conectividad, género, colorabilidad, fluye en las redes, los apareamientos y recorridos). Se usa en la solución de problemas.(Czumaj, Jansen, Meyer auf der Heide, & Schiermeyer, 2006)


Czumaj, A., Jansen, K., Meyer auf der Heide, F., & Schiermeyer, I. (2006). Algorithmic Graph Theory. "Oberwolfach Reports", 379–460.

Hinz, A. M. (2012). Graph theory of tower tasks. In "Behavioural Neurology" (Vol. 25, pp. 13–22).

Lohmann, G., Margulies, D. S., Horstmann, A., Pleger, B., Lepsien, J., Goldhahn, D., … Turner, R. (2010). Eigenvector centrality mapping for analyzing connectivity patterns in fMRI data of the human brain. "PLoS ONE", "5"(4). <nowiki>http://doi.org/10.1371/journal.pone.0010232</nowiki>

Pavlopoulos, G. a, Secrier, M., Moschopoulos, C. N., Soldatos, T. G., Kossida, S., Aerts, J., … Bagos, P. G. (2011). Using graph theory to analyze biological networks. "BioData Mining", "4"(1), 10. Retrieved from <nowiki>http://www.biodatamining.org/content/4/1/10</nowiki>

Rocca, M. A., Valsasina, P., Meani, A., Falini, A., Comi, G., & Filippi, M. (2014). Impaired functional integration in multiple sclerosis: a graph theory study. "Brain Structure and Function", 115–131. <nowiki>http://doi.org/10.1007/s00429-014-0896-4</nowiki>



</doc>
<doc id="8298" url="https://es.wikipedia.org/wiki?curid=8298" title="Misuri (desambiguación)">
Misuri (desambiguación)

El término Misuri (en inglés Missouri) puede referirse a:

</doc>
<doc id="8308" url="https://es.wikipedia.org/wiki?curid=8308" title="El hobbit">
El hobbit

El hobbit (título original en inglés: The Hobbit, or There and Back Again, usualmente abreviado como The Hobbit) es una novela fantástica del filólogo y escritor británico J. R. R. Tolkien. Fue escrita por partes desde finales de los años 1920 hasta principios de los años 1930 y, en un principio, tan sólo tenía el objetivo de divertir a los hijos pequeños de Tolkien. No obstante, el manuscrito de la obra aún sin acabar fue prestado por el escritor a varias personas y finalmente acabó en manos de la editorial George Allen & Unwin. Dispuestos a publicarla, los editores pidieron a Tolkien que finalizara la obra y "El hobbit" fue publicada el 21 de septiembre de 1937 en el Reino Unido.

Es la primera obra que explora el universo mitológico creado por Tolkien y que más tarde se encargarían de definir "El Señor de los Anillos" y "El Silmarillion". Dentro de dicha ficción, el argumento de "El hobbit" se sitúa en el año 2941 de la Tercera Edad del Sol, y narra la historia del hobbit Bilbo Bolsón, que junto con el mago Gandalf y un grupo de enanos, vive una aventura en busca del tesoro custodiado por el dragón Smaug en la Montaña Solitaria.

Debido al éxito que tuvo y a las buenas críticas que recibió, los editores pidieron a Tolkien una continuación. Bautizada como "El Señor de los Anillos", su cambio a un tono alejado del infantil provocó que "El hobbit" tuviera que ser modificado ligeramente para que ambas historias coincidieran mejor. 

Tras la publicación en 1964 de la edición argentina titulada "El hobito", Ediciones Minotauro compró los derechos de las y publicó una traducción mejorada de la novela en España en 1982 y en Argentina en 1984.

La BBC Radio 4 realizó una adaptación radiofónica de "El hobbit" en 1968, mientras que Arthur Rankin Jr. y Jules Bass produjeron una película sobre ella para la televisión. En la actualidad ha finalizado la producción de una trilogía basada en "El Señor de los Anillos" producida por el director Peter Jackson. A pesar de haberse involucrado por más de dos años en el diseño y producción de las adaptaciones al cine, el 31 de mayo de , el mexicano Guillermo del Toro renunció a la dirección de las cintas debido al retraso en el comienzo de la filmación por los problemas financieros que atraviesa la productora Metro-Goldwyn-Mayer. En diciembre de se estrenó la primera parte, "", en se estrenó "", mientras que el estreno de la última película, "", se realizó en diciembre de . También se han creado varios videojuegos basados en la novela.

El reino enano de Erebor, también conocido como la Montaña Solitaria, fue fundado en el año 1999 de la Tercera Edad del Sol, por el rey Thráin I, quien acababa de huir con parte de su pueblo de Khazad-dûm tras la aparición de un balrog. Siete siglos después, el dragón Smaug llegó a Erebor y, tras expulsar a los enanos, se apoderó del tesoro que éstos habían acumulado.

En 2463 T. E. algunos hobbits de la rama de los Fuertes vivían en los Campos Gladios, donde milenios atrás el Isildur de Arnor y Gondor fue asesinado por los orcos y el Anillo Único del Señor Oscuro Sauron se hundió en el río Anduin. Sméagol y su primo Déagol se encontraban pescando en el río cuando este último encontró el Anillo. Su poder despertó la codicia de Sméagol, que asesinó a su primo para arrebatárselo y, al ser desterrado por su pueblo, vagó hasta llegar a las Montañas Nubladas. Allí el poder del Anillo le corrompió, alargando su vida más allá de lo natural y convirtiéndole en una criatura que pasó a ser conocida como Gollum. 

Cien años antes de los hechos narrados en la novela, el por entonces rey de los enanos, Thráin II, decidió regresar a Erebor. No obstante, fue apresado durante el viaje por los siervos de Sauron y le llevaron a la fortaleza de Dol Guldur, donde le arrebataron el último de los siete Anillos de los Enanos. Pocos años después el mago Gandalf entró en Dol Guldur y descubrió que Sauron había recuperado sus fuerzas de nuevo y que estaba reuniendo todos los Anillos de Poder. Encontró también allí a Thráin y éste le dio la llave de Erebor antes de morir. Gandalf se reunió entonces con el Concilio Blanco e intentó convencer a los miembros para que atacaran Dol Guldur, pero Saruman, líder del concilio, se opuso y comenzó a buscar por su cuenta el Anillo Único en los Campos Gladios.

La historia comienza un día en el que el hobbit Bilbo Bolsón, habitante de la Comarca, recibe la inesperada visita del mago Gandalf y de una compañía de trece enanos, liderada por Thorin "Escudo de Roble", y compuesta por Balin, Glóin, Bifur, Bofur, Bombur, Dwalin, Ori, Dori, Nori, Óin, Kíli y Fíli. Los enanos necesitaban un miembro más en el grupo, un saqueador experto, para poder llevar a cabo su plan: alcanzar Erebor, derrotar al dragón Smaug y recuperar el reino y su tesoro. Gandalf les había recomendado para esta misión a Bilbo y de esta forma el hobbit se ve envuelto en la aventura. 

De camino a las Montañas Nubladas, la compañía entró en el Bosque de los Trolls. Allí se encontraron con un grupo de tres trolls de los que se libraron gracias a la astucia de Gandalf. El mago, conociendo el punto débil de estas criaturas, los distrajo hasta el amanecer, momento en el cual se convirtieron en piedra por el efecto de la luz del sol. En la guarida de los trolls, la compañía encontró unas espadas del antiguo reino élfico de Gondolin. Thorin cogió a Orcrist, la espada de Ecthelion, capitán de la ciudad, y Gandalf a Glamdring, la espada que pertenecía al rey Turgon. A Bilbo le dio una daga (aunque él podía usarla como espada por su estatura) a la que llamó Dardo. Poco después, llegaron a Rivendel, la tierra gobernada por el medio elfo Elrond, quien les ayudó a descifrar el mapa del tesoro de Smaug y las inscripciones de runas que tenían las espadas.

Ya en las Montañas Nubladas, una tormenta les obligó a resguardarse y fueron a parar a una caverna llena de trasgos que les persiguieron, haciendo que Bilbo se perdiese. Tras encontrar un misterioso anillo, el hobbit llegó a la orilla de un lago subterráneo, donde vivía una criatura llamada Gollum. Éste le desafió a un juego de acertijos. En el caso de que Gollum ganara, se comería a Bilbo, y si no, guiaría al hobbit hasta la salida. Tras perder, Gollum se negó a cumplir su promesa y fue en busca de su anillo para matar a Bilbo, pero éste había descubierto que la joya le volvía invisible cuando se la ponía y gracias a ello logró escapar y reunirse con sus compañeros. De nuevo un grupo de trasgos y huargos les persiguieron, pero gracias a que las águilas gigantes les rescataron, consiguieron huir. 

Habiendo descendido ya de las montañas, la compañía llegó a la casa de Beorn, un hombre con la capacidad de convertirse en oso. Éste les prestó armas y ponis para que pudieran llegar al Bosque Negro. Una vez allí, Gandalf tuvo que dejarlos por un tiempo. Dos veces fueron apresados los enanos, una por las arañas gigantes y otra por los elfos del bosque, gobernados por el rey Thranduil, pero Bilbo, que escapó haciéndose invisible con el anillo, logró rescatarles. 

La compañía llegó por fin a Esgaroth, la Ciudad del Lago, desde donde se encaminaron hacia Erebor. Una vez allí, Bilbo logró entrar a la guarida de Smaug gracias al anillo y se llevó una copa. El dragón, creyendo que los hombres del Lago eran los ladrones de la copa, se dirigió a la ciudad dispuesto a destruirla, pero Bardo, príncipe de la Ciudad de Valle, logró matarle al clavarle la Flecha Negra en la única parte de su vientre que no estaba cubierta de piedras preciosas.

Fue entonces cuando tanto los hombres de Esgaroth y de Valle como los elfos del Bosque Negro se dieron cuenta que el tesoro que guardaba Smaug se había quedado sin dueño. Al llegar a Erebor descubrieron que los enanos habían colocado defensas en la puerta principal, pues el cuervo Roäc les había informado de sus intenciones y Thorin, quien creía que el tesoro era suyo por derecho, se negó a realizar cualquier tipo de negociación. Mientras los hombres y los elfos asediaban la montaña, los enanos buscaron la Piedra del Arca, el objeto más preciado por Thorin de todo el tesoro, pues no sabían que ésta estaba en poder de Bilbo y que la mantuvo escondida por miedo a las represalias. El hobbit acudió al campamento de los hombres y elfos al anochecer e intentó que Bardo y Thranduil no atacaran Erebor, entregándoles la Piedra del Arca como ayuda para la negociación. 

Al día siguiente se dispusieron a negociar de nuevo con Thorin, mostrándole la Piedra del Arca y provocando su ira al creer que Bilbo le había traicionado. En ese momento llegó Gandalf y los enanos de las Colinas de Hierro, a quienes Röac había avisado por orden de Thorin para que acudieran en su ayuda, pero cuando parecía que se iban a enfrentar contra los hombres y los elfos, un ejército de trasgos y huargos descendió de las montañas y les hizo unirse contra ellos en la que se llamó la Batalla de los Cinco Ejércitos. Cuando la batalla parecía perdida, aparecieron las águilas y Beorn, quien mató al jefe de los trasgos, Bolgo, haciendo que su ejército se dispersara y fuera destruido. A pesar de la victoria, Thorin y sus sobrinos Fíli y Kíli, murieron. Los distintos pueblos se repartieron el tesoro y Bilbo regresó a su casa con una pequeña parte y con el anillo.

En una carta que envió al escritor estadounidense W. H. Auden en 1955, J. R. R. Tolkien recuerda como en un verano, cuando era profesor de anglosajón en el Pembroke College de la Universidad de Oxford, escribió la primera frase de "El hobbit" durante la corrección de unos exámenes de literatura inglesa (llamados por aquel entonces ensayos de graduación). Encontró un papel en blanco y escribió en él: «En un agujero en el suelo vivía un hobbit», sin saber ni siquiera de donde había sacado la idea. Antes de su muerte, Tolkien dijo que no recordaba la fecha exacta en la que escribió esta frase, ni siquiera el primer capítulo, ya que no lo organizó y fue escribiendo la obra según le iban surgiendo ideas. Michael, el segundo hijo de Tolkien, señaló 1929 como el posible año en el que su padre comenzó a escribir la obra, ya que conservaba algunas composiciones propias que estaban fechadas de dicho año y eran claras imitaciones de "El hobbit", el cual les fue leído a él y a sus hermanos según se escribía.

Durante un breve período tras la invención de la primera frase, Tolkien no hizo nada más que elaborar el mapa de Thrór, donde describió la geografía en la que se desarrolla la mayor parte de la novela. No obstante, una vez iniciada la composición de la obra, los capítulos fueron escritos con fluidez y sin apenas correcciones hasta el punto en el que el dragón Smaug (que por entonces se llamaba Pryftan) muere. En esta primera versión, Gandalf era el nombre del enano principal y el mago se llamaba Bladorthin, palabra de origen gnómico (lengua que más tarde evolucionaría al noldorin) que significa ‘amplia tierra gris’ y que en el texto final quedó en una única mención de un cierto rey que compraba armas enanas. Posiblemente fuera precursor del apelativo Mithrandir (‘peregrino gris’ en sindarin), uno de los nombres que recibe Gandalf en "El Señor de los Anillos". 
Desde el comienzo de la obra se aprecia en ella la influencia de lo que más tarde sería "El Silmarillion". Tolkien hizo referencia a "La balada de Leithian", la historia de Beren y Lúthien, cuando Bladorthin cuenta a los enanos como sólo ellos dos habían conseguido vencer al Nigromante, relacionando así a este último con el personaje de Thû (nombre que recibió Sauron en su primera versión). Además, el rey trasgo que Bandobras Tuk mata en la batalla de los Campos Verdes se llamaba en un primer momento Fingolfin, igual que el hijo del rey elfo Finwë en los "Cuentos perdidos", y el personaje de Elrond fue emparentado con los medios elfos, hecho que fue descrito por Tolkien como «un afortunado accidente, consecuencia de la dificultad de estar inventando continuamente buenos nombres para los nuevos personajes». Incluyó también referencias a los gnomos (nombre que recibieron las primeras versiones de los Noldor) y a la destrucción del reino élfico de Gondolin por parte de los dragones. 

Para escribir el viaje de la compañía desde Rivendel hasta el otro lado de las Montañas Nubladas, incluyendo la fuerte tormenta que ven, Tolkien se basó en sus vacaciones en Suiza en 1911. Años antes Tolkien había escrito una serie de poemas que recopiló bajo el título "Cuentos y canciones de Bimble Bay" y entre los cuales había uno, "Glip", protagonizado por una criatura viscosa de ojos luminosos que roía huesos en una caverna y que sirvió de base para el posterior personaje de Gollum. El capítulo que trascurre en el Bosque Negro fue uno de los que más cambió con respecto a la versión publicada de "El hobbit", ya que el pasaje en el río Encantado y la captura de los enanos por parte de los elfos del bosque no aparecen en esta primera versión.

Tolkien rebautizó al enano principal usando otro nombre de "Völuspá", Eikinskjaldi, que, traducido como «escudo de roble», se trataba de un nombre propio y no un apodo como en la versión definitiva de la novela, mientras que el mago pasó a llamarse Gandalf, un nombre más apropiado por significar «elfo del bastón» en islandés. El nombre de Bladorthin se conservó para un misterioso rey que tan sólo es nombrado en una línea y no vuelve a aparecer en ningún otro texto de Tolkien. 

También cambió el nombre del dragón Pryftan por Smaug, pasado del verbo germánico primitivo "smugan" («meter por un agujero»), algo que consideraba «un mal chiste filológico». Originalmente había dispuesto que fuese Bilbo quien lo matara con la ayuda de Dardo y el anillo mágico, pero al querer ofrecer algo más espectacular fue finalmente el arquero Bardo quien lo mató. Tolkien abandonó la composición de la obra en este punto, dejándola inconclusa.

A finales de 1932, le prestó una copia de "El hobbit" a su amigo C. S. Lewis para que la leyera. También se la prestó a Elaine Griffiths, una ex-alumna y amiga de la familia que trabajaba para la editorial británica George Allen & Unwin por recomendación del propio Tolkien. En 1936 Griffiths le comentó a una ex-compañera de Oxford, Susan Dagnall, que también trabajaba en George Allen & Unwin, sobre la existencia de la obra y ella habló con Tolkien para pedirle prestado el libro. Una vez leído e interesada en él, Dagnall le pidió que finalizara la obra para poder presentarla en la editorial. 

Originalmente había previsto que la batalla contra los trasgos tuviera lugar durante el regreso de Bilbo a la Comarca y se llamara batalla de los Valles del Anduin, pero la situación se complicó cuando Tolkien se ocupó de la cuestión relacionada con la codicia que despertaba el tesoro y el hecho de que Esgaroth reclamara su parte para reconstruir la ciudad destruida, desembocando todo en la Batalla de los Cinco Ejércitos. "El hobbit" estaba acabado en verano de 1936, pero Tolkien no envió la copia hasta el 3 de octubre. Stanley Unwin, presidente de la editorial, pensaba que los niños eran los mejores jueces de literatura infantil, así que le dio la obra a su hijo Rayner, de tan solo diez años de edad, para que la leyera y le gustó tanto que decidió publicarla. 

Cuando recibió la prueba de composición en febrero de 1937, Tolkien encontró algunos pasajes que tenía que rectificar, ya que no había tenido suficiente tiempo para examinar con minuciosidad el manuscrito que envió a la editorial en un principio. Debido a esto y algunos problemas con las ilustraciones, la novela no sería publicada hasta septiembre.

J. R. R. Tolkien siempre dijo que "El hobbit" había sido influenciado por "El Silmarillion", novela aún inacabada cuando fue publicada la primera, además de por distintos poemas épicos, mitos y cuentos de hadas que había leído. Entre algunas de las fuentes que se han citado se encuentran "Beowulf", el poema épico anglosajón con el que Tolkien trabajó durante parte de su vida, los cuentos de hadas de Andrew Lang y de los hermanos Grimm, "La princesa y el trasgo" y su secuela "La princesa y Curdie", o "El maravilloso país de los snergs". 
Según la biografía de Tolkien elaborada por Humphrey Carpenter, el origen del mago Gandalf fue una postal que el escritor compró en 1911 durante unas vacaciones en Suiza y que reproducía una pintura del artista alemán Josef Madlener titulada "Der Berggeist" («El espíritu de la montaña»). En ella aparece un anciano de barba canosa, vestido con un largo manto y un sombrero de ala ancha, junto a un cervatillo en un bosque con las montañas de fondo. Carpenter afirma en su biografía que, años después de adquirir la postal, Tolkien escribió en el sobre que la guardaba: «Origen de Gandalf»; sin embargo, el artículo "The Origin of Gandalf and Josef Madlener" («"El origen de Gandalf y Josef Madlener"»), de Manfred Zimmerman, ofrece una investigación más profunda sobre los orígenes del mago y revela como la hija de Josef Madlener confirmó en una entrevista que la pintura databa de mediados de la década de 1920. Cuando preguntaron a Carpenter por la discrepancia en las fechas, afirmó que se había basado en las notas escritas en una copia del sobre donde Tolkien guardaba la postal y de la cual no encontró ningún rastro ante las peticiones de los fans para que la mostrara. 
La figura definitiva de Gandalf estuvo particularmente influida por la deidad de la mitología nórdica Odín en su encarnación como Vegtamr, un anciano de larga barba blanca, con un sombrero de ala ancha y un bastón de caminante. El nombre de Gandalf y los de los enanos de la compañía fueron tomados por Tolkien de "Edda poética", una colección de poemas escritos en nórdico antiguo, y más concretamente de "Völuspá", el primero de ellos. Debido al hecho de que Gandalf abandona a los enanos y a Bilbo en varias ocasiones justo antes de ser capturados, Douglas A. Anderson, autor de la edición anotada de "El hobbit", comenta en ella que la conducta del personaje recuerda a la del espíritu de la montaña de las Riesengebirge checo-polacas, Rübezahl, quien disfruta haciendo que los viajeros se extravíen.

T. A. Shippey señala en "El camino a la Tierra Media" otro de los poemas de "Edda poética", "Skirnismál" («El cantar de Skirnir»), como influencia para las Montañas Nubladas, y más concretamente la frase: Observa además que Beorn tiene ciertas similitudes con Beowulf y Bothvarr Bjarki, personaje de la saga nórdica de Hrólfr Kraki.

Cuando le preguntaron a Tolkien si el pasaje en el que Bilbo roba el copón a Smaug se basaba en el episodio del robo del copón en "Beowulf", respondió:
Shippey afirma que la conversación entre ambos personajes está inspirada además en el poema "Fáfnismál" («La balada de Fáfnir») de "Edda poética", mientras que Anderson apunta en su edición anotada de "El hobbit" que los dos grandes dragones que aparecen en la literatura nórdica, Fafner, de la saga "Volsunga", y el de "Beowulf", murieron cuando les acuchillaron el vientre, igual que Smaug. Este último cita también algunas influencias de las novelas de George MacDonald, "La princesa y el trasgo" y su secuela "La princesa y Curdie", como el aspecto semejante de los trasgos, la oscura visión de las raíces de las montañas o la conducta de Galion, mayordomo de Thranduil, que es similar a la del mayordomo del rey en la secuela, pues a ambos les gustaba beberse los mejores vinos de sus respectivos señores en su misma bodega.

En 1955, Tolkien admitió en una carta dirigida al poeta y ensayista W. H. Auden que la novela infantil "El maravilloso país de los snergs", de E. A. Wyke-Smith, era «probablemente el libro que sirvió de fuente inconsciente sólo para los hobbits y para nada más». Ésta narra la historia de un snerg, un miembro de una raza antropomorfa caracterizada por su baja estatura, igual que los hobbits. En un principio, Tolkien dijo que pudo haber creado la palabra hobbit inspirándose en la novela satírica "Babbitt", de Sinclair Lewis, pero más tarde, en los apéndices de "El Señor de los Anillos", establece que la palabra viene del anglosajón "hol-bytla" (‘morador de agujeros’).

El pasaje de los huargos se inspiró en parte en la batalla de los licántropos, en "The Black Douglas", considerada por Tolkien como una de las mejores novelas de S. R. Crockett y una de las que más le impresionó durante su infancia. Anderson señala en su edición anotada de "El hobbit" que el gobernador y los consejeros de la Ciudad del Lago podrían estar inspirados en el alcalde y los miembros del ayuntamiento que regían la ciudad de Hamelín en el poema "El flautista de Hamelín" (1842), de Robert Browning, pues todos los personajes se caracterizan por ser «tacaños, egoístas y conscientes de los intereses de los ciudadanos sólo en la medida que les convenga».

George Allen & Unwin había previsto en un primer momento que el libro fuera ilustrado sólo con mapas, pero los primeros esbozos de Tolkien gustaron tanto que optaron por incluirlos sin aumentar el precio del libro a pesar del coste adicional. Alentado por ello, el escritor les ofreció un segundo lote de ilustraciones y, tras aceptarlas, la editorial le convenció para que diseñara también la sobrecubierta. No obstante, esta ilustración requería varios colores para su impresión, rojo, azul, verde y negro, lo cual aumentaba el coste y les obligó a eliminar el primero de ellos. 

Como ya había pasado con la sobrecubierta, los mapas originales que Tolkien había diseñado para la novela tenían varios colores y tuvo que volver a dibujarlos. Poco después sugirió que el mapa general fuera colocado en las retiraciones, pero el de Thrór debía ir dentro del primer capítulo; la editorial lo rechazó y decidió colocar ambos en las retiraciones, desbaratando de este modo una idea de Tolkien que consistía en colocar unas runas lunares «mágicas» en el segundo mapa que debían verse sólo a contraluz.

La editorial estadounidense Houghton Mifflin, que estaba preparando la edición americana, sugirió encargar a un diseñador anónimo que hiciera algunas ilustraciones en color para acompañar a los dibujos en blanco y negro de Tolkien. Éste estuvo de acuerdo, con la única condición de que las ilustraciones no recordaran ni estuvieran influenciadas por Disney, hacia el que Tolkien había adquirido un verdadero odio. George Allen & Unwin no estaba de acuerdo con esta decisión y creía mejor que fuera el propio Tolkien quien ilustrara todo el libro, por lo que finalmente fue así.

George Allen & Unwin publicó la primera edición de "El hobbit" en el Reino Unido el 21 de septiembre de 1937, que contó con apenas 1.500 ejemplares y fue vendida antes del mes de diciembre debido a las buenas críticas que recibió. Por ello, la editorial puso a la venta a finales de año una segunda reimpresión, esta vez incorporando las ilustraciones en color. La edición americana fue publicada el 1 de marzo de 1938, con cuatro de las ilustraciones en color y fue todo un éxito, habiendo vendido casi 3.000 copias en junio del mismo año. 

A pesar de la popularidad de "El hobbit", el racionamiento de papel durante la Segunda Guerra Mundial y su continuidad hasta 1949, hicieron que las ventas bajaran y que el libro no estuviera a menudo disponible durante este período, a pesar de que se logró realizar dos reimpresiones, una en 1942 y otra en 1946. Las ventas del libro aumentaron considerablemente con la publicación de "El Señor de los Anillos", llegando a su máximo en los años 1960 y alcanzando a principios de 2008 los cien millones de copias vendidas en todo el mundo.

Desde entonces, la novela ha sido reimpresa con frecuencia por muchos editores y además ha sido traducida a más de cuarenta idiomas, algunos de ellos con varias traducciones. En verano de 1938, poco antes del estallido de la Segunda Guerra Mundial, la editorial alemana Rütten & Loening, encargada de la publicación de "El hobbit" en su país, le escribió a Tolkien para saber si era de origen judío debido a que tenía un apellido alemán. Aunque no era este el caso, Tolkien se sintió muy ofendido, pues consideraba la doctrina nazi como «racista, perniciosa y del todo anticientífica», y pidió a George Allen & Unwin que retrasara la traducción alemana.

Uno de los libros de la primera edición en inglés fue vendido a finales del año 2004 en una subasta por 6.000 libras esterlinas, mientras que otro, esta vez firmado, alcanzó la puja de 60.000 libras en una subasta realizada en marzo de 2008.

En diciembre de 1937 Stanley Unwin le pidió a Tolkien una secuela de "El hobbit". En respuesta, él le proporcionó los borradores de "El Silmarillion", pero los editores los rechazaron creyendo que el público quería «más información sobre hobbits». Posteriormente, Tolkien comenzó a trabajar en lo que se convertiría en "El Señor de los Anillos", un rumbo que no sólo cambió el contexto de la historia original, sino que también provocó cambios sustanciales para el personaje Gollum. 

En la primera edición de "El hobbit", Gollum apuesta su anillo mágico en el juego de los acertijos y al perder está dispuesto a dárselo a Bilbo amistosamente, pero al haberlo perdido le pidió incluso perdón y le ayudó a escapar de las cavernas. Con el fin de reflejar el nuevo concepto del anillo y su capacidad corruptora, Tolkien hizo que Gollum fuera más agresivo con Bilbo y que tras perder se reflejara la codicia producida por el anillo. 

En 1947 Tolkien le envió a Unwin esta versión reescrita del quinto capítulo, «Acertijos en las tinieblas», como un ejemplo del tipo de cambios necesarios para poner el libro en conformidad con "El Señor de los Anillos". Cuando en julio de 1950 le fueron enviadas las pruebas de la nueva edición, Tolkien se sorprendió al ver la incorporación del texto cambiado en ella, ya que en ningún momento había dicho que se cambiara. Esta segunda edición fue publicada en 1951, con una nota explicativa en la que Tolkien atribuía los cambios del capítulo V a que la historia de la primera edición fue la que contó Bilbo a sus amigos, mientras que la que aparece en la segunda edición es la que de verdad ocurrió y que contó a Gandalf al ser presionado por éste. 

En 1965, cuando la editorial estadounidense Ace Books publicó una edición de bolsillo no autorizada de "El Señor de los Anillos", Houghton Mifflin y Ballantine Books le pidieron a Tolkien que revisara la obra con el fin de corregir pequeños fallos y de que esta versión pudiera competir en el mercado con la no autorizada. A pesar de que la revisión de "El Señor de los Anillos" era más urgente a causa de esto, Tolkien revisó de nuevo "El hobbit", introduciendo nuevos cambios y ajustando la descripción aún más a su secuela y a la evolución de su todavía inédito "Quenta Silmarillion", tal como estaba en ese momento. Esta tercera edición fue publicada en febrero de 1966. 

Algo común en las distintas ediciones del libro son las erratas de imprenta, que se trataron de solucionar con la versión anotada de Douglas A. Anderson, publicada en 1988, fecha que coincidía el quincuagésimo aniversario de la publicación americana de "El hobbit".

Desde la muerte de J. R. R. Tolkien se han publicado tres obras sobre "El hobbit" conmemorando el aniversario de la publicación original: "El hobbit anotado", editado por Douglas A. Anderson con comentarios y apéndices; una edición ilustrada por el artista Alan Lee; y "La historia de El hobbit", dividida en dos volúmenes elaborados por J. D. Rateliff y que, a semejanza de "La historia de la Tierra Media", ofrece el proceso de composición de la novela. 

La edición anotada de Anderson, publicada en 1988, conmemora el quincuagésimo aniversario de "El hobbit" desde su publicación en Estados Unidos y proporciona el texto completo de la novela con comentarios en el margen, precedido de una introducción y seguido por dos apéndices. Para compilar esta edición, Anderson utilizó escritos de otros autores: "El camino a la Tierra Media" de T. A. Shippey, "J. R. R. Tolkien, una biografía" y "Las cartas de J. R. R. Tolkien" de Humphrey Carpenter, y varios textos recopilados por Christopher Tolkien, quien además leyó la obra antes de su publicación y aportó algunas sugerencias. Además reunió un conjunto de ilustraciones para acompañar al texto, entre las que se incluyen algunas realizadas por el propio Tolkien, una selección en blanco y negro de las que han aparecido en las múltiples traducciones de "El hobbit", y algunas realizadas por los artistas Tove Jansson y Eric Fraser. En la introducción, Anderson ofrece información sobre el origen, la composición, las influencias, la publicación y algunas críticas de "El hobbit" original publicado en 1937, mientras que en los apéndices se ofrece una detallada crónica sobre las revisiones y una pequeña explicación sobre el uso de las runas.

La edición ilustrada por Alan Lee fue publicada en 1997 para conmemorar el sexagésimo aniversario desde la publicación de "El hobbit" en Reino Unido. La obra incluye 26 ilustraciones en color y 38 en blanco y negro, todas elaboradas por Lee.

"La historia de El hobbit" fue publicada en 2007, esta vez en conmemoración del septuagésimo aniversario. Se divide en dos tomos con notas y extensos comentarios de J. D. Rateliff: "Mr. Baggins" («"Señor Bolsón"»), que comprende la historia desde el primer borrador que se conserva hasta la llegada a la Ciudad del Lago, y "Return to Bag End" («"Regreso a Bolsón Cerrado"»). Además incluyen los borradores para las revisiones realizadas en 1947 y 1960, así como un apéndice con la cronología de composición.

Existen dos traducciones en español de "El hobbit", una hecha por Teresa Sánchez Cuevas para Fabril Editora y otra hecha por Manuel Figueroa para Ediciones Minotauro. Además, la novela ha sido publicada por otras editoriales, aunque usando siempre la traducción de Figueroa.

La primera traducción publicada de "El hobbit" fue la de Fabril Editora, compañía perteneciente a la familia Muchnik, que fue la primera en obtener los derechos de las . Bajo el título de "El hobito", fue publicada en Argentina en el año 1964. Esta versión recibió algunas críticas por su traducción y por la ilustración de su portada, de Luis Videla, en la que aparece Bilbo hablando con un Gandalf de nariz pequeña, con tacos y la punta del sombrero cortada, descripción contradictoria con la que hace Tolkien en la novela. Aparte de la traducción errónea del título, aparecen otras como la adaptación de la primera frase a los cuentos de hadas españoles, «Había una vez un hobito que vivía en una cueva en la tierra», o como los cambios de raza: troll por «enano» (en vez de «troll»), goblin por «duende» (en vez de «trasgo») y dwarf por «gnomo» (en vez de «enano»). Este último cambio hizo que el propio Tolkien interviniera, ya que la traducción confundía dos razas distintas de su legendarium en la frase «"the elves that are now called Gnomes"», que hacía referencia a uno de los linajes de los Altos Elfos, los Noldor, y no a los enanos. Tolkien había utilizado el término «gnomo», que deriva del griego "gnosis" («conocimiento» o «inteligencia»), en sus escritos anteriores pensando que era un buen nombre para designar a aquellos que eran más sabios entre los elfos, pero, finalmente y debido a la denominación común del término como gnomo de jardín, acabó sustituyéndolo por el de Noldor. Hoy en día es difícil encontrar esta versión de "El hobbit" y se ha convertido en un objeto de coleccionistas.

A pesar de poseer también "El Señor de los Anillos", los Muchnik no pudieron publicarlo por falta de dinero y Francisco Porrúa, de la editorial Minotauro, compró los derechos de autor, publicó los tres volúmenes de la obra (1977, 1979, 1980) y encargó una nueva traducción de "El hobbit" a Manuel Figueroa. La obra fue finalmente publicada en España en 1982 y en Argentina en 1984, ambas con una ilustración de Tolkien en la portada. En 1990 se publicó "El hobbit anotado" y dieciséis años después una versión revisada y ampliada bajo el título "El hobbit. Anotado e ilustrado", mientras que "La historia de El hobbit" quedó sin traducción.

El desarrollo y la maduración del protagonista, Bilbo Bolsón, es el tema principal de la historia. Matthew Grenby, autor de "Children's Literature", señala en este libro que "El hobbit" es una novela de desarrollo personal y la considera un "bildungsroman" (novela de aprendizaje o formación) en lugar de la tradicional aventura fantástica, pues el protagonista adquiere un sentido más fuerte de su identidad y una mayor confianza en el mundo exterior gracias al viaje que realiza. En su ensayo "The Psychological Journey of Bilbo Baggins", recogido en la obra "A Tolkien Compass" de Jared Lobdell, Dorothy Matthews señala que en varios capítulos se ve reflejado el concepto jungiano de individuación y describe el viaje de Bilbo como una búsqueda de madurez y como una metáfora de este proceso de individuación. La analogía del «» y del héroe que regresa de él con un premio (como el anillo o las espadas élficas) que lo beneficia, encaja con los arquetipos míticos relativos a la iniciación y madurez masculina tal como los describe el mitólogo Joseph Campbell. Por otro lado, Jane Chance compara en "Tolkien's Art" el desarrollo y el crecimiento de Bilbo, en contraste con otros personajes, con los conceptos de mera realeza versus realeza derivados del Ancrene Wisse y de una interpretación cristiana de "Beowulf".

Matthew Grenby también señala en "Children's Literature" que la superación de la codicia y el egoísmo es el centro moral de la historia. Además, otro tema de "El hobbit" que ha sido tratado por varios autores es el animismo, un concepto importante en la antropología y en el desarrollo infantil basado en la idea de que todas las cosas, incluyendo objetos inanimados, fenómenos naturales, animales y plantas, poseen una inteligencia humana. En "La historia de El hobbit", John D. Rateliff lo llama el «tema del doctor Dolittle» y cita la multitud de animales que hablan como indicativo para confirmar dicho tema, por ejemplo el dragón Smaug, los trasgos o el cuervo Roäc. Patrick Curry señala en "Defending Middle-Earth" que el animismo se encuentra activo durante toda la novela y que también aparece en otras obras de Tolkien; menciona las «raíces de las montañas» y los «pies de los árboles» como cambio de nivel desde lo inanimado a lo animado.

Los críticos literarios dedicaron a "El hobbit" reseñas mayoritariamente favorables cuando fue publicado por primera vez en 1937. Muchos de ellos recogían la nota de propaganda realizada por la editorial Allen & Unwin para la obra y que la comparaba con "Alicia en el país de las maravillas" y "A través del espejo", ambas novelas de Lewis Carroll. No obstante, a J. R. R. Tolkien no le gustaba y finalmente fue cambiada en la segunda impresión.

El escritor C. S. Lewis escribió dos reseñas anónimas, ambas en el periódico "The Times", en las que elogiaba tanto a la novela como a Tolkien y predecía su futuro éxito. 

"El hobbit" recibió buenas críticas en la "Horn Book Magazine" por parte de la editora Bertha E. Mahony y de la columnista Anne Carroll Moore, mientras que William Rose Benét lo calificaba en el "Saturday Review of Literature" como una «espléndida fantasía. Anne T. Eaton, de "The New York Times", lo describía como «un cuento maravilloso sobre una aventura magnífica, llena de suspense y condimentada con un humor sereno que es irresistible» y lo situaba entre los mejores libros infantiles publicados en mucho tiempo. En el Second Annual Children's Festival celebrado el 1 de mayo de 1938, el "New York Herald Tribune" concedió a "El hobbit" un premio valorado en 250 dólares y le consideró el mejor libro infantil publicado esa primavera.

Por otro lado, "Júnior Bookshelf" se mostraba contrario a los buenos comentarios y sostenía que la obra estaba compuesta de manera tal «que no se respira la animosa libertad de la verdadera aventura». Muchos años después, en 1981, Constance B. Hieatt escribió un artículo sobre las distintas revisiones de "El hobbit" y en él defendía además la obra de algunas críticas señalando que «muchos críticos se han equivocado sencillamente porque no han sido capaces de diferenciar las diversas revisiones».

"El hobbit" fue nominada al premio Carnegie Medal, entregado anualmente por "The Library Association" al libro infantil/juvenil más destacado. También fue reconocida como «la novela más importante del siglo XX» en la encuesta «Libros infantiles del siglo», organizada por la revista "Books for Keeps".

El 27 de noviembre de 1977, se estrenó en la cadena de televisión estadounidense NBC la primera versión fílmica de "El hobbit". La película, de dibujos animados, fue producida por Arthur Rankin Jr. y Jules Bass y tuvo un coste de unos tres millones de dólares estadounidenses, convirtiéndose en la película de animación para televisión más cara de la historia. El guion fue escrito por Romeo Muller, siendo bastante fiel a la novela, excepto en algunos puntos, como la desaparición del personaje de Beorn. Además, algunas de las canciones de Tolkien fueron adaptadas por Maury Laws para la película. Si bien tuvo bastante éxito, algunos puntos, sobre todo de la animación, fueron criticados, como el aspecto gatuno del dragón Smaug o la apariencia de los elfos. El sello discográfico Walt Disney Records publicó un LP basado en el doblaje del audio completo de la película, que más tarde sería editado también por Buena Vista Records, alcanzando un buen éxito en ventas.
El director neozelandés Peter Jackson y su esposa Fran Walsh expresaron en 1995 su interés por filmar una adaptación cinematográfica de "El hobbit", que sería la primera parte de una trilogía completada con dos películas sobre "El Señor de los Anillos". Sin embargo, en esos momentos y dado que los derechos de "El hobbit" aún pertenecían a la empresa United Artists, sólo les fue posible filmar "El Señor de los Anillos", cuyos derechos habían sido comprados por New Line Cinema. 

Después del éxito de la trilogía de Jackson, en septiembre de 2006, la compañía Metro-Goldwyn-Mayer, que había comprado United Artists y por tanto era ahora propietaria de los derechos de "El hobbit", expresó su interés en trabajar en equipo con New Line y Jackson para hacer la protosecuela. No obstante, Jackson había presentado una demanda contra New Line a causa de la pérdida de ingresos procedentes de los productos de "", por lo que las relaciones entre el director y la compañía estaban rotas. Finalmente, ambas parte llegaron a un acuerdo en diciembre de 2007, y tras pagar a Jackson el dinero que le debía, New Line le confirmó como productor de "El hobbit".

La adaptación iba a ser dirigida por Guillermo del Toro, quien también intervino en la elaboración del guion junto con Jackson, Fran Walsh y Philippa Boyens. No obstante, el 31 de mayo de 2010, después de haberse involucrado por más de dos años en el diseño y producción de las adaptaciones, Del Toro renunció a la dirección de las cintas debido al retraso en el comienzo de la filmación por los problemas financieros de Metro-Goldwyn-Mayer. El guion fue en un principio dividido en dos películas e iba a estar acabado en 2008, pero Del Toro confirmó en una entrevista a principios de 2009 que aún les quedaba trabajo por hacer. Originalmente querían que la primera parte abordara la mayoría de los acontecimientos que aparecen en la novela, mientras que la segunda completaría la historia y, además, abordaría los cincuenta años que transcurren hasta el comienzo de "El Señor de los Anillos"; sin embargo, acabaron descartando este plan y simplemente dividieron "El hobbit" para que ocupara las dos películas. El rodaje, que tuvo lugar en Nueva Zelanda, comenzó finalmente el 21 de marzo de 2011, pese a los retrasos, y finalizó el 6 de julio de 2012, tras 266 días de filmación. Poco después de acabar el rodaje, Jackson anunció que había material para dividir la historia en tres películas e interés para ello, por lo que habría una tercera parte. Los estrenos están programados para diciembre de 2012, diciembre de 2013 y julio de 2014.

Desde su publicación, varios juegos de mesa y de rol se han basado en la novela. Bajo el nombre "The Battle of the Five Armies", Larry Smith diseñó y puso a la venta en 1975 un juego de mesa cuyos derechos fueron comprados posteriormente por la compañía estadounidense TSR (Tactical Studies Rules). En 1983 Iron Crown Enterprises (ICE) lanzó otro juego de mesa llamado "The Lonely Mountain: Lair of Smaug the Dragon" que fue diseñado por Coleman Charlton; un año después, la misma compañía creó su versión de "The Battle of the Five Armies", desarrollada también por Charlton junto a Richard H. Britton y John Crowll, y un juego de rol llamado "Middle-earth Role Playing", basado tanto en "El hobbit" como en "El Señor de los Anillos". En 2001 salió a la venta un nuevo juego de mesa con el nombre "The Hobbit: The Defeat of the Evil Dragon Smaug", diseñado por Keith Meyers y Michael Stern e ilustrado por Ted Nasmith.

Además se han basado en la historia varios videojuegos, tanto con licencia como sin ella. Uno de los que tuvo mayor éxito fue el juego para ordenador "El hobbit", desarrollado en 1982 por Manga Software y distribuido por Melbourne House, que en 1983 ganó el premio Golden Joystick en la categoría de «Juego de estrategia del año». Sierra Entertainment puso a la venta en 2003 un juego de plataformas con elementos de videojuego de rol, titulado también "El hobbit", para PC, PlayStation 2, Xbox y Nintendo GameCube. Una versión basada en los mismos diseños de personajes e historia, pero usando una plataforma isométrica en 2D y personajes en 3D, fue creada para Game Boy Advance.

Desde septiembre hasta noviembre de 1968, la emisora BBC Radio 4 emitió una adaptación radiofónica de "El hobbit" con guion de Michael Kilgarriff y producida por John Powell. Fue dividida en ocho partes con una duración total de cuatro horas y un guion muy próximo al texto de la novela (en su edición de 1951). Contaba con el actor Anthony Jackson como narrador, Paul Daneman en el papel de Bilbo, Wolfe Morris como Gollum, John Justin como Thorin, John Pullen como Elrond, Peter Williams como Bardo y Heron Carvic como Gandalf. La serie fue puesta a la venta en formato casete en 1988 y en formato CD en 1997.

En 1989 la editorial Eclipse Comics publicó una novela gráfica de "El hobbit" escrita por Charles “Chuck” Dixon y Sean Deming, e ilustrada por David Wenzel. Fue dividida en tres volúmenes, finalizando la trama del primero en el momento en que Gollum plantea a Bilbo el juego de los acertijos y la del segundo cuando la compañía escapa de los elfos del Bosque Negro. Lorenzo Díaz se encargó de la traducción española, publicada en 1991 por Norma Editorial y reeditada en 2001 debido al aumento de las ventas de los libros de Tolkien producido por el estreno de la trilogía cinematográfica de "El Señor de los Anillos".





</doc>
<doc id="8310" url="https://es.wikipedia.org/wiki?curid=8310" title="Albrecht Kossel">
Albrecht Kossel

Ludwig Karl Martin Leonhard Albrecht Kossel (Rostock, Alemania, 16 de septiembre de 1853 - Heidelberg, 5 de julio de 1927). médico alemán.

Hijo de Albrecht Kossel, cónsul de Prusia y su esposa Clara. Estudia medicina en la Universidad de Rostock y en 1872 continúa sus estudios en la Universidad de Estrasburgo*, donde recibe lecciones de Bary, Waldeyer, Kundt, Baeyer y Felix Hoppe-Seyler. Obtiene la licenciatura en 1878.

Dirigió el Instituto de Fisiología de Berlín. Más tarde accede a la Cátedra de Fisiología de Marburgo, siendo titular de la misma en Heidelberg, ciudad donde fallece en 1927.

Descubrió los ácidos nucleicos. A este bioquímico alemán le fue otorgado el Premio Nobel de Fisiología o Medicina en 1910 por sus contribuciones en el desciframiento de la química de ácidos nucleicos y proteínas, descubriendo los ácidos nucleicos, bases en la molécula de ADN, que constituye la sustancia genética de la célula.

Su vocación investigadora le introdujo en el área de la fisiología celular y siguiendo los descubrimientos de Miescher comenzó a desarrollar una serie de estudios que le llevaron a importantes conclusiones sobre la síntesis de las proteínas, a destacar la importancia de las enzimas y a intuir el papel de los ácidos nucleicos en la herencia. Establece las bases de la estructura del ADN, al estudiar las nucleínas (nucleoproteínas) mostrando que consistían en una porción proteica y otra no-proteica (ácidos nucleicos). 

Posteriormente, describe sus componentes, distinguiendo entre adenina, citosina, guanina, timina y uracilo. Kossel estableció las bases que condujeron a esclarecer la estructura del ADN.


(*)A la sazón parte de Alemania tras la guerra franco-prusiana de 1870.

Fue un noble caballero, fin.


</doc>
<doc id="8312" url="https://es.wikipedia.org/wiki?curid=8312" title="Adenina">
Adenina

La adenina es una de las cinco bases nitrogenadas que forman parte de los ácidos nucleicos (ADN y ARN) y en el código genético se representa con la letra A. Las otras cuatro bases son la guanina, la citosina, la timina y el uracilo. En el ADN la adenina siempre se empareja con la timina y en el ARN con el uracilo.

Forma los nucleósidos adenosina (Ado) y desoxiadenosina (dAdo), y los nucleótidos adenilato (AMP) y desoxiadenilato (dAMP). En la bibliografía antigua, la adenina fue alguna vez llamada vitamina B4; sin embargo, hoy no se la considera una verdadera vitamina.

Su fórmula es . Es un derivado de la purina (es una base púrica) en la que un hidrógeno ha sido sustituido por un grupo amino ():

Al igual que la guanina, la citosina, la timina y el uracilo (todas bases nitrogenadas), forma parte de los nucleótidos que constituyen las largas cadenas de ácidos nucleicos; cada nucleótido está formado por un grupo fosfato, un azúcar de cinco carbonos (ribosa o desoxirribosa) y una base nitrogenada de las ya mencionadas. 
En la estructura de doble hélice en forma de ‘escalera retorcida’ que presenta el ácido desoxirribonucleico (ADN), cada base se acopla con otra base específica, formando los ‘travesaños’ de la escalera. La unión de estas bases se produce por afinidad química, de forma que en el ADN, la adenina siempre se une a la timina. En las secuencias de nucleótidos, la adenina se representa por la letra A.

También forma parte de la molécula de trifosfato de adenosina, que constituye la fuente principal de energía a nivel celular, y está presente en muchas sustancias naturales como la remolacha, el té y la orina.

La adenina, junto con la timina, fue descubierta en 1885 por el bioquímico alemán Albrecht Kossel. En 1959 el bioquímico español Juan Oró pudo sintetizar la adenina a partir del ácido cianhídrico.

El metabolismo de las purinas conlleva la formación de adenina (A) y guanina (G). Ambas derivan del ácido inosínico el cual es sintetizado sobre el precursor ribosa-5-fosfato, usando átomos de los aminoácidos glicina, glutamina y ácido aspártico.

Síntesis de novo de nucleótidos de purina: en el primer paso determinante de esta ruta, un grupo amino proporcionado por la glutamina, se une al C1 del fosforribosil pirofosfato (PRPP). La 5-fosforribosilamida resultante es muy inestable y el anillo de purina se construye a continuación sobre esta estructura.

El segundo paso consiste en la incorporación de tres átomos de la glicina, cuyo grupo carboxilo se activa gastando ATP. El grupo amino de la glicina incorporada se formila a continuación por el N10-formilmetiltetrahidrofolato, y se incorpora un nitrógeno suministrado por la glutamina, antes de que la deshidratación y el cierre del anillo den lugar al anillo de imidazol de cinco átomos del núcleo de la purina, en forma de 5-aminoimidazol ribonucleótido (AIR).

En los eucariotas superiores, el AIR es carboxilado a carboxiaminoimidazol ribonucleótido en un solo paso por la AIR carboxilasa (dos etapas en bacterias y hongos). El aspartato cede su grupo amino en dos pasos: formación del enlace amida y eliminación de su esqueleto carbonado.

El último átomo de carbono es proporcionado por el N10-formilmetiltetrahidrofolato y luego tiene lugar la segunda carboxilación que proporciona el segundo de los anillos adyacentes del núcleo de purina. El primer intermediario con un anillo purínico completo es en inosinato (IMP). La conversión del inosinato en adenilato requiere la incorporación de un grupo amino procedente del aspartato. El guanilato se forma por oxidación del inosinato en C2.

Anteriormente la literatura se refería a veces a la Adenina como Vitamina B4. Ya no se considera una verdadera vitamina o parte del complejo de la Vitamina B. Sin embargo, dos vitaminas B, niacina y riboflavina se unen a la adenina para formar cofactores esenciales: nicotinamín adenín dinucleótido (NAD) y flavín adenín dinucleótido (FAD), respectivamente. Hermann Emil Fischer fue uno de los primeros científicos en estudiar la adenina.

Fue nombrada en 1885 por Albrecht Kossel, refiriéndose al páncreas (del griego "aden") de donde provenía la muestra de Kossel. Experimentos llevados a cabo en 1961 por el bioquímico catalán Joan Oró mostraron que gran cantidad de adenina podía ser sintetizada a partir de la polimerización de amonio con cinco moléculas de cianuro de hidrógeno (HCN) en disolución acuosa lo que tiene implicaciones en el debate sobre el origen de vida en la Tierra.

El 8 de agosto de 2011 se publicó un informe basado en estudios de la NASA con meteoritos encontrados en la Tierra, sugiriendo que DNA y RNA podían haberse originado en el espacio exterior.



</doc>
<doc id="8315" url="https://es.wikipedia.org/wiki?curid=8315" title="Guanina">
Guanina

La guanina es una base nitrogenada púrica, una de las cinco bases nitrogenadas que forman parte de los ácidos nucleicos (ADN y ARN) y en el código genético se representa con la letra G. Las otras cuatro bases son la adenina, la citosina, la timina y el uracilo. Forma los nucleósidos guanosina (Guo) y desoxiguanosina (dGuo) y los nucleótidos guanilato (GMP) y desoxiguanilato (dGMP). La guanina siempre se empareja en el ADN con la citosina mediante tres puentes de hidrógeno. Además es una de las bases más importantes de los ácidos nucleicos.

Esta sustancia está presente en los excrementos de los ácaros, que es un alérgeno causante de enfermedades como la rinitis y faringitis.

La guanina fue aislada por primera vez en 1844 a partir de los excrementos de aves marinas, conocidos como guano, que se usaban como fuente de fertilizante. Entre 1882 y 1906, Emil Fischer determinó su estructura y también mostró que el ácido úrico se puede convertir en guanina.



</doc>
<doc id="8323" url="https://es.wikipedia.org/wiki?curid=8323" title="Código genético">
Código genético

El código genético es el conjunto de reglas que define cómo se traduce una secuencia de nucleótidos en el ARN a una secuencia de aminoácidos en una proteína. El código es común a todos los seres vivos (aunque hay pequeñas variaciones), lo cual demuestra que ha tenido un origen único o universal, al menos en el contexto de nuestro planeta.

El código define la relación entre cada secuencia de tres nucleótidos, llamada codón, y cada aminoácido.

La secuencia del material genético se compone de cuatro bases nitrogenadas distintas, que tienen una representación mediante letras en el código genético: adenina (A), timina (T), guanina (G) y citosina (C) en el ADN y adenina (A), uracilo (U), guanina (G) y citosina (C) en el ARN.

Debido a esto, el número de codones posibles es 64,de los cuales 61 codifican aminoácidos (siendo además uno de ellos el codón de inicio, AUG) y los tres restantes son sitios de parada (UAA, llamado ocre; UAG, llamado ámbar; UGA, llamado ópalo). La secuencia de codones determina la secuencia de aminoácidos en una proteína en concreto, que tendrá una estructura y una función específicas.

Cuando Francis Crick, Rosalind Franklin, James Watson y Maurice Wilkins presentaron el modelo de la estructura del ADN se comenzó a estudiar en profundidad el proceso de traducción en las proteínas.

En 1955, Severo Ochoa y Marianne Grunberg-Manago aislaron la enzima polinucleótido fosforilasa, capaz de sintetizar ARNm sin necesidad de molde a partir de cualquier tipo de nucleótidos que hubiera en el medio. Así, a partir de un medio en el cual tan sólo hubiera UDP (uridína difosfato) se sintetizaba un ARNm en el cual únicamente se repetía el ácido uridílico, es decir, un poli-U.

George Gamow postuló que el código genético estaría formado por tripletes de bases nitrogenadas (A;U;C;G) y que a partir de estas se formarían los 20 aminoácidos esenciales para la vida. La primera demostración de que los codones constan de tres nucleótidos la proporcionó el experimento de Crick, Brenner y colaboradores. Marshall Nirenberg y Heinrich J. Matthaei en 1961 en los Institutos Nacionales de Salud descubrieron la primera correspondencia codón-aminoácido. Empleando un sistema libre de células, tradujeron una secuencia ARN de poli-uracilo (UUU...) y descubrieron que el polipéptido que se sintetizaba sólo contenía fenilalanina. De esto se deduce que el codón UUU especifica el aminoácido fenilalanina. Continuando con el trabajo anterior, Nirenberg y Philip Leder fueron capaces de determinar la traducción de 54 codones, utilizando diversas combinaciones de ARNm, pasadas a través de un filtro que contiene ribosomas. Los ARNt se unían a tripletes específicos.

Posteriormente, Har Gobind Khorana completó el código, y poco después, Robert W. Holley determinó la estructura del ARN de transferencia, la molécula adaptadora que facilita la traducción. Este trabajo se basó en estudios anteriores de Severo Ochoa, quien recibió el premio Nobel en 1959 por su trabajo en la enzimología de la síntesis de ARN. En 1968, Khorana, Holley y Nirenberg recibieron el Premio Nobel en Fisiología o Medicina por su trabajo.

El genoma de un organismo se encuentra en el ADNo, en el caso de algunos virus, en el ARN. La porción de genoma que codifica varias proteínas o un ARN se conoce como gen. Esos genes que codifican proteínas están compuestos por unidades de trinucleótidos llamadas codones, cada una de los cuales codifica un aminoácido. Cada nucleótido está formado por un fosfato, una desoxirribosa y una de las cuatro posibles bases nitrogenadas. Las bases purínicas adenina (A) y guanina (G) son más grandes y tienen dos anillos aromáticos. Las bases pirimidínicas citosina (C) y timina (T) son más pequeñas y sólo tienen un anillo aromático. En la configuración en doble hélice, dos cadenas de ADN están unidas entre sí por puentes de hidrógeno en una asociación conocida como emparejamiento de bases. Además, estos puentes siempre se forman entre una adenina de una cadena y una timina de la otra y entre una citosina de una cadena y una guanina de la otra. Esto quiere decir que el número de residuos A y T será el mismo en una doble hélice y lo mismo pasará con el número de residuos de G y C. En el ARN, la timina (T) se sustituye por uracilo (U), y la desoxirribosa por una ribosa. 

Cada gen que codifica una proteína se transcribe en una molécula plantilla, que se conoce como ARN mensajero o ARNm. Éste, a su vez, se traduce en el ribosoma, en una cadena polipeptídica (formada por aminoácidos). En el proceso de traducción se necesita un ARN de transferencia, o ARNt, específico para cada aminoácido, con dicho aminoácido unido a él de forma covalente, guanosina trifosfato como fuente de energía y ciertos factores de traducción. Los ARNt tienen anticodones complementarios a los codones del ARNm y se pueden “cargar” covalentemente en su extremo 3' terminal con aminoácidos. Los ARNt individuales se cargan con aminoácidos específicos gracias a las enzimas llamadas aminoacil-ARNt sintetasas, que tienen alta especificidad tanto por un aminoácido como por un ARNt. Esta alta especificidad es el motivo fundamental del mantenimiento de la fidelidad en la traducción de proteínas.

Para un codón de tres nucleótidos (un triplete) son posibles 4³ = 64 combinaciones diferentes; los 64 codones están asignados a aminoácido o a señales de parada en la traducción. Si, por ejemplo, tenemos una secuencia de ARN, UUUAAACCC, y la lectura del fragmento empieza en la primera U (convenio 5' a 3'), habría tres codones que serían UUU, AAA y CCC, cada uno de los cuales especifica un aminoácido. Esta secuencia de ARN se traducirá en una secuencia de tres aminoácidos.

El código genético es compartido por todos los organismos conocidos, incluyendo virus y orgánulos, aunque pueden aparecer pequeñas diferencias. Así, por ejemplo, el codón UUU codifica el aminoácido fenilalanina tanto en bacterias como en arqueas y en eucariontes. Este hecho indica que el código genético ha tenido un origen único en todos los seres vivos conocidos. La palabra "universal" en este contexto aplica solamente a la vida en la Tierra, ya que no se ha establecido la existencia de vida en otro planeta.

Gracias a la genética molecular, se han distinguido 22 códigos genéticos, que se diferencian del llamado código genético estándar por el significado de uno o más codones. La mayor diversidad se presenta en las mitocondrias, orgánulos de las células eucariotas que se originaron evolutivamente a partir de miembros del dominio Bacteria a través de un proceso de endosimbiosis. El genoma nuclear de las eucariotas sólo suele diferenciarse del código estándar en los codones de iniciación y terminación.

Ningún codón codifica más de un aminoácido; de no ser así, conllevaría problemas considerables para la síntesis de proteínas específicas para cada gen.
Tampoco presenta solapamiento: los tripletes se hallan dispuesto de manera lineal y continua, de manera que entre ellos no existan comas ni espacios y sin compartir ninguna base nitrogenada. Su lectura se hace en un solo sentido (5' - 3'), desde el codón de iniciación hasta el codón de parada. Sin embargo, en un mismo ARNm pueden existir varios codones de inicio, lo que conduce a la síntesis de varios polipéptidos diferentes a partir del mismo transcrito.

El código genético tiene redundancia pero no ambigüedad (ver tablas de codones). Por ejemplo, aunque los codones GAA y GAG especifican ambos el ácido glutámico (redundancia), ninguno especifica otro aminoácido (no ambigüedad). Los codones que codifican un aminoácido pueden presentar diferencias puntuales en la tercera posición. Es por ello que de forma general la tolerancia al cambio en esta posición es mayor que en la primera y segunda, y por tanto tiende a estar menos enriquecida en variantes patológicas (situación que se concentra fundamentalmente en el primer nucleótido del codón). Debido a que las mutaciones de transición (purina a purina o pirimidina a pirimidina) son más probables que las de transversión (purina a pirimidina o viceversa), la equivalencia de purinas o de pirimidinas en los lugares dobles degenerados añade una tolerancia a los fallos complementarios.

Una consecuencia práctica de la redundancia es que algunos errores del código genético sólo causen una mutación silenciosa o un error que no afectará a la proteína porque la hidrofilidad o hidrofobidad se mantiene por una sustitución equivalente de aminoácidos; por ejemplo, un codón de NUN (N =cualquier nucleótido) tiende a codificar un aminoácido hidrófobo. NCN codifica residuos aminoacídicos que son pequeños en cuanto a tamaño y moderados en cuanto a hidropatía; NAN codifica un tamaño promedio de residuos hidrofílicos; UNN codifica residuos que no son hidrofílicos. Estas tendencias pueden ser resultado de una relación de las aminoacil ARNt sintetasas con los codones heredada un ancestro común de los seres vivos conocidos.

Incluso así, las mutaciones puntuales pueden causar la aparición de proteínas disfuncionales. Por ejemplo, un gen de hemoglobina mutado provoca la enfermedad de células falciformes. En la hemoglobina mutante un glutamato hidrofílico (Glu) se sustituye por una valina hidrofóbica (Val), es decir, GAA o GAG se convierte en GUA o GUG. La sustitución de glutamato por valina reduce la solubilidad de β-globina que provoca que la hemoglobina forme polímeros lineales unidos por interacciones hidrofóbicas entre los grupos de valina y causando la deformación falciforme de los eritrocitos. La enfermedad de las células falciformes no está causada generalmente por una mutación de novo. Más bien se selecciona en regiones de malaria (de forma parecida a la talasemia), ya que los individuos heterocigotos presentan cierta resistencia ante el parásito malárico Plasmodium (ventaja heterocigótica o heterosis).

La relación entre el ARNm y el ARNt a nivel de la tercera base se puede producir por bases modificadas en la primera base del anticodón del ARNt, y los pares de bases formados se llaman “pares de bases wobble” (tambaleantes). Las bases modificadas incluyen inosina y los pares de bases que no son del tipo Watson-Crick U-G.

La expresión "código genético" se utiliza con frecuencia en los medios de comunicación como sinónimo de genoma, de genotipo, o de ADN. Frases como «Se analizó el código genético de los restos y coincidió con el de la desaparecida», o «se creará una base de datos con el código genético de todos los ciudadanos» son científicamente incorrectas. Es insensato, por ejemplo, aludir al «código genético de una determinada persona», porque el código genético es el mismo para todos los individuos. Sin embargo, cada organismo tiene un genotipo propio, aunque es posible que lo comparta con otros si se ha originado por algún mecanismo de multiplicación asexual.

El código genético estándar se refleja en las siguientes tablas. La tabla 1 muestra qué aminoácido está codificado por cada uno de los 64 codones. La tabla 2 muestra qué codones especifican cada uno de los 20 aminoácidos que intervienen en la traducción. Estas tablas se llaman tablas de avance y retroceso respectivamente. Por ejemplo, el codón AAU es el aminoácido asparagina, y UGU y UGC representan cisteína (en la denominación estándar por 3 letras, Asn y Cys, respectivamente).

Nótese que el codón AUG codifica la metionina pero además sirve de sitio de iniciación; el primer AUG en un ARNm es la región que codifica el sitio donde la traducción de proteínas se inicia.

La siguiente tabla inversa indica qué codones codifican cada uno de los aminoácidos.

Existen otros dos aminoácidos codificados por el código genético en algunas circunstancias y en algunos organismos. Son la selenocisteína y la pirrolisina.

La selenocisteína (Sec, U) es un aminoácido presente en multitud de enzimas (glutatión peroxidasas, tetraiodotironina 5' deiodinasas, tiorredoxina reductasas, formiato deshidrogenasas, glicina reductasas y algunas hidrogenasas). Está codificado por el codón UGA (que normalmente es de parada) cuando están presentes en la secuencia los elementos SecIS (secuencia de inserción de la selenocisteína).

El otro aminoácido, la pirrolisina (Pyl, O), es un aminoácido presente en algunas enzimas de arqueas metanógenas. Está codificado por el codón UAG (que normalmente es de parada) cuando están presentes en la secuencia los elementos PylIS (secuencia de inserción de la pirrolisina).

Como se mencionó con anterioridad, se conocen 22 códigos genéticos. He aquí algunas diferencias con el estándar:

A pesar de las variaciones que existen, los códigos genéticos utilizados por todas las formas conocidas de vida son muy similares. Esto sugiere que el código genético se estableció muy temprano en la historia de la vida y que tiene un origen común en las formas de vida actuales. El análisis filogenético sugiere que las moléculas ARNt evolucionaron antes que el conjunto actual de aminoacil-ARNt sintetasas.

El código genético no es una asignación aleatoria de los codones a aminoácidos. Por ejemplo, los aminoácidos que comparten la misma vía biosintética tienden a tener la primera base igual en sus codones y aminoácidos con propiedades físicas similares tienden a tener similares a codones.

Experimentos recientes demuestran que algunos aminoácidos tienen afinidad química selectiva por sus codones. Esto sugiere que el complejo mecanismo actual de traducción del ARNm que implica la acción ARNt y enzimas asociadas, puede ser un desarrollo posterior y que, en un principio, las proteínas se sintetizaran directamente sobre la secuencia de ARN, actuando éste como ribozima y catalizando la formación de enlaces peptídicos (tal como ocurre con el ARNr 23S del ribosoma).

Se ha planteado la hipótesis de que el código genético estándar actual surgiera por expansión biosintética de un código simple anterior. La vida primordial pudo adicionar nuevos aminoácidos (por ejemplo, subproductos del metabolismo), algunos de los cuales se incorporaron más tarde a la maquinaria de codificación genética. Se tienen pruebas, aunque circunstanciales, de que formas de vida primitivas empleaban un menor número de aminoácidos diferentes, aunque no se sabe con exactitud que aminoácidos y en que orden entraron en el código genético.

Otro factor interesante a tener en cuenta es que la selección natural ha favorecido la degeneración del código para minimizar los efectos de las mutaciones y es debido a la interacción de dos átomos distintos en la reacción
. Esto ha llevado a pensar que el código genético primitivo podría haber constado de codones de dos nucleótidos, lo que resulta bastante coherente con la hipótesis del balanceo del ARNt durante su acoplamiento (la tercera base no establece puentes de hidrógeno de Watson y Crick).


Servicios en línea para convertir ADN (DNA, en inglés) en proteína:
Tablas del código genético
Revisiones


</doc>
<doc id="8327" url="https://es.wikipedia.org/wiki?curid=8327" title="Vi">
Vi

Vi ("Visual") es un programa que entra en la categoría de los editores de texto, pues a diferencia de un procesador de texto no ofrece herramientas para determinar visualmente cómo quedará el documento impreso. Por esto carece de opciones como centrado o justificación de párrafos, pero permite mover, copiar, eliminar o insertar caracteres con mucha versatilidad. Este tipo de programas es frecuentemente utilizado por programadores para escribir código fuente de software.

Vi fue originalmente escrito por Bill Joy en 1976, tomando recursos de "ed" y "ex", dos editores de texto para Unix, que trataban de crear y editar archivos, de ahí, la creación de "vi". 

Hay una versión mejorada que se llama Vim, pero "Vi" se encuentra en —casi— todo sistema de tipo Unix, de forma que conocer rudimentos de "Vi" es una salvaguarda ante operaciones de emergencia en diversos sistemas operativos.

El editor vi tiene dos modos de operación:

En el modo de comandos, podemos desplazarnos dentro de un archivo y efectuar operaciones de edición como buscar texto, eliminar texto, modificar texto, etc. Vi suele iniciarse en modo de comandos.

En el modo insertar, podemos escribir texto nuevo en el punto de inserción de un archivo. Para volver al modo de comandos, presione la tecla .

Para usar vi, estos son los comandos esenciales para escribir, editar, borrar, copiar y pegar.





Para pasar de modo edición a modo de comandos se emplea la tecla , para desplazarse sobre el archivo se emplean las teclas (abajo), (arriba), (izquierda) y (derecha).

También puede emplear las flechas si su terminal lo permite, (PgUp), (PgDn). 

Para ir a una línea específica puede escribir el número de la línea seguido de o , por ejemplo o también puede utilizar seguido del número de línea y . Para mostrar el número de las líneas, se puede ejecutar codice_3, y para quitar los números codice_4. Para ir al final de la línea en la que está el cursor , para ir al comienzo . Para llegar al inicio del archivo o y para llegar al final del archivo . Para buscar un texto: codice_5 seguido del "texto" que desea buscar y . Luego puede presionar o para el siguiente o anterior resultado de la búsqueda. Después de hacer cambios puede salvarlos con codice_6 o para salvar y salir puede emplear . Para ejecutar un comando del intérprete de comandos puede emplear codice_7 seguido del comando y . Si se escribe codice_8, se podrá ver en la esquina inferior derecha el modo en el que se está, ya sea comandos o de edición. Puedes teclear codice_9 para ver las opciones disponibles. 

Una de las utilidades más comunes es el uso de codice_10 que corresponde a la unión de las opciones guardar ( ) y salir ( ), o bien el modo forzado es codice_11 que sale de vi sin guardar cambios.

Si se desea consultar otro comando, ya sea del editor vi o de cualquier otro, puedes revisar el manual en línea que tiene el sistema UNIX, tecleando: codice_12, por ejemplo: codice_13




</doc>
<doc id="8328" url="https://es.wikipedia.org/wiki?curid=8328" title="Abetalipoproteinemia">
Abetalipoproteinemia

La abetalipoproteinemia o Síndrome de Bassen-Kornzweig, es una rara enfermedad , autosómica recesiva, que afecta al tracto digestivo, cuya principal característica es la incapacidad que tiene el organismo de absorber adecuadamente los componentes grasos del alimento a través del intestino, lo que tiene como consecuencia la producción de heces grasosas (esteatorrea), deficiencia en el desarrollo infantil y problemas en los nervios.

Esta enfermedad es de origen genético, probándose la existencia de una mutación en un gen en los pacientes: proteína de transferencia de triglicérido microsómico (MTP). Esta enfermedad afecta a ambos sexos, sin embargo es más frecuente en los hombres, llegando estos a representar el 70% de los casos.

La mutación ocasiona que el organismo sea incapaz de producir lipoproteínas sanguíneas, entre las que incluyen, lipoproteínas de muy baja densidad VLDL (b-100) y los quilomicrones (b-48).El motivo subyace en la incapacidad de fabricar apolipoproteína B (b48 y b100 ). Alterando la funcionalidad de las lipoproteínas del plasma.

Debido a esto los pacientes con este síndrome son incapaces de digerir grasas, lo que conlleva a una deficiencia en el desarrollo de los nervios (neuropatía) y ataxia.

Entre los síntomas se encuentran un retraso en el desarrollo durante la lactancia, heces anormales (excesivamente grasosas, fétidas y espumosas), dificultades del habla, coordinación y equilibrio y debilidad muscular que conlleva a una curvatura de la columna.

El médico tratante puede solicitar un CSC (Conteo Sanguíneo Completo), en el cual se notará la presencia de glóbulos rojos anormales (con aspecto espinoso, véase Acantocitosis); estudios de colesterol, los cuales muestran un bajo nivel de lipoproteínas; recolección de material fecal, con el cual se comprobará la presencia de altos niveles de grasas; y, un examen genético comprobará las mutaciones en los genes APOB o MTP.

El tratamiento consiste en seguir una dieta baja en grasas para evitar los síntomas digestivos, y el uso de suplementos vitamínicos. Debido a que las grasas son parte fundamental del desarrollo, los triglicéridos de cadena larga son sustituidos por triglicéridos de cadena media, los cuales son absorbidos de manera diferente por el intestino.

Los resultados del tratamiento varían mucho, dependiendo del grado y alcance de los problemas neurológicos. En casos muy graves se presenta una enfermedad neurológica irreversible alrededor de los 30 años.



</doc>
<doc id="8330" url="https://es.wikipedia.org/wiki?curid=8330" title="Río Misisipi">
Río Misisipi

El Misisipi o Misisipí es un largo río del centro de Estados Unidos que fluye en dirección sur a través de diez estados —Minnesota, Wisconsin, Iowa, Misuri, Illinois, Kentucky, Tennessee, Arkansas, Misisipi y Luisiana— hasta desaguar en el golfo de México (océano Atlántico), cerca de Nueva Orleans. Tiene una longitud de 3734 km, pero si se considera el sistema Misisipi-Misuri alcanza los 6275 km, que lo sitúan como el , tras los ríos Amazonas, Nilo y Yangtsé. 

Drena una cuenca de 3 238 000 km², la cuarta más extensa del mundo, por detrás de las del Amazonas (6 145 000 km²), Congo (3 170 000 km²) y Nilo (3 255 000 km²).

Durante la época precolombina ya constituía una importante vía de navegación y los amerindios lo llamaban «"Meschacebé"» que significa «padre de las aguas». Hoy en día, constituye un elemento fundamental de la economía y de la cultura estadounidense.

El 11 de septiembre de 1997 el presidente Bill Clinton designó dos secciones de este río, el Alto y Bajo Misisipí, como uno de los catorce ríos que integran el sistema de ríos del patrimonio estadounidense.

Su nacimiento está situado en el extremo norte del lago Itasca (al norte de Minnesota), a 450 msnm. El río alcanza pronto los 210 metros después de las cascadas de Saint Anthony, cerca de Minneapolis y se le unen los ríos Illinois y Misuri en San Luis (Misuri) y el Ohio en Cairo (Illinois).
Se puede dividir el curso del río en dos partes: el Misisipi superior, desde su nacimiento hasta la confluencia con el Ohio, y el Misisipi inferior, desde el Ohio hasta su desembocadura. El río describe numerosos meandros, en particular entre Memphis (Tennessee) y el delta. La gran mayoría pertenecen a la categoría de meandros de llanura aluvial (también llamados meandros libres o divagantes), esto es, que tienen su origen en la dinámica del curso mismo; se trata de meandros muy móviles que conciernen a sectores húmedos o abandonados como los «bayous» del Sur. En varios lugares, ciertos meandros se recortaron y dejaron brazos muertos denominados "oxbow" o «lagos en forma de herradura».

La parte inferior es compleja: "bayous", lagos, bifurcaciones, afluentes...

Su cuenca hidrográfica es la más grande de Norteamérica y la cuarta del mundo, tras las del Amazonas, Congo y Nilo. Su superficie total es de 3.238.000 km², es decir un tercio del territorio de los Estados Unidos. La cuenca del Misisipi drena agua de 31 estados y es la fuente del 23% del abastecimiento de aguas superficiales públicas de los Estados Unidos. Está dividida en seis subcuencas, que corresponden a los cursos inferior y superior, así como a los principales afluentes: el Misuri (4.370 km), el Arkansas, el Ohio, etc. Finalmente, la planicie inundable del sistema fluvial mide cerca de 90.000 km². Más de 72 millones de personas viven en su cuenca, es decir, uno de cada cuatro ciudadanos de los Estados Unidos.

Drena la mayor parte de la zona comprendida entre las Montañas Rocosas y los Apalaches, salvo la zona próxima a los Grandes Lagos. Atraviesa o bordea diez estados (Minnesota, Wisconsin, Iowa, Illinois, Misuri, Kentucky, Arkansas, Tennessee, Misisipi y Luisiana) antes de desembocar en el golfo de México, 160 km río abajo de Nueva Orleans. Una gota de lluvia que caiga en el lago Itasca tarda cerca de 90 días en llegar al golfo de México.

Es un río con un caudal importante y poderosas crecidas, teniendo en cuenta la naturaleza de la cuenca que baña. Éstas últimas pertenecen en efecto a la zona templada y no a la zona intertropical como el Amazonas o el Congo. Estos ríos tienen un caudal muy superior al del Misisipi, por la abundancia de las precipitaciones sobre sus cuencas. Hasta el río Orinoco, que tiene una cuenca casi cuatro veces menor y una longitud de algo más de un tercio que la del Misisipi-Misouri, tiene un caudal promedio de casi el doble que dicho río.

El régimen hidrológico es complejo ya que el río es alimentado por afluentes muy diferentes: el curso superior tiene un régimen pluvionival mientras que el curso inferior atraviesa una región subtropical húmeda. Recibe las aguas del Misuri incrementadas por las del deshielo de las nieves de las Montañas Rocosas en primavera. Su parte inferior es alimentada por lluvias abundantes en verano y al principio del otoño, con riesgos ciclónicos en la parte más meridional.

Por consiguiente, el caudal del Misisipi se caracteriza por grandes variaciones en función del lugar y la temporada; generalmente oscila entre 8.000 m³/s y 50.000 m³/s. En la desembocadura el caudal medio es de 18.000 m³/s, que es mucho para un río situado en la zona templada, y ocupa la sexta posición mundial por su caudal. Pero durante el período de crecidas, el caudal puede subir fácilmente a 70.000 m³/s, alcanzando incluso los 300.000 m³/s durante la crecida de 1927. El río Ohio contribuye con más de la mitad del caudal total del Misisipi (8.000 m³/s). Mencionar finalmente que su caudal medio interanual, referido a la extensión de su cuenca hidrográfica, es de 5,9 litros por segundo y por km².

El Misisipi arrastra aluvión compuesto de arenas y gravas que provienen en gran parte de Las Montañas Rocosas. Los sedimentos sólidos vertidos en el golfo de México oscilan entre 312 y 450 millones de toneladas al año. Es por causa de estos materiales el que se formen las numerosas islas y su delta. A lo largo de la mayor parte del río la pendiente es media o poco pronunciada, por lo cual los depósitos sedimentarios son relativamente importantes. Este caudal sólido es mixto, compuesto por partículas en suspensión y sedimentos del fondo. En total, en un año, el río transporta 131 millones de toneladas de materias en suspensión en el agua, dos veces menos que el Amazonas.

El sector situado río arriba de Minneapolis, en Minnesota, está próximo al nacimiento del río. El río nace en el norte del lago Itasca a unos 450 msnm de altitud. El clima de esta región es de tipo continental y está influenciado por las masas de aire polar en invierno, lo que conlleva que a menudo esté helado en esa época del año. En su nacimiento es sólo un pequeño río de aguas claras; pero a medida que avanza, el río crece, se carga de aluviones y partículas orgánicas y se vuelve pardo rojizo, y pierde progresivamente su carácter natural y salvaje.

Esta primera parte del río desciende el mayor desnivel de su curso. Atraviesa zonas pantanosas, lagos y rápidos poblados de numerosas especies de peces, aves y mamíferos. La vegetación de este sector incluye pinos, alisos, arroz silvestre y colonias de aneas. Entre las ciudades de Aitkin y Brainerd, en Minnesota, el río atraviesa una región de colinas, de relieves morrénicos cubiertos de bosques, de planicies de origen glaciar y de sectores dunares y pantanosos. Antes de la explotación humana, los bosques de coníferas cubrían esta región.

El curso superior ("Upper Mississippi River") va desde las cataratas de San Antonio (en Minneapolis) a la desembocadura del río Ohio, cerca de la ciudad de Cairo en el estado de Illinois. Recorre 1.462 km siguiendo una trayectoria en dirección sur-este. El régimen del río es pluvionival con crecidas en primavera y lluvias tempestuosas en verano. El cauce se ensancha considerablemente después de la confluencia del Minnesota. El río atraviesa un valle profundo cavado en lechos sedimentarios en una región que no ha sido afectada por los glaciares de Wisconsin. El lago Pepin, que se formó hace cerca de 9.500 años, se extiende a lo largo de unos 35 km de longitud con una profundidad media de cinco metros. Tiene la capacidad de retener una parte de los sedimentos y de la polución que proviene de la parte más elevada.

Justo al norte de San Luis, el Misuri se encuentra con el Misisipi proveniente del oeste. Las aguas del Misuri están cargadas de sedimentos y de partículas arrancadas por la erosión. En los años 1950-1960, la construcción de grandes presas en la cuenca hidrográfica del Misuri formó depósitos que retienen los aluviones. Los acondicionamientos humanos afectaron ampliamente al Misisipi superior y a su llanura aluvial.
Así, el río recibió más sedimentos, mientras que el cauce aumentó a causa de la urbanización y drenaje de las marismas. La construcción de diques y la canalización aumentaron la sedimentación del lecho fluvial. La llanura aluvial también ha sido transformada por la construcción de terraplenes con el fin de proteger las instalaciones humanas de las inundaciones. Las tierras agrícolas sustituyen desde hace tiempo las zonas pantanosas y los bosques; estos últimos se encuentran actualmente restringidos a las orillas del río o a las islas y no miden más que unos kilómetros de anchura. Sin embargo, los esfuerzos para calificar porciones de las orillas como reservas naturales protegidas permitieron salvaguardar 800 km² del valle del curso superior del río. Las principales especies de árboles son: el arce plateado, el fresno verde, el olmo americano, el sauce negro, el álamo de Virginia, el arce, el abedul negro, el almez, etc. La vegetación acuática comprende las aneas, las lilas de agua, las elodeas, vallisneria americana, etc. La salicaria común es una planta de origen europeo, introducida a principios de siglo XX a lo largo de sus orillas.

El curso inferior ("Lower Mississippi River") discurre al sur de la confluencia con el Misuri. El Misisipi continúa su trayectoria hacia el sudeste y posteriormente hacia el sur después de la confluencia con el Arkansas. La llanura aluvial se caracteriza por numerosos meandros cargados de barro que multiplican por tres la longitud del curso. Se trata de un sector relativamente ancho, de pendiente suave hacia el golfo de México, dominado por terrazas aluviales poco elevadas. Las altitudes son poco considerables, en general unas decenas de metros por encima del nivel medio del mar. Aparte de los sectores desbrozados, subsisten las grandes zonas de pantano y de bosques. También encontramos numerosos lagos de herradura y meandros de gran amplitud. En el sur de Cairo, la llanura aluvial se ensancha y se hace menos profunda a causa de la erosión de los lechos terciarios llamados "bluffs". En el bajo Misisipi, numerosos afluentes discurren paralelamente al río a lo largo de una distancia bastante grande, antes de desembocar finalmente en el río. Al sur está sometido a un clima tropical marcado por los ciclones a finales del verano y principios de otoño. La helada invernal evita generalmente esta región. El paisaje se caracteriza por zonas húmedas y pantanosas, a menudo insalubres, en el delta del Misisipi y el bayou: se trata de brazos y meandros abandonados por el río, que forman largas vías de agua estancada y constituyen en total una red navegable de varios millares de kilómetros.

Su delta cubre una superficie de 75.000 km² (más de 400 km de anchura —de este a oeste— y 200 km de profundidad —del norte al sur—), sobre la que viven unos 2,2 millones de habitantes, la mayoría en la ciudad de Nueva Orleans. Sin embargo, comparado con otros deltas, la densidad de población de la región es relativamente escasa.

La desembocadura se desplazó muchas veces a lo largo de la historia. En 5.000 años, el río cambió nueve veces de desembocadura y la actual data sólo del siglo X. Cuando se construyó un canal a principios del siglo XIX, el río buscó reunir el lecho y la desembocadura del río Atchafalaya, a 95 km de Nueva Orleans.

El delta avanza aproximadamente 100 m al año, alimentado por los 730 millones de toneladas de aluvión que deposita a razón de 6 dm al año sobre el fondo de su lecho, lo que hace necesario un dragado constante para asegurar la navegación. Estos depósitos forman un inmenso abanico aluvial que gana terreno sobre las aguas del golfo de México debido a la poca profundidad de las aguas y de la escasa amplitud de las mareas. El cieno y el barro impiden al río la posibilidad de serpentear.

Su planicie deltaica incluye las marismas costeras de Luisiana y cubre 28.568 km². Se caracteriza por una red compleja de brazos y de levantamientos naturales en disposición radial río bajo de Baton Rouge.

El río Misisipi es una importante vía fluvial en la que históricamente se han concentrado muchos e importantes asentamientos estadounidenses, como Memphis (TN) (), Minneapolis (MN) (), New Orleans (LA) (), St. Louis (MO) (), Saint Paul (MN) () y Baton Rouge (LA) (). 

Las principales áreas metropolitanas en sus inmediaciones son las siguientes:

Muchas de las comunidades a lo largo del río Misisipi se enumeran a continuación; la mayoría tienen un significado histórico o tradición cultural que la relaciona con el río. Están secuenciados desde el nacimiento del río hasta su final y los habitantes corresponden todos al Censo de 2010 (con un asterisco las de otra fecha). Se resaltan en negrilla las localidades de más de .

El río Misisipi y su llanura aluvial albergan una fauna y flora muy ricas que componen el mayor sistema continuo de marismas del continente norteamericano. Las al menos 260 especies de peces que viven el río, constituyen la cuarta parte de todas las existentes en América del Norte. El río sirve de paso para la migración de numerosas aves: el 60% de las aves de Norteamérica (326 especies) utilizan la cuenca del Misisipi en sus migraciones. En el curso inferior podemos contar 60 especies diferentes de mejillones. El curso superior abriga a más de 50 especies de mamíferos y 145 especies de anfibios y reptiles (incluido el conocido aligátor, cuya especie prospera de nuevo después de estar amenazada de extinción a mediados del siglo XX). En todo el valle del Misisipi encontramos mamíferos como el castor, el mapache boreal, la nutria de río, el visón americano, el zorro rojo, la rata almizclera o la mofeta rayada. Otros animales son comunes en América del Norte: el coyote, el ciervo de Virginia, la ardilla gris, la ardilla rayada, la ardilla voladora del sur o el lince.

Algunos mamíferos presentes en el valle del Misisipi:

Numerosas porciones del río son preservadas gracias a reservas naturales y muchos espacios están todavía poblados con árboles e inundados. El ecosistema del curso superior está protegido por el "Upper Mississippi River National Wildlife and Fish Refuge" que se extiende desde Wabasha (Minnesota) hasta Rock Island (Illinois) con un trayecto de cerca de 500 km. Esta reserva cubre cerca de 80.000 hectáreas situadas en cuatro estados distintos, y se encarga de la protección de medios muy diversos (marismas, zonas húmedas, lagos, bosques de llanura aluvial, playas de arena y vertientes).

Con todo, el medioambiente fluvial ha sido transformado por los seres humanos para adecuarlo a sus necesidades de navegación y de desarrollo económico: una gran parte de la llanura aluvial sufre los excesos de la agricultura intensiva y los afluentes vierten en él cantidades importantes de aluviones, de fertilizantes y de pesticidas. Las aglomeraciones urbanas y los polígonos industriales ribereños suponen también una fuente de polución. Sin embargo, según un estudio llevado a cabo por el Servicio Geológico de Estados Unidos, las emisiones de aguas residuales disminuyeron en el curso superior del río (antes de San Luis) desde la adopción de la "Clean Water Act" en 1972. La situación es menos satisfactoria en San Luis donde las concentraciones de coliformes son importantes. Las concentraciones de pesticidas y de herbicidas provienen de la actividad agrícola: éstas aumentan por debajo de la confluencia con el Misuri, al drenar este último la región cerealista de las Grandes Llanuras. El EDTA, utilizado en la industria papelera, la fotográfica o la industria agroalimentaria, está menos presente en el Misisipi que en los grandes ríos de Europa. Los PCB persisten en los sedimentos a pesar de su prohibición. Una parte del nitrógeno y el fósforo presentes en la corriente inferior proviene del Ohio que drena regiones industriales y agrícolas.

Los grandes trabajos de acondicionamiento del Misisipi y sus afluentes tienen un objetivo triple: limitar las inundaciones, favorecer la navegación y luchar contra la erosión de los márgenes.

Los proyectos que pretenden reducir los efectos de sus desbordamientos son antiguos y numerosos. A principios del siglo XIX, la idea del encauzamiento del río es preponderante: fundado en 1775, el Cuerpo de Ingenieros del Ejército de los Estados Unidos ("United States Army Corps of Enginneers") emprende varios estudios e inicia grandes obras entre 1812 y 1815. Hay que esperar los años 1860 para ver nacer un debate entre los que quieren encauzar su curso (James Buchanan Eads por ejemplo) y los que no (Andrew Humphrey); la primera opción prevalece finalmente. Se emprenden grandes obras entre 1875 y 1880 en la región del delta. Hoy, el Cuerpo de Ingenieros del Ejército de los Estados Unidos mantiene estos diques para conservar el curso habitual del río. Sin embargo, el efecto de los diques se revela ineficaz cuando el nivel del río aumenta.

La crecida de 1927 revela el problema. Se decide entonces transferir parte de las aguas del Misisipi a su afluente el río Atchafalaya ("Project Flood"). Los trabajos se dirigen también a verter gran cantidad de agua en los lagos del delta. Un sistema de estaciones permite también vigilar el nivel del río y dar la alerta en caso de problemas.

El curso superior ha sido acondicionado con 37 presas y esclusas (la mayoría construidas en los años 1930), con el fin de mantener un canal de tres metros de profundidad para el tráfico fluvial. También se han construido lagos artificiales para utilizarse en la pesca y otras actividades de ocio náutico. Las presas no tienen en cambio la función de regular el curso del río. En período de crecida, están simplemente abiertos y dejan de funcionar. Después de San Luis el curso del río está menos encauzado, aunque a menudo está flanqueado por diques.

Otros cambios se llevaron a cabo en respuesta a temblores de tierra a lo largo de la falla de Nueva Madrid, próxima a Memphis (Tennessee) y San Luis. En 1811 y 1812, seísmos conocidos bajo el nombre de «terremotos de Nueva Madrid», alcanzaron una magnitud 8, y se asegura que por un momento invirtieron la dirección de la corriente del río. Estos cataclismos también crearon el lago Reelfoot, en Tennessee. A excepción de Davenport, la mayoría de las ciudades que bordean el río están protegidas por muelles sobrealzados o diques.

Si fue objeto de colosales trabajos de acondicionamiento, es también porque es una vía de comunicación esencial para el país. El diez por ciento de las mercancías de los Estados Unidos son transportadas sobre su curso. Desde la época precolombina, el Misisipi es un medio fundamental para el transporte de mercancías. Su orientación meridiana hace de él un eje esencial de penetración al continente norteamericano y una vía de acceso a los Grandes Lagos. Situado en su desembocadura, Nueva Orleans se desarrolló gracias a esta estratégica situación. Hoy, cerca de la mitad del sistema fluvial Misuri-Misisipi es navegable. Barcos de 2,7 m de calado pueden utilizar su curso y subir hasta Minneapolis.

A partir de 1878, 29 esclusas son construidas entre Minneapolis y San Luis con el fin de permitir navegación río arriba de los buques hasta Minneapolis. Entre 1929 y 1942, se han suprimido 16 meandros en el curso inferior para acortar el trayecto de los barcos en cerca de 240 km. Las consecuencias de estos trabajos fueron un aumento de la pendiente y de la capacidad de erosión río arriba, y de la sedimentación río abajo.

El conjunto de la red hidrográfica del Misisipi y sus afluentes alcanza los 8.000 km de longitud. Un canal de navegación lo une con el lago Míchigan (a la altura de Chicago), comunicando así el Misisipi con los Grandes Lagos. Otro canal, el "Illinois Waterway", va desde la desembocadura del río Chicago hasta la confluencia del Illinois y el Misisipi. En el sur comunica Florida y Texas a través de un canal lateral, el "Gulf Intracoastal Waterway". Varias autopistas comunican los centros urbanos del Misisipi con las diferentes fachadas marítimas del país.

Las principales actividades económicas del valle del Misisipi son la industria, el turismo, la agricultura y la acuicultura.

El sector primario comprende las actividades ligadas a la pesca fluvial y a la acuicultura (cangrejos de río, suribís, ostras, etc.) en los estados del sur. El puerto de Empire-Venice situado sobre el delta es el primero de la región en volumen y el sexto de los Estados Unidos en valor. En el medio marino, las principales especies capturadas son el cangrejo de mar, el camarón y la alosa, de la que su biotopo depende estrechamente del Misisipi.

La producción de madera para papel o como material de construcción es importante sobre todo en Luisiana, en el estado de Misisipi y en Arkansas. Los bosques del curso superior están más preservados. La caza se realiza desde la antigüedad, y todavía se practica en el valle del Misisipi, especialmente de animales de piel (mapaches, ratas almizcleras, coipos), pero también los aligátores cuya caza ha sido legalizada de nuevo en 1972. Luisiana produjo un total de 32.500 pieles en 2002.

Los productos cultivados en el valle del Misisipi varían con arreglo a la latitud: en el sur, el clima subtropical permite el cultivo del arroz, la caña de azúcar, y el algodón. En ciertos sectores (Arkansas), es necesaria la irrigación. Pero, en general, los condados que rodean el Misisipi se dedican especialmente al cultivo de los cereales, en particular la soja y el maíz. Las cosechas son exportadas fácilmente por vía fluvial. La parte central de la cuenca del Misisipi también es una región ganadera, una actividad que es fuente de polución (nitratos) para el río.

Los recursos minerales y de hidrocarburos se concentran en el sur: Luisiana es uno de los principales productores de petróleo, de gas natural y de sal en los Estados Unidos. Los 23.000 pozos de Plaquemines (delta del Misisipi) produjeron más de 21 millones de barriles de petróleo crudo en 2001. La llanura del Misisipi también proporciona arcilla (Luisiana, Misuri), arena y grava. En las regiones del curso superior, también se explota el mineral de hierro (Minnesota) y yacimientos de carbón bituminoso y antracitas (Illinois).

Las centrales eléctricas del valle del Misisipi funcionan mayoritariamente a base de carbón. Las unidades más importantes se encuentran cerca de los grandes centros urbanos. Se sirven de las aguas del río para su enfriamiento. Sobre el curso inferior se encuentran 92 centrales que utilizan combustibles fósiles, 14 la biomasa y tres centrales nucleares. La central hidroeléctrica de Keokuk (Iowa) es la única de este tipo a lo largo de todo su curso: fue construida en 1913 y produce cada año 105 megavatios.

La industria es la principal actividad económica en valor y en número de asalariados. Los principales focos industriales se encuentran en las grandes aglomeraciones. El Misisipi desempeña un papel muy importante en la localización de las industrias: permite el transporte de las materias primas y de los productos acabados o semiacabados. Además, el agua es utilizada en numerosas actividades, tales como la fabricación de papel (Memphis y Baton Rouge) o el refinado. Sobre el curso inferior del río, las industrias principales son la química (la primera industria en valor; ej: plásticos, fertilizantes), la industria agroalimentaria (el primer sector en número de asalariados; ej: productos del mar, productos derivados de la soja, las bebidas), la transformación del petróleo y los transportes (construcción naval en Avondale, Luisiana). 75 instalaciones petroquímicas y refinerías (Shell en Norco (Luisiana) y en St. Rose (Luisiana)) están localizadas en el corredor entre Baton Rouge y Nueva Orleans, generando una polución importante.

El sector terciario está dominado por el turismo, el ocio y la navegación comercial: en 1996, en la esclusa de Dresbach en Wisconsin, el tráfico total era de 13,9 millones de toneladas, entre los que estaban 9,5 millones de toneladas de productos agrícolas (principalmente maíz). Los barcos transportan graneles (cereales, carbón, petróleo) o bienes de consumo transportados en contenedores, máquinas así como productos químicos. El tráfico fluvial no deja de crecer: se pasó de 70 millones de toneladas en 1960 a 500 millones de toneladas en 2000. Este dinamismo es notable en comparación con el tráfico de los demás ríos.

Varios puertos fluviales se desarrollaron sobre los lugares de confluencia o en los puntos de rotura de carga entre diferentes modos de transporte. Los más modestos disponen sólo de unos muros que actúan de muelles. Los grandes complejos industriales portuarios se encuentran en las ciudades importantes. Las principales terminales fluviales y marítimas se encuentran en Nueva Orleans y Baton Rouge. Estos puertos exportan principalmente cereales. Las tierras adentro del Misisipi representan 23.000 km de red navegable que comunican 800 empresas. 100.000 gabarras pasan por el puerto de Nueva Orleans cada año. Ciertos convoyes de gabarras pueden alcanzar las 15.000 toneladas. El 60% de los cereales exportado por los Estados Unidos son transportados a través de su curso hacia los puertos de Nueva Orleans y de Luisiana del Sur: el tráfico total de estos dos puertos es respectivamente de 49 y de 98 millones de toneladas en 2000.

Los numerosos parques estatales y reservas naturales del valle del Misisipi atraen a turistas y a ciudadanos de la región. La diversidad del patrimonio histórico (lugares prehistóricos, fuertes, barcos, plantaciones, ciudades con barrios históricos como Nueva Orleans, atraen a numerosos visitantes y estimulan la economía de la región. El patrimonio cultural constituye una de las riquezas de su valle: tradiciones amerindias, la gastronomía de Luisiana, la herencia musical en Memphis (blues), etc. El recorrido turístico llamado «"The Great River Road"» transcurre a lo largo del río mostrando la cultura local y ofreciendo múltiples opciones de recreo. Es también posible recorrer el río en barco, por ejemplo sobre el famoso "Delta Queen" (que está catalogado como Lugar Nacional de Interés Histórico). Las orillas también cuentan con varios casinos que ingresan cada año varios cientos de millones de dólares y proporcionan millares de empleos.

Los primeros rastros de ocupación amerindia son antiguos: los arqueólogos encontraron vestigios de pobladores indígenas en el delta que datan por lo menos de 11.000 años. La civilización misisipiense, asimilada a la cultura de los mound builders, era conocida por sus grandes construcciones de túmulos en tierra (yacimientos arqueológicos de Poverty Point o Jaketown Site), que los natchez todavía utilizaban en el momento de la colonización francesa de Luisiana (Nueva Francia). Pero la mayor ciudad era Cahokia que contaba en el siglo XII con una población entre 15 y 30 mil habitantes. Los especialistas tienen la certeza de que el Misisipi servía como vía de comunicación antes de la llegada de los europeos: los amerindios lo recorrían a bordo de canoas de corteza; transportaban los troncos flotando en el río. En Cahokia hacían intercambios por cobre, nácar, carne de bisonte y de uapití. El río y sus afluentes proporcionaban también pesca.

Cuando los franceses exploraron el Misisipi, encontraron varios pueblos amerindios: los siux en el norte, quapaws en la desembocadura del Arkansas, tamarois en la confluencia con el Misuri, choctaws en el Misisipi inferior o bayagoulas en el delta.

El 8 de mayo de 1541, Hernando de Soto fue el primer europeo en alcanzar el Misisipi, que bautizó como "Gran Río del Espíritu Santo". A partir de los años 1660, Francia decide la intrusión en los territorios españoles del golfo de México, llegado así hasta lo que es el actual Canadá. Los objetivos eran encontrar un paso hacia China (Paso del Noroeste), explotar las riquezas naturales de los territorios conquistados (pieles, minerales) y evangelizar las poblaciones autóctonas.

El 17 de mayo de 1673, los franceses Louis Jolliet y Jacques Marquette inician la exploración del río, que conocían bajo el nombre siux "Ne Tongo" («el gran río») y al que ellos llamaron «río Colbert». Alcanzaron la desembocadura del Arkansas y volvieron a remontar el río tras comprobar que discurría hacia el golfo de México y no hacia el «mar de California» (océano Pacífico). Unos años más tarde, en 1682, Cavelier de La Salle y Henri de Tonti descienden también el Misisipi hasta su delta. Construyen el "Fort Prud'homme" que se convertirá más tarde en la ciudad de Memphis. En abril de 1682, la expedición llega a su desembocadura; Cavelier de La Salle hace levantar una cruz y una columna que lleva las armas del rey de Francia: la soberanía francesa se extiende desde ese momento a la totalidad del valle del Misisipi, llamado «Louisiane» (Luisiana) en honor del rey Luis XIV. La expedición utiliza el mismo camino hacia Canadá y La Salle regresa a Versalles. Allí, este último convence al ministro de la marina para que le conceda el mando de la Luisiana. Le hace creer que ésta está próxima a Nueva España dibujando un plano sobre el cual el Misisipi aparece mucho más al oeste que su curso real. Pone en marcha una nueva expedición, pero ésta se dirige al desastre: Cavelier de La Salle no consigue volver a encontrar su delta y es asesinado en 1687.

En 1698, Pierre Le Moyne d'Iberville explora a su vez su desembocadura. Veinte años más tarde, su hermano Jean-Baptiste manda una nueva expedición a la Luisiana. Éste funda la ciudad de Nueva Orleans, bautizada así en homenaje al regente, el duque de Orléans, así como un fuerte cerca de la actual Baton Rouge. A principios del siglo XVIII, John Law crea la Compañía de Occidente o «Compañía del Misisipi». Esclavos negros son transportados desde las Antillas para trabajar en las plantaciones. Otras ciudades son fundadas por los franceses, como por ejemplo San Luis (1764).

Progresivamente, los franceses imponen su presencia construyendo fuertes o puestos comerciales en posiciones estratégicas del río: Fuerte Beauharnois en el curso superior del río, Cahokia en la confluencia con el Misuri o Fuerte de Chartres a orillas del río Meramec.

Los tratados de Utrecht (1712-1714) ponen fin a la Guerra de Sucesión Española en Europa y consagran la regresión del poder de Francia en la zona. La Luisiana permanece francesa, pero inquieta por la creciente influencia de las colonias británicas. El rey trata de contener esta influencia al este de los Apalaches. Intenta una aproximación con Nueva España, situada al oeste de la Luisiana. Esta política está motivada por sus lazos familiares pero también por el afán de lucrarse con las minas y el comercio de las colonias españolas.

El Tratado de París (1763) pone fin a la Guerra de los Siete Años y establece la cesión al Reino de Gran Bretaña de toda la parte del valle al este del Misisipi y a España las tierras occidentales. Sin embargo, Francia fuerza a una debilitada España a que le retroceda la Luisiana mediante el Tratado de San Ildefonso (1800); al mes siguiente, Napoleón venderá la enorme colonia a Estados Unidos por 15 millones de dólares, equivalentes a (80 millones de francos) de la época. Tal operación se conoce como la revenderlo en 1803. Desde el este, los Estados Unidos ya tenían previsto lanzarse a la conquista del oeste: en 1795, la navegación comercial a través de su curso está abierta a los norteamericanos. En 1805, el ejército estadounidense construye Fort Snelling en el emplazamiento de la actual ciudad de Minneapolis.

En el siglo XIX, el Misisipi es conocido por los bandidos que pululan por los alrededores, entre los que se encontraba el asesino John Murrell, ladrón de caballos y traficante de esclavos de la época, que tenía su cuartel general en una isla. Su notoriedad era tal que Mark Twain le consagra un capítulo de su libro "Vida en el Misisipi". Este libro relata también las carreras de barcos de vapor ("riverboats" o "steamboats") entre los años 1830 y 1870. El primer buque de vapor que navegó entre la confluencia del Ohio y Nueva Orleans fue el "New Orleans" en 1811, durante la serie de temblores de tierra de Nueva Madrid.

La economía de plantación esclavista se desarrolla en la primera mitad del siglo XIX y produce algodón y caña de azúcar en el sur. Los terratenientes ricos se hacen construir bellas mansiones, algunas sobrealzadas sobre pilares de ladrillo para prevenir los riesgos de crecidas del río.

Durante la Guerra de Secesión, el control del río se convierte en uno de los principales objetivos. El 4 de julio de 1863 después de un asedio de cuarenta días, Vicksburg es tomado por el general Ulysses S. Grant, lo que le permitía a la Unión controlar el río y dividir la Confederación en dos.

Con el advenimiento del ferrocarril esta vía fluvial encuentra una seria competencia: el tren permite comunicar las costas atlántica y pacífica de los Estados Unidos. El tráfico de los puertos del este sobrepasa en lo sucesivo al de Nueva Orleans.

En la primavera de 1927 el río se salió de su curso en 145 lugares e inundó km² de tierras, hasta una altura de 10 m y una anchura de 30 km. En Cairo, las aguas se elevaron hasta 17 m. Las inundaciones provocaron la muerte de 200 personas y el desplazamiento forzado de otras .

A pesar de la magnitud de las crecidas anteriores, fue en 1993 cuando los Estados Unidos conocieron la inundación más devastadora y costosa (12 mil millones de dólares) hasta la actualidad. Precipitaciones excepcionales durante la primavera y verano de ese año hicieron crecer el Misisipi y su afluente principal, el Misuri. Ciertas ciudades fueron inundadas durante más de 200 días. El caudal del río sobrepasó m³/s en San Luis.

En 2002, el nadador esloveno Martin Strel descendió el Misisipi a lo largo de toda su longitud en 68 días, recibiendo por su hazaña la felicitación oficial de la Cámara de Representantes de los Estados Unidos.

Las primeras descripciones por escrito del Misisipi se hacen en los libros de viaje de los europeos en América del Norte. El escritor francés François-René de Chateaubriand (1768-1848) viajó por Norteamérica a finales del siglo XVIII y escribió varias obras con informes sobre la región del Misisipi: "Atala" (1801) y "René" (1802) se desarrollan entre amerindios de Luisiana.

En la literatura, el río está indisociablemente unido a la obra del escritor estadounidense Mark Twain (1835-1910). En su novela autobiográfica "Vida en el Misisipi" ("Life on the Mississippi") comienza con una breve historia del descubrimiento del río por Hernando de Soto en 1541 y continúa con anécdotas sobre la formación de Twain como piloto de barco de vapor trabajando como grumete de un piloto con experiencia y describe con detalle la ciencia de navegar sobre sus siempre cambiantes aguas, y la trama de dos de sus obras más populares, "Las aventuras de Tom Sawyer" (1876) y "Las aventuras de Huckleberry Finn" (1884), también tienen al Misisipi como protagonista. Por otra parte, el río inspiró a otro escritor estadounidense del siglo XIX, Herman Melville (1819-1891) en su novela "The Confidence-Man", en la que los pasajeros de un barco de vapor se cuentan historias mientras descienden el río.

El segundo capítulo (titulado "The Master of the Mississippi") de la serie de cómics "The Life and Times of Scrooge McDuck" escrita por Don Rosa se desarrolla a lo largo del río. Scrooge McDuck trabaja en un barco y se encuentra por primera vez a los «Golfos Apandadores».

"Remontando el Misisipi" ("En remontant le Mississippi") es el 16.º álbum de Lucky Luke, publicado en 1961. Los dibujos son de Morris sobre un argumento de René Goscinny.

Con su multitud de islas, sus numerosos meandros, sus bosques y su fauna, el Misisipi ofrece numerosos temas de inspiración a los artistas: el naturalista John James Audubon (1785-1851) descendió el río y pintó las aves de esta región. George Catlin (1796-1872) se interesó por las sociedades amerindias y a la exploración del Misisipi. Los paisajes de su curso superior constituyen parte de la obra de Edwin Whitefield (1816-1892) y los barcos de vapor en el río aparecen entre otros pintores. Siempre en el siglo XIX, George Caleb Bingham (1811-1879), represento escenas de caza y de la vida cotidiana en el Misisipi: su cuadro "Mississippi Boatman" (1850) forma parte de una serie dedicada a los barqueros. Esta tradición fue recuperada por fotógrafos del siglo XX como Walker Evans (1903-1975), Arthur Rothstein (1915-1985) o Robert Frank (1924-).

El Misisipi y su delta son conocidos por ser la cuna del blues. El delta blues es un estilo que hace referencia directa a esta región: los músicos tenían la costumbre de viajar a través de los diferentes estados del delta como Misisipi, Arkansas, Luisiana, Texas y Tennessee. Muchos de ellos nacieron a las orillas del río, como James Cotton, Muddy Waters, Skip James y Robert Johnson.

En el siglo XX, la comedia musical "The Show Boat", compuesta por Jerome Kern, tiene como marco el mundo de los tradicionales barcos de vapor con rueda de paletas. Ferde Grofé compuso una "Mississippi Suite". La canción de Johnny Cash "Big River" también hace alusión al Misisipi. El tema "When the Levee Breaks" (Cuando los diques se rompen) es una canción de blues escrita en 1929 por Kansas Joe McCoy y Memphis Minnie. Tiene por tema la gran crecida de 1927 y fue recuperada por el grupo de rock Led Zeppelin en 1971 en su álbum "Led Zeppelin IV".

También podemos añadir la canción country "Louisiana Woman, Mississippi Man" interpretada por el dúo Conway Twitty - Loretta Lynn: esta canción evoca la barrera que el río intenta levantar frente al amor de un hombre de Misisipi y de una chica de Luisiana.

Ha sido tomado como marco en numerosas películas: tres películas musicales tituladas "Show Boat" han sido adaptadas de la novela del mismo título de Edna Ferber. La versión de 1951, protagonizada por George Sidney y Ava Gardner, tuvo un gran éxito entre los espectadores norteamericanos. Las novelas de Mark Twain han sido llevadas en numerosas ocasiones a la gran pantalla desde los años 1930, como "The Adventures of Huckleberry Finn", filme de 1939 protagonizado por Mickey Rooney, hasta producciones recientes como el largometraje de los estudios Disney "The Adventures of Huck Finn" de 1993, protagonizado por Elijah Wood y Courtney B. Vance. La Disney estrenó en 2009 una película musical de animación, "The Princess and the Frog", que tiene como marco Nueva Orleans y el Misisipi.

No sólo el Misisipi, sino muchos de sus afluentes han constituido el escenario para un gran número de películas de Hollywood, entre las cuales podemos destacar, además de muchas otras, "Río Rojo" ("Red River"), de 1948, uno de los mejores westerns de Hollywood, dirigido por Howard Hawks y protagonizado por John Wayne, y "Striking Distance" ("Persecución mortal" en España y "Zona de impacto" en Argentina), con Bruce Willis y Sarah Jessica Parker como protagonistas, película que transcurre en los ríos Allegheny y Monongahela, que se unen en Pittsburgh para formar el río Ohio. En esta película los protagonistas son oficiales de policía de la patrulla fluvial que actúa en estos tres ríos.

En la tabla siguiente se recogen todos los ríos de la cuenca del Misisipi cuya longitud supera los 200 km, aunque pueden faltar algunos. Se ha ordenado siguiendo el río aguas abajo, desde la fuente hasta la desembocadura, dividiendo el curso en las dos partes (Misisipi Bajo o Inferior y Misisipi Alto o Superior) en que habitualmente se hace.

En general, se usan los anglicismos de los nombres de los ríos más comúnmente utilizados, en aquellos casos en que no existe uso habitual de la transliteración al español. Se debe tener cuidado a la hora de la utilización de los datos de la tabla, ya que las distintas fuentes proporcionan desigual información. En relación a la longitud, hay que tener presente que se han construido muchas presas para regular el caudal y evitar las inundaciones, lo que ha provocado la disminución de algunos cursos, en algunos casos de forma muy importante al haber quedado sumergidos largos tramos tortuosos y meándricos. Esto debe ser estudiado con cuidado al utilizar fuentes antiguas. (Ver también: .)






</doc>
<doc id="8332" url="https://es.wikipedia.org/wiki?curid=8332" title="Acondroplasia">
Acondroplasia

La acondroplasia es una displasia ósea ocasionada por un desorden genético y la principal causa de enanismo. Su principal rasgo físico son extremidades cortas mientras que el tronco es de tamaño promedio. El 75% de los casos son nuevas mutaciones y el 25% restante son trastornos autosómicos dominantes, es decir, heredada de los padres. El trastorno consiste en una modificación al ADN causada por alteraciones en el receptor del factor de crecimiento 3 de los fibroblastos, lo que a su vez genera anomalías en la formación de cartílago y por lo tanto en el crecimiento de los huesos.
La condición se presenta en uno de cada 25.000 niños nacidos vivos, es el tipo más frecuente de enanismo y se caracteriza por talla baja al nacimiento y una serie de dismorfias, entre las cuales destacan: macrocefalia, hipoplasia de la región maxilar, acortamiento de los huesos largos y dedos, y radiológicamente, platispondilia, disminución de la distancia interpedicular de la columna lumbar, deformidad de las regiones metafiso-epifisiarias, huesos ilíacos cuadrados y marcadamente disminuidos de altura y un foramen magnum estrecho, entre otras. El diagnóstico fenotípico es evidente en cualquier etapa de la vida, incluso en ocasiones durante el período prenatal

Las personas con esta mutación genética tienen un cincuenta por ciento de probabilidad de tener un hijo propio con este mismo trastorno. Si dos personas con acondroplasia tienen un hijo existe un 50% de probabilidades de heredar la condición, un 25% de probabilidades de tener un bebé de altura promedio y un 25% de posibilidades de que el bebé tenga lo que se conoce como "doble "dominio de la acondroplasia. Esta última es en todos los casos fatal en la infancia. Los bebés que nacen con doble dominancia sufren de costillares muy pequeñas, como las anomalías cerebrales graves.

Las estatura esperada para las personas con acondroplasia es de 131 centímetros (51,5 pulgadas) para los hombres y de 123 centímetros (48,4 pulgadas) para las mujeres, sin embargo la estatura puede ser tan corta como 62,8 cm (24,7 pulgadas). Una característica distintiva de este síndrome es el gibbus toracolumbar en la infancia.

La causa de este trastorno es una mutación en el gen que codifica para el receptor 3 del factor de crecimiento de fibroblastos 3 (FGF3), localizado en el cromosoma 4. Existen dos mutaciones posibles que afectan a este gen: 
G1138A y G1138C. Ambas son puntuales, donde dos pares de bases complementarias del ADN se intercambian:
En ambas situaciones, la repercusión en la cadena aminoacídica de la proteína FGFR3 es la misma: el cambio del aminoácido glicina por una arginina. 

Dicha mutación puede darse de dos formas distintas: por herencia autosómica dominante, cuando hay antecedentes familiares de enfermedad (alrededor del 10% de los casos) y por una "mutación de novo", con padres sanos (es la causa más frecuente, hasta en el 90% de los pacientes).

La herencia de este trastorno es autosómica dominante lo que significa que, para padecerlo, basta con que se herede el gen mutado de, al menos, uno de los progenitores. Las posibilidades genotípicas y su correspondencias fenotípicas, son las siguientes:

En torno al 80% de los afectados de acondroplasia no tienen antecedentes familiares del trastorno. El motivo son mutaciones espontáneas o "de novo" (G1138A o G1138C) que afectan a la línea germinal paterna. Son, por tanto, mutaciones que ocurren en los gametos del padre (espermatozoides) durante la espermatogénesis. Estas alteraciones se dan, como su nombre indica, de forma espontánea, lo que implica un desconocimiento de su causa; sin embargo, numerosos estudios parecen constatar una relación de la mutación de novo con la edad del padre en el momento de la fecundación, de tal manera que tener más de 35-40 años parece suponer un factor de riesgo para tener un hijo acondroplásico. La frecuencia de aparición de la acondroplasia se distribuye de igual manera entre individuos de ambos sexos y de cualquier raza.

Normalmente el factor FGFR3 tiene efecto regulador en el crecimiento de los huesos. En la acondroplasia el receptor de este factor se encuentra mutado, por lo que este se encuentra constitutivamente activo lo cual lleva al acortamiento de los huesos.
Las personas con acondroplasia tienen una copia normal del gen del factor FGFR3, pero también tienen una copia mutada. Dos copias del gen mutado es fatal desde antes del nacimiento. En cuanto a la herencia genética, una persona con acondroplasia tiene el 50% de probabilidades de heredar esta enfermedad a sus hijos, lo cual significa que hay un 50% de probabilidades de que cada niño herede esta enfermedad.Por otro lado si ambos padres tienen acondroplasia, sus hijos tienen un 25% de probabilidades de morir poco tiempo después de su nacimiento, y un 50% de probabilidades de que tenga acondroplasia y un 25% de que el niño presente el fenotipo. 
No todas las personas que nacen con acondroplasia tienen padres con esta misma condición, ya que esto puede ser resultado de una nueva mutación.Esta condición no se adquiere necesariamente por herencia genética ya que existen nuevas mutaciones de los genes que pueden llevar a la acondroplasia.Existen algunos estudios que suponen que la edad avanzada de los padres puede ser un factor de riesgo 
Estudios actuales han demostrado que las nuevas mutaciones de los genes para acondroplasia son heredados exclusivamente del padre y que ocurre durante las espermatogénesis "[falta referencia]"; (sobra)pues durante la ovogénesis existe algún tipo de mecanismo regulador que impide la mutación de los genes, sin embargo las mujeres siguen siendo capaces de presentar el fenotipo y genotipo, y (por lo tanto)de transmitir el alelo mutante.
Más del 99% de (la) acondroplasia es causada por dos mutaciones diferentes del factor FGFR3. En aproximadamente el 98% de los casos, un punto mutado G a uno A dentro del nucleótido 1138 del gen factor FGFR3 causa una substitución de la glicina por la arginina (Bellus et al. 1995, Shiang et al. 1994, Rousseau et al. 1996). El otro 1% de los casos (restantes) son causados por un punto mutado G a uno C dentro del nucleótido 1138. El gen mutantefue descubierto por John Wasmuth y sus colegas en 1994.
Existen dos síndromes que tienen una base genética similar a la acondroplasia: hipocrondroplasia y la displasia tanatofórica.

Las personas con acondroplasia muestran una presencia física característica como consecuencia de la interrupción del desarrollo del cartílago en las epífisis de los huesos, haciéndose más notable en los huesos largos húmero y fémur, que son los que presentan un crecimiento más rápido. De esta forma presentan una baja estatura, que no suele sobrepasar los 144 cm en la edad adulta, con acortamiento de las extremidades y agrandamiento del cráneo, mientras que el tronco conserva su tamaño normal. A continuación se muestran los hallazgos anatomoclínicos más importantes de esta enfermedad:

La acondroplasia puede ser detectada antes del nacimiento mediante un ultrasonido, el diagnóstico consiste en una ecografía fetal por discordancia progresiva entre la longitud del fémur y el diámetro biparietal por edad. 
Una prueba de ADN puede ser realizada antes del nacimiento para detectar la homocigosidad de la mutación, una condición que como ya se mencionó resulta letal.

Un estudio radiológico del esqueleto es útil para confirmar el diagnóstico de acondroplasia. En este estudio se puede observar un cráneo grande, con un estrecho agujero occipital y una base relativamente pequeña; cuerpos vertebrales cortos y aplanados con un espacio intervertebral relativamente grande; alas iliacas pequeñas y cuadradas con una muesca ciática estrecha y un techo acetabular horizontal; huesos tubulares cortos y gruesos con ventosas metafisarias y placas de crecimiento irregulares; crecimiento excesivo del peroné; manos anchas con metacarpos y falanges cortas y costillas cortas con forma de copa en los extremos anteriores.
Si las características radiológicas no son clásicas, la búsqueda de un diagnóstico diferencial debe ser considerado.

A pesar de que se conoce la mutación del gen en el receptor del factor de crecimiento, actualmente no existe un tratamiento conocido para tratar la acondroplasia.
La hormona de crecimiento es usada por personas que no tienen acondroplasia para ayudarlas con su crecimiento, sin embargo esta no es efectiva en las personas que si la padecen. A pesar de ello si lo desean pueden someterse a una cirugía de alargamiento de miembros, que a pesar de ser un tema controvertido ha sido efectiva en algunos casos.Generalmente los mejores resultados aparecen dentro del primer y segundo año de la terapia con hormona del crecimiento. 
Después del segundo año de terapia el efecto benéfico del crecimiento del hueso comienza a disminuir. Es por esto que la terapia con hormonas del crecimiento no tiene efectos satisfactorios a largo plazo.
La terapia génica se encuentra aún en desarrollo. Una compañía de EUA, BioMarin Pharmaceutical Inc. anunció recientemente el inicio de un estudio de fase I en voluntarios sanos para BMN-111, un análogo del péptido natriurético del tipo C, para el tratamiento de la acondroplasia. Las últimas investigaciones son reguladas y controladas por la organización no lucrativa [Growing Stronger].

Debido a que la acondroplasia es una enfermedad que tiene manifestaciones similares a otras 19 enfermedades congénitas (osteogénesis imperfecta, displasia epifisaria múltiple tardía, acondrogénesis, osteopetrosis, displasia tanatofórica, etc.), las estimaciones de su prevalencia son difíciles puesto que los criterios diagnósticos son subjetivos y cambian con el tiempo. Un ejemplo de lo anterior es que estudio detallado y de larga duración en los Países Bajos mostró que la prevalencia determinada al nacer de sólo 1,3 por cada 100.000 nacidos vivos, mientras que otro estudio realizado al mismo tiempo encontró una tasa de 1 por cada 10.000.

Debido a un enanismo desproporcionado en algunas razas de perros, estos han sido clasificados como acondroplásicos. Como tal es el caso de la raza: Dachshund, Basset hound y Bulldog, esto solo por mencionar algunas de ellas.
Datos de la Asociación de genómica, en estudios en perros con extremidades cortas, han mostrado su estrecha relación con una codificación retro genética del factor de crecimiento 4 (FGF4) Por lo tanto parece poco probable que los perros y humanos desarrollen acondroplasia por las mismas razones. Sin embargo, estudios histológicos en algunos perros acondroplásicos han mostrado alteraciones en los patrones de células en los cartílagos, lo cual es muy similar a lo observado en los humanos que presentan la enfermedad.
Un tipo similar de acondroplasia se encontró en una camada de lechones daneses que tenía un fenotipo normal aparente. El enanismo fue dominante en la descendencia de la madre, y aunque los lechones nacieron fenotípicamente normales, con el tiempo se fue haciendo cada vez más evidente el padecimiento.


</doc>
<doc id="8339" url="https://es.wikipedia.org/wiki?curid=8339" title="Bilbo Bolsón">
Bilbo Bolsón

Bilbo Bolsón es un personaje ficticio del "legendarium" creado por el escritor británico J. R. R. Tolkien que protagoniza su novela "El hobbit", y también aparece en "El Señor de los Anillos". Además, Tolkien narró su historia desde una perspectiva diferente en «La búsqueda de Erebor», en "El libro de los cuentos perdidos". En La Comarca se le tenía por un hobbit normal y con buen sentido común, hasta que en el año 2941 de la Tercera Edad del Sol, Gandalf y trece enanos le visitaron. En aquel momento comenzó su "Historia de una ida y de una vuelta".

En el concepto narrativo de Tolkien, para el que sus escritos sobre la Tierra Media son auténticas traducciones del ficticio "Libro Rojo de la Frontera del Oeste", Bilbo sería el autor de "El hobbit", y el traductor de "El Silmarillion" desde las lenguas élficas a la común.

En el 2941 de la Tercera Edad trece enanos se presentaron en su casa a tomar té. Llegaron por este orden: Dwalin, Balin, Kíli y Fíli, Dori, Nori, Ori, Óin y Glóin y más tarde, llegó Gandalf acompañado de Bifur, Bofur, Bombur y Thorin. Todos los enanos y Gandalf se quedaron a cenar y a dormir allí y le contaron a Bilbo que necesitaban un saqueador para recuperar lo que era suyo de las garras de Smaug, un terrible dragón. Bilbo a pesar de pensar que estaban locos, aceptó. 

Cuando iban por el Bosque de los Trolls vieron un fuego, y eran unos trolls. Los Enanos y Bilbo se quedaron atrapados en unos sacos, pero gracias a la ayuda de Gandalf consiguieron escapar. En la guarida de los trolls Bilbo encontró una daga de los Elfos de Gondolin y la llamó Dardo. 

Cuando iban por las Montañas Nubladas decidieron parar a dormir en una cueva. Por la noche esa cueva se abrió y unos orcos de las montañas raptaron a los enanos y a Bilbo. En la cueva intentaron huir, y los enanos se iban turnando a Bilbo para llevarlo a la espalda. Cuando uno de los enanos lo llevaba, se dio un golpe en la cabeza y se cayó al suelo. Bilbo, en el suelo encontró un Anillo. Más tarde se encontraría con Gollum e hicieron un juego de acertijos, gracias a esto Bilbo logró escapar de la cueva.

Bilbo consiguió escapar de la cueva gracias a que el Anillo le daba invisibilidad. Cuando salió de la cueva estuvo un rato buscando a sus compañeros, hasta que los encontró y les contó como había escapado, pero sin mencionar el anillo. Por el camino fueron perseguidos por los Trasgos, mas lograron escapar porque las Águilas los rescataron. Allí les dieron cobijo un tiempo y los dejaron cerca del Bosque Negro. De allí, fueron a la casa de Beorn. Bilbo fue el primero en entrar, junto con Gandalf. Allí estuvieron un tiempo y luego partieron sin Gandalf. 

En el bosque, arañas capturaron a todos menos a Bilbo, gracias al Anillo. Bilbo rescató a sus compañeros a base de ir liberándolos uno a uno e ir distrayendo a las Arañas para que sus compañeros pudieran desatarse. La Compañía también fue capturada por los Elfos, pero a Bilbo no lo capturaron gracias al Anillo. Como podía hacerse invisible iba comunicándose con los enanos por la noche para contarle su plan. Un día, soltó a todos los enanos y los fue metiendo en barriles vacíos para vino que iban a Esgaroth. Allí son muy bien recibidos y en unos días parten hacia la Montaña Solitaria. 

Cuando encontraron la puerta secreta de Erebor, Bilbo hizo varias incursiones dentro de la Montaña y habló con Smaug y también encontró la Piedra del Arca. Una vez derrotado el dragón, Thorin quería todo el tesoro, y Bilbo, creyendo que arreglaría algo le entregó a Bardo la Piedra del Arca. Más tarde, tendría lugar la Batalla de los Cinco Ejércitos en la que Bilbo usó el Anillo y se escondió. 

Bilbo regresó a la Comarca el 22 de junio de 2942 de la Tercera Edad del Sol, portando consigo oro, su espada Dardo, una cota de mithril y el Anillo Único. En el 2948, Balin va junto con Gandalf a visitarlo.

En la trilogía cinematográfica de "El Señor de los Anillos", dirigida por el director neozelandés Peter Jackson (2001, 2002, 2003), fue interpretado por el actor británico Ian Holm. En "" tiene un papel breve, en "" no aparece, y en "" sale en una única escena, la de los Puertos Grises.

En la adaptación al cine de "El hobbit", también dirigida por Peter Jackson, es interpretado en su forma juvenil por Martin Freeman, mientras que Ian Holm hace un cameo del Bilbo anciano al principio de la primera película y al final de la última.

De acuerdo a la obra póstuma "Los pueblos de la Tierra Media", el «verdadero» nombre de Bilbo en la lengua común de la Tierra Media es «"Bilba Labingi"», que se representa en las novelas de Tolkien por «"Bilbo Baggins"» en inglés. Habitualmente en las traducciones se cambia su apellido por uno de significado equivalente:

Bilbo era tío segundo de Frodo por parte de madre y tío tercero por parte de padre.





</doc>
<doc id="8340" url="https://es.wikipedia.org/wiki?curid=8340" title="Frodo Bolsón">
Frodo Bolsón

Frodo Bolsón (originalmente en inglés, "Frodo Baggins", y de acuerdo con la obra póstuma "Los pueblos de la Tierra Media", este nombre es una representación del oestron "Maura Labingi") es un personaje ficticio que pertenece al "legendarium" del escritor J. R. R. Tolkien, uno de los principales de la novela "El Señor de los Anillos". Es un hobbit, hijo de Drogo Bolsón y Prímula Brandigamo, nacido el 22 de septiembre de 2968 de la Tercera Edad del Sol. Llamado también "Portador del Anillo", Frodo fue el encargado de llevar el Anillo Único hasta el Monte del Destino para destruirlo. 

En la trilogía cinematográfica de "El Señor de los Anillos" de Peter Jackson (2001, 2002, 2003), Frodo es interpretado por el actor estadounidense Elijah Wood.

Como otros muchos nombres de "El Señor de los Anillos", la palabra proviene de un idioma diferente del inglés: "Frodo", en nórdico antiguo, significa "sabio".

Posee los principales rasgos que diferencian a los hobbits de otras especies: los pies muy peludos, mejillas rojas, rechonchez… Aunque tiene el pelo de color castaño como la mayoría de los hobbits, Frodo es de tez pálida debido a su ascendencia de la rama de los albos. Según las palabras de Gandalf en "La Comunidad del Anillo", Frodo es más alto que algunos y más pálido que la mayoría de los hobbits, tiene un hoyuelo en la barbilla, ojos brillantes y la cabeza erguida. Y según la opinión de Faramir, Frodo tenía un cierto aire élfico. Tolkien remarca bastante ese último aspecto durante toda la historia en varias ocasiones, donde se hace mención de su modo de hablar y sus modales, dándole una imagen no muy común entre los hobbits. Frodo era diferente, un amigo de los elfos, como solían llamarlo. Además conocía su lenguaje y podía traducir las lenguas antiguas. Su indumentaria no se especifica con detalle, si bien a partir de su salida de Rivendel llevó consigo la cota de malla de mithril y la espada Dardo que le regaló su tío Bilbo, y tras su paso por Lothlórien se añadió el manto élfico regalado por Galadriel a su vestimenta. Únicamente se separó de estas prendas al ser capturado en Cirith Ungol, momento en que las perdió y tuvo que vestirse con ropas de orco.

A pesar de ser gran amante de La Comarca, el lugar donde vivía, siempre se sintió atraído por los viajes y las aventuras, motivado probablemente por las historias de su tío Bilbo, al que él quería mucho. No obstante, su misión a Mordor le resultó tan dura que posteriormente desapareció todo deseo de aventuras, queriendo únicamente descanso y tranquilidad. Se lo describe como tímido pero decidido, y muy inteligente. Aborrece la violencia, y es especialmente característico de él su compasión y sensibilidad, demostradas por ejemplo con el trato que dispensó siempre a Gollum. Su personalidad quedó afectada poco a poco por el hecho de portar el Anillo Único, sobre todo en Mordor, donde esto unido al hambre y a la fatiga extrema que sufrió cambiaron su constitución física haciéndole adelgazar mucho y también volviéndole depresivo y dependiente del anillo. Estos rasgos desaparecieron tras la destrucción del anillo, sin embargo quedó en él cierta influencia del anillo, por lo que para curarse por completo terminó viajando a las Tierras Imperecederas para acabar allí sus días.

El Anillo había causado en él heridas que no afectaban su cuerpo físico, por lo tanto las consecuencias de dicha afección no eran del todo claras, ni siquiera para Gandalf. Se suponía que iría desapareciendo, como sucedía con todo aquel que usaba el anillo, pero que esta desaparición acabaría siendo de modo permanente, debiendo vivir para siempre en el mundo de las sombras, tal y como había sucedido con los jinetes negros. 
Su viaje a Tol Eressea, en el continente de Aman, tenía como fin sanar su espíritu y librarlo de tan terrible final.
Frodo partió hacia La Isla solitaria para ser restaurado.

Frodo pasó su infancia en Casa Brandi, en Los Gamos, viviendo con la familia de su madre. Los padres de Frodo murieron cuando él tenía doce años, en un accidente mientras navegaban en un bote. En el año 2989 T. E., Frodo fue adoptado por su tío Bilbo Bolsón, y se marchó a vivir con él a su casa llamada Bolsón Cerrado, en Hobbiton. Muy pronto Bilbo realizó los trámites necesarios para convertir a Frodo en su heredero, lo cual causa malestar en los Sacovilla-Bolsón, quienes esperaban ser los dueños de Bolsón Cerrado cuando Bilbo muriera. Primula, la madre de Frodo, era prima hermana de Bilbo y Drogo, el padre de Frodo, era primo segundo de éste, así que Frodo es, en realidad, primo segundo por parte de madre y primo tercero por parte de padre del señor Bilbo.

El 22 de septiembre del año 3001 T. E., Frodo cumple la mayoría de edad (que entre los hobbits es de 33 años), y pasa a posesión oficial de la herencia de su tío Bilbo. Ese mismo día, Bilbo abandona la Comarca y viaja a Rivendel a pasar sus días allí; ya que, según cuenta el mismo libro, quería salir de aquel lugar y tomarse unas vacaciones, pasar su tiempo entre los elfos. Entre la herencia que Bilbo dejó a Frodo se encuentra un extraño anillo. Gandalf le da una explicación sobre el poder de esos objetos, y le pide a Frodo que evite al máximo su uso. A partir de 3004 T. E., Gandalf empieza a hacer visitas periódicas a Frodo para tener el anillo vigilado. En 3018 T. E., Gandalf realiza su última visita a Frodo. Para ese entonces, Gandalf ya tiene la certeza de que el anillo de Frodo es el Anillo Único, y le advierte al hobbit sobre el peligro que corre al tenerlo, ya que su dueño Sauron desea recuperarlo. Para evitar que el Único caiga en poder del mal, Frodo emprende su viaje con él, en primera instancia hacia Rivendel.

El 23 de septiembre de 3018, con 50 años (joven aún para un hobbit) Frodo abandona Bolsón Cerrado junto con Pippin y Sam, dirigiéndose a Los Gamos perseguido por los Nazgûl. Llegan a los campos del viejo Maggot, ofreciéndoles llevarlos lo más cerca posible, se encuentran con Merry y llegan a Los Gamos. El traslado es disimulado como una mudanza, pero al llegar allí se ponen en marcha en secreto hacia Rivendel, internándose en el Bosque Viejo. Allí, el grupo de hobbits conoce a Tom Bombadil, un sujeto que adora cantar y bailar, y que les brinda su ayuda para salir del bosque, además de acogerlos en su casa y más tarde salvarlos de los "Tumularios Negros".

El 29 de septiembre llegan a Bree. En un hostal, llamado el Póney Pisador, donde pernoctan esa noche los esperaba Aragorn, quien en ese momento se presentó como Trancos. Gandalf le dejó una nota al posadero para que se la entregase sin más demora, donde les informaba que Aragorn los acompañaría hasta Rivendel. El grupo abandona Bree al día siguiente, tras haber sufrido un ataque fallido de los Nazgûl. Durante el viaje, el grupo hace un descanso en la Cima de los Vientos, donde son atacados de nuevo y Frodo es herido por un arma envenenada, empeorando su estado de salud durante el resto del viaje a Rivendel y estando a punto de morir. Una vez en la ciudad, Elrond le opera y le salva la vida.La herida de aquel ataque, hecho con un arma fabricada en Morgul, no le curaría nunca del todo, doliéndole en cada aniversario del momento en que fue herido y cada vez que un Nazgûl se acercaba a él.

En Rivendel, Elrond le dice a Gandalf que el anillo no se puede quedar allí, y por consecuente, se realiza el Concilio de Elrond, con la participación de representantes de todas las razas. A solicitud suya se le encomienda la misión de llevar el Anillo Único al Monte del Destino y arrojarlo ahí para que sea destruido. Para ayudarlo en esta tarea, se establece La Comunidad del Anillo de 9 miembros. El número de miembros elegidos se decide para igualar a los Nazgûl:

La Comunidad inicia el viaje a Mordor, visitando parajes como las minas de Moria o Lothlórien, y perdiendo en el trayecto de dicho viaje a dos miembros del grupo: Gandalf (que no murió del todo), en una lucha con un Balrog y Boromir, en una lucha encarnizada contra uruk-hai de Isengard; quien poco antes de morir intentó quitarle el Anillo a Frodo seducido por su poder. Esto hizo que el hobbit tomara la decisión de viajar él sólo hasta el Monte del Destino para que el poder del Anillo no corrompiera a ninguno de sus compañeros, aunque finalmente Sam le acompañó.
Ambos hobbits se perdieron en las colinas de Emyn Muil, y durante su estancia allí se encontraron con Gollum, que intentaba recuperar el Anillo. Al contrario que Sam, Frodo se apiadó de él y le convenció para que les guiara hasta la Puerta Negra por la que se entraba a Mordor. Sin embargo, al llegar allí comprobaron que estaba demasiado bien vigilada para poder entrar por allí. Gollum les habló entonces de otra entrada, que pasaba junto a Minas Morgul, y finalmente decidieron intentarlo por allí. Durante el viaje, conocieron al hermano de Boromir, Faramir, quien les acogió entre sus hombres (soldados de Gondor) durante unos días. Se enteró de la existencia del Anillo y del papel de Frodo, pero le permitió continuar su viaje. Cuando Frodo está con Faramir, se entera de que Boromir murió en la Disolución de la Comunidad del Anillo.

Gollum cumplió su palabra y les guio hacia el paso de Cirith Ungol, pero allí les traicionó dejándolos a merced de Ella-Laraña, monstruo cuya presencia los hobbits no conocían pero Gollum sí. La araña gigante picó a Frodo, dejándolo inconsciente temporalmente, herida que también seria curada, pero también dejó sus secuelas. Sam hirió al monstruo haciéndolo retroceder (no se sabe si lo mató definitivamente, pero el libro hace entender que no) dio a Frodo por muerto y decidió ocultarse de los orcos que se acercaban desde la torre de guardia. Estos llevaron a Frodo a la torre, y Sam, una vez se enteró de que Frodo estaba vivo oyendo a un orco decirlo, se las ingenió para entrar en la torre. Aprovechando un enfrentamiento entre los orcos que querían quedarse la cota de malla de mithril de Frodo, éste (ya recuperado) y Sam huyen, y se adentran en Mordor en rumbo al Monte del Destino.

Durante el viaje, la carga e influencia del Anillo, la falta de alimentos y la atmósfera opresiva reinante en esas tierras afectaron a Frodo profundamente, sumiéndolo en un estado depresivo y dejándolo sin apenas fuerzas, teniendo que ser constantemente ayudado por Sam para seguir adelante, pero sin querer dejar que éste llevara el Anillo, aunque cada vez parecía pesarle más. Cuando por fin llegaron al Monte del Destino, fueron atacados por Gollum (que los había seguido desde Cirith Ungol), pero rechazaron su ataque, llegando por fin a una grieta por la que pudieron acceder a la lava del volcán. Sin embargo, una vez allí Frodo reclamó el Anillo como suyo y se negó a destruirlo. Gollum atacó nuevamente a Frodo, arrancándole de un mordisco el dedo en el que llevaba puesto el Anillo pero cayendo después al fuego del volcán y derritiéndose junto al Anillo.

Destruido el Anillo, Sauron cayó definitivamente y sus tropas, que en ese momento atacaban al resto de la Compañía del Anillo y al ejército de Gondor frente a la Puerta Negra, huyeron en desbandada. Gandalf fue transportado por las águilas al Monte del Destino, que había entrado en erupción al ser destruido el Anillo, y rescató a Frodo y a Sam. Ambos fueron llevados a un hospital de campaña, donde se recuperaron y más tarde festejaron la victoria definitiva frente a Sauron. Frodo y los otros tres hobbits volvieron más tarde a la Comarca, encontrándola destrozada en manos de Saruman. Merry y Pippin encabezaron el ejército de hobbits que lo expulsó definitivamente, restaurando la paz en la Comarca. De este modo, Frodo pudo por fin descansar en Bolsón Cerrado, dedicándose a escribir un libro con todas sus experiencias.

Según la novela, a Frodo se le concedió como gracia especial, por haber sido «Portador del Anillo», poder ir a las Tierras Imperecederas. De esta manera, abandonó la Tierra Media para siempre, en el mismo barco en que lo hicieron Elrond, Galadriel, Bilbo, Gandalf, Celeborn y Círdan. En sus "Cartas", Tolkien explica repetidamente que la estancia de Frodo en Aman fue temporal y que murió tras un tiempo de curación y descanso:

Algunos estudiosos piensan que Frodo podría ser un tipo de «espejo» del propio J. R. R. Tolkien: Frodo quedó traumatizado tras volver de cumplir su misión. Tolkien estuvo en las trincheras durante la Primera Guerra Mundial, quedando horrorizado durante su experiencia en ellas, entre tantos muertos, su experiencia en la Batalla de Somme le traumatizó, como a Frodo el estar en Mordor. Tolkien, al regresar de Francia, perdió dos de sus mejores amigos, Frodo perdió también a dos compañeros.

A Frodo también se le ha comparado con Cristo, pero esto ha sido rotundamente negado por estudiosos, ya que Frodo no cumplió con su misión, sino que sucumbió ante la tentación, y, si no hubiese sido por la trágica muerte de Gollum, Sauron habría vencido.



</doc>
<doc id="8342" url="https://es.wikipedia.org/wiki?curid=8342" title="Bilbo">
Bilbo

El término Bilbo puede hacer referencia a:

</doc>
<doc id="8346" url="https://es.wikipedia.org/wiki?curid=8346" title="Literatura en catalán">
Literatura en catalán

La literatura en catalán es la escrita en la lengua catalana, llamada valenciano en la Comunidad Valenciana.

Aunque ya se encuentran a lo largo del siglo XI algunos documentos de carácter feudal en los que se utiliza la lengua romance catalana, en su totalidad o mezclada con un latín deficiente, como los "Greuges de Guitard Isarn, senyor de Caboet", el primer testimonio de uso literario de la lengua son las Homilías de Organyà de finales del siglo XI o principios del XII, que consisten en fragmentos de un sermonario destinado a la predicación del Evangelio.

La literatura "en catalán" vio sus primeras grandes obras antes en prosa que en verso. Esto se debió a que los poetas cultos, hasta el siglo XV, preferían utilizar el provenzal literario en vez de la variedad autóctona, como en el caso de Alfonso II, llamado "el Trovador", Cerverí de Girona o Guillem de Berguedan y otros de obra menos conocida, como Guillem de Cabestany, Guerau de Cabrera, Thomás Périz de Fozes, Ponç de la Guàrdia, Gilabert de Próixita, Guillem de Masdovelles o Raimon Vidal de Bezaudun. Todos estos, y muchos más, aparecen cuidadosamente compilados, traducidos, estudiados y anotados en los tres volúmenes antológicos que les dedicó el gran erudito y medievalista Martín de Riquer ("Los trovadores", Barcelona: Planeta, 1975; reimpresos posteriormente varias veces en Barcelona: Editorial Ariel).

Cabe destacar, sin embargo, que existían pocas diferencias entre la lengua catalana y las diversas variedades occitanas (provenzal, lenguadociano, lemosín, gascón...), muchas menos en la Edad Media, ya que durante esa época y en siglos posteriores se consideraban la misma lengua. Sin embargo, en el caso de los poetas catalanes, la variedad de provenzal o occitano utilizado era una "koiné" literaria, o lengua común procedente de la unificación o mezcla de diversas variedades dialectales, de tipo áulico, cultivada también en las antiguas cortes y feudos-estado de Occitania y parte del norte de Italia. 

Las primeras manifestaciones en poesía culta en Europa en una lengua moderna fueron escritas por los trovadores, quienes seguían unas normas estrictas y codificadas para elaborar su poesía, utilizando los códigos de la literatura trovadoresca como el amor cortés. Los trovadores y poetas catalanes participaron de esta cultura trovadoresca. La gran proximidad política, lingüística y cultural entre los condados catalanes y los antiguos feudos de Occitania (Aquitania, Tolosa, Provenza...) hizo posible compartir una variedad literaria común trovadoresca, de tipo cortesano, que se mantuvo en la Corona de Aragón hasta el siglo XV para escribir poesía hasta la aparición del valenciano Ausiàs March, el primer poeta que abandona la influencia de la koiné literaria occitana y los principales elementos de este tipo de poesía. Por este motivo, en la historia de la literatura catalana medieval se incluye toda la nómina de trovadores conocidos, ya sean propiamente catalanes u occitanos. La poesía en la variedad catalana estrictamente territorial era utilizada por los juglares en sus espectáculos ante públicos populares, sin que se conserve, a la actualidad, ningún ejemplo. En el siglo XII aparecen también las primeras traducciones de textos jurídicos, como la de "Liber iudiciorum" ("Llibre dels Judicis" o "Llibre Jutge").

La lengua catalana fue utilizada también en la narración de las gestas y crónicas de los soberanos. Aunque la primera versión de la "Gesta comitum barchinonensium" fue escrita en latín a finales del siglo XII, la siguiente edición revisada de la obra, conocida como "versión intermedia", fue escrita en catalán alrededor de 1268 o 1269 y publicada en Barcelona.

Existe consenso filológico en que la primera versión del "Llibre dels feits" de Jaime I fue escrita en catalán y en su mayor parte poco antes de la muerte del rey, es decir, en una fecha anterior a 1276. El "Llibre del rei en Pere d'Aragó e dels seus antecessors passats", más conocido como "Crónica de Bernat Desclot", escrito alrededor de 1290, narra diversos hechos notables de los reinados de Jaime I, Pedro el Grande, Alfonso el Liberal y Jaime II. La "Crónica de Ramon Muntaner" fue escrita por este autor entre 1325 y 1332 y destaca por su vívida descripción de las expediciones de los almogávares. Finalmente, la "Crónica de Pedro el Ceremonioso" fue escrita por orden de este rey para glorificar sus acciones y las de su padre Alfonso el Benigno.

El mallorquín Ramon Llull (siglo XIII), figura capital de la literatura catalana, es considerado el padre de la prosa en lengua catalana, también escribió poesía de tipo cortesano en koiné occitana, aunque su obra fue destruida por el propio autor al considerarla banal y superficial. Entre sus obras cabe destacar su novela "Blanquerna", un libro místico: "Llibre d'amic e amat", el "Libro del gentil y los tres sabios", el "Libro del Orden de Caballería", poesías como el "Cant de Ramon" ("Canto de Raimundo") o "Lo desconhort" ("El desconsuelo") o su autobiografía, "Vida coetània", "Les cents noms de Déu" ('Los cien nombres de Dios', 1289), el "Libro de los mil proverbios" y "Félix o Libro de las maravillas" (que incluye el "Libro de las bestias"), entre muchas obras de carácter ante todo científico, filosófico o teológico.

Destacan autores influidos ya en cierto modo por el humanismo, como Pere March, Jaume Gassull, Bernat Fenollar, Bernat Hug de Rocabertí o el poeta misógino Pere Torroella (algunos de ellos pertenecientes a la generación del Siglo de Oro de la literatura valenciana) entre muchos otros. Paralelamente destacaron autores religiosos como Francesc Eiximenis, sin duda uno de los autores en valenciano más leídos en su época; san Vicente Ferrer y el pícaro fraile franciscano converso al Islam Anselm Turmeda, que escribió igualmente en árabe.

En el mismo filo entre los siglos XIV y XV surgen poetas que escribieron en la "koiné" literaria occitana o en un catalán occitanizado, a menudo siguiendo las pautas trovadorescas, fueron Jordi de Sant Jordi y Andreu Febrer, traductor este último de la "Divina comedia" de Dante Alighieri al catalán. El notario y prehumanista Bernat Metge escribió en 1381 el "Llibre de Fortuna e Prudència" («Libro de Fortuna y Prudencia»), poema alegórico en el que se debate la cuestión de la Providencia divina al más puro estilo de la tradición medieval, fundándose en el precedente inevitable del "De consolatione philosophiae" del romano Boecio. También realizó la traducción del relato de "Valter y Griselda", última de las "novelle" del "Decamerón" de Boccaccio, pero no la hizo a partir del original italiano, sino de la traducción en latín de Petrarca (el "Griseldis"). La importancia de la traducción de Metge se debe, además de a su elegante prosa, a la carta introductoria que acompaña al relato, pues supone la primera muestra de admiración por Petrarca que se conoce en España. Su obra maestra fue "Lo somni" («El sueño»), redactado en 1399, donde se le aparece Juan I en el Purgatorio. Lo escribió en la cárcel, tras caer en desgracia y ser encarcelado por la nueva reina María de Luna junto al resto de colaboradores del difunto monarca. El valenciano Ausiàs March, "hombre de asaz elevado espíritu" según la "Carta e proemio al Condestable don Pedro de Portugal" del Marqués de Santillana, es considerado el poeta del apogeo de la literatura valenciana del (siglo XV); abandona ya los tópicos y elementos propios de los trovadores y crea su propio sistema de imágenes y conceptos amorosos, en un estilo austero y grave de feroz introspección que nada debe a las florituras italianas. Sus obras principales se reparten en tres grupos, los "Cantos de amor", los "Cantos de muerte" y el "Canto espiritual".

En el siglo XV se escribe también "Curial e Güelfa", un extraño y original híbrido de libro de caballerías y novela sentimental muy verosímil, escrito probablemente entre 1435 y 1462 por un autor anónimo que conocía muy bien la literatura antigua y moderna. De esta época es también el llamado siglo de oro valenciano, con una producción muy destacada de escritores en poesía y prosa que culmina con "Tirante el Blanco" de Joanot Martorell (publicada en 1490). "L'Espill" o "Llibre de les dones" es una novela en verso de carácter misógino. Fue escrita por el valenciano Jaume Roig entre 1455 y 1462 y se compone de más de dieciséis mil versos tetrasílabos. El "Espill" consta de un prefacio y de cuatro libros que, a su vez, se dividen en cuatro partes. En el prefacio, el narrador hace una declaración de principios, éticos y estilísticos. En el libro primero, "De su juventud", conocemos al protagonista, que habla siempre en primera persona y explica cómo fue su infancia. Huérfano de padre y expulsado de su casa por su madre, se ve obligado a ganarse la vida en Valencia. Poco después emprende un viaje aventurero, primero por Cataluña y luego por Francia. Lucha en la guerra de los Cien Años con las tropas francesas y en París, cuando ya es rico gracias a los botines obtenidos, interviene en la vida caballeresca. El segundo libro, "De cuando estuvo casado", narra los sucesivos fracasos matrimoniales del protagonista, primero con una doncella que al final resultó que no lo era, después con una viuda, en tercer lugar con una novicia y, finalmente, explica el frustrado intento de casarse con una beguina. En el tercer libro, "De la lección de Salomón", el protagonista, desesperado por no poder encontrar una esposa adecuada, pretende casarse con una pariente suya. Entonces se le aparece en sueños Salomón, el sabio bíblico por antonomasia, que le suelta una larga invectiva contra las mujeres que corrobora con ejemplos bíblicos las malas experiencias relatadas en los dos libros anteriores. Y el cuarto libro se titula "De enviudar". Cierran la escuela valenciana Joan Roís de Corella (1433/43-1497), que luce un humanismo erudito, y sor Isabel de Villena (1430-1490), pluma tierna e intimista, que contrarresta la misoginia de "L'Espill" en su "Vita Christi".

Históricamente se ha aceptado que tras una época de esplendor que culmina con "Tirant lo Blanc", el catalán como lengua literaria entra en una larga fase de decadencia desde el siglo XVI a 1833. Pero actualmente, estudios recientes están revalorizando las obras de los autores renacentistas (Cristòfor Despuig, Joan Timoneda, Pere Serafí), barrocos (Francesc Vicenç Garcia, Francesc Fontanella, Josep Romaguera) y neoclásicos (Joan Ramis, Francesc Mulet), de modo que se va hacia una revisión del concepto Decadència.

Como hitos de este período para la lengua valenciana, se pueden contar la primera impresión de una traducción de la Biblia en lengua no latina, la Biblia Valenciana, impresa por encargo de Bonifacio Ferrer en 1478, o la redacción de una Teología en 1440 por el valenciano Francesc Pertusa, la cual es la única obra escrita sobre esta ciencia en una lengua diferente del latín de la Edad Media.

Durante el siglo XV, tuvo lugar el Compromiso de Caspe (1412) donde se eligió como rey a Fernando de Antequera (1410-1416), lo que introdujo en Cataluña la dinastía de los Trastámara. La lengua de la corte pasó a ser la castellana. El año 1479 se produjo la unión dinástica de Aragón y Castilla que, aunque jurídicamente no cambió nada, sí que tuvo consecuencias negativas para el uso del catalán:

La literatura en catalán, sin romper con la tradición medieval, recuperó algunos de los cánones estéticos y de los modelos formales del clasicismo. Pero la tradición por el estudio de las lenguas clásicas, característica del humanismo, no impidió el desarrollo de la literatura en lenguas vulgares. Mientras la minoría aristocrática vacilaba entre el uso del español y el catalán, la mayoría popular continuó rehaciendo y ampliando en la lengua propia la tradición que se había elaborado en el transcurso de los siglos. 

La prosa más valiosa del periodo son "Los col·loquis de la insigne ciutat de Tortosa" (1557) de Cristòfor Despuig, tanto por el uso del diálogo, una forma literaria clásica, como por el espíritu crítico de su autor, en una prosa noble con algunos ecos erasmistas. Dentro también de la narrativa histórica, hay que señalar las crónicas de Pere Miquel Carbonell, Pere Antoni Beuter y la novela alegórica representada por "L'espill de la vida religiosa", obra anónima publicada en 1515, atribuida por algunos a Miquel Comalada, con influencias lulianas y reformistas que estaban alcanzando una verdadera proyección europea.

En el campo de la literatura de entretenimiento, aparecen las "novelle" y las "facecias" como las de Jordi Centelles y Joan Timoneda. Durante esta época funcionaba en Valencia un teatro de intención realista y satírica que da muestras tan espléndidas como "La vesita" de Joan Ferrandis d'Herèdia.

El mejor poeta en catalán del momento fue Pere Serafí, quien alternó el idealismo amoroso de inspiración petrarquista o ausiasmarquista con la glosa de refranes y canciones populares. Otros poetas, como Andreu Martí Pineda y Valeri Fuster, insistieron con una cierta originalidad en los modelos costumbristas valencianos de finales del siglo XV. Los poemas de Joan Pujol, ya en la segunda mitad del siglo XVI, y los actos sacramentales de Joan Timoneda reflejan el cambio de la contrarreforma que tenía que culminar con el barroco. Con la contrarreforma desaparece el espíritu de crítica y de búsqueda para propugnar una visión más rígida y ascética de la vida.

Las primeras manifestaciones propiamente barrocas (autores anteriores como Joan Timoneda o Joan Pujol pueden considerarse como unos síntomas literarios iniciales de la contrarreforma) no se produjeron hasta los inicios del siglo XVII y se prolongaron durante todo el siglo XVIII ya con elementos de estética rococó.

En este periodo se recibieron claras influencias del gran barroco castellano, con autores como Garcilaso de la Vega, Góngora, Quevedo, Calderón de la Barca, Baltasar Gracián, etc., que actuaban sin mucha relación entre unos y otros.

Una figura crucial del barroco en catalán fue el poeta y comediógrafo Francesc Vicent Garcia i Torres, quien fue el único que consiguió formar una escuela que le imitó en aquellos aspectos más secundarios y que se prolongó hasta bien entrada en siglo XIX. Francesc Fontanella y Josep Romaguera aportarían también destacadas contribuciones durante el barroco.

El momento culminante de esta corriente puede situarse durante la Guerra de los Segadores, cuando aparecieron intentos ya conscientes de novar y revitalizar la cultura en catalán y que fracasó con la derrota de las tropas catalanas. Hay que destacar la obra poética y dramática de Francesc Fontanella. Otras figuras del barroco en catalán fueron Pere Jacint Morlà y Josep Blanch, también de mediados del siglo XVII. Josep Romaguera, entre los siglos XVII y XVIII, mantenía con relativa eficacia estos propósitos. Agustí Eura Joan de Boixadors, Guillem Roca i Segui y Francesc Tagell, de la primera mitad del siglo XVIII, insistieron con desigual fortuna.

Desde finales del siglo XVIII, la filosofía crítica y la erudición lingüística e histórica de la Ilustración habían renovado todo el concepto de cultura. Nace una nueva mentalidad que consideraba que la obra había de ser útil o no ser. Es una época de inicios de actividades científicas y de un desprecio razonado de la lírica gratuita. La máxima consideración la obtenían los textos de moral o pedagogía.

En Cataluña estuvo representada por un grupo brillante, siguiendo la iniciación metodológica del valenciano Jacint Segura retomada por el impulso del obispo Ascensi Sales. Hay que destacar también la novela del alicantino Pedro Montengón, "Eusebio", escrita en castellano. También hay que destacar un grupo formado por diversos traductores y escritores como Francesc Mulet, Antoni Febrer así como Joan Ramis y sus piezas de teatro neoclásico, entre las cuales destaca "Lucrècia".

Puede afirmarse que la literatura en catalán en este periodo fue prácticamente inexistente por lo que se refiere a la literatura "culta", ya que la mayoría de los ilustrados prefirieron en su expresión lenguas más internacionales, como el propio gran erudito Gregorio Mayáns y Siscar. Únicamente han perdurado el conjunto de los géneros populares como portavoces más cualificados de la época. El terreno era el teatro jocoso y satírico (entremeses) con textos, la mayoría inéditos, redactados por no profesionales; y también epistolarios, narraciones privadas y dietarios que reflejan la mentalidad de la época, de que son muestra los cincuenta y dos volúmenes del "Calaix de sastre" del Barón de Maldà, situado en el lado opuesto a la ilustración al preludiar la literatura costumbrista que tanto éxito alcanzará en el romanticismo. 

La poesía popular, espontánea y multiforme, confirió a la literatura del siglo XVIII unos títulos con una categoría superior a los alcanzados por los intentos cultos. Esta literatura popular sólo compartía con la ilustración la época y el hecho de desarrollarse bajos los signos de "naturaleza" y "libertad". Pero sobre todo estaba asociada con la lengua del pueblo. 

A principios del siglo XIX, y gracias al conde d'Aiamans y al autor anónimo de "Lo Temple de la Glòria", ya se incorporan algunos elementos románticos.

El primer romanticismo, planteado en temas inequívocamente catalanes, fue escrito básicamente en lengua castellana. Hacia finales de este periodo, sin embargo, ya se empezó a tomar consciencia de la contradicción que existía entre el contenido y el público al que se dirigía y la lengua en que se realizaba. Así, por ejemplo, Pere Mata escribió un largo poema en catalán "El Vapor" (1836) y Joaquim Rubió i Ors empezó a publicar sus poemas en el Diario de Barcelona (1839).

En esta época se forma una intelectualidad burguesa de inspiración liberal, joven y combativa y, en muchos casos, revolucionaria. La evolución de los hechos, con escritores exiliados o recluidos en la clandestinidad, no permitió su eclosión. Algunos, como Manuel Milá y Fontanals y su gripo, renegaron de los inicios liberales. Otros, como Antoni Ribot i Fontserè o Pere Mata, emigraron a Madrid.

Entre 1844 y 1870, el romanticismo conservador, más o menos teñido de elementos populares o clásicos, monopolizó las letras en catalán. Algunos, como Víctor Balaguer, insistieron en una literatura que fuera también un instrumento de progreso.

La Renaixença es el nombre dado al gran movimiento restaurador de la lengua, literatura y de la cultura catalana que se inició en el Principado en la primera mitad del siglo XIX. Coincidió más o menos con la segunda parte del estallido del Romanticismo en Europa. Aunque cada tendencia siguió un camino propio, se produjo una integración por lo que respecta al uso de la lengua y a los ideales políticos.
Suele situarse en el periodo comprendido entre la aparición en 1833, en el periódico "El vapor", de "La Pàtria" de Bonaventura Carles Aribau y de la presentación en los juegos florales de 1877 de "L'Atlantida" de Jacinto Verdaguer.

En 1835 se restauró la Universidad de Barcelona y en 1839 fue publicado el primer libro de poesía en catalán, "Llàgrimes de viudesa", de Miquel Anton Martí. En las revistas y periódicos de Barcelona, iban apareciendo composiciones catalanas, aunque la primera revista escrita íntegramente en catalán, "Lo Vertader Català", no apareció hasta 1843.

En sus inicios, la consciencia de la "Renaixença" fue potenciada por la recuperación de la propia historia, por el poder creciente de la burguesía liberal (especialmente la de Barcelona), por ser decididamente liberal y romántica, por utilizar con una relativa normalidad la propia lengua y, además, con una producción literaria seria y perseverante.. Entre sus miembros más destacados hay que señalar a Marian Aguiló, Joan Cortada, Manuel Milá y Fontanals, Pau Piferrer y Joaquín Rubió i Ors.

En la segunda mitad del siglo XIX, el enderezamiento es cada vez más claro, ayudado por algunas instituciones como la Real Academia de las Buenas Letras de Barcelona, la Universidad de Barcelona o algunos sectores de la iglesia (que representan Jaume Collell i Bancells y Josep Torras i Bages).

Por lo que se refiere a la lengua, se promovieron los instrumentos culturales más urgentes y básicos como gramáticas y diccionarios. Se crearon unos mitos políticos propios (Jaime I y Felipe V) y literarios (los trovadores) y su proyección se extendió más allá de la erudición y la lírica en un intento de catalanizar otros campos como la filosofía, la ciencia, el arte o el derecho.

En 1859 se fundaron los Juegos Florales que significaron una gran proyección popular. Esta fundación era estéticamente e ideológicamente conservadora, pero social e idiomáticamente ayudó a difundir la cultura dentro de los medios populares y, especialmente, rurales. Contaron con el prestigio de un reconocimiento público notable y, si bien empezaron en Barcelona, fueron reproducidos en muchos otros sitios y tomaron el carácter de órgano supremo de la Renaixença. Ayudó a hacer surgir un número considerable de autores, a menudo procedentes de la pequeña burguesía urbana y dedicados casi exclusivamente a la poesía, entre los que destacan Antoni de Bofarull y Víctor Balaguer.

Fruto de un carácter más bien conservador, el movimiento, por otra parte, casi no afectó a la literatura popular que se había ido generando en catalán casi sin interrupción en todo el periodo de la decadència. Más bien fue visto con recelo por parte de sus autores (Abdó Terrades, Anselmo Clavé o Frederic Soler).
En 1862 se instauró el primer premio dedicado a la narrativa dentro de los Juegos Florales, resultando ganadora la novela "L'orfeneta de Menargues" de Antoni de Bofarull. El teatro no formó parte de los Juegos hasta 1865 con el estreno del primer drama en catalán del siglo "Tal faràs, tal trobaràs" de Vidal i Valenciano. La poesía culta, en cambio, fue editada en catalán desde 1839, pero su proceso se coronó con el poema épico "L'Atlàntida" de Jacinto Verdaguer, publicado en 1878. Autores destacados de este siglo fueron: el poeta y dramaturgo Àngel Guimerà, los poetas Teodor Llorente o Manuel Pons i Gallarza o el novelista Narcís Oller.

A finales del siglo XIX destaca el modernismo, con autores como Joan Maragall, Joaquim Ruyra o Víctor Català. 
Destacó después el llamado Noucentisme o Novecentismo catalán, y en particular la llamada Escuela Mallorquina. Miquel Costa i Llobera y Joan Alcover, que también bebieron de la poesía novecentista, serían sus autores más destacados. La novela, que todavía arrastraba los déficits de la falta de tradición, recibió el impulso de autores como Raimon Casellas, Prudenci Bertrana, Joaquim Ruyra y Caterina Albert, más conocida por su seudónimo Víctor Català. Ella construyó una narrativa que fusionaba el simbolismo y costumbrismo, con protagonistas que luchaban por transformar una realidad adversa. Una de sus obras más importantes fue "Solitud" ["Soledad"], una de las novelas más representativas del Modernismo y entre las más importantes de la narrativa del siglo XX español. Santiago Rusiñol, pintor impresionista, tuvo también un gran papel como escritor introduciendo ideas modernistas en todos los ámbitos, especialmente en el teatro ("L’auca del senyor Esteve" ("Las aleluyas del señor Esteve"). Sus novelas fueron también muy celebradas por su sano humor, no exento de crítica social.

Durante el siglo XX se consolida el catalán como lengua literaria, a pesar de las condiciones adversas durante las dictaduras de Primo de Rivera y Franco.

La abolición del Estatuto de Autonomía de Cataluña en 1938 significó el inicio de un largo proceso de españolización que intentó poner fin a toda manifestación explícita de la cultura catalana. De esta manera, ante la imposibilidad de manifestarse en la propia lengua, los intelectuales catalanes optaron por claudicar ideológicamente y lingüisticamente, continuaron de forma clandestina o bien terminaron exiliándose. Los intelectuales que decidieron claudicar ideológicamente y lingüísticamente tuvieron que decantarse por una literatura de expresión española, con manifestaciones derivadas de la ideología dominante, o bien intentando adaptar ideas propias de la cultura catalana de antes de la guerra (este es el caso de Juan Ramón Masoliver, vanguardista durante los años 20, que creó la colección "Poesía en la Mano ").

Sin embargo, quedaron en Cataluña otros intelectuales que se negaron a romper definitivamente con la lengua y la cultura autóctonas. Fue el caso de aquellos que optaron por el exilio interior, es decir, para actuar en la clandestinidad. Se organizaron tertulias literarias , reuniones culturales , lecturas poéticas, cursos de cultura catalana, etc, con el objetivo de llegar a la juventud y formar así nuevos escritores capaces de evitar la ruptura con la cultura y la lengua catalanas. Todos estos actos eran celebrados, sin embargo, en círculos privados y ante un público reducido y selecto. También comenzaron a publicarse una serie de revistas de vital importancia, algunas de las cuales fueron "Poesía" (1944-1945) , Ariel (revista) (1946-1951) y "Dau al Set" (1.948-1.955) . La primera, creada por Palau i Fabre, era de carácter vanguardista, porque además de un compromiso con la lengua tenía también un claro compromiso poético. La segunda, " Ariel ", fue ideada por Palau i Fabre, José Romeu y Parras, Miquel Tarradell, Joan Triadú y Frederic-Pau Verrié. Su antecedente más claro era "Poesía", y aunque no tenía un programa ideológico totalmente unificado, sobrevivió gracias a la voluntad común de recuperar determinados valores del pasado. Se negaron a lamentarse por el genocidio cultural y de ahí surgió el deseo de no detenerse en el resentimiento y traducirlo en acción. Querían demostrar que no había habido ruptura con el pasado, y lo hicieron recuperando estrategias pedagógicas de la tradición novecentista encaradas al futuro. Es en esta revista donde podemos vislumbrar claramente las dos tendencias del momento: por un lado , la tendencia que defendía el clasicismo como base de nuestra cultura y como modelo a seguir; por otro, la tendencia más vanguardista que defendía el arte moderno (Baudelaire, Rimbaud, Mallarmé, etc.). No obstante, tanto " Poesía" como " Ariel " supieron innovar sin dejar nunca de lado la tradición anterior. No obstante, tanto " Poesía" como " Ariel " supieron innovar sin dejar nunca de lado la tradición anterior. Por último, la revista "Dau al Set" contó con la colaboración de artistas como Modest Cuixart, Joan Ponç, Antoni Tàpies, Joan-Josep Tharrats, Arnau Puig y Joan Brossa que tenían como objetivo común conectar con la tradición vanguardista anterior a 1936, especialmente con el surrealismo.

La mayoría de los intelectuales catalanes que marcharon al exilio establecieron en Francia, excepto aquellos que pudieron evitar la invasión alemana marchando en América, México y Venezuela. Aunque tener que renunciar a la propia lengua, estos catalanes pudieron insertarse en una infraestructura cultural que les permitió trabajar como profesionales en diferentes campos (periodismo, mundo editorial , artes gráficas ) . Y si algo tenían en común estos exiliados con los intelectuales que se habían quedado en Cataluña trabajando en la clandestinidad, era la voluntad de dar continuidad a la lengua y la tradición literaria catalanas. De este modo, ya pesar de las penurias que tuvieron que sufrir los que marcharon al exilio, enseguida surgieron iniciativas para llevar a cabo esta continuidad. En 1939 ya se empezaron a editar libros y revistas en catalán, como por ejemplo la revista "Cataluña", editada con el objetivo de continuar la obra de los escritores catalanes. "Resurgimiento" y " Hermandad " son algunas de las otras revistas que también publicaron en el exilio los intelectuales catalanes. Con la misma voluntad de continuidad lingüística y cultural hay que destacar también el mantenimiento de los Juegos Florales de la Lengua Catalana entre 1941 y 1945.

En conclusión podemos decir que, a partir del año 1939, los intelectuales catalanes que se muestran reacios al nuevo orden político ponen en marcha, ya sea desde el exilio o desde la clandestinidad, diferentes iniciativas con una única voluntad común: la continuidad cultural y lingüística.

Autores como Josep Carner, el gran poeta Joan Salvat-Papasseit, Carles Riba, Josep Vicenç Foix, Salvador Espriu (cuyo poemario "La pell de brau", "La piel de toro", reivindica la unión de los pueblos hispánicos en un mutuo respeto a sus culturas y tradiciones), Pere Quart, Josep Maria de Sagarra, los grandes prosistas Josep Pla (cuyas obras completas sobrepasan los cuarenta volúmenes), Llorenç Villalonga (autor de la famosa novela "Bearn o La sala de las muñecas" y Mercè Rodoreda, célebre por su novela "La plaza del diamante", sobre la juventud y guerra civil de una joven muchacha barcelonesa en preguerra, guerra y posguerra civil, Maria Barbal, Pere Calders, Gabriel Ferrater, Manuel de Pedrolo, Vicent Andrés Estellés, el audaz poeta experimental Joan Brossa, Jesús Moncada, Quim Monzó, Miquel Martí i Pol o Miquel de Palol han sido reconocidos por todo el mundo, con ediciones y traducciones en diversos idiomas. A principios del siglo XXI, la producción de libros en catalán es importante, tanto en calidad como en cantidad, con autores como Julià de Jòdar, Jaume Cabré o Feliu Formosa entre otros.

Existen varios premios en la literatura catalana de renombre, entre ellos:




</doc>
<doc id="8352" url="https://es.wikipedia.org/wiki?curid=8352" title="Jules Cotard">
Jules Cotard

Jules Cotard (Issoudun, 1 de junio de 1840 — Vanves, 19 de agosto de 1889) fue un neurólogo y psiquiatra francés. Es conocido por describir y nombrar el síndrome de Cotard o delirio nihilista, que consiste en la creencia de que la persona está muerta, no existe o carece de órganos internos.

Estudió medicina en París, y después trabajó como interno en el Hospice de la Salpêtrière, donde trabajó para Jean-Martin Charcot. Allí se interesó por los accidentes cerebrovasculares y sus consecuencias, practicando autopsias para entender el efecto del cerebro.

En 1869 Cotard salió de la Salpêtrière y se incorporó al regimiento de infantería, al comienzo de la guerra franco-prusiana, como cirujano de esta rama militar.

Se mudó a Vanves en 1874, donde quedó por los últimos 15 años de su vida. Allí, hizo contribuciones importantes a la comprensión de la diabetes y delusiones (ideas deliriantes).

En agosto de 1889 la hija de Cotard contrajo la difteria. Él se negó a irse de su lado por 15 días. La hija de Cotard se recuperó, pero Cotard se contagió de la misma enfermedad y falleció el 19 de agosto de 1889.



</doc>
<doc id="8358" url="https://es.wikipedia.org/wiki?curid=8358" title="Timina">
Timina

La timina es un compuesto heterocíclico derivado de la pirimidina. Es una de las cinco bases nitrogenadas constituyentes de los ácidos nucleicos (las otras cuatro son la adenina, la guanina, la citosina, y el uracilo, este último sólo presente en el ARN). Forma parte del ADN y en el código genético se representa con la letra T. Forma el nucleósido timidina (dThd) y el nucleótido timidilato (dTMP). La timina fue descubierta en 1885 por el bioquímico alemán Albrecht Kossel.

En el ADN, la timina siempre se empareja con la adenina mediante dos enlaces o puentes de hidrógenos. Las uniones transversales en la estructura de doble hélice del ADN tienen lugar a través de las bases, que siempre se emparejan de forma específica.

wikipedia.org/wiki/Adenina


</doc>
<doc id="8359" url="https://es.wikipedia.org/wiki?curid=8359" title="Apoptosis">
Apoptosis

La apoptosis es una vía de destrucción o muerte celular programada o provocada por el mismo organismo, con el fin de controlar su desarrollo y crecimiento, puede ser de naturaleza fisiológica y está desencadenada por señales celulares controladas genéticamente. La apoptosis tiene una función muy importante en los organismos, pues hace posible la destrucción de las células dañadas, evitando la aparición de enfermedades como el cáncer, consecuencia de una replicación indiscriminada de una célula dañada.

En contraste con la necrosis —que en realidad no es una forma de muerte celular, sino que es un patrón morfológico que ocurre después de la muerte de un tejido en organismos vivos— resultante de un daño agudo a los tejidos, la apoptosis es un proceso ordenado, que generalmente confiere ventajas al conjunto del organismo durante su ciclo normal de vida. Por ejemplo, la diferenciación de los dedos humanos durante el desarrollo embrionario requiere que las células de las membranas intermedias inicien un proceso apoptótico para que los dedos puedan separarse, o la renovación epitelial de la mucosa intestinal. 

La apoptosis ha sido tema de creciente atención en la biología celular y en el estudio del desarrollo de los organismos, así como en la investigación de enfermedades tales como el cáncer. Así lo demuestra el hecho que el premio Nobel del año 2002 para Fisiología o Medicina fuese otorgado a Sydney Brenner (Gran Bretaña), H. Robert Horvitz (EUA) y John E. Sulston (GB) "por sus descubrimientos concernientes a la regulación genética del desarrollo de órganos y la muerte celular programada".

La palabra apoptosis procede del griego "apóptōsis", que significa: "apó" "a partir de" + "ptōsis" "caída". Se puede referir a la apoptosis mediante las expresiones: "muerte celular apoptótica" y "muerte celular programada".

La apoptosis puede ocurrir por ejemplo, cuando una célula se halla dañada y no tiene posibilidades de ser reparada, o cuando ha sido infectada por un virus. La "decisión" de iniciar la apoptosis puede provenir de la célula misma, del tejido circundante o de una reacción proveniente del sistema inmunológico. Cuando la capacidad de una célula para realizar la apoptosis se encuentra dañada (por ejemplo, debido a una mutación), o si el inicio de la apoptosis ha sido bloqueado (por un virus), la célula dañada puede continuar dividiéndose sin mayor restricción, resultando en un tumor, que puede ser un tumor canceroso (ver cáncer).

Ciertas células del sistema inmunitario, los linfocitos B y linfocitos T, pueden llegar a desarrollar propensión a atacar células de tejido sano del propio organismo al que pertenecen. Estas células "autorreactivas" son eliminadas mediante apoptosis. Por ejemplo, antes de ser liberados hacia el resto del organismo, los linfocitos T, una vez formados en la médula ósea, son sometidos a pruebas de reacciones autoinmunes dentro del timo, que es el órgano encargado de su maduración para evitar reacciones de autoinmunidad. De esta forma, alrededor del 95% de los linfocitos T recién creados —y que son los que han mostrado propensión a atacar tejido propio— son destruidos vía apoptosis.

La muerte celular programada es parte integral del desarrollo de los tejidos tanto de plantas (viridiplantae) como de animales pluricelulares (metazoa), y no provoca la respuesta inflamatoria característica de la necrosis (sobre apoptosis en metazoarios, véase "Mechanisms and Genes of Cellular Suicide", por Hermann Steller, "Science" Vol. 267, 10 de Mar., 1995, p. 1445). En otras palabras, la apoptosis no se parece al tipo de reacción resultante del daño a los tejidos debido a infecciones patogénicas o accidentes. En lugar de hincharse y reventar -y, por tanto, derramar su contenido, posiblemente dañino, hacia el espacio intercelular-, las células en proceso de apoptosis y sus núcleos se encogen, y con frecuencia se fragmentan. De esta manera, pueden ser eficientemente englobadas vía fagocitosis y, consecuentemente, sus componentes son reutilizados por macrófagos o por células del tejido adyacente. 

Existen dos razones diferentes para explicar por qué las células mueren por apoptosis: La eliminación de células en exceso y la eliminación de células que representan un peligro para la integridad del organismo.

Ejemplos de eliminación de células en exceso: 
Ejemplos de eliminación de células que representan un peligro para la integridad del organismo:

En un organismo adulto, la cantidad de células que componen un órgano o tejido debe permanecer constante, dentro de ciertos límites. Las células de la sangre y de piel, por ejemplo, son constantemente renovadas por sus respectivas células progenitoras. Por lo tanto, esta proliferación de nuevas células tiene que ser compensada por la muerte de otras células. A este proceso se le conoce como "homeostasis", aunque algunos autores e investigadores como Steven Rose y Antonio Damasio han sugerido "homeodinámica" como un término más preciso y elocuente (véase Damasio: "The Feeling of What Happens", Harcourt Brace & Co., New York, 1999, p. 141).

Los procesos de la apoptosis pueden ser activados por: 
Investigación científica indica que hay tres vías de apoptosis, el extrínseco, el intrínseco y el perforina/granzima. 

La vía extrínseca de señalización es la encargada del inicio de apoptosis, se encuentra involucrada a interacciones mediadas por receptores transmembrales como los receptores de muerte caracterizados por presentar un dominio extracelular, rico en cisteína con dominios citoplásmicos de aproximadamente 80 aminoácidos, y un segundo dominio de localización citoplasmática conocido como el «dominio de la muerte» que es el responsable de la apoptosis. Su activación siempre conduce a la muerte de la célula, en estos encontramos a: 
Apoptosis mediada por receptores de muerte se puede inhibir por la proteína c-FLIP lo cual se ata a FADD y la caspasa-8 causando ineficaces (Elmore 2007). 
En contraste con la vía extrínseca que se induce extracelularmente, la vía intrínseca se induce intracelularmente. La vía intrínseca de la apoptosis puede ser desencadenada por daño en el ADN, grandes aumentos en la concentración de calcio citosólico o estrés celular, así como un aumento en la generación de especies reactivas de oxígeno en la mitocondria. Esto activa la expresión del gen supresor de tumores p53, que a continuación, activa las proteínas pro-apoptóticas. PUMA y NOXA se expresan por el gen p53 y codifican para los dos miembros de la familia Bcl2 que gobiernan la permeabilización de la membrana mitocondrial externa, Bax y Bak. La expresión de estas proteínas provoca una translocación de la mitocondria, reduciendo la su membrana que resulta en la liberación de citocromo C y Apaf-1. Una vez que el citocromo C se une a Apaf-1 y procaspasa-9, se forma un apoptosoma. Este apoptosoma a continuación, activa la caspasa-9. Una vez que la caspasa-9 se activa, la mayor activación de otras caspasas, como las caspasas-3 y -7 permiten la digestión de los objetivos esenciales que afectan a la viabilidad celular.

La activación de los segundos mensajeros (p 53 y Bcl2) suele conducir a la disfunción de las organelas citoplasmáticas, como la mitocondria y el retículo endoplásmico, o la regulación de la actividad de complejos enzimáticos como cinasas y fosfatasas que a su vez regulan la función de otras proteínas.

Durante el procesamiento normal de señales tienen lugar aumentos transitorios de la [Ca2+] i . Sin embargo, incrementos aberrantes pueden producir daño celular y en algunos casos su muerte. En estos procesos el calcio puede activar enzimas como proteasas y lipasas, induciendo la producción de radicales libres, además de regular y potenciar la expresión génica al modular la actividad de factores de trasncripción. En condiciones fisiológicas, las células presentan un equilibrio entre la generación de radicales libres y los sistemas antioxidantes de defensa. En algunos procesos de muerte celular se ha descrito la ruptura de este equilibrio, observándose un aumento en la oxidación de proteí-nas con la formación de grupos carbonilo y peroxidación lipídica, habiéndose demostrado la existencia de una localización compartimentada de derivados carbonílicos libres a partir de lípidos, proteínas, hidratos de carbono y ácidos nucleicos (2,4- dinitrofenilhidracina).

La translocación de la ceramida a la mitocondria provoca cambios iónicos entre la matriz mitocondrial y el citoplasma, produciendo un descenso del potencial transmembranal y la formación del poro de permeabilidad transitoria mitocondrial, conduciendo a la apoptosis. Los valores de ceramida pueden ser aumentados tanto por factores externos (radiación UV, agentes oxidantes), como a través de receptores de membrana (FasR y TNFR) o directamente por glucocorticoides.

El aumento en los valores p53 conduce a la inducción en la transcripción de otros genes como p21/WAF1/Cip1, un inhibidor de proteínas cinasas reguladas por ciclinas, inhibiendo la entrada en fase S del ciclo celular. Como resultado la célula se detiene en la fase G1, la cual provee de una barrera cinética en la replicación de un genoma potencialmente dañado. Si la célula no puede reparar el daño genético, p53 induce la muerte celular por un mecanismo que se postula que puede estar mediado por aumentos en la síntesis de Bax, una proteína de la familia de Bcl-2 con propiedades proapoptóticas. un mal funcionamiento del gen p53 puede promover el desarrollo de tumores debido a la proliferación de células con una reparación del ADN de forma incompleta.

Una vez que la célula recibe una señal de muerte, debe decidir si debe sobrevivir o desencadenar los procesos de muerte. En esta fase de decisión se ha situado a la mitocondria como organelo fundamental. Como uno de los acontecimientos principales se altera la permeabilidad de las membranas mitocondriales a causa de la formación de un complejo multiproteico que conduce a la liberación del contenido intramitocondrial como el citocromo C, el factor inductor de apoptosis y miembros de la familia de caspasas. otros acontecimientos desencadenados en la membrana son la alteración de la cadena transportadora de electrones, la perdida de potencial electroquímico y cambios en el ciclo metabólico de óxido/reducción.

Una vez que la célula ha tomado la decisión de morir, en su interior se produce una serie de procesos bioquímicos que conducen a la degradación de proteínas y de la cromatina. Entre las proteasas implicadas en los procesos de muerte celular se encuentran las caspasas, las calpaínas, la granzima B y el complejo multiproteico denominado proteosoma.

La activación de las caspasas puede tener lugar en respuesta a estímulos tanto extracelulares como intracelulares. Estas hidrolizan secuencias específicas de tetrapéptidos que contienen un residuo aspartato. Entre sus sustratos se encuentran: elementos del citoesqueleto (actina, fodrina, proteína Tau y catenina), enzimas encargadas de reparar (PARP) o degradar (ADNasa) el ADN celular, factores de transcripción (retinoblastoma, HDM2), proteínas reguladoras (proteína cinasa C, fosfatasas 2A, cinasas de adhesión focal), así como miembros de la familia del oncogén Bcl-2 (Bid). Las calpaínas son cisteína proteasas que requieren Ca2+ para su traslocación hasta la membrana citoplasmática, rápida autólisis y activación. Entre sus sustratos se encuentran también factores de transcripción, oncogenes, proteí-nas de membrana y del citoesqueleto. Estas están sobreactivadas durante procesos excitotóxicos e isquémicos y en patologías como la enfermedad de Alzheimer.





</doc>
